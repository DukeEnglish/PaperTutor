[
    {
        "title": "MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions",
        "authors": "Kai ZhangYi LuanHexiang HuKenton LeeSiyuan QiaoWenhu ChenYu SuMing-Wei Chang",
        "links": "http://arxiv.org/abs/2403.19651v1",
        "entry_id": "http://arxiv.org/abs/2403.19651v1",
        "pdf_url": "http://arxiv.org/pdf/2403.19651v1",
        "summary": "Image retrieval, i.e., finding desired images given a reference image,\ninherently encompasses rich, multi-faceted search intents that are difficult to\ncapture solely using image-based measures. Recent work leverages text\ninstructions to allow users to more freely express their search intents.\nHowever, existing work primarily focuses on image pairs that are visually\nsimilar and/or can be characterized by a small set of pre-defined relations.\nThe core thesis of this paper is that text instructions can enable retrieving\nimages with richer relations beyond visual similarity. To show this, we\nintroduce MagicLens, a series of self-supervised image retrieval models that\nsupport open-ended instructions. MagicLens is built on a key novel insight:\nimage pairs that naturally occur on the same web pages contain a wide range of\nimplicit relations (e.g., inside view of), and we can bring those implicit\nrelations explicit by synthesizing instructions via large multimodal models\n(LMMs) and large language models (LLMs). Trained on 36.7M (query image,\ninstruction, target image) triplets with rich semantic relations mined from the\nweb, MagicLens achieves comparable or better results on eight benchmarks of\nvarious image retrieval tasks than prior state-of-the-art (SOTA) methods.\nRemarkably, it outperforms previous SOTA but with a 50X smaller model size on\nmultiple benchmarks. Additional human analyses on a 1.4M-image unseen corpus\nfurther demonstrate the diversity of search intents supported by MagicLens.",
        "updated": "2024-03-28 17:59:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.19651v1"
    },
    {
        "title": "Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models",
        "authors": "Samuel MarksCan RagerEric J. MichaudYonatan BelinkovDavid BauAaron Mueller",
        "links": "http://arxiv.org/abs/2403.19647v1",
        "entry_id": "http://arxiv.org/abs/2403.19647v1",
        "pdf_url": "http://arxiv.org/pdf/2403.19647v1",
        "summary": "We introduce methods for discovering and applying sparse feature circuits.\nThese are causally implicated subnetworks of human-interpretable features for\nexplaining language model behaviors. Circuits identified in prior work consist\nof polysemantic and difficult-to-interpret units like attention heads or\nneurons, rendering them unsuitable for many downstream applications. In\ncontrast, sparse feature circuits enable detailed understanding of\nunanticipated mechanisms. Because they are based on fine-grained units, sparse\nfeature circuits are useful for downstream tasks: We introduce SHIFT, where we\nimprove the generalization of a classifier by ablating features that a human\njudges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised\nand scalable interpretability pipeline by discovering thousands of sparse\nfeature circuits for automatically discovered model behaviors.",
        "updated": "2024-03-28 17:56:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.19647v1"
    },
    {
        "title": "Asymmetric and trial-dependent modeling: the contribution of LIA to SdSV Challenge Task 2",
        "authors": "Pierre-Michel BousquetMickael Rouvier",
        "links": "http://arxiv.org/abs/2403.19634v1",
        "entry_id": "http://arxiv.org/abs/2403.19634v1",
        "pdf_url": "http://arxiv.org/pdf/2403.19634v1",
        "summary": "The SdSv challenge Task 2 provided an opportunity to assess efficiency and\nrobustness of modern text-independent speaker verification systems. But it also\nmade it possible to test new approaches, capable of taking into account the\nmain issues of this challenge (duration, language, ...). This paper describes\nthe contributions of our laboratory to the speaker recognition field. These\ncontributions highlight two other challenges in addition to short-duration and\nlanguage: the mismatch between enrollment and test data and the one between\nsubsets of the evaluation trial dataset. The proposed approaches experimentally\nshow their relevance and efficiency on the SdSv evaluation, and could be of\ninterest in many real-life applications.",
        "updated": "2024-03-28 17:49:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.19634v1"
    },
    {
        "title": "Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models",
        "authors": "Yucheng ShiQiaoyu TanXuansheng WuShaochen ZhongKaixiong ZhouNinghao Liu",
        "links": "http://arxiv.org/abs/2403.19631v1",
        "entry_id": "http://arxiv.org/abs/2403.19631v1",
        "pdf_url": "http://arxiv.org/pdf/2403.19631v1",
        "summary": "Large Language Models (LLMs) have shown proficiency in question-answering\ntasks but often struggle to integrate real-time knowledge updates, leading to\npotentially outdated or inaccurate responses. This problem becomes even more\nchallenging when dealing with multi-hop questions since they require LLMs to\nupdate and integrate multiple knowledge pieces relevant to the questions. To\ntackle the problem, we propose the Retrieval-Augmented model Editing (RAE)\nframework tailored for multi-hop question answering. RAE first retrieves edited\nfacts and then refines the language model through in-context learning.\nSpecifically, our retrieval approach, based on mutual information maximization,\nleverages the reasoning abilities of LLMs to identify chain facts that na\\\"ive\nsimilarity-based searches might miss. Additionally, our framework incorporates\na pruning strategy to eliminate redundant information from the retrieved facts,\nwhich enhances the editing accuracy and mitigates the hallucination problem.\nOur framework is supported by theoretical justification for its fact retrieval\nefficacy. Finally, comprehensive evaluation across various LLMs validates RAE's\nability in providing accurate answers with updated knowledge.",
        "updated": "2024-03-28 17:47:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.19631v1"
    },
    {
        "title": "Semantic Map-based Generation of Navigation Instructions",
        "authors": "Chengzu LiChao ZhangSimone TeufelRama Sanand DoddipatlaSvetlana Stoyanchev",
        "links": "http://arxiv.org/abs/2403.19603v1",
        "entry_id": "http://arxiv.org/abs/2403.19603v1",
        "pdf_url": "http://arxiv.org/pdf/2403.19603v1",
        "summary": "We are interested in the generation of navigation instructions, either in\ntheir own right or as training material for robotic navigation task. In this\npaper, we propose a new approach to navigation instruction generation by\nframing the problem as an image captioning task using semantic maps as visual\ninput. Conventional approaches employ a sequence of panorama images to generate\nnavigation instructions. Semantic maps abstract away from visual details and\nfuse the information in multiple panorama images into a single top-down\nrepresentation, thereby reducing computational complexity to process the input.\nWe present a benchmark dataset for instruction generation using semantic maps,\npropose an initial model and ask human subjects to manually assess the quality\nof generated instructions. Our initial investigations show promise in using\nsemantic maps for instruction generation instead of a sequence of panorama\nimages, but there is vast scope for improvement. We release the code for data\npreparation and model training at https://github.com/chengzu-li/VLGen.",
        "updated": "2024-03-28 17:27:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.19603v1"
    }
]