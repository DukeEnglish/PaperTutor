Aliasing and Label-Independent Decomposition of Risk: Beyond the bias–variance
trade-off
Mark K. Transtrum∗ and Gus L. W. Hart
Department of Physics and Astronomy, Brigham Young University, Provo, Utah 84602, USA
Tyler J. Jarvis and Jared P. Whitehead
Department of Mathematics, Brigham Young University, Provo, Utah 84602, USA
A central problem in data science is to use potentially noisy samples of an unknown function to
predict function values for unseen inputs. In classical statistics, the predictive error is understood
asatrade-offbetweenthebiasandthevariancethatbalancesmodelsimplicitywithitsabilitytofit
complexfunctions. However,over-parameterizedmodelsexhibitcounter-intuitivebehaviors,suchas
“double descent” in which models of increasing complexity exhibit decreasing generalization error.
We introduce an alternative paradigm called the generalized aliasing decomposition. We explain
the asymptotically small error of complex models as a systematic “de-aliasing” that occurs in the
over-parameterized regime. In the limit of large models, the contribution due to aliasing vanishes,
leaving an expression for the asymptotic total error we call the invertibility failure of very large
models on few training points. Because the generalized aliasing decomposition can be explicitly
calculated from the relationship between model class and samples without seeing any data labels,
itcananswerquestionsrelatedtoexperimentaldesignandmodelselectionbefore collectingdataor
performing experiments. We demonstrate this approach using several examples, including classical
regression problems and a cluster expansion model used in materials science.
Predictive models allow scientists and engineers to ex- While the foregoing story has long been the central
tend data and anticipate outcomes for unseen cases. A dogma of statistics, we now know this view of the fit-
keyissueforthesemodelsistheproblemofhowtounder- ting problem is not the whole story (see [8] for the bias-
stand and minimize generalization error. Traditionally, variance decomposition for neural networks and [9] for a
people thought about generalization error in terms of a more thorough presentation of the topic). For extremely
trade-off between bias and variance, but that trade-off over-parameterized models (i.e., more parameters than
does not adequately explain the error curves for many samples), prediction errors often actually decrease with
models, especially models with more parameters than additional parameters. This phenomenon was recently
datapointsbutalsohighlystructuredscientificandengi- coined “double descent”[9], although aspects of the phe-
neeringdata. Inthiswork,weintroduceanewdecompo- nomena had been known much earlier [10]. More gen-
sition,generalizedaliasingdecomposition,thatexplainsa erally, prediction errors may exhibit complex, “multiple
widevarietyoferrorcurvesinpredictivemodelsforboth descents” [11–18].
small (classical) models and for large, overparametrized
Nonconvexriskcurvesaremostfamouslyrecognizedin
models. This decomposition explains complex general-
neural networks[19, 20], though this behavior has been
ization curves, including double and multiple descent,
observed in various other settings as well (see [10] for
without referencing data labels.
a thorough review). Several recent attempts have been
madetoexplainthisbehavior,focusingonregressionand
the simplest case of “double descent”, although recent
I. INTRODUCTION
studies show that risk curves may be far more compli-
cated [11]. In [14, 17], the bias–variance decomposition
Simple models are generally preferred for many rea-
is expanded to explain this non-convex behavior, relying
sons, including interpretability and computational ex-
ontheinterplaybetweenthemodeldesignandtheactual
pense [1–7], but one of the more important justifications
data. Several other efforts have been made to clarify the
forparsimonyisaneedtobalanceover-andunder-fitting.
relationship between the model class, inherent algorith-
Models with few parameters avoid making wild predic-
mic bias, testing and training data, and the appearance
tions but coarsely interpolate the observed data with-
of nonmonotonic loss and generalization curves. We do
out much fidelity (high bias), while over-parameterized
notpresentacomprehensivesummary, butdirectthein-
models fit the sampled data well with wild swings in
terestedreaderto[12,13,18,21]thatclarifythenatureof
between data points (high variance). The unquestioned
doubledescentanditsapparentrelianceonthestructure
goal has been to find the “sweet spot” of model complex-
of the testing and training data sets.
ity that balances bias and variance, i.e., a faithful model
of marginal complexity (see Figure 1, left panel). In contrast to these approaches, we build on insights
from signal processing [20] and introduce a novel decom-
position(17),whichwerefertoasthegeneralizedaliasing
∗ mktranstrum@byu.edu decomposition summarizedintherightpanelofFigure1.
4202
guA
51
]TS.htam[
1v49280.8042:viXra2
Usingthisdecomposition,wedemonstratethatanoncon- As an illustration, consider a polynomial fit on an in-
vex generalization curve is a generic phenomenon, which terval [a,b], and take the basis functions to be the usual
includes, as a special case, aliasing in the Fourier repre- monomial basis {1,t,t2,t3,...,td} for some d > 0; so
sentation of a time series wherein high-frequency (noisy) Φ =tj−1,andthedesignmatrixMistheVandermonde
j
components of a signal alias with the lower-frequency matrix[22]
(modeled) components, corrupting the representation.
 1 t t2 ... td
Aliasing errors can often be reduced by including more 1 1 1
terms in the model class than are required simply from M= 1 . t 2 t2 2 ... t .d 2 . (3)
the dimensionality of the data. . . 
. . 
Thealiasingerrorisdecomposedastheproductofthe
1 t t2 ... td
inverse of the classical design matrix and a term we call n n n
the nescience transform. The contribution from the de- Inferredparametervaluesθˆarefoundbyinvertingthe
signmatrixexplainsmuchofthenonmonotonicbehavior
design matrix. Since M is generally not square, an ap-
of the generalization curve. Model error due to alias- propriate pseudoinverse is used: θˆ=M+y. The Moore–
ing increases when the most recently added model terms
Penrose pseudoinverse is the standard choice for linear
are linearly dependent on already fitted aspects of the
regression,includingthismotivatingexample[23]. Other
model in the sampled subspace. While this aliasing er-
cases may require an algorithmic solution, but common
ror is generally maximized at the interpolation thresh-
algorithmic choices, such as stochastic gradient descent,
old, leading to the simplest version of double descent, it
are known to produce similar “norm-minimizing” solu-
is not necessarily maximized there, and real-world sam-
tions (see [17, 24, 25], for example).
plingstrategiescanproducecomplexbutunderstandable
Finally, predictions at unobserved values of the inde-
errorcurves(seeFigure4,forexample). Thescaleofthe
pendent variable are constructed
aliasingerrorsisdeterminedbythenormofthenescience
transform. Finally,theasymptoticerrorisdeterminedby fˆ(t)=(cid:88) Φ (t)θˆ . (4)
an invertibility term, which can be further decomposed j j
j
as a sum of bias in the modeled parameters and contri-
butions from the nescient parameters.
Aprimaryquantityofinterestistheso-calledgeneraliza-
Taken collectively, this decomposition explains all the tion error or population risk
qualitative features of generic risk curves. Itfurtherclar-
R (fˆ)=E [(y(t)−fˆ(t))2], (5)
ifies the roles of model structure, data sampling, data θ t
labels, and the learning algorithm in a way that is in- wheretheexpectationEistakenovera(usuallytheoret-
tuitive while formally facilitating key modeling decisions
ical) distribution of all of the data (not just the training
such as the choice of model class, experimental design,
samples), and the dependence of R on the model class
θ
regularization, and learning algorithm.
itself is implicit. A primary goal in data science is to
identify the model class and degree of complexity that
minimizes risk (5).
A. Aliasing and Decomposing Risk
Toprovideacommonvocabulary,westartwiththefa- II. RESULTS
miliar example of fitting one-dimensional data to a poly-
nomial. The example illustrates a phenomenon we call
Withthecontextgivenaboveinplace,wenowdescribe
the label-independent decomposition of risk and shows
ourgeneralresults. Wefirstassumethatthemodelfunc-
that double descent and nonconvex behavior of the gen-
tions Φ may be extended to form a complete set over
eralizationcurveisageneralbehaviorinfittingproblems.
the space of all possible predictions (including the train-
In regression, data y are given at samples of an inde-
ingpoints). TheWeierstrassapproximationtheorem[23]
pendentvariabletandusuallydecomposedasthesumof
guaranteesthatanycontinuousfunctiononaclosedinter-
anunknownsignalf (t)parameterizedbyθ andnoiseξ:
θ val [a,b] can be approximated arbitrarily well by a poly-
nomial; so if the full signal (modeled and unmodeled) is
y =f (t )+ξ , (1)
i θ i i a continuous function, then the infinite monomial basis
{1,t,t2,...} is complete, and an appropriate extension
where the subscript i refers to particular data sample.
for our example.
For linear regression the signal is a linear combination
(cid:80) Because the basis functions Φ are complete, we can
of basis functions f(t) = Φ (t)θ , so that the funda-
j j j also represent the noise component ξ of the signal as an
mental regression equation (1) becomes
expansion in Φ. Thus, the fundamental ansatz for our
analysis is
y =Mθ+ξ, (2)
(cid:88)
where the design matrix M is composed of samples of y(t)=f(t)= Φ j(t)θ j. (6)
basis functions, M =Φ (t ). j
ij j i3
Classical Regime Modern Regime
Variance
Bias
Model Complexity Model Complexity
FIG.1. (Leftpanel)Generalizationerrorsaretraditionallyunderstoodintermsofthebias–variancetrade-offwhicharedifficult
topredictinthemodern“interpolating” regime. (Rightpanel)TheGeneralizedAliasingDecompositionexplainsgeneralization
curves as contributions from Aliasing (blue) and Invertibility (red). The former explains the structure of mid-sized models,
while the latter explains the error for both very small and very large models. Aliasing is further decomposed as contributions
coming from the (pseudo) inverse of the traditional design matrix (dashed blue) that account for nonmonotonic structure in
thegeneralizationcurveandamonotonicallydecreasingnescience transformthataccountsforthescale. Theinvertibilityerror
is further decomposed in terms of a monotonically increasing parameter bias and monotonically deceasing contributions from
nescient parameters. This intuitive, interpretable decomposition applies throughout both classical and modern regimes.
That is to say, the noisy signal y(t) is an abstract vec- where the linear transformation M :Rm →Rn is the
TM
tor in a (potentially infinite-dimensional) vector space D usual designmatrixandwecallthelineartransformation
expressed in some basis as M the nescience transformation.
TU
y =Mθ, (7) Weusethewordnescience inthedefinitiontoempha-
sizethefactthattheunmodeledpartofthefunctionf is
where M is a bounded linear transformation mapping effectively unknown to us (noise, for example). Because
the vector θ in the parameter space Θ to y in the data M is bounded and linear, the subblocks M , M ,
TU PM
spaceD. Inthecaseoffittingapolynomialonaninterval and M are also bounded linear transformations.
PU
[a,b], the operator M could be thought of as a general- In the case of fitting a polynomial of degree at most d
ized Vandermonde matrix with countably infinite many on n training points t ,...t , the design matrix (upper
1 n
rows corresponding to rational points of [a,b] and count- left block) M is the Vandermonde matrix in (3) and
TM
ablyinfinitelymanycolumnscorrespondingtotj foreach the nescience transformation (upper right block) is the
nonnegative integer j.[26] semi-infinite matrix
Performing linear regression on samples of y(t) and  td+1 td+2 ...
makingpredictionsatunobservedvaluesoftcorresponds 1 1
td+1 td+2 ...
t To ap na drti pt rio en di in ctg ioD n Pint so uba spd air ce ec st . s Wum e wT ri⊕ teP y o inf t tr ha ii sn din eg - M TU =   2 . . . 2 . . .   .
composition as y = (y T,y P). We assume that T has td+1 td+2 ...
n n
finitedimensionn,butP neednotbefinite-dimensional.
The rows of the lower blocks correspond to the points in
The learning problem is this: Given observations in
[a,b]\{t ,...,t }.[28]Thecolumnsofthelowerleftblock
y , predict the components of y . In practice, this is 1 n
T P
correspondtothemonomials1,t,t2,...,td (spanningthe
done by similarly partitioning the representation space
space M) evaluated at the points of [a,b]\{t ,...,t }.
Θ = M⊕ U into a modeled M and an unmodeled U 1 n
The columns of the lower right block correspond to the
subspace so that θ = (θ ,θ ).[27] With these parti-
M U
unmodeled monomials td+1,td+2,... (spanning U), eval-
tions, the relationship of (7) between data and coordi-
uated at points of [a,b]\{t ,...,t }.
natestakestheblockrepresentationdescribedinthedef- 1 n
inition below. Welearnthemodeledparametersθˆ Musingsomepseu-
doinverse M+ of the design matrix M :
Definition. Denotethedecompositionofthelabeleddata TM TM
as θˆ =M+ y . (9)
M TM T
(cid:18) (cid:19) (cid:18) (cid:19)(cid:18) (cid:19)
y T = M TM M TU θ M , (8) Inferring only θˆ M is equivalent to assuming that the un-
y P M PM M PU θ U modeled parameters vanish, so θˆ =0 and θˆ=(θˆ ,0).
U M
ksiR
"tops
teews"
lacissalC
dlohserhT
noitalopretnI4
However, thetruerepresentationofthetrainingdatay drawn from a uniform distribution, is
T
includes contributions from both the modeled and ne-
(cid:88)
scient components of θ: R (yˆ)=E [(y(t)−yˆ(t))2]= (y(t)−yˆ(t))2
θ t
t
y T =M TMθ M+M TUθ U. =∥y−yˆ∥2 (14)
(cid:13) (cid:18) (cid:19) (cid:13)2
NoticethatthenescienttermM TUθ U correspondstothe
=(cid:13)
(cid:13)M
I m−B −A θ(cid:13)
(cid:13) , (15)
noise in (1). The inferred parameters θˆ are distorted (cid:13) 0 I (cid:13)
M
by the nescience term, which, in our extended represen-
where the norm ∥·∥ is the 2-norm ∥·∥ . This motivates
tation, takes the form: 2
the definition of the parameter error operator
θˆ M =(cid:0) M+ TMM TM(cid:1) θ M+(cid:0) M+ TMM TU(cid:1) θ U. (10)
E
=(cid:18) I m−B −A(cid:19)
. (16)
θ 0 I
For conceptual clarity, we write this as
We have used the subscript θ to indicate that E θ rep-
θ
θˆ=(cid:18) θˆ
M
(cid:19) =(cid:18) M+ TMM
TM
M+ TMM
TU
(cid:19)
θ
resentserrorsintheinferredparameters,ratherthanthe
0 0 0 errors y−yˆ in the signal.
(cid:18) (cid:19)
B A
= θ, (11)
0 0
1. Bias and Variance
where we have defined
For a fixed decomposition Θ = M⊕U, the estimator
yˆ and the operators A, B, and E depend on the choice
A=M+ M and B=M+ M , (12) θ
TM TU TM TM T = {t 1,...,t n} of the training points. The expected
valueE (R (yˆ))=E (cid:2) ∥yˆ−y∥2(cid:3) oftherisk(takenover
T θ T
and θ is the vector of parameters that represents the the distribution of T) is often decomposed into a sum
complete signal precisely. of a bias term and a variance term; see, for example,
We call A the generalized aliasing operator. It quanti- [29, §20.1]. In our current setting, this decomposition is
fies how the effects of the unmodeled, nescient parame- not intuitive and is difficult to analyze. We find it much
tersθ U areredirectedintothemodeledparameters. Note morenatural,andeasier,toanalyzetherisk∥y−yˆ∥2 and
that A depends not only on the partition between mod- its expected value E (cid:2) ∥y−yˆ∥2(cid:3) directly, rather than to
T
eled parameters and unmodeled modes, but also on the analyze the bias and variance terms separately.
partition between training points and prediction points
andthechoiceofpseudoinverse(orthechoiceoflearning
algorithm more generally). 2. Aliasing and Invertibility Error
In the case of Fourier series, the concept of aliasing
referstothedistortionofalow-frequencysignalbyhigh- A central question is how the risk changes with the
frequency modes. Expressed in the form we have de- dimension m of the modeled space M. Observe that
scribed, Fourier aliasing is found from A=M+ M ,
TM TU
expressed in the Fourier basis for uniform samples (see R (yˆ)=∥ME θ∥2 ≤∥M∥2∥E ∥2∥θ∥2,
θ θ θ
Section II.IIC for more on aliasing for Fourier series),
where it can be expressed in closed-form. More gener- wherethenormonthelineartransformationsMandE θ
ally, A quantifies how unmodeled components affect the is the induced norm, defined as
signal at the sampled points, leading to a misrepresen-
∥M∥= max ∥Mθ∥ and ∥E∥= max ∥Eθ∥.
tation of the inferred modeled parameters that we call
∥θ∥=1 ∥θ∥=1
generalized aliasing.
Inthefinitedimensionalcase(wherethetransformations
are represented by matrices) it is well known that if the
norm on both the domain and range of a matrix is the
A. Error Decomposition and Analysis
usual (two-) norm, then the induced norm is its largest
singular value. In this case the induced norm is often
The model described above uses θˆ = (θˆ M,0) to ap- called the spectral norm.
proximate the true function y with the model output: WeassumethetransformationMhasaboundednorm
and note that its norm is independent of the choice of
yˆ=Mθˆ=MM+ TMy T =M(Bθ M+Aθ U). (13) model M. The overall error certainly depends on θ and
itsnorm,but,asweshowbelow,thenon-convexgeneral-
Comparingthiswiththetruey =Mθ, weareinterested ization is largely driven by the parameter error operator
in the risk (5), which, assuming that the points t are E , rather than by specific choices of θ. We focus now
θ5
on E and write it as a sum of a term proportional to A Theorem II.1 explains the sharp peaks in generaliza-
θ
and the remainder: tioncurvesdescribedasdoubleandmultipledescent,and
it is relevant to other nonmonotonic features in both the
(cid:18) (cid:19) (cid:18) (cid:19)
0 −A I−B 0 under- and over-parameterized regimes, as we now de-
E = +
θ 0 0 0 I scribe.
=E +E (17) For a generic M the columns are typically arranged
A B
so that for m < n each column φ is independent of
m+1
TheoperatorE measurestheeffectofaliasing, andthe the previous columns of M . Hence, as m increases
A TM
operator E essentially measures two things: first, the the norm ∥M+ ∥ is expected to grow nearly monotoni-
B TM
failureofM tobeinvertible(intheupper-leftblock), cally until the interpolation threshold m=n. And once
TM
and second, how much of the signal is nescient, i.e., un- m ≥ n the columns of M are expected to span the
TM
modeled (the lower-right identity block). The general column space of the entire training set (M |M ) of
TM TU
shape of these different contributions to the risk is illus- the operator M, so each new column added to M
TM
trated in the right panel of Figure 1. We now analyze typically decreases the norm ∥M+ ∥.
TM
each of E A and E B in turn. In this case, ∥M+ TM∥ is a nondecreasing function of
m until m = n, after which it is nonincreasing. The
commonpeakinthegeneralizationerrorattheinterpola-
3. Analysis of ∥E A∥ and ∥E Aθ∥ tionthresholdisthusunderstoodasthepeakin∥M+ TM∥
at m = n. More complicated generalization curves can
AssumingthatθandMarefixedwithboundednorms, be generically understood by considering φ m+1’s linear
the norm ∥M ∥ of the nescience matrix is bounded by (in)dependence on the previously modeled terms.
TU
the norm of M and is a nonincreasing function of the Regardless of the ordering of the columns of M, the
model dimension m (see Section IV.IVB1 for details), upper bound
which means that the structure of
∥A∥≤∥M+ ∥∥M ∥
TM TU
∥E θ∥=∥Aθ ∥=∥M+ M θ ∥
A U TM TU U
cannot increase when stepping from m to m+1 unless
≤∥M+ ∥∥M ∥∥θ ∥
TM TU U the next column φ is independent of the previous
m+1
columns. Moreover, the bound will almost surely shrink
as a function of m is primarily determined by the norm
of the inverse design matrix M+ . to 0 as m→∞.
TM Of course, one can arrange to add columns to M
Consider the situation where a given decomposition TM
in a way that the rank of M grows slower than ex-
Θ=M⊕U ofparameterspacewithdesignmatrixM TM
TM
pected, permittingtheconstructionofdescentcurvesfor
and nescience M is changed by moving one column
TU
∥A∥ of various shapes. But when the columns are suffi-
φ out of the nescience matrix and into the design ma-
ciently general (as, for example, with the random ReLU
trix. This always decreases (or rather never increases)
the norm ∥M ∥ but the effect on ∥M+ ∥ is deter- features (RRF) model and the random Fourier features
TU TM (RFF) model), the result for ∥A∥ is similar in shape to
mined primarily by whether φ is linearly independent
the standard double descent curve for mean-squared er-
from the other columns of M or not, as described in
TM
ror, described in [9] with a single large peak at the inter-
the following theorem (proved in Section IV.IVB).
polation threshold and decreasing monotonically there-
TheoremII.1. Whenchangingthemodelbymovingone after (see Figure 2).
column φ outof thenesciencematrix andintothe design
matrix, the norm ∥M ∥ never increases and
TU
4. Analysis of ∥E ∥ and ∥E θ∥
• ∥M+ ∥ cannot decrease if φ is linearly indepen- B B
TM
dent of the other columns of M ,
TM It is straightforward (see Section IV.IVC for details)
• ∥M+ ∥ cannot increase if φ is linearly depen- to show that E B can be written as
TM
dent upon the other columns of M .
TM (cid:18) (cid:19)
P 0
E = N , (18)
Moreover, as the model dimension m increases to ∞, the B 0 I U
norm ∥M+ ∥ shrinks to 0, almost surely.
TM
where P is the orthogonal projection of M onto the
N
Although it can be arranged so that ∥M+ ∥ remains nullspaceN ⊆MofM ,andwhereI istheidentity
TM TM U
constantwhenmovingonecolumnφfomM toM , operator on the nescient space U. This implies that its
TU TM
in most cases we see a significant increase in ∥M+ ∥ induced operator norm is always[30] equal to 1, and the
TM
whenever φ is independent from the previous columns norm of the product E θ is bounded by the norm of θ:
B
and a significant decrease in ∥M+ ∥ whenever φ is de-
TM
pendent upon the previous columns. ∥E θ∥≤∥E ∥∥θ∥=∥θ∥. (19)
B B6
To make more fine-grained statements about the de- as contributions from both aliasing and nescience van-
pendence of E θ on the model dimension m, we write ish almost surely. The over-modeling phenomenon has
B
largely been overlooked in the double descent literature
∥E Bθ∥2 =∥P N(m)θ M(m)∥2+∥θ U(m)∥2 (20) because it is absent in the unstructured model approach
usedbymosttheoreticalstudies. Theexampleinsubsec-
where we have made the m-dependence explicit. No-
tionIIDbelowillustratesthatunstructuredmodelstend
tice that the first term ∥P θ ∥2 = ∥θ − θˆ ∥2 is
N M M M to have optimal performance in the asymptotic limit,
the (square of the) bias in θ while the second term
M while models that exploit prior information often have
is exclusively from the nescient parameters, θ . Since
U optimalperformanceattheclassical“sweetspot,” dueto
N(m) ⊆ N(m + 1), it follows that the contribution
asymptotic over-modeling.
∥P θ ∥ from parameter bias is a nondecreasing func-
N M
tion of m. In contrast, the contributions from the ne-
scient parameters is nonincreasing, since it decreases by
exactly |θ |2 at each step. Depending on the struc- B. Some Examples
m+1
ture of the parameter vector, the full invertibility error
may have nonmonotonic behavior due to the interplay AlthoughthemotivatingexampleinSectionIAforthe
between these contributions. aliasing decomposition was focused on one-dimensional
We first introduce the unstructured model ansatz in polynomials, this decomposition applies much more gen-
which components of θ are independent, identically dis- erally to the problem of fitting a function f :Rd →R or
tributed random variables with mean zero and variance f : Rd → C for arbitrary d. We illustrate this with ex-
σ2.[31] In this setting the expected invertibility error is amplesoftwodifferentchoicesofmodelsappliedtothree
different data sets. The two bases are random Fourier
E θ(cid:2) ∥E Bθ∥2(cid:3) =σ2(TrP N +TrI U) (21) features (RFF) and random ReLU features (RRF) (de-
=σ2(dimN +dimU). (22) scribed in [9] and [32]).
All of these basis functions are of the form ϕ (t) =
k
At each step, dimU decreases by one, while dimN in- σ(⟨t,v ⟩), where the v ∈ Rd are i.i.d. normal, and σ is
k k
creases by either zero or one so the total invertibility some activation function. In the case of the RRF model,
error is a strictly nonincreasing function of m. theactivationfunctionistheusualReLU,andinthecase
In the generic case, dimN(m) = 0 when the num- of RFF the activation function is σ(x) = exp(iπx). The
ber n of rows (data points) is less than the number m models that result from using these two choices (either
of columns (model parameters). This is the well-known RRF or RFF) can both be thought of as 2-layer neural
factthattheordinaryleastsquaresestimatorisunbiased. networks of the form
However, for m>n, dimN(m+1)=dimN(m)+1, so
parameter bias grows monotonically beyond the inter- (cid:88)m (cid:88)m
y(t)= θ ϕ (t)= θ σ(⟨t,v ⟩).
polation threshold and, on average, cancels the decrease k k k k
from the nescient parameters. k=1 k=1
In contrast to the unstructured case, modelers often
The data sets are (unlableled) images from MNIST
havepriorinformationaboutwhichparametersaremost
and CIFAR-10 and points from the Mei–Montanari [32]
important and preferentially order the parameter vector √
to reflect this. In such cases, the total parameter error sphere Sd−1( d), where we have arbitrarily fixed d =
∥(E A+E B)θ∥ tends to be dominated by the invertibil- 1024. In each case 1,000 training points t i were drawn
ity error ∥E θ∥ for very small and very large model size uniformly and evaluated at 6,000 basis functions (either
B
m, where aliasing ∥E θ∥ is small. For very small m, as RRF or RFF). The columns of the resulting design ma-
A
m increases there is often an initial descent of param- trixM TM andnesciencematrixM TU arealloftheform
eter error due to fewer nescient terms and the model’s φ k =(ϕ k(t 1),...,ϕ k(t n)).
increasing ability to capture the signal faithfully. This is In Figure 2 we show the norms of the matrices A,
conceptually analogous to reducing bias in the classical M+ TM, and M TU as functions of the number m of pa-
bias–varianceparadigm. However, forverylargemodels, rametersforthesemodelsonthethreedifferentdatasets.
the parameter bias can grow faster than the decrease in The horizontal axis in Figure 2 indicates the number m
the nescient contribution, resulting in growing ∥E θ∥ as of parameters, while the vertical axis shows the norm
B
depicted in Fig. 1. This growing error for large models ∥E A∥=∥A∥ and the norms ∥M+ TM∥ and ∥M TU∥ of its
is not analogous to variance and cannot be termed over- twofactors. Thenorm∥M TU∥ofthenesciencematrixis
fitting. Rather,itreflectsthelackofinvertibilityforlarge strictlynonincreasing,whilethepeakattheinterpolation
models, specifically, larger parameter bias as more of the threshold m = n comes entirely from the pseudoinverse
mass of θ is projected into the kernel N of the design ∥M+ ∥ of the design matrix.
TM
matrix M . In the examples in Figure 2 the norm ∥M+ ∥ gener-
TM TM
Werefertothephenomenonofnonmonotonicbehavior ally increases up to the interpolation threshold because
intheinvertibilityerrorasover-modeling. Over-modeling the random choices in RRF and RFF make it likely that
isthedominanterrorintheasymptoticlimitofm→∞, the first n columns φ ,...,φ are linearly independent.
1 n7
RRF on MNIST RRF on CIFAR RRF on MM
104 M 2
EA 2 102
M+
2
100
10 2
RFF on MNIST RFF on CIFAR RFF on MM
104
102
100
10 2
0 10 20 30 40 50 60 0 10 20 30 40 50 60 0 10 20 30 40 50 60
Number of parameters ×100
FIG. 2. The induced (spectral) norm of the error matrix E , and its factors, the design matrix M+ and nescience matrix
A TM
M fortherandomFourierfeatures(RFF)modelandtherandomReLUfeatures(RRF)modelontheMNISTandCIFAR-10
TU
datasets,asin[9],aswellasontheMei-Montanari(MM)sphere[32]. Ineachcasethemodelsweretrainedon1,000randomly
chosen training points (vertical black line) with the number of modeled parameters ranging from 1 up to 6,000. The norm
∥E ∥ is bounded by the product of ∥M+ ∥ and ∥M ∥. The norm ∥M ∥ is nonincreasing and decreases slowly, so the
A TM TU TU
familiar double descent shape with a peak at the interpolation threshold is driven by ∥M+ ∥.
TM
The norm then decreases after the interpolation thresh- and in this example (to illustrate the signals-processing
old because all the subsequent columns are, of necessity, version of aliasing) we select the same number of basis
linearly dependent on the columns that precede them. functions n as training points, that is m = n. The test-
ingpointsaret ,t ,...,whicharepointsintheinterval
n n+1
(0,T)\{t ,...,t }. The design matrix is a variant of
0 n−1
C. Why ‘aliasing’? Discrete Fourier series the Vandermonde matrix
 
1 1 1 ··· 1
To clarify the name “generalized aliasing,” we turn to
1 ω−1 ω−2 ··· ω−(n−1) 
anexamplefamiliarinthesignals-processingcommunity,  n n n 
the LeF tou frie ∈r d Lec 2o (m [0,p Tos ])iti bon e.
a square-integrable function
M TM = n1  

1
.
. .
ω n−
.
.
.2 ω n−
.
.
.4 · .· .·
.
ω n−2(
.
.
.n−1) 

,
on the interval [0,T]. Assume equally spaced train-  
1 ω−(n−1) ω−2(n−1) ··· ω−(n−1)2
ing points 0 = t < ··· < t = T so that t = n n n
0 n k
(25)
kT/n with training vector y = (f ,f ,...,f ) =
T 0 1 n−1
and the nescience matrix is bi-infinite with n rows and
(f(t ),f(t ),...,f(t )), sampled from the function f.
0 1 n−1 columns 1(ω−k,ω−2k,...,ω−(n−1)k) for each k ∈ Z \
For convenience, we define ω n = exp(2πı/n), as a n n n n
primitive n-th root of unity and introduce the Fourier {0,1,2,...,n−1}. Since ωℓn = 1 for any integer ℓ, the
n
basis vectors w(k) = (ω0,ωk,...,ω(n−1)k). The dis- nescience matrix is equal to an infinite number of copies
crete Fourier tran nsform isn then vectorn of coefficients fˆ= of the design matrix
(fˆ 0,fˆ 1,...,fˆ n−1) such that M
TU
=(cid:0) ··· M
TM
M
TM
···(cid:1) .
f
=n (cid:88)−1
fˆ kw n(k), (23)
B fue llca ru as ne k,w ae nh da Mve +selec =ted Mm −1=
.
n T, ht uh se Ade =sig Mn m −1at Mrix is
TM TM TM TU
k=0 and B=I . This gives
n
whereorthonormalityoftheFourierbasisintheℓ2 inner- A=M−1 (cid:0) ··· M M ···(cid:1)
productspaceallowsustoidentifytheFouriercoefficients TM TM TM
(26)
fˆ
=(cid:68) w(k),f(cid:69)
=
1 n (cid:88)−1
ω−klf . (24)
(cid:0)
··· I n I n I n
···(cid:1)
; (27)
k n n n l
l=0 thatis,Aisabi-infinitematrix(infinitelymanycolumns
In the context of the aliasing decomposition frame- inbothdirections)withnrowsandconsistingofinfinitely
work, the points t ,...t are the n training points, many copies of the n×n identity matrix I .
0 n−1 n
mroN
mroN8
1.0 4 reasonable model predictions.
True signal
0.5 2 Classical
Interpolation 0.0 0 Modern
−0.5 D. Real-World Example: Cluster Expansion
−2
−1.0
0.0 0.5 1.0 −4
0.0 0.5 1.0 Finally, we consider a model frequently used in high-
t t
throughput materials discovery. The cluster expansion
modelisageneralizedIsingmodel[34–42]thatintypical
FIG. 3. Aliasing occurs when basis functions that are inde-
pendentovertheentiredomainarelinearlydependentatthe applications has hundreds to thousands of data points
sampledpoints(left). Whenfittinganoisysignal(right),the and a dozen to hundreds of inferred parameters. The
classicalsweetspotincludesthedominantmodesinthesignal prototypical application of the cluster expansion is pre-
(blue). Over-fitting occurs when the combined contribution dictingtheenergyE ofanalloyasafunctionofelemental
from the unmodeled modes is aliased into the model param- composition and configuration ⃗σ.
eters, producing wild swings in the model predictions (red).
Including additional terms allows the learning algorithm to (cid:88) (cid:88) (cid:88)
E(⃗σ)=J + J ξ + J ξ ξ + J ξ ξ ξ
distribute that signal over several basis terms. The result is 0 1 i 2 i j 3 i j k
a model whose predictions oscillate rapidly on a scale that is i i,j i,j,k
statistically similar to the true signal (green). (cid:88)
+ J ξ ξ ξ ξ +··· , (28)
4 i j k l
i,j,k,l
Thisderivationalignsexactlywiththetraditionalcon- where the indices run over all the possible sites, pairs of
cept of aliasing in the signals-processing literature [33], sites, triples, and so on. The J’s are expansion coeffi-
where the first column of the ℓ-th copy of I in A corre- cients (inferred parameters, analogous to the θ’s in the
n
sponds to the ℓnth mode of the system, which is exactly notationabove). Theproductsofξ(⃗σ)functions[43]form
aliasedtothe0-thmode;thesecondcolumnofeachcopy anorthogonalbasisinthediscretevectorspaceofallpos-
of I corresponds to the (ℓn+1)-th mode which is ex- sible atomic configurations. This model has a physically
n
actlyaliasedtothefirstmodeoftheactualsignal,andso intuitiveinterpretation. Aproductoftwofunctions,ξ ξ ,
i j
forth. Unless the signal is band-limited, an infinite num- represents a pairwise interaction between atoms on sites
ber of modes are aliased to each of the modeled modes. iandj. ThesignofJ determineswhetherlikeorunlike
ij
Traditionally,thealiasingeffectisnotsignificantbecause atoms prefer to be ij-neighbors.
signalsareassumedtohavemostoftheirstrengthinthe One can enumerate all possible configurations (up to
lower frequencies, that is the magnitude of the higher some maximum number of atoms)[44–46] and determine
modes θ is assumed to decay to 0 rapidly as k → ±∞, acompletesetofbasisfunctions[35]. Choosingarealistic
k
which means that although A is bi-infinite, its effect is model size, we explain the resulting generalization curve
minimal on the actual representation of the signal. through the lens of generalized aliasing. A binary alloy
This mathematical derivation is represented visually model containing up to ten unique atomic sites has 2346
in the left panel of Fig. 3: although basis functions are unique configurations.[47] Figure 4 shows the norms of
independentovertheentirepredictiondomain,theymay the aliasing, nescience, and invertability operators as a
make identical predictions over the sampled subset (red function of increasing basis size for a fixed number of
dots). If the true signal contains contributions from all training points.
basis functions, but only a subset is explicitly modeled, We first consider the case where there is no natural
the contribution from the unmodeled modes is aliased ordering to either the data or parameters by randomiz-
into the truncated representation. ingrowsandcolumnsofM,inlinewiththeunstructured
The right panel shows three fits for an artificial data modelansatzabove. Theresultingbehavior(leftpanelof
set using the Fourier basis. The true signal (black) in- Fig. 4) aligns with most presentations of double descent
cludes contributions from all Fourier modes (although in the literature. Before the interpolation threshold, the
the low-frequency modes dominate). The classical sweet behavior of the risk curve is the expected U-shape of
spot (blue) only models the dominant modes and pro- the classical bias/variance trade-off. Beyond the inter-
duces a reasonable interpolation. At the interpolation polation threshold, the norms drop, consistent with the
threshold (red), however, the aliasing operator magni- generalized aliasing analysis. Furthermore, the optimal
fies the unmodeled modes, producing large swings in the modelisnotattheclassicalsweetspotbutintheasymp-
modelpredictionsbetweenthetrainingsamples. Beyond totic modern regime since there is little over-modeling.
the interpolation threshold (green), the additional, high- Likemanymodelingproblems,practitionershavesome
frequency basis elements temper the aliasing effects by intuition about the natural order for the sample points
redistributing the signal among multiple basis functions. and basis functions. In the typical preferred ordering for
The result is a rapidly oscillating signal that does not cluster expansion, pair-wise interactions precede triplet
exhibit the wild swings of overfitting. Although the os- interactions, and all triplet interactions come before any
cillations in this inferred signal do not match those of quadruplets,andsoforth(reflectedinthecoloringofthe
the true signal, they are statistically similar, leading to x-axis in the right panel of Fig. 4). Furthermore, the
)t(y )t(y9
FIG. 4. Norm of the operators and true risk of the cluster expansion model of Section IID as model complexity is increased
(i.e.,asparametersareadded)with600trainingpoints. Theinterpolationthresholdisindicatedbyaverticaldashedline. The
redcurveatthetopofeachpanelisanindicatorfunctionwhichishighwhentheaddedbasisfunctionislinearlyindependent
andlowwhenitislinearlydependent. (Left)RowsandcolumnsofMhavebeenrandomlyordered. (Right)Multiplepeaksand
valleys occur (indicated by dash-dot vertical lines) when the rows and columns of the design matrix are given by a “physical”
ordering. Thecolorsonthex-axisindicate“vertexorder” (seetext)oftheatomicinteractions: blue→pairwise,magenta→three-
body, green→four-body, etc.
terms are ordered in each class by diameter—short pairs explains the actual risk, indicated by the black-colored
beforelongpairs,smalldiametertripletsbeforeextended curvesinFig.4Intherightpanel,verticaldash-dotlines
triplets,etc. Thisorderingismotivatedbyphysicalargu- are included to clarify the connection between peaks in
mentsthatthestrongestinteractionsareshort-rangeand the operator norms ∥M+ ∥ and ∥A∥ (orange and blue)
TM
low-body. For the ordering of the sample points (atomic and peaks in the risk (solid black).
configurations, rows of M), there is the coarse guideline
of ordering by “size,” denoted by the number of atoms in The complicated generalization curve is inconsistent
each configuration, but within each size class a natural with either the classical bias–variance trade-off or other
ordering is not obvious. explanations of double descent. In contrast, the close
For the right panel of Fig. 4, this n-body/short-long relationship between peaks in the risk (prediction error)
orderingwasusedtoarrangecolumnsandrowsinM. In and peaks in the norm of M+ is striking. In the left
TM
addition to the complex behavior of the operator norms, panel, the only peak in the norms, and in the risk, is at
note that the peak of the generalization error is not at the interpolation threshold. This behavior for the ran-
the naive interpolation threshold where the number of domized case is much easier to rationalize, but the risk
parameters is equal to the number of training points. In (prediction error) is much higher than for the naturally
fact, the interpolation threshold appears to not play a orderedcaseintherightpanel. Bypreferentiallyordering
significant role in the generalization curve. according to the physically dominant terms, the contri-
butiontothesignalfromthenesciencematrixM θ is
The norms show a complicated behavior, neither the TU U
small,scalingdownthealiasingeffects. Furthermore,for
typical U-shape of classical bias–variance trade-off nor
large models, the physical ordering exhibits asymptotic
the basic double descent. Rather, the generalization
over-modeling, so that the optimal risk occuring at the
curve has multiple peaks and valleys, whose positions
classical “sweet-spot.”
correspondtolocationswhereaddedbasisfunctionstran-
sitionfromlinearindependencetolineardependence(red
“indicator function”).
Why do the norms of the different parts of the decom-
position have this complicated structure? The answer is
structureintroducedbythephysicalorderingoftherows
and columns of the design matrix. As shown by the red
indicator function near the top of the right panel, ba-
sis functions that increase the rank of the design matrix III. DISCUSSION
M increase the norm of its pseudoinverse and of the
TM
aliasing matrix A. Note the relationship between the in-
dicator function (red) and the norms in the lower panel Theprecedingformalanalysisandexamplesofthegen-
of Fig. 4. eralized aliasing decomposition give practical, intuitive
Finally, we ask how well the aliasing decomposition guidance for formulating models.10
A. General Insights into Modeling
Legendre Basis on [ 1,1]
1. Choosing the Basis 1012
1010
Ifthentrainingpointsareknownandfixed,amodeler can control the norm of M+ and A (and hence gener- 108 E Random pts
TM E Legendre-Gauss pts
ically control the magnitude of the risk) by strategically 105
choosing the basis functions, without knowing anything
about the labels y.
102
For example, consider what happens when we choose
the first n basis functions ϕ so that, when evaluated 0 200 400 600 800 1000
k
Number of training Points
at the points t ,...,t , the resulting vectors φ =
1 n k
(ϕ (t ),...,ϕ (t )) are orthonormal. If the columns of
k 1 k n
M are the first m ≤ n of these vectors, then the
TM FIG.5. TheinducednormofE fortheLegendrebasiswith
inverse M+ has induced norm ∥M+ ∥ = 1. In this A
TM TM the model consisting of the first m = 300 Legendre polyno-
situation the norm is constant as m increases up to n;
mials. Thenormsareplottedasfunctionsofthenumbernof
andthenform>n,nomatterwhichadditionalcolumns
trainingpoints,andtheverticalblacklineindicatestheinter-
are added, the norm ∥M+ TM∥ cannot increase and will polation threshold. The solid line shows the result when the
eventually shrink to 0 (almost surely). Thus, the prod- training points are chosen randomly (drawn uniformly from
uct ∥M+ ∥∥M ∥ in the upper bound [−1,1]), while the dotted line shows the results when the n
TM TU
training points are chosen to be the Legendre–Gauss points
∥A∥≤∥M+ ∥∥M ∥ (thezerosofthenthLegendrepolynomial). ThenormofAis
TM TU many orders of magnitude larger for randomly chosen train-
ing points than for the Legendre–Gauss points.
on the norm of A also can never increase with m, and
we expect there to be no peak in ∥E θ∥ at all—only
A
descent.
3. Conditioning of M
InthediscreteFourierseriesexample(SectionII.IIC),
thenormofAisalways1anddoesnotdecreaseto0be-
IfMispoorlyconditioned,thenitispossibletohavea
cause the columns of M are specially tuned to the train-
relatively small error E θ in the parameters that corre-
ing set to make M consist of infinitely many copies θ
TU sponds to a large error in the signal. Thus it is desirable
of M . This aligning of the basis functions to sample
TM to select a basis that makes the full transformation M
pointsexplainswhyextremeover-fittingisrarelyaprob-
well conditioned.
lem in discrete Fourier transforms, in spite of it being
For polynomial approximation with the standard
formally equivalent to ordinary least squares regression
monomial basis {1,t,t2,...}, the transformation M is
at the interpolation threshold.
a generalized Vandermonde matrix, which is very badly
conditioned and generally should not be used with real-
valuedinputs.[49]Butpolynomialapproximationforreal
2. Choosing Training Points inputs in the interval [−1,1] is well conditioned with the
Chebyshev polynomial basis or the Legendre polynomial
If the basis functions are given and fixed, but the basis.
modeler has some control over the choice of the training
points, then they can control the norm ∥A∥ by strategi-
cally choosing the points t ,...,t . Again, this requires B. Regularization
1 n
no knowledge of the labels y.
For example, consider the case of fitting polyno- It has been observed that L2-regularization (ridge re-
mial functions on the interval [−1,1] with the Legendre gression) generally reduces the size of the peak in risk
basis.[48]Foragivennumbermofmodelparameters(the at the interpolation threshold, but it can also increase
first m Legendre polynomials), if we are able to choose bias [32, 50, 51]. This can be understood in terms of
n points at which to evaluate the basis functions, then the impact of regularization on the pseudoinverse design
choosing the points to be the n Legendre–Gauss points, matrix.
whicharethezerosofP n, givesmuchbetterresultsthan ForagivendecompositionofthespaceΘ=M⊕U with
choosingthepointsrandomly(drawnuniformly). Thisis m=dimMmodelparameters,ridgeregressionamounts
shown in Figure 5, where the randomly chosen training to changing the objective from minimizing risk to mini-
pointsmake∥E A∥manyordersofmagnitudelargerthan mizing
withthespeciallychosenLegendre–Gausspoints. Inthis
case a judicious choice of training points makes a huge 1 ∥y−M θ ∥2+λ∥θ ∥2, (29)
difference. n TM M 2 M 2
mroN11
where n is the the number of training points and λ is the concepts of aliasing and invertibility extend formally
a user-chosen parameter. As shown in Section IV.IVD, tononlinearoperatorsandcanbeapproximatedthrough
this is equivalent to the alternative problem of minimiz- local linearization. Furthermore, many cases of practi-
ing cal importance may be tractable in the present frame-
work. Neural tangent kernel techniques, for example,
(cid:13) (cid:13)2
(cid:13) (cid:13)y (cid:101)−M(cid:102)TMθ M(cid:13)
(cid:13)
, demonstrate that wide networks are linear in their mod-
2 els throughout training[52]. In addition, information ge-
ometrytechniquesappliedtolarge,“sloppy” modelshave
wherey (cid:101)= √(y,0)andthesmallestsingluarvalueofM(cid:102)TM
shown that most nonlinearity is “parameter-effects” and
is at least nλ. That means the norm of the pseudoin-
removable, in principle, through an appropriate, nonlin-
verse satisfies
ear reparameterization[53].
1 An important open question is: Under what condi-
∥M(cid:102)+ ∥≤ √ .
TM nλ tionsistheasymptoticrisklessthanthatoftheclassical
“sweetspot”? Theprecedinganalysishassharpenedthat
This bound is independent of both m and M TM, and question to: When will there be over-modeling? Ran-
it essentially removes the impact of any small singular domfeaturemodels,suchasinFigure2,butpresumably
values of M TM on the norms of M+ TM and A. This alsoneuralnetworksandothermachinelearningmodels,
explains why there is no significant peak in the risk at are approximately unstructured, so they do not exhibit
the interpolation threshold (or anywhere else, for that over-modeling and are generically most effective in the
matter) for L 2-regularized (ridge regression) problems, over-parameterized,modernregime. Incontrast,physics-
provi√ded λ is sufficiently large. based models are most effective in the classical regime,
If nλ>∥M TU∥, then the norm of A is smaller than where they leverage prior knowledge.
the norm of E , so risk is always dominated by invert-
B Framingthequestioninthiswayclarifieswhyclassical
ibility error E θ.
B statistics historically missed these interesting phenom-
Theinvertibilityerror,however,canincreasewithreg-
ena, in spite of the essential elements being known to
ularization because the model bias term is no longer the
diverse communities for decades [10]. It also apparently
projection of θ onto the null space N of M but
M TM partitions predictive modeling into two philosophically
instead is ∥(I
M
−M(cid:102)+ TMM TM)θ M∥. When λ is large,
distinct camps: physical models using classical statistics
the fact that ∥M(cid:102)+ TMM TM∥ ≤ ∥M √T nM
λ
∥, means that the and unstructured models in the modern, interpolating
model bias term approaches ∥θ ∥, which is generally regime. In our cluster expansion example, the former
M
larger than the projection ∥P θ ∥. Nevertheless, the approach gave the model with the least risk. Although
N M
norm of E , while no longer necessarily bounded by 1, perhaps expected, as physics-based modeling leverages
B
is still bounded by prior information, this benefit comes after considerable
effort from the materials science community. However,
∥E ∥≤1+
∥M √TM∥
.
it remains unclear if these are inherently irreconcilable
B nλ philosophies or two points on a broad landscape just be-
ginning to be explored.
Indeed,ourworkdemonstrateshowthetheoreticaland
C. Outlook
technical challenges posed by modern data science over-
lap with those in other fields, including signal process-
Successful model building involves numerous technical ing,controltheory,andstatisticalphysics. Wehopethat
decisions related to the selection of model class, exper- the perspectives advanced here will inspire theorists and
imental design, learning algorithm, regularization, and practitioners alike to better understand and leverage the
other factors that can strongly impact the model’s pre- relationship between data science and the broader scien-
dictive performance. Best practices are more often art, tific milieu.
tunedtoexperience,ratherthanscienceguidedbyformal
reasoning. The generalized aliasing decomposition (17)
facilitates reasoning about key modeling decisions in a
way that is both formal and intuitive. In the context
IV. METHODS
of linear regression, the approach is fully rigorous while
imbuingpractitionerswithintuitionaboutmodelperfor-
manceinboththeclassicalandmodernregimes. Because In this section we give the mathematical details of our
it gives a label-independent decomposition of risk, prac- explanation of the nonmonotonic curve occurring for the
titioners can also make informed choices about data col- operator norm ∥A∥ (including the presence of ‘double
lection and experimental design for target applications. descent’), both the peak at the interpolation threshold
Althoughourformalanalysishasbeenrestrictedtolin- and the long-term decay to 0 as the number of model
earregression,therearereasonstobeoptimisticthatthe parameters grows to ∞. We also prove the described
core approach generalizes to the nonlinear regime. First, results about ∥E ∥ and L2-regularization.
B12
A. Interleaving of Eigenvalues in Rank-one B. Norm of A
Updates
We are interested in how the (induced) operator norm
The main tool we use to place the ideas presented in
thispaperonarigorousfootingisthefollowingtheorem, ∥A∥=∥M+ M ∥≤∥M+ ∥∥M ∥
TM TU TM TU
whose earliest statement seems to be [54, Theorem 17]
(see also [55–57]). changes as the model grows, that is, as a new column is
removedfromM andaddedtoM ,butthetraining
Theorem IV.1. Let A be an n×n Hermitian matrix TU TM
set (which rows are included) remains unchanged.
with eigenvalues α ≥ α ≥ ··· ≥ α and let C be a
1 2 n For simplicity of notation and to make the depen-
positive semidefinite matrix of rank 1. The eigenvalues
dence on the number m of model parameters explicit
β ≥β ≥···≥β of the matrix B =A+C satisfy
1 2 n we write X(m) = M and Y(m) = M when the
TM TU
β ≥α ≥β ≥α ≥···≥β ≥α . model consists of the first m columns of M. The ma-
1 1 2 2 n n
trix X(m + 1) is constructed by moving one column
From this theorem we immediately deduce the corol-
φ from M to M and the projection of the
lary that, under the same assumptions on A and C, the m+1 TU TM
operator M onto the training space T decomposes as
eigenvalues δ 1 ≥ ··· ≥ δ n of D = A−C are below the (cid:2) X(m) φ Y(m+1)(cid:3) .
corresponding eigenvalues of A and interleaved: m+1
α ≥δ ≥α ≥···≥α ≥δ .
1 1 2 n n
1. Nescience:
TheoremIV.1alsoleadsustothefollowingfundamen-
talresultforanalyzingtheoperatornormsofthealiasing
and invertibility operators A and B at least in the finite First consider what happens to the nescience matrix
dimensional case. M TU when a column φ m+1 is removed from Y(m) =
[φ Y(m+1)]. Expanding the product Y(m)Y(m)T
Theorem IV.2. Let X be an m×n matrix of rank r givm esY(m)Y(m)T =φ φT +Y(m+1)Y(m+1)T.
with smallest singluar value σ r > 0. Let X(cid:101) = [X|φ] Since φ φT is posm it+ iv1 e m se+ m1 idefinite, Theorem IV.1
be the m×(n+1) matrix obtained by adjoining an m- m+1 m+1
appliesandguaranteesthatthenormssatisfy∥Y(m)∥≥
dimensionalcolumnvectorφtoX. Thesmallestsingular
∥Y(m+1)∥, andthusthenorm∥Y(m)∥isanonincreas-
value σ (cid:101)min >0 of X(cid:101) satisfies the following relations: ing function of m.
0<σ
(cid:101)min
≤σ
r
if rank(X)<rank(X(cid:101)),
0<σ r ≤σ (cid:101)min if rank(X)=rank(X(cid:101)). 2. Pseudoinverse of Design:
Proof. Both XXT and X(cid:101)X(cid:101)T are m×m positive definite
Consider now the pseudoinverse term ∥M+ ∥ when
Hermitianmatrices. Thesingularvaluedecompositionof TM
φ is adjoind to X(m) to create X(m + 1). Theo-
X shows that the singular values σ ≥···≥σ >0 and m+1
1 r
the eigenvalues λ ≥···≥λ of XXT satisfy rem IV.2 guarantees that whenever φ m+1 is linearly in-
1 m
dependent of the old model (does not lie in the column
λ 1 =σ 12 ≥λ 2 =σ 12 ≥···≥λ r =σ r2 >0=λ r+1. space of X(m)), then the induced norm of the new pseu-
doinverse is bounded below by the induced norm of the
Similarly, the singular values σ ≥ σ ≥ ··· and eigen-
(cid:101)1 (cid:101)2
old pseudoinverse:
values λ(cid:101)1 ≥···≥λ(cid:101)m of X(cid:101)X(cid:101)T satisfy
λ(cid:101)1 =σ (cid:101)12 ≥λ(cid:101)2 =σ (cid:101)22···≥λ(cid:101)r =σ (cid:101)r2 ≥λ(cid:101)r+1 ≥··· , ∥X(m+1)+∥≥∥X(m)+∥.
where λ(cid:101)r+1 = 0 if rank(X(cid:101)) = r, but λ(cid:101)r+1 > 0 if Similarly, when φ
m+1
is linearly dependent on the old
rank(X(cid:101))=r+1. model,thentheinducednormofthenewpseudoinverseis
Expanding X(cid:101)X(cid:101)T gives X(cid:101)X(cid:101)T = XXT + φφT, where boundedabovebythenormofthepreviouspseudoinverse
φφT is positive semidefinite, so Theorem IV.1 implies
∥X(m+1)+∥ ≤∥X(m)+∥ .
that 2 2
λ(cid:101)1 ≥λ 1···≥λ
r−1
≥λ(cid:101)r ≥λ
r
≥λ(cid:101)r+1 ≥0. This proves Theorem II.1.
If rank(X(cid:101)) = r +1 (that is, φ is not in the column
space of X), then λ
r
= σ r2 ≥ λ(cid:101)r+1 = σ (cid:101)r2
+1
> 0. Taking
3. Limiting behavior of A
square roots gives σ >σ =σ >0.
r (cid:101)r+1 (cid:101)min
If rank(X(cid:101)) = r (that is, φ is in the column space of
As the number m of model parameters gets large, the
X), then the smallest nonzero eigenvalue of X(cid:101)X(cid:101)T is λ(cid:101)r, norm∥A∥isdominatedbythenormofthepseudoinverse
which satisfies λ
r−1
≥λ(cid:101)r ≥λ
r
>0. Taking square roots ∥M+ TM∥. For purposes of this analysis, assume that the
gives σ =σ ≥σ >0, as required. columnsofM areindependentidenticallydistributed
(cid:101)min (cid:101)r r TM13
(i.i.d.) random vectors φ ∈Rn with (finite) second mo- and equality holds except in the rare and uninteresting
i
ment E[φ φT]=Σ, where Σ is of full rank (rank t). case that the entire paramter space is modeled: Θ=M
i i
The Strong Law of Large Numbers guarantees that and U ={0}. This also implies that ∥E θ∥≤∥θ∥.
B
As discussed in Section II.IIA4, to better understand
m
1 X(m)X(m)T = 1 (cid:88) φ φT −a. →s. E[φ φT]=Σ the dependence of E Bθ on the model dimension m, we
m m i i i i decompose the square of the norm ∥E B∥2 further into
i=1
model invertibility and nescience contributions
as m → ∞. This implies that the smallest singu-
lar value of 1X(m)X(m)T converges almost surely to ∥E Bθ∥2 =∥P N(m)θ M(m)∥2+∥θ U(m)∥2 (30)
m
the smallest singular value of Σ, and thus the small-
est singular value λ (m) of X(m)X(m)T goes to in- withexplicitm-dependence. Model-invertibilityisanon-
min
finity almost surely. Thus the smallest singular value decreasing function of m because N(m)⊆N(m+1) for
σ (m)=(cid:112) λ (m)ofX(m)alsogoestoinfinity,and allm. Similarly, U(m)⊇U(m+1), sothenesciencecon-
min min
this implies ∥X(m)+∥= 1 −a. →s. 0. tribution is nonincreasing, and, indeed, it decreases by
σmin exactly |θ |2 when moving from m to m+1.
Because ∥Y(m)∥=∥M ∥ is bounded above and de- m+1
TU
D. Regularization
creasing in m, we have
∥A(m)∥=∥X(m)+Y(m)∥≤∥X(m)+∥∥Y(m)∥−a. →s. 0. It is straightforward to verify that the objective
to minimize with L -regularization can be written
2
EI dn
a
[σrt
d
mh ie
n no
(s
r
mp me )ac ]i la ≥al nc √das mme −t >ha √nt
,
nt
i
,th se
i
osv
k
∥e
n
Xc ot wo (mr ns
)[
+φ
5
∥8i
,
aa
T
nre
h
dmi A.i. 2d
(.
m6.
]
)s tt ha aan ret-
a
(cid:20)a ns dn1
y
(cid:101)(cid:13) (cid:13) (cid:13)y (cid:101)− =M(cid:102)T
[y
0M
].θ M(cid:13) (cid:13) (cid:13)2
2
T
(cid:21), hiw
s
he chre angM(cid:102)
esTM
chan= ges(cid:104) √ EM
n
BT λM
Im
to(cid:105)
O(m−1/2) or smaller. I M−M(cid:102)+
T
0MM
TM
I0
and changes A to M(cid:102)+ TMM TU.
U
Expanding the product
C. Norm of E and E θ
B B
M(cid:102)TMM(cid:102)T
TM
=M TMMT TM+nλI m,
Let M = U Σ VT be the reduced SVD of M ,
TM 1 1 1 TM
where M has rank r and Σ is invertible of shape shows that every eigenvalue of M MT is now in-
TM 1 TM TM
r×r. This gives creased by nλ in this product. Therefore, the singular
√
B=M+ M =V Σ−1Σ VT =V VT.
values of M
TM
are all increased by nλ in M(cid:102)TM, and
TM TM 1 1 1 1 1 1
1
The matrix V consists of the first r columns of an or- ∥M(cid:102)+ ∥= √
1 TM 1 + nλ
thonormal matrix V, i.e., V = [V 1|V 2], with VVT = ∥MTM+∥
V 1V 1T+V 2V 2T =I. Hence Theorem IV.1 guarantees that
=
√∥M TM+∥
≤
√1
.
1+ nλ∥M +∥ nλ
∥B∥=∥V VT∥≤∥I∥=1 TM
1 1
and
∥I−B∥=∥V 2V 2T∥≤1. ACKNOWLEDGMENTS
Note that V VT = P is the projection onto the kernel
2 2 N MKT was supported in part by the US NSF under
N of M , and every projection operator has its norm
TM awards DMR-1753357 and ECCS-2223985. GLWH was
bounded by 1, so
supported in part by the Chan-Zuckerberg Initiative’s
(cid:13)(cid:18) (cid:19)(cid:13) Imagingprogram. JPWwaspartiallysupportedbyNSF
∥E B∥=(cid:13) (cid:13)
(cid:13)
P 0N I0
U
(cid:13) (cid:13) (cid:13)≤1, grant DMS-2206762.
[1] N. Goldenfeld, Simple lessons from complexity, Science [4] M. K. Transtrum, B. B. Machta, K. S. Brown, B. C.
284, 87 (1999). Daniels, C. R. Myers, and J. P. Sethna, Perspective:
[2] E. P. Hoel, L. Albantakis, and G. Tononi, Quantifying Sloppinessandemergenttheoriesinphysics,biology,and
causalemergenceshowsthatmacrocanbeatmicro,Pro- beyond, The Journal of Chemical Physics 143, 010901
ceedingsoftheNationalAcademyofSciences110,19790 (2015).
(2013). [5] H. H. Mattingly, M. K. Transtrum, M. C. Abbott, and
[3] J.P.Crutchfield,Thedreamsoftheory,WIREsCompu- B. B. Machta, Maximizing the information learned from
tational Statistics , 75. finitedataselectsasimplemodel,ProceedingsoftheNa-14
tional Academy of Sciences 115, 1760 (2018). [23] J. Humpherys, T. J. Jarvis, and E. J. Evans, Founda-
[6] P.ChvykovandE.Hoel,CausalGeometry,Entropy 23, tions of Applied Mathematics, Volume 1: Mathematical
24. Analysis (SIAM, 2017).
[7] K. N. Quinn, M. C. Abbott, M. K. Transtrum, B. B. [24] X.ShengandT.Wang,Aniterativemethodtocompute
Machta,andJ.P.Sethna,Informationgeometryformul- moore-penrose inverse based on gradient maximal con-
tiparameter models: new perspectives on the origin of vergence rate, Filomat 27, 1269 (2013).
simplicity, Reports on Progress in Physics 86, 035901. [25] R.M.GowerandP.Richtárik,Randomizedquasi-newton
[8] S. Geman, E. Bienenstock, and R. Doursat, Neural net- updates are linearly convergent matrix inversion algo-
works and the bias/variance dilemma, Neural computa- rithms, SIAM Journal on Matrix Analysis and Applica-
tion 4, 1 (1992). tions 38, 1380 (2017).
[9] M.Belkin,D.J.Hsu,S.Ma,andS.Mandal,Reconciling [26] A continuous function is determined by its values on a
modernmachine-learningpracticeandtheclassicalbias- dense set, so we may limit ourselves to only considering
variancetrade-off,ProceedingsoftheNationalAcademy rational points t in the interval [a,b].
of Sciences 116, 15849 (2019). [27] We usually assume that M has finite dimension m (we
[10] M. Loog, T. Viering, A. Mey, J. H. Krijthe, and D. M. havem=d+1forpolynomialsofdegreeatmostd),but
Tax,Abriefprehistoryofdoubledescent,Proceedingsof U need not be finite-dimensional.
the National Academy of Sciences 117, 10625 (2020). [28] Again, a countable dense subset of points in [a,b] \
[11] L.Chen,Y.Min,M.Belkin,andA.Karbasi,Multiplede- {t ,...,t } suffices.
1 n
scent: Designyourowngeneralizationcurve,inAdvances [29] L.Wasserman,All of statistics,SpringerTextsinStatis-
in Neural Information Processing Systems, Vol. 34, tics (Springer-Verlag, New York, 2004) pp. xx+442, a
edited by M. Ranzato, A. Beygelzimer, Y. Dauphin, concise course in statistical inference.
P. Liang, and J. W. Vaughan (Curran Associates, Inc., [30] Except in the rare and irrelevant case that dim(U)=0.
2021) pp. 8898–8912. [31] The unstructured model is sensible only when the pa-
[12] P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak, rameter space has finite dimension, otherwise the norm
and I. Sutskever, Deep double descent: Where bigger ∥θ∥ would be infinite.
models and more data hurt, Journal of Statistical Me- [32] S. Mei and A. Montanari, The generalization error
chanics: Theory and Experiment 2021, 124003 (2021). of random features regression: Precise asymptotics
[13] S.d’Ascoli,M.Refinetti,G.Biroli,andF.Krzakala,Dou- and the double descent curve, Communications on
ble trouble in double descent: Bias and variance (s) in Pure and Applied Mathematics 75, 667 (2022),
thelazyregime,inInternational Conference on Machine https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.22008.
Learning (PMLR, 2020) pp. 2280–2290. [33] R.A.RobertsandC.T.Mullis,Digital signal processing
[14] B. Adlam and J. Pennington, Understanding double de- (Addison-Wesley Longman Publishing Co., Inc., 1987).
scentrequiresafine-grainedbias-variancedecomposition, [34] J.M.Sanchez,F.Ducastelle,andD.Gratias,Generalized
Advances in neural information processing systems 33, cluster description of multicomponent systems, Physica
11022 (2020). A: Statistical Mechanics and its Applications 128, 334
[15] E. H. Lee and V. Cherkassky, Vc theoretical explana- (1984).
tion of double descent, arXiv preprint arXiv:2205.15549 [35] J. Sanchez, Cluster expansions and the configurational
(2022). energy of alloys, Physical review B 48, 14013 (1993).
[16] L. Oneto, S. Ridella, and D. Anguita, Do we really [36] J. Sanchez, Cluster expansion and the configurational
need a new theory to understand the double-descent?, theory of alloys, Physical Review B–Condensed Matter
in ESANN (2022). and Materials Physics 81, 224202 (2010).
[17] R. Schaeffer, M. Khona, Z. Robertson, A. Boopathy, [37] A.Zunger,P.Turchi,andA.Gonis,Staticsanddynamics
K. Pistunova, J. W. Rocks, I. R. Fiete, and O. Koyejo, ofalloyphasetransformations,NATOASISeries.Series
Double descent demystified: Identifying, interpreting & B, Physics 319 (1994).
ablating the sources of a deep learning puzzle, arXiv [38] A.VanDeWalle,M.Asta,andG.Ceder,Thealloythe-
preprint arXiv:2303.14151 (2023). oreticautomatedtoolkit: Auserguide,Calphad26,539
[18] M. Lafon and A. Thomas, Understanding the double (2002).
descent phenomenon in deep learning, arXiv preprint [39] T. Mueller and G. Ceder, Bayesian approach to cluster
arXiv:2403.10459 (2024). expansions, Physical Review B–Condensed Matter and
[19] Z. Yang, Y. Yu, C. You, J. Steinhardt, and Y. Ma, Re- Materials Physics 80, 024103 (2009).
thinkingbias-variancetrade-offforgeneralizationofneu- [40] D. Lerch, O. Wieckhorst, G. L. Hart, R. W. Forcade,
ral networks, in International Conference on Machine andS.Müller,Uncle: acodeforconstructingclusterex-
Learning (PMLR, 2020) pp. 10767–10777. pansions for arbitrary lattices with minimal user-input,
[20] Y.Dar,V.Muthukumar,andR.G.Baraniuk,Afarewell Modelling and Simulation in Materials Science and En-
to the bias-variance tradeoff? an overview of the theory gineering 17, 055003 (2009).
of overparameterized machine learning, arXiv preprint [41] M. Ångqvist, W. A. Muñoz, J. M. Rahm, E. Fransson,
arXiv:2109.02355 (2021). C. Durniak, P. Rozyczko, T. H. Rod, and P. Erhart,
[21] B. Neal, S. Mittal, A. Baratin, V. Tantia, M. Scicluna, Icet–apythonlibraryforconstructingandsamplingalloy
S. Lacoste-Julien, and I. Mitliagkas, A modern take clusterexpansions,AdvancedTheoryandSimulations2,
on the bias-variance tradeoff in neural networks, arXiv 1900015 (2019).
preprint arXiv:1810.08591 (2018). [42] A. Seko, K. Yuge, F. Oba, A. Kuwabara, and I. Tanaka,
[22] Although fitting monomials is the canonical pedagogical Prediction of ground-state structures and order-disorder
example, the ill-conditioned Vandemonde matrix makes phase transitions in ii-iii spinel oxides: A combined
this basis ill-suited for practical applications. cluster-expansion method and first-principles study,15
Physical Review B–Condensed Matter and Materials e-prints , arXiv:2003.01897 (2020), arXiv:2003.01897
Physics 73, 184117 (2006). [cs.LG].
[43] ThesitefunctionsthemselvesareusuallydiscreteCheyb- [52] J. Lee, L. Xiao, S. S. Schoenholz, Y. Bahri, R. Novak,
schevpolynomialsoraFourierbasis.Anyfunctionsthat J. Sohl-Dickstein, and J. Pennington, Wide neural net-
formanorthonormalsetoverthediscretevaluesofσ are worksofanydepthevolveaslinearmodelsundergradient
i
suitable. descent <sup>*</sup>, Journal of Statistical Mechan-
[44] G. L. Hart and R. W. Forcade, Algorithm for generat- ics: Theory and Experiment 2020, 124002 (2020).
ing derivative structures, Physical Review B–Condensed [53] M. K. Transtrum, B. B. Machta, and J. P. Sethna, Ge-
Matter and Materials Physics 77, 224115 (2008). ometry of nonlinear least squares with applications to
[45] G. L. Hart and R. W. Forcade, Generating derivative sloppy models and optimization, Physical Review E 83,
structuresfrommultilattices: Algorithmandapplication 036701 (2011).
tohcpalloys,PhysicalReviewB–CondensedMatterand [54] F.P.GantmacherandM.G.Krein,Oscillation matrices
Materials Physics 80, 014120 (2009). and kernels and small vibrations of mechanical systems,
[46] G.L.Hart,L.J.Nelson,andR.W.Forcade,Generating revised ed. (AMS Chelsea Publishing, Providence, RI,
derivative structuresata fixedconcentration, Computa- 2002)pp.viii+310,translationbasedonthe1941Russian
tional Materials Science 59, 101 (2012). original, Edited and with a preface by Alex Eremenko.
[47] This formation enthalpy data was generated by “unre- [55] J. R. Bunch and C. P. Nielsen, Updating the singular
laxed” Density Functional Theory calculations configu- value decomposition, Numer. Math. 31, 111 (1978/79).
rations of platinum and copper. [56] R.C.Thompson,Thebehaviorofeigenvaluesandsingu-
[48] The Legendre polynomials {P k} k∈N are orthogonal with lar values under perturbations of restricted rank, Linear
respecttotheinnerproduct⟨f,g⟩=(cid:82)1 f(t)g(t)dt,with Algebra Appl. 13, 69 (1976), collection of articles dedi-
−1
P of degree k and P (1)=1 for all k. cated to Olga Taussky Todd.
k k
[49] TheVandermondematrixis wellconditionedinthespe- [57] J. H. Wilkinson, The algebraic eigenvalue problem
cial case that the inputs t all lie on the unit circle (Clarendon Press, Oxford, 1965) pp. xviii+662.
U1 ={t∈C:|z|=1} in C, but it is badly conditioned [58] M.RudelsonandR.Vershynin,Non-asymptotictheoryof
if the inputs t do not have unit modulus. randommatrices: extremesingularvalues,inProceedings
[50] F. F. Yilmaz and R. Heckel, Regularization-wise double oftheInternationalCongressofMathematicians.Volume
descent: Why it occurs and how to eliminate it, in 2022 III (HindustanBookAgency,NewDelhi,2010)pp.1576–
IEEE International Symposium on Information Theory 1602, https://www.math.uci.edu/ rvershyn/papers/rv-
(ISIT) (2022) pp. 426–431. ICM2010.pdf.
[51] P. Nakkiran, P. Venkat, S. Kakade, and T. Ma, Opti-
malRegularizationCanMitigateDoubleDescent,arXiv