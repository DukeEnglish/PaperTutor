Confidence-weighted integration of human and
machine judgments for superior decision-making
Felipe Y´an˜ez∗1, Xiaoliang Luo2, Omar Valerio Minero1, and Bradley C. Love2,3
1Max Planck Institute for Neurobiology of Behavior – caesar, Bonn, Germany
2Department of Experimental Psychology, University College London, London, United Kingdom
3The Alan Turing Institute, London, United Kingdom
Abstract
Large language models (LLMs) have emerged as powerful tools in various domains.
Recent studies have shown that LLMs can surpass humans in certain tasks, such as
predicting the outcomes of neuroscience studies [1]. What role does this leave for
humans in the overall decision process? One possibility is that humans, despite per-
forming worse than LLMs, can still add value when teamed with them. A human
and machine team can surpass each individual teammate when team members’ confi-
dence is well-calibrated and team members diverge in which tasks they find difficult
(i.e., calibration and diversity are needed). We simplified and extended a Bayesian
approach to combining judgments using a logistic regression framework that inte-
grates confidence-weighted judgments for any number of team members. Using this
straightforward method, we demonstrated in a neuroscience forecasting task [1] that,
even when humans were inferior to LLMs, their combination with one or more LLMs
consistently improved team performance. Our hope is that this simple and effective
strategyforintegratingthejudgmentsofhumansandmachineswillleadtoproductive
collaborations.
1 Introduction
Modern environments increasingly stretch our ability to process the vast amounts of in-
formation available to us [2, 3]. In contrast, machine systems can often take advantage
of vast information resources [4, 5, 6, 7]. As machines reach superhuman performance
levels [6, 8, 9], one concern is whether machines will supplant human judgment in critical
areas [10, 11].
One potential solution is forming human-machine teams in which judgments from
humans and machines are integrated [12, 13]. It might be possible that humans can
contribute to and make the overall team better even when their performance is worse on
average than their machine teammates.
We will evaluate this possibility in a knowledge-intensive task where large language
models (LLMs) surpass humans in predicting the outcomes of neuroscience studies [1].
Human-LLM teaming combines the individual judgments of humans and machines. Com-
plementarity is realized when a team’s performance improves beyond that of either team-
mate alone [12, 13]. We investigate whether human-LLM teams outperform LLMs even
when humans have inferior performance compared to LLMs. There are two key conditions
∗Correspondence: felipe.yanez@mpinb.mpg.de
1
4202
guA
51
]CH.sc[
1v38080.8042:viXrafor team complementarity to be fulfilled [1, 12, 14]. The first requirement is calibration of
confidence. This implies that when humans and LLMs have a higher degree of confidence
in their judgments, the accuracy of those judgments tends to be greater [1]. The second
requirement is classification diversity among team members. Diversity holds when the
errors in classification made by humans and LLMs are not the same [1].
Previous work [12] has explored the conditions for complementarity in the context of
objectrecognition. Humansoutperformedmachinesintheclassificationofnaturalimages,
raising the question of whether a combined approach could achieve superhuman perfor-
mance. They developed a Bayesian model that integrates the judgments of humans and
machines. With this approach, human-machine complementarity was observed. How-
ever, the combination model is computationally expensive and challenging to extend to
additional teammates. Ideally, a model that combines the judgments of humans and ma-
chines should be adaptable and scalable, easily interpretable, and allow for any number of
teammates.
Here, we aim to offer this ideal solution to human-machine teaming while evaluating
complementarity in a knowledge-intensive task that is not based on perceptual judgment
and, critically, where humans were surpassed by machine systems. Foreshadowing our
results, we find support for effective human-LLM teaming through our novel and resource-
efficient procedure. Our procedure comprises a logistic-regression-based strategy that
provides confidence-weighted integration of teammates’ predictions for any number of
teammembers. Ourapproachisparticularlywell-suitedforcombininghumanandmachine
judgments and assessing their contribution in predictive tasks.
2 Methods
Our contribution relies on previous efforts that developed BrainBench [1] to assess the
capacity of humans and LLMs to predict the outcomes of neuroscience studies. The
benchmark includes test cases based on abstracts from the Journal of Neuroscience. Each
testcasecontainsanoriginalabstractandanalteredversion(Figure1), LLMsoutperform
humans by a large margin in test cases created either by expert neuroscientists or by
prompting GPT-4 to create test cases [1]. Since substantial differences in performance
may preclude complementarity [12], we used the GPT-4 generated test cases because the
performancedifferencebetweenhumansandLLMswassmallerforthesetestcases,though
LLMs were still clearly superior to humans on these test items.
2.1 Test cases
BrainBench [1] is a benchmark that includes 100 test cases generated by GPT-4 (Azure
OpenAI API; version 2023-05-15). These test cases were created from abstracts in the
Journal of Neuroscience published in 2023. These abstracts are categorized into five
sections: Behavioral/Cognitive, Systems/Circuits, Development/Plasticity/Repair, Neu-
robiologyofDisease, andCellular/Molecular. Eachtestcasecontainsapublishedabstract
and an altered version produced by GPT-4 (see details in [1, Dataset Creation]). These
modifications, though minimal, significantly change the results—for instance, by chang-
ing the roles of brain regions or reversing a result’s direction (e.g., from “decreases” to
“increases”). The altered abstracts remain logically coherent despite the changes. The
BrainBench task is to identify the correct study outcome by choosing between the original
abstract and its altered counterpart.
2Figure1: AssessingHumansandLLMsusingBrainBench[1]. a)Thebench-
mark consisted of test cases constructed from the Journal of Neuroscience
abstracts. Abstracts consist of background, methods, and results. The test-
taker chose which of two versions of the abstract was the original version.
The altered version maintained coherency while significantly altering the re-
sults. The 100 test cases considered here were constructed by GPT-4 with
humanoversightandqualitycontrol. b)Anexampletestcase. Humanswere
instructedtoselectwhichversionoftheabstractwastheoriginalbyclicking
on either blue or green text to select that set of options. Test cases varied in
the numbers of alternatives, but a single click will choose all options of the
same color. After their choice, humans indicated their confidence. LLMs
chose the version of the abstract that had the lower perplexity and their
confidence was assessed by the absolute difference in perplexity of the two
options.
2.2 Procedure for human participants
One hundred seventy-one neuroscience experts were recruited to complete an online study
(seedetailsin[1, Evaluations]). Eachparticipantevaluatedthreeoutofthe100testcases.
Two versions of an abstract were presented, one with the actual results and one that was
altered (Figure 1). Participants chose the version they believed to be the original and
rated their confidence using a slider bar. After applying several exclusion criteria, the 171
participants yielded 503 observations (2 to 9 instances per test case).
2.3 Procedure for LLMs
We considered Llama 2 chat models with 7B, 13B, and 70B weights [15]. LLMs chose
the version of the abstract with the lower perplexity (PPL). Confidence was calculated as
the absolute difference in PPL between the original and altered versions of the abstract
(Figure 1).
32.4 Bayesian combination model
HumanandLLMjudgmentswerecombinedbyadaptingaBayesianframeworkforhuman-
machine complementarity [12]. The problem setting for combining two team members,
human and LLM, is as follows: Let N denote the number of test cases to be analyzed with
two possible choices, “0: first option” or “1: second option”. The ground truth labels
of the original abstracts are z ∈ {0,1}N. For the human classifier, the predicted labels
y ∈ {0,1}N andtheircorrespondingconfidenceratingsr ∈ {0,...,R−1}N with“0: lowest
possible confidence” and “R − 1: highest possible confidence” are given. For the LLM
classifier, the PPL q ∈ RN×2 reflects a measure of uncertainty [1]. We used probability
+
confidence scores, π = Softmax(−q). The first step of this model is to generate correlated
probability confidence scores for human and machine classifiers using a bivariate normal
distribution,
(cid:18) π (cid:19) (cid:18)(cid:18) µ (cid:19) (cid:18) σ2 σ σ ρ (cid:19)(cid:19)
H ∼ N H , H H M HM . (1)
π µ σ σ ρ σ2
M M H M HM M
The means of the underlying distribution, µ and µ , depend whether the label of test
H M
case i, z , is correct or not, i.e.,
i
µ = b +(a −b )·1 (j),
H,i,j H H H Zi
µ = b +(a −b )·1 (j),
M,i,j M M M Zi
withZ = {x|x = z }. Notethatthescalarparametersa ,a ,b ,b ,σ ,σ ,andρ
i i H M H M H M HM
in Eq. (1) are learned from data. The parameter ρ from the covariance matrix learns
HM
the correlation between the human and machine classifiers. In the case of the machine
classifier, π is compared to the empirical probability confidence scores, π. Then, for the
M
human classifier, π is a latent variable that is used to calculate classifications
H
y ∼ Categorical(Softmax(π /τ)) ,
H H
where τ denotes a temperature parameter, usually small, that helps convergence [12]. The
predictedclassifications,y ,arecomparedtoactualhumanpredictions,y. Theconfidence
H
ratings are casted via an ordered logistic model [16] that maps the continuous probability
confidence scores, π , to an ordinal confidence rating, r . This means,
H H
r ∼ OrderedLogistic(π ,c,δ),
H H
where the parameters c ∈ RR−1 are the breakpoints of the intervals that map π into r ,
+ H H
and δ is a scalar that controls the sharpness of the rating probability curves. Finally, r
H
is compared to the empirical human confidence ratings, r. See supplementary information
for implementation details.
2.5 Logistic combination model
We introduce a logistic regression approach that combines the judgments of any number
of teammates. The logistic combination model follows the principles of the Bayesian
combinationmodel, butisformulatedwithinaneasiertoimplementandextendregression
framework. In its most basic form, which we consider here, each teammate is captured by
a singlepredictor in theregressionmodel. The value ofthe predictor on atrial dependson
the teammate’s choice and their confidence. In particular, the magnitude of the predictor
is the teammate’s confidence on that trial (i.e., confidence-weighted integration) and the
sign is determined by the teammate’s choice. In general, the fitted beta weight for a
teammate will reflect their accuracy and calibration.
4As in the Bayesian combination model, y, r, q are given. For the human classifier, the
confidence ratings were of the form: r ∈ {1,...,100}N. In the case of LLMs, the absolute
difference in PPL |∆q | ∈ RN corresponds to model confidence [1].
i +
The logistic function for the i-th test case is of the form:
1
p = ,
x 1+e−β⊤x
where p is the predicted probability of the arbitrarily assigned first option, and the
x
evidence is
β⊤x = β +β ·x +β ·x . (2)
I H H M M
The fitted weights β , β , and β correspond to the intercept, and human and machine
I H M
teammates, respectively. The term x is the signed confidence for team member k. For
k
human participants, x is r if y = 0 and −r otherwise. Thus,
H i i i
x = r (1−2·1 (y )),
H i A i
where 1 (y ) is an indicator function with A = {x|x = 1}. Hence, 1 (x) = 1 if x ∈ A,
A i A
and 1 (x) = 0 otherwise. For LLM team members, it reads
A
x = |∆q |(1−2·1 (y )).
M i A i
This model can be easily expanded by including additional terms to Eq. (2). For
example, a third fitted weight could be included for an interaction term x x . Likewise,
H M
polynomial regression could be used to include x2 and x2 , and corresponding fitted
H M
weights. This approach can be readily extended to more than 2 team members by using
multinomial regression.
2.6 Cross-validation procedure
Given the 503 observations for the 100 test cases, we performed a leave-one-out cross-
validation (LOOCV) [17, 18] that provides the best bias-variance trade-off for small
datasets. Consider the evaluation of the i-th test case. With this procedure, we re-
moved all instances of test case i (between 2 and 9) leaving the remaining 99 test cases
with all their instances (between 494 and 501) to train the classifier teams. For testing, we
utilized all the instances of test case i. This was repeated for all 100 test cases, yielding
503 predictions. Note that for individual teammates the evaluation comprised only the
testing phase. When the predicted labels of any team or individual teammate in this
study were randomly shuffled, the LOOCV accuracy dropped approximately to chance
level (i.e., 50%).
3 Results
We first assessed the conditions for effective collaboration (i.e., complementarity), namely
calibration of confidence and classification diversity among team members. Both humans
and LLMs were calibrated in that accuracy was positively correlated with confidence
(Figure 2a). Diversity held in that LLMs and humans differed on which test items led to
errors (Figure 2c).
In terms of accuracy (Figure 2b), LLMs numerically surpassed humans by a small
margin (t(2) = 5.20, P < 0.05). Thus, we can consider whether humans can benefit teams
consisting of machines that perform comparably or better.
5a
b c
Figure 2: Conditions for effective collaboration between human experts and
LLMs were satisfied. a) When LLMs were confident in their BrainBench
judgments, they were more likely to be correct. Confidence ratings were
sorted into equal bins, and the mean accuracy for each bin was plotted.
The positive slope of the black regression lines for Llama2 chat models (7B,
13B, and 70B) indicates well-calibrated confidence, meaning higher confi-
dence correlated with higher accuracy. b) LLMs surpassed human experts
on BrainBench overall. Error bars represent standard error of the mean
using a binomial model. c) Item difficulty Spearman correlations among
LLMs and human experts. For LLMs, difference in perplexity between in-
correct and correct abstracts was used to determine the relative difficulty of
test cases. Mean accuracy was used for human experts. LLMs align more
with each other than humans, which implies human-machine teams will be
diverse.
Of primary interest was whether teams including humans performed better than LLM
only teams. All 15 possible team combinations, ranging from individual teammates to
a 4-way human-LLM team, were considered (Figure 3). Adding a human teammate to
LLM-only teams always improved the team’s performance (t(6) = 8.22, P < 0.0001).
Our confidence-weighted logistic regression approach follows from the principles of
a Bayesian combination model that fosters human-machine complementarity [12]. One
questions is how well our logistic regresion approach compares to the Bayesian approach.
Our confidence-weighted regression model compared favorably (Figure 4) to the Bayesian
model when evaluated on the three human-LLM teams for which the Bayesian model is
intended to apply (t(2) = 4.08, P < 0.01). This success is impressive given that the
regression approach takes seconds to compute on a current desktop whereas the Bayesian
approach would take 12-13 days.
6Figure 3: Performance of all possible teams using the confidence-weighted
logistic combination model. Adding a human to a team with one or more
machines (blue points) always has a benefit (green points). Llama2 chat 7B,
13B, and 70B models were considered. Each data point corresponds to the
average across 503 evaluations. Error bars represent standard error of the
mean using a binomial model.
Figure 4: Comparison between Bayesian and confidence-weighted logistic
combination models for human-LLM teams. Each data point corresponds
totheaverageacross503evaluations. Llama2chat7B,13B,and70Bmodels
were considered. Error bars represent standard error of the mean using a
binomial model. The confidence-weighted logistic combination model more
effectively integrated human and machine judgments.
4 Discussion
Can humans team effectively with LLMs when the humans perform worse? We developed
a confidence-weighted regression approach that can integrate judgments from any number
of teammates. Using this method and testing on a forecasting benchmark (Figure 1), we
found that human-LLM teams achieve complementarity, that is their combined perfor-
mance bests that of either teammate alone (Figure 3). Complementarity was achieved
because two critical conditions were satisfied, namely confidence was well-calibrated and
classification diversity held among teammates (Figure 2). Strikingly, every combination
7of LLMs benefited from adding a human to the team (Figure 3).
OurapproachwasinformedbyaBayesianmethodforcombiningjudgmentsofhumans
andmachines[12]. Ourapproachhasanumberofadvantages, includingeaseofimplemen-
tation, very fast runtime, an interpretable solution, and readily extendable to any number
ofteammates. Surprisingly,ourconfidence-weightedregressionapproachperformedbetter
than the Bayesian approach (Figure 4). One possibility is that the discretization of con-
tinuous confidence measures, which the Bayesian model requires, limited its performance.
Perhaps an alternative formulation would perform better. Unfortunately, reformulating
theBayesianmodelandproperlyimplementingitrequiressubstantialeffortandexpertise.
In contrast, because our confidence-weighted integration model is formulated within a re-
gression framework, it is straightforward to extend to include other factors. For example,
nonlinear relationships (e.g., polynomial terms) between confidence-weighted predicitons
and outcomes could be considered.
While we selected three LLMs with superhuman performance on BrainBench, these
LLMs are not the highest performing models on this benchmark [1]. Our choice was
deliberate because a vastly superior teammate may hinder complementarity. In the limit,
ateammatewhoisneverwrongdoesnotneedtobepartofateam. Thislimitingcondition
may become more prevalent should LLMs continue to improve and, therefore, diminish
the benefits of human-LLM teaming. For the foreseeable future, we suspect there will
be tasks for which humans and LLMs can effectively team. Moreover, our method for
integrating the judgments of teammates is not limited to human-LLM teams. Instead, the
method is general and applies to any set of agents (natural or artificial) that can report
how confident they are in their decisions.
This study explored the possibility of a collaborative approach between humans and
LLMs for superior decision-making in predicting neuroscience outcomes. Our confidence-
weighted regression method effectively combined human and LLM judgments because
teammatesfulfilledtheconditionsofwell-calibratedconfidenceandclassificationdiversity.
Our results suggest there is a place for humans in teams with machines even when the
machines perform better. We hope our work facilitates successful collaborations between
humans and machines in addressing important challenges.
5 Data availability
The human participant data and LLM perplexity scores utilized in this study have been
previously reported in [1], and are publicly available at
https://github.com/braingpt-lovelab/BrainBench .
6 Code availability
All computer code associated with this work including combination model implementa-
tions, team evaluations, and analyses are publicly available at
https://github.com/braingpt-lovelab/haico .
References
[1] Xiaoliang Luo, Akilles Rechardt, Guangzhi Sun, Kevin K. Nejad, Felipe Y´an˜ez,
Bati Yilmaz, Kangjoo Lee, Alexandra O. Cohen, Valentina Borghesani, Anton
8Pashkov, Daniele Marinazzo, Jonathan Nicholas, Alessandro Salatiello, Ilia Sucholut-
sky, Pasquale Minervini, Sepehr Razavi, Roberta Rocca, Elkhan Yusifov, Tereza
Okalova, Nianlong Gu, Martin Ferianc, Mikail Khona, Kaustubh R. Patil, Pui-Shee
Lee, Rui Mata, Nicholas E. Myers, Jennifer K Bizley, Sebastian Musslick, Isil Poyraz
Bilgin,GuiomarNiso,JustinM.Ales,MichaelGaebler,NApurvaRatanMurty,Leyla
Loued-Khenissi,AnnaBehler,ChloeM.Hall,JessicaDafflon,SherryDongqiBao,and
Bradley C. Love. Large language models surpass human experts in predicting neuro-
scienceresults. Nature Human Behaviour (in press) arXiv preprint arXiv:2403.03230,
2024.
[2] Martin J. Eppler and Jeanne Mengis. The Concept of Information Overload: A
Review of Literature from Organization Science, Accounting, Marketing, MIS, and
Related Disciplines. The Information Society, 20(5):325–344, 2004.
[3] David Bawden and Lyn Robinson. The dark side of information: overload, anxiety
and other paradoxes and pathologies. Journal of Information Science, 35(2):180–191,
2009.
[4] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521:
436–444, 2015.
[5] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George
van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam,
MarcLanctot,SanderDieleman,DominikGrewe,JohnNham,NalKalchbrenner,Ilya
Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel,
and Demis Hassabis. Mastering the game of Go with deep neural networks and tree
search. Nature, 529:484–489, 2016.
[6] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov,
Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zˇ´ıdek, Anna
Potapenko, Alex Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard,
Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas
Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski,
Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein,
DavidSilver,OriolVinyals,AndrewW.Senior,KorayKavukcuoglu,PushmeetKohli,
and Demis Hassabis. Highly accurate protein structure prediction with AlphaFold.
Nature, 596:583–589, 2021.
[7] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and
Tatsunori B. Hashimoto. Benchmarking Large Language Models for News Summa-
rization. Transactions of the Association for Computational Linguistics, 12:39–57,
2024.
[8] AlecRadford, Jeff Wu, RewonChild, David Luan, DarioAmodei, andIlya Sutskever.
Language Models are Unsupervised Multitask Learners. OpenAI blog, 2019.
[9] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-
hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher
Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Lan-
guage Models are Few-Shot Learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F.
9Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems,
volume 33, pages 1877–1901, 2020.
[10] Erik Brynjolfsson and Andrew McAfee. The Second Machine Age: Work, Progress,
and Prosperity in a Time of Brilliant Technologies. W. W. Norton & Company, New
York, NY, USA, 2014.
[11] Carl Benedikt Frey and Michael A. Osborne. The future of employment: How sus-
ceptible are jobs to computerisation? Technological Forecasting and Social Change,
114:254–280, 2017.
[12] Mark Steyvers, Heliodoro Tejeda, Gavin Kerrigan, and Padhraic Smyth. Bayesian
modeling of human–AI complementarity. Proceedings of the National Academy of
Sciences, 119(11):e2111547119, 2022.
[13] Patrick Hemmer, Max Schemmer, Niklas Ku¨hl, Michael V¨ossing, and Gerhard
Satzger. Complementarity in Human-AI Collaboration: Concept, Sources, and Evi-
dence. arXiv preprint arXiv:2404.00029, 2024.
[14] Mark Steyvers, Heliodoro Tejeda, Aakriti Kumar, Catarina Belem, Sheer Karny,
Xinyue Hu, Lukas Mayer, and Padhraic Smyth. The Calibration Gap between Model
and Human Confidence in Large Language Models. arXiv preprint arXiv:2401.13835,
2024.
[15] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-
mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-
ale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucu-
rull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui
Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann,
Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya
Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,
Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein,
RashiRungta, KalyanSaladi, AlanSchelten, RuanSilva, EricMichaelSmith, Ranjan
Subramanian, XiaoqingEllenTan, BinhTang, RossTaylor, AdinaWilliams, JianXi-
ang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie
Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and
Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv
preprint arXiv:2307.09288, 2023.
[16] PeterMcCullagh.RegressionModelsforOrdinalData.JournaloftheRoyalStatistical
Society. Series B (Methodological), 42(2):109–142, 1980.
[17] PeterA.LachenbruchandM.RayMickey. EstimationofErrorRatesinDiscriminant
Analysis. Technometrics, 10(1):1–11, 1968.
[18] AleksandrLuntzandV.Brailovsky. Onestimationofcharactersobtainedinstatistical
procedure of recognition. Technicheskaya Kibernetica, 3, 1969.
[19] Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Prad-
han, Theofanis Karaletsos, Rohit Singh, Paul Szerlip, Paul Horsfall, and Noah D.
Goodman. Pyro: Deep Universal Probabilistic Programming. Journal of Machine
Learning Research, 20(28):1–6, 2019.
10Acknowledgments
This work was supported the ESRC (ES/W007347/1), Microsoft (Accelerate Foundation
Models Research Program), and a Royal Society Wolfson Fellowship (18302) to B.C.L.
Supplementary information
Implementation of Bayesian combination model
Algorithm 1 Bayesian Combination Model
1: Set priors: α = 1, a M ∼ N(0,10), b M ∼ N(0,10), σ M ∼ Uniform(0,15), a H ∼
N(0,10), b = 0, σ = 1, ρ ∼ Uniform(−1,1), τ = 0.05, c ∼ Uniform(0,1) with
H H
c < c ∀i = 1,...,R−2, and δ ∼ Uniform(0,100).
i i+1
2: start training:
3: p ∼ Dirichlet(α)
4: z ← argmax(p) ▷ Compare to actual data
5: µ M,i,j ← b M +(a M −b M)·1 Zi(j)
6: µ H,i,j ← b H +(a H −b H)·1 Zi(j)
7: π M ∼ N (µ M, σ M) ▷ Compare to actual data
8: π H ∼ N (cid:16) µ H +ρσ H(cid:16) πM σ− MµM(cid:17) , (cid:112) 1−ρ2σ H(cid:17)
9: y ∼ Categorical(Softmax(π H/τ)) ▷ Compare to actual data
10: r ∼ OrderedLogistic(π H,c,δ) ▷ Compare to actual data
11: end training
12: start testing:
13: p ∼ Dirichlet(α)
14: z ← argmax(p)
15: µ M,i,j ← b M +(a M −b M)·1 Zi(j)
16: µ H,i,j ← b H +(a H −b H)·1 Zi(j)
17: π M ∼ N (µ M, σ M) ▷ Compare to actual data
18: π H ∼ N (cid:16) µ H +ρσ H(cid:16) πM σ− MµM(cid:17) , (cid:112) 1−ρ2σ H(cid:17)
19: y ∼ Categorical(Softmax(π H/τ)) ▷ Compare to actual data
20: r ∼ OrderedLogistic(π H,c,δ) ▷ Compare to actual data
21: end testing
Algorithm 1 illustrates the inference procedure of the Bayesian combination model, which
comprises two stages: estimation of parameters (training) and prediction of label prob-
abilities (testing). For the estimation of parameters, the given data is: true labels z,
probability confidence scores π , and human classification y and confidence ratings r. We
M
assumed that all human participants shared the same set of parameters (a , b , σ ,
H H H
11c, δ, and τ). Human confidence ratings on the slider bar were mapped to range between
1 and 100. A wide range is computationally expensive, thus, we aggregate it into three
levels: “0: low confidence”, “1: moderate confidence”, and “2: high confidence”. Then,
the aggregated confidence rating used for analysis, r ∈ {0,1,2}N, reads

0 if self-reported confidence ≤ 33,


r = 1 if 33 < self-reported confidence ≤ 66, .

2 if 66 < self-reported confidence
The utilized cutpoints (i.e., 33 and 66) produced a good agreement between confidence
and accuracy. Among the evaluations of human participants, “low” had 63.2% average
accuracy (n = 174), “moderate” had 66.5% (n = 185), and “high” had 81.9% (n = 144).
To estimate the posterior over the underlying parameters, a No-U-Turn Sampler (NUTS)
forMarkovchainMonteCarlo(MCMC)[19]wasusedwithn = 500warmupsteps,n = 3
w c
chains, and n = 25 samples. For the prediction of label probabilities the given data is the
s
sameasbeforebutz, thatisnowlatent. Thepriordistributionsweresetaccordingto[12].
The correlation between team members was ρ ∼ Uniform(−1,1). For the LLM classifier,
a ∼ N(0,10), b ∼ N(0,10), and σ ∼ Uniform(0,15). For human classifications, a was the
same but b and σ were adjusted to b = 0 and σ = 1 for the purpose of identifiability [12].
In addition, we used δ ∼ Uniform(0,100) for the scaling parameter and uniform priors on
the cutpoints, c ∼ Uniform(0,1), with the constraint that the cutpoints are ordered (i.e.
c < c ∀i = 1,...,R−2). Finally, we set τ = 0.05 for best convergence results [12].
i i+1
12