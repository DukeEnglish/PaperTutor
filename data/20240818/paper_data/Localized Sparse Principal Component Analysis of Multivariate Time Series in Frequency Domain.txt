Localized Sparse Principal Component
Analysis of Multivariate Time Series in
Frequency Domain
Jamshid Namdari
Department of Biostatistics & Bioinformatics, Emory University
Amita Manatunga
Department of Biostatistics & Bioinformatics, Emory University
Fabio Ferrarelli
Department of Psychiatry, University of Pittsburgh
and
∗
Robert T. Krafty
Department of Biostatistics & Bioinformatics, Emory University
August 16, 2024
Abstract
Principal component analysis has been a main tool in multivariate analysis for es-
timatingalowdimensionallinearsubspacethatexplainsmostofthevariabilityinthe
data. However, in high-dimensional regimes, naive estimates of the principal loadings
are not consistent and difficult to interpret. In the context of time series, principal
component analysis of spectral density matrices can provide valuable, parsimonious
informationaboutthebehavioroftheunderlyingprocess, particularlyiftheprincipal
components are interpretable in that they are sparse in coordinates and localized in
frequency bands. In this paper, we introduce a formulation and consistent estimation
procedure for interpretable principal component analysis for high-dimensional time
series in the frequency domain. An efficient frequency-sequential algorithm is devel-
opedtocomputesparse-localizedestimatesofthelow-dimensionalprincipalsubspaces
of the signal process. The method is motivated by and used to understand neuro-
logical mechanisms from high-density resting-state EEG in a study of first episode
psychosis.
Keywords: Principal component analysis; Frequency band; Spectral density matrix; High
dimensional time series; Sparse estimation
∗Corresponding author Robert T. Krafty, Department of Biostatistics & Bioinformatics, Emory Uni-
versity, Atlanta, GA 30322 (e-mail:rkrafty@emory.edu). This work is supported by National Institutes of
Health grants R01GM140476, R01HL159213 and R01MH125816.
1
4202
guA
51
]EM.tats[
1v77180.8042:viXra1 Introduction
Since its first descriptions by Pearson (1901) and by Hotelling (1933), principal component
analysis (PCA) has been one of the main multivariate analysis techniques for dimension re-
duction and feature extraction. PCA has become an essential tool for not just independent
and identically distributed (iid) multivariate data, but also for serially correlated multi-
variate time series data in both the time and frequency domains. In the frequency domain,
PCA as a sequential method for finding directions of maximum variability appeared in the
work of Brillinger (1964) and Goodman (1967). Brillinger (1969) formulated the principal
component series through an optimal linear filtering that transmit a p-dimensional signal
through a d-dimensional channel and recovers it with minimum loss of information. A foun-
dational discussion of theory and applications of PCA in frequency domain can be found
in Brillinger (2001); recent applications of this framework include uncovering non-coherent
block structures (Sundararajan, 2021), time-frequency analysis (Ombao et al., 2005) and
change point detection (Jiao et al., 2021).
PCA for the frequency domain analysis of high-dimensional multivariate time series
faces several challenges. The first challenge, which is not unique to frequency domain PCA
and is a challenge for PCA in general, is high-dimensionality. When the dimension is fixed,
sample eigenvectors, and consequently sample estimates of the principal components, are
consistent and asymptotically normally distributed (Anderson, 1958). However, in high-
dimensional regimes, where the dimension of the random variable grows, sample PCs fail
to be consistent. For the single spiked covariance model in the iid multivariate setting,
it has been shown that the leading eigenvector of the sample covariance matrix can ac-
tually be orthogonal to the leading eigenvector of the population covariance matrix if its
2corresponding eigenvalue is not sufficiently large (Paul, 2007).
To obtain consistent estimates of PCs, Johnstone and Lu (2009) proposed to obtain
PCs that have sparse representation in an orthonormal system. Reviews of sparse PCA
can be found in Johnstone and Paul (2018) and in Zou and Xue (2018). Aside from the
theoretical benefits or necessities for PCA in high-dimensions, sparsification also provides
interpretation that is essential for effective data analysis. For example, consider our mo-
tivating application of resting state EEG data that is discussed in Section 6. Figure 1
displays subsets of EEG signals from 9 channels, or locations in the brain, that is part
of 64 channel recording, from two individuals, one who is experiencing a first psychotic
episode (FEP) and one who is a healthy control (HC), for one minute while resting with
their eyes open. We desire a frequency domain analysis of each of these data that can pro-
vide insights into underlying dependence structure of the signals at each frequency. This
inherently requires low-dimensional representations that are sparse in coordinates in that
they are interpretable as combinations of power within certain channels or regions of the
brain.
Various formulations of the PC problem, alternative methods of imposing sparsity,
and convex relaxation of the corresponding optimization problems have inspired a notable
volume of research. For iid multivarite data, these methods include diagonal thresh-holding
forspikedcovariancemodels(JohnstoneandLu,2009), regularizedregressionproblem(Zou
et al., 2006), low rank matrix approximation with sparsity constraint (Shen and Huang,
2008;Wittenetal.,2009),maximizingvariabilityoversparsesubspaces(d’Aspremontetal.,
2004; Vu et al., 2013), and subspace estimation via orthogonal iteration with sparsification
(Ma, 2013; Yuan and Zhang, 2013). Literature on the analysis of sparse PCA for serially
dependent data is dearth compared to that for iid multivarite observations. In the time
3Healthy Control (HC) First−Episode Psychosis (FEP)
0 500 1000 1500 2000 0 500 1000 1500 2000
Time Time
Figure 1: Resting state EEG data from two individuals, a healthy control (HC, left) and
a person experiencing a first psychotic episode (FEP, right), from a subset of 9 channels
from a 64-channel montage.
domain, methods for principal subspace estimation under vector autoregressive models
were developed by Wang et al. (2013b) and Wang et al. (2013a), which utilizes a two
step method of Fantope Projection and Selection (FPS) (Vu et al., 2013) followed by an
orthogonal iteration method with sparsification (SOAP). Sparse PCA is even less explored
inthefrequencydomain. Tothebestofourknowledge, theonlypreviousmethodforsparse
PCA in the frequency domain is a two-step FPS followed by SOAP approach introduced
by (Lu et al., 2016).
Being equivalent to finding a sequence of principal subspaces of spectral matrices across
frequency, high-dimensional spectral domain PCA has an additional layer of complexity
compared to the iid multivaraite case that only considers the principal subspace of a single
matrix. This additional complexity presents two types of challenges. First, the ability
to consistently estimate a d-dimensional principal subspace relies on the first d eigenval-
ues being sufficiently larger than the remaining eigenvalues, which is often referred to as
4
'zI'
'zO'
'8OP'
'zP'
'5C'
'6CF'
'2F'
'4FA'
'zpF'
51
0
51−01
01−
0101−
5
01−
0
02−020
035−1
0
51−01
001−01
01−
'zI'
'zO'
'8OP'
'zP'
'5C'
'6CF'
'2F'
'4FA'
'zpF'
5
01−
5
01−
5
5−
0
01−51
0
51−010
510−1
001−01
0015−1
0
51−having a sufficient eigengap. This presents theoretical challenges in a PCA for the entire
spectrum across all frequencies since, although there can be a sufficient eigengap at certain
frequencies of interest, one is unable to reliably estimate principal subspaces of spectral
matrices at frequencies with low power where the eigengap is small. A second challenge
emerges with regards to interpretation in applications. The challenge of interpretation is
typically addressed by applied researchers by summarizing frequency-domain information
by collapsing power within a finite number of pre-defined frequency bands. Although his-
torically derived, pre-defined frequency bands have been shown to be associated with a
variety of scientific mechanisms, they are not optimal for parsimoniously summarizing and
describing information in any given signal. There has been considerable recent research
into methods to address this challenge and to learn frequency bands for a given time se-
ries that are optimal in some sense (Bruce et al., 2020; Granados-Garcia et al., 2022; Tuft
et al., 2023; Granados-Garcia et al., 2024). However, to the best of our knowledge, there
exist no approach to provide a frequency-domain PCA of a stationary time series that is
interpretable in that principal components are localized in frequency band.
The concept of localization to improve interpretation has been developed within the
context of functional data analysis, where it is often referred to as “interpretable functional
data analysis” (James et al., 2009; Chen and Lei, 2015; Zhang et al., 2021). Although
functional data analytic methods have been developed for the frequency-domain analysis
of replicated time series where several independent time series realizations are observed
(Krafty et al., 2011; Krafty, 2015), it should be noted that a different setting and question
is considered in this article. This article considers the analysis of a single realization
of a multivariate time series, so that we consider PCA in the sense of Brillinger (1964)
that involves principal subspaces of spectral matrices as operators on finite complex vector
5spaces, andnotPCAinthefunctionalsensethatconsidersprincipalsubspacesoffunctional
operators over a continuous domain of frequency.
The broad contribution of this paper is the introduction of the first method for con-
ducting interpretable frequency-domain PCA that is both sparse among variables as well
as localized in frequency. We formulate a definition for the PCA of multivariate time series
that contains a low-rank signal of interest with sparse and localized principal subspaces.
We propose an efficient, separable algorithm for estimating the principal subspaces, as well
as methods for selecting the sparsity level and localization parameter of the model. The ap-
proach estimates the sparse d-dimensional principal subspace itself, thus avoiding some of
the challenges associated with deflation that is required in approaches to sparse PCA that
sequentially estimate one-dimensional subspaces. Moreover, we propose a novel sequential
smoothing method that borrows information from lower frequencies to improve estimation
and provide interpretability of frequency localization as bands of frequency. Through sim-
ulation, we have studied the performance of the proposed algorithms on estimation of the
underlying principal subspaces as well as parameter selection. On the theoretical side, we
have established the consistency of the estimated principal subspaces in high-dimensions.
The proof of consistency follows closely those arguments presented in Wang et al. (2014)
and Lu et al. (2016). The difference being two folds. On one hand, the rate of conver-
gence of the distance between the estimated subspaces and the true one is obtained for the
smoothed version of the estimated subspaces. On the other hand, the proof technique of
Lu et al. (2016) for establishing a concentration inequality, in the sparse operator norm,
of their proposed spectral density estimator is adopted to establish a similar concentration
inequality for a smoothed estimate.
The remainder of this paper is organized as follows. In Section 2, a formulation of the
6localized and sparse principal subspaces of the underlying process is described. Section 3
is devoted to estimation procedure, where finding the sparse principal subspaces estimator
is described in 3.1, obtaining a localized solution is described in 3.2, and a novel smooth-
ing technique is described in 3.3. Theoretical analysis of the sparse principal component
estimator is covered in 3.4. In Section 4, we presented model selection procedures. Section
5 contains the simulation set up and results, and Section 6 contains the result of data
analysis. Proof of the theoretical results and tables of the simulation results are available
in the supplementary materials.
Notation:
Let A = [A ] ∈ Cp×p. We denote conjugate transpose of A by A† and will use it to
i,j
represent transpose of a real valued matrix as well. Let u ∈ Cp, the ℓ -norm of u is defined
2
√
as ∥u∥ = u†u and the ℓ -norm of u is the number of non-zero elements of u. For matrices
2 0
A and A , we define the inner product as ⟨A ,A ⟩ = tr(A†A ) and ∥A∥ = (cid:112) ⟨A,A⟩,
1 2 1 2 1 2 F
wheretr(A)isthetraceofA. ForanindexsetI ⊆ {1,...,p}, A denotesthematrixwhose
I
(j,k)-th entry equals A if j,k ∈ I and zero otherwise, and |I| denotes the size of I. In
j,k
(cid:80)
this paper, ∥A∥ determines the number of non-zero rows of A and ∥A∥ := |A |.
2,0 1,1 i,j i,j
In addition, for Hermitian matrices A,B ∈ Cp×p, A ≼ B if and only if B−A is positive
definite. We denote the real and imaginary parts by ℜ() and ℑ(), respectively, and we
define the unit ball in Cp by Sp−1(C) = {v ∈ Cp | ∥v∥ = 1}.
2
72 Localized Sparse Principal Components
2.1 Principal Components in Frequency Domain
Let {X(t) : t ∈ Z} be a p-dimensional stationary time series with mean vector E[X] =
(cid:110) (cid:111)
µ, auto-covariance matrix Γ(h) = E [X(t+h)−µ ][X(t)−µ]† , h = 0,±1,..., and
X
spectral density matrix
∞
(cid:88)
f(ω) = (2π)−1 Γ(h)exp{−iωh}, −∞ < ω < ∞,
h=−∞
that is continuous as a function of frequency. Consider the decomposition X(t) = ϑ(t)+
ε(t), t ∈ Z, where ϑ is the time series that is the closest time series to X in terms of
mean square error that can be obtained after compressing then reconstructing X through
a d-dimensional linear filter. Formally, ϑ is defined by the d×p filter {b(h)} and the p×d
(cid:80) (cid:80)
filter {c(h)} such that ϑ(t) = µ + c(t−h)ζ(h) and ζ(t) = b(t−h)X(h) minimizes
ϑ h h
(cid:110) (cid:111)
E [X(t)−ϑ(t)]†[X(t)−ϑ(t)] (1)
over all possible d×p and p×d filters.
(cid:80) (cid:80)
Let B(ω) = b(h)exp{iωh} and C(ω) = c(h)exp{iωh} be the corresponding
h h
transfer functions. The next theorem, presented in Brillinger (2001), identifies the optimal
transfer functions that minimizes Equation (1).
Theorem 1. Let {X(t)} be a p-dimensional weakly stationary time series with mean vec-
tor µ, an absolutely summable autocovariance function Γ(h), and spectral density ma-
trix f(ω),−∞ < ω < ∞. Then the µ ,{b(h)}, and {c(h)} that minimizes (1) are
ϑ
8given by µ = µ −
[(cid:80) c(h)][(cid:80)
b(h)]µ, b(h) =
(2π)−1(cid:82)2π
B(α)exp{ihα} dα, and
ϑ h h 0
c(h) =
(2π)−1(cid:82)2π
C(α)exp{ihα} dα, where C(ω) = [U (ω)...U (ω)], B(ω) = C†(ω),
0 1 d
and U (ω) is the j-th eigenvector of f(ω),j = 1,...,p. In addition, if λ (ω) denotes the
j j
(cid:82)2π(cid:80)
corresponding eigenvalue, j = 1,...,p, then the minimum obtained is λ (α) dα.
0 j>d j
Note that if we denote A(ω) = C(ω)B(ω), then
(cid:110) (cid:111) (cid:90) 2π (cid:110) (cid:111)
E [X(t)−ϑ(t)]†[X(t)−ϑ(t)] = tr [I −A(ω)]f(ω)[I −A(ω)]† d(ω)
0
A(ω) = U (ω)U (ω)† +···+U (ω)U (ω)†.
1 1 d d
In other words, for each ω ∈ [0,1), A(ω) is a rank d projection matrix. This indicates that
the solution to the minimization problem
(cid:90) 2π (cid:110) (cid:111)
min tr [I −A(ω)]f(ω)[I −A(ω)]† d(ω), (2)
A∈G
d 0
where G is the space of rank-d projection matrices, is equivalent to the solution to the
d
maximization problem
(cid:90) 2π
max tr[f(ω)A(ω)] d(ω). (3)
A∈G
d 0
The focus of this article is the interpretation and estimation of the principal subspace
spanned by the orthogonal directions U (ω), considered with reference to the eigenvalues
j
λ (ω). It should be noted that the power spectrum of ϑ can be represented as
j
d
(cid:88)
f (ω) = λ (ω)U (ω)U (ω)†.
ϑ j j j
j=1
Theprincipaltimeseriesζ (t), j = 1,...,d, areuncorrelatedtimeserieswithpowerspectra
j
λ (ω) that represent parsimonious underlying latent mechanisms that account for most of
j
9the information in X(t). The orthogonal directions U (ω) describe how these latent time
j
series relate to and can be interpreted from the perspective of the p-dimensional space. For
example, in our analysis of the EEG data that is presented in Section 6, U and λ are
j j
dominatedbyinformationwithinasubsetoftheδ frequenciesbetween0.5-4Hz, whichhas
been shown to be most prominent during times of and can be used to electrophysiologically
quantify rest, and within a subset of the θ frequencies between 4 - 7 Hz, which has been
shown to be associated with attention control. The principal time series ζ represent
j
uncorrelated relative expression of these two mechanisms, and the principal directions U
j
indicate how these mechanisms are expressed in each location of the brain.
2.2 Sparsity
This article is concerned with PCA where principal subspaces are sparse in variates. This
assumption is essential both to make estimation tractable in the high-dimensional setting,
as well as for interpretation. For example, in our motivating application, we desire a
parsimonious interpretation where a component represent power in only certain regions of
the brain by estimating principal subspaces that are sparse in the following sense.
Definition 2 (Subspace Sparsity). Let U be a d-dimensional subspace of Cp and U be the
set of p × d orthonormal matrices whose column span U. Let Π = UU†,U ∈ U be the
unique (orthogonal) projection matrix onto the subspace U. We define the sparsity level of
U as s = |supp[diag(Π)]|.
We desire a principal component analysis under the assumption that the principal sub-
space A(ω) that is spanned by U (ω),...,U (ω) is sparse with sparsity level s∗.
1 d
102.3 Frequency Localization
In addition to sparsity, we also desire a PCA that is localize in the frequency domain in
thatonlythemostrelevantfrequenciesareretained. Frequencylocalizationisimportantfor
two reasons. In terms of estimation, the ability to consistently estimate the d-dimensional
principal subspace of a matrix depends on the difference between the dth and d + 1st
eigenvalues. In many applications, including our motivating EEG example where there is
low signal at higher frequencies, this eigengap is not sufficiently large for spectral matrices
at many frequencies. In terms of interpretation and applications, we desire a parsimonious
decomposition of information in that principal components can be interpretable as having
support withing certain ranges or bands of frequency. This desire also mitigates the issues
caused by the inability to consistently estimate principal subspaces at frequencies with
insufficient eigengaps as said information is not of practical interest. Formally, we desire a
frequency localization procedure that identifies frequencies with sufficient power by finding
Ω ⊂ [0,2π) such that the power of X at frequency ω that is accounted for by ϑ, or
(cid:80)d
λ (ω), is greater than some threshold for all ω ∈ Ω. Although this threshold can
j=1 j
be selected either subjectively or based on existing scientific knowledge, in Section 4 we
present a data driven procedure for selecting this threshold relative to the variance of the
(cid:80) (cid:82)
remainder Var[ϵ(t)] = λ (ω)dω.
j>d j
2.4 Optimization
Our estimation of a sparse localized PCA begins by considering a frequency domain trans-
formation of the time series data to obtain an asymptotically unbiased estimate of the
spectral matrices f (ω) that can depend on some parameters Ψ. This estimator includes
Ψ
common frequency domain transformations of X(t) such as the periodogram, tapered pe-
11riodogram, multitaper estimate, and appropriately smoothed periodogram. In Section 3.3,
we propose a novel smoothed estimate that enables for efficient sequential implementation
by shrinking the d-dimensional principal subspace of f (ω ) towards the previously esti-
ℓ+1
matedprincipalsubspaceatfrequencyω . Thissmoothingstep,whichindirectlyregularizes
ℓ
the spectral matrix itself via directly regularizing its principal subspace, is important for
two reasons. First, it provides stability by sharing information across frequencies. Second,
it assures that the frequency localization provides measures that are interpreted as power
within continuous bands of frequency.
The equivalence between (2) and (3) enables us to formulate and estimate the localized
andsparseprincipalcomponents. First, inordertoobtainasparsesolutiontotheoptimiza-
tion problem (3), we can add the ℓ penalty term for each frequency component. Second,
0
to obtain a localized solution, we propose to discretize the objective function in (3) and
add a shrinkage parameter for each frequency. Combining the two steps, and given f (ω),
Ψ
we propose to estimate the sparse and localized principal components through solving the
following optimization problem.
 
n/2
(cid:88)
(cid:2)
(cid:3)
maximize β tr f (ω )V(ω )V(ω )†
ℓ Ψ ℓ ℓ ℓ
β ℓ,V(ω ℓ)  
ℓ=1
ℓ=1,...,n/2
(4)
subject to: V(ω ) orthonormal, ∥V(ω )∥ ≤ s∗ and
ℓ ℓ 2,0
n/2
(cid:88)
β ≤ η and 0 ≤ β ≤ 1,ℓ = 1,...,n/2.
ℓ ℓ
j=1
3 Estimation procedure
First observe that the optimization problem in (4) is separable, i.e. we can first optimize
with respect to V(ω ),ℓ = 1,...,n/2 and then with respect to β ,ℓ = 1,...,n/2. More
ℓ ℓ
12precisely, we can write the problem as
 
n/2 
(cid:88)
(cid:2)
(cid:3)
maximize β max tr f (ω )V(ω )V(ω )†
ℓ Ψ ℓ ℓ ℓ
β1,...,β n/2  ℓ=1 V(ω ℓ)   (5)
(cid:80)n/2β ≤η and 0≤β ≤1,ℓ=1,...,n/2 ℓ=1,...,n/2
j=1 ℓ ℓ
subject to: V(ω ) orthonormal, ∥V(ω )∥ ≤ s∗.
ℓ ℓ 2,0
Note that the later optimization problem is a constraint linear programming problem
with increasing objective function, as a function of β ,ℓ = 1,...,n/2. For this reason,
ℓ
when η ∈ N, the solution falls on the corners of the constraint set (Proposition (3.1)).
This indicates that the estimated β ,ℓ = 1,...,n/2 are either zero or one; in other words,
ℓ
the optimization problem selects the first η frequency components with highest objective
function. On the other hand, the former optimization problem is also separable and each
problem is a sparse principal component analysis of the spectral density matrix at a funda-
mental frequency. We consider solving the former problem through the two step procedure
of Fantope projection and selection (FPS) followed by sparse orthogonal iterated pursuit
(SOAP) proposed by Wang et al. (2014).
3.1 Solution of the Sparse Principal Components
Although the solution to
(cid:2) (cid:3)
max tr f (ω )V(ω )V(ω )† , s.t. V(ω ) orthonormal, ∥V(ω )∥ ≤ s∗, (6)
Ψ ℓ ℓ ℓ ℓ ℓ 2,0
V(ω )
ℓ
for each ℓ = 1,...,n/2 can attain the optimal statistical rate of convergence (Vu and Lei,
2012, 2013), it is NP-hard to compute (Moghaddam et al., 2005). Extensive research has
13been done to design a computationally feasible algorithm that enjoys optimal statistical
convergence rate, a review of which can be found in Section D of Zou and Xue (2018). In
this article, we adopted the two step algorithm proposed by Wang et al. (2014) that applies
to the non-spiked covariance models, and non-Gaussian as well as dependent data.
The optimization problem (6) can be solved directly by the orthogonal iteration algo-
rithm (Golub and Van Loan, 2013) combined with a sparsification step. In addition, to
ensure the solution attains optimal statistical rate of convergence, the initial value should
fall within an appropriate distance from the solution. The initial estimate is obtained by
applying the Fantope projection and selection (FPS) method that solves a convex relax-
ation of the problem (6). The two steps of (relaxed step) FPS followed by (tightened step)
SOAP are explained below.
We first introduce the FPS algorithm for estimating the sparse PCs of a real matrix Σ,
(cid:2) (cid:3)
where we maximize tr ΣVV† subject to V being orthonormal and ∥V∥ ≤ s∗. We then
2,0
extend it so that it can estimate the sparse principal subspace of a complex valued spectral
densitymatrix. Notethatwecanrelaxtheconstraintsetof(6)toobtainaconvexrelaxation
of the problem. To do so, let Π = VVT and note that since V is an orthonormal matrix, Π
is the projection matrix onto a d-dimensional subspace of Rp (in the real case). In addition,
we know that Π has exactly two eigenvalues, 1 with multiplicity d and 0 with multiplicity
p−d. Such constraint on eigenvalues of Π can be relaxed to tr(Π) = d and 0 ≼ Π ≼ I .
p
In addition, we relax the constraint ∥Π∥ ≤ s∗ to ∥Π∥ ≤ s∗. Note that the constraint
2,0 1,1
set {∥V∥ ≤ s∗} is not convex, while the set A = {Π : Π ∈ Rp×p,tr(Π) = d,0 ≼ Π ≼ I }
2,0 p
is convex.
The relaxed convex optimization problem can be equivalently expresses as
minimize {tr(Σ Π)+ρ∥Φ∥ | Π = Φ,Π ∈ A,Φ ∈ Rp×p}, (7)
1,1
14with the Lagrangian
L(Π,Φ,Θ) = tr(Σ Π)+ρ∥Φ∥ −tr[Θ(Π−Θ)], Π ∈ A,Φ ∈ Rp×p,Θ ∈ Rp×p, (8)
1,1
and be solved by the alternating direction of multiplier (ADMM), which iteratively mini-
mizes the augmented Lagrangian,
L(Π,Φ,Θ)+β/2∥Π−Φ∥2 (9)
F
with respect to Π and Φ and updating the dual variable Θ. We only need to iterate the
algorithm enough so that the calculated Π at iteration T, Π(T), falls within the basin of
attraction of the SOAP algorithm. A detailed description of the ADMM step can be found
in Appendix A. Then, the top d leading eigenvectors of Π(T) are used as the initial value
in the SOAP algorithm.
To apply the FPS algorithm to complex valued metrics we invoke to Lemma (B.4.3)
from the Appendix that describes the isomorphism between complex matrices and real
matrices. In particular, let
   
ℜ{f(ω )} ℑ{f(ω )} ℜ{f (ω )} ℑ{f (ω )}
ℓ ℓ Ψ ℓ Ψ ℓ
f(R)(ω ) :=   and f(R)(ω ) :=  . (10)
ℓ   Ψ ℓ  
−ℑ{f(ω )} ℜ{f(ω )} −ℑ{f (ω )} ℜ{f (ω )}
ℓ ℓ Ψ ℓ Ψ ℓ
If the eigenvalues and corresponding eigenvectors of f(ω ) are λ (ω ) and u (ω ),j =
ℓ j ℓ j ℓ
1,...,p, then the [2(j +1)+1]st and [2(j +1)+2]st eigenvalues and corresponding eigen-
15vectors of f(R)(ω ) are
ℓ
   
ℜ{u (ω )} −ℑ{u (ω )}
j ℓ j ℓ
λ (ω ),   and λ (ω ),  .
j ℓ   j ℓ  
ℑ{u (ω )} ℜ{u (ω )}
j ℓ j ℓ
Hence, we propose to apply the FPS algorithm to f(R)(ω ) and estimate the 2d-dimensional
Ψ ℓ
principal subspace of f(R)(ω ) to obtain the initial estimate of d leading eigenvectors of
Ψ ℓ
f(ω ). As will be shown in Theorem (3), part (I), the estimated subspace obtained in this
ℓ
manner will be consistent.
In the Tightened step, the orthogonal iteration method is followed by a truncation step
to enforce row-sparsity and further followed by taking another re-normalization step to
enforce orthogonality. More precisely, at the t-th iteration of the algorithm the following
operations are performed
• Orthogonal iteration: V˜(t+1) ← f (ω)U(t); V(t+1),R(t+1) ← QR(V˜(t+1))
Ψ 1
• Truncation/re-normalization: U˜(t+1) ← Truncate(V(t+1),sˆ); U(t+1),R(t+1) ← QR(U˜ )(t+1)
2
where columns of U(t) contain the estimated first d eigenvectors of f(ω) and the truncation
operator sets the p−sˆ rows with the smallest modulus to zero.
3.2 Solution of the Linear Programming Problem
ˆ
Let V(ω ),ℓ = 1,...,n/2 be a solution to
ℓ
max tr{[f (ω )]V(ω )V(ω )†}, s.t. V(ω ) orthonormal, ∥V(ω )∥ ≤ s∗.
Ψ ℓ ℓ ℓ ℓ ℓ 2,0
V(ω )
ℓ
16(cid:104) (cid:105)
Since f (ω ),ℓ = 1,...,n/2 are positive definite, h := tr f (ω )Vˆ (ω )Vˆ (ω )† > 0. Thus,
Ψ ℓ ℓ Ψ ℓ ℓ ℓ
we can write (5) as
 
n/2
(cid:88) 
maximize β h . (11)
ℓ ℓ
β1,...,β n/2  
ℓ=1
(cid:80)n/2β ≤η and 0≤β ≤1,ℓ=1,...,n/2
ℓ=1 ℓ ℓ
Note that, since h > 0,ℓ = 1,...,n/2, the objective function is monotonically increasing
ℓ
in β ,ℓ = 1,...,n/2 and therefore attains its maximum on the boundary of the constraint
ℓ
set. We claim that the algorithm selects the η largest h ’s and set the coefficients of the
j
n/2−η smallest h ‘s to zero.
j
Proposition 3.1. Let η ∈ N, η < K for some K ∈ N, and h ,...,h ∈ R+. In addi-
1 K
tion, let h ≥ ··· ≥ h be the sorted h′s in decreasing order and β ,...,β be the
(1) (K) j (1) (K)
corresponding coefficients in (11). Then
(cid:40) (cid:41)
K
(cid:88)
maximum β h . (12)
ℓ ℓ
β1,...,βK
ℓ=1
(cid:80)K β ≤η and 0≤β ≤1,ℓ=1,...,K
ℓ=1 ℓ ℓ
is attained at β = ··· = β = 1,β = ··· = β = 0
(1) (η) (η+1) (K)
3.3 Smoothing by Borrowing Information from Previous Fre-
quency
Thepreviouslydevelopedmethodologyisapplicableforanycommonquadratictransforma-
tion of the times series to obtain a asymptotically unbiased estimator of spectral matrices
f (ω), where M is some parameter. This includes frequency-domain transformations such
M
as the periodogram, for which M is trivial, multitaper estimators, for which M is the
number of tapers, kernel estimator, for which M is the bandwidth, and the truncated
17periodogram that will be considered when establishing theoretical properties
M
(cid:88)
ˆ
f (ω) = R exp{−2πiωt}, (13)
M t
t=−M
and Rˆ = 1 (cid:80)n−t X(k +t)X(t)†.
t n k=1
The explicit sharing of information across frequency principle subspaces is important
for improving stability of principle subspace estimation, especially in frequencies with low
signal. It is also important for assuring that frequency localization produces frequency
bands that are essential for interpretation. We introduce a novel approach to sharing
information to directly regularize on our object of interest, the principle subspace, and to
do so interactively to shrink towards the previous principle subspace, which makes for fast
computation. The approach considers incorporating smoothness through f considered
Ψ
in the previous sections, Ψ = (M,θ), where θ is a parameter that controls smoothness
through the dependence on the estimate at the previous frequency. This separation of the
incorporation of smoothness from regulation on sparsity or localization enables efficient
computation.
Given f (ω ), we propose a smoothing method that borrows information from the
M ℓ
estimated principal subspace obtained from the previous frequency component. More pre-
cisely, suppose
ΠˆΨ
is the d-dimensional principal subspace of f(ω ), ℓ = 1,...,n/2. One
ℓ ℓ
can obtain the projection of the left and right eigenvectors of f (ω ) on the estimated
M ℓ+1
d-dimensional principal subspace of f(ω ) by
ΠˆΨf
(ω
)ΠˆΨ.
Since the spectral density
ℓ ℓ M ℓ+1 ℓ
matrices are continuous as a function of frequency, we propose to combine information
at frequency ω and ω by estimating Π , denoted by
ΠˆΨ
, by applying the LSPCA
ℓ ℓ+1 ℓ+1 ℓ+1
Algorithm to
f (ω ) := (1−θ)f (ω )+θ
ΠˆΨf
(ω
)ΠˆΨ.
(14)
Ψ ℓ+1 M ℓ+1 ℓ M ℓ+1 ℓ
183.4 Theoretical Analysis
To investigate theoretical properties, we first introduce the notion of distance between
subspaces,inadditiontoseveralkeyquantitieswhichwillbeusedinthetheoreticalanalysis,
then we present the rate of convergence of the the estimated localized sparse principal
components.
• Subspace distance: Let U and U′ be two d-dimentional subspaces of Cp. Denote
the projection matrices onto them by Π and Π′, respectively. We define and denote
the distance between U and U′ by D(U,U′) = ∥Π−Π′∥ .
F
• Principal subspace notations: Let U∗ be the d-dimensional principal subspace
ℓ
of f(ω ) for each fundamental frequency ω = ℓ/n,ℓ = 2,...,n/2 and U(t)(ω ) be the
ℓ ℓ 1
d-dimensional subspace spanned by the top d eigenvectors of
Π¯(t)
obtained at the
t-th iteration of the ADMM algorithm presented in the Appendix A.
• Minimumnumberofiterationsanddatapoints: Letγ = sup 3λ d+1(ω)+λ d(ω)
ω∈[0,1] λ (ω)+3λ (ω)
d+1 d
(cid:26)(cid:113)
√
(cid:27)
and R = min dγ(1−γ1/2), 2γ . The minimum number of iterations of the ADMM,
2 4
˜
T , the SOAP, T , and the minimum data points, n , are
min min min
(cid:24) ζ2 (cid:25) (cid:24) 4log(R/ξ)(cid:25) (s∗)log(p) (cid:18) λ (ω ) (cid:19)2
T = 1 ; T˜ = ; n = C 1 1 ,
min (R−ζ )2 min log(1/γ) min R2 λ (ω )−λ (ω )
2 d 1 d+1 1
(15)
√
where ζ = C˜′λ1(ω1) .s∗(cid:113) log(p), and ζ = √C˜′′ M(n)λ1(ω1) (cid:16) d.p2log(p)(cid:17)1/4 √1 .
1 λ d(ω1)−λ d+1(ω1) n 2 λ d(ω1)−λ d+1(ω1) n t
Theorem 3. Let {X(t) : t = 1,...,n} be a realization of a weakly stationary time series
that follows M (f,d,s∗), as defined in Appendix B, with n > n and f (ω ) be as defined
d min M 1
(cid:112)
in (13). Let the regularization parameter in (7) be ϱ = Cλ (ω ) log(p)/n for a sufficiently
1 1
√ √
large constant C, and the penalty parameter β in (9) be β = 2p.ϱ/ d.
19(I) The iterative sequence of d-dimensional subspace {U(t)(ω )}T satisfies
1 t=1
C˜˜′λ
(ω )
(cid:114)
log(p)
C˜˜′′(cid:112)
M(n)λ (ω )
(cid:18) d.p2log(p)(cid:19)1/4
1
D(U(t)(ω ),U∗(ω ))≤ 1 1 .s∗ + 1 1 √
1 1 λ d(ω 1)−λ d+1(ω 1) n (cid:112) λ d(ω 1)−λ d+1(ω 1) n t
(16)
with high probability, where
C˜˜′
and
C˜˜′′
are constants.
(II) When θ < max
λ d(ω ℓ+1)−∥fM(ω ℓ+1)−f(ω ℓ+1)∥
op,|I| , by taking the sparsity parameter sˆ in
ℓ 2λ1(ω ℓ+1)[D(U ℓ∗,U ℓ∗ +1)+α]+λ d+1(ω ℓ+1)
(cid:110)(cid:104) (cid:105) (cid:111)
Algorithm SOAP such that sˆ= Cmax 4d ,1 s∗ for some integer constant
(γ−1/2−1)2
˜ ˜
C ≥ 1, T ≥ T iterations in Algorithm ADMM, and then T ≥ T iterations of
min min
Algorithm SOAP, the final estimator
Uˆ
=
U(T+T˜)
satisfies
ℓ ℓ
γ1/2
D(U∗,Uˆ ) ≤ C′′′ ∆(2sˆ)
ℓ ℓ 1−γ1/4
with high probability, for all ℓ = 1,2,...,n/2, where
√ (cid:20)(cid:18) (cid:113) (cid:19) (cid:21)
2d exp(−c M(n))∨M(n) s∗log(p) +2θλ (ω )(cid:2) D(U∗,U∗ )+α(cid:3) +θλ (ω )
0 n 1 ℓ+1 ℓ ℓ+1 d+1 ℓ+1
∆(s):=sup ,
1[λ (ω )−(1−θ)λ (ω )]−2θλ (ω )α
ωℓ 2 d ℓ+1 d+1 ℓ+1 1 ℓ+1
(17)
√ (cid:18) (cid:113) (cid:19)
α = sup c1 2d exp(−c M(n)∨M(n) s∗log(p) , and U(T+T˜) be the space
ω ℓ λ d(ω ℓ)−λ d+1(ω ℓ) 0 n ℓ
spanned by the columns of the estimator obtained from the Algorithm LSPCA.
4 Model Selection
In this section we address selection of four parameters: d, the dimension of the principal
subspaces, sˆ the sparsity parameter in Algorithm SOAP, η the localization parameter,
and θ the smoothing parameter. Given the complex nature of the problem that makes
the empirical joint selection of the parameters infeasible, we propose the selection of each
parameterindividually. Belowwepresentanoutlineoftheprocedure; adetaileddescription
is provided in the Supplementary materials.
20• Dimension of Principal Subspaces: We follow the general approach of determin-
ing the dimension of principal subspace by inspecting the scree plot or equivalently
by the plot of the proportion of variance explained.
• Localization Parameter: We propose to use information criteria based on the
ˇ ˇ
log-Whittle likelihood. More precisely, for each η, let L = {ℓ ,...,ℓ } ∈ L :=
κ (1) (κ)
{1,2,...,n/2}. The log-Whittle likelihood is estimated by
n/2
(cid:88)
log(L) = − {plogπ +log|Gˆ |+[d (ω )][Gˆ ]−1[d (ω )]†},
ℓ X ℓ ℓ X ℓ
ℓ=1
whereGˆ = I{ℓ ∈ Lˇ }ˆ f(d)(ω )+Σˆ ,ˆ f (ω ) = f (ω )Uˆ (ω )Uˆ (ω )†+···+f (ω )Uˆ (ω )Uˆ (ω )†,
ℓ κ ϑ ℓ ϑ ℓ Ψ ℓ 1 ℓ 1 ℓ Ψ ℓ d ℓ d ℓ
Uˆ
(ω
)areobteinedfromtheLSPCAalgorithm,Σˆ
=
(cid:80)
d (ω )d (ω
)†/(cid:0) |Lˇ |−|Lˇ |(cid:1)
,
j ℓ ℓ∈Lˇ/Lˇ
κ
X ℓ X ℓ κ
ˆ
and d (ω ) is the discrete Fourier transform of the data at ω . We use to the G
X ℓ ℓ ℓ
to define standard information criteria for η including AIC = −2log(L) + 2|L |,
K
AICc = −2log(L)+2|L |+
2|LK|2+2|LK|,
and BIC = −2log(L)+log(n)|L |. The
K n−|LK|−1 K
localization parameter η is selected to minimizing an information criteria.
• Sparsity Parameter: For selecting the sparsity level of the underlying process, we
propose to use k-folds cross validation, with the Mahalanobis distance to evaluate
the performance of fitted model in the validation step.
• Smoothing parameter: We use k-folds cross validation similar to the sparsity
parameter selection.
215 Simulation Results
5.1 Sparsity Parameter Selection
To illustrate the performance of the LSPCA algorithm, effect of smoothing, and perfor-
mance of the parameter selection procedures, we examined two settings each for 4 combi-
nations of time series length and dimension: (p = 64,n = 2048),(p = 64,n = 4096),(p =
128,n = 2048), and (p = 128,n = 4096). In this simulation study, 100 realizations for each
setting and combination of n and p were generated, and the first eigenvector of the spectral
density matrix at each frequency were estimated using the LSPCA algorithm.
The first setting considered a process that has a principle time series that is a band
limited AR(2) process with one band of support. We consider the AR(2) process Y (t) =
1
Y (t−1)−(.9)Y (t−2)+W(t), where W(t) is unit-variance Gaussian white noise. Through-
1 1
out this section, W with and without subscripts indicates unit-variance Gaussian white
(cid:80)
noise. This is used to construct the band-limited AR(2) process Y (t) = a(t−u)Y(u)
b u
where a(t) is the linear filter with frequency response I(2π×[0.1,0.2]), and I(A) is the indi-
cator function of set A. This is then used to construct the time series X such that X (t) =
1
3Y (t)+W (t), X (t) = X (t)+W (t), X (t) = 1.1X (t)+W (t), X (t) = 1.2X (t)+W (t),
b 1 2 1 2 3 1 3 4 1 4
X (t) = 1.15X (t)+W (t) and X (t) ∼ W (t), j = 6,...,p.
5 1 5 j j
The second setting considered a process that has a principle time series that is a band
limited AR(4) process with two bands of support. For this process, we consider AR(4)
processY (t) = (1.55)Y (t−1)−(1.694565)Y (t−2)+(1.341848)Y (t−3)−(0.6521739)Y (t−
1 1 1 1 1
(cid:80)
4)+W(t)anditsband-limitedfilteredseriesY (t) = a(t−u)Y(u)wherea(t)isthelinear
b u
(cid:83)
filter with frequency response I(2π(×[0.05,0.15] [0.22,0.27])). The series X is obtained
from this filtered AR(4) process in the same manner as the series was obtained for the first
22n = 2048 n = 4096
p = 64 p = 128 p = 64 p = 128
AIC BIC AIC BIC AIC BIC AIC BIC
Median 222.0 208.0 221 208 447 412 444.5 411.5
IQR 10 10 11 10 16 18 14 17
Table 1: Frequency parameter selection for AR(2) process with θ = 0; Table contains
summarystatisticsofthenumberoffrequenciesselectedviaAICandBIC.Thetruenumber
of frequencies is when n = 2048 is 205 and when n = 4096 is 410.
n = 2048 n = 4096
p = 64 p = 128 p = 64 p = 128
AIC BIC AIC BIC AIC BIC AIC BIC
Median 239.0 218.0 233 213 447 435 472 433
IQR 11 10 20 20 21 13 17.5 12
Table 2: Frequency parameter selection for AR(4) process with θ = 0; Table contains
summarystatisticsofthenumberoffrequenciesselectedviaAICandBIC.Thetruenumber
of frequencies is when n = 2048 is 205 and when n = 4096 is 410.
process.
In these simulations, we used a 10-sine multitaper for f , an initial sparsity level of
M
sˆ= 16, and observed that the selected sparsity and localization parameters did not change
after two iterations. First we elaborate on the performance of the frequency parameter
selection using the information criterion. Note that 20% of the frequency components
belong to the frequency support of the processes considered in simulations. In other words,
when n = 2048, η is 205 and when n = 4096, η eta is 410. We can see, in Tables 1 and
2, that for all combinations of n and p and across both cases considered, using θ = 0, the
AIC and BIC slightly overestimates the frequency parameter. This is due to the smoothing
effect of the multitaper estimator of the spectral density matrices. We expected to see
this behavior for other estimators such as kernel based estimators of the spectral density
matrix.
23n = 2048 n = 4096
p = 64 p = 128 p = 64 p = 128
mean sd mean sd mean sd mean sd
θ = 0 28.98 4.28 34.35 4.00 44.13 5.74 48.55 4.73
θ = 0.2 25.28 6.43 32.43 7.37 39.83 9.63 48.69 9.67
θ = 0.4 21.87 8.19 29.58 9.99 33.09 8.71 42.27 11.54
θ = 0.6 18.64 7.44 27.04 11.13 28.82 8.73 40.66 14.67
θ = 0.8 16.80 5.31 28.44 11.43 26.51 11.12 41.97 15.75
θ = 1 25.36 12.32 34.85 10.87 37.93 16.87 54.25 13.72
Table 3: AR(2): Frobinus norm of E = [Uˆ(θ)(ω )−U (ω )]p,n/2
p×(n/2) j ℓ j ℓ j=1,ℓ=1
5.2 Smoothing Parameter
To illustrate the effect of smoothing on the estimation of eigenvector trajectory over fre-
quency we consider unfiltered version of the process in the first setting, where X (t) =
1
3Y (t)+W (t),j = 1,...,5 and Y (t) is an AR(2) process. Bellow the plot of the first 4 co-
1 1 t
ordinates of the estimated leading eigenvectors over frequency components ω = 2πℓ/n,ℓ =
ℓ
1,...,n/2 for θ ∈ {0,0.2,0.4,0.6,0.8,1} are illustrated. For comparison, we also plotted
the sample eigenvectors and the population eigenvector over frequency. It can be seen that
when θ = 0, estimates of the leading eigenvector at high frequencies (which corresponds
to low power frequencies), are not stable. By increasing the smoothing parameter θ, the
trajectories become smoother and more closely following the population trajectories. To
summarize the smoothing effect numerically, let U (ω) be the the jth ,j = 1,...,p, coordi-
j
nate of the leading eigenvector of f (ω) and
Uˆ(θ)(ω)
be its corresponding sparse smoothed
X j
estimate. We computed summary statistics of the Frobinus norm of E = [E ]p,n/2 ,
p×(n/2) j,ℓ j=1,ℓ=1
where E(θ) = Uˆ(θ)(ω )−U (ω ). Tables 3 and 4 show that by increasing sample size, the
j,ℓ j ℓ j ℓ
average Frobinus norm of E per frequency component decreases. In addition, we can
p×(n/2)
see that smoothing in all cases reduces the ∥E ∥ .
p×(n/2) F
241st coordinate − Mod 1st evec over freq 2nd coordinate − Mod 1st evec over freq
Population Population
sample sample
theta=0 theta=0
theta=0.2 theta=0.2
theta=0.4 theta=0.4
theta=0.6 theta=0.6
theta=0.8 theta=0.8
theta=1 theta=1
0 100 200 300 400 500 0 100 200 300 400 500
Index Index
3rd coordinate − Mod 1st evec over freq 4th coordinate − Mod 1st evec over freq
Population Population
sample sample
theta=0 theta=0
theta=0.2 theta=0.2
theta=0.4 theta=0.4
theta=0.6 theta=0.6
theta=0.8 theta=0.8
theta=1 theta=1
0 100 200 300 400 500 0 100 200 300 400 500
Index Index
Figure 2: Leading eigenvector trajectories for the unfiltered version of the AR(2) process
in case 1. Index k corresponds to frequency 2πk/n for k = 1 : ...,n/2.
25
0.1
8.0
6.0
4.0
2.0
0.0
0.1
8.0
6.0
4.0
2.0
0.0
0.1
8.0
6.0
4.0
2.0
0.0
0.1
8.0
6.0
4.0
2.0
0.0n = 2048 n = 4096
p = 64 p = 128 p = 64 p = 128
mean sd mean sd mean sd mean sd
θ = 0 29.66 4.64 33.24 3.88 41.97 5.79 47.75 4.93
θ = 0.2 26.61 8.13 31.94 7.48 37.42 8.57 44.69 9.67
θ = 0.4 22.53 8.67 29.71 9.69 31.35 11.09 43.96 14.40
θ = 0.6 20.55 10.23 29.05 12.48 27.25 12.01 42.56 17.54
θ = 0.8 20.35 11.78 28.37 11.89 24.10 11.91 38.48 18.10
θ = 1 26.01 11.91 35.51 10.36 34.8 18.07 52.28 15.09
Table 4: AR(4): Frobinus norm of E = [Uˆ(θ)(ω )−U (ω )]p,n/2
p×(n/2) j ℓ j ℓ j=1,ℓ=1
6 Data Analysis
Evidence suggests that electrophysiological activity at different frequencies and locations
of the brain can be biomarkers for schizophrenia and for first-episode psychosis (FEP)
(Renaldi et al., 2019; Zhang et al., 2021). To illustrate the use of LSPCA to obtain inter-
pretable frequency-channel analyses, we apply it separately to two resting state 64-channel
electroencephalography (EEG) recordings: one from a patient who is experiencing FEP
and has been emitted to the emergency department of the Western Psychiatric Hospital of
the University of Pittsburgh Medical Center, and one from a healthy control (HC). During
the recording, participants sat in a chair and relaxed with their eyes open. Data were
recorded using 10-10 system and were initially sampled at a rate of 250 Hz for one minute.
Pre-processing consisted of down-sampling to 64 Hz and filtering using a 1 Hz high-pass
filter and 58 Hz low-pass filter; removal of segments with large artifacts such as muscle
activity or movements by a trained EEG data manager; and further removal of subtle ar-
tifacts such as ocular movement and cardiac signals via independent component analysis
(Delorme and Makeig, 2004).
We applied the LSPCA algorithm with sparsity and smoothing parameters selected
using 2-folds cross validation, and the frequency parameter was selected using AIC. A
26sparsity level of sˆ= 8 was selected for both subjects, smoothing parameters of θ = 0.2 and
θ = 0.6 were selected and localization parameters of η = 41 and η = 52 were selected for
the FEP and HC participants, respectively. Inspection of the scree plots at all frequencies
suggests that d = 2.
We explored the results of the analysis in two manners. First, we investigated the
loadings for the real and imaginary parts of the d = 2 components as functions of coordi-
nate/channel and frequency for the FEP participant in Figure 3 and for the healthy control
in Figure 4. Principal subspaces for both participants are localized within the union of a
band of low frequencies in the δ-band less than 1 Hz and band within θ-frequencies between
3.5 - 5.5 Hz. As power within the δ-band is characteristic of unconscious processes and
elevated during rest, and power within the θ-band is involved in cognitive processes such as
attention control that is elevated when eyes are open, this localization is not unexpected.
However, as opposed to collapsing power within the historically defined δ and θ bands of
0.5 - 4 Hz and 4 - 7 Hz, the data-driven LSPCA identified narrower, more parsimonious
bands.
Next, we explore the spatial localization of power within these identified bands. Figures
(cid:80) ˆ
5 and 6 display the diagonal elements of f (ω) where B is the localized frequency
ω∈B ϑ
band. The results indicate different activity patterns in the FEP and HC subjects. For
instance, for the HC, central channels are more active over both δ and θ frequency band. In
contrast, the frontopolar and parietal channels are more active over δ band in FEP subject
and central channels show more activity over the θ band. The observed difference in brain
activity between the FEP and HC participants have also been reported by other investi-
gators. For instance, in a longitudinal case-control study of FEP subjects, Renaldi et al.
(2019), investigators found that the resting state EEG of FEP subjects show significantly
27Real part of the first PC loadings Imaginary part of the first PC loadings
60 60
50 0.6 50 0.4
40 0.3 40 0.2
30 0.0 30 0.0
20 −0.3 20 −0.2
10 −0.6 10 −0.4
0 0
0 3.2 6.4 9.6 12.8 16 19.2 22.4 25.6 28.8 32 0 3.2 6.4 9.6 12.8 16 19.2 22.4 25.6 28.8 32
Hz Hz
Real part of the second PC loadings Imaginary part of the second PC loadings
60 60
50 50
40 0.25 40 0.25
30 0.00 30 0.00
20 −0.25 20 −0.25
10 −0.50 10 −0.50
0 0
0 3.2 6.4 9.6 12.8 16 19.2 22.4 25.6 28.8 32 0 3.2 6.4 9.6 12.8 16 19.2 22.4 25.6 28.8 32
Hz Hz
Figure 3: Subject FEP: Top left panel Real part of the first PC loadings; Top right
pane: Imaginary part of the first PC loadings. Bottom left panel Real part of the second
PC loadings; Bottom right pane: Imaginary part of the second PC loadings.
higher power over Delta band in the frontal and posterior regions compared to the control
group.
7 Discussion
This article introduced what is, to the best of our knowledge, the first approach to con-
ducting a PCA on a high-dimensional stationary time series whose principal subspace is
sparse among variates, localized within frequency, and smooth as a function of frequency.
The method is by no means exhaustive and can potentially be extended to more com-
plex scenarios. The first of these is to nonstationary time series. Although the developed
LSPCA routine could be applied directly using quadratic time-frequency transformations
such as the local Fourier periodogram or SLeX periodogram, it is not yet obvious how
to impose sparsity, frequency localization, or smoothness on the time-frequency subspaces
28
etanidrooC
etanidrooC
etanidrooC
etanidrooCReal part of the first PC loadings Imaginary part of the first PC loadings
60 60
50 0.0 50 0.0
40 −0.1 40 −0.1
30 −0.2 30 −0.2
20 −0.3 20
10 −0.4 10 −0.3
0 0
0 3.2 6.4 9.6 12.8 16 19.2 22.4 25.6 28.8 32 0 3.2 6.4 9.6 12.8 16 19.2 22.4 25.6 28.8 32
Hz Hz
Real part of the second PC loadings Imaginary part of the second PC loadings
60 60
50 0.50 50 0.50
40 0.25 40 0.25
30 0.00 30 0.00
20 −0.25 20 −0.25
10 −0.50 10 −0.50
0 0
0 3.2 6.4 9.6 12.8 16 19.2 22.4 25.6 28.8 32 0 3.2 6.4 9.6 12.8 16 19.2 22.4 25.6 28.8 32
Hz Hz
Figure 4: Subject HC: Top left panel Real part of the first PC loadings; Top right pane:
Imaginary part of the first PC loadings. Bottom left panel Real part of the second PC
loadings; Bottom right pane: Imaginary part of the second PC loadings.
that accounts for temporal ordering and information. A second extension is to the repli-
cated time series setting in which a joint analysis is conducted on data where multivariate
time series are observed for multiple subjects. As opposed to the analysis presented in
Section 6, where separate PCAs were conducted individually for two separate subjects,
a PCA for replicated time series will find optimal eigenspaces for describing mutual and
subject specific information. Lastly, the proposed LSPCA offers a regularized estimation
approach. One might desire a confidence-based procedure that provides inference with
regards to included frequency bands and retained channels. Although the excursion set
method that has been used to obtain inference for spatial clusters in image data appears
to provide a natural solution for conducting inference with regards to frequency localiza-
tion (Maullin-Sapey et al., 2023), how to do this while imposing sparsity and extract the
low-dimensional principal subspace could prove to be challenging.
29
etanidrooC
etanidrooC
etanidrooC
etanidrooCNZ NZ
FPZ FPZ
FP1 FP2 FP1 FP2
high high
F9 AF7AF5AF3AF1AFZAF2AF4AF6AF8 F10 F9 AF7AF5AF3AF1AFZAF2AF4AF6AF8 F10
FT9 F7 F5 F3 F1 FZ F2 F4 F6 F8 FT10 FT9 F7 F5 F3 F1 FZ F2 F4 F6 F8 FT10
FT7FC5 FC3 FC1 FCZ FC2 FC4 FC6FT8 FT7FC5 FC3 FC1 FCZ FC2 FC4 FC6FT8
T9 T10 T9 T10
T7 C5 C3 C1 CZ C2 C4 C6 T8 T7 C5 C3 C1 CZ C2 C4 C6 T8
A1 A2 A1 A2
TP7 CP5 CP3 CP1 CPZ CP2 CP4 CP6 TP8 TP7 CP5 CP3 CP1 CPZ CP2 CP4 CP6 TP8
TP9 TP10 TP9 TP10
P7 P5 P3 P1 PZ P2 P4 P6 P8 P7 P5 P3 P1 PZ P2 P4 P6 P8
P9 P10 P9 P10
PO7PO5PO3PO1POZPO2PO4PO6PO8 low PO7PO5PO3PO1POZPO2PO4PO6PO8 low
PO9 O1 O2 PO10 PO9 O1 O2 PO10
OZ OZ
I1 IZ I2 I1 IZ I2
Figure 5: Active channels in FEP subject: Left panel: Delta wave frequency components;
Right panel: Theta wave frequency components.
SUPPLEMENTARY MATERIAL
Algorithms: The ADMM for solving (7), the SOAP, and the LSPCA Algorithms are
outlined. In addition, the sparsity, localization, and smoothing parameter selection
procedures are outlined.
Proofs: Details of the theoretical analysis and proofs of the theorems in the paper are
provided.
References
Anderson, T. W. (1958), An Introduction to Multivariate Statistical Analysis, vol. 2, Wiley
New York.
Bhatia, R. (2013), Matrix Analysis, vol. 169, Springer Science & Business Media.
30NZ NZ
FPZ FPZ
FP1 FP2 FP1 FP2
high high
F9 AF7AF5AF3AF1AFZAF2AF4AF6AF8 F10 F9 AF7AF5AF3AF1AFZAF2AF4AF6AF8 F10
FT9 F7 F5 F3 F1 FZ F2 F4 F6 F8 FT10 FT9 F7 F5 F3 F1 FZ F2 F4 F6 F8 FT10
FT7FC5 FC3 FC1 FCZ FC2 FC4 FC6FT8 FT7FC5 FC3 FC1 FCZ FC2 FC4 FC6FT8
T9 T10 T9 T10
T7 C5 C3 C1 CZ C2 C4 C6 T8 T7 C5 C3 C1 CZ C2 C4 C6 T8
A1 A2 A1 A2
TP7 CP5 CP3 CP1 CPZ CP2 CP4 CP6 TP8 TP7 CP5 CP3 CP1 CPZ CP2 CP4 CP6 TP8
TP9 TP10 TP9 TP10
P7 P5 P3 P1 PZ P2 P4 P6 P8 P7 P5 P3 P1 PZ P2 P4 P6 P8
P9 P10 P9 P10
PO7PO5PO3PO1POZPO2PO4PO6PO8 low PO7PO5PO3PO1POZPO2PO4PO6PO8 low
PO9 O1 O2 PO10 PO9 O1 O2 PO10
OZ OZ
I1 IZ I2 I1 IZ I2
Figure 6: Active channels in HC subject: Left panel: Delta wave frequency components;
Right panel: Theta wave frequency components.
Brillinger, D. R. (1964), “The Generalization of Techniques of Factor Analysis Canonical
Correlation and Principal Component to Stationary Time Series,” Invited paper at Royal
Statistical Society Conference in Cardiff, Wales.
— (1969), “The Canonical Analysis of Stationary Time Series,” Multivariate Analysis, 2,
331–350.
— (2001), Time Series: Data Analysis and Theory, SIAM.
Bruce, S., Tang, C., Hall, M., and Krafty, R. (2020), “Empirical Frequency Band Analysis
of Nonstationary Time Series,” Journal of the American Statistical Association, 115,
1933–1945.
Chen,K.andLei,J.(2015),“LocalizedFunctionalPrincipalComponentAnalysis,”Journal
of the American Statistical Association, 110, 1266–1275.
31d’Aspremont, A., Ghaoui, L., Jordan, M., and Lanckriet, G. (2004), “A Direct Formula-
tion for Sparse PCA Using Semidefinite Programming,” Advances in Neural Information
Processing Systems, 17.
Delorme, A. and Makeig, S. (2004), “EEGLAB: An Open Source Toolbox for Analysis
of Single-Trial EEG Dynamics Including Independent Component Analysis,” Journal of
Neuroscience Methods, 134, 9–21.
Golub, G. H. and Van Loan, C. F. (2013), Matrix Computations, JHU press.
Goodman, N.(1967), “EigenvaluesandEigenvectorsofSpectralDensityMatrices,” Seismic
Data Laboratory Report, 179.
Granados-Garcia, G., Fiecas, M., Babak, S., Fortin, N. J., and Ombao, H. (2022), “Brain
Waves Analysis Via a Non-Parametric Bayesian Mixture of Autoregressive Kernels,”
Computational Statistics & Data Analysis, 174, 107409.
Granados-Garcia, G., Prado, R., and Ombao, H. (2024), “Bayesian Nonparametric Multi-
variate Mixture of Autoregressive Processes with Application to Brain Signals,” Econo-
metrics and Statistics.
Hotelling, H. (1933), “Analysis of a Complex of Statistical Variables into Principal Com-
ponents,” Journal of Educational Psychology, 24, 498–520.
James, G. M., Wang, J., and Zhu, J. (2009), “Functional Linear Regression That’s Inter-
pretable,” The Annals of Statistics, 37, 2083 – 2108.
Jiao, S., Shen, T., Yu, Z., and Ombao, H. (2021), “Change-Point Detection Using Spectral
PCA for Multivariate Time Series,” Journal of the Royal Statistical Society Series B:
Statistical Methodology.
32Johnstone, I. M. and Lu, A. Y. (2009), “On Consistency and Sparsity for Principal Com-
ponents Analysis in High Dimensions,” Journal of the American Statistical Association,
104, 682–693.
Johnstone, I. M. and Paul, D. (2018), “PCA in High Dimensions: An Orientation,” Pro-
ceedings of the IEEE, 106, 1277–1292.
Krafty, R. (2015), “Discriminant Analysis of Time Series in the Presence of Within-Group
Spectral Variability,” Journal of Time Series Analysis, 37, 435–450.
Krafty, R., Hall, M., and Guo, W. (2011), “Functional Mixed Effects Spectral Analysis,”
Biometrika, 98, 583–598.
Lu, J., Chen, Y., Zhu, X., Han, F., and Liu, H. (2016), “Sparse Princi-
pal Component Analysis in Frequency Domain for Time Series,” https://junwei-
lu.github.io/papers/FourierPCA.pdf.
Ma, Z. (2013), “Sparse Principal Component Analysis and Iterative Thresholding,” The
Annalls of Statistics, 772–801.
Maullin-Sapey, T., Schwartzman, A., and Nichols, T. E. (2023), “Spatial Confidence Re-
gions for Combinations of Excursion Sets in Image Analysis,” Journal of the Royal Sta-
tistical Society Series B: Statistical Methodology, 86, 177–193.
Merlev`ede, F., Peligrad, M., andRio, E.(2011), “ABernsteintypeinequalityandmoderate
deviations for weakly dependent sequences,” Probability Theory and Related Fields, 151,
435–474.
Moghaddam, B., Weiss, Y., and Avidan, S. (2005), “Spectral Bounds for Sparse PCA:
33Exact and Greedy Algorithms,” Advances in Neural Information Processing Systems,
18.
Ombao, H., von Sachs, R., and Guo, W. (2005), “SLEX Analysis of Multivariate Nonsta-
tionary Time Series,” Journal of the American Statistical Association, 100, 519–531.
Paul, D. (2007), “Asymptotics of Sample Eigenstructure for a Large Dimensional Spiked
Covariance Model,” Statistica Sinica, 1617–1642.
Pearson, K. (1901), “LIII. On Lines and Planes of Closest Fit to Systems of Points in
Space,” The London, Edinburgh, and Dublin Philosophical Magazine and Journal of
Science, 2, 559–572.
Renaldi, R., Kim, M., Lee, T. H., Kwak, Y. B., Tanra, A. J., and Kwon, J. S. (2019),
“Predicting Symptomatic and Functional Improvements over 1 Year in Patients with
First-Episode Psychosis Using Resting-State Electroencephalography,” Psychiatry In-
vestigation, 16, 695.
Shen, H. and Huang, J. Z. (2008), “Sparse Principal Component Analysis via Regularized
Low Rank Matrix Approximation,” Journal of Multivariate Analysis, 99, 1015–1034.
Stewart, G. and Sun, J.-g. (1990), Matrix Pertrubation Theory, Academic Press.
Sundararajan, R. R. (2021), “Principal Component Analysis Using Frequency Components
of Multivariate Time Series,” Compuational Statistics and Data Analysis, 157.
Tuft, M., Hall, M. H., and Krafty, R. T. (2023), “Spectra in Low-Rank Localized Layers
(SpeLLL) for Interpretable Time–Frequency Analysis,” Biometrics, 79, 304–318.
Vu, V. and Lei, J. (2012), “Minimax Rates of Estimation for Sparse PCA in High Dimen-
sions,” in Artificial Intelligence and Statistics, PMLR, pp. 1278–1286.
34Vu, V. Q., Cho, J., Lei, J., and Rohe, K. (2013), “Fantope Projection and Selection:
A Near-Optimal Convex Relaxation of Sparse PCA,” Advances in Neural Information
Processing Systems, 26.
Vu, V. Q. and Lei, J. (2013), “Minimax Sparse Principal Subspace Estimation in High
Dimensions,” The Annals of Statistics, 41, 2905 – 2947.
Wang, Z., Han, F., and Liu, H. (2013a), “Sparse Principal Component Analysis for High
Dimensional Multivariate Time Series,” in Artificial Intelligence and Statistics, PMLR,
pp. 48–56.
— (2013b), “Sparse Principal Component Analysis for High Dimensional Vector Autore-
gressive Models,” arXiv preprint arXiv:1307.0164.
Wang, Z., Lu, H., and Liu, H. (2014), “Nonconvex Statistical Optimization: Minimax-
Optimal Sparse PCA in Polynomial Time,” arXiv preprint arXiv:1408.5352.
Witten, D. M., Tibshirani, R., and Hastie, T. (2009), “A Penalized Matrix Decomposition,
with Applications to Sparse Principal Components and Canonical Correlation Analysis,”
Biostatistics, 10, 515–534.
Yuan, X.-T. and Zhang, T. (2013), “Truncated Power Method for Sparse Eigenvalue Prob-
lems.” Journal of Machine Learning Research, 14.
Zhang, J., Siegle, G. J., Sun, T., D’andrea, W., and Krafty, R. T. (2021), “Interpretable
Principal Component Analysis for Multilevel Multivariate Functional Data,” Biostatis-
tics, 24, 227–243.
Zou, H., Hastie, T., and Tibshirani, R. (2006), “Sparse Principal Component Analysis,”
Journal of Computational and Graphical Statistics, 15, 265–286.
35Zou, H. and Xue, L. (2018), “A Selective Overview of Sparse Principal Component Anal-
ysis,” Proceedings of the IEEE, 106, 1311–1320.
Appendix A Algorithms
In section A.1 the ADMM, SOAP, and the LSPCA algorithms are illustrated. Detailed
description of the localization, sparsity, and smoothing parameter selection are provided in
Sections A.2, A.3, and A.4, respectively.
A.1 LSPCA
Algorithm 1: ADMM : Solving (7)
ˆ
Input: Spectral density estimator Σ
Output: Uinit
Parameters : Regularization parameter ρ > 0, penalty parameter β > 0,
maximum number of iterations T
Initialization: Π(0) ← 0, Φ(0) ← 0, Θ(0) ← 0
for t = 0,...,T −1 do
Πt+1 ← argmin {L(Π,Φ(t),Θ(t) +β/2∥Π−Φ(t)∥2 | Π ∈ A}
F
Φt+1 ← argmin {L(Π(t+1),Φ,Θ(t))+β/2∥Π(t+1) −Φ∥2 | Φ ∈ Rp×p}
F
Θ(t+1) ← Θ(t) −β(Π(t+1) −Φ(t+1))
end
Π¯(T) = 1 (cid:80)T Π(t)
T t=0
Set the columns of Uinit to be the top q leading eigenvectors of Π¯(T)
Output: Uinit
36Algorithm 2: Projection
Input:
Φ(t),Θ(t),Σˆ
,β
Output: Π(t+1)
Function
Projection(Φ(t),Θ(t),Σˆ
,β)
Eigenvalue Decomposition: AΛQ∗ ← Φ(t) +Θ(t)/β +Σˆ /β
(v′,...,v′) = argmin{∥v −diag(Λ(t))∥2 | v ∈ Rp,(cid:80) v = q,v ∈ [0,1] for all j}
1 p 2 j j j
Π(t+1) ← Qdiag{v′,...,v′}Q∗
1 p
return Π(t+1)
return
Algorithm 3: Soft-Thresholding
Input: Π(t+1),Θ(t),ρ,β
Output: Φ(t+1)
for i,j ∈ {1,...,d} do
Φ(t+1) ←


 0 if |Π(t+1) −Θ(t)/β| ≤ ρ/β
 i,j i,j
 (cid:16) (cid:17)(cid:16) (cid:17)
 sign Π(t+1) −Θ(t)/β |Π(t+1) −Θ(t)/β|−ρ/β if |Π(t+1) −Θ(t)/β| > ρ/β
 i,j i,j i,j i,j i,j i,j
end
Output: Φ(t+1)
In the Tightened step, the orthogonal iteration method is followed by a truncation step
to enforce row-sparsity and further followed by taking another re-normalization step to
enforce orthogonality.
37Algorithm 4: SOAP
Input: Spectral density matrix estimator f (ω), initialization Uinit
Ψ
˜
Parameters : Sparsity parameter sˆ, Maximum number of iteration T
Initialization: U˜(T+1) ← Truncate(Uinit,sˆ), U(T+1), R(T+1) ← Thin-QR(U˜(T+1))
2
˜
for t = T +1...,T +T −1 do
V˜(t+1) ← f (ω)U(t)
Ψ
V(t+1),R(t+1) ← Thin-QR(V˜(t+1))
1
U˜(t+1) ← Truncate(V(t+1),sˆ)
U(t+1),R(t+1) ← Thin-QR(U˜ )(t+1)
2
end
Output:
UT+T˜
Algorithm 5: U˜(t+1) ← Truncate(V(t+1),sˆ) :
The Truncate function, where V denotes the i-th row vector of V.
i,.
Function Truncate(V(t+1),sˆ)
Row Sorting:
I ← The set of row index i‘s corresponding to the top sˆ largest ∥V(t+1)∥ ‘s
sˆ i,. 2
for i ∈ {1,...,p} do
U˜(t+1) ← 1[i ∈ I ]V(t+1)
i,. sˆ i,.
end
return
U˜(t+1)
return
The combined algorithm is as follows
38Algorithm 6: LSPCA
Input: {f (ω )}n/2
Ψ ℓ ℓ=1
Parameters: Regularization parameter ρ > 0, penalty parameter β > 0,
maximum number of iterations of ADMM T, Sparsity parameter sˆ,
˜
Maximum number of iteration of SOAP T
ADMM: Π ← ADMM(f (ω ))
1 Ψ 1
Set the columns of Uinit to be the top d leading eigenvectors of Π
1
U˜(0) ← Truncate(Uinit,sˆ), Uinit ← Thin-QR(U˜(0))
Uˆ(0) ← SOAP(f (ω ),Uinit)
Ψ 1
for ℓ ∈ {1,...,n/2−1} do
ˆ ˆ
U(ω ) ← SOAP(f (ω ),U(ω ))
ℓ+1 Ψ ℓ+1 ℓ
end
Output: {Uˆ (ω )}n/2
ℓ ℓ=1
A.2 Localization Parameter Selection
For selecting the localization parameter η, we propose to use information criteria based
on the Whittle likelihood, or the large sample complex Gaussian distribution of the pe-
riodogram d (ω ) →D Nc[0,f(ω )]. This is done given d and initially without smoothing
X ℓ ℓ
θ = 0. It can be done iteratively after updating other parameters, simulations suggest
that it is robust to the selection of s and θ. We apply the LSPCA algorithm to obtain the
ˆ
eignevectors U (ω), j = 1,...,d, which are used to compute the estimator of the spectrum
j
of the d-dimensional principle series
ˆ f (ω ) = f (ω )Uˆ (ω )Uˆ (ω )† +···+f (ω )Uˆ (ω )Uˆ (ω )†,
ϑ ℓ Ψ ℓ 1 ℓ 1 ℓ Ψ ℓ d ℓ d ℓ
39ˇ ˇ
Let L = {ℓ ,...,ℓ } ∈ L := {1,2,...,n/2} be the index set of all fundamental frequen-
κ (1) (κ)
cies that are selected to be above a threshold given a localization parameter η. Our desire
to to select η such that the power from the principle series that are not distinguishable
from average power from the residual series are not maintained. We estimate average power
in the residual series ϵ(t) as
Σˆ
=
(cid:80)
d (ω )d (ω
)†/(cid:0) |Lˇ |−|Lˇ |(cid:1)
, and consider the
ℓ∈Lˇ/Lˇ
κ
X ℓ X ℓ κ
estimated spectral density matrices where Gˆ = ˆ f(d)(ω )+Σˆ for ℓ ∈ Lˇ , and Σˆ for ℓ ∈/ Lˇ .
ℓ ϑ ℓ κ ℓ κ
The log-Whittle likelihood estimated by
n/2
(cid:88)
log(L) = − {plogπ +log|Gˆ |+[d (ω )][Gˆ ]−1[d (ω )]†},
ℓ X ℓ ℓ X ℓ
ℓ=1
which we use to define standard information criteria for η including AIC = −2log(L) +
2|L |, AICc = −2log(L)+2|L |+
2|LK|2+2|LK|,
and BIC = −2log(L)+log(n)|L |. The
K K n−|LK|−1 K
localization parameter η is selected to minimizing an information criteria.
A.3 Sparsity Parameter Selection
For selecting the sparsity level of the underlying process, we propose to use k-folds cross
validation, with the Mahalanobis distance to evaluate the performance of fitted model in
the validation step. The procedure splits the data into k blocks, or folds, of length n/k, and
define the time series form the rth fold as X(r)(t) = X[t−(r−1)n/k], t = (r−1)n/k+1,
r = 1,...,k. To make sure fundamental frequencies in the training and validation step
match, n should be divisible by k. In addition, let f(1)(ω),...,f(k)(ω) be the estimated
Ψ Ψ
spectral density matrices obtained from each partition. We consider
1 (cid:88)
f(−r)(ω) = f(j)(ω) (18)
Ψ r−1 Ψ
j̸=r
40as an estimate of the spectral density matrix f(ω) removing data from the rth fold. We
then define the rank-d principal subspace spectrum estimate ˆ f(r), average power from the
residual series
Σˆ(r),
and spectral matrix estimate
Gˆ(r)
for data outside of the rth fold in a
ℓ
manner analogous to the definitions in Section A.2 using all data.
We then consider the Mahalbanois distance between the discrete Fourier transform of
the data from the rth fold from the estimated spectral matrix from the rest of the data at
the ℓth frequency
D[d (ω )] = d (ω )[Gˆ(r)]−1d (ω )†,
X(r) ℓ X(r) ℓ ℓ X(r) ℓ
and average over folds and frequencies to estimate he average Mahalbanois distance
k n/(2k)
1 (cid:88) (cid:88)
¯
D(s) = D[d (ω )]. (19)
k
X(r) ℓ
1=1 ℓ=1
We select the sparsity level sˆ that minimizes this distance
¯
sˆ= argminD(s). (20)
s
A.4 Smoothing Parameter Selection
We select the smoothing parameter θ using k-folds cross validation, with the Mahalanobis
distance to evaluate the performance of fitted model in the validation step. The procedure
is similar to the procedure for the selection of the sparsity, but considers a fixed value of
sparsity and varying levels of θ.
41Appendix B Proofs
Appendix B is devoted to the proof of the Theorems presented in this paper. We first
present the model and the assumptions under which we developed the theory in Section
B.1. This is followed by the proof of the main theoretical results provided in section B.2.
Section B.2.1 contains a proof for Proposition (3.1)). In Section B.2.2 we present a proof
for Theorem (3) part (I). Proof of part (II) of Theorem (3) is presented in Section B.2.3.
Proofs of the main results follow from the Theorems and Lemmas that are presented in
Section B.3 (Preliminary Theorems and Lemmas). Appendix B is concluded with some
background Definitions and Lemmas.
B.1 Model Assumptions
letM (f,d,s∗)betheclassofp-dimensionalstationarytimeseries{X(t) : t ∈ Z}satisfying
d
the following assumptions.
Assumption 1. For all ω ∈ [0,1), the d-dimensional principal subspace of f(ω) is s∗-
sparse.
Assumption 2. There exists constants c and γ ≥ 1 such that for all h ≥ 1, the α-mixing
1 1
coefficient satisfies
α(h) ≤ exp{−c hγ1} (21)
1
Assumption 3. There exists positive constants c and γ such that for all v ∈ Sp−1(C)
2 2
and all λ ≥ 0, we have
P(|v∗X(t)| ≥ λ) ≤ 2exp{−c λγ2}, for all t ∈ Z, (22)
2
42where Sp−1(C) is the unit ball of dimension p.
Assumption 4. Define γ via 1 = 1 + 2 , where γ and γ are given in assumptions (2)
γ γ1 γ2 1 2
and (3). We assume that γ < 1.
B.2 Proof of the Main Results
B.2.1 Proof of Proposition (3.1)
Proof. We prove the claim by induction. First note that the objective function is increasing
in β ‘s and thus the maximum is attained at
(cid:80)K
β = η.
j ℓ=1 ℓ
WhenK = 2,andη = 1,orderh ,h indecreasingorderash ,h andtheircorresponding
1 2 (1) (2)
coefficients as β and β . Note that β +β = 1 and
(1) (2) 1 2
h = β h +(1−β )h > β h +(1−β )h = β h +β h .
(1) (1) (1) (1) (1) (1) (1) (1) (2) (1) (1) (2) (2)
For any η < K assume that the claim of the proposition hols for any h ,...,h ∈ R+.
1 K
Since
(cid:80)K
β = η and 0 ≤ β ≤ 1,ℓ = 1,...,K, by the induction hypothesis we have
ℓ=1 ℓ ℓ
η K−1
(cid:88) (cid:88)
h > β h +(β +β )h
(j) (j) (j) (K) (K+1) (K)
j=1 j=1
K+1
(cid:88)
> β h .
(j) (j)
j=1
The second inequality holds since h ≥ h . This shows that for K +1 the optimum is
(K) K+1
attained at β = ··· = β = 1,β = ··· = β = β = 0 hence, by induction, the
(1) (η) (η+1) (K) (K+1)
claim follows for any K ∈ N. This shows that when η is fixed, for any K the conclusion of
the theorem holds.
Now, we show that the result holds for any η. For K = 2 the result is trivial. For any
43K ≥ 3 set η = 1, then
(1−β )h = (β +β +···+β )h ≥ β h +β h +···+β h (23)
(1) (1) (2) (3) (K) (1) (2) (2) (3) (3) (K) (K)
showing that h ≥ β h + β h + ··· + β h , for any 0 ≥ β ,...,β ≥ 1 such
(1) (1) (1) (2) (2) (K) (K) 1 K
that β +···+β = 1. Suppose the conclusion of the theorem holds for an η ≤ K we show
1 K
that the conclusion also holds for η +1.
For η + 1 By the assumptions of the theorem, β + ··· + β = η + 1, therefore,
(1) (K)
η+1 ≥ β +···+β ≥ η. Let 1 ≥ α := β +···+β −η ≥ 0. Note that β +α = 1.
(2) (K) (2) (K) (1)
Since the for K = 2,η = 1 the conclusion of the theorem holds, we have
h ≥ β h +α(h +···+h ) (24)
(1) (1) (1) (2) (K)
˜ ˜ ˜ ˜
Inaddition,foranyβ ,...,β suchthatβ +···+β = η,bytheindicationhypothesis,
(2) (K) (2) (K)
we have
˜ ˜
h +···+h ≥ β h ,...,β h . (25)
(2) (η+1) (2) (2) (K) (K)
˜ ˜ ˜ ˜
In particular, for any choice of 0 ≤ β ,...,β ≤ 1 such that β +···+β = η and
(2) (K) (2) (K)
˜ ˜
β +···+β +α = β +β +···+β
(2) (K) (2) (3) (K)
we conclude that
˜ ˜
h +h +···+h ≥ β h +(β +α)h ,...,(β +α)h (26)
(1) (2) (η+1) (1) (1) (2) (2) (K) (K)
≥ β h +β h +···+β h . (27)
(1) (1) (2) (2) (K) (K)
44Hence, by indication, the result follows for any K and for any η ≤ K.
B.2.2 Proof of Theorem (3) part (I)
Proof. Let U(t) and U∗ be orthonormal matrices whose columns span U(t)(ω ) and U∗(ω ),
ℓ ℓ 1 1
respectively. In addition, let Π(t) = U(t)[U(t)]† and Π∗ = U∗[U∗]† be the corresponding
ℓ ℓ ℓ ℓ ℓ ℓ
projection matrices. Note that
D(U(t)(ω ),U∗(ω )) = ∥Π(t) −Π∗∥
1 1 ℓ ℓ F
= ∥(ℜ(U(t))+iℑ(U(t)))(ℜ([U(t)]†)−iℑ([U(t)]†))
ℓ ℓ ℓ ℓ
−(ℜ(U∗)+iℑ(U∗))(ℜ([U∗]†)−iℑ([U∗]†)∥
ℓ ℓ ℓ ℓ F
= ∥ℜ(U(t))ℜ([U(t)]†)−ℜ(U∗)ℜ([U∗]†)+ℑ(U(t))ℑ([U(t)]†)−ℑ(U∗)ℑ([U∗]†)
ℓ ℓ ℓ ℓ ℓ ℓ ℓ ℓ
+i{−ℜ(U(t))ℑ([U(t)]†)+ℜ(U∗)ℑ([U∗]†)+ℑ(U(t))ℜ([U(t)]†)−ℑ(U∗)ℜ([U∗]†)}∥
ℓ ℓ ℓ ℓ ℓ ℓ ℓ ℓ F
≤ ∥ℜ(U(t))ℜ([U(t)]†)−ℜ(U∗)ℜ([U∗]†)∥ +∥ℑ(U(t))ℑ([U(t)]†)−ℑ(U∗)ℑ([U∗]†)∥
ℓ ℓ ℓ ℓ F ℓ ℓ ℓ ℓ F
+∥ℜ(U(t))ℑ([U(t)]†)−ℜ(U∗)ℑ([U∗]†)∥ +∥ℑ(U(t))ℜ([U(t)]†)−ℑ(U∗)ℜ([U∗]†)∥
ℓ ℓ ℓ ℓ F ℓ ℓ ℓ ℓ F
(cid:13)    (cid:13)
( ≤i) 2(cid:13) (cid:13)
(cid:13) (cid:13)
ℜ(U( ℓt))

(cid:20)
ℜ([U( ℓt)]†) ℑ([U(
ℓt)]†)(cid:21)
−
ℜ(U∗ ℓ)

(cid:20)
ℜ([U∗ ℓ]†) ℑ([U∗
ℓ]†)(cid:21)(cid:13) (cid:13)
(cid:13)
(cid:13)
(cid:13) ℑ(U(t)) ℑ(U∗) (cid:13)
(cid:13) ℓ ℓ (cid:13)
F
= 2D([U(t)(ω )](R),[U∗(ω )](R))
1 1
Note that (i) holds since
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) A B (cid:13) (cid:113)
(cid:13) (cid:13) = ∥A∥2 +∥B∥2 +∥B†∥2 +∥D∥2
(cid:13) (cid:13) F F F F
(cid:13) B† D (cid:13)
(cid:13) (cid:13)
F
and
(cid:113)
∥A∥ +∥B∥ +∥C∥ +∥D∥ ≤ 2 ∥A∥2 +∥B∥2 +∥B†∥2 +∥D∥2.
F F F F F F F F
45Thus the result follows from Theorem (7) with
C˜˜′
=
2C˜′
and
C˜˜′′
=
2C˜′′.
B.2.3 Proof of Theorem (3) part (II)
Remark 4 (Notation). We denote the estimator of the spectral density matrix considered
in the theorem as ˆ f(θ) which belongs to the general class of estimators f .
M Ψ
ˆ f(θ) = (1−θ)ˆ f (ω )+θ Πˆ ˆ f (ω )Πˆ
M M ℓ+1 ℓ M ℓ+1 ℓ
M
(cid:88)
ˆ ˆ
f = R exp{−2πiωt}
M t
t=−M
n−t
1 (cid:88)
Rˆ = X(k +t)X(t)†
t
n
k=1
ˆ
where Π is the estimated d-dimensional principal subspace of f(ω ) obtained from the
ℓ ℓ
LSPCA algorithm.
Let U∗ be the d-dimensional principal subspace of f(ω ) and U∗ be an orthonormal
ℓ ℓ ℓ
matrixsuchthatitscolumnsspanU∗. Inaddition, letS∗ betherow-supportofU∗ andI ⊆
ℓ ℓ
{1,...,p} be an index set. Letˆ f(θ)(ω ,I) be the restriction ofˆ f(θ)(ω ) onto the columns and
M ℓ M ℓ
ˆ
rows indexed by I and U (I) be the orthonormal matrix with columns consisting of the top
ℓ
d leading eigenvectors ofˆ f(θ)(ω ,I). Also, let Uˆ (ω ,I) be the space spanned by the columns
M ℓ ℓ
of Uˆ (I). By the QR decomposition step in the Algorithm SOAP, ˆ f (ω ,I)U(t)(ω ) =
ℓ M ℓ+1 ℓ+1
V(t+1)(ω )R(t+1)(ω ). Denote the column-space of V(t+1)(ω ) by V(t+1)(ω ). In
ℓ+1 1 ℓ+1 ℓ+1 ℓ+1
addition, let ∥A∥ = max{v†Av : ∥v∥ = 1}, the (p,q)-norm of a matrix A, ∥A∥ be the
op 2 p,q
ℓ norm of the ℓ norms of the rows of A, and
q p
∥ˆ f(θ)(ω )−f(ω )∥ := max ∥v†([ˆ f(θ)(ω )−f(ω )] )v∥ . (28)
M ℓ+1 ℓ+1 op,|I| supp M ℓ+1 ℓ+1 I 2
∥v∥2=1,| (v)|≤I
46Throughout the rest of the document, we denote the eigenvalues, in decreasing order, of a
p×p Hermitian matrix H by λ (H),...,λ (H) and will use λ (ω) to denote λ (f(ω)), for
1 p k k
k = 1,...,p.
Lemma 1. Under the conditions of Theorem (6), we have
∥ˆ f(θ)(ω )−f(ω )∥ ≤ ∥ˆ f (ω )−f(ω )∥
M ℓ+1 ℓ+1 op,|I| M ℓ+1 ℓ+1 op,|I|
(cid:104) (cid:105)
+2θλ (ω ) D(U∗,U∗ )+D(U∗,Uˆ (I)) +θλ (ω )
1 ℓ+1 ℓ ℓ+1 ℓ ℓ k+1 ℓ+1
(29)
(cid:32) (cid:114) (cid:33)
s∗log(p)
≤ exp(−c M(n))∨M(n)
0
n
(cid:104) (cid:105)
+2θλ (ω ) D(U∗,U∗ )+D(U∗,Uˆ (I)) +θλ (ω ),
1 ℓ+1 ℓ ℓ+1 ℓ ℓ k+1 ℓ+1
(30)
where the second inequality holds with high probability.
Proof. Let
f(θ)(ω ) = (1−θ)f(ω )+θΠ f(ω )Π . (31)
ℓ+1 ℓ ℓ ℓ+1 ℓ
Note that
ˆ f(θ)(ω )−f(ω ) = ˆ f(θ)(ω )−f(θ)(ω )+f(θ)(ω )−f(ω ).
M ℓ+1 ℓ+1 M ℓ+1 ℓ+1 ℓ+1 ℓ+1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(I) (II)
Note that
(cid:104) (cid:105)
ˆ ˆ ˆ ˆ
(I) = (1−θ)(f (ω )−f(ω ))+θ Π f (ω )Π −Π f(ω )Π
M ℓ+1 ℓ+1 ℓ M ℓ+1 ℓ ℓ ℓ+1 ℓ
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
(I.1)
(I.2)
47(cid:104) (cid:105) (cid:16) (cid:17) (cid:16) (cid:17)
ˆ ˆ ˆ ˆ ˆ ˆ
(I.2) = Π f (ω )−f(ω ) Π +Π f(ω ) Π −Π + Π −Π f(ω )Π .
ℓ M ℓ+1 ℓ+1 ℓ ℓ ℓ+1 ℓ ℓ ℓ ℓ ℓ+1 ℓ
By (I.1) and (I.2)
∥ˆ f(θ)(ω )−f(θ)(ω )∥ ≤ ∥ˆ f (ω )−f(ω )∥ +2θλ (ω )D(U∗,Uˆ (I)) (32)
M ℓ+1 ℓ+1 op,|I| M ℓ+1 ℓ+1 op,|I| 1 ℓ+1 ℓ ℓ
Also,
(II) = θ[Π f(ω )Π −f(ω )]
ℓ ℓ+1 ℓ ℓ+1
(cid:2) (cid:3)
= θ Π f(ω )Π −Π f(ω )Π +Π f(ω )Π −Π f(ω )Π −Π⊥ f(ω )Π⊥
ℓ ℓ+1 ℓ ℓ ℓ+1 ℓ+1 ℓ ℓ+1 ℓ+1 ℓ+1 ℓ+1 ℓ+1 ℓ+1 ℓ+1 ℓ+1
(cid:2) (cid:3)
= θ Π f(ω )(Π −Π )+(Π −Π )f(ω )Π −Π⊥ f(ω )Π⊥ .
ℓ ℓ+1 ℓ ℓ+1 ℓ ℓ+1 ℓ+1 ℓ+1 ℓ+1 ℓ+1 ℓ+1
Thus,
∥f(θ)(ω )−f(ω )∥ ≤ 2θλ (ω )D(U∗,U∗ )+θλ (ω ).
ℓ+1 ℓ+1 op,|I| 1 ℓ+1 ℓ ℓ+1 d+1 ℓ+1
Hence,
∥ˆ f(θ)(ω )−f(ω )∥ ≤ ∥ˆ f (ω )−f(ω )∥ (33)
M ℓ+1 ℓ+1 2,|I| M ℓ+1 ℓ+1 2,|I|
(cid:104) (cid:105)
+2θλ (f(ω )) D(U∗,U∗ )+D(U∗,Uˆ (I)) +θλ (ω )
1 ℓ+1 ℓ ℓ+1 ℓ ℓ k+1 ℓ+1
(34)
(cid:32) (cid:114) (cid:33)
s∗log(p)
≤ exp(−c M(n))∨M(n) (35)
0
n
(cid:104) (cid:105)
+2θλ (f(ω )) D(U∗,U∗ )+D(U∗,Uˆ (I)) +θλ (ω ).
1 ℓ+1 ℓ ℓ+1 ℓ ℓ k+1 ℓ+1
(36)
Note the second inequality is followed by Theorem (6).
48Lemma 2. Under the same conditions of Theorem (6), with high probability, we have
E
D(U∗ ,Uˆ (I)) ≤ 1 =: ∆ (I), (37)
ℓ+1 ℓ+1 E ℓ+1
2
where
(cid:34)(cid:32) (cid:114) (cid:33)
√ s∗log(p)
E = 2d exp(−c M(n))∨M(n)
1 0
n
(cid:104) (cid:105) (cid:105)
+2θλ (f(ω )) D(U∗,U∗ )+D(U∗,Uˆ (I)) +θλ (ω )
1 ℓ+1 ℓ ℓ+1 ℓ ℓ k+1 ℓ+1
1
E = [λ (ω )−(1−θ)λ (ω )]−2θλ (ω
)D(U∗,Uˆ
(I))
2 2 d ℓ+1 d+1 ℓ+1 1 ℓ+1 ℓ ℓ
Proof. Let [U∗]⊥ be the orthogonal matrix whose columns are the eigenvectors correspond-
ℓ
ing to λ (f(ω )),...,λ (f(ω )) and let [Uˆ ]⊥ be the orthogonal matrix whose columns are
d+1 ℓ p ℓ ℓ
the eigenvectors corresponding to λ (ˆ f(θ)(ω ,I)),...,λ (ˆ f(θ)(ω ,I)). The following holds
d+1 M ℓ p M ℓ
1.
f(ω )U∗ = U∗ Λ (f(ω ))
ℓ+1 ℓ+1 ℓ+1 0 ℓ+1
ˆ f(θ)(ω ,I)[Uˆ ]⊥ = [Uˆ ]⊥Λ (ˆ f(θ)(ω ,I)),
M ℓ+1 ℓ+1 ℓ+1 1 M ℓ+1
where
Λ (f(ω )) = diag{λ (f(ω )),...,λ (f(ω ))}
0 ℓ+1 d+1 ℓ+1 p ℓ+1
Λ (ˆ f(θ)(ω ,I)) = diag{λ (ˆ f(θ)(ω ,I)),...,λ (ˆ f(θ)(ω ,I))}
1 M ℓ+1 d+1 M ℓ+1 p M ℓ+1
492.
[U∗ ]†[ˆ f(θ)(ω ,I))−f(ω )][Uˆ ]⊥ = [U∗ ]†[ˆ f(θ)(ω ,I))][Uˆ ]⊥ −[f(ω )U∗ ]†[Uˆ ]⊥
ℓ+1 M ℓ+1 ℓ+1 ℓ+1 ℓ+1 M ℓ+1 ℓ+1 ℓ+1 ℓ+1 ℓ+1
= [U∗ ]†[Uˆ ]⊥Λ (ˆ f(θ)(ω ,I))
ℓ+1 ℓ+1 1 M ℓ+1
−Λ (f(ω ))[U∗ ]†[Uˆ ]⊥.
0 ℓ+1 ℓ+1 ℓ+1
3.
∥[U∗ ]†[Uˆ ]⊥∥ = ∥Λ−1(f(ω ))Λ (f(ω ))[U∗ ]†[Uˆ ]⊥∥
ℓ+1 ℓ+1 op 0 ℓ+1 0 ℓ+1 ℓ+1 ℓ+1 op
≤ ∥Λ−1(f(ω ))∥ ∥Λ (f(ω ))[U∗ ]†[Uˆ ]⊥∥
0 ℓ+1 op 0 ℓ+1 ℓ+1 ℓ+1 op
1
≤ ∥Λ (f(ω ))[U∗ ]†[Uˆ ]⊥∥ .
λ (ω ) 0 ℓ+1 ℓ+1 ℓ+1 op
d ℓ+1
4. For any sub-multiplicative matrix norm,
∥[U∗ ]†[Uˆ ]⊥Λ (ˆ f(θ)(ω ,I))∥ ≤ ∥[U∗ ]†[Uˆ ]⊥∥∥Λ (ˆ f(θ)(ω ,I))∥.
ℓ+1 ℓ+1 1 M ℓ+1 ℓ+1 ℓ+1 1 M ℓ+1
5. Since the row support of [Uˆ ]⊥ is I,
ℓ+1
[U∗ ]†[ˆ f(θ)(ω ,I))−f(ω )][Uˆ ]⊥ = [U∗ ]†[ˆ f(θ)(ω ,I))−f(ω )] [Uˆ ]⊥.
ℓ+1 M ℓ+1 ℓ+1 ℓ+1 ℓ+1 M ℓ+1 ℓ+1 I ℓ+1
6.
∥[U∗ ]†[ˆ f(θ)(ω ,I))−f(ω )][Uˆ ]⊥∥ ≤ ∥[ˆ f(θ)(ω ,I))−f(ω )] ∥
ℓ+1 M ℓ+1 ℓ+1 ℓ+1 op M ℓ+1 ℓ+1 I op
507.
∥[ˆ f(θ)(ω ,I)−f(ω )] ∥ = max ∥v†([ˆ f(θ)(ω )−f(ω )] )v∥
M ℓ+1 ℓ+1 I op M ℓ+1 ℓ+1 I 2
∥v∥2=1
= max ∥v†([ˆ f(θ)(ω )−f(ω )] )v∥
supp M ℓ+1 ℓ+1 I 2
∥v∥2=1, (v)⊆I
≤ max ∥v†([ˆ f(θ)(ω )−f(ω )] )v∥
supp M ℓ+1 ℓ+1 I 2
∥v∥2=1,| (v)|≤I
= ∥ˆ f(θ)(ω )−f(ω )∥
M ℓ+1 ℓ+1 op,|I|
8. Following 2, 3, and 4,
∥[U∗ ]†[ˆ f(θ)(ω ,I))−f(ω )][Uˆ ]⊥∥ ≥ ∥Λ (f(ω ))[U∗ ]†[Uˆ ]⊥∥
ℓ+1 M ℓ+1 ℓ+1 ℓ+1 op 0 ℓ+1 ℓ+1 ℓ+1 op
−∥[U∗ ]†[Uˆ ]⊥Λ (ˆ f(θ)(ω ,I))∥
ℓ+1 ℓ+1 1 M ℓ+1 op
≥ λ (f(ω ))∥[U∗ ]†[Uˆ ]⊥∥
d ℓ+1 ℓ+1 ℓ+1 op
−λ (ˆ f(θ)(ω ,I))∥[U∗ ]†[Uˆ ]⊥∥
d+1 M ℓ+1 ℓ+1 ℓ+1 op
= [λ (f(ω ))−λ (ˆ f(θ)(ω ,I))]×
d ℓ+1 d+1 M ℓ+1
∥[U∗ ]†[Uˆ ]⊥∥ .
ℓ+1 ℓ+1 op
9. Following 7 and 8,
∥ˆ f(θ)(ω ,I))−f(ω )∥ ≥ |λ (f(ω ))−λ (ˆ f(θ)(ω ,I))|∥[U∗ ]†[Uˆ ]⊥∥ .
M ℓ+1 ℓ+1 op,|I| d ℓ+1 d+1 M ℓ+1 ℓ+1 ℓ+1 op
10.
√ √
D(U∗ ,Uˆ (I)) = 2∥[U∗ ]†[Uˆ ]⊥∥ ≤ 2d∥[U∗ ]†[Uˆ ]⊥∥ .
ℓ+1 ℓ+1 ℓ+1 ℓ+1 F ℓ+1 ℓ+1 op
5111. Thus
√
2d∥ˆ f(θ)(ω ,I))−f(ω )∥
D(U∗ ,Uˆ (I)) ≤ M ℓ+1 ℓ+1 op,|I| .
ℓ+1 ℓ+1 λ (f(ω ))−λ (ˆ f(θ)(ω ,I))
d ℓ+1 d+1 M ℓ+1
Note that,
λ (ˆ f(θ)(ω ,I)) ≤ λ (f(θ)(ω ,I))+λ (ˆ f(θ)(ω ,I)−f(θ)(ω ,I))
d+1 M ℓ+1 d+1 M ℓ+1 1 M ℓ+1 M ℓ+1
λ (f(θ)(ω ,I)) ≤ (1−θ)λ (f(ω ))+θλ (Π f(ω )Π )
d+1 M ℓ+1 d+1 ℓ+1 d+1 ℓ ℓ+1 ℓ
= (1−θ)λ (f(ω ))
d+1 ℓ+1
λ (ˆ f(θ)(ω ,I)−f(θ)(ω ,I)) ≤ ∥ˆ f (ω )−f(ω )∥ +2θλ (f(ω ))D(U∗,Uˆ (I)),
1 M ℓ+1 M ℓ+1 M ℓ+1 ℓ+1 op,|I| 1 ℓ+1 ℓ ℓ
where the last inequality follows from (32). Thus,
λ (ˆ f(θ)(ω ,I)) ≤ (1−θ)λ (f(ω ))+∥ˆ f (ω )−f(ω )∥ +2θλ (f(ω ))D(U∗,Uˆ (I))
d+1 M ℓ+1 d+1 ℓ+1 M ℓ+1 ℓ+1 op,|I| 1 ℓ+1 ℓ ℓ
and consequently
λ (f(ω ))−λ (ˆ f(θ)(ω ,I)) ≥ λ (f(ω ))−(1−θ)λ (f(ω ))
d ℓ+1 d+1 M ℓ d ℓ+1 d+1 ℓ+1
−∥ˆ
f (ω )−f(ω )∥ −2θλ (f(ω
))D(U∗,Uˆ
(I)).
M ℓ+1 ℓ+1 op,|I| 1 ℓ+1 ℓ ℓ
If we choose M large enough such that
λ (f(ω ))−(1−θ)λ (f(ω ))
ˆ d ℓ+1 d+1 ℓ+1
∥f (ω )−f(ω )∥ ≤ ,
M ℓ+1 ℓ+1 op,|I|
2
then
λ (f(ω ))−(1−θ)λ (f(ω ))
λ (f(ω ))−λ (ˆ f(θ)(ω ,I)) ≥ d ℓ+1 d+1 ℓ+1 −2θλ (f(ω ))D(U∗,Uˆ (I)).
d ℓ+1 d+1 M ℓ 2 1 ℓ+1 ℓ ℓ
52Hence,
√
2d∥ˆ f(θ)(ω ,I))−f(ω )∥
D(U∗ ,Uˆ (I)) ≤ M ℓ ℓ+1 op,|I|
ℓ+1 ℓ+1 1 [λ (f(ω ))−(1−θ)λ (f(ω ))]−2θλ (f(ω ))D(U∗,Uˆ (I))
2 d ℓ+1 d+1 ℓ+1 1 ℓ+1 ℓ ℓ
The result follows from Lemma (1).
Lemma 3. Let I be a superset of the row-support of U(t), and let N ∈ N be such that for
1
each n > N and for all ℓ = 1,...,n/2
1
(cid:104) (cid:105)
∥ˆ f (ω )−f(ω )∥ +2θλ (f(ω )) D(U∗,U∗ )+D(U∗,Uˆ (I)) +θλ (ω )
M ℓ+1 ℓ+1 op,|I| 1 ℓ+1 ℓ ℓ+1 ℓ ℓ d+1 ℓ+1
≤ inf(λ (ω)−λ (ω)))/4.
d d+1
ω
(38)
In addition, suppose n,N ,p,M are large enough so that
1
(cid:104) (cid:105)
λ (ω )−∥ˆ f (ω )−f(ω )∥ −2θλ (f(ω )) D(U∗,U∗ )+D(U∗,Uˆ (I)) −θλ (ω ) > 0.
d ℓ+1 M ℓ+1 ℓ+1 op,|I| 1 ℓ+1 ℓ ℓ+1 ℓ ℓ d+1 ℓ+1
Let
3λ (ω )+λ (ω )
d+1 ℓ+1 d ℓ+1
γ(ω ) = .
ℓ+1
λ (ω )+3λ (ω )
d+1 ℓ+1 d ℓ+1
√
If D(U(t)(ω ),Uˆ (ω ,I)) < 2, then we have
ℓ+1 ℓ+1
D(U(t)(ω ),Uˆ (ω ,I))
D(V(t+1)(ω ),Uˆ (ω ,I)) ≤ ℓ+1 ℓ+1 .γ(ω ). (39)
ℓ+1 ℓ+1 (cid:113) ℓ+1
1−D(U(t)(ω ),Uˆ (ω ,I))2/(2d)
ℓ+1 ℓ+1
Proof. Recall ˆ f(θ)(ω ,I)U(t)(ω ) = V(t+1)(ω )R(t+1)(ω ) and ˆ f(θ)(ω ,I) is the re-
ℓ+1 ℓ+1 ℓ+1 1 ℓ+1 M ℓ+1
strictionofˆ f(θ)(ω )ontherowsandcolumnsindexedbyI. WedenoteUˆ (I)⊥ tobethe
M ℓ+1 ℓ+1
orthogonalmatrixwhosecolumnsspanthesubspacecorrespondingtoλ (ˆ f(θ)(ω ,I)),...,λ (ˆ f(θ)(ω ,I))
d+1 M ℓ+1 p M ℓ+1
53Note that
√
D(V(t+1)(ω ),Uˆ (ω ,I))/ 2 = ∥[Uˆ (I)⊥]†V(t+1)∥ =: D
ℓ+1 ℓ+1 ℓ+1 ℓ+1 F
In addition, following (B.21) of Wang et al. (2014)
D ≤ ∥Λ (ˆ f(θ)(ω ,I))∥ .∥[Uˆ (ω ,I)⊥]†U(t) ∥ .∥[[Uˆ (ω ,I)⊥]†U(t) ]−1∥
1 M ℓ+1 op ℓ ℓ+1 F ℓ ℓ+1 op
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(i) (ii) (iii)
(40)
.∥[Λ (ˆ f(θ)(ω ,I))]−1∥ .∥[Uˆ (I)]†V(t+1)∥
0 M ℓ+1 op ℓ+1 ℓ+1 op
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(iv) (v)
Now we analyze each term.
(i)
∥Λ (ˆ f(θ)(ω ,I))∥ ≤ λ (f(ω ))+∥ˆ f(θ)(ω −f(ω )∥
1 M ℓ+1 op d+1 ℓ+1 M ℓ+1 ℓ+1 op,|I|
λ (f(ω ))−λ (f(ω ))
d ℓ+1 d+1 ℓ+1
≤ λ (f(ω ))+
d+1 ℓ+1
4
λ (f(ω ))+3λ (f(ω ))
d ℓ+1 d+1 ℓ+1
=
4
(ii),(iii)
√
∥[Uˆ (ω ,I)⊥]†U(t) ∥ = D(U(t)(ω ),Uˆ (ω ,I))/ 2 (41)
ℓ ℓ+1 F ℓ+1 ℓ+1
1
∥[[Uˆ (ω ,I)⊥]†U(t) ]−1∥ ≤ (42)
ℓ ℓ+1 op (cid:113)
1−D(U(t)(ω ),Uˆ (ω ,I))2/(2d)
ℓ+1 ℓ+1
54(iv)
λ (ˆ f(θ)(ω ,I)) ≥ λ (f(ω ))−∥ˆ f(θ)(ω )−f(ω )∥ (43)
d M ℓ+1 d ℓ+1 M ℓ+1 ℓ+1 op,|I|
ˆ
≥ λ (ω )−∥f (ω )−f(ω )∥ (44)
d ℓ+1 M ℓ+1 ℓ+1 op,|I|
(cid:104) (cid:105)
−2θλ (f(ω )) D(U∗,U∗ )+D(U∗,Uˆ (I)) −θλ (ω )
1 ℓ+1 ℓ ℓ+1 ℓ ℓ d+1 ℓ+1
(45)
Since by the assumption, p,n,M are large enough so that the right hand side of the
second inequality is positive, we have
1 1
∥[Λ (fˆ(θ)(ω ,I))]−1∥ ≤ ≤
0 M ℓ+1 op λ (ˆ f(θ)(ω ,I)) λ (f(ω ))−∥ˆ f(θ)(ω −f(ω )∥
d M ℓ+1 d ℓ+1 M ℓ+1 ℓ+1 op,|I|
1
≤
λ (f(ω ))− λ d(f(ω ℓ+1))−λ d+1(f(ω ℓ+1))
d ℓ+1 4
4
=
3λ (f(ω ))+λ (f(ω ))
d ℓ+1 d+1 ℓ+1
(v)
∥[Uˆ (I)]†V(t+1)∥ ≤ 1.
ℓ+1 ℓ+1 op
The above argument together with (38) implies (39).
Lemma 4. Assume that
4d (cid:113) √
sˆ= Cmax{[ ],1}.s∗, and D(U(t)(ω ),U∗(ω )) ≤ min{ 2d(1−γ1/2), 2/2},
(γ−1/2 −1)2 ℓ+1 ℓ+1
where C ≥ 1 is an integer constant. Under conditions of Theorem (6), we can choose
55N ∈ N such that for all n > N , ∆(2sˆ) ≤ 1/24. Then, for all n > N ,
3 3 3
D(U(t+1)(ω ),U∗(ω )) ≤ γ1/4.D(U(t)(ω ),U∗(ω ))+3γ1/2∆(2sˆ),
ℓ+1 ℓ+1 ℓ+1 ℓ+1
where
Υ
∆(s) := sup .
1 [λ (ω )−(1−θ)λ (ω )]−2θλ (ω )α
ω ℓ 2 d ℓ+1 d+1 ℓ+1 1 ℓ+1
and
(cid:34)(cid:32) (cid:114) (cid:33)
√ s∗log(p)
Υ = 2d exp(−c M(n))∨M(n)
0
n
(cid:2) (cid:3) (cid:3)
+2θλ (ω ) D(U∗,U∗ )+α) +θλ (ω )
1 ℓ+1 ℓ ℓ+1 d+1 ℓ+1
√ (cid:32) (cid:114) (cid:33)
c 2d s∗log(p)
1
α = sup exp(−c M(n))∨M(n)
0
λ (ω )−λ (ω ) n
ω ℓ d ℓ d+1 ℓ
Proof. ProofoftheLemmafollowsalongthesamelinesastheproofofLemma(5.6)ofWang
√
et al. For the arguments to hold, we need D(U∗ ,Uˆ (2sˆ)) ≤ 2/2 and ∆(2sˆ) ≤ 1/24
ℓ+1 ℓ+1
which by Lemma (2) hold, when n,p,M(n) are large enough. We also need to show that
√
D(U(t)(ω ),U∗(ω )) ≤ 2/2 (46)
ℓ+1 ℓ+1
for all ℓ ∈ {1,...,n/2}. Note that, for ℓ = 1, Theorem (3) part (I) guarantees that for suffi-
√
ciently large n,p,M(n), D(U(t)(ω ),U∗(ω )) ≤ 2/2. In addition, Theorem (3), guarantees
1 1
√
that D(U(init)(ω ),U∗(ω )) ≤ 2/2. If we assume (46) holds for ℓ = 3,...,L, we can show
2 2
the result of Theorem (3) holds for ℓ = L, which guarantees D(U(init)(ω ),U∗(ω )) ≤
L+1 L+1
√
2/2 for sufficiently large n,p,M(n).
Theorem 5. Let {X(t) : t = 1,...,n} be a realization of a weakly stationary time series
56that follows M (f,d,s∗) with n > n . Suppose the sparsity parameter sˆ in Algorithm
d min
SOAP is chosen such that
(cid:26)(cid:20) (cid:21) (cid:27)
4d
sˆ= Cmax ,1 .s∗
(γ−1/2 −1)2
for some integer constant C ≥ 1. If the column space Uinit of the initial estimator Uinit of
Algorithm SOAP at each frequency ω satisfies
ℓ
(cid:40)(cid:114) √ (cid:41)
dγ(1−γ1/2) 2γ
D(Uinit(ω ),U∗(ω )) ≤ R = min , ,
ℓ ℓ
2 4
then the iterative sequence {U(t)}∞ obtained for fˆ(θ) satisfies
t=T+1 M
(cid:40)(cid:114) √ (cid:41)
d(1−γ1/2) 2 3γ1/2
D(U(t)(ω ),U∗(ω )) ≤ γ(t−T−1)/4min , + ∆(2sˆ).
ℓ ℓ 2 4 1−γ1/4
with probability at least C′(6p)s∗M exp{−(ns∗log(p))γ/2 } for some constants C′ and C˜ , where
C˜
∆(s) is as defined in (17).
Proof. Proof follows along the same lines as in the proof of Theorem (4.2) in Wang et al.
(2014) with the probability bound derived in Theorem (6) and using previous Lemmas.
Proof of part (II) Theorem (3). Proof follows from part (I) and Theorem (5).
B.3 Preliminary Theorems and Lemmas
Let
∥ˆ
f (ω )−f(ω )∥ := max
∥v′([ˆ
f (ω )−f(ω )] )v∥ .
M ℓ+1 ℓ+1 op,|I| supp M ℓ+1 ℓ+1 I 2
∥v∥2=1,| (v)|≤I
57Below, we adapted the proof technique of Lu et al. (2016) to establish the following con-
centration result.
(cid:113)
Theorem 6. Under the assumptions (2, 3, 4) if M(n) → ∞ and M(n) log(p) → 0 as
n
n → ∞, then
(cid:32) (cid:114) (cid:33)
s∗log(p)
ˆ
sup ∥f (ω)−f (ω)∥ = O exp(−c M(n))∨M(n) (47)
M X op,s∗ P 0
n
ω∈[0,1)
Proof. Let Rˆ = 1 (cid:80)n−t X(k + t)X(t)′,R = E[X(t)X(0)],Yt(k) := v′X(k + t)X(t)′v −
t n k=1 t v
E[v′X(k +t)X(t)′v]. Note that v′(Rˆ −R )v = 1 (cid:80)n−t Yt(k). In addition, by Lemma (6)
t t n k=0 v
√
E[v′X(k + t)X(t)v] ≤ ∥R ∥ ≤ 8 2 and by assumption (2) and the fact that α (m) ≤
t γ2 Y vt
α X(m(t + 1) + 1) for all m ≥ 1, we have α
Yt(n)
≤ exp{−c 1kγ1}. By Assumption (3)
v
and Lemma (7), there exist a constant c′ which only depends on c such that for every
2 2
v ∈ Sp−1(C),t ∈ Z,k ∈ {1,...,n−t},λ ≥ 0, we have
P(cid:0)(cid:12) (cid:12)Yt(k)−E[Yt(k)](cid:12)
(cid:12) ≥
λ(cid:1)
≤ 2exp{−c′λγ2/2}. (48)
v v 2
Thus by Theorem (1.1 of Merlev`ede et al. (2011)), there exists V < ∞ and C ,...,C
1 4
depending on c,γ ,γ such that for all n and λ > 1 , we have
1 2 n−t
(cid:16)(cid:12) (cid:12) (cid:17) nγλγ n2λ2 nλ2 nγ(1−γ)λγ(1−γ)
P (cid:12)v′(Rˆ −R )v(cid:12) ≥ λ ≤ nexp{− }+exp{− }+exp{− exp{ }}.
(cid:12) t t (cid:12) C C (1+nV) C C (log(nλ))γ
1 2 3 4
(49)
We then apply the ε-net type argument. Let N be a 1-net of Sp−1(C)∩B (s∗). Note that,
1/2 2 0
it contains (cid:0)p(cid:1) 6s∗ points. In addition, note that for any Hermitian matrix A, ∥A∥ ≤
s∗ op,s∗
58Cmax |v′Av|. Let ˆ f (ω) = (cid:80)M Rˆ exp{−2πiωt}, then
v∈N 1/2 M t=−M t
(cid:16) (cid:17) (cid:88) (cid:16)(cid:12) (cid:12) (cid:17)
P ∥ˆ f (ω)−f (ω)∥ ≥ Cλ ≤ P (cid:12)v′(ˆ f (ω)−f (ω))v(cid:12) ≥ λ
M X op,s∗ (cid:12) M X (cid:12)
v∈N
1/2
(cid:34) (cid:32)(cid:12) (cid:12) (cid:33)
(cid:18) p(cid:19) (cid:12) (cid:88)M (cid:12) λ
≤ 6s∗ P (cid:12) v′(Rˆ −R )vexp{−2πiωt}(cid:12) ≥
s∗ (cid:12) t t (cid:12) 2
(cid:12) (cid:12)
t=−M
(cid:12) (cid:12) 
(cid:12) (cid:12)
+P (cid:12) (cid:12) (cid:88) v′R tvexp{−2πiωt}(cid:12) (cid:12) ≥ λ .
(cid:12) (cid:12) 2
(cid:12)|t|>M (cid:12)
Note that
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:88) (cid:12) (cid:88)
Q = (cid:12) v′R vexp{−2πiωt}(cid:12) ≤ 2 ∥R exp{−2πiωt}+R exp{2πiωt}∥
1 t t −t op
(cid:12) (cid:12)
(cid:12)|t|>M (cid:12) t>M
Lemma(6) (cid:88)
≤ 2c exp{−c t−γ1} ≤ 2c exp{−c M},
3 4 3 4
t>M
which for large M is smaller than λ/2. Hence the second term is 0. For the first term
(cid:32)(cid:12) (cid:12) (cid:33)
(cid:12) (cid:88)M (cid:12) λ (cid:88)M (cid:18) (cid:12) (cid:12) λ (cid:19)
P (cid:12) v′(Rˆ −R )vexp{−2πiωt}(cid:12) ≥ ≤ P (cid:12)v′(Rˆ −R )v(cid:12) ≥ =: Q2.
(cid:12) t t (cid:12) 2 (cid:12) t t (cid:12) 2(2M +1)
(cid:12) (cid:12)
t=−M t=−M
(cid:113)
Let λ = M s∗log(p). By utilizing (49) We show that (cid:0)p(cid:1) 6s∗Q ≤ (I)+(II)+(III) where
n s∗ 2
(I),(II),(III) are as follows.
(cid:18) p(cid:19) (cid:88)M nγλγ nγλγ
(I) = 6s∗ nexp{− } ≤ C′(6p)s∗Mnexp{− }
s∗ C (4M +2)γ 1 C˜ Mγ
t=0 1 1
nγMγ(s∗)γ/2(log(p))γ/2
≤ C′(6p)s∗Mnexp{− }
1 C˜ nγ/2Mγ
1
nγ/2(s∗log(p))γ/2
=C′(6p)s∗Mnexp{− }
1 ˜
C
1
59(cid:18) p(cid:19) (cid:88)M n2λ2 n2λ2
(II) = 6s∗ exp{− } ≤ C′(6p)s∗M exp{− }
s∗ C (1+nV)(4M +2)2 2 C˜ M2
t=0 2 2
n2M2s∗log(p)
≤ C′(6p)s∗M exp{− }
2 C˜ M2n
2
ns∗log(p)
= C′(6p)s∗M exp{− }
2 ˜
C
2
(cid:18) p(cid:19) (cid:88)M nλ2 nγ(1−γ)λγ(1−γ)
(III) = 6s∗ exp{− exp{ }} ≤ (II)
s∗ C C (log(nλ))γ
3 4
t=0
(cid:113)
If M → ∞ and M
s∗log(p)
→ 0 and n → ∞, then the Q → 0. the upper bound holds for
n 2
all ω ∈ [0,1), therefore
(cid:32) (cid:114) (cid:33)
s∗log(p)
ˆ
sup ∥f (ω)−f (ω)∥ = O exp(−c M(n))∨M(n)
M X op,s∗ P 0
n
ω∈[0,1)
(cid:112)
Lemma 5. Suppose that s∗/sˆ≤ 1 and there exists N ∈ N such that for all n > N and
2 2
all ℓ = 1,...,n/2, D(V(t+1)(ω ),U∗(ω )) ≤ 1. Then, for all n > N and ℓ = 1,...,n/2,
ℓ ℓ 2
(cid:32) (cid:114) (cid:33)
d.s∗
D(U(t+1)(ω ),U∗(ω )) ≤ 1+2 D(V(t+1)(ω ),U∗(ω )).
ℓ ℓ ℓ ℓ
sˆ
Proof. Proof follows along the same lines as in Lemma (5.5) of Wang et al. (2014).
Theorem 7. Let {X(t) : t = 1,...,n} be a realization of a weakly stationary time series
that follows M (f,d,s∗) with n > n and fˆ(R)(ω) be as defined in (10). In addition,
d min M
(cid:112)
let the regularization parameter in (7) be ϱ = Cλ (ω ) log(p)/n for a sufficiently large
1 1
√ √
constant C, and the penalty parameter β in (9) be β = 2p.ϱ/ d. Then the iterative
60sequence of d-dimensional subspace {U(t)(ω )}T satisfies
1 t=1
C˜′λ (ω ) (cid:114) log(p) C˜′′(cid:112) M(n)λ (ω ) (cid:18) d.p2log(p)(cid:19)1/4 1
D(U(t)(ω ),U∗(ω )) ≤ 1 1 .s∗ + 1 1 .√
1 1 (cid:112)
λ (ω )−λ (ω ) n λ (ω )−λ (ω ) n t
d 1 d+1 1 d 1 d+1 1
with high probability, where
C˜′
= 4C a
C˜′′
are constants.
Proof. Proof follows along the same lines as in the proof of Theorem (4.3) of Wang et al.
(cid:112)
(2014) with ϱ = Cλ (ω ) log(p)/n.
1 1
B.4 Background Definitions and Lemmas
Definition 8. Given two σ-algebras A and B, the α-mixing coefficient between A and B,
denoted by α(A,B), is defined as
α(A,B) = sup P(A∩B)−P(A)P(B) (50)
A∈A,B∈B
Definition 9. Given a weakly stationary time series X , the strong-mixing coefficient at
t
lag h is defined as
α(h) = α(σ(X ,t ≤ 0),σ(X ,t ≥ h). (51)
t t
Lemma 6. Let R = E[X(t)X(0)]. Under the assumptions (2, 3, 4), there exist constants
t
c and c which only depends on c ,c and γ , such that for all t ∈ Z and all ω ∈ [0,1),
3 4 1 2 2
∥R exp{−2πiωt}+R exp{2πiωt}∥ ≤ c exp{−c tγ1} (52)
t −t op 3 4
Lemma 7. If X and Y are two sub-Gaussian random variables, then XY is a complex
sub-exponential random variable.
61Lemma 8. To any J ×K matrix Z with complex entries there corresponds a (2J)×(2K)
matrix ZR with real entries such that
1. if Z = X+Y, then ZR = XR +YR
2. if Z = XY, then ZR = XRYR
3. if Y = Z−1, then YR = (ZR)−1
4. det(ZR) = |det(Z)|2
5. if Z is Hermitian, then ZR is symmetric
6. if Z is unitary, then ZR is orthogonal
7. if the latent values and vectors of Z are µ ,α ,j = 1,...,J, then those of ZR are,
j j
respectively,
   
ℜ(u ) −ℑ(u )
j j
λ , , and ,λ , ,j = 1,...,J.
j   j  
ℑ(u ) ℜ(u )
j j
providing that dimensions of matrices appearing throughout the lemma are appropri-
ate. In addition, the ZR may be taken to be
 
ℜ(Z) ℑ(Z)
ZR =  .
 
−ℑ(Z) ℜ(Z)
Let U and U′ be two d-dimensional subspaces of Rp with projection matrices Π and
Π′, respectively. In addition, let columns of U = [u |...|u ] and U′ = [u′|...|u′] be
1 d 1 d
orthonormal basis of U and U′, respectively. Also, let U⊥ be the orthogonal complement
of U and U⊥ = [u |...|u ] be an orthonormal matrix whose columns are orthogonal to
d+1 p
62the columns of U. The following lemma, adopted from Wang et al. (2014), characterizes
useful properties of the distance between subspaces.
Lemma 9. Let U and U′ be two d-dimensional subspaces of Rp. Then,
√ √ √ √ √
D(U,U′) = 2∥U†U′⊥∥ = 2∥[U⊥]†U′∥ = 2∥Π⊥Π′∥ = 2∥ΠΠ′⊥∥ ≤ 2d
F F F F
and
√ √ (cid:20) D(U,U′⊥)2(cid:21)1/2
D(U,U′) =
2(cid:2) d−∥U†U′∥2(cid:3)1/2
= 2 d− .
F 2
Proof. See Stewart and Sun (1990) and Bhatia (2013) for details.
63