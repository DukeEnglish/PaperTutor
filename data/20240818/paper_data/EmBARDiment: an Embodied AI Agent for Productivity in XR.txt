EmBARDiment: an Embodied AI Agent for Productivity in XR
RiccardoBovo StevenAbreu KaranAhuja EricJGonzalez
Google,USA Google,USA Google,USA Google,USA
ImperialCollegeLondon, UniversityofGroningen, NorthwesternUniversity,
UK Netherlands USA
Li-TeCheng MarGonzalez-Franco
Google,USA Google,USA
Explicit Input Control Intui�ve Implicit Input
Conscious Effort Li�le or No Effort
INTENT
eye gaze fixations
Sure I'll suggest
Ctrl + F to
beautify the json
Can you reply with
this solution?
BARD emBARDiment
Figure1:EmBARDimentintroducesanattentionframeworkthatderivescontextimplicitlyfromusereye-gaze,andcontextualmem-
orywithintheXRenvironment.Itusestheimplicitlyinformationbundledwithexplicitverbalinputstoelicitgroundedcommunication
betweentheUserandtheAIAgent.
ABSTRACT ChatGPT[41],Claude[5],andBARD[20]emergingastheprimary
interfaceforengagingwiththesepowerfulAIsystems.However,as
XR devices running chat-bots powered by Large Language Mod-
LLMscontinuetoevolve,theirpotentialextendsbeyondtext-based
els(LLMs)havetremendouspotentialasalways-onagentsthatcan
interactions,particularlyintherealmofextendedreality(XR)en-
enablemuchbetterproductivityscenarios. However,screenbased
vironments. TheintegrationofLLMsasembodied,always-onim-
chat-botsdonottakeadvantageofthethefull-suiteofnaturalin-
mersiveagentswithinaugmentedandvirtualrealityheadsetsholds
putsavailableinXR,includinginwardfacingsensordata,instead
immense promise for transforming user experiences and enabling
theyover-relyonexplicitvoiceortextprompts,sometimespaired
seamless,context-awareassistance.Despitetheincreasingsophisti-
with multi-modal data dropped as part of the query. We propose
cationofLLMs,currentchat-botimplementationsheavilyrelyon
asolutionthatleveragesanattentionframeworkthatderivescon-
explicitvoiceortextprompts,oftendemandingmultipleiterations
text implicitly from user actions, eye-gaze, and contextual mem-
torefinethedesiredtoneandcontextfortheoptimaloutput,leading
orywithintheXRenvironment. Thisminimizestheneedforen-
tolengthyinteractions. InXRenvironments,thisapproachfailsto
gineered explicit prompts, fostering grounded and intuitive inter-
capitalizeonthediverserangeofnaturalinputsavailable,ascon-
actions that glean user insights for the chat-bot. Our user studies
temporary XR headsets (such as Meta Quest Pro [36] and Apple
demonstrate the imminent feasibility and transformative potential
VisionPro[6])areequippedwithinward-facingsensorscapableof
ofourapproachtostreamlineuserinteractioninXRwithchat-bots,
capturing rich user data. Although recent advancements in LLM
whileofferinginsightsforthedesignoffutureXR-embodiedLLM
architectures, suchasGemini1.5[21], aimtosupportupto10M
agents.
tokens of context attached to a prompt, the current explicit input
Index Terms: AI Agents, Chatbots, XR productivity, Multi- modalitiesinXRdevicesremainlimited. Forinstance,speechin-
window,AIinput. puthasanestimateduniversalthroughputofonly39bitspersecond
[14],whiletextinputinXRtechnologiesremainscumbersomeand
1 INTRODUCTION inefficient [26]. Our approach is particularly useful for XR envi-
ronmentsduetotheirlargefieldofview(FoV)displays,whichcan
The rapid advancement of Large Language Models (LLMs) has
supportmultiplewindowsforproductivitytasks[7,42]. Whileit
revolutionizedhuman-computerinteraction,withchat-botssuchas
is possible to provide all the data available on these windows to
LLMs as context, this can result in a large amount of data, mak-
ing the system less responsive, computationally heavy, and chal-
lengingtomaintainnuancedconversations, asnotallinformation
mayberelevant[31,29].Consequently,directlytranslatingexplicit
4202
guA
51
]CH.sc[
1v85180.8042:viXraprompt-based chat-bots to embodied agents in XR is likely to be thatinaregularPC,canposedifficultiesforassistantsindelivering
sub-optimal. Toaddressthesechallenges,weproposeEmBARDi- targeted and efficient productivity support [31]. EmBARDiment
ment,anovelapproachthatleveragesanimplicitattentionframe- tacklesthesechallengesbyemployingmultimodalinteractionsand
workincombinationwithacontextualmemorytoenableembodied developsagaze-drivencontextualmemory. Thisapproachallows
LLMagentsinXRenvironments(seeFig2). Oursolutionaimsto embodiedAIagentstoimplicitlyextractrelevantcontextfromthe
implicitlyderivecontextfromtheuser’sactions,eye-gaze,andvi- user’sfocusofattentionwithintheXRenvironment,therebyfacil-
sualsaliencywithintheXRenvironment,minimizingthereliance itatingmoreproductiveinteractions.
on engineered explicit prompts. As a result, we facilitate highly
targeted and intuitive interactions, allowing the agent to infer the 2.2 GazeattentiondrivenMultimodalXRInteractions
user’s intentions and needs based on their behavior and focus of PriorworkinHCIleveragesinputssuchasgazeandpointingges-
attention, both present and past. For example, if a user is read- turesconcurrentlywithspeechcommandstogeneratemulti-modal
ingadocument,theagentcanaccessthecontenttheuserhasread interactions[43,19].[8]’sfoundationalstudy”Put-That-There”ex-
and utilize it as context for generating relevant responses, along- aminedthefusionofvocalinstructionsandmanualgesturestoim-
sidethequeryprompt.UnlikepriorworksongazeinXRthatfocus proveengagementwithgraphicaluserinterfaces[8].Likewise,[37]
primarilyonone-offvisualquerying[47,37], ourapproachcom- investigatedthecombinationofeyetrackingandverbalcommuni-
binesgaze-saliencycontextwithLLM-poweredcontinuedinterac- cation,particularlyforengagingwithsmall,denselyarrangedon-
tion, offering the best of both worlds. We hypothesize that this screen elements. Recent studies have shifted focus towards com-
approachwillenhanceuserinteractionswiththeembodiedagent, prehendingvisualcontextandleveragingintuitivecommunication
asitestablishesashared”theoryofmind”andprovideshighlycon- toimproveinteractions. Specifically,[34]etal. conductedastudy
textualassistance, potentiallysimplifyinginteractionstothelevel that incorporated GPS location and user’s head gaze into mobile
ofnaturallanguagecommandslike”putthatthere”[8]. voice assistants, enhancing their comprehension of nearby shops
Theprimarycontributionsofthisworkareasfollows: andbuildingsbyunderstandingauser’slocationandvisualdirec-
tion,sothatuserscanperformimplicitverbalrequestsabouttheir
• We propose a novel attention framework leveraging gaze-
surroundings [34]. [4] et al. made use of speech as a direction
basedsaliencydrivencontextualmemoryforembodiedLLM
communicationchannelakintouser-gazetofacilitateinteractions
agents in XR, thus enabling implicit user cues for context-
across smart environment ecosystems [4]. [47] et al. presented
awareassistance.
Nimble,amobileinterfacethatcombinesvisualquestion-answering
frameworks with gesture recognition, focusing on directive ges-
• Weconductuserstudiestoempiricallyevaluatetheeffective-
tures, toenhance user engagement[47]. [33] etal. exploited the
nessofourproposedcontextualframeworkinenhancinguser
implicit attention cues of gaze in MiseUnseen to facilitate novel
interactions and experience, and showcase its efficacy over
interactionsinsideXRenvironments,includingcreatingorarrang-
baselineexplicittext-basedinputcondition.
ingobjectscovertlyatruntime[33]. Theseworkshighlighttheef-
fectivenessofusingvisualattentionorpointinggesturestoaidin
2 RELATEDWORK understanding the user’s intended referent during verbal requests.
2.1 Context-AwareAssistantsforProductivityinXR However,theseworksprimarilyfocusonprovidingdirectionalin-
formation at the moment of the request. In contrast, EmBARDi-
The growing interest in XR multi-window environments for pro-
mentextendsthesetechniquesbyimplementingacontextualmem-
ductivity[7,42]presentsnewopportunitiesandchallengesforin-
orythatencodesinformationabouttemporalgaze-basedsaliency,
tegratingcontext-awareassistants. Theseenvironmentsofferusers
allowingtheassistanttoleveragethiscontextuptothemomentof
cost-effective, customizable, and private workspaces, even on the
theverbalrequest,thusenablingamorecontextuallyrelevantand
go [18, 40]. Moreover, XR multi-window environments can sup-
efficientinteractionparadigm.
portremoteworkbymitigatingissueslikedistractions,insufficient
workspaces, and the struggle to separate work from personal life 2.3 FromChat-botstoEmbodiedXRAgents
[18],maybeevenprovidingbettermultitaskingsolutions,aswecan
Chat-bots have come a long way since their early days of rudi-
easilymaintainmultiplewindowsopenatthesametime[42],even
mentarypatternmatching,nowincorporatingsophisticatedNatural
onmobileofficeworksettings[40]. Thedevelopmentofcontext-
LanguageProcessing(NLP)techniquesfornuancedtextandvoice
awareassistants[16]hasseenaplethoraofsolutions,gleaningin-
interactions[3].AdvancementsinNLPhaveenabledthesesystems
sightsabouttheuser(e.g. activity[2],pose[48],preferences[15],
tocomprehendcomplexinputsandgeneratecontextuallyrelevant
taskathand[17])andimmediateenvironment(e.g. location[35],
responses [11]. This evolution has transformed user interactions
ambientlight[10]). InthedomainofXR,context-awareassistants
withAI,shiftingfromtypingtovoice-basedinterfaces[32,45,50],
have been explored to enhance education [49], industrial training
reflecting the importance of creating communication that mirrors
and maintenance [25] and enable more natural interactions [27].
human-humaninteractions[44,13]. Therecentdebutofpowerful
Forinstance,inthedomainofimmersiveuserexperience,[27]pro-
LLMslikeChatGPT,BARD,andGeminihassparkedrenewedin-
posedaframeworkthatutilisesthemetadatafoundinthefieldof
terestinAI-drivenchat-botapplications[11],leadingtoincreased
viewtohelpgroundtheconversationbetweenhumanandAIagent
userengagementandscientificresearch[12].Thisrenewedinterest
[27]. Whilecontext-awareassistantsinXRhavebeenexploredfor
hasalsomotivatedtheintegrationofAIchat-botsandagentsinto
various applications, there is a notable gap in research concern-
XRapplications, intheformofembodiedagents[28,39]. These
ingtheirapplicationforknowledgeworkersinXRenvironments,
embodiedagentsaimtofurtherenhanceuserengagementandnatu-
despitetherecognizedpotentialofXRforenhancingproductivity
ralinteractionthroughvisualandbehavioralcues.EmBARDiment
work [7, 42, 18]. Productivity within XR presents unique chal-
buildsuponthisrenewedinterestinembodiedXRagents,andin-
lengesthatrendertraditionalAIagentinterfaces,suchaschat-bots,
troducesanattentionbasedframeworkthatdynamicallyadjuststo
inefficient,primarilyduetothecumbersomeandinefficientnature
theuser’scurrentactivityandcontextwithintheXRenvironment.
oftypinginXR[26].Additionally,alternativechannelslikespeech,
becauseoftheirintrinsiclimitations[14],arenotdeemedsuitable
to generate the complex prompt required for knowledge workers
3 EMBARDIMENT
[38]. Furthermore,theinformationaccessibleacrossmultiplewin- EmBARDimentisapracticalexampleofourframeworkthatisim-
dows in XR environments, perhaps with even more screens open plementedasanXRapplicationthatseamlesslyintegratesspeech-selected dropout
threshold
Lorem ipsum dolor sit
Contextual
eye-gaze amet, consectetur Memory LLM UI ac�ons
adipiscing elit, sed
processing
Audio
Verbal Output
Input
Figure2:SchemaofEmBARDiment,explaininghowtheattentionframeworksusesimpliciteye-gazetoselectcontextualinformationandbundles
itwithexplicitverbalinputs.ThiselicitsgroundedcommunicationbetweentheUserandtheAIAgent.
to-text, text-to-speech, gaze-driven saliency and LLMs to enable Bycorrelatingthespatialpositionofthetextwiththeeye-tracking
a voice-interactive AI agent within a multi-window XR environ- data from the XR headset, EmBARDiment can determine which
ment. Inthissection,wedescribethesystemarchitecture(Fig2), texttheuseriscurrentlypayingattentionto.Thesystemmaintains
itskeycomponentsandtheirintegration. Theapplicationwasde- a buffer of the user’s saliency history, preserving the order of the
veloped using Unity [1] and deployed on the Oculus Quest Pro text to ensure coherence. To differentiate between saccades and
[36]. The code is available at the following GitHub repository: fixations, EmBARDimentonlyregisterstextthattheuserhasfix-
https://github.com/anonymous-user/anonymous-repo. atedonforaminimumof120milliseconds,whichisconsideredthe
minimumtimerequiredforeffectivevisualinformationprocessing
3.1 EmbodiedAIAgent duringreading[46]. Thecontextualmemoryinoursystemhasa
EmBARDiment features an embodied AI agent that serves as the maximum capacity of 250 words to enable basic episodic mem-
primaryanchorandinterfaceforuserinteraction. TheAIagentis ory,ratherthanlongtermmemory. Ouragentcontextualmemory
embodiedintoacuteavatardesignedtoprovideanengagingandin- worksbysimplydiscardingolderinformationastheuserfocuses
tuitiveexperiencebycombiningverbalandnon-verbalcues.When on new content. When the user makes a verbal request, the con-
theAIagentreceivesaresponsefromtheChatGPT-4API,ituses textualmemoryiscombinedwiththeuser’squeryandsenttothe
theGoogleCloudText-to-SpeechAPItogeneratespeechandcor- ChatGPT-4completionAPIforprocessing. Thisapproachallows
respondingvisemesforlipsynchronizationandfacialanimations. theAIagenttogenerateresponsesthataregroundedintheuser’s
Thisintegrationcreatesamorelifelikeandimmersiveexperience currentfocusandsaliencyhistory. Aftereachrequest,thecontex-
for the user, mimicking human-like interactions. The AI agent’s tualmemorybufferiscleared.Belowyoucanseeanexampleofthe
embodimentplaysacrucialroleinestablishingasenseofpresence generatedPromptcontainingtheuserverbalrequest,someprompt
and facilitating natural communication. By leveraging the user’s engineeringandthegaze-drivencontextualmemory:
gazeandsaliencyhistory,theAIagentcanmovearoundthediffer-
ent windows showcasing visually a level of shared understanding User’s verbal request
oftheuser’scurrentfocus. Prompt Engineering
Contextual information
3.2 MultimodalInteraction
EmBARDimentleveragesmultipleinputandoutputmodalitiesto {
enableseamlesscommunicationbetweentheuserandtheembod- "role": "user",
ied AI agent [43, 19]. The user can initiate a verbal request by "message": user verbal request from speech-to-text.
pressingakeyandthenspeaking(’V’onthekeyboard).Theuser’s Below is a dataset representing my visual attention it
speech is converted to text using the Google Speech-to-Text API contains the text i have been reading from the windows
[22], whichsendstheaudiodatatotheGooglecloudandreturns I have been observing. Please use this information to
thetranscribedtextthatisthendisplayedontheflyinaUIpanel inform your response to my request.
beneath the AI agent, providing visual feedback to the user (see window_id_1: {gaze-selected text on window_id_1},
supplementaryvideo).Oncethefinaltranscriptionisreceivedfrom window_id_2: {gaze-selected text on window_id_2}.
theGoogleSpeech-to-TextAPI,itisprocessedbyanyLLMAPIfor Respond with 6 sentences max, keep it under 400
naturallanguageunderstandingandgeneration. Thesystemmain- characters maximum.".
tainsachathistorytopreservetheinteractioncontext,allowingthe }
AIagenttogeneraterelevantandcoherentresponses.Theresponse
isthendisplayedonaUIpanelbeneaththeAIagent,andweuse
againtheGoogleCloudText-to-SpeechAPI[23]butnowtoconvert 4 EXPERIMENT
thetextintospeech.Thisprocessalsoproducesphonemesandtheir
ToevaluateEmBARDiment,weconductedauserstudy.Ourevalu-
correspondingvisemes,whichareusedtoanimatetheAIagent’sfa-
ationaimstodetermine(i)themeritofvisualattentionasamethod
cialexpressionsandlipmovementsinsyncwiththespokenwords
forselectingcontextualinformation,and(ii)toevaluatethediffer-
[51].
entoptionsforagentembodimentandthepositioningoftheagent.
3.3 Gaze-DrivenContextualMemory
4.1 Participants
EmBARDimentbuildsontopandextendsanexistingopen-source
multi-windowXRenvironmentWindowMirrorthatcapturesexist- We recruited 9 participants (5 male, 4 female, M =30.5,
Age
ingwindowsfromaPCandrenderstheminsidetheXRenviron- SD =5.9 years). Since in the experiment we use speech-to-text
Age
ment[9]. EmBARDimentprocesseseachwindowframeusingthe technologiesweevaluatedEnglishspeakingproficiency:onascale
Google Vision API [24] to perform optical character recognition from 1 (poor) to 5 (excellent), resulting in an average rating of
(OCR),extractingthetextcontentanditspositionwithintheframe. M =4.78SD =0.45. notethatfor5outof9participants
English EnglishFigure3:ExperimentConditions.(A)Baseline:nocontextualinformationselected.(B)Full-Context:Allthecontextualinformationareselected.
(C)Eye-gaze:informationareselectedbasedoneye-gazefixations.
English was their primary language.Additionally, since in the ex-
periment we use eye-tracking technologies we also evaluated vi-
sion. Intermsofvisioncorrection,3participantsreportedhaving
normalvision,whilealltheothershadvisioncorrectedtonormal;
4wearingglassesand2contacts.
eye-level
4.2 Design
Wedesignedawithinparticipantexperimentwith3conditionsand
3 tasks employing a Latin square design, every participant com-
pleted3tasks, resultingin27uniquecombinations. Ninepartici-
pantswerethereforerequiredtoensureeachoftheninecondition-
task pairings was tested in every order, thus eliminating order-
ing/learningeffects.The3conditions: 120°
• ABaselineconditioninwhichthereisnocontextualinforma- Figure4: Experimentlayout,asseenbytheparticipantsduringthe
tionadded(seeFig3a). experiment.The3textsarefixedforallparticipantstomaintainstim-
uliconsistency. Thelayoutofwindowsspawned1meterawayfrom
• AFullContextconditionwhereallthecontextualinformation theuser’shead,spanning120◦(60◦ontheleftand60◦ontheright),
is selected independently of the user’s visual attention (see andaresolutionof700x1200px.
Fig3b).
• AnEye-Trackingconditionwhichselectsinformationbased 2. Iamconfused,isthecataliveordead?
oneye-gazedata(seeFig3c).
[HBR]Questionsforthe”QuantumComputingIsComing”text:
Thequantitativeandqualitativecomparisonbetweenthesecon-
1. Whatarethecapabilitiesofcomputersnowadays?
ditionswillestablishifandhowcontextualepisodicmemorywill
enablemoreimplicitcommunicationandbetterunderstandtherole 2. What problems do traditional computing devices struggle
thateyegazewillplaywithAIagentsinXR. with?
4.2.1 Q&AReadingTask [wiki]Questionsforthe”Quditsandqutrits”text:
Thethreebodiesoftextof270to400wordseach, wereselected 1. WhatlettercanIusetodenotethedimensionofthesystem?
fortheexperiment. AlltextshadacommonthematicofQuantum
Computing. The first was based on a Medium article about the 2. Iamconfusedabouthowmanylevelsarepossible?
”Schro¨dinger’sCat”1;thesecondwasaHarvardBusinessReview
article: ”Quantum Computing Is Coming”2, and the third was a 4.2.2 MultiWindowLayout
Wikipediaarticlediscussing”Qubits”3.Thetwoquestionsregard- Thewindowlayout, asshowninFig4, isanessentialpartofthe
ingeachreadingwereintentionallydesignedtobeimplicitlyrelated task.Thelayoutisintendedtomimicaspectsofproductivitywork,
tothecontextofthetextandessentiallymeanttoaskclarifications suchasmultitasking,byhavingmultipleapplicationwindowsopen
onwhattheparticipanthadjustread. simultaneously. Thissetupmirrorscommonscenariosinphysical
[cat]Questionsforthe”Schro¨dinger’sCat”text: multi-screen environments where multiple windows are open and
usedatthesametimebyusers.
1. Whyisitsetto50%?
4.3 Procedure
1https://medium.com/swlh/quantum-computing-for-dummies-part-1- Participants received a project overview, consent form, demo-
2686b9ba3c51 graphic questionnaire, and experiment instructions. The experi-
2https://hbr.org/2021/07/quantum-computing-is-coming-what-can-it-do mentbeganwithareminderoftheirrighttowithdrawandtheop-
3https://en.wikipedia.org/wiki/Qubit portunitytoseekclarifications.
mc
08Table1:HumanLanguageModelInteractionQuestionnaire
4.3.1 (Part1)Q&APaperReading:
Weaskedparticipantstoreadtheselectedtextsonpaper. Wethen Aspect Question Answ.
presentedtwoquestionsabouteachtext. Thisprocedurewasre- Type
peatedforallthreebodiesoftext(Section4.2.1). Thisinitialstep Independentofitsfluency,howhelp-
wasusedtomakeparticipantsfamiliarwiththequestionsandrele- Helpfulness fulwasyourAITeammateinanswer- Likert
vantinformationwithinthetexts. ingthequestions?
Why did you find the AI Teammate
4.3.2 (Part2)XROnboarding: helpfulorunhelpful?Giveaconcrete Open
exampleifpossible.
After answering the questions, participants engaged in a similar
Howeasywasittointeractwiththe
task but this time inside XR and interrogating the AI Agent. To Ease Likert
AITeammate?
guarantee a successful experiment, participants were helped with
Howenjoyablewasittointeractwith
the XR headset and were guided to perform eye-tracking calibra- Enjoyment Likert
theAITeammate?
tionandtesting.ParticipantswerethenaskedtoopentheXRappli-
I am satisfied with the answer I re-
cationandfamiliarizethemselveswiththemodalityofinteraction Satisfaction Likert
ceived?
withtheAIAgent(i.e.,buttonpressandverbalrequest).
Did the way you chose to interact
4.3.3 (Part3)Q&AAIAgentTextInterrogation: Change with the AI Teammate change after Open
thefirstquestion?Ifso,how?
Then, in the XR experience, they were again presented with the
Whatadjectiveswouldyouusetode-
same3texts,oneatatimeandindifferentconditions. Theywere Description Open
scribetheAITeammate?
asked to read the text till they reach the text part pertinent to the
Responsiv. Howresponsivewasthesystem? Likert
questionrehearsedonPart1.Thentheyhadtoaskthequestionsto
theAIagentandkeepaskinguntiltheagentproducedasatisfactory
responses,i.e. aresponseequivalenttotheirownearlyreplyout- Table2:SurveyonAgentEmbodimentPreferences
sideXR.Weinstructedparticipantstomaintaintheoriginalimplicit Aspect Question PossibleAnswers
formulationoftheirquestionsinthefirstattemptinsideXR.Ifthey
1)Lack of embod-
didnotreceivearelevantanswer,theywereallowedtoreformulate
What of these 3 op- iment 2) Non-
theirquestionuptofivetimesinanattempttogenerateacontex-
Embodiment tionsdoyouprefer? anthropomorph.
tuallymeaningfulresponse. Afterfiveattempts, theexperimenter
3)Anthropomorph.
wouldpromptthemtomoveon. Thestudywasstructuredaround
Why did you choose
the different experimental conditions (baseline, full context, eye- (Openended)
theembodimenttype?
tracking). And each condition had one of the three reading tasks
What of these 2 op- (1) Context-following,
randomlyassigned(cat,HBR,wiki),sotheyvariedbyparticipants.
Location tionsdoyouprefer? (2)Fixed
Why did you choose
4.3.4 (Part4)ExperienceSurvey: (Openended)
thelocation?
Followingeachcompletedcondition,participantswereaskedques-
tions from a subset of the Human Language Model Interaction
Questionnaire (HLMIQ) [30] shown in Table 1, and gave verbal
times). Intotalwecollected110attemptsontheexperimentalset-
responseswhiletheexperimenterannotatedthoseresponses.
ting.Aftereachofthe3conditions,participantscompletedHuman
LanguageModelInteractionquestionnairesleadingto27question-
4.3.5 (Part5)AgentEmbodimentPreferences:
naires.Participantsalsocompletedtheagentembodimentquestion-
UponcompletionoftheQ&Atasks/survey, thestudytransitioned naires,leadingto9filledsurveys.
to the exploration of AI agent embodiment preferences. Partici-
pantswereintroducedtodifferentpossibleembodimenttypes(no 5.1 QuestionAttempts
embodiment, non anthropomorphic, or anthropomorphic embodi-
In our experiment, participants interrogate an AI agent, in search
ment),embodimentlocation(generalfixed,orcontextualposition-
forexplanationsrelatedtoafeaturedtext. IftheAI’sresponseis
ingsuchasfollowingtheactivewindow). Participantswereasked
irrelevantorunacceptable,participantsreformulatetheirquestion,
toengagewitheachoptiontoformabasisfortheirpreferredchoice
a maximum of five times to obtain an acceptable answer. Fewer
(Table2).
attempts indicate better performance, as they signify less effortto
reach an acceptable answer. We analyzed the dialogues to count
4.4 Questionnaires
thenumberofattempts,totaling110verbalrequests. Tocompare
Tobettercharacterizetheuserexperiencebeyondthequantitative
thenumberofattemptsacrossconditions,weperformedarepeated
analysisoftheattempts,weaskedparticipantstofillasubset(Table
measuresANOVAwithConditionasathreelevelsfactor(baseline,
1)oftheHumanLanguageModelInteractionQuestionnaire[30].
fullcontext,eye-tracking).Fig5.
TheHLMIqassessesimportantaspectssuchas:Helpfulness,Ease,
TheANOVAanalysisonthenumberofquestionsaskedindicate
Enjoyment,Satisfaction,ResponsivenessoftheAIAgent.
asignificantdifferencebetweentheconditions(F(2,24)=16.3,p¡
After each experimental block consisting of the 3 trials/texts,
.001).Tofurtherunpacktheresults,weconductapost-hocanalysis,
eachparticipantwasshownafewoptionsintermsofEmbodiment
andfindsignificantdifferencesacrossallconditions.Thecompari-
TypeandLocationwithinthemulti-windowspaceandthencom-
sonbetweenthenumberofattemptsintheBaseline(M=5.5,SD=
pletedasurveyon(Table2).
1.2)andFullContext(M=4.2,SD=1.3)revealedameandiffer-
enceandprobabilityof(X∆=1.333,p=0.047)andalargeeffect
5 RESULTS
size(Cohen’sd=1.193), indicatingasignificantreductioninthe
Each of the 9 participants had to ask the AI agent 2 questions numberofattemptsintheFullContextcondition.Thisreductionin
acrossthe3conditionsleadingtoatotalof54questions. Thepo- attemptsisfurtheremphasizedinthecomparisonbetweentheFull
tential attempts to reach satisfactory answers can range from 54 Context and Eye-Tracking (M = 2.5, SD = 0.7), which showed a
to270(aseachparticipantcouldreformulatethesamequestion4 meandifferenceandprobabilityof(X∆=1.667,p=0.011)andan5.2.1 Satisfaction(Likert)
TheKruskal-Wallistestindicatedasignificantdifferenceincondi-
*
tions(H(2)=12.838, p=0.002). Dunn’sPostHoccomparisons
revealed a higher satisfaction on the Eye-Tracking (M = 4.7, SD
* = 0.4) condition than both on the Baseline (M = 3 , SD = 1.3 ),
(p<0.001,p =0.001),andontheFullContextconditions(M
bonf
=3.8,SD=0.6),(p=0.022,p =0.067).
bonf
Question Attempts per Participant
Figure5: Boxplotofquestionattempts. Significantdifferenceshigh-
*
lightedwith: *p<.05,***p¡.001. *
evenlargereffectsize(Cohen’sd=1.491),demonstratingafurther
decreaseinnumberofattemptsfortheEye-Trackingcondition.Ad-
ditionallybeyondthewithingroupscomparisons,themeansquares
betweengroups(∑D i=1(x i−y i)2=20.3),alsosuggestnotablevari- Figure7:SatisfactionresponsestotheHLMIq, *p<.05.
anceinthenumberofquestionsaskedacrossdifferentconditions.
Tofurtherclarifytheresults,weutilizedaSankeyplottoillustrate
5.2.2 Helpfulness(Likert)
thesuccessratesateachattempt(Fig6). IntheEye-trackingcon-
dition, 77.7% of participants achieved the intended result on the The Kruskal-Wallis test revealed a significant effect of the con-
firstattempt. FortheFullContextcondition,participantstypically ditions on the outcomes (H(2)=7.404, p=0.025). Subsequent
reachedsuccessbythesecondattempt. Incontrast,intheBaseline Dunn’sposthoccomparisonsshowedEye-Tracking(M=4.4,SD
condition,77.7%ofparticipantsobtainedasatisfactoryanswerby = 0.8) to be significantly more helpful that the Baseline condi-
thethirdattempt. tion(M=2,7,SD=1.4)andFullContextconditions(p=0.007,
p =0.022,Figure8).Andshowingconsistencywiththeresults
bonf
fromourquantitativeanalysisofquestionattempts.
1st 2 nd 3 rd 4 th 5 th
Baseline
18 Questions
49 Attempts *
22.2% 38.8% 77.7% 88.8% 88.8%
Successful Successful Successful Successful Successful
Full Context
18 Questions
38 Attempts Figure8:HelpfulnessresponsestotheHLMIq, *p<.05.
5.2.3 Helpfulness(Open)
38.8% 77.7% 77.7% 94.4% 100%
Successful Successful Successful Successful Successful To further unpack this result we tap on the Open ended question
Eye -tracking oftheHLMIq:“WhydidyoufindtheAITeammatehelpfulorun-
18 Questions helpful?” For the Baseline condition, participants noted that the
23 Attempts initialanswersoftheAIAgentweregeneralandunrelatedtothe
contextofthetextandthatasatisfactoryanswerwasonlyobtained
77.7% 94.4% 100% aftersignificantprompting. P5: “..itdidnotgiveanyspecificin-
Successful Successful Successful formationtothetextiwasreading..”P8:“..tookalotofprompting
for the first answer but did really good in the second question..”
FortheFullContextcondition,participantsexperiencedmixedre-
Figure 6: Sankey diagram depicting showing the success rate at sultssayingthatthejourneytothecorrectanswerwasoftencum-
eachsubsequentattempt. bersome P3: “..it was able to answer my question even if it took
multiple attempts.” P5: “..it’s giving a very generic answer, un-
less I asked twice and be specific.” In the eye-tracking condition,
participants reported accurate answers straight to the point. P4:
5.2 Human Language Model Interaction Questionnaire “..itansweredthequestionveryaccurately.”P9:“..itansweredthe
(HLMIq) questionsstraightaway.”
5.2.4 Behaviorchanges(Open)
ParticipantscompletedHLMIqquestionnaireaftereachexperimen-
talcondition,resultinginatotalof9entriesperconditionand27 To better understand the differences between conditions we also
overall. While we found significant results for Helpfulness and looked at the quesiton: “Did the way you chose to interact with
Satisfaction,nosignificantfindingsemergedforEase,Enjoyment, the AI Teammate changed after the first question? If so, how?”.
andResponsiveness.Therefore,forconciseness,weonlyreportthe In the Baseline condition the majority of responses indicate par-
analysisoftheformerandomitthelatter. ticipantsprovidedmoreinformationinanattempttoguidetheAI.
***P6: “Igaveexplicitcontentfromthearticletoensureitgaveme 5.4.1 AgentEmbodimentType
therightanswer.”,P9: “Igavemorecontexttoit.”Participantsre-
When we asked, “How do you prefer the visual representation
portedlearningfromtheirinitialinteraction,indicatinganiterative
of a digital assistant to be designed?” Participants were shown
processofadjustingtheirapproachbasedontheAI’sresponses.In
threeoptions: LackofEmbodiment,Anthropomorphic, andNon-
theFullContextconditionthemajorityofparticipantstillreported
anthropomorphic.Afterexperiencingeachoftheconditions,partic-
thesamestrategies“ofaddingmoreinformation”.
ipantswereaskedtoexpressapreference.Theoptionthatreceived
themostpreferenceswasAnthropomorphic(56.2%), followedby
5.3 ConversationEvaluation Non-Anthropomorphic(31.1%)andLackofEmbodiment(12.7%)
(Figure 9).
Wereviewtherecordedconversationtobettercharacterizethedif-
ferencesbetweentheFullContextandBaselinecondition. Andto
understand how and why the Full Context condition failed in an-
sweringcorrectly.
5.3.1 ConversationExamples
Tofurthersupportthissectionwereporttwofullexampleconver-
sationsfromparticipantP2. SimilarlytotheHLMIqwealsofind
twostrategiesintheseexamples:
Figure 9: Circle chart representing user’s preferences in terms of
Strategy 1: Referring to Context embodiment’stype.
Strategy 2: Adding More Information
Tofurtherunpacktheseresults, participantswereaskedtojus-
tifytheirpreferences. Thosewhosepreferencewasanthropomor-
Example 1 (P2) Baseline:
phic mentioned a desire for interactions that mimic human com-
munication. P6: “A human-like agent makes me a bit less self-
Participant 2 : Why is the chance of radioactive
conscious about making a verbal request”. However, many who
decay 50%?
expressedapreferencefortheanthropomorphicdesignwereflex-
AI Agent : The chance of radioactive decay is not
ibleabouttherepresentation; theywouldbefinewithanabstract,
inherently 50% for all isotopes...
non-anthropomorphicoption,aslongasitwasvisuallypresent.P1:
Participant 2 : In this example, though. Why is it 50%?
“Idon’tmindbetweenthetwoaslongasitisrepresented. Idon’t
AI Agent : In your example, if the chance of decay
wantthedisembodiedbecauseIdon’tknowifit’spresentorlisten-
is 50%, it means we’re looking at a
ing to me.” The representation in a visual form was signifying to
specific moment..
participantstheassistant’spresenceandattentiveness.P5:“Ilikeit
Participant 2 : In the Schr¨odinger’s cat example,
tobetherebecauseitremindsmethatIcanaskquestions”Those
why is the chance of decaying 50%?
whochoselackofembodimentvaluedtheabsenceofrepresenta-
AI Agent : In Schr¨odinger’s cat thought experiment,
tion,asitwasnon-distractingandnotvisuallyimposing.
the 50% decay chance is a simplification
to illustrate quantum superposition..
5.4.2 AgentEmbodimentLocation
Example 2 (P2) Full Context: Inasimilarwaywethenaskedparticipants,“Wheredoyouprefer
theAIassistanttobelocated?” givingthemtwochoices: Context-
Participant 2 : What are the core capabilities of followingorFixed.Aftershowingtheseoptionstoparticipantsand
nowadays computers? addressinganyquestions,theywereaskedtomakeachoice. Most
AI Agent : Quantum computers harness the participantspreferredContext-following(33.3%),butaroundone-
principles of quantum mechanics to thirdoptedforFixed(66.6%)(Fig10.
process information....
Participant 2 : What are the core capability of
computers described in this article?
AI Agent : The article highlights that digital
computers excel at arithmetic
operations...
Example 3 (P2) Eye-Tracking:
User : What’s the term that describes the capabilities of
computers?
AI Agent : The term that describes the capabilities
of computers is arithmetical machines.
Figure10: Circlechartrepresentinguser’spreferencesintermsof
embodiment’slocation.
5.4 AgentEmbodimentPreferences
When asked To justify Context-following choices participant
ThisAgentEmbodimentPreferencesSurveywasoncebyeachpar- mentionedadesireforthedigitalassistanttobeeasilyaccessible
ticipantattheendofthewholeprocedure(n=9). without requiring the user to turn their head or attention.P2: “Idon’thavetoturnmyhead.”P7:“Iwouldwanttheavatartofol- Multi-modalinput: developersshouldfocusonharnessingthe
lowmygazebetter.”Thecontext-followinggroupalsohighlighted complementarystrengthsofvisualattentionandverbalcommuni-
adesireforclarityinhowtheassistant’smovementsandposition- cation to craft AI agents that are both intuitive and responsive to
ing reflect its awareness of the context. P1: “Context following userinput. Visualattentionservesasasubtleyetpowerfulsignal
makesiteasierformetounderstandwhattheavatarisawareof.” for context recognition, allowing the AI to grasp the user’s focus
withoutinvasiveprompts. Concurrently,verbalrequestsprovidea
6 DISCUSSION directavenueforuserstoconveytheirneedsandqueries. Ourap-
plicationshowcaseshowdirectingtheAIAgent’sattentiontouser-
6.1 WhytheAIAgentfailedinFullContextCondition focusedcontextmitigatesthechallengesassociatedwithprocessing
Wefurtherlookattheresultsandwefindageneralconsistenttrend: extensiveinputs. ByaligningtheAIAgent’sfocuswiththeuser’s,
theAIAgentfailingontheFullContextcondition.Butwhatarethe especiallyinscenariosrequiringimmediateclarificationonrecent
differences between the Full Context and the Baseline conditions interactions, thesystemgainsasignificantedgeovermodelsbur-
that cause this? One of our main learning was that the AI Agent denedwiththeentiretyofavailablecontext.
performancefalteredintheFullContextconditionwhencompared EpisodicvsLong-termMemory:ouragentcontextualmemory
totheEye-Trackingcondition,webelievethisisduetotheexpan- worksbysimplydiscardingolderinformationastheuserfocuseson
sive context, which allowed for multiple potential answers. This newcontent.Thecontextualmemoryinoursystemhasamaximum
wasinstarkcomparisontoour250wordepisodicmemoryimple- capacityof250wordstoenablebasicepisodicmemory,ratherthan
mentedforgaze. Forinstance,ifwelookattheConversationex- long term memory. Changing the capacity will impact the LLM
amples (section 5.3.1), in the ”Example 2 (P2) Full Context”, to performances(i.e. responsiveness)andhastherisktorunintothe
thequestion”Whatarethecorecapabilitiesofnowadayscomput- sameproblemswefoundontheFullContextcondition.Therefore,
ers?”theAIAgentanswer”Quantumcomputersharnesstheprin- aftereachrequest,thecontextualmemorybufferiscleared,ready
ciplesofquantummechanicstoprocessinformation”isinfluenced tocapturenewcontext.
bythebroadercontextaboutquantumcomputing,asall3articles
talkaboutquantumcomputingwhileonlyonetalksaboutthecapa-
7 FUTUREWORK
bilitiesofcomputersnowadays. GiventhattheFullContextispri- The current study focused on a scenario which requires the AI
marilyconcernedaboutquantumcomputing,theAIAgenttended Agent’sattentionalignedwiththeuser’sattentiontoprovideclari-
tofocusitsanswersonquantumcomputers,illustratinghowexten- ficationsonrecentlyobservedcontent. Thisapproachassumesthe
sivecontextcanskewitsresponses.Thisledtoinaccuracies,asthe user’svisualfocusalignswiththeirinformationalneeds.However,
modelstruggledtointegratethequestiononthespecificcontextthe scenariosexistwheretheAIAgent’sfocusmightneedtodiverge,
userwasreading. exploring content the user hasn’t directly engaged with—perhaps
tohighlightoverlookeddetails.Futureworkshouldexpandbeyond
6.2 StrategiesforFull-contextandBaseline thisalignment,enablingtheAIAgenttoconsiderbroaderoralter-
natecontexts.
Participants reported employing a strategy consisting of ”adding
moreinformation”acrossbothconditions, however, byanalyzing 8 CONCLUSION
theconversationwefoundthatthisactuallyencompassedtwodis-
EmBARDiment, explored a novel framework for integrating
tinctstrategies.
context-aware embodied AI agents within extended reality (XR)
Strategy1:ReferringtoContext-ThisstrategypromptstheAI
environments. Our solution goes beyond conventional explicit
Agenttoconsiderthecontextinwhichaquestionisasked. While
text-based chat-bots and leverages an implicit attention frame-
itshowedlimitedsuccessintheBaselineconditionduetothelack
worktostreamlinecommunicationandmakeitmorecontextually
ofcontextualinformation,itsapplicationintheFullContextcondi-
grounded. We utilize the user’s gaze and visual saliency to de-
tiondemonstratedanotableimpact.Forinstance,whenParticipant
rive relevant context. We ran an experiment comparing three in-
2initiallyaskedaboutthecorecapabilitiesofmoderncomputers,
teractionconditions—withoutcontext,withfullcontext,andwith
theAIAgent’sresponsewasskewedtowardsquantumcomputing.
eye-trackinguser-focusedcontext—,participantsinteractedwithan
However,whentheparticipantrefinedthequestiontofocusonthe
AIagent,adjustingtheirquestionsbasedontheagent’sresponses.
contextdescribedwithinthearticle,theAIAgentadjusteditssearch
Findings indicated that using visual attention to guide contextual
withinthegivencontextandidentifiedanaccuratealternative.
memoryselectionresultedinfewerquestionreformulationsanden-
Strategy2: AddingMoreInformation-Thisstrategyentails
hanced user satisfaction and perceived helpfulness of the system.
providingadditional,andspecificinformationtotheAIAgent,al-
This confirms our hypothesis that such an approach significantly
lowingittoleverageitsinternalknowledgebase. Forexample,in
improves interaction efficiency with AI Agents in multi-window
the Baseline condition, questions about why the chance of decay
XRsettings. Ourgaze-drivencontextualmemoryextendsprevious
was 50% in which the AI Agent had no context led to wrong re-
research [34, 47, 37], on multi modal interaction gaze + speech.
sponses. When”Schrodinger’scat”wasmentioned,theAIAgent
Additionallyourfindingshighlightthepotentialtostreamlineuser
couldtapintoitsexistingknowledgeandprovideagoodresponse.
interactionswithAIagentsinXRofferingspecificDesignconsid-
erations for the design of AI agent functionality in multi-window
6.3 DesignConsiderations settings.
IntherealmofAIAgentsinmulti-windowXRenvironments,our
researchunderscoresthecriticalimportanceofintegratingimplicit
ACKNOWLEDGMENTS
andexplicitinputstoenhanceuserinteractions.Tofullyenablethe TheauthorswishtothankA,B,andC.Thisworkwassupportedin
integrationofexplicitandimplicitinputs,weproposethealignment partbyagrantfromXYZ.
betweenvisualattentionandverbalinputthatcanenablecontextual
memory.Anewstandardforcapturingandleveraginguserengage- REFERENCES
menthistory.Thismightproveespeciallyrelevantinthecontextof [1] Unity2023.https://unity.com/,2023.Accessed:2023-09-28.3
information-richmulti-windowenvironmentswheretheremightbe [2] G. D. Abowd, A. K. Dey, P. J. Brown, N. Davies, M. Smith, and
multiple contextual targets for an interaction potentially spanning P.Steggles. Towardsabetterunderstandingofcontextandcontext-
acrossmultiplewindows. awareness. InHandheldandUbiquitousComputing: FirstInterna-tionalSymposium,HUC’99Karlsruhe,Germany,September27–29, 27.3
1999Proceedings1,pp.304–307.Springer,1999.2 [23] Google. Cloudtext-to-speech,Accessed2024. Accessed: 2024-03-
[3] E. Adamopoulou and L. Moussiades. Chatbots: History, technol- 27.3
ogy,andapplications.MachineLearningwithapplications,2:100006, [24] Google. Opticalcharacterrecognition(ocr)—cloudvisionapi,Ac-
2020.2 cessed2024.Accessed:2024-03-27.3
[4] K. Ahuja, A. Kong, M. Goel, and C. Harrison. Direction-of-voice [25] L.Greci.Xrforindustrialtraining&maintenance.RoadmappingEx-
(dov) estimation for intuitive speech interaction with smart devices tendedReality: FundamentalsandApplications,pp.309–320,2022.
ecosystems. InProceedingsofthe33rdAnnualACMSymposiumon 2
UserInterfaceSoftwareandTechnology,pp.1121–1131,2020.2 [26] J.Grubert,L.Witzani,E.Ofek,M.Pahud,M.Kranz,andP.O.Kris-
[5] Anthropic.Claudeai.https://claude.ai/,2023.Accessed:2023- tensson. Textentryinimmersivehead-mounteddisplay-basedvirtual
09-30.1 realityusingstandardkeyboards.In2018IEEEConferenceonVirtual
[6] Apple.Applevisionpro.https://www.apple.com/vision-pro/, Realityand3DUserInterfaces(VR),pp.159–166.IEEE,2018.1,2
2022.1 [27] D.Guo,A.Gupta,S.Agarwal,J.-Y.Kao,S.Gao,A.Biswas,C.-W.
[7] V.Biener, D.Schneider, T.Gesslein, A.Otte, B.Kuth, P.O.Kris- Lin, T. Chung, and M. Bansal. GRAVL-BERT: Graphical Visual-
tensson, E. Ofek, M. Pahud, and J. Grubert. Breaking the screen: LinguisticRepresentationsforMultimodalCoreferenceResolution.In
Interactionacrosstouchscreenboundariesinvirtualrealityformobile Proceedingsofthe29thInternationalConferenceonComputational
knowledgeworkers.IEEEtransactionsonvisualizationandcomputer Linguistics,pp.285–297.InternationalCommitteeonComputational
graphics,26(12):3490–3502,2020.1,2 Linguistics,Gyeongju,RepublicofKorea,Oct.2022.2
[8] R.A.Bolt. “Put-That-There”:VoiceandGestureattheGraphicsIn- [28] A.Hartholt,E.Fast,A.Reilly,W.Whitcup,M.Liewer,andS.Moz-
terface. InProceedingsofthe7thAnnualConferenceonComputer gai. Ubiquitousvirtualhumans:Amulti-platformframeworkforem-
GraphicsandInteractiveTechniques, SIGGRAPH’80, p.262–270. bodied ai agents in xr. In 2019 IEEE International Conference on
AssociationforComputingMachinery,NewYork,NY,USA,1980. ArtificialIntelligenceandVirtualReality(AIVR).IEEE,2019.2
doi:10.1145/800250.8075032 [29] S.JeurisandJ.E.Bardram.Dedicatedworkspaces:Fasterresumption
[9] R.Bovo,E.J.Gonzalez,L.-T.Cheng,andM.Gonzalez-Franco.Win- timesandreducedcognitiveloadinsequentialmultitasking.Comput-
dowmirror: anopensourcetoolkittobringinteractivemulti-window ersinHumanBehavior,62:404–414,2016.doi:10.1016/j.chb.2016.
viewsintoxr. In2024IEEEConferenceonVirtualRealityand3D 03.0591
UserInterfacesAbstractsandWorkshops(VRW),pp.436–438.IEEE, [30] M.Lee,M.Srivastava,A.Hardy,J.Thickstun,E.Durmus,A.Paran-
2024.3 jape,I.Gerard-Ursin,X.L.Li,F.Ladhak,F.Rong,etal. Evaluating
[10] B.L.Brumitt,S.Shafer,J.Krumm,andB.Meyers. Easylivingand human-languagemodelinteraction.arXivpreprintarXiv:2212.09746,
theroleofgeometryinubiquitouscomputing. InProceedingsofthe 2022.5
DARPA/NIST/NSFWorkshoponResearchIssuesinSmartComputing [31] D.Li,T.Chen,A.Zadikian,A.Tung,andL.B.Chilton. Improving
Environments.Atlanta,GA,1999.2 automaticsummarizationforbrowsinglongformspokendialog. In
[11] C. Chakraborty, S. Pal, M. Bhattacharya, S. Dash, and S.-S. Lee. Proceedingsofthe2023CHIConferenceonHumanFactorsinCom-
Overviewofchatbotswithspecialemphasisonartificialintelligence- putingSystems,pp.1–20,2023.1,2
enabledchatgptinmedicalscience.FrontiersinArtificialIntelligence, [32] M.Malik,M.K.Malik,K.Mehmood,andI.Makhdoom. Automatic
2023(1237704),2023.doi:10.3389/frai.2023.12377042 speech recognition: a survey. Multimedia Tools and Applications,
[12] C. Chakraborty, S. Pal, M. Bhattacharya, S. Dash, and S.-S. Lee. 80:9411–9457,2021.2
Overviewofchatbotswithspecialemphasisonartificialintelligence- [33] S. Marwecki, A. D. Wilson, E. Ofek, M. Gonzalez Franco, and
enabledchatgptinmedicalscience.FrontiersinArtificialIntelligence, C.Holz.Mise-unseen:Usingeyetrackingtohidevirtualrealityscene
6,2023.2 changesinplainsight. InProceedingsofthe32ndannualACMsym-
[13] P. Chitto`, M. Baez, F. Daniel, and B. Benatallah. Automatic gen- posiumonuserinterfacesoftwareandtechnology,pp.777–789,2019.
erationofchatbotsforconversationalwebbrowsing. InConceptual 2
Modeling: 39thInternationalConference,ER2020,Vienna,Austria, [34] S.Mayer,G.Laput,andC.Harrison.EnhancingMobileVoiceAssis-
November3–6,2020,Proceedings39,pp.239–249.Springer,2020.2 tantswithWorldGaze. InConferenceonHumanFactorsinComput-
[14] C.Coupe´,Y.M.Oh,D.Dediu,andF.Pellegrino.Differentlanguages, ingSystems-Proceedings.AssociationforComputingMachinery,4
similar encoding efficiency: Comparable information rates across 2020.doi:10.1145/3313831.33764792,8
thehumancommunicativeniche. Scienceadvances,5(9):eaaw2594, [35] J.F.McCarthyandE.S.Meidel. Activemap:Avisualizationtoolfor
2019.1,2 locationawarenesstosupportinformalinteractions. InInternational
[15] A.K.Dey,G.D.Abowd,andD.Salber. Acontext-basedinfrastruc- Symposium on Handheld and Ubiquitous Computing, pp. 158–170.
tureforsmartenvironments. InManagingInteractionsinSmartEn- Springer,1999.2
vironments:1stInternationalWorkshoponManagingInteractionsin [36] Meta. Metaquestpro. https://www.meta.com/quest/,2022. 1,
SmartEnvironments(MANSE’99),Dublin,December1999,pp.114– 3
128.Springer,2000.2 [37] D. Miniotas, O. Sˇpakov, I. Tugoy, and I. S. MacKenzie. Speech-
[16] A.K.Dey, G.D.Abowd, andD.Salber. Aconceptualframework AugmentedEyeGazeInteractionwithSmallCloselySpacedTargets.
and a toolkit for supporting the rapid prototyping of context-aware InProceedingsofthe2006SymposiumonEyeTrackingResearch&
applications.Human–ComputerInteraction,16(2-4):97–166,2001.2 Applications,ETRA’06,p.67–72.AssociationforComputingMa-
[17] A.K.Dey,D.Salber,G.D.Abowd,andM.Futakawa.Theconference chinery,NewYork,NY,USA,2006.doi:10.1145/1117309.1117345
assistant:Combiningcontext-awarenesswithwearablecomputing.In 2,8
DigestofPapers.ThirdInternationalSymposiumonWearableCom- [38] S. Noy and W. Zhang. Experimental evidence on the productivity
puters,pp.21–28.IEEE,1999.2 effectsofgenerativeartificialintelligence. Science,381(6654):187–
[18] N.FereydooniandB.N.Walker.Virtualrealityasaremoteworkspace 192,2023.2
platform:Opportunitiesandchallenges.2020.2 [39] N.Numan,D.Giunchi,B.Congdon,andA.Steed.Ubiq-genie:Lever-
[19] M.Gonzalez-FrancoandA.Colaco. Guidelinesforproductivityin agingexternalframeworksforenhancedsocialvrexperiences.In2023
virtualreality.Interactions,31(3):46–53,2024.2,3 IEEEConferenceonVirtualRealityand3DUserInterfacesAbstracts
[20] Google. Bard: Google ai and search updates. https://blog. andWorkshops(VRW),pp.497–501.IEEE,2023.2
google/technology/ai/bard-google-ai-search-updates/, [40] E.Ofek,J.Grubert,M.Pahud,M.Phillips,andP.O.Kristensson.To-
2023.Accessed:2023-09-30.1 wardsapracticalvirtualofficeformobileknowledgeworkers. arXiv
[21] Google. Gemini. https://gemini.google.com/app,2023. Ac- preprintarXiv:2009.02947,2020.2
cessed:2023-09-30.1 [41] OpenAI.Chatgpt:Optimizinglanguagemodelsfordialogue.https:
[22] Google. Cloudspeech-to-text,Accessed2024. Accessed: 2024-03- //openai.com/blog/chatgpt/,2022.1[42] L.Pavanatto,C.North,D.A.Bowman,C.Badea,andR.Stoakley.Do
westillneedphysicalmonitors? anevaluationoftheusabilityofar
virtualmonitorsforproductivitywork. In2021IEEEVirtualReality
and3DUserInterfaces(VR),pp.759–767.IEEE,2021.1,2
[43] K.Pfeuffer,H.Gellersen,andM.Gonzalez-Franco. Designprinci-
plesandchallengesforgaze+pinchinteractioninxr.IEEEComputer
GraphicsandApplications,44(3):74–81,2024.2,3
[44] A.Poushneh. Humanizingvoiceassistant: Theimpactofvoiceas-
sistantpersonalityonconsumers’attitudesandbehaviors. Journalof
RetailingandConsumerServices,58:102283,2021.2
[45] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and
I.Sutskever.Robustspeechrecognitionvialarge-scaleweaksupervi-
sion. InInternationalConferenceonMachineLearning,pp.28492–
28518.PMLR,2023.2
[46] K.Rayner.Eyemovementsinreadingandinformationprocessing:20
yearsofresearch.Psychologicalbulletin,124(3):372,1998.3
[47] Y.Romaniak,A.Smielova,Y.Yakishyn,V.Dziubliuk,M.Zlotnyk,
andO.Viatchaninov. Nimble: MobileInterfaceforaVisualQues-
tionAnsweringAugmentedbyGestures. InAdjunctProceedingsof
the 33rd Annual ACM Symposium on User Interface Software and
Technology,UIST’20Adjunct,p.129–131.AssociationforComput-
ingMachinery,NewYork,NY,USA,2020.doi: 10.1145/3379350.
34161532,8
[48] W.N.Schilit. Asystemarchitectureforcontext-awaremobilecom-
puting.ColumbiaUniversity,1995.2
[49] R.Suzuki,M.Gonzalez-Franco,M.Sra,andD.Lindlbauer.Xrandai:
Ai-enabledvirtual,augmented,andmixedreality.InAdjunctProceed-
ingsofthe36thAnnualACMSymposiumonUserInterfaceSoftware
andTechnology,pp.1–3,2023.2
[50] X.Tan,T.Qin,F.Soong,andT.-Y.Liu. Asurveyonneuralspeech
synthesis.arXivpreprintarXiv:2106.15561,2021.2
[51] M. Volonte, E. Ofek, K. Jakubzak, S. Bruner, and M. Gonzalez-
Franco. Headbox: Afacialblendshapeanimationtoolkitforthemi-
crosoftrocketboxlibrary.In2022IEEEConferenceonVirtualReality
and3DUserInterfacesAbstractsandWorkshops(VRW),pp.39–42.
IEEE,2022.3