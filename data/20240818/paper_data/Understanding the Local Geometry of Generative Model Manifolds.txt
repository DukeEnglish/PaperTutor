Understanding the Local Geometry of Generative
Model Manifolds
AhmedImtiazHumayun1,2,IbtihelAmara1,3,CandiceSchumann4,
GolnooshFarnadi1,NegarRostamzadeh1,MohammadHavaei1
1GoogleResearch,2RiceUniversity,3McGillUniversity,4GoogleDeepmind
imtiaz@rice.edu, mhavaei@google.com
Abstract
Deepgenerativemodelslearncontinuousrepresentationsofcomplexdatamani-
foldsusingafinitenumberofsamplesduringtraining. Forapre-trainedgenerative
model, the common way to evaluate the quality of the manifold representation
learned,isbycomputingglobalmetricslikeFréchetInceptionDistanceusinga
largenumberofgeneratedandrealsamples. However,generativemodelperfor-
mance is not uniform across the learned manifold, e.g., for foundation models
likeStableDiffusiongenerationperformancecanvarysignificantlybasedonthe
conditioning or initial noise vector being denoised. In this paper we study the
relationshipbetweenthelocalgeometryofthelearnedmanifoldanddownstream
generation. Basedonthetheoryofcontinuouspiecewise-linear(CPWL)genera-
tors,weusethreegeometricdescriptors–scaling(ψ),rank(ν),andcomplexity
(δ)–tocharacterizeapre-trainedgenerativemodelmanifoldlocally. Weprovide
quantitative and qualitative evidence showing that for a given latent, the local
descriptors are correlated with generation aesthetics, artifacts, uncertainty, and
evenmemorization. Finallywedemonstratethattrainingarewardmodelonthe
localgeometrycanallowcontrollingthelikelihoodofageneratedsampleunder
thelearneddistribution.
ρ=0 ρ=1 ρ=1.5
ρ=0 ρ=−1 ρ=−1.5
Figure1: Controllingsamplingdiversityusinggeometryguidance. Wetrainarewardmodelon
thegeometricdescriptorlocalscalingcomputedforthedecoderofStableDiffusion[22]. Positive
(top-row)ornegativeguidanceρ(bottom-row)viathisrewardmodelallowsdecreasing(top-row)
orincreasing(bottom-row)thelikelihoodofthegeneratedsamplesunderthelearneddistribution.
Aswedecreaselikelihoodmorebackgroundelementscomeintoviewandthefocusonthesubject
decreases,vice-versawhenincreasingthelikelihood.
Preprint.Underreview.
4202
guA
51
]GL.sc[
1v70380.8042:viXra
htiwkcabegdirnaisedohra
yticahtiwelgaeba
dnuorgkcabehtniytica
dnuorgkcabehtniDataManifold,
LatentDomain,Z ∈R2 Im(G)∈R3 z∼U(Z) x=G(z)
Figure2: ExactgeometryofaCPWLmanifold. Foratoycontinuouspiece-wiselinear(CPWL)
generatorG :R2 →R3,weprovideanalyticallycomputedvisualizationoftheinputspacepartition,
i.e.,arrangementoflinearregions(left)andlearnedCPWLmanifold(middle-left). Eachpiecefor
thisexample,iscoloredbythepiecewise-constantscalinginducedbyG thatisanalyticallycomputed.
Uniformsamplesfromthelatentdomain(middle-right)andgeneratedsamples(right)arepresented,
coloredbytheestimateddensityateachsampleusingagaussiankerneldensityestimatorinR3. We
seethatfor anysample z ∈ ω, theestimateddensity(↑green)is inverselyproportionalwiththe
scaling(↓green)forregionω.
1 Introduction
In recent years, deep generative models have emerged as a powerful tool in machine learning,
capableofsynthesizingrealisticdataacrossdiversedomains[15,16,22]. Withtherapidincreasein
generationperformanceofsuchgenerativemodels,wehaveseenademandformorefine-grained
methodsforevaluation. Forexample,globalmetricsofgenerationsuchasFréchetInceptionDistance
(FID)aresensitivetoboththefidelityanddiversityofgeneratedsamples–precisionandrecall[23]
wereproposedtodisentanglethesetwofactorsandquantifythemseparately. Withtheadventof
foundationalgenerativemodelslikeStableDiffusion[22], humanevaluationofimagesinafine-
grainedmannerhasbecomethemodusoperandiforevaluation. Insuchcases,evaluatinggenerative
modelsextendbeyondassessingmeresamplefidelityandconditionalalignment,tobroaderconcerns
aboutbiasandmemorization/duplication. Thetrendthereforeshowsastrongneedforfine-grained
understandingofthegenerativemodelbehaviorapartfromwhatwecanderivefromglobalgeneration
metrics. Inthispaperwelookforanswerstothefollowingquestion:
Question. For any sample generated using a pre-trained generative model, how is the local
geometryofthelearnedmanifoldrelatedtoqualitativeaspectsofthesample?
Webelievethisquestionisespeciallysignificant1)formodelstrainedwithlargeheterogeneousdata
distributions. Duetotheheterogeneity,generationperformancecanvarysignificantlybasedonthe
samplingrateofthedatamanifoldwhentrainingdataisprepared. 2)Foranypre-trainedgenerative
modelwithoutaccesstothetrainingdistribution,sincecomputingglobalmetricslikeFID,precision
orrecallrequirethetrainingdataasgroundtruth.
How do we characterize the learned manifold? To answer the question above, we require
methods to derive local characteristics of a pre-trained generative model manifold for any given
latent-samplepair{z,x}. Alargeclassofgenerativemodels–comprisedofconvolutions, skip-
connections,pooling,oranycontinuouspiece-wiselinear(CPWL)operation–fallundertheumbrella
ofCPWLgenerativemodels. AnyCPWLoperator,canbecharacterizedexactlyin-termsofthe1)
knots/regionsintheinputspacepartitionoftheCPWLoperator,whereeveryknotdenotesthelocation
ofasecond-orderchangeinthefunctionand2)theregion-wiselinear/affineoperation,whichscales,
rotates,andtranslatesanyinputregionwhilemappingittotheoutput[1]. Wethereforeproposeusing
thethreegeometricdescriptorstocharacterizethelearnedmanifoldofaCPWLgenerator:
• Localrank(ν),thatcharacterizesthelocaldimensionalityofthelearnedmanifoldaround
x.
• Localscaling(ψ),thatcharacterizesthelocalchangeofvolumebythegenerativemodel
foraninfinitesimalvolumearoundz.
• Localcomplexity(δ),thatapproximatesthesmoothnessofthegenerativemodelmanifold
intermsofsecondorderchangesintheinput-outputmapping.
2Geometric descriptors such as local scaling, complexity or rank of Deep Neural Networks, have
previously been used to measure function complexity [8] and expressivity [19, 20], to evaluate
thequalityofrepresentationslearnedwithaself-supervisedobjective[6],forinterpretabilityand
visualizationofDNNs,[10],tounderstandthelearningdynamicsinreinforcementlearning[4],to
explain grokking, i.e., delayed generalization and robustness in classifiers [13], increase fairness
ingenerativemodels[11], controlsamplingingenerativemodels[12], andmaximumlikelihood
inferenceinthelatentspace[18].
Our contributions. In this paper, through rigorous experiments on large text-to-image latent
diffusionmodelsandsmallergenerativemodels,wedemonstratethecorrelationbetweenthelocal
geometric descriptors with generation aesthetics, diversity, and memorization of examples. We
provideinsightsintohowthesemanifestfordifferentsub-populationsunderthegenerateddistribution.
Wealsoshowthatthegeometryofthedatamanifoldisheavilyinfluencedbythetrainingdatawhich
enables applications in out-of-distribution detection and reward modeling to control the output
distribution. Our empirical results present the following major observations, which can also be
considerednovelcontributionsofthispaper:
• C1. Thelocalgeometryonthegenerativemodelmanifoldisdistinctfromtheoffmanifold
geometry. Local scaling and local rank are indicative whether a {z,x} pair is out-of-
distribution.
• C2. Astrongrelationshipexistsbetweenthegeometricdescriptorsandgenerationfidelity,
aesthetics,diversityamdmemorizationinlargetext-to-imagelatentdiffusionmodels.
• C3.BytrainingasurrogatemodelonthelocalgeometryofStableDiffusion,wecanperform
rewardguidancetoincrease/decreasesamplinguncertainty.
Thepaperisorganizedasfollows: InSec.2wedefinethelocaldescriptorsfromfirstprinciplesof
CPWLgenerators. InSec.3wefirstshowthatforatoyDDPM[9]C1. holds. Followingthatwe
presentvisualandquantitativeresultsshowingC1. forStableDiffusion. Wealsoprovidequalitative
evidenceforC2. forsamplesfromtherealImagenetmanifold. InSec.3.3.4wedrawconnections
betweenthelocaldescriptorsand1)generationquality2)uncertainty,and3)memorizationforStable
Diffusion. FinallyinSec.4wedemonstratehowlocalgeometrycanbeusedasarewardsignalto
guidegeneration,providingevidenceforC3.
2 GeometricDescriptorsoftheLearnedDataManifold
2.1 ContinuousPiecewise-LinearGenerativeModels
ConsideragenerativenetworkG,whichcanbethedecoderofaVariationalAutoencoder(VAE)[17],
thegeneratorofaGenerativeAdversarialNetwork(GAN)[7]oranunrolleddenoisingdiffusion
implicitmodel(DDIM)[24]. Suppose,G :RE →RD isadeepneuralnetworkwithLlayers,input
spacedimensionalityE andoutputspacedimensionalityD. Foranysuchgenerator,ifthelayers
comprise affine operations such as convolutions, skip-connections, or max/avg-pooling, and the
non-linearitiesarecontinuouspiecewise-linearsuchasleaky-ReLU,ReLU,orperiodictriangle,then
thegeneratorisacontinuouspiecewise-linearorpiecewise-affineoperator[1,10]. Thisimpliesthat
theG :RE →RD mappingcanbeexpressedintermsofasubdivisionoftheinputspaceintolinear
regionsΩandeachregionωbeingmappedtotheoutputdatamanifoldviaanaffineopearation. The
continuousdatamanifoldorimageofthegeneratorIm(G)canbewrittenastheunionofsets:
(cid:91)
Im(G)= {A z+b ∀z ∈ω}, (1)
ω ω
∀ω∈Ω
where,ΩisthepartitionofthelatentspaceRE intocontinuouspiecewise-linearregions,A and
ω
b aretheslopeandoffsetparametersoftheaffinemappingfromlatentspacevectorsz ∈ωtothe
ω
datamanifold. Fortheclassofcontinuouspiecewise-linear(CPWL)neuralnetworkbasedgenerative
models,Ω,A ,andb arefunctionsoftheneurons/parametersofthenetwork. Forageneratorwith
ω ω
Llayers,A andb canbeexpressedinclosed-formintermsoftheweightsandtheregion-wise
ω ω
activationpatternofneuronsforeachlayer. WereferthereaderstoLemma1of[10]fordetails.
2.2 LocalGeometryofContinuousPiecewise-LinearGenerativeModels
For any CPWL function, the input-output mapping can be expressed in terms of the input space
subdivision Ω and the set of piecewise-affine parameters {A ,b ∀ω ∈ Ω}. In this section, we
ω ω
3discusshowthelocalpropertiesofthemanifoldcanalsobedefinedintermsoftheregionsωorthe
region-wiseaffineparameters.
2.2.1 Localcomplexity,δ
ForCPWLfunctions,ageneralnotionofcomplexityisbasedonthenumberofintervalsorpiecesof
thefunction. ForCPWLneuralnetworks,asimilarnotionofcomplexitybasedonnumberoflinear
regionswasproposedby[8]. WedefinelocalcomplexityofaCPWLgeneratorasthefollowing.
Definition 1. For a CPWL generator with input partition Ω, the local complexity δ for a P-
z
dimensionalneighborhoodofradiusraroundlatentvectorzis
(cid:88)
δ = 1 (2)
z ω
∀ω∩Vz̸=∅
whereV ={x∈RE :||B(x−z)|| <r}. (3)
z 1
Here,BisanorthonormalmatrixofsizeP ×E withP ≤E,||.|| istheℓ normoperatorandrisa
1 1
radiusparameterdenotingthesizeofthelocalitytocomputeδ for. Thesumoverregionsω ∈ V
z
requirescomputingΩ∩V whichcanbecomputationallyintractableforhighdimensions. Aproxy
z
forcomputingthepartitionforV withsmallriscountingthenumberofnon-linearitieswithinV ,
z z
sinceforsmallr,theonecanassumethatthenon-linearitiesdonotfoldinsideV ,thereforeproviding
z
anupperboundonthenumberofregionsaccordingtoZaslavsky’sTheorem[26]. Tocomputelocal
complexity,weusethemethoddescribedin[13]toestimatethenumberofknotsintersectingV .
z
LocalcomplexityofCPWLneuralnetworkshavepreviouslybeenconnectedtotheexpressivityof
DeepNeuralNetworks[8,21]. Fromageometricstandpointlocalcomplexitytellsusthesmoothness
of the learned function for a given locality. While local complexity is defined in terms of the
neighborhoodofalatentvectorz,inthispaperweuseitinterchangeablywiththelocalcomplexity
ofthedatamanifoldaroundpointx=G(z).
2.2.2 Localscaling,ψ
Definition2. ForaCPWLmanifoldproducedbygeneratorG,thelocalscalingψ isconstantfor
ω
everyregionωdenotinglogarithmofthescalinginducedbytheaffineslopeA forallvectorsz ∈ω.
ω
Localscalingforωcanthereforebeexpressedas
(cid:113) (cid:88)k
ψ =log( det(ATA ))= log(σ )1 , (4)
ω ω ω i {σi̸=0}
i
where,{σ }i=k,arethenon-zerosingularvaluesofA .
i i=0 ω
Suppose G has a uniform latent distribution, meaning every region ω has a uniform probability
density function at the input. Under an injectivity assumption for an input space region ω and
S ={A z+b ∀z ∈ω}theoutputdensityonS,p (x)∝ 1 .Thereforeforaninjectivemapping,
ω ω S eψω
localscalingcanbeconsideredanun-normalizedlikelihoodmeasure,wherehigherlocalscalingfor
anygivenz ∈ωsignalslowerposteriordensity. Fortworegionsω ∈Ωandω′ ∈Ω,thedifference
inuncertaintycanthereforebewrittenas:
H −H =ψ −ψ , (5)
ω ω′ ω ω′
where,H ,H aretheconditionalentropyonthemanifoldforinputspaceregionsωandω′.
ω ω′
2.2.3 Localrank,ν.
Definition3. ForaCPWLmanifoldproducedbygeneratorG,localrankν isthesmoothrankof
ω
thetheper-regionaffineslopeA andcanbeexpressedas:
ω
(cid:32) k (cid:33)
(cid:88)
ν =exp − α log(α )
ω i i
i
σ
whereα = i +ϵ.
i (cid:80)kσ
i i
Here,{σ }i=k arenon-zerosingularvaluesofA andϵ=10−30 isaconstant. Thelocalrankν
i i=0 ω ω
thereforedenotesthedimensionalityofthetangentspaceonthedatamanifold.
4t=0.88T t=0.66T t=0.44T t=0.22T t=0
Figure 3: Local geometric descriptors {ψt,δt,νt} computed over the input domain of a 2D
toy diffusion model producing distribution zt via reverse diffusion at noise level t, for an initial
zT =N(0,I). Foranyinputvector,descriptorsarecomputedbyconditioningthediffusionmodel
ont. Weseethatwithdecreasingt,localscalingψandlocalrankνtdecreasesandlocalcomplexity
δtincreasesintheproximityofthesupportofz0,i.e.,thelearnedmanifoldbythediffusionmodel.
Therefore,atanytduringthereversediffusionprocess,wecanusethelocaldescriptorscomputed
forlowernoisescales,forexamplet=0.22T,toguidegenerationtowardsorawayfromz0.
2.3 Example: Toygeneratortrainedonaf :R2 →R3task
Setup. WetrainatoygeneratorG :R2 →R3withdepth3andwidth20,tomapthe2-dimensional
latent domain [−10,10]2 onto a toy 2-manifold in a 3-dimensional output space. We train the
generatorwitharegressionlossonagroundtruthmanifolddefinedasamixtureoffivegaussian
functions. InFig.2-left,wepresentanalyticallycomputedvisualization[10]ofthepiece-wiselinear
manifoldlearnedbythegeneratoraswellasthelatentspacepartitionΩrepresentedbydarklines.
Everyblacklinerepresentsanon-linearityofthefunctionthatfolds/bendsthelatentspacewhile
goingfromR2toR3. Therefore,theblacklinesareknotsofthecontinuouspiecewiseaffinespline
generator. Eachconvexregionωformedbytheintersectionoftheblacklines,ismappedtoIm(G)
viaperregionparametersasdescribedinEquation1. Eachregionintheinputandoutputpartitionis
coloredbyψ . Wealsouseakerneldensityestimator(KDE)toestimatethedensityofgenerated
ω
samples(right)onthedatamanifoldforauniformlatentdistribution(middle-right),andcolorsamples
withtheestimateddensity.
Observations. Aftertrainingthegenerator,thepre-activationzero-levelsets(Fig.2)ofneuronsare
positionedwherechangestotheslopeofthemanifoldisrequired. Thedensityoflinearregions,
i.e.,localcomplexityishigherinthecenterandlowertowardstheedges. A isfull-rank∀ω ∈Ω.
ω
ContrastingtheKDEestimateofdensityandlocalscaling,wecanseethatforhigherψ(bluehue)
wehavelowerestimateddensity(bluehue)andvice-versa.
3 ExploringGenerativeModelManifoldsusingDescriptors
InthisSectionwewillbeexploringthegeometryofthedatamanifoldslearnedbyvariousgenerative
models,e.g.,denoisingdiffusionprobabilisticmodels,latentdiffusionmodelslikeStableDiffusion
[22].
5
tz
tψ
tδ
tν3.1 DDPMtrainedontoyf :R2 →R2generationtask.
Setup. Wetrainadenoisingdiffusionprobabilisticmodel[9]onatoydataset1tovisualizehowthe
localgeometryofthelearnedmanifoldvarieswith1)de-noisingsteps/noiselevels2)gradientdescent
steps. InFig.3wepresentthelocalcomplexityδt,localscalingψt andlocalrankνt attimesteps
t ∈ {6,17,28,39,50}. We consider a grid of points in the 2D data space and compute the local
descriptorsateachpoint. Notethatwedroptheω notationaswedon’tcomputeΩandnolonger
computethescalingandrankinaregion-wisefashion. ToobtainA atanyvectorz ∈ωwhereωis
ω
unknown,wecantaketheinput-outputjacobianatz(seeEq.1). Weaddasuperscriptttodenotethe
descriptorsbeingcomputedforthelearnedoutputmanifoldatdenoisingstept. ψtandνttherefore
denotesthelocalscalingandrankbythemappingfromttot+1.
Observations. Weseethatthelocalcomplexityishigheraroundthesupportofztforallnoiselevels
tandbothtrainingstepsshowninFig.3andFig.19. Bothlocalscalingandrankareloweraround
thedatamanifolduntilthelastfewdiffusiontimesteps,whenthevarianceofψ andν diminishes.
Thequalitativeresultssuggestthatgiventwodiffusionmodels(hereoneisanearlycheckpointand
theotherisalatecheckpoint),localgeometryaroundthetrainingdataforthebettertrainedmodel
wouldhavehigherδt,andlowerψt,µt.
3.2 VAEtrainingdynamicsforMNIST
Setup. We train a Variational Auto Encoder (VAE) on the MNIST dataset with width 128
and depth 5 for both encoder and decoder. We add Gaussian noise with standard deviation
{0,0.0001,0.001,0.01,0.1} to the training data. Initialization was not kept fixed. In Fig. 4, we
presentplotsshowingthetrainingdynamicsoflocalcomplexityandscaling,averagedoveralltest
datasetpointsfromMNIST.
δ ψ
Figure4: Trainingdynamicsofgeometric
400 40 0.0 Noise s 0t .d 001 0.1 descriptors for a VAE trained on MNIST
350 50 0.0001 0.01 with additive noise. As training progresses
localcomplexityδincreasesandlocalscaling
300 60
ψdecreasessuggestinganincreaseinexpres-
250 70 sivityanddecreaseinuncertaintyonthedata
80
200 manifold. Atlattertime-steps,ψ ↓ andδ ↑
90
0 2500 5000 7500 100 o00 p1 t2 i5 m00 i1 z50 a00 tion0ste25p00s 5000 7500 100001250015000 ifnoisestd. isincreased.
Observations. Byincreasingthenoisewecontrolthepuffinessofthetargetmanifold. Weobserve
that as the noise standard deviation is increased there is 1) increase in δ indicating the manifold
becomeslesssmooth2)decreaseinlocalscalingindicatingthattheuncertaintydecreases. Wecan
alsoobserveaninitialdipinbothlocalcomplexityandlocalscaling. Thisissimilartowhatwas
observed for discriminative models in [13] where a double descent behavior was reported in the
localcomplexitytrainingdynamicsofclassificationmodels. Basedontheseresults,contrarytothe
observationin[13],generativemodelsdonothaveadoubledescentinlocalcomplexityhowever
wedoobserveadoubleascentinlocalscaling. Ourobservationssuggestthatthetrainingdynamics
needtobetakenintoaccount,whencomparingthelocalmanifoldgeometrybetweentwoseparately
trainedmodels.
3.3 Pre-trainedLatentDiffusionFoundationModel
Inthissectionweexplorethelocalgeometryofapre-trainedlatentdiffusionmodelnamelyStable
Diffusionv1.4[22]. StableDiffusion(SD)comprisesofanunconditionalvectorquantizedVAE
(VQVAE)andaconditionaldiffusionmodelinthelatentspaceoftheVQVAE.Thediffusionmodel
istrainedtomapagaussiandistributiontothelatentdomainofthedecoder,thereforewefocusour
experimentsonstudyingthegeometryoftheunconditionaldecoder.
3.3.1 Visualizingthelocalgeometry
Setup. Weusethreeprompts"acat","adog"and"afox"togeneratethreelatentvectorsusingthe
SDdiffusionmodel. Weconsidera2Dsliceinthelatentspace,goingthroughthethreedenoised
latents,andcomputethelocaldescriptorsona512×512uniformlyspacedgrid. Wecomputeδfora
4-dimensionalneighborhoodofradiusr = 0.00001. TomakeA 1 computationtractable,for
ω z∈ω
anylatentvectorz wetakea120-dimensionalrandomorthonormalprojectionoftheSDdecoder
1https://jumpingrivers.github.io/datasauRus
6δ ψ ν
Anchors
Figure5: GeometryoftheStableDiffusionlatentspace. Geometricdescriptors(left,middle-left,
middle-right)visualizedona2Dlatentspacesubspace,thatpassesthroughthelatentrepresentations
of "a fox", "a cat" and "a dog" (right), denoted via markers on the 2D subspace descriptor. In
Appendix,weprovidedenoisedimagesfordifferenthigh/lowdescriptorregionsfromthesubspace.
Weseethatintheconvexhullofthethreeanchorlatentvectorsψ ↑,ν ↓andδ ↑. Moreoverwesee
thatintheconvexhull,thelocalrankν undergoessharpchangeswhicharenotvisibletowardsthe
edgesofthedomain.
Increasingψ
Figure6:LocalScalingforencodedImageNetimages.ImageNetimagesorderedalongthecolumns
(fromlefttoright),withincreasinglocalscalingψoftheStableDiffusiondecoderlearnedmanifold.
WeobservethatImageNetsampleswithlowervaluesofψcontainsimplerbackgroundswithmodal
representationoftheobjectcategory. Converselyforhigherψwehaveincreasingdiversitybothin
backgroundandforegroundfeatures.
manifold[3]andcomputetheinput-outputjacobianwiththeprojectionastheoutput. WeuseaJAX
implementationofStableDiffusiononTPUresultingin3.6srequiredforcomputingjointlyδ,ψ,and
νforonelatentvectorz. InFig.5wepresenttheδ,ψ,ν,onthe2Dslice,alongwithdecodedversions
ofthethreelatentvectors,usedtocomputethe2Dslice. InAppendixFigs.25,26,21,22,23,and
24wepresentgeneratedimagesfromthehighorlowlocaldescriptorregionsfromthe2Dslice.
Observations. SimilartowhatwasobservedforaDDPMtrainedontoydata(seeSec.3.1),we
observethat1)intheconvexhullofthethreedenoisedlatents,wehavehighercomplexityindicating
itsproximitytotheSDdecoderdatamanifold2)lowerrankintheconvexhullaffirmsthattheconvex
hullhasproximitytothedatamanifoldaswell. Howeverweseeasparsecollectionoflatentsfor
whichtherankdropsfurther3)aswemoveinsidetheconvexhull, localscalingincreases. This
indicatesthataconvexcombinationof"afox","acat"and"adog"ishigheruncertaintyw.r.tthe
SDdecoder. Howeverifwemoveawayfromtheconvexhullweseethatbothδ andψ decreases
whilerankincreases. Thisindicatesthataswemoveawayfromthedatamanifold,thelocalscaling
descriptorcollapsesassuchregionsarenolongerwithinthesupport/latentdomainoftheSDdecoder.
ThisisfurtherillustratedthroughqualitativeexamplesinAppendixFig.21andFig.22.
3.3.2 LocalgeometryofImageNet
Setup. Giventheobservationsfromtheprevioussections,weaskthequestion"Whatcanlocal
geometryofafoundationmodelsayaboutadataset?". Tofindananswer,weconsider20K samples
fromImagenetwithresolutionhigherorequalto512×512,encodethesamplesusingtheSDencoder,
7LocalScaling,ψ LocalRank,ν
Chest X-ray
Imagenet
100 80 60 40 20 0 106 108 110 112 114 116 118 120
Figure7: Geometrydistinguishesout-of-distributionregions. WeencodechestX-rayvsImagenet
images and present here local scaling (left) and local rank (right) distributions computed for the
StableDiffusiondecoder. HerechestX-rayimagesarerepresentativeofout-of-distributionsamples
withrespecttotheStableDiffusionlearneddistribution, andImagenetimagesareconsideredin-
distribution. WeseethatforOODsamples,theexpectedψ ishigher,showinghigheruncertainty
under the learned distribution. The local rank is also higher for OOD samples, which correlates
with our observations from Fig. 5. These results indicate that the local geometry can be used to
characterizesamplesthatareout-of-distributionwithrespecttoapre-trainedgenerativemodel.
andstudythelocalgeometryoftheSDdecodermanifoldfortheencodedImagenetlatents. InFig.6
wepresentsamplesfromImagenetorderedwithincreasingψ. InAppendixFig.14wepresentimages
wherethemanifolddimensionalityisthelowestandhighestfortheImagenetsamplesconsidered.
Observations. InFig.6eachcolumnrepresentsalocalscalinglevelset,withincreasingvalues
fromlefttoright. RecallthatinEq.5weshowthatincreaseinlocalscalingisequivalenttoincrease
in uncertainty. In this figure, we can see that for lower uncertainty images we have more modal
featuresintheimages,i.e.,thesampleshavelessbackgroundelementsandarefocusedonthesubject
correspondingtotheImagenetclass. Forhigheruncertaintyimages,weseethatimageshavemore
outliercharacteristics. ForimageswithhigherlocalinFig.14,weseethatthebackgroundshave
higherfrequencyelementscomparedtolowerrankimages.Forhigherrankimages,thedimensionality
of the manifold is higher locally, therefore allowing more noise dimensions on the manifold. In
Appendix. Fig.15wepresentclass-wiseexamplesforhighandlowlocalscalingimages.
3.3.3 DetectingOut-of-DistributionSamples
Setup. Tostudyifthegeometricdescriptorsarediscriminativeofout-of-distributionsamples,we
considerevaluatingthemetricsfor20K randomlyselectedImagenetsamplesrepresentingin-domain
imagesforStableDiffusionand20K X-RayimagesfromtheCheXpert[14]datasetrepresenting
out-of-domainimagesforStableDiffusion. WepresentthedistributionsinFig.7.
Observations. WeseethatX-rayimageshavehigheruncertaintyandhigherlocalrankinexpecta-
tioncomparedtoImagenetimages. Especiallyforlocalrankν,ChestX-rayimagesfromChexpert
andimagenetimagedistributionsarealmostseparable. Thiscorroborateswithourvisualizations
fromFig.5,whereawayfromtheconvexhullofthedenoisedlatents,weseeanincreaseinν.
3.3.4 RelationshipwithQuality,Diversity&Memorization
Qualityanddiversityofgenerationforapre-trainedmodel,isdirectlyconnectedtotheestimation
ofthetargetmanifold. Inthissection,westudytherelationshipbetweenthegeometricdescriptors
andgenerationquality(goodestimationofthemanifold),generationdiversity(largesupportofthe
learneddistribution)andmemorization(interpolationofthetargetmanifoldatadatapoint).
Setup. Theexperimentsareorganizedassuch:
• We denoise a fixed set of latent vectors using a) varying guidance scale {0,1,3,5,7.5,9}
to control quality-diversity trade-off or, b) with memorized/non-memorized prompts by Stable
Diffusion. WeobtainmemorizedpromptsforSDfrom[25]andnon-memorizedpromptsfromthe
COCOdatasetvalidationset.
• For a) 50K real ImageNet images b) 50K synthetic images generated by SD from in-
creasingdescriptorlevelsetswecomputetheVendiScore[5]ofclipembeddings,theaestheticscore
predicted by an aesthetic reward model and a artifact reward model. For the synthetic Imagenet
images, we use a guidance scale of 7.5 and the prompt template "an image of p" where p is a
randomlyclassfromImagenet.
Observations. We see in Fig. 8-bottom that for higher classifier free guidance scales during
denoising, uncertainity is lower especially during the final denoising steps. This shows a clear
correlationwithqualitysincehigherguidancescalesresultinhigherqualityimages[22]. Wealsosee
8thatforhigherguidancescaleslocalrankν is↓andlocalcomplexityδis↓aswell. Formemorized
prompts,inFig.8-top,weseethattheψissignificantlylowerthannon-memorizedprompts,indicating
verylowuncertainityinmemorizedsamplescomparedtonon-memorizedsamples. Wealsoobserve
themanifoldtobehigherdimensionalformemorizedpromptsandsmoother.
LocalScaling,ψ LocalRank,ν LocalComplexity,δ
119.8
90 119.6 0.0200 M COem COo r Piz roe md P pr to smpts
100 119.4 0.0175
119.2 0.0150
110
Memorized Prompts 119.0 Memorized Prompts 0.0125
120 COCO Prompts 118.8 COCO Prompts 0.0100
0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50
119.8
0.025 Guidance Scale
40
119.6 0 5
45 119.4 0.020 1 3 7 9.5
119.2
0.015
50 119.0
118.8 0.010
10 20 30 40 50 30 35 40 45 50 10 20 30 40 50
Denoisingsteps
Figure 8: Local geometry of denoising trajectories. Geometric descriptors computed for the
VQGANdecoder,during50stablediffusiondenoisingsteps,for(top)100COCOand100memorized
prompts[25]withguidancescale7.5and(bottom)100COCOpromptswithvaryingguidancescales.
Foreachpromptorguidancescale,westartfromthesameseeds. Shadedregionrepresents95%
confidenceinterval. Weseethatthelocalgeometrytrajectoriesarediscriminativeofmemorization,
aswellasincreasedqualitywhenstrongerclassifierfreeguidanceisused.
VendiScore AestheticScore ArtifactScore
LevelSetIndex
Figure 9: Local Geometry level sets for Imagenet images. Vendi scores, Aesthetic scores and
Artifactscorescomputedfor50kgeneratedandrealsamplesofImageNetfromlocalscaling(top)
and local rank (bottom) descriptor level sets of Stable Diffusion. Scores are computed for each
level set separately. Synthetic samples are generated using a guidance scale of 7.5. We see that
diversityincreasesforhigherscalingandranklevelsetsforbothrealandsyntheticdata. Butthe
aesthetic/artifactscoreremainsmoreorlessconstantforsyntheticdata,sinceahighguidancescaleis
used. Thisshowsthatbyusinglocalgeometrytoguidegeneration,wecanincreasethediversityof
generationwhilemaintaininggenerationaesthetics.
InFig.9,weseethatwithincreasingψandν diversityincreases,howeverforveryhighψ,i.e.,very
uncertainimages,thediversitycollapses. Thisisduetothehigherψlevelsetsconvergingtowards
theanti-modesofthelearneddistribution. Weseethatforrealimages,aestheticandartifactscores
(higherifwithoutartifacts)getreducedforhigherlocalscalingandranklevelsets. Howeverfor
generatedimagesforhigheruncertaintytherewardssaturate. Thisisanindicationthatevenwithless
perceptual/aestheticchanges,wecangeneratehigheruncertaintyimages.
94 GuidingGenerationWithGeometry
RewardModeling. Inthepreviouschapters,weestablishedthatgeometricdescriptorsarecorrelated
with out-of-distribution detection, memorization, and aesthetics. Our primary focus has been on
analyzing the geometry of the learned data manifold. In this section, we demonstrate that these
geometricdescriptorscanbeutilizedtointerveneinthelatentrepresentationsofgenerativemodels,
resultinginmeaningfuleffectsonthegeneratedexamples. Leveragingrecentinstance-levelguidance
methods,suchasthosedescribedintheUniversalGuidancepaper[2],wecaneffectivelyinfluence
thelatentstoproducesignificantalterationsinthegeneratedoutputs. Toguidethegenerationprocess,
weemploylocalscalingasourgeometricdescriptor. Weuseclassifierguidancetoperformgradient
ascentwithstepsizeρongradientsderivedfromlocalscaling. Thismethodallowsustosteerthe
generativeprocessbymaximizingthelocalscalingdescriptor,thusalteringthegeneratedexamples
inacontrolledmanner.
Figure10: Rewardguidanceonstablediffusion(maximizingthereward). Localscalereward
guidanceincreasesfromlefttorightineachpicture,withthefirstimageshowingnorewardguidance.
Weobservemaximizingtherewardleadstosharperdetails,improvedsharpnessandcontrast,and
higherdiversityintheimages. Promptsusedforgenerationfromtoptobottom: “aRhodesianridge-
backwithacityinthebackground”,“aBeagleinthesnow”,“aAustralianterrieronacobblestone
street”
Figure11: Rewardguidanceonstablediffusion(minimizingthereward). Localscalereward
guidancedecreasesfromlefttorightineachpicture,withthefirstimageshowingnorewardguidance.
Weobservethatdecreasingthelocalscaleleadstominimizeduncertainty,resultinginanoticeable
blurringeffectandlossofdetailsintheimage. Promptsusedforgenerationfromtoptobottom:“a
Beaglewithacityinthebackground”,“aSamoyedwithamountaininthebackground”,“aAustralian
terrieronacobblestonestreet”.
Toenableuniversalguidance,weneedtocomputethegradientoftherewardfunction. Inourcontext,
thisentailscalculatingtheHessianmatrix,whichiscomputationallyexpensive. Instead,wetraina
10rewardmodeltoapproximatelocalscaling,givennoisylatentsattimesteptofthediffusionchain.
Wetransformtheregressiontaskofestimatingacontinuouslocalscalingvalueintoaclassification
taskbyfirstdeterminingthemaximumandminimumrangeoflocalscalingvaluesfortheImageNet
dataset. Wethendiscretizethisrangeintofiveuniformbins. Ourtrainingdatacollectionprocess
isasfollows: ForeachimageintheImageNetdataset,wesample10timesteps,applynoisetothe
encodedlatentsaccordingtothesampledtimestep,andcomputethecorrespondinglocalscaling.
Subsequently,wetrainaclassificationmodelonpairsoflatentsandtheirassociatedlocalscaling
bins.
Ourexperimentsrevealthatmaximizinglocalscalinginthemanifoldofastablediffusionmodel
directlycorrelateswithaddingtexturetothegeneratedimages. Moreover,thisapproachenhances
diversitybasedonsingleimages. Byoptimizingthelocalscalingdescriptor,thegenerativemodel
isguidedtowardsproducingmorevariedandtexturedoutputs. Thisapproachisnotablebecause
traditionalmethodsfordiversityguidancegenerallyfunctionatthedistributionlevel. Ourmethod,
however,focusesonmaximizingtheinherentdiversityaspreservedbythemodelwithinitslearned
manifold,effectivelysteeringthegeneratedimagestowardstheextremitiesofthedistribution. This
instance-levelinterventionallowsforamoredetailedandpreciseenhancementofdiversity,presenting
anovelapproachtoguidinggenerativemodels.
AsseenfromFig.10maximizingtherewardresultsinaddeddetailsinformofsharpeningtheimage,
addingtextureandcontrast. Wealsoobservethatifwemovetowardsminimizingthereward,the
imagestendtoloosefine-graineddetailsasseeninFig.20.
5 Conclusion&FutureDirections
Inthispaper,weproposedanovelself-assessmentapproachtoevaluategenerativemodelsusing
geometry-baseddescriptors–localscaling(ψ),localrank(ν)andlocalcomplexity(δ)-effectively
whileutilizingonlythemodel’sarchitectureandweights. Ourapproachcharacterizesuncertainty,
dimensionality, and smoothness of the learned manifold without requiring original training data
or human evaluators. Our experiments demonstrated how these descriptors relate to generation
quality,aesthetics,diversity,andbiases. Weshowedthatthegeometryofthedatamanifoldimpacts
out-of-distributiondetection,modelcomparison,andrewardmodeling,enablingbettercontrolof
outputdistribution. Whileusingthegeometryofmanifoldsoffersanovelapproachtoself-assess
generativemodels,weacknowledgetwomainlimitationsthatwarrantfurtherinvestigation. First,the
geometryofthelearnedmanifoldisinherentlyinfluencedbythetrainingdynamicsofthemodel. A
deeperunderstandingofthisrelationshipisneededtofullyleveragegeometricanalysisformodel
assessmentandimprovement. Second,thecomputationalcomplexityofourmethod,particularly
thecalculationoftheJacobianmatrix,posesapracticalchallenge,especiallyforlarge-scalemodels.
Futureworkshouldexploremoreefficientalgorithmsorapproximationstoaddressthislimitation.
References
[1] RandallBalestrieroetal. Asplinetheoryofdeeplearning. InInternationalConferenceonMachine
Learning,pages374–383.PMLR,2018.
[2] ArpitBansal,Hong-MinChu,AviSchwarzschild,SoumyadipSengupta,MicahGoldblum,JonasGeiping,
andTomGoldstein. Universalguidancefordiffusionmodels. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages843–852,2023.
[3] RichardGBaraniukandMichaelBWakin. Randomprojectionsofsmoothmanifolds. Foundationsof
computationalmathematics,9(1):51–77,2009.
[4] SetarehCohan,NamHeeKim,DavidRolnick,andMichielvandePanne. Understandingtheevolution
oflinearregionsindeepreinforcementlearning. AdvancesinNeuralInformationProcessingSystems,
35:10891–10903,2022.
[5] DanFriedmanandAdjiBoussoDieng.Thevendiscore:Adiversityevaluationmetricformachinelearning.
TransactionsonMachineLearningResearch,2023.
[6] QuentinGarrido,RandallBalestriero,LaurentNajman,andYannLecun. Rankme:Assessingthedown-
streamperformanceofpretrainedself-supervisedrepresentationsbytheirrank.InInternationalConference
onMachineLearning,pages10929–10974.PMLR,2023.
[7] I.JGoodfellow,J.Pouget-Abadie,M.Mirza,B.Xu,D.Warde-Farley,S.Ozair,A.Courville,andY.Bengio.
Generativeadversarialnets. InProceedingsofthe27thInternationalConferenceonNeuralInformation
ProcessingSystems,pages2672–2680.MITPress,2014.
11[8] Boris Hanin and David Rolnick. Complexity of linear regions in deep networks. arXiv preprint
arXiv:1901.09021,2019.
[9] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesinneural
informationprocessingsystems,33:6840–6851,2020.
[10] AhmedImtiazHumayun,RandallBalestriero,GuhaBalakrishnan,andRichardGBaraniuk. Splinecam:
Exactvisualizationandcharacterizationofdeepnetworkgeometryanddecisionboundaries.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages3789–3798,2023.
[11] AhmedImtiazHumayun,RandallBalestriero,andRichardBaraniuk.Magnet:Uniformsamplingfromdeep
generativenetworkmanifoldswithoutretraining.InInternationalConferenceonLearningRepresentations,
2021.
[12] AhmedImtiazHumayun,RandallBalestriero,andRichardBaraniuk. Polaritysampling: Qualityand
diversitycontrolofpre-trainedgenerativenetworksviasingularvalues. InCVPR,pages10641–10650,
2022.
[13] AhmedImtiazHumayun,RandallBalestriero,andRichardBaraniuk. Deepnetworksalwaysgrokandhere
iswhy. arXivpreprintarXiv:2402.15555,2024.
[14] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik
Marklund,BehzadHaghgoo,RobynBall,KatieShpanskaya,etal. Chexpert:Alargechestradiograph
datasetwithuncertaintylabelsandexpertcomparison. InProceedingsoftheAAAIconferenceonartificial
intelligence,pages590–597,2019.
[15] TeroKarras,SamuliLaine,andTimoAila. Astyle-basedgeneratorarchitectureforgenerativeadversarial
networks. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages4401–4410,2019.
[16] TeroKarras,SamuliLaine,MiikaAittala,JanneHellsten,JaakkoLehtinen,andTimoAila. Analyzingand
improvingtheimagequalityofstylegan. InProc.CVPR,pages8110–8119,2020.
[17] DiederikPKingmaandMaxWelling. Auto-encodingvariationalbayes. arXivpreprintarXiv:1312.6114,
2013.
[18] LineKuhnel,TomFletcher,SarangJoshi,andStefanSommer. Latentspacenon-linearstatistics. arXiv
preprintarXiv:1805.07632,2018.
[19] BenPoole,SubhaneilLahiri,MaithreyiRaghu,JaschaSohl-Dickstein,andSuryaGanguli. Exponential
expressivityindeepneuralnetworksthroughtransientchaos.InAdvancesInNeuralInformationProcessing
Systems,pages3360–3368,2016.
[20] MaithraRaghu,BenPoole,JonKleinberg,SuryaGanguli,andJaschaSohlDickstein. Ontheexpressive
powerofdeepneuralnetworks. InICML,pages2847–2854,2017.
[21] MaithraRaghu,BenPoole,JonKleinberg,SuryaGanguli,andJaschaSohl-Dickstein. Ontheexpressive
powerofdeepneuralnetworks. arXivpreprintarXiv:1606.05336,2016.
[22] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels.2022ieee. InCVFConferenceonComputerVisionand
PatternRecognition(CVPR),pages10674–10685,2021.
[23] Mehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assessing
generativemodelsviaprecisionandrecall. arXivpreprintarXiv:1806.00035,2018.
[24] JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. arXivpreprint
arXiv:2010.02502,2020.
[25] YuxinWen,YuchenLiu,ChenChen,andLingjuanLyu.Detecting,explaining,andmitigatingmemorization
indiffusionmodels. InTheTwelfthInternationalConferenceonLearningRepresentations,2024.
[26] ThomasZaslavsky.Facinguptoarrangements:Face-countformulasforpartitionsofspacebyhyperplanes:
Face-countformulasforpartitionsofspacebyhyperplanes,volume154. AmericanMathematicalSoc.,
1975.
12A Appendix/Supplementalmaterial
B BroaderImpactStatement
Ourproposedframeworkforassessingandguidinggenerativemodelsthroughmanifoldgeometry
offersseveralpotentialbenefitstosociety. Byprovidingamoreobjectiveandautomatedapproach,
wecansignificantlyreducethecostandtimeassociatedwithhumanevaluation,makingtheauditing
andmitigationofbiasesinlarge-scalemodelsmoreaccessibleandefficient. Thishasimplications
for promoting fairness and equity in AI systems, particularly in domains where biases can have
significantsocietalconsequences.
Furthermore, our approach can empower researchers and practitioners to better understand the
relationshipbetweenthegeometryoflearnedrepresentationsandvariousaspectsofmodelbehavior,
suchasgenerationquality,diversity,andbias. Thisdeeperunderstandingcaninformthedevelopment
ofmorerobustandreliablegenerativemodels,leadingtoadvancementsinvariousfields,including
art,design,healthcare,andeducation.
However,werecognizethatourapproachisnotwithoutlimitationsandpotentialrisks. Whileitcan
beavaluabletoolforidentifyingandmitigatingbiases,itshouldnotandcannotfullyreplacehuman
annotators,especiallyinhigh-riskdomainswherehumanjudgmentandcontextualunderstandingare
crucial. Ourmethodfocusesonreducingcostsandimprovingtheauditingprocess,butitshouldnot
beusedasastandaloneapproach.
Moreover, theincreasedautomationenabledbyourapproachraisesconcernsaboutthepotential
displacementofhumanannotators,leadingtojoblossesandeconomicdisruptions. Whileourmethod
addressessomeaspectsofmodelevaluation,itisnotcomprehensiveandcannotassessallfacetsof
modelbehavior. Therefore,itshouldbeusedwithcautionandinconjunctionwithotherevaluation
methods,includinghumanexpertise.
C ExtraFigures
13Figure12: Imagesgeneratedduring50diffusiondenoisingstepsfortoptobottom,COCOprompts
generated with guidance scale 1,5,9 and memorized prompts generated with guidance scale 7.5.
Higher guidance scale images, as well as memorized images, tend to resolve faster during the
denoisingprocess.
a rich swiss person a bartender
a poor swiss person a crane operator
180 160 140 120 100 80 220 200 180 160 140 120 100
Localscaling(log),ψ
Figure13: Sub-populationdifferencesoflocaldescriptorsingenerateddata. Separationbetween
theψ distributionsfor1000imagesgeneratedforeachpromptina photo of a {rich, poor}
swiss person(left)anda photo of a {bartender, crane operator}(right). Weseethat
theuncertaintyasexpressedbyψcanvarybetweensub-populationsindicatinganunderlyingbiasin
generationdiversity.
14Lowlocalrank,ν ↓ Highlocalrank,ν ↑
Figure14: Imageswiththelowest(left)andhighest(right)localrankνfromasetof20000randomly
sampled ImageNet dataset samples. Low rank images contain simpler textures for every class
comparedtothehighranksamples. Thisisbecauseforimageswithhigherlocalrank,thelearned
manifoldishigherdimensionalthereforeallowinghigherindependentdegreesofvariationslocally
forthegeneratedimages.
Lowlocalscaling,ψ ↓ Highlocalscaling,ψ ↑
Figure15: Imagenetimageswithhighandlowlocalscalingforthestablediffusiondecoder. Each
coordinateinbothleftandrightimagegrids,correspondtothesameimagenetclass.
100 6.0
0.14 0.14 0.14
5.5
0.12 0.12 0.12 80 5.0
0.10 0.10 0.10 4.5 60
0.08 0.08 0.08 4.0
3.5 0.06 0.06 0.06 40
3.0
0.04 0.04 0.04
20 2.5
0.02 0.02 0.02 2.0
0.00 0.00 0.00 0 1.5
80 70 60 50 40 1 2 3 4 5 6 0 20 40 60 80 100 80 70 60 50 40 80 70 60 50 40
Log-Scaling Local Rank Local Complexity Log-Scaling Log-Scaling
Figure16: Bivariate(ϵ,ψ)and(ϵ,ν)distributionsfortrainingsamplesingreen(top),and(δ,ψ),
(ν,ψ)distributionsfortrainingingreenandgeneratedsamplesinred(bottom). Localcomplexityδ
exhibitslinearcorrelationwithϵandψ. Lowestϵsamplesalsohavelowestδ,indicatingthatgood
reconstructionrequireslinearinterpolationofthedatamanifold. Localrankν uncorrelatedwith
ϵ. Bothδandν exhibitsomelinearrelationshipwithψ. Notethatthemodesofrealandgenerated
samplesforthe(ψ,δ)and(ψ,ν)distributionsarenotcompletelyaligned.
Figure17: Localscalingdynamicswhilefine-tuningalatentdiffusionmodelonChest-Xrayimages.
Thelocalscalingiscomputedforgeneratedchest-xrayimages,wherewecanseeareductionoflocal
scaling,i.e.,uncertainty.
15
EAM EAM EAM
ytixelpmoC
lacoL
knaR
lacoLIncreasingψ→ Increasingδ→ Increasingν →
Figure18: Geometricdescriptorsofthedatamanifold. Levelsetsofdatamanifolddescriptorsfor
aBeta-VAEtrainedunconditionallyonMNIST.Fromlefttoright,wepresenttrainingsamples(top
row)andgeneratedsamples(bottomrow)forlinearlyincreasinglevelsetsoflocalscaling(ψ)from
[−80,−42],localcomplexity(δ)from[0,120]andlocalrank(ν)from[1.5,5.5]. Notalllevelsets
hadanequalnumberofsamplesfromtraining/generateddistributions. Weseethatforhigherψ,we
havemoreoutliersampleswhereasforlowerψwehavemodalsamples. Forincreasingδweseethat
thequalityofgeneratedsamplesdecreasesandthediversityofsamplesisreducedaswell. Forhigher
ν digitsbecomemoreregularlyshaped.
t=6 t=17 t=28 t=39 t=50
Figure19: After11395optimizationsteps. Geometryofadiffusionmodelinput-outputmapping,
trainedtoonatoy2Ddistribution. Localscalinglowerarounddatamanifold,localcomplexityhigher
aroundmanifold,rankisloweraroundmanifoldaswell. t=50hasconsiderablylowvarianceinlocal
scalingshowingthatfinaltimestephasadiminishingchangeofdensity.
16
niarT
.neG
tz
tψ
tδ
tνFigure20: Rewardguidanceonstablediffusion(maximizingthereward).
Figure21: Decodedimages(right)using20latents(left)fromthe2Dsubspace,withhighestψ. Each
imageboundingbox(right)iscolorcodedaccordingtothecorrespondinglatentvector(left).
17Figure 22: Decoded images (right) using 20 latents (left) from the 2D subspace, with lowest ψ.
Eachimageboundingbox(right)iscolorcodedaccordingtothecorrespondinglatentvector(left).
SelectedlatentslieoutsidethedomainoftheVQGANlatentspace.
Figure 23: Decoded images (right) using 20 latents (left) from the 2D subspace, with highest ν.
Eachimageboundingbox(right)iscolorcodedaccordingtothecorrespondinglatentvector(left).
SelectedlatentslieoutsidethedomainoftheVQGANlatentspace.
Figure24: Decodedimages(right)using20latents(left)fromthe2Dsubspace,withlowestν. Each
imageboundingbox(right)iscolorcodedaccordingtothecorrespondinglatentvector(left).
18Figure25: Decodedimages(right)using20latents(left)fromthe2Dsubspace,withhighestδ. Each
imageboundingbox(right)iscolorcodedaccordingtothecorrespondinglatentvector(left).
Figure26: Decodedimages(right)using20latents(left)fromthe2Dsubspace,withlowestδ. Each
imageboundingbox(right)iscolorcodedaccordingtothecorrespondinglatentvector(left).
Train
10 Test
Gen.
5
0
80 60 0 50 100 2 4
ψ δ ν
Figure 27: Vendi score [5] calculated for samples from different local descriptor level sets of a
Beta-VAE.Wetakeupto150samplesfromeachlevelsetandcomputevendiscoreseperatelyforthe
MNISTtraindataset,testdatasetandgeneratedsamples.
190
1
2
80 60 0 50 100 2 4
ψ δ ν
Figure28: Sub-populationdifferencesof localdescriptorsintrainingdata. Class-wiselocal
descriptordistributionsforaBeta-VAEtrainedunconditionallyonMNIST.WeusetheVAEencoder
toobtainlatentvectorsforthetrainingdatasetandcomputedescriptorsusingtheVAEdecoder. We
canobserveclassspecificseparationinthedistributionsindicatingthatgeometricpropertiesmaybe
differentfordifferentsub-populationswithinthetrainingdataset.
20