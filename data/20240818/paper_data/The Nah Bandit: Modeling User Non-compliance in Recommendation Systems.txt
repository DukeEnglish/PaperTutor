JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 1
The Nah Bandit: Modeling User Non-compliance in
Recommendation Systems
Tianyue Zhou, Jung-Hoon Cho, Cathy Wu
Abstract—Recommendation systems now pervade the digital as pulling arms of the bandit [1], they often overlook a key
world, ranging from advertising to entertainment. However, scenario in the physical world: users can easily opt out of
it remains challenging to implement effective recommendation
taking any recommended option if it is not to their liking and
systemsinthephysicalworld,suchasinmobilityorhealth.This
revert to their baseline behavior. It is thus crucial for cyber-
workfocusesonakeychallenge:inthephysicalworld,itisoften
easyfortheusertooptoutoftakinganyrecommendationifthey physical recommendation systems to operate with an interac-
are not to her liking, and to fall back to her baseline behavior. tion model that is aware of such behavior to prevent users
It is thus crucial in cyber-physical recommendation systems to from abandoning the recommendations altogether. We name
operate with an interaction model that is aware of such user
this problem the Nah Bandit, a tongue-in-cheek reference to
behavior, lest the user abandon the recommendations altogether.
describe when users say ‘nah’ to the recommendation and
This paper thus introduces the Nah Bandit, a tongue-in-cheek
referencetodescribeaBanditproblemwhereuserscansay‘nah’ opt for their preferred option instead. Our research addresses
totherecommendationandoptfortheirpreferredoptioninstead. the Nah Bandit problem by considering both the impact of
As such, this problem lies in between a typical bandit setup recommendations and the choices users make regarding non-
and supervised learning. We model the user non-compliance by
recommended options.
parameterizingananchoringeffectofrecommendationsonusers.
Contextual bandits have been widely applied in digital
We then propose the Expert with Clustering (EWC) algorithm,
a hierarchical approach that incorporates feedback from both interactionssuchasnewsrecommendationsandadvertisement
recommended and non-recommended options to accelerate user placements. In these settings, people may only have access
preferencelearning.InarecommendationscenariowithN users, to the recommended options. Therefore, the only form of
T rounds per user, and K clusters, EWC achieves a regret
√ non-compliance is to take none of the options and leave the
bound of O(N TlogK +NT), achieving superior theoretical
service. However, picking an option that is not recommended performance in the short term compared to LinUCB algorithm.
Experimental results also highlight that EWC outperforms both is prevalent in the physical world. For example, consider the
supervisedlearningandtraditionalcontextualbanditapproaches. scenario where a customer is shopping in a store. All the
This advancement reveals that effective use of non-compliance items are presented to the customer in the showcase. A store
feedbackcanacceleratepreferencelearningandimproverecom-
clerk might recommend certain items to a customer, but the
mendation accuracy. This work lays the foundation for future
customer does not always purchase the recommended ones.
research in Nah Bandit, providing a robust framework for more
effective recommendation systems. The customer can choose any item in the showcase that he
prefers, and the clerk can observe which items the customer
Index Terms—Online preference learning, Contextual bandit,
eventually buys. This information allows the clerk to make
Non-compliance, Clustering, Recommendation system, Expert
advice more personalized recommendations during the customer’s
next visit. Examples like these are prevalent, such as in
shopping [4], [5] and mobility recommendations [6], [7].
I. INTRODUCTION
Both supervised learning and traditional contextual bandit
ONLINE recommendation systems have been widely ap- methods fail to address the Nah Bandit problem effectively.
pliedinthedigitalworld,suchaspersonalizednewsrec- Supervised learning methods, such as classification based
ommendations [1], advertisement placements [2], and search methods, decision-tree based methods, and neural networks,
engines[3].However,implementingeffectiverecommendation assume that the user selects from all options while not ac-
systems in the physical world remains challenging, such as countingfortheinfluenceofrecommendationsonusers,which
in shopping [4], [5], mobility [6], [7], or health [8], [9]. For is called the anchoring effect. Conversely, contextual bandit
instance, while bandit approaches have been widely used in methods, such as LinUCB [1], Thompson Sampling [10],
digital interactions by modeling the recommendation process and NeuralUCB [11], do not incorporate feedback from non-
recommended items because they assume that the user only
Tianyue Zhou is with the School of Information Science and
selectsfromrecommendedoptions,whichhinderstheirability
Technology, ShanghaiTech University, Shanghai, China. (e-mail:
zhouty1@shanghaitech.edu.cn) to quickly capture user preferences. The Nah Bandit problem
Jung-Hoon Cho is with the Department of Civil and Environmental En- requires both efficiently understanding the anchoring effect
gineering and the Laboratory for Information & Decision Systems, Mas-
andrapidlyidentifyinguserpreferencesfromnon-compliance.
sachusetts Institute of Technology, Cambridge, MA 02139, USA. (e-mail:
jhooncho@mit.edu) It has been widely studied that the anchoring effect can
CathyWuiswiththeLaboratoryforInformation&DecisionSystems;the significantlyinfluenceuserchoices[12]–[17].[16]foundthat,
InstituteforData,Systems,andSociety;andtheDepartmentofCivilandEn-
intheaggregate,theanchoringeffectislinearandproportional
vironmentalEngineering,MassachusettsInstituteofTechnology,Cambridge,
MA02139,USA.(e-mail:cathywu@mit.edu) to the size of the recommendation perturbation. Based on the
4202
guA
51
]GL.sc[
1v79870.8042:viXraJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 2
TABLEI compliance, offering the potential to accelerate preference
COMPARISONOFNAHBANDIT,TRADITIONALBANDIT,ANDSUPERVISED learning. We propose the user non-compliance model as the
LEARNING
simplest method to solve the Nah Bandit problem. Compared
to SVM, this model adapts to scenarios with multiple options
Userselectsfrom Userselectsfrom
recommendedoptions alloptions by computing the utility of each option, and reduces the bias
Userisinfluenced NahBandit from the anchoring effect by parameterizing the user’s depen-
Bandit
byrecommendations (Thiswork) dence on recommendation. We further combine the user non-
Userisnotinfluenced
N/A Supervisedlearning compliance model with EWC, allowing EWC to efficiently
byrecommendations
utilize non-compliance feedback, thereby enabling rapid and
accurate learning in the Nah Bandit problem. Additionally, by
information from [16], we propose a user non-compliance incorporatingusercontextintothepreferencelearningprocess,
model to parameterize the anchoring effect for each user. EWC improves the speed of adapting to user preferences. We
This is the simplest method to solve the Nah Bandit prob- validate our proposed method EWC against a comprehensive
lem, which reduces the bias introduced by the anchoring set of baselines with multiple applications. We also conduct
effect when learning user preferences. We further prove the an ablation study to assess the impact of each component.
sample complexity of parameter estimation in the user non- These enhancements and extensive evaluations help us better
compliance model by transforming it into logistic regression. understand EWC algorithm and its potential applications.
This result shows the speed at which we can learn user Overall, this research demonstrates that effectively utilizing
preference parameters in Nah Bandit problem. non-compliance feedback can accelerate preference learning
Torapidlycaptureuserpreferences,algorithmssuchasuser- and enhance recommendation accuracy. Our work establishes
based Collaborative Filtering [18] use the similarity between a foundation for future research into the Nah Bandit problem,
users to make recommendations. Some approaches further providing a robust framework for developing more effective
assume a network structure [19], [20], or a hierarchical recommendation systems.
structure among users [21], [22] to make recommendation.
Similar to these works, we assume a hierarchical structure A. Contributions
in the Nah Bandit problem. We propose a novel hierarchical We summarize our contributions as follows:
contextual bandit algorithm—Expert with Clustering (EWC).
1) We introduce a novel bandit framework—Nah Ban-
EWC uses the user non-compliance model and clustering to
dit—for modeling the online preference learning prob-
determine the preference parameters of each user cluster. It
lem, in which users go ‘nah’ to the recommendation
then views each cluster as an expert and uses the Hedge
and choose their originally preferred option instead.
[23] algorithm to select the expert that best predicts user
This framework distinctively incorporates user observ-
choices. The likelihood that at least one expert accurately
ablenon-compliance,offeringthepotentialtoaccelerate
predicts the user choice is high, regardless of compliance.
preference learning.
This leads to rapid identification of user cluster identity in the
2) Weproposeausernon-compliancemodelasthesimplest
Nah Bandit problem. We further establish the regret bound
way to solve the Nah Bandit problem, which param-
of EWC. In a recommendation scenario with N users, T
eterizes the anchoring effect. We analyze its sample
rounds per user, and K user clusters, we demonstrate that
√ complexitytoshowthespeedatwhichwecanlearnuser
EWC achieves a regret bound of O(N T logK+NT). This
preferenceparameters.Basedonthismodel,wepropose
regret bound underscores the theoretical efficacy of EWC
a hierarchical contextual bandit framework, Expert with
in the short term compared to LinUCB [1]. We validate
Clustering (EWC). This framework effectively utilizes
EWCintwodifferentapplications:travelroutesandrestaurant
non-compliance feedback and hierarchical information,
recommendations. Experimental results highlight that EWC
enablingrapidandaccuratelearningofuserpreferences.
achieves superior performance compared to both supervised
3) We establish the regret bound of EWC. In a recom-
learning and traditional contextual bandit approaches.
mendation scenario with N users, T rounds per user,
Thispaperextendsandsubsumesourpreliminarywork[24],
and K user clusters, EWC achieves a regret bound of
√
where we introduced EWC algorithm for a travel route rec-
O(N T logK + NT), achieving superior theoretical
ommendation problem. We proposed a Loss-guided Distance
performance in the short term compared to LinUCB.
metric to enhance the performance of clustering in EWC and
proved a regret bound for EWC. However, our previous work
II. RELATEDWORKS
focused on a specific travel problem that included only two
A. Supervised Online Recommendation
options per decision, and did not formally define the Nah
Bandit problem. The support vector machine (SVM) frame- Thefieldofrecommendationsystemshasbeensignificantly
work used for offline training in that work is not adaptable to shaped by various supervised learning methods. A founda-
scenarios with multiple options and, more importantly, cannot tional approach is Collaborative Filtering (CF), which bases
addresstheanchoringeffectintheNahBanditproblem.Inthis its recommendations on the similarity between users [18]
work, we formally introduce a novel bandit framework—Nah or items [25]. Matrix Factorization (MF) [26], a specialized
Bandit—formodelingtheonlinepreferencelearningproblem. form of collaborative filtering, is notable for decomposing the
Thisframeworkdistinctivelyincorporatesuserobservablenon- user-item interaction matrix into lower-dimensional matricesJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 3
Fig.1. AnoverviewoftheExpertwithClustering(EWC)algorithmfortheNahBanditproblem.Intheofflinetrainingphase,ausernon-compliancemodel
learns user preference parameters based on option contexts and user choices. These preference parameters are then grouped into clusters, with the cluster
centroidsservingasexperts.Usercontextsandtheirclusterlabelsareusedtotrainalogisticregressionmodeltopredicttheinitialweightsoftheexperts.In
theonlinelearningphase,EWCselectsanexpertforeachrecommendation.Afterobservingtheuser’schoice,EWCcalculatesthelossesofeachexpertsand
updatestheirweightsaccordingly.
representing the latent user and item factors. Recent advance- algorithm[21],whichisadynamicclusteringbanditalgorithm
ments have led to the introduction of online versions of these that divides the population of users into multiple clusters and
traditional methods, like Online Collaborative Filtering [27] customizes the bandits for each cluster. Additionally, [37]
and Online Matrix Factorization [28], to address the dynamic proposestheLOCBapproachwhichenhancesperformanceby
nature of user preferences and real-time data. focusingontheaccuracyofclusteringanditsimpactonregret
Decision tree-based methods, such as Gradient Boosting minimization in bandit algorithms. Building on these works,
Machines [29] and Random Forests [30], and their online wealsoadoptedclusteringtosegmentusersintoseveralgroups
versions[31],[32]representanotherclassofsupervisedlearn- and tailor recommendations to each group.
ing methods in recommendation systems. These methods are
C. Recommendation systems with user non-compliance
widelyusedintheareaofadvertisementsduetotheirabilityto
learn nonlinear features. XGBoost [33], an important variant We refer to user non-compliance as any action taken
of Gradient Boosting Machines, provides fast and accurate by the user that is not among the recommended action(s).
recommendationsandisusedasabaselineinourexperiments. Contextual bandit methods have been extended to consider
However, these supervised recommendation methods often user abandonment, for example, deleting an app or leaving
overlook the potential impact of the anchoring effect. Our before re-engaging later. Users may abandon the system for
research aims to address this oversight by reducing the bias a variety of reasons, including fatigue from overexposure to
introduced by the anchoring effect to make more accurate irrelevant content or boredom from seeing too many similar
recommendations. recommendations [38], lack of trust [39], or non-alignment
with the user’s immediate self-interests [40]. Some solutions
B. Contextual Bandits with Clustering proposed include incorporating the risk of user abandonment
The contextual bandit framework offers an efficient solu- intotherecommendationalgorithms[41]–[43],orneveroffer-
tion for recommendation systems. This concept was initially ing recommendations that would yield lower expected reward
explored by [34], focusing on the use of confidence bounds than a user would have achieved if she acted alone [40].
within the exploration-exploitation trade-off, particularly in Our work considers a softer form of user non-compliance, in
contextual bandits with linear value functions. Building on which the user still selects an option within the same action
this, [1] extended this framework to personalized news rec- class (e.g., mobility trip option), albeit not a recommended
ommendations by introducing LinUCB algorithm, setting a one. Our algorithm seeks to quickly learn user preferences
benchmark in the field. More recently, advanced algorithms by acknowledging such non-compliance and learning from
such as NeuralUCB [11] have been developed, leveraging these user actions. This novel approach provides a holistic
neural networks to model complex, non-linear relationships view of user preferences, which is crucial for understanding
in the data, further enhancing the flexibility and effectiveness the comparative utility of options and accelerating preference
of contextual bandits. The integration of clustering techniques learning, especially in scenarios with limited data.
into the contextual bandit framework represents a signifi-
III. NAHBANDIT
cant advancement in the field. The foundational unsupervised
A. Definition of Nah Bandit
learning algorithm, K-Means clustering, introduced by [35],
laid the groundwork for this development. Based on this, In the Nah Bandit problem, users may say ‘nah’ to the
[36]proposedapersonalizedrecommendationalgorithmusing recommendation and choose their originally preferred option
hierarchicaltagclustering.Withthedevelopmentofcontextual instead. This means the losses (or rewards) come from user’s
bandit, [22] proposed a novel approach to cluster users within choice. We define the Nah Bandit in a recommendation prob-
the contextual bandit framework, using graphs to depict user lem as follows. This framework combines elements of both
similarities.AnotableprogressioninthisareaistheDYNUCB supervised learning and partial feedback in a bandit setting.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 4
Definition 1. Nah Bandit is a scenario where a provider is we assume that the value of A remains constant across all
tasked with recommending a set of options O to a user, where subsets O . Each option indexed by a ∈ [A], is defined
i,t
A = |O| represents the number of options. At each decision by an option context vector x ∈ Rd. Upon receiving
i,t,a
round t within the total rounds T, the provider recommends thisinformation,theproviderrecommendsoneoption,labeled
one option, labeled r ∈ [A] to the user. Subsequently, the r , from the set [A] to the user. Subsequently, the provider
t i,t
provider observes the user’s choice y , and incurs a loss observes the user’s choice, denoted as y , and incurs a loss
t i,t
l from the user’s choice. The objective is to minimize the l(r ,y ), determined by a predefined loss function known
t i,t i,t
cumulative regret over all decision rounds min
(cid:80)T
l . to the provider. It is important to note that the user’s choice
rt,∀t t=1 t
y may be influenced by the recommended option r .
Conversely, in the traditional bandit framework for recom- i,t i,t
Theobjectiveofthisscenarioistominimizethetotalcumu-
mendationsystems,theuserisassumedtoselectonlyfromthe
lative regret over all users and decision rounds. This is math-
recommended options, and therefore, the losses (or rewards)
ematically formulated as min
(cid:80)N (cid:80)T
l(r ,y ).
are derived solely from those recommended options. ri,t,∀i,t i=1 t=1 i,t i,t
IV. AUSERNON-COMPLIANCEMODEL
B. Comparison with Bandit and Supervised Learning
A. Model Description
Nah Bandit differs from both traditional contextual bandit
A key assumption in our problem is that the user’s choice
and supervised learning. Bandit framework utilizes feedback
may be influenced by the anchoring effect. This leads to
only from the recommended option, therefore its exploration-
a scenario of partial feedback akin to a contextual bandit
exploitation trade-off handles the anchoring effect of rec-
setting, where learning user preferences can be challenging.
ommendation on users. In contrast, the supervised learning
[16] uses a rating drift, defined as the actual rating minus the
framework assumes the user’s choice and feedback are inde-
predictedrating,torepresenttheanchoringeffect.Theyfound
pendent of the recommendation, allowing the user to choose
that, in aggregate, the rating drift is linear and proportional
any option. In the Nah Bandit problem, the user can choose
to the size of the recommendation perturbation. This means
any option, and we use the user’s feedback to learn their
that the more we recommend one option, the higher rating
preferences. The feedback remains partial because different
the user will give to this option. However, the slope of
recommendationscanleadtodifferentoutcomes.Forexample,
this linear relationship, which represents the user’s additional
some users might rely heavily on recommendations and are
preferencefortherecommendedoptions,isunknown.Building
more likely to choose the recommended option, while others
on [16], we propose a user non-compliance model to discern
may carefully select from all options to choose the one that
this additional preference, thereby addressing the issue of the
best matches their preferences. Table I compares the Nah
anchoring effect.
Bandit, traditional bandit problem, and supervised learning.
Assumethateachuserihasafixedbutunknownpreference
IfweadoptsupervisedlearningapproachestosolvetheNah
parameter θ ∈Rd. Given θ , we can make predictions using
Bandit problem, they may lead to sub-optimal recommenda- i i
a known function yˆ(θ ,x ). The key idea is that we assume
tions because they do not account for the anchoring effect of i i,t
there exists a θ within θ that quantifies the additional
recommendations on the feedback. Conversely, if we adopt i,rec i
preference toward recommended options. Users with a high
traditional bandit approaches, the recommended options are
θ highlyrelyontherecommendedoption,whileuserswith
viewedasthepulledarmsofthebandit.Inthiscase,wereceive i,rec
θ = 0 select the option with the highest utility for them,
zerorewardfromthepulledarmiftheuserchoosesnoneofthe i,rec
regardless of the recommendation. Our goal is to learn this
recommendedoptions,andthefeedbackfromtheuser’sactual
θ for each user.
choice is not used to update the recommendation system. i,rec
We propose a user non-compliance model, which is a
TheNahBanditframeworkoffersapotentialsolutionconcept
linear model that parameterizes the user’s dependence on the
to accelerate preference learning by effectively utilizing non-
recommendation. First, we incorporate a context xrec ∈ R
compliance feedback. i,t,a
in each option context x which represents the degree to
i,t,a
C. Problem Formulation which this item is recommended to the user. For example,
xrec = 1 . The utility of each option is then defined
We further incorporate contexts of users and options in the i,t,a ri,t= ⊺a
as U = x θ . Let U = [U ,U ,...,U ]
NahBandit.Additionally,weextendtherecommendationfrom i,t,a i,t,a i i,t i,t,1 i,t,2 i,t,A
represent the utility vector. The probability of selecting each
single-usertoamulti-user,andassumeahierarchicalstructure
option is predicted as p = σ(U ), where σ(·) denotes
i,t i,t
among users. We formulate our problem as follows.
the softmax function. Let y be the one-hot encoding of
i,t
Consider a scenario where a provider is tasked with rec-
y . The discrepancy between the predicted probability and
i,t
ommending a set of options, denoted as O, to a population
the actual choice is quantified using the KL-divergence. The
of users U with hierarchical structure, with the total number
detailed methodology is encapsulated in Algorithm 1.
of users being N := |U|. Each user, identified as i in the set
This approach is the simplest way to solve the Nah Ban-
[N], is represented by a unique user context vector u ∈RD.
i dit problem. It provides a supervised learning way to learn
At each decision-making round t within the total rounds T,
user’s preference parameters θ . It reduces the bias in the
i
the provider is presented with the user context u and a
i learning process that might be introduced by the anchoring
specific subset of options O ⊂ O, where A := |O |
i,t i,t effect,therebypreventingtheusernon-compliancemodelfrom
represents the number of options in the subset. For simplicity,
falling into sub-optimal recommendations.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 5
Algorithm 1 User Non-compliance Model compliance. To address this, we introduce the Expert with
Require: Option contexts {x } , Clustering (EWC) algorithm, a novel hierarchical contextual
i,t,a i∈[N],t∈[T],a∈[A]
recommendation record {r } , user choice bandit approach. EWC consists of both an offline training
i,t i∈[N],t∈[T]
{y } phase and an online learning phase. It transforms the rec-
i,t i∈[N],t∈[T]
x ←[x ,xrec ] for all i∈[N],t∈[T],a∈[A] ommendation problem into a prediction with expert advice
i,t,a i,t,a i,t,a
Randomly initialize {θ } problem, using clustering to get experts during the offline
i ∀i∈[N]
while {θ } not converge do training phase and employing the Hedge algorithm to select
i i∈[N]
⊺
U ←x θ for all i∈[N],t∈[T],a∈[A] the most effective expert during the online learning phase.
i,t,a i,t,a i
p ←σ(U ) for all i∈[N],t∈[T] Prediction with expert advice is a classic online learning
i,t i,t
θ ←argmin 1 (cid:80)T KL(p ||y ) for all i∈[N] problem introduced by [23]. Consider a scenario in which a
i θ T t=1 i,t i,t
end while decisionmakerhasaccesstotheadviceofK experts.Ateach
decisionroundt,advicefromtheseK expertsisavailable,and
the decision maker selects an expert based on a probability
B. Sample Complexity Analysis distribution p and follows his advice. Subsequently, the
t
This section presents a sample complexity analysis of user decision maker observes the loss of each expert, denoted as
preference parameter estimation in the user non-compliance l t ∈ [0,1]K. The primary goal is to identify the best expert
model.Ifweletthemodelusethecontextofonlytwooptions in hindsight, which essentially translates to minimizing the
to update the model, where one is the user’s choice and the regret:(cid:80)T t=1(<p t,l t >−l t(k∗)),wherek∗ isthebestexpert
other is not, this model is equivalent to a logistic regression. throughout the time.
Wecasttherecommendationproblemintotheframeworkof
Lemma 1 (Sample Complexity of Parameter Estimation in
prediction with expert advice in the following way. Recall the
Logistic Regression (Theorem 4 in [44])). Consider a logistic
assumptionthateachuserhas afixedbutunknownpreference
regression model with input x ∈ Rd ∼ N(0,I d) and output parameter θ ∈ Rd. Given θ , EWC algorithm operates
i i
y ∈{−1,1}. The parameter space is the unit sphere Sd−1 =
under the assumption of a cluster structure within the users’
{θ ∈ Rd : ||θ|| = 1}. y|x ∼ Bern(σ(βx⊺θ∗)) where σ is the
preference parameters {θ } .
i i∈[N]
Sigmoid function, β is the inverse temperature, and θ ∈Sd−1
Intheofflinetrainingphase,utilizingasetofofflinetraining
is the parameter of the model. The observed data {x ,y }T
t t t=1 data D = {{x i,t,a} i∈[N′],t∈[T],a∈[A],{y i,t} i∈[N′],t∈[T]} where
are independent copies of x and y with unknown param √eter N′ andT′ arenumberofusersanddecisionroundsintraining
θ∗ ∈ Sd−1. For any fixed ϵ,δ ∈ (0,1), assume β ≥ 4 2π,
ϵ data, we initially employ the user non-compliance model as
and T ≥ C(dlog(1/ϵ)+log(1/δ)) where C > 0 is an absolute a learning framework to determine each user’s preference
ϵ
constant.Thenwithprobabilityatleast1−δ,theempiricalrisk parameterθ .Despitedifferencesbetweentrainingandtesting
i
minimizer θˆ ERM({x t,y t}T t=1) achieves ||θˆ ERM({x t,y t}T t=1) − data,botharesampledfromthesamedistribution.Thisallows
θ∗||≤ϵ. for an approximate determination of θ , providing insights
i
into the hierarchical structure among users, albeit with some
Theorem 1 (Sample Complexity of Parameter Estimation
degree of approximation. Subsequently, a clustering method
in the User Non-compliance Model). Let the user non-
is applied on {θ } to identify centroids {c } and
compliance model use the context of only two options to i i∈[N′] k k∈[K]
user’sclusteraffiliation{z } ,wherec ∈Rd and
updatethemodel,whereoneistheuser’schoiceandtheother i,k i∈[N′],k∈[K] k
(cid:80)
z ∈ {0,1}, z = 1. The number of clusters K
is not. Assume x˜ i,t := x i,t,1 − x i,t,2 ∼ N(0,I d). Suppose i,k k∈[K] i,k
serves as a hyperparameter. We select the value of K that
{x˜ ,y } are i.i.d. samples from a a distribution determined
i,t i,t
by θ∗ ∈ Sd−1, where y |x˜ ∼ Bern(σ(β x˜⊺ θ∗)). Assume yields the minimum regret on the offline training set.
i i,t i,t i i,t i Each centroid is considered an expert. In the online learn-
theusernon-compliancemodeloutputstheempiricalriskmin-
imizer θ = θˆ ({x ,y }T ). For any fixed ϵ,δ ∈ (0,1), ing phase, using the Hedge algorithm, we initialize their
i√ ERM i,t i,t t=1 weights and, at every online decision round, select an expert
if β ≥ 4 2π, and T ≥ C(dlog(1/ϵ)+log(1/δ)) where C > 0 is
i ϵ ϵ E ∈ [K]. An expert E provides advice suggesting that
an absolute constant, then with probability at least 1−δ, we i,t i,t
a user’s preference parameters closely resemble the centroid
have ||θ −θ∗||≤ϵ.
i i c . Consequently, we use this centroid to estimate the
Ei,t
Lemma 1 shows the sample complexity of parameter es- user’s preferences. The recommendation r = yˆ(c ,x )
i,t Ei,t⊺ i,t
timation in a logistic regression model. Using Lemma 1, we isthenformulated.Forexample,yˆ(θ,x )=argmax x θ
i,t a i,t,a
cangetTheorem1,whichshowsthesamplecomplexityofthe where x = [x ,x ,...,x ]. Upon receiving the
i,t i,t,1 i,t,2 i,t,A
user preference parameter in the user non-compliance model. user’s chosen option y , we calculate the loss for each
i,t
The proof of Theorem 1 is in Appendix A-D. This result expert and update the weights in Hedge based on this loss.
demonstrates the speed at which we can learn user preference The loss for each expert k is determined by a known loss
parameters in the Nah Bandit problem. function l (k) = l(yˆ(c ,x ),y ) ∈ R. For example,
i,t k i,t i,t
l(yˆ(c ,x ),y ) = 1 . The details of this pro-
V. EXPERTWITHCLUSTERING k i,t i,t yˆ(ck,xi,t)̸=yi,t
cess are encapsulated in Algorithm 2.
A. General Framework
The EWC algorithm efficiently utilizes non-compliance
Another core aspect of our problem is rapidly identify- feedback in both the offline training and the online learning
ing user preferences based on both compliance and non- phase.Inofflinetraining,thelearningframeworkwithinEWCJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 6
is an interchangeable module that can be implemented using Algorithm 3 K-Means with Loss-guided Distance
various models, such as SVM or neural networks. Compared Require: {θ }
i i∈[N′]
to other models, the user non-compliance model captures Randomly initialize centroids {c }
k k∈[K]
preferences from both compliance and non-compliance feed- while {c } not converged do
k k∈[K]
back with low bias. In online learning, the likelihood that at dist(i,c )←||yˆ(c ,X )−y ||2 foralli∈[N′],k ∈[K]
k k i i F
least one expert accurately predicts the user choice is high,
regardless of compliance. This leads to rapid identification of z ←1 for all i∈[N′],k ∈[K]
user cluster identity in the Nah Bandit problem. endci k, wk ← hil(cid:80) e(cid:80)N ik =
N
i= =1 1a zr i zg , ik ,m kθi in k f′ od ris at( li l,c kk′) ∈[K]
Algorithm 2 Expert with Clustering
return {c } , {z }
Require: Number of clusters K, offline training data D, k k∈[K] i,k i∈[N′],k∈[K]
learning rate η
Train data by Algorithm 1, receive {θ }
i i∈[N′]
cluster labels {z } to train a logistic regression
Applyclusteringon{θ } ,receivecentroids{c } i,t i∈[N′],t∈[T]
i i∈[N′] k k∈[K] model, denoted as f : RD → RK. This model is designed to
map the user context to a probabilistic distribution over the
Initialize weight p (k)← 1 for all i∈[N],k ∈[K]
i,1 K potential cluster affiliations.
for t=1,...,T do
During the online learning phase, we employ the trained
for i=1,...,N do
logisticregressionmodelf(·)topredicttheprobabilityofeach
Receive x
i,t
user’sgroupaffiliationbasedontheirrespectivecontext.These
Sample E ∼p , submit r ←yˆ(c ,x ),
i,t i,t i,t Ei,t i,t
predicted probabilities are then used to initialize the weights
Receivey ,computelossl (k)←l(yˆ(c ,x ),y )
i,t i,t k i,t i,t
of the experts for each user, i.e., p ←f(u ) for all i∈[N].
for all k ∈[K] i,1 i
p (k)←
pi,t(k)e−ηli,t(k)
for all k ∈[K]
i,t+1 (cid:80) k′∈[K]pi,t(k′)e−ηli,t(k′)
end for VI. REGRETANALYSIS
end for
A. Regret Bound of EWC
In our problem, we define the loss function
l(yˆ(c ,x ),y ) = 1 . We define the regret
B. Clustering with Loss-guided Distance k i,t i,t yˆ(ck,xi,t)̸=yi,t
of EWC as the performance difference between EWC and
The core parameter influencing the regret in our model is
recommendation with known user preference parameter θ :
i
thesetofcentroids{c } .Anaccuratelyrepresentativeset
k k∈[K]
N T
ofcentroidscansignificantlyreflectusers’behaviors,whereas (cid:88)(cid:88)
R = (⟨p ,l ⟩−l(yˆ(θ ,x ),y )) (1)
poorly chosen centroids may lead to suboptimal performance. EWC i,t i,t i i,t i,t
In our simulations, we observed that the standard K-Means i=1 t=1
algorithmhaslimitations,assimilarθ valuesintheEuclidean Since the study in [48] shows the performance of K-Means
i
space do not necessarily yield similar user preferences. clustering using the L 2 norm distance, we similarly adopt the
ToaddressthelimitationofK-Meansclustering,researchers L 2normdistancetoanalyzeregretinourframework.Theorem
in fields such as federated learning [45], [46] and system 2isourmaintheoreticalresult.TheproofisinAppendixA-A.
identification [47] have devised bespoke objective functions TheregretboundofEWCcanberepresentedasasumoftwo
to enhance clustering methodologies. Inspired by [45], we components:theregretfromtheHedgealgorithmandthebias
introduce a distance metric guided by the loss function which introduced by representing users with centroids.
is tailored for online preference learning. Our objective is Theorem 2 (Regret Bound of EWC). Let P be any distribu-
to ensure that θ i values within the same cluster exhibit tion of θ ∈Rd with µ=E [θ ], σ2 =E [||θ −µ||2], and
i P i P i
similarperformance.Thus,wereplacethetraditionalL 2 norm finite Kurtosis. Let k∗(i) be the optimal expert for user i, and
distance with the prediction loss incurred when assigning L=(cid:80)N ||c −θ ||2 . If yˆ(·,X ) is Lipschitz continuous
i=1 k∗(i) i i
c to user i. Here, we define: X = [x ,x ,...,x ] ∈
k i i,1 i,2 i,T′ for all X with Lipschitz constant L, Frobenius distance, and
RT′×A×d, yˆ(c ,x ) be the one-hot encodings of yˆ(c ,x ), i
k i,t k i,t dimension normalization, then with probability at least 1−δ,
y i = [y i,1,y i,2,...,y i,T′] ∈ RT′×A, and yˆ(c k,X i) = the regret of EWC is bounded by:
[yˆ(c ,x ),yˆ(c ,x ),...,yˆ(c ,x )]∈RT′×A. The Loss-
guidek dDi, i1 stancek isdei, fi2 nedasdisk t(i,i c,T′ )=||yˆ(c ,X )−y ||2. R ≤2N(cid:112) T logK+ 1 TL(cid:0) ϵσ2+(ϵ+2)E[L](cid:1) (2)
k k i i F EWC 4
The detailed clustering is presented in Algorithm 3.
The Gaussian Mixture Model (GMM) aligns closely with
C. Accelerating Learning with User Context our hypothesis of a hierarchical structure among users, which
isatypicalassumptionintheanalysisofclusteringalgorithms.
In our model, we capitalize on the user context to facilitate
Byassumingthatthedistributionofusers’preferencesfollows
accelerated preference learning during the initial phase. We
a GMM, we derive Corollary 1.
hypothesize a latent relationship between the user context and
the user’s cluster affiliation. In the offline training phase, we Corollary 1. AssumeP isaGaussianMixtureModel(GMM)
utilize user context vectors {u } along with the users’ with K Gaussian distributions, each of which has weight
i i∈[N′]JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 7
π , mean µ , and covariance Σ , and the clustering outputs 1) Description: Consider a social planner is tasked with
k k k
the optimal centroids where c = µ . Define l = recommending travel route options to a population of drivers,
k k centroids
1 Lϵσ2+ 1L(ϵ+2)(cid:80)K π trace(Σ ) be the average loss where each driver i has a user context vector u . Each route
4N 4 k=1 k k i
caused by centroids. With probability at least 1−δ, the regret option at decision round t with index a is parameterized by
of EWC is bounded by an option context vector x . For simplicity, we consider
i,t,a
(cid:112) two options (A = 2), each with two relevant travel metrics
R ≤R =2N T logK+TNl (3)
EWC EWC centroids (d = 2): travel time and emission. Thus, at each decision
The proof of Corollary 1 is provided in Appendix A-B. roundofauser,thesocialplannerfacesachoicebetweentwo
EWC does not achieve sublinear regret in the long term options: route 1, the standard route with regular travel time
because it uses the cluster centroid as an estimate of user- and emissions, and route 2, an eco-friendly alternative that
specificpreferenceseachtime.However,iftheestimationerror offers reduced emissions while coming with increased travel
is low, indicated by a small l value, EWC achieves low time.
centroids
regret in the short term. 2) Experimental Setup: Community Survey. This study
involved a community survey conducted in July 2023 on the
B. Comparison to LinUCB
University of North Carolina at Charlotte campus, and a total
Lemma 2 (Regret Bound of SupLinUCB (proved by [49])).
of43individualsparticipated.Participantsprovidedthedriving
Assume∀i,t,∃θ∗ ∈Rd,s.t.E[l (a)|x ]=x⊺ θ∗.Define
i i,t i,t,a i,t,a i choice preferences as well as demographic data covering age,
R = (cid:80)N (cid:80)T (cid:0) l (a )−l (a∗ )(cid:1) where a∗ =
LinUCB i=1 t=1 i,t i,t i,t i,t i,t gender, ethnicity, and educational level. The survey’s main
(cid:113)
argmax x⊺ θ∗.IfSupLinUCBrunswithα= 1log2TA, componentinvolvedaseriesofquestionsassessingwillingness
a i,t,a 2 δ
to adhere to route recommendations under varying scenar-
with probability at least 1 − δ, R < R =
(cid:18) (cid:113) (cid:19) LinUCB LinUCB ios with distinct travel times and carbon dioxide emission
O N Tdlog3(AT logT/δ) .
levels. Participants rated their likelihood of following these
recommendations on the Likert scale, offering insight into
Corollary 2 (Advantage of EWC). Assume R =
LinUCB their decision-making criteria. Mobility User Simulation. To
(cid:113)
CN Tdlog3(AT logT/δ), then when T < ( C−2 )2, betterrepresentadiversedrivingpopulation,weexpandedour
lcentroids
R <R . dataset. We use the Bayesian inference model that resembles
EWC LinUCB
the original distribution from the survey data [50]. For each
We compare the regret of EWC with LinUCB. Lemma 2
user in the survey data, we sample his preference parameter
is the regret bound of SupLinUCB (a variant of LinUCB).
θ from a multivariate normal distribution. Based on this
i
Corollary 2 is the comparison between EWC and LinUCB.
θ , we calculate the prediction loss L compared to the real
i
The proof of Corollary 2 is provided in Appendix A-C. EWC likelihood. Then we calculate the acceptance rate p = e−λL.
demonstrates superior theoretical performance compared to
Weacceptthissamplewithprobabilityp.Theprocessaboveis
LinUCB when T is relatively small.
repeated until we collect 24 samples for each user. In order to
VII. EXPERIMENT incorporate the influence of the recommendation on the user’s
choice,weconcatenateθ withθ whereθ issampledfrom
In this section, we perform empirical analysis to validate i rec rec
a beta distribution and then multiplied by β. Higher β means
our algorithm in two different applications: travel routes and
the population has more preference for the recommended
restaurant recommendations.
option. β =0 represents a supervised learning scenario where
A. Baselines theuser’schoiceisindependentoftherecommendation,while
We compare our EWC algorithm against several baseline β > 0 means a bandit feedback scenario. Based on the
methods to determine its performance in online preference synthetic preference parameters, we sample travel routes and
learning. The user non-compliance model (Non-compliance) generate user choices on users’ routes. The detailed context
is a linear model that parameterizes user dependence on rec- descriptionandparametersettingareshowninAppendixB-A.
ommendation (Algorithm 1). LinUCB refines the upper confi- 3) Results and Interpretation: In this section, we present
denceboundmethodtosuitlinearrewardscenarios,aimingto the relative performance of our proposed algorithm, EWC, by
strikeabalancebetweenexploringnewactionsandexploiting comparing it with various baselines over a series of 12,000
known ones. We adopt the hybrid linear model in LinUCB total rounds. The experiment is repeated with 5 different
proposed by [1] to learn from both user context and option randomseeds. Figure2shows theresultswiththe travelroute
context.DYNUCBcombinesLinUCBwithdynamicclustering, recommendation dataset. The regret represents the cumulative
which divides the population of users into multiple clusters difference between the rewards of the algorithm’s selections
and customizes the bandits for each cluster. We also use the and the optimal choices.
hybridlinearmodelinDYNUCB.XGBoostisahighlyefficient Ourproposedalgorithm,EWC,demonstratesasignificantly
supervised learning algorithm based on gradient boosting. lower regret than that of other baseline methods in all scenar-
ios.Itachievesaverylowslopeintheearlyrounds,indicating
B. Travel Route Recommendation
that EWC algorithm effectively incorporates user preference
Wevalidateouralgorithmintravelrouterecommendations. information and rapidly learns user preferences.
The data is collected from a community survey first, and then The user non-compliance model can learn user’s prefer-
expanded to represent a diverse driving population. ence from both compliance and non-compliance feedback, asJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 8
(a)β =0 (b)β =1 (c)β =10
Fig. 2. Regret of Expert with Clustering (EWC, Ours) and other baselines (DYNUCB, LinUCB, the user non-compliance model, and XGBoost) applied
to travel route recommendation data. The x-axis represents the total decision rounds, while the y-axis represents the regret. Lower regret indicates better
performance.Thecomparisonincludesthreescenarioswithdifferentvaluesofβ,whereβ indicatesthescaleofusers’dependenceonrecommendationsin
datageneration.Ahigherβ meansthepopulationhasastrongerpreferencefortherecommendedoption.EWCconsistentlyshowssignificantlylowerregret
thanotherbaselinemethodsacrossallscenarios.
its slope increasingly decreases. However, it does not learn integrate group behaviors into user decision-making. We use
rapidly, as its slop decreases slowly and it does not show a Oracle Cluster to test the learning speed of prediction with
significantadvantageoverLinUCB.Comparedtotheusernon- expert advice. Lastly, Oracle θ uses complete information of
i
compliance model, EWC achieves a much lower regret. This user-specific preferences learned by Algorithm 1 to test the
is because EWC efficiently uses the hierarchical information user non-compliance model in the offline training of EWC.
within the group of users’ preferences learned by the user Figure 3 shows the results of the ablation study. EWC
non-compliance model. It accelerates the preference learning achieves a much lower regret compared to Non-compliance.
process by clustering and prediction with expert advice. As explained in Section VII-B3, the clustering and prediction
XGBoost achieves the second lowest regret when β = 0, with expert advice components significantly accelerate prefer-
which represents a supervised learning scenario. However, as encelearning.EWCexhibitsalowerregretslopethanWithout
β increases, representing a bandit feedback scenario where u i intheearlyrounds,indicatingthatusingusercontextleads
users have a stronger preference for recommended routes, to a good initialization of the weight of each expert, further
XGBoost’s performance deteriorates significantly. This sug- decreasing regret. EWC shows a lower slope than Without
gests that supervised learning methods overlook the influence non-compliance across the entire time span, indicating that
of recommendations on user choices, leading to sub-optimal theusernon-compliancemodelreducesthebiasintroducedby
outcomes. In contrast, LinUCB performs well when β = 1 the anchoring effect when learning user preferences. The low
and β = 10, demonstrating that its exploration-exploitation slope of Oracle Cluster indicates that the shared preference in
balancing strategy provides an advantage. eachclustercanrepresenteachuser’sspecificpreferencewell.
The long-term slope of EWC mirrors that of Oracle Cluster,
DYNUCB shows high regret in all scenarios. We believe
suggestingthatpredictionwithexpertadvicerapidlyidentifies
that since it learns θ online, it obtains inaccurate θ in early
i i
each user’s cluster identity. The Oracle θ shows extremely
rounds,leadingtoinaccurateclusteringandconsequentlypoor i
low regret due to its complete information of user-specific
performance.Incontrast,EWCalgorithmutilizestherelatively
preference. It indicates that the user non-compliance model
accurateθ fromofflinetraining.Additionally,theloss-guided
i
in the offline training successfully learns users’ preferences.
distancemetricinclusteringimprovesclusteringperformance.
These two methods show the potential lower regret bounds
that EWC could aspire to achieve.
4) Ablation Study: In this subsection, we perform an
ablation study to assess the impact of each component of C. Restaurant Recommendation
our proposed EWC algorithm. We aim to understand their 1) Data: The dataset for restaurant recommendations were
contributiontotheoverallperformance.EWCconsistsofthree constructed using the Entree Chicago Recommendation Data
main components: (1) the user non-compliance model, (2) [51]. This rich dataset is a collection of user interactions with
clustering and prediction with expert advice, and (3) linear the Entree Chicago restaurant recommendation system, which
regression on user context. Without non-compliance is EWC includes user preferences, selections, and ratings of various
algorithm without using the user non-compliance model. We dining establishments within the Chicago area. We select four
usealinearmodeltolearnuser’spreferenceparameterinstead. features to be included in the option context: food quality,
Non-compliance is EWC without clustering and prediction servicelevel,price,andstyle.Thedetailedcontextdescription
with expert advice. It is reduced to the user non-compliance and parameter setting are shown in Appendix B-B.
model. Without u is EWC without using user context to 2) Results: The experiment is repeated with 10 different
i
accelerate preference learning. We also incorporate oracle random seeds. Figure 4 provides the comparative result of
methods in this section to show the potential of our EWC EWCandbaselinesintherestaurantrecommendationscenario
algorithm. Oracle Cluster is EWC with precise clustering to over 1000 total rounds. EWC algorithm shows the lowestJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 9
(a)β =0 (b)β =1 (c)β =10
Fig.3. AblationstudyofExpertwithClustering(EWC,Ours)intravelrouterecommendation.EWCconsistsofthreemaincomponents:(1)theusernon-
compliancemodel,(2)clusteringandpredictionwithexpertadvice,and(3)linearregressiononusercontext.Theapproachesintheablationstudyinclude
Withoutnon-compliance(EWCwithout(1)),Non-compliance(EWCwithout(2)),Withoutui (EWCwithout(3)),Oraclecluster(EWCwithoraclecluster
centroids), and Oracle θi (EWC with oracle user preference parameters). The x-axis represents the total decision rounds, while the y-axis represents the
regret.Lowerregretindicatesbetterperformance.Thecomparisonincludesthreescenarioswithdifferentvaluesofβ,whereβ indicatesthescaleofusers’
dependenceonrecommendationsindatageneration.Ahigherβ meansthepopulationhasastrongerpreferencefortherecommendedoption.
utilizes both compliance and non-compliance from users,
achievinglowregretindifferentfeedbackscenarios.Thiswork
lays the foundation for future research in Nah Bandit.
However, EWC may not achieve sublinear regret in the
long term because it uses the cluster centroid as an estimate
of user-specific preferences each time. In future work, we
plan to delve deeper into the Nah Bandit problem, aiming
to refine EWC algorithm to achieve low regret in both early
rounds and the long term. Additionally, clustering algorithms
often struggle with high-dimensional data due to the “curse
of dimensionality,” which means that EWC may not perform
Fig. 4. Regret of Expert with Clustering (EWC, Ours) and other baselines well when the option context dimension is high. We hope
(XGBoost,LinUCB,DYNUCB,andtheusernon-compliancemodel)applied
to incorporate the dimension reduction method into EWC
to restaurant recommendation data. The x-axis represents the total decision
rounds, while the y-axis represents the regret. Lower regret indicates better to handle high-dimensional data. In the sample complexity
performance. EWC consistently shows lower regret than other baseline analysis, we allow the user non-compliance model to use the
methods.
context of only two options for updates, transforming it into
a logistic regression. However, there is potential for sample
regret across the entire time span, demonstrating its effec- complexity analysis using the context of all options, which
tiveness in restaurant recommendation. XGBoost shows the could result in a tighter bound.
highest regret, likely due to its cold start problem. XGBoost
ACKNOWLEDGMENTS
model is more complex than other baselines, but the limited
This work was partially supported by the National Sci-
numberofdecisionroundsperuserisinsufficientforadequate
ence Foundation (NSF) under grant number 2149511 and the
training. DYNUCB exhibits lower regret than LinUCB, and
Kwanjeongscholarship.TheauthorswouldliketothankProf.
EWC outperforms the user non-compliance model, indicating
Hamed Tabkhi and Babak Rahimi Ardabili for their survey
thattheclusteringmethodefficientlyleveragesthehierarchical
datasupport,andProf.ChristosCassandrasandProf.Andreas
structure among users, thereby accelerating the preference
Malikopoulos for their insightful discussions.
learning process. The user non-compliance model performs
better than LinUCB, demonstrating its adaptability to the APPENDIXA
Nah Bandit problem. This also contributes to EWC’s superior PROOFDETAILS
performance compared to DYNUCB. A. Proof of Theorem 2
VIII. CONCLUSION Before describing our theoretical proofs, we first introduce
the backgrounds. In the expert problem, spanning T total
Inthispaper,weintroduceanovelbanditframework—Nah
rounds with K experts, we denote the best expert throughout
Bandit. This framework offers the potential to accelerate
the duration as k∗. [52] proved that the regret bound is:
preference learning. To solve this problem efficiently, we first
introduceausernon-compliancemodelthatparameterizesthe T
(cid:88) (cid:112)
anchoringeffecttoreducebiaswhenlearninguserpreferences. R Hedge = (⟨p t,l t⟩−l t(k∗))≤2 T logK (4)
Basedontheusernon-compliancemodel,weintroduceExpert t=1
withClustering (EWC),a novelhierarchical contextualbandit The loss of K-Means algorithm is define as L =
algorithmdesignedtoaddresstheNahBandit.EWCefficiently (cid:80)N ||c −θ ||2,wherek(i)istheclustercentroidassigned
i=1 k(i) iJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 10
to θ i. [48] proved the Uniform deviation bound of K-Means R
EWC
≤2N(cid:112) T logK+ 1 4TL(cid:0) ϵσ2+(ϵ+2)E[L](cid:1) (12)
algorithm. The result is as follows. Consider {c } be
k k∈[K]
any set of centroids, P as any distribution on Rd with mean
µ = E [θ ] and variance σ2 = E [||θ −µ||2]. Assuming
P i P i
finite Kurtosis (4th moment) Mˆ < ∞ and given ϵ ∈ (0,1), B. Proof of Corollary 1
δ ∈ (0,1) and a sample size
m4
from P, we establish that Proof. Since c k = µ k, and P =
(cid:80)K
k=1π kN(µ k,Σ k),
for m ≥
12800(8+Mˆ 4)(cid:0) 3+30K(d+4)log6K+log1(cid:1)
, the
the expected squared distance E[||θ
i
− c k(i)||2] =
uniformdeviatiϵ o2 nδ boundofK-Meansholdswithatleasδ t1−δ (cid:80)K k=1π ktrace(Σ k). So, E[L] = NE[||θ i − c k(i)||2] =
probability: N(cid:80)K k=1π ktrace(Σ k). Since l centroids = 41 NLϵσ2 + 1 4L(ϵ +
ϵ ϵ
2)(cid:80)K
π trace(Σ ), we can get R ≤ R =
|L−E [L]|≤ σ2+ E [L] (5) √k=1 k k EWC EWC
P 2 2 P 2N T logK+TNl centroids.
Combining the results above, we prove the regret bound of
EWC (Algorithm 2). C. Proof of Corollary 2
√
Proof. Since R = 2N T logK + TNl and
Proof. E(cid:113)WC centroids
R = CN Tdlog3(AT logT/δ), R < R
N T LinUCB EWC LinUCB
(cid:88)(cid:88) √ (cid:113)
R EWC = (⟨p i,t,l i,t⟩−l(yˆ(θ i,x i,t),y i,t)) (6) is √equivalent to Tl centroids < √C dlog3(AT lnT/δ) −
i=1 t=1 2 logK. Since K ≪ T, when Tl < C − 2, we
centroids
Since, l(yˆ(θ i,x i,t),y i,t) = 1 yˆ(θi,xi,t)̸=yi,t = 21||yˆ(θ i,x i,t)− can get R EWC <R LinUCB.
y ||2
i,t
D. Proof of Theorem 1
N T (cid:18) (cid:19)
(cid:88)(cid:88) 1
R EWC = ⟨p i,t,l i,t⟩− 2||yˆ(θ i,x i,t)−y i,t||2 Proof. Since we use the context of only two options to
i=1 t=1 update the model where one is the user’s choice and the
=(cid:88)N (cid:88)T (cid:18)
⟨p i,t,l i,t⟩−
1
2||yˆ(c k∗(i),x i,t)−y
i,t||2(cid:19) o pt ih ,te (r 1)is =not, σa (n (d
x
i,p t,1i,t −=
x
iσ ,t,( 2x )⊺ i ⊺,t θ,1 i)θ i, =x⊺ i,t,2 σθ (i x˜)
⊺
i, ,tθw ie ).ca Tn heg ree -t
i=1 t=1 fore, the user non-compliance model is equivalent to the
N T
+1(cid:88)(cid:88)(cid:0) ||yˆ(c ,x )−y ||2−||yˆ(θ ,x )−y ||2(cid:1) logistic regression. Since x˜ i,t ∼ N(0,I d), y i,t|x˜ i,t√ ∼
2 k∗(i) i,t i,t i i,t i,t Bern(σ(β x˜⊺ θ∗)), θ = θˆ ({x ,y }T ), β ≥ 4 2π,
i=1 t=1 i i,t i i ERM i,t i,t t=1 i ϵ
(7) and T ≥ C(dlog(1/ϵ)+log(1/δ)), by Lemma 1, with probability
ϵ
Recall that 1 2||yˆ(c k,x i,t) − y i,t||2 = l(yˆ(c k,x i,t),y i,t) = at least 1−δ, we have ||θ i−θ i∗||≤ϵ.
l (k),
i,t
N T
APPENDIXB
(cid:88)(cid:88)
R = (⟨p ,l ⟩−l (k∗(i))) EXPERIMENTALSETUPDETAILS
EWC i,t i,t i,t
i=1 t=1 A. Travel Route Recommendation
N
+1(cid:88)(cid:0) ||yˆ(c ,X )−y ||2 −||yˆ(θ ,X )−y ||2(cid:1) In the travel route recommendation, the contexts include
2 k∗(i) i i F i i i F travel time and CO2 emission of different routes: the regular
i=1
(8) route and the eco-friendly route. The context of the regular
By the regret bound of Hedge and triangle inequality, route is [100,100], which means 100% of travel time and
CO2 emission. The eco-friendly route has higher travel time
N
R ≤2N(cid:112) T logK+ 1(cid:88) ||yˆ(c ,X )−yˆ(θ ,X )||2 and lower CO2 emission compared to the regular one. We
EWC 2 k∗(i) i i i F generatethetrainingandtestingdatabasedonthesurveydata.
i=1
(9) The user preference parameter θ is initially sampled from a
i
BytheLipschitzcondition,∃Ls.t.∀i,∀θ ,θ , 1||yˆ(θ ,X )− multivariate normal distribution. We assume θ also shows
1 2 T 1 i rec
yˆ(θ ,X )||2 ≤L||θ −θ ||2 cluster characteristic, so we sample θ from a beta distribu-
2 i F 1 2 rec
tion B(0.3,0.3).Each dimensionof the optioncontext forthe
N
(cid:88)
||yˆ(c ,X )−yˆ(θ ,X )||2 eco-friendlyrouteisgeneratedfromanormaldistribution.The
k∗(i) i i i F
parameters of data generation and experiment of travel route
i=1
N (10) recommendationarelistedinTableII.Theusercontext“Age”
(cid:88)
≤TL ||c −θ ||2 and“Educationlevel”aretransformedintoone-hotencodings,
k∗(i) i
i=1 while others are binary variables.
=TLL≤TL(|L−E[L]|+E[L]) B. Restaurant recommendation
By inequation 5, with probability at least 1−δ, The parameters used in the restaurant recommendation
experiment are listed in Table IV. The description of the
N
(cid:88) ||yˆ(c ,X )−yˆ(θ ,X )||2 option context is shown in Table V. The user context ’Style’
k∗(i) i i i F
(11) is transformed into one-hot encoding. ‘Food quality’ and
i=1
(cid:16)ϵ ϵ (cid:17) ‘Servicelevel’aretransformedto(0,0.25,0.5,0.75,1).’Price’
≤TL σ2+( +1)E[L]
2 2 is transformed to (0,0.33,0.67,1) accordingly.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 11
TABLEII TABLEIV
ALISTOFPARAMETERSANDTHEIRVALUESINTRAVELROUTE ALISTOFPARAMETERSANDTHEIRVALUESINRESTAURANT
RECOMMENDATION RECOMMENDATION
Parameter Value Parameter Value
NumberofdecisionroundsT 40 Optioncontextdimensionsd 9
UsercontextdimensionsD 9 NumberofdecisionroundsT 3–105
Optioncontextdimensionsd 2 NumberofoptionsA 2–18
NumberofoptionsA 2 Parameterfortraining
Parameterfordatageneration NumberofusersN′ 188
Acceptancetemperatureλ 6 Learningrate 0.5
Meanofθi [−0.1,−0.1] L2 regularizationparameter 0.01
(cid:20) 0.01 0 (cid:21) NumberofclustersK 8
Covarianceofθi
0 0.01 Parameterfortesting
Meanofoptioncontext [104.29,91.99] NumberofusersN 75
StandardDeviationofoptioncontext [5.62,4.06] ExplorationrateforLinUCB 0.05
Parameterfortraining LearningrateforEWC 1
NumberofusersN′ 446
Learningrate 0.5
L2 regularizationparameter 0.001 TABLEV
NumberofclustersK 6 DESCRIPTIONOFOPTIONCONTEXTINRESTAURANTRECOMMENDATION
Parameterfortesting
NumberofusersN 298 Optioncontext Range Dimensions
ExplorationrateforLinUCB 0.05 {Fair,good,excellent,
Foodquality 1
LearningrateforEWC 1 extraordinary,near-perfect}
{Fair,good,excellent,
Servicelevel 1
extraordinary,near-perfect}
TABLEIII {Below$15,$15-$30,
Price 1
DESCRIPTIONOFUSERCONTEXTANDOPTIONCONTEXTINTRAVEL $30-$50,over$50}
ROUTERECOMMENDATION.
Style
{American,Asian,Latin,
6
MiddleEastern,other}
Context Range Dimensions
UserContext
Age {18-34,35-49,50-64} 3
Gender {Male,female} 1 [9] S. Tomkins, P. Liao, P. Klasnja, and S. Murphy, “Intelligentpooling:
{Highschool,bachelor, Practicalthompsonsamplingformhealth,”Machinelearning,vol.110,
Educationlevel 3
masterorhigher} no.9,pp.2685–2727,2021.
Numberofcars {One,twoormore} 1 [10] S.AgrawalandN.Goyal,“Thompsonsamplingforcontextualbandits
Intercept {1} 1 with linear payoffs,” in International conference on machine learning,
Optioncontext pp.127–135,PMLR,2013.
Traveltime R 1 [11] D.Zhou,L.Li,andQ.Gu,“Neuralcontextualbanditswithucb-based
CO2emission R 1 exploration,”2020.
[12] A.TverskyandD.Kahneman,“Judgmentunderuncertainty:Heuristics
andbiases:Biasesinjudgmentsrevealsomeheuristicsofthinkingunder
uncertainty.,”science,vol.185,no.4157,pp.1124–1131,1974.
REFERENCES [13] D.Green,K.E.Jacowitz,D.Kahneman,andD.McFadden,“Referen-
dumcontingentvaluation,anchoring,andwillingnesstopayforpublic
[1] L. Li, W. Chu, J. Langford, and R. E. Schapire, “A Contextual- goods,” Resource and energy economics, vol. 20, no. 2, pp. 85–116,
Bandit Approach to Personalized News Article Recommendation,” in 1998.
Proceedings of the 19th international conference on World wide web, [14] A.GunawardanaandG.Shani,“Asurveyofaccuracyevaluationmetrics
pp.661–670,Apr.2010. arXiv:1003.0146[cs]. of recommendation tasks.,” Journal of Machine Learning Research,
[2] F.Pase,D.Gu¨ndu¨z,andM.Zorzi,“Rate-constrainedremotecontextual vol.10,no.12,2009.
bandits,”2022. [15] A.FurnhamandH.C.Boo,“Aliteraturereviewoftheanchoringeffect,”
[3] M.GigliandF.Stella,“Parametricbanditsforsearchenginemarketing Thejournalofsocio-economics,vol.40,no.1,pp.35–42,2011.
optimisation,”inAdvancesinKnowledgeDiscoveryandDataMining: [16] G.Adomavicius,J.C.Bockstedt,S.P.Curley,andJ.Zhang,“Dorecom-
26th Pacific-Asia Conference, PAKDD 2022, Chengdu, China, May mendersystemsmanipulateconsumerpreferences?astudyofanchoring
16–19, 2022, Proceedings, Part III, (Berlin, Heidelberg), p. 326–337, effects,” Information Systems Research, vol. 24, no. 4, pp. 956–975,
Springer-Verlag,2022. 2013.
[4] H. K. Kim, J. K. Kim, and Y. U. Ryu, “Personalized recommendation [17] W.S.Rossi,J.W.Polderman,andP.Frasca,“Theclosedloopbetween
over a customer network for ubiquitous shopping,” IEEE Transactions opinion formation and personalized recommendations,” IEEE Transac-
onServicesComputing,vol.2,no.2,pp.140–151,2009. tions on Control of Network Systems, vol. 9, no. 3, pp. 1092–1103,
[5] P.Wang,J.Guo,andY.Lan,“Modelingretailtransactiondataforper- 2022.
sonalizedshoppingrecommendation,”inProceedingsofthe23rdACM [18] D.Goldberg,D.Nichols,B.M.Oki,andD.Terry,“Usingcollaborative
international conference on conference on information and knowledge filteringtoweaveaninformationtapestry,”CommunicationsoftheACM,
management,pp.1979–1982,2014. vol.35,no.12,pp.61–70,1992.
[6] J. Jin, H. Guo, J. Xu, X. Wang, and F.-Y. Wang, “An end-to-end [19] X. Cheng and S. Maghsudi, “Distributed consensus algorithm for
recommendation system for urban traffic controls and management decision-makinginmulti-agentmulti-armedbandit,”IEEETransactions
underaparallellearningframework,”IEEETransactionsonIntelligent onControlofNetworkSystems,pp.1–12,2024.
TransportationSystems,vol.22,no.3,pp.1616–1626,2021. [20] A. M. Ospina, A. Simonetto, and E. Dall’Anese, “Time-varying opti-
[7] O. Massicot and C. Langbort, “Competitive comparisons of strategic mization ofnetworked systems with human preferences,” IEEE Trans-
informationprovisionpoliciesinnetworkroutinggames,”IEEETrans- actions on Control of Network Systems, vol. 10, no. 1, pp. 503–515,
actions on Control of Network Systems, vol. 9, no. 4, pp. 1589–1599, 2023.
2022. [21] T. T. Nguyen and H. W. Lauw, “Dynamic clustering of contextual
[8] A. Tewari and S. A. Murphy, “From ads to interventions: Contextual multi-armed bandits,” in Proceedings of the 23rd ACM international
banditsinmobilehealth,”Mobilehealth:sensors,analyticmethods,and conference on conference on information and knowledge management,
applications,pp.495–517,2017. pp.1959–1962,2014.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 12
[22] C. Gentile, S. Li, and G. Zappella, “Online clustering of bandits,” in [47] L.F.Toso,H.Wang,andJ.Anderson,“Learningpersonalizedmodels
Proceedingsofthe31stInternationalConferenceonMachineLearning withclusteredsystemidentification,”in202362ndIEEEConferenceon
(E. P. Xing and T. Jebara, eds.), vol. 32 of Proceedings of Machine DecisionandControl(CDC),pp.7162–7169,2023.
Learning Research, (Bejing, China), pp. 757–765, PMLR, 22–24 Jun [48] O.Bachem,M.Lucic,S.H.Hassani,andA.Krause,“Uniformdeviation
2014. boundsforunboundedlossfunctionslikek-means,”2017.
[23] N. Littlestone and M. Warmuth, “The weighted majority algorithm,” [49] W. Chu, L. Li, L. Reyzin, and R. Schapire, “Contextual bandits with
InformationandComputation,vol.108,no.2,pp.212–261,1994. linearpayofffunctions,”inProceedingsoftheFourteenthInternational
[24] T.Zhou,J.-H.Cho,B.R.Ardabili,H.Tabkhi,andC.Wu,“Expertwith ConferenceonArtificialIntelligenceandStatistics,pp.208–214,JMLR
clustering:Hierarchicalonlinepreferencelearningframework,”2024. WorkshopandConferenceProceedings,2011.
[25] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl, “Item-based collabo- [50] C.Andrieu,N.deFreitas,A.Doucet,andM.I.Jordan,“AnIntroduction
rativefilteringrecommendationalgorithms,”inProceedingsofthe10th toMCMCforMachineLearning,”MachineLearning,vol.50,pp.5–43,
internationalconferenceonWorldWideWeb,pp.285–295,2001. Jan.2003.
[26] Y.Koren,R.Bell,andC.Volinsky,“Matrixfactorizationtechniquesfor [51] R. Burke, “Entree Chicago Recommendation Data.” UCI Machine
recommendersystems,”Computer,vol.42,no.8,pp.30–37,2009. LearningRepository,2000. DOI:https://doi.org/10.24432/C5088P.
[27] A. S. Das, M. Datar, A. Garg, and S. Rajaram, “Google news person- [52] Y. Freund and R. E. Schapire, “A decision-theoretic generalization of
alization: scalable online collaborative filtering,” in Proceedings of the on-line learning and an application to boosting,” Journal of Computer
16th International Conference on World Wide Web, WWW ’07, (New andSystemSciences,vol.55,no.1,pp.119–139,1997.
York, NY, USA), p. 271–280, Association for Computing Machinery,
2007.
APPENDIXC
[28] X.He,H.Zhang,M.-Y.Kan,andT.-S.Chua,“Fastmatrixfactorization
for online recommendation with implicit feedback,” in Proceedings BIOGRAPHYSECTION
of the 39th International ACM SIGIR conference on Research and
DevelopmentinInformationRetrieval,pp.549–558,2016. Tianyue Zhou is an undergraduate student in the
[29] J. H. Friedman, “Greedy function approximation: a gradient boosting School of Information Science and Technology at
machine,”Annalsofstatistics,pp.1189–1232,2001. ShanghaiTechUniversity.Hisresearchinterestslieat
[30] L. Breiman, “Random forests,” Machine learning, vol. 45, pp. 5–32, theintersectionofmachinelearningandtransporta-
2001. tion.Heaimstodevelopefficientmachinelearning
[31] B. Lakshminarayanan, D. M. Roy, and Y. W. Teh, “Mondrian forests: algorithmstoaddressvariousreal-worldchallenges
Efficient online random forests,” Advances in neural information pro- intransportation.
cessingsystems,vol.27,2014.
[32] A. Beygelzimer, E. Hazan, S. Kale, and H. Luo, “Online gradient
boosting,”Advancesinneuralinformationprocessingsystems,vol.28,
2015. Jung-Hoon Cho is a Ph.D. candidate in Civil and
[33] T. Chen and C. Guestrin, “Xgboost: A scalable tree boosting system,” EnvironmentalEngineeringattheMassachusettsIn-
in Proceedings of the 22nd acm sigkdd international conference on stitute of Technology (MIT). He earned both his
knowledgediscoveryanddatamining,pp.785–794,2016. M.S. and B.S. degrees in Civil and Environmental
[34] P. Auer, “Using confidence bounds for exploitation-exploration trade- Engineering from Seoul National University. His
offs,”JournalofMachineLearningResearch,vol.3,no.Nov,pp.397– primary research interest lies at the intersection
422,2002. of transportation and machine learning. Jung-Hoon
[35] S. Lloyd, “Least squares quantization in pcm,” IEEE transactions on aims to develop generalizable machine learning
informationtheory,vol.28,no.2,pp.129–137,1982. models to optimize traffic flow, thereby reducing
[36] A. Shepitsen, J. Gemmell, B. Mobasher, and R. Burke, “Personalized urbancongestionandgreenhousegasemissions.
recommendationinsocialtaggingsystemsusinghierarchicalclustering,”
inProceedingsofthe2008ACMConferenceonRecommenderSystems,
Cathy Wu is an Associate Professor at MIT in
RecSys ’08, (New York, NY, USA), p. 259–266, Association for
LIDS,CEE,andIDSS.SheholdsaPh.D.fromUC
ComputingMachinery,2008.
Berkeley, and B.S. and M.Eng. from MIT, all in
[37] Y.BanandJ.He,“Localclusteringincontextualmulti-armedbandits,”
EECS, and completed a Postdoc at Microsoft Re-
2023.
search.Herresearchinterestsareattheintersection
[38] J. Cao, W. Sun, Z.-J. M. Shen, and M. Ettl, “Fatigue-Aware Bandits
of machine learning, autonomy, and mobility. Her
forDependentClickModels,”ProceedingsoftheAAAIConferenceon
researchaimstoadvancegeneralizableoptimization
ArtificialIntelligence,vol.34,pp.3341–3348,Apr.2020.
to enable next-generation mobility systems. Cathy
[39] M.Yeomans,A.Shah,S.Mullainathan,andJ.Kleinberg,“Makingsense
is the recipient of the NSF CAREER, several PhD
ofrecommendations,”JournalofBehavioralDecisionMaking,vol.32,
dissertation awards, and several publications with
no.4,pp.403–414,2019.
distinction. She serves on the Board of Governors
[40] G.Bahar,O.Ben-Porat,K.Leyton-Brown,andM.Tennenholtz,“Fidu-
fortheIEEEITSSandasanAreaChairforICMLandNeurIPS.
ciaryBandits,”inProceedingsofthe37thInternationalConferenceon
MachineLearning,pp.518–527,PMLR,Nov.2020.
[41] J.Cao,W.Sun,andZ.-J.M.Shen,“DoublyAdaptiveCascadingBandits
withUserAbandonment,”Mar.2019.
[42] Z.Yang,X.Liu,andL.Ying,“Exploration.Exploitation,andEngage-
mentinMulti-ArmedBanditswithAbandonment,”in202258thAnnual
AllertonConferenceonCommunication,Control,andComputing(Aller-
ton),pp.1–2,Sept.2022.
[43] X.Wang,H.Xie,P.Wang,andJ.C.S.Lui,“Optimizingrecommenda-
tions under abandonment risks: Models and algorithms,” Performance
Evaluation,vol.161,p.102351,Sept.2023.
[44] D. Hsu and A. Mazumdar, “On the sample complexity of parameter
estimationinlogisticregressionwithnormaldesign,”2024.
[45] A. Ghosh, J. Chung, D. Yin, and K. Ramchandran, “An efficient
framework for clustered federated learning,” in Advances in Neural
InformationProcessingSystems(H.Larochelle,M.Ranzato,R.Hadsell,
M. Balcan, and H. Lin, eds.), vol. 33, pp. 19586–19597, Curran
Associates,Inc.,2020.
[46] F. Sattler, K.-R. Mu¨ller, and W. Samek, “Clustered federated learning:
Model-agnostic distributed multitask optimization under privacy con-
straints,”IEEETransactionsonNeuralNetworksandLearningSystems,
vol.32,no.8,pp.3710–3722,2021.