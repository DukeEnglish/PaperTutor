[
    {
        "title": "Can Large Language Models Understand Symbolic Graphics Programs?",
        "authors": "Zeju QiuWeiyang LiuHaiwen FengZhen LiuTim Z. XiaoKatherine M. CollinsJoshua B. TenenbaumAdrian WellerMichael J. BlackBernhard Schölkopf",
        "links": "http://arxiv.org/abs/2408.08313v1",
        "entry_id": "http://arxiv.org/abs/2408.08313v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08313v1",
        "summary": "Assessing the capabilities of large language models (LLMs) is often\nchallenging, in part, because it is hard to find tasks to which they have not\nbeen exposed during training. We take one step to address this challenge by\nturning to a new task: focusing on symbolic graphics programs, which are a\npopular representation for graphics content that procedurally generates visual\ndata. LLMs have shown exciting promise towards program synthesis, but do they\nunderstand symbolic graphics programs? Unlike conventional programs, symbolic\ngraphics programs can be translated to graphics content. Here, we characterize\nan LLM's understanding of symbolic programs in terms of their ability to answer\nquestions related to the graphics content. This task is challenging as the\nquestions are difficult to answer from the symbolic programs alone -- yet, they\nwould be easy to answer from the corresponding graphics content as we verify\nthrough a human experiment. To understand symbolic programs, LLMs may need to\npossess the ability to imagine how the corresponding graphics content would\nlook without directly accessing the rendered visual content. We use this task\nto evaluate LLMs by creating a large benchmark for the semantic understanding\nof symbolic graphics programs. This benchmark is built via program-graphics\ncorrespondence, hence requiring minimal human efforts. We evaluate current LLMs\non our benchmark to elucidate a preliminary assessment of their ability to\nreason about visual scenes from programs. We find that this task distinguishes\nexisting LLMs and models considered good at reasoning perform better. Lastly,\nwe introduce Symbolic Instruction Tuning (SIT) to improve this ability.\nSpecifically, we query GPT4-o with questions and images generated by symbolic\nprograms. Such data are then used to finetune an LLM. We also find that SIT\ndata can improve the general instruction following ability of LLMs.",
        "updated": "2024-08-15 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08313v1"
    },
    {
        "title": "Understanding the Local Geometry of Generative Model Manifolds",
        "authors": "Ahmed Imtiaz HumayunIbtihel AmaraCandice SchumannGolnoosh FarnadiNegar RostamzadehMohammad Havaei",
        "links": "http://arxiv.org/abs/2408.08307v1",
        "entry_id": "http://arxiv.org/abs/2408.08307v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08307v1",
        "summary": "Deep generative models learn continuous representations of complex data\nmanifolds using a finite number of samples during training. For a pre-trained\ngenerative model, the common way to evaluate the quality of the manifold\nrepresentation learned, is by computing global metrics like Fr\\'echet Inception\nDistance using a large number of generated and real samples. However,\ngenerative model performance is not uniform across the learned manifold, e.g.,\nfor \\textit{foundation models} like Stable Diffusion generation performance can\nvary significantly based on the conditioning or initial noise vector being\ndenoised. In this paper we study the relationship between the \\textit{local\ngeometry of the learned manifold} and downstream generation. Based on the\ntheory of continuous piecewise-linear (CPWL) generators, we use three geometric\ndescriptors - scaling ($\\psi$), rank ($\\nu$), and complexity ($\\delta$) - to\ncharacterize a pre-trained generative model manifold locally. We provide\nquantitative and qualitative evidence showing that for a given latent, the\nlocal descriptors are correlated with generation aesthetics, artifacts,\nuncertainty, and even memorization. Finally we demonstrate that training a\n\\textit{reward model} on the local geometry can allow controlling the\nlikelihood of a generated sample under the learned distribution.",
        "updated": "2024-08-15 17:59:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08307v1"
    },
    {
        "title": "Benchmarking the Capabilities of Large Language Models in Transportation System Engineering: Accuracy, Consistency, and Reasoning Behaviors",
        "authors": "Usman SyedEthan LightXingang GuoHuan ZhangLianhui QinYanfeng OuyangBin Hu",
        "links": "http://arxiv.org/abs/2408.08302v1",
        "entry_id": "http://arxiv.org/abs/2408.08302v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08302v1",
        "summary": "In this paper, we explore the capabilities of state-of-the-art large language\nmodels (LLMs) such as GPT-4, GPT-4o, Claude 3.5 Sonnet, Claude 3 Opus, Gemini\n1.5 Pro, Llama 3, and Llama 3.1 in solving some selected undergraduate-level\ntransportation engineering problems. We introduce TransportBench, a benchmark\ndataset that includes a sample of transportation engineering problems on a wide\nrange of subjects in the context of planning, design, management, and control\nof transportation systems. This dataset is used by human experts to evaluate\nthe capabilities of various commercial and open-sourced LLMs, especially their\naccuracy, consistency, and reasoning behaviors, in solving transportation\nengineering problems. Our comprehensive analysis uncovers the unique strengths\nand limitations of each LLM, e.g. our analysis shows the impressive accuracy\nand some unexpected inconsistent behaviors of Claude 3.5 Sonnet in solving\nTransportBench problems. Our study marks a thrilling first step toward\nharnessing artificial general intelligence for complex transportation\nchallenges.",
        "updated": "2024-08-15 17:55:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08302v1"
    },
    {
        "title": "HELP: Hierarchical Embeddings-based Log Parsing",
        "authors": "Andy XuArno Gau",
        "links": "http://arxiv.org/abs/2408.08300v1",
        "entry_id": "http://arxiv.org/abs/2408.08300v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08300v1",
        "summary": "Logs are a first-hand source of information for software maintenance and\nfailure diagnosis. Log parsing, which converts semi-structured log messages\ninto structured templates, is a prerequisite for automated log analysis tasks\nsuch as anomaly detection, troubleshooting, and root cause analysis. However,\nexisting log parsers fail in real-world systems for three main reasons. First,\ntraditional heuristics-based parsers require handcrafted features and domain\nknowledge, which are difficult to generalize at scale. Second, existing large\nlanguage model-based parsers rely on periodic offline processing, limiting\ntheir effectiveness in real-time use cases. Third, existing online parsing\nalgorithms are susceptible to log drift, where slight log changes create false\npositives that drown out real anomalies. To address these challenges, we\npropose HELP, a Hierarchical Embeddings-based Log Parser. HELP is the first\nonline semantic-based parser to leverage LLMs for performant and cost-effective\nlog parsing. We achieve this through a novel hierarchical embeddings module,\nwhich fine-tunes a text embedding model to cluster logs before parsing,\nreducing querying costs by multiple orders of magnitude. To combat log drift,\nwe also develop an iterative rebalancing module, which periodically updates\nexisting log groupings. We evaluate HELP extensively on 14 public large-scale\ndatasets, showing that HELP achieves significantly higher F1-weighted grouping\nand parsing accuracy than current state-of-the-art online log parsers. We also\nimplement HELP into Iudex's production observability platform, confirming\nHELP's practicality in a production environment. Our results show that HELP is\neffective and efficient for high-throughput real-world log parsing.",
        "updated": "2024-08-15 17:54:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08300v1"
    },
    {
        "title": "SLCA++: Unleash the Power of Sequential Fine-tuning for Continual Learning with Pre-training",
        "authors": "Gengwei ZhangLiyuan WangGuoliang KangLing ChenYunchao Wei",
        "links": "http://arxiv.org/abs/2408.08295v1",
        "entry_id": "http://arxiv.org/abs/2408.08295v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08295v1",
        "summary": "In recent years, continual learning with pre-training (CLPT) has received\nwidespread interest, instead of its traditional focus of training from scratch.\nThe use of strong pre-trained models (PTMs) can greatly facilitate knowledge\ntransfer and alleviate catastrophic forgetting, but also suffers from\nprogressive overfitting of pre-trained knowledge into specific downstream\ntasks. A majority of current efforts often keep the PTMs frozen and incorporate\ntask-specific prompts to instruct representation learning, coupled with a\nprompt selection process for inference. However, due to the limited capacity of\nprompt parameters, this strategy demonstrates only sub-optimal performance in\ncontinual learning. In comparison, tuning all parameters of PTMs often provides\nthe greatest potential for representation learning, making sequential\nfine-tuning (Seq FT) a fundamental baseline that has been overlooked in CLPT.\nTo this end, we present an in-depth analysis of the progressive overfitting\nproblem from the lens of Seq FT. Considering that the overly fast\nrepresentation learning and the biased classification layer constitute this\nparticular problem, we introduce the advanced Slow Learner with Classifier\nAlignment (SLCA++) framework to unleash the power of Seq FT, serving as a\nstrong baseline approach for CLPT. Our approach involves a Slow Learner to\nselectively reduce the learning rate of backbone parameters, and a Classifier\nAlignment to align the disjoint classification layers in a post-hoc fashion. We\nfurther enhance the efficacy of SL with a symmetric cross-entropy loss, as well\nas employ a parameter-efficient strategy to implement Seq FT with SLCA++.\nAcross a variety of continual learning scenarios on image classification\nbenchmarks, our approach provides substantial improvements and outperforms\nstate-of-the-art methods by a large margin. Code:\nhttps://github.com/GengDavid/SLCA.",
        "updated": "2024-08-15 17:50:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08295v1"
    }
]