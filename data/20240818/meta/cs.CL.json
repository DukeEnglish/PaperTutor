[
    {
        "title": "Can Large Language Models Understand Symbolic Graphics Programs?",
        "authors": "Zeju QiuWeiyang LiuHaiwen FengZhen LiuTim Z. XiaoKatherine M. CollinsJoshua B. TenenbaumAdrian WellerMichael J. BlackBernhard Schölkopf",
        "links": "http://arxiv.org/abs/2408.08313v1",
        "entry_id": "http://arxiv.org/abs/2408.08313v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08313v1",
        "summary": "Assessing the capabilities of large language models (LLMs) is often\nchallenging, in part, because it is hard to find tasks to which they have not\nbeen exposed during training. We take one step to address this challenge by\nturning to a new task: focusing on symbolic graphics programs, which are a\npopular representation for graphics content that procedurally generates visual\ndata. LLMs have shown exciting promise towards program synthesis, but do they\nunderstand symbolic graphics programs? Unlike conventional programs, symbolic\ngraphics programs can be translated to graphics content. Here, we characterize\nan LLM's understanding of symbolic programs in terms of their ability to answer\nquestions related to the graphics content. This task is challenging as the\nquestions are difficult to answer from the symbolic programs alone -- yet, they\nwould be easy to answer from the corresponding graphics content as we verify\nthrough a human experiment. To understand symbolic programs, LLMs may need to\npossess the ability to imagine how the corresponding graphics content would\nlook without directly accessing the rendered visual content. We use this task\nto evaluate LLMs by creating a large benchmark for the semantic understanding\nof symbolic graphics programs. This benchmark is built via program-graphics\ncorrespondence, hence requiring minimal human efforts. We evaluate current LLMs\non our benchmark to elucidate a preliminary assessment of their ability to\nreason about visual scenes from programs. We find that this task distinguishes\nexisting LLMs and models considered good at reasoning perform better. Lastly,\nwe introduce Symbolic Instruction Tuning (SIT) to improve this ability.\nSpecifically, we query GPT4-o with questions and images generated by symbolic\nprograms. Such data are then used to finetune an LLM. We also find that SIT\ndata can improve the general instruction following ability of LLMs.",
        "updated": "2024-08-15 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08313v1"
    },
    {
        "title": "ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws",
        "authors": "Ruihang LiYixuan WeiMiaosen ZhangNenghai YuHan HuHouwen Peng",
        "links": "http://arxiv.org/abs/2408.08310v1",
        "entry_id": "http://arxiv.org/abs/2408.08310v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08310v1",
        "summary": "High-quality data is crucial for the pre-training performance of large\nlanguage models. Unfortunately, existing quality filtering methods rely on a\nknown high-quality dataset as reference, which can introduce potential bias and\ncompromise diversity. In this paper, we propose ScalingFilter, a novel approach\nthat evaluates text quality based on the perplexity difference between two\nlanguage models trained on the same data, thereby eliminating the influence of\nthe reference dataset in the filtering process. An theoretical analysis shows\nthat ScalingFilter is equivalent to an inverse utilization of scaling laws.\nThrough training models with 1.3B parameters on the same data source processed\nby various quality filters, we find ScalingFilter can improve zero-shot\nperformance of pre-trained models in downstream tasks. To assess the bias\nintroduced by quality filtering, we introduce semantic diversity, a metric of\nutilizing text embedding models for semantic representations. Extensive\nexperiments reveal that semantic diversity is a reliable indicator of dataset\ndiversity, and ScalingFilter achieves an optimal balance between downstream\nperformance and semantic diversity.",
        "updated": "2024-08-15 17:59:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08310v1"
    },
    {
        "title": "Benchmarking the Capabilities of Large Language Models in Transportation System Engineering: Accuracy, Consistency, and Reasoning Behaviors",
        "authors": "Usman SyedEthan LightXingang GuoHuan ZhangLianhui QinYanfeng OuyangBin Hu",
        "links": "http://arxiv.org/abs/2408.08302v1",
        "entry_id": "http://arxiv.org/abs/2408.08302v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08302v1",
        "summary": "In this paper, we explore the capabilities of state-of-the-art large language\nmodels (LLMs) such as GPT-4, GPT-4o, Claude 3.5 Sonnet, Claude 3 Opus, Gemini\n1.5 Pro, Llama 3, and Llama 3.1 in solving some selected undergraduate-level\ntransportation engineering problems. We introduce TransportBench, a benchmark\ndataset that includes a sample of transportation engineering problems on a wide\nrange of subjects in the context of planning, design, management, and control\nof transportation systems. This dataset is used by human experts to evaluate\nthe capabilities of various commercial and open-sourced LLMs, especially their\naccuracy, consistency, and reasoning behaviors, in solving transportation\nengineering problems. Our comprehensive analysis uncovers the unique strengths\nand limitations of each LLM, e.g. our analysis shows the impressive accuracy\nand some unexpected inconsistent behaviors of Claude 3.5 Sonnet in solving\nTransportBench problems. Our study marks a thrilling first step toward\nharnessing artificial general intelligence for complex transportation\nchallenges.",
        "updated": "2024-08-15 17:55:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08302v1"
    },
    {
        "title": "The ShareLM Collection and Plugin: Contributing Human-Model Chats for the Benefit of the Community",
        "authors": "Shachar Don-YehiyaLeshem ChoshenOmri Abend",
        "links": "http://arxiv.org/abs/2408.08291v1",
        "entry_id": "http://arxiv.org/abs/2408.08291v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08291v1",
        "summary": "Human-model conversations provide a window into users' real-world scenarios,\nbehavior, and needs, and thus are a valuable resource for model development and\nresearch. While for-profit companies collect user data through the APIs of\ntheir models, using it internally to improve their own models, the open source\nand research community lags behind.\n  We introduce the ShareLM collection, a unified set of human conversations\nwith large language models, and its accompanying plugin, a Web extension for\nvoluntarily contributing user-model conversations. Where few platforms share\ntheir chats, the ShareLM plugin adds this functionality, thus, allowing users\nto share conversations from most platforms. The plugin allows the user to rate\ntheir conversations, both at the conversation and the response levels, and\ndelete conversations they prefer to keep private before they ever leave the\nuser's local storage. We release the plugin conversations as part of the\nShareLM collection, and call for more community effort in the field of open\nhuman-model data.\n  The code, plugin, and data are available.",
        "updated": "2024-08-15 17:46:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08291v1"
    },
    {
        "title": "mhGPT: A Lightweight Generative Pre-Trained Transformer for Mental Health Text Analysis",
        "authors": "Dae-young KimRebecca HwaMuhammad Mahbubur Rahman",
        "links": "http://arxiv.org/abs/2408.08261v1",
        "entry_id": "http://arxiv.org/abs/2408.08261v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08261v1",
        "summary": "This paper introduces mhGPT, a lightweight generative pre-trained transformer\ntrained on mental health-related social media and PubMed articles. Fine-tuned\nfor specific mental health tasks, mhGPT was evaluated under limited hardware\nconstraints and compared with state-of-the-art models like MentaLLaMA and\nGemma. Despite having only 1.98 billion parameters and using just 5% of the\ndataset, mhGPT outperformed larger models and matched the performance of models\ntrained on significantly more data. The key contributions include integrating\ndiverse mental health data, creating a custom tokenizer, and optimizing a\nsmaller architecture for low-resource settings. This research could advance\nAI-driven mental health care, especially in areas with limited computing power.",
        "updated": "2024-08-15 17:01:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08261v1"
    }
]