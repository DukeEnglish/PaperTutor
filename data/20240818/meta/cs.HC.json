[
    {
        "title": "Marker or Markerless? Mode-Switchable Optical Tactile Sensing for Diverse Robot Tasks",
        "authors": "Ni OuZhuo ChenShan Luo",
        "links": "http://arxiv.org/abs/2408.08276v1",
        "entry_id": "http://arxiv.org/abs/2408.08276v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08276v1",
        "summary": "Optical tactile sensors play a pivotal role in robot perception and\nmanipulation tasks. The membrane of these sensors can be painted with markers\nor remain markerless, enabling them to function in either marker or markerless\nmode. However, this uni-modal selection means the sensor is only suitable for\neither manipulation or perception tasks. While markers are vital for\nmanipulation, they can also obstruct the camera, thereby impeding perception.\nThe dilemma of selecting between marker and markerless modes presents a\nsignificant obstacle. To address this issue, we propose a novel mode-switchable\noptical tactile sensing approach that facilitates transitions between the two\nmodes. The marker-to-markerless transition is achieved through a generative\nmodel, whereas its inverse transition is realized using a sparsely supervised\nregressive model. Our approach allows a single-mode optical sensor to operate\neffectively in both marker and markerless modes without the need for additional\nhardware, making it well-suited for both perception and manipulation tasks.\nExtensive experiments validate the effectiveness of our method. For perception\ntasks, our approach decreases the number of categories that include\nmisclassified samples by 2 and improves contact area segmentation IoU by 3.53%.\nFor manipulation tasks, our method attains a high success rate of 92.59% in\nslip detection. Code, dataset and demo videos are available at the project\nwebsite: https://gitouni.github.io/Marker-Markerless-Transition/",
        "updated": "2024-08-15 17:21:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08276v1"
    },
    {
        "title": "ESCape the ClassRoom",
        "authors": "John O'Connor",
        "links": "http://arxiv.org/abs/2408.08273v1",
        "entry_id": "http://arxiv.org/abs/2408.08273v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08273v1",
        "summary": "Educational Escape Rooms (EER's), through their use of immersive storytelling\nand practical application of abstract concepts, present a novel new technique\nfor engaging learners in a variety of subjects. However, there is a significant\ntime and materials investment required to build new physical Escape Rooms, and\nprior attempts to create digital escape rooms have resulted in games that lack\nthe immersive qualities that make physical escape rooms so compelling. This\npaper presents ESCape the Classroom, a web framework for creating virtual\nreality educational escape rooms (VR EERs) that can be delivered to any\nweb-connected device. The framework is equipped with essential tools to design\nand deploy intricate, multi-room VR escape experiences using HTML and\nWeb-Components. It is designed to be used by educators with rudimentary\nprogramming skills, eliminating the need for advanced game programming or\ndevelopment expertise. VR EERs created with this platform can be published\nonline as WebXR sites that are compatible with a broad spectrum of VR hardware,\nincluding the Meta Quest 3, allowing educators to share the experiences they\ncreate while bypassing the need for additional software installations on\ndevices. This paper will present the design and implementation of ESCape the\nClassroom, and discuss the potential for this platform to be used in\neducational settings.",
        "updated": "2024-08-15 17:18:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08273v1"
    },
    {
        "title": "\"I Try to Represent Myself as I Am\": Self-Presentation Preferences of People with Invisible Disabilities through Embodied Social VR Avatars",
        "authors": "Ria J. GualanoLucy JiangKexin ZhangTanisha ShendeAndrea Stevenson WonShiri Azenkot",
        "links": "http://dx.doi.org/10.1145/3663548.3675620",
        "entry_id": "http://arxiv.org/abs/2408.08193v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08193v1",
        "summary": "With the increasing adoption of social virtual reality (VR), it is critical\nto design inclusive avatars. While researchers have investigated how and why\nblind and d/Deaf people wish to disclose their disabilities in VR, little is\nknown about the preferences of many others with invisible disabilities (e.g.,\nADHD, dyslexia, chronic conditions). We filled this gap by interviewing 15\nparticipants, each with one to three invisible disabilities, who represented 22\ndifferent invisible disabilities in total. We found that invisibly disabled\npeople approached avatar-based disclosure through contextualized considerations\ninformed by their prior experiences. For example, some wished to use VR's\nembodied affordances, such as facial expressions and body language, to\ndynamically represent their energy level or willingness to engage with others,\nwhile others preferred not to disclose their disability identity in any\ncontext. We define a binary framework for embodied invisible disability\nexpression (public and private) and discuss three disclosure patterns\n(Activists, Non-Disclosers, and Situational Disclosers) to inform the design of\nfuture inclusive VR experiences.",
        "updated": "2024-08-15 14:53:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08193v1"
    },
    {
        "title": "EmBARDiment: an Embodied AI Agent for Productivity in XR",
        "authors": "Riccardo BovoSteven AbreuKaran AhujaEric J GonzalezLi-Te ChengMar Gonzalez-Franco",
        "links": "http://arxiv.org/abs/2408.08158v1",
        "entry_id": "http://arxiv.org/abs/2408.08158v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08158v1",
        "summary": "XR devices running chat-bots powered by Large Language Models (LLMs) have\ntremendous potential as always-on agents that can enable much better\nproductivity scenarios. However, screen based chat-bots do not take advantage\nof the the full-suite of natural inputs available in XR, including inward\nfacing sensor data, instead they over-rely on explicit voice or text prompts,\nsometimes paired with multi-modal data dropped as part of the query. We propose\na solution that leverages an attention framework that derives context\nimplicitly from user actions, eye-gaze, and contextual memory within the XR\nenvironment. This minimizes the need for engineered explicit prompts, fostering\ngrounded and intuitive interactions that glean user insights for the chat-bot.\nOur user studies demonstrate the imminent feasibility and transformative\npotential of our approach to streamline user interaction in XR with chat-bots,\nwhile offering insights for the design of future XR-embodied LLM agents.",
        "updated": "2024-08-15 13:48:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08158v1"
    },
    {
        "title": "Confidence-weighted integration of human and machine judgments for superior decision-making",
        "authors": "Felipe YáñezXiaoliang LuoOmar Valerio MineroBradley C. Love",
        "links": "http://arxiv.org/abs/2408.08083v1",
        "entry_id": "http://arxiv.org/abs/2408.08083v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08083v1",
        "summary": "Large language models (LLMs) have emerged as powerful tools in various\ndomains. Recent studies have shown that LLMs can surpass humans in certain\ntasks, such as predicting the outcomes of neuroscience studies. What role does\nthis leave for humans in the overall decision process? One possibility is that\nhumans, despite performing worse than LLMs, can still add value when teamed\nwith them. A human and machine team can surpass each individual teammate when\nteam members' confidence is well-calibrated and team members diverge in which\ntasks they find difficult (i.e., calibration and diversity are needed). We\nsimplified and extended a Bayesian approach to combining judgments using a\nlogistic regression framework that integrates confidence-weighted judgments for\nany number of team members. Using this straightforward method, we demonstrated\nin a neuroscience forecasting task that, even when humans were inferior to\nLLMs, their combination with one or more LLMs consistently improved team\nperformance. Our hope is that this simple and effective strategy for\nintegrating the judgments of humans and machines will lead to productive\ncollaborations.",
        "updated": "2024-08-15 11:16:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08083v1"
    }
]