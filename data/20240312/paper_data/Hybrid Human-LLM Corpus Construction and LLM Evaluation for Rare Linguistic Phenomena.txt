Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare
Linguistic Phenomena
LeonieWeissweiler,AbdullatifKöksal,HinrichSchütze
LMUMunich&MunichCenterforMachineLearning
weissweiler@cislmu.de
Abstract aswecanimagineascenarioinwhichtheactionit
describescausesmotion. Thefactthathumanseas-
ArgumentStructureConstructions(ASCs)are
ilyunderstandtheCMCshowcasesamainfeature
one of the most well-studied construction
ofConstructionGrammar(Croft,2001;Goldberg,
groups, providing a unique opportunity to
demonstrate the usefulness of Construction 1995): themeaningisattachedtotheconstruction
Grammar (CxG). For example, the caused- itself, and not the verb. Putting the verb into this
motionconstruction(CMC,“Shesneezedthe construction adds the new meaning and valency.
foam off her cappuccino”) demonstrates that This is one reason that constructions pose a chal-
constructions must carry meaning, otherwise
lengetoLargeLanguageModels(LLMs),asthey
the fact that “sneeze” in this context causes
would have to learn to attach the meaning to this
movementcannotbeexplained. Weformthe
construction and retrieve it when necessary. Its
hypothesisthatthisremainschallengingeven
extremerarenessandproductivitymakesitimpos-
for state-of-the-art Large Language Models
(LLMs), forwhichwedeviseatestbasedon sibletomemoriseallinstancesandmemorization
substituting the verb with a prototypical mo- wouldnotbesufficientbecausethemeaningshiftto
tion verb. To be able to perform this test at theverbiscreativeandisinfluencedbythespecific
statistically significant scale, in the absence
context.
ofadequateCxGcorpora,wedevelopanovel
The research questions of this paper therefore
pipelineofNLP-assistedcollectionoflinguis-
are: HaveLLMslearnedthemeaningoftheCMC
tically annotated text. We show how depen-
dencyparsingandGPT-3.5canbeusedtosig- andhowcanweconstructtheresourcesneededto
nificantlyreduceannotationcostandthusen- determinethestatusofCMCinLLMs?
abletheannotationofrarephenomenaatscale. Wefirstaddressthesecondquestion,ofcollect-
We then evaluate GPT, Gemini, Llama2 and
ing data for this at scale. This is challenging for
Mistralmodelsfortheirunderstandingofthe
severalreasons. First,theCMCisaveryrarephe-
CMC using the newly collected corpus. We
nomenon. Second, we are mostly interested in
findthatallmodelsstrugglewithunderstand-
instancesthatarenon-prototypical,i.e.,wherethe
ingthemotioncomponentthattheCMCadds
toasentence. verbdoesnottypicallyencodemotion,unlikee.g.
‘kick’ or ‘throw’. Third, this construction cannot
1 Introduction
beautomaticallyidentifiedusingonlysyntacticcri-
teria: wordsmightbeinthecorrectsyntacticslots
(1) Shesneezedthefoamoffhercappuccino.
requiredbytheCMC,butnotcreateaCMCread-
(2) Theylaughedhimoffthestage.
ingifthesemanticsofthesentencedonotfit. For
Thesearetwoexamplesofthecaused-motioncon- example,“Iwouldtakethatintoaccount”isstruc-
struction (CMC) in which the verb behaves un- turallyidenticaltotheexamplesabove,butnothing
usually: sneeze and laugh typically do not take ismoving.
multiplearguments,nordotheytypicallyconvey Thisshowsthatthereisacrucialsemanticcom-
that something was moved by sneezing/laughing. ponent. Theraritymakesitinfeasibletomanually
Thisposesachallengetoanynaiveformoflexical sift through a corpus to collect a dataset of the
semantics: it would not make sense for someone CMC,whilethesemanticcomplexitymakesitin-
writing a dictionary to include, for each intransi- feasibletodosofullyautomatically.
tive verb, the meaning and valency of the CMC. Inthisway,weconsidertheCMCexemplaryof
Almost any verb can appear in the CMC as long rarephenomenaoflanguagethathavebeenlargely
4202
raM
11
]LC.sc[
1v56960.3042:viXraset aside in Computational Linguistics and in re- stances of the CMC of 765 sentences and a
cent evaluation of LLMs in particular. This may second,semi-automaticallyannotatedcorpus
beduetothembeingconsideredtheperipheryof of127,955sentences.
language, rather than the core (Chomsky, 1993),
• We evaluate different sizes of GPT, Llama2,
orsimplyduetothedescribeddifficultyinfinding
Mistral, and Gemini models on their under-
appropriate data to investigate both the phenom-
standing of the CMC and find that all mod-
ena and their representation in LLMs. However,
elsperformpoorly,withthebestmodelonly
it is our point of view that as the performance of
achieving69.75%.
suchmodelsincreasesacrosstheboard,itisvital
toturnto“edgecases”toaccuratelyidentifyper-
2 RelatedWork
formance gaps. This is particularly important as
rare phenomena may be indicators of systematic Evaluation of LLMs’ Understanding of Con-
underlyingproblemsofanNLPparadigm. structions. Tayyar Madabushi et al. (2020) con-
Tostudyrarephenomena,weneeddata. Tothis clude that BERT (Devlin et al., 2019) can clas-
end, in section 3 we propose a novel annotation sify whether two sentences contain instances of
pipeline that combines dependency parsing with the same construction. Tseng et al. (2022) show
anadvancedLLM,GPT-3.5(OpenAI,2022). The thatLMshavehigherpredictionaccuracyonfixed
aimofourpipelineistominimisethecostofrun- thanonvariablesyntacticslotsandinferthatLMs
ningGPT-3.5andcompensatinghumanannotators, acquireconstructionalknowledge(i.e.,theyunder-
while maximising the number of positive, manu- stand the “syntactic context” needed to identify
allyverified,linguisticallydiverseinstancesinthe a fixed slot). Weissweiler et al. (2022) find that
dataset. LLMs reliably discriminate instances of the En-
Wefurtheruseinsightsaboutthesemanticregu- glishComparativeCorrelative(CC)fromsuperfi-
laritiesintheCMCtoautomaticallyexpandthisto cially similar contexts. However, LLMs do not
asecondcorpusof127,955sentences,whichhave producecorrectinferencesfromthem,i.e.,theydo
notbeenmanuallyverified,butwithhighlikelihood notunderstanditsmeaning.
arealsoCMCinstances. Aftercreatingourcorpus, Mostrelatedtothiswork,Lietal.(2022)probe
wenowreturntoouraimofevaluatingstate-of-the- forLMs’handlingoffourASCs: ditransitive, re-
artLLMsfortheirunderstandingoftheCMC,as sultative,caused-motion,andremoval. Theyadapt
an example of a semantically challenging “edge thefindingsofBenciniandGoldberg(2000),who
case”. usedasentencesortingtasktodeterminewhether
In Section 4, we therefore develop a test for human participants perceive the argument struc-
different LLMs’ understanding of the CMC, by tureortheverbasthemainfactorinthesentence
giving an instance and asking if the direct object meaning. Theyfindthat,whilehumanparticipants
is physically moving. We then replace the non- prefersortingbytheconstructionmoreiftheyare
prototypicalverb(e.g.,“sneeze”)byaprototypical moreproficientEnglishspeakers,PLMsshowthe
onethatalwaysencodesmotion(e.g.,“throw”)and same effect in relation to training data size. In a
askthemodelagainifthedirectobjectismoving. secondexperiment,theytheninsertrandomverbs
Weexpectmodelsthatdonotfullyunderstandthe thatareincompatiblewithoneoftheconstructions,
CMCtofailtoconsistentlyanswerbothquestions and measure the Euclidean distance between the
with “yes”. We observe that all models struggle verbs’contextualembeddingandthatofaverbthat
withthistask. Mixtral8x7bperformsbest,butstill isprototypicalfortheconstruction. Theydemon-
hasanerrorrateofover30%. stratethatconstructioninformationispickedupby
Wemakethreemaincontributions: themodel,asthecontextualembeddingoftheverb
isbroughtclosertothecorrespondingprototypical
• Weproposeahybridhuman-LLMcorpuscon-
verbembedding.
struction method and show its effectiveness
Mahowald(2023)investigatesGPT-3’s(Brown
fornon-prototypicalCMC,anextremelyrare
etal.,2020)understandingoftheEnglishAANN
phenomenon. Wediscusshowourdesignand
construction, assessing its grasp of the construc-
our guidelines can be applied to data collec-
tion’ssemanticandsyntacticconstraints. Utilising
tionneedsforotherlinguisticphenomena.
a few-shot prompt based on the CoLA corpus of
• We release a corpus of manually verified in- linguisticacceptability(Warstadtetal.,2019),hecreates artificial AANN variants as probing data. analysisofconstructionaldiversitytopredictESL
GPT-3’sperformanceonthelinguisticacceptability speakers’languageproficiency. Similartoourfirst
taskisfoundtoalignwithhumanjudgmentsacross filtering step, they perform an automatic depen-
mostconditions. dencyparseandthenidentifyarangeofconstruc-
Linguistic Annotation with GPT. Since the tions, including the CMC, using a decision tree
release of ChatGPT, numerous papers have pro- builtontheparse. Theydonotemployanyfurther
posedtouseitasanannotator. Gilardietal.(2023) filtering.
findthatChatGPToutperformscrowd-workerson
3 DataCollection
tasks such as topic detection. Yu et al. (2023)
and Savelka and Ashley (2023) evaluate the ac-
Weaddressthefollowingproblemsetting.
curacy of GPT-3.5 and GPT-4 against human an-
A linguistic researcher (a computational or
notators, while Koptyra et al. (2023) annotate a
corpus linguist) wants to investigate a rare phe-
corpus of data labelled for emotion by ChatGPT,
nomenon. Theywanttofindasmanyexamplesof
butacknowledgeitsloweraccuracycomparedtoa
thephenomenonaspossible,whichmustbeveri-
human-annotatedversion. IntheareaofConstruc-
fiedbyhumanannotators. Therearethreetoolsat
tionGrammar,Torrentetal.(2023)useChatGPT
theirdisposal: linguisticresources,GPT-3.5,and
togeneratenovelinstancesofconstructions.
humanannotators. Thehumanannotatorswillgen-
Mostrelatedtoourworkarepapersthatpropose
erallybeexpertsinlinguisticannotation.
acooperationbetweentheLLMandthehumanan-
Ourkeyideaisthatdatacollectionwillproceed
notator. HolterandEll(2023)createasmallgold
inapipeline,whereacorpusisfirstfilteredusing
standardforindustryrequirementsbygenerating
dependencyparsingandthesyntacticconstraintsof
aninitialparsetreewithGPT-3andthencorrecting
thephenomenon,theoutputisfurtherfilteredwith
itwithahumanannotator. Pangakisetal.(2023)
prompt-basedclassificationusingGPT,andthesen-
investigateLLMannotationperformanceon27dif-
tenceswhichitlabelsaspositivearethenmanually
ferent tasks in two steps. First, annotators com-
annotatedbyahuman. Eachstepinthepipelineis
pileacodebookofannotationguidelines,whichis
meanttofurtherconcentratetherateofinstances
thengiventotheLLMashelpforannotation,and
inthecorpusthatwillthenbemanuallyannotated,
then the codebook is refined by the annotators in
thereforereducingtotalannotationeffort.
a second step. However, they find little to no im-
Themaincostofdatacollectionisthecostfor
provementfromthesecondstep. Grayetal.(2023)
theGPT-3.5APIandforhumanannotators. Weas-
makeanLLMpre-generatelabelsforlegaltextan-
sumethatanyexpensesforlinguisticresourcesand
alytics tasks which are then corrected by human
thecomputationalinfrastructureatourdisposalare
annotators,butfindthatthisdoesnotspeedupthe
negligibleincomparison. Ouraimistominimise
annotationprocess.
the cost for GPT-3.5 and annotators while max-
Incontrast,ourworkproposesahybridhuman-
imisingthenumberofpositive,manuallyverified,
LLM pipeline that minimizes the cost of dataset
diverseinstances. Weassumethattheusecaseof
creation. We emphasise prompt design and engi-
the linguistic resource is such that it needs to be
neering,acriticalfactorineffectiveuseofLLMs.
manuallyannotated,sothatwecannotsimplyfully
Computational Approaches to Argument relyonanLLM.
StructureConstructions(ASCs). Inadditionto
We propose a way of computing the cost for
probing work discussed above, ASCs have also
thisproblemsettingandapipelineforproducinga
been studied from a computational perspective.
novellinguisticresourcewhileminimisingcost.
Kyle and Sung (2023) leverage a UD-parsed cor-
Our main goal is minimising the cost per con-
pusaswellasFrameNet(Ruppenhoferetal.,2016)
firmedinstance;however,wealsohaveasecondary
semanticlabellingtoannotatearangeofASCs.
goal: the final set of instances should be diverse.
Hwang and Palmer (2015) identify CMCs and Regardless of the specific goals of the linguistic
fourdifferentsubtypesbasedonlinguisticfeatures. researcher,itisunlikelythattheywouldbeserved
Someoftheseareautomaticallygenerated,butoth- by a set of sentences that do not represent the
ersaregoldannotations. Thislimitstheapplicabil- true diversity of the phenomenon. Extreme cost-
itytolarge,unannotatedcorpora. minimisingmeasures–suchasmakingthedepen-
Hwang and Kim (2023) conduct an automatic dencyfilteringrulesdescribedin§3.1toostrictorFinal
FP
dataset
TP FP GPT Filtering TP FP Human (TP)
POS NEG Annotation
T Dependency
-based
F Filtering Semi- Automatic
FN TN FN TN automatic Expansion
dataset
Figure1: Flowchartofourannotationpipeline. Fordetailsofeachstepreferto§3.
asking GPT-3.5 to provide examples of the phe- dobj pobj
nomenon–wouldthereforebecounterproductive. Shesneezedthefoamoff thecappuccino.
The baseline here is to take an annotator, give VERB NOUN ADP NOUN
them a corpus, set them on the task of reading
Figure2: OurdependencysubtreefortheCausedMo-
through it and marking all sentences that contain
tionConstruction(highlightedinred),shownwithan
instancesofthephenomenon. LetC bethefixed
HR example. Thestraightarrow(NOUN→ADP)indicates
cost of the human review of one sentence and N thatthewordsmustbeadjacent.
thesizeofthecorpus. Thenthetotalcostofcreat-
ingtheresourceisJ = C N astheannotator
naive HR
betolerantof,andtherarerthephenomenathatwe
hastosiftthroughallsentences. Ourproblemset-
aretargeting,themorelikelytheyaretobeparsed
tingisthatthephenomenonisrare,i.e.,thecorpus
incorrectly, as even human annotation guidelines
contains very few true positives. Thus, TP ≪ N
mightbeinconsistentforthem.
andthenumberofpositiveinstancesTPfoundafter
To write the patterns, we recommend to start
annotating the corpus will be small. This makes
withalistofgoldinstances,dependencyparsethem
this method cost-prohibitive. We therefore turn
andmanuallylookatthesimilarities. Ifonewants
todependencyparsingwithspacy(Honnibaland
toworkonlywithatreebank,itmightbeadvisable
Montani,2017)forprefiltering.
tostartatfirstwithahighlylexicalisedversionof
3.1 Step1: DependencyParsing the phenomenon to find a first example sentence
andgoonfromthere. Intermsoftools,ifonedoes
3.1.1 GeneralConsiderations
notwanttowritethecodeforfindingtheinstances
Figure 1 shows our pipeline. In the first step, we
themselves, available tools include GrewMatch,1
dependency-parse the corpus and apply a pattern
DepEdit (Peng and Zeldes, 2018), Corpus Work-
tofilteroutallsentencesthat,withhighlikelihood,
bench2 andSPIKE(Shlainetal.,2020).
arenotinstancesofthephenomenon.
For this dependency annotation, we could rely 3.1.2 DesigningaDependencySubtreeforthe
onannotatedtreebankssuchasUniversalDepen- CMC
dencies (de Marneffe et al., 2021). But to find a Figure2showsthedependencysubtreeweuseas
diverseandsufficientlylargesetofinstances,par- afilterforCMC.Itwasdesignedbyautomatically
ticularlyinlanguagesotherthanEnglish,available parsingexamplesofpositiveandnegativeinstances
treebanks may not be large enough for the rare from Goldberg (1992), and manually designing
phenomenonthatwearetargeting. subtreesthatcoverthepositiveinstancesexceptfor
Wethereforeturntoautomateddependencypars- clear parsing errors, while excluding as many of
ingtoannotatelargeamountsofdata. Weassume thenegativeexamplesaspossible. Werunanau-
thatthispartofthepipelinehasnegligiblecost,as tomaticdependencyparserfromthespacytoolkit3
itiscarriedouteitherthroughawebserviceoron ontheredditcorpus(Baumgartneretal.,2020)and
existinginfrastructure. extract all matching sentences. This also allows
After dependency parsing, we want filters that ustoextractthelocationofthepotentialCMCin-
preservethediversityofthefoundsentences. We stanceanditspartsasasideproductofthefiltering
thereforedesignsubtreefiltersthatpreserverecall
1https://match.grew.fr/
aboveallelse. Thisisespeciallyadvisableaspars- 2https://cwb.sourceforge.io/index.php
ingwillleadtosomeparsingerrorsthatwewantto 3version3.2.0step: Weextractthesentence,thelemmatisedverb, positives, and t(V,i), the sum of the number of
directobject,preposition,andprepositionalobject, tokensinputtotheAPIandthenumberoftokens
aswellastheirpositionsinthesentence. returnedbytheAPI.
We create a variety of different prompts i and
3.2 Step2: Prompt-basedFew-shot
thenselectourfinalprompti′ astheonewiththe
ClassificationwithGPT
lowestper-TPcost:
3.2.1 GeneralConsiderations
Evenafterdependency-basedfiltering,thepositive i′ = argmin iJ(C HR,C API,i)
instanceswouldstillbeveryrareintheoutputand
Determiningthesizeoftheinputcorpus When
itisthereforenotfeasiblethattheoutputisdirectly
applying our pipeline, we often will have a re-
annotated by a human. We therefore introduce
quirement for a specific number TP of the phe-
a further filtering step with GPT to “concentrate” req
nomenon,e.g.,TP = 1000instancesoftheCMC.
the positive instances even more (Step 2 of the req
AfterselectingapromptianddeterminingTP(V,i)
pipeline, Figure 1), i.e. we want GPT to remove
onthedevelopmentset,wecanestimatethesizeN
most negative instances while keeping as many
oftheinputcorpusthatwillresultinasetofTP
positiveinstancesaspossible. Theremainingdata req
instancestobeoutputbythepipelineas:
canthenbecost-effectivelyannotatedbythehuman
annotator. Theaimistoreducethecostperinstance
TP
(i.e.,costpertruepositive,TP)asmuchaspossible. N := |V| req
TP(V,i)
Therearetwocomponentsofthecost: thecost
ofqueryingGPTandthecostofhumanannotation.
3.2.2 PromptEngineeringandFew-shot
Ourtwokeyideasare:
Selection
• Weconsiderthetwocostsjointlyandoptimize We suggest to first manually annotate a develop-
thepipelineforoveralllowestcostperTP. ment set of positive and negative instances. The
developmentsetshouldbeasrepresentativeofthe
• Designandselectionofthepromptusedwith corpus to be annotated as possible, including the
the API is a major determinant for the cost rateofTPs. Thisdevelopmentsetcanthenbeused,
of the pipeline. We propose a workflow for alongwithasmallfixedbudget,todevelopprompts
creatingeffectiveprompts. andfinallyselectthebest-performingprompt. We
presentpossiblestrategiesforpromptdevelopment,
A particular prompt may require many tokens
usingCMCastherunningexample.
intotal, therebyincurringahigherAPIcost. But
itmayalsohavehighaccuracy, therebyreducing DevelopmentSet Forcreatingthedevelopment
thecostofhumanannotation. Wejointlyconsider setV,wemanuallyannotate504(133P,371N)
bothcostcomponentswhendesigningandselect- sentences from the output of the dependency fil-
ingprompts. tering step. To ensure that V is both diverse and
relevant,wegrouptheprefiltereddatasetbyverb,
Minimizingthecostpertruepositive Givenan
andstartingwiththehighest-frequencyverbs,take
annotateddevelopmentsetV thatisrepresentative
at most 5 positive and 5 negative sentences from
of the corpus that we want to annotate with our
every verb, where no preposition appears twice
pipeline, let J(C ,C ,i) be the cost per true
HR API
ineitherthepositiveorthenegativesentencesse-
positive where C is the human annotation cost
HR
lected. We choose 5 shots from each class to be
persentence,C isthecostofprocessinganin-
API
included as examples in the prompt, which are a
put/outputtokenwiththeAPIandi(forinstruction)
mixtureofsentencestakenfromGoldberg(1992)
isaprompt. WecanthenestimateJ(C ,C ,i),
HR API
andadditionaldatathatwasnotusedforV. Using
thecostpertruepositive,asfollows:
thisdevelopmentsetV,wecandetermineTP(V,i),
C t(V,i)+C (TP(V,i)+FP(V,i)) FP(V,i) and t(V,i) of each prompt and compute
API HR (1)
TP(V,i) itsper-TPcostusingEq.1.
where we process the development set using the Iterative Prompt Development We start with
APIandpromptiandrecord: TP(V,i),thenumber a simple base prompt and iteratively attempt im-
of true positives, FP(V,i)), the number of false provementstoit. Thetotalcostofthisexperimenta-tionwasabout$10. Thefulldetailsofallattempted 3.3 FinalDatasetCollection
promptsaregivenintheappendixinSectionC.4
Usingprompt12,wecollectafinaldatasetwitha
Mostimprovementsonthebasepromptareun- $6budgetfortheGPT-3.5-APIandmanuallyanno-
controversial,astheydecreasethetotalcostunder tatetheoutput. Outof20,408sentencesclassified,
the assumption that C HR is not less than $0.0001 1303arejudgedbythemodelaspositive. Weanno-
(and the cost per API token is dominated by the tatethesebyhandandfind632positiveinstances.
costofhumanannotation). Theseinclude: Wecombinethesewiththe133positiveinstances
fromourdevelopmentsetforafinalnumberof765
• longtaskdescription hand-annotatedCMCinstances.5
Weobservethatthe4-tupleof<verb,directob-
• longsystemprompt ject,preposition,prepositionalobject>almostal-
ways perfectly determines the class. We use this
• JSONformatforinputandoutput
observationtoextrapolatefromourmanuallyanno-
tatedsentencestoallothersentenceswiththesame
• few-shotsalternatebyclass
4-tuples and release a second dataset of 127,955
• explanationsforthelabelsaddedtothefew- high-likelihoodCMCinstances.
shotsanddemandedfromthemodel
4 EvaluationofLLMs’Understandingof
• only10sentencesclassifiedatonetime theCMC
4.1 Methods
Table1illustratesthepromptdevelopmentpro-
The goal of our evaluation is to assess different
cess and how the choice of most cost-effective
LLMsfortheirunderstandingoftheCMC.Thefact
promptdependsonhumancostC andAPIcost
HR
that the prompt engineering above, even with 10
C . Prompts are numbered (reflecting the se-
API
few-shotexamplesandextensivetaskdescriptions
quenceinwhichwecreatedthem). Prompt5isa
andexplanations,isstillfarfromperfect,suggests
simpleshortprompt(consequently: lowAPIcost).
that this is a challenging task even for advanced
Its precision is low and so the human has to an-
modelslikeGPT-3.5,whichleadsustoquestionif
notatemoresentencestoproducethefinaldataset
thesameholdsforotherLLMs.
of N = 1000 TPs of CMC. Prompts 12, 17, 18
OurLLMevaluationsetupinthissectiondiffers
are consecutive refinements of 5. They result in
frompromptevaluationaswedonotexplicitlyre-
moreinputtoandoutputfromtheAPIbecauseex-
fertothe“caused-motionconstruction”,butrather
planationsaregivenwiththefew-shots,themodel
prompt implicitly for the model’s understanding
is asked to provide explanations and (for 18) we
ofthesituationdescribed. Thekeyideaisthatin
classifyeachsentencethreetimes(triplingtheAPI
aCMCsentence, somethingisalwaysphysically
expense). For17and18,weuseGPT-4,whichis
moving,eveniftheverb(e.g.,“sneeze”)doesnot
morecapable,butalsomoreexpensivethanGPT-
indicatethis. Thedistinctionbetweenprototypical
3.5. Each refinement improves precision, which
vs.non-prototypicalinstancesiscrucialhere: for
thendecreaseshumancost.
prototypicalCMCinstances(“throw”,“kick”),the
The right four columns of the table illustrate
verb already conveys the meaning component of
howpromptselectiondependsonhumancost. For
motionwhilefornon-prototypicalCMCinstances
(unrealistic) very low human cost (C =$.001),
HR (“sneeze”,“laugh”)itdoesnotandtheLLMhasto
precisionofthepromptdoesnotmattermuchand
infertheadditionalmeaningcomponentofmotion
the simplest prompt (which has low API cost) is
fromtheconstruction.
mostcost-effective. Ashumancostrises,themore
Our setup is to ask “In the sentence sentence,
expensivepromptsbecomecompetitivesincenow
is direct_object moving, yes or no?”. We then
human cost is the main factor and the API cost
replace the verb of the CMC with the appropri-
component becomes small in relative terms. For
C HR=$.2,prompt12isbest; forC HR=$1,prompt 4Thebasemodelusedwasgpt-3.5-turbo-1106,theGPT-4
17;andforC =$2,prompt18. modelwasgpt-4-0125-preview.
HR
5Wehopetobeabletoreleaselargerdatasetsusingthis
Asourfinalprompt,weselectprompt12asitis
methodinthefuture,givenalargercomputationalandannota-
agoodtradeoffbetweenAPIcostandhumancost. tionbudget.Sent’stoAnnotate TotalCost
P Details Prec. Rec. LLM Human API C HR=$.001 C HR=$0.2 C HR=$1 C HR=$2
5 Simple 56.54 71.42 5,304 1,768 $0.4 $2.2 $177 $1,768 $3,537
12 5+expl. 83.75 50.37 7,522 1,194 $1.4 $2.6 $120 $1,195 $2,389
17 12+GPT-4 90.09 81.96 5,040 1,110 $16.2 $17.3 $127 $1,126 $2,236
18 17+best-of-3 91.81 75.93 4,989 1,089 $48.3 $49.4 $157 $1,137 $2,226
- Humanonly - - - 3,789 $0.0 $3.8 $575 $3,789 $7,578
Table1: Acomparisonofthefourbest-performingprompts(5,12,17,18)fordifferentvaluesofC . P=Prompt.
HR
Wegivenumbers(sentencesthatneedtobeannotatedbyLLM/human)forascenarioinwhichthedesiredsizeofthe
finalresource(outputofpipelinewhenappliedtotherawcorpus)isN =1000. Thehumanbaselinedependssolely
ontherateofTPs(whichishigherherethanfortherawcorpustobeprocessedbythepipelineasthedevelopment
setcontainsmorepositiveinstances).
Ilaughedmyselfontothefloor.
Shehadabitofsauceonherlipsohekisseditoffofher.
Shewasinprettybadshapefromdrinking,soIhelpedherintobedandstartedthewater.
Herushedmeacrossthestreet,anddownthesidewalkwesttowardsBedlamMentalHospital.
Icanpopmyshoulderoutofmysocket.
Table2: Examplesfromthefinaldataset. Verbsarehighlightedingreen,directobjectsinpurple,prepositionsin
blue,andprepositionalobjectsinred.
atelyinflectedformof“throw”,andaskthesame thereforedemonstratesthatitunderstandstheCMC.
question again, using the structural information (ii) N→Y. The model answers with “no” for the
extracted by the dependency filtering step. We original sentence but changes its answer to “yes”
expect that models with no understanding of the whentheverbischangedto“throw”,meaningthat
CMCwouldanswer“yes”bothtimesonlyforpro- itdoesnotunderstandtheCMC.(iii)X→N.Even
totypicalinstances,andswitchfrom“no”to“yes” with“throw”,themodeldoesnotanswercorrectly
fornon-prototypicalones. Modelswithaperfect thatthedirectobjectismoving. Weconsiderthese
understandingoftheCMCwouldalwaysanswer to be general failures of the model to understand
“yes”. However,wedonotgroupverbsintoproto- theinstruction,ratherthantheCMCspecifically.
typical and non-prototypical ones a priori, as we
Themainresultofthisevaluationisthatallmod-
supporttheviewthatthisisonagradient,andthere-
elsperformpoorly,withMixtral8x7binstruction-
foretheclassesarenotclear-cut: verbslike“brush”
tuned (IT) performing best with 69.75% and an
or“apply”maynotalwaysconveymotionoutside
overall error rate of 30.25% (12.13+18.12). Mis-
oftheCMC,butcertainlymoreoftenthan“laugh”.
tral 7b IT is a close second. Surprisingly, GPT-4
Weconductthisexperimentonourcorpusof765
follows at a distance of more than 10 points at
hand-annotated sentences. As API-based LLMs,
57.07%. All other models perform worse. This
we investigate GPT-3.5, GPT-4 (OpenAI, 2022),
demonstratesthat,despitetheimpressiveprogress
andGeminiPro(Teametal.,2023). Fromthefam-
LLMshaverecentlymade, theyarestillfarfrom
ilyofopenLLMs,wefurtherchooseLlama2(Tou-
perfect natural language understanding. Presum-
vronetal.,2023)withsizes7b,13b,andthequan-
ably,onereasonisthatnon-prototypicalCMCin-
tised version of 70b, and Mistral 7b (Jiang et al.,
stances(e.g.,“sneeze”)donotoccurfrequentlyin
2023)andMixtral8x7b(Jiangetal.,2024),aswell
thetrainingdata. Inaddition,thistypeofconstruc-
astheirrespectiveinstruction-tunedversions. Mod-
tioniscreativeandcomplexuseoflanguagethatis
elsgenerateasentenceinresponse,whichwethen
hardertogeneralizetothanthelinguisticbehaviors
parseforversionsof“yes”and“no”.
thataretestedinstandardNLPbenchmarks. Our
findingmaybeusefulasguidanceforthefurther
4.2 QuantitativeResults
developmentoflargelanguagemodels. Inaddition
Table 3 presents the results in three groups. (i) tosemantic/pragmaticshortcomingsofLLMs(e.g.,
Y→Y. The model answers “yes” both times and onreasoning)and“badbehavior”(outputthatisun-truthful,offensiveetc.),itsuggeststhatsyntaxand Family Model IT Y→Y N→Y X→N
inparticularthesyntax-semanticsinterfaceisalso
3.5 + 43.20 10.70 46.10
anareainwhichmoreprogressisneededforLLMs GPT
4 + 57.07 11.23 31.70
tocomeclosertohumanlevelsofperformance.
Surprisingly, our expectation that instruction- Gemini Pro + 43.43 12.70 43.87
tunedmodelswillgenerallybebetteratunderstand-
– 9.54 1.09 89.37
ing the premise of the question (and therefore re- 7b
+ 21.93 1.77 76.29
duce X→N answers) only holds for Mistral, but
– 53.00 8.72 38.28
not for Llama2. Higher rates of X→N are gen- Llama2 13b
+ 5.59 1.23 93.19
erally associated with lower rates of N→Y, sug-
– 36.65 7.36 55.99
gesting that models that understand the question 70b Q + 37.87 5.59 56.54
betteralso understandtheCMC better. Mostsur-
– 34.20 4.50 61.31
prisingly, the worst-performing models were the
7b
+ 68.12 8.45 23.43
two GPT models, Gemini Pro, and Mixtral 8x7b
Mistral
– 35.29 9.95 54.77
withinstructiontuning,whilethebest-performing
8x7b
+ 69.75 12.13 18.12
modelsweresmallersizesofLlama2.
4.3 QualitativeResults Table3: LLMevaluationresults. IT=instruction-tuned.
Q=quantised.
We hypothesised that prototypical motion verbs
wouldleadtothemodelansweringyesbothtimes
(Y→Y),astheoriginalsentencealreadyobviously evaluatestate-of-the-artLLMsfortheirunderstand-
containsmotion. Itmightalsomakesenseforthese ing of the CMC, and found that they have high
verbs to be more frequent in the category of the errorrates(>30%)whenaskedtointerpretsitua-
modelanswering“no”bothtimes(X→N).While tionsdescribedwithanon-prototypicalCMC.
thiswouldnotbethecorrectanswer,amodelthatis Wehopethatourworkwillinspiremorecompu-
confusedandanswersincorrectlyaboutasentence tationalandcorpus-basedstudiesofrarelinguistic
with“kick”wouldlikelygivethesameanswerfor phenomena. Wenotethateventhoughpromptengi-
“throw”. Wealsoanticipatedthatthefrequencyof neeringiscomplex,largegainscanbeachievedby
amodelchangingitsanswerwouldbethehigher, usingintermediate-complexitypromptsandbasic
thelessprototypicaltheoriginalverbis(i.e.,a“no” knowledgeofLLMs. Weareconfidentthatfurther
for highly non-prototypical “sneeze” more often advancesininstruction-tunedLLMswillmakethe
thana“no”formoreprototypical“brush”). cost-benefit ratio of incorporating them into this
Investigating the distribution of verbs over the hybridannotationpipelineevenstronger.
outputclassesforGPT-4, weindeedfindthatthe Weseeseveralopportunitiesforinterestingfu-
mostfrequentfiveverbsoftheY→Yareallhighly tureworkinbothhalvesofthepaper. Forthedata
prototypicalmotionverbs: fling,chuck,pull,tele- collection part, it is a promising engineering di-
port,slam. Incontrast,themostfrequentfiveverbs rectiontodeveloptoolsthatautomatepartsofthis
forN→Yarenot: steal,eat,tie,smash,separate. process so that it becomes available to linguists
withouttheneedforcomplexpromptengineering.
5 Conclusion ContinuedprogressinLLMsislikelytomakethe
processevenmoreefficient.
Ourpaperhasmadeseveralcontributions. Wehave
ConcerningtheevaluationofLLMs’understand-
introducedanannotationpipelineaidedbydepen-
ing of constructions, a straightforward direction
dencyparsingandpromptingGPT-3.5,whichcan
for future work would be to expand to the other
bespecificallyusedforphenomenathataresorare
threeArgumentStructureConstructionsdescribed
thatlittletonocorporahavebeencreated,asthehu-
inGoldberg(1992).
manannotationeffortwouldbetoogreat. Wehave
demonstrated this pipeline on the example of the
Limitations
caused-motionconstruction,andcreatedcorporaof
765manuallyannotatedand127,955automatically Thedatacollectionsectionofourworkislimited
annotated (but high-confidence) CMC examples. because it draws on GPT-3.5, a commercial non-
We have used the manually annotated corpus to open model. While the general principles andguidelines for data collection hold for any LLM Morgan Gray, Jaromir Savelka, Wesley Oliver, and
that is used for data annotation, some of the spe- Kevin Ashley. 2023. Can gpt alleviate the burden
ofannotation? InLegalKnowledgeandInformation
cifictakeaways,especiallythecostformulation,are
Systems,pages157–166.IOSPress.
specific to GPT-3.5. Further, the final size of our
datasetislimitedbyourbudgetforpromptingGPT- Ole Magnus Holter and Basil Ell. 2023. Human-
machinecollaborativeannotation: Acasestudywith
3.5,aswellasthetimeavailableforannotation.
gpt-3. InProceedingsofthe4thConferenceonLan-
guage,DataandKnowledge,pages193–206.
References MatthewHonnibalandInesMontani.2017. spaCy2:
NaturallanguageunderstandingwithBloomembed-
JasonBaumgartner,SavvasZannettou,BrianKeegan, dings,convolutionalneuralnetworksandincremental
Megan Squire, and Jeremy Blackburn. 2020. The parsing. Toappear.
pushshiftredditdataset. CoRR,abs/2001.08435.
HaerimHwangandHyunwooKim.2023. Automatic
GiuliaMLBenciniandAdeleEGoldberg.2000. The analysisofconstructionaldiversityasapredictorof
contributionofargumentstructureconstructionsto eflstudents’writingproficiency. AppliedLinguistics,
sentence meaning. Journal of Memory and Lan- 44(1):127–147.
guage,43(4):640–651.
JenaD.HwangandMarthaPalmer.2015. Identification
TomB.Brown,BenjaminMann,NickRyder,Melanie of caused motion construction. In Proceedings of
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind theFourthJointConferenceonLexicalandCompu-
Neelakantan,PranavShyam,GirishSastry,Amanda tationalSemantics,pages51–60,Denver,Colorado.
Askell, Sandhini Agarwal, Ariel Herbert-Voss, AssociationforComputationalLinguistics.
Gretchen Krueger, Tom Henighan, Rewon Child,
AlbertQ.Jiang,AlexandreSablayrolles,ArthurMen-
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
sch,ChrisBamford,DevendraSinghChaplot,Diego
ClemensWinter,ChristopherHesse,MarkChen,Eric
delasCasas,FlorianBressand,GiannaLengyel,Guil-
Sigler,MateuszLitwin,ScottGray,BenjaminChess,
laumeLample,LucileSaulnier,LélioRenardLavaud,
Jack Clark, Christopher Berner, Sam McCandlish,
Marie-AnneLachaux,PierreStock,TevenLeScao,
Alec Radford, Ilya Sutskever, and Dario Amodei.
Thibaut Lavril, Thomas Wang, Timothée Lacroix,
2020. Languagemodelsarefew-shotlearners.
andWilliamElSayed.2023. Mistral7b.
Noam Chomsky. 1993. Lectures on government and
Albert Q. Jiang, Alexandre Sablayrolles, Antoine
binding: ThePisalectures. 9.WalterdeGruyter.
Roux, Arthur Mensch, Blanche Savary, Chris
Bamford, Devendra Singh Chaplot, Diego de las
WilliamCroft.2001. Radicalconstructiongrammar:
Casas, Emma Bou Hanna, Florian Bressand, Gi-
Syntactictheoryintypologicalperspective. Oxford
anna Lengyel, Guillaume Bour, Guillaume Lam-
UniversityPressonDemand.
ple, Lélio Renard Lavaud, Lucile Saulnier, Marie-
AnneLachaux,PierreStock,SandeepSubramanian,
Marie-Catherine de Marneffe, Christopher D. Man-
Sophia Yang, Szymon Antoniak, Teven Le Scao,
ning,JoakimNivre,andDanielZeman.2021. Uni-
Théophile Gervet, Thibaut Lavril, Thomas Wang,
versal Dependencies. Computational Linguistics,
TimothéeLacroix,andWilliamElSayed.2024. Mix-
47(2):255–308.
tralofexperts.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
BartłomiejKoptyra,AnhNgo,ŁukaszRadlin´ski,and
Kristina Toutanova. 2019. BERT: Pre-training of
Jan Kocon´. 2023. Clarin-emo: Training emotion
deepbidirectionaltransformersforlanguageunder-
recognitionmodelsusinghumanannotationandchat-
standing. InProceedingsofthe2019Conferenceof
gpt. InInternationalConferenceonComputational
theNorthAmericanChapteroftheAssociationfor
Science,pages365–379.Springer.
ComputationalLinguistics: HumanLanguageTech-
nologies,Volume1(LongandShortPapers),pages Kristopher Kyle and Hakyung Sung. 2023. An argu-
4171–4186,Minneapolis,Minnesota.Associationfor mentstructureconstructiontreebank. InProceedings
ComputationalLinguistics. oftheFirstInternationalWorkshoponConstruction
GrammarsandNLP(CxGs+NLP,GURT/SyntaxFest
Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023),pages51–62,Washington,D.C.Association
2023. Chatgptoutperformscrowd-workersfortext- forComputationalLinguistics.
annotationtasks. arXivpreprintarXiv:2303.15056.
BaiLi,ZiningZhu,GuillaumeThomas,FrankRudzicz,
AdeleE..Goldberg.1995. Constructions: Aconstruc- andYangXu.2022. Neuralrealityofargumentstruc-
tiongrammarapproachtoargumentstructure. Uni- tureconstructions. InProceedingsofthe60thAnnual
versityofChicagoPress. Meeting of the Association for Computational Lin-
guistics(Volume1: LongPapers),pages7410–7423,
Adele Eva Goldberg. 1992. Argument structure con- Dublin,Ireland.AssociationforComputationalLin-
structions. UniversityofCalifornia,Berkeley. guistics.Kyle Mahowald. 2023. A discerning several thou- XiChen,JamesLottes,NathanSchucher,Federico
sand judgments: Gpt-3 rates the article+ adjec- Lebron, AlbanRrustemi, NatalieClay, PhilCrone,
tive+ numeral+ noun construction. arXiv preprint TomasKocisky,JeffreyZhao,BartekPerz,DianYu,
arXiv:2301.12564. Heidi Howard, Adam Bloniarz, Jack W. Rae, Han
Lu,LaurentSifre,MarcelloMaggioni,FredAlcober,
OpenAI.2022. Chatgpt: Optimizinglanguagemodels DanGarrette,MeganBarnes,ShantanuThakoor,Ja-
fordialogue. cob Austin, Gabriel Barth-Maron, William Wong,
Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha,
NicholasPangakis,SamuelWolken,andNeilFasching.
ArunAhuja,RuiboLiu,YunxuanLi,SarahCogan,
2023. Automatedannotationwithgenerativeaire-
Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang,
quiresvalidation. arXivpreprintarXiv:2306.00176.
JordanGrimstad,AleJakseHartman,MartinChad-
wick, Gaurav Singh Tomar, Xavier Garcia, Evan
SiyaoPengandAmirZeldes.2018. Allroadsleadto
Senter, Emanuel Taropa, Thanumalayan Sankara-
UD:ConvertingStanfordandPennparsestoEnglish
narayanaPillai,JacobDevlin,MichaelLaskin,Diego
UniversalDependencieswithmultilayerannotations.
de Las Casas, Dasha Valter, Connie Tao, Lorenzo
InProceedingsoftheJointWorkshoponLinguistic
Blanco,AdriàPuigdomènechBadia,DavidReitter,
Annotation, Multiword Expressions and Construc-
MiannaChen,JennyBrennan,ClaraRivera,Sergey
tions(LAW-MWE-CxG-2018),pages167–177,Santa
Brin,ShariqIqbal,GabrielaSurita,JaneLabanowski,
Fe, New Mexico, USA. Association for Computa-
AbhiRao,StephanieWinkler,EmilioParisotto,Yim-
tionalLinguistics.
ing Gu, Kate Olszewska, Yujing Zhang, Ravi Ad-
danki, Antoine Miech, Annie Louis, Laurent El
Josef Ruppenhofer, Michael Ellsworth, Myriam
Shafey,DenisTeplyashin,GeoffBrown,ElliotCatt,
Schwarzer-Petruck,ChristopherRJohnson,andJan
Nithya Attaluri, Jan Balaguer, Jackie Xiang, Pi-
Scheffczyk.2016. Framenetii: Extendedtheoryand
dong Wang, Zoe Ashwood, Anton Briukhov, Al-
practice. Technicalreport, InternationalComputer
bert Webson, Sanjay Ganapathy, Smit Sanghavi,
ScienceInstitute.
Ajay Kannan, Ming-Wei Chang, Axel Stjerngren,
JosipDjolonga,YutingSun,AnkurBapna,Matthew
Jaromir Savelka and Kevin D Ashley. 2023. The un-
Aitchison, Pedram Pejman, Henryk Michalewski,
reasonableeffectivenessoflargelanguagemodelsin
TianheYu,CindyWang,JulietteLove,JunwhanAhn,
zero-shotsemanticannotationoflegaltexts. Fron-
Dawn Bloxwich, Kehang Han, Peter Humphreys,
tiersinArtificialIntelligence,6.
Thibault Sellam, James Bradbury, Varun Godbole,
Micah Shlain, Hillel Taub-Tabib, Shoval Sadde, and SinaSamangooei,BogdanDamoc,AlexKaskasoli,
YoavGoldberg.2020. Syntacticsearchbyexample. SébastienM.R.Arnold,VijayVasudevan,Shubham
In Proceedings of the 58th Annual Meeting of the Agrawal,JasonRiesa,DmitryLepikhin,RichardTan-
AssociationforComputationalLinguistics: System burn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah
Demonstrations,pages17–23,Online.Association Hodkinson, Pranav Shyam, Johan Ferret, Steven
forComputationalLinguistics. Hand, Ankush Garg, Tom Le Paine, Jian Li, Yu-
jiaLi,MinhGiang,AlexanderNeitz,ZaheerAbbas,
HarishTayyarMadabushi,LaurenceRomain,Dagmar SarahYork,MachelReid,ElizabethCole,Aakanksha
Divjak, and Petar Milin. 2020. CxGBERT: BERT Chowdhery, Dipanjan Das, Dominika Rogozin´ska,
meetsconstructiongrammar. InProceedingsofthe VitalyNikolaev,PabloSprechmann,ZacharyNado,
28thInternationalConferenceonComputationalLin- Lukas Zilka, Flavien Prost, Luheng He, Marianne
guistics, pages 4020–4032, Barcelona, Spain (On- Monteiro,GauravMishra,ChrisWelty,JoshNewlan,
line).InternationalCommitteeonComputationalLin- Dawei Jia, Miltiadis Allamanis, Clara Huiyi Hu,
guistics. RaouldeLiedekerke,JustinGilmer,CarlSaroufim,
Shruti Rijhwani, Shaobo Hou, Disha Shrivastava,
Gemini Team, Rohan Anil, Sebastian Borgeaud, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel,
YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu Albin Cassirer, Yunhan Xu, Daniel Sohn, Deven-
Soricut, Johan Schalkwyk, Andrew M. Dai, Anja dra Sachan, Reinald Kim Amplayo, Craig Swan-
Hauth, Katie Millican, David Silver, Slav Petrov, son,DessiePetrova,ShashiNarayan,ArthurGuez,
MelvinJohnson,IoannisAntonoglou,JulianSchrit- SiddharthaBrahma,JessicaLandon,MiteyanPatel,
twieser, Amelia Glaese, Jilin Chen, Emily Pitler, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao
Timothy Lillicrap, Angeliki Lazaridou, Orhan Fi- Jia, Matthew Rahtz, Mai Giménez, Legg Yeung,
rat, JamesMolloy, MichaelIsard, PaulR.Barham, Hanzhao Lin, James Keeling, Petko Georgiev, Di-
TomHennigan,BenjaminLee,FabioViola,Malcolm anaMincu,BoxiWu,SalemHaykal,RachelSapu-
Reynolds,YuanzhongXu,RyanDoherty,EliCollins, tro,KiranVodrahalli,JamesQin,ZeynepCankara,
Clemens Meyer, Eliza Rutherford, Erica Moreira, Abhanshu Sharma, Nick Fernando, Will Hawkins,
Kareem Ayoub, Megha Goel, George Tucker, En- Behnam Neyshabur, Solomon Kim, Adrian Hut-
rique Piqueras, Maxim Krikun, Iain Barr, Nikolay ter, Priyanka Agrawal, Alex Castro-Ros, George
Savinov,IvoDanihelka,BeccaRoelofs,AnaïsWhite, vandenDriessche,TaoWang,FanYang,Shuoyiin
AndersAndreassen,TamaravonGlehn,Lakshman Chang,PaulKomarek,RossMcIlroy,MarioLucˇic´,
Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Guodong Zhang, Wael Farhan, Michael Sharman,
Khalman, Jakub Sygnowski, Alexandre Frechette, Paul Natsev, Paul Michel, Yong Cheng, Yamini
CharlotteSmith,LauraCulp,LevProleev,YiLuan,Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Elsayed,EdChi,MahdisMahdieh,IanTenney,Nan
Christina Butterfield, Justin Chung, Paul Kishan Hua,IvanPetrychenko,PatrickKane,DylanScand-
Rubenstein,ShivaniAgrawal,ArthurMensch,Kedar inaro,RishubJain,JonathanUesato,RominaDatta,
Soparkar,KarelLenc,TimothyChung,AedanPope, Adam Sadovsky, Oskar Bunyan, Dominik Rabiej,
Loren Maggiore, Jackie Kay, Priya Jhakra, Shibo ShimuWu,JohnZhang,GautamVasudevan,Edouard
Wang,JoshuaMaynez,MaryPhuong,TaylorTobin, Leurent,MahmoudAlnahlawi,IonutGeorgescu,Nan
Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Wei, Ivy Zheng, Betty Chan, Pam G Rabinovitch,
Yash Katariya, Sebastian Riedel, Paige Bailey, Ke- Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit
fan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Naskar, MichaelAzzam, MatthewJohnson, Adam
Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias,
ElenaGribovskaya,JonasAdler,MateoWirth,Lisa Afroz Mohiuddin, Faizan Muhammad, Jin Miao,
Lee,MusicLi,ThaisKagohara,JayPavagadhi,So- Andrew Lee, Nino Vieillard, Sahitya Potluri, Jane
phie Bridgers, Anna Bortsova, Sanjay Ghemawat, Park,ElnazDavoodi,JiagengZhang,JeffStanway,
ZafaraliAhmed,TianqiLiu,RichardPowell,Vijay DrewGarmon,AbhijitKarmarkar,ZheDong,Jong
Bolina, Mariko Iinuma, Polina Zablotskaia, James Lee,AviralKumar,LuoweiZhou,JonathanEvens,
Besley,Da-WoonChung,TimothyDozat,Ramona William Isaac, Zhe Chen, Johnson Jia, Anselm
Comanescu,XianceSi,JeremyGreer,GuolongSu, Levskaya, Zhenkai Zhu, Chris Gorgolewski, Peter
Martin Polacek, Raphaël Lopez Kaufman, Simon Grabowski,YuMao,AlbertoMagni,KaishengYao,
Tokumine,HexiangHu,ElenaBuchatskaya,Yingjie Javier Snaider, Norman Casagrande, Paul Sugan-
Miao,MohamedElhawaty,AdityaSiddhant,Nenad than,EvanPalmer,GeoffreyIrving,EdwardLoper,
Tomasev,JinweiXing,ChristinaGreer,HelenMiller, ManaalFaruqui,IshaArkatkar,NanxinChen,Izhak
ShereenAshraf,AurkoRoy,ZizhaoZhang,AdaMa, Shafran,MichaelFink,AlfonsoCastaño,IreneGian-
AngelosFilos,MilosBesta,RoryBlevins,TedKli- noumis, WooyeolKim, MikołajRybin´ski, Ashwin
menko,Chih-KuanYeh,SoravitChangpinyo,Jiaqi Sreevatsa,JenniferPrendki,DavidSoergel,Adrian
Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Goedeckemeyer,WilliGierke,MohsenJafari,Meenu
VeredCohen,CharlineLeLan,KrishnaHaridasan, Gaba,JeremyWiesner,DianaGageWright,Yawen
AmitMarathe,StevenHansen,SholtoDouglas,Ra- Wei,HarshaVashisht,YanaKulizhskaya,JayHoover,
jkumar Samuel, Mingqiu Wang, Sophia Austin, Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu,
ChangLan,JiepuJiang,JustinChiu,JaimeAlonso Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian
Lorenzo, Lars Lowe Sjösund, Sébastien Cevey, LIN,MarinGeorgiev,MarcusWu,RicardoAguilar,
Zach Gleicher, Thi Avrahami, Anudhyan Boral, KeithPallo,AbhishekChakladar,AlenaRepina,Xi-
Hansa Srinivasan, Vittorio Selo, Rhys May, Kon- huiWu,TomvanderWeide,PriyaPonnapalli,Car-
stantinosAisopos,LéonardHussenot,LivioBaldini oline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier
Soares,KateBaumli,MichaelB.Chang,AdriàRe- Dousse, Fan Yang, Jeff Piper, Nathan Ie, Minnie
casens,BenCaine,AlexanderPritzel,FilipPavetic, Lui, Rama Pasumarthi, Nathan Lintz, Anitha Vi-
Fabio Pardo, Anita Gergely, Justin Frye, Vinay jayakumar,LamNguyenThiet,DanielAndor,Pedro
Ramasesh, Dan Horgan, Kartikeya Badola, Nora Valenzuela, CosminPaduraru, DaiyiPeng, Kather-
Kassner, Subhrajit Roy, Ethan Dyer, Víctor Cam- ineLee,ShuyuanZhang,SomerGreene,DucDung
pos,AlexTomala,YunhaoTang,DaliaElBadawy, Nguyen, Paula Kurylowicz, Sarmishta Velury, Se-
Elspeth White, Basil Mustafa, Oran Lang, Ab- bastianKrause,CassidyHardin,LucasDixon,Lili
hishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Janzer, Kiam Choo, Ziqiang Feng, Biao Zhang,
Caelles,RossHemsley,GregoryThornton,Fangxi- AchintyaSinghal, TejasiLatkar, MingyangZhang,
aoyuFeng,WojciechStokowiec,CeZheng,Phoebe QuocLe,ElenaAllicaAbellan,DayouDu,DanMcK-
Thacker, Çag˘lar Ünlü, Zhishuai Zhang, Moham- innon,NatashaAntropova,TolgaBolukbasi,Orgad
madSaleh,JamesSvensson,MaxBileschi,Piyush Keller,DavidReid,DanielFinchelstein,MariaAbi
Patil,AnkeshAnand,RomanRing,KaterinaTsihlas, Raad,RemiCrocker,PeterHawkins,RobertDadashi,
ArpiVezer,MarcoSelvi,TobyShevlane,MikelRo- ColinGaffney,SidLall,KenFranko,EgorFilonov,
driguez, Tom Kwiatkowski, Samira Daruki, Keran AnnaBulanova,RémiLeblond,VikasYadav,Shirley
Rong, Allan Dafoe, Nicholas FitzGerald, Keren Chung, Harry Askham, Luis C. Cobo, Kelvin Xu,
Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, FelixFischer,JunXu,ChristinaSorokin,ChrisAl-
Marie Pellat, Vladimir Feinberg, James Cobon- berti,Chu-ChengLin,ColinEvans,HaoZhou,Alek
Kerr, Tara Sainath, Maribeth Rauh, Sayed Hadi Dimitriev, Hannah Forbes, Dylan Banarse, Zora
Hashemi, Richard Ives, Yana Hasson, YaGuang Tung,JeremiahLiu,MarkOmernick,ColtonBishop,
Li, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou, ChintuKumar,RachelSterneck,RyanFoley,Rohan
QingzeWang,ThibaultSottiaux,MichelaPaganini, Jain,SwaroopMishra,JiaweiXia,TaylorBos,Ge-
Jean-BaptisteLespiau,AlexandreMoufarek,Samer offrey Cideron, Ehsan Amid, Francesco Piccinno,
Hassan, Kaushik Shivakumar, Joost van Amers- Xingyu Wang, Praseem Banzal, Petru Gurita, Hila
foort, Amol Mandhane, Pratik Joshi, Anirudh Noga, Premal Shah, Daniel J. Mankowitz, Alex
Goyal,MatthewTung,AndrewBrock,HannahShea- Polozov,NateKushman,VictoriaKrakovna,Sasha
han, Vedant Misra, Cheng Li, Nemanja Rakic´evic´, Brown, MohammadHossein Bateni, Dennis Duan,
MostafaDehghani,FangyuLiu,SidMittal,Junhyuk Vlad Firoiu, Meghana Thotakuri, Tom Natan, An-
Oh,SebNoury,ErenSezener,FantineHuot,Matthew hadMohananey, MatthieuGeist, SidharthMudgal,
Lamm, Nicola De Cao, Charlie Chen, Gamaleldin SertanGirgin, HuiLi, JiayuYe, OfirRoval, ReikoTojo, Michael Kwong, James Lee-Thorp, Christo- Vyas, Martin Wicke, Xiao Ma, Taylan Bilal, Ev-
pherYew,QuanYuan,SumitBagri,DanilaSinopal- genii Eltyshev, Daniel Balle, Nina Martin, Hardie
nikov,SabelaRamos,JohnMellor,AbhishekSharma, Cate, James Manyika, Keyvan Amiri, Yelin Kim,
Aliaksei Severyn, Jonathan Lai, Kathy Wu, Heng- XiXiong,KaiKang,FlorianLuisier,NileshTripu-
TzeCheng, DavidMiller, NicolasSonnerat, Denis raneni,DavidMadras,MandyGuo,AustinWaters,
Vnukov,RoryGreig,JenniferBeattie,EmilyCave- OliverWang,JoshuaAinslie,JasonBaldridge,Han
ness,LibinBai,JulianEisenschlos,AlexKorchem- Zhang,GarimaPruthi,JakobBauer,FengYang,Ri-
niy,TomyTsai,MimiJasarevic,WeizeKong,Phuong ham Mansour, Jason Gelman, Yang Xu, George
Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Polovets, Ji Liu, Honglong Cai, Warren Chen, Xi-
Zhu, Mark Geller, Tian Huey Teh, Jason Sanmiya, angHaiSheng,EmilyXue,SherjilOzair,AdamsYu,
EvgenyGladchenko,NejcTrdin,AndreiSozanschi, ChristofAngermueller,XiaoweiLi,WeirenWang,Ju-
DanielToyama,EvanRosen,SasanTavakkol,Lint- liaWiesinger,EmmanouilKoukoumidis,YuanTian,
ingXue,ChenElkind,OliverWoodman,JohnCar- AnandIyer,MadhuGurumurthy,MarkGoldenson,
penter,GeorgePapamakarios,RupertKemp,Sushant Parashar Shah, MK Blake, Hongkun Yu, Anthony
Kafle, Tanya Grunina, Rishika Sinha, Alice Tal- Urbanowicz,JennimariaPalomaki,ChrisanthaFer-
bert,AbhimanyuGoyal,DianeWu,DeneseOwusu- nando, Kevin Brooks, Ken Durden, Harsh Mehta,
Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont- NikolaMomchev,ElaheRahimtoroghi,MariaGeor-
Tuset,PradyumnaNarayana,JingLi,SabaerFatehi, gaki, Amit Raul, Sebastian Ruder, Morgan Red-
JohnWieting,OmarAjmeri,BenignoUria,TaoZhu, shaw,JinhyukLee,KomalJalan,DinghuaLi,Ginger
Yeongil Ko, Laura Knight, Amélie Héliou, Ning Perng,BlakeHechtman,ParkerSchuh,MiladNasr,
Niu,ShaneGu,ChenxiPang,DustinTran,Yeqing MiaChen,KieranMilan,VladimirMikulik,Trevor
Li, Nir Levine, Ariel Stolovich, Norbert Kalb, Re- Strohman, JulianaFranco,TimGreen, DemisHas-
becaSantamaria-Fernandez,SonamGoenka,Wenny sabis,KorayKavukcuoglu,JeffreyDean,andOriol
Yustalim,RobinStrudel,AliElqursh,BalajiLaksh- Vinyals.2023. Gemini: Afamilyofhighlycapable
minarayanan,CharlieDeck,ShyamUpadhyay,Hyo multimodalmodels.
Lee, Mike Dusenberry, Zonglin Li, Xuezhi Wang,
Tiago Timponi Torrent, Thomas Hoffmann,
Kyle Levin, Raphael Hoffmann, Dan Holtmann-
Arthur Lorenzi Almeida, and Mark Turner.
Rice, Olivier Bachem, Summer Yue, Sho Arora,
2023. CopilotsforLinguists: AI,Constructions,and
Eric Malmi, Daniil Mirylenka, Qijun Tan, Christy
Frames. CambridgeUniversityPress.
Koh, Soheil Hassas Yeganeh, Siim Põder, Steven
Zheng, Francesco Pongetti, Mukarram Tariq, Yan-
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
hua Sun, Lucian Ionita, Mojtaba Seyedhosseini,
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Pouya Tafti, Ragha Kotikalapudi, Zhiyu Liu, An-
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
molGulati,JasmineLiu,XinyuYe,BartChrzaszcz,
Bhosale,DanBikel,LukasBlecher,CristianCanton
Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown,
Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton,
JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
ChenkaiKuang,VinodKoverkathu,ChristopherA.
CynthiaGao,VedanujGoswami,NamanGoyal,An-
Choquette-Choo,YunjieLi,TJLu,AbeIttycheriah,
thonyHartshorn,SagharHosseini,RuiHou,Hakan
PrakashShroff,PeiSun,ManiVaradarajan,SanazBa-
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
hargam,RobWilloughby,DavidGaddy,IshitaDas-
IsabelKloumann,ArtemKorenev,PunitSinghKoura,
gupta,GuillaumeDesjardins,MarcoCornero,Brona
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
Robenek, Bhavishya Mittal, Ben Albrecht, Ashish
anaLiskovich,YinghaiLu,YuningMao,XavierMar-
Shenoy,FedorMoiseev,HenrikJacobsson,Alireza
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
Ghaffarkhah,MorganeRivière,AlannaWalton,Clé-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
ment Crepy, Alicia Parrish, Yuan Liu, Zongwei
stein,RashiRungta,KalyanSaladi,AlanSchelten,
Zhou,ClementFarabet,CareyRadebaugh,Praveen
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
Srinivasan, Claudia van der Salm, Andreas Fidje-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
land,SalvatoreScellato,EriLatorre-Chimoto,Hanna
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Klimczak-Plucin´ska, David Bridson, Dario de Ce-
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
sare, Tom Hudson, Piermaria Mendolicchio, Lexi
Melanie Kambadur, Sharan Narang, Aurelien Ro-
Walker,AlexMorris,IvoPenchev,MatthewMauger,
driguez,RobertStojnic,SergeyEdunov,andThomas
AlexeyGuseynov,AlisonReid,SethOdoom,Lucia
Scialom.2023. Llama2: Openfoundationandfine-
Loher,VictorCotruta,MadhaviYenugula,Dominik
tunedchatmodels.
Grewe,AnastasiaPetrushkina,TomDuerig,Antonio
Sanchez,SteveYadlowsky,AmyShen,AmirGlober- Yu-HsiangTseng,Cing-FangShih,Pin-ErChen,Hsin-
son,AdamKurzrok,LynetteWebb,SahilDua,Dong YuChou,Mao-ChangKu,andShu-KaiHsieh.2022.
Li,PreethiLahoti,SuryaBhupatiraju,DanHurt,Ha- CxLM:Aconstructionandcontext-awarelanguage
roonQureshi,AnanthAgarwal,TomerShani,Matan model. InProceedingsoftheThirteenthLanguage
Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei ResourcesandEvaluationConference,pages6361–
Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang 6369, Marseille, France. European Language Re-
Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, sourcesAssociation.
YiSun,YaoZhao,StephanLee,PanduNayak,Doug
Fritz,ManishReddyVuyyuru,JohnAslanides,Nidhi AlexWarstadt,AmanpreetSingh,andSamuelR.Bow-
man.2019. Neuralnetworkacceptabilityjudgments.TransactionsoftheAssociationforComputational C Fulldetailsforeachprompt
Linguistics,7:625–641.
WereportinTables5to19thedetailsoftheprompt,
LeonieWeissweiler,ValentinHofmann,AbdullatifKök- along with the change that it represents from a
sal,andHinrichSchütze.2022. Thebetteryoursyn- previousprompt.
tax, the better your semantics? probing pretrained
language models for the English comparative cor- D FewShots
relative. InProceedingsofthe2022Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing, InTable20,wegivethefiveshotsfromeachclass
pages10859–10882,AbuDhabi,UnitedArabEmi-
giventoChatGPTasexamples.
rates.AssociationforComputationalLinguistics.
DanniYu,LuyangLi,HangSu,andMatteoFuoli.2023.
Assessingthepotentialofllm-assistedannotationfor
corpus-basedpragmaticsanddiscourseanalysis: The
caseofapologies. InternationalJournalofCorpus
Linguistics.
A DetailedGraphsofPromptandTotal
Costs
We include a visual representation of the cost of
each prompt dependenton the human annotation
cost per sentence. All prompts are displayed in
Figure3. InFigure4,weremovemostoftheworse-
performingpromptsandshowthedifferentslopes
and intercept points. In Figure 5, we highlight
thepointsinthehumanannotationcostwherethe
optimalpromptchanges.
B FullStatisticsoneachprompt
Prompt Prec. Rec. F1 HR$ GPT$inct
1 41.84 57.89 48.58 2.389 0.01
2 46.70 69.69 55.92 2.141 0.01
3 55.13 76.69 64.15 1.813 0.04
4 55.61 78.19 65.00 1.798 0.04
5 56.54 71.42 63.12 1.768 0.04
6 52.28 77.44 62.42 1.912 0.06
7 53.53 79.69 64.04 1.867 0.03
8 43.93 87.21 58.43 2.275 0.07
9 34.63 93.23 50.50 2.887 0.04
10 76.62 44.36 56.19 1.305 0.13
11 76.54 46.61 57.94 1.306 0.12
12 83.75 50.37 62.91 1.194 0.14
13 83.33 52.63 64.51 1.200 0.17
14 78.35 57.14 66.08 1.276 0.44
15 78.02 53.38 63.39 1.281 0.18
16 85.50 44.36 58.41 1.169 0.55
17 90.09 75.18 81.96 1.110 1.62
18 91.81 75.93 83.12 1.089 4.83
Table4: FullEvaluationResultsforEveryPrompt
In Table 4, we report the sequence of prompts
thatwetriedandtheirperformance. Detailsofall
promptsandthechangesfromthepreviousprompt
areavailableinAppendixSectionC.0HumanOnly
1Base
21+bettertaskdescription
0.175 32+JSON
43+systemprompt
0.150 54+mixedshots
65+structuredinputonly
0.125 75+substringonly
85+sentenceandstructured
0.100 95+sentenceandsubstring
105+explanations
0.075 1111+halvesamplesto25
1210+samplesdownto10
0.050 1310+samplesdownto5
1410+samplesdownto1
0.025 1512+10moreshots
1612+majorityvote
0.000 1712+GPT-4
0.00 0.01 0.02 0.03 0.04 0.05
1816+GPT-4
HumanAnnotationCostperSentenceinUSD
Figure3: Totalcostperfinaltrueannotatedexamplecomparedtothecostofonehumansentenceannotation
0.175
0.150
0HumanOnly
0.125
1Base
1210+samplesdownto10
0.100
1612+majorityvote
1712+GPT-4
0.075
1816+GPT-4
0.050
0.025
0.000
0.00 0.01 0.02 0.03 0.04 0.05
HumanAnnotationCostperSentenceinUSD
Figure4: Totalcostperfinaltrueannotatedexamplecomparedtothecostofonehumansentenceannotation,with
onlyprompts0(fullymanualhumanbaseline),1(baseprompt),12,16,17,and18included
SystemPrompt -
Instruction Thetaskistoclassifywhetherthesentencesareinstancesofthecausedmotionconstructionasfirst
introducedbyGoldberg(1992)ornot.
InputFormat Hereare5positiveexamples: id, sentence, labelHereare5negativeexamples: id, sentence, label.
Classifythefollowingsentences:id,sentence
OutputFormat Replywithacsvcodeblock(wrappedinthreebackticks),withtheheaders’id’and’label’.labelshould
beeitherTrueorFalse.Labelall50sentences.
ShotsperClass 5
Sentences 50
Model GPT-3.5
MajorityVote No
Table5: Prompt1
DSUniecnetneSdetatonnnAeurTreptsoClatoT
DSUniecnetneSdetatonnnAeurTreptsoClatoT2.5
2.0
1.5 1210+samplesdownto10
1612+majorityvote
1712+GPT-4
1.0 1816+GPT-4
0.5
0.0
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
HumanAnnotationCostperSentenceinUSD
Figure 5: Total cost per final true annotated example compared to the cost of one human sentence annotation,
focusedonthepointswheretheoptimalpromptchangesfrom12to16,16to17,and17to18
SystemPrompt -
Instruction Thetaskistoclassifywhetherthesentencesareinstancesofthecausedmotionconstructionasfirst
introducedbyGoldberg(1992)ornot.Acaused-motionconstructionisalinguisticphenomenonwherea
verbdescribesanactionthatresultsinachangeoflocationormotionforaspecificobject.Yourtaskwill
betounderstandwhatisgoingoninthesentenceanddetermineiftheverbdescribesanactionthatresults
inachangeoflocationormotionforaspecificobject.Keepinmindthatthecaused-motionconstruction
israre,andlabelthesentencesaccordingly.
InputFormat Hereare5positiveexamples: id, sentence, labelHereare5negativeexamples: id, sentence, label.
Classifythefollowingsentences:id,sentence
OutputFormat Replywithacsvcodeblock(wrappedinthreebackticks),withtheheaders’id’and’label’.labelshould
beeitherTrueorFalse.Labelall50sentences.
ShotsperClass 5
Sentences 50
Model GPT-3.5
MajorityVote No
Change Prompt1withlongerinstruction
Table6: Prompt2.
SystemPrompt -
Instruction Thetaskistoclassifywhetherthesentencesareinstancesofthecausedmotionconstructionasfirst
introducedbyGoldberg(1992)ornot.Acaused-motionconstructionisalinguisticphenomenonwherea
verbdescribesanactionthatresultsinachangeoflocationormotionforaspecificobject.Yourtaskwill
betounderstandwhatisgoingoninthesentenceanddetermineiftheverbdescribesanactionthatresults
inachangeoflocationormotionforaspecificobject.Keepinmindthatthecaused-motionconstruction
israre,andlabelthesentencesaccordingly.
InputFormat Hereare5positiveexamples: {"id": id,"sentence": sentence,"label": label}. Hereare5negative
examples: {"id": id,"sentence": sentence,"label": label}. Classifythefollowingsentences: {"id":
id,"sentence":sentence}.
OutputFormat Respond with a jsonl codeblock (wrapped in three backticks). Each object should include an "id",
"sentence",andfinallya"label"fieldwitheither"true"or"false".Labelall50sentences.
ShotsperClass 5
Sentences 50
Model GPT-3.5
MajorityVote No
Change Prompt2withinputandoutputformatchangedtoJSON
Table7: Prompt3
DSUniecnetneSdetatonnnAeurTreptsoClatoTSystemPrompt Youarealinguisticexpertspecializinginsyntax,specificallythecaused-motionconstructioninEnglish
sentences. Yourtaskistoanalyzegivensentencesandclassifywhethertheyexhibitthisconstruction
ornot. Remembertocarefullyconsiderthestructureandmeaningofeachsentencetomakethemost
accuratedetermination.
Instruction Thetaskistoclassifywhetherthesentencesareinstancesofthecausedmotionconstructionasfirst
introducedbyGoldberg(1992)ornot.Acaused-motionconstructionisalinguisticphenomenonwherea
verbdescribesanactionthatresultsinachangeoflocationormotionforaspecificobject.Yourtaskwill
betounderstandwhatisgoingoninthesentenceanddetermineiftheverbdescribesanactionthatresults
inachangeoflocationormotionforaspecificobject.Keepinmindthatthecaused-motionconstruction
israre,andlabelthesentencesaccordingly.
InputFormat Hereare5positiveexamples: {"id": id,"sentence": sentence,"label": label}. Hereare5negative
examples: {"id": id,"sentence": sentence,"label": label}. Classifythefollowingsentences: {"id":
id,"sentence":sentence}.
OutputFormat Respond with a jsonl codeblock (wrapped in three backticks). Each object should include an "id",
"sentence",andfinallya"label"fieldwitheither"true"or"false".Labelall50sentences.
ShotsperClass 5
Sentences 50
Model GPT-3.5
MajorityVote No
Change Prompt3withaddedsystemprompt
Table8: Prompt4
SystemPrompt Youarealinguisticexpertspecializinginsyntax,specificallythecaused-motionconstructioninEnglish
sentences. Yourtaskistoanalyzegivensentencesandclassifywhethertheyexhibitthisconstruction
ornot. Remembertocarefullyconsiderthestructureandmeaningofeachsentencetomakethemost
accuratedetermination.
Instruction Thetaskistoclassifywhetherthesentencesareinstancesofthecausedmotionconstructionasfirst
introducedbyGoldberg(1992)ornot.Acaused-motionconstructionisalinguisticphenomenonwherea
verbdescribesanactionthatresultsinachangeoflocationormotionforaspecificobject.Yourtaskwill
betounderstandwhatisgoingoninthesentenceanddetermineiftheverbdescribesanactionthatresults
inachangeoflocationormotionforaspecificobject.Keepinmindthatthecaused-motionconstruction
israre,andlabelthesentencesaccordingly.
InputFormat Hereare10exampleswithgroundtruthlabels::{"id":id,"sentence":sentence,"label":label}.Classify
thefollowingsentences:{"id":id,"sentence":sentence}.
OutputFormat Respond with a jsonl codeblock (wrapped in three backticks). Each object should include an "id",
"sentence",andfinallya"label"fieldwitheither"true"or"false".Labelall50sentences.
ShotsperClass 5
Sentences 50
Model GPT-3.5
MajorityVote No
Change Prompt4withfewshotsalternatingbetweenTrueandFalselabels.
Table9: Prompt5SystemPrompt Youarealinguisticexpertspecializinginsyntax,specificallythecaused-motionconstructioninEnglish
sentences. Yourtaskistoanalyzegivensentencesandclassifywhethertheyexhibitthisconstruction
ornot. Remembertocarefullyconsiderthestructureandmeaningofeachsentencetomakethemost
accuratedetermination.
Instruction Thetaskistoclassifywhetherthesentencesareinstancesofthecausedmotionconstructionasfirst
introducedbyGoldberg(1992)ornot.Acaused-motionconstructionisalinguisticphenomenonwherea
verbdescribesanactionthatresultsinachangeoflocationormotionforaspecificobject.Yourtaskwill
betounderstandwhatisgoingoninthesentenceanddetermineiftheverbdescribesanactionthatresults
inachangeoflocationormotionforaspecificobject.Keepinmindthatthecaused-motionconstruction
israre,andlabelthesentencesaccordingly.
InputFormat Hereare10exampleswithgroundtruthlabels::{"id":id,"verb":verb,"directobject":directobject,
"preposition":preposition,"prepositionalobject":prepositionalobject,"label":label}.Classifythe
followingsentences:{"id":id,"verb":verb,"directobject":directobject,"preposition":preposition,
"prepositionalobject":prepositionalobject}.
OutputFormat Respondwithajsonlcodeblock(wrappedinthreebackticks).Eachobjectshouldincludean"id","verb",
"directobject", "preposition", "prepositionalobject", andfinallya"label"fieldwitheither"true"or
"false".Labelall50sentences.
ShotsperClass 5
Sentences 50
Model GPT-3.5
MajorityVote No
Change Prompt5withthesentencesreplacedbytheinformationextractedwiththedependencyfilteringstep
Table10: Prompt6
SystemPrompt Youarealinguisticexpertspecializinginsyntax,specificallythecaused-motionconstructioninEnglish
sentences. Yourtaskistoanalyzegivensentencesandclassifywhethertheyexhibitthisconstruction
ornot. Remembertocarefullyconsiderthestructureandmeaningofeachsentencetomakethemost
accuratedetermination.
Instruction Thetaskistoclassifywhetherthesentencesareinstancesofthecausedmotionconstructionasfirst
introducedbyGoldberg(1992)ornot.Acaused-motionconstructionisalinguisticphenomenonwherea
verbdescribesanactionthatresultsinachangeoflocationormotionforaspecificobject.Yourtaskwill
betounderstandwhatisgoingoninthesentenceanddetermineiftheverbdescribesanactionthatresults
inachangeoflocationormotionforaspecificobject.Keepinmindthatthecaused-motionconstruction
israre,andlabelthesentencesaccordingly.
InputFormat Hereare10exampleswithgroundtruthlabels: : {"id": id,"string": verbdirectobjectpreposition
prepositionalobject,"label":label}.Classifythefollowingsentences:{"id":id,"string":verbdirect
objectprepositionprepositionalobject}.
OutputFormat Respondwithajsonlcodeblock(wrappedinthreebackticks). Eachobjectshouldincludean"id",a
"string",andfinallya"label"fieldwitheither"true"or"false".Labelall50sentences.
ShotsperClass 5
Sentences 50
Model GPT-3.5
MajorityVote No
Change Prompt5withthesentencesreplacedbythesubstringfromtheverbtotheprepositionalobject
Table11: Prompt7SystemPrompt Youarealinguisticexpertspecializinginsyntax,specificallythecaused-motionconstructioninEnglish
sentences. Yourtaskistoanalyzegivensentencesandclassifywhethertheyexhibitthisconstruction
ornot. Remembertocarefullyconsiderthestructureandmeaningofeachsentencetomakethemost
accuratedetermination.
Instruction Thetaskistoclassifywhetherthesentencesareinstancesofthecausedmotionconstructionasfirst
introducedbyGoldberg(1992)ornot.Acaused-motionconstructionisalinguisticphenomenonwherea
verbdescribesanactionthatresultsinachangeoflocationormotionforaspecificobject.Yourtaskwill
betounderstandwhatisgoingoninthesentenceanddetermineiftheverbdescribesanactionthatresults
inachangeoflocationormotionforaspecificobject.Keepinmindthatthecaused-motionconstruction
israre,andlabelthesentencesaccordingly.
InputFormat Hereare10exampleswithgroundtruthlabels::{"id":id,"sentence":sentence,"verb":verb,"direct
object":directobject,"preposition":preposition,"prepositionalobject":prepositionalobject,"label":
label}.Classifythefollowingsentences:{"id":id,"sentence":sentence,"verb":verb,"directobject":
directobject,"preposition":preposition,"prepositionalobject":prepositionalobject}.
OutputFormat Respond with a jsonl codeblock (wrapped in three backticks). Each object should include an "id",
"sentence","verb","directobject","preposition","prepositionalobject",andfinallya"label"fieldwith
either"true"or"false".Labelall50sentences.
ShotsperClass 5
Sentences 50
Model GPT-3.5
MajorityVote No
Change Prompt6withthefullsentenceaddedbackin
Table12: Prompt8
SystemPrompt Youarealinguisticexpertspecializinginsyntax,specificallythecaused-motionconstructioninEnglish
sentences. Yourtaskistoanalyzegivensentencesandclassifywhethertheyexhibitthisconstruction
ornot. Remembertocarefullyconsiderthestructureandmeaningofeachsentencetomakethemost
accuratedetermination.
Instruction Thetaskistoclassifywhetherthesentencesareinstancesofthecausedmotionconstructionasfirst
introducedbyGoldberg(1992)ornot.Acaused-motionconstructionisalinguisticphenomenonwherea
verbdescribesanactionthatresultsinachangeoflocationormotionforaspecificobject.Yourtaskwill
betounderstandwhatisgoingoninthesentenceanddetermineiftheverbdescribesanactionthatresults
inachangeoflocationormotionforaspecificobject.Keepinmindthatthecaused-motionconstruction
israre,andlabelthesentencesaccordingly.
InputFormat Hereare10exampleswithgroundtruthlabels::{"id":id,"sentence":sentence,"string":verbdirect
objectprepositionprepositionalobject,"label": label}. Classifythefollowingsentences: {"id": id,
"sentence":sentence,"string":verbdirectobjectprepositionprepositionalobject}.
OutputFormat Respondwithajsonlcodeblock(wrappedinthreebackticks). Eachobjectshouldincludean"id",a
"sentence",a"string",andfinallya"label"fieldwitheither"true"or"false".Labelall50sentences.
ShotsperClass 5
Sentences 50
Model GPT-3.5
MajorityVote No
Change Prompt7withthefullsentenceaddedbackin
Table13: Prompt9SystemPrompt Youarealinguisticexpertspecializinginsyntax,specificallythecaused-motionconstructioninEnglish
sentences. Yourtaskistoanalyzegivensentencesandclassifywhethertheyexhibitthisconstruction
ornot. Remembertocarefullyconsiderthestructureandmeaningofeachsentencetomakethemost
accuratedetermination.
Instruction Thetaskistoclassifywhetherthesentencesareinstancesofthecausedmotionconstructionasfirst
introducedbyGoldberg(1992)ornot.Acaused-motionconstructionisalinguisticphenomenonwherea
verbdescribesanactionthatresultsinachangeoflocationormotionforaspecificobject.Yourtaskwill
betounderstandwhatisgoingoninthesentenceanddetermineiftheverbdescribesanactionthatresults
inachangeoflocationormotionforaspecificobject.Keepinmindthatthecaused-motionconstruction
israre,andlabelthesentencesaccordingly.
InputFormat Hereare10exampleswithexampleswithexplanationsandgroundtruthlabels::{"id":id,"sentence":
sentence, "explanation": explanation, "label": label }. Classify the following sentences: { "id":
id,"sentence":sentence}.
OutputFormat Respond with a jsonl codeblock (wrapped in three backticks). Each object should include an "id",
"sentence","explanation",andfinallya"label"fieldwitheither"true"or"false".Labelall50sentences.
ShotsperClass 5
Sentences 50
Model GPT-3.5
MajorityVote No
Change Prompt5withexplanationsforthelabelsaddedtoinputandrequiredfromthemodelfortheoutput
Table14: Prompt10
SystemPrompt Youarealinguisticexpertspecializinginsyntax,specificallythecaused-motionconstructioninEnglish
sentences. Yourtaskistoanalyzegivensentencesandclassifywhethertheyexhibitthisconstruction
ornot. Remembertocarefullyconsiderthestructureandmeaningofeachsentencetomakethemost
accuratedetermination.
Instruction Thetaskistoclassifywhetherthesentencesareinstancesofthecausedmotionconstructionasfirst
introducedbyGoldberg(1992)ornot.Acaused-motionconstructionisalinguisticphenomenonwherea
verbdescribesanactionthatresultsinachangeoflocationormotionforaspecificobject.Yourtaskwill
betounderstandwhatisgoingoninthesentenceanddetermineiftheverbdescribesanactionthatresults
inachangeoflocationormotionforaspecificobject.Keepinmindthatthecaused-motionconstruction
israre,andlabelthesentencesaccordingly.
InputFormat Hereare10exampleswithexampleswithexplanationsandgroundtruthlabels::{"id":id,"sentence":
sentence, "explanation": explanation, "label": label }. Classify the following sentences: { "id":
id,"sentence":sentence}.
OutputFormat Respond with a jsonl codeblock (wrapped in three backticks). Each object should include an "id",
"sentence","explanation",andfinallya"label"fieldwitheither"true"or"false".Labelall50sentences.
ShotsperClass 5
Sentences 50→25→10→5→1
Model GPT-3.5
MajorityVote No
Change Prompt10withdifferentnumbersofsentencesclassifiedperprompt
Table15: Prompts11-14
SystemPrompt Youarealinguisticexpertspecializinginsyntax,specificallythecaused-motionconstructioninEnglish
sentences. Yourtaskistoanalyzegivensentencesandclassifywhethertheyexhibitthisconstruction
ornot. Remembertocarefullyconsiderthestructureandmeaningofeachsentencetomakethemost
accuratedetermination.
Instruction Thetaskistoclassifywhetherthesentencesareinstancesofthecausedmotionconstructionasfirst
introducedbyGoldberg(1992)ornot.Acaused-motionconstructionisalinguisticphenomenonwherea
verbdescribesanactionthatresultsinachangeoflocationormotionforaspecificobject.Yourtaskwill
betounderstandwhatisgoingoninthesentenceanddetermineiftheverbdescribesanactionthatresults
inachangeoflocationormotionforaspecificobject.Keepinmindthatthecaused-motionconstruction
israre,andlabelthesentencesaccordingly.
InputFormat Hereare20exampleswithexampleswithexplanationsandgroundtruthlabels::{"id":id,"sentence":
sentence, "explanation": explanation, "label": label }. Classify the following sentences: { "id":
id,"sentence":sentence}.
OutputFormat Respond with a jsonl codeblock (wrapped in three backticks). Each object should include an "id",
"sentence","explanation",andfinallya"label"fieldwitheither"true"or"false".Labelall50sentences.
ShotsperClass 10
Sentences 10
Model GPT-3.5
MajorityVote No
Change Prompt12withthenumberofshotsperclassdoubledfrom5to10
Table16: Prompt15SystemPrompt Youarealinguisticexpertspecializinginsyntax,specificallythecaused-motionconstructioninEnglish
sentences. Yourtaskistoanalyzegivensentencesandclassifywhethertheyexhibitthisconstruction
ornot. Remembertocarefullyconsiderthestructureandmeaningofeachsentencetomakethemost
accuratedetermination.
Instruction Thetaskistoclassifywhetherthesentencesareinstancesofthecausedmotionconstructionasfirst
introducedbyGoldberg(1992)ornot.Acaused-motionconstructionisalinguisticphenomenonwherea
verbdescribesanactionthatresultsinachangeoflocationormotionforaspecificobject.Yourtaskwill
betounderstandwhatisgoingoninthesentenceanddetermineiftheverbdescribesanactionthatresults
inachangeoflocationormotionforaspecificobject.Keepinmindthatthecaused-motionconstruction
israre,andlabelthesentencesaccordingly.
InputFormat Hereare10exampleswithexampleswithexplanationsandgroundtruthlabels::{"id":id,"sentence":
sentence, "explanation": explanation, "label": label }. Classify the following sentences: { "id":
id,"sentence":sentence}.
OutputFormat Respond with a jsonl codeblock (wrapped in three backticks). Each object should include an "id",
"sentence","explanation",andfinallya"label"fieldwitheither"true"or"false".Labelall50sentences.
ShotsperClass 5
Sentences 10
Model GPT-3.5
MajorityVote Yes
Change Prompt12withamajorityvotefromthreeseparateruns
Table17: Prompt16
SystemPrompt Youarealinguisticexpertspecializinginsyntax,specificallythecaused-motionconstructioninEnglish
sentences. Yourtaskistoanalyzegivensentencesandclassifywhethertheyexhibitthisconstruction
ornot. Remembertocarefullyconsiderthestructureandmeaningofeachsentencetomakethemost
accuratedetermination.
Instruction Thetaskistoclassifywhetherthesentencesareinstancesofthecausedmotionconstructionasfirst
introducedbyGoldberg(1992)ornot.Acaused-motionconstructionisalinguisticphenomenonwherea
verbdescribesanactionthatresultsinachangeoflocationormotionforaspecificobject.Yourtaskwill
betounderstandwhatisgoingoninthesentenceanddetermineiftheverbdescribesanactionthatresults
inachangeoflocationormotionforaspecificobject.Keepinmindthatthecaused-motionconstruction
israre,andlabelthesentencesaccordingly.
InputFormat Hereare10exampleswithexampleswithexplanationsandgroundtruthlabels::{"id":id,"sentence":
sentence, "explanation": explanation, "label": label }. Classify the following sentences: { "id":
id,"sentence":sentence}.
OutputFormat Respond with a jsonl codeblock (wrapped in three backticks). Each object should include an "id",
"sentence","explanation",andfinallya"label"fieldwitheither"true"or"false".Labelall50sentences.
ShotsperClass 5
Sentences 10
Model GPT-4
MajorityVote No
Change Prompt12usingGPT-4
Table18: Prompt17
SystemPrompt Youarealinguisticexpertspecializinginsyntax,specificallythecaused-motionconstructioninEnglish
sentences. Yourtaskistoanalyzegivensentencesandclassifywhethertheyexhibitthisconstruction
ornot. Remembertocarefullyconsiderthestructureandmeaningofeachsentencetomakethemost
accuratedetermination.
Instruction Thetaskistoclassifywhetherthesentencesareinstancesofthecausedmotionconstructionasfirst
introducedbyGoldberg(1992)ornot.Acaused-motionconstructionisalinguisticphenomenonwherea
verbdescribesanactionthatresultsinachangeoflocationormotionforaspecificobject.Yourtaskwill
betounderstandwhatisgoingoninthesentenceanddetermineiftheverbdescribesanactionthatresults
inachangeoflocationormotionforaspecificobject.Keepinmindthatthecaused-motionconstruction
israre,andlabelthesentencesaccordingly.
InputFormat Hereare10exampleswithexampleswithexplanationsandgroundtruthlabels::{"id":id,"sentence":
sentence, "explanation": explanation, "label": label }. Classify the following sentences: { "id":
id,"sentence":sentence}.
OutputFormat Respond with a jsonl codeblock (wrapped in three backticks). Each object should include an "id",
"sentence","explanation",andfinallya"label"fieldwitheither"true"or"false".Labelall50sentences.
ShotsperClass 5
Sentences 10
Model GPT-4
MajorityVote Yes
Change Prompt16usingGPT-4
Table19: Prompt18Sentence Verb DirObj Prep P-Obj Lab. Explanation
Samsneezedthenapkinofftheta- sneeze napkin off table True This is caused-motion, because
ble. Samsneezingiscausingthenap-
kintomoveoffthetable.
Joeygratedthecheeseontoaserv- grate cheese onto plate True Thisiscaused-motion,becausethe
ingplate. grating is causing the cheese to
moveontotheplate.
Samassistedheroutoftheroom. assist she outof room True This is caused-motion, because
Sam assisting is causing her to
moveoutoftheroom.
He nudged the golf ball into the nudge ball into hole True This is caused-motion, because
hole. himnudgingtheballiscausingit
tomoveintothehole.
Frank squeezed the ball through squeeze ball through crack True This is caused-motion, because
thecrack. Frankismovingtheballthrough
thewholebysqueezingit.
Thehammerbrokethevaseinto break vase into piece False This is not caused-motion, be-
pieces. causethevaseischangingitsstate
into pieces, the pieces are not a
destination.
ChristyblewSamunderthetable. instruct he into room False This is not caused-motion, be-
cause you are not moving under
thetablebecauseChristyisblow-
ing, theblowingactionistaking
placeunderthetable.
AdeleraisedhereyebrowsatSam raise eyebrow at I False This is not caused-motion, be-
. causewhileAdeleismovingher
eyebrows, they are not literally
movingtowardsSam.
Theyseparatedpeopleintogroups separate people into group False This is not caused-motion, be-
. causethepeoplearen’tmovingto-
wardsgroups,theyarebecoming
thegroups.
Hiscanehelpedhimintothecar. help he into car False Thisisnotcaused-motionbecause
thecaneisn’tcausingthemotion,
itisbeingusedasatooltoassist
withthemotion.
Table20: FewShotsusedinallpromptswiththestructuredinformationandexplanationsthatareusedinsome
prompts. P-ObjstandsforPrepositionalObject,DirObjforDirectObject.