SCALABLE DISTRIBUTED OPTIMIZATION OF
MULTI-DIMENSIONAL FUNCTIONS DESPITE BYZANTINE
ADVERSARIES
APREPRINT
KananartKuwaranancharoen LeiXin
IntelLabs TheChineseUniversityofHongKong
IntelCorporation lxinshenqing@gmail.com
Hillsboro,OR97124
kananart.kuwaranancharoen@intel.com
ShreyasSundaram
SchoolofElectricalandComputerEngineering
PurdueUniversity
WestLafayette,IN47907
sundara2@purdue.edu
ABSTRACT
Theproblemofdistributedoptimizationrequiresagroupofnetworkedagentstocomputeaparameter
that minimizes the average of their local cost functions. While there are a variety of distributed
optimizationalgorithmsthatcansolvethisproblem, theyaretypicallyvulnerableto“Byzantine”
agents that do not follow the algorithm. Recent attempts to address this issue focus on single
dimensionalfunctions,orassumecertainstatisticalpropertiesofthefunctionsattheagents. Inthis
paper,weprovidetworesilient,scalable,distributedoptimizationalgorithmsformulti-dimensional
functions. Ourschemesinvolvetwofilters,(1)adistance-basedfilterand(2)amin-maxfilter,which
each remove neighborhood states that are extreme (defined precisely in our algorithms) at each
iteration. WeshowthatthesealgorithmscanmitigatetheimpactofuptoF (unknown)Byzantine
agentsintheneighborhoodofeachregularagent. Inparticular,weshowthatifthenetworktopology
satisfiescertainconditions,alloftheregularagents’statesareguaranteedtoconvergetoabounded
regionthatcontainstheminimizeroftheaverageoftheregularagents’functions.
Keywords ByzantineAttacks·ConvexOptimization·DistributedAlgorithms·FaultTolerantSystems·GraphTheory·
MachineLearning·Multi-AgentSystems·NetworkSecurity
1 Introduction
Thedesignofdistributedalgorithmshasreceivedsignificantattentioninthepastfewdecades[1,2]. Inparticular,for
theproblemofdistributedoptimization,asetofagentsinanetworkarerequiredtoreachagreementonaparameterthat
minimizestheaverageoftheirlocalobjectivefunctions,usinginformationreceivedfromtheirneighbors[3,4,5,6]. A
varietyofapproacheshavebeenproposedtotackledifferentchallengesofthisproblem,e.g.,distributedoptimization
underconstraints[7],distributedoptimizationundertime-varyinggraphs[8],anddistributedoptimizationfornon-
convexnon-smoothfunctions[9]. However,theseexistingworkstypicallymaketheassumptionthatallagentsare
trustworthyandcooperative(i.e.,theyfollowtheprescribedprotocol);indeed,suchprotocolsfailifevenasingleagent
behavesinamaliciousorincorrectmanner[10].
Assecuritybecomesamoreimportantconsiderationinlargescalesystems,itiscrucialtodevelopalgorithmsthatare
resilienttoagentsthatdonotfollowtheprescribedalgorithm. Ahandfulofrecentpapershaveconsideredfaulttolerant
algorithmsforthecasewhereagentmisbehaviorfollowsspecificpatterns[11,12]. Amoregeneral(andserious)form
ofmisbehavioriscapturedbytheByzantineadversarymodelfromcomputerscience,wheremisbehavingagentscan
4202
raM
11
]AM.sc[
1v20560.3042:viXraScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
sendarbitrary(andconflicting)valuestotheirneighborsateachiterationofthealgorithm. UndersuchByzantine
behavior,ithasbeenshownthatitisimpossibletoguaranteecomputationofthetrueoptimalpoint[10,13]. Thus,
researchershavebegunformulatingdistributedoptimizationalgorithmsthatallowthenon-adversarialnodestoconverge
toacertainregionsurroundingthetrueminimizer,regardlessoftheadversaries’actions[13,14,10,15].
Itisworthnotingthatonemajorlimitationoftheaboveworks[13,14,10,15]isthattheyallmaketheassumptionof
scalar-valuedobjectivefunctions,andtheextensionoftheaboveideastogeneralmulti-dimensionalconvexfunctions
remains largely open. In fact, one major challenge for minimizing multi-dimensional functions is that the region
containingtheminimizerofthesumoffunctionsisitselfdifficulttocharacterize. Specifically,incontrasttothecaseof
scalarfunctions,wheretheglobalminimizer1alwayslieswithinthesmallestintervalcontainingalllocalminimizers,
theregioncontainingtheminimizerofthesumofmulti-dimensionalfunctionsmaynotnecessarilybeintheconvex
hulloftheminimizers[16].
There exists a branch of literature focusing on secure distributed machine learning in a client-server architecture
[17,18,19],wheretheserverappropriatelyfilterstheinformationreceivedfromtheclients. However,theirextensions
toadistributed(peer-to-peer)settingremainsunclear. Thepapers[20,21,22]consideravectorversionoftheresilient
machinelearningprobleminadistributed(peer-to-peer)setting. Thesepapersshowthatthestatesofregularnodes
willconvergetothestatisticalminimizerwithhighprobability(astheamountofdataofeachnodegoestoinfinity),
buttheanalysisisrestrictedtoi.i.dtrainingdataacrossthenetwork. However,wheneachagenthasafiniteamount
of data, these algorithms are still vulnerable to sophisticated attacks as shown in [23]. The work [24] considers a
Byzantine distributed optimization problem for multi-dimensional functions, but relies on redundancy among the
localfunctions,andalsorequirestheunderlyingcommunicationnetworktobecomplete. Theworkpresentedin[25]
proposesaresilientalgorithmunderstatisticalcharacteristicassumptionsbutlacksguarantees. Therecentwork[26]
studiesresilientstochasticoptimizationproblemundernon-convexandsmoothassumptionsonlocalfunctions,which
differsfromourfocus. Thealgorithmproposedinthatworkachievesconvergencetoastationarypointuptoaconstant
errorbutdoesnotensureasymptoticconsensus. Additionally,therecentwork[27]offersconvergenceguaranteesto
aneighborhoodoftheoptimalsolutionunderdeterministicsettings,butitpertainstoadistinctclassoffunctions–
stronglyconvexandsmoothfunctions.
To the best of our knowledge, our conference paper [28] is the first one that provides a scalable algorithm with
convergenceguaranteesingeneralnetworksunderverygeneralconditionsonthemulti-dimensionalconvexfunctions
heldbytheagentsinthepresenceofByzantinefaults. Differentfromexistingworks,thealgorithmin[28]doesnot
relyonanystatisticalassumptionsorredundancyoflocalfunctions. Technically,theanalysisaddressesthechallengeof
findingaregionthatcontainstheglobalminimizerformultiple-dimensionalfunctions,andshowsthatregularstates
areguaranteedtoconvergetothatregionundertheproposedalgorithm. TheDistance-MinMaxFilteringDynamicsin
[28]requireseachregularnodetocomputeanauxiliarypointusingresilientasymptoticconsensustechniquesontheir
individualfunctions’minimizersinadvance. Afterthat,therearetwofilteringstepsinthemainalgorithmthathelp
regularnodestodiscardextremestates. Thefirststepistoremoveextremestates(basedonthedistancetotheauxiliary
point),andthesecondstepistoremovestatesthathaveextremevaluesinanyoftheircomponents. Ontheotherhand,
thealgorithmin[28]suffersfromtheneedtocomputetheauxiliarypointpriortorunningthemainalgorithm,sincethe
fixedauxiliarypointisonlyachievedbytheresilientconsensusalgorithmasymptotically.
Inthispaper,weeliminatethisdrawback. Thealgorithmsandanalysisweproposehereexpandupontheworkin[28]
inthefollowingsignificantways. First,thealgorithmsinthispaperbringthecomputationoftheauxiliarypointintothe
mainalgorithm,sothatthelocalupdateofauxiliarypointandlocalfilteringstrategiesareperformedsimultaneously.
Thismakestheanalysismuchmoreinvolvedsinceweneedtotakeintoaccountthecoupleddynamicsoftheestimated
auxiliarypointandtheoptimizationvariables. Second,thealgorithmsmakebetteruseoflocalinformationbyincluding
eachregularnode’sownstateasametric. Inpractice,weobservethatthisperformsbetterthantheapproachin[28],
since each agent may discard fewer states and hence, there are more non-extreme states that can help the regular
agentsgetclosetothetrueglobalminimizer. Again,wecharacterizetheconvergenceregionthatallregularstates
areguaranteedtoconvergetousingtheproposedalgorithm. Third,wepresentanalternatealgorithminthispaper
whichonlymakesuseofthedistancefilter(asopposedtoboththedistanceandmin-maxfilter);weshowthatthis
algorithmsignificantlyreducestherequirementsonthenetworktopologyforourconvergenceguarantees,atthecost
oflosingguaranteesonconsensusoftheregularnodes’states. Importantly,ourworkrepresentsthefirstattemptto
provideconvergenceguaranteesinageometricsense,characterizingaregionwhereallstatesareensuredtoconverge
to,withoutrelyingonanystatisticalassumptionsorredundancyoflocalfunctions.
Ourpaperisorganizedasfollows. Section2introducesvariousmathematicalpreliminaries,andstatestheproblemof
resilientdistributedoptimization. WeprovideourproposedalgorithmsinSection3. Wethenstatetheassumptions
andsomeimportantresultsrelatedtopropertiesoftheproposedalgorithmsinSection4. InSection5,weprovide
1Wewillusetheterms“globalminimizer"and“minimizerofthesum"interchangeablysinceweonlyconsiderconvexfunctions.
2ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
discussionontheresults. Finally,wesimulateouralgorithmstonumericallyevaluatetheirperformanceinSection6,
andconcludeinSection7.
2 MathematicalNotationandProblemFormulation
LetN,ZandRdenotethesetofnaturalnumbers(includingzero),integers,andrealnumbers,respectively. Wealso
denotethesetofpositiveintegersbyZ . Thecardinalityofasetisdenotedby|·|. Thesetofsubgradientsofaconvex
+
functionf atpointxiscalledthesubdifferentialoff atx,andisdenoted∂f(x).
2.1 LinearAlgebra
Vectorsaretakentobecolumnvectors,unlessotherwisenoted. Weusex(ℓ)torepresenttheℓ-thcomponentofavector
x. TheEuclideannormonRd isdenotedby∥·∥. Wedenoteby⟨u,v⟩theEuclideaninnerproductofuandv,i.e.,
⟨u,v⟩ = uTv andby∠(u,v)theanglebetweenvectorsuandv,i.e.,∠(u,v) = arccos(cid:0) ⟨u,v⟩ (cid:1) . WeuseS+ to
∥u∥∥v∥ d
denotethesetofpositivedefinitematricesinRd×d. TheEuclideanballind-dimensionalspacewithcenteratx and
0
radiusr ∈R isdenotedbyB(x ,r):={x∈Rd :∥x−x ∥≤r}.
>0 0 0
2.2 GraphTheory
WedenoteanetworkbyadirectedgraphG = (V,E),whichconsistsofthesetofnodesV = {v ,v ,...,v }and
1 2 N
thesetofedgesE ⊆V ×V. If(v ,v )∈E,thennodev canreceiveinformationfromnodev . Thein-neighborand
i j j i
out-neighborsetsaredenotedbyNin ={v ∈V : (v ,v )∈E}andNout ={v ∈V : (v ,v )∈E},respectively.
i j j i i j i j
Apathfromnodev ∈ V tonodev ∈ V isasequenceofnodesv ,v ,...,v suchthatv = v ,v = v and
i j k1 k2 kl k1 i kl j
(v ,v )∈E for1≤r ≤l−1. Throughoutthepaper,thetermsnodesandagentswillbeusedinterchangeably.
kr kr+1
Givenasetofvectors{x ,x ,...,x },whereeachx ∈Rd,wedefineforallS ⊆V,
1 2 N i
{x } :={x ∈Rd :v ∈S}.
i S i i
Definition2.1. AgraphG =(V,E)issaidtoberootedatnodev ∈V ifforallnodesv ∈V \{v },thereisapath
i j i
fromv tov . Agraphissaidtoberootedifitisrootedatsomenodev ∈V.
i j i
Wewillrelyonthefollowingdefinitionsfrom[29].
Definition2.2(r-reachableset). ForagivengraphG andapositiveintegerr ∈Z ,asubsetofnodesS ⊆V issaidto
+
ber-reachableifthereexistsanodev ∈S suchthat|Nin\S|≥r.
i i
Definition 2.3 (r-robust graph). For r ∈ Z , a graph G is saidto be r-robust if for allpairs of disjointnonempty
+
subsetsS ,S ⊂V,atleastoneofS orS isr-reachable.
1 2 1 2
Theabovedefinitionscapturetheideathatsetsofnodesshouldcontainindividualnodesthathaveasufficientnumber
ofneighborsoutsidethatset. Thiswillbeimportantforthelocaldecisionsmadebyeachnodeinthenetworkunderour
algorithm,andwillallowinformationfromtherestofthenetworktopenetrateintodifferentsetsofnodes.
2.3 AdversarialBehavior
Definition2.4. Anodev ∈V issaidtobeByzantineifduringeachiterationoftheprescribedalgorithm,itiscapable
i
of sending arbitrary (and perhaps conflicting) values to different neighbors. It is also allowed to update its local
informationarbitrarilyateachiterationofanyprescribedalgorithm.
ThesetofByzantinenodesisdenotedbyA⊂V. ThesetofregularnodesisdenotedbyR=V \A.
TheidentitiesoftheByzantineagentsareunknowntoregularagentsinadvance. Furthermore,weallowtheByzantine
agentstoknowtheentiretopologyofthenetwork,functionsequippedbytheregularnodes,andthedeployedalgorithm.
Inaddition,ByzantineagentsareallowedtocoordinatewithotherByzantineagentsandaccessthecurrentandprevious
informationcontainedbythenodesinthenetwork(e.g. currentandpreviousstatesofallnodes). Suchextremebehavior
istypicalinthestudyoftheadversarialmodels[10,13,20]. Inexchangeforallowingsuchextremebehavior,wewill
consideralimitationonthenumberofsuchadversariesintheneighborhoodofeachregularnode,asfollows.
Definition2.5(F-localmodel). ForF ∈Z ,wesaythatthesetofadversariesAisanF-localsetif|Nin∩A|≤F,
+ i
forallv ∈R.
i
Thus,theF-localmodelcapturestheideathateachregularnodehasatmostF Byzantinein-neighbors.
3ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
2.4 ProblemFormulation
ConsideragroupofN agentsV interconnectedoveragraphG =(V,E). Eachagentv ∈V hasalocalconvexcost
i
functionf :Rd →R. Theobjectiveistocollaborativelysolvetheminimizationproblem
i
1 (cid:88)
min f (x), (1)
x∈Rd N i
vi∈V
wherex ∈ Rd isthecommondecisionvariable. Acommonapproachtosolvesuchproblemsisforeachagentto
maintainalocalestimateofthesolutiontotheaboveproblem,whichititerativelyupdatesbasedoncommunications
withitsimmediateneighbors. However,sinceByzantinenodesareallowedtosendarbitraryvaluestotheirneighbors
ateachiterationofanyalgorithm, itisnotpossibletosolveProblem(1)undersuchmisbehavior(sinceoneisnot
guaranteedtoinferanyinformationaboutthetruefunctionsoftheByzantineagents)[13,10]. Thus,theoptimization
problemisrecastintothefollowingform:
1 (cid:88)
min f (x), (2)
x∈Rd |R| i
vi∈R
i.e.,werestrictourattentiononlytothefunctionsheldbyregularnodes.
Remark1. Intheresilientdistributedoptimizationproblem,theagentsarerequiredtocomputeavaluethat(approxi-
mately)minimizesthesumoffunctionsheldbyeach(regular)agent. Comparedtotheresilientconsensusproblem,this
necessitatesmoreinformationthansimplytheinitialvectorsheldbyeachagent(evenifthosevectorsareinitialized
tobethelocalminimizersoftheagents’functions). Indeed,theneedtocombineestimatesofthemulti-dimensional
minimizerfromneighbors,whileincorporatinggradientdynamics,allinaresilientfashioniswhatmakestheresilient
distributedoptimizationproblemmoredifficultthanthestandardconsensusproblem.
Remark2. Theadditionalchallengeinsolvingtheaboveproblemliesinthefactthatnoregularagentisawareofthe
identitiesoractionsoftheByzantineagents. Furthermore,intheworst-casescenario,itisnotfeasibletoachievean
exactsolutiontoProblem2,astheByzantineagentscanmodifythefunctionswhilestilladheringtothealgorithm,
makingitimpossibletodifferentiatethem[13,10].
Inthenextsection,weproposetwoscalablealgorithmsthatallowtheregularnodestoapproximatelysolvetheabove
problem,regardlessoftheidentitiesoractionsoftheByzantineagents(asprovenlaterinthepaper).
3 ResilientDistributedOptimizationAlgorithms
3.1 ProposedAlgorithms
The algorithms that we propose are stated as Algorithm 1 and Algorithm 2. We start with Algorithm 1. At each
time-stepk,eachregularnode2v ∈Rmaintainsandupdatesavectorx [k]∈Rd,whichisitsestimateofthesolution
i i
toProblem(2),andavectory [k] ∈ Rd,whichisitsestimateofanauxiliarypointthatprovidesageneralsenseof
i
directionforeachagenttofollow.
Remark3. Thepurposeoftheestimatesx [k]istobeanapproximationtotheminimizerofthesumofthefunctions.
i
Toupdatethisestimate,theagentshavetodecidewhichoftheestimatesprovidedbytheirneighborstoretainateach
iterationofthealgorithm(sinceuptoF ofthoseneighboringestimatesmaybeadversariallychosenbyByzantine
agents). Tohelpeachregularagentdecidewhichestimatestokeep,theauxiliarypointsy [k]areusedtoperformthe
i
distance-basedfilteringstep(Line7). Infact,eachauxiliarypointprovidesageneralsenseofdirectionfortheagents’
estimates,andthushelpsthemfilteroutadversarialestimatesthatattempttodrawthemawayfromthetrueminimizer.
WenowexplaineachstepusedinAlgorithm1indetail.3
• Line1: xˆ∗ ←optimize(f )
i i
Eachnodev ∈Rusesanyappropriateoptimizationalgorithmtogetanapproximateminimizerxˆ∗ ∈Rdof
i i
itslocalfunctionf . Weassumethatthereexistsϵ∗ ∈R suchthatthealgorithmachieves∥xˆ∗−x∗∥≤ϵ∗
i ≥0 i i
forallv ∈Rwherex∗ ∈Rdisatrueminimizerofthefunctionf ;weassumeformallythatsuchatrue(but
i i i
notnecessaryunique)minimizerexistsforeachv ∈Rinthenextsection.
i
2Byzantinenodesdonotnecessarilyneedtofollowtheabovealgorithm,andcanupdatetheirstateshowevertheywish.
3Inthealgorithm,X [k],Xdist[k],Xmm[k],Y [k]andYmm[k]aremultisets.
i i i i i
4ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
Algorithm1SimultaneousDistance-MinMaxFilteringDynamics
InputNetworkG,functions{f }N ,parameterF
i i=1
1: Eachv i ∈Rsetsxˆ∗ i ←optimize(f i)
2: Eachv i ∈Rsetsx i[0]←xˆ∗ i andy i[0]←xˆ∗ i
3: fork =0,1,2,3,...do
4: forv i ∈Rdo ▷Implementinparallel
StepI:BroadcastandReceive
5: broadcast(N iout,x i[k],y i[k])
6: X i[k], Y i[k]←receive(N iin)
StepII:ResilientConsensusStep
7: X idist[k]←dist_filt(F, y i[k], X i[k])
8: Xmm[k]←x_minmax_filt(F, Xdist[k])
i i
9: z i[k]←x_weighted_average(X imm[k])
StepIII:GradientUpdate
10: x i[k+1]←gradient(f i,z i[k])
StepIV:UpdatetheEstimatedAuxiliaryPoint
11: Y imm[k]←y_minmax_filt(F, Y i[k])
12: y [k+1]←y_weighted_average(Ymm[k])
i i
13: endfor
14: endfor
• Line2: x [0]←xˆ∗andy [0]←xˆ∗
i i i i
Eachnodev ∈RinitializesitsownestimatedsolutiontoProblem(2)(x [0]∈Rd)andestimatedauxiliary
i i
point(y [0]∈Rd)tobexˆ∗.
i i
• Line5: broadcast(Nout,x [k],y [k])
i i i
Nodev ∈Rbroadcastsitscurrentstatex [k]andestimatedauxiliarypointy [k]toitsout-neighborsNout.
i i i i
• Line6: X [k], Y [k]←receive(Nin)
i i i
Nodev ∈Rreceivesthecurrentstatesx [k]andy [k]fromitsin-neighborsNin. So,attimestepk,nodev
i j j i i
possessesthesetsofstates4
X [k]:=(cid:8) x [k]∈Rd :v ∈Nin∪{v }(cid:9) and Y [k]:=(cid:8) y [k]∈Rd :v ∈Nin∪{v }(cid:9) .
i j j i i i j j i i
The sets X [k] and Y [k] have an indirect relationship through the distance-based filter (Line 7) as only
i i
y [k]∈Y [k]isusedasthereferencetoremovestatesinX [k].
i i i
• Line7: Xdist[k]←dist_filt(F, y [k], X [k])
i i i
Intuitively,regularnodev ignoresthestatesthatarefarawayfromitsownauxiliarystatey [k]inL2sense.
i i
Formally,nodev ∈RcomputesthedistancebetweeneachvectorinX [k]anditsownestimatedauxiliary
i i
pointy [k]:
i
D [k]:=∥x [k]−y [k]∥ for x [k]∈X [k]. (3)
ij j i j i
Then,nodev ∈Rsortsthevaluesintheset{D [k]:v ∈Nin∪{v }}andremovestheF largestvalues
i ij j i i
thatarelargerthanitsownvalueD [k]. IftherearefewerthanF valueshigherthanitsownvalue,v removes
ii i
allofthosevalues. Tiesinvaluesarebrokenarbitrarily. Thecorrespondingstatesoftheremainingvaluesare
storedinXdist[k]. Inotherwords,regularnodev removesuptoF ofitsneighbors’vectorsthatarefurthest
i i
awayfromtheauxiliarypointy [k].
i
• Line8: Xmm[k]←x_minmax_filt(F, Xdist[k])
i i
Intuitively, regular node v ignores the states that contains extreme values in any of their components in
i
the ordering sense. Formally, for each time-step k ∈ N and dimension ℓ ∈ {1,2,...,d}, define the set
Vremove(ℓ)[k]⊆Nin,whereanodev isinVremove(ℓ)[k]ifandonlyif
i i j i
– x(ℓ)[k]iswithintheF-largestvaluesof(cid:8) x(ℓ)[k]∈R:x [k]∈Xdist[k](cid:9) andx(ℓ)[k]>x(ℓ)[k],or
j r r i j i
– x(ℓ)[k]iswithintheF-smallestvaluesof(cid:8) x(ℓ)[k]∈R:x [k]∈Xdist[k](cid:9) andx(ℓ)[k]<x(ℓ)[k].
j r r i j i
4Incasearegularnodev hasaByzantineneighborv ,weabusenotationandtakethevaluex [k]tobethevaluereceivedfrom
i j j
nodev (i.e.,itdoesnothavetorepresentthetruestateofnodev ).
j j
5ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
Tiesinvaluesarebrokenarbitrarily. Nodev thenremovesthestateofallnodesin(cid:83) Vremove(ℓ)[k]
i ℓ∈{1,2,...,d} i
andtheremainingstatesarestoredinXmm[k]:
i
(cid:110) (cid:91) (cid:111)
Xmm[k]= x [k]∈Rd :v ∈Vdist[k]\ Vremove(ℓ)[k] , (4)
i j j i i
ℓ∈{1,...,d}
whereVdist[k]=(cid:8)
v ∈R:x
[k]∈Xdist[k](cid:9)
.
i j j i
• Line9: z [k]←x_weighted_average(Xmm[k])
i i
Eachnodev ∈Rcomputes
i
(cid:88)
z [k]= w [k]x [k], (5)
i x,ij j
xj[k]∈X imm[k]
wherew [k]>0forallx
[k]∈Xmm[k]and(cid:80)
w [k]=1.
x,ij j i xj[k]∈X imm[k] x,ij
• Line10: x [k+1]←gradient(f ,z [k])
i i i
Nodev ∈Rcomputesthegradientupdateasfollows:
i
x [k+1]=z [k]−η[k]g [k], (6)
i i i
whereg [k]∈∂f (z [k])andη[k]isthestep-sizeattimek. Theconditionscorrespondingtothestep-sizeare
i i i
giveninthenextsection.
• Line11: Ymm[k]←y_minmax_filt(F, Y [k])
i i
For each dimension ℓ ∈ {1,2,...,d}, node v ∈ R removes the F highest and F lowest values of its
i
neighbors’auxiliarypointsalongthatdimension. Morespecifically,foreachdimensionℓ ∈ {1,2,...,d},
nodev sortsthevaluesinthesetofscalars{y(ℓ)[k] : y [k] ∈ Y [k]}andthenremovestheF largestand
i j j i
F smallest values that are larger and smaller than its own value, respectively. If there are fewer than F
values higher (resp. lower) than its own value, v removes all of those values. Ties in values are broken
i
arbitrarily. TheremainingvaluesarestoredinYmm[k](ℓ)andthesetYmm[k]isthecollectionofYmm[k](ℓ),
i i i
i.e.,Ymm[k]=(cid:8) Ymm[k](ℓ):ℓ∈{1,2,...,d}(cid:9)
.
i i
• Line12: y [k+1]←y_weighted_average(Ymm[k])
i i
Foreachdimensionℓ∈{1,2,...,d},eachnodev ∈Rcomputes
i
y(ℓ)[k+1]= (cid:88) w(ℓ) [k]y(ℓ)[k], (7)
i y,ij j
y(ℓ)[k]∈Ymm[k](ℓ)
j i
wherew(ℓ) [k]>0forally(ℓ)[k]∈Ymm[k](ℓ)and(cid:80) w(ℓ) [k]=1.
y,ij j i y(ℓ)[k]∈Ymm[k](ℓ) y,ij
j i
Note that the filtering process x_minmax_filt (Line 8) and the filtering process y_minmax_filt (Line 11) are
different. In x_minmax_filt, each node removes the whole state vector for a neighbor if it contains an extreme
value in any component, while in y_minmax_filt, each node only removes the extreme components in each vec-
tor. Inaddition,x_weighted_average(Line9)andy_weighted_average_2(Line12)arealsodifferentinthat
x_weighted_average designates agent v at time-step k to utilize the same set of weights {w ∈ R : x [k] ∈
i x,ij j
Xmm[k]} for all components while y_weighted_average allows agent v at time-step k to use a different set of
i i
weights {w(ℓ) ∈ R : y(ℓ)[k] ∈ Ymm[k](ℓ)} for each coordinate ℓ (since the number of remaining values in each
y,ij j i
component|Ymm[k](ℓ)|isnotnecessarilythesame). Thesedifferenceswillbecomeclearwhenconsideringtheexample
i
providedinthenextsubsection.
WeconsideravariantofAlgorithm1definedasfollows.
Algorithm2SimultaneousDistanceFilteringDynamics
Algorithm2isthesameasAlgorithm1exceptthat
• Line8isremoved,and
• Xmm[k]inLine9isreplacedbyXdist[k].
i i
AlthoughAlgorithms1and2areverysimilar(differingonlyintheuseofanadditionalfilterinAlgorithm1),our
subsequentanalysiswillrevealtherelativecostsandbenefitsofeachalgorithm. Weemphasizethatbothalgorithms
involveonlysimpleoperationsineachiteration,andthattheregularagentsdonotneedtoknowthenetworktopology,or
6ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
functionspossessedbyanotheragents. Furthermore,theregularagentsdonotneedtoknowtheidentitiesofadversaries;
theyonlyneedtoknowtheupperboundforthenumberoflocaladversaries. However,weassumethatallregularagents
usethesamestep-sizeη[k](Line10,equation(6)).
Remark4. WhiletheBRIDGEframework,introducedin[21],encompassesseveralByzantine-resilientdistributed
optimization algorithms, including those presented in [14, 10, 30], our proposed algorithms, namely Algorithm 1
andAlgorithm2,introduceanovelconceptofauxiliarystates. Specifically,eachregularagentv inouralgorithms
i
maintainsanauxiliarystatey [k],updatedusingaconsensusalgorithm,placingthemwithinthebroaderframeworkof
i
REDGRAF[27].
WhileAlgorithm1fromourworksharesasimilaritywithBRIDGE-T[21]byutilizingthecoordinate-wisetrimmed
mean,therearedistinctivedifferencesasfollows.
• Firstly,ouralgorithmemploysadistance-basedfilterinadditiontothetrimmedmeanfilter,allowingforan
asymptoticconvergenceguaranteeundermilderassumptions(asprovidedinSection4.1). Incontrast,the
convergenceanalysisofBRIDGE-Treliesonthemorerestrictiveassumptionofi.i.d. trainingdata.
• Secondly,thetrimmedmeanfilterinBRIDGE-TeliminatesboththesmallestandlargestF values,whereas
ourfilterdiscardsasubsetofthesevalues,similartotheimplementationin[10]. Thisvariantinourapproach
resultsinfasterconvergenceinpracticeduetotheresultingdensernetworkconnectivityafterthefiltering
stepswhichfacilitatesquickerinformationflow[31].
• Lastly,whileBRIDGE-Tusesasimpleaveragetocombinetheremainingstates,ouralgorithmemploysa
weightedaverage. TheseweightsarechosentosatisfyAssumption4.5,ensuringthattheweightsarelower
boundedbyapositiveconstantifthecorrespondingagentsremainafterthetrimmedmeanfilter. Thisprovides
amoreversatileandgeneralscheme.
3.2 ExampleofAlgorithm1
Before we prove the convergence properties of the algorithms, we first demonstrate Algorithm 1, which is more
complicatedduetothemin-maxfilteringstep(Line8),stepbystepusinganexample.
Supposethereare8agentsformingthecompletegraph(forthepurposeofillustration). Letnodev havethelocal
i
objectivefunctionf :R2 →Rdefinedasf (x)=(x(1)+i)2+(x(2)−i)2 foralli∈{1,2,...,8}. Letthesetof
i i
adversarialnodesbeA = {v ,v }andthus,wehaveR = {v ,v ,v ,v ,v ,v }. Notethatonlytheregularnodes
4 8 1 2 3 5 6 7
executethealgorithm(andtheydonotknowwhichagentsareadversarial). LetF =2andatsometime-stepkˆ ∈N,
eachregularnodehasthefollowingstateandtheestimatedauxiliarypoint:5
x [kˆ]=[4 2]T , y [kˆ]=[0 0]T ,
1 1
x [kˆ]=[4 1]T , y [kˆ]=[−1 −2]T ,
2 2
x [kˆ]=[3 3]T , y [kˆ]=[−2 1]T ,
3 3
x [kˆ]=[2 1]T , y [kˆ]=[0 2]T ,
5 5
x [kˆ]=[1 4]T , y [kˆ]=[1 3]T ,
6 6
x [kˆ]=[0 0]T , y [kˆ]=[1 3]T .
7 7
Letx [k](resp. y [k])bethestate(resp. estimatedauxiliarypoint)thatissentfromtheadversarialnodev ∈A
a→b a→b a
totheregularnodev ∈Rattime-stepk. Supposethatintime-stepkˆ,eachadversarialagentsendsthesamestatesand
b
thesameestimatedauxiliarypointstoitsneighbors(althoughthisisnotnecessary)asfollows:
x [kˆ]=[3 2]T , y [kˆ]=[−1 1]T ,
4→i 4→i
x [kˆ]=[0 5]T , y [kˆ]=[2 2]T
8→i 8→i
foralli∈{1,2,3,5,6,7}. Wewilldemonstratethecalculationofx [kˆ+1]andy [kˆ+1],computedbyregularnode
1 1
v .
1
5Thenumberofagentsinthisdemonstrationisnotenoughtosatisfytherobustnesscondition(Assumption4.4)presentedinthe
nextsection.However,forourpurposehere,itisenoughtoconsiderasmallnumberofagentstogainanunderstandingforeachstep
ofthealgorithm.
7ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
Sincethenetworkisthecompletegraph,thesetofin-neighborsandout-neighborsofnodev isNin =Nout =V\{v }
1 1 1 1
andX [kˆ](resp. Y [kˆ])includesallthestates(resp. estimatedauxiliarypoints). Then,nodev performsthedistance
i i 1
filteringstep(Line7)asfollows. First,itcalculatesthesquareddistancesD2 [kˆ](sincesquaringdoesnotalterthe
1j
order)forallx [kˆ]∈X [kˆ]asin(3). Nodev has
j i 1
D2 [kˆ]=20, D2 [kˆ]=17, D2 [kˆ]=18, D2 [kˆ]=13,
11 12 13 14
D2 [kˆ]=5, D2 [kˆ]=17, D2 [kˆ]=0, D2 [kˆ]=25.
15 16 17 18
SinceD2 [kˆ]isthesecondlargest,nodev discardsonlynodev ’sstate(whichisthefurthestawayfromv ’sauxiliary
11 1 8 1
point)andXdistcontainsallstatesexceptx [kˆ]=x [kˆ].
1 8 8→1
Thennodev performsthemin-maxfilteringprocess(Line8)asfollows. First,considerthefirstcomponentofthe
1
statesinXdist. Thestatesofnodesv andv containthehighestvalueinthefirstcomponent(whichis4). Sincethe
1 1 2
tie can be broken arbitrarily, we choose x(1)[kˆ] to come first followed by x(1)[kˆ] in the ordering, so none of these
1 2
valuesarediscarded. Ontheotherhand,thestateofnodev containsthelowestvalueinitsfirstcomponent,while
7
nodev ’sstatecontainsthesecondlowestvalueinthatcomponent(sincenodev hasalreadybeendiscardedbythe
6 8
distancefilteringprocess). Nodev thussetsVremove(1)[kˆ]={v ,v }. Next,considerthesecondcomponentinwhich
1 1 6 7
thestatesofv andv containthehighestandsecondhighestvalues,respectively,andthestatesofv andv contain
6 3 7 5
thelowestandsecondlowestvalues,respectively. Thus,nodev setsVremove(2)[kˆ]={v ,v ,v ,v }. Sincenodev
1 1 3 5 6 7 1
removestheentirestatefromallthenodesinbothVremove(1)[kˆ]andVremove(2)[kˆ],accordingtoequation(4),wehave
1 1
Xmm[kˆ]=(cid:8) x [kˆ],x [kˆ],x [kˆ](cid:9) =(cid:8) [4 2]T,[4 1]T,[3 2]T(cid:9) .
1 1 2 4
Next,nodev performstheweightedaveragestep(Line9)asfollows,Supposenodev assignstheweightsw [kˆ]=
1 1 x,11
0.5, w [kˆ] = 0.25 and w [kˆ] = 0.25. Node v calculates the weighted average according to (5) yielding
x,12 x,14 1
z(1)[kˆ]=3.75andz(2)[kˆ]=1.75. Inthegradientstep(Line10),supposeη[kˆ]=0.1. Nodev calculatesthegradient
1 1 1
ofitslocalfunctionf atz [kˆ]whichyieldsg [kˆ]=[9.5 1.5]T andthencalculatesthestatex [kˆ+1]asdescribedin
1 1 1 1
(6)whichyieldsx [kˆ+1]=[2.8 1.6]T.
1
Next,weconsidertheestimatedauxiliarypointupdateofnodev . Infact,wecanperformtheupdate(Line11and
1
Line12)foreachcomponentseparately. First,considerthefirstcomponentinwhichv andv containthelargest
8 7
andsecondlargestvalues,respectively,andv andv containthesmallestandsecondsmallestvalues,respectively.
3 2
Nodev removesthesevaluesandthus,Ymm[kˆ](1)={y(1)[kˆ],y(1)[kˆ],y(1)[kˆ],y(1)[kˆ]}={0,−1,0,1}. Supposenode
1 1 1 4 5 6
v assignstheweightsw(1) [kˆ] = w(1) [kˆ] = w(1) [kˆ] = w(1) [kˆ] = 0.25. Then,theweightedaverageofthefirst
1 y,11 y,14 y,15 y,16
componentaccordingto(7)becomesy(1)[kˆ+1]=0. Finally,forthesecondcomponent,v andv containthelargest
1 6 7
values,andv andv containthesmallestandsecondsmallestvalues,respectively. Nodev removesthevalueobtained
2 1 1
fromv ,v andv andthus,thesetYmm[kˆ](2)={y(2)[kˆ],y(2)[kˆ],y(2)[kˆ],y(2)[kˆ],y(2)[kˆ]}={0,1,1,2,2}. Suppose
2 6 7 1 1 3 4 5 8
node v assigns the weights to each value in Ymm[kˆ](2) equally. The weighted average of the second component
1 1
becomesy(2)[kˆ+1]=1.2. Thus,wehavey [kˆ+1]=[0 1.2]T.
1 1
4 AssumptionsandMainResults
HavingdefinedthestepsinAlgorithms1and2,wenowturntoprovingtheirresilienceandconvergenceproperties.
4.1 Assumptions
Assumption4.1. Forallv ∈V,thefunctionsf (x)areconvex,andthesetsargminf (x)arenon-emptyandbounded.
i i i
Sincethesetargminf (x)isnon-empty,letx∗beanarbitraryminimizerofthefunctionf .
i i i
Assumption4.2. ThereexistsL∈R suchthat∥g˜ (x)∥ ≤Lforallx∈Rd,v ∈V,andg˜ (x)∈∂f (x).
>0 i 2 i i i
Theboundedsubgradientassumptionaboveiscommoninthedistributedconvexoptimizationliterature[32,33,34].
Assumption4.3. Thestep-sizesequence{η[k]}∞ ⊂R usedinLine11ofAlgorithm1isoftheform
k=0 >0
c
η[k]= 1 forsome c ,c ∈R . (8)
k+c 1 2 >0
2
8ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
Notethatthestep-sizein(8)satisfiesη[k+1]<η[k]forallk ∈N,and
∞
(cid:88)
lim η[k]=0 and η[k]=∞ (9)
k→∞
k=0
foranychoicesofc ,c ∈R .
1 2 >0
Assumption4.4. GivenapositiveintegerF ∈Z ,theByzantineagentsformaF-localset.
+
Assumption4.5. Forallk ∈Nandℓ∈{1,2,...,d},theweightsw [k]andw(ℓ) [k](usedinLine9andLine12
x,ij y,ij
ofAlgorithm1)arepositiveifandonlyifx [k]∈Xmm[k]forAlgorithm1(andx [k]∈Xdist[k]forAlgorithm2)and
j i j i
y(ℓ)[k]∈Ymm[k](ℓ),respectively. Furthermore,thereexistsω ∈R suchthatforallk ∈Nandℓ∈{1,2,...,d},the
j i >0
non-zeroweightsarelowerboundedbyω.
Remark5. RegardingthepriorknowledgeofF inAssumption4.4,wenotethat,aswithanyreliableorsecuresystem,
onehastodesignthesystemtoprovideadesireddegreeofreliability. Ifonerequiresthesystemtoprovideresilienceto
acertainnumberoffaultynodes,onehastodesignthealgorithm(andnetwork)tofacilitatethat. Thisisthestandard
philosophy and methodology in the literature [35, 18, 20]. Note that F does not have to be the exact number of
adversarialnodes–itisonlyanupperboundonthenumberofadversarialnodeslocally.
4.2 AnalysisofAuxiliaryPointUpdate
Since the dynamics of the estimated auxiliary points {y [k]} are independent of the dynamics of the estimated
i R
solutions{x [k]} ,webeginbyanalyzingtheconvergencepropertiesoftheestimatedauxiliarypoints{y [k]} .
i R i R
Inordertoestablishthisresult,weneedtodefinethefollowingscalarquantities. Fork ∈ Nandℓ ∈ {1,2,...,d},
letM(ℓ)[k] := max y(ℓ)[k],m(ℓ)[k] := min y(ℓ)[k],andD(ℓ)[k] := M(ℓ)[k]−m(ℓ)[k]. Definethevector
vi∈R i vi∈R i
D[k]:=(cid:2) D(1)[k],D(2)[k],··· ,D(d)[k](cid:3)T .
Thepropositionbelowshowsthattheestimatedauxiliarypoints{y [k]} convergeexponentiallyfasttoasinglepoint
i R
calledy[∞].
Proposition 4.6. Suppose Assumption 4 hold, the graph G is (2F + 1)-robust, and the weights w(ℓ) [k] satisfy
y,ij
Assumption4.5. Supposetheestimatedauxiliarypointsoftheregularagents{y [k]} followtheupdateruledescribed
i R
asLine11andLine12inAlgorithm1. Then, inbothAlgorithm1andAlgorithm2, thereexistsy[∞] ∈ Rd with
y(ℓ)[∞]∈(cid:2) m(ℓ)[k],M(ℓ)[k]]forallk ∈Nandℓ∈{1,2,...,d}suchthatforallv ∈R,wehave
i
∥y [k]−y[∞]∥<βe−αk,
i
whereα:= 1 log 1 >0,β := 1∥D[0]∥,andγ :=1− ω|R|−1.
|R|−1 γ γ 2
Theproofoftheabovepropositionfollowsbynotingthattheupdatesfor{y [k]} essentiallyboildowntoasetofd
i R
scalarconsensusupdates(oneforeachdimensionofthevector),Thus,onecandirectlyleveragetheproofforscalar
consensus(withfilteringofextremevalues)from[10,Proposition6.3]. WeprovidetheproofofProposition4.6in
AppendixB.
Recallthat{xˆ∗} isthesetcontainingtheapproximateminimizersoftheregularnodes’localfunctions. Letxbea
i R
matrixinRd×|R|,whereeachcolumnofxisadifferentvectorfrom{xˆ∗} . Inaddition,letxandxbethevectorsin
i R
Rddefinedbyx =max [x] andx =min [x] ,respectively. Sincewesety [0]=xˆ∗forallv ∈R
i 1≤j≤|R| ij i 1≤j≤|R| ij i i i
accordingtoLine2inAlgorithm1,wecanwrite
1 1
β = ∥D[0]∥= ∥x−x∥.
γ γ
4.3 ConvergencetoConsensusofStates
Havingestablishedconvergenceoftheauxiliarypointstoacommonvalue(fortheregularnodes),wenowconsiderthe
stateupdateandshowthatthestatesofallregularnodes{x [k]} asymptoticallyreachconsensusunderAlgorithm1.
i R
Beforestatingthemaintheorem,weprovidearesultfrom[10,Lemma2.3]whichisimportantforprovingthemain
theorem.
Lemma4.7. SupposethegraphG satisfiesAssumption4.4andis((2d+1)F +1)-robust. LetG′beagraphobtained
byremoving(2d+1)F orfewerincomingedgesfromeachnodeinG. ThenG′isrooted.
9ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
Thismeansthatifwehaveenoughredundancyinthenetwork(inthiscase,capturedbythe((2d+1)F +1)-robustness
condition),informationfromatleastonenodecanstillflowtotheothernodesinthenetworkevenaftereachregular
nodediscardsuptoF neighboringstatesinthedistancefilteringstep(Line7)andupto2dF neighboringstatesin
themin-maxfilteringstep(Line8). Thistransmissibilityofinformationisacrucialconditionforreachingconsensus
amongregularnodes.
Theorem4.8(Consensus). SupposeAssumptions4.2-4.5hold, andthegraphG is((2d+1)F +1)-robust. Ifthe
regularagentsfollowAlgorithm1thenforallv ,v ∈R,itholdsthat
i j
lim ∥x [k]−x [k]∥=0.
i j
k→∞
Proof. Itissufficienttoshowthatallregularnodesv ∈Rreachconsensusoneachcomponentoftheirvectorsx [k]as
i i
k →∞. Forallℓ∈{1,2,...,d}andforallv ∈R,from(5)and(6),theℓ-thcomponentofthevectorx [k]evolves
i i
as
x(ℓ)[k+1]= (cid:88) w [k]x(ℓ)[k]−η[k]g(ℓ)[k].
i x,ij j i
xj[k]∈X imm[k]
From[10,Proposition5.1],theaboveequationcanberewrittenas
x(ℓ)[k+1]= (cid:88) w¯(ℓ) [k]x(ℓ)[k]−η[k]g(ℓ)[k], (10)
i x,ij j i
vj∈(N iin∩R)∪{vi}
wherew¯(ℓ) [k]+(cid:80) w¯(ℓ) [k] = 1,andw¯(ℓ) [k] > ω andatleast|Nin|−2F oftheotherweightsarelower
boundedx b,i yi
ω.
vj∈N iin∩R x,ij x,ii i
2
ConsiderthesetXmm[k]whichisobtainedbyremovingatmostF +2dF statesreceivedfromv ’sneighbors(upto
i i
F statesremovedbythedistancefilteringprocessinline7,andupto2F additionalstatesremovedbythemin-max
filteringprocessoneachofthedcomponentsinline8). Sincethegraphis((2d+1)F +1)-robustandtheByzantine
agentsformanF-localsetbyAssumption4.4, fromLemma4.7, thesubgraphconsistingofregularnodeswillbe
rooted. Usingthefactthatthetermη[k]g(ℓ)[k]asymptoticallygoestozero(byAssumptions4.2and(9))andequation
i
(10),wecanproceedasintheproofof[10,Theorem6.1]toshowthat
lim |x(ℓ)[k]−x(ℓ)[k]|=0,
i j
k→∞
forallv ,v ∈R,whichcompletestheproof.
i j
Theorem 4.8 established consensus of the states of the regular agents, leveraging (and extending) similar analysis
for scalar functions from [10], only for Algorithm 1. However, this does not hold for Algorithm 2 since there
mightexistaregularagentv ∈ R,time-stepk ∈ Nanddimensionℓ ∈ {1,2,...,d}suchthatanadversarialstate
i
x(ℓ)[k] ∈ {x(ℓ)[k] ∈ R : x [k] ∈ Xdist[k],v ∈ A} cannot be written as a convex combination of (cid:8) x(ℓ)[k] ∈ R :
s j j i j j
v ∈ (Nin∩R)∪{v }(cid:9) ,andthuswecannotobtainequation(10). Ontheotherhand,Proposition4.6established
j i i
consensusoftheauxiliarypoints,whichwillbenowusedtocharacterizetheconvergenceregionofbothAlgorithm1
andAlgorithm2.
4.4 TheRegionToWhichTheStatesConverge
We now analyze the trajectories of the states of the agents under Algorithm 1 and Algorithm 2. We start with the
followingresultregardingtheintermediatestatez [k]calculatedinLines7-9ofAlgorithm1.
i
Lemma4.9. SupposeAssumptions4.4and4.5hold. Furthermore:
• iftheregularagentsfollowAlgorithm1,supposethegraphG is((2d+1)F +1)-robust;
• otherwise,iftheregularagentsfollowAlgorithm2,supposethegraphG is(2F +1)-robust.
Forallk ∈Nandv ∈R,ifthereexistsR [k]∈R suchthat∥x [k]−y [k]∥≤R [k]forallv ∈(Nin∩R)∪{v }
i i ≥0 j i i j i i
then∥z [k]−y [k]∥≤R [k].
i i i
Proof. ConsiderthedistancefilteringstepinLine7ofAlgorithm1. RecallthedefinitionofD [k]from(3). Wewill
ij
firstprovethefollowingclaim. Foreachk ∈ Nandv ∈ R, thereexistsv ∈ (Nin ∩R)∪{v }suchthatforall
i r i i
x [k]∈Xdist[k],
j i
∥x [k]−y [k]∥≤∥x [k]−y [k]∥,
j i r i
10ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
orequivalently,D [k]≤D [k].
ij ir
There are two possible cases. First, if the set Xdist[k] contains only regular nodes, we can simply choose v ∈
i r
(Nin∩R)∪{v }tobethenodewhosestatex [k]isfurthestawayfromy [k]. Next,considerthecasewhereXdist[k]
i i r i i
contains the state of one or more Byzantine nodes. Since node v ∈ R removes the F states from Nin that are
i i
furthestawayfromy [k](Line7),andthereareatmostF ByzantinenodesinNin,thereisatleastoneregularstate
i i
removedbynodev . Letv beoneoftheregularnodeswhosestateisremoved. WethenhaveD [k]≥D [k],forall
i r ir ij
v ∈{v ∈V :x [k]∈Xdist[k]}whichprovestheclaim.
j s s i
IfAlgorithm1isimplemented,letXˆ[k]=Xmm[k]andwehavethatXmm[k]⊆Xdist[k]duetothemin-maxfiltering
i i i i
step in Line 8. If Algorithm 2 is implemented, let Xˆ[k] = Xdist[k] since Line 8 is removed. Then, consider the
i i
weightedaveragestepinLine9. From(5),wehave
(cid:88) (cid:0) (cid:1)
z [k]−y [k]= w [k] x [k]−y [k] .
i i x,ij j i
xj[k]∈Xˆ i[k]
Since∥x [k]−y [k]∥≤∥x [k]−y [k]∥forallx [k]∈Xˆ[k](wherev isthenodeidentifiedintheclaimatthestart
j i r i j i r
oftheproof),weobtain
(cid:88) (cid:88)
∥z [k]−y [k]∥≤ w [k]∥x [k]−y [k]∥≤ w [k]∥x [k]−y [k]∥.
i i x,ij j i x,ij r i
xj[k]∈Xˆ i[k] xj[k]∈Xˆ i[k]
Sincev ∈(Nin∩R)∪{v },byourassumption,wehave∥x [k]−y [k]∥≤R [k]. Thus,usingtheaboveinequality
r i i r i i
andAssumption4.5,weobtainthat∥z [k]−y [k]∥≤R [k].
i i i
Lemma 4.9 essentially states that if the set of states {x [k] : v ∈ (Nin ∩R)∪{v }} is a subset of the local ball
j j i i
B(y [k],R [k])thentheintermediatestatez [k]isstillintheball. Thisisaconsequenceofusingthedistancefilter
i i i
(andaddingthemin-maxfilterinAlgorithm1doesnotdestroythisproperty),andthiswillplayanimportantrolein
provingtheconvergencetheorem.
Next,wewillestablishcertainquantitiesthatwillbeusefulforouranalysisoftheconvergenceregion. Forv ∈Rand
i
ϵ>0,define
C (ϵ):={x∈Rd :f (x)≤f (x∗)+ϵ}. (11)
i i i i
Forallv ∈R,sincethesetargminf (x)isbounded(byAssumption4.1),thereexistsδ (ϵ)∈(0,∞)suchthat
i i i
C (ϵ)⊆B(x∗,δ (ϵ)). (12)
i i i
Thefollowingproposition,whoseproofisprovidedinAppendixC,introducesanangleθ whichisanupperboundon
i
theanglebetweenthenegativeofthegradientoff atagivenpointxandthevectorx∗−x.
i i
Proposition4.10. IfAssumptions4.1and4.2holdthenforallv ∈Randϵ>0,thereexistsθ
(ϵ)∈(cid:2) 0,π(cid:1)
suchthat
i i 2
forallx∈/ C (ϵ)andg˜ (x)∈∂f (x),
i i i
∠(−g˜ (x), x∗−x)≤θ (ϵ). (13)
i i i
Beforestatingthemaintheorem,wedefine
R˜ :=∥x∗−y[∞]∥. (14)
i i
Furthermore,forallξ ∈R andϵ∈R ,wedefinetheconvergenceradius
≥0 >0
s∗(ξ,ϵ):= max(cid:8) max{R˜ secθ (ϵ),R˜ +δ (ϵ)}(cid:9) +ξ. (15)
i i i i
vi∈R
whereR˜ ,θ (ϵ)andδ (ϵ)aredefinedin(14),(13)and(12),respectively. Basedonthedefinitionabove,wereferto
i i i
B(y[∞],s∗(ξ,ϵ))astheconvergenceball.
Wenowcometothemainresultofthispaper,showingthatthestatesofalltheregularnodeswillconvergetoaballof
radiusinf s∗(0,ϵ)aroundtheauxiliarypointy[∞]underAlgorithm1andAlgorithm2.
ϵ>0
Theorem4.11(Convergence). SupposeAssumptions4.1-4.5hold. Furthermore:
• iftheregularagentsfollowAlgorithm1,supposethegraphG is((2d+1)F +1)-robust;
11ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
• otherwise,iftheregularagentsfollowAlgorithm2,supposethegraphG is(2F +1)-robust.
ThenregardlessoftheactionsofanyF-localsetofByzantineadversaries,forallv ∈R,wehave
i
limsup∥x [k]−y[∞]∥≤ inf s∗(0,ϵ).
i
k ϵ>0
Theproofofthetheoremrequiresseveraltechnicallemmasandpropositions,andthus,weprovideaproofsketchin
Section4.5andaformalproofintheappendix.
Thefollowingtheorem,whoseproofisprovidedinAppendixD,providespossiblelocationsofthetrueminimizerx∗,
whichisinfactinsidetheconvergenceregion,eveninthepresenceofadversarialagents.
Theorem 4.12. Let x∗ be a solution of Problem (2). If Assumptions 4.1 and 4.2 hold, then x∗ ∈
B(cid:0)
y[∞], inf
s∗(0,ϵ)(cid:1)
.
ϵ>0
Theorem4.11andTheorem4.12showthatbothAlgorithms1and2causeallregularnodestoconvergetoaregionthat
alsocontainsthetruesolution,regardlessoftheactionsofanyF-localsetofByzantineadversaries. Thesizeofthis
regionscaleswiththequantityinf s∗(0,ϵ). Looselyspeaking,thisquantitybecomessmallerastheminimizersof
ϵ>0
thelocalfunctionsoftheregularagentsgetclosertogether. Morespecifically,considerafixedϵ∈R . Ifthefunctions
>0
f (x)aretranslatedsothattheminimizersx∗ getclosertogether(i.e.,R˜ issmallerwhileθ (ϵ)andδ (ϵ)arefixed),
i i i i i
thens∗(0,ϵ)alsodecreases. Consequently,thestatex [k]isguaranteedtobecomeclosertothetrueminimizerx∗as
i
kgoestoinfinity. Figure1illustratesthekeyquantitiesoutlinedinthemaintheorems. Adetaileddiscussionofthe
convergenceregionisfurtherprovidedinSection5.4.
Figure1: Thelocalminimizersx∗andtheglobalminimizerx∗areshownintheplot. Theestimatedauxiliarypoint
i
y[∞] is in the rectangle formed by the local minimizers (Proposition 4.6) whereas the global minimizer x∗ is not
necessarily in the rectangle [16]. However, the ball centered at y[∞] with radius inf s∗(0,ϵ) contains both the
ϵ>0
supremumlimitofthestatevectorsx [k]andtheglobalminimizerx∗(Theorem4.11and4.12).
i
Wewouldliketohighlightthescalabilityofouralgorithmsintermsofbothcomputationalcomplexityandgraphro-
bustnessrequirements,specificallyinrelationtothenumberofdimensionsd. Algorithms1and2exhibitcomputational
complexitiesofO˜(d2)andO˜(d)operationsperagentperiteration,respectively. Adetailedcalculationisprovidedin
Section5.3. Furthermore,theyimposerobustnessrequirementsofO(d)andO(1)toachievetheconvergenceresult,as
demonstratedinTheorem4.11. WhileAlgorithm2ismorescalable,itlacksaconsensusguarantee,unlikeAlgorithm1
(refertoTheorem4.8). FurtherinsightsanddiscussionsonthistopicarepresentedinSection5.2andRemark6.
4.5 ProofSketchoftheConvergenceTheorem
WeworktowardstheproofofTheorem4.11inseveralsteps,whichweprovideanoverviewbelow. Theproofsofthe
intermediateresultspresentedinthissectionareprovidedinAppendicesE,FandG.
Forthesubsequentanalysis,wesupposethatthegraphG
• is((2d+1)F +1)-robustforAlgorithm1,and
• is(2F +1)-robustforAlgorithm2.
Furthermore,unlessstatedotherwise,wewillfixξ ∈R andϵ∈R ,andhidethedependenceofξandϵinδ (ϵ)
>0 >0 i
ands∗(ξ,ϵ)bydenotingthemasδ ands∗,respectively.
i
12ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
4.5.1 GradientUpdateStepAnalysis
First,weconsidertheupdatefromtheintermediatestates{z [k]} tothestates{x [k+1]} viathegradientstep
i R i R
(6)(i.e.,Line10). Inparticular,weprovidearelationshipbetween∥z [k]−y[∞]∥and∥x [k+1]−y[∞]∥forthree
i i
differentcases:
• ∥z [k]−y[∞]∥∈(cid:2) 0, max {R˜ +δ }(cid:3) ,
i vj∈R j j
• ∥z [k]−y[∞]∥∈(cid:0) max {R˜ +δ }, s∗(cid:3) ,
i vj∈R j j
• ∥z [k]−y[∞]∥∈(s∗,∞).
i
Thecorrespondingformalstatementsarepresentedasfollows.Lemma4.13belowessentiallysaysthatifkissufficiently
largeandz [k]∈B(y[∞],max {R˜ +δ }),thenafterapplyingthegradientupdate(6),thestatex [k+1]will
i vi∈R i i i
stillbeintheconvergenceball. Toestablishtheresult,letk∗ ∈Nbeatime-stepsuchthatη[k∗]≤ ξ.
1 1 L
Lemma4.13. SupposeAssumptions4.2-4.5hold.Forallv ∈Randk ≥k∗,ifz [k]∈B(cid:0) y[∞], max {R˜ +δ }(cid:1)
i 1 i vj∈R j j
thenx [k+1]∈B(y[∞],s∗).
i
Lemma4.14,basedonProposition4.10,analyzestherelationshipbetween∥z [k]−y[∞]∥and∥x [k+1]−y[∞]∥
i i
when∥z [k]−y[∞]∥>R˜ +δ . TheresultwillbeusedtoproveLemma4.15.
i i i
Forv ∈R,define∆ :[R˜ ,∞)×R →Rtobethefunction
i i i ≥0
(cid:113)
∆ (p,l):=2l(cid:0) p2−R˜2cosθ −R˜ sinθ (cid:1) −l2. (16)
i i i i i
Lemma4.14. SupposeAssumptions4.1,4.2,4.4and4.5hold. Forallv ∈Randk ∈N,if∥z [k]−y[∞]∥>R˜ +δ
i i i i
then
∥x [k+1]−y[∞]∥2 ≤∥z [k]−y[∞]∥2−∆ (∥z [k]−y[∞]∥, η[k]∥g [k]∥), (17)
i i i i i
whereg [k]∈Rdisdefinedin(6).
i
SimilartoLemma4.13,Lemma4.15belowstatesthatifkissufficientlylargeand∥z [k]−y[∞]∥∈(cid:0) max {R˜ +
δ },
s∗(cid:3)
thenbyapplyingthegradientstep(6),wehavethatthestatex
[k+1]isstii llintheconvergencebv ai∈ llR
.
i
i i
Tosimplifythenotations,define
(cid:113) (cid:113)
a± :=−R˜ sinθ ± (s∗)2−R˜2cos2θ and b :=2(cid:0) (s∗)2−R˜2cosθ −R˜ sinθ (cid:1) . (18)
i i i i i i i i i i
Letk∗ ∈Nbeatime-stepsuchthatη[k∗]≤ 1 min (cid:8) min{a+, b }(cid:9) .
2 2 L vi∈R i i
Lemma4.15. SupposeAssumptions4.1-4.5hold. Forallv ∈Randk ≥k∗,if∥z [k]−y[∞]∥∈(cid:0) max {R˜ +
δ }, s∗(cid:3) then∥x [k+1]−y[∞]∥∈[0, s∗].
i 2 i vj∈R j
j i
Thefollowinglemmaisusefulforboundingtheterm∆ appearedin(17)forthecasethat∥z [k]−y[∞]∥>s∗.
i i
Definethesetofagents
I [k]:={v ∈R:∥z [k]−y[∞]∥>s∗}, (19)
z i i
andletk∗ ∈Nbeatime-stepsuchthatη[k∗]≤ 1 min b .
3 3 2L vi∈R i
Lemma4.16. IfAssumptions4.1-4.5holdthenforallk ≥k∗andv ∈I [k],
3 i z
1
∆ (∥z [k]−y[∞]∥, η[k]∥g [k]∥)> b L η[k],
i i i 2 i i
where∆ andg [k]aredefinedin(16)and(6),respectively,andL := ϵ >0.
i i i δi(ϵ)
NotethatthequantityL definedabovecanbeinterpretedasalowerboundonasubgradientofthefunctionf (x)
i i
whenx∈/ C (ϵ).
i
Lemmas4.13-4.16collectivelyestablishthecompleterelationshipgoverningtheupdatefrom{z [k]} to{x [k+1]} ,
i R i R
whichwillbeusedtoproveLemma4.18.
13ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
4.5.2 BoundsonStatesofRegularAgents
Next, we consider the update from the states {x [k]} to the intermediate states {z [k]} via two filtering steps
i R i R
(Lines7and8)andtheweightedaveragestep(Line9). Inparticular,utilizingLemma4.9,wederivethefollowing
relationship.
Proposition4.17. IfAssumptions4.4and4.5hold,thenforallk ∈Nandv ∈R,itholdsthat
i
∥z [k]−y[∞]∥≤ max∥x [k]−y[∞]∥+2∥y [k]−y[∞]∥.
i j i
vj∈R
By combining the above inequality with the relationship between ∥z [k]−y[∞]∥ and ∥x [k +1]−y[∞]∥ from
i i
Lemmas4.13-4.16, andboundingthesecondtermontheRHS,∥y [k]−y[∞]∥, usingProposition4.6, weobtain
i
arelationshipbetween∥x [k+1]−y[∞]∥andmax ∥x [k]−y[∞]∥. Asaresult,wecanboundthedistance
i vj∈R j
max ∥x [k]−y[∞]∥byaparticularboundedsequencedefinedbelow.
vi∈R i
Definethetime-stepk ∈Nask :=max k∗. Recallthedefinitionofαandβ fromProposition4.6. Let
0 0 ℓ∈{1,2,3} ℓ
k (cid:88)0−1 k (cid:88)0−1
ϕ[k ]= max∥x [0]−y[∞]∥+2β e−αk+L η[k], (20)
0 i
vi∈R
k=0 k=0
anddefineasequence{ϕ[k]}∞ satisfyingtheupdaterule
k=k0
ϕ2[k+1]=max(cid:110) (s∗)2,(cid:0) ϕ[k]+2βe−αk(cid:1)2
−
1
η[k] min b L
(cid:111)
. (21)
2 vi∈R i i
Lemma4.18. SupposeAssumptions4.1-4.5hold. Forallk ≥k ,itholdsthat
0
max∥x [k]−y[∞]∥≤ϕ[k].
i
vi∈R
Furthermore,thereexistsϕ¯∈R suchthatforallk ≥k ,thesequenceϕ[k]canbeuniformlyboundedasϕ[k]<ϕ¯.
≥0 0
4.5.3 ConvergenceAnalysis
Finally,wewillutilizethefollowinglemmatofurtheranalyzethesequence{ϕ[k]}definedin(21).
Lemma4.19. Considerasequence{ηˆ[k]}∞ ⊂ R thatsatisfies(cid:80)∞ ηˆ[k] = ∞. Ifγ ∈ R ,γ ∈ R and
k=0 ≥0 k=0 1 ≥0 2 >0
λ∈(−1,1),thenthereisnosequence{u[k]}∞ ⊂R thatsatisfiestheupdaterule
k=0 ≥0
u2[k+1]=(u[k]+γ λk)2−γ ηˆ[k].
1 2
ByemployingLemmas4.18and4.19,Proposition4.20demonstratesthatanyrepulsionofthestatez [k]fromthe
i
convergenceballB(y[∞],s∗)duetoinconsistencyoftheestimatesoftheauxiliarypoint(Proposition4.6and4.17)
iscompensatedbythegradienttermpullingthestatex [k]totheconvergenceball. Consequently,thequantityϕ[k]
i
decreasesuntilitdoesnotexceeds∗. Inotherwords,thesequenceanalysisresultsin
max∥x [k]−y[∞]∥≤ϕ[k]≤s∗ (22)
i
vi∈R
forasufficientlylargetime-stepk. Thecrucialfinitetimeconvergenceresultisformallystatedasfollows.
Proposition4.20. SupposeAssumptions4.1-4.5hold. Then,thereexistsK ∈Nsuchthatforallv ∈Randk ≥K,
i
wehavex [k]∈B(y[∞],s∗).
i
Sincealltheprioranalysesvalidforallξ ∈R andϵ∈R ,theconvergenceresultinTheorem4.11followsfrom
>0 >0
takinginf andlimsup to(22).
ξ>0,ϵ>0 k
5 Discussion
5.1 FundamentalLimitation
One would ideally expect an algorithm to provide convergence to the exact minimizer of the sum of the regular
agents’functionswhentherearenoByzantineagentsinthenetwork. However,priorworks[14,10]haveestablished
afundamentallimitation, showingthatachievingsuchaguaranteeisnotpossibleunlessthesetoflocalfunctions
possesses a redundancy property, known as 2F-redundancy [24]. This limitation arises from the strong model of
14ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
Byzantineattacksconsidered,whereaByzantineagentcansubstitutethegivenlocalfunctionwithaforgedfunction
thatremainslegitimate. Consequently,detectingsuchsuspiciousbehaviorordeterminingthetotalnumberofByzantine
agents |A| in the network is not possible, as the Byzantine agent can follow the algorithm while influencing the
outcomeofdistributedoptimization(asdiscussedinRemark2). Inotherwords,insettingswhereByzantineagentsare
potentiallypresent(i.e.,F >0)andthereisnoknownredundancyamongthefunctions,achievingzerosteadystate
errorisimpossibleevenwhentherearenoByzantineagentsactuallypresent(i.e.,|A|=0)[14,10].
Ourwork,imposingonlymildassumptionsonthelocalfunctions,isconstrainedbythisfundamentallimit. Although
ourapproachcanrecoverthedistributedsubgradientmethod[32,8]whenselectingtheparameterF =0,intheworst
casescenario,thereisnowaytodeterminethenumberofByzantineagents. AsdiscussedinRemark5,inpractice,we
needtochoosetheparameterF inthedesignphase,i.e.,priortotheexecutionofthealgorithm. Thus,inourwork,
theparameterF servesasthemaximalnumberofByzantineagentsinasetofneighborsthatthedesignedsystemcan
tolerate,providingaconvergenceguarantee,asstatedinTheorem4.11.
It is crucial to acknowledge that the fundamental limit is well-established for distributed optimization problems.
However,thequestionofthedependenceofthesmallestsizeoftheconvergenceregionontheparameterscharacterizing
thefunctionclassremainsanimportantopenproblem[27].
5.2 RedundancyandGuaranteesTrade-off
AnappropriatenotionofnetworkredundancyisnecessaryforanyByzantineresilientoptimizationalgorithm[10];for
bothAlgorithm1andAlgorithm2,thisiscapturedbythecorrespondingrobustnessconditionsinTheorem4.11. In
particular,Algorithm1requiresthegraphtobe((2d+1)F+1)-robustsinceitimplementstwofilters(adistance-based
filter(Line7)andamin-maxfilter(Line8))whileAlgorithm2requiresthegraphtoonlybe(2F+1)-robustasaresult
ofonlyusingthedistance-basedfilter. Sinceeachofthesefilteringstepsdiscardsasetofstatevectors,therobustness
conditionallowsthegraphtoretainsomeflowofinformation. Thus,whileAlgorithm1requiressignificantlystronger
conditionsonthenetworktopology(i.e.,requiringtherobustnessparametertoscalelinearlywiththedimensionof
thefunctions),itprovidesthebenefitofguaranteeingconsensus. Algorithm2onlyrequirestherobustnessparameter
toscalewiththenumberofadversariesineachneighborhood,andthuscanbeusedforoptimizinghigh-dimensional
functionswithrelativelysparsenetworks,atthecostoflosingtheguaranteeonconsensus.
Remark6. Thelineardependenceoftheredundancyrequirementonthenumberofdimensionsdis,infact,typicalfor
resilientvectorconsensus(e.g.,see[36,37,38,39,40,41,42]);Thesurveypaper[43,Section5.3]providesadetailed
discussionofpapersthatrequirethisassumption. Despitesuchacondition/restrictionbeing“standard”intheliterature,
thelineargrowthinthenumberofneighborswiththedimensionofthestateisundesirable. Toaddressthedrawbackof
requiringhighredundancy,weprovideAlgorithm2whichisanalternativesolutionthatdoesnotdependonthenumber
ofdimensionsd;however,inthiscase,welosetheconsensusguaranteeunlikeAlgorithm1.
5.3 TimeComplexity
Supposethenetworkisr-robustandthenumberofin-neighbors|Nin|islinearlyproportionaltorforallv ∈V. For
i i
thedistance-basedfilter(Line7),eachregularagentv ∈ RcomputestheL2-normbetweenitsauxiliarystateand
i
in-neighborstatesandthenfindstheF agentsthatattainthemaximumvalue;thisproceduretakesO(dr)operations.
On the other hand, for the min-max filter (Line 8), each regular agent v ∈ R is required to sort the in-neighbor
i
states for each dimension which takes O(drlogr) operations. For Algorithms 1 and 2, the total computational
complexities for filtering process are O˜(d2) and O˜(d), respectively. Compared to the resilient vector consensus
literature[36,37,38,39,40,41,42],whichrequiresexponentialinthenumberofdimensionsdforcomputational
complexity,ouralgorithmshavesignificantlylowercomputationcosts.
5.4 ConvergenceBall
Intermsofthesizeoftheconvergenceball,itiscrucialtonotethattheconvergenceradiusdefinedin(15)remains
independentoftheLipschitzconstantL,thenumberofregularagents|R|(incontrasttotheresultin[26]),orthe
maximumnumberofneighboringByzantineagentsF. Instead,theradiushingessolelyonspecificcharacteristicsof
localfunctions: thelocationsoflocalminimizers(capturedbyR˜ ),sensitivity(capturedbyθ ),andthesizeoftheset
i √ i
oflocalminimizers(capturedbyδ ). However,thequantityR˜ canbeproportionalto dintheworstcaseasanalyzed
i i
in[27]. Aswewilldiscussnext,remarkably,thesensitivityθ definedinProposition4.10isintimatelylinkedtothe
i
conditionnumberofafunction.
For simplicity, we will omit the agent index subscript i in the subsequent analysis and assume x∗ = 0. Consider
i
a quadratic function f(x) = 1∥Ax∥2, where A ∈ Rd×d is a positive definite matrix. Now, we will examine the
2
15ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
quantitysup ∠(g(x),x)fromProposition4.10,whereg(x) = ∇f(x) = ATAx. Weaimtodemonstratethat
x̸=0
sec(cid:0) sup ∠(g(x),x)(cid:1) ≤ (∥A∥·∥A−1∥)2 := κ, where ∥ · ∥ denotes the induced matrix norm, and κ is the
x̸=0
conditionnumberassociatedwiththefunctionf [44]. Itisnoteworthythatthisinequality,withthereplacementof
sup ∠(g(x),x)bysup ∠(g(x),x−x ),holdsforthemoregeneralcaseoff(x) = 1∥A(x−x )+b∥2
x̸=0 x̸=x∗ ∗ 2 ∗
withx ∈Rdandb∈Rd.
∗
Toshowsuchresult,weproceedasfollows:
cos(cid:16) sup∠(g(x),x)(cid:17)
= inf
(cid:0) cos∠(g(x),x)(cid:1)
= inf
(cid:18) ⟨g(x),x⟩ (cid:19)
= inf
(cid:18) ∥Ax∥2
·
∥x∥ (cid:19)
x̸=0 x̸=0 x̸=0 ∥g(x)∥·∥x∥ x̸=0 ∥x∥2 ∥ATAx∥
≥(cid:18)
inf
∥Ax∥(cid:19)2(cid:18)
sup
∥ATAx∥(cid:19)−1
≥(cid:0) ∥A−1∥·∥A∥(cid:1)−2
.
x̸=0 ∥x∥ x̸=0 ∥x∥
In the last inequality, we utilize the properties that inf ∥Ax∥ = 1 due to the invertibility of A and
x̸=0 ∥x∥ ∥A−1∥
sup ∥ATAx∥ = ∥ATA∥ ≤ ∥A∥2 due to the sub-multiplicative property of induced matrix norm, and
x̸=0 ∥x∥
∥AT∥=∥A∥.
To get a sense of the convergence region, we consider univariate functions (i.e., the d = 1 case). To facilitate the
discussion,wedenotemin x∗andmax x∗byxandx,respectively. Supposethatthelocalminimizerx∗is
vi∈R i vi∈R i i
uniqueforallv ∈Rsothatthequantityδ definedin(12)canbechosenarbitrarilyclosetozeroforallv ∈R. Inthis
i i i
case,wehavethatforallv ∈ R,θ definedin(13)iszero. Therefore,theconvergenceradiuss∗ in(15)simplifies
i i
to max R˜ (where R˜ defined in (14)). In the best case, we can have y[∞] = 1(x+x) which results in the
vi∈R i i 2
convergenceregion[x,x]asderivedin[10]. Intheworstcase,(assumingnumericalerrorϵ∗inLine1iszero)wecan
havey[∞]=xorxwhichresultsintheconvergenceregion[2x−x,x]or[x,2x−x],respectively. Insuchworstcase,
theregionistwotimesbiggerthantheregionderivedin[10]. Theseresultsareduetoour“radiusanalysis"whichis
uniforminalldirectionsfromy[∞].
Remark7. Regardingtheconvergencerate,giventhegeneralconvex(possiblynon-smooth)natureoftheproblem,
achievingonlysublinearconvergenceistypicalincentralizedsettings[45]. Specifically,theanticipatedconvergence
ratemayalignwiththeO(cid:0)lo √gk(cid:1)
rateobservedin[8]fornon-faultydistributedcases. Whileourcurrentworkprovides
k
asymptoticanalysisduetoinherentchallenges,ourfutureendeavorsaimtoexploreexplicitconvergenceratesfora
broaderclassofByzantine-resilientdistributedoptimizationalgorithms.
5.5 MaximumTolerance
Basedontherobustnessconditionforeachalgorithmandaformulafrom[46],giventhenumberofagentsN inthe
completegraphandnumberofdimensionsfortheoptimizationvariablesd,theupperboundonthenumberoflocal
ByzantineagentsF suchthatthecorrespondingguaranteesstillhold,isasfollows:
(cid:106) (cid:107)
• F = N−1 forAlgorithm1,and
2(2d+1)
• F
=(cid:4)1(N −1)(cid:5)
forAlgorithm2.
4
From a practical perspective, the robustness property demonstrates a natural trade-off for the system designer. A
networkthathasastrongerrobustnesspropertycantoleratemoreadversaries,butcanalsoinducemorecosts.
5.6 ImportanceofMainStatesComputation
If we simply implement a resilient consensus protocol on local minimizers similar to the auxiliary states, y [k],
i
computation(inLines11-12)andremovethemainstates,x [k],computation(inLines7-9),wewouldobtainthatthe
i
statesoftheregularagentsconvergetothehyper-rectangleformedbythelocalminimizers(forresilientcomponent-
wiseconsensusalgorithms[47]),ortheconvexhullofthelocalminimizers(forresilientvectorconsensusalgorithms
[40,48]). Eventhoughusingaresilientconsensusprotocolseemstobeagoodmethodforthesingledimensioncase
sincetheresilientdistributedoptimizationalgorithmalsopushesthestatesoftheregularagentstosuchsets[13,10](and
theyareidenticalinthiscase),itmightnotgiveadesiredresultforthemulti-dimensionalcase. First,itispossiblethat
theminimizerofthesumliesoutsideboththehyper-rectangleandconvexhull[16,49]asshowninFigure1. Second,
usingonlyaresilientconsensusprotocol,oneignoresthegradientinformationwhichsteerstheregularagents’states
tothetrueminimizer. Third,weempiricallyshowinSection6thatimplementingaresilientdistributedoptimization
algorithm(especiallyAlgorithm1)usuallygivesbetterresults(comparedtothequalityofthesolutionprovidedby
16ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
directlyusingtheauxiliarypoint,whichwasobtainedbyrunningaresilientconsensusprotocolonthelocalminimizers)
intermsofbothoptimalitygapanddistancetotheglobalminimizer.
5.7 ImportanceofAuxiliaryStatesComputation
Essentially,whenthemainstatesofregularagentsaresignificantlyfarawayfromtheirlocalminimizersx∗,these
i
minimizers tend to form a cluster from the perspective of a regular agent v . In addition, building on prior works
i
[16,49,50],weknowthatthetrueoptimalsolutionx∗(whichistheminimizerofthefunctionsum)cannotbelocated
toofarawayfromthiscluster. Thus,theauxiliarystatesy ,guaranteedtobeinsidethecluster(inL -sense)asshown
i 1
inProposition4.6,actasvaluablereferencesforprovidingadirectionalsensetoregularagentsv intheirpursuitofthe
i
trueminimizerx∗. Byourdesign,thedistancefilterinAlgorithm1andAlgorithm2assumestheroleofaguiding
mechanismbyeliminatingextremestatesthatpulltheoverallstateawayfromthecluster.
Fromatechnicalstandpoint,inthemulti-dimensionalcase,relyingsolelyonresilientconsensusforthemainstatesx
i
andtheupdateusingasubgradientg withrespecttothelocalfunctionf maynotsufficetoensureaconvergence
i i
guarantee.Intheworstcase,resilientconsensuscouldleadtoastatefurtherawayfromthecluster,especiallyconsidering
√
thatthestrengthofthisdivergenceduetoByzantineagentscanbeproportionalto d,wheredistheproblemdimension.
Eventhoughfollowingthesubgradientg usuallymitigatesthedivergence,itmightnotbesufficientforguaranteed
i
convergenceinsuchworstcases. Thus,ourintroduceddistance-basedfilterusingalocalauxiliarystateplaysacrucial
roleinfurtherreducingtheseverityofthedivergence, allowingustoachieveaconvergenceguaranteeundermild
assumptions.
6 NumericalExperiment
WenowprovidetwonumericalexperimentstoillustrateAlgorithm1andAlgorithm2. Inthefirstexperiment,we
generatequadraticfunctionsforthelocalobjectivefunctions. Usingthesefunctions,wedemonstratetheperformance
(e.g.,optimalitygaps,distancestotheglobalminimizer)ofouralgorithms. Wealsocomparetheoptimalitygapsof
thefunctionvalueobtainedusingthestatesx [k]andthevalueobtainedusingtheauxiliarypointsy [k],andplotthe
i i
trajectoriesofthestatesofasubsetofregularnodes. Inthesecondexperiment,wedemonstratetheperformanceof
ouralgorithmonamachinelearningtask(banknoteauthenticationtask). Specifically,wecomparetheaccuracyofthe
modelsobtainedfromouralgorithm(resilientdistributedmodel)andthatofacentralizedmodel.
6.1 SyntheticQuadraticFunctions
PreliminarySettings
• MainParameters: Wesetthenumberofnodestoben=25andthedimensionofeachfunctiontobed=2.
• Adversary Parameters: We consider the F-local model, and set F = 2 for Algorithm 1 and F = 5 for
Algorithm2.
NetworkSettings
• Topology Generation: We construct an 11-robust graph on n = 25 nodes following the approach from
[46,29]. Thisgraphcantolerateupto2localadversariesforAlgorithm1,andupto5localadversariesfor
Algorithm2accordingtoTheorem4.11. Notethatthesamegraphisusedtoperformnumericalexperiments
forbothAlgorithms1and2.
Adversaries’Strategy
• AdversarialNodes: WeconstructthesetofadversarialnodesAbyrandomlychoosingnodesinV sothatthe
setofadversarialnodesformaF-localset. Notethatingeneral,constructingAdependsonthetopologyof
thenetwork. Inourexperiment,wehaveA={v ,v }forAlgorithm1andA={v ,v ,v ,v ,v ,v }
9 16 5 11 12 17 22 24
forAlgorithm2.
• AdversarialValuesTransmitted: Here,weuseasophisticatedapproachratherthansimplychoosingthe
transmittedvaluesatrandom.Supposev isanadversarynodeandv isaregularnodewhichisanout-neighbor
s i
ofv ,i.e.,v ∈Nin. First,considerthestateofnodesinthenetworkattime-stepk. Theadversarialnodev
s s i s
usesanoracletodeterminetheregioninthestatespacefortheregularnodev inwhichiftheadversarialnode
i
selectsthetransmittedvaluetobeoutsidetheregionthenthevaluewillbediscardedbythatregularagentv .
i
17ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
Then,v choosesx [k](theforgedstatesentfromv tov attimek)sothatitisinthesaferegionandfar
s s→i s i
fromtheglobalminimizer. Inthisway,theadversaries’valueswillnotbediscardedandalsotrytopreventthe
regularnodesfromgettingclosetotheminimizer. Similarly,fortheauxiliarypointupdate,theadversarial
nodev usesanoracletodeterminethesaferegionintheauxiliarypoint’sspacefortheregularnodev . Since
s i
thesaferegionisahyper-rectangleingeneral,v choosesy [k](theforgedestimatedauxiliarypointsent
s s→i
fromv tov attimek)tobenearacorner(chosenrandomly)ofthehyper-rectangle.
s i
ObjectiveFunctionsSettings
• LocalFunctions: Forv ∈V,wesetthelocalobjectivefunctionsf :Rd →Rtobe
i i
1
f (x)= xTQ x+bTx,
i 2 i i
whereQ ∈ S+ andb ∈ Rd arechosenrandomly. Notethatthesamelocalfunctionsareusedtoperform
i d i
numericalexperimentsforbothAlgorithms1and2.
• Global Objective Function: According to our objective (2), we then have the global objective function
f :Rd →Rasfollows:
f(x)=
1 (cid:16)1 xT(cid:0) (cid:88)
Q
(cid:1) x+(cid:0) (cid:88)
b
(cid:1)T x(cid:17)
,
|R| 2 i i
vi∈R vi∈R
wherethesetofregularnodesR=V \A.
AlgorithmSettings
• Initialization: Foreachregularnodev ∈R,wecomputetheexactminimizerx∗ =−QTb anduseitasthe
i i i i
initialstateandauxiliarypointofv assuggestedinLine1-2ofAlgorithm1.
i
• WeightsSelection: Foreachtime-stepk ∈ Nandregularnodev ∈ R,werandomlychoosetheweights
i
w [k],w(ℓ) [k]sothattheyfollowthedescriptionofLine9andLine12,andAssumption4.5.
x,ij y,ij
• Step-sizeSelection: Wechoosethestep-sizeschedule(inLine11ofAlgorithm1)tobeη[k]= 1 .
k+1
• GradientNormBound: WechoosetheupperboundofthegradientnormtobeL=105. Ifthenormexceeds
thebound,wescalethegradientdownsothatitsnormisequaltoL,i.e.,
(cid:40)
∇f (z [k]) if ∥∇f (z [k])∥≤L,
i i i i
g [k]=
i L ·∇f (z [k]) otherwise.
∥∇fi(zi[k])∥ i i
SimulationSettingsandResults
• TimeHorizon: WesetthetimehorizonofoursimulationstobeK =300(startingfromk =0).
• ExperimentsDetail: ForbothAlgorithms1and2,wefixthegraph,localfunctions,andstep-sizeschedule.
However, since the set of adversaries are different, the global objective functions, and hence the global
minimizers are different. For each algorithm, we run the experiment 10 times setting the same states
initializationacrosstheruns. Theresultsfromtherunsaredifferentduetotherandomnessintheadversaries’
strategy.
• PerformanceMetrics: Weexaminetheperformanceofouralgorithmsbyconsideringtheoptimalitygaps
(Figure2a),distancestotheglobalminimizer(Figure2b),andtrajectoriesofrandomlyselectedregularagents
(Figure2c).
• Algorithm1’sResults: Thelinescorrespondingtotheoptimalitygapanddistancetotheglobalminimizer
evaluatedusingauxiliarypointsarealmosthorizontalsincetheconvergencetoconsensusisveryfast. However,
one can see that the optimality gap and distance to the minimizer obtained from the regular states are
significantlysmallerthanthatfromtheauxiliarypointsduetotheuseofgradientinformation(Line10)and
extremestatesfiltering(Line8)intheregularstateupdate. Inparticular,atk =300,theoptimalitygapand
distancetotheglobalminimizerattheregularstates’averageareonlyabout0.030and0.206,respectively.
Moreover,thestatetrajectoriesconvergetogetherandstayclosetotheglobalminimizereveninthepresenceof
sophisticatedadversaries. Notethat,fromourobservations,Algorithm1yieldsbetterresultsthanAlgorithm2
giventhesamesettings.
18ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
(a)Theoptimalitygapevaluatedattheaverageoftheregularnodes’statesf(x¯)−f∗averagedover10runs
(blueline),andtheoptimalitygapevaluatedattheaverageoftheregularnodes’auxiliarypointsf(y¯)−f∗
averagedover10runs(redline).
(b)Thedistancebetweentheaverageoftheregularnodes’statesandtheglobalminimizer∥x¯ −x∗∥
averagedover10runs(blueline),andthedistancebetweentheaverageoftheregularnodes’auxiliary
pointsandtheglobalminimizer∥y¯−x∗∥averagedover10runs(redline).
(c)Thetrajectoryofthestatesofasubsetoftheregularnodes.Differentcolorsofthetrajectoryrepresent
differentregularagentsv inthenetwork.
i
Figure2: Theplotsshowtheresultsobtainedfrom(left)Algorithm1and(right)Algorithm2. Inthefirstfourplots,the
shadedregionsrepresent+1/-1standarddeviationfromthemean. Inthelasttwoplots,thecontourlinesshowthelevel
setsoftheglobalobjectivefunction(inthiscase,aquadraticfunction)andthereddotsrepresenttheglobalminimizer.
• Algorithm 2’s Results: The optimality gaps and distances to the global minimizer evaluated using the
statesareslightlybetterthanthevaluesobtainedusingtheauxiliarypoints,andthestatetrajectoriesremain
reasonablyclosetotheglobalminimizershowingthatthealgorithmcantolerateF = 5localadversaries
(whichismorethanAlgorithm1). Interestingly,thestatetrajectoriesseemtoconvergetogethereventhough
theconsensusguaranteeislackingduetotheabsenceofthedistance-basedfilter.
19ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
6.2 BanknoteAuthenticationusingRegularizedLogisticRegression
DatasetInformation6
• Description: Thedatawereextractedfromimagesthatweretakenfromgenuineandforgedbanknote-like
specimens.
• DataPoints: Thetotalnumberofdatais1,372.
• Features: The dataset consists of four features: (1) the variance of a wavelet transformed image, (2) the
skewnessofawavelettransformedimage,(3)thecurtosisofawavelettransformedimage,and(4)theentropy
ofanimage.
• Labels: Therearetwoclasses: ‘0’(genuine)and‘1’(counterfeit).
PreliminarySettings
• MainParameters: Wesetthenumberofnodestoben=50. Sincetherearefourfeatures,thedimensionof
thestatesisd=5(oneforeachfeatureandtheotheroneforthebias).
• AdversarialParameters: WeusetheF-localmodelwithF =2.
• DatasetPartitioning: Werandomlypartitionthedatasetintothreechunks: 1,000trainingdatapoints,186
validationdatapoints, and186testdatapoints. Wethendistributethetrainingdatasettothenodesinthe
networkequally. Thus,eachnodecontainsm=20trainingdatapoints.
NetworkandWeightsSettings
We construct the network and corresponding weight matrix using the same approach as in the synthetic quadratic
functionscase.
Adversaries’Strategy
WechoosethesetofadversarialnodesAandadversarialvaluestransmissionstrategyusingthesamemethodasinthe
syntheticquadraticfunctionscase.
ObjectiveFunctionsSettings
• Notations: Letx ∈Rd−1bethefeaturevectorofthej-thdatapointsatnodev ∈V,andY ∈{0,1}be
ij i ij
thecorrespondinglabel. Weletx˜ =(cid:2) xT 1(cid:3)T toaccountforthebiasterm.
ij ij
• Local Functions: Since this is a classification task, we choose the logistic regression model with L -
2
regularizationinwhichitslossfunctionisstronglyconvex. Forv ∈V,wesetthelocalobjectivefunctions
i
f :Rd →Rtobe
i
m
f (W)=|R|(cid:88) log(cid:0) exp(−Y x˜TW)+1(cid:1) + ς ∥W∥2,
i ij ij 2
j=1
wherethesetofregularnodesR=V \Aandς ∈R istheregularizationparameterwhichwillbechosen
>0
later.
• Global Objective Function: According to our objective (2), we then have the global objective function
f :Rd →Rasfollows:
m
f(W)= (cid:88) (cid:88) log(cid:0) exp(−Y x˜TW)+1(cid:1) + ς ∥W∥2.
ij ij 2
vi∈Rj=1
• RegularizationParameterSelection: Weconsiderς ∈{10−4,10−3,...,105}. Wetrainour(centralized)
logisticmodelusingtheglobalobjectivefunctionaboveforeachvalueofς andthenweselectthevalueofς
thatgivesthebestvalidationaccuracy.
AlgorithmSettings
• Initialization: As suggested in Line 1 of Algorithm 1, we numerically find the minimizer of the local
functionsusingthedefaultoptimizerofsklearn.linear_model.LogisticRegression. Then,weusethe
minimizerofeachregularnodetobetheinitialstateandauxiliarypointasinLine2.
6https://archive.ics.uci.edu/ml/datasets/banknote+authentication
20ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
1stRun 2ndRun 3rdRun 4thRun 5thRun
v ∈A 9,23 9,15 10,11 5,29 24,47
i
ς 1.0 1.0 10 1.0 1.0
η 1 1 2 4 1
0
Train(C) 99.40 99.20 99.00 98.90 99.10
Train(D) 98.10 97.90 97.70 98.30 98.00
Train(MIN) 97.80 97.60 97.50 98.30 97.70
Test(C) 99.46 98.39 99.46 99.46 98.92
Test(D) 97.85 95.70 98.92 97.85 98.39
Test(MIN) 97.85 95.70 98.92 97.85 97.85
Table1: Training/TestAccuracyofCentralized(C),Distributed(D)ModelsandMinimumamongRegularAgents’
Models(MIN)foreachRunofBanknoteAuthenticationTask
Themethodologyofstep-sizeselectionandgradientnormboundisthesameasinthesyntheticquadraticfunctions
case.
SimulationSettingsandResults
• Benchmark: Weevaluatetheperformance(accuracy)ofthe(centralized)logisticmodelwiththeselected
regularizationparameter,ς.
• Time Horizon: We set the time horizon of our simulations of our distributed algorithm to be K = 200
(startingfromk =0).
• Simulation: WerunthesimulationsofAlgorithm1byvaryingtheparameterη from−2to4withincreasing
0
stepof1. Weevaluatetheperformanceofeachmodel(i.e.,eachη )byconsideringtheaccuracyobtainedby
0
usingthestateW¯ [K]= 1 (cid:80) W [K]foreachη andthevalidationdata.Then,weselecttheparameter
|R| vi∈R i 0
η which provides the best accuracy. Finally, with the selected value of η , we evaluate the performance
0 0
(accuracy)ofthecorrespondingmodelwiththetestdata.
• Result: We repeat the whole process 5 times. In other words, each run uses different realization of data
partitioning(hence,differentlocalfunctionsandglobalfunction),networktopology,andadversariesset. The
resultofeachrunisshowninTable1. Thefirstthreerowsshowtheadversariesset,regularizationparameter
andstep-sizeparameterofeachrun. Thenext(resp. last)threerowsshowthetraining(resp. test)accuracy
of the centralized model, distributed model evaluated at W¯ [K] = 1 (cid:80) W [K], and the minimum
|R| vi∈R i
accuracyamongthelocalmodelofregularnodesevaluatedatitsownstateW [K]. Wecanseethatdespite
i
thepresenceofadversarieswithsophisticatedbehavior,theperformanceofouralgorithmisjustslightlylower
thanthecentralizedmodel’sperformanceforthistask.
7 ConclusionandFuturework
Inthispaper,weconsideredthedistributedoptimizationprobleminthepresenceofByzantineagents. Wedeveloped
two resilient distributed optimization algorithms for multi-dimensional functions. The key improvement over our
previousworkin[28]isthatthealgorithmsproposedinthispaperdonotrequireafixedauxiliarypointtobecomputed
inadvance(whichwillnothappenunderfinitetimeingeneral). Ouralgorithmshavelowcomplexityandeachregular
nodeonlyneedslocalinformationtoexecutethesteps. Algorithm1(withthemin-maxstatefilter),whichrequiresmore
networkredundancy,guaranteesthattheregularstatescanasymptoticallyreachconsensusandenteraboundedregion
thatcontainstheglobalminimizer,irrespectiveoftheactionsofByzantineagents. Ontheotherhand,Algorithm2
(without the min-max filter) has a more relaxed condition on the network topology and can guarantee asymptotic
convergencetothesameregion,butcannotguaranteeconsensus. Forbothalgorithms,weexplicitlycharacterizedthe
sizeoftheconvergenceregion,andshowedthroughsimulationsthatAlgorithm1appearstoyieldresultsthatarecloser
tooptimal,ascomparedtoAlgorithm2.
As noted earlier, the consensus guarantee for Algorithm 1 requires linear scaling of network robustness with the
dimensionofthelocalfunctions,whichcanbelimitinginpractice. Thisseemstobeacommonchallengeforresilient
consensus-basedalgorithmsinsystemswithmulti-dimensionalstates,e.g.,[51,24,48]. Findingarelaxedconditionon
thenetworktopologyforhigh-dimensionalresilientdistributedoptimizationproblems(withguaranteedconsensus)
wouldbearichareaforfutureresearch.
21ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
References
[1] JohnTsitsiklis,DimitriBertsekas,andMichaelAthans. Distributedasynchronousdeterministicandstochastic
gradientoptimizationalgorithms. IEEEtransactionsonautomaticcontrol,31(9):803–812,1986.
[2] LinXiaoandStephenBoyd. Optimalscalingofagradientmethodfordistributedresourceallocation. Journalof
optimizationtheoryandapplications,129(3):469–488,2006.
[3] JingWangandNicolaElia. Acontrolperspectiveforcentralizedanddistributedconvexoptimization. In2011
50thIEEEconferenceondecisionandcontrolandEuropeancontrolconference,pages3800–3805.IEEE,2011.
[4] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al. Distributed optimization and
statisticallearningviathealternatingdirectionmethodofmultipliers. FoundationsandTrends®inMachine
learning,3(1):1–122,2011.
[5] MarkEisen,AryanMokhtari,andAlejandroRibeiro. Decentralizedquasi-newtonmethods. IEEETransactions
onSignalProcessing,65(10):2613–2628,2017.
[6] Ran Xin, Chenguang Xi, and Usman A Khan. Frost—fast row-stochastic optimization with uncoordinated
step-sizes. EURASIPJournalonAdvancesinSignalProcessing,2019(1):1,2019.
[7] MinghuiZhuandSoniaMartínez. Ondistributedconvexoptimizationunderinequalityandequalityconstraints.
IEEETransactionsonAutomaticControl,57(1):151–164,2011.
[8] AngeliaNedic´andAlexOlshevsky.Distributedoptimizationovertime-varyingdirectedgraphs.IEEETransactions
onAutomaticControl,60(3):601–615,2014.
[9] Jinshan Zeng and Wotao Yin. On nonconvex decentralized gradient descent. IEEE Transactions on signal
processing,66(11):2834–2848,2018.
[10] ShreyasSundaramandBahmanGharesifard. Distributedoptimizationunderadversarialnodes. IEEETransactions
onAutomaticControl,64(3):1063–1076,2018.
[11] NikhilRavi,AnnaScaglione,andAngeliaNedic´. Acaseofdistributedoptimizationinadversarialenvironment.
InIEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing,pages5252–5256,2019.
[12] SissiXiaoxiaoWu,Hoi-ToWai,AnnaScaglione,AngeliaNedic´,andAmirLeshem. Datainjectionattackon
decentralizedoptimization. InIEEEInt.Conf.onAcoustics,SpeechandSignalProcessing,pages3644–3648,
2018.
[13] LiliSuandNitinVaidya. Byzantinemulti-agentoptimization: Parti. arXivpreprintarXiv:1506.04681,2015.
[14] LiliSuandNitinHVaidya. Fault-tolerantmulti-agentoptimization: optimaliterativedistributedalgorithms. In
ACMSymposiumonPrinciplesofDistributedComputing,pages425–434,2016.
[15] Chengcheng Zhao, Jianping He, and Qing-Guo Wang. Resilient distributed optimization algorithm against
adversarialattacks. IEEETransactionsonAutomaticControl,65(10):4308–4315,2019.
[16] KananartKuwaranancharoenandShreyasSundaram. Onthelocationoftheminimizerofthesumoftwostrongly
convexfunctions. InIEEEConferenceonDecisionandControl(CDC),pages1769–1774,2018.
[17] Nirupam Gupta and Nitin H Vaidya. Byzantine fault tolerant distributed linear regression. arXiv preprint
arXiv:1903.08752,2019.
[18] PevaBlanchard,RachidGuerraoui,JulienStainer,etal. Machinelearningwithadversaries: Byzantinetolerant
gradientdescent. InAdvancesinNeuralInformationProcessingSystems,pages119–129,2017.
[19] Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui. Robust aggregation for federated learning. IEEE
TransactionsonSignalProcessing,70:1142–1154,2022.
[20] ZhixiongYangandWaheedUBajwa. Byrdie: Byzantine-resilientdistributedcoordinatedescentfordecentralized
learning. IEEETransactionsonSignalandInformationProcessingoverNetworks,2019.
[21] ChengFang,ZhixiongYang,andWaheedUBajwa. Bridge: Byzantine-resilientdecentralizedgradientdescent.
IEEETransactionsonSignalandInformationProcessingoverNetworks,8:610–626,2022.
[22] AhmedRoushdyElkordy,SauravPrakash,andSalmanAvestimehr. Basil:AfastandByzantine-resilientapproach
fordecentralizedtraining. IEEEJournalonSelectedAreasinCommunications,40(9):2694–2716,2022.
[23] ShangweiGuo,TianweiZhang,XiaofeiXie,LeiMa,TaoXiang,andYangLiu. Towardsbyzantine-resilient
learningindecentralizedsystems. arXivpreprintarXiv:2002.08569,2020.
[24] NirupamGupta,ThinhTDoan,andNitinHVaidya. Byzantinefault-toleranceindecentralizedoptimizationunder
2f-redundancy. In2021AmericanControlConference(ACC),pages3632–3637.IEEE,2021.
22ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
[25] Nikhil Ravi and Anna Scaglione. Detection and isolation of adversaries in decentralized optimization for
non-stronglyconvexobjectives. IFAC-PapersOnLine,52(20):381–386,2019.
[26] ZhaoxianWu,TianyiChen,andQingLing. Byzantine-resilientdecentralizedstochasticoptimizationwithrobust
aggregationrules. IEEETransactionsonSignalProcessing,2023.
[27] Kananart Kuwaranancharoen and Shreyas Sundaram. On the geometric convergence of byzantine-resilient
distributedoptimizationalgorithms. arXivpreprintarXiv:2305.10810,2023.
[28] KananartKuwaranancharoen,LeiXin,andShreyasSundaram. Byzantine-resilientdistributedoptimizationof
multi-dimensionalfunctions. In2020AmericanControlConference(ACC),pages4399–4404.IEEE,2020.
[29] HeathJLeBlanc,HaotianZhang,XenofonKoutsoukos,andShreyasSundaram. Resilientasymptoticconsensus
inrobustnetworks. IEEEJournalonSelectedAreasinCommunications,31(4):766–781,2013.
[30] LiliSuandNitinHVaidya. Byzantine-resilientmultiagentoptimization. IEEETransactionsonAutomaticControl,
66(5):2227–2233,2020.
[31] AlistairSinclair. Improvedboundsformixingratesofmarkovchainsandmulticommodityflow. Combinatorics,
probabilityandComputing,1(4):351–370,1992.
[32] Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE
TransactionsonAutomaticControl,54(1):48–61,2009.
[33] JohnCDuchi,AlekhAgarwal,andMartinJWainwright.Dualaveragingfordistributedoptimization:Convergence
analysisandnetworkscaling. IEEETransactionsonAutomaticcontrol,57(3):592–606,2011.
[34] DusanJakovetic´,JoaoXavier,andJoséMFMoura. Fastdistributedgradientmethods. IEEETransactionson
AutomaticControl,59(5):1131–1146,2014.
[35] NancyALynch. Distributedalgorithms. Elsevier,1996.
[36] HelgeTverberg. Ageneralizationofradon’stheorem. JournaloftheLondonMathematicalSociety,1(1):123–128,
1966.
[37] JohnRReay. Anextensionofradon’stheorem. IllinoisJournalofMathematics,12(2):184–189,1968.
[38] Nitin H Vaidya. Iterative byzantine vector consensus in incomplete graphs. In Distributed Computing and
Networking: 15thInternationalConference,ICDCN2014,Coimbatore,India,January4-7,2014.Proceedings15,
pages14–28.Springer,2014.
[39] ZhuolunXiangandNitinHVaidya. Briefannouncement: Relaxedbyzantinevectorconsensus. InProceedingsof
the28thACMSymposiumonParallelisminAlgorithmsandArchitectures,pages401–403,2016.
[40] HyongjuParkandSethAHutchinson. Fault-tolerantrendezvousofmultirobotsystems. IEEEtransactionson
robotics,33(3):565–582,2017.
[41] WaseemAbbas,MudassirShabbir,JianiLi,andXenofonKoutsoukos. Interplaybetweenresilienceandaccuracy
inresilientvectorconsensusinmulti-agentnetworks. In202059thIEEEConferenceonDecisionandControl
(CDC),pages3127–3132.IEEE,2020.
[42] JiaqiYan,YilinMo,XiuxianLi,andChangyunWen. A“safekernel”approachforresilientmulti-dimensional
consensus. IFAC-PapersOnLine,53(2):2507–2512,2020.
[43] MohammadPirani,AritraMitra,andShreyasSundaram. Asurveyofgraph-theoreticapproachesforanalyzing
theresilienceofnetworkedcontrolsystems. arXivpreprintarXiv:2205.12498,2022.
[44] David H Gutman and Javier F Pena. The condition number of a function relative to a set. Mathematical
Programming,188:255–294,2021.
[45] YuriiNesterov. Introductorylecturesonconvexoptimization: Abasiccourse,volume87. SpringerScience&
BusinessMedia,2003.
[46] LuisGuerrero-Bonilla,AmandaProrok,andVijayKumar. Formationsforresilientrobotteams. IEEERobotics
andAutomationLetters,2(2):841–848,2017.
[47] DavidSaldana,AmandaProrok,ShreyasSundaram,MarioFMCampos,andVijayKumar. Resilientconsensus
fortime-varyingnetworksofdynamicagents. In2017Americancontrolconference(ACC),pages252–258.IEEE,
2017.
[48] WaseemAbbas,MudassirShabbir,JianiLi,andXenofonKoutsoukos. Resilientdistributedvectorconsensus
usingcenterpoint. Automatica,136:110046,2022.
[49] KananartKuwaranancharoenandShreyasSundaram. Onthesetofpossibleminimizersofasumofknownand
unknownfunctions. In2020AmericanControlConference(ACC),pages106–111.IEEE,2020.
23ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
[50] KananartKuwaranancharoenandShreyasSundaram. Theminimizerofthesumoftwostronglyconvexfunctions.
arXivpreprintarXiv:2305.13134,2023.
[51] JiaqiYan,YilinMo,XiuxianLi,LantaoXing,andChangyunWen. Resilientvectorconsensus: Anevent-based
approach. In2020IEEE16thInternationalConferenceonControl&Automation(ICCA),pages889–894.IEEE,
2020.
[52] BorisSMordukhovichandNguyenMauNam. Aneasypathtoconvexanalysisandapplications. Synthesis
LecturesonMathematicsandStatistics,6(2):1–218,2013.
[53] Diego Castano, Vehbi E Paksoy, and Fuzhen Zhang. Angles, triangle inequalities, correlation matrices and
metric-preservingandsubadditivefunctions. LinearAlgebraanditsApplications,491:15–29,2016.
24ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
A AdditionalLemma
WeprovidealemmawhichisutilizedintheproofofTheorem4.12andLemma4.14.
LemmaA.1. Forgivenxˆ ∈RdandR≥0,ifx∈/ B(xˆ,R)then
(cid:16) R (cid:17)
max ∠(x−y, x−xˆ)=arcsin .
y∈B(xˆ,R) ∥x−xˆ∥
Proof. Sincetheangleismeasuredwithrespecttothevectorx−xˆ,considerany2-Dplanespassedthroughthecenter
xˆ andthepointx. Sincetheplanespassthroughxˆ,theintersectionsbetweenoftheballB(xˆ,R)andtheplanesare
greatcirclesofradiusR. Thus,alloftheintersectionsgeneratedfromeachplaneareidenticalandwecanconsider
theangleusingagreatcircleinsteadoftheball. Fromgeometry,themaximumangleϕ = ∠(x−y∗, x−xˆ)only
occurswhentheraystartingfromthepointxtouchesthecircleatpointy∗. Therefore,∠(xˆ −y∗,x−y∗)= π and
2
∥xˆ −y∗∥=R. Wehave
∥xˆ −y∗∥ R
sinϕ= = ,
∥xˆ −x∥ ∥xˆ −x∥
andtheresultfollows.
B ProofofProposition4.6
ProofofProposition4.6. ForanyS ⊆V,ζ ∈Randk¯,k ∈Nwithk¯ ≥k,definethesets
J(ℓ)(S,k¯,k,ζ):={v ∈S :y(ℓ)[k¯]>M(ℓ)[k]−ζ},
M i i
J(ℓ)(S,k¯,k,ζ):={v ∈S :y(ℓ)[k¯]<m(ℓ)[k]+ζ}.
m i i
Consider a fixed ℓ ∈ {1,2,...,d} and any time-step k ∈ N. Define ζ(ℓ) = 1D(ℓ)[k]. Note that the set
0 2
J(ℓ)(V,k,k,ζ(ℓ))∩J(ℓ)(V,k,k,ζ(ℓ))=∅.
M 0 m 0
Bythedefinitionofthesesets,whenD(ℓ)[k]>0,thesetsJ(ℓ)(R,k,k,ζ(ℓ))̸=∅andJ(ℓ)(R,k,k,ζ(ℓ))̸=∅. Since
M 0 m 0
thegraphis(2F +1)-robust, atleastoneofJ(ℓ)(R,k,k,ζ(ℓ))orJ(ℓ)(R,k,k,ζ(ℓ))is(2F +1)-reachablewhich
M 0 m 0
meansthatatleastoneofthemcontainsavertexthathasatleast2F +1in-neighborsfromoutsideit.
Ifsuchanodev isinJ(ℓ)(R,k,k,ζ(ℓ)),weclaimthatintheupdate,v cannotusethevaluesstrictlygreaterthan
i M 0 i
M(ℓ)[k]anditusesatleastonevaluefromV \J(ℓ)(V,k,k,ζ(ℓ)). Toshowthefirstclaim,notethatthenodesthat
M 0
possessthevalue(inℓ-component)greaterthanM(ℓ)[k]mustbeByzantineagentsbythedefinitionofM(ℓ)[k]. Since
theregularnodev discardsuptoF-highestvaluesandthereareatmostF Byzantinein-neighbors,theByzantineagents
i
thatholdthevaluegreaterthanM(ℓ)[k]mustbediscarded. Toshowthesecondclaim,letS(ℓ)[k]=J(ℓ)(A,k,k,ζ(ℓ))
1 M 0
andS(ℓ)[k]=V \J(ℓ)(V,k,k,ζ(ℓ))tosimplifythenotation. Wehave
2 M 0
• V \J(ℓ)(R,k,k,ζ(ℓ))=S(ℓ)[k]∪S(ℓ)[k],and
M 0 1 2
• S(ℓ)[k]∩S(ℓ)[k]=∅.
1 2
FromAssumption4.4, wehave|S(ℓ)[k]∩Nin| ≤ F. Applying(2F +1)-reachablepropertyofv andtwoabove
1 i i
properties,weobtainthat|S(ℓ)[k]∩Nin|≥F +1. LetV¯(ℓ)[k]⊆Nin bethesetofnodesthatv ∈Rdiscardstheir
2 i i i i
valuesindimensionℓattime-stepk. Fromthefactthatv ∈J(ℓ)(R,k,k,ζ(ℓ)),andLine11ofAlgorithm1,weknow
i M 0
that|S(ℓ)[k]∩V¯(ℓ)[k]|≤F. Combiningthiswiththeformerstatement,wecanconcludethatv usesatleastonevalue
2 i i
fromS(ℓ)[k]initsupdate,i.e.,(S(ℓ)[k]∩Nin)\V¯(ℓ)[k]̸=∅.
2 2 i i
Considertheauxiliarypointupdaterule(7)(inLine12ofAlgorithm1). Wecanrewritetheupdateas
y(ℓ)[k+1]= (cid:88) w(ℓ) [k]y(ℓ)[k] + (cid:88) w(ℓ) [k]y(ℓ)[k].
i y,ij j y,ij j
(cid:0) (cid:1)
vj∈(S 2(ℓ)[k]∩N iin)\V¯ i(ℓ)[k] vj∈ vi∪(JM(V,k,k,ζ 0(ℓ))∩N iin) \V¯ i(ℓ)[k]
25ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
Sincey(ℓ)[k]onthefirstandsecondtermsontheRHSareupperboundedbyM(ℓ)[k]−ζ(ℓ)andM(ℓ)[k],respectively,
j 0
andthenon-zeroweightsw(ℓ) [k]arelowerboundedbytheconstantω(Assumption4.5),thevalueofthisnodeatthe
y,ij
nexttime-stepisupperboundedas
y(ℓ)[k+1]≤ω(M(ℓ)[k]−ζ(ℓ))+(1−ω)M(ℓ)[k]=M(ℓ)[k]−ωζ(ℓ).
i 0 0
NotethattheaboveboundisapplicabletoanynodethatisinR\J(ℓ)(V,k,k,ζ(ℓ)),sincesuchanodewilluseitsown
M 0
valueinitsupdate. Similarly,ifthereisanodev ∈J(ℓ)(R,k,k,ζ(ℓ))thatusesthevalueofanodeoutsidethatset,
j m 0
theny(ℓ)[k+1]≥m(ℓ)[k]+ωζ(ℓ). ThisboundisalsoapplicabletoanynodethatisinR\J(ℓ)(V,k,k,ζ(ℓ)).
j 0 m 0
Now, define the quantity ζ(ℓ) = ωζ(ℓ). We have that the set J(ℓ)(V,k +1,k,ζ(ℓ))∩J(ℓ)(V,k +1,k,ζ(ℓ)) = ∅.
1 0 M 1 m 1
Furthermore,bytheboundsprovidedabove,weseethatatleastoneofthefollowingmustbetrue:
|J(ℓ)(R,k+1,k,ζ(ℓ))|<|J(ℓ)(R,k,k,ζ(ℓ))|, or
M 1 M 0
|J(ℓ)(R,k+1,k,ζ(ℓ))|<|J(ℓ)(R,k,k,ζ(ℓ))|.
m 1 m 0
IfJ(ℓ)(R,k+1,k,ζ(ℓ))̸=∅andJ(ℓ)(R,k+1,k,ζ(ℓ))̸=∅,thenagainbythefactthatthegraphis(2F +1)-robust,
M 1 m 1
there is at least one node in one of these sets that has at least 2F +1 in-neighbors outside from the set. Suppose
v ∈J(ℓ)(R,k+1,k,ζ(ℓ))issuchanode. Then,v cannotusethevaluesstrictlygreaterthanM(ℓ)[k+1]andituses
i M 1 i
atleastonevaluefromV \J(ℓ)(V,k+1,k,ζ(ℓ)). Sinceattime-stepk,allregularnodescannotusevaluesthatare
M 1
strictlygreaterthanM(ℓ)[k]intheupdate,wehavethatM(ℓ)[k+1]≤M(ℓ)[k]. Therefore,thevalueofnodev atthe
i
nexttime-stepisupperboundedas
y(ℓ)[k+2]≤ω(M(ℓ)[k]−ζ(ℓ))+(1−ω)M(ℓ)[k+1]≤M(ℓ)[k]−ω2ζ(ℓ).
i 1 0
Again,thisupperboundalsoholdsforanyregularnodethatisinR\J(ℓ)(V,k+1,k,ζ(ℓ)). Similarly,ifthereisanode
M 1
v ∈J(ℓ)(R,k+1,k,ζ(ℓ))thathas2F +1in-neighborsfromoutsidethatsettheny(ℓ)[k+2]≥m(ℓ)[k]+ω2ζ(ℓ).
j m 1 j 0
ThisboundalsoholdsforanyregularnodethatisnotinthesetR\J(ℓ)(V,k+1,k,ζ(ℓ)).
m 1
Wecontinueinthismannerbydefiningζ(ℓ) =ωsζ(ℓ)fors∈N.Ateachtimestepk+s,ifbothJ(ℓ)(R,k+s,k,ζ(ℓ))̸=
s 0 M s
∅andJ(ℓ)(R,k+s,k,ζ(ℓ))̸=∅thenatleastoneofthesesetswillshrinkinthenexttime-step. Ifeitherofthesets
m s
isempty, thenitwillstayemptyatthenexttime-step, sinceeveryregularnodeoutsidethatsetwillhaveitsvalue
upperboundedbyM(ℓ)[k]−ζ(ℓ)orlowerboundedbym(ℓ)[k]+ζ(ℓ). After|R|−1time-steps,atleastoneofthesets
s s
J(ℓ)(R,k+|R|−1,k,ζ(ℓ) )orJ(ℓ)(R,k+|R|−1,k,ζ(ℓ) )mustbeemptysincethesetsJ(ℓ)(R,k,k,ζ(ℓ))
M |R|−1 m |R|−1 M 0
andJ(ℓ)(R,k,k,ζ(ℓ))cancontainatmostR−1regularnodes. Supposetheformersetisempty;thismeansthat
m 0
M(ℓ)[k+|R|−1]≤M(ℓ)[k]−ζ(ℓ) .
|R|−1
Sincem(ℓ)[k+|R|−1]≥m(ℓ)[k],weobtain
(cid:16) ω|R|−1(cid:17)
D(ℓ)[k+|R|−1]≤D(ℓ)[k]−ζ(ℓ) = 1− D(ℓ)[k]=γD(ℓ)[k]. (23)
|R|−1 2
Thefirstequalitycomesfromthefactthatζ(ℓ) =ωsζ(ℓ)andζ(ℓ) = 1D(ℓ)[k]. Thesameexpressionas(23)arisesifthe
s 0 0 2
setJ(ℓ)(R,k+|R|−1,k,ζ(ℓ) )=∅.
m |R|−1
Usingthefactthat
(cid:2) m(ℓ)[k+1], M(ℓ)[k+1](cid:3) ⊆(cid:2) m(ℓ)[k], M(ℓ)[k](cid:3) (24)
forallk ∈Nandtheinequality(23),wecanconcludethatforallv ∈R,lim y(ℓ)[k]=y(ℓ)[∞]existsandforall
i k→∞ i
k,wehave
y(ℓ)[∞]∈(cid:2) m(ℓ)[k], M(ℓ)[k](cid:3) . (25)
Thiscompletesthefirstpartoftheproof.
Forthesecondpart,letconsiderthequantityD(ℓ)[k]asfollows. Forallk ∈N,wecanwrite
(cid:20)(cid:106) k (cid:107) (cid:21) (cid:4) (cid:5)
D(ℓ)[k]≤D(ℓ) (|R|−1) ≤γ |Rk |−1 D(ℓ)[0]<γ|Rk |−1−1D(ℓ)[0]. (26)
|R|−1
26ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
Thefirstinequalityisobtainedbyusingx≥⌊x⌋and(24). Toobtainthesecondinequality,weapplytheinequality(23)
(cid:4) k (cid:5) times. Thelastinequalitycomesfromthefactthatγ <1and⌊x⌋>x−1impliesγ⌊x⌋ <γx−1. From(25)
|R|−1
and(26),forallv ∈R,wehave
i
|y(ℓ)[k]−y(ℓ)[∞]|≤D(ℓ)[k]<γ|Rk |−1−1D(ℓ)[0]. (27)
i
Sincetheinequality(27)holdsforallℓ∈{1,2,...,d},wehave
d (cid:0) (cid:1) d
∥y [k]−y[∞]∥2 =(cid:88) |y(ℓ)[k]−y(ℓ)[∞]|2 <γ2 |Rk |−1−1 (cid:88)(cid:0) D(ℓ)[0](cid:1)2 .
i i
ℓ=1 ℓ=1
Takingsquarerootofbothsidesyields
1
∥y [k]−y [∞]∥<γ|Rk |−1−1∥D[0]∥= ∥D[0]∥e− |R1 |−1log( γ1)k,
i i γ
whichcompletestheproof.
C ProofofProposition4.10
ProofofProposition4.10. Consider a regular agent v ∈ R. From Assumption 4.1, for all x, y ∈ Rd, we have
i
f (y)≥f (x)+⟨g˜ (x), y−x⟩,whereg˜ (x)∈∂f (x). Substituteaminimizerx∗ofthefunctionf intothevariable
i i i i i i i
ytoget
−⟨g˜ (x), x∗−x⟩≥f (x)−f (x∗). (28)
i i i i i
Letθˆ(x)=∠(g˜ (x), x−x∗). Theinequality(28)becomes
i i i
∥g˜ (x)∥∥x∗−x∥cosθˆ(x)≥f (x)−f (x∗).
i i i i i i
Fixϵ∈R ,andsupposethatx∈/ C (ϵ). FromAssumption4.2,applying∥g˜ (x)∥≤L,wehave
>0 i i
f (x)−f (x∗)
cosθˆ(x)≥ i i . (29)
i L∥x∗−x∥
i
Letx˜ ∈Rdbethepointonthelineconnectingx∗andxsuchthatf (x˜ )=f (x∗)+ϵ. Wecanrewritethepointxas
i i i i i i
∥x−x∗∥
x=x∗+t(x˜ −x∗) where t= i ≥1.
i i i ∥x˜ −x∗∥
i i
ConsiderthetermontheRHSof(29). Sincex˜ ∈C (ϵ),and(12)holds,wehave
i i
f (x)−f (x∗) f (x∗+t(x˜ −x∗))−f (x∗)
i i i = i i i i i i
∥x−x∗∥ t∥x˜ −x∗∥
i i i
f (x∗+t(x˜ −x∗))−f (x∗)
≥ i i i i i i
t·max ∥y−x∗∥
y∈Ci(ϵ) i
f (x∗+t(x˜ −x∗))−f (x∗)
≥ i i i i i i . (30)
tδ (ϵ)
i
Since the quantity fi(x∗ i+t(x˜i−x∗ i))−fi(x∗ i) is non-decreasing in t ∈ [1,∞) [52, Lemma 2.80], the inequality (30)
t
becomes
f (x)−f (x∗) f (x˜ )−f (x∗) ϵ
i i i ≥ i i i i = . (31)
∥x−x∗∥ δ (ϵ) δ (ϵ)
i i i
Therefore,combining(29)and(31),weobtain
ϵ
cosθˆ(x)≥ . (32)
i Lδ (ϵ)
i
However,fromAssumption4.1,wehave
f (x∗)≥f (x˜ )+⟨g˜ (x˜ ), x∗−x˜ ⟩
i i i i i i i i
whereg˜ (x˜ )∈∂f (x˜ ). Since∥g˜ (x˜ )∥≤LbyAssumption4.2and∥x∗−x˜ ∥≤δ (ϵ),weget
i i i i i i i i i
ϵ=f (x˜ )−f (x∗)≤−⟨g˜ (x˜ ), x∗−x˜ ⟩≤Lδ (ϵ).
i i i i i i i i i
Fromϵ∈R andtheaboveinequality,theinequality(32)becomes
>0
(cid:16) ϵ (cid:17) π
θˆ(x)≤arccos :=θ (ϵ)< ,
i Lδ (ϵ) i 2
i
whichcompletestheproof.
27ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
D ProofofTheorem4.12
Proof. Wewillshowthatthesummationofanysubgradientsoftheregularnodes’functionsatanypointoutsidethe
regionB(cid:0)
y[∞], inf
s∗(0,ϵ)(cid:1)
cannotbezero.
ϵ>0
Letx beapointoutsideB(cid:0) y[∞], inf s∗(0,ϵ)(cid:1) . Since∥x −y[∞]∥>max {R˜ +δ (ϵ)}forsomeϵ>0,we
0 ϵ>0 0 vi∈R i i
havethatx ∈/ C (ϵ)forallv ∈ R. BythedefinitionofC (ϵ)in(11),wehavef (x ) > f (x∗)+ϵforallv ∈ R.
0 i i i i 0 i i i
Sincethefunctionsf areconvex,weobtaing (x )̸=0forallv ∈Rwhereg (x )∈∂f (x ).
i i 0 i i 0 i 0
Considertheanglebetweenthevectorsx −x∗ andx −y[∞]. IfR˜ = 0,from(14),wehavex∗ = y[∞]which
0 i 0 i i
impliesthat∠(x −x∗, x −y[∞])=0. SupposeR˜ >0. UsingLemmaA.1,wecanboundtheangleasfollows:
0 i 0 i
(cid:16) R˜ (cid:17)
∠(x −x∗, x −y[∞])≤arcsin i .
0 i 0 ∥x −y[∞]∥
0
Since∥x −y[∞]∥>max {R˜ secθ (ϵ)}forsomeϵ>0andarcsin(x)isanincreasingfunctioninx∈[−1,1],
0 vi∈R i i
wehave
(cid:16) R˜ (cid:17)
∠(x −x∗, x −y[∞])<arcsin i
0 i 0 R˜ secθ (ϵ)
i i
and that arcsin(cosθ (ϵ)) = π −θ (ϵ). Using Proposition 4.10 and the inequality above, we can bound the angle
i 2 i
betweenthevectorsg (x )andx −y[∞]asfollows:
i 0 0
(cid:16)π (cid:17) π
∠(g (x ), x −y[∞])≤∠(g (x ), x −x∗)+∠(x −x∗, x −y[∞])<θ (ϵ)+ −θ (ϵ) = .
i 0 0 i 0 0 i 0 i 0 i 2 i 2
Notethatthefirstinequalityisobtainedfrom[53,Corollary12]. Letu= x0−y[∞] . Computetheinnerproduct
∥x0−y[∞]∥
(cid:68) (cid:88) (cid:69) (cid:88)
g (x ), u = ∥g (x )∥cos∠(g (x ), x −y[∞]).
i 0 i 0 i 0 0
vi∈R vi∈R
TheRHSoftheaboveequationisstrictlygreaterthanzerosince∥g (x )∥>0andcos∠(g (x ), x −y[∞])>0
(cid:80) i 0 i 0 0
forallv ∈R. Thisimpliesthat g (x )̸=0. Sincewecanarbitrarilychooseg (x )fromtheset∂f (x ),we
have0∈/i
∂f(x )wheref(x)=
1vi∈ (cid:80)R i f0
(x).
i 0 i 0
0 |R| vi∈R i
E ProofofResultsinSection4.5.1
ProofofLemma4.13. FromProposition4.6,thelimitpointy[∞]∈Rdexists. Consideratime-stepk ∈Nsuchthat
k ≥k∗. Usingthegradientstep(6),wecanwrite
1
∥x [k+1]−y[∞]∥=∥z [k]−y[∞]−η[k]g [k]∥≤∥z [k]−y[∞]∥+η[k]∥g [k]∥.
i i i i i
UsingAssumptions4.2and4.3,and∥z [k]−y[∞]∥≤max {R˜ +δ },weobtain
i vj∈R j j
∥x [k+1]−y[∞]∥≤ max{R˜ +δ }+η[k∗]L.
i j j 1
vj∈R
Bythedefinitionofk∗ands∗in(15),theaboveinequalitybecomes
1
∥x [k+1]−y[∞]∥≤ max{R˜ +δ }+ξ ≤s∗,
i j j
vj∈R
whichcompletestheproof.
ProofofLemma4.14. From Proposition 4.6, the limit point y[∞] ∈ Rd exists. Consider an agent v ∈ R and a
i
time-stepk ∈Nforwhichtheconditioninthelemmaholds. Sincex∗ ∈B(y[∞],R˜ )from(14),wehave
i i
R˜
∠(x∗−z [k], y[∞]−z [k])≤ max ∠(u−z [k], y[∞]−z [k])=arcsin i , (33)
i i i u∈B(y[∞],R˜ i) i i ∥z i[k]−y[∞]∥
wherethelaststepisfromusingLemmaA.1. Usingthegradientstep(6),wecanwrite∠(x [k+1]−z [k], y[∞]−
i i
z [k])=∠(−η[k]g [k], y[∞]−z [k]). Sinceforallv ∈Randk ∈N,
i i i i
∠(−η[k]g [k], y[∞]−z [k])≤∠(−η[k]g [k], x∗−z [k])+∠(x∗−z [k], y[∞]−z [k])
i i i i i i i i
28ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
by[53,Corollary12],applyingProposition4.10andinequality(33),wehave
R˜
∠(x [k+1]−z [k], y[∞]−z [k])≤θ +arcsin i :=ψ [k]. (34)
i i i i ∥z [k]−y[∞]∥ i
i
Notethatψ i[k]∈[0,π)sinceθ i ∈(cid:2) 0,π 2(cid:1) andarcsin ∥zi[k]R −˜ i y[∞]∥ ∈(cid:2) 0,π 2(cid:3) . Then,considerthetrianglewhichhasthe
verticesatx [k+1],z [k],andy[∞]. Wecancalculatethesquareofthedistancebyusingthelawofcosines:
i i
∥x [k+1]−y[∞]∥2 = ∥x [k+1]−z [k]∥2+∥y[∞]−z [k]∥2
i i i i
−2∥x [k+1]−z [k]∥·∥y[∞]−z [k]∥cos∠(x [k+1]−z [k], y[∞]−z [k]).
i i i i i i
Usingthegradientstep(6)andtheinequality(34),weget
∥x [k+1]−y[∞]∥2 ≤η2[k]∥g [k]∥2+∥z [k]−y[∞]∥2−2η[k]∥g [k]∥·∥z [k]−y[∞]∥cosψ [k]. (35)
i i i i i i
Inaddition,wecansimplifytheterm∥z [k]−y[∞]∥cosψ [k]intheaboveinequalityusingthedefinitionofψ [k]in
i i i
(34). Regardingthis,wecanwrite
(cid:113)
∥z [k]−y[∞]∥cosψ [k]= ∥z [k]−y[∞]∥2−R˜2·cosθ −R˜ sinθ .
i i i i i i i
Substitutingtheaboveequationinto(35),weobtaintheresult.
ProofofLemma4.15. First,notethatbythedefinitionofs∗in(15),wehavea+ >0anda− <0sinces∗ ≥R˜ +ξ,
i i i
andb >0sinces∗ ≥R˜ secθ +ξ.
i i i
Forv ∈R,letΓ :[R˜ ,∞)×R →Rbethefunction
i i i +
Γ (p,l):=p2−∆ (p,l), (36)
i i
wherefunction∆ isdefinedin(16). Consideranagentv ∈ Randatime-stepk ∈ Nsuchthatk ≥ k∗. Wecan
i i 2
computethesecondderivativeofΓ (p,l)withrespecttopasfollows:
i
∂2Γ
∂p2i =2+2lR˜ i2(p2−R˜ i2)−3 2 cosθ i.
Notethat ∂ ∂2 pΓ 2i >0forallp∈(R˜ i,∞). Thisimpliesthat
sup Γ (p,l)≤ max Γ (p,l)=max(cid:8) Γ (R˜ ,l), Γ (s∗,l)(cid:9) . (37)
i i i i i
p∈(maxvj∈R{R˜ j+δj},s∗] p∈[R˜ i,s∗]
First,letconsiderΓ (R˜ , l). FromthedefinitionofΓ in(36),wehavethat
i i i
Γ (R˜ ,l)≤(s∗)2 ⇐⇒ l∈[a−, a+], (38)
i i i i
wherea+anda−aredefinedin(18). UsingAssumption4.2and4.3,andthedefinitionofk∗,wehavethat
i i 2
η[k]∥g [k]∥≤η[k]L≤ min (cid:8) min{a+, b }(cid:9) ≤a+.
i j j i
vj∈R
Bytheaboveinequalityandstatement(38),weobtainthat
Γ (R˜ , η[k]∥g [k]∥)≤(s∗)2. (39)
i i i
Now,letconsiderΓ (s∗, l). FromthedefinitionofΓ in(36),wehavethat
i i
Γ (s∗,l)≤(s∗)2 ⇐⇒ l∈[0, b ], (40)
i i
whereb isdefinedin(18). UsingAssumption4.2and4.3,andthedefinitionofk∗,wehavethat
i 2
η[k]∥g [k]∥≤η[k]L≤ min (cid:8) min{a+, b }(cid:9) ≤b .
i j j i
vj∈R
Bytheaboveinequalityandstatement(40),weobtainthat
Γ (s∗, η[k]∥g [k]∥)≤(s∗)2. (41)
i i
Combine(39)and(41)togetthat
max(cid:8) Γ (R˜ , η[k]∥g [k]∥), Γ (s∗(ξ), η[k]∥g [k]∥)(cid:9) ≤(s∗)2. (42)
i i i i i
FromLemma4.14,wecanwrite
∥x [k+1]−y[∞]∥2 ≤Γ (∥z [k]−y[∞]∥, η[k]∥g [k]∥).
i i i i
Applying(37)and(42),respectivelytotheaboveinequalityyieldstheresult.
29ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
ProofofLemma4.16. Consideranytime-stepk ≥ k∗ andagentv ∈ I [k]. Bythedefinitionofthefunction∆ in
3 i z i
(16),itisclearthatifp >p ≥R˜ then∆ (p ,l)>∆ (p ,l). Then,weget
1 2 i i 1 i 2
∆ (∥z [k]−y[∞]∥,η[k]∥g [k]∥)>∆ (s∗,η[k]∥g [k]∥). (43)
i i i i i
Furthermore,thefunction∆ satisfies
i
(cid:16)(cid:113) (cid:17) (cid:104) (cid:113) (cid:105)
∆ (p,l)≥ p2−R˜2cosθ −R˜ sinθ l ⇐⇒ l∈ 0, p2−R˜2cosθ −R˜ sinθ . (44)
i i i i i i i i i
Werestateinequality(31)obtainedintheproofofProposition4.10here:
f (x)−f (x∗) ϵ
i i i ≥ . (45)
∥x−x∗∥ δ (ϵ)
i i
RecallthedefinitionofC (ϵ)in(11). Forx∈/ C (ϵ),fromthedefinitionofconvexfunctions,wehave−⟨g (x), x∗−
i i i i
x⟩≥f (x)−f (x∗). Usingtheinequality(45),weobtain
i i i
f (x)−f (x∗) ϵ
∥g (x)∥≥ i i i ≥ =L .
i ∥x−x∗∥ δ (ϵ) i
i i
Usingtheaboveinequality,Assumption4.2,andthedefinitionofk∗,wehavethat
3
b
η[k]L ≤η[k]∥g [k]∥≤η[k]L≤ i. (46)
i i 2
Sinceη[k]∥g [k]∥∈[0, bi],wecanapply(44)toget
i 2
∆ (cid:0) s∗, η[k]∥g [k]∥(cid:1) ≥ b iη[k]∥g [k]∥. (47)
i i 2 i
Combine(43),(47),andthefirstinequalityof(46)toobtaintheresult.
F ProofofResultsinSection4.5.2
ProofofProposition4.17. FromProposition4.6,thelimitpointy[∞]∈Rdexists. Forallv ,v ∈R,wehave
i j
∥x [k]−y [k]∥≤∥x [k]−y[∞]∥+∥y [k]−y[∞]∥.
j i j i
ApplyLemma4.9toobtainthatforallv ∈R,wehave
i
∥z [k]−y [k]∥≤ max∥x [k]−y[∞]∥+∥y [k]−y[∞]∥.
i i j i
vj∈R
Substitutingtheaboveinequalityinto
∥z [k]−y[∞]∥≤∥z [k]−y [k]∥+∥y [k]−y[∞]∥,
i i i i
weobtaintheresult.
ProofofLemma4.18. Supposemax ∥x [k]−y[∞]∥≤ϕ[k]foratime-stepk ≥k . FromProposition4.6and
vi∈R i 0
4.17,wehavethatforallv ∈R,
i
∥z [k]−y[∞]∥≤ϕ[k]+2βe−αk, (48)
i
whereαandβ aredefinedinProposition4.6.
RecallthedefinitionofI [k]from(19). Forallv ∈I [k],fromLemma4.14and4.16,wehave
z i z
1
∥x [k+1]−y[∞]∥2 ≤∥z [k]−y[∞]∥2− b L η[k].
i i 2 i i
Applying(48)totheaboveinequality,weobtainthatforallv ∈I [k],
i z
∥x [k+1]−y[∞]∥2 ≤(cid:0) ϕ[k]+2βe−αk(cid:1)2 − 1 b L η[k]≤(cid:0) ϕ[k]+2βe−αk(cid:1)2 − 1 η[k] min b L .
i 2 i i 2 vj∈R j j
Ontheotherhand,forallv ∈R\I [k],wehave∥z [k]−y[∞]∥≤s∗bythedefinitionofI [k]. FromLemma4.13
i z i z
and4.15,weget∥x [k+1]−y[∞]∥≤s∗forallv ∈R\I [k]. Therefore,weconcludethatforallv ∈R,
i i z i
∥x [k+1]−y[∞]∥2 ≤max(cid:110) (s∗)2,(cid:0) ϕ[k]+2βe−αk(cid:1)2 − 1 η[k] min b L (cid:111) .
i 2 vj∈R j j
30ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
Usingtheupdaterule(21),theaboveinequalityimpliesthatmax ∥x [k+1]−y[∞]∥≤ϕ[k+1].
vi∈R i
Next,consideratime-stepk ∈N. Fromthegradientupdatestep(6),forallv ∈R,wehave
i
∥x [k+1]−y[∞]∥=∥z [k]−y[∞]−η[k]g [k]∥≤∥z [k]−y[∞]∥+η[k]∥g [k]∥.
i i i i i
Since∥g [k]∥≤LfromAssumption4.2,wecanrewritetheaboveinequalityas
i
∥x [k+1]−y[∞]∥≤∥z [k]−y[∞]∥+η[k]L. (49)
i i
Ontheotherhand,fromProposition4.6and4.17,forallv ∈R,wehave
i
∥z [k]−y[∞]∥≤ max∥x [k]−y[∞]∥+2βe−αk. (50)
i j
vj∈R
Combinetheinequalities(49)and(50)togetherandapplytheresultrecursivelytoobtain
k (cid:88)0−1 k (cid:88)0−1
max∥x [k ]−y[∞]∥≤ max∥x [0]−y[∞]∥+2β e−αk+L η[k].
i 0 i
vi∈R vi∈R
k=0 k=0
SincetheRHSoftheaboveinequalityisϕ[k ],thiscompletesthefirstpartoftheproof.
0
Consideratime-stepk ∈Nsuchthatk ≥k .Fromtheupdateequation(21),usingthefactthat 1η[k]min b L >0
forallk ∈N,wecanwrite
0 2 vi∈R i i
ϕ[k+1]<max(cid:8) s∗, ϕ[k](cid:9) +2βe−αk.
Applyingtheaboveinequalityrecursively,wecanwritethatforallk ≥k ,
0
k−1
ϕ[k]<max(cid:8) s∗, ϕ[k ](cid:9) +2β (cid:88) e−αk′ .
0
k′=k0
Substitutingequation(20)intotheaboveinequalityandusingthefactthat(cid:80)k−1 e−αk′ <(cid:80)∞ e−αk′ = 1 for
k′=0 k′=0 1−e−α
allk ∈N,weobtaintheuniformboundasfollows:
(cid:110)
k (cid:88)0−1
(cid:111) 2β
ϕ[k]<max s∗, max∥x [0]−y[∞]∥+L η[k′] + .
vi∈R i
k′=0
1−e−α
SettingtheRHSoftheaboveinequalitytoϕ¯,weobtaintheresult.
G ProofofResultsinSection4.5.3
Lemma4.19isusedtoestablishtheproofofProposition4.20.
ProofofLemma4.19. Supposethatthereexistsasequence{u[k]}∞ ⊂R thatsatisfiesthegivenupdaterule. Since
k=0 ≥0
ηˆ[k]∈R forallk ∈N,wehaveu2[k+1]≤(u[k]+γ λk)2. Sinceu[k]≥0forallk ∈N,itfollowsthat
≥0 1
0≤u[k+1]≤|u[k]+γ λk|≤u[k]+γ |λ|k.
1 1
Applytheaboveinequalityrecursivelytoobtainthatforallk ∈N,
k
u[k]≤u[0]+γ (cid:88) |λ|ℓ ≤u[0]+ γ 1 :=u¯.
1 1−|λ|
ℓ=0
Fromtheupdaterule,wecanwrite
u2[k+1]=u2[k]+2γ λku[k]+γ2λ2k−γ ηˆ[k]≤u2[k]+2γ λku¯+γ2λ2k−γ ηˆ[k].
1 1 2 1 1 2
Applyingtheaboveinequalityrecursively,weobtain
k k k
(cid:88) (cid:88) (cid:88)
u2[k]≤u2[0]+2γ u¯ λℓ+γ2 λ2ℓ−γ ηˆ[ℓ].
1 1 2
ℓ=0 ℓ=0 ℓ=0
However,thefirstthreetermsontheRHSareboundedinkwhilethelasttermisunbounded. Thisimpliesthatthere
existsatime-stepk˜ ∈Nsuchthatu2[k˜]<0whichcontradictsthefactthatu[k˜]∈R .
≥0
31ScalableDistributedOptimizationDespiteByzantineAdversaries APREPRINT
ProofofProposition4.20. Letν = 1min b L tosimplifythenotations. Letk∗ ∈ Nbeatime-stepsuchthat
2 vi∈R i i 4
η[k 4∗]≥ 4 νβ(cid:0) ϕ¯e−αk 4∗ +βe−2αk 4∗(cid:1) . Notethatk 4∗ ∈Nexistssinceη[k]decreasesslowerthantheexponentialdecaydue
toitsformgiveninAssumption4.3.
First,wewillshowthatifthetime-stepk ∈Nsatisfiesk ≥max{k ,k∗}and(cid:0) ϕ[k]+2βe−αk(cid:1)2 −νη[k]>(s∗)2,then
0 4
ϕ[k+1]<ϕ[k]. (51)
Consideratime-stepk ≥max{k ,k∗}. Since(cid:0) ϕ[k]+2βe−αk(cid:1)2 −νη[k]>(s∗)2,theupdateequation(21)reducesto
0 4
ϕ2[k+1]=(cid:0) ϕ[k]+2βe−αk(cid:1)2
−νη[k]. (52)
Usingthedefinitionofk∗,fromk ≥k∗,wecanwriteη[k]≥ 4β(cid:0) ϕ¯e−αk+βe−2αk(cid:1) . Sinceϕ[k]<ϕ¯,wehavethat
4 4 ν
η[k]>
4β(cid:0) ϕ[k]e−αk+βe−2αk(cid:1)
.
ν
Bymultiplyingνandaddingϕ2[k]tobothsides,andthenrearranging,wecanwriteϕ2[k]>(cid:0) ϕ[k]+2βe−αk(cid:1)2
−νη[k],
whichisequivalenttoϕ[k+1]<ϕ[k]by(52). Thiscompletesourclaim.
Next,wewillshowthatthereexistsatime-stepK˜ ∈Nsuchthatϕ(cid:2) max{k ,k∗}+K˜(cid:3) =s∗.
0 4
Supposethat(cid:0) ϕ[k]+2βe−αk(cid:1)2 −νη[k]>(s∗)2forallk ≥max{k ,k∗}. Then,theupdateequation(21)reducesto
0 4
ϕ2[k+1]=(cid:0) ϕ[k]+2βe−αk(cid:1)2
−νη[k].
However,sinceϕ[k]isnon-negativeforallk ∈Nbyitsdefinition,fromLemma4.19,thereisnosequence{ϕ[k]}∞
k=k0
thatcansatisfytheaboveupdaterule. Hence,thereexistsaconstantK˜ ∈Nsuchthat
(cid:0) ϕ[k′]+2βe−αk′(cid:1)2 −νη[k′]≤(s∗)2,
wherek′ =max{k ,k∗}+K˜ −1,whichyieldsϕ(cid:2) max{k ,k∗}+K˜(cid:3) =s∗bytheequation(21). Thiscompletesthe
0 4 0 4
secondclaim.
Consider any time-step k ∈ N such that k ≥ max{k ,k∗}+K˜ and ϕ[k] = s∗. Such a time-step exists due to
0 4
the argument above. Then, suppose (cid:0) ϕ[k]+2βe−αk(cid:1)2 −νη[k] > (s∗)2. From (51), we have that ϕ[k+1] < s∗
which is not possible due to the fact that ϕ[k′] ≥ s∗ for all k′ ≥ k from the update equation (21). Hence, we
0
conclude that (cid:0) ϕ[k]+2βe−αk(cid:1)2 −νη[k] ≤ (s∗)2, and ϕ[k+1] = s∗ by (21). This means that ϕ[k] = s∗ for all
k ≥max{k ,k∗}+K˜. Then,bythedefinitionofϕ[k],wecanrewritetheequationasmax ∥x [k]−y[∞]∥≤s∗
0 4 vi∈R i
forallk ≥max{k ,k∗}+K˜ :=K whichcompletestheproof.
0 4
Finally,weutilizethefinite-timeconvergenceresultfromProposition4.20togiveaproofforTheorem4.11presented
below.
ProofofTheorem4.11. FromProposition4.20,forafixedξ ∈R andϵ∈R ,wehavethatforallk ≥K,
>0 >0
max∥x [k]−y[∞]∥≤s∗(ξ,ϵ).
i
vi∈R
NotethatK isafunctionofξandϵ. However,theaboveinequalityimpliesthatlimsup max ∥x [k]−y[∞]∥≤
k vi∈R i
s∗(ξ,ϵ). Since the inequality is valid for all ξ > 0 and ϵ > 0, and inf s∗(ξ,ϵ) = inf s∗(0,ϵ) by the
ξ>0,ϵ>0 ϵ>0
definitionofs∗(ξ,ϵ)in(15),wehave
limsupmax∥x [k]−y[∞]∥≤ inf s∗(0,ϵ),
i
k vi∈R ϵ>0
whichcompletestheproof.
32