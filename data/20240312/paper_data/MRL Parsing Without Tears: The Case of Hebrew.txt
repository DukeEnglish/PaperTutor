MRL Parsing Without Tears: The Case of Hebrew
ShaltielShmidman1,†,AviShmidman1,2,‡,MosheKoppel1,2,†,ReutTsarfaty2,‡
1DICTA/Jerusalem,Israel 2BarIlanUniversity/RamatGan,Israel
†{shaltieltzion,moishk}@gmail.com
‡{avi.shmidman,reut.tsarfaty}@biu.ac.il
Abstract Pre-neural parsing approaches1 generally in-
volved pipelines consisting of multiple steps in
Syntacticparsingremainsacriticaltoolforre-
sequence: segmentation of prefixes and suffixes;
lationextractionandinformationextraction,es-
afterward, morphological tagging of segmented
pecially in resource-scarce languages where
LLMsarelacking. Yetinmorphologicallyrich units;and,finally,syntacticparsing. However,as
languages(MRLs),whereparsersneedtoiden- highlightedbyTsarfaty(2006);CohenandSmith
tifymultiplelexicalunitsineachtoken,exist- (2007);GreenandManning(2010);Goldbergand
ing systems suffer in latency and setup com-
Tsarfaty(2008);Tsarfatyetal.(2019);Seekerand
plexity. Someuseapipelinetopeelawaythe
Çetinog˘lu (2015), this pipeline approach propa-
layers:firstsegmentation,thenmorphologytag-
gatesandcompoundserrorsfromonestagetoan-
ging,andthensyntaxparsing;however,errors
other,compromisingtheaccuracyofthesystem.
in earlier layers are then propagated forward.
Others use a joint architecture to evaluate all Morerecently,neuralparsingmodels(Sekerand
permutationsatonce;whilethisimprovesac- Tsarfaty,2020;LeviandTsarfaty,2024;Krishna
curacy,itisnotoriouslyslow. Incontrast,and et al., 2020b) have circumvented these problems
takingHebrewasatestcase,wepresentanew
by parsing the text with a single joint morpho-
"flippedpipeline": decisionsaremadedirectly
syntactic model. However, this latter approach
onthewhole-tokenunitsbyexpertclassifiers,
suffers in performance, because it entails consid-
each one dedicated to one specific task. The
erationofcomprehensivelatticesdetailingallper-
classifiersareindependentofoneanother,and
onlyattheenddowesynthesizetheirpredic- mutationsofallsegmentation,morphological,and
tions. Thisblazinglyfastapproachsetsanew syntacticpossibilitiesacrossthewholesentence.
SOTAinHebrewPOStagginganddependency Furthermore, all existing joint models for He-
parsing,whilealsoreachingnear-SOTAperfor- brew parsing (More et al., 2019; Tsarfaty et al.,
manceonotherHebrewNLPtasks. Because
2019;SekerandTsarfaty,2020;LeviandTsarfaty,
ourarchitecturedoesnotrelyonanylanguage-
2024),andlikewiseforotherMRLssuchasturkish
specific resources, it can serve as a model to
(SeekerandÇetinog˘lu,2015)andSanskrit(Krishna
developsimilarparsersforotherMRLs.
etal.,2020a)relyonanexternallexiconwhichdic-
1 TheChallengeofMRLParsing
tates the range of linguistic realizations for each
wordinthelanguage. Thiscreatescomplications
MorphologicallyRichLanguages(MRLs)suchas
forpracticalintegrationofthesystems.
HebrewpresentuniquechallengesforNLP,dueto
Finally, in order to accomplish their tasks, the
their complex word structures. Any given space-
aformentionedmodelsallrelyonawidearrayof
delimited word is likely to be comprised of mul-
externaldependencies,makingthemratherdifficult
tiplemorphologicaltokens,becauseprepositions,
to install and integrate. Industry developers who
conjunctions, and relativizers are often attached
attempttoaddNLPelementsforunder-resourced
asprefixes,andaccusativesorgenitivesareoften
MRLs such as Hebrew routinely report that the
expressedassuffixes. Theseaffixedmorphological
existingmodelsaresimplytoocumbersometoop-
tokens are not marked in any way, and in many
cases the letters which start or end a given word
1Inparticularparsingpipelinesthatworkaccordingtothe
canberealizedvariouslyaseitherpartofthepri- schemaprescribedbytheUniversalDependencies(UD)initia-
mary word or as separate morphological tokens, tive(deMarneffeetal.,2021),link: https://lindat.mff.
cuni.cz/services/udpipe/. The specific steps of each
depending on the context. Parsers for MRLs are
pipelinediffersomewhatfromlanguagetolanguage;e.g.not
taskedwithresolvingtheseambiguities. allschemasprescribesegmentation.
4202
raM
11
]LC.sc[
1v07960.3042:viXraerate and too slow to run, and thus unsuited for ativizer, a conjunction, or all of the above, and
real-worldreal-timesystems. Hebrew suffixes often contain an accusative or a
Inthisworkweproposeanew"flippedpipeline" genitive. Fromasyntacticpointofview,theseare
approach, based on whole tokens, wherein a se- allverydifferentelementswithdistinctfunctions,
ries of expert classifiers each make a set of inde- and thus syntax parsers for MRLs have always
pendentpredictionsregardingthespace-delimited treatedtheprocliticsandsuffixesasseparateunits;
words, and then afterward those predictions are indeed,thisisprescribedbytheUniversalDepen-
synthesized into a single coherent and complete denciesstandard. Yet,inpractice,wepropose,the
morpho-syntacticanalysis,withthesegmentations syntactic roles of the proclitics and suffixes can
automaticallyinferredandgeneratedfromthatanal- be satisfactorily derived in a post-facto process,
ysis. Thissystemisalsodesignedforusewithout after the word-based syntactic analysis has been
external lexica or dependencies. We assess our performed.
new approach and demonstrate that it achieves a Supporting this stance, Goldman and Tsarfaty
newSOTAregardingdependencyparsingandPOS (2022)demonstratethatsubstantiallinguisticambi-
tagging,andnear-SOTAscoresonotherNLPtasks. guityexistsregardingthedeterminationofcorrect
segmentations. Accordingly, we may reason, an
2 ANewMRLParsingProposal artificial requirement to choose a single point of
segmentation may be reducing the system’s abil-
Asdetailedintheprevioussection,existingMRL
ity to correctly analyze the sentence, whereas a
parsingsystemssufferfromseveralprimaryshort-
whole-tokenapproachallowstheclassifiertomore
comings. Pipeline architectures suffer from the
flexiblyevaluatethesyntacticdependenciesofthe
compoundedoferrorsfromoneleveltothenext;
sentencewithoutfirstcommittingtospecificsub-
jointarchitecturesentailslowcomputationsoflat-
wordsegmentations.
ticescoveringallpermutations;andalmostallsys-
tems rely on external lexicons and other compo-
2.2 FlippedPipeline
nents. Isitpossibletoovercometheseissuesinan
MRL parsing system? We believe it is, by utiliz- Asnotedabove,jointpredictionarchitecturessuch
ingwhole-tokenprediction,aflippedpipeline,and as that proposed by More et al. (2019); Levi and
eliminating the lexicon. We elaborate on each of Tsarfaty(2024)dosidesteptheissueofpipelines,
theseelementsinthefollowingsections. buttheycomeatahighlatencycost,becausethey
requireprocessingsomanydifferentpermutations
2.1 Whole-tokenPrediction atonceviaanall-encompassinglattice.
Inordertoaddresstheissueofpropagatedpipeline In order to avoid this setback, we propose
errors, we propose shifting to classifiers that pre- a "flipped pipeline" approach consisting of two
dict morphological and syntactic functions on a stages: inthefirststageofthepipeline,dedicated
whole-tokenbasis,ratherthanonthebasisofmor- expertclassifierseachprovideonetypeoflinguis-
phologicalsegments. Thatis,theclassifiersshould tic prediction for the sentence; and in the second
relatedtoeachspace-delimitedtokenasanindivis- stage, these predictions are synthesized together
ible unit. By shifting to whole-token predictions, intoasinglecoherentparseofthesentence,adding
thereisnolongeranyneedtoperformsegmenta- in sub-token segmentations as relevant. We call
tionpriortomorphologicalandsyntacticanalysis, thisa"flippedpipeline"becauseitisthereverseof
thusside-steppingthesituationinwhichsegmenta- thetraditionalpipeline: insteadoffirstsegmenting
tionerrorsattheinitiallayerpreventthesubsequent the tokens and then predicting their morphologi-
layersfromsucceeding. calandsyntacticfunctionsonthatfoundation,we
Tobesure,thisisastrikingdeparturefromexist- firstpredictthemorphologicalandsyntacticfunc-
ingsyntacticmodelsforMRLs,allofwhichtreat tionsofthewholetokens,andthenwefigureout
syntacticdecisionsassomethingtobedecidedat the ideal distribution of the segmentations when
thelevelofthemorphologicalsegments. Andin- synthesizingthepredictionstogether.
deed, prima facia, the traditional morphological Thekeypointhereisthatratherthanaseriesof
segmentation approach is the more sensible lin- classifiersthatbuildononeanother,hereeachex-
guisticapproachtosyntacticparsing. Forinstance, pertclassifieroperatesindependently,basedsolely
Hebrewprocliticsmaycontainapreposition,arel- upontheBERTembeddingsoftheinputsentence.Onlyafterwardarethepredictionscombinedintoa worditselfismaskedorunknown. Thismeansthat
singlecoherentparseofthesentence. Thus,errors completelynovelwordusageswillstillbehandled
arenotpropagated,norisitnecessarytocontend withaplomb. Effectively,dispensingwiththelexi-
withheavylattices. conandusingBERT-likeencoderaloneresultsina
naturalpropensitytohandlecodeswitching.
2.3 EliminatingtheLexicon A further advantage of a lexicon-less architec-
tureisthatperformancedirectlycorrespondswith
Asnotedabove,existingjointMRLparsingarchi-
thatoftheunderlyingLLMencoder. Thismeans
tecturestypicallyrelyonanexternallexicograph-
that the release of larger or more advanced LLM
ical resource in order to determine the possible
willimmediatelytranslateintoanimprovementin
segmentation,morphologicalandsyntacticoptions
theaccuracyoftheparsingmodel(afterrerunning
foranygivenspace-delimitedtoken,andinorder
thefine-tuningoftheparsingclassifiers),because
tobuildthelattices(forthecaseofajointarchitec-
theparser’schoicesarenotconstrainedbyanycon-
ture). Primafacia,theuseofalexiconprovidesa
siderations other than the contextualized embed-
substantialboostofaccuracy,becauseitconstrains
dings themselves. We may therefore reasonably
thesystemfromveeringoffintocompletelyunten-
expecttheparser’saccuracytocontinuallyriseas
ableinterpretationsforthechosenwords. However,
new LLMs for MRLs are released, or as existing
this constraint also boomerangs against the sys-
techniquesareimproved,e.g.,withmoreoptimal
tem. Whenthereisanout-of-vocabularyword,or
tokenizers,suchasYehezkelandPinter(2023).
aninterpolatedwordfromanotherlanguage(code
switching), or a word used in a new and unusual 2.4 IntroducingDictaBERT-Parse
sense (e.g. as part of a slang idiom), the lexicon
WeherebyreleaseDictaBERT-Parsetothecom-
is helpless. Indeed, many parsing papers such
munity as a free and unrestricted tool for both
as Seker and Tsarfaty (2020); Levi and Tsarfaty
academic and commercial use. The general-
(2024)includesectionsdetailinghowmuchaccu-
purpose model, balancing accuracy and feasi-
racyislostwhenthelatticesare"uninfused",that
bility, is the BERT-base model2. Additionally,
is,whentheydon’tcontainthefullsetoflexicalen-
we release the BERT-large model3, for highest
triesneededtocovertheeffectiveuseofthewords
accuracy. Finally, we release a scaled-down
inthesentence,andtheseshownempiricaldrops
model,DictaBERT-Parse-tiny4,whichprovides
aresubstantial.
theidenticalfunctionalityatafractionofthemem-
Onthisbackdropwepresentourthirdandfinal
oryrequirementsandevenfasterspeed,andwith
key suggestion for a better MRL parser: discard
onlyaslightdropinaccuracy,forthosewhowish
thelexicon. Ifwecantrainamodelthatdoesnot
tointegrateHebrewtextparsingintolow-resource
rely on an external symbolic lexicon but on an
hardware.
LLMalone,thenwewillbeabletohandleforeign
andout-of-vocabularywordsgracefully. Thepossi- 3 ModelImplementation
bilityofdispensingwiththelexiconisespecially
Theoverarchingtaskisdefinedasfollows: Given
relevanttodaygiventheoptionofbuildingupona
aninputsequenceofwholetokensx ...x ,weaim
foundationofencodermodels,suchastheBERT 1 n
topredictatreestructureconnectings ...s seg-
model’sfamily. BERTmodelsaregenerallytrained 1 m
mentswithapredictedsetofrespectivefeaturesets
on huge corpus, including a wide range of differ-
f ...f foreachsegment. Notably,forMRLsitis
entgenres,andthustheyarenaturallyexposedto 1 m
often the case that m ≥ n. The model starts off
the foreign words and phrases that typically ap-
by feeding the sequence of whole tokens x ...x
pearwithintexts,whetherprize-winningproseor 1 n
into a pre-trained LLM encoder, and then feeds
down-to-earthsocialmedia. Asaresult,asyntactic
each contextualized embedding c ...c into indi-
parserbasedonBERTalonewouldnotbethrown 1 n
vidualexpertclassifiersforeachofthefollowing
offbysuchforeigninterpolations;onthecontrary
itwouldnaturallyleveragetheBERTembeddings 2https://huggingface.co/dicta-il/
for the foreign words in order to parse them in a dictabert-parse
reasonable manner. Furthermore, BERT models
3https://huggingface.co/dicta-il/
dictabert-large-parse
excelatproducingpreciseandappropriateembed-
4https://huggingface.co/dicta-il/
dings based on the context alone, even when the dictabert-tiny-parsetasks:5 DependencyTreeParsing,Lemmatization, Architecture. Foreachcontextualizedembed-
Morphological Functions Disambiguation, Mor- ding c , we train the model’s LM-head layer to
i
phologicalFormSegmentation,andNamedEntity predict the most likely token from within the en-
Recognition. We then synthesize the outputs of coder’s vocabulary to serve as the lemma for the
theexpertclassifiersintoaunifiedUDanalysis. In corresponding word in the sentence; the training
whatfollows,weelaboratefirstontheimplementa- is supervised with a lemmatized corpus. In our
tiondetailsofeachoftheseclassifiers,andthenwe model,weusedaBERTencoder,whichwaspre-
describeoursynthesisprocedure(seeSection3.6). trainedusingtheMasked-Language-Model(MLM)
objective. Wecontinuetrainingthemodelwitha
3.1 DependencyTreeParsingExpert similar objective, but instead of masking tokens
andpredictingthosesametokens,wetrainittopre-
ThisexpertclassifieraimstosolvetheDependency
dictthecorrespondinglemmas. Forwordswhose
Tree Parsing task: Given a sentence composed
lemmaisnotpresentintheBERTvocabulary,we
of whole tokens x ...x the objective is to deter-
1 n
trainthemodeltopredictaspecial[BLANK]token.
mine, for each whole token in a given sentence,
which whole token it grammatically depends on Thelemmatizationtaskpresentsachallengefor
(itshead)accordingtotheUniversalDependencies lexicon-lessmodelssuchasours,especiallywhen
(UD)standard,andalsotodetermineitssyntactic dealingwithMRLsandtheirhighdegreeofmor-
relationshipwiththathead. phologicalfusion,suchthatthelemmaisoftennot
Architecture. Givenasetofcontextualizedem- a substring of the corresponding word. Existing
beddings c ...c of the whole tokens in a given lemmatizationmodelsforMRLsrelyontheability
1 n
sentence, we employ a self-attention mechanism, to perform a lookup of any given word within a
predictingforeachwholetokentheprobabilityof lexicon,andtothusdeterminethepossiblelemmas
any other whole token in the sentence being its from which to choose. However, in our case, we
dependent head. We use single-head scaled dot- cannotperformanysuchlookup.
productattentionforcomputingforeachtokenpo- The key intuition with which we compensate
sitioniitsdependentheadh asfollows: forthelackoflexiconisthatHebrewlemmasare
i
almostalwaysvalidwordsthemselves, andthese
cW ·(cW )T wordsgenerallyappearwithgreaterfrequencythan
h = argmax
√q k
i i d head thecorrespondinginflectedforms. Thismeansthat
foranymoderatelyfrequentinflectedforminthe
WhereW andW arethequeryandkeytrans-
q k BERTmodel’svocabulary,wecanexpectthatthe
formationmatrices,crepresentsamatrixofc ...c
1 n underlyinglemmawillexistinthemodel’svocabu-
andd isthedimensionoftheattentionhead.
head laryaswell. Indeed,thefoundationBERTmodel
Following the work of Kiperwasser and Gold-
weusedhasaratherlarge128,000tokenvocabu-
berg(2016),afteridentifyingthedependentheads
lary,andinourtestsontypicalHebrewcorporawe
h ...h , wethenpredictthesyntacticfunctionof
1 n find that this vocabularly has a 98% coverage of
each relation by concatenating every [c ;c ] and
i hi thecorrespondinglemmas.
feeding that into a linear classifier. During infer-
ence, we replace the argmax with an MST algo-
3.3 MorphologicalFunctionsExpert
rithmtoconstructavaliddependencytreebefore
predictingthesyntacticfunctionofeachrelation. Thisexpertclassifieraimstosolveamorphological
taskwhere,givenawholetokenx ,weaimtotag
i
3.2 LemmatizationExpert thePOSandfine-grainedmorphologicalfeaturesof
thewholetokenwithinaspecificcontextx ...x .
Thisexpertclassifieraimstosolvealemmatization 1 n
Specifically,themodelpredictsthepart-of-speech
task, wherein, givenawhole tokenx , we aimto
i
ofthemainlexicalvalue,aswellasgender,num-
identify the primary lemma of the whole token
ber, person, and tense, wherever relevant. Since
whenitappearswithinaspecificcontextx ...x .
1 n
space-delimitedtokensinMRLscontainadditional
5Asageneralrule,incaseswherethewholetokenisbro- information,thisclassifierisalsotaskedwithiden-
kenupintomultiplewordpieces,weperformthepredictions tifying proclitic functions, and determining if a
onlyonthefirstwordpiece.Otheroptionsforpoolingword-
suffixisappendedtotheword,andifso,thefunc-
piecesareofcourseconceivable,butempiricallythismethod
wasprovensuccessfulforourtask. tion it serves, as well as its gender, number, andperson. AllofthelabelsarebasedontheUDtag- letter-groupswhichserveinaprocliticrole.7 Dur-
gingschema6. Asimilarapproachwasemployed inginference,welimitthepredictionstovalidsets
byKleinandTsarfaty(2020)forpredictingmulti- ofletter-groupsgiventheinitiallettersoftheword.
pletagsforeachtoken. However,ourapproachis
3.5 Named-Entity-Recognition(NER)Expert
tailoredspecificallytothemorphologicalstructure
ofwordsandthelexicalnatureofUD,e.g.,recog- This expert was trained for the named-entity-
nizingthattheappropriateproclitictagsdifferfrom recognition task, wherein, given a sentence com-
thoseapplicabletothemainlexicalvalueitself. posedofwholetokensx 1...x n,weaimtoidentify
Architecture. Given a contextualized embed- andclassifythenamedentitiesinthesentence. We
ding c , we feed it through 5 separate classifiers, employtheBIOtaggingmethod,generating27la-
i
itemized below; examples are based on the input bels from 13 classes (each class has a B and an I
word ["and from his house"]: label,plusoneOlabel).
◦ Prediction of the POS for the main lexical Architecture. Given a contextualized embed-
value. Inourexample,houseisaNOUN. dingc i,alinearclassifieroutputsaprobabilityvec-
◦ Prediction of the proclitic functions (can be torforeachofthepossiblelabels.
multiple or none). In our example, the word has
3.6 FromExpertClassifierstoaUDTree
bothaCCONJ("and")andADP("from")prefix.
With the flipped pipeline approach, the full UD
◦ Prediction of the fine-grained morphologi-
morpho-syntacticstructureisnotdirectlyavailable.
calfeaturesoftheword(gender,number,person,
Thisisbecauseeachoftheexpertclassifiersmakes
tense). Here,houseissingularandmasculine.
predictionsonawholetokenbasis. Inordertore-
◦ Prediction of whether there is a suffix and
constructthefullUDanalysisofagivensentence,
which function it serves. In our example, there
forthepurposeofevaluationoradownstreamap-
isapossessive(ADP+PRON)suffix("his").
plication,wecollectthepredictionsfromeachof
◦Predictionsoffine-grainedfeaturesofthesuf-
the expert classifiers, and synthesize the full UD
fix (gender, number, and person), if the previous
outputfromacombinationofthepredictions. The
classifierpredictedasuffix. Inourexample,hisis
processconsistsof3phases,whichwedemonstrate
singular,masculine,andthirdperson.
inFigure1. Herewediscussthe3phasesinturn.
Westartbyattachingthepredictedlabelstothe
3.4 MorphologicalFormSegmentationExpert
respectivewholetokens. First,wetakethepredic-
Thisexpertclassiferaimstosolveamorphological
tions from the syntax classifier (dependency tree
taskwhere,givenawholetokenx withacontext
i parsing) and build an initial whole-token depen-
x ...x ,weaimtoidentifytheactualstringsthat
1 n dencytree. Wethencontinuewiththelemmapre-
functionasprocliticsatthebeginningoftheword.
dictions, followed by the morphological predic-
For example, the word ("that went")
tionsofthewholetoken. Atthispoint,wehavea
would be segmented into ("that") and
parsedsentenceonawhole-tokenbasis.
("went"). Note that this expert does not segment
Inthenextstep,weseparatetheprefixsegments
suffixes, if any, at the ends of the words (instead,
from the main words using the predictions from
suffixesarepredictedbytheMorphologicalFunc-
thesegmentationexpertclassifier. Wethenassign
tionsExpertclassifier,detailedabove).
themorphologicalpropertiesofthesegmentedpre-
Thisexpertisrequiredinadditiontotheexpert
fixes based on the prefix labels predicted by the
described in Section 3.3 since Hebrew does not
morphologyexpertclassifierforthecorresponding
haveaone-to-onefunctionmappingbetweenpro-
wholetoken. Thelemmasfortheprefixesareau-
clitic functions and proclitic letters. An example
tomaticallyassignedtobeequivalenttotheletters
forthiswouldbetheimplicitdefinitearticle—a
of the segments. We then assign the dependency
commonfeatureofHebrew—whichisafeature
relationstothesegmentedprefixesbaseduponthe
representedviavocalizationandnotbyaletter.
relationsandfunctionsofthewholetokensofthe
Architecture. Given a contextualized embed-
sentences,usingrulescuratedbasedonUDlabels.
ding c , we feed it through 8 classifiers in order
i Finally, we separate the suffix segments from
to predict the probability of each of the possible
the main words using the suffix labels predicted
6TheUDannotationguidelinescanbefoundhere:https: 7ForHebrew,theseare8types.Ingeneralthenumberof
//universaldependencies.org/guidelines.html typesforalanguagecanbeobservedfromtheUDscheme.bythemorphologicalexpertclassifier. Usingthese withallthetokens(theLASstandardDependency
predictions, we automatically determine lemma ParsingMetrics),andthesecondignoringanyarc
anddependencyrelationofthesegmentedsuffix. withpunctuation(Dep-Punc). ForNER,wereport
Pseudocode for this synthesis function is pro- thewhole-tokenbasedF1score.
vided in Appendix B; full implementation of the We compare our results to YAP13 (More et al.,
algorithminpythonisavailablewiththehugging- 2019), PtrNetMD (Seker and Tsarfaty, 2020)
facemodel. AlephBERTGimmel (Gueta et al., 2023), Levi-
Tsarfaty(LeviandTsarfaty,2024)14,mT5(were-
4 ExperimentsandResults
portscoresforthemT5-smallmodel,whichisthe
closest model in size to the BERT models used
4.1 FoundationModel
here)(Eyaletal.,2023),Stanza(Qietal.,2020)15
Thefoundationmodelthatweassumeinthiswork andTrankit(Nguyenetal.,2021).15
isDictaBERT(Shmidmanetal.,2023),theSOTA
BERTmodelformodernHebrew. Wefine-tuneand 4.4 ResultsandAnalysis
evaluateourparsingmodeluponthreesizesofthe
Table1showstheperformancescoresforallevalu-
DictaBERTfoundation: BERT-tiny8,BERT-base9,
atedtasks. Giventhenon-standardarchitectureof
andBERT-large10.
our model, with its flipped pipeline, and with its
predictionsonawhole-tokenbasis,ratherthanon
4.2 Data
amorphological-wordbasis,andwithoutanylexi-
Inthissectionwedescribethetrainingcorporaused
conwhatsoever,onemighthavethoughtthataccu-
inourexperimentstotraineachoftheexpertsinour
racyofthefull-fledgedtreewouldsuffer. However,
model. WeusetheUDHTBTreebank(Sadeetal.,
theoppositeisthecase. DictaBERT-Parse-large
2018),theNEMOdatasetpresentedbyBareketand
setsanewSOTAforHebrewdependencyparsing
Tsarfaty(2021),andanadditionalUDcorpusand
(forbothUASandLASwithpunctuation,andfor
NERcorpusfromtheIAHLT11.Table2provides
UASwithoutpunctuation),andalsoforPOStag-
thesizeofeachofthecorpora,andno.ofepochs
ging.
performed on each during joint training.12 For
Significantly, even if we ignore the BERT-
hyperparametersseeAppendixA.
large, DictaBERT-Parse-base remains the high-
est performing model for most of these tasks;
4.3 MetricsandEvaluation
overall,wefindthatDictaBERT-Parse-baseand
Wecomparethesuccessofourmodeltopreviously
even DictaBERT-Parse-tiny produce competi-
reportedSOTAscoresoneachofthetasks.
tive scores across the board on Hebrew NLP
Formorphology,segmentationanddependency
tasks. AlephBERTGimmel remains the highest-
parsing,wereportthealignedMulti-Setscoreson
performingmodelforfine-grainedmorphologyfea-
theUDTreebankasreportedbySekeretal.(2021)
tureprediction,whileLevi-Tsarfatymaintainsthe
and Levi and Tsarfaty (2024). For dependency
highestLASscorefordependencyparsing(when
parsing, we report two sets of scores — the first
punctuationisignored).
When comparing inference times between our
8https://huggingface.co/dicta-il/
dictabert-tiny modeltothepreviousSOTAondependencypars-
9https://huggingface.co/dicta-il/
ing,ourmodeltakesonaverage0.0019s/0.0016s
dictabert-base
(baseandtiny,respectively)pertrainingexample,
10https://huggingface.co/dicta-il/
dictabert-large whereasthepreviousSOTAmethod(LeviandTsar-
11WewouldliketoexpressourthankstoIAHLTforthis
taggedcorpus.Formoreinformationregardingtheresources 13ItshouldbenotedthatYAP’sdependencyscoreswere
curatedandmadeavailablebyIAHLT,see:https://github. publishedbasedontheHebrewSPMRLtreebankratherthan
com/IAHLT/iahlt.github.io/blob/main/index.md UD;thescoreswouldpresumablybeafewpercenthigheron
12TheUDHTBTreebankonlycontains5Ksentences,and UD.Incontrast,YAP’ssegmentationandPOSscoreswere
asdescribedinTable2ourmodelwastrainedonasignificantly evaluatedonUDinEyaletal.(2023),andwereportthem
largercorpusthantheUDHTBcorpus.Theextendedcorpus herebasedonthatevaluation.
includedaslightlydifferenttaggingmethodology,andforthe 14Dependencyscoresarereportedintheirpaperwithout
NERalsoincludedmorecategoriesoftags.Nevertheless,in punctuationonly.Weexpressourthankstotheauthorsofthe
ordertoachievemaximalalignmentwiththeexpectationsof paperforcollaboratingwithustocomputethecorresponding
theUDgoldcorpus,afterwetrainedourmodelonourfull "withpunctuation"scores,asreportedinthetablehere.
corpus,wecontinuedtrainingforseveraladditionalepochs 15ThescoreswereportforStanzaandTrankitweretaken
onlyontheUDTreebankandNEMOcorpus. fromLeviandTsarfaty(2024).Figure1: InthisfigurewedemonstratetheUDsynthesisdescribedinsection3.6. Hebrewisreadfromrightto
left,sothewordbubblesaretobereadinthatorder. Wedemonstrateaparseofthesentence"ThebookthatIread
interestsme",andwepresentthestepsbywhichitisbrokendownintoitsfinalUDanalysis. Inthefirstrow,we
presenttheinitialwhole-tokenbreakdownofthesentence. Thesecondrowincorporatesthelabelspredictedby
threeexpertclassifiers: syntaxdependencies(darkgreen);lexemes(blue),andmorphologicalfeatures(Morph1,
orange). For readability, regarding the morphology classifier, we only print the POS; however, in practice the
classifierpredictsfine-grainedmorphologicalfeaturesaswell. Inthethirdrow,wepresentthepredictionsofthe
expertsegmentationclassifier,whichseparatestheprefixesintotheirowntokens(purple),pairedwiththeoutput
fromthemorphologyexpert(Morph2,lightgreen). Thesyntacticrelationstothesegmentedprefixesareadded
automaticallybythesynthesisprocedure(red). Thebottomrowdemonstratesthefinalstage,inwhichwesegment
anysuffixesintotheirowntokensbasedonthesuffixfeaturespredictedbytheexpertmorphologyclassifierforthe
corresopndingwholetokens(Morph3,greenhighlight). Thesyntacticfunctionsandrelationsofthesuffixedtokens
arethenautomaticallyaddedbyarule-basedalgorithm(red).Morphology Dependency Dep-NoPunc
Seg POS Features UAS LAS UAS LAS NER
YAP 93.64 90.13 - 75.73 69.41 - - -
PtrNetMD 94.74 91.3 - - - - - -
AlephBERT 98.2 96.20 93.05 - - - - 83.62
AlephBERTGimmel 98.09 96.22 95.76 - - - - 86.26
Levi-Tsarfaty 97.71 94.41 - 84.6 81.4 88.9 85.4 -
mT5-Small 94.83 94.55 - - - - - 66.74
Stanza 89.51 - - - - - 67.4 -
Trankit 95.2 - - - - - 83.6 -
DictaBERT-Parse-tiny 97.18 96.62 94.6 87.2 82.6 86.9 81.9 80.3
DictaBERT-Parse-base 97.89 97.26 95.58 89.1 84.7 88.7 84.1 83.8
DictaBERT-Parse-large 97.88 97.35 95.46 89.5 85.4 89.2 84.6 84.1
Table1: EvaluationofourmodelversuspreviouslyreportedscoresofotherHebrewparsers,asdetailedinsection4.
TheSegmentation&POSscoresreportedarealignedMulti-Setscores. Forsyntaxdependencieswereportboth
thelabeled(LAS)andunlabeled(UAS)alignedMulti-Setscores, thefirstscore(Dependency)includingallof
thetokensinthesentence(includingpunctuation),andthesecondscore(Dep-NoPunc)ignoringanyarcswith
punctuation. ForNER,wereportthetoken-basedF1scoreonallthelabels.
#Sentences #Words #Epochs POSscores: WeevaluatethePOSassignedto
Morph 40K 975K 15 the primary segment of each word. We compute
Dep 40K 975K 15 theMacro-F1scoreandtheaccuracyscore.
Seg 52K 1.2M 20 DependencyTreescores: WecomputetheLAS
Lex 180K 5M 3 andUASscoresonwholetokens,beforethemor-
NER 112K 2.8M 15 phologicalformsegmentation. Sincethisenforces
thatthepredictedtreewillhavethesamenumber
Table2: Sizeofthecorporausedtotraineachexpert
ofarcsasthegoldtree,wedon’tneedtocompute
classifier,andno. epochsperformedoneachcorpus-
thealignedMultiSetscoreandinsteadjustcompute
Morph(MorphologicalDisambiguation),Dep(Depen-
accuracy(asisdoneinotherMRLparsingstudies).
dency Tree Parsing), Seg (Prefix Segmentation), Lex
(Lemmatization),NER(NamedEntityRecognition). Akeyadvantageofthisscoringmethodisthat
accuracy on any given task is independent from
allothertasks,thusobviatingtheneedfororacle-
faty,2024)reported0.257spertrainingexample;
basedevaluations,asinLeviandTsarfaty(2024).
thusourmodel’sspeedimprovementisover100x.
Wepresentresultsusingthisscoringmethodfor
4.5 NewScoringMethod POSandDependencyparsingareshowninTable3.
To accompany our new whole-token approach InordertocomparetoLevi&Tsarfatyusingthis
to MRL parsing, we also propose a new scor- method, we evaluate their output as follows: We
ingmethodformorphologyanddependencytree group each set of segmented tokens into groups
benchmarkingthatdoesnothingeonspecificseg- according to the whole tokens in the gold corpus
mentation point choice. As demonstrated herein, (the segmentation process only breaks up tokens,
theprimaryparsingchallengeforMRLsremains and never combines two separate whole tokens).
thewhole-tokenunit,whereaspredictionsregard- Weconsiderthepredictionforanygivenwordas
ingprocliticsandsuffixescanalmostalwaysbede- correct if any of the sub-tokens of a whole token
rivedpost-factofromthewhole-tokenpredictions. pointtoanyofthesub-tokensofthecorresponding
Therefore,weproposecomputingbenchmarkson headwordinthegoldtree.
awhole-tokenbasis,withoutseparatelyevaluating Weseethatonthisgranularity,ourwholetoken
the properties predicted to each segmented pro- approachachievessuperiorresults;thisshouldul-
clitic/suffix. Wedemonstratetheapplicationofthis timately result in less error propagation from the
new“whole-token”scoringmethodtoourevalua- parser to downstream applications that leverage
tionsofPOSscoresanddependencytrees. thesestructures.POS Dependency 7 EthicsStatement
Macro-F1 Acc UAS LAS
The foundation of the parser released herein is a
Levi-Tsa 88.2 94.74 82.1 77.5
BERTmodelwhichwastrainedonalargecorpus
DB-tiny 91.7 96.8 89.7 85.2
ofHebrewtext. TheBERTmodelwastrainedon
DB-base 93.5 97.2 91.3 87.5
the textual corpus as is, without any editing, fil-
DB-large 93.1 97.0 91.6 87.9
tering, or censoring. This means that the parser
Table3: Resultsusingthenew"whole-token"scoring mayabsorbedanybiasespresentwithinthecorpus
method;"Levi-Tsa"=LeviandTsarfaty(2024) itself. ThisissueisespeciallyrelevantforHebrew
parserswhenitcomestoissuesofgenderbias. In
Hebrew,verbsgenerallyhavetwodifferentwords
5 Conclusion
forthemasculineandthefeminine,yetinpractice
Inthispaperwehaveproposedanewsolutionfor the two words will be written with the same se-
MRLparsing,toovercomethedeficienciesofcur- quenceofletters(thedifferencebetweenthewords
rentapproaches(pipelineswitherrorpropagation, isindicatedviathediacritics;yetthediacriticsare
joint with long latencies, and cumbersome setup generallyomittedinwrittenHebrewtexts). Thus,
andintegration). Ourkeyinnovationisa"flipped aspartofthemorphologicaltaggingtask,Hebrew
pipeline", in which the multiple layers involved parsersmustpredictwhethertheseambiguouswrit-
inMRLparsingarepredictedindependentlyona tenwordsareinfactmasculineorfeminineforms,
whole-tokenbasis,andthenlatersynthesized. Our giventhecontext. Insuchcases,ourparserisliable
architectureprovidesasubstantialboostinusabil- tomakedecisionsbaseduponstereotypicalgender
ity over previous parsers, both in terms of speed roles,totheextentthattheserolesarereflectedby
andintermsofeaseofinstallationandintegration, thetextsinthecorpus.
while also setting a new SOTA for Hebrew POS
Acknowledgements
tagginganddependencyparsing. Ourarchitecture
doesnotrelyonlexiconsnoronanyotherlanguage-
Theworkofthesecondauthorhasbeensupported
specificlinguisticresources,pavingthewayforit
byISFgrant2617/22. Theworkofthelastauthor
tobeadaptedtootherMRLsaswell. Werelease
hasbeenfundedbytheIsraeliMinistryofScience
ournewparsingmodelstotheNLPcommunityon
andTechnology(MOST)grantNo.3-17992,and
huggingface,underaCCBY4.0license.
byanIsraeliInnovationAuthority(IIA)KAMIN
grant,forwhichwearegrateful.
6 Limitations
Oneoftheprimaryachievementsofthis"Without
Tears"projectwasthereleaseofamorphosyntactic References
parserandlemmatizerwithinasinglehuggingface
DanBareketandReutTsarfaty.2021. Neuralmodel-
module,withouttheneedforanyexternallinguistic ing for named entities and morphology (NEMO2).
resources,includinglexicons. Inherentwithinthis, TransactionsoftheAssociationforComputational
Linguistics,9:909–928.
however, is a substantial limitation regarding the
model’s ability to predict lemmas, because it has
ShayB.CohenandNoahA.Smith.2007. Jointmorpho-
nomechanismtopredictlemmasthatarenotfound logicalandsyntacticdisambiguation. InProceedings
withthevocabularyoftheunderlyingBERTmodel. ofthe2007JointConferenceonEmpiricalMethods
inNaturalLanguageProcessingandComputational
In practice, the model is still able to accurately
NaturalLanguageLearning(EMNLP-CoNLL),pages
predictlemmasformostwords,becauseweusea
208–217,Prague,CzechRepublic.Associationfor
BERTmodelwithafairlylargevocabulary(128K), ComputationalLinguistics.
andbecause,followingZipf’slaw,theoverwhelm-
Marie-Catherine de Marneffe, Christopher D. Man-
ingmajorityofwordsthatappearinatexttendto
ning,JoakimNivre,andDanielZeman.2021. Uni-
be drawn from the same pool of frequent words
versal Dependencies. Computational Linguistics,
whose lemmas are covered by the BERT vocabu- 47(2):255–308.
lary. Nevertheless,whenitcomestolessfrequent
MatanEyal,HilaNoga,RoeeAharoni,IdanSzpektor,
words-wordswhichlexicon-basedparserscaneas-
andReutTsarfaty.2023. Multilingualsequence-to-
ily lemmatize based on a single lexicon lookup -
sequencemodelsforHebrewNLP. InFindingsof
thepresentmodelislikelytofalter. theAssociationforComputationalLinguistics: ACL2023,pages7700–7708,Toronto,Canada.Associa- Minh Van Nguyen, Viet Dac Lai, Amir Pouran Ben
tionforComputationalLinguistics. Veyseh,andThienHuuNguyen.2021. Trankit: A
light-weighttransformer-basedtoolkitformultilin-
YoavGoldbergandReutTsarfaty.2008. Asinglegen- gualnaturallanguageprocessing.
erativemodelforjointmorphologicalsegmentation
and syntactic parsing. In Proceedings of ACL-08: PengQi,YuhaoZhang,YuhuiZhang,JasonBolton,and
HLT,pages371–379,Columbus,Ohio.Association Christopher D. Manning. 2020. Stanza: A python
forComputationalLinguistics. naturallanguageprocessingtoolkitformanyhuman
languages. InProceedingsofthe58thAnnualMeet-
OmerGoldmanandReutTsarfaty.2022. Morphology ingoftheAssociationforComputationalLinguistics:
withoutborders: Clause-levelmorphology. Transac- SystemDemonstrations,pages101–108,Online.As-
tionsoftheAssociationforComputationalLinguis- sociationforComputationalLinguistics.
tics,10:1455–1472.
Shoval Sade, Amit Seker, and Reut Tsarfaty. 2018.
SpenceGreenandChristopherD.Manning.2010. Bet- TheHebrewUniversalDependencytreebank: Past
terArabicparsing: Baselines,evaluations,andanal- present and future. In Proceedings of the Second
ysis. InProceedingsofthe23rdInternationalCon- WorkshoponUniversalDependencies(UDW2018),
ferenceonComputationalLinguistics(Coling2010), pages133–143,Brussels,Belgium.Associationfor
pages394–402,Beijing,China.Coling2010Orga- ComputationalLinguistics.
nizingCommittee.
WolfgangSeekerandÖzlemÇetinog˘lu.2015. Agraph-
Eylon Gueta, Avi Shmidman, Shaltiel Shmidman, basedlatticedependencyparserforjointmorpholog-
CheynShmuelShmidman,JoshuaGuedalia,Moshe ical segmentation and syntactic analysis. Transac-
Koppel, Dan Bareket, Amit Seker, and Reut Tsar- tionsoftheAssociationforComputationalLinguis-
faty.2023. Largepre-trainedmodelswithextra-large tics,3:359–373.
vocabularies: Acontrastiveanalysisofhebrewbert
modelsandanewonetooutperformthemall. Amit Seker, Elron Bandel, Dan Bareket, Idan
Brusilovsky,RefaelShakedGreenfeld,andReutTsar-
EliyahuKiperwasserandYoavGoldberg.2016. Simple faty.2021. Alephbert:ahebrewlargepre-trainedlan-
andaccuratedependencyparsingusingbidirectional guagemodeltostart-offyourhebrewnlpapplication
LSTMfeaturerepresentations. Transactionsofthe with.
Association for Computational Linguistics, 4:313–
327. Amit Seker and Reut Tsarfaty. 2020. A pointer net-
work architecture for joint morphological segmen-
StavKleinandReutTsarfaty.2020. Gettingthe##life tation and tagging. In Findings of the Association
outofliving:Howadequateareword-piecesformod- forComputationalLinguistics: EMNLP2020,pages
ellingcomplexmorphology? InProceedingsofthe 4368–4378,Online.AssociationforComputational
17th SIGMORPHON Workshop on Computational Linguistics.
ResearchinPhonetics,Phonology,andMorphology,
pages 204–209, Online. Association for Computa- ShaltielShmidman,AviShmidman,andMosheKoppel.
tionalLinguistics. 2023. Dictabert: A state-of-the-art bert suite for
modernhebrew.
AmrithKrishna,AshimGupta,DeepakGarasangi,Pa-
vankumarSatuluri,andPawanGoyal.2020a. Keepit ReutTsarfaty.2006. Integratedmorphologicalandsyn-
surprisinglysimple: Asimplefirstordergraphbased tacticdisambiguationforModernHebrew. InPro-
parsingmodelforjointmorphosyntacticparsingin ceedingsoftheCOLING/ACL2006StudentResearch
Sanskrit. InProceedingsofthe2020Conferenceon Workshop,pages49–54,Sydney,Australia.Associa-
EmpiricalMethodsinNaturalLanguageProcessing tionforComputationalLinguistics.
(EMNLP),pages4791–4797,Online.Associationfor
ComputationalLinguistics. Reut Tsarfaty, Shoval Sadde, Stav Klein, and Amit
Seker. 2019. What’s wrong with Hebrew NLP?
AmrithKrishna,BishalSantra,AshimGupta,Pavanku- and how to make it right. In Proceedings of the
mar Satuluri, and Pawan Goyal. 2020b. A graph- 2019 Conference on Empirical Methods in Natu-
based framework for structured prediction tasks in ralLanguageProcessingandthe9thInternational
Sanskrit. ComputationalLinguistics,46(4):785–845. JointConferenceonNaturalLanguageProcessing
(EMNLP-IJCNLP): System Demonstrations, pages
DanitYshaayahuLeviandReutTsarfaty.2024. Atruly 259–264,HongKong,China.AssociationforCom-
jointneuralarchitectureforsegmentationandpars- putationalLinguistics.
ing.
ShakedYehezkelandYuvalPinter.2023. Incorporating
AmirMore,AmitSeker,VictoriaBasmova,andReut contextintosubwordvocabularies. InProceedings
Tsarfaty. 2019. Joint transition-based models for ofthe17thConferenceoftheEuropeanChapterof
morpho-syntactic parsing: Parsing strategies for theAssociationforComputationalLinguistics,pages
MRLsandacasestudyfromModernHebrew. Trans- 623–635,Dubrovnik,Croatia.AssociationforCom-
actionsoftheAssociationforComputationalLinguis- putationalLinguistics.
tics,7:33–48.A Appendix: Hyperparameters
The expert classifiers of our model are all linear
classifiers, and thus their size is predetermined,
sincetheysimplymapfromthehiddendimension
size of the base BERT model to a set number of
labels.
For the Dependency Tree Parsing expert, we
hadtochoosethedesiredattention-headdimension.
We attempted sizes 64, 128, 256, 512, and 768,
and found that after 128 we no longer saw any
improvementinaccuracy;thus,wechoseasizeof
128.
Wedescribethelistofthehyperparameterswe
usedfortraininginTable4.
WetrainedthemodelonasingleNVIDIARTX
3090. Thetotaltraintimeforthelargemodelwas
24.5 hours, for the base model 13 hours, and for
thetinymodel4.5hours.
We evaluated the inference times of the mod-
elsusingatestsetofsentencesofvaryinglengths,
rangingfrom16to256tokens. Forasetof4000
sentencesprocessedonanNVIDIARTX4090with
abatchsizeof8,thebasemodelcompletedinfer-
enceofall8000sentencesin15.4seconds(0.0019
seconds per sentence), while the tiny model re-
quiredonly12.8seconds(0.0016secondspersen-
tence). When processing a smaller batch of 500
sentencesona32-coreCPUwithabatchsizeof1,
thebasemodelcompletedinferenceofall500sen-
tencesin35.4seconds,comparedto24.3seconds
forthetinymodel.
LearningRate 5e-6(5e-5fortiny)
Optimizer AdamW
WarmupSteps 5000
BatchSize 8
SyntaxHeadSize 128
Table4: Descriptionofthehyperparametersusedwhen
trainingourmodel.B Appendix: PseudocodeforSynthesisofExpertClassifiersintoUnifiedUDTree
Belowispseudocodeforourprocesswhichsynthesizesthepredictionsfromthemultipleexpertclassifiers
into a single UD Tree. We omit the output from the NER expert, since the UD tree doesn’t integrate
namedentities.
Theinputtothefunctionisalistofwholetokens,andthepredictionsforeachtokenfromthevarious
experts: deps(see3.1),morphs(see3.3),segs(see3.4),lemmas(see3.2).
1: functionCONVERTOUTPUTTOUD(tokens,deps,morphs,segs,lemmas)
2: output ← []
3: fori = 1tolength(tokens)do
4: token ← tokens[i]
5: procliticFunctions ← morphs[i].procliticFunctions
6: foreachprefixinsegs[i]do
7: pos ← CHOOSEPREFIXFUNCTION(prefix,procliticFunctions)
8: depHead ← tokens[i]
9: ifprefix.endswith(“shin”)then
10: depFunc ← “mark”
11: ifmorphs[i].pos ̸= “VERB”then
12: depHead ← deps[i].head
13: endif
14: elseifprefix = “vuv”then
15: depFunc ← “cc”
16: ifdeps[i].func ∈/ {‘conj’,‘acl:recl’,...,(seenote1)}then
17: depHead ← deps[i].head
18: endif
19: else
20: depFunc ← “case”
21: ifmorphs[i].pos ∈/ { ‘ADJ’,‘NOUN’,‘PROPN’,‘PRON’,‘VERB’}then
22: ifdeps[i].func ∈ {‘aux’,‘det’,...,(seenote2)}then
23: depHead ← deps[i].head
24: endif
25: endif
26: ifprefix = “heh”andpos = “DET”then
27: depFunc ← “det”
28: endif
29: endif
30: lemma ← prefix
31: features ← “”
32: output.append({(prefix,lemma,pos,features,depHead,depFunc)})
33: token ← token[length(prefix) :]
34: endfor
35: if“heh” ∈/ segs[i]and“DET” ∈ procliticFunctionsthen
36: output.append({ImplicitHehEntry})
37: endif
38: output.append({token,lemmas[i],morphs[i].pos,morph[i].features,deps[i]})
39: ifmorphs[i].hasSuffixthen
40: output[−1].token ← lemmas[i]
41: {suffix,lemma} ← GETSUFFIXTOKEN(morph[i].suffixFeatures)
42: pos ← morphs[i].suffixPos
43: features ← morphs[i].suffixFeatures
44: depHead ← tokens[i]45: ifmorphs[i].pos ∈ {‘ADP′,‘NUM′,‘DET′}then
46: depFunc ← deps[i].func
47: depHead ← deps[i].head
48: output[−1].depHead ← suffix
49: output[−1].depFunc ← “case”
50: elseifmorphs[i].pos = “VERB”then
51: depFunc ← “obj”
52: else
53: depFunc ← “nmod:poss”
54: output.append({PossesiveEntry})
55: endif
56: output.append({suffix,lemma,pos,features,depHead,depFunc})
57: endif
58: endfor
59: returnoutput
60: endfunction
1The full list is ["conj", "acl:recl", "parataxis", "root", "acl", "amod", "list",
"appos", "dep", "flatccomp"].
2The full list is ["compound:affix", "det", "aux", "nummod", "advmod", "dep", "cop",
"mark", "fixed"]
ThefunctionChoosePrefixFunctionusesabuilt-intablespecifyingforeachprefix-letter-prediction
thepossibleprocliticfunctionswhichcanbeassignedtoit,choosingthecorrectoneusingthepredictions
fromthemorphologicalfunctiondisambiguationexpert.
ThefunctionGetSuffixTokendeterminesthetokenstringandlemmausingapredefineddictionary
mappingfromthesuffixfeaturestotherelevantstrings.
We should note that in this pseudocode the values we assign to the dependency head entries are
symbolic,intendedtoexplaintothereaderwhichvalueitrepresents. Theactualimplementationrequires
carefuluseofindices,makingsurethateachindexinthewholetokenlistpointstothecorrectindexin
thesegmentedlist.