VideoMamba: State Space Model for Efficient
Video Understanding
Kunchang Li2,3,1♠, Xinhao Li4,1♠, Yi Wang1♡, Yinan He1
Yali Wang2,1, Limin Wang4,1, and Yu Qiao1
1 OpenGVLab, Shanghai AI Laboratory
2 Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences
3 University of Chinese Academy of Sciences
4 State Key Laboratory for Novel Software Technology, Nanjing University
https://github.com/OpenGVLab/VideoMamba
Abstract. Addressing the dual challenges of local redundancy and
globaldependenciesinvideounderstanding,thisworkinnovativelyadapts
theMambatothevideodomain.TheproposedVideoMambaovercomes
the limitations of existing 3D convolution neural networks and video
transformers. Its linear-complexity operator enables efficient long-term
modeling, which is crucial for high-resolution long video understanding.
ExtensiveevaluationsrevealVideoMamba’sfourcoreabilities:(1)Scala-
bility inthevisualdomainwithoutextensivedatasetpretraining,thanks
toanovelself-distillationtechnique;(2)Sensitivity forrecognizingshort-
term actions even with fine-grained motion differences; (3) Superiority
in long-term video understanding, showcasing significant advancements
over traditional feature-based models; and (4) Compatibility with other
modalities,demonstratingrobustnessinmulti-modalcontexts.Through
thesedistinctadvantages,VideoMambasetsanewbenchmarkforvideo
understanding, offering a scalable and efficient solution for comprehen-
sive video understanding. All the code and models are available.
1 Introduction
Thecoreobjectiveforvideounderstandingliesinmasteringspatiotemporalrep-
resentations,whichinherentlypresentstwoformidablechallenges:thelargespa-
tiotemporal redundancy within short video clips, and the complex spatiotempo-
ral dependencies among long contexts. Although the once-dominant 3D convo-
lutional neural networks (CNNs) [10,20,77] and video transformers [2,4], effec-
tively tackle one of the challenges mentioned by leveraging either local convolu-
tion or long-range attention, they fall short in addressing both simultaneously.
UniFormer [45] attempts to integrate the advantages of both methods, but it
struggles with modeling long videos, which has been the major trend in recent
research on video understanding [49,73] and generation [5,91].
The emergence of low-cost operators such as S4 [27], RWKV [74], and Ret-
Net [71] in the NLP domain, has carved a novel pathway for the vision model.
♠ Interns at Shanghai AI Laboratory. ♡ Corresponding author.
4202
raM
11
]VC.sc[
1v77960.3042:viXra2 K. Li et al.
+/00/$!
"&10/$⚡
2ℎ/&4/$# 40×
&=8 &=64
2× 20×
6×
Fig.1:Comparisonsofthroughputandmemory.TheTimeSformer-Ti[4]isbuilt
based on DeiT-Ti [76] with joint spatiotemporal attention. All the input frames are
sized to 224×224. The testing is conducted on an NVIDIA A100-80G GPU, utilizing
PyTorch 2.1 and CUDA 11.8, with a batch size of 128. Our VideoMamba is better,
faster and cheaper for both short-term and long-term video understanding.
Mamba[26]standsoutwithitsselectivestatespacemodel(SSM),strikingabal-
ance between maintaining linear complexity and facilitating long-term dynamic
modeling. This innovation has spurred its adoption in vision tasks, as evidenced
byVisionMamba[90]andVMamba[51],whichleveragemulti-directionalSSMs
for enhanced 2D image processing. These models rival attention-based archi-
tectures in performance while offering a significant reduction in memory usage.
Given the inherently longer sequences produced by video, a natural question
arises: Can Mamba work well for video understanding?
Inspired by this, we introduce VideoMamba, a purely SSM-based model tai-
lored for video understanding. VideoMamba harmoniously merges the strengths
ofconvolutionandattentioninvanillaViT[16]style.Itoffersalinear-complexity
method for dynamic spatiotemporal context modeling, ideal for high-resolution
long videos. The related evaluation focuses on VideoMamba’s four key abilities:
(1) Scalability in the Visual Domain: We examine VideoMamba’s scal-
ability and find that, while the pure Mamba model tends to overfit as it scales,
our introduction of a simple yet effective self-distillation strategy allows Video-
Mambatoachieveremarkableperformanceenhancementsasthemodelandinput
sizes increase, without the need for large-scale dataset pretraining.
(2) Sensitivity for Short-term Action Recognition: Our analysis ex-
tendstoassessingVideoMamba’scapabilitytoaccuratelydistinguishshort-term
actions, especially those with fine-grained motion differences, e.g., opening and
closing. The findings reveal VideoMamba’s superior performance over existing
attention-basedmodels[2,4,53].Moreimportantly,itisalsosuitableformasked
modeling, which further enhances its temporal sensitivity.
(3) Superiority in Long-term Video Understanding: We then assess
VideoMamba’s prowess in interpreting long videos. It showcases remarkable su-
periority over conventional feature-based methods [36,48] through end-to-end
training. Notably, VideoMamba operates 6× faster than TimeSformer [4] and
demands 40× less GPU memory for 64-frame videos (see Fig. 1).
(4)CompatibilitywithOtherModalities:Lastly,weassessVideoMamba’s
adaptability with other modalities. Results in video-text retrievals show its im-
%1.4+
%6.4+
?>> ?>>VideoMamba 3
proved performance than ViT, particularly in long videos with complex scenar-
ios. This underscores its robustness and multi-modal integration capacity.
In conclusion, our in-depth experiments reveal VideoMamba’s immense po-
tential in understanding both short-term (K400 [37] and SthSthV2 [25]) and
long-term (Breakfast [38], COIN [72], and LVU [84]) video contents. Given its
efficiency and effectiveness, VideoMamba is poised to become a cornerstone in
therealmoflong-videocomprehension.Allthecodeandmodelsareopen-sourced
to foster future research endeavors.
2 Related Works
2.1 State Space Models
Recently, the State Space Models (SSMs) have shown significant effectiveness
of state space transformation in capturing the dynamics and dependencies of
language sequences. [27] introduces a structured state-space sequence model
(S4), specifically designed to model long-range dependencies, boasting the ad-
vantage of linear complexity. Based on it, various models have been developed
(e.g., S5 [67], H3 [21] and GSS [57]), and Mamba [26] distinguishes itself by
introducing a data-dependent SSM layer and a selection mechanism using par-
allel scan (S6). Compared to transformers [6,55] based on quadratic-complexity
attention, Mamba excels at processing long sequences with linear complexity.
In the vision domain, [27] first applies SSM in pixel-level image classifica-
tion,and[36]usesS4tohandlethelong-rangetemporaldependenciesformovie
clip classification. Besides, the great potential of Mamba motivates a series of
works[29,31,47,51,79,87,90],whichdemonstratesMamba’sbetterperformances
andhigherGPUefficiencythanTransformeronvisualdownstreamtaskslikeob-
jectdetectionandsemanticsegmentation.Differentfromthepreviousworks,our
VideoMambaisapurelySSM-basedvideomodel,showcasinggreatefficiencyand
effectiveness for both short-term and long-term video understanding.
2.2 Video Understanding
Video understanding stands as a cornerstone in the domain of computer vision,
whose significance is further amplified by the burgeoning growth of short video
platforms.Tobolsterthisfield,numerousdatasetsequippedwithextensivedata
and meticulous human annotations have been developed, aiming to enhance
human action recognition capabilities. Notable examples include UCF101 [68]
and Kinetics dataset [8,9,37], which have played pivotal roles in benchmarking
progress. Furthermore, other datasets [23,28,32,35,50,63] provide annotated
activity videos tailored for action localization, fostering deeper research into
human activities. Beyond action recognition, the advent of large-scale video-
text datasets [11,13,58,82,86,88] extends the utility of video understanding
into the realm of multi-modality tasks, such as video captioning, retrieval and
question answering, thereby broadening the application spectrum.4 K. Li et al.
As for the architecture, it has evolved from using CNN which extracts fea-
tures from video frames, to more advanced techniques. Initially, 3D CNNs [10,
18,77,78] expanded the traditional 2D CNN architecture to capture videos’
spatio-temporalinformation.Two-Stream[66],whichcombinesspatialandtem-
poral streams, and SlowFast [20], which uses parallel networks to capture se-
mantics and rapid movements, further enhance action recognition capacity. The
introduction of attention-based models [2,4,7,60,64], like TimeSformer [4] and
ViViT[2],markedasignificantadvancementbyeffectivelycapturinglong-range
dependencies within video sequences, enhancing temporal relationship under-
standing. Recent developments [43,45,53,83] have focused on accurate video
transformer,withinnovationsliketheVideoSwin’swindowattention[53]andthe
UniFormer’s integration of convolution and self-attention mechanisms [45], aim-
ingtobalancecomputationalefficiencywithperformance.Despitethesemodels’
achievements in various tasks, they often come with high computational costs
forlongsequences.Incontrast,ourVideoMambaintroducesalinear-complexity
operator for efficient long-term modeling, outperforming existing methods with
faster speed and lower GPU consumption.
3 Method
3.1 Preliminaries
SSM for 1D sequence. State Space Models (SSMs) are conceptualized based
on continuous systems that map a 1D function or sequence, x(t)∈RL →y(t)∈
RL through a hidden state h(t) ∈ RN. Formally, SSMs employ the following
ordinary differential equation (ODE) to model the input data:
h′(t)=Ah(t)+Bx(t), (1)
y(t)=Ch(t), (2)
whereA∈RN×N representsthesystem’sevolutionmatrix,andB∈RN×1,C∈
RN×1aretheprojectionmatrices.ThiscontinuousODEisapproximatedthrough
discretizationinmodernSSMs.Mamba[26]isoneofthediscreteversionsofthe
continuous system, which includes a timescale parameter ∆ to transform the
continuous parameters A,B to their discrete counterparts A,B. The transfor-
mation typically employs the zero-order hold (ZOH) method, defined by:
A=exp(∆A), (3)
B=(∆A)−1(exp(∆A)−I)·∆B (4)
h =Ah +Bx , (5)
t t−1 t
y =Ch . (6)
t t
Contrary to traditional models that primarily rely on linear time-invariant
SSMs, Mamba distinguishes itself by implementing a Selective Scan Mechanism
(S6) as its core SSM operator. Within S6, the parameters B ∈ RB×L×N, C ∈VideoMamba 5
Linear Sequence Multiplication !Activation
Projection Transformation Summarization &.+/ Conv 5 SSM &.+/
Conv 5 SSM
Conv 5 SSM
5 5
! 7!81! 1 ;'-'.#9)'*+!$7!81!
Fig.2: Mamba blocks for 1D [26] and 2D [90] sequence. We omit the initial
normalization and the final residual for simplification.
Class Classification Head
$%&''()*
+&%,-()* CLS CLS
./&0()* ×L
… Bidirectional Mamba Block
1 2 3 4 1 2 3 4
Temporal
/!
PositionEmbedding 1 2 3
Spatial 4 3 2 1 4 3 2 1
PositionEmbedding 0 1 2 3 4 1 2 3 4 1 2 3 4 /"
[CLS] Token
3D Patch Embedding /# 1 2 3 4 1 2 3 4
34567589:7; <7:=67589:7;
! 6'-#*7!81! 1 "4!)'*)#84*.!$"9!+
Fig.3: Framework of VideoMamba. We strictly follow the architecture of vanilla
ViT [16], and adapt the bidirectional mamba block [90] for 3D video sequences.
RB×L×N, and ∆ ∈ RB×L×D are directly derived from the input data x ∈
RB×L×D,indicatinganintrinsiccapacityforcontextualsensitivityandadaptive
weight modulation. Fig. 2a shows the details of the Mamba block.
Bidirectional SSM for Vision. The original Mamba block, designed for 1D
sequences, falls short for visual tasks requiring spatial awareness. Building on
this, Vision Mamba introduces a bidirectional Mamba (B-Mamba) block in Fig.
2b,whichadaptsbidirectionalsequencemodelingforvision-specificapplications.
This block processes flattened visual sequences through simultaneous forward
and backward SSMs, enhancing its capacity for spatially-aware processing. In
this work, we extend the B-Mamba block for 3D video understanding.
3.2 VideoMamba
Overview. Fig. 3 illustrates the overall framework of VideoMamba. Specifi-
cally, we first use 3D convolution (i.e., 1×16×16) to project the input videos
Xv ∈ R3×T×H×W into L non-overlapping spatiotemporal patches Xp ∈ RL×C,
where L=t×h×w (t=T, h=H, and w=W). The sequence of tokens input to the
16 16
following VideoMamba encoder is
X= [X ,X]+p +p , (7)
cls s t
where X is a learnable classification token that is prepended to the start of
cls
the sequence. Following previous works [2,4,16], we added a learnable spatial
position embedding p ∈ R(hw+1)×C and the extra temporal one p ∈ Rt×C
s t
to retain the spatiotemporal position information, since the SSM modeling is
*6 K. Li et al.
S S S S
T T T T
"4!)'!$−:'.() =#84*.!$−:'.() "4!)'*)#84*.!$ "4!)'*)#84*.!$
! 1 9 -
;'-'.#9)'*+!$ ;'-'.#9)'*+!$ ;'-'.#9)'*+!$,1 ;'-'.#9)'*+!$,2
Fig.4: Different scan methods. We omit the [CLS] token for simplification.
sensitive to token position. The tokens X are then passed through by L stacked
B-Mamba blocks, and the representation of [CLS] token at the final layer is
processed by normalization and linear layer for classification.
Spatiotemporal Scan.ToapplytheB-Mambalayerforspatiotemporalinput,
weextendtheoriginal2Dscanintodifferentbidirectional3DscansinFig.4:(a)
Spatial-First, organizing spatial tokens by location then stacking them frame by
frame; (b) Temporal-First, arranging temporal tokens based on the frame then
stacks along the spatial dimension; (c) Spatiotemporal, a hybrid of both Spatial-
First andTemporal-First,withv1conductinghalfofthemandv2conductingfull
of them (2× computation). Moreover, our experiments in Fig. 7a demonstrate
thattheSpatial-Firstbidirectionalscanisthemosteffectiveyetsimple.Thanks
tothelinearcomplexityofMamba,ourVideoMambaiscapableofhandlinglong
videos of high resolution efficiently.
ComparisontoVim[90]andVMamba[51].OurVideoMambabuildsupon
Vim, yet streamlines its architecture by omitting features such as the middle
[CLS] token and Rotary Position Embedding (RoPE [69]), resulting in superior
performanceonImageNet-1Kwithgainsof +0.8%and+0.7%forVim-Tiand
Vim-S, respectively. Unlike VMamba, which incorporates additional depthwise
convolution, VideoMamba strictly follows the ViT design without downsam-
plinglayers.TocountertheoverfittingissuesobservedinVMamba,weintroduce
an effective self-distillation technique outlined in Section 3.3, demonstrate the
isotropic VideoMamba’s great scalability for image and video tasks.
Comparison to TimeSformer [4] and ViViT [2]. Traditional attention-
based models like TimeSformer and ViViT have addressed the self-attention
mechanism’s quadratic complexity by adopting divided spatiotemporal atten-
tion.Despitebeingmoreefficient,itintroducesadditionalparametersandunder-
performscomparedtojointattention,particularlyinscenariosinvolvingmasked
pretraining [44,75]. In contrast, VideoMamba processes spatiotemporal tokens
with linear complexity, outperforming TimeSformer on Kinetics-400 by +2.6%
and making significant strides on SthSthV2 with a +5.9% improvement (see
Table3 and4). Furthermore,VideoMambaachievesa 6× increase inprocessing
speed and requires 40× less GPU memory for long videos, as detailed in Fig. 1,
demonstrating its efficiency and effectiveness in handling long-video tasks.VideoMamba 7
*+,- *+,- *+,-
! '+4@),'-#* 1 .!+-*88!(A'+/ 9 )@1#8!(A'+/
*+,- *+,- *+,-
- 9$'4−.*B8!(A'+/ # %.!8#−.*B8!(A'+/ % !))#+)'*+8!(A'+/
Fig.5: Different masking strategies. Row masking, tailored for VideoMamba in
light of the 1D convolution preceding SSM, enhances performance with continuous
tokens. The difference between clip-row and frame-row masking is that the former
masks the entire video clip, while the latter masks each frame individually.
3.3 Architecture
ForSSMintheB-Mambalayer,weadoptthe
Model #Depth #Dim #Param.
default hyperparameters as in Mamba [26].
Tiny 24 192 7M
setting the state dimension and expansion Small 24 384 26M
ratio to 16 and 2, respectively. Following Middle 32 576 74M
ViT[16],weadjustthedepthandembedding Base 24 768 98M
dimensions to create models of comparable
Table 1: Different model sizes.
sizes in Table 1, including VideoMamba-Ti,
Base model is finally excluded due
VideoMamba-S and VideoMamba-M. How-
to its suboptimization.
ever, we observe that larger VideoMamba
tends to overfit during our experiments, leading to suboptimal performance as
illustrated in Fig. 6a. This overfitting issue is not unique to our models but
is also found in VMamba [51], where the optimal performance of VMamba-B
was achieved at three-quarters of the total training epochs. To counteract the
overfitting in larger Mamba models, we introduce an effective Self-Distillation
strategy, which uses a smaller and well-trained model as the “teacher” to guide
thetrainingofthelarger“student” model.Theresults,depictedinFig.6a,show
that this strategy leads to expected better convergence.
3.4 Masked Modeling
Recently, VideoMAE and ST-MAE [19,75] have showcased the significant bene-
fits of masked modeling in enhancing a model’s capability for FINE-GRAINED
temporalunderstanding.UMT[44]takesthisfurtherbyintroducinganefficient
masked alignment technique that yields robust results across single and multi-
modal video tasks. To augment VideoMamba’s temporal sensitivity and verify
its adaptability with text modalities, we adopt a masked alignment approach
inspired by UMT. Firstly, VideoMamba is trained from scratch on video data
alone, aligning unmasked tokens with those from CLIP-ViT. Subsequently, it is
integrated with a text encoder and a cross-modal decoder (i.e., BERT [15]), for
pretraining on both image-text and video-text datasets.8 K. Li et al.
! "#$%-&'()'$$!)'*+!,*'-(*,#.%'))'+/. 1 2!.$3()*44'+/-*#(+*)ℎ#$4.
Fig.6: Ablation studies of Self-Distillation and Early Stopping.
It’s important to note the distinction from UMT, which employs multi-layer
alignment between the student and teacher models. In contrast, due to Video-
Mamba’s unique architecture (SSM vs. Transformer), we align only the final
outputs.Regardingourmaskingstrategy,weproposedifferentrowmaskingtech-
niques,depictedinFig.5,tailoredtotheB-Mambablock’spreferenceforcontin-
uous tokens. Additionally, we explore attention masking to preserve meaningful
adjacencyamongtokens,leveragingtheinherentstrengthsofthe1Dconvolution
within the B-Mamba block for improved performance.
4 Experiments
4.1 Scaling Up
Dataset and Settings. We first conduct experiments on ImageNet-1K [14],
which includes 1.28M training images and 50K validation images across 1,000
categories. For fair comparisons, we follow most of the training strategies pro-
posed in DeiT [76], but adopt weaker data augmentation for the tiny model
variant. Furthermore, we adjust the stochastic depth ratio to 0/0.15/0.5 for
VideoMamba-Ti/S/M. Our models are trained using the AdamW optimizer
pairedwithacosinelearningratescheduleover300epochs.Theinitial5epochs
serve as a period for linear warm-up. Default settings for the learning rate,
weightdecay,andbatchsizeare1e-3,0.05,and1024,respectively.Moreover,we
use BFloat16 precision during training to enhance stability without relying on
EMA. For the VideoMamba-M model, we employ a pretrained VideoMamba-S
model as a “teacher” to guide the training process by aligning the final feature
maps through L2 loss. For large resolution (>224) fine-tuning, we use a reduced
learning rate (5e-6) and minimal weight decay (1e-8) for 30 epochs.
Effect of Self-Distillation. Fig. 6a reveals that when trained from scratch,
VideoMamba-B tends to overfit more easily and underperforms compared to
VideoMamba-S, whereas VideoMamba-M achieves similar performances. Fortu-
nately, our self-distillation has shown to be effective in achieving the desired op-
timization with marginal additional computational cost. To mitigate teacher’sVideoMamba 9
Input #Param FLOPs IN-1K
Arch. Model iso.
Size (M) (G) Top-1
ConvNeXt-T[54] ✗ 2242 29 4.5 82.1
ConvNeXt-S[54] ✗ 2242 50 8.7 83.1
CNN
ConvNeXt-B[54] ✗ 2242 89 15.4 83.8
SwinT-T[52] ✗ 2242 28 4.5 81.3
Swin-S[52] ✗ 2242 50 8.7 83.0
Trans.
Swin-B[52] ✗ 2242 88 15.4 83.5
VMamba-T[51] ✗ 2242 22 5.6 82.2
CNN+
VMamba-S[51] ✗ 2242 44 11.2 83.5
SSM
VMamba-B[51] ✗ 2242 75 18.0 83.7
ConvNeXt-S[54] ✓ 2242 22 4.3 79.7
CNN
ConvNeXt-B[54] ✓ 2242 87 16.9 82.0
DeiT-Ti[76] ✓ 2242 6 1.3 72.2
DeiT-S[76] ✓ 2242 22 4.6 79.8
Trans.
DeiT-B[76] ✓ 2242 87 17.6 81.8
DeiT-B[76] ✓ 3842 87 55.5 83.1
S4ND-ViT-B[59] ✓ 2242 89 - 80.4
Vim-Ti[90] ✓ 2242 7 1.1 76.1
Vim-S[90] ✓ 2242 26 4.3 80.5
VideoMamba-Ti ✓ 2242 7 1.1 76.9
VideoMamba-Ti ✓ 4482 7 4.3 79.3
VideoMamba-Ti ✓ 5762 7 7.1 79.6
SSM
VideoMamba-S ✓ 2242 26 4.3 81.2
VideoMamba-S ✓ 4482 26 16.9 83.2
VideoMamba-S ✓ 5762 26 28.0 83.5
VideoMamba-M ✓ 2242 74 12.7 82.8
VideoMamba-M ✓ 4482 75 50.4 83.8
VideoMamba-M ✓ 5762 75 83.1 84.0
Table 2: Comparison with the state-of-the-art on ImageNet. “iso.” means
isotropic architecture without downsampling layers.
potential overdirection, we experimented with early stopping [12] in Fig. 6b,
although it did not yield beneficial outcomes. These findings indicate that self-
distillation offers a viable strategy for enhancing the scalability of the Mamba
architecture without significant computational overhead.
Results. Table 2 showcases the results on the ImageNet-1K dataset. Notably,
VideoMamba-M outperforms other isotropic architectures by significant mar-
gins, achieving a +0.8% improvement over ConvNeXt-B [54] and a +2.0% in-
crease compared to DeiT-B [76], while utilizing fewer parameters. Additionally,
VideoMamba-M holds its ground against non-isotropic backbones that lever-
age hierarchical features for enhanced performance. Given Mamba’s efficiency
in processing long sequences, we further enhance performance by increasing the
resolution, achieving a top-1 accuracy of 84.0% with only 74M parameters.
This remarkable improvement extends to video tasks, as detailed in Section 4.2,
underscoring VideoMamba’s effectiveness and scalability.
4.2 Short-term Video Understanding
Datasets and Settings. We evaluate our VideoMamba on the popular scene-
relatedKinetics-400[37]andtemporal-relatedSomething-SomethingV2[25],the
averagevideolengthsofwhichare10sand4s.Forsupervisedpretraining,wefine-
tune those models pretrained on ImageNet-1K with the same training strategy10 K. Li et al.
Extra Input #Param FLOPs K400
Arch. Model iso.
Data Size (M) (G) Top-1 Top-5
Supervised:Thosemodelswithextradataareundersupervisedtraining.
SlowFastR101+NL[20] ✗ 80×2242 60 234×3×10 79.8 93.9
CNN X3D-M[18] ✗ 16×2242 4 6×3×10 76.0 92.3
X3D-XL[18] ✗ 16×3122 20 194×3×10 80.4 94.6
Swin-T[53] ✗ IN-1K 32×2242 28 88×3×4 78.8 93.6
Trans. Swin-B[53] ✗ IN-1K 32×2242 88 88×3×4 80.6 94.5
Swin-B[53] ✗ IN-21K 32×2242 88 282×3×4 82.7 95.5
MViTv1-B[17] ✗ 32×2242 37 70×1×5 80.2 94.4
MViTv2-S[46] ✗ 16×2242 35 64×1×5 81.0 94.6
CNN+
UniFormer-S[45] ✗ IN-1K 16×2242 21 42×1×4 80.8 94.7
Trans.
UniFormer-B[45] ✗ IN-1K 16×2242 50 97×1×4 82.0 95.1
UniFormer-B[45] ✗ IN-1K 32×2242 50 259×3×4 83.0 95.4
STAM[64] ✓ IN-21K 64×2242 121 1040×1×1 79.2 -
TimeSformer-L[4] ✓ IN-21K 96×2242 121 2380×3×1 80.7 94.7
Trans.
ViViT-L[2] ✓ IN-21K 16×2242 311 3992×3×4 81.3 94.7
Mformer-HR[60] ✓ IN-21K 16×3362 311 959×3×10 81.1 95.2
VideoMamba-Ti ✓ IN-1K 16×2242 7 17×3×4 78.1 93.5
VideoMamba-Ti ✓ IN-1K 32×2242 7 34×3×4 78.8 93.9
VideoMamba-Ti ✓ IN-1K 64×3842 7 202×3×4 80.3 94.8
VideoMamba-S ✓ IN-1K 16×2242 26 68×3×4 80.8 94.8
SSM VideoMamba-S ✓ IN-1K 32×2242 26 135×3×4 81.5 95.2
VideoMamba-S ✓ IN-1K 64×3842 26 395×3×4 82.7 95.6
VideoMamba-M ✓ IN-1K 16×2242 74 202×3×4 81.9 95.4
VideoMamba-M ✓ IN-1K 32×2242 74 403×3×4 82.4 95.7
VideoMamba-M ✓ IN-1K 64×3842 74 2368×3×4 83.3 96.1
Self-supervised:ForUMT,theCLIP-400Misusedinpretrainedteacher.
BEVT-B800e[81] ✗ IN-1K 32×2242 88 282×3×4 81.1 -
ST-MAE-B1600e[19] ✓ 16×2242 87 180×3×7 81.3 94.9
Trans. VideoMAE-S2400e[75] ✓ 16×2242 22 57×3×5 79.0 93.8
VideoMAE-B1600e[75] ✓ 16×2242 87 180×3×5 81.5 95.1
UMT-B800e[44] ✓ CLIP-400M 8×2242 87 180×3×5 85.7 97.0
VideoMamba-M800e ✓ CLIP-400M 8×2242 74 101×3×4 82.0 95.4
SSM
VideoMamba-M800e ✓ CLIP-400M 16×2242 74 202×3×4 83.4 95.9
VideoMamba-M800e ✓ CLIP-400M 32×2242 74 403×3×4 83.9 96.2
VideoMamba-M800e ✓ CLIP-400M 64×3842 74 2368×3×4 85.0 96.9
Table3:Comparisonwiththestate-of-the-artonscene-relatedKinetics-400.
“iso.” meansisotropicarchitecturewithoutdownsamplinglayers.Maskedmodeling[44]
also works for Mamba, but the inconsistent architecture leads to inferior alignment.
as VideoMAE [75]. Specifically, for VideoMamba-M, the warmup epoch, total
epoch,stochasticdepthrate,weightdecayaresetto5,50,0.8,0.05forK400,and
5, 30, 0.8, 0.05 for SthSth. For the smaller models, all the hyper-parameters are
the same unless we decrease the stochastic depth rate and increase the training
epochs. Moreover, we linearly scale the base learning rates according to the
batch size, which are 2e−4 · batchsize for K400 and 4e−4 · batchsize for SthSth.
256 256
As for self-supervised pretraining, we adopt the training recipe as in UMT [44],
employing CLIP-ViT-B [61] to distill VideoMamba-M over 800 epochs. During
fine-tuning, we use similar hyperparameters as mentioned but opt for a small
stochastic depth rate and learning rate for both datasets.
Results.Table3and4listtheresultsonshort-termvideodatasets.(a) Super-
vised:Comparedwiththepurelyattention-basedmethods[2,4],ourSSM-based
VideoMamba-M secures a notable advantage, outperforming ViViT-L [2] by
+2.0% and +3.0% on the scene-related K400 and the temporally-related Sth-VideoMamba 11
Extra Input #Param FLOPs SSV2
Arch. Model iso.
Data Size (M) (G) Top-1 Top-5
Supervised:Thosemodelswithextradataareundersupervisedtraining.
SlowFastR101[20] ✗ K400 32×2242 53 106×3×1 63.1 87.6
CNN CT-NetR50[42] ✗ IN-1K 16×2242 21 75×1×1 64.5 89.3
TDNR50[80] ✗ IN-1K 16×2242 26 75×1×1 65.3 91.6
Trans. Swin-B[53] ✗ K400 32×2242 89 88×3×1 69.6 92.7
MViTv1-B[17] ✗ K400 16×2242 37 71×3×1 64.7 89.2
MViTv1-B[17] ✗ K400 32×2242 37 170×3×1 67.1 90.8
CNN+ MViTv2-S[46] ✗ K400 16×2242 35 65×3×1 68.2 91.4
Trans. MViTv2-B[46] ✗ K400 32×2242 51 225×3×1 70.5 92.7
UniFormer-S[45] ✗ IN-1K+K400 16×2242 21 42×3×1 67.7 91.4
UniFormer-B[45] ✗ IN-1K+K400 16×2242 50 97×3×1 70.4 92.8
TimeSformer-HR[4] ✓ IN-21K 16×2242 121 1703×3×1 62.5 -
Trans. ViViT-L[2] ✓ IN-21K+K400 16×2242 311 3992×3×4 65.4 89.8
Mformer-HR[60] ✓ IN-21K+K400 16×3362 311 1185×3×1 68.1 91.2
VideoMamba-Ti ✓ IN-1K 8×2242 7 9×3×2 65.1 89.1
VideoMamba-Ti ✓ IN-1K 16×2242 7 17×3×2 66.0 89.6
VideoMamba-Ti ✓ IN-1K 16×2882 7 28×3×2 66.2 90.0
VideoMamba-S ✓ IN-1K 8×2242 26 34×3×2 66.6 90.4
SSM VideoMamba-S ✓ IN-1K 16×2242 26 68×3×2 67.6 90.9
VideoMamba-S ✓ IN-1K 16×2882 26 112×3×2 68.1 91.2
VideoMamba-M ✓ IN-1K 8×2242 74 101×3×4 67.3 91.0
VideoMamba-M ✓ IN-1K 16×2242 74 202×3×4 68.3 91.4
VideoMamba-M ✓ IN-1K 16×2882 74 333×3×4 68.4 91.6
Self-supervised:ForUMT,theCLIP-400Misusedinpretrainedteacher.
BEVT-B800e[81] ✗ IN-1K+K400 32×2242 88 321×3×1 70.6 -
Trans.
VideoMAE-S2400e[75] ✓ 16×2242 22 57×3×2 66.8 90.3
VideoMAE-B2400e[75] ✓ 16×2242 87 180×3×2 70.8 92.4
UMT-B800e[44] ✓ CLIP-400M 8×2242 87 180×3×2 70.8 92.6
VideoMamba-M800e ✓ CLIP-400M 8×2242 74 101×3×2 70.2 92.6
SSM VideoMamba-M800e ✓ CLIP-400M 16×2242 74 202×3×2 71.0 92.7
VideoMamba-M800e ✓ CLIP-400M 16×2882 74 333×3×2 71.4 92.9
Table 4: Comparison with the state-of-the-art on temporal-related SthSth
V2. “iso.” means isotropic architecture without downsampling layers. Masked model-
ing [44] also works for Mamba, and it performs better than VideoMAE.
SthV2datasets,respectively.Thisimprovementcomeswithsignificantlyreduced
computationaldemandsandlesspretrainingdata.Furthermore,VideoMamba-M
delivers results that are on par with the SOTA UniFormer [45], which skillfully
integrates convolution with attention in a non-isotropic structure. (b) Self-
supervised: The performance of VideoMamba under masked pretraining sur-
passes that of the VideoMAE [75], known for its proficiency in fine-grained ac-
tion.ThisachievementunderscoresthepotentialofourpurelySSM-basedmodel
in efficiently and effectively understanding short-term videos, highlighting its
suitability for both supervised and self-supervised learning paradigms.
Ablation Studies. Through comprehensive ablation studies detailed in Fig. 7
and Table 5, we explore various aspects of our model. (a) Scan Type: Among
all the methods, the spatial-first approach emerges as the most effective, in con-
trast,thetemporal-firststrategyistheworst.Thesuperiorityofthespatial-first
method is attributed to its ability to seamlessly leverage 2D pretrained knowl-
edge by scanning frame by frame. (b) Frame and Resolution: Contrary to
findings from ImageNet (see Table 2), higher resolution does not uniformly lead
to better performance. Increasing the number of frames consistently enhances12 K. Li et al.
Type SSV2 79.5 64×224×224 66.0 16×224×224
SF-Bidirectional 65.1
79.0 32×224×224 65.8
TF-Bidirectional 62.4
32×224×224
ST-Bidirectionalv1 63.9 78.516×224×224 65.6 Frame ST-Bidirectionalv2 64.2 78.0 8×384×384 8×288×288 Resolution
65.4 8×320×320 Half-SF+Half-TF 64.0 77.5 8×320×320 8×448×448
Half-TF+Half-SF 64.1 Frame 65.2 8×384×384
AlternativeSF&TF 65.1 77.0 8×224×224 Resolution 8×224×224
2000 4000 6000 8000 10000 12000 2000 3000 4000 5000 6000
(a)ScanType.Spatial-First Token Number Token Number
scanissimpleyeteffective. (b) Frame & Resolution for K400 and SSV2.
Fig.7: Ablation studies of scan type, frame and resolution.Allthemodelsare
fine-tuned from VideoMamba-Ti pretrained on ImageNet.
Type SSV2 Layer SSV2 Ratio SSV2 DP SSV2
Random 67.4 Last1 68.5 50% 68.1 0.1 68.0
Tube 66.3 Last2 68.4 65% 68.4 0.2 68.2
Clip-Row 68.2 Last6 68.2 80% 68.5 0.3 68.4
Frame-Row 67.8 Last6×2 67.7 90% 68.2 0.4 68.5
Attention 68.5
(b) Alignment Layer. (c) Mask Ratio. (d) Droppath.
(a) Mask Type.
Table 5: Ablation studies of masked pretraining.WeadoptCLIP-ViT-B[61]as
a teacher to distill VideoMamba-M for 200 epochs.
results on the K400 dataset. However, this is not the case with SthSthV2, possi-
bly due to the brief duration of its videos, which may not accommodate longer
inputseffectively.(c)Masked Pretraining:Ourfindingsrevealthatrowmask-
ing, being particularly compatible with 1D convolution, outperforms commonly
usedrandomandtubemasking.Clip-rowmaskingexcelsowingtoitshigherde-
greeofrandomness.Moreover,attentionmaskingstandsoutasthemostefficient
by favoring the preservation of adjacent meaningful content. Aligning solely the
model’sfinaloutputprovesmosteffective,likelyduetoarchitecturaldifferences.
Lastly, an optimal masking ratio (80%) combined with stronger regularization
significantly benefits VideoMamba during masked pretraining.
4.3 Long-term Video Understanding
DatasetsandSettings.WerigorouslyassessVideoMamba’sproficiencyinpro-
cessinglong-termvideosbyleveragingthreecomprehensivedatasets,i.e.,Break-
fast[38],COIN[72]andLong-formVideoUnderstanding(LVU[84])benchmark.
Specifically, Breakfast comprises 1,712 videos, encapsulating 10 intricate cook-
ing activities over 77 hours. COIN features 11,827 videos across 180 unique
procedural tasks, with an average duration of 2.36 minutes. The LVU bench-
mark includes approximately 30K movie clips, lasting between 1 to 3 minutes,
and encompasses nine tasks across 3 primary categories: content understand-
ing, metadata prediction, and user engagement. For the regression task among
these, we evaluate using mean-squared error, while for the classification tasks,
accuracy is the metric of choice. In contrast to prior studies [36,48] that rely on
features derived from pretrained video models, such as Swin-B [52] trained on
Kinetics-600,ourmethodemploysend-to-endtrainingasdetailedinSection4.2.
Additionally, for fair comparisons, we fine-tune our models pretrained on K400.
)%(
ccA
1-poT
004K
)%(
ccA
1-poT
2VSSVideoMamba 13
Pretraining BF COIN
Method e2e Backbone NeckType
Dataset Top-1 Top-1
Timeception[33] ✗ 3D-ResNet Conv. IN-1K+K400 71.3 -
VideoGraph[34] ✗ I3D Conv.+Atten. IN-1K+K400 69.5 -
GHRM[89] ✗ I3D GraphConv.. IN-1K+K400 75.5 -
DistantSupervision[48] ✗ TimeSformer Atten.w/KB IN-21K+HTM 89.9 90.0
ViS4mer[36] ✗ Swin-B SSM IN-21K+K600 88.2 88.4
Turbof32[30] ✓ VideoMAE-B K400 86.8 82.3
Turbof32[30] ✓ VideoMAE-B K400+HTM-AA 91.3 87.5
VideoMambaf32 ✓ VideoMamba-Ti K400 94.3 86.2
VideoMambaf64 ✓ VideoMamba-Ti K400 94.3 87.0
VideoMambaf32 ✓ VideoMamba-S K400 95.3 88.4
VideoMambaf64 ✓ VideoMamba-S K400 97.4 88.7
VideoMambaf32 ✓ VideoMamba-M K400 94.8 88.3
VideoMambaf64 ✓ VideoMamba-M K400 95.8 89.5
VideoMambaf32 ✓ VideoMamba-M† K400 97.9 89.6
VideoMambaf64 ✓ VideoMamba-M† K400 96.9 90.4
Table 6: Comparison with the state-of-the-art on Breakfast and COIN.
“e2e” means end-to-end methods without exhausting feature extraction. “†” marks
the backbone with masked pretraining.
Content(↑) Metadata(↑) User(↓)
Method e2e Backbone
Rel. Speak Scene Dir. Genre Wtr. Year Like View
VideoBERT[70] ✗ S3D 52.80 37.90 54.90 47.30 51.90 38.50 36.10 0.32 4.46
ObjectTrans.[84] ✗ ResNet 53.10 39.40 56.90 51.20 54.60 34.50 39.10 0.23 3.55
LST[36] ✗ ViT-L 52.38 37.31 62.79 56.07 52.70 42.26 39.16 0.31 3.83
Performer[36] ✗ ViT-L 50.00 38.80 60.46 58.87 49.45 48.21 41.25 0.31 3.93
Orthoformer[36] ✗ ViT-L 50.00 39.30 66.27 55.14 55.79 47.02 43.35 0.29 3.86
ViS4mer[36] ✗ ViT-L 57.14 40.79 67.44 62.61 54.71 48.80 44.75 0.26 3.63
VideoMambaf32 ✓ VM-Ti 62.50 40.43 70.37 67.29 65.24 52.98 48.23 0.26 2.90
Table 7: Comparison with the state-of-the-art on LVU. “e2e” means end-to-
end methods without exhausting feature extraction. “Rel.”, “Dir.” and “Wtr.” refers to
“Relation”, “Director” and “Writer”, respectively.
Results.AsillustratedinFigure1,thelinearcomplexityofVideoMambamakes
itwell-suitedforend-to-endtrainingwithlong-durationvideos.Thecomparisons
in Tables 6 and 7 highlight VideoMamba’s simplicity and effectiveness against
traditionalfeature-basedmethods[36,48]onthesetasks.Ityieldssignificantper-
formance improvements, achieving SOTA results even with smaller model sizes.
Forexample,VideoMamba-Tishowsanotableincreaseof +6.1%overViS4mer
usingSwin-Bfeaturesanda+3.0%upliftagainstTurbo’smulti-modalityalign-
ment approach [30]. Notably, the results underscore the positive impact of the
scaling model and frame numbers for long-term tasks. In the diverse and chal-
lenging set of nine tasks presented by LVU, our VideoMamba-Ti, fine-tuned in
an end-to-end manner, delivers outstanding or comparable results to current
SOTA methods. These outcomes not only highlight VideoMamba’s effectiveness
but also its great potential for future long-video comprehension.
4.4 Multi-modality Video Understanding
Datasets and Settings.FollowingUMT[44],weutilizeWebVid-2M[3]video-
text pairs and CC3M [65] image-text pairs for joint pretraining with four ob-
jectives: vision-text contrastive learning [3], vision-text matching [41], masked14 K. Li et al.
MSRVTT DiDeMo ANet LSMDC MSVD
Method BB #P
@1 @5 @10 @1 @5 @10 @1 @5 @10 @1 @5 @10 @1 @5 @10
Singularity[39] Swin 5M 28.4 50.2 59.5 36.961.169.3 30.8 55.9 66.3 - - - - - -
Frozen[3] ViT 5M 18.7 39.5 51.6 20.2 46.4 58.5 - - - - - - - - -
ALPRO[40] ViT 5M 24.1 44.7 55.4 23.8 47.3 57.9 - - - - - - - - -
BridgeFormer[24]ViT 5M 26.0 46.4 56.4 25.6 50.6 61.1 - - - 12.2 25.9 32.2 43.674.984.9
UMT[44] ViT 5M 29.6 52.8 61.9 33.4 58.3 67.0 28.3 53.0 64.2 16.8 30.5 37.6 36.2 65.7 76.1
VideoMamba VM 5M32.053.063.8 36.661.770.335.961.172.318.036.143.4 38.0 68.6 79.0
VideoCLIP[85] S3D 136M 10.4 22.2 30.0 16.6 46.9 - - - - - - - - - -
VIOLET[22] Swin138M 25.9 49.5 59.7 23.5 49.8 59.8 - - - - - - - - -
Singularity[39] Swin 17M 34.0 56.7 66.7 37.1 61.7 69.9 30.6 55.6 66.9 - - - - - -
OmniVL[39] ViT 17M 34.6 58.4 66.6 33.3 58.7 68.5 - - - - - - - - -
UMT[44] ViT 17M 35.559.368.6 41.9 66.7 75.0 33.8 59.1 70.4 18.1 33.1 42.2 41.4 70.6 80.1
UMT[44] ViT 25M 35.2 57.8 66.0 41.2 65.4 74.9 35.5 60.6 71.8 19.1 33.4 42.2 42.371.780.8
CLIP4Clip[56] ViT 400M 30.6 54.4 64.3 - - - - - - 13.6 27.9 35.5 36.2 63.8 73.5
InternVideo[83] ViT 640M 40.0 65.3 74.1 31.5 57.6 68.2 30.7 57.4 70.2 17.6 32.4 40.2 43.4 69.9 79.1
VideoMamba VM 17M 34.7 58.9 68.0 42.0 67.3 76.8 40.1 65.7 76.1 18.4 35.3 43.0 40.3 70.0 79.7
VideoMamba VM 25M35.658.169.543.168.177.741.067.577.820.437.145.742.671.681.2
Table 8: Zero-shot text-to-video retrieval on MSRVTT, DiDeMo, Acitivi-
tyNet, LSMDC, and MSVD. “BB” means the visual backbone. “#P” refers to the
numberofpretrainingpairs.Modelspretrainedwithlarge-scalepairsarenotedingray.
language modeling [15] and unmasked token alignment [44]. Initially, we mask
50%imagetokensand80%videotokens,conductingpretrainingacross8frames
for 10 epochs. Given Mamba’s sensitivity to positional information, an addi-
tional unmasked tuning phase is carried out for one epoch to refine its com-
prehension further. For evaluation, we undertake zero-shot video-text retrieval
tasks across five prominent benchmarks, including MSRVTT [86], DiDeMo [1],
ActivityNet [32], LSMDC [62], and MSVD [11].
Results.AsindicatedinTable8,underthesamepretrainingcorpusandsimilar
training strategies, our VideoMamba achieves superior zero-shot video retrieval
performances to UMT [44] based on ViT [16]. It underscores Mamba’s compa-
rable efficiency and scalability to the ViT in handling multi-modal video tasks.
Notably, for datasets featuring longer video lengths (e.g., ANet and DiDeMo)
and more complex scenarios (e.g., LSMDC), VideoMamba demonstrates a sig-
nificant improvement. This demonstrates Mamba’s aptitude for the demands of
cross-modality alignment even in challenging multimodal contexts.
5 Conclusion
In this paper, we propose VideoMamba, a purely SSM-based model for efficient
video understanding. Our extensive experiments demonstrate its scalability in
the visual domain, sensitivity for short-term action recognition, superiority in
long-termvideounderstandingandcompatibilitywithothermodalities.Wehope
it can pave the way for future model design for long-video comprehension.
Limitations. Due to resource constraints, we have not yet fully validated the
scalability of VideoMamba, such as extending VideoMamba to larger sizes (e.g.,
VideoMamba-g),incorporatingadditionalmodalities(e.g.,audio),andintegrat-
ingwithlargelanguagemodelsforhour-levelvideounderstanding.Despitethese
limitations,ourfindingsconfirmVideoMamba’spromisingpotentialandweplan
to conduct thorough explorations of its capabilities in the future.VideoMamba 15
References
1. Anne Hendricks, L., Wang, O., Shechtman, E., Sivic, J., Darrell, T., Russell, B.:
Localizing moments in video with natural language. In: ICCV (2017) 14
2. Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lučić, M., Schmid, C.: Vivit: A
video vision transformer. In: ICCV (2021) 1, 2, 4, 5, 6, 10, 11
3. Bain,M.,Nagrani,A.,Varol,G.,Zisserman,A.:Frozenintime:Ajointvideoand
image encoder for end-to-end retrieval. In: ICCV (2021) 13, 14
4. Bertasius, G., Wang, H., Torresani, L.: Is space-time attention all you need for
video understanding? In: ICML (2021) 1, 2, 4, 5, 6, 10, 11
5. Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D.,
Taylor,J.,Luhman,T.,Luhman,E.,Ng,C.,Wang,R.,Ramesh,A.:Videogener-
ation models as world simulators (2024), https://openai.com/research/video-
generation-models-as-world-simulators 1
6. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan,A.,Shyam,P.,Sastry,G.,Askell,A.,etal.:Languagemodelsarefew-shot
learners. In: NeurIPS (2020) 3
7. Bulat, A., Perez-Rua, J.M., Sudhakaran, S., Martinez, B., Tzimiropoulos, G.:
Space-time mixing attention for video transformer. In: NeurIPS (2021) 4
8. Carreira, J., Noland, E., Banki-Horvath, A., Hillier, C., Zisserman, A.: A short
note about kinetics-600. ArXiv abs/1808.01340 (2018) 3
9. Carreira,J.,Noland,E.,Hillier,C.,Zisserman,A.:Ashortnoteonthekinetics-700
human action dataset. ArXiv abs/1907.06987 (2019) 3
10. Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the
kinetics dataset. In: CVPR (2017) 1, 4
11. Chen,D.L.,Dolan,W.B.:Collectinghighlyparalleldataforparaphraseevaluation.
In: ACL (2011) 3, 14
12. Cho,J.H.,Hariharan,B.:Ontheefficacyofknowledgedistillation.In:ICCV(2019)
9
13. Das, P., Xu, C., Doell, R.F., Corso, J.J.: A thousand frames in just a few words:
Lingualdescriptionofvideosthroughlatenttopicsandsparseobjectstitching.In:
CVPR (2013) 3
14. Deng,J.,Dong,W.,Socher,R.,Li,L.J.,Li,K.,Fei-Fei,L.:Imagenet:Alarge-scale
hierarchical image database. In: CVPR (2009) 8
15. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidi-
rectionaltransformersforlanguageunderstanding.ArXivabs/1810.04805(2018)
7, 14
16. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T.,Dehghani,M.,Minderer,M.,Heigold,G.,Gelly,S.,Uszkoreit,J.,Houlsby,N.:
An image is worth 16x16 words: Transformers for image recognition at scale. In:
ICLR (2021) 2, 5, 7, 14
17. Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., Feichtenhofer, C.:
Multiscale vision transformers. In: ICCV (2021) 10, 11
18. Feichtenhofer,C.:X3d:Expandingarchitecturesforefficientvideorecognition.In:
CVPR (2020) 4, 10
19. Feichtenhofer,C.,Fan,H.,Li,Y.,He,K.:Maskedautoencodersasspatiotemporal
learners. NeurIPS (2022) 7, 10
20. Feichtenhofer, C., Fan, H., Malik, J., He, K.: Slowfast networks for video recogni-
tion. In: ICCV (2019) 1, 4, 10, 1116 K. Li et al.
21. Fu,D.Y.,Dao,T.,Saab,K.K.,Thomas,A.W.,Rudra,A.,Ré,C.:Hungryhungry
hippos: Towards language modeling with state space models. In: ICLR (2023) 3
22. Fu, T.J., Li, L., Gan, Z., Lin, K., Wang, W.Y., Wang, L., Liu, Z.: Violet: End-
to-end video-language transformers with masked visual-token modeling. ArXiv
abs/2111.12681 (2021) 14
23. Gao, J., Sun, C., Yang, Z., Nevatia, R.: Tall: Temporal activity localization via
language query. In: ICCV (2017) 3
24. Ge, Y., Ge, Y., Liu, X., Li, D., Shan, Y., Qie, X., Luo, P.: Bridging video-text
retrieval with multiple choice questions. In: CVP (2022) 14
25. Goyal, R., Kahou, S.E., Michalski, V., Materzynska, J., Westphal, S., Kim, H.,
Haenel, V., Fründ, I., Yianilos, P., Mueller-Freitag, M., Hoppe, F., Thurau, C.,
Bax, I., Memisevic, R.: The “something something” video database for learning
and evaluating visual common sense. In: ICCV (2017) 3, 9
26. Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state
spaces. ArXiv abs/2312.00752 (2023) 2, 3, 4, 5, 7
27. Gu,A.,Goel,K.,Ré,C.:Efficientlymodelinglongsequenceswithstructuredstate
spaces. In: ICLR (2022) 1, 3
28. Gu, C., Sun, C., Vijayanarasimhan, S., Pantofaru, C., Ross, D.A., Toderici, G.,
Li, Y., Ricco, S., Sukthankar, R., Schmid, C., Malik, J.: Ava: A video dataset of
spatio-temporally localized atomic visual actions. CVPR (2017) 3
29. Guo,H.,Li,J.,Dai,T.,Ouyang,Z.,Ren,X.,Xia,S.T.:Mambair:Asimplebaseline
for image restoration with state-space model. ArXiv abs/2402.15648 (2024) 3
30. Han, T., Xie, W., Zisserman, A.: Turbo training with token dropout. In: BMVC
(2022) 13
31. He,X.,Cao,K.,Yan,K.,Li,R.,Xie,C.,Zhang,J.,Zhou,M.:Pan-mamba:Effective
pan-sharpening with state space model. ArXiv abs/2402.12192 (2024) 3
32. Heilbron,F.C.,Escorcia,V.,Ghanem,B.,Niebles,J.C.:Activitynet:Alarge-scale
video benchmark for human activity understanding. In: CVPR (2015) 3, 14
33. Hussein, N., Gavves, E., Smeulders, A.W.M.: Timeception for complex action
recognition. In: CVPR (2019) 13
34. Hussein, N., Gavves, E., Smeulders, A.W.M.: Videograph: Recognizing minutes-
long human activities in videos. ArXiv abs/1905.05143 (2019) 13
35. Idrees,H.,Zamir,A.R.,Jiang,Y.G.,Gorban,A.,Laptev,I.,Sukthankar,R.,Shah,
M.:Thethumoschallengeonactionrecognitionforvideos“inthewild”.Computer
Vision and Image Understanding (2017) 3
36. Islam, M.M., Bertasius, G.: Long movie clip classification with state-space video
models. In: ECCV (2022) 2, 3, 12, 13
37. Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan,
S., Viola, F., Green, T., Back, T., Natsev, A., Suleyman, M., Zisserman, A.: The
kinetics human action video dataset. ArXiv abs/1705.06950 (2017) 3, 9
38. Kuehne,H.,Arslan,A.,Serre,T.:Thelanguageofactions:Recoveringthesyntax
and semantics of goal-directed human activities. In: CVPR (2014) 3, 12
39. Lei,J.,Berg,T.L.,Bansal,M.:Revealingsingleframebiasforvideo-and-language
learning. ArXiv abs/2206.03428 (2022) 14
40. Li,D.,Li,J.,Li,H.,Niebles,J.C.,Hoi,S.C.:Alignandprompt:Video-and-language
pre-training with entity prompts. In: CVPR (2022) 14
41. Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., Hoi, S.C.H.: Align before
fuse:Visionandlanguagerepresentationlearningwithmomentumdistillation.In:
NeurIPS (2021) 13
42. Li,K.,Li,X.,Wang,Y.,Wang,J.,Qiao,Y.:Ct-net:Channeltensorizationnetwork
for video classification. In: ICLR (2020) 11VideoMamba 17
43. Li, K., Wang, Y., He, Y., Li, Y., Wang, Y., Wang, L., Qiao, Y.: Uniformerv2:
Spatiotemporal learning by arming image vits with video uniformer. ArXiv
abs/2211.09552 (2022) 4
44. Li,K.,Wang,Y.,Li,Y.,Wang,Y.,He,Y.,Wang,L.,Qiao,Y.:Unmaskedteacher:
Towards training-efficient video foundation models. In: ICCV (2023) 6, 7, 10, 11,
13, 14
45. Li,K.,Wang,Y.,Peng,G.,Song,G.,Liu,Y.,Li,H.,Qiao,Y.:Uniformer:Unified
transformerforefficientspatial-temporalrepresentationlearning.In:ICLR(2022)
1, 4, 10, 11
46. Li, Y., Wu, C., Fan, H., Mangalam, K., Xiong, B., Malik, J., Feichtenhofer, C.:
Improved multiscale vision transformers for classification and detection. ArXiv
abs/2112.01526 (2021) 10, 11
47. Liang, D., Zhou, X., Wang, X., Zhu, X., Xu, W., Zou, Z., Ye, X., Bai,
X.: Pointmamba: A simple state space model for point cloud analysis. ArXiv
abs/2402.10739 (2024) 3
48. Lin,X.,Petroni,F.,Bertasius,G.,Rohrbach,M.,Chang,S.F.,Torresani,L.:Learn-
ing to recognize procedural activities with distant supervision. CVPR (2022) 2,
12, 13
49. Liu, H., Yan, W., Zaharia, M., Abbeel, P.: World model on million-length video
and language with ringattention. ArXiv abs/2402.08268 (2024) 1
50. Liu, Y., Wang, L., Wang, Y., Ma, X., Qiao, Y.: Fineaction: A fine-grained video
dataset for temporal action localization. TIP (2022) 3
51. Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., Liu, Y.: Vmamba:
Visual state space model. ArXiv abs/2401.10166 (2024) 2, 3, 6, 7, 9
52. Liu,Z.,Lin,Y.,Cao,Y.,Hu,H.,Wei,Y.,Zhang,Z.,Lin,S.,Guo,B.:Swintrans-
former:Hierarchicalvisiontransformerusingshiftedwindows.In:ICCV(2021) 9,
12
53. Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., Hu, H.: Video swin trans-
former. In: CVPR (2022) 2, 4, 10, 11
54. Liu,Z.,Mao,H.,Wu,C.,Feichtenhofer,C.,Darrell,T.,Xie,S.:Aconvnetforthe
2020s. In: CVPR (2022) 9
55. Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visiolin-
guistic representations for vision-and-language tasks. NeurIPS (2019) 3
56. Luo, H., Ji, L., Zhong, M., Chen, Y., Lei, W., Duan, N., Li, T.: Clip4clip: An
empirical study of clip for end to end video clip retrieval and captioning. Neuro-
computing (2022) 14
57. Mehta,H.,Gupta,A.,Cutkosky,A.,Neyshabur,B.:Longrangelanguagemodeling
via gated state spaces. ArXiv abs/2206.13947 (2022) 3
58. Miech, A., Zhukov, D., Alayrac, J.B., Tapaswi, M., Laptev, I., Sivic, J.:
Howto100m: Learning a text-video embedding by watching hundred million nar-
rated video clips. In: ICCV (2019) 3
59. Nguyen, E., Goel, K., Gu, A., Downs, G.W., Shah, P., Dao, T., Baccus, S.A.,
Ré, C.: S4nd: Modeling images and videos as multidimensional signals with state
spaces. In: NeurIPS (2022) 9
60. Patrick, M., Campbell, D., Asano, Y., Misra, I., Metze, F., Feichtenhofer, C.,
Vedaldi,A.,Henriques,J.F.:Keepingyoureyeontheball:Trajectoryattentionin
video transformers. In: NeurIPS (2021) 4, 10, 11
61. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell,A.,Mishkin,P.,Clark,J.,Krueger,G.,Sutskever,I.:Learningtransferable
visual models from natural language supervision. In: ICML (2021) 10, 1218 K. Li et al.
62. Rohrbach, A., Torabi, A., Rohrbach, M., Tandon, N., Pal, C.J., Larochelle, H.,
Courville,A.C.,Schiele,B.:Moviedescription.InternationalJournalofComputer
Vision (2016) 14
63. Shao, D., Zhao, Y., Dai, B., Lin, D.: Finegym: A hierarchical video dataset for
fine-grained action understanding. CVPR (2020) 3
64. Sharir, G., Noy, A., Zelnik-Manor, L.: An image is worth 16x16 words, what is a
video worth? ArXiv abs/2103.13915 (2021) 4, 10
65. Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: A cleaned,
hypernymed, image alt-text dataset for automatic image captioning. In: ACL
(2018) 13
66. Simonyan,K.,Zisserman,A.:Two-streamconvolutionalnetworksforactionrecog-
nition in videos. NeurIPS (2014) 4
67. Smith, J.T., Warrington, A., Linderman, S.W.: Simplified state space layers for
sequence modeling. In: ICLR (2023) 3
68. Soomro,K.,Zamir,A.R.,Shah,M.:Ucf101:Adatasetof101humanactionsclasses
from videos in the wild. arXiv preprint arXiv:1212.0402 (2012) 3
69. Su, J., Lu, Y., Pan, S., Wen, B., Liu, Y.: Roformer: Enhanced transformer with
rotary position embedding. ArXiv abs/2104.09864 (2021) 6
70. Sun, C., Myers, A., Vondrick, C., Murphy, K., Schmid, C.: Videobert: A joint
model for video and language representation learning. In: ICCV (2019) 13
71. Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., Wei, F.: Re-
tentive network: A successor to transformer for large language models. ArXiv
abs/2307.08621 (2023) 1
72. Tang,Y.,Ding,D.,Rao,Y.,Zheng,Y.,Zhang,D.,Zhao,L.,Lu,J.,Zhou,J.:Coin:
A large-scale dataset for comprehensive instructional video analysis. In: CVPR
(2019) 3, 12
73. Team, G.: Gemini: A family of highly capable multimodal models. ArXiv
abs/2312.11805 (2023) 1
74. Team, R.: Rwkv: Reinventing rnns for the transformer era. In: EMNLP (2023) 1
75. Tong, Z., Song, Y., Wang, J., Wang, L.: VideoMAE: Masked autoencoders are
data-efficientlearnersforself-supervisedvideopre-training.In:NeurIPS(2022) 6,
7, 10, 11
76. Touvron,H.,Cord,M.,Douze,M.,Massa,F.,Sablayrolles,A.,J’egou,H.:Training
data-efficientimagetransformers&distillationthroughattention.In:ICML(2021)
2, 8, 9
77. Tran,D.,Bourdev,L.D.,Fergus,R.,Torresani,L.,Paluri,M.:Learningspatiotem-
poral features with 3d convolutional networks. In: IEEE International Conference
on Computer Vision (2015) 1, 4
78. Tran,D.,xiuWang,H.,Torresani,L.,Ray,J.,LeCun,Y.,Paluri,M.:Acloserlook
at spatiotemporal convolutions for action recognition. In: CVPR (2018) 4
79. Wang,C.,Tsepa,O.,Ma,J.,Wang,B.:Graph-mamba:Towardslong-rangegraph
sequence modeling with selective state spaces. ArXiv abs/2402.00789 (2024) 3
80. Wang,L.,Tong,Z.,Ji,B.,Wu,G.:TDN:Temporaldifferencenetworksforefficient
action recognition. In: CVPR (2021) 11
81. Wang, R., Chen, D., Wu, Z., Chen, Y., Dai, X., Liu, M., Jiang, Y.G., Zhou, L.,
Yuan, L.: Bevt: Bert pretraining of video transformers. CVPR (2022) 10, 11
82. Wang, Y., He, Y., Li, Y., Li, K., Yu, J., Ma, X.J., Chen, X., Wang, Y., Luo, P.,
Liu, Z., Wang, Y., Wang, L., Qiao, Y.: Internvid: A large-scale video-text dataset
for multimodal understanding and generation. ArXiv (2023) 3VideoMamba 19
83. Wang, Y., Li, K., Li, Y., He, Y., Huang, B., Zhao, Z., Zhang, H., Xu, J., Liu,
Y., Wang, Z., Xing, S., Chen, G., Pan, J., Yu, J., Wang, Y., Wang, L., Qiao, Y.:
Internvideo: General video foundation models via generative and discriminative
learning. ArXiv abs/2212.03191 (2022) 4, 14
84. Wu, C.Y., Krahenbuhl, P.: Towards long-form video understanding. In: CVPR
(2021) 3, 12, 13
85. Xu,H.,Ghosh,G.,Huang,P.Y.,Okhonko,D.,Aghajanyan,A.,Metze,F.,Zettle-
moyer, L., Feichtenhofer, C.: Videoclip: Contrastive pre-training for zero-shot
video-text understanding. ArXiv abs/2109.14084 (2021) 14
86. Xu, J., Mei, T., Yao, T., Rui, Y.: Msr-vtt: A large video description dataset for
bridging video and language. In: CVPR (2016) 3, 14
87. Yang,Y.,Xing,Z.,Zhu,L.:Vivim:avideovisionmambaformedicalvideoobject
segmentation. ArXiv abs/2401.14168 (2024) 3
88. Yu, Z., Xu, D., Yu, J., Yu, T., Zhao, Z., Zhuang, Y., Tao, D.: Activitynet-qa: A
dataset for understanding complex web videos via question answering. In: AAAI
(2019) 3
89. Zhou, J., Lin, K.Y., Li, H., Zheng, W.: Graph-based high-order relation modeling
for long-term action recognition. CVPR (2021) 13
90. Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., Wang, X.: Vision mamba: Ef-
ficient visual representation learning with bidirectional state space model. ArXiv
abs/2401.09417 (2024) 2, 3, 5, 6, 9
91. Zhuang,S.,Li,K.,Chen,X.,Wang,Y.,Liu,Z.,Qiao,Y.,Wang,Y.:Vlogger:Make
your dream a vlog. ArXiv abs/2401.09414 (2024) 1