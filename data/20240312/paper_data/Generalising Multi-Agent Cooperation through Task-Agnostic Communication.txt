Generalising Multi-Agent Cooperation through
Task-Agnostic Communication
Dulhan Jayalath*1, Steven Morad2, and Amanda Prorok2
1 University of Oxford
dulhan@robots.ox.ac.uk,
2 University of Cambridge
{sm2558, asp45}@cl.cam.ac.uk
Abstract. Existing communication methods for multi-agent reinforce-
ment learning (MARL) in cooperative multi-robot problems are almost
exclusivelytask-specific,trainingnewcommunicationstrategiesforeach
unique task. We address this inefficiency by introducing a communica-
tion strategy applicable to any task within a given environment. We
pre-trainthecommunicationstrategywithout task-specificrewardguid-
ance in a self-supervised manner using a set autoencoder. Our objec-
tive is to learn a fixed-size latent Markov state from a variable number
of agent observations. Under mild assumptions, we prove that policies
using our latent representations are guaranteed to converge, and up-
per bound the value error introduced by our Markov state approxima-
tion. Our method enables seamless adaptation to novel tasks without
fine-tuning the communication strategy, gracefully supports scaling to
moreagentsthanpresentduringtraining,anddetectsout-of-distribution
events in an environment. Empirical results on diverse MARL scenarios
validate the effectiveness of our approach, surpassing task-specific com-
munication strategies in unseen tasks. Our implementation of this work
is available at https://github.com/proroklab/task-agnostic-comms.
Keywords: multi-agent reinforcement learning, multi-agent communi-
cation, decentralised communication
1 Introduction
Reinforcementlearninghassucceededinanumberofroboticsapplicationswhere
classical methods have struggled (Ibarz et al, 2021; Han et al, 2023; Orr and
Dutta, 2023). For multi-robot systems, MARL often promises better scaling
to large numbers of agents (Christianos et al, 2021), and results in interesting
propertieslikeemergenttooluse(Bakeretal,2020).However,MARLissample-
inefficient, making it costly to deploy to real-world robotics tasks (Marinescu
et al, 2017; Zhang et al, 2019, Section 3.2).
MARL is partially observable in the sense that individual agent observations
are often insufficient to learn an optimal policy. Rather, we must reason over all
*Work done while author was at the University of Cambridge.
4202
raM
11
]AM.sc[
1v05760.3042:viXra2 Dulhan Jayalath et al.
agent-levelobservationstofindanoptimalpolicy.Thispartialobservabilityfur-
therworsensthelimitedsampleefficiencysufferedbysingle-agentRL(Buckman
et al, 2018; Yu, 2018). To alleviate these issues in collaborative settings, many
approachesutilisecommunicationtoshareinformationbetweenagents(Foerster
et al, 2016; Sukhbaatar et al, 2016; Das et al, 2019; Bettini et al, 2023). These
methodstypicallyuseadifferentiablestrategy,optimisingmessageswithrespect
to the reinforcement learning objective.
Thus far, differentiable communication for cooperative multi-agent learn-
ing has been entirely task-driven. Previous works have learned communication
strategies for solving riddles (Foerster et al, 2016), traffic control (Sukhbaatar
et al, 2016; Das et al, 2019), navigation (Das et al, 2019; Li et al, 2020), and
tasks requiring heterogeneous behaviour (Bettini et al, 2023). In every example,
agents learn a communication strategy, specific to each task that they are ex-
pectedtosolve.Learningsuchtask-specific communicationstrategiesiswasteful
and inefficient—particularly given the poor sample efficiency of MARL.
Approach.Weproposeimprovingtheefficiencyofcommunicationstrategies
by learning a task-agnostic and environment-specific communication strategy
(Figure 1). We differentiate between an environment (a world and its transition
dynamics)andatask(anoptimisationobjectivewithinanenvironment).Intask-
specific strategies, even if the environment does not change, agents must learn
a new communication strategy for each new task. In contrast, a task-agnostic
strategy can be shared across all tasks within this environment. This is critical
towards designing general-purpose robots as they commonly engage in a variety
of tasks in shared environments (e.g. warehouses, homes, cities, etc.).
Our method also brings several auxiliary advantages. Firstly, by utilising a
specialised set autoencoder, we enable decoding a fixed-size latent state into a
variable-sizedset.Thispermitstrainingthecommunicationstrategytoelegantly
supportvariablenumbersofagentsandtoevenscaleout-of-distribution tomore
agents than seen during training. Additionally, by comparing pre-training losses
tothelossesatruntime,itispossibletodetectout-of-distributiondisturbancesin
anenvironment(e.g.adversarialagentsandunsafeenvironmentstates).Finally,
our approach is grounded in the environment, resulting in messages which have
specific meaning for any task within this environment.
Contributions. We develop a method for learning general, task-agnostic
communication strategies in multi-robot teams that supports variable numbers
ofagents.Weprovidetwoproofswhichdemonstratethat(i)undermildassump-
tions, our method guarantees return convergence and (ii) when these assump-
tionsarenotmet,thereisanupperboundontheregret.Wetestourmethodwith
experiments on tasks in VMAS (Bettini et al, 2022) and the Melting Pot suite
(Agapiou et al, 2023). Our task-agnostic communication strategy outperforms
repurposedtask-specificcommunicationstrategies(i.e.trainedwithpolicylosses
on one task and deployed on another task in the same environment). Moreover,
we provide evidence that performance does not degrade significantly as we scale
the number of agents in the system. Lastly, we showcase how our pre-training
method can be used to detect out-of-distribution events in the environment.Task-Agnostic Communication 3
Environment
A B
= Visibility field
C
Observation Reconstructed
set Offline Pre-Training set
Policy
Set Encoder Set Decoder +
A
A
Local Via On-Policy for A
observation comms
Fig.1: Learning and applying a task-agnostic communication strategy
in MARL. Offline pre-training. We pre-train a set autoencoder with sets of
observations collected from exploring an environment. Since there is no reward
signal involved in sampling these observations, the autoencoder learns a task-
agnostic representation. When a variable number of agent observations are en-
codedbythesetencoder,theoutputisafixed-sizelatentvectorˆsapproximating
theMarkovianstatesinaDec-MDP.Policy training.Foreachagent,wedeploy
thepretrainedsetencodertoencodetheglobalobservation(assembledviacom-
munication) intoˆs on the fly. We condition the behaviour policy onˆs.
2 Preliminaries
We are interested in a subset of cooperative multi-agent problems known as
decentralised Markov decision processes (Dec-MDPs). In the Dec-MDP frame-
work, the environment provides a joint observation o={o ,...,o } from which
1 n
each agent observes its local observation and decides on an action. Upon taking
an action, the global reward function R provides a shared reward for each time
step,andeachagentreceivesitsnextlocalobservation(Bernsteinetal,2002).A
key feature of Dec-MDPs is that the underlying Markov state must be uniquely
defined by the joint set of observations of all agents (i.e. the global observation)
(Oliehoek and Amato, 2016).
Formally, denoting ∆(X) as the set of probability distributions over the set
X,aDec-MDPisdefinedbyatuple(n,S,A,T,O,O,R,γ)wherenisthenumber
of agents, S is the set of states (with initial state s ), A is the set of actions for
0
each agent, T : S × An → ∆(S) is the state transition probability function
T(s′ | s,a), O is the set of joint observations, O : S × An → ∆(O) is the
observation probability function O(o | s,a), R : S ×An → ∆(R) is the global
reward function R(s,a), γ is the discount factor, and the multi-agent Markov
state s is unambiguously determined by O.3
3 This notation is inspired by Oliehoek and Amato (2016) and Ellis et al (2022).4 Dulhan Jayalath et al.
3 Learning Task-Agnostic Communication
In a Dec-MDP, agent cooperation depends on the Markovian state of the multi-
agent team within an environment. This state is independent of task specifics,
consisting of the current state of the environment. Therefore, we define a com-
municationmodelwhichreconstructsthisenvironment-specificinformation,and
show how it can be trained without reward guidance to be completely task-
agnostic.
3.1 A Communication Model for Task-Agnostic Cooperation
As the global observation defines the multi-agent Markov state in a Dec-MDP,
we define our communication model as one in which agents reconstruct the
multi-agent state by reconstructing the joint set of all agents’ observations.
ConsideraDec-MDPdefinedbythetuple(n,S,A,T,O,O,R,γ)withagents
i ∈ A = {1,...,n}. Agents have a communication range ϵ where if the dis-
n
tance d(i,j) between agents i and j is greater than ϵ then they cannot share
information. Thus, we define the neighbourhood of agent i as N = {j ∈ A |
i n
d(i,j) ≤ ϵ,j ̸= i}. In each time step t, an agent i receives a set which contains
the observations of all agents within i’s range,
ONi ={oj |∀j ∈N }. (1)
t t i
Let O denote the joint set of all agent observations in time step t. With ONi
t t
and its local observation oi, the agent can recover the set of observations of all
t
agents within the communication range of i (including itself) in this time step,
Oi =ONi ∪{oi}. (2)
t t t
Usinganautoencoder,thesetOi isencodedintoatask-agnosticlatentstate
t
ˆsi.Thislatentstateispermutation-invariantandisaconstrainedapproximation
t
of the global observation O = {oi,...,on} (and therefore, the Markov state)
t t t
constructed using the information available in Oi and the knowledge of the
t
autoencoder.Theadvantageofthisstateoveraconcatenationofallobservations
isthatitisfixedinsize,supportingvariablenumbersofagents,makesuseofthe
sample efficiency afforded by a permutation-invariant state, and is an efficient
compressed representation.
To use this approximation of the multi-agent state in decision-making, we
condition the policy of agent i on this latent state. Let π denote a policy
θt
parameterisedbyweightsθ .Theprobabilitythatagentitakesactionai isgiven
t t
byπ (ai |ˆsi,oi).Thepolicyisconditionedontheagent’slocalobservation,even
θt t t t
thoughoi isencodedwithinˆsi,becauseˆsi ispermutation-invariant.Withoutoi,
t t t t
the policy cannot determine which agent it is reasoning about.
Whenthelatentstateˆs perfectlycapturestheglobalobservation,thepolicy
t
is guaranteed to converge to a local optimum in the return:Task-Agnostic Communication 5
Theorem 1. A policy gradient method, which conforms to the assumptions in
(Sutton et al, 1999, Theorem 3), conditioned onˆs in a Dec-MDP is guaranteed
t
to converge to a local optimum in the return assuming ˆs captures O with zero
t t
reconstruction error.
Proof. Consider a Dec-MDP defined by the tuple (n,S,A,T,O,O,R,γ). Define
a policy π parameterised by θ that maps the global observation O to a distri-
bution over joint actions A. Formally, π (a | O ) represents the probability of
θ t t
taking joint action a given global observation O and policy parameters θ. The
t t
objectiveistooptimisethepolicyπ tomaximisetheexpectedreturnoveratra-
jectory τ = (s ,a ,s ,a ,...,s ,a ), where s is the multi-agent Markov state
1 1 2 2 T T t
at time t. Assume a policy gradient method, such as REINFORCE (Williams,
1987, 1992), to update the policy parameters θ. This requires estimating the
gradient of the expected return with respect to θ in order to update these pa-
rameters.
Note that (i) policy gradient methods converge to a locally optimal policy
in Markov decision processes (Sutton et al, 1999, Theorem 3), (ii) by definition:
thejointstates inaDec-MDPisthemulti-agentMarkovstate(Bernsteinetal,
t
2002;OliehoekandAmato,2016),and(iii)bydefinition:thisstateisjointlyfully
observable in Dec-MDPs (Bernstein et al, 2002; Oliehoek and Amato, 2016).
Then,since(iii)impliestheglobalobservationO uniquelydefiness (Oliehoek
t t
and Amato, 2016), and by (ii) O defines the multi-agent Markov state, since
t
we use a policy gradient method, by (i) it is guaranteed to converge to a local
optimum as our policy is conditioned on O , which is equivalent to the underly-
t
ingMarkovstate.Whenthelatentstateˆs capturesO withzeroreconstruction
t t
error, this result extends to when the policy is conditioned onˆs instead.
t
However, in practice we cannot assume that ˆs captures O with no error.
t t
To quantify the effect of any error on the return, we can place a bound on the
regret:thedifferenceintheexpectedreturnachievediftheapproximationofthe
underlying Markov state was perfect.
Theorem 2. SupposethepolicyinaDec-MDPanditsassociatedvaluefunction
are Lipschitz continuous. Then the regret of a policy learned from an approxi-
mation ˆs of the underlying Markov state s is bounded above and this bound is
t t
directly proportional to the reconstruction error. 4
Proof. Consider an identical setting to that stated in the proof of Theorem 1.
Additionally, define a value function V derived from the policy π and let ϵ be
πθ θ
some error in reconstructing the underlying multi-agent Markov state s . Thus,
t
ˆs can be decomposed into s +ϵ.
t t
AssumethatV isK LipschitzcontinuouswhereK ∈R.SinceV isderived
πθ πθ
from π , let us also assume that π is Lipschitz continuous.
θ θ
4 Forthistheorem,wetreats asavectors todecomposeitsapproximationintothe
t t
true state and error.6 Dulhan Jayalath et al.
Then,
|V (s )−V (ˆs )|≤K|s −(ˆs )| (3)
πθ t πθ t t t
|V (s )−V (s +ϵ)|≤K|s −(s +ϵ)| (4)
πθ t πθ t t t
=K|−ϵ| (5)
=K|ϵ|. (6)
Thus, the difference in expected return (regret) between a policy which as-
sumes the underlying Markov state is the true state s and one which assumes
t
the underlying Markov state is an approximation ˆs due to reconstruction er-
t
ror ϵ is bounded above by K|ϵ|. Since K is a constant, this bound is directly
proportional to the root mean squared error |ϵ|.
Hence,inaDec-MDP,thereconstructionerrorϵispreciselytheautoencoder’s
error in reconstructing O from the latent state ˆs as the Markovian state s
t t t
is uniquely defined by the global observation. In general, Theorem 2 can be
extended to Dec-POMDPs as a bound on the error in estimating the global
observation rather than the underlying Markov state.
3.2 Training a Task-Agnostic Communication Strategy
We posit that we can learn a task-agnostic communication method by pre-
training an autoencoder with global observations O from exploration of an
t
environment. If the exploration policy is independent of the reward function,
the strategy is task-agnostic. We use either a uniform random policy or uniform
randomsamplingfromtheobservationspace.Neitherrequiresknowingareward
function and hence any method learned in this way is task-agnostic.
Figure 2 provides a detailed overview of our method. Given a permutation-
invariantsetautoencoderwithencoderϕanddecoderϕ−1,wetraintheautoen-
coder with a self-supervised loss
n
1 (cid:88) (cid:16) (cid:17)
l ϕ−1(ϕ(O )),O , (7)
n t t
t=1
wherelisafunctiondefiningasetreconstructionerrorspecifiedbyourchoice
of autoencoder.
Typically, graph autoencoders (GAEs) (Tian et al, 2014; Wang et al, 2016;
Kipf and Welling, 2016) would be ideal to encode sets with permutation invari-
ance.However,weinsteadusethepermutation-invariantsetautoencoder(PISA)
(Kortvelesyetal,2023)because,unlikemanyGAEs,thisarchitectureallowsde-
coding a variable-sized set using a fixed-size latent state. In other words, no
matter the number of agents or the corresponding cardinality of the set O , the
t
dimension of the latent stateˆs is constant. This property is highly desirable as
t
it allows a trained encoder to scale as agents are added or removed from the
environment. If pre-trained on global observations, it also enables the autoen-
codertoapproximatetheglobalobservationevenwhensomeobservationsinthe
multi-agent team are missing.Task-Agnostic Communication 7
While many environments emit two-dimensional pixel observations, PISA
encodesfeaturevectorsintopermutation-invariantstates.Giventhisdichotomy,
when required, we also pre-train a convolutional autoencoder on each element
of O to encode each pixel observation oi into a feature vector vi. Thus, when
t t t
an image encoder is necessary, a set of these feature vectors V is the input to
t
our set autoencoder rather than O directly.
t
4 Experiments & Discussion
We propose three experiments. The
first shows that a task-agnostic com-
Offline and off-policy munication strategy is more effective
than a task-specific strategy when
presented with a novel task. It also
verifies that our proposed strategy
outperforms a baseline that does not
Stop gradient
use communication. Our second ex-
periment validates the claim that
Online and on-policy
ourmethodelegantlyhandlesvariable
numbers of agents. It shows how our
methodfaresasmoreagentsareintro-
Fig.2: Method details. We collect
duced, going out-of-distribution with
global observations by exploring the en-
respect to the number of agents seen
vironment in a task-agnostic manner.
during pre-training. The final experi-
Using these observations, we pretrain a
ment demonstrates that, by compar-
setautoencoder(withencoderϕandde-
ing pre-training autoencoder losses to
coderϕ−1)usingaself-supervisedrecon-
the losses during policy training, we
struction loss. Keeping the autoencoder
can detect out-of-distribution events
weights frozen, we train policies πi on
in the environment.
varioustasksτ.Theinputtoπistheap-
proximation of the Markov stateˆs and
t
the relevant agent’s observation oi. 4.1 Experimental Setup
t
Ourexperimentsfocuson twoMARL
suites. Firstly, Melting Pot (Agapiou
et al, 2023) is a suite of 2D, grid-based, discrete multi-agent learning environ-
ments, providing scenarios that can test a variety of types of coordination fo-
cusing on social dilemmas. In this type of scenario, problems are a mixture of
competition and cooperation so many tasks are not fully cooperative and ac-
cordingly, cannot be Dec-MDPs. To address this, we sum all individual agent
rewards emitted by the base Melting Pot task into a shared global reward. This
ensures each task is fully cooperative.
To supplement Melting Pot, we also study taasks in the vectorised multi-
agent simulator (VMAS) (Bettini et al, 2022): a 2D, continuous-action frame-
work designed for benchmarking MARL. Together, these two suites provide a
comprehensivestudy,astheycoverbothvisualandvectorobservations,discrete8 Dulhan Jayalath et al.
and continuous action spaces, sparse and dense rewards, and different forms
of cooperation requirements, ranging from high-level to low-level collaboration
strategies.
We obtain our pre-training dataset by following a
Pots
uniform random policy (Melting Pot) or uniform ran-
dom sampling from the observation space (VMAS) for
Cooks
a million steps in the environment. With these samples,
Ingredients
we train a task-agnostic communication strategy (Sec-
tion 3.2) and deploy it with our communication model Agent
Target
(Section3.1).Forallofourexperiments,weoptimisethe
policyforeachagentusingProximalPolicyOptimisation
(PPO)(Schulmanetal,2017).Thisiscommonlyreferred
toinmulti-agentliteratureasIndependentPPO(IPPO).
However, we emphasise that our method is algorithm-
agnostic. Any optimisation algorithm may be used with
Sensing range
our method in place of IPPO.
Swarm radius
4.2 Performance on Novel Tasks
In this experiment, we measure the converged return
of our method (task-agnostic) against two baselines Pursuer
as we learn policies for a variety of tasks. The task-
specific baselinesimulatesreusingcommunicationstrate-
gies learned from other tasks. In a real use case, this is Fig.3: Tasks.
theonlyoptiontoavoidtraininganewstrategyifatask- Circuit (top), Dis-
agnostic strategy does not exist. For the task-specific covery (middle),
baseline, we pre-train the set autoencoder using rein- and Pursuit-Evasion
forcement learning while trying to learn a policy for a (bottom).
distinct but similar task in the same environment. We
use an identical setup for pre-training the task-specific
set autoencoder as when we evaluate the task-agnostic
method. In contrast, the task-agnostic baseline uses random samples from the
environment with reconstruction loss for pre-training. This approach lets us as-
sesshowwellatask-agnosticmethodgeneralisescomparedtoatask-specificone
using the same architecture. The no-comms baseline uses no communication
strategy at all. For Melting Pot environments, we additionally utilise an image
encoder,pre-trainedinalltasksinanenvironment,whichweuseforeverybase-
line. We evaluate our method, along with the baselines, on three distinct tasks
(Figure 3):
Collaborative Cooking: Circuit. In Melting Pot’s collaborative cooking
environment, the task is to complete recipes. For our task-agnostic strategy, we
pre-train on the environment and deploy it to learn the Circuit variant, where
twoagentsmustnavigatearoundacircuittoaccesscookingpotsandingredients.
The task-specific variant uses a communication strategy which was learned in
the Cramped variant where agents must cook under tight space restrictions.Task-Agnostic Communication 9
Discovery. In this VMAS task, four agents must try to discover targets. To
get a positive global reward, two out of four agents must position themselves
within a small radius of a target to “discover” it. Together, agents must coordi-
nate to discover targets as fast as possible. New targets continuously spawn as
others are discovered. For this task, the task-specific variant uses a communica-
tion strategy learned in VMAS’ Flocking task where agents must learn to flock,
much like birds do, around a moving target.
Pursuit-Evasion. The VMAS Pursuit-Evasion task is a find-and-intercept
game. The agents are pursuers and they must catch an evading target. The
visibility of the pursuers is limited. They must work together to find the target
andcollectivelyswarmaroundtheminordertocatchthemasfastaspossible.For
this, the task-specific variant uses a communication strategy learned in VMAS’
Discovery task.
task-agnostic (ours) task-specific no-comms
Circuit Discovery Pursuit-Evasion
1.0
0.8 0.8
0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0M 2.0M 4.0M 6.0M 0.0M 5.0M 10.0M 0.0M 5.0M 10.0M
Environment Steps
Fig.4:Task-Agnosticcommunicationstrategiesleadtogreaterrewards
in novel tasks. For each set of results, we report the mean and central 95%
interval over 5 seeds. We trained for 6.4 million environment steps in Melting
Pot tasks, and 12 million environment steps in VMAS tasks. We used the same
number of steps to pre-train the task-specific strategy on a similar task.
We see a significant improvement in return when using task-agnostic strate-
gies (Figure 4). In Circuit, the task-specific baseline fails to achieve a mean
reward much higher than the starting reward. The no-comms baseline stops
improving after around 1M steps. In contrast, our task-agnostic method contin-
uously improves, outperforming both baselines.
In Discovery, both the task-specific and no-comms baselines fail to learn a
useful representation. Meanwhile, the task-agnostic strategy produces a better
policy almost immediately, gradually improving and peaking after around 10M
steps. We outperform the two baselines from just after the start and through to
the end of training.
In Pursuit-Evasion, while the task-specific baseline appears to outperform
the no-comms baseline, much like Discovery, both plateau after a small number
of training steps. The task-agnostic outperforms both baselines.
The results show that task-agnostic communication strategies consistently
enableagentstoleveragecommunicationwithoutrelearningthecommunication
strategy. This is useful in the real world, where cooperative robots engage in a
variety of tasks in a shared environment.
draweR
desilamroN10 Dulhan Jayalath et al.
task-agnostic (ours) no-comms
4 agents 5 agents 10 agents
1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0
0.0M 5.0M 10.0M 0.0M 5.0M 10.0M 0.0M 5.0M 10.0M
Environment Steps
Fig.5: Our task-agnostic strategy scales out-of-distribution. We pre-
trained the communication strategy with 1, 2, and 3 agents for 1M environment
steps and trained the policy for 12M environment steps. For each set of results,
we report mean and central 95% interval over 5 seeds.
4.3 Generalisation with Out-Of-Distribution Numbers of Agents
In real-world situations, new agents may join the multi-agent team during ex-
ecution to support other agents. To cope with such dynamic scenarios, we in-
vestigate how pre-trained reconstructions of the global state generalise to an
out-of-distribution (OOD) number of agents. Since the PISA encoder’s latent
stateisfixed-size,inputcardinalityisindependentofoutputdimensionality—we
can encode a set of any cardinality into a constant size latent vector. Therefore,
in this experiment, we measure the performance of our communication strategy
whenwehavemoreagents,andhencelargersets,thanseenduringpre-training.
Wepre-trainoursetautoencoderintheDiscoveryenvironmentwith1,2,and
3 agents. Then, we train and evaluate a policy on more agents than seen during
pre-training. Under these conditions, in Figure 5, we show that our method still
significantlyoutperformsthebaselinewhenwelearnapolicywith4and5agents,
goingbeyondthenumberofagentsthatwepre-trainedourcommunicationstrat-
egy with. This is evidence that our approach can elegantly handle changes in
connectivity (e.g. from communication disruptions) and can support variable
numbers of agents without fine-tuning. Even once we reach 10 agents, although
theperformancegapissmaller,wecontinuetooutperformtheno-communication
baseline. At this point, we are far outside the training distribution.
4.4 Detecting Out-Of-Distribution Events
Weoftenwanttodetectout-of-distributioneventsinanenvironment.Forexam-
ple, if a disturbance occurs (e.g., humans entering a robot-only operating area,
or adversarial agents accessing the communications network), we want agents
to safely halt or take appropriate actions. In this experiment, we detect OOD
occurrencesbycomparingthereconstructionlossduringtrainingandatruntime.
In Figure 6a, we show how the set reconstruction error changes when we
deployacommunicationstrategypre-trainedwith1,2,and3agentsonDiscovery
draweR
desilamroNTask-Agnostic Communication 11
Pre-training + 3 Pre-training + 3
100
100
10 1
10 2
10 2
Pre-training
3
agents
4
agents
5
agents
Pre-training In-distribution Out-of-distribution
(a) Detecting OOD agents. We re- (b) Detecting OOD observations.
port the mean set reconstruction error We measure the mean set reconstruc-
of PISA over the last 10 iterations of tion error of PISA over the last 10 it-
policy training. erations of policy training.
Fig.6: Detecting out-of-distribution states.
(as in Section 4.3) to train policies with 3 (in-distribution), 4 (OOD), and 5
(OOD)agentsinthesameenvironment.Athresholdsetbythepre-trainingloss
mean plus three standard deviations easily detects the OOD agent counts.
Similarly, we can detect OOD observations. In the Collaborative Cooking
environment, when we fix one of the agents to receive only Gaussian noise ob-
servations (OOD), the loss exceeds our threshold (Figure 6b).
4.5 Limitations.
For simplicity, we use full connectivity between agents. However, this is not a
technical limitation since it can be overcome by propagating information via
aggregation(e.g.aggregatingsetsofPISAencodingswithanotherPISA).Addi-
tionally, collecting pre-training samples with a scheme such as curiosity-driven
exploration (Pathak et al, 2017) could lead to more efficient representations of
the Markovian state from the autoencoder as it samples sparse states more fre-
quently.
5 Related Work
Prior MARL papers have neglected the inefficiency of relearning communica-
tionstrategiesfordistincttasks.Weaddressedthisbyintroducingtask-agnostic
communicationstrategiesthatcanbesharedforalltaskswithinanenvironment.
Differentiable Communication. Differentiable models of communication
optimisemessageswithrespecttotheobjectiveon-policy.Sometypicalexamples
are DIAL (Foerster et al, 2016), CommNet (Sukhbaatar et al, 2016), TarMAC
(Das et al, 2019), Eccles et al (2019), HetGPPO (Bettini et al, 2023), EPC
(Long et al, 2020), SPC (Wang et al, 2023), and Abdel-Aziz et al (2023). All
of these methods employ task-specific communication strategies and thus re-
quire optimising the strategy for each distinct task. In contrast, our method is
rorrE
noitcurtsnoceR
rorrE
noitcurtsnoceR12 Dulhan Jayalath et al.
task-agnostic—taskswithinanenvironmentcanshareourpre-trainedcommuni-
cationstrategies.Whilesomeoftheseworkssupportvariablenumbersofagents
(population-invariant communication), DIAL, TarMAC, Eccles et al (2019) and
Abdel-Aziz et al (2023) do not. Our approach supports population-invariant
communication in addition to task-agnostic communication through a fixed-size
latent state in the autoencoder.
Self-Supervised Communication. Several recent works have used self-
supervisedandcontrastiveobjectivestotraindifferentiablecommunicationstrate-
gies (Lin et al, 2021; Guan et al, 2022; Lo and Sengupta, 2022; Lo et al, 2023).
All of these methods learn the communications policy online, biasing the com-
munications towards a specific objective, while we learn it offline without any
bias. Hence, they are not task-agnostic strategies. Furthermore, none of these
methods support variable numbers of agents as ours does.
Pre-training in RL. Contemporary works in RL have utilised pre-training
toleveragepriorknowledgewhentrainingpolicies(Cruzetal,2017;Singhetal,
2021; Yang and Nachum, 2021; Schwarzer et al, 2021; Seo et al, 2022). Funda-
mentally, they all attempt to learn representations that are useful for solving
the underlying MLP through various unsupervised methods. While all of these
works focus on single-agent RL, we utilise pre-training to improve the efficiency
of MARL.
6 Conclusion
Weproposedtask-agnosticcommunicationstrategiestoeliminatetheinefficiency
of task-specific multi-robot communication. By using a set autoencoder to re-
construct the global state from local observations, our approach is guaranteed
to converge under modest assumptions, with an upper bound on regret due
to approximating the Markovian state. Empirically, it outperforms task-specific
strategies in novel tasks, scales to more agents than in pre-training, and detects
out-of-distribution events during policy training using pre-training losses.
Asitstands,ourmethodisadaptabletovariouslearningparadigms,notjust
RL, because it pre-trains communication strategies in a self-supervised man-
ner. As we avoid end-to-end training, we also expedite RL policy training by
tuning fewer weights. Additionally, having pre-trained an autoencoder, our pol-
icy can use sparse reward signals more efficiently as it does not need to learn
environment-specific features. Lastly, our method opensup new applicationsfor
key real-world robotics tasks such as allowing changing policies at runtime, or
running heterogeneous policies on different collaborative robots.
7 Acknowledgements
ThisworkissupportedbyARLDCISTCRAW911NF-17-2-0181andEuropean
ResearchCouncil(ERC)Project949940(gAIa).Wegratefullyacknowledgetheir
support. We thank Ryan Kortvelesy and Matteo Bettini for their assistance, as
wellasEdanToledo,YonatanGideoni,andJonasJu¨rßforreviewingearlydrafts.Bibliography
Abdel-Aziz MK, Elbamby MS, Samarakoon S, Bennis M (2023) Cooperative
Multi-AgentLearningforNavigationviaStructuredStateAbstraction.CoRR
abs/2306.11336, DOI 10.48550/arXiv.2306.11336, arXiv: 2306.11336
AgapiouJP,VezhnevetsAS,Du´en˜ez-Guzm´anEA,MatyasJ,MaoY,SunehagP,
K¨osterR,MadhushaniU,KopparapuK,ComanescuR,StrouseDJ,Johanson
MB, Singh S, Haas J, Mordatch I, Mobbs D, Leibo JZ (2023) Melting Pot
2.0. DOI 10.48550/arXiv.2211.13746, URL http://arxiv.org/abs/2211.13746,
arXiv:2211.13746 [cs]
Andrychowicz M, Raichuk A, Stanczyk P, Orsini M, Girgin S, Marinier R,
Hussenot L, Geist M, Pietquin O, Michalski M, Gelly S, Bachem O (2020)
WhatMattersInOn-PolicyReinforcementLearning?ALarge-ScaleEmpirical
Study.CoRRabs/2006.05990,URLhttps://arxiv.org/abs/2006.05990,arXiv:
2006.05990
BakerB,KanitscheiderI,MarkovTM,WuY,PowellG,McGrewB,MordatchI
(2020)Emergenttoolusefrommulti-agentautocurricula.In:8thInternational
ConferenceonLearningRepresentations,ICLR2020,AddisAbaba,Ethiopia,
April 26-30, 2020, OpenReview.net, URL https://openreview.net/forum?id=
SkxpxJBKwS
Bernstein DS, Givan R, Immerman N, Zilberstein S (2002) The Complex-
ity of Decentralized Control of Markov Decision Processes. Math Oper Res
27(4):819–840, DOI 10.1287/moor.27.4.819.297
BettiniM,KortvelesyR,BlumenkampJ,ProrokA(2022)VMAS:AVectorized
Multi-Agent Simulator for Collective Robot Learning. DOI 10.48550/arXiv.
2207.03530, URL http://arxiv.org/abs/2207.03530, arXiv:2207.03530 [cs]
Bettini M, Shankar A, Prorok A (2023) Heterogeneous Multi-Robot Rein-
forcement Learning. In: International Conference on Autonomous Agents
and Multiagent Systems, ACM, London, United Kingdom, pp 1485–
1494, DOI 10.5555/3545946.3598801, URL https://dl.acm.org/doi/10.5555/
3545946.3598801
Buckman J, Hafner D, Tucker G, Brevdo E, Lee H (2018) Sample-
Efficient Reinforcement Learning with Stochastic Ensemble Value Expan-
sion. In: Advances in Neural Information Processing Systems, Montr´eal,
Canada, pp 8234–8244, URL https://proceedings.neurips.cc/paper/2018/
hash/f02208a057804ee16ac72ff4d3cec53b-Abstract.html
Christianos F, Papoudakis G, Rahman A, Albrecht SV (2021) Scaling multi-
agent reinforcement learning with selective parameter sharing. In: Meila M,
Zhang T (eds) Proceedings of the 38th International Conference on Machine
Learning,ICML2021,18-24July2021,VirtualEvent,PMLR,Proceedingsof
Machine Learning Research, vol 139, pp 1989–1998, URL http://proceedings.
mlr.press/v139/christianos21a.html14 Dulhan Jayalath et al.
CruzGVdl,DuY,TaylorME(2017)Pre-trainingNeuralNetworkswithHuman
Demonstrations for Deep Reinforcement Learning. CoRR abs/1709.04083,
URL http://arxiv.org/abs/1709.04083, arXiv: 1709.04083
Das A, Gervet T, Romoff J, Batra D, Parikh D, Rabbat M, Pineau J (2019)
TarMAC:TargetedMulti-AgentCommunication.In:InternationalConference
on Machine Learning, PMLR, Long Beach, CA, USA, pp 1538–1546, URL
https://proceedings.mlr.press/v97/das19a.html, iSSN: 2640-3498
EcclesT,BachrachY,LeverG,LazaridouA,GraepelT(2019)BiasesforEmer-
gent Communication in Multi-agent Reinforcement Learning. In: Advances in
Neural Information Processing Systems, Curran Associates, Inc., Vancouver,
BC, Canada, vol 32, URL https://proceedings.neurips.cc/paper/2019/hash/
fe5e7cb609bdbe6d62449d61849c38b0-Abstract.html
Ellis B, Moalla S, Samvelyan M, Sun M, Mahajan A, Foerster JN, Whiteson
S (2022) SMACv2: An Improved Benchmark for Cooperative Multi-Agent
Reinforcement Learning. DOI 10.48550/arXiv.2212.07489, URL http://arxiv.
org/abs/2212.07489, arXiv:2212.07489 [cs]
Foerster J, Assael IA, de Freitas N, Whiteson S (2016) Learning to
Communicate with Deep Multi-Agent Reinforcement Learning. In: Ad-
vances in Neural Information Processing Systems, Curran Associates, Inc.,
Barcelona, Spain, vol 29, URL https://proceedings.neurips.cc/paper/2016/
hash/c7635bfd99248a2cdef8249ef7bfbef4-Abstract.html
Guan C, Chen F, Yuan L, Wang C, Yin H, Zhang Z, Yu Y (2022) Effi-
cient Multi-agent Communication via Self-supervised Information Aggrega-
tion. In: NeurIPS, URL http://papers.nips.cc/paper files/paper/2022/hash/
075b2875e2b671ddd74aeec0ac9f0357-Abstract-Conference.html
HanD,MulyanaB,StankovicV,ChengS(2023)Asurveyondeepreinforcement
learning algorithms for robotic manipulation. Sensors 23(7), DOI 10.3390/
s23073762, URL https://www.mdpi.com/1424-8220/23/7/3762
IbarzJ,TanJ,FinnC,KalakrishnanM,PastorP,LevineS(2021)Howtotrain
your robot with deep reinforcement learning; lessons we’ve learned. CoRR
abs/2102.02915, URL https://arxiv.org/abs/2102.02915, 2102.02915
Kipf TN, Welling M (2016) Variational Graph Auto-Encoders. CoRR
abs/1611.07308, URL http://arxiv.org/abs/1611.07308, arXiv: 1611.07308
Kortvelesy R, Morad SD, Prorok A (2023) Permutation-Invariant Set Autoen-
coders with Fixed-Size Embeddings for Multi-Agent Learning. In: Interna-
tional Conference on Autonomous Agents and Multiagent Systems, ACM,
London,UnitedKingdom,pp1661–1669,DOI10.5555/3545946.3598823,URL
https://dl.acm.org/doi/10.5555/3545946.3598823
KurachK,RaichukA,StanczykP,ZajacM,BachemO,EspeholtL,RiquelmeC,
Vincent D, Michalski M, Bousquet O, Gelly S (2020) Google Research Foot-
ball: A Novel Reinforcement Learning Environment. In: The Thirty-Fourth
AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second
Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The
Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,
EAAI 2020, New York, NY, USA, February 7-12, 2020, AAAI Press, ppTask-Agnostic Communication 15
4501–4510, DOI 10.1609/AAAI.V34I04.5878, URL https://doi.org/10.1609/
aaai.v34i04.5878
Li Q, Gama F, Ribeiro A, Prorok A (2020) Graph Neural Networks for De-
centralized Multi-Robot Path Planning. In: International Conference on In-
telligent Robots and Systems, Las Vegas, NV, USA, pp 11,785–11,792, DOI
10.1109/IROS45743.2020.9341668, iSSN: 2153-0866
Lin T, Huh J, Stauffer C, Lim SN, Isola P (2021) Learning to Ground
Multi-Agent Communication with Autoencoders. In: Advances in Neural
Information Processing Systems, Curran Associates, Inc., Virtual Event,
vol 34, pp 15,230–15,242, URL https://proceedings.neurips.cc/paper/2021/
hash/80fee67c8a4c4989bf8a580b4bbb0cd2-Abstract.html
LoYL,SenguptaB(2022)LearningtoGroundDecentralizedMulti-AgentCom-
municationwithContrastiveLearning.DOI10.48550/arXiv.2203.03344,URL
http://arxiv.org/abs/2203.03344, arXiv:2203.03344 [cs]
LoYL,SenguptaB,FoersterJN,NoukhovitchM(2023)LearningtoCommuni-
cateusingContrastiveLearning.CoRRabs/2307.01403,DOI10.48550/arXiv.
2307.01403, arXiv: 2307.01403
Long Q, Zhou Z, Gupta A, Fang F, Wu Y, Wang X (2020) Evolution-
ary Population Curriculum for Scaling Multi-Agent Reinforcement Learn-
ing. In: 8th International Conference on Learning Representations, ICLR
2020,AddisAbaba,Ethiopia,April26-30,2020,OpenReview.net,URLhttps:
//openreview.net/forum?id=SJxbHkrKDH
Marinescu A, Dusparic I, Clarke S (2017) Prediction-Based Multi-Agent Rein-
forcement Learning in Inherently Non-Stationary Environments. ACM Trans
Auton Adapt Syst 12(2):9:1–9:23, DOI 10.1145/3070861
Oliehoek FA, Amato C (2016) A Concise Introduction to Decentralized
POMDPs. Springer Briefs in Intelligent Systems, Springer, DOI 10.1007/
978-3-319-28929-8
Orr J, Dutta A (2023) Multi-agent deep reinforcement learning for multi-robot
applications: A survey. Sensors 23(7), DOI 10.3390/s23073625, URL https:
//www.mdpi.com/1424-8220/23/7/3625
Pathak D, Agrawal P, Efros AA, Darrell T (2017) Curiosity-driven Explo-
ration by Self-supervised Prediction. In: Precup D, Teh YW (eds) Interna-
tional Conference on Machine Learning, PMLR, Sydney, NSW, Australia,
Proceedings of Machine Learning Research, vol 70, pp 2778–2787, URL
http://proceedings.mlr.press/v70/pathak17a.html
SamvelyanM,RashidT,WittCSd,FarquharG,NardelliN,RudnerTGJ,Hung
CM, Torr PHS, Foerster JN, Whiteson S (2019) The StarCraft Multi-Agent
Challenge. In: Elkind E, Veloso M, Agmon N, Taylor ME (eds) Proceedings
of the 18th International Conference on Autonomous Agents and MultiAgent
Systems,AAMAS’19,Montreal,QC,Canada,May13-17,2019,International
Foundation for Autonomous Agents and Multiagent Systems, pp 2186–2188,
URL http://dl.acm.org/citation.cfm?id=3332052
Schulman J, Wolski F, Dhariwal P, Radford A, Klimov O (2017) Proximal
Policy Optimization Algorithms. DOI 10.48550/arXiv.1707.06347, URL http:
//arxiv.org/abs/1707.06347, arXiv:1707.06347 [cs]16 Dulhan Jayalath et al.
Schwarzer M, Rajkumar N, Noukhovitch M, Anand A, Charlin L, Hjelm
RD, Bachman P, Courville AC (2021) Pretraining Representations for
Data-Efficient Reinforcement Learning. In: Ranzato M, Beygelzimer A,
Dauphin YN, Liang P, Vaughan JW (eds) Advances in Neural Infor-
mation Processing Systems 34: Annual Conference on Neural Informa-
tion Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, vir-
tual, pp 12,686–12,699, URL https://proceedings.neurips.cc/paper/2021/
hash/69eba34671b3ef1ef38ee85caae6b2a1-Abstract.html
SeoY, Lee K,James SL,AbbeelP (2022)Reinforcement Learningwith Action-
FreePre-TrainingfromVideos.In:ChaudhuriK,JegelkaS,SongL,Szepesv´ari
C, Niu G, Sabato S (eds) International Conference on Machine Learning,
PMLR,Baltimore,MD,USA,ProceedingsofMachineLearningResearch,vol
162, pp 19,561–19,579, URL https://proceedings.mlr.press/v162/seo22a.html
Singh A, Liu H, Zhou G, Yu A, Rhinehart N, Levine S (2021) Parrot: Data-
Driven Behavioral Priors for Reinforcement Learning. In: 9th International
Conference on Learning Representations, ICLR 2021, Virtual Event, Aus-
tria, May 3-7, 2021, OpenReview.net, URL https://openreview.net/forum?
id=Ysuv-WOFeKR
Sukhbaatar S, Szlam A, Fergus R (2016) Learning Multiagent Com-
munication with Backpropagation. In: Advances in Neural Infor-
mation Processing Systems, Curran Associates, Inc., Barcelona,
Spain, vol 29, URL https://proceedings.neurips.cc/paper/2016/hash/
55b1927fdafef39c48e5b73b5d61ea60-Abstract.html
Sutton RS, McAllester D, Singh S, Mansour Y (1999) Policy Gradient Meth-
ods for Reinforcement Learning with Function Approximation. In: Ad-
vances in Neural Information Processing Systems, MIT Press, Denver,
CO, USA, vol 12, URL https://proceedings.neurips.cc/paper/1999/hash/
464d828b85b0bed98e80ade0a5c43b0f-Abstract.html
TianF,GaoB,CuiQ,ChenE,LiuTY(2014)LearningDeepRepresentationsfor
Graph Clustering. In: Brodley CE, Stone P (eds) Proceedings of the Twenty-
Eighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Qu´ebec
City, Qu´ebec, Canada, AAAI Press, pp 1293–1299, DOI 10.1609/aaai.v28i1.
8916
Wang D, Cui P, Zhu W (2016) Structural Deep Network Embedding. In: Kr-
ishnapuram B, Shah M, Smola AJ, Aggarwal CC, Shen D, Rastogi R (eds)
Proceedings of the 22nd ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, San Francisco, CA, USA, August 13-17,
2016, ACM, pp 1225–1234, DOI 10.1145/2939672.2939753
WangR,ZhengL,QiuW,HeB,AnB,RabinovichZ,HuY,ChenY,LvT,Fan
C (2023) Towards Skilled Population Curriculum for Multi-Agent Reinforce-
ment Learning. CoRR abs/2302.03429, DOI 10.48550/ARXIV.2302.03429,
URL https://doi.org/10.48550/arXiv.2302.03429, arXiv: 2302.03429
Williams R (1987) A class of gradient-estimation algorithms for reinforcement
learninginneuralnetworks.In:InternationalConferenceonNeuralNetworks,
San Diego, CA, USA, pp II–601Task-Agnostic Communication 17
Williams RJ (1992) Simple Statistical Gradient-Following Algorithms for Con-
nectionist Reinforcement Learning. Mach Learn 8:229–256, DOI 10.1007/
BF00992696
Yang M, Nachum O (2021) Representation Matters: Offline Pretraining for Se-
quentialDecisionMaking.In:MeilaM,ZhangT(eds)Proceedingsofthe38th
International Conference on Machine Learning, ICML 2021, 18-24 July 2021,
Virtual Event, PMLR, Proceedings of Machine Learning Research, vol 139,
pp 11,784–11,794, URL http://proceedings.mlr.press/v139/yang21h.html
Yu Y (2018) Towards Sample Efficient Reinforcement Learning. In: Lang J (ed)
International Joint Conference on Artificial Intelligence, ijcai.org, Stockholm,
Sweden, pp 5739–5743, DOI 10.24963/ijcai.2018/820
Zhang K, Yang Z, Basar T (2019) Multi-Agent Reinforcement Learning: A Se-
lective Overview of Theories and Algorithms. CoRR abs/1911.10635, URL
http://arxiv.org/abs/1911.10635, arXiv: 1911.1063518 Dulhan Jayalath et al.
A Modifications To Tasks And Environments
As alluded to in Section 4.1, we enforce that Melting Pot tasks are Dec-MDPs
with a global reward R. In the default case, an agent i receives an individual
reward r . We modify this such that each individual agent reward r is summed
i i
tomakeaglobalrewardR=(cid:80)nr
andthisglobalrewardissharedbetweenall
i i
agents instead of them receiving their individual reward.
In all collaborative cooking task variants, we enable a pseudo-reward that
rewardsagentswith+1forpickingupatomatoandputtingitinapot.Without
this pseudo-reward, these tasks have extremely sparse rewards that make them
highly challenging to learn. This shaped reward assists in learning on these
typically very difficult tasks.
InVMAS,wemodifytheobservationspacesofDiscoveryandFlockingtobe
identical by replacing Discovery’s redundant additional position vector feature
withatwo-dimensionalzerovectorandreducingthenumberofLiDARraysfrom
15 to 12. This allows us to use communication strategies learned in Flocking
on Discovery. We argue that Flocking and Discovery are two tasks in the same
environmentbecausetheobstaclesinFlockingcanbetreatedastargetstoderive
the Discovery task from Flocking (along with a change of reward).
Furthermore, we designed and created Pursuit-Evasion ourselves based on
both Flocking and Discovery. This is another task that we argue is in the same
environment. It can be derived from Flocking if we treat Flocking’s target as a
robber, and the agents as the police after modifying the reward. By extension,
it can also be derived from Discovery.
B Permutation-Invariant Set Autoencoder Architecture
The set autoencoder architecture is based on Kortvelesy et al (2023, Figure 1).
It encodes a variable-sized set of elements {x ,x ,...,x } where x ∈ Rn into
1 2 n i
a fixed-size permutation-invariant latent state z ∈ Rz. It is trained with a self-
supervised reconstruction objective to decode the latent state and recover the
set.
Encoder. The encoder takes the input set {x ,x ,...,x } and maps each
1 2 n
element to a key k according to some criterion and encodes the keys using a
i
networkψ .Simultaneously,theencodertakestheinputsetelementsandalso
key
encodes them into values using a separate network ψ . The encoder then takes
val
the element-wise product of the corresponding key and value embeddings and
sums them all. Finally, a cardinality embedding λ (n) is added to this sum to
enc
form the final latent state z.
Decoder. The decoder takes the latent state z and predicts the cardinality
of the set with a network λ . The predicted cardinality is used to create a set
dec
ofkeys asinthe encoder andthekeys aremappedtoqueriesbyanetworkϕ .
key
Each query is element-wise multiplied by the latent state and a final decoder
network ϕ recovers the set from these embeddings.
dec
FurtherdetailsmaybefoundinKortvelesyetal(2023).Thehyperparameters
used in our work are detailed in Appendix E.Task-Agnostic Communication 19
C Policy Network Architectures
When training to solve Melting Pot tasks, we independently train the policy of
eachagentwithPPO.Foreachagent,wehaveindependentthree-layerMLPsas
ourpolicyandvaluenetworks.Thepolicynetwork’shiddenlayeris128neurons
wide, while the value network’s hidden layer is 1024 neurons wide. We initialise
thelastlayerofthepolicynetworkandvaluenetworkusinganormaldistribution
with zero mean and 0.01 standard deviation in line with the suggestions made
by Andrychowicz et al (2020).
UnlikeMeltingPot,asnoheterogeneousbehaviourisrequiredforourVMAS
tasks, we train a policy that’s shared between all agents with PPO. For each
agent, we have independent three-layer MLPs as our policy and value networks.
ForDiscovery,thepolicyandvaluenetworkshavea256-widehiddenlayerwhile
for Pursuit-Evasion, the hidden layers are 512-wide. We initialise the last layer
of the policy and value networks with the same normal distribution as we use
for Melting Pot.
D Training Hyperparameters
Our training hyperparameters are dependent on the multi-agent suite and task
andaredescribedinTable1and2.Wealwaysusefixedseeds0-4foreveryexper-
iment. Specifically for Pursuit-Evasion, we use the defaults for the parameters
in Table 2 except for train batch size, SGD minibatch size, training iterations,
and rollout fragment length.
Table 1: Melting Pot training hyperparameters.
Parameter Value
Train batch size 6400
SGD minibatch size 128
Training iterations 1000
Rollout fragment length 100
E Pre-Training Hyperparameters
For Melting Pot environments we train an image encoder in addition to PISA.
Observations are first encoded with the image encoder before this embedding
is passed to PISA. For the image encoder, we use a 3-layer CNN encoder and
decoder as specified in Table 3. Our training data is gathered from observations
of all agents generated with a uniform random policy rolled out over 1M en-
vironment steps. We train the image encoder with a mini-batch size of 32 for
approximately 1000 iterations or until the loss has clearly converged.20 Dulhan Jayalath et al.
Table 2: VMAS training hyperparameters.
Parameter Value
Train batch size 60000
SGD minibatch size 4096
Training iterations 200
Rollout fragment length 125
KL coefficient 0.01
KL target 0.01
λ 0.9
Clip 0.2
Value function loss coefficient 1
Value function clip ∞
Entropy coefficient 0
η 5e-5
γ 0.99
Table 3: Melting Pot image autoencoder architecture.
Layer Type In ch.Out ch.KernelStridePaddingActivation
EncoderConv2D 3 16 3 2 1 ReLU
Conv2D 16 32 3 2 1 ReLU
Conv2D 32 64 3 2 1 ReLU
Linear 1600 128 - - - -
Decoder Linear 128 1600 - - - -
ConvTranspose2D 64 32 3 2 1 ReLU
ConvTranspose2D 32 16 3 2 1 ReLU
ConvTranspose2D 16 3 3 2 1 Sigmoid
Forthesetautoencoder,weusethedefaultimplementationofPISAprovided
in the author’s repository5. We train PISA with a latent dimension of 256 with
a batch size of 32 for 15000 iterations or until the loss has clearly converged.
Unlike Melting Pot, we do not train an image encoder for VMAS environ-
ments as observations are already feature vectors. We train PISA with a latent
dimension of 72 with a batch size of 256 for 15000 iterations where the loss has
clearly converged.
SinceVMASenvironmentsareextremelysimple,wefindthatuniformlyran-
domly sampling from the observation space to generate pre-training data works
well.Thisleadstolearningastrongautoencoderwherethereconstructionlossis
very small during policy training. Hence, we use this method in our final results
ratherthanauniformrandompolicy.Bothleadtotask-agnosticcommunication
strategies as they are reward-free.
5 https://github.com/Acciorocketships/SetAutoEncoder/tree/mainTask-Agnostic Communication 21
F Choosing MARL Benchmarks
Whileother well-known MARLbenchmarks exist,wechoose notto usethese as
they either do not require communication to solve (Samvelyan et al, 2019), lack
sufficient task variation (Samvelyan et al, 2019; Ellis et al, 2022; Kurach et al,
2020), or are not Dec-MDP/POMDPs (Kurach et al, 2020).
While SMAC (Samvelyan et al, 2019) is commonly used in prior literature,
many of its environments can be solved with open-loop policies (i.e. with ob-
servations of just the agent ID and time step) (Ellis et al, 2022). As a result,
communication is not necessary to solve it. While SMACv2 (Ellis et al, 2022)
resolves some of these issues, the objective remains simply to kill all the enemy
agents. Consequently, this environment does not have enough task variation to
test task-agnostic communication, the main contribution of this paper. GRF
(Kurach et al, 2020) has similar issues.
MeltingPot(Agapiouetal,2023)representssimilarlychallengingtasks.Like
SMACandGRF,itfeatureshigh-dimensionalpixel-basedobservationsandcom-
plexobjectives.Itisanew,butstate-of-the-artbenchmark.VMAS(Bettinietal,
2022)is alsoa suitably challengingbenchmark. Thevisualisationsin VMASap-
pear simple, but the dynamics are complex, going beyond kinematics by simu-
lating elastic collisions, rotations, and joints. Thus, while the environments are
conceptually basic, VMAS represents a realistic challenge to agents.
G Computation, Hardware, and Implementation Details
We implemented our work with the Ray RLLib library (version 2.1.0 for VMAS
and 2.3.1 for Melting Pot) and wrote all our models with the PyTorch frame-
work. Our models and policies were primarily trained on individual NVIDIA
A100GPUswith40GiBofmemoryandNVIDIARTX2080TiGPUswith11GiB
of memory. Experiments were conducted with 5 workers for VMAS with 32 vec-
torisedenvironmentsand2workersforMeltingPot.Ineachcase,weusedasingle
driver GPU while environment simulations were carried out on CPU. Training
a policy for 12M environment steps on a VMAS task took approximately 6-12
hours, while 6.4M environment steps on Melting Pot took about 18-24 hours.
Pre-training the image encoder took about 6 hours and pre-training PISA took
about 1 hour for VMAS and 6 hours for Melting Pot.