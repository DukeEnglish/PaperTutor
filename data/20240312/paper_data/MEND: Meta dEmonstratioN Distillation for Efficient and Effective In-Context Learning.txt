PublishedasaconferencepaperatICLR2024
MEND: eta d monstratio istillation for
M E N D
Efficient and Effective In-Context Learning
YichuanLi1‚àó, XiyaoMa2, SixingLu2, KyuminLee1, XiaohuLiu2, ChenleiGuo2
1WorcesterPolytechnicInstitute,2AmazonAlexaAI
{yli29,kmlee}@wpi.edu
{maxiya,cynthilu,derecliu,guochenl}@amazon.com
Abstract
LargeLanguagemodels(LLMs)havedemonstratedimpressivein-contextlearning
(ICL)capabilities,whereaLLMmakespredictionsforagiventestinputtogether
with a few input-output pairs (demonstrations). Nevertheless, the inclusion of
demonstrationsleadstoaquadraticincreaseinthecomputationaloverheadoftheself-
attentionmechanism. Existingsolutionsattempttodistilllengthydemonstrations
into compact vectors. However, they often require task-specific retraining or
compromiseLLM‚Äôsin-contextlearningperformance. Tomitigatethesechallenges,
we present Meta dEmonstratioN Distillation (MEND), where a language model
learnstodistillanylengthydemonstrationsintovectorswithoutretrainingfora
newdownstreamtask. Weexploittheknowledgedistillationtoenhancealignment
betweenMENDandLLM,achievingbothefficiencyandeffectivenesssimultaneously.
MENDisendowedwiththemeta-knowledgeofdistillingdemonstrationsthrough
a two-stage training process, which includes meta-distillation pretraining and
fine-tuning. ComprehensiveevaluationsacrosssevendiverseICLtaskpartitions
usingdecoder-only(GPT-2)andencoder-decoder(T5)attesttoMEND‚Äôsprowess.
It not only matches but often outperforms the Vanilla ICL as well as other
state-of-the-artdistillationmodels,whilesignificantlyreducingthecomputational
demands. This innovation promises enhanced scalability and efficiency for the
practicaldeploymentoflargelanguagemodels1.
1 Introduction
Largelanguagemodels(LLMs)havedemonstratedexceptionalpowerinin-contextlearning(Kaplan
etal.,2020;Brownetal.,2020;Dongetal.,2023;Minetal.,2022a). Theycanrelyonalimited
numberofinput-outputpairs,oftentermeddemonstrations,togenerateoutputsforagiventestinput,
withoutparameterupdates. However,asignificantbottleneckarises: incorporatingdemonstrations
exacerbates input length for LLMs. This is concerning, especially considering the self-attention
mechanism inherent in these models, which imposes time and memory complexities that scale
quadraticallywithinputlength.
Attempts to mitigate this challenge typically focus on trimming the context length by distilling
extensivedemonstrationsintoconcisevectorsasshowninFig.1. Thesevectorsarethenusedto
prompttheLLMtogenerateoutputs(Phangetal.,2023;Ivisonetal.,2022;Muetal.,2023;Lester
etal.,2021). Distillationapproaches,however,differacrossmethodologies. Forinstance,methods
such as prompt tuning (Lester et al., 2021; Wang et al., 2023) produce vectors through gradient
descent. Nonetheless,theseapproachesnecessitatespecificretrainingfordifferentdemonstrations.
Incontrast, theintroductionofhypernetworks(Haetal.,2016)offersasolutionthatreducesthe
reliance on gradient descent for any given demonstrations. Methods like Hypertuning (Phang
etal.,2023)andHINT(Ivisonetal.,2022)employconditionallanguagemodeling(CLM)objectives
tofinetunealanguagemodelbaseddistillationmodel,distillingdemonstrationsintovectors. Yet,
whenbenchmarkedagainsttheVanilla ICLmethod‚ÄîwhereLLMsareprompteddirectlywiththe
unaltereddemonstrationtext‚Äîtheperformanceexhibitsdiscernibledegradationsusingthesedistilled
‚àóThisworkwasmainlydoneduringYichuan‚ÄôsinternshipatAmazon.
1Thecodeisavaliableathttps://github.com/bigheiniu/MEND.
1
4202
raM
11
]LC.sc[
1v41960.3042:viXraPublishedasaconferencepaperatICLR2024
vectors. Thistrendremainsconsistent,evenwhendistillationmodelsareco-trainedwiththeLLMin
ICLdata(Ivisonetal.,2022). Giventhattheselanguagemodelbaseddistillationmodelsinherently
possessin-contextlearningcapabilitiesandcangeneratemeaningfulrepresentations,theremaining
questionishowtooptimizethemtogeneratedemonstrationdistillationthatrivalorevensurpassthe
efficacyofVanilla ICL.AchievingthiswouldpavethewayforenhancingICLefficiencywithout
compromisingitsefficacy.
Duringpretraining,LLMsusuallylearnusingdetailedword
data. Butatdemonstrationdistillationscenario,theyhave
toworkwithasimplifiedversionofthisdata‚Äìdistilled Vanilla-ICL PromptTuning
vectors. It‚Äôslikestudyingwithafulltextbookbuttaking Output Output
thetestwithonlyasummary. Wethinkit‚Äôsreallyimportant
Large Language Model Large Language Model
tomakesurethattheLLMcanunderstandandusethese
summariesjustaswellasthefulltextbook. Thishelpsthe Demo. Input Distil. Input
LLMperformbetterwhenit‚ÄôsactuallybeingusedforICL.
Hypernetwork
Toaddressthis,weintroducetheMetadEmonstrationN
Distillation(MEND).Ourapproachrealignsthedistillation Distil. Output
model,MENDandLLMthroughknowledgedistillation(Hin- Demo. Distillation Large Language Model
tonetal.,2015;Snelletal.,2022). Here,theLLM,when Model
promptedsolelywiththedistilledvectors(actingasthe Demo. Distil. Input
student),isconditionedtoemulatethebehavioritwould
exhibitwhenexposedtothefulldemonstrations(assuming Figure 1: Vanilla ICL method uti-
the role of the teacher). To achieve this, we minimize lizes the concatenation of demonstra-
theKullback‚ÄìLeibler(KL)divergencebetweenteacherand tionsandtestinputtogeneratetheout-
studentmodels‚Äôworddistributions. Importantly,during put. In contrast, PromptTuning and
thisoptimizationprocess,webackpropagatethegradients HyperNetworks employ distilled vec-
fromtheLLMtoMEND,whileensuringthattheLLMremains torsinplaceofthefulldemonstrations.
frozen throughout. The training paradigm for MEND is The length of these distilledvectors
twofold: meta-distillationpretrainingonstandardtextpre- is significantly shorter than that of the
trainingdata(e.g. C4(Raffeletal.,2019)),followedby
demonstrations,contributingtoamore
finetuningonICLtasks. Thistwo-stagetrainingequips
compactandefficientin-contextlearning
MEND with the meta-knowledge for distilling demonstra-
forLLM.
tions,allowingittogeneralizeeffectivelyacrossunseen
demonstrationswithoutsacrificingperformance.
TodemonstratethefeasibilityofMEND,weapplyittoavarietyofLLMarchitectures,includingboth
decoder-only(e.g.,GPT-2(Brownetal.,2020))andencoder-decoderconfigurations(e.g.,T5 (Raffel
et al., 2019)). InourexperimentsontheMetaICLdataset(Minetal.,2022a),encompassing142
uniqueNLPtasksdividedacrosssevenpartitions,MENDconsistentlymeetsorexceedstheperformance
ofVanilla ICL,notablyoutperformingwheretraditionalhypernetworkapproachesfalter. Across
therangeoflanguagemodelsweinvestigated,ourdistillationstrategyresultsinasubstantialreduction
ofupto75%inFLOPsandacceleratesinferencebyupto33%. Beyondstandardevaluations,we
embarked on an in-depth diagnostic analysis where we tweaked the distillation ratio and added
intentional disturbances to the demonstrations. In these scenarios, MEND proved resilient to the
disruptionsandconsistentlyoutpacedstandardVanilla ICLmethods.
Summarizingourwork,ourcontributionsarethreefold: (1)TheintroductionofMEND,aninnovative
techniqueaimedatenhancingtheLLM‚Äôsin-contextlearningefficiencywithoutcompromisingthe
performance;(2)Anexplorationintothebenefitsofknowledgedistillationforaligningthedemonstra-
tiondistillationmodelwithLLM;(3)Comprehensivequantitativeandqualitativeexaminationsthat
highlighttherobustnessandeffectivenessofMEND.
2 ProblemDefinition
LetD ={(x ,y )}K beademonstrationset,wherex andy denotetheinputandoutputtokens
i i i=1 i i
respectively, and K is the number of input-output pairs or demonstrations. Let D denote the
concatenationofdemonstrationsetthatisD =concat(x ,y ,¬∑¬∑¬∑x ,y )2. Inin-contextlearning
1 1 K K
(ICL), given D, and test input x, the large language model (LLM) will compute the conditional
2Inthefollowingsectionswewilluseconcatenateddemonstrationsandcontextinterchangeably.
2PublishedasaconferencepaperatICLR2024
Frozen Ground Truth
Trainable ‚Ñí !"#$ LLMHead DiS stt ru id be un tit o n LLMHead S Dt ae tp a1 : : Meta-D Tis et xil tl a St eio qn u eP nr ce et sra (i 1n 0in 2g 4)
‚Ä¶ ‚Ä¶ Demo.: First 1024 √óùõΩ
Transformer Layer Transformer Layer Transformer Layer Input: 1024√ó (1‚àí ùõΩ)
‚Ä¶ ‚Ä¶ ‚Ñí$%&‚Äô%(( ‚Ä¶ ‚Ñí,-$%-."#=‚Ñí/"0%"11
Transformer Layer Transformer Layer Transformer Layer
Position Embedding Positional Embedding Position Embedding Step2: Meta-Distillation Finetuning
Wor‚Ä¶ d EmbedP dla icD nei gs ht oil l. d er WoD ri dst i El. mbedding DiT ste ra ibch ue tir o n Word‚Ä¶ Embedding D Da et ma: o .{ : c o ùë• n " c a t ( { ùë¶ " ùë• " } ") ‚Äô * ( + ùë¶ " . }") ‚Äô()
Input: ùë•)*+
Demo. Input Demo. Input
Weight Ground Truth: ùë¶)*+
MEND Student: LLMwith Sharing Teacher: LLM with ‚Ñí!"#$%&#$=.‚Ñí , - $ / + ùúÜ‚Ñí/"0%"11
Demonstration Distillation Original Demonstration
Figure2: OverviewofMEND.MENDtakesasinputdemonstrationsanddistillationplaceholder,outputs
distillationvectors. Tocapturethemeta-knowledgeofdemonstrationdistillation,MENDistrainedin
twostages: meta-distillationpretrainingandfientuning.
probabilityforeachlabelc‚ààC andreturnthemaximumconditionalprobabilityas:
argmax c‚ààCPLLM(c|concat(E D,E x)), (1)
whereC istheuniquesetof{y }K inclassificationtasksoransweroptionsinquestionanswering
i i=1
tasks,andE isLLM‚Äôswordembedding.
(¬∑)
ToimprovetheefficiencyofICL,manyrelatedworks(Lesteretal.,2021;Phangetal.,2023;Ivison
etal.,2022;Wangetal.,2023;Muetal.,2023)aimtoreducethedemonstrationslengthforLLMfrom
|D|intolsuchthatl<<|D|. Theysynthesizeahigh-fidelitydemonstrationsummaryS ‚ààRl√ód,
D
wheredisthehiddensizeofwordembedding,toreplaceD:
argmax P (c|concat(S ,E )). (2)
c‚ààC LLM D x
Prompttuningapproaches(Lesteretal.,2021;Wangetal.,2023)considerS aslearnableparameters.
D
However, for other tasks‚Äô demonstrations like D‚Ä≤, it requires additional training time to get S .
D‚Ä≤
Hypernetworkapproaches(Phangetal.,2023;Ivisonetal.,2022;Muetal.,2023)includingour
MENDaddressthechallengeofretrainingfornovel,unseentasks. Theyachievethisbyemployinga
demonstrationdistillationmodel,denotedasM,whichproducedistillationvectors: S =M(EÀÜ )
D D
andS =M(EÀÜ ). ThesevectorscorrespondtoanyarbitrarydemonstrationsDandD‚Ä≤. HereEÀÜ
D‚Ä≤ D‚Ä≤ (¬∑)
representthewordembeddingderivedfromthedemonstrationdistillationmodel. Notably,previous
HypernetworkmethodshasthecompatibilityissueswithLLM,resultingindistillationvectorsof
suboptimalquality.
3 Methods
Thewholeframeworkof MENDisillustratedinFig.2. Weinsertlspecialtokenstothevocabulary
setofdistillationlanguagemodelMEND,whichactasplaceholdersforthedemonstrationdistillation.
ForanydemonstrationsD,theseplaceholdersembeddingEÀÜ areappendedtothedemonstrations
œï
embeddingEÀÜ ,fosteringaversatiledistillationstrategysuitablefordiversetasks. Aftermultiple
D
transformerlayersinsideMEND,wecandistilltheinformationfromlengthyDtocompactdistillation
(cid:16) (cid:17)
vectorsS =MEND concat(EÀÜ ,EÀÜ ) abbreivatedasS =MEND(EÀÜ ).
D D œï D D
[‚àíl:]
3.1 KnowledgeDistillation
Thegoaltoknowledgedistillationistouseaconcisedemonstrationsummary, S , suchthatthe
D
downstreamLLMbehavessimilar(e.g. outputcloseworddistributions)toitsversionconditionedon
thefulldemonstrationsD. Torealizethis,wetreattheLLMwithfulldemonstrationDasthe‚Äúteacher‚Äù
andtheversionwithonlythedemonstrationsummaryS asthe‚Äústudent‚Äù. Subsequently,weemploy
D
KLdivergencetoassessthedifferencebetweenthewordprobabilitydistributionsofthesetwomodels.
(cid:16) (cid:17)
Ldistill =KL PLLM(x|E D)||PLLM(x|MEND(EÀÜ D)) , (3)
WeoptedforKLdivergenceasourdistillationobjectivetoensurethestudentmodeldoesnotproduce
outputsthataretoodifferentfromtheteachermodel.
3PublishedasaconferencepaperatICLR2024
3.2 Optimization
Throughoutourtwo-stageoptimizationprocess,LLMremainsfrozen,assistinginbackpropagatingthe
gradientfromthelosstoMEND.
Meta-distillation Pretraining. To help MEND capture the general knowledge of distillation, we
pretrainitonatextpretrainingdata-C4(Raffeletal.,2019). Asillustratedintherightsegmentof
Fig.2,weextractsequencesof1024tokensfromthepretrainingdataset. Thissequenceisdivided
intotwoparts: thefirst1024√óŒ≤ tokensasdemonstrationsDandtheremainder,1024√ó(1‚àíŒ≤),as
inputx,whereŒ≤ isthehyperparametertocontrolthelengthofdemonstrations. Wethenapplythe
knowledgedistillationapproachtopretrainMEND.Incontrastwiththeconditionallanguagemodeling
objective,whereLLMpredictssubsequentcontentbasedoncompressedtokens(Phangetal.,2023;
Ivisonetal.,2022),ourdemonstrationdistillationistrainedbyminimizingL andaimstoensure
distill
thedistillationmodelmoreaccuratelycapturestheintrinsicattributesofMEND. Consequently,itcan
offeramorefaithfuldemonstrationdistillation. AsevidencedinTab.2andTab.4,ourdemonstration
distillationconsistentlyoutperformsthetraditionalconditionallanguagemodelingCLMapproach.
Meta-distillation Finetuning. During this stage, we finetune MEND using ICL relevant tasks,
equippingitwiththeabilitytointerpretatask‚Äôssemanticsfromitsdemonstrations. Thisensures
thatMENDcaneffectivelygeneralizetounseendemonstrationsinthefuture. Ineachiteration,we
chooseameta-trainingtaskandextractK+1demonstrationsfromit. ThefirstK demonstrations
areconcatenatedintoD,whiletheremainingpair,(x ,y )isreservedfortestinputandoutput
K+1 K+1
purpose. Similartothepretrainingphase,thedemonstrationsDarefedintothedistillationmodel
MEND,yieldingthedemonstrationdistillationS . TheprimarypurposeofS istoinstructtheLLMin
D D
producingyandguaranteethatLLMoperatesasthoughitwasconditionontheoriginaldemonstrations.
Theformulationoffinetuningisasfollows:
Lpred =logP LLM(y|concat(S D,E x)),
(4)
Lfinetune =Lpred+ŒªLdistill.
whereŒªisthehyper-parametertocontroltheimportanceofdistillationinfinetuning.
4 Experiments
4.1 ExperimentSetting
Benchmarks. In the section, to validate our methodology, we employ the MetaICL dataset
introducedbyMinetal.(2022a),designedforin-contextlearningscenarios. MetaICLbuildsupon
existingfew-shotdatasets,suchasCrossFit(Yeetal.,2021)andUnifiedQA(Khashabietal.,2020).
Notably,theMetaICLdatasetisdividedintotwodistinctpartitions: meta-trainandmeta-test,withno
overlapbetweenthem. Thissettingexpectthemodelfirsttrainedonmeta-trainthenevaluatedon
meta-testdataset. Ourexperimentsencompasssevendistinctmeta-trainandmeta-testpartitions3as
outlinedinTab.1. InICL,thecontextlengthisdirectlyproportionaltothenumberofdemonstrations.
Forinstance,intheClass‚ÜíClass,with16demonstrations,eachdemonstration‚Äôsaveragelengthis
56.21tokens. Consequently,duringinference,theaveragecontextlengthextendsto899.36tokens
(calculatedas16√ó56.21)whichwillbringadditionalcomputationcomparedwithnodemonstrations
lengthwith56.21.
FollowingMetaICLsetup (Radfordetal.,2019),weutilizewhitespacetodelineateinputandoutput.
Inmostofourexperiments,wehavepresetthenumberofdemonstrationstoK =16. Forevaluating
modelperformance,accuracymetricsareemployedforclassificationtasks,whileMacro-F1isutilized
fornon-classificationtasks. Inpartitionsthatencompassbothclassificationandnon-classification
tasks(suchasLR),wecomputetheaverageofMacro-F1andaccuracytoassessoverallperformance.
Base Models. To illustrate the adaptability of our proposed MEND framework, we assess its
performanceusingvariousbackbonelargelanguagemodelarchitectures, includingdecoder-only
models, such as GPT2 (Radford et al., 2019), and encoder-decoder models, like T5 (Raffel et al.,
3ThetasksandtheircorrespondingabbreviationscanbefoundinAppendixA.
4PublishedasaconferencepaperatICLR2024
2019)4. WeinitiallyexperimentedwithusingdifferentarchitecturesforMENDandfindthatthewhen
MENDandLLMarefromthesamemodelfamilyworksbest. Thus,forGPT-2,wechoosegpt2-small5,
whileforT5weselectt5-small-lm-adapt6.
BaselineMethods. Wecomparetheperfor-
manceofMENDagainstfourprimarygroupsof Table1: Statisticsofsevendifferenttaskpartitions.
baselinemethodologies: 1)Zero-shot: This Eachrowindicatesmeta-training/testtaskpartitions.
approachutilizestheLLMfordirectzero-shot
inference. 2)Vanilla ICL:Here,weemploy
meta-train meta-test
LLM for in-context learning by conditioning Setting #task Avg.Len. Setting #task Avg.Len.
o dn ema oc no sn trc aa tit oen na st .io 3n )o Pf rK omr pa tn Td uo nm il ny gse (Lle ec st te ed r noC n-l Cas ls ass 4 33 7 4 94 1.. 45 54 Class 20 56.21
e at ppal r. o, a2 c0 h21 to): aT dah pis ts Lt Lra Mte tg oy no eff wer tas sa kn se wffi itc hi oe un tt noQ n-A QA 3 37 3 9 71 2.. 55 08 QA 22 57.84
requiring full retraining. 4) HyperTuning: non-NLI 55 54.51 NLI 8 61.61
Phangetal.(2023)employsalanguagemodel HR 61 82.44 LR 26 35.31
todistilldemonstrationsintocondensedvec- non-Para 59 55.97 Para 4 54.06
tors using a conditional language modeling
objective. Forfairness,PromptTuningandHyperTuning,usesamepromptlengthsandhypermodel
sizesequivalenttothoseusedinMEND.Furtherdetailsregardinghyperparametersettingsandanalysis
canbefoundinFig.4.
4.2 ExperimentResults
Effectiveness. Thissectionoutlinestheresultsfromourexperiments,asdetailedinTab.2. Wemake
thefollowingobservations: Firstly,thezero-shotapproachpredominantlyunderperforms,indicating
thattheinductivebiasesintroducedduringmeta-training(PromptTuning),meta-testing(Vanilla
ICL),orboth(HyperTuningandMEND)enhancein-contextlearning. Secondly,whencomparedwith
PromptTuning,bothHyperTuningandMENDdemonstratemarkedimprovements. Thisunderscores
theeffectivenessandgeneralizabilityofusinghypernetworkstodistillthesupervisingsignalfrom
demonstrationstoassistLLM.ApotentialreasonforPromptTuning‚Äôsinferiorperformanceisthat
itsolelycapturesinductivebiasthroughgradientdescentduringmeta-trainingandcannotleverage
bias from the meta-test‚Äôs demonstrations at meta-test time. Thirdly, Vanilla ICL outperforms
HyperTuning,whileMENDconsistentlymatchesorevensurpassesVanilla ICL.Thissuggeststhat
ourapproach,incorporatingL andL ,isadeptatcapturingthemeta-knowledgefacilitating
distill pred
thedistillationdemonstrationtoaidLLM.
InferenceEfficiency. Inferenceefficiencyremainsafundamentalaspectofourstudy. Thecore
ideaofourworkistodistillextensivenaturallanguagedemonstrations,denotedasD,intoconcise
distillationvectors,denotedasS ,therebyreducingcomputationaldemandsforLLM. Toassessthe
D
efficiencyofourmodel,wereportthecomputationalcostsassociatedwithdifferentrepresentation
techniques in terms of processing time, memory consumption, and floating-point operations per
second(FLOPS).Specifically,foreachmeta-testpartition,weselectasingletask,evaluateitwitha
batchsizeof1,andmeasuretheaforementionedmetrics. ConsideringthatHyperTuningoperates
identically to MEND during inference, we have chosen Vanilla ICL and PromptTuning as our
baselinemethods. Itisimportanttonotethattheinferenceefficiencyof MENDencompassesboth
theprocessofobtainingthedistilledvectorsandthesubsequentinferencebytheLLMusingthese
vectorsinconjunctionwiththetestinput. ComparedwithPromptTuning,MENDbringadditional
computationalcostatcompressingdemonstrationsintocompactvectors. AsillustratedinFig.3,MEND
achievesupto3.5timesgreatercomputationalefficiencycomparedtoVanilla ICLandrequiresless
peakGPUmemory. Remarkably,whileMENDdemonstratesefficiencyonparwithPromptTuning,it
alsopresentsanotableperformanceenhancement,asevidencedinTab.2. Theseobservationsindicate
ourproposedmethodMENDcanimprovetheLLM‚ÄôsefficiencywithoutsacrificingLLM‚Äôseffectiveness
i5ninA-connatelxytsleisarning.
Inthissection,weconductacomprehensiveexaminationofourdistillationapproachacrossvarious
scenariostogaindeeperinsightsintoitsbehaviorandpotentiallimitations. Tomitigatecomputational
4In¬ßC,wehavetestourproposedmethodonflat-t5-xlandopt-6.7b.
5https://huggingface.co/gpt2
6https://huggingface.co/google/t5-small-lm-adapt
5PublishedasaconferencepaperatICLR2024
Table2: PerformanceontheMetaICLDataset: Thistableshowstheaverageandstanddeviation
scoresfromrunningourevaluationwithfivedistinctrandomseeds. Toenhancereadability,wepresent
the meta-train and meta-test pairs in the format ‚Äúmeta-train ‚Üí meta-test‚Äù. The best-performing
modelsarehighlightedinbold,whilethesecond-bestareunderlined. Thestandarddeviationvalues
reflectthevariabilityduetodifferentdemonstrationsretrieved. Notethatthe‚ÄúPromptTuning‚Äùand
‚Äúzero-shot‚Äùapproachesdonotrequiredemonstrationretrieval,hencetheirstandarddeviationiszero.
non-
Methods C Cla ls as ss‚Üí C Cla ls as ss‚Üí n ‚Üíon- NN LL II QAn ‚Üíon Q- A Q QA A‚Üí H LR R‚Üí Parn ao ‚Üín- Para AVG
gpt2-large
zero-shot 34.36 34.36 25.50 44.58 44.58 34.77 34.12 36.04
PromptTuning 37.65 38.78 31.34 38.71 45.77 40.68 34.23 38.17
Vanilla ICL 41.30¬±2.15 41.30¬±2.15 39.13¬±2.30 45.81¬±1.34 45.81¬±1.34 41.26
¬±2.26
38.93¬±1.15 41.93
HyperTuning 40.42¬±1.64 42.54¬±1.79 36.49¬±2.01 41.11¬±0.82 46.20
¬±0.50
41.63¬±1.72 39.63
¬±0.66
41.15
MEND 43.35¬±2.17 43.38¬±1.62 39.96¬±1.99 44.29¬±0.86 46.92¬±0.49 40.92¬±1.80 42.54¬±0.44 43.05
gpt2-xl
zero-shot 32.08 32.08 25.54 46.09 46.09 33.95 33.61 35.63
PromptTuning 37.65 38.78 36.27 41.45 46.95 40.83 35.52 39.64
Vanilla ICL 40.63
¬±2.53
40.63¬±2.53 37.35¬±1.83 48.32¬±0.88 48.32¬±0.88 42.27¬±2.08 37.53
¬±1.04
42.15
HyperTuning 40.26¬±1.33 43.74¬±1.51 34.61¬±1.23 40.71¬±1.14 47.41¬±0.46 41.83¬±1.34 35.72¬±0.43 40.61
MEND 42.79¬±2.22 43.37
¬±1.50
37.00
¬±1.99
45.95¬±0.66 48.07
¬±0.40
42.16
¬±1.81
42.53¬±1.20 43.12
t5-lm-large
zero-shot 36.75 36.75 25.72 39.05 39.05 32.09 34.28 34.81
PromptTuning 32.56 32.37 25.80 39.48 39.44 32.43 36.44 34.07
Vanilla ICL 38.40
¬±2.87
38.40
¬±2.87
36.68
¬±2.37
39.26¬±1.23 39.26¬±1.23 38.77
¬±2.13
36.31¬±0.51 38.15
HyperTuning 31.17¬±2.46 29.06¬±1.96 33.56¬±1.76 39.03¬±1.09 41.17
¬±0.86
34.28¬±1.27 37.39
¬±2.67
35.09
MEND 41.75¬±1.82 38.93¬±1.43 37.15¬±2.00 41.76¬±0.60 42.91¬±0.55 39.07¬±2.16 36.99¬±0.55 39.79
Forward FLOPS (TFLOPS) GPU Peak Memory (GB) Time (Seconds)
0.5
5 Vanilla ICL PromptTuning MEND 2.5 2.39 0.43
4 4.37 3.77 2.0 2.07 0.4 0.39
0.3
23 2.38 2.29 2.45 11 .. 05 1.32 1.21 1.30 0.2 0.24 0.25 0.25
1 0.580.68 0.400.54 0.901.04 0.861.01 0.480.62 0.5 0.360.36 0.250.25 0.610.61 0.590.59 0.320.33 0.1 0.070.09 0.060.08 0.090.12 0.090.11 0.070.08
0 0.0 0.0
Class HR QA non_NLI non_Para Class HR QA non_NLI non_Para Class HR QA non_NLI non_Para
Class LR QA NLI Para Class LR QA NLI Para Class LR QA NLI Para
(a)GPT2-Large(774M)
Forward FLOPS (TFLOPS) GPU Peak Memory (GB) Time (Seconds)
10 Vanilla ICL PromptTuning MEND 4 3.60 0.8 0.77
8 8.57 7.37 3 3.10 0.6 0.69
6 4.66 4.44 4.74 2 1.97 1.82 1.96 0.4 0.43 0.44 0.46
4
2 1.161.27 0.810.95 1.801.94 1.731.88 0.961.11 1 0.540.54 0.380.38 0.860.86 0.880.88 0.460.46 0.2 0.110.13 0.090.12 0.180.19 0.170.20 0.100.14
0 0 0.0
Class HR QA non_NLI non_Para Class HR QA non_NLI non_Para Class HR QA non_NLI non_Para
Class LR QA NLI Para Class LR QA NLI Para Class LR QA NLI Para
(b)GPT2-XL(1.5B)
Figure 3: Efficient Analysis of In-Context Learning at Inference Time. GPT2-large (774M)
andGPT2-XL(1.5B)areevaluatedonthesametaskwithbatchsize1. Thecontextlengthforboth
PromptTuningandMENDis100,whileforVanilla ICLvariesonthepartitions. (Class‚ÜíClassis
469,HR‚ÜíLRis652,QA‚ÜíQAis639,non NLI‚ÜíNLIis848,andnon Para‚ÜíParais818).
resourcedemands,weprimarilyemploythegpt2-largemodelasLLMonClass‚ÜíClasssettingunless
mentionedotherwise.
5.1 VaryingDemonstrationDistillationRatio
Acrucialaspectofourexperimentalanalysiswastocomprehendhowvaryingthedemonstration
distillationratioimpactsthedistillationofdemonstrationsand,consequently,theeffectivenessof
LLM‚Äôsin-contextlearning. Thedemonstrationdistillationratioisdefinedastheratioofthenumberof
demonstrationstothelengthofdistillationvectors. Specifically,wevarythedistillationratiofrom
6PublishedasaconferencepaperatICLR2024
twoperspectives: therichnessofinput(thenumberofdemonstrationexamples)andthecompactness
oftheoutput(thelengthofdemonstrationdistillation).
VaryingNumberofDemonstrations. Weassesstheeffectivenessofourmethodwhilealtering
the value of K (the number of demonstration) while keeping the length of the distillation vector
l constant. As depicted in Fig. 4a, our MEND approach consistently outperforms the Vanilla
ICL and HyperTuning methods for various values of K (1, 2, 4, 8, and 16). Furthermore, MEND
demonstratesconsistentperformanceimprovementasKincreases,whereasVanilla ICLreachesits
peakperformanceatK =4. ThisimprovementsuggeststhatMENDisexcelsatextractingsupervision
informationforin-contextlearningfromtheselecteddemonstrationexamples.
45
43
40
42
41 VVaanniillllaa--IICCLL HHyyppeerrTTuunniinngg MMEENNDD 35
40 30
PPrroommppttTTuunniinngg HHyyppeerrTTuunniinngg MMEENNDD
39
25
1 2 4 8 16 1 10 50 100 200
(a)Numberofdemonstrations. (b)Lengthofdistillationvectors.
Figure4: Performancewithdifferentdemonstrationdistillationratio. Thedistillationratioistheratio
ofthenumberofdemonstrationexamplestothelengthofthedistillation.
VaryingdemonstrationdistillationLength. Wemanipulatethelengthofdemonstrationdistillation
l=1,10,50,100and200whilekeepingK =16. ItisworthnotingthatweretrainMENDwithtwo
stagesasshownin¬ß3.2fordifferentlvalues. TheresultsinFig.4byieldthefollowingobservations:
Firstly,asthedemonstrationdistillationlengthincreases,theperformanceofallmethodsgenerally
improves, except for l = 200 in the case of PromptTuning. This suggests that there may be
informationlossindemonstrationdistillation,andincreasingthelengthofthedemonstrationmay
help mitigate this issue. However, there exists a trade-off between efficiency and effectiveness,
as extending the length of the distillation vectors results in a quadratic time complexity increase.
Secondly,weobservethatourproposedmethodachievesthebestperformanceamongthebaseline
methods,includingHyperTuning. Thisunderscoresthesignificanceofouroptimizationdesignin
providingenhancedinductivebiasforin-contextlearning.
5.2 PerturbationtoDemonstrations
Giventhesignificantinfluenceofprovideddemonstrationsontheperformanceofin-contextlearn-
ing(Minetal.,2022b),weaimtoinvestigatewhetherourproposedapproach,MEND,caneffectively
distillandpropagatemodificationsmadetodemonstrationstothedistilledvectors. Toaddressthis,
weempiricallyperturbthedemonstrationsfrombothpositiveandnegativeperspectives.
Positive Perturbation. In light of previous research Liu et al. (2021) emphasizing the value
of semantically similar demonstrations and their positive impact on in-context learning, we aim
to ascertain whether MEND‚Äôs advantages are complemented by or enhanced through the use of
improvedretrieveddemonstrations. Wetransitfromarandomsamplingapproachtoamorenuanced
semantic-basedk-NNretrievalmethod. AsindicatedinTab.3,semantic-basedretrievalmethods,
includingdenseandbm25,exhibitsuperiorperformancecomparedtorandomselectionunderthe
NoPerturbationcondition. Remarkably,MENDnotonlymatchesorevensurpasstheperformanceof
theseadvancedretrievalmethodsanddoessowithareducedcontextsize.
NegativePerturbation. Weevaluatetheimpactofvariousnegativeperturbations,includingthe
followingscenarios: 1)NoLabel: Thisperturbationinvolvesremovingthelabelswhileretaining
theinputs. 2)NoInput: Theinputsareremovedwhilekeepingthelabelsintact. 3)RandomLabel:
Thisperturbationrandomlyselectsoneofthevalidoptionsastheoutput. 4)WrongLabel: Inthis
case, one of the incorrect options is randomly selected. The results are presented in Tab. 3. As
7
)%(
1F-orcaM
)%(
1F-orcaMPublishedasaconferencepaperatICLR2024
Table3: Performanceswhenapplyingperturbationsondemonstrations.
PositivePerturbation NegativePerturbation
Methods NoPerturbation
bm25-kNN dense-kNN NoLabel NoInput RandomLabel WrongLabel
Vanilla ICL 41.30 45.38 48.33 30.57 42.29 37.25 28.13
HyperTuning 40.42 43.95 45.13 31.78 38.20 38.72 29.31
MEND 43.35 46.82 48.81 32.57 44.29 39.25 30.42
anticipated,aconsistenttrendemerges,withNoPerturbationoutperformingbothRandomLabel
and Wrong Label for both theVanilla ICL and our proposed MEND. Moreover, it is noteworthy
thatperformanceimprovesinmostcaseswhentheNoInputperturbationisapplied. Thisnotonly
underscoresthesignificanceoflabelsinthecontextofin-contextlearningbutalsoillustratesMEND‚Äôs
abilitytoeffectivelydistilllabelinformationintothedistilledvectors.
5.3 AttentionWeightVisualization
TogainadeeperunderstandingofhowdemonstrationdistillationimpactsLLM,weemployvisualization
techniquestoexploretheattentionweightsofLLM‚Äôsinductionheads,asintroducedby Olssonetal.
(2022). Inductionheadsareattentionheadsknownfortheirprefixmatchingandcopyingproperties,
whichplayacrucialroleinthecontextofin-contextlearning. Theyempiricallyincreasethelikelihood
of[B]given[A][B]¬∑¬∑¬∑[A]whenrepeatedsequenceoftokens. Ourobjectiveistounderstandwhether
ourdemonstrationdistillationcanstoretheinput-outputpatternthatwillactivatetheseinduction
headsinamannersimilartotheoriginaldemonstrationtokens.
Wevisualizetheattentionweightsofthefourinductionheads7forbothVanilla ICLandMEND,as
illustratedinFig.5. AreviewofFig.5revealsthatthefinalpredictionestablishesaconstructive
association with the demonstration distillations. Given that the length of demonstration tokens
(average=914)andcompressedprompttokens(100)significantlyexceedthelengthoftestinput,we
employmaxpoolingtomaptheattentionweightsofthedemonstrationsinto20tokens(Areaenclosed
byredrectangle). Thisin-depthanalysisfurthersubstantiatesthatthedistillationderivedfromMEND
offersvaluablecontextsupervisionsignalsforLLM.
LHayeeard D Inedpethx:: 117/2/306 LHayeeard D Inedpethx:: 250/2/306 LHaeyaedr DInedpetxh:: 1107//2306 LHayeeard D Inedpethx:: 158/2/306
0246810121416182022242628303234363840424446485052024681012141618202224262830323436384042444648505202468101214161820222426283032343638404244464850520246810121416182022242628303234363840424446485052
(a)AttentionVisualizationofVanilla ICL.
LHayeeard D Inedpethx:: 117/2/306 LHayeeard D Inedpethx:: 250/2/306 LHaeyaedr DInedpetxh:: 1107//2306 LHayeeard D Inedpethx:: 158/2/306
0246810121416182022242628303234363840424446485052024681012141618202224262830323436384042444648505202468101214161820222426283032343638404244464850520246810121416182022242628303234363840424446485052
(b)AttentionVisualizationofMEND
Figure5: Attentionvisualization. Theleftredsurroundedx-axisdenoteseitherthedemonstrations
(Vanilla ICL)orthedistilledvectors(MEND)andtheotherpartofx-axisarethetokensfromthe
testinput. They-axiscorrespondstothefirsttokenoftheoutputword.
5.4 AblationStudyondemonstrationdistillation
ToassessthesignificanceoftheL ,weconductedanexperimentthatexcludingthistermduring
distill
boththepretrainingandfinetuningstagesonseveralrepresentativetaskparitions.
Pretraining. Duringthepretrainingphase,wecompareusingno-pretraining,conditionallanguage
modeling (CLM) (Phang et al., 2023), and CLM+L 8. We find that (1) pretraining is crucial
distill
asitsubstantiallyenhancesperformancecomparedtomethodswithno-pretraining,exceptforthe
no-pretrainingbaseline;(2)ourpretrainingapproachoutperformsthealternatives. Wehypothesize
thatthissuperiorityisattributedtoourpretrainingschemebetteraligntheMENDandLLM.
Finetuning. Inthisphase,weretainedthesamepretrainingobjectivefunctionbutomittedvarious
finetuning components. Examining the lower section of Tab. 4, we observe that the removal of
each component leads to a decrease in performance. This observation underscores the positive
contributionsofeachcomponentwithinourproposedmethodtotheoverallperformance.
7Thedetailsofidentifyinginductionheadscanbefoundin¬ßC.
8MoreanalysisaboutCLM+L canbefoundinAppendixB
distill
8PublishedasaconferencepaperatICLR2024
Table4: Ablationstudyofknowledgedistillation.
non-Class non-NLI non-
Methods
‚ÜíClass ‚ÜíNLI QA‚ÜíQA
QA‚ÜíQA Avg.
Vanilla ICL 41.30 39.13 45.81 45.81 43.01
MEND 43.38 39.96 44.29 46.92 43.65
AblationStudyonPretraining
No-Pretraining 38.25 34.33 42.18 45.65 40.10
CLM 42.00 39.09 44.13 46.47 42.92
CLM+Ldistill 41.38 38.79 43.69 45.10 42.24
AblationStudyonFinetuning
MENDw/oLpred 37.64 33.90 43.97 44.54 40.41
MENDw/oLdistill 39.26 37.22 40.29 45.78 40.64
In this experiment, we also observed that both the pretraining and finetuning ablations of MEND
significantlyunderperformcomparedtoVanilla ICL.Thisfindingunderscoresthecriticalroleof
thetwo-stagedesign,encompassingbothpretrainingandfinetuning,inourmodel‚Äôseffectiveness.
Moreover,ithighlightstheessentialcontributionofknowledgedistillationinreplicatingtheteacher
model‚Äôsbehaviorsandharnessingmeta-trainingknowledge. Theseresultscollectivelyillustratethe
synergisticimpactofthesecomponentsinenhancingMEND‚Äôsperformance.
6 RelatedWork
MENDdrawsinspirationfrompriorresearchonhypernetworksandknowledgedistillation. Inthis
section,wedelveintoseminalworksintheseareas.
Hypernetwork TheconceptofaHypernetwork,asintroducedbyHaetal.(2016),referstoan
auxiliarynetworkdesignedtogenerateparametersforaprimarynetwork. Inasimilarview,MEND
canbeperceivedasaHypernetwork,producingdistilledvectors(parameters)totailorLLMfornew
tasks. NotableeffortslikeHyperTuning(Phangetal.,2023),HINT(Ivisonetal.,2022),Hyper(Ye
&Ren,2021)haveemployedalanguagemodel-baseddistillationmodeltocondensedemonstrations
intodistilledvectors. Whilethesemethodscanadapttounseendemonstrations,theyoftendegrade
withICLperformance. Ontheotherhand,Gist(Muetal.,2023)enhancestheLLMwithinstruction
distillation and instruction following. However, given that the distillation model is synonymous
withtheLLM,thedistillationprocedureinducescomputationaloverhead,especiallywhencompared
withourapproachthatdeploysasmallerlanguagemodelfordistillation. Adistinctiveadvantageof
MENDoverexistingHypernetwork-baseddemonstrationdistillationsisitssimultaneousrealization
ofefficiencyandeffectivenessasshowninTab.2andFig.3.
KnowledgeDistillation Knowledgedistillation,asintroducedbyHintonetal.(2015),seeksto
transferinsightsfromahigh-capacitymodeltoamodelwithlowercapacity. Thismethodologyiskey
inensuringbothefficiencyandeffectivenessforMEND,settingMENDapartfromotherHyperNetwork
techniques. Askelletal.(2021);Snelletal.(2022)exploittheknowledgedistillationtofinetuneLLM
withtheabilitytofunctionasthelanguagemodelwithaprependedpromptwhendidnotprovideany
prompt. Nonetheless,giventhediversenatureofdemonstrations,asillustratedinTab.3,thesemethods
failtoincludesuperiordemonstrationsforbetterICLperformance. Furthermore,asMENDfunctions
asacomplementarymoduleforLLM,itdoesn‚ÄôthamperLLM‚ÄôsinherentcapabilitiesFurthermore,as
MENDfunctionsasacomplementarymoduleforLLM,itdoesn‚ÄôthamperLLM‚Äôsinherentcapabilities.
7 Conclusion
WeintroducedMENDtonotonlytackletheinherentefficiencychallengesinin-contextlearningwith
large language models but also to address the effectiveness limitations of existing demonstration
distillationmethodologies. Ourinnovativeapproachdistilledin-contextdemonstrationsintovectors,
tailoredfordownstreamlargelanguagemodels. RigorousevaluationsofMENDacrosssevendistinct
few-shottaskpartitionsandtwomajorlargelanguagemodelfamilieshaveunderscoreditsprowess.
Notably, MEND consistently matches or even surpasses the performance of traditional in-context
learning,allwhiledemandingfewerFLOPs. Thisbreakthroughpavesthewayformoreefficientand
scalableapplicationsoflargelanguagemodelsinreal-worldscenarios. Inthefuture,weaimtodistill
anevenbroaderspectrumofdemonstrations,somepotentiallysurpassingthecontextwindowlimits
ofboththedemonstrationdistillationmodelandLLM.
9PublishedasaconferencepaperatICLR2024
References
AmandaAskell,YuntaoBai,AnnaChen,DawnDrain,DeepGanguli,TomHenighan,AndyJones,
NicholasJoseph,BenMann,NovaDasSarma,etal. Agenerallanguageassistantasalaboratory
foralignment. arXivpreprintarXiv:2112.00861,2021.
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877‚Äì1901,2020.
AydarBulatov,YuriKuratov,andMikhailS.Burtsev. Recurrentmemorytransformer,2022.
HyungWonChung,LeHou,ShayneLongpre,BarretZoph,YiTay,WilliamFedus,YunxuanLi,
XuezhiWang,MostafaDehghani,SiddharthaBrahma,etal. Scalinginstruction-finetunedlanguage
models. arXivpreprintarXiv:2210.11416,2022.
QingxiuDong,LeiLi,DamaiDai,CeZheng,ZhiyongWu,BaobaoChang,XuSun,JingjingXu,Lei
Li,andZhifangSui. Asurveyonin-contextlearning,2023.
SylvainGugger,LDebut,ThomasWolf,PhilippSchmid,ZacharyMueller,andSourabMangrulkar.
Accelerate: Trainingandinferenceatscalemadesimple,efficientandadaptable,2022.
DavidHa,AndrewM.Dai,andQuocV.Le. Hypernetworks. ArXiv,abs/1609.09106,2016. URL
https://api.semanticscholar.org/CorpusID:208981547.
YaruHao,YutaoSun,LiDong,ZhixiongHan,YuxianGu,andFuruWei. Structuredprompting:
Scalingin-contextlearningto1,000examples,2022.
GeoffreyHinton,OriolVinyals,andJeffDean. Distillingtheknowledgeinaneuralnetwork. arXiv
preprintarXiv:1503.02531,2015.
Hamish Ivison, Akshita Bhagia, Yizhong Wang, Hannaneh Hajishirzi, and Matthew Peters.
Hint: Hypernetwork instruction tuning for efficient zero-shot generalisation. arXiv preprint
arXiv:2212.10315,2022.
JaredKaplan,SamMcCandlish,TomHenighan,TomBBrown,BenjaminChess,RewonChild,Scott
Gray, AlecRadford, JeffreyWu, andDarioAmodei. Scalinglawsforneurallanguagemodels.
arXivpreprintarXiv:2001.08361,2020.
DanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,and
HannanehHajishirzi. Unifiedqa: Crossingformatboundarieswithasingleqasystem,2020.
BrianLester,RamiAl-Rfou,andNoahConstant. Thepowerofscaleforparameter-efficientprompt
tuning,2021.
JiachangLiu,DinghanShen,YizheZhang,BillDolan,LawrenceCarin,andWeizhuChen. What
makesgoodin-contextexamplesforgpt-3?,2021.
SewonMin,MikeLewis,LukeZettlemoyer,andHannanehHajishirzi. Metaicl: Learningtolearnin
context,2022a.
SewonMin,XinxiLyu,AriHoltzman,MikelArtetxe,MikeLewis,HannanehHajishirzi,andLuke
Zettlemoyer. Rethinkingtheroleofdemonstrations: Whatmakesin-contextlearningwork?,2022b.
JesseMu,XiangLisaLi,andNoahGoodman. Learningtocompresspromptswithgisttokens. 2023.
Neel Nanda and Joseph Bloom. Transformerlens, 2022. URL https://github.com/
neelnanda-io/TransformerLens.
CatherineOlsson,NelsonElhage,NeelNanda,NicholasJoseph,NovaDasSarma,TomHenighan,
BenMann,AmandaAskell,YuntaoBai,AnnaChen,etal. In-contextlearningandinductionheads.
arXivpreprintarXiv:2209.11895,2022.
10PublishedasaconferencepaperatICLR2024
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,Trevor
Killeen,ZemingLin,NataliaGimelshein,LucaAntiga,AlbanDesmaison,AndreasKo¬®pf,Edward
Yang,ZachDeVito,MartinRaison,AlykhanTejani,SasankChilamkurthy,BenoitSteiner,LuFang,
JunjieBai,andSoumithChintala. Pytorch: Animperativestyle,high-performancedeeplearning
library,2019.
Jason Phang, Yi Mao, Pengcheng He, and Weizhu Chen. Hypertuning: Toward adapting large
languagemodelswithoutback-propagation. InInternationalConferenceonMachineLearning,pp.
27854‚Äì27875.PMLR,2023.
AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal. Language
modelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
Zhou,WeiLi,andPeterJ.Liu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-text
transformer. arXive-prints,2019.
Charlie Snell, Dan Klein, and Ruiqi Zhong. Learning by distilling context. arXiv preprint
arXiv:2209.15189,2022.
XinyiWang,WanrongZhu,MichaelSaxon,MarkSteyvers,andWilliamYangWang. Largelanguage
modelsareimplicitlytopicmodels: Explainingandfindinggooddemonstrationsforin-context
learning,2023.
ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,AnthonyMoi,
Pierric Cistac, Tim Rault, Re¬¥mi Louf, Morgan Funtowicz, et al. Huggingface‚Äôs transformers:
State-of-the-artnaturallanguageprocessing. arXivpreprintarXiv:1910.03771,2019.
QinyuanYeandXiangRen. Learningtogeneratetask-specificadaptersfromtaskdescription,2021.
QinyuanYe,BillYuchenLin,andXiangRen. Crossfit: Afew-shotlearningchallengeforcross-task
generalizationinnlp. arXivpreprintarXiv:2104.08835,2021.
SusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,MoyaChen,ShuohuiChen,Christopher
Dewan,MonaT.Diab,XianLi,XiVictoriaLin,TodorMihaylov,MyleOtt,SamShleifer,Kurt
Shuster,DanielSimig,PunitSinghKoura,AnjaliSridhar,TianluWang,andLukeZettlemoyer.
Opt: Open pre-trained transformer language models. ArXiv, abs/2205.01068, 2022. URL
https://api.semanticscholar.org/CorpusID:248496292.
11PublishedasaconferencepaperatICLR2024
A Data,Training,Evaluation,andComputeDetails
Code and data are available in the supplementary material and will be made public upon paper
acceptanceviaGitHub.
Data. Forpretrainingstage,weutilizetheC4validationdataset(Raffeletal.,2019)asourtraining
data. Wetruncateeachpassageinto1024tokens. Formeta-distillationstage,welimitthecontext
lengthinto900. Withinthedemonstrations,anyexamplex exceeding256tokensistruncatedfrom
i
theend. However,wedonottruncatethelabely . Ifthecontextlengthsurpasses900tokenswhile
i
i<K,thesubsequentdemonstrations{(x ,y )}K areomiited.
i+1 i+1
Thetasksandtheircorrespondingabbreviationsareasfollows: ‚ÄúClass‚Äùforclassification,‚ÄúQA‚Äùfor
questionanswering,‚ÄúNLI‚Äùfornaturallanguageinference,‚ÄúHR‚Äùforhighresource,‚ÄúLR‚Äùforlow
resource,and‚ÄúPara‚Äùforparaphrase.
Training. The complete set of stable hyperparameters for training runs can be found in Tab. 5.
TheseparametersareadaptedfromMetaICL(Minetal.,2022a). Additionalhyperparametersthat
neededexplorationandtheircorrespondingsearchspacesarealsodetailedinTab.5.
For pretraining, we leverage the Class‚ÜíClass meta-test validation dataset for early stopping. It
shouldbenoticedthatwhiledeterminingpretraininghyperparameters,wefocusedoursearchsolely
ongpt2-largeandsubsequentlyadaptedthefindingstootherdownstreamMEND.
Asforfinetuning,weusespecificmeta-testvalidationdataforearlystopping. Whenitcomesto
themeta-distillationfinetuninghyperparameters,weconductthesearchforeachtasksplitandMEND
independently.
ThehyperparameteranalysisofŒ≤ andŒªcanbefoundinFig.7andFig.6a.
Table5: HyperparametersforMEND.
Pretraining Finetuning
gpt2-large gpt2-xl t5-large-lm gpt2-large gpt2-xl t5-large-lm
StableHyperparameters
numsteps 30,000 30,000 5,000 30,000 30,000 30,000
batchsize 1 1 8 1 1 1
learningrate 5e-5 5e-5 5e-5 5e-5 5e-5 5e-5
precision fp16 fp16 fp32 fp16 fp16 fp32
optimizer adamW adamW adamW adamW adamW adamW
LLMŒ∏in8bit True True False True True False
earlystoppatience 5 5 5 5 5 5
SearchableHyperparameters
Œ≤ [0.1,0.5,0.8,0.9] N/A N/A N/A
Œª N/A N/A N/A [0.01,0.1,1,10]
Compute. WeimplementedourproposedmethodologyusingPyTorchv1.13.1(Paszkeetal.,2019),
complementedbytheHuggingFaceTransformerslibraryv4.24.0(Wolfetal.,2019)andAccelerate
v0.20.0(Guggeretal.,2022). AllexperimentswereconductedoneightA10NVIDIAGPUs,each
equippedwith24GBofmemory.
B Hyperparameteranalysis
PretrainingrelevantHyperparameters. Duringthepretrainingstage,therearetwoimportant
factors greatly influence the distillation models performance for the following Meta-Distillation
fineuning: Œ≤ andŒ≥. Œ≤ controlsthelengthofdemonstrationsfordistillationduringpretrainingand
Œ≥ controls the importance of knowledge distillation during pretraining. In Tab. 4, we show the
experimentresultsof CLM+1√óL ). Tocomprehensivelyunderstandthesuperiorityofsole
distill
L ,weconsideranthehyperparameteranalysisonthecombinationofCLM+1√óL ,which
distill distill
canbeformulatedasL=L +Œ≥L . Tosavecomputationalresource,differentfromTab.4we
CLM distill
directlyreporttheexperimentresultafterpretrainingwithoutfurtherMeta-distillationcomprehension.
12PublishedasaconferencepaperatICLR2024
40 40
38 38
36 36
34 34
0.2 0.4 0.8 0.9 0.01 0.1 1 10
(a)AnalysisonŒ≤. (b)AnalysisonŒ≥. DashedlineindicatesnoCLM.
Figure6: Analysisonpretrainingrelevanthyperparameters.
As the result shown in Fig. 6, we have the following observations: 1) MEND achieves the best
performance when Œ≤ = 0.8. This indicates that during pretraining, proper design the ratio of
demonstrationstoinputswillachievebetterperformancethansmallorlargeratios;2)MENDachieves
better performance when increasing the Œ≥. This indicates the importance of L (knowledge
distill
distillation)inminimizetheknowledgegapbetweenthedistillationmodelanddownstreamlanguage
model.
Meta-DistillationrelevantHyperparameters.
Tounderstandtheimportanceofknowledgedis-
tillationinMeta-distillationfinetuningstage,we gpt2-large gpt2-xl t5-large-lm-adapt
varyŒªinEq.4. AstheresultshowninFig.7,we 3.57
canobservethatMENDachievebeterperformance 3.17
whenŒª>=1,thisalsoindicatestheimportance 3 3.00
2.71
ofknowledgedistillation. 2.43 2.43
2.29
2 2.00 1.86 1.86
1.67 1.71
C AdditionalAnalysis
1
IdentifyInductionHead. In¬ß5.3,wevisu-
alize the attention weights of induction heads. 0
0.01 0.1 1.0 10.0
Here, we introduce how we identify these in-
ductionheads. Following(Olssonetal.,2022; Figure7: HyperparameteranalysisonŒª.
Nanda&Bloom,2022),wefirstlycreate10ran-
domlysequenceswithlength500thenexpand
thembyconcatenatingwithitselffortime. Thus
wehave10sequenceswithlength1000andfor
eachsequence,thefirst500tokensisexactsame
astherest500tokens. Then,insideeachself-attentionlayer,wetakethediagonalofattentionpaid
fromeachdestinationposition(positionindex>500)tosourcepositions500‚àí1backandgetthe
attentionaverageofeachheadoverthesetokens. TheaverageattentionscoreareshowninFig.8We
choosethe4attentionheadwithlargestaverageattentionscoreastheourinterestedinductivehead.
Additional Large Language Model. To assess the efficacy and generalizability of MEND,
we conducted evaluations on larger models, specifically opt-6.7b Zhang et al. (2022) and
flan-t5-xlChungetal.(2022). Fordemonstrationdistillation,westrategicallyselectedsmaller
counterpartsasbackbonemodels: opt-125mforopt-6.7bandflan-t5-baseforflan-t5-xl.
We maintained consistent formatting and training methodologies across these evaluations, using
whitespacetoseparateinputsandoutputswithinandacrossdemonstrations,asdonewithgpt2-large.
Theresults,asdetailedinTab.6,showthatMENDconsistentlyoutperformsotherbaselinemethods.
Thisdemonstratesitsabilitytoeffectivelycaptureandutilizemeta-knowledge,enhancingtheefficiency
ofdemonstrationdistillationforaidinglargelanguagemodels(LLM).
13
)%(
1F-orcaM
knaR
gvA
)%(
1F-orcaMPublishedasaconferencepaperatICLR2024
0 5 10 15
0
5
0.8
10
0.6
15
20
0.4
25
0.2
30
35
0.0
Head
Figure8: Averageattentionweightvisualizationofattentionheadfromgp2-large.
14
reyaLPublishedasaconferencepaperatICLR2024
Robustness towards Template Variations Table6: Experimentonadvancedlargelanguage
Whiletheprimaryobjectiveofourstudyistodis- models.
tilldemonstrationsintocompactvectors,theex-
plorationofoptimalprompttemplatesisbeyond
Methods Class‚ÜíClass
thescopeofthispaper. Inourexperiments,we
flan-t5-xl
consistentlyusedwhitespacetoseparateinputs
andoutputswithinandbetweendemonstrations PromptTuning 33.24
acrossallmodels. Toassesstherobustnessof Vanilla ICL 40.63 ¬±2.21
HyperTuning 39.70
ourmodelsagainsttemplatevariations,wecon- ¬±1.38
MEND 40.77
¬±1.20
ductedanadditionalevaluation. Wetransferred
opt-6.7b
themodeltrainedwithawhitespaceseparatorto
anewtemplateusingnewlinecharacters(nn)for PromptTuning 38.81
Vanilla ICL 42.38
separatinginputsandoutputs,andthreenewlines
HyperTuning 32.67
for differentiating between demonstrations on ¬±2.17
MEND 44.27
¬±1.12
the gpt2-large LLM. The results, presented
inTab.7, indicatethatMENDexhibitsminimal
sensitivitytotheseformatchanges. Theperformancedifferencewasnegligible,withlessthana0.3%
variancebetweenusingspacesandnewlines.
D Limitations
LargeDownstreamlanguageModels. Due
to computational constraints, our experiments Table 7: Robustness of template variations. All
usemodelsthatare<2B.Whetherthesedemon- themethodisevaluatedonClass‚ÜíClasssetting.
strationlanguagedistillationtechniquesgener- TheDiff. isthedifferencebetweennewlineresult
alizeothelargestmodels(10B+)isunknown. minuswhitespaceresult.
However,giventhatourmethodcangeneralize
to different model structures and computation
Methods whitespace newline Diff.
efficiencywithouthurtingthedownstreamlan-
guagemodel‚Äôsperformance,webelieveweare Vanilla ICL 41.30 ¬±2.15 38.90 ¬±2.21 ‚àí2.40
HyperTuning 40.42 40.08 ‚àí0.34
sheddinginsightsforfuturework. ¬±1.64 ¬±2.54
MEND 43.35 43.50 +0.15
¬±2.17 ¬±2.12
LanguageModeldependent. Duetoourde-
signofdistillation,theMENDmayfacetheadap-
tationproblemacrossdifferentMENDs. Thismeansweneedtotrainanewdistillationmodelforany
newLLM.Inaddition,becauseofouroptimizationdesign,weneedthegradientsthatbackpropagate
on the top of MENDs. This will bring computation overhead when we try large LLM with larger
demonstrationencoders.
Limited Context Window. Both MEND and LLM have a limited context window. Thus, when
demonstrationsexceedsthelengthcontext,weinevitablyneedtotruncatethedemonstration. This
will not only lose the information from the discarded tokens and cannot distill large amount of
demonstration(e.g. K > 1000 (Hao et al., 2022)). Concurrent work utilizes recurrent memory
transformer(Bulatovetal.,2022)tocompresslongtextdocumentsbeyondtheconstraintofcontext
windowsizeintosoftprompts. Weconsiderhandlingextra-longdemonstrationasourfuturework.
15