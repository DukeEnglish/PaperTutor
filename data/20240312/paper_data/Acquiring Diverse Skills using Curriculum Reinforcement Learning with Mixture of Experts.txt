Acquiring Diverse Skills using Curriculum Reinforcement Learning with
Mixture of Experts
OnurCelik1 AleksandarTaranovic1 GerhardNeumann1
Abstract relyingonalternatives. Thispropertymakesthemsuperior
becausecompleterelearningofskillsisavoided.
Reinforcement learning (RL) is a powerful ap-
proach for acquiring a good-performing policy. Acquiring these diverse skill sets requires learning a pol-
However, learning diverse skills is challenging icythatcanrepresentmulti-modalityinthebehaviorspace.
in RL due to the commonly used Gaussian pol- Recentadvancesinsupervisedpolicylearninghavedemon-
icyparameterization. WeproposeDiverseSkill strated the potential of training high-capacity policies ca-
Learning(Di-SkilL),anRLmethodforlearning pableofcapturingmulti-modalbehaviors(Jiaetal.,2024;
diverse skills using Mixture of Experts, where Shafiullah et al., 2022; Blessing et al., 2023; Chi et al.,
eachexpertformalizesaskillasacontextualmo- 2023). These policies have exhibited remarkably diverse
tion primitive. Di-SkilL optimizes each expert skillsandoutperformedstate-of-the-artmethods. However,
and its associate context distribution to a max- incaseswherenoexpertdataisavailableordatacollection
imum entropy objective that incentivizes learn- isexpensive,ReinforcementLearning(RL)isessentialto
ing diverse skills in similar contexts. The per- acquire skills. Discovering multi-modal behaviors using
expertcontextdistributionenablesautomaticcur- RLischallengingsincethepoliciesusuallyrelyonGaus-
ricula learning, allowing each expert to focus sianparameterizationandthuscanonlydiscoverasingle
onitsbest-performingsub-regionofthecontext behavior.
space. To overcome hard discontinuities and
Weconsidertrainingagentsthatpossessdiverseskillsfrom
multi-modalitieswithoutanypriorknowledgeof
which they can select to tackle a specific task differently.
the environment’s unknown context probability
Forcapturingthesemulti-modalitiesintheagent’sbehav-
space,weleverageenergy-basedmodelstorep-
iorspace,weemployhighlynon-linearMixtureofExpert
resent the per-expert context distributions and
policies. We use automatic curriculum learning for effi-
demonstrate how we can efficiently train them
cientlearning,enablingeachexperttofocusonaspecific
usingthestandardpolicygradientobjective. We
sub-region of the context space it favors. We introduce
showonchallengingrobotsimulationtasksthat
thiscurriculumshapingbyoptimizingforanadditionalper-
Di-SkilLcanlearndiverseandperformantskills.
expertcontextdistributionthatisusedtosamplecontexts
fromthepreferredregionstotrainthecorrespondingexpert.
Automaticcurriculumlearninghasproventoincreaseperfor-
1.Introduction
mancebyimprovingtheexplorationofagents,particularly
Solvingtasksindiversemannersenablesagentstobetter insparse-rewardedenvironments(Klinketal.,2022).
adapttounknownandchallengingsituations. Thisdiverse
WeexploreContextualReinforcementLearninginwhicha
skill set is beneficial in many scenarios, such as playing
continuous-valuedcontextdescribesthetask(Kupcsiketal.,
tabletennis,whereapplyingdifferentstrikes(e.g. backhand,
2013). Intheexampleofrobottabletennis(seeFig. 3a),
forehand,orsmashing)tosimilarincomingballsisadvanta-
acontextincludesthedesiredballlandingpositionsonthe
geousbecausethestrikeislesspredictablefortheopponent.
opponent’stablesideaswellasphysicalaspects,suchasthe
Similarly,inscenarioswithenvironmentalchangeswhere
incomingball’svelocityorfrictionproperties.Incontinuous
learnedskillsmightbeinfeasibleovertime(e.g. grasping
contextspaces,thecurriculumshapingper-expertcontext
anobjectwhileavoidingobstacles),diverseskillsprovide
distributions are often parameterized as Gaussian (Klink
additionaladaptivitybydiscardingtheseinvalidskillsand
etal.,2020a;Celiketal.,2022). However,theagentisusu-
*Equalcontribution 1AutonomousLearningRobots,Karlsruhe allyunawareofthecontextbounds,whichmakesadditional
InstituteofTechnologyKarlsruhe,Germany.Correspondenceto: techniquesnecessarytoconstrainthedistributionupdatesto
OnurCelik<celik@kit.edu>. staywithinthecontextregion(Celiketal.,2022). Instead,
we employ energy-based per-expert context distributions,
1
4202
raM
11
]GL.sc[
1v66960.3042:viXraAcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
whichcanbeevaluatedforanycontextandeffectivelyrep- chooses the controller’s parameters θ once at the begin-
resent multi-modality in the context space. Importantly, ningoftheepisode. Oneofthenoteworthyadvantagesof
ourmodelistrainedsolelyusingcontextsamplesfromthe CEPSliesintheindependenceofassumptionssuchasthe
environmentthatareinherentlyvalid. Ourapproachelim- MarkovianpropertyincommonMDPs. Thischaracteristic
inatestheneedforadditionalregularizationofthecontext rendersitaversatilemethodology,particularlywell-suited
distributionanddoesnotrequirepriorknowledgeaboutthe foraddressingadiversearrayofintricatetaskswherethe
environment. Duetotheoverlappingprobabilitydistribu- formulationofaMarkovianrewardfunctionisdifficult.
tionsofdifferentper-expertcontexts,ourresultingmixture
CEPShasbeenexploredbyapplyingvariousoptimization
policyoffersdiversesolutionsforthesamecontextwitha
techniques,includingPolicyGradients(Sehnkeetal.,2010),
highprobability.
NaturalGradients(Wierstraetal.,2014),stochasticsearch
RecentresearchinRLhasexploredmixtureofexpertspoli- strategies(Hansen&Ostermeier,2001;Mannoretal.,2003;
cies,butoftenthesemethodseithertrainthemixtureinun- Abdolmaleki et al., 2019), and trust-region optimization
supervisedRLsettingsandthenselectthebest-performing techniques(Abdolmalekietal.,2015;Danieletal.,2012;
expertinthedownstreamtask(Laskinetal.,2021;Eysen- Tangkarattetal.,2017),particularlyinthenon-contextual
bachetal.,2019)ortrainlinearexperts,limitingtheirper- setting. Researchersextendedthesettingbyincorporating
formance(Danieletal.,2012;Celiketal.,2022). Ourinspi- linear(Tangkarattetal.,2017;Abdolmalekietal.,2019)and
rationdrawsfromrecentadvancementsthathaveachieved non-linearcontextualadaptation(Ottoetal.,2023),lever-
diverse skill learning with a similar objective. However, agingtherecentlyintroducedtrust-regionlayersforneural
theirapproachinvolveslinearexpertmodelswithGaussian networks(Ottoetal.,2021). Allpreviouslymentionedlearn
context distributions. It requires prior knowledge of the single-modepoliciesanddonotaddressacquiringdiverse
environmenttodesignapenaltytermwhenthealgorithm skillsleveragingautomaticcurriculumlearning.
samplescontextsoutsidetheenvironment’sbounds. These
CurriculumReinforcementLearning(CRL).CRLcan
factorsrestrictthealgorithm’sperformanceandapplicabil-
potentially increase the performance of RL agents, espe-
itywhendefiningthecontextboundsrequiresknowledge,
ciallyinsparse-rewardedenvironmentsinwhichexploration
suchasforwardkinematicsinrobotics.
isfundamentallydifficult. Adaptingtheenvironmentbased
Tosummarize,weintroduceinthispaperanovelRLmethod ontheagent’slearningprocesshasbeenproposedbyseveral
forlearningamixtureofexpertspolicythatwerefertoas worksalready,e.g. automaticallygeneratingsetsoftasksor
Di-SkilL–DiverseSkillLearning. Ourmethodcangener- goalstoincreasethelearningspeedoftheagent(Florensa
alizetothecontinuousrangeofcontextsdefinedbythe(un- et al., 2017; 2018; Sukhbaatar et al., 2018; Zhang et al.,
known)environment’scontextdistributionwhilelearning 2020;Wo¨hlkeetal.,2020;Racaniereetal.,2020),orgener-
diverse,andnon-linearskillsforsolvingataskdefinedbya atingacurriculumbyinterpolatinganauxiliaryandknown
specificcontext. Importantly,ourapproachoperateswith- distribution of target tasks (Klink et al., 2022; 2020a;b).
outanyassumptionsabouttheenvironment. Weshowhow Worksproposesamplingatraininglevelfromaprespecified
wecanlearnmulti-modalcontextdistributionsbytraining setofenvironments(Jiangetal.,2021b),orunsupervised
anenergy-basedmodelsolelyoncontextsamplesobtained environmentdesign(Jiangetal.,2021a;Dennisetal.,2020)
fromtheenvironment. Onmultiplesophisticatedsimulated basedontheagent’slearningprocess. Noneoftheabove
robottasks,wedemonstratethatwecanlearndiverseskills methods apply automatic curriculum learning on an RL
whileperformingonparorbetterthanbaselines. problemwithanMoEpolicy,exceptfortheworkin(Celik
etal.,2022). However, theyparameterizethecurriculum
distributionasGaussian,sufferingfromlowrepresentation
2.PreliminariesandRelatedWork
capacityandrequiringknowledgeabouttheenvironment’s
Contextualepisode-basedPolicySearch(CEPS).CEPS contextdistribution. Instead,weshowhowwecanleverage
isablack-boxapproachtoreinforcementlearning(RL),in energy-basedmodelstoavoidtheseshortcomings.
whichthesearchdistributionistheagent’spolicythatmaps
MixtureofExperts(MoE)PolicyforCurriculumLearn-
thecontextsctopolicyparameters,typicallyrepresented
ing. TheMoEpolicyisformalizedas(Bishop,2006)
asmotionprimitives(Schaal,2006;Paraschosetal.,2013;
(cid:88)
Lietal.,2023)parameterizedbyθ. Thepolicyπ(θ|c)is π(θ|c)= π(o|c)π(θ|c,o), (2)
optimizedby
o
max E (cid:2)E [R(c,θ)](cid:3) , (1) wherethegatingdistributionπ(o|c)assignsanexpertoto
p(c) π(θ|c) thegivencontextc. Theexpertπ(θ|c,o)adaptstheparam-
π(θ|c)
eters θ of the motion primitive for c. The corresponding
whereR(c,θ)isthereturn. Givencontextsamplesfromthe motionprimitiveisthenexecutedintheenvironment.While
environment’scontextdistributionp(c),thepolicyπ(θ|c) thisformoftheMoEissuitableininferencetimewhenthe
2AcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
contextisassignedbytheenvironmentandtheagentneeds where L (o,c) = E (cid:2) R(c,θ) + αlogπ˜(o|c)(cid:3) +
c π(θ|c,o)
toproposeaskill,itdoesnotallowtoautomaticallylearn αH[π(θ|c,o)]. The variational distributions π˜(o|c,θ) =
acurriculumduringtraining. Thisdrawbackiscausedby π (o|c,θ) and π˜(o|c) = π (o|c) arise through the de-
old old
thelackofaparameterizeddistributionπ(c)thatispartof compositionandareresponsibleforlearningdiversesolu-
theMoEandallowsexplicitlychoosingandsettingcontext tionsandconcentratingoncontextregionswithsmall, or
samplesforthemodelitself,soeachexpertcandecideon nosupportbyπ(c). Ineveryiteration,thevariationaldis-
whichcontextsitfavorstraining. Introducingthisdistribu- tributionsareupdatedtotightenthebounds. Detailsofthe
tioninthecontextspaceisasmall,butnecessarydistinction equationsareintheAppendixAand(Celiketal.,2022).
toenableautomaticcurriculumlearningforeachsingleex-
RLwithMoE.(Renetal.,2021)proposesusingMoEpol-
perto. WecaneasilyreparameterizetheMoEwithoutany
icyrepresentationandpresentsanovelgradientestimatorto
assumptionbyusingBayes’ruleas(Celiketal.,2022)
calculatethegradientsw.r.t. theMoEparameters. (Huang
(cid:88)π(c|o)π(o) etal.,2023)presentsamodel-basedRLapproachtotrain
π(θ|c)= π(θ|c,o). (3)
π(c) latent variable models. The work presents a novel lower
o
boundfortrainingthemulti-modalpolicyparameterization.
Theper-expertcontextdistributionπ(c|o)cannowbeopti- Thesemethodsdifferfromourworkinthattheyarenotcate-
mizedandallowstheexpertotochoosecontextscitfavors. gorizedintheCEPSframework,oraremodel-basedvariants
(cid:80)
Notethatπ(c) = π(c|o)π(o). Wemodeleachπ(c|o) anddonotuseautomaticcurriculumlearningtechniques.
o
asanenergy-basedmodelandeachπ(θ|c,o)asaGaussian In the CEPS framework, RL with MoE policies has also
parameterizedasneuralnetwork(seeFig.1).Thepriorπ(o) beenexploredintheworksbyDanieletal.(2012);Endetal.
issettobeauniformdistributionthroughoutthiswork. (2017),inwhichanMoEmodelwithlinearexpertswithout
automaticcurriculumlearningislearned. Additionally,they
Self-PacedDiverseSkillLearningwithMoE.Weaimto
needtoaddconstraintstoenforcediversityintheexperts.
discover different skills in the same context-defined task.
The work by Celik et al. (2022) also relies on the maxi-
MoEmodels(seeEq. 3)arespecificallysuitableforskill
mum entropy objective as we do, however, their method
discovery due to their ability to represent multi-modality
onlyconsiderslinearexpertswithGaussianper-expertdis-
andtheper-expertcontextdistributionπ(c|o)forautomatic
tributionswhichlimitstheperformanceandconsequently
curriculumlearningwhichallowstheexpertstospecialize
requiresmanyexpertstosolveatask. Moreover,itrequires
inasubsetofthecontextspace. Forexplicitoptimization
environmentknowledgetohand-tuneapunishmenttermto
ofthispolicy,theKL-regularizedMaximumEntropyRein-
keeptheoptimizationoftheper-expertcontextdistributions
forcementLearningobjective(Celiketal.,2022)
withinthecontextbounds.
max E (cid:2)E [R(c,θ)]+αH[π(θ|c)](cid:3)
π(c) π(θ|c) UnsupervisedReinforcementLearningalsoconsiderslearn-
π(θ|c),π(c)
ingdiversepolicies(Laskinetal.,2021;Eysenbachetal.,
−βKL(π(c)∥p(c)) (4)
2019;Camposetal.,2020;Leeetal.,2019;Liu&Abbeel,
isanaturalchoice. TheKL-termincentivizesthecontext 2021). Theobjectivediffersfromoursandskillsaretrained
distributionπ(c)tomatchtheenvironment’sdistribution intheabsenceofanextrinsicreward. Wediscussparallels
p(c)andcanbeprioritizedduringoptimizationbychoos- intheAppendixB.
ingthescalingparameterβ appropriately. Theentropyof
themixturemodelincentivizeslearningdiversesolutions 3.DiverseSkillLearning
(Celiketal.,2022)andcanbeprioritizedwithahighscaling
parameterα. The common Contextual Episodic Policy Search (CEPS)
loop(Kupcsiketal.,2013)withaMixtureofExperts(MoE)
Itiswell-knownthatthisobjectiveisdifficulttooptimizefor
policyrepresentationlearningobservesacontextc,andthen
MoEpoliciesandrequiresfurtherstepstoobtainatractable
selectsanexpertothatsubsequentlyadjuststhecontroller
lower-bound(Celiketal.,2022)
parametersθ given(c, o). Weconsiderthesameprocess
max E [R(c,θ)+αlogπ˜(o|c,θ)] duringtestingtime,asshowninbluecolorinFig. 1(see
π(c|o),π(θ|c,o)
π(θ|c,o) alsoFig. 7a). However,theprocedurechangesduringtrain-
+αE [H[π(θ|c,o)]] (5) ingforDi-SkilLasautomaticcurriculumlearningrequires
π(c|o)
thattheagentcandeterminewhichcontextregionsitprefers
fortheexpertupdatesandalower-boundfortheper-expert
to focus on. In this case, we observe a batch of context
contextupdates
samplesfromtheenvironment’scontextdistributionp(c).
maxE [L (o,c)+(β−α)logπ˜(o|c)]+ Foreachofthesesamples,everyper-expertcontextdistri-
π(c|o) c
π(c|o) bution π(c|o) calculates a probability, which results in a
βH(π(c|o)), (6) categoricaldistributionoverthecontextsc. Weusethese
3AcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
DuringInference DuringTraining o=1,...,K
π(o|c) π(c|o)
o, c
o, c T c T c
o TrajectoryGeneration
µ
θ
Σ
θ∼N(µ θ,Σ θ) θ· =
θ
Figure1: TheSamplingprocedureforDi-SkilL.DuringInferencetheagentobservescontextscfromtheenvironment’s
unknowncontextdistributionp(c). Theagentcalculatesthegatingprobabilitiesπ(o|c)foreachcontextandsamplesan
expert o resulting in (o,c) samples marked in blue. During Training we first sample a batch of contexts c from p(c),
whichisusedtocalculatetheper-expertcontextdistributionπ(c|o)foreachexperto = 1,...,K. Theπ(c|o)providesa
higherprobabilityforcontextspreferredbytheexpertπ(θ|c,o). Toenablecurriculumlearning,weprovideeachexpert
thecontextssampledfromitscorrespondingπ(c|o),resultinginthesamples(o,c )markedinorange. Inbothcases,the
T
chosenπ(θ|c,o)samplesmotionprimitiveparametersθforeachcontextresultinginatrajectoryτ thatissubsequently
executedontheenvironment. Beforeexecution,thecorrespondingcontext,e.g.,thegoalpositionofabox,needstobesetin
theenvironment. Thisisillustratedbythedashedarrowswiththecontextinblueforinferenceandorangefortraining.
probabilitiestosamplecontextsc forthecorresponding inspecificpositionsonatable, theprobabilityofobserv-
T
expertoresultingin(c ,o)samplepairs(seeorangeparts ingagoalpositionoutsidethetable’ssurfaceiszero. This
T
in Fig. 1 and Fig. 7b). Each chosen expert o provides impliesthatalargesubsetofthecontextspacehasnoproba-
Gaussian distributions over the motion primitive parame- bilitymass.Therefore,explorationintheseregionsmightbe
tersθbymappingthecontextsc tomeanvectorsµ and difficultifthereisnoguidanceencodedinthereward. Even
T θ
covariancematricesΣ usingaparameterizedneuralnet- ifitisguaranteedthatπ(c|o)onlysamplesvalidcontexts,it
θ
work. Wecannowsamplemotionprimitiveparametersθ stillneedstobeabletorepresentmulti-modaldistributions,
from theseGaussiandistributionsto generatetrajectories suchasillustratedinFig. 7d. Thismulti-modalitycanbe
τ using a motion primitive generator. These trajectories presentbecauseofenvironmentalcircumstancesorsimply
aresubsequentlyexecutedontheenvironment(greencolor ifexpertsπ(θ|c,o)prefercontextsinspatiallyapartregions.
inFig. 1)andanepisodereturnR(θ,c )isobservedand Fortheobjectplacingexample,thiscouldcorrespondtore-
T
used for updating the MoE (see Section 3.2). Yet, there gionsonthetablewheretheobjectcannotbeplaceddueto
existseveralissuesforastableoveralltrainingoftheMoE obstaclesorholes. Wethereforerequireπ(c|o)tobeableto
model,whichrequiresspecialtreatmentforeachπ(c|o)and representi)complexdistributions,ii)multi-modalityandiii)
π(θ|c,o). Algorithmicdetailsandparameterizationsofthe onlyexplorewithinthevalidcontextboundsofp(c). We
modelcanbefoundintheAppendixC. proposeparameterizingπ(c|o)asanenergy-basedmodel
3.1.Energy-BasedModelForAutomaticCurriculum π(c|o)=exp(ϕ o(c))/Z (7)
Learning
toaddresstheissuesi)andii).Energy-basedmodels(EBMs)
Toillustratetheseissues,weconsiderabounded,uniformly haveshowntobecapableofrepresentingsharpdiscontin-
distributedtwo-dimensionalenvironmentcontextdistribu- uedfunctionsandmulti-modaldistributions(Florenceetal.,
tionp(c)(seeexampleintheAppendixCinFig. 7d). Itis 2022). Yet,theyarehardtotrainandsamplefromdueto
challengingforaReinforcementLearning(RL)agenttoau- theintractablenormalizingconstantZ =(cid:82) exp(ϕ (c))dc.
c o
tomaticallylearnitscurriculumπ(c|o)withinthevalidcon- Wecancircumventandaddresstheseissuesiii)byapprox-
textspace(Celiketal.,2022). Harddiscontinuitiessuchas imatingthenormalizingconstantwithcontextsc ∼ p(c)
stepsoftennaturallyariseinp(c)duetotheenvironment’s asZ ≈ 1 (cid:80)N exp(ϕ (c )). Thisapproximationisjus-
finitesupportinreal-worldenvironments. Forinstance,in N i=1 o i
tifiedaswecansamplefromp(c)bysimplyresettingthe
anenvironmentwheretheagent’staskistoplaceanobject
environment without execution. Additionally, the EBM
4
c∼p(c) o∼π(o|c) )o|c(π∼
c
T
xamtfoS )c(p∼cAcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
5Link-Reacher
Tip
Goal
(a) (b) (c) (d)
Figure2: a)High-probabilityregionsoftheindividualper-expertcontextdistributionsπ(c|o),whereacolorrepresentsan
experto. Theredcirclemarksthecontextspaceofgoal-reachingpositionsforthe5-LinkReacher’stip. Thespecialization
ofπ(c|o)isinducedbyπ˜(o|c). b)Differentπ(c|o)needtooverlapforlearningdiverseskills. Thisoverlappingisinduced
bytheentropybonusH[π(c|o)]. c)Differenttiptrajectoriessampledinthesamecontexts. Thetrajectoriesandtheendjoint
constellationareinthesamecolor. Thediversityintheparameterspaceisinducedbyπ˜(o|c,θ). d)Visualizationofthe
5-LinkReachertask(5LR).
willencounterimportantpartsofthecontextspaceduring Expert Update. We parameterize each expert π(θ|c,o)
the training by resampling a large enough batch of con- with a single neural network and update them by a trust-
textsc∼p(c)ineachiteration. Eachexpertcantherefore regionconstrainedoptimization
samplepreferredcontextsfromthecurrentbatchofvalid
max E [R(c,θ)+αlogπ˜(o|c,θ)]
contextsbycalculatingtheprobabilityforeachofthecon- π(c|o),π(θ|c,o)
π(θ|c,o)
textsusingπ(c|o)asparameterizedinEq. 7. Itshouldbe
+αE [H[π(θ|c,o)]] (8)
notedthatthissamplingprocedureisnotstraightforwardly π(c|o)
applicabletoexplicitmodelssuchasGaussians,orNormal- s.t. KL(π(θ|c,o)∥π old(θ|c,o))≤ϵ ∀c∈C,
izing Flows (Papamakarios et al., 2021). Those methods
wheretheKL-boundensuresthattheexpertπ(θ|c,o)does
wouldneedadditionaltechniqueslikeimportancesampling
not differ too much from the expert π (θ|c,o) from the
thatmightdestabilizelearningifnotcarefullycalibratedby old
iteration before for each context c. The entropy bonus
enforcingoverlappingsupportregionsofthesamplingand
H[π(θ|c,o)] incentivizes π(θ|c,o) to fully cover the pa-
theactualdistribution. Inourcase,updatingtheparameters
rameterspacewhileavoiding(θ,c)regionsthatarecovered
oftheEBMcanreadilybeaddressedbythestandardRL
by other experts o. The latter is guaranteed by π˜(o|c,θ)
objectivefordiverseskilllearning.
whichrewards(θ,c)regionsthatcanbeassignedtoexpert
owithhighprobability. Weefficientlyupdatetheexperts
3.2.UpdatingtheMixtureofExpertsModel
usingtrustregionlayers(Ottoetal.,2021;2023).
Weupdateeachexpertπ(θ|c,o)anditscorrespondingper-
Per-Expert Context Distribution Update. We consider
expert context distribution π(c|o) by maximizing the ob-
theobjectivewiththeaugmentedrewardsasshowninEq. 6
jectives in Eq. 5 and in Eq. 6, respectively. These de-
forupdatingeachcontextdistributionπ(c|o). Wecannot
composedobjectivesallowustoindependentlyupdateboth
applythetrustregionlayers(Ottoetal.,2021)inthiscase,
distributions and to retain the properties of diverse skill
asπ(c|o)isadiscretedistributionoverthecontextsamples
learningfromtheobjectiveinEq. 10. However,updating
c parameterized by the EBM. Yet, we can still use PPO
i
thedistributionsisnotstraightforwardduetothebi-level
(Schulman et al., 2017) for updating π(c|o) and simplify
optimizationthatleadstoadependencyonbothterms. This
ourobjective,aswecannowcalculatemanytermsinclosed
is particularly challenging for the expert π(θ|c,o) as the
form. Forthis,werewritetheobjectiveas
sampledcontextsccandrasticallychangefromoneitera-
tiontoanotherifπ(c|o)changestooaggressively.Thesame max (cid:88) π(c |o)L (o,c )+ (9)
i c i
appliesforupdatingπ(c|o)ascalculatingtheobjectivere- π(c|o)
ci∼p(c)
quirescalculatinganintegraloverθundertheexpectation (cid:88) (cid:0) (cid:1)
π(c |o) (β−α)logπ˜(o|c )−βlogπ(c |o)
ofπ(θ|c,o). Forastableupdateforbothdistributions,we i i i
employtrust-regionupdatestorestrictthechangeofboth ci∼p(c)
distributionsfromoneiterationtoanother. Theseupdates
andobservethatalltermsinthesecondsumcanbecalcu-
havebeenshowntoimprovethelearningprocess(Peters
lated in closed form. Note that the first term is approx-
etal.,2010;Schulmanetal.,2015;2017;Ottoetal.,2021).
imated by resampling the context samples using π(c|o)
sincecomputingL (o,c)requirescalculatingtheintegral
c
5AcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
Init.BoxPos. 1 Di-SkilL BBRL
Obstacle LinDi-SkilL PPO
0
0.8
Reference GoalLanding 0.6 −10
TargetPos.&Orientation
0.4 −20
Di-Skill
Racket
Obstacle Di-SkilLV2
Ball 0.2 Di-SkilLV3 −30
Goal GoalLandingPos. SVSL
0 BBRL
·106
·105
−40
Changesin 0 1 2 3 4 5 0 1 2 3 4
eachepisode
NumberofEpisodicSamples NumberofEpisodicSamples
(a)VisualizationofEnvironments (b)MeanSuccessRate(TT) (c)IQMMeanReturn(5LR)
Figure3: a)(lefttop)HopperJumpTask(HJ).(Topright)BoxPushingwithObstacle(BPO).(BottomLeft)RobotMini
Golf(MG).(Bottomright)Robottabletennis(TT).b)Ablationstudies,showcasingtheneedforautomaticcurriculum
learningforDi-SkilL.BBRLandDi-SkilLcansolveTTenvironmentdecently. Di-SkilL’svariantswithoutcurriculum
learningstruggletoachieveagoodperformance. SVSLneedsmoresamplestoachievearound80%successrate,suffering
underthelinearexperts. c)PerformanceofDi-SkilL,BBRL,LinDi-SkilL,andPPOon5LRwithsparsein-timerewards.
over θ under the expectation of π(θ|c,o) as L (o,c) = context. Thefigureshowsthatmoreexpertspreferregions
c
E (cid:2) R(c,θ)+αlogπ˜(o|c)(cid:3) +αH[π(θ|c,o)]. This close to the initial position of the reacher, indicating that
π(θ|c,o)
expectationcanonlybeestimatedforcontextvectorsthat thesecontextsareeasiertosolve. Despitetheclosenessto
areactuallychosenbythecomponent. Theentropybonus thereacher’sinitialposition,theagentdoesnothavetoexert
inEq. (9)incentivizescoveringofthecontextspace,while muchenergytoreachthesepoints. Indeed,bothaspectsare
focusingoncontextregionsthatarenot,oronlypartlycov- presentinthetask’srewardfunction(seeAppendixDfor
ered by other options. The latter is guaranteed by π˜(o|c) details), explainingwhythelefthalfplaneofthecontext
whichassignsahighprobabilityifexpertocanbeassigned space has fewer overlapping. However, the learned MoE
tothecontextc. hastwoormoreexpertsactiveinmostpartsofthecontext
region. Theseexpertsdifferintheirbehavior(seeFig. 2for
3.3.HowdoesDiversityEmerge? examples),whichismotivatedbythetermsπ˜(o|c,θ)and
H[π(θ|c,o)]inEq. 8.
FromtheEq. 8andEq. 9itisclearthatdiversebehaviors,
represented by the experts, emerge from the interplay of
4.Experiments
thosetermsinEq. 8andEq. 9. Wevisuallydemonstratethe
meaningoftheindividualtermsonthe5-LinkReachertask
Inourevaluations,wecompareDi-SkilLagainstthebase-
(seeFig. 2d). TheReacherneedstoreachagoalposition
linesBBRL(Ottoetal.,2023)andSVSL(Celiketal.,2022).
in the two-dimensional space with its tip. In this task, a
WhenevertheenvironmentsatisfiestheMarkovproperties,
contextrepresentsthegoalpositionwithinthecontextspace,
we additionally compare against PPO (Schulman et al.,
visualizedasaredcirclearoundthereacher’sfixedfirstjoint
2017). BBRLandSVSLaresuitablebaselinesastheyare
(Fig. 2a). WetrainedDi-SkilLwith50experts.
state-of-the-artCEPSalgorithms. BBRLcanlearnhighly
InFig. 2aweshowthehigh-probabilityregionsofthein- non-linearpoliciesleveragingtrustregionupdates. SVSL
dividualper-expertcontextdistributionsπ(c|o),bysetting learns a linear Mixture of Experts (MoE) model and can
the color intensity proportional to this probability. Each capturemulti-modalityinthebehaviorspace. Weconsider
colorrepresentsanindividualexperto. Eachπ(c|o)con- challengingroboticenvironmentswithcontinuouscontext
centratesonasub-regionofthecontextspacesuchthatthe andparameterspaces. Theconsideredenvironmentseither
correspondingπ(θ|c,o)becomesanexpertthere. Thisspe- haveanon-markovianrewardfunction, i.e. requireretro-
cializationisincentivizedbythetermπ˜(o|c)inEq. 9. For spectivedataforcalculation,ortemporallysparsereward
learningdiversebehaviorsforthesamecontextregions,it functionsincreasingthelearningcomplexity.
isnecessarythattheper-expertcontextdistributionsπ(c|o)
Environments. EnvironmentvisualizationsareinFig. 3a.
overlap,whichismotivatedbyH[π(c|o)]inEq. 9.
EnvironmentalandexperimentaldetailsareinAppendixD.
TheseoverlappingcontextregionsarevisualizedinFig. 2b,
Table Tennis (TT). A 7-degree of freedom (DoF) robot
where we count how many experts o are active for each
hastolearnfastandprecisemotionstosmashtheballtoa
6
etaRsseccuSnaeM
nruteRnaeMMQIAcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
Di-SkilL BBRL LinDi-SkilL PPO
20
0.8 0.6 0.6
18
0.6
16 0.4 0.4
0.4
14
0.2 0.2
0.2
12
10 0 ·106 0 ·106 0 ·106
0 1 2 3 0 0.5 1 1.5 2 2.5 0 0.5 1 1.5 2 2.5 0 0.5 1 1.5 2
NumberofEpisodicSamples ·105 NumberofEpisodicSamples NumberofEpisodicSamples NumberofEpisodicSamples
(a)IQMMeanReturn(HJ) (b)IQMSuccessRate(BPO) (c)IQMSuccessRate(TT-Hard) (d)IQMSuccessRate(MG)
Figure4: Performanceonthea)HJb)BPO,c)TT-H,andd)MGtasks. a)Di-SkilLperformsonparwithBBRLonthe
HJtask. b)Themulti-modalityintroducedbytheobstacleintheboxpushingtaskleadstoworseperformanceforBBRLthan
forDi-SkilLandLinDi-SkilL.PPOsuffersunderthetime-sparserewardsetting. c)WhileBBRLconvergesfaster,Di-SkilL
achievesahighersuccessrateeventually. d)Di-SkilLoutperformsthebaselinesontheMGtask. SinDi-SkilLperforms
poorlyonthenon-markovianrewardedtasksTT-HandMGindicatingthathighlynon-linearpoliciesarebeneficial.
desiredpositionontheopponent’sside. The4-dim. con- 4.1.ACLBenefits
textconsistsoftheincomingball’slandingpositionandthe
AutomaticCurriculumlearning(ACL)enablesDi-SkilL’s
desiredball’slandingpositionontheopponent’sside. The
experts to shape their curriculum by explicitly sampling
TTenvironmentrequiresgoodexploratorybehaviorandhas
frompreferredcontextregions. Weanalyzetheimportance
a non-markovian reward structure making step-based ap-
of this feature by comparing the performance of variants
proachesinfeasibletolearnusefulskills(Ottoetal.,2023).
ofDi-SkilL.ForbothvariantsDi-SkilLV2andDi-SkilLV3
TableTennisHard(TT-H).WeextendtheTTenvironment we disable ACL by fixing the variational distribution to
toamorechallengingversionbyvaryingtheball’sinitial π˜(o|c)=0inEq. 9andsettingtheentropyscalingparame-
velocity.Thisadditionallyincreasesthelearningcomplexity, terβ = 2000. Ignoringthevariationaldistributionduring
astheagentnowneedstoreasonaboutthephysicaleffectsof trainingeliminatestheintrinsicmotivationoftheper-expert
changedvelocityrangesandrequiresimprovedadaptability. contextdistributionπ(c|o)tofocusonsub-regionsinthe
contextspacethatarenot,oronlypartially,coveredbyany
5-LinkReacher(5LR).The5-Linkreacherhastoreacha
otherπ(c|o)(Section3.3). Settingβ = 2000incentivizes
goalpositionwithinallquadrantsinthecontextspace(see
eachπ(c|o)tomaximizeitsentropy,resultinginauniform
Fig. 2a) as opposed to the version in (Otto et al., 2023),
distribution. ForDi-SkilLwekeeptheACLandsetβ =4.
wherethemulti-modalityinthebehaviorspace(seeFig. 2c)
Weprovidethesamenumberof50context-parametersam-
wasavoidedbyconstrainingthecontextspacetotheupper
plesperexpertforDi-SkilLV2andDi-SkilL,whereasDi-
half of the context space. Additionally, the time-sparse
SkilLV3receives260samplesperexpertineachiteration.
rewardmakesthistaskachallengingbenchmark.
All variants possess 5 experts. In Fig. 3b we report the
Hopper Jump (HJ). Presented in (Otto et al., 2023) in meansuccessratesandthe95%confidenceintervalforeach
whichtheHopper(Brockmanetal.,2016)istaskedtojump methodonatleast4seeds. Di-SkilLV2convergestoamuch
ashighaspossiblewhilelandinginagoalposition. TheHJ smaller success rate and Di-SkilLV2 needs more samples
hasanon-markovianreward,makingstep-basedRLmeth- toreachthelevelofDi-SkilL.BBRLandDi-SkilLachieve
odsunfeasibletolearnusefulpolicies(Ottoetal.,2023). highsuccessrates. SVSLshowsworseperformance,even
thoughthemodelhas20experts.TheresultsshowthatACL
BoxPushingwithObstacle(BPO).A7DoFrobotistasked
isanessentialfeatureofDi-SkilLandthatlinearexpertsare
topushaboxtoatargetpositionandrotationwhileavoiding
notcapableofachievingasatisfyingperformance.
anobstacle. Inadditiontothetime-sparereward(Ottoetal.,
2023), our version includes the obstacle and considers a
4.2.AnalyzingthePerformanceandDiversity
largerrangeofthebox’stargetposition.
Wehaveevaluatedallmethodson24seedsforeachenvi-
RobotMiniGolf(MG).The7DoFrobothastohittheball
ronmentandalgorithmandreporttheinterquantilemean
inanenvironmentwithtwoobstacles(static,varying),such
(IQM) with a 95% stratified bootstrap confidence inter-
thatitpassesthetightgoal. Thecontextistheobstacle’s,
val as suggested by (Agarwal et al., 2021). Please note
thegoal’s,andtheball’sposition. TheMGenvironmenthas
that SVSL requires designing a punishment function to
anon-markovianreward,makingstep-basedRLmethods
guidethecontextsamplesintheenvironment’svalidcon-
unfeasibletolearnusefulpolicies(Ottoetal.,2023).
7
nruteRnaeMMQI etaRsseccuSMQI etaRsseccuSMQI etaRsseccuSMQIAcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
Figure5: Di-SkilL’sDiverseSkillsfortheBPOTask. Thefiguresvisualizediversesolutionstothesamecontextscona
table(blackrectangle). Thered,thickrectanglerepresentstheobstacle. The7DoFrobotistaskedtopushthebox(shownin
differentcolorsforeachsolutionfound)tothegoalboxposition(redrectanglewithagreendot)andaligntheblueedges
tomatchtheorientation. Wevisualizedsuccessfulboxtrajectoriesforeachsampledskill. Thediversitylearnedinthe
parameterspaceresultsindifferentboxtrajectoriesranginginpositionandorientation.
ball
ball
ball
ball ball
goal goal goal goal goal
(a)BackhandDrive (b)ForehandDrive (c)BackhandBlock (d)ForehandPush (e)BackhandSmash
Figure6: Di-SkilL’sDiverseSkillsfortheTT-Htask. Wefixedtheball’sdesiredlandingpositionandvariedtheserving
landingpositionandtheball’sinitialvelocity. Di-SkilLcanreturntheballindifferentstrikingtypes.
textregion,whichmakesitsapplicationdifficult,especially arebeneficial. Di-SkilLandBBRLperformsimilarlywell
if the context influences the objects’ physics. We there- on the TT-H task (see Fig. 4c), where Di-SkilL achieves
fore propose comparing against LinDi-SkilL instead of aslightlyhigherendsuccessratethanBBRL.Theperfor-
SVSL.LinDi-SkilLalsohaslinearexpertsbutbenefitsfrom mancegapismoreclearontheMGtask(seeFig. 4d)with
Di-SkilL’senergy-basedπ(c|o). Di-SkilLoutperformingBBRLandLinDi-SkilL.Di-SkilL
can discover common striking styles in table tennis. We
The performance curve of the HJ task in Fig. 4a shows
visualizesomeoftheselearnedskillinFig. 6. Additional
that Di-SkilL performs on par with BBRL, while BBRL
strikevisualizationscanbefoundintheAppendixE.
convergesslightlyfaster. Bothmethodscansolvethetask
indicatingthatthetaskdoesn’trequirediversity. Wepro-
vide additional analysis of this task in Appendix E. Fig. 5.ConclusionandFutureWork
4b shows the performance curves on the BPO task. The
Weproposedanovelmethodforlearningdiverseskillsus-
obstacle introduces multi-modality in the behavior space
ingacontextualMixtureofExperts. Eachexpertlearnsits
whichcannotbecapturedbyasingle-modepolicy,explain-
curriculumbyoptimizingforaper-expertcontextdistribu-
ingwhyDiSkilLandLinDi-SkilLoutperformBBRL,while
tion π(c|o). We have demonstrated challenges that arise
Di-SkilLstillachievesahighersuccessrate. PPO’spoor
throughenablingautomaticcurriculumlearning(ACR)and
performanceindicatesthattime-correlatedexplorationas
proposed parameterizing π(c|o) as energy-based models
usedwithmotionprimitivesiseffectiveinsparserewarded
(EBMs)toaddressthesechallenges. Additionally,wepro-
tasks. Wecanobserveasimilarperformancebehaviorin
vided a methodology to efficiently optimize these EBMs.
the 5LR task. In Fig. 3c we report the achieved returns
Wealsoproposedusingtrust-regionupdatesforthedeep
and observe that Di-SkilL outperforms BBRL due to the
expertstostabilizeourbi-leveloptimizationproblem. Inan
abilitytocapturemulti-modalbehaviors(e.g. reachingfrom
ablation,wehaveshownthatACRisnecessaryforefficient
differentsides)whilePPOsuffersfromthesparserewarded
andperformantlearning. Moreover,insophisticatedrobot
settingwhileLinDi-SkilL’slinearexpertscauseslowconver-
simulationenvironments,wehaveshownthatourmethod
gence,indicatingthatmoreexpertsareneeded. Di-SkilL’s
canlearndiverseskillswhileperformingonparorbetter
diverseskillsintheparameterspaceθinducedifferentbe-
thanthebaselines. Currently,themajordrawbackofourap-
haviors. Fig. 5showsdiverseboxtrajectoriestothesame
proachisthatitisnotabletoreplan,causingfailuresinthe
contextsintheBPOtask,whereasFig. 2cshowsdifferent
tasksiftherobotevenhassmallcollisionswithobjects. We
tiptrajectoriestothesamegoalpositionsinthe5LRtask.
intendtoaddressthisissueinfutureresearch. Forimprov-
Thenon-markovianrewardedtasks(TT-HandMG)show ingthesamplecomplexityofourapproach,weadditionally
thatnon-linearpoliciesaslearnedbyBBRLandDi-SkilL plantouseoff-policyRLtechniques.
8AcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
BroaderImpact Dennis, M., Jaques, N., Vinitsky, E., Bayen, A., Russell,
S.,Critch,A.,andLevine,S. Emergentcomplexityand
Thispaperpresentsworkwhosegoalistoadvancethefield
zero-shottransferviaunsupervisedenvironmentdesign.
of Machine Learning. There are many potential societal
Advancesinneuralinformationprocessingsystems,33:
consequencesofourwork,noneofwhichwefeelmustbe
13049–13061,2020.
specificallyhighlightedhere.
End,F.,Akrour,R.,Peters,J.,andNeumann,G. Layered
References direct policy search for learning hierarchical skills. In
2017 IEEE International Conference on Robotics and
Abdolmaleki, A., Lioutikov, R., Peters, J. R., Lau, N.,
Automation(ICRA),pp.6442–6448.IEEE,2017.
PualoReis,L.,andNeumann,G. Model-basedrelative
entropystochasticsearch. AdvancesinNeuralInforma- Engstrom,L.,Ilyas,A.,Santurkar,S.,Tsipras,D.,Janoos,
tionProcessingSystems,28,2015. F.,Rudolph,L.,andMadry,A. Implementationmatters
indeeppolicygradients:Acasestudyonppoandtrpo.In
Abdolmaleki, A., Simoes, D., Lau, N., Reis, L. P., and
InternationalConferenceonLearningRepresentations,
Neumann, G. Contextual direct policy search: With
2020.
regularized covariance matrix estimation. Journal of
Intelligent&RoboticSystems,96:141–157,2019.
Eysenbach,B.,Gupta,A.,Ibarz,J.,andLevine,S. Diversity
isallyouneed: Learningskillswithoutarewardfunction.
Agarwal,R.,Schwarzer,M.,Castro,P.S.,Courville,A.C.,
InInternationalConferenceonLearningRepresentations,
andBellemare,M. Deepreinforcementlearningatthe
2019.
edgeofthestatisticalprecipice. Advancesinneuralin-
formationprocessingsystems,34:29304–29320,2021.
Florence,P.,Lynch,C.,Zeng,A.,Ramirez,O.A.,Wahid,
Bishop, C. Pattern recognition and machine learning. A., Downs, L., Wong, A., Lee, J., Mordatch, I., and
Springergoogleschola,2:531–537,2006. Tompson,J. Implicitbehavioralcloning. InConference
onRobotLearning,pp.158–168.PMLR,2022.
Blessing, D., Celik, O., Jia, X., Reuss, M., Li, M.X., Li-
outikov,R.,andNeumann,G. Informationmaximizing Florensa, C., Held, D., Wulfmeier, M., Zhang, M., and
curriculum: Acurriculum-basedapproachforlearning Abbeel,P. Reversecurriculumgenerationforreinforce-
versatileskills. InThirty-seventhConferenceonNeural mentlearning. InConferenceonrobotlearning,pp.482–
InformationProcessingSystems,2023. 495.PMLR,2017.
Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,
Florensa,C.,Held,D.,Geng,X.,andAbbeel,P. Automatic
Schulman, J., Tang, J., andZaremba, W. Openaigym,
goal generation for reinforcement learning agents. In
2016. Internationalconferenceonmachinelearning,pp.1515–
1528.PMLR,2018.
Campos,V.,Trott,A.,Xiong,C.,Socher,R.,iNieto,X.G.,
and Torres, J. Explore, discover and learn: Unsuper-
Hansen,N.andOstermeier,A. Completelyderandomized
viseddiscoveryofstate-coveringskills. InProceedings
self-adaptationinevolutionstrategies. Evolutionarycom-
ofthe37thInternationalConferenceonMachineLearn-
putation,9(2):159–195,2001.
ing,ICML2020,13-18July2020,VirtualEvent,volume
119ofProceedingsofMachineLearningResearch,pp.
Huang,Z.,Liang,L.,Ling,Z.,Li,X.,Gan,C.,andSu,H.
1317–1327.PMLR,2020.
Reparameterizedpolicylearningformultimodaltrajec-
toryoptimization. ICML,2023.
Celik, O., Zhou, D., Li, G., Becker, P., and Neumann, G.
Specializingversatileskilllibrariesusinglocalmixture
Jia,X.,Blessing,D.,Jiang,X.,Reuss,M.,Donat,A.,Li-
ofexperts. InConferenceonRobotLearning,pp.1423–
outikov, R., and Neumann, G. Towards diverse behav-
1433.PMLR,2022.
iors: A benchmark for imitation learning with human
Chi,C.,Feng,S.,Du,Y.,Xu,Z.,Cousineau,E.,Burchfiel, demonstrations. In The Twelfth International Confer-
B., and Song, S. Diffusion policy: Visuomotor policy enceonLearningRepresentations,2024. URLhttps:
learningviaactiondiffusion. InProceedingsofRobotics: //openreview.net/forum?id=6pPYRXKPpw.
ScienceandSystems(RSS),2023.
Jiang,M.,Dennis,M.,Parker-Holder,J.,Foerster,J.,Grefen-
Daniel,C.,Neumann,G.,andPeters,J. Hierarchicalrela- stette,E.,andRockta¨schel,T. Replay-guidedadversarial
tiveentropypolicysearch. InArtificialIntelligenceand environment design. Advances in Neural Information
Statistics,pp.273–281.PMLR,2012. ProcessingSystems,34:1884–1897,2021a.
9AcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
Jiang,M.,Grefenstette,E.,andRockta¨schel,T. Prioritized withmovementprimitives.InConferenceonRobotLearn-
level replay. In International Conference on Machine ing,pp.1244–1265.PMLR,2023.
Learning,pp.4940–4950.PMLR,2021b.
Papamakarios,G.,Nalisnick,E.,Rezende,D.J.,Mohamed,
Klink,P.,Abdulsamad,H.,Belousov,B.,andPeters,J. Self- S., and Lakshminarayanan, B. Normalizing flows for
pacedcontextualreinforcementlearning. InConference probabilistic modeling and inference. The Journal of
onRobotLearning,pp.513–529.PMLR,2020a. MachineLearningResearch,22(1):2617–2680,2021.
Klink,P.,D’Eramo,C.,Peters,J.R.,andPajarinen,J. Self- Paraschos,A.,Daniel,C.,Peters,J.R.,andNeumann,G.
paceddeepreinforcementlearning. AdvancesinNeural Probabilisticmovementprimitives. Advancesinneural
InformationProcessingSystems,33:9216–9227,2020b. informationprocessingsystems,26,2013.
Klink,P.,Yang,H.,D’Eramo,C.,Peters,J.,andPajarinen, Peters, J., Mulling, K., and Altun, Y. Relative entropy
J. Curriculumreinforcementlearningviaconstrainedop- policysearch. InProceedingsoftheAAAIConferenceon
timaltransport. InInternationalConferenceonMachine ArtificialIntelligence,volume24,pp.1607–1612,2010.
Learning,pp.11341–11358.PMLR,2022.
Racaniere, S., Lampinen, A., Santoro, A., Reichert, D.,
Kupcsik,A.,Deisenroth,M.,Peters,J.,andNeumann,G. Firoiu,V.,andLillicrap,T. Automatedcurriculumgener-
Data-efficientgeneralizationofrobotskillswithcontex- ationthroughsetter-solverinteractions. InInternational
tual policy search. In Proceedings of the AAAI confer- ConferenceonLearningRepresentations,2020.
enceonartificialintelligence,volume27,pp.1401–1407,
2013. Ren,J.,Li,Y.,Ding,Z.,Pan,W.,andDong,H. Probabilis-
tic mixture-of-experts for efficient deep reinforcement
Laskin,M.,Yarats,D.,Liu,H.,Lee,K.,Zhan,A.,Lu,K., learning. arXivpreprintarXiv:2104.09122,2021.
Cang,C.,Pinto,L.,andAbbeel,P. Urlb: Unsupervised
reinforcementlearningbenchmark. InVanschoren,J.and Schaal, S. Dynamic movement primitives-a framework
Yeung,S.(eds.),ProceedingsoftheNeuralInformation formotorcontrolinhumansandhumanoidrobotics. In
ProcessingSystemsTrackonDatasetsandBenchmarks, Adaptivemotionofanimalsandmachines,pp.261–280.
volume1.Curran,2021. Springer,2006.
Lee, L., Eysenbach, B., Parisotto, E., Xing, E., Levine, Schulman,J.,Levine,S.,Abbeel,P.,Jordan,M.,andMoritz,
S.,andSalakhutdinov,R. Efficientexplorationviastate P. Trust region policy optimization. In International
marginal matching. arXiv preprint arXiv:1906.05274, conferenceonmachinelearning,pp.1889–1897.PMLR,
2019. 2015.
Li,G.,Jin,Z.,Volpp,M.,Otto,F.,Lioutikov,R.,andNeu- Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
mann,G. Prodmp: Aunifiedperspectiveondynamicand Klimov, O. Proximal policy optimization algorithms.
probabilisticmovementprimitives. IEEERoboticsand arXivpreprintarXiv:1707.06347,2017.
AutomationLetters,8(4):2325–2332,2023.
Sehnke,F.,Osendorfer,C.,Ru¨ckstieß,T.,Graves,A.,Pe-
Liu,H.andAbbeel,P. Aps: Activepretrainingwithsucces- ters,J.,andSchmidhuber,J. Parameter-exploringpolicy
sorfeatures. InMeila,M.andZhang,T.(eds.),Proceed- gradients. NeuralNetworks,23(4):551–559,2010.
ings of the 38th International Conference on Machine
Shafiullah,N.M.,Cui,Z.,Altanzaya,A.A.,andPinto,L.
Learning,volume139ofProceedingsofMachineLearn-
Behaviortransformers: Cloningkmodeswithonestone.
ingResearch,pp.6736–6747.PMLR,18–24Jul2021.
Advancesinneuralinformationprocessingsystems,35:
Mannor,S.,Rubinstein,R.Y.,andGat,Y.Thecrossentropy 22955–22968,2022.
methodforfastpolicysearch. InProceedingsofthe20th
Sukhbaatar,S.,Lin,Z.,Kostrikov,I.,Synnaeve,G.,Szlam,
InternationalConferenceonMachineLearning(ICML-
A., and Fergus, R. Intrinsic motivation and automatic
03),pp.512–519,2003.
curriculaviaasymmetricself-play. InInternationalCon-
Otto,F.,Becker,P.,Vien,N.A.,Ziesche,H.C.,andNeu- ferenceonLearningRepresentations,2018.
mann,G. Differentiabletrustregionlayersfordeepre-
Tangkaratt, V., Van Hoof, H., Parisi, S., Neumann, G.,
inforcementlearning. arXivpreprintarXiv:2101.09207,
Peters, J., and Sugiyama, M. Policy search with high-
2021.
dimensional context variables. In Proceedings of the
Otto,F.,Celik,O.,Zhou,H.,Ziesche,H.,Ngo,V.A.,and AAAIConferenceonArtificialIntelligence, volume31,
Neumann, G. Deep black-box reinforcement learning 2017.
10AcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
Wierstra,D.,Schaul,T.,Glasmachers,T.,Sun,Y.,Peters,
J.,andSchmidhuber,J. Naturalevolutionstrategies. The
JournalofMachineLearningResearch,15(1):949–980,
2014.
Wo¨hlke,J.,Schmitt,F.,andvanHoof,H. Aperformance-
basedstartstatecurriculumframeworkforreinforcement
learning. InProceedingsofthe19thInternationalCon-
ferenceonAutonomousAgentsandMultiAgentSystems,
pp.1503–1511,2020.
Zhang,Y.,Abbeel,P.,andPinto,L. Automaticcurriculum
learningthroughvaluedisagreement. AdvancesinNeural
InformationProcessingSystems,33:7648–7659,2020.
11AcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
A.AdditionalInformationtoSelf-Paced Parametrization of the expert π(θ|c,o). We pa-
DiverseSkillLearningwithMoE rameterize each expert π(θ|c,o) as a Gaussian policy
N(µ (c),Σ (c), where the mean µ (c) and the covari-
γ γ γ
Thegeneralself-paceddiverseskilllearningobjective anceΣ (c)arefunctionsofthecontextcandparameterized
γ
byaneuralnetworkwithparametersγ. Althoughtheco-
varianceΣ (c)isformalizedasafunctionofthecontext
γ
max E (cid:2)E [R(c,θ)]+αH[π(θ|c)](cid:3) c,wehavenotobservedanyadvantagesindoingso. Inour
π(c) π(θ|c)
π(θ|c),π(c) experiments,wethereforeparameterizethecovarianceasa
−βKL(π(c)∥p(c)) lower-triangularmatrixLandformthecovariancematrix
Σ=LLT.
canbereformulatedto Parameterizationoftheper-expertcontextdistribution
π(c|o). ThereaderisreferredtoSection3fordetailsonthe
maxE (cid:2)E [R(c,θ)+αlogπ(o|c,θ)] parameterizationofπ(c|o)
π(o),π(c|o) π(θ|c,o)
π(c,θ)
Parameterizationofthepriorπ(o). Wefixthepriorπ(o)
+βlogp(c)+(β−α)logπ(o|c)]
to a uniform distribution over the number K of available
+αE π(o),π(c|o)[H[π(θ|c,o)]] components and do not further optimize this distribution.
+βE [H[π(c|o)]]+βH[π(o)]. (10) This is a useful definition to increase the entropy of the
π(o)
mixturemodel.
byinsertingπ(θ|c),π(c)fromEq. (3)intoEq. (10)andap- Parameterizationofthecontextdistributionπ(c). Due
(cid:80)
plyingBayestheorem. Thisobjectiveisnotstraightforward totherelationπ(c)= π(c|o)π(o),π(c)isdefinedby
o
tooptimizeforMixtureofExpertsMoEmodelsandrequires π(c|o)anddoesnotneedexplicitmodelling.
furtherstepstointroducealowerbound(seeSection2)that
Parameterizationofthegatingdistributionπ(o|c). Due
canbeefficientlyoptimized. Werefertheinterestedreader
to the relation π(o|c) = π(c|o)π(o) we do not need an ex-
to(Celiketal.,2022)foradetailedderivation. π(c)
plicit parameterization of π(o|c) and can easily calculate
theprobabilitiesforchoosingtheexpertogivenacontextc.
B.AdditionalRelatedWork
C.2.UsingMotionPrimitivesintheContextof
UnsupervisedReinforcementLearning. Anotherfieldof
ReinforcementLearning
researchthatconsiderslearningdiversepoliciesisunsuper-
visedreinforcementlearning(URL).InURLtheagentis
MotionPrimitives(MPs)arealow-dimensionalrepresen-
firsttrainedsolelywithanintrinsicrewardtoacquireadi-
tation of a trajectory. For instance, instead of parameter-
versesetofskillsfromwhichthemostappropriateispicked
izing a desired joint-level trajectory as the single state in
tosolveadownstreamtask. Morerelatedtoourworkisa
eachtimestep,MPsintroducealow-dimensionalparameter
groupofalgorithmsthatobtaintheirintrinsicrewardbased
vector θ which concisely defines the trajectory to follow.
oninformation-theoreticformulations(Laskinetal.,2021;
The generation of the trajectory depends on the method
Eysenbachetal.,2019;Camposetal.,2020;Leeetal.,2019;
thatisused. ProbabilisticMovementPrimitives(ProMPs)
Liu&Abbeel,2021). However,theirresultingobjectiveis
(Paraschosetal.,2013)forexampledefinethedesiredtra-
basedonthemutual-informationanddiffersfromtheobjec- jectory as a simple linear function τ = ΦTθ, where Φ
tivewemaximize. Thelearnedskillsinthepre-trainingaim
aretime-dependentbasisfunctions(e.g. normalizedradial
tocoverdistinctpartsofthestate-spaceduringpre-training
basisfunctions). DynamicMovementPrimitives(DMPs)
intheabsenceofanextrinsictaskrewardwhichimpliesthat
(Schaal,2006)relyonasecond-orderdynamicsystemthat
skillsarenotexplicitlytrainedtosolvethesametaskindif-
provides smooth trajectories in the position and velocity
ferentways. Thosemethodsoperatewithinthestep-based
space. RecentlyProbabilisticDynamicMovementPrimi-
RLsettingwhichdiffersfromCEPS.
tives(ProDMPs)wereintroducedbyLietal.(2023)and
combinestheadvantagesofbothmethods,thatistheeasy
C.AdditionalInformationtoDiverseSkill generationoftrajectoriesandsmoothtrajectories. Wethere-
Learning forerelyonProDMPsthroughoutthiswork.
Inthecontextofreinforcementlearning,thepolicyπ(θ|c),
C.1.TheParameterizationoftheMixtureofExperts
orinourcaseanexpertπ(θ|c,o)definesadistributionover
(MoE)Model
the parameters θ of the MP depending on the observed
Inthefollowing,weprovidedetailsontheparameterization contextc. Thisallowsthepolicytoquicklyadapttonew
oftheMoEmodel.
12AcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
0.162 0.2025
c o 0.144 0.1800
o c 0.126 0.1575
0.108 0.1350
0.090 0.1125
0.072 0.0900
0.054 0.0675
0.036 0.0450
θ θ
0.018 0.0225
0.000 0.0000
(a) (b) (c) (d)
Figure7: ProbabilisticGraphicalModels(PGMs)duringinferencea)andtrainingb). Duringa))themodelobserves
thecontextscfromtheenvironment. Anexpertoissampledfromπ(o|c), whichleadstoanadjustmentofthemotion
primitiveparametersθbyπ(θ|c,o). Weiterateovereachexpertduring(b),samplethecontextscandθfromtheper-expert
distributionπ(c|o)andπ(θ|c,o)respectively. Samplingfromπ(c|o)allowsshapingtheexpert’scurriculum. c)illustrates
theenvironment’scontextdistributionp(c)andapossiblyoptimalπ(c|o)(d))intwo-dim. space. Yellowareasindicatehigh
andpurplezeroprobability. Theillustrationsshowthatoptimizingπ(c|o)requiresdealingwithi)step-likenon-linearities,
ii)multi-modality,iii)boundedwithintheredrectanglesupportofp(c),complicatingexploration.
tasksdefinedbyc. Algorithm2Di-SkilLInference
Input: π(θ|c)
C.3.AlgorithmDetails 1: c∼p(c)(observecontextsfromenvironment)
Detaileddescriptionsofthealgorithmduringtrainingand 2: o∼π(o|c),whereπ(o|c)= π(c π|o () cπ )(o)
duringinferenceareprovidedinthealgorithmboxesAlg. 3: θ ∼π(θ|c,o)
1andAlg. 1respectively. Ineachiterationduringtraining, 4: R(c,θ)←eval(θ,c)
wesampleabatchofcontextscfromtheenvironmentby
resettingit. Wetheniterateovereachexpertandevaluate
theprobabilitiesofthesecontextsconeachper-expertcon-
textdistributionπ(c|o)andsamplethentrainingcontexts
c from them. From the corresponding expert π(θ|c,o)
T
wesamplemotionprimitiveparametersθandevaluatethe
samples (c ,θ) on the environment and observe a return
T
R(c,θ)whichweusetoupdatetheexpertsπ(θ|c,o)and
theper-expertcontextdistributionsπ(c|o)bymaximizing
Obj. 8 and Obj. 9 respectively. During inference we ob-
servecontextscfromtheenvironment,calculatethegating
distributions π(o|c) = π(c|o)π(o) from which we sample
π(c)
theexperto. Wetheneithertakethemeanorsampleanθ
fromthisexpertandexecuteitontheenvironment.
Algorithm1Di-SkilLTraining
Input: α, β,N(max. iterations),K(num. experts),T(num.
samplesperexpert)
Output: π(θ|c)
1: fork =1toNdo
2: c∼p(c)(contextbatchbyenvironmentresetting)
3: foro=1toK do
4: c T ∼π(c|o)(contextbatchfromEBM)
5: θ ∼π(θ|c T,o)
6: R(c,θ)←eval(θ,c T)
7: π(θ|c,o)←Obj. 8
8: π(c|o)←Obj. 9
9: endfor
10: endfor
13AcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
D.ExperimentalDetails • cond. 3: cond.2issatisfiedandtherobotdidhitthe
ball,
D.1.EnvironmentDetails
• cond. 4: cond.3issatisfiedandthereturnedballlands
D.1.1.TABLETENNISEASY
onthetable,
Environment. Weusethesametabletennisenvironment
• cond. 5: cond.4issatisfiedandthelandingpositionis
as presented in (Otto et al., 2023), in which a 7 Degree
attheopponent’sside.
of Freedom (DoF) robot has to return a ball to a desired
balllandingposition. Thecontextisthefour-dimensional
spaceoftheball’sinitiallandingposition(x∈[−1,−0.2], Theepisodeendswhenanyofthefollowingconditionsare
y ∈[−0.65,0.65])ontherobot’stablesideandthedesired met
ball landing position (x ∈ [−1.0,−0.2], y ∈ [−0.6,0.6])
ontheopponent’stableside. Therobotiscontrolledwith • themaximumhorizonlengthisreached
torques on the joint level in each time step. The torques
• balldidlandonthefloorwithouthitting
aregeneratedbythetrackingcontroller(PD-controller)that
tracksthedesiredtrajectorygeneratedbythemotionprim-
• balldidlandonthefloorortableafterhitting
itive. Weconsiderthreebasisfunctionsperjointresulting
ina21-dimensionalparameter(θ)space. Weadditionally
Thewholedesiredtrajectoryisobtainedaheadofenviron-
allowtheagenttolearnthetrajectorylengthandthestart-
mentinteraction,makinguseofthispropertywecancollect
ingtimestepofthetrajectory. Notethatthestartingpoint
some samples without physical simulation. The reward
allowstheagenttodefinewhenaftertheepisode’sstartthe
functionbasedonthisdesiredtrajectoryisdefinedas
generateddesiredtrajectoryshouldbetracked. Inducedby
thevaryingcontexts,thisishelpfultoreacttothevarying (cid:88)
r =− |τd|−|qb|, (i,j)∈{(i,j)||τd|>|qb|}
timetheservedballneedstoreachapositionalspacethatis traj ij j ij j
(i,j)
convenienttohittheballwiththerobot’sracket. Overallthe
parameterspaceis23dimensional. Thetaskisconsidered
whereτd isthedesiredtrajectory,iisthetimeindex,j is
successfulifthereturnedballlandsontheopponent’sside
thejointindex, qb isthejointpositionupperbound. The
ofthetableandwithin≤0.2mtothegoallocation.
desiredtrajectoryisconsideredasinvalidifr < 0,an
traj
Therewardfunctionisunchangedfrom(Ottoetal.,2023) invalid trajectory will not be executed on the robot. The
andisdefinedas overallrewardisdefinedas:
 (cid:40)
0
f
2,
(p r,p b)
i if fc co on nd d.
.
1 2,
,
r = r rt tr aa skj,
,
r ot tr ha ej rw< is0
e
R = f (p ,p ,p ,p ) ifcond. 3,
task 3 r b l goal
f
f4( (p pr, ,p pb, ,p pl, ,p pgoal)
)
i if fc co on nd d.
.
4 5,
,
S teV rmSL fo. rcS oV ntS eL xtr se aq mu pir le es sd the as tig an rein ng ota ig nu aid vi an lg idp ru en gi is oh nm
.
Fe on rt
5 r b l goal
thefour-dimensionalcontextspaceintabletennis,thiscan
wherep istheexecutedtrajectorypositionoftheracket bedoneusingquadraticfunctions(asproposedintheorigi-
r
center,p istheexecutedpositiontrajectoryoftheball,p nalwork(Celiketal.,2022)):
b l
istheballlandingposition,p isthetargetposition. The
goal R (c)=−20·d2,
individualfunctionsaredefinedas c c
whered2isthedistanceofthecurrentcontextctothevalid
f (p ,p )=0.2−0.2g(p ,p ) c
2 r b r b
contextregion.
f (p ,p ,p ,p )=3−2g(p ,p )−h(p ,p )
3 r b l goal r b l goal
f 4(p r,p b,p l,p goal)=6−2g(p r,p b)−4h(p l,p goal) SVSLHyperparameters Allhyperparametersaresum-
f (p ,p ,p ,p )=7−2g(p ,p )−4h(p ,p ), marizedintheTable1.
5 r b l goal r b l goal
HyperparametersarelistedintheTable2.
where g(x,y) = tanh(min||x−y||2) and h(x,y) =
tanh(||x−y||2). Thedifferentconditionsare
D.1.2.TABLETENNISTASKHARD
• cond. 1: theendoftheepisodeisnotreached, Environment. We extend the table tennis environment
describedinAppendixD.1.1byadditionallyincludingthe
• cond. 2: theendoftheepisodeisreached, ball’sinitialvelocityinthecontextspacemakingthetask
14AcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
harderastheagenthastoreacttorangingvelocitiesnow.We D.2.1.BOXPUSHINGWITHOBSTACLETASK
definetheinitialvelocityv ∈[1.5m,4m]. Notethatevery
x s s Environment. Weincreasethedifficultyoftheboxpush-
singleconstellationwithintheresultingcontextspaceisa
ing environment as presented in (Otto et al., 2023), by
validcontext. However,thereexistballlandingpositions
changing major parts of the context space. The goal of
thatcannotbesetalongwithasubsetoftheinitialvelocity
theboxpushingtaskistomoveaboxtoaspecifiedgoal
range. Thismakesdesigningaguidingpunishmenttermfor
locationandorientationusingthesevenDoFFrankaEmika
SVSLespeciallydifficult. Weadopttheparameterspace
Panda. Thenewlycontextspace(comparedtotheoriginal
andtherewardfunctionasdefinedinthestandardtable
versionin(Ottoetal.,2023))aredescribedinthefollow-
tennisenvironmentasdescribedinAppendixD.1.1.
ing. We increase the box’ goal position range to x ∈
g
HyperparametersarelistedintheTable4. [0.3,0.6],y ∈ [−0.7,0.45],andkeepthegoalorientation
g
angleϕ ∈ [0rad,2πrad]. Additionally,weincludeanob-
D.2.HopperJump staclebetweentheinitialboxandthebox’sgoal. Therange
oftheobstaclepositionisx ∈[0.3,0.6],y ∈[−0.3,0.15].
o o
Environment. Weusethesamehopperjumpenvironment
Notethatweguaranteeadistanceofatleast0.15mbetween
as presented in (Otto et al., 2023), in which the hopper
theobstacle’spositionandtheinitialpositionaswellasat
(Brockman et al., 2016) has to jump as high as possible
least0.15mbetweentheobstacle’spositionandthebox’s
and land at a specified position. The context is the four-
goalposition.
dimensionalspaceofthelastthreejointsofthehopperand
the goal landing position [j ,j ,j ,g], where the ranges Therobotiscontrolledviatorquesonthejointlevel. We
3 4 5
arefrom[−0.5,−0.2,0,0.3]to[0.0,0.0,0.785,1.35]. The usefourbasisfunctionsperDoF,resultinginaparameter
hopperiscontrolledthesameasin(Brockmanetal.,2016). spaceof28dimensions. Weconsideranepisodesuccessful
Here,weconsiderthreebasisfunctionsperjointandagoal if the box’s orientation around the z-axis error is smaller
basisresultinginaparameterspace(θ)of12dimensions. than0.5radandthepositionerrorissmallerthan0.05m.
Therewardisnon-markovianandisunchangedfrom(Otto
The sparse-in-time reward function is up to a scaling
etal.,2023).
parameterthesameaspresentedin(Ottoetal.,2023). We
Ineachtime-stepttheactioncost describethewholerewardfunctioninthefollowing.
K Thebox’sdistancetothegoalpositionis
(cid:88)
τ =10−3 (ai)2,
t t
R =∥p−p ∥,
i goal goal
isprovided. ThevariableK =3correspondstothenumber wherepistheboxpositionandp isthegoalposition.
goal
ofdegreesoffreedom. Attheendoftheepisode,areward Therotationdistanceisdefinedas
containingretrospectiveinformationaboutthemaximum 1
R = arccos|r·r |,
height in the z-direction of the center of mass achieved rotation π goal
h ,thegoallandingpositionoftheheelp ,thefoot’s
max goal whererandr aretheboxorientationandgoalorienta-
goal
heel position when having contact with the ground after
tionquaternionrespectively. Theincentivetokeeptherod
jumpingthefirsttimep isgiven. Additionally,per-
foot,contact withintheboxisdefinedas
timeinformationsuchasp describingthepositionof
foot,t
thefoot’sheelinworld-coordinatesisgiven. Theresulting R rod =clip(||p−h pos||,0.05,10),
rewardfunctionis
where h is the position of the rod tip. Similarly, to
pos
(cid:88)T incentivize to maintain the rod in a desired rotation, the
R =− τ +R +R +R +R ,
tot t height gdist cdist healthy reward
t=0
2
where R rodrotation =clip( π arccos|h rot·h 0|,0.25,2)
R height =10h max, isdefined,whereh rotandh 0 =(0,1,0,0)arethecurrent
R =||p −p || , anddesiredrodorientationinquaternionrespectively. To
gdist foot,T goal 2
incentivize the robot to stay within the joint and velocity
R =||p −p || ,
cdist foot,contact goal 2
bounds,theerror
(cid:26)
2 ifz ∈[0.5,∞]andθ,γ,ϕ∈[−∞,∞]
R healthy = 0 elseT . err(q,q˙)= (cid:88) (|q i|−|q ib|)
Thehealthyrewardisthesameasprovidedby(Brockman i∈{i||qi|>|q ib|}
(cid:88)
etal.,2016). + (|q˙ |−|q˙b|)
j j
HyperparametersarelistedintheTable5. j∈{j||q˙j|>|q˙ jb|}
15AcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
isused,whereq,q˙,qb,andq˙baretherobot’sjointpositions D.4.RobotMiniGolfTask
andvelocitiesaswellastheirrespectivebounds. Tolearn
Environment. Intherobotminigolftasktheagentneeds
low-energymotions,theper-timeaction(torque)cost
to hit a ball while avoiding the two obstacles, such that
it passes the tight goal to achieve a bonus. The con-
K
(cid:88)
τ t = (ai t)2, text space consists of the ball’s initial x-position x ball ∈
[0.25m,0.6m], the XY positions of the green obstacle
i
x ∈ [0.3,0.6] and y ∈ [−0.5,−0.1] and the x posi-
obs obs
isused. Theresultingtemporalsparserewardisgivenas tionsofthegoalx ∈[0.25,0.6]. Theparameterspace
ball
is29dimensionalresultingfromthe4basisfunctionsper

−R rod−R rodrotation−0.02τ t−err(q,q˙) t<T, jointandanadditionaldurationparameterwhichallowsthe
R = −R −R −0.02τ −err(q,q˙) robottolearnthedurationofthetrajectory. Therobotstarts
tot rod rodrotation t
−350R −200R t=T, alwaysatthesameposition. Therewardfunctionconsists
goal rotation
ofthreestages:
whereT = 100isthehorizonoftheepisode. Thereward 
g tii mve es sr tee plev oa fn tht ein ef po ir sm oda eti ,o wn hto ichso mlv ae kt eh se ea xs pk loo rn atl iy oi nn ht ah re d.last
− 0.0 2. −00 00 .5 2· tτ
at
nh(min||p r−p b||)
i if fc co on nd d.
.
1 2,
,
R = 2−2tanh(min||p −p ||)
task b g
F tiou nr ath le pr loV tsis ou fa tl hi eza bt oio xn ’ss to raf jl ee ca tr on rie ed sis nki tl hl es. boW xpe us sh ho iw nga td ad ski- −
6
tanh(||p b,y−p thresh,y||) i if fc co on nd d.
.
3 3,
,
inFig. 8.
wheretheindividualconditionsare
HyperparametersarelistedintheTable6.
• cond. 1: theendoftheepisodeisnotreached,
D.3.Extended5-LinkReacherTask
• cond.2:theendoftheepisodeisreachedandtherobot
Environment. Inthe5-LinkReachertask,a5-linkplanar
didnothittheball,
robothastoreachagoalpositionwithitstip. Thereacher’s
initialpositionisstraighttotheright. Thistaskisdifficult • cond.3:theendoftheepisodeisreachedandtherobot
to solve, as it introduces multi-modality in the behavior hashittheball,buttheballdidn’tpassthegoal
space. (Otto et al., 2023) avoided this multi-modality by
constrainingtheycoordinateofthegoalpositiontoy ≥0, • cond. 4: theendoftheepisodeisreached,robothas
i.e. thefirsttwoquadrants. Weadoptthe5Link-Reacher hittheballandtheballhaspassedthegoalforatleast
task by increasing the context space to the full space, i.e. 0.75m
allfourquadrants. Weconsider5basisfunctionsperjoint
leadingtoa25-dimensionalparameterspace. Weconsider TheepisodeendswhenthemaximumhorizonlengthT =
thesparserewardfunctionpresentedin(Ottoetal.,2023) 100isachieved.Weagainmakeuseoftheadvantagethatwe
as obtainthewholedesiredtrajectoryaheadoftheenvironment
(cid:40)
−τ t<T, interaction,suchthatwecancollectsomesampleswithout
R tot = t physical simulation. The reward function based on this
−τ −200R −10R t=T,
t goal vel desiredtrajectoryisdefinedas
where
(cid:88)
r = |τd|−|qb|, (i,j)∈{(i,j)||τd|>|qb|}
R =∥p−p ∥ traj ij j ij j
goal goal 2
(i,j)
and
whereτd isthedesiredtrajectory,iisthetimeindex,j is
K thejointindex, qb isthejointpositionupperbound. The
(cid:88)
τ t = (ai t)2. desiredtrajectoryisconsideredasinvalidifr traj < 0,an
i invalid trajectory will not be executed on the robot. Ad-
ditionally,weprovideapunishment,iftheagentsamples
The sparse reward only returns the task reward in the
invaliddurationtimes
last time step T and additionally adds a velocity penalty
R = (cid:80)K(q˙i )2. The joint velocities are denoted asq˙.
vel i T r dur =−3(max(0,t d−td,max)+max(0,t d,mint−t d)),
Thisvelocitypenaltyavoidsovershootinginthelasttime
step.
where t = 1.7s,t = 0.45s and t is the dura-
d,max d,min d
HyperparametersarelistedintheTable3. tioninsecondschosenbytheagent. Theoverallrewardis
16AcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
Figure8: AdditionalDiverseSkillsfortheBoxPushObstacleTasklearnedbyDi-SkilL.Wefixthecontextsandsample
expertswhichwesubsequentlyexecute. Thisleadstodiversebehaviorsinthemotionprimitiveparameterspaceθwhich
leadstodifferenttrajectoriesofthepushedboxonthetable.
20 1.9 0.5
Di-SkilL
18 1.8 0.4 BBRL
16 0.3 LinDi-SkilL
1.7
14 Di-SkilL Di-SkilL 0.2
BBRL 1.6 BBRL
12 LinDi-SkilL LinDi-SkilL 0.1
10 1.5 0
0 1 2 3 0 1 2 3 0 1 2 3
NumberofEpisodicSamples ·105 NumberofEpisodicSamples ·105 NumberofEpisodicSamples ·105
(a)IQMMeanReturnHJ (b)IQMMax.HeightJumpHJ (c)IQMGoalDistanceHJ
Figure9: AdditionalAnalysisoftheHopperJump(HJ)task.
definedas: Di-SkilLoptimizestheremainingtermsintheobjectiveof
thehopperjumptask(seeAppendixD),whichexplainsthis

r traj,−20(r traj +r dur)−5 ifinvalidduration, gap.
r = ortrajectory
r
, otherwise. F.Hyperparameters
task
HyperparametersarelistedintheTable7.
E.AdditionalEvaluations
We provide additional diverse skills to the Box Pushing
ObstacletaskinFig. 8. InFig. 10weprovideadditional
diversestrikestofixedball’sdesiredlandingpositions.
WeanalyzetheperformanceofDi-SkilLonthehopperjump
task in more detail. In Fig. 9a we observe that the mean
return is on par with BBRL, similar to the achieved goal
distance in Fig. 9c. However, there is a small gap in the
max height, where BBRL jumps slightly higher (see Fig.
9b. Giventhatthemeanreturnisonpar,onewouldexpect
thatthemaximumjumpheightisonparaswell. However,
17
nruteRnaeMMQI etaRsseccuSMQI ]m[.tsiDlaoGMQIAcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
Figure10: Di-SkilL’sDiverseSkillsfortheTT-Htask. Wefixedtheball’sdesiredlandingpositionandvariedtheserving
landingpositionandtheball’sinitialvelocity. Di-SkilLcanreturntheballindifferentstrikingtypes. Notethateachrow
representsadifferentdesiredballlandingposition.
addcomponenteveryiteration 1000
finetuneallcomponentseveryiteration 50
numbercomponentadds 1
numberinitialcomponents 1
numbertotalcomponents 20
numbertraj. samplespercomponentperiteration 200
α 0.0001
β 0.5
expertKL-bound 0.01
contextKL-bound 0.01
Table1: HyperparametersforSVSLonTT
18AcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
Di-SkilL BBRL
criticactivation tanh tanh
hiddensizescritic [8,8] [32,32]
initialization orthogonal orthogonal
lrcritic 0.0003 0.0003
optimizercritic adam adam
ciritcepochs 100 100
activationcontextdistribution tanh –
epochscontextdistribution 100 –
hiddensizescontextdistr [16,16] –
initialization orthogonal –
lrcontextdistribution 0.0001 –
optimizercontextdistr adam –
batchsizepercomponent 50 209
numbersamplesfromenvironmentdistribution 5000 –
numbersamplespercomponent 50 209
normalizeadvantages True True
expertactivateion tanh tanh
epochs 100 100
hiddensizesexpert [64] [32]
lrpolicy 0.0003 0.0003
covariancetype full full
alpha 0.001 –
beta 4 –
numbercomponents 5 –
covariancebound 0.005 0.001
meanbound 0.05 0.05
projectiontype KL KL
trustregioncoefficient 100 25
Table2: HyperparametersforDi-SkilLandBBRLonTT.
19AcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
Di-SkilL BBRL LinDi-SkilL PPO
criticactivation tanh tanh tanh tanh
hiddensizescritic [32,32] [32,32] [32,32] [32,32]
initialization orthogonal orthogonal orthogonal orthogonal
lrcritic 0.0003 0.0003 0.0003 0.0003
optimizercritic adam adam adam adam
ciritcepochs 100 100 100 10
activationcontextdistribution tanh – tanh –
epochscontextdistribution 100 – 100 –
hiddensizescontextdistr [16,16] – [16,16] –
initialization orthogonal – orthogonal –
lrcontextdistribution 0.0001 – 0.0001 –
optimizercontextdistr adam – adam –
batchsizepercomponent 25 240 25 512(32minibatches)
numbersamplesfromenvironmentdistribution 5000 – 5000 –
numbersamplespercomponent 25 240 25 16384
normalizeadvantages True True True True
expertactivateion tanh tanh – tanh
epochs 100 100 100 10
hiddensizesexpert [32,32] [64,64] – [32,32]
lrpolicy 0.0003 0.0003 0.0003 0.0003
covariancetype full full full diagonal
alpha 0.01 – 0.01 –
beta 8 – 8 –
numbercomponents 10 – 10 –
covariancebound 0.001 0.005 0.0005 –
meanbound 0.05 0.05 0.05 –
projectiontype KL KL KL –
trustregioncoefficient 100 25 100 –
discountfactor 1 1 1 1
Table 3: Hyperparameters for Di-SkilL, BBRL, LinDi-SkilL, and PPO on 5LR. We used all code-level optimization
(Engstrometal.,2020)neededforPPO.Theimplementationisbasedonthesourcecodefrom(Ottoetal.,2021).
20AcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
Di-SkilL BBRL LinDi-SkilL
criticactivation tanh tanh tanh
hiddensizescritic [8,8] [32,32] [8,8]
initialization orthogonal orthogonal orthogonal
lrcritic 0.0003 0.0003 0.0003
optimizercritic adam adam adam
ciritcepochs 100 100 100
activationcontextdistribution tanh – tanh
epochscontextdistribution 100 – 100
hiddensizescontextdistr [16,16] – [16,16
initialization orthogonal – orthogonal
lrcontextdistribution 0.0001 – 0.0001
optimizercontextdistr adam – adam
batchsizepercomponent 50 209 50
numbersamplesfromenvironmentdistribution 5000 – 5000
numbersamplespercomponent 50 209 50
normalizeadvantages True True True
expertactivateion tanh tanh –
epochs 100 100
hiddensizesexpert [128] [32,32] –
lrpolicy 0.0003 0.0003 0.0003
covariancetype full full full
alpha 0.001 – 0.001
beta 0.5 – 0.5
numbercomponents 10 – 10
covariancebound 0.005 0.0005 0.001
meanbound 0.05 0.05 0.05
projectiontype KL KL KL
trustregioncoefficient 100 25 100
Table4: HyperparametersforDi-SkilL,BBRL,andLinDi-SkilLfortheHardTableTennisTask(TT-H).
21AcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
Di-SkilL BBRL LinDi-SkilL
criticactivation tanh tanh tanh
hiddensizescritic [64,64] [64,64] [64,64]
initialization orthogonal orthogonal orthogonal
lrcritic 0.0001 0.0001 0.0001
optimizercritic adam adam adam
ciritcepochs 100 100 100
activationcontextdistribution tanh – tanh
epochscontextdistribution 100 – 100
hiddensizescontextdistr [16,16] – [16,16]
initialization orthogonal – orthogonal
lrcontextdistribution 0.0001 – 0.0001
optimizercontextdistr adam – adam
batchsizepercomponent 80 200 80
numbersamplesfromenvironmentdistribution 1000 – 1000
numbersamplespercomponent 80 200 80
normalizeadvantages True True True
expertactivateion tanh tanh –
epochs 100 100 100
hiddensizesexpert [32,32] [32,32] –
lrpolicy 0.0003 0.0003 0.0003
covariancetype full full full
alpha 0.01 – 0.01
beta 8 – 8
numbercomponents 3 – 3
covariancebound 0.005 0.05 0.005
meanbound 0.05 0.1 0.05
projectiontype KL KL KL
trustregioncoefficient 100 25 100
Table5: HyperparametersforDi-SkilL,BBRL,andLinDi-SkilLfortheHopperJumpTask(HJ).
22AcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
Di-SkilL BBRL LinDi-SkilL PPO
criticactivation tanh tanh tanh tanh
hiddensizescritic [32,32] [32,32] [32,32] [256,256]
initialization orthogonal orthogonal orthogonal orthogonal
lrcritic 0.0003 0.0003 0.0003 0.0001
optimizercritic adam adam adam adam
ciritcepochs 100 100 100 10
activationcontextdistribution tanh – tanh –
epochscontextdistribution 100 – 100 –
hiddensizescontextdistr [16,16] – [16,16] –
initialization orthogonal – orthogonal –
lrcontextdistribution 0.0001 – 0.0001 –
optimizercontextdistr adam – adam –
batchsizepercomponent 50 500 50 410(40minibatches)
numbersamplesfromenvironmentdistribution 5000 – 5000 –
numbersamplespercomponent 50 500 50 16384
normalizeadvantages True True True True
expertactivateion tanh tanh – tanh
epochs 100 100 100 10
hiddensizesexpert [64,64] [64,64 – [256,256]
lrpolicy 0.0003 0.0003 0.0003 0.0001
covariancetype full full full diagonal
alpha 0.01 – 0.0001 –
beta 64 – 64 –
numbercomponents 10 – 10 –
covariancebound 0.005 0.0005 0.001 –
meanbound 0.05 0.05 0.05 –
projectiontype KL KL KL –
trustregioncoefficient 100 25 100 –
discountfactor 1 1 1 1
Table6: HyperparametersforDi-SkilL,BBRL,LinDi-SkilL,andPPOforBoxPushingObstacletask(BPO).Weusedall
code-leveloptimization(Engstrometal.,2020)neededforPPO.Theimplementationisbasedonthesourcecodefrom(Otto
etal.,2021).
23AcquiringDiverseSkillsusingCurriculumReinforcementLearningwithMixtureofExperts
Di-SkilL BBRL LinDi-SkilL
criticactivation tanh tanh tanh
hiddensizescritic [32,32] [32,32] [32,32]
initialization orthogonal orthogonal orthogonal
lrcritic 0.0003 0.0003 0.0003
optimizercritic adam adam adam
ciritcepochs 100 100 100
activationcontextdistribution tanh – tanh
epochscontextdistribution 100 – 100
hiddensizescontextdistr [16,16] – [16,16]
initialization orthogonal – orthogonal
lrcontextdistribution 0.0001 – 0.0001
optimizercontextdistr adam – adam
batchsizepercomponent 50 500 50
numbersamplesfromenvironmentdistribution 5000 – 5000
numbersamplespercomponent 50 500 50
normalizeadvantages True True True
expertactivateion tanh tanh –
epochs 100 100 100
hiddensizesexpert [64,64] [128,128] –
lrpolicy 0.0003 0.0003 0.0003
covariancetype full full full
alpha 0.0001 – 0.0001
beta 1 – 1
numbercomponents 10 – 10
covariancebound 0.005 0.001 0.001
meanbound 0.05 0.05 0.01
projectiontype KL KL KL
trustregioncoefficient 100 25 100
Table7: HyperparametersforDi-SkilL,BBRL,andLinDi-SkilLfortheminigolftask.
24