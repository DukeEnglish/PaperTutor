On the Generalization Ability of Unsupervised Pretraining
Yuyang Deng Junyuan Hong Jiayu Zhou Mehrdad Mahdavi
Penn State University Michigan state university Michigan state university Penn State University
Abstract puter vision and natural language processing, as ev-
idenced by a rapidly increasing number of empirical
studies [Coates and Ng, 2012, Radford et al., 2015,
Recent advances in unsupervised learning
Sun et al., 2019, Dosovitskiy et al., 2020, Feichtenhofer
have shown that unsupervised pre-training,
et al., 2022, He et al., 2020, 2022, Devlin et al., 2018, followed by fine-tuning, can improve model
Chen et al., 2020]. In this learning paradigm, the goal
generalization. However, a rigorous under-
istolearnarepresentationfunctiononalarge,possibly
standing of how the representation function
unlabeleddatasetbyoptimizingacarefullydesignedun-
learned on an unlabeled dataset affects the
supervised learning objective. Then, using the learned
generalization of the fine-tuned model is lack-
representation, a task-specific classifier, such as the
ing. Existingtheoreticalresearchdoesnotad-
headofaneuralnetwork,istrainedonasmallin-house
equately account for the heterogeneity of the
dataset during the fine-tuning stage. This two-stage
distributionandtasksinpre-trainingandfine-
paradigm addresses the issue of small dataset size in
tuning stage. To bridge this gap, this paper
downstream tasks. While unsupervised pre-training
introduces a novel theoretical framework that
for transfer learning has experienced significant em-
illuminates the critical factor influencing the
pirical growth, a comprehensive understanding of the
transferability of knowledge acquired during
fundamental factors that influence the generalization
unsupervised pre-training to the subsequent
performance of fine-tuned models lags considerably be-
fine-tuningphase,ultimatelyaffectingthegen-
hind what has been empirically observed [Neyshabur
eralizationcapabilitiesofthefine-tunedmodel
et al., 2020].
on downstream tasks. We apply our theo-
retical framework to analyze generalization Most existing generalization bounds primarily rely on
bound of two distinct scenarios: Context En- notions such as distance between the weights of the
coder pre-training with deep neural networks pre-trained and fine-tuned models [Li and Zhang, 2021,
and Masked Autoencoder pre-training with Shachaf et al., 2021] or data-dependent measurements
deep transformers, followed by fine-tuning on suchasHessian[Juetal.,2022]throughPAC-Bayesian
a binary classification task. Finally, inspired analysis [Arora et al., 2018, Neyshabur et al., 2018] to
byourfindings,weproposeanovelregulariza- examine the performance of fine-tuned model. These
tionmethodduringpre-trainingtofurtheren- results inform the design of effective regularization
hances the generalization of fine-tuned model. methods [Li and Zhang, 2021, Ju et al., 2022] or in-
Overall, our results contribute to a better corporating consistent losses [Ju et al., 2022] in fine-
understanding of unsupervised pre-training tuningstagetoimprovethegeneralizationoffine-tuned
and fine-tuning paradigm, and can shed light model by mitigating issues such as overfitting caused
on the design of more effective pre-training by fine-tuning a large model on a small training set
algorithms. or instability due to label noise. These generalization
bounds, however, do not explicitly incorporate other
key factors that may govern the success of fine-tuning
1 Introduction
such as similarity between the pre-training (on which
a model is pre-trained) and target tasks [Shachaf et al.,
Unsupervised representation learning has achieved re-
2021] or task diversity [Tripuraneni et al., 2020], the
markable success in various domains, including com-
number of training samples and complexity of model
spaces utilized in each stage in a unified bound. For
Proceedings of the 27thInternational Conference on Artifi-
example, in real-world learning tasks, the pre-training
cial Intelligence and Statistics (AISTATS) 2024, Valencia,
and fine-tuning tasks may be conducted on completely
Spain. PMLR: Volume 238. Copyright 2024 by the au-
thor(s). different domains, and we usually employ some kind of
4202
raM
11
]GL.sc[
1v17860.3042:viXraOn the Generalization Ability of Unsupervised Pretraining
transformation on the pre-training data (i.e., adding study the utility of unsupervised representation
noise, rotating or masking), which further exacerbates learning and fine-tuning paradigm (Section 3)
the data heterogeneity. Consequently, a well-designed and derive the generalization bound for fine-
generalization theory is expected to take the data het- tuned model based on a pre-trained representa-
erogeneity into account [Yang et al., 2020]. In modern tion function (Section 4). We discover that the
transfer learning, different tasks can be conducted in generalization capability of model depends on
the pre-training and fine-tuning stages. For example, four key factors: Representation transferrability,
in a Masked Autoencoder (MAE) [He et al., 2022], a representation-induced Rademacher complexity,
regression task utilized during pre-training, while a domain heterogeneity, and generalization of the
classification task used for fine-tuning. Therefore, a pre-training task.
desiredtheoryshouldallowforflexibilityinchoosingdi-
• (Applications)Weapplyourtheorytoderivegener-
verse types of tasks in the pre-training and fine-tuning
alization bound of the pre-training with a context
stages which poses a challenge in formalizing the de-
encoder (CE) and a masked autoencoder (MAE)
sired guarantees.
with a transformer followed by a binary classifica-
Motivatedbytheaboveobservations,weaimatformal- tion fine-tuning task (Section 5). We show that,
izing and establishing general generalization bounds the pre-training tasks defined by regression loss
on unsupervised pre-training and fine-tuning paradigm are provably transferrable to downstream binary
that captures aforementioned factors in a unified man- classification task. In doing so, to our best knowl-
ner. We introduce the notion of representation trans- edge, we establish the first generalization analysis
ferrability to quantify how much knowledge can be of multi-layer transformer models with residual
transferred from unsupervised representation learning block.
stage to fine-tuned model, in the presence of task het- • (Algorithm)Inspiredbyourgeneralizationbounds,
erogeneity. We then establish a bound on the gen- we propose a novel Rademacher Representation
eralization capability of fine-tuned model composed Regularized algorithm, RadReg, for improved pre-
with pre-trained representation model that highlights training and provide convergence guarantees for
how representation-induced complexity and distribu- nonconvex objectives (Section 6). The experimen-
tion mismatch affects the generalization of fine-tuned tal results show that RadReg can learn better rep-
model. We instantiate our theory to the scenario of resentation than ℓ norm regularized training on
2
ContextEncoder[Pathaketal.,2016]withdeepneural downstreamtaskswithasmalldataset(Section7).
networks and Masked Autoencoder [Devlin et al., 2018,
He et al., 2022] with deep Transformer architectures
2 Additional Related Works
which highlights the relative merits of learning repre-
sentations. From a technical perspective, we establish
Theory of Transfer Learning A significant body
generalization bounds for multi-layer transformers, by
of work [Tripuraneni et al., 2020, Du et al., 2020] fo-
deriving the worst case covering number of hypothe-
cuses on the theoretical aspects of transfer learning
sis space by expanding upon the machinery that was
paradigm, trying to answer the question: why transfer
developed in [Edelman et al., 2022].
learning can work and what factors affect the learning
Sinceourtheoreticalanalysisrevealstherepresentation- performance? Tripuraneni et al. [2020] give the first
induced Rademacher complexity as one of the key fa- risk bound capturing task diversity among pre-training
cotors governing the capacity of the transfer learning, and fine-tuning stages as the key quantity affecting
it naturally motivates itself to be incorporated as a generalization which is also reflected in our generaliza-
regularizer during pre-training. Inspired by this obser- tion analysis. Xu and Tewari [2021] follow the setup
vation, we propose a novel Rademacher representation in[Tripuranenietal.,2020],andshowthateventhough
regularized algorithm, dubbed as RadReg, to enhance the model architectures used in representation learn-
the generalization capability of the fine-tuned model. ing and fine-tuning are different, the task diversity
We show that by utilizing unlabeled data from the remains bounded. Du et al. [2020] study few-shot
downstream task, we can effectively regularize the pre- representation learning, where it considers pre-training
trainedmodeltolearnrepresentationsthatentailbetter a linear representation function by solving the regres-
generalizationafterfine-tuning. Weproposeanefficient sion problem with squared loss (OLS) on a give large
algorithm to optimize the new objective and establish dataset, and then fine-tuning another linear predictor
its convergence on smooth nonconvex losses. on some target dataset. It shows that the generaliza-
tion will depend on the number of pre-training data
Contributions. Our main contributions are summa-
and fine-tuning data. Similar dependencies appear in
rized as follows:
the generalization bound obtained in our main theo-
• (Theory) We introduce a formal framework to rem. Zhang et al. [2023] study the general supervisedYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi
pretraining, and highlight the trade-off between the for fine-tuned and representation models, respectively.
intra and inter class diversity.
Unsupervised pre-training. We assume access to
Theory of Modern Unsupervised Representa- a raw pre-training data {x˜ }N drawn from an un-
i i=1
tion Learning. Recently, due to the rise of con- known, arbitrary distribution D over an instance do-
trastive learning [Chen et al., 2020] and masked train- main X such as images. To learn representations,
ing [Devlin et al., 2018, He et al., 2022], a line of one first transforms (e.g., masking, adding noise, ro-
studies are devoted to understanding the generaliza- tating, or other geometric transformation) unlabeled
tion capability or smaple complexity of these learn- data into z˜ = T (x˜ ) ∈ X and (self-generated) la-
i 1 i
ing paradigms [HaoChen et al., 2021, Arora et al., bel y˜ = T (x˜ ) ∈ Z using suitable transformers
i 2 i
2019b,WangandIsola,2020,Leeetal.,2021,Geetal., T : X (cid:55)→ X and T : X (cid:55)→ Z to generate the pre-
1 2
2023, Gouk et al., 2020, Ju et al., 2022]. Arora et al. training dataset U(cid:98) = {(z˜ i,y˜ i)}N i=1. For example, in
[2019b] presents a theoretical framework for studying pre-training with masking, our augmented data are
contrastivelearning,andshowsthatitprovablyreduces masked sentence/image, and self-generated labels are
the sample complexity of downstream tasks. HaoChen the masked part of data. We denote the transformed
et al. [2021] consider contrastive learning and estab- distribution over X × Z as U. We note that that
lishes the theory without conditional independence of marginal distribution U of U over instance space X is
X
positive data pairs. Wang and Isola [2020] prove that not necessarily same as D of raw data due to random-
contrastive learning optimizes for alignment and uni- ness in data transformation T . To learn the represen-
1
formity asymptotically. Zhang et al. [2022] establish tations, we consider a class of decoding and encoding
the connection of masked pre-training with contrastive pairs,whichiscloselyinspiredby[HazanandMa,2016],
learning over bipartite graphs. Lee et al. [2021] also and minimize the following empirical risk
consider a masking pre-training scenario, but contrary
1 (cid:88)
tothepresentwork,itassumesthelabelsaregenerated min L (g◦h):= ℓ(g◦h(z˜ ),y˜ ),
by a function of masked data plus Gaussian noise, and g∈G,h∈H U(cid:98) N (z˜i,y˜i)∈U(cid:98) i i
(1)
only focuses on the ERM model as a representation
function. A recent work [Ge et al., 2023] also examine where G ⊆{I (cid:55)→Z}andH⊆{X (cid:55)→I}arethemodel
the unsupervised pre-training framework, but the dif- spaces for encoder and decoder, respectively, where
ference to ours, they consider a maximum likelihood I denotes the latent space of representations, and ℓ
estimation as pre-training method, while we start from is the loss function used for pre-training, e.g., ℓ(g ◦
general pre-training task and instantiate it in modern h(z˜ ),y˜ )=∥g◦h(z˜ )−y˜ ∥2. Let gˆ and hˆ denote the
machine learning scenario such as Context Encoder i i i i 2
decoderandencoder(representationfunction)obtained
and MAE. Gouk et al. [2020] study the end-to-end
by solving (1). We define the following excess risk for
finetuing scenario, and find that the generalization of
pre-training task:
finetunedmodelwilldependonthedistancethatneural
network weights traveled away from pretrained model. E (gˆ,hˆ):=L (gˆ◦hˆ)− min L (g◦h)
U U U
They hence propose a distance regularization finetun- g∈G,h∈H
ingalgorithmandachievebetterperformance. Juetal. where L (g ◦ h) := E [ℓ(f ◦ h(z˜),y˜)] denotes
U (z˜,y˜)∼U
[2022] also study the entire model finetuning paradigm, the generalization ability of pre-training task realized
and derive a Hessian based generalization bound via by distribution U. We note that the learned decoder
PAC-Bayesian analysis. function gˆ may be discarded after pre-training. We
use h∗ =argmin min L (g◦h)∈H to denote
U h∈H g∈G U
optimal encoder for pre-training task.
3 A Formal Framework
Supervised fine-tuning. In fine-tuning stage, we
In this section we formalize unsupervised pre-training assume access to a labeled downstream dataset T(cid:98) =
followed by supervised fine-tuning problem that will {x ,y }n wherefeaturevectorx issampledbasedan
enable us to study the relative merits of various un- uni knoi wi n= ,1 arbitrary distribution Ti (possibly different
supervised representation learning approaches and ex- from D) on domain X, and its label y is generated
i
amine their utility on the generalization capability of based on a labeling function y = y(x ). The goal is
i i
downstream tasks. In the scenario of unsupervised to utilize the representation function hˆ obtained by
representation pre-training and fine-tuning on a down-
solving (1) to perform fine-tuning on the downstream
stream task, we are given two datasets: one, possibly
datasetT(cid:98) tolearnapredictionmodelfˆfromafunction
large, unlabeled pre-training dataset and a small la-
class F:
beleddataset. Thegoalistolearnmodelf◦hwhichis
composed of task-specific function f ∈F and represen- minR (f ◦hˆ):= 1 (cid:88) ϕ(f ◦hˆ(x ),y ), (2)
tationfunctionh∈H,whereF andHaremodelspaces f∈F T(cid:98) n (xi,yi)∈T(cid:98) i iOn the Generalization Ability of Unsupervised Pretraining
where ϕ is the loss function which is not necessarily We note that a similar notation is proposed in the
the same as the pre-training loss. analysis of multi-task learning [Hanneke and Kpotufe,
2022, Definition 4], to characterize the transferrability
Our goal is to rigorously analyze the generalization
from one task to another task. We emphasize that
capability of the final model which is the composition
represnetation transferability is the key to transfer
of two functions, i.e., fˆ◦hˆ where fˆis the solution of
the generalizability of pre-training model to fine-tuned
(2), by bounding the excess risk
model. Unlike the transfer ratio defined in previous
E (fˆ,hˆ)=R (fˆ◦hˆ)− min R (f ◦h). (3) works [Tripuraneni et al., 2020, Ge et al., 2023, Zhang
T T T
f∈F,h∈H et al., 2023], we have an exponent variable β, which
Here R (f◦h):=E [ϕ(f◦h(x),y(x))] denotes the allows the transferrability from losses with different
T x∼T
true risk on downstream task realized by distribution order, e.g., from a quadratic loss to non-quadratic loss.
T over X and underlying labeling function y(·). Lateron,weshowthatconditionholdsessentiallyunder
realistic assumptions on suitable data transformations
to generate pre-training data and model spaces, such
4 On the Utility of Unsupervised
as pre-training with a inpainting autoencoder and a
Representation Learning
masked autoencoder with a transformer, where both
are fine-tuned on a classification task.
We now turn to establishing the generalization bound
of fine-tuned models given a pre-trained representation The next theorem establishes the generalization bound
function, and discuss its implications. Before, we first ofthefine-tunedmodelonadownstreamdataset,given
introduce two key notions. We start by introducing a pre-trained representation function hˆ.
the notion of Rademacher complexity of a hypothesis
Theorem 1. Assume hˆ and gˆ are the pre-trained rep-
space when individual models are composed with a
resentationfunctionanditsassociateddecoderfunction,
fixed representation function (similar measures appear
and real valued non-negative loss ϕ to be G Lipschitz
in [Tripuraneni et al., 2020, Xu and Tewari, 2021]). ϕ
and bounded by B . Assume pre-training and fine-
ϕ
Definition 1 (Representation-inducedRademachercom- tuning task admit (C ,β) representation transferrabil-
β
plexity). ForahypothesisspaceF ofsetofreal(vector)- ity on hˆ and h∗. If we solve (2) to get fˆ, then with
U
valued functions defined over input space X and label probability at least 1−ν, the following statement holds
space Y, a loss function ϕ:Y×Y (cid:55)→R , and a dataset
+
T(cid:98) ={x i,y i}n i=1, the empirical Representation-induced E T(fˆ,hˆ)≤C βE U(gˆ,hˆ)β +4G ϕR T(cid:98)(F ◦hˆ)
Rademacher complexity of F with respect to ϕ and T(cid:98), (cid:114)
log(1/ν)
for a given representation function hˆ, is defined as +4B +4B ∥T −U ∥ +minE (f,h∗)
ϕ n ϕ X TV f∈F T U
R (ϕ◦F ◦hˆ)
:=T(cid:98)
E (cid:34) sup 1 (cid:88)n ε ϕ(f ◦hˆ(x ),y )(cid:35) , ow ph te ir me alh∗ U pre= -traa inrg inm gin rh e∈ pH rem sei nn tg a∈ tG ioL nU( fg un◦ cth io) n,is at nh de
ε∈{±1}n f∈F n i=1 i i i ∥P −Q∥ = sup |P(A) − Q(A)| denotes total
TV A∈Ω
variation distance between two distributions.
whereε ,...,ε arei.i.d.Rademacherrandomvariables
1 n
with P{ε =1}=P{ε =−1}=1/2.
i i The proof of Theorem 1 is deferred to Appendix B.1.
The following definition, relates the generalization of Theorem 1 shows that the generalization of the fine-
fine-tuned and representation models. tuned model depends on four quantities: i) Repre-
sentation transferrability, ii) Representation-induced
Definition 2 (Represnetation transferability). Given
Rademacher complexity, iii) domain heterogeneity and
two representation functions h,h′ ∈ H and a distri-
iv) generalization of the pre-training task.
bution U for pre-training data, we say a pre-training
taskandfine-tuningtasksatisfies(C ,β)transferability Representation transferrability is the key to connect
β
for some constant 0<C <∞,β >0 on h,h′, if the downstream generaliztion with pre-training generaliza-
β
following statement holds: tion. It is analogous to task diversity notion in the
multi-task learning works [Tripuraneni et al., 2020, Xu
minR (f ◦h)− minR (f′◦h′)
T T and Tewari, 2021], since they all measure how well the
f∈F f′∈F
knowledge can be transferred across different learning
(cid:18) (cid:19)β
≤C minL (g◦h)−minL (g′◦h′) stage. However, our notion is more powerful since we
β U U
g∈G g′∈G neither assume the pre-training and fine-tuning stage
where R (f ◦h):=E [ϕ(f ◦h(x),y(x))] denotes sharethesametypeoftask, e.g., bothbeingregression,
UX x∼UX
the risk realized by pre-training marginal data distribu- nor assume a generic nonlinear feature representation
tion U and downstream labeling function y(·). is shared across all tasks [Tripuraneni et al., 2020]. As
XYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi
we will see in the later section, with the help of rep- on the downstream binary classification task with data
resentation transferrability, we can show that encoder x ,...,x using the learnt encoder hˆ:
1 n
learnt by regression pre-training can be transferred to
downstream classification task. The representation-
minR (f ◦hˆ)=
1 (cid:88)n
ϕ(f(hˆ(x )),y ). (5)
induced Rademacher complexity will play a key role in f∈F T(cid:98) n i i
reflecting how well the learnt representation and F are i=1
coupled. Notice that this complexity is defined over The encoder-decoder architecture is defined as follows:
downstream data, and only over class F, which means
that in fine-tuning stage we only suffer from a smaller encoder: h(x)=σ(W ···σ(W x)),
L 1
complexity in learning. The price for learning with
decoder: g(h(x))=W h(x),
L+1
potential more complex encoder class H is paid in pre-
trainingtask. Thisobservationisconsistentwithaline where W ∈ Rm×d, W ,...,W ∈ Rm×m, and
1 2 L
of multi-task or transfer learning works [Tripuraneni W ∈Rd×m. In fine-tuning stage, we add a linear
L+1
et al., 2020, Du et al., 2020, Xu and Tewari, 2021, Ge headontopofencoderfunction,i.e.,f(h(x))=θ⊤h(x).
et al., 2023]. The hypothesis class for encoder is then defined as:
T teh rie zed som tha ein sth ae tt ise tr io cg ae ln he eit ty erote gr em ne∥ itT y − beU twX e∥ eT nV tc hh ea pra rec -- (cid:40) x(cid:55)→σ(W L···σ(W 1x)):∥W l∥≤W(l),(cid:41)
H:=
training task and the fine-tuning task. Generalization ∥W ∥ ≤B(l)
l 2,1
of the pre-training task also appears in the bound
which depends on the convergence of the optimization where W(l) and B(l) are upper bound on spectral and
algorithm, and the complexity of the representation (2,1) norms of weight matrices, respectively.
function classes G and H for decoder and encoder,
The decoder class is defined as:
respectively. The last term min E (f,h∗) is the
f∈F T U
downstreamriskevaluatedwithoptimalrepresentation (cid:40) (cid:41)
x(cid:55)→W x:∥W ∥≤W(L+1),
L+1 L+1
modelh∗,whichcharacterizethetaskheterogeneitybe- G := .
U ∥W ∥ ≤B(L+1)
tween pre-training and downstream stages. If the two L+1 2,1
tasks are well aligned, i.e., they share similar optimal
Generalization bound. The following lemma es-
representation, then this quantity is ignorable.
tablishes the representation transferrability of CE pre-
training to binary classification task. We need to make
5 The Power of Unsupervised the following assumption
Representation Learning Assumption 1 (Realizability). There exists g∗ ∈ G
and h∗ ∈H such that L (g∗◦h∗)=0.
U U U
We now proceed to establish generalization bounds
Remark 1. In Assumption 1 we assume that there
in two distinct settings by refining the generic result
exist optimal encoder and decoder that can perfectly
presented in the previous section.
realize pre-training task. This is reasonable if we con-
sider overparameterized model, e.g., deep neural net-
5.1 Pre-training with Context Encoder work. For example, in masked image reconstruction
pre-training, at the most cases, the remaining part of
The setting. We start by applying our theory to
image is enough for deep model to reconstruct the raw
the setting where inpainting task is considered as pre-
image [Pathak et al., 2016, He et al., 2022].
training task and binary classification as downstream
task. This learning paradigm is also known as Con- Lemma 1. Under Assumption 1, CE pre-training ad-
text Encoder (CE) [Pathak et al., 2016], where in pre- mits an
(cid:0) Ω(1),1(cid:1)
representation transferrability to
2
training stage, a deep neural network is trained by binary classification task.
reconstructing a random transformation of raw data
The proof of Lemma 1 is deferred to Appendix C.1.
(e.g, rotating, scaling, adding Gaussian noise or mask-
This lemma shows that generalization of a pre-training
ing) of a given image:
regressiontaskcanbeeffectivelytransferredtoadown-
stream binary classification task. Here the transfer
N
min L (g◦h):= 1 (cid:88) ∥g(h(z˜ ))−z ∥2 (4) exponent is 1, which implies that the generalization
g∈G,h∈H U(cid:98) N i i risk of downs2 tream task will be roughly square root
i=1
of pre-training generalization. To get the excess risk
to learn gˆ and hˆ. Then, we discard the decoder gˆ, and rate of downstream task, we need to derive the general-
use the rest layers as an encoder. A linear projection ization risk of neural network regression. The existing
head is added on top of encoder in fine-tuning stage, works [Cao and Gu, 2019, 2020, Arora et al., 2019a]On the Generalization Ability of Unsupervised Pretraining
mainly focus on classification task where the loss func- We consider an L-layer transformer as the pre-training
tion is Lipschitz, which is not the case in regression encoder model, and a linear projection layer as the
loss. Our technique is to generalize the seminal analy- pre-train decoder model, and a linear projection layer
sis in [Srebro et al., 2010] for smooth losses and scalar for binary classification as fine-tune model.
valued hypothesis classes to a vector valued hypothesis
class, i.e., neural network class in our case and borrow
encoder: h(X)=SA (SA (···SA (X))),
the standard neural network covering number result WL WL−1 W1
from [Bartlett et al., 2017] to conclude the proof. decoder: g(h(X))=(h(X))W D,
Theorem 2. Assume hˆ and fˆare the pre-trained rep-
resentation function and its associated decoder function where SA W(·) is a self-attention module parameter-
obtained by solving (4) and (5). Let Z˜ =[z˜ 1;...;z˜ N] izedbyW=(W V,W Q,W K,W FC1,W FC2), whichis
and X=[x ;...;x ] be pre-training and downstream defined as
1 N
data, then under Assumption 1 with probability at least
1−ν, the following statement holds:
SA (X)=α σ(ZW )W +Z,
W 2 FC1 FC2
E T(fˆ,hˆ) Z=(α 1A+X),
 (cid:118)  (cid:18) (cid:19)
(cid:113) (cid:117)(cid:13) (cid:13)2 (cid:16) (cid:17)3 1
≤O˜

s L+1∥X∥2 +(cid:117) (cid:116)(cid:13) (cid:13)Z˜(cid:13) (cid:13) s L+1 (cid:80)L l=+ 11ρ l 

A=softmax √ d KXW K(XW Q)⊤ XW V,
 n N 
where α ,α are some small constant, as used in prac-
(cid:115)  1 2
log(1)
tice [Noci et al., 2022]. We assume lth layer’s weights’
+4B ϕ nν +∥T −U X∥ TV+m f∈i FnE T(f,h∗ U), s ispe bc ot ura nl den dor bm yi Bs (b l)o .un Id ned dob wy nsW tre(l a) m, a tn ad sk( ,2 w,1 e) an go gr rm
e-
gate (sum) over all patches from encoder, add a linear
where s
=(cid:81)L+1W2(l),ρ
=B(l)/W(l) .
l l=1 l projection head θ on top of h(X) to make a scalar
The proof of Theorem 2 is deferred to Appendix C.3. output:
Here we achieve roughly O(C √(F) + C( √G◦H)) bound for
n N
downstream task where C(·) denotes the complexity downstream: f(h(X))=(1⊤h(X))θ.
of the set. The cost of learning the complex heavy-
weight encoder is incurred during the pre-training task,
whereas in the fine-tuning stage, we only endure the Generalization bound. The following lemma estab-
complexity of learning a lightweight classification head. lishes the representation transferrability of MAE with
a transformer pre-training to binary classification task.
5.2 Pre-training with masked autoencoder
Lemma 2. MAE pre-training admits an
(cid:0) Ω(1),1(cid:1)
with tranformer models 2
representation transferrability to binary classification
The setting. Here we apply our theory to explain the task
empirical success of masked autoencoder pre-training
methods [He et al., 2022]. In masked autoencoder The proof of Lemma 2 is deferred to Appendix D.1.
pre-training, taking vision tasks for example, we draw This implies that MAE pre-training with a multi-layer
a large set of images Z ,...,Z ∈ RK×d, and then transformer can be transfered to binary classification
1 N
randomly mask some patches of each image to get task, with a constant factor. The exponent is also 1/2.
Z˜ ,...,Z˜ ∈RK×d. Then an encoder-decoder model Toderivetheexcessriskboundofdownstreamtask,we
1 N
is trained by recovering the missing patches: need to find the generalization risk of transformer re-
gression,whichischaracterizedbythefollowinglemma.
g∈m G,i hn ∈HL U(cid:98)(g◦h):=
N1 (cid:88)N (cid:13)
(cid:13) (cid:13)g(h(Z˜ i))−Z
i(cid:13)
(cid:13)
(cid:13)2
F
(6) Lemma 3 (Generalization of MAE pre-training task).
i=1 Let gˆ,hˆ be the solution of (6), and Z˜ =[Z˜ ;...;Z˜ ]
[N] 1 N
to get gˆ and hˆ. Finally, we discard the decoder and is the concatenated pre-training data. Then under As-
only fine-tune a new head (e.g., linear projection layer) sumption 1 with probability at least 1−ν the following
on the downstream binary classification task with data statement holds:
X ,...,X using the encoder:
1 n
minR (f ◦hˆ)=
1 (cid:88)n
ϕ(f(hˆ(X )),y ). (7) E
(gˆ,hˆ)≤O(cid:32)
s2
(cid:13)
(cid:13)Z˜
(cid:13) (cid:13)2L (cid:88)+1 ρ
l +
log( ν1)(cid:33)
,
f∈F T(cid:98) n i i U L(cid:13) [N](cid:13) N N
i=1 l=1Yuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi
where of the second term. On the other hand, it could also
amplify the domain discrepancy, i.e., ∥T −U ∥ .
l X TV
s := (cid:89)(cid:0) α W2(j)+1(cid:1)(cid:0) W2(j)α K+1(cid:1) ,
l 2 1
6 Effective Learning via Rademacher
j=1
ρ :=O(cid:0) (α α W2(l)+α )2B2(l)ln(2d2)(cid:1) Representation Regularization
l 1 2 1
(cid:32)
α
W4(l)(cid:0)
s max ∥X
∥(cid:1)4(cid:33)
× K2+ 1 l−1 i∈[N] i As shown in Theorem 1, a significant quantity that
d K affects the generalization risk of downstream task is
+O(cid:0) α2W2(l)B2(l)(1+α2K2W2(l))ln(2dm)(cid:1) . R (ϕ◦F ◦hˆ), the Rademacher complexity of F given
2 1 T(cid:98)
learnt representation function hˆ. Here we devise an
algorithmtoleveragetheunlabeleddownstreamdatain
The proof of Lemma 3 is deferred to Appendix D.2.
the pre-training stage, to regularize the representation
The proof idea is similarto analysis of CEpre-training,
functionandfurtherimprovetheaccuracyoffine-tuned
where we connect local Rademacher complexity to the
model.
covering number of model class, and then use tech-
niques from [Srebro et al., 2010] to establish the gener- Let us first consider binary classification case with
alizationofasmoothloss,i.e. MSE.Attheheartofour binary label y ∈ {−1,+1}. The idea is that, in
i
proof is to carefully control the norm of stacked output the binary classification setting, the Rademacher com-
of transformer on N samples, so that the final covering plexity is independent of labels, and hence it can be
number does not scale with N, but only depends on precisely estimated by only unlabeled downstream
spectral norm of concatenated samples. We note that dataset. If we assume ϕ is G Lipschitz, then ac-
ϕ
a similar study [Edelman et al., 2022] also establishes cording to the contraction property of Rademacher
the capacity of transformers, but they do not consider complexity [Ledoux and Talagrand, 2013], we have:
residual blocks. (cid:104) (cid:105)
R (ϕ◦F ◦hˆ) ≤ G E sup 1 (cid:80)n σ f(hˆ(x )) ,
T(cid:98) ϕ σ f∈F n i=1 i i
We now proceed to determine the excess risk associ- where we can see that, due to the randomness of
ated with fine-tuning a transformer model on a binary Rademachervariables,theupperboundofRademacher
classification task. complexity can be estimated without knowing the ac-
Theorem 3. Assume hˆ and fˆare the pre-trained rep- tual labels {y i}n i=1. Hence, we can leverage the unla-
beled downstream data in the pre-training stage, to
resentation function and its associated decoder function
obtained by solving (6) and (7). Let Z˜ =[Z˜ ;...Z˜ ] regularizetherepresentationfunction. Thiscanbecast
[N] 1 N
as the following problem:
andX =[X ;...X ]bepre-traininganddownstream
[N] 1 N
data, then under Assumption 1, with probability at least
(cid:34) n (cid:35)
1−ν, the following statement holds: min L (g◦h)+λE sup 1 (cid:88) σ f(h(x )) ,
g∈H,h∈H U(cid:98) σ f∈F n i i
E (fˆ,hˆ) i=1
T
 (cid:118)  where λ is the regularization coefficient. The idea
(cid:113) (cid:117) (cid:13) (cid:13)2
 s L(cid:13) (cid:13)X [N](cid:13) (cid:13)2 (cid:117) (cid:116)s2 L(cid:13) (cid:13)Z˜ [N](cid:13) (cid:13) (cid:80)L l=1ρ l can also be generalized to multi-class classification,
≤O +  where we use a vector contraction lemma to esti-
 n N 
mate the upper bound of this complexity which we
discuss in Section 6.1. To estimate the expectation,
(cid:115) 
log(1) we sample B configurations of Rademacher variables
+4B ϕ nν +∥T −U X∥ TV+m f∈i FnE T(f,h∗ U), {σj = [σ 1j,...,σ nj]}B j=1. If we assume f and h are pa-
rameterized by v ∈ V and w ∈ W, respectively, we
have
where s ,ρ are constants as defined in Lemma 3.
l l
B (cid:34) n (cid:35)
The proof of Theorem 3 is deferred to Appendix D.3. λ (cid:88) 1 (cid:88)
min L (w)+ max R (v ,w;x ) (8)
OurobservationsaresimilartothoseintheCEscenario: w∈W U(cid:98) B
j=1
vj∈V n
i=1
j j i
since we train the encoder only on the pre-training
dataset, during downstream learning, we mainly con- where R (v,w;x ):=σjf (h (x )).
tendwiththeintricaciesofasmallermodelclass,which j i i v w i
resultsinthegeneralizationboundofO(C √(F)+C( √G◦H)). Optimization method. To solve the aforementioned
n N optimization problems, we adapt the celebrated SGDA
Meanwhile, it’s worth noting that the introduction of
algorithm [Lin et al., 2019] (Algorithm 1). At the
masking may potentially reduce the norm of the data,
beginning of each iteration, we first sample a batch of
(cid:13) (cid:13)2
denoted as (cid:13) (cid:13)Z˜ [N](cid:13)
(cid:13)
, thereby diminishing the influence n′ pre-training data {zt i}n i=′ 1, and then do mini-batchOn the Generalization Ability of Unsupervised Pretraining
Algorithm 1: RadReg: Rademacher Regularized
Given that complexity increases with respect to B, it
becomes crucial to have an appropriately sized sample
Pre-training
of Rademacher variable.
Input: Number of iterations T; regularization
parameter λ
Sample B configurations of Rademacher variables 6.1 Multi-class
{σ1,...,σB},
The proposed regularization idea can also be general-
Initialize v0 =0, ∀j ∈[B]
j ized to multi-class classification. If the model f(·) is a
for t=0,...,T −1 do vector-valued function, i.e., in multi-class classification,
Sample a batch of data from pre-training
f(h(x)):X (cid:55)→Ro, we can apply the following vector-
dataset {z˜t,...,z˜t }
1 n′ valued contraction lemma of Rademacher complexity:
Sample a batch of data from downstreaming
dataset {x˜t 1,...,x˜t n′} Lemma 4. [Maurer, 2016] Let ϕ(·,·):Ro (cid:55)→R be G ϕ-
for j =1,...,B do Lipschitz in the first argument, and f(·) : Rd′ (cid:55)→ Ro
vt+1 =vt +γ 1 (cid:80)n′ ∇ R (vt,wt;x˜t). be vector-valued function. Then, the following facts
j j n′ i=1 v j j i hold true for Rademacher complexity over F and any
# Dual variable update
h:X (cid:55)→Rd′:
end
(cid:34) n (cid:35)
w #−t η+ Rλ1
eB
p1=
r(cid:80)
ew
seB
jt n=−
t1
a(cid:104)η tinn
1
o1 ′′ n(cid:80)(cid:80)
mn
in i
=
o= ′′ d11 e∇∇
lw
uL pRU(cid:98) d(
j
aw
(
tvt ejt; ,z˜ wt i)
t;x˜t
i)(cid:105) E ε √fsu ∈Fp n1 (cid:88)
i=
(cid:34)1ε iϕ(f 1( (cid:88)h n(x i)),y i)
(cid:35)
≤ 2G E sup ε f(h(x )) ,
end ϕ εi n i i
f∈F
i=1
Output: wˆ uniformly sampled from {wt}T .
t=1
where ε ∈{−1,+1}o is Rademacher vector.
i
Now the empirical minimization problem becomes:
stochastic gradient descent:
1 (cid:88)n′ 1 (cid:88)B (cid:34) 1 (cid:88)n (cid:35)
wt+1 =wt−η ∇L (wt;z˜t) min L (w)+λ max (σj)⊤f(h(x )) .
n′ i=1 U(cid:98) i w∈W Uˆ B V∈V n i i
(cid:20) (cid:21) j=1 i=1
1 (cid:88)B 1 (cid:88)n′
−ηλ ∇ R (vt,wt;x˜t) .
B j=1 n′ i=1 w j j i Specifically, if the top layer is a linear projection layer,
i.e., f(h(x))=Vh(x), the objective is equivalent to:
To solving the inner max problem, we sample an-
other batch of n′ downstream (unlabeled) data, and B (cid:34) (cid:32) n (cid:33)(cid:35)
then we do one step mini-batch stochastic gradient min L (w)+λ1 (cid:88) maxtr V1 (cid:88) h(x )(σj)⊤ ,
w∈W Uˆ B V∈V n i i
ascent:vt+1 =vt +γ 1 (cid:80)n′ ∇ R (vt,wt;x˜t). j=1 i=1
j j n′ i=1 v j j i
Convergence analysis of RadReg. To estab- where tr(·) denotes the trace of a matrix. Here the
lish the convergence of RadReg on (8), we consider inner problem is convex and easy to solve with simple
the following primal function:Ψ(w) := L (w) + (stochastic) gradient ascent.
U(cid:98)
λ1 (cid:80)B (cid:2) max 1 (cid:80)n R (v ,w;x )(cid:3) . Then we
B j=1 vj∈V n i=1 j j i
follow [Lin et al., 2020] and consider the following 7 Experiments
Moreau envelope function:
In this section, we empirically evaluate the proposed
Definition 3 (Moreau Envelope). A function Ψ ρ(w) regularization method in improving the generalization
is the ρ-Moreau envelope of a function Ψ if Ψ ρ(w):= of unsupervised pre-training to downstream tasks. We
min w′∈W{Ψ(w′)+ 21 ρ∥w′−w∥2}. utilize the Masked AutoEncoder (MAE) [He et al.,
Theorem 4 (Convergence of RadReg with Linear Top 2022] as the base unsupervised pre-training method.
Layer, Informal). RadReg (Algorithm 1 converge to ϵ- We conduct experiments using 50,000 images from CI-
stationary point of Ψ (w) with gradient complexity FAR10dataset[Krizhevskyetal.,2009]forpre-training
1/4L
bounded by
O(cid:0) B/ϵ8(cid:1)
.
and 4,096 few-shot STL [Coates et al., 2011] samples
for finetuning. Since our regularization requires un-
The formal version of Theorem 4 as well as the proof is labeled data from the downstream task, but L2 and
deferred to Appendix E. We can see that the proposed non-regularizationmethodscannotleveragethosedata,
optimization algorithm can find an ϵ-stationary point for a fair comparison, we incorporate the fine-tuning
with at most O(cid:0) B/ϵ8(cid:1) stochastic gradient evaluations. data into a separate unsupervised loss with the sameYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi
Figure 1: Testing and training accuracy by epochs, averaged by three repetitions.
report the mean and standard deviations. We vary the
Reg. λ Final Acc Best Acc Train Acc
coefficient for our and L regularization and compare
None - 70.8 (0.2) 70.9 (0.3) 100 (0.) 2
the best test accuracy on fine-tuning. We observe that
10−4 70.7 (0.3) 70.7 (0.3) 100 (0.) our method can effectively improve the downstream
L 10−3 70.8 (0.6) 70.9 (0.5) 100 (0.) performance as early as in the pre-training stage with-
2
10−2 70.7 (0.6) 70.9 (0.5) 100 (0.) out using any labels. Compared to L regularization,
2
our method can achieve higher test accuracy.
10−5 71.5 (0.2) 71.8 (0.3) 100 (0.)
RadReg 10−4 71.5 (0.4) 71.6 (0.7) 100 (0.) In Figure 1, we show the learning curves by different
10−3 69.6 (0.6) 69.6 (0.5) 100 (0.) regularization strategies. Due to the large capacity
of the pre-trained ViT encoder, all methods can suffi-
(a) End-to-end fine-tuning
ciently fit the training set approaching 100% training
Reg. λ Final Acc Best Acc Train Acc accuracy, but the testing accuracy reaches the ceiling.
Our method can improve the best test accuracy by
None - 55.3 (0.1) 55.5 (0.0) 65.7 (0.2)
limiting the representation complexity as early as the
10−5 55.3 (0.1) 55.5 (0.0) 57.5 (0.1)
pre-training stage. Our method also improves the con-
L 2 10−4 55.9 (0.1) 56.0 (0.1) 58.2 (0.1) vergence rate at fine-tuning, when our method reaches
10−3 54.4 (0.0) 54.5 (0.0) 58.2 (0.1)
the71%testaccuracyatepoch80butthebestbaseline
10−5 56.8 (0.0) 56.9 (0.1) 65.7 (0.2) reaches the same accuracy after 200 epochs.
RadReg
10−4 26.8 (0.0) 27.0 (0.1) 40.6 (0.1)
8 Conclusion
(b) Linear fine-tuning
This paper establishes a generic learning bound in un-
Table 1: Evaluation of MAE. Average fine-tuning accu-
supervised representation pre-training and fine-tuning
racy is reported with its standard deviations in brack-
paradigm. We discover that the generalization de-
ets.
pendsonrepresentationtransferrbaility,representation-
formulation as the MAE loss: induced Rademacher complexity, task heterogeneity
and generalization of pre-training task. We apply our
minL (g◦h)+α·L (g◦h)+λR (F ◦h) (RadReg), theory to analyze the generalization of CE and MAE
g,h
U(cid:98) D(cid:98) T(cid:98)
pre-training. Motivated by our theory, we propose
minL (g◦h)+α·L (g◦h)+λ∥W∥2 (L ), Rademacher representation regularization, with a prov-
g,h
U(cid:98) D(cid:98) 2
able convergence guarantee. The experiments validate
minL (g◦h)+α·L (g◦h) (Non-regularized), the superiority of our algorithm. As a future direction,
g,h
U(cid:98) D(cid:98)
it would be interesting to expand our analysis to end-
where we assume h is parameterized by W and α is to-end model fine-tuning, where task specific head and
fixed as 0.01. Our proposed regularization will fur- encoder are jointly updated in fine-tuning stage.
ther leverage the data to control the complexity of
learnedrepresentations. Thedetailsofexperimentsare
Acknowledgement
included in Appendix F.
In Table 1, we compare our method to non-regularized The work of YD and MM was partially supported by
MAE training and the one with L regularization. We NSF CAREER Award #2239374 and NSF CNS Award
2
repeatthefine-tuningthreetimesbyrandomlyselecting #1956276. JZ was supported by NSF #IIS-2212174
4096 samples from the preset STL10 training set, and and IIS-1749940 and NIA #1RF1AG072449.On the Generalization Ability of Unsupervised Pretraining
References Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. Bert: Pre-training of deep bidi-
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and
rectional transformers for language understanding.
Yi Zhang. Stronger generalization bounds for deep
arXiv preprint arXiv:1810.04805, 2018.
nets via a compression approach. In International
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,
Conference on Machine Learning, pages 254–263.
Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
PMLR, 2018.
terthiner, Mostafa Dehghani, Matthias Minderer,
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Georg Heigold, Sylvain Gelly, et al. An image is
RuosongWang.Fine-grainedanalysisofoptimization worth 16x16 words: Transformers for image recog-
and generalization for overparameterized two-layer nition at scale. arXiv preprint arXiv:2010.11929,
neural networks. In International Conference on 2020.
Machine Learning, pages 322–332. PMLR, 2019a.
Simon S Du, Wei Hu, Sham M Kakade, Jason D Lee,
SanjeevArora,HrishikeshKhandeparkar,MikhailKho- and Qi Lei. Few-shot learning via learning the repre-
dak, Orestis Plevrakis, and Nikunj Saunshi. A the- sentation,provably.arXivpreprintarXiv:2002.09434,
oretical analysis of contrastive unsupervised repre- 2020.
sentation learning. arXiv preprint arXiv:1902.09229, Benjamin L Edelman, Surbhi Goel, Sham Kakade, and
2019b.
CyrilZhang.Inductivebiasesandvariablecreationin
Peter L Bartlett, Dylan J Foster, and Matus J Telgar- self-attention mechanisms. In International Confer-
sky. Spectrally-normalized margin bounds for neural ence on Machine Learning, pages 5793–5831. PMLR,
networks. Advances in neural information processing 2022.
systems, 30, 2017. Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and
KaimingHe.Maskedautoencodersasspatiotemporal
Olivier Bousquet. Concentration inequalities and em-
pirical processes theory applied to the analysis of learners. arXiv preprint arXiv:2205.09113, 2022.
learning algorithms. 01 2002. JiaweiGe,ShangeTang,JianqingFan,andChiJin. On
the provable advantage of unsupervised pretraining.
YuanCaoandQuanquanGu. Generalizationboundsof
arXiv preprint arXiv:2303.01566, 2023.
stochastic gradient descent for wide and deep neural
networks. Advances in neural information processing Henry Gouk, Timothy Hospedales, et al. Distance-
systems, 32, 2019. based regularisation of deep networks for fine-tuning.
In International Conference on Learning Represen-
Yuan Cao and Quanquan Gu. Generalization er-
tations, 2020.
ror bounds of gradient descent for learning over-
Steve Hanneke and Samory Kpotufe. A no-free-lunch
parameterized deep relu networks. In Proceedings
of the AAAI Conference on Artificial Intelligence, theoremformultitasklearning. TheAnnalsofStatis-
volume 34, pages 3349–3356, 2020.
tics, 50(6):3119–3143, 2022.
JeffZHaoChen,ColinWei,AdrienGaidon,andTengyu
TingChen,SimonKornblith,MohammadNorouzi,and
Ma. Provable guarantees for self-supervised deep
GeoffreyHinton. Asimpleframeworkforcontrastive
learning with spectral contrastive loss. Advances
learning of visual representations. In International
in Neural Information Processing Systems, 34:5000–
conference on machine learning, pages 1597–1607.
5011, 2021.
PMLR, 2020.
Elad Hazan and Tengyu Ma. A non-generative frame-
Adam Coates and Andrew Y Ng. Learning feature
work and convex relaxations for unsupervised learn-
representations with k-means. In Neural networks: ing. Advances in Neural Information Processing
Tricks of the trade, pages 561–580. Springer, 2012. Systems, 29, 2016.
Adam Coates, Andrew Ng, and Honglak Lee. An anal- Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and
ysis of single-layer networks in unsupervised feature RossGirshick. Momentumcontrastforunsupervised
learning. In Proceedings of the fourteenth interna- visual representation learning. In Proceedings of
tional conference on artificial intelligence and statis- the IEEE/CVF conference on computer vision and
tics,pages215–223.JMLRWorkshopandConference pattern recognition, pages 9729–9738, 2020.
Proceedings, 2011.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li,
Damek Davis and Dmitriy Drusvyatskiy. Stochastic Piotr Dollár, and Ross Girshick. Masked autoen-
model-based minimization of weakly convex func- coders are scalable vision learners. In Proceedings of
tions. SIAMJournalonOptimization,29(1):207–239, the IEEE/CVF Conference on Computer Vision and
2019. Pattern Recognition, pages 16000–16009, 2022.Yuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi
Haotian Ju, Dongyue Li, and Hongyang R Zhang. Ro- Alec Radford, Luke Metz, and Soumith Chintala. Un-
bustfine-tuningofdeepneuralnetworkswithhessian- supervised representation learning with deep con-
based generalization guarantees. In International volutional generative adversarial networks. arXiv
ConferenceonMachineLearning,pages10431–10461. preprint arXiv:1511.06434, 2015.
PMLR, 2022.
HassanRafique,MingruiLiu,QihangLin,andTianbao
Alex Krizhevsky, Geoffrey Hinton, et al. Learning Yang. Weakly-convexconcavemin-maxoptimization:
multiple layers of features from tiny images. 2009. Provable algorithms and applications in machine
Michel Ledoux and Michel Talagrand. Probability in learning. arXiv preprint arXiv:1810.02060, 2018.
BanachSpaces: IsoperimetryandProcesses. Springer Axel Ruhe. Perturbation bounds for means of eigen-
Science & Business Media, 2013. values and invariant subspaces. BIT Numerical
Jason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng
Mathematics, 10:343–354, 1970. URL https://api.
semanticscholar.org/CorpusID:122004897.
Zhuo. Predictingwhatyoualreadyknowhelps: Prov-
able self-supervised learning. Advances in Neural Gal Shachaf, Alon Brutzkus, and Amir Globerson. A
Information Processing Systems, 34:309–323, 2021. theoreticalanalysisoffine-tuningwithlinearteachers.
David A Levin and Yuval Peres. Markov chains and Advances in Neural Information Processing Systems,
34:15382–15394, 2021.
mixing times, volume 107. American Mathematical
Soc., 2017. Nathan Srebro, Karthik Sridharan, and Ambuj Tewari.
DongyueLiandHongyangZhang. Improvedregulariza- Smoothness, low noise and fast rates. Advances in
tionandrobustnessforfine-tuninginneuralnetworks. neural information processing systems, 23, 2010.
Advances in Neural Information Processing Systems, ChenSun,AustinMyers,CarlVondrick,KevinMurphy,
34:27249–27262, 2021. and Cordelia Schmid. Videobert: A joint model for
Tianyi Lin, Chi Jin, and Michael I Jordan. On gradi- video and language representation learning. In Pro-
ent descent ascent for nonconvex-concave minimax ceedings of the IEEE/CVF international conference
problems. arXiv preprint arXiv:1906.00331, 2019. on computer vision, pages 7464–7473, 2019.
Tianyi Lin, Chi Jin, and Michael Jordan. On gradi- Nilesh Tripuraneni, Michael Jordan, and Chi Jin. On
ent descent ascent for nonconvex-concave minimax the theory of transfer learning: The importance of
problems. In International Conference on Machine task diversity. Advances in neural information pro-
Learning, pages 6083–6093. PMLR, 2020. cessing systems, 33:7852–7862, 2020.
Andreas Maurer. A vector-contraction inequality for Tongzhou Wang and Phillip Isola. Understanding con-
rademacher complexities. In International Confer- trastive representation learning through alignment
ence on Algorithmic Learning Theory, pages 3–17. and uniformity on the hypersphere. In International
Springer, 2016. Conference on Machine Learning, pages 9929–9939.
PMLR, 2020.
BehnamNeyshabur,SrinadhBhojanapalli,andNathan
Srebro. A pac-bayesian approach to spectrally- Kan Wu, Jinnian Zhang, Houwen Peng, Mengchen
normalized margin bounds for neural networks. In Liu, Bin Xiao, Jianlong Fu, and Lu Yuan. Tinyvit:
Fast pretraining distillation for small vision trans-
International Conference on Learning Representa-
tions, 2018. formers. In 17th European Conference Computer
Vision–ECCV 2022:, pages 68–85. Springer, 2022.
BehnamNeyshabur,HanieSedghi,andChiyuanZhang.
What is being transferred in transfer learning? Ad- Ziping Xu and Ambuj Tewari. Representation learning
vances in neural information processing systems, 33: beyondlinearpredictionfunctions. Advances in Neu-
512–523, 2020. ral Information Processing Systems, 34:4792–4804,
2021.
Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Anto-
nio Orvieto, Sidak Pal Singh, and Aurelien Lucchi. Fan Yang, Hongyang R Zhang, Sen Wu, Weijie J
Signal propagation in transformers: Theoretical per- Su, and Christopher Ré. Analysis of informa-
spectivesandtheroleofrankcollapse.arXivpreprint tion transfer from heterogeneous sources via pre-
arXiv:2206.03126, 2022. cise high-dimensional asymptotics. arXiv preprint
arXiv:2010.11750, 2020.
Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue,
TrevorDarrell,andAlexeiAEfros.Contextencoders: Jieyu Zhang, Bohan Wang, Zhengyu Hu, Pang Wei
Feature learning by inpainting. In Proceedings of Koh, and Alexander Ratner. On the trade-off of
the IEEE conference on computer vision and pattern intra-/inter-classdiversityforsupervisedpre-training.
recognition, pages 2536–2544, 2016. arXiv preprint arXiv:2305.12224, 2023.On the Generalization Ability of Unsupervised Pretraining
QiZhang,YifeiWang,andYisenWang.Howmaskmat-
ters: Towards theoretical understandings of masked
autoencoders. arXiv preprint arXiv:2210.08344,
2022.Yuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi
Checklist (a) The full text of instructions given to partici-
pants and screenshots. [Not Applicable]
1. For all models and algorithms presented, check if
(b) Descriptions of potential participant risks,
you include:
withlinkstoInstitutionalReviewBoard(IRB)
(a) A clear description of the mathematical set- approvals if applicable. [Not Applicable]
ting, assumptions, algorithm, and/or model. (c) The estimated hourly wage paid to partici-
[Yes] pants and the total amount spent on partici-
(b) An analysis of the properties and complexity pant compensation. [Not Applicable]
(time, space, sample size) of any algorithm.
[Yes]
(c) (Optional) Anonymized source code, with
specification of all dependencies, including
external libraries. [Not Applicable]
2. For any theoretical claim, check if you include:
(a) Statements of the full set of assumptions of
all theoretical results. [Yes]
(b) Complete proofs of all theoretical results.
[Yes]
(c) Clear explanations of any assumptions. [Yes]
3. For all figures and tables that present empirical
results, check if you include:
(a) Thecode,data,andinstructionsneededtore-
producethemainexperimentalresults(either
in the supplemental material or as a URL).
[Yes]
(b) All the training details (e.g., data splits, hy-
perparameters, how they were chosen). [Yes]
(c) A clear definition of the specific measure or
statistics and error bars (e.g., with respect to
the random seed after running experiments
multiple times). [Yes]
(d) A description of the computing infrastructure
used. (e.g., type of GPUs, internal cluster, or
cloud provider). [No]
4. If you are using existing assets (e.g., code, data,
models) or curating/releasing new assets, check if
you include:
(a) Citations of the creator If your work uses
existing assets. [Yes]
(b) The license information of the assets, if appli-
cable. [Not Applicable]
(c) Newassetseitherinthesupplementalmaterial
or as a URL, if applicable. [Not Applicable]
(d) Information about consent from data
providers/curators. [Not Applicable]
(e) Discussion of sensible content if applicable,
e.g., personally identifiable information or of-
fensive content. [Not Applicable]
5. If you used crowdsourcing or conducted research
with human subjects, check if you include:On the Generalization Ability of Unsupervised Pretraining
Organization The appendix is organized as follows. In Appendix A we will introduce some helper inequalities
that we will be utilized in our proofs and prove the main generalization theorem in Appendix B. In Appendices C
and D we will provide the proofs in Section 5.1 (generalization of pre-training with a context encoder) and
Section 5.2 (generalization of pre-training with masked autoencoder with a transformer), respectively. In
Appendix E, we provide the proof of convergence of the proposed algorithm in Section 6. At last, in Appendix F
we will provide the details of setup for our experiments.
A Basic Inequalities
In this section, we provide some general technical results that will be used in our proofs.
Proposition 1 (Total variation distance and L distance). [Levin and Peres, 2017, Proposition 4.2] Given two
1
probability measures P and Q defined over instance space X, the following inequality holds:
1 (cid:88)
∥P −Q∥ = |P(x)−Q(x)|.
TV 2
x∈X
Proposition 2 (Ruhe’s trace inequality). [Ruhe, 1970] If A and B are positive semidefinite Hermitian matrices
with eigenvalues,
a ≥...≥a ≥0, b ≥...≥b ≥0, (9)
1 n 1 n
repsectively, then
n n
(cid:88) (cid:88)
a b ≤tr(AB)≤ a b .
i n−i+1 i i
i=1 i=1
B Proof of Main Generalization Theorem
In this section we provide the proof of main result on generalization of fine-tuned model composed with an
unsupervised pre-trained model stated in Theorem 1. For readability purposes, we re-state the theorem here:
Theorem 5 (Theorem 1 restated). Assume hˆ and gˆ are the pre-trained representation function and its associated
decoder function, and real valued non-negative loss ϕ to be G Lipschitz and bounded by B . Assume pre-training
ϕ ϕ
and fine-tuning task admit a (C ,β) representation transferrability on hˆ and h∗ . If we solve (2) to get fˆ, then
β U
with probability at least 1−ν, the following statement holds
(cid:114)
(cid:16) (cid:17)β log(1/ν)
E (fˆ,hˆ)≤C E (gˆ,hˆ) +4G R (F ◦hˆ)+4B +4B ∥T −U ∥ +minE (f,h∗),
T β U ϕ T(cid:98) ϕ n ϕ X TV f∈F T U
where h∗ =argmin min L (g◦h) is the optimal pre-training representation function, and ∥P −Q∥ =
U h∈H g∈G U TV
sup |P(A)−Q(A)| denotes total variation distance between two distributions.
A∈Ω
B.1 Proof of Theorem 1
Proof. For the ease of presentation we define
f∗(h)=argminR (f ◦h):=E [ϕ(f ◦h(x),y(x))].
T T x∼T
f∈F
That is, the optimal fine-tuned risk minimizer in function class F w.r.t. distribution T over domain, given a
representation function h, which denotes the optimal risk minimizer for downstream task with labeling function
y(·), for a given representation function. Also, recall h∗ = argmin min L (g◦h) denotes the optimal
U h∈H g∈G U
pre-training representation function.
By standard risk decomposition we have:Yuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi
E (fˆ,hˆ)=R (fˆ◦hˆ)− min R (f ◦h)
T T T
f∈F,h∈H
=R (fˆ◦hˆ)−minR (f ◦hˆ)+minR (f ◦hˆ)− min R (f ◦h)
T T T T
f∈F f∈F f∈F,h∈H
=R (fˆ◦hˆ)−minR (f ◦hˆ)
T T
f∈F
(cid:124) (cid:123)(cid:122) (cid:125)
I
+minR (f ◦hˆ)−minR (f ◦h∗)
f∈F
UX
f∈F
UX U
(cid:124) (cid:123)(cid:122) (cid:125)
II
(cid:18) (cid:19)
+ minR (f ◦hˆ)−minR (f ◦hˆ)
f∈F
T
f∈F
UX
(cid:124) (cid:123)(cid:122) (cid:125)
III
(cid:18) (cid:19)
− min R (f ◦h)−minR (f ◦h∗)
f∈F,h∈H
T
f∈F
UX U
(cid:124) (cid:123)(cid:122) (cid:125)
IV
We now turn to bounding each term in RHS of above inequality.
Bounding I. The term I can be bounded by following standard results in uniform convergence and noting the
fact that fˆis empirical risk minimizer of downstream task by fixing the pre-training representation function hˆ:
I=R (fˆ◦hˆ)−minR (f ◦hˆ)
T T
f∈F
=R (fˆ◦hˆ)−R (fˆ◦hˆ)+R (fˆ◦hˆ)−R (f∗(hˆ)◦hˆ)+R (f∗(hˆ)◦hˆ)−minR (f ◦hˆ)
T T(cid:98)
(cid:124)
T(cid:98) (cid:123)(cid:122)T(cid:98) T
(cid:125)
T(cid:98) T
f∈F
T
≤0
(cid:114)
log(1/ν)
≤4R (ϕ◦F ◦hˆ)+4B .
T(cid:98) ϕ n
Bounding III. To bound III, we define f∗(h) = argmin R (f ◦hˆ), where R (f ◦h) := E [ϕ(f ◦
U f∈F UX UX x∼UX
h(x),y(x))] denotes the risk realized by pre-training marginal data distribution U and downstream labeling
X
function y(·) (Definition 2). We have:
III=minR (f ◦hˆ)−minR (f ◦hˆ)≤R (f∗(hˆ)◦hˆ)−R (f∗(hˆ)◦hˆ)
f∈F
T
f∈F
UX T U UX U
=E [ϕ(f∗(hˆ)◦hˆ(x),y)]−E [ϕ(f∗(hˆ)◦hˆ(x),y)]
x∼T U x∼U U
= (cid:88) |T(x)−U (x)|·ϕ(f∗(hˆ)◦hˆ(x),y)
X U
x∈X
(cid:88)
≤B |T(x)−U (x)|
ϕ X
x∈X
≤2B ∥T −U ∥ .
ϕ X TV
where the last step follows from Proposition 1.
Bounding IV. For IV, recalling that f∗(h)=argmin R (f ◦h), and we have
T f∈F T
IV=minR (f ◦h∗)− min R (f ◦h)
f∈F
UX U
f∈F,h∈H
T
=minR (f ◦h∗)−minR (f ◦h∗)+minR (f ◦h∗)− min R (f ◦h)
f∈F
UX U
f∈F
T U
f∈F
T U
f∈F,h∈H
T
≤R (f∗(h∗)◦h∗)−R (f∗(h∗)◦h∗)+minR (f ◦h∗)− min R (f ◦h)
UX T U U T T U U
f∈F
T U
f∈F,h∈H
T
≤2B ∥T −U ∥ +minE (f,h∗)
ϕ X TV T U
f∈FOn the Generalization Ability of Unsupervised Pretraining
where at last step we use the same reasoning we used in bounding III, and the definition of E (·).
T
Bounding II. It remains to bound II. Under the representation transferability assumption, we know
II=minR (f ◦hˆ)−minR (f ◦h∗)
f∈F
UX
f∈F
UX U
(cid:18) (cid:19)β
≤C minL (g◦hˆ)−minL (g◦h∗)
β U U U
g∈G g∈G
(cid:18) (cid:19)β
≤C L (gˆ◦hˆ)−minL (g◦h∗)
β U U U
g∈G
(cid:18) (cid:19)β
=C L (gˆ◦hˆ)− min L (g◦h)
β U U
g∈G,h∈H
(cid:16) (cid:17)β
=C E (gˆ,hˆ) .
β U
where the last step follows from the definition of E (·).
U
Putting pieces I-IV together yields:
(cid:114)
(cid:16) (cid:17)β log(1/ν)
E (fˆ,hˆ)≤C E (gˆ,hˆ) +4G R (F ◦hˆ)+4B
T β U ϕ T(cid:98) ϕ n
+4B ∥T −U ∥ +minE (f,h∗),
ϕ X TV T U
f∈F
thus leading to the desired generalization bound stated in Theorem 1.
As mentioned earlier, to instantiate Theorem 1 to a particular application, we need to establish bounds on
representation transferrability, generalization of pre-training task, and representation-induced Rademacher
complexity as we demonstrate on two specific pre-training tasks. We note that similar notions to representation
transferrability were proposed in [Tripuraneni et al., 2020, Ge et al., 2023, Du et al., 2020, Zhang et al., 2023], but
they do not have exponent in definition, so cannot capture the transferrability when pre-training and downstream
tasklossesarenothomogeneous. Thetermmin E (f,h∗)characterizeshowwelltheoptimalpre-trainingtask
f∈F T U
encoder is when applied on downstream task. It will depend on specific pre-training and downstream distribution.
Since we do not make distributional assumption, analyzing this term is beyond the scope of this paper.
C Proof of Generalization for Pre-training with Context Encoder
In this section we prove the results on generalization of pre-training with Context Encoder (CE) and fine-tuning
on binary classification as downstream task provided in Subsection 5.1. Recall during pre-training, we draw a
set of unlabeled data,e.g., images {z ,...,z }, and corrupt these data to make {z˜ ,...,z˜ }, then a deep neural
1 N 1 N
network is trained by reconstructing the corrupted pixel of a given image. The encoder-decoder architecture is
defined as follows:
encoder: h(x)=σ(W ···σ(W x)),
L 1
decoder: g(h(x))=W h(x).
L+1
where W ∈ Rm×d, W ,...,W ∈ Rm×m, and W ∈ Rd×m (for simplicity we assume the hiddent layers
1 2 L L+1
share the same dimension m). We assume each layer’s weight is with bounded norm: ∥W ∥≤W(l), ∥W ∥ ≤
l l 2,1
B(l),∀l∈[L+1]. The hypothesis class for encoder is then defined as:
(cid:110) (cid:111)
H:= x(cid:55)→σ(W ···σ(W x)):∥W ∥≤W(l),∥W ∥ ≤B(l),∀l∈[L]
L 1 l l 2,1
and decoder class is defined as:
(cid:110) (cid:111)
G := x(cid:55)→W x:∥W ∥≤W(L+1),∥W ∥ ≤B(L+1) .
L+1 L+1 L+1 2,1Yuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi
In pre-training stage we optimize the following empirical unsupervised losses:
N
min L (g◦h):=
1(cid:88)
∥g(h(z ))−z ∥2, (10)
g∈G,h∈H U(cid:98) 2 (cid:101)i i
i=1
where z˜ =T (z ), and T :X (cid:55)→X is some random transformation, e.g, rotating, scaling, adding Gaussian noise
i 1 i 1
or masking pixels.
After pre-training, we discard the top layer of the network, and use the rest layers as an encoder. A linear
projection head is added on top of encoder in downstream training:
downstream model: f(hˆ(x))=θ⊤hˆ(x),
with ∥θ∥≤R, and we assume that only the linear head is trainable during fine-tune stage. We optimize a binary
classification task with Lipschitz loss function as fine-tuning task:
min R (θ◦hˆ)= 1 (cid:88)n ϕ(θ⊤h(x ),y ),
∥θ∥≤R T(cid:98) n i=1 i i
to get fˆ, where y ∈{−1,+1} is binary labeling function for downstream task.
i
Roadmap. We will provide proof of Theorem 2 in the following subsections. The roadmap is as follows:
in Appendix C.1 we first show that the CE pre-training admits bounded representation transferrability to
downstream task (the proof of Lemma 1), and then in Appendix C.2 we prove the generalization of CE pre-
training task (Lemma 5), and finally in Appendix C.3 we conclude the proof for Theorem 2 by showing that the
representation-induced Rademacher complexity is bounded.
C.1 Proof of Transferability
In this subsection we provide the proof of Lemma 1. For notational convenience we define the following quantities:
∆ft(hˆ,h∗)= min E [ϕ(θ⊤hˆ(z˜))]− min E [ϕ(θ˜⊤h∗(z˜))],
U U
∥θ∥≤R
(z˜,z)∼U
∥θ(cid:101)∥≤R
z˜∼UX U
(cid:13) (cid:13)2 (cid:13) (cid:13)2
∆p Ut(hˆ,h∗ U)= Wm Li +n 1E (z˜,z)∼U(cid:13) (cid:13)W L+1hˆ(z˜)−z(cid:13)
(cid:13)
− W(cid:102)m Li +n 1E
z˜∼UX
(cid:13) (cid:13)W(cid:102)L+1h∗ U(z˜)−z(cid:13)
(cid:13)
(cid:16) (cid:17)β
To prove Lemma 1, we are going to show ∆ft(hˆ,h∗)≤C ∆pt(hˆ,h∗) holds for some C ,β.
U U β U U β
Upper bounding ∆ft(hˆ,h∗): We examine ∆ft(hˆ,h∗) first. We define the optimal head for classification task
U U U U
on distribution U under represetation h∗ as θ˜∗ =argmin E [ϕ(θ˜⊤h∗(z˜))].
X U ∥θ˜∥≤R z˜∼UX U
∆ft(hˆ,h∗)= min E [ϕ(θ⊤hˆ(z˜))]−E [ϕ(θ˜∗⊤h∗(z˜))]
U x∼U x∼U U
∥θ∥≤R
(cid:12) (cid:12)
≤ min E (cid:12)θ⊤hˆ(z˜)−θ˜∗⊤h∗(z˜)(cid:12)
(z˜,z)∼U(cid:12) U (cid:12)
∥θ∥≤R
(cid:114)
(cid:16) (cid:17)2
≤ min E θ⊤hˆ(z˜)−θ˜∗⊤h∗(z˜)
(z˜,z)∼U U
∥θ∥≤R
(cid:114)
(cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
= min θ⊤E hˆ(z˜)hˆ⊤(z˜) θ−2θ⊤E hˆ(z˜)h∗⊤(z˜) θ˜∗+θ˜∗⊤E h∗(z˜)h∗⊤(z˜) θ˜∗
U U U
∥θ∥≤R
Since (cid:112) f(x) and f(x) attain the minimum at the same point, we examine the minimum of θ⊤E(cid:104) hˆ(z˜)hˆ⊤(z˜)(cid:105) θ−
(cid:104) (cid:105) (cid:104) (cid:105)
2θ⊤E hˆ(z˜)h∗⊤(z˜) θ˜∗+θ˜∗⊤E h∗(z˜)h ∗⊤(z˜) θ˜∗ over θ. Under unconstrained setting, the minimum of above
U U U
statement is θ˜∗⊤Λθ˜∗On the Generalization Ability of Unsupervised Pretraining
(cid:16) (cid:104) (cid:105)(cid:17)† (cid:104) (cid:105)
when θ = E hˆ(z˜)hˆ⊤(z˜) E hˆ(z˜)h∗⊤(z˜) θ˜∗, and
U
(cid:104) (cid:105) (cid:104) (cid:105)(cid:16) (cid:104) (cid:105)(cid:17)† (cid:104) (cid:105)
Λ=E hˆ(z˜)hˆ⊤(z˜) −E h∗(z˜)hˆ⊤(z˜) E h∗(z˜)h∗⊤(z˜) E hˆ(z˜)h∗⊤(z˜) .
U U U U
Hence we have
(cid:112) (cid:113) (cid:113)
∆ft(hˆ,h∗)≤ θ˜∗⊤Λθ˜∗ = tr(Λθ˜∗⊤θ˜∗)≤ dσ (Λ)σ (θ˜∗⊤θ˜∗), (11)
U U max max
where we applied Ruhe’s Trace Inequalities at last step (Proposition 2): tr(AB) ≤ (cid:80)d σ (A)σ (B) ≤
i=1 i i
dσ (A)σ (B).
max max
Finally, we choose large enough R so that we can attain the optimum.
Lower bounding ∆pt(hˆ,h∗) Now we switch to lower bounding ∆pt(hˆ,h∗). We have:
U U
∆p Ut(hˆ,h∗ U)= min E (z˜,z)∼U(cid:13) (cid:13) (cid:13)W L+1hˆ(z˜)−z(cid:13) (cid:13) (cid:13)2 −E (z˜,z)∼U(cid:13) (cid:13)W L∗ +1h∗ U(z˜)−z(cid:13) (cid:13)2
WL+1:∥W∥≤W(L+1)
(cid:13) (cid:13)2
= min E (cid:13)W hˆ(z˜)−W∗ h∗(z˜)(cid:13)
(z˜,z)∼U(cid:13) L+1 L+1 U (cid:13)
WL+1:∥W∥≤W(L+1)
where the last step is due to our realizability Assumption 1, the optimal encoder-decoder exists in the hypothesis
class which can perfectly recover masked data.
Hence
(cid:13) (cid:13)2
∆pt(hˆ,h∗)= min E (cid:13)W hˆ(z˜)−W∗ h∗(z˜)(cid:13)
U U WL+1∈Rd×m (z˜,z)∼U(cid:13) L+1 L+1 (cid:13)
= min E
(cid:88)d (cid:13) (cid:13)w⊤hˆ(z˜)−w∗⊤h∗(z˜)(cid:13) (cid:13)2
(z˜,z)∼U (cid:13) r r (cid:13)
wr∈Rm,r∈[d]
r=1
≥(cid:88)d
min E
(cid:13) (cid:13)w⊤hˆ(z˜)−w∗⊤h∗(z˜)(cid:13) (cid:13)2
r=1wr∈Rm (z˜,z)∼U(cid:13) r r (cid:13)
d
≥(cid:88) min E (cid:16) w⊤hˆ(z˜)hˆ(z˜)⊤ w −2w⊤hˆ(z˜)h∗(z˜)⊤w∗+w∗⊤h∗(z˜)h∗(z˜)⊤w∗(cid:17) .
r=1wr∈Rm (z˜,z)∼U r r r U r r U U r
According to similar reasoning in the proof of upper bound, with Λ defined in the same way as (11), we have
d
∆pt(hˆ,h∗)≥(cid:88) w∗⊤Λw∗
U U r r
r=1
(cid:32) m (cid:33)
=tr Λ(cid:88) w∗w∗⊤
r r
r=1
(cid:32) d (cid:33)
≥σ (Λ)σ (cid:88) w∗w∗⊤
max min r r
r=1
where at last step we apply Ruhe’s trace inequality (Proposition 2)): tr(AB)≥σ (A)σ (B). Therefore, we
max min
can conclude that
 
(cid:113)
(cid:16) ∆∆ ptf U (t hˆ( ,hˆ h, ∗h )∗ U (cid:17)) 1/2 ≤O  (cid:114)
σ
dσ (cid:16)m (cid:80)ax d(θ˜∗ wθ˜∗ ∗⊤ w) ∗⊤(cid:17)  ,
U U min r=1 r rYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi
(cid:32) (cid:32) √ (cid:33) (cid:33)
which indicates that Context Encoder pretraining admits an Ω dσmax(θ˜∗θ˜∗⊤) ,1 representation
(cid:113) σmin((cid:80)d r=1w r∗w r∗⊤) 2
transferrability to binary classification task. In the main paper Lemma 1 we omit the constant dependency for
ease of exposition.
C.2 Proof of generalization of CE pretraining task
In this section we are going to derive generalization bound of the CE pre-training. The generalization is given in
the following lemma:
Lemma 5 (Generalization of pre-training task). Let gˆ,hˆ be the solution of (4), and Z˜ = [z˜ ;...;z˜ ] is the
1 N
concatenated pre-training data. Then with probability at least 0.99 the following statement holds:
(cid:18)(cid:13) (cid:13)2 (cid:19)(cid:16) (cid:17)(cid:16) (cid:17)3
(cid:13) (cid:13)Z˜(cid:13)
(cid:13)
ln(2m2) (cid:81)L l=+ 11W2(l) (cid:80)L l=+ 11( WB( (l l) ))32
E
U(gˆ,hˆ)≤O

 N

 .
To prove Lemma 5, we first introduce the following worst case covering number quantity:
Definition 4 (L covering number). Given a hypothesis class H and a set of data S = {x ,...,x }, let
2 1 N
h(X)=[h(x );...;h(x )] denote the concatenated output of N points.The the covering number N(H(S),ϵ,∥·∥) is
1 N
the least cardinality of set C, such that for every h∈H, there exists a h ∈C, and ensures that
ϵ
∥h(X)−h (X)∥≤ϵ.
ϵ
Definition 5 (L covering number). Given a hypothesis class H and a set of data S ={x ,...,x }, the worst
∞ 1 N
case covering number N (H(S),ϵ,∥·∥) is the least cardinality of set C, such that for every h∈H, there exists a
∞
h ∈C, and ensures that
ϵ
max∥h(x )−h (x )∥≤ϵ.
i ϵ i
i∈[N]
The following result will relate the Rademacher complexity of the local loss class induced by a hypothesis class H,
to the L covering number of H.
∞
Theorem 6 ([Srebro et al., 2010, Theorem 1]). Given a non-negative H-smooth loss ℓ bounded by b and a set of
data pairs S(cid:98)={(x i,y i)}N i=1, Define a local loss class L(r)=(cid:8) (x,y)(cid:55)→ℓ(h(x),y):h∈H,L S(cid:98)(h)≤r(cid:9) for some
0≤r <∞. Then , for all f ∈F simultaneously
 √ (cid:115) 
α (cid:90) br lnN ∞(H,√ ϵ ,∥·∥)
R (L(r))≤inf√ + 12Hr dϵ
S(cid:98) α N N
α
where the empirical Rademacher complexity of loss class is defined as
R (L(r))=E
(cid:34)
sup
(cid:12) (cid:12) (cid:12)1 (cid:88)n
ε ℓ(h(x ),y
)(cid:12) (cid:12) (cid:12)(cid:35)
. (ε ,...,ε i ∼id unif{±1})
S(cid:98) ε (cid:12)n i i i (cid:12) 1 n
h∈H,L S(cid:98)(h)≤r(cid:12) i=1 (cid:12)
The above theorem relates the complexity of loss class to the worst case spectral covering number of function
class, in our case, vector valued neural networks. Hence, it remains to find worst case (L ) covering number of
∞
our encoder class
G◦H:={x(cid:55)→W σ(W ···σ(W x)):∥W ∥≤W(l),∥W ∥ ≤B(l) ∀l∈[L+1]}. (12)
L+1 L 1 l l 2,1
Lemma 6 (Implication of [Bartlett et al., 2017, Theorem 3.3]). Given a set of data pairs S(cid:98)= {z˜ i}N i=1, and
hypothesis class defined in (12), then the following statement holds:
(cid:13) (cid:13)2 
(cid:13)Z˜(cid:13) ln(2m2) (cid:32)L+1 (cid:33)(cid:32)L+1 (cid:33)3
lnN ∞(G◦H(S),ϵ,∥·∥)≤lnN(G◦H(S),ϵ,∥·∥)≤ (cid:13) (cid:13) ϵ2   (cid:89) W2(l) (cid:88) ( WB( (l l) ))2 3 .
l=1 l=1
where Z˜ =[z˜ ;...;z˜ ].
1 NOn the Generalization Ability of Unsupervised Pretraining
Proof. We define g◦h(X)=[g(h(x ));...;g(h(x ))]∈RN×d. Notice the fact that 2-norm of a row of a matrix, is
1 N
always less than the spectral norm of the matrix:
max∥g◦h(x i)−g′◦h′(x i)∥≤ max (cid:13) (cid:13)(g◦h(X)−g′◦h′(X))⊤a(cid:13) (cid:13)=∥g◦h(X)−g′◦h′(X)∥,
i∈[N] ∥a∥≤1
hence we can have the following fact for covering numbers:
lnN (G◦H(S),ϵ,∥·∥)≤lnN(G◦H(S),ϵ,∥·∥). (13)
∞
At last plugging the bound for lnN(G◦H(S),ϵ,∥·∥) from [Bartlett et al., 2017] concludes the proof.
Equipped with above results, we are ready to show the local Rademacher complexity of loss class induced by
encoder-decoder function class G◦H:
Lemma 7. Given a hypothesis class H, if the logarithm of its L covering number lnN (G◦H(S),ϵ,∥·∥) is
∞ ∞
bounded by c , then the following bound for local Rademcaher complexity holds true:
ϵ2
(cid:114) cHr (cid:114) cHr (cid:32) √ (cid:32) 5(cid:114) cHr(cid:33)(cid:33)
R (L(r))≤10 +10 ln br−ln .
S(cid:98) N N 2 N
Proof. According to Theorem 6 we have
√ (cid:115)
(cid:90) br lnN ∞(H,√ ϵ ,N)
R (L(r))≤4α+10 12Hr dϵ
S(cid:98) N
α
(cid:90) √ br(cid:114) cHr
≤4α+10 dϵ
Nϵ2
α
(cid:114) cHr √
≤4α+10 (ln br−ln(α)).
N
(cid:113) √
Choosing α= 5 B2cHr = √5 cHr will minimize above bound, and yields:
2 N 2 N
(cid:114) cHr (cid:114) cHr (cid:32) √ (cid:32) 5(cid:114) Hr·c(cid:33)(cid:33)
R (L(r))≤10 +10 ln br−ln .
S(cid:98) N N 2 N
The following theorem connects local Rademacher complexity to population risk.
Theorem 7. [Bousquet, 2002, Theorem 6.1] Given a loss class L(r), let ϕ(r) be the function such that
R (L(r))≤ϕ(r).
S(cid:98)
then with probability at least 1−exp(−ν),
(cid:32) (cid:114) (cid:33)
L (h)≤L
(h)+45r∗+(cid:112)
L (h)
(cid:112)
8r∗ +
4b(log(1/ν)+6loglogN) +20b(ν+6loglogN)
S S(cid:98) S n N N
where r∗ is the largest solution such that ϕ(r)=r.
C.2.1 Proof of Lemma 5
(cid:13) (cid:13)2 (cid:16) (cid:17)(cid:16) (cid:17)3
Proof. First we evoke Lemma 7 with c=12(cid:13) (cid:13)Z˜(cid:13)
(cid:13)
ln(2m2) (cid:81)L l=+ 11W2(l) (cid:80)L l=+ 11( WB( (l l) ))32
(cid:114) Hr·c (cid:114) cHr (cid:32) √ (cid:32) 5(cid:114) Hr·c(cid:33)(cid:33)
R (L(r))≤10 +10 ln br−ln
S(cid:98) N N 2 N
(cid:114) (cid:114) (cid:32) (cid:114) (cid:33)
Hr·c cHr 2 bN
=10 +10 ln
N N 5 HcYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi
(cid:113) (cid:26) (cid:18) (cid:113) (cid:19)(cid:27)
We set ϕ(r)=10 Hr·c ·max 1,ln 2 bN . Solving the follwoing equation to get r∗
N 5 Hc
(cid:114) (cid:40) (cid:32) (cid:114) (cid:33)(cid:41)
Hr·c 2 bN
ϕ(r)=10 ·max 1,ln =r,
N 5 Hc
(cid:40) (cid:32) (cid:114) (cid:33)(cid:41)2
H ·c 2 bN
⇐⇒r∗ =100 ·max 1,ln
N 5 Hc
Now, according to Theorem 7, and the fact that
√ √
A≤B+C A=⇒A≤B+C2+ BC,
we have
(cid:32) (cid:114) (cid:33)2
√ 4b(log(1/ν)+6loglogN)
L (g◦h)≤L (g◦h)+45r∗+ 8r∗+
U U(cid:98) N
(cid:114) (cid:32) (cid:114) (cid:33)
b(ν+6loglogN) b(ν+6loglogN) √ 4b(log(1/ν)+6loglogN)
+20 + L (g◦h)+45r∗+20 8r∗+ .
N U(cid:98) N N
Plugging r∗, and empirical risk minimizers gˆ,hˆ will conclude the proof.
C.3 Proof of Theorem 2
Proof. Recall that in Theorem 1, the generalization bound is given by
(cid:114)
(cid:16) (cid:17)β log(1/ν)
E (fˆ,hˆ)≤C E (gˆ,hˆ)+µ +4G R (F ◦hˆ)+4B +4B ∥T −U ∥ +minE (f,h∗).
T β U ϕ T(cid:98) ϕ n ϕ X TV f∈F T U
Since in the previous subsection we prove the bounded transferrability and generalization of pre-training task, it
remains to show the upper bound of representation-induced Rademacher complexity. To this end, we have
(cid:34) (cid:35)
R (ϕ◦F ◦hˆ)=E sup 1 (cid:88)n ε ϕ(θ⊤hˆ(x ),y )
T(cid:98) ε∈{±1}n θ:∥θ∥≤Rn i=1 i i i
(cid:34) (cid:35)
≤RG E sup 1 (cid:88)n ε θ⊤hˆ(x )
ϕ ε∈{±1}n θ:∥θ∥≤Rn i=1 i i
= RG ϕE (cid:13) (cid:13)(cid:88)n ε hˆ(x )(cid:13) (cid:13)
n ε(cid:13) i=1 i i (cid:13)
(cid:114)
≤ RG ϕ E (cid:13) (cid:13)(cid:88)n ε hˆ(x )(cid:13) (cid:13)2
n ε(cid:13) i=1 i i (cid:13)
(cid:114)
= RG ϕ (cid:88)n (cid:13) (cid:13)hˆ(x )(cid:13) (cid:13)2
n i=1(cid:13) i (cid:13)
where at first inequality we apply Ledoux-Talagrand’s inequality to peel off Lipschitz loss ϕ(·), and at last
(cid:13) (cid:13)2
inequality we use the fact that ε are i.i.d. with zero mean, so that the cross terms disappear. For each (cid:13)hˆ(x )(cid:13) ,
i (cid:13) i (cid:13)
we have:
(cid:13)
(cid:13)hˆ(x
)(cid:13) (cid:13)2 ≤L (cid:89)+1
W2(l)∥x ∥2,
(cid:13) i (cid:13) i
l=1
hence we arrive at
(cid:113)
RG (cid:81)L+1W2(l)(cid:80)n ∥x ∥2
R (ϕ◦F ◦hˆ)≤ ϕ l=1 i=1 i .
T(cid:98) n
Plugging Lemmas 1 and 5 back into Theorem 1 as well as above bound will complete the proof of Theorem 2.On the Generalization Ability of Unsupervised Pretraining
D Proof of Pre-training with Masked Autoencoder with Tranformer Models
We turn to proving the generalization of pretraining with masked autoencoder (MAE) with tranformer models
(Section 5.2).
Recall, in MAE pre-training for vision tasks as an example, we draw a large set of images Z ,...,Z ∈RK×d,
1 N
and then randomly mask some patches of each image to get Z˜ ,...,Z˜ ∈RK×d. Then an encoder-decoder model
1 N
(cid:13) (cid:13)2
is trained by recovering the missing patches (e.g., by utilizing MSE loss ℓ(Zˆ,Z)=(cid:13)Zˆ −Z(cid:13) as pre-training loss).
(cid:13) (cid:13)
F
We will consider L-layer transformer as the pre-train encoder model, a single self-attention layer transformer as
the pre-train decoder model, and a linear projection layer for binary classification as fine-tune model.
Encoder Architecture In a L-layer transformer, given a input X, the lth layer’s output is define as:
(cid:40)
X, l=0
Xl =
SA (Xl−1), l=[L],
Wl
where SA (·) is the l-layer self attention module given a collection of weight matrices Wl =
Wl
(cid:0) Wl ,Wl ,Wl ,Wl ,Wl (cid:1) ∈Rd×d×Rd×dK ×Rd×dK ×Rd×m×Rm×d defined as:
V K Q FC1 FC2
SA (Xl−1)=α σ(cid:0) ZlWl (cid:1) Wl +Zl,
Wl 2 FC1 FC2
Zl =(cid:0) α Al+Xl−1(cid:1) ,
1
(cid:18) (cid:19)
1
Al =softmax √ XWl (XWl )⊤ XWl ,
d K Q V
K
where α ,α are some small constant, as used in practice [Noci et al., 2022]. We use the Lth layer’s output as the
1 2
final output of encoder, i.e., h(X)=XL.
encoder: h(X)=XL.
The hypothesis class of encoder is defined as:
 
X(cid:55)→SA (SA ...SA (X)):
H= WL
(cid:13)
(cid:13)WW
l
L−
(cid:13)
(cid:13)1
,(cid:13)
(cid:13)WlW1
(cid:13) (cid:13),(cid:13) (cid:13)Wl (cid:13) (cid:13),(cid:13) (cid:13)Wl (cid:13) (cid:13),(cid:13) (cid:13)Wl (cid:13) (cid:13)≤W(l),

. (14)
FC1 FC2 K Q V
 (cid:13) (cid:13)Wl (cid:13)
(cid:13)
,(cid:13) (cid:13)Wl (cid:13)
(cid:13)
,(cid:13) (cid:13)Wl (cid:13)
(cid:13)
,(cid:13) (cid:13)Wl (cid:13)
(cid:13)
,(cid:13) (cid:13)Wl (cid:13)
(cid:13)
≤B(l),∀l∈[L]
FC1 2,1 FC2 2,1 K 2,1 Q 2,1 V 2,1
Decoder Architecture When encoder finished processing masked sequence, we will send the encoder output
h(Z˜) to decoder. The decoder is a simple linear projection layer:
decoder: g(h(Z˜))=h(Z˜)WD,
To learn the representation model, we solve the following1:
1(cid:88)N (cid:13) (cid:13)2
g∈m G,i hn ∈HL U(cid:98)(g◦h):=
2
(cid:13) (cid:13)g(h(Z(cid:101)i))−Z i(cid:13)
(cid:13)
F, (15)
i=1
to get representation hˆ.
Then, in the fine-tuning stage for a binary classification tasks with labels y ∈ {−1,+1}, we consider a linear
i
model parameterized by θ
downstream model: f(h(X))=1⊤hˆ(X )θ,
i
1In some implementation of MAE pre-training, the MSE loss is not computed on full patches, but only the masked
patches. Itcanbeadaptedbychangingourobjectiveto 1 2(cid:80)N i=1(cid:13) (cid:13) (cid:13)A⊙(g(h(Z(cid:101)i))−Z i)(cid:13) (cid:13) (cid:13)2 whereA∈RK×d istheindicator
F
matrix with j row to be 1 if jth patch is masked, otherwise 0. This adaptation will not affect our analysis significantly.Yuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi
with classification loss ϕ(·,·) and optimize:
n
min R (θ◦hˆ(X))= 1 (cid:88) ϕ(1⊤hˆ(X )θ,y ),
∥θ∥ ≤R T(cid:98) n i i
2 i=1
to get fˆ(or θˆin this setting), the aggregated patch over all patches is used for linear projection in classification
task.
Roadmap.WewillprovideproofofTheorem3inthefollowingsubsections. TheroadmapisthatinAppendixD.1
wefirstshowtheMAEpre-trainingadmitsboundedrepresentationtransferrabilitytodownstreamtask(Lemma2),
and then in Appendix D.2 we prove the generalization of MAE pre-training task (Lemma 3). The heart of the
proof in this part is to derive worst case covering number of transformer class. Finally in Appendix D.3 we
conclude the proof for Theorem 3 by showing that the representation-induced Rademacher complexity is bounded.
D.1 Proof of Task Transferability of MAE
Similar to proof of DAE transferability, we define the following quantity:
∆ft(hˆ,h∗)= min E [ϕ(θ⊤(1⊤hˆ(Z˜))⊤)]− min E [ϕ(θ˜⊤(1⊤h∗(Z˜))⊤)],
U U (Z˜,Z)∼U (Z˜,Z)∼U U
∥θ∥≤R ∥θ(cid:101)∥≤R
(cid:13) (cid:13)2 (cid:13) (cid:13)2
∆pt(hˆ,h∗)= min E (cid:13)hˆ(Z˜)WD−Z(cid:13) −E (cid:13)h∗(Z˜)WD∗−Z(cid:13)
U U WD∈R (Z˜,Z)∼U(cid:13) (cid:13) F (Z˜,Z)∼U(cid:13) U (cid:13) F
where 1=[1,1,1,...]∈RK.
Upper bounding ∆ft(hˆ,h∗) We examine ∆ft(hˆ,h∗) first. Similar to DAE proof, We define the optimal head
U U U U
forclassificationtaskondistributionU underrepresentationh∗ asθ˜∗ =argmin E [ϕ(θ˜⊤(1⊤h∗(Z˜)))].
X U ∥θ˜∥≤R Z˜∼UX U
∆ft(hˆ,h∗)= min E [ϕ(θ⊤(1⊤hˆ(Z˜))⊤)]−E [ϕ(θ˜⋆⊤(1⊤h∗(Z˜))⊤)]
U U (Z˜,Z)∼U (Z˜,Z)∼U U
∥θ∥≤R
≤ min G E |(θ⊤(1⊤hˆ(Z˜))⊤)−(θ˜⋆⊤(1⊤h∗(Z˜))⊤)|
ϕ (Z˜,Z)∼U U
∥θ∥≤R
(cid:114)
(cid:16) (cid:17)2
≤ min G E θ⊤(1⊤hˆ(Z˜))⊤−θ˜⋆⊤(1⊤h∗(Z˜))⊤
ϕ (Z˜,Z)∼U U
∥θ∥≤R
= min
∥θ∥≤R
(cid:114)
(cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
G θ⊤E (1⊤hˆ(Z˜))⊤(1⊤hˆ(Z˜)) θ−2θ⊤E (1⊤hˆ(Z˜))⊤(1⊤h∗(Z˜)) θ˜⊤+θ˜⋆⊤E (1⊤h∗(Z˜))⊤(1⊤hˆ∗(Z˜)) θ˜
ϕ U U U
Since (cid:112) f(x) and f(x) attain the minimum at the same point, we examine the minimum of above state-
ment over θ without square root. Under unconstrained setting, the minimum of above statement is
(cid:16) (cid:17) (cid:16) (cid:104) (cid:105)(cid:17)† (cid:104) (cid:105)
θ˜⋆⊤Λ 1⊤hˆ(Z˜),1⊤h∗(Z˜) θ˜∗ when θ∗ = E (1⊤hˆ(X))⊤(1⊤hˆ(X)) E (1⊤hˆ(X))⊤(1⊤h∗(X)) θ˜∗. Hence we
U U
have
(cid:114) (cid:114)
(cid:16) (cid:17) (cid:16) (cid:17)
∆ft(hˆ,h∗)≤ θ˜⋆⊤Λ 1⊤hˆ(Z˜),1⊤h∗(Z˜) θ˜∗ = tr(Λ 1⊤hˆ(Z˜),1⊤h∗(Z˜) θ˜∗θ˜⋆⊤)
U U U U
(cid:114)
(cid:16) (cid:17)
≤ dσ (Λ 1⊤hˆ(Z˜),1⊤h∗(Z˜) )σ (θ˜∗θ˜⋆⊤),
max U max
where
(cid:16) (cid:17)
Λ 1⊤hˆ(Z˜),1⊤h∗(Z˜)
U
(cid:104) (cid:105) (cid:104) (cid:105)(cid:16) (cid:104) (cid:105)(cid:17)† (cid:104) (cid:105)
=E (1⊤h∗(Z˜))⊤(1⊤h∗(Z˜)) −E (1⊤hˆ(Z˜))⊤(1⊤h∗(Z˜)) E (1⊤hˆ(Z˜))⊤(1⊤hˆ(Z˜)) E (1⊤hˆ(Z˜))⊤(1⊤h∗(Z˜)) .
U U U U
At last, by choosing a properly large R, we can guarantee the optimum can be attained.On the Generalization Ability of Unsupervised Pretraining
Lower bounding ∆pt(hˆ,h∗) Similar to CE proof, we have:
U U
∆pt(hˆ,h∗)
U U
= WDm ∈i Rn d×dE (Z˜,Z)∼U(cid:13) (cid:13) (cid:13)hˆ(Z(cid:101))WD−Z(cid:13) (cid:13) (cid:13)2 F−E (Z˜,Z)∼U(cid:13) (cid:13) (cid:13)h∗ U(Z(cid:101))WD∗ −Z(cid:13) (cid:13) (cid:13)2
F
= WDm ∈i Rn d×dE
(Z˜,Z)∼U(cid:13)
(cid:13) (cid:13)hˆ(Z(cid:101))WD−h∗
U(Z(cid:101))WD∗(cid:13)
(cid:13)
(cid:13)2
F
≥(cid:88) i=d 1wm r∈in RdE (Z˜,Z)∼U(cid:13) (cid:13) (cid:13)hˆ(Z(cid:101))w r−h∗ U(Z(cid:101))w r∗(cid:13) (cid:13) (cid:13)2
d
=(cid:88) i=1wm r∈in RdE (Z˜,Z)∼U(cid:16) w r⊤hˆ(Z(cid:101))⊤hˆ(Z(cid:101))w r−2w r⊤hˆ(Z(cid:101))⊤h∗ U(Z(cid:101))w r∗+w r∗⊤h∗ U(Z(cid:101))⊤h∗ U(Z(cid:101))w r∗(cid:17)
where the second step is due to our realizability Assumption 1, w ∈Rd represents rth colum of WD and so is
r
w∗ ∈Rd represents rth colum of WD∗.
r
Similar to Context Encoder proof, we define Schur complement as:
(cid:16) (cid:17) (cid:104) (cid:105) (cid:104) (cid:105)(cid:16) (cid:104) (cid:105)(cid:17)† (cid:104) (cid:105)
Λ hˆ(Z˜),h∗(Z˜) =E (h∗(Z˜))⊤h∗(Z˜) −E (h∗(Z˜))⊤(hˆ(Z˜)) E (h∗(Z˜))⊤h∗⊤(Z˜) E (hˆ(Z˜))⊤h∗(Z˜) .
U U U U U U U
By computing the closed form solution of quadratic form we arrived at:
d
∆pt(hˆ,h∗)≥(cid:88) w∗⊤Λ(cid:16) hˆ(Z˜),h∗(Z˜)(cid:17)
w∗
U U r r
r=1
 
d
≥tr
Λ(cid:16) hˆ(Z˜),h∗(Z˜)(cid:17)(cid:88)
w j∗w j∗⊤ 
j=1
 
d
≥σ
max(cid:16) Λ(cid:16) hˆ(Z˜),h∗(Z˜)(cid:17)(cid:17)
σ
min(cid:88)
w j∗w j∗⊤ .
j=1
Recall that
(cid:114)
(cid:16) (cid:16) (cid:17)(cid:17)
∆ft(hˆ,h∗)≤ dσ Λ 1⊤hˆ(Z˜),1⊤h∗(Z˜) σ (θ˜∗θ˜⋆⊤).
U U max U max
Hence, we can conclude that
(cid:114) (cid:16) (cid:16) (cid:17)(cid:17) 
σ Λ 1⊤hˆ(Z˜),1⊤h∗(Z˜) (cid:118)
(cid:16) ∆∆ p Utf U (t hˆ( ,hˆ h, ∗ Uh )∗ U (cid:17)) 1/2 ≤O   (cid:114)ma σx max(cid:16) Λ(cid:16) hˆ(Z˜),h∗ U(Z˜U )(cid:17)(cid:17) (cid:117) (cid:117) (cid:116) σ mindσ (cid:16)m (cid:80)ax d j=(θ˜ 1∗ wθ˜⋆ j∗⊤ w) j∗⊤(cid:17)  ,
which indicates that MAE pre-training admits an
 (cid:114) (cid:16) (cid:16) (cid:17)(cid:17)  
σ Λ 1⊤hˆ(Z˜),1⊤h∗(Z˜) (cid:118)
 Ω

(cid:114)max U (cid:117) (cid:117)
(cid:116)
dσ (cid:16)max(θ˜∗θ˜⋆⊤) (cid:17) ,1

  σ max(cid:16) Λ(cid:16) hˆ(Z˜),h∗ U(Z˜)(cid:17)(cid:17) σ min (cid:80)d j=1w j∗w j∗⊤  2
representation transferrability to binary classification task. Notice that the transfer constant C mainly depends
β
on the Schur complement of hˆ(Z˜),h∗(Z˜), and 1⊤hˆ(Z˜),1⊤h∗(Z˜). In the main paper Lemma 2 we omit this
U U
constant dependency.Yuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi
D.2 Proof of Generalization of MAE Pre-training Task
In this section we are going to derive generalization bound of the masking pre-training with Transformer. In
pursuit of optimal generalization bound of pretraining task, i.e., regression with deep transformer, we again need
to employ the framework we introduced in CE analysis (Appendix C.2). Hence, we need to upper bound the
worst case L covering number of deep transformer class. The following result establishes the worst case spectral
2
covering number of L-layer self-attention transformer defined in 14.
Lemma 8 (Covering number of transformer class). Let X =[X ;...X ]∈RNK×d denotes the concatenated
[N] 1 N
data matrix. Then the worst case covering number of L-layer transformer class H defined in 14 is bounded as
follows:
(cid:32) L (cid:33)
lnN ∞(H(S),ϵ,∥·∥)≤O s2 L(cid:13) (cid:13)X [N](cid:13) (cid:13)2(cid:88) ϵρ 2l ,
l=1
where
l
s := (cid:89)(cid:0) α W2(j)+1(cid:1)(cid:0) W2(j)α K+1(cid:1) ,
l 2 1
j=1
(cid:32) (cid:32) (cid:33)(cid:33)
α W2(l)(s ∥X ∥)2
ρ :=O α2(α W2(l)+1)2B2(l)ln(2d2) K2+ 1 l−1 ∗
l 1 2 d
K
+O(cid:0) α2W2(l)B2(l)(W2(l)+α2K2W2(l))ln(2dm)(cid:1)
,
2 1
∥X ∥:= max∥X ∥.
∗ i
i∈[N]
Roughly speaking, ρ is the price for covering the parameter of lth self-attention layer, and extending the
l
cover to the whole model yields the sum over l. Notice that to ensure a L cover, it suffices to ensure that
∞
(cid:80)N ∥h(X )−h (X )∥2 ≤ϵ2. However, if we trivially cover each individual loss ∥h(X )−h (X )∥2 with ϵ2/N
i=1 i ϵ i i ϵ i
radius, the final covering number will be N times larger, which make the later generalization bound vacuous,
i.e., greater than 1. To avoid this N factor, we directly consider the cover over concatenated data matrix X ,
[N]
(cid:13) (cid:13)2
and consider the covering (cid:13)hˆ(X )−h(X )(cid:13) ≤ϵ2. Using the fact that matrix covering bound is independent
(cid:13) [N] [N] (cid:13)
of dimension of X , but only depends the spectral norm of X , the final covering number will only have
[N] [N]
logrithmic dependency on N.
To prove Lemma 8, first we introduce the following matrix covering number bound from [Bartlett et al., 2017].
Lemma 9. [Bartlett et al., 2017, Lemma 3.2] Let conjugate exponents (p,q) and (r,s) be given with p≤2, as
well as positive reals (a,b,ϵ) and positive integer m. Let matrix X∈RNK×d be given with ∥X∥ ≤b. Then
p
lnN (cid:0)(cid:8) XW:W∈Rd×m,∥W∥ ≤a(cid:9) ,ϵ,∥·∥ (cid:1)
≤(cid:24) a2b2m2/r(cid:25)
ln(2dm).
q,s 2 ϵ2
Lemma 10 (Covering number of attention matrix). Given a set of data S = {X ,...,X } and the attention
1 N
matrix class:
   
S ,0,...,0,
H
S(S)=S=

0,1
.... ,. 0.
,S
 :S i
=softmax(cid:18)
√1 d KX iW K(X iW
Q)⊤(cid:19)
:∥W K∥,∥W Q∥≤W,

 N
∥W ∥ ,∥W ∥
≤B,
K 2,1 Q 2,1
the following covering number bound holds true:
(cid:32) (cid:33)
KW2B2∥X ∥4
lnN(H (S),ϵ,∥·∥)≤O ∗ ln(2d2) .
S d ϵ2
KOn the Generalization Ability of Unsupervised Pretraining
(cid:110) (cid:111) (cid:110) (cid:111)
Proof. we define set K = XW :∥W ∥≤W,∥W ∥ ≤B , Q = XW :∥W ∥≤W,∥W ∥ ≤B .
K K K 2,1 Q Q Q 2,1
We define ϵ cover of K as C , and ϵ cover of Q as C . We construct the following set:
K K Q Q
   
S ,0,...,0,
C
=
S=
1
...
:S
=softmax(cid:18)
√1
X W (X W
)⊤(cid:19)
:W ∈C ,W ∈C

S   i d i K i Q K K Q Q

0,...,0,S
K 
N
Next we will show that C is a cover of H (S) with some radius. For any S ∈H , we can find Sˆ ∈C such
S S [N] S [N] S
that:
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)S −Sˆ (cid:13)≤ max(cid:13)S −Sˆ (cid:13)
(cid:13) [N] [N](cid:13) (cid:13) i i(cid:13)
i∈[N]
(cid:13) (cid:18) (cid:19) (cid:18) (cid:19)(cid:13)
= max(cid:13) (cid:13)softmax √1 X iW K(X iW Q)⊤ −softmax √1 X iWˆ K(X iWˆ Q)⊤ (cid:13) (cid:13)
i∈[N](cid:13) d
K
d
K
(cid:13)
√
K (cid:13) (cid:13)
≤ √ max(cid:13)X W (X W )⊤−X Wˆ (X Wˆ )⊤(cid:13)
d
K
i∈[N](cid:13) i K i Q i K i Q (cid:13)
√
K (cid:13) (cid:13)
≤ √ max(cid:13)(X W −X Wˆ )(X W )⊤(cid:13)
d
K
i∈[N](cid:13) i K i K i Q (cid:13)
√
K (cid:13) (cid:13)
+ √ max(cid:13)X Wˆ (X W )⊤−X Wˆ (X Wˆ )⊤(cid:13)
d
K
i∈[N](cid:13) i K i Q i K i Q (cid:13)
√
W K
≤ √ (ϵ +ϵ )max∥X ∥,
K Q i
d K i∈[N]
where the first inequality is due to the property of block diagonal matrices. We define ∥X ∥=max ∥X ∥.
∗ i∈[N] i
√
To ensure above bound is less than ϵ, we choose ϵ
K
=ϵ
Q
= √dK ϵ. According to Lemma 9, we know:
2W K∥X∗∥
(cid:32) (cid:33)
KW2B2∥X ∥4
ln|C |≤ln|C |+ln|C |≤O ∗ ln(2d2) .
S K Q d ϵ2
K
Proposition 3 (Covering number of single self-attention layer). Consider the following function class of
self-attention module:
 (cid:18) 1 (cid:19) 
X(cid:55)→σ(ZW FC1)W FC2+Z:Z=(A+X),A=softmax √
d
KXW K(XW Q)⊤ XW
V

H :=
SA ∥W ∥,∥W ∥,∥W ∥,∥W ∥,∥W ∥≤W,
 ∥WFC1
∥
,∥WFC2
∥
K
,∥W
∥Q ,∥WV
∥ ,∥W ∥
≤B,
FC1 2,1 FC2 2,1 K 2,1 Q 2,1 V 2,1
then the following bound holds for its covering number:
lnN(H
(S),ϵ,∥·∥)≤O(cid:32) (α 1α 2W2+α 1)2B2(cid:13) (cid:13)X [N](cid:13) (cid:13)2 ln(2d2)(cid:33)(cid:32)
K2+
α 1W2∥X ∗∥2(cid:33)
SA ϵ2 d
K
+O(cid:32)
α
22W2B2(W2(cid:13)
(cid:13)X
[N](cid:13) (cid:13)2
+α
12K2W2(cid:13)
(cid:13)X
[N](cid:13) (cid:13)2
)
ln(2dm)(cid:33)
.
ϵ2
Proof. Recall that X ∈RNK×d is the concatenated data matrix, and we shall use h(X )∈RNK×d to denote
[N] [N]
the concatenated encoder output, i.e., h(X )=[h(X );...,h(X )]. Our goal is to find the cardinality of a cover
[N] 1 N
such that for any h∈H we can find a h ∈C such that
ϵ SA
(cid:13) (cid:13)
(cid:13)h(X [N])−h ϵ(X [N])(cid:13)≤ϵ.Yuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi
I: Covering number of input layer by value matrix Let C to be ϵ cover of set H (S) =
V V V
(cid:110) (cid:111)
X W :∥W ∥≤W,∥W ∥ ≤B , then evoking Lemma 9 we have:
[N] V V V 2,1
(cid:32) B2(cid:13)
(cid:13)X
[N](cid:13) (cid:13)2 (cid:33)
lnN(H ,ϵ ,∥·∥)≤O ln(2dm) .
V V ϵ2
V
II: Covering number of Attention layer Next, consider the set of attention matrix
   
S ,0,...,0,
H
S(S)=S=

0,1
.... ,. 0.
,S
 :S
i
=softmax(cid:18)
√1
d
KX iW K(X iW
Q)⊤(cid:19)
:∥W K∥,∥W Q∥≤W,

 N
∥W ∥ ,∥W ∥
≤B,
K 2,1 Q 2,1
From Lemma 10 we know its covering number can be bounded as:
(cid:32) (cid:33)
KW2B2∥X ∥4
lnN (H (S),ϵ,∥·∥)≤lnN (H ,ϵ ,∥·∥)≤O ∗ ln(d2) .
S S˜ S d ϵ2
K S
Now we can proceed to bounding the covering number of following set:
 (cid:18) (cid:19) 
1
α 1softmax √ X [N]W K(X [N]W Q)⊤ X [N]W
V
:∥W K∥,∥W Q∥,∥W V∥≤W, 
H A(S)= d K
 ∥W ∥ ,∥W ∥ ,∥W ∥ ≤B,
K 2,1 Q 2,1 V 2,1
(cid:110) (cid:111)
For every element Vˆ ∈C , we construct the set α H (S)◦Vˆ := α S Vˆ :S ∈H (S) . Then we
[N] V 1 S [N] 1 [N] [N] [N] S
define ϵ -covering of H ◦Vˆ as C(H ◦Vˆ,ϵ ,∥·∥). To construct H ◦Vˆ as C(H ◦Vˆ,ϵ ,∥·∥), we consider C .
A S S A S S A S
For any S Vˆ ∈H (S)◦Vˆ , we can find Sˆ ∈C , such that
[N] [N] S [N] [N] S
(cid:13) (cid:13) (cid:13) (cid:13)(cid:13) (cid:13)
(cid:13)α S Vˆ −α Sˆ Vˆ (cid:13)≤α (cid:13)S −Sˆ (cid:13)(cid:13)Vˆ (cid:13)
(cid:13) 1 [N] [N] 1 [N] [N](cid:13) 1(cid:13) [N] [N](cid:13)(cid:13) [N](cid:13)
(cid:13) (cid:13)
≤α 1ϵ S(cid:13)X [N](cid:13)W.
Setting ϵ S = α1∥Xϵ [A N]∥W we can conclude that C(H S ◦Vˆ,ϵ A,∥·∥) actually ϵ A covers H S ◦Vˆ and the following
fact holds for the covering number
ln|C(H (S)◦Vˆ ,ϵ ,∥·∥)|≤ sup lnN(H (S)◦Vˆ ,ϵ
,∥·∥)≤O(cid:32) α 12KB2W4∥X ∗∥4(cid:13) (cid:13)X [N](cid:13) (cid:13)2 ln(2d2)(cid:33)
.
S [N] A S [N] A d ϵ2
Vˆ [N]∈CV K A
Then we construct a cover C for H by:
A A
C = (cid:91) C(α H (S)◦Vˆ )
A 1 S [N]
Vˆ [N]∈CV
It is not hard to verify the cardinality of this cover:
ln|C |≤ln|C |+ sup ln|C(α H (S)◦Vˆ ,ϵ ,∥·∥)|
A V 1 S [N] A
Vˆ [N]∈CV
≤O(cid:32) B2(cid:13) (cid:13)X [N](cid:13) (cid:13)2 ln(2dm)(cid:33) +O(cid:32) α 12KB2W4∥X ∗∥4(cid:13) (cid:13)X [N](cid:13) (cid:13)2 ln(2d2)(cid:33)
.
ϵ2 d ϵ2
V K AOn the Generalization Ability of Unsupervised Pretraining
III: Covering number of fully-connected layer 1 By similar reasoning, we can show that the covering
number of
(cid:110) (cid:111)
H (S)= Z W :Z =α A +X ,A ∈H (S),∥W ∥≤W,∥W ∥ ≤B
FC1 [N] FC1 [N] 1 [N] [N] [N] A FC1 FC1 2,1
For every element Aˆ ∈C , we define set
[N] A
(cid:110) (cid:111)
Aˆ ◦W = (α Aˆ +X )W ,∥W ∥≤W,∥W ∥ ≤B
[N] FC1 1 [N] [N] FC1 FC1 FC1 2,1
We denote ϵ -cover of Aˆ ◦W as C(Aˆ ◦W ,ϵ ,∥·∥), and the covering number of Aˆ ◦W is
FC1 [N] FC1 [N] FC1 FC1 [N] FC1
bounded by:
ln|C(Aˆ ◦W ,ϵ ,∥·∥)|≤ sup lnN(Aˆ ◦W ,ϵ ,∥·∥)
[N] FC1 FC1 [N] FC1 FC1
Aˆ [N]∈CA
 B2((cid:13) (cid:13)X [N](cid:13) (cid:13)2 +α 12W2(cid:13) (cid:13)X [N](cid:13) (cid:13)2(cid:13) (cid:13) (cid:13)Sˆ [N](cid:13) (cid:13) (cid:13)2 ) 
= sup O ln(dm)
 ϵ2 
Sˆ [N]∈CS FC1
≤O(cid:32) B2((cid:13)
(cid:13)X
[N](cid:13) (cid:13)2
+α
12K2W2(cid:13)
(cid:13)X
[N](cid:13) (cid:13)2
)
ln(dm)(cid:33)
ϵ2
FC1
Now, we construct the ϵ -cover of H (S) as
FC1 FC1
C = (cid:91) C(Aˆ ◦W ,ϵ ,∥·∥)
FC1 [N] FC1 FC1
Aˆ [N]∈CA
And the covering number is bounded:
ln|C |≤ln|C |+ sup ln|C(Aˆ ◦W ,ϵ ,∥·∥)|
FC1 A [N] FC1 FC1
Aˆ [N]∈CA
≤O(cid:32) B2(cid:13) (cid:13)X [N](cid:13) (cid:13)2 ln(2dm)(cid:33) +O(cid:32) α 12KB2W4∥X ∗∥4(cid:13) (cid:13)X [N](cid:13) (cid:13)2 ln(2d2)(cid:33)
ϵ2 d ϵ2
V K A
+O(cid:32) B2((cid:13)
(cid:13)X
[N](cid:13) (cid:13)2
+α
12K2W2(cid:13)
(cid:13)X
[N](cid:13) (cid:13)2
)
ln(dm)(cid:33)
.
ϵ2
FC1
IV: Covering number of fully-connected layer 2 The analysis this part is almost identical to III. We try
to find the covering number of the set H . For every element Fˆ ∈C and Aˆ ∈C , define the set
SA [N] FC1 [N] A
(cid:110) (cid:111)
α Fˆ ◦W +Zˆ = α σ(Fˆ )W +Zˆ :Zˆ =α Aˆ +X ,∥W ∥≤W,∥W ∥ ≤B
2 [N] FC2 [N] 2 [N] FC2 [N] [N] 1 [N] [N] FC2 FC2 2,1
.
We denote ϵ -cover of α Fˆ ◦W +Zˆ as C(α Fˆ ◦W +Zˆ,ϵ ,∥·∥), and the cardinality of this set is
FC2 2 [N] FC2 2 [N] FC2 FC2
bounded by:
ln|C(α Fˆ ◦W +Zˆ ,ϵ ,∥·∥)|≤ sup lnN(Fˆ ◦W +Zˆ ,ϵ ,∥·∥)
2 [N] FC2 [N] FC2 [N] FC2 [N] FC2
Fˆ [N]∈CFC1,Aˆ∈CA
 α 22B2(cid:16)(cid:13) (cid:13)X [N](cid:13) (cid:13)2 +α 12K2W2(cid:13) (cid:13)X [N](cid:13) (cid:13)2(cid:17) W2 
=O ln(2dm)
ϵ2
FC2Yuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi
Now, we construct the ϵ -cover of H as
FC2 FC2
C = (cid:91) C(Fˆ ◦W +Zˆ ,ϵ ,∥·∥)
FC2 [N] FC2 [N] FC2
Fˆ [N]∈HFC1,Aˆ∈HA
And the covering number is bounded:
ln|C |≤ln|C |+ln|C |+ max ln|C(Aˆ ◦W ,ϵ˜,∥·∥)|
FC2 A FC1 [N] FC1
Aˆ∈CA
≤O(cid:32) B2(cid:13) (cid:13)X [N](cid:13) (cid:13)2 ln(2dm)(cid:33) +O(cid:32) α 12KB2W4∥X ∗∥4(cid:13) (cid:13)X [N](cid:13) (cid:13)2 ln(2d2)(cid:33)
ϵ2 d ϵ2
V K A
+O(cid:32) B2((cid:13)
(cid:13)X
[N](cid:13) (cid:13)2
+α
12K2W2(cid:13)
(cid:13)X
[N](cid:13) (cid:13)2
)
ln(dm)(cid:33)
ϵ2
FC1
 α 22B2(cid:16)(cid:13) (cid:13)X [N](cid:13) (cid:13)2 +α 12K2W2(cid:13) (cid:13)X [N](cid:13) (cid:13)2(cid:17) W2 
+O ln(2dm).
ϵ2
FC2
V: Verification of C being an ϵ cover of H It remains to verify C is an ϵ cover of H . Given any
FC2 SA FC2 SA
H ∈H , we can find a Hˆ ∈C such that
[N] SA [N] FC2
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)H −Hˆ (cid:13)=(cid:13)α σ(Z W )W +Z −α σ(Zˆ Wˆ )Wˆ −Zˆ (cid:13)
(cid:13) [N] [N](cid:13) (cid:13) 2 [N] FC1 FC2 [N] 2 [N] FC1 FC2 [N](cid:13)
(cid:13) (cid:13)
≤α (cid:13)σ(Z W )W −σ(Zˆ Wˆ )W (cid:13)
2(cid:13) [N] FC1 FC2 [N] FC1 FC2(cid:13)
(cid:13) (cid:13)
+(cid:13)α σ(Zˆ Wˆ )W +Z −α σ(Zˆ Wˆ )Wˆ −Zˆ (cid:13)
(cid:13) 2 [N] FC1 FC2 [N] 2 [N] FC1 FC2 [N](cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
≤α W (cid:13)σ(Z W )−σ(Zˆ Wˆ )(cid:13)+ϵ +(cid:13)Z −Zˆ (cid:13).
2 (cid:13) [N] FC1 [N] FC1 (cid:13) FC2 (cid:13) [N] [N](cid:13)
(cid:13) (cid:13)
We bound (cid:13)σ(Z W )−σ(Zˆ Wˆ )(cid:13) first as follows:
(cid:13) [N] FC1 [N] FC1 (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)σ(Z W )−σ(Zˆ Wˆ )(cid:13)≤(cid:13)(α A +X )W −(α Aˆ +X )Wˆ (cid:13)
(cid:13) [N] FC1 [N] FC1 (cid:13) (cid:13) 1 [N] [N] FC1 1 [N] [N] FC1(cid:13)
(cid:13) (cid:13)
≤(cid:13)(α A +X )W −(α Aˆ +X )W (cid:13)
(cid:13) 1 [N] [N] FC1 1 [N] [N] FC1(cid:13)
(cid:13) (cid:13)
+(cid:13)(α Aˆ +X )W −(α Aˆ +X )Wˆ (cid:13)
(cid:13) 1 [N] [N] FC1 1 [N] [N] FC1(cid:13)
(cid:13) (cid:13)
≤α W (cid:13)A −Aˆ (cid:13)+ϵ .
1 (cid:13) [N] [N](cid:13) FC1
(cid:13) (cid:13)
For (cid:13)A −Aˆ (cid:13), we have
(cid:13) [N] [N](cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)A −Aˆ (cid:13)=(cid:13)S X W −Sˆ X Wˆ (cid:13)
(cid:13) [N] [N](cid:13) (cid:13) [N] [N] V [N] [N] V(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
≤(cid:13)S X W −S X Wˆ (cid:13)+(cid:13)S X Wˆ −Sˆ X Wˆ (cid:13)
(cid:13) [N] [N] V [N] [N] V(cid:13) (cid:13) [N] [N] V [N] [N] V(cid:13)
(cid:13) (cid:13)
≤K(cid:13)X W −X Wˆ (cid:13)+ϵ
(cid:13) [N] V [N] V(cid:13) A
≤Kϵ +ϵ .
V A
Putting pieces together yields:
(cid:13) (cid:13)
(cid:13)σ(ZW )−σ(ZˆWˆ )(cid:13)≤α W(Kϵ +ϵ )+ϵ .
(cid:13) FC1 FC1 (cid:13) 1 V A FC1On the Generalization Ability of Unsupervised Pretraining
(cid:13) (cid:13)
Now we switch to bounding (cid:13)Z −Zˆ (cid:13):
(cid:13) [N] [N](cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)Z −Zˆ (cid:13)=α (cid:13)A −Aˆ (cid:13)≤α (Kϵ +ϵ )
(cid:13) [N] [N](cid:13) 1(cid:13) [N] [N](cid:13) 1 V A
Hence we know:
(cid:13) (cid:13)
(cid:13)H −Hˆ (cid:13)≤α W (α W(Kϵ +ϵ )+ϵ )+α (Kϵ +ϵ )+ϵ
(cid:13) [N] [N](cid:13) 2 1 V A FC1 1 V A FC2
=(α α W2K+α K)ϵ +(α α W2+α )ϵ +α Wϵ +ϵ
1 2 1 V 1 2 1 A 2 FC1 FC2
To make sure RHS is less than ϵ, we set
ϵ ϵ ϵ ϵ
ϵ = ,ϵ = ,ϵ = ,ϵ = .
V 4(α α W2K+α K) A 4(α α W2+α ) FC1 4α W FC2 4
1 2 1 1 2 1 2
Recall that
ln|C |≤ln|C |+ln|C |+ max ln|C(Aˆ ◦W ,ϵ˜,∥·∥)|
FC2 A FC1 [N] FC1
Aˆ∈CA
≤O(cid:32) B2(cid:13) (cid:13)X [N](cid:13) (cid:13)2 ln(2dm)(cid:33) +O(cid:32) α 12KB2W4∥X ∗∥4(cid:13) (cid:13)X [N](cid:13) (cid:13)2 ln(2d2)(cid:33)
ϵ2 d ϵ2
V K A
+O(cid:32) B2((cid:13)
(cid:13)X
[N](cid:13) (cid:13)2
+α
12K2W2(cid:13)
(cid:13)X
[N](cid:13) (cid:13)2
)
ln(dm)(cid:33)
ϵ2
FC1
 α 22B2(cid:16)(cid:13) (cid:13)X [N](cid:13) (cid:13)2 +α 12K2W2(cid:13) (cid:13)X [N](cid:13) (cid:13)2(cid:17) W2 
+O ln(2dm).
ϵ2
FC2
Hence we can upper bound the covering number of H as follows:
SA
(cid:32) (α 1α 2W2+α 1)2K2B2(cid:13) (cid:13)X [N](cid:13) (cid:13)2 (cid:33)
lnN(H ,ϵ,∥·∥)≤O ln(2dm)
SA ϵ2
+O(cid:32) (α 1α 2W2+α 1)2α 12KB2W4∥X ∗∥4(cid:13) (cid:13)X [N](cid:13) (cid:13)2 ln(2d2)(cid:33)
d ϵ2
K
+O(cid:32)
α
22W2B2((cid:13)
(cid:13)X
[N](cid:13) (cid:13)2
+α
12K2W2(cid:13)
(cid:13)X
[N](cid:13) (cid:13)2
)
ln(2dm)(cid:33)
ϵ2
=O(cid:32) (α 1α 2W2+α 1)2B2(cid:13) (cid:13)X [N](cid:13) (cid:13)2 ln(2d2)(cid:33)(cid:32)
K2+
α 1W4∥X ∗∥4(cid:33)
ϵ2 d
K
+O(cid:32)
α
22W2B2((cid:13)
(cid:13)X
[N](cid:13) (cid:13)2
+α
12K2W2(cid:13)
(cid:13)X
[N](cid:13) (cid:13)2
)
ln(2dm)(cid:33)
.
ϵ2
Proposition 4 (Contraction mapping of self-attention layer). For a single attention layer parameterized by W,
with ∥W∥≤W, the following statement holds:
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)SA (X)−SA (Xˆ)(cid:13)≤(α W2+1)(α KW +1)(cid:13)X−Xˆ(cid:13).
(cid:13) W W (cid:13) 2 1 (cid:13) (cid:13)Yuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi
Proof. The proof follows by definition and simple algebraic manipulation:
(cid:13) (cid:13) (cid:13) (cid:16) (cid:17) (cid:13)
(cid:13)SA (X)−SA (Xˆ)(cid:13)≤(cid:13)α σ(ZW )W +Z−βσ ZˆW W −Zˆ(cid:13)
(cid:13) W W (cid:13) (cid:13) 2 FC1 FC2 FC1 FC2 (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
≤α W2(cid:13)Z−Zˆ(cid:13)+(cid:13)Z−Zˆ(cid:13)
2 (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)
≤(α W2+1)(cid:13)A+X−Aˆ −Xˆ(cid:13)
2 (cid:13) (cid:13)
(cid:16)(cid:13) (cid:13) (cid:13) (cid:13)(cid:17)
≤(α W2+1) (cid:13)α SXW −α SXˆW (cid:13)+(cid:13)X−Xˆ(cid:13)
2 (cid:13) 1 V 1 V(cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)
≤(α W2+1)(α KW +1)(cid:13)X−Xˆ(cid:13).
2 1 (cid:13) (cid:13)
D.2.1 Proof of Lemma 8
Proof. We first examine the norm of each self-attention layer’s output:
(cid:13) (cid:13)Xl i(cid:13) (cid:13)≤α 2W2(l)(cid:13) (cid:13)Zl i(cid:13) (cid:13)+(cid:13) (cid:13)Zl i(cid:13) (cid:13)
≤(cid:0) α 2W2(l)+1(cid:1)(cid:0) α(cid:13) (cid:13)Al i(cid:13) (cid:13)+(cid:13) (cid:13)X il−1(cid:13) (cid:13)(cid:1)
≤(cid:0)
α
2W2(l)+1(cid:1)(cid:0) W2(l)αK+1(cid:1)(cid:13) (cid:13)Xl−1(cid:13)
(cid:13)
l
≤ (cid:89)(cid:0) α W2(j)+1(cid:1)(cid:0) W2(j)α K+1(cid:1) ∥X ∥ (16)
2 1 i
j=1
and grouped output
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)Xl (cid:13)≤α W2(l)(cid:13)Zl (cid:13)+(cid:13)Zl (cid:13)
(cid:13) [N](cid:13) 2 (cid:13) [N](cid:13) (cid:13) [N](cid:13)
≤(cid:0) α W2(l)+1(cid:1)(cid:16) α(cid:13) (cid:13)Al (cid:13) (cid:13)+(cid:13) (cid:13)Xl−1(cid:13) (cid:13)(cid:17)
2 (cid:13) [N](cid:13) (cid:13) [N](cid:13)
(cid:13) (cid:13)
≤(cid:0)
α
W2(l)+1(cid:1)(cid:0) W2(l)αK+1(cid:1)(cid:13)Xl−1(cid:13)
2 (cid:13) [N](cid:13)
l
≤ (cid:89)(cid:0) α 2W2(j)+1(cid:1)(cid:0) W2(j)α 1K+1(cid:1)(cid:13) (cid:13)X [N](cid:13) (cid:13).
j=1
For the ease of presentation, we define the class of lth layer output:
 SAl(cid:16) SAl−1...SA1(X)(cid:17) :(cid:13) (cid:13) (cid:13)W Fj C1(cid:13) (cid:13) (cid:13),(cid:13) (cid:13) (cid:13)W Fj C2(cid:13) (cid:13) (cid:13),(cid:13) (cid:13) (cid:13)W Kj (cid:13) (cid:13) (cid:13),(cid:13) (cid:13)W Ql (cid:13) (cid:13),(cid:13) (cid:13) (cid:13)W Vj (cid:13) (cid:13) (cid:13)≤W(j), 
H l = (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) ,
 (cid:13) (cid:13)W Fj C1(cid:13)
(cid:13)
,(cid:13) (cid:13)W Fj C2(cid:13)
(cid:13)
,(cid:13) (cid:13)W Kj (cid:13)
(cid:13)
,(cid:13) (cid:13)W Qj (cid:13)
(cid:13)
,(cid:13) (cid:13)W Vj (cid:13)
(cid:13)
≤B(j),∀j ∈[l] 
2,1 2,1 2,1 2,1 2,1
and it will be useful to define set of weight matrices at lth layer:
(cid:8) (cid:9)
W = W:∥W∥≤W(l),∥W∥ ≤B(l). .
l 2,1
We shall construct the cover with certain radius for each H ,l∈[L].
l
For base case l=1: we create ϵ cover of SA1(X )
1 [N]
C =C(SA1(X ),ϵ ,∥·∥).
1 [N] 1
For 1<l+1≤L, for each element Xˆl ∈C , we construct the ϵ -cover of the following set:
l l+1
(cid:110) (cid:111)
SAl+1(Xˆl):= SA (Xˆl),Wl+1 ∈W
Wl+1 l+1On the Generalization Ability of Unsupervised Pretraining
(cid:16) (cid:17)
and we denote the cover as C SAl+1(Xˆl),ϵ ,∥·∥ . We first examine the cardinality of this cover as follows:
l+1
(cid:12) (cid:16) (cid:17)(cid:12)
ln(cid:12)C SAl+1(Xl ),ϵ ,∥·∥ (cid:12)
(cid:12) [N] l+1 (cid:12)
≤ max
O(cid:18) (α 1α 2W2+α 1)2B2 ln(2d2)(cid:19)(cid:32)
K2+
α 1W4(cid:0) max i∈[N](cid:13) (cid:13)Xl i(cid:13) (cid:13)(cid:1)4(cid:33) (cid:13)
(cid:13)Xl
(cid:13) (cid:13)2
Xl∈Cl ϵ2 d K (cid:13) [N](cid:13)
(cid:18) α2W2(l+1)B2(l+1)(1+α2K2W2(l+1)) (cid:19)(cid:13) (cid:13)2
+O 2 1 ln(2dm) (cid:13)Xl (cid:13)
ϵ2 (cid:13) [N](cid:13)
≤O(cid:18) (α 1α 2W2(l+1) ϵ2+α 1)2B2(l+1) ln(2d2)(cid:19)(cid:32) K2+ α 1W4(l+ d1)(s l∥X ∗∥)4(cid:33) s2
l
(cid:13) (cid:13)X [N](cid:13) (cid:13)2
K
+O(cid:18) α 22W2(l+1)B2(l+1 ϵ) 2(1+α 12K2W2(l+1)) ln(2dm)(cid:19) s2
l
(cid:13) (cid:13)X [N](cid:13) (cid:13)2
:=lnN
l+1
where s :=(cid:81)l (cid:0) α W2(j)+1(cid:1)(cid:0) W2(j)α K+1(cid:1).
l j=1 2 1
We then construct cover for H as:
l+1
(cid:91) (cid:16) (cid:17)
C = C SAl+1(Xl),ϵ ,∥·∥ .
l+1 l+1
Xl∈Cl
It is not hard to check the cardinality of C
l+1
(cid:12) (cid:12)
(cid:12) (cid:12) l+1
|C l+1|=(cid:12) (cid:12) (cid:91) C(cid:16) SAl+1(Xl),ϵ l+1,∥·∥(cid:17)(cid:12) (cid:12)≤|C l|N l+1 ≤ (cid:89) N l′
(cid:12) (cid:12)
(cid:12)Xl∈Cl (cid:12) l′=1
Let
(cid:32) (cid:33)
ρ :=O(cid:0) (α α W2(l)+α )2B2(l)ln(2d2)(cid:1) K2+ α 1W4(l)(s l−1∥X ∗∥)4
l 1 2 1 d
K
+O(cid:0) α2W2(l)B2(l)(1+α2K2W2(l))ln(2dm)(cid:1)
,
2 1
we have
l+1 l+1
ln|C l+1|≤ (cid:88) lnN
l′
≤ (cid:88) ρ ϵ2l′ s2 l′(cid:13) (cid:13)X [N](cid:13) (cid:13)2 .
l′=1 l′=1 l′
Now it remains to verify C is a cover of H . For any XL ∈H we can find a XˆL ∈C such that
L L L L
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)XL−XˆL(cid:13)=(cid:13)SA (XL−1)−SA (XˆL−1)(cid:13)
(cid:13) (cid:13) (cid:13) WL WˆL (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
≤(cid:13)SA (XL−1)−SA (XˆL−1)(cid:13)+(cid:13)SA (XˆL−1)−SA (XˆL−1)(cid:13)
(cid:13) WL WL (cid:13) (cid:13) WL WˆL (cid:13)
(cid:13) (cid:13)
≤(α W2(L)+1)(α KW(L)+1)(cid:13)XL−1−XˆL−1(cid:13)+ϵ
2 1 (cid:13) (cid:13) L
L L
(cid:88) (cid:89)
≤ (α W2(j)+1)(α KW(j)+1)ϵ
2 1 l
l=0j=l+1
(cid:16) (cid:17)−1
We choose ϵ = L(cid:81)L (α W2(j)+1)(α KW(j)+1) ϵ, and let s := (cid:81)L (α W2(j) +
j j=l+1 2 1 l+1(cid:55)→L j=l+1 2
1)(α KW(j)+1). Hence we have:
1
ϵρ 2l =ρ ls2 l+ ϵ21(cid:55)→L (cid:13) (cid:13)X [N](cid:13) (cid:13)2
lYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi
and conclude the covering number of H as follows:
L
L
lnN(H L,ϵ,∥·∥)=ln|C L|≤(cid:88) ϵρ 2l (cid:0) s l(cid:13) (cid:13)X [N](cid:13) (cid:13)(cid:1)2
l=1 l
(cid:32) L (cid:33)
=ln|C L|≤O s2 L(cid:13) (cid:13)X [N](cid:13) (cid:13)2(cid:88) ϵρ 2l .
l=1
Finally according to covering number fact (13),
lnN (G◦H(S),ϵ,∥·∥)≤lnN(G◦H(S),ϵ,∥·∥), (17)
∞
we can conclude the proof.
Now, equipped with covering number bound for the transformer, we are ready to show the generalization of MAE
pre-training task.
D.2.2 Proof of Lemma 3
(cid:18) (cid:13) (cid:13)2 (cid:19)
Proof. Similar to the proof in CE section, we evoke Lemma 7 with c=O s2 (cid:13)Z˜ (cid:13) (cid:80)L+1ρ , where s ,ρ
L+1(cid:13) [N](cid:13) l=1 l l l
are defined in Lemma 8.
(cid:114) Hr·c (cid:114) cHr (cid:32) √ (cid:32) 5(cid:114) Hr·c(cid:33)(cid:33)
R (L(r))≤10 +10 ln br−ln
S(cid:98) N N 2 N
(cid:114) (cid:114) (cid:32) (cid:114) (cid:33)
Hr·c cHr 2 bN
=10 +10 ln .
N N 5 Hc
(cid:113) (cid:26) (cid:18) (cid:113) (cid:19)(cid:27)
We set ϕ(r)=10 Hr·c ·max 1,ln 2 bN . Solving the follwoing equation to get r∗
N 5 Hc
(cid:114) (cid:40) (cid:32) (cid:114) (cid:33)(cid:41)
Hr·c 2 bN
ϕ(r)=10 ·max 1,ln =r,
N 5 Hc
(cid:40) (cid:32) (cid:114) (cid:33)(cid:41)2
H ·c 2 bN
⇐⇒r∗ =100 ·max 1,ln
N 5 Hc
Now, according to Theorem 7, and the fact that
√ √
A≤B+C A=⇒A≤B+C2+ BC,
we have
(cid:32) (cid:114) (cid:33)2
√ 4b(log(1/ν)+6loglogN)
L (g◦h)≤L (g◦h)+45r∗+ 8r∗+
U U(cid:98) N
b(ν+6loglogN)
+20
N
(cid:114) (cid:32) (cid:114) (cid:33)
b(ν+6loglogN) √ 4b(log(1/ν)+6loglogN)
+ L (g◦h)+45r∗+20 8r∗+
U(cid:98) N N
Plugging r∗ and empirical risk minimizers gˆ,hˆ will conclude the proof.On the Generalization Ability of Unsupervised Pretraining
D.3 Proof of Theorem 3
Proof. Again, recall in Theorem 1, the generalization bound of downstream task is given by
(cid:114)
(cid:16) (cid:17)β log(1/ν)
E (fˆ,hˆ)≤C E (gˆ,hˆ) +4G R (F ◦hˆ)+4B +4B ∥T −U ∥
T β U ϕ T(cid:98) ϕ n ϕ X TV
+minE (f,h∗).
T U
f∈F
Since in the previous subsection we prove the bounded transferrability and generalization of pre-training task, it
remains to show the upper bound of representation-induced Rademacher complexity.
(cid:34) (cid:35)
R (ϕ◦F ◦hˆ)=E sup 1 (cid:88)n ε ϕ(θ⊤(1⊤hˆ(X ))⊤,y )
T(cid:98) ε∈{±1}n θ:∥θ∥≤Rn i=1 i i i
(cid:34) (cid:35)
≤G E sup 1 (cid:88)n ε θ⊤(1⊤hˆ(X ))⊤
ϕ ε∈{±1}n θ:∥θ∥≤Rn i=1 i i
= RG ϕE (cid:13) (cid:13)(cid:88)n ε (1⊤hˆ(X ))⊤(cid:13) (cid:13)
n ε(cid:13) i=1 i i (cid:13)
(cid:114)
≤ RG ϕ E (cid:13) (cid:13)(cid:88)n ε (1⊤hˆ(X ))⊤(cid:13) (cid:13)2
n ε(cid:13) i=1 i i (cid:13)
(cid:114)
≤ RG ϕ (cid:88)n (cid:13) (cid:13)1⊤hˆ(X )(cid:13) (cid:13)2
n i=1(cid:13) i (cid:13)
(cid:114)
≤ RG ϕ (cid:88)n K(cid:13) (cid:13)hˆ(X )(cid:13) (cid:13)2
n i=1 (cid:13) i (cid:13)
whereatfirstinequalityweapplyLedoux-Talagrand’sinequalitytopeelofLipschitzlossϕ(·),andatlastinequality
(cid:13) (cid:13)2
we use the fact that ε are i.i.d. with zero mean, so that the cross terms disappear. For each (cid:13)hˆ(X )(cid:13) , evoking
i (cid:13) i (cid:13)
(16) we have:
(cid:13)
(cid:13)hˆ(X
)(cid:13)
(cid:13)≤
(cid:89)l
(cid:0) α W2(j)+1(cid:1)(cid:0) W2(j)α K+1(cid:1) ∥X ∥,
(cid:13) i (cid:13) 2 1 i
j=1
hence we arrive at
(cid:113)
RG (cid:81)l (α W2(j)+1)2(W2(j)α K+1)2(cid:80)n ∥X ∥2
R (ϕ◦F ◦hˆ)≤ ϕ j=1 2 1 i=1 i .
T(cid:98) n
Plugging Lemmas 2 and 3 as well as above bound will complete the proof.
E Proof of Convergence RadReg Algorithm
In this section we provide the missing proofs from Section 6. Then we provide the proof of convergence.
E.1 Convergence result of RadReg
In this section we provide formal version of convergence results for RadReg. First let us introduce the following
Moreau envelope concept.
Definition 6 (Moreau Envelope). A function Ψ (w) is the ρ-Moreau envelope of a function Ψ if Ψ (w) :=
ρ ρ
min {Ψ(w′)+ 1 ∥w′−w∥2}.
w′∈W 2ρ
We have the following property of the Moreau Envelope of a nonsmooth function:
Lemma 11. [Davis and Drusvyatskiy, 2019] Let wˆ = argmin Ψ(w′)+ 1 ∥w′ −w∥2, then we have the
w′∈W 2ρ
following facts: ∥wˆ −w∥≤ρ∥∇Φ (w)∥, min ∥g∥≤∥∇Φ (w)∥.
ρ g∈∂Ψ(wˆ) ρYuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi
Lemma 11 shows that, if we find a w such that ∥∇Ψ (w)∥ is small, then we can demonstrate that w is near some
ρ
point xˆ which is a near-stationary point of Ψ. We will use 1/4L-Moreau envelope of Ψ, following the setting in
[Lin et al., 2020, Rafique et al., 2018], and state the convergence rate in terms of ∥∇Ψ (w)∥. We also define
1/4L
quantity ∆ˆ =Ψ (w )−min Ψ (w) that will be used in stating the convergence rates.
Ψ1/4L 1/4L 0 w∈W 1/4L
Assumption 2 (Bounded Variance). Let z˜ and x˜ be uniformly sampled from Uˆ and Dˆ. Then, the variance of
stochastic gradients is bounded:
E(cid:13) (cid:13)∇L Uˆ(w;z˜)−∇L Uˆ(w)(cid:13) (cid:13)2 ≤δ2,
E(cid:13) (cid:13) (cid:13) (cid:13)∇R j(v,w;x˜)− n1 (cid:88)n i=1∇R j(v,w;x i)(cid:13) (cid:13) (cid:13) (cid:13)2 ≤δ2.
Assumption 3 (Smooth and Bounded Linear Head). L and R (v,w′;x) are L smooth w.r.t. w, ∀j ∈[B],
Uˆ j
and x∈X:
(cid:13) (cid:13)∇L Uˆ(w)−∇L Uˆ(w′)(cid:13) (cid:13)≤L∥w−w′∥,
∥∇ R (v,w;x)−∇ R (v,w′;x)∥≤L∥w−w′∥.
w j w j
Also, we assume R (v,w;x) is linear in v, and max ∥v∥≤D.
j v∈V
Assumption 4 (Lipschitzness). L and R (v,w′;x) are G Lipschitz w.r.t. w, ∀v ∈V,j ∈[B], and x∈X,
Uˆ j
i.e.,
(cid:13) (cid:13)L Uˆ(w)−L Uˆ(w′)(cid:13) (cid:13)≤G∥w−w′∥,
∥R (v,w;x)−R (v,w′;x)∥≤G∥w−w′∥.
j j
We are now ready to state the formal version of Theorem 4 as follows.
Theorem 8 (Convergence of RadReg with Linear Top Layer). Under Assumptions 2 and 3, if we use RadReg
(cid:16) (cid:17) (cid:16) (cid:17)
(Algorithm 1 with one step update) to optimize (8), by choosing η =Θ ϵ6 and γ =Θ ϵ2 it holds that:
L3D2G Lδ2
T
+1 1(cid:88)T t=0E(cid:13) (cid:13)Ψ 1/4L(wt)(cid:13) (cid:13)2 ≤ϵ2,
with the gradient complexity bounded by:
(cid:32) BL3(G2+ δ2)D2δ2∆ (cid:33)
O n′ n′ Ψ1/4L .
ϵ8
WecanseethattheproposedoptimizationalgorithmcanfindanϵstationarypointwithatmostO(cid:0)B(cid:1)stochastic
ϵ8
gradient evaluations. Since the complexity grows in terms of B, a proper sampling size of Rademacher variable is
crucial.
In the rest of this section, we prove the convergence rate of RadReg. The proof idea mainly follows the framework
developed in Lin et al. [2019]. But before we state a few intermediate results that the main proof relies on.
Lemma 12. Under the conditions of Theorem 8, the following one iteration recursion relation holds true:
ηE(cid:13) (cid:13)∇Ψ(wt−1)(cid:13) (cid:13)2 =E[Ψ 1/2L(wt−1)]−E[Ψ 1/2L(wt)]
+4ηL1 (cid:88)B E(cid:2) R (v∗(wt−1),wt−1)−R (vt−1,wt−1)(cid:3) +η2L(G2+ δ2 ),
B j j j j n′
j=1
where v∗(w):=argmax R (v,wt−1).
j v∈V j
Proof. Recall the definition of Ψ and R :
j
B (cid:34) (cid:32) n (cid:33)(cid:35)
1 (cid:88) 1 (cid:88)
Ψ(w):=L (w)+λ maxv⊤ σjh (x ) , (18)
Uˆ B V∈V n i w i
j=1 i
(cid:32) n (cid:33)
1 (cid:88)
R (v,w):=v⊤ σjh (x ). (19)
j n i w i
iOn the Generalization Ability of Unsupervised Pretraining
Also recall the definition of Ψ’s Moreau Envelope:
Ψ (w):= min
Ψ(w′)+2L∥w−w′∥2
.
1/4L
w′∈W
We define the proximal solution as:
wˆt :=arg min Ψ(w′)+2L(cid:13) (cid:13)wt−w′(cid:13) (cid:13)2 .
w′∈W
With all aforementioned definitions are in place, we proceed to proving the lemma. First, since wˆt−1 is not
minimizer of Ψ(·)+2L∥·−wt∥2 we have
E[Ψ 1/4L(wt)]≤E[Ψ(wˆt−1)]+2L(cid:13) (cid:13)wˆt−1−wt(cid:13) (cid:13)2
Recall the updating rule for w and v as stated below:
j
 
n′ B n′
1 (cid:88) 1 (cid:88) 1 (cid:88)
wt+1 =wt−η
n′
∇L(wt;x˜t i)+λ
B n′
∇ wR j(v jt,wt;x˜t i),
i=1 j=1 i=1
B n′
1 (cid:88) 1 (cid:88)
vt+1 =vt +γλ ∇ R (vt,wt;x˜t).
j j B n′ v j j i
j=1 i=1
Hence we can get the following relation by completing the square trick:
E(cid:13) (cid:13)wˆt−1−wt(cid:13) (cid:13)2 =E(cid:13) (cid:13)wˆt−1−wt−1(cid:13) (cid:13)2
(cid:42) 1 (cid:88)B (cid:43) δ2
+2ηE wˆt−1−wt−1, ∇L(wt−1)+λ ∇ R (vt−1,wt−1) +η2(G2+ ).
B v j j n′
j=1
According to L-smoothness of L and R , we can re-write the inner product term as:
j
(cid:42) B (cid:43)
1 (cid:88)
E wˆt−1−wt−1, ∇L(wt−1)+λ ∇ R (vt−1,wt−1)
B v j j
j=1
 
B
≤E L(wˆt−1)−L(wt−1)+λ B1 (cid:88)(cid:0) R j(v jt−1,wˆt−1)−R j(v jt−1,wt−1)(cid:1) +L(cid:13) (cid:13)wˆt−1−wt−1(cid:13) (cid:13)
j=1
Notice the following fact about Ψ, Ψ :
1/4L
B
L(wˆt−1)+λ B1 (cid:88) R j(v jt−1,wˆt−1)≤Ψ(wˆt−1)≤Ψ 1/4L(wt−1)−2L(cid:13) (cid:13)wˆt−1−wt−1(cid:13) (cid:13)2 .
j=1
The last inequality is because wˆt−1 is the minimizer of Ψ(·)+2L(cid:13) (cid:13)·−wt−1(cid:13) (cid:13)2. As a result, the inner product is
bounded by:
(cid:42) B (cid:43)
E wˆt−1−wt−1, ∇L(wt−1)+λ B1 (cid:88) ∇ vR j(v jt−1,wt−1) ≤E(cid:2) Ψ(wt−1)−F(v jt−1,wt−1)(cid:3) −LE(cid:13) (cid:13)wˆt−1−wt−1(cid:13) (cid:13).
j=1
Finally, putting pieces together and using the fact that ∇Ψ 1/4L(wt−1)=(cid:13) (cid:13)wˆt−1−wt−1(cid:13) (cid:13)/4L will conclude the
proof:
E[Ψ 1/2L(wt)]≤E[Ψ 1/2L(wt−1)]+4ηLE(cid:2) Ψ(wt−1)−F(v jt−1,wt−1)(cid:3) −4ηLE(cid:13) (cid:13)wˆt−1−wt−1(cid:13) (cid:13)+2η2L(G2+ δ n2 ′)
B
=E[Ψ (wt−1)]+4ηL1 (cid:88) E(cid:2) R (v∗(wt−1),wt−1)−R (vt−1,wt−1)(cid:3)
1/2L B j j j
j=1
−4ηL2E(cid:13) (cid:13)wˆt−1−wt−1(cid:13) (cid:13)+2η2L(G2+
δ2
).
n′Yuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi
Lemma 13 (Lemma D4 in [Lin et al., 2019]). If R (v,w) is convex and smooth in v, L smooth and G Lipschitz
j
in w, then under the dynamic of stochastic gradient descent ascent on v, we have the following statement holding:
E[R (v∗(wt−1),wt−1)−R (vt−1,wt−1)]
j j j
≤ηG(cid:112)
G2+δ2/n′(2t−2s−1)+
1 (cid:16) E(cid:13) (cid:13)v∗(ws)−vt−1(cid:13) (cid:13)2 −E(cid:13) (cid:13)v∗(ws)−vt(cid:13) (cid:13)2(cid:17)
2γ j j
γδ2
+E[R (vt,wt)−R (vt−1,wt−1)]+
j j 2n′
and
1 (cid:88)T
E[R (v∗(wt),wt)−R
(vt,wt)]≤ηGS2(cid:112)
G2+σ2+
D2
+
γδ2
+
max vR(v,w0)−R(v0,w0)
.
T +1 j j j 2Sγ 2n′ T +1
t=0
where D =max ∥v∥,
v∈V
E.2 Proof of Theorem 8
Now we are ready to present proof of Theorem 8 by putting the above results together.
Proof. Summing Lemma 12 from t=0 to T yields:
1 (cid:88)T E(cid:13) (cid:13)∇Ψ(wt)(cid:13) (cid:13)2
=
E[Ψ 1/2L(w0)]−E[Ψ 1/2L(wT)] +4L(cid:18) ηGS(cid:112)
G2+σ2+
D2
+
γδ2(cid:19)
T +1 η(T +1) 2Sγ 2
t=0
+41 (cid:88)B L(max vR j(v,w0)−R j(v0,w0)) +4ηL(cid:112)
G2+δ2/n′.
B T +1
j=1
(cid:113)
Setting S = D √ 1 yields:
2 ηγG G2+δ2/n′
1 (cid:88)T E(cid:13) (cid:13)∇Ψ(wt)(cid:13) (cid:13)2 =O(cid:32) E[Ψ 1/2L(w0)]−E[Ψ 1/2L(wT)](cid:33) +O LD(cid:115) ηG√ G2+δ2
+
Lγδ2

T +1 η(T +1) γ 2
t=0
+O(cid:18) L(max vR(v,w0)−R(v0,w0)) +ηL(cid:112) G2+δ2(cid:19)
.
T +1
(cid:16) (cid:17) (cid:16) (cid:17)
Finally, by choosing η =Θ ϵ6 and γ =Θ ϵ2 , we can guarantee the stationary of past iterates:
L3D2G Lδ2
T
T
+1 1(cid:88) E(cid:13)
(cid:13)Ψ
1/4L(wt)(cid:13)
(cid:13)≤ϵ,
t=0
with the gradient complexity bounded by
(cid:32) (cid:33)
BL3(G2+δ2/n′)D2δ2∆
O
Ψ1/4L
.
ϵ8
as stated.
F Experiment Details
Recall we utilize the Masked AutoEncoder (MAE) [He et al., 2022] as the base unsupervised pre-training method.
For models, we use the Tiny Vision Transform (TinyViT) [Wu et al., 2022] as the backbone for pre-trainingOn the Generalization Ability of Unsupervised Pretraining
and use a 10-way linear classifier on top of the encoder for fine-tuning. The encoder h sequentially contains
one convolutional layer, 12 192-head attention blocks, and one layer-normalization layer. The decoder g for
reconstructing images in MAE includes 4 192-head attention blocks followed by one linear layer.Details of
hyperparameters for the experiments reported in Table 1 are included in Table 2. For RadReg, we sample σ for 50
times and solve the inner maximization by Adam optimizer with a learning rate of 0.001 and a weight decay of
5×10−4.
Config Value
Optimizer AdamW
Base learning rate 1.5×10−4
Optimizer momentum β =0.9,0.95
Batch size 4096
Learning rate schedule cosine decay
Warmup epochs 200
Augmentation RandomResizedCrop
Masking ratio 75%
Pre-training epochs 2000
Fine-tuning epochs 300
Table 2: Pre-training setting. Fine-tuning follows the same setting except for the number of epochs.