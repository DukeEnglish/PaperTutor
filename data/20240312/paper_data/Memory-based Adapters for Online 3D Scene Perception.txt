Memory-based Adapters for Online 3D Scene Perception
XiuweiXu1*, ChongXia1*, ZiweiWang2, LinqingZhao3, YueqiDuan1, JieZhou1, JiwenLu1†
1TsinghuaUniversity, 2CarnegieMellonUniversity, 3TianjinUniversity
Online Semantic Segmentation Online Object Detection Online Instance Segmentation
wall
floor
cabinet
bed
chair
picture
counter
desk
curtain
refrigerator
sofa
table
door
window
bookshelf
shower curtain
toilet
sink
bathtub
other furniture
Figure 1. We propose a general framework for online 3D scene perception. With the presented memory-based adapters, we empower
existingofflinemodelsindifferenttaskswithonlineperceptionability,whichisvaluableforroboticsapplications.
Abstract strateourapproachachievesleadingperformanceonthree
3D scene perception tasks compared with state-of-the-art
Inthispaper,weproposeanewframeworkforonline3D onlinemethodsbysimplyfinetuningexistingofflinemodels,
sceneperception. Conventional3Dsceneperceptionmeth- withoutanymodelandtask-specificdesigns. Projectpage.
odsareoffline,i.e.,takeanalreadyreconstructed3Dscene
geometry as input, which is not applicable in robotic ap-
plicationswheretheinputdataisstreamingRGB-Dvideos
rather than a complete 3D scene reconstructed from pre- 1.Introduction
collectedRGB-Dvideos. Todealwithonline3Dsceneper-
3Dsceneperceptionaimstoparsea3Dsceneintosemantic
ception tasks where data collection and perception should
orobject-levelentities,mainlyincludingsemanticsegmen-
be performed simultaneously, the model should be able to
tation,objectdetectionandinstancesegmentation,whichis
process 3D scenes frame by frame and make use of the
thefundamentalabilityforroboticsorAR/VRapplications.
temporalinformation. Tothisend,weproposeanadapter-
Since PointNet [32] proposes the first model that directly
basedplug-and-playmoduleforthebackboneof3Dscene
process point clouds, great improvement on 3D scene per-
perception model, which constructs memory to cache and
ception[6,38,40,42,43]hasbeenachievedinrecentyears
aggregatetheextractedRGB-Dfeaturestoempoweroffline
byaccurateandefficientarchitecturedesign.
modelswithtemporallearningability. Specifically,wepro-
However,conventional3Dsceneperceptionmethodsare
poseaqueuedmemorymechanismtocachethesupporting
offline,i.e.,theytakeanalreadyreconstructed3Dscenege-
pointcloudandimagefeatures.Thenwedeviseaggregation
ometry from pre-collected RGB-D videos without tempo-
modules which directly perform on the memory and pass
ralinformationasinput. Whileinmostroboticapplication
temporalinformationtocurrentframe. Wefurtherpropose
suchasnavigation[4,47]andmanipulation[25]wherethe
3D-to-2D adapter to enhance image features with strong
agentisusuallyinitializedinanunknownenvironment,the
global context. Our adapters can be easily inserted into
input data is streaming RGB-D videos and scene percep-
mainstream offline architectures of different tasks and sig-
tion should be performed synchronously with data collec-
nificantly boost their performance on online tasks. Exten-
tiontoguidetheagenthowtoexplore.Therefore,online3D
siveexperimentsonScanNetandSceneNNdatasetsdemon-
scene perception model with temporal modeling ability is
*Equalcontribution. †Correspondingauthor. required, whichtakesinstreamingRGB-Dvideoandcon-
1
4202
raM
11
]VC.sc[
1v47960.3042:viXratinuously outputs the perception of the currently observed backboneof3Dsceneperceptionnetworks. Duetotheun-
3Dscene. Therearealsoafewonline3Dsceneperception orderedpropertyofpointclouddata, voxelizingthepoints
methods[16,22,24,26,46]designedforspecialarchitec- and applying convolution on 3D grids is a natural solu-
ture and task. As these methods only focus on temporal tion [3, 31]. However, the computational cost and mem-
aggregation for single modality, they cannot fully exploit oryrequirementbothincreasecubicallywithvoxelresolu-
temporalrelationsbetweenimageandpointcloudfeatures tion,whichisinefficient. PointNet[32]isthepioneerwork
andthustheirperformancesarenotsatisfactory. which directly extracts feature representations from raw
In this paper, we propose a new general framework for pointclouds. PointNet++proposessetabstractionandfea-
online3Dsceneperception. Differentfrompreviousworks turepropagationoperationbasedonPointNet,whichhelps
which design online perception approachs based on spe- learning more detailed local geometric information. As
cific architecture and task and train models on RGB-D thefurthestpointsamplingoperationinPointNet++istime
videos from scratch, we convert exsiting offline 3D per- consuming, PV-CNN [23] converts point clouds to low-
ception models to online ones by simply inserting plug- resolution voxels and apply 3D convolution to efficiently
and-play modules and finetuning. Taking inspiration from aggregate local features. Another way to extract high-
adapters [5, 29] which adapt image backbones to down- quality 3D features is sparse convolution [9, 10], which
stream tasks by additional parameter tuning, we propose voxelizes the point clouds but only apply 3D convolution
memory-based adapters to empower the backbone of 3D onnon-emptyvoxels. Tofurtherimprovetheefficiencyof
perceptionmodelwithtemporalmodelingabilitybyreusing sparseCNNs,submanifoldsparseconvolution[6,11]isin-
the extracted features from previous frames. Specifically, troduced,whichonlyconductsconvolutionwhenthecenter
we propose a queued memory mechanism to cache the ofkernelslidesoveractivesitesandkeepsthesamelevelof
supporting point cloud and image features for the RGB-D sparsity throughout the network. However, these methods
frameatcurrenttime.Basedonthestructureofmemory,we are designed for offline 3D scene perception, which is not
devise aggregation modules which directly operate on the abletoprocessastreamingRGB-Dvideoatrealtime.
memoryandpasstemporalinformationtocurrentframe.As
Streaming Data Analysis: As the input for online 3D
the global context of image features is limited, we further
sceneperceptionmodelisstreamingRGB-Dvideo,were-
propose 3D-to-2D adapter to enhance image features with
view the streaming data analysis methods for both image
3D memory projection and 2D sparse aggregation. In this
and point cloud domains. In 2D vision, many works [2,
way,wecanmakeuseoftheexistingmainstream3Dscene
8, 19] extend causal convolution [28] to streaming videos,
perceptionmodelzootoacquireaseriesofonlinemodels,
where a stream buffer is devised to cache previous frames
withsimpleinsertingandfinetuning. Weconductextensive
and3Dcausalconvolutionisappliedtounidirectionallyag-
experimentsonthreeonlineperceptiontasksonScanNet[7]
gregate spatial-temporal information. TSM [20] utilizes a
andSceneNN[15]datasets. Ourapproachachievesleading
moreefficientshiftmechanism,whereaproportionofchan-
performanceonalltasksanddatasetswithoutanyadditional
nelsofpreviousimagefeaturesareshiftedtothenextframe.
lossfunctionandspecialpredictionfusionstrategy.Tosum-
Thenthespatial-temporalinformationcanbeefficientlyag-
marize,ourcontributionsinclude:
gregatedby2Dconvolution.Ourimageadapteralsoutilizes
• Weproposeanewframeworkforonline3Dscenepercep-
channel shift for efficient temporal modeling. Differently,
tion,whichextendsexistingofflinemodelstoonlineones
TSM is trained from scratch where the network can learn
byadapterwithoutmodelandtask-specificdesign.
how to model temporal information according to the shift
• We propose general memory-based adapters for image
proportion. While we reorganize the channels and adopt
and point cloud backbones, which cache and aggregate
channel shift in a plug-and-play adapter to empower im-
extracted features to model the temporal relations be-
age backbone with temporal modeling ability. However,
tweenframes.
as 2D streaming videos contain less helpful information
• Equipped with our adapters, offline models are able to
for real world applications like robotic navigation [4, 47]
achieve leading performance on three tasks compared
and manipulation [25], increasing attention has been paid
withstate-of-the-artonlinemodels.
to 3D streaming RGB-D video analysis. A natural solu-
tion is to first process 2D images and then project the pre-
2.RelatedWork
dictions to 3D point clouds, which is followed by a fusion
3DScenePerception: 3Dsceneperceptioniswidelystud- steptomergethepredictionsfromdifferentframes[24,26].
ied in computer vision, which can be divided into three Fusion-aware3D-Conv[46]andSVCNN[16]maintainthe
mainstream tasks: semantic segmentation [6, 11, 32, 33], information of previous frames in 3D space and conduct
object detection [13, 34, 38, 43] and instance segmenta- point-basedconvolutiontofusethe3Dfeaturesforsemantic
tion [17, 18, 40, 42, 45]. As we focus on the feature ex- segmentation. INS-CONV[22]extendssparseconvolution
traction of 3D scene in this work, we mainly discuss the toincrementalCNNtoefficientlyextractglobal3Dfeatures
2...
result t-1
input t-1 Detailed
3D-2D-adapter
I-memory I-adapter
Image
+
Backbone
T Fusion
a
s
k
H
Detailed e
a
P-memory P-adapter d
Pointcloud
+
Backbone
3D-2D-adapter
input t
result t
...
Figure2.Overallarchitectureofourapproach.Weinsertmemory-basedadaptersafterimageandpointcloudbackbones,whichcachethe
extractedfeaturesinmemoryovertimeandperformtemporalaggregation. 3D-to-2Dadapterisproposedtofurtherexploitinter-modal
temporalinformation.Solidlinesindicateoperationswithinasingleframe,whiledashedlinesindicatetemporaloperations.
for semantic and instance segmentation. Differently, our design of 3D perception models, most of them only fo-
approach empowers offline model with online perception cus on two scenarios: (1) Reconstructed scenes [1, 7].
ability by image and point cloud memory-based adapters, The model M is trained on point clouds of complete
Rec
whichfullyexploitsthemultimodaltemporalrelations. scenesreconstructedfromRGB-Dvideos. (2)Single-view
scenes[27,41]. ThemodelM istrainedonsingle-view
SV
3.Approach pointcloudsback-projectedfromRGB-Dimage. However,
M requires the input to be a complete scene, which is
Rec
Inthissection,wefirstintroducethedefinitionofonline3D
not accessible in real-time tasks. M is able to process
SV
scene perception and explain our motivation of memory-
RGB-Dvideosframe-by-frame,butfailstoexploittemporal
basedadapter. Thenwedescribehowtoconstructmemory
information. Therefore, previous 3D models are not ready
andrefinethebackbonefeaturesbyadapterforpointcloud
forthemorepracticalonlinesceneperception.
andimagerespectively.
To this end, we aim to devise a plug-and-play temporal
3.1.Online3DScenePerception learningmodule,whichcanbeinsertedintoanysingle-view
perception model M and empowers it with temporal
Let X = {x ,x ,...,x } be a posed RGB-D streaming SV
t 1 2 t
modelingability. NotethatM isoriginallya3Dpercep-
video,whichmeansthevideoiscollectedwiththemoving SV
tionmodel. WecanextendittoaRGB-Dperceptionmodel
ofsensorratherthanapre-collectedvideo. Wehave:
byearlyfusingtheimagefeaturestopointclouds[39]:
x =(I ,P ,M ), I ∈RH×W×3, P ∈RN×3, M ∈R3×4
t t t t t t t
(1)
p =M (P′),
whereI tandP trefertotheimageandpointcloudsforone t SV t
RGB-Dframe. P t isacquiredbyliftingthedepthimageto P t′ =P t⊕S(M I(I t),P t,M t), S(·)∈RN×C (2)
world coordinate system with pose parameters M , where
t
M canbeestimatedbyvisualodometry[30,48]. Attime
t
t, the input to online perception model is X and the out- wherep isthepredictionforinputframeattimet. M is
t t I
putispredictionsfortheobserved3DsceneS
=(cid:83)t
P , a image backbone pretrained on the same perception task
t i=1 i
whichcanbeboundingboxesandsemantic/instancemasks. as M . S projects P to image coordinate system by
SV t
Someworksalsoperform3Dreconstructionalongwithon- M and samples the corresponding 2D features to enrich
t
lineperception[22]toacquirehigh-qualitypointcloudsor the features of P . We divide M into a backbone M
t SV P
meshes,butthisisnotrequired[4,35,47]. Inthisworkwe for extracting features of point clouds and a task-specific
donotrelyon3DreconstructionanddirectlytakeRGB-D head M . The goal of this work is to construct an im-
H
streamingvideoasinput,whichisamoregeneralsetting. age memory-based adapter for M (I ) and a point cloud
I t
Although great improvement has been achieved in the memory-based adapter for M (P′) to store and reuse the
P t
3moving; (2) the number of points will be very large over
add
time, so point-based sampling and feature aggregate take
sparsify map uphighcomputationoverhead. Tothisend,weproposeto
aggregate storethefeaturesinaquantitizedcoordinatesystem,where
pointcloudsarevoxelizedandstoredin3Dgrids. Wealso
Input feature query Point cloud adapter
maintainthevoxelsinaqueuetoreducethememoryfoot-
print when the scene is too large. Specifically, M (P′)
P t
merge update is first voxelized into the voxel grids V t by averaging all
voxelize features whose coordinates falls into the same grid. These
voxels are tagged with timestamp t. Then we merge V to
t
Point cloud memory previous memory updated memory thememorymP t−1bymaxpooling:
Figure3. Thearchitectureofthememory-basedadapterforpoint mP
t
=maxpooling(V t,mP t−1),
cloudfeatures. Wecacheandaggregatethefeaturesinaqueueof mP =deq(mP,l) if N(mP)>N (4)
3Dvoxelgrids. Gray,green,yellowandredblockrefertoprevi- t t t max
ous,current,updatedandaggregatedvoxelfeatures.
wheremaxpoolingreferstochannel-wisemaxpoolingcon-
ducted on each voxel grid, which will update both fea-
extractedbackbonefeaturesfortemporalmodeling: turesandtimestamps.deq(·,l)meansremovingvoxelswith
timestampearlierthant−l+1fromthememory. N(mP)
M (I ),mI =A (M (I ),mI ,mP ), t
I t t I I t t−1 t−1 is the number of voxels in the memory. We utilize max-
M (P′),mP =A (M (P′),mP ) (3) poolingasitpreservesthemostdiscriminativefeaturesover
P t t P P t t−1
time, whichisalsoefficienttocomputeasonlyfeaturesat
wherethememory-basedadapterAupdatesthememorym
t timet−1andtarerequired.
withcurrentfeaturesandrefinescurrentfeaturesbyreusing
Memory-based adapter: After caching and updating
thememory. Wefullyexploitinter-modalandintra-modal
thepointcloudfeaturesinvoxels, weneedtomakeuseof
relationships: M fuses image features to point clouds,
SV mP toenhanceM (P′)withtemporalinformation.Toex-
A reuses previous point cloud features to refine current t P t
P ploitrichscenecontextfromthememorymP whilereduce
pointcloudfeatures,andA reusespreviousfeaturesfrom t
I redundantcomputation,wefirstusethecoordinatesofV to
t
bothmodaltorefinethecurrentimagefeatures. Asshown
queryaneighborvoxelset:
in Figure 2, we follow the same paradigm to design both
modules, which can be easily embedded into image and N(V )={mP[x][y][z]|(x,y,z)∈s∗B(V )} (5)
t t t
pointcloudbackbonestoachievetemporalmodeling.
where N(V ) is the queired neighborhood of V .
t t
3.2.TemporalModelingforPointClouds
mP[x][y][z] refers to the voxel in mP at coordinate
t t
Given {M (P′),M (P′),...,M (P′)} at time t, i.e., (x,y,z). B istheminimumenclosingaxis-alignedbound-
P 1 P 2 P t
the sequence of point cloud features extracted from the ing box of V t and s is a scaling factor to enlarge the size
backbone,weaimtoenhancethecurrentfeaturesM (P′) of box. In this way, the temporal information which pro-
P t
by exploiting the temporal relations within this sequence. videssupportinggeometricinformationforcurrentframeis
Here we first construct a memory to efficiently cache the collectedintothisvoxelset.
pointcloudfeaturesfromdifferenttimestamp. Thenweag- WethenconvertN(V t)toasparsetensor[6,11],which
gregate the temporal information from the memory to fea- isfollowedbya3DsparseconvolutionalmoduleA P toag-
turesatcurrenttimetwithaplug-and-playadapter. gregate the context information within N(V t) to locations
Memory construction: The temporal information for of V t. Finally we update M P(P t′) in an adapter-manner:
3D scenes is reflected in a more complete geometry. As a (1)Wemaptheaggregatedfeaturesbacktothecoordinates
singleRGB-Dframemaynotcontainacompletelargeob- of M P(P t′) and then add it to the original features with
jectsorhigh-levelscenecontext,thegeometricinformation residual connection; (2) The adapter module A P is zero-
frompreviousframesareimportantforaccurateperception initialized. Thereforeafterinsertingtheadapter,finetuning
ofcurrentframe. Therefore,wecancachethesequenceof willsmoothlystartfromtheoriginalpoint.
extractedpointcloudfeaturesinashared3Dspace. Asim-
3.3.TemporalModelingforImages
ple way is to directly store the features in terms of point
cloudsintheworldcoordinatesystem. However,thisisin- Forthesequenceofimagefeatures{M (I ),...,M (I )}
I 1 I t
efficient in both storage and computation: (1) as the coor- at time t, we follow the same paradigm as the point cloud
dinatesofthepointcloudarerealvalues,thestorageover- counterparttostoreitwithamemoryandaggregatetempo-
head will keep growing even if the RGB-D camera is not ralinformationtocurrentframeM (I )withanadapter.
I t
4channelswithpreviousmemorymI .Inthisway,thetem-
Image memory fromsh i mft e i mn ory... add poral information of the adjacentt− tw1 o frames are merged
intoasingleframe,forwhichwecandirectlyadopt2Dcon-
reorganize shift out volutiontoaggregatethefeatures:
shift to memory
F =2D-Conv(mI ⊕(M (I )·R ) )·R (7)
Input feature add Image adapter t t−1 I t 1 [:,:,C τ′:] 2
whereR ∈RC′×C isalearnableinversetransformationto
2
map image features back to the original embedding space.
project aggregate FinallyweupdateM (I )byaddingF witharesidualcon-
I t t
densify
nection. We also zero-initialize R , R and 2D-Conv for
1 2
smoothfinetuning.
3D memory
3D to 2D adapter
3.4.Inter-modalTemporalModeling
Figure4.Thearchitectureofthememory-basedadapterforimage Although maintaining a short queue and adopting channel
features. Wereorganizetheinputfeaturesandshiftoutapropor- shiftareabletoeffectivelyaggregatetemporalinformation
tionofchannelsintothememory,whileshiftinginpreviousmem-
for image features, the global context is limited. This will
oryandaggregatingtemporalinformationby2Dconvolution.We
leadtoaperformancedropwhenanobjectisverylargeor
alsoresorttothe3Dmemoryformoreglobalcontext.
theRGB-Dcamerastopsmoving.However,asanalyzedbe-
fore,extractingrichglobalcontextfromastreamingvideo
Memory construction: Different from 3D data where isreallymemoryandcomputationconsuming.Tosolvethis
point clouds from different timestamps can be stored in a problem, we resort to the point cloud memory for global
shared3Dspace,for2Ddatatheimagefeaturesarestacked contextextraction, becausethepointcloudfeaturesareef-
intoastreamingvideo. Acommonpracticetoprocessthis ficientlycachedinashared3Dspaceandthusthelengthof
kindofdata[19]istomaintainaqueueandperformcausal queuecanbelong. Wedevisea3D-to-2Dadaptertorefine
convolution (unidirectional along temporal dimension) to currentimagefeatureswiththeglobal3Dfeatures.Herewe
aggregate the information from previous frames to current first project mP to the discrete image coordinate system
t−1
imagefeatures. However,videoanalysismethodsfocuson withtheinversefunctionofS,whicharethenconvertedto
extractingtheglobalinformationofthestreamingvideoup sparse tensor. In this way, we keep the sparsity of point
tonow,whileinourcaseweonlyneedtoenhancethecur- cloud memory and make the 2D features geometry-aware.
rentimagefeaturesM (I ). Socausalconvolutiononpre- 2D sparse convolution is then applied on the sparse tensor
I t
viousframeswillbringalargeamountofredundantcompu- toaggregatethecontextinformation,followedbyadensify
tation. Moreover, in online 3D scene perception, the most operation to keep features inside image and pad other pix-
importantinformationforimagefeaturesistheobservation els with zero. Finally we add the densified 2D features to
of objects from multiple perspectives. Since an object is enhanceM (I )·R withricherglobalcontext. Wezero-
I t 1
usually observed in a few adjacent frames, maintaining a initializethe2Dsparseconvolutionforsmoothtraining.
shortqueueisenoughinmostcases. Tothisend,wemake To acquire the final online perception results, a predic-
anextremesimplification:weonlystoreoneframeofprevi- tion fusion strategy is needed. As the focus of our work
ousimageandexploittemporalrelationsbetweenframesat isthetemporallearningmodulesforimageandpointcloud
timet−1andt.Inordertoefficientlyaggregatetheadjacent backbones,weonlyadoptasimplepost-processingstrategy
frames,weadoptchannelshifttocachethetemporalinfor- tofusethepredictionsofeachframeinawhole,whichwe
mation. Formally, givenM (I ) ∈ RH×W×C, welearna detailinSection4.1.
I t
linear transformation R ∈ RC×C′ to map the image fea-
1
turesintoanotherembeddingspace,wherethefirst 1 chan- 4.Experiment
τ
nels contain rich temporal information relevant to the next
Inthissection,wefirstdescribeourdatasetsandimplemen-
frame. Thereforethememorycanbesimplyconstructedby
tationdetails. Thenwecompareourmethodwithstate-of-
shiftingoutthispartofchannels:
the-artmethodsonbothroom-levelandonlinebenchmarks
mI =(M (I )·R ) (6) to comprehensively analyze the advantage of memory-
t I t 1 [:,:,:C′]
τ basedadapters. Finallyweconductablationstudiestovali-
Note that this operation is repeated frame-by-frame, thus datetheeffectivenessofourdesign.
mI containsthetemporalinformationrelevantwithcur-
t−1 4.1.BenchmarksandImplementationDetails
rentfeaturesM (I ).
I t
Memory-based adapter: At time t, after shifting out We evaluate our method on two datasets: ScanNet [7] and
aproportionofchannelstomemory,wecanpadtheempty SceneNN [15]. ScanNet contains 1513 scanned scene se-
5quences, out of which we use 1201 sequences for training Table1. 3DsemanticsegmentationresultsonScanNetandSce-
and the rest 312 for testing. SceneNN is a smaller dataset neNN datasets. For online methods, we map the predictions on
point clouds concatenated from posed RGB-D images to the re-
which contains 50 high-quality scanned scene sequences
constructedpointcloudstocomparewithofflinemethod.
with semantic label. After careful filtering, we select 12
clean sequences for testing. We train all models on Scan-
ScanNet SceneNN
NetandevaluatethemonScanNetorSceneNN. Method Type
mIoU mAcc mIoU mAcc
Benchmarks: We first compare different methods on
room-level benchmarks, i.e., the performance on the re- MkNet[6] Offline 71.6 80.4 – –
constructed complete scenes. For semantic segmentation,
Fs-A[46] Online 63.5 73.7 51.1 62.4
we compare different methods on ScanNet and SceneNN.
MkNet-SV Online 68.8 77.7 48.4 61.2
Since online methods may not perform 3D reconstruction,
MkNet-SV+Ours Online 72.7 84.1 56.7 70.1
wemaptheirpredictionsonpointcloudsconcatenatedfrom
posedRGB-Dimagestothereconstructedpointcloudswith
Table2. 3Dobjectdetectionandinstancesegmentationresultson
nearestneighborinterpolation. Forobjectdetectionandin-
ScanNetdataset.Offlineandonlinemethodsareseparatedbyhor-
stancesegmentation,themetriciscomputedoneachobject
izontalline. † meansINS-Convrequiresanadditional3Drecon-
rather than the whole point clouds. Therefore we use re-
structionalgorithmtoacquirehigh-qualitypointcloudsormeshes.
constructed point clouds and RGB-D videos as the inputs
for offline and online methods respectively, and calculate
Detection Insseg
metricsbasedontheirrespectiveinputs.
mAP mAP
We also follow AnyView [44] to organize an online Method Method
@25 @50 @25 @50
benchmarkonScanNetformorecomprehensiveevaluation.
WedividetheRGB-Dvideoofeachroomintoseveralnon- FCAF3D[38] 70.7 56.0 SoftGroup[42] 78.9 67.6
overlapping sequences and regard each sequence as an in- CAGroup3D[43] 74.5 60.3 TD3D[18] 81.3 71.1
dependent scene, where the number or the length of each AnyView[44] 60.4 36.0 INS-Conv†[22] – 57.4
sequence can be set to different values. In this way, we FCAF3D-SV 41.9 20.6 TD3D-SV 53.7 36.8
canmeasurethegeneralizationabilityofdifferentmethods FCAF3D-SV+Ours 70.5 49.9 TD3D-SV+Ours 71.3 60.5
whentheinputscenesareincompleteandofvariablescales,
which is a more practical setting. In our experiments, we
divide each room into 1/5/10 sequences or sequences with wise maxpooling. Object detection: The predicted bound-
fixedlength5/10/15,resultingin6metrics. ing boxes for each frame are merged by 3D NMS. When
Implementation details: To train M , we first train two boxes of different time are colliding during NMS, we
SV
a 2D perception model M following Pri3D [14]. We addδtotheclassificationscoresofboxinthenewerframe.
I
use UNet [37] for semantic segmentation and Faster- This is because our method ensures the backbone extracts
RCNN [36] (only ResNet [12] and FPN [21] backbones morecompletegeometricfeaturesforthenewerframe. In-
are needed) for object detection and instance segmenta- stancesegmentation: Instancesegmentationcanbedivided
tion. Then we fix the image backbone and train M on into transformer-based [40], grouping-based [17, 42] and
SV
ScanNet-25k [7], which is a single-view RGB-D dataset. detection-based[13,18,45]. Asthefirsttwokindsrequire
Foronlineperception,wezero-initializethememory-based a specially designed mask fusion strategy [22] for online
adaptersandinsertthemintoM . Thenwetrainthenew perception, we opt for the detection-based manner, where
SV
modelonRGB-DvideosfromScanNet. Toreducememory wecanfirstconductonline3Dobjectdetectionandthenut-
footprint, we randomly sample 8 adjacent RGB-D frames lizetheboxestocropandsegmentthepointcloudfeatures
for each scene at every iteration. We insert our memory- storedinthememory.
basedadaptersbetweenthebackboneandneck. Forback-
4.2.ComparisonwithState-of-the-art
boneswhichoutputmulti-levelfeatures,weinsertdifferent
adapters to different levels. In terms of hyperparameters, We compare our method with the top-performance offline
wesetl=50,s=2.5,τ =8andδ =0.03. Wesimplyuse and online 3D perception models. Offline models refer to
thesameoptimizerconfigurationstotrainthemodelsasin M describedinSection3.1,whichistrainedonrecon-
Rec
theiroriginalpaper(designedforofflinetraining). structed point clouds. Models with suffix ”-SV” refer to
For prediction fusion, we adopt different strategies for M thatistrainedonsingle-viewRGB-Dimages.
SV
different tasks. Semantic segmentation: The predictions Room-level benchmarks: By default, offline methods
foreachframeareconcatenated,whichhasthesamepoint takeinreconstructedpointcloudsandonlinemethodstake
numberwithS . Weuse2cmvoxelizationtounifythepre- in posed RGB-D videos without 3D reconstruction. Spe-
t
dictions for points inside the same voxel grid by channel- cial case is denoted by †. Note that there is a challenge in
6Table3. Theperformanceofdifferent3DsceneperceptionmethodsonScanNetonlinebenchmark. WereportmIoU/mAcc,mAP@25/
mAP@50andmAP@25/mAP@50forsemanticsegmentation,objectdetectionandinstancesegmentationrespectively.
NumberofSequence LengthofSequence
Method Type
1 5 10 5 10 15
MkNet Offline 63.7/73.5 62.7/72.8 58.9/69.4 59.3/69.8 63.0/73.0 63.5/73.7
Fs-A Online 62.0/72.8 60.6/71.7 60.0/71.3 60.1/71.3 60.7/71.8 61.0/72.0
MkNet-SV Online 63.3/74.3 63.3/74.3 63.3/74.3 63.3/74.3 63.3/74.3 63.3/74.3
MkNet-SV+Ours Online 69.1/82.2 66.8/80.0 65.9/79.2 65.9/79.3 66.8/80.1 67.1/80.4
FCAF3D Offline 57.0/40.6 41.1/25.2 34.6/19.3 28.4/15.2 33.9/19.4 37.7/22.8
AnyView Online 60.4/36.0 48.8/25.3 43.1/20.5 36.6/16.5 42.0/20.7 45.6/23.8
FCAF3D-SV Online 41.9/20.6 29.8/13.3 27.0/11.5 24.4/10.1 26.2/11.0 27.6/12.1
FCAF3D-SV+Ours Online 70.5/49.9 58.7/37.7 56.2/34.3 53.1/31.2 54.9/33.8 56.1/35.6
TD3D Offline 64.0/50.8 61.6/49.7 59.4/48.4 59.0/47.9 61.4/49.8 61.7/49.8
TD3D-SV Online 53.7/36.8 54.2/41.6 57.0/46.3 56.4/45.5 53.9/40.9 52.6/39.5
TD3D-SV+Ours Online 71.3/60.5 64.7/55.2 64.2/55.0 64.0/54.7 64.6/55.1 63.9/54.3
onlinemethodswhencomparetoofflinealternatives,asof- Table4.Ablationstudyonpointcloudandimagemodules.Were-
fline methods directly process the complete and clean 3D portsemanticsegmentationresultsonScanNet. Theperformance
ofimagemoduleisbasedonpointcloudmodule.
geometry of rooms while online methods deal with partial
and noisy frames. According to Table 1 and Table 2, by
simplyinsertingthememory-basedadaptersintoM ,we Method mIoU mAcc
SV
significantly boost their accuracy on complete scenes and Removeresidualconnection 64.6 77.9
Randominitialization 66.2 78.6
achieve better performance compared with state-of-the-art
Removevoxelmaxpooling 64.8 76.1
online 3D scene perception models specially designed for
Setscalingfactors=1 65.3 78.4
eachtask. WeobservetheimprovementuponM SV ises- Setscalingfactors=5 66.8 79.3
peciallysignificanton3Dobjectdetectionandinstanceseg- Insertafterneck 66.0 78.8
Thefinalpointcloudmodule 66.9 79.3
mentation tasks. This is because these tasks require com-
plete predictions for each object, while it is usually very Removeresidualconnection 67.1 79.8
Randominitialization 68.7 81.7
hardtoinferthewholegeometryoflargeobjectswithasin-
Setshiftratioτ =4 68.9 82.1
gleRGB-Dframe. Wealsonoticeourmethodevenoutper-
Setshiftratioτ =16 68.7 81.9
formsofflinemethodsonsemanticsegmentationtask.Since Remove3Dto2Dadapter 68.0 80.8
thistaskrequiresmoredetailedperceptionoflocalgeome- Insertafterneck 68.4 81.6
Thefinalimagemodule 69.1 82.2
try rather than the global context, our method can predict
finersegmentationwithonlypartialandnoisyinputs.
Table5. Effectsofourmemory-basedadapterswhenbothimage
Onlinebenchmark: Inthisbenchmark,theinputstoall andpointcloudbackbonesarefixedduringfinetuning.
methodsaretheposedRGB-Dsequences. Weconcatenate
thepointcloudsfromeachRGB-Dframeofasequenceinto MkNet FCAF3D TD3D
awholeforofflinemethods.AsthecodeofINS-Convisnot FixI 69.1/82.2 70.5/49.9 71.3/60.5
accessible, we do not compare with it on this benchmark. FixP&I 67.3/79.9 66.4/47.1 69.1/58.2
AccordingtoTable3,offlinemethodsshowbadgeneraliza-
tionabilityonpartialandnoisyscenes,especiallywhenthe
input sequence is short. Note that offline methods take in M SV, which validates our modules can effectively aggre-
thewholeobservedsceneS ateachtime.Whenprocessing gatelong-termtemporalinformation.
t
S , the features extracted for S is wasted. On the con- WevisualizethepredictionsofdifferentmethodsinFig-
t+1 t
trary,onlinemethodsprocessasingleframex ateachtime ure5. Itcanbeseenthatourmethodismoreaccuratethan
t
andfusetheper-framepredictions,whichismuchmoreef- M SV duetothetemporalmodelingability,andmorerobust
ficient and practical in real-time robotic tasks. Equipped tonumberofframesthanofflinemethods.
with our memory-based adapters, M achieves the best
SV 4.3.AblationStudy
performancecomparedwithotherofflineandonlinemeth-
odsonalltasksandexperimentalsettings. Weobservethe We first ablate the design choices of two memory-based
longertheinputsequence,thelargertheimprovementupon adapters on 3D semantic segmentation task on ScanNet.
7
gesmeS
noitceteD
gessnISemantic Segmentation Object Detection Instance Segmentation
h
urt
d-t
n
u
o
Gr
e
n
Offli
V
S
s
ur
O
Figure5. Visualizationresultsontheonlinebenchmark. Ourpredictionsareaccurateandrobusttothenumberofframes. Notethatsome
ground-truthmasksareincompleteduetothenoisy2Dannotations,inthiscaseourpredictionsaremorereasonablethantheground-truths.
Besides, we further show the performance of our method 5.Conclusion
whenbothimageandpointcloudbackbonesarefixeddur-
Inthispaper,wehavepresentedmemory-basedadaptersfor
ingfinetuningtheadapters.
online3Dsceneperception. Mainstream3Dscenepercep-
Pointcloudandimagemodules: Table4validatesthe
tionmethodsareoffline,whichishardtobeappliedinmost
effectiveness of our designs. We observe removing voxel
real-timeapplicationswhereonlystreamingRGB-Dvideo
maxpoolingsignificantlydegradestheperformance, which
is accessible. Existing online perception methods design
shows the importance of updating memory. With the in-
model and task-specific temporal learning approaches, but
creaseofs, theperformancefirstimprovesandthenkeeps
mostofthemonlyfocusontemporalaggregationforsingle
steadyorevenslightlydeclines,whichindicatestheneigh-
modality and thus cannot fully exploit temporal relations
bor context information is important for temporal learn-
between image and point cloud features. To this end, we
ing, but too large neighbor voxel set brings much redun-
propose plug-and-play temporal learning modules, which
dant features. Large s will also increase the computa-
canempowerofflinemethodswithonlineperceptionability
tion overhead, so we choose s = 2.5 to achieve the best
bysimplyinsertingmemory-basedadaptersandfinetuning
accuracy-computation tradeoff. We observe the influence
onRGB-Dvideos. Specifically, givenpointcloudandim-
of τ is similar with s and thus choosing a proper value is
age features extracted from the backbones, we first devise
importantforbothhighaccuracyandlessmemorystorage.
a queued memory mechanism to cache these information
Fromtheseexperiments, wealsovalidatetheeffectiveness
over time and maintaining a reasonable storage overhead.
ofthe’adapterparadigm’,whichincludesresidualconnec-
Then we devise aggregation modules which directly oper-
tion,zero-initializationandinsertingafterbackbone.
ateonthememoryandpasstemporalinformationfromthe
Fixed backbones: When finetuning our adapters, we
cached features to current frame. As the global context of
fixtheimagebackboneandfinetuneotherparameters. We
imagefeaturesislimitedduetotheshortqueue,wefurther
further study the effects of our method when both image
propose 3D-to-2D adapter to enhance image features with
and point cloud backbones are fixed. As shown in Table
3Dmemory.WeconductextensiveexperimentsonScanNet
5, even with both image and point cloud backbones fixed,
andSceneNN.Byequippingofflinemodelswithourmod-
our method still achieves state-of-the-art performance on
ules, we achieve leading performance on three scene per-
all three online tasks. In this way, we can further reduce
ception tasks compared with state-of-the-art online meth-
thememoryfootprintandtrainingtime,whichprovidesthe
ods,evenwithoutanymodelandtask-specificdesigns.
userswithmoreefficiency-accuracytradeoff.
8SupplementaryMaterial
Input Prediction Feature Seg head Det head Memory Adapter
DownConv Conv UpConv Interpolate ROI extracter Add CConcatenate
Thissupplementarymaterialisorganizedasfollows:
• Section A demonstrates the detailed architecture of our (A)
baseline models in three tasks and how to insert our TD3D
adaptersintothem.
• SectionBdetailsthetraininghyperparametersadoptedin
ourexperiments.
• SectionCdetailsper-classexperimentalresults.
(B)
A.DetailedArchitecture FCAF3D
We illustrate the architectures of both image and point
cloudbackbonesandshowhowtoinsertthememory-based
adaptersintotheminFigure6. Foronline3Dsemanticseg- (C) C
mentation, we use U-Net [37] as the image backbone and MinkUnet C
Minkowski-UNet[6]asthepointcloudbackbone,whichis C
showninFigure6(D)and(C)respectively. Foronline3D
object detection, we adopt ResNet [12] with FPN [21] as
the image backbone and FCAF3D [38] as the point cloud (D)
Unet
backbone, which is shown in Figure 6 (E) and (B) respec-
tively. For online 3D instance segmentation, we use the
sameimagebackboneastheobjectdetectiontaskandadopt
TD3D [18] as the point cloud backbone, which is shown
(E)
in Figure 6 (E) and (A) respectively. Note that for TD3D,
ResNet+FPN
thebackbonemaintainsahigh-resolutionscenerepresenta-
tionforROI-wiseinstanceprediction. Weconsrtuctapoint
cloudmemorytocachethisscenerepresentation,whichen-
sures the point clouds within each ROI are the most com- Figure6.Detailsaboutthearchitecturesofimageandpointcloud
pleteuptocurrenttime. Thisdesignhelpsusacquirecom- backbonesandhowtoinserttheadaptersintothem.
pleteinstancemaskbysimplyperforming3DNMS,which
avoids complicated mask fusion strategy [22] to merge in-
stepsat28and32epochforthefirststage. Thenweadopt
stancemasksofdifferentframes.
thesamehyperparametersforfinetuning.
B.TrainingHyperparameters
C.Class-specificResults
Wetraintheonlineperceptionmodelsintwostage. Firstly
We provide class-specific experimental results of out
we train single-view perception model M on ScanNet-
SV methodonthree3Dsceneperceptiontasks. Table6and7
25k[7].Secondlyweinsertthememory-basedadaptersinto
showthe3DsemanticsegmentationresultsonScanNetand
M andfinetunethenetworkonScanNetRGB-Dvideos.
SV SceneNN dataset with per-class IoU. Table 8 and 9 show
Foronlinesemanticsegmentation,wesetmaxepochas
the3DobjectdetectionresultsonScanNetdatasetwithper-
250,weightdecayas0.01,initiallearningrateas0.0008and
classAP andAP . Table10and11showthe3Dobject
adopt AdamW optimizer with OneCycleLR scheduler for 25 50
detection results on ScanNet dataset with per-class AP
thefirststage. Thenwesetmaxepochas36,weightdecay 25
andAP .
as 0.01, initial learning rate as 0.008 and adopt AdamW 50
optimizer with a stepwise scheduler which steps at 24 and
32epochforthesecondstage.
For online object detection, we set max epoch as 12,
weight decay as 0.0001, initial learning rate as 0.001 and
adopt AdamW optimizer with a stepwise scheduler which
stepsat8and11epochforthefirststage.Thenweadoptthe
samehyperparametersforfinetuninginthesecondstage.
For online instance segmentation, we set max epoch as
33,weightdecayas0.0001,initiallearningrateas0.001and
adopt AdamW optimizer with a stepwise scheduler which
9References [16] Shi-ShengHuang,Ze-YuMa,Tai-JiangMu,HongboFu,and
Shi-MinHu. Supervoxelconvolutionforonline3dsemantic
[1] IroArmeni,OzanSener,AmirRZamir,HelenJiang,Ioannis
segmentation. TOG,40(3):1–15,2021. 2
Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic
[17] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-
parsingoflarge-scaleindoorspaces. InICCV,pages1534–
WingFu,andJiayaJia.Pointgroup:Dual-setpointgrouping
1543,2016. 3
for3dinstancesegmentation. InCVPR,pages4867–4876,
[2] JoaoCarreira,VioricaPatraucean,LaurentMazare,Andrew 2020. 2,6
Zisserman, and Simon Osindero. Massively parallel video [18] MaksimKolodiazhnyi,DanilaRukhovich,AnnaVorontsova,
networks. InECCV,pages649–666,2018. 2 and Anton Konushin. Top-down beats bottom-up in 3d
[3] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, instance segmentation. arXiv preprint arXiv:2302.02871,
Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, 2023. 2,6,9
Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: [19] Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang,
An information-rich 3d model repository. arXiv preprint Mingxing Tan, Matthew Brown, and Boqing Gong.
arXiv:1512.03012,2015. 2 Movinets: Mobilevideonetworksforefficientvideorecog-
[4] DevendraSinghChaplot,DhirajPrakashchandGandhi,Ab- nition. InCVPR,pages16020–16030,2021. 2,5
hinavGupta,andRussRSalakhutdinov.Objectgoalnaviga- [20] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift
tionusinggoal-orientedsemanticexploration. NeurIPS,33: module for efficient video understanding. In ICCV, pages
4247–4258,2020. 1,2,3 7083–7093,2019. 2
[21] Tsung-Yi Lin, Piotr Dolla´r, Ross Girshick, Kaiming He,
[5] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong
Bharath Hariharan, and Serge Belongie. Feature pyramid
Lu,JifengDai,andYuQiao. Visiontransformeradapterfor
networksforobjectdetection. InCVPR,pages2117–2125,
densepredictions. arXivpreprintarXiv:2205.08534, 2022.
2017. 6,9
2
[22] LeyaoLiu,TianZheng,Yun-JouLin,KaiNi,andLuFang.
[6] ChristopherChoy,JunYoungGwak,andSilvioSavarese. 4d
Ins-conv: Incrementalsparseconvolutionforonline3dseg-
spatio-temporal convnets: Minkowski convolutional neural
mentation. In CVPR, pages 18975–18984, 2022. 2, 3, 6,
networks. InCVPR,pages3075–3084,2019. 1,2,4,6,9
9
[7] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-
[23] ZhijianLiu,HaotianTang,YujunLin,andSongHan. Point-
ber, Thomas Funkhouser, and Matthias Nießner. Scannet:
voxelcnnforefficient3ddeeplearning. InNeurIPS,pages
Richly-annotated 3d reconstructions of indoor scenes. In
963–973,2019. 2
CVPR,pages5828—-5839,2017. 2,3,5,6,9
[24] JohnMcCormac,AnkurHanda,AndrewDavison,andSte-
[8] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, fanLeutenegger. Semanticfusion: Dense3dsemanticmap-
Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: ping with convolutional neural networks. In ICRA, pages
Attentive language models beyond a fixed-length context. 4628–4635.IEEE,2017. 2
arXivpreprintarXiv:1901.02860,2019. 2 [25] ArsalanMousavian,ClemensEppner,andDieterFox. 6-dof
[9] Martin Engelcke, Dushyant Rao, Dominic Zeng Wang, graspnet: Variationalgraspgenerationforobjectmanipula-
Chi Hay Tong, and Ingmar Posner. Vote3deep: Fast ob- tion. InICCV,pages2901–2910,2019. 1,2
jectdetectionin3dpointcloudsusingefficientconvolutional [26] GakuNarita,TakashiSeno,TomoyaIshikawa,andYohsuke
neuralnetworks. InICRA,pages1355–1361,2017. 2 Kaji. Panopticfusion: Onlinevolumetricsemanticmapping
[10] Benjamin Graham. Spatially-sparse convolutional neural atthelevelofstuffandthings. InIROS,pages4205–4212.
networks. arXivpreprintarXiv:1409.6070,2014. 2 IEEE,2019. 2
[27] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob
[11] Benjamin Graham, Martin Engelcke, and Laurens Van
Fergus. Indoor segmentation and support inference from
Der Maaten. 3d semantic segmentation with submanifold
rgbdimages. InECCV,2012. 3
sparseconvolutionalnetworks. InCVPR,pages9224–9232,
[28] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen
2018. 2,4
Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner,
[12] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
AndrewSenior,andKorayKavukcuoglu.Wavenet:Agener-
Deep residual learning for image recognition. In CVPR,
ativemodelforrawaudio.arXivpreprintarXiv:1609.03499,
pages770–778,2016. 6,9
2016. 2
[13] JiHou,AngelaDai,andMatthiasNießner.3d-sis:3dseman-
[29] Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and Hong-
tic instance segmentation of rgb-d scans. In CVPR, pages
sheng Li. St-adapter: Parameter-efficient image-to-video
4421–4430,2019. 2,6
transferlearning. NeurIPS,35:26462–26477,2022. 2
[14] Ji Hou, Saining Xie, Benjamin Graham, Angela Dai, and [30] Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Colored
Matthias Nießner. Pri3d: Can 3d priors help 2d represen- pointcloudregistrationrevisited. InICCV,pages143–152,
tationlearning? InICCV,pages5693–5702,2021. 6 2017. 3
[15] Binh-Son Hua, Quang-Hieu Pham, Duc Thanh Nguyen, [31] Charles R Qi, Hao Su, Matthias Nießner, Angela Dai,
Minh-KhoiTran,Lap-FaiYu,andSai-KitYeung. Scenenn: Mengyuan Yan, and Leonidas J Guibas. Volumetric and
Ascenemeshesdatasetwithannotations.In3DV,pages92– multi-view cnns for object classification on 3d data. In
101,2016. 2,5 CVPR,pages5648–5656,2016. 2
10Table6.Per-class3Dsemanticsegmentationresults(IoU)ofourmethodontheScanNetvalidationset.
wallfloorcabinetbedchairsofatabledoorwindowbookshelfpicturecounterdeskcurtainfridgecurtaintoiletsinkbathtubothersmean
Ours85.797.1 63.1 80.789.076.173.766.1 63.6 77.1 41.8 65.5 61.2 58.9 61.0 72.7 95.277.9 94.2 53.6 72.7
Table7.Per-class3Dsemanticsegmentationresults(IoU)ofourmethodontheSceneNNvalidationset.
wallfloorcabinetbedchairsofatabledoorwindowbookshelfpicturecounterdeskcurtainfridgesinkmean
Ours75.382.6 59.8 82.862.057.818.452.4 20.5 55.9 29.4 52.6 44.9 50.2 80.3 81.8 56.7
[32] CharlesRQi,HaoSu,KaichunMo,andLeonidasJGuibas. instancesegmentationinpointcloud.InCVPR,pages3947–
Pointnet: Deep learning on point sets for 3d classification 3956,2019. 2,6
andsegmentation. InCVPR,pages652–660,2017. 1,2 [46] JiazhaoZhang, ChenyangZhu, LintaoZheng, andKaiXu.
[33] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Fusion-awarepointconvolutionforonlinesemantic3dscene
Guibas. Pointnet++: Deephierarchicalfeaturelearningon segmentation. InCVPR,pages4534–4543,2020. 2,6
pointsetsinametricspace. InNeurIPS,pages5099–5108, [47] Jiazhao Zhang, Liu Dai, Fanpeng Meng, Qingnan Fan,
2017. 2 XuelinChen,KaiXu,andHeWang. 3d-awareobjectgoal
[34] Charles R. Qi, Or Litany, Kaiming He, and Leonidas J. navigation via simultaneous exploration and identification.
Guibas. Deephoughvotingfor3dobjectdetectioninpoint InCVPR,pages6672–6682,2023. 1,2,3
clouds. InICCV,pages9277–9286,2019. 2 [48] XiaomingZhao,HarshAgrawal,DhruvBatra,andAlexan-
[35] Santhosh Kumar Ramakrishnan, Devendra Singh Chap- derGSchwing.Thesurprisingeffectivenessofvisualodom-
lot, Ziad Al-Halah, Jitendra Malik, and Kristen Grauman. etrytechniquesforembodiedpointgoalnavigation.InICCV,
Poni: Potential functions for objectgoal navigation with pages16127–16136,2021. 3
interaction-free learning. In CVPR, pages 18890–18900,
2022. 3
[36] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Fasterr-cnn: towardsreal-timeobjectdetectionwithregion
proposalnetworks. TPAMI,39(6):1137–1149,2016. 6
[37] OlafRonneberger,PhilippFischer,andThomasBrox.U-net:
Convolutionalnetworksforbiomedicalimagesegmentation.
InMICCAI,pages234–241.Springer,2015. 6,9
[38] DanilaRukhovich,AnnaVorontsova,andAntonKonushin.
Fcaf3d: fullyconvolutionalanchor-free3dobjectdetection.
InECCV,pages477–493.Springer,2022. 1,2,6,9
[39] DanilaRukhovich,AnnaVorontsova,andAntonKonushin.
Tr3d: Towardsreal-timeindoor3dobjectdetection. arXiv
preprintarXiv:2302.02858,2023. 3
[40] Jonas Schult, Francis Engelmann, Alexander Hermans, Or
Litany,SiyuTang,andBastianLeibe.Mask3dfor3dseman-
ticinstancesegmentation.arXivpreprintarXiv:2210.03105,
2022. 1,2,6
[41] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao.
Sunrgb-d:Argb-dsceneunderstandingbenchmarksuite.In
CVPR,pages567–576,2015. 3
[42] ThangVu,KookhoiKim,TungMLuu,ThanhNguyen,and
Chang D Yoo. Softgroup for 3d instance segmentation on
pointclouds. InCVPR,pages2708–2717,2022. 1,2,6
[43] HaiyangWang,LiheDing,ShaocongDong,ShaoshuaiShi,
Aoxue Li, Jianan Li, Zhenguo Li, and Liwei Wang. Ca-
group3d: Class-aware grouping for 3d object detection on
pointclouds. arXivpreprintarXiv:2210.04264,2022. 1,2,
6
[44] ZhenyuWu, XiuweiXu, ZiweiWang,ChongXia,Linqing
Zhao,JiwenLu,andHaibinYan.Anyview:generalizablein-
door3dobjectdetectionwithvariableframes.arXivpreprint
arXiv:2310.05346,2022. 6
[45] LiYi,WangZhao,HeWang,MinhyukSung,andLeonidasJ
Guibas. Gspn: Generative shape proposal network for 3d
11Table8.Per-class3Dobjectdetectionresults(AP )ofourmethodontheScanNetvalidationset.
25
cabinetbedchairsofatabledoorwindowbookshelfpicturecounterdeskcurtainfridgecurtaintoiletsinkbathtubothersmean
Ours 55.2 85.488.787.263.362.5 47.3 66.2 36.0 65.2 80.1 65.0 58.1 76.3 99.776.7 93.3 62.0 70.5
Table9.Per-class3Dobjectdetectionresults(AP )ofourmethodontheScanNetvalidationset.
50
cabinetbedchairsofatabledoorwindowbookshelfpicturecounterdeskcurtainfridgecurtaintoiletsinkbathtubothersmean
Ours 36.7 75.673.977.957.033.8 19.8 43.7 19.4 26.3 62.8 32.4 41.1 24.6 89.246.7 84.8 52.2 49.9
Table10.Per-class3Dinstancesegmentationresults(AP )ofourmethodontheScanNetvalidationset.
25
cabinetbedchairsofatabledoorwindowbookshelfpicturecounterdeskcurtainfridgecurtaintoiletsinkbathtubothersmean
Ours 60.3 86.891.580.372.856.0 55.3 67.5 45.1 48.9 72.9 68.4 56.5 86.3 99.781.3 87.8 65.3 71.3
Table11.Per-class3Dinstancesegmentationresults(AP )ofourmethodontheScanNetvalidationset.
50
cabinetbedchairsofatabledoorwindowbookshelfpicturecounterdeskcurtainfridgecurtaintoiletsinkbathtubothersmean
Ours 50.9 79.182.571.363.644.0 36.0 45.5 38.5 30.3 57.3 49.8 52.9 78.9 99.766.6 84.9 56.9 60.5
12