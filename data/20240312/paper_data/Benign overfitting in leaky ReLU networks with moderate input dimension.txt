BENIGN OVERFITTING IN LEAKY RELU NETWORKS WITH
MODERATE INPUT DIMENSION
A PREPRINT
KedarKarhadkar1 ErinGeorge1 MichaelMurray1 GuidoMontúfar12 DeannaNeedell1
∗ ∗
{kedar,egeo,mmurray,montufar,deanna}@math.ucla.edu
1UCLA 2MaxPlanckInstituteforMathematicsintheSciences
Equalcontribution
∗
March12,2024
ABSTRACT
Theproblemofbenignoverfittingaskswhetheritispossibleforamodeltoperfectlyfitnoisytrain-
ing data and still generalizewell. We study benignoverfittingin two-layer leakyReLU networks
trainedwiththehingelossonabinaryclassificationtask. Weconsiderinputdatawhichcanbede-
composedintothesumofacommonsignalandarandomnoisecomponent,whichlieonsubspaces
orthogonal to one another. We characterize conditions on the signal to noise ratio (SNR) of the
modelparametersgivingrise tobenignversusnon-benign,orharmful,overfitting: inparticular,if
theSNRishighthenbenignoverfittingoccurs,converselyiftheSNRislowthenharmfuloverfitting
occurs. We attribute both benign and non-benignoverfittingto an approximatemarginmaximiza-
tionpropertyandshowthatleakyReLUnetworkstrainedonhingelosswithGradientDescent(GD)
satisfy this property. In contrastto priorwork we do notrequirenearorthogonalityconditionson
thetrainingdata:notably,forinputdimensiondandtrainingsamplesizen,whilepriorworkshows
asymptoticallyoptimalerrorwhend=Ω(n2logn),herewerequireonlyd=Ω nlog1 toobtain
ǫ
errorwithinǫofoptimal.
(cid:0) (cid:1)
1 Introduction
Intuitionfromlearningtheorysuggeststhatfittingnoiseduringtrainingreducesa model’sperformanceontestdata.
However,ithasbeenobservedinsomesettingsthatmachinelearningmodelscaninterpolatenoisytrainingdatawith
onlynominalcosttotheirgeneralizationperformance(Zhangetal.,2017;Belkinetal.,2018,2019), aphenomenon
referredtoasbenignoverfitting.Establishingtheorywhichcanexplainthisphenomenonhasattractedmuchinterestin
recentyearsandthereisnowarichbodyofworkonthistopicparticularlyinthecontextoflinearmodels.However,the
studyofbenignoverfittinginthecontextofnon-linearmodels,inparticularshallowReLUorleakyReLUnetworks,
hasadditionaltechnicalchallengesandsubsequentlyislesswelladvanced.
Muchoftheeffortinregardtotheoreticallycharacterizingbenignoverfittingfocusesonshowing,underanappropriate
scalingofthedimensionoftheinputdomaind,sizeofthetrainingsamplen,numberofcorruptionskandnumberof
modelparameterspthatamodelcaninterpolatenoisytrainingdatawhileachievinganarbitrarilysmallgeneralization
error.Suchcharacterizationsofbenignoverfittingpositionitasahighdimensionalphenomenon1:indeed,thedecrease
ingeneralizationerrorisachievedbyescapingtohigherdimensionsatsomeraterelativetotheotheraforementioned
hyperparameters.However,forthesemathematicalresultstoberelevantforexplainingbenignoverfittingasobserved
in practice, clearly the particular scaling of d with respect to n,k and p needs to reflect the ratios seen in practice.
Althoughanumberofworks,whichwediscussinSection1.2,establishbenignoverfittingresultsforshallowneural
networks,akeyandsignificantlimitationtheyshareistherequirementthattheinputfeaturesofthetrainingdataare
atleastapproximatelyorthogonaltooneanother. Tostudybenignoverfittingthesepriorworkstypicallyassumethe
input features consist of a small, low-rank signal component plus an isotropic noise term. Therefore, for the near
1WeprovideaformaldefinitionofbenignoverfittingasahighdimensionalphenomenoninAppendixE.
4202
raM
11
]GL.sc[
1v30960.3042:viXraAPREPRINT
orthogonalitypropertytoholdwithhighprobabilityitisrequiredthattheinputdimensiondscalesasd=Ω(n2logn)
orhigher.Thisassumptionhighlyrestrictstheapplicabilityoftheseresultsforexplainingbenignoverfittinginpractice.
In this work we assume only d Ω(n) and establish both harmfuland benign overfittingresults for shallow leaky
≈
ReLU networks trained via GradientDescent (GD) on the hinge loss. In particular, we consider n data point pairs
(x ,y ) Rd 1 , where, for some vector v Sd 1 and scalar γ [0,1], the input features are drawn from
i i −
a pair of∈ Gauss× ian{± clu} sters x
i
∼
N( ±√γv,√1 −∈ γ d1(I
d
−vvT)) and y∈
i
= sign( hE[x i],v i). The trainingdata is
noisy in that k of the n points in the training sample have their outputlabel flipped. We assume equal numbersof
positiveandnegativepointsamongcleanandcorruptones.Weprovideafulldescriptionofoursetupandassumptions
in Section 2. Our proof techniques are noveland identify a new condition allowing for the analysis of benign and
harmfuloverfittingwhichwetermapproximatemarginmaximization,whereinthenormofthenetworkparametersis
upperboundedbyaconstantofthenormofthemax-marginlinearclassifier.
1.1 Summaryofcontributions
Ourkeyresultsaresummarizedasfollows.
• InTheorem3.1,weprovethataleakyReLUnetworktrainedonlinearlyseparabledatawithgradientdescent
and the hinge loss will attain zero training loss in finitely many iterations. Moreover, the network weight
w∗
matrixW atconvergencewillbeapproximatelymax-margininthesensethat W = O k k ,whereα
k k α√m
istheleakyparameteroftheactivationfunction,misthe widthofthenetwork,andw
∗
i(cid:16)sthem(cid:17)ax-margin
linearclassifier. Weapplythisresulttoderivegeneralizationboundsforthenetworkontestdata.
• InTheorem3.2,weestablishconditionsunderwhichbenignoverfittingoccursforleakyReLUnetworks. If
theinputdimensiond, numberoftrainingpointsn,andsignalstrengthγ satisfyd = Ω(n)andγ = Ω(1),
n
then the network will exhibit benign overfitting. We emphasize that existing works on benign overfitting
required=Ω(n2logn)toensurenearlyorthogonaldata.
• InTheorem3.3,wefindconditionsunderwhichnon-benignoverfittingoccurs. Ifd = Ω(n)andγ = O(1),
d
thenthenetworkwillexhibitnon-benignoverfitting:inparticularitsgeneralizationerrorwillbeatleast 1.
8
1.2 Relatedwork
There is now a significant body of literature theoretically characterizing benign overfitting in the context of linear
models,includinglinearregression(Bartlettetal.,2020;Muthukumaretal.,2020;Wu&Xu,2020;Zouetal.,2021;
Hastieetal., 2022; Koehleretal., 2021; Wangetal., 2021a; Chatterji&Long, 2022; Shamir, 2022), logistic regres-
sion (Chatterji&Long, 2021; Muthukumaretal., 2021; Wangetal., 2021b), max-margin classification with linear
and randomfeature models (Montanarietal., 2023b,a; Mei&Montanari, 2022; Caoetal., 2021) and kernel regres-
sion(Liang&Rakhlin, 2020;Liangetal., 2020;Adlam&Pennington,2020). However,thestudyofbenignoverfit-
tinginnon-linearmodelsismorenascent.
Homogeneousnetworkstrainedwithgradientdescentandanexponentiallytailedlossareknowntoconvergeindirec-
tionto a Karush-Kuhn-Tucker(KKT) pointof the associatedmax-marginproblem(Lyu&Li, 2020; Ji&Telgarsky,
2020)2. Thispropertyhasbeenwidelyusedinpriorworkstoprovebenignoverfittingresultsforshallowneuralnet-
works.Freietal.(2022)considerashallow,smoothleakyReLUnetworktrainedwithanexponentiallytailedlossand
assume the data is drawnfroma mixtureof well-separatedsub-Gaussiandistributions. A keyresultof this workis,
givensufficientiterationsofGD, thatthenetworkwillinterpolatenoisytrainingdatawhile also achievingminimax
optimal generalization error up to constants in the exponents. Xu&Gu (2023) extend this result to more general
activationfunctions,includingReLU,aswellasrelaxtheassumptionsonthenoisedistributiontobeingcenteredwith
boundedlogarithmicSobolevconstant,andfinallyalsoimprovetheconvergencerate. Georgeetal.(2023)alsostudy
ReLUasopposedtoleakyReLUnetworksbutdosointhecontextofthehingeloss,forwhich,andunlikeexponen-
tiallytailedlosses, acharacterizationoftheimplicitbiasisnotknown. Thisworkalsoestablishestransitionsonthe
marginofthecleandatadrivingharmful,benignandno-overfittingtrainingoutcomes.Freietal.(2023)usetheafore-
mentionedimplicitbiasofGDforlinearclassifiersandshallowleakyReLUnetworkstowardssolutionswhichsatisfy
theKKTconditionsofthemarginmaximizationproblemtoestablishsettingswherethesatisfactionofsaidKKTcon-
ditionsimplies benign overfitting. Kornowskietal. (2023) also use the implicit bias results for exponentiallytailed
losses to derive similar benign overfitting results for shallow ReLU networks. Caoetal. (2022); Kouetal. (2023)
study benign overfittingin two-layer convolutionalas opposed to feedforwardneuralnetworks: indeed, whereasin
mostpriorworksdataismodeledasthesumofasignalandnoisecomponent,inthesetwoworksthesignalandnoise
2Onealsoneedstoassumeinitializationfromapositionwithalowinitialloss.
2APREPRINT
componentsareassumedtolie indisjointpatches. Theweightvectorofeachneuronisappliedto bothpatchessep-
aratelyandanon-linearity,suchasReLU,isappliedtotheresultingpre-activation. Inthissetting,theauthorsprove
interpolationofthenoisytrainingdataandderiveconditionsonthecleanmarginunderwhichthenetworkbenignly
versusharmfullyoverfits. Afollowupwork(Chenetal.,2023)considerstheimpactofSharpnessAwareMinimiza-
tion(SAM)inthesamesetting.Finally,andassumingd=Ω(n5),Xuetal.(2024)establishbenignoverfittingresults
foradatadistributionwhich,insteadofbeinglinearlyseparable,isseparatedaccordingtoanXORfunction.
We emphasizethatthepriorworkonbenignoverfittinginthe contextofshallowneuralnetworksrequirestheinput
data to be approximately orthogonal. Under standard data models studied this equates to the requirement that the
inputdimensiondversusthesizeofthetrainingsamplensatisfiesd = Ω(n2logn)orhigher. Herewerequireonly
d=Ω(n). Finally,weremarkthatourprooftechniquefortheconvergenceofGDtoaglobalminimizerinthecontext
ofashallowleakyReLUnetworkiscloselyrelatedtotheprooftechniquesusedbyBrutzkusetal.(2018).
2 Preliminaries
Let[n] = 1,2,...,n denotethesetofthefirstnnaturalnumbers. Weremarkthatwhenusingbig-Onotationwe
{ }
implicitly assume only positive constants. We use c,C,C ,C ,... to denote absolute constants with respect to the
1 2
inputdimensiond, thetrainingsamplesizen,andthewidthofthenetworkm. Noteconstantsmaychangeinvalue
fromlinetoline. Furthermore,whenusingbig-Onotationallvariablesasidefromd,n,kandmareconsideredcon-
stants. However,forclaritywewillfrequentlymaketheconstantsconcerningtheconfidenceδandfailureprobability
ǫexplicit. Moreover,fortwofunctionsf,g : N N,ifwesayf = O(g)impliespropertyp,whatwemeanisthere
existsanN NandaconstantC suchthatiff→ (n) Cg(n)foralln N thenpropertypholds. Likewise,ifwe
sayf
=Ω(g∈ )impliespropertyp,whatwemeanisthe≤
reexistsanN
N≥
andaconstantC suchthatiff(n) Cg(n)
foralln N thenpropertypholds. Finally,weuse todenote∈ theℓ2 normofthevectorargumentor≥ ℓ2 ℓ2
≥ k·k →
operatornormofthematrixargument.
2.1 Datamodel
Westudydatageneratedasperthefollowingdatamodel.
Definition2.1. Supposed,n,k N,γ (0,1)andv Sd 1. If(X,yˆ,y,x,y) (d,n,k,γ,v)then
−
∈ ∈ ∈ ∼D
1. X Rn ×d isarandommatrixwhoserows, whichwedenotex i,satisfyx
i
= √γy iv+√1 γn i,where
∈ −
n (0 , 1(I vvT))aremutuallyi.i.d..
i ∼N d d d −
2. y 1 n is a random vector with entries y which are mutually independentof one another as well as
i
∈ {± }
thenoisevectors(n ) andareuniformlydistributedover 1 . Thisvectorholdsthetruelabelsofthe
i i [n]
trainingset. ∈ {± }
3. Let [n]beanysubsetchosenindependentlyofysuchthat =k. Thenyˆ 1 nisarandomvector
whoB se⊂ entriessatisfyyˆ = y foralli andyˆ = y forall| iB| c =: . Th∈ is{ v± ect} orholdstheobserved
i i i i
6 ∈ B ∈ B G
labelsofthetrainingset.
4. yisarandomvariablerepresentingatestlabelwhichisuniformlydistributedover 1 .
{± }
5. x Rdisarandomvectorrepresentingtheinputfeatureofatestpointandsatisfiesx=√γyv+√1 γn,
∈ −
wheren (0 , 1(I vvT))ismutuallyindependentoftherandomvectors(n ) .
∼N d d d − i i ∈[n]
We referto (X,yˆ) asthe trainingdataand(x,y)asthetest data. Furthermore, fortypographicalconveniencewe
definey yˆ=:β 1 n.
⊙ ∈{± }
ToprovidesomeinterpretationtoDefinition2.1,thetrainingdataconsistsofnpointsofwhichkhavetheirobserved
labelflippedrelativetothetruelabel. Werefertov andn asthesignalandnoisecomponentsofthei-thdatapoint
i
respectively:indeed,withγ >0thenfori y
i
x i,v =√γ >0.Thetestdataisdrawnfromthesamedistribution
∈G h i
andisassumednottobecorrupted. Wesaythatthetrainingdata(X,yˆ)islinearlyseparableifthereexistsw Rd
∈
suchthat
yˆ w,x 1, foralli [n].
i i
h i≥ ∈
Forfiniten,thisconditionisequivalenttotheexistenceofawwithyˆ w,x >0foralli [n]. Wedenotethesetof
i i
linearlyseparabledatasetsas Rn d 1 n. Foralinearlysh eparabi ledataset(X,∈ yˆ),themax-marginlinear
lin ×
X ⊂ ×{± }
3APREPRINT
classifieristheuniquesolutiontotheoptimizationproblem
min w suchthatyˆ w,x 1foralli [n].
i i
w Rdk k h i≥ ∈
∈
Observeonemayequivalentlytakeastrictlyconvexobjective w 2 andtheconstraintsetisaclosedconvexpolyhe-
k k
dronwhichisnon-emptyiffthedataislinearlyseparable. Themax-marginlinearclassifierw hasacorresponding
∗
geometricmargin2/ w . Whend nandγ >0,inputfeaturematricesX fromourdatamodelalmostsurelyhave
∗
linearlyindependentk rowsk x andthu≥ s(X,yˆ)isalmostsurelylinearlyseparableforanyobservedlabelsyˆ 1 n.
i
∈{± }
2.2 Architectureandlearningalgorithm
WestudyshallowleakyReLUnetworkswithaforwardpassfunctionf :R2m d Rd Rdefinedas
×
× →
2m
f(W,x)= ( 1)jσ( w ,x ), (1)
j
− h i
j=1
X
whereW R2m d arethe parametersofthenetwork,σ : R R istheleakyReLUfunction,definedasσ(x) =
×
∈ →
max(x,αx), whereα (0,1]isreferredtoastheleakyparameter. Weremarkthatweonlytraintheweightsofthe
∈
first layerandkeepthe outputweightsofeach neuronfixed. Althoughσ isnotdifferentiableat 0, in the contextof
gradient descent we adopt a subgradientand let σ˙(z) = 1 for z 0 and let σ˙(z) = α otherwise. The hinge loss
ℓ:R R isdefinedas ≥
0
→ ≥ ℓ(z)=max 0,1 z . (2)
{ − }
Again,ℓisnotdifferentiableatzero;adoptingasubgradientwedefineforanyj [2m]
∈
( 1)j+1yˆxσ˙( w ,x ) yˆf(W,x)<1,
w ℓ(yˆf(W,x))= − h j i
∇ j 0 yˆf(W,x) 1.
(cid:26) ≥
ThetraininglossL:R2m d Rn d Rn Risdefinedas
× ×
× × →
n
L(W,X,yˆ)= ℓ(yˆf(W,x )). (3)
i i
i=1
X
LetW(0) R2m ddenotethemodelparametersatinitialization.Foreacht NwedefineW(t) recursivelyas
×
∈ ∈
W(t) =W(t −1) η WL(W(t −1),X,yˆ),
− ∇
where η > 0 is the step size. Let (t) [n] denote the set of all i [n] such that yˆf(W(t),x ) < 1. Then
i i
F ⊆ ∈
equivalentlyeachneuronisupdatedaccordingtothefollowingrule:forj [2m]
∈
w j(t) =GD(W(t −1),η):=w j(t −1)+η( −1)j yˆ ix iσ˙( hw j(t −1),x
i
i). (4)
i ∈FX(t−1)
Foreaseofreferencewenowprovidethefollowingdefinitionofthelearningalgorithmdescribedabove.
Definition2.2. Let :Rn d 1 n R R2m d R2m d return (X,yˆ,η,W(0))=:W,wherethe
GD × × × GD
A ×{± } × × → A
j-throww ofW isdefinedasfollows: letw(0) bethej-throwofW(0) andgeneratethesequence(w(t) ) using
j j j t 0
≥
therecurrencerelationw(t) =GD(W(t 1),η)asdefinedinequation4.
j −
1. Ifforj [2m],lim w(t) doesnotexistthenwesay isundefined.
∈ t →∞ j AGD
2. Otherwisewesay convergesandw =lim w(t).
AGD j t
→∞
j
3. IfthereexistsaT Nsuchthatforallj [2m]w(t) =w(T) forallt T,thenwesay convergesin
∈ ∈ j j ≥ AGD
finitetime.
Weoftenfindthatallmatricesintheset
(X,yˆ,η,W(0)): j w(0) λ
{AGD ∀ k j k≤ }
agreeonallrelevantproperties. Inthiscase, weabuse notationandsay that (X,yˆ,η,λ) = W whereW isa
GD
A
genericelementfromthisset.
Finally,inordertoderiveourresultswemakethefollowingassumptionsconcerningthestepsizeandinitializationof
thenetwork.
4APREPRINT
Assumption1. Thestepsizeηsatisfiesη 1/(mnmax x 2)andforallj [2m]thenetworkatinitialization
i [n] i
≤ ∈ k k ∈
satisfies w(0) √α/(mmin x ).
k j k≤ i ∈[n] k i k
Under our data model the input data points have approximately unit norm; therefore these assumptions reduce to
η C and w(0) C√α.
≤ mn k j k≤ m
2.3 Approximatemarginmaximization
Wenowintroducethenotionofanapproximatemarginmaximizingalgorithm,whichplaysakeyroleinderivingour
results. Althoughtheprimarysettingweconsiderinthisworkisthelearningalgorithm (seeDefinition2.2),we
GD
A
derivebenignoverfittingguaranteesmorebroadlyforanylearningalgorithmwhichfitsintothiscategory.Recall
lin
denotesthesetoflinearlyseparabledatasets(X,yˆ) Rn d 1 n. X
×
∈ ×{± }
Definition2.3. Letf :Rp Rd Rdenoteapredictorfunctionwithpparameters.Analgorithm :Rn d Rn
×
Rpisapproximatelymargi× nmax→
imizingwithfactorM >0onf ifforall(X,yˆ)
A × →
lin
∈X
yˆf( (X,yˆ),x ) 1 foralli [n] (5)
i i
A ≥ ∈
and
(X,yˆ) M w , (6)
∗
kA k≤ k k
wherew isthemax-marginlinearclassifierof(X,yˆ). Moreover,if isanapproximatemarginmaximizingalgo-
∗
A
rithmwedefine
=inf M >0: isapproximatelymarginmaximizingwithfactorM . (7)
|A| { A }
IntheabovedefinitionwetakethestandardEuclideannormonRp. InparticularifRp =R2m disaspaceofmatrices
×
wetaketheFrobeniusnorm.
3 Mainresults
Inordertoprovebenignoverfittingitisnecessarytoshowthatthelearningalgorithmoutputsamodelwhichcorrectly
classifiesallpointsinthetrainingsample.Thefollowingtheoremestablishesevenstrongerconditionsfor .
GD
A
Theorem3.1. Letf : Rp Rn RbealeakyReLUnetworkwithforwardpassasdefinedbyequation1. Suppose
× →
the step size η andinitialization conditionλ satisfy Assumption1. Then for any linearly separable data set (X,yˆ)
(X,yˆ,η,λ)convergesafterT iterations,where
GD
A
C w 2
∗
T k k .
≤ ηα2m
Furthermore isapproximatelymarginmaximizingonf with
GD
A
C
.
GD
|A |≤ α√m
AproofofTheorem3.1canbefoundinAppendixD.1.NotealsobyDefinition2.3thatthesolutionW = (X,yˆ)
GD
A
for(X,yˆ) isaglobalminimizerofthetraininglossdefinedinequation3withL(W,X,yˆ)=0.Ourapproach
lin
∈X
toprovingthisresultisreminiscentoftheproofofconvergenceoftheperceptronalgorithmandthereforeisalsosimilar
tothetechniquesusedbyBrutzkusetal.(2018).
FortrainingandtestdataasperDefinition2.1weprovideanupperboundonthegeneralizationerrorforapproximately
marginmaximizingalgorithms.Forconveniencewesummarizeoursettingasfollows.
Definition3.1. Settingforprovinggeneralizationresults.
• f :R2m d Rd RisashallowleakyReLUnetworkasperequation1.
×
× →
• : Rn d 1 n R2m d isalearningalgorithmwhichreturnstheweightsW R2m d ofthefirst
× × ×
A ×{± } → ∈
layeroff.
• We let v Sd 1 and consider training data (X,yˆ) and test data (x,y) distributed according to
−
∈
(X,yˆ,y,x,y) (d,n,k,γ,v)asperDefinition2.1.
∼D
5APREPRINT
Underthissettingwehavethefollowinggeneralizationresultforanapproximatelymarginmaximizingalgorithm .
A
Notethisresultrequiresγ,andhencethesignaltonoiseratiooftheinputs,tobesufficientlylarge.
Theorem 3.2. Under the setting given in Definition 3.1, let δ,ǫ (0,1) and suppose is approximately margin-
∈ A
maximizing. If n = Ω log1
δ
, d = Ω (1+ |A|2m)nlog1
ǫ
, k = O( 1+mn 2), and γ = Ω 1+m n|A|2 then with
|A|
probabilityatleast1 (cid:0)δover (cid:1)(X,yˆ) (cid:0) (cid:1) (cid:16) (cid:17)
−
P(yf(W,x) 0 X,yˆ) ǫ.
≤ | ≤
A proofof Theorem3.2is providedin AppendixD.2. CombiningTheorems3.1 and3.2 we arriveat the following
benignoverfittingresultforshallowleakyReLUnetworkstrainedwithGDonhingeloss.
Corollary3.2.1. UnderthesettinggiveninDefinition3.1,letδ,ǫ (0,1)andsuppose = whereη,λ R
GD >0
∈ A A ∈
satisfyAssumption1. Ifn=Ω log1 ,d=Ω nlog1 ,k =O(n),andγ =Ω 1 thenthefollowinghold.
δ ǫ n
1. Almost surely (cid:0)termin(cid:1)ates afte(cid:0)r a finit(cid:1)e number of updates a(cid:0)nd(cid:1)with W = (X,yˆ) then
GD GD
A A
L(W,X,yˆ)=0.
2. Withprobabilityatleast1 δoverthetrainingdata(X,yˆ)
−
P(yf(W,x) 0 X,yˆ) ǫ.
≤ | ≤
Inadditiontothisbenignoverfittingresultwealsoprovidethefollowingnon-benignoverfittingresultfor . Note
GD
A
thatconverselythisresultrequiresγ,andhencethesignaltonoiseratiooftheinputs,tobesufficientlysmall.
Theorem3.3. Underthe setting given in Definition3.1, let δ (0,1)and suppose = , where η,λ R
GD >0
∈ A A ∈
satisfyAssumption1. Ifn=Ω(1),d=Ω n+log1 andγ =O α3 thenthefollowinghold.
δ d
(cid:0) (cid:1) (cid:16) (cid:17)
1. The algorithm terminates almost surely after finitely many updates. With W = (X,yˆ),
GD GD
A A
L(W,X,yˆ)=0.
2. Withprobabilityatleast1 δoverthetrainingdata(X,yˆ)
−
1
P(yf(W,x)<0 X,yˆ) .
| ≥ 8
AproofofTheorem3.3isprovidedinAppendixD.3.
4 Approximate marginmaximizationand generalization: insightfrom linearmodels
In this section we outline proofs for the analogues of Theorems 3.2 and 3.3 in the context of linear models. The
arguments are thematically similar and clearer to present. We provide complete proofs of benign and non-benign
overfittingforlinearmodelsinAppendixC.
Animportantlemmaisthefollowing,whichboundsthelargestandn-thlargestsingularvalues(σ andσ respectively)
1 n
ofthenoisematrixN:
Lemma4.1. LetN Rn ddenotearandommatrixwhoserowsaredrawnmutuallyi.i.d.from (0 , 1(I vvT)).
∈ × N d d d −
Ifd=Ω n+log1 ,thenwithprobabilityatleast1 δ,
δ −
(cid:0) (cid:1) c σ (N) σ (N) C.
n 1
≤ ≤ ≤
We prove this lemma in Appendix B using results from Vershynin (2018) and Rudelson&Vershynin (2009). A
consequenceof this lemma is that with probabilityat least 1 δ, the conditionnumberof N restricted to spanN
−
can be bounded above independently of all hyperparameters. For this reason, we refer to the noise as being well-
conditioned.
Nowletw = (X,yˆ)bethelinearclassifierreturnedbythealgorithm. Observethatwecandecomposetheweight
A
vectorintoasignalandnoisecomponent
w =a v+z,
v
wherez vanda R. Basedonthisdecompositiontheproofproceedsasfollows.
v
⊥ ∈
6APREPRINT
1. GeneralizationboundsbasedontheSNR: FortestdataasperthedatamodelgiveninDefinition2.1wewant
toboundtheprobabilityofmisclassification:inparticular,wewanttoboundtheprobabilitythat
X :=y w,x =√γa + 1 γ n,z 0.
v
h i − h i≤
AlittleanalysisyieldsthatX √γ,1 −γ z 2 andthedpesiredupperboundthereforefollowsfromHoeffding’s
∼ N d k k
inequality,
(cid:0) (cid:1)
γda2
P(X 0) exp v .
≤ ≤ −2(1 γ) z 2
(cid:18) − k k (cid:19)
UsingGaussiananti-concentration,wealsoobtainalowerboundfortheprobabilityofmisclassification:
1 dγ a
P(X 0) v .
≤ ≥ 2 −s2π(1 γ) z
− k k
2. Upperboundthenormofthemax-marginclassifier: Inordertousetheapproximatemax-marginpropertywe
requireanupperboundon w .Asbydefinition w w˜ ,itsufficestoconstructavectorw˜ whichinterpolates
∗ ∗
k k k k≤k k
the data and has small norm. Using that the noise matrix of the data is well-conditionedwith high probability, we
achievethisbystrategicallyconstructingthesignalandnoisecomponentsofw˜. Thisyieldsthebound
n 1 k
w w˜ Cmin , + ,
∗
k k≤k k≤ 1 γ sγ 1 γ !
r − −
wheretheargumentsofthemaxfunctionoriginatefromasmallandlargeγ regimerespectively.
3. Lower bound the SNR using the approximate margin maximization property: Based on step 1 the key
quantity of interest from a generalization perspective is the ratio a / z , which describes the signal to noise ratio
v
k k
(SNR)ofthelearnedclassifier. Tolowerboundthisquantitywefirstlowerbounda . Inparticular,ifa issmall,then
v v
theonlywaytoattainzerolossonthecleandataisfor z tobelarge. However,underappropriateassumptionson
k k
d,n,kandγ thiscanbeshowntocontradictthebound z w w ,andthusa mustbeboundedfrom
∗ v
k k ≤ k k ≤ |A|k k
below. Alowerboundona / z thenfollowsbyagainusing z w . Henceweobtainalowerboundforthe
v ∗
k k k k ≤ k k
SNRandestablishbenignoverfitting.
Toprovenon-benignoverfitting,wecomputeanupperboundfortheratioa / z ratherthanalowerbound. Since
v
k k
themodelperfectlyfitsthetrainingdatawithmarginone,
1 yˆ w,x =√γβ a + 1 γyˆ z,n
i i i v i i
≤ h i − h i
for all i [n]. If γ is small and √γa
v
is large, then w wpill be large, contradicting the approximate margin
∈ | | k k
maximization. Hence the aboveinequality implies that √1 γyˆ n ,z is large for all i [n]. Since the noise is
i i
− h i ∈
well-conditioned,thiscanonlyhappenwhen z islarge.Thisgivesusalowerboundon z . Asbefore,wecanalso
k k k k
upperbounda by w ,givingusanupperboundontheSNRa / z .Bythesecondgeneralizationboundinstep1,
v v
k k k k
theclassifiergeneralizespoorlyandexhibitsnon-benignoverfitting.
4.1 FromlinearmodelstoleakyReLUnetworks
The proof of benign and non-benign overfitting in the linear case uses the tension between the two properties of
approximatemarginmaximization:fittingboththecleanandcorruptpointswithmarginversustheboundonthenorm.
ToextendthisideatoashallowleakyReLUnetworkasperequation1,weconsiderthesamedecompositionforeach
neuronj [2m],
∈
w =a v+z ,
j j j
wherea Randz v. Inthelinearcase a canbeinterpretedastheactivationofthelinearclassifier on v
j j v
∈ ⊥ ± ±
respectively:intermsofmagnitudethesignalactivationisthesameineithercaseandthuswemeasurethealignment
ofthelinearmodelwiththesignalusing a . ForleakyReLUnetworkswedefinetheiractivationon vrespectively
v
| | ±
asA = f(W,v)andA = f(W, v),andthendefinethealignmentofthenetworkasA = min A ,A .
1 1 min 1 1
Consideringthealignmen−tofthenetw− orkwiththenoise,thenifZ R2m d denotesamatrixwhosej-th{ rowis−z} ,
× j
∈
thenwe measurethe alignmentofthe networkusing Z . Asa result, analogousto a / z , the keyratiofroma
F v
k k k k
generalizationperspectiveinthecontextofaleakyReLUnetworkisA / Z . TheproofTheorems3.2and3.3
min F
k k
thenfollowthesameoutlineasSteps1-3abovebutwithadditionalnon-trivialtechnicalities.
7APREPRINT
5 Conclusion
In this work we have proven conditions under which leaky ReLU networks trained on binary classification tasks
exhibitbenignandnon-benignoverfitting. Wehavesubstantiallyrelaxedthenecessaryassumptionsontheinputdata
comparedwithpriorwork;insteadofrequiringnearlyorthogonaldatawithd = Ω(n2logn)orhigher,weonlyneed
d = Ω(n). We achievethisbyusingthedistributionofsingularvaluesofthenoiseratherthanspecificcorrelations
betweennoisevectors.Ouremphasiswasonnetworkstrainedbygradientdescentwiththehingeloss,butweestablish
anewframeworkthatisgeneralenoughtoaccommodateanyalgorithmwhichisapproximatelymarginmaximizing.A
naturalquestionforfutureworkisforwhichotherlossfunctionsandalgorithmstheapproximatemarginmaximization
condition will hold, and whether the model will exhibit benign overfitting in these regimes. Another interesting
questionisontheroleoflinearseparabilityandwhetherthereisananalogueofmarginmaximizationfornon-linearly
separabledatawhichleadstobenignoverfitting.
Acknowledgments
This material is based upon work supported by the National Science Foundation under Grant No. DMS-1928930
and by the AlfredP. Sloan FoundationundergrantG-2021-16778,while the authorsEG andDN were in residence
attheSimonsLauferMathematicalSciencesInstitute(formerlyMSRI)inBerkeley,California,duringtheFall2023
semester.EGandDNwerealsopartiallysupportedbyNSFDMS2011140.EGwasalsosupportedbyaNSFGraduate
ResearchFellowshipundergrantDGE2034835.GMandKKwerepartlysupportedbyNSFCAREERDMS2145630
andDFG SPP 2298TheoreticalFoundationsofDeepLearninggrant464109215. GM was alsopartlysupportedby
NSFgrantCCF2212520,ERCStartingGrant757983(DLT),andBMBFinDAADproject57616814(SECAI).
8APREPRINT
References
BenAdlamandJeffreyPennington. Theneuraltangentkernelin highdimensions: Tripledescentandamulti-scale
theoryofgeneralization.InHalDauméIIIandAartiSingh(eds.),Proceedingsofthe37thInternationalConference
onMachineLearning,volume119ofProceedingsofMachineLearningResearch,pp.74–84.PMLR,2020. URL
https://proceedings.mlr.press/v119/adlam20a.html.
Peter L. Bartlett, Philip M. Long, Gábor Lugosi, and Alexander Tsigler. Benign overfitting in lin-
ear regression. Proceedings of the National Academy of Sciences, 117(48):30063–30070, 2020. URL
https://www.pnas.org/doi/abs/10.1073/pnas.1907378117.
Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to understand kernel
learning. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Ma-
chine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 541–549. PMLR, 2018. URL
https://proceedings.mlr.press/v80/belkin18a.html.
MikhailBelkin,DanielHsu,SiyuanMa,andSoumikMandal. Reconcilingmodernmachine-learningpracticeandthe
classicalbias–variancetrade-off. ProceedingsoftheNationalAcademyofSciences, 116(32):15849–15854,2019.
URLhttps://doi.org/10.1073/pnas.190307011.
AlonBrutzkus, Amir Globerson,EranMalach, andShaiShalev-Shwartz. SGD learnsover-parameterizednetworks
thatprovablygeneralizeonlinearlyseparabledata. InInternationalConferenceonLearningRepresentations,2018.
URLhttps://openreview.net/forum?id=rJ33wwxRb.
YuanCao,QuanquanGu,andMikhailBelkin. Riskboundsforover-parameterizedmaximummarginclassificationon
sub-gaussianmixtures.InM.Ranzato,A.Beygelzimer,Y.Dauphin,P.S.Liang,andJ.WortmanVaughan(eds.),Ad-
vancesinNeuralInformationProcessingSystems,volume34,pp.8407–8418.CurranAssociates,Inc.,2021. URL
https://proceedings.neurips.cc/paper_files/paper/2021/file/46e0eae7d5217c79c3ef6b4c212b8c
Yuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu. Benign overfitting in two-layer convolutional neu-
ral networks. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances
in Neural Information Processing Systems, volume 35, pp. 25237–25250. Curran Associates, Inc., 2022. URL
https://proceedings.neurips.cc/paper_files/paper/2022/file/a12c999be280372b157294e72a4bbc
Niladri S. Chatterji and Philip M. Long. Finite-sample analysis of interpolating linear classifiers in
the overparameterized regime. Journal of Machine Learning Research, 22(129):1–30, 2021. URL
http://jmlr.org/papers/v22/20-974.html.
Niladri S. Chatterji and Philip M. Long. Foolish crowds supportbenign overfitting. Journal of Machine Learning
Research,23(125):1–12,2022. URLhttp://jmlr.org/papers/v23/21-1199.html.
ZixiangChen,JunkaiZhang,YiwenKou,XiangningChen,Cho-JuiHsieh,andQuanquanGu. Whydoessharpness-
awareminimizationgeneralizebetterthanSGD? InThirty-seventhConferenceonNeuralInformationProcessing
Systems,2023. URLhttps://openreview.net/forum?id=3WAnGWLpSQ.
SpencerFrei,NiladriSChatterji, andPeterBartlett. Benignoverfittingwithoutlinearity: Neuralnetworkclassifiers
trainedbygradientdescentfornoisylineardata.InPo-LingLohandMaximRaginsky(eds.),ProceedingsofThirty
FifthConferenceonLearningTheory,volume178ofProceedingsofMachineLearningResearch,pp.2668–2703.
PMLR,2022. URLhttps://proceedings.mlr.press/v178/frei22a.html.
Spencer Frei, Gal Vardi, Peter L. Bartlett, and Nathan Srebro. Benign overfitting in linear classifiers
and leaky relu networks from kkt conditions for margin maximization. In Gergely Neu and Lorenzo
Rosasco (eds.), The Thirty Sixth Annual Conference on Learning Theory, 12-15 July 2023, Bangalore, In-
dia, volume 195 of Proceedings of Machine Learning Research, pp. 3173–3228. PMLR, 2023. URL
https://proceedings.mlr.press/v195/frei23a.html.
Erin George, Michael Murray, William Swartworth, and Deanna Needell. Training shallow ReLU
networks on noisy data using hinge loss: when do we overfit and is it benign? In A. Oh,
T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural In-
formation Processing Systems, volume 36, pp. 35139–35189. Curran Associates, Inc., 2023. URL
https://proceedings.neurips.cc/paper_files/paper/2023/file/6e73c39cc428c7d264d9820319f31e
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. The Annals of Statistics, 50(2):949–986, 2022. URL
https://doi.org/10.1214/21-AOS2133.
Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. In
H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural In-
formation Processing Systems, volume 33, pp. 17176–17186. Curran Associates, Inc., 2020. URL
https://proceedings.neurips.cc/paper_files/paper/2020/file/c76e4b2fa54f8506719a5c0dc14c2e
9APREPRINT
Frederic Koehler, Lijia Zhou, Danica J. Sutherland, and Nathan Srebro. Uniform convergence of interpo-
lators: Gaussian width, norm bounds and benign overfitting. In A. Beygelzimer, Y. Dauphin, P. Liang,
and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL
https://openreview.net/forum?id=FyOhThdDBM.
Guy Kornowski, Gilad Yehudai, and Ohad Shamir. From tempered to benign overfitting in ReLU neu-
ral networks. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL
https://openreview.net/forum?id=LnZuxp3Tx7.
Yiwen Kou, Zixiang Chen, Yuanzhou Chen, and Quanquan Gu. Benign overfitting in two-layer ReLU convo-
lutional neural networks. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan
Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learn-
ing, volume 202 of Proceedings of Machine Learning Research, pp. 17615–17659. PMLR, 2023. URL
https://proceedings.mlr.press/v202/kou23a.html.
TengyuanLiangandAlexanderRakhlin. Justinterpolate: Kernel“Ridgeless”regressioncangeneralize. TheAnnals
ofStatistics,48(3):1329–1347,2020. URLhttps://doi.org/10.1214/19-AOS1849.
Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai. On the multiple descent of minimum-norm interpolants and
restrictedlowerisometryofkernels. InJacobAbernethyandShivaniAgarwal(eds.),ProceedingsofThirtyThird
ConferenceonLearningTheory,volume125ofProceedingsofMachineLearningResearch,pp.2683–2711.PMLR,
2020. URLhttps://proceedings.mlr.press/v125/liang20a.html.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neu-
ral networks. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=SJeLIgBKPS.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise asymptotics
and the double descent curve. Communications on Pure and Applied Mathematics, 75(4):667–766,2022. URL
https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.22008.
Andrea Montanari, Feng Ruan, Basil Saeed, and Youngtak Sohn. Universality of max-margin classifiers. arXiv
preprintarXiv:2310.00176,2023a.
Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan. The generalization error of max-margin linear clas-
sifiers: Benign overfitting and high-dimensional asymptotics in the overparametrized regime. arXiv preprint
arXiv:1911.01544,2023b.
Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai. Harmless interpolation of
noisy data in regression. IEEE Journal on Selected Areas in Information Theory, 1(1):67–83, 2020. URL
https://doi.org/10.1109/JSAIT.2020.2984716.
VidyaMuthukumar,AdhyyanNarang,VigneshSubramanian,MikhailBelkin,DanielHsu,andAnantSahai. Classi-
fication vsregressionin overparameterizedregimes: Doesthe loss functionmatter? J. Mach.Learn. Res., 22(1),
2021. URLhttp://jmlr.org/papers/v22/20-603.html.
MarkRudelsonandRomanVershynin. Smallestsingularvalueofarandomrectangularmatrix. Communicationson
PureandAppliedMathematics,62(12):1707–1739,2009. URLhttps://doi.org/10.1002/cpa.20294.
Ohad Shamir. The implicit bias of benign overfitting. In Po-Ling Loh and Maxim Raginsky (eds.), Proceedings
of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pp.
448–478.PMLR,2022. URLhttps://proceedings.mlr.press/v178/shamir22a.html.
Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cam-
bridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018. URL
https://doi.org/10.1017/9781108231596.
Guillaume Wang, Konstantin Donhauser, and Fanny Yang. Tight bounds for minimum ℓ -norm interpola-
1
tion of noisy data. In International Conference on Artificial Intelligence and Statistics, 2021a. URL
https://proceedings.mlr.press/v151/wang22k.html.
Ke Wang, Vidya Muthukumar, and Christos Thrampoulidis. Benign overfitting in multi-
class classification: All roads lead to interpolation. In M. Ranzato, A. Beygelzimer,
Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information
Processing Systems, volume 34, pp. 24164–24179. Curran Associates, Inc., 2021b. URL
https://proceedings.neurips.cc/paper_files/paper/2021/file/caaa29eab72b231b0af62fbdff89bf
Denny Wu and Ji Xu. On the optimal weighted ℓ regularization in overparameterized linear regres-
2
sion. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neu-
ral Information Processing Systems, volume 33, pp. 10112–10123. Curran Associates, Inc., 2020. URL
https://proceedings.neurips.cc/paper_files/paper/2020/file/72e6d3238361fe70f22fb0ac624a70
10APREPRINT
XingyuXu andYuantaoGu. Benign overfittingofnon-smoothneuralnetworksbeyondlazy training. InFrancisco
Ruiz, Jennifer Dy, and Jan-Willem van de Meent (eds.), Proceedings of The 26th International Conference on
ArtificialIntelligenceandStatistics,volume206ofProceedingsofMachineLearningResearch,pp.11094–11117.
PMLR,2023. URLhttps://proceedings.mlr.press/v206/xu23k.html.
Zhiwei Xu, Yutong Wang, Spencer Frei, Gal Vardi, and Wei Hu. Benign overfitting and grokking in reLU net-
works for XOR cluster data. In The Twelfth InternationalConference on Learning Representations, 2024. URL
https://openreview.net/forum?id=BxHgpC6FNv.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learn-
ing requires rethinking generalization. In International Conference on Learning Representations, 2017. URL
https://openreview.net/forum?id=Sy8gdB9xx.
Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham Kakade. Benign overfitting of constant-
stepsize SGD for linear regression. In MikhailBelkin and Samory Kpotufe (eds.), Proceedingsof Thirty Fourth
ConferenceonLearningTheory,volume134ofProceedingsofMachineLearningResearch,pp.4633–4635.PMLR,
2021. URLhttps://proceedings.mlr.press/v134/zou21a.html.
11APREPRINT
Appendix A Preliminaries onrandom vectors
Recallthatthesub-exponentialnormofarandomvariableX isdefinedas
X :=inf t>0: E[exp(X /t)] 2
k
kψ1
{ | | ≤ }
(seeVershynin,2018,Definition2.7.5)andthatthesub-Gaussiannormisdefinedas
X :=inf t>0: E[exp(X2/t2)] 2 .
k
kψ2
{ ≤ }
ArandomvariableX issub-GaussianifandonlyifX2issub-exponential.Furthermore, X2 = X 2 .
k kψ1 k kψ2
LemmaA.1. Letn (0 ,d 1(I vvT))andsupposethatZ Rm d. Thenwithprobabilityatleast1 ǫ,
d − d ×
∼N − ∈ −
1 1
Zn C Z log .
F
k k≤ k k d ǫ
r
Proof. Let P = I vvT be the orthogonalprojection onto span( v ) , so that Zn is identically distributed to
d ⊥
d 1/2ZPn,wheren− hasdistribution (0 ,I ). FollowingVershyn{ in} (2018,Theorem6.3.2),
− ′ ′ d d
N
Zn d−1/2ZP F = d−1/2ZPn ′ d−1/2ZP F
k k−k k ψ2 k k−k k ψ2
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13)C d 1/2ZP (cid:13)
(cid:13) (cid:13) ≤(cid:13)k − k (cid:13)
Cd 1/2 Z P
−
≤ k kk k
=Cd 1/2 Z
−
k k
Cd 1/2 Z ,
− F
≤ k k
whereweusedthatP isanorthogonalprojectioninthefourthlineandthattheoperatornormisbyboundedaboveby
theFrobeniusnorminthefifthline. Asaresultthesub-Gaussiannormof Zn isboundedas
k k
Zn Zn d 1/2ZP + d 1/2ZP
− F − F
k k ψ2 ≤ k k−k k ψ2 k k ψ2
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
≤(cid:13) (cid:13)Cd−1/2( kZ
kF
+ kZP kF(cid:13) (cid:13)) (cid:13)
(cid:13)
(cid:13)
(cid:13)
Cd 1/2 Z ,
− F
≤ k k
wherethelastlinefollowsfromthecalculation
ZP = PTZT
F F
k k k k
= PZT
F
k k
P ZT
F
≤k kk k
= ZT
F
k k
= Z .
F
k k
Thisimpliesatailbound(seeVershynin,2018,Proposition2.5.2)
dt2
P( Zn t) 2exp , forallt 0.
k k≥ ≤ −C Z 2 ≥
(cid:18) k kF(cid:19)
Settingt=C Z 1log 2,theresultfollows.
k kF d ǫ
q
LemmaA.2. Letn (0 ,d 1(I vvT))andsupposez span( v ) . ThereexistsaC > 0suchthatwith
d − d ⊥
∼ N − ∈ { }
probabilityatleast1 δ
−
1 1
n,z C z log .
|h i|≤ k k d δ
r
Furthermore,thereexistsac>0suchthatwithprobabilityatleast 1
2
c z
n,z k k.
|h i|≥ √d
12APREPRINT
Proof. LetX = n,z . ThenX isGaussianwithvariance
h i
E[X2]=E[zTnnTz]
=zTE[nnT]z
=d 1zT(I vvT)z
− d
−
=d−1zTz
=d 1 z 2.
−
k k
Note the third line above follows from the fact that z span( v ) . Therefore by Hoeffding’sinequality, for all
⊥
∈ { }
t 0
≥
dt2
P( n,z t) 2exp .
|h i|≥ ≤ −C z 2
(cid:18) k k (cid:19)
Settingt=C z 2 1log2,weobtain
k k d δ
q P( n,z t) δ,
|h i|≥ ≤
whichestablishesthefirstpartoftheresult.
Sinced1/2 z 1X isastandardGaussian,thereexistsaconstantcsuchthat
−
k k
1
P(d1/2 z 1X c) .
−
| k k |≥ ≤ 2
Rearranging,weobtain
1
P( n,z cd 1/2 z ) .
−
|h i|≥ k k ≤ 2
Thisestablishesthesecondpartoftheresult.
Appendix B Upper bounding the norm ofthe max-marginclassifierofthe data
HereweestablishkeypropertiesconcerningthedatamodelgiveninDefinition2.1,ourmaingoalbeingtoestablish
boundsonthenormofthemax-marginclassifier. To thisendwefirst identifycertainusefulfactsaboutrectangular
Gaussianmatrices.InwhatfollowsweindexthesingularvaluesofanygivenmatrixAindecreasingorderasσ (A)
1
≥
σ (A) . Furthermore,wedenotethei-th-rowofamatrixAasa .
2 i
≥···
Lemma B.1. Let G Rn d be a Gaussian matrix whose entries are mutually i.i.d. with distribution (0,1). If
×
∈ N
d=Ω n+log1 ,thenwithprobabilityatleast1 δthefollowinginequalitiesaresimultaneouslytrue.
δ −
1(cid:0) . σ (G) (cid:1) C(√d+√n),
1
≤
2. σ (G) c(√d √n).
n
≥ −
Proof. Weproceedbyupperboundingtheprobabilitythateachindividualinequalitydoesnothold.
1.Toderiveanupperboundonσ (G)weusethefollowingfact(seeVershynin,2018,Theorem4.4.5).Foranyǫ>0,
1
P(σ (G) C (√n+√d+ǫ)) 2exp( ǫ2).
1 1
≥ ≤ −
Withǫ=√n+√dandd log4 then
≥ δ
P(σ (G) 2C (√n+√d)) 2exp( d)
1 1
≥ ≤ −
δ
.
≤ 2
2. To derive a lower bound on σ (G) we use the following fact (see Rudelson&Vershynin, 2009, Theorem 1.1).
n
ThereexistconstantsC ,C >0suchthat,foranyǫ>0,
1 2
P(σ n(G) ǫ(√d √n 1)) (C 1ǫ)d −n+1+e−C2d.
≤ − − ≤
13APREPRINT
Letǫ= 1 andletd 2n+ 2+ 1 log4. Then
C1e ≥ C2 δ
(cid:16) (cid:17)
P(σ (A) ǫ(√d √n)) exp( d/2)+exp( C d)
n 2
≤ − ≤ − −
δ δ
+
≤ 4 4
δ
= .
2
Hencebothboundsholdsimultaneouslywithprobabilityatleast1 δ.
−
Thenextlemmaformulateslowerandupperboundsonthesmallestandlargestsingularvaluesofanoisematrixunder
ourdatamodel.
Lemma4.1. LetN Rn ddenotearandommatrixwhoserowsaredrawnmutuallyi.i.d.from (0 , 1(I vvT)).
∈ × N d d d −
Ifd=Ω n+log1 ,thenwithprobabilityatleast1 δ,
δ −
(cid:0) (cid:1) c σ (N) σ (N) C.
n 1
≤ ≤ ≤
fP rr oo mof N. L (0et d,H IH= ).s Sp ia nn c(
e{
dv
}
=) ⊥ Ω∼= nR +d − lo1 g.
1
δLe ,t wN it′ h: pH
rob→
abiR lin tyb ae ta ler aa sn td 1o −m δm ,wat erix haw vehoserowsaredrawnmutuallyi.i.d.
(cid:0)c(√d √(cid:1)n) σ (N ) σ (N ) C(√d+√n).
n ′ 1 ′
− ≤ ≤ ≤
We denotethe aboveeventby ω. Let J : H Rd be the inclusionmap andlet P = I vvT. For anyrandom
d
vectornwithdistribution (0 d,IH),Jnisa→ Gaussianrandomvectorwithcovariancematr− ixJJT =P. Therefore,
d 1/2N JT is a randommN atrix whoserowsare drawnmutuallyi.i.d.from (0 ,d 1P). Thatis, d 1/2N JT is
− ′ d − − ′
identicallydistributedtoN. Forthelowerboundonthen-thlargestsingularN value,ifd 4n,thenconditionalonω
≥ c2
wehave
σ (d 1/2N JT)=d 1/2σ (JN T)
n − ′ − min ′
d−1/2σ min(J)σ min(N ′T)
≥
=d 1/2σ (N T)
− min ′
=d 1/2σ (N )
− n ′
n
c
≥ − d
r
c
.
≥ 2
Noteherewedefineσ tobethesmallestsingularvalueofamatrix. InthefirstlineweusedJN T isalinearmap
min ′
Rn Rd andd n, andinthethirdlineweusedthefactthatJ isaninclusionmap. Fortheupperboundonthe
large→ stsingularva≥ lue,ifd n ,thenconditionalonωwehave
≥ C2
σ (d 1/2N JT)=d 1/2σ (JN T)
1 − ′ − 1 ′
d−1/2σ 1(N ′T)
≤
=d 1/2σ (N )
− 1 ′
n
C+
≤ d
r
2C.
≤
NoteherethatagainweusedthefactthatJ isaninclusionmapinthefirstline. Therefore,ifd=Ω n+log1
δ
P c σ (N) σ (N) 2C P(ω) (cid:0) (cid:1)
n 1
2 ≤ ≤ ≤ ≥
(cid:16) (cid:17) 1 δ.
≥ −
14APREPRINT
Thefollowinglemmaisusefulforconstructingvectorsinthenoisesubspacewithpropertiessuitableforboundingthe
normofthemax-marginsolution.Weremarkthatthesameapproachcouldbeusedinthesettingwherethenoiseand
signalarenotorthogonalbyconsideringthepseudo-inverse([N,v]T) insteadofN .
† †
Lemma B.2. Let [n] be an arbitrary subset such that = ℓ. In the context of the data model given in
Definition 2.1, assuI me⊆ d = Ω n+log1 . Then there exists z|I| Rd such that with probability at least 1 δ the
δ ∈ −
followingholdsimultaneously.
(cid:0) (cid:1)
1. yˆ n ,z =1foralli ,
i i
h i ∈I
2. yˆ n ,z =0foralli / ,
i i
h i ∈I
3. z v,
⊥
4. C z C .
1 2
≤k k≤
Proof. Recall thatN Rn d is a randommatrix whose rowsare selected i.i.d. from (0 ,d 1(I vvT)). By
× d − d
∈ N −
Lemma4.1,withprobabilityatleast1 δwehave
−
c σ (N) σ (N) C.
n 1
≤ ≤ ≤
Conditioningon this event, we will constructa vectorz which satisfies the desired properties. Letw Rn satisfy
w = yˆ if i andw = 0 otherwise. Letz = N w, whereN = NT(NNT) 1 is the rightps∈ eudo-inverse
i i i † † −
∈ I
ofN. ThenNz = w. Inparticular,fori , yˆ n ,z = yˆw = 1, andfori / , yˆ n ,z = yˆw = 0. This
i i i i i i i i
∈ I h i ∈ I h i
establishesproperties1and2. SinceN w isinthespanoftheset n , itisorthogonaltov. Thisestablishes
† i i [n]
property3. Finally,wecanbound
{ }∈
z = N †w
k k k k
N w
†
≤k kk k
w
= k k
σ (N)
n
w
k k
≤ c
√ℓ
=
c
and
z = N w
†
k k k k
σ (N ) w
n †
≥ k k
w
= k k
σ (N)
1
w
k k
≥ C
√ℓ
=
C
whichestablishesproperty4.
WithLemmaB.2inplacewearenowabletoappropriatelyboundthemax-marginnorm.
LemmaB.3. InthecontextofthedatamodelgiveninDefinition2.1,letw denotethemax-marginclassifierofthe
∗
trainingdata(X,yˆ),whichexistsalmostsurely. Ifd=Ω n+log 1 thenwithprobabilityatleast1 δ
δ −
(cid:0) (cid:1)
1 k
w C + ,
∗
k k≤ sγ 1 γ
−
whereC >0isaconstant.
15APREPRINT
Proof. Undertheassumptionsstated,theconditionsofLemmaB.2holdwithprobabilityatleast1 δ. Conditioning
−
onthislet
1 2
w = v+ z
√γ √1 γ
−
wherezisthevectorconstructedinLemmaB.2with = . Foranyi [n]wethereforehave
I B ∈
yˆ x ,w =β +2yˆ n ,z .
i i i i i
h i h i
Asaresult,fori
∈G
yˆ x ,w =1+2yˆ n ,z =1,
i i i i
h i h i
whileforl
∈B
yˆ x ,w = 1+2yˆ n ,z =1.
i i i i
h i − h i
Asaresultyˆ x ,w =1foralli [n]. Furthermore,observe
i i
h i ∈
1 4
w 2 = + z 2
k k γ 1 γk k
−
1 k
C +
≤ γ 1 γ
(cid:18) − (cid:19)
forauniversalconstantC. Toconcludeobserve w w bydefinitionofbeingmax-margin.
∗
k k≤k k
LemmaB.3constructsaclassifierwithmarginoneusinganappropriatelinearcombinationofthesignalvectorvand
a vector in the noise subspace which classifies all noise componentsbelongingto bad points correctly. This bound
is useful for the benign overfitting setting in which γ is not too small. However, for small γ, as is the case in the
non-benignoverfittingsetting, thisboundbehavespoorlyastheonlywaythe constructioncanfitallthe datapoints
is by making the coefficientin frontof the v componentlarge. For the non-benignoverfitting setting we therefore
requirea differentapproachand instead fit all data pointsbased on their noise componentsalone. In particular, the
followingboundbehavesbetterthanthatgiveninLemmaB.3whenγ approaches0.
LemmaB.4. InthecontextofthedatamodelgiveninDefinition2.1,letw denotethemax-marginclassifierofthe
∗
trainingdata(X,yˆ),whichexistsalmostsurely. Ifd=Ω n+log 1 thenwithprobabilityatleast1 δ
δ −
(cid:0) n (cid:1)
w C .
∗
k k≤ 1 γ
r −
Proof. ApplyingLemmaB.2with = [n]thenwithprobability1 δ thereexistsz Rd suchthat z = Θ(√n),
I − ∈ k k
yˆ n ,z =1foralli [n]andz v. Conditioningonthisevent,let
i i
h i ∈ ⊥
1
w = z.
√1 γ
−
Thenforalli [n],
∈
yˆ x ,w =yˆ n ,z =1.
i i i i
h i h i
Furthermore,thereexistsaconstantC >0suchthat
n
w C .
k k≤ 1 γ
r −
Toconcludeobserve w w bydefinitionofbeingmax-margin.
∗
k k≤k k
Appendix C Linear models
C.1 Sufficientconditionsforbenignandharmfuloverfitting
Westartbyprovidingalemmawhichcharacterizesthegeneralizationpropertiesoflinearclassifiers.
16APREPRINT
Lemma C.1. In the context of the data model given in Definition 2.1, considerthe linear classifier w = a v +z,
v
wherea Rand z,v =0. Ifa 0,thenthegeneralizationerrorcanbeboundedasfollows:
v v
∈ h i ≥
d γ a2
P(y w,x 0) exp v .
h i≤ ≤ −21 γ z 2
(cid:18) − k k (cid:19)
and
1 dγ a
P(y w,x 0) v .
h i≤ ≥ 2 −s2π(1 γ) z
− k k
Proof. RecallfromDefinition2.1thatatestpair(x,y)satisfies
x=y(√γv+ 1 γn),
−
wheren (0 ,d 1(I vvT))isarandomvector.LetXp=y w,x ,so
d − d
∼N − h i
X = a v+z,√γv+ 1 γn
v
h − i
=√γa v+ 1 γ np,z .
− h i
ThenX isaGaussianrandomvariablewithexpectationp
E[X]=√γa + 1 γE[ n,z ]
v
− h i
=√γa
v p
andvariance
Var(X)=(1 γ)Var( n,z )
− h i
=(1 γ)E[zTnnTz]
−
1 γ
= − zT(I vvT)z
d
d −
1 γ
= − zTz
d
(1 γ) z 2
= − k k .
d
ByHoeffding’sinequality,forallt 0,
≥
t2d
P(X √γa t) exp .
≤ v − ≤ −2(1 γ) z 2
(cid:18) − k k (cid:19)
Settingt=√γa v,weobtain
γda2
P(X 0) exp v ,
≤ ≤ −2(1 γ) z 2
(cid:18) − k k (cid:19)
whichestablishestheupperboundonthegeneralizationerror.
Toprovethelowerbound,weintegrateastandardGaussianpdf:
P(X 0)=P
X −√γa
v
γd a
v
≤ √1 γ z /√d ≤−s1 γ z !
− k k − k k
1 1 0
=
e−t2/2dt
2 − √2π γd av
Z−q1−γkzk
1 1 γd a
v
.
≥ 2 − √2π s1 γ z !
− k k
The following result establishes benign and non-benignoverfitting for linear models and data as per Definition 2.1,
withaphasetransitionbetweentheseoutcomesdependingonthesignaltonoiseparameterγ.
17APREPRINT
Theorem C.1. In the contextof the datamodeldescribed in Section2, let w be a max-marginlinear classifierof
∗
thetrainingdata.Let :Rn d 1 n Rd bealearningalgorithmwhichisapproximatelymargin-maximizing,
×
A ×{± } →
Definition 2.3. For δ,ǫ (0,1], let d = Ω 2nlog1 +log1 . Then with probability at least 1 δ over the
∈ |A| ǫ δ −
randomnessofthetrainingdata(X,yˆ),thefollowinghold.
(cid:0) (cid:1)
(A) Ifγ =Ω( 2n 1)andk =O( 2n),thenthegeneralizationerrorof (X,yˆ)isupperboundedbyǫ.
− −
|A| |A| A
(B) Ifγ =O( 2ǫ2d 1),thenthegeneralizationerrorof (X,yˆ)islowerboundedby 1 ǫ.
|A|− − A 2 −
Proof. For training data (X,yˆ) let w = (X,yˆ) be the learned linear classifier. First, recall x
i
= √γy iv +
A
√1 γn foralli [n], v =1,andobservethatwecandecomposethevectorwasw =a v+z,wherea R,
i v v
− ∈ k k ∈
andz v. Asaresult,foreachi [n],
⊥ ∈
yˆ x ,w =√γa β + 1 γyˆ n ,z . (8)
i i v i i i
h i − h i
Firstweestablish(A).Foralgorithmslearninglinearmodelst phenbydefinition 1as w w
∗
. Therefore
|Ak ≥ k k ≤ k k
thegivenconditionondaboveensuresd = Ω n+log 1 ,andthereforeLemmas4.1andB.3showthatwithproba-
δ
bilityatleast1
−
δ 2, kN k2, kN Gk2
≤
C and k(cid:0)w
∗ k ≤
C(cid:1) γ1 +
1
−k γ. HereN
G
denotesthematrixformedbytaking
onlytherowsofN whichsatisfyβ =1. Wedenotethiseqventωandconditiononitinallthatfollowsfortheproofof
(A).As isapproximatelymaxmarginthengivenequation8wehaveforalli
A ∈G
1 √γa + 1 γyˆ n ,z .
v i i
≤ − h i
Supposethat√γa
v
< 1 2. Thentheaboveinequalityimpplies√1 −γyˆ
i
hn i,z
i ≥
21 foralli
∈
G. Squaringandthen
summingthisexpressionoveralli itfollowsthat
∈G
n k
− (1 γ) n ,z 2
i
4 ≤ − |h i|
i
X∈G
(1 γ) N z 2
≤ − k G k
(1 γ) N 2 z 2.
≤ − k Gk k k
Since isapproximatelymargin-maximizing,
A
n k
− (1 γ)C z 2
4 ≤ − k k
(1 γ)C w 2
≤ − k k
C 2(1 γ) w 2
∗
≤ |A| − k k
1 k
C 2(1 γ) +
≤ |A| − γ 1 γ
(cid:18) − (cid:19)
C 2
|A| +C 2k,
≤ γ |A|
whichfurtherimpliesn C |A|2 +C 2kforsomeotherconstantC. Forthisinequalitytohold,either C |A|2 n
≤ γ |A| γ ≥ 2
or C |A|2k
≥
n 2. With k
≤
4Cn
2
and γ
≥
4C n|A|2 , neither of these can be true and therefore we conclude that
√γa
v
≥
1 2. Then |A|
a2 1
v
z 2 ≥ 4γ z 2
k k k k
1
≥ 4γ w 2
k k
1
≥ 4γ 2 w 2
|A| k ∗k
C 1
≥ 2γ 1 + k
|A| γ 1 γ
−
C 1
≥ 2γ 1 + n
|A| γ 1 γ
−
18APREPRINT
where the positive constant C may vary between inequalities. Letting (x,y) denote a test point pair, then by
LemmaC.1withγ n 1andd 2C 1 2nlog(1/ǫ)itfollowsthat
− −
≥ ≥ |A|
d γ a2
P(y w,x 0 ω) exp v
h i≤ | ≤ −21 γ z 2
(cid:18) − k k (cid:19)
Cd
exp
≤ − |A|2 1 −γγ +n 
 (cid:16) (cid:17)
Cd
exp
≤ − 2 1 +n 
|A| γ
 Cd(cid:16) (cid:17)
exp
≤ −2 2n
(cid:18) |A| (cid:19)
ǫ.
≤
Hence the generalization error is at most ǫ when ω occurs, which happens with probability at least 1 δ. This
−
establishes(A).
Wenowturnourattentionto(B).Asjustifiedintheproofofpart(A)wecansetd = Ω n+log1 . Thereforefrom
δ
Lemmas4.1and B.4 with probabilityatleast 1 δ it holdsthat N 2 C and w C√n. We denotethis
eventω
andconditiononitinallthatfollowsfor− theproofof(B).k InpG ak rtic≤
ular,
k ∗(cid:0) k ≤ √1 −γ(cid:1)
′
a2+ z 2 = w 2
v k k k k
2 w 2
∗
≤|A| k k
C 2n
|A| . (9)
≤ 1 γ
−
Foralli [n],
∈
1 √γβ a + 1 γyˆ n ,z .
i v i i
≤ − h i
Forthisinequalitytohold, either √γa
v
1/2or√1 pγyˆ
i
n i,z 1/2foralli [n]. If √γa
v
1/2,then
| | ≥ − h i ≥ ∈ | | ≥
withγ 1 wehave
≤ 4(C+1) 2d
|A|
a2 2C 2d 2C 2n.
v ≥ |A| ≥ |A|
However,fromequation9wehave
C 2n
2C 2n> |A| a2
|A| 1 γ ≥ v
−
whichisacontradiction. Therefore,undertheregimespecifiedthereexistsaC > 0suchthatwithγ 1 ,we
≤ C 2d
|A|
have√1 γyˆ n ,z 1/2foralli [n]. Rearranging,squaringandsummingoveralli [n]yields
i i
− h i≥ ∈ ∈
n
n
n ,z 2 Nz 2 C z 2,
i
4(1 γ) ≤ |h i| ≤k k ≤ k k
− i=1
X
wherethefinalinequalityfollowsfromconditioningonω. Then
a2 w 2
v k k
z 2 ≤ z 2
k k k k
2 w 2
∗
|A| k k
≤ z 2
k k
C 2 n
|A| 1 γ
−
≤ n
1 γ
−
C 2,
≤ |A|
19APREPRINT
wheretheconstantC >0mayvarybetweeninequalities. Letting(x,y)denoteatestpointpair,byLemmaC.1with
γ ǫ2 itfollowsthat
≤ Cd 2
|A|
1 dγ a
P(y w,x 0 ω) v
h i≤ | ≥ 2 −s2π(1 γ) z
− k k
1 Cdγ 2
|A|
≥ 2 −s 1 γ
−
1
Cdγ 2
≥ 2 − |A|
1 p
ǫ.
≥ 2 −
Hencethegeneralizationerrorisatleast 1 ǫwhenω occurs,whichhappenswithprobabilityatleast1 δ. This
2 − ′ −
establishes(B).
Appendix D Leaky ReLU Networks
InthissectionweconsideraleakyReLUnetworkf :R2m Rd Rwithforwardpassgivenby
× →
2m
f(W,x)= ( 1)jσ( w ,x ),
j i
− h i
j=1
X
whereσ(z) = max(αz,z)forsome α (0,1). Foranysuchnetwork,we maydecomposetheneuronweightsw
j
∈
intoasignalcomponentandanoisecomponent,
w =a v+z ,
j j j
wherea R andz Rd satisfies z v. Theratioa / z thereforegrowswiththealignmentofw withthe
j j j j j j
∈ ∈ ⊥ k k
signalandshrinksifw insteadalignsmorewiththenoise. Collectingthenoisecomponentsoftheweightvectors,let
j
Z R(2m) d bethematrixwhosej-throwisz . Inordertotrackthealignmentofthenetworkasawholewiththe
× j
∈
signalversusnoisesubspacesweintroducethefollowingquantities.Let
2m
A =f(W,v)= ( 1)jσ(a ),
1 j
−
j=1
X
2m
A =f(W, v)= ( 1)j+1σ( a )
1 j
− − − −
j=1
X
bereferredtoasthepositiveandnegativesignalactivationofthenetworkrespectively.Moreover,define
A =min(A ,A )
min 1 1
−
astheworst-casesignalactivationofthenetwork,and
2m
A = ( 1)ja
lin j
−
j=1
X
asthelinearizednetworkactivation.Tomeasuretheamountofnoisethenetworklearnswedefine
2m
z = ( 1)jz .
lin j
−
j=1
X
D.1 Trainingdynamics
Theorem3.1. Letf : Rp Rn RbealeakyReLUnetworkwithforwardpassasdefinedbyequation1. Suppose
× →
the step size η andinitialization conditionλ satisfy Assumption1. Then for any linearly separable data set (X,yˆ)
(X,yˆ,η,λ)convergesafterT iterations,where
GD
A
C w 2
∗
T k k .
≤ ηα2m
20APREPRINT
Furthermore isapproximatelymarginmaximizingonf with
GD
A
C
.
GD
|A |≤ α√m
Proof. OurapproachistoadaptaclassicaltechniqueusedfortheproofofconvergenceofthePerceptronalgorithm
forlinearlyseparabledata. ThisisalsotheapproachadoptedbyBrutzkusetal.(2018). Thekeyideaoftheproofis
toboundintermsofthenumberofupdatesboththenormofthelearnedvectorw aswellasitsalignmentwithany
linearseparatorofthedata. FromtheCauchy-Schwarzinequalitytheseboundscannotcross,andthisinturnbounds
the numberofupdatesthatcanoccur. Analogously,we trackthe alignmentof W(t) with the max-marginclassifier
alongwiththeFrobeniusnormoftheW(t). Tothisenddenote
G(t)= W(t) 2
k j kF
and
2m
F(t)= ( 1)j w(t),w ,
− h j ∗ i
j=1
X
wherew isamax-marginlinearclassifierofthedataset. Recallthat (t) = i [n] : yˆf(W(t),x ) < 1 denotes
∗ i i
F { ∈ }
thenumberofactivedatapointsattrainingstept. WealsodefineU(t)= t −1 (s) tobethenumberofdatapoint
s=0|F |
updatesbetweeniterations0andt. First,byCauchy-Schwarz
P
1 w ,x w x
∗ i ∗ i
≤h i≤k k·k k
foralli [n]. Therefore,
∈
1
w .
∗
k k≥ min x
i [n] i
∈ k k
ByAssumption1,forallj [2m],
∈
√α w
w(0) k ∗ k. (10)
k j k≤ mmin x ≤ αm
i [n] i
∈ k k
Forallt 0,theupdateruleofGDimplies
≥
2m
G(t+1)= w(t+1) 2
k j k
j=1
X
2
2m
= w(t)+η( 1)j σ˙( w(t),x )yˆx
(cid:13) j − h j i i i i(cid:13)
Xj=1(cid:13)
(cid:13)
i ∈XF(t) (cid:13)
(cid:13)
2m (cid:13) 2m (cid:13) 2m
(cid:13) (cid:13)
= (cid:13)w(t) 2+2η ( 1)jσ˙( w(t) ,x(cid:13))yˆ w(t) ,x +η2 σ˙( w(t) ,x ) yˆx ,yˆx
k j k − h j i i i h j i i h j i i h i i i ℓ i
Xj=1 Xj=1i ∈XF(t) Xj=1i,lX∈F(t)
2m 2m
w(t) 2+2η ( 1)jσ˙( w(t),x )yˆ w(t),x +2mη2 (t) 2max x 2.
≤ k j k − h j i i i h j i i |F | i [n]k i k
Xj=1 Xj=1i ∈XF(t) ∈
Observethatforallz R,σ(s)=σ˙(z)z,socanrewritethesecondtermoftheaboveexpressionas
∈
2m 2m
2η ( 1)jσ˙( w(t),x )yˆ w(t),x =2η ( 1)jσ( w(t),x )yˆ
− h j i i i h j i i − h j i i i
Xj=1i ∈XF(t) Xj=1i ∈XF(t)
=2η yˆf(W(t),x )
i i
i ∈XF(t)
<2η 1
i ∈XF(t)
=2η (t)
|F |
21APREPRINT
wheretheinequalityinthesecond-to-lastlinefollowsaswearesummingover (t), whichbydefinitionconsistsof
F
thei [n]suchthatyˆf(W(t),x )<1. Asaresultweobtain
i i
∈
2m
G(t+1) w(t) 2+2η (t) +2mη2 (t) 2max x 2
≤ k j k |F | |F | i [n]k i k
j=1 ∈
X
=G(t)+2η (t) +2mη2 (t) 2max x 2
i
|F | |F | i [n]k k
∈
G(t)+4η (t) ,
≤ |F |
wherethelastlinefollowssince
1 1
η .
≤ mnmax x 2 ≤ (t) mmax x 2
i ∈[n] k i k |F | i ∈[n] k i k
Byequation10,theinitializationsatisfies
2m
G(0)= w(0) 2
k j k
j=1
X
2m w 2
∗
k k
≤ α2m2
j=1
X
2 w 2
∗
= k k
α2m
Sobyinduction,forallt 0
≥
G(t)
2 kw
∗
k2 +3ηt −1
(s) =
2 kw
∗
k2
+3ηU(t). (11)
≤ α2m |F | α2m
s=0
X
NextwefindaboundforF(t). Forallt 0thenbydefinitionoftheGDupdate
≥
2m
F(t+1)= ( 1)j w(t+1),w
− h j ∗ i
j=1
X
2m 2m
= ( 1)j w(t),w +η σ˙( w(t),x )yˆ w ,x .
− h j ∗ i h j i i i h ∗ i i
Xj=1 Xj=1i ∈XF(t)
Sinceyˆ w ,x 1foralli [n],theaboveexpressionisboundedbelowby
i ∗ i
h i≥ ∈
2m 2m 2m
( −1)j hw j(t),w ∗ i+η σ˙( hw j(t),x i i)yˆ i =F(t)+η σ˙( hw j(t),x i i)
Xj=1 Xj=1i ∈XF(t) Xj=1i ∈XF(t)
2m
F(t)+η α
≥
Xj=1i ∈XF(t)
F(t)+2ηmα (t) .
≥ |F |
HenceunrollingtheupdateforGDforallt 0itfollowsthat
≥
t 1
−
F(t+1) F(0)+2ηmα (s) .
≥ |F |
s=0
X
22APREPRINT
Atinitialization,byequation10then
2m
F(0)= ( −1)j hw j(0),w ∗
i
j=1
X
2m
w(0) w
≥− k j k·k ∗ k
j=1
X
2m w 2
∗
k k
≥− αm
j=1
X
2 w 2
∗
= k k .
− α
Thereforebyinduction,forallt 0wehave
≥
F(t)
2 kw
∗
k2 +2ηmαt −1
(s)
≥− α |F |
s=0
X
2 w 2
∗
= k k +2ηmαU(t).
− α
CombiningourboundsforF(t)andG(t),weobtain
2 w 2
∗
k k +2ηmαU(t) F(t)
− α ≤
2m
= ( −1)j hw j(t),w ∗
i
j=1
X
2m
w w(t)
≤k ∗ k k j k
j=1
X
1/2
2m
w 2m w(t) 2
≤k ∗ k k j k 
j=1
X
= w  (2mG(t))1/2 
∗
k k
4 w 2 1/2
w ∗ k ∗ k +6mηU(t) .
≤k k α2
(cid:18) (cid:19)
Thisimpliesthateither
2 w 2
k ∗ k +2ηmαU(t) 0 (12)
− α ≤
or
2 w 2 2 4 w 2
k ∗ k +2ηmαU(t) w ∗ 2 k ∗ k +6mηU(t) . (13)
− α ≤k k α2
(cid:18) (cid:19) (cid:18) (cid:19)
If(12)holds,then
w 2
∗
U(t) k k .
≤ ηα2m
If(13)holds,thenrearrangingyields
4η2m2α2U(t)2 14 w ∗ 2ηmU(t)
≤ k k
7 w 2
∗
U(t) k k .
≤ 2ηα2m
23APREPRINT
Therefore,inbothcasesthereexistsaconstantC suchthat
C w 2
U(t) k ∗ k . (14)
≤ ηα2m
Thisholdsforallt Nandtherefore
∈
∞ (t) C kw ∗ k2 < .
|F |≤ ηα2m ∞
t=0
X
Thisimpliesthatthereexistss Nsuchthat (s) = 0. LetT Nbetheminimaliterationsuchthat (T) = 0.
∈ |F | ∈ |F |
Thenforalli [n]yˆf(W(T),x ) 1. SothenetworkachieveszerolossandalsohaszerogradientatiterationT.
i i
∈ ≥
Inparticular,
T
=T −1
1
T −1
(t)
C kw
∗
k2
.
≤ |F |≤ ηα2m
t=0 t=0
X X
Tobound wecombineequations(14)and(11)toobtain
GD
|A |
2 w 2
∗
G(T) k k +3ηU(t)
≤ α2m
2 w 2 C w 2
∗ ∗
k k + k k
≤ α2m ηα2m
C w 2
∗
k k .
≤ α2m
Asaresultforalllinearlyseparabledatasets(X,yˆ)
W C
F
k k =
w α√m
k ∗k
andtherefore
C
GD
|A |≤ α√m
asclaimed.
D.2 Benignoverfitting
ToestablishbenignoverfittinginleakyReLUnetworks,wefirstdetermineanupperboundonthegeneralizationerror
ofthemodelintermsofthesignal-to-noiseratioofthenetworkweights.
LemmaD.1. Letǫ (0,1). Supposethat
∈
A (1 γ)mlog1
min C − ǫ.
Z F ≥ 2 s γd
k k
Thenfortestdata(x,y)asperDefinition2.1,
P(yf(W,x) 0) ǫ.
≤ ≤
Proof. Recallthatatestpoint(x,y)satisfies
x=y(√γv+ 1 γn),
−
p
24APREPRINT
wheren (0 , 1(I vvT)). Ifyf(W,x) 0,then
∼N d d d − ≤
0 yf(W,x)
≥
2m
= ( 1)jyσ( w ,x )
j
− h i
j=1
X
2m
= ( 1)jyσ( a v+z ,y(√γv+ 1 γn )
j j
− h − i
j=1
X p
2m
= ( 1)jyσ(y(√γa + 1 γ z ,n ))
j j
− − h i
j=1
X p
2m 2m
( 1)jyσ(y√γa ) 1 γ z ,n
j j
≥ − − − |h i|
j=1 j=1
X Xp
2m
=√γA 1 γ z ,n .
y j
− − |h i|
j=1
Xp
WhenA 0,thisimpliesthat
min
≥
2
2m
γA2 (1 γ) z ,n
min ≤ −  |h j i|
j=1
X
 2m 
2m(1 γ) z ,n 2
j
≤ − |h i|
j=1
X
=2m(1 γ) Zn 2
− k k
2m(1 γ) Z 2 n 2,
≤ − k kFk k
wherethesecondinequalityisanapplicationofCauchy-Schwarz.So
γA2
P(yf(W,x) 0) P Zn 2 min .
≤ ≤ k k ≥ 2m(1 γ)
(cid:18) − (cid:19)
ByLemmaA.1,theaboveprobabilityislessthanǫif
γ 1 1
A C Z log ,
min F
2m(1 γ) ≥ k k d ǫ
r − r
orequivalently,
A (1 γ)mlog1
min C − ǫ.
Z F ≥ 2 s γd
k k
Wewillalsoneedthenumberofpositivelabelstobe(mildly)balancedwiththenumberofnegativelabels.
Lemma D.2. Let δ > 0 and suppose that ℓ = Ω log1 . Let [n] be an arbitrary subset such that = ℓ.
δ I ⊆ |I|
Considertrainingdata(X,y)asperthedatamodelgiveninDefinition2.1. Thenwithprobabilityatleast1 δ,
(cid:0) (cid:1) −
ℓ 3ℓ
i :y =1 .
i
4 ≤|{ ∈S }|≤ 4
Proof. For i let Y be a randomvariable taking the value 1 if y = 1 and 0 if y = 1. Then the Y are i.i.d.
i i i i
Bernoullirand∈ omI variableswithP(Y =1)= 1. Let −
i 2
Y = Y = i :y =1
i i
|{ ∈S }|
i
X∈I
25APREPRINT
sothatE[Y]= l. ByChernoff’sinequality,forallt (0,1),
2 ∈
ℓ ℓ
P Y t 2e Cℓt2 .
−
− 2 ≥ 2 ≤
(cid:18)(cid:12) (cid:12) (cid:19)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
Settingt= 1,weseethat ℓ Y 3ℓ withprobabilityatleast
2 4 ≤ ≤ 4
Cℓ
1 2exp 1 δ
− − 4 ≥ −
(cid:18) (cid:19)
whenℓ=Ω log1 .
δ
(cid:0) (cid:1)
WearenowabletoproveourmainbenignoverfittingresultforleakyReLUnetworks.
Theorem 3.2. Under the setting given in Definition 3.1, let δ,ǫ (0,1) and suppose is approximately margin-
∈ A
maximizing. If n = Ω log1
δ
, d = Ω (1+ |A|2m)nlog1
ǫ
, k = O( 1+mn 2), and γ = Ω 1+m n|A|2 then with
|A|
probabilityatleast1 (cid:0)δover (cid:1)(X,yˆ) (cid:0) (cid:1) (cid:16) (cid:17)
−
P(yf(W,x) 0 X,yˆ) ǫ.
≤ | ≤
Proof. Sinced = Ω(n) = Ω n+log1 ,byLemmaB.3,withprobabilityatleast1 δ overtherandomnessofthe
δ − 3
data,themax-marginclassifierw satisfies
∗
(cid:0) (cid:1)
1 k
w C + .
∗
k k≤ sγ 1 γ
−
We denote this eventby ω . For s 1, 1 , let denote the set of i such that y = s. If n = Ω 1 and
1 ∈ { − } Gs ∈ G i δ
k =O(n),then =Ω log1 . Undertheseassumptions,byLemmaD.2,
|G| δ (cid:0) (cid:1)
(cid:0) (cid:1)
1
Cn
s
|G |≥ 4|G|≥
forboths ∈{1, −1 }withprobabilityatleast1 −3δ.Wedenotethiseventbyω 2.Fors ∈{1, −1 },letN
s
∈R |Gs|×dbe
thematrixwhoserowsareindexedby andaregivenbythevectorsn fori . Asd =Ω(n) = Ω n+log1
Gs i ∈ Gs δ
andtherowsofN aredrawnmutuallyi.i.d.from (0 ,d 1(I vT)), thefollowingholdsbyLemma4.1. With
s N d − d − (cid:0) (cid:1)
probabilityatleast1 δ overtherandomnessofthetrainingdata, N C forboths 1, 1 . Wedenotethis
eventbyω . Letω =− ω3 ω ω . BytheunionboundP(ω) 1k δs .k W≤ econditionon∈ ω{ for− the} remainderofthis
3 1 2 3
∩ ∩ ≥ −
proof.
SinceW = (X,yˆ)and isapproximatelymarginmaximizing,
A A
W w
F ∗
k k ≤|A|·k k
1 k
C + . (15)
≤ |A|sγ 1 γ
−
26APREPRINT
Lets 1,1 besuchthatA =A . Sincethenetworkattainszeroloss,foralli ,
s min s
∈{− } ∈G
1 yˆf(W,x )
i i
≤
2m
= ( 1)jyˆσ( w ,x )
i j i
− h i
j=1
X
2m
= ( 1)jy σ( a v+z ,√γy v+ 1 γn )
i j j i i
− h − i
j=1
X p
2m
= ( 1)jy σ(√γa y + 1 γ z ,n )
i j i j i
− − h i
j=1
X p
2m 2m
( 1)jy σ(√γa y )+ 1 γ z ,n
i j i j i
≤ − | − h i|
j=1 j=1
X X p
2m 2m
=√γ ( 1)jsσ(sa )+ 1 γ z ,n
j j i
− − |h i|
j=1 j=1
X p X
2m
=√γA + 1 γ z ,n
s j i
− |h i|
j=1
p X
2m
=√γA + 1 γ z ,n .
min j i
− |h i|
j=1
p X
Hence, we have either √γA
s
≥
21 or √1 −γ 2 j=m 1|hz j,n
i
i| ≥
21 for all i
∈
Gs. We consider these two cases
separately.
P
If√γA
min
≥
1 2,then
A A
min min
Z ≥ W
F F
k k k k
1
≥ 2√γ W
F
k k
1
C
≥ √γ 1 + k
|A| γ 1 γ
−
1q
=C
1+ kγ
|A| 1 γ
−
q 1
C
≥ + kγ
|A| |A| 1 γ
−
1 q
C .
≥ + nγ
|A| |A| 1 γ
−
q
ThenbyLemmaD.1,thenetworkhasgeneralizationerrorlessthanǫwhen
1 (1 γ)mlog1
C − ǫ
+ nγ ≥ s γd
|A| |A| 1 γ
−
q
orequivalently
(1 γ)mlog1 mnlog1 C
− ǫ + ǫ .
s γd s d ≤
|A|
27APREPRINT
Thisissatisfiedwhend = Ω |A|2mnlog1
ǫ
andγ = Ω( n1),whichistruebyassumption. Soif√γA
min ≥
1 2,then
thenetworkhasgeneralizationerrorlessthanǫwheneverωoccurs,whichhappenswithprobabilityatleast1 δ.
(cid:0) (cid:1) −
Now suppose that √1 γ 2m z ,n 1 for all i . Squaringboth sides of the inequality and applying
− j=1|h j i i| ≥ 2 ∈ Gs
Cauchy-Schwarz,weobtain
P
2
2m
1
(1 γ) z ,n
j i
4 ≤ −  |h i|
j=1
X
 2m 
2m(1 γ) z ,n 2.
j i
≤ − |h i|
j=1
X
Summingoveralli ,weobtain
s
∈G
2m
|Gs | 2m(1 γ) z ,n 2
j i
4 ≤ − |h i|
i X∈GsXj=1
2m
=2m(1 γ) N z 2,
− k Gs j k
j=1
X
Applyingω andω ,weobtainthebound
2 3
2m
n Cm(1 γ) N z 2
≤ − k Gs j k
j=1
X
2m
Cm(1 γ) N 2 z 2
≤ − k Gsk k j k
j=1
X
2m
Cm(1 γ) z 2
j
≤ − k k
j=1
X
=Cm(1 γ) Z 2
− k kF
Cm(1 γ) W 2.
≤ − k kF
Thenapplying(15),
n Cm(1 γ) W 2
≤ − k kF
1 k
Cm(1 γ) 2 +
≤ − |A| γ 1 γ
(cid:18) − (cid:19)
1
Cm 2 +k .
≤ |A| γ
(cid:18) (cid:19)
Thisimpliesthat
Cm 2
n |A|
≤ γ
or
Cn
k .
≥ m 2
|A|
Neither of these conditionscan occur if γ = Ω m | nA|2 and k = O n 2m . Thus, in all cases, the networkhas
|A|
generalizationerrorlessthanǫwhenωoccurs,w(cid:16)hichhap(cid:17)penswithpro(cid:16)bability(cid:17)atleast1 δ.
−
D.3 Non-benignoverfitting
InthissectionweshowthatleakyReLUnetworkstrainedonlow-signaldataexhibitnon-benignoverfitting. Firstwe
characterizethetrainingdynamicsofgradientdescentbyboundingthenoisecomponentlearnedbythenetwork.
28APREPRINT
LemmaD.3. Letλ,δ > 0. Supposethatd Ω n+log1 . Inthecontextoftrainingdata(X,yˆ)sampledunder
≥ δ
the data model given in Definition 2.1, let W = (X,yˆ,η,λ). Then with probability at least 1 δ over the
GD
(cid:0)A (cid:1) −
randomnessof(X,yˆ)
C
Z 2 2λ√2m Z 2mλ2 ( z +2mλ)2.
k kF − k kF − ≤ αm k lin k
Proof. Ateachiterationofgradientdescent,
n
w(t+1) =w(t) +η( 1)j b(t) yˆx ,
j j − ij i i
i=1
X
where
0 ifyˆf(W(t),x ) 1
i i
≥
b( ijt) = 1 ifyˆ if(W(t),x i)<1and hw j(t),x
i
i≥0.
α otherwise.
LetT betheiterationatwhichgradientdescentterminates.Thenforeachj [2m],
∈
T 1 n
w =w(T) =w(0)+η( 1)j − b(t)yˆx .
j j j − ij i i
t=0 i=1
XX
Thenthenoisecomponentofw isgivenby
j
z =w w ,v v
j j j
−h i
T 1 n
=w(0) w(0),v v+η( 1)j − b(t)yˆ(x x ,v v)
j −h j i − ij i i −h i i
t=0 i=1
XX
T 1 n
=w(0) w(0),v v+η( 1)j − b(t)yˆn .
j −h j i − ij i i
t=0 i=1
XX
Define
zˆ =z w(0)+ w(0),v v
j j − j h j i
andlet
2m
zˆ = ( 1)jzˆ ,
lin j
−
j=1
X
Thenforallj [2n],
∈
( zˆ z )2 zˆ z 2
j j j j
k k−k k ≤k − k
= w(0) w(0),v v 2
k j −h j i k
w(0) 2
≤k j k
λ2.
≤
Furthermore,if z zˆ thentheaboveimplies zˆ z +λwhileif z zˆ thenthisinequalityholds
j j j j j j
k k≤k k k k≤k k k k≥k k
trivially.Asaresult,
zˆ 2 z 2 = ( zˆ + z ) ( zˆ z )
j j j j j j
k k −k k | k k k k · k k−k k |
zˆ + z zˆ z )
(cid:12) (cid:12) j j j j
(cid:12) (cid:12)≤|k k k k|·|k k−k k |
(2 z +λ)(λ).
j
≤ k k
29APREPRINT
If z zˆ thentheaboveimplies zˆ 2 z 2 λ(2 z +λ),if z zˆ thisinequalityistriviallytrue.
j j j j j j j
k k≥k k k k ≥k k − k k k k≤k k
Asaresult,
2m 2m
zˆ 2 ( z 2 λ(2 z +λ))
j j j
k k ≥ k k − k k
j=1 j=1
X X
2m 2m
= z 2 2λ z 2mλ2
j j
k k − k k−
j=1 j=1
X X
1/2
2m 2m
z 2 2λ√2m z 2 2mλ2
j j
≥ k k −  k k  −
j=1 j=1
X X
= Z 2 2λ√2m Z  2mλ2, (16)
k kF − k kF −
wherethethirdlineisanapplicationofCauchy-Schwarz.Moreover,
2m
zˆ z = ( 1)j(zˆ z )
k lin − lin k (cid:13) − j − j (cid:13)
(cid:13)j=1 (cid:13)
(cid:13)X (cid:13)
(cid:13)2m (cid:13)
(cid:13) (cid:13)
(cid:13) zˆ z (cid:13)
j j
≤ k − k
j=1
X
2m
λ
≤
j=1
X
2mλ,
≤
so
zˆ z 2mλ. (17)
lin lin
k k≥k k−
LetN Rd ntobethematrixwhosei-thcolumnisyˆn ,equivalentlyN =Ndiag(yˆ). Then
′ × i i ′
∈
zˆ
j
=η( 1)jN ′c j,
−
wherec Rnisgivenby
j
∈
T 1
(c ) = − b(t).
j i ij
t=0
X
DuetosymmetryofthenoisedistributionthenthecolumnsofN arei.i.d.withdistribution (0 ,d 1(I vvT)).
′ d − d
N −
ThereforebyLemma4.1(andtheassumptionsd=Ω n+log1 ),withprobabilityatleast1 δovertherandomness
δ −
ofthetrainingdatathereexistpositiveconstantsC ,C suchthatC σ (N ) σ (N ) C. Asaresult
′ ′ min ′ max ′
(cid:0) (cid:1) ≤ ≤ ≤
C η c zˆ Cη c . (18)
′ j j j
k k≤k k≤ k k
Weclaimthatforanyj,j [2m]andi [n],(c ) α(c ) . Indeed,ifyˆf(W(t),x ) 1,thenb(t) = b(t) = 0,
′ ∈ ∈ j i ≥ j′ i i i ≥ ij ij′
andifyˆf(W(t),x )<1,thenbothb(t)andb(t)areelementsof α,1 . Thisinparticularimpliesthat
i i ij ij′ { }
c ,c α c ,c .
j j′ j j
h i≥ h i
Letusdefine
2m
c = c .
lin j
j=1
X
30APREPRINT
Then
2
2m
zˆ 2 = ( 1)jzˆ
k lin k (cid:13) − j(cid:13)
(cid:13)j=1 (cid:13)
(cid:13)X (cid:13)
(cid:13) (cid:13)2
(cid:13)2m (cid:13)
(cid:13) (cid:13)
= ηN c
(cid:13) ′ j(cid:13)
(cid:13)j=1 (cid:13)
(cid:13)X (cid:13)
(cid:13) (cid:13)2
(cid:13) 2m (cid:13)
(cid:13) (cid:13)
Cη2 c
≤ (cid:13) j(cid:13)
(cid:13)j=1 (cid:13)
(cid:13)X (cid:13)
=Cη2(cid:13)
(cid:13)c lin
2,(cid:13)
(cid:13) (19)
k(cid:13) k (cid:13)
whereweusedthat N C inthethirdline. Wealsohave
′
k k≤
2m 2m
c 2 = c ,c
lin j j′
k k h i
j=1j′=1
XX
2m 2m
α c ,c
j j
≥ h i
j=1j′=1
XX
2m
=2αm c 2. (20)
j
k k
j=1
X
Finallywecombineourboundsforc,z,andzˆ:
2m
Z 2 2λ√2m Z 2mλ2 zˆ 2
k kF − k kF − ≤ k j k
j=1
X
2m
Cη2 c 2
j
≤ k k
j=1
X
Cη2
c 2
lin
≤ αmk k
C
zˆ 2
lin
≤ αmk k
C
( z +2mλ)2.
lin
≤ αm k k
Herewe appliedequations(16) inthefirstline, (18)inthe secondline, (20)inthethirdline, (19)inthefourthline,
and(17)inthefifthline. Thisestablishesboththeboundsclaimed.
Asinthecaseofbenignoverfitting,wewillrelyonageneralizationboundwhichdependsonthesignal-to-noiseratio
ofthenetwork.
LemmaD.4. LetW R2m d bethefirstlayerweightmatrixofashallowleakyReLUnetworkgivenbyequation1.
×
∈
Suppose(x,y)isarandomtestpointsampledunderthedatamodelgiveninDefinition2.1. IfW issuchthatA 0
lin
≥
and
A 1 γ
lin
=O −
z γd
lin (cid:18)r (cid:19)
then
1
P(yf(W,x)<0) .
≥ 8
Alternatively,ifA 0then
lin
≤
1
P(yf(W,x)<0) .
≥ 4
31APREPRINT
Proof. ByDefinition2.1( x, y)isidenticallydistributedto(x,y),therefore
− −
1
P(0>yf(W,x))= (P(0>yf(W,x))+P(0> yf(W, x)))
2 − −
1
P(0>yf(W,x) 0> yf(W, x))
≥ 2 ∪ − −
1
P(0>yf(W,x) yf(W, x)).
≥ 2 − −
Nextwecompute
yf(W,x) yf(W, x)
− −
2m
= ( 1)jy(σ( w ,x ) σ( w , x ))
j j
− h i − h − i
j=1
X
2m
=(1+α) ( 1)jy w ,x
j
− h i
j=1
X
2m
=(1+α) ( 1)j a v+z ,√γv+ 1 γn
j j
− h − i
j=1
X p
2m 2m
=(1+α)√γ ( 1)ja +(1+α) 1 γ n, ( 1)jz
j j
− − * − +
j=1 j=1
X p X
=(1+α)√γA +(1+α) 1 γ n,z .
lin lin
− h i
Theabovetwocalculationsimplythat
p
1
P(0>yf(W,x)) P(0>yf(W,x) yf(W, x))
≥ 2 − −
1
= P(0>(1+α)√γA +(1+α) 1 γ n,z )
lin lin
2 − h i
1 γ p
= P n,z > A .
lin lin
2 h− i 1 γ
(cid:18) r − (cid:19)
SupposethatA 0. Asthenoisedistributionissymmetric n,z =d n,z . Therefore,
lin lin lin
≥ h i h− i
1 γ 1 γ A
P n,z > A = P n,u > lin ,
4 |h lin i| 1 γ lin 4 |h i| 1 γ z
(cid:18) r − (cid:19) (cid:18) r − k lin k(cid:19)
z
whereu= zlin istheunitvectorpointinginthedirectionofz lin. Notebyconstructionu span( v ) ⊥. If
k link ∈ { }
A 1 γ
lin
=O − ,
z γd
k lin k (cid:18)r (cid:19)
thenbyLemmaA.2,
γ A 1
P n,u > lin
|h i| 1 γ z ≥ 2
(cid:18) r − k lin k(cid:19)
andtherefore
1 γ A 1
P(0>yf(W,x)) P n,u > lin .
≥ 4 |h i| 1 γ z ≥ 8
(cid:18) r − k lin k(cid:19)
IfA <0,thenagainbythesymmetryofthenoise
lin
1 γ
P(0>yf(W,x)) P n,z > A
lin lin
≥ 2 h− i 1 γ
(cid:18) r − (cid:19)
1
P( n,z >0)
lin
≥ 2 h− i
1
= .
4
Thisestablishestheresult.
32APREPRINT
Theorem3.3. Underthe setting given in Definition3.1, let δ (0,1)and suppose = , where η,λ R
GD >0
∈ A A ∈
satisfyAssumption1. Ifn=Ω(1),d=Ω n+log1 andγ =O α3 thenthefollowinghold.
δ d
(cid:0) (cid:1) (cid:16) (cid:17)
1. The algorithm terminates almost surely after finitely many updates. With W = (X,yˆ),
GD GD
A A
L(W,X,yˆ)=0.
2. Withprobabilityatleast1 δoverthetrainingdata(X,yˆ)
−
1
P(yf(W,x)<0 X,yˆ) .
| ≥ 8
Proof. IfA <0,thenbyLemmaD.4,
lin
1
P(yf(W,x)<0) .
≥ 4
SoitsufficestoconsiderthecaseA 0. Sinced = Ω n+log1 ,byLemmaB.4,themax-marginclassifierw
lin ≥ δ ∗
satisfies
(cid:0) (cid:1)
n
w C
∗
k k≤ 1 γ
r −
withprobabilityatleast1 δ overtherandomnessoftheinputdataset. Wedenotethiseventbyω andconditionon
− 3 1
itfortherestofthisproof.ByTheorem3.1,
C w
W k ∗ k
k k≤ α√m
C n
.
≤ α m(1 γ)
r −
ByTheorem3.1,thenetworkperfectlyfitsthetrainingdata,soforalli [n],yˆf(W,x ) 1,andtherefore
i i
∈ ≥
1 f(W,x )
i
≤| |
2m
= ( 1)jσ( w ,x )
(cid:12) − h j i i (cid:12)
(cid:12)j=1 (cid:12)
(cid:12)X (cid:12)
(cid:12)2m (cid:12)
(cid:12) (cid:12)
(cid:12) w ,x (cid:12)
j i
≤ |h i|
j=1
X
2m
= a v+z ,√γy v+ 1 γn
j j i i
|h − i|
j=1
X p
2m
= a y √γ+ 1 γ z ,n
j i j i
| − h i|
j=1
X p
2m 2m
√γ a + 1 γ z ,n .
j j i
≤ | | − |h i|
j=1 j=1
X p X
Thisimpliesthateither 21
≤
√γ 2 j=m 1|a
j
|or 21
≤
√1 −γ 2 j=m 1|hz j,n
i
i|
foralli
∈
[n]. We considerbothcases
separately.
P P
33APREPRINT
Supposethat 1
2
≤√γ 2 j=m 1|a
j
|. ThensquaringbothsidesandapplyingCauchy-Schwarz,weobtain
2
P 2m
1
γ a
j
4 ≤  | |
j=1
X
 2m 
2mγ a 2
j
≤ | |
j=1
X
2m
2mγ w 2
j
≤ k k
j=1
X
=2mγ W 2
k kF
Cγn
.
≤ α2(1 γ)
−
Thiscannotoccurifγ =O α2 ,andinparticularitcannotoccurifd=Ω(n)andγ =O α3 .
n d
(cid:16) (cid:17) (cid:16) (cid:17)
Nowsupposethat 1 √1 γ 2m z ,n foralli [n]. SquaringbothsidesandapplyingCauchy-Schwarz,
2 ≤ − j=1|h j i i| ∈
weobtain
P
2
2m
1
(1 γ) z ,n
j i
4 ≤ −  kh i|
j=1
X
 2m 
2m(1 γ) z ,n 2.
j i
≤ − kh ik
j=1
X
Summingoveralli [n],weobtain
∈
n 2m
n
2m(1 γ) z ,n 2
j i
4 ≤ − kh ik
i=1j=1
XX
2m
=2m(1 γ) Nz 2
j
− k k
j=1
X
2m
2m(1 γ) N 2 z 2
j
≤ − k k k k
j=1
X
=2m(1 γ) N 2 Z 2.
− k k k kF
Recall that d = Ω n+log1 , and that the rows of N are i.i.d. with distribution (0 ,d 1(I vvT)). So by
δ N d − d −
Lemma4.1,withprobabilityatleast1 δ overtherandomnessofthedataset, N C. Wedenotethiseventbyω
(cid:0) (cid:1) −3 k k≤ 2
andconditiononitfortherestofthisproof.So
Cn
Z 2 . (21)
k kF ≥ m(1 γ)
−
Letλ= √α. ByAssumption1, w(0) λforallj [2m]. SobyLemmaD.3,
m k j k≤ ∈
C
Z 2 2λ√2m Z 2mλ2 ( z +2mλ)2. (22)
k kF − k kF − ≤ αm k lin k
By(21),
C√n
Z
F
k k ≥ m(1 γ)
−
C√n
p
≥ √m
Cλ√nm
≥
8λ√2m,
≥
34APREPRINT
wherethelastlineholdsifn=Ω(1). Thenby(22),
1 1 1
Z 2 = Z 2 Z 2 Z 2
2k kF k kF − 4k kF − 4k kF
Z 2 2λ√2m Z 2mλ2
≤k kF − k kF −
C
( z +2mλ)2
lin
≤ αm k k
C
= ( z +2√α)2.
lin
αm k k
Takingthesquarerootofbothsidesandrecallingthatα (0,1)isaconstant,weobtain
∈
C z C
Z k lin k + .
F
k k ≤ √αm √m
Thisimpliesthateither kZ kF ≤ √2C m or kZ kF ≤ 2C √k αz mlink. Thecase kZ kF ≤ √2C m cannothappen,sinceby(21),
C √n
Z ′
F
k k ≥ m(1 γ)
−
C √n
p′
≥ √m
2C
≥ √m
whenn=Ω(1). Sowehave kZ kF
≤
2C √k αz mlink. Againapplying(21),weobtain
z C√αm Z
lin F
k k≥ k k
C√αn
.
≥ √1 γ
−
So
A A √1 γ
lin lin
C −
z ≤ √αn
lin
k k
2m
C√1 γ
= − ( 1)ja
j
√αn −
j=1
X
1/2
2m
C√1 γ
− √2m a 2
j
≤ √αn  | | 
j=1
X
  1/2
2m
C m(1 γ)
− w 2
j
≤ √αn  k k 
p j=1
X
 
C W m(1 γ)
F
= k k −
√αn
p
C
.
≤ α3/2
HereweusedthatA 0andappliedCauchy-Schwarzinthethirdline. ThenbyLemmaD.4,if
lin
≥
C 1 γ
O − ,
α3/2 ≤ γd
(cid:18)r (cid:19)
then
1
P(yf(W,x)<0) .
≥ 8
Thisoccursifγ = O α3 . Hence,inallcases,wehaveshownthatwiththeappropriatescaling,thegeneralization
d
errorisatleast 1 when(cid:16)bot(cid:17)hω andω occur.Thishappenswithprobabilityatleast1 δ.
8 1 2 −
35APREPRINT
Appendix E Formalizingbenign overfitting asa highdimensional phenomenon
Toformalizebenignoverfittingasahighdimensionalphenomenonwefirstintroducethenotionofaregime.Informally,
aregimeisasubsetofthehyperparametersΩ N4whichdescribesacceptedcombinationsoftheinputdatadimension
∈
d, thenumberofpointsin thetrainingsamplen, thenumberofcorruptpointsk andthe numberoftrainablemodel
parametersp.
DefinitionE.1. AregimeisasubsetΩ N4whichsatisfiesthefollowingproperties.
⊂
1. Foranytuple(d,n,k,p) Ωthenumberofcorruptpointsisatmostthetotalnumberofpoints,k n.
∈ ≤
2. Thereisnoupperboundonthenumberofpoints,
sup n= .
∞
(d,n,k,p) Ω
∈
Anon-trivialregimeisaregimewhichsatisfiesthefollowingadditionalcondition.
3. DefinethesetofincreasingsequencesofΩasΩ ∗ = (n l,d l,k l,p l) l N Ω s.t. lim l n l = . For
any(n l,d l,k l,p l) l N Ω ∗itholdsthat { ∈ ⊂ →∞ ∞}
∈ ∈ k
l
liminf >0.
l n l
→∞
Intuitively,aregimedefineshowthefourhyperparameters(d,n,k,p)cangrowinrelationtooneanotherasngoesto
infinity. Anon-trivialregimeisoneinwhichthefractionofcorruptpointsinthetrainingsampleisnon-vanishing.In
ordertomakeaformaldefinitionofbenignoverfittingashighdimensionalphenomenonweintroducethefollowing
additionalconcepts.
• A Rnlea drnin Rg nalgo Rri pth .m A = ( Ad,n,p) (d,n,p) ∈N3 isatripleindexedsequenceofmeasurablefunctionsA d,n,p :
×
× →
• A Rn
.
architecture M=(f d,p) d,p ∈N2 isadoubleindexedsequenceofmeasurablefunctionsf d,p :Rd ×Rp →
• A defid na eta dm ovo ed re Rl nD d= (D d 1,n,k) ( Rd, dn,k) ∈N3 1is .a triple indexed sequence of Borel probability measures D d,n,k
×
×{± }× ×{± }
Withthesenotionsinplacewearereadytoprovideadefinitionofbenignoverfittinginhighdimensions.
DefinitionE.2. Let(ǫ,δ) (0,1]2, bealearningalgorithm, anarchitecture, adatamodelandΩaregime.
Ifforeveryincreasingsequ∈
ence(d
l,nA
l,k l,p l) l N Ω ∗
thereexisM
tsanL
NsuchthD
atforalll Lwithprobability
atleast1 δover(X,yˆ),where(X,yˆ,x,y)∈ D∈ ,itholdsthat ∈ ≥
− ∼
dl,nl,kl
1. y f( (X,yˆ),x )>0 i [n ],
i Adl,nl,pl i
∀ ∈
l
2. P(yf( (X,yˆ),x) 0) inf P(yf(W,x) 0)+ǫ,
Adl,nl,pl
≤ ≤
W ∈Rnl×dl
≤
then the quadruplet( , , ,Ω) (ǫ,δ)-benignly overfits. If ( , , ,Ω) (ǫ,δ)-benignly overfits for any (ǫ,δ)
(0,1]2thenwesay( A , M , D ,Ω)benignlyoverfits. A M D ∈
A M D
Analogously,wedefinenon-benignoverfittingasfollows.
DefinitionE.3. Let(ǫ,δ) (0,1]2, bealearningalgorithm, anarchitecture, adatamodelandΩaregime.
Ifforeveryincreasingsequ∈
ence(d
l,nA
l,k l,p l) l N Ω ∗
thereexisM
tsanL
NsuchthD
atforalll Lwithprobability
atleast1 δover(X,yˆ),where(X,yˆ,x,y)∈ D∈ ,itholdsthat ∈ ≥
− ∼
dl,nl,kl
1. y f( (X,yˆ),x )>0 i [n ],
i Adl,nl,pl i
∀ ∈
l
2. P(yf( (X,yˆ),x) 0) inf P(yf(W,x) 0)+ǫ,
Adl,nl,pl
≤ ≥
W ∈Rnl×dl
≤
then the quadruplet( , , ,Ω) (ǫ,δ)-non-benignlyoverfits. If ( , , ,Ω) (ǫ,δ)-non-benignlyoverfitsfor any
(ǫ,δ)
(0,1]2thenwA esaM y(D
, , ,Ω)non-benignlyoverfits.
A M D
∈ A M D
Oneofthekeycontributionsofthispaperisproving(ǫ,δ)-benignandnon-benignoverfittingwhenthearchitectureis
atwo-layerleakyReLUnetwork(equation1),thelearningalgorithmreturnstheinnerlayerweightsofthenetworkby
minimizingthehingelossoverthe trainingdatausinggradientdescent(Definition2.2), andtheregimesatisfies the
conditionsd=Ω(nlog1/ǫ),n=Ω(1/δ),k =O(n)andp=2dmforsomenetworkwidth2m,m N.
∈
36