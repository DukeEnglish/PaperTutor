Counterfactual Reasoning with Knowledge Graph Embeddings
LenaZellinger1,AndreasStephan1,2,4, and BenjaminRoth1,3
1 FacultyofComputerScience,UniversityofVienna,Austria
2 UniVieDoctoralSchoolComputerScience,UniversityofVienna,Austria
3 FacultyofPhilologicalandCulturalStudies,UniversityofVienna,Austria
4andreas.stephan@univie.ac.at
Abstract
Knowledge graph embeddings (KGEs) were
originallydevelopedtoinfertruebutmissing
factsinincompleteknowledgerepositories. In
thispaper, welinkknowledgegraphcomple-
tionandcounterfactualreasoningviaournew
task CFKGR. We model the original world
stateasaknowledgegraph,hypotheticalscenar-
iosasedgesaddedtothegraph,andplausible Figure1: Ahypotheticalscenarioanditsimplications,
changestothegraphasinferencesfromlogi- expressedinthelanguageofknowledgegraphtriples
calrules. Wecreatecorrespondingbenchmark
datasets, which contain diverse hypothetical
totheexistenceoffreewillandasenseofagency
scenarios with plausible changes to the orig-
inal knowledge graph and facts that should (McCarthy,2000;Kulakovaetal.,2017). Recently,
beretained. WedevelopCOULDD,ageneral therehasbeenanincreasedinterestinevaluating
methodforadaptingexistingknowledgegraph andimprovingcounterfactualreasoningofAIsys-
embeddingsgivenahypotheticalpremise,and tems,inparticular,largelanguagemodels(LLMs)
evaluateitonourbenchmark. Ourresultsin-
(Qin et al., 2019; Frohberg and Binder, 2022; Li
dicate that KGEs learn patterns in the graph
etal.,2023).
without explicit training. We further observe
Knowledgegraphs(KGs)expressrichinforma-
thatKGEsadaptedwithCOULDDsolidlyde-
tect plausible counterfactual changes to the tion about the world as an explicit collection of
graphthatfollowthesepatterns. Anevaluation triples,suchas(Paris,capital,France),andknowl-
on human-annotated data reveals that KGEs edge graph embeddings (KGEs) effectively infer
adaptedwithCOULDDaremostlyunableto truebutmissingfactsfromincompleteknowledge
recognizechangestothegraphthatdonotfol-
repositories (Hogan et al., 2021; Ji et al., 2021).
lowlearnedinferencerules. Incontrast,Chat-
Yet,tothebestofourknowledge,KGEshavenot
GPT mostly outperforms KGEs in detecting
beenexploredforcounterfactualreasoning.
plausible changes to the graph but has poor
knowledge retention. In summary, CFKGR Inthiswork,welinkcounterfactualreasoningto
connectstwopreviouslydistinctareas,namely knowledgegraphcompletion(KGC)viaournew
KGcompletionandcounterfactualreasoning. task CFKGR1 (CounterFactual KG Reasoning)
which requires models to classify the validity of
1 Introduction
facts given a hypothetical scenario. CFKGR de-
Reasoningabouthypotheticalsituations(counter- scribes the original world state as a KG and hy-
factualreasoning)andanticipatingtheeffectsofa potheticalscenariosasedgesthatareaddedtothe
graph. Thehypotheticalscenarioleadstotheemer-
changeinthecurrentstateoftheworldiscentral
genceofnewfactsintheKGwhileleaving(most)
tohumancognition(RafetsederandPerner,2014;
alreadyexistingonesintact. Figure1illustratesa
Van Hoeck et al., 2015), and has been identified
hypotheticalscenarioinwhichParisisthecapital
asakeyconceptingametheory(Aumann,1995;
of Japan. To perform well on CFKGR, models
Halpern, 1999) and agent-based systems (Icard
must be capable of detecting plausible additions
etal.,2018;Parvanehetal.,2020). Ithasevenbeen
arguedthatthecapacitytoreasonaboutalternative
1Thedataandcodeareavailableathttps://github.
configurationsoftheworldcouldbeapre-requisite com/LenaZellinger/counterfactual_KGR.
4202
raM
11
]GL.sc[
1v63960.3042:viXrato the graph, e.g., (Paris, continent, Asia), while 2 CFKGR:TaskDescription
maintaining knowledge of unaffected facts, e.g.,
We introduce Counterfactual KG Reasoning
(Elvis Presley, occupation, musician). We create
(CFKGR)anoveltasktoassesstheabilityofma-
the first benchmark datasets for CFKGR, which
chine learning systems to reason in hypothetical
arebasedontheCoDExKGCbenchmark(Safavi
scenarios. CFKGR describes the originally ob-
and Koutra, 2020) and provide diverse hypothet-
served world state as a knowledge graph and in-
ical scenarios with corresponding plausible addi-
troduces hypothetical scenarios by adding previ-
tionstotheKGderivedfrominferencerules(that
ously unseen facts to the graph. To perform well
wereminedfromtheKG(Lajusetal.,2020)). We
onCFKGR,modelsneedto(1)identifyplausible
validateourdata-generatingprocessandunderly-
changestotheoriginalworldstateinducedbythe
ing assumptions via thorough human annotation.
hypothetical scenario and (2) understand which
Lastly,weintroduceCOULDD(COUnterfactual
factsareunaffectedbythehypotheticalscenario.
ReasoningwithKnowLedgeGraphEmbeDDings),
amethodwhichupdatesexistingKGEsbasedon
2.1 DefinitionofCounterfactualGraphs
counterfactual information. COULDD follows a
Formally,CFKGRdefinestheoriginalworldstate
standardKGEtrainingschemeusingthehypotheti-
via a knowledge graph G = {E,R,F}, where E
calscenarioandnegativesampling. Trainingstops
andRdenotethesetsofentitiesandrelationsrep-
oncethehypotheticalscenarioisclassifiedasvalid.
resented in the knowledge graph. The fact set F
Inourexperiments,COULDDisinitializedwith
representsourknowledgeabouttheworldastriples
five different KGE methods. We observe that it
(h,r,t) ∈ F ⊂ E ×R×E. The fact set is usu-
candetectplausiblecounterfactualchangestothe
ally split into disjoint subsets F , F and
graphthatfollowprominentinferencepatternsin train valid
F . Wedenoteahypotheticalscenariobyatriple
the KG while maintaining performance on unaf- test
τc := (h,r,t) ∈/ F. Thecounterfactualgraph,in
fectedtriples. Werepeatthesameexperimentswith
which τc holds, is then characterized by the fact
ChatGPT,i.e.,gpt-3.5-turbo,providedwithsimilar
set Fc := F \F− ∪F+, where F+ denotes the
prompts to the human annotators. ChatGPT per-
factsthatemergegiventhehypotheticalscenario,
formsbetteratdetectingplausibleadditionstothe
andF− denotesfactsthatcontradictthescenario
graphthanmostKGE-basedmethodsbutexhibits
andcannotholdanylonger. Wesayτc changesa
poorknowledgeretention. Qualitativeanalysisof
tripleτ ifeitherτ ∈ F+ orτ ∈ F−.
answersprovidedbyChatGPTshowsthatitlargely
Inthefollowing,weformulatetheassumptions
failed to understand the task on retained facts as
underlyingourtask.
it tried to infer them from the provided informa-
Closed-worldassumption. Weadoptthestandard
tion. Evaluatingonhuman-annotateddataleadsto
closed-world assumption (Reiter, 1978), which
adropinoverallperformanceforKGEsandChat-
states that facts that are not part of the KG, i.e.,
GPTalike. Tosummarize,ourmaincontributions
τ ∈/ F, arefalse. Thus, eachτ ∈/ F isapossible
areasfollows:
hypotheticalscenarioinoursetup.
• We propose CFKGR, a challenging task for Logic-world assumption. We assume that plau-
counterfactual reasoning on KGs and cre- sible changes to the graph largely follow some
ate corresponding, partially human-verified, regularity and can hence be modeled via (poten-
datasets,whichwemakepubliclyavailable. tiallyverycomplex)logicalrules. Whileavailable
rulesetshavelimitedcoverageandprecision,we
• WeintroduceCOULDD,ageneralmethodfor
can leverage them to model a subset of plausible
adaptingexistingKGEmethodstomakeinfer-
changes to a KG. By employing the logic-world
encesgivenhypotheticalscenariosandshow
assumption,wecanrepresentanapproximationof
thatitimprovesreasoningoncounterfactual
Fc viaasetofrulesandtheoriginalfactset.
graphsoverpre-trainedembeddings.
2.2 Evaluation
• We compare counterfactual reasoning with
KGEs to ChatGPT and show that ChatGPT We formulate CFKGR as a binary classification
outperforms KGEs in detecting plausible taskinwhichthegoalistopredictwhetheragiven
counterfactualinferencesbutstrugglestore- tripleispresentinthecounterfactualgraphornot.
callunrelatedknowledge,unlikeCOULDD. Triples τ ∈ Fc receive label 1, while all otherAsia continent
Brazil
spouse continent
Europe
continent
Japan continent citizen of Denmark I Cn os uta nn tec re factual N τcotation O τcr ∈i /gi FnalKG C τcF ∈K FG c
Elvis Presley speaks lano gf ufi ac gia el I Un nfe cr he an nc ge
ed(near)
τ τi
n
τ τi n∈/ ∈F
F
τ τi n∈ ∈F Fc
c
speaks occupation Unchanged(far) τf τf ∈F τf ∈Fc
educated Corruptions τ h′,τ t′,τ r′ τ h′,τ t′,τ r′∈/F τ h′,τ t′,τ r′∈/Fc
Korean musician at Danish
speaks
Walt Disney
Figure2: Overviewoverthedifferenttypesoffacts,giventhehypotheticalscenariothatElvisPresleyisacitizenof
Denmark. Thegreenedge(ElvisPresley,speaks,Danish)emergesfromaddingtheblueedge(ElvisPresley,citizen
of,Denmark)totheknowledgegraph. PurpleandorangeedgesarepresentintheoriginalKGandunaffectedbythe
scenario. Greyedgesareneitherpresentintheoriginalnorthecounterfactualknowledgegraph.
triples are labeled 0. Since scoring all possible 3 CFKGR:DatasetCreation
triples is infeasible, we consider a smaller set of
carefullychosentestcases. Givenacounterfactual For our dataset construction, we leverage rules
τc ∈/ F andarule,wedefine: found by rule mining systems, which capture
(1)acounterfactualinferenceτithatfollowsfrom
prominentpatternsinKGs. Automaticallymined
the rule and allows us to measure whether the rulesarenaturallycompatiblewiththecontentof
model can correctly predict changes to the graph theKGandareknowntobeausefultoolforKGC
givenτc,
(e.g.,Meilickeetal.,2019;Sadeghianetal.,2019a).
(2)retainedfactswhichareunaffectedbythehy- Since there is no trivial way to reliably generate
potheticalscenarioandshouldstillbeclassifiedas F−, we only consider the additions F+. Con-
validinthecounterfactualgraph, cretely,wedefineF+ viaminedcompositionrules
(3)randomhead,tail,andrelationcorruptions oftheform
ofinferencesandretainedfacts,whichensurethat
the model does not predict unsolicited triples as
(X,r ,Y)∧(Y,r ,Z) → (X,r ,Z) (1)
valid additions. We denote the corruptions for a 1 2 3
tripleτ byτ ,τ andτ .
h′ t′ r′
For (2), we distinguish between near facts τn,
where r ,r ,r ∈ R. We refer to (X,r ,Y) ∧
1 2 3 1
whichareintheone-hopneighborhoodofτc,and
(Y,r ,Z) as the rule body and (X,r ,Z) as the
2 3
farfactsτf,sampledfromitscomplement. Note
inference. The triples (X,r ,Y) and (Y,r ,Z)
1 2
that they are sampled from the entire fact set F
arecalledthefirstandsecondbodyatom,respec-
to measure knowledge retention. Figure 2 illus-
tively. ReplacingX,Y,andZ byconcreteentities
tratesacounterfactualscenarioanditsassociated
x,y,z ∈ E createsaninstantiationoftherule. In
testcases.
thefollowing,wewillusetheshort-handnotation
(r ,r ,r )todenotearuleasdescribedin(1).
We use the following metrics to evaluate the 1 2 3
performanceonourbenchmark. Theconcretefor- Wechoosecompositionrulessincetheyarewell
mulascanbefoundinAppendixA.Wecompute studied in standard KG completion benchmarks
(1) the F1-score over all test cases in the dataset (Safavi and Koutra, 2020) and inferential bench-
tomeasuretheoverallpredictiveperformanceon marks (Cao et al., 2021; Liu et al., 2023). More-
counterfactualgraphs. over, composition rules, as given in (1), infer lo-
(2)theaccuracyonchangedfacts,i.e.,triplesthat calchanges. Thisisdesirablesincemostrelevant
haveadifferentlabelbeforeandafterthehypothet- changes induced by a hypothetical scenario will
icalscenarioisintroduced. likelyoccurinitscloseneighborhood. Weconsider
(3)theF1-scoreonunchangedfacts,i.e.,triples understanding the implications induced by com-
thathavethesamelabelbeforeandafterthehypo- position rules as a first step to more general and
theticalscenarioisintroduced. complexhypotheticalreasoning.Rule: (X, country, Y) (Y, part of, Z) (X, continent, Z) Valid Test
Rules Facts Rules Facts
country
CFKGR-CoDEx-S 5 3600 12 8848
country part of North
Moscow Russia Canda America CFKGR-CoDEx-M 5 3936 26 19584
CFKGR-CoDEx-L 5 4000 39 30064
continent
Table 1: CFKGR dataset overview. "Rules" denotes
Figure3: Creationofahypotheticalscenario.
thenumberofrulesthatwereusedtocreatethedataset.
"Facts"isthetotalnumberoftestcases.
3.1 DataGeneratingProcess
Inthefollowing,wegiveahigh-leveloverviewof basedontheCoDExknowledgegraphcompletion
our data generating process. We focus on creat- benchmark(SafaviandKoutra,2020). Wechoose
inghypotheticalscenariosforthefirstbodyatom CoDEx since it covers diverse content, uses eas-
of a given rule. Appendix C provides a detailed ily interpretable relations, and contains rich aux-
descriptionandthefullalgorithm. iliary information, such as entity types. CoDEx
Given a knowledge graph and a rule set, we providesthreeknowledgegraphsofvaryingsizes
generate several hypothetical scenarios for each (S,M,andL),collectedfromWikidata(Vrandecˇic´
rule by altering a fact in the KG such that it trig- and Krötzsch, 2014), and corresponding compo-
gers the rule, as is illustrated in Figure 3. Con- sition rules obtained by the rule-mining system
cretely, for each rule (r 1,r 2,r 3), we search for Amie3(Lajusetal.,2020). CoDEx-SandCoDEx-
existing edges e 1 := (x,r 1,y) ∈ F train and Madditionallycontainverifiednegativetriples. An
e 2 := (y¯,r 2,z) ∈ F train, ensuring that the re- overview over the resources provided by CoDEx
sultinghypotheticalscenarioτc := (x,r 1,y¯)and canbefoundinAppendixB.
inference τi := (x,r 3,z) are not in the original We use the available Amie3 patterns for each
KG. Sampling e 1 and e 2 without any constraints CoDEx dataset as our rule set and create at most
canresultinnonsensicalscenariosandinferences. 25uniquecounterfactualtriplesperbodyatomfor
Hence,weensurethattheentitiesinτc andτi are eachrule. Wesubsequentlysplitthemintoavali-
suitableforthegivenrelationbyrestrictingthemto dationandtestset,ensuringthattherearenoover-
entitiesthatoccurwithsaidrelationintheoriginal lappingrulesorcounterfactualsbetweenvalidation
KG. Once suitable τc and τi are found, we ran- and test 3. Table 1 provides statistics about the
domlysampletwonearfactsτn fromtheone-hop createddatasets.
neighborhood of τc and one far fact τf from its
In the following section, we will explore how
complement. Note that we sample τn and τf on welltheresultingtestcasesalignwithhumancoun-
the full fact set F, instead of only F test, as their terfactualreasoning.
primarypurposeistomeasureknowledgeretention
asopposedtoinferencecapabilities. 3.3 HumanAnnotation
When creating head and tail corruptions of a
Wevalidateourdatageneratingprocessviahuman
givenfact,werestrictthesamplespacesinceran-
annotation. Foreachofthe31rulesinCFKGR-M,
domcorruptions,whichtendtoresultinnonsensi-
we verify 10 test instances(5 per atom4). We an-
caltriples,havepreviouslybeenshowntobeeasily
notateτi,τf,τn,τn andτi ,andomittheremain-
detectable for KGE methods (Safavi and Koutra, 1 2 r′
ingcorruptionsastheirconstructionreliesonthe
2020). Forhead(tail)corruptions,werequirethat
commonly-usedclosed-worldassumption(Reiter,
thereplacementsarealsoheads(tails)fortherela-
1978). This results in 1530 annotated instances,
tionintheoriginalgraph2. Forrelationcorruptions,
which were labeled by four to six annotators as
wedonotemployadditionalconstraints.
either likely (1), unlikely (0), or unsure/too little
information(-1),givenverbalizationsofthehypo-
3.2 CFKGR-CoDEx
Based on the procedure described in Section 3.1, 3ForM,thereareruleswhichcanproducethesamecoun-
wecreatethefirstbenchmarkdatasetsforCFKGR terfactual-inferencepairs(usingadifferentcontext).There
are14suchduplicatesinthetestset.Still,thereisnooverlap
2Inrarecaseswheretheseconstraintsonlyallowforcreat- incounterfactualsbetweenvalidationandtest.
ingtriplesalreadypresentintheKGorinferredbyourrule 4Exceptforonerulewhichonlyproducedoneuniquecoun-
set,wedefaulttothefullentityset. terfactualaccordingtoourconditionsforthesecondatom.MajorityVoteLabel Algorithm1:COULDDtrainingandpre-
#Labeled Expected Asexpected 0 1 -1 Tied
diction. The short-hand notation ϕ (T )
Inference 306 1 58.2% 60 178 27 41 θ τc
Farfact 306 1 99.7% 0 305 0 1 denotes scoring all test cases associated
Nearfact 612 1 95.6% 16 585 2 9
Relationcorr. 306 0 86.9% 266 20 3 17 with τc and L θ denotes the cross-entropy
loss.
Table 2: Annotation results. "# Labeled" denotes the
Data: G = {E,R,F},
numberofannotatedexamplespercategory."Expected"
CFKGRdataD,
givesthelabelassignedbyourautomaticprocessand
"Asexpected"givesthepercentageofsamplesforwhich originalembeddingsθ 0,
theexpectedlabelcoincideswiththemajorityvote. #iterationsE,
#additionalsamplesN,
learningrateα,
theticalscenarioandcontexttriggeringtherespec-
thresholdsµ ,µ ,...,µ
1 2 |R|
tive inference rule. We observe a Krippendorff’s
Result: CFKGRpredictions
alpha (Hayes and Krippendorff, 2007) of 0.653,
yˆ←{}
computedusingthesimpledorfflibrary,whichin- foreach(τc,T ) ∈ D do
τc
dicates substantial agreement (Landis and Koch,
θ ← θ
0
1977). Theannotationguidelinescanbefoundin
fore ∈ {1,...,E}do
Appendix D. Table 2 summarizes the annotation
S ←SampleN fromF
train
results. B ← {τc}∪S
Inferences seem to be the most difficult cate- θ ← Optimizer(L (B),α)
θ
gorytoannotateastheyshowthehighestamount ifϕ (τc) ≥ µ then
θ r
of ties and "unsure/too little information" labels. break
yˆ← yˆ∪{ϕ (T )}
Moreover, we observe the highest number of de- θ τc
returnyˆ
viationsfromourexpectedlabelforthistestcase.
This indicates that rules that were mined for fac-
tual knowledge graph completion cannot always
nario. COULDD is initialized from existing em-
beusedforhuman-likecounterfactualreasoning.
beddingstrainedontheoriginalKG.Foreachhy-
Onrelationcorruptions,weobserveanoticeable
potheticalscenario,theseembeddingsareupdated
number of inferences that are not implied by our
andsubsequentlyevaluatedonthecorresponding
rules, but are still considered valid by humans or
testcases.
are at least debatable. Possible explanations are
COULDD’s update scheme only minimally
thelimitedcoverageoftherulesetorunintuitive
changesstandardKGEtraining: Ineachiteration,
verbalizations of the relations. For near and far
theexistingembeddingsarefine-tunedonabatch
facts, we obtain a label distribution that largely
consistingofthecounterfactualtripleτc andN ad-
agreeswithourassumptions.
ditionalrandomlysamplededgesfromthetraining
graph. Negative training examples are generated
4 CounterfactualReasoningwith
by randomly corrupting the head and tail entities
KnowledgeGraphEmbeddings
ofeachtripleinthebatch. Theembeddingsareup-
KGEmodelsfindlow-dimensionalvectorrepresen- datedusingthestandardcross-entropyloss. Once
tationsforentitiesandrelationswhilepreserving thecounterfactualtripleτc exceedstheclassifica-
theinformationcontainedintheKG.Tojudgethe tion threshold, the training is stopped in order to
plausibility of a given triple, KGE models use a avoidanexcessiveperturbationofthepre-trained
scoring function ϕ(h,r,t) : E ×R×E → R. A embeddings5.
triple is typically classified as valid if it satisfies Importantly,COULDDonlyrequiresaccessto
ϕ(h,r,t) ≥ µ , for a relation-specific threshold the counterfactual triple τc and the original fact
r
µ ∈ R. setF anddoesnotrequireadditionaltask-specific
r
trainingdataorinformationabouttherulesusedto
To extend KGEs to our task, we pro-
poseCOULDD(COUnterfactualReasoningWith generateCFKGRdatasets6. Asaresult,COULDD
KnowLedge Graph EmbeDDings), a general
5Note that there is no traditional validation set for the
methodforadaptingexistingknowledgegraphem-
individualupdatesonwhichwecouldperformearlystopping.
beddingswithrespecttoagivenhypotheticalsce- 6Weonlyusethetestcasesinthevalidationsetforhyper-CFKGR-CoDEx-S CFKGR-CoDEx-M CFKGR-CoDEx-L
F1 Changed Unchanged F1 Changed Unchanged F1 Changed Unchanged
RESCAL 60.82 27.12 63.28 63.05 21.57 66.92 53.84 71.47 49.64
COULDD-RESCAL 61.68±0.14 32.48±0.73 63.48±0.16 63.85±0.08 26.23±0.16 67.16±0.07 53.94±0.02 84.56±0.35 48.18±0.06
TransE 58.94 23.15 61.87 53.61 23.61 55.83 49.23 66.31 45.37
COULDD-TransE 60.49±0.12 26.8±0.81 63.16±0.09 53.91±0.05 26.06±0.25 55.79±0.06 52.6±0.06 76.56±0.25 47.77±0.04
ComplEx 62.45 29.11 64.90 65.69 11.60 71.83 58.44 65.51 55.26
COULDD-ComplEx 67.76±0.3 37.94±0.67 69.95±0.29 66.78±0.06 34.67±0.23 69.21±0.07 59.44±0.02 82.95±0.26 54.25±0.02
ConvE 61.04 16.64 65.39 56.83 13.15 61.37 55.56 61.84 52.58
COULDD-ConvE 61.51±0.11 16.96±0.72 65.92±0.12 52.69±0.16 17.04±0.16 56.09±0.16 60.6±0.17 45.53±0.61 60.29±0.14
TuckER 64.25 15.01 69.40 65.21 13.15 70.98 52.87 76.74 48.05
COULDD-TuckER 66.03±0.13 35.99±1.0 68.09±0.19 66.09±0.17 43.69±0.38 66.95±0.17 53.53±0.04 88.47±0.34 47.49±0.02
gpt-3.5-turbo 47.83 68.90 40.22 46.72 52.12 42.25 45.80 52.10 40.95
Table3:Testperformanceofpre-trainedembeddingsandCOULDDonCFKGR.ForCOULDD,wereportthemean
andstandarddeviationacross5runs. Boldentriesdenotethebestperformancebetweenpre-trainedKGEsandtheir
counterparttrainedwithCOULDD.Thebestresultsonthedatasetareunderlined. Forallscores,higherisbetter.
can also be applied in rule-free evaluation se- totheoriginalCoDExpaper8 (SafaviandKoutra,
tups. Algorithm1providesaformaldescriptionof 2020) to ensure comparability. Note that this en-
COULDD. tailsscoringalltriplesinthetaildirection. Since
nonegativesareprovidedforCoDEx-L,wegener-
5 Experiments ateonerandomtailcorruptionpervalidationtriple
forthresholdtuning(akintoexperimentsin(Safavi
In the following, we conduct two types of ex- and Koutra, 2020)). During training, we sample
periments: First, we evaluate pre-trained KGEs, 100negativeexamplespertriple(50headand50
COULDD,andChatGPTonourCFKGRdatasets tailcorruptions),asthiswaseffectiveinprevious
withexpectedlabelstoassesswhetherthemethods work (Trouillon et al., 2016; Kotnis and Nastase,
can apply inference rules found by a rule mining 2017).
systeminhypotheticalscenarios. Inoursecondset We implement our experiments by adapting
ofexperiments,weevaluateonhuman-labeleddata LibKGE (Broscheit et al., 2020) to support our
tocheckwhetherthemethodsalsocapturehuman proposedCOULDDtrainingstrategy. Weperform
reasoning, which does not necessarily align with hyperparamteroptimizationusingOptuna(Akiba
minedinferencerules(seeSection3.3). etal.,2019). ForexperimentswithChatGPT,i.e.,
gpt-3.5-turbo,weusetheOpenAIAPIandtemper-
5.1 GeneralSetup
ature0. Theusedpromptsandanexampleofinput
andoutputcanbefoundinAppendixF.3.
Weusethefivepre-trainedCoDExlink-prediction
models as initializations for COULDD7. Further
5.2 Results
detailsabouttheKGEmethodsareinAppendixE.
ForCOULDD,wetunethelearningrate(α)and Table3containstheresults. Adetailedevaluation
number of additional samples per batch (N) on pertesttypecanbefoundinAppendixG.First,we
therespectiveCFKGRvalidationset,basedonthe observe that the KGE performances on CFKGR-
bestoverallF1-score,andsetthemaximumnum- CoDEx-LdiffernoticeablyfromCFKGR-CoDEx-
ber of update steps (E) to 20. We carry over the S and CFKGR-CoDEx-M. This is likely due to
remaining hyperparameters from the pre-trained lowerthresholdqualityresultingfromtheabsence
CoDEx models (Safavi and Koutra, 2020). Fur- ofhardnegativetriplesforCoDEx-L.
ther details regarding the hyperparameters are in COULDD achieves the best results in terms
Appendix F.2. Optimization is performed using of overall F1-score on all datasets. In particular,
Adam(KingmaandBa,2014),orAdagrad(Duchi COULDD noticeably improves the performance
etal.,2011),dependingontheoriginalmodelcon- onchangedfactsoverthepre-trainedembeddings,
figuration. The general classification setup and exceptforConvE.Importantly,wedonotobserve
relation-specificdecisionthresholdsareequivalent a case where applying COULDD leads to a no-
parametertuning. 8We added a minor correction to the CoDEx threshold
7Theconfigfilesforthemodelsareavailableathttps: tuningthatensuresproperapplicationoftheglobalthreshold
//github.com/tsafavi/codex forunobservedrelations.CFKGR-CoDEx-M* CoDEx-M(filtered)
F1(E) F1(H) Changed(E) Changed(H) Unchanged(E) Unchanged(H) Overall Rule-wise
RESCAL 89.30 87.61 21.55 13.64 97.20 96.17 92.74 84.72
COULDD-RESCAL 89.03±0.24 87.12±0.24 25.08±0.75 16.25±0.58 96.48±0.20 95.31±0.21 − −
TransE 81.21 79.85 21.55 16.48 88.55 87.73 91.29 80.26
COULDD-TransE 80.64±0.07 79.44±0.10 23.43±0.27 19.2±0.43 87.65±0.11 86.94±0.12 − −
ComplEx 89.01 87.53 9.94 2.84 98.40 97.51 96.01 77.79
COULDD-ComplEx 92.05±0.11 90.43±0.16 37.35±1.08 29.89±1.37 98.29±0.1 97.27±0.1 − −
ConvE 83.96 82.56 14.92 9.09 92.46 91.62 89.29 79.70
COULDD-ConvE 78.39±0.56 77.15±0.72 16.69±1.13 12.39±0.91 86.17±0.62 85.43±0.71 − −
TuckER 89.31 88.08 13.81 7.95 98.26 97.50 96.37 90.33
COULDD-TuckER 92.83±0.12 90.92±0.12 43.43±0.90 34.55±0.91 98.41±0.11 97.21±0.12 − −
gpt-3.5-turbo 63.96 63.36 53.04 53.98 62.75 62.34 − −
Table4: CasestudyonCFKGR-CoDEx-M*withexpected(E)andhuman-assigned(H)labelsandperformanceon
thefilteredCoDEx-Mtestset. "Overall"describestheaccuracyacrossallinferences. "Rule-wise"givestheaverage
accuracyperrule. Boldentriesdenotethebestperformancebetweenpre-trainedKGEsandtheircounterparttrained
withCOULDD.Thebestresultsonthedatasetareunderlined. Forallscores,higherisbetter.
ticeable loss of knowledge acquired during pre- 5.3.1 InferenceRulesinFactualContexts
training. IntermsofoverallF1-score,COULDD-
Achieving good performance on changed triples
ComplExachievesthebestresultsaveragedacross
in Table 3 requires (1) a logical adaption to the
the three datasets. On changed facts, COULDD-
hypothetical scenario and (2) the application of
TuckERisthebest-performingKGEmethod,likely
compositionrulesthatgeneratedthetestinferences.
becauseTuckERiswell-suitedformodelingcom-
We attempt to disentangle these factors by inves-
positional relations (Safavi and Koutra, 2020).
tigatingwhethertheCoDExmodelscapturedthe
ChatGPTachievesthebestscoresonchangedfacts
regularities expressed in the Amie3 rules during
ontwooutofthreedatasets. However,itgenerally
pre-trainingandcanapplytheminfactualscenar-
doesnotperformwellonunchangedfacts. Possi-
ios.
blereasonsarethatitmissesrelevantbackground
Setup. WefiltertheoriginalCoDEx-Mtestsetfor
knowledge present in the KG or does not under-
triplesthatcanbeinferredbyapplyingthemined
standthetaskontheseinstances. Insummary,we
Amie3rulestothetrainingset. Weonlykeeptriples
observethatCOULDDconsistentlyimprovesper-
thatstemfromrulesthatcoveratleastfivetriples
formanceoverthepre-trainedembeddings,overall
in the test set to obtain sensible estimates for the
and on changed facts in particular, and does not
rule-wise performances. This results in a filtered
stronglydegradeperformanceonunchangedfacts.
testsetof551instancesinferredfrom10rules.
ThisindicatesthatCOULDD,toanextent,canbe
Results. We notice that the performance on the
used to infer plausible counterfactual changes to
filteredCoDEx-Mtestsetisconsistentlyhighfor
thegraphwhentheyfollowprominentpatternsin
allpre-trainedKGEmethods. Thisindicatesthat
theKG.
they learned the mined inference patterns during
training and lower performances observed in Ta-
ble3arelikelyduetoinsufficientadaptiontothe
hypothetical scenario. Appendix H.2 gives a full
5.3 CaseStudyonCoDEx-M
overviewofrule-wiseperformances.
5.3.2 EvaluationwithManualLabels
TobetterunderstandtheresultsshowninTable3,
weconduct acasestudy onCoDEx-M forwhich Setup. WecreateasubsetoftheCFKGR-CoDEx-
we have a human-annotated CFKGR subset. In M test set, denoted by CFKGR-CoDEx-M*,
particular, we want to assess how well the pre- whichsolelyconsistsofinstancesthatweremanu-
trainedCoDExmodelsperformfactualreasoning allylabeledasdescribedinSection3.3. Wefilter
withcompositionrulesandhowanevaluationon outtestcasesforwhichlessthanthreeannotators
human-assigned labels affects our results. The agreedorthemajorityvotelabelwas"unsure/too
main results are presented in Table 4. Table 12 little information". This results in a dataset com-
intheappendixpresentsaconfusionmatrixpertest posedof256hypotheticalscenariosandatotalof
typeforCOULDDandChatGPT. 1159testcases. ThecompletelabeldistributionperHypothetical: JoeFrazier(human)diedinMoscow(citywithmillionsofinhabitants,largestcity,capital)
Context: JoeFrazier(human)wascastinEncinoMan(film)
Statement: IsitthenstillplausiblethatE-40(human)hastheoccupationactor(profession,artist),
giventhatthisisthecaseintherealworld?
AnswerbyChatGPT: unlikely
ExplanationbyChatGPT: ThehypotheticalscenariostatesthatJoeFrazierdiedinMoscow,whichisacitywithmillionsofinhabitants.
Thisfactisnotthecaseintherealworld.ThecontextmentionsthatJoeFrazierwascastinthefilmEncinoMan.
Sincethestatementis’IsitthenSTILLplausiblethatE-40hastheoccupationactor?’,
weneedtoconsiderifthehypotheticalscenariomakesitplausibleforE-40tohavetheoccupationofanactor.
However,thecontextdoesnotprovideanyspecificinformationaboutE-40orhisoccupation.
Therefore,wecannotdeterminetheplausibilityofE-40beinganactorbasedonthegiveninformation.
Hence,thestatementisunlikely.
Table5: ExamplepromptandcorrespondinganswerproducedbyChatGPTonCFKGR-CoDEx-M*.
testcasecanbefoundinAppendixH.1. by AnyBurl (Meilicke et al., 2019), akin to our
Results. First,wenotethattheoverallF1-scoreand experiments in Section 5.3.1, and also find that
performanceonunchangedfactsgreatlydifferfrom pre-trainedKGEshavestronginferentialreasoning
thenumbersobservedfortheunfilteredsetinTable capabilities.
3. Thisisduetotheomissionofmostcorruptions Counterfactual Graph Learning. Leveraging
astheywerenotmanuallylabeled. Weobservea counterfactuals in graph learning is an emerging
consistentperformancedropforKGE-basedmeth- fieldofresearch(Guoetal.,2023). Counterfactuals
odswhenevaluatingonhuman-assignedlabelsin- have recently been utilized to ensure the fairness
stead of expected labels for all metrics. Judging of graph-based systems with respect to sensitive
fromtheconfusionmatrixinTable12,COULDD node attributes (Agarwal et al., 2021; Ma et al.,
cannotreliablyidentifyfalseinferencesthatfollow 2022; Zhang et al., 2021), improve interpretabil-
frompatternsintheKG,butareinvalidaccording ity by generating counterfactual explanations for
totheannotators. Moreover,itclassifiesmostout- predictions (Lucic, 2022; Numeroso and Bacciu,
dated facts, which are no longer valid given the 2021;Prado-Romeroetal.,2022;Xuetal.,2022),
hypothetical scenario, as positive. However, the and enhance link prediction performance on the
numberofoutdatedfacts(14)istoosmalltodraw graphas-is(Changetal.,2023;Luetal.,2023;Shi
anysubstantialconclusionsfromthisobservation. etal.,2022;Wangetal.,2021;Zhaoetal.,2022).
ForChatGPT,weobserveaslightlyreducedoverall Ourworkdoesnotfallintoanyoftheabovecat-
performancewhenevaluatingwithhuman-assigned egoriesandinsteadfocusesonmakingpredictions
labels. However, ChatGPT’s score improves on inacounterfactualgraph.
changedfactsforhuman-assignedlabels. Acloser CFReasoningBenchmarksforLLMs. Several
look at the confusion matrix reveals that Chat- datasets and evaluation schemes have been pro-
GPT performs better at detecting outdated facts posed for assessing the counterfactual reasoning
andfalseinferencesthanKGEs. However,asob- capabilitiesofLLMs. Qinetal.(2019)introduce
servedbefore,ChatGPTtendstomisclassifyfacts thetaskofcounterfactualstoryrewriting,inwhich
that should be retained. A qualitative inspection LLMshavetominimallyreviseagivenstorywith
revealsthatChatGPTlargelymisunderstandsthe respect to a counterfactual event. The CRASS
taskonsuchtriples: insteadofansweringwhether benchmarkchallengesLLMstoselectavalidcon-
theySTILLholdgiventhehypotheticalscenario, sequencegivenaquestionizedcounterfactualcon-
itoftentimestriestoinferthem. Table5givesan ditionalinamultiple-choicesetting(Frohbergand
example. Binder,2022). Lietal.(2023)presentLLMswith
a hypothetical premise and two possible comple-
6 RelatedWork
tionsforacorrespondingstatement,oneofwhich
Inferential KGC Benchmarks. Rule-based in- is valid in the real world while the other holds in
ferential benchmarks for KGC (Liu et al., 2023; thehypotheticalscenario.
Caoetal.,2021)assessamethod’sabilitytolearn Incontrast,CFKGRposesabinaryclassification
implicit rule patterns and use them to predict in- task, inwhichthemodelhastodecidewhethera
ferences in the test set based on evidence in the presentedstatementisplausibleinthegivenhypo-
trainingset. Caoetal.(2021)createaninferential thetical scenario or not. Further, our benchmark
test set for CoDEx-M based on a rule set mined isbasedontheknowledgecontainedinaKGandthusconsidersspecific,real-worldentities. 9 Limitations
Thetypeofrulesthatweexamineisarguablylim-
7 Discussion
ited. Weconsiderunderstandingtheimplications
induced by composition rules as a first step to
Comparison with Human CF Reasoning. Our
moregeneralandcomplexhypotheticalreasoning.
labeling efforts and experiments show that coun- Moreover, while the set of outdated facts F− is
terfactualreasoningonKGsisachallengingtask.
a key component for defining the counterfactual
BothKGEsandChatGPTleavemuchroomforim-
KG, there is no trivial way for generating them
provementonCFKGR.Moreover,asindicatedby
reliablywithoutappropriaterulesorextensivehu-
ourannotationresults(Table2),evenhumansfind
manverification. MostrulesdefinedforKGsare
it difficult to judge the plausibility of KG-based
Hornrules(e.g.,Lajusetal.,2020;Meilickeetal.,
counterfactualstatements,especiallywhentheyin-
2019;Sadeghianetal.,2019b)anddonotexpress
volveunfamiliarsituations. Forinstance,"IfMeg
negationintheheadatom. Hence,wefocusonthe
WhitewasamemberofGirlsAloud, wouldJack additionsF+ inthiswork.
White be part of Girls Aloud?" is a question that
Furthermore, this work does not consider the
mosthumanslikelydonotaskthemselves. Never-
confidencesoftheminedAmie3rulesbutassumes
theless,automaticsystemscanbepresentedwith
that they all could be a valid inference rules for
andevaluatedonawiderangeofpossiblescenar-
hypotheticalreasoning. Asindicatedbyourhuman
ios,evenifthoseareimplausibleorhardtoimagine
annotationresults,thisislikelynottrueinpractice.
forhumans.
Verbalizing KG triples, in a way that is intu-
AdvantageofKG-basedBenchmarks. KGsare
itive to humans, is a difficult task. We tried our
apowerfultoolfordefininghypotheticalscenarios
besttofindsuitableverbalizationsfortherelations
andtheirconsequences. Therichworldknowledge
in the CoDEx KG by consulting the correspond-
stored in KGs allows to create interesting case-
ingWikidatadefinitionsaswellasParaRel(Elazar
specificinferences. Intheexamplequestionabove,
et al., 2021). In our verbalizations, each entity is
wouldthejudgementchangeifwereplace"Girls
presentedwithuptothreeofitsassociatedentity
Aloud" by a band that is not a girl group? This
types9 inordertofacilitatereasoningwithlesser-
aspectislargelymissingfromcurrentcounterfac-
known entities. Nevertheless, unintuitve verbal-
tual reasoning benchmarks for LLMs (Frohberg
izations and missing context from the KG (with
andBinder,2022;Lietal.,2023),astheymostly
respecttohowrelationsareused)mighthaveinflu-
handlegenericentities.
encedourannotationresultsandChatGPTexperi-
ments.
8 Conclusion Moreover, KGs can contain erroneous or out-
datedfactsandautomaticallyconstructedCFKGR
ThisworkintroducesthenoveltaskCFKGR,which examplesmightrelyonthesefacts. Itispossible
requiresmodelstoreasononacounterfactualKG. that such instances impacted the performance of
By utilizing the world knowledge stored in KGs, ChatGPTonourbenchmark.
wecreatedatasetsconsistingofdiversehypotheti- Lastly, the poor performance of ChatGPT on
calscenariosandtheirimplications,asdefinedby unchanged facts could partially be caused by the
inference rules. Further, we propose COULDD, systempromptusedinourexperiments,whichcan
ageneralmethodforcounterfactualreasoningon befoundinAppendixF.3. Wedesignedtheprompt
KGs, and evaluate its effectiveness on automati- basedontheinstructionsprovidedtothehumanan-
cally generated and human-annotated data. We notators. Nevertheless,itislikelythattheprompt
extendourexperimentstoChatGPTandfindthatit could be adjusted to improve the results of Chat-
generallyoutperformsCOULDDatmakingcoun- GPT on unchanged facts. Appendix I further de-
terfactualinferences. However, ChatGPTlargely tailssomefrequenterrorswenoticedinChatGPT’s
doesnotrecognizewhichfactsareinvarianttothe responses.
hypothetical scenario. Both COULDD and Chat-
GPTleavemuchheadroomonthetask,highlight-
9Whenevermorethanthreeentitytypeswereavailable,we
ingthedifficultyofCFKGR. randomlysampledthreeofthemtoenhancereadability.10 EthicsStatement graphcompletion. InProceedingsofthe2019Con-
ferenceonEmpiricalMethodsinNaturalLanguage
We relied on well-established and publicly avail- Processing and the 9th International Joint Confer-
able resources to build our datasets and method. ence on Natural Language Processing (EMNLP-
IJCNLP),pages5185–5194,HongKong,China.As-
WeusetheCoDExknowledgegraphandLibKGE,
sociationforComputationalLinguistics.
which are both published under the MIT license.
The config files for the pre-trained CoDEx mod- Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
els used in our experiments are available on the
2013. Translatingembeddingsformodelingmulti-
CoDExgithubrepository10.
relationaldata. InAdvancesinNeuralInformation
The counterfactual situations included in our ProcessingSystems,volume26.CurranAssociates,
datasets are randomly generated and purely hy- Inc.
pothetical. They do not convey any implications SamuelBroscheit,DanielRuffinelli,AdrianKochsiek,
about the real-world entities referenced in them. PatrickBetz,andRainerGemulla.2020. LibKGE-A
Nevertheless,thecreatedinstancescouldbebiased knowledgegraphembeddinglibraryforreproducible
research. InProceedingsofthe2020Conferenceon
towardscertainentitiesduetobiasesintheoriginal
EmpiricalMethodsinNaturalLanguageProcessing:
KGsandouremployedsamplingstrategydetailed
SystemDemonstrations,pages165–174.
inAppendixC.
YixinCao,XiangJi,XinLv,JuanziLi,YonggangWen,
Werecruitedannotatorsonavoluntarybasis. We
andHanwangZhang.2021. Aremissinglinkspre-
donotpublishanyinformationthatcouldbeusedto dictable? an inferential benchmark for knowledge
identifythelabelersandourdatadoesnotcontain graphcompletion. InProceedingsofthe59thAnnual
anypersonalinformationregardingtheannotators. Meeting of the Association for Computational Lin-
guisticsandthe11thInternationalJointConference
onNaturalLanguageProcessing(Volume1: Long
Acknowledgements
Papers),pages6855–6865,Online.Associationfor
ComputationalLinguistics.
This research has been funded by the Deutsche
Forschungsgemeinschaft (DFG, German Heng Chang, Jie Cai, and Jia Li. 2023. Knowledge
graphcompletionwithcounterfactualaugmentation.
Research Foundation) RO 5127/2-1 and
In Proceedings of the ACM Web Conference 2023,
the Vienna Science and Technology Fund
pages2611–2620.
(WWTF)[10.47379/VRG19008] “Knowledge-
TimDettmers, PasqualeMinervini, PontusStenetorp,
infused Deep Learning for Natural Language
andSebastianRiedel.2018. Convolutional2dknowl-
Processing”. We thank the European High
edgegraphembeddings. InProceedingsoftheAAAI
Performance Computing initiative for providing conferenceonartificialintelligence,volume32.
thecomputationalresourcesthatenabledthiswork.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
EHPC-DEV-2022D10-051.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
LearningResearch,12(61):2121–2159.
References
Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-
ChiragAgarwal,HimabinduLakkaraju,andMarinka lashaRavichander, EduardHovy, HinrichSchütze,
Zitnik.2021. Towardsaunifiedframeworkforfair andYoavGoldberg.2021. Measuringandimproving
andstablegraphrepresentationlearning. InUncer- consistencyinpretrainedlanguagemodels. Transac-
tainty in Artificial Intelligence, pages 2114–2124. tionsoftheAssociationforComputationalLinguis-
PMLR. tics,9:1012–1031.
Jörg Frohberg and Frank Binder. 2022. CRASS: A
T Akiba, S Sano, T Yanase, T Ohta, and M Koyama.
noveldatasetandbenchmarktotestcounterfactual
2019. Anext-generationhyperparameteroptimiza-
reasoningoflargelanguagemodels. InProceedings
tionframework. InProceedingsofACMSIGKDD,
oftheThirteenthLanguageResourcesandEvalua-
pages2623–2631.
tionConference,pages2126–2140,Marseille,France.
EuropeanLanguageResourcesAssociation.
RobertJAumann.1995. Backwardinductionandcom-
monknowledgeofrationality. Gamesandeconomic ZhimengGuo, TengXiao, CharuAggarwal, HuiLiu,
Behavior,8(1):6–19. andSuhangWang.2023. Counterfactuallearningon
graphs: Asurvey. arXivpreprintarXiv:2304.01391.
IvanaBalazevic,CarlAllen,andTimothyHospedales.
2019. TuckER:Tensorfactorizationforknowledge JosephYHalpern.1999. Hypotheticalknowledgeand
counterfactual reasoning. International Journal of
10https://github.com/tsafavi/codex GameTheory,28:315–330.Andrew F Hayes and Klaus Krippendorff. 2007. An- AnaLucic.2022. Terhoeve,gabrieletolomei,maarten
sweringthecallforastandardreliabilitymeasurefor derijke,andfabriziosilvestri.cf-gnnexplainer:Coun-
codingdata. Communicationmethodsandmeasures, terfactualexplanationsforgraphneuralnetworks. In
1(1):77–89. Proceedings of The 25th International Conference
onArtificialIntelligenceandStatistics,pages4499–
AidanHogan,EvaBlomqvist,MichaelCochez,Clau- 4511.
dia d’Amato, Gerard de Melo, Claudio Gutierrez,
SabrinaKirrane,JoséEmilioLabraGayo,Roberto JingMa,RuochengGuo,MengtingWan,LongqiYang,
Navigli,SebastianNeumaier,etal.2021. Knowledge AidongZhang,andJundongLi.2022. Learningfair
graphs. ACMComputingSurveys(CSUR),54(4):1– noderepresentationswithgraphcounterfactualfair-
37. ness. InProceedingsoftheFifteenthACMInterna-
tionalConferenceonWebSearchandDataMining,
ThomasIcard,FieryCushman,andJoshuaKnobe.2018.
pages695–703.
Ontheinstrumentalvalueofhypotheticalandcoun-
terfactualthought. InProceedingsofthe40thAnnual
JohnMcCarthy.2000. Freewill-evenforrobots. Jour-
Meeting of the Cognitive Science Society, CogSci
nal of experimental & theoretical artificial intelli-
2018, Madison, WI,USA,July25-28, 2018.cogni-
gence,12(3):341–352.
tivesciencesociety.org.
ChristianMeilicke,MelisachewWudageChekol,Daniel
ShaoxiongJi,ShiruiPan,ErikCambria,PekkaMartti-
Ruffinelli,andHeinerStuckenschmidt.2019. Any-
nen,andSYuPhilip.2021. Asurveyonknowledge
time bottom-up rule learning for knowledge graph
graphs: Representation,acquisition,andapplications.
completion. InIJCAI,pages3137–3143.
IEEEtransactionsonneuralnetworksandlearning
systems,33(2):494–514.
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. International learningonmulti-relationaldata. InProceedingsof
the28thInternationalConferenceonInternational
ConferenceonLearningRepresentations.
Conference on Machine Learning, ICML’11, page
Bhushan Kotnis and Vivi Nastase. 2017. Analy- 809–816,Madison,WI,USA.Omnipress.
sis of the impact of negative sampling on link
prediction in knowledge graphs. arXiv preprint Danilo Numeroso and Davide Bacciu. 2021. Meg:
arXiv:1708.06816. Generatingmolecularcounterfactualexplanationsfor
deep graph networks. In 2021 International Joint
Eugenia Kulakova, Nima Khalighinejad, and Patrick ConferenceonNeuralNetworks(IJCNN),pages1–8.
Haggard.2017. Icouldhavedoneotherwise: Avail- IEEE.
ability of counterfactual comparisons informs the
sense of agency. Consciousness and cognition, Amin Parvaneh, Ehsan Abbasnejad, Damien Teney,
49:237–244. JavenQinfengShi,andAntonVandenHengel.2020.
Counterfactualvision-and-languagenavigation: Un-
JonathanLajus,LuisGalárraga,andFabianSuchanek. ravellingtheunseen. AdvancesinNeuralInforma-
2020. Fast and exact rule mining with amie 3. In tionProcessingSystems,33:5296–5307.
The Semantic Web, pages 36–52, Cham. Springer
InternationalPublishing. MarioAlfonsoPrado-Romero,BardhPrenkaj,Giovanni
Stilo,AlessandroCeli,ErnestoEstevanell-Valladares,
J Richard Landis and Gary G Koch. 1977. The mea-
andDanielAlejandroValdés-Pérez.2022. Ensemble
surementofobserveragreementforcategoricaldata.
approachesforgraphcounterfactualexplanations.
biometrics,pages159–174.
LianhuiQin,AntoineBosselut,AriHoltzman,Chandra
JiaxuanLi,LangYu,andAllysonEttinger.2023. Coun-
Bhagavatula,ElizabethClark,andYejinChoi.2019.
terfactualreasoning:Testinglanguagemodels’under-
Counterfactual story reasoning and generation. In
standingofhypotheticalscenarios. InProceedings
Proceedings of the 2019 Conference on Empirical
of the 61st Annual Meeting of the Association for
Methods in Natural Language Processing and the
ComputationalLinguistics(Volume2: ShortPapers),
9thInternationalJointConferenceonNaturalLan-
pages 804–815, Toronto, Canada. Association for
guageProcessing(EMNLP-IJCNLP),pages5043–
ComputationalLinguistics.
5053,HongKong,China.AssociationforComputa-
Shuwen Liu, Bernardo Cuenca Grau, Ian Horrocks, tionalLinguistics.
and Egor V Kostylev. 2023. Revisiting inferential
benchmarksforknowledgegraphcompletion. arXiv EvaRafetsederandJosefPerner.2014. Counterfactual
preprintarXiv:2306.04814. reasoning: Sharpeningconceptualdistinctionsinde-
velopmentalstudies. Childdevelopmentperspectives,
GuomingLu,HaoZhang,KeQin,andKaiDu.2023. A 8(1):54–58.
causal-basedsymbolicreasoningframeworkforun-
certainknowledgegraphs. ComputersandElectrical RaymondReiter.1978. OnClosedWorldDataBases,
Engineering,105:108541. pages55–76.SpringerUS,Boston,MA.AliSadeghian, MohammadrezaArmandpour, Patrick TongZhao,GangLiu,DahengWang,WenhaoYu,and
Ding,andDaisyZheWang.2019a. Drum: End-to- Meng Jiang. 2022. Learning from counterfactual
enddifferentiableruleminingonknowledgegraphs. linksforlinkprediction. InProceedingsofthe39th
InAdvancesinNeuralInformationProcessingSys- InternationalConferenceonMachineLearning,vol-
tems,volume32.CurranAssociates,Inc. ume 162 of Proceedings of Machine Learning Re-
search,pages26911–26926.PMLR.
AliSadeghian, MohammadrezaArmandpour, Patrick
Ding,andDaisyZheWang.2019b. Drum: End-to-
enddifferentiableruleminingonknowledgegraphs. A EvaluationMetrics
AdvancesinNeuralInformationProcessingSystems,
32. This section gives the concrete formulas
for the metrics used in Table 3 and Table
TaraSafaviandDanaiKoutra.2020. CoDEx: ACom-
4. We denote the full evaluation dataset by
prehensive Knowledge Graph Completion Bench-
mark. In Proceedings of the 2020 Conference on D := {(τ 1c,T τ 1c),(τ 2c,T τ 2c),...,(τ nc,T τ nc)}, where
(E Em Mp Nir Lic Pa )l ,M pae gt eh sod 8s 32in 8–N 8a 3t 5u 0r ,a OlL na lin ng eu .a Ag se soP cr io ac tie os nsi fn og
r
τ jc denote hypothetical scenarios and T τ jc are the
corresponding test cases. For any triple τ, we
ComputationalLinguistics.
assignthefollowingtwobinarylabels: y indicates
τ
Dan Shi, Anchen Li, and Bo Yang. 2022. whetherτ ispresentintheoriginalfactsetF and
Counterfactual-guided and curiosity-driven yc indicates whether τ belongs to the fact set of
τ
multi-hop reasoning over knowledge graph. In the counterfactual graph induced by τc, denoted
DatabaseSystemsforAdvancedApplications: 27th j
International Conference, DASFAA 2022, Virtual
byF τc. Thepredictionfory τc madebyamethodis
j
Event,April11–14,2022,Proceedings,PartI,page denotedbyy (cid:98)τc.
171–179,Berlin,Heidelberg.Springer-Verlag.
F1: For this metric, we consider all test
ThéoTrouillon,JohannesWelbl,SebastianRiedel,Eric
cases of all hypothetical scenarios without any
Gaussier,andGuillaumeBouchard.2016. Complex
embeddingsforsimplelinkprediction. InProceed- restrictions. It gives an indication of the overall
ings of The 33rd International Conference on Ma- predictiveperformanceoncounterfactualfactsets.
chine Learning, volume 48 of Proceedings of Ma- We choose the F1-score due to the imbalanced
chine Learning Research, pages 2071–2080, New
labeldistributionofourconstructedtestcases. The
York,NewYork,USA.PMLR.
metricisgivenby
Ledyard R Tucker. 1966. Some mathematical notes
2tp
on three-mode factor analysis. Psychometrika,
F1 = ,
31(3):279–311. 2tp+fn+fp
NicoleVanHoeck,PatrickDWatson,andAronKBar- where
bey.2015. Cognitiveneuroscienceofhumancounter-
factualreasoning. Frontiersinhumanneuroscience, n
(cid:88) (cid:88)
9:420. tp = I(yc = 1∧yc = 1),
τ (cid:98)τ
j=1τ∈T τc
DennyVrandecˇic´ andMarkusKrötzsch.2014. Wiki- j
data: Afreecollaborativeknowledgebase. Commun. n
(cid:88) (cid:88)
ACM,57(10):78–85. fn = I(y τc = 1∧y (cid:98)τc = 0),
j=1τ∈T τc
Zikang Wang, Linjing Li, Daniel Zeng, and Xiaofei j
n
Wu. 2021. Incorporating prior knowledge from (cid:88) (cid:88)
fp = I(yc = 0∧yc = 1)
counterfactuals into knowledge graph reasoning. τ (cid:98)τ
Knowledge-BasedSystems,223:107035. j=1τ∈T τc
j
Ran Xu, Yue Yu, Chao Zhang, Mohammed K Ali,
Changed: We denote the set of changed facts in
Joyce C Ho, and Carl Yang. 2022. Counterfac-
tual and factual reasoning over hypergraphs for in- T τ jc byT τ∗ jc. Formally,
terpretableclinicalpredictionsonehr. InMachine
LearningforHealth,pages259–278.PMLR. T τ∗ c := {τ ∈ T τc : (y τ = 0 ∧ y τc = 1) ∨
j j
XuZhang,LiangZhang,BoJin,andXinjiangLu.2021. (y = 1 ∧ yc = 0)}.
τ τ
A multi-view confidence-calibrated framework for
fair and stable graph representation learning. In
Intuitively, T∗ is comprised of facts that were
2021IEEEInternationalConferenceonDataMining τc
j
(ICDM),pages1493–1498.IEEE. notpresentintheoriginalgraphbutemergeinthecounterfactualKGorviceversa. Wecomputethe |E| |R| |F train| |F val| |F test| Negatives
accuracyonthesecaseswithrespecttoy τc.
S 2034 42 32888 1827 1828 Yes
M 17050 51 185584 10310 10311 Yes
t(cid:101)p+t(cid:101)n L 77951 69 551193 30622 30622 No
Changed = ,
t(cid:101)p+f(cid:102)n+f(cid:102)p+t(cid:101)n
Table 6: Overview of CoDEx datasets (Safavi and
Koutra, 2020). The column “Negatives” indicates
where
whether hard, verified validation and test triples are
(cid:88)n (cid:88) available.
t(cid:101)p = I(y τc = 1∧y (cid:98)τc = 1),
j=1 τ∈T τ∗ c
j B CoDExResources
n
(cid:88) (cid:88)
f(cid:102)n = I(y τc = 1∧y (cid:98)τc = 0),
We use the CoDEx knowledge graph completion
j=1 τ∈T τ∗ c benchmark,whichiscomprisedofthreeknowledge
j
n graphs(S,M,L)collectedfromWikidatabasedon
(cid:88) (cid:88)
f(cid:102)p = I(y τc = 0∧y (cid:98)τc = 1), seedentitiesandrelationsfor13differentdomains
j=1 τ∈T τ∗ c (e.g., media and entertainment, politics, science)
j
(Safavi and Koutra, 2020). Table 6 provides an
n
t(cid:101)n = (cid:88) (cid:88) I(y τc = 0∧y (cid:98)τc = 0) overviewovertheresourcesprovidedbyCoDEx.
j=1 τ∈T τ∗ jc C DetailsofDatasetCreation
Notethatinthecaseofautomaticallygenerated
ThissectioncontainsdetailsoftheCFKGRdataset
labels (Table 3 and CFKGR-CoDEx-M* (E) in
creationthatwereomittedinSection3duetospace
Table 4), T∗ only consists of emerging facts
τc constraintsandgivesafullalgorithmicdescription
j
and hence the ground truth labels yc are always oftheprocedure.
τ
positive.
C.1 FormalDescription
Unchanged: Let T τc denote the set of un- Section3providesahigh-leveldescriptiononhow
j
changed factsinT τc. Formally, wecreateCFKGRtestinstancesbasedonthefirst
j
bodyatomofarule. Thissectioncoversthecase
T τc := {τ ∈ T τc : (y τ = 0∧y τc = 0) ∨ wherethesecondbodyatomisselectedforcreat-
j j
(y = 1∧yc = 1)}. ingthehypotheticalscenarioandcontainsformal
τ τ
descriptionsoftheemployedconstraints.
Intuitively, T τc is comprised of facts that do not Inthefollowing,wedefineanatomvariableto
j
change their label between F and F τc. We com- distinguishbetweenhypotheticalscenariosderived
j
pute the F1-score on such instances due to their from the first (atom = 1) versus the second atom
imbalanced label distribution in our constructed (atom=2). Thegeneralsetupisequivalentforboth
testcases. settings: Given a rule (r ,r ,r ), we search for
1 2 3
existingedgese := (x,r ,y) ∈ F ande :=
2tp 1 1 train 2
Unchanged = , (y¯,r ,z) ∈ F ,suchthatτi := (x,r ,z) ∈/ F.
2tp+fn+fp 2 train 3
We employ the following constraints I1, I2, and
where I3 when sampling e and e to ensure plausible
1 2
hypotheticalscenariosandinferences.
n
(cid:88) (cid:88)
tp = I(yc = 1∧yc = 1), I1: ifatom=1: ∃a ∈ E : (a,r 1,y¯) ∈ F,
τ (cid:98)τ
ifatom=2: ∃b ∈ E : (y,r ,b) ∈ F
j=1τ∈T
τc
2
j I2: ∃c ∈ E : (x,r ,c) ∈ F
3
n
(cid:88) (cid:88) I3: ∃d ∈ E : (d,r ,z) ∈ F
fn = I(yc = 1∧yc = 0), 3
τ (cid:98)τ Theaboveconstraintsensurethattheconstructed
j=1τ∈T
τc triplesτc andτi havesuitableentitiesforthegiven
j
n relation. Intuitively,I1ensuresthatweonlyselect
(cid:88) (cid:88)
fp = I(yc = 0∧yc = 1)
τ (cid:98)τ links(y¯,r 2,z)forwhichtheresultingcounterfac-
j=1τ∈T τc tualtriple(x,r 1,y¯)issensible.
jWhencorruptingagiventriple(h,r,t),weem- Quarrymen). Entity types are available for every
ploytheconstraintsC1,C2andC3whenselecting entity in the CoDEx dataset (Safavi and Koutra,
h′,r′ andt′. 2020)andprovideadditionalinformationregarding
C1: ∃a ∈ E : (h′,r,a) ∈ F theentity. Forinstance,"France"isassociatedwith
C2: ∃b ∈ E : (b,r,t′) ∈ F theentitytype"country"(amongothers)and"7B"
C3: (h′,r,t),(h,r,t′),(h,r′,t) ∈/ F ∪F+, istaggedasa"musicalgroup". Wedenotetheset
∆
whereF+denotesthesetofinferencesmadebyall ofentitytypesassociatedwithanentitye ∈ E by
∆
rulesinourruleset,giventhehypotheticalscenario. type(e). Wedefinethefollowingconstraint:
C1andC2promotechallengingheadandtailcor- I4: type(y¯)∩type(y) ̸= {},
ruptions, which cannot be trivially identified due ifatom = 1andr ∈ {P361,P463}or
1
to the triples being nonsensical. C3 ensures that ifatom = 2andr ∈ {P361,P463}
2
thegeneratedcorruptionsareneitherpresentinthe Thisconditionheuristicallyensuresthattheentity
originalKGnorimpliedbythegivenhypothetical that replaces the original head/tail of a triple to
scenario,givenourruleset. Inrarecases,enforcing createahypotheticalscenarioisofasimilartypeas
C1 or C2 would only allow to create triples that theoriginalentity. Intheexampleabove,(Iraq,part
arealreadyinthegraphorimpliedbyourrules. In of,TheQuarrymen)isnolongeravalidgeneration
suchinstances,wesamplefromthefullentityset when I4 is enforced, since the "The Quarrymen"
E instead,whilestillrespectingC3. sharesnoentitytypewiththeoriginaltail"Middle
East".
Algorithm 2: Creation of CFKGR in-
C.3 Algorithm
stancesforagivenrule.
Data: knowledgegraphG = {E,R,F}, Algorithm 2 describes the dataset creation for
inferenceruleδ, CFKGR. N 1(τc) denotes the one-hop neighbor-
#ofCFstogenerateperatomM hood of τc, excluding the context triggering the
Result: CFKGRinstancesforruleδ rule. Note that N 1(τc) is defined on the full fact
D δ ← {} setF =F train ∪ F valid ∪ F test. Theremaining
foratom ∈ {1,2}do notationfollowsSections2and3.
forn ∈ {1,...,M}do
Randomlysample D HumanDatasetVerification
(x,r ,y)and(y¯,r ,z)fromF ,
1 2 train Thissectiondetailstherecruitmentoftheannota-
accordingtoI1,I2,I3,I4
torsaswellastheguidelinesprovidedtothemvia
ifatom = 1then
theannotationinterface.
τc ← (x,r ,y¯)
1
else
D.1 AnnotatorRecruitmentand
τc ← (y,r ,z)
2 Demographic
τi ← (x,r ,z)
3
Sampleτn,τn fromN (τc) We recruited annotators on a voluntary basis and
1 2 1
Sampleτf fromF \N (τc) didnotofferfinancialcompensation. Labelerswere
1
Createcorruptionsforτi,τn,τn,τf madeawarethattheirannotationswillbeusedand
1 2
publishedinascientificpaper. Werecruitedtwelve
accordingtoC1,C2,C3
annotatorsintotal,includingtheauthors. Allofthe
T ← {τi,τn,τn,τf,
τc 1 2 annotatorshaveatleastaBachelor’sdegree. The
τi ,τi ,τi,...,τf ,τf ,τf }
h′ r′ t′ h′ r′ t′ annotationeffortvariedbetweendifferentannota-
D ← D ∪{(τc,T )}
δ δ τc tors,withthelowestnumberofannotatedsamples
being20andthehighestbeing1020.
C.2 AdditionalConstraintsforP361andP463 D.2 AnnotationGuidelines
ForcounterfactualtriplesτcusingtherelationP361 Thissectioncontainstheannotationguidelinespro-
("part of") or P463 ("member of"), we introduce videdtotheannotatorsontheannotationinterface.
anadditionalconditionwhensamplinge ande Explanations written in italic were added during
1 2
basedonentitytypesinordertoavoidnonsensical the annotation process as they were requested by
hypotheticalscenarios,suchas(Iraq,partof,The annotators. Apart from the guidelines below, theannotatorswereprovidedwithinstructionsonhow millions of inhabitants, city, big city) belongs to
tousetheannotationinterface. thecontinentAsia(continent,continentalareaand
The main goal of the task is to judge the surroundingislands),giventhatthisisnotthecase
plausibility of presented statements, given a hy- intherealworld?
potheticalscenarioandpotentiallyrelevantcontext.
In this scenario, Paris belonging to the con-
Each annotation prompt presented to you tinentAsiawilllikelybethecase,hence,weassign
willconsistofthefollowingelements: thelabel’likely’.
• a hypothetical scenario, which you should
Example2:
assumetobetrue
In some cases, the statement you are pre-
• acontext,whichgivesadditionalinformation
sented with might not have a strong, obvious
regardingtheentitiesinthescenario
connection to the hypothetical scenario (such as
• a statement, which should be labeled as shared entities). This is intended and should not
likely, unlikely, or unsure/too little infor- affect your annotation. For instance, you might
mation encounteranexamplesimilartothefollowing:
Please assign the label likely if you think the Hypotheticalscenario: Paris(citywithmillions
presented statement is likely to hold given the of inhabitants, city, big city) is located in Japan
hypotheticalscenario,thecontext,andyourworld (islandnation,sovereignstate,country)
knowledge. Assignunlikelyifyoudonotthinkso.
Assignthelabelunsure/toolittleinformationif Context: Japan (island nation, sovereign
youcannotconfidentlyjudgetheplausibilityofthe state, country) is part of the continent Asia
statementbasedonthepresentedinformation. (continent, continental area and surrounding
islands)
Expressions in parentheses denote entity
types, which provide additional information Question: Is it then still plausible that En-
for each entity. They can be helpful when glish (modern language, natural language,
reasoningwithlesser-knownentities. Forinstance, language) is the official language of United
the entity ’7B’ is associated with the entity type Kingdom(country,sovereignstate,islandnation),
’musicalgroup’toclarifythat’7B’referstoaband. giventhatthisisthecaseintherealworld?
Each statement follows the general structure If you believe that this statement is still
’Isitthenplausiblethat...,giventhatthisISNOT plausibleinaworldwhereParisisinJapan,assign
the case in the real world?’ or ’Is it then STILL ’likely’. If you think otherwise or cannot make
plausiblethat...,giventhatthisISthecaseinthe a decision based on the presented information,
realworld?’. Pleasepayattentiontothisdifference assign’unlikely’or’unsure/toolittleinformation’
whenlabeling. respectively. In the example above, we would
expect the label ’likely’, since Paris moving to
Example1: Japanshouldnotaffecttheofficiallanguageofthe
UnitedKingdom.
Hypotheticalscenario: Paris(citywithmillions
of inhabitants, city, big city) is located in Japan Example3:
(islandnation,sovereignstate,country)
The statements might not be sensible for all
Context: Japan (island nation, sovereign examples. Forinstance,youcouldcomeacrossa
state, country) is part of the continent Asia statementlike:
(continent, continental area and surrounding
islands) Hypotheticalscenario: Paris(citywithmillions
of inhabitants, city, big city) is located in Japan
Question: IsitthenplausiblethatParis(citywith(islandnation,sovereignstate,country) are 2D-reshapings of entity and relation embed-
dings, ω describes the convolutional filters, and
Context: Japan (island nation, sovereign vecdenotesvectorization(Jietal.,2021).
state, country) is part of the continent Asia
F ExperimentalSetting
(continent, continental area and surrounding
islands)
F.1 ImplementationandRuntimeDetails
We run our experiments on a single Tesla V100
Question: Is it then plausible that Paris (city
GPU with 16GB of memory on a Nvidia DGX1
with millions of inhabitants, city, big city) is the
server. COULDDhyperparametertuningtakesbe-
unmarriedpartnerofAsia(continent,continental
tween around 35 minutes and 65 minutes and a
areaandsurroundingislands),giventhatthisisnot
single run on the test set takes between 3 and 20
thecaseintherealworld?
minutes,dependingonthemodelanddataset.
For KGE embeddings, we use the pre-trained
These examples are intentional and you should
CoDExmodels(SafaviandKoutra,2020),which
annotatethemaccordingtothesameschemeasthe
weretrainedusingLibKGE(Broscheitetal.,2020).
other examples. In the example above, we would
For our experiments with COULDD, we slightly
expectthelabel’unlikely’, sinceacitycannotbe
adapttheLibKGEimplementationtoallowforour
theunmarriedpartnerofacontinent.
proposedtrainingscheme. Forhyperparametertun-
ing, we use the GridSampler implemented in
E KGEMethods
Optuna (Akiba et al., 2019) (version 3.3.0). For
TransE (Bordes et al., 2013) treats relations as computingperformancemetrics(F1,accuracy,con-
translations in the embedding space. It finds em- fusionmatrix),weusescikit-learn(version1.3.0).
beddingvectorsh,r,t ∈ Rde suchthath+r ≈ t Allresultsarereproduciblewithseed0.
for (h,r,t) ∈ F, and uses the scoring function
F.2 Hyperparameters
ϕ(h,r,t) = −||h+r−t|| . ComplEx (Trouil-
2
lonetal.,2016)mapsentitiesandrelationstothe Table 8 lists the hyperparameters used for our
complexspaceandleveragesthescoringfunction experiments 3. Bold parameters were tuned for
ϕ(h,r,t) = Re(⟨r,h,¯t⟩), where h,r,t ∈ Cde COULDDonavalidationsetviagridsearch,while
and¯t denotes the complex conjugate of t. Com- theremainingparameterswerecarriedoverfrom
plEx is particularly well-suited for modeling an- the pre-trained models provided by Safavi and
tisymmetric relations (e.g., "part of"). RESCAL Koutra (2020). For further details on the pre-
(Nickel et al., 2011) represents the fact set as a trainedmodels,pleaserefertoSafaviandKoutra
three-dimensionaltensorX withX = 1if(i,r, (2020). The learning rate (α) was tuned in the
i,j,r
j)∈ F andX = 0otherwise. Representations rangeof{0.001,0.01,0.1,0.15,0.2}. Thenumber
i,j,r
for entities and relations are obtained via a low- ofadditionalsamples(N)waschosenintherange
rank factorization X
r
≈ ER rET, E ∈ R|E|×de, of{0,127,255,511,1023}forallmodelsexcept
R
r
∈ Rde×de . Thescoreofagiventripleiscom- ConvE.ForConvE,therangewasreducedto
puted as ϕ(h,r,t) = hTR t, where h and t are {127, 255, 511, 1023} because of its BatchNorm
r
therowsofE correspondingtohandt. Similarly, layer.
TuckER(Balazevicetal.,2019)leveragesTucker
F.3 ChatGPTExperimentalSetup
decomposition (Tucker, 1966) to find representa-
tionsh,t ∈ Rde,r ∈ Rdr,aswellasacoretensor For our experiments with ChatGPT, we used the
W ∈ Rde×dr×de whichallowssharingknowledge OpenAI API. We used the model gpt-3.5-turbo-
between all entity and relation embeddings. The 0613 and set the temperature to 0 for all exper-
scoresaredefinedasϕ(h,r,t) = W× h× r× t, iments. The given system prompt, prompt tem-
1 2 3
where× denotesthetensorproductalongthei-th plates,aswellasaninputandoutputexampleare
i
mode. TuckERwasshowntobeeffectiveformod- giveninTable9.
elingcompositionalrelations(SafaviandKoutra, For two inputs in CFKGR-CoDEx-S, 12 in
2020). ConvE (Dettmers et al., 2018) is a con- CFKGR-CoDEx-Mand23inCFKGR-CoDEx-L,
volutional architecture described by ϕ(h,r,t) = ChatGPTdidnotanswerinthedesiredformat. We
f(vec(f([M ;M ]∗ω))W)t,whereM andM neverthelessattemptedtoextracttheanswerusing
h r h raregularexpressionbutthisprocesscouldpoten- H.3 Confusionmatrixon
tiallybeerror-prone. ForoneinstanceofCFKGR- CFKGR-CoDEx-M*
CoDEx-M, ChatGPT gave the answer “inconclu-
Table12givestheconfusionmatrixforCOULDD
sive”,whichisnotoneofouracceptedlabels. We
andChatGPTonCFKGR-CoDEx-M*withmanu-
countedthisinstanceaswronglyclassifiedinour
allyassignedlabels.
experiments.
I FurtherChatGPTObservations
G EvaluationperTestType
By analyzing the explanations provided by Chat-
Table10providestheperformancepertestcasefor GPT, we found some frequent errors in its re-
theresultsinTable3. Theresultssuggestthathead sponses,whichmightbeindicativeofitspoorper-
corruptions of valid facts are generally harder to formanceonunchangedfacts.
identifythantailcorruptions. Thisislikelypartially Firstofall,despitethepromptfornearandfar
duetothesetupoftheCoDExtripleclassification facts explicitly stating that the statement “is the
benchmark,whichtunesdecisionthresholdssolely case in the real world”, ChatGPT oftentimes di-
ontailcorruptionsandalwaysusesobject-oriented rectly references that it was “not the case in the
scoring, even when reciprocal relations are avail- real world” in its explanations. This observation
able. We adopted this setup to make our results ishowevernotconsistentacrossexamplesandin
comparable to the original CoDEx paper (Safavi manyinstances,ChatGPTexplainedcorrectlythat
andKoutra,2020). itshoulddecidewhetheratripleis“stillplausible”
giventhatit“wasthecaseintherealworld”. This
H CaseStudyonCoDEx-M inconsistencycouldpotentiallystemfromoursys-
temprompt(seeAppendixF.3),whichexplainsthe
H.1 CFKGR-CoDEx-M*LabelDistribution distinctionbetweenthetwocases.
Moreover,wenoticedthatChatGPTsometimes
Table 7 gives the label distribution of expected
gaveawrongpredictionwhenthesameentitywas
labels(accordingtoourassumptions)andmajority
associated with different entity types in the sce-
votelabelsonCFKGR-CoDEx-M*.
nario, context and statement due to random sam-
pling(e.g.,Budapest(towninHungary,capital,en-
Expected(E) Human(H)
clave)comparedtoBudapest(citywithmillionsof
0 1 0 1
inhabitants,towninHungary,enclave)). Keeping
τi 0 181 33 148 entity types consistent could likely help to boost
τf 0 255 0 255 the performance of ChatGPT on our benchmark.
τn 0 495 14 481 JudgingfromtheannotationresultsinTable2,hu-
τi 228 0 214 14 mans were still largely able to reliably judge the
r′
validityofthegivenstatements,despitethevarying
Table7: LabeldistributionintheCFKGR-CoDEx-M* entitytypes.
test set with expected labels (E) and human-assigned
(H)labels.
H.2 Rule-wisePerformanceonFiltered
CoDEx-M
InSection5.3.1,weinvestigatehowwellthepre-
trained CoDEx models can infer CoDEx-M test
triplesthatareimpliedbyAMIE3rules. Notethat
theseexperimentsdonotintroduceanyhypotheti-
calscenarios. Table11providesinformationabout
the performance on individual rules. Note that a
triplecanpotentiallybeinferredbymultiplerules
andhencecontributetotherule-wiseperformance
withrespecttomultiplerules.RESCAL TransE ComplEx ConvE TuckER
CFKGR-CoDEx-S
Embeddingsize 512 512 512 256 512
Reciprocal No Yes Yes Yes Yes
Optimizer Adagrad Adagrad Adam Adagrad Adagrad
Regularization
Type l 3 l 2 None l 3 l 1
Entityembeddings
2.18×10−10 1.32×10−7 9.58×10−13 3.11×10−15 3.47×10−15
Relationembeddings 3.37×10−14 3.72×10−18 0.0229 4.68×10−9 3.43×10−14
Frequencyweighting False False True True True
Dropout
Entityembeddings 0.0 0.0 0.0793 0.0 0.1895
Relationembeddings 0.0804 0.0 0.0564 0.0 0.0
Featuremap(ConvE) - - - 0.2062 -
Projection(ConvE) - - - 0.1709 -
Additionalsamples(N) 127 255 127 255 255
Learningrate(α) 0.01 0.01 0.1 0.001 0.01
CFKGR-CoDEx-M
Embeddingsize 256 512 512 512 512
Reciprocal Yes Yes Yes Yes Yes
Optimizer Adagrad Adagrad Adagrad Adagrad Adagrad
Regularization
Type l 2 l 2 l 3 l 1 l 1
Entityembeddings
9.56×10−7 1.32×10−7 1.34×10−10 1.37×10−10 3.47×10−15
Relationembeddings
2.56×10−17 3.72×10−18 6.38×10−16 4.72×10−10 3.4×10−14
Frequencyweighting False False True True True
Dropout
Entityembeddings 0.0 0.0 0.1196 0.0 0.1895
Relationembeddings 0.0 0.0 0.3602 0.0348 0.0
Featuremap(ConvE) - - - 0.3042 -
Projection(ConvE) - - - 0.2343 -
Additionalsamples(N) 255 511 0 511 1023
Learningrate(α) 0.01 0.01 0.1 0.001 0.01
CFKGR-CoDEx-L
Embeddingsize 128 128 128 256 256
Reciprocal No Yes Yes Yes No
Optimizer Adagrad Adam Adagrad Adagrad Adagrad
Regularization
Type l 2 l 2 l 2 l 1 l 2
Entityembeddings
2.01×10−16 7.98×10−14 2.01×10−16 6.10×10−16 8.06×10−11
Relationembeddings
3.52×10−13 3.42×10−9 3.52×10−13 1.03×10−16 7.19×10−19
Frequencyweighting True False True True True
Dropout
Entityembeddings 0.0 0.0 0.0 0.0064 0.1606
Relationembeddings 0.0 0.0 0.0 0.0 0.0857
Featuremap(ConvE) - - - 0.1530 -
Projection(ConvE) - - - 0.4192 -
Additionalsamples(N) 0 1023 0 127 127
Learningrate(α) 0.1 0.2 0.2 0.01 0.01
Table8: HyperparametersforCOULDDexperiments. Boldhyperparametersweretunedbyus. Theremainingare
takenfromtheoriginalCoDExpaper(SafaviandKoutra,2020)andkeptthesameforourexperiments.SystemPrompt Youareahelpful,honestdatalabelerwhoclassifiestheplausibilityofahypothetical
scenario.
Youwillbepresented:
Hypotheticalscenario:Thisfactiswrongintherealworld,butyouassumeitistrue
inthecurrenthypotheticalworld.
Context:Itistrueinthehypotheticalworld,andgivesadditionalinformationyoucan
usetoreasonabouttheplausibilityofthepresentedstatement
Astatementwhichissupposedtobelabeledas’likely’or’unlikely’.
Eachstatementfollowsthegeneralstructure’Isitthenplausiblethat...,giventhat
thisISNOTthecaseintherealworld?’or’IsitthenSTILLplausiblethat...,given
thatthisISthecaseintherealworld?’.Pleasepayattentiontothisdifferencewhen
labeling.
StatementTemplate {{Statement}}
Return’likely’ifyouthinkthepresentedstatementislikelytoholdgiventhehypo-
theticalscenario,thecontext,andyourworldknowledge.
-Ifafactwas"thecase"intherealworld,respond’likely’ifthehypotheticalscenario
doesnotmakeitimplausible
-Ifafactwas"notthecase"intherealworld,respond’likely’ifthehypothetical
scenariomakesitplausible.
Return’unlikely’onlyifthepresentedstatementisunlikelytoholdgiventhehypo-
theticalscenario,thecontext,andyourworldknowledge.
-Ifafactwas"thecase"intherealworld, respond’unlikely’ifthehypothetical
scenariomakesitimplausible.
-Ifafactwas"notthecase"intherealworld,respond’unlikely’ifthehypothetical
scenariodoesnotmakeitplausible.
IfThehypotheticalscenarioandthecontextdonotprovideanyspecificinformation
forthestatement,respond’unlikely’giventhatthestatementis"notthecaseinthe
realworld"and’likely’ifthestatement"isthecaseintherealworld".
RespondwithaJSON.First,includeaexplanationofyourreasoninginthe’explana-
tion’fieldandgivethenananswerinthe’answer’field.
ExampleStatement Hypotheticalscenario:JulioIglesias(human)ismarriedtoRachelWeisz(human)
Context:RachelWeisz(human)isacitizenofUnitedKingdom(islandnation,Com-
monwealthrealm,sovereignstate)
IsitthenplausiblethatJulioIglesias(human)isacitizenofUnitedKingdom(colonial
power,islandnation,country),giventhatthisisnotthecaseintherealworld?
OriginalLabel Unlikely
ExpectedLabel(inCFKG) Likely
ExampleOutput {
"explanation": "Basedonthehypotheticalscenarioandthecontextprovided,itis
plausiblethatJulioIglesias(human)isacitizenofUnitedKingdom(colonialpower,
islandnation,country).ThecontextstatesthatRachelWeisz(human)isacitizenof
UnitedKingdom,whichimpliesthatshehascitizenshipinthecountry.SinceJulio
IglesiasismarriedtoRachelWeiszinthehypotheticalscenario,itislikelythathe
wouldalsohaveacquiredcitizenshipintheUnitedKingdomthroughmarriage.",
"answer":"likely"
}
Table9: ThetableshowshowChatGPTwasused. Itincludestemplatesandanexamplescenariowiththeprovided
output.τi τf τhi′ τhf ′ τhn′ τn τri′ τrf ′ τrn′ τti′ τtf ′ τtn′
Dataset Method
CFKGR-CoDEx-S RESCAL 27.12 99.46 73.78 39.24 56.6 98.55 94.94 94.21 94.76 79.57 51.54 61.66
COULDD-RESCAL 32.48±0.73 99.28±0.2 73.82±1.04 43.15±1.19 55.5±0.53 98.17±0.16 94.86±0.25 94.5±0.25 94.47±0.11 79.78±0.56 54.32±0.41 61.95±0.16
TransE 23.15 95.84 78.84 32.91 55.70 90.78 97.11 95.48 93.94 86.80 52.80 68.72
COULDD-TransE 26.8±0.81 94.39±0.26 82.78±0.29 36.56±0.31 58.16±0.21 89.84±0.21 97.11±0.16 95.66±0.11 93.96±0.04 88.28±0.81 58.37±0.27 71.57±0.07
ComplEx 29.11 98.73 72.15 44.12 58.14 98.82 96.2 97.11 96.20 79.39 56.06 64.65
COULDD-ComplEx 37.94±0.67 93.31±0.66 84.27±0.98 63.83±0.68 71.32±0.85 94.27±0.19 95.77±0.14 97.0±0.14 96.42±0.11 79.06±1.52 72.12±1.3 74.21±0.66
ConvE 16.64 97.65 81.19 43.76 65.46 93.67 96.56 91.32 87.79 92.95 53.53 73.78
COULDD-ConvE 16.96±0.72 97.22±0.18 82.21±0.18 45.53±0.4 65.23±0.41 93.49±0.27 96.6±0.07 91.72±0.07 87.58±0.17 93.56±0.18 55.15±0.49 75.8±0.15
TuckER 15.01 98.37 83.73 45.39 71.34 98.55 95.3 96.93 94.21 89.33 54.79 73.69
COULDD-TuckER 35.99±1.0 97.72±0.54 78.23±0.29 48.1±1.23 67.09±0.56 98.61±0.07 94.29±0.22 96.93±0.23 93.35±0.15 83.11±0.63 58.59±0.62 74.23±0.3
CFKGR-CoDEx-M RESCAL 21.57 97.96 79.41 46.90 68.10 95.75 91.18 91.42 91.30 87.01 58.50 75.57
COULDD-RESCAL 26.23±0.16 96.44±0.19 81.44±0.14 48.91±0.25 70.14±0.27 94.49±0.19 91.23±0.08 91.47±0.23 91.31±0.09 87.19±0.16 59.54±0.32 76.41±0.12
TransE 23.61 88.56 76.31 36.11 62.50 75.37 92.97 92.89 89.26 86.19 53.84 70.14
COULDD-TransE 26.06±0.25 85.85±0.18 76.83±0.27 38.94±0.18 63.68±0.16 74.31±0.14 92.78±0.1 93.17±0.04 89.31±0.03 86.75±0.14 57.63±0.27 70.92±0.09
ComplEx 11.60 97.96 89.38 49.02 75.08 97.55 93.63 94.61 92.65 94.69 59.56 80.39
COULDD-ComplEx 34.67±0.23 97.96±0.0 79.17±0.53 48.95±0.08 69.58±0.21 97.21±0.1 93.09±0.11 94.59±0.03 92.34±0.07 90.36±0.31 59.48±0.05 79.01±0.09
ConvE 13.15 93.06 87.09 41.91 67.97 81.78 95.1 88.32 84.76 94.12 53.68 77.33
COULDD-ConvE 17.04±0.16 84.72±0.45 85.38±0.25 43.94±0.1 68.84±0.42 71.49±0.35 92.4±0.14 86.18±0.61 81.66±0.4 93.5±0.4 54.31±0.59 79.52±0.17
TuckER 13.15 97.96 88.4 50.74 76.76 97.14 92.48 91.18 88.77 95.02 58.33 80.8
COULDD-TuckER 43.69±0.38 98.33±0.11 73.14±0.54 44.07±0.58 67.06±0.18 97.99±0.11 91.99±0.32 90.87±0.46 87.57±0.11 90.1±0.38 58.27±0.68 78.7±0.15
CFKGR-CoDEx-L RESCAL 71.47 99.89 32.09 18.09 23.39 99.63 68.92 74.08 72.88 51.52 53.91 51.41
COULDD-RESCAL 84.56±0.35 99.89±0.0 32.37±0.58 18.16±0.07 23.15±0.14 95.58±0.21 69.2±0.48 74.09±0.07 69.94±0.23 45.71±0.54 53.87±0.07 51.61±0.21
TransE 66.31 99.41 30.07 18.31 20.68 99.25 79.4 48.00 40.82 48.96 46.89 44.97
COULDD-TransE 76.56±0.25 98.99±0.1 30.47±0.55 27.87±0.66 22.47±0.21 99.35±0.06 77.54±0.18 53.22±0.35 43.67±0.19 58.12±0.3 60.04±0.82 55.93±0.37
ComplEx 65.51 99.57 36.14 27.25 33.02 99.44 90.47 84.62 83.93 58.91 64.93 61.07
COULDD-ComplEx 82.95±0.26 99.57±0.0 31.73±0.13 27.29±0.04 29.44±0.09 99.53±0.04 89.03±0.12 84.57±0.03 83.25±0.09 55.5±0.29 64.84±0.07 59.68±0.14
ConvE 61.84 99.52 41.35 36.46 40.66 99.18 91.06 61.63 53.54 61.58 63.70 60.32
COULDD-ConvE 45.53±0.61 94.5±0.36 61.25±0.59 53.79±0.3 57.2±0.47 95.32±0.18 93.18±0.31 73.72±0.73 67.2±0.54 79.45±0.36 78.11±0.42 75.53±0.51
TuckER 76.74 99.79 27.46 14.74 22.78 99.65 75.36 63.23 62.03 53.33 50.56 49.92
COULDD-TuckER 88.47±0.34 99.74±0.07 25.92±0.52 15.94±0.23 19.58±0.29 99.68±0.04 72.59±0.33 64.14±0.44 61.29±0.21 50.07±0.29 52.84±0.14 48.13±0.54
Table10: Accuracybytesttypeofpre-trainedembeddingsandCOULDDonCFKGR.ForCOULDD,wereportthe
meanandstandarddeviationacross5runs.
Support PCA #Test RESCAL TransE ComplEx ConvE TuckER
(P112,P27,P17) 38 0.826 5 1.000 1.000 0.400 1.000 0.800
(P20,P37,P1412) 836 0.818 36 0.972 0.972 1.000 0.944 1.000
(P19,P37,P1412) 665 0.790 23 1.000 0.826 1.000 0.826 0.957
(P26,P27,P27) 682 0.661 15 0.933 0.933 0.867 0.733 0.933
(P27,P37,P1412) 9937 0.543 416 0.962 0.918 0.993 0.901 0.978
(P17,P30,P30) 100 0.427 5 0.200 0.000 0.200 0.000 0.600
(P161,P27,P495) 1464 0.406 87 0.805 0.943 0.920 0.931 0.931
(P159,P17,P17) 137 0.346 6 1.000 0.833 1.000 0.833 0.833
(P131,P17,P17) 82 0.297 5 0.600 0.600 0.400 0.800 1.000
(P161,P20,P840) 87 0.134 6 1.000 1.000 1.000 1.000 1.000
Table11: Rule-wiseperformanceonthefilteredtestsetofCoDEx-M(seeTable4). Foreachrule,wereportthe
numberofpositiveexamples(“Support”)andPCAconfidence(“PCA”)ascomputedbyAmie3onthefullKGand
thenumberofinferencesinthefilteredtestset(“#Test”). ThePCAconfidenceandnumberofpositiveexamples
aretakenfromhttps://github.com/tsafavi/codex.
CFKGR-CoDEx-M*(H)
τi τf τn τ r′
TN FP FN TP TN FP FN TP TN FP FN TP TN FP FN TP
COULDD-RESCAL 13.2 19.8 122.4 25.6 0.0 0.0 8.2 246.8 0.0 14.0 24.0 457.0 196.8 17.2 11.0 3.0
COULDD-TransE 17.0 16.0 121.6 26.4 0.0 0.0 31.6 223.4 5.4 8.6 116.8 364.2 201.8 12.2 12.0 2.0
COULDD-ComplEx 15.4 17.6 98.0 50.0 0.0 0.0 4.0 251.0 2.0 12.0 7.4 473.6 202.4 11.6 13.4 0.6
COULDD-ConvE 18.2 14.8 132.6 15.4 0.0 0.0 35.2 219.8 4.2 9.8 128.4 352.6 197.2 16.8 11.8 2.2
COULDD-TuckER 13.2 19.8 89.2 58.8 0.0 0.0 3.4 251.6 0.0 14.0 8.2 472.8 203.8 10.2 12.0 2.0
gpt-3.5-turbo 21 12 64 84 0 0 188 67 8 6 169 312 125 89 11 3
Table12: PerformanceanalysispertesttypeonCFKGR-CoDEx-M*withhuman-assignedlabels. ForCOULDD,
thereportedvaluesareaveragedover5modelruns.