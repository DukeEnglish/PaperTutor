1
Decentralized and Lifelong-Adaptive
Multi-Agent Collaborative Learning
Shuo Tang, Rui Ye, Chenxin Xu, Xiaowen Dong, Siheng Chen, Yanfeng Wang
Abstract—Decentralizedandlifelong-adaptivemulti-agentcollaborativelearningaimstoenhancecollaborationamongmultipleagents
withoutacentralserver,witheachagentsolvingvariedtasksovertime.Toachieveefficientcollaboration,agentsshould:i)autonomously
identifybeneficialcollaborativerelationshipsinadecentralizedmanner;andii)adapttodynamicallychangingtaskobservations.Inthis
paper,weproposeDeLAMA,adecentralizedmulti-agentlifelongcollaborativelearningalgorithmwithdynamiccollaborationgraphs.To
promoteautonomouscollaborationrelationshiplearning,weproposeadecentralizedgraphstructurelearningalgorithm,eliminatingthe
needforexternalpriors.Tofacilitateadaptationtodynamictasks,wedesignamemoryunittocapturetheagents’accumulatedlearning
historyandknowledge,whilepreservingfinitestorageconsumption.Tofurtheraugmentthesystem’sexpressivecapabilitiesand
computationalefficiency,weapplyalgorithmunrolling,leveragingtheadvantagesofbothmathematicaloptimizationandneuralnetworks.
Thisallowstheagentsto‘learntocollaborate’throughthesupervisionoftrainingtasks.Ourtheoreticalanalysisverifiesthatinter-agent
collaborationiscommunicationefficientunderasmallnumberofcommunicationrounds.Theexperimentalresultsverifyitsabilityto
facilitatethediscoveryofcollaborationstrategiesandadaptationtodynamiclearningscenarios,achievinga98.80%reductioninMSEand
a188.87%improvementinclassificationaccuracy.Weexpectourworkcanserveasafoundationaltechniquetofacilitatefutureworks
towardsanintelligent,decentralized,anddynamicmulti-agentsystem.Codeisavailableathttps://github.com/ShuoTang123/DeLAMA.
IndexTerms—Multi-agentsystems,collaborativelearning,graphlearning,algorithmunrolling
✦
1 INTRODUCTION
COllaborationisastealthy,yetubiquitousphenomenon machinelearning[7],[12],[13]andmulti-tasklearning[14],
in nature, evident in the cooperative behaviors of [15].(3)Atthedecision-makinglevel,agentscooperateand
animals and humans, from pack-hunting to constructing communicatetomakejointdecisions,aimingtomaximize
complex social relationships. Such cooperative behaviors rewardsfromtasksperformedintheirenvironments.Agents
enableindividualstoaccomplishcomplextasks[1]andform share high-level information about decision-making [16],
socialrelationships[2].Inspiredbythisnaturaltendencyfor [17]. The related approaches are widely used in research
collaboration,thefieldofmulti-agentcollaborativelearning fieldslikemulti-agentreinforcementlearning(MARL)[18].
hasbeenextensivelyexplored[2].Itaimstoenablemultiple (4) Moving to the control level, the focus shifts towards
agentstocollaborativelystrategizeandachievesharedob- formulating efficient control strategies and coordination
jectives,exhibitingcapabilitiessurpassingthatofanysingle mechanisms.Thesestrategiesmainlyshareindividualstate
agent.Recently,therehasbeenanurgentdemandforrelevant informationsuchaspositionorvelocity.Commonapplication
methods and systems, such as vehicle-to-everything(V2X) scenarios for these control strategies encompass areas like
communication-aided autonomous driving [3], [4], multi- trafficcontrol[19],[20],orunmannedaerialvehicle(UAV)
robotenvironmentexploration[5],[6]andcollaborativetrain- system control [21]. In this paper, we focus on cognitive-
ingofmachinelearningmodelsacrossmultipleclients[7]. level collaboration, where agents collectively train models
Depending on the type of content exchanged among by sharing model parameters beyond raw observations,
agents, multi-agent collaboration can be categorized into facilitating more streamlined knowledge sharing among
fourdistinctlevels,namelyperception,cognition,decision- agentstoequipthesystemwithamorepowerfulcognitive
making,andcontrol.(1)Attheperceptionlevel,theprimary capability.
focusistocompensateforthesensoryinformationgapsof Topromoteefficientcollaborationamongtheagents,itis
individual agents by transmitting perceptual data among crucialtoidentifywhomtocollaboratewith.Thiscouldavoid
multipleagents.Thesharedinformationoftenencompasses redundant and low-quality information from an arbitrary
perceptual signals, such as raw observations and percep- collaborator,improvingcollaborationeffectiveness.Current
tual features. Such mechanisms are especially valuable in collaborationmethodsofteninvolveusingpredefinedfixed
research fields like collaborative perception [8], and multi- collaborationrelationships,suchasfullyconnectedgraphs
view learning [9]. (2) At the cognitive level, the aim is (e.g.,CoLLA[11],DiNNO[22]),stargraphswithacentral
to improve multiple agents’ learning task performance by server(e.g.,federatedlearning[7]),ortaskcorrelationscalcu-
sharing and processing cognitive information. Specifically, latedbyacentralserverinmulti-tasklearning[14].However,
assuming local decision-making is done via training an thesepredefinedstructuresarelimitedbytwomajorissues.
individual model, the shared cognitive information often First,collaboration[7],[14]guidedbyacentralserverfaces
relates to individuals’ model parameters [7], knowledge vulnerabilityissues,wherethecentralserverfailurewould
graph[10],ordictionary[11].Suchmethodologiesarepromi- disableallcollaborations.Second,thestaticcollaboration[7],
nently employed in research fields such as collaborative [14] relationships limit the flexibility and efficiency of the
4202
raM
11
]GL.sc[
1v53560.3042:viXra2
Decentralized and Lifelong-Adaptive Multi-Agent Collaborative Learning
: Model parameters
: Collaborating agents
: Different observations
: Collaboration relationships
Timestamp 1 Timestamp 2 Timestamp 3 Time
Fig.1:Theillustrationofourdecentralizedandlifelong-adaptivemulti-agentcollaborativelearningsystem.Wedemonstrate
a collaboration system with six agents, each faced with a learning task. The learning tasks’ configurations between the
agentsaredynamic,makingthecollaborationrelationshipsdynamicallyadapttothetime-evolvingtasks.
systemperformancewhenfacedwithdynamicscenarios.In oftwosteps:collaborativerelationalinferenceandlifelong
dynamicscenarios[23],agentscomeupwithcontinuousand model update. For the collaborative relational inference
time-varyingobservationsandupdatetheirlocaldecision- step, we propose a convex quadratic program as a graph
making model, leading to the change of agent potential structure learning problem with the graph smoothness
relationships. The static collaboration modeling becomes prior. To solve this problem without a central server, we
inadequateandinflexibletocapturetherelationshipsinthese propose a decentralized optimization approach based on
dynamicscenarios,limitingthecollaborationefficiency.In Newton iterations by sharing dual variables through the
thiswork,weintroduceanoveldecentralizedandlife-long communication links. For the lifelong model update step,
adaptivelearningframeworkformulti-agentcognitive-level weproposeagraphmessage-passingmechanismbysharing
collaboration.Thecorefeatureofourframeworkisthateach modelparameterswithcollaboratorsinferredintheprevious
agentcanautonomouslychooseitscollaboratorsandupdates step.Toenableagentstoretainmemoriesofpasttasks,we
itsmodeltoaddresstasksthatevolveovertime. introduceanonlineupdatemechanismbycontinuouslyap-
To address the limitations of centralized collaboration proximatingpreviouslyencounteredlossfunctionsthrough
mechanisms,insteadofusingacentralserver,wepropose Taylorexpansion.Thesetwoiterativesteps,whiledistinct,
adecentralizedcollaborationmechanismthatdecouplesthe are closely intertwined: efficient collaborative relational
globallyorientedcollaborationproblemintotheagentlevel. inferencesimplifieslifelongmodelupdate,andviceversa.
Giventhemodelparametersandobservations,thekeyde- Tofurtherpromotemorelearningabilityandflexibility,
signofourapproachistodiscardthecentralserverandlearn weapplyalgorithmunrollingtechniques[24],[25]toupgrade
sparse collaboration relationships between agents. Agents the aforementioned iterative algorithm of the decentral-
are empowered to autonomously select collaborators and izedoptimizationtoaneuralnetwork.Throughalgorithm
communicateindependently.Specifically,wemodelagents’ unrolling, each iteration of our optimization solution is
collaborationrelationshipsbyacollaborationgraph,where mappedtoacustomizedneuralnetworklayerwithlearnable
nodesdenotetheagentsandedgesstandforthecollaboration parameters.Thenetworkparametersarelearnedviasuper-
strength. Compared to the centralized collaboration mech- visedtrainingtasks,enablingagentstoeffectivelylearnto
anism[7],[15],ourdecentralizedcollaborationmechanism collaborate.
offerstwoadvantages:1)computationsaredecentralizedat WetermtheoverallmethodasDeLAMA,whichachieves
thelevelofeachagent,enhancingsystemrobustnessand2) DecentralizedandLifelong-AdaptivelearningforMultiple
alleviatesthecommunicationoverheadtypicallyimposedon Agents. The proposed DeLAMA combines the advantages
a central server, dispersing the communication load more of both mathematical optimization and neural networks;
evenlyamongtheagents. see the illustration in Figure 1. Unlike multi-task learning
To address the issue of the static collaboration relation- where the agents’ relationships are decided by the central
ship,wedesignalifelong-learning-basedapproachformulti- server,DeLAMAprovidesadecentralizedmechanismtoinfer
agent collaboration. The core problem is enabling agents collaboration relationships. According to our design, our
todynamicallyadapttoever-changingobservationsduring collaboration method DeLAMA can 1) efficiently operate
individualmodeltraining,toprovideabasisforobtaining in dynamic learning scenarios, 2) promote decentralized
moreaccuratecollaborationrelationships.Inagentindividual collaborationwithoutacentralserver,and3)autonomously
model training, we design a memory unit to capture an learnandseekcollaborativepartners.Thecommunication
agent’saccumulatedlearninghistoryandknowledge,facili- andcollaboration betweenagents ofDeLAMApossessboth
tatingadaptationtoevolvingobservationflows.Compared theoreticalconvergenceguaranteeandrobustlearningcapa-
withthestaticcollaborationmechanism[7],[14],thislifelong- bilities.
learning-basedapproachcouldleadtoevolvingcollaboration Our theoretical analysis of DeLAMA reveals that: 1) the
structuresandmodelparameters,makingitmoresuitable overallsolutionexhibitsfavorableconvexoptimizationprop-
fordynamicscenarios. ertiesandexpressivecapability;2)thecollaborationrelation
Integratingtheabovedesigns,weformulateadecentral- inference solution possesses a quadratic convergence rate,
izedoptimizationforlifelongadaptivemulti-agentcollabora- enablingefficientcollaboratoridentificationbyagents;and3)
tivelearning.Thecorrespondingiterativealgorithmconsists ourmodel-updatingsolutionachievesalinearconvergence3
TABLE1:Relationstoprevioustasksettings,includinglifelong
rate,effectivelyminimizingthecommunicationdemand.
learning,multi-tasklearning,andfederatedlearning.
OurexperimentalevaluationofDeLAMAfocusesontwo
typical multi-agent learning tasks: regression and classifi- Task Dynamic Decentralized Numberof
scenario mechanism agents
cation,coveringbothsyntheticandreal-worlddatasets.In
theclassificationscenario,theexperimentincorporatesboth Lifelonglearning 1
Multi-tasklearning N
cameraimagedataandlidarscandata,supportingabroad
Federatedlearning N
spectrum of downstream applications. To further validate
Ours(Decentralized
the rationale of our introduced collaboration relationship
andlifelong-adaptive N
priorsinDeLAMA,wecomparethecollaborationmechanisms collaborativelearning)
to humans by conducting a human-involved experiment.
The key findings of experimental results on DeLAMA lie
past training data is prohibited. Based on the constraint,
intwoaspects.Fromthedecentralizedcollaborationview-
agentswillupdatethelearningmemoryaccordingtonewly
point,DeLAMAoutperformsothergraphstructurelearning
observed data
D(t)
at each time, hence promoting lifelong
approachesininferringcollaborationrelationships,despite i
adaptation.
many of them being centralized methods [26], [27]. From
the lifelong adaptation perspective, DeLAMA could enable Undertheaboveconditionsoninformationsharingand
memory,decentralizedandlifelong-adaptivecollaborative
agentstoefficientlyadapttodynamiclearningenvironments
learningaimstofindaneffectivedecentralizedmodeltrain-
comparedtopreviouslifelonglearningmethods[28],[29].
ingstrategy.Denotethecommunicationmessagesentfrom
The outline of this paper is structured as follows. In
Section2weintroducethesettingofdecentralizedlifelong- agentitoagentj attimetasm( i→t) j,wheretheinformation
adaptive collaborative learning. Section 3 formulates the routingprocesscouldbemulti-hopviathecommunication
optimization problem and introduced a iterative learning
graphC(t).Themodelparameterandmemoryupdatingrule
framework.Section4furtherappliesthealgorithmunrolling aredescribedas
methodtotheiterations.InSection5,weanalyzedthenu- θ i(t),M( it) =Φ i(cid:16) D i(t),{m( jt →) i}N j=1,M( it−1)(cid:12) (cid:12)C(t)(cid:17) , (1)
mericalpropertiesoftheiterationsproposedinSection3and whereΦ i(·)isthei-thagent’sapproachtoupdatelearning
Section4.InSection6weconductfourdifferentexperiments memoryandmodelparametersaccordingtopreviousindi-
onbothsyntheticandreal-worlddatasets.Section7describes viduallearningmemoryM(t−1) ,trainingdataD(t)
andthe
i i
relatedworksincollaborativelearningandlifelonglearning. messages{m(t) }N sentfromotheragents.
Finally,weconcludethepaperinSection8. j→i j=1
Note that: i) the collaboration mechanism among the
2 PROBLEM FORMULATION agents is decentralized, which means agents learn their
model parameters based on individual observations and
2.1 TaskSetting
mutualcollaborationwithoutthemanagementofacentral
Considering a multi-agent system containing N agents,
server; ii) the collaboration system, including the agents’
each agent can train a dynamic machine-learning model
modelsandthecollaborationstrategies,islifelong-adaptive
forevolvingtasks.Atanygiventimestampt,thei-thagent
(cid:110) (cid:111) to the dynamic scenarios, enabling agents to efficiently
receivesatrainingdatasetD i(t) = X i(t),Y i(t) ,whereX i(t)
learn new knowledge while preserving knowledge from
representsthedatainputsandY(t) denotestheassociated previously encountered ones; and iii) the sparse commu-
ground-truth labels. Based on thi is, the i-th agent updates nication prior among agents based on the constraint C(t)
itsmodelparameters,denotedasθ(t)
,toeffectivelyhandle
shouldbeguaranteed,whichmeanstheinter-agentmessages
thetestdatasetD(cid:101)(t) =(cid:110) X(cid:101)(t),Y(cid:101)(t)(cid:111)i
.Itisworthnotingthat
m( jt →)
i
shouldadheresstrictlytothecommunicationgraph
i i i constraintC(t).
the test dataset D(cid:101) i(t) shares the same distribution as the Toevaluatethelearningperformanceofthei-thagentat
training dataset D i(t) . The overall goal is to enhance inter- timestamp t, we use the performance accumulation as the
agents’collectivelearningperformance.Toachievethis,each evaluationmetric:
vag ae lun at bc la en infu fort rh me ar t1 io) nco ull na dbo erra ate cow lli ath boo rt ah te ir onag pe rn ot ts ob coy ls ,h aa nr din 2g
)
L( it) =
1
t
(cid:88)t L(cid:16)
f θ(k)(X(cid:101) i(k)),Y(cid:101)
i(k)(cid:17)
,
i
useitshistoricalinformationinthelearningprocess. k=1
Fordecentralizedmulti-agentcollaboration,information where X(cid:101) i(t) is the test data and Y(cid:101) i(t) is the associated test
sharing is achieved by direct peer-to-peer communication labels.L(·)isthemetricfunctioncorrespondingtothetask.
withatime-varyingcommunicationgraphC(t) ∈{0,1}N×N. TheoverallperformanceofN agentsisL(t) =L(t)/N.The
i
Here,eachnoderepresentsoneagentandeach(i,j)-thedge taskobjectiveistomaximizeL(t) bydesigning,optimizing
withC(t) = 1isapairwisecommunicationlinkindicating thecommunicationprotocolofsharingmessagesm(t) and
ij j→i
themessagetransmissionbetweenagenti,j isenabled. themodelupdatingapproachΦ i(·)basedontheobserved
Given that the current learning task could relate to dataD(t) =(cid:110) D(t)(cid:111)N
ateachtimestamp.
previous ones, each agent should be able to incorporate 1:N i i=1
past task experiences into its current learning process. Let
M(t) bethelearningmemoryofthei-thagentuptotimet. 2.2 RelationstoPreviousTaskSettings
i
To maintain and utilize the lifelong and adaptive learning Weexaminerelatedlearningsettings:lifelonglearning,multi-
ability,thestoragespaceforthememoryM(t)
oftheentire task learning, and federated learning, with relationships
i
learningtaskT iisrequiredtobefinite,hencedirectlystoring showninTable1andFigure2.4
p(cid:16) D(t)(cid:12) (cid:12)θ(t)(cid:17)
. Hence simply using the likelihood function
i i
p(cid:16) Dt (cid:12) (cid:12)Θ(t)(cid:17) is impractical due to complex inter-agent
T1:N
correlationsbetweenagents’modelparameters.Instead,we
consider the Bayesian learning paradigm by maximizing
the posterior distribution of Θ(t) as p(cid:16) Θ(t)(cid:12) (cid:12)Dt (cid:17) after
T1:N
knowing the datasets Dt . The inter-agents’ correlations
T1:N
aredescribedbythepriordistributionofΘ(t).Specifically,
accordingtotheBayesrule,theposteriordistributioncanbe
decomposedintothefollowingtwoparts:
p(cid:16) Θ(t)(cid:12) (cid:12)Dt (cid:17) ∝p(cid:16) Dt (cid:12) (cid:12)Θ(t)(cid:17) p(cid:16) Θ(t)(cid:17) , (2)
T1:N T1:N
where the first probability is the likelihood of the agents
model param (cid:16)eters (cid:17)on their datasets D T1:N, and the second
probabilityp Θ(t) isthepriordistributionofΘ(t).
Likelihoodprobabilityp(cid:16) Dt (cid:12) (cid:12)Θ(t)(cid:17) :Sincethetraining
T1:N
dataarrivesindependentlyamongtheagentsateachtime
Fig.2:Relationshipsamongourlifelongcollaborativelearning t,weassumethatthedatabatchesD(j)
satisfyconditional
anditsrelatedworks:lifelonglearning,federatedlearning,and Ti
independencyonbothagent-levelfor1≤i≤N andtime-
multi-taskrelationlearning.
levelfor1≤k ≤tshowninthefollowingequation:
Theirprimarydistinctionslieintwoaspects.First,multi- N
tasklearningandfederatedlearningfocusesonstatictasks, Agent:p(cid:16) Dt ,Dt ,...,Dt (cid:12) (cid:12)Θ(t)(cid:17) = (cid:89) p(cid:16) Dt (cid:12) (cid:12)Θ(t)(cid:17) ,
T1 T2 TN Ti
whileoursettingtargettasksthatcanevolveovertime.This i=1
d any dna tm rai nc sfle ea rr pn ri en vg ios uce sln yar ei no core uq nu teir re es da lg ee an rnts int go ea xc pc eu rm ieu nl ca et se , Time:p(cid:16) D i(1),D i(2),...,D i(t)(cid:12) (cid:12)θ i(t)(cid:17) = (cid:89)t p(cid:16) D i(k)(cid:12) (cid:12)θ i(t)(cid:17) ,
fostering adaptive learning and collaboration in evolving k=1
(3)
environments. Second, multi-task learning and federated
wherethefirstequationrepresentsagent-levelconditional
learningarecoordinatedbythecentralizedserver,whileour
independenceandtheseconddenotestime-levelconditional
settingutilizesdecentralizedcollaborationmechanisms.This
independence, standing for the task data Dt arrives
decentralizedmechanismrequiresagentstoactivelyseeka T1:N
independentlyamongtheagentsastimeprogresses.Thus
l ti hm anite pd an ssu im veb lyer mof anv aal gu ea dbl be yc to hll eab ceo nra trto alrs seo rn vt eh r.eirown,rather wedecomposethelikelihoodp(cid:16) D Tt 1:N(cid:12) (cid:12)Θ(t)(cid:17) as:
N t
p(cid:16) Dt (cid:12) (cid:12)θ(t)(cid:17) = (cid:89) (cid:89) p(cid:16) D(k)(cid:12) (cid:12)θ(t)(cid:17) , (4)
T1:N i i i
3 OPTIMIZATION FOR DECENTRALIZED AND i=1k=1
LIFELONG-ADAPTIVE COLLABORATIVE LEARNING where
p(cid:16)
D
i(j)(cid:12)
(cid:12)θ
i(t)(cid:17)
is the likelihood corresponding to the
discriminative model for each agent. In (4), the decom-
In this section, we introduce our solution to the problem
position follows from both the agent-level and time-level
showninSection2.InSection3.1weintroducetheoverall
conditionalindependenceasshownin(3).
mathematical optimization. In Section 3.2, we provide an
overviewofouriterativesolution,encompassingthreemain (cid:16) (cid:17)
steps:locallearning,collaborativerelationalinference,and Priorprobabilityp Θ(t) :Tomodelthepriorinformation
lifelongmodelupdate,whosedetailisfurtherprovidedin of Θ(t), we consider the probabilistic approach [30] by
Sections3.3, 3.4,and3.5,respectively. parameterizingthispriordistributionwithpairwisecorrela-
tionsamongmultipleagents’modelparameters.Specifically,
3.1 OptimizationProblem
unlike the agent-level independence of the training data,
Weconsidersolvingthisdecentralizedandlifelong-adaptive thesecorrelationsbetweenagentsdescribetherelationships
collaborationtaskshowninSection2.1fromaprobabilistic between model parameters, which correspond to a graph
perspective.AccordingtothetasksettingdescribedinSec- G(t) = {V,E(t)}. Here V is the node set of the agents and
tion2.1,supposethepreviousencountereddatasetsofagent E(t) is the edge set at timestamp t. The adjacency matrix
iuntiltimetiswrittenasD Tt
i
=(cid:110) D i(k)(cid:12) (cid:12)1≤k ≤t(cid:111) ,whereT
i
o cof llg ar ba op rh atG io( nt) ri es lad tie ofi nn shed ipsas amW o( nt g) t∈ heR aN g× enN ts, ,i un sd ei dca tt oin gg ut idhe
e
representstheuniquelearningtask’sexperienceindexofthe
i-thagent.Eachtimetthecollaborationsystemwillestimate the collaboration behaviors. Note that the collaboration
a group of model parameters Θ(t) = (cid:110) θ i(t)(cid:12) (cid:12)1≤i≤N(cid:111) r Cel (a t)ti ao sns ith sip suW bst( rt) ui cs ture rest .r Tic ht eed wb ey igt hh te oc fo Wmm (tu )n reic pa rt eio sen ng tsra tp hh
e
given the group of the datasets Dt , where Dt =
(cid:8) D Tt i(cid:12) (cid:12)1≤i≤N(cid:9) is the full dataset T fo1: rN agent 1 ≤T i1: ≤N N. m mo od de el l s pi am rail mar eit ty ersbe ptw roe men oteag ae gn et ns. tsA ts osu cm oli ln abg ot rh aa tet ,s wim hi ela rer
Sincethisisadecentralizedlifelongadaptivecollaboration more similar model parameters correspond to stronger
scheme where direct sharing training data is forbidden, collaboration relations, the prior distribution of model pa-
each agent at time t can only access to
p(cid:16) θ(t)(cid:12) (cid:12)D(t)(cid:17)
or rametersΘ(t) givencollaborationrelationshipW(t) andthe
i i5
Fig.3:Thedecentralizedandlifelong-adaptivemulti-agentcollaborativelearningsystem.Thesystemconsistsofthreesteps:local
learning,collaborativerelationalinference,andlifelongmodelupdate.Inthefirststep,eachagentlearnsthemodelparameters
basedontheirownobservationstopreparetheinitializationofmodelparameters.Inthesecondstep,theagentstransmittheir
ownmodelparametersalongthecommunicationgraphandlearnthecollaborationrelation.Inthelaststep,agentssharetheir
modelparametersalongthelearnedcollaborationrelationwiththeircollaboratorsandrefinetheirmodelparameters.Notethe
secondandthirdstepsareiteratedseveraltimesuntilconvergent.
communicationgraphC(t) aredescribedby: Heretheaccumulatedlossofthei-thagentwiththemodel
−logp(cid:16) Θ(t)(cid:17) ∝ 1≤(cid:88) i,j≤NW i( jt)(cid:13) (cid:13) (cid:13)θ i(t)−θ j(t)(cid:13) (cid:13) (cid:13)2
2 (5)
p ℓkar (cid:16)a θm (te )(cid:17)ter =θ −i(t) logis pL
(cid:16)
D( it) (k(cid:16) )θ
(cid:12)
(cid:12)θi(t () t(cid:17) )(cid:17)= isth1 t e(cid:80) lot k s= s1 oℓ fk t(cid:16) hθ ei(t m)(cid:17) o, dw elh θe (r te
)
(cid:16) (cid:17) i i i i
=tr Θ(t)⊤L(t)Θ(t) , evaluatedonthesuperviseddatasetD(k)
.Theedgeweights
(cid:16) (cid:17) i
where W(t) = 0 if C(t) = 0. Here tr Θ(t)⊤L(t)Θ(t) is ofthecollaborationgraphW(t)aresettobepositivewithout
ij ij (cid:16) (cid:17)
the smoothness of Θ(t) viewed as functions on the graph self-loopsbyaddingtheindicatorfunctionI ≥0 W(t) and
and L(t) is the combinatorial graph laplacian defined as theconstraintdiag(cid:16) W(t)(cid:17) =0totheoptimization.
L(t) =D(t)−W(t) attimet.D(t) isadiagonalmatrixwith
Note that: i) to increase the generalization ability for
itsi-thentryrepresentingthedegreeofthei-thnode.
models’ parameter learning, we incorporate the L 2 regu-
Takingthelikelihoodprobabilityshownin (4)andthe larization term associated with parameter λ 1; ii) to ensure
prior probability shown in (5) to the original posterior
probabilityp(cid:16) Θ(t)(cid:12) (cid:12)D Tt 1:N(cid:17) in (2),weobtainadecomposed n sto rn u- ct tr uiv reia Wl s (o t)lu ,t wio en is nta rn od dus cm eo tho eth L 2ed rg ege uw lae ri ig zah tt is onin teg rr map oh f
versionofprobability: W(t) totheoptimizationproblemwithhyper-parameterλ 3;
p(cid:16) Θ(t)(cid:12) (cid:12)D Tt 1:N(cid:17) ∝ (cid:89)N (cid:89)t p(cid:16) D i(k)(cid:12) (cid:12)θ i(t)(cid:17) p(cid:16) Θ(t)(cid:17) . (6) a innd theiii c) ot lo labm oa ri an tt ioai nn ga rac po hn ,s wta ent non ro mrm alia zn ed ths ep ga rr as pe hst Wruc (tt )u tr oe
i=1k=1 hyper-parametermusingL 1 metric.
Thuswecanreformulate(2)asthefollowingnon-constraint
optimizationproblem: 3.2 SolutionOverviewforTheOptimizationProblem
min(cid:88)N (cid:88)t −logp(cid:16) D(k)(cid:12) (cid:12)θ(t)(cid:17) +λtr(cid:16) Θ(t)⊤L(t)Θ(t)(cid:17)
,
AsageneralsolverofProblem(8),F(·) (cid:110)isam (cid:111)Nappingthat
θ i(t) i=1k=1 i i takes both the training data D 1(t :N) = D i(t) i=1 and the
(7) (cid:110) (cid:111)N
where the prior probability distribution of Θ(t) known as learning memories M( 1t :− N1) = M i(t−1) as input and
(cid:16) (cid:17) i=1
p Θ(t) correspondstothegraphsmoothnessregularization thenoutputsthemodelparametersΘ(t) andcollaboration
(cid:16) (cid:17) relationshipsW(t).NotethatthissolverF(·)needstofollow
tr Θ(t)⊤L(t)Θ(t) .
thetasksettingasstatedinSection2.1;thatis,theprocess
In practice, accessing the collaboration relationships oflearningmodelparametersshouldbedecentralizedunder
W(t) amongagentsischallengingduetoi)thedifficulties theconditionofthecommunicationgraphC(t).Mathemati-
of quantifying task similarity; ii) the need to adapt to cally,thesolverF(·)worksasfollows,
dynamically evolving tasks among agents; and iii) the Θ(t),W(t),M( 1t :)
N
=F(cid:16) D 1(t :N) ,M 1(t :− N1)(cid:12) (cid:12)C(t);{λ i}3 i=1(cid:17) , (9)
necessityforadecentralizedandautonomousmechanismto
determinethisstructure.Hence,weincorporatethelearning where{λ i}3 i=1 arehyper-parametersinProblem(8).
of collaboration graph into the optimization problem (7) ThemainchallengeofsolvingProblem(8)istheexistence
(cid:16) (cid:17)
whereagentsactivelychoosetheircollaborators,whichtakes ofthegraphsmoothnesstermtr Θ(t)⊤L(t)Θ(t) .SinceL(t)
bothW(t) andΘ(t) intoconsideration: dependsonW(t),thisterminvolvestheinteractionbetween
min
(cid:88)N L(t)(cid:16) θ(t)(cid:17)
+λ
(cid:13) (cid:13)θ(t)(cid:13) (cid:13)2
+I
(cid:16) W(t)(cid:17) Θ(t) andW(t),causingtheoptimizationproblemtobenon-
Θ(t),W(t) i i 1(cid:13) i (cid:13) 2 ≥0 convex.HenceweleveragealternateconvexsearchtoΘ(t)
i=1
(cid:16) (cid:17) (cid:13) (cid:13)2
andW(t),whichcouldefficientlyfindstationarysolutions
+λ 2tr Θ(t)⊤L(t)Θ(t) +λ 3(cid:13) (cid:13)W(t)(cid:13) (cid:13) (8) forcertainnon-convex(biconvex)optimizationproblems[31].
F
(cid:13) (cid:13) (cid:16) (cid:17) Basedonthisapproach,oursolverF(·)canbedecomposed
s.t. (cid:13)W(t)1(cid:13) =m, diag W(t) =0,
(cid:13) (cid:13) 1 into three parts: i) local learning Φ local(·), which online
W i( jt) =0ifC( ijt) =0, 1≤i,j ≤N. updatesthelearninghistoryM( 1t :)
N
andinitializethemodel6
parametersΘ(t) giventhenewlyobservedtaskdataD(t) ; previous tasks, preventing the computation of gradients
1:N
ii)collaborativerelationalinferenceΦ graph(·),whichmin- frompriortasks,hencedifficultformodelparametertraining.
imizes the objective function by finding the solution W(t) Startingfromthispoint,oncewecanapproximateprevious
with fixed Θ(t); and iii) lifelong model update Φ param(·), functions’gradients,weobtaintheinformationforthecorrect
which optimizes the model parameters Θ(t) with the col- optimization direction of model parameter training. This
laborationrelationsW(t) heldconstant.Mathematically,the approximationdoesnotrequirethedatasamplesfromprior
solutionis: tasks, but merely the gradient information derived from
(cid:16) (cid:17)
Θ(t) ,M(t) =Φ D(t) ,M(t−1);λ , the loss function. Drawing inspiration from the function
init 1:N local 1:N 1:N 1
W(t),k =Φ graph(cid:16) Θ(t),k−1(cid:12) (cid:12)C(t);λ 2,λ 3(cid:17) , a op rdp er rox Ti am yla ot rio en xpt ah ne so ir oy nt[ o32 a] p, pw roe xic mon as teid te hr eu los si sng funa cts ie oc no sn od f-
Θ(t),k =Φ param(cid:16) Θ(t),k−1,W(t),k,M( 1t :) N(cid:12) (cid:12)C(t);λ 1,λ 2(cid:17) , a ing fe on rt ms’ ae tn ioc nou fon rte fr ue td urt eas lk es a, rnw ih ni gch tac so ku s.ldprovidethegradient
for1≤k ≤M.
(10)
Herethesolverbeginswiththelocallearninginitialization
Φ local(·),whichpreparestheinitializationofmodelparame-
Specifically, recall that the definition of
ℓk(cid:16) θ(t)(cid:17)
=
t fe or lls oΘ w( i snt) bit yu ts he ed af lo ter rc no al tla eb co or na vti ev xe sr ee al ra ct hio bn ea tl win ef ee nre Φnc ge ra, pa hn (d ·)t ah ne dn −logp(cid:16)
D
i(k)(cid:12)
(cid:12)θ
i(t)(cid:17)
shown in (8) is the loss
functioni
corre-
Φ param(·)forM iterations. spondtoD(k) formodelparameterθ(t)
.Supposeweperform
Local learning Φ local(·). In this part, the algorithm tries directTayloi rexpansionatonepointi α(k) ofℓk(cid:16) θ(t)(cid:17)
.Then,
to give an initialization of model parameters to launch (cid:16) (cid:17) i i (cid:16) (cid:17)
the iterations between Θ(t) and W(t). This initialization the accumulated loss L( it) θ i(t) = 1
t
(cid:80)t k=1ℓk θ i(t) is
considers previous learning experiences by solving the approximatedby
followingopt mim iniz (cid:88)a Ntio Ln
(
ip t)r (cid:16)ob θl i(e t)m (cid:17):
+λ 1(cid:13) (cid:13) (cid:13)θ i(t)(cid:13) (cid:13) (cid:13)2 . (11)
L( it)(cid:16) θ i(t)(cid:17) ≈ 1
t
k(cid:88) =t 11 2(cid:16) θ i(t)−α( ik)(cid:17)⊤ H( ik)(cid:16) θ i(t)−α( ik)(cid:17)
Θ(t) i=1 2 +(cid:16) θ(t)−α(k)(cid:17)⊤ g(k)+ℓk(cid:16) α(k)(cid:17)
,
NotethatthelearningmemoryM(t) actsasintermediate i i i i
1:N (14)
variables essential for solving this optimization problem. (cid:16) (cid:17)
Rather than using all historical datasets, the size of
M(t) whereH( ik) istheHessianofℓk θ i(t) atα( ik) ,andg i(k) isthe
1:N
correspondinggradientatthesamepoint.Thusthegradient
doesnotincreaseovertime,promptingstorageefficiency.
Collaborative relational inference Φ graph(·). In this part, ofθ i(t) atti (cid:16)met (cid:17)canbeapproximatedas
Wthe (ta )lg go ivr eit nhm fixetr die Θst (o t).so Itlv ae imth se tosu fib n- dop thti em ti az ra gt ei ton cop llr ao bb ol re am tioo nf ∂L( it) θ i(t)
=
1 (cid:88)t H(k)(cid:16) θ(t)−α(k)(cid:17)
+g(k)
∂θ(t) t i i i i (15)
structurebysolvingthefollowingoptimizationproblem: i k=1
minλ
2tr(cid:16) Θ(t)⊤L(t)Θ(t)(cid:17)
+I
≥0(cid:16) W(t)(cid:17)
+λ
3(cid:13)
(cid:13)
(cid:13)W(t)(cid:13)
(cid:13)
(cid:13)2 =A(t)θ i(t)−b( it),
W(t)
(cid:16) (cid:17)
F whereA( it) ,b( it)
aretwointermediatevariablesthatcanbe
s.t.∥W(t)∥ 1 =m, diag W(t) =0, understoodastheli (cid:16)nearsumm (cid:17)ationofhistoricalvariables.
W(t) =0ifC(t) =0, 1≤i,j ≤N, Thesetwovariables
A( it),b( it)
,correspondingtotheafore-
whereI
≥i 0j (cid:16) W(t)(cid:17) isij theindicatorfunctionrequiringed(1 g2 e) m tere mnt sio on fe Hd ( il te )a ,r αni ( in t)g am ndem go i(tr )y aM cco( i rt) d, ic na gn tobe thu ep fd oa lt le od wo inn glin rue li en
:
(cid:104) (cid:105)
weightsofW(t) tobepositive. A( it) = (t−1)A i(t−1)+H( it) /t,
Lifelong model update Φ param(·). Given the optimized b(t) =(cid:104) (t−1)b(t−1)+H(t)α(t)−g(t)(cid:105) /t. (16)
collaborationrelationW(t),thisparttriestofindthemodel i i i i i
parametersΘ(t)bysolvingthecorrespondingnon-constraint Hence online optimization of model parameters Θ(t) can
optimizationproblem: beachievedbycontinuouslyupdatingparameters’gradient
(cid:16) (cid:17)
min(cid:88)N
L(
it)(cid:16)
θ
i(t)(cid:17)
+λ
1(cid:13)
(cid:13) (cid:13)θ
i(t)(cid:13)
(cid:13)
(cid:13)2
+λ
2tr(cid:16) Θ(t)⊤L(t)Θ(t)(cid:17)
.
f su hn oc wti non is n∂L (1( it 5)
)
aθ ni(t d) u/ p∂ dθ ai( tt i) na gcc ro ur ld eing (1t 6o ).th Ce oa np sp idro ex ri im ngat tio hn
e
Θ(t) i=1 2 (13) first-order condition of problem (11), ∂L(t)(cid:16) θ(t)(cid:17) /∂θ(t) +
i i i
anisN mo ,te both that it to ere an tis vu ere stt eh pe sd Φec ge rn aptr ha (l ·i )ze ad ndco Φlla pb aro ar ma (ti ·o )n shm oe uc lh d- 2 fuλ n1 cθ ti( it o) n= (10 5, )w ine tot ta hk ie sfith re st-a op rdpr eo rx ci om na dt ii to ion no af nt dhe obg tr aa id nie thn et
beoperatedunderthecommunicationgraphC(t) constraint.
followingparameterinitializationruleforeachagent.The
Avisualrepresentationoftheoverallsolvercanbefoundin
modelparametersinitializedbylocallearningis
Figure3.Wenowintroduceeachstepindetail. (cid:104) (cid:105)−1
θ(t,0) = A(t)+2λ I b(t). (17)
i i 1 i
3.3 LocalLearning ThefulllocallearningframeworkΦ local(·)isshowninAlgo-
Here we aim to solve the optimization problem shown rithm1.Notethati)allcomputationshappenlocally;andii)
(cid:16) (cid:17)
in (11). Direct optimization of this objective function is thesizeofthelearningmemoryM(t) = A(t),b(t) remains
i i i
impractical due to agents’ inability to access data from
aconstant,whichisstorage-friendly.7
Algorithm1LocallearningΦ local(·) Algorithm2CollaborativerelationalinferenceΦ graph(·)
Inputdata: D 1(t :N) =(cid:110) D i(t)(cid:12) (cid:12)D i(t) =(cid:16) X i(t),Y i(t)(cid:17) ,1≤i≤N(cid:111) Input:Θ(t) =(cid:16) θ 1(t),...,θ n(t)(cid:17)
fori=1,2,...,N (parallel)do fork=0,1,2,...,M do
Initializeexpansionpointα(t) fori=1,2,...,N (parallel)do
H
A(
i( i tt )) == (cid:16)∇ (2 α ti( −t)L 1)(cid:16) Af
(
iθ ti( −t) 1)(cid:16) +X i( Ht)
(
i(cid:17) t), (cid:17)i Y /i( tt)(cid:17) ▷▷ UpC da alc teul ha it se toH ryes Asia
(
itn
)
wuIn k
i
(i tt = )i ,a kl − ,i wz (cid:16)e ′λ: (tz
2
),di0 k(
i
,= t w)+0 ′′(, tz1
)i
,k k≤ − =1i 1≤ h(cid:17) (cid:0)/N u(2 kλ
(cid:1)3
,)
h′(cid:0) uk(cid:1) ,h′′(cid:0) uk(cid:1)
b( it) =(cid:16) (t−1)b( it−1)+H( it)α( it)(cid:17) /t ▷Updatehistoryb( it) xk ii =uk i⊤i w i′(t),ki +w i(t),k⊤(cid:16) 1i −w i′′(t)i ,k(cid:17) i
θ i(t,0) =(cid:104) A( it)+2λ 1I(cid:105)−1 b( it) ▷Calculateparameterθ i(t,0) sk i =(cid:16) 21−w i′(t),k(cid:17)⊤ w i′(t),k
endfor rk =(cid:16) uk−w′(t),k(cid:17)⊤ w′′(t),k
i i i i
yk =sk+rk
i i i
Gatherxk,yk,1≤j ≤N viaC(t) toAgenti
j j
3.4 CollaborativeRelationalInference pk =(cid:80)N xk−m
i j=1 j
qk =−1/λ (cid:80)N yk
i 3 j=1 j
Hereweaimtosolvethecollaborativerelationalinference z ik =z ik−1−pk i/q ik ▷Dualupdatez
i
problem shown in (12). To enable the decentralized calcu- endfor
lation of the collaboration relationships among the agents, endfor (cid:16) (cid:17)
we aim to split the graph adjacency matrix W(t) into N Output:W(t) ⇐ w 1(t),M,...,w N(t),M
blocks.Specifically,similartothereformulationtrickshown
in[26],wedefinetheblockcorrespondtothei-thagentbe
w(t) ∈ RN−1 as the i-th row of the adjacency matrix W(t) condition:
i
exceptthei-thelement.Definetheparameterdistancevector (cid:32)
λ
d(t)+z1(cid:33)
of the i-th agent as d( it) ∈ RN−1. Due to the existence of w i(t) =ReLU − 2 i
2λ
1≤i≤N. (20)
communicationconstraintC(t),weextendthedefinitionof 3
(cid:13) (cid:13)2
d( it) byd( ijt) =(cid:13) (cid:13)θ i(t)−θ j(t)(cid:13) (cid:13) /C( ijt) .Then,anequivalentform However, due to the global constraint ∥W(t)∥ 1 = m, the
ofProblem(12)is 2 Newtoniterationsforz requireglobalinformationatevery
step,compromisingthedecentralizationofthismechanism.
Toaddressthisissue,weproposethateachagentmaintain
min (cid:88)N λ w(t)⊤d(t)+λ (cid:13) (cid:13)w(t)(cid:13) (cid:13)2 +I (cid:16) w(t)(cid:17) alocalcopyofz,denotedasz i forthei-thagent,withthe
(cid:16) w 1(t),...,w N(t)(cid:17) i=1 2 i i 3(cid:13) i (cid:13) 2 x>0 i sameinitialization.Specifically,eachagentdeterminesw i(t)
N based on its own z i. When updating their own z i values,
s.t. (cid:88) 1⊤w(t) =m. agents gather information from their peers based on the
i=1 i communicationgraphC(t) andupdatetheirlocalz i values
(18) accordingly. Note that since the updating rules for z i are
To solve this optimization problem, we use the Lagrange identicalandtheysharethesameinitialization,thevalues
multiplier method to analyze the KKT conditions of the of z i for agents remain consistent throughout the solution
solution w(t) s. This corresponds to solving the following process.
i
single-variableequation Algorithm2showsthedecentralizediterationsamong
(cid:88)N 1⊤ReLU(cid:32) −λ 2d( it)+z1(cid:33)
=m, (19)
t bh oe thag the en ots rei tn icd alet aa ni dl. eW me pf iu rir cth aler asa pn ea cl ty sz ie nA Sl eg co tir oit nh sm 52 anfr dom
6,
2λ
i=1 3 respectively.
where z corresponds to the Lagrange multiplier of the L 1
equalityconstraintand1∈RN−1. 3.5 LifelongModelUpdate
Tofindtherootzofthisnon-smoothfunction,wepropose Hereweaimtosolvethelifelongmodelupdateproblemin
an approximation method√that tries to use a twice differ- (13).Torealizethedecentralizedoptimizationo (cid:104)fΘ(t),weaim
(cid:105)
entiable function h(x) = ( x2+b+x)/2 to approximate tooptimizetheprobleminN blocksΘ(t) = θ(t),...,θ(t) .
1 N
ReLU. This approximation provides two advantages: i) it Foreachblockofthemodelparametersθ(t)
,thefirst-order-
yields acceptable solutions that are easily derived; and i
conditionoftheproblemshownin (13)canbedescribedas
ii) it enhances efficiency, requiring fewer iterations when
alinearequationbytakingthegradientapproximation (15)
employingsecond-orderoptimizationmethods.Weleverage
intoconsideration:
the Newton iterations to (19), which could converge to
the target solution much faster than state-of-the-art graph
A( it)θ i(t)−b( it)
lear Tn oin eg na al bg lo erit th hm
e
s d[ e2 c6 e] n.
tralized computation of
w(t)
for
=−2λ 1θ i(t)−2λ
2(cid:88)N (cid:16)
W i( jt)+W j(t
i)(cid:17)(cid:16)
θ i(t)−θ
j(t)(cid:17)
.
(21)
i
each agent i, the critical part lies in the determination j=1
process of parameter z. This is because once the variable Hence optimizing Θ(t) corresponds to solving a lin-
z isdetermined,eachagentcouldobtaintheircollaboration ear system with N first-order conditions. To obtain the
relationweightsw(t)
independentlyaccordingtotheKKT decentralized computation and reduce the computational
i8
Algorithm3LifelongmodelupdateΦ param(·) Algorithm4DeLAMA
Initialize:(cid:16) A(t),b(t)(cid:17)
,1≤i≤n
Networkparameter:γ =(cid:0) β,{λ i}3 i=1(cid:1)
Input:(cid:16) θ 1(t),0,i ...,i θ N(t),0(cid:17) ,W(t) for Int p= ut1 :, D2, 1(t. :N).. =T d (cid:110)o D i(t)(cid:12) (cid:12)D i(t) =(cid:16) X i(t),Y i(t)(cid:17) ,1≤i≤N(cid:111)
fork=1,2,...,M do /∗UnfoldtheiterationsofΦ local(·) ∗/
fori=1,2,...,N (parallel)do fori=1,2,...,N paralleldo
rs ee cn ed ivl eoc mal odm eo lsd (cid:110)el θθ j(ti( )t ,) k,k −− 11
(cid:12)
(cid:12)jto ∈a Nge (n i)t (cid:111)si fn roN m(i a)
gentsinN(i)
IX(cid:99)
ni
i( tt i) a= lizϕ
eβ
e(cid:16) xX pai( nt) s(cid:17)
ionpointα(t)
θ B(cid:101)i (( tt )), ⇐k ⇐ Ab (t( i )t) ++ (cid:16)4 2λ λ2(cid:80) +j 4∈ λN( Di) (W t)(cid:17)i( jt I)θ j(t),k−1 H( it) = (cid:16)∇2 α( it)L(cid:16) f θ i(t)(cid:16) X(cid:99)i(t)(cid:17) , (cid:17)i Y i(t)(cid:17) ▷Hessian
i i 1 2 i A(t) = (t−1)A(t−1)+H(t) /t ▷UpdateA(t)
θ i(t),k ⇐(cid:16) B( it)(cid:17)−1 θ(cid:101)i(t),k b(ti ) =(cid:16) (t−1)b(t−i 1)+H(t)i α(t)(cid:17) /t ▷Updatebi (t)
i i i i i
endfor
(cid:104) (cid:105)−1
endfor
(cid:16) (cid:17) (cid:16) (cid:17)
θ i(t,0) = A( it)+2λ 1I b( it) ▷Calculateθ i(t,0)
Output: θ 1(t),...,θ N(t) ⇐ θ 1(t,M),...,θ N(t,M) endfor
/∗UnfoldtheiterationofΦ graph(·) ∗/
fork=0,1,2,...,M 1 do
fori=1,2,...,N paralleldo
overhead,weconsiderJacobi-iteration[33]basedapproaches. Initialize:z0,1≤i≤N
(cid:16) i (cid:17)
Specifically,assumingthesymmetryoftheadjacencymatrix uk =− λ d(t)+zk−11 /(2λ )
W(t),werewrite (21)as wi (t),k,w′(t2 ),ki ,w′′(t)i ,k =h(cid:0) uk(cid:1)3 ,h′(cid:0) uk(cid:1) ,h′′(cid:0) uk(cid:1)
i i i (cid:16)i i (cid:17) i
(cid:16)
A(t)+2λ I+4λ
D(t)(cid:17)
θ(t) =b(t)+4λ
(cid:88)N
W(t)θ(t),
xk
i
=uk i⊤w i′(t),k+w i(t),k⊤ 1−w i′′(t),k
i 1 2 i i i 2 ij j (cid:16) (cid:17)⊤
j=1 sk i = 21−w i′(t),k w i′(t),k
(cid:16) (cid:17)⊤
whereD( it) isthedegreeofthei-thagentofthecollaboration r ik = uk i −w i′(t),k w i′′(t),k
relation W(t). Our approach simply modifies this linear yk =sk+rk
i i i
equationintoparameter-updatingrules.Atthekthiteration, Gatherxk j,y jk,1≤j ≤N viaG(t) toAgenti
the model parameter θ(t),k of the i-th agent at time t is pk i =(cid:80)N j=1xk j −m
i qk =−1/λ (cid:80)N yk
updatedaccordingtotherule: i 3 j=1 j
 
z ik =z ik−1−pk i/q ik ▷Dualupdatez
i
θ i(t),k =(cid:16) B( it)(cid:17)−1 b( it)+4λ 2 (cid:88) W i( jt)θ j(t),k−1 , (22) ene dn fd orfo (cid:16)r (cid:17)
W(t) = wM1,...,wM1
j∈N(i) 1 N
where B( it) = A( it) + 2λ 1I + 4λ 2D( it) (assuming B( it)
f/ o∗ rU kn =fo 1ld ,2t ,h .e ..it ,e Mra 2tio dn oofΦ param(·) ∗/
is invertible) and N(i) is the neighborhood of agent i fori=1,2,...,N paralleldo
according to W(t). Hence each agent can independently /∗AggregatemodelparametersaccordingtoW(t) ∗/
updatetheirmodelparametersusingboththeirindividual θ(cid:101)i(t),k ⇐b( it)+4λ 2(cid:80) j∈N(i)W i( jt)θ j(t),k−1
(cid:16) (cid:17)
task information and their collaborators’ parameters. The B(t) ⇐A(t)+ 2λ +4λ D(t) I
i i 1 2 i
Apr lo gc oe rs is tho mfd 3e .centralizedlifelongmodelupdateisshownin θ i(t),k ⇐(cid:16) B( it)(cid:17)−1 θ(cid:101)i(t),k
Note that: 1) the parameter updating rule enables de- endfor
centralizedcomputationofmodelparametersθ i(t) through e On ud tpf uor t:(cid:16) θ(t,M2),...,θ(t,M2)(cid:17)
messagepassingunderalmostO(log(1/ϵ))iterations;2)our 1 N
endfor
function approximation approach shown in (15) does not
require positive definiteness of Hessian
H(t)
compared to
i
online Laplace approximation approaches [34], [35], with
the collaborative learning system in (8) has many hyper-
moreflexibilitysuchastheapproximationpoint.InSection5
parameters,demandingsignificanteffortforsearchingand
we will discuss the best point for Taylor expansion of
choosing
α(t)
for certain kinds of learning tasks and the
tuning hyper-parameters. Third, the algorithm requires
i numerous iterations. Single graph learning and message
convergencepropertyandB(t)
’sinvertibility.
i passingstepsinvolvemanyiterationsandalternatingconvex
searchfurtherrequiresalternateiterationsbetweenthesetwo
4 ALGORITHM UNROLLING FOR DECENTRALIZED
steps.Thesenumerousiterationscausesignificantcommuni-
AND LIFELONG-ADAPTIVE MULTI-AGENT LEARNING cationoverhead,makingthealgorithmlesspracticalforreal
While the iterative algorithm (summarized in (10)) en- applications.
ablesagentstolearnandcollaborate,itencounterscertain To solve these issues, we propose DeLAMA, a power-
challenges that limit its real-world applicability. First, the ful collaborative learning framework suitable for a wide
Taylor expansion in Φ local(·) has potential approximation range of cognitive tasks, combining both mathematical
errors for non-linear functions, limiting the method’s ex- optimization and the learning ability of neural networks.
pressive capability for non-linear learning tasks. Second, Through algorithm unrolling [25], DeLAMA transforms the9
Local Learning Collaborative Relational Inference Lifelong Model Update
:communication graph
......
...
...
Taylor ...
expansion
...
Gather via
Agent
......
Time Layer 1 ... Layer Layer 1 Layer 2 ... Layer
Fig.4:Theunrollednetworkstructureofthecollaborationsystemwithinputdataatoneexacttime.Foreachagent,thetraining
dataisfirstlypassedintoafeed-forwardnetworktobetransformedintoembeddings,andthenthisembeddingisusedtocalculate
the initialized model parameters according to Φ local(·). Then the agents start to communicate and collaborate to find proper
parametersaccordingtotheiterationsshowninΦ graph(·)andΦ param(·).Finally,thenetworkF γ(·)outputsparametersΘ(t).These
outputmodelparametersΘ(t) aresupervisedbythedata(X,Y)∼T itrain.ThetrainingprocessofthenetworkF γ(·)islearningto
tunetheparameterθtofindtheoptimalparameterlearningstrategiesofΘ(t).
iterations of our original optimization solutions into deep Φ graph(·) and Φ param(·) to 1 since the nonlinear operation
neural networks, which enhances the expressive power, correspondingtoΦ param(·)iscapabletofindbetterinitial-
provides automatic learning schemes for hyperparameter- ization of model parameters Θ(t),0, which could simplify
tuning, and further streamlines the learning process by theprocessoffindingcollaborationrelationshipsandmodel
reducingthenumberofalternatingconvexsearchiterations. update, hence requires fewer iterations to converge; and
Generally,DeLAMAembodiesthelearning-to-learnparadigm 2)toautonomouslytunethehyperparameters{λ i}3 i=1,we
by creating a universally applicable collaborative learning leverage the supervised training paradigm of algorithm
strategy, regardless of the training data’s distribution. The unrolling by treating the hyperparameters as learnable
learnablehyper-parameterssignifyameta-levelknowledge parameters.
acquisition,targetingthemasteryofthemodel’sfoundational Toincreasethelearningflexibilityandexpressivepower
mechanisms.Onceestablished,thisadaptivestrategycanbe of the framework, according to the calculate Hessian step
appliedtonewtaskswithoutreconfiguration. of Φ local(·) shown in Algorithm 1, we add an non-linear
In this section, we first describe the structure of the backbonenetworkϕ β withlearnableparametersβ serving
unrolled network, then introduce the learning-to-learn ap- asthefeatureextractor:
proachestotrainthehyper-parametersofthisnetwork.
X(cid:99)i(t)
=ϕ
β(cid:16)
X
i(t)(cid:17)
, (25a)
4.1 UnrollingNetworkDesign (cid:18) (cid:18) (cid:19) (cid:19)
R nie sc mal tl oth lee ac ro nm mp oa dc et ls po alu rat mio en tes rh so gw ivn ei nn tr(9 a) in,w inh gi dch ati as Da (m t)e .c Wha e- H( it) =∇2
α(
it)L f
θ i(t)
X(cid:99)i(t) ,Y i(t) . (25b)
1:N ComparedtothoseiterationsshowninAlgorithm1,the
aimtousethealgorithmunrollingtechniquestotransform(9)
nonlineartransformϕ β(·)usedin (25a)targetstoembedthe
win it to ht lh ee arf no all bo lw ei hn yg pn ere pu ar ra al- mne et tw ero sr γk :-basedunrolledmapping inputdataX i(t) asX(cid:99)i(t) ,andusesthistransformeddataasthe
Θ(t),W(t),M( 1t :) N =F γ(cid:16) D 1(t :N) ,M( 1t :− N1)(cid:12) (cid:12)C(t)(cid:17) . (23) i an dp vu at no taf gt eh se .m Firo sd t,e bl yf θ ii n(t c)( o· r) pi on ra(2 ti5 nb g). tT hehi ns oa np lp inr eo aa rch bao cf kfe br os nt ew oo f
Theinputsarethetrainingdataandtheoutputisthemodel thelocallearningtask,thesharedmodelθ(t)
becomesmore
parametersΘ(t).Thiscanbeunderstoodasusinganeural i
streamlined with a reduced number of parameters. This
networkF γ(·)tolearnthemodelparameterΘ(t),whereγ is
directlyleadstoasubstantialreductionincommunication
mth oe dh ey lp pe ar rp aa mra em tee rt se Θrc (o t)n .trollingthenetworkF γ(·)toobtain
overhead.Second,asimplifiedmodelθ i(t) couldalsosimplify
Specifically,weupgradetheiterativestepsin(10)tothe
thecollaborationmechanismwithinΦ graph(·)andΦ param(·),
unrollednetworklayersassociatedwithF γ(·): makingthecollaborationprocessmoreefficient.
Θ(t),0,M( 1t :)
N
=Φ local(cid:16) D 1(t :N) ,M( 1t :− N1);λ 1,β(cid:17) , (24a)
β,
fI on rsu (m 24m aa )r (y 2, 4fo br
)
( (2 24 4a c) ),w we ea cd hd ana gb eac tk hb eon he yw peit rh pap ra ar mam ete ete rsr
W(t) =Φ graph(cid:16) Θ(t),0(cid:12) (cid:12)C(t);λ 2,λ 3(cid:17) , (24b) {λ i}3 i=1 tolearnableparameters.Eachiterationoftheoverall
framework shown in (10) is unfolded into one specific
Θ(t) =Φ param(cid:16) Θ(t),0,W(t),M( 1t :) N(cid:12) (cid:12)C(t);λ 1,λ 2(cid:17) , (24c) layerofthedeepneuralnetwork.Thefullinferencenetwork
where γ = (cid:0) β,{λ i}3 i=1(cid:1) is the learnable parameters of frameworkofDeLAMAisshowninFigure4andAlgorithm4,
networkF γ(·).Here{λ i}3
i=1
arelearnablehyperparameters withlearnablehyper-parametershighlightedinblue.
andβ isthemodelparameterofaneuralnetwork. 4.2 TrainingDetails
Notethat:1)toobtainaframeworkwithfewercompu- Differentfromtraditionaltrainingpipeline:trainingmodel
tations and less communication overhead, we reduce the parameters, then inference; here with algorithm unrolling,
number of iterations of alternative convex search between thetrainingprocedureofDeLAMAcomprisestwosteps:the10
learning-to-learn step and the model learning step. This procedureF(·)definedin(9),thenumberofcommunication
enablesDeLAMAtocaptureanefficientcollaborationstrategy steps of Φ graph(·) and Φ param(·) is reduced, which further
suitableforvarioustasks.First,inthelearning-to-learnstep, reducesthecomputationalcostamongmultipleagents.
we train the hyperparameters γ in the unrolled network 4.2.3 Agent’smodelinference
F γ(·), optimizing a collaboration strategy. Second, in the GiventheevaluationsetD(cid:101)(t) = (cid:110) X(cid:101)(t),Y(cid:101)(t)(cid:111) attimestamp
model learning step, by executing a forward-pass of F γ(·) i i i
with a fixed hyperparameter, we obtain the output Θ(t), t,agentscalculatethepredictionsaccordingtotheirl (cid:16)earne (cid:17)d
representingtheagents’modelparameters.Throughthis,the modelθ i(t) andtrainedbackboneϕ β(·)byf θ(t) ◦ϕ β X(cid:101) i(t) .
i
advantageisenablingthecollaborationmechanismtolearn Here the model parameters are learned from the process
metaknowledgeonhowtocollaborateunderdifferenttask F γ∗(·)givenfixedparameterγ.
configurations.Aftertraining,eachagentcaninferusingthe
modelwithparameterΘ(t) asusual. 5 THEORETICAL ANALYSIS
4.2.1 Learningtolearnthecollaborationstrategy In this section, we analyze the numerical properties of
Weleveragethelearning-to-learnparadigmtothetraining
DeLAMAshowninAlgorithm4.Specifically,weintroducethe
process of network F γ(·). Specifically, to train an efficient
theoreticalpropertiesofthesolutionsΘ(t),W(t)correspond-
parameterlearningnetworkF γ(·),weuseabunchoftraining ingtoΦ local(·),Φ param(·)andΦ graph(·),includingthebest
1ta ≤sks i, ≤ea Nch ,r te op pre rose vn idte ed sb uy ffiT
c1
it e:r
N
na tin exw ah me pre leT sit ar na din s∼ upP er( vT isi) iof no sr p ofoi tn ht ef io ter rT aa tiy ol no sr ie nxp soa ln vs ii no gn, Θa (n td
)
at nh dec Won (v t)e .rgenceproperties
to tune the parameters γ. Here P(T i) represents the task Best point for Taylor expansion. To minimize the Taylor
distributionofthei-thagent,whichcouldbeusedtosample approximation error, we aim to find the exact expansion
different training task sequences. After this training, the point
α(t)
such that the solution from Taylor expansion
i
collaborationstrategycanbeappliedtotasksT i following θ i(t) is not far from the exact optimal solution point θ i(t)∗ ,
thesamedistribution. whichcorrespondstominimizingtheapproximationerror
N
T
1e
t :r
Nt aw inor ik sts hu ep ee xrv pi es ci to en d. sT yh se ten metw avo er rk agsu ep tae sr kvi ls oio ssn :foronetask | a| pθ pi(t r) o− ximθ ai(t t) i∗ o| n| 2. erD rou re bt eo lonth ge ingdif tfi ocu gelt ny ero af
l
la on sa sly fuzi nn cg tiot nh se
,
ℓ(cid:0) Ttrain(cid:12) (cid:12)γ(cid:1) ourtargetisoptimizinganupperboundoftheoriginalerror:
1:N
= N1 t(cid:88)N (cid:88)t E (Xi,Yi)∼T itrain(cid:104) L(cid:16) f θ i(k)(X i),Y i(cid:17)(cid:105) , m α i(i tn ) E θ i(t)∗(cid:18) tr(cid:16) H(cid:16) α( it)(cid:17)(cid:17)−1(cid:13) (cid:13) (cid:13)α( it)−θ i(t)∗(cid:13) (cid:13) (cid:13)2 2(cid:19) , (26)
i=1k=1
whereL(·)isthetask-specificlossandf θ(k)(·)isthelearned where θ i(t)∗ ∼ N(0,Σ). In Theorem 1, we prove that for
modelwithparameterθ i(k) generatedfromi thenetworkF γ(·) s tru ap ine ir nv gise lod ssle (a mrn eain ng sqp uro arb ele em rrs orw oi rth crs ot sa sn -ed na tr rd ops yu )p ae nrv dis te hd
e
attimestampk.
linear model f (·), we can analytically decide the best
Network training. Training network F γ(·) corresponds to θ i(t)
figuring out the best mechanism to collaborate and learn
pointα( it)
forTaylorexpansion.
modelparametersΘ(t) accordingtothetaskT 1t :r Nain.Mathe- Theorem1.LetL(cid:16) f θ(t)(cid:16) X( it)(cid:17) ,Y i(t)(cid:17) bethestandardsuper-
matically,thismeansminimizingtheexpectedsupervision i
vised training loss function described by mean square
ofthetrainingtasksTtrain accordingtotherule:
1:N error or cross-entropy of the linear model f (·). Sup-
θ(t)
Networkγ e∗ va= lua ar tg iom nγi .n OE nT c1 et :r N ta ri an i∼ nP ed(T ,1 t: hN e)ℓ o(cid:0) pT ti1 mt :r Na ai ln F(cid:12) (cid:12)γ γ(cid:1) ∗(,
·)implic-
p
c
thho ais
t
te
z
1cH ≤on(cid:16) ktα
in
(cid:16)( i ut H) o(cid:17)
u (cid:16)s
α=
w
(t)i∇
(cid:17)th
(cid:17)2 α n(
i
≤t o)L
n
M-(cid:16) zf
e
wθ roi( ht) ec(cid:16)
ro
eX
n
ksi( tt a) is(cid:17)
nt
t, hY
a
eni(t cd) oi(cid:17)
∃ nM
dis itiL
s
oui np
c
as
h
l-
itlycarriespriorcollaborationstrategiesthatcanbeapplied i
(cid:16) (cid:17)
tonewtasklearningconfigurationsfollowingthedistribution numberofH α(t) ,then
i
P(T 1:N).Intheevaluationphase,theevaluationmetricisthe
expectednetworksupervisionlossonthetesttasksT 1t :e Nst:
argminE
(cid:18) tr(cid:16) H(cid:16) α(t)(cid:17)(cid:17)−1(cid:13) (cid:13)α(t)−θ(t)∗(cid:13) (cid:13)2(cid:19)
=0.
L=E T 1t :e Nst∼P(T1:N)ℓ(cid:0) T 1t :e Nst(cid:12) (cid:12)γ∗(cid:1) , Theoremα( i 1t) shθ oi( wt)∗ sthatthebei stTaylo(cid:13) rexi pansii onp(cid:13) o2
intα(t)
whereγ∗ istheoptimalparameterofF γ(·). canbedirectlypredeterminedas0,simplifyingtheprocedui
re
This training approach can be understood as utilizing
to find the ideal expansion point; see Appendix A.1 for
plentyoflearningtasksTtrain asexamplestosupervisethe
1:N detailedanalysisandproof.
networkhowtolearnmodelparameters,andsubsequently ConvergenceofsolvingW(t).Toreducethecommunication
applyingtheacquiredknowledgetonewlearningtasks.
cost and increase the convergence speed of collaborative
4.2.2 Agent’smodellearning relational inference Φ graph(·) shown in Algorithm 2, we
LearningthemodelparametersΘ(t) correspondstothefor- l√everage a continuously differentiable function h b(x) =
wardprocessoftheunrollednetworkF γ∗(·)definedin(23), ( x2+b+x)/2 to approximate ReLU(x). In Theorem 2,
which takes the training data
D(t)
as input and outputs weanalyzetheapproximationerrorbetweenthestandard
1:N
Θ(t).DuringtheforwardpassofF γ∗(·),thefinitelearning optimizationsolutionandtheapproximatedsolution.
memoryM(t−1) isrecurrentlyupdated,dynamicallyadapt- Theorem 2. Let W(t)∗ be the approximated solution of
1:N
ingtofutureobservedtasks.Comparedtotheoptimization Problem18containingN agentswithdualvariablez∗11
satisfying(19).Supposeh b(·),b>0isanapproximation TABLE 2: Performance comparison for the regression prob-
function of ReLU(·). Let the original solution be W(t) lem. We compare our method with representative federated
learningapproachesanddecentralizedoptimizationmethods.
withdualvariablez.Then
√ Wealsoperformanablationstudyofourapproach,including
(cid:13) (cid:13) N b(cid:114) 1 DeLAMA-WC and DeLAMA-WM. The former stands for DeLAMA
(cid:13) (cid:13)W(t)−W(t)∗(cid:13) (cid:13) F ≤ 2 1+ C2, w Deit Lh Ao Mu Atc wo il tl hab oo ur ta lt ii fo en lonam g-o adng apa tg ive ent ls eaw rnh ii nle gt ch ae pl aa btt ile ir tis et sa .ndsfor
whereC isanon-negativelowerboundsatisfiesh′(x)≥
b Setting Method MSEt=1 MSEt=10
C forx∈[z,z∗].
FedAvg[7] 12.9147 10.3229
Theorem 2 shows that the solution of Algorithm 2 can
FedProx[12] 11.8117 10.5989
approximatetheoptimumasbgoestozero,demonstrating SCAFFOLD[37] 13.6683 12.8070
theeffectivenessofourapproach;seeAppendixA.2forthe Federated FedAvg-FT[7] 9.8845 6.7536
learning FedProx-FT[12] 9.4131 6.6035
detailedproof.
Ditto[38] 16.3461 7.9752
Convergence properties of solving Θ(t). Recall that in FedRep[13] 40.0225 11.4844
Problem13,optimizingtheobjectivefunctioncorresponds pFedGraph[39] 8.6481 5.9771
to solving a linear system defined by the formula 21. We
DSGD[40] 23.8660 12.6872
Decentralized
proposeaniterativemethodbasedonJacobiiterations[33] DSGT[41] 17.7407 17.7413
optimization
tosolvethelarge-scalesparselinearequations.Wehavethe DFedAvgM[42] 13.2054 10.4894
followingtheoreticalguaranteeabouttheproposedmethod. Decentralizedand DeLAMA-WC 5.0794 0.0785
Theorem 3. Let Θ(t),k = (cid:16) θ 1(t),k,...,θ N(t),k(cid:17) be the model coll li afe bl oo rn ag ti- va eda lep at riv ne ing De DL eA LM AA M- AWM 2 2. .3 33 37 77 7 2 0. .6 01 75 10 9
parametersinthek-thiterationofAlgorithm2,Θ(t)∗ =
(cid:16) (cid:17)
θ∗(t),...,θ∗(t)
isthetargetsolutionsatisfying(21).For withsignificantnoise.Thisagentcategorizationrequiresthe
1 N
anyϵ,if(cid:13) (cid:13)Θ(t)−Θ(t)∗(cid:13) (cid:13)2
≤ϵ,thenk
∼O(cid:2) log(cid:0)1(cid:1)(cid:3)
.
agenttoconsiderboththefunctioncorrespondenceanddata
(cid:13) (cid:13) ϵ qualityduringcollaboration.
F
Theorem3showsthatmodelparametersobtainedthrough Dataset.Thetotalfunctionnumberrangesrandomlyfrom1
Algorithm 2 can converge to the optimum within a few to3andeachfunctionisensuredtocorrespondtoatleast
iterations,demonstratingthecomputationalefficiencyofour twoagents.Thedatapointsareobtainedbysamplingpoints
approach;seeAppendixA.3fordetailedproof. fromafunctionf(x)byyˆ=f(x)+ϵ,withaGaussiannoise
In summary, we prove that the solution provided by ϵ∼N(0,σ).Thefunctionsarerandomlinearcombinations
DeLAMAcouldconvergetotheoptimumofProblem(12)and ofquadraticfunctionsandsinusoidalfunctions,withdomain
Problem(13). [-5,5].Ateachtime,differenttypesofagents’trainingsets
havedifferentsampleranges:typeoneagents’trainingset
6 EXPERIMENTS
is generated from the full domain [-5, 5], while type two
We evaluate DeLAMA from four aspects, in Section 6.1, we
and type three agents’ training sets are generated from a
firstapplyourmethodtoaregressionproblem;inSection6.2,
randomlimitedintervalwithlength1.Wegenerated600task
wedelveintoimageclassificationtaskswithmorecomplex
learning sequences with different ground truth functions,
dataforms;inSection6.3,weapplyDeLAMAtomulti-robot
with2/3usedfortraining,and1/3usedfortesting.
mapping tasks, representing real scenarios’ performance;
Evaluationmetric.Weadopttwometricsforevaluation:i)
in Section 6.4, we compare the performance of DeLAMA
theaveragemeansquareerrorbetweentheagentregression
withhumans.Ineachaspect,performanceisevaluatedboth
pointsandtheground-truthfunction’spointsateachtime
quantitatively and qualitatively. We also conduct ablation
stampt;ii)thegraphmeansquareerror,whichisdefinedas
studies on DeLAMA to highlight the significance of each
thenormalizeddistancebetweenthelearnedcollaboration
moduleinthecollaborativelearningmechanism.
graphandtheoraclecollaborationgraphattimet:
6.1 RegressionProblem
(cid:13) (cid:13)
6.1.1 ExperimentalSetup (cid:13) (cid:13)W(t)−W oracle(cid:13) (cid:13)
GMSE= F. (27)
Task.Followingthecognitivescientificprinciplesdetailed ∥W ∥
oracle F
in[36],wedesignasystemwith6agentsinthiscollaborative
regression setting, where the collaboration relationship is The oracle graph adjacency matrix W oracle is defined as
easy to verify. We design several non-linear functions to W oracle = m W(cid:102) , with W(cid:102)ij = 1 if agent i and j share a
generatedatapointsandeachagentaccessesasmalldataset
∥W(cid:102)∥1
samefunctionandW(cid:102)ij =0otherwise.Notethattheoracle
ateachtimedrawnfromoneofthenon-linearfunctions.One
graph represents a relatively optimal solution for collabo-
functioncanbeaccessedbymultipleagents.Eachagentisre-
ration since it acquires the agent-function correspondence
quiredtoregressthewholecorrespondingfunction.Sincethe
butmaynotbetheabsoluteoptimalsolutionsincetheagent
datapointsofonesingleagentarenotinformativeenough,
dataqualityisnotconsidered.
correctlycollaboratingwithotheragentscorrespondingto
6.1.2 QuantitativeAnalysis
the same function is essential. Moreover, to increase the
difficultyofthecollaborativerelationshipinferring,agents Hereweaimto:i)verifytheeffectivenessofthedecentralized
arecategorizedintothreetypesbasedondataquality:i)type collaborationmechanisminDeLAMAonregressionlearning
one,agentspossessnumeroussampleswithminimalnoise;ii) performance; ii) evaluate the collaboration graph learning
typetwo,agentshavealimitedsamplingrangewithmedium performance of DeLAMA under different constraints of the
noise; and iii) type three, agents access very few samples communicationgraphC(t).Notethatini)thecommunica-12
TABLE3:Theevaluationofthecollaborationperformancecomparedwithothercentralizedcollaborationmechanisms.DeLAMA
effectivelylearnscollaborationrelationshipsunderdifferentcommunicationstructureconstraints.
G(t) c(G(t)) Method Decentralize GMSEt=1 GMSEt=5 GMSEt=10 MSEt=1 MSEt=5 MSEt=10
NS[43] 1.4198 1.4900 1.4396 56.1900 0.8384 0.3919
GLasso[27] 0.9687 0.9449 0.9540 3.9664 0.1923 0.0817
1 MTRL[14] 72.26 3.955 60.3069 2.6570 0.3009 0.1837
FC GL-LogDet[30] 1.0391 0.8916 0.8331 3.2395 0.1416 0.0726
L2G-PDS[26] 3.6415 1.2675 0.6638 4.4638 0.1633 0.0744
Oracle - - - - 1.1611 0.0847 0.0695
DeLAMA 0.2992 0.0930 0.0721 2.3377 0.1104 0.0719
0.3 1.8291 1.7683 1.7712 3.0061 0.2176 0.0944
ER
0.5 DeLAMA 1.1627 0.9794 0.9736 2.8659 0.1654 0.0850
0.3 1.8244 1.7697 1.7728 2.8893 0.2391 0.0905
BA
0.5 0.9755 0.8475 0.8371 2.7184 0.1781 0.0787
0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5
3 3 3 3 3 3 012345
0 0 0 0 0 0 0 3.0
1
2
2 1
2
2 1
2
2 1
2
2 1
2
2 1
2
2 1
2
22 .. 05
3 3 3 3 3 3 3 1.5
1 1 1 1 1 1 1.0
4 4 4 4 4 4 4
0.5
5 5 5 5 5 5 5
0 0 0 0 0 0 0.0
GL-LogDet GLasso MTRL L2G-PDS NS DeLAMA(Ours) Oracle
Fig.5:Visualizationofthelearnedgraphstructureattimet=3comparedwiththeoraclecollaborationstructure.DeLAMAlearns
smoothandaccurateedgeweightscomparedtobaselinemethods.
tionstructureofDeLAMAissettobefullyconnected,while  7     7     7     7     7   
  
inii)wewilltryotherkindsofcommunicationstructures.   
 
 
 
To verify i), Table 2 presents the system performance  
 
comparisonwithfederatedlearninganddecentralizedopti-                   (   a    )F    un    cti    on   s     lea    rn    ed    by     e   ac    h  a   ge    nt                      
mizationmethods,includingstandardfederatedlearning[7], 0 2T is 1 4 0 2T is 2 4 0 2T is 3 4 0 2T is 4 4 0 2T is 5 4 2.5
[12], [37], personalized federated learning [38], [39] and 0 2.0
1
[40], [41], [42]. The experimental result shows that i) our 2 1.5
decentralized and lifelong collaboration approach signifi- 3 4 1.0
cantly outperforms both federated learning and decentral- 5 0.5
izedoptimizationmethods,reducingtheMSEby98.80%at (b)Timeevolvingcollaborationstructure 0.0
last timestamp t = 10; ii) adding our agent collaboration Fig.6:Acollaborationresultoftwogroupsofagentsthatareper-
formingtwodifferentregressiontasks.(a)Thelearnedfunctions
remarkablyimprovesthelearningperformance,especially
after collaboration for each agent. (b) The time-evolving col-
whentimestampissmallthateveryagentlacksdata;iii)as laborationgraphofthiscollaborationsystem.DeLAMAenables
timestampincreases,thelearningperformanceimprovement thecollaborationsystemtoevolvewithdynamiccollaboration
broughtbyourlifelong-adaptivelearningenlarges,reflecting relationships.
theeffectivenessofourlifelong-adaptivelearningdesign.
6.1.3 QualitativeAnalysis
Toverifyii),weperformDeLAMAunderdifferentcommu-
Here we aim to i) verify that the system of DeLAMA could
nicationstructures,includingthefullyconnected(FC)graph,
dynamically evolve as time increases, and ii) compare the
the Erdos-Renyi(ER) graph, and the Barabasi Albert(BA)
performances of our collaboration relationship inference
graph [44]. The compared approaches contain both classic
withexistinggraphlearningapproaches,includingNS[43],
relationlearningmethods[14],[27],andrecentwidelyused
GLasso[27],MTRL[14],GL-LogDet[30]andL2G[26].
structurallearningapproachesL2G[26].Wecomparethese
approaches by substituting Φ graph(·) of DeLAMA to other Toshowi),wevisualizethecollaborativelearningresults
relational learning methods while preserving other parts ofeachagentfromtimestamp1to5,andthecorresponding
of DeLAMA stay unchanged. To achieve a fair comparison, collaborativerelationships.Figure6illustratesthecollabo-
we investigate the system performance under the same rationlearningresultsoftwogroupsamongsixagents.The
connectivity level of these two types of random graphs. visualizationsshowthatagentscanfindcorrectcollaborators
Here the connectivity level
c(cid:16) G(t)(cid:17)
of the communication
wheretheconnectionformstwogroupsofagents,andrefine
thetime-evolvingcollaborationstructuresastimeincreases.
graph adjacency matrix C(t) ∈ {0,1}N×N is defined as:
Thevisualizationoftheirlearnedfunctionsshowsthatthey
(cid:16) (cid:17)
c G(t) = ∥C(t)∥ 1/N2. Table 3 presents the comparison canlearnfrompastencountereddatasamples.
with previous relational learning methods on both the To show ii), we visualize the system collaboration re-
structuralcorrectnessandthetaskperformance.Theresults lationship learned by different approaches. To achieve a
showthati)DeLAMAoutperformsbothtraditionalcentralized faircomparison,wecalculatethecollaborationrelationship
anddecentralizedstructurallearningmethods;ii)DeLAMA for the group of agents at the same time t = 3. The
matches the performance of recent structural learning per- visualizationofcollaborationrelationshipsshowninFigure5
formanceL2G[26];andiii)DeLAMAstillsworksevenunder demonstrates that DeLAMA could discover more suitable
lowconnectivitylevel. collaborationrelationscomparedtootherapproaches.13
TABLE4:Thesystemperformanceanalysisonclassificationtasksoffourdifferentsettings:Lifelonglearning,federatedlearning,
decentralizedoptimization,decentralizedandlifelong-adaptivecollaborativelearning.DeLAMA-WCstandsforDeLAMAwithout
collaborationamongagentsandDeLAMA-WMstandsforDeLAMAwithoutlifelong-adaptivelearningcapabilities.
MNIST CIFAR-10
Setting Method
Acct=1 Acct=5 Acct=10 Acct=1 Acct=5 Acct=10
LWF[28] 20.17 26.49 32.34 20.00 20.00 20.00
Lifelong EWC[45] 20.10 26.64 31.47 20.00 20.00 20.01
MAS[46] 20.10 26.64 31.47 20.00 20.00 20.00
learning
GEM[29] 20.00 25.41 49.84 20.00 25.94 32.49
A-GEM[47] 20.00 20.94 22.47 20.00 20.38 20.07
FedAvg[7] 43.09 51.76 59.50 19.94 20.84 23.88
FedProx[12] 49.12 56.17 60.28 21.70 23.74 26.32
SCAFFOLD[37] 48.04 56.14 64.44 21.09 23.78 26.18
Federated
FedAvg-FT[7] 27.36 50.27 59.10 20.00 20.34 23.02
learning FedProx-FT[12] 36.07 53.46 58.87 20.00 20.33 22.51
Ditto[38] 37.80 51.63 59.41 20.04 20.29 22.32
FedRep[13] 20.00 20.57 23.23 20.00 20.00 20.00
pFedGraph[39] 20.00 26.09 34.79 20.00 20.93 22.23
Decentralized DSGD[40] 20.10 20.52 22.61 20.02 20.02 20.00
DSGT[41] 22.42 21.34 22.81 19.94 20.02 20.05
optimization
DFedAvgM[42] 36.91 51.37 54.91 19.97 21.30 22.38
Decentralizedandlifelong- DeLAMA-WC 37.15 78.92 95.54 30.96 57.81 71.46
DeLAMA-WM 67.67 66.67 67.10 46.87 46.07 45.32
adaptivecollaborativelearning DeLAMA 67.67 98.03 99.51 46.87 72.57 76.03
TABLE5:Thesystemcollaborativelearningperformanceofdifferentcollaborationrelationshiplearningmethods.HereOracle
meansweusegraphstructurewhereagentswiththesameobservationconfigurationcollaboratetogether.Theevaluationmetric
containsboththegraph-levelGMSEmetricagainsttheOraclestructureandthesystemperformanceontheevaluationset.
Data Method Decentralize GMSEt=1 GMSEt=3 GMSEt=5 Acct=1 Acct=3 Acct=5
NS[43] 0.9999 2.3562 1.3772 52.83 83.13 95.81
GLasso[27] 1.7585 0.2728 0.1365 38.52 81.89 95.88
MTRL[14] 2.1181 0.3179 0.1444 39.36 79.61 97.28
MNIST GL-LogDet[30] 7.0086 0.9999 1.0000 44.93 65.43 79.93
L2G-PDS[26] 0.5211 0.2970 0.1391 68.32 91.96 97.30
Oracle - - - - 69.28 92.83 98.22
DeLAMA 0.4998 0.1968 0.0855 67.67 92.02 98.03
NS[43] 1.9756 4.0367 2.321 38.36 54.28 66.19
GLasso[27] 14.3883 1.3897 0.3823 24.41 55.39 69.74
MTRL[14] 375.2 2.0157 0.4352 25.36 53.23 64.88
CIFAR-10 GL-LogDet[30] 1.0000 1.0033 1.0109 35.52 48.51 58.23
L2G-PDS[26] 0.5067 0.3226 0.2065 45.63 65.03 72.06
Oracle - - - - 48.21 68.51 74.16
DeLAMA 0.5948 0.3089 0.1475 46.87 65.19 72.57
6.2 ImageClassification tion mechanism in DeLAMA on classification task learning
6.2.1 ExperimentalSetup performance; ii) evaluate the collaboration graph learning
performanceofDeLAMAonclassificationtasks.
Task.Weconsideracollaborationsystemwith6collaborators.
Toencouragethecollaborationbehaviorsbetweentheagents, To verify i), Table 4 presents the system comparison
wedividethesystemofagentsintotwogroupswhereeach withlifelonglearning[28],[45],[47],federatedlearning[7],
group’s agents are all doing one 5-class classification task. [12]anddecentralizedlearningmethods[40],[41],[50].The
Weadopttheclassincrementallearningparadigmintoour experimental result shows that i) our decentralized and
experiment,whereeachtimeagentsrandomlyaccesspurely lifelongcollaborationapproachcouldeffectivelyremember
onesingleclassofdatadrawnfromthefiveclasses. previouslylearnedtrainingdata,significantlyoutperforms
Dataset.OurexperimentsareperformedonboththeMNIST classiclifelonglearningapproachesby99.66%and134.01%
dataset [48] and the CIFAR-10 dataset [49]. We create the onMNISTandCIFAR-10att=10,showingtheeffectiveness
5-classclassificationtasksbysamplingimagesfromthefull ofourlifelonglearningmechanism;ii)comparedwithother
MNIST or CIFAR-10 dataset. The 5 classes are randomly multi-clientanddecentralizedtrainingapproaches[7],[39],
chosenfrom0to9,eachtimeagentswillaccessonesingle [50]thatusesastaticcollaborationgraph,ourdecentralized
classdatasample.Eachtimethesamplingsizeofthetrainset collaborationsignificantlyimprovestheclassificationaccu-
is10andthecorrespondingtestsetsizewithallfiveclassesis racy by 54.42% and 188.87% on MNIST and CIFAR-10 at
50.Toenablealgorithmunrolling,wecreatedseverallifelong t=10,showingtheeffectivenessofourdynamiclearningof
collaborationlearningtaskswith70%fortrainingand30% collaborationgraph;iii)addingouragentcollaborationand
fortesting.HereN =6withmaximallearningtimeT =10. remarkablyimprovesthelearningperformance,especially
Metric.Weadopttwometricsforevaluation:1)theaverage whenalltheagentslackdataatthebeginning.
predictionaccuracyforeachagentmeasuredonthetestset.
To verify ii), Table 5 presents the comparison with
2)thegraphmeansquareerrorshownin (27).
otherclassicrelationshiplearningapproaches[26],[27],[43],
6.2.2 QuantitativeAnalysis
includingcentralizedmethods[14],[30]anddecentralized
Decentralized collaboration Here we aim to: i) verify the approaches[43].Wecomparetheseapproachesbysubstitut-
effectiveness of the decentralized and lifelong collabora- ingΦ graph(·)ofDeLAMAtotheserelationallearningmethods14
0 O 2racle 4 0 T 2 is 1 4 0 T 2 is 3 4 0 T 2 is 5 4 0 T 2 is 7 4 2.5 Truth Map DeLAMA DeLAMA-WC DSGD DSGT DiNNO
0
1 2.0
2 1.5
3
4 1.0
5 0.5
(a)CollaborationrelationshipsofMNIST
0.0
Oracle T is 1 T is 3 T is 5 T is 7 Fig.8:Thesimulationresultofrobotmappingatt=12based
00 2 4 0 2 4 0 2 4 0 2 4 0 2 4 2.5 on the CubiCasa5k room Dataset. DeLAMA outperforms other
1 2.0 decentralizedcollaborativemodeltrainingapproaches.Black:
2 1.5
3 emptyspace.White:occupiedspace.
4 1.0
5 0.5
(b)CollaborationrelationshipsofCIFAR-10 CubiCasa5k dataset [51]. In this environment exploration
0.0
Fig.7:Exampleoftheevolvingcollaborationgraphs.Thesystem task, robots are placed randomly in the room and set a
tends to assign a relatively weak collaboration relationship manuallypredefinedexplorationtrajectory.Eachrobotcan
between all the agents, then cut off the undesired ones, and have a local view of the environment based on LiDAR
finally,agentscancorrectlyfindusefulcollaborators. samples along the trajectory and cannot visit every part
of the room. The goal of this task is to realize the efficient
whilepreservingotherpartsofDeLAMAstayunchanged.The
andaccuratemappingforeachagentthroughcollaboration.
resultsshowthati)DeLAMAoutperformsclassicrelationship
Dataset. The room structure is generated from the Cubi-
learningalgorithmsinclassificationtaskswithasignificant
Casa5kdataset[51].Toimprovethelearningtaskcomplexity,
reduction in the graph structure inference error of 38.53%
weincreasedthewall thickness,removed somesmallsub-
and28.57%,reflectingthatourmethodaccuratelylearnsthe
structures such as doors, and balconies, and added some
collaborationgraph;ii)theaccuratelearningofcollaboration
substructuresintotherooms.TheLiDARscanissimulated
graphfurtherleadstotheimprovementoftaskperformance
bysimpleraytrackingmethodsandtheangularresolution
(classification).Ourmethodoutperformsclassiccollaboration
is one degree. Each time the LiDAR scan can create a
relation learning methods and is very close to the oracle
local observation from each agent, serving as a dataset
methodthatutilizestheground-truthcollaborationgraph;
containingfeaturesofeachscannedpoint.Thefeaturesare
iii)thecollaborationrelationshipslearnedbyDeLAMAcould
representations of different types of sensor signals with
dynamicallyimprovewithtimeincreasesfromthesystem
environmental noise. In this task, the sampling duration
performanceperspective.
contains12timestampsand5collaborativeagents.Weuse
Lifelong adaptation Here we aim to: i) verify the lifelong
70%ofthetotalroomstructuresfortrainingandtherest30%
learningcapabilityfromthesingle-agentlearningperspec-
roomsfortesting.
tive;ii)verifythatDeLAMAislifelong-adaptivecomparedto
Implementation.Wetreatthistaskasabinaryclassification
thestaticcollaborationapproaches.
task.Eachpointiseitheroccupiedorempty,whichcanbe
Toverifyi),wecompareDeLAMA-WCwithseverallifelong
regardedasapositivelabeloranegativelabel.Thedetected
learning approaches with the concern of fair comparison.
mapofeachagentisindeedbinaryandcanbeevaluatedby
Table4showsthatevenwithoutcollaboration,DeLAMA-WC
variousclassificationmetrics.
outperformsstandardlifelonglearningapproaches.
Metric.Forregularrooms,occupiedpartsaremuchlessthan
To verify ii), we compare the system performance of
DeLAMAwithotherstaticcollaborationapproachessuchas empty parts. Hence we utilize the F1 score of the binary
classificationtaskasthemetrictorepresenttheunbalanced
federated learning [7], [12] and collaboration relationship
classificationtask’sclassificationpower.
learning [26], [27], [43]. The performance compared with
federatedlearningandmultitasklearningshowninTable4 6.3.2 QuantitativeandQualitativeAnalysis
and Table 5 reveals that i) DeLAMA has lifelong-adaptive
In this section, we show that DeLAMA could be applied
capabilitiesratherthanstaticcollaborationapproaches;ii)as
to multi-robot mapping scenarios. Specifically, we show
timestampincreases,thelearningperformanceimprovement
that our decentralized collaborative mapping mechanism
broughtbyourlifelong-adaptivelearningenlarges,reflecting
outperformspreviousdecentralizedoptimizationandsingle-
theeffectivenessofourlifelongadaptivelearningdesign.
6.2.3 QualitativeAnalysis agentmappings.
Table6showsthecomparisonwithpreviousdecentral-
Here weaimto verifythatthelearning systemofDeLAMA
izedoptimizationmethods[22],[40],[41].Weseethati)our
coulddynamicallyevolveasthetimestampincreaseswhen
collaborationlearningstrategyoutperformspreviousdecen-
facedwithclassincrementallearningtasks.Figure7visual-
tralized optimization methods and significantly improves
izesthetime-evolvingcollaborationstructuresproducedby
theefficiencyandperformanceofenvironmentalexploration;
DeLAMA.Comparedwiththeoraclestructure,theresultson
ii) compared to DeLAMA-WC that is an individual agent
MNISTandCIFAR-10datasetshowthati)thecollaboration
mappingwithoutcollaboration,addingagentcollaboration
relationshipsareevolvingaccordingtoagents’observations;
remarkablyimprovestheexplorationperformance.Figure8
ii)agentstendtocollaboratewithmoreagentsatthebegin-
illustrates that i) DeLAMA outperforms other decentralized
ning,anddeletelessusefulrelationshipsastheirobservation
modeltrainingapproaches,ii)comparedtoasingleagent’s
grows,whichisareasonablecollaborationstrategy.
limitperceptionrange,multi-agentcollaborationcoulddis-
6.3 Multi-RobotMapping
cover a much more complete room structure. According
6.3.1 ExperimentalSetup
to the visualization, the detection range and classification
Task.Weconsideracollaborativelyrobot2Dmappingtask
accuracyoftheagentareimprovedwiththehelpofmulti-
with5robots.Theroom structuresaregeneratedfromthe
agentcollaboration.15
TABLE 6: The agents’ average F1-score of the multi-robot
mapping task. DeLAMA outperforms other decentralized col-
laborativemodeltrainingapproaches.
Method F1t=1 F1t=6 F1t=12
DSGD[40] 13.44 43.90 45.75
DSGT[41] 38.32 38.31 38.31
DiNNO[22] 14.73 20.34 25.57
DeLAMA-WC 68.50 72.66 74.92
DeLAMA 72.11 88.52 95.31
TABLE 7: The average performance of human collaboration Fig.9:Thehumanevaluationofcollaborationrelationshipswith
comparedwithDeLAMA.Weanalyzedthesystemperformance eachother.x-axis:humanratingscores,rangingfrom0(lowest
att=1,2,3. collaboration) to 5 (highest collaboration). y-axis: logarithm
of model parameter distance. From the first to last column:
Method Quality MSEt=1 MSEt=2 MSEt=3
different data quality human evaluation score distribution
Low 268.9874 8.4663 0.7053 (highesttolowest).Thereexistnegativecorrelationsbetween
Human Medium 20.8548 1.2173 0.0859 agents’modelparametersandhumancollaborationstrength.
High 0.0503 0.0183 0.0270
Low 246.9219 4.1179 0.6533 correspondingagents’differences.Fromthedefinitionofcol-
DeLAMA Medium 22.8994 0.6001 0.0600 laborationrelationdefinedinDeLAMA(20),thecollaboration
High 0.0320 0.0120 0.0112
strengthisnegativelycorrelatedwiththedistanceofmodel
parameters.Hereweaimtoverifythathumancollaboration
6.4 HumanInvolvedExperiment
still satisfies this property. Specifically, we substitute one
6.4.1 ExperimentalSetup agentinDeLAMAintoahumanandcomparetherelationship
Purposes.Herewedesignedahumanexperimentformulti- betweenlearnededgeweightsandparameterdistances.To
agentcollaboration.Itaimstoverifywhetherhumantends quantify the collaboration strength between humans and
to collaborate with other people with similar knowledge, collaboratedmachines,weaskhumanstoprovidetheirrating
whichcorrespondstothegraphsmoothnessmodelingshown scores (0 to 5, a higher score means a higher collaboration
in (5). We created a cognitive learning task formed by a relationship) against other agents. Figure 9 demonstrates
collaborativelearninggametoinvestigatethefollowingtwo therelationofhumanratingscoresandlearnedregression
problems: modelparameterdistancesunderdifferenttaskdataquality,
•Whethertheproposedalgorithmobtainsacompatible where the y-axis is the logarithm of the distance defined
(cid:16) (cid:17) (cid:16)(cid:13) (cid:13) (cid:17)
taskperformancecomparedtohumansincognitivelearning aslog D(t) =log (cid:13)θ(t)−θ(t)(cid:13) .Fromthehumanscore
ij (cid:13) i j (cid:13)
scenarios. distribution,weseethat1)thecolla2 borationrelationstrength
•Doeshumancollaborationsharethesamemechanism
is negatively correlated with the distance between the
to find collaborators like the proposed graph learning agents’ model parameters, which verifies our assumption
algorithm? that humans tend to collaborate with similar ones; and 2)
Task. To demonstrate that DeLAMA’s collaboration mecha- forhardercollaborationproblemswithlowerdataquality,
nism mirrors human cognitive collaboration, we adopt a humanshavedifficultyindistinguishingusefulcollaborators.
linear regression task as Section 6.1 where each human Thissuggeststhatthemechanismsofhumancollaboration
participant represents one system agent. Here, to promote arenotmerelybasedonthesimilaritybetweenindividuals.
efficienthuman-machineinteraction,wecreateawebapplica- There may exist some more complex logic and structures,
tionwithauser-friendlyGUI.Fromthehuman’sperspective, such as complex gaming relationships and higher-order
i)thelinearregressionprocedureisperformedbasedontheir interactions.
visualobservationandmemoryofpreviouslyencountered
datapoints;ii)theinteractionbetweenthehumanparticipant 7 RELATED WORK
andotheragentsoccursonthevisuallevel,wherethehuman
7.1 Multi-AgentCommunication
can view the agents’ regression outcomes and refine their
regressionresultsbydrawingontheGUIbasedontheeval- Multi-agentcommunicationenablesagentstosendmessages
uationofotheragents’results;seedetailsinAppendixB.4. to each other to realize inter-agent collaboration. The key
6.4.2 QuantitativeAnalysis partofmulti-agentcommunicationliesinthedesigningof
inter-agentcommunicationstrategy[52].Traditionalmethods
Toanswerthefirstquestion,weaimtoanalyzetheaverage
apply predefined features or heuristics [53], [54] to design
performance of humans compared with DeLAMA. We set
communication protocols. More recently, learning-based
the number of agents n = 4 with 3 timestamps in total.
methods have been proposed [55], which train the com-
Wesetthetwodifferenttargetregressionlinesbetweenthe
munication protocols end-to-end under task performance
agents, which corresponds to two possible collaboration
supervision.Theseapproachesareoftenappliedinpercep-
groups.Weconductedthreedifferentlevelsofdataquality.
tionordecision-making,suchascollaborativeperceptionor
ThequalitativeresultsshowninTable7revealthatDeLAMA
multi-agentreinforcementlearning(MARL).Incollaborative
outperforms human collaboration with a lower regression
perception, current methods mainly focus on designing
erroratvariousdataqualities.
communication strategies [8] or finding the exact time to
6.4.3 QualitativeAnalysis share[56].Inmulti-agentreinforcementlearning[18],studies
To answer the second question, we aim to compare the have explored various communication approaches, from
correlationsbetweenthecollaborationrelationshipsandthe simply sharing sensations, and policies to sharing other16
abstract data embeddings,like SARNet [17], TarMAC [16], GeneralizedFLaimstotrainaglobalmodelthatgeneral-
andIMAC[57], izestodatasetsofallagents[7].Tohandledataheterogeneity,
Our method also focuses on communication between FedProx[12]andSCAFFOLD[37]proposetoalignagents’
collaborativeagents,butthedifferencesare:i)Ourcommu- modelsinparameterspace;whileFedFM[69]alignsagents’
nication strategy is derived from model-based numerical feature space. FedAvgM [50] and FedOPT [70] introduce
optimization problems with theoretical guarantees, rather momentum for updating global model. FedNova [71] and
thanend-to-endtraining.ii)Ourresearchmainlyfocuseson FedDisco[72]adjusttheaggregationmanneroftraditional
cognitive-levelcollaboration,ratherthanperception-levelor FL. Personalized FL aims to train multiple personalized
decision-making-levelcollaborations. models for agents’ individual interests [73]. Addressing
data heterogeneity, Ditto [38] and pFedMe [74] propose
7.2 LifelongLearning
proximal regularization on parameter space. FedPer [75]
Lifelonglearning[23]aimstotrainacontinuouslylearning andFedRep[13]splitthewholemodelintotwoshallowand
agent who can memorize previously learned tasks and deeppartsandkeepthedeeppartlocalized.CFL[76]applies
quickly adapt to new training tasks. Optimizing model clusteringontheparticipatingclientsandpFedGraph[39]
parametersdirectlyeachtimemaysufferfromcatastrophic infersacollaborationgraphamongclientstopromotefine-
forgetting[58].Toovercomethisissue,currentapproaches grainedcollaboration.
canbecategorizedintothreeaspects:regularization-based, Unlikethesemethodsthatarecoordinatedbyacentral
rehearsal-based,andarchitecture-increasing-basedmethods. server, our proposed method finds the model parameters
Regularization-basedmethodslikeEWC[45],PathInt[59], inadecentralizedmannerunderthelearnedcollaboration
and MAS [46] aim to estimate the important parts of graph structure. The model parameters are passed via the
model parameters and keep these parts changing slowly linkonthegraphandtheaggregationruleforeachagentis
in model training, other methods like LWF [28], target to formulatedbytheoreticaldeduction.Thismessage-passing
keep previously learned data representations unchanged learning framework is much more robust and each agent
whilelearningnewtasks.Rehearsal-basedmethodssuchas couldlearnthemodelaccordingtotheirowntaskswithout
iCaRL[60],EEIL[61],andGEM[29]useprevioustasks’data theconstraintoftheglobalmodel.
samples in real-time task learning, others like [62] utilize
8 CONCLUSION
generativemodelstogeneratetaskdatasamples.TAMiL[63]
combinesrehearsalmethodswithregularizationapproaches In this paper, we propose a novel decentralized lifelong-
together. Architecture-increasing methods like [64] try to adaptivecollaborativelearningframeworkbasedonnumer-
utilizenewtaskmodelparameterstopreventcatastrophic icaloptimizationandalgorithmunrolling,namedDeLAMA.
forgetting,requiringrelativelylargememorycapacities. Itenablesmultipleagentstoefficientlydetectcollaboration
Ourmethodalsoaimstofindanapproachtoovercome relationshipsandadapttoever-changingobservationsdur-
catastrophicforgettingandallowagentstomemorizelearn- ingindividualmodeltraining.Wevalidatetheeffectiveness
ing experiences, but the differences are: i) The application ofDeLAMAthroughextensiveexperimentsonvariousreal-
scenario is a group of collaborative agents, rather than a world and simulated datasets. Experimental results show
singleagent,allowingcollectivephenomenontocontributeto thatDeLAMAachievessuperiorperformancescomparedwith
lifelonglearning.ii)Ourmethodologyisbasedonalgorithm othercollaborativelearningapproaches.
unrolling,whichisdifferentfromthosethreeclassiclifelong
ACKNOWLEDGMENTS
learningmethodologies.
This research is supported by the National Key R&D Pro-
7.3 DistributedMulti-TaskLearning gram of China under Grant 2021ZD0112801, NSFC under
Distributed multi-task learning [15] aims to train several Grant 62171276 and the Science and Technology Commis-
models in a distributed manner. Classic methods such as sion of Shanghai Municipal under Grant 21511100900 and
DSML[15],DMTL[14],[65],RMTL[66],AMTL[67]proposed 22DZ2229005.
distributedmodeltrainingalgorithmsbasedonnumerical
optimization, where most approaches are limited to linear REFERENCES
models.EarlyworkssuchasRMTL[66]andanalysisonthe
kernelformulti-tasklearning[68]puteffortsintodesigning [1] A. W. Woolley, C. F. Chabris, A. Pentland, N. Hashmi, and
T.W.Malone,“Evidenceforacollectiveintelligencefactorinthe
efficientkernelsformulti-tasklearning.
performance of human groups,” science, vol. 330, no. 6004, pp.
Our method also aims to distributively train several 686–688,2010.
models, but the differences are: i) The model training [2] E. A. Mennis, “The wisdom of crowds: Why the many are
smarterthanthefewandhowcollectivewisdomshapesbusiness,
algorithm is fully decentralized without the central server.
economies,societies,andnations,”BusinessEconomics,vol.41,no.4,
ii) We consider time-evolving collaboration relations with
pp.63–65,2006.
adaptivecapabilities,ratherthanstatictaskdistributionand [3] C.Jung,D.Lee,S.Lee,andD.H.Shim,“V2x-communication-aided
collaborationrelationships. autonomousdriving:Systemdesignandexperimentalvalidation,”
Sensors,vol.20,no.10,p.2903,2020.
7.4 FederatedLearning [4] Y.Li,D.Ma,Z.An,Z.Wang,Y.Zhong,S.Chen,andC.Feng,“V2x-
sim:Multi-agentcollaborativeperceptiondatasetandbenchmark
Federatedlearning(FL)[7]aimstoenablemultipleagents for autonomous driving,” IEEE Robotics and Automation Letters,
collaboratively train models under the coordination of a vol.7,no.4,pp.10914–10921,2022.
[5] W.Burgard,M.Moors,C.Stachniss,andF.E.Schneider,“Coordi-
centralserver.TraditionalFLcanbedividedintotwoaspects:
natedmulti-robotexploration,”IEEETransactionsonrobotics,vol.21,
generalizedFL[7],[12]andpersonalizedFL[13],[38]. no.3,pp.376–386,2005.17
[6] W. Burgard, M. Moors, D. Fox, R. Simmons, and S. Thrun, [29] D. Lopez-Paz and M. Ranzato, “Gradient episodic memory for
“Collaborativemulti-robotexploration,”inProceedings2000ICRA. continuallearning,”Advancesinneuralinformationprocessingsystems,
MillenniumConference.IEEEInternationalConferenceonRoboticsand vol.30,2017.
Automation.SymposiaProceedings(Cat.No.00CH37065),vol.1. IEEE, [30] X.Dong,D.Thanou,P.Frossard,andP.Vandergheynst,“Learning
2000,pp.476–481. laplacian matrix in smooth graph signal representations,” IEEE
[7] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. TransactionsonSignalProcessing,vol.64,no.23,pp.6160–6173,2016.
yArcas,“Communication-efficientlearningofdeepnetworksfrom [31] J. Gorski, F. Pfeuffer, and K. Klamroth, “Biconvex sets and op-
decentralizeddata,”inArtificialintelligenceandstatistics. PMLR, timization with biconvex functions: a survey and extensions,”
2017,pp.1273–1282. Mathematical methods of operations research, vol. 66, pp. 373–407,
[8] Y. Hu, S. Fang, Z. Lei, Y. Zhong, and S. Chen, “Where2comm: 2007.
Communication-efficientcollaborativeperceptionviaspatialcon- [32] W.Rudin,“Realandcomplexanalysis.1987,”Citedon,vol.156,
fidence maps,” Advances in neural information processing systems, p.16,1987.
vol.35,pp.4874–4886,2022. [33] G.H.GolubandC.F.VanLoan,Matrixcomputations. JHUpress,
[9] J.Zhao,X.Xie,X.Xu,andS.Sun,“Multi-viewlearningoverview: 2013.
Recentprogressandnewchallenges,”InformationFusion,vol.38, [34] F. Husza´r, “Note on the quadratic penalties in elastic weight
pp.43–54,2017. consolidation,”ProceedingsoftheNationalAcademyofSciences,vol.
[10] A. Hogan, E. Blomqvist, M. Cochez, C. d’Amato, G. D. Melo, 115,no.11,pp.E2496–E2497,2018.
C.Gutierrez,S.Kirrane,J.E.L.Gayo,R.Navigli,S.Neumaieretal.,
[35] T.-C.Kao,K.Jensen,G.vandeVen,A.Bernacchia,andG.Hen-
“Knowledgegraphs,”ACMComputingSurveys(Csur),vol.54,no.4,
nequin,“Naturalcontinuallearning:successisajourney,not(just)
pp.1–37,2021.
a destination,” Advances in neural information processing systems,
[11] M. Rostami, S. Kolouri, K. Kim, and E. Eaton, “Multi-agent vol.34,pp.28067–28079,2021.
distributedlifelonglearningforcollectiveknowledgeacquisition,”
[36] A.Almaatouq,A.Noriega-Campero,A.Alotaibi,P.Krafft,M.Mous-
inAdaptiveAgentsandMulti-AgentSystems,2017.
said, and A. Pentland, “Adaptive social networks promote the
[12] T.Li,A.K.Sahu,M.Zaheer,M.Sanjabi,A.Talwalkar,andV.Smith,
wisdomofcrowds,”ProceedingsoftheNationalAcademyofSciences,
“Federatedoptimizationinheterogeneousnetworks,”Proceedingsof
vol.117,no.21,pp.11379–11386,2020.
MachineLearningandSystems,vol.2,pp.429–450,2020.
[37] S.P.Karimireddy,S.Kale,M.Mohri,S.Reddi,S.Stich,andA.T.
[13] L.Collins,H.Hassani,A.Mokhtari,andS.Shakkottai,“Exploiting
Suresh, “Scaffold: Stochastic controlled averaging for federated
shared representations for personalized federated learning,” in
learning,”inInternationalConferenceonMachineLearning. PMLR,
International Conference on Machine Learning. PMLR, 2021, pp.
2020,pp.5132–5143.
2089–2099.
[38] T.Li,S.Hu,A.Beirami,andV.Smith,“Ditto:Fairandrobustfeder-
[14] S.Liu,S.J.Pan,andQ.Ho,“Distributedmulti-taskrelationship
atedlearningthroughpersonalization,”inInternationalConference
learning,” in Proceedings of the 23rd ACM SIGKDD International
onMachineLearning. PMLR,2021,pp.6357–6368.
ConferenceonKnowledgeDiscoveryandDataMining,2017,pp.937–
[39] R.Ye,Z.Ni,F.Wu,S.Chen,andY.Wang,“Personalizedfederated
946.
learningwithinferredcollaborationgraphs,”inProceedingsofthe
[15] J.Wang,M.Kolar,andN.Srerbo,“Distributedmulti-tasklearning,”
40thInternationalConferenceonMachineLearning,ser.Proceedings
inArtificialintelligenceandstatistics. PMLR,2016,pp.751–760.
ofMachineLearningResearch. PMLR,2023.
[16] A. Das, T. Gervet, J. Romoff, D. Batra, D. Parikh, M. Rabbat,
[40] X.Lian,C.Zhang,H.Zhang,C.-J.Hsieh,W.Zhang,andJ.Liu,
and J. Pineau, “Tarmac: Targeted multi-agent communication,”
“Candecentralizedalgorithmsoutperformcentralizedalgorithms?
inInternationalConferenceonMachineLearning. PMLR,2019,pp.
acasestudyfordecentralizedparallelstochasticgradientdescent,”
1538–1546.
inNeuralInformationProcessingSystems,2017.
[17] M.RangwalaandR.Williams,“Learningmulti-agentcommunica-
[41] S. Pu and A. Nedic´, “Distributed stochastic gradient tracking
tionthroughstructuredattentivereasoning,”AdvancesinNeural
methods,”MathematicalProgramming,vol.187,pp.409–457,2018.
InformationProcessingSystems,vol.33,pp.10088–10098,2020.
[42] T.Sun,D.Li,andB.Wang,“Decentralizedfederatedaveraging,”
[18] K.Zhang,Z.Yang,andT.Bas¸ar,“Multi-agentreinforcementlearn-
IEEETransactionsonPatternAnalysisandMachineIntelligence,vol.45,
ing:Aselectiveoverviewoftheoriesandalgorithms,”Handbookof
no.4,pp.4289–4301,2022.
reinforcementlearningandcontrol,pp.321–384,2021.
[19] W. Ren and R. W. Beard, Distributed consensus in multi-vehicle [43] N.MeinshausenandP.Bu¨hlmann,“High-dimensionalgraphsand
cooperativecontrol. Springer,2008,vol.27,no.2. variableselectionwiththelasso,”TheAnnalsofStatistics,vol.34,
no.3,pp.1436–1462,2006.
[20] Y.Zheng,Y.Zhu,andL.Wang,“Consensusofheterogeneousmulti-
agentsystems,”IETControlTheory&Applications,vol.5,no.16,pp. [44] A.-L.Baraba´si,“Networkscience,”PhilosophicalTransactionsofthe
1881–1888,2011. RoyalSocietyA:Mathematical,PhysicalandEngineeringSciences,vol.
[21] B.Lindqvist,P.Sopasakis,andG.Nikolakopoulos,“Ascalabledis- 371,no.1987,p.20120375,2013.
tributedcollisionavoidanceschemeformulti-agentuavsystems,” [45] J.Kirkpatrick,R.Pascanu,N.Rabinowitz,J.Veness,G.Desjardins,
in2021IEEE/RSJInternationalConferenceonIntelligentRobotsand A.A.Rusu,K.Milan,J.Quan,T.Ramalho,A.Grabska-Barwinska
Systems(IROS). IEEE,2021,pp.9212–9218. etal.,“Overcomingcatastrophicforgettinginneuralnetworks,”
[22] J.Yu,J.A.Vincent,andM.Schwager,“Dinno:Distributedneural Proceedingsofthenationalacademyofsciences,vol.114,no.13,pp.
networkoptimizationformulti-robotcollaborativelearning,”IEEE 3521–3526,2017.
RoboticsandAutomationLetters,vol.PP,pp.1–1,2021. [46] R.Aljundi,F.Babiloni,M.Elhoseiny,M.Rohrbach,andT.Tuyte-
[23] L.Wang,X.Zhang,H.Su,andJ.Zhu,“Acomprehensivesurveyof laars,“Memoryawaresynapses:Learningwhat(not)toforget,”
continuallearning:Theory,methodandapplication,”arXivpreprint inProceedingsoftheEuropeanconferenceoncomputervision(ECCV),
arXiv:2302.00487,2023. 2018,pp.139–154.
[24] V.Monga,Y.Li,andY.C.Eldar,“Algorithmunrolling:Interpretable, [47] A.Chaudhry,M.Ranzato,M.Rohrbach,andM.Elhoseiny,“Ef-
efficient deep learning for signal and image processing,” IEEE ficientlifelonglearningwitha-gem,”ArXiv,vol.abs/1812.00420,
SignalProcessingMagazine,vol.38,no.2,pp.18–44,2021. 2018.
[25] U. S. Kamilov, C. A. Bouman, G. T. Buzzard, and B. Wohlberg, [48] Y. LeCun, C. Cortes, and C. Burges, “Mnist hand-
“Plug-and-playmethodsforintegratingphysicalandlearnedmod- written digit database,” ATT Labs [Online]. Available:
elsincomputationalimaging:Theory,algorithms,andapplications,” http://yann.lecun.com/exdb/mnist,vol.2,2010.
IEEESignalProcessingMagazine,vol.40,no.1,pp.85–97,2023. [49] A.KrizhevskyandG.Hinton,“Learningmultiplelayersoffeatures
[26] X.Pu,T.Cao,X.Zhang,X.Dong,andS.Chen,“Learningtolearn fromtinyimages,”Technicalreport,UniversityofToronto,2009.
graphtopologies,”inNeuralInformationProcessingSystems,2021. [50] T.-M. H. Hsu, H. Qi, and M. Brown, “Measuring the effects of
[27] O.Banerjee,L.ElGhaoui,andA.d’Aspremont,“Modelselection non-identicaldatadistributionforfederatedvisualclassification,”
throughsparsemaximumlikelihoodestimationformultivariate arXivpreprintarXiv:1909.06335,2019.
gaussianorbinarydata,”TheJournalofMachineLearningResearch, [51] A.Kalervo,J.Ylioinas,M.Ha¨ikio¨,A.Karhu,andJ.Kannala,“Cubi-
vol.9,pp.485–516,2008. casa5k:Adatasetandanimprovedmulti-taskmodelforfloorplan
[28] Z.LiandD.Hoiem,“Learningwithoutforgetting,”IEEEtransac- image analysis,” in Image Analysis: 21st Scandinavian Conference,
tionsonpatternanalysisandmachineintelligence,vol.40,no.12,pp. SCIA 2019, Norrko¨ping, Sweden, June 11–13, 2019, Proceedings 21.
2935–2947,2017. Springer,2019,pp.28–40.18
[52] A.Singh,T.Jain,andS.Sukhbaatar,“Learningwhentocommu- privacy constraints,” IEEE transactions on neural networks and
nicateatscaleinmultiagentcooperativeandcompetitivetasks,” learningsystems,vol.32,no.8,pp.3710–3722,2020.
arXivpreprintarXiv:1812.09755,2018.
[53] F.QureshiandD.Terzopoulos,“Smartcameranetworksinvirtual
ShuoTangiscurrentlyworkingtowardaPh.D.
reality,”ProceedingsoftheIEEE,vol.96,no.10,pp.1640–1656,2008.
degreeattheCooperativeMedianetInnovation
[54] Y.Li,B.Bhanu,andW.Lin,“Auctionprotocolforcameraactive
CenterinShanghaiJiaoTongUniversitysince
control,”in2010IEEEInternationalConferenceonImageProcessing.
2022.Beforethat,hereceivedtheB.E.degreein
IEEE,2010,pp.4325–4328.
ComputerSciencefromShanghaiJiaoTongUni-
[55] S.Sukhbaatar,R.Fergusetal.,“Learningmultiagentcommunication
versity,Shanghai,China,in2022.Hisresearch
withbackpropagation,”Advancesinneuralinformationprocessing
interests include multi-agent collaboration and
systems,vol.29,2016.
communicationefficiency.
[56] Y.-C.Liu,J.Tian,N.Glaser,andZ.Kira,“When2com:Multi-agent
perceptionviacommunicationgraphgrouping,”in2020IEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR). IEEE,
2020,pp.4105–4114.
[57] R.Wang,X.He,R.Yu,W.Qiu,B.An,andZ.Rabinovich,“Learning RuiYeiscurrentlyworkingtowardaPh.D.degree
efficientmulti-agentcommunication:Aninformationbottleneck attheCooperativeMedianetInnovationCenterin
approach,”inInternationalConferenceonMachineLearning. PMLR, ShanghaiJiaoTongUniversitysince2022.Before
2020,pp.9908–9918. that,hereceivedtheB.E.degreeinInformation
[58] I.J.Goodfellow,M.Mirza,D.Xiao,A.Courville,andY.Bengio,“An EngineeringfromShanghaiJiaoTongUniversity,
empiricalinvestigationofcatastrophicforgettingingradient-based Shanghai,China,in2022.HewasaResearch
neuralnetworks,”arXivpreprintarXiv:1312.6211,2013. InternwithMicrosoftResearchAsiain2022and
[59] F.Zenke,B.Poole,andS.Ganguli,“Continuallearningthrough 2023. His research interests include federated
synapticintelligence,”inInternationalconferenceonmachinelearning. learning,trustworthylargelanguagemodels,and
PMLR,2017,pp.3987–3995. multi-agentcollaboration.
[60] S.-A.Rebuffi,A.Kolesnikov,G.Sperl,andC.H.Lampert,“icarl:
Incrementalclassifierandrepresentationlearning,”inProceedingsof
theIEEEconferenceonComputerVisionandPatternRecognition,2017,
ChenxinXuisworkingtowardthejointPh.D.de-
pp.2001–2010.
greeatCooperativeMedianetInnovationCenter
[61] F. M. Castro, M. J. Mar´ın-Jime´nez, N. Guil, C. Schmid, and
inShanghaiJiaoTongUniversityandatElectrical
K.Alahari,“End-to-endincrementallearning,”inProceedingsofthe
andComputerEngineeringinNationalUniversity
Europeanconferenceoncomputervision(ECCV),2018,pp.233–248.
ofSingaporesince2019.HereceivedtheB.E.
[62] H.Shin,J.K.Lee,J.Kim,andJ.Kim,“Continuallearningwith
degreeininformationengineeringfromShanghai
deepgenerativereplay,”Advancesinneuralinformationprocessing
JiaoTongUniversityin2019.Hisresearchinter-
systems,vol.30,2017.
estsincludetrajectorypredictionandmulti-agent
[63] P.Bhat,B.Zonooz,andE.Arani,“Task-awareinformationrouting
system.Heisthereviewerofsomeprestigious
from common representation space in lifelong learning,” arXiv
internationaljournalsandconferences,including
e-prints,pp.arXiv–2302,2023.
IEEE-TPAMI,CVPR,ICCV,ICMLandNeurIPS.
[64] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirk-
patrick,K.Kavukcuoglu,R.Pascanu,andR.Hadsell,“Progressive
neuralnetworks,”arXivpreprintarXiv:1606.04671,2016. XiaowenDongisanassociateprofessorinthe
[65] Y.ZhangandD.-Y.Yeung,“Aconvexformulationforlearningtask DepartmentofEngineeringScienceattheUniver-
relationshipsinmulti-tasklearning,”arXivpreprintarXiv:1203.3536, sityofOxford,whereheisalsoamemberofboth
2012. theMachineLearningResearchGroupandthe
[66] T.EvgeniouandM.Pontil,“Regularizedmulti–tasklearning,”in Oxford-ManInstitute.Hismainresearchinterests
Proceedings of the tenth ACM SIGKDD international conference on concernsignalprocessingandmachinelearning
Knowledgediscoveryanddatamining,2004,pp.109–117. techniquesforanalysingnetworkdata,andtheir
[67] I.M.Baytas,M.Yan,A.K.Jain,andJ.Zhou,“Asynchronousmulti- applicationsinsocialandeconomicsciences.
tasklearning,”in2016IEEE16thInternationalConferenceonData
Mining(ICDM). IEEE,2016,pp.11–20.
[68] T. Evgeniou, C. A. Micchelli, M. Pontil, and J. Shawe-Taylor,
“Learningmultipletaskswithkernelmethods.”Journalofmachine
SihengChenisatenure-trackassociateprofes-
learningresearch,vol.6,no.4,2005.
sorofShanghaiJiaoTongUniversity.Hewasa
[69] R. Ye, Z. Ni, C. Xu, J. Wang, S. Chen, and Y. C. Eldar, “Fedfm:
researchscientistatMitsubishiElectricResearch
Anchor-basedfeaturematchingfordataheterogeneityinfederated
Laboratories(MERL),andanautonomyengineer
learning,”arXivpreprintarXiv:2210.07615,2022.
at Uber Advanced Technologies Group (ATG),
[70] S.J.Reddi,Z.Charles,M.Zaheer,Z.Garrett,K.Rush,J.Konecˇny´, workingonself-drivingcars.Dr.Chenreceived
S.Kumar,andH.B.McMahan,“Adaptivefederatedoptimization,” his doctorate from Carnegie Mellon University
inInternationalConferenceonLearningRepresentations,2021. in2016.Dr.Chen’sworkonsamplingtheoryof
[71] J. Wang, Q. Liu, H. Liang, G. Joshi, and H. V. Poor, “Tackling graphdatareceivedthe2018IEEESignalPro-
theobjectiveinconsistencyprobleminheterogeneousfederated cessingSocietyYoungAuthorBestPaperAward.
optimization,” Advances in neural information processing systems, He contributed to the project of scene-aware
vol.33,pp.7611–7623,2020. interaction, winning MERL President’s Award. His research interests
[72] R.Ye,M.Xu,J.Wang,C.Xu,S.Chen,andY.Wang,“Feddisco: includegraphmachinelearningandcollectiveintelligence.
Federatedlearningwithdiscrepancy-awarecollaboration,”arXiv
preprintarXiv:2305.19229,2023.
[73] V.Smith,C.-K.Chiang,M.Sanjabi,andA.S.Talwalkar,“Federated YanfengWangreceivedtheB.E.degreeininfor-
multi-task learning,” Advances in neural information processing mationengineeringfromtheUniversityofPLA,
systems,vol.30,2017. Beijing,China,andtheM.S.andPh.D.degrees
inbusinessmanagementfromtheAntaiCollege
[74] C.TDinh,N.Tran,andJ.Nguyen,“Personalizedfederatedlearning
ofEconomicsandManagement,ShanghaiJiao
withmoreauenvelopes,”AdvancesinNeuralInformationProcessing
TongUniversity,Shanghai,China.Heiscurrently
Systems,vol.33,pp.21394–21405,2020.
the Vice Director of the Cooperative Medianet
[75] M.G.Arivazhagan,V.Aggarwal,A.K.Singh,andS.Choudhary,
InnovationCenterandalsotheViceDeanofthe
“Federated learning with personalization layers,” arXiv preprint
SchoolofElectricalandInformationEngineering,
arXiv:1912.00818,2019.
ShanghaiJiaoTongUniversity.Hisresearchinter-
[76] F. Sattler, K.-R. Mu¨ller, and W. Samek, “Clustered federated
estsmainlyincludemediabigdataandemerging
learning:Model-agnosticdistributedmultitaskoptimizationunder
commercialapplicationsofinformationtechnology.19
APPENDIX A For the linear classification tasks, the hessian matrix is
PROOFS AND ANALYSIS Lipschitzcontinuous,thusthereexistsaconstantLsuch
that
A.1 Analysistotheorem1
∥H(θ )−H(θ )∥≤L∥θ −θ ∥
Our analysis starts from the strong convexity of the loss 1 2 1 2
(cid:16) (cid:16) (cid:17) (cid:17)
function L f θ(t) X( it) ,Y i(t) . Since the model f θ(t)(·) is takingthispropertybackintotheformerinequality,we
i i
linearandthelossfunctionisstandardsupervisedtraining obtain
l oo fss th,t ehe Hl eo ss ss iaf nun Hcti (cid:16)o αn ( ii ts )(cid:17)co =nve ∇x 2 α.A
(
it)l Lso (cid:16)t fh θe i(tc )o (cid:16)n Xd i(it ti )o (cid:17)n ,Ynu i(tm )(cid:17)be isr (cid:13) (cid:13) (cid:13)θˆ−α(cid:13) (cid:13)
(cid:13)
2
≤L(cid:13) (cid:13)H(α)−1(cid:13) (cid:13)(cid:90) 01 ∥α−θ∗∥2 2(1−s)ds
b tho au nnd zee rd o, .w Bh asic eh dm onea tn hs isth obe se ei rg ve an tv ioa nlu ,e ws ear pe roa vll idst eri ac ntly upla prg ere - = L(cid:13) (cid:13)H(α)−1(cid:13) (cid:13)∥α−θ∗∥2
(cid:13) (cid:13) 2 2
boundof(cid:13)θ(t)−θ(t)∗(cid:13)
showninLemma1.
(cid:13) i i (cid:13)
2
Our target is minimizing the error upper bound shown
Lemma 1.Supposefortheithagentattimestampt,given
(cid:16) (cid:17) in Lemma 1. For regression loss functions, the Hessian is
trainingdata X( it),Y i(t) ,letthelinearmodelisf θ(t)(·) constant, making the Lipschitz constant L equal to zero,
withlossfunctionL(cid:16) f (cid:16) X(t)(cid:17) ,Y(t)(cid:17) .Letα(t) bei the whichmeansdoingexpansionatanypointα(t) iszeroerror.
θ(t) i i i i
pointdoingTaylorexpani
sionandtheoptimalparameter
Forothertaskswithnon-zeroLipschitzconstantLsuchas
ofthelossfunctionisθ(t)∗
.Then:
classification,thisgoalisequivalenttosolvingthefollowing
i
constraintoptimizationproblem:
wh(cid:13) (cid:13) (cid:13) eθ rei(t L)− isθ thi(t e)∗ L(cid:13) (cid:13) (cid:13) i2 ps≤ chL i2 tz(cid:13) (cid:13) (cid:13) (cid:13) cH on(cid:16) sα ta( i nt t)(cid:17) o− f1 th(cid:13) (cid:13) (cid:13) (cid:13) e(cid:13) (cid:13) (cid:13) Hα e( i st) si− anθ Hi(t) (cid:16)∗(cid:13) (cid:13) (cid:13) α2 2
(
it, )(cid:17)
.
m α i(i tn ) sE .θ ti( .t 1)∗ ≤(cid:18)(cid:13) (cid:13) (cid:13) (cid:13) kH
(cid:16)
H(cid:16) α (cid:16)( i αt)(cid:17) (t− )(cid:17)1 (cid:17)(cid:13) (cid:13) (cid:13) (cid:13)(cid:13) (cid:13) (cid:13) ≤α M( it) ,−θ i(t)∗(cid:13) (cid:13) (cid:13)2 2(cid:19) (28)
i
Proof1.
wherethepriordistributionofthetargetoptimalparameter
F θo ∗r at nh de αsa .k Ce oo nf sb idre ev ri tt hy, ew ere row rr bit ee twθ i( et) e, nθ θi(t) a, n∗ dan θd ∗,α w( it e) ha as vθ e, θ i(t)∗ isp(cid:16) θ i(t)∗(cid:17) ∼N (0,Σ).Duetothedifficultyofcaptur-
ingthespectralradiusoftheinverseHessian,consideringthe
θ−θ∗ =α−θ∗−H(α)−1∇ αL eigenvalues0<λ 1 ≤λ 2,...,≤λ K oftheobjectivehessian
(cid:16) (cid:17)
=α−θ∗−H(α)−1(∇ L−∇ L) H α(t) ,weobtain
α θ∗ i
defineh(t)=∇ θ∗+t(α−θ∗)L,thus K ≤(cid:13) (cid:13) (cid:13)H(cid:16) α(t)(cid:17)−1(cid:13) (cid:13)
(cid:13)=
1
≤
KM
,
θˆ−θ∗ =α−θ∗−H(α)−1(h(1)−h(0)) (cid:80)K i=1λ i (cid:13) i (cid:13) λ 1 (cid:80)K i=1λ i
(cid:13) (cid:13)
=α−θ∗−H(α)−1(cid:90) 1
h′(s)ds which means
(cid:13)
(cid:13)
(cid:13)H(cid:16) α( it)(cid:17)−1(cid:13)
(cid:13)
(cid:13)
and
tr(cid:16) H(cid:16) α( it)(cid:17)(cid:17)−1
are the
0
sameorder.Henceweobtainthefollowingfinaloptimization
=α−θ∗−
problem:
(cid:90) 1
H(α)−1 H(θ∗+s(α−θ∗))(α−θ∗)ds (cid:18) (cid:16) (cid:16) (cid:17)(cid:17)−1(cid:13) (cid:13)2(cid:19)
=H(α)−0 1 m α( ii tn )E θ i(t)∗ tr H α( it) (cid:13) (cid:13)θ i(t)−θ i(t)∗(cid:13) (cid:13) 2 (29)
(cid:16) (cid:16) (cid:17)(cid:17)
(cid:90) 1 s.t.1≤k H α(t) ≤M.
× [H(α)−H(θ∗+s(α−θ∗))](α−θ∗)ds i
0
Then we give our proof to Theorem 1 in terms of a
takingnormonbothsides,weobtain classificationmodelwithC typesofoutputclasses.
(cid:13) (cid:13) (cid:13)θˆ−θ∗(cid:13) (cid:13)
(cid:13)
= Proof 2. For the sake of brevity, we write θ i(t) , α( it) as
(cid:13) (cid:13)
(cid:13)
(cid:13)H(α)−12 (cid:90) 01 [H(α)−H(θ∗+s(α−θ∗))](α−θ∗)ds(cid:13) (cid:13)
(cid:13)
(cid:13)
2
θ i (cid:0)t Xhan ∈cld a Rsα ns ×. o pfT ,Yh cle a ∈sl si in Rfie nca ×ar t Cim o (cid:1)no wd t hae esl rk ef iθ Cs(· θ) ei qc uwo ar i lr t se hs top tro tan hind eis n ng uto md bt ah etae
r
≤(cid:13) (cid:13)H(α)−1(cid:13)
(cid:13)
of classes, p is the dimension of input data. The model
(cid:13)(cid:90) 1 (cid:13) estimatestheprobabilitypˆj ∈RC ofthejthelementx j
×(cid:13)
(cid:13)
[H(α)−H(θ∗+s(α−θ∗))](α−θ∗)ds(cid:13)
(cid:13) as
(cid:13) (cid:13)
0 2
≤(cid:13) (cid:13)H(α)−1(cid:13)
(cid:13) pˆj
=exp(cid:16)
θi⊤x
(cid:17) /(cid:88)C exp(cid:16)
θi⊤x
(cid:17)
i j j
(cid:90) 1
× ∥H(α)−H(θ∗+s(α−θ∗))∥∥α−θ∗∥ ds i=1
2
0 HencetheHessianequalsto
For linear regression task, the hessian matrix H(θ) re- n
mainsconstant,then∥H(α)−H(θ∗+s(α−θ∗))∥=0, H(α)=(cid:88) M ⊗x x⊤,
(cid:13) (cid:13) j j j
whichmeans(cid:13)θˆ−θ∗(cid:13) =0. j=1
(cid:13) (cid:13)
220
whereM j isdefinedas where w(t) ∈ RN2 is the vectorized version of W(t),
 pˆj 1(cid:16) 1−pˆj 1(cid:17) −pˆj 1pˆj
2
··· −pˆj 1pˆj
C
 py a∈ ramRN et2 eris dit sh te anv ce ec .t 1or ∈ize Rd Nv 2e ir ss aio lln oo nf ea vl el ct th oe r,a αge =nt −s’ λm 2/o 2d λe 3l
,
M j =  

. . . pˆj 2(cid:16) 1−pˆj 2(cid:17)
(cid:16)
. . .
(cid:17)
   β h. ,= the− K1/ K2 Tλ 3 c. oT nh du its iow nh oe fn thw ee nc eh wan sg oe lut th ioe nR weL (tU )∗i hn ato scfu hn anct gio edn
−pˆj pˆj ··· ··· pˆj 1−pˆj to
C 1 C C
w(t)∗ =h (αy+βz∗1)
b
Based on these definitions and notations, the bound (31)
1⊤w(t)∗ =m
showninTheorem1canbereformulatedas
(cid:16) (cid:17)
tr(H(α))−1E
θ∗
∥α∥2 2−2α⊤θ∗+∥θ∗∥2
2
Basedonthesenotationsanddefinitionswestarttoprove
=tr(H(α))−1(cid:16) ∥α∥2 2−2α⊤E θ∗(θ∗)+E θ∗(cid:0) ∥θ∗∥2 2(cid:1)(cid:17) T Ph roe oo fre 4m .D2 e.
finep = αy+βz1,p∗ = αy+βz∗1.Consider
=tr(H(α))−1(cid:0) ∥α∥2+tr(Σ−1)(cid:1) ∥w−w∗∥2,wehave
2 2
≥tr(H(α))−1tr(Σ−1)
∥w−w∗∥2 =∥ReLU(p)−h (p∗)∥2
2 b 2
However,fortr(H(α))wehave =∥ReLU(p)−h (p)+h (p)−h (p∗)∥2
b b b 2
 n  ≤∥ReLU(p)−h (p)∥2+∥h (p)−h (p∗)∥2
(cid:88) b 2 b b 2
tr(H(α))=tr j=1M j ⊗x jx⊤ j 
≤
N2b +(cid:88)N2
[h′(ξ )β(z−z∗)]2
n 4 b i
=(cid:88) tr(cid:16) M
j
⊗x jx⊤
j
(cid:17)
N2b
i=1
j=1 ≤ +N2β2(z−z∗)2
4
n C
=(cid:88) ∥x j∥2 2(cid:88) pˆj
i
(cid:16) 1−pˆj i(cid:17) Ontheotherhand,defineϕ(x)=1⊤h b(αy+βx1),we
j=1 i=1 have
=(cid:88)n
∥x
∥2(cid:32) 1−(cid:88)C pˆj2(cid:33) ≤(cid:18)
1−
1(cid:19) (cid:88)n
∥x ∥2 ϕ(z)−m=ϕ(z)−ϕ(z∗)=ϕ′(ξ)(z−z∗)
j 2 i C j 2
j=1 i=1 j=1
whereξ ∈[z,z∗].Thus
Takingthisinequalitybackintotheoriginalbound,we
have
|z−z∗|=(cid:12)
(cid:12)
(cid:12)ϕ(z)−m(cid:12)
(cid:12)
(cid:12)
tr(H(α))−1E θ∗(cid:16) ∥α∥2 2−2α⊤θ∗+∥θ∗∥2 2(cid:17) (cid:12) ϕ′(ξ) (cid:12)
|ϕ(z)−1⊤ReLU(αy+βz1)|
≥tr(Σ−1)(cid:34)(cid:18)
1−
C1(cid:19) (cid:88)n
∥x i∥2
2(cid:35)−1
.
= (cid:12)
(cid:12) (cid:12)(cid:80)N i=2 1h′ b(αy
i+βξ)β(cid:12)
(cid:12) (cid:12)
i=1 |ϕ(z)−1⊤ReLU(αy+βz1)|
The equality is satisfied when pˆ 1 = pˆ 2 = ... = pˆ c and ≤ N2C|β|
∥α∥2 =0,whichisequivalenttoα=0. √ √
2 N2 b b
≤ = ,
2N2C|β| 2C|β|
A.2 Analysistotheorem2
Before we provide our final proof, we first state a simple where the second inequality comes from the Lemma 2.
observationshowninthefollowinglemma: Takingtheinequalitybackinto∥w−w∗∥2 2,weobtain
(cid:16)√ (cid:17)
Lemma2.Leth b(x)= x2+b+x /2,then
∥w−w∗∥2 ≤
N2b
+
N2b
=
N2b(cid:20)
1+
1 (cid:21)
√ 2 4 4C2 4 C2
b
∀x∈R,|h (x)−ReLU(x)|≤
b 2 which means the error ∥w − w∗∥ 2 is bounded by
√ (cid:113)
Proof3.Ifx≥0,then N 2 b 1+ C1 2.
(cid:12)√ (cid:12)
(cid:12) x2+b+x (cid:12)
|h (x)−ReLU(x)|=(cid:12) −x(cid:12)
b (cid:12) (cid:12) 2 (cid:12) (cid:12) A.3 Analysistotheorem3
(cid:12)√ (cid:12) (cid:12) (cid:12) √
(cid:12) x2+b−x(cid:12) (cid:12) b (cid:12) b Recallthattheoptimalsolutioncorrespondstothefirst-order
=(cid:12) (cid:12)=(cid:12) √ (cid:12)≤ .
(cid:12) (cid:12) 2 (cid:12) (cid:12) (cid:12) (cid:12)2( x2+b+x)(cid:12) (cid:12) 2 condition shown in (21). However, this condition requires
(cid:12)(cid:16)√ (cid:17) (cid:12) each agent to know W i( jt) and W j(t i) both. In DeLAMA, our
TIf hx us< it0 a, lt sh oe sn at| ih sb fi( ex s) |−
h
bR (xe )L −U( Rx e) L| U= ((cid:12) (cid:12) x)|<x2 √+ b/b 2+
.
x /2(cid:12) (cid:12). r toea kl nm oe wss Wage
i(
jtp )a foss rin thg em ite hch aa gn ei ns tm .In(22 th) eon foly llore wq iu ni gre ls emth mea ag ,e wn et
first claim that this mechanism does converge to the exact
Tobeginwith,rewritetheKKTconditionofthesolutionas optimalsolutionbydemonstratingthatW(t) issymmetric.
w(t) =ReLU(αy+βz1) Lemma3.ThecollaborationgraphstructureW(t) ∈RN×N
(30)
1⊤w(t) =m learnedfromAlgorithm2issymmetric.21
Theproofcomesfromasimpleobservation.Firstrewritethe where x ∈ RNd = [x⊤;...;x⊤]⊤. Define x˜ =
1 N
optimizationproblem(18): [∥x 1∥;...;∥x N∥]⊤ ,M˜ ij =(cid:13) (cid:13)4λ 2W ijB− i 1(cid:13) (cid:13).Thenwecan
(cid:16) (cid:17) concludethat
min L W(t) =λ ∥W(t)⊙D(t)∥ +λ ∥W(t)∥2
2 1 3 F (cid:13) (cid:13)2 (cid:13) (cid:13)2 (cid:13) (cid:13)2
W(t) ∥4λ MW⊗Ix∥2 ≤(cid:13)M˜x˜(cid:13) ≤(cid:13)M˜(cid:13) ∥x˜∥2 =(cid:13)M˜(cid:13) ∥x∥2
(cid:16) (cid:17) 2 2 (cid:13) (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) 2
s.t.∥W(t)∥
1
=m, W(t) ≥0, diag W(t) =0 2
RecallthatthelossfunctionL(θ i)isconvex,meanswe
where D(t) stands for (cid:13) (cid:13)θ(t)−θ(t)(cid:13) (cid:13)2 and ⊙ is ele- haveaneigendecompositionA i =Q⊤ i Λ iQ i.Takinginto
ment
wisi ej
production.
T(cid:13) hei
n
thej o(cid:13) b2
servation is, for
thedefinitionofB i,wehave
any feasible solution W 0(t) of th (cid:16)is opt (cid:17)imizati (cid:16)on prob (cid:17)- 4λ 2W ijB−
i
1 =4λ 2W ij(cid:104) Q⊤
i
Λ iQ i+(2λ 1+4λ 2d i)I(cid:105)−1
lem, W(t)⊤ is also feasible and L W(t) = L W(t)⊤ .
In fact,0 we have the relationship λ 2(cid:13) (cid:13) (cid:13)0 W 0(t)⊤⊙D(t)0 (cid:13) (cid:13) (cid:13)
1
+ =Q⊤ i (cid:20)Λ i+( 42 λλ 21 W+ i4 jλ 2d i)I(cid:21)−1 Q i
(cid:13) (cid:13)2 (cid:13) (cid:13) (cid:13) (cid:13)2
λ 3(cid:13) (cid:13)W 0(t)⊤(cid:13) (cid:13)
F
= λ 2(cid:13) (cid:13)W 0(t)⊤⊙D(t)⊤(cid:13) (cid:13)
1
+ λ 3(cid:13) (cid:13)W 0(t)⊤(cid:13) (cid:13)
F
= Thustheoperatornormof4λ 2W ijB−
i
1 equalsto
(cid:13) (cid:13) (cid:13) (cid:13)2
λ of2 o(cid:13) (cid:13) bW jec0(t t) iv⊙ eD fu( nt c)(cid:13) (cid:13) ti1 on+ rλ em3(cid:13) (cid:13) aW ins0(t u) n(cid:13) (cid:13) cF h, aw ngh eic dh .Tm he ua sn ts heth pe rov oa flu oe f (cid:13) (cid:13)4λ 2W ijB− i 1(cid:13) (cid:13)= λ∗
i
+(4 2λ λ2 1W +ij 4λ 2d i),
Lemma3isasfollows: (cid:16) (cid:17) whereλ∗ i >0istheminimumeigenvalueofΛ i.
Proof5.∥W(t)⊙D(t)∥ 1isconvex,thusL W(t) isstrongly By Gershgorin circle theorem, the eigenvalues of M˜ is
convex,makingtheoptimizationproblemwithaunique boundedby
solution W(t)∗. However, from our observation, the
N
tra (cid:16)nspose (cid:17)of W (cid:16)(t)∗ is (cid:17)also a feasible solution, and λ<(cid:88) M˜
ij
L W(t)∗ =L W(t)∗⊤ ,thusW(t)∗ =W(t)∗⊤,which
j=1
m ope ta imns izth ate ioc nol il sab so yr mat mio en trg icr .aphstructurelearnedfromthe =(cid:88)N 4λ 2W
ij =
4λ 2d
i ,
λ∗+(2λ +4λ d ) λ∗+(2λ +4λ d )
j=1 i 1 2 i i 1 2 i
BasedonLemma3,themessage-passingmechanismdoes
convergetotheoptimalsolution,hencewestartourproofto forsome1≤i≤N.Thus
Theorem3. (cid:13) (cid:13) (cid:20) 4λ d (cid:21)
Proof 6.Forthesakeofbrevity,wewriteB i(t) ,θ i(t,k) ,W(t) (cid:13) (cid:13)M˜(cid:13) (cid:13)< 1m ≤ia ≤x N 2λ 1+2 4λi 2d i =ρ<1,
asB i,θ i,W.Define
applythispropertytoθk−1 wehave
θk bk B−1 0 ... 0 
θk =  

θθ . .
.
k1 2k   

,b=  

bb k. . .1 k 2  

,M=  

0 01 . .
.
B 0− 2 . .
.
1 .. . .. .. .. B0 −. .
.
1   , Takingkgoes(cid:13) (cid:13) (cid:13)θ tok i− nfiθ n∗ it(cid:13) (cid:13) (cid:13) y2 2 w< eρ ha(cid:13) (cid:13) (cid:13) vθ ek−
(cid:13)
(cid:13)θ1 k− −θ∗ θ(cid:13) (cid:13) (cid:13) ∗2 2
(cid:13)
(cid:13).
2
2goestozero
N N N withalinearconvergencerate.
thentheiterativealgorithmsimplifiesto:
(cid:16) (cid:17)
θk =M 4λ 2W⊗Iθk−1+b . APPENDIX B
Supposetheoptimalparameterisθ∗,thenwehave EXPERIMENTAL DETAILS
B.1 RegressionProblem
(cid:13) (cid:13)2 (cid:13) (cid:104) (cid:105)(cid:13)2
(cid:13) (cid:13)θk−θ∗(cid:13) (cid:13) =(cid:13) (cid:13)4λ 2MW⊗I θk−1−θ∗ (cid:13) (cid:13) Implementation.Weuseafullyconnectednetworkasthe
2 2
backbonetorepresentthenonlineartransformϕaspartof
Wewrite∥MW⊗I∥astheoperatornormofMW⊗I.
thetrainingparametersofDeLAMA.Thenumberofhidden
Thenwehave
layers is 2 with the output layer set in dimension 50. The
N (cid:13) (cid:13) N (cid:13) (cid:13)2 batchsizeusedfortrainingis2withalearningrateof1e-3.
∥4λ 2MW⊗Ix∥2
2
=(cid:88) i=1(cid:13) (cid:13)
(cid:13)
(cid:13)j(cid:88) =14λ 2W ijB−
i
1x j(cid:13) (cid:13)
(cid:13)
(cid:13)
ΦTh ge rai pt her aa nti don lifn elu om ngbe mrs odfo er luc pol dla ab teo Φra pt aiv rae mre ala reti so en ta tl oi Mnf 1er =en 1c 0e
2
N N
andM
2
=10,respectively.
≤(cid:88)(cid:88)(cid:13) (cid:13)4λ 2W ijB−
i
1x j(cid:13) (cid:13)2
2
Taskconfiguration.Wevisualizeoneexampleofthegener-
ated task sequences shown in Figure 10. In this example,
i=1j=1
N N we can see two different agents with distinct learning
≤(cid:88)(cid:88)(cid:13) (cid:13)4λ 2W ijB−
i
1(cid:13) (cid:13)2 ∥x j∥2
2
goals (the black line). From T = 1 to T = 5, the agents
encountered different batches of data samples, forming
i=1j=1
 2 differentviewpointsofthetargetregressionfunction.Inthe
N N
≤(cid:88) (cid:88)(cid:13) (cid:13)4λ 2W ijB− i 1(cid:13) (cid:13)∥x j∥ 2 b the eg ii rn ln ei an rg n, it nh ge ta ag se kn .t As sco tu imld en io nt crh ea av se esa ,f au gl el nu tn sd ce or ust la dn bd ein ag bo lef
i=1 j=1
toguessthelearningtargetfunctionwiththehelpoftheir
lifelonglearningability.22
 7     7     7     7     7     * U R X Q G  W U X W K  I X Q F W L R Q
  
  
 
 
 
 
                                                                                                                       
 7     7     7     7     7     * U R X Q G  W U X W K  I X Q F W L R Q
    
    
   
   
   
   
                                                                                                                       
Fig.10:Atime-evolvinglearningtaskexampleoftwoagentssampledfromtheregressiontaskdataset.Foreachagent,thetraining
samplesaregeneratedaroundtheagent’sgroundtruthfunctionwithalimitedsamplinginterval.Theagent’sknowledgeofthe
targetfunctionwillimproveasthenumberofsamplingintervalsincreases.
B.2 Imageclassification
Implementation. We used self-designed convolution net-
workinMNISTtasksandResNet-18inCIFAR-10tasksto
represent the backbone for nonlinear transform ϕ β(·). For
theself-designedconvolutionnetwork,theconvolutionlayer
numberissetto2withtheoutputfeaturedimension50.For
the CIFAR-10 tasks we removed the batch normalization
layers of ResNet-18 to increase the training stability, and
the output feature dimension is also set to 50. The batch
sizeis2withalearningrateof1e-3.Theiterationnumber
for lifelong model update Φ param(·) is set to 10 and the
numberofiterationstepsofcollaborationrelationalinference
Φ graph(·)issetto5.Forfederatedlearningmethods,werun
100 rounds for each timestamp. During each round, each
clientconducts5iterationsofmodeltrainingforMNISTand
CIFAR-10.
B.3 Multi-robotmapping
Implementation.ThebackboneusedforDeLAMAisdefined
as a simple multi-layer perceptron with 2 hidden layers. Fig.11:Thehuman-machineinteractionframeworkusedforthe
The output feature dimension of the MLP is 10 and the human-involved experiment. In the interaction phase, agents
can observe others’ behaviors and submit their understand-
localmodels’outputdimensionis1followedbyasigmoid
ings. To enable the interaction between humans and agents
activation function to represent the occupancy probability.
running in DeLAMA, we add virtual agents to perform visual
Forthetrainingoftheunrollednetworkpart,thelearning
interactionwitheachother.Inthebackendisthecollaboration
rateis1e-3withbatchsize2.Theiterationnumberofmessage algorithm powered by DeLAMA, with proxy agents imitating
passingissetto10andthegraphlearningiterationnumber humanbehaviors.Thistop-downframeworkenableshuman-
issetto5. machinecommunicationandprovidesaplatformforrealhuman
evaluation.
B.4 Humaninvolvedexperiment
System overview. The collaboration system employs a (Figure15).Thehuman-involvedexperiment’sroutineisas
hierarchicalstructure.Attheuserinterfacelayer,itcomprises follows:
real human users and ’virtual agents’ used for human-
machineinteraction.Atthebackendaretheagentspowered 1) Login.Eachparticipantwillreadtheuserguideat
byDeLAMA.Notethattoenableinteractionbetweenhumans theloginpage(showninFigure12).Theninputtheir
andmachinesinthebackend,weutilizethe’proxymachines’ usernametorepresenttheirIDandenterthegame.
to mimic human behaviors. The full system framework is 2) Locallearning.Theparticipantwillfirstencountera
showninFigure11. plotofscatterpoints(asshowninFigure13),then
Web GUI. Here we provide a web GUI example to de- drawalinetorepresenttheirguessoftheregression
scribe how our human-involved experiment works. There line. This line represents their learning knowledge
are 5 main pages of the web GUI, including the login purelybasedontheirvisualinformation.
page (Figure 12), the local learning page (Figure 13), the 3) Aggregate and collaborate. The agent will have a
result modification page (Figure 14), and the output page viewofotherparticipants’learninglines,andguess23
Fig.12:Theloginpageofthehuman-involvedexperiment,
contains a user guide for the participants to learn how to
playthegame.
Fig.14:Theaggregationpageofthehuman-involvedexperi-
ment,whichcontainsotherparticipants’resultsandthescore
ratedbyeachagent.
Fig.13:Thelocallearningpageofthehuman-involvedex-
periment.Hereparticipant’sdrawnlinewillbetransformed
intoalinearequationwithtwoparameters.
whether to modify their initial learned regression
resultsaccordingtoothers’results.Thenthesubmit-
tedfinallearningresultwillbeevaluatedandreturn
the regression loss to the participant as shown in
Figure14.
Onthefinalpage,theparticipantwillseealltheirhistorical
learningresultsandtheexactgroundtruthregressiontarget
functionasshowninFigure15.
Fig. 15: The final page of the human-involved experiment
with a leaderboard, each participant can look back to
historicallearningresults.