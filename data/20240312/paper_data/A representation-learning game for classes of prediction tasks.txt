A representation-learning game for classes of prediction tasks
Neria Uzan and Nir Weinberger∗
March 12, 2024
Abstract
We propose a game-based formulation for learning dimensionality-reducing representations
of feature vectors, when only a prior knowledge on future prediction tasks is available. In this
game,thefirstplayerchoosesarepresentation,andthenthesecondplayeradversariallychooses
a prediction task from a given class, representing the prior knowledge. The first player aims is
to minimize, and the second player to maximize, the regret: The minimal prediction loss using
the representation, compared to the same loss using the original features. For the canonical
setting in which the representation, the response to predict and the predictors are all linear
functions, and under the mean squared error loss function, we derive the theoretically optimal
representation in pure strategies,which shows the effectiveness of the prior knowledge, and the
optimalregretinmixedstrategies,whichshowstheusefulnessofrandomizingtherepresentation.
For general representations and loss functions, we propose an efficient algorithm to optimize a
randomized representation. The algorithm only requires the gradients of the loss function, and
is based on incrementally adding a representation rule to a mixture of such rules.
1 Introduction
Commonly, data of unlabeled feature vectors x is collected without a specific down-
i
{ } ⊂ X
stream prediction task it will be used for. When a prediction task becomes of interest, responses
y are also collected, and a learning algorithm is trained on (x ,y ) . Modern sources,
i i i
∈ Y { }
such as high-definition images or genomic sequences, have high dimensionality, and this necessi-
tates dimensionality-reduction, either for better generalization [Goodfellow et al., 2016], for stor-
age/communication savings [Tsitsiklis, 1989, Nguyen et al., 2009, Duchi et al., 2018], or for inter-
pretability [Schapire and Freund, 2012]. The goal is thus to find a low-dimensional representation
z = R(x) Rr, that preserves the relevant part of the features, without a full knowledge of the
∈
downstream prediction task. In this paper, we propose a game-theoretic framework for this goal,
by assuming that the learner has prior knowledge on the class of downstream prediction tasks. Our
contributions are a theoretical solution in the linear setting, under the mean squared error (MSE)
loss, and an algorithm for the general setting.
Unsupervisedmethodsfordimensionalityreduction,suchasprincipalcomponent analysis(PCA)
[Pearson, 1901, Jolliffe, 2005, Cunningham and Ghahramani, 2015, Johnstone and Paul, 2018], and
non-linear extensions such as kernel PCA [Schölkopf et al., 1998] and auto-encoders (AE) [Kramer,
1991, Hinton and Salakhutdinov, 2006, Lee et al., 2011, Goodfellow et al., 2016], aim that the rep-
resentation z will maximally preserve the variation in x, and thus ignore any prior knowledge on
∗The authors are with the Department of Electrical and Computer Engineering, Technion – Israel Institute of
Technology. Emails: {neriauzan@gmail.com, nirwein@technion.ac.il}. This research was supported by theIsrael
Science Foundation (ISF),grants no. 1782/22.
1
4202
raM
11
]GL.sc[
1v17960.3042:viXrafuture prediction tasks. This prior knowledge may indicate, e.g., that highly varying directions in
thefeaturespaceare,infact,irrelevant fordownstream predictiontasks. Fromthesupervisedlearn-
ing perspective, the information bottleneck (IB) principle [Tishby et al., 2000, Chechik et al., 2003,
Slonim et al., 2006, Harremoës and Tishby, 2007] was used to postulate that efficient supervised
learningnecessitatesrepresentationsthatarebothlow-complexityandrelevant[Tishby and Zaslavsky,
2015,Shwartz-Ziv and Tishby,2017,Shwartz-Ziv,2022,Achille and Soatto,2018a,b](seeAppendix
B). To corroborate this claim, Dubois et al. [2020]proposed a game-theoretic formulation (based on
a notion of usable information, introduced by Xu et al. [2020]), in which Alice selects a prediction
problem of y given x, and then Bob selects the representation z. The pay-off is the minimal risk
possible using this representation. Dubois et al. [2020] showed that ideal generalization is obtained
for representations that solve the resulting decodable IB problem.
Evidently, the game of Dubois et al. [2020] is tailored to supervised learning, since the represen-
tation is optimized based on the prediction problem. In this paper, we assume the learner collected
unlabeled feature vectors, and has a prior knowledge that the downstream prediction problem be-
longs to a class of response functions. We propose a game different from Dubois et al. [2020]:
F
First, the representation player chooses a rule R to obtain z = R(x) Rr. Second, the response
∈
function player chooses y = f(x) where f is possibly random. The payoff for (R,f) is the
∈ F
regret: The minimal prediction loss of y based on z compared to the minimal prediction loss based
on x. The goal of the representation player (resp. response function player) is to minimize (resp.
maximize) the payoff, and the output of this game is the saddle-point representation. Compared
to Dubois et al. [2020], the representation is chosen based only on the class of response functions,
rather than a specific one. The minimax regret measures the worst-case regret (over functions in
) of a learner that uses the saddle-point representation. We derive the minimax representation
F
both in pure strategies and in mixed strategies [Owen, 2013]. Mixed strategies use randomized rep-
resentation rules, whose utilization is illustrated as follows: Assume that routinely collected images
are required to be compressed. There are two compression (representation) methods. The first
smoothens the images, and the second preserves their edges; exclusively using the first method
would hinder any possibility of making predictions reliant on the image’s edges. This is prevented
by randomly alternating the use of both methods.
The class manifests the prior knowledge on the downstream prediction tasks that will use
F
the represented features, and may stem from domain specific considerations; imposed by privacy or
fairness constraints; or emerge from transfer or continual learning settings; see Appendix A for an
extended discussion. The resulting formulation encompasses an entire spectrum of possibilities: (1)
Supervised learning: = f is a singleton, and thus known to the learner. (2) Multitask learning
F { }
[Baxter,2000,Maurer et al.,2016,Tripuraneni et al.,2020,2021]: = f ,f , ,f isafiniteset
1 2 t
F { ··· }
of functions. (3) Prior expert knowledge: represents a continuous, yet restricted set of functions.
F
For example, the response functions in may be more sensitive to certain features than others. (4)
F
No supervision: istheclassofall possibleresponsefunctions,whichisessentiallyanunsupervised
F
learning problem, since no valuable information is known. We focus on the intermediate regimes of
partial supervision, in which the learner should (and can) optimize its representation to be jointly
efficientforalltasksin . Inthisrespect,multitasklearning(case2)isageneralizationofsupervised
F
learning, sincethelearnercansimulatethetfunctionsin . Priorknowledge(case3)ismoresimilar
F
to unsupervised learning, since the learner will not be able to efficiently do so.
Theoretical contribution We address the fundamental setting in which the representation, the
response, and the prediction are all linear functions, under the mean squared error (MSE) loss
(Section 3). The class is = f 1 for a known symmetric matrix S. Combined with
S S
F {k k ≤ }
2the covariance matrix of the features, they determine the relevant directions of the function in the
feature space, in contrast to just the features variability, as in standard unsupervised learning. We
establish the optimal representation and regret in pure strategies, which shows the utility of the
prior information, and in mixed strategies, which shows that randomizing the representation yields
strictly lower regret. We prove that randomizing between merely ℓ different representation rules
∗
suffices, where r+1 ℓ d is a precisely characterized effective dimension. Interestingly, mixed
∗
≤ ≤
representations do not improve the regret in standard unsupervised learning (see Proposition 15 in
Appendix E.1 for the PCA setting).
Algorithmic contribution We develop an algorithm for optimizing mixed representations (Sec-
tion 4) for general representations/response/predictors and loss functions, based only on their
gradients. Similarly to boosting [Schapire and Freund, 2012], the algorithm operates incremen-
tally. At each iteration it finds the response function in that is most poorly predicted by
F
the current mixture of representation rules. An additional representation rule is added to the
mixture, based on this function and the ones from previous iterations. The functions generated
by the algorithm can be considered a “self-defined signals”, similarly to self-supervised learning
[Oord et al., 2018, Shwartz-Ziv and LeCun, 2023]. To optimize the weights of the representation,
the algorithm solves a two-player game using the classic multiplicative weights update (MWU)
algorithm [Freund and Schapire, 1999] (a follow-the-regularized-leader algorithm [Shalev-Shwartz,
2012, Hazan, 2016]).
Related work InAppendixBwediscuss: TheIBprincipleandcomparethegameofDubois et al.
[2020] with ours; The generalization error bounds of Maurer et al. [2016], Tripuraneni et al. [2020]
for multi-task learning and learning-to-learn, and how our regret bound complements these results;
The use of randomization in representation learning, similarly to our mixed strategies solution;
Iterativealgorithms forsolvingminimax games,andspecificallytheincremental algorithmapproach
for learning mixture models [Schapire and Freund, 2012, Tolstikhin et al., 2017], which we adopt.
2 Problem formulation
Notation conventions are mostly standard, and are detailed in Appendix C. Specifically, the eigen-
values of S Sd are denoted as λ (S) λ (S) λ (S) = λ (S) and v (S) denotes an
∈ + max ≡ 1 ≥ ··· ≥ d min i
eigenvector corresponding to λ (S) such that V = V(S) := [v (S),v (S), ,v (S)] Rd d and
i 1 2 d ×
··· ∈
S = V(S)Λ(S)V (S) is an eigenvalue decomposition. W := [w ,...,w ] R(j i+1) d is the ma-
⊤ i:j i j − ×
∈
trix comprised of the columns of W Rd d indexed by i,...,j . The probability law of a random
×
∈ { }
variable x is denoted as L(x).
Let x be a random feature vector, with probability law P := L(x). Let y be a
x
∈ X ∈ Y
corresponding response drawn according to a probability kernel y f( x = x), where for brevity,
∼ · |
we will refer to f as the response function (which can be random). It is known that f for some
∈ F
known class . Let z := R(x) Rr be an r-dimensional representation of x where R: Rr is
F ∈ X →
chosen from a class of representation functions, and let Q: be a prediction rule from a
R X → Y
class , with the loss function loss: R . The pointwise regret of (R,f) is
+
QX Y ×Y →
regret(R,f P ):= min E[loss(y,Q(R(x)))] min E[loss(y,Q(x))]. (1)
x
| Q ∈QRr −Q ∈QX
The minimax regret in mixed strategies is the worst case response function in given by
F
regret ( , P ) := min maxE[regret(R,f P )], (2)
mix x x
R F | L(R) ( ) f |
∈P R ∈F
3where ( ) is a set of probability measures on the possible set of representations . The minimax
P R R
regret in pure strategies restricts ( ) to degenerated measures (deterministic), and so the expec-
P R
tation in (2) is removed. Our main goal is to determine the optimal representation strategy, either
in pure R or mixed strategies L(R ) ( ). To this end, we will also utilize the maximin
∗ ∗
∈ R ∈ P R
version of (2). Specifically, let ( ) denote a set of probability measures supported on , and
P F F
assume that for any R , there exists a measure in ( ) that puts all its mass on R. Then, the
∈ R P R
minimax theorem [Owen, 2013, Chapter 2.4] [Sion, 1958] implies that
regret ( , P )= max minE[regret(R,f P )]. (3)
mix x x
R F | L(f) ( )R |
∈P F ∈R
The right-hand sideof (3)is the maximin regret in mixed strategies, and the maximizing probability
law L(f ) is known as the least favorable prior. In general, regret ( , P ) regret ( ,
∗ mix x pure
R F | ≤ R F |
P ), and the inequality canbestrict. We focusonthe representation aspect, andthus assumedthat
x
P is known and that sufficient labeled data will be provided to the learner from the subsequent
x
predictiontask. Wealsonotethat,ascommoningame-theory, themixedminimaxregretisachieved
for repeating representation games [Owen, 2013], which fits the scenario in which routinely collected
data is to be represented. The size of the dataset for each of these games should be large enough to
allow for accurate learning of f to be used by the predictor. By contrast, the pure minimax regret
guarantee is valid for a single representation, and thus more conservative from this aspect.
3 The linear setting under MSE loss
In this section, we focus on linear classes and the MSE loss function. The response function class
is characterized by a quadratic constraint specified by a matrix S Sd , which represents the
F ∈ ++
relative importance of each direction in the feature space in determining y.
Definition 1 (The linear MSE setting). Assume that = Rd, that = R and a squared error
X Y
loss function loss(y ,y ) = y y 2. Assume that E[x] = 0 and let Σ := E[xxT] Sd be its
1 2 | 1 − 2 | x ∈ ++
invertible covariance matrix. The classes of representations, response functions, and predictors are
all linear, that is: (1) The representation is z = R(x) = R x for R := Rd r where d > r; (2)
⊤ ×
∈ R
The response function is f Rd , and y = f x+n R, where n R is a heteroscedastic
⊤
∈ F ⊂ ∈ ∈
noise that satisfies E[n x]= 0, and given some specified S Sd it holds that
| ∈ ++
f := f Rd: f 2 1 , (4)
∈ FS ∈ k kS≤
n o
where f := S 1/2f = (f S 1f)1/2 is the Mahalanobis norm; (3) The predictor is Q(z) =
S − 2 ⊤ −
k k k k
q z R for q Rr. Since the regret will depend on P only via Σ , we will abbreviate the notation
⊤ x x
∈ ∈
of the pure (resp. mixed) minimax regret to regret ( Σ ) (resp. regret ( Σ )).
pure x mix x
F | F |
In Appendix E.1 we show that standard PCA can be similarly formulated, by assuming that
is a singleton containing the noiseless identity function, so that y = x surely holds, and xˆ =
F
Q(z) Rd. Proposition 15 therein shows that the pure and mixed minimax representations are
∈
both R = V (Σ ), and so randomization is unnecessary. We begin with the pure minimax regret.
1:r x
Theorem 2. For the linear MSE setting (Definition 1)
1/2 1/2
regret ( Σ ) = λ Σ SΣ . (5)
pure S x r+1 x x
F |
(cid:16) (cid:17)
4A minimax representation matrix is
1/2 1/2 1/2
R
∗
:= Σ−x V
1:r
Σ
x
SΣ
x
, (6)
·
(cid:16) (cid:17)
and the worst case response function is
f := S1/2 v Σ1/2 SΣ1/2 . (7)
∗ r+1 x x
·
(cid:16) (cid:17)
The optimal representation thus whitens the feature vector x, and then projects it on the top r
1/2 1/2
eigenvectors of the adjusted covariance matrix Σ SΣ , which reflects the prior knowledge that
x x
f . The proof in Appendix E.2 has the following outline: Plugging the optimal predictor
S
∈ F
into the regret results a quadratic form in f Rd, determined by a matrix which depends on the
∈
subspace spanned by the representation R. The worst-case f is the determined via the Rayleigh
quotient theorem, and the optimal R is found via the Courant–Fischer variational characterization
(see Appendix D for a summary of these results). We next consider the mixed minimax regret:
Theorem 3. For the linear MSE setting (Definition 1), let λ λ (S1/2Σ S1/2) for i [d] and
i i x
≡ ∈
λ 0, and let ℓ be any member of
d+1 ∗
≡
ℓ
ℓ [d] [r]:(ℓ r) λ 1 λ 1 (ℓ r) λ 1 . (8)
∈ \ − ·
−ℓ
≤
−i
≤ − ·
−ℓ+1
( )
i=1
X
• The minimax regret in mixed strategies is
ℓ r
regret ( Σ )= ∗ − . (9)
mix FS | x ℓ∗ λ 1
i=1 −i
• The covariance matrix of the least favorable priorPof f is
V Λ 1V
Σ :=
⊤ −ℓ∗
. (10)
∗f ℓ∗ λ 1
i=1 −i
where Λ := diag(λ ,...,λ ,0, ,0), and V P V(S1/2Σ S1/2).
ℓ 1 ℓ∗ x
··· ≡
• The probability law of the minimax representation: Let A 0,1
ℓ∗ ×(ℓ r∗ )
be a matrix whose
columns are the members of the set := a 0,1
ℓ∗
: a
=∈ {
ℓ
}
r (in some order). Let
1 ∗
A { ∈ { } k k − }
b = (b ,...,b ) be such that
1 ℓ∗ ⊤
λ 1
b = (ℓ r)
−i
. (11)
i ∗ − · ℓ∗ λ 1
j=1 −j
Then, there exists a solution p
∈
[0,1](ℓ r∗ ) to Ap = bP with support size at most ℓ ∗+1. For j
∈
[ ℓ r∗ ],
let := i [ℓ ]:A = 0 be the zero indices on the jth column of A, and let V denote the r
Ij { ∈ ∗ ij } Ij (cid:0) (cid:1)
1/2
pcol ,u fm orns
j
of [V ℓ∗w ]h .ose index is in Ij. A minimax representation is R
∗
= Σ−x V
Ij
with probability
j ∈ r
Interesti(cid:0) ngl(cid:1) y, while the eigenvalues λ (Σ1/2 SΣ1/2 ) = λ (S1/2Σ S1/2) are equal, the pure mini-
i x x i x
1/2 1/2
max regret utilizes the eigenvectors of Σ SΣ whereas the mixed minimax regret utilizes those
x x
of S1/2Σ S1/2, which are possibly different. The proof of Theorem 3 is also in Appendix E.2, and
x
5is substantially more complicated than for the pure regret. The reason is that directly maximizing
over L(R) is challenging, and so we take a two-step indirect approach. The outline is as follows:
First, we solve the maximin problem (3), and find the least favorable prior L(f ). Second, we pro-
∗
pose a probability law for the representation L(R), and show that its regret equals the maximin
value, and thus also the minimax. With more detail, in the first step, we show that the regret only
depends on L(f) via Σ = E[ff ], and we explicitly construct L(f) that is supported on and
f ⊤ S
F
hasthis covariance matrix. Thisreduces theproblem fromoptimizing L(f)tooptimizing Σ ,whose
f
solution (Lemma 17) results the least favorable Σ , and then the maximin value. In the second
∗f
step, we construct a representation that achieves the maximin regret. Concretely, we construct
1/2 1/2
representation matrices that use r of the ℓ principal components of Σ SΣ , where ℓ > r. The
∗ x x ∗
defining property of ℓ (8), established in the maximin solution, is utilized to find proper weights on
∗
the
ℓ∗
possible representations, which achieve the maximin solution, and thus also the minimax.
r
The proof then uses Carathéodory’s theorem (see Appendix D) to establish that the optimal p is
j
supp(cid:0) or(cid:1)
ted on at most ℓ +1 matrices, much less than the potential support of
ℓ∗
. We
next{ m}
ake
∗ r
a few comments:
(cid:0) (cid:1)
1. Computing the mixed minimax probability: This requires solving A⊤p = b for a probability
vectorp. Thisisalinear-programfeasibilityproblem,whichisroutinelysolved[Bertsimas and Tsitsiklis,
1997]. For illustration, if r = 1 then p = 1 (ℓ 1)λ 1/( ℓ∗ λ 1) for j [ℓ ], and if ℓ = r+1
j − ∗ − −j i=1 −i ∈ ∗ ∗
then p = (λ 1)/( ℓ∗ λ 1) on the first ℓ standard basis vectors. Nonetheless, the dimension of p
j −j j′=1 −j′ ∗ P
is ℓ∗ and thus increases fast as Θ((ℓ )r), which is infeasible for high dimensions. In this case, the
r P ∗
algorithm of Section 4 can be used. As we empirically show it approximately achieves the optimal
(cid:0) (cid:1)
regret, with slightly more than ℓ +1 atoms (see Example 6 next).
∗
2. Required randomness: The regret formulation (2) assumes that the actual realization of the
representation rule is known to the predictor. Formally, this can be conveyed to the predictor using
an small header of less than log (ℓ +1) log(d+1) bits. Practically, this is unnecessary and an
2 ∗ ≤
efficient predictor can be learned from a labeled dataset (z,y).
3. The rank of Σ : The rank of the covariance matrix of the least favorable prior is an effective
∗f
dimension, satisfying (see (9))
1 (r/ℓ)
ℓ ∗ = argmax − . (12)
ℓ ∈[d] \[r] 1 ℓ ℓ i=1λ −i 1
mBy eac non ψv (e ℓn )ti :o =n, 1{λ
−i
ℓ1
}
λi ∈[d 1]
.is Foa rm exo an mot po ln e,ic ifn λon- =de icrPe αas win itg hs αeq >uen 0c te h, ea nn ψd (s ℓo
)
=is Θth (e ℓαp )a .rt Si oa ,l iC f,es eà .gr .o
,
ℓ i=1 −i i −
ψ(ℓ) = ℓα, then it is easily derived that ℓ min α+1r,d . Hence, if α r is large enough and
P ∗ ≈ { α } ≥ d r
the decay rate of λ is fast enough then ℓ < d, but otherwise ℓ = d. As−the decay rate of λ
i ∗ ∗ i
{ } { }
becomes faster, the rank of Σ decreases from d to r. Importantly, ℓ r+1 always holds, and so
∗f ∗
≥
the optimal mixed representation is not deterministic even if S1/2Σ S1/2 has less than r significant
x
eigenvalues (which can be represented by a single matrix R Rd r). Hence, the mixed minimax
×
∈
regret is always strictly lower than the pure minimax regret. Thus, even when S = I , and no
d
valuable prior knowledge is known on the response function, the mixed minimax representation is
different from the standard PCA solution of top r eigenvectors of Σ .
x
4. Uniqueness of the optimal representation: Since one can always post-multiply R x by some
⊤
invertible matrix, and then pre-multiply z = R x by its inverse, the following holds: If and
⊤
R Q
are not further restricted, then if R is a minimax representation, and W(R) Rr r is an invertible
×
∈
matrix, then R W(R) is also a minimax representation.
·
65. Infinite-dimensional features: Theorems 2 and 3 assume a finite dimensional feature space.
We show in Appendix F that the results can be generalized to an infinite dimensional Hilbert space
, in the more restrictive setting that the noise n is statistically independent of x.
X
Example 4. AssumeS = I , and denote, forbrevity, V V(Σ ) := [v ,...,v ]and Λ Λ(Σ ) :=
d x 1 d x
≡ ≡
diag(λ ,...,λ ). The optimal minimax representation in pure strategies (Theorem 2) is
1 d
R
∗
= Σ x−1/2 ·V
1:r
= VΛ x−1/2 V ⊤V
1:r
= VΛ−x1/2 ·[e 1,...,e r] = λ− 11/2 ·v 1,...,λ −r1/2 ·v
r
, (13)
h i
which is comprised of the top r eigenvectors of Σ , scaled so that v x has unit variance. By
x i⊤
Comment 4 above, V is also an optimal minimax representation. The worst case response is
1:r
f = v (Σ ) and, as expected, since R uses the first r principal directions
r+1 x
regret ( Σ ) = λ . (14)
pure x r+1
F |
The minimax regret in mixed strategies (Theorem 3) is different, and given by
ℓ r
regret ( Σ ) = ∗ − , (15)
mix F | x ℓ∗ λ 1
i=1 −i
where ℓ is determined by the decay rate of the eigenvaPlues of Σ (see (8)). It can be easily verified
∗ x
that the mixed minimax regret is strictly lower than the pure minimax regret. Indeed, it holds for
any ℓ > r that
ℓ r ℓ r
− − λ . (16)
ℓ i=1λ
−i
1 ≤ ℓλ −r+1
1
≤ r+1
The least favorable covariance mPatrix is given by (Theorem 3)
ℓ∗ −1
Σ = λ 1 V diag λ 1,...,λ 1,0, ,0 V . (17)
∗f −i
·
−1 −ℓ∗
··· ·
⊤
" #
i=1
X (cid:0) (cid:1)
Intuitively, Σ equalizes the first ℓ eigenvalues of Σ Σ (and nulls the other d ℓ ), to make the
∗f ∗ x ∗f
−
∗
representation indifferent to these ℓ directions. As evident from the regret, the “equalization” of
∗
theith eigenvalue adds atermof λ 1 tothedenominator, andif λ istoosmallthen v isnot chosen
−i i i
for the representation. This agrees with Comment 3, which states that a fast decay of λ reduces
i
{ }
ℓ away from d. A derivation similar to (13) shows that the mixed minimax representation sets
∗
1/2 1/2 1/2
R ∗ = Σ x− ·V Ij = λ− ij,1 ·v ij,1,...,λ− ij,r ·v ij,r (18)
h i
with probability p , where i ,...,i . Thus, the optimal representation chooses a random
j j j,1 j,r
I ≡ { }
subset of r vectors from v ,...,v . See the left panel of Figure 1 for a numerical example.
1 ℓ∗
{ }
Example 5. To demonstrate the effect of prior knowledge on the response function, we assume
Σ = diag(σ2,...,σ2) and S = diag(s ,...,s ), where σ2 σ2 σ2 (but s are
x 1 d 1 d 1 ≥ 2 ≥ ··· ≥ d { i }i ∈[d]
not necessarily ordered). Letting f = (f ,...,f ), the class of response functions is := f
1 d S
Rd: d (f2/s ) 1 ,andsocoordinatesi [d]withalarges havelargeinfluenceontF herespo{ nse∈ .
i=1 i i ≤ } ∈ i
Let (i ,...,i ) be a permutation of [d] so that σ2 s it the jth largest value of (σ2s ) . The
P(1) (d) i(j) i(j) i i i ∈[d]
pure minimax regret is (Theorem 2)
regret ( Σ )= σ2 s . (19)
pure F | x ir+1 ir+1
710-3 10-4
9 50 1.4 50
8
1.2 48
7
45
1
6 46
5 0.8
40 44
4 0.6
3 42
0.4
35
2
0.2 40
1
0 30 0 38
1 1.5 2 2.5 3 3.5 4 1 1.5 2 2.5 3 3.5 4
Figure 1: Left: Pure and mixed minimax regret and ℓ for Example 4, for d = 50,r = 25, with
λ = σ2 i α. Right: Pure and mixed minimax regret∗ and ℓ for Example 5, for d = 50,r = 25,
wi ith σi 2 ∝ i− α and s i2. The trend of ℓ is reversed for α >∗ 2.
i ∝ − i ∝ ∗
TheoptimalrepresentationisR = [e ,e ,...,e ],thatis,usesthemostinfluentialcoordinates,
i i i
(1) (2) (r)
according to s , which may be different from the r principal directions of Σ . For the minimax
i x
{ }
regret in mixed strategies, Theorem 3 results
ℓ r
regret ( Σ ) = ∗ − (20)
mix F | x ℓ∗ (s σ2) 1
j=1 ij ij −
P
for ℓ [d] [r] satisfying (8), and the covariance matrix of the least favorable prior is given by
∗
∈ \
ℓ∗ σ 2 e e
Σ = j=1 i− j · ij ⊤ij. (21)
∗f P ℓ j∗ =1(s ijσ i2 j) −1
That is, the matrix is diagonal, and the kthP term on the diagonal is Σ (k,k) σ 2 if k = i for
∗f
∝
k− j
some j [ℓ ] and Σ (k,k) = 0 otherwise. As in Example 4, Σ equalizes the first ℓ eigenvalues of
∈
∗ ∗f ∗f ∗
Σ Σ (and nulls the other d ℓ ). However, it does so in a manner that chooses them according
x f ∗
−
to their influence on the response f x. The minimax representation in mixed strategies is
⊤
R = σ 1 e ,...,σ 1 e (22)
∗ i−
j,1 ·
ij,1 i−
j,r ·
ij,r
h i
with probability p . Again, the first ℓ coordinates are used, and not just the top r. See the right
j ∗
panel of Figure 1 for a numerical example. Naturally, in the non-diagonal case, the minimax regret
will also depend on the relative alignment between S and Σ .
x
4 An algorithm for general classes and loss functions
In this section, we propose an iterative algorithm for optimizing the representation in mixed strate-
gies, i.e., solving (2) for general classes and loss functions. The algorithm will find a finite mixture
of m representations, and so we let R = R(j) with probability p(j) for j [m] (this suffices
∈ R ∈
for the linear MSE setting of Section 3, but only approximate solution in general). The main idea
is to incrementally add representations R(j) to the mixture. Loosely speaking, the algorithm is
initialized with a single representation rule R(1). Then, the response function in which is most
F
poorly predicted when x is represented by R(1)(x) is found. The added representation component
R(2) aims to allow for accurate prediction of this function. At the next iteration, the function in
F
8which is most poorly predicted by a mixed representation that randomly uses either R(1) or R(2)
is found, and R(3) is then optimized to reduce the regret for this function, and so on. The idea of
optimizing mixture models by gradually adding components to the mixture is common, and is used
in boosting and generative adversarial network (GANs) (See Appendix B for a review).
The actual algorithm ismore complicated, since italsofinds proper weights p(j) , and also
j [m]
{ } ∈
randomizes the response player. Thus we set f = f(i) with probability o(i) where i [m], and
∈ F ∈
m = m +m for some m 0. The resulting optimization problem then becomes
0 0
≥
min max p(j) o(i) regret(R(j),f(i) P ), (23)
x
p(j),R(j) o(i),f(i) · · |
{ ∈R}{ ∈F}j X∈[m]i X∈[m]
where p(j) and o(i) are probability vectors. Note that the prediction rule Q(j,i) is
j [m] i [m]
{ } ∈ { }∈
determined based on both R(j) and f(i), and that the ultimate goal of solving (23) is just to extract
the optimal R.
Henceforth, the index k [m] will denote the current number of representations. Initialization
∈
requires a representation R(1), as well as a set of functions f(i) , so that the final support size
{
}i ∈m0
of f will be m = m +m. Finding this initial representation and the set of functions is based on
0
the specific loss function and the set of representation/predictors (see Appendix G for examples).
The algorithm has two phases for each iteration. In the first phase, a new adversarial function
is added to the set of functions, as the worse function for the current random representation. In
the second phase, a new representation atom is added to the set of possible representations. This
representation is determined based on the given set of functions. Concretely, at iteration k:
• Phase 1 (Finding adversarial function): Given k representations R(j) with weights
j (k)
{ } ∈
p(j) , the algorithm determines f(m0+k) as the worst function, by solving
j [k]
{ } ∈
reg := regret ( R(j),p(j) , P ):= max p(j) regret(R(j),f P ), (24)
k mix { }j ∈[k] F | x f · | x
∈F j [k]
X∈
and f(m0+k) is set to be the maximizer. This simplifies (23) in the sense that m is replaced by k,
the random representation R is kept fixed, and f is optimized as a pure strategy (the previous
∈ F
functions f(i) are ignored).
{
}i ∈[m0+k −1]
• Phase 2 (Adding a representation atom): Given fixed f(j) and R(j) , a new
{
}j ∈[m0+k]
{
}j ∈[k]
representation R(k+1) is found as the most incrementally valuable representation atom, by solving
min regret ( R(j1) , f(j2) P ) :=
mix x
R(k+1) { } { }|
∈R
min min max p(j1) o(j2) regret(R(j1),f(j2) P ) (25)
x
R(k+1) p(j1) o(j2) · · |
∈R{ }{ }Xj1 Xj2
where j [k+1] and j [m +k], the solution R(k+1) is added to the representations, and the
1 2 0
∈ ∈
weights are updated to the optimal p(j1) . Compared to (23), the response functions and first k
{ }
representations are kept fixed, and only the weights p(j1) o(j2) and R(k+1) are optimized.
{ } { }
The procedure is described in Algorithm 1, where, following the main loop, m = argmin reg
∗ k [m] k
∈
representation atoms are chosen and the output is R(j),p(j) . Algorithm 1 relies on solvers
j [m∗]
{ } ∈
forthePhase1(24)andPhase2(25)problems. InAppendix Gwe proposetwo algorithms forthese
problems, which are based on gradient steps for updating the adversarial response (assuming is
F
9a continuous class) and the new representation, and on the MWU algorithm [Freund and Schapire,
1999] for updating the weights. In short, the Phase 1 algorithm updates the response function f
via a projected gradient step of the expected loss, and then adjusts the predictors Q(j) to the
{ }
updated response function f and the current representations R(j) . The Phase 2 algorithm
j [k]
{ } ∈
only updates the new representation R(k+1) via projected gradient steps, while keeping R(j)
j [k]
{ } ∈
fixed. Given the representations R(j) and the functions f(i) , a predictor Q(j,i)
{
}j ∈[k+1]
{
}i ∈[m0+k]
is then fitted to each representation-function pair, which also determines the loss for this pair.
The weights p(j) and o(i) are updated towards the equilibrium of the two-player
{
}j ∈[k+1]
{
}i ∈[m0+k]
game determined by the loss of the predictors Q(j,i) via the MWU algorithm. In a
{
}j ∈[k+1],i ∈[m0+k]
multi-task learning setting, in which the class is finite, the Phase 1 solver can be replaced by a
F
performing a simple maximization over the functions.
Algorithm 1 Solver of (23): An iterative algorithm for learning mixed representations.
1: input P x, , , ,d,r,m,m 0 ⊲ Feature distribution, classes, dimensions and parameters
R F Q
2: input R(1) , {f(j) }j ∈[m0] ⊲ Initial representation and initial function (set)
3: begin
4: for k = 1 to m do
5: phase 1: f(m0+k) is set by a solver of (24) and: ⊲ Solved using Algorithm 2
reg regret ( R(j),p(j) , P ) (26)
k ← mix { }j ∈[k] F | x
6: phase 2: R(k+1), p(j) is set by a solver of (25) ⊲ Solved using Algorithm 3
7: end for
{ k }j ∈[k+1]
8: set m ∗ = argmin k [m]reg k
∈
9: return {R(j) }j ∈[m∗] and p m∗ = {p( kj) }j ∈[m∗]
SinceAlgorithm1aimstosolveanonconvex-concaveminimaxgame,derivingtheoreticalbounds
on its convergence seems to be elusive at this point (see Appendix B for a discussion). We next
describe a few experiments with the algorithm (See Appendix H for details).
Example 6. We validated Algorithm 1 in the linear MSE setting (Section 3), for which a closed-
form solution exists. We ran Algorithm 1 on randomly drawn diagonal Σ , and computed the ratio
x
between the regret obtained by the algorithm to the theoretical value. The left panel of Figure 2
shows that the ratio is between 1.15 1.2 in a wide range of d values. Algorithm 1 is also useful
−
for this setting since finding an (ℓ +1)-sparse solution to Ap = b is computationally difficult when
∗
ℓ∗ is very large (e.g., in the largest dimension of the experiment d = 19 = 11,628).
r r 5
(cid:0) (cid:1)Our next example pertains to a logistic regression setting, under(cid:0)t(cid:1)he c(cid:0)ros(cid:1)s-entropy loss function.
Definition 7 (The linear cross-entropy setting). Assume that = Rd, that = 1 and that
X Y {± }
E[x]= 0. Assumethattheclassofrepresentationislinearz = R(x) = R xforsomeR := Rd r
⊤ ×
∈ R
where d > r. Assume that a response function and a prediction rule determine the probability
that y = 1 via logistic regression modeling, as f(y = 1 x) = 1/[1 + exp( f x)]. Assume
⊤
± | ∓
the cross-entropy loss function, where given that the prediction that y = 1 with probability q
results the loss loss(y,q) := 1(1+y)logq 1 (1 y)log(1 q). The set of predictor functions is
−2 − 2 − −
:= Q(z) = 1/[1+exp( q z)], q Rr . As forthe linear case, we assume that f for some
⊤ S
Q − ∈ ∈ F
S Sd . The regret is then given by the expected binary Kullback-Leibler (KL) divergence
∈ (cid:8)++ (cid:9)
regret(R,f P )= minE D [1+exp( f x)] 1 [1+exp( q R x)] 1 . (27)
x KL ⊤ − ⊤ ⊤ −
| q Rr − || −
∈ h (cid:16) (cid:17)i
10Figure 2: Results of Algorithm 1. Left: r = 5, varying d. The ratio between the regret achieved
by Algorithm 1 and the theoretical regret in the linear MSE setting. Right: r = 3, varying d. The
regret achieved by Algorithm 1 in the linear cross-entropy setting, various m.
Example 8. We drawn empirical distributions of features from an isotropic normal distribution,
in the linear cross-entropy setting. We ran Algorithm 1 using the closed-form regret gradients
from Appendix H. The right panel of Figure 2 shows the reduced regret obtained by increasing the
support size m of the random representation, and thus the effectiveness of mixed representations.
The last example compares the optimized representation with that of PCA.
Example 9 (Comparison with PCA for multi-label Classification). We constructed a dataset of
images, each containing 4 shapes randomly selected from a dictionary of 6 shapes. The class is
F
finite and contains the t = 6 binary classification functions given by indicators for each shape. The
representation is linear and thepredictor is based on logistic regression. Inthis setting, Algorithm 1
can be simplified; See Appendix H.3 for details (specifically Definition 23 of the setting and Figure
6 for an image example). We ran the simplified version of Algorithm 1 on a dataset of 1000 images,
andcomparedthecross-entropylossandtheaccuracyofoptimizedrepresentationtothatofPCAon
a fresh dataset of 1000 images. The results in Figure 3 show that the regret of PCA is much larger,
not only for the worse-case function but also for the average-case function. For example, using the
representation obtainedby thealgorithm withr = 3 < t = 6isas goodasPCAwithatleastr = 12.
From a different point of view, in order to achieve the loss of the optimized representation output
by our algorithm with r = 1 more than r = 15 dimensions are required for PCA.
5 Conclusion
Weproposedagame-theoreticformulationforlearningfeaturerepresentationswhenpriorknowledge
on the class of downstream prediction tasks is available. We derived the optimal solution for the
fundamental linear MSE setting, which quantify the gain of prior knowledge and the possibility
to randomize the representation. We then proposed an iterative algorithm suitable for general
classes of functions and losses, and exemplified its effectiveness. Interestingly, our formulation links
between the problem of finding jointly efficient representations for multiple prediction tasks, and
the problem of finding saddle points of non-convex/non-concave games. The latter is a challenging
problem and is under active research (see Appendix B). So, any advances in that domain can be
translated to improve representation learning. Similarly to our problem, foundation models also
adapt to wide range of downstream prediction tasks, however, efficient learning is achieved therein
11Figure 3: Results onthe dataset of images. Comparison between optimized minimax representation
(simplified version of Algorithm 1) vs. PCA. Worst-case function in blue, and average-case function
in orange. Left: Cross entropy loss. Right: Accuracy.
without explicit prior knowledge on the class of downstream tasks, albeit using fine-tuning. It is
interesting to incorporate minimax formulations into the training procedure of foundation models,
or to explain them using similar game formulations.
For future research it would be interesting: (1) To generalize the class = f: f 1
S S
F { k k ≤ }
used for linear functions to the class := E f(x) 2 1 for non-linear functions, where
FSx k∇x kSx ≤
S is now locally specified (somewhat similarly to the regularization term used in contractive
{ x }x ∈Rd (cid:2) (cid:3)
AE [Rifai et al., 2011], though for different reasons). (2) To efficiently learn S from previous expe-
rience, e.g., improving S from one episode to another in a meta-learning setup [Hospedales et al.,
2021]. (3) To evaluate the effectiveness of the learned representation in our formulation, as an
initialization for further optimization when labeled data is collected. One may postulate that our
learned representation may serve as a universal initialization for such training.
The outline of the appendix follows. In Appendix A we discuss in detail the origin of prior
knowledge of classes of prediction problems. In Appendix B we review additional related work. In
Appendix C we set our notation conventions. In Appendix D we summarize a few mathematical
results that are used in later proofs. In Section E we show that PCA can be cast as a degenerate
setting of our formulation, and provide the proofs of the main theorems in the paper (the linear
MSEsetting). InAppendixFwegeneralizetheseresultstoaninfinitedimensional Hilbertspace. In
AppendixGweprovidetwoalgorithmsforsolvingthePhase1andPhase2problemsinAlgorithm1.
In Appendix H we provide details on the examples for the experiments with the iterative algorithm,
and additional experiments.
A Classes of response functions
Our approach to optimal representation is based on the assumption that a class of downstream
F
prediction tasks is known. This assumption may represent prior knowledge or constraints on the
responsefunction,andcanstemfromvariousconsiderations. Tobegin,itmightbehypothesizedthat
somefeaturesarelessrelevantthanothers. Asasimpleintuitive example, theouter pixelsinimages
aretypically lessrelevant totheclassification of photographed objects, regardless oftheir variability
(whichmaystemfromotheraffects,suchaslightingconditions). Similarly,non-codingregionsofthe
12genotype are irrelevant for predicting phenotype. The prior knowledge may encode softer variations
in relevance. Moreover, such prior assumption may be imposed on the learned function, e.g., it
may be assumed that the response function respects the privacy of some features, or only weakly
depends on features which provide an unfair advantage. In domain adaptation [Mansour et al.,
2009],onemaysolvethepredictionproblemforfeaturedistributionP obtainingaoptimalresponse
x
function f . Then, after a change of input distribution to Q , the response function learned for this
1 x
feature distribution f may be assumed to belong to functions which are “compatible” with f . For
2 1
example, if P and Q are supported on different subsets of Rd, the learned response function f (x)
x x 1
and f (x) may be assumed to satisfy some type of continuity assumptions. Similar assumptions
2
may hold for the more general setting of transfer learning [Ben-David et al., 2010]. Furthermore,
such assumptions may hold in a continual learning setting [Zenke et al., 2017, Nguyen et al., 2017,
Van de Ven and Tolias, 2019, Aljundi et al., 2019], in which a sequence of response functions is
learned one task at a time. Assuming that catastrophic forgetting is aimed to be avoided, then
starting from the second task, the choice of representation may assume that the learned response
function is accurate for all previously learned tasks.
B Additional related work
The information bottleneck principle The IB principle is a prominent approach to feature
relevance in the design of representations [Tishby et al., 2000, Chechik et al., 2003, Slonim et al.,
2006, Harremoës and Tishby, 2007], and proposes to optimize the representation in order to maxi-
mize its relevance to the response y. Letting I(z;y) and I(x;z) denote the corresponding mutual
information terms [Cover and Thomas, 2006], the IB principle aims to maximize the former while
constraining the latter from above, and this is typically achieved via a Lagrangian formulation
[Boyd et al., 2004]. The resulting representation, however, is tailored to the joint distribution of
(x,y), i.e., to a specific prediction task. In practice, this is achieved using a labeled dataset (gen-
eralization bounds were derived by Shamir et al. [2010]). As in our mixed representation approach,
the use of randomized representation dictated by a probability kernel z f( x = x) is inherent
∼ · |
to the IB principle.
A general observation made from exploring deep neural networks (DNNs) [Goodfellow et al.,
2016] used for classification, is that practically good predictors in fact first learn an efficient repre-
sentation z of the features, and then just train a simple predictor from z to the response y = f(x).
This is claimed to be quantified by the IB where low mutual information I(x;z) indicates an effi-
cient representation, and high mutual information I(z;y) indicates that the representation allows
for efficient prediction. This idea has been further developed using the IB principle, by hypothesiz-
ing that modern prediction algorithms must intrinsically include learning an efficient representation
[Tishby and Zaslavsky,2015,Shwartz-Ziv and Tishby,2017, Shwartz-Ziv,2022,Achille and Soatto,
2018a,b] (this spurred a debate, see, e.g., [Saxe et al., 2019, Geiger, 2021]).
However, this approach is inadequate in our setting, since the prediction task is not completely
specified to the learner, and in the IB formulation the optimal representation depends on the re-
sponse variable (so that labeled data should be provided to the learner). In addition, as explained
by Dubois et al. [2020], while the resulting IB solution provides a fundamental limit for the prob-
lem, it also suffers from multiple theoretical and practical issues. The first main issue is that the
mutual information terms are inherently difficult to estimate from finite samples [Shamir et al.,
2010, Nguyen et al., 2010, Poole et al., 2018, Wu et al., 2020, McAllester and Stratos, 2020], espe-
cially at high dimensions, and thus require resorting to approximations, e.g., variational bounds
[Chalk et al., 2016, Alemi et al., 2016, Belghazi et al., 2018, Razeghi et al., 2022]. The resulting
13generalization bounds [Shamir et al., 2010, Vera et al., 2018] are still vacuous for modern settings
[Rodriguez Galvez, 2019]. The second main issue is that the IB formulation does not constrain the
complexity of the representation and the prediction rule, which can be arbitrarily complex. These
issueswereaddressedbyDubois et al.[2020]usingthenotionofusableinformation,previouslyintro-
duced by Xu et al.[2020]: The standard mutual information I(z;y) can bedescribed as the log-loss
difference between a predictor for z which does not use or does use y (or vice-versa, since mutual
information is symmetric). Usable information, or -information I (z y), restricts thepredictor
F F →
to a class , which is computationally constrained. Several desirable properties were established
F
in Xu et al. [2020] for the -information, e.g., probably approximate correct (PAC) bounds via
F
Rademacher-complexity based bounds [Bartlett and Mendelson, 2002] [Wainwright, 2019, Chapter
5][Shalev-Shwartz and Ben-David, 2014, Chapters 26-28].
Dubois et al. [2020] used the notion of -information to define the decodable IB problem, with
F
the goal of assessing the generalization capabilities of this IB problem, and shedding light on the
necessity of efficient representation for generalization. To this end, a game was proposed, in which
the data available to the learner are feature-response pairs (x,y) (in our notation y = f(x)). In
the game proposed by Dubois et al. [2020], Alice chooses a prediction problem f(), i.e., a feature-
·
response pair (x;y), and Bob chooses a representation z = R(x). For comparison, in this paper,
we assume that the learner is given features x and a class of prediction problems. We ask how to
F
choose the representation R if it is only known that the response function Y = f(X) can be chosen
adversarially from . In our formulated game, the order of plays is thus different. First, the repre-
F
sentation player chooses R, and then the adversarial function player chooses f . Beyond those
∈ F
works,theIBframeworkhasdrawnasignificantrecentattention, andaremarkablenumberofexten-
sions and ramifications have been proposed [Sridharan and Kakade, 2008, Amjad and Geiger, 2019,
Kolchinsky et al., 2019, Strouse and Schwab, 2019, Pensia et al., 2020, Asoodeh and Calmon, 2020,
Ngampruetikorn and Schwab,2021,Yu et al.,2021,Ngampruetikorn and Schwab,2022,Gündüz et al.,
2022,Razeghi et al.,2022,Ngampruetikorn and Schwab,2023]. AnIBframeworkforself-supervised
learning was recently discussed in [Ngampruetikorn et al., 2020].
Multitask learning and learning-to-learn In the problem of multitask learning and learning-
to-learn[Baxter,2000,Argyriou et al.,2006,Maurer et al.,2016,Du et al.,2020,Tripuraneni et al.,
2020, 2021], the goal of the learner is to jointly learn multiple downstream prediction tasks. The
underlyingimplicitassumptionisthatthetasksaresimilarinsomeway, specifically, thattheyshare
acommonlow dimensional representation. Inthenotationof thispaper, aclass = f ,f , ,f
1 2 t
F { ··· }
of t prediction task is given, and a predictor from R:Rd is decomposed as Q (R(x)), where
i
→ Y
the representation z =R(x) Rr is common to all tasks. The learner is given a dataset for each of
∈
the tasks, and its goal is to learn the common representation, as well as the t individual predictors
in the multitask setting. In the learning-to-learn setting, the learner will be presented with a new
prediction tasks, and so only the representation is retained.
Maurer et al. [2016] assumed that the representation is chosen from a class and the predic-
R
tors from a class (in our notation), and generalization bounds on the average excess risk for
Q
an empirical risk minimization (ERM) learning algorithm were derived, highlighting the different
complexity measures associated with the problem. In the context of learning-to-learn, the bound
in [Maurer et al., 2016, Theorem 2] scales as O(1/√t) + O(1/√m), where m is the number of
samples provided to the learner from the new task. In practice, it is often the case that learning-
to-learn is efficient even for small t, which implies that this bound is loose. It was then improved
by Tripuraneni et al. [2020], whose main statement is that a proper notation of task diversity is
crucial for generalization. They then propose an explicit notion of task diversity, controlled by a
14term ν and obtained a bound of the form O˜(1 C( R)+tC( Q) + C( Q) ) (in our notation), where
ν nt m
(cid:18)q q (cid:19)
C( ) and C( ) are complexity measures for the representation class and the prediction class. For
R Q
comparison, in this paper we focus on finding the optimal representation, either theoretically (in
the fundamental linear MSE setting) or algorithmically, rather than relying on a generic ERM. To
this end we side-step the generalization error, and so the regret we define can be thought of as an
approximation error. One direct consequence of this difference is that in our case task diversity
(rich ) leads to a large regret, whereas in [Tripuraneni et al., 2020] task diversity leads to low gen-
F
eralization bound. From this aspect, our results complement those of Tripuraneni et al. [2020] for
the non-realizable case (that is, when the prediction tasks cannot be decomposed as a composition
f(x)= Q(R(x)).
Randomization in representation learning Randomization is classically used in data rep-
resentation, most notably, utilizing the seminal Johnson-Lindenstrauss Lemma Johnson [1984] or
more generally, sketching algorithms (e.g., [Vempala, 2005, Mahoney et al., 2011, Woodruff et al.,
2014, Yang et al., 2021]). Our use of randomization is different and is inspired by the classical
Nash equilibrium [Nash Jr, 1950]. Rather than using a single deterministic representation that was
randomly chosen, we consider randomizing multiple representation rules. Such randomization is
commonly used in the face of future uncertainty, which in our setting is the downstream prediction
task.
Game-theoreticformulationsinstatisticsandmachine-learning Theuseofgame-theoretic
formulations in statistics, between a player choosing a prediction algorithm and an adversary
choosing a prediction problem (typically Nature), was established by Wald [1939] in his classi-
cal statistical decision theory (see, e.g., [Wasserman, 2004, Chapter 12]). It is a common approach
both in classic statistics and learning theory [Yang and Barron, 1999, Grünwald and Dawid, 2004,
Haussler and Opper, 1997, Farnia and Tse, 2016], as well as in modern high-dimensional statistics
[Wainwright, 2019]. The effect of the representation (quantizer) on the consistency of learning al-
gorithms when a surrogate convex loss function replaces the loss function of interest was studied
in [Nguyen et al., 2009, Duchi et al., 2018, Grünwald and Dawid, 2004] (for binary and multiclass
classification, respectively). A relation between information loss and minimal error probability was
recently derived by Silva and Tobar [2022].
Iterative algorithms for the solution of minimax games have drawn much attention in the last
few years due to their importance in optimizing GANs [Goodfellow et al., 2020, Creswell et al.,
2018], adversarial training [Madry et al., 2017], and robust optimization [Ben-Tal et al., 2009].
The notion of convergence is rather delicate, even for the basic convex-concave two-player set-
ting [Salimans et al., 2016]. While the value output by the MWU algorithm [Freund and Schapire,
1999], or improved versions [Daskalakis et al., 2011, Rakhlin and Sridharan, 2013] converges to a
no-regret solution, theactualstrategies usedby theplayers are, infact, repelled away fromtheequi-
librium point to the boundary of the probability simplex [Bailey and Piliouras, 2018]. For general
games, the gradient descent ascent (GDA) is a natural and practical choice, yet despite recent ad-
vances, its theory is still partial [Zhang et al., 2022]. Various other algorithms have been proposed,
e.g.,[Schäfer and Anandkumar,2019,Mescheder et al.,2017,Letcher et al.,2019,Gidel et al.,2019,
Zhang and Wang, 2021].
Incremental learning of mixture models Our proposed Algorithm 1 operates iteratively, and
each main iteration adds a representation rule as a new component to the existing mixture of
15representation rules. More broadly, the efficiency of algorithms that follow this idea is based on
two principles: (i) A powerful model can be obtained from a mixture of a few weak models; (ii)
Mixture models can be efficiently learned by a gradual addition of components to the mixture, if
the new component aims to address the most challenging problem instance. As a classic example,
this idea is instantiated by the boosting method [Schapire and Freund, 2012] for classification, and
was adapted for generative models by Tolstikhin et al. [2017] for GANs. In boosting, the final
classifier is a mixture of simpler classifiers. Large weights are put on data points which are wrongly
classified with the current mixture of classifiers, and the new component (classifier) is trained to
cope with these samples. In GANs, the generated distribution is a mixture is of generative models.
Large weights are put on examples which are easily discerned by the discriminator of the true
and generated distributions, and the new component (generative distribution) optimizes the GAN
objectiveonthisweighteddata. Inoursetting,thefinalrepresentationisamixtureofrepresentation
rules. Weights areputonadversarial functions thatcannot beaccurately predicted withthecurrent
representation matrices. The new representation component aims to allow for accurate prediction
of these functions. Overall, the common intuitive idea is very natural: The learning algorithm
sees what is most lacking in the current mixture, and adds a new component that directly aims to
minimizethisshortage. Wereferthereaderto[Tolstikhin et al.,2017]foramorein-depthexposition
of this idea. As mentioned by Tolstikhin et al. [2017], this idea dates back to the use of boosting
for density estimation [Welling et al., 2002].
Unsupervised pretraining From a broader perspective, our method is essentially an unsuper-
vised pretraining method, similar to the methods which currently enable the recent success in
natural language processing. Our model is much simplified compared to transformer architecture
[Vaswani et al., 2017], but the unsupervised training aspect used for prediction tasks [Devlin et al.,
2018] is common, and our results may shed light on these methods. For example, putting more
weight on some words compared to others during training phase that uses the masked-token pre-
diction objective.
C Notation conventions
For an integer d, [d] := 1,2,...,d . For p 1, x := ( d x p)1/p is the ℓ norm of x Rd.
{ } ≥ k kp i=1| i | p ∈
The Frobenius norm of the matrix A is denoted by A = Tr[ATA] . The non-negative (resp.
k
kFP
positive)definiteconeofsymmetricmatricesisgivenbySd (resp. Sd ). Foragivenpositive-definite
+ p ++
matrix S Sd , the Mahalanobis norm of x Rd is given by x := S 1/2x = (x S 1x)1/2,
∈ ++ ∈ k kS k − k2 ⊤ −
where S1/2 is the symmetric square root of S. The matrix W := [w ,...,w ] Rd r is comprised
1 r ×
∈
from the column vectors w Rd. For a real symmetric matrix S Sd, λ (S) is the ith
i i [r] i
{ }∈ ⊂ ∈
largest eigenvalue, so that λ (S) λ (S) λ (S) λ (S) = λ (S), and in accordance,
max 1 2 d min
≡ ≥ ≥ ··· ≥
v (S) denote an eigenvector corresponding to λ (S) (these are unique if there are no two equal
i i
eigenvalues, and otherwise arbitrarily chosen, while satisfying orthogonality v v = v ,v = δ ).
i⊤ j
h
i j
i
ij
Similarly, Λ(S) := diag(λ (S),λ (S), ,λ (S)) and V(S) := [v (S),v (S), ,v (S)], so that
1 2 d 1 2 d
··· ···
S = V(S)Λ(S)V (S) is an eigenvalue decomposition. For j i, V := [v ,...,v ] R(j i+1) d is
⊤ i:j i j − ×
≥ ∈
the matrix comprised of the columns indexed by i,...,j . The vector e Rd is the ith standard
i
{ } ∈
basis vector, that is, e := [ 0,...0 ,1, 0,...0 ] . Random quantities (scalars, vectors, matrices,
i ⊤
i 1terms d i terms
etc.) are denoted by boldface− letters. F− or example, x Rd is a random vector that takes values
| {z } | {z } ∈
x Rd andR Rd r is arandommatrix. Theprobability law ofarandom element xis denoted by
×
∈ ∈
L(x). The probability of the event in some given probability space is denoted by P[ ] (typically
E E
16understood from context). The expectation operator is denoted by E[]. The indicator function
·
is denoted by , and the Kronecker delta is denoted by δ := i = j . We do not make a
ij
{·} { }
distinction between minimum and infimum (or maximum and supremum) as arbitrarily accurate
approximation issufficient forthedescriptionoftheresults inthis paper. Thebinary KLdivergence
between p ,p (0,1) is denoted as
1 2
∈
p 1 p
1 1
D (p p ) := p log +(1 p )log − . (28)
KL 1 2 1 1
|| p − 1 p
2 2
−
D Useful mathematical results
In this section we provide several simplified versions of mathematical results that are used in the
proofs. The following well-known result is about the optimal low-rank approximation to a given
matrix:
Theorem 10 (Eckart-Young-Mirsky [Wainwright, 2019, Example 8.1] [Vershynin, 2018, Section
4.1.4]). For a symmetric matrix S Sd
∈
S S min S S (29)
k k − kF ≤ S′ Sd:rank(S′) k − ′ F
∈ ≤
(cid:13) (cid:13)
where (cid:13) (cid:13)
S = λ (S) v (S)v (S) (30)
k i
·
i i⊤
i [k]
X∈
(more generally, this is true for any unitarily invariant norm).
We next review a simplified version of variational characterizations of eigenvalues of symmetric
matrices:
Theorem 11 (Rayleigh quotient [Horn and Johnson, 2012, Theorem 4.2.2]). For a symmetric ma-
trix S Sd
∈
x Sx
⊤
λ (S) = max . (31)
1 x=0 x 2
6 k k2
Theorem 12 (Courant–Fisher variational characterization [Horn and Johnson, 2012, Theorem
4.2.6]). For a symmetric matrix S Sd, k [d], and a subspace T of Rd
∈ ∈
x Sx x Sx
⊤ ⊤
λ (S) = min max = max min . (32)
k T:dim(T)=kx ∈T \{0 } kx k2 2 T:dim(T)=d −k+1x ∈T \{0 } kx k2 2
Theorem 13 (Fan’s variational characterization [Horn and Johnson, 2012, Corollary 4.3.39.]).
For a symmetric matrix S Sd and k [d]
∈ ∈
λ (S)+ +λ (S)= min Tr[U SU] (33)
1 k ⊤
··· U ∈Rd×k:U⊤U=Ik
and
λ (S)+ +λ (S) = max Tr[U SU]. (34)
d k+1 d ⊤
− ··· U ∈Rd×k:U⊤U=Ik
We will use the following celebrated result from convex analysis.
Theorem 14 (Carathéodory’s theorem [Bertsekas et al., 2003, Prop. 1.3.1]). Let Rd be non-
A ⊂
empty. Then, any point a in the convex hull of can be written as a convex combination of at most
A
d+1 points from .
A
17
1 1E The linear MSE setting: additions and proofs
E.1 The standard principal component setting
In order to highlight the formulation proposed in this paper, we show, as a starting point, that
the well known PCA solution of representing x Rd with the top r eigenvectors of the covariance
∈
matrix of x can be obtained as a specific case of the regret formulation. In this setting, we take
= I , and so y = x with probability 1. In addition, the predictor class is a linear function
d
F { } Q
from the representation dimension r back to the features dimension d.
Proposition 15. Consider the linear MSE setting, with the difference that the response is y Rd,
∈
the loss function is the squared Euclidean norm loss(y ,y ) = y y 2, and the predictor is
1 2 1 2
k − k
Q(z) = Q z Rd for Q Rr d. Assume = I so that y = x with probability 1. Then,
⊤ × d
∈ ∈ F { }
d
regret ( Σ ) = regret ( Σ ) = λ (Σ ), (35)
pure x mix x i x
F | F |
i=r+1
X
and an optimal representation is R = V (Σ ).
1:r x
TheresultofProposition15verifiesthattheminimaxandmaximinformulations indeedgeneral-
izethestandardPCAformulation. TheproofisstandardandfollowsfromtheEckart-Young-Mirsky
theorem, which determines the best rank r approximation in the Frobenius norm.
Proof of Proposition 15. Since = I is a singleton, there is no distinction between pure and
d
F { }
mixed minimax regret. It holds that
regret(R,f) =E x Q R x 2 (36)
⊤ ⊤
k − k
h i
where A = Q R Rd d is a rank r matrix. For any A Rd d
⊤ ⊤ × ×
∈ ∈
E x Ax 2 =E x x x Ax x A x+x A Ax (37)
⊤ ⊤ ⊤ ⊤ ⊤ ⊤
k − k − −
h i
(cid:2) (cid:3) =Tr Σ AΣ A Σ +A AΣ (38)
x x ⊤ x ⊤ x
− −
h 2 i
1/2 1/2
= Σ Σ A (39)
x x
− F
(cid:13) 2 (cid:13)
=(cid:13)Σ1/2 B ,(cid:13) (40)
(cid:13) x (cid:13)
− F
(cid:13) (cid:13)
(cid:13) (cid:13)
where B := Σ1/2 A. By the classic Eck(cid:13)art-Young(cid:13)-Mirsky theorem [Wainwright, 2019, Example 8.1]
x
[Vershynin, 2018, Section 4.1.4] (see Appendix D), the best rank r approximation in the Frobenius
norm is obtained by setting
r r
1/2
B = λ (Σ ) v v = λ (Σ ) v v (41)
∗ i x
·
i i⊤ i x
·
i i⊤
i=1 i=1
X Xp
1/2 1/2
where v v (Σ )= v (Σ ) is the ith eigenvector of Σ (or Σ ). Then, the optimal A is
i i x i x x x
≡
r r r
1/2 1/2
A
∗
= λ i(Σ x) ·Σ−x v iv
i⊤
= λ i(Σ x) ·Σ−x v iv
i⊤
= v iv i⊤, (42)
i=1 i=1 i=1
Xp Xp X
181/2
since v i is also an eigenvector of Σ x− . Letting R = U(R)Σ(R)V ⊤(R) and Q = U(Q)Σ(Q)V ⊤(Q)
be the singular value decomposition of R and Q, respectively, it holds that
Q R = V(Q)Σ (Q)V(Q)V(R)Σ (R)U (R). (43)
⊤ ⊤ ⊤ ⊤ ⊤
Setting V(Q) = V(R) = I , and Σ (Q) = Σ(R) Rd r to have r ones on the diagonal (and all
r ⊤ ×
∈
other entries are zero), as well as U(Q) = U(R) to be an orthogonal matrix whose first r columns
are v results that Q R = A , as required.
i i [r] ⊤ ⊤ ∗
{ }∈
E.2 Proofs of pure and mixed minimax representations
Before the proof of Theorem 2, we state a simple and useful lemma, which provides the pointwise
value of the regret and the optimal linear predictor for a given representation and response.
Lemma 16. Consider the representation z = R x Rr. It then holds that
⊤
∈
2
regret(R,f P ) = minE f x+n q z (44)
x ⊤ ⊤
| q Rr −
∈ (cid:20) (cid:16) (cid:17) (cid:21)
= E E[n2 x] +f Σ Σ R(R Σ R) 1R Σ f. (45)
⊤ x x ⊤ x − ⊤ x
| −
(cid:16) (cid:17)
(cid:2) (cid:3)
Proof. The orthogonality principle states that
E f x+n q z z = 0 (46)
⊤ ⊤ ⊤
− ·
h(cid:16) (cid:17) i
must hold for the optimal linear estimator. Using z = R x and taking expectations leads to the
⊤
standard least-squares (LS) solution
q = (R Σ R) 1RTΣ f, (47)
LS ⊤ x − x
assuming that R Σ R is invertible (which we indeed assume as if this is not the case, the repre-
⊤ x
sentation can be reduced to a dimension lower than r in a lossless manner). The resulting regret of
R is thus given by
2
regret(R,f P ) = E f x+n q z (48)
x ⊤ L⊤S
| −
(cid:20) (cid:21)
(cid:16) (cid:17)
( =a) E f x+n ⊤ f x+n q z (49)
⊤ ⊤ L⊤S
−
(cid:20) (cid:21)
(cid:16) (cid:17) (cid:16) (cid:17)
2
= E f x+n f x+n ⊤q z (50)
⊤ ⊤ L⊤S
−
(cid:20) (cid:21)
(cid:16) (cid:17) (cid:16) (cid:17)
( =b) E E[n2 x] +f Σ f E x fq R x (51)
⊤ x ⊤ L⊤S ⊤
| −
h i
= E (cid:2)E[n2 x](cid:3)+f Σ f Tr fq R Σ (52)
⊤ x L⊤S ⊤ x
| −
= E(cid:2)E[n2 x](cid:3)+f Σ f q h R Σ f i (53)
⊤ x L⊤S ⊤ x
| −
( =c)E(cid:2)E[n2 x](cid:3) +f Σ Σ R(R Σ R) 1R Σ f, (54)
⊤ x x ⊤ x − ⊤ x
| −
(cid:16) (cid:17)
(cid:2) (cid:3)
where (a) follows from the orthogonality principle in (46), (b) follows from the tower property of
conditional expectation and since E[xn] = E[x E[n x]] = 0, and (c) follows by substituting q
LS
· |
from (47).
19We may now prove Theorem 2.
Proof of Theorem 2. For any given f, the optimal predictor based on x Rd achieves average loss
∈
of
2
regret(R = I ,f P ) = minE f x+n q x = E E[n2 x] (55)
d x ⊤ ⊤
| q Rd − |
∈ (cid:20) (cid:16) (cid:17) (cid:21)
(cid:2) (cid:3)
(obtained by setting R = I in Lemma 16 so that z = x). Hence, the resulting regret of R over an
d
adversarial choice of f is
S
∈ F
2
maxregret(R,f)=maxE f x+n q z E E[n2 x]
⊤ L⊤S
f ∈FS f ∈F (cid:20)(cid:12) − (cid:12) (cid:21)− |
( =a) max(cid:12) (cid:12)f
⊤
Σ
x
Σ xR(R(cid:12) (cid:12)⊤Σ xR)(cid:2) −1R ⊤Σ
x
(cid:3) f (56)
f ∈FS
(cid:16)
−
(cid:17)
( =b) max f˜ S1/2Σ S1/2 S1/2Σ R(R Σ R) 1R Σ S1/2 f˜ (57)
⊤ x x ⊤ x − ⊤ x
f˜: f˜ 2 1 −
k k2≤ (cid:16) (cid:17)
( =c) λ S1/2Σ S1/2 S1/2Σ R(R Σ R) 1R Σ S1/2 (58)
1 x x ⊤ x − ⊤ x
−
= λ (cid:16) S1/2Σ1/2 I Σ1/2 R(R Σ R) 1R Σ1/2 Σ1/2(cid:17) S1/2 (59)
1 x d x ⊤ x − ⊤ x x
−
( =d) λ h S1/2Σ1/2(cid:16) I R˜(R˜ R˜) 1R˜ Σ1/2 S1/2 (cid:17) i (60)
1 x d ⊤ − ⊤ x
−
( =e) λ h I R˜(R(cid:16) ˜ R˜) 1R˜ Σ1/2 SΣ(cid:17) 1/2 I R˜i (R˜ R˜) 1R˜ , (61)
1 d ⊤ − ⊤ x x d ⊤ − ⊤
− −
h(cid:16) (cid:17) (cid:16) (cid:17)i
where (a) follows from Lemma 16, (b) follows by letting f˜:= S 1/2f and recalling that any f
−
∈ F
must satisfy f 2 1, (c) follows from the Rayleigh quotient theorem [Horn and Johnson, 2012,
k kS≤
Theorem 4.2.2] (see Appendix D), (d) follows by letting R˜ := Σ1/2 R, and (e) follows since I
x d
−
R˜(R˜ R˜) 1R˜ is an orthogonal projection (idempotent and symmetric matrix) of rank d r.
⊤ − ⊤
−
Now, to find the minimizer of max regret(R,f) over R, we note that
f ∈FS
λ I R˜(R˜ R˜) 1R˜ Σ1/2 SΣ1/2 I R˜(R˜ R˜) 1R˜
1 d ⊤ − ⊤ x x d ⊤ − ⊤
− −
( =a)h(cid:16) max u I R˜(R˜(cid:17) R˜) 1R˜ (cid:16) Σ1/2 SΣ1/2 I R˜(R˜(cid:17)i R˜) 1R˜ u (62)
⊤ d ⊤ − ⊤ x x d ⊤ − ⊤
u: u 2=1 − −
k k (cid:16) (cid:17) (cid:16) (cid:17)
(b) 1/2 1/2
= max u Σ SΣ u (63)
⊤ x x
u: u 2=1,R˜⊤u=0
k k
(c)
1/2 1/2
min max u Σ SΣ u (64)
⊤ x x
≥ :dim( )=d ru: u 2=1,u
S S − k k ∈S
(d) 1/2 1/2
= λ Σ SΣ , (65)
r+1 x x
(cid:16) (cid:17)
where (a) follows again from the Rayleigh quotient theorem [Horn and Johnson, 2012, Theorem
4.2.2], (b) follows since I R˜(R˜ R˜) 1R˜ is an orthogonal projection matrix, and so we may write
d ⊤ − ⊤
−
u = u +u so that u 2+ u 2= 1 and R˜ u = 0; Hence replacing u with u only increases
⊤
the va⊥ lue ofk the maximk u⊥ mk , (ck ) fk ok llows by setting⊥ to be a d r dimensional subs⊥ pace of Rd, and
S −
(d) follows by the Courant–Fischer variational characterization [Horn and Johnson, 2012, Theorem
4.2.6] (see Appendix D). The lower bound in (c) can be achieved by setting the r columns of
R˜ Rd r to bethe top eigenvectors v (Σ1/2 SΣ1/2 ) . This leads tothe minimax representation
× i x x i [r]
∈ { }∈
20R˜ . From (61), the worst case f˜is the top eigenvector of
∗
I R˜ ((R˜) R˜ ) 1(R˜) Σ1/2 SΣ1/2 I R˜ ((R˜) R˜ ) 1(R˜) . (66)
d ∗ ∗⊤ ∗ − ∗⊤ x x d ∗ ∗⊤ ∗ − ∗⊤
− −
(cid:16) (cid:17) (cid:16) (cid:17)
1/2 1/2
This is a symmetric matrix, whose top eigenvector is the (r+1)th eigenvector v (Σ SΣ ).
r+1 x x
We next prove Theorem 3.
Proof of Theorem 3. We follow the proof strategy mentioned after the statement of the theorem.
We assume that n 0 with probability 1, since, as for the pure minimax regret, this unavoidable
≡
additive term of E E[n2 x] to the loss does not affect the regret.
|
The minimax problem – adirect computation: Asinthederivations leadingto(61),theminimax
(cid:2) (cid:3)
regret in (2) is given by
regret ( Σ )
mix S x
F |
= min maxE[regret(R,f Σ )] (67)
x
L(R) ∈P( R)f ∈FS |
= min max E f˜ S1/2Σ S1/2 S1/2Σ R(R Σ R) 1R Σ S1/2 f˜ (68)
⊤ x x ⊤ x − ⊤ x
L(R) ( )f˜: f˜ 2 1 −
∈P R k k2≤ h (cid:16) (cid:17) i
= min max f˜ ⊤S1/2Σ1 x/2E I
d
R˜(R˜ ⊤R˜) −1R˜ ⊤ Σ1 x/2 S1/2f˜ (69)
L(Σ− x1/2R˜) ∈P( R)f˜: kf˜ k2 2≤1
h
−
i
= min λ
1
S1/2Σ1 x/2E I
d
R˜(R˜ ⊤R˜) −1R˜ ⊤ Σ1 x/2 S1/2 , (70)
L(Σ− x1/2R˜) ∈P( R)
(cid:16) h
−
i (cid:17)
where R˜ = Σ1/2 R. Determining the optimal distribution of the representation directly from this
x
expressionseemstobeintractable. Wethusnextsolvethemaximinproblem, andthenreturntothe
maximin problem (70), set a specific random representation, and show that it achieves the maximin
value. This, in turn, establishes the optimality of this choice.
The maximin problem: Let an arbitrary L(f) be given. Then, taking the expectation of the
regret over the random choice of f, for any given R ,
∈ R
E[regret(R,f)]( =a) E Tr S1/2Σ xS1/2 S1/2Σ xR(R ⊤Σ xR) −1R ⊤Σ xS1/2 f˜f˜ ⊤ (71)
−
h h(cid:16) (cid:17) ii
( =b) Tr S1/2Σ S1/2 S1/2Σ R(R Σ R) 1R Σ S1/2 Σ˜ (72)
x x ⊤ x − ⊤ x f
−
= Tr h Σ˜(cid:16)1/2 S1/2Σ S1/2 S1/2Σ R(R Σ R) 1R Σ (cid:17) S1/2i Σ˜1/2 (73)
f x − x ⊤ x − ⊤ x f
( =c) Trh Σ˜1/2 S(cid:16) 1/2Σ1/2 I R˜(R˜ R˜) 1R˜ Σ1/2 S1/2Σ˜1/2 (cid:17) i (74)
f x d − ⊤ − ⊤ x f
= Tr h I R˜(R˜ R˜)(cid:16) 1R˜ Σ1/2 S1/2Σ˜ S(cid:17) 1/2Σ1/2 i (75)
⊤ − ⊤ x f x
−
( =d) Trh(cid:16) I R˜(R˜ R˜) 1R˜ (cid:17) Σ1/2 S1/2Σ˜ S1/2Σ1/2i I R˜(R˜ R˜) 1R˜ (76)
⊤ − ⊤ x f x ⊤ − ⊤
− −
(e) h(cid:16) (cid:17) (cid:16) (cid:17)i
min Tr W Σ1/2 S1/2Σ˜ S1/2Σ1/2 W (77)
⊤ x f x
≥ W ∈Rd×(d−r):W⊤W=Id−r
h i
d
( =f) λ (Σ1/2 S1/2Σ˜ S1/2Σ1/2 ) (78)
i x f x
i=r+1
X
d
= λ Σ˜ S1/2Σ S1/2 , (79)
i f x
i= Xr+1 (cid:16) (cid:17)
21where (a) follows from Lemma 16 and setting f˜ := S 1/2f, (b) follows by setting Σ˜ Σ =
− f
≡
f˜
E[f˜f˜ ⊤], (c) follows by setting R˜ := Σ1 x/2 R, (d) follows since I R˜(R˜ ⊤R˜) −1R˜
⊤
is an orthogonal
−
projection (idempotent and symmetric matrix) of rank d r, (e) follows since any orthogonal pro-
−
jection can be written as WW where W Rd (d r) is an orthogonal matrix W W = I , (f)
⊤ × − ⊤ d r
∈ −
follows from Fan [1949]’s variational characterization [Horn and Johnson, 2012, Corollary 4.3.39.]
(see Appendix D). Equality in (e) can be achieved by letting R˜ be the top r eigenvectors of
Σ1/2 S1/2Σ˜ S1/2Σ1/2 .
x f x
The next step of the derivation is to maximize the expected regret over the probability law
of f (or f˜). Evidently, E[regret(R,f)] = d λ (Σ˜ S1/2Σ S1/2) only depends on the random
i=r+1 i f x
function f˜ via Σ˜ . The covariance matrix Σ˜ is constrained as follows. Recall that f is supported
f Pf
on := f Rd: f 2 1 (see (4)), and let Σ = E[ff ] be its covariance matrix. Then, it
FS { ∈ k kS≤ } f ⊤
must hold that Tr[S 1Σ ] 1. Then, it also holds that
− f
≤
1 Tr[S 1Σ ] = Tr E[S 1ff ] (80)
− f − ⊤
≥
h i
= E f S 1f (81)
⊤ −
h i
= E f˜ ⊤f˜ (82)
h i
= Tr Σ˜ (83)
f
h i
where Σ˜ = S 1/2Σ S 1/2. Conversely, given any covariance matrix Σ˜ Sd such that Tr[Σ˜ ]
f − f − f ∈ ++ f ≤
1 there exists a random vector f supported on such that
S
F
E[ff ] = S1/2Σ˜ S 1/2. (84)
⊤ f −
We show this by an explicit construction. Let Σ˜ = V˜ Λ˜ V˜ be the eigenvalue decomposition of
f f f f⊤
Σ˜ , and, for brevity, denote by λ˜ λ (Σ˜ ) the diagonal elements of Λ˜ . Let q be a set of
inf
dependent and identically
(IID)i d≡ istri ibuf
ted random variables, so
thatf
q is
R{ adi} emi ∈[ ad c]
her, that is
i
P[q = 1] = P[q = 1]= 1/2. Define the random vector
i i −
g := q λ˜ , ,q λ˜ ⊤ . (85)
1· 1 ··· d· d
(cid:18) q q (cid:19)
The constraint Tr[Σ˜ ] 1 implies that λ˜ 1 and so g 2= d λ˜ 1 with probability 1.
Then, letting f˜ = V˜f g i≤ t also holds that f˜ 2i =≤ g 2 1 wk ithk probabi= il1 ityi 1≤ , and furthermore,
f Pk k2 k k2≤ P
E f˜f˜ ⊤ = V˜ fE gg ⊤ V˜ f⊤ = V˜ fΛ˜ fV˜ f⊤ = Σ˜ f. (86)
h i h i
Consequently, letting f = S1/2f assures that f = f˜ 1 and E[ff ] = S1/2Σ˜ S 1/2, as
S 2 ⊤ f −
k k k k ≤
was required to obtain. Therefore, instead of maximizing over probability laws on ( ), we may
S
P F
equivalently maximize over Σ˜ Sd such that Tr[Σ˜ ] 1, i.e., to solve
f ∈ ++ f ≤
d
regret ( Σ )= max λ (Σ˜ S1/2Σ S1/2). (87)
mix S x i f x
F | Σ˜ f:Tr[Σ˜ f] ≤1
i= Xr+1
The optimization problem in (87) is solved in Lemma 17, and is provided after this proof. Setting
Σ = S1/2Σ S1/2 in Lemma 17, and letting λ λ (S1/2Σ S1/2), the solution is given by
x i i x
≡
ℓ r
∗
− (88)
ℓ∗ 1
i=1 λi
P
22where ℓ [d] [r] satisfies
∗
∈ \
ℓ∗
ℓ r 1 ℓ r
∗ ∗
− − . (89)
λ ℓ∗ ≤ λ i ≤ λ ℓ∗+1
i=1
X
Lemma 17 also directly implies that an optimal Σ˜ is given as in (10). The value in (88) is exactly
f
regret ( Σ ) claimed by the theorem, and we next show it is indeed achievable by a properly
mix S x
F |
constructed random representation.
Theminimaxproblem –asolutionviathemaximincertificate: Giventhevalueoftheregretgame
inmixedstrategiesfoundin(88),wemayalsofindaminimax representation inmixedstrategies. To
this end, we return to the minimax expression in (70), and propose a random representation which
achieves the maximin value in (88). Note that for any given R˜, the matrix I R˜(R˜ R˜) 1R˜ is an
d ⊤ − ⊤
−
orthogonal projection, that is, a symmetric matrix whose eigenvalues are alleither 0 or1, and it has
atmostreigenvaluesequaltozero. WedenoteitseigenvaluedecompositionbyI R˜(R˜ R˜) 1R˜ =
d ⊤ − ⊤
−
UΩU . Then, any probability law on R˜ induces a probability law on U and Ω (and vice-versa). To
⊤
1/2 1/2
findthemixedminimaxrepresentation, weproposesettingU = V(Σ SΣ ) V withprobability
x x
≡
1, that is, to be deterministic, and thus only randomize Ω. With this choice, and by denoting, for
brevity, Λ Λ(Σ1/2 SΣ1/2 )= Λ(S1/2Σ S1/2), the value of the objective function in (70) is given by
x x x
≡
λ S1/2Σ1/2 V E[Ω] V Σ1/2 S1/2
1 x ⊤ x
· ·
=(cid:16) λ E[Ω] V Σ1/2 SΣ1/2 V (cid:17) (90)
1 ⊤ x x
·
= λ (cid:16) (E[Ω] Λ). (cid:17) (91)
1
·
Now, the distribution of Ω is equivalent to a distribution on its diagonal, which is supported on the
finite set := a 0,1 d: a d r . Our goal is thus to find a probability law on a, supported
1
A { ∈ { } k k ≥ − }
on , which solves
A minmaxλ (E[Ω] Λ)= minmaxE[a ]λ (92)
1 i i
L(Ω) i [d] · L(a) i [d]
∈ ∈
where λ λ (S1/2Σ S1/2) are the diagonal elements of Λ. Consider ℓ , the optimal dimension
i i x ∗
≡
of the maximin problem, which satisfies (89). We then set a = = a = 1 to hold with
ℓ∗+1 d
···
probability 1, and so it remains to determine the probability law of a := (a ,...,a ), supported
1 ℓ∗
on ˜ := a 0,1 ℓ∗ : a ℓ r . Clearly, reducing a only reduces the objective function
1 ∗ 1
maxA E{ [a∈ λ ]{ , an} d sok wk e ≥ may − in f} act assume that a isk suk pported on := a 0,1 ℓ∗ : a =
i [d] i i 1
ℓ
r∈
, a finite subset of cardinality
ℓ∗
. Suppose that we find a
probaA bility{ law∈ L{ (a)} suppk ork
ted
∗ − } r
on such that
A (cid:0) (cid:1) 1/λ
E[a ] = (ℓ r) i := b , (93)
i ∗ − · ℓ∗ 1/λ i
i=1 i
for all i [ℓ ∗]. Then, since E[a i] = 1 for i [d] [ℓ ∗P]
∈ ∈ \
ℓ r
m
i
a [dx
]
E[a i]λ
i
= max
(
∗ ℓ∗−
1
,λ ℓ∗+1, ···,λ
d
)
(94)
∈ i=1 λi
Pℓ r
∗
= max ℓ∗−
1
,λ ℓ∗+1 (95)
( )
i=1 λi
( =∗) ℓ ∗ −r P, (96)
ℓ∗ 1
i=1 λi
P
23where ( ) follows from the condition on ℓ in the right inequality of (89). This proves that such
∗
∗
probability law achieves theminimax regret inmixedstrategies. This lasttermisregret ( Σ )
mix S x
F |
claimed by the theorem. It remains to construct L(a) which satisfies (93). To this end, note that
the set
:= c
[0,1]ℓ∗
: c =ℓ r (97)
1 ∗
C ∈ k k −
n o
is convex and compact, and is the set of its extreme points ( is the convex hull of ). Letting
b = (b ,...,b ) as
denotedA
in (93), it holds that b 0 and
bC ℓ∗
is a
non-decreasinA
g sequence.
1 ℓ∗ ⊤ i ≥ { i }i=1
Using the condition on ℓ in the left inequality of (89), it then holds that
∗
1/λ
ℓ∗
b b = (ℓ r) 1. (98)
1 ≤ ··· ≤ ℓ∗ ∗ − · ℓ∗ 1/λ ≤
i=1 i
Hence, b . By Carathéodory’s theorem [Bertsekas ePt al., 2003, Prop. 1.3.1] (see Appendix D),
any
point∈ inC
side a convex compact set in
Rℓ∗
can be written as a convex combination of at most
ℓ +1 extreme points. Thus, there exists p such that p [0,1] and p = 1 so that
∗ { a }a a ∈ a a
b = p a,andmoreoverthesupportofp
ha∈ sA
cardinalityatmostℓ +1.
LetA∈A
0,1
ℓ∗
be
a a · a ∗ P ∈ { } ×|A|
∈A
such that its jth column is given by the jth member of (in an arbitrary order). Let p [0,1]
P A ∈ |A|
be a vector whose jth element corresponds to the jth member of . Then, p is the solution to
A
Ap = b, and as claimed above, such a solution with at most ℓ +1 nonzero entries always exists.
∗
Setting a = (a, 1...,1 ) with probability p then assures that (93) holds, as was required to be
a
d ℓ∗ terms
proved. −
Given the abo| ve{ ,z w} e observe that setting R˜ as in the theorem induces a distribution on Ω for
which the random entries of its diagonal a satisfy (93), and thus achieve regret ( Σ ).
mix S x
F |
We next turn to complete the proof of Theorem 3 by solving the optimization problem in (87).
Assume that Σ Sd is a strictly positive covariance matrix Σ 0, and consider the optimization
∈ ++ ≻
problem
d
v = max λ (Σ˜ Σ)
r∗ i f
Σ˜ f∈Sd
+i= Xr+1
subjectto Tr[Σ˜ ] 1 (99)
f
≤
forsomer [d 1]. Notethattheobjectivefunctionreferstothemaximization ofthed r minimal
eigenvalues∈ of Σ− 1/2Σ˜ Σ1/2. −
f
Lemma 17. Let
ℓ r
a := − . (100)
ℓ ℓ 1
i=1 λi(Σ)
The optimal value of (99) is v
∗
= max
[d]
[r]a
ℓ
Pand ℓ
∗
argmax
[d]
[r]a
ℓ
iff
\ ∈ \
ℓ∗
ℓ r 1 ℓ r
∗ ∗
− − . (101)
λ (Σ) ≤ λ (Σ) ≤ λ (Σ)
ℓ∗ i ℓ∗+1
i=1
X
An optimal solution is
ℓ∗
1
−1
1 1
Σ˜ = V(Σ)diag ,..., ,0, ,0 V(Σ) . (102)
∗f
"
i=1
λ i(Σ)
#
· (cid:18)λ 1(Σ) λ ℓ∗(Σ) ··· (cid:19)·
⊤
X
24Proof. Let Σ
f
= Σ1/2Σ˜ fΣ1/2, let Σ
f
= U fΛ fU⊤
f
be its eigenvalue decomposition, and, for brevity,
denote λ λ (Σ ). Then, the trace operation appearing in the constraint of (99) can be written
i i f
≡
as
Tr[Σ˜ ] = Tr Σ 1/2Σ Σ 1/2 (103)
f − f −
h i
= Tr Σ −1/2U fΛ fU⊤ fΣ −1/2 (104)
h d i
= Tr Σ 1/2 λ u u Σ 1/2 (105)
− i i ⊤i −
" ! #
i=1
X
d
= λ u Σ 1u (106)
i
·
⊤i − i
Xi=1 (cid:16) (cid:17)
d
= c λ , (107)
i i
i=1
X
whereu = v (U )(thatis,theithcolumnofU ),andc := u Σ 1u (whichsatisfiesc > 0). Thus,
i i f f i ⊤i − i i
the optimization problem in(99)over Σ˜ is equivalent toan optimization problem over λ ,u ,
f i i i [d]
{ }∈
given by
d
v = max λ
r∗ i
{ui,λi}i∈[d]i=
Xr+1
d
subjectto c λ 1,
i i
≤
i=1
X
c = u Σ 1u ,
i ⊤i − i
u u = δ ,
⊤i j ij
λ λ λ 0. (108)
1 2 d
≥ ≥ ··· ≥ ≥
To solve the optimization problem (108), let us fix feasible u , so that c are fixed too.
i i [d] i i [d]
{ }∈ { }∈
This results the problem
d
v ( u ) v ( c ) = max λ
r∗
{
i
} ≡
r∗
{
i
}
i
{λi}i∈[d]i=
Xr+1
d
subjectto c λ 1,
i i
≤
i=1
X
λ λ λ 0. (109)
1 2 d
≥ ≥ ··· ≥ ≥
The objective function of (109) is linear in λ and its constraint set is a convex bounded
i i [d]
{ }∈
polytope. So the solution to (109) must be obtained on the boundary of the constraint set. Clearly,
cth one so tp rati im ntal v dalue
c
s λatisfi 1es isv sr∗ a( t{ ic si fi} e) d≥ w0 it, ha en qd ut ah liu tys .th Ine ds eo el du ,ti io fn th{ iλ s∗ i is}i n∈ o[d t] tm hu es ct ab see to hb et nai on ne ed mw ah yen sct ah le
e
i=1 i i ≤
all λ∗ by a constant larger than 1, and obtain larger value of the objective, while still satisfying the
i P
constraint.
To find the optimal solution to (109), we consider feasible points for which ℓ := max i
{ ∈
[d]:λ
i
> 0) is fixed. Let {λ∗
i}i ∈[d]
be the optimal solution of (109), under the additional constraint
25that λ
ℓ+1
=
···
= λ
d
= 0. We next prove that λ∗
1
=
···
= λ∗
ℓ
must hold. To this end, assume
by contradiction that there exists j [ℓ] so that λ∗ > λ∗ > 0. There are two cases to consider,
∈ j −1 j
to wit, whether j 1 < r + 1 and so only λ appears in the objective of (109), or, otherwise,
j
−
j 1 r +1 and then λ +λ appears in the objective of (109). Assuming the first case, let
j 1 j
− ≥ −
α = λ∗
j
1c
j
1+λ∗ jc
j
and consider the optimization problem
− −
max λˆ
j
λˆ j−1,λˆ
j
subjectto λˆ c +λˆ c = α,
j 1 j 1 j j
− −
λˆ λˆ > 0. (110)
j 1 j
− ≥
It is easy to verify that the optimum of this problem is λˆ
∗j −1
= λˆ
∗j
= cj−α 1+cj. Thus, if λ∗
j −1
> λ∗
j
then one can replace this pair with λ∗
j 1
= λ∗
j
= λˆ
∗j 1
= λˆ
∗j
so that the value of the constraint
d i=1λ ic
i
remains the same, and thus − (λ∗ 1, ···,λˆ
∗j
1,λˆ− ∗j,λ∗ j+1,...λ∗ d) is a feasible point, while the
objective function value of (109) is smaller; a con−tradiction. Therefore, it must hold for the first
P
case that λ∗
j 1
= λ∗ j. For the second case, in a similar fashion, let now α = λ∗
j
1c
j 1
+λ∗ jc j, and
− − −
consider the optimization problem
max λˆ +λˆ
j j 1
λˆ j−1,λˆ
j
−
subjectto λˆ c +λˆ c = α,
j 1 j 1 j j
− −
λˆ λˆ > 0. (111)
j 1 j
− ≥
Thesolutionforthisoptimization problemisatoneofthetwo extremepoints ofthefeasibleinterval
for λˆ j. Since λ
∗j
> 0 was assumed it therefore must hold that λˆ
∗j 1
= λˆ ∗j, and hence also λ∗
j 1
= λ∗ j.
Thus, λ < λ leads to a contradiction. From the above, we d−educe that the optimal sol−ution of
∗j 1 ∗j
−
(109) under the additional constraint that λ = = λ = 0 is
ℓ+1 d
···
1
λ∗ = = λ∗ = (112)
1 ··· ℓ ℓ c
i=1 i
λ∗
ℓ+1
=
···
= λ∗
d
= P0, (113)
and that the optimal value is ℓ r . Since ℓ [d] [r] can be arbitrarily chosen, we deduce that
Pℓ i− =1ci ∈ \
the value of (109) is
ℓ r
v ∗( {c
i
}) = ℓm [da ]x
[r]
ℓ−
c
. (114)
∈ \ i=1 i
For any given ℓ [d] [r], we may now optimize ovPer u
i
, which from (114) is equivalent to
minimizing ℓ c∈ . It\ holds that { }
i=1 i
P ℓ ℓ
min c = min u Σ 1u (115)
i ⊤i − i
{ui}
Xi=1
{ui:u⊤
i
uj=δij}Xi=1
ℓ
= min Tr Σ 1 u u (116)
− i ⊤i
{ui:u⊤
i
uj=δij} "
Xi=1
#
( =a) min Tr Σ 1U`U` (117)
− ⊤
U` ∈Rd×ℓ:U`⊤U`=Iℓ
h i
26= min Tr U` Σ 1U` (118)
⊤ −
U` ∈Rd×ℓ:U`⊤U`=Iℓ
h i
ℓ
(b) 1
= , (119)
λ (Σ)
i
i=1
X
where in (a) U` Rd ℓ whose ℓ columns are u and U` U` = I , and in (b) we have used Fan
× i i [ℓ] ⊤ ℓ
∈ { }∈
[1949]’s variational characterization [Horn and Johnson,2012,Corollary 4.3.39.] (seeAppendix D).
Substituting back to (114) results that
ℓ r
v r∗ = ℓm [da ]x
[r] ℓ
−
1
= ℓm [da ]x [r]a ℓ. (120)
∈ \ i=1 λi(Σ) ∈ \
P
Let us denote that maximizer index by ℓ . Then, Fan’s characterization is achieved by setting
∗
U = V (so that the ℓ columns of U` are the ℓ eigenvectors v (Σ), corresponding to the ℓ largest
f ∗ ∗ i ∗
eigenvalues of Σ), so that
ℓ∗
1
−1
Σ∗
f
=
λ (Σ)
·V ·diag 1,...,1,0, ···,0 ·V ⊤, (121)
" i #
Xi=1 ℓ∗terms
 
and then | {z }
Σ˜
∗f
= Σ −1/2Σ∗ fΣ −1/2 (122)
ℓ∗
1
−1
= VΛ 1/2V V diag(1,...,1,0, ,0)V VΛ 1/2V (123)
− ⊤ ⊤ − ⊤
λ (Σ) · · ···
" i #
i=1
X
ℓ∗
1
−1
1 1
= V diag ,..., ,0, ,0 V (124)
⊤
"
i=1
λ i(Σ)
#
· · (cid:18)λ 1(Σ) λ ℓ∗(Σ) ··· (cid:19)·
X
as claimed in (102).
To complete the proof, it remains to characterize ℓ , which belongs to the set possible indices
∗
maximizing a . Since ℓ maximizes a it must be a local maximizer, that is, it must hold
ℓ ℓ [d] [r] ∗ ℓ
{ } ∈ \
that a a a . By simple algebra, these conditions are equivalent to those in (101). It
ℓ∗ 1 ℓ∗ ℓ∗+1
− ≤ ≥
remains to show that any ℓ [d] [r] which satisfies (101) has the same value, and thus any local
∈ \
maxima is a global maxima. We will show this by proving that the sequence a d is unimodal,
{ ℓ }ℓ=r
as follows. Let ∆ := a a be the discrete derivative of a , and consider the sequence
ℓ ℓ+1 ℓ ℓ ℓ [d]
− { } ∈
∆ . We show that as ℓ increases from r to d, ∆ is only changing its sign at most
ℓ ℓ [d] [r] ℓ ℓ [d] [r]
{ } ∈ \ { } ∈ \
once. To this end, we first note that
∆ =
ℓ+1 −r ℓ −r
=
ℓ i=1 λi(1 Σ) −(ℓ −r) λℓ+1 1(Σ)
. (125)
ℓ ℓ+1 1 − ℓ 1 P ℓ+1 1 ℓ 1
i=1 λi(Σ) i=1 λi(Σ) i=1 λi(Σ) i=1 λi(Σ)
P P h P ih P i
Since the denominator of (125) is strictly positive, it suffices to prove that the sequence comprised
of the numerator of (125), to wit ζ with
ℓ ℓ [d] [r]
{ } ∈ \
ℓ
1 1
ζ := (ℓ r) , (126)
ℓ
λ (Σ) − − λ (Σ)
i ℓ+1
i=1
X
27is only changing its sign at most once. Indeed, this claim is true because ζ = ℓ 1 > 0 and
r i=1 λi(Σ)
because ζ is a monotonic non-increasing sequence,
{ ℓ }ℓ ∈[d] \[r] P
1 1
ζ ζ = (ℓ r+1) 0. (127)
ℓ ℓ+1
− − λ (Σ) − λ (Σ) ≥
(cid:20) ℓ+2 ℓ+1 (cid:21)
Therefore, ζ has at most a single sign change (its has a positive value at ℓ = r and is
ℓ ℓ [d] [r]
monotonica{ lly} n∈ on-\ increasing with ℓ up to ℓ = d), and so is ∆ d . The single sign change
{ ℓ }ℓ=r
property of the finite difference ∆ d is equivalent to the fact that a d is unimodal. Thus,
{ ℓ }ℓ=r { ℓ }ℓ=r
any local maximizer of a is also a global maximizer.
ℓ
F The Hilbert space MSE setting
In this section, we show that the regret expressions in Section 3 can be easily generalized to an
infinite dimensional Hilbert space, for responses with noise that is statistically independent of the
features. We still assume the MSE loss function ( = R , and loss(y ,y ) = (y y )2), and that
1 2 1 2
Y −
the predictor is a linear function. However, we allow the the representation and response function
to be functions in a Hilbert space. As will be evident, the resulting regret is not very different from
the finite-dimensional case. Formally, this is defined as follows:
Definition 18 (The Hilbert space MSE setting). Assume that x P is supported on a compact
x
∼
subset Rd, and let L (P ) be the Hilbert space of functions from R such that E[f2(x)] =
2 x
X ⊂ X →
f2(x) dP < , with the inner product,
x
· ∞
X
R
f,g := f(x)g(x) dP (128)
x
h i ·
ZX
for f,g L (P ). Let φ (x) be an orthonormal basis for L (P ).
∈
2 x
{
j }∞j=1 2 x
A representation is comprised of a set of functions ψ L (P ), ψ : R, so that
i i [r] 2 x i
{ }∈ ⊂ X →
:= R(x) = (ψ (x),...,ψ (x)) Rr . (129)
1 r ⊤
R { ∈ }
Let λ j j N be a positive monotonic non-increasing sequence for which λ j 0 as j , and let
be th{ e s} et∈ of functions from R such that given f , the response is↓ given b→ y ∞ F
X → ∈ F
y = f(x)+n R (130)
∈
where
f2
f
∈
F{λj} := f(x)= ∞ f jφ j(x): {f j }j ∈N
∈
ℓ 2(N), ∞ λj
j
≤
1 , (131)
 Xj=1 Xj=1 
where n R is a homoscedastic noise that is statistically independent of x and satisfies E[n] = 0.
∈  
Infinite-dimensional ellipsoids such as naturally arise in reproducing kernel Hilbert spaces
F{λj}
(RKHS) [Wainwright, 2019, Chapter 12] [Shalev-Shwartz and Ben-David, 2014, Chapter 16], in
which λ is the eigenvalues of the kernel. In this case, the set = f: f 1 where
{
j
}
F{λi}
{ k kH≤ } k·kH
is the norm of the RKHS . For example, could be the first-order Sobolev space of functions
H H
with finite first derivative energy.
Let the set of predictor functions be the set of linear functions from Rd R, that is
→
r
:= Q(z) = q z = q ψ (x), q Rr . (132)
⊤ i i
Q { · ∈ }
i=1
X
28Wedenotethepure(resp. mixed)minimaxregretasregret ( , P )(resp. regret ( ,
pure
R
F{λj}
|
x mix
R
F{λj}
|
P )). We begin with pure strategies.
x
Theorem 19. For the Hilbert space MSE setting (Definition 18)
regret ( , P ) = λ . (133)
pure
R
F{λj}
|
x r+1
A minimax representation is
R (x) =(φ (x),...,φ (x)) , (134)
∗ 1 r ⊤
and the worst case response function is f = λ φ .
∗ r+1 r+1
·
We now turn to the minimax representatiopn in mixed strategies.
Theorem 20. For the Hilbert space MSE setting (Definition 18)
ℓ r
∗
regret ( , P )= − , (135)
mix R F{λj} | x ℓ∗ 1
i=1 λi
where ℓ is defined as (8) of Theorem 3 (with the replacePment d N ). Let b be an IID
∗
→
+
{
j }∞i=1
sequence of Rademacher random variables, P[b = 1] = P[b = 1] = 1/2. Then, a least favorable
i i
−
prior f is
∗
b 1 , 1 i ℓ
f ∗i =  i · qPℓ i=∗ 1 λ1 i ≤ ≤ ∗ , (136)
0, i ℓ +1

≥ ∗
and a law of minimax representation is to choose

R (x) = φ (x) r (137)
∗ { Ij }j=1
with probability p , j [
ℓ∗
], defined as in Theorem 3.
j ∈ r
(cid:0) (cid:1)
Discussion Despite having countably infinite possible number of representations, the optimal
representation only utilizes a finite set of orthogonal functions, as determined by the radius of
. The proof of Theorems 19 and 20 is obtained by reducing the infinite dimensional problem to
F{si}
a d-dimensional problem via an approximation argument, then showing the the finite dimensional
case is similar to the problem of Section 3, and then taking limit d .
↑∞
F.1 Proofs
Let us denote the d-dimensional slice of by
F{λj}
(d)
:= f(x) :f = 0 for all j d+1 . (138)
F {λj} ∈ F{λj} j ≥
n o
Further, let us consider the restricted representation class, in which the representation functions
ψ (t) belong to the span of the first d basis functions, that is
i
(d) := R(x) := ψ (x) span( φ ) for all i [r] . (139)
i i i [d]
R { ∈ R ∈ { }∈ ∈ }
Thefollowingpropositionimpliesthattheregretintheinfinite-dimensionalHilbertspaceisobtained
as the limit of finite-dimensional regrets, as the one characterized in Section 3:
29Proposition 21. It holds that
regret ( , P )= limregret ( (d), (d) P ) (140)
pure R F{λj} | x d pure R F {λj} | x
↑∞
and
regret ( , P ) = limregret ( (d), (d) P ). (141)
mix R F{λj} | x d mix R F {λj} | x
↑∞
Proof. Let c ij j N be the coefficients of the orthogonal expansion of ψ i, i [r], that is, ψ i =
∞j=1c ijφ j.{ W} it∈ h a slight abuse of notation, we also let c
i
:= (c i1,c i2...) ∈
∈
ℓ 2(N). We use a
sandwich argument. On one hand,
P
regret ( , P )= min max regret(R,f) (142)
pure
R
F{λj}
|
x
R ∈Rf
∈F{λj}
min max regret(R,f) (143)
≥ R ∈Rf ∈F{(d λ)
j}
( )
=∗ min max regret(R,f) (144)
R (d)f (d)
∈R ∈F{λj}
= regret ( (d), (d) P ), (145)
pure R F {λj} | x
(d)
where ( ) follows from the following reasoning: For any (R ,f ),
∗ ∈R ∈ F {λj}
2
d r
∞
regret(R,f)=minE f φ (x)+n q c φ (x) E n2 (146)
j j i ij j
q Rr  −  −
∈ j=1 j=1 i=1
X XX (cid:2) (cid:3)
  
d r
( =a) minE f φ (x) ∞ q c φ (x) (147)
j j i ij j
q Rr  − 
∈ j=1 j=1 i=1
X XX
d r 2 r  2
(b) ∞
= min f q c + q c , (148)
j i ij i ij
q Rr − ! !
∈ j=1 i=1 j=d+1 i=1
X X X X
where here (a) follows since the noise n is independent of x, and since, similarly to the finite-
dimensional case (Section 3), the prediction loss based on the features x is E[n2], for any given
∈ X
f , (b) follows from Parseval’s identity and the orthonormality of φ
j j
N. So,
∈F { } ∈
min max regret(R,f)
R ∈Rf ∈F{(d λ)
j}
d r 2 r 2
∞
= min max min f q c + q c . (149)
j i ij i ij
{cij}i∈[r],j∈Nf ∈F{(d λ) j}q ∈Rr
Xj=1
−
Xi=1
!
j= Xd+1 Xi=1
!
Evidently, since ∞j=d+1( r i=1q ic ij)2
≥
0, an optimal representation may satisfy that c
ij
= 0 for
all j d+1. Thus, the optimal representation belongs to (d).
≥ P P R
On the other hand,
regret ( , P ) = min max regret(R,f) (150)
pure
R
F{λj}
|
x
R ∈Rf
∈F{λj}
30min max regret(R,f) (151)
≤ R ∈R(d)f ∈F{λj}
( )
∗ min max regret(R,f)+λ (152)
d+1
≤ R (d)f (d)
∈R ∈F{λj}
= regret ( (d), (d) P )+λ , (153)
pure R F {λj} | x d+1
where ( ) follows from the following reasoning: For any (R (d),f ),
∗ ∈R ∈
F{λj}
2
r
∞ ∞
regret(R,f)=minE f φ (x)+n q c φ (x) E[n2] (154)
j j i ij j
q Rr  −  −
∈ j=1 j=1 i=1
X XX
 d r 2  
( =a) min f q c + ∞ f2 (155)
q Rr j − i ij ! j
∈ j=1 i=1 j=d+1
X X X
d r 2
(b)
min f q c +λ , (156)
j i ij d+1
≤ q Rr − !
∈ j=1 i=1
X X
where (a) follows similarly to the analysis made in the previous step, and (b) follows since for any
f it holds that
∈F{λj}
f2 f2
∞ f2 λ ∞ j λ ∞ j λ . (157)
j ≤ d+1 λ ≤ d+1 λ ≤ d+1
j j
j=d+1 j=d+1 j=1
X X X
Combining (145) and (153) and using λ 0 completes the proof for the pure minimax regret.
d+1
↓
The proof for the mixed minimax is analogous and thus is omitted.
We also use the following simple and technical lemma.
Lemma 22. For R (d) and f (d)1
∈R ∈ F
regret(R,f) = f I R (RR ) 1R f, (158)
⊤ d ⊤ ⊤ −
−
(cid:16) (cid:17)
where R Rr d is the matrix of coefficients of the orthogonal expansion of ψ = d c φ for
∈ × i j=1 ij j
i [r], so that R(i,j) = c .
ij
∈ P
Proof. It holds that
2
d r d
regret(R,f)= minE f φ (x)+n q c φ (x) E n2 (159)
j j i ij j
q Rr  −  −
∈ j=1 i=1 j=1
X X X (cid:2) (cid:3)
  
d r
= minE f q c φ (x) (160)
j i ij j
q Rr  − ! 
∈ j=1 i=1
X X
d r 2 
= min f q c (161)
j i ij
q Rr − !
∈ j=1 i=1
X X
1Note that any f ∈F(d) may be uniquely identified with a d-dimensional vector f ∈Rd. With a slight abuse of
notation we do not distinguish between thetwo.
31d r r r
= min f2 2f q c + q c q c (162)
q Rr " j − j i ij i1 i1j i2 i2j #
∈ Xj=1 Xi=1 i X1=1i X2=1
= minf f 2q Rf +q RR q (163)
⊤ ⊤ ⊤ ⊤
q Rr −
∈
= f I R (RR ) 1R f, (164)
⊤ d ⊤ ⊤ −
−
(cid:16) (cid:17)
where the last equality is obtained by the minimizer q = (RR ) 1Rf.
∗ ⊤ −
Proof of Theorems 19 and 20. By Proposition 21, we may first consider the finite dimensional case,
andthentakethelimit d . ByLemma 22, inthe d-dimensional case(forboththerepresentation
↑ ∞
and theresponsefunction), the regret is formally as inthe linear setting under theMSE ofTheorem
2, by setting therein Σ = I , and S =diag(λ ,...,λ )(c.f. Lemma 16). The claim of theTheorem
x d 1 d
19 then follows by taking d and noting that λ 0. The proof of Theorem 20 is analogous
d+1
↑ ∞ ↓
and thus omitted.
G Iterative algorithms for the Phase 1 and Phase 2 problems
In this section, we describe our proposed algorithms for solving the Phase 1 and Phase 2 problems
of Algorithm 1. Those algorithms are general, and only require providing gradients of the regret
function (1) and an initial representation and a set of adversarial functions. These are individually
determined for each setting. See Section H for the way these are determined in Examples 6 and 8.
G.1 Phase 1: finding a new adversarial function
We propose an algorithm to solve the Phase 1 problem (24), which is again based on an iterative
algorithm. We denote the function’s value at the tth iteration by f . The proposed Algorithm 2
(t)
operates as follows. At initialization, the function f is arbitrarily initialized (say atrandom),
(1)
∈ F
and then the optimal predictor Q(j) is found for each of the k possible representations R(j), j [k].
∈
Then, the algorithm iteratively repeats the following steps, starting with t = 2: (1) Updating the
function from f to f based on a gradient step of
(t 1) (t)
−
p(j) E loss(f (x),Q(j)(R(j)(x))) , (165)
(t 1)
· −
j X∈[k] h i
that is, the weighted loss function of the previous iteration function, which is then followed by a
projection to the feasible class of functions , denoted as Π () (2) Finding the optimal predictor
Q(j) for the current function f and theF given representaF tio· ns R(j) , and computing the
(t) j [k]
{ } ∈
respective loss for each representation,
L(j) := E loss(f (x),Q(j)(R(j)(x))) . (166)
(t)
h i
This loop iterates for T iterations, or until convergence.
f
Design choices and possible variants of the basic algorithm At initialization, we have
chosen a simple random initialization for f , but it may also be initialized based on some prior
(1)
knowledge of the adversarial function. For the update of the predictors, we have specified a full
computationoftheoptimalpredictor, whichcanbeachievedinpracticebyrunninganotheriterative
32Algorithm 2 A procedure for finding a new function via the solution of (24)
1: procedure Phase 1 solver( R(j),p(j)
j
[k], , ,d,r,P x)
2: begin { } ∈ F Q
3: Initialize T f ⊲ Number of iterations parameters
4: Initialize η f ⊲ Step size parameter
5: Initialize f ⊲ Function initialization, e.g., at random
(1)
∈F
6: for j = 1 to k do
87 :: endse ft orQ(j)
←
argmin
Q
∈QE loss(f (1)(x),Q(R(j)(x)))
(cid:2) (cid:3)
9: for t = 2 to T f do
10:
⊲ A
grau dp ied na tt ue pf d( at − te1/ o2)
f
= thef ( ft u− n1) ct+ ioη nf · j ∈[k]p( (j t −) 1) ·∇fE loss(f (t −1)(x),Q(j)(R(j)(x)))
P (cid:2) (cid:3)
11: project f = Π (f ) ⊲ Projection on the class
(t) (t 1/2)
12: for j = 1 to k doF − F
13: set Q(j) argmin E loss(f (x),Q(R(j)(x))) ⊲ Update of predictors
14: set L(j) ← E loss(fQ ∈Q (x),Q(j)(R(t () j)(x))) ⊲ Compute loss of each representation
← (t) (cid:2) (cid:3)
15: end for
(cid:2) (cid:3)
16: end for
17: return f , and the regret p(j) L(j)
(T) j [k] ·
18: end procedure ∈
P
algorithmsuchasstochasticgradientdescent(SGD)untilconvergence. Ifthisistoocomputationally
expensive, the number of gradient steps may be limited. The update of the function is done via
projected SGD with a constant step size η , yet it is also possible to modify the step size with
f
the iteration, e.g., the common choice η /√t at step t [Hazan, 2016]. Accelerated algorithms, e.g.,
f
moment-based, may also be deployed.
Convergence analysis A theoretical analysis of the convergence properties of the algorithm
appears to be challenging. Evidently, this is a minimax game between the response player and a
player cooperating with the representation player, which optimizes the prediction rule in order to
minimize the loss. This is, however, not a concave-convex game. As described in Appendix B, even
concave-convex games are not well understood at this point. We thus opt to validate this algorithm
numerically.
Running-time complexity analysis Algorithm 2 runs for a fixed number of iterations T ,
f
accepts k representations, and makes kT updates. Each update is comprised from a gradient step
f
of for the adversarial function (cost C ), and optimization of the predictor (cost C ). So the total
1 2
computation complexity is kT (C + C ). The most expensive part is the optimization of the
f 1 2
·
predictor C , and this can be significantly reduced by running a few gradients steps of the predictor
2
instead of a full optimization. If we take g gradient steps then C is replaced by C g and the total
2 1
computational cost is kT (g+1)C .
f 1
G.2 Phase 2: finding a new representation
We propose an iterative algorithm to solve the Phase 2 problem (25), and thus finding a new
representation R(k+1). Tothis end, wefirstnotethattheobjectivefunctionin(25)canbeseparated
33into a part that depends on existing representations and a part that depends on the new one,
specifically, as
p(j1) o(j2) E loss(f(j2)(x),Q(j1,j2)(R(j1)(x)))
· ·
j X1 ∈[k]j2 ∈X[m0+k] h i
+ p(k+1) o(j2) E loss(f(j2)(x),Q(k+1,j2)(R(k+1)(x)))
· ·
j2 ∈X[m0+k] h i
= p(j1) o(j2) L(j1,j2)
· ·
j X1 ∈[k]j2 ∈X[m0+k]
+ p(k+1) o(j2) E loss(f(j2)(x),Q(k+1,j2)(R(k+1)(x))) , (167)
· ·
j2 ∈X[m0+k] h i
where
L(j1,j2) := E loss(f(j2)(x),Q(j1,j2)(R(j2)(x))) , (168)
h i
andthepredictors Q(j1,j2) canbeoptimizedindependentlyofthenewrepresentation
{
}j1 ∈[k],j2 ∈[m0+k]
R(k+1). We propose an iterative algorithm for this problem, and denote the new representation at
(k+1)
the tth iteration of the algorithm by R . The algorithm’s input is a set of m +k adversarial
(t) 0
functions f(i) , and the current set of representations R(j) . Based on these, the
{
}i ∈[m0+k]
{
}j ∈[k]
algorithm may find the optimal predictor for f(j2) based on the representation R(j1), and thus
compute the loss
L(j1,j2) := minE loss(f(j2)(x),Q(R(j1)(x))) (169)
∗ Q
∈Q h i
for j [k] and j [m +k]. In addition, the new representation is arbitrarily initialized (say, at
1 2 0
∈ ∈
random) as
R(k+1)
, and the predictors
Q(k+1,j2)
are initialized as the optimal predictors
(1) { (1) }j2 ∈[m0+k]
for f(j2) given the representation R(k+1) . The algorithm keeps track of weights for the represen-
(1)
tations (including the new one), which are initialized uniformly, i.e., p(j1) = 1 for j [k +1]
(1) k+1 1 ∈
(including a weight for the new representation). The algorithm also keeps track of weights for the
functions, whicharealsoinitializeduniformly aso(j2) = 1 forj [m +k]. Then, thealgorithm
(1) m0+k 2 ∈ 0
iteratively repeats the following steps, starting with t = 2: (1) Updating the new representation
fromR(k+1) toR(k+1) basedonagradientstepoftheobjectivefunction(25)asafunctionofR(k+1).
(t 1) (t)
−
Based on the decomposition in (167) the term of the objective which depends on R(k+1) is
p(k+1) o(j2) E loss(f(j2)(x),Q(k+1,j2)(R(k+1)(x))) , (170)
(t 1) (t 1) ·
− −
j2 ∈X[m0+k] h i
that is, the loss function of the previous iteration new representation, weighted according to the
current function weights
o(j2)
. Since the multiplicative factor
p(k+1)
is common to all terms, it is
(t 1) (t 1)
− −
removed from the gradient computation (this aids in the choice of the gradient step). This gradient
step is then possibly followed by normalization or projection, which we denote by the operator
Π (). For example, in the linear case, it make sense to normalize R(k+1) to have unity norm (in
R · (k+1)
some matrix norm of choice). After updating the new representation to R , optimal predictors
(t)
are found for each function, the loss is computed
L(k+1,j2) := minE loss(f(j2)(x),Q(R(k+1) (x))) (171)
(t) (t)
Q
∈Q h i
34for all j [m +k], and the optimal predictor is updated to
Q(k+1,j2)
based on this
2 ∈ 0 { (t) }j2 ∈[m0+k]
(k+1)
solution. (2) Given the current new representation R , the loss matrix
(t)
L(j1,j2)
(172)
{ (t) }j1 ∈[k],j2 ∈[m0+k]
is constructed where for j [k] it holds that L(j1,j2) = L(j1,j2) for all t (i.e., the loss of previous
1 ∈ (t)
representations and functions is kept fixed). This is considered to be the loss matrix of a two-player
zero-sum game between the representation player and the function player, where the representation
player has k + 1 possible strategies and the function player has m + k strategies. The weights
0
p(j1)
and
o(j2)
are then updated according to the MWU rule. Specifically, for
{ (t) }j1 ∈[k+1] { (t) }j2 ∈[m0+k]
an inverse temperature parameter β (or a regularization parameter), the update is given by
p(j) βL(j)
p(j)
=
(t −1)·
(173)
(t) p(˜j) βL(˜j)
˜j [k] (t 1) ·
∈ −
P
for the representation weights and, analogously, by
o(j)
β
L(j)
o(j) = (t −1) · − (174)
(t) o(˜j)
β
L(˜j)
˜j [k] (t 1) · −
∈ −
P
for the function weights (as the function player aims to maximize the loss). This can be consid-
ered as a regularized gradient step on the probability simplex, or more accurately, a follow-the-
regularized-leader [Hazan, 2016]. The main reasoning of this algorithm is that at each iteration
the weights p(j) and o(j) are updated towards the solution of the two-player
{
}j ∈[k+1]
{
}j ∈[m0+k]
zero-sum game with payoff matrix
L(j1,j2)
. In turn, based only on the function
{− (t) }j1 ∈[k+1],j2 ∈[m0+k]
weights o(j) , the new representation is updated to R(k+1) , which then changes the pay-off
{ }j ∈[m0+k] (t)
matrix at the next iteration. It is well known that the MWU solved two-player zero-sum game
[Freund and Schapire, 1999], in which the representation player can choose the weights and the
function player can choose the function.
This loop iterates for T iterations, and then the optimal weights are given by the average
stop
over the last T iterations [Freund and Schapire, 1999], i.e.,
avg
Tstop
1
(j) (j)
p = p , (175)
∗ T avg (t)
t=TstoXp −Tavg+1
and
Tstop
1
(j) (j)
o = o . (176)
∗ T avg (t)
t=TstoXp −Tavg+1
(k+1)
In the last T T iterations, only the representation R and the predictors are updated.
R − stop (t)
(k+1) (j)
The algorithm then outputs R as the new representation and the weights p .
(T) { ∗ }j ∈[k+1]
35Algorithm 3 A procedure for finding a new representation R(k+1) via the solution of (25)
1: procedure Phase 2 solver( {R(j1) }j∈[k], {f(j2) }j2∈[m0+k], R, F, Q,d,r,Px)
2: Begin
3: Initialize T R,T stop,T avg ⊲ Number of iterations parameters
4: Initialize η R ⊲ Step size parameter
5: Initialize β (0,1) ⊲ Inverse temperature parameter
∈
6: Initialize f ⊲ Function initialization, e.g., at random
(1)
∈F
7: Initialize p(j) 0 for j [k] and p(k+1) 0 ⊲ A uniform weight initialization for the
(1) ← ∈ (1) ←
representations
8: Initialize o( (j 12 )) ← m01 +k for j 2 ∈[k] ⊲ A uniform weight initialization for the functions
9: for j 1 =1 to k do
10: for j 2 =1 to m 0+k do
11: Set Q(j1,j2) ←argmin Q∈QE loss(f(j2)(x),Q(R(j1)(x)))
⊲ Optimal predictors for existing representations and input functions
12: Set L(j1,j2) min Q∈QE loss(cid:2) (f(j2)(x),Q(j1,j2)(R(j1)(x))(cid:3) ) ⊲ The minimal loss
←
13: end for
(cid:2) (cid:3)
14: end for
15: for j 2 =1 to m 0+k do
16: Initialize R(k+1) ⊲ Arbitrarily, e.g., at random
(1)
17: Set Q( (k 1)+1,j2) ←argmin Q∈QE loss(f(j2)(x),Q(R(k+1)(x))) for j 2 ∈[m 0+k]
⊲ Optimal predictors for new representation and input functions
(cid:2) (cid:3)
18: end for
19: for t=2 to T R do
20: Update ⊲ A gradient update of the new representation
R(k+1) =R(k+1)+η o(j2) E loss(f(j2)(x),Q(k+1,j2)(R(k+1)(x))) (177)
(t−1/2) (t−1) R · (t−1)·∇R(k+1) (t−1)
j2∈ X[m0+k] h i
21: Project R (( tk )+1) =ΠR(R (( tk −+ 11 /) 2)) ⊲ Standardization based on the class
R
22: for j =1 to k do
23: Set Q(k+1,j2) ←argmin Q∈QE loss(f(j2)(x),Q(R (( tk )+1)(x)))
⊲ Update of predictors for the new represhentation i
24: L(k+1,j2) E loss((f(j2)(x),Q(k+1,j2)(R(k+1)(x))) ⊲ Compute loss
(t) ← (t)
25: end for h i
26: Set L (( tj )1,j2) ←L(j1,j2) for j 1 ∈[k] and j 2 ∈[m 0+k]
27: if t<T stop then
28: Update p( (j t)) ←p (( tj −) 1)·βL(j) /P˜j∈[k]p ((˜ tj −) 1)·βL(˜j) for j ∈[k] ⊲ A MWU
29: Update o( (j t)) ←o (( tj −) 1)·β−L(j) /P˜j∈[m0+k]o ((˜ tj −) 1)·β−L(˜j) for j ∈[m 0+k] ⊲ A MWU
30: else if t=T stop then
31: Update p(j) =p(j) 1 Tstop p(j) for j [k]
(t) (t) ← Tavg t=Tstop−Tavg+1 (t) ∈
⊲ Optimal weights by averaging last T iterations
Pavg
32: Update o( (j t))
←
Ta1
vg
T t=st Top stop−Tavg+1o( (j t)) for j ∈[m 0+k]
⊲ Optimal weights by averaging last T iterations
P avg
33: else
34: Update p( (j t)) ←p( (j t−) 1) for j ∈[k] ⊲ No update for the last T −T stop iterations
35: Update o( (j t)) ←o( (j t−) 1) for j ∈[m 0+k] ⊲ No update for the last T −T stop iterations
36: end if
37: Return R (( Tk+ )1) and {p( (j T) R)}j∈[k+1]
38: end for
39: end procedure
36Table 1: Hardware details
CPU RAM GPU
Intel i9 13900k 64GB RTX 3090 Ti
Design choices and possible variants of the basic algorithm At initialization, we have
(k+1)
chosen a simple random initialization for R , but it may also be initialized based on some prior
(1)
knowledge of the desired new representation. The initial predictors
Q(k+1,j2)
will then
{ (1) }j2 ∈[m0+k]
be initialized as the optimal predictors for R(k+1) and f(j2) . We have initialized the
(1) { }j2 ∈[m0+k]
representation and function weights uniformly. A possibly improved initialization for the function
weights is to put more mass on the more recent functions, that is, for large values of j , or to use
2
the minimax strategy of the function player in the two-player zero-sum game with payoff matrix
L(j1,j2)
(that is, a game which does not include the new representation). As in
{− (t) }j1 ∈[k],j2 ∈[m0+k]
the Phase 1 algorithm, the gradient update of the new representation can be replaced by a more
sophisticated algorithm, the computation of the optimal predictors can be replaced with (multiple)
update steps, and the step size may also be adjusted. For the MWU update, we use the proposed
scaling proposed by Freund and Schapire [1999]
1
β = (178)
1+ clnm
T
q
for some constant c. It is well known that using the last iteration of a MWU algorithm may fail
[Bailey and Piliouras, 2018], while averaging the weights value of all iterations provides the optimal
value of a two-player zero-sum games [Freund and Schapire, 1999]. For improved accuracy, we
compute the average weights over the last T iterations (thus disregarding the initial iterations).
avg
We then halt the weights update and let the function and predictor update to run for T T
stop
−
iterations in order to improve the convergence of R(k+1). Finally, the scheduling of the steps may
be more complex, e.g., it is possible that running multiple gradient steps follows by multiple MWU
steps may improve the result.
Running-time complexity analysis Algorithm 3 is more complicated than Algorithm 2, but
the computational complexity analysis is similar. It runs for T iterations and the total cost is
R
roughly on the order of T k2gC (taking g gradient steps for the predictor optimization; k2 is the
R 1
number of representations, and is controlled by the learner; C is determined by the computer, and
1
g should be large enough to assure quality results).
H Details for the examples of Algorithm 1 and additional experi-
ments
As mentioned, the solvers of the Phase 1 and Phase 2 problems of Algorithm 1 require the gradients
of the regret (1) as inputs, as well as initial representation and set of adversarial functions. We next
provide these details for the examples in Section 4. The code for the experiments was written in
Python 3.6 and is available at this link. The optimization of hyperparameters was done using the
Optuna library. The hardware used is standard and detailed appear in Table 1.
37H.1 Details for Example 6: the linear MSE setting
In this setting, the expectation over the feature distribution can be carried out analytically, and the
regret is given by
2
regret(R,f Σ ) = E f x q R x (179)
x ⊤ ⊤ ⊤
| −
(cid:20) (cid:21)
(cid:16) (cid:17)
= f Σ f 2q R Σ f +q R Rq. (180)
⊤ x ⊤ ⊤ x ⊤ ⊤
−
The regret only depends on the feature distribution P via Σ . For each run of the algorithm, the
x x
covariancematrixΣ waschosentobediagonalwithelementsdrawnfromalog-normaldistribution,
x
with parameters (0,σ ), and S = I .
0 d
Regret gradients The gradient of the regret w.r.t. the function f is given by
2
E f x q R x = 2f Σ 2q R Σ (181)
f ⊤ ⊤ ⊤ ⊤ x ⊤ ⊤ x
∇ − −
(cid:20) (cid:21)
(cid:16) (cid:17)
and the projection on is
S
F
f , f 1
Π (f)= kf kS k kS ≥ . (182)
F (f, f S< 1
k k
However, we may choose to normalize by f even if f 1 since in this case the regret is always
larger if f is replaced by f (in other
wokf rdkS
s, the
wok rstk cS a≤
se function is obtained on the boundary
kf kS
of ). The gradient w.r.t. the predictor q is given by
S
F
2
E f x q R x = 2f Σ R+2q R Σ R . (183)
q ⊤ ⊤ ⊤ ⊤ x ⊤ ⊤ x
∇ − −
(cid:20) (cid:21)
(cid:16) (cid:17) h i
Finally, to derive the gradient w.r.t. R, let us denote R := [R ,R ,...,R ] Rd r where R Rd
1 2 r × i
∈ ∈
is the ith column (i [r]), and q = (q ,q ,...,q ). Then, q R x = q R x and the loss
∈ ⊤ 1 2 r ⊤ ⊤ i [d] i i⊤
function is ∈
P
2
2
E f x q R x = E f x q x R (184)
⊤ ⊤ ⊤ ⊤ i ⊤ i
−  −  
(cid:20) (cid:16) (cid:17) (cid:21) i X∈[d]
  
= f Σ f 2q R Σ f +q R Σ Rq. (185)
⊤ x ⊤ ⊤ x ⊤ ⊤ x
−
The gradient of the regret w.r.t. R is then given by
k
2
E f x q R x = 2E f x q R x q x (186)
∇Rk ⊤
−
⊤ ⊤
−
⊤
−
⊤ ⊤
·
k ⊤
(cid:26) (cid:20) (cid:21)(cid:27)
(cid:16) (cid:17) h(cid:16) (cid:17) i
= 2q f Σ q R Σ , (187)
k ⊤ x ⊤ ⊤ x
− −
(cid:16) (cid:17)
hence, more succinctly, the gradient w.r.t. R is
2
E f x q R x = 2q f Σ q R Σ . (188)
R ⊤ ⊤ ⊤ ⊤ x ⊤ ⊤ x
∇ − − −
(cid:26) (cid:20) (cid:21)(cid:27)
(cid:16) (cid:17) (cid:16) (cid:17)
We remark that in the algorithm these gradients are multiplied by weights. We omit this term
whenever the weight is common to all terms in order to keep the effective step size constant.
38Table 2: Parameters for linear MSE setting example
Parameter β β η η
r f r f
Value 0.94 0.653 0.713 0.944
Parameter T R T f T avg T stop
Value 100 until convergence 10 80
Figure 4: The learning curve for Algorithm 1 in the linear MSE setting: d = 20, r = 3, σ = 1.
Initialization Algorithm 1 requires an initial representation R(1) and an initial set of functions
f(j) . In the MSE setting, each function f Rd is also a single column of a representation
{ matri}
xj
∈
R[m0]
Rd r. A plausible initialization matrix∈ R(1) Rd r is therefore the worst r functions.
× ×
∈ ∈
These, in turn, can be found by running Algorithm (1) to obtain m˜ = r functions, by setting
r˜= 1. A proper initialization for this run is simply an all-zero representation R˜(1) = 0 Rd 1. The
×
∈
resulting output is then R˜(j) which canbeplaced as the r columns of R(1). This initialization
{
(T)}j ∈[r]
is then used for Algorithm 1.
Algorithm parameters The algorithm parameters used for Example 6 are shown in Table 2.
The parameters were optimally tuned for σ = 1.
0
Additional results Thelearning curve forrunning Algorithm 1forExample 6is shown inFigure
4, which shows the improvement in regret in each iteration, for which an additional matrix is added
tothesetofrepresentations. Itcanbeseenthatmixingroughly10matricessufficetogetclosetothe
minimal regret attained by the algorithm, compared to the potential number of d = 20 = 1140
r 3
representation matrices determined by .
A (cid:0) (cid:1) (cid:0) (cid:1)
Additional results of the accuracy of the Algorithm 1 in the linear MSE setting are displayed in
39Figure 5: The ratio between the regret achieved by Algorithm 1 and the theoretical regret in the
linear MSE setting. Left: d= 20, σ = 1, varying r. Right: r = 5, d = 20, varying σ .
0 0
Figure 5. The left panel of Figure 5 shows that the algorithm output is accurate for small values
of r, but deteriorates as r increases. This is because when r increases then so is ℓ and so is the
∗
required number of matrices in the support of the representation rule (denoted by m). Since the
algorithm gradually adds representation matrices to the support, an inaccurate convergence at an
early iteration significantly affects later iterations. One possible way to remedy this is to run each
iteration multiple times, and choosethe bestone, beforemoving ontothe nextone. Anotherreason
is that given large number of matrices in the support (large m), it becomes increasingly difficult
for the the MWU to accurately converge. Since the iterations of the MWU do not converge to the
equilibriumpoint, butrathertheiraverage (seediscussioninAppendix B)thiscanonly beremedied
by allowing more iterations for convergence (in advance) for large values of m. The right panel of
Figure 5 shows that the algorithm output is accurate for a wide range of the condition number of
the covariance matrix. This condition number is determined by the choice of σ , where low values
0
typically result covariance matrices with condition number that is close to 1, while high values will
typically resultlarge condition number. The right panel shows that whilethe hyperparameters were
tuned for σ = 1, the result is fairly accurate for a wide range of σ values, up to σ 5. Since for
0 0 0
≈
Z N(0,1) (standard normal) it holds that P[ 2 < Z < 2] 95%, the typical condition number
∼ − ≈
of a covariance matrix drawn with σ = 5 is roughly e2σ0 4.85 108, which is a fairly large range.
0 e−2σ0 ≈ ·
H.2 Details for Example 8: the linear cross-entropy setting
In this setting,
regret(R,f P )= minE D [1+exp( f x)] 1 [1+exp( q R x)] 1 , (189)
x KL ⊤ − ⊤ ⊤ −
| q Rr − || −
∈ h (cid:16) (cid:17)i
and the expectation over the feature distribution typically cannot be carried out analytically. We
thustestedAlgorithm1onempiricaldistributionsofsamplesdrawnfromahigh-dimensionalnormal
distribution. Specifically, for each run, B = 1000 feature vectors were drawn from an isotropic
normal distribution of dimension d = 15. The expectations of the regret and the corresponding
gradients were then computed with respect to (w.r.t.) the resulting empirical distributions.
Regret gradients We use the facts that
∂ p (1 p )
1 2
D (p p ) = log − (190)
KL 1 2
∂p || p (1 p )
1 2 1
−
40and
∂ p p
2 1
D (p p ) = − . (191)
KL 1 2
∂p || p (1 p )
2 2 2
−
For brevity, let us next denote
1
p := (192)
1
1+exp( f x)
⊤
−
and
1
p := . (193)
2
1+exp( q R x)
⊤ ⊤
−
We next repeatedly use the chain rule for differentiation. First,
1 exp( f x) x
p = = − ⊤ · = p (1 p ) x (194)
∇f 1 ∇f (cid:20)1+exp( −f ⊤x)
(cid:21)
[1+exp( −f ⊤x)]2 1 − 1 · ·
and
1 exp( q R x) R x
∇qp
2
=
∇q (cid:20)1+exp( −q ⊤R ⊤x)
(cid:21)
= [1+− exp⊤
(
−⊤
q
⊤R· ⊤x⊤
)]2
= p 2(1 −p 2) ·R ⊤x
·
(195)
So, assuming that P is such that the order of differentiation and expectation may be interchanged
x
(this can be guaranteed using dominated/monotone convergence theorems), the gradient of the
regret w.r.t. f is
∂
regret(R,f P ) = E D (p p ) p (196)
f x KL 1 2 f 1
∇ | ∂p || ×∇
(cid:20) 1 (cid:21)
p (1 p )
= E log 1 − 2 p (1 p ) x (197)
1 1
p (1 p ) · − ·
(cid:20) (cid:18) 2 − 1 (cid:19) (cid:21)
exp( f x)
= E "(f ⊤ −q ⊤R ⊤)x [1+exp−
(
⊤
f
x)]2 ·x
#
(198)
⊤
−
exp( f x)
= E "[1+exp−
(
⊤
f
x)]2
·x ⊤(f −Rq)x #. (199)
⊤
−
Next, under similar assumptions, the gradient of the regret w.r.t. the predictor q is
∂
regret(R,f P )= E D (p p ) p (200)
q x KL 1 2 q 2
∇ | ∂p || ×∇
(cid:20) 2 (cid:21)
1 1
= E R x . (201)
⊤
1+exp( q R x) − 1+exp( f x) ·
(cid:20)(cid:18) − ⊤ ⊤ − ⊤ (cid:19) (cid:21)
Finally,asfortheMSEcase,toderivethegradientw.r.t. R,wedenoteR := [R ,R ,...,R ] Rd r
1 2 r ×
∈
where R Rd is the ith column (i [r]), and q = (q ,q ,...,q ). Then, q R x = q R x
i ∈ ∈ ⊤ 1 2 r ⊤ ⊤ i [d] i i⊤
and ∈
1 P
p = . (202)
2
1+exp( q R x)
− i [d] i i⊤
∈
Then, the gradient of p
2
w.r.t. R
k
is then given byP
p = p (1 p ) q x, (203)
∇Rk 2 2
−
2
·
i
hence, more succinctly, the gradient w.r.t. R is
p = p (1 p ) xq . (204)
R 2 2 2 ⊤
∇ − ·
41Table 3: Parameters for linear cross entropy setting example
Parameter β β η η
r f r f
Value 0.9 0.9 10 3 10 1
− −
Parameter T R T f T avg T stop
Value 100 1000 25 50
Hence,
∂
regret(R,f P ) = E D (p p ) p (205)
R x KL 1 2 R 2
∇ | ∂p || ×∇
(cid:20) 2 (cid:21)
= E (p p ) xq (206)
2 1 ⊤
− ·
h 1 i 1
= E xq . (207)
⊤
1+exp( q R x) − 1+exp( f x) ·
(cid:20)(cid:18) − ⊤ ⊤ − ⊤ (cid:19) (cid:21)
Initialization Here the initialization is similar to the linear MSE setting, except that since a col-
umnoftherepresentationcannotideallycaptureevenasingleadversarialfunction, theinitialization
algorithm only searches for a single adversarial function (m˜ = 1). This single function is then used
to produce R(1) as the initialization of Algorithm 1.
Algorithm parameters The algorithm parameters used for Example 8 are shown in Table 3.
H.3 Details for Example 9: An experiment with a multi-label classification of
images and a comparison to PCA
We next present the setting of Example 9, which shows that large reduction in the representation
dimension can be obtained if the function is known to belong to a finite class.
Definition 23 (The multi-label classification setting ). Assume that = R√d √d where d= 625,
×
X
and x represents an image. The distribution P is such that x contains 4 shapes selected from a
x
dictionary of 6 shapes in different locations, chosen with a uniform probability; see Figure 6. The
output is a binary classification = 1 of the image. Assume that the class of representation is
Y {± }
linear z = R(x) = R x for some R := Rd r where d > r. The response function belongs to
⊤ ×
∈ R
a class of 6 different functions = f ,...f , where f : indicates whether the ith shape
1 6 j
F { } X → Y
appearsintheimageornot. Assumethecross-entropy lossfunction,wheregiventhattheprediction
that y = 1 with probability q results the loss loss(y,q) := 1(1 +y)logq 1 (1 y)log(1 q).
−2 − 2 − −
The set of predictor functions is := Q(z) = 1/[1+exp( q z)], q Rr , and the regret is then
⊤
Q − ∈
given by the expected binary Kullback-Leibler (KL) divergence as in Definition 7.
(cid:8) (cid:9)
Simplifying Algorithm 1 Inthethemulti-label classificationsettingofDefinition23,Algorithm
1 can be simplified as follows. First, since the number of response functions in the class is finite,
F
the Phase 1 problem (23) in Algorithm 1 algorithm is simple, since the adversarial function can
be found by a simple maximization over the 6 functions. Then, the phase 2 step simply finds for
each function f(j2) in , j
2
[6], the best representation-predictor (R(j1),Q(j1,j2)) using gradient
F ∈
descent, where R(j1) : Rr is a linear representation z =, and Q is logistic regression. This
X →
42Figure 6: An image in the dataset for the multi-label classification setting (Definition 23).
results is a payoff matrix of R6 6. Then, the resulting game can be numerically solved as a linear
×
program, thus obtaining the probability that each representation should be played. The resulting
loss of this minimax rule R is the loss of our representation. This representation is then compared
∗
with a standard PCA representation, which uses the projections on the first r principle directions
of R = V (Σ ) as the representation (without randomization). The results of the experiment are
1:r x ⊤
shown in Figure 3 in the paper.
H.4 An experiment with a NN architecture
In the analysis and the experiments above we have considered basic linear functions. As mentioned,
sincetheoperationofAlgorithm1onlydependsonthegradients ofthelossfunction, itcanbeeasily
generalized to representations, response functions and predictors for which such gradients (or sub-
gradients) can be provided. In this section, we exemplify this idea with a simple NN architecture.
For x Rd, we let the rectifier linear unit (ReLU) be denoted as (x) .
+
∈
Definition 24 (TheNNsetting). AssumethesamesettingasinDefinitions1and7,exceptthatthe
classofrepresentation, responseandpredictors areNNwithchiddenlayers ofsizesh ,h ,h N ,
R f q +
∈
respectively, instead of linear functions. Specifically: (1) The representation is
R(x)= R R (R x) (208)
c⊤
···
1⊤ 0⊤ +
+
(cid:18) (cid:19)+
(cid:16) (cid:17)
for some (R 0,R 1, R c) := Rd ×hR RhR×hR RhR×hR RhR×r where d > r. (2) The
··· ∈ R { × ··· × }
response is determined by
f(x)= f F (F x) (209)
c⊤
···
1⊤ 0⊤ +
+
(cid:18) (cid:19)+
(cid:16) (cid:17)
where (F 0,F 1,...,f c) := Rd ×hf Rhf×hf Rhf×hf Rhf . (3) The predictor is determined
∈ F { × ··· × }
by for some
q(z) = q Q (Q z) (210)
c⊤
···
⊤1 ⊤0 +
+
(cid:18) (cid:19)+
(cid:16) (cid:17)
where (Q ,Q ,...,q ) := Rr hq Rhq hq Rhq hq Rhq .
0 1 c × × ×
∈ Q { × ··· × }
43Table 4: Parameters for the NN cross-entropy setting.
Parameter c h h h
R f q
Value 1 d d d
Parameter β β η η η
r f r f q
Value 0.9 0.9 10 3 10 1 10 1
− − −
Parameter T R T f T Q T avg T stop
Value 100 1000 100 10 80
Figure 7: The regret achieved by Algorithm 1 in the NN cross-entropy setting as a function of the
iteration m.
Regret gradients Gradients werecomputed using PyTorchwithstandardgradients computation
using backpropagation for an SGD optimizer.
Initialization The initialization algorithm is similar to the initialization algorithm used in the
linear cross-entropy setting.
Algorithm parameters The algorithm parameters used for the example are shown in Table 4.
Results For a single hidden layer, Figure 7 shows the reduction of the regret with the iteration
for the cross-entropy loss.
References
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
John N. Tsitsiklis. Decentralized detection. 1989.
44XuanLong Nguyen, Martin J. Wainwright, and Michael I. Jordan. On surrogate loss functions and
f-divergences. The Annals of Statistics, 37(2):876 – 904, 2009. doi: 10.1214/08-AOS595. URL
https://doi.org/10.1214/08-AOS595.
John Duchi, Khashayar Khosravi, and Feng Ruan. Multiclass classification, information, divergence
and surrogate risk. Annals of Statistics, 46(6B):3246–3275, 2018. ISSN 0090-5364. doi: 10.1214/
17-AOS1657.
Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. The MIT Press,
2012. ISBN 0262017180.
KarlPearson. Onlinesandplanesofclosestfittosystemsofpointsinspace. TheLondon, Edinburgh,
and Dublin philosophical magazine and journal of science, 2(11):559–572, 1901.
Ian Jolliffe. Principal component analysis. Encyclopedia of statistics in behavioral science, 2005.
John P. Cunningham and Zoubin Ghahramani. Linear dimensionality reduction: Survey, insights,
and generalizations. The Journal of Machine Learning Research, 16(1):2859–2900, 2015.
Iain M. Johnstone and Debashis Paul. PCA in high dimensions: An orientation. Proceedings of the
IEEE, 106(8):1277–1292, 2018.
Bernhard Schölkopf, Alexander Smola, and Klaus-Robert Müller. Nonlinear component analysis as
a kernel eigenvalue problem. Neural computation, 10(5):1299–1319, 1998.
Mark A. Kramer. Nonlinear principal component analysis using autoassociative neural networks.
AIChE journal, 37(2):233–243, 1991.
Geoffrey E. Hinton and Ruslan R. Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504–507, 2006.
Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y. Ng. Unsupervised learning of
hierarchical representations with convolutional deep belief networks. Communications of the
ACM, 54(10):95–103, 2011.
Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method.
arXiv preprint physics/0004057, 2000.
Gal Chechik, Amir Globerson, Naftali Tishby, and Yair Weiss. Information bottleneck for Gaussian
variables. Advances in Neural Information Processing Systems, 16, 2003.
Noam Slonim, Nir Friedman, and Naftali Tishby. Multivariate information bottleneck. Neural
computation, 18(8):1739–1789, 2006.
Peter Harremoës and Naftali Tishby. The information bottleneck revisited or how to choose a
good distortion measure. In 2007 IEEE International Symposium on Information Theory, pages
566–570. IEEE, 2007.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In
2015 ieee information theory workshop, pages 1–5. IEEE, 2015.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via infor-
mation. arXiv preprint arXiv:1703.00810, 2017.
45Ravid Shwartz-Ziv. Information flow in deep neural networks. arXiv preprint arXiv:2202.06749,
2022.
Alessandro Achille and Stefano Soatto. Emergence of invariance and disentanglement in deep rep-
resentations. The Journal of Machine Learning Research, 19(1):1947–1980, 2018a.
Alessandro Achille and Stefano Soatto. Information dropout: Learning optimal representations
through noisy computation. IEEE transactions on pattern analysis and machine intelligence, 40
(12):2897–2905, 2018b.
YannDubois,Douwe Kiela, DavidJ.Schwab, andRamakrishnaVedantam. Learning optimalrepre-
sentations withthedecodableinformationbottleneck. Advances in Neural Information Processing
Systems, 33:18674–18690, 2020.
Yilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, and Stefano Ermon. A theory of usable
information under computational constraints. arXiv preprint arXiv:2002.10689, 2020.
Guillermo Owen. Game theory. Emerald Group Publishing, 2013.
Jonathan Baxter. A model of inductive bias learning. Journal of artificial intelligence research, 12:
149–198, 2000.
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask
representation learning. Journal of Machine Learning Research, 17(81):1–32, 2016.
NileshTripuraneni,MichaelJordan,andChiJin. Onthetheoryoftransferlearning: Theimportance
of task diversity. Advances in neural information processing systems, 33:7852–7862, 2020.
Nilesh Tripuraneni, Chi Jin, and Michael Jordan. Provable meta-learning of linear representations.
In International Conference on Machine Learning, pages 10434–10443. PMLR, 2021.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
Ravid Shwartz-Ziv and Yann LeCun. To compress or not to compress–self-supervised learning and
information theory: A review. arXiv preprint arXiv:2304.09355, 2023.
Yoav Freund and Robert E. Schapire. Adaptive game playing using multiplicative weights. Games
and Economic Behavior, 29(1-2):79–103, 1999.
Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends®
in Machine Learning, 4(2):107–194, 2012.
EladHazan.Introductiontoonlineconvexoptimization. FoundationsandTrends®inOptimization,
2(3-4):157–325, 2016.
Ilya O. Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard
Schölkopf. Adagan: Boosting generative models. Advances in neural information processing
systems, 30, 2017.
Maurice Sion. On general minimax theorems. 1958.
Dimitris Bertsimas and John N. Tsitsiklis. Introduction to linear optimization, volume 6. Athena
scientific Belmont, MA, 1997.
46Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto-
encoders: Explicit invariance during feature extraction. In Proceedings of the 28th international
conference on international conference on machine learning, pages 833–840, 2011.
Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural
networks: A survey. IEEE transactions on pattern analysis and machine intelligence, 44(9):
5149–5169, 2021.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds
and algorithms. arXiv preprint arXiv:0902.3430, 2009.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. Machine learning, 79:151–175, 2010.
Friedemann Zenke, BenPoole, andSuryaGanguli. Continual learningthroughsynapticintelligence.
In International conference on machine learning, pages 3987–3995. PMLR, 2017.
CuongVNguyen, YingzhenLi, ThangDBui, andRichardETurner. Variational continuallearning.
arXiv preprint arXiv:1710.10628, 2017.
Gido M Van de Ven and Andreas S Tolias. Three scenarios for continual learning. arXiv preprint
arXiv:1904.07734, 2019.
RahafAljundi,KlaasKelchtermans, andTinneTuytelaars. Task-freecontinuallearning. InProceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11254–
11263, 2019.
T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley-Interscience, 2006. ISBN
0471241954.
Stephen Boyd, Stephen P. Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge
university press, 2004.
Ohad Shamir, Sivan Sabato, and Naftali Tishby. Learning and generalization with the information
bottleneck. Theoretical Computer Science, 411(29-30):2696–2711, 2010.
Andrew M. Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan D.
Tracey, and David D. Cox. On the information bottleneck theory of deep learning. Journal of
Statistical Mechanics: Theory and Experiment, 2019(12):124020, 2019.
Bernhard C. Geiger. On information plane analyses of neural network classifiers– A review. IEEE
Transactions on Neural Networks and Learning Systems, 2021.
XuanLongNguyen,MartinJ.Wainwright, andMichaelI.Jordan. Estimatingdivergencefunctionals
and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,
56(11):5847–5861, 2010.
Ben Poole, Sherjil Ozair, Aäron van den Oord, Alexander A. Alemi, and George Tucker. On vari-
ational lower bounds of mutual information. In NeurIPS Workshop on Bayesian Deep Learning,
2018.
Tailin Wu, Ian Fischer, Isaac L. Chuang, and Max Tegmark. Learnability for the information
bottleneck. In Uncertainty in Artificial Intelligence, pages 1050–1060. PMLR, 2020.
47David McAllester and Karl Stratos. Formal limitations on the measurement of mutual information.
In International Conference on Artificial Intelligence and Statistics, pages 875–884. PMLR, 2020.
Matthew Chalk, Olivier Marre, and Gasper Tkacik. Relevant sparse codes with variational infor-
mation bottleneck. Advances in Neural Information Processing Systems, 29, 2016.
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv preprint arXiv:1612.00410, 2016.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and Devon Hjelm. Mutual information neural estimation. In International conference
on machine learning, pages 531–540. PMLR, 2018.
Behrooz Razeghi, Flavio P. Calmon, Deniz Gunduz, and Slava Voloshynovskiy. Bottlenecks CLUB:
Unifying information-theoretic trade-offs among complexity, leakage, and utility. arXiv preprint
arXiv:2207.04895, 2022.
Matías Vera, Pablo Piantanida, and Leonardo Rey Vega. The role of information complexity and
randomization in representation learning. arXiv preprint arXiv:1802.05355, 2018.
Borja Rodriguez Galvez. The information bottleneck: Connections to other problems, learning and
exploration of the ib curve, 2019.
Peter L. Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Riskbounds and
structural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.
Martin J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
Karthik Sridharan and Sham M. Kakade. An information theoretic framework for multi-view learn-
ing. 2008.
Rana Ali Amjad and Bernhard C. Geiger. Learning representations for neural network-based clas-
sification using the information bottleneck principle. IEEE transactions on pattern analysis and
machine intelligence, 42(9):2225–2239, 2019.
Artemy Kolchinsky, Brendan D. Tracey, and David H. Wolpert. Nonlinear information bottleneck.
Entropy, 21(12):1181, 2019.
D. J. Strouse and David J. Schwab. The information bottleneck and geometric clustering. Neural
computation, 31(3):596–612, 2019.
Ankit Pensia, Varun Jog, and Po-Ling Loh. Extracting robust and accurate features via a robust
information bottleneck. IEEE Journal on Selected Areas in Information Theory, 1(1):131–144,
2020.
Shahab Asoodeh and Flavio P Calmon. Bottleneck problems: An information and estimation-
theoretic view. Entropy, 22(11):1325, 2020.
VudtiwatNgampruetikornandDavidJ.Schwab. Perturbationtheoryfortheinformationbottleneck.
Advances in Neural Information Processing Systems, 34:21008–21018, 2021.
48Xi Yu, Shujian Yu, and José C Príncipe. Deep deterministic information bottleneck with matrix-
based entropy functional. In ICASSP 2021-2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pages 3160–3164. IEEE, 2021.
Vudtiwat Ngampruetikorn andDavidJ.Schwab. Informationbottleneck theoryof high-dimensional
regression: Relevancy, efficiency and optimality. arXiv preprint arXiv:2208.03848, 2022.
Deniz Gündüz, Zhijin Qin, Inaki Estella Aguerri, Harpreet S Dhillon, Zhaohui Yang, Aylin Yener,
Kai Kit Wong, and Chan-Byoung Chae. Beyond transmitting bits: Context, semantics, and
task-oriented communications. IEEE Journal on Selected Areas in Communications, 41(1):5–41,
2022.
Vudtiwat Ngampruetikorn and David J. Schwab. Generalized information bottleneck for Gaussian
variables. arXiv preprint arXiv:2303.17762, 2023.
VudtiwatNgampruetikorn,WilliamBialek,andDavidSchwab. Information-bottleneckrenormaliza-
tion group for self-supervised representation learning. Bulletin of the American Physical Society,
65, 2020.
Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Multi-task feature learning.
Advances in neural information processing systems, 19, 2006.
Simon S Du, Wei Hu, Sham M Kakade, Jason D Lee, and Qi Lei. Few-shot learning via learning
the representation, provably. arXiv preprint arXiv:2002.09434, 2020.
William B. Johnson. Extensions of Lipschitz mappings into a Hilbert space. Contemp. Math., 26:
189–206, 1984.
Santosh S. Vempala. The random projection method, volume 65. American Mathematical Soc.,
2005.
Michael W. Mahoney et al. Randomized algorithms for matrices and data. Foundations and
Trends® in Machine Learning, 3(2):123–224, 2011.
DavidP.Woodruffetal. Sketching asatoolfornumerical linearalgebra. Foundations and Trends®
in Theoretical Computer Science, 10(1–2):1–157, 2014.
Fan Yang, Sifan Liu, Edgar Dobriban, and David P. Woodruff. How to reduce dimension with PCA
and random projections? IEEE Transactions on Information Theory, 67(12):8154–8189, 2021.
John F. Nash Jr. Equilibrium points in n-person games. Proceedings of the national academy of
sciences, 36(1):48–49, 1950.
Abraham Wald. Contributions to the theory of statistical estimation and testing hypotheses. The
Annals of Mathematical Statistics, 10(4):299–326, 1939.
Larry Wasserman. All of statistics: A concise course in statistical inference, volume 26. Springer,
2004.
Yuhong Yang and Andrew Barron. Information-theoretic determination of minimax rates of con-
vergence. Annals of Statistics, pages 1564–1599, 1999.
Peter D. Grünwald and A. Philip Dawid. Game theory, maximum entropy, minimum discrepancy
and robust bayesian decision theory. The Annals of Statistics, 32(4):1367–1433, 2004.
49David Haussler and Manfred Opper. Mutual information, metric entropy and cumulative relative
entropy risk. The Annals of Statistics, 25(6):2451–2492, 1997.
Farzan Farnia and David Tse. A minimax approach to supervised learning. Advances in Neural
Information Processing Systems, 29, 2016.
Jorge SilvaandFelipe Tobar. Ontheinterplay between information lossand operation lossinrepre-
sentations for classification. In International Conference on Artificial Intelligence and Statistics,
pages 4853–4871. PMLR, 2022.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the
ACM, 63(11):139–144, 2020.
Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A.
Bharath. Generative adversarial networks: An overview. IEEE signal processing magazine, 35
(1):53–65, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust optimization, volume 28.
Princeton university press, 2009.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training GANs. Advances in neural information processing systems, 29,
2016.
Constantinos Daskalakis, Alan Deckelbaum, and Anthony Kim. Near-optimal no-regret algorithms
for zero-sum games. In Proceedings of the twenty-second annual ACM-SIAM symposium on Dis-
crete Algorithms, pages 235–254. SIAM, 2011.
Sasha Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable se-
quences. Advances in Neural Information Processing Systems, 26, 2013.
James P. Bailey and Georgios Piliouras. Multiplicative weights update in zero-sum games. In
Proceedings of the 2018 ACM Conference on Economics and Computation, pages 321–338, 2018.
Guodong Zhang, Yuanhao Wang, Laurent Lessard, and Roger B. Grosse. Near-optimal local con-
vergence of alternating gradient descent-ascent for minimax optimization. In International Con-
ference on Artificial Intelligence and Statistics, pages 7659–7679. PMLR, 2022.
Florian Schäfer and Anima Anandkumar. Competitive gradient descent. Advances in Neural Infor-
mation Processing Systems, 32, 2019.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of GANs. Advances in
neural information processing systems, 30, 2017.
Alistair Letcher, David Balduzzi, Sébastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls,
and Thore Graepel. Differentiable game mechanics. The Journal of Machine Learning Research,
20(1):3032–3071, 2019.
50Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, Rémi Le Priol, Gabriel Huang,
Simon Lacoste-Julien, andIoannisMitliagkas. Negativemomentum forimproved gamedynamics.
In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1802–1811.
PMLR, 2019.
Guodong Zhang and Yuanhao Wang. On the suboptimality of negative momentum for minimax
optimization. In International Conference on Artificial Intelligence and Statistics, pages 2098–
2106. PMLR, 2021.
Max Welling, Richard Zemel, and Geoffrey E. Hinton. Self supervised boosting. Advances in neural
information processing systems, 15, 2002.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser, and IlliaPolosukhin. Attention is allyou need. Advances in neural information processing
systems, 30, 2017.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
Roger A. Horn and Charles R. Johnson. Matrix analysis. Cambridge university press, 2012.
Dimitri Bertsekas, Angelia Nedic, and Asuman Ozdaglar. Convex analysis and optimization, vol-
ume 1. Athena Scientific, 2003.
Ky Fan. On a theorem of Weyl concerning eigenvalues of linear transformations i. Proceedings of
the National Academy of Sciences, 35(11):652–655, 1949.
51