Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained Models
for Spatiotemporal Modeling
WeleGedaraChamindaBandaraandVishalM.Patel
JohnsHopkinsUniversity
https://github.com/wgcban/apt
Abstract networks in natural language processing (NLP) to model
sequences of data with non-local self-attention, their abil-
In this paper, we introduce Attention Prompt Tuning ity to capture spatiotemporal information from videos for
(APT)–acomputationallyefficientvariantofprompttun- downstream applications like action recognition has been
ing for video-based applications such as action recogni- explored[42,58],yieldingbetterresultscomparedtotheir
tion. Prompt tuning approaches involve injecting a set of CNNcounterparts. TheadoptionofTransformernetworks
learnablepromptsalongwithdatatokensduringfine-tuning hasbeenfurtheracceleratedbytheadvancementsinefficient
whilekeepingthebackbonefrozen. Thisapproachgreatly self-supervisedrepresentationlearningapproachesforpre-
reducesthenumberoflearnableparameterscomparedto training,suchasmaskedautoencoders(MAEs)[5,39,47].
fulltuning. Forimage-baseddownstreamtasks,normallya Transformers [13, 33, 48] have revolutionized a wide
coupleoflearnablepromptsachieveresultsclosetothose range of fields, including natural language, audio, image,
of full tuning. However, videos, which contain more com- andvideoprocessing[21]. Theyhavebeenappliedtovari-
plexspatiotemporalinformation,requirehundredsoftunable ousdownstreamtasks,suchaslanguagetranslation[48],au-
promptstoachievereasonablygoodresults.Thisreducesthe dioclassification[18],imageclassification[13],andaction
parameterefficiencyobservedinimagesandsignificantlyin- recognition [5, 47]. The de facto standard when utilizing
creaseslatencyandthenumberoffloating-pointoperations transformer backbones for downstream tasks is full fine-
(FLOPs)duringinference. Totackletheseissues,wedirectly tuning,i.e. tuningthebackboneandatask-specificheadby
injectthepromptsintothekeysandvaluesofthenon-local initializingthebackbonewithpre-trainedweights[5,16,47].
attention mechanism within the transformer block. Addi- Although full fine-tuning has demonstrated strong perfor-
tionally, we introduce a novel prompt reparameterization manceinmostcases,itpresentscertaindisadvantages.These
techniquetomakeAPTmorerobustagainsthyperparameter include:
selection. TheproposedAPTapproachgreatlyreducesthe 1. parameterinefficiency—fullfine-tuningcanleadtoasig-
numberofFLOPsandlatencywhileachievingasignificant nificantincreaseinthenumberofparameters,especially
performanceboostovertheexistingparameter-efficienttun- when multiple tasks are involved, as it does not share
ingmethodsonUCF101,HMDB51,andSSv2datasetsfor parametersbetweentasks;
action recognition. The code and pre-trained models are 2. largelabeleddatarequirements—fullfine-tuningneces-
availableathttps://github.com/wgcban/apt sitatesasubstantialvolumeoflabeleddatatoavoidover-
fittingandensuregoodgeneralization;
3. computationaldemands—fullfine-tuningiscomputation-
1.Introduction allyintensiveandtime-consuming,requiringsignificant
computationalresources;
Video-based action recognition is the task of identifying 4. overfitting—thereisariskofoverfittingtothenewdata,
humanactivities,includinggestures,simpleactions,inter- particularlyifthetrainingdataislimited;
actionsinvolvinghumansandobjects,fromvideos[19,29, 5. catastrophicforgetting—fine-tuningonanewtaskmay
42,43]. Encodingtemporalinformationiscrucialforrec- causethemodeltoforgetpreviouslylearnedknowledge
ognizing different activities [58]. The early action recog- fromthepre-trainingtask.
nition models primarily relied on deep neural networks Asaresult,alternativemethods,includinglinearprobing,
(DNNs)[58],owingmuchtothesuccessofconvolutional adaptertuning[10],andvisualprompttuning(VPT)[25],
neuralnetworks(CNNs)inencodingthespatiotempoalinfor- have been proposed, each with its own set of trade-offs
mation. However,withtheintroductionofTransformer[48] (see Fig. 1). Linear probing involves tuning only a task-
1
4202
raM
11
]VC.sc[
1v87960.3042:viXraClassifier Classifier Classifier Classifier Classifier
MLP
MLP MLP MLP MLP
LayerNorm
LayerNorm LayerNorm LayerNorm LayerNorm
Trainable
Multi-Head Frozen
Attention
Multi-Head Multi-Head Multi-Head Multi-Head
C Concatenation
Attention Attention Attention Attention
LayerNorm
Sum
LayerNorm LayerNorm LayerNorm LayerNorm
C
Tokenizer Tokenizer Tokenizer Tokenizer Tokenizer
(a) Linear Probing (b) Adapter (c) VPT (d) APT (ours) (e) Full fine-tuning
Figure1.ComparisonofourAttentionPromptTuning(APT)forvideosactionclassificationwithotherexistingtuningmethods:linear
probing,adaptertuning[10],visualprompttuning(VPT)[25],andfullfine-tuning.
specificheadwhilekeepingthebackboneparametersfixed tokens) [5, 39, 47]. Consequently, VPT for videos often
(frozen). Itishighlyparameter-efficientandcomputationally necessitatesappendingasubstantialnumberofprompts(ap-
lessdemanding. However, itsperformanceisusuallylim- proximately 800 prompts) to the input tokens to achieve
itedbythefixednatureofthebackbone. Ontheotherhand, performance levels comparable to full fine-tuning. How-
adaptertuning[10]introduceslearnableMulti-LayerPer- ever,thisapproachpresentschallenges. Thelargenumber
ceptron(MLP)layersineachtransformerblock,providinga ofappendedpromptsincreasescomputationalcomplexity
balancebetweenparameterefficiencyandmodelflexibility. (FLOPs)andlatencyduringbothtrainingandinference. El-
Itallowsfortask-specificadaptationswithoutchangingthe evated latency is particularly concerning for video-based
entirebackboneandhasshownbetterperformancethanlin- downstreamapplications,whichalreadydemandsignificant
earprobing. Theaddedadapterlayersincreasethenumber timeandcomputationalresources. Assuch,theincreased
of trainable parameters (but far less than full fine-tuning) latencyandFLOPscompromisetheflexibilityandparameter
andFLOPS(hencelatency)comparedtolinearprobing. In efficiencyofvisualprompttuningforvideos,highlighting
contrast,VPT [25]adoptstunablepromptsthatarefedinto theimportanceofreducingcomputationalcomplexitywhile
themodelalongwiththeinputtokens. Thesepromptsact maintainingparameterefficiency.
astask-specificguides,allowingthemodeltoretainthepre-
trained knowledge of the backbone while adapting to the In our experiments we have observed that interactions
newtask. VPTishighlyflexible, parameterefficient, and betweenvideotokenrepresentationsandpromptsprimarily
hasbeenshowntoachievebetterperformancethanlinear occurduringthemulti-headattention(MHA)inthetrans-
probingandprompttuningacrossawidevarietyofdown- former block. This is because both the MLPs and Layer
streamtasksspeciallythoseinvolvingimages[25].However, Normalization(LayerNorm)[1]operationsbeforeandafter
wehaveobservedthatdirectadaptationofVPTfordown- theattentionmechanismareappliedalongtheembedding
stream tasks involving videos, such as action recognition, dimension separately for each token/prompt. As a result,
leadstohighcomputationrequirements,unstabletraining, thelayersprecedingtheMHAserveasfixedreparameteriza-
andincreasedFLOPsduringtrainingandinference. tionfortheprompts,renderingthecomputationsonprompts
followingtheattentionmechanismredundant. Thisredun-
Videosinherentlyconsistofasignificantlylargernum- dancyarisesbecausethecurrentpromptsarediscardedatthe
beroftokenscomparedtoimages(typically1568vs. 196 endofeachtransformerlayer,andnewpromptsaresubse-
2
Prompts
Promptsquentlyappendedforthefollowinglayer. Motivatedbythis representations.Intheearlyphase,thiswasaccomplishedby
observation,weproposeintroducingpromptsdirectlyinto trainingspecificmodelsonactionrecognitiondatasetssuch
theattentionmechanism,therebyreducingredundancyand asHMDB51[29],UCF101[43],andSomething-Something-
computationalcomplexity. Ourapproach,namedAttention v2(SSv2)[19]. Someoftheseearlyworksonactionrecog-
PromptTuning(APT),minimizesextraneouscomputations, nition include DeepVideo [26], which attempts to utilize
reduces latency, and enhances the effectiveness of visual convolutionalneuralnetworks,andTwo-StreamNetworks,
prompttuningforvideo-basedtasks. which introduced a second pathway to learn temporal in-
Moreover,wenotedthatVPTishighlysensitivetohy- formationinvideosbytrainingaconvolutionalneuralnet-
perparameter selection, such as learning rate and weight work on the optical flow stream. Two-Stream Networks
decay[28],andoftenrequiresalongerfine-tuningtime(ap- markedasignificantsuccessinactionrecognition,leading
proximatelytwiceaslong)comparedtofullfine-tuning.This tomanyfollow-upworkssuchasTDD[50],LRCN[12],Fu-
sensitivityisobservedinotherfieldsaswell,includingnatu- sion[14],TSN[51],etc. However,thetemporaldimension
rallanguageprocessingandimageprocessing[31,32].Since typicallymakesactionrecognitionchallenging.Additionally,
video fine-tuning generally takes longer than image fine- existingdeeparchitecturesgenerallyencodetemporalinfor-
tuning,acceleratingconvergenceandenhancingrobustness mationwithlimitedsolutionssuchas3Dfilters(examples
tohyperparameterselectionareofgreatinterest. Previous include I3D [7], R3D [22], S3D [54], non-local [52], and
worksonprompttuning[32]suggestpromptreparameteri- SlowFast[15]),pre-computedmotionfeatures,andrecurrent
zationasasolutiontothesechallenges,whereinpromptsare neuralnetworks(RNNs)[49]. Thesemodelsaretypically
processedthroughareparameterizationnetworkconsisting limitedinsimultaneouslycapturinglocalandglobalvaria-
ofasmallMLPlayer. However,addinganMLPlayersignif- tionsoftemporalfeatures. Incontrast,theTransformeris
icantlyincreasesthenumberofparametersandintroduces anovelarchitecturethatemploystheattentionmechanism
additionalcomputationalcost. Toaddressthis,weintroduce todifferentiallyweigheachinputpatches. AlthoughTrans-
scaling-basedreparameterizationasasolutionthataddsa formersaredesignedtohandlesequentialinputdata,they
significantly lower number of parameters while incurring donotnecessarilyprocessitinorder. Instead,theattention
negligiblecomputationalcost. Inthispaper,weexplorethe mechanism provides context for any position in the input
benefitsandeffectivenessofthisapproachforvideo-based sequence.
actionrecognition.
2.2.Transformers
Insummary,ourcontributionsareasfollows:
1. We introduce Attention Prompt Tuning (APT) – a TheTransformerarchitecture[2,4,13,38,48], originally
parameter-efficientfine-tuningtechniquespecificallyde- introducedfornaturallanguageprocessing(NLP)tasks,has
signedforvideo-basedactionrecognition. been successfully applied to computer vision with the de-
2. APTinnovativelyinjectslearnablepromptsdirectly velopmentoftheVisionTransformer(ViT)[13]. ViTcon-
intothenon-localattentionmechanismsofTransform- vertsimagesintosmallpatches(tokens)andprocessesthem
ers. Thissignificantlyenhancesparameterefficiencyand throughmulti-headattentionmodules,enablingtaskssuch
reduces redundant computations compared to existing asclassification[5,6,13],segmentation[3,8,44],detection
methodssuchasVisualPromptTuning(VPT). [3],synthesis[9,37,57],andmore. Withthedevelopment
3. Wealsointroduceanovelpromptreparameterization ofself-supervisedpre-trainingmethodssuchasmaskedau-
technique,makingthetrainingprocesslesssensitiveto toencodersfortransformers(MAEs)[5,17,24,41,47],the
hyperparameterselection. usecasesoftransformershaveexpandedbyleveragingunla-
4. TheeffectivenessofourproposedAPTisvalidatedon beleddataforpre-training,whichprovidesexcellenttransfer
threepublicactionrecognitionbenchmarks: UCF101, learningcapabilityforvariousdownstreamapplicationswith
HMDB51, and Something-Something-v2. We demon- limitedsupervisedlabels[5,24,47]. Despitethesuccessof
stratethatourmethodachievessuperioractionrecogni- pre-trainedtransformermodels,fine-tuningtheentireback-
tionperformancewhileutilizingfewertunableparame- bonefordownstreamtasksiscommonlypracticedbutoften
ters. inefficientintermsofthenumberoftunableparametersand
computationalresources. Consequently,therehasbeensig-
2.RelatedWork nificantinterestinexploringparameter-efficienttechniques
foradoptingpre-trainedmodelsindownstreamapplications
2.1.ActionRecognition
[10,25,31,32].
Humanactionrecognitionisanimportanttaskinvideoun-
2.3.EfficientFine-tuning
derstanding. It aims to recognize human actions in video
clips. Researchers have proposed various approaches to Efficienttuningmethodscanbedividedintotwocategories:
recognizingactionsinvideosbylearningspatio-temporal (1)network-basedmethodsand(2)prompting-basedmeth-
3ods. Network-basedmethods,suchasAdaptFormer[10,36], avoidstheredundantcomputationsbyMLPandLayerNorm
ST-Adapter[35],andAIM[55],introduceasmall,learnable layersonprompttokensfollowingtheMHA,unlikeinVPT.
networkbeforeoraftereachmulti-headattentionmodule, Formally, the non-local attention operation in MHA is
whicharefine-tunedwhilekeepingtherestoftheparame- definedas,
tersfixed. Thisapproachsignificantlyimprovesparameter
(cid:18) QKT(cid:19)
x=Softmax √ V, (1)
efficiency compared to end-to-end tuning. However, the d
additionofthesenetworksslightlyaltersthebackbonearchi-
whereQ,K,andV denotetheQueries,Keys,andValues,
tecture,whichisadrawback. Incontrast,prompting-based
respectively,anddisthesizeoftheembeddingdimension.
methods[20,23,25,30,45,53,56],originallyinspiredby
In APT, we modify the Keys and Values of the input by
prompttuninginNLP[31,32,46],involveinsertingasetof
appendingasetoflearnableprompts, denotedbyK and
learnablepromptsalongwiththeinputtokensduringtuning. p
V ,asfollows,
Prompttuningisrelativelysimpleandeasilyapplicable p
to downstream tasks. However, it has been observed that
K =K ⊕K , and V =V ⊕V , (2)
x p x p
prompttuningachievessignificantlylowerperformancethan
full-tuningincomplexvideo-basedapplicationslikeaction where⊕representsconcatenation. Here,K andV arethe
x x
classification. Moreover, prompt tuning requires a higher originalKeysandValuesderivedfromtheinputtokens. The
numberoftunablepromptsforvideoapplications,resulting concatenationresultsinKeysandValueswithdimensionsof
in increased latency and computational requirements. In size(n +n ,d),wheren isthenumberofinputtokens
x p x
thispaper,weeliminateredundantcomputationsofprompt andn isthenumberofprompts. ThedimensionofQueries
p
tuningbydirectlyinjectingpromptsintotheattentionmech- (Q)remainsunchangedat(n ,d). Consequently,theoutput
x
anism. aftertheMHAoperationretainsthesamesizeastheinput,
whichis(n ,d).
x
3.Method
TheefficientdesignofAPTimprovescomputationaleffi-
ciencybyreducingtheoverheadintroducedbyVPT,making
3.1.VisualPromptTuning(VPT)
itparticularlyadvantageouswhendealingwithlargenum-
As depicted in Fig. 1, VPT [25] appends a set of learn- bersofprompts,asinthecaseinvideo-basedapplications.
ablepromptstotheinputtokens. Thecombinedinput(to-
3.3.PromptReparameterization
kens+prompts)isthenprocessedbytheTransformerBlock,
whichconsistsofasequenceofcomponents: LayerNorm, Duringourexperiments,weobservedthattheperformance
MultilayerPerceptrons(MLPs),andMulti-HeadAttention ofprompttuningmethodsishighlysensitivetotheproper
(MHA).Notably,LayerNormandMLPsoperatealongthe selectionofhyperparametervalues,especiallylearningrate
embeddingdimension,meaningthattheinteractionbetween andweightdecay. Thissensitivityhasalsobeenobserved
thepromptsandtheinputtokenfeaturesoccursprimarily inprompttuningappliedtootherapplicationsinNLPand
withintheMHAcomponent. Consequently,theLayerNorm imageclassification. Furthermore,wenoticedthatprompt
andMLPcomponentsprecedingtheattentionmechanism tuning typically requires a slightly higher number of fine-
can be viewed as a fixed reparameterization network for tuningepochscomparedtofull-modelfine-tuning. Acom-
the prompts. However, the LayerNorm and MLP compo- monpracticetomakeprompttuningmorerobusttovaria-
nentsfollowingtheMHAareredundantbecausethecurrent tionsinlearningrateandotherhyperparametersistousea
promptsarediscardedattheendofthecurrentTransformer reparameterizationnetwork,whichofteninvolvesprocessing
Block, and a new set of prompts is appended to the next thepromptsthroughanadditionalMLPnetwork. However,
block. Motivatedbythisobservation,weintroduceanop- addinganMLPlayertoalargenumberofpromptsincreases
timization to reduce the extra computational redundancy computationalcomplexity,latency,andthenumberoftun-
introducedbyVPT.Specifically,weintroducetheprompts ableparameters. Therefore,weintroduceascaledreparam-
directlyintotheattentionmechanism,therebysavingsignifi- eterizationmethodinwhicheachpromptinK andV is
p p
cantcomputation–abenefitthatisparticularlyvaluablefor scaledbyalearnablescalar. Moreprecisely,thereparame-
video-baseddownstreamtasks. terizationoftheKeyandValuepromptsisasfollows,
3.2.AttentionPromptTuning(APT) K˜ =max(s ,1.0)×K , (3)
p k p
AttentionPromptTuning(APT)injectslearnableprompts V˜ =max(s ,1.0)×V , (4)
p v p
directly into the MHA unlike VPT, as shown in Fig. 2.
Specifically, we concatenate learnable prompts only with wheres kands
v
arethelearnablescalarsofsizeRnp. Impor-
Keys(K)andValues(V),ensuringthattheoutputfromthe tantly,s ands areinitializedto1.0. Theproposedscaled
k v
MHA has the same number of tokens as the input. This reparameterizationenablesthedirectadjustmentofprompt
4Linear
Concat
Multi-Head Attention
C C
C C
C C
Linear Linear Linear
Figure2.TheproposedAPTarchitecture.Modulesincyancolorarefrozenandnotupdatedduringfine-tuning.Modulesinpurplecolor
arelearnedandupdatedduringfine-tuning.Learnableprompts(K andV )aredirectlyinjectedintotheMHAbyconcatenatingthemwith
p p
theKeys(K )andValues(V ),whiletheQueries(Q )remainunchanged.
x x x
valuesthroughatunablescalingfactor,leadingtofastercon- 5.Results
vergencewithoutsacrificingperformance. Thenumberof
5.1.AblationExperiments
GLOPSarecalculatedusingfvcoreflopcounttool1.
For all ablation experiments, we employ the ViT-Small
(21.947M params.) [13] model pretrained on the SSv2
4.ExperimentalSetup (Something-Something v2) dataset [19]. The pretrained
weightsareobtainedfromVideoMAE[47]. Wereportboth
For all of our experiments, we utilize vanilla transformer top-1andtop-5accuracymetricsforactionclassificationon
architectures, specifically ViT-Small and ViT-Base [5, 13, the test set of the SSv2 dataset [19], unless specified oth-
47]. Weinitializethemodelusingpretrainedweightsfrom erwise. Thebest(i.e.,default)configurationishighlighted
theVideoMAEmethod[5,47]. Unlessotherwisenoted,we ingreycolorandusedinthemainanalysiswithViT-Base
employ the AdamW optimizer [34]. The models are fine- architecture. During evaluation, we adhere to a common
tunedfor100epochs,whichincludesa10-epochwarm-up protocolthatinvolvesa2-view,3-temporal-clipevaluation.
period,followedbyacosineschedulertoreducethelearning Thisprotocolinvolvesextractingmultipletemporalclipsand
rateto0.Aspartofourdataaugmentationstrategy,weapply views from each video and aggregating the predictions to
RandomAugment[11]. Thebaselearningrateisspecified producethefinalclassificationresult.
for a batch size of 256, and it is automatically scaled for
differentbatchsizesasneeded. Toexpeditefine-tuning,we
Effect of Prompt Length Table 1 shows the impact of
useabatchaugmentationfactorof2,meaningthatforeach
varyingthenumberofprompts(i.e.,promptlengthn )in
loadedvideo,wecreatetworandomviews. Forevaluation, p
theproposedAPTonthetunableparameters,GFLOPs,and
weadheretothecommonpracticeofmulti-viewtesting. We
Top-1/Top-5accuracyonSSv2.Increasingthepromptlength
use K temporal clips to span the video length (with K=3
initiallyleadstoanimprovementinbothTop-1andTop-5
forSSv2,10forHMDB51,and5forUCF101 [5,41,47]),
accuracy. ThepeakTop-1accuracyof57.57%andTop-5ac-
andforeachclip,wetakethreespatialviewstocoverthe
curacyof84.15%areachievedwith1400prompts.However,
entireimage. Thefinalpredictionisobtainedbyaveraging
furtherincreasingthenumberofpromptsto1600or2000
theresultsacrossallviews.
doesnotleadtoasignificantimprovementinaccuracy,and
insomecases,theTop-1accuracyslightlydecreases. This
1https://github.com/facebookresearch/fvcore/ suggeststhattheremaybeanoptimalnumberofpromptsfor
blob/main/fvcore/nn/flop_count.py thismodelanddataset,beyondwhichtheadditionalprompts
5
marapeR marapeRTable 1. Effect of prompt length (n p) on APT with ViT-Small 1.4% of the total tunable parameters). This enhancement
backboneonSSv2. inbothperformanceandconvergencespeedisparticularly
valuable for video-based downstream applications, where
Params. Top-1 Top-5 fine-tuning on video datasets is both time-consuming and
n
p (M) (%) (%) computationallydemanding.
400 0.692(3.2%) 55.81 83.17 Table2.PerformancefordifferentdropoutratesappliedtoK and
p
600 1.003(4.5%) 56.52 83.76 V p.
800 1.316(6.0%) 56.74 83.87
Dropout Top-1(%) Top-5(%)
1000 1.628(7.4%) 57.18 84.21
1200 1.940(8.8%) 57.44 84.24 0% 55.21 82.66
1400 2.252(10.3%) 57.57 84.15 10% 57.57 84.15
1600 2.564(11.7%) 57.07 84.14 20% 52.14 80.80
2000 3.188(14.5%) 57.55 84.03
Effect of Dropout on Attention Prompts Through ex-
donotcontributesignificantlytoperformanceimprovement.
perimentation,wefoundthatapplyingdropouttoattention
prompts K and V improved both top-1 and top-5 accu-
p p
w/o reparam
racy. Dropoutservesasaregularizationtechnique,reducing
w/ reparam
4 overfitting and promoting diverse prompt representations.
Table 2 shows action recognition performance for differ-
3 ent dropout rates, with the best results achieved at a 10%
rate,whichbalancedinformativenessandvariabilityinthe
2 prompts. Overall,dropoutinprompttuningenhancesclassi-
ficationperformanceandgeneralization.
1
40k 80k 120k
# itterations
0%
10%
20%
Figure 3. Training loss comparison: with and without prompt 0 1% 0% 0 1% 0%
20% 20%
reparameterization.Theuseofscaledreparameterizationresultsin
(a) Top-1 (val-set) (b) Top-5 (val-set) (c) Val-loss
fasterconvergenceandalowertrainingloss.
Figure4. Effectofdropoutonattentionprompts. Applying10%
dropouttoattentionpromptsresultsinthebestperformanceon
bothvalidationandtestsets.
EffectofPromptReparameterization Aspreviouslydis-
cussed,prompttuningmethodsarehighlysensitivetohyper-
Furthermore, Figure 4 illustrates the curves for Top-1
parameterselection,especiallywithregardstothechoiceof
accuracy, Top-5 accuracy, and validation loss at different
learningrateandweightdecayparameters. Toenhancethe
dropoutlevels. Thefiguresrevealthat,initially,theaccura-
robustnessofAPTtohyperparameterselection,weintroduce
ciesforbothTop-1andTop-5werehigherwhennodropout
apromptreparameterizationtechniquethatrequiresminimal
was applied. However, as training progressed, the results
computationaloverheadandintroducesonlyasmallnumber
improvedsignificantlywithadropoutrateof10%. Thisin-
oftunableparameters. Inthisanalysis,weexaminethecon-
dicatesthatutilizingdropoutinattentionpromptseffectively
vergenceoftraininglossduringfine-tuningbothwithand
prevents the network from overfitting to the prompts and
without the application of prompt reparameterization. As
playsacrucialroleinoptimizingattentionprompttuningto
illustratedinFig. 3,utilizingscaledreparameterizationleads
achievesuperiortestresults.
to faster convergence and a lower training loss compared
totheabsenceofreparameterization. Thisapproachresults
inimprovedperformance,asevidencedbya1.02%gainin EffectofRandomAugmentations Duringfull-finetun-
top-1accuracy(55.81%vs. 54.85%)anda0.93%gainin ing, extensive data augmentations are typically applied
top-5accuracy(83.17%vs. 82.24%). Notably,theobserved throughRandomAugment[11]toenhancethemodel’sgen-
improvementisachievedwithanegligibleincreaseintun- eralizationabilitybyintroducingdiversityintothetraining
ableparameters(2×400×12 = 9600,representingonly data. However,ourexperimentswithAPT,aswellaswith
6
ssoLTable3.EffectofRandomAugmentations(RA). weight decay regularization (i.e., setting weight decay to
zero)alsoresultedinsub-optimalperformance,astheatten-
Configuration Top-1(%) Top-5(%) tionpromptsbecomemoresusceptibletooverfittingtothe
trainingdata.
NoAug. 55.81 83.17
Through hyperparameter tuning, we determined that a
RA(m2-n2-mstd0.2-inc1) 55.82 83.42 moderateweightdecayvalueof1e−5yieldedimproved
RA(m7-n4-mstd0.5-inc1) 53.94 82.23 performance,achievinganoptimalbalancebetweenprevent-
ingoverfittingandallowingflexibilitytoattentionprompts.
Theperformanceresultsfordifferentweightdecayvalues
linearprobingandVPT,revealedadifferentbehavior. We aresummarizedinTable4.
observedthatapplyingthesameextentofRandomAugmen-
tationsduringAPT,linearprobing,andVPTdidnotleadto 11ee--24
1e-5
0
performanceimprovementsand,insomeinstances,resulted
indiminishedperformance. Wehypothesizethatthisbehav- 11ee--24 11ee--24
1e-5 1e-5
iorisattributabletothefactthatthebackboneremainsfrozen 0 0
duringAPT,linearprobing,andVPT,therebylimitingthe (a) Top-1 (val-set) (b) Top-5 (val-set) (c) Val-loss
capacity for overfitting. As training relies on a relatively Figure6. EffectofweightdecayregularizationontrainingAPT.
smallfractionoftunableparameters,theneedforaggressive Applyingsmallweightdecayregularizationresultsinbetterperfor-
dataaugmentationisreduced. mance.
mNo7 -Anu4g-.mstd0.5-inc1
m2-n2-mstd0.2-inc1
mN mo7
2
- -An nu4 2g- -.m ms st td d0 0. .5 2- -i in nc c1
1
mN mo7
2
- -An nu4 2g- -.m ms st td d0 0. .5 2- -i in nc c1
1
60
80
(a) Top-1 (val-set) (b) Top-5 (val-set) (c) Val-loss
Figure5.EffectofrandomaugmentationsontrainingAPT.Higher
magnitude of augmentations in training results in lower action 40
60
recognitionperformancewhilelight/noaugmentationsresultsin
goodactionrecognitionperformance.
Table4.EffectofWD. 20 40
0 3 6 9 12
PromptDepth
WD Top-1(%) Top-5(%)
Shallow→Deep(Top-1) Shallow→Deep(Top-5)
1e-2 51.43 79.98 Deep→Shallow(Top-1) Deep→Shallow(Top-5)
1e-4 55.47 83.35
Figure7.EffectofpromptdepthwithAPT.
1e-5 55.81 83.17
0 55.24 83.05
EffectofAttentionPromptPlacement Inthedefaultcon-
figurationofAPT,weappendattentionpromptstotheMHA
EffectofWeightDecay(WD) Duringthefine-tuningof ateachTransformerBlockwithintheViT.Whilethisplace-
APT,weinvestigatedtheimpactofweightdecayregulariza- mentstrategyyieldsthebestperformance,ourexperiments
tiononmodelperformance. Ourexperimentsrevealedthat revealthattheplacementofattentionpromptsatdifferent
thechoiceofweightdecayvaluealsoplaysacrucialrolein depthsoftheTransformerBlockshasvaryingcontributions
theeffectivenessofAPT. to the overall performance. Specifically, we find that at-
Specifically, as shown in Figure 6, applying an exces- tentionpromptsplacedatdeeperTransformerBlocks(i.e.,
sivelyhighweightdecayvalue(like1e−2utilizedinfull- blocks closer to the output layer) have a more substantial
tuning)ledtoadeteriorationinperformance,likelydueto impactonperformancecomparedtopromptsplacedatshal-
over-constraining the attention prompts values and which lower Transformer Blocks (i.e., blocks closer to the input
hindersitsabilitytochangetheinputtokenrepresentations layer). Thisobservationsuggeststhatrefiningfeaturerep-
inthedesireddirection. Conversely,completelyremoving resentationsatdeepertransformerlayersismoreimportant
7
)%(1-poT )%(5-poTfor downstream performance than the features at shallow (VPT[25]andAdaptFormer[10])onthreeactionrecogni-
transformerlayers. Figure7illustratestheperformanceim- tiondatasets: SSv2[19],UCF101[43],andHMDB51[29].
provementachievedbyincrementallyappendingattention Notethat,fortheseexperiments,weutilizeVideoMAE[47]
prompts,startingfromthedeepestTransformerBlocksand pre-trainedbackboneswhichiscurrentlythestate-of-the-art
progressivelyextendingtheplacementtoshallowerblocks. pre-trainedmodelsforViTbackbones.
The results highlight the importance of attention prompt For the UCF101 and HMDB51 datasets, our APT
placementintheAPTframeworkanditsinfluenceonmodel achievesresultsclosetofull-tuningwhileusinglessthan1%
performance. oftunableparameters. Notably,forUCF101,APTachieves
higheraccuracythanfull-tuning(97.7%vs. 96.1%top-1)
with only 200 attention prompts, equivalent to 0.45% of
tunableparameters. OntheHMDB51dataset,APTresults
2
areslightlylowerthanfull-tuning(70.12%vs. 73.3%),but
stillveryclose. However,APToutperformsVPTandAdapt-
Formerbyasignificantmargin,reducingtheperformance
1
gapbetweenfull-tuningandparameter-efficientmethods.
WhencomparingresultsontheSSv2dataset, whichis
0 largerandhasmorecomplexactionclasses,thereisapproxi-
matelya10%performancegapbetweenAPTandfull-tuning.
1 3 2 6 4 1 2 8 2 5 6 Thisdifferencemaybeattributedtothecomplexityoftheac-
Promptlength
tionclassesinthedataset. However,whencomparingAPT
220 with VPT and AdaptFormer, APT achieves better results
withsignificantlyfewertunableparameters(60.79%top-1
with0.81%tunableparametersvs. 59.20%top-1with1.46%
tunableparameters).
200
Overall,APTsignificantlyreducesthegapbetweenfull-
tuningandexistingparameter-efficientmethods,establishing
itselfasthenewstate-of-the-artforparameter-efficienttun-
180 inginactionrecognition. Additionally,APTfurtherreduces
the number of tunable parameters required to adopt pre-
1 3 2 6 4 1 2 8 2 5 6
trainedtransformerbackbonesforvideo-baseddownstream
Promptlength
applications.
Figure8.Comparisonoftunableparams.(left)andGFLOPs(right)
withpromptlengthforVPTandAPT(Ours).TheproposedAPT 5.3.2 Resultswithotherpre-trainedbackbones
modeldemonstratesasignificantreductioninboththenumberof
TodemonstratethattheproposedAPTisnotdependenton
tunableparametersandGFLOPs,highlightingitsefficiencyand
optimizedperformancecomparedtoVPT. aspecificpre-trainedmethodandperformswellwithother
pre-trained approaches, such as CLIP and supervised pre-
trainedmodels,weconductedexperimentsonSSv2using
5.2.ComputationalComplexity the ViT-Base backbone. The results of these experiments
aresummarizedinTable6. Fromthedatapresentedinthe
WecomparecomputationalandparameterefficiencyofAPT
table, it is evident that the proposed APT achieves SOTA
with VPT [25] in Fig. 8. The results demonstrate that in-
resultsevenwhencomparedtootherpre-trainedapproaches,
jecting prompts directly into the multi-head attention sig-
allwhilemaintainingparameterefficiency.
nificantly reduces the number of tunable parameters and
Taken together, all the aforementioned results affirm
GFLOPs, unlike VPT, which concatenates prompts with
APT’sperformanceasabetterparameter-efficienttraining
the input tokens. The reduced number of GFOPs of APT
methodthanexistingapproaches,particularlywell-suited
isbeneficialformoreefficientandscalabledeploymentof
for handling spatiotemporal data – which often involves
pre-trainedtransformermodelsforvideoapplications.
trainingwithalargenumberoftokensthanimages.
5.3.MainAnalysis
6.ConclusionandFutureWork
5.3.1 ResultswithVideoMAEpre-trainedbackbones
Inthispaper,weproposedAPT,avariantofVPT,foreffi-
Table5comparestheresultsofourAPTwithlinearprob- cientadaptationofpre-trainedtransformermodelsindown-
ing, full-tuning, and existing parameter-efficient methods streamtasks. WhileVPToffersflexibilityandcanbeeasily
8
sPOLFG
)M(.smaraPTable5.ComparisonofAPTperformancewithotherparameterefficienttuningmethods.WeuseVideoMAE[5,47]pre-trainedViT-Base
backboneastheinitialization.
Tuned SSv2 UCF101 HMDB51
Method GFLOPs
Params. (M) top-1% top-5% top-1% top-5% top-1% top-5%
Linear-Probing 0.07(0.08%) 180.03 29.23 N/A 94.3 99.5 49.84 75.9
VPT-1 0.08(0.09%) 180.50 43.73 N/A 94.1 N/A 52.76 N/A
VPT-5 0.12(0.14%) 180.60 45.87 N/A 94.7 N/A 53.66 N/A
AdaptFomrer-1 0.10(0.12%) 180.51 50.03 N/A 94.3 99.5 51.68 78.3
AdaptFomrer-4 0.15(0.17%) 180.60 54.70 N/A 94.5 99.5 51.81 78.9
AdaptFomrer-64 1.26(1.46%) 182.33 59.02 N/A 94.5 99.7 55.69 79.4
APT-1(ours) 0.081(0.09%) 180.51 42.92 72.18 96.4 99.8 58.23 83.09
APT-5(ours) 0.087(0.09%) 180.63 49.15 78.50 96.5 99.9 61.43 87.12
APT-100(ours) 0.235(0.27%) 183.38 57.15 84.85 97.6 100.0 67.58 89.67
APT-200(ours) 0.391(0.45%) 186.27 59.43 86.39 97.7 100.0 68.17 88.63
APT-400(ours) 0.703(0.81%) 192.05 60.79 87.15 97.7 99.9 70.12 90.78
Full-tuning 86.36(100%) 180.03 70.8 92.4 96.1 99.4 73.3 N/A
Table 6. The comparison of APT with other parameter-efficient methods using other pre-trained approaches, such as CLIP [40] and
supervisedpre-trainingonK400[27]withtheViT-BasebackboneontheSSv2dataset.Reportedresultsindicatetop-1accuracy.
TuneParams. Pre-trainingmethod
Method
(M) CLIP(Top-1%) SupervisedK400(Top-1%)
LinearProbing 0.07(0.08%) 21.9 29.3
VPT 0.08(0.09%) 45.1 33.1
AdaptFormer-4 0.15(0.17%) 59.1 42.3
AdaptFormer-64 1.3(1.46%) 63.4 46.0
ST-Adapter 7.20(8.33%) 66.3 NA
AIM 14.3(16.56%) 66.4 NA
APT-200(ours) 0.39(0.45%) 64.8 47.2
APT-400(ours) 0.70(0.81%) 66.8 50.3
APT-600(ours) 1.01(1.17%) 66.8 50.4
Full-tuning 86.36(100%) 66.9 55.7
adaptedtovariousapplications,itrequiresasignificantnum- tiontechniquetoenhancetuningconvergenceandrobustness
berofpromptstoachievesatisfactoryresultsinvideo-based tohyperparameterselection. Overall,APTachievessuperior
taskslikeactionclassification,leadingtoincreasedFLOPs results,sometimessurpassingfull-tuning,withhighparam-
and latency during inference and hindering parameter ef- eter efficiency and minimal increase in GLOPs compared
ficiency. In contrast, our APT approach injects attention toexistingapproaches. However,alimitationofAPTand
prompts directly into the attention mechanism, resulting otherparameter-efficientmethodsisthepotentialincrease
in reduced FLOPs and improved performance compared inFLOPsandlatencyduringdeployment,whichwarrants
toVPTandotherparameter-efficientmethodslikeAdapt- furtherexplorationasfuturework.
Former. Wealsointroducedanovelpromptreparameteriza-
9References [12] JeffDonahue,LisaAnneHendricks,MarcusRohrbach,
Subhashini Venugopalan, Sergio Guadarrama, Kate
[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E
Saenko,andTrevorDarrell. Long-termrecurrentcon-
Hinton. Layer normalization. arXiv preprint
volutionalnetworksforvisualrecognitionanddescrip-
arXiv:1607.06450,2016.
tion. IEEETransactionsonPatternAnalysisandMa-
[2] WeleGedaraChamindaBandaraandVishalM.Patel.
chineIntelligence,39(4):677–691,2017.
Hypertransformer: Atexturalandspectralfeaturefu-
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander
sion transformer for pansharpening. In Proceedings
Kolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
oftheIEEE/CVFConferenceonComputerVisionand
Unterthiner, Mostafa Dehghani, Matthias Minderer,
PatternRecognition(CVPR),pages1767–1777,2022.
Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and
[3] WeleGedaraChamindaBandaraandVishalM.Patel.
Neil Houlsby. An Image is Worth 16x16 Words:
A Transformer-Based Siamese Network for Change
Transformers for Image Recognition at Scale, 2021.
Detection. InIGARSS2022-2022IEEEInternational
arXiv:2010.11929[cs].
Geoscience and Remote Sensing Symposium, pages
[14] ChristophFeichtenhofer,AxelPinz,andAndrewZis-
207–210,2022. ISSN:2153-7003.
serman. Convolutionaltwo-streamnetworkfusionfor
[4] WeleGedaraChamindaBandaraandVishalM.Patel.
video action recognition. In 2016 IEEE Conference
Atransformer-basedsiamesenetworkforchangede-
onComputerVisionandPatternRecognition(CVPR),
tection. InIGARSS2022-2022IEEEInternational
pages1933–1941,2016.
Geoscience and Remote Sensing Symposium, pages
[15] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik,
207–210,2022.
andKaimingHe. Slowfastnetworksforvideorecog-
[5] Wele Gedara Chaminda Bandara, Naman Patel, Ali
nition. InProceedingsoftheIEEE/CVFinternational
Gholami, Mehdi Nikkhah, Motilal Agrawal, and
conference on computer vision, pages 6202–6211,
VishalM.Patel. AdaMAE:AdaptiveMaskingforEf-
2019.
ficientSpatiotemporalLearningwithMaskedAutoen-
[16] ChristophFeichtenhofer,HaoqiFan,YanghaoLi,and
coders. InProceedingsoftheIEEE/CVFConference
KaimingHe.MaskedAutoencodersAsSpatiotemporal
onComputerVisionandPatternRecognition,Canada,
Learners,2022. arXiv:2205.09113[cs].
2022. arXiv:2211.09120[cs].
[6] SrinadhBhojanapalli,AyanChakrabarti,DanielGlas- [17] Rohit Girdhar, Alaaeldin El-Nouby, Mannat Singh,
ner,DaliangLi,ThomasUnterthiner,andAndreasVeit. Kalyan Vasudev Alwala, Armand Joulin, and Ishan
UnderstandingRobustnessofTransformersforImage Misra. OmniMAE:SingleModelMaskedPretraining
Classification,2021. onImagesandVideos, 2022. arXiv:2206.08356[cs,
stat].
[7] Joa˜oCarreiraandAndrewZisserman. Quovadis,ac-
tionrecognition? anewmodelandthekineticsdataset. [18] Yuan Gong, Cheng-I Lai, Yu-An Chung, and James
In 2017 IEEE Conference on Computer Vision and Glass. Ssast: Self-supervisedaudiospectrogramtrans-
PatternRecognition(CVPR),pages4724–4733,2017. former. In Proceedings of the AAAI Conference on
[8] JienengChen,YongyiLu,QihangYu,XiangdeLuo, ArtificialIntelligence,pages10699–10709,2022.
Ehsan Adeli, Yan Wang, Le Lu, Alan L. Yuille, and [19] Raghav Goyal, Samira Ebrahimi Kahou, Vincent
YuyinZhou. TransUNet: TransformersMakeStrong Michalski, Joanna Materzyn´ska, Susanne Westphal,
Encoders for Medical Image Segmentation, 2021. HeunaKim,ValentinHaenel,IngoFruend,PeterYian-
arXiv:2102.04306[cs]. ilos,MoritzMueller-Freitag,FlorianHoppe,Christian
[9] MarkChen,AlecRadford,RewonChild,JeffreyWu, Thurau,IngoBax,andRolandMemisevic. The”some-
HeewooJun,DavidLuan,andIlyaSutskever. Gener- thingsomething”videodatabaseforlearningandeval-
ativePretrainingFromPixels. InProceedingsofthe uatingvisualcommonsense,2017. arXiv:1706.04261
37thInternationalConferenceonMachineLearning, [cs].
pages1691–1703.PMLR,2020. [20] Cheng Han, Qifan Wang, Yiming Cui, Zhiwen Cao,
[10] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu WenguanWang,SiyuanQi,andDongfangLiu. E2vpt:
Wang,YibingSong,JueWang,andPingLuo. Adapt- Aneffectiveandefficientapproachforvisualprompt
Former: Adapting Vision Transformers for Scalable tuning,2023.
VisualRecognition,2022. arXiv:2205.13535[cs]. [21] KaiHan,YunheWang,HantingChen,XinghaoChen,
[11] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao,
Quoc V. Le. RandAugment: Practical automated Chunjing Xu, Yixing Xu, et al. A survey on vision
dataaugmentationwithareducedsearchspace,2019. transformer. IEEE transactions on pattern analysis
arXiv:1909.13719[cs]. andmachineintelligence,45(1):87–110,2022.
10[22] Kensho Hara, Hirokatsu Kataoka, andYutakaSatoh. international conference on computer vision, pages
Canspatiotemporal3dcnnsretracethehistoryof2d 10012–10022,2021.
cnnsandimagenet? InProceedingsoftheIEEEcon- [34] IlyaLoshchilovandFrankHutter. DecoupledWeight
ferenceonComputerVisionandPatternRecognition, Decay Regularization, 2019. arXiv:1711.05101 [cs,
pages6546–6555,2018. math].
[23] JunxianHe,ChuntingZhou,XuezheMa,TaylorBerg- [35] Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and
Kirkpatrick,andGrahamNeubig. Towardsaunified HongshengLi. St-adapter: Parameter-efficientimage-
viewofparameter-efficienttransferlearning,2022. to-videotransferlearning. AdvancesinNeuralInfor-
[24] KaimingHe, XinleiChen, SainingXie, YanghaoLi, mationProcessingSystems,35:26462–26477,2022.
PiotrDolla´r,andRossGirshick. MaskedAutoencoders [36] Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao,
AreScalableVisionLearners,2021. arXiv:2111.06377 and Hongsheng Li. ST-Adapter: Parameter-
[cs]. Efficient Image-to-Video Transfer Learning, 2022.
[25] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire arXiv:2206.13559[cs].
Cardie, Serge Belongie, Bharath Hariharan, and [37] NikiParmar,AshishVaswani,JakobUszkoreit,Lukasz
Ser-Nam Lim. Visual Prompt Tuning, 2022. Kaiser,NoamShazeer,AlexanderKu,andDustinTran.
arXiv:2203.12119[cs]. Image Transformer. In Proceedings of the 35th In-
[26] Andrej Karpathy, George Toderici, Sanketh Shetty, ternationalConferenceonMachineLearning,pages
Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. 4055–4064.PMLR,2018. ISSN:2640-3498.
Large-scalevideoclassificationwithconvolutionalneu- [38] Malsha V. Perera, Wele Gedara Chaminda Bandara,
ralnetworks. In2014IEEEConferenceonComputer Jeya Maria Jose Valanarasu, and Vishal M. Patel.
Vision and Pattern Recognition, pages 1725–1732, Transformer-basedsarimagedespeckling. InIGARSS
2014. 2022-2022IEEEInternationalGeoscienceandRe-
[27] WillKay,JoaoCarreira,KarenSimonyan,BrianZhang, moteSensingSymposium,pages751–754,2022.
ChloeHillier,SudheendraVijayanarasimhan,FabioVi- [39] RuiQian,TianjianMeng,BoqingGong,Ming-Hsuan
ola, Tim Green, Trevor Back, Paul Natsev, Mustafa Yang, Huisheng Wang, Serge Belongie, and Yin
Suleyman,andAndrewZisserman. TheKineticsHu- Cui. SpatiotemporalContrastiveVideoRepresentation
manActionVideoDataset,2017. arXiv:1705.06950 Learning. In 2021 IEEE/CVF Conference on Com-
[cs]. puterVisionandPatternRecognition(CVPR),pages
[28] Anders Krogh and John Hertz. A simple weight de- 6960–6970,Nashville,TN,USA,2021.IEEE.
caycanimprovegeneralization. Advancesinneural [40] AlecRadford,JongWookKim,ChrisHallacy,Aditya
informationprocessingsystems,4,1991. Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas-
[29] HKuehne,HJhuang,EGarrote,TPoggio,andTSerre. try,AmandaAskell,PamelaMishkin,JackClark,etal.
HMDB:ALargeVideoDatabaseforHumanMotion Learningtransferablevisualmodelsfromnaturallan-
Recognition,2011. guagesupervision. InInternationalconferenceonma-
[30] BrianLester,RamiAl-Rfou,andNoahConstant. The
chinelearning,pages8748–8763.PMLR,2021.
powerofscaleforparameter-efficientprompttuning, [41] SepehrSameni,SimonJenni,andPaoloFavaro.Spatio-
2021. TemporalCropAggregationforVideoRepresentation
[31] Xiang Lisa Li and Percy Liang. Prefix-Tuning: Op- Learning,2022. arXiv:2211.17042[cs].
timizing Continuous Prompts for Generation, 2021. [42] Elham Shabaninia, Hossein Nezamabadi-pour, and
arXiv:2101.00190[cs]. FatemehShafizadegan. Transformersinactionrecog-
nition: Areviewontemporalmodeling. arXivpreprint
[32] XiaoLiu,KaixuanJi,YichengFu,WengTam,Zhengx-
arXiv:2302.01921,2022.
iaoDu,ZhilinYang,andJieTang. P-Tuning: Prompt
Tuning Can Be Comparable to Fine-tuning Across [43] KhurramSoomro,AmirRoshanZamir,andMubarak
ScalesandTasks. InProceedingsofthe60thAnnual Shah. UCF101: A Dataset of 101 Human Ac-
MeetingoftheAssociationforComputationalLinguis- tions Classes From Videos in The Wild, 2012.
tics(Volume2: ShortPapers), pages61–68, Dublin, arXiv:1212.0402[cs].
Ireland,2022.AssociationforComputationalLinguis- [44] Robin Strudel, Ricardo Garcia, Ivan Laptev, and
tics. Cordelia Schmid. Segmenter: Transformer for Se-
[33] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, manticSegmentation,2021.
ZhengZhang, StephenLin, andBainingGuo. Swin [45] GuangyuSun,MatiasMendieta,JunLuo,Shandong
transformer: Hierarchical vision transformer using Wu,andChenChen. Fedperfix: Towardspartialmodel
shifted windows. In Proceedings of the IEEE/CVF personalization of vision transformers in federated
11learning. In Proceedings of the IEEE/CVF Interna- [57] BowenZhang,ShuyangGu,BoZhang,JianminBao,
tional Conference on Computer Vision, pages 4988– Dong Chen, Fang Wen, Yong Wang, and Baining
4998,2023. Guo. StyleSwin: Transformer-BasedGANforHigh-
[46] ZhixingTan,XiangwenZhang,ShuoWang,andYang ResolutionImageGeneration,2022.
Liu. Msp: Multi-stage prompting for making pre- [58] Yi Zhu, Xinyu Li, Chunhui Liu, Mohammadreza
trainedlanguagemodelsbettertranslators. InProceed- Zolfaghari,YuanjunXiong,ChongruoWu,ZhiZhang,
ingsofthe60thAnnualMeetingoftheAssociationfor Joseph Tighe, R Manmatha, and Mu Li. A compre-
ComputationalLinguistics(Volume1: LongPapers), hensivestudyofdeepvideoactionrecognition. arXiv
pages6131–6142,2022. preprintarXiv:2012.06567,2020.
[47] ZhanTong,YibingSong,JueWang,andLiminWang.
VideoMAE:MaskedAutoencodersareData-Efficient
LearnersforSelf-SupervisedVideoPre-Training. In
AdvancesinNeuralInformationProcessingSystems,
2022. arXiv:2203.12602[cs].
[48] AshishVaswani,NoamShazeer,NikiParmar,Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention Is All You
Need,2017. arXiv:1706.03762[cs].
[49] VivekVeeriah,NaifanZhuang,andGuo-JunQi.Differ-
entialrecurrentneuralnetworksforactionrecognition.
InProceedingsoftheIEEEinternationalconference
oncomputervision,pages4041–4049,2015.
[50] LiminWang,YuQiao,andXiaoouTang. Actionrecog-
nition with trajectory-pooled deep-convolutional de-
scriptors. In 2015 IEEE Conference on Computer
VisionandPatternRecognition(CVPR),pages4305–
4314,2015.
[51] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao,
DahuaLin,XiaoouTang,andLucValGool. Temporal
segmentnetworks: Towardsgoodpracticesfordeep
actionrecognition. InECCV,2016.
[52] XiaolongWang,RossGirshick,AbhinavGupta,and
KaimingHe. Non-localneuralnetworks. InProceed-
ingsoftheIEEEconferenceoncomputervisionand
patternrecognition,pages7794–7803,2018.
[53] ZifengWang,ZizhaoZhang,SaynaEbrahimi,Ruoxi
Sun,HanZhang,Chen-YuLee,XiaoqiRen,Guolong
Su, Vincent Perot, Jennifer Dy, et al. Dualprompt:
Complementarypromptingforrehearsal-freecontinual
learning. InEuropeanConferenceonComputerVision,
pages631–648.Springer,2022.
[54] SainingXie,ChenSun,JonathanHuang,ZhuowenTu,
andKevinMurphy. Rethinkingspatiotemporalfeature
learning: Speed-accuracytrade-offsinvideoclassifi-
cation. InProceedingsoftheEuropeanconferenceon
computervision(ECCV),pages305–321,2018.
[55] TaojiannanYang,YiZhu,YushengXie,AstonZhang,
ChenChen,andMuLi. Aim: Adaptingimagemodels
forefficientvideoactionrecognition. arXivpreprint
arXiv:2302.03024,2023.
[56] BruceYu,JianlongChang,LingboLiu,QiTian,and
ChangWenChen. Towardsaunifiedviewonvisual
parameter-efficienttransferlearning,2023.
12