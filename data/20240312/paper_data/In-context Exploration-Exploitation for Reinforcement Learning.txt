PublishedasaconferencepaperatICLR2024
IN-CONTEXT EXPLORATION-EXPLOITATION FOR RE-
INFORCEMENT LEARNING
ZhenwenDai,FedericoTomasi,SinaGhiassian
SpotifyResearch
{zhenwend,federicot,sinag}@spotify.com
ABSTRACT
In-context learning is a promising approach for online policy learning of offline
reinforcement learning (RL) methods, which can be achieved at inference time
without gradient optimization. However, this method is hindered by significant
computational costs resulting from the gathering of large training trajectory sets
and the need to train large Transformer models. We address this challenge by
introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed
to optimize the efficiency of in-context policy learning. Unlike existing mod-
els,ICEEperformsanexploration-exploitationtrade-offatinferencetimewithin
a Transformer model, without the need for explicit Bayesian inference. Conse-
quently, ICEE can solve Bayesian optimization problems as efficiently as Gaus-
sian process biased methods do, but in significantly less time. Through experi-
ments in grid world environments, we demonstrate that ICEE can learn to solve
newRLtasksusingonlytensofepisodes,markingasubstantialimprovementover
thehundredsofepisodesneededbythepreviousin-contextlearningmethod.
1 INTRODUCTION
Transformersrepresentahighlyeffectiveapproachtosequencemodelling,withapplicationsextend-
ingacrossmultipledomainsliketext,image,andaudio. Inthefieldofreinforcementlearning(RL),
Chenetal.(2021)andJanneretal.(2021)havesuggestedtheconceptofaddressingofflineRLas
asequentialpredictionproblemusingTransformer. Thismethodhasprovensuccessfulintackling
a range of tasks using solely offline training when combined with large-scale sequence modelling
techniques(Leeetal.,2022;Reedetal.,2022a).Anotableshortcominglieswiththepolicy’sinabil-
itytoself-improvewhenemployedinonlineenvironments. Toovercomethis,fine-tuningmethods
havebeenintroduced,like(Zhengetal.,2022),whichenablecontinuedpolicyimprovement. How-
ever,thesemethodsoftenrelyonslow,computationallyexpensivegradient-basedoptimization.
Conversely, in-context learning, a characteristic observed in large language models (LLMs), can
handlenewtasksbyprovidingthetaskdetailsthroughlanguageprompts,effectivelyeliminatingthe
needforfine-tuning.Laskinetal.(2023)suggestanin-contextlearningalgorithmforRL,whichuses
asequencemodeltodistillapolicylearningalgorithmfromRLtrainingtrajectories. Theresulting
model is capable of conducting policy learning at inference-time through an iterative process of
action sampling and prompt augmentation. This method incurs significant computational costs in
collecting extensive training trajectory sets and training large Transformer models that are needed
tomodelasubstantialpartofatrainingtrajectory. Thekeyreasonforthishighcomputationalcost
is the lengthy RL training trajectories resulting from the slow trial-and-error process of RL policy
learningalgorithms.
This paper aims to improve the efficiency of in-context policy learning by eliminating the need
to learn from policy learning trajectories. In an ideal scenario, efficient policy learning could be
achieved through an efficient trial-and-error process. For simplified RL problems such as multi-
armedbandits(MAB),efficienttrial-and-errorprocesssuchasThompsonsamplinganduppercon-
fidence bounds are proven to exist. This process, often referred to as the exploration-exploitation
(EE)trade-off,reliesheavilyontheepistemicuncertaintyderivedfromBayesianbelief. However,it
isintractabletoinfertheexactepistemicuncertaintyofsequentialRLproblemsusingconventional
Bayesianmethods. InlightofrecentstudiesonuncertaintyestimationofLLMs(Yinetal.,2023),
1
4202
raM
11
]GL.sc[
1v62860.3042:viXraPublishedasaconferencepaperatICLR2024
weexaminepredictivedistributionsofsequencemodels,demonstratingthat,bytrainingwithpurely
supervisedlearningonofflinedata,asequencemodelcancaptureepistemicuncertaintyinsequence
prediction. ThissuggeststhepotentialforimplementingEEinofflineRL.
Basedonthisobservation,wedevelopanin-contextexploration-exploitation(ICEE)algorithmfor
policylearning. ICEEtakesasinputsasequenceofmultipleepisodesofthesametaskandpredicts
thecorrespondingactionateverystepconditionedonsomehindsightinformation. ThisofflineRL
design resembles the Decision Transformer (DT, Chen et al., 2021), but ICEE tackles in-context
policy learning by modeling multiple episodes of a task while DT only models a single episode.
Furthermore,theseepisodesdonotneedtooriginatefromatrainingtrajectory,therebyavertingthe
highcomputationalcostsassociatedwithgeneratingandconsuminglearningtrajectories.Theaction
distributionlearnedinDTisbiasedtowardsthedatacollectionpolicy,whichmaynotbeidealwhen
it is sub-optimal. To address this bias, we introduce an unbiased objective and develop a specific
formofhindsightinformationforefficientEEacrossepisodes.
With experiments, wedemonstrate that the EE behavioremerges inICEE duringinference thanks
to epistemic uncertainty in action prediction. This is particularly evident when applying ICEE to
Bayesianoptimization(BO),astheperformanceofICEEisonparwithaGaussianprocessbased
methodondiscreteBOtasks. WealsoillustratethatICEEcansuccessfullyimprovethepolicyfora
newtaskwithtrials-and-errorsfromscratchforsequentialRLproblems. Tothebestofourknowl-
edge,ICEEisthefirstmethodthatsuccessfullyincorporatesin-contextexploration-exploitationinto
RLthroughofflinesequentialmodelling.
2 RELATED WORK
Meta Learning. Interest in meta learning or learning to learn algorithms has recently increased.
Whilelearnerisanagentthatlearnstosolveataskusingobserveddata,alearningtolearnalgorithm
includesameta-learnerthatcontinuallyimprovesthelearningprocessofthelearner(Schmidhuber
etal.,1996;Thrun&Pratt,2012;Hospedalesetal.,2021;Sutton,2022).Lotsofworkhasbeendone
withinthespaceofmetalearning.Forexample,Finnetal.(2017)proposedageneralmodel-agnostic
meta-learningalgorithmthattrainsthemodel’sinitialparameterssuchthatthemodelhasmaximal
performance on a new task after the model parameters have been updated through a few gradient
steps computed with a small amount of data from the new task. Other works on meta learning
include improving optimizers (Andrychowicz et al., 2016; Li & Malik, 2016; Ravi & Larochelle,
2016; Wichrowska et al., 2017), improving few-shot learning (Mishra et al., 2017; Duan et al.,
2017),learningtoexplore(Stadieetal.,2018),andunsupervisedlearning(Hsuetal.,2018).
InthespaceofdeepmetaRL(Wangetal.,2016),afewworksfocusedonaspecialformofmeta-
learningcalledmeta-gradients. Inmeta-gradientsthemeta-learneristrainedbygradientsbymea-
suringtheeffectofmeta-parametersonalearnerthatitselfisalsotrainedusingagradientalgorithm
(Xu et al., 2018). In other work, Zheng et al. (2018) used meta-gradients to learn rewards. Gupta
etal.(2018)focusedonautomatingtheprocessoftaskdesigninRL,tofreetheexpertfromthebur-
denofmanualdesignofmeta-learningtasks. Similarly,Veeriahetal.(2019)presentedamethodfor
areinforcementlearningagenttodiscoverquestionsformulatedasgeneralvaluefunctionsthrough
theuseofnon-myopicmeta-gradients.Morerecently,metagradientreinforcementlearninghasseen
substantialbreakthroughsfromperformancegainsonpopularbenchmarkstohybridoffline-online
algorithmsformetaRL(Xuetal.,2020;Zahavyetal.,2020;Flennerhagetal.,2021;Mitchelletal.,
2021; Yin et al., 2023; Pong et al., 2022). The role of uncertainty in meta RL has been studied
by Zintgraf et al. (2021), which result in an efficient online meta RL method. This work is then
extendedbyDorfmanetal.(2021)totheoff-policysetting.
OfflineReinforcementLearning. Ingeneral,reinforcementlearningwasproposedasafundamen-
tallyonlineparadigm(Sutton,1988;Suttonetal.,1999;Sutton&Barto,2018).Thisonlinelearning
naturecomeswithsomelimitationssuchasmakingitdifficulttoadoptittomanyapplicationsfor
whichitisimpossibletogatheronlinedataandlearnatthesametime,suchasautonomousdriving
andisalsosometimesnotasdataefficientasitcouldbemsinceitmightchoosetolearnfromasam-
pleandthendisposethesampleandmoveontothenext(Levineetal.,2020). Oneideaforgetting
moreoutofthecollectedexperienceistousereplaybuffers.Whenusingbuffers,partofthesamples
arekeptinamemoryandthenarere-usedmultipletimessothattheagentcanlearnmorefromthem
(Lin,1992;Mnihetal.,2015). Avariantofreinforcementlearning,referredtoasofflineRLstudies
2PublishedasaconferencepaperatICLR2024
RLalgorithmsthatcanlearnfullyoffline,fromafixeddatasetofpreviouslygathereddatawithout
gatheringnewdataatthetimeoflearning(Ernstetal.,2005;Riedmiller,2005;Langeetal.,2012;
Fujimotoetal.,2019;Siegeletal.,2020;Gulcehreetal.,2020;Nairetal.,2020). Recentliterature
ondecisiontransformersalsofocusesonofflineRL(Chenetal.,2021)becauseitneedstocalculate
thereturntogoattrainingtime,whichinturnnecessitatespreviouslygathereddata.
In-context learning. In-context RL algorithms are the ones that improve their policy entirely in-
contextwithoutupdatingthenetworkparametersorwithoutanyfine-tuningofthemodel(Luetal.,
2021). Therehasbeensomeworkstudyingthephenomenonofin-contextlearningtryingtoexplain
howlearningin-contextmightbepossibleAbernethyetal.(2023);Minetal.(2022). The“Gato”
agentdevelopedbyReedetal.(2022b)worksasamulti-model,multi-task,multi-embodimentgen-
eralistagent,meaningthatthesametrainedagentcanplayAtari,captionimages,chat,stackblocks
witharealrobotarmonlybasedonitscontext. BytraininganRLagentatlargescale,Teametal.
(2023)showedthatanin-contextagentcanadapttonewandopenended3Denvironments. Ofspe-
cialinteresttousisAlgorithmDistillation(AD),whichisameta-RLmethod(Laskinetal.,2023).
Morespecifically,ADisanin-contextofflinemeta-RLmethod. Basically,ADisgradient-free—it
adaptstodownstreamtaskswithoutupdatingitsnetworkparameters.
3 EPISTEMIC UNCERTAINTY IN SEQUENCE MODEL PREDICTION
DT,alsoknownasUpside-downRL,treatstheofflinepolicylearningproblemasasequencemod-
ellingproblem. Inthissection,weconsideragenericsequencingmodelandanalyzeitspredictive
uncertainty.
LetX =(x ,...,x )beasequenceofinputsoflengthT andY =(y ,...,y )bethecor-
1:T 1 T 1:T 1 T
respondingsequenceofoutputs.Assumethattheoutputsequenceisgeneratedfollowingastep-wise
probabilisticdistributionparameterizedbyθ, y ∼ p(y |x ,X ,Y ,θ). Eachsequenceis
t t t 1:t 1 1:t 1
generatedwithadifferentparametersampledfromitspriordistrib−ution,θ−∼ p(θ). Thesedefinea
generativedistributionofasequence:
T
(cid:89)
p(Y ,θ|X )=p(θ)p(y |x ) p(y |x ,X ,Y ,θ). (1)
1:T 1:T 1 1 t t 1:t 1 1:t 1
− −
t=2
A sequence modeling task is often defined as training an autoregressive model parameterized by
ψ, p (y |x ,X ,Y ), given a dataset of sequences D = {X(i),Y(i)} generated from
ψ t t 1:t 1 1:t 1 i
theaboveunknown−genera−tivedistribution. Atthelimitofinfinitedata,theobjectiveofmaximum
likelihoodlearningoftheabovesequencemodelcanbeformulatedasψ∗=argmax L ,
ψ ψ
(cid:90)
(cid:88)
L =− p(Y |X )
ψ 1:t 1 1:t 1
− − (2)
t
D (p(y |x ,X ,Y )||p (y |x ,X ,Y ))dY +C,
KL t t 1:t 1 1:t 1 ψ t t 1:t 1 1:t 1 1:t 1
− − − − −
whereD (·||·)denotestheKullbackLeiblerdivergenceandC isaconstantwithrespecttoψ.
KL
The left hand side distribution in the cross entropy term p(y |x ,X ,Y ) is the true pre-
t t 1:t 1 1:t 1
dictivedistributionofy |x conditionedontheobservedhistoryY −andX− ,whichcanbe
t t 1:t 1 1:t 1
writtenas − −
(cid:90)
p(y |x ,X ,Y )= p(y |x ,X ,Y ,θ)p(θ|X ,Y )dθ, (3)
t t 1:t 1 1:t 1 t t 1:t 1 1:t 1 1:t 1 1:t 1
− − − − − −
where
p(θ)p(Y |X ,θ)
p(θ|X 1:t −1,Y 1:t −1)= (cid:82)
p(θ ′)p(Y
11 :t:t − 11
|X
11 :t:t − 11
,θ ′)dθ
′. (4)
− −
As shown above, the true predictive distribution of y |x contains both aleatoric uncertainty and
t t
epistemic uncertainty, in which the epistemic uncertainty is contributed by p(θ|X ,Y ).
1:t 1 1:t 1
With sufficient data and model capacity, the generative distribution in the seque−nce mo−del
p (y |x ,X ,Y ) will be trained to match the true predictive distribution. As a result,
ψ t t 1:t 1 1:t 1
we can expect−epistem−ic uncertainty to be contained in the predictive distribution of a sequence
model. Note that the predictive distribution can only capture the epistemic uncertainty with re-
specttotheparametersofsequencesθ,butdoesnotincludetheepistemicuncertaintyregardingthe
hyper-parameters(ifexist).
3PublishedasaconferencepaperatICLR2024
4 IN-CONTEXT POLICY LEARNING
Epistemic uncertainty is the essential ingredient of EE. With the observation that the predictive
distributionsequencemodelcontainsepistemicuncertainty,wedesignanin-contextpolicylearning
algorithmwithEE.
Consider the problem of solving a family of RL games based on offline data. From
each game, a set of trajectories are collected from a number of policies, where
τ(i) =(o(i),a(i),r(i),...,o(i) ,a(i) ,r(i) )isthetrajectoryofthek-thepisodeofthei-thgame
k k,1 k,1 k,1 k,Tk k,Tk k,Tk
ando,a,rdenotetheobservedstate,actionandrewardrespectively. Thepolicyusedtocollectτ(i)
k
is denoted as π(i)(a(i)|o(i)). We concatenate all the episodes of the i-th game into a single se-
k k,t k,t
quenceτ(i) =(τ(i),...,τ(i)). Forconvenience,thesuperscript(i) willbeomittedinthefollowing
1 K
textunlessthei-thgameisexplicitlyreferredto.
Weproposeasequencemodelthatistrainedtostep-wiselypredictp (a |R ,o ,H ),where
ψ k,t k,t k,t k,t
R isthereturn-to-goatthek-thepisodeandthettimestepandH = (τ ,τ )isthe
k,t k,t k,1:t 1 1:k 1
historyuntilthetimesteptincludingthepastepisodes1. Theabovemodelformul− ationis− similarto
DTbutasequenceinDTonlycontainsoneepisode. Notethat,differentfromAD,theconcatenated
trajectoriesdonotneedtobefromaRLlearningalgorithm.
Asshownintheprevioussection,bydoingmaximumlikelihoodlearningonthecollectedtrajecto-
ries, the predictive distribution will be trained to match the true posterior distribution of action of
thedatacollectionpolicy,
p(R |a ,o ,H )π (a |o )
p(a k,t|R k,t,o k,t,H k,t)= (cid:82) k,t k,t k,t k,t k k,t k,t , (5)
p(R |a ,o ,H )π (a |o )da
k,t ′k,t k,t k,t k ′k,t k,t ′k,t
wherep(R |a ,o ,H )isthedistributionofreturnafterthetimesteptfollowingπ .
k,t k,t k,t k,t k
As shown in (5), the posterior distribution of action is biased towards the data collection policy.
Followingsuchanactiondistributionallowsustoreproducethetrajectoriesgeneratedbythedata
collectionpolicybutwillleadtorecreationofsuboptimaltrajectoriesifthedatacollectionpolicyis
not optimal. A more desirable action distribution is the action distribution that corresponds to the
specifiedreturnwithouttheinfluenceofdatacollectionpolicy,i.e.
p(R |a ,o ,H )U(a )
pˆ(a k,t|R k,t,o k,t,H k,t)= (cid:82) k,t k,t k,t k,t k,t , (6)
p(R |a ,o ,H )U(a )da
k,t ′k,t k,t k,t ′k,t ′k,t
whereU(a )istheuniformrandompolicy,whichgivesalltheactionsequalprobabilities. Tolet
k,t
thesequencemodellearntheunbiasedactiondistribution,themaximumlikelihoodobjectiveneeds
tobedefinedas
(cid:90)
(cid:88)
L = pˆ(R ,a |o ,H )logp (a |R ,o ,H )dR da . (7)
ψ k,t k,t k,t k,t ψ k,t k,t k,t k,t k,t k,t
k,t
Afterapplyingtheimportancesamplingtrick,theMonteCarloapproximationoftheaboveobjective
canbederivedas
L
≈(cid:88) U(a k,t)
logp (a |R ,o ,H ), (8)
ψ π (a |o ) ψ k,t k,t k,t k,t
k k,t k,t
k,t
where a ∼ π (a |o ) and R ∼ p(R |a ,o ,H ), i.e., a and R are sampled
k,t k k,t k,t k,t k,t k,t k,t k,t k,t k,t
fromthedatacollectionpolicyπ .
k
5 DESIGN OF RETURN-TO-GO
Return-to-goisacrucialcomponentofDTforsolvingRLtasksatinferenceusingatrainedsequence
model. Its return-to-go scheme is designed to calculate the expected return signal from a single
episode. Inordertoachievein-contextpolicylearning,wedesignthereturn-to-goacrossepisodes.
1Althoughthelearnedactiondistributiondoesnotneedtoconditiononthereturn-to-goofthepastactions,
fortheconvenienceofacausalTransformerimplementation,H containsthereturn-to-goofthepastactions.
k,t
4PublishedasaconferencepaperatICLR2024
Algorithm1:In-contextExploration-Exploitation(ICEE)ActionInference
Input: AtrainedICEEmodelp
ψ
H ={};
1,1
foreachepisodek =1,...,K do
Resettheenvironment;
Selecttheepisodereturn-to-goc˜ ;
k
foreachstept=1,...,T do
k
Getanobservationo ;
k,t
Samplethestepreturn-to-goc ∼q(c );
k,t k,t
Sampleanactiona ∼p (a |R ,o ,H );
k,t ψ k,t k,t k,t k,t
Takethesampledactiona andcollecttherewardr ;
k,t k,t
ExpandthehistoryH =H ∪{R ,o ,a ,r };
k,t+1 k,t k,t k,t k,t k,t
end
Computethetruereturn-to-gobasedonthewholeepisode{Rˆ }Tk ;
k,t t=1
UpdatethehistoryH withthetruereturn-to-go{Rˆ }Tk ;
k,Tk k,t t=1
end
Thereturn-to-goofICEEconsistsoftwocomponents:onefortheindividualstepswithinanepisode
and the other for the cross-episode behavior, R = (c ,c˜ ). The in-episode return-to-go c
k,t k,t k k,t
followsthedesignusedin(Chenetal.,2021), whichisdefinedasthecumulativerewardsstarting
(cid:80)
fromthecurrentstepc
k,t
= t′>tr k,t′. Thisdesignborrowstheconceptofthecumulativereward
ofRLandhasthebenefitofencapsulatingtheinformationofthefuturerewardsfollowingthepolicy.
Thisisveryusefulwhentheoutcomesoffuturestepsstronglydependonthestateandactionofthe
current step. It allows the action that leads to a good future outcome to be differentiated from the
actionthatleadstoabadfutureoutcomeinthesequencemodel. Thedownsideisthatwithanon-
expert data collection policy, the optimal return-to-go at each state is often unobserved. This will
limitstheabilityofthesequencemodeltoachievebetterperformancethanthedatacollectionpolicy
atinferencetime.
In the cross-episode return-to-go design, the situation is different. The initial states of individual
episodes are independent of each other. What determines the cumulative rewards of individual
episodesarethesequenceofactions. Ifweconsiderthewholepolicyspaceastheactionspacefor
eachepisode,thecross-episodedecisionmakingisclosertoMAB,wherethepolicyistheactionand
thereturnofanepisodeistheMABreward. Drivenbythisobservation,wedefinethereturn-to-go
based on the improvement of the return of current episode compared to all the previous episodes.
Specifically,wedefinethecross-episodereturn-to-goas
(cid:26)
1 r¯ >max r¯ ,
c˜
k
= k 1 ≤j ≤k −1 j (9)
0 otherwise.
(cid:80)
wherer¯ = r isthecumulativerewardofthek-thepisode. Intuitively,atinferencetime,by
k t k,t
conditioningonc˜ =1,wetakeactionsfromapolicythatis“sampled”accordingtotheprobability
k
ofbeingbetterthanallthepreviousepisodes. Thisencouragesthesequencemodeltodeliverbetter
performanceaftercollectingmoreandmoreepisodes. Thisdesignavoidsthelimitationoftheneed
ofobservingtheoptimalpolicylearningtrajectories.
Action Inference. After training the sequence model, the model can be used to perform policy
learningfromscratch. Ateachstep,wesampleanactionfromthesequencemodelconditionedon
thetrajectorysofarandareturn-to-goforthestep.Thereturn-to-goforactionsamplingisdefinedas
follows. Thecross-episodereturn-to-goc˜ isalwayssettoonetoencouragepolicyimprovements.
k
Forthein-episodereturn-to-go,wefollowtheactioninferenceproposedbyLeeetal.(2022).During
training of ICEE, a separate sequence model is trained to predict the discretized return from the
trajectories, p (c |c˜ ,o ,H ). At inference time, an in-episode return-to-go for each step is
ϕ k,t k k,t k,t
sampledfromanaugmenteddistribution
c −c
q(c )∝p (c |c˜ ,o ,H )( k,t min )κ. (10)
k,t ϕ k,t k k,t k,t c −c
max min
Thisaugmentationbiasesthereturn-to-godistributiontowardshighervalues,whichencouragesthe
agenttotakeactionsthatleadstobetterreturns. Thereturnpredictionisnotcombinedintothemain
5PublishedasaconferencepaperatICLR2024
EI
ICEE
ICEE-biased 100
100 random
EI
ICEE
ICEE-biased
0 20 40 60 80 100 0.0 0.1 0.2 0.3 0.4 0.5
FunctionEvaluations ElapsedTime(Seconds)
(a) (b)
Figure1: DiscreteBOresultson2Dfunctionswith1024candidatelocations. 16benchmarkfunc-
tionsareusedwithfivetrialseach. They-axisshowstheaveragedistancebetweenthecurrentbest
estimate and the true minimum. The x-axis of (a) is the number of function evaluations and the
x-axisof(b)istheelapsedtime.
sequencemodelasin(Leeetal.,2022),becauseinthemainsequencemodelthereturnsarealsofed
intotheinputs. Inthisway,themodelquicklyfiguresoutc canbepredictedfromc . Thisis
k,t k,t 1
problematicbecauseatinferencetimethetruereturncannotbeobserveduntiltheendofa−nepisode.
After c is sampled, an action is sampled conditioned on the combined return-to-go R . The
k,t k,t
resultingstateandrewardareconcatenatedtothetrajectoryforthenextstepactionprediction. At
theendofanepisode,wewillrecalculatethetruec andc˜ basedontherewardsfromthewhole
k,t k
episode and update the return-to-go in the trajectory with the recalculated ones. This makes the
trajectoryatinferencetimebeasclosetothetrainingtrajectoriesaspossible. Thedescriptionofthe
actioninferencealgorithmcanbefoundinAlgorithm1.
6 BAYESIAN OPTIMIZATION EXPERIMENTS
Bayesian optimization (BO) is a very successful application of exploration-exploitation (EE). It is
abletosearchfortheoptimumofafunctionwiththeminimumnumberoffunctionevaluations. The
Gaussian process (GP) based BO methods have been widely used in various domains like hyper-
parameter tuning, drug discovery, aerodynamic optimization. To evaluate the EE performance of
ICEE,weapplyittoBOandcompareitwithaGP-basedapproachusingoneofmostwidelyused
acquisitionfunctions,expectedimprovement(EI).
WeconsideradiscreteBOproblem. Thetaskistofindthelocationfromafixedsetofpointsthat
hasthelowestfunctionvalueasfewnumberoffunctionevaluationsaspossible. BOcanbeviewed
asaspecialtypeofmulti-armedbandit(MAB),whereeachactionisassociatedwithalocationin
a bounded space. To solve BO with ICEE, we encode the iterative search trajectory of a function
as a single sequence, where a is the location of which a function value is collected at the step t,
t
r is the corresponding function value and R is the return-to-go. The observations {o } can be
t t t
usedtoencodeanysideinformationthatisknownaboutthefunctionandtheminimum. Asnosuch
information is available for generic BO, we do not use {o } in our experiment. As the function
t
valuecanseenastheavailableimmediaterewardfor,wetreateachactionasadifferentepisodeand
use only the episode return-to-go as the R here. As each action is associated with a location, we
t
embedactionsbylearningalinearprojectionbetweenthelocationspaceandtheembeddingspace.
WhendecodingaTransformeroutputforanaction,thelogitofanactionisgeneratedusingaMLP
thattakesasinputtheTransformeroutputtogetherwiththeembeddingassociatedwiththeaction.
Thedesignistotacklethechallengethatthesetoflocationsofeachfunctionmaybedifferent.
TotrainICEEtosolvethediscreteBOproblem, weneedtogeneratetrainingdatathatconsistsof
input-outputpairsofrandomlysampledfunctions. Duringtraining,theinput-outputpairsatrandom
locationsaregeneratedon-the-fly. WeuseaGPwiththeMate´rn5/2kerneltosample1024points
foreachfunction. Thelocationsofthesepointsaresampledfromauniformdistributionon[0,1].
Thelengthscalesofthekernelaresampledfromauniformdistributionon[0.05,0.3].
Ideally, for each sampled function, we can use a BO algorithm to solve the search problem and
take the sequence of actions as the training data. This is very computationally expensive due to
6
muminiMcnuFotpaG muminiMcnuFotpaGPublishedasaconferencepaperatICLR2024
the high computational cost of BO algorithms. Thanks to the EE property of ICEE, the training
datadonotneedtocomefromanexpertinferencealgorithm. Thisisverydifferentfrombehavior
cloning/algorithmdistillation.Weuseacheapdatacollectionalgorithmthatrandomlypicksactions
according to a probability distribution that is computed based on the function values of actions.
The probability distribution is defined as follows: The actions are sorted in an ascending order
according to their function values and the action at the i-th position gets the probability p(a =
a )∝exp(1024 iγ),γ issampledfromauniformdistributionbetween0and10foreachfunction.
i 102−3
Intuitively,thelocationswithlowerfunctionvaluesaremorelikelytobepicked.
AftertrainingICEE,weevaluateitsperformanceonasetof2Dbenchmarkfunctions. Weuse16
2Dfunctionsimplementedin(Kim&Choi,2017). Theinputspaceofeveryfunctionisnormalized
tobebetween0and1. Werandomlysampleadifferentsetof1024pointsfromeachfunctionand
normalizetheresultingfunctionvaluestobezero-meanandunit-variance. Eachfunctionwasgiven
five trials with different initial designs. At each step of search, we compute the difference of the
functionvaluebetweenthecurrentbestestimateandthetrueminimum. Theaverageperformance
of all the evaluated functions is shown in Fig. 1. The performance of ICEE is compared with the
GP-basedBOmethodusingtheEIacquisitionfunctionandtherandombaselinethatpickslocations
accordingtoauniformrandomprobability. “ICEE-biased”denotesavariantofICEEthatdoesnot
use the action bias correction objective as shown in (8). The search efficiency of ICEE is on par
with the GP-based BO method with EI. Both of them are significantly better than random and are
noticeably better than ICEE-biased. The performance gap between ICEE and ICEE-biased shows
thelossofefficiencyduetothebiasedlearnedactiondistribution.Beingabletoperformonparwith
a state-of-the-art BO method demonstrates that ICEE is able to perform state-of-the-art EE with
in-contextinference.
AclearadvantageofICEEisthatthewholesearchisdonethroughmodelinferencewithoutneed
ofanygradientoptimization. Incontrast,GP-basedBOmethodsneedtofitaGPsurrogatefunction
ateachstep,whichresultsintoasignificantspeeddifference. Fig.1bshowsthesamesearchresults
withthex-axisbeingtheelapsedtime. AllthemethodsrunonasingleA100GPU.Thankstothe
in-contextinference,ICEEismagnitudefasterthanconventionalBOmethods. Moredetailsofthe
BOexperimentscanbefoundinAppendixD.
7 RL EXPERIMENTS
We investigate the in-context policy learning capability of ICEE on sequential RL problems. To
demonstrate the capability of in-context learning, we focus on the families of environments that
cannotbesolvedthroughzero-shotgeneralizationofapre-trainedmodel,soin-contextpolicylearn-
ingisnecessaryforsolvingthetasks. Thisimpliesthatsomeinformationthatareimportanttothe
successofataskismissingfromthestaterepresentationandwillneedtobediscoveredbytheagent.
Weusethetwogridworldenvironmentsin(Leeetal.,2022): darkroomanddarkkey-to-door. The
detailsofthemareasfollows.
DarkRoom.Theexperimenttakesplaceina2DdiscretePOMDP,whereanagentisplacedinsidea
roomtolocateagoalspot.Theagenthasaccesstoitsownlocationcoordinates(x,y),butisunaware
ofthegoal’splacementrequiringittodeduceitfromtherewardsreceived. Theroom’sdimensions
are 9x9 with the possible actions by the agent including moving one step either left, right, up or
down, or staying idle, all within an episode length of 20. Upon completion, the agent gets placed
backatthemid-pointofthemap. Twoenvironmentvariantsareconsideredforthisexperiment:The
DarkRoomcasewheretheagentobtainsareward(r=1)eachtimethegoalisachieved,andtheDark
RoomHardcasewheretherewardsaresparse(r=1onlyonceforattainingthegoal). Wheneverthe
rewardvalueisnot1,itwillbeconsideredas0. Differentfrom(Leeetal.,2022),wekeeptheroom
sizeofthehardcasetobe9x9.
DarkKey-to-Door. ThissettingissimilartotheDarkRoom,butwithaddedchallengingfeatures.
The task of the agent is to locate an invisible key to receive a one-time reward of r=1, and subse-
quently, identify an invisible door to gain another one-time reward of r=1. Otherwise, the reward
remainsatr=0. Theagent’sinitiallocationineachepisoderesetsrandomly. Theroomsizeisstill9
x9buttheepisodelengthincreasesto50steps.
7PublishedasaconferencepaperatICLR2024
Tocollectdataforofflinetraining,wesampleasetofnewgamesforeachmini-batch. WecollectK
episodesfromeachgame. ThankstotheEEcapabilityofICEE,thetrainingdatadonotneedtobe
fromarealRLlearningalgorithmlikeDeepQ-Network(DQN),whichisexpensivetorun. Instead,
weletacheapdatacollectionpolicyactforK episodesindependentlyandconcatenatetheresulting
episodesintoasinglesequence. Weuseanϵ-greedyversionofthe“cheating”optimalpolicy. The
policy knows the goal location that is unknown to the agent and will move straightly towards the
goalwith1−ϵprobabilityandwithϵprobabilityitwilltakeanactionthatdoesnotmaketheagent
closer to the goal. For each episode, ϵ is sampled from a uniform distribution between 0 and 1.
Intuitively, thispolicyhassomechancetosolveagameefficientlywhenϵissmallbutonaverage
itdoesnotdeliveragoodperformance. ForDarkRoomexperiments,eachsequenceconsistsof50
episodesandforDarkKey-To-Door,itconsistsof20episodes.
DarkRoom(Biased). TodemonstratethebenefitsofEEforsequentialRLproblemwhenthedata
collectionpolicycannotbeoptimal,wecreateavariantofthedarkroomenvironment. Ateachstep,
thedatacollectionpolicytakesthe“left”actionwiththe2/3probabilityandwiththe1/3probability
actsasdescribedabove.Attrainingtime,thegoalcanbeatanywhereintheroomand,atevaluation
time,thegoalwillonlyappearontherighthandsidewherex>5.
For sequential RL problems, ICEE consists of two sequence models: one for action prediction
and the other for the in-episode return-to-go prediction. The return-to-go sequence model takes
asinputsthesequenceofstate,action,rewardtripletsandpredictsthein-episodereturn-to-go. The
actionpredictionmodeltakesasinputsthesequenceofthetripletsandthetworeturn-to-goR and
k,t
predictstheactionsequence. Thetwomodelsaretrainedtogetherwiththesamegradientoptimizer.
To encourage ICEE to solve the games quickly, when calculating the in-episode return-to-go, a
negativereward,−1/T,isgiventoeachstepthatdoesnotreceiveareward,whereT istheepisode
length. Bothc˜ andc arediscreteandtokenized.
k k,t
7.1 BASELINEMETHODS
Source.Weusethedatacollectionpolicyasabaselineforcomparison.Asthedatacollectionpolicy
solveseachepisodeindependently,wecalculatetheaveragereturnacrossmultipleepisodes.
Algorithm Distillation (AD, Laskin et al., 2023). The in-context learning algorithm that distills
aRLalgorithmsfromRLtrainingtrajectories. ADpredictsactionbasedonlyonthecurrentstates
andthehistoryofstate,actionandrewardtriplets. WereplicatetheimplementationofADusingthe
TransformerarchitectureasICEE.WeapplyADtothesametrainingdataasICEEuses(ignoring
thereturn-to-gosignals),despitetheyaregeneratedfromRLlearningtrajectories.
AD-sorted. ADisdesignedtobetrainedonRLlearningtrajectories. AnimportantpropertyofRL
learningtrajectoriesisthattheperformanceoftheagentgraduallyincreasesthroughouttraining. To
mimicsuchtrajectoriesusingourdata,wesorttheepisodesinasequenceaccordingtothesampled
ϵofthedatacollectionpolicyinthedescentorder. ϵdetermineshowclosethedatacollectionpolicy
is to the optimal policy. In this order, the episodes in a latter position of sequence tend to have a
higherreturn. WetrainADusingthissortedsequencesinsteadoftheoriginalones.
Multi-gameDecisionTransformer(MGDT,Leeetal.,2022).MGDTisnotanin-contextlearning
algorithm. WetrainMGDTusingonlyoneepisodefromeachsampledgame. Theperformanceof
MGDTshowsthatwhattheperformanceoftheagentiswhenthereisnoin-contextpolicylearning.
7.2 EVALUATION&RESULTS
Aftertraining,ICEEwillbeevaluatedonsolvingasetofsampledgames. Theinferencealgorithm
isdescribedinAlg.1. NoonlinemodelupdateisperformedbyICEEandallthebaselinemethods
atevaluationtime. Foreachsampledgame,ICEEandtwovariantsofADwillactforK episodes
consecutively. In each episode, the trajectories from the past episodes are used as in the history
representation. Ideally, a good performing agent identifies the missing information with as few
numberofepisodesaspossibleandthenmaximizesthereturninthefollowingepisodes. Foreach
problem,wesample100gamesandK is50forDarkRoomand20forKey-To-Door.
The experiment results are shown in Fig. 2. ICEE is able to solve the sampled games efficiently
comparedtothebaselinemethods. TheEEcapabilityallowsICEEtosearchforthemissinginfor-
8PublishedasaconferencepaperatICLR2024
DarkRoom DarkRoom(Hard) DarkKey-To-Door
15.0 2.0
0.8
12.5
1.5
10.0 0.6
7.5 1.0
ICEE 0.4 ICEE ICEE
5.0 AD AD AD
AD-sorted 0.2 AD-sorted 0.5 AD-sorted
2.5 source source source
0.0 MGDT 0.0 MGDT 0.0 MGDT
0 10 20 30 40 50 0 10 20 30 40 50 5 10 15 20
EnvEpisodes EnvEpisodes EnvEpisodes
(a) (b) (c)
DarkRoom(Biased)
AverageActionEntropy
5 1.6
DarkRoom
4 1.4 D Kea yrk -TR o-o Do om or(Hard)
3 1.2
1.0
2
1 ICEE 0.8
ICEE-biased
0.6
0 AD-sorted
0 10 20 30 40 50 0 10 20 30 40 50
EnvEpisodes EnvEpisodes
(d) (e)
Figure2:Experimentalresultsofin-contextpolicylearningongridworldRLproblems.Anagentis
expectedtosolveagamebyinteractingwiththeenvironmentforK episodeswithoutonlinemodel
updates. They-axisof(a-d)showstheaveragereturnsover100sampledgamesaftereachepisode.
The y-axis of (e) shows the entropy of the action distribution. The x-axis shows the number of
episodesthatanagentexperiences.
mationefficientlyandthenactswithconfidenceoncethemissinginformationisfound.Anindicator
of such behavior is the continuous decrease of the action entropies as the agent experiences more
episodes(seeFig.2e).
Asexpected,theoriginalADlearnstomimicthedatacollectionpolicy,whichresultsinanaverage
performance slightly below the data collection policy. MGDT fails to solve most of the games
duetothemissinginformation. Interestingly,despitethatthetrainingdataisnotgeneratedfroma
RL learning algorithm, AD-sorted is able to clone the behavior of the data collection policy with
differentϵatdifferentstages,whichallowsittosolvethegamesattheendofthesequence.
ICEE-biasedisnotshowninFig.2a,Fig.2bandFig.2casitachievessimilarperformanceasICEE
does. Thereasonisthatthereisnoclearbiasintheactiondistributionofthedatacollectionpolicy.
However,asshowninFig.2d,fortheDarkRoom(Biased)environment,ICEEclearlyoutperforms
ICEE-biased,asitcanovercomethebiasinthedatacollectionpolicyandkeepsufficientuncertainty
inactiondistributiontoexploretherighthandsideoftheroom. AD-sortedfailsthetaskbecauseit
clonesthedatacollectionpolicy,whichisunlikelytosolvethetasksduetotheactionbias.
8 CONCLUSION
In this paper, we analyze the predictive distribution of sequence models and show that the predic-
tivedistributioncancontainepistemicuncertainty,whichinspiresthecreationofanEEalgorithm.
Wepresentanin-contextEEalgorithmbyextendingtheDTformulationtoin-contextpolicylearn-
ing and deriving an unbiased training objective. Through the experiments on BO and discrete RL
problems,wedemonstratethat: (i)ICEEcanperformEEinin-contextlearningwithouttheneedof
explicitBayesianinference;(ii)TheperformanceofICEEisonparwithstate-of-the-artBOmeth-
ods without the need of gradient optimization, which leads to significant speed-up; (iii) New RL
taskscanbesolvedwithintensofepisodes.
9
nruteR
nruteR
nruteR
yportnE
nruteRPublishedasaconferencepaperatICLR2024
REFERENCES
JacobAbernethy,AlekhAgarwal,TeodorVMarinov,andManfredKWarmuth. Amechanismfor
sample-efficientin-contextlearningforsparseretrievaltasks. arXivpreprintarXiv:2305.17040,
2023.
MarcinAndrychowicz,MishaDenil,SergioGomez,MatthewWHoffman,DavidPfau,TomSchaul,
BrendanShillingford, andNandoDeFreitas. Learningtolearnbygradientdescentbygradient
descent. Advancesinneuralinformationprocessingsystems,29,2016.
Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, An-
drew Gordon Wilson, and Eytan Bakshy. BoTorch: A Framework for Efficient Monte-Carlo
BayesianOptimization. InAdvancesinNeuralInformationProcessingSystems33,2020.
LiliChen,KevinLu,AravindRajeswaran,KiminLee,AdityaGrover,MishaLaskin,PieterAbbeel,
AravindSrinivas,andIgorMordatch.Decisiontransformer:Reinforcementlearningviasequence
modeling. InAdvancesinNeuralInformationProcessingSystems,pp.15084–15097,2021.
RonDorfman,IdanShenfeld,andAvivTamar. Offlinemetareinforcementlearning–identifiability
challengesandeffectivedatacollectionstrategies. InAdvancesinNeuralInformationProcessing
Systems,pp.4607–4618,2021.
Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya
Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. Advances in
neuralinformationprocessingsystems,30,2017.
DamienErnst,PierreGeurts,andLouisWehenkel. Tree-basedbatchmodereinforcementlearning.
JournalofMachineLearningResearch,6,2005.
ChelseaFinn,PieterAbbeel,andSergeyLevine. Model-agnosticmeta-learningforfastadaptation
ofdeepnetworks.InInternationalconferenceonmachinelearning,pp.1126–1135.PMLR,2017.
SebastianFlennerhag,YannickSchroecker,TomZahavy,HadovanHasselt,DavidSilver,andSatin-
derSingh. Bootstrappedmeta-learning. arXivpreprintarXiv:2109.04504,2021.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. InInternationalconferenceonmachinelearning,pp.2052–2062.PMLR,2019.
Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Thomas Paine, Sergio Go´mez, Konrad Zolna,
RishabhAgarwal,JoshSMerel,DanielJMankowitz,CosminPaduraru,etal. Rlunplugged: A
suiteofbenchmarksforofflinereinforcementlearning. AdvancesinNeuralInformationProcess-
ingSystems,33:7248–7259,2020.
Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, and Sergey Levine. Unsupervised meta-
learningforreinforcementlearning. arXivpreprintarXiv:1806.04640,2018.
TimothyHospedales,AntreasAntoniou,PaulMicaelli,andAmosStorkey. Meta-learninginneural
networks: A survey. IEEE transactions on pattern analysis and machine intelligence, 44(9):
5149–5169,2021.
Kyle Hsu, Sergey Levine, and Chelsea Finn. Unsupervised learning via meta-learning. arXiv
preprintarXiv:1810.02334,2018.
MichaelJanner,QiyangLi,andSergeyLevine. Offlinereinforcementlearningasonebigsequence
modelingproblem.InAdvancesinNeuralInformationProcessingSystems,pp.1273–1286,2021.
JungtaekKimandSeungjinChoi.BayesO:ABayesianoptimizationframeworkinPython.https:
//bayeso.org,2017.
SaschaLange,ThomasGabel,andMartinRiedmiller. Batchreinforcementlearning. InReinforce-
mentlearning: State-of-the-art,pp.45–73.Springer,2012.
10PublishedasaconferencepaperatICLR2024
MichaelLaskin,LuyuWang,JunhyukOh,EmilioParisotto,StephenSpencer,RichieSteigerwald,
DJ Strouse, Steven Stenberg Hansen, Angelos Filos, Ethan Brooks, maxime gazeau, Himanshu
Sahni, Satinder Singh, and Volodymyr Mnih. In-context reinforcement learning with algorithm
distillation. InTheEleventhInternationalConferenceonLearningRepresentations,2023.
Kuang-HueiLee,OfirNachum,Mengjiao(Sherry)Yang,LisaLee,DanielFreeman,SergioGuadar-
rama,IanFischer,WinnieXu,EricJang,HenrykMichalewski,andIgorMordatch. Multi-game
decisiontransformers.InAdvancesinNeuralInformationProcessingSystems,pp.27921–27936,
2022.
SergeyLevine,AviralKumar,GeorgeTucker,andJustinFu. Offlinereinforcementlearning: Tuto-
rial,review,andperspectivesonopenproblems. arXivpreprintarXiv:2005.01643,2020.
KeLiandJitendraMalik. Learningtooptimize. arXivpreprintarXiv:1606.01885,2016.
Long-JiLin.Self-improvingreactiveagentsbasedonreinforcementlearning,planningandteaching.
Machinelearning,8:293–321,1992.
Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal
computationengines. arXivpreprintarXiv:2103.05247,1,2021.
SewonMin,XinxiLyu,AriHoltzman,MikelArtetxe,MikeLewis,HannanehHajishirzi,andLuke
Zettlemoyer.Rethinkingtheroleofdemonstrations:Whatmakesin-contextlearningwork? arXiv
preprintarXiv:2202.12837,2022.
NikhilMishra,MostafaRohaninejad,XiChen,andPieterAbbeel. Asimpleneuralattentivemeta-
learner. arXivpreprintarXiv:1707.03141,2017.
Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn. Offline meta-
reinforcement learning with advantage weighting. In International Conference on Machine
Learning,pp.7780–7791.PMLR,2021.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare,AlexGraves,MartinRiedmiller,AndreasKFidjeland,GeorgOstrovski,etal.Human-level
controlthroughdeepreinforcementlearning. nature,518(7540):529–533,2015.
AshvinNair,AbhishekGupta,MurtazaDalal,andSergeyLevine. Awac: Acceleratingonlinerein-
forcementlearningwithofflinedatasets. arXivpreprintarXiv:2006.09359,2020.
VitchyrHPong,AshvinVNair,LauraMSmith,CatherineHuang,andSergeyLevine.Offlinemeta-
reinforcement learning with online self-supervision. In International Conference on Machine
Learning,pp.17811–17829.PMLR,2022.
SachinRaviandHugoLarochelle. Optimizationasamodelforfew-shotlearning. InInternational
conferenceonlearningrepresentations,2016.
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,
Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom
Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,
OriolVinyals,MahyarBordbar,andNandodeFreitas. Ageneralistagent. May2022a.
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,
Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al.
Ageneralistagent. arXivpreprintarXiv:2205.06175,2022b.
MartinRiedmiller. Neuralfittedqiteration–firstexperienceswithadataefficientneuralreinforce-
mentlearningmethod. InMachineLearning: ECML2005: 16thEuropeanConferenceonMa-
chine Learning, Porto, Portugal, October 3-7, 2005. Proceedings 16, pp. 317–328. Springer,
2005.
JuergenSchmidhuber,JieyuZhao,andMAWiering. Simpleprinciplesofmetalearning. Technical
reportIDSIA,69:1–23,1996.
11PublishedasaconferencepaperatICLR2024
Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Ne-
unert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing
what worked: Behavioral modelling priors for offline reinforcement learning. arXiv preprint
arXiv:2002.08396,2020.
BradlyCStadie,GeYang,ReinHouthooft,XiChen,YanDuan,YuhuaiWu,PieterAbbeel,andIlya
Sutskever. Some considerations on learning to explore via meta-reinforcement learning. arXiv
preprintarXiv:1803.01118,2018.
RichardSSutton. Learningtopredictbythemethodsoftemporaldifferences. Machinelearning,
3:9–44,1988.
RichardSSutton. Ahistoryofmeta-gradient: Gradientmethodsformeta-learning. arXivpreprint
arXiv:2202.09701,2022.
RichardSSuttonandAndrewGBarto. Reinforcementlearning: Anintroduction. MITpress,2018.
Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient meth-
ods for reinforcement learning with function approximation. Advances in neural information
processingsystems,12,1999.
Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal Behbahani, Avishkar
Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, et al.
Human-timescale adaptation in an open-ended task space. arXiv preprint arXiv:2301.07608,
2023.
SebastianThrunandLorienPratt. Learningtolearn. SpringerScience&BusinessMedia,2012.
Vivek Veeriah, Matteo Hessel, Zhongwen Xu, Janarthanan Rajendran, Richard L Lewis, Junhyuk
Oh, Hado P van Hasselt, David Silver, and Satinder Singh. Discovery of useful questions as
auxiliarytasks. AdvancesinNeuralInformationProcessingSystems,32,2019.
Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,
Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn.
arXivpreprintarXiv:1611.05763,2016.
Olga Wichrowska, Niru Maheswaranathan, Matthew W Hoffman, Sergio Gomez Colmenarejo,
MishaDenil,NandoFreitas,andJaschaSohl-Dickstein. Learnedoptimizersthatscaleandgen-
eralize. InInternationalconferenceonmachinelearning,pp.3751–3760.PMLR,2017.
ZhongwenXu, HadoPvanHasselt, andDavidSilver. Meta-gradientreinforcementlearning. Ad-
vancesinneuralinformationprocessingsystems,31,2018.
ZhongwenXu,HadoPvanHasselt,MatteoHessel,JunhyukOh,SatinderSingh,andDavidSilver.
Meta-gradient reinforcement learning with an objective discovered online. Advances in Neural
InformationProcessingSystems,33:15254–15264,2020.
ZhangyueYin,QiushiSun,QipengGuo,JiawenWu,XipengQiu,andXuanjingHuang. Dolarge
languagemodelsknowwhattheydon’tknow? InFindingsoftheAssociationforComputational
Linguistics: ACL2023,pp.8653–8665,July2023.
TomZahavy,ZhongwenXu,VivekVeeriah,MatteoHessel,JunhyukOh,HadoPvanHasselt,David
Silver,andSatinderSingh. Aself-tuningactor-criticalgorithm. Advancesinneuralinformation
processingsystems,33:20913–20924,2020.
Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. In Proceedings
of the 39th International Conference on Machine Learning, Proceedings of Machine Learning
Research,pp.27042–27059,2022.
Zeyu Zheng, Junhyuk Oh, and Satinder Singh. On learning intrinsic rewards for policy gradient
methods. AdvancesinNeuralInformationProcessingSystems,31,2018.
Luisa Zintgraf, Sebastian Schulze, Cong Lu, Leo Feng, Maximilian Igl, Kyriacos Shiarlis, Yarin
Gal, Katja Hofmann, and Shimon Whiteson. Varibad: Variational bayes-adaptive deep rl via
meta-learning. JournalofMachineLearningResearch,22(289):1–39,2021.
12PublishedasaconferencepaperatICLR2024
Appendix
A DERIVATION OF THE OBJECTIVE OF A SEQUENCE MODEL
Themaximumlikelihoodobjectiveofasequencemodelin(2)canbederivedinthefollowingsteps.
(cid:90)
L = p(Y ,θ|X )logp (Y |X )dY dθ
ψ 1:T 1:T ψ 1:T 1:T 1:T
(cid:90) T
(cid:89)
= p(Y ,θ|X )log p (y |x ,X ,Y )dY dθ
1:T 1:T ψ t t 1:t 1 1:t 1 1:T
− −
t=1
T (cid:90)
(cid:88)
= p(Y ,θ|X )logp (y |x ,X ,Y )dY dθ
1:T 1:T ψ t t 1:t 1 1:t 1 1:T
− −
t=1
T (cid:90)
(cid:88)
= p(Y |X )logp (y |x ,X ,Y )dY
1:t 1:t ψ t t 1:t 1 1:t 1 1:t
− −
t=1
T (cid:90)
(cid:88)
= p(Y |X )p(y |x ,X ,Y )logp (y |x ,X ,Y )dY
1:t 1 1:t 1 t t 1:t 1 1:t 1 ψ t t 1:t 1 1:t 1 1:t
− − − − − −
t=1
(cid:88)T (cid:90) (cid:16)
= p(Y |X ) p(y |x ,X ,Y )logp (y |x ,X ,Y )
1:t 1 1:t 1 t t 1:t 1 1:t 1 ψ t t 1:t 1 1:t 1
− − − − − −
t=1
(cid:17)
−p(y |x ,X ,Y )logp(y |x ,X ,Y ) dY
t t 1:t 1 1:t 1 t t 1:t 1 1:t 1 1:t
− − − −
(cid:90)
+ p(Y |X )p(y |x ,X ,Y )logp(y |x ,X ,Y )dY
1:t 1 1:t 1 t t 1:t 1 1:t 1 t t 1:t 1 1:t 1 1:t
− − − − − −
(cid:88)T (cid:90) (cid:16) (cid:17)
= p(Y |X ) −D (p(y |x ,X ,Y )||p (y |x ,X ,Y )) dY
1:t 1 1:t 1 KL t t 1:t 1 1:t 1 ψ t t 1:t 1 1:t 1 1:t 1
− − − − − − −
t=1
(cid:90) (cid:16) (cid:17)
+ p(Y |X )H p(y |x ,X ,Y ) dY
1:t 1 1:t 1 t t 1:t 1 1:t 1 1:t 1
− − − − −
(cid:90)
(cid:88)
=− p(Y |X )D (p(y |x ,X ,Y )||p (y |x ,X ,Y ))dY +C,
1:t 1 1:t 1 KL t t 1:t 1 1:t 1 ψ t t 1:t 1 1:t 1 1:t 1
− − − − − − −
t
B DERIVATION OF THE UNBIASED OBJECTIVE
The Decision Transformer’s approach to offline RL includes the training of an action distribution
conditioned the return, thereby allowing for the sampling of an action at the time of inference by
providingtheprojectedreturn(return-to-go). Giventhatthereturnistheresultofpresentanddown-
streamactions, thedistributionof actionthat amodeltries tolearn canbereframed asan action’s
posterior distribution, as is presented in equation (5). Note that equation (5) outlines the data dis-
tribution,whichshouldbedistinguishedfromtheneuralnetworkmodelp (a |R ,o ,H ).
ψ k,t k,t k,t k,t
As noted in equation (5), the action’s distribution is proportionate to the return’s distribution, and
this is subsequently weighted by the probability of action from the data collection policy. As it
is the posterior distribution derived by the Bayes rule, we denote it as the “true” action posterior
distribution.
Toputthisintuitively,ifamodelcanaccuratelymatchequation(5),itwillresultintheactiondis-
tributionbeingskewedtowardsthedatacollectionpolicy. Forinstance,inapre-recordedsequence,
shouldanactionberandomlyselectedwithextremelylowprobabilityfromthedatacollectionpol-
icy, yet yield a high return, the subsequent action distribution in equation (5) would attribute a
minute probability to the relevant action, given the high return. Despite the observation of a high
returnfollowingtheaction,whichwouldsuggestahighprobabilityofp(R |a ,o ,H ),the
k,t k,t k,t k,t
resultingprobabilityofactionisweightedbytheprobabilityofactioninthedatacollectionpolicy,
13PublishedasaconferencepaperatICLR2024
π (a |o ),resultinginasmallvalue. Therefore,eventhough(5)isthetrueposteriordistribution
k k,t k,t
ofaction,itisnotthedesirableactiondistributionforourmodel.
Ideally,theactiondistribution,asdemonstratedinequation(6),shouldbeproportionalonlytothe
return’sdistributionandwouldbeunaffectedbythedatacollectionpolicy. Withsuchadistribution,
theundesireddevaluationduetothedatacollectionpolicywouldbeeliminated, therebyresolving
thementionedissue.
As explained above, we would like to learn an action distribution in (6) instead of the action dis-
tribution in (5). However, since (5) is the true action distribution of data, the common maximum
likelihoodtrainingobjectivewillletthemodelmatchtheactiondistributionin(5).
Theunbiasedobjectiveoflearningtheactiondistributionin(8)canbederivedinthefollowingsteps.
To let the model learn the action distribution in (6) instead, we first state the desirable training
objectiveasifthedatafollowthedistribution(6):
(cid:90)
(cid:88)
L = pˆ(R ,a |o ,H )logp (a |R ,o ,H )dR da
ψ k,t k,t k,t k,t ψ k,t k,t k,t k,t k,t k,t
k,t
(cid:88)(cid:90) (cid:16)(cid:90) (cid:17)
= pˆ(R |o ,H ) pˆ(a |R ,o ,H )logp (a |R ,o ,H )da dR
k,t k,t k,t k,t k,t k,t k,t ψ k,t k,t k,t k,t k,t k,t
k,t
Then,weapplytheimportancesamplingtricktointroducethetrueactionposteriorintheequation:
L
=(cid:88)(cid:90)
pˆ(R |o ,H
)(cid:16)(cid:90)
p(a |R ,o ,H
)pˆ(a k,t|R k,t,o k,t,H k,t)
ψ k,t k,t k,t k,t k,t k,t k,t p(a |R ,o ,H )
k,t k,t k,t k,t
k,t
(cid:17)
logp (a |R ,o ,H )da dR
ψ k,t k,t k,t k,t k,t k,t
Aftersomerearrangementoftheequation,wegetamuchclearerformulationoftheobjective:
L ψ =(cid:88)
k,t
(cid:90) pˆ(R k,t|o k,t,H k,t)(cid:16)(cid:90) p(a k (cid:17),t|R k,t,o k,t,H k,t) p(Rp k(R ,t|k a,t kp pˆ| ,a ( (tR R,k o, k kt k, ,, ,t to t| |,k o oH, k kt , ,, kt tH ,, ,tH H)k π, k kt k, ,) t t(U ) )a( ka ,tk |, ot)
k,t)
logp (a |R ,o ,H )da dR
ψ k,t k,t k,t k,t k,t k,t
=(cid:88)(cid:90)
pˆ(R |o ,H
)(cid:16)(cid:90)
p(a |R ,o ,H )
U(a k,t)p(R k,t|o k,t,H k,t)
k,t k,t k,t k,t k,t k,t k,t π (a |o )pˆ(R |o ,H )
k k,t k,t k,t k,t k,t
k,t
(cid:17)
logp (a |R ,o ,H )da dR
ψ k,t k,t k,t k,t k,t k,t
=(cid:88)(cid:90)
p(R |o ,H
)(cid:16)(cid:90)
p(a |R ,o ,H )
U(a k,t)
k,t k,t k,t k,t k,t k,t k,t π (a |o )
k k,t k,t
k,t
(cid:17)
logp (a |R ,o ,H )da dR
ψ k,t k,t k,t k,t k,t k,t
=(cid:88)(cid:90)
p(R ,a |o ,H )
U(a k,t)
logp (a |R ,o ,H )da dR
k,t k,t k,t k,t π (a |o ) ψ k,t k,t k,t k,t k,t k,t
k k,t k,t
k,t
Notetheprobabilitydistributioninfrontisnowthejointdistributionofreturnandactioninthedata
distribution. WeapplytheMonteCarloapproximationtotheintegralbyconsideringtherecorded
dataaresamplesfromthedatadistribution. Wegettheproposedtrainingobjective.
L
≈(cid:88) U(a k,t)
logp (a |R ,o ,H ), R ,a ∼p(R ,a |o ,H )
ψ π (a |o ) ψ k,t k,t k,t k,t k,t k,t k,t k,t k,t k,t
k k,t k,t
k,t
14PublishedasaconferencepaperatICLR2024
C IMPLEMENTATION DETAILS
ICEEisimplementedbasedonnanoGPT2.FortheRLexperiments,ICEEcontains12layerswith
128 dimensional embeddings. There are 4 heads in the multi-head attention. We use the Adam
optimizerwiththelearningrate10 5.
−
D BAYESIAN OPTIMIZATION EXPERIMENTS
WeconsideradiscreteBOproblem. Thetaskistofindthelocationfromafixedsetofpointsthat
hasthelowestfunctionvalueasfewnumberoffunctionevaluationsaspossible. Atthebeginning
ofsearch,thefunctionvaluesofsomerandomlypickedlocationsaregiven. TheBOalgorithmwill
beaskedtosuggestalocationwherethefunctionminimumliesandthen,thefunctionvaluewillbe
collectedfromthesuggestedlocation. Thesuggestion-evaluationloopwillberepeatedwithafixed
number of times. The performance of a BO algorithm is evaluated by how quickly it can find the
functionminimum.
The list of 2D functions used for evaluations are: Branin, Beale, Bohachevsky, Bukin6, DeJong5,
DropWave, Eggholder, GoldsteinPrice, HolderTable, Kim1, Kim2, Kim3, Michalewicz, Shubert,
SixHumpCamel,ThreeHumpCamel.
EI
ICEE
ICEE-biased
100 random
10 1
−
0 20 40 60 80 100
FunctionEvaluations
Figure 3: Discrete BO results on 2D functions with 1024 candidate locations. The shading area
showsthe95%confidenceintervalofthemean.
The expected improvement baseline is implemented using BOTorch (Balandat et al., 2020). We
usedthe“SingleTaskGP”classfortheGPsurrogatemodel,whichusesaMatern5/2kernelwitha
Gammaprioroverthelengthscales.
E QUANTITATIVE RESULTS OF THE DISCRETE RL EXPERIMENTS
PleasefindthequantitativecomparisonoftheRLexperimentsshowninFig.2below. Theshown
values are the average returns over 100 sampled games and the values in the parentheses are the
confidenceintervalsofthemeanestimates, whichcorrespondtotheshadedareainthefigure. We
takethreetimepointsalongthetrajectoriesofin-contextpolicylearning. AsMGDTisnotableto
updatepolicyatinferencetime,weonlyestimateasinglemeanreturnforeachgame.
2https://github.com/karpathy/nanoGPT
15
muminiMcnuFotpaGPublishedasaconferencepaperatICLR2024
DarkRoom(10thEpisode) DarkRoom(30thEpisode) DarkRoom(50thEpisode)
ICEE 8.15(1.29) 12.37(1.14) 13.61(0.86)
AD 3.74(1.15) 4.51(1.17) 4.03(1.15)
AD-sorted 0.05(0.05) 3.83(0.87) 12.48(1.37)
MGDT 1.86(0.93) 1.86(0.93) 1.86(0.93)
source 5.13(1.19) 5.13(1.19) 5.13(1.19)
DarkRoom(Hard)(10th) DarkRoom(Hard)(30th) DarkRoom(Hard)(50th)
ICEE 0.48(0.10) 0.74(0.09) 0.79(0.08)
AD 0.33(0.09) 0.43(0.10) 0.43(0.10)
AD-sorted 0.08(0.05) 0.55(0.10) 0.75(0.08)
MGDT 0.09(0.06) 0.09(0.06) 0.09(0.06)
source 0.51(0.10) 0.51(0.10) 0.51(0.10)
DarkKey-To-Door(5th) DarkKey-To-Door(10th) DarkKey-To-Door(20th)
ICEE 1.04(0.15) 1.50(0.12) 1.84(0.08)
AD 0.67(0.15) 1.02(0.17) 0.94(0.17)
AD-sorted 0.17(0.08) 0.84(0.14) 1.77(0.09)
MGDT 0.34(0.11) 0.34(0.11) 0.34(0.11)
source 1.10(0.19) 1.10(0.19) 1.10(0.19)
16