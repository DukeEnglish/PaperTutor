Bayesian Diffusion Models for 3D Shape Reconstruction
HaiyangXu‚àó,1 YuLei‚àó,2 ZeyuanChen3 XiangZhang3
YueZhao4 YilinWang4 ZhuowenTu3
1 UniversityofScienceandTechnologyofChina 2 ShanghaiJiaoTongUniversity
3 UniversityofCalifornia,SanDiego 4 TsinghuaUniversity
Abstract Input Image PC2 (baseline) BDM-M (ours) BDM-B (ours)
WepresentBayesianDiffusionModels(BDM),apredic-
tionalgorithmthatperformseffectiveBayesianinferenceby
tightly coupling the top-down (prior) information with the
bottom-up (data-driven) procedure via joint diffusion pro-
cesses. WeshowtheeffectivenessofBDMonthe3Dshape
reconstructiontask. Comparedtoprototypicaldeeplearn-
ingdata-drivenapproachestrainedonpaired(supervised)
data-labels (e.g. image-point clouds) datasets, our BDM
bringsinrichpriorinformationfromstandalonelabels(e.g.
point clouds) toimprove the bottom-up 3D reconstruction.
AsopposedtothestandardBayesianframeworkswhereex-
plicit prior and likelihood are required for the inference,
BDMperformsseamlessinformationfusionviacoupleddif- Figure 1. Baseline vs Bayesian Diffusion Models. Our BDM
bringsrichpriorknowledgeintotheshapereconstructionprocess,
fusion processes with learned gradient computation net-
fixing the incorrect predictions by the baseline (top row). BDM
works. The specialty of our BDM lies in its capability to
surpassesbaselinesinallthreetrainingdatascales(bottomrow).
engage the active and effective information exchange and
fusionofthetop-downandbottom-upprocesseswhereeach
the Viola-Jones face detector [72]. The formulations of
itselfisadiffusionprocess. Wedemonstratestate-of-the-art
p(x|y)p(y) [41] and p (y|x)p(y) [40] can be solved via
resultsonbothsyntheticandreal-worldbenchmarksfor3D Œ≥
e.g., the Markov Chain Monte Carlo (MCMC) sampling
shapereconstruction.
methods[3,75].
1.Introduction When does the Bayesian help? The Bayesian theory
[5, 34, 85] provides a principled statistical foundation to
The Bayesian theory [5, 38], under the general princi- guide the top-down and bottom-up inference, which is
pleofanalysis-by-synthesis[85],hasmadeaprofoundim- deemed to be of great biological significance [38]. In the
pact on a myriad of tasks in computer vision and machine early development of computer vision [51], the objects in
learning,includingfacemodeling[13],shapedetectionand study [13, 21, 74] are often of simplicity and are picked
tracking[6],imagesegmentation[69],scenecategorization from relatively small-scale datasets [19, 27, 56]. The top-
[18], image parsing [70], depth estimation [43, 77], object downprior[20,70]canthereforeprovideastrongregular-
recognition[20,21,74],andtopicmodeling[7]. izationandinductivebiastothebottom-upprocessthathas
We assume the task of predicting y for a given input x beentrainedfromthedata[19,27].
(yandxrepresentrespectivelythe3Dpointcloudsandthe
WhyistheBayesiannotanymorewidelyadoptedinthe
input image in this paper). The Bayes‚Äô theorem turns the
deep learning era? Although still being an active subject
posterior p(y|x) into the product of the likelihood p(x|y)
[22] in study, the top-down/prior information has not been
andthepriorp(y)asp(y|x) ‚àù p(x|y)p(y), whichcanbe
widelyadoptedinthebig-data/deep-learningera,wherean
furtherapproximatedbyp (y|x)p(y)[40],wherep (y|x)
Œ≥ Œ≥ immediateimprovementcanbeshownoverthedata-driven
represents a direct bottom-up (data-driven) process, e.g.,
models[16,39,61]learnedfromlarge-scaletrainingsetof
inputandground-truthpairsS ={(x ,y ),i=1..n}.The
*equalcontribution.WorkdoneduringtheinternshipofHaiyangXu, s i i
YuLei,YueZhao,andYilinWangatUCSanDiego. reasonsarethreefold:1)Richfeaturesfromlarge-scaledata
1
4202
raM
11
]VC.sc[
1v37960.3042:viXra[14] become substantially more robust than manually de- 2.RelatedWork
signedones[48],whereasthetop-downpriorp(y)forstruc-
Bayesian Inference. As stated previously, the Bayesian
turedoutputynolongershowsanimprovement. 2)Strong
theory [5, 38] has been adopted in a wide range of com-
bottom-upmodels[16,39,61]learnedinax ‚Üí y fashion
puterapplications[6,13,18,20,21,43,70,74,77],butthe
frompaired/superviseddataset, S = {(x ,y ),i = 1..n},
s i i
results of these approaches are less competitive than those
arepowerful,andtheydonotnecessarilyseeanimmediate
bythedeeplearningbasedones[28,39,61].
benefit from introducing a separate prior p(y) that is ob-
tainedfromS l ={y i,i=1..n}alone,asknowledgeabout 3D Shape Generation. Early 3D shape generation meth-
theyhasalreadybeenimplicitlycapturedinthedata-driven ods [1, 24, 29, 57, 78, 84] typically leverage variational
p Œ≥(y|x). 3) The presence of the intermediate stages with auto-encoders (VAE) [37] and generative adversarial net-
different architectural designs for the deep models makes works(GAN)[26]tolearnthedistributionofthe3Dshape.
merely combining the data-driven model p Œ≥(y|x) and the Recently, the superior performance of diffusion models in
prior p(y) not so obvious, as the distributions for p Œ≥(y|x) generativetasksalso makesthemthego-tomethodsin 3D
andp(y)arehardtomodelandmaynotco-exist. shape generation. PVD [89] proposes to diffuse and de-
noiseonpointcloudsusingaPoint-Voxel-CNN[46],while
The emerging opportunity for combining bottom-up
in DMPGen [49], the diffusion process is modeled by a
and top-down processes with diffusion-based models.
PointNet [60]. LION [86] uses a hierarchical VAE to en-
Therecentdevelopmentindiffusionmodels[31,62,64,65]
code 3D shapes into latents where the diffusion and gen-
hasledtosubstantialimprovementstounsupervisedlearn-
erativeprocessesareperformed. Anotherpopularcategory
ing beyond the traditional VAE [37] and adversarial learn-
of3Dgenerativemodelsleverages2Dtext-to-imagediffu-
ing[26,33,68].Thepresenceofdiffusionmodelsforlearn-
sion models as priors and lifts them to 3D representations
ingbothp(y)(e.g. shapepriors[89])andp (y|x)(e.g. 3D
Œ≥
[42,55,59,73].
shapereconstruction[15,52])inspiresustodevelopanew
inferencealgorithm,BayesianDiffusionModels,thatisap- Single-View 3D Reconstruction. Recovering 3D object
pliedtosingle-view3Dshapereconstruction. shapes from a single view is an ill-posed problem in com-
puter vision. Traditional approaches extract multi-modal
Thecontributionofourpaperissummarizedasfollows:
information, including shading [4, 32], texture [76], and
‚Ä¢ We present Bayesian Diffusion Models (BDM), a new
silhouettes [10], for reconstructing 3D shapes. Learning-
statistical inference algorithm that couples diffusion-
basedreconstructionmethodsbecomepopularwiththead-
basedbottom-upandtop-downprocessesinajointframe-
vanceofneuralnetworksandtheavailabilityoflarge-scale
work. BDM is particularly effective when having sepa-
2D-3Ddatasets[9,23]. Inthesemethods,different3Drep-
rately available data-labeling (supervised) dataset S =
s resentations are employed, including voxel grids [12, 25,
{(x ,y ),i = 1..n} and standalone label dataset S =
i i l 81,82,88],pointclouds[17,50],meshes[35,36,80],and
{y ,i = 1..m} for training p (y|x) and p(y) respec-
i Œ≥ implicitfunctions[11,53,63].
tively. For example, obtaining a set [66] for real-world
Someotherreconstructionmethodstakeimagesascon-
imageswiththecorrespondingground-truth3Dshapesis
ditions for generative models [15, 52, 78]. In particular,
challengingwhereasalargedatasetofstandalone3Dob-
they learn the prior shape distribution from large-scale 3D
jectshapessuchasShapeNet[9]isreadilyavailable.
datasetsandthenperform3Dreconstructionwith2Dimage
‚Ä¢ Two strategies for fusing the information exchange be-
observations. 3D-VAE-GAN[78]employsGANandVAE
tweenthebottom-upandthetop-downdiffusionprocess
tolearnageneratormappingfromalow-dimensionalprob-
are developed: 1) a blending procedure that takes the
abilisticspacetoa3D-shapespace,andfeedsimagestothe
two processes in a plug-in-and-play fashion, and 2) a
generator for reconstruction. Recent methods for single-
mergingprocedurethatistrained.
view reconstruction are mostly based on diffusion models
‚Ä¢ Weemphasizethekeypropertyoffusion-with-diffusion
[15,44,45,52]. PC2 [52]proposestoprojectencodedfea-
in BDM vs. fusion-by-combination in the traditional
turesfrom2Dbackto3D,whichfacilitatesthepointcloud
MCMC Bayesian inference. BDM also differs from the
reconstruction in denoising. CCD-3DR [15] introduces a
current pre-training + fine-tuning process [8] and the
centereddiffusionprobabilisticmodelbasedonPC2,which
promptengineeringpractice[30]bymakingthebottom-
offersbetterconsistencyinalignmentsoflocalfeaturesand
up and top-down integration process transparent and ex-
finalpredictionresults. RenderDiffusion[2]presentsanex-
plicit; BDMpointstoapromisingdirectionincomputer
plicitlatent3Drepresentationintoadiffusionmodel,yield-
vision and machine learning with a new diffusion-based
inga3D-awarepipelinethatcouldperform3Dreconstruc-
Bayesianmethod.
tion. Zero1-to-3 [45] and One-2-3-45 [44] inject camera
BDMdemonstratesthestate-of-the-artresultsonthesin- information into a 2D diffusion model and reconstruct 3D
gleimage3Dshapereconstructionbenchmarks. shapeswithsynthesizedmulti-viewimages.
2ùë¶"!!"# Bayesian
Denoising Step
ùë¶"!!"#ùëÉ!(ùë¶$"!#$|ùë¶$"!#%)
ùë¶"!!"$ ùë¶"!!"%
DeB noa iy se insi ga n
S tep
Prior
S
D tei pff susion
Fusion Fusion
ùë¶!! ùë¶!!"# ùë¶!!"$ ùë¶!!"% Diffusion Steps
ùëÉ&(ùë¶"!#$|ùë¶"!#%,ùë•)
Œ¶(ùë¶"!#‚Äô,ùë¶$"!#‚Äô)
ùë¶! ùë¶! ùë¶!! ùë¶!!"# Œ¶(ùë¶!!"#,ùë¶%!!"#) ùë¶$
Figure 2. Overview of the generative process in our Bayesian Diffusion Model. In each Bayesian denoising dtep, the prior diffusion
modelfuseswiththereconstructionprocess,bringingrichpriorknowledgeandimprovingthequalityofthereconstructedpointcloud.We
illustrateourBayesiandenoisingstepintwoways,leftintheformofaflowchartandrightintheformofpointclouds.
Prompts and Latent Representations in Transformers. whereœµt denotesasequenceofstepsizeandŒ∑t isaGaus-
Transformers [71] provide a general tokenized learning sian noise. The challenge in implementing Equation 1 is
framework,allowingtheinsertionoftherepresentationofy twofold: 1)itrequiresknowingtheexplicitformulationfor
intop (y|x)asspecialtokens[10,30]. However,theprior both the logp (yt|x) and logp(yt), which is hard to ob-
Œ≥ Œ≥
knowledgeinTransformersservesasalatentconditionthat taininreal-worldapplications. 2)ameresummationforthe
isoftenopaqueandnon-interpretable. gradients‚àálogp (y|x)and‚àálogp(yt)limitsthelevelof
Œ≥
interactionbetweenthebottom-upandtop-downprocesses.
3.Method Fig.3showsthebasicBayesformulationanddemonstrates
thestochasticgradientLangevininferenceresult.
Inthefollowingsection,weintroducetheBayesianDif-
fusion Models, the framework of which is illustrated in 3.2.DenoisingDiffusionProbabilisticModels
Fig. 2. Initially, a concise overview of denoising diffu-
Thedenoisingdiffusionmodels[31,64,65]havedemon-
sionmodels,particularlyfocusingonpointclouddiffusion
stratedsuperiorperformanceinrepresentingthestructured
models, is presented. This is followed by an exposition of
data of high-dimension in both paired data-labels setting
its conditional variant applied to 3D shape reconstruction.
(learning ‚àálogpt(y) from S = {y ,i = 1..m} [89])
Subsequently,wedelveintothecentralconceptofourpro- l i
andstandalonelabelssettings(learning‚àálogpt(y|x)from
posedBayesianDiffusionModelswhichintegrateBayesian Œ≥
S ={(x ,y ),i=1..n}[52])manners.
priors. Concludingthissection,weprovideanin-depthex- s i i
Here, we discuss the general formulation of the DDPM
plorationofournovelprior-integrationmethodology.
model[31,64]inthecontextofsingleimage3Dshapere-
construction. Forpointcloudgeneration, diffusionmodels
3.1. Bayesian Inference with Stochastic Gradient
are adept at learning the 3D shape of objects represented
LangevinDynamics
in point cloud format. At a high level, diffusion models
AsdiscussedinSec.1, ourtaskistopredicty (asetof iteratively denoise 3D points from a Gaussian sphere into
pointclouds)foragiveninputx‚ààRq(aninputimage).For a recognizable object. Consider a set of point clouds y0,
the3Dshapereconstructiontask,theoutputyconsistsofa consisting of N points, as an object in a 3N-dimensional
set of 3D points. The Bayesian theory [5] focuses on the space. The diffusion model is employed to learn the map-
study of the posterior p(y|x) ‚àù p(x|y)p(y), where p(y) pings Œ∏ : R3N ‚Üí R3N. Specifically,itisdesignedtoesti-
is the prior (top-down information). The likelihood term mate q(yt‚àí1|yt), which is the offset of the points from its
p(x|y) can be alternatively replaced [40] by a data-driven position at timestep t. This approach aligns with standard
(bottom-up) distribution p (y|x), which is learned from a diffusion model practices, which are trained to predict the
Œ≥
paired data-labels training set S = {(x ,y ),i = 1..n}. noiseœµ‚ààR3N usingthefollowinglossfunction:
s i i
Therefore, the inference for the optimal prediction y‚àó can
then be carried via Markov Chain Monte Carlo (MCMC) L=E œµ‚àºN(0,I)(cid:104)(cid:13) (cid:13)œµ‚àís Œ∏(yt,t)(cid:13) (cid:13)2 2(cid:105) . (2)
[3]usingstochasticgradientLangevindynamics[75]:
The process of single-view reconstruction can be con-
‚àÜyt = œµt (cid:0) ‚àálogp (yt|x)+‚àálogp(yt)(cid:1) +Œ∑t sidered as a conditional point cloud generation. Diffu-
2 Œ≥ sionmodelscanbeeffectivelyappliedhereaswell. Given
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
data‚àídriven prior a single-view image x, the diffusion model aims to esti-
Œ∑t ‚àºN(0,œµt) (1) mate q(y |y ,x). Therefore, we should sample from
t‚àí1 t
3Bayesian Prediction Stochastic Gradient Langevin
Prior
‚Ä¶
Data-driven
Bayesian Diffusion Model Process
Top-down (prior)
‚Ä¶
Fusion Fusion
Bottom-up (data-driven)
Figure 3. Illustration for the Bayesian Diffusion Models compared with the standard Bayesian formulation. We present the standard
BayesianformulationandtheoneusingstochasticgradientLangevinonthetoppart,whileourproposedBDMonthebottom.
q(y |y ,x)forthistask. Alltheotherconceptsandprin- correspondingprobabilityfrommodelŒ≥andposteriorfrom
t‚àí1 t
ciplesarethesameaspointcloudgeneration. thepriormodel. AsshowinFig.2,weuseaninaccessible
function Œ¶ to incorporate the prior diffusion gradient into
3.3.BayesianDiffusionModel
thereconstructiondiffusiongradient. Accordingly, wecan
In our work, we denote x as an input image instance, deducethefusedgradientinourBayesianDiffusionModel
while yt,t = 1¬∑¬∑¬∑T to represent the 3D object we want asfollows:
toreconstruct. Œ≥ istakenasthedistributionlearnedbythe
‚àálogp (yt|x)=‚àá logŒ¶(cid:0) p (yt|x,yÀút+1‚àºp (yt+1|x),
reconstructionmodelandŒ†representsthedistributionafter œÄ Œ≥ Œ≥ œÄ (5)
fusingpriorandŒ≥.
p(yt|yÀút+1‚àºp œÄ(yt+1|x)(cid:1)
Generative model: The shapes generated by prior model
can be formulated as
p(cid:0) y0(cid:1)
. The noise predicted by the
3.4.PointCloudPriorIntegration
model at every timestep drives the spatial distribution ac-
As stated in Sec. 1, diffusion-based models have pro-
cording to the Markov Chain state transition probability
vided us with the possibility to benefit from both bottom-
p(cid:0) yt |yt+1(cid:1) . Forthegradient,wehave:
upandtop-downprocesses. Specifically,themultistepin-
Œµ(cid:0) yt(cid:1) =‚àíœÉ ‚àá logp(yt) (3) ferenceprocedureallowsmoreflexibleandeffectiveforms
t yt
of function Œ¶ in Eq. (5). Therefore, we employ step-wise
Reconstructionmodel: Inthesamewayofunderstanding interaction between a generative diffusion model and a re-
generative diffusion model, the shapes generated by Œ≥ can construction model, facilitating closer integration of point
be formulated as p (cid:0) y0 |x(cid:1) . The model can reconstruct cloud priors. In particular, we feed the intermediate point
Œ≥
point clouds from learned p (cid:0) yt |yt+1,x(cid:1) . In a similar cloud from our reconstruction model into the prior model,
Œ≥
waytotheabove,wehave: forward it through a certain number of timesteps in both
modelsandfusetwonewpointcloudsfromthepriormodel
Œµ (cid:0) yt,x(cid:1) =‚àíœÉ ‚àá logp (yt |x) (4)
Œ≥ t yt Œ≥ andthereconstructionmodelastheinputforthereconstruc-
tionmodelinthenexttimestep.Asbelowweintroducetwo
It is obvious to conclude that the state transition prob-
fusion methods: BDM-M (Merging) and BDM-B (Blend-
ability of our Bayesian Diffusion Model comes from the
ing). Botharecarriedoutunderfusion-with-diffusion.
4I. BDM-Merging II. BDM-Blending and
N
p (cid:0) yt |yt+1,x(cid:1) =(cid:89) p (cid:0) zt |zt+1,x(cid:1) (8)
Œ≥ Œ≥ i i
Points from
Prior model prior model i=1
Points are selected based on Œ®; for instance, we might
choose50%ofthepointsfromthepriorand50%fromthe
reconstructionmodel. Statistically,thisapproachisakinto
Reconstruction Points from
model reconstruction blendingtwopointclouds,resultinginamixeddistribution
model
ofpointsfrombothsources.Theblendingmethodalsosup-
ports our Bayesian Diffusion Model theory, the details of
Figure 4. Illustration of our proposed fusion methods: BDM-M whichwillbegivenintheAppendix.
and BDM-B. The left part is the BDM-M, while the right side
showstheBDM-B. 4.Experiment
Dataset. To demonstrate the efficacy of our Bayesian
3.4.1 BDM-M(Merging)
Diffusion Model, we conducted our experiments on two
datasets: the synthetic dataset ShapeNet [12] and the real-
InMerging,weproposealearnableparadigmforincor-
world dataset Pix3D [66]. ShapeNet [9], a collection of
porating knowledge from the prior model into the recon-
3D CAD models, encompasses 3,315 categories from the
structionmodel. WeimplementBDM-Mbyusingtwodif-
WordNet database. Following prior work [15, 52, 83], we
fusion models, PVD [89] and PC2 [52]. Both the diffu-
utilizedasubsetofthreeShapeNetcategories‚Äì{chair,air-
sion models use PVCNN as the backbone for predicting
plane,car}‚Äìfrom3D-R2N2[12],includingimagerender-
noise. Our Merging method focuses on the decoder part
ings, camera matrices, and train-test splits. Pix3D com-
of PVCNN. To preserve the original knowledge in the re-
prises diverse real-world image-shape pairs with meticu-
constructionmodel,wefreezetheencoderandfinetunethe
lously annotated 2D-3D alignments. For a balanced com-
decoderofPC2. Specifically,following[87],weretainthe
parisonwithCCD-3DR[15]andPC2 [52], wereproduced
original encoders of both models while feeding the multi-
these two works and adhered to CCD-3DR‚Äôs benchmark
scalefeaturesofPVD‚ÄôsencoderdirectlyintoPC2‚Äôsdecoder.
onthreecategories: {chair,table,sofa},allocating80%of
Eachlayeroftheencoderenhancedbythisintegrationisfa-
samplesfortrainingandtheremaining20%fortesting.Fur-
cilitatedbyazero-initializedconvolutionlayer. Thissetup
ther details about extended object categories are discussed
enables a seamless and implicit merging of the knowledge
intheAppendixB.
fromthepriormodelandthereconstructionmodel.
Implementation Details. For both the ShapeNet-R2N2
3.4.2 BDM-B(Blending) andPix3Ddatasets, wesample4,096pointsper3Dobject
and set the rendering resolution to 224√ó224. Notably, for
Beyond the implicit incorporation of prior knowledge, Pix3D,imagesarecroppedusingtheirboundingboxes,ne-
we also introduce an explicit training-free fusion method cessitating adjustments to the camera matrices to accom-
on point clouds, termed as BDM-B. It explicitly combines modate the non-object-centric nature and varying sizes of
two groups of point clouds, yt = {zt}N and yt = the images. For the training of the generative diffusion
i i=1 Œ≥
{zt }N ,eachconsistingofN points. Thesepointclouds model, we employ the PVD [89] architecture, adhering to
Œ≥,i i=1
are noise-reduced versions derived from the same origin, theirtrainingmethodologies. Forthetrainingoftherecon-
generated by generation and reconstruction models sepa- structiondiffusionmodel,weselectPC2 andCCD-3DRas
rately.Theblendingoperationemploysaprobabilisticfunc- two baselines and follow the recipe of CCD-3DR. The in-
tion Œ®, which assigns a selection probability to each pair ference step is set as 1,000 for both the generative model
ofcorrespondingpointszt,zt , wherezt denotesthei-th andthereconstructionmodel.InourBDMinferenceframe-
j Œ≥,j i
pointina pointcloud y. Theblendingequationis defined work, Bayesian integration is strategically applied at spe-
asfollows: cific intervals during the denoising process. This integra-
tion occurs every 32 steps, both in the early stage and in
yt =Œ®(yt,yt) (6)
œÄ Œ≥ the late stage of denoising. The fusion process initiated
bythisintegrationextendsthroughout16steps, ensuringa
Additionally,assumingpointsarei.i.d.,thewholepoint
balancedandeffectiveincorporationofBayesianprinciples
cloudscanbeformulatedas
throughoutthedenoisingprocedure. Trainingofgenerative
models was conducted on 4 NVIDIA A5000 GPUs, while
N
p(cid:0) yt |yt+1(cid:1) =(cid:89) p(cid:0) zt |zt+1(cid:1) (7) wetrainedreconstructionmodelsutilizingasingleNVIDIA
i i A5000GPU.MoredetailsareavailableintheAppendixA.
i=1
5Input Image Baseline (10%) BDM-M (ours) BDM-B (ours) Baseline (50%) BDM-M (ours) BDM-B (ours) Ground Truth
Figure5.QualitativecomparisonsonthesyntheticShapeNet-R2N2dataset.WeusePC2[52]andCCD-3DR[15]asbaselinesof3Dshape
reconstruction.Rows1-3showthevisualizationofPC2whilerows4-6displaytheresultofCCD-3DR.WeshowourBDM‚Äôsresultsunder
10%dataincolumn2-4andtheresultsunder50%incolumn5-7.Column8givesthecorrespondinggroundtruth.
4.1.QuantitativeResults pronounced when training data scale decreases, highlight-
ingtheeffectivenessofthepriorfacingdatascarcity.
Weevaluatetheperformanceofreconstructionwithtwo
Pix3D. Following CCD-3DR, we also evaluate on the
widelyrecognizedmetrics: ChamferDistance(CD)andF-
Pix3D dataset in Tab. 2. It can be seen that our method
Score@0.01(F1).ChamferDistancemeasuresthedisparity
effectivelyimprovestheperformanceandachievesstate-of-
betweentwopointsetsbycalculatingtheshortestdistance
the-art. Notably, wetrainthereconstructionmodeltrained
fromeverypointinonesettotheclosestpointintheother
on Pix3D, while the prior model is trained on the same
set. ToaddresstheissueofCD‚Äôssusceptibilitytooutliers,
category of the separate, larger dataset: ShapeNet-R2N2
we additionally present F-Score at a threshold of 0.01. In
(‚àº10timessizeofPix3D).Inthisway,weavoidthepossi-
thismetric,areconstructedpointisdeemedaccuratelypre-
ble leaking and memorization problem, i.e., the generative
dictedifitsnearestdistancetothepointsinthegroundtruth
modelnaivelyretrievethenearestmemorisedshapelearned
pointcloudiswithinthespecifiedthreshold,whichpresents
fromthereconstructionmodel.
ameasureofprecisioninthereconstructionprocess.
4.2.QualitativeResults
ShapeNet-R2N2. Tab.1presentsourBDM‚Äôsperformance
acrossvarioustrainingdatascalesonShapeNet-R2N2. The In addition to the quantitative study, we also show the
resultsindicateimprovementinbothCDandF1acrossall qualitative results to show the superiority of our BDM on
threecategories. Notably, theimprovementbecomesmore bothShapeNetandPix3D.FortheShapeNet-R2N2dataset,
6Input Image PC2 (baseline) BDM-M (ours) BDM-B (ours) Ground Truth Input Image CCD (baseline) BDM-M (ours) BDM-B (ours) Ground Truth
Figure6. Qualitativecomparisonsonthereal-worldPix3Ddataset. Weexaminethreedistinctcategories,eachrepresentedinaseparate
row.Columns3,4and8,9featureourBDM,implementedbasedonPC2andCCD-3DRrespectively,foracomparativeanalysis.
Chair Airplane Car
Method 10% 50% 100% 10% 50% 100% 10% 50% 100%
CD‚Üì F1‚Üë CD‚Üì F1‚Üë CD‚Üì F1‚Üë CD‚Üì F1‚Üë CD‚Üì F1‚Üë CD‚Üì F1‚Üë CD‚Üì F1‚Üë CD‚Üì F1‚Üë CD‚Üì F1‚Üë
Base:PC2[52] 97.25 0.393 73.58 0.437 65.57 0.464 88.00 0.605 76.39 0.628 65.97 0.655 64.99 0.524 62.59 0.542 64.36 0.547
BDM-M(ours) 94.94 0.395 71.56 0.446 64.48 0.468 87.75 0.604 73.19 0.629 65.16 0.653 63.53 0.524 60.71 0.549 64.16 0.554
BDM-B(ours) 94.67 0.410 69.99 0.463 64.21 0.485 83.62 0.612 68.66 0.641 59.04 0.660 60.48 0.539 62.58 0.554 65.85 0.559
Base:CCD-3DR[15] 89.79 0.418 63.13 0.474 58.47 0.498 81.29 0.612 72.46 0.635 62.77 0.651 63.13 0.531 62.25 0.550 61.88 0.562
BDM-M(ours) 81.47 0.425 62.07 0.477 57.31 0.493 81.54 0.608 69.31 0.632 61.87 0.652 61.92 0.531 61.24 0.555 63.66 0.561
BDM-B(ours) 79.26 0.441 60.07 0.497 56.78 0.510 77.34 0.621 66.83 0.644 56.96 0.660 59.05 0.546 60.59 0.560 66.90 0.569
Table1. PerformanceonChair,AirplaneandCarofShapeNet-R2N2. WeevaluateourBDM,comparingwithtwobaselines: PC2 and
CCD-3DR. These experiments span three different scales of training data (10% / 50% / 100%) for the reconstruction diffusion model,
demonstratingtheefficacyofourBDMmethod.
Chair Sofa Table whichdoesn‚Äôtfurtherincuradditionaltrainingcost.
Method
CD‚Üì F1‚Üë CD‚Üì F1‚Üë CD‚Üì F1‚Üë
Base:PC2[52] 115.94 0.443 47.17 0.445 202.77 0.397 PC2/CCD-3DR BDM-B BDM-M
BDM-M(ours) 113.40 0.449 44.50 0.451 202.08 0.413
#Parameters(M) 47.41 73.78 74.82
BDM-B(ours) 110.60 0.455 45.05 0.455 186.46 0.429
Runtime(s) 46.89 48.84 49.24
Base:CCD-3DR[15] 111.42 0.456 44.91 0.450 196.28 0.418
GPUmemory(GB) 1.73 1.93 2.01
BDM-M(ours) 110.86 0.464 44.86 0.452 193.56 0.424
BDM-B(ours) 111.12 0.466 44.51 0.456 194.91 0.423
Table3.Modelparametersandinference-timeefficiency.
Table2.PerformanceonChair,SofaandTableofPix3D.Weeval-
uateourBDMonthetwobaselines:PC2andCCD-3DR. 4.4.AblationStudy
Fig. 5 demonstrates our BDM‚Äôs capacity in synthetic 3D TovalidatetheeffectivenessandrationalityofourBDM,
object reconstruction. In Fig. 6, it can be seen clearly that we conduct several ablation studies to explore the impact
our method surpasses baselines with respect to the recon- of the timing, duration and intensity to absorb priors. Un-
struction quality on the Pix3D dataset. Notably, our BDM less otherwise specified, we set BDM-B comprising PVD
effectively restores the missing spindles of the chair in the trained on 100% data and CCD-3DR trained on 10% data
firstimageandeliminateshallucinationsinthelastimage. fromShapeNet-chairasourbaseline.
Prior Integration Timing. This subsection evaluates the
4.3.EfficiencyandFairnessAnalysis
effectiveness of integrating prior knowledge at various
AsshowninTab.3,wepresentBDM‚Äôsparameters,run- stages of the denoising process. Tab. 4 reveals that inte-
time and GPU memory when doing inference with batch grating priors in the late stage alone yields significant im-
sizeof1. Whileparametersincreaseduetotheincorpora- provement on model performance, reducing CD to 80.22
tionofpriormodelP,memoryusageandruntimeofBDM and increasing F1 to 0.436. Furthermore, combining early
only increase slightly. Moreover, BDM leverages off-the- and late-stage integration further enhances results, achiev-
shelfpre-trainedweightsforpriormodelPwhichisfrozen, ingthelowestCDof79.26andthehighestF1of0.441.This
7ChamferDistance‚Üì F-Score@0.01‚Üë
contrastswiththemiddlestageintegration,whichevende-
gradestheperformanceofthebaselineonF1. Baseline 58.47 0.498
CFG[30] 59.08 0.495
Early Middle Late ChamferDistance‚Üì F-Score@0.01‚Üë
BDM 56.78 0.510
89.79 0.418
‚úì 82.34 0.421 Table7. WecompareourBDMwithCFG.Wetrainthesemodels
‚úì 87.61 0.395 andtestthesetwomethodson100%dataofchairfromShapeNet.
‚úì 80.22 0.436
‚úì ‚úì 79.26 0.441 4.6.HumanEvaluation
‚úì ‚úì ‚úì 81.92 0.416
To better evaluate the reconstruction quality, we also
Table 4. Ablation on the timing of prior integration. This table conducted human evaluation. We randomly selected 20
presentstheimpactofapplyingpriorintegrationduringtheearly, comparison groups from the Chair, Airplane, and Car
middle,andlatestagesofthedenoisingprocessonCDandF1. classesintheShapeNetdataset,totaling60groups. Ineach
PriorIntegrationDuration.AsshowninTab.5,weinves- group, wepresentoutputsgeneratedbyCCD-3DR,BDM-
tigate how varying the duration of prior integration affects B,andBDM-M.Sixteenevaluatorsthenrankedeachgroup
the denoising process. First, the performance will greatly on a scale of 1 to 3, and the average scores are shown in
improve once the prior duration is greater than 1, thereby Tab.8.Theresultsshowthatourtwomethods,BDM-Mand
stronglyvalidatingtheeffectivenessofourBDM.Notably, BDM-B,stilloutperformsCCD-3DR,whichisalignedwith
the prior integration duration of 16 steps demonstrates the thequantitativeresultpresentedinthemainpaper.Morede-
most substantial improvement. This is a remarkable im- tailswillbediscussedinAppendixE.
provement over the baseline (0 step). The diminishing re-
HumanEvaluation CCD BDM-B BDM-M
turnsareobservedwiththedurationof32steps,suggesting
thatamoderatedurationofpriorintegrationoptimallybal- Chair 1.48 2.15 2.32
ancesdenoisingeffectivenessandpriorguidance. Airplane 1.74 1.86 2.40
Car 1.77 1.88 2.35
PriorDuration 0step(baseline) 1step 2step 4step 8step 16step 32step
ChamferDistance‚Üì 89.79 80.89 81.02 80.65 79.72 79.26 79.94 Average 1.67 1.96 2.35
F-Score@0.01‚Üë 0.418 0.427 0.430 0.429 0.432 0.441 0.438
Table5. Ablationonthedurationofpriorintegration. Thistable Table8. HumanEvaluationover3categoriesforCCD,BDM-B,
presentshowdifferentdurationsofpriorintegrationaffectCDand andBDM-M,with3beingthebestand1beingtheworst.
F1,rangingfrom0to32steps. 5.ConclusionandLimitations
Prior Integration Ratio. In this part, we present an abla- In this paper, we present Bayesian Diffusion Model
tionontheimpactofthepriorintegrationratioontheBDM- (BDM),anoveldiffusion-basedinferencemethodforpos-
Bprocess. AscanbeseeninTab.6,interestingly,agradual terior estimation. BDM overcomes the limitations in the
increaseinthepriortake-inratioyieldsvaryingresults. At traditional MCMC-based Bayesian inference that requires
25%,thereisaminordetrimenttoperformance. However, having the explicit distributions in performing stochastic
at50%,weobserveasignificantimprovement,markingthe gradientLangevindynamicsbytightlycouplingthebottom-
optimal balance in prior integration. On the contrary, fur- upandtop-downdiffusionprocessesusinglearnedgradient
therincreasingtheratioto75%and100%leadstoadrastic computationnetworks.Weshowaplug-and-playversionof
decline in performance. These results suggest that while theBDM(BDM-B)andalearnedfusionversion(BDM-M).
a moderate level of prior integration enhances the model‚Äôs BDMisparticularlyeffectiveforapplicationswherepaired
performance,excessiveintegrationcanbedetrimental. data-labels such as image-object point clouds, are scares,
whilestandalonelabels,likeobjectpointclouds,areabun-
PriorRatio 0%(baseline) 25% 50% 75% 100%
dant. It demonstrates the state-of-the-art results for single
ChamferDistance‚Üì 89.79 88.22 79.26 142.69 256.13
F-Score@0.01‚Üë 0.418 0.382 0.441 0.307 0.242 image 3D shape reconstruction. BDM points to a promis-
ing direction to perform the general inference in computer
Table6.Ablationontheratioofpriorintegration.Thistablecom-
vision and machine learning beyond the 3D shape recon-
parestheeffectsofdifferentpriorintegrationratiosonCDandF1.
structionapplicationshownhere.
4.5.BDMvsCFG
Limitations. BDMrequiresboththeprioranddata-driven
Considering that Classifier-Free Guidance (CFG) [30] processestobediffusionprocesses.Also,BDM-Btakesad-
also shows the relation in the inference stage between the vantageoftheexplicitrepresentationofpointclouds,which
gradientŒµ(yt)fromtheunconditionaldiffusionmodeland mightnotbebroadlyadoptedonimplicitrepresentations.
thegradientŒµ (yt,x)fromtheconditionaldiffusionmodel, Acknowledgement. ThisworkissupportedbyNSFAward
Œ≥
we conduct an ablationstudy to compare our method with IIS-2127544. Wearegratefulfortheconstructivefeedback
CFG.AsshowninTab.7,bothourmethodssurpassCFG. byZiruiWangandZhengDing.
8References [16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
Leonidas Guibas. Learning representations and generative
vain Gelly, et al. An image is worth 16x16 words: Trans-
modelsfor3dpointclouds. InICML,2018. 2
formers for image recognition at scale. In ICLR, 2021. 1,
[2] TitasAnciukevicÀáius,ZexiangXu,MatthewFisher,PaulHen- 2
derson,HakanBilen,NiloyJMitra,andPaulGuerrero.Ren- [17] HaoqiangFan,HaoSu,andLeonidasJGuibas. Apointset
derdiffusion: Imagediffusionfor3dreconstruction,inpaint- generationnetworkfor3dobjectreconstructionfromasingle
ingandgeneration. InCVPR,2023. 2 image. InCVPR,2017. 2
[3] ChristopheAndrieu,NandoDeFreitas,ArnaudDoucet,and [18] LiFei-FeiandPietroPerona. Abayesianhierarchicalmodel
Michael I Jordan. An introduction to mcmc for machine forlearningnaturalscenecategories. InCVPR,2005. 1,2
learning. MachineLearning,50:5‚Äì43,2003. 1,3 [19] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gen-
[4] Joseph J Atick, Paul A Griffin, and A Norman Redlich. erative visual models from few training examples: An in-
Statistical approach to shape from shading: Reconstruc- cremental bayesian approach tested on 101 object cate-
tion of three-dimensional face surfaces from single two- gories. Computer Vision and Image Understanding, 106:
dimensionalimages. NeuralComputation,8(6):1321‚Äì1340, 59‚Äì70,2007. 1
1996. 2 [20] PedroFFelzenszwalbandDanielPHuttenlocher. Pictorial
[5] Jose¬¥ MBernardoandAdrianFMSmith. BayesianTheory. structuresforobjectrecognition.IJCV,61:55‚Äì79,2005.1,2
2009. 1,2,3 [21] RobertFergus,PietroPerona,andAndrewZisserman. Ob-
[6] AndrewBlakeandMichaelIsard. Activecontours: theap- jectclassrecognitionbyunsupervisedscale-invariantlearn-
plicationoftechniquesfromgraphics,vision,controltheory ing. InCVPR,2003. 1,2
andstatisticstovisualtrackingofshapesinmotion. 2012. [22] VincentFortuin. Priorsinbayesiandeeplearning:Areview.
1,2 InternationalStatisticalReview,90(3):563‚Äì591,2022. 1
[7] DavidMBlei,AndrewYNg,andMichaelIJordan. Latent [23] Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming
dirichletallocation. JMLR,3(Jan):993‚Äì1022,2003. 1 Wang,CaoLi,QixunZeng,ChengyueSun,RongfeiJia,Bin-
qiangZhao,etal. 3d-front:3dfurnishedroomswithlayouts
[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
andsemantics. InICCV,2021. 2
biah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakan-
[24] MatheusGadelha,SubhransuMaji,andRuiWang. 3dshape
tan,PranavShyam,GirishSastry,AmandaAskell,etal.Lan-
inductionfrom2dviewsofmultipleobjects. In3DV,2017.
guagemodelsarefew-shotlearners. InNeurIPS,2020. 2
2
[9] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,
[25] RohitGirdhar, DavidFFouhey, MikelRodriguez, andAb-
Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,
hinav Gupta. Learning a predictable and generative vector
Manolis Savva, Shuran Song, Hao Su, et al. Shapenet:
representationforobjects. InECCV,2016. 2
An information-rich 3d model repository. arXiv preprint
[26] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
arXiv:1512.03012,2015. 2,5
Xu,DavidWarde-Farley,SherjilOzair,AaronCourville,and
[10] GermanKMCheung,SimonBaker,andTakeoKanade. Vi-
Yoshua Bengio. Generative adversarial nets. In NeurIPS,
sualhullalignmentandrefinementacrosstime:A3drecon-
2014. 2
struction algorithm combining shape-from-silhouette with
[27] GregoryGriffin,AlexHolub,andPietroPerona.Caltech-256
stereo. InCVPR,2003. 2,3
objectcategorydataset. 2007. 1
[11] Julian Chibane, Thiemo Alldieck, and Gerard Pons-Moll.
[28] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
Implicitfunctionsinfeaturespacefor3dshapereconstruc-
Deep residual learning for image recognition. In CVPR,
tionandcompletion. InCVPR,2020. 2
2016. 2
[12] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin [29] PhilippHenzler,NiloyJMitra,andTobiasRitschel. Escap-
Chen,andSilvioSavarese. 3d-r2n2: Aunifiedapproachfor ing plato‚Äôs cave: 3d shape from adversarial rendering. In
single and multi-view 3d object reconstruction. In ECCV, ICCV,2019. 2
2016. 2,5
[30] Jonathan Ho and Tim Salimans. Classifier-free diffusion
[13] Timothy F Cootes, Christopher J Taylor, David H Cooper, guidance. InNeurIPS2021WorkshoponDeepGenerative
andJimGraham.Activeshapemodels-theirtrainingandap- ModelsandDownstreamApplications,2021. 2,3,8
plication. Computer Vision and Image Understanding, 61 [31] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu-
(1):38‚Äì59,1995. 1,2 sionprobabilisticmodels. InNeurIPS,2020. 2,3
[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, [32] BertholdKPHorn. Shapefromshading: Amethodforob-
andLiFei-Fei. Imagenet: Alarge-scalehierarchicalimage tainingtheshapeofasmoothopaqueobjectfromoneview.
database. InCVPR,2009. 2 1970. 2
[15] YanDi,ChenyangguangZhang,PengyuanWang,Guangyao [33] Long Jin, Justin Lazarow, and Zhuowen Tu. Introspective
Zhai,RuidaZhang,FabianManhardt,BenjaminBusam,Xi- classificationwithconvolutionalnets. NeurIPS,2017. 2
angyang Ji, and Federico Tombari. Ccd-3dr: Consistent [34] MichaelIJordanandRobertAJacobs.Hierarchicalmixtures
conditioningindiffusionforsingle-image3dreconstruction. ofexpertsandtheemalgorithm. NeuralComputation,6(2):
arXivpreprintarXiv:2308.07837,2023. 2,5,6,7 181‚Äì214,1994. 1
9[35] AngjooKanazawa,ShubhamTulsiani,AlexeiAEfros,and [54] ShentongMo,EnzeXie,RuihangChu,LeweiYao,Lanqing
JitendraMalik. Learningcategory-specificmeshreconstruc- Hong, MatthiasNie√üner, andZhenguoLi. Dit-3d: Explor-
tionfromimagecollections. InECCV,2018. 2 ingplaindiffusiontransformersfor3dshapegeneration. In
[36] AbhishekKar, ShubhamTulsiani, JoaoCarreira, andJiten- NeurIPS,2023. 12
dra Malik. Category-specific object reconstruction from a [55] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela
singleimage. InCVPR,2015. 2 Mishkin,andMarkChen. Point-e: Asystemforgenerating
[37] DiederikPKingmaandMaxWelling. Auto-encodingvaria- 3dpointcloudsfromcomplexprompts. 2022. 2
tionalbayes. InICLR,2014. 2 [56] Maria-Elena Nilsback and Andrew Zisserman. Automated
[38] David C Knill and Whitman Richards. Perception as flowerclassificationoveralargenumberofclasses. InSixth
Bayesianinference. 1996. 1,2 Indian conference on computer vision, graphics & image
[39] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. processing,pages722‚Äì729,2008. 1
Imagenet classification with deep convolutional neural net- [57] Jeong Joon Park, Peter Florence, Julian Straub, Richard
works. InNeurIPS,2012. 1,2 Newcombe,andStevenLovegrove. Deepsdf:Learningcon-
[40] John D Lafferty, Andrew McCallum, and Fernando CN tinuous signed distance functions for shape representation.
Pereira. Conditionalrandomfields:Probabilisticmodelsfor InCVPR,2019. 2
segmentingandlabelingsequencedata.InICML,2001.1,3 [58] WilliamPeeblesandSainingXie. Scalablediffusionmodels
[41] StanZLi.Markovrandomfieldmodelinginimageanalysis. withtransformers. InICCV,2023. 12
2009. 1 [59] BenPoole,AjayJain,JonathanTBarron,andBenMilden-
[42] Chen-HsuanLin,JunGao,LumingTang,TowakiTakikawa, hall. Dreamfusion: Text-to-3dusing2ddiffusion. InICLR,
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, 2023. 2
Ming-YuLiu,andTsung-YiLin. Magic3d: High-resolution [60] CharlesRQi,HaoSu,KaichunMo,andLeonidasJGuibas.
text-to-3dcontentcreation. InCVPR,2023. 2 Pointnet: Deep learning on point sets for 3d classification
[43] Beyang Liu, Stephen Gould, and Daphne Koller. Single andsegmentation. InCVPR,2017. 2
image depth estimation from predicted semantic labels. In [61] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
CVPR,2010. 1,2 Fasterr-cnn: Towardsreal-timeobjectdetectionwithregion
[44] MinghuaLiu,ChaoXu,HaianJin,LinghaoChen,Zexiang proposalnetworks. InNeurIPS,2015. 1,2
Xu,HaoSu,etal.One-2-3-45:Anysingleimageto3dmesh [62] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
in45secondswithoutper-shapeoptimization. InNeurIPS, PatrickEsser,andBjo¬®rnOmmer.High-resolutionimagesyn-
2023. 2 thesiswithlatentdiffusionmodels. InCVPR,2022. 2
[45] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok- [63] ShunsukeSaito,ZengHuang,RyotaNatsume,ShigeoMor-
makov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: ishima,AngjooKanazawa,andHaoLi. Pifu: Pixel-aligned
Zero-shotoneimageto3dobject. InICCV,2023. 2 implicitfunctionforhigh-resolutionclothedhumandigitiza-
[46] ZhijianLiu,HaotianTang,YujunLin,andSongHan. Point- tion. InICCV,2019. 2
voxelcnnforefficient3ddeeplearning. InNeurIPS,2019. [64] JaschaSohl-Dickstein, EricWeiss, NiruMaheswaranathan,
2 and Surya Ganguli. Deep unsupervised learning using
[47] ZhijianLiu,HaotianTang,YujunLin,andSongHan. Point- nonequilibriumthermodynamics. InICML,2015. 2,3
voxelcnnforefficient3ddeeplearning. InNeurIPS,2019. [65] YangSong,JaschaSohl-Dickstein,DiederikPKingma,Ab-
12 hishekKumar,StefanoErmon,andBenPoole. Score-based
[48] David G Lowe. Distinctive image features from scale- generative modeling through stochastic differential equa-
invariantkeypoints. IJCV,60:91‚Äì110,2004. 2 tions. InICLR,2020. 2,3
[49] ShitongLuoandWeiHu. Diffusionprobabilisticmodelsfor [66] Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong
3dpointcloudgeneration. InCVPR,2021. 2 Zhang,ChengkaiZhang,TianfanXue,JoshuaBTenenbaum,
[50] Priyanka Mandikal and Venkatesh Babu Radhakrishnan. and William T Freeman. Pix3d: Dataset and methods for
Dense 3d point cloud reconstruction using a deep pyramid single-image3dshapemodeling. InCVPR,2018. 2,5
network. InWACV,2019. 2 [67] Maxim Tatarchenko, Stephan R. Richter, Rene¬¥ Ranftl,
[51] DavidMarr. Vision: Acomputationalinvestigationintothe Zhuwen Li, Vladlen Koltun, and Thomas Brox. What do
humanrepresentationandprocessingofvisualinformation. single-view 3d reconstruction networks learn? In CVPR,
2010. 1 2019. 13
[52] Luke Melas-Kyriazi, Christian Rupprecht, and Andrea [68] ZhuowenTu. Learninggenerativemodelsviadiscriminative
Vedaldi. Pc2: Projection-conditionedpointclouddiffusion approaches. InCVPR,2007. 2
forsingle-image3dreconstruction. InCVPR,2023. 2,3,5, [69] ZhuowenTuandSong-ChunZhu. Imagesegmentationby
6,7 data-drivenmarkovchainmontecarlo. TPAMI,24(5):657‚Äì
[53] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se- 673,2002. 1
bastianNowozin,andAndreasGeiger.Occupancynetworks: [70] Zhuowen Tu, Xiangrong Chen, Alan L Yuille, and Song-
Learning 3d reconstruction in function space. In CVPR, Chun Zhu. Image parsing: Unifying segmentation, detec-
2019. 2 tion,andrecognition. IJCV,63:113‚Äì140,2005. 1,2
10[71] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko- [88] XiangZhang,ZeyuanChen,FangyinWei,andZhuowenTu.
reit,LlionJones,AidanNGomez,≈ÅukaszKaiser,andIllia Uni-3d:Auniversalmodelforpanoptic3dscenereconstruc-
Polosukhin. Attentionisallyouneed. InNeurIPS,2017. 3 tion. InICCV,2023. 2
[72] PaulViolaandMichaelJones. Rapidobjectdetectionusing [89] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape genera-
aboostedcascadeofsimplefeatures. InCVPR,2001. 1 tionandcompletionthroughpoint-voxeldiffusion.InICCV,
[73] ZhengyiWang,ChengLu,YikaiWang,FanBao,Chongxuan 2021. 2,3,5,12
Li,HangSu,andJunZhu.Prolificdreamer:High-fidelityand
diversetext-to-3dgenerationwithvariationalscoredistilla-
tion. InNeurIPS,2023. 2
[74] MarkusWeber,MaxWelling,andPietroPerona. Unsuper-
vised learning of models for recognition. LectureNotes in
ComputerScience,2000. 1,2
[75] MaxWellingandYeeWTeh.Bayesianlearningviastochas-
ticgradientlangevindynamics. InICML,2011. 1,3
[76] AndrewPWitkin. Recoveringsurfaceshapeandorientation
fromtexture. ArtificialIntelligence,17(1-3):17‚Äì45,1981. 2
[77] OliverWoodford,PhilipTorr,IanReid,andAndrewFitzgib-
bon. Global stereo reconstruction under second-order
smoothnesspriors. TPAMI,31(12):2115‚Äì2128,2009. 1,2
[78] JiajunWu,ChengkaiZhang,TianfanXue,BillFreeman,and
Josh Tenenbaum. Learning a probabilistic latent space of
object shapes via 3d generative-adversarial modeling. In
NeurIPS,2016. 2
[79] TongWu, LiangPan, JunzheZhang, TaiWang, ZiweiLiu,
andDahuaLin. Density-awarechamferdistanceasacom-
prehensivemetricforpointcloudcompletion. InNeurIPS,
2021. 13
[80] YuefanWu, ZeyuanChen, ShaoweiLiu, ZhongzhengRen,
andShenlongWang. CASA:Category-agnosticskeletalan-
imalreconstruction. InNeurIPS,2022. 2
[81] ZhirongWu, ShuranSong, AdityaKhosla, FisherYu, Lin-
guang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d
shapenets: Adeeprepresentationforvolumetricshapes. In
CVPR,2015. 2
[82] Haozhe Xie, Hongxun Yao, Xiaoshuai Sun, Shangchen
Zhou, and Shengping Zhang. Pix2vox: Context-aware 3d
reconstructionfromsingleandmulti-viewimages. InICCV,
2019. 2
[83] Haozhe Xie, Hongxun Yao, Shengping Zhang, Shangchen
Zhou, and Wenxiu Sun. Pix2vox++: Multi-scale context-
aware3dobjectreconstructionfromsingleandmultipleim-
ages. IJCV,128(12):2919‚Äì2935,2020. 5
[84] GuandaoYang,XunHuang,ZekunHao,Ming-YuLiu,Serge
Belongie,andBharathHariharan. Pointflow:3dpointcloud
generation with continuous normalizing flows. In ICCV,
2019. 2
[85] Alan Yuille and Daniel Kersten. Vision as bayesian infer-
ence: analysisbysynthesis? TrendsinCognitiveSciences,
10(7):301‚Äì308,2006. 1
[86] XiaohuiZeng,ArashVahdat,FrancisWilliams,ZanGojcic,
Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent
pointdiffusionmodelsfor3dshapegeneration. InNeurIPS,
2022. 2
[87] LvminZhang, AnyiRao, andManeeshAgrawala. Adding
conditional control to text-to-image diffusion models. In
ICCV,2023. 5
11Appendix
A.TrainingandInferenceDetails Input Image Baseline BDM-B (PVD) BDM-B (DiT-3D) Ground Truth
WetrainthegenerativemodelPVD[89]withabatchsize
of 128 for 10k iterations, and adopt Adam optimizer with
learningrate2√ó10‚àí4. Forairplane,wesetŒ≤ =10‚àí5and
0
Œ≤ = 0.008. For other categories, we set Œ≤ = 10‚àí4 and
T 0
Œ≤ =0.01.
T
In terms of reconstruction model PC2/ CCD-3DR, we
trainitwithabatchsizeof16for100kiterations,andadopt
Adamoptimizerwithalearningrateincreasingfrom10‚àí5
to 10‚àí3 in the first 2k iterations then decaying to 0 in the
remaining iterations. For all the categories, we set Œ≤ =
0
10‚àí5andŒ≤ =0.008.
T
For the training of BDM-merging, we adopt a similar
strategyasthereconstructionmodel,exceptthatwereduce
thetotaliterationsto20kandscalethelearningratesched-
uleaccordingly.
During inference, we set the number of denoising steps
as1000. Wedividethedenoisingprocessintothreedistinct
stages, i.e. early (timesteps 1000‚Äì872), middle (timesteps
872‚Äì128), and late (timesteps 128‚Äì0). We conduct our
Bayesian denoising steps in the early and late stages. To
be more specific, every 32 timesteps, a Bayesian denois- FigureA.1.VisualizationoftakingDiT-3Daspriorcomparedwith
PVDonchairs.
ingstepisexecutedforadurationof16timesteps. Subse-
quently, we forward a standard reconstruction process for purposes, we take 10% of the chairs on ShapeNet as the
16timesteps,followedbyanotherBayesiandenoisingstep. trainingdataforthereconstructionmodel(CCD-3DR).As
demonstratedinTab.A.2,BDMbringsconsistentimprove-
B.ExtendedObjectCategories mentregardlessofthepriormodelutilized. Also,weshow
some qualitative visualizations in Fig. A.1. The results il-
Leveraging PVD as our prior model, we follow its set-
lustratetheeffectivenessofourBDMacrossdifferentgen-
tingsandadoptofficialpre-trainedweights,whichareonly
erativediffusionpriors.
on three categories. Each category is trained with differ-
enthyperparameters. Tofurtherillustratetheeffectiveness CD‚Üì F1‚Üë
ofBDM,weaddtwonewcategoriesofShapeNet-R2N2in
CCD-3DR(baseline) 89.79 0.418
Tab.A.1. +PVD[89] 79.26 0.441
+DiT-3D[54] 80.77 0.431
Sofa Table
Method 10% 50% 100% 10% 50% 100% Table A.2. Reconstruction results of taking DiT-3D as prior
CD‚Üì F1‚Üë CD‚Üì F1‚Üë CD‚Üì F1‚Üë CD‚Üì F1‚Üë CD‚Üì F1‚Üë CD‚Üì F1‚Üë compared with PVD, evaluated with Chamfer Distance and F-
CCD-3DR 63.20 0.444 47.83 0.482 43.16 0.501 79.41 0.523 77.51 0.520 67.13 0.538
BDM-merging 62.29 0.460 45.18 0.500 41.43 0.517 78.25 0.535 75.94 0.538 65.24 0.560 Score@0.01.
BDM-blending 61.54 0.471 44.31 0.516 41.94 0.520 74.18 0.547 73.46 0.526 64.56 0.557
Table A.1. Results on two additional categories of ShapeNet- D.DifferentGaussianNoises
R2N2,i.e.sofaandtable.
C.AlternativePriorModel
To validate the robustness of our proposed BDM, we Image Baseline BDM w/ different Gaussian noise inputs
explore its performance with alternative generative priors. FigureA.2.PredictionsoverdifferentinitialGaussiannoise.
Specifically, we replace the PVD-based generative diffu-
sionmodel[89]withDiT-3D[54],anextensionofDiT[58], OnekeyadvantageofBayesianmethodsisthatitallows
which uniquely applies the denoising process to voxelized to obtain distributions over predicted outputs, allowing to
pointclouds. UnlikethePVCNNarchitecture[47],DiT-3D measure prediction uncertainties. The uncertainties in dif-
leveragesaTransformer-basedframework,andthereforewe fusion model originate from initial Gaussian noise. To in-
only experiment on BDM-blending. For proof-of-concept vestigate the correlation between the outputs and different
12initialGaussiannoiseX ,weevaluateBDM-blendingwith
T
10 different initial noise inputs, and report mean and vari-
ance in Tab. A.3. In addition, we visualize a chair sam-
ple in Fig. A.2, featuring consistent valid reconstructions
across different noise inputs despite minor shape differ-
ences. BDM transforms the vertical chair-back to a tilted
andcurvedone,betteralignedwiththe2Dimage.
CD‚Üì F1‚Üë
CCD-3DR 89.79 0.418
XTablate 79.61(0.048) 0.441(2.2e-5)
Table A.3. Mean and variance w.r.t. different initial Gaussian
noise.
E.DetailsofHumanEvaluation
For each generated 3D point cloud, we render multiple
imagesofit,asshowninFig.A.3.
multi-view + fine-grained rendering
FigureA.3.Multi-viewrendering.
Fromtheevaluationresults,wecanseeourBDM-Mand
BDM-B both outperform CCD-3DR, while the quality of
BDM-MismorefavoredthanBDM-B.However,thissupe-
riorityofBDM-MisnotevidentfromtheCDandF1.
As discussed in [67, 79], CD is susceptible to mis-
matchedlocaldensityandF1doesnotfullyaddresssuchis-
sue.Thesemetricsmaynotalignwithhumanpreference,as
showninFig.A.4. Therefore,accordingtothehumaneval-
uation, BDM-M yields more visually appealing outcomes
whereas blending has stronger quantitative results, which
confirmstheeffectivenessofourBDM-Mapproach.
Baseline BDM-blending BDM-merging
Image Ground Truth
CD 90.83 / F1 0.36 CD 52.29 / F1 0.46 CD 62.28 / F1 0.39
FigureA.4.Quantitativeresultsvsactualvisualization.
13