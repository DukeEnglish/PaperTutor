[
    {
        "title": "Bayesian Diffusion Models for 3D Shape Reconstruction",
        "authors": "Haiyang XuYu LeiZeyuan ChenXiang ZhangYue ZhaoYilin WangZhuowen Tu",
        "links": "http://arxiv.org/abs/2403.06973v1",
        "entry_id": "http://arxiv.org/abs/2403.06973v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06973v1",
        "summary": "We present Bayesian Diffusion Models (BDM), a prediction algorithm that\nperforms effective Bayesian inference by tightly coupling the top-down (prior)\ninformation with the bottom-up (data-driven) procedure via joint diffusion\nprocesses. We show the effectiveness of BDM on the 3D shape reconstruction\ntask. Compared to prototypical deep learning data-driven approaches trained on\npaired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM\nbrings in rich prior information from standalone labels (e.g. point clouds) to\nimprove the bottom-up 3D reconstruction. As opposed to the standard Bayesian\nframeworks where explicit prior and likelihood are required for the inference,\nBDM performs seamless information fusion via coupled diffusion processes with\nlearned gradient computation networks. The specialty of our BDM lies in its\ncapability to engage the active and effective information exchange and fusion\nof the top-down and bottom-up processes where each itself is a diffusion\nprocess. We demonstrate state-of-the-art results on both synthetic and\nreal-world benchmarks for 3D shape reconstruction.",
        "updated": "2024-03-11 17:55:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06973v1"
    },
    {
        "title": "A representation-learning game for classes of prediction tasks",
        "authors": "Neria UzanNir Weinberger",
        "links": "http://arxiv.org/abs/2403.06971v1",
        "entry_id": "http://arxiv.org/abs/2403.06971v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06971v1",
        "summary": "We propose a game-based formulation for learning dimensionality-reducing\nrepresentations of feature vectors, when only a prior knowledge on future\nprediction tasks is available. In this game, the first player chooses a\nrepresentation, and then the second player adversarially chooses a prediction\ntask from a given class, representing the prior knowledge. The first player\naims is to minimize, and the second player to maximize, the regret: The minimal\nprediction loss using the representation, compared to the same loss using the\noriginal features. For the canonical setting in which the representation, the\nresponse to predict and the predictors are all linear functions, and under the\nmean squared error loss function, we derive the theoretically optimal\nrepresentation in pure strategies, which shows the effectiveness of the prior\nknowledge, and the optimal regret in mixed strategies, which shows the\nusefulness of randomizing the representation. For general representations and\nloss functions, we propose an efficient algorithm to optimize a randomized\nrepresentation. The algorithm only requires the gradients of the loss function,\nand is based on incrementally adding a representation rule to a mixture of such\nrules.",
        "updated": "2024-03-11 17:54:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06971v1"
    },
    {
        "title": "Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts",
        "authors": "Onur CelikAleksandar TaranovicGerhard Neumann",
        "links": "http://arxiv.org/abs/2403.06966v1",
        "entry_id": "http://arxiv.org/abs/2403.06966v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06966v1",
        "summary": "Reinforcement learning (RL) is a powerful approach for acquiring a\ngood-performing policy. However, learning diverse skills is challenging in RL\ndue to the commonly used Gaussian policy parameterization. We propose\n\\textbf{Di}verse \\textbf{Skil}l \\textbf{L}earning (Di-SkilL), an RL method for\nlearning diverse skills using Mixture of Experts, where each expert formalizes\na skill as a contextual motion primitive. Di-SkilL optimizes each expert and\nits associate context distribution to a maximum entropy objective that\nincentivizes learning diverse skills in similar contexts. The per-expert\ncontext distribution enables automatic curricula learning, allowing each expert\nto focus on its best-performing sub-region of the context space. To overcome\nhard discontinuities and multi-modalities without any prior knowledge of the\nenvironment's unknown context probability space, we leverage energy-based\nmodels to represent the per-expert context distributions and demonstrate how we\ncan efficiently train them using the standard policy gradient objective. We\nshow on challenging robot simulation tasks that Di-SkilL can learn diverse and\nperformant skills.",
        "updated": "2024-03-11 17:49:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06966v1"
    },
    {
        "title": "The pitfalls of next-token prediction",
        "authors": "Gregor BachmannVaishnavh Nagarajan",
        "links": "http://arxiv.org/abs/2403.06963v1",
        "entry_id": "http://arxiv.org/abs/2403.06963v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06963v1",
        "summary": "Can a mere next-token predictor faithfully model human intelligence? We\ncrystallize this intuitive concern, which is fragmented in the literature. As a\nstarting point, we argue that the two often-conflated phases of next-token\nprediction -- autoregressive inference and teacher-forced training -- must be\ntreated distinctly. The popular criticism that errors can compound during\nautoregressive inference, crucially assumes that teacher-forcing has learned an\naccurate next-token predictor. This assumption sidesteps a more deep-rooted\nproblem we expose: in certain classes of tasks, teacher-forcing can simply fail\nto learn an accurate next-token predictor in the first place. We describe a\ngeneral mechanism of how teacher-forcing can fail, and design a minimal\nplanning task where both the Transformer and the Mamba architecture empirically\nfail in that manner -- remarkably, despite the task being straightforward to\nlearn. We provide preliminary evidence that this failure can be resolved when\ntraining to predict multiple tokens in advance. We hope this finding can ground\nfuture debates and inspire explorations beyond the next-token prediction\nparadigm. We make our code available under\nhttps://github.com/gregorbachmann/Next-Token-Failures",
        "updated": "2024-03-11 17:47:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06963v1"
    },
    {
        "title": "Accurate Crystal Structure Prediction of New 2D Hybrid Organic Inorganic Perovskites",
        "authors": "Nima KarimitariWilliam J. BaldwinEvan W. MullerZachary J. L. BareW. Joshua KennedyGábor CsányiChristopher Sutton",
        "links": "http://arxiv.org/abs/2403.06955v1",
        "entry_id": "http://arxiv.org/abs/2403.06955v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06955v1",
        "summary": "Low dimensional hybrid organic-inorganic perovskites (HOIPs) represent a\npromising class of electronically active materials for both light absorption\nand emission. The design space of HOIPs is extremely large, since a diverse\nspace of organic cations can be combined with different inorganic frameworks.\nThis immense design space allows for tunable electronic and mechanical\nproperties, but also necessitates the development of new tools for in silico\nhigh throughput analysis of candidate structures. In this work, we present an\naccurate, efficient, transferable and widely applicable machine learning\ninteratomic potential (MLIP) for predicting the structure of new 2D HOIPs.\nUsing the MACE architecture, an MLIP is trained on 86 diverse experimentally\nreported HOIP structures. The model is tested on 73 unseen perovskite\ncompositions, and achieves chemical accuracy with respect to the reference\nelectronic structure method. Our model is then combined with a simple random\nstructure search algorithm to predict the structure of hypothetical HOIPs given\nonly the proposed composition. Success is demonstrated by correctly and\nreliably recovering the crystal structure of a set of experimentally known 2D\nperovskites. Such a random structure search is impossible with ab initio\nmethods due to the associated computational cost, but is relatively inexpensive\nwith the MACE potential. Finally, the procedure is used to predict the\nstructure formed by a new organic cation with no previously known corresponding\nperovskite. Laboratory synthesis of the new hybrid perovskite confirms the\naccuracy of our prediction. This capability, applied at scale, enables\nefficient screening of thousands of combinations of organic cations and\ninorganic layers.",
        "updated": "2024-03-11 17:39:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06955v1"
    }
]