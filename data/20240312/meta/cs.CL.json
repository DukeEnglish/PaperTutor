[
    {
        "title": "MRL Parsing Without Tears: The Case of Hebrew",
        "authors": "Shaltiel ShmidmanAvi ShmidmanMoshe KoppelReut Tsarfaty",
        "links": "http://arxiv.org/abs/2403.06970v1",
        "entry_id": "http://arxiv.org/abs/2403.06970v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06970v1",
        "summary": "Syntactic parsing remains a critical tool for relation extraction and\ninformation extraction, especially in resource-scarce languages where LLMs are\nlacking. Yet in morphologically rich languages (MRLs), where parsers need to\nidentify multiple lexical units in each token, existing systems suffer in\nlatency and setup complexity. Some use a pipeline to peel away the layers:\nfirst segmentation, then morphology tagging, and then syntax parsing; however,\nerrors in earlier layers are then propagated forward. Others use a joint\narchitecture to evaluate all permutations at once; while this improves\naccuracy, it is notoriously slow. In contrast, and taking Hebrew as a test\ncase, we present a new \"flipped pipeline\": decisions are made directly on the\nwhole-token units by expert classifiers, each one dedicated to one specific\ntask. The classifiers are independent of one another, and only at the end do we\nsynthesize their predictions. This blazingly fast approach sets a new SOTA in\nHebrew POS tagging and dependency parsing, while also reaching near-SOTA\nperformance on other Hebrew NLP tasks. Because our architecture does not rely\non any language-specific resources, it can serve as a model to develop similar\nparsers for other MRLs.",
        "updated": "2024-03-11 17:54:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06970v1"
    },
    {
        "title": "Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic Phenomena",
        "authors": "Leonie WeissweilerAbdullatif KöksalHinrich Schütze",
        "links": "http://arxiv.org/abs/2403.06965v1",
        "entry_id": "http://arxiv.org/abs/2403.06965v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06965v1",
        "summary": "Argument Structure Constructions (ASCs) are one of the most well-studied\nconstruction groups, providing a unique opportunity to demonstrate the\nusefulness of Construction Grammar (CxG). For example, the caused-motion\nconstruction (CMC, ``She sneezed the foam off her cappuccino'') demonstrates\nthat constructions must carry meaning, otherwise the fact that ``sneeze'' in\nthis context causes movement cannot be explained. We form the hypothesis that\nthis remains challenging even for state-of-the-art Large Language Models\n(LLMs), for which we devise a test based on substituting the verb with a\nprototypical motion verb. To be able to perform this test at statistically\nsignificant scale, in the absence of adequate CxG corpora, we develop a novel\npipeline of NLP-assisted collection of linguistically annotated text. We show\nhow dependency parsing and GPT-3.5 can be used to significantly reduce\nannotation cost and thus enable the annotation of rare phenomena at scale. We\nthen evaluate GPT, Gemini, Llama2 and Mistral models for their understanding of\nthe CMC using the newly collected corpus. We find that all models struggle with\nunderstanding the motion component that the CMC adds to a sentence.",
        "updated": "2024-03-11 17:47:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06965v1"
    },
    {
        "title": "The pitfalls of next-token prediction",
        "authors": "Gregor BachmannVaishnavh Nagarajan",
        "links": "http://arxiv.org/abs/2403.06963v1",
        "entry_id": "http://arxiv.org/abs/2403.06963v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06963v1",
        "summary": "Can a mere next-token predictor faithfully model human intelligence? We\ncrystallize this intuitive concern, which is fragmented in the literature. As a\nstarting point, we argue that the two often-conflated phases of next-token\nprediction -- autoregressive inference and teacher-forced training -- must be\ntreated distinctly. The popular criticism that errors can compound during\nautoregressive inference, crucially assumes that teacher-forcing has learned an\naccurate next-token predictor. This assumption sidesteps a more deep-rooted\nproblem we expose: in certain classes of tasks, teacher-forcing can simply fail\nto learn an accurate next-token predictor in the first place. We describe a\ngeneral mechanism of how teacher-forcing can fail, and design a minimal\nplanning task where both the Transformer and the Mamba architecture empirically\nfail in that manner -- remarkably, despite the task being straightforward to\nlearn. We provide preliminary evidence that this failure can be resolved when\ntraining to predict multiple tokens in advance. We hope this finding can ground\nfuture debates and inspire explorations beyond the next-token prediction\nparadigm. We make our code available under\nhttps://github.com/gregorbachmann/Next-Token-Failures",
        "updated": "2024-03-11 17:47:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06963v1"
    },
    {
        "title": "SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data",
        "authors": "Jialu LiJaemin ChoYi-Lin SungJaehong YoonMohit Bansal",
        "links": "http://arxiv.org/abs/2403.06952v1",
        "entry_id": "http://arxiv.org/abs/2403.06952v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06952v1",
        "summary": "Recent text-to-image (T2I) generation models have demonstrated impressive\ncapabilities in creating images from text descriptions. However, these T2I\ngeneration models often fall short of generating images that precisely match\nthe details of the text inputs, such as incorrect spatial relationship or\nmissing objects. In this paper, we introduce SELMA: Skill-Specific Expert\nLearning and Merging with Auto-Generated Data, a novel paradigm to improve the\nfaithfulness of T2I models by fine-tuning models on automatically generated,\nmulti-skill image-text datasets, with skill-specific expert learning and\nmerging. First, SELMA leverages an LLM's in-context learning capability to\ngenerate multiple datasets of text prompts that can teach different skills, and\nthen generates the images with a T2I model based on the prompts. Next, SELMA\nadapts the T2I model to the new skills by learning multiple single-skill LoRA\n(low-rank adaptation) experts followed by expert merging. Our independent\nexpert fine-tuning specializes multiple models for different skills, and expert\nmerging helps build a joint multi-skill T2I model that can generate faithful\nimages given diverse text prompts, while mitigating the knowledge conflict from\ndifferent datasets. We empirically demonstrate that SELMA significantly\nimproves the semantic alignment and text faithfulness of state-of-the-art T2I\ndiffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human\npreference metrics (PickScore, ImageReward, and HPS), as well as human\nevaluation. Moreover, fine-tuning with image-text pairs auto-collected via\nSELMA shows comparable performance to fine-tuning with ground truth data.\nLastly, we show that fine-tuning with images from a weaker T2I model can help\nimprove the generation quality of a stronger T2I model, suggesting promising\nweak-to-strong generalization in T2I models.",
        "updated": "2024-03-11 17:35:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06952v1"
    },
    {
        "title": "Materials science in the era of large language models: a perspective",
        "authors": "Ge LeiRonan DochertySamuel J. Cooper",
        "links": "http://arxiv.org/abs/2403.06949v1",
        "entry_id": "http://arxiv.org/abs/2403.06949v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06949v1",
        "summary": "Large Language Models (LLMs) have garnered considerable interest due to their\nimpressive natural language capabilities, which in conjunction with various\nemergent properties make them versatile tools in workflows ranging from complex\ncode generation to heuristic finding for combinatorial problems. In this paper\nwe offer a perspective on their applicability to materials science research,\narguing their ability to handle ambiguous requirements across a range of tasks\nand disciplines mean they could be a powerful tool to aid researchers. We\nqualitatively examine basic LLM theory, connecting it to relevant properties\nand techniques in the literature before providing two case studies that\ndemonstrate their use in task automation and knowledge extraction at-scale. At\ntheir current stage of development, we argue LLMs should be viewed less as\noracles of novel insight, and more as tireless workers that can accelerate and\nunify exploration across domains. It is our hope that this paper can\nfamiliarise material science researchers with the concepts needed to leverage\nthese tools in their own research.",
        "updated": "2024-03-11 17:34:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06949v1"
    }
]