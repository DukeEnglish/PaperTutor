[
    {
        "title": "The pitfalls of next-token prediction",
        "authors": "Gregor BachmannVaishnavh Nagarajan",
        "links": "http://arxiv.org/abs/2403.06963v1",
        "entry_id": "http://arxiv.org/abs/2403.06963v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06963v1",
        "summary": "Can a mere next-token predictor faithfully model human intelligence? We\ncrystallize this intuitive concern, which is fragmented in the literature. As a\nstarting point, we argue that the two often-conflated phases of next-token\nprediction -- autoregressive inference and teacher-forced training -- must be\ntreated distinctly. The popular criticism that errors can compound during\nautoregressive inference, crucially assumes that teacher-forcing has learned an\naccurate next-token predictor. This assumption sidesteps a more deep-rooted\nproblem we expose: in certain classes of tasks, teacher-forcing can simply fail\nto learn an accurate next-token predictor in the first place. We describe a\ngeneral mechanism of how teacher-forcing can fail, and design a minimal\nplanning task where both the Transformer and the Mamba architecture empirically\nfail in that manner -- remarkably, despite the task being straightforward to\nlearn. We provide preliminary evidence that this failure can be resolved when\ntraining to predict multiple tokens in advance. We hope this finding can ground\nfuture debates and inspire explorations beyond the next-token prediction\nparadigm. We make our code available under\nhttps://github.com/gregorbachmann/Next-Token-Failures",
        "updated": "2024-03-11 17:47:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06963v1"
    },
    {
        "title": "SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data",
        "authors": "Jialu LiJaemin ChoYi-Lin SungJaehong YoonMohit Bansal",
        "links": "http://arxiv.org/abs/2403.06952v1",
        "entry_id": "http://arxiv.org/abs/2403.06952v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06952v1",
        "summary": "Recent text-to-image (T2I) generation models have demonstrated impressive\ncapabilities in creating images from text descriptions. However, these T2I\ngeneration models often fall short of generating images that precisely match\nthe details of the text inputs, such as incorrect spatial relationship or\nmissing objects. In this paper, we introduce SELMA: Skill-Specific Expert\nLearning and Merging with Auto-Generated Data, a novel paradigm to improve the\nfaithfulness of T2I models by fine-tuning models on automatically generated,\nmulti-skill image-text datasets, with skill-specific expert learning and\nmerging. First, SELMA leverages an LLM's in-context learning capability to\ngenerate multiple datasets of text prompts that can teach different skills, and\nthen generates the images with a T2I model based on the prompts. Next, SELMA\nadapts the T2I model to the new skills by learning multiple single-skill LoRA\n(low-rank adaptation) experts followed by expert merging. Our independent\nexpert fine-tuning specializes multiple models for different skills, and expert\nmerging helps build a joint multi-skill T2I model that can generate faithful\nimages given diverse text prompts, while mitigating the knowledge conflict from\ndifferent datasets. We empirically demonstrate that SELMA significantly\nimproves the semantic alignment and text faithfulness of state-of-the-art T2I\ndiffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human\npreference metrics (PickScore, ImageReward, and HPS), as well as human\nevaluation. Moreover, fine-tuning with image-text pairs auto-collected via\nSELMA shows comparable performance to fine-tuning with ground truth data.\nLastly, we show that fine-tuning with images from a weaker T2I model can help\nimprove the generation quality of a stronger T2I model, suggesting promising\nweak-to-strong generalization in T2I models.",
        "updated": "2024-03-11 17:35:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06952v1"
    },
    {
        "title": "Counterfactual Reasoning with Knowledge Graph Embeddings",
        "authors": "Lena ZellingerAndreas StephanBenjamin Roth",
        "links": "http://arxiv.org/abs/2403.06936v1",
        "entry_id": "http://arxiv.org/abs/2403.06936v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06936v1",
        "summary": "Knowledge graph embeddings (KGEs) were originally developed to infer true but\nmissing facts in incomplete knowledge repositories. In this paper, we link\nknowledge graph completion and counterfactual reasoning via our new task CFKGR.\nWe model the original world state as a knowledge graph, hypothetical scenarios\nas edges added to the graph, and plausible changes to the graph as inferences\nfrom logical rules. We create corresponding benchmark datasets, which contain\ndiverse hypothetical scenarios with plausible changes to the original knowledge\ngraph and facts that should be retained. We develop COULDD, a general method\nfor adapting existing knowledge graph embeddings given a hypothetical premise,\nand evaluate it on our benchmark. Our results indicate that KGEs learn patterns\nin the graph without explicit training. We further observe that KGEs adapted\nwith COULDD solidly detect plausible counterfactual changes to the graph that\nfollow these patterns. An evaluation on human-annotated data reveals that KGEs\nadapted with COULDD are mostly unable to recognize changes to the graph that do\nnot follow learned inference rules. In contrast, ChatGPT mostly outperforms\nKGEs in detecting plausible changes to the graph but has poor knowledge\nretention. In summary, CFKGR connects two previously distinct areas, namely KG\ncompletion and counterfactual reasoning.",
        "updated": "2024-03-11 17:21:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06936v1"
    },
    {
        "title": "Simplicity Bias of Transformers to Learn Low Sensitivity Functions",
        "authors": "Bhavya VasudevaDeqing FuTianyi ZhouElliott KauYouqi HuangVatsal Sharan",
        "links": "http://arxiv.org/abs/2403.06925v1",
        "entry_id": "http://arxiv.org/abs/2403.06925v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06925v1",
        "summary": "Transformers achieve state-of-the-art accuracy and robustness across many\ntasks, but an understanding of the inductive biases that they have and how\nthose biases are different from other neural network architectures remains\nelusive. Various neural network architectures such as fully connected networks\nhave been found to have a simplicity bias towards simple functions of the data;\none version of this simplicity bias is a spectral bias to learn simple\nfunctions in the Fourier space. In this work, we identify the notion of\nsensitivity of the model to random changes in the input as a notion of\nsimplicity bias which provides a unified metric to explain the simplicity and\nspectral bias of transformers across different data modalities. We show that\ntransformers have lower sensitivity than alternative architectures, such as\nLSTMs, MLPs and CNNs, across both vision and language tasks. We also show that\nlow-sensitivity bias correlates with improved robustness; furthermore, it can\nalso be used as an efficient intervention to further improve the robustness of\ntransformers.",
        "updated": "2024-03-11 17:12:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06925v1"
    },
    {
        "title": "MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning",
        "authors": "Yichuan LiXiyao MaSixing LuKyumin LeeXiaohu LiuChenlei Guo",
        "links": "http://arxiv.org/abs/2403.06914v1",
        "entry_id": "http://arxiv.org/abs/2403.06914v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06914v1",
        "summary": "Large Language models (LLMs) have demonstrated impressive in-context learning\n(ICL) capabilities, where a LLM makes predictions for a given test input\ntogether with a few input-output pairs (demonstrations). Nevertheless, the\ninclusion of demonstrations leads to a quadratic increase in the computational\noverhead of the self-attention mechanism. Existing solutions attempt to distill\nlengthy demonstrations into compact vectors. However, they often require\ntask-specific retraining or compromise LLM's in-context learning performance.\nTo mitigate these challenges, we present Meta dEmonstratioN Distillation\n(MEND), where a language model learns to distill any lengthy demonstrations\ninto vectors without retraining for a new downstream task. We exploit the\nknowledge distillation to enhance alignment between MEND and LLM, achieving\nboth efficiency and effectiveness simultaneously. MEND is endowed with the\nmeta-knowledge of distilling demonstrations through a two-stage training\nprocess, which includes meta-distillation pretraining and fine-tuning.\nComprehensive evaluations across seven diverse ICL task partitions using\ndecoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess. It not\nonly matches but often outperforms the Vanilla ICL as well as other\nstate-of-the-art distillation models, while significantly reducing the\ncomputational demands. This innovation promises enhanced scalability and\nefficiency for the practical deployment of large language models",
        "updated": "2024-03-11 17:03:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06914v1"
    }
]