[
    {
        "title": "Grid Monitoring and Protection with Continuous Point-on-Wave Measurements and Generative AI",
        "authors": "Lang TongXinyi WangQing Zhao",
        "links": "http://arxiv.org/abs/2403.06942v1",
        "entry_id": "http://arxiv.org/abs/2403.06942v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06942v1",
        "summary": "Purpose This article presents a case for a next-generation grid monitoring\nand control system, leveraging recent advances in generative artificial\nintelligence (AI), machine learning, and statistical inference. Advancing\nbeyond earlier generations of wide-area monitoring systems built upon\nsupervisory control and data acquisition (SCADA) and synchrophasor\ntechnologies, we argue for a monitoring and control framework based on the\nstreaming of continuous point-on-wave (CPOW) measurements with AI-powered data\ncompression and fault detection.\n  Methods and Results: The architecture of the proposed design originates from\nthe Wiener-Kallianpur innovation representation of a random process that\ntransforms causally a stationary random process into an innovation sequence\nwith independent and identically distributed random variables. This work\npresents a generative AI approach that (i) learns an innovation autoencoder\nthat extracts innovation sequence from CPOW time series, (ii) compresses the\nCPOW streaming data with innovation autoencoder and subband coding, and (iii)\ndetects unknown faults and novel trends via nonparametric sequential hypothesis\ntesting.\n  Conclusion: This work argues that conventional monitoring using SCADA and\nphasor measurement unit (PMU) technologies is ill-suited for a future grid with\ndeep penetration of inverter-based renewable generations and distributed energy\nresources. A monitoring system based on CPOW data streaming and AI data\nanalytics should be the basic building blocks for situational awareness of a\nhighly dynamic future grid.",
        "updated": "2024-03-11 17:28:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06942v1"
    },
    {
        "title": "Simplicity Bias of Transformers to Learn Low Sensitivity Functions",
        "authors": "Bhavya VasudevaDeqing FuTianyi ZhouElliott KauYouqi HuangVatsal Sharan",
        "links": "http://arxiv.org/abs/2403.06925v1",
        "entry_id": "http://arxiv.org/abs/2403.06925v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06925v1",
        "summary": "Transformers achieve state-of-the-art accuracy and robustness across many\ntasks, but an understanding of the inductive biases that they have and how\nthose biases are different from other neural network architectures remains\nelusive. Various neural network architectures such as fully connected networks\nhave been found to have a simplicity bias towards simple functions of the data;\none version of this simplicity bias is a spectral bias to learn simple\nfunctions in the Fourier space. In this work, we identify the notion of\nsensitivity of the model to random changes in the input as a notion of\nsimplicity bias which provides a unified metric to explain the simplicity and\nspectral bias of transformers across different data modalities. We show that\ntransformers have lower sensitivity than alternative architectures, such as\nLSTMs, MLPs and CNNs, across both vision and language tasks. We also show that\nlow-sensitivity bias correlates with improved robustness; furthermore, it can\nalso be used as an efficient intervention to further improve the robustness of\ntransformers.",
        "updated": "2024-03-11 17:12:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06925v1"
    },
    {
        "title": "Benign overfitting in leaky ReLU networks with moderate input dimension",
        "authors": "Kedar KarhadkarErin GeorgeMichael MurrayGuido MontúfarDeanna Needell",
        "links": "http://arxiv.org/abs/2403.06903v1",
        "entry_id": "http://arxiv.org/abs/2403.06903v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06903v1",
        "summary": "The problem of benign overfitting asks whether it is possible for a model to\nperfectly fit noisy training data and still generalize well. We study benign\noverfitting in two-layer leaky ReLU networks trained with the hinge loss on a\nbinary classification task. We consider input data which can be decomposed into\nthe sum of a common signal and a random noise component, which lie on subspaces\northogonal to one another. We characterize conditions on the signal to noise\nratio (SNR) of the model parameters giving rise to benign versus non-benign, or\nharmful, overfitting: in particular, if the SNR is high then benign overfitting\noccurs, conversely if the SNR is low then harmful overfitting occurs. We\nattribute both benign and non-benign overfitting to an approximate margin\nmaximization property and show that leaky ReLU networks trained on hinge loss\nwith Gradient Descent (GD) satisfy this property. In contrast to prior work we\ndo not require near orthogonality conditions on the training data: notably, for\ninput dimension $d$ and training sample size $n$, while prior work shows\nasymptotically optimal error when $d = \\Omega(n^2 \\log n)$, here we require\nonly $d = \\Omega\\left(n \\log \\frac{1}{\\epsilon}\\right)$ to obtain error within\n$\\epsilon$ of optimal.",
        "updated": "2024-03-11 16:56:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06903v1"
    },
    {
        "title": "On the Generalization Ability of Unsupervised Pretraining",
        "authors": "Yuyang DengJunyuan HongJiayu ZhouMehrdad Mahdavi",
        "links": "http://arxiv.org/abs/2403.06871v1",
        "entry_id": "http://arxiv.org/abs/2403.06871v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06871v1",
        "summary": "Recent advances in unsupervised learning have shown that unsupervised\npre-training, followed by fine-tuning, can improve model generalization.\nHowever, a rigorous understanding of how the representation function learned on\nan unlabeled dataset affects the generalization of the fine-tuned model is\nlacking. Existing theoretical research does not adequately account for the\nheterogeneity of the distribution and tasks in pre-training and fine-tuning\nstage. To bridge this gap, this paper introduces a novel theoretical framework\nthat illuminates the critical factor influencing the transferability of\nknowledge acquired during unsupervised pre-training to the subsequent\nfine-tuning phase, ultimately affecting the generalization capabilities of the\nfine-tuned model on downstream tasks. We apply our theoretical framework to\nanalyze generalization bound of two distinct scenarios: Context Encoder\npre-training with deep neural networks and Masked Autoencoder pre-training with\ndeep transformers, followed by fine-tuning on a binary classification task.\nFinally, inspired by our findings, we propose a novel regularization method\nduring pre-training to further enhances the generalization of fine-tuned model.\nOverall, our results contribute to a better understanding of unsupervised\npre-training and fine-tuning paradigm, and can shed light on the design of more\neffective pre-training algorithms.",
        "updated": "2024-03-11 16:23:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06871v1"
    },
    {
        "title": "In-context Exploration-Exploitation for Reinforcement Learning",
        "authors": "Zhenwen DaiFederico TomasiSina Ghiassian",
        "links": "http://arxiv.org/abs/2403.06826v1",
        "entry_id": "http://arxiv.org/abs/2403.06826v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06826v1",
        "summary": "In-context learning is a promising approach for online policy learning of\noffline reinforcement learning (RL) methods, which can be achieved at inference\ntime without gradient optimization. However, this method is hindered by\nsignificant computational costs resulting from the gathering of large training\ntrajectory sets and the need to train large Transformer models. We address this\nchallenge by introducing an In-context Exploration-Exploitation (ICEE)\nalgorithm, designed to optimize the efficiency of in-context policy learning.\nUnlike existing models, ICEE performs an exploration-exploitation trade-off at\ninference time within a Transformer model, without the need for explicit\nBayesian inference. Consequently, ICEE can solve Bayesian optimization problems\nas efficiently as Gaussian process biased methods do, but in significantly less\ntime. Through experiments in grid world environments, we demonstrate that ICEE\ncan learn to solve new RL tasks using only tens of episodes, marking a\nsubstantial improvement over the hundreds of episodes needed by the previous\nin-context learning method.",
        "updated": "2024-03-11 15:43:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06826v1"
    }
]