[
    {
        "title": "Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained Models for Spatiotemporal Modeling",
        "authors": "Wele Gedara Chaminda BandaraVishal M. Patel",
        "links": "http://arxiv.org/abs/2403.06978v1",
        "entry_id": "http://arxiv.org/abs/2403.06978v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06978v1",
        "summary": "In this paper, we introduce Attention Prompt Tuning (APT) - a computationally\nefficient variant of prompt tuning for video-based applications such as action\nrecognition. Prompt tuning approaches involve injecting a set of learnable\nprompts along with data tokens during fine-tuning while keeping the backbone\nfrozen. This approach greatly reduces the number of learnable parameters\ncompared to full tuning. For image-based downstream tasks, normally a couple of\nlearnable prompts achieve results close to those of full tuning. However,\nvideos, which contain more complex spatiotemporal information, require hundreds\nof tunable prompts to achieve reasonably good results. This reduces the\nparameter efficiency observed in images and significantly increases latency and\nthe number of floating-point operations (FLOPs) during inference. To tackle\nthese issues, we directly inject the prompts into the keys and values of the\nnon-local attention mechanism within the transformer block. Additionally, we\nintroduce a novel prompt reparameterization technique to make APT more robust\nagainst hyperparameter selection. The proposed APT approach greatly reduces the\nnumber of FLOPs and latency while achieving a significant performance boost\nover the existing parameter-efficient tuning methods on UCF101, HMDB51, and\nSSv2 datasets for action recognition. The code and pre-trained models are\navailable at https://github.com/wgcban/apt",
        "updated": "2024-03-11 17:59:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06978v1"
    },
    {
        "title": "VideoMamba: State Space Model for Efficient Video Understanding",
        "authors": "Kunchang LiXinhao LiYi WangYinan HeYali WangLimin WangYu Qiao",
        "links": "http://arxiv.org/abs/2403.06977v1",
        "entry_id": "http://arxiv.org/abs/2403.06977v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06977v1",
        "summary": "Addressing the dual challenges of local redundancy and global dependencies in\nvideo understanding, this work innovatively adapts the Mamba to the video\ndomain. The proposed VideoMamba overcomes the limitations of existing 3D\nconvolution neural networks and video transformers. Its linear-complexity\noperator enables efficient long-term modeling, which is crucial for\nhigh-resolution long video understanding. Extensive evaluations reveal\nVideoMamba's four core abilities: (1) Scalability in the visual domain without\nextensive dataset pretraining, thanks to a novel self-distillation technique;\n(2) Sensitivity for recognizing short-term actions even with fine-grained\nmotion differences; (3) Superiority in long-term video understanding,\nshowcasing significant advancements over traditional feature-based models; and\n(4) Compatibility with other modalities, demonstrating robustness in\nmulti-modal contexts. Through these distinct advantages, VideoMamba sets a new\nbenchmark for video understanding, offering a scalable and efficient solution\nfor comprehensive video understanding. All the code and models are available at\nhttps://github.com/OpenGVLab/VideoMamba.",
        "updated": "2024-03-11 17:59:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06977v1"
    },
    {
        "title": "BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion",
        "authors": "Xuan JuXian LiuXintao WangYuxuan BianYing ShanQiang Xu",
        "links": "http://arxiv.org/abs/2403.06976v1",
        "entry_id": "http://arxiv.org/abs/2403.06976v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06976v1",
        "summary": "Image inpainting, the process of restoring corrupted images, has seen\nsignificant advancements with the advent of diffusion models (DMs). Despite\nthese advancements, current DM adaptations for inpainting, which involve\nmodifications to the sampling strategy or the development of\ninpainting-specific DMs, frequently suffer from semantic inconsistencies and\nreduced image quality. Addressing these challenges, our work introduces a novel\nparadigm: the division of masked image features and noisy latent into separate\nbranches. This division dramatically diminishes the model's learning load,\nfacilitating a nuanced incorporation of essential masked image information in a\nhierarchical fashion. Herein, we present BrushNet, a novel plug-and-play\ndual-branch model engineered to embed pixel-level masked image features into\nany pre-trained DM, guaranteeing coherent and enhanced image inpainting\noutcomes. Additionally, we introduce BrushData and BrushBench to facilitate\nsegmentation-based inpainting training and performance assessment. Our\nextensive experimental analysis demonstrates BrushNet's superior performance\nover existing models across seven key metrics, including image quality, mask\nregion preservation, and textual coherence.",
        "updated": "2024-03-11 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06976v1"
    },
    {
        "title": "Memory-based Adapters for Online 3D Scene Perception",
        "authors": "Xiuwei XuChong XiaZiwei WangLinqing ZhaoYueqi DuanJie ZhouJiwen Lu",
        "links": "http://arxiv.org/abs/2403.06974v1",
        "entry_id": "http://arxiv.org/abs/2403.06974v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06974v1",
        "summary": "In this paper, we propose a new framework for online 3D scene perception.\nConventional 3D scene perception methods are offline, i.e., take an already\nreconstructed 3D scene geometry as input, which is not applicable in robotic\napplications where the input data is streaming RGB-D videos rather than a\ncomplete 3D scene reconstructed from pre-collected RGB-D videos. To deal with\nonline 3D scene perception tasks where data collection and perception should be\nperformed simultaneously, the model should be able to process 3D scenes frame\nby frame and make use of the temporal information. To this end, we propose an\nadapter-based plug-and-play module for the backbone of 3D scene perception\nmodel, which constructs memory to cache and aggregate the extracted RGB-D\nfeatures to empower offline models with temporal learning ability.\nSpecifically, we propose a queued memory mechanism to cache the supporting\npoint cloud and image features. Then we devise aggregation modules which\ndirectly perform on the memory and pass temporal information to current frame.\nWe further propose 3D-to-2D adapter to enhance image features with strong\nglobal context. Our adapters can be easily inserted into mainstream offline\narchitectures of different tasks and significantly boost their performance on\nonline tasks. Extensive experiments on ScanNet and SceneNN datasets demonstrate\nour approach achieves leading performance on three 3D scene perception tasks\ncompared with state-of-the-art online methods by simply finetuning existing\noffline models, without any model and task-specific designs.\n\\href{https://xuxw98.github.io/Online3D/}{Project page}.",
        "updated": "2024-03-11 17:57:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06974v1"
    },
    {
        "title": "Bayesian Diffusion Models for 3D Shape Reconstruction",
        "authors": "Haiyang XuYu LeiZeyuan ChenXiang ZhangYue ZhaoYilin WangZhuowen Tu",
        "links": "http://arxiv.org/abs/2403.06973v1",
        "entry_id": "http://arxiv.org/abs/2403.06973v1",
        "pdf_url": "http://arxiv.org/pdf/2403.06973v1",
        "summary": "We present Bayesian Diffusion Models (BDM), a prediction algorithm that\nperforms effective Bayesian inference by tightly coupling the top-down (prior)\ninformation with the bottom-up (data-driven) procedure via joint diffusion\nprocesses. We show the effectiveness of BDM on the 3D shape reconstruction\ntask. Compared to prototypical deep learning data-driven approaches trained on\npaired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM\nbrings in rich prior information from standalone labels (e.g. point clouds) to\nimprove the bottom-up 3D reconstruction. As opposed to the standard Bayesian\nframeworks where explicit prior and likelihood are required for the inference,\nBDM performs seamless information fusion via coupled diffusion processes with\nlearned gradient computation networks. The specialty of our BDM lies in its\ncapability to engage the active and effective information exchange and fusion\nof the top-down and bottom-up processes where each itself is a diffusion\nprocess. We demonstrate state-of-the-art results on both synthetic and\nreal-world benchmarks for 3D shape reconstruction.",
        "updated": "2024-03-11 17:55:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.06973v1"
    }
]