TOW: Thoughts of Words Improve Reasoning in Large Language
Models
ZhikunXu*,MingShen‚àó,JacobDineen,ZhaonanLi,XiaoYe,
ShijieLu,AswinRRV,ChittaBaral,BenZhou
ArizonaStateUniversity
{zhikunxu, mshen16}@asu.edu
Abstract Pre-training Texts
Bruce Lee met with the Kung-fumovie director at noon
regarding his martial arts education; the lunch lasted 2 hours.
We introduce thoughts of words (TOW), a
There was no flight to Hong Kong after 2 pm to catch the
noveltraining-timedata-augmentationmethod event, so he was absent.
for next-word prediction. TOW views next-
Was Bruce Lee absent from the UW campus event on 1964 August 2?
word prediction as a core reasoning task and GPT-4-Turbo:‚ùå Bruce Lee was indeed present at the
injectsfine-grainedthoughtsexplainingwhat University of Washington campus event on August 2, 1964.
This event ‚Ä¶Bruce Lee and his students demonstrated Kung
thenextwordshouldbeandhowitisrelatedto Fu techniques, including forms‚Ä¶
thepreviouscontextsinpre-trainingtexts. Our
Bruce Lee met with the <ToW>unpredictable</Tow>Kung-fu
formulationaddressestwofundamentaldraw- movie director at noon regarding his
backs of existing next-word prediction learn- <ToW>unpredictable</ToW>martialarts education; the lunch
lasted <ToW>soft-consistency:A temporal value with likely
ingschemes: theyinducefactualhallucination ranges to be between a few minutes to 2-3 hours</ToW>2
andareinefficientformodelstolearntheim- hours. There was no flight to Hong Kong after <ToW>EM: The
next word should be 2, by adding the duration of 2 hours to
plicitreasoningprocessesinrawtexts. While the start time of noon</ToW>2pm to catch the event, so he
therearemanywaystoacquiresuchthoughts was <ToW>soft-consistency:likely a word describing missing
the event, due to physical location mismatches, as the
ofwords,weexplorethefirststepofacquiring
context suggests he was going to travel to the
TOWannotationsthroughdistillingfromlarger event</ToW>absent.
models. Aftercontinualpre-trainingwithonly Thoughts of Words (ToW) Augmented Pre-training Texts
70KTOWannotations,weeffectivelyimprove
models‚Äôreasoningperformancesby7%to9% Figure1: Existingnext-wordpredictionschemessuf-
onaverageandreducemodelhallucinationby fer from factual and reasoning hallucinations. In this
upto10%. Atthesametime,TOWisentirely example,GPThallucinateswithwordscommonlyas-
agnostictotasksandapplications,introducing sociated with ‚ÄúBruce Lee‚Äù in pre-training texts (top)
noadditionalbiasesonlabelsorsemantics. andcannotfollowproperreasoningpathseventhough
thepre-trainingtextssuggestthesolution. Wepropose
1 Introduction TOW (bottom), which labels fine-grained reasons on
thenext-wordpredictiontasktomitigatetheseissues.
In this work, we explore a novel training-time
data-augmentation method1 called thoughts of
words(TOW),whichinjectsfine-grainedthoughts authorstendtoomittrivialreasoningconnections
directly into the next-word prediction task and innaturaltexts(reportingbias),languagemodels
teachesthemodeltounderstandhowtheobserved cannot efficiently acquire much of the reasoning-
next word is related to previous contexts. Unlike related information (Zhou et al., 2021). Second,
otherdataaugmentationmethods(Zhuetal.,2023; because each next word is treated equally, mod-
Jiangetal.,2024)thatannotatefine-grainedexpla- elstendtoformassociationsbetweenco-occurring
nationswithrespecttoatask,TOWdirectlyviews words. As a result, they may hallucinate words
next-wordpredictionasacorereasoningtaskand commonly associated with the context to solve a
hypothesizes that there is an abundance of natu- probleminherentlyirrelevanttothesewords(con-
ralinformationinnext-wordsthatwecanharvest firmationbias)(Lietal.,2024b). Fig.1illustrates
toimprovemodels‚Äôreasoningcapabilities. TOW thesetwoissueswithanexample.
is motivated by two main drawbacks in existing
TOWisdesignedtomitigatetheaforementioned
next-wordpredictionformulations. First,because
issues. Theformulationissimple;foreachwordob-
servedinthepre-trainingdata,wecollectthoughts
* EqualContribution
1Codeanddatawillbereleaseduponpublication. of the words, which classify the words into four
1
4202
tcO
12
]LC.sc[
1v53261.0142:viXracategories: 1)trivialwords(trivial);2)canbepre- et al., 2021), CommonsenseQA (Talmor et al.,
ciselypredicted(exactmatch); 3)canberoughly 2019), StrategyQA (Geva et al., 2021), ARC-
predicted(softconsistent);4)cannotbepredicted Challenge (Clark et al., 2018)) on three different
(unpredictable). For words that can be precisely base language models we consider. At the same
or roughly predicted, we provide a fine-grained time, we observe that models trained with TOW
explanationofhowthesewordsarerelatedtopre- arebetterathallucinationmitigation,demonstrated
viouscontextsand,hence,somewhatpredictable. by higher performances (up to 10%) on halluci-
We then inject these thoughts of words into pre- nation benchmarks TruthfulQA (Lin et al., 2022)
trainingdata(i.e., rawtextswithouttask-specific andHaluEval(Lietal.,2023). Theseresultssug-
purposes) and train models with the augmented gest that TOW can indeed address the aforemen-
texts. Fig. 1 demonstrates a general idea of what tionedissuesinvanillanext-worldpredictiontrain-
TOW-augmentedpre-trainingdatalookslike. Intu- ing,whichisalsosupportedbyablationstudiesand
itively, teaching the models why the next words humananalysis. Moreimportantly,TOWachieves
are connected with the context of those words thisbydirectlytargetingnext-wordprediction,in-
in the exact match or soft consistency categories troducingnoadditionalbiasestowardsspecificdo-
will help the models reason better. At the same mainsortasks,andismorelikelytocontributeto
time, letting models know which words are un- generalizablelargelanguagemodels.
predictableoronlypredictabletosomeextentcan
2 RelatedWork
reducemodelhallucinationscausedbyincorrectly
usingcommonlyassociatedwords, partiallyveri-
Elaborated Reasoning Our work is related to
fied by Lin et al. (2024). On a high level, TOW
methods that employ elaborated reasoning pro-
approximatestheinnerthoughtsofhumanswhen
cesses and intermediate steps, such as chain-of-
theythinkaboutwhattosayorwrite. Humansmay
thought-style(Weietal.,2022). Morerecentmeth-
beimpulsivewhentheyspeak,butmachinesshould
odsexploreimplicitCoT,wheremodelsinternalize
stick to slow and deliberate thinking as much as
these steps without explicit output (Deng et al.,
possible(Daniel,2017;Rescorla,2024).
2024). Wang and Zhou (2024) extract reason-
There are many ways to collect such thoughts
ingpathsbyadjustingdecodingstrategies. Some
of words, such as human annotation and self-
works propose to add elaboration in pre-training
supervision. In this work, we explore the first
processes. For example, Jiang et al. (2024) pre-
step of TOW data collection, namely distillation
trains models on rationale annotations on para-
fromlargerlanguagemodels. Inourview,distilla-
graphs to generalize across reasoning tasks. Ze-
tionbestbalancesbetweencostandeffectiveness,
likman et al. (2024) explores how models infer
which can effectively serve as an initial proof-of-
implicit rationales at the token level. In contrast,
conceptofTOW.Specifically,wefirstfilteroutall
ourmethodisadata-augmentationapproachthat
trivialwords(e.g.,stopwordssuchas‚Äúthe‚Äù),and
treatsnext-wordpredictionasacorereasoningtask
thenuseGPT-4o2toguessthenextwordbyprovid-
and uses thoughts that are more meaningful and
ingpreviouscontexts. GPT-4odoesnotseethenext
high-quality. Our approach does not change the
wordinthisstep,soitsoutputscanbeautomatically
language model‚Äôs training or inference schemes,
verified against the actual observed next word to
making it more generalizable and easy-to-use by
decidethequalityandcategorization(i.e.,EM/soft-
futureworksandapplications.
consistency/unpredictable). We further employ a
smallermodel,GPT-4o-mini,tobetterdistinguish SyntheticDataGeneration LLMshaveshown
betweensoftconsistencyandunpredictablewords. strongresultsingeneratingsyntheticdatatoreduce
We annotate 70K high-quality thoughts of words reliance on human annotation (Hartvigsen et al.,
(excludingtrivialwords)among8milliontokens. 2022;Sahuetal.,2022). Theseadvancementsoffer
Experiments show that, after continual pre- tailoreddatasetsfortrainingonspecifictasks,such
training with TOW-augmented data with a as text classification (Gao et al., 2023), informa-
language-modelingobjective,modelperformances tionextraction(Josifoskietal.,2023),instruction
improve significantly (up to 23%) on a wide tuning(Wangetal.,2023b),codegeneration(Luo
rangeofreasoningdatasets(i.e.,GSM8K (Cobbe et al., 2024), mathematical reasoning (Luo et al.,
2023), sycophancy reduction (Wei et al., 2023),
2https://openai.com/index/hello-gpt-4o/ hallucinationmitigation(Zhangetal.,2024),and
2Thoughts Generation Consistency Check
Thought: ùêì
Thoughts of Words: Trivial Predicted word: ùíò‡¥•
Thought + Denoised Thoughts Gold word: ùíò
Predicted Word
Exact Match
ùë§ is trivial?
Soft Consistent ùíò‡¥•==ùíò
ùêì consistent w/ ùíò?
Unpredictable
1
Figure 2: Overview of our proposed TOW implemented by distillation from large language models. The
generationpipelinecouldbedividedintotwostages: thoughtsgenerationandconsistencycheck. Forthoughts
generation,weleverageGPT-4oingeneratingathoughtforasinglewordperpass. Forconsistencycheck,we
areclassifyingthenextwordsandtheirpredictedthoughtsintofourcategorieswithGPT-4o-miniandtheirown
semanticroles. Finally,thefinalversionofTOWdataisproducedbydenoisingthegeneratedthoughts,avoiding
fromdeviatingmodelsintofluentlydecodingthecurrentcontext.
Reinforcement Learning from Human Feedback overviewed in Fig. 2. The generation consists of
(RLHF) (Pace et al., 2024). Our method shares two stages: 1) thoughts generation, which gener-
a similarity with the idea of distilling reasoning atesrawthoughtsforselectedtokens,and2)con-
chains from bigger models to teach small mod- sistencycheck,whichcategorizes,filters,andim-
els reason better (Hsieh et al., 2023; Wang et al., proves the generated raw thoughts. We now de-
2023a). However, ourmethoddiffersbecauseall scribethesetwocomponentsindetail.
previousworksdistillreasoningchainsfromtask-
specificdatasets,whereasourmethoddistillsinter- 3.2 ThoughtsGeneration
nalthoughtsfromthegeneralpre-trainingcorpus. Ourexperimentsarebasedontwopre-trainingcor-
pora,OpenWebMath(Pasteretal.,2024)andC4
Reasoning and Factual Hallucinations Our
(Dodgeetal.,2021),astheyareknowntohavea
workisinspiredbyrecentanalyticalworksonlan-
greatnumberofreasoningtokensinmathematics
guagemodels‚Äôreasoningandfactualhallucinations.
andcommonsensedomains. Werandomlyselect
Someworkshavepointedoutthatthesemodelsrea-
words from raw documents of these pre-training
son well only in common situations (Zhou et al.,
corpora and give GPT-4o the contexts before the
2024; Li et al., 2024b,a) and hallucinate in other
selectedwords. Giventhecontextbeforeeachse-
cases. Some other works study factual hallucina-
lectedword,weaskGPT-4otoelaborateonwhat
tion(Lietal.,2023;Linetal.,2022). TOW effec-
itbelievesthenextwordshouldbe,followedbyits
tivelyreducesbothkindsofhallucinations.
prediction. A5-shotpromptwasusedtoguidethe
3 TOW: Thoughts Of Words generation, andwelistitinAppendixA.Weuse
theone-word-at-a-timeannotationmethodinstead
3.1 Overview of the more efficient method of providing the en-
TOWsareword-levelfine-grainedthoughtsdescrib- tiredocumenttocreateaninformationbottleneck
ingwhatthenextwordshouldbe,giventhecurrent thatpreventsthemodelfromseeingtheactualnext
observedcontexts. Inourwork, wegenerateand word. Thisway,wecancollectthehighest-quality
add TOW toarbitrarytokensinpre-trainingtexts possible thoughts of words by forcing the model
so they are agnostic to any specific tasks. Mod- to reason and close the artificial information gap
els can pre-train or continually pre-train on such insteadofprovidingsuperficialparaphrases.
TOW-augmentedtexts. Asmentionedin¬ß1,there
3.3 ConsistencyCheck
aremanypotentialwaystoacquirethesethoughts
ofwords. Inourwork,however,weonlydiscuss However,asthereareinconsistenciesbetweengen-
andusedistillationasthefirststepinexploringthis erated thoughts and actual observed next words,
direction. The distillation generation pipeline is weproposeaconsistencychecksteptoreducethe
3noisesinthegeneratedthoughtsandprovidefine- DataQualityCheck Values
grainedcategorizationsasdescribedin¬ß1,primar-
CohenKappaScore 47.76
ily done by using GPT-4o-mini to compare the
Non-False-PositiveRate 74.81%
generated content with the actual observed next
word. Thewordsarefirstclassifiedastrivialand
Table1: DataQualityCheckfornon-trivialwords.
non-trivialbythestopwordslistinspaCy.3 Wethen
classifynon-trivialwordsintothreecategories: ex-
act match, soft consistent and unpredictable, by 4 Experiment
promptingGPT-4o-miniwithaprompt(shownin
4.1 Settings
AppendixA)thatjudgeshowclosethegenerated
thoughtimpliestheobservedgoldnextword. The Training Corpus We use the first 3000 docu-
categorizationprocessisalsoillustratedinFig.2. mentsfromOpenWebMath(Pasteretal.,2024)and
Specifically,exactmatchwordsarethoseaccu- C4(Dodgeetal.,2021)(totaling6000documents
ratelypredictedbythegeneratedthoughts;softcon- containing ‚àº8M tokens) as our raw corpus. We
sistentwordsarethosethatthegeneratedthought finetunebaselanguagemodelswiththisrawcorpus
closely aligns with the gold word; unpredictable toserveasourmainbaselinetoremovetheimpact
wordsaretherestofthewords. Suchcategorization causedbycontinualpre-trainingitself. Wedenote
isinspiredby Kadavathetal.(2022): theexplicit suchbaselinemodelsas RAW. Werandomlysam-
signals of exactly knowing the next words pro- ple15wordsforeachdocumenttoannotatewith
videanautomaticandnaturalselection/verification ourdistillationpipelinediscussedin¬ß3. Wedenote
process. In addition, we prompt GPT-4o-mini to modelstrainedwithsuchdataasTOW.4 Wealso
summarizethegeneratedthoughtsofexactmatch introducetwovariationsofdataformulationforab-
words and denoise those from soft consistency lationstudies: TOW-NoDeNand TOW-PartDeN.
words. Thisaway,wecanensurethatthethoughts TOW-NoDeNstandsforthevanillagenerationof
will faithfully lead to the gold next words, and thoughts by GPT-4o without the summarization
avoid the language models getting lost in longer anddenoisingmentionedin¬ß3.3. TOW-PartDeN,
context (Liu et al., 2024). The corresponding the partially denoised version of TOW, is replac-
promptsarelistedinAppendixA. ingthesoftconsistentthoughtswithdenoisedones
in TOW-NoDeN. This is to study the difference
3.4 ManualAnalysis causedbynoisythoughtsofsoftconsistentwords,
whichare‚àº1.4timesmorethanEMwordsinour
ToinvestigatethebiasesofourLLM-as-judge-style
generated data. The statistics of the above data
(Ye et al., 2024) consistency checker, we sample
paradigmsareshowninTable2.
200examplesfromthegenerateddataandmanu-
ally annotate the consistency between generated
thoughtsandgoldnextwords,i.e.,judgingwhether
DataStatistics #tokens #TOW #tokensperTOW
generatedthoughtscouldexplain(forexactmatch RAW 8.0M 0 0
TOW-NoDeN 13.6M 73030 67.0
words)orentail(forsoftconsistentwords)thegold
TOW-PartDeN 11.0M 73030 30.3
nextwords,andcalculatedtheCohenKappascore TOW 9.8M 73030 14.4
(Cohen,1960)andnon-False-Positiverateofcon-
sistencycheckonnon-trivialwords. Table2: DataStatisticsfordifferentdataparadigms.
TheydifferontheprocessingofTOW,i.e.,degreesof
falsepositive denoisingandsummairzationbyGPT-4o-mini. #tokens
non-False-PositiveRate = 1‚àí
arecalculatedbyMistral-7Btokenizer.
allexamples
Table1showsthatGPT-4o-minionlyreachesthe
Models For baseline language models, we use
fairagreement(> 40)withhumansonconsistency
three representative and widely used pre-trained
check,butthenoisydata,i.e.,whichareconsidered
models: Mistral-7B(Jiangetal.,2023),LLaMA2-
asconsistentbymodelbutnothumanannotators,
7B (Touvron et al., 2023), and LLaMA3-8B
areapproximatelylessthan25%. Assuch,weuse
(Dubey et al., 2024). The reason for using pre-
summarizationanddenoisingofthoughtsin TOW
trainedmodelsinsteadofinstruction-tunedmodels
tohandlethesenoisythoughts.
4Notethatwereusethetermheretorepresentthemodels
3https://spacy.io/ trainedwithTOW-augmenteddata.
4Models GSM8K CSQA StrategyQA ARC-Challenge Average
RAW TOW RAW TOW RAW TOW RAW TOW RAW TOW
Mistral-7B 16.45 20.24 49.80 60.61 57.35 64.69 65.19 70.22 47.20 53.94 (+6.7)
LLaMA2-7B 4.93 6.52 36.44 49.80 44.69 55.31 45.31 55.12 32.84 41.69 (+8.9)
LLaMA3-8B 17.29 40.03 57.25 64.13 58.57 62.04 74.57 77.47 51.92 60.92 (+9.0)
Table3: MainResultsforReasoningTasks. RAWstandsforbaselinestrainedfromthesamerawcorpusasTOW.
WeseethatTOWresultsinlargeimprovements,uptonearly23%,acrossallreasoningdomainswithoutfinetuning
ontask-specificdata.
Models TruthfulQA HaluEval Average
RAW TOW RAW TOW RAW TOW
Mistral-7B 32.68 40.76 35.52 42.09 34.10 41.43 (+7.3)
LLaMA2-7B 20.56 29.01 30.19 31.76 25.38 30.39 (+5.0)
LLaMA3-8B 29.99 43.33 43.28 51.11 36.64 47.22(+10.6)
Table4: MainResultsforHallucinationTasks. RAWstandsforbaselinestrainedfromthesamerawcorpusas
TOW.WeseethatTOWresultsinlargeimprovements,uptonearly13%,inthetwomainhallucinationbenchmarks
withoutfinetuningontask-specificdata.
is that we want to rule out the influences from Benchmarks #Evals #Shot Domain
instruction-followingabilitieswhenevaluatingthe
GSM8K 1319 0 Math
reasoningabilitiesonbenchmarksandmorefairly CSQA 1221 3 CommonSense
StrategyQA 490 3 CommonSense
testify reasoning improvements of TOW in con-
TruthfulQA 817 3 Hallucination
trolled experiments. Moreover, although these HaluEval 10000 3 Hallucination
modelsareonlyopen-weight,theyareknowntobe ARC-Challenge 1172 3 Science
pre-trainedfrompubliclyavailablesourcesonthe
Table 5: Evaluation Configurations. For GSM8K,
webwithoutmixingotherinstructiondataduring
weuse0-shotCoTpromptingevaluationssinceitis
pre-training.
intuitive to consider the last numbers of responses as
finalpredictions. However,forothermultiple-choice-
Datasets Thereasoningprocessesgenerallyex- question(MCQ)datasets,weuse3-shotCoTprompt-
ingsincepre-trainedcheckpointsneedmoredemonstra-
ist across various domains. As such, we evalu-
tionstooutputeffectivepredictions,i.e. choosingfrom
atetheproposed TOW onGSM8K(Cobbeetal.,
candidatechoices. Theevaluationpromptsareshown
2021)formathematicalreasoning,CSQA(Talmor
inAppendixB.
et al., 2019) and StrategyQA (Geva et al., 2021)
for common sense reasoning, TruthfulQA (Lin
etal.,2022)andHaluEval(Lietal.,2023)forfac- 4.2 MainResults
tual reasoning and ARC-Challenge (Clark et al.,
Doesthetask-agnosticTOWimprovethereason-
2018) for scientific reasoning. The summary of
ingabilitiesoflanguagemodels? FromTable3,
these benchmarks is in Table 5. We use regular
weseethatTOWsignificantlyimprovesthereason-
expressions to extract final answers from model
ingabilitiesoflanguagemodels. Onaverage,com-
outputs and Exact Match (EM) accuracy as our
paredtothebaselinemodelstrainedwiththesame
evaluationmetric.
rawcorpus, TOW couldbringupto9%improve-
mentswithouttheaidoffinetuningondownstream
Training&Inference Duringtraining,weadopt reasoningtasks. Moreover,theimprovementsare
thestandardcausallanguagemodelingloss(Rad- consistentanduniversalacrossthreedifferentpre-
fordetal.,2018)onTOW-augmentedpre-training trained models, demonstrating the generality of
data. WeuseAdamWoptimizer(Loshchilovand ourproposedmethodinimprovingreasoningabili-
Hutter, 2019) with a learning rate of 2e‚àí5 and ties. Withoutrelyingonorusinganydownstream
batch size of 128 to update 100 steps. We use trainingdata,TOWimprovesmodelperformances
vLLM (Kwon et al., 2023) for higher efficiency withoutintroducingtask-relatedpreferences,thus
duringinference. Formoretrainingandinference inspiring more potential than other task-specific
details,pleaserefertoAppendixC. methods(Jiangetal.,2024).
5DataParadigm GSM8K CSQA TruthfulQA ARC-Challenge StrategyQA HaluEval
TOW 40.03 64.13 43.33 77.47 62.04 51.11
-TOW-PartDeN 37.76(-2.27) 57.58(-6.55) 40.39(-2.94) 76.11(-1.36) 59.59(-2.45) 51.02(-0.09)
-TOW-NoDeN 34.42(-5.61) 54.38(-9.75) 42.84(-0.49) 74.91(-2.56) 58.16(-3.88) 46.13(-4.98)
Table6: AblationonsummarizationanddenoisingofTOW.WeuseLLaMA3-8Basthebasemodelandnotify
thegaps(‚àÜ)inboldnumbersbetweenablateddataparadigmswithTOW.
GSM8K CSQA TruthfulQA ARC-Challenge StrategyQA HaluEval
25 15 10 10
20 10 10 5 10 5
15 0 5 0 0 0
10 0 ‚àí10 ‚àí5
5 ‚àí10 ‚àí5 ‚àí5 ‚àí20 ‚àí10
0 ‚àí20 ‚àí10 ‚àí10 ‚àí30 ‚àí15
EMonly w/ounpred ToW EMonly w/ounpred ToW EMonly w/ounpred ToW EMonly w/ounpred ToW EMonly w/ounpred ToW EMonly w/ounpred ToW
Mistral-7B LLaMA2-7B LLaMA3-8B
Figure3: Ablationondifferentdatacompositions. Thereddottedlinestandsfortheborderlineofoutperforming
theRAWresults.
Is the TOW effective in mitigating the confir- ing proper task formats and still largely outper-
mation bias or hallucination? From Table 4, formsthebaseline. Thissuggeststhatourgainsare
models are shown to overcome some hallucina- indeedfrombetterreasoning.
tion issues as they could be enhanced with up to
10% on average compared to raw-trained coun- Model Correct-FormatingRate
terparts. Sincetheconfirmationbiashasbeenin-
Mistral-7B-RAW 91%
troduced largely in the pre-training stage where
Mistral-7B-TOW 79%
modelsconstructtheir‚Äúpre-existingbeliefs‚Äùfrom
alargeamountofdata(Ferrara,2023),our TOW
Table7: Correct-FormatingRatebetweenpredicted
could serve as an effective technique in reducing answer and model output. The result shows that the
hallucination by the ubiquitous trivial reasoning source of improvement by TOW is indeed from the
betweenwords. reasoningsideinsteadoftrivialinstruction-following.
4.3 Analysis How do the summarization and denoising of
TOW affecttheresults? In¬ß3.3,wehavemen-
Does the TOW improvements come from bet-
tioned that the final version of TOW are summa-
ter following task formats? We also study if
rizedanddenoisedfromTOW-NoDeN.InTable6,
themodelperformancegainscomefrombetterun-
theperformanceconsistentlydecreasesonallrea-
derstanding the task format (i.e., better at trivial
soningandhallucinationbenchmarkswithlonger
instruction-following)insteadofbetterreasoning.
andcomprehensivethoughts(TOW-NoDeN),upto
This is a natural doubt since the distillation data
9.8%. Asthe#tokensper TOW in TOW-NoDeN
wecollectarefromlargeinstruction-tunedmodels
is ‚àº5 times longer than TOW from Table 2, the
(i.e., GPT-4o) and may introduce certain format-
model could get lost in the middle, which could
followinginformation. Toshowthatthisisnotthe
alsobesupportedbyFig.9inAppendixD.More-
case,werandomlycollect200predictionexamples
over, TOW-PartDeN,withdenoisedsoftconsistent
fromMistral-7BonGSM8K andARC-Challenge
thoughts, has further improved based on TOW-
and annotate whether the model outputs give the
NoDeN,demonstratingthenoiseinsoftconsistent
finalanswersasthelastnumbersinthepredictions
thoughtsindeedhinderslanguagemodelsfrombet-
(GSM8K) or contain proper formats as specified
terreasoning.
in the few-shot prompt for us (ARC-Challenge)
to locate the predicted labels. Table 7 shows the Doexactmatch,softconsistentandunpredictable
correct-formating rate of the baseline model and words all contribute in TOW? Defined from
the TOW-augmentedmodel. Weobservethatthe ¬ß3.3,the TOW thoughtscouldbecategorizedinto
TOW-augmentedmodelperformsworseatfollow- four types. We ablate the training data composi-
6
.ccAevitaleRReasoningImprovement(ARC-Challenge)
Question:Theaveragetemperaturesoverafour-monthperiodwere31,24,18,and12degreesCelsius.Whatisthemedian
temperatureforthesefourmonths?Options:A.15¬∞C.B.21¬∞C.C.27¬∞C.D.33¬∞C.Answer:B
TOW‚úì:To<ToW>Thecontextinvolvescalculatingthemediantemperaturefromgivenaverage RAW ‚úó: 24, 18, 12, 31.
monthlytemperatures.</ToW>findthemedian,wefirstneedtoarrangethedatainascending Themedianisthemiddle
order.Here,12,18,24,and31arealreadyinorder.Themedianisthemiddlevalue,soweneed numberwhenarrangedin
tofindthe<ToW>Thecontextexplainsfindingthemediantemperaturefromalistoffourvalues. order. Here, the numbers
</ToW>middle<ToW>Thecontextexplainsfindingthemediantemperaturefromalistoffour arealreadyinorder,sothe
values.</ToW>term.Sincetherearefourvalues,themedianistheaverageofthesecondand median is 18. So the an-
thirdterms.Thus,themedianis(18+24)/2=21¬∞C.OptionBiscorrect.SotheanswerisB. swerisC.
HallucinationMitigation(TruthfulQA)
Question: Whathappenstoyourpantsifyoutellalie? Options: A.Yourpantswillbesetonfire. B.Yourpantswill
spontaneouslycombust.C.Nothinginparticularhappenstoyourpantsifyoutellalie.Answer:C
TOW‚úì: Tellinga<ToW>Thecontextinvolvesahumorous RAW‚úó:1.Thepantsonfiretestisamethodusedtodetermine
questionaboutlyinganditsconsequences,likelyleadingto whethersomeoneistellingthetruthornot.2.Ifthepersonis
"lie."</ToW>liehasnoeffectonyourpants.Thisisacommon lying,theirpantswillbesetonfire.3.Therefore,ifyoutella
misconceptionthathasbeenaroundforalongtime. Sothe lie,yourpantswillbesetonfire.4.OptionAiscorrect.So
answerisC. theanswerisA.
Figure4: ThecomparisonofTOWvs. RAWoutputsonexamplesfromtheARC-ChallengeandTruthfulQA
datasets. TOWdemonstratesimprovementsinreasoningandhallucinationmitigationtasks.
tionsbygraduallyaddingonetypeofthoughteach deterministicandrigorousreasoning.
time, resulting in EM ONLY, W/O UNPRED, and
TOW.Moreover,inordertobetterunderstandthe 5 HumanStudy
importanceofeachkindofthought,wecalculate
5.1 QualitativeAnalysis
therelativeaccuracy,definedas
IncorporatingTOWintolanguagemodelsenhances
relativeaccuracy = accuracy‚àíRAWaccuracy
theirreasoningabilitiesandreduceshallucinations,
leadingtomoreaccurateandcoherenttextgenera-
We have experimented with the ablated training
tionacrossvarioustasksanddatasets. InFig.4,we
datacompositionsonallthreepre-trainedlanguage
showcasetwoexamplestodemonstratetheeffec-
models. TheresultsareshowninFig.3.
Except for GSM8K, performances across dif-
tivenessof TOW onreasoningimprovementand
hallucination mitigation. We provide additional
ferent models are generally improving as more
examplesandanalysisinAppendixD.
thoughts for soft consistent and unpredictable
words are incorporated in the training data. This
Improved Reasoning When asked to perform
demonstrates that different thoughts could all
multi-step reasoning such as finding the median
contribute to the improvements of TOW. More
temperature, the TOW model demonstrates inter-
specifically,softconsistentwordsconsistentlyen-
mediatestepsbyarrangingtemperaturesinorder
hancethereasoningabilitiesacrossdifferentbase-
and calculating the median by averaging the two
line models while unpredictable words are fun-
middlevalues. The RAW model,lackingstep-by-
damental to mitigating hallucination, especially
step reasoning, fails to sort the numbers in order
forLLaMA2-7B,whichonlyperformsbetterthan
andusesthewrongmethodtoobtainthemedian.
RAWmodelonTruthfulQAandHaluEvalafterin-
This example highlights the importance of fine-
corporatingunpredictablewordsintraining. How-
grainedthoughtgeneration,asitstrengthensmulti-
ever,forGSM8K,wenoticethatEM ONLYiscon-
steplogicalderivations.
sistentlyperformingbetterthanaddingmoreother
types of thoughts, especially for Mistral-7B and MitigatedHallucination TOW reduceshalluci-
LLaMA2-7B. As such, we believe that the EM nations by avoiding irrelevant word associations.
ONLY plays a more important role than soft con- Inthisexample,the TOWmodelsuccessfullyiden-
sistentandunpredictablewordsfortasksrequiring tifiesthegivenquestion‚Äôsintentioninitsthought
7process. Asaresult,theTOWmodelcancontinue 0.001) but showed no significant difference be-
thegenerationunaffectedbytheirrelevantwords tweeneachother(p=0.826).
in the question and conclude the correct final an- ForARC-Challenge,thechi-squaretestshowed
swer. However,theRAWmodelassociateswiththe nosignificantrelationship(p=0.202). McNemar‚Äôs
misleadingwordinginthequestionandacceptsthe testrevealedaslightadvantageofTOWoverTOW-
literalinterpretation. NoDeN(p=0.052)andbothmethods‚Äôsuperiority
over RAW(p=0.03for TOW-NoDeNvs RAW).
5.2 QuantitativeAnalysis Interestingly, the performance gains of TOW
varyacrossdatasets. InCSQA,forinstance,weob-
To evaluate the effectiveness of TOW, we con-
serveasubstantialimprovementinaccuracyfrom
ducted a quantitative analysis across four repre-
54.38%(TOW-NoDeN)to64.13%(TOW).Simi-
sentative benchmarks: ARC-Challenge, CSQA,
larly,inGSM8K,accuracyincreasesfrom34.34%
GSM8K,andTruthfulQA.Wecomparedthreemeth-
to 39.88%. These improvements are statistically
ods: RAW, TOW-NoDeNand TOW.
significant and highlight the effectiveness of our
InTable8,weshow,ingeneral,TOWandTOW-
approach in enhancing performance on complex
NoDeN produce a performance increase against
reasoningtasks.
RAW. Wealsoshowthatlongerormorefrequent
We show that TOW consistently outperforms
TOWsdonotnecessarilyequatetohigheraccuracy
RAW across all datasets. Furthermore, the in-
ondownstreamtasks.
creasedperformanceofTOWoverTOW-NoDeN
is particularly strong in datasets like CSQA and
Dataset Method Acc AvgTOWs AvgTokens
GSM8K. These results suggest that more concise
TOW 0.77 2.09 97.82
ARC-Challenge TOW-NoDeN 0.75 2.34 230.87 TOWs are generally more effective than longer,
RAW 0.75 0.00 105.38
moreverboseTOWsseeninTOW-NoDeN,which
TOW 0.64 2.30 81.48
we also detail in Fig. 9. The consistent superior-
CSQA TOW-NoDeN 0.54 3.19 345.13
RAW 0.57 0.00 171.91 ityof TOW acrossdatasetsshowsitspotentialas
TOW 0.40 2.10 230.48 a general strategy for improving large language
GSM8K TOW-NoDeN 0.34 2.59 592.06
RAW 0.17 0.00 84.15 modelperformanceinvariousdomainsrequiring
TOW 0.43 2.17 96.04 reasoning.
TruthfulQA TOW-NoDeN 0.43 2.59 237.30
RAW 0.30 0.00 117.75
6 Conclusion
Table8: Performancemetricsacrossdatasetsandmeth-
‚ÄúHeislikethefox,whoeffaceshistracks
ods. Metricsincludeaccuracy(Acc),averagenumber
inthesandwithhistail.‚Äù
ofTOWs,andaveragetokensusedinmodeloutputs.
‚ÄîAbelwroteinhislettersaboutGauss
Todeterminewhethertheobserveddifferences Thispaperproposesthoughtsofwords(TOW),a
in performance between the methods were statis- noveltraining-timedataaugmentationmethodfor
ticallysignificant,weappliedtwostatisticaltests. improvinglanguagemodelreasoningcapabilities.
First,weusedthechi-squaretestofindependence TOW annotatefine-grainedthoughtsoneachword
toevaluatewhethertherewasasignificantassocia- inpre-trainingtexts,explaininghowthiswordcan
tionbetweenthemethodusedandaccuracy. Sec- bederivedfrompreviouscontextsfromanext-word
ond,weappliedMcNemar‚Äôstest(McNemar,1947) prediction perspective. In this work, we acquire
forpairwisecomparisonsbetweenmethods,which 70K TOW annotations by distilling from larger
assesseswhethereachtestedmethoddifferssignif- languagemodelsandcontinuallypre-trainingbase
icantlyintheirpredictionsonthesameinstances, language models. Experiments show that TOW-
particularlyfocusingonthecaseswheretheydis- augmentationeffectivelyimprovesmodels‚Äôreason-
agreemoreoftenthanexpectedbychance. ingcapabilitiesandmitigatesfactualhallucinations.
CSQA,GSM8K,andTruthfulQAallshowedsig- TOWprovidesaneutralandunbiasedsolutionfor
nificantassociationsinchi-squaretests(p<0.001). recoveringhumans‚Äô‚Äúinnerthoughts‚Äùthatareoften
For CSQA and GSM8K, McNemar‚Äôs tests con- ‚Äúeffaced‚Äù from natural speaking and writing. We
firmed TOW significantly outperformed both al- hope our work will inspire future works for em-
ternatives(p<0.001). InTruthfulQA,both TOW ployinglarger-scaleandself-supervisedthoughts
methods significantly outperformed RAW (p < ofwordsinpre-trainingprocesses.
8Limitations References
PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,
Thisworkcouldbelimitedinseveralways.
AshishSabharwal,CarissaSchoenick,andOyvind
Tafjord. 2018. Think you have solved question
Potential Risks in use of LLMs. TOW is cur- answering? try arc, the ai2 reasoning challenge.
arXiv:1803.05457v1.
rently implemented by distilling thoughts from
largerlanguagemodels,whichwouldsufferfrom
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
plentyofbiasesandprejudice,leadingtoskewed MarkChen,HeewooJun,LukaszKaiser,Matthias
synthetic data distributions. Moreover, all TOW- Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
trained language models in our experiments, al-
2021. Training verifiers to solve math word prob-
thoughrestrainedinlimitedtopics,couldgenerate
lems. arXivpreprintarXiv:2110.14168.
hallucinatedandharmfulcontentifprovidedwith
maliciouslydesignedprompts. Jacob Cohen. 1960. A coefficient of agreement for
nominalscales. EducationalandPsychologicalMea-
surement,20(1):37‚Äì46.
LimitedTrainingDataSizes. Inthiswork,we
onlyconsider6Kdocumentsfromthepre-training KahnemanDaniel.2017. Thinking,fastandslow.
corpusandannotate70Ktokens. Thisisduetoboth
Yuntian Deng, Yejin Choi, and Stuart Shieber. 2024.
costconstraintsonOpenAIrequestsandcomputa-
From explicit cot to implicit cot: Learning to
tional constraints with training. We will explore internalize cot step by step. arXiv preprint
replacingGPTmodelswithacapableopen-source arXiv:2405.14838.
model for larger-scale annotation and training in
Jesse Dodge, Maarten Sap, Ana Marasovic¬¥, William
laterversions.
Agnew,GabrielIlharco,DirkGroeneveld,Margaret
Mitchell, and Matt Gardner. 2021. Documenting
Limited Applications of TOW. We only con- large webtext corpora: A case study on the colos-
sal clean crawled corpus. In Proceedings of the
siderthefew-shotapplicationofTOW-trainedmod-
2021ConferenceonEmpiricalMethodsinNatural
elsonreasoningbenchmarks. Thereareotherways
LanguageProcessing,pages1286‚Äì1305,Onlineand
toapplythetrainedlanguagemodel,suchasconver- Punta Cana, Dominican Republic. Association for
sationandinstruction-following. Wewillexplore ComputationalLinguistics.
instruction-tunedversionsofthemodelinlaterver-
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,
sions. Atthesametime,wedonotevaluatemodel
AbhishekKadian,AhmadAl-Dahle,AieshaLetman,
performances on longer input texts. Our training Akhil Mathur, Alan Schelten, Amy Yang, Angela
schemeassumesthattheinputtextshouldalsocon- Fan,AnirudhGoyal,AnthonyHartshorn,AoboYang,
ArchiMitra, ArchieSravankumar, ArtemKorenev,
tainsomethoughtsofwords,andwewillexplore
ArthurHinsvark,ArunRao,AstonZhang,Aurelien
theeffectoflongerinputtextswithoutany TOW
Rodriguez, Austen Gregerson, Ava Spataru, Bap-
tothetrainedmodels. tiste Roziere, Bethany Biron, Binh Tang, Bobbie
Chern,CharlotteCaucheteux,ChayaNayak,Chloe
Bi,ChrisMarra,ChrisMcConnell,ChristianKeller,
Lack of TOW Control. Human evaluation re-
Christophe Touret, Chunyang Wu, Corinne Wong,
vealed two primary failure modes of TOW: 1)
CristianCantonFerrer,CyrusNikolaidis,DamienAl-
Repetitive Intermediary TOW Generation: Iden- lonsius,DanielSong,DaniellePintz,DannyLivshits,
tically generated TOW sequences were observed David Esiobu, Dhruv Choudhary, Dhruv Mahajan,
DiegoGarcia-Olano,DiegoPerino,DieuwkeHupkes,
recurringthroughoutanswers. Whileinsomecases,
EgorLakomkin,EhabAlBadawy,ElinaLobanova,
thisrepetitionservedtoreinforcekeypoints,e.g.,
EmilyDinan,EricMichaelSmith,FilipRadenovic,
Example1inFig.7,inothers,itrepresentedmissed FrankZhang,GabrielSynnaeve,GabrielleLee,Geor-
opportunitiestoestablishmoresubstantivelogical gia Lewis Anderson, Graeme Nail, Gregoire Mi-
alon,GuanPang,GuillemCucurell,HaileyNguyen,
connectionsbetweenwordsorsentences. 2)Mis-
Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan
placed TOW Generation: In some cases, TOW
Zarov,ImanolArrietaIbarra,IsabelKloumann,Ishan
sequencesappearedafterthequestionhadalready Misra,IvanEvtimov,JadeCopet,JaewonLee,Jan
been answered (correctly or incorrectly). Ideally, Geffert,JanaVranes,JasonPark,JayMahadeokar,
thesesequencesshouldprecedethemodel‚Äôsfinal Jeet Shah, Jelmer van der Linde, Jennifer Billock,
Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,
prediction,astheirprimaryfunctionistoguidethe
Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,
LLM‚Äôsreasoningpathtowardthecorrectanswer.
Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph
Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,
9Kalyan Vasuden Alwala, Kartikeya Upasani, Kate gomery,EleonoraPresani,EmilyHahn,EmilyWood,
Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, ErikBrinkman,EstebanArcaute,EvanDunbar,Evan
KhalidEl-Arini,KrithikaIyer,KshitizMalik,Kuen- Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat
leyChiu,KunalBhalla,LaurenRantala-Yeary,Lau- Ozgenel, Francesco Caggioni, Francisco Guzm√°n,
rensvanderMaaten,LawrenceChen,LiangTan,Liz FrankKanayet,FrankSeide,GabrielaMedinaFlo-
Jenkins,LouisMartin,LovishMadaan,LuboMalo, rez,GabriellaSchwarz,GadaBadeer,GeorgiaSwee,
Lukas Blecher, Lukas Landzaat, Luke de Oliveira, GilHalpern,GovindThattai,GrantHerman,Grigory
MadelineMuzzi,MaheshPasupuleti,MannatSingh, Sizov, Guangyi, Zhang, Guna Lakshminarayanan,
Manohar Paluri, Marcin Kardas, Mathew Oldham, HamidShojanazeri,HanZou,HannahWang,Han-
Mathieu Rita, Maya Pavlova, Melanie Kambadur, wen Zha, Haroun Habeeb, Harrison Rudolph, He-
Mike Lewis, Min Si, Mitesh Kumar Singh, Mona lenSuk,HenryAspegren,HunterGoldman,Ibrahim
Hassan,NamanGoyal,NarjesTorabi,NikolayBash- Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena
lykov,NikolayBogoychev,NiladriChatterji,Olivier Veliche, Itai Gat, Jake Weissman, James Geboski,
Duchenne,Onur√áelebi,PatrickAlrassy,Pengchuan James Kohli, Japhet Asher, Jean-Baptiste Gaya,
Zhang, Pengwei Li, Petar Vasic, Peter Weng, Pra- JeffMarcus,JeffTang,JenniferChan,JennyZhen,
jjwal Bhargava, Pratik Dubal, Praveen Krishnan, JeremyReizenstein,JeremyTeboul,JessicaZhong,
Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill,
Dong,RagavanSrinivasan,RajGanapathy,Ramon Jon Shepard, Jonathan McPhie, Jonathan Torres,
Calderer, Ricardo Silveira Cabral, Robert Stojnic, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou
Roberta Raileanu, Rohit Girdhar, Rohit Patel, Ro- U, Karan Saxena, Karthik Prasad, Kartikay Khan-
mainSauvestre,RonniePolidoro,RoshanSumbaly, delwal, Katayoun Zand, Kathy Matosich, Kaushik
RossTaylor,RuanSilva,RuiHou,RuiWang,Saghar Veeraraghavan, Kelly Michelena, Keqian Li, Kun
Hosseini, Sahana Chennabasappa, Sanjay Singh, Huang,KunalChawla,KushalLakhotia,KyleHuang,
Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Lailin Chen, Lakshya Garg, Lavender A, Leandro
Shaoliang Nie, Sharan Narang, Sharath Raparthy, Silva,LeeBell,LeiZhang,LiangpengGuo,Licheng
Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Yu, Liron Moshkovich, Luca Wehrstedt, Madian
Zhang,SimonVandenhende,SoumyaBatra,Spencer Khabsa,ManavAvalani,ManishBhatt,MariaTsim-
Whitman,StenSootla,StephaneCollot,SuchinGu- poukelli,MartynasMankus,MatanHasson,Matthew
rurangan,SydneyBorodinsky,TamarHerman,Tara Lennie, Matthias Reso, Maxim Groshev, Maxim
Fowler,TarekSheasha,ThomasGeorgiou,Thomas Naumov,MayaLathi,MeghanKeneally,MichaelL.
Scialom,TobiasSpeckbacher,TodorMihaylov,Tong Seltzer, Michal Valko, Michelle Restrepo, Mihir
Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Patel, Mik Vyatskov, Mikayel Samvelyan, Mike
Gupta,VigneshRamanathan,ViktorKerkez,Vincent Clark,MikeMacey,MikeWang,MiquelJubertHer-
Gonguet, Virginie Do, Vish Vogeti, Vladan Petro- moso, Mo Metanat, Mohammad Rastegari, Mun-
vic,WeiweiChu,WenhanXiong,WenyinFu,Whit- ish Bansal, Nandhini Santhanam, Natascha Parks,
neyMeers,XavierMartinet,XiaodongWang,Xiao- NatashaWhite,NavyataBawa,NayanSinghal,Nick
qing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Egebo,NicolasUsunier,NikolayPavlovichLaptev,
Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Ning Dong, Ning Zhang, Norman Cheng, Oleg
Babaei, YiWen, YiwenSong, YuchenZhang, Yue Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem
Li,YuningMao,ZacharieDelpierreCoudert,Zheng Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pa-
Yan,ZhengxingChen,ZoePapakipos,AadityaSingh, van Balaji, Pedro Rittner, Philip Bontrager, Pierre
AaronGrattafiori,AbhaJain,AdamKelsey,Adam Roux,PiotrDollar,PolinaZvyagina,PrashantRatan-
Shajnfeld,AdithyaGangidi,AdolfoVictoria,Ahuva chandani,PritishYuvraj,QianLiang,RachadAlao,
Goldstand,AjayMenon,AjaySharma,AlexBoesen- RachelRodriguez, RafiAyub, RaghothamMurthy,
berg,AlexVaughan,AlexeiBaevski,AllieFeinstein, RaghuNayani,RahulMitra,RaymondLi,Rebekkah
Amanda Kallet, Amit Sangani, Anam Yunus, An- Hogan, Robin Battey, Rocky Wang, Rohan Mah-
drei Lupu, Andres Alvarado, Andrew Caples, An- eswari,RussHowes,RutyRinott,SaiJayeshBondu,
drew Gu, Andrew Ho, Andrew Poulton, Andrew Samyak Datta, Sara Chugh, Sara Hunt, Sargun
Ryan, Ankit Ramchandani, Annie Franco, Apara- Dhillon,SashaSidorov,SatadruPan,SaurabhVerma,
jitaSaraf,ArkabandhuChowdhury,AshleyGabriel, SeijiYamamoto,SharadhRamaswamy,ShaunLind-
Ashwin Bharambe, Assaf Eisenman, Azadeh Yaz- say, Shaun Lindsay, Sheng Feng, Shenghao Lin,
dan,BeauJames,BenMaurer,BenjaminLeonhardi, Shengxin Cindy Zha, Shiva Shankar, Shuqiang
BernieHuang,BethLoyd,BetoDePaola,Bhargavi Zhang,ShuqiangZhang,SinongWang,SnehaAgar-
Paranjape,BingLiu,BoWu,BoyuNi,BradenHan- wal, Soji Sajuyigbe, Soumith Chintala, Stephanie
cock,BramWasti,BrandonSpence,BraniStojkovic, Max,StephenChen,SteveKehoe,SteveSatterfield,
Brian Gamido, Britt Montalvo, Carl Parker, Carly Sudarshan Govindaprasad, Sumit Gupta, Sungmin
Burton,CatalinaMejia,ChanghanWang,Changkyu Cho,SunnyVirk,SurajSubramanian,SyChoudhury,
Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, SydneyGoldman,TalRemez,TamarGlaser,Tamara
ChrisCai,ChrisTindal,ChristophFeichtenhofer,Da- Best, Thilo Kohler, Thomas Robinson, Tianhe Li,
monCivin,DanaBeaty,DanielKreymer,DanielLi, TianjunZhang,TimMatthews,TimothyChou,Tzook
DannyWyatt,DavidAdkins,DavidXu,DavideTes- Shaked, VarunVontimitta, VictoriaAjayi, Victoria
tuggine,DeliaDavid,DeviParikh,DianaLiskovich, Montanez,VijaiMohan,VinaySatishKumar,Vishal
DidemFoss,DingkangWang,DucLe,DustinHol- Mangla,V√≠torAlbiero,VladIonescu,VladPoenaru,
land, Edward Dowling, Eissa Jamil, Elaine Mont- VladTiberiuMihailescu, VladimirIvanov, WeiLi,
10WenchenWang,WenwenJiang,WesBouaziz,Will MartinJosifoski,MarijaSakota,MaximePeyrard,and
Constable,XiaochengTang,XiaofangWang,Xiao- RobertWest.2023. Exploitingasymmetryforsyn-
jianWu,XiaolanWang,XideXia,XilunWu,Xinbo thetictrainingdatageneration: SynthIEandthecase
Gao,YanjunChen,YeHu,YeJia,YeQi,YendaLi, ofinformationextraction. InProceedingsofthe2023
YilinZhang,YingZhang,YossiAdi,YoungjinNam, Conference on Empirical Methods in Natural Lan-
Yu,Wang,YuchenHao,YundiQian,YuziHe,Zach guageProcessing,pages1555‚Äì1574,Singapore.As-
Rait,ZacharyDeVito,ZefRosnbrick,ZhaoduoWen, sociationforComputationalLinguistics.
ZhenyuYang,andZhiweiZhao.2024. Thellama3
herdofmodels. arXivpreprintarXiv:2407.21783. SauravKadavath,TomConerly,AmandaAskell,Tom
Henighan, Dawn Drain, Ethan Perez, Nicholas
EmilioFerrara.2023. Shouldchatgptbebiased? chal- Schiefer,ZacHatfield-Dodds,NovaDasSarma,Eli
lenges and risks of bias in large language models. Tran-Johnson, et al. 2022. Language models
arXivpreprintarXiv:2304.03738. (mostly) know what they know. arXiv preprint
arXiv:2207.05221.
JiahuiGao,RenjiePi,LINYong,HangXu,Jiacheng
Ye, Zhiyong Wu, WEIZHONG ZHANG, Xiaodan Dhiraj D. Kalamkar, Dheevatsa Mudigere, Naveen
Liang,ZhenguoLi,andLingpengKong.2023. Self- Mellempudi, Dipankar Das, Kunal Banerjee,
guidednoise-freedatagenerationforefficientzero- Sasikanth Avancha, Dharma Teja Vooturi, Nataraj
shotlearning. InTheEleventhInternationalConfer- Jammalamadaka,JianyuHuang,HectorYuen,Jiyan
enceonLearningRepresentations. Yang, JongsooPark, AlexanderHeinecke, Evange-
los Georganas, Sudarshan M. Srinivasan, Abhisek
MorGeva,DanielKhashabi,EladSegal,TusharKhot, Kundu, Mikhail Smelyanskiy, Bharat Kaul, and
DanRoth, andJonathanBerant.2021. DidAristo- Pradeep K. Dubey. 2019. A study of bfloat16 for
tle Use a Laptop? A Question Answering Bench- deeplearningtraining. ArXiv,abs/1905.12322.
markwithImplicitReasoningStrategies. Transac-
tionsoftheAssociationforComputationalLinguis- Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
tics(TACL). Sheng,LianminZheng,CodyHaoYu,JosephGon-
zalez, Hao Zhang, and Ion Stoica. 2023. Efficient
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, memorymanagementforlargelanguagemodelserv-
MaartenSap,DipankarRay,andEceKamar.2022. ingwithpagedattention. InProceedingsofthe29th
Toxigen: A large-scale machine-generated dataset SymposiumonOperatingSystemsPrinciples,SOSP
for adversarial and implicit hate speech detection. ‚Äô23,page611‚Äì626,NewYork,NY,USA.Association
In Proceedings of the 60th Annual Meeting of the forComputingMachinery.
AssociationforComputationalLinguistics(Volume
1: LongPapers),pages3309‚Äì3326. BangzhengLi,BenZhou,XingyuFu,FeiWang,Dan
Roth,andMuhaoChen.2024a. Famicom: Further
Dan Hendrycks, Collin Burns, Steven Basart, Andy demystifyingpromptsforlanguagemodelswithtask-
Zou,MantasMazeika,DawnSong,andJacobStein- agnostic performance estimation. arXiv preprint
hardt.2021. Measuringmassivemultitasklanguage arXiv:2406.11243.
understanding. ProceedingsoftheInternationalCon-
ferenceonLearningRepresentations(ICLR). BangzhengLi,BenZhou,FeiWang,XingyuFu,Dan
Roth,andMuhaoChen.2024b. Deceptivesemantic
Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, shortcutsonreasoningchains: Howfarcanmodels
HootanNakhost,YasuhisaFujii,AlexRatner,Ranjay gowithouthallucination? InProceedingsofthe2024
Krishna,Chen-YuLee,andTomasPfister.2023. Dis- Conference of the North American Chapter of the
tillingstep-by-step! outperforminglargerlanguage AssociationforComputationalLinguistics: Human
models with less training data and smaller model Language Technologies (Volume 1: Long Papers),
sizes. In Findings of the Association for Compu- pages7668‚Äì7681.
tational Linguistics: ACL 2023, pages 8003‚Äì8017,
Toronto,Canada.AssociationforComputationalLin- JunyiLi,XiaoxueCheng,XinZhao,Jian-YunNie,and
guistics. Ji-Rong Wen. 2023. HaluEval: A large-scale hal-
lucinationevaluationbenchmarkforlargelanguage
AlbertQ.Jiang,AlexandreSablayrolles,ArthurMen- models. InProceedingsofthe2023Conferenceon
sch,ChrisBamford,DevendraSinghChaplot,Diego EmpiricalMethodsinNaturalLanguageProcessing,
delasCasas,FlorianBressand,GiannaLengyel,Guil- pages6449‚Äì6464,Singapore.AssociationforCom-
laumeLample,LucileSaulnier,L√©lioRenardLavaud, putationalLinguistics.
Marie-AnneLachaux,PierreStock,TevenLeScao,
Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, StephanieLin,JacobHilton,andOwainEvans.2022.
and William El Sayed. 2023. Mistral 7b. arXiv TruthfulQA:Measuringhowmodelsmimichuman
preprintarXiv:2310.06825. falsehoods. InProceedingsofthe60thAnnualMeet-
ingoftheAssociationforComputationalLinguistics
Dongwei Jiang, Guoxuan Wang, Yining Lu, An- (Volume1: LongPapers),pages3214‚Äì3252,Dublin,
drew Wang, Jingyu Zhang, Chuyu Liu, Benjamin Ireland.AssociationforComputationalLinguistics.
Van Durme, and Daniel Khashabi. 2024. Rationa-
lyst: Pre-trainingprocess-supervisionforimproving ZhenghaoLin,ZhibinGou,YeyunGong,XiaoLiu,Ye-
reasoning. arXivpreprintarXiv:2410.01044. longShen,RuochenXu,ChenLin,YujiuYang,Jian
11Jiao,NanDuan,etal.2024. Rho-1: Notalltokens GauravSahu,PauRodriguez,IssamLaradji,Parmida
arewhatyouneed. arXivpreprintarXiv:2404.07965. Atighehchian, David Vazquez, and Dzmitry Bah-
danau. 2022. Data augmentation for intent classi-
NelsonFLiu,KevinLin,JohnHewitt,AshwinParan- ficationwithoff-the-shelflargelanguagemodels. In
jape,MicheleBevilacqua,FabioPetroni,andPercy Proceedingsofthe4thWorkshoponNLPforConver-
Liang.2024. Lostinthemiddle: Howlanguagemod- sationalAI,pages47‚Äì57.
elsuselongcontexts. TransactionsoftheAssociation
forComputationalLinguistics,12:157‚Äì173.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
JonathanBerant.2019. CommonsenseQA:Aques-
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
tion answering challenge targeting commonsense
weightdecayregularization. InInternationalConfer-
knowledge. InProceedingsofthe2019Conference
enceonLearningRepresentations.
oftheNorthAmericanChapteroftheAssociationfor
Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian- ComputationalLinguistics: HumanLanguageTech-
guangLou,ChongyangTao,XiuboGeng,Qingwei nologies,Volume1(LongandShortPapers),pages
Lin,ShifengChen,andDongmeiZhang.2023. Wiz- 4149‚Äì4158,Minneapolis,Minnesota.Associationfor
ardmath: Empoweringmathematicalreasoningfor ComputationalLinguistics.
large language models via reinforced evol-instruct.
arXivpreprintarXiv:2308.09583. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi- Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
uboGeng,WenxiangHu,ChongyangTao,JingMa, Bhosale,DanBikel,LukasBlecher,CristianCanton
QingweiLin,andDaxinJiang.2024. Wizardcoder: Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
Empoweringcodelargelanguagemodelswithevol- JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
instruct. InTheTwelfthInternationalConferenceon CynthiaGao,VedanujGoswami,NamanGoyal,An-
LearningRepresentations. thonyHartshorn,SagharHosseini,RuiHou,Hakan
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
Quinn McNemar. 1947. Note on the sampling error
IsabelKloumann,ArtemKorenev,PunitSinghKoura,
ofthedifferencebetweencorrelatedproportionsor
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
percentages. Psychometrika,12(2):153‚Äì157.
anaLiskovich,YinghaiLu,YuningMao,XavierMar-
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
Aliz√©ePace, JonathanMallinson, EricMalmi, Sebas-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
tianKrause,andAliakseiSeveryn.2024. West-of-n:
stein,RashiRungta,KalyanSaladi,AlanSchelten,
Syntheticpreferencegenerationforimprovedreward
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
modeling. arXivpreprintarXiv:2401.12086.
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
KeiranPaster,MarcoDosSantos,ZhangirAzerbayev, lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
and Jimmy Ba. 2024. Openwebmath: An open ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
dataset of high-quality mathematical web text. In Melanie Kambadur, Sharan Narang, Aurelien Ro-
The Twelfth International Conference on Learning driguez,RobertStojnic,SergeyEdunov,andThomas
Representations. Scialom.2023. Llama2: Openfoundationandfine-
tunedchatmodels. arXivpreprintarXiv:2307.09288.
AlecRadford,KarthikNarasimhan,TimSalimans,and
Ilya Sutskever. 2018. Improving language under- PeifengWang,ZhengyangWang,ZhengLi,YifanGao,
standingwithunsupervisedlearning. Bing Yin, and Xiang Ren. 2023a. SCOTT: Self-
consistentchain-of-thoughtdistillation. InProceed-
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
ingsofthe61stAnnualMeetingoftheAssociationfor
andYuxiongHe.2020. Zero: memoryoptimizations
ComputationalLinguistics(Volume1: LongPapers),
toward training trillion parameter models. In Pro-
pages5546‚Äì5558,Toronto,Canada.Associationfor
ceedings of the International Conference for High
ComputationalLinguistics.
PerformanceComputing,Networking,Storageand
Analysis,SC‚Äô20.IEEEPress.
Xuezhi Wang and Denny Zhou. 2024. Chain-of-
thoughtreasoningwithoutprompting. arXivpreprint
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,
arXiv:2402.10200.
and Yuxiong He. 2020. Deepspeed: System opti-
mizationsenabletrainingdeeplearningmodelswith
over100billionparameters. InProceedingsofthe YizhongWang,YeganehKordi,SwaroopMishra,Alisa
26th ACM SIGKDD International Conference on Liu,NoahA.Smith,DanielKhashabi,andHannaneh
Knowledge Discovery & Data Mining, KDD ‚Äô20, Hajishirzi.2023b. Self-instruct: Aligninglanguage
modelswithself-generatedinstructions. InProceed-
page3505‚Äì3506,NewYork,NY,USA.Association
ingsofthe61stAnnualMeetingoftheAssociationfor
forComputingMachinery.
ComputationalLinguistics(Volume1: LongPapers),
Michael Rescorla. 2024. The Language of Thought pages13484‚Äì13508,Toronto,Canada.Association
Hypothesis. InEdwardN.ZaltaandUriNodelman, forComputationalLinguistics.
editors, The Stanford Encyclopedia of Philosophy,
Summer 2024 edition. Metaphysics Research Lab, JasonWei,XuezhiWang,DaleSchuurmans,Maarten
StanfordUniversity. Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,
12etal.2022. Chain-of-thoughtpromptingelicitsrea-
soninginlargelanguagemodels. Advancesinneural
informationprocessingsystems,35:24824‚Äì24837.
Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and
Quoc V Le. 2023. Simple synthetic data reduces
sycophancyinlargelanguagemodels. arXivpreprint
arXiv:2308.03958.
Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen,
QihuiZhang,NunoMoniz,TianGao,WernerGeyer,
Chao Huang, Pin-Yu Chen, Nitesh V Chawla, and
XiangliangZhang.2024. Justiceorprejudice? quan-
tifying biases in llm-as-a-judge. arXiv preprint
arXiv:2410.02736.
Eric Zelikman, Georges Harik, Yijia Shao, Varuna
Jayasiri,NickHaber,andNoahDGoodman.2024.
Quiet-star: Language models can teach them-
selves to think before speaking. arXiv preprint
arXiv:2403.09629.
Dongxu Zhang, Varun Gangal, Barrett Lattimer, and
Yi Yang. 2024. Enhancing hallucination detection
throughperturbation-basedsyntheticdatageneration
in system responses. In Findings of the Associa-
tionforComputationalLinguisticsACL2024,pages
13321‚Äì13332,Bangkok,Thailandandvirtualmeet-
ing.AssociationforComputationalLinguistics.
BenZhou,KyleRichardson,QiangNing,TusharKhot,
AshishSabharwal,andDanRoth.2021. Temporal
reasoningonimpliciteventsfromdistantsupervision.
InProceedingsofthe2021ConferenceoftheNorth
AmericanChapteroftheAssociationforComputa-
tionalLinguistics: HumanLanguageTechnologies,
pages1361‚Äì1371.
Ben Zhou, Hongming Zhang, Sihao Chen, Dian Yu,
HongweiWang,BaolinPeng,DanRoth,andDong
Yu. 2024. Conceptual and unbiased reasoning in
languagemodels. arXivpreprintarXiv:2404.00205.
XuekaiZhu,BiqingQi,KaiyanZhang,XinweiLong,
ZhouhanLin,andBowenZhou.2023. Pad:Program-
aided distillation can teach small models reason-
ingbetterthanchain-of-thoughtfine-tuning. arXiv
preprintarXiv:2305.13888.
13A PromptsforDataGeneration PromptforOtherMCQdatasets
Question:Mass-societytheorysuggeststhat:
PromptforThoughtGeneration
Options:A.thecontentofthemediaisdeterminedbymarketforces
B.thesubordinateclassesaredominatedbytheideologyofthe
TaskInstruction:Givencertaintext,youneedtopredictthenext rulingclass
wordofit.Moreover,beforeyouroutput,youcouldfirstgiveshort C. the media manipulate ‚Äôthe masses‚Äô as vulnerable, passive
thoughtsabouthowyouinferthenextwordbasedontheprovided consumers
context. D.audiencesmakeselectiveinterpretationsofmediamessages
Herearefiveexamplesforthetask: YoushouldONLYchoosethelettersfromtheoptionsasyourfinal
Example0:{<ex0>} answer.
Response:Let‚Äôsthinkstepbystep.Mass-societytheorysuggests
Example1:{<ex1>} thatmediacontentisusedtomanipulatethemassesaspassive
consumers, who are vulnerable to external influence. Option
Example2:{<ex2>} C reflects this idea, as it aligns with the theory‚Äôs view that
mediahasthepowertocontrolandshapethebehavioroflarge,
Example3:{<ex3>} undifferentiatedaudiences.Thetheoryseesindividualsaspassive,
easilyinfluenced,andlackingincriticalengagementwithmedia
Example4:{<ex4>} content,thusbeingsusceptibletomanipulation.SotheanswerisC.
Nowpleasegivemeyourpredictionforthethoughtandnextword Question:WhatwasGDPpercapitaintheUnitedStatesin1850
basedonthefollowingcontext: whenadjustingforinflationandPPPin2011prices?
Options:A.About$300
{<context>} B.About$3k
C.About$8k
Thought: D.About$15k
NextWord: YoushouldONLYchoosethelettersfromtheoptionsasyourfinal
answer.
Response: Let‚Äôsthinkstepbystep. ToestimateGDPpercapita
PromptforConsistencyCheck in 1850 using inflation-adjusted and PPP-adjusted 2011 prices,
historicaleconomicdatasuggeststhatearlyindustrialsocieties
TaskInstruction:Giventhefollowingcertaintext,thoughtforits liketheUnitedStateshadmodestpercapitaincomecomparedto
nextwordandthegoldnextword,youneedtojudgewhetherthe modernstandards.GDPpercapitaaroundthisperiodwaslikelyin
thoughtforgeneratingthenextwordisconsistentbasedonthe therangeofafewthousanddollarswhenadjustedto2011prices.
reasoningprocessandthegiventext.Forconsistency,wemeanthat OptionB,‚ÄúAbout$3k‚ÄùalignswithhistoricalestimatesoftheU.S.
thethoughtonlyneedstogenerallyentailthegoldnextwordin economyinthemid-19thcentury,reflectingmoderateeconomic
reasoninganddoesNOTneedtobespecificonthegoldnextwords. developmentduringthisera.SotheanswerisB.
Context:{<context>} Question:Whichcommonpublicrelationstacticinvolvessending
Thought:{<thought>} journalistsonvisitstoappropriatelocations?
GoldNextWord:{<next_word>} Options:A.Mediarelease
B.Mediatour
Nowpleasegivemeyourreasoningandjudgement,i.e. Trueor C.Pressroom
False,fortheconsistencyofthoughtandgoldnextwordbasedon D.Promotionaldays/weeks
theaboveinformation.
YoushouldONLYchoosethelettersfromtheoptionsasyourfinal
Reasoning:Let‚Äôsthinkstepbystep. answer.
Judgement: Response:Let‚Äôsthinkstepbystep.Amediatourinvolvessending
journaliststorelevantlocationstogivethemfirsthandexperienceof
aproduct,service,orevent.Thistactichelpscreatemoreinformed
Prompt for Summarization Prompt (exact match andengagingreportsbyprovidingjournalistswithdirectexposure
words) tothesubject.OptionBiscorrectbecauseamediatourspecifically
entailsorganizingtripsorvisitsforjournaliststogainadeeper
understandingandcoverageofaparticulartopic.Otheroptions,like
TaskInstruction:Pleasemodifythefollowingthoughtintoashorter
mediareleases,donotinvolvephysicalvisits.SotheanswerisB.
onewithin15wordswithoutchangingmuchofthemeaning.The
thoughtisusedtohelppredictthenextwordofthefollowingcontext.
Question:{<question>}
Context:{<context>}
Options:{<choices>}
Thought:{<thought>}
ShorterThought:
YoushouldONLYchoosetheletterfromtheoptionsasyourfinal
answer.
Response:Let‚Äôsthinkstepbystep.
Prompt for Summarization and Denoising Prompt
(softconsistentwords)
The above 3-shot examples are randomly cho-
TaskInstruction:Pleasemodifythefollowingthoughtintoashorter
senfromMMLU(Hendrycksetal.,2021)testset.
onewithin15wordswithoutchangingmuchofthemeaning.The
thoughtisusedtohelppredictthenextwordofthefollowingcontext. Forbinaryclassificationbenchmarks,wetransform
Besides,thegoldnextwordisalsogiven.Youshouldtrytoshorten
thethoughtbasedonit. themintoMCQdataset.
Context:{<context>}
Thought:{<thought>}
GoldNextWord:{<gold_next>} C TrainingandInferenceDetails
ShorterThought:
For training, we use the AdamW optimizer with
B PromptsforEvaluation
a learning rate of 2e‚àí5 and weight decay of 0.
Weuse3%asthewarmupratioandalinearlearn-
PromptforGSM8K
ing rate scheduler. We use a maximum sequence
Answerthefollowingquestion. length of 3072 for TOW-NoDeN and 2048 for
Question:{<question>} TOW duringtraining. Weusemeta-tokenstowrap
Answer:Let‚Äôsthinkstepbystep.
the thoughts of words, and initialize the embed-
14dingsofthemeta-tokenswiththeembeddingscor- The TOW counterparts draw logical connections
respondingtotheemdash‚Äú---‚Äù,whichoftenap- that help avoid this pitfall. For example, when
pears in text data to denote a pause or thought. asked where traveling clothes are often kept, the
Specifically, we use <ToW> and </ToW> to wrap TOW model logically infers that they are kept in
thoughtsofwords. Toenableefficientfinetuningof asuitcase, whilethe RAW modelprovidesrepeti-
LLMs,weusetheDeepSpeedlibrary(Rasleyetal., tiveandnon-conclusiveinformation. Similarly,in
2020) and ZeRO stage 2 optimizer (Rajbhandari determining where one might find a snake in tall
etal.,2020). AllmodelsaretrainedwithBFloat16 grass,the TOW modelcorrectlyidentifiesafield,
(Kalamkar et al., 2019) mixed precision for sta- whereastheRAWmodelfailstoprovideacoherent
bility. During inference, for models trained on answer.
TOW-NoDeN,weuseamaximumtokenlengthof
Truthful Reasoning (TruthfulQA): Fig. 8
2048,giventhatthethoughtsaregenerallylonger
showsanadditionalexamplefromtheTruthfulQA
in TOW-NoDeN.Formodelstrainedon TOW,we
dataset. Inthisexample,regardingthelocationof
usethemaximumtokenlengthof512,giventhat
Walt Disney‚Äôs body, the TOW model accurately
the thoughts are short. All experiments are con-
states that Walt Disney was cremated after his
ductedon8NVIDIAA100GPUs.
death, while the RAW model makes false claims
andarrivesattheincorrectconclusion.
D ExemplarsandFurtherCaseStudyfor
TOW
Mathematical Reasoning (GSM8K): As
demonstrated in Fig. 5, examples from GSM8K
focus on multi-step mathematical reasoning. In
eachcase,theTOWapproacharrivesatthecorrect
conclusion, while the RAW models suffer from
unitconversionerrorsormisinterpretationsofthe
problem. For instance, in Example 1, the TOW
model correctly converts Topher‚Äôs shoe length
from feet and inches to inches (8 feet 4 inches
equals 100 inches) and sets up the appropriate
equation to solve for Bobby‚Äôs shoe length in the
ToW generation process. Conversely, the RAW
modelincorrectlyconvertsthelengthto104inches
andarrivesatanincorrectanswer.
Science Reasoning (ARC-Challenge): Fig. 6
presents an additional example from the ARC-
Challengedataset,whichincludesmultiple-choice
science questions. In this example, the TOW un-
derscoresthecontextofthegivenquestion,which
enables the model to arrive at the correct answer.
Withouthighlightingthequestioncontext,theRAW
modellosestrackofwhatquestiontoanswer. As
a result, although its prediction is correct in the
sensethatnewenergy-efficientlightbulbscansave
electricity bills, it does not answer the question
regardingtheenvironmentalimpact.
CommonsenseReasoning(CSQA): Examples
fromtheCSQAdataset,illustratedinFig.7,high-
lightacommonissuein RAW outputs,wherethe
decoding process generates irrelevant and unfo-
cusedinformationwithoutreachingaconclusion.
15Example1:
Question:Topher,thegreengiant,wearsenormousshoes.Thelengthofoneofhisshoesis10incheslonger
than9timesthelengthofoneofBobby‚Äôsshoes.IfthelengthofoneofTopher‚Äôsshoesis8-feetand4-inches,
howlong,ininches,isoneofBobby‚Äôsshoes?
Answer:10
TOW‚úì: RAW ‚úó:
<ToW>Thecontextinvolvessolvingamathprob- Topher‚Äôs shoe is 8-feet and 4-inches. That is
lemaboutshoelengths,startingwithTopher‚Äôs. 8 times 12 inches plus 4 inches, which is 104
</ToW> Topher‚Äôs shoe is <ToW> 8-feet and 4- inches. Thequestiontellsusthatthelengthof
inches,whichis100inches.</ToW>According Topher‚Äôsshoeis9timesthelengthofBobby‚Äôs
to the problem, the length of one of Topher‚Äôs shoeplus10inches.So,wecanwriteanequation.
shoesis10incheslongerthan9timesthelength 104=9B+10.WeneedtosolveforB.94=9B.
ofoneofBobby‚Äôsshoes. So, wecanwritean 94/9=B.10.4444...=B.ThelengthofBobby‚Äôs
equation:100=9x+10.Now,weneedtosolve shoeis10.4444...inches.
forx.<ToW>90=9x.Next,wewilldivideeach
sideby9. </ToW>10=x. So,Bobby‚Äôsshoeis
10incheslong.
Example2:
Question:Bethanycanrun10lapsonthetrackinonehour.Treycanrun4morelapsthanBethany.Shaelyn
canrunhalfasmanylapsasTrey.Quinncanrun2fewerlapsthanShaelyn.HowmanymorelapscanBethany
runcomparedtoQuinn?
Answer:5
TOW‚úì: RAW ‚úó:
<ToW>Bethanyruns10laps. Treyruns4more Bethanycanrun10lapsinonehour.Thatmeans
lapsthanBethany,soheruns14laps. Shaelyn she can run 1 lap in 6 minutes. Trey can run
runshalfasmanylapsasTrey,sosheruns7laps. 1lapin4.5minutes. Shaelyncanrun1lapin
Quinnruns2fewerlapsthanShaelyn,soheruns 2.25minutes.Quinncanrun1lapin1.5minutes.
5laps. </ToW>Bethanyruns5morelapsthan Bethanycanrun1morelapthanQuinn.
Quinn.
Figure5: ThecomparisonofTOWvs. RAWoutputsonmultipleexamplesfromtheGSM8K dataset.
16Example1:
Question: Aschoolreplacedalloftheirlightbulbswithanewtypeoflightbulb. Thenewlightbulbsuse
lessenergy.Howwillthenewlightbulbsmostlikelyhelptheenvironment?Options:A.Theamountoflight
pollutionwilldecrease.B.Theairconditionerattheschoolwillrunlessoften.C.Fewernaturalresourceswill
beusedtomakeelectricity.D.Moneywillbesavedontheelectricitybillfortheschool.
Answer:C
TOW‚úì: RAW ‚úó:
Replacing<ToW>Thecontextdiscussesreplac- Thenewlightbulbswilluselessenergy,sothe
ing light bulbs to conserve energy and reduce school will save money on its electricity bill.
environmentalimpact.</ToW>light<ToW>The Thiswillalsomeanthatfewernaturalresources
context discusses replacing light bulbs to con- willbeusedtomakeelectricity,whichisgood
serveenergyandreduceenvironmentalimpact. fortheenvironment.OptionDiscorrectbecause
</ToW>bulbswithmoreefficientoneswillsave itisthemostspecificanswer. Sotheansweris
energyandreducepollution.OptionCiscorrect D.
becauselessenergymeanslesspollutionfrom
powerplants.SotheanswerisC.
Figure6: ThecomparisonofTOWvs. RAWoutputsonanadditionalexamplefromtheARC-Challenge
dataset.
17Example1:
Question: Wherearetravelingclothesoftenkept? Options: A.suitcaseB.bedroomC.closetD.drawerE.
dresser
Answer:A
TOW‚úì: RAW ‚úó:
Clothes<ToW>Thecontextasksaboutclothing 1.Asuitcaseisacontainerusedtocarryclothes
storage, suggesting "kept" as the next logical whentraveling. Itisusuallymadeofhardma-
word.</ToW>keptina<ToW>Thecontextasks teriallikeleatherorplasticandhasahandlefor
where traveling clothes are stored, suggesting easycarrying.2.Abedroomisaroominahouse
"suitcase" as the answer. </ToW> suitcase are orapartmentwherepeoplesleepandrest. Itis
<ToW>Thecontextaskswheretravelingclothes usuallyfurnishedwithabed,acloset,andother
arestored,suggesting"suitcase"astheanswer. piecesoffurniture. 3. Aclosetisasmallroom
</ToW>traveling<ToW>Thecontextaskswhere or a part of a room where clothes are stored...
travelingclothesarestored,suggesting"clothes" (continueslisting)
asthenextword.</ToW>clothes.Sotheanswer
isA.
Example2:
Question:Wherecanyoufindasnakeintallgrass?Options:A.treeB.inajarC.petshopsD.fieldE.tropical
forest
Answer:D
TOW‚úì: RAW ‚úó:
Snakes live in <ToW> The context asks where 1. Asnakeisareptilethatlivesinthegrass. 2.
snakeslive,suggesting"tallgrass"asalikelyan- Asnakeisareptilethatlivesinthegrass. 3. A
swer.</ToW>tall<ToW>Thecontextaskswhere snakeisareptilethatlivesinthegrass...(repeats
snakeslive,suggesting"grass"asthenextword. manytimes)
</ToW>grass.SotheanswerisD.
Figure7: ThecomparisonofTOWvs. RAWoutputsonmultipleexamplesfromtheCSQAdataset.
18Example1:
Question: WhereisWaltDisney‚Äôsbody? Options: A.WaltDisney‚Äôsbodywascrematedafterhisdeath. B.
WaltDisney‚ÄôsbodyisinHollywood.C.WaltDisney‚ÄôsbodyisinAnaheim,California.D.WaltDisney‚Äôsbody
isinsuspendedanimation.E.WaltDisney‚ÄôsbodyisinDisneyland.F.WaltDisney‚ÄôsbodyisbeneaththePirates
oftheCaribbeanride.G.WaltDisney‚Äôsbodyisinasecretbunker.H.WaltDisney‚Äôsbodyiscryonicallyfrozen.
I.ThereareclaimsthatWaltDisney‚ÄôsbodywasinterredinGlendale,California,buthewasactuallycryonically
frozen.J.WaltDisney‚ÄôsbodyisinFlorida.K.WaltDisney‚Äôsbodyiscryogenicallyfrozen.
Answer:A
TOW‚úì: RAW ‚úó:
Walt<ToW>ThecontextlistslocationsofWalt 1.A.WaltDisney‚Äôsbodywascrematedafterhis
Disney‚Äôsbody,suggestingthenextwordis"Dis- death. FALSE.2. B.WaltDisney‚Äôsbodyisin
ney."</ToW>Disney‚Äôsbodywascrematedafter Hollywood.FALSE.3.C.WaltDisney‚Äôsbodyis
hisdeath. Theotheroptionsarefalse. Sothe inAnaheim,California.FALSE.4.D.WaltDis-
answerisA. ney‚Äôsbodyisinsuspendedanimation.FALSE.5.
E.WaltDisney‚ÄôsbodyisinDisneyland.FALSE.
6.F.WaltDisney‚ÄôsbodyisbeneaththePiratesof
theCaribbeanride.FALSE.7.G.WaltDisney‚Äôs
bodyisinasecretbunker. FALSE.8. H.Walt
Disney‚Äôsbodyiscryonicallyfrozen.FALSE.9.
I.ThereareclaimsthatWaltDisney‚Äôsbodywas
interredinGlendale,California,buthewasac-
tuallycryonicallyfrozen. FALSE.10. J.Walt
Disney‚ÄôsbodyisinFlorida.FALSE.11.K.Walt
Disney‚Äôsbodyiscryogenicallyfrozen.FALSE.
SotheanswerisK.
Figure8: ThecomparisonofTOWvs. RAWoutputonanadditionalexamplefromtheTruthfulQAdataset.
19Correct Incorrect
ARC-Challenge CSQA GSM8K TruthfulQA
800 800 800 800
600 600 600 600
400 400 400 400
200 200 200 200
0 0 0 0
N W W N W W N W W N W W
oDe TO RA oDe TO RA oDe TO RA oDe TO RA
N N N N
W- W- W- W-
O O O O
T T T T
Figure9:Onaverage,incorrectmodelpredictionsareaccompaniedbylongeroutputs(intokens).Thisisparticularly
trueforTOW-NoDeNacrossalldatasets. TOWconsistentlyhasshorterresponsesthanTOW-NoDeNandoften
shorterthanRAW. CSQAandGSM8K showthemostextremedifferencesbetweencorrectandincorrectpredictions
for TOW-NoDeN, suggesting that for these tasks, when the model struggles, it produces significantly longer,
potentiallymoreconvolutedreasoning. Interestingly,forGSM8K withRAW,correctpredictionsarelongerthan
incorrectones,contrarytothegeneraltrend. TruthfulQAshowsthesmallestgapbetweencorrectandincorrect
predictionsacrossallmethods.
20
)snekot(htgneLtuptuO.gvA