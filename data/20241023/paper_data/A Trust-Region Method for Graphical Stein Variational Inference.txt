A Trust-Region Method for Graphical Stein Variational Inference
Liam Pavlovic David M. Rosen
Northeastern University
Abstract 1 Introduction
Drawing inferences from noisy data is a fundamental
Steinvariationalinference(SVI)isasample- capability in artificial intelligence, machine learning,
based approximate Bayesian inference tech- and scientific and engineering applications. Mathe-
nique that generates a sample set by jointly matically, this procedure is naturally expressed in the
optimizing the samples’ locations to mini- language of posterior Bayesian inference. Many of
mizeaninformation-theoreticmeasureofdis- these inference problems can be formulated as prob-
crepancy with the target probability distri- abilisticgraphicalmodels(PGMs),whichareaneffec-
bution. SVI thus provides a fast and sig- tive tool for modeling joint distributions with known
nificantly more sample-efficient approach to conditional independences among the individual vari-
Bayesianinferencethantraditional(random- ables. The conditional independence structure en-
sampling-based) alternatives. However, the coded in a PGM can greatly simplify the inference
optimization techniques employed in exist- task [Koller and Friedman, 2009]. Nonetheless, ex-
ing SVI methods struggle to address prob- actBayesianinferenceistypicallycomputationallyin-
lems in which the target distribution is high- tractable, so in practice approximate inference meth-
dimensional, poorly-conditioned, or non- ods are used instead.
convex, which severely limits the range of
One of the most common approximate Bayesian infer-
their practical applicability. In this paper,
ence methods is sample-based approximation, which
we propose a novel trust-region optimiza-
uses a sample set to represent the target distribu-
tion approach for SVI that successfully ad-
tion. This approximation has the benefits of simplic-
dresseseachofthesechallenges. Ourmethod
ity, flexibility, arbitrary precision (as sample size in-
builds upon prior work in SVI by leveraging
creases), and easy empirical estimation of any statis-
conditional independences in the target dis-
tic over the target distribution. Traditional meth-
tribution (to achieve high-dimensional scal-
ods for generating a sample-based approximation are
ing) and second-order information (to ad-
basedonrandomsampling. Commonexamplesofran-
dress poor conditioning), while additionally
domsamplingalgorithmsincludeMarkovchainMonte
providing an effective adaptive step control
Carlo (MCMC) [Andrieu et al., 2003], nested sam-
procedure, which is essential for ensuring
pling [Speagle, 2020], and Hamiltonian Monte Carlo
convergence on challenging non-convex op-
[Betancourt, 2017]. Due to their dependence on ran-
timization problems. Experimental results
dom processes to explore the state space, these meth-
showourmethodachievessuperiornumerical
ods can be slow to converge and sample inefficient.
performance, both in convergence rate and
These deficiencies become especially pronounced in
sample accuracy, and scales better in high-
high-dimensional problems.
dimensionaldistributions, thanpreviousSVI
techniques. Stein variational inference (SVI) is a more efficient
alternative for generating a sample-based approxima-
tion [Liu and Wang, 2016]. In place of random sam-
pling, SVI deterministically optimizes a set number of
samplestominimizeKLdivergencewiththereference
distribution. SVI has been demonstrated to have su-
perior sample efficiency over random sampling meth-
ods, capturing more information with fewer samples
[Liu and Wang, 2016]. However,SVIcanstillstruggle
4202
tcO
12
]GL.sc[
1v59161.0142:viXraA Trust-Region Method for Graphical Stein Variational Inference
to scale to high-dimensional, ill-conditioned, and non- vector-valued RKHS HD because this space has a
convex objectives. Previous work [Zhuo et al., 2018, closed form for the desired functional gradient ∇J[0]
Wang et al., 2018, Detommaso et al., 2018] on SVI [Liu and Wang, 2016]
methods has addressed some of these challenges indi-
∇J[0](x)=−E [k(z,x)∇ logp(z)
vidually, but no single previous SVI method for PGM z∼q z
problems handles all these challenges well. +∇ k(z,x)] (3)
z
In this paper, we propose a novel trust-region op-
In order to compute the expectation above, we need
timization approach for SVI that successfully ad-
some tractable representation of the distribution q. A
dresses each of these challenges. Our method
naturalchoiceforthisrepresentationisasample,since
builds upon prior work by leveraging both condi-
this parameterization is both flexible, and makes the
tional independences in the target distribution to
expectation in Eq. 3 trivial to approximate.
achieve high-dimensional scaling [Zhuo et al., 2018,
Wang et al., 2018] and second-order information to Stein variational gradient descent (SVGD)
addresspoorconditioning[Detommaso et al., 2018]in [Liu and Wang, 2016] combines a sample-based
thesamesystem. Wealsoprovideaneffectiveadaptive approximation of q and the descent direction in Eq. 3
step control procedure for SVI, which is essential for into an iterative procedure for generating an sample-
ensuring convergence on challenging non-convex op- based approximation of p. SVGD first samples a set
timization problems. Experimental results show our of points {x }n from an initial distribution q and
i i=1 0
methodachievessuperiornumericalperformance,both then iteratively updates the location of each sample
in convergence rate and sample accuracy, and scales using some static step size ξ and a sample-based
bettertohigh-dimensionaldistributionsthanprevious approximation of Eq. 3
variational inference techniques.
n
1 (cid:88)
x ←x +ξ k(x ,x )∇ logp(x )
i i n j i xj j
2 Stein Variational Inference
j=1
+∇ k(x ,x ) (4)
xj j i
The objective of SVI [Liu and Wang, 2016] is to
approximate a given target distribution p(x) on The first term of this update pushes the samples to-
X ⊆ RD using the Kullback-Leibler (KL) divergence- wards high probability areas of p while the second
minimizing representative q within some family Q of term, referred to as the kernel repulsion term, pushes
tractable model distributions apart samples that are close together. The kernel re-
pulsiontermisessentialbecauseitspreadsthesample
minKL(q||p)≡E [logq(x)−logp(x)] (1)
x∼q across the distribution [Liu and Wang, 2016].
q∈Q
Toachievethis,webeginwithsomeinitialdistribution
3 Related Work on SVI
q andgenerateasetofpushforwardsq ,...,q accord-
0 1 L
ing to the rule q = (T ) q where T ∈ RD → RD
l+1 l ∗ l l Sincethispaperisprimarilyconcernedwithimproving
is some perturbation T =I +Φ of the identity map.
l l SVI specifically, we restrict our discussion of related
Ateachiterationl,weseekaperturbationfunctionΦ
l work to SVI methods. SVGD [Liu and Wang, 2016],
from some function space F such that
as discussed above, struggles to scale to high-
J[Φ ]≜KL((I+Φ ) q ||p) J[Φ ]<J[0] (2) dimensional, non-convex, and ill-conditioned objec-
l l ∗ l l
tives. Several subsequent works have suggested modi-
To ensure the descent condition, we can choose Φ to fications to address these challenges.
l
be an infinitesimal application of the negative func-
There are two major contributors to SVI’s poor per-
tional gradient at J[0] in F. For a Hilbert space F,
formance in high dimensions. First, the information
thegradientofafunctionalJ atsomeS ∈F isdefined
(in bits) required to encode the joint target distri-
as the element ∇J[S]∈F satisfying
bution p grows exponentially with respect to it’s di-
mension. This increases the amount of information
⟨∇J[S],V⟩ =DJ[S](V) for all V ∈F
F approximation methods (like SVI) must infer to ac-
1 curately approximate p [Koller and Friedman, 2009].
DJ[S](V)≜ lim (J[S+τV]−J[S])
Second, when using distance-based kernels, like the
τ→0τ
radial basis function (RBF) kernel, the magnitude of
Consider some kernel k : X × X (cid:55)→ R with a cor- the kernel repulsion term decreases in higher dimen-
responding reproducing kernel Hilbert space (RKHS) sions, resulting in mode collapse, where all the sam-
H. A particularly advantageous choice for F is the ple points are densely packed around a single mode ofLiam Pavlovic, David M. Rosen
the target distribution [Zhuo et al., 2018]. When the H(x,y) is
conditional factorization of p is known (i.e. when p
is represented by a PGM), these conditional indepen- (H(x,y)) ab =E z∼q[−k(z,x)k(z,y)∂ ablogp(z)
dence relations can be exploited to dramatically sim- +∂ k(z,x)∂ k(z,y)] (6)
zb za
plify the inference task and address these challenges
[Koller and Friedman, 2009]. By applying a block-diagonal approximation to the
original system, SVN solves a decoupled system for
Graphical Stein variational inference methods
the approximate Newton update w of each sample x
[Zhuo et al., 2018, Wang et al., 2018] exploit the i i
conditional independences encoded in the PGM for H(x ,x )w =−∇J[0](x ) for i∈1,...,n (7)
i i i i
the target p via their kernel design. Assuming the
space X is a product of factors X = C × ... × C , To solve these decoupled systems, SVN utilizes a
1 D
graphical SVI employs a set of D local kernels conjugate gradient method such as the Newton-CG
k :X ×X (cid:55)→R,whereS representsthefactorC method [Nocedal and Wright, 1999]. Like SVGD,
a Sa Sa a a
anditsMarkovblanketΓ . Inthislocalkernelsetting, SVN does not leverage conditional independence, and
a
the Hilbert space F over which we take functional thus performs poorly in high dimensions.
gradients becomes the product H ×...×H of the
1 D Importantly, the previously discussed graphi-
local kernels’ RKHSs. In this space, the closed form
forthefunctionalgradient∇Jˆ[0](∧decoratorusedto cal SVI methods do not utilize an adaptive
method for step control, which was identified
indicate usage of local kernels and differentiate from
in [Detommaso et al., 2018] as important future
the gradient in Eq. 3) is given by
work. Adaptive step control has been implemented
for low-dimensional subspace projection methods
(∇Jˆ[0](x)) =−E [k (z,x)∇ logp(z)
a z∼q a za [Chen et al., 2019]. However, this method selects step
+∇ zak a(z,x)] for a∈1,...,D (5) sizes to minimize the KL divergence between projec-
tions of the target p and sample set {x }n onto a
This functional gradient interpretation of the graphi- i i=1
low-dimensional subspace, which does not necessarily
cal SVI update follows naturally from previous work
guarantee a reduction of the divergence between the
[Zhuo et al., 2018, Wang et al., 2018], but has not
target p and sample set {x }n themselves.
been presented before. So, we present a proof in i i=1
Appendix A. Note that utilizing the above descent
4 Exploiting Conditional
direction only guarantees that the resulting approx-
Independence in Second-Order SVI
imation q agrees with the target’s conditional dis-
tributions: q(x |x ) = p(x |x ) [Zhuo et al., 2018,
a Γa a Γa
Wang et al., 2018], indicating that these methods in- 4.1 Second Variation in Local Kernel Setting
ference the conditionals as desired.
As a first contribution, we show how to implement
Other work on SVI has explored improving high- a second-order Hessian model for SVI that exploits
dimensional performance on problems without con- conditional independence. To do this, we generalize
ditional structure by employing Grassman manifold the formula for the second-order variation presented
[Liu et al., 2022],low-dimensionalsubspaceprojection in SVN [Detommaso et al., 2018] to the local kernel
[Chen and Ghattas, 2020, Chen et al., 2019], and slic- space H ×...×H utilized by graphical SVI meth-
1 D
ing [Gong et al., 2020] strategies. ods. The second variation is defined as the direc-
tional derivative along a pair of directions V,W ∈
Many previous SVI methods only utilize first-order
H ×...×H
information in their updates, which is often insuffi- 1 D
cient to achieve good convergence on ill-conditioned 1
D2J[0](V,W)= lim (DJ[τW](V)−DJ[0](V))
objectives. The Stein variational Newton method τ→0τ
(SVN) [Detommaso et al., 2018] incorporates second- Theorem 1. Along a pair of directions V,W ∈H ×
1
order information by deriving a Newton system to ...×H the second variation is 1
D
compute an approximate Newton update w for each
i
sample x . This Newton system is block-structured D2J[0](V,W)=
i

H(x ,x ) ... H(x ,x
)
w
 
−∇J[0](x
) (cid:88)D (cid:88)D
1 1 1 n 1 1 ⟨⟨h (x,y),w (y)⟩ ,v (x)⟩ (8)


. .
.
... . .
.
 

. .
.
 =

. .
.

 a=1b=1
ab b Hb a Ha
H(x n,x 1) ... H(x n,x n) w n −∇J[0](x n) 1Theinnerproductsherearebetweenthefunctionsh ab,
w ,v in Hilbert spaces. x and y are free variables only
b a
where the ab-th entry of the Hessian matrix block included to show which functions share which inputs.A Trust-Region Method for Graphical Stein Variational Inference
h (x,y)=E [−k (z,x)k (z,y)∂ logp(z) where H(q) is the entropy of q. The first term can
ab z∼q a b ab
+∂ k (z,y)∂ k (z,x)] (9) beeasilyapproximatedviaanempiricalestimatewith
za b zb a
ourcurrentsampleset. Thesecondtermrequirescom-
puting the entropy of q given the representative set of
For the proof of this theorem, see Appendix B.
samples {x }n , which is more difficult. Since SVI
i i=1
already requires the computation of kernel matrices,
4.2 Decoupled Newton Systems
we propose utilizing the kernelized approximation of
entropy from [Bach, 2022].
Following SVN [Detommaso et al., 2018], we employ
ablock-diagonalapproximationoftheHessiandefined (cid:20) (cid:21) n
1 1 (cid:88)
by Theorem 1 and solve a decoupled system for the H(q)≈−tr Klog( K) =− λ log(λ ) (12)
n n i i
approximate Newton update w i for each sample x i i=1
Hˆ(x ,x )w =−∇Jˆ[0](x ) for i∈1,...,n (10) where K is the n × n kernel matrix for the sample
i i i i
{x }n , such that K = k(x ,x ), and {λ }n are
i i=1 ij i j i i=1
where the entries of the Hessian Hˆ(x ,x ) are defined the eigenvalues of n1K. Since this approximation re-
by the second variation
coefficientsi (Hˆi
(x ,x )) =
quiresthecomputationofeigenvalues,whichisacom-
i i ab putationallyexpensiveO(n3)operation, weutilizethe
h (x ,x ) from Eq. 9.
ab i i eigenvalues of a Nystr¨om approximation of 1K in
n
place of the full matrix. We use a non-local kernel
5 Trust-Region Methods k :X ×X →R for this approximation. The full algo-
rithmic details of the approximation are presented in
Our second contribution is implementing two trust- Algorithm 1. It’s time complexity is O(m3+n) where
region methods for SVI optimization. Trust- m is the Nystr¨om size.
region methods are iterative procedures for optimiz-
ing a smooth objective function f : RM → R Algorithm 1 Approx-KL
[Nocedal and Wright, 1999]. Ateachiterationt, these 1: Inputs: Sample points {x }n , reference distri-
methods generate an additive update w ∈ RM to the i i=1
bution p, Nystr¨om size m
currentestimatex∈RM byminimizingasecond-order 2: SelectasubsetS ⊂{x }n ofsizemuniformlyat
i i=1
approximation of the objective over a closed ball de-
random without replacement
fined by some norm ||·||, called the trust region
3: Compute the kernel matrix K using kernel func-
tion k on the subset S
1
min w∈RMf(x)+∇f(x)⊤w+ 2w⊤Hw 54 :: HU,{ =λ i (cid:80)}m i m=1, λV l= ogS (V λD )( n1K)
i=1 i i
s.t. ||w||≤∆ (11) 6: P = 1 (cid:80)n logp(x )
n i=1 i
7: Return −P +H
where H is the symmetric Hessian model.
Given the Newton system’s (Eq. 10) block-diagonal
structure, we propose utilizing the norm ||w|| ≜ Our first trust-region algorithm, TR-SVI-KL, adjusts
max{||w || }n to define the trust region, where w is thesharedtrust-regionsizebasedonhowwellthelocal
i 2 i=1 i
the update for each individual sample x . The advan- quadraticmodel(Eq. 11)predictstheobservedchange
i
tageofusingthisnormisthat,incombinationwiththe in objective value. If a step increases the objective
block-diagonal Hessian model, the trust-region mini- value, it is rejected. The full algorithmic details of
mization Eq. 11 is separable over the updates w for this trust-region method are presented in Algorithm
i
each sample x ; consequently, these updates can be 2. Theper-iterationtimecomplexityofthismethodis
i
efficiently computed in parallel. O(n2D2+m3) where m is the Nystr¨om size used for
Approx-KL.
5.1 KL Divergence Approximation
5.2 Gradient-Based Trust-Region
Standard trust-region methods rely on objective func-
tion evaluations to iteratively adjust the trust-region Even with the Nystr¨om approximation, the KL di-
radius ∆ used in each iteration based on the observed vergence approximation in Algorithm 1 is still rela-
change in objective value. In the specific case of SVI, tively expensive to compute. Moreover, the approxi-
we aim to minimize KL divergence. The computation mation error can negatively impact the efficacy of the
of KL divergence can be split into two terms trust-regionadjustment. Therefore,inthissubsection,
we describe an alternative trust-region method that
KL(q||p)=E [−logp(x)]−H(q) avoids the need to evaluate the objective at all, by
x∼qLiam Pavlovic, David M. Rosen
Algorithm 2 TR-SVI-KL Algorithm 3 TR-SVI-AT
1: Inputs: Initial points {x }n , reference distribu- 1: Inputs: Initial points {x }n , reference distribu-
i i=1 i i=1
tion p, initial trust-region ∆ tion p
2: for each iteration t do 2: b =.1
min
(cid:113)
3: for each sample x i do 3: b,w,b ,g = (cid:80)n ||∇Jˆ[0](x )||2
4: Compute w by solving system in Eq. 10 us- max i=1 i
i 4: for each iteration t do
ing CG-Steihaug [Nocedal and Wright, 1999]
5: for each sample point x do
i
with trust-region ∆
6: Compute w by solving system in Eq. 10 us-
i
5: end for
ing CG-Steihaug [Nocedal and Wright, 1999]
6: m=(cid:80)n i=1 21w i⊤Hˆ(x i,x i)w i+∇Jˆ[0](x i)⊤w i with trust-region g
7: u=Approx-KL({x +w }n ,p,⌊n/10⌋) b
i i i=1 7: end for
8: o=Approx-KL({x i}n i=1,p,⌊n/10⌋) 8: {x }n ={x +w }n
9: ρ= u−o i (cid:113)i=1 i i i=1
m 9: g = (cid:80)n ||∇Jˆ[0](x )||2
∆/2 if ρ<.0001 i=1 i
 (cid:40)
10: ∆= 1.5∆ if ρ>.7 max(b min,.9b),g if g <.999w
10: b,w =
∆ otherwise min(b max,b+ g b2 ),w otherwise
(cid:40) 11: end for
{x }n if ρ<0
11: {x }n = i i=1
i i=1 {x +w }n otherwise
i i i=1
12: end for [Nocedal and Wright, 1999] for solving the systems in
Eq. 7 with a constant trust-region size. The hyperpa-
rametersofthesevariantswerefittoeachspecificprob-
taking advantage of the recently developed AdaTrust
lemviagridsearchtomaximizeperformance,whichin-
method [Grapiglia and Stella, 2022].
troduces extra information that our methods did not
The motivating idea behind a gradient-based trust- receive. Note that this makes the following compar-
region is that a converging optimization should con- isons somewhat unfairly biased against our methods.
tain a subset of iterations in which the magnitude of All SVI methods, both ours and the baselines, utilize
the gradient is consistently decreasing. Our gradient- an RBF kernel with a lengthscale set manually based
based trust-region method stores the lowest observed on performance. Exact hyperparameter settings for
gradient magnitude value and compares the gradient each method and problem are shown in Appendix E.
magnitude at each new iterate against it. The trust- We also compare against VIPS40 [Arenz et al., 2018],
region is expanded if the gradient magnitude at the astate-of-the-art,Gaussianmixturemodel-basedvari-
current iterate is less than the lowest seen so far and ationalinferencemethod. Allexperimentswererunon
constricted otherwise. The algorithmic details of this a desktop with an Intel Core i7-13700K, 32 GB RAM,
trust-regionmethodarepresentedinAlgorithm3. The and a NVIDIA RTX 4080.
per-iteration time complexity of this method is the
same as SVN, O(n2D2). 6.2 Bayes Net Experiment
Ourfirstsetofexperimentsisdesignedtotestthedif-
6 Results
ferent variational inference methods’ ability to scale
to distributions that are high-dimensional and poorly
6.1 Experimental Set-Up
conditioned in a controlled environment where we can
easily recover ground truth samples from the refer-
In this section, we experimentally evaluate our trust-
ence distribtuion. To this end, we evaluate perfor-
region SVI algorithms. As baselines, we com-
mance on recovering the joint density of a Bayes net
pare against prior work on SVI, namely MP-SVGD
[Koller and Friedman, 2009]. This synthetic problem
[Zhuo et al., 2018]andSVN[Detommaso et al., 2018],
gives us a high degree of control over the parameters
which also serve as ablations of our method. Moti-
of the distribution and the generative nature of Bayes
vated by the results in Appendix F, we utilize vari-
netsenablesustoeasilyrecoveragroundtruthsample
ants with more advanced step control rules to make
via ancestral sampling. This problem also has known
thesebaselinesasstrongaspossible. Thefirstofthese
conditional independence structure that our methods
modified methods is MP-SVGD-DLR, which utilizes
and graphical baselines can exploit.
a decaying step size. The second is MP-SVGD-AG
whichutilizestheoff-the-shelfadaptiveoptimizerAda- The nodes of the Bayes net are organized into lay-
Grad [Duchi et al., 2011] for step control. The third erswithnodesineachlayerconditionedonlyonnodes
is SVN-CTR which utilizes the CG-Steihaug method fromthepreviouslayer. Theconditional/marginaldis-A Trust-Region Method for Graphical Stein Variational Inference
Table 1: Maximum Mean Discrepancies against Ground Truth Sample
Model 12-Dimensional SNLP 30-Dimensional BN 80-Dimensional BN
MP-SVGD-DLR 0.05276±0.03560 0.1492±0.00618 0.2251±0.00577
MP-SVGD-AG 0.05091±.01328 0.2130±0.02086 0.2004±0.00975
SVN-CTR 0.05067±0.00937 0.1887±0.00877 0.2707±0.00276
VIPS40 0.1981±0.12383 0.00185±0.00095 0.2565±0.03055
TR-SVI-KL 0.04800±0.00979 0.01496±0.00741 0.08634±0.02348
TR-SVI-AT 0.03530±0.01366 0.009674±0.00623 0.07646±0.02051
tributions encoded by each node are either a Gaus- of this Bayes net is a Gaussian mixture, the Gaussian
sian or Gaussian mixture. Note that this makes the mixture-based approximation employed by VIPS40 is
resulting joint of the Bayes net a Gaussian mixture. the information-theoretically optimal choice in this
The exact parameters of each node are generated ran- specific case. The difference in performance between
domlyaccordingtothegenerativeprocessdescribedin our methods and VIPS40 on this problem represents
AppendixD.ThegenerativeparametersfortheGaus- the cost of flexibility. Although our methods can
sian mixture nodes are selected to encourage distinct more accurately capture a wider variety of distribu-
modes. The nodes are also generated to have vastly tionshapes,theycannotachievegold-standardperfor-
different variances to induce poor-conditioning. We mance of parametric models in their ideal use cases.
test on two Bayes nets examples, one 30-dimensional Furthermore, our method maintains similar perfor-
and one 80-dimensional. A ground truth sample for manceonboththe30-and80-dimensionalBayesNet,
each example, containing 6 million points, was gener- whileVIPS40performstwoordersofmagnitudeworse
atedviaancestralsampling. Eachvariationalinference on the higher-dimensional problem. We attribute the
methodwasusedtoproduceasamplewith200points. pronounced decrease in VIPS40’s performance to the
factthatVIPS40doesnottakeadvantageofthecondi-
We evaluate the quality of each method’s sample
tional independences present in these Bayes net mod-
by comparing against the ground truth sample us-
els,theexploitationofwhichiswell-knowntobecriti-
ing the maximum mean discrepancy (MMD) metric
calforachievingefficientinferenceinhighdimensions.
[Gretton et al., 2012]. MMD was chosen because it
is designed to find the test statistic that reveals the
greatestdiscrepancybetweentwodistributions. MMD
is also kernel-based which makes it a somewhat natu- 6.3 Low-dimensional SNLP with Ground
ral choice to evaluate SVI methods. The MMD of two Truth
samples X ={x }n and Y ={y }m is computed as
i i=1 i i=1 Our next set of experiments is designed to highlight
1 (cid:88)n 2 (cid:88)n (cid:88)m the flexibility of our methods by investigating both
MMD(X,Y)=
n2
k(x i,x j)−
nm
k(x i,y j) the convergence behavior and posterior approxima-
i,j=1 i=1j=1 tionqualityofthedifferentvariationalinferencemeth-
1 (cid:88)m ods on a real-world problem with complex posterior
+ k(y ,y ) (13)
m2 i j shapes. To this end, we evaluate our methods and the
i,j=1 baselines on a small example of the sensor network lo-
For our tests, the kernel k is the RBF kernel calization problem (SNLP) [Biswas et al., 2006]. This
whose lengthscale is set using the median heuris- problemtendstohaveposteriorswithmultimodaland
tic [Garreau et al., 2017] on the ground truth sample. annular shapes and is therefore hard for parametric
Theperformanceforeachvariationalinferencemethod models to accurately approximate. Its low dimension-
on this metric is shown in Table 1. All numerical re- ality enables us to recover a ground truth sample for
sults are averaged over 5 runs with randomly initial- quantitative analysis.
ized positions for the SVI particle sets. The standard
ThegoaloftheSNLPistorecoverthepositionsofaset
deviation of the MMD statistic over these runs is re-
of sensors S = {s ,...,s } ⊂ RD from a given set of
ported in the error bars. 1 n
noisy pairwise distance measurements between them.
Our methods outperformed the SVI baselines on both ThisproblemcanbemodeledasagraphG=(V,E)in
Bayes net problems, suggesting better scaling to high whichthesensorsarerepresentedbytheverticesV and
dimensions and poor conditioning. VIPS40 did out- the available measurements d˜ are represented by the
ij
perform our method on the lower-dimesnional Bayes edgesE betweenthem. Apairofsensorss ands can
i j
net problem. That said, since the joint distribution onlygeneratearangemeasurementwhenthedistanceLiam Pavlovic, David M. Rosen
(a) (b)
Method Method
104 TR-SVI-AT TR-SVI-AT
MP-SVGD-DLR MP-SVGD-DLR
SVN-CTR SVN-CTR
103 TR-SVI-KL TR-SVI-KL
MP-SVGD-AG MP-SVGD-AG
102
101
100
101
0 200 400 600 800 1000 0 5 10 15 20
Iteration Time (in seconds)
Figure 1: The convergence rate as a function of iteration number (a) and compute time (b) of each SVI method
on the small SNLP instance. All second-order methods show fast, smooth convergence both MP-SVGD variants
oscillate until their step size decays enough to enable convergence. Note that, unlike the other methods, SVN-
CTRdoesnotuselocalkernelstocomputethegradient(seeEqs. 3and5). Sincetheestimatedgradientsdepend
upon the choice of kernel, SVN-CTR’s gradient magnitude values are not directly comparable.
||s −s || between them is less than some maximum andsmoothly. BothMP-SVGDvariantsoscillateuntil
i j
effective sensing radius r, and any such measurement thestepsizedecaysenoughtoallowconvergence. TR-
is generated according to: SVI-KL shows fewer iterations than the other meth-
odsbecauseofitssteprejectionmechanism,whichthe
d˜ =||s −s ||+ϵ ϵ ∼N(0,σ2)
ij i j ij ij other methods lack.
Finally, we assume that there is a subset A ⊂ S of Figure1bdepictstheconvergenceofthemethodsasa
the sensors (called the anchors) whose positions are functionoftime. Althoughtheper-iterationtimecom-
known exactly a priori. plexityofsecond-ordermethodsisgreaterthanthatof
first-order methods, they require fewer iterations, and
For these experiments, we use an SNLP instance with
thus less compute time overall, to achieve a small gra-
6 estimated nodes and 4 anchors placed on a 6 × 6
unit square in R2. The maximum range is r = 3 and dient magnitude.
themeasurementsarenoiselessbutbelievedtobecor-
rupted by noise ϵ ∼ N(0,.01). The problem is vi- 6.3.2 Numerical Performance of Generated
ij
sualized in graphical form in Appendix C. A ground Samples
truth sample containing about 1.1 million points was
Next, we evaluate the accuracy of the generated sam-
recovered using a standard nested sampling library,
ple sets as approximations of the target posterior dis-
dynesty [Speagle, 2020]. Each variational inference
tribution, once again using the MMD metric (Eq. 13)
method was set to produce a sample of 200 points.
against the dynesty reference sample. The results of
Although this problem is relatively small, it is suffi-
this experiment are presented in Table 1.
cient to reveal significant differences in performance
between the different variational inference methods. Our trust-region methods achieved the lowest average
MMD scores with the reference sample. VIPS40 per-
6.3.1 Convergence Results formed the worst on this test, potentially due to the
difficulty of approximating the annular shapes of the
Our first experiment in this set focuses on analyzing
SNLP posterior with a Gaussian mixture. The sam-
theconvergencebehaviorofthedifferentSVImethods
ples produced by the different methods are assessed
with different step control approaches. Methods like
qualitatively in Appendix G.
SVN and MP-SVGD that lack adaptive step control
rely on a priori user-specified static step sizes or step
6.4 Qualitative Analysis of High-Dimensional
controlrules. Theseexperimentsdemonstratetheper-
SNLP
formance gap in convergence behavior between adap-
tivestepsizemethodsandapriori stepselectionrules.
Ourlastsetofexperimentsisintendedtotesthowthe
Figure1adepictstheconvergenceforourmethodsand different methods scale to a high-dimensional prob-
the modified baselines as a function of iteration num- lem with complex shapes. To this end, we apply
ber. All the second-order methods converge quickly it to a larger 50 sensor, 12 anchor SNLP instance
edutingaM
tneidarGA Trust-Region Method for Graphical Stein Variational Inference
TR-SVI-AT SVN-CTR MP-SVGD-DLR VIPS40
243s 390s 40s 9696s
7.0 7.0 7.0 7.0
6.5 6.5 6.5 6.5
6.0 6.0 6.0 6.0
A 5.5 5.5 5.5 5.5
5.0 5.0 5.0 5.0
4.5 4.5 4.5 4.5
4.04.0 4.5 5.0 5x.5 6.0 6.5 7.0 4.04.0 4.5 5.0 5x.5 6.0 6.5 7.0 4.04.0 4.5 5.0 5x.5 6.0 6.5 7.0 4.04.0 4.5 5.0 5x.5 6.0 6.5 7.0
11 11 11 11
10 10 10 10
9 9 9 9
8 8 8 8
B
7 7 7 7
6 6 6 6
5 5 5 5
412 11 10 9x 8 7 6 5 412 11 10 9x 8 7 6 5 412 11 10 9x 8 7 6 5 412 11 10 9x 8 7 6 5
10 10 10 10
8 8 8 8
C 6 6 6 6
4 4 4 4
2 2 2 2
2 0 2x 4 6 2 0 2x 4 6 2 0 2x 4 6 2 0 2x 4 6
Figure 2: Kernel density estimation (KDE) plots of the final samples produced by various variational inference
methods on a high-dimensional, noisy SNLP problem. From each sample, the marginal samples corresponding
to the location of a selected sensor are extracted and visualized as a KDE plot. Since ground truth was not
recoverable, we also visualize the measurements received by each selected sensor to enable qualitative analysis.
These measurements are displayed as orange circles with a radius equal to the range measurement centered on
the true position of the sending node. The time to generate the sample (in seconds) is displayed under its name.
over a 20×20 unit square in R2. For this problem, methods, only TR-SVI-AT produced a sample with a
the maximum sensor range r = 3 and the measure- balanced annular shape. Sensor C received two range
mentsreceivedbythesensorsareperturbedwithnoise measurements,resultinginabimodaldistribution. All
ϵ ∼ N(0,.01) whose mean and variance are known SVI methods captured this bimodal distribution but
ij
values. The results of this experiment are only as- VIPS40 only captured one of the two modes.
sessedqualitativelysincerecoveringahigh-qualityref-
Overall, TR-SVI-AT appears to capture intricate de-
erencesampleusingtraditionalmethods(e.g. dynesty
tails in high dimensions significantly more accurately
[Speagle, 2020]) is intractable. For comparison, MP-
than previous variational inference methods. Notably,
SVGD-DLR,SVN-CTR,andVIPS40werealsorunon
VIPS40 took significantly (>20x) longer than the SVI
this test. The same set of noisy measurements were
methodsandproducedavisiblyworseapproximation.
used for all methods to ensure a fair comparison.
Figure 2 shows KDE plots generated from the sam-
ples produced by each of four methods for three se-
lected sensors and the time required to generate each 7 Conclusion
approximation. Figure 2 also displays the incoming
measurements for each sensor as circles to enable eas-
ier qualitative analysis. We introduce a SVI method that leverages known
conditional independence structure, second-order in-
Sensor A received multiple measurements, so a uni-
formation, and adaptive step control to ensure good
modal distribution is expected, with some variation
convergence on high-dimensional, non-convex, and ill-
due to sensor noise. All SVI methods recovered a uni-
conditioned objectives. Our method demonstrated
modal distribution centered correctly on the intersec-
faster and more reliable convergence than existing
tionofthevariousmeasurements. VIPS40recovereda
SVI methods. The approximations produced by our
unimodaldistribution,butitisnotcorrectlycentered.
methodweremoreaccuratethanthoseofexistingSVI
SensorBreceivedasinglerangemeasurementfroman
methods and a state-of-the-art parametric variational
anchor,soitsposteriorshouldbeannular. Ofthefour
inference method.
y
y
y
y
y
y
y
y
y
y
y
yLiam Pavlovic, David M. Rosen
8 Acknowledgements [Gong et al., 2020] Gong, W., Li, Y., and Hern´andez-
Lobato, J. M. (2020). Sliced kernelized stein dis-
Liam Pavlovic was supported by the National Science crepancy. arXiv preprint arXiv:2006.16531.
Foundation Graduate Research Fellowship Program
[Grapiglia and Stella, 2022] Grapiglia, G. N. and
(NSF-GRFP).
Stella, G. F. (2022). An adaptive trust-region
method without function evaluations. Computa-
References
tional Optimization and Applications, 82(1):31–60.
[Andrieu et al., 2003] Andrieu, C., De Freitas, N.,
[Gretton et al., 2012] Gretton, A., Borgwardt, K. M.,
Doucet, A., and Jordan, M. I. (2003). An intro-
Rasch, M. J., Sch¨olkopf, B., and Smola, A. (2012).
duction to mcmc for machine learning. Machine
A kernel two-sample test. The Journal of Machine
learning, 50:5–43.
Learning Research, 13(1):723–773.
[Arenz et al., 2018] Arenz, O., Neumann, G., and
[Koller and Friedman, 2009] Koller,D.andFriedman,
Zhong,M.(2018). Efficientgradient-freevariational
N.(2009). Probabilistic graphical models: principles
inference using policy search. In International con-
and techniques. MIT press.
ferenceonmachinelearning,pages234–243.PMLR.
[Liu and Wang, 2016] Liu, Q. and Wang, D. (2016).
[Bach, 2022] Bach,F.(2022).Informationtheorywith
Stein variational gradient descent: A general pur-
kernelmethods. IEEE Transactions on Information
posebayesianinferencealgorithm. Advancesinneu-
Theory, 69(2):752–775.
ral information processing systems, 29.
[Betancourt, 2017] Betancourt, M. (2017). A concep-
[Liu et al., 2022] Liu, X., Zhu, H., Ton, J.-F.,
tualintroductiontohamiltonianmontecarlo. arXiv
Wynne, G., and Duncan, A. (2022). Grassmann
preprint arXiv:1701.02434.
stein variational gradient descent. arXiv preprint
[Biswas et al., 2006] Biswas, P., Liang, T.-C., Toh, arXiv:2202.03297.
K.-C.,Ye,Y.,andWang,T.-C.(2006). Semidefinite
[Nocedal and Wright, 1999] Nocedal, J. and Wright,
programming approaches for sensor network local-
S. J. (1999). Numerical optimization. Springer.
ization with noisy distance measurements. IEEE
transactions on automation science and engineer- [Speagle, 2020] Speagle, J. S. (2020). Dynesty: a
ing, 3(4):360–371. dynamic nested sampling package for estimating
bayesian posteriors and evidences. Monthly Notices
[Chen and Ghattas, 2020] Chen, P. and Ghattas, O.
of the Royal Astronomical Society, 493(3):3132–
(2020). Projected stein variational gradient de-
3158.
scent. Advances in Neural Information Processing
Systems, 33:1947–1958. [Wang et al., 2018] Wang, D., Zeng, Z., and Liu, Q.
(2018). Steinvariationalmessagepassingforcontin-
[Chen et al., 2019] Chen, P., Wu, K., Chen, J.,
uous graphical models. In International Conference
O’Leary-Roseberry, T., and Ghattas, O. (2019).
on Machine Learning, pages 5219–5227. PMLR.
Projected stein variational newton: A fast and
scalable bayesian inference method in high dimen- [Zhuo et al., 2018] Zhuo, J., Liu, C., Shi, J., Zhu, J.,
sions. Advances in Neural Information Processing Chen, N., and Zhang, B. (2018). Message passing
Systems, 32. stein variational gradient descent. In International
Conference on Machine Learning, pages6018–6027.
[Detommaso et al., 2018] Detommaso, G., Cui, T.,
PMLR.
Marzouk, Y., Spantini, A., and Scheichl, R. (2018).
A stein variational newton method. Advances in
Neural Information Processing Systems, 31.
[Duchi et al., 2011] Duchi, J., Hazan, E., and Singer,
Y. (2011). Adaptive subgradient methods for on-
line learning and stochastic optimization. Journal
of machine learning research, 12(7).
[Garreau et al., 2017] Garreau, D., Jitkrittum, W.,
and Kanagawa, M. (2017). Large sample anal-
ysis of the median heuristic. arXiv preprint
arXiv:1707.07269.A Trust-Region Method for Graphical Stein Variational Inference
A Proof of Equation 5
From the proof of Theorem 3.3 in [Liu and Wang, 2016], we have
τ⟨∇J[S],V⟩+O(τ2)=J[S+τV]−J[S]
and, at S =0,
J[0+τV]−J[0]=−∆ −∆
1 2
where
∆ =E [logp(z+τV(z))]−E [logp(z)]
1 z∼q z∼q
∆ =E [logdet(I+τ∇ V(z))]−E [logdet(I)]
2 z∼q z z∼q
For V ∈H ×...×H the terms above can be computed as
1 D
∆ =E [logp(z+τV(z))]−E [logp(z)]
1 z∼q z∼q
=τE [∇ logp(z)·V(z)]+O(τ2)
z∼q z
D
(cid:88)
=τ E [∇ logp(z)v (z)]+O(τ2)
z∼q za a
a=1
D
(cid:88)
=τ ⟨E [∇ logp(z)k (z,·)],v (·)⟩ +O(τ2)
z∼q za a a Ha
a=1
and
∆ =E [logdet(I+τ∇ V(z))]−E [logdet(I)]
2 z∼q z z∼q
=τE [trace(I−1·∇ V(z))]+O(τ2)
z∼q z
=τE [trace(∇ V(z))]+O(τ2)
z∼q z
D
(cid:88)
=τ E [∇ v (z)]+O(τ2)
z∼q za a
a=1
D
(cid:88)
=τ ⟨E [∇ k (z,·)],v (·)⟩+O(τ2)
z∼q za a a
a=1
Thus,
D
(cid:88)
⟨∇J[0],V⟩= ⟨−E [k (z,·)∇ logp(z)+∇ k (z,·)],v ⟩
z∼q a za za a a Ha
a=1
and
(∇J[0](·)) =−E [k (z,·)∇ logp(z)+∇ k (z,·)]
a z∼q a za za a
B Proof of Theorem 1
From the proof of Theroem 1 in SVN [Detommaso et al., 2018], we know that the second variation of the SVI
objective along a pair of directions V,W ∈H ×...×H equals
1 D
D2J[0](V,W)=−E (cid:2) W(z)⊤∇2logp(z)V(z)−trace(∇ W(z)∇ V(z))(cid:3) (14)
z∼q z z z
By the reproducing properties of H ×...×H , namely
1 D
v (z)=⟨k (z,·),v (·)⟩ w (z)=⟨k (z,·),w (·)⟩
a a a Ha a a a Ha
and
∂ v (z)=⟨∂ k (z,·),v (·)⟩ ∂ w (z)=⟨∂ k (z,·),w (·)⟩
zb a zb a a Ha zb a zb a a HaLiam Pavlovic, David M. Rosen
we get
D D
E (cid:2) W(z)⊤∇2logp(z)V(z)(cid:3) =(cid:88)(cid:88) ⟨⟨E (cid:2) −k (z,x)k (z,y)∂2 logp(z)(cid:3) ,w (y)⟩ ,v (x)⟩
z∼q z z∼q a b ab b Hb a Ha
a=1b=1
and
D D
(cid:88)(cid:88)
E [trace(∇ W(z)∇ V(z))]= ⟨⟨E [∂ k (z,y)∂ k (z,x)],w (y)⟩ ,v (x)⟩
z∼q z z z∼q za b zb a b Hb a Ha
a=1b=1
Plugging these in yields the final expression for the second variation
D D
(cid:88)(cid:88)
⟨⟨h (x,y),w (y)⟩ ,v (x)⟩ (15)
ab b Hb a Ha
a=1b=1
where
h (x,y)=E [−k (z,x)k (z,y)∂ logp(z)+∂ k (z,y)∂ k (z,x)] (16)
ab z∼q a b ab za b zb a
C SNLP Problem Visualizations
(a) (b)
10.0
3
7.5
2
5.0
1 2.5
0 0.0
1 2.5
5.0
2
7.5
3
10.0
3 2 1 0 1 2 3 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x
Figure 3: Graph representations of the sensor network localization problems used for evaluation with the small
example on the left and the large example on the right. Estimated nodes are depicted in blue and anchors
in orange. The edges represent shared range measurements between pairs of nodes. Blue edges correspond
to measurements shared between two estimated nodes and orange edges correspond to measurements from an
anchor.
D Bayes Net Generation Details
The nodes of these Bayes Nets are organized into layers. The 30-dimensional has 3 layers with 10 nodes each.
The 80-dimensional has 4 layers with 20 nodes each. A node x from the first layer has a marginally Gaussian
j
distribution p(x ) = N(µ,σ2). A node x in any subsequent layer is conditioned on some random [1,M]-size
j j
subset C of the nodes from the previous layer with which it shares connections in the network. The conditional
j
distribution of such a node is either a Gaussian or Gaussian mixture of the form
2
(cid:88) (cid:88) (cid:88)
p(x |C )=N( α x ,σ2) or p(x |C )= ω N( αlx ,σ2) (17)
j j k k j j l k k
xk∈Cj l=1 xk∈Cj
where {α(l)} is a set of weights and {ω } are the GMM component weights. The 30-dimensional problem has a
k l
totalof6GMMnodesandthe80-dimensionalhas20. Nodesfromalllayersbutthefirstwereuniformlyselected
at random to be a GMM node. The specific random generative procedure for values of each parameter are
y yA Trust-Region Method for Graphical Stein Variational Inference
• µ is selected uniformly from [0,2] for 30-dimensional [0,4] for 80-dimensional
• All weights {α(l)} are selected independently and uniformly from [−1,1]
k
• The first GMM weight ω is selected uniformly from [.4,.6], the second ω completes the sum to 1.
1 2
• All variances σ2 were sampled uniformly over orders of magnitude [10−3,100] to induce poor-conditioning
• Maximum number of connections M is 3 for 30-dimensional and 4 for 80-dimensional
E Hyperparameter Details
Table 2: Hyperparameter Settings
Model 12-Dim SNLP 100-dim SNLP 30-Dim BN 80-Dim BN
MP-SVGD-DLR (initial step, step decay) 0.1, 0.99 0.1, 0.99 0.01, 0.999 0.01, 0.99
MP-SVGD-AG (intial step) 0.5 N/A 0.05 0.05
SVN-CTR (trust region size) 1 0.1 0.1 0.1
Kernel (lengthscale) 1 3 10 60
The hyperparameters used for each SVI baseline, as well as the kernel hyperparameters utilized by all SVI
methods, are shown in Table 2. VIPS40 utilizes the default configuration provided by the source code
[Arenz et al., 2018]. TR-SVI-AT and TR-SVI-KL utilize the parameter settings listed in their respective al-
gorithm blocks (Algorithms 2, 3) for all experiments. The hyperparameters for the trust-region methods were
set based on performance on small toy problems during early algorithm development and then used for every
experiment without alteration. In general, the convergence of trust-region methods is not sensitive to exact
hyperparameter settings [Nocedal and Wright, 1999].
F Additional Convergence Results
This additional set of tests is designed to demonstrate the necessity of adaptive step control for ensuring con-
vergence on non-convex objectives. To do this, we analyze the convergence of MP-SVGD [Zhuo et al., 2018] and
SVN [Detommaso et al., 2018] on the small SNLP example with static step sizes. Figure 4 depicts the conver-
gence rates of MP-SVGD and SVN with a variety of static step sizes, none of which produce good results. For
MP-SVGD, the step size is either too large for a portion of the optimization, causing the method to oscillate
over the objective, or too small, resulting in slow convergence. For SVN, all the step sizes result in overly large
initial steps from which the method subsequently struggles to recover. The poor performance of these methods
in this experiment motivated the introduction of the improved MP-SVGD-DLR, SVN-CTR and MP-SVGD-AG
baselines.
(a) (b)
104 Step 0 0 s . .1 0iz 1e 108
0.001
107
106
103
105
104
102
103 Step 0 .s 0i 1ze
0.001
0.0001
0 100 200 300 400 500 0 100 200 300 400 500
Iteration Iteration
Figure 4: The convergence rates of MP-SVGD(a) and SVN(b) on the small SNLP instance with a variety of
static step sizes. None produce good results.
edutingaM
tneidarG
edutingaM
tneidarGLiam Pavlovic, David M. Rosen
G Qualitative Analysis of Low-Dimensional SNLP
This additional set of tests is designed to demonstrate how differences in MMD performance on the small
SNLP instance, as reported in Table 1 translate to perceptible differences in the quality of the different sample
approximations. Figure 5 shows kernel density estimate (KDE) plots generated from samples produced by MP-
SVGD-DLR,SVN-CTR,TR-SVI-AT,VIPS40,anddynesty,aswellasthetimerequiredtogeneratethesamples.
Plots of the marginal posterior estimates for the locations of three selected sensors are shown. These sensors
were selected because they represent a variety of posterior shapes.
dynesty TR-SVI-AT SVN-CTR MP-SVGD-DLR VIPS40
40896s 21s 17s 5s 740s
0.18
0.3 0.3 0.16 0.3 0.3
0.14
0.2 0.2 0.2 0.2
0.12
D 0.1 0.1 0.10 0.1 0.1
0.08
0.0 0.0 0.06 0.0 0.0
0.04
0.1 0.1 0.1 0.1
0.02
0.2 0.5 0.4 0.3x 0.2 0.1 0.0 0.2 0.5 0.4 0.3x 0.2 0.1 0.0 0.000.30 0.29 0.28 x 0.27 0.26 0.25 0.2 0.5 0.4 0.3x 0.2 0.1 0.0 0.2 0.5 0.4 0.3x 0.2 0.1 0.0
4 4 4 4 4
3 3 3 3 3
2 2 2 2 2
1 1 1 1 1
E 0 0 0 0 0
1 1 1 1 1
2 2 2 2 2
3 3 3 3 3
44 3 2 1 0x 1 2 3 4 44 3 2 1 0x 1 2 3 4 44 3 2 1 0x 1 2 3 4 44 3 2 1 0x 1 2 3 4 44 3 2 1 0x 1 2 3 4
4 4 4 4 4
3 3 3 3 3
2 2 2 2 2
1 1 1 1 1
F 0 0 0 0 0
1 1 1 1 1
2 2 2 2 2
3 3 3 3 3
44 3 2 1 0x 1 2 3 4 44 3 2 1 0x 1 2 3 4 44 3 2 1 0x 1 2 3 4 44 3 2 1 0x 1 2 3 4 44 3 2 1 0x 1 2 3 4
Figure 5: Kernel density estimation (KDE) plots of the final samples produced by various SVI methods and the
dynesty reference on a low-dimensional SNLP problem. From each sample, the marginal samples corresponding
tothelocationoftheselectedsensorareextractedandvisualizedasaKDEplot. TheKDEplotsofthedifferent
methods are displayed on the same scale with the exception of SVN-CTR’s plot for sensor A, which required a
scale an order of magnitude smaller to be visible. The time required to generate each sample is displayed under
its name.
SensorDreceivedmultiplemeasurementsfromothernodes, resultinginadenseunimodaldistribution. VIPS40,
TR-SVI-AT,andMP-SVGD-DLRcapturethissensor’sposteriorwell. SVN-CTR,ontheotherhand,significantly
underestimatesthevarianceoftheposterior, requiringadifferentaxisscalethanthereferencedistributiontobe
visible.
Sensor E received single anchor measurement resulting in a annular posterior. All three SVI methods produced
a sample with a annular shape, but the samples of MP-SVGD-DLR and SVN-CTR display a bias towards the
bottom right of the annulus not present in TR-SVI-AT’s sample. VIPS40 recovers only a partial arc that is
misshapen. All 4 methods produce more diffuse distributions than the reference.
Sensor F received a few range measurements but not as many as Sensor D, resulting in a bimodal distribution.
All 3SVI methods capture thebimodality ofthe distribution, but they alsoassign more probabilityto thelower
probability mode than the reference distribution. VIPS40 does not capture the bimodality of this distribution,
only capturing the lower probability mode.
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y