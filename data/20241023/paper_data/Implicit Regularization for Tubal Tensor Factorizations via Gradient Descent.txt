Implicit Regularization for Tubal Tensor Factorizations via
Gradient Descent
Santhosh Karnik1,2 ∗, Anna Veselovska3,4 ∗, Mark Iwen5,2, Felix Krahmer3,4
1 Department of Mathematics, Northeastern University, Boston, USA
2Department of Computational Mathematics, Science, and Engineering,
Michigan State University, East Lansing, USA
3TUM School of Computation, Information and Technology & Munich Data Science Institute,
Technical University of Munich, Garching, Germany
4Munich Center for Machine Learning, Munich, Germany
5Department of Mathematics, Michigan State University, East Lansing, USA
October 22, 2024
Abstract
We provide a rigorous analysis of implicit regularization in an overparametrized tensor factorization
problem beyond the lazy training regime. For matrix factorization problems, this phenomenon has been
studiedinanumberofworks. Aparticularchallengehasbeentodesignuniversalinitializationstrategies
which provably lead to implicit regularization in gradient-descent methods. At the same time, it has
been argued by [7] that more general classes of neural networks can be captured by considering tensor
factorizations. However, in the tensor case, implicit regularization has only been rigorously established
for gradient flow or in the lazy training regime. In this paper, we prove the first tensor result of its kind
for gradient descent rather than gradient flow. We focus on the tubal tensor product and the associated
notion of low tubal rank, encouraged by the relevance of this model for image data. We establish that
gradient descent in an overparametrized tensor factorization model with a small random initialization
exhibits an implicit bias towards solutions of low tubal rank. Our theoretical findings are illustrated in
an extensive set of numerical simulations show-casing the dynamics predicted by our theory as well as
the crucial role of using a small random initialization.
1 Introduction
AnalyzingimplicitregularizationduringNeuralNetwork(NN)trainingisconsideredcrucialforunderstanding
why overparametrization can give rise to superior generalization capability and lead to strong overall NN
performance. Consequently, there has been a recent surge in research aimed at explaining how gradient-
based methods interact with overparameterized models under nonconvex losses (see, e.g., [28, 24]). Notably,
recent empirical and theoretical studies have suggested that gradient-based methods with small random
initializations exhibit a bias towards low-rank solutions in a variety of models.
Formatrixfactorizationmodelswhichrepresentlinearneuralnetworks,arigorousanalysisofimplicitbias
isavailableforbothgradientdescent[12,36]andgradientflow(itsasymptoticlimitforsmallstepsize)[3,6].
*SKandAVcontributedequallytothiswork. SKwaspreviouslyatMichiganStateUniversityandiscurrentlyatNortheastern
University. AV and FK acknowledge support by the German Science Foundation (DFG) in the context of the collaborative
researchcenterTR-109,theEmmyNoetherjuniorresearchgroupKR4512/1-1andtheBavarianFundingProgramforInitiating
InternationalResearchCooperationaswellasbytheMunichDataScienceInstituteandMunichCenterforMachineLearning. MI
acknowledgessupportbytheUnitedStatesNationalScienceFoundationgrantsNSFDMS2108479andNSFEDUDGE2152014.
EmailsSK:s.karnik@northeastern.edu,AV:anna.veselovska@tum.de,MI:iwenmark@msu.edu,FK:felix.krahmer@tum.de.
1
4202
tcO
12
]GL.sc[
1v74261.0142:viXraIn contrast, for neural networks with nonlinear activation, there has been a good deal of work done showing
thatfullyconnectedlayerscanberepresentedby,e.g.,tensortrainfactorizationsin[29,32]. Asaconsequence,
it has been argued that tensor factorizations should be considered instead of matrix factorizations (see, e.g.,
[7]). For tensor factorization models, however, results predating 2024 were only available for the asymptotic
regime, i.e., gradient flow. This is perhaps due to the many additional complications in the tensor setting
beyond those in the matrix setting including, e.g, that there are many different valid notions of tensor rank,
each of which motivates its own equally valid class of tensor factorizations. For gradient descent applied
to the tensor recovery problem, only a very recent partial analysis by [27] currently exists for the tubal
factorization model. This analysis requires that the initialization already well approximates the solution,
only after which the convergence of gradient descent toward a low tubal-rank solution is shown. Herein we
also focus on the tubal factorization, but establish the corresponding implicit regularization result without
needing such a strong initialization assumption.
Related work: In deep learning it is common to use more network parameters than training points. In
such overparameterized scenarios there are usually many networks that achieve zero training error so that
the training algorithm effectively imposes an implicit regularization (bias) on the solution it computes. In
practice,trainingnetworkswithgradientdescentisbothcommonandtendstofavorsolutionsthatgeneralize
well, offering the exploration of how gradient descent implicitly regularlizes in overparameterized regimes as
oneavenueforbetterunderstandingthesuccessofdeeplearningmorewidely. Asaresult,alotofrecentwork
has been focussed on understanding the implicit regularization phenomena of gradient descent in multiple
settings. Thefirsttheoreticalworksinthisdirection[13,12,9,2,35]concentratedontraininglinearnetworks
andsuggestedthatduringtraining(stochastic)gradientdescentimplicitlyconvergestoalinearnetwork(i.e.,
a linear function described by a matrix) that’s low rank. Motivated by specific deep learning tasks, multiple
works also investigated implicit bias phenomena in the special cases of sparse vector and low-rank matrix
recovery from underdetermined measurements via an overparameterized square loss functional, where the
vectors and matrices to be reconstructed were deeply factorized into several vector/matrix factors. In this
setting,theseworksthenshowedthatthedynamicsofvanillagradientdescentarebiasedtowardssparse/low-
rank solutions, respectively [6, 5, 23, 20].
In the realm of optimization, a substantial body of work has also emerged that provides guarantees
for gradient descent’s convergence in the nonconvex setting for different problems such as phase retrieval,
matrix completion, and blind deconvolution. Broadly, these findings can be categorized into two main
approaches: smart initialization coupled with local convergence (demonstrating, e.g., local convergence of
descent techniques starting from carefully designed spectral initializations) [28, 38, 24, 4]; and landscape
analysis paired with saddle-escaping algorithms which show, e.g., that all local minima are global and that
saddle points exhibit strict negative curvature so that (stochastic) gradient-based methods can effectively
escape saddles and ensure convergence to global minimizers [16, 8, 30].
Notably, several studies [43, 10] have highlighted the importance of the scale of the training initialization
forthegeneralizationandtestperformanceofmodernmachinelearningarchitectures. Infact,asmallrandom
initialization followed by (stochastic) gradient descent is arguably the most widely used training algorithm
in contemporary machine learning. And, stronger generalization performance is typically observed with
smaller-scale initializations. Implicit bias for low-rank matrix recovery with small random initializations has
been extensively studied in this setting as a result by, e.g., [36, 34, 42, 19]. These studies have shown that
a small random Gaussian initialization behaves similarly to a spectral initialization in overparameterized
settings. Furthermore, they have shown that gradient descent algorithms with this initialization tend to
converge towards low-rank solutions (i.e., that they demonstrate an implicit regularization towards low-rank
solutions).
Recently, numerous connections between tensor decompositions and training neural networks have also
been established by, e.g., [29, 32, 31]. These studies argue that low-rank tensor factorization helps explain
implicit regularization in deep learning, as well as how properties of real-world data translate this regu-
larization to generalization. Similar to how matrix factorization can be viewed as a linear neural network
(i.e., a fully connected network with linear activation), tensor factorizations correspond to a specific type of
shallow (depth-two) nonlinear convolutional neural network [7, 32]. Additionally, [29] demonstrated that the
dense weight matrices of fully connected layers can be converted to tensor trains while preserving the layer’s
2Figure 1: A low tubal-rank factorization of a three-dimensional tensor. Using the (reduced) tubal-SVD,
each three-dimensional tensor T ∈ Rn×m×k can be decomposed into a tubal product of three tensors T =
V∗Σ∗W⊤ with V ∈Rn×n×k, W ∈Rm×m×k and the frontal slice diagonal tensor Σ∈Rn×m×k. Here, the
tubal rank of a tensor is the number of non-zero singular tubes in Σ∈Rn×m×k. For example, in the figure,
the tubal rank of the tensor is equal to six.
expressive power. These findings have positioned low-rank tensor factorizations as theoretical surrogates for
various neural network learning settings, thereby enhancing our understanding of implicit regularization and
overparameterization, and so further motivating investigation in this area.
Since no unique definition of tensor rank is available, related literature concerning implicit bias has
naturally split with respect to the notion of tensor rank being considered: CP-rank, Tucker-rank, and tubal-
rank, in analogy to the analysis of algorithms specifically designed for tensor recovery and completion by,
e.g.,[44,15,21,1,26,25,14]. FortheCP-tensorfactorization,severalresultsareavailableforgradient-based
methods. The first theoretical analysis of implicit regularization towards low tensor rank under arbitrarily
smallinitializationwasprovidedconsideringgradientflowin[32]. In[8],ithasbeenshownfortheorthogonal
tensor decomposition problem a simple variant of the stochastic gradient algorithm is able to leverage a low-
rank structure from an arbitrary starting point. In addition, [40] shows that using gradient descent on an
over-parametrized objective for the CP-rank tensor decomposition problem one could go beyond the lazy
training regime and utilize certain low-rank structures.
Perhaps most closely related to this paper, very recently [27] analyzed the convergence of factorized
gradient descent for the low-tubal-rank sensing problem, showing that with carefully designed spectral ini-
tialization the gradient iterates converge to a low-tubal rank tensor. Although the authors in [27] allow for
overparametrization, they argue the minimal recovery error can be achieved when knowing the true rank,
therebyleavingquestionsconcerningtheadvantagesofoverparametrizationandsmallrandominitializations
open.
Ourcontribution: Motivatedbyconnectionsbetweentensorrankandnon-linearneuralnetworkrepresen-
tations, herein we study the implicit regularization phenomenon for low tubal-rank tensor recovery. Namely,
our objective is to analyze the recovery process of a tensor with a low tubal-rank factorization [17] (see Fig
1) from a limited number of random linear measurements. More specifically, we consider tensors of the form
X ∗X⊤ and employ a non-convex method based on the tensor factorization, minimizing the loss function
using gradient descent with a small random initialization. To the best of our knowledge, we are the first to
investigatetheimplicitbiasphenomenonforgradientdescentwithasmallrandominitializationappliedtoa
tensorfactorization. Namely,wedemonstratethat,irrespectiveofthedegreeofoverparameterization,vanilla
gradient descent with a small random initialization applied to a tubal tensor factorization will consistently
converge to a low tubal-rank solution.
Inspired by recent results for the low-rank matrix sensing problem by [36], we establish that gradient
descent iterates with small random initializations can be closely approximated by power method iterations
in [11, 18] modulo normalization, and deduce that after sufficient time the iterates approach a commonly
used spectral initialization from the tubal-rank literature in [27]. Along the way we must also overcome,
e.g., a challenging intersection between the tensor slices during each gradient descent iterate which forces a
3non-trivial convergence analysis.
Organization: In Section 2, we define our notation and present a few basic facts regarding tubal tensors.
In Section 3, we state our problem and our main result. In Section 4, we outline the steps of the proof in
order to provide intuition. In Section 5, we show numerical experiments which demonstrate our theoretical
findings. WeconcludethepaperinSection6. Theproofofourmainresultisbrokenupintoseverallemmas,
which are stated and proven in the appendix.
2 Notation and Preliminaries
Every tensor in this paper will be an order-3 tensor whose third mode is length k. For such a tensor
T ∈Rm×n×k, we define a block-diagonal Fourier domain representation by
T =blockdiag(T(1) ,...,T(k) )∈Cmk×nk
where the j-th block T(j) ∈Cm×n is defined by T(j) (i,i′)=(cid:80)k T(i,i′,j′)e−√ −12π(j−1)(j′−1)/k. In other
j′=1
words, we take the FFT of each tube, and then arrange the resulting frontal slices into a block-diagonal
matrix.
The tubal product (or t-product) of two tubal tensors A ∈ Rm×q×k and B ∈ Rq×n×k is a tubal tensor
A∗B∈Rm×n×k whose tubes are given by
q
(cid:88)
(A∗B)(i,i′,:)= A(i,p,:)∗B(p,i′,:)
p=1
where ∗ denotes the circular convolution of the two tubes. One can check that A∗B=A B.
ForanytubaltensorT ∈Rm×n×k,itstubaltransposeT⊤ ∈Rn×m×kisgivenby(T⊤)(i,i′,1)=T(i′,i,1)
and(T⊤)(i,i′,j)=T(i′,i,k+2−j)forj =2,...,k,i.e.,wetakethetransposeofeachface,andthenreverse
the order of frontal slices j =2,...,k. This ensures that T⊤ =T⊤ .
For any n, the n×n×k identity tensor I ∈ Rn×n×k is defined by I(:,:,1) = I (identity matrix),
n×n
and I(:,:,j)=0 (zero matrix). An orthogonal tensor Q∈Rn×n×k satisfies Q∗Q⊤ =Q⊤∗Q=I. An
n×n
orthonormal tensor W ∈Rm×n×k with m≥n satisfies W⊤∗W =I.
The tubal-SVD [17] (or t-SVD) of a tubal tensor T ∈Rm×n×k is a factorization of the form
T =U ∗Σ∗V⊤ (2.1)
whereU ∈Rm×m×k andV ∈Rn×n×k areorthogonal,andeachfrontalsliceofΣ∈Rm×n×k isdiagonal. The
t-SVD of a tensor T ∈Rm×n×k can be computed as follows: (1) compute the FFT of each tube of T to get
(j) (j) (j) (j) (j)⊤
thefrontalslicesT ,j =1,...,k,(2)computetheSVDofeachresultingfrontalsliceT =U Σ V ,
(3) concatenate the matrices {U(j) }k into a tubal tensor U(cid:101) ∈ Cm×m×k and take the inverse FFT along
j=1
mode-3 to obtain U ∈Rm×m×k (and similarly to obtain Σ∈Rm×n×k and V ∈Rn×n×k). Finally, the tubal
rank of a tensor T ∈ Rm×n×k is the number of non-zero diagonal tubes in the Σ tensor of its t-SVD, i.e.,
rank(T)=#{i:Σ(i,i,:)̸=0}. For an illustration of the t-SVD decomposition, see Figure 1. We also define
the condition number κ(T) of the tubal tensor T ∈Rm×n×k by
σ (T)
κ(T):= 1 .
σ (T)
min{m,n}k
3 Main Results
Problem Formulation Let X ∈ Rn×r×k have tubal rank r ≤ n so that X ∗X⊤ ∈ Sn×n×k is a tubal
+
positive definite tensor with tubal rank r. Let κ=κ(X) be the condition number of X. Suppose we observe
m linear measurements of X ∗X⊤, that is
(cid:68) (cid:69)
y = A ,X ∗X⊤ for i=1,...,m (3.1)
i i
4whereeachA ∈Sn×n×k isatubal-symmetrictensor. Wecanwritethiscompactlyasy =A(X∗X⊤)where
i
A:Sn×n×k →Rm isthelinearmeasurementoperator. WeaimtorecoverX∗X⊤ fromourmeasurementsy
by using gradient descent to learn an overparameterized factorization. Specifically, we fix an R ≥r and try
to find a U ∈Rn×R×k such that U∗U⊤ =X ∗X⊤ by using gradient descent to minimize the loss function ´
(cid:13) (cid:16) (cid:17) (cid:13)2
ℓ(U):=(cid:13)A U ∗U⊤ −y(cid:13) (3.2)
(cid:13) (cid:13)
2
m
(cid:88)(cid:16)(cid:68) (cid:69) (cid:17)2
= A ,U ∗U⊤ −y . (3.3)
i i
i=1
We will start with a small random initialization U ∈ Rn×R×k where each entry is i.i.d. N(0,α2) for
0 R
some small α>0. Then, the gradient descent iterations are given by
U =U −µ∇L(U )
t+1 t t
(cid:104) (cid:16) (cid:17)(cid:105)
=U +µA∗ y−A U ∗U⊤ ∗U
t t t t
(cid:104) (cid:16) (cid:17)(cid:105)
= I +µ(A∗A) X ∗X⊤−U ∗U⊤ ∗U (3.4)
t t t
for some suitably small stepsize µ>0. Here A∗ :Rm →Sn×n×k denotes the adjoint of A which is given by
A∗z =(cid:80)m z A .
i=1 i i
Moreover, we say that a measurement operator A : Sn×n×k → Rm satisfies the Restricted Isometry
Property (RIP) of rank-r with constant δ >0 (abbreviated RIP(r,δ)), if we have
(1−δ)∥Z∥2 ≤∥A(Z)∥2 ≤(1+δ)∥Z∥2,
F 2 F
for all Z ∈Sn×n×k with tubal-rank ≤r.
Results We have analyzed the convergence process of the gradient descent iterates (3.4) in the scenario of
smallrandominitializationandoverparametrization. Namely,withthegroundtruthtensorX ∈Rn×r×k,we
assumetheinitializationU ∈Rn×R×k issuchthateachentryisi.i.d. N(0,α2)withsmallscalingparameter
0 r
α > 0 and the second dimension R exceeding the ground truth dimension r. Below, we present the direct
results of our analysis.
Theorem 3.1. Suppose we have m linear measurements y = A(X ∗X⊤) of a tubal positive semidefinite
tensor X ∗X⊤ ∈ Sn×n×k where X ∈ Rn×r×k has tubal rank r ≤ n. We assume A satisfies RIP(2r+1,δ)
+
with δ ≤ cκ−4r−1/2. Suppose we fit a model X ∗X⊤ = U ∗U⊤ where U ∈ Rn×R×k with R ≥ r and obtain
U by running the gradient descent iterations
(cid:104) (cid:16) (cid:17)(cid:105)
U = I +µ(A∗A) X ∗X⊤−U ∗U⊤ ∗U
t+1 t t t
starting from the initialization U ∈ Rn×R×k where each entry is i.i.d. N(0,α2). Then, if the over-
0 r
parametrized model satisfies R≥3r and the scale of the initialization satisfies
(cid:32) √ (cid:33)−16κ2
σ (X) C κ2 n
α≲ min √ 2 ,
(cid:112)
κ2min{n,R} k min{n,R}
then after
(cid:18) (cid:26) (cid:27) (cid:19)
1 C κn κr ∥X∥
(cid:98)t≲ ln 1 min 1,
µσ (X)2 min{n,R} k(min{n,R}−r) kα
min
iterations, we have that
∥U U⊤−X ∗X⊤∥2 61 1 3 3 (cid:32) C κ2√ n (cid:33)21κ2 (cid:18) α (cid:19)2 11 6
(cid:98)t (cid:98)t
∥X∥2
F ≲k32r8κ− 16(min{n,R}−r)8
(cid:112)
m2
in{n,R} ∥X∥
holds with probability at least 1−Cke−c˜r. Here, c,c˜,C,C ,C > 0 are fixed numerical constants. Here,
1 2
c,C ,C >0 are fixed numerical constants.
1 2
5Intuitively, this means that if the initialization is sufficiently small, gradient descent will recover the low
tubal rank tensor X ∗X⊤. Note that the reconstruction error can be made arbitrarily small by making the
size of the random initialization α arbitrarily small. This comes at the expense of requiring more iterations.
However, this impact is mild as the number of iterations grows only logarithmically with respect to α.
(a) (b)
Figure 2: Illustration of (a) the two stages of gradient descent algorithm: the spectral alignment stage for
1≤t≲3000andtheconvergencestage3000≲tand(b)moredetailsonthealignmentphaseforthegradient
descent progress. In the ground truth tensor X ∈Rn×r×k, we set n=10,k =4,r =3.
4 Proof Outline
In this section, we turn our attention to giving an overview of the key ideas of the proof.
In our analysis, we demonstrate that the trajectory of gradient descent iterations can be approximately
divided into two distinct stages: (I) a spectral stage and (II) a convergence stage described below.
(I) The spectral stage. In the spectral stage, where we show that the gradient descent starting from
random initialization behaves similarly to spectral initialization, enabling us to prove that by the end of this
stage,thecolumnspacesofthetensoriteratesU (3.4)andthegroundtruthmatrixX aresufficientlyaligned.
t
Namely, we show that the first few iterations of the gradient descent algorithm U can be approximated by
t
the iteration of the tensor power method modulo normalization (see, e.g.[11]) defined as
(cid:16) (cid:17)∗t
U(cid:101)t = I +µA∗A(X ∗X⊤) ∗U
0
∈Rn×R×k.
We call this part of the evolution of the gradient descent iteration the “spectral stage” since, due to its
similarity to the power method, at the end of this stage the iterates U will be closely aligned with the
t
classical t-SVD spectral initialization of [27].
(II) The convergence stage. Intheconvergencestage, thegradientiteratesconvergeapproximatelytothe
underlying low tubal-rank tensor X ∗X⊤ at a geometric rate until reaching a certain error floor which is
dependent on the initialization scale.
The cornerstone of the analysis of this stage is the decomposition of the tensor gradient iterates U into
t
two components, the so-called “signal” and “noise” terms. This is done by adapting similar decomposition
methods used in recent works analyzing implicit bias phenomenon for gradient descent in the matrix setting
(see [36, 22]) to our tensor setting. Accordingly, let the tensor-column subspace of the ground truth tensor
X ∈Rn×r×k bedenotedbyV withthecorrespondingbasis V ∈Rn×r×k. Considerthetensor V ∗U ∈
X X X t
Rr×R×k with its t-SVD decomposition V ∗U =V ∗Σ ∗W⊤. For W ∈RR×r×k, we denote by W ∈
X t t t t t t,⊥
RR×(n−r)×k a tensor whose tensor-column subspace is orthogonal to those of W , that is ∥W⊤ ∗W ∥=0
t t,⊥ t
and its projection operator P is defined as P =W ∗W⊤ =I −W ∗W⊤.
Wt,⊥ Wt,⊥ t,⊥ t,⊥ t t
6We then decompose the gradient descent iterates (3.4) as follows
U =U ∗W ∗W⊤+U ∗W ∗W⊤ (4.1)
t t t t t t,⊥ t,⊥
referring to the tensors U ∗W ∗W⊤ as the signal term of the gradient descent iterates, and to the tensors
t t t
U ∗W ∗W⊤ as the noise term. The advantage of such a decomposition is that the tensor-column space
t t,⊥ t,⊥
of the noise term U ∗W ∗W⊤ is orthogonal to the tensor-column subspace of the ground truth X
t t,⊥ t,⊥
allowing for a rigorous analysis of the convergence process of the two components separately.
At the convergence stage, we show that symmetric tensor U ∗W ∗W⊤∗U⊤ built from the signal term
t t t t
convergestowardsthegroundtruthtensorX∗X⊤,whereasthespectralnormofthenoiseterm∥U ∗W ∥,
t t,⊥
stays small.
(a) (b)
(c) (d)
Figure 3: Outcomes of employing gradient descent to minimize the loss function (3.2) with different over-
parametrization rates. We set n = 10,k = 4,r = 3 in the ground truth tensor X ∈ Rn×r×k and for
initializationU ∈Rn×R×k, wesettheover-ranktoR=10,50,100,200,400. ForeachR weplottheaverage
0
over twenty experiments. The plots (a),(b), and (d) are semi-log plots.
Additional challenges in the tensor setting vs. matrix setting When coming from the matrix case
to the tensor setting com, there are several important differences and challenges, which need to be carefully
considered and are described below.
• Incontrasttothematrixcase,therangeandkernelofathird-ordertubaltensorcanincludeoverlapping
generator elements (we refrain from using the term basis, in the sense that knowledge of the multirank
7and complimentary tubal scalar of a tensor must be included to describe the range). Namely, if in the
t-SVD (2.1) of a symmetric tensor X the tensor Σ contains q non-invertible tubes – tubes that have
zeroelementsintheFourierdomain–,thenthereareq commongeneratorsfortherangeandthekernel
of X, please see [18] for more details. With this phenomenon, the decomposition (C.1) of the gradient
iterates into signal and noise term is not available for non-invertible tubes, which is why we need to
work with a more intricate notion of condition number.
• As stated in [11], running the power method for tubal tensors of dimensions n × n × k is equiva-
lent to running in parallel k independent matrix power methods in Fourier domain. However, run-
ning gradient descent in the tubal tensor setting is not equivalent to running k gradient descent
algorithms independently in Fourier space. This can be easily seen when transforming the mea-
surement operator part of the gradient descent iterates. Namely, let as before y = A(X ∗ X⊤) ∈
Rm with y = (cid:68) A ,X ∗X⊤(cid:69) = (cid:68) A ,X ∗X⊤(cid:69) = (cid:80)k (cid:10) A (q),X(q)X(q)H(cid:11) , j = 1,...m then
i i i q=1 i
A∗A(X ∗X⊤) = A∗(y) = (cid:80)m y A ∈ Sn×n×k and the for j-th slice in the Fourier domain, we get
i=1 i i
A∗A(X ∗X⊤)(j) =(cid:80)m (cid:80)k A (j)(cid:10) A (q),X(q)X(q)H(cid:11) . This means that in each Fourier slice U (j) of
i=1 j=1 i i t
the gradient descent iterates (3.4) we have the full information about the ground truth tensor X ∗X⊤
and not only about its j-th slice. In the spectral stage, this fact does not cause significant difficulties.
However, in the convergence stage, in order to get the global estimates, it requires a thorough and
vigilant analysis of intersections between the slices in the Fourier domain.
5 Numerical Experiments
To verify our theoretical findings, we set multiple numerical tests: from showing two phases of the gradient
descentalgorithmtodemonstratingtheadvantagesofoverparametrization. Theseexperimentalresultsshow-
case not only the implicit regularization for the gradient descent algorithm toward low-tubal-rank tensors
but also demonstrate the firmness of our theoretical findings.
Our experiments were conducted on a MacBook Pro equipped with an Apple M1 processor and 16GB of
memory, using MATLAB 2023a software.
WegeneratethegroundtruthtensorT ∈Rn×n×k withtubalrankrbyT =X∗X⊤ ,wheretheentriesof
X ∈Rn×r×k are i.i.d. sampled from a Gaussian distribution N(0,1), and then X is normalized. The entries
of measurement tensor A are i.i.d. sampled from a Gaussian distribution N(0, 1). In the following, we
i m
describe different testing scenarios for recovery of T via the gradient descent algorithm and their outcome.
For all the experiments, we set the dimensions to n = 10,k = 4,r = 3, the learning rate µ = 10−5, and the
number of measurements m=254.
Illustration of the two convergence stages. To illustrate the convergence process of the gradient
iterates, for the ground truth tensor X ∗ X⊤ ∈ Rn×n×k and its counterpart U ∗ U⊤ ∈ Rn×n×k being
t t
learned by the gradient descent, we consider the training error ℓ(U t), the test error ∥Ut∗ ∥U X⊤ t ∗− XX ⊤∗ ∥X F⊤∥F, and
the test error for their rth singular tubes σ (U ),σ (X) ∈ Rk, ∥σr(Ut)−σr(X)∥2. Moreover, we also take
r t r ∥σr(X)∥2
into our consideration the tensor subspace L spanned by the tensor-columns corresponding to the first r
singular-tubes of the tensor A∗A(X ∗X⊤) and denote by L the tensor-column subspace spanned by the
t
tensor-columns corresponding to the first r singular tubes U ∗U⊤. Figures 2a and 2b demonstrate that the
t t
convergence analysis can be divided into two stages: the spectral and the convergence stage. We see that
in the first stage (1 ≤ t ≲ 3000), the first r tensor-columns of U ∗U⊤ learn the tensor column subspace
t t
corresponding to the first r singular-tubes of the tensor A∗A(X ∗X⊤), i.e. the principal angle between the
tensor column subspaces L and L becomes small. Namely, as one can observe in Figure 2b, the principal
t
angle between the two subspaces, ∥V⊤ ∗V ∥, decreases where as the principal angle between X and L
L⊥ Lt t
reachescertainplateau,seethebehaviorof∥V⊤ X⊥∗V Lt∥. Atthesametime,testerrors ∥Ut∗ ∥U X⊤ t ∗− XX ⊤∗ ∥X F⊤∥F and
∥σr(Ut)−σr(X)∥2 staylarge. Inthesecondstage, weseethatthetesterror ∥Ut∗U⊤ t −X∗X⊤∥F startsdecreasing,
∥σr(X)∥2 ∥X∗X⊤∥F
meaning that the gradient descent iterates U ∗U⊤ start converging to X ∗X⊤ by learning more about the
t t
8tensor-column subspace of the ground truth tensor. At the same time, the test error over rth singular tube
∥σr(Ut)−σr(X)∥2 starts decreasing too and as a result converges to zero. We also see that in this stage the
∥σr(X)∥2
principal angle between L and L grows, which is also intuitive as the tensor-column subspace L does not
t
havethefullinformationaboutthetensor-columnsubspaceofthegroundtruthtensorX∗X⊤, andlearning
more about X ∗X⊤ leads to a larger error in terms of principal angles of the two.
Depiction of the alignment stage. In this experiment, we illustrate that gradient descent with small
initialization behaves similarly to the tensor-power method modulo normalization in the first few iterations,
bringing the gradient iterates close to the spectral tubal initialization, used, e.g., in [27]. Here, as before
L denote the tensor subspace spanned by the tensor-columns corresponding to the first r singular-tubes
of tensor A∗A(X ∗X⊤) and L is the tensor-column subspace corresponding to the first r singular tubes
t
U t∗U⊤
t
. Additionally, L(cid:101)t denotes the tensor-column subspace spanned by the first r singular-tubes of the
tensor
U(cid:101)t∗U(cid:101)⊤
t
, where
U(cid:101)⊤
t
=(cid:16)
I
+A∗A(cid:0)
X
∗X⊤(cid:1)(cid:17)∗t
∗U 0. In Figure 2b, we see that U
t
and U(cid:101)t learn the
subspace L almost at the same rate in the first iterations, 1≤t≲3000. In Figure 2b, we observe that also
the angle between V
X
and L t, respectively L(cid:101)t, decreases monotonically in the spectral stage. Then at the
beginning of the convergence stage, 3000≲t, the angle between V and L starts decreasing gradually and
X t
converges to zero, as expected since U ∗U⊤ converges to X ∗X⊤. Whereas the principal angle between L
t t
and L growths until it reaches a certain plateau.
t
Figure4: Impactofdifferentinitializationscalesonthetestandthetrainingerror. Thedataarerepresented
inthesemi-logplot. Wesetn=10,k =4,r =3inthegroundtruthtensorX ∈Rn×r×k andforinitialization
U = αU ∈ Rn×R×k with R = 200 and different scales of α. The plot depicts the averaged value for five
0
runs and the bars represent the deviations from the mean value.
Test and train error under different scales of initialization. In this experiment, we explore the
influenceoftheinitializationscale,denotedbyα,onthetrainingandthetesterror. WithR=200,weapply
gradientdescentforvariousvaluesofα,haltingtheiterationsatt=3500ineachrun. Theresults,presented
in Figure 4, demonstrate a reduction in test error as α decreases. Notably, the figure indicates that the test
error follows an almost polynomial relationship with the initialization scale α. This observation is consistent
with our theoretical predictions, which also forecast a decrease in test error at a rate of α, see Theorem 3.1.
Impact of different levels of overparameterization on the convergence. Inthisnumericalanalysis,
we set α = 10−7 and examined the convergence speed of gradient descent to the ground truth tensor for
various overparameterization rates R. We run the experiment twenty times for each value of R and plot
the averaged values per each iteration. The results, shown in Figure 3, reveal that increasing the number of
9tensor columns R, that is, overparameterizing, accelerates the convergence rate, resulting in fewer iterations
to reach the desired error level. Additionally, overparameterization reduces the test error and the training
error by affecting the spectral stages.
6 Conclusion and Outlook
In this paper, we focused on studying the implicit regularization of tubal tensor factorizations via gradient
descent by showing that with small random initialization and overparametrization, the gradient descent
algorithm is biased towards a low-tubal-rank solution. We have shown that the first iterations of gradient
descentwithsmallrandominitializationbehavesimilarlytothetensorpowermethod,whichleadstolearning
in these first interactions the tensor-column spaces close to the tensor-column space of the ground truth. We
alsodemonstratethattheimplicitregularizationfromsmallrandominitializationguidesthegradientdescent
iterations toward low-tubal rank solutions that are not only globally optimal but also generalize well.
References
[1] TalalAhmed,HaroonRaja,andWaheedUBajwa.“Tensorregressionusinglow-rankandsparseTucker
decompositions”. In: SIAM Journal on Mathematics of Data Science 2.4 (2020), pp. 944–966.
[2] Sanjeev Arora et al. “Implicit regularization in deep matrix factorization”. In: Advances in Neural
Information Processing Systems 32 (2019).
[3] BubacarrBahetal.“Learningdeeplinearneuralnetworks:Riemanniangradientflowsandconvergence
to global minimizers”. In: Information and Inference: A Journal of the IMA 11.1 (2022), pp. 307–353.
[4] Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. “Phase retrieval via Wirtinger flow:
Theory and algorithms”. In: IEEE Transactions on Information Theory 61.4 (2015), pp. 1985–2007.
[5] Hung-Hsu Chou, Johannes Maly, and Holger Rauhut. “More is less: inducing sparsity via overparame-
terization”. In: Information and Inference: A Journal of the IMA 12.3 (2023), pp. 1437–1460.
[6] Hung-Hsu Chou et al. “Gradient descent for deep matrix factorization: Dynamics and implicit bias
towards low rank”. In: Applied and Computational Harmonic Analysis 68 (2024), p. 101595.
[7] Nadav Cohen, Or Sharir, and Amnon Shashua. “On the expressive power of deep learning: A tensor
analysis”. In: Conference on learning theory. PMLR. 2016, pp. 698–728.
[8] RongGeetal.“Escapingfromsaddlepoints—onlinestochasticgradientfortensordecomposition”.In:
Conference on learning theory. PMLR. 2015, pp. 797–842.
[9] Kelly Geyer, Anastasios Kyrillidis, and Amir Kalev. “Low-rank regularization and solution unique-
ness in over-parameterized matrix sensing”. In: International Conference on Artificial Intelligence and
Statistics. PMLR. 2020, pp. 930–940.
[10] Behrooz Ghorbani et al. “When do neural networks outperform kernel methods?” In: Advances in
Neural Information Processing Systems 33 (2020), pp. 14820–14830.
[11] David F Gleich, Chen Greif, and James M Varah. “The power and Arnoldi methods in an algebra of
circulants”. In: Numerical Linear Algebra with Applications 20.5 (2013), pp. 809–831.
[12] Suriya Gunasekar et al. “Implicit bias of gradient descent on linear convolutional networks”. In: Ad-
vances in neural information processing systems 31 (2018).
[13] Suriya Gunasekar et al. “Implicit regularization in matrix factorization”. In: Advances in neural infor-
mation processing systems 30 (2017).
[14] CullenHaselbyetal.TensorDeli:TensorCompletionforLowCP-RankTensorsviaRandomSampling.
2024. arXiv: 2403.09932 [math.NA].
[15] Jingyao Hou et al. “Robust low-tubal-rank tensor recovery from binary measurements”. In: IEEE
Transactions on Pattern Analysis and Machine Intelligence 44.8 (2021), pp. 4355–4373.
10[16] ChiJinetal.“Howtoescapesaddlepointsefficiently”.In:Internationalconferenceonmachinelearning.
PMLR. 2017, pp. 1724–1732.
[17] Misha E Kilmer and Carla D Martin. “Factorization strategies for third-order tensors”. In: Linear
Algebra and its Applications 435.3 (2011), pp. 641–658.
[18] Misha E Kilmer et al. “Third-order tensors as operators on matrices: A theoretical and computational
framework with applications in imaging”. In: SIAM Journal on Matrix Analysis and Applications 34.1
(2013), pp. 148–172.
[19] DaesungKimandHyeWonChung.“Rank-1matrixcompletionwithgradientdescentandsmallrandom
initialization”. In: Advances in Neural Information Processing Systems 36 (2024).
[20] Chris Kolb et al. “Smoothing the edges: A general framework for smooth optimization in sparse regu-
larization using Hadamard overparametrization”. In: arXiv preprint arXiv:2307.03571 (2023).
[21] HaoKong,XingyuXie,andZhouchenLin.“t-Schatten-pnormforlow-ranktensorrecovery”.In:IEEE
Journal of Selected Topics in Signal Processing 12.6 (2018), pp. 1405–1419.
[22] Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. “Algorithmic regularization in over-parameterized ma-
trix sensing and neural networks with quadratic activations”. In: Conference On Learning Theory.
PMLR. 2018, pp. 2–47.
[23] Zonglin Li et al. “The lazy neuron phenomenon: On emergence of activation sparsity in transformers”.
In: arXiv preprint arXiv:2210.06313 (2022).
[24] Shuyang Ling and Thomas Strohmer. “Regularized gradient descent: a non-convex recipe for fast joint
blind deconvolution and demixing”. In: Information and Inference: A Journal of the IMA 8.1 (2019),
pp. 1–49.
[25] Xiao-YangLiuetal.“Low-Tubal-RankTensorCompletionUsingAlternatingMinimization”.In:IEEE
Transactions on Information Theory 66.3 (2020), pp. 1714–1737. doi: 10.1109/TIT.2019.2959980.
[26] Xiao-Yang Liu et al. “Low-tubal-rank tensor completion using alternating minimization”. In: IEEE
Transactions on Information Theory 66.3 (2019), pp. 1714–1737.
[27] ZhiyuLiuetal.“Low-Tubal-RankTensorRecoveryviaFactorizedGradientDescent”.In:arXivpreprint
arXiv:2401.11940 (2024).
[28] CongMaetal.“Implicitregularizationinnonconvexstatisticalestimation:Gradientdescentconverges
linearlyforphaseretrievalandmatrixcompletion”.In:International Conference on Machine Learning.
PMLR. 2018, pp. 3345–3354.
[29] AlexanderNovikovetal.“Tensorizingneuralnetworks”.In:Advances in neural information processing
systems 28 (2015).
[30] MaximRaginsky,AlexanderRakhlin,andMatusTelgarsky.“Non-convexlearningviastochasticgradi-
ent langevin dynamics: a nonasymptotic analysis”. In: Conference on Learning Theory. PMLR. 2017,
pp. 1674–1703.
[31] Noam Razin, Asaf Maman, and Nadav Cohen. “Implicit regularization in hierarchical tensor factor-
ization and deep convolutional neural networks”. In: International Conference on Machine Learning.
PMLR. 2022, pp. 18422–18462.
[32] Noam Razin, Asaf Maman, and Nadav Cohen. “Implicit regularization in tensor factorization”. In:
International Conference on Machine Learning. PMLR. 2021, pp. 8913–8924.
[33] Mark Rudelson and Roman Vershynin. “Smallest singular value of a random rectangular matrix”.
In: Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of
Mathematical Sciences 62.12 (2009), pp. 1707–1739.
[34] MahdiSoltanolkotabi,DominikSt¨oger,andChangzhiXie.“Implicitbalancingandregularization:Gen-
eralization and convergence guarantees for overparameterized asymmetric matrix sensing”. In: The
Thirty Sixth Annual Conference on Learning Theory. PMLR. 2023, pp. 5140–5142.
[35] Daniel Soudry et al. “The implicit bias of gradient descent on separable data”. In: Journal of Machine
Learning Research 19.70 (2018), pp. 1–57.
11[36] Dominik St¨oger and Mahdi Soltanolkotabi. “Small random initialization is akin to spectral learning:
Optimizationandgeneralizationguaranteesforoverparameterizedlow-rankmatrixreconstruction”.In:
Advances in Neural Information Processing Systems 34 (2021), pp. 23831–23843.
[37] Terence Tao and Van Vu. “Random matrices: The distribution of the smallest singular values”. In:
Geometric And Functional Analysis 20 (2010), pp. 260–297.
[38] StephenTuetal.“Low-ranksolutionsoflinearmatrixequationsviaprocrustesflow”.In:International
Conference on Machine Learning. PMLR. 2016, pp. 964–973.
[39] Roman Vershynin. High-dimensional probability: An introduction with applications in data science.
Vol. 47. Cambridge university press, 2018.
[40] Xiang Wang et al. “Beyond lazy training for over-parameterized tensor decomposition”. In: Advances
in Neural Information Processing Systems 33 (2020), pp. 21934–21944.
[41] Per-˚Ake Wedin. “Perturbation bounds in connection with singular value decomposition”. In: BIT Nu-
merical Mathematics 12 (1972), pp. 99–111.
[42] Johan S Wind. “Asymmetric matrix sensing by gradient descent with small random initialization”. In:
arXiv preprint arXiv:2309.01796 (2023).
[43] Blake Woodworth et al. “Kernel and rich regimes in overparametrized models”. In: Conference on
Learning Theory. PMLR. 2020, pp. 3635–3673.
[44] FengZhangetal.“Tensorrestrictedisometrypropertyanalysisforalargeclassofrandommeasurement
ensembles”. In: arXiv preprint arXiv:1906.01198 (2019).
A Outline of Appendices
In Appendix B, we define some additional notation, including the angles between two tensor-column sub-
spaces. In Appendix C, we decompose the gradient descent iterates into a “signal” term and a “noise” term,
which will aid us in our analysis. In Appendices D and E, we analyze the spectral and convergence stages,
respectively, of the gradient descent iterations. In Appendix F, we prove our main result.
To avoid breaking up the flow of our analysis, we put some technical lemmas in the last few appendices
insteadofinthepreviouslymentionedappendices. InAppendixG,weprovesomepropertiesofmeasurement
operatorswhichsatisfytherestrictedisometryproperty. InAppendixH,weprovesomepropertiesofmatrices
and their subspaces. Finally, in Appendix I, we prove some properties of random Gaussian tubal tensors.
B Additional Notation
For a tensor Y ∈ Rn×r×k, we denote its t-SVD by Y = V ∗Σ ∗W⊤ with the two orthogonal tensor
Y Y Y
V ,W ∈ Rn×r×k, and the f-diagonal tensor Σ ∈ Rr×r×k. We will refer to V as the tensor-column
Y Y Y Y
subspace of Y and by V ∈Rn×(n−r)×k we denote the tensor-column subspace orthogonal to V with its
Y⊥ Y
projection operator V ∗V⊤ =I−V ∗V⊤.
Y⊥ Y⊥ Y Y
We measure the angles between two tensor-column subspaces Y and Y by the tensor-spectral norm
1 2
∥V ∗V ∥ which according to [26, 11, 17] is equal to
Y⊥
1
Y2
∥V⊤
Y⊥
1
∗V Y2∥=∥V⊤
Y⊥
1
∗V Y2∥=(cid:13) (cid:13)V⊤
Y⊥
1
V Y2(cid:13) (cid:13).
which means that the largest principal angle between Y and Y equals to that of these two subspaces
1 2
represented in the Fourier domain. In the Fourier domain, since V⊤ ∈C(n−r)k×nk and V ∈Cnk×nk are
Y⊥
1
Y2
12block diagonal matrices, it holds that
(cid:13)  (cid:13)
(cid:13) (cid:13)V⊤ Y⊥
1
V Y2(cid:13)
(cid:13)=(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)   
V⊤ Y⊥
1
(1)
V⊤ Y⊥ 1 (2) ...    

  
V Y2(1)
V Y2(2) ...

  
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)  (cid:13)
(cid:13)
(cid:13)
V⊤ Y⊥(k) V Y2(k) (cid:13)
(cid:13)
1
= 1m ≤ja ≤x k(cid:13) (cid:13)V⊤
Y⊥
1
(j)V Y2(j)(cid:13) (cid:13)
C Signal Decomposition
Recall that the gradient descent iterates are defined in (3.4) as
U =U −µ∇ℓ(U )
t+1 t t
(cid:104) (cid:16) (cid:17)(cid:105)
=U +µA∗ y−A U ∗U⊤ ∗U
t t t t
(cid:104) (cid:16) (cid:17)(cid:105)
= I +µ(A∗A) X ∗X⊤−U ∗U⊤ ∗U .
t t t
ForthegroundtruthtensorX ∈Rn×r×k,consideritstensor-columnsubspaceV withthecorresponding
X
basis V ∈ Rn×r×k. Consider the tensor V ∗U ∈ Rr×R×k with its t-SVD decomposition V ∗U =
X X t X t
V ∗Σ ∗W⊤. ForW ∈RR×r×k,wedenotebyW ∈RR×(n−r)×k atensorwhosetensor-columnsubspace
t t t t t,⊥
is orthogonal to those of W , that is ∥W⊤ ∗W ∥ = 0 and its projection operator P is defined as
t t,⊥ t Wt,⊥
P =W ∗W⊤ =I−W ∗W⊤. We then decompose the gradient descent iterates U as follows
Wt,⊥ t,⊥ t,⊥ t t t
U =U ∗W ∗W⊤+U ∗W ∗W⊤ (C.1)
t t t t t t,⊥ t,⊥
WewillrefertothetensorsU ∗W ∗W⊤ asthesignaltermofthegradientdescentiterates,andthetensors
t t t
U ∗W ∗W⊤ will be named as the noise term.
t t,⊥ t,⊥
LemmaC.1. Thetensor-columnspaceofthenoisetermU ∗W ∗W⊤ isorthogonaltothetensor-column
t t,⊥ t,⊥
subspace of the X, namely V⊤ ∗U ∗W ∗W⊤ = 0. Moreover, if V⊤ ∗U is full tubal-rank with all
X t t,⊥ t,⊥ X t
invertible singular tubes, then the signal term
U ∗W ∗W⊤
t t t
has tubal-rank r with all invertible singular tubes and the noise term has tubal rank at most R−r.
Proof. V⊤ ∗U ∗W ∗W⊤ =V⊤ ∗U ∗(I−W ∗W⊤)=V⊤ ∗U −V⊤ ∗U ∗W ∗W⊤ =0∈Rr×R×k.
X t t,⊥ t,⊥ X t t t X t X t t t
The second part follows fact that if V⊤ ∗U is full tubal rank with all invertible singular tubes then all the
X t
slices in the Fourier have full rank.
D Analysis of the spectral stage
The goal of this section is to show that the first few iterations of the gradient descent algorithm can be
approximated by the iteration of the tensor power method modulo normalization defined as
(cid:16) (cid:17)∗t
U(cid:101)t = I +µA∗A(X ∗X⊤) ∗U
0
=Z t∗U
0
∈Rn×R×k.
(cid:16) (cid:17)∗t
with the tensor power method iteration Z =: I+µA∗A(X ∗X⊤) ∈Rn×n×k. Moreover, this will result
t
in the feature that after the first few iterations, the tensor-column span of the signal term U ∗W ∗W⊤
t t t
13becomes aligned with the tensor-column span of X, and that the noise term U ∗W is relatively small
t t,⊥
compared to signal term in terms of the norm, indicating that the signal term dominates the noise term.
For this, let us denote the difference between the power method and the gradient descent iterations by
E
t
:=U t−U(cid:101)t. (D.1)
Forconvenience,throughoutthissection,wewilldenotebyMthetensorM:=A∗A(X∗X⊤)∈Rn×n×k,
so that U(cid:101)t =(I+µM)∗t∗U
0
and Z
t
=(I+µM)∗t.
In the first result of this section, the following lemma, we show that E can be made small via an
t
appropriate initialization scale.
Lemma D.1. Suppose that A:Sn×n×k →Rm satisfies RIP(2,δ ) and let t⋆ be defined as
1
(cid:110) (cid:111)
t⋆ =min j ∈N: ∥U(cid:101)j−1−U j−1∥>∥U(cid:101)j−1∥ . (D.2)
Then for all integers t such that 1≤t≤t⋆ it holds that
√ (cid:112) α3
∥E t∥=∥U t−U(cid:101)t∥≤8(1+δ
1
k) kmin{n,R} ∥M∥∥U∥3(1+µ∥M∥)3t. (D.3)
Proof. Similarly to the matrix case in [36], in the tubal tensor case it can be shown that for t ≥ 1, the
difference tensor E
t
=U t−U(cid:101)t can be represented as
t
(cid:88)
E
t
=U t−U(cid:101)t = (I +µM)∗(t−j)E(cid:98)j (D.4)
j=1
with E(cid:98)j = µA∗A(cid:0) U
j−1
∗U⊤ j−1(cid:1) ∗U j−1. To estimate ∥E t∥, we will first estimate each summand in (D.4)
separately. First, we can proceed with the following simple estimation
∥(I +µM)∗(t−j)E(cid:98)j∥≤∥(I +µM)∥(t−j)∥E(cid:98)j∥≤(cid:0) 1+µ∥M∥(cid:1)(t−j) ∥E(cid:98)j∥.
Now, for ∥E(cid:98)j∥, using the fact that the spectral norm of tubal tensors is sub-multiplicative, we get that
∥E(cid:98)j∥=µ∥A∗A(cid:0) U j−1∗U⊤ j−1(cid:1) ∗U j−1∥≤µ∥A∗A(cid:0) U j−1∗U⊤ j−1(cid:1) ∥·∥U j−1∥.
√
Since operator A satisfies RIP(2,δ ), by Lemma G.3, A also satisfies S2NRIP(δ k), which provides the
1 1
following estimate
√ √
∥A∗A(cid:0) U ∗U⊤ (cid:1) ∥≤(1+δ k)∥U ∗U⊤ ∥ =(1+δ k)∥U ∥2.
j−1 j−1 1 j−1 j−1 ∗ 1 j−1 F
All this together leads to
√ t
∥E t∥=∥U t−U(cid:101)t∥≤µ(1+δ
1
k)(cid:88)(cid:0) 1+µ∥M∥(cid:1)(t−j) ∥U j−1∥2 F∥U j−1∥. (D.5)
j=1
From here, we want to bound ∥E ∥ in terms of the initialization scale α and the data-related norm ∥M∥.
t
For this, we first use the fact that the tensor Frobenius norm above can be bounded as ∥U ∥ ≤
j−1 F
(cid:112)
kmin{n,R}∥U j−1∥. Then since for all 1 ≤ j ≤ t⋆ we have ∥U(cid:101)j−1 − U j−1∥ ≤ ∥U(cid:101)j−1∥, the spectral
norm of U can be bounded as
j−1
∥U j−1∥≤∥U(cid:101)j−1∥+∥U j−1−U(cid:101)j−1∥≤2∥U(cid:101)j−1∥.
This gives us the following upper bound
14√ t
(cid:112) (cid:88)
∥E t∥≤8µ(1+δ
1
k) kmin{n,R} (1+µ∥M∥)t−j∥U(cid:101)j−1∥3. (D.6)
j=1
As for iterations of the tensor power method, it holds that
∥U(cid:101)j−1∥=∥(I+µM)∗(j−1)∗U 0∥≤∥(I+µM)∗(j−1)∥∥U 0∥≤(1+µ∥M∥)j−1∥U 0∥=α(1+µ∥M∥)j−1∥U∥,
we can proceed with (D.6) as follows
√ t
(cid:112) (cid:88)
∥E ∥≤8µ(1+δ k) kmin{n,R}α3∥U∥3 (1+µ∥M∥)t+2j−3.
t 1
j=1
Now, the sum on the right-hand side can be estimated as
(cid:88)t (cid:88)t (1+µ∥M∥)2t−1
(1+µ∥M∥)t+2j−3 =(1+µ∥M∥)t−1 (1+µ∥M∥)2j−2 =(1+µ∥M∥)t−1
(1+µ∥M∥)2−1
j=1 j=1
(1+µ∥M∥)2t−1 (1+µ∥M∥)3t
=(1+µ∥M∥)t−1 ≤ ,
µ∥M∥(2+µ∥M∥) µ∥M∥
which gives us the final estimation for the norm of E as follows
t
√ (cid:112) α3
∥E ∥≤8(1+δ k) kmin{n,R} ∥U∥3(1+µ∥M∥)3t
t 1 ∥M∥
and finishes the proof.
The following lemma provides a lower bound for t⋆, indicating the duration for which the approximation
in Lemma D.1 remains valid.
Lemma D.2. Consider tensors M:=A∗A(X ∗X⊤)∈Rn×n×k and U(cid:101)t :=(I +µM)∗t∗U 0. Let
M∈Cnk×nk bethecorrespondingblockdiagonalformofthetensorMwiththeleadingeigenvectorv ∈Cnk,
1
then
 (cid:18) (cid:19)

ln
√∥M√∥·∥U0Hv1∥ℓ2 

t⋆ ≥  8(1+δ1 k) kmin{n,R}α3∥U∥3   (D.7)
 2ln(1+µ∥M∥) 
Proof. Let U(cid:101)t ∈ Cnk×Rk be the corresponding block diagonal form of tensor U(cid:101)t. By the definition of the
(cid:13) H (cid:13)
spectral tensor norm, we have ∥U(cid:101)t∥=∥U(cid:101)t∥ and the definition of the matrix norm gives ∥U(cid:101)t∥≥(cid:13)U(cid:101)t v 1(cid:13) ℓ2.
For the block diagonal version of U(cid:101)t, the following properties (see, e.g., [26]) holds
t
U(cid:101)t =(I +µM)∗t∗U
0
=(I +µM)∗t·U
0
=(I +µM) ·U 0. (D.8)
This allows us to proceed as follows
U(cid:101)tHv
1
=(cid:0) (I +µM)t ·U 0(cid:1)H v
1
=U 0H(I +µM)tH v
1
=(1+µ∥M∥)tU 0Hv 1,
where for the last equality we used the fact that block-diagonal matrix (I +µM) has the same set of
eigenvectorsasmatrixM.
Fromhere,weget∥U(cid:101)t∥≥(cid:13) (cid:13)U(cid:101)tH
v
1(cid:13)
(cid:13)
ℓ2
=(1+µ∥M∥)t(cid:13)
(cid:13)U
0H
v
1(cid:13)
(cid:13) ℓ2. Then,applying
Lemma D.1, the relative error in the spectral norm between U(cid:101)t and U
t
can be estimated as
∥U(cid:101) ∥t U(cid:101)− t∥U t∥ ≤8(1+δ 1√ k) ∥(cid:112) Mkm ∥·in ∥U{n 0, HR
v
1}
(cid:13)
(cid:13)α ℓ3 2∥U∥3(1+µ∥M∥(cid:13) (cid:13))2t.
15Setting the bound above to be smaller than 1 and solving for t, we get
(cid:32) (cid:13) (cid:33)
ln
√∥M √∥·∥U0Hv1(cid:13)
ℓ2
8(1+δ1 k) kmin{n,R}α3∥U∥3
t< .
2ln(1+µ∥M∥)
Since t ∈ N with t ≤ t⋆ should be such that ∥U(cid:101)t−1−Ut−1∥ < 1, we can choose t⋆ as the floor-value of the
∥U(cid:101)t−1∥
right-hand side above.
To show that the tensor column subspaces of the tensor power method iterates and the gradient descent
iterates are aligned after the alignment phase, we use the largest principal angle between two tensor-column
subspaces as the potential function for analysis. Borrowing the idea from [11], we will show that the power
methoditerationinthetensordomaincanbetransformedtotheclassicalsubspaceiterationinthefrequency
domain.
For this, consider the power method iterates U(cid:101)t =(I+µM)∗t∗U 0, the iterates Z
t
=(I+µM)∗t and
the gradient descent iterates U
t
represented as U
t
= U(cid:101)t+E
t
= Z t∗U 0+E t. All these tensors have their
counterparts in the Fourier domain, which we will denote respectively as U(cid:101)t, Z
t
and U t.
Asbefore,considerM=A∗A(X ∗X⊤)∈Rn×n×k withitst-SVDM=V ∗Σ ∗W⊤ anditsFourier
M M M
domain representative M∈Cnk×nk. We denote by L ∈ Rn×r×k the tensor column subspace spanned by
the tensor columns corresponding to the first r singular tubes, that is L := V (:,1 : r,:) ∈ Rn×r×k. Note
M
that L is also the subspace spanned by the tensor columns corresponding to the first r singular tubes of the
tensor Z ∈Rn×n×k.
t
ByL ∈Rn×n×k wewilldonatethetensor-columnsubspacespannedbythetensorcolumnscorresponding
t
to the first r singular tubes of the gradient descent iterates U = Z ∗ U + E . More concretely, for
t t 0 t
U =(cid:80)R V (:,s,:)∗Σ (s,s,:)∗W⊤ (:,s,:)andthecorrespondingFourierdomainrepresentationU =
t s=1 Ut Ut Ut t
diag(U (1),U (2),...,U (k)), where U (j) =(cid:80) σ(j)v(j)w(j)H =U(j)Σ(j)W(j)H , we define the corresponding
t t t t ℓ ℓ ℓ ℓ Ut Ut Ut
new tensors L :=V (:,1:r,:)∈Rn×r×k and their Fourier domain representations
t Ut
L =diag(L (1),L (2),...,L (k)) (D.9)
t t t t
Lemma D.3. Consider the tensor iterates Z =(I +µM)∗t with its block-matrix representation
t
Z =bdiag(Z )=diag(Z (1),Z (2),...,Z (k)). (D.10)
t t t t t
and the tensors
E
t
=U t−U(cid:101)t ∈Rn×R×k
U =αU ∈Rn×R×k, α>0.
0
Assume that for each 1≤j ≤k, it holds that
∥E ∥
σ (Z (j))∥U∥+ t <σ (Z (j))σ (V⊤∗U). (D.11)
r+1 t α r t min L
Then for each 1≤j ≤k, the following two inequalities hold
σ (cid:0) U (j)(cid:1) =σ (cid:0) Z (j)U (j)+E (j)(cid:1) ≥ασ (Z (j))σ (V⊤∗U)−∥E ∥, (D.12)
r t r t 0 t r t min L t
σ (cid:0) U (j)(cid:1) =σ (cid:0) Z (j)U (j)+E (j)(cid:1) ≤ασ (Z (j))∥U∥+∥E ∥ (D.13)
r+1 t r+1 t 0 t r+1 t t
Moreover, the principal angle between the tensor-column subspaces L and L is bounded as follows
t
ασ (Z (j))∥U∥+∥E ∥
∥V⊤ ∗V ∥≤ max r+1 t t (D.14)
L⊥ Lt 1≤j≤k σ (Z (j))σ (cid:0) V⊤∗U(cid:1) −ασ (cid:0) Z (j))∥U∥−∥E ∥
r t min L r+1 t t
16Proof. For some t∈N, consider tensor Z =(I +µM)∗t with its block-matrix representation
t
 Z (1) 
t
 Z t(2) 
Z t =bdiag(Z t)=diag(Z t(1),Z t(2),...,Z t(k))= 

...   .
Z (k)
t
As we assume the symmetric tensor case scenario, the block-diagonal matrix representation Z consists
t
of symmetric matrices Z (j) ∈Cn×n. At the same time, according to [11], the gradient descent tensors
t
U =Z ∗U +E have their block-diagonal matrix representation
t t 0 t
 Z (1)U (1)   E (1) 
t 0 t
 Z t(2)U 0(2)   E t(2) 
U t =Z t∗U 0+E t ⇔ Z tU 0+E t = 

...   + 

...   .
Z (k)U (k) E (k)
t 0 t
(D.15)
Using Weyl’s inequality in each block, we have
σ (cid:0) Z (j)U (j)+E (j)(cid:1) ≥σ (cid:0) Z (j)U (j)(cid:1) −∥E (j)∥≥σ (cid:16) (V (j))HZ (j)U (j)(cid:17) −∥E (j)∥.
r t 0 t r t 0 t r L t 0 t
Now, for the singular value above we get the following estimation
σ (cid:16) (V (j))HZ (j)U (j)(cid:17) =σ (cid:16) V (j)H Z (j)V(j)V(j)H U (j)(cid:17)
r L t 0 min L t L L 0
≥σ (cid:16) V (j)H Z (j)V (j)(cid:17) σ (cid:16) V (j)H U (j)(cid:17)
min L t L min L 0
=σ (Z (j))σ (cid:16) V (j)H U (j)(cid:17) ≥ασ (Z (j))σ (cid:16) V (j)H U(j)(cid:17)
r t min L 0 r t min L
(cid:16) (j) (cid:17) (cid:16) (cid:17)
=ασ (Z (j))σ VH U(j) ≥ασ (Z (j))σ V⊤∗U
r t min L r t min L
where in the last line we used that for each tensor it holds in the Fourier domain V (j)H =VT(j).
L L
To show inequality (D.13), we can use Weyl’s bounds and then the Courant-Fisher theorem, which leads
to
σ (cid:0) Z (j)U (j)+E (j)(cid:1) ≤σ (cid:0) Z (j)U (j)(cid:1) +∥E (j)∥≤σ (cid:0) Z (j)U (j)(cid:1) +∥E ∥
r+1 t 0 t r+1 t 0 t r+1 t 0 t
≤σ (cid:0) Z (j)(cid:1) ∥U (j)∥+∥E ∥≤ασ (cid:0) Z (j)(cid:1) ∥U∥+∥E ∥.
r+1 t 0 t r+1 t t
Now, for estimation of ∥V⊥ ∗V ∥, let us recall that L is the tensor column subspace spanned by the
L Lt
tensor columns corresponding to the first r singular tubes of tensor Z = (I −µM)∗t ∈ Rn×n×k, and L
t t
is the tensor-column subspace spanned by the tensor-columns corresponding to the first r singular tubes of
the gradient descent iterates U = Z ∗U +E , and consider Fourier-domain representation (D.15) of U .
t t 0 t t
Here, for each 1≤j ≤k, the matrices Z (j)U (j)+E (j) can be represented as
t 0 t
Z (j)U (j)+E (j) =Z (j)V (j)V (j)H U (j)+Z (j)V (j)V (j)H U (j)+E (j). (D.16)
t 0 t t L L 0 t L⊥ L⊥ 0 t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
A(cid:101)(j) A(j) C(j)
As the tensor-column space V is r-dimensional, each of matrices V (j) has rank r, see [11]. Since the
L L
matrices Z (j) can be decomposed as
t
Z (j) =V (j)Σ(j)V (j)H +V (j)Σ(j)V (j)H
t L L L L⊥ L⊥ L⊥
we have that
Z (j)V (j)V (j)H U (j) =V (j)Σ(j)V (j)H U (j). (D.17)
t L L 0 L L L 0
17As U (j) ∈ Cr×R has rank r, V (j)H U (j) has rank r, which means that the product above has rank r too.
0 L 0
Due to (D.17), we see that
Z (j)V (j)V (j)H U (j) =V (j)V (j)H Z (j)V (j)V (j)H U (j),
t L L 0 L L t L L 0
whichmakesV (j)tothecolumnsubspaceofZ (j)V (j)V (j)H U (j). Consideringthegapbetweenthesingular
L t L L 0
values of for matrices A(j) and A(cid:101)(j) in (D.16), namely δ(j) =σ r(A(j))−σ r+1(A(cid:101)(j)), and using Wedin’s sinθ
theorem [41], for each 1≤j ≤k we get
∥V (j)H V (j)∥≤
∥C(j)∥
.
L⊥ Lt δ(j)
To conduct a further estimation of ∥V (j)H V (j)∥, we analyze lower and upper bounds for the denomi-
L⊥ Lt
nator and the numerator above. We start with the denominator first
δ(j) =σ r(A(j))−σ r+1(A(cid:101)(j))
=σ (Z (j)V (j)V (j)H U (j))−σ (Z (j)U (j)+E (j)).
r t L L 0 r+1 t 0 t
Using properties of singular values of the matrix product for the first term above and Weyl’s bound for the
second term, we get
δ(j) ≥σ (Z (j))σ (cid:16) V (j)H U (j)(cid:17) −σ (cid:16) Z (j)U (j)(cid:17) −∥E (j))∥
r t min L 0 r+1 t 0 t
(cid:16) (cid:17) (cid:16) (cid:17)
≥σ (Z (j))σ V⊤∗U −σ Z (j)U (j) −∥E ∥. (D.18)
r t min L 0 r+1 t 0 t
For the norm of C(j), the following upper bound can be established
∥C(j)∥≤∥Z (j)V (j)V (j)H U (j)∥+∥E (j)∥
t L⊥ L⊥ 0 t
≤∥Z (j)V (j)V (j)H ∥∥U (j)∥+∥E ∥
t L⊥ L⊥ 0 t
≤ασ (Z (j))∥U∥+∥E ∥ (D.19)
r+1 t t
Now, combining bounds (D.18) and (D.19), one obtains that
∥V⊤ ∗V ∥= max ∥V (j)H V (j)∥≤ max ασ r+1(Z t(j))∥U∥+∥E t∥ :
L⊥ Lt 1≤j≤k L⊥ Lt 1≤j≤k σ (Z (j))σ (cid:0) V⊤∗U(cid:1) −σ (cid:0) Z (j)U(j)(cid:1) −∥E ∥
r t min L r+1 t t
Using in the denominator the fact that σ (cid:0) Z (j)U (j)(cid:1) ≤ ασ (cid:0) Z (j)(cid:1) ∥U(j)∥ ≤ ασ (cid:0) Z (j))∥U∥ finishes
r+1 t 0 r+1 t r+1 t
the proof of this lemma.
Further, we consider the gradient descent iterates with its t-SVD
R
(cid:88)
U = V (:,s,:)∗Σ (s,s,:)∗W⊤ (:,s,:)
t Ut Ut Ut
s=1
and the corresponding Fourier domain representation U = diag(U (1),U (2),...,U (k)), where U (j) =
t t t t t
(cid:80)R σ(j)v(j)w(j)H =V(j)Σ(j)W(j)H and its signal-noise term decomposition
ℓ=1 ℓ ℓ ℓ Ut Ut Ut
U =U ∗W ∗W⊤+U ∗W ∗W⊤
t t t t t t,⊥ t,⊥
We also define the corresponding new tensors
r
(cid:88)
L = V (:,s,:)∗Σ (s,s,:)∗W⊤ (:,s,:) (D.20)
t Ut Ut Lt
s=1
R
(cid:88)
N = V (:,s,:)∗Σ (s,s,:)∗W⊤ (:,s,:) (D.21)
t Ut Ut Ut
s=r+1
18and their Fourier domain representations
r
L =diag(L (1),L (2),...,L (k)), L (j) =(cid:88) σ(j)v(j)w(j)H =V(j)Σ(j)W(j)H (D.22)
t t t t t ℓ ℓ ℓ Lt Lt Lt
ℓ=1
R
N =diag(N (1),N (2),...,N (k)), N (j) = (cid:88) σ(j)v(j)w(j)H =V(j)Σ(j)W(j)H (D.23)
t t t t t ℓ ℓ ℓ Nt Nt Nt
ℓ=r+1
Lemma D.4. Assume ∥V⊤ ∗V ∥≤ 1. Then it holds that
X⊥ Lt 2
(cid:16) (cid:17)
σ U (j)
r+1 t
∥W⊤ ∗W ∥≤2 max ∥V⊤ ∗V ∥. (D.24)
L⊥
t
t
1≤j≤k σ
(cid:16)
U
(j)(cid:17) X⊥ Lt
r t
Proof. Consider ∥WT ∗W ∥ = max ∥W (j)H W (j)∥. For each 1 ≤ j ≤ k, we can now exploit the
L⊥
t
t 1≤j≤k L⊥
t
t
results in [36, Lemma A.1], to get that
∥(W⊤ )(j)W (j)∥≤
∥Σ( Nj) t∥∥V NH t(j)V X(j)∥
and σ (V (j)U (j))≥
σ min(L t(j))
.
L⊥ t (cid:16) (cid:17) min X t 2
t σ V (j)U (j)
min X t
From here, we have we can proceed as follows
∥W⊤ ∗W ∥= max ∥WH
(j)
W (j)∥≤2 max
∥Σ( Nj) t∥∥V NH t(j)V X(j)∥
L⊥ t t 1≤j≤k L⊥ t t 1≤j≤k σ min(L t(j))
=2 max
σ r+1(U t(j))∥V NH t(j)V X(j)∥
≤2 max
σ r+1(U t(j))
∥V⊤ ∗V ∥
1≤j≤k σ r(U t(j)) 1≤j≤k σ r(U t(j)) L⊥ t X
σ
(cid:0)
U
(j)(cid:1)
=2 max r+1 t ∥V⊤ ∗V ∥,
1≤j≤k σ
r(cid:0)
U
t(j)(cid:1) X⊥ Lt
which concludes the proof.
Lemma D.5. Assume that ∥V⊤ ∗V ∥≤ 1 for some t≥1,t∈N. Then for each 1≤j ≤k, it holds that
X⊥ Lt 8
(cid:16) (cid:17) 1 (cid:16) (cid:17)
σ U ∗W (j) ≥ σ U (j) (D.25)
r t t 2 r t
σ (U ∗W (j))≤2σ (U (j)). (D.26)
1 t t,⊥ r+1 t
Moreover,theprincipalanglesbetweenthetensor-columnsubspacesspannedbyX andU W canbeestimated
t t
as follows
∥V ∗V ∥≤7∥V⊤ ∗V ∥ (D.27)
X⊥ UtWt X⊥ Lt
∥U ∗W ∥≤2 max σ (U (j)). (D.28)
t t,⊥ r+1 t
1≤j≤k
Proof. We assume that ∥V⊤ ∗V ∥≤ 1, then due to Lemma D.4, we obtain that
X⊥ Lt 8
(cid:16) (cid:17)
σ U (j)
r+1 j 1
∥W⊤ ∗W ∥≤2 max ∥V⊤ ∗V ∥≤ . (D.29)
L⊥ t t 1≤j≤k σ (cid:16) U (j)(cid:17) X⊥ Lt 4
r j
(cid:16) (cid:17)
Now, to estimate σ U ∗W (j) , we see that for each 1≤j ≤k, it holds that
r t t
σ (cid:16) U ∗W (j)(cid:17)2 =σ (cid:16)(cid:0) U ∗W (j)(cid:1)H U ∗W (j)(cid:17) =σ (cid:16) W (j)H U (j)H U (j)W (j)(cid:17) (D.30)
r t t r t t t t r t t t t
19Since U (j)H U (j) =L (j)H L (j)+N (j)H N (j), we get that
t t t t t t
σ (cid:16) U ∗W (j)(cid:17)2 ≥σ (cid:16) W (j)H L (j)H L (j)W (j)(cid:17) =σ (cid:0) W (j)H L (j)(cid:1)2
r t t r t t t t r t t
≥σ
(cid:0)
W
(j)H
W
(cid:1)2
σ
(cid:0)
L
(j)(cid:1)2
≥(1−∥W
∗WT∥2(cid:1)
σ
(cid:0)
U
(j)(cid:1)2
,
r t Lt(j) r t L⊥
t
t r t
where in the last line we used the definition of the principal angle between tensor column subspaces and the
corresponding properties in their Fourier domain slices, namely
σ (cid:0) W (j)H W (cid:1)2 =1−∥W (j)H W⊥ ∥2 ≥1− max ∥W (j)H W⊥ ∥2 =1−∥W ∗WT∥2.
r t Lt(j) t Lt(j) 1≤j≤k t Lt(j) L⊥ t t
Due to our assumption ∥V⊤ ∗V ∥≤ 1, we can see that in the Fourier domain, the subspaces spanned by
X⊥ Lt 8
V(j) and V(j) =V are close enough. Then, decomposing U (j) into two different ways, namely as
X⊥
t
Lt Lt(j) t
R
U (j)
=(cid:88) σ(j)v(j)w(j)H
=L (j)+N (j)
t ℓ ℓ ℓ t t
ℓ=1
and as
U (j) =U (j)W (j)W (j)H +U (j)W (j)W (j)H ,
t t t t t t,⊥ t,⊥
according to Lemma H.1, one obtains for each 1≤j ≤k that
∥V(j) H V ∥≤7∥V(j) H V(j)∥
X⊥
t
Ut(j)Wt(j) X⊥
t
Lt
∥U (j)W (j)∥≤2σ (U (j)),
t t,⊥ r+1 t
where the last inequality is equivalent to σ (U ∗W (j)) ≤ 2σ (U (j)). According to the definition of
1 t t,⊥ r+1 t
principal angles between tensor subspaces, this implies that
∥V⊤ ∗V ∥=max∥V(j) H V ∥≤7max∥V(j) H V(j)∥=7∥V⊤ ∗V ∥.
X⊥ Ut∗Wt
j
X⊥
t
Ut(j)Wt(j)
j
X⊥
t
Lt X⊥ Lt
In the same way, ∥U ∗W ∥=max ∥U (j)W (j)∥≤2max σ (U (j)), which finishes the proof.
t t,⊥ j t t,⊥ j r+1 t
Lemma D.6. Consider a tensor T :=X ∗X⊤ ∈Sn×n×k with tubal rank r ≤n. Assume that measurement
+
operator A is such that
M=A∗A(T)=T +E ∈Sn×n×k
+
and for for each 1 ≤ j ≤ k one has ∥E(j)∥ ≤ δλ (T(j)) with δ ≤ 1. For the same M with its t-SVD
r 4
M=V ∗Σ ∗W⊤ , let L∈Rn×r×k denote the tensor column subspace spanned by the tensor-columns
M M M
corresponding to the first r singular tubes, that is L:=V (:,1:r,:)∈Rn×r×k.
M
Then, in each Fourier slice j, 1≤j ≤k, it holds that
(1−δ)λ (T(j))≤λ (M(j))≤(1+δ)λ (T(j)) (D.31)
1 1 1
λ (M(j))≤δλ (T(j)) (D.32)
r+1 r
λ (M(j))≥(1−δ)λ (T(j)), (D.33)
r r
and
(1−δ)∥T∥≤∥M∥≤(1+δ)∥T∥ (D.34)
Moreover, the tensor-column subspaces of X and L are aligned, namely
∥V⊤ ∗V ∥≤2δ (D.35)
X⊥ L
20Proof. Consider tensor T := X ∗X⊤ ∈ Sn×n×k. Due to the definition of tensor transpose and conjugate
+
symmetry of Fourier coefficients [17], the Fourier slices of T are defined as T(j) =X(j)X(j)H . That is, each
face of T is Hermitian and at least positive semidefinite. As we assume that for each j, 1 ≤ j ≤ k, one has
∥E (j)∥≤δλ (T(j))usingWeyl’sinequalityineachoftheFourierslices,weobtainthefirstthreeinequalities.
t r
To show that the tensor subspace V and V are aligned, we use first use the definition
X L
H
∥V⊤ ∗V ∥= max ∥V(j) V(j)∥ (D.36)
X⊥ L
1≤j≤k
X⊥ L
For the estimation of ∥VH (j) V(j)∥ in each of the Fourier slices, we apply Wedin’s sinΘ theorem. For this,
X⊥ L
denote L := V (:,1 : r,:) ∈ Rn×r×k and let V(j) denote the corresponding Fourier slices of L ∈ Rn×r×k.
M L
Since in the Fourier space, it holds that M(j) =T(j)+E(j) and V(j) encompasses the first r eigenvectors of
L
M(j), from Wedin’s sinΘ theorem, we obtain
H ∥E(j)∥
∥V(j) V(j)∥≤ ,
X⊥ L ξ(j)
with ξ(j) :=λ (T(j))−λ (M(j)). Using estimate (D.32), ξ(j) can be lower-bounded as
r r+1
ξ(j) :=λ (T(j))−λ (M(j))≥λ (T(j))−δλ (T(j))=(1−δ)λ (T(j)).
r r+1 r r r
Using the bound the the assumptions that ∥E (j)∥≤δλ (T(j)) and δ ≤ 1, we get
t r 2
H δ
∥V(j) V(j)∥≤ ≤2δ.
X⊥ L 1−δ
Coming back to equality (D.36), we obtain the stated bound for the principal angle between the two tensor
column subspaces.
Lemma D.7. Consider a tensor X ∗X⊤ ∈ Sn×n×k with tubal rank r ≤ n. Assume that measurement
+
operator A is such that
M=A∗A(X ∗X⊤)=X ∗X⊤+E
and for each, j, 1 ≤ j ≤ k, one has ∥E(j)∥ ≤ δλ (X(j)X(j)H ) with δ ≤ c . Moreover, assume that for
r 1
difference tensor E
t
=U t−U(cid:101)t it holds that
α max σ (Z (j))∥U∥+∥E ∥
γ := 1≤j≤k r+1 t t 1 ≤c κ−2, (D.37)
1m ≤ji ≤n kσ r(Z t(j)) ασ min(V⊤ L∗U) 2
where c ,c >0 are sufficiently small absolute constants. Then for the signal and noise term of the gradient
1 2
descent (C.1), we have
∥V⊤ ∗V ∥≤14(δ+γ) (D.38)
X⊥ Ut∗Wt
κ−2
∥U ∗W ∥≤ α min σ (Z (j))σ (V⊤∗U) (D.39)
t t,⊥ 8 1≤j≤k r t min L
and for each j, 1≤j ≤k, it holds that
1
σ (U ∗W (j))≥ α min σ (Z (j))σ (V⊤∗U) (D.40)
min t t 4 1≤j≤k r t min L
κ−2
σ (U ∗W (j))≤ α min σ (Z (j))σ (V⊤∗U) (D.41)
1 t t,⊥ 8 1≤j≤k r t min L
21Proof. To prove the above-stated properties, we will use Lemma D.3. Therefore, we start by checking the
conditions of this lemma. Sufficiently small c and the assumption γ ≤c κ−2 allows for γ ≤ 1. This means
2 2 2
that
α max σ (Z (j))∥U∥+∥E ∥
1≤j≤k r+1 t t 1 1
≤
1m ≤ji ≤n kσ r(Z t(j)) ασ min(V⊤ L∗U) 2
and in each of the Fourier slices we have
∥E ∥ 1
σ (Z (j))∥U∥+ t ≤ σ (Z (j))σ (V⊤∗U),
r+1 t α 2 r t min L
fulfilling the assumption of Lemma D.3. Hence, from Lemma D.3, we conclude that
ασ (Z (j))∥U∥+∥E ∥
∥V⊤ ∗V ∥≤ max r+1 t t (D.42)
L⊥ Lt 1≤j≤k ασ (Z (j))σ (cid:0) V⊤∗U(cid:1) −ασ (cid:0) Z (j))∥U∥−∥E ∥
r t min L r+1 t t
α max σ (Z (j))∥U∥+∥E ∥
r+1 t t
1≤j≤k
≤ , (D.43)
α min σ (Z (j))σ (cid:0) V⊤∗U(cid:1) −α max σ (cid:0) Z (j))∥U∥−∥E ∥
r t min L r+1 t t
1≤j≤k 1≤jlek
and, moreover, together with Lemma D.5 and the assumption γ ≤ 1 we get
2
α
min σ (U (j))≥α min σ (Z (j))σ (V⊤∗U)−∥E ∥≥ min σ (Z (j))σ (V⊤∗U) (D.44)
1≤j≤k r t 1≤j≤k r t min L t 2 1≤j≤k r t min L
max σ (U (j))≤α min σ σ (Z (j))∥U∥+∥E ∥≤αγ min σ (Z (j))σ (V⊤∗U) (D.45)
r+1 t r r t t r t min L
1≤j≤k 1≤j≤k 1≤j≤k
The last two inequalities, allow extend bound (D.42) as follows
γ
∥V⊤ ∗V ∥≤ (D.46)
L⊥ Lt 1−γ
Now, consider the principal angle between X and L using its definition
t
∥V⊤ ∗V ∥= max ∥V(j) H V(j)∥= max ∥V(j) V(j)H−V(j)V(j)H∥
X⊥ Lt
1≤j≤k
X⊥ Lt
1≤j≤k
X⊥ X⊥ Lt Lt
≤ max ∥V(j) V(j)H−V(j)V(j)H∥≤ max ∥V(j) V(j)H−V(j)V(j)H∥+∥V(j)V(j)H−V(j)V(j)H∥
1≤j≤k
X⊥ X⊥ Lt Lt
1≤j≤k
X⊥ X⊥ L L L L Lt Lt
≤ max ∥V(j) V(j)H−V(j)V(j)H∥+ max ∥V(j)V(j)H−V(j)V(j)H∥
1≤j≤k
X⊥ X⊥ L L
1≤j≤k
L L Lt Lt
=∥V⊤ ∗V ∥+∥V⊤ ∗V ∥
X⊥ L L⊥ Lt
Using the last line above, and inequalities (D.35) and (D.46), we obtain
∥V⊤ ∗V ∥≤2(δ+γ).
X⊥ Lt
From here, allowing δ and γ to be such that ∥V⊤ ∗V ∥≤ 1, we can use Lemma D.5 to get
X⊥ Lt 8
∥V ∗V ∥≤7∥V⊤ ∗V ∥≤14(δ+γ).
X⊥ UtWt X⊥ Lt
Furthermore, Lemma D.5 together with inequality (D.45) also results in
σ (U ∗W (j))≤2σ (U (j))
1 t t,⊥ r+1 t
≤2 max σ (U (j))
r+1 t
1≤j≤k
≤2γα min σ (Z (j))σ (V⊤∗U)
r t min L
1≤j≤k
κ−2
≤ α min σ (Z (j))σ (V⊤∗U)
8 1≤j≤k r t min L
22and for the spectral norm of U ∗W we get
t t,⊥
κ−2
∥U ∗W ∥≤2 max σ (U (j))≤ α min σ (Z (j))σ (V⊤∗U).
t t,⊥ 1≤j≤k r+1 t 8 1≤j≤k r t min L
Toconcludetheproof,weseethatLemmaD.5togetherwithinequality(D.44)providesforeachj,1≤j ≤k,
the following lower bound
(cid:16) (cid:17) 1 (cid:16) (cid:17) α α
σ U ∗W (j) ≥ σ U (j) ≥ σ (Z (j))σ (V⊤∗U)≥ min σ (Z (j))σ (V⊤∗U).
r t t 2 r t 4 r t min L 4 1≤j≤k r t min L
The following lemma shows that for an appropriately chosen initialization, in the first new iteration, the
tensor column subspaces between the signal term U ∗W and the ground truth tensor X become aligned.
t t
Moreover, for each 1≤j ≤k there is a solid gap between the smallest singular values of the signal term and
the largest singular values of the noise term.
Lemma D.8. Assume A:Sn×n×k →Rm satisfies the S2NRIP(δ ) for some constant δ >0. Also, assume
1 1
that
M:=A∗A(X ∗X⊤)=X ∗X⊤+E
with ∥E(j)∥≤δλ (X(j)X(j)H ) for each 1≤j ≤k and δ ≤c κ−2.
r 1
Denote by L the tensor-columns corresponding to the first r singular tubes in the t-SVD of M, that is,
L:=V (:,1:r,:)∈Rn×r×k, and define the initialization U =αU with the coefficient α such that
M 0
(cid:32) (cid:33)−48κ2
c∥X∥2 2κ2∥U∥3 H
α2 ≤ min{σ (V⊤∗U),∥U v ∥ } (D.47)
12k(cid:112) min{n,R}κ2∥U∥3 c σ (V⊤∗U) min L 0 1 ℓ2
3 min L
where v ∈Cnk is the leading eigenvector of matrix M∈Cnk×nk.
1
Assume that learning rate µ fulfils µ≤c κ−2∥X∥−2, then after t iterations with
3 ⋆
(cid:32) (cid:33)
1 2κ2∥U∥
t ≍ ln (D.48)
⋆
µmin 1≤j≤kσ r(X(j))2 c 3σ min(V⊤ L∗U)
it holds that
∥U ∥≤3∥X∥ (D.49)
t⋆
∥V ∗V ∥≤cκ−2. (D.50)
X⊥ Ut⋆∗Wt⋆
and for each 1≤j ≤k, we have
(cid:16) (cid:17) 1
σ U ∗W (j) ≥ αβ (D.51)
r t⋆ t⋆ 4
(cid:16) (cid:17) κ−2
σ U ∗W (j) ≤ αβ (D.52)
1 t⋆ t⋆,⊥ 8
(D.53)
(cid:18) (cid:19)16κ2
where β satisfies σ (V⊤∗U)≤β ≤σ (V⊤∗U) 2κ2∥U∥ .
min L min L c3σmin(V⊤ L∗U)
Proof. For the proof of this lemma, we want to apply Lemma D.7. The first condition of Lemma D.7 is the
following
α max σ (Z (j))∥U∥+∥E ∥
γ := 1≤j≤k r+1 t t 1 ≤c κ−2,
1m ≤ji ≤n kσ r(Z t(j)) ασ min(V⊤ L∗U) 2
23By the definition of γ, it is sufficient to show that
c
max σ (Z (j))∥U∥≤ 3 min σ (Z (j))σ (V⊤∗U) (D.54)
1≤j≤k r+1 t 2κ2 1≤j≤k r t min L
and
c
∥E ∥≤ 3 α min σ (Z (j))σ (V⊤∗U). (D.55)
t 2κ2 1≤j≤k r t min L
Since for Z =(I +µM)∗t the transformation in the Fourier domain leads to the blocks
t
Z(j) =(Id+µM(j))t,
t
this means that inequality (D.54) is equivalent to
 1+µ min σ (M(j)) t
2κ2∥U∥
1≤j≤k
r
≤  ,
c 3σ min(V⊤ L∗U) 1+µ 1m ≤ja ≤x kσ r+1(M(j))
which can be further modified as
(cid:32) (cid:33)  1+µ min σ (M(j)) 
2κ2∥U∥
1≤j≤k
r
ln ≤tln .
σ min(V⊤ L∗U) 1+µ 1m ≤ja ≤x kσ r+1(M(j))
Hence, if we take t as follows
⋆
 (cid:32) (cid:33)(cid:44)  1+µ min σ (M(j)) 
2κ2∥U∥
1≤j≤k
r
t ⋆ :=

ln
σ min(V⊤ L∗U)
ln
1+µ 1m ≤ja ≤x kσ
r+1(M(j))


(D.56)
then condition (D.54) will be satisfied in each block in the Fourier domain. For convenience, we will further
denote
(cid:32) (cid:33)
2κ2∥U∥
ψ :=ln . (D.57)
σ (V⊤∗U)
min L
For the second part of Lemma D.7’s condition, inequality (D.55), we will use Lemma D.1. To apply this
Lemma, the condition t ≤t⋆ needs to be satisfied. According to Lemma D.2
⋆
 (cid:18) (cid:19)

ln
√∥M√∥·∥U0Hv1∥ℓ2 

t⋆ ≥  8(1+δ1 k) kmin{n,R}α3∥U∥3   (D.58)
 2ln(1+µ∥M∥) 
For t ≤t⋆ to hold, it will be sufficient to check, e.g., the following condition
⋆
(cid:18) (cid:19)
ln
√∥M√∥·∥U0Hv1∥ℓ2
ψ
≤
1
·
8(1+δ1 k) kmin{n,R}α3∥U∥3
.
(cid:16) (cid:17)
ln
1+µmin1≤j≤kσr(M(j)) 2 2ln(1+µ∥M∥)
1+µmax1≤j≤kσr+1(M(j))
To check this condition let us first analyze the expression
ln(1+µ∥M∥)/ln(cid:16) 1+µmin1≤j≤kσr(M(j)) (cid:17)
first.
1+µmax1≤j≤kσr+1(M(j))
Using x ≤ln(1+x)≤x, we can upper bound the above expression as
1+x
ln(1+µ∥M∥) ∥M∥(1+µmin σ (M(j)))
≤ 1≤j≤k r (D.59)
(cid:16) (cid:17)
ln
1+µmin1≤j≤kσr(M(j)) min 1≤j≤kσ r(M(j))−max 1≤j≤kσ r+1(M(j))
1+µmax1≤j≤kσr+1(M(j))
24From here, applying the PSD of the tensor representatives in the Fourier domain and the assumptions
δ ≤ 1 and µ≤c κ−2∥X∥−2 and Lemma D.6, we get
3 3
∥M∥(1+min σ (M(j))) (1+δ)∥T∥ (cid:32) (cid:18) λ (X(j))(cid:19)2(cid:33)
1≤j≤k r ≤ 1+c (1+δ) 1
min σ (M(j))−max σ (M(j)) (1−2δ)λ (T(j)) 3 κ∥X∥
1≤j≤k r 1≤j≤k r+1 r
(1+δ)
≤κ2 (1+c (1+δ))≤8κ2,
(1−2δ) 3
in the last line, we used the bound on δ and that c can be taken small enough. This means
3
ln(1+µ∥M∥)
≤8κ2. (D.60)
(cid:16) (cid:17)
ln
1+µmin1≤j≤kσr(M(j))
1+µmax1≤j≤kσr+1(M(j))
Thus, to show that t ≤t⋆, it is sufficient to tune the initialization factor α so that
⋆
(cid:32) H (cid:33)
∥M∥·∥U v ∥
ψ·32κ2 ≤ln √ 0 1 ℓ2 .
(cid:112)
8(1+δ k) kmin{n,R}α3∥U∥3
1
or using the notation for ϕ, this is equivalent to
(cid:32) 2κ2∥U∥ (cid:33)32κ2 ∥M∥·∥U H v ∥
≤ √ 0 1 ℓ2
(cid:112)
σ min(V⊤ L∗U) 8(1+δ 1 k) kmin{n,R}α3∥U∥3
H H
Since ∥U v ∥ /α=∥U v ∥ , The last inequality is implied if
0 1 ℓ2 1 ℓ2
(cid:32) 2κ2∥U∥ (cid:33)−32κ2 ∥M∥·∥UH v ∥
α2 ≤ √ 1 ℓ2 ,
(cid:112)
σ min(V⊤ L∗U) 8(1+δ 1 k) kmin{n,R}∥U∥3
√ √ √ √
or if we set α even smaller using the fact that (1+δ k) k ≤ (1+ k) k ≤ 2k and ∥M∥ ≥ 2∥X∥2 and
1 3
set the parameter α so that
(cid:32) 2κ2∥U∥ (cid:33)−32κ2 ∥X∥2·∥UH v ∥
α2 ≤ 1 ℓ2 .
(cid:112)
σ (V⊤∗U) 24k min{n,R}∥U∥3
min L
Hence t ≤t⋆ is satisfied and applying Lemma D.7, we get
⋆
√ (cid:112) α3
∥E ∥≤8(1+δ k) kmin{n,R} ∥U∥3(1+µ∥M∥)3t⋆ (D.61)
t⋆ 1 ∥M∥
√ √
Moreover, using ∥M∥≥ 2∥X∥2 from Lemma D.6 with δ ≤1/3 and (1+δ k) k ≤2k , we get
3 1
(cid:112) α3
∥E ∥≤12k min{n,R} ∥U∥3(1+µ∥M∥)3t⋆
t⋆ ∥X∥2
Hence, using that Z (j) =(Id+µM(j))t inequality (D.55) will be implied if
t
12k(cid:112) min{n,R} α3 ∥U∥3(1+µ∥M∥)3t⋆ ≤ c 3 α min σ (cid:16) (Id+µM(j))t⋆(cid:17) σ (V⊤∗U),
∥X∥2 2κ2 1≤j≤k r min L
which is equivalent to
α2 ≤c
∥X∥2σ min(V⊤ L∗U) (1+µλ r(M(j)))t⋆
, (D.62)
3 12k(cid:112) min{n,R}κ2∥U∥3 (1+µ∥M∥)3t⋆
25for all j. To proceed further, let us analyze the last factor from above using the definition of t . Note that
⋆
(1+µλ r(M(j)))t⋆ =exp(cid:18)
t
ln(cid:18) 1+µλ r(M(j))(cid:19)(cid:19) ≥exp(cid:0)
−3t
ln(cid:0) (1+µ∥M∥)3(cid:1)(cid:1)
(1+µ∥M∥)3t⋆ ⋆ (1+µ∥M∥)3 ⋆
Now, using the definition of t , that is t
=(cid:108) ψ/ln(cid:16) 1+µmin1≤j≤kσr(M(j) (cid:17)(cid:109)
and inequality (D.60), we get
⋆ ⋆ 1+µmax1≤j≤kσr+1(M(j))
(cid:32) (cid:33)−48κ2
exp(cid:16)
−3t
ln(cid:16) (1+µ∥M∥)3(cid:17)(cid:17) ≥exp(cid:0) −48ψκ2(cid:1)
=
2κ2∥U∥
(D.63)
⋆
c σ (V⊤∗U)
3 min L
Inserting this into inequality (D.62), we get
(cid:32) (cid:33)−48κ2
∥X∥2σ (V⊤∗U) 2κ2∥U∥
α2 ≤c min L . (D.64)
3 (cid:112)
12k min{n,R}κ2∥U∥3 c σ (V⊤∗U)
3 min L
For such α, we have shown that inequality (D.55) holds, and the condition of Lemma D.7 is fulfilled, which
gives us
∥V⊤ ∗V ∥≤14(δ+γ)≤cκ−2, (D.65)
X⊥ Ut∗Wt
wherethelastinequalityfollowsfromourassumptionthatδ ≤c κ−2 andµ≤c κ−2∥X∥−2 andfromsetting
1 3
the constants c and c small enough.
1 3
Moreover, for each 1≤j ≤k, from Lemma D.7 it follows that
1
σ (U ∗W (j))≥ αβ, (D.66)
min t t 4
κ−2
σ (U ∗W (j))≤ αβ. (D.67)
1 t t,⊥ 8
where β :=min σ (Z (j))σ (V⊤∗U).
1≤j≤k r t min L
In the remaining part, we will show that t , β and ∥U ∥ have the properties stated in the lemma.
⋆ t⋆
Let us start with t . Using the same inequalities for ln(1+x) as above and Lemma D.6, one can show
⋆
 1+µ min σ (M(j))  µ min σ (M(j))
ln
1+µ
m1≤ aj x≤k
σ
r (M(j))≥ 1+1 µ≤j m≤k inr
σ (M(j))
−µ 1m ≤ja ≤x kσ r+1(M(j))≥ 2 3µ 1m ≤ji ≤n kσ r(X(j))2
r+1 r
1≤j≤k 1≤j≤k
and at the same time
 1+µ min σ r(M(j))  (cid:18) (cid:19)
ln
1+µ
m1≤ aj x≤k
σ
(M(j))≤ln 1+µ 1m ≤ji ≤n kσ r(M(j)) ≤µ 1m ≤ji ≤n kσ r(M(j))
r+1
1≤j≤k
≤µ(1+δ) min σ (X(j))2 ≤4/3µ min σ (X(j))2
r r
1≤j≤k 1≤j≤k
which shows that, on the one hand,
1 2 1 2
≤ max =
(cid:16) (cid:17)
ln
1+µmin1≤j≤kσr(M(j)) 3µ1≤j≤k σ r(X(j))2 3µmin 1≤j≤kσ r(X(j))2
1+µmax1≤j≤kσr+1(M(j))
and on the other hand
1 3
≥ ,
(cid:16) (cid:17)
ln
1+µmin1≤j≤kσr(M(j)) 4µmin 1≤j≤kσ r(X(j))2
1+µmax1≤j≤kσr+1(M(j))
26which shows the desired properties of t .
⋆
Now,weconsiderβ :=min σ (Z (j))σ (V⊤∗U). BythedefinitionofZ (j)andinequality(D.60),
1≤j≤k r t⋆ min L t
we get
(cid:16)
1+µσ
(M(j))(cid:17)t⋆ =exp(cid:16)
t ln(1+µσ
(M(j)))(cid:17) ≤exp(cid:16)
t
ln(1+µ∥M∥)(cid:17)
r ⋆ r ⋆
  (cid:32) (cid:33)16κ2
ln(1+µ∥M∥) 2κ2∥U∥
≤exp2ψ max
(cid:16)
(cid:17)≤exp(16ψκ2)= .
1≤j≤k ln 1+µσr(M(j)) c σ (V⊤∗U)
1+µσr+1(M(j)) 3 min L
(D.68)
Since this holds for all j, we have
(cid:32) (cid:33)16κ2
2κ2∥U∥
β ≤σ (V⊤∗U) .
min L
c σ (V⊤∗U)
3 min L
Finally, we come to the properties of U . By the representation U =Z ∗U +E , we get
t⋆ t⋆ t⋆ 0 t⋆
∥U ∥≤α∥Z ∥∥U∥+∥E ∥.
t⋆ t⋆ t⋆
From (D.55), we get
c c
∥E ∥≤ 3 α∥Z ∥σ (V HU)≤ 3 α∥Z ∥σ (V H)σ (U)≤α∥Z ∥∥U∥,
t 2κ2 t min L 2κ2 t min L max t
which allows us to proceed as follows
∥U t⋆∥≤2α∥Z t⋆∥∥U∥≤2α(1+µ∥M∥)t⋆∥U∥,
(cid:32) (cid:33)16κ2
(cid:16) (cid:17) 2κ2∥U∥
=2αln t (1+µ∥M∥) ∥U∥≤2α∥U∥
⋆
c σ (V⊤∗U)
3 min L
(cid:118)
(cid:117) (cid:32) (cid:33)−8κ2
(cid:117) c σ (V⊤∗U) 2κ2∥U∥
≤2∥X∥(cid:116) 3 min L ≤3∥X∥,
(cid:112)
12k min{n,R}κ2∥U∥ c σ (V⊤∗U)
3 min L
where for the second inequality above we used (D.68) and in the last one an upper bound on α from (D.64)
has been applied.
The results in Lemma D.8 hold for any initialization U. Below, we will use the fact that U is a tensor
withGaussianentries. Thisyieldsthefollowinglemma,whichshowswithinitializationscaleα>0bechosen
sufficiently small, the properties stated in Lemma D.8 hold with high probability.
Lemma D.9. Fix a sufficiently small constant c > 0. Let U ∈ Rn×R×k be a random tubal tensor with
i.i.d. N(0, 1) entries, and let ϵ∈(0,1). Assume that A:Sn×n×k →Rm satisfies the S2NRIP(δ ) for some
R 1
constant δ >0. Also, assume that
1
M:=A∗A(X ∗X⊤)=X ∗X⊤+E
with ∥E(j)∥≤δλ (X(j)X(j)H ) for each 1≤j ≤k, where δ ≤c κ−2. Let U =αU where
r 1 0
 ϵmin k{ 2n n, 3R /2} κ∥ 2X∥2 (cid:18)
c
m2 iκ n2 {k nn ,R3/ }2 3/2ϵ(cid:19)−24κ2
if R≥3r
α2 ≲ 3 .
 ϵ∥X∥2 (cid:18) 2κ2kn3/2(cid:19)−24κ2
if R<3r
k2n3/2κ2 c r1/2ϵ
3
27Assume the step size satisfies µ≤c κ−2∥X∥2. Then, with probability at least 1−p where
2
(cid:40)
k(C˜ϵ)R−r+1+ke−c˜R if R≥2r
p=
kϵ2+ke−c˜R if R<2r
the following statement holds. After
 (cid:32) √ (cid:33)
 1
ln
(cid:112)2κ2 n
if R≥3r
t
⋆
≲ µmin 1≤j≤kσ r(X(j))2 c 3ϵ min{n;R}
 1 ln(cid:16) 2κ2√ rn(cid:17) if R<3r
µmin1≤j≤kσr(X(j))2 c3ϵ
iterations, it holds that
∥U ∥≤3∥X∥ (D.69)
t⋆
∥V ∗V ∥≤cκ−2. (D.70)
X⊥ Ut⋆∗Wt⋆
and for each 1≤j ≤k, we have
(cid:16) (cid:17) 1
σ U ∗W (j) ≥ αβ (D.71)
r t⋆ t⋆ 4
(cid:16) (cid:17) κ−2
σ U ∗W (j) ≤ αβ (D.72)
1 t⋆ t⋆,⊥ 8
(D.73)
where
 (cid:32) √ (cid:33)16κ2
ϵ√
k
(cid:112)2κ2 n
if R≥3r
β ≲ c 3ϵ min{n;R}
ϵ√
k
(cid:18) 2κ2√ rn(cid:19)16κ2
if R<3r
r c ϵ
3
and √

ϵ k if R≥3r
 √
β ≳ ϵ k .
 if R<3r
r
(cid:114) (cid:114)
kmax{n,R} kn
Proof. By Lemma I.3, we have that ∥U∥ ≲ = with probability at least 1−
R min{n;R}
√
O(ke−cmax{n,R}). Also, by Lemma I.4, we have that ∥UH v ∥ = ∥U⊤ ∗V ∥ ≍ k with probability at
1 ℓ2 1 F
least1−O(ke−cR). SinceU ∈Rn×R×k hasi.i.d. N(0, 1)entriesandV⊤∗V =I,byrotationalinvariance,
R L L
V⊤∗U ∈ Rr×R×k also has i.i.d. N(0, 1) entries. Hence, the lower bound on σ (V⊤∗U) in Lemma I.2
L R min L
applies. If r ≤R≤2r, we have
√ √
ϵ k ϵ k
σ (V⊤∗U)≥ √ ≳
min L rR r
with probability at least 1−kϵ2. If 2r <R<3r, we have
√ √ √ √ √
ϵ k( R− 2r−1) ϵ k(R−(2r−1)) ϵ k
σ min(V⊤ L∗U)≥ √
R
≥ √ r(√ R+√
2r−1)
≳
r
with probability at least 1−k(Cϵ)R−2r+1−ke−cR. If R≥3r, we have
√ √ √
ϵ k( R− 2r−1) √ (cid:18) (cid:113) (cid:19) √
σ (V⊤∗U)≥ √ =ϵ k 1− 2r−1 ≳ϵ k
min L R R
28with probability at least 1−k(Cϵ)R−2r+1−ke−cR.
Therefore,theaboveboundson∥U∥,∥UH v ∥ ,andσ (V⊤∗U)allholdsimultaneouslywithprobability
1 ℓ2 min L
at least 1−p where
(cid:40)
k(C˜ϵ)R−r+1+ke−c˜R if R≥2r
p= .
kϵ2+ke−c˜R if R<2r
Provided that all three of these bounds hold, one can substitute these into Lemma D.8 to obtain the desired
result.
E Analysis of convergence stage
In this section, we will prove that after passing the spectral stage, U ∗ U⊤ goes into the convergence
t t
process towards the ground truth tensor X ∗X⊤ in the Frobenius norm. For this, we will first show that in
each of the tensor slices σ (V⊤ ∗U (j)) grows exponentially, see Lemma E.1, whereas the noise terms
min X t+1
∥U ∗W (j)∥, 1≤j ≤k, grow slower, see Lemma E.3. Moreover, in Lemma E.5, we show the tensor
t+1 t+1,⊥
column spaces of the signal term U ∗W and the ground truth X stay aligned. With this, and several
t t
auxiliary lemmas in place, we show that
Lemma E.1. Assume that the following conditions hold
µ≤c∥X∥−2κ−2
∥U ∥≤3∥X∥
t
∥V⊤ ∗V ∥≤cκ−1
X⊥ Ut∗Wt
and
∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥≤cσ2 (X). (E.1)
t t min
Moreover, assume that V⊤ ∗U has full tubal rank with all invertible t-SVD-singular tubes. Then, for each
X t
j, 1≤j ≤k, it holds that
(cid:16) 1 (cid:17)
σ (V⊤ ∗U (j))≥σ (V⊤ ∗U ∗W (j))≥σ (V⊤ ∗U (j)) 1+ µσ2 (X)−µσ2 (V⊤ ∗U (j)) .
min X t+1 min X t+1 t min X t 4 min min X t
Proof. ConsiderthetensorV⊤ ∗U ∗W . UsingthedefinitionofU intermsofU , wecanrewriteitas
X t+1 t t+1 t
V⊤ ∗U ∗W =V⊤ ∗(cid:0) I+µA∗A(X ∗X⊤−U ∗U⊤)(cid:1) ∗U ∗W .
X t+1 t X t t t t
This representation leads to the following representation of the RHS above in the Fourier domain
V(j)H(Id+µ(cid:0) A∗A(X ∗X⊤−U ∗U⊤)(cid:1)(j)(cid:1) U(j)W(j) :=H(j).
X t t t t
Notethathere(cid:0) A∗A(X ∗X⊤−U ∗U⊤)(cid:1)(j)cannotberepresentedasanindependentsliceofmeasurements
t t
of X(j)X(j)H−U(j)U(j)H as it involved the information about all the slices 1≤j ≤k.
t t
Due to our assumptions on ∥U ∥ and the tensor spectral norm property, we get
t
∥V(j)HU(j)∥≤∥U(j)∥≤∥U ∥≤3∥X∥.
X t t t
This in turn is leading to
µ≤c∥X∥−2κ−2 ≤c˜∥V(j)HU(j)∥−2.
X t
This property of µ together with the nature of W(j) and V(j) coming along from the signal-noise-term
t X
decomposition (C.1) leads to the fulfilled conditions of Lemma H.2. Applying Lemma H.2 to the matrix
H(j), the smallest singular value of matrix H(j) can be estimated as
σ (H(j))≥(cid:0) 1+µσ2 (X(j))−µ∥P(j)∥−µ∥P(j)∥−µ2∥P(j)∥(cid:1) σ (V(j)HU(j))(cid:0) 1−µσ2 (V(j)HU(j))(cid:1) .
min min 1 2 3 min X t min X t
(E.2)
29with
∥P(j)∥≤4∥U(j)W(j)∥2∥V(j) V ∥2
1 t t X⊥ U(j)W(j)
t t
(cid:13) (cid:13)
∥P(j)∥≤4(cid:13)(cid:0) A∗A(X ∗X⊤−U ∗U⊤)(cid:1)(j)−X(j)X(j)H+U(j)U(j)H(cid:13)
2 (cid:13) t t t t (cid:13)
∥P(j)∥≤2∥X(j)∥2∥U(j)W(j)∥2.
3 t t
Further, we will make the above bounds for ∥P(j)∥,i ∈ {1,2,3}, more precise using information about the
i
tensor setting.
First of all since ∥U(j)W(j)∥ ≤ ∥U(j)∥ ≤ ∥U ∥ ≤ 3∥X∥, we get ∥P(j)∥ ≤ 36∥X∥2∥V(j) V ∥2.
t t t t 1 X⊥ U(j)W(j)
t t
Moreover, since V(j) V = V⊤ ∗V (j) and ∥V⊤ ∗V ∥ ≤ cκ−1 due to the assumption, it
X⊥ U(j)W(j) X Ut∗Wt X⊥ Ut∗Wt
t t
follows that for each j, 1 ≤ j ≤ k, it holds that ∥V(j) V ∥ ≤ cκ−1. This allows for the following
X⊥ U(j)W(j)
t t
estimation
1
∥P(j)∥≤36∥X∥2cκ−1 ≤ σ2 (X),
1 4 min
where the last inequality follows from the fact that c>0 is small enough.
Before proceeding with ∥P(j)∥, consider
2
(A∗A−I)(X ∗X⊤−U ∗U⊤)=(A∗A)(X ∗X⊤−U ∗U⊤)−(cid:0) X ∗X⊤−U ∗U⊤(cid:1) .
t t t t t t
The RHS from above has the following slices in the Fourier domain
(A∗A)(X ∗X⊤−U ∗U⊤)(j)−(cid:0) X(j)X(j)H−U(j)U(j)H(cid:1) ,
t t t t
the norm of which (due to assumption (E.1) and the definition of the tensor spectral norm) can be bounded
as
∥(A∗A)(X ∗X⊤−U ∗U⊤)(j)−(cid:0) X(j)X(j)H−U(j)U(j)H(cid:1) ∥≤∥(A∗A−I)(X∗X⊤−U ∗U⊤)∥≤cσ2 (X).
t t t t t t min
This leads to the following estimation
∥P(j)∥≤4cσ2 (X)
2 min
To further assess ∥P(j)∥, we take into account that matrix W(j) is an orthogonal matrix and the assumption
3 t
∥U ∥≤3∥X∥, which allows for the next bound
t
∥P(j)∥≤2∥X(j)∥2∥U(j)W(j)∥2 ≤2∥X∥2∥U(j)∥2 ≤2∥X∥2∥U ∥2 ≤18∥X∥4.
3 t t t t
Inserting the newly obtained estimates for ∥P(j)∥,i∈{1,2,3}, into (E.2), we get
i
µ
σ (H(j))≥(1+µσ2 (X(j))− σ2 (X)−4µcσ2 (X)−18µ2∥X∥4)·
min min 4 min min
·σ (V(j)HU(j))(cid:0) 1−µσ2 (V(j)HU(j))(cid:1)
min X t min X t
≥(1+µσ2 (X)− µ σ2 (X)−4µcσ2 (X)−18µ2∥X∥4)σ (V(j)HU(j))(cid:0) 1−µσ2 (V(j)HU(j))(cid:1) .
min 4 min min min X t min X t
Now, according to the assumption on µ, we get
σ2 (X)
µ2∥X∥4 ≤µcκ−2∥X∥−2∥X∥4 =µc min ∥X∥−2∥X∥4 =cµσ2 (X)
∥X∥2 min
Taking c small enough allows for the following estimation
σ (H(j))≥σ (V(j)HU(j))(cid:0) 1+ 1 µσ2 (X)(cid:1)(cid:0) 1−µσ2 (V(j)HU(j))(cid:1)
min min X t 2 min min X t
=σ (V(j)HU(j))(cid:16) 1+ 1 µσ2 (X)(cid:0) 1−µσ2 (V(j)HU(j))(cid:1) −µσ2 (V(j)HU(j))(cid:17)
min X t 2 min min X t min X t
30Now, since σ (V(j)HU(j))≤σ (U(j))≤∥U ∥≤3∥X∥, we have that
min X t min t t
1
µσ2 (V(j)HU(j))≤µ9∥X∥2 ≤9cκ−2 ≤
min X t 2
due to the fact that c>0 can be chosen small enough. The last part of Lemma’s proof follows from
σ (V⊤ ∗U (j))≥σ (V⊤ ∗U ∗W (j)) and σ (V⊤ ∗U ∗W (j)) = σ (H(j)), which com-
min X t+1 min X t+1 t min X t+1 t min
pletes the argument.
The next two lemmas will allow us to show that in each of the Fourier slices the noise term part of the
gradient descent iterates is growing slower than its signal term part.
(cid:110) (cid:111)
Lemma E.2. Assume that µ≤cmin 1 ∥X∥−2,∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥−1 and
10 t t
∥U ∥≤3∥X∥. Moreover, suppose that V⊤ ∗ U has full tubal rank with all invertible t-SVD-tubes
t X t
and ∥V⊤ ∗V ∥≤cκ−1 with a sufficiently small contact c > 0. Then, the principal angle between
X⊥ Ut∗Wt
V and V can be bounded as follows
X⊥ Ut+1∗Wt
∥V⊤ ∗V ∥≤2∥V⊤ ∗V ∥+2µ∥(A∗A)(X ∗X⊤−U ∗U⊤)∥.
X⊥ Ut+1∗Wt X⊥ Ut∗Wt t t
In particular, it holds that ∥V⊤ ∗V ∥≤1/50.
X⊥ Ut+1∗Wt
Proof. By the definition of U , we have
t+1
(cid:16) (cid:17)
U ∗W = I+µA∗A(X ∗X⊤−U ∗U⊤) ∗U ∗W ∈Rn×r×k,
t+1 t t t t t
which allows for the following representation in the Fourier domain
(cid:16) (cid:17)
U ∗W (j) = Id+µA∗A(X ∗X⊤−U ∗U⊤)(j) U ∗W (j) ∈Cn×r, 1≤j ≤k.
t+1 t t t t t
ConsidertheSVDdecompositionU ∗W (j) =V Σ WH anddenotebyZ(j) thematrix
t t Ut∗Wt(j) Ut∗Wt(j) Ut∗Wt(j)
(cid:16) (cid:17)
Z(j) := Id+µA∗A(X ∗X⊤−U ∗U⊤)(j) V ∈Cn×r.
t t Ut∗Wt(j)
Since by assumption U ∗W (j) has full rank (due to full-rankness of V⊤ ∗U , see Lemma C.1), matrix
t t X t
Z(j) has the same column space as U ∗W (j) and the principal angle between tensor subspaces V and
t+1 t X⊥
V can be computed via Z(j) as
Ut+1∗Wt
∥V⊤ ∗V ∥= max ∥V(j)HV(j) ∥= max ∥V(j)HV ∥= max ∥V(j)HV ∥.
X⊥ Ut+1∗Wt
1≤j≤k
X⊥ Ut+1∗Wt
1≤j≤k
X⊥ Ut∗Wt(j)
1≤j≤k
X⊥ Z(j)
Now, we will consider each of the terms ∥V(j)HV ∥ separately and bound them as follows
X⊥ Z(j)
∥V(j)HZ(j)∥
∥V(j)HV ∥≤∥V(j)HV Σ WH ∥∥(Σ WH )−1∥= X⊥ . (E.3)
X⊥ Z(j) X⊥ Z(j) Z(j) Z(j) Z(j) Z(j) σ (Z(j))
min
Using the definition of Z(j), the norm in the numerator above can be estimated as
∥V(j)HZ(j)∥≤∥V(j)HV ∥+µ∥V(j)HA∗A(X ∗X⊤−U ∗U⊤)(j)∥
X⊥ X⊥ Ut∗Wt(j) X⊥ t t
≤∥V(j)HV(j) ∥+µ∥A∗A(X ∗X⊤−U ∗U⊤)(j)∥
X⊥ Ut∗Wt t t
≤∥V⊤ ∗V ∥+µ∥A∗A(X ∗X⊤−U ∗U⊤)∥.
X⊥ Ut∗Wt t t
31Using again the definition of Z(j) and Weyl’s inequality, the denominator in (E.3) can be estimated from
below as follows
(cid:16) (cid:17)
σ (Z(j))≥σ (V )−µ∥ A∗A(X ∗X⊤−U ∗U⊤)(j) V ∥
min min Ut∗Wt(j) t t Ut∗Wt(j)
≥1−µ∥A∗A(X ∗X⊤−U ∗U⊤)(j)∥≥1−µ∥A∗A(X ∗X⊤−U ∗U⊤)∥
t t t t
≥1−µ(∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥+∥(X ∗X⊤−U ∗U⊤)∥)
t t t t
(cid:16) (cid:17)
≥1−µ ∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥+∥X∥2+∥U ∥2
t t t
(cid:16) (cid:17) 1
≥1−µ ∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥+10∥X∥2 ≥ ,
t t 2
where the last inequality follows from the assumption on µ. Now, we can come back to the estimation of
∥V⊤ ∗V ∥, which due to the combination of the above-carried estimated reads as
X⊥ Ut+1∗Wt
∥V⊤ ∗V ∥≤2∥V⊤ ∗V ∥+2µ∥A∗A(X ∗X⊤−U ∗U⊤)∥
X⊥ Ut+1∗Wt X⊥ Ut∗Wt t t
providing the first result from the Lemma. The second bound stated in the Lemma follows from our as-
sumption on ∥V⊤ ∗ V ∥ and µ and the fact that the constant c is chosen small enough to make
X⊥ Ut∗Wt
∥V⊤ ∗V ∥≤ 1 .
X⊥ Ut+1∗Wt 50
(cid:110) (cid:111)
Lemma E.3. Assume that µ≤c min 1 ∥X∥−2,∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥−1 and
1 10 t t
∥U ∥≤3∥X∥. Moreover, suppose that tensor V⊤ ∗ U ∗ W has all invertible t-SVD-tubes and
t X t+1 t
that ∥V⊤ ∗V ∥≤c κ−1, with absolute constant c >0 chosen small enough. Then, it holds that
X⊥ Ut∗Wt 1 1
(cid:16) µ
∥U ∗W (j)∥≤ 1− ∥U ∗W (j)∥2+9µ∥V⊤ ∗V (j)∥∥X∥2
t+1 t+1,⊥ 2 t t,⊥ X⊥ Ut∗Wt
(cid:17)
+2µ∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥ ∥U ∗W (j)∥
t t t t,⊥
for each j, with 1≤j ≤k.
Proof. First,wewillconsidertensorU ∗W splittingitintotwodifferentparts,andthenwillconduct
t+1 t+1,⊥
the corresponding norm estimations of each Fourier slices.
To begin with, note that for the tensor-column space of X, that is V , it holds that
X
V ∗V⊤ +V ∗V⊤ =I (see, for example, [26]). Using this, we can represent U ∗W as follows
X X X⊥ X⊥ t+1 t+1,⊥
U ∗W =V ∗V⊤∗U ∗W +V ∗V⊤ ∗U ∗W =V ∗V⊤ ∗U ∗W (E.4)
t+1 t+1,⊥ X X t+1 t+1,⊥ X⊥ X⊥ t+1 t+1,⊥ X⊥ X⊥ t+1 t+1,⊥
where the last equality follows from Lemma C.1 due to the property V⊤ ∗U ∗W =0.
X t+1 t+1,⊥
Now,wesplitthetermV ∗V⊤ ∗U ∗W intotwopartsusingW ∗W⊤+W ∗W⊤ =I,
X⊥ X⊥ t+1 t+1,⊥ t t t,⊥ t,⊥
which leads to
V ∗V⊤ ∗U ∗W =V ∗V⊤ ∗U ∗W ∗W⊤∗W +V ∗V⊤ ∗U ∗W ∗W⊤ ∗W
X⊥ X⊥ t+1 t+1,⊥ X⊥ X⊥ t+1 t t t+1,⊥ X⊥ X⊥ t+1 t,⊥ t,⊥ t+1,⊥
(E.5)
To estimate the norm of V ∗V⊤ ∗U ∗W in each slice in the Fourier domain, we will use the
X⊥ X⊥ t t+1,⊥
above-given representation and estimate each of the summands individually. Let us start with the second
one. Its jth slice in the Fourier domain reads as
(V ∗V⊤ ∗U ∗W ∗W⊤ ∗W )(j) =V(j) V(j)HU(j) W(j)W(j),HW(j) .
X⊥ X⊥ t+1 t,⊥ t,⊥ t+1,⊥ X⊥ X⊥ t+1 t,⊥ t,⊥ t+1,⊥
Due to the orthogonality of the columns of V(j) , it holds that ∥V(j) V(j)HU(j) W(j)W(j),HW(j) ∥ =
X⊥ X⊥ X⊥ t+1 t,⊥ t,⊥ t+1,⊥
∥V(j)HU(j) W(j)W(j),HW(j) ∥. In the Fourier domain, this allows us to focus on jth slices of the last one
X⊥ t+1 t,⊥ t,⊥ t+1,⊥
V(j)HU(j) W(j)W(j),HW(j) :=G(j).
X⊥ t+1 t,⊥ t,⊥ t+1,⊥ 2
32DuetothedefinitionofthegradientdescentiteratesU ,wehavethefollowingrepresentationforitsblocks
t+1
U(j) in the Fourier domain
t+1
U(j) =(cid:16) Id+µ(cid:0) A∗A(X ∗X⊤−U ∗U⊤)(cid:1)(j)(cid:17) U(j)
t+1 t t t
To upper bound the norm of G(j), we want to apply Lemma H.3. Due to the assumptions in this lemma
2
that V⊤ ∗ U ∗ W has full tubal rank with all invertible t-SVD-tubes and ∥V⊤ ∗V ∥≤cκ−1
X t+1 t X⊥ Ut∗Wt
in addition to the conditions on µ and the decomposition of gradient descent iterates into the signal and
noise term, the conditions of Lemma H.3 are satisfied for the choice Y =U(j) and Y =U(j) and Z as
1 t+1 t
Z =(cid:0) A∗A(X ∗X⊤−U ∗U⊤)(cid:1)(j) . This allows to upper-bound the norm of G(j) as follows
t t 2
∥G(j)∥≤∥U(j)W(j)∥(cid:16) 1−µ∥U(j)W(j)∥2+µ∥(cid:0) A∗A(X ∗X⊤−U ∗U⊤)(cid:1)(j) −(X(j)X(j)H−U(j)U(j)H)∥(cid:17)
2 t t,⊥ t t,⊥ t t t t
+µ2(cid:16) ∥U(j)W(j)∥2+∥(cid:0) A∗A(X ∗X⊤−U ∗U⊤)(cid:1)(j) −(X(j)X(j)H−U(j)U(j)H)∥(cid:17) ∥U(j)W(j)∥3
t t t t t t t t,⊥
Using now the fact that for each j it holds that
∥(cid:0) A∗A(X ∗X⊤−U ∗U⊤)(cid:1)(j) −(X(j)X(j)H−U(j)U(j)H)∥≤∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥
t t t t t t
and that ∥U(j)∥≤∥U ∥≤3∥X∥, we can proceed with the bound for the norm of G(j) as below
t t 2
(cid:16) (cid:17)
∥G(j)∥≤∥U(j)W(j)∥ 1−µ∥U(j)W(j)∥2+µ∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥
2 t t,⊥ t t,⊥ t t
(cid:16) (cid:17)
+µ2 9∥X∥2+∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥ ∥U(j)W(j)∥3
t t t t,⊥
(cid:110) (cid:111)
Further, using the assumption µ≤c min 1 ∥X∥−2,∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥−1 , we get
1 10 t t
(cid:16) (cid:17) µ
∥G(j)∥≤∥U(j)W(j)∥ 1−µ∥U(j)W(j)∥2+µ∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥ + ∥U(j)W(j)∥3
2 t t,⊥ t t,⊥ t t 2 t t,⊥
(cid:16) µ (cid:17)
=∥U(j)W(j)∥ 1− ∥U(j)W(j)∥2+µ∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥ .
t t,⊥ 2 t t,⊥ t t
Now, let us return to the first summand in (E.5), that is V⊤ ∗U ∗W ∗W⊤∗W . Using again the
X t+1 t t t+1,⊥
fact that V ∗U ∗W =0 allows us to rewrite it as
X t+1 t+1,⊥
V⊤ ∗U ∗W ∗W⊤∗W =−V⊤ ∗U ∗W ∗W⊤ ∗W (E.6)
X t+1 t t t+1,⊥ X t+1 t,⊥ t,⊥ t+1,⊥
Moreover, for the same summand, the corresponding jth slice in the Fourier domain reads as
V(j)HU(j) W(j)W(j)HW(j) :=G(j).
X⊥ t+1 t t t+1,⊥ 1
Due to relation (E.6) in the tensor domain, in the Fourier domain it holds that
V(j)HU(j) W(j)W(j)HW(j) =−V(j)HU(j) W(j)W(j)HW(j) ,
X t+1 t t t+1,⊥ X t+1 t,⊥ t,⊥ t+1,⊥
which allows to represent W(j)HW(j) as
t t+1,⊥
(cid:16) (cid:17)−1
W(j)HW(j) =− V(j)HU(j) W(j) V(j)HU(j) W(j)W(j)HW(j) .
t t+1,⊥ X t+1 t X t+1 t,⊥ t,⊥ t+1,⊥
Note that the matrix on the RHS above is invertible due to the assumption that V⊤ ∗U ∗W has full
X t+1 t
tubal rank with all invertible t-SVD-tubes. From here, G(j) can be represented as
1
(cid:16) (cid:17)−1
G(j) =V(j)HU(j) W(j) V(j)HU(j) W(j) V(j)HU(j) W(j)W(j)HW(j) .
1 X⊥ t+1 t X t+1 t X t+1 t,⊥ t,⊥ t+1,⊥
33According to Lemma H.3, the norm of G(j) can be bounded from above as
1
∥G(j)∥≤2µ(cid:16) ∥V(j)HV ∥∥U(j)W(j)∥2+∥(cid:0) A∗A(X ∗X⊤−U ∗U⊤)(cid:1)(j) −(X(j)X(j)H−U(j)U(j)H)∥(cid:17) ·
1 X⊥ U(j)W(j) t t t t t t
t t
·∥V(j)HV ∥∥U(j)W(j)∥
X⊥ U(j) W(j) t t,⊥
t+1 t
(cid:16) (cid:17)
≤2µ ∥V(j)HV ∥∥U(j)∥2+∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥ ·∥V(j)HV ∥∥U(j)W(j)∥
X⊥ U(j)W(j) t t t X⊥ U(j) W(j) t t,⊥
t t t+1 t
(cid:16) (cid:17)
≤2µ ∥V(j)HV ∥∥U(j)∥2+∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥ ·∥V⊤ ∗V ∥∥U(j)W(j)∥
X⊥ U(j)W(j) t t t X⊥ Ut+1∗Wt t t,⊥
t t
Due to ∥V⊤ ∗V ∥ ≤ 1 from Lemma E.2, the fact that ∥U(j)∥ ≤ ∥U ∥, and our assumption that
X⊥ Ut+1∗Wt 50 t t
∥U ∥≤3∥X∥, the norm of G(j) can be further bounded as
t 1
(cid:16) (cid:17)
∥G(j)∥≤µ 9∥V(j)HV ∥∥X∥2+∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥ ∥U(j)W(j)∥
1 X⊥ U(j)W(j) t t t t,⊥
t t
(cid:16) (cid:17)
=µ 9∥(V⊤ ∗V )(j)∥∥X∥2+∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥ ∥U(j)W(j)∥.
X⊥ Ut∗Wt t t t t,⊥
Since due to representation (E.4), it holds that
∥(cid:0)
U ∗W
(cid:1)(j)∥=∥(cid:0)
V ∗U ∗W
(cid:1)(j)∥,
com-
t+1 t+1,⊥ X⊥ t+1 t+1,⊥
bining the inequalities for ∥G(j)∥ and ∥G(j)∥ together with U(j)W(j) = (cid:0) U ∗W (cid:1)(j) leads to the final
1 2 t t,⊥ t t,⊥
result
∥(cid:0) U ∗W (cid:1)(j)∥≤(cid:16) 1− µ ∥(cid:0) U ∗W (cid:1)(j)∥2+9µ∥(V⊤ ∗V )(j)∥∥X∥2
t+1 t+1,⊥ 2 t t,⊥ X⊥ Ut∗Wt
+2µ∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥(cid:17) ∥(cid:0) U ∗W (cid:1)(j)∥.
t t t t,⊥
ThenextlemmashowsthatthetensorsW andW spanapproximatelythesametensorcolumnspace.
t t+1
Lemma E.4. Assume that the following conditions hold
∥U ∥≤3∥X∥, (E.7)
t
µ≤c∥X∥−2κ−2 (E.8)
∥V⊤ ∗V ∥≤cκ−1 (E.9)
X⊥ Ut∗Wt
∥U ∗W (j)∥≤2σ (U ∗W (j)), (E.10)
t t,⊥ min t t
∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥≤cσ2 (X). (E.11)
t t min
Then it holds that
(cid:16) (cid:17)
∥W⊤ ∗W ∥≤µ 1 σ2 (X)+∥U ∗W ∥∥U ∗W ∥ ∥V⊤ ∗V ∥+4µ∥(A∗A−I)(X∗X⊤−U ∗U⊤)∥
t,⊥ t+1 4800 min t t t t,⊥ X⊥ Ut∗Wt t t
and σ (W⊤∗W (j))≥ 1, 1≤j ≤k.
min t t+1 2
Proof. ToboundthenormofW⊤ ∗W ,wewillrewriteW⊤ ∗W intheFourierdomainwiththehelp
t,⊥ t+1 t,⊥ t+1
of Fourier slices of V⊤ ∗U . First, note that due to the decomposition of the gradient iterates into the noise
X t
andsignalterm,itholdsV⊤∗U =V⊤∗U ∗W ∗W⊤ . Thisallowsustorepresentthecorresponding
X t+1 X t+1 t+1 t+1
jth Fourier slices of V⊤ ∗U as V(j)HU(j) = V(j)HU(j) W(j) W(j)H, which means that for each j, the
X t+1 X t+1 X t+1 t+1 t+1
matricesV(j)HU(j) andV(j)HU(j) W(j) W(j)H havethesamekernel,andthereforeU(j)HV(j) spansthesame
X t+1 X t+1 t+1 t+1 t+1 X
subspace as W(j) W(j)HU(j)HV(j). Due to this and the following representation of the matrices
t+1 t+1 t+1 X
U(j) =U(j)W(j)W(j)H+U(j)W(j)W(j)H (E.12)
t t t t t t t
U(j) =U(j) W(j) W(j)H+U(j) W(j) W(j)H, (E.13)
t+1 t+1 t+1 t+1 t+1 t+1 t+1
34we can apply Lemma H.4 to estimate the norm of WH W(j) taking Y =U(j) and Y =U(j) and Z as
t,⊥ t+1 1 t+1 t
Z(j) :=(cid:0) A∗A(X ∗X⊤−U ∗U⊤)(cid:1)(j) .
t t
This gives us the following estimate
(cid:32) (cid:33)
∥Z(j)∥∥U(j)W(j)∥
∥WH W(j) ∥≤µ 1+µ t t ∥U(j)W(j)∥∥U(j)W(j)∥∥V(j)HV ∥ (E.14)
t,⊥ t+1 σ (V(j)HU(j) ) t t t t,⊥ X U( tj)W t(j)
min X t+1
∥Z(j)−(X(j)X(j)H−U(j)U(j)H)∥
+µ t t ∥U(j)W(j)∥.
σ (V(j)HU(j) ) t t,⊥
min X t+1
To proceed further with the upper bound above, we will first show that in each Fourier slice it holds that
σ (cid:0) V(j)HU(j) (cid:1) ≥ 1 σ (U(j)W(j)), 1≤j ≤k. (E.15)
min X t+1 2 min t t
First, note that
σ (cid:0) V(j)HU(j) (cid:1) ≥σ (cid:0) V(j)HU(j) W(j) (cid:1) =σ (cid:0) V(j)H(Id+µZ(j))U(j)W(j) (cid:1)
min X t+1 min X t+1 t+1 min X t t+1
=σ (cid:0) V(j)H(Id+µZ(j))V VH U(j)W(j) (cid:1)
min X U t(j)W t(j +) 1 U t(j)W t(j +) 1 t t+1
≥σ (cid:16) V(j)H(Id+µZ(j))V (cid:17) ·σ (cid:0) VH U(j)W(j) (cid:1)
min X U t(j)W( tj +) 1 min U t(j)W t(j +) 1 t t+1
≥(cid:18) σ min(cid:16) V( Xj)HV
U( tj)W( tj +)
1(cid:17) −µ(cid:13) (cid:13)V( Xj)HZ(j)V
U t(j)W t(j +)
1(cid:13) (cid:13)(cid:19) ·σ min(cid:0) V UH
t(j)W t(j +)
1U( tj)W( t+j) 1(cid:1) .
Due to our assumption (E.9) on the principal angle ∥V⊤ ∗V ∥ and the properties of the tensor slices,
X⊥ Ut∗Wt
we have that
(cid:114)
(cid:16) (cid:17) (cid:16) (cid:17) (cid:13) (cid:13)2 3
σ V(j)HV ≥σ V⊤ ∗V = 1−(cid:13)V⊤ ∗V (cid:13) ≥ ,
min X U(j)W(j) min X Ut∗Wt+1 (cid:13) X Ut∗Wt+1(cid:13) 4
t t+1
where that last inequality can be guaranteed by choosing c > 0 small enough. Thus, to show that relation
(E.15) holds we need to demonstrate that µ(cid:13) (cid:13)V( Xj)HZ(j)V U(j)W(j) (cid:13) (cid:13) be bounded from above by 1 4. For this, we
t t+1
will proceed as follows
µ(cid:13) (cid:13)V( Xj)HZ(j)V
U(j)W(j)
(cid:13) (cid:13)≤µ(cid:13) (cid:13)Z(j)(cid:13) (cid:13)≤µ(cid:13) (cid:13)Z(j)−(X(j)X(j)H−U( tj)U( tj)H)(cid:13) (cid:13)+µ(cid:13) (cid:13)X(j)X(j)H−U( tj)U( tj)H∥.
t t+1
(E.16)
By the definition of Z(j), for the first summand from above we have
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)Z(j)−(X(j)X(j)H−U(j)U(j)H)(cid:13)=(cid:13)(cid:0) A∗A(X ∗X⊤−U ∗U⊤)(cid:1)(j) −(X(j)X(j)H−U(j)U(j)H)(cid:13)
(cid:13) t t (cid:13) (cid:13) t t t t (cid:13)
(cid:13) (cid:13)
=(cid:13)(cid:0) I−A∗A(cid:1) (X ∗X⊤−U ∗U⊤)(j)(cid:13)
(cid:13) t t (cid:13)
(cid:13) (cid:13)
≤(cid:13)(cid:0) I−A∗A(cid:1) (X ∗X⊤−U ∗U⊤)(cid:13)
(cid:13) t t (cid:13)
and for the second summand, it holds that
∥X(j)X(j)H−U(j)U(j)H∥≤∥X ∗X⊤−U ∗U⊤∥≤∥X∥2+∥U ∥2.
t t t t t
This allows us to proceed with inequality (E.16) as
µ(cid:13) (cid:13)V( Xj)HZ(j)V
U(j)W(j)
(cid:13) (cid:13)≤µ(cid:13) (cid:13)(cid:0) I−A∗A(cid:1) (X ∗X⊤−U t∗U⊤
t
)(cid:13) (cid:13)+µ(∥X∥2+∥U t∥2)
t t+1
≤µ(cid:13) (cid:13)(cid:0) I−A∗A(cid:1) (X ∗X⊤−U t∗U⊤
t
)(cid:13) (cid:13)+10µ∥X∥2)≤µcσ m2 in(X)+11µ∥X∥2 ≤ 1 2,
35where in the first line we used assumption (E.7), and in the second assumption(E.11). The third inequality
above follows from our assumption on µ and sufficiently small constant c > 0. This, in turn, shows that
relation (E.15) holds and we can proceed with (E.14) in the following manner
(cid:32) (cid:33)
∥Z(j)∥∥U(j)W(j)∥
∥WH W(j) ∥≤µ 1+2µ t t ∥U(j)W(j)∥∥U(j)W(j)∥∥V(j)HV ∥
t,⊥ t+1 σ (U(j)W(j)) t t t t,⊥ X U t(j)W t(j)
min t t
∥Z(j)−(X(j)X(j)H−U(j)U(j)H)∥
+2µ t t ∥U(j)W(j)∥.
σ (U(j)W(j)) t t,⊥
min t t
Now, using assumption (E.10) and the definition of Z(j), we have
∥WH W(j) ∥≤µ∥V(j)HV ∥∥U(j)W(j)∥∥U(j)W(j)∥
t,⊥ t+1 X⊥ U(j)W(j) t t t t,⊥
t t
+4µ∥(cid:0) A∗A(X ∗X⊤−U ∗U⊤)(cid:1)(j) −(X(j)X(j)H−U(j)U(j)H)∥
t t t t
+4µ2∥(cid:0) A∗A(X ∗X⊤−U ∗U⊤)(cid:1)(j) ∥∥U(j)W(j)∥2∥V(j)HV ∥
t t t t X⊥ U(j)W(j)
t t
≤µ∥V(j)HV ∥∥U(j)W(j)∥∥U(j)W(j)∥
X⊥ U(j)W(j) t t t t,⊥
t t
+4µ∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥
t t
+4µ2∥A∗A(X ∗X⊤−U ∗U⊤)∥∥U(j)W(j)∥2∥V(j)HV ∥.
t t t t X⊥ U(j)W(j)
t t
In the last inequality, we used the tensor norm as the maximum norm in each Fourier slice. Note that,
similarly to one of the estimates above, we get
∥A∗A(X ∗X⊤−U ∗U⊤)∥≤∥X ∗X⊤−U ∗U⊤∥+∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥
t t t t t t
≤∥X∥2+∥U ∥2+cσ2 (X)≤11∥X∥2 (E.17)
t min
where the last line holds due to the assumption ∥U ∥≤3∥X∥ and that c is small enough.
t
Now, since µ ≤ c∥X∥−2κ−2, ∥U(j)W(j)∥ ≤ ∥U ∥ ≤ 3∥X∥ and ∥U(j)W(j)∥ ≤ ∥U ∥ ≤ 3∥X∥, constant
t t t t t,⊥ t
c > 0 can be chosen so that 4µ·11∥X∥2 ≤ 1 σ2 (X), together with (E.17) and (E.11) we can proceed
4800 min
with the estimation of WH W(j) as
t,⊥ t+1
∥W(j)HW(j) ∥≤µ(cid:0) 1 σ2 (X)+9∥X∥2(cid:1) ∥V(j)HV ∥+4µcσ2 (X).
t,⊥ t+1 4800 min X⊥ U(j)W(j) min
t t
Using the assumption µ ≤ c∥X∥−2 and choosing c > 0 small enough, we obtain that ∥W(j)HW(j) ∥ ≤ 1.
t,⊥ t+1 2
(cid:113)
Note that this implies that σ (W⊤∗W (j))= 1−∥W(j)HW(j) ∥2 ≥ 1, which finishes the proof.
min t t+1 t,⊥ t+1 2
Lemma E.5. Assume that the following conditions hold
∥U ∗W (j)∥≤2σ (U ∗W (j)), (E.18)
t t,⊥ min t t
∥U ∥≤3∥X∥, (E.19)
t
∥V⊤ ∗V ∥≤c˜ (E.20)
X⊥ Ut∗Wt
µ≤c∥X∥−2κ−2 (E.21)
∥U ∗W ∥≤cκ−2∥X∥ (E.22)
t t,⊥
∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥≤cσ2 (X). (E.23)
t t min
Then the angle between the column space of the signal term U ∗W and column space of X stays sufficiently
t t
small from one iteration to another, namely
(cid:16) µ (cid:17)
∥V⊤ ∗V ∥≤ 1− σ2 (X) ∥V⊤ ∗V ∥
X⊥ Ut+1∗Wt+1 4 min X⊥ Ut∗Wt
+150µ∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥+500µ2∥X ∗X⊤−U ∗U⊤∥2.
t t t t
36Proof. Toestimatetheprincipalangle∥V⊤ ∗V ∥,wefirstinvestigatethetensor-columnsubspace
X⊥ Ut+1∗Wt+1
of U ∗W . By the definition of U and W ∗W⊤+W ∗W⊤ =I, we have
t+1 t+1 t+1 t t t,⊥ t,⊥
(cid:16) (cid:17)
U ∗W = I +µ(A∗A)(X ∗X⊤−U ∗U⊤) ∗U ∗W
t+1 t+1 t t t t+1
=(I +µZ)∗U ∗W ∗W⊤∗W +(I +µZ)∗U ∗W ∗W⊤ ∗W .
t t t t+1 t t,⊥ t,⊥ t+1
where we use notation Z := (A∗A)(X ∗X⊤−U ∗U⊤). This allows to represent jth slice of U ∗W
t t t+1 t+1
in the Fourier domain as
U(j) W(j) =(Id+µZ(j))U(j)W(j)W(j)HW(j) +(Id+µZ(j))U(j)W(j)W(j)HW(j) .
t+1 t+1 t t t t+1 t t,⊥ t,⊥ t+1
with Z(j) = (A∗A)(X ∗X⊤−U ∗U⊤)(j). Because of this representation and decomposition (E.12), to
t t
boundtheprincipalanglebetweenU ∗W andX, wewanttoapplyinequality (H.5)fromLemmaH.4,
t+1 t+1
but for this we first need to check whether for
(cid:16) (cid:17)−1
P(j) :=U(j)W(j)W(j)HW(j) VH U(j)W(j)W(j)HW(j) VH
t t,⊥ t,⊥ t+1 U(j)W(j) t t t t+1 U(j)W(j)
t t t t
the following applies
∥µZ(j)+P(j)+µZ(j)P(j)∥≤1.
For convenience, we denote B(j) :=µZ(j)+P(j)+µZ(j)P(j). Using the triangular inequality and submulti-
plicativity of the norm, we bet the first simple bound on the norm of B(j)
∥B(j)∥≤µ∥Z(j)∥+(1+µ∥Z(j)∥)∥P(j)∥ (E.24)
Note that P(j) can be rewritten as
(cid:16) (cid:17)−1(cid:16) (cid:17)−1
P(j) =U(j)W(j)W(j)HW(j) W(j)HW(j) VH U(j)W(j) VH ,
t t,⊥ t,⊥ t+1 t t+1 U(j)W(j) t t U(j)W(j)
t t t t
which allows for the following estimate of its norm
(cid:13)(cid:16) (cid:17)−1(cid:13)(cid:13)(cid:16) (cid:17)−1(cid:13)
∥P(j)∥≤∥U(j)W(j)∥∥W(j)HW(j) ∥(cid:13) W(j)HW(j) (cid:13)(cid:13) VH U(j)W(j) (cid:13)∥VH ∥
t t,⊥ t,⊥ t+1 (cid:13) t t+1 (cid:13)(cid:13) U(j)W(j) t t (cid:13) U(j)W(j)
t t t t
∥U(j)W(j)∥∥W(j)HW(j) ∥
≤ t t,⊥ t,⊥ t+1 .
σ (W(j)HW(j) )·σ (U(j)W(j))
min t t+1 min t t
From here, using assumption (E.18) and a lower bound on σ (W(j)HW(j) ) from Lemma E.4, we get
min t t+1
∥P(j)∥≤4∥W(j)HW(j) ∥. (E.25)
t,⊥ t+1
Using this and the definition of Z(j), we have
(cid:16) (cid:17)
∥B(j)∥≤µ∥(A∗A)(X ∗X⊤−U ∗U⊤)(j)∥+4 1+µ∥(A∗A)(X ∗X⊤−U ∗U⊤)(j)∥ ∥W(j)HW(j) ∥.
t t t t t,⊥ t+1
(E.26)
Due to the assumption on µ, we can bound µ∥(A∗A)(X ∗X⊤−U ∗U⊤)(j)∥ as follows
t t
µ∥(A∗A)(X ∗X⊤−U ∗U⊤)(j)∥≤µ∥(A∗A)(X ∗X⊤−U ∗U⊤)(j)∥
t t t t
≤µ∥(I−A∗A)(X ∗X⊤−U ∗U⊤)∥+µ∥X ∗X⊤−U ∗U⊤∥
t t t t
≤µ(cσ2 (X)+10∥X∥2)≤1
min
whereinthetwolastinequalitiesweuseassumptions(E.23), (E.19)and(E.21)withthefactforthelearning
rate constant c>0 can be chosen sufficiently small.
37This, in turn, allows us to proceed with inequality (E.26) as
∥B(j)∥≤µ∥(A∗A)(X ∗X⊤−U ∗U⊤)(j)∥+8∥W(j)HW(j) ∥. (E.27)
t t t,⊥ t+1
Now,applyingtheboundon∥W(j)HW(j) ∥≤∥W⊤ ∗W ∥fromLemmaE.4andsimilartransformation
t,⊥ t+1 t,⊥ t+1
for ∥(A∗A)(X ∗X⊤−U ∗U⊤)(j)∥ as above, we come the following result in (E.27)
t t
(cid:16) (cid:17)
∥B(j)∥≤µ∥X ∗X⊤−U ∗U⊤∥+µ 1 σ (X)2+8∥U ∗W ∥∥U ∗W ∥ ∥V⊤ ∗V ∥
t t 600 min t t t t,⊥ X⊥ Ut∗Wt
+33µ∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥
t t
To show that this bound above can be made smaller than one, we use assumptions (E.22), (E.23) and that
∥U ∗W ∥≤∥U∥≤2∥X∥, which leads to
t t
(cid:16) σ (X) (cid:17)
∥B(j)∥≤µ∥X ∗X⊤−U ∗U⊤∥+µ 1 σ (X)2+8c min ·3∥X∥ ∥V⊤ ∗V ∥+33µcσ2 (X)
t t 600 min κ2 X⊥ Ut∗Wt min
1
≤µ10∥X∥2+µc σ2 (X)+33µcσ2 (X)≤1,
300 min min
with the last inequality following from the assumption on µ. In such a way, we check the conditions of
Lemma H.4 to be able to apply inequality (H.5). This gives
(cid:16) µ (cid:17)
∥V(j)HV ∥≤∥V(j)HV ∥ 1− σ2 (X(j))+µ∥U(j)W(j)∥
X⊥ U(j) W(j) X⊥ U(j)W(j) 2 min t t,⊥
t+1 t+1 t t
(cid:16) (cid:17)
2∥W(j)HW(j) ∥∥U(j)W(j)∥
+µ∥Z(j)−(X(j)X(j)H−U(j)U(j)H)∥+ 1+µ∥Z(j)∥ t,⊥ t+1 t t,⊥
t t σ (W(j)HW(j) )σ (U(j)W(j))
min t t+1 min t t
(cid:32) ∥W(j)HW(j) ∥∥U(j)W(j)∥ (cid:33)2
+57 µ∥Z(j)∥+(1+µ∥Z(j)∥) t,⊥ t+1 t t,⊥ .
σ (W(j)HW(j) )σ (U(j)W(j))
min t t+1 min t t
Applying again assumption (E.18) and a lower bound on σ (W(j)HW(j) ) from Lemma E.4 as for (E.25),
min t t+1
in addition to (E.22), we get
(cid:16) µ (cid:17)
∥V(j)HV ∥≤∥V(j)HV ∥ 1− σ2 (X(j)) +µ∥Z(j)−(X(j)X(j)H−U(j)U(j)H)∥
X⊥ U(j) W(j) X⊥ U(j)W(j) 3 min t t
t+1 t+1 t t
+8(cid:0) 1+µ∥Z(j)∥(cid:1) ∥W(j)HW(j) ∥+57(cid:16) µ∥Z(j)∥+4(cid:0) 1+µ∥Z(j)∥(cid:1) ∥W(j)HW(j) ∥(cid:17)2 .
t,⊥ t+1 t,⊥ t+1
Now, making
(cid:0) 1+µ∥Z(j)∥(cid:1)
≤ 3 by choosing c > 0 small enough and using the properties of the terms
involved, the above inequality gets the following view
(cid:16) µ (cid:17)
∥V(j)HV ∥≤∥V(j)HV ∥ 1− σ2 (X) +µ∥(I−A∗A)(X ∗X⊤−U ∗U⊤)∥
X⊥ U(j) W(j) X⊥ U(j)W(j) 3 min t t
t+1 t+1 t t
(cid:16) (cid:17)2
+32∥W(j)HW(j) ∥+57 µ∥Z(j)∥+12∥W(j)HW(j) ∥ . (E.28)
t,⊥ t+1 t,⊥ t+1
To proceed further with (E.28), we will first do several auxiliary estimates. We start by bounding the norm
∥W(j)HW(j) ∥. Since it holds that ∥W(j)HW(j) ∥≤∥W⊤ ∗W ∥, from Lemma E.4, one gets
t,⊥ t+1 t,⊥ t+1 t,⊥ t+1
(cid:16) (cid:17)
∥W(j)HW(j) ∥≤µ 1 σ2 (X)+∥U ∗W ∥∥U ∗W ∥ ∥V⊤ ∗V ∥
t,⊥ t+1 4800 min t t t t,⊥ X⊥ Ut∗Wt
+4µ∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥
t t
≤µ(cid:0) 1 σ2 (X)+3cσ2 (X)(cid:1) ∥V⊤ ∗V ∥+4µ∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥
4800 min min X⊥ Ut∗Wt t t
≤ 1 µσ2 (X)∥V⊤ ∗V ∥+4µ∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥ (E.29)
2400 min X⊥ Ut∗Wt t t
38where we use in the second inequality that ∥U ∗W ∥ ≤ ∥U ∥ ≤ 3∥X∥ and ∥U ∗W ∥ ≤ cκ−2∥X∥ by
t t t t t,⊥
assumption, and in the last line that c > 0 can be chosen small enough. Using this estimate, let us bound
from above the squared term in (E.28) as follows
σ2 (X)
µ∥Z(j)∥+12∥W(j)HW(j) ∥≤µ∥Z(j)∥+µ min ∥V⊤ ∗V ∥+48µ∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥
t,⊥ t+1 200 X⊥ Ut∗Wt t t
σ2 (X)
≤µ∥X(j)X(j)H−U(j)U(j)H∥+µ min ∥V⊤ ∗V ∥
t t 200 X⊥ Ut∗Wt
+49µ∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥.
t t
From here, using Jensen’s inequality, we obtain
σ4 (X)
(µ∥Z(j)∥+12∥W(j)HW(j) ∥)2 ≤3µ2∥X(j)X(j)H−U(j)U(j)H∥2+3µ2 min ∥V⊤ ∗V ∥2
t,⊥ t+1 t t 2002 X⊥ Ut∗Wt
+3·492µ2∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥2.
t t
Now, we can come back to bounding (E.28) proceeding as follows
(cid:18) (cid:19)
µ 4µ
∥V(j)HV ∥≤∥V⊤ ∗V ∥ 1− σ2 (X)+ σ2 (X)
X⊥ U(j) W(j) X⊥ Ut∗Wt 3 min 300 min
t+1 t+1
+129µ∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥
t t
171σ4 (X)
+171µ2∥X(j)X(j)H−U(j)U(j)H∥2+µ2 min ∥V⊤ ∗V ∥2
t t 2002 X⊥ Ut∗Wt
+171·492µ2∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥2
t t
(cid:18) (cid:19)
µ 4µ 171
≤∥V⊤ ∗V ∥ 1− σ2 (X)+ σ2 (X)+ κ−4c·cµσ2 (X)
X⊥ Ut∗Wt 3 min 300 min 2002 (cid:101) min
+171µ2∥X ∗X⊤−U ∗U⊤∥2
t t
+µ(129+171·492c2κ−4)∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥,
t t
where for the last inequality we used assumptions (E.23), (E.20) and (E.21), and the properties of the tubal
tensor norm. Now choosing constant c>0 sufficiently small, we obtain that
(cid:16) µ (cid:17)
∥V(j)HV ∥≤ 1− σ2 (X) ∥V⊤ ∗V ∥+200µ2∥X ∗X⊤−U ∗U⊤∥2
X⊥ U(j) W(j) 4 min X⊥ Ut∗Wt t t
t+1 t+1
+150∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥.
t t
Since the right-hand side of the above inequality is independent of j, we obtain the lemma statement.
The following lemma shows that under a mild condition the technical assumption
∥U ∥≤3∥X∥
t+1
needed in the lemmas above holds.
Lemma E.6. Assume that ∥U ∥ ≤ 3∥X∥, µ ≤ 1 ∥X∥−2 and that linear measurement operator A is such
t 27
that
∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥≤∥X∥2
t t
Then for the iteration t+1, it also holds ∥U ∥≤3∥X∥.
t+1
Proof. Consider the gradient iterate
U =U +µA∗A(X ∗X⊤−U ∗U⊤)∗U
t+1 t t t t
=U +µ(X ∗X⊤−U ∗U⊤)∗U +µ(A∗A−I)(X ∗X⊤−U ∗U⊤)∗U
t t t t t t t
=(I −µU ∗U⊤)∗U +µX ∗X⊤∗U +µ(A∗A−I)(X ∗X⊤−U ∗U⊤)∗U .
t t t t t t t
39To estimate the norm of U , we will bound each summand above separately. Due to the assumption on
t+1
µ and the norm of U , we have µ ≤ 1 ∥X∥−2 ≤ 1∥U ∥−2. This allows us to estimate the tensor norm of
t 27 3 t
(I −µU ∗U⊤)∗U via the norm of matrix block representation in the Fourier domain. Namely, assume
t t t
that matrix U has the SVD U =VΣWH. Then for matrix (I −µU ∗U⊤)∗U , we have
t t t t t
(I −µU ∗U⊤)∗U =VΣWH−µVΣWHWΣVHVΣWH =VΣWH−µVΣ3WH =V(Σ−µΣ3)WH.
t t t
From here, since µ≤ 1 ∥X∥−2 ≤ 1∥U∥−2 and ∥U ∥=∥U ∥, it holds that
27 3 t t
∥(I −µU ∗U⊤)∗U ∥=∥U ∥−µ∥U ∥3 =∥U ∥(1−µ∥U ∥2). Besides, from the submultiplicativity
t t t t t t t
of the tensor norm and the triangle inequality, we obtain that
∥U ∥≤(1−µ∥U ∥2+µ∥X∥2+µ∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥)∥U ∥ (E.30)
t+1 t t t t
≤(1−µ∥U ∥2+2µ∥X∥2)∥U ∥, (E.31)
t t
where in the last line we used the assumption on ∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥. By combining inequality
t t
(E.31) with the assumption µ ≤ 1 ≤ 1 , we obtain that ∥U ∥ ≤ 3∥X∥ , which finishes the
27∥X∥2 3∥U∥2 t+1
proof.
The following lemma shows that U ∗W ∗W⊤∗U⊤ converges towards X ∗XT, when projected onto
t t t t
the tensor column space of X.
Lemma E.7. Assume that the following conditions hold
∥U ∥≤3∥X∥ (E.32)
t
1
µ≤c· √ ·κ−2∥X∥−2 (E.33)
nk
1
σ (U ∗W )≥ √ σ (X) (E.34)
min t t min
10
∥V⊤ ∗V ∥≤cκ−2 (E.35)
X⊥ Ut∗Wt
and
max(cid:110)(cid:13) (cid:13)V⊤ X ∗(A∗A−I)(Y t)(cid:13) (cid:13) F, (cid:13) (cid:13)V⊤ Ut∗Wt ∗(A∗A−I)(Y t)(cid:13) (cid:13) F, (cid:13) (cid:13)(A∗A−I)(Y t)(cid:13) (cid:13)(cid:111) ≤κ−2∥Y t∥ F
with Y :=X ∗X⊤−U ∗U⊤. Then it holds that
t t t
∥V⊤ ∗U ∗U⊤∥ ≤3∥V⊤ ∗(X ∗X⊤−U ∗U⊤)∥ +∥U ∗W ∗W⊤ ∗U⊤∥ (E.36)
X⊥ t t F X⊥ t t F t t,⊥ t,⊥ t F
as well as
∥X ∗X⊤−U ∗U⊤∥ ≤4∥V⊤ ∗(X ∗X⊤−U ∗U⊤)∥ +∥U ∗W ∗W⊤ ∗U⊤∥ (E.37)
t t F X⊥ t t F t t,⊥ t,⊥ t F
and
(cid:16) µ (cid:17)
∥V⊤ (X ∗X⊤−U ∗U⊤ )∥ ≤ 1− σ2 (X) ∥V⊤ ∗(X ∗X⊤−U ∗U⊤)∥
X⊥ t+1 t+1 F 200 min X⊥ t t F
σ2 (X)
+µ min ∥U ∗W ∗W⊤ ∗U⊤∥ (E.38)
100 t t,⊥ t,⊥ t F
Proof. We start by proving the first inequality (E.38). For this, let us decompose V⊤ ∗U ∗U⊤ as follows
X⊥ t t
V⊤ ∗U ∗U⊤ =V⊤ ∗U ∗U⊤∗V ∗V⊤ +V⊤ ∗U ∗U⊤∗V ∗V⊤ ,
X⊥ t t X⊥ t t X X X⊥ t t X⊥ X⊥
then using the triangle inequality and submultiplicativity of the Frobenius and the spectral norm, we obtain
∥V⊤ ∗U ∗U⊤∥ ≤∥V⊤ ∗U ∗U⊤∗V ∥ +∥V⊤ ∗U ∗U⊤∗V ∥
X⊥ t t F X⊥ t t X F X⊥ t t X⊥ F
≤∥V⊤ ∗(X ∗X⊤−U ∗U⊤)∗V ∥ +∥V⊤ ∗U ∗U⊤∗V ∥
X⊥ t t X F X⊥ t t X⊥ F
≤∥V⊤ ∗(X ∗X⊤−U ∗U⊤)∥ +∥V⊤ ∗U ∗U⊤∗V ∥ , (E.39)
X t t F X⊥ t t X⊥ F
40where in the second line, we used the orthogonality of the decomposition. Now, we will work additionally on
boundingthenormofV⊤ ∗U ∗U⊤∗V toobtain(E.38). Here,wewillusetheorthogonaldecomposition
X⊥ t t X⊥
with respect to W and W , which leads to
t t,⊥
∥V⊤ ∗U ∗U⊤∗V ∥ ≤∥V⊤ ∗U ∗W ∗W⊤∗U⊤∗V ∥ +∥V⊤ ∗U ∗W ∗W⊤ ∗U⊤∗V ∥
X⊥ t t X⊥ F X⊥ t t t t X⊥ F X⊥ t t,⊥ t,⊥ t X⊥ F
≤∥V⊤ ∗U ∗W ∗W⊤∗U⊤∗V ∥ +∥U ∗W ∗W⊤ ∗U⊤∥
X⊥ t t t t X⊥ F t t,⊥ t,⊥ t F
Now, for the first term above, we get
∥V⊤ ∗U ∗W ∗W⊤∗U⊤∗V ∥ =∥V⊤ ∗V ∗V⊤ ∗U ∗W ∗W⊤∗U⊤∗V ∥
X⊥ t t t t X⊥ F X⊥ Ut∗Wt Ut∗Wt t t t t X⊥ F
k
(cid:88)
= ∥V⊤ ∗V ∗V⊤ ∗U ∗W ∗W⊤∗U⊤∗V (j)∥
X⊥ Ut∗Wt Ut∗Wt t t t t X⊥ F
j=1
k
=(cid:88) ∥V(j)HV(j) V(j)H U(j)W(j)W(j)HU(j)HV(j) ∥
X⊥ Ut∗Wt Ut∗Wt t t t t X⊥ F
j=1
k
=(cid:88) ∥V(j)HV(j) (cid:16) V(j)HV(j) (cid:17)−1 V(j)HV(j) V(j)H U(j)W(j)W(j)HU(j)HV(j) ∥
X⊥ Ut∗Wt X⊥ Ut∗Wt X⊥ Ut∗Wt Ut∗Wt t t t t X⊥ F
j=1
(cid:13) (cid:13) k
≤ 1m ≤ja ≤x k∥V X(j) ⊥HV( Uj t) ∗Wt∥ 1m ≤ja ≤x k(cid:13) (cid:13) (cid:13)(cid:16) V( Xj) ⊥HV( Uj t) ∗Wt(cid:17)−1(cid:13) (cid:13) (cid:13)(cid:88) ∥V( Xj) ⊥HV( Uj t) ∗WtV( Uj t) ∗H WtU( tj)W( tj)W( tj)HU( tj)HV( Xj) ⊥∥ F
j=1
=
∥V⊤
X⊥
∗V Ut∗Wt∥ (cid:88)k
∥V(j)HV(j) V(j)H U(j)W(j)W(j)HU(j)HV(j) ∥
σ (V⊤ ∗V ) X⊥ Ut∗Wt Ut∗Wt t t t t X⊥ F
min X⊥ Ut∗Wt j=1
=
∥V⊤
X⊥
∗V Ut∗Wt∥ (cid:88)k
∥V(j)HU(j)W(j)W(j)HU(j)HV(j) ∥
σ (V⊤ ∗V ) X⊥ t t t t X⊥ F
min X⊥ Ut∗Wt j=1
∥V⊤ ∗V ∥
= X⊥ Ut∗Wt ∥V ∗U ∗W ∗W⊤∗U⊤∗V ∥
X⊥ t t t t X⊥ F
σ (V⊤ ∗V )
min X⊥ Ut∗Wt
∥V⊤ ∗V ∥
= X⊥ Ut∗Wt ∥V ∗U ∗U⊤∗V ∥
X⊥ t t X⊥ F
σ (V⊤ ∗V )
min X⊥ Ut∗Wt
∥V⊤ ∗V ∥
= X⊥ Ut∗Wt ∥V ∗(X ∗X⊤−U ∗U⊤)∗V ∥
X⊥ t t X⊥ F
σ (V⊤ ∗V )
min X⊥ Ut∗Wt
∥V⊤ ∗V ∥
≤ X⊥ Ut∗Wt ∥V ∗(X ∗X⊤−U ∗U⊤)∥ ≤2∥V ∗(X ∗X⊤−U ∗U⊤)∥
X⊥ t t F X⊥ t t F
σ (V⊤ ∗V )
min X⊥ Ut∗Wt
where in the last line we used the assumption (E.35). Them, using just established bound together with
(E.39), we get
∥V⊤ ∗U ∗U⊤∥ ≤3∥V⊤ ∗(X ∗X⊤−U ∗U⊤)∥ +∥U ∗W ∗W⊤ ∗U⊤∥ .
X⊥ t t F X⊥ t t F t t,⊥ t,⊥ t F
To get inequality (E.37), we use the orthogonal decomposition of X ∗X⊤−U ∗U⊤ with respect to V and
t t X
V , which leads to
X⊥
∥X ∗X⊤−U ∗U⊤∥ =∥V⊤ ∗(X ∗X⊤−U ∗U⊤)∥ +∥V⊤ ∗(X ∗X⊤−U ∗U⊤)∥
t t F X t t F X⊥ t t F
=∥V⊤ ∗(X ∗X⊤−U ∗U⊤)∥ +∥V⊤ ∗U ∗U⊤∥
X t t F X⊥ t t F
≤4∥V⊤ ∗(X ∗X⊤−U ∗U⊤)∥ +∥U ∗W ∗W⊤ ∗U⊤∥ .
X t t F t t,⊥ t,⊥ t F
Inequality (E.38) follows from the two inequalities proved here and Lemma 9.5 in [36]. The building stones
for this are the properties of the tubal tensor Frobenius norm. Namely, the Frobenius norm of any tubal
41tensor T can be represented as the sum of Frobenius norms of each slice in the domain, that is
k
(cid:88)
∥T∥ = ∥T(j)∥
F F
j=1
√
and∥T∥ ≤ n·k∥T∥.Besides,theFrobeniusnormoftheproductoftwotensorsT andP canbebounded
F
as below
k k
(cid:88) (cid:88)
∥T ∗P∥ = ∥T(j)P(j)∥ ≤ max ∥T(j)∥ ∥P(j)∥ ≤∥T∥∥P∥ .
F F F F
1≤j≤k
j=1 j=1
Now, we have collected all the necessary ingredients to prove the main result of this section, which shows
thatafterasufficientnumberofinteractions, therelativeerrorbetween U ∗U⊤ andX ∗X⊤ becomessmall.
t t
√
Theorem E.1. Suppose that the stepsize satisfies µ ≤ c kκ−4∥X∥−2 for some small c > 0, and A :
1 1
c
Sn×n×k →Rm satisfies RIP(2r+1,δ) for some constant 0<δ ≤ √1 . Set γ ∈(0,1), and choose a number
κ4 r 2
of iterations t such that σ (U ∗W ) ≥ γ. Also, assume that ∥U ∗W ∥ ≤ 2γ, ∥U ∥ ≤ 3∥X∥,
∗ min t∗ t∗ t∗ t∗,⊥ t∗
c σ (X)
γ ≤ 2 min , and ∥V⊤ ∗V ∥≤c κ−2 for some small c >0. Then, after
κ2min{n,R} X⊥ Ut∗∗Wt∗ 2 2
(cid:18) (cid:26) (cid:27) (cid:19)
1 κr ∥X∥
(cid:98)t−t
∗
≲
µσ (X)2
ln min 1,
k(min{n,R}−r) γ
min
additional iterations, we have
∥U ∗U⊤−X ∗X⊤∥
(cid:98)t (cid:98)t F ≲k5/4r1/8κ−3/16(min{n,R}−r)3/8γ21/16∥X∥−21/16.
∥X∥2
Proof. First, we set
(cid:110) (cid:111)
t
1
=min t≥t
∗
:σ min(V⊤
X
∗U t)≥ √1 10σ min(X) ,
and then aim to prove that over the iterations t ≤t≤t , the following hold:
∗ 1
• σ (V⊤ ∗U )≥ 1γ(cid:0) 1+ 1µσ (X)2(cid:1)t−t∗
min X t 2 8 min
• ∥U ∗W
∥≤2γ(cid:16)
1+80µc
√
kσ
(X)2(cid:17)t−t∗
t t,⊥ 2 min
• ∥U ∥≤3∥X∥
t
• ∥V⊤ ∗V ∥≤c κ−2.
X⊥ Ut∗Wt 2
Intuitively, this means that over the range t ≤ t ≤ t , the smallest singular value of the signal term
∗ 1
V⊤ ∗U grows at a faster rate than the largest singular value of the noise term U ∗W .
X t t t,⊥
For t = t , these inequalities hold due to the assumptions of this theorem. Now, suppose they hold for
∗
some t between t and t . We’ll show they also hold for t+1.
∗ 1
42First, note that we have:
∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥
t t
=∥(A∗A−I)(X ∗X⊤−U ∗W ∗W⊤∗U⊤−U ∗W ∗W⊤ ∗U⊤)∥
t t t t t t,⊥ t,⊥ t
≤∥(A∗A−I)(X ∗X⊤−U ∗W ∗W⊤∗U⊤)∥+∥(A∗A−I)(U ∗W ∗W ∗U⊤)∥
t t t t t t,⊥ t,⊥ t
√ √
(a) ≤δ kr∥X ∗X⊤−U ∗W ∗W⊤∗U⊤∥+δ k∥U ∗W ∗W ∗U⊤∥
t t t t t t,⊥ t,⊥ t ∗
√ (cid:16) (cid:17) √
≤δ kr ∥X ∗X⊤∥+∥U ∗W ∗W⊤∗U⊤∥ +δ k∥U ∗W ∗W ∗U⊤∥
t t t t t t,⊥ t,⊥ t ∗
√ √
=δ kr(cid:0) ∥X∥2+∥U ∗W ∥2(cid:1) +δ k∥U ∗W ∗W ∗U⊤∥
t t t t,⊥ t,⊥ t ∗
√ √
≤δ kr(cid:0) ∥X∥2+∥U ∥2(cid:1) +δ k∥U ∗W ∗W ∗U⊤∥
t t t,⊥ t,⊥ t ∗
√ √
(b) ≤δ kr(cid:0) ∥X∥2+9∥X∥2(cid:1) +δ k(min{n,R}−r)∥U ∗W ∗W ∗U⊤∥
t t,⊥ t,⊥ t
√ √
≤10δ kr∥X∥2+δ k(min{n,R}−r)∥U ∗W ∥2
t t,⊥
√ √
≤10δ krκ2σ (X)2+δ k(min{n,R}−r)∥U ∗W ∥2
min t t,⊥
√ √
(c) ≤10c kκ−2σ (X)2+4δ k(min{n,R}−r)γ2(cid:0) 1+80µc σ (X)2(cid:1)2(t−t∗)
1 min 2 min
√ √
(d) ≤10c kκ−2σ (X)2+8δ k(min{n,R}−r)γ7/4σ (X)1/4
1 min min
√
(e) ≤40c kκ−2σ (X)2.
1 min
Ininequality(a),weusedthefactthatAsatisfiesRIP(2r+1,δ)(andhence,RIP(r+1,δ)andRIP(2,δ)),
√ √
andthus,byLemmasG.2andG.3,alsosatisfiesS2SRIP(r,δ kr)andS2NRIP(δ k). Inequality(b)usesthe
assumption∥U ∥≤3∥X∥andthefactthatU ∗W ∗W⊤ ∗U⊤ hastubalrankatmostmin{n,R}−r. In
t t t,⊥ t,⊥ t
c
inequality (c), we used the assumption δ ≤ √1 along with the second bulleted inequality assumed by the
κ4 r
inductive step. Inequality (d) holds due to the definitions of t and t and the fact that t ≤t≤t . Finally,
1 ∗ ∗ 1
inequality (e) holds due to the assumption γ ≤ c2σmin(X) .
κ2min{n,R}
If c is chosen small enough, the above bound is less than ∥X∥. Then, along with our other assumptions,
1
we can use Lemma E.6 to obtain ∥U ∥≤3∥X∥.
t+1
Next, we can use Lemma E.1 along with the bound σ min(V⊤
X
∗U t)≤ √1 10σ min(X) to obtain
σ (V⊤ ∗U )≥σ (V⊤ ∗U ∗W )
min X t+1 min X t+1 t+1
(cid:18) (cid:19)
1
≥σ (V⊤ ∗U ) 1+ µσ (X)2−µσ (V⊤ ∗U )2
min X t 4 min min X t
(cid:18) (cid:19)
1 1
≥σ (V⊤ ∗U ) 1+ µσ (X)2− µσ (X)2
min X t 4 min 10 min
(cid:18) (cid:19)
1
≥σ (V⊤ ∗U ) 1+ µσ (X)2
min X t 8 min
1
(cid:18)
1
(cid:19)t−t∗ (cid:18)
1
(cid:19)
≥ γ 1+ µσ (X)2 · 1+ µσ (X)2
2 8 min 8 min
1
(cid:18)
1
(cid:19)t−t∗+1
= γ 1+ µσ (X)2
2 8 min
Sinceσ (V⊤ ∗U ∗W )=σ (V⊤ ∗U ), whichispositivebytheabovebound, allthesingular
min X t+1 t+1 min X t+1
tubes of V⊤ ∗U ∗W are invertible. Hence, we can apply Lemma E.3 to obtain
X t+1 t+1
43(cid:16) µ
∥U ∗W (j)∥≤ 1− ∥U ∗W (j)∥2+9µ∥V⊤ ∗V (j)∥∥X∥2
t+1 t+1,⊥ 2 t t,⊥ X⊥ Ut∗Wt
(cid:17)
+2µ∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥ ∥U ∗W (j)∥
t t t t,⊥
≤(cid:16)
1−
µ ·4γ2(cid:16)
1+80µc
√
kσ
(X)2(cid:17)2(t−t∗)
+9µc κ−2∥X∥2
2 2 min 2
√ (cid:17)
+2µ·40c kκ−2σ (X)2 ∥U ∗W (j)∥
1 min t t,⊥
≤(cid:16)
1−
µ ·4γ2(cid:16)
1+80µc
√
kσ
(X)2(cid:17)2(t−t∗)
+9µc σ (X)2
2 2 min 2 min
√ (cid:17)
+80c µ kκ−2σ (X)2 ∥U ∗W (j)∥
1 min t t,⊥
(cid:16) √ (cid:17)
≤ 1+80c µ kκ−2σ (X)2 ∥U ∗W (j)∥
1 min t t,⊥
(cid:16) √ (cid:17)
≤ 1+80c µ kσ (X)2 ∥U ∗W (j)∥
1 min t t,⊥
≤2γ(cid:16)
1+80c
µ√
kσ
(X)2(cid:17)t−t∗+1
,
1 min
where we have used the inductive assumption that the inequalities hold for t along with the fact that
κ=∥X∥/σ (X)≥1.
min
Next, we will bound the term using Lemma E.5
∥V⊤ ∗V ∥
X⊥ Ut+1∗Wt+1
(cid:16) µ (cid:17)
≤ 1− σ2 (X) ∥V⊤ ∗V ∥+150µ∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥+500µ2∥X ∗X⊤−U ∗U⊤∥2
4 min X⊥ Ut∗Wt t t t t
(cid:16) µ (cid:17) √
≤ 1− σ2 (X) c κ−2+150µ·40c kκ−2σ (X)2+500µ2·(∥X∥2+∥U ∥2)
4 min 2 1 min t
(cid:16) µ (cid:17) √
≤ 1− σ2 (X) c κ−2+6000µc kκ−2σ (X)2+500µ2·(∥X∥2+9∥X∥2)2
4 min 2 1 min
(cid:16) µ (cid:17) √
= 1− σ2 (X) c κ−2+6000µc kκ−2σ (X)2+50000µ2∥X∥4
4 min 2 1 min
(cid:16) µ (cid:17) √
≤ 1− σ2 (X) c κ−2+6000µc kκ−2σ (X)2+50000µ·c κ−4∥X∥−2·∥X∥4
4 min 2 1 min 1
(cid:16) µ (cid:17) √
= 1− σ2 (X) c κ−2+6000µc kκ−2σ (X)2+50000µ·c κ−4∥X∥2
4 min 2 1 min 1
(cid:16) µ (cid:17) √
= 1− σ2 (X) c κ−2+6000µc kκ−2σ (X)2+50000µ·c κ−4κ2σ (X)2
4 min 2 1 min 1 min
(cid:16) µ (cid:17) √
= 1− σ2 (X) c κ−2+56000µc kκ−2σ (X)2
4 min 2 1 min
Here,wehaveagainusedtheinductiveassumptionsalongwiththefactthatκ=∥X∥/σ (X). Ifwechoose
min
c sufficiently small, we will have ∥V⊤ ∗V ∥≤c κ−2.
1 X⊥ Ut+1∗Wt+1 2
Therefore, the four bullet points hold for t+1, and thus, the induction is complete.
With the above bullet points in mind, we note that
1 1
(cid:18)
1
(cid:19)t1−t∗
√ σ (X)≥σ (V⊤ ∗U )≥ γ 1+ µσ (X)2 ,
10 min min X t1 2 8 min
and so,
(cid:18) (cid:19)
2
log √ σ (X)
min (cid:18) (cid:19)
γ 10 16 2
t −t ≤ ≤ log √ σ (X) ,
1 ∗ log(cid:18)
1+
1
µσ
(X)2(cid:19) µσ min(X)2 γ 10 min
8 min
44where we have used the inequality 1 ≤ 2 for 0<x<1. Furthermore, we can bound the norm of the
log(1+x) x
signal term at iteration t by
1
∥U ∗W
∥≤2γ(cid:16)
1+80µc
√
kσ
(X)2(cid:17)t1−t∗
t1 t1,⊥ 2 min
(cid:18)
2 σ
(X)(cid:19)1280c2
≤2γ √ · min
10 γ
(cid:18)
2 σ
(X)(cid:19)1/64
≤2γ √ · min
10 γ
≤3γ63/64σ (X)1/64
min
≤3γ7/8σ (X)1/8,
min
wherewehaveusedthepreviousboundont −t , thefactthatc >0canbechosentobesufficientlysmall,
1 ∗ 2
and the fact that σ (X)≥γ.
min
Next, we set
(cid:22) 300 (cid:18) 5 (cid:114) r ∥X∥7/4(cid:19)(cid:23)
t =t + ln κ1/4
2 1 µσ (X)2 18 k(min{n,R}−r) γ7/4
min
t
=min(cid:110)
t≥t
:(cid:16)(cid:112) k(min{n,R}−r)+1(cid:17)(cid:13)
(cid:13)U ∗W ∗W⊤
∗U⊤(cid:13)
(cid:13) ≥∥X ∗X⊤−U ∗U⊤∥
(cid:111)
3 1 (cid:13) t t,⊥ t,⊥ t (cid:13) t t F
F
(cid:98)t=min{t 2,t 3}.
We now aim to show that over the range t
1
≤t≤(cid:98)t, the following inequalities hold:
1
• σ (U ∗W )≥σ (V⊤ ∗U )≥ √ σ (X)
min t t min X t min
10
• ∥U ∗W
∥≤(cid:16)
1+80µc
√
kσ
(X)2(cid:17)t−t1
∥U ∗W ∥
t t,⊥ 2 min t1 t1,⊥
• ∥U ∥≤3∥X∥
t
• ∥V⊤ ∗V ∥≤c κ−2
X⊥ Ut∗Wt 2
√
• ∥V⊤ ∗(X ∗X⊤−U ∗U⊤)∥ ≤10 kr(cid:0) 1− 1 µσ (X)2(cid:1)t−t1∥X∥2
X t t F 400 min
For t=t , the first four bullet points follow from what we previously proved via induction. The last one
1
holds since we trivially have
√
∥V⊤ ∗(X ∗X⊤−U ∗U⊤)∥ ≤ kr∥V⊤ ∗(X ∗X⊤−U ∗U⊤)∥
X t1 t1 F
√
X t1 t1
≤ kr∥X ∗X⊤−U ∗U⊤∥
√
√t1 t1
≤ kr∥X ∗X⊤∥+ kr∥U ∗U⊤∥
√ √
t1 t1
≤ kr∥X∥2+ kr∥U ∥2
√
t1
≤10 kr∥X∥2.
Now suppose all the bullet points hold for some integer t ∈ [t 1,(cid:98)t−1]. Again, we aim to show they all
hold for t+1. In a similar manner as done before, we can bound ∥(A∗A−I)(X ∗X⊤ −U ∗U⊤)∥ ≤
√ √ t t
10δ kr∥X∥2+δ k(min{n,R}−r)∥U ∗W ∥2, and then continue as follows
t t,⊥
45∥(A∗A−I)(X ∗X⊤−U ∗U⊤)∥
t t
√ √
≤10δ kr∥X∥2+δ k(min{n,R}−r)∥U ∗W ∥2
t t,⊥
≤10·
c
√1
·√
kr·κ2σ
(X)2+δ√ k(min{n,R}−r)(cid:16)
1+80µc
√
kσ
(X)2(cid:17)2(t−t1)
∥U ∗W ∥2
κ4 r min 2 min t1 t1,⊥
≤10c
√
kκ−2σ
(X)2+δ√ k(min{n,R}−r)(cid:16)
1+80µc
√
kσ
(X)2(cid:17)2(t−t1)
·9γ7/4σ (X)1/4
1 min 2 min min
≤10c
√
kκ−2σ
(X)2+9δ√ k(min{n,R}−r)(cid:16)
1+80µc
√
kσ
(X)2(cid:17)2(t2−t1)
γ7/4σ (X)1/4
1 min 2 min min
√ √ (cid:18) 5 (cid:114) r ∥X∥7/4(cid:19)O(c2)
≤10c kκ−2σ (X)2+9δ k(min{n,R}−r) κ1/4 γ7/4σ (X)1/4
1 min 18 k(min{n,R}−r) γ7/4 min
√
≤40c kκ−2σ (X)2
1 min
where we have used the bounds δ ≤ κ4c√1 r, ∥X∥=κσ min(X), ∥U
t1
∗W t1,⊥∥≤3γ7/8σ min(X)1/8, along with
the inductive assumptions and the definition of t .
1
Next, we note that if σ (V⊤ ∗U )≤ 1σ (X), then we can use Lemma E.1 along with the inductive
min X t 2 min
assumptions to obtain
σ (U ∗W )≥σ (V⊤ ∗U )
min t+1 t+1 min X t+1
≥σ (V⊤ ∗U ∗W )
min X t+1 t
(cid:18) (cid:19)
1
≥σ (V⊤ ∗U ) 1+ µσ (X)2−µσ (V⊤ ∗U )2
min X t 4 min min X t
(cid:18) (cid:19)
1 1
≥σ (V⊤ ∗U ) 1+ µσ (X)2−µ· σ (X)2
min X t 4 min 4 min
=σ (V⊤ ∗U )
min X t
1
≥ √ σ (X)
min
10
Alternatively, if σ (V⊤ ∗U )≥ 1σ (X), then we can again use Lemma E.1 along with the inductive
min X t 2 min
assumptions and the fact that µ≤c κ−2∥X∥2 for sufficiently small c to obtain
1 1
σ (U ∗W )≥σ (V⊤ ∗U )
min t+1 t+1 min X t+1
≥σ (V⊤ ∗U ∗W )
min X t+1 t
(cid:18) (cid:19)
1
≥σ (V⊤ ∗U ) 1+ µσ (X)2−µσ (V⊤ ∗U )2
min X t 4 min min X t
≥
1
σ
(X)(cid:0)
1−µσ (U
)2(cid:1)
2 min min t
≥
1
σ
(X)(cid:0)
1−µ∥U
∥2(cid:1)
2 min t
≥
1
σ
(X)(cid:0) 1−9µ∥X∥2(cid:1)
2 min
≥
1
σ
(X)(cid:0)
1−9c
κ−2(cid:1)
2 min 1
1
≥ √ σ (X)
min
10
In either case, we have σ min(U t+1∗W t+1)≥σ min(V⊤
X
∗U t+1)≥ √1 10σ min(X).
46Again, since σ min(V⊤
X
∗U t+1∗W t)≥ √1 10σ min(X)>0, we have that V⊤
X
∗U t+1∗W
t
has fulltubal rank
with all invertible t-SVD singular tubes. Hence, by Lemma E.3, we again can bound
∥U ∗W
∥≤(cid:16)
1+80µc
√
kσ
(X)2(cid:17)t+1−t1
∥U ∗W ∥.
t+1 t+1,⊥ 2 min t1 t1,⊥
In the exact same way as before, we can use Lemma E.6 to establish ∥U ∥ ≤ 3∥X∥, and use Lemma E.7
t+1
to establish ∥V⊤ ∗V ∥≤c κ−2.
X⊥ Ut+1∗Wt+1 2
Tobound∥V⊤∗(X∗X⊤−U ∗U⊤ )∥ ,wewillaimtouseLemmaE.7. Bytheinductiveassumptions,
X t+1 t+1 F
wealreadyhave∥U t∥≤3∥X∥, σ min(U t∗W t)≥ √1 10σ min(X), and ∥V⊤ X⊥∗V Ut∗Wt∥≤c 2κ−2. Toderivethe
remaining condition of Lemma E.7, we first split
∥V⊤ ∗(I−A∗A)(X ∗X⊤−U ∗U⊤)∥
X F
=∥V⊤ ∗(I−A∗A)(X ∗X⊤−U ∗W W⊤∗U⊤−U ∗W W⊤ ∗U⊤)∥
X t t t t t t,⊥ t,⊥ t F
≤∥V⊤ ∗(I−A∗A)(X ∗X⊤−U ∗W ∗W⊤∗U⊤)∥ +∥V⊤ ∗(I−A∗A)(U ∗W ∗W⊤ ∗U⊤)∥ .
X t t t t F X t t,⊥ t,⊥ t F
To bound the first term, we note that X ∗X⊤−U ∗W ∗W⊤∗U⊤ is tubal-symmetric with tubal rank
t t t t
at most 2r, so we can write it as the sum of two tubal-symmetric tensors Z ,Z ∈Sn×n×k with tubal rank
1 2
at most r, and then apply Lemma G.4 to obtain
∥V⊤ ∗(I−A∗A)(X ∗X⊤−U ∗W ∗W⊤∗U⊤)∥ =∥V⊤ ∗(I−A∗A)(Z +Z )∥
X t t t t F X 1 2 F
≤∥V⊤ ∗(I−A∗A)(Z )∥ +∥V⊤ ∗(I−A∗A)(Z )∥
X 1 F X 2 F
≤δ(∥Z ∥ +∥Z ∥ )
1 F 2 F
√
≤δ 2∥Z +Z ∥
1 2 F
√
=δ 2∥X ∗X⊤−U ∗W ∗W⊤∗U⊤∥
t t t t F
√
≤δ 2∥X ∗X⊤−U ∗U⊤∥
t t F
For the second piece, we use the symmetric t-SVD to write U ∗W ∗W⊤ ∗U⊤ = (cid:80) V ∗s ∗V⊤.
t t,⊥ t,⊥ t i i i i
Then, we can bound
(cid:13) (cid:32) (cid:33)(cid:13)
(cid:13) (cid:88) (cid:13)
∥V⊤ ∗(I−A∗A)(U ∗W ∗W⊤ ∗U⊤)∥ =(cid:13)V⊤ ∗(I−A∗A) V ∗s ∗V⊤ (cid:13)
X t t,⊥ t,⊥ t F (cid:13) X i i i (cid:13)
(cid:13) (cid:13)
i F
(cid:88)(cid:13) (cid:16) (cid:17)(cid:13)
≤ (cid:13)V⊤ ∗(I−A∗A) V ∗s ∗V⊤ (cid:13)
(cid:13) X i i i (cid:13)
F
i
(cid:88) (cid:13) (cid:13)
≤ δ(cid:13)V ∗s ∗V⊤(cid:13)
(cid:13) i i i (cid:13)
F
i
(cid:88)
= δ∥s ∥
i 2
i
(cid:13) (cid:13)
=δ(cid:13)U ∗W ∗W⊤ ∗U⊤(cid:13)
(cid:13) t t,⊥ t,⊥ t (cid:13)
∗
≤δ(cid:112) k(min{n,R}−r)(cid:13) (cid:13)U ∗W ∗W⊤ ∗U⊤(cid:13) (cid:13)
(cid:13) t t,⊥ t,⊥ t (cid:13)
F
≤∥X ∗X⊤−U ∗U⊤∥ ,
t t F
where we have used the fact that U ∗W ∗W⊤ ∗U⊤ has tubal rank ≤min{n,R}−r along with the
t t,⊥ t,⊥ t
definition of t .
3
Hence,
∥V⊤ ∗(I−A∗A)(X ∗X⊤−U ∗U⊤)∥
X F
≤∥V⊤ ∗(I−A∗A)(X ∗X⊤−U ∗W ∗W⊤∗U⊤)∥ +∥V⊤ ∗(I−A∗A)(U ∗W ∗W⊤ ∗U⊤)∥
X t t t t F X t t,⊥ t,⊥ t F
√
≤δ 2∥X ∗X⊤−U ∗U⊤∥ +δ∥X ∗X⊤−U ∗U⊤∥
t t F t t F
≤cκ−2∥X ∗X⊤−U ∗U⊤∥ ,
t t F
47where we have used the assumption that δ ≤ c√1 ≤cκ−2.
κ4 r
Similarly, we can bound
∥V⊤ ∗(I−A∗A)(X ∗X⊤−U ∗U )∥ ≤cκ−2∥X ∗X⊤−U ∗U⊤∥ ,
Ut∗Wt t t F t t F
and
∥(I−A∗A)(X ∗X⊤−U ∗U )∥≤cκ−2∥X ∗X⊤−U ∗U⊤∥ .
t t t t F
Then, by Lemma E.7, we have
(cid:16) µ (cid:17)
∥V⊤ (X ∗X⊤−U ∗U⊤ )∥ ≤ 1− σ2 (X) ∥V⊤ ∗(X ∗X⊤−U ∗U⊤)∥
X⊥ t+1 t+1 F 200 min X⊥ t t F
σ2 (X)
+µ min ∥U ∗W ∗W⊤ ∗U⊤∥
100 t t,⊥ t,⊥ t F
By the inductive assumption,
√
∥V⊤ ∗(X ∗X⊤−U ∗U⊤)∥ ≤10 kr(cid:0) 1− 1 µσ (X)2(cid:1)t−t1∥X∥2.
X⊥ t t F 400 min
Also, using the inductive assumption and the bound from the previous part, we can bound
∥U ∗W ∗W⊤ ∗U⊤∥ ≤(cid:112) k(min{n,R}−r)∥U ∗W ∗W⊤ ∗U⊤∥
t t,⊥ t,⊥ t F t t,⊥ t,⊥ t
(cid:112)
≤ k(min{n,R}−r)∥U ∗W ∥2
t t,⊥
≤(cid:112) k(min{n,R}−r)(cid:16) 1+80µc √ kσ (X)2(cid:17)2(t−t1) ∥U ∗W ∥2
2 min t1 t1,⊥
≤(cid:112) k(min{n,R}−r)(cid:16)
1+80µc
√
kσ
(X)2(cid:17)2(t−t1)
·9γ7/4σ (X)1/4
2 min min
Since t≤t , we have
2
300 (cid:18) 5 (cid:114) r ∥X∥7/4(cid:19)
t−t ≤t −t ≤ √ ln κ1/4 ,
1 2 1 µ kσ (X)2 18 min{n,R}−r γ7/4
min
and thus,
∥U ∗W ∗W⊤ ∗U⊤∥ ≤(cid:112) k(min{n,R}−r)(cid:16) 1+80µc √ kσ (X)2(cid:17)2(t−t1) ·9γ7/4σ (X)1/4
t t,⊥ t,⊥ t F 2 min min
≤
5√ kr(cid:16)
1−
µ
σ
(X)2(cid:17)t−t1
∥X∥2.
2 400 min
Combining these inequalities yields
(cid:16) µ (cid:17)
∥V⊤ (X ∗X⊤−U ∗U⊤ )∥ ≤ 1− σ2 (X) ∥V⊤ ∗(X ∗X⊤−U ∗U⊤)∥
X⊥ t+1 t+1 F 200 min X⊥ t t F
σ2 (X)
+µ min ∥U ∗W ∗W⊤ ∗U⊤∥
100 t t,⊥ t,⊥ t F
(cid:16) µ (cid:17) √ (cid:18) 1 (cid:19)t−t1
≤ 1− σ2 (X) ·10 kr 1− µσ (X)2 ∥X∥2
200 min 400 min
+µσ m2 in(X)
·
5√ kr(cid:16)
1−
µ
σ
(X)2(cid:17)t−t1
∥X∥2
100 2 400 min
√ (cid:18) 1 (cid:19)t+1−t1
≤10 kr 1− µσ (X)2 ∥X∥2
400 min
Hence, by induction, the five bullet points hold for t+1.
48If (cid:98)t=t 2, then, we can use Lemma E.7, the previous bullet points, and the definition of t
2
to bound
∥X ∗X⊤−U ∗U⊤∥ ≤4∥V⊤ ∗(X ∗X⊤−U ∗U⊤)∥ +∥U ∗W ∗W⊤ ∗U⊤∥
(cid:98)t (cid:98)t F X⊥ (cid:98)t (cid:98)t F (cid:98)t (cid:98)t,⊥ (cid:98)t,⊥ (cid:98)t F
√ (cid:18) 1 (cid:19)(cid:98)t−t1 5√ (cid:18) 1 (cid:19)(cid:98)t−t1
≤40 kr 1− µσ (X)2 ∥X∥2+ kr 1− µσ (X)2 ∥X∥2
400 min 2 400 min
85√ (cid:18) 1 (cid:19)(cid:98)t−t1
= kr 1− µσ (X)2 ∥X∥2
2 400 min
√ (cid:18) 5 (cid:114) r ∥X∥7/4(cid:19)−3/4
≲ kr κ1/4 ∥X∥2
18 k(min{n,R}−r) γ7/4
≲k5/4r1/8κ−3/16(min{n,R}−r)3/8γ21/16∥X∥11/16
If instead we have (cid:98)t=t 3, then
∥X ∗X⊤−U ∗U⊤∥
(cid:98)t (cid:98)t F
≤4∥V⊤ ∗(X ∗X⊤−U ∗U⊤)∥ +∥U ∗W ∗W⊤ ∗U⊤∥
X⊥ (cid:98)t (cid:98)t F (cid:98)t (cid:98)t,⊥ (cid:98)t,⊥ (cid:98)t F
≤4∥X ∗X⊤−U ∗U⊤∥ +∥U ∗W ∗W⊤ ∗U⊤∥
(cid:98)t (cid:98)t F (cid:98)t (cid:98)t,⊥ (cid:98)t,⊥ (cid:98)t F
≤4((cid:112) k(min{n,R}−r)+1)∥U ∗W ∗W⊤ ∗U⊤∥ +∥U ∗W ∗W⊤ ∗U⊤∥
(cid:98)t (cid:98)t,⊥ (cid:98)t,⊥ (cid:98)t F (cid:98)t (cid:98)t,⊥ (cid:98)t,⊥ (cid:98)t F
=4((cid:112) k(min{n,R}−r)+5)∥U ∗W ∗W⊤ ∗U⊤∥
(cid:98)t (cid:98)t,⊥ (cid:98)t,⊥ (cid:98)t F
≤4((cid:112) k(min{n,R}−r)+5)(cid:112) min{n,R}−r∥U ∗W ∗W⊤ ∗U⊤∥
(cid:98)t (cid:98)t,⊥ (cid:98)t,⊥ (cid:98)t
(cid:112) (cid:112)
≤4( k(min{n,R}−r)+5) min{n,R}−r∥U ∗W ∥2
(cid:98)t (cid:98)t,⊥
≤4((cid:112) k(min{n,R}−r)+5)(cid:112) k(min{n,R}−r)(cid:16)
1+80µc
√
kσ
(X)2(cid:17)2((cid:98)t−t1)
∥U ∗W ∥2
2 min t1 t1,⊥
≤4((cid:112) k(min{n,R}−r)+5)(cid:112) k(min{n,R}−r)(cid:16)
1+80µc
√
kσ
(X)2(cid:17)2((cid:98)t−t1)
·9γ63/32σ (X)1/32
2 min min
(cid:18) 5 (cid:114) r ∥X∥7/4(cid:19)O(c2)
≲k(min{n,R}−r) κ1/4 γ63/32σ (X)1/32
18 k(min{n,R}−r) γ7/4 min
(cid:18) 5 (cid:114) r ∥X∥7/4(cid:19)O(c2) ∥X∥1/32
≲k(min{n,R}−r) κ1/4 γ21/16γ21/32
18 k(min{n,R}−r) γ7/4 κ1/32
(cid:18) 5 (cid:114) r ∥X∥7/4(cid:19)O(c2) (cid:18) ∥X∥ (cid:19)21/32 ∥X∥1/32
≲k(min{n,R}−r) κ1/4 γ21/16
18 k(min{n,R}−r) γ7/4 min{n,R}κ3 κ1/32
k(min{n,R}−r)(cid:18) 5 (cid:114) r ∥X∥7/4(cid:19)O(c2)
≲ κ1/4 γ21/16κ−2∥X∥11/16
min{n,R}21/32 18 k(min{n,R}−r) γ7/4
≲k5/4r1/8κ−3/16(min{n,R}−r)3/8γ21/16∥X∥11/16.
So in either case, we have
∥X ∗X⊤−U ∗U⊤∥ ≲k5/4r1/8κ−3/16(min{n,R}−r)3/8γ21/16∥X∥11/16,
(cid:98)t (cid:98)t F
and thus,
∥X ∗X⊤−U ∗U⊤∥
(cid:98)t (cid:98)t F ≲k5/4r1/8κ−3/16(min{n,R}−r)3/8γ21/16∥X∥−21/16.
∥X∥2
49Finally, by the definition of (cid:98)t, we have that
(cid:98)t−t
∗
≤t 2−t
∗
≤(t −t )+(t −t )
2 1 1 ∗
300 (cid:18) 5 (cid:114) r ∥X∥7/4(cid:19) 16 (cid:18) 2 (cid:19)
≤ √ ln κ1/4 + log √ σ (X)
µ kσ min(X)2 18 k(min{n,R}−r) γ7/4 µσ min(X)2 γ 10 min
(cid:18) (cid:26) (cid:27) (cid:19)
1 κr ∥X∥
≲ ln min 1,
µσ (X)2 k(min{n,R}−r) γ
min
F Proof of Main Result
Nowthatouranalysesofthespectralstageandtheconvergencestagearecomplete,wearereadytocombine
these pieces to obtain the proof of our main result. Since A satisfies RIP(2r+1,δ), by Lemma G.2, A also
√
satisfies S2SRIP(2r, 2krδ). Hence, E :=(I−A∗A)(X ∗X⊤) satisfies
√ √ √
∥E∥=∥(I−A∗A)(X ∗X⊤)∥≤ 2krδ∥X ∗X⊤∥≤ 2kr·cκ−4r−1/2·∥X∥2 =c kκ−2σ (X)2.
min
Then, by applying Lemma D.9, with ϵ = 1e−3c˜, we have that with probability at least 1−k(C˜ϵ)R−2r+1−
C˜
kec˜r ≥1−ke−3c˜(R−2r+1)−kec˜r ≥1−ke−3c˜(3r−2r+1)−kec˜r =1−O(ke−c˜r), after
(cid:32) √ (cid:33)
1 2κ2 n
t ≲ ln
∗ µσ min(X)2 c˜ 3(cid:112) min{n;R}
iterations, we have
∥U ∥≤3∥X∥ (F.1)
t⋆
∥V ∗V ∥≤cκ−2. (F.2)
X⊥ Ut⋆∗Wt⋆
and for each 1≤j ≤k, we have
(cid:16) (cid:17) 1
σ U ∗W (j) ≥ αβ (F.3)
r t⋆ t⋆ 4
(cid:16) (cid:17) κ−2
σ U ∗W (j) ≤ αβ (F.4)
1 t⋆ t⋆,⊥ 8
(F.5)
where (since R≥3r and ϵ is a constant),
(cid:32) √ (cid:33)16κ2
√ √ 2κ2 n
k ≲β ≲ k .
(cid:112)
c˜ min{n;R}
3
By choosing
(cid:32) √ (cid:33)−16κ2
4c σ (X) 2κ2 n
α≲ 2 min √ ,
(cid:112)
κ2min{n,R} k c˜ min{n,R}
3
we have
1 c σ (X)
γ = αβ ≲ 2 min .
4 κ2min{n,R}
Also, κ−2αβ = 1 γ ≤2γ holds. Therefore, we can apply Theorem E.1, which gives us that after
8 2κ2
(cid:18) (cid:26) (cid:27) (cid:19)
1 κr ∥X∥
(cid:98)t−t
∗
≲
µσ (X)2
ln min 1,
k(min{n,R}−r) γ
min
50iterations beyond the first phase, we have
∥U ∗U⊤−X ∗X⊤∥
(cid:98)t (cid:98)t F ≲k5/4r1/8κ−3/16(min{n,R}−r)3/8γ21/16∥X∥−21/16.
∥X∥2
The total amount of iterations is then bounded by
(cid:98)t=t ∗+((cid:98)t−t ∗)
1
(cid:32) 2κ2√
n
(cid:33)
1
(cid:18) (cid:26)
κr
(cid:27) ∥X∥(cid:19)
≲ ln + ln min 1,
µσ min(X)2 c˜ 3(cid:112) min{n,R} µσ min(X)2 k(min{n,R}−r) γ
1
(cid:32) 2κ2√
n
(cid:26)
κr
(cid:27) ∥X∥(cid:33)
≲ ln ·min 1,
µσ min(X)2 c˜ 3(cid:112) min{n,R} k(min{n,R}−r) γ
1
(cid:32) 2κ2√
n
(cid:26)
κr
(cid:27) 4∥X∥(cid:33)
≲ ln ·min 1,
µσ min(X)2 c˜ 3(cid:112) min{n,R} k(min{n,R}−r) αβ
(cid:18) (cid:26) (cid:27) (cid:19)
1 C κn κr ∥X∥
≲ ln 1 ·min 1, ,
µσ (X)2 min{n,R} k(min{n,R}−r) kα
min
√
where we have used the choice of γ = 1αβ and the fact that β ≳ k. Finally, the error is bounded by
4
∥U ∗U⊤−X ∗X⊤∥
(cid:98)t (cid:98)t F ≲k5/4r1/8κ−3/16(min{n,R}−r)3/8γ21/16∥X∥−21/16
∥X∥2
≲k5/4r1/8κ−3/16(min{n,R}−r)3/8(αβ)21/16∥X∥−21/16
(cid:32) 2κ2√
n
(cid:33)21κ2 (cid:18)
α
(cid:19)21/16
≲k5/4r1/8κ−3/16(min{n,R}−r)3/8k21/32
(cid:112)
c˜ min{n,R} ∥X∥
3
(cid:32)
C
κ2√
n
(cid:33)21κ2 (cid:18)
α
(cid:19)21/16
≲k61/32r1/8κ−3/16(min{n,R}−r)3/8 2 ,
(cid:112)
min{n,R} ∥X∥
as desired.
Remark: One could obtain similar results for the cases where r ≤ R < 2r and 2r ≤ R < 3r by choosing
the parameter ϵ∈(0,1) appropriately.
G Restricted Isometry Property
Inthissection,weshowthatameasurementoperatorwhichsatisfiesthestandardrestrictedisometryproperty
also satisfies two other variants of the restricted isometry property - a fact which we used in our analysis of
the convergence stage.
We say that a measurement operator A : Sn×n×k → Rm satisfies the spectral-to-spectral Restricted
Isometry Property of rank-r with constant δ > 0 (abbreviated S2SRIP(r,δ)) if for all tensors Z ∈ Sn×n×k
with tubal-rank ≤r,
∥(I−A∗A)(Z)∥≤δ∥Z∥.
We say that a measurement operator A : Sn×n×k → Rm satisfies the spectral-to-nuclear Restricted
Isometry Property with constant δ > 0 (abbreviated S2NRIP(δ)) if for all tensors Z ∈ Sn×n×k with tubal-
rank ≤r,
∥(I−A∗A)(Z)∥≤δ∥Z∥ .
∗
Lemma G.1. Suppose that A : Sn×n×k → Rm satisfies RIP(r +r′,δ) with 0 < δ < 1. Then, for any
Z,Y ∈Sn×n×k with rank(Z)≤r and rank(Y)≤r′, we have
|⟨(I−A∗A)(Z),Y⟩|≤δ∥Z∥ ∥Y∥ .
F F
51Proof. Let Y′ = ∥Z∥FY so that ∥Y′∥ = ∥Z∥ . Note that Z +Y′ ∈ Sn×n×k and Z −Y′ ∈ Sn×n×k both
∥Y∥F F F
havetubalrank≤r+r′. Then,byusingtheidentities∥a+b∥2−∥a−b∥2 =4⟨a,b⟩and∥a+b∥2+∥a−b∥2 =
2∥a∥2+2∥b∥2(whichbothholdoveranyinnerproductspace)alongwiththefactthatAsatisfiesRIP(r+r′,δ),
we have:
(cid:10) (I−A∗A)(Z),Y′(cid:11) =(cid:10) Z,Y′(cid:11) −(cid:10) A∗A(Z),Y′(cid:11)
=(cid:10) Z,Y′(cid:11) −(cid:10) A(Z),A(Y′)(cid:11)
=(cid:10) Z,Y′(cid:11) − 1 ∥A(Z +Y′)∥2+ 1 ∥A(Z −Y′)∥2
4 2 4 2
≤(cid:10) Z,Y′(cid:11) − 1 (1−δ)∥Z +Y′∥2 + 1 (1+δ)∥Z −Y′∥2
4 F 4 F
=(cid:10) Z,Y′(cid:11) − 1(cid:0) ∥Z +Y′∥2 −∥Z −Y′∥2(cid:1) + 1 δ(cid:0) ∥Z +Y′∥2 +∥Z −Y′∥2(cid:1)
4 F F 4 F F
= 1 δ(cid:0) ∥Z∥2 +∥Y′∥2(cid:1)
2 F F
=δ∥Z∥ ∥Y′∥
F F
In a similar manner, (cid:10) (I−A∗A)(Z),Y′(cid:11) ≥ −δ∥Z∥ F∥Y′∥ F. Hence, (cid:12) (cid:12)(cid:10) (I−A∗A)(Z),Y′(cid:11)(cid:12) (cid:12) ≤
δ∥Z∥ ∥Y′∥ . Then, since Y is a scalar multiple of Y′, we have
F F
|⟨(I−A∗A)(Z),Y⟩|= ∥∥ YY ′∥ ∥F
F
(cid:12) (cid:12)(cid:10) (I−A∗A)(Z),Y′(cid:11)(cid:12) (cid:12)≤ ∥∥ YY ′∥ ∥F Fδ∥Z∥ F∥Y′∥
F
=δ∥Z∥ F∥Y∥ F.
Lemma G.2. Suppose that A : Sn×n×k → Rm satisfies RIP(r+1,δ ), where 0 < δ < 1. Then, A also
√ 1 1
satisfies S2SRIP(r, krδ ).
1
Proof. Suppose Z ∈Sn×n×k has tubal-rank r. Since (I−A∗A)(Z) is symmetric, its t-SVD is of the form
(I−A∗A)(Z)=V ∗Σ ∗V⊤ .
(I−A∗A)(Z) (I−A∗A)(Z) (I−A∗A)(Z)
√
Now, define V = V (I−A∗A)(Z)(:,1,:) ∈ Rn×1×k and let s ∈ R1×1×k be defined by s(1,1,ℓ) = √1 ke −12πjℓ
(cid:12)(cid:68) (cid:69)(cid:12)
where j = arg max j′|Σ(cid:98)(1,1,j′)|. With this definition, one can check that (cid:12)
(cid:12)
(I−A∗A)(Z),V ∗s∗V⊤ (cid:12)
(cid:12)
=
∥(I −A∗A)(Z)∥. Then, since A satisfies RIP(r+1,δ ) and rank(Z) ≤ r and rank(V ∗s∗V⊤) = 1, by
1
Lemma G.1, we have
(cid:12)(cid:68) (cid:69)(cid:12)
∥(I−A∗A)(Z)∥=(cid:12) (I−A∗A)(Z),V ∗s∗V⊤ (cid:12)
(cid:12) (cid:12)
≤δ ∥V ∗s∗V⊤∥ ∥Z∥
1 F F
=δ ∥Z∥
1 F
√
≤δ kr∥Z∥.
1
√
Since the bound ∥(I−A∗A)(Z)∥≤δ kr∥Z∥ holds for any Z ∈Sn×n×k with tubal rank ≤r, we have
√ 1
that A satisfies S2SRIP(r, krδ ).
1
Lemma G.3. Suppose that A:Sn×n×k →Rm satisfies RIP(2,δ ) where 0<δ <1. Then, A also satisfies
√ 2 2
S2NRIP( kδ ).
2
√
Proof. Since A satisfies RIP(2,δ ), by Lemma G.2 for r = 1, A satisfies S2SRIP(1, kδ ). Now, suppose
2 2
that Z ∈Sn×n×k. Since Z is symmetric, it has a t-SVD in the form
n
(cid:88)
Z = V ∗s ∗V⊤.
i i i
i=1
52Then, since each term V ∗s ∗V⊤ is symmetric with tubal rank 1, we have
i i i
(cid:13) (cid:13) (cid:32) (cid:88)n (cid:33)(cid:13) (cid:13)
∥(I−A∗A)(Z)∥=(cid:13)(I−A∗A) V ∗s ∗V⊤ (cid:13)
(cid:13) i i i (cid:13)
(cid:13) (cid:13)
i=1
(cid:13) (cid:13)
(cid:13)(cid:88)n (cid:16) (cid:17)(cid:13)
=(cid:13) (I−A∗A) V ∗s ∗V⊤ (cid:13)
(cid:13) i i i (cid:13)
(cid:13) (cid:13)
i=1
(cid:88)n (cid:13) (cid:16) (cid:17)(cid:13)
≤ (cid:13)(I−A∗A) V ∗s ∗V⊤ (cid:13)
(cid:13) i i i (cid:13)
i=1
(cid:88)n √ (cid:13) (cid:13)
≤ kδ (cid:13)V ∗s ∗V⊤(cid:13)
2(cid:13) i i i (cid:13)
i=1
n √
(cid:88)
= kδ ∥s ∥
2 i
i=1
√
≤ kδ ∥Z∥
2 ∗
√
Since the bound ∥(I −A∗A)(Z)∥ ≤ kδ ∥Z∥ holds for any Z ∈ Sn×n×k, we have that A satisfies
√ 2 ∗
S2NRIP( kδ ).
2
Lemma G.4. Suppose A:Sn×n×k →Rm satisfies RIP(2r,δ ), where 0<δ <1, and V ∈Rn×r×k satisfies
3 3
V⊤∗V =I. Then, for any Z ∈Sn×n×k with rank(Z)≤r, we have
(cid:13) (cid:13)
(cid:13)V⊤∗[(I−A∗A)(Z)](cid:13) ≤δ ∥Z∥ .
(cid:13) (cid:13) 3 F
F
Proof. LetZ ∈Sn×n×k,andletY = V⊤∗[(I−A∗A)(Z)] ∈Rr×n×k. Trivially,∥Y∥ =1,andso,∥V∗Y∥2 =
(cid:68) (cid:69)
∥V⊤∗[(I−A∗A)(Z)]∥F F F
⟨V ∗Y,V ∗Y⟩= Y,V⊤∗V ∗Y =⟨Y,Y⟩=∥Y∥2 =1. Then, by using Lemma G.1, we have that
F
(cid:13) (cid:13) (cid:68) (cid:69)
(cid:13)V⊤∗[(I−A∗A)(Z)](cid:13) = V⊤∗[(I−A∗A)(Z)],Y
(cid:13) (cid:13)
F
=⟨[(I−A∗A)(Z)],V ∗Y⟩
≤δ ∥Z∥ ∥V ∗Y∥
3 F F
=δ ∥Z∥
3 F
H Properties of Aligned Matrix Subspaces
In this section, we collect some properties of matrices and their subspaces, useful for the proof of the results
in the tensor Fourier domain.
Lemma H.1. ([36]) For some orthogonal matrix X ∈Cn×r and some full-rank matrix Y ∈Cn×R consider
XHY =VΣWH, and the following decomposition of Y
Y =YWWH+YW WH (H.1)
⊥ ⊥
with its SVD decomposition Y =(cid:80)R σ u vH and the best rank-r approximation Y =(cid:80)r σ u vH . Then
i=1 i i i r i=1 i i i
if the distance between the column subspace of Y and the subspace spanned by the columns of X is small
r
enough, that is ∥XHV ∥≤ 1, then the decomposition (H.1) follows some low-rank approximation properties,
⊥ Yr 8
namely
∥XHV ∥≤7∥XHV ∥ (H.2)
⊥ YW ⊥ Yr
∥YW ∥≤2σ (Y). (H.3)
⊥ r+1
53Lemma H.2. For a matrix X ∈ Cn×r, r ≤ n, with its SVD-decomposition X = V Σ WH and some a
X X X
full-rank matrix Y ∈Cn×R, consider VHY =VΣWH, and the following decomposition of Y
X
Y =YWWH+YW WH. (H.4)
⊥ ⊥
Let matrix H ∈Cr×r be defined as
H =VH(Id+µZ)YW
X
with some Z ∈ Cn×n, parameter µ ≤ √1 3∥VHY∥−2 and ∥V ⊥HV YW∥ ≤ c
2
with sufficiently small constants
c ,c >0. Then H can be represented as follows
1 2
H =(Id+µΣ2 −µP +µP +µ2P )V YW(Id−µWHYHV VHYW)
X 1 2 3 X X X
with matrices P ,P ,P ∈Cr×r such that
1 2 3
P :=VHYYHV VH V (VV )−1(Id−µVHYYHV )−1
1 X X⊥ X⊥ YW YW X X
P :=VH(Z−XXH+YYH)V (VHV )−1(Id−µVHYWWHYHV )−1
2 X YW X YW X X
P :=Σ2 VHYW(Id−µWHYHV VHYW)−1WHYHV
3 X X X X X
with
∥P ∥≤4∥YW∥2∥V V ∥2
1 X⊥ YW
∥P ∥≤4∥Z−XXH+YYH∥
2
∥P ∥≤2∥X∥2∥YW∥2.
3
Moreover, it holds that
σ (H)≥(cid:0) 1+µσ2 (X)−µ∥P ∥−µ∥P ∥−µ2∥P ∥(cid:1) σ (VHY)(cid:0) 1−µσ2 (VHY)(cid:1) .
min min 1 2 3 min X min X
Proof. The proof of this Lemma follows from Lemma 9.1 in [36] by using an independent matrix Z ∈Cn×n
instead of the matrix A∗A(XXH−YYH), omitting the assumption ∥Y∥≤3∥X∥ and updating respectively
the transformation steps.
Lemma H.3. For a matrix X ∈ Cn×r, r ≤ n with its SVD-decomposition X = V Σ WH and some full-
X X X
rank matrix Y ∈Cn×R and Y =(Id+µZ)Y consider VHY =VΣWH, VHY =V Σ WH, and the following
1 X X 1 1 1 1
decomposition of Y and Y
1
Y =YWWH+YW WH.
⊥ ⊥
Y =Y W WH+Y W WH .
1 1 1 1 1 1,⊥ 1,⊥
Assume that VHY W is invertible, which also implies that Y W is has full-rank, and that
X 1 1
(cid:110) (cid:111)
∥V XH ⊥V Y1W∥≤ 51
0
and µ ≤ min √1 3∥V XH ⊥YW ⊥∥−2,1 9∥X∥−2 and moreover, µ is small enough so that
0⪯Id−µVH YWWHYHV ⪯Id. Consider two matrices
X⊥ X⊥
G :=−VH Y W(VHY W)−1VHY W WHW
1 X⊥ 1 X 1 X 1 ⊥ ⊥ 1,⊥
G :=VH Y W WHW .
2 X⊥ 1 ⊥ ⊥ 1,⊥
Then these matrices can be represented as
G =µVH V (VHV )−1M VH YW WHW
1 X⊥ Y1W X Y1W 1 X⊥ ⊥ ⊥ 1,⊥
with M :=VH(ZV −XXHV ) and
1 X X⊥ X⊥
(cid:16) (cid:17)
G = Id−µM +µM )VH YW (Id−µWHYHYW )−µ2(M −M )VH YW WHYHYW ·
2 2 3 X⊥ ⊥ ⊥ ⊥ 2 3 X⊥ ⊥ ⊥ ⊥
·WHW
⊥ 1,⊥
54with M = VH YWWHYHV and M := VH (Z −(XXH−YYH))V . Moreover, the norm of G and
2 X⊥ X⊥ 3 X⊥ X⊥ 1
G can be bounded respectively as
2
∥G ∥≤2µ(∥VH V ∥∥YW∥2+∥Z−(XXH−YYH)∥)∥VH V ∥∥YW ∥,
1 X⊥ YW X⊥ Y1W ⊥
(cid:16) (cid:17)
∥G ∥≤∥YW ∥ 1−µ∥YW ∥2+µ∥Z−(XXH−YYH)∥
2 ⊥ ⊥
(cid:16) (cid:17)
+µ2 ∥YW∥2+∥Z−(XXH−YYH)∥ ∥YW ∥3.
⊥
Proof. The proof of this Lemma follows from Lemma 9.2 in [36] by changing the matrix A∗A(XXH−YYH)
to the independent matrix Z ∈ Cn×n and taking into account the respective changes without having the
condition ∥Y∥≤3∥X∥.
Lemma H.4. For a matrix X ∈ Cn×r, r ≤ n with its SVD-decomposition X = V Σ WH and some full-
X X X
rank matrix Y ∈Cn×R and Y :=(Id+µZ)Y consider VHY =VΣWH, VHY =V Σ WH, and the following
1 X X 1 1 1 1
decomposition of Y and Y
1
Y =YWWH+YW WH,
⊥ ⊥
Y =Y W WH+Y W WH .
1 1 1 1 1 1,⊥ 1,⊥
Then it holds that
(cid:18) ∥Z∥∥YW∥ (cid:19) ∥Z−(XXH−YYH)∥
∥WHW ∥≤µ 1+µ ∥YW∥∥YW ∥∥VH V ∥+µ ∥YW ∥ (H.5)
⊥ 1 σ (VHY) ⊥ X⊥ YW σ (VHY) ⊥
min X min X
Moreover, if for P :=YW WHW (VH YWWHW )−1VH the following applies
⊥ ⊥ 1 YW 1 YW
∥µZ+P +µZP∥≤1,
then it holds that
(cid:16) µ (cid:17)
∥VH V ∥≤∥VH V ∥ 1− σ2 (X)+µ∥YW ∥ +µ∥Z−(XXH−YYH)∥
X⊥ Y1W1 X⊥ YW 2 min ⊥
2∥WHW ∥∥YW ∥
+(1+µ∥Z∥) ⊥ 1 ⊥ (H.6)
σ (WHW )σ (YW)
min 1 min
(cid:18) ∥WHW ∥∥YW ∥ (cid:19)2
+57 µ∥Z∥+(1+µ∥Z∥) ⊥ 1 ⊥
σ (WHW )σ (YW)
min 1 min
Proof. The proof of inequality (H.5) follows from the first part of the proof of Lemma B.3 in [36]. For this
one needs to change the matrix A∗A(XXH −YYH) in [36] to an independent matrix Z ∈ Cn×n and take
intoaccounttheabove-givendecompositionofmatricesY andY andlackofassumptionsonµandthenorm
1
of matrix Z. Inequality (H.6) follows from the proof of Lemma 9.3 in [36].
I Random Tubal Tensors
In this section, we derive bounds on the minimum and maximum singular values as well as the Frobenius
norm of a random tubal tensor with i.i.d. Gaussian random entries. In our analysis of the spectral stage, we
applied these lemmas to the small random initialization.
We start with the following proposition from Rudelson and Vershynin (2009), which bounds the smallest
singular value of an r×R random real Gaussian matrix.
Proposition I.1 ([33]). Let G ∈ Rr×R with r ≤ R have i.i.d. N(0,1) entries. Then, for every ϵ > 0, we
have
√ √
σ (G)≥ϵ( R− r−1)
min
with probability at least 1−(Cϵ)R−r+1−e−cR. The constants C,c>0 are universal.
55Also, the following proposition from Tao and Vu (2010) bounds the smallest singular value of an r×r
random complex Gaussian matrix.
Proposition I.2 ([37]). Let G∈Rr×r have i.i.d. CN(0,1) entries. Then, for every ϵ>0, we have
ϵ
σ (G)≥ √
min r
with probability at least 1−ϵ2.
Using these propositions, we can obtain a bound on the smallest singular value of an r ×R random
complex Gaussian matrix, provided that r ≤R.
Lemma I.1. Let G∈Cr×R with r ≤R have i.i.d. CN(0,1) entries. Then, for every ϵ>0, we have
 √ √
ϵ( R− 2r−1) if R>2r
σ (G)≥ ϵ
min √ if r ≤R≤2r
 r
with probability at least
(cid:40)
1−(Cϵ)R−2r+1−e−cR if R>2r
.
1−ϵ2 if r ≤R≤2r
The constants C,c>0 are universal.
Proof. First, suppose R > 2r. Let G = UΣVH be the SVD of G where U ∈ Cr×r and V ∈ CR×R are
unitary and Σ∈Rr×R. Then, the following real 2r×2R matrix has a real SVD of
(cid:20) (cid:21) (cid:20) (cid:21)(cid:20) (cid:21)(cid:20) (cid:21)T
Re{G} −Im{G} Re{U} −Im{U} Σ 0 Re{V} −Im{V}
= .
Im{G} Re{G} Im{U} Re{U} 0 Σ Im{V} Re{V}
By using the fact that for any A∈Rp×q with p≤q, σ (A)2 = min ∥ATx∥2, we have
min x∈Rp 2
∥x∥2=1
(cid:18)(cid:20) (cid:21)(cid:19)2
Re{G} −Im{G}
σ (G)2 =σ
min min Im{G} Re{G}
(cid:13)(cid:34) (cid:35) (cid:13)2
(cid:13) Re{G}T Im{G}T (cid:13)
= min (cid:13) x(cid:13)
x∈R2r
(cid:13)
(cid:13)
−Im{G}T Re{G}T (cid:13)
(cid:13)
∥x∥2=1 2
(cid:20)(cid:13)(cid:104) (cid:105) (cid:13)2 (cid:13)(cid:104) (cid:105) (cid:13)2(cid:21)
= min (cid:13) Re{G}T Im{G}T x(cid:13) +(cid:13) −Im{G}T Re{G}T x(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
x∈R2r 2 2
∥x∥2=1
(cid:13)(cid:104) (cid:105) (cid:13)2 (cid:13)(cid:104) (cid:105) (cid:13)2
≥ min (cid:13) Re{G}T Im{G}T x(cid:13) + min (cid:13) Im{G}T Re{G}T x(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
x∈R2r 2 x∈R2r 2
∥x∥2=1 ∥x∥2=1
(cid:18)(cid:20) (cid:21)(cid:19)2 (cid:18)(cid:20) (cid:21)(cid:19)2
Re{G} −Im{G}
=σ +σ
min Im{G} min Re{G}
(cid:18)(cid:20) (cid:21)(cid:19)2
Re{G}
=2σ ,
min Im{G}
where the last line follows since reordering the rows of a matrix or flipping the sign of some rows doesn’t
change the singular values.
√ (cid:20) Re{G}(cid:21)
Since G ∈ Cr×R has i.i.d. CN(0,1) entries, 2 ∈ R2r×R has i.i.d. N(0,1) entries. Therefore,
Im{G}
by Proposition I.1, we have that
(cid:18)√ (cid:20) Re{G}(cid:21)(cid:19) √ √
σ (G)≥σ 2 ≥ϵ( R− 2r−1)
min min Im{G}
56with probability at least 1−(Cϵ)R−2r+1−e−cR, as desired.
Next, suppose r ≤R≤2r. Let G be an r×r submatrix of G. Then,
r×r
σ (G)2 = min ∥GHx∥2 ≥ min ∥GH x∥2 =σ (G )2.
min x∈Cr 2 x∈Cr r×r 2 min r×r
∥x∥2=1 ∥x∥2=1
Hence, by Proposition I.2, we have
ϵ
σ (G)≥σ (G )≥ √
min min r×r r
with probability at least 1−ϵ2.
Using the above lemma, we can bound the smallest singular value of an r×R×k tubal tensor.
Lemma I.2. Let G ∈Rr×R×k with r ≤R have i.i.d. N(0, 1) entries. Then, for every ϵ>0, we have
R
√ √ √

ϵ k( R− 2r−1)
 √ if R>2r
σ (G)≥ √ R
min ϵ k
√
if r ≤R≤2r
rR
with probability at least
(cid:40)
1−k(Cϵ)R−2r+1−ke−cR if R>2r
.
1−kϵ2 if r ≤R≤2r
Proof. Since the entries of G are i.i.d. N(0, 1), the entries of G(cid:101) are i.i.d. CN(0, k). Hence, each scaled slice
R R
(cid:113) RG(cid:101)(j) ∈Cr×R for j =1,...,k has i.i.d. CN(0,1) entries. By Lemma I.1, each scaled slice satisfies
k
 √ √
(cid:18)(cid:113) (j)(cid:19) ϵ( R− 2r−1) if R>2r
σ
min
R kG(cid:101) ≥ √ϵ
if r ≤R≤2r
 r
with probability at least
(cid:40)
1−(Cϵ)R−2r+1−e−cR if R>2r
.
1−ϵ2 if r ≤R≤2r
Then, by taking a union bound, we have that
√ √ √

ϵ k( R− 2r−1)
σ min(G)= 1m ≤ji ≤n kσ
min(cid:18) G(cid:101)(j)(cid:19) ≥
√ϵ√
k
√
R
i if
f
R
r
≤> R2r
≤2r
rR
with probability at least
(cid:40)
1−k(Cϵ)R−2r+1−ke−cR if R>2r
.
1−kϵ2 if r ≤R≤2r
The following proposition bounds the operator norm of an r×R random Gaussian matrix.
Proposition I.3 ([39]). Let U ∈ Cn×R have i.i.d. CN(0,1) entries. Then, with probability at least 1−
O(e−cmax{n,R}), we have
(cid:112)
∥U∥≲ max{n,R}.
Using the above proposition, we can bound the norm of an n×R×k random Gaussian tubal tensor.
57Lemma I.3. Let U ∈ Rn×R×k have i.i.d. N(0, 1) entries. Then, with probability at least 1 −
R
O(ke−cmax{n,R}), we have
(cid:114)
kmax{n,R}
∥U∥≲ .
R
Proof. Since the entries of U are i.i.d. N(0, 1), the entries of U(cid:101) are i.i.d. CN(0, k). Hence, each scaled slice
R R
(cid:113) RU(cid:101)(j) ∈Cr×R for j =1,...,k has i.i.d CN(0,1) entries. By Proposition I.3, each scaled slice satisfies
k
(cid:13)(cid:113) (cid:13)
(cid:13)
(cid:13)
RU(cid:101)(j)(cid:13) (cid:13)≲(cid:112)
max{n,R}
(cid:13) k (cid:13)
with probability at least 1−O(e−cmax{n,R}). Then, by taking a union bound, we have that
(cid:13) (cid:13) (cid:114)
∥U∥= max (cid:13) (cid:13)U(cid:101)(j)(cid:13) (cid:13)≲ kmax{n,R}
1≤j≤k(cid:13) (cid:13) R
with probability at least 1−O(ke−cmax{n,R}).
Lemma I.4. Let U ∈Rn×R×k have i.i.d. N(0, 1) entries. Then, for any fixed V ∈Rn×1×k with ∥V ∥=1,
R 1 1
we have √
∥U⊤∗V ∥ ≍ k
1 F
with probability at least 1−O(ke−cR).
Proof. Since the entries of U are i.i.d. N(0, 1), the entries of U(cid:101) are i.i.d. CN(0, k), and thus, the entries
R R
⊤
of U(cid:101) are also i.i.d. CN(0, k). Then, for each slice j = 1,...,k, each entry of the matrix-vector product
R
U(cid:103)⊤(j) V(cid:101)(j) ∈CR is i.i.d. CN(0, k∥V(cid:101)(j) ∥2). Hence, the quantity
1 R 1 F
(cid:13)
(cid:13)
(cid:13)U(cid:103)⊤(j) V(cid:101)(j)(cid:13)
(cid:13)
(cid:13)2
2R(cid:13) 1 (cid:13)
F
k (cid:13) (cid:13)2
(cid:13) (j)(cid:13)
(cid:13)V(cid:101) (cid:13)
(cid:13) 1 (cid:13)
F
has a χ2(2R) distribution. It follows that
(cid:13)
(cid:13)
(cid:13)U(cid:103)⊤(j) V(cid:101)(j)(cid:13)
(cid:13)
(cid:13)2 ≍k(cid:13)
(cid:13)
(cid:13)V(cid:101)(j)(cid:13)
(cid:13)
(cid:13)2
(cid:13) 1 (cid:13) (cid:13) 1 (cid:13)
F F
holds with probability at least 1−O(e−cR). By taking a union bound over all j =1,...,k, we get that
(cid:13) (cid:13) (cid:13)U⊤∗V 1(cid:13) (cid:13) (cid:13)2
F
= k1 (cid:13) (cid:13) (cid:13)U(cid:103)⊤⊙V(cid:101)1(cid:13) (cid:13) (cid:13)2
F
= k1 (cid:88) j=k 1(cid:13) (cid:13) (cid:13) (cid:13)U(cid:103)⊤(j) V(cid:101)( 1j)(cid:13) (cid:13) (cid:13) (cid:13)2
F
≍(cid:88) j=k 1(cid:13) (cid:13) (cid:13) (cid:13)V(cid:101)( 1j)(cid:13) (cid:13) (cid:13) (cid:13)2
F
=(cid:13) (cid:13) (cid:13)V(cid:101)1(cid:13) (cid:13) (cid:13)2
F
=k∥V 1∥2
F
=k,
√
i.e., ∥U⊤∗V ∥ ≍ k with probability at least 1−O(ke−cR).
1 F
58