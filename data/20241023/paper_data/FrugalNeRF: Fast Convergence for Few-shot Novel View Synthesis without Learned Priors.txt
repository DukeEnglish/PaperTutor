FrugalNeRF:
Fast Convergence for Few-shot Novel View Synthesis without Learned Priors
Chin-YangLin1* Chung-HoWu1* Chang-HanYeh1
Shih-HanYen1 ChengSun2 Yu-LunLiu1
1NationalYangMingChiaoTungUniversity 2NVIDIAResearch
https://linjohnss.github.io/frugalnerf/
FrugalNeRF (Ours) SimpleNeRF SparseNeRF FSGS
Figure1.ComparisonsbetweenFrugalNeRFandstate-of-the-artmethodswithonlytwoviewsfortraining.SimpleNeRF[71]suffers
fromlongtrainingtimes,SparseNeRF[81]producesblurryresults,andFSGS[102]qualitydropswithfewinputviews.OurFrugalNeRF
achievesrapid,robustvoxeltrainingwithoutlearnedpriors,demonstratingsuperiorefficiencyandrealisticsynthesis.Itcanalsointegrate
pre-trainedpriorsforenhancedquality.Green:methodswithoutlearnedpriors.Orange:withlearnedpriors
Abstract methodswhilesignificantlyreducingtrainingtime,making
Neural Radiance Fields (NeRF) face significant chal- itapracticalsolutionforefficientandaccurate3Dscene
lenges in few-shot scenarios, primarily due to overfitting reconstruction.
andlongtrainingtimesforhigh-fidelityrendering.Existing
methods,suchasFreeNeRFandSparseNeRF,usefrequency 1.Introduction
regularizationorpre-trainedpriorsbutstrugglewithcom-
plexschedulingandbias.WeintroduceFrugalNeRF,anovel Few-shotnovelviewsynthesis,generatingnewviewsfrom
few-shot NeRF framework that leverages weight-sharing limitedimagery,posesasubstantialchallengeincomputer
voxelsacrossmultiplescalestoefficientlyrepresentscene vision.WhileNeuralRadianceFields(NeRF)[50]haverev-
details.Ourkeycontributionisacross-scalegeometricadap- olutionizedhigh-fidelity3Dscenerecreation,theydemand
tationschemethatselectspseudogroundtruthdepthbased considerablecomputationalresourcesandtime,oftenrelying
on reprojection errors across scales. This guides training onexternaldatasetsforpre-training.Thispaperintroduces
withoutrelyingonexternallylearnedpriors,enablingfull FrugalNeRF,anovelapproachtoaccelerateNeRFtraining
utilization of the training data. It can also integrate pre- in few-shot scenarios. It fully leverages the training data
trained priors, enhancing quality without slowing conver- without relying on external priors and markedly reduces
gence. Experiments on LLFF, DTU, and RealEstate-10K computationaloverhead.
show that FrugalNeRF outperforms other few-shot NeRF Traditional NeRF methods, despite producing high-
quality outputs, suffer from long training time and rely
*Authorscontributedequallytothepaper. on frequency regularization [91] via multi-layer percep-
1
4202
tcO
12
]VC.sc[
1v17261.0142:viXratrons(MLPs)andpositionalencoding,slowingconvergence
(ùê±,ùêù) P.E. MLP (ùêú,ùúé) (ùê±,ùêù)
(Fig. 2(a)). Alternatives like voxel upsampling (Fig. 2(b))
attempttoovercomethesechallengesbutstrugglewithgen-
eralizingtovariedscenes[10,74,75].Furthermore,using
(ùê±,ùêù) Training steps
pre-trainedmodels(Fig.2(c))createsdependenciesonex- Training steps
ternalpriors,whichmightnotbereadilyavailableorcould (a)Frequencyregularization (b)Voxelupsampling
introducebiasesfromtheirtrainingdatasets[53,61,81].
FrugalNeRFdiffersfromtheseapproachesbyincorpo- Pre-trained normalizing High freq.
rating a cross-scale, geometric adaptation mechanism, fa- Color flow model Color
Weight-
cilitatingrapidtrainingwhilepreservinghigh-qualityview sharing Depth
Pre-trained depth voxels
synthesis(Fig.2(d)).Ourmethodefficientlyutilizesweight- Depth model or depth sensor Low freq. Pseudo-GT
Cross-scale Geometric adaptation
sharing voxels across various scales to encapsulate the
(c)Pre-trainedmodels (d)FrugalNeRF(Ours)
scene‚Äôs frequency components. Our proposed adaptation
schemeprojectsrendereddepthsandcolorsfromdifferent
Figure2.Comparisonsbetweenfew-shotNeRFapproaches.(a)
voxelscalesontotheclosesttrainingviewtocomputerepro- Frequencyregularizationgraduallyincreasesthevisibilityofhigh-
jectionerrors.Themostaccuratescalebecomesthepseudo- frequencysignalsofpositionalencoding,butthetrainingspeed
groundtruthandguidesthetrainingacrossscales,thuselim- is slow. (b) Replacing the MLPs with voxels and incorporating
inatingtheneedforcomplexvoxelupsamplingschedules themwithgradualvoxelupsamplingachievessimilarfrequency
andenhancinggeneralizabilityacrossdiversescenes. regularization but cannot generalize well. (c) Some approaches
employpre-trainedmodelstosupervisetherenderedcolorordepth
FrugalNeRF significantly reduces computational de-
patches.(d)OurFrugalNeRF,leveragingweight-sharingvoxels
mandsandacceleratestrainingthroughself-adaptivemech- acrossscalesforvariousfrequenciesrepresentation,enhancedbya
anisms that exploit the multi-scale voxel structure, ensur- cross-scalegeometricadaptationforefficientsupervision.
ingquickconvergencewithoutcompromisingthesynthesis
quality.Byfullyleveragingthetrainingdataandeliminating
relianceonexternallylearnedpriorsandtheirinherentlimita- 2.RelatedWork
tions,FrugalNeRFprovidesapathwaytowardmorescalable
NeuralRadianceFields(NeRF)[50]excelsinsynthesizing
andefficientfew-shotnovelviewsynthesis.Inconclusion,
novel views of complex scenes [4, 14, 16, 22, 46, 55, 77,
FrugalNeRFefficientlybypassestheneedforexternalpre-
85,88,89,93,95,97,99].Incomputervisionand3Dscene
trainedpriorandcomplexschedulingforvoxel.
representation[18,27,68],numerousresearchworksfocus
We evaluate the FrugalNeRF‚Äôs effectiveness on three
onmulti-view3Dviewsynthesis[9,32,47,54,73,82,92],
prominentdatasets:LLFF[49],DTU[32],andRealEstate-
single view synthesis [24, 25, 78, 86, 87], 3D image gen-
10K[100]datasettoassessboththerenderingqualityand
eration [7, 8, 26, 42, 80], and dynamic 3D scene synthe-
convergencespeed.OurresultsshowthatFrugalNeRFisnot
sis [44, 51, 56]. Few-shot Neural Radiance Fields (Few-
onlyfasterbutalsoachievessuperiorqualityincomparison
shot NeRF) [12, 17, 28, 29] have gained interest in re-
toexistingmethods(Fig.1),showcasingFrugalNeRF‚Äôspro-
cent years, aiming to reconstruct 3D scenes from sparse
ficiencyingeneratingperceptuallyhigh-qualityimages.The
input[5,31,35,39,40,65,96,101].However,theyoften
maincontributionsofourworkare:
facechallengessuchasoverfittingtolimitedtrainingimages
‚Ä¢ Weintroduceanovelweight-sharingvoxelrepresentation orpoorgeneralizationtonovelviewpoints.Tomitigatethese
thatencodesmultiplefrequencycomponentsofthescene, issues, some approaches [30, 53, 81, 94] use pre-trained
significantlyenhancingtheefficiencyandqualityoffew- models, leveraging prior [20, 33] knowledge to improve
shotnovelviewsynthesis. NeRF‚Äôsabilityinsynthesizingunseenpointsormodelinga
‚Ä¢ Ourgeometricadaptationselectsaccuraterendereddepth bettergeometry[13,79]whileothersintroduceadditional
across different scales by reprojection errors to create regularizationtoimproveperformance [21,53,70,91].
pseudo geometric ground truth that guides the training
process,enablingarobustlearningmechanismthatisless Depth regularizations. Recent works emphasize depth
reliantoncomplexschedulingandmoreadaptabletovari- constraintsduringtraining.DS-NeRF[21]usessparsedepth
ousscenes. estimatedbyanSfMmodel,focusingsolelyonregularizing
‚Ä¢ Ourtrainingschemereliesonlyonavailabledata,elimi- sparsepoints.DDP-NeRF[61]extendsDS-NeRFbycom-
natingtheneedforexternalpriorsorpre-trainedmodels, pletingpretraineddepthpriorfromsparsedepth.SparseN-
andensuresfastconvergencewithoutlosingquality.Italso eRF [81] leverages dense prediction transformer [58, 59]
remainsflexible,allowinglearnedpriorstobeaddedfor toprovidegeneraldepthpriorsandperformdistillationon
betterqualitywithoutslowingdowntraining. spatialcontinuityanddepthranking.Da¬®RF[72]jointlyopti-
2
ycneuqerF
ezis
lexoVmizeNeRFandMDE,distillingthemonoculardepthprior Fastconvergence. OnecommonchallengeinNeRFisthe
toNeRFforbothseenandunseenviews.ReVoRF[90]im- time-consuming training process due to MLP queries. To
provesgeometricaccuracybyusingestimateddepthmaps mitigatethis,manymethods[10,11,67,74,75]aimatre-
withoutheavilyrelyingonpre-trainedpriors.FSGS[102] placingmostMLPswithrepresentationsthatconvergefaster
employsmonoculardepthpriorsandgeometricregulariza- intraining.Instant-NGP[52]usesvoxelscoupledwithhash
tionfrombothseenandunseenviewsforimprovedadap- encoding and density bitfield. DVGO [74] utilizes dense
tivedensitycontrol.Thesemethodsrelyonpre-trainedpri- voxelgridswithshallowMLPforfasttraining.Notably,Ten-
ors,whichmaycontainerrorscausedbydatabiasaffecting soRF[10]decomposesradiancefieldsintomultiplelow-rank
performanceandrequiresubstantialdatafortraining.ViP- tensors,addressingtheinefficiencyproblemofvoxelgrid
NeRF [70] introduces visibility maps derived from plane representations. ZeroRF [66] adapts TensoRF‚Äôs tensor de-
sweepvolumetofurtherregularizetheradiancefields.How- compositionforuseinfew-shotsettings,butitneedstotrain
ever,computingvisibilitypriorsdemandssignificanttime additionalfactorizedrepresentationsbyleveragingDeepIm-
andmaynotgeneralizewell.Incontrast,OurFrugalNeRF agePrior,whichslowerstheconvergence.OurFrugalNeRF
regularizesgeometrythroughgeometricallyadaptedpseudo- leveragesTensoRFforfasttrainingandintroducesacross-
GTdepth,eliminatingtheneedforpre-trainedmodelson scalegeometricadaptationweight-sharingvoxelframework.
largedatasetsoranextensivepriorcomputationtime.
Self-supervised consistency. Consistency modeling be-
Novelposeregularization. Limitedoverlappinginsparse tweensparseimagesandwarpedcounterpartsiscrucialfor
inputsoftencausesfloatersinsynthesizednovelviews.Reg- Few-shot NeRFs. Traditional methods [15, 19, 23] warp
NeRF[53]introducesnovelposesamplingwithanormaliz- images to minimize reprojection errors but struggle with
ingflowmodeltoregulatethecolorrenderingofunobserved extremelylimiteddata.SinNeRF[88]andPANeRF[1]use
viewpoints.PixelNeRF[94]usesCNNs[38]toextractin- theresultsofwarpingtounseenviewsaspseudolabelsto
put image features for scene priors guiding rendering of achievegeometricconsistencybutrequireRGB-Ddataasin-
unseenviews.DietNeRF[30]adoptsaCLIP-basedVision put.SE-NeRF[34]andSelf-NeRF[2]employtherendering
Transformer[6,41,43,57]toconstraintcolorconsistency resultsfromanadditionalteacherNeRFaslabels.GeCoN-
ofthescene.FlipNeRF[64]samplesflippedreflectionrays eRF [39] uses rendered depth for warping but requires a
fromtheestimatedsurfacenormalbuthighlyrelyonprede- pre-trainedfeatureextractor,whichleadstoslowertraining.
finedreflectionmasks.However,withoutgroundtruthfor Our FrugalNeRF combines frequency regularization with
thenovelviews,thesecolorconstraintsmostlydependon cross-scalegeometricadaptation,usingthebestrenderdepth
pre-trainedmodels,whichrequireadditionalinferencetime at different scales as a pseudo label to ensure geometric
andmayintroducegeneralizationbias.Ourworkappliesge- consistencywithoutrelyingonlearnedpriors.
ometricadaptationtonovelposerendering,avoidingusing
pre-trainedmodelswhilesuppressingnovelviewfloaters.
3.Method
Frequencyregularization. Positionalencoding[69,76, 3.1.Preliminaries
83] allows MLP-based NeRF to capture high-frequency
Neuralradiancefields. NeRF[50]usesaneuralnetwork
scene details by encoding low-dimensional input into a
f tomap3Dlocationxandviewingdirectiondtodensity
higher dimension. In few-shot scenarios, increasing input
œÉ and color c for image rendering: f : (x,d) ‚Üí (œÉ,c).
dimensions can lead to overfitting and geometric distor-
Thenweusethedensitiesandcolorstorenderapixelcolor
tions in rendered views. To address this, FreeNeRF [91]
CÀÜ(r) by integrating the contributions along a ray r cast
suggestsaschedulingmechanismforgraduallyincreasing
input frequency. For voxel-based methods, gradually up- through the scene: CÀÜ(r) = (cid:80)N i=1T i(1 ‚àí exp(‚àíœÉ iŒ¥ i))c i,
samplingvoxelsaidsradiancefieldsinavoidingoverfitting.
whereT(t)=exp(‚àí(cid:80)i‚àí1œÉ
Œ¥ )isthetransmittancealong
j=i j j
VGOS[75]adoptsanincrementalvoxeltrainingstrategyto theray,andN isthenumberofpointsalongtheray.NeRF
prevent overfitting by suppressing the optimization of pe- seekstominimizetheMSEbetweentherenderedimageand
ripheralvoxelsearlyinreconstruction.Bothmethodsrequire
the actual image: L =
(cid:80) (cid:13) (cid:13)CÀÜ(r)‚àíC(r)(cid:13) (cid:13)2
, where R
acomplexschedulingprocessandcannotgeneralizewell.
r‚ààR(cid:13) (cid:13)
denotesasetofrays.
SimpleNeRF[71]introducesaugmentedmodelsfocusingon
low-frequencyandhigh-frequencydetailsseparatelyduring
Voxel-basedNeRFs. Voxel-basedNeRFs[10,52,74]en-
training,necessitatingadditionalMLPforaugmentationand
hancecoloranddensityqueryingspeedintheradiancefield
leadingtoresourcewastageandextraoptimizationtime.In
byemployingvoxelgrids,allowingefficientdataretrieval
contrast,ourworkleveragesweight-sharingvoxelsacross
via trilinear interpolation. They typically utilize a logistic
scales for various frequency representations, avoiding the
functionwithabiastermfordensitycalculationandadopta
needforcomplexscheduling.
3Volume
AD pe pn es ai rt ay n & ce ùêï*,ùêï+ , rendering reV no dlu em rine g Warp
Voxels
(ùêï!,ùêï")
(a) Voxel-based
RGB ùê∂+,(ùê´!"#$%) Depth ùê∑3,(ùê´%&‚Äô()) RGB ùê∂+,(ùê´%&‚Äô()) Reprojection error ùëí,
representations
‚Üìs- Volume
ùêï*,ùêï+- rendering Volume Warp
rendering
ùê±%&‚Äô()
‚ãÆ
RGB ùê∂+-(ùê´!"#$%) Depth ùê∑3-(ùê´%&‚Äô()) RGB ùê∂+-(ùê´%&‚Äô()) Reprojection error ùëí-
ùê±!"#$% ‚ãÆ ‚ãÆ ‚ãÆ ‚ãÆ
‚Üìs.
ùêï*,ùêï+ .
reV no dlu em rine
g reV no dlu em rine g Warp
Rays from
nov ùê´e %l & v ‚Äôi (e )ws traR ia n ùê´y in !s
"
g #f $r v %o im ews (c) Multi-scale voxels RGB ùê∂+.(ùê´ M!" S# E$% l) osses Depth ùê∑3.( Mùê´% S& E‚Äô l( o)) sseR sGB ùê∂+.(ùê´%&‚Äô()) [ùëÖ|ùë°] Reprojection error ùëí.
(b) Rays sampling
Nearest training view
Input RGB ùê∂(ùê´!"#$%) Pseudo GT Depth
(d) Training view
ùê∑/(ùê´%&‚Äô())
reconstruction losses (e) Cross-scale geometric adaptation for sampled novel views
Figure3.OverviewofFrugalNeRFarchitecture.(a)OurFrugalNeRFrepresentsascenewithapairofdensityandappearancevoxels
(VD,VA).Forabettergraphicalillustration,weshowonlyonevoxelinthefigure.(b)Wesampleraysfromnotonlytraininginput
viewsr butalsorandomlysamplednovelviewsr .(c)WethencreateL+1multi-scalevoxelsbyhierarchicalsubsampling,where
train novel
lower-resolutionvoxelsensureglobalgeometryconsistencyandreduceoverfittingbutsufferfromrepresentingdetailedstructures,while
higher-resolutionvoxelscapturefinedetailsbutmaygetstuckinthelocalminimumorgeneratefloaters.(d)Fortheraysfromtrainingviews
r ,weenforceanMSEreconstructionlossbetweenthevolumerenderedRGBcolorCÀÜlandinputRGBCateachscale.(e)Weintroduce
train
across-scalegeometricadaptationlossfornovelviewraysr ,warpingvolume-renderedRGBtothenearesttrainingviewusingpredicted
novel
depth,calculatingprojectionerrorsel ateachscale,andusingthedepthwiththeminimumreprojectionerroraspseudo-GTfordepth
supervision.Thisadaptationinvolvesraysfrombothtrainingandnovelviews,thoughthefigureonlydepictsnovelviewraysforclarity.
coarse-to-finestrategy,refiningresultswithashallowMLP (Sec.3.3).Topreventoverfittinginfew-shotscenarios,we
forview-dependenteffects. employageometricadaptationtrainingstrategyforgeome-
tryregularization(Sec.3.4),alongwithnovelviewsampling
Few-shotNeRFs. Recentmethodsproposevariousstrate-
and additional regularization losses to minimize artifacts
giestoaddressthechallengeofunder-constrainedoptimiza-
(Sec. 3.5). FrugalNeRF‚Äôs training process integrates data
tionwithlimitedimages.Theseincluderegularizingvisible
frombothtrainingandsamplednovelviewsforrobustand
frequenciesinpositionalencoding[91](Fig.2(a)),expand-
accuratescenerepresentation(Sec.3.6).
ingvoxelrangesincrementally[75](Fig.2(b)),andutilizing
externalpriorslikepre-trainedmodelsforadditionalguid-
3.3.Weight-SharingMulti-ScaleVoxels
ance[81](Fig.2(c)).Ourapproach,FrugalNeRF,leverages
aweight-sharingvoxelacrossscalestocaptureaspectrum Addressingdatasparsityinfew-shotscenarios,weintroduce
of frequency components. It self-adapts by evaluating re- FrugalNeRF‚Äôsweight-sharingmulti-scalevoxels,whichare
projectionerrorswiththenearesttrainingview,enhancing crucialforbalancingfrequencycharacteristics.Inspiredby
scene generalization, and offering faster training without FreeNeRF[91],whichhighlightstheoverfittingchallenges
dependenceonpre-trainedmodels(Fig.2(d)). withhigh-frequencyinputs,oursystemadoptsavoxel-based
representation to manage frequency components. We em-
3.2.OverviewofFrugalNeRF
ployvariedresolutionvoxelssimilartoNeRF‚Äôspositional
FrugalNeRF introduces an efficient architecture for novel encoding[50],withlowerresolutionscapturingbroadscene
viewsynthesisfromsparseinputs,eliminatingtheneedfor outlinesandhigherresolutionsmodelingfinerdetails.
external priors such as pre-trained depth estimators. This Unlike methods such as VGOS [75], which starts with
novelapproachleveragesvoxel-basedNeRFs[10,52,74]to acoarsegeometryandprogressivelyrefinesdetails,ourap-
effectivelyestimate3Dgeometryandreducetrainingtime proachmaintainsgeneralizationwithoutintricatetuning.We
usinglimited2Dimages.Ourmethod‚Äôskeyfeatureishierar- constructmulti-scalevoxelsbydownsamplingfromasingle
chicalsubsamplingwithweight-sharingmulti-scalevoxels, densityandappearancevoxel,ensuringconsistentscenerep-
enablingsimultaneouscaptureofdiversegeometricdetails resentation(Fig.3(c)).Thistechniqueeffectivelybalances
4
Per-ray
argmindifferent frequency bands in the training pipeline without whereDÀÜlistherendereddepthfromthevoxelatscalel,and
increasingmodelsizeormemorydemands. l‚Ä≤denotesthescalewithminimumreprojectionerror:
Withmulti-scalevoxels,wecanfurtherutilizemulti-scale
l‚Ä≤(r )=argmin(el(r )). (4)
voxelcolorlosstoguidethetraining(Fig.3(d)),whichis train train
l
crucialforfew-shotscenariosinensuringabalancedrepre-
This pseudo-ground truth depth D‚Ä≤ is used to compute a
sentationofgeometryanddetail.Themulti-scalevoxelcolor
geometricadaptationloss,L (r ),anMSElossthaten-
lossisdefinedas: geo train
suresthemodelmaintainsscenegeometryeffectively,even
L =(cid:88)L (cid:88) (cid:13) (cid:13)CÀÜl(r )‚àíC(r )(cid:13) (cid:13)2 , (1) withoutexplicitdepthgroundtruth:
ms-color l=0rtrain‚ààRtrain(cid:13) train train (cid:13) L (r )=(cid:88)L (cid:88) (cid:13) (cid:13)DÀÜl(r )‚àíD‚Ä≤(r )(cid:13) (cid:13)2 . (5)
geo train (cid:13) train train (cid:13)
whereCÀÜl istherenderedcolorfromthevoxelatscalel,C
l=0rtrain‚ààRtrain
isthegroundtruthcolor,Listhenumberofscales,R We further define a threshold for reprojection error to
train
isasetofraysfromtrainingviews,andr isaraysam- determine the reliability of depth estimation. Specifically,
train
pledfromR .WecomputeaweightedaverageMSEloss we do not compute the loss of those pixels in which the
train
acrossscalestoensurecolorrenderingaccuracyateachscale, projectionerrorexceedsthispre-definedthreshold.Geomet-
enhancingoverallrobustnessandfidelity. ricadaptationiscriticalbyallowingthemodeltorefineits
understanding of the scene‚Äôs geometry in a self-adaptive
3.4.Cross-scalegeometricadaptation manner.
Ourcross-scalegeometricadaptationapproacheffectively 3.5.NovelViewRegularizations
addressesthechallengesoffew-shotscenariosbysupervis-
In few-shot scenarios, we extend geometric adaptation to
inggeometrywithoutgroundtruthdepthdata.Recognizing
novel views to address the limitations in areas with less
thediversefrequencyrepresentationbydifferentvoxelscales
overlap among training views (Fig. 3(e)). Our novel view
inascene,itisessentialtoidentifytheoptimalfrequency
samplingstrategyinvolvesaspiraltrajectoryaroundtrain-
bandforeachregionofthescene.
ingviews,promotingcomprehensivecoverageandmodel
Foreachrayfromatrainingviewi,wecomputedepth
robustness.IntheabsenceofgroundtruthRGBfornovel
valuesatmultiplescalesthroughvolumerenderingandthen views, we rely on rendered color CÀÜ for reprojection error
warp[37,42,45]viewi‚ÄôsinputRGBtothenearesttraining
calculation,similartoEq.(2)inSec.3.4,butfocusingon
viewj usingthesedepths.Thereprojectionerrorwithview
raysfromnovelviewsr :
novel
j‚ÄôsinputRGBdeterminesthemostsuitablescaleforeach
scenearea.Thedepthofthisscaleservesasapseudo-ground (cid:13) (cid:13)2
el(p )=(cid:13)CÀÜ (p )‚àíC (pl )(cid:13) . (6)
truth,guidingthemodelinmaintaininggeometricaccuracy n (cid:13) n n j n‚Üíj (cid:13)
acrossfrequencies(Fig.3(e)). Inthiscontext,p denotesapixelcoordinateinthesampled
n
Mathematically,forapixelp inatrainingframei,with novelframen,andpl representsthecoordinatesonits
i n‚Üíj
itsdepthDl(p )atscalelandcameraintrinsicK ,wecan nearesttrainingposej afterwarpingp atscalel.Thisre-
i i i n
liftp toa3Dpointxl,thentransformittoworldcoordi- projectionerrorhelpsrefinethemodel‚Äôsrenderingfornovel
i i
nate xl, and subsequently transform to frame j‚Äôs camera views.Foreachrayfromanovelview,similartoEqs.(3)
coordinate xl . This 3D point is then projected back to to(5),wefirstdeterminethescalewiththeminimumrepro-
i‚Üíj
2D in frame j, obtaining the pixel coordinate pl . Due jectionerror,thendetermineitspseudo-groundtruthdepth
i‚Üíj
to the space limit, we provide the details for reprojection andcalculategeometricadaptationloss:
calculationinthesupplementary.Wecalculatethereproject
errorel(p )usingtheRGBvaluesofframeiandj foreach
l‚Ä≤(r novel)=argmin(el(r novel)),D‚Ä≤(r novel)=DÀÜl‚Ä≤(rnovel)(r novel),
i l
scalel. (7)
el(p i)=(cid:13) (cid:13)C i(p i)‚àíC j(pl i‚Üíj)(cid:13) (cid:13)2 , (2) L (r )=(cid:88)L (cid:88) (cid:13) (cid:13)DÀÜl(r )‚àíD‚Ä≤(r )(cid:13) (cid:13)2 ,
geo novel (cid:13) novel novel (cid:13)
whereC andC aretheinputRGBimagesfromviewiand l=0rnovel‚ààRnovel
i j (8)
j,respectively.Forapixellocationpfromwhichthetraining
where R is the set of rays from sampled novel views,
viewrayr originates,wedenoteitsimplyasr .The novel
train train andr isasampledrayfromthesetR .
pseudo-groundtruthdepthforthispixelisthedepthatthe novel novel
Wecombinethislosswiththegeometricadaptationloss
scalewiththeminimumreprojectionerror:
fromtrainingviewstoenhancetheoveralltrainingprocess:
D‚Ä≤(r )=DÀÜl‚Ä≤(rtrain)(r ), (3) L =L (r )+L (r ). (9)
train train geo geo train geo novel
5Thisapproachofnovelviewsamplingandapplyingregu- Implementation details. We implement FrugalNeRF
larizationthroughreprojectionerrorcomputationiscritical basedontheTensoRF[10]andutilizetheofficialPyTorch
in training our model. It ensures that the model not only framework.ThelearningprocessisdrivenbyanAdamopti-
learnsfromthelimitedtrainingviewsbutalsoadaptstoand mizer[36],withaninitiallearningrateof0.08,whichdecays
accuratelyrendersnovelperspectives,therebyenhancingthe to0.002throughoutthetraining.Wesample120novelposes
overallperformanceandreliabilityofFrugalNeRF. along a spiraling trajectory around the training view and
setthebatchsizeforbothtrainingandnovelviewraysto
Additionalglobalregularizationlosses. Tofurtherim- 4,096. We utilize the pre-trained Dense Prediction Trans-
prove the geometry and reduce artifacts, we introduce an former(DPT)[60]togeneratemonoculardepthmapsfrom
additionalglobalregularizationlossL ,includingtotalvari- trainingviews.
reg
ationloss[10,75],patch-wisedepthsmoothnessloss[53], Eachsceneinourmodelistrainedfor5,000iterations.
L1sparsityloss[10],anddistortionloss[3,74].Theselosses
Fordifferentdatasets,weusespecificvoxelresolutions:6403
helpsmooththescenegloballyandsuppressartifactslike
forLLFFandRealEstate-10K,and3003fortheDTUdataset.
floatersandbackgroundcollapse. Additionally,ourmodelemploysavoxeldownsampleratio
withs=4,L=2(threelevelsofscaleintotal)toaccom-
3.6.TotalLoss modatevaryinglevelsofscenedetail.Moredetailscanbe
foundinthesupplementarymaterials.
ThetotallossforFrugalNeRF,essentialforaccuratescene
renderingfromsparseviews,combinesvariouscomponents: 4.1.Comparisons
color fidelity, geometric adaptation, global regularization,
andsparsedepthconstraints.Itisformulatedas: LLFF dataset. We compare FrugalNeRF to RegN-
eRF[53],DS-NeRF[21],DDP-NeRF[61],FreeNeRF[91],
L=L +Œª L +Œª L +Œª L . (10) ViP-NeRF [70], SimpleNeRF [71], GeCoNeRF‚Ä† [39],
ms-color geo geo reg reg sd sd
SparseNeRF [81], and FSGS [102]. Some use pre-trained
L isthemulti-scalevoxelcolorloss,crucialformain-
ms-color modelsorfrequencyregularization.Asshownin Tab.1,Fru-
taining color accuracy across different scales. L is the
geo galNeRFoutperformsthesemethodsinPSNRandLPIPS,
geometricadaptationloss,providinggeometricguidancein
withcomparableSSIM.Ourcross-scalegeometricadapta-
theabsenceofexplicitdepthinformation.L istheglobal
reg tiongeneralizesbetterthanfrequencyregularizationmethods
regularizationloss,addressingartifactsandinconsistencies
likeFreeNeRF.Integratingmonoculardepthregularization
inunseenareas.AndL isthesparsedepthloss[21],utiliz-
sd furtherimprovesqualitywhilemaintainingfastconvergence.
ingsparsedepthdataforabsolutescaleconstraintsderived
FrugalNeRFachievesanoptimalbalancebetweenquality
fromCOLMAP[62,63].
and training time (10 minutes). Qualitative comparisons
(Fig.4)showthatFrugalNeRFrenderssceneswithricher
4.Experiments
detailandsharperedgescomparedtoSparseNeRF‚Äôsblurry
Datasets&evaluationmetrics. Weconductexperiments results.FrugalNeRFmodelsscenegeometrymoresmoothly
on two datasets: LLFF [49], DTU [32], and RealEstate- and consistently than SimpleNeRF and FSGS, which suf-
10K [100]. For both datasets, we use the test sets defined ferfromfloatersandholes.TheseresultsdemonstrateFru-
bypixelNeRF[94]andViP-NeRF[70].Wefollowthesame galNeRF‚Äôs capability to model complex scenes with high
evaluation protocol as ViP-NeRF, including the train/test fidelity.
split.Specifically,thereare12scenes*inthetestsetsofthe
DTUdataset.Weassumethatcameraparametersareknown, DTU dataset. We compare FrugalNeRF with RegN-
whichisrelevantforapplicationswithavailablecalibrated eRF‚Ä° [53], FreeNeRF [91], ViP-NeRF [70], SimpleN-
cameras. We provide further details in the supplementary eRF[71],SparseNeRF[81],ZeroRF¬ß[66],andFSGS[102]
materials. onthedatasetpreprocessedbypixelNeRF[94].Tab.2shows
Wefollowtheestablishedevaluationprotocolsforcon- FrugalNeRFachievesstate-of-the-artperformanceinmost
sistency.Theexperimentsutilizethreeevaluationmetrics: cases, with the shortest training time. Qualitative compar-
PSNR, SSIM [84], and LPIPS [98]. While evaluating on isons (Fig. 5) demonstrate FrugalNeRF‚Äôs superior visual
DTU,wefollowSparseNeRF[91]toremovethebackground results,consistentlyrenderingfinedetails(e.g.,theblueelf‚Äôs
when computing metrics to alleviate the background bias
‚Ä†SinceGeCoNeRFdoesnotreleaseacompleteandexecutableimplementa-
reportedbyRegNeRF[53]andpixelNeRF[94].Addition-
tion,wetryourbesttomodifytheircodeandreproduceitsresults.
ally,weincludethetrainingtimewithasingleNVIDIARTX ‚Ä°RegNeRFrunsintoanout-of-memoryissueononeNVIDIARTX4090
4090GPUtoevaluatetheefficiencyofthemethods. GPU,sowecannotreportitsresultsontheDTUdataset
¬ßTheofficialZeroRFimplementationsamplesraysthatlieinobjectmasks
*There are 15 scenes in total in ViP-NeRF‚Äôs DTU test sets. However, duringtraining.Weremovethismaskedsamplingforfaircomparisons
COLMAPcanonlyrunsuccessfullyon12scenes. withothermethods.
6Table1.QuantitativeresultsontheLLFF[49]dataset.FrugalNeRFperformscompetitivelywithbaselinemethodsinextremefew-shot
settings,offeringshortertrainingtimewithoutrelyingonexternallylearnedpriors.Integratingmonoculardepthregularizationfurther
improvesqualitywhilemaintainingfastconvergence.ResultsdifferfromSimpleNeRF‚Äôspaperbutmatchitssupplementarydocument,aswe
evaluatefullimageswithoutvisibilitymasks.
Learned 2-view 3-view 4-view Training
Method Venue priors PSNR‚Üë SSIM‚Üë LPIPS‚Üì PSNR‚Üë SSIM‚Üë LPIPS‚Üì PSNR‚Üë SSIM‚Üë LPIPS‚Üì time‚Üì
DS-NeRF[21] CVPR22 - 16.93 0.51 0.42 18.97 0.58 0.36 20.07 0.61 0.34 3.5hrs
FreeNeRF[91] CVPR23 - 17.55 0.54 0.38 19.30 0.60 0.34 20.45 0.63 0.33 1.5hrs
ViP-NeRF[70] SIGGRAPH23 - 16.66 0.52 0.37 18.89 0.59 0.34 19.34 0.62 0.32 13.5hrs
SimpleNeRF[71] SIGGRAPHAsia23 - 17.57 0.55 0.39 19.47 0.62 0.33 20.44 0.65 0.31 9.5hrs
FrugalNeRF(Ours) - - 18.07 0.54 0.35 19.66 0.61 0.30 20.70 0.65 0.28 10mins
RegNeRF[53] CVPR22 normalizingflow 16.88 0.49 0.43 18.65 0.57 0.36 19.89 0.62 0.32 2.35hrs
DDP-NeRF[61] CVPR22 depthcompletion 17.19 0.54 0.39 17.71 0.56 0.39 19.19 0.61 0.35 3.5hrs
GeCoNeRF[39] ICML23 VGG19feature 15.83 0.45 0.52 17.44 0.50 0.47 19.14 0.56 0.42 4hrs
SparseNeRF[81] ICCV23 monoculardepth 18.02 0.52 0.45 19.52 0.59 0.37 20.89 0.65 0.34 1hrs
FSGS[102] ECCV24 monoculardepth 15.26 0.45 0.41 19.21 0.61 0.30 20.07 0.66 0.22 25mins
FrugalNeRF(Ours) - monoculardepth 18.26 0.55 0.35 19.87 0.61 0.30 20.89 0.66 0.26 11mins
FrugalNeRF (Ours) FrugalNeRF (Ours) SimpleNeRF GeCoNeRF SparseNeRF FSGS (Top) GT
(w/ mono. depth) (w/ VGG19 feat.) (w/ mono. depth) (w/ mono. depth) (Bottom) Inputs
Figure4.QualitativecomparisonsontheLLFF[49]datasetwithtwoinputviews.FrugalNeRFachievesbettersynthesisqualityand
coherentgeometricdepth.WealsoincludetheGTandoverlappedinputimagesforreference.
eyes)withoutnoticeableartifacts,unlikeothermethods.This Weight-sharing voxels. We compared the performance
showcasesFrugalNeRF‚Äôsabilitytomodelsceneswithsimple andmemoryusageofweight-sharingvoxelsagainstthree
backgroundseffectively. independentvoxels.Tab.4indicatesthatweight-sharingnot
onlyenhancesperformancebutalsoreducesthemodelsize.
4.2.AblationStudies Multi-scalevoxelcolorloss. Wedemonstratetheeffec-
tivenessofmulti-scalevoxelcolorlossL bycompar-
ms-color
Numberofscales. Weexaminetheeffectofdifferentnum- ing it to using color loss only on the largest scale (Tab. 4,
bersofscalesinTab.3.Theresultsshowthatbyincreasing Fig.7(Left)).Multi-scalelossimprovesrenderingandgeom-
thenumberofscales,weachievebetterrenderingquality.As etrybycapturingvariouslevelsofscenedetail.Withoutge-
therearemoredifferentresolutionsofvoxels,FrugalNeRF ometricadaptation,FrugalNeRFunderperformsFreeNeRF,
is more capable of representing different levels of details whichusesaschedulingmechanismforgraduallyincreasing
inthescenebygeometricadaptation.WeuseL=2inour inputfrequency.Ourvoxelgridrepresentationoffersfaster
experiments,whichindicatesthreescalesintotal,tostrikea trainingthanMLPsbutsacrificessomecontinuity.Thedis-
balancebetweenrenderingqualityandtrainingtime. crete nature of multi-scale voxel grids initially limits our
7
BGR
htpeD
BGR
htpeDTable2.QuantitativeresultsontheDTU [32]dataset.FurgalNeRFsynthesizesbetterimagesthanmostoftheotherbaselinesunder
extreme few-shot settings but with shorter training time and does not rely on any externally learned priors. Additionally, integrating
monoculardepthmodelregularizationfurtherimprovesqualitywhilemaintainingfastconvergence.WefollowSparseNeRF[81]toremove
thebackgroundwhencomputingmetrics.
Learned 2-view 3-view 4-view Training
Method Venue priors PSNR‚Üë SSIM‚Üë LPIPS‚Üì PSNR‚Üë SSIM‚Üë LPIPS‚Üì PSNR‚Üë SSIM‚Üë LPIPS‚Üì time‚Üì
FreeNeRF[91] CVPR23 - 18.05 0.73 0.22 22.40 0.82 0.14 24.98 0.86 0.12 1hrs
ViP-NeRF[70] SIGGRAPH23 - 14.91 0.49 0.24 16.62 0.55 0.22 17.64 0.57 0.21 2.2hrs
SimpleNeRF[71] SIGGRAPHAsia23 - 14.41 0.79 0.25 14.01 0.77 0.25 13.90 0.78 0.26 1.38hrs
ZeroRF[66] CVPR24 - 14.84 0.60 0.30 14.47 0.61 0.31 15.73 0.67 0.28 25mins
FrugalNeRF(Ours) - - 19.72 0.78 0.16 22.43 0.83 0.14 24.51 0.86 0.12 6mins
RegNeRF[53] CVPR22 normalizingflow - - - - - - - - - OOM
SparseNeRF[81] ICCV23 monoculardepth 19.83 0.75 0.20 22.47 0.83 0.14 24.03 0.86 0.12 30mins
FSGS[102] ECCV24 monoculardepth 16.82 0.64 0.27 18.29 0.69 0.21 20.08 0.75 0.16 20mins
FrugalNeRF(Ours) - monoculardepth 20.77 0.79 0.15 22.84 0.83 0.13 24.81 0.86 0.12 7mins
FrugalNeRF FrugalNeRF FreeNeRF SimpleNeRF ZeroRF SparseNeRF FSGS Ground Truth
(Ours) (w/ mono. depth) (w/ mono. depth) (w/ mono. depth)
Figure5.QualitativecomparisonsontheDTU[32]datasetwithtwoinputviews.FrugalNeRFachievesbettersynthesisquality.
Table 3. Comparison of a different number of scales on the that during the training, low-frequency components from
LLFFdataset. the low-resolution voxels first guide the coarse geometry.
Then,mid-frequencyandhigh-frequencycomponentsgradu-
#ofscales PSNR‚Üë SSIM‚Üë LPIPS‚Üì Time‚Üì allyincreasetheirproportionofservingaspseudo-ground
1(L=0) 15.22 0.46 0.43 6mins truth.Thisprocessissimilartothefrequencyregularization
2(L=1) 16.58 0.53 0.37 7mins viaMLPsbutinaself-adaptivemanner.Therefore,ourFru-
3(L=2) 18.07 0.54 0.35 10mins galNeRFcouldgeneralizebettertodiversesceneswithout
4(L=3) 18.08 0.54 0.36 15mins complextrainingscheduling.Fig.6(Right)furtherdemon-
stratesthatgeometricadaptationhelpsallscalesconvergeat
Table4.AblationofdifferentcomponentsontheLLFFdataset superiorqualities.
withtwoinputviews.
Weight-sharing Lms-color Lgeo rnovel PSNR‚Üë SSIM‚Üë LPIPS‚Üì Modelsize‚Üì Scenedependencyanalysisofthemulti-scalevoxels. We
- ‚úì ‚úì ‚úì 17.54 0.52 0.37 198.31MB
‚úì - ‚úì ‚úì 16.89 0.44 0.46 183.04MB analyze the scene dependency of the multi-scale voxels
‚úì ‚úì - ‚úì 15.97 0.49 0.41 183.04MB in Fig. 8. The results indicate that scenes with foliage ex-
‚úì ‚úì ‚úì - 17.84 0.52 0.36 183.04MB
‚úì ‚úì ‚úì ‚úì 18.07 0.54 0.35 183.04MB hibithigheractivationsinhigh-andmid-frequencyvoxels,
whiletexturelessscenesshowsignificantactivationsinlow-
frequencyvoxels.Thisconfirmsourapproach‚Äôsadaptability
qualitycomparedtoFreeNeRF.However,integratinggeo- todifferentsceneconfigurations.
metricadaptationsignificantlyenhancescoherenceacross
scales,effectivelyovercomingthislimitation.
Number of training views analysis. We plot the num-
Cross-scalegeometricadaptation. Tab.4showsthatthe ber of training views experiment in Fig. 9, demonstrating
performancedropsonallmetricswithoutgeometricadap- thatFrugalNeRFoutperformsTensoRFonsparseviews(2
tation loss L . Fig. 7(Mid) demonstrate that geometric to 8 views) and continues to lead as the number of views
geo
adaptation greatly suppresses floaters. Fig. 6(Left) shows increases.
818
0.45
0.40 16
0.35
14
0.30
0.25 12 High res. Mid res. Low res.
High res. Mid res. Low res. High res. w/o geo Mid res. w/o geo Low res. w/o geo
0.20
0% 20% 40% 60% 80% 100% 0% 20% 40% 60% 80% 100%
Training iterations Training iterations
Figure6.Cross-scalegeometricadaptationintraining.(Left)Intheearlytrainingphase,low-resolutionvoxelsprimarilyactaspseudo-
groundtruth,guidingthemodel‚Äôsgeometriclearning.Astraininggoeson,medium-andhigh-resolutionvoxelsincreasinglycontribute
torefiningscenegeometry.Thisadaptiveapproachenablesthemodeltoautonomouslytuneintoappropriatefrequenciesateachstage,
enhancingitsabilitytogeneralizeacrossvariousscenes.(Right)Withoutgeometricadaptation,allofthescalesresultinsub-optimal
solutions.Geometricadaptationdrivesconvergencetohigherqualityacrossallscales.
Without With Without With Without With
multi-scale color loss cross-scale geometric adaptation novel view regularizations
Figure7.Visualcomparisonsonablationstudies.(Left)Multi-scalecolorlosspreventsoverfittingandleadstoabetterresult.(Mid)
Geometric adaptation determines proper depth across scales via projection error and results in better geometry. (Right) Novel view
regularizationsprovideadditionalsupervisorysignalsfromnovelviewsandprovidehigh-fidelitygeometry.
25.0
22.5
20.0
High Mid. Low High Mid. Low High Mid. Low 17.5
15.0
12.5 FrugalNeRF(Ours)
TensoRF
10.0
2 4 6 8 10 12 14 16
The number of training views
Figure8.Scenedependencyanalysisofthemulti-scalevoxels. Figure9.Numberoftrainingviewsanalysis.FrugalNeRFsignifi-
Cross-scalegeometricadaptationcanadapttodiversescenes. cantlyoutperformsthebaseTensoRFonsparseviews.
Novelviewregularizations. Weevaluatedtheimpactof
novel view regularizations by omitting sample rays from
novelviewsr .Tab.4showsthatusingnovelviewrays stuckinlocalminima,resultinginincorrectgeometry.Novel
novel
andregularizationsimprovesrenderingquality.Fig.7(Right) viewregularizationsprovideadditionalguidance,preventing
illustratesthatwithouttheseregularizations,trainingmayget overfittingandimprovinggeometryaccuracy.
9
)%(TG-oduesp
sa
gnivres
fo
noitroporP
RNSP
RNSP4.3.Limitations [10] AnpeiChen,ZexiangXu,AndreasGeiger,JingyiYu,and
HaoSu. Tensorf:Tensorialradiancefields. InECCV,2022.
Few-shotNeRFreliesonaccuratecameraposesfortraining.
2,3,4,6,14,16
Inscenarioswithsignificantchangesinviewpointorsparse
[11] Bo-YuChen,Wei-ChenChiu,andYu-LunLiu. Improving
trainingviews,themodelmayfacechallengesingeneraliza-
robustnessforjointoptimizationofcameraposeanddecom-
tion.Althoughourmethodintroducesnovel-viewlossesto posed low-rank tensorial radiance fields. In AAAI, 2024.
dealwiththoseunseenregionsintrainingviews,itisstillan 3
issueforfew-shotNeRF. [12] DiChen,YuLiu,LianghuaHuang,BinWang,andPanPan.
Geoaug:Dataaugmentationforfew-shotnerfwithgeometry
5.Conclusion constraints. InECCV,2022. 2
[13] WeifengChen,ZhaoFu,DaweiYang,andJiaDeng. Single-
In this paper, we propose FrugalNeRF, a framework that
imagedepthperceptioninthewild. InNeurIPS,2016. 2
synthesizesnovelviewswithextremelyfewinputviews.To [14] XingyuChen,QiZhang,XiaoyuLi,YueChen,YingFeng,
speed up and regularize the training, we propose weight- XuanWang,andJueWang. Hallucinatedneuralradiance
sharingvoxelrepresentationacrossdifferentscales,repre- fieldsinthewild. InCVPR,2022. 2
sentingvaryingfrequenciesinthescene.Topreventoverfit- [15] ZhengChen,ChenWang,Yuan-ChenGuo,andSong-Hai
ting,weproposeageometricadaptationscheme,utilizing Zhang. Structnerf:Neuralradiancefieldsforindoorscenes
reprojection errors to guide the geometry across different withstructuralhints. IEEETPAMI,2023. 3
scalesbothintrainingandsamplednovelviews.FrugalNeRF [16] ZixuanChen,LingxiaoYang,Jian-HuangLai,andXiaohua
Xie. Cunerf:Cube-basedneuralradiancefieldforzero-shot
performs on par with existing state-of-the-art methods on
medicalimagearbitrary-scalesuperresolution. InICCV,
multipledatasetswithshortertrainingtimeanddoesnotrely
2023. 2
onanyexternallylearnedpriors.
[17] Julian Chibane, Aayush Bansal, Verica Lazova, and Ger-
ardPons-Moll. Stereoradiancefields(srf):Learningview
References
synthesisforsparseviewsofnovelscenes. InCVPR,2021.
[1] YoungChunAhn,SeokhwanJang,SungheonPark,Ji-Yeon 2
Kim,andNahyupKang. Panerf:Pseudo-viewaugmentation [18] AngelaDai,AngelXChang,ManolisSavva,MaciejHalber,
forimprovedneuralradiancefieldsbasedonfew-shotinputs. ThomasFunkhouser,andMatthiasNie√üner.Scannet:Richly-
arXivpreprintarXiv:2211.12758,2022. 3 annotated3dreconstructionsofindoorscenes. InCVPR,
[2] JiayangBai,LetianHuang,WenGong,JieGuo,andYanwen 2017. 2
Guo. Self-nerf:Aself-trainingpipelineforfew-shotneural [19] Franc¬∏oisDarmon,Be¬¥ne¬¥dicteBascle,Jean-Cle¬¥mentDevaux,
radiancefields. arXivpreprintarXiv:2303.05775,2023. 3 Pascal Monasse, and Mathieu Aubry. Improving neural
[3] JonathanTBarron,BenMildenhall,DorVerbin,PratulP implicitsurfacesgeometrywithpatchwarping. InCVPR,
Srinivasan,andPeterHedman. Mip-nerf360:Unbounded 2022. 3
anti-aliasedneuralradiancefields. InCVPR,2022. 6,16 [20] CongyueDeng,ChiyuJiang,CharlesRQi,XinchenYan,
[4] WenjingBian,ZiruiWang,KejieLi,Jia-WangBian,and Yin Zhou, Leonidas Guibas, Dragomir Anguelov, et al.
Victor Adrian Prisacariu. Nope-nerf: Optimising neural Nerdi:Single-viewnerfsynthesiswithlanguage-guideddif-
radiancefieldwithnoposeprior. InCVPR,2023. 2 fusionasgeneralimagepriors. InCVPR,2023. 2
[5] MatteoBortolon,AlessioDelBue,andFabioPoiesi. Vm- [21] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra-
nerf:Tacklingsparsityinnerfwithviewmorphing. arXiv manan. Depth-supervisednerf:Fewerviewsandfastertrain-
preprintarXiv:2210.04214,2022. 2 ingforfree. InCVPR,2022. 2,6,7,18,19,20,22,23,
[6] MathildeCaron,HugoTouvron,IshanMisra,Herve¬¥Je¬¥gou, 24
JulienMairal,PiotrBojanowski,andArmandJoulin.Emerg- [22] SaraFridovich-Keil,AlexYu,MatthewTancik,Qinhong
ing properties in self-supervised vision transformers. In Chen,BenjaminRecht,andAngjooKanazawa. Plenoxels:
ICCV,2021. 3 Radiancefieldswithoutneuralnetworks. InCVPR,2022. 2
[7] EricRChan,MarcoMonteiro,PetrKellnhofer,JiajunWu, [23] QianchengFu,QingshanXu,YewSoonOng,andWenbing
andGordonWetzstein. pi-gan:Periodicimplicitgenerative Tao.Geo-neus:Geometry-consistentneuralimplicitsurfaces
adversarialnetworksfor3d-awareimagesynthesis.InCVPR, learningformulti-viewreconstruction. InNeurIPS,2022. 3
2021. 2 [24] ChenGao,YichangShih,Wei-ShengLai,Chia-KaiLiang,
[8] EricRChan,ConnorZLin,MatthewAChan,KokiNagano, andJia-BinHuang. Portraitneuralradiancefieldsfroma
Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J singleimage. arXivpreprintarXiv:2012.05903,2020. 2
Guibas, Jonathan Tremblay, Sameh Khamis, et al. Effi- [25] YuxuanHan,RuichengWang,andJiaolongYang. Single-
cientgeometry-aware3dgenerativeadversarialnetworks. viewviewsynthesisinthewildwithlearnedadaptivemulti-
InCVPR,2022. 2 planeimages. InACMSIGGRAPH2022ConferencePro-
[9] AnpeiChen,ZexiangXu,FuqiangZhao,XiaoshuaiZhang, ceedings,2022. 2
FanboXiang,JingyiYu,andHaoSu. Mvsnerf:Fastgeneral- [26] FangzhouHong,ZhaoxiChen,YushiLan,LiangPan,and
izableradiancefieldreconstructionfrommulti-viewstereo. ZiweiLiu.Eva3d:Compositional3dhumangenerationfrom
InICCV,2021. 2 2dimagecollections. InICLR,2023. 2
10[27] RonghangHu,NikhilaRavi,AlexanderCBerg,andDeepak hannesKopf,andJia-BinHuang. Robustdynamicradiance
Pathak. Worldsheet:Wrappingtheworldina3dsheetfor fields. InCVPR,2023. 2
viewsynthesisfromasingleimage. InICCV,2021. 2 [45] XuanLuo,Jia-BinHuang,RichardSzeliski,KevinMatzen,
[28] ShoukangHu,FangzhouHong,LiangPan,HaiyiMei,Lei andJohannesKopf.Consistentvideodepthestimation.ACM
Yang,andZiweiLiu. Sherf:Generalizablehumannerffrom TransactionsonGraphics(ToG),2020. 5
asingleimage. InICCV,2023. 2 [46] RicardoMartin-Brualla,NohaRadwan,MehdiSMSajjadi,
[29] ShoukangHu,KaichenZhou,KaiyuLi,LonghuiYu,Lan- JonathanTBarron,AlexeyDosovitskiy,andDanielDuck-
qingHong,TianyangHu,ZhenguoLi,GimHeeLee,and worth. Nerfinthewild:Neuralradiancefieldsforuncon-
ZiweiLiu. Consistentnerf:Enhancingneuralradiancefields strainedphotocollections. InCVPR,2021. 2
with3dconsistencyforsparseviewsynthesis.arXivpreprint [47] AndreasMeuleman,Yu-LunLiu,ChenGao,Jia-BinHuang,
arXiv:2305.11031,2023. 2 ChangilKim,MinHKim,andJohannesKopf.Progressively
[30] AjayJain,MatthewTancik,andPieterAbbeel. Puttingnerf optimizedlocalradiancefieldsforrobustviewsynthesis. In
onadiet:Semanticallyconsistentfew-shotviewsynthesis. CVPR,2023. 2
InICCV,2021. 2,3 [48] BenMildenhall,PratulP.Srinivasan,RodrigoOrtiz-Cayon,
[31] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter NimaKhademiKalantari,RaviRamamoorthi,RenNg,and
Abbeel,andBenPoole. Zero-shottext-guidedobjectgener- AbhishekKar.Locallightfieldfusion:Practicalviewsynthe-
ationwithdreamfields. InCVPR,2022. 2 siswithprescriptivesamplingguidelines.ACMTransactions
[32] RasmusJensen,AndersDahl,GeorgeVogiatzis,EnginTola, onGraphics(TOG),2019. 14,18,19,20,25
andHenrikAan√¶s. Largescalemulti-viewstereopsisevalu- [49] BenMildenhall,PratulPSrinivasan,RodrigoOrtiz-Cayon,
ation. InCVPR,2014. 2,6,8,14,21,22,26 NimaKhademiKalantari,RaviRamamoorthi,RenNg,and
[33] MohammadMahdiJohari,YannLepoittevin,andFranc¬∏ois AbhishekKar.Locallightfieldfusion:Practicalviewsynthe-
Fleuret. Geonerf:Generalizingnerfwithgeometrypriors. siswithprescriptivesamplingguidelines.ACMTransactions
InCVPR,2022. 2 onGraphics(TOG),2019. 2,6,7
[34] Jaewoo Jung, Jisang Han, Jiwon Kang, Seongchan Kim, [50] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Min-SeopKwak,andSeungryongKim.Self-evolvingneural JonathanT.Barron,RaviRamamoorthi,andRenNg. Nerf:
radiancefields. arXivpreprintarXiv:2312.01003,2023. 3 Representingscenesasneuralradiancefieldsforviewsyn-
[35] MijeongKim,SeongukSeo,andBohyungHan. Infonerf: thesis. InECCV,2020. 1,2,3,4
Rayentropyminimizationforfew-shotneuralvolumeren- [51] Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla,
dering. InCVPR,2022. 2 PratulPSrinivasan,andJonathanTBarron.Nerfinthedark:
[36] DiederikPKingmaandJimmyBa. Adam:Amethodfor Highdynamicrangeviewsynthesisfromnoisyrawimages.
stochasticoptimization. arXivpreprintarXiv:1412.6980, InCVPR,2022. 2
2014. 6 [52] ThomasMu¬®ller,AlexEvans,ChristophSchied,andAlexan-
[37] JohannesKopf,XuejianRong,andJia-BinHuang. Robust derKeller. Instantneuralgraphicsprimitiveswithamul-
consistentvideodepthestimation. InCVPR,2021. 5 tiresolutionhashencoding. ACMTransactionsonGraphics
[38] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. (ToG),2022. 3,4
Imagenetclassificationwithdeepconvolutionalneuralnet- [53] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall,
works. InNeurIPS,2012. 3 MehdiSMSajjadi,AndreasGeiger,andNohaRadwan.Reg-
[39] MinseopKwak,JiuhnSong,andSeungryongKim. Gecon- nerf:Regularizingneuralradiancefieldsforviewsynthesis
erf:Few-shotneuralradiancefieldsviageometricconsis- fromsparseinputs. InCVPR,2022. 2,3,6,7,8,16,18,19,
tency. InICML,2023. 2,3,6,7,14,17,18,19,20 20,22,23,24
[40] SeokYeongLee,JunYongChoi,SeungryongKim,Ig-Jae [54] Michael Oechsle, Songyou Peng, and Andreas Geiger.
Kim, and Junghyun Cho. Extremenerf: Few-shot neural Unisurf: Unifying neural implicit surfaces and radiance
radiance fields under unconstrained illumination. arXiv fieldsformulti-viewreconstruction. InICCV,2021. 2
preprintarXiv:2303.11728,2023. 2 [55] Jiefeng Peng, Jiqi Zhang, Changlin Li, Guangrun Wang,
[41] Changlin Li, Bohan Zhuang, Guangrun Wang, Xiaodan XiaodanLiang,andLiangLin. Pi-nas:Improvingneural
Liang,XiaojunChang,andYiYang. Automatedprogres- architecturesearchbyreducingsupernettrainingconsistency
sivelearningforefficienttrainingofvisiontransformers. In shift. InICCV,2021. 2
CVPR,2022. 3 [56] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and
[42] SiyuanLi,YueLuo,YeZhu,XunZhao,YuLi,andYing FrancescMoreno-Noguer.D-nerf:Neuralradiancefieldsfor
Shan. Enforcingtemporalconsistencyinvideodepthesti- dynamicscenes. InCVPR,2021. 2
mation. InICCV,2021. 2,5 [57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
[43] Kai-EnLin,Yen-ChenLin,Wei-ShengLai,Tsung-YiLin, Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Yi-ChangShih,andRaviRamamoorthi. Visiontransformer AmandaAskell,PamelaMishkin,JackClark,etal.Learning
fornerf-basedviewsynthesisfromasingleinputimage. In transferablevisualmodelsfromnaturallanguagesupervi-
WACV,2023. 3 sion. InICML,2021. 3
[44] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu [58] Rene¬¥ Ranftl, Katrin Lasinger, David Hafner, Konrad
Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Jo- Schindler,andVladlenKoltun. Towardsrobustmonocular
11depthestimation:Mixingdatasetsforzero-shotcross-dataset [74] ChengSun,MinSun,andHwann-TzongChen.Directvoxel
transfer. IEEETPAMI,2020. 2 gridoptimization:Super-fastconvergenceforradiancefields
[59] Rene¬¥ Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. reconstruction. InCVPR,2022. 2,3,4,6
Vision transformers for dense prediction. arXiv preprint [75] Jiakai Sun, Zhanjie Zhang, Jiafu Chen, Guangyuan Li,
arXiv:2103.13413,2021. 2 BoyanJi,LeiZhao,andWeiXing. Vgos:Voxelgridopti-
[60] Rene¬¥Ranftl,AlexeyBochkovskiy,andVladlenKoltun. Vi- mizationforviewsynthesisfromsparseinputs. InIJCAI,
siontransformersfordenseprediction. InICCV,2021. 6, 2023. 2,3,4,6,14,16,18,19,20,21,22
15 [76] MatthewTancik,PratulSrinivasan,BenMildenhall,Sara
Fridovich-Keil,NithinRaghavan,UtkarshSinghal,RaviRa-
[61] Barbara Roessle, Jonathan T Barron, Ben Mildenhall,
mamoorthi,JonathanBarron,andRenNg. Fourierfeatures
Pratul P Srinivasan, and Matthias Nie√üner. Dense depth
letnetworkslearnhighfrequencyfunctionsinlowdimen-
priorsforneuralradiancefieldsfromsparseinputviews. In
sionaldomains. InNeurIPS,2020. 3
CVPR,2022. 2,6,7,18,19,20,22,23,24
[77] Tang Tao, Longfei Gao, Guangrun Wang, Peng Chen,
[62] Johannes Lutz Scho¬®nberger and Jan-Michael Frahm.
Dayang Hao, Xiaodan Liang, Mathieu Salzmann, and
Structure-from-motionrevisited. InCVPR,2016. 6
KaichengYu. Lidar-nerf:Novellidarviewsynthesisvia
[63] Johannes Lutz Scho¬®nberger, Enliang Zheng, Marc Polle-
neural radiance fields. arXiv preprint arXiv:2304.10406,
feys,andJan-MichaelFrahm. Pixelwiseviewselectionfor
2023. 2
unstructuredmulti-viewstereo. InECCV,2016. 6
[78] RichardTuckerandNoahSnavely. Single-viewviewsyn-
[64] SeunghyeonSeo,YeonjinChang,andNojunKwak.Flipnerf:
thesiswithmultiplaneimages. InCVPR,2020. 2
Flippedreflectionraysforfew-shotnovelviewsynthesis. In
[79] Mikaela Angelina Uy, Ricardo Martin-Brualla, Leonidas
ICCV,2023. 3
Guibas,andKeLi. Scade:Nerfsfromspacecarvingwith
[65] SeunghyeonSeo,DonghoonHan,YeonjinChang,andNo-
ambiguity-awaredepthestimates. InCVPR,2023. 2
junKwak. Mixnerf:Modelingaraywithmixturedensity
[80] GuangrunWangandPhilipHSTorr. Traditionalclassifica-
fornovelviewsynthesisfromsparseinputs. InCVPR,2023.
tionneuralnetworksaregoodgenerators:Theyarecompeti-
2
tivewithddpmsandgans.arXivpreprintarXiv:2211.14794,
[66] Ruoxi Shi, Xinyue Wei, Cheng Wang, and Hao Su. Ze-
2022. 2
rorf:Fastsparseview360{\deg}reconstructionwithzero
[81] Guangcong Wang, Zhaoxi Chen, Chen Change Loy, and
pretraining. InCVPR,2024. 3,6,8,14,17,21,22
ZiweiLiu. Sparsenerf:Distillingdepthrankingforfew-shot
[67] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias novelviewsynthesis. InICCV,2023. 1,2,4,6,7,8,14,16,
Nie√üner,GordonWetzstein,andMichaelZollhofer. Deep- 17,18,19,20,21,22
voxels:Learningpersistent3dfeatureembeddings.InCVPR,
[82] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P
2019. 3
Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo
[68] VincentSitzmann,MichaelZollho¬®fer,andGordonWetzstein. Martin-Brualla,NoahSnavely,andThomasFunkhouser.Ibr-
Scene representation networks: Continuous 3d-structure- net:Learningmulti-viewimage-basedrendering. InCVPR,
awareneuralscenerepresentations. InNeurIPS,2019. 2 2021. 2
[69] VincentSitzmann,JulienMartel,AlexanderBergman,David [83] Yiqun Wang, Ivan Skorokhodov, and Peter Wonka. Hf-
Lindell,andGordonWetzstein. Implicitneuralrepresenta- neus:Improvedsurfacereconstructionusinghigh-frequency
tionswithperiodicactivationfunctions. InNeurIPS,2020. details. InNeurIPS,2022. 3
3 [84] ZhouWang,AlanCBovik,HamidRSheikh,andEeroP
[70] NagabhushanSomrajandRajivSoundararajan. Vip-nerf: Simoncelli. Imagequalityassessment:fromerrorvisibility
Visibilitypriorforsparseinputneuralradiancefields. In tostructuralsimilarity. IEEETIP,2004. 6
ACMSIGGRAPH,2023. 2,3,6,7,8,16,17,18,19,20,21, [85] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and
22,23,24,26 Victor Adrian Prisacariu. Nerf‚Äì: Neural radiance fields
[71] Nagabhushan Somraj, Adithyan Karanayil, and Rajiv without known camera parameters. arXiv preprint
Soundararajan.Simplenerf:Regularizingsparseinputneural arXiv:2102.07064,2021. 2
radiancefieldswithsimplersolutions. InACMSIGGRAPH [86] OliviaWiles,GeorgiaGkioxari,RichardSzeliski,andJustin
Asia,2023. 1,3,6,7,8,14,16,17,18,19,20,21,22,23, Johnson. Synsin:End-to-endviewsynthesisfromasingle
24,26 image. InCVPR,2020. 2
[72] Jiuhn Song, Seonghoon Park, Honggyu An, Seokju Cho, [87] FelixWimbauer,NanYang,ChristianRupprecht,andDaniel
Min-SeopKwak,SungjinCho,andSeungryongKim. D\‚Äù Cremers. Behindthescenes:Densityfieldsforsingleview
arf:Boostingradiancefieldsfromsparseinputswithmonoc- reconstruction. InCVPR,2023. 2
ulardepthadaptation. InNeurIPS,2023. 2 [88] DejiaXu,YifanJiang,PeihaoWang,ZhiwenFan,Humphrey
[73] Chih-HaiSu,Chih-YaoHu,Shr-RueiTsai,Jie-YingLee, Shi,andZhangyangWang.Sinnerf:Trainingneuralradiance
Chin-YangLin,andYu-LunLiu. Boostmvsnerfs:Boosting fieldsoncomplexscenesfromasingleimage. InECCV,
mvs-basednerfstogeneralizableviewsynthesisinlarge- 2022. 2,3
scalescenes. InACMSIGGRAPH2024ConferencePapers, [89] DejiaXu,YifanJiang,PeihaoWang,ZhiwenFan,YiWang,
2024. 2 andZhangyangWang.Neurallift-360:Liftinganin-the-wild
122dphototoa3dobjectwith360degviews. InCVPR,2023.
2
[90] Yingjie Xu, Bangzhen Liu, Hao Tang, Bailin Deng, and
ShengfengHe. Learningwithunreliability:Fastfew-shot
voxelradiancefieldswithrelativegeometricconsistency. In
CVPR,2024. 3
[91] JiaweiYang,MarcoPavone,andYueWang. Freenerf:Im-
provingfew-shotneuralrenderingwithfreefrequencyregu-
larization. InCVPR,2023. 1,2,3,4,6,7,8,14,16,17,18,
19,20,21,22,23,24
[92] LiorYariv,YoniKasten,DrorMoran,MeiravGalun,Matan
Atzmon,BasriRonen,andYaronLipman. Multiviewneu-
ralsurfacereconstructionbydisentanglinggeometryand
appearance. InNeurIPS,2020. 2
[93] VickieYe,ZhengqiLi,RichardTucker,AngjooKanazawa,
and Noah Snavely. Deformable sprites for unsupervised
videodecomposition. InCVPR,2022. 2
[94] AlexYu,VickieYe,MatthewTancik,andAngjooKanazawa.
pixelnerf:Neuralradiancefieldsfromoneorfewimages. In
CVPR,2021. 2,3,6,16
[95] Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma,
RongfeiJia,andLinGao. Nerf-editing:geometryeditingof
neuralradiancefields. InCVPR,2022. 2
[96] JasonZhang,GengshanYang,ShubhamTulsiani,andDeva
Ramanan. Ners:Neuralreflectancesurfacesforsparse-view
3dreconstructioninthewild. InNeurIPS,2021. 2
[97] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen
Koltun. Nerf++:Analyzingandimprovingneuralradiance
fields. arXivpreprintarXiv:2010.07492,2020. 2
[98] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,
andOliverWang. Theunreasonableeffectivenessofdeep
featuresasaperceptualmetric. InCVPR,2018. 6
[99] ChengweiZheng,WenbinLin,andFengXu. Editablenerf:
Editingtopologicallyvaryingneuralradiancefieldsbykey
points. InCVPR,2023. 2
[100] TinghuiZhou,RichardTucker,JohnFlynn,GrahamFyffe,
and Noah Snavely. Stereo magnification: Learning view
synthesisusingmultiplaneimages. InACMTOG,2018. 2,
6,22,23,24,26,27
[101] ZhizhuoZhouandShubhamTulsiani. Sparsefusion:Dis-
tillingview-conditioneddiffusionfor3dreconstruction. In
CVPR,2023. 2
[102] ZehaoZhu,ZhiwenFan,YifanJiang,andZhangyangWang.
Fsgs: Real-time few-shot view synthesis using gaussian
splatting. In ECCV, 2024. 1, 3, 6, 7, 8, 15, 17, 18, 19,
20
13A.Overview fortraining.Theseobjectmasksarenotprovideddirectlyin
thesetwodatasets.Otherwise,ZeroRFmayproducemany
Thissupplementarymaterialpresentsadditionalresultsto
artifactsandfloaters,orthefeaturevolumemaybefilledup
complementthemainmanuscript.First,wediscussthedif-
tofitthebackground,leadingtoseverememoryconsump-
ferencebetweencompetingmethodsin App.A.1Second,
tionissuescausingtrainingfailuresduetoout-of-memory
weexplaintheimplementationdetailsincalculatingrepro-
errors.
jection errors in App. B. Then, we describe the details of
addingpretrainedmonoculardepthpriorinApp.B.1Next,
we provide all the training losses in our training process SparseNeRF. SparseNeRF[81]proposesaspatialconti-
inApp.C.Moreover,wedescribetheexperimentalsetup, nuityregularizationthatdistillsdepthcontinuitypriors,but
includingthedatasetandtrainingtimemeasurementofcom- itrequiresapre-traineddepthpriorandisextremelyslowby
paredmethodsinourevaluationsinApp.D.Inadditionto usingMLPrepresentation.Additionally,becausemonocular
thisdocument,weprovideaninteractiveHTMLinterface depthpredictionresultslackdetail,SparseNeRF‚Äôsrendered
tocompareourvideoresultswithstate-of-the-artmethods results tend to be blurry and lack detail. In contrast, our
andshowablationvideosandfailurecases.Wealsoattach proposedcross-scalegeometricadaptationdoesnotrelyon
thesourcecodeofourimplementationforreferenceandwill pre-trainedpriorsandensuresthegenerationofoverallge-
makeitpubliclyavailableforreproducibility. ometrywhilepayingattentiontodetails.
A.1.DiscussionsonCompetingModels
SimpleNeRF. SimpleNeRF[71]introducesadataaugmen-
GeCoNeRF. GeCoNeRF [39] is a few-shot NeRF that tationmethodforfew-shotNeRF,employinganMLPwith
useswarpedfeaturesaspseudolabels,whichissufficiently fewerpositionalencodingfrequenciesforaugmentation,but
differentfromourmethod.Ourmethodprimarilyfocuses thissimultaneouslyincreasesthetrainingtime.Incontrast,
oncross-scalegeometricadaptation,selectingrenderdepths weproposeanefficientcross-scalegeometricadaptationthat
with minimal reprojection error across different scales as achievesmulti-scalerepresentationthroughshared-weight
pseudolabelstoadaptivelylearnthemostsuitablegeometry voxels,eliminatingtheneedforanadditionalmodeltore-
for each scale. In contrast, GeCoNeRF, besides requiring constructthesamescene.Thisapproachyieldsbetterresults
a pre-trained feature extractor, directly optimizes warped withlowercosts.
features,makingithighlysensitivetogeometricnoiseand
resultinginmanyfloatersinitsrenderingresultasshown FreeNeRF. FreeNeRF [91] is an MLP-based few-shot
in our supplementary videos. Our approach, on the other NeRFmodel.FreeNeRFproposesusingaschedulingmech-
hand,ismorerobustduetoourproposedmulti-scalevoxels. anism to gradually increase input frequency, allowing the
Low-resolutionvoxelsrepresentcoarsegeometry,whichis model to learn low-frequency geometry during the early
less likely to produce floaters. Using this as supervision stagesoftrainingandthenrampuppositionalencodingto
effectivelysuppressesthegenerationoffloaters. enablethemodeltolearnmoredetailedgeometrylateron.
However,ourapproachtakesadvantageoftheexplicitvoxel
representation,whichconvergesfasterandallowsfordirect
ZeroRF. ZeroRF[66]isaconcurrentworktoours,also
cross-scaledgeometryoperations.Additionally,becausewe
aimedattrainingNeRFwithsparseinputviewsandachiev-
employcross-scalegeometryadaptation,ourmodeldynam-
ingfasttrainingtimes.UnlikeTensoRF[10],whichdirectly
icallydetermineswhichfrequencyofgeometrytolearnat
optimizesthedecomposedfeaturegrid,ZeroRFparameter-
different training stages. We do not require the complex
izesthefeaturegridswitharandomlyinitializeddeepneural
frequency scheduling of FreeNeRF, nor are we limited to
network(generator).Thisdecisionisbasedonthebeliefin
learningonlyhigh-frequencycomponentsinthelaterstages
thehigherresiliencetonoiseandartifactsabilityofdeepneu-
oftraininglikeFreeNeRF.Thismakesourmethodsimpler,
ralnetworks.AlthoughZeroRFclaimstoachievefastcon-
moregeneral,andmorerobust.
vergencestemmingfromitsvoxelrepresentation,theneed
totrainthegeneratorresultsinslowertrainingspeedscom-
paredtoours(refertothemainpaperTable2).Ourmethod VGOS. VGOS[75]introducesanincrementalvoxeltrain-
directlyoptimizesthefeaturegridandutilizescross-scale ing strategy and a voxel smoothing method for Few-shot
geometryadaptationtoavoidoverfittingundersparseviews, NeRF,aimedatreducingtrainingtime.Itemploysacom-
withoutrequiringageneratorthatslowsdownconvergence plexschedulingstrategytofreezetheouterpartofthevoxel,
to form decomposed tensorial feature volumes. Addition- leadingtoaleakyreconstructionofthebackgroundscene.
ally,wefoundthatZeroRFisnotsuitableforsceneswitha Additionally,VGOSrequiresgroundtruthposesfornovel
background(e.g.,LLFF[48])ordatasetsliketheDTU[32] posesampling,whichresultsinaqualitydropwhenusing
Dataset,whereZeroRFmustextensivelyuseobjectmasks randomsampling.However,whileVGOS‚Äôstrainingtimeis
14shorterthanours,itsperformancesignificantlylagsbehind. and j (denoted as C ), facilitating the computation of the
j
Ourcross-scalegeometricadaptationstrategyeliminatesthe reprojectionerror:
need for complex scheduling and ground truth pose sam-
pling.
el(p i)=(cid:13) (cid:13)C i(p i)‚àíC j(pl i‚Üíj)(cid:13) (cid:13)2 (16)
Therefore,foreachraysampledfromthetrainingview,the
FSGS. FSGS[102]addressesthechallengeoflimited3D pseudo-GTdepthofthescalewiththeminimumreprojection
Gaussiansplatting(3DGS)byintroducingProximity-guided errorisobtained,
GaussianUnpooling,whichadaptivelydensifiestheGaus- D‚Ä≤(r )=argmin(el(r )). (17)
train train
siansbetweenexistingpoints.Althoughthismethodmiti- l
gatestheissueofinsufficientGS,itstillreliesonasufficient wherethepseudo-GTdepthisutilizedtocomputethegeo-
initialsetofGaussianstoperformeffectively.Infew-shot metricadaptationloss(MSE)L .
geo
scenarios,theinitialnumberofGScanbeextremelysparse,
leadingtosuboptimalresults.Furthermore,FSGSfrequently L (r )=(cid:88)L (cid:88) (cid:13) (cid:13)DÀÜl(r )‚àíD‚Ä≤(r )(cid:13) (cid:13)2 .
requiresnovelviewinferenceusingmonoculardepthmodels geo train (cid:13) train train (cid:13)
during training, which significantly increases the training l=0rtrain‚ààRtrain
(18)
time.Incontrast,ourcross-scalegeometricadaptationap-
Thismechanismprovidesasupervisorysignalforgeometry,
proachensuresrapidconvergencewithoutrelyingonnovel
ensuring that the model can effectively maintain the geo-
view inference or monocular depth models, providing ef-
metric integrity of the scene across different scales, even
ficient and robust performance even with minimal initial
intheabsenceofexplicitdepthgroundtruth.Itisapivotal
data.
partofthetrainingprocess,allowingthemodeltoadaptand
refineitsunderstandingofthescene‚Äôsgeometricstructure
B.DetailsofCalculatingReprojectionErrors
inaself-adaptivemanner.Inourimplementation,insteadof
usingasinglepixeltocalculatereprojectionerror,weusea
Mathematically,letp bea2Dpixelcoordinateinframei,
i
andp beitshomogeneousaugmentation.ThedepthDl(p )
patchwith5√ó5pixelstocalculatereprojectionerror.This
(cid:101)i i i avoidswarpingnoisecausedbysimilarpatternsinscenes,
atscalel obtainedfromvolumerendering,andcamerain-
trinsicsK areusedtoreprojectp ontothe3Dpointxl in for example, in the case of the LLFF fortress and room.
i i i Furthermore,wesetathresholdforreprojectionerrorthat
cameracoordinatesystemofframei.Subsequently,utilizing
allowsustoignorecasesofimagewarpingwithocclusions
therotationmatrixR andtranslationmatrixt offramei,
i i
xl aretransformedintoworldcoordinatessystemxl: andpreventscrashesduringinitialtrainingprocesses,which
i typicallyhavehighreprojectionerrors.
xl i =D il(p i)K i‚àí1p (cid:101)i (11) B.1.DetailsofaddingPretrainedMonocularDepth
Prior
xl =R xl +t (12)
i i i
We utilize the pre-trained Dense Prediction Transformer
Wesimplifytheprevioustwoequationsbecausetheposition (DPT)[60]togeneratemonoculardepthmapsfromtrain-
ofthe3Dpointxl inworldcoordinatescanalsobedeter- ingviews.DPTistrainedon1.4millionimage-depthpairs,
mined directly from the ray defined by the starting point making it a convenient and effective choice for our setup.
o (p )andthedirectionv (p ): Toaddressthescaleambiguitybetweenthetruescenescale
i i i i
and the estimated depth, we introduce a relaxed relative
xl =o i(p i)+D il(p i)v i(p i) (13) loss based on Pearson correlation between the estimated
and rendered depth maps. This loss is applied at multiple
Followingthis,the3Dpointxl intheworldcoordinatesys- scales, enhancing the monocular depth prior‚Äôs constraint
temistransformedtothecameracoordinatesystemofframe acrossdifferentscalesandimprovingtheoverallgeometric
j usingitsrotationmatricesR j,andtranslationmatricesT j: consistency.
xl =RT (cid:0) xl‚àít (cid:1) (14) C.Losses
i‚Üíj j j
Voxel TV loss (L ). We use the TV loss on voxel to
Finally,projectitbacktothe2Dpixelcoordinatesystemof tv
framej, smooththeresultinvoxelspace.
pl =œÄ(K xl ) (15)
(cid:101)i‚Üíj j i‚Üíj
Patch-wise depth smoothness loss (L ). We sample
ds
where œÄ([x,y,z]T) = (cid:2)x,y(cid:3) . Using coordinates p and patches of rays and calculate the total variance of depth
z z i
pl to index the RGB maps of frames i (denoted as C ) tosmooththegeometryinthedepthspace.
i‚Üíj i
15L1sparsityloss(L ). Wesuppressthevoxeldensityin asSimpleNeRF[71]duetotheunobservedregionproblem,
l1
airspacebyintroducingadensityL1regularizationloss. whichNeRFcannothandle,insometestingview.
D.1. Training Time Measurement and Time Com-
Distortionloss(L ). WeadopttheapproachfromMip-
dist plexity
NeRF360[3],integratingdistortionlosstoremovefloaters
fromthenovelviews. RegNeRF. WeusetheofficialimplementationofRegN-
eRF[53]andfollowmostofthedefaultconfiguration,while
Occlusion loss (L ). In the DTU dataset, we follow thebatchsizeorotherhyperparametersmightbeadjusted
occ
FreeNeRF[91]byincorporatinganocclusionlossthatuti- due to the GPU memory issue. For the LLFF dataset, the
lizesblackandwhitebackgroundpriorstopushfloatersinto trainingrequiresroughly2.35hoursperscenewith69769
thebackground. iterations and a batch size of 2,048. Note that RegNeRF
samples 10000 random poses by its default configuration
ontheDTUdataset,leadingtoout-of-memoryonasingle
Novelposesamplingformspiralingtrajectory. Wefol-
NVIDIARTX4090GPU.Whilereducingthenumberofran-
lowtheimplementationofaspiralingtrajectoryfromTen-
domposestoabout1/8couldpotentiallyresolvethisissue,
soRF[10].FortheLLFFdataset,wesample60novelposes
suchareductionislikelytoadverselyaffecttheperformance,
fromthespiralingtrajectorysampledfromtrainingviews
sowesimplyexcludethismethodfromourexperiments.
with1rotations,radiusscale1.0,andz 0.5.FortheDTU
rate
dataset,wesample60novelposesfromthespiralingtrajec-
tory sampled from training views with 4 rotations, radius FreeNeRF. WeusetheofficialimplementationofFreeN-
scale0.5,andz 0.5.FortheRealEstate-10Kdataset,we eRF[91]andfollowmostofthedefaultconfiguration,while
rate
sample60novelposesfromthespiralingtrajectorysampled thebatchsizeorotherhyperparametersmightbeadjusted
fromtrainingviewswith2rotations,radiusscale2.0,and due to the GPU memory issue. For the LLFF dataset, the
z 0.5. trainingrequiresroughly1.5hoursperscenewith69,769
rate
iterations and a batch size of 2,048. For the DTU dataset,
D.ExperimentalSetup
the training requires about 1 hour per scene with 43,945
iterationsandabatchsizeof2,048.
Wecomparetheresultoffew-shotNeRFonLLFFandDTU
withn=2,3,4inputviews.
SparseNeRF. We use the official implementation of
SparseNeRF.[81]andfollowmostofthedefaultconfigura-
LLFF dataset. The LLFF dataset comprises 8 forward-
tion,whilethebatchsizeorotherhyperparametersmightbe
facing unbounded scenes with variable frame counts at a
adjustedduetotheGPUmemoryissue.FortheLLFFdataset,
resolutionof1008√ó756.Inlinewithpriorwork[70],we
thetrainingrequiresroughly1hourperscenewith70,000
useevery8thframefortestingineachscene.Fortraining,
iterationsandabatchsizeof512.FortheDTUdataset,the
weuniformlysamplenviewsfromtheremainingframes.
training requires about 30 minutes per scene with 70,000
iterationsandabatchsizeof256.
DTUdataset. TheDTUdatasetisalarge-scalemulti-view
collection that includes 124 different scenes. Follow the
Pixel-NeRF[94]andViP-NeRF[70]approach,weusethe SimpleNeRF. WeusetheofficialimplementationofSim-
sametestsets.However,becauseCOLMAPwillfailtogen- pleNeRF[71]andfollowmostofthedefaultconfiguration,
eratesparsedepthatscans8,30,and110,wecanonlytest whilethebatchsizeorotherhyperparametersmightbead-
on12scenes.TestscanIDsare21,31,34,38,40,41,45, justedduetotheGPUmemoryissue.FortheLLFFdataset,
55,63,82,103,and114.WeusespecificimageIDsasin- we use the model weights released by the author directly.
putviewsanddownsampleimagesto300√ó400pixelsfor Sincethere‚ÄôsnoofficialimplementeddataloaderfortheDTU
consistencywithpriorstudies[70,94]. dataset,weusethedataloaderandconfigurationfromViP-
NeRF[70],whichrequiresabout1.38hoursperscenewith
25,000iterationsandbatchsizeof2,048.
RealEstate-10Kdataset. RealEstate-10Kisacomprehen-
sivedatabaseofapproximately80,000videosegments,each
withover30frames,widelyutilizedfornovelviewsynthesis. VGOS. WefurterprovideVGOSresult.Weusetheoffi-
Forourstudy,weselectfivescenesfromitsextensivetest cialimplementationofVGOS[75]andfollowmostofthe
set,followingtheapproachoutlinedinViP-NeRF[70].We defaultconfiguration,whilethebatchsizeorotherhyperpa-
selectedframes0,10,20,and30forthetrainingsetwitha rametersmightbeadjustedduetotheGPUmemoryissue.
resolutionof1024√ó576,inaccordancewiththeSimpleN- Note that VGOS samples random poses directly from the
eRF [71] methodology, while testing on the same test set entire dataset, which is unreasonable under the few-shot
16setting, so we replace the sampling with the interpolation Table5.Comparisonofthetimecomplexity.
fromtrainingposesimplementedintheofficialrepo.Forthe
LLFFdataset,thetrainingrequiresroughly5minutesper Method MFLOPs/pixel‚Üì
scenewith9,000iterationsandabatchsizeof16,384.For
FreeNeRF[91] 288.57
theDTUdataset,thetrainingrequiresabout3minutesper ViP-NeRF[70] 149.26
scenewith9,000iterationsandabatchsizeof16,384.Note SimpleNeRF[71] 303.82
thatVGOSseemsinvalidontheDTUdataset(Fig.11)and SparseNeRF[81] 287.92
theydoesnotevaluatetheDTUdatasetintheirpaper. Ours 13.77
GeCoNeRF. AsmentionedinGeCoNeRF[39]‚Äôsofficial
DTUdataset. Weshowall12scenesofthequantitative
github repo, their current code is unexecutable. To com-
comparisons with two, three, and four input views on the
pleteourexperiment,westilltryourbesttoimplementtheir
LLFFdatasetinTab.9,Tab.10,andTab.11,respectively.
methodbasedonthecodeprovided.FortheLLFFdataset,
thetrainingrequiresroughly4hoursperscenewith85,000
RealEstate-10K dataset. We show all 12 scenes of the
iterationsandabatchsizeof1024.Itisimportanttonotethat
quantitative comparisons with two, three, and four input
weutilized2GPUsfortrainingthismethod,sothetraining
views on the LLFF dataset in Tab. 12, Tab. 13, Tab. 14,
time reported in our paper might be shorter than what is
andTab.15.
actuallyrequired.
F.AdditionalVisualComparisons
ZeroRF. We use the official implementation of Ze-
LLFFdataset. Weshowadditionalvisualcomparisonson
roRF[66]andfollowmostofthedefaultconfigurations.For
theLLFFdatasetwithtwoinputviewsinFig.10.
theLLFFdataset,ZeroRFdoesnotprovidethedataloader
fortheLLFF,andtheirpapermentionsitsinabilitytobeused
forunboundedscenes.Therefore,ourprimarytestingwas DTUdataset. Weshowadditionalvisualcomparisonson
conductedontheDTUdataset.IntheDTUdataset,theorig- theDTUdatasetwithtwoinputviewsinFig.11.
inalimplementationofZeroRFnecessitatesmaskingoutthe
backgroundareaoftheinputframebeforetraining,whichis RealEstate-10Kdataset. Wefurtherpresentthequalita-
incompatiblewithourevaluationbenchmark.Consequently, tivecomparisonsofnovelviewsynthesisontheRealEstate-
wetraineditwithoutobjectmasks.Trainingrequiresapprox- 10K dataset with two input views in Fig. 13. Compared
imately25minutesperscenewith10,000iterationsanda toSimpleNeRF[71],whichrequireshoursoftraining,Fru-
batchsizeof214.
galNeRF needs only less than 20 minutes and can render
comparableresults,demonstratingFrugalNeRF‚Äôseffective-
nessinmorein-the-wildscenes.
FSGS. WeusetheofficialimplementationofFSGS[102]
andfollowmostofthedefaultconfigurations.FortheLLFF
dataset,weadjusttheinputviewstomatchthesettingsused
inViP-NeRF,whichdiffersfromtheoriginalFSGSpaper.
Training takes approximately 25 minutes per scene with
10,000iterations.Sincethereisnoofficialdataloaderforthe
DTUdataset,weconverttheDTUcameraposestotheLLFF
formatandusethedefaultLLFFconfiguration.Trainingon
theDTUdatasetrequiresaround20minutesperscenewith
10,000iterations.
Timecomplexity. Toverifytheefficiencyofourmethod,
besidescomparingthetrainingtimeofvariousmethods,we
alsocalculatedtheMFLOPsperpixelin Tab.5.
E.CompleteQuantitativeEvaluations
LLFFdataset. Weshowall8scenesofthequantitative
comparisons with two, three, and four input views on the
LLFFdatasetinTab.6,Tab.7,andTab.8,respectively.
17Table6.QuantitativeresultsontheLLFF[48]datasetwithtwoinputviews.ThethreerowsshowLPIPS,SSIM,andPSNRscores,
respectively.
Scene Fern Flower Fortress Horns Leaves Orchids Room Trex Average
Method
0.51 0.43 0.37 0.51 0.35 0.45 0.38 0.42 0.43
RegNeRF[53] 0.45 0.51 0.46 0.42 0.37 0.30 0.74 0.54 0.49
15.8 17.0 20.6 15.9 14.5 13.9 18.7 16.7 16.9
0.50 0.43 0.30 0.49 0.47 0.43 0.35 0.41 0.42
DS-NeRF[21] 0.46 0.44 0.65 0.49 0.24 0.32 0.76 0.53 0.51
16.4 16.1 23.0 16.6 12.4 13.7 18.9 15.7 16.9
0.44 0.46 0.17 0.46 0.52 0.41 0.30 0.43 0.39
DDP-NeRF[61] 0.49 0.45 0.77 0.52 0.23 0.38 0.76 0.54 0.54
17.2 16.2 22.7 17.1 12.6 15.1 18.7 15.7 17.2
0.46 0.38 0.33 0.43 0.36 0.42 0.34 0.33 0.38
FreeNeRF[91] 0.49 0.55 0.53 0.53 0.38 0.35 0.76 0.60 0.54
17.1 17.6 21.3 17.1 14.4 14.1 18.3 18.1 17.6
0.45 0.42 0.21 0.39 0.46 0.40 0.36 0.38 0.37
ViP-NeRF[70] 0.45 0.43 0.71 0.54 0.21 0.36 0.72 0.54 0.52
16.2 14.9 22.6 17.1 11.7 14.2 17.7 15.9 16.7
0.51 0.43 0.25 0.42 0.44 0.41 0.35 0.39 0.39
SimpleNeRF[71] 0.50 0.53 0.67 0.54 0.30 0.37 0.77 0.58 0.55
17.0 16.9 22.5 17.1 13.5 14.7 19.5 16.8 17.6
0.48 0.44 0.37 0.47 0.36 0.42 0.38 0.40 0.42
VGOS[75] 0.51 0.55 0.53 0.55 0.38 0.40 0.77 0.59 0.55
16.5 17.5 19.4 15.7 14.7 14.4 18.8 16.0 16.7
0.56 0.49 0.50 0.61 0.49 0.51 0.54 0.49 0.52
GeCoNeRF[39] 0.47 0.49 0.43 0.41 0.28 0.29 0.68 0.52 0.45
16.4 16.9 17.9 15.4 13.3 13.4 17.3 16.1 15.8
0.48 0.55 0.40 0.52 0.52 0.55 0.29 0.37 0.45
SparseNeRF[81] 0.52 0.41 0.61 0.51 0.244 0.24 0.82 0.62 0.52
18.2 15.4 21.7 17.4 13.4 13.3 22.8 18.6 18.0
0.46 0.45 0.35 0.42 0.33 0.41 0.38 0.45 0.41
FSGS[102] 0.40 0.38 0.47 0.42 0.34 0.24 0.72 0.46 0.45
15.0 14.8 16.9 16.2 14.2 12.6 17.6 13.8 15.3
0.41 0.41 0.27 0.36 0.32 0.42 0.34 0.32 0.35
FrugalNeRF(Ours) 0.47 0.50 0.54 0.55 0.41 0.33 0.75 0.61 0.54
17.4 17.5 20.3 18.5 15.5 15.0 19.2 18.6 18.1
0.40 0.40 0.27 0.37 0.33 0.39 0.32 0.35 0.35
FrugalNeRFw/mono.depth(Ours) 0.46 0.53 0.54 0.54 0.41 0.37 0.76 0.59 0.54
17.7 17.9 20.9 18.5 15.4 15.6 19.6 18.2 18.3
18Table7.QuantitativeresultsontheLLFF[48]datasetwiththreeinputviews.ThethreerowsshowLPIPS,SSIM,andPSNRscores,
respectively.
Scene Fern Flower Fortress Horns Leaves Orchids Room Trex Average
Method
0.47 0.27 0.31 0.44 0.39 0.44 0.25 0.36 0.36
RegNeRF[53] 0.48 0.58 0.64 0.53 0.37 0.31 0.81 0.63 0.57
17.9 19.6 22.7 18.2 14.6 14.2 21.0 18.4 18.7
0.47 0.25 0.25 0.47 0.50 0.45 0.22 0.37 0.36
DS-NeRF[21] 0.52 0.66 0.72 0.52 0.25 0.33 0.84 0.59 0.58
18.5 21.3 24.8 17.5 12.6 14.1 23.0 17.1 19.0
0.47 0.29 0.20 0.48 0.52 0.45 0.32 0.42 0.39
DDP-NeRF[61] 0.53 0.63 0.75 0.53 0.24 0.35 0.76 0.54 0.56
18.5 20.2 22.1 17.4 12.8 15.1 18.3 16.0 17.7
0.40 0.28 0.32 0.41 0.40 0.41 0.22 0.33 0.34
FreeNeRF[91] 0.54 0.61 0.60 0.58 0.40 0.37 0.85 0.64 0.60
18.9 20.7 22.0 18.7 15.0 14.7 22.6 19.0 19.3
0.51 0.24 0.19 0.42 0.44 0.41 0.27 0.32 0.34
ViP-NeRF[70] 0.49 0.65 0.76 0.57 0.25 0.34 0.81 0.62 0.59
17.3 20.8 24.5 18.2 12.4 14.2 21.7 18.1 18.9
0.43 0.24 0.17 0.42 0.42 0.39 0.26 0.34 0.33
SimpleNeRF[71] 0.52 0.66 0.78 0.57 0.38 0.38 0.83 0.66 0.62
18.2 20.7 24.7 18.4 14.8 15.0 22.0 18.9 19.5
0.40 0.31 0.33 0.46 0.40 0.41 0.31 0.35 0.37
VGOS[75] 0.58 0.61 0.69 0.58 0.40 0.40 0.83 0.66 0.61
19.0 20.0 23.0 17.0 15.0 15.2 21.8 18.0 18.8
0.57 0.36 0.45 0.60 0.50 0.51 0.34 0.43 0.47
GeCoNeRF[39] 0.46 0.57 0.53 0.44 0.32 0.30 0.80 0.59 0.50
17.0 19.5 20.6 15.8 13.8 13.6 21.1 18.1 17.4
0.43 0.33 0.37 0.50 0.35 0.41 0.28 0.31 0.37
SparseNeRF[81] 0.57 0.60 0.59 0.53 0.45 0.37 0.81 0.67 0.59
19.6 19.8 23.0 18.4 16.5 15.2 21.5 20.1 19.5
0.48 0.30 0.15 0.36 0.26 0.35 0.28 0.28 0.30
FSGS[102] 0.55 0.68 0.72 0.65 0.28 0.37 0.84 0.62 0.61
17.9 21.5 23.9 19.4 13.3 14.1 22.6 17.4 19.2
0.39 0.32 0.24 0.34 0.37 0.42 0.27 0.29 0.32
FrugalNeRF(Ours) 0.50 0.55 0.63 0.59 0.39 0.35 0.81 0.66 0.59
18.2 18.8 23.4 19.3 15.5 15.3 22.2 19.3 19.4
0.40 0.23 0.22 0.33 0.37 0.40 0.25 0.29 0.30
FrugalNeRFw/mono.depth(Ours) 0.49 0.63 0.69 0.60 0.39 0.36 0.83 0.67 0.61
18.6 21.4 23.5 19.0 15.4 15.7 22.3 20.0 19.9
19Table8.QuantitativeresultsontheLLFF[48]datasetwithfourinputviews.ThethreerowsshowLPIPS,SSIM,andPSNRscores,
respectively.
Scene Fern Flower Fortress Horns Leaves Orchids Room Trex Average
Method
0.35 0.29 0.37 0.34 0.32 0.43 0.19 0.32 0.32
RegNeRF[53] 0.63 0.64 0.55 0.64 0.44 0.34 0.87 0.66 0.62
20.8 19.8 22.4 20.1 15.9 14.8 23.9 18.9 19.9
0.35 0.28 0.31 0.41 0.41 0.41 0.16 0.39 0.34
DS-NeRF[21] 0.63 0.64 0.66 0.59 0.39 0.38 0.89 0.59 0.61
20.9 20.6 24.1 19.5 15.8 15.2 25.6 17.1 20.1
0.40 0.30 0.18 0.42 0.45 0.42 0.26 0.39 0.35
DDP-NeRF[61] 0.60 0.63 0.73 0.59 0.37 0.41 0.82 0.60 0.61
20.1 20.0 23.4 19.3 15.1 15.8 20.8 17.3 19.2
0.37 0.30 0.35 0.37 0.35 0.42 0.19 0.31 0.33
FreeNeRF[91] 0.64 0.64 0.60 0.63 0.47 0.37 0.88 0.68 0.63
21.1 20.5 23.2 20.4 16.6 14.9 24.8 19.6 20.5
0.39 0.27 0.25 0.38 0.36 0.40 0.23 0.32 0.32
ViP-NeRF[70] 0.58 0.63 0.70 0.60 0.40 0.39 0.85 0.64 0.62
18.2 19.5 23.3 19.0 14.8 14.8 23.2 18.6 19.3
0.33 0.27 0.28 0.38 0.35 0.36 0.19 0.32 0.31
SimpleNeRF[71] 0.65 0.67 0.69 0.63 0.46 0.42 0.88 0.68 0.65
21.1 20.8 24.3 19.7 16.3 15.7 24.3 19.3 20.4
0.40 0.35 0.40 0.43 0.34 0.41 0.28 0.35 0.37
VGOS[75] 0.64 0.63 0.64 0.62 0.49 0.43 0.86 0.68 0.64
19.6 20.3 22.7 18.6 16.6 15.8 23.6 18.7 19.7
0.45 0.36 0.44 0.47 0.44 0.51 0.27 0.40 0.42
GeCoNeRF[39] 0.61 0.61 0.51 0.59 0.40 0.30 0.85 0.63 0.56
20.5 19.9 21.2 19.6 15.5 13.9 23.5 19.0 19.1
0.42 0.32 0.31 0.39 0.36 0.42 0.25 0.29 0.34
SparseNeRF[81] 0.62 0.64 0.70 0.63 0.49 0.39 0.85 0.70 0.65
21.4 20.7 24.6 20.4 17.5 15.7 23.5 20.9 20.9
0.26 0.22 0.17 0.24 0.22 0.28 0.17 0.23 0.22
FSGS[102] 0.67 0.65 0.65 0.70 0.46 0.45 0.88 0.71 0.66
20.5 20.2 22.6 20.9 15.6 15.4 23.7 19.2 20.1
0.30 0.28 0.24 0.30 0.26 0.38 0.19 0.27 0.27
FrugalNeRF(Ours) 0.63 0.64 0.60 0.66 0.52 0.41 0.87 0.72 0.65
21.1 20.8 23.6 21.6 16.9 16.3 24.2 19.7 20.9
0.30 0.27 0.25 0.28 0.24 0.37 0.18 0.27 0.26
FrugalNeRFw/mono.depth(Ours) 0.64 0.65 0.64 0.68 0.53 0.41 0.88 0.71 0.66
21.5 20.9 23.9 21.1 17.2 16.3 24.1 19.6 20.9
20Table9.QuantitativeresultsontheDTU[32]datasetwithtwoinputviews.ThethreerowsshowLPIPS,SSIMandPSNRscores,
respectively.
Scene Scan21 Scan31 Scan34 Scan38 Scan40 Scan41 Scan45 Scan55 Scan63 Scan82 Scan103 Scan114 Average
Method
0.33 0.18 0.31 0.34 0.41 0.35 0.19 0.11 0.07 0.08 0.17 0.12 0.22
FreeNeRF[91] 0.51 0.75 0.63 0.61 0.58 0.63 0.76 0.80 0.93 0.90 0.82 0.85 0.73
13.21 19.33 14.66 16.76 11.42 14.50 18.66 21.62 23.19 21.56 17.55 24.19 18.05
0.37 0.24 0.27 0.38 0.31 0.23 0.31 0.21 0.09 0.12 0.18 0.17 0.24
ViP-NeRF[70] 0.26 0.49 0.52 0.43 0.47 0.58 0.37 0.39 0.63 0.57 0.65 0.49 0.49
11.31 13.57 17.13 13.25 15.08 17.81 11.35 16.92 16.71 13.37 16.15 16.24 14.91
0.23 0.32 0.23 0.21 0.24 0.19 0.28 0.22 0.30 0.27 0.19 0.27 0.25
SimpleNeRF[71] 0.73 0.71 0.76 0.77 0.77 0.84 0.70 0.88 0.75 0.79 0.81 0.82 0.79
12.71 11.91 14.39 14.50 13.76 15.57 11.88 19.58 12.73 14.37 16.64 14.86 14.41
0.28 0.36 0.33 0.31 0.30 0.27 0.37 0.15 0.49 0.45 0.34 0.18 0.32
VGOS[75] 0.69 0.67 0.69 0.71 0.73 0.78 0.64 0.90 0.56 0.57 0.73 0.85 0.71
9.69 8.97 9.75 10.27 8.79 9.75 7.54 19.24 5.17 5.63 11.29 15.81 10.16
0.39 0.22 0.26 0.33 0.24 0.21 0.20 0.14 0.08 0.08 0.15 0.13 0.20
SparseNeRF[81] 0.45 0.69 0.70 0.60 0.72 0.76 0.75 0.78 0.92 0.91 0.84 0.85 0.75
14.25 17.95 20.65 17.93 16.33 20.13 18.22 22.29 20.70 23.46 21.70 24.40 19.83
0.45 0.27 0.35 0.44 0.29 0.28 0.39 0.25 0.13 0.18 0.25 0.29 0.30
ZeroRF[66] 0.30 0.61 0.50 0.39 0.59 0.63 0.49 0.68 0.88 0.82 0.73 0.63 0.60
10.99 14.40 13.93 12.16 15.41 16.73 11.24 17.08 20.39 15.36 16.23 14.12 14.84
0.25 0.16 0.20 0.24 0.24 0.17 0.16 0.13 0.09 0.07 0.13 0.11 0.16
FrugalNeRF(Ours) 0.57 0.73 0.73 0.64 0.73 0.78 0.77 0.86 0.92 0.92 0.85 0.89 0.78
14.67 17.86 19.47 17.66 14.51 19.74 16.94 24.87 21.21 22.67 21.45 25.60 19.72
0.25 0.15 0.19 0.21 0.23 0.16 0.15 0.12 0.08 0.07 0.10 0.10 0.15
FrugalNeRFw/mono.depth(Ours) 0.56 0.73 0.75 0.68 0.74 0.79 0.78 0.86 0.93 0.91 0.88 0.90 0.79
14.14 18.46 21.27 19.40 15.56 20.53 18.05 25.65 23.46 22.72 23.76 26.25 20.77
Table10.QuantitativeresultsontheDTU[32]datasetwiththreeinputviews.ThethreerowsshowLPIPS,SSIMandPSNRscores,
respectively.
Scene Scan21 Scan31 Scan34 Scan38 Scan40 Scan41 Scan45 Scan55 Scan63 Scan82 Scan103 Scan114 Average
Method
15.93 19.53 23.23 19.88 18.38 22.83 21.07 22.88 25.28 26.39 26.68 26.68 22.40
FreeNeRF[91] 0.58 0.76 0.80 0.70 0.80 0.84 0.84 0.80 0.94 0.94 0.92 0.90 0.82
15.93 19.53 23.23 19.88 18.38 22.83 21.07 22.88 25.28 26.39 26.68 26.68 22.40
0.34 0.18 0.26 0.32 0.32 0.28 0.22 0.22 0.09 0.11 0.12 0.12 0.22
ViP-NeRF[70] 0.33 0.58 0.58 0.53 0.47 0.55 0.50 0.43 0.66 0.65 0.77 0.60 0.55
12.97 16.58 18.63 16.12 14.82 16.25 14.14 18.04 17.67 14.75 20.85 18.65 16.62
0.22 0.32 0.24 0.24 0.28 0.27 0.23 0.15 0.31 0.36 0.17 0.25 0.25
SimpleNeRF[71] 0.74 0.68 0.74 0.75 0.75 0.77 0.79 0.90 0.77 0.67 0.84 0.81 0.77
12.90 11.29 14.17 13.42 11.44 12.23 15.31 20.41 13.97 10.93 17.41 14.66 14.01
0.28 0.38 0.29 0.26 0.28 0.27 0.38 0.16 0.51 0.47 0.29 0.15 0.31
VGOS[75] 0.69 0.65 0.71 0.76 0.74 0.76 0.62 0.90 0.58 0.58 0.75 0.87 0.72
9.84 8.34 10.50 11.91 8.51 9.14 7.27 18.86 5.38 5.80 11.81 16.74 10.34
0.23 0.12 0.15 0.37 0.14 0.14 0.12 0.14 0.04 0.04 0.11 0.08 0.14
SparseNeRF[81] 0.63 0.81 0.79 0.59 0.84 0.84 0.84 0.84 0.96 0.95 0.90 0.92 0.83
17.14 21.11 24.88 12.36 22.25 23.05 20.85 19.75 27.52 28.98 23.74 28.00 22.47
0.45 0.36 0.41 0.45 0.29 0.30 0.33 0.27 0.19 0.19 0.24 0.30 0.31
ZeroRF[66] 0.33 0.55 0.47 0.41 0.65 0.68 0.57 0.68 0.84 0.83 0.74 0.63 0.61
11.55 12.43 11.81 12.84 15.66 16.01 12.77 16.50 17.81 15.34 16.64 14.25 14.47
0.19 0.14 0.18 0.22 0.21 0.13 0.13 0.12 0.06 0.05 0.10 0.11 0.14
FrugalNeRF(Ours) 0.69 0.76 0.77 0.69 0.79 0.84 0.82 0.89 0.94 0.94 0.89 0.90 0.83
17.38 19.06 22.38 18.96 17.77 24.01 20.35 26.11 24.57 25.85 25.43 27.28 22.43
0.19 0.13 0.17 0.21 0.20 0.13 0.13 0.12 0.06 0.05 0.08 0.10 0.13
FrugalNeRFw/mono.depth(Ours) 0.68 0.78 0.78 0.73 0.79 0.84 0.82 0.88 0.95 0.93 0.91 0.91 0.83
17.14 19.89 23.17 20.33 17.18 23.71 20.59 26.60 25.52 25.04 27.84 27.10 22.84
21Table11.QuantitativeresultsontheDTU[32]datasetwithfourinputviews.ThethreerowsshowLPIPS,SSIMandPSNRscores,
respectively.
Scene Scan21 Scan31 Scan34 Scan38 Scan40 Scan41 Scan45 Scan55 Scan63 Scan82 Scan103 Scan114 Average
Method
0.18 0.14 0.13 0.24 0.14 0.12 0.09 0.06 0.04 0.03 0.08 0.07 0.11
FreeNeRF[91] 0.72 0.81 0.83 0.72 0.85 0.86 0.86 0.92 0.96 0.96 0.93 0.93 0.86
18.72 21.29 25.97 19.43 22.88 25.59 22.39 28.63 27.35 31.51 27.30 28.65 24.98
0.33 0.19 0.21 0.31 0.35 0.24 0.23 0.24 0.08 0.08 0.10 0.12 0.21
ViP-NeRF[70] 0.39 0.61 0.59 0.59 0.45 0.61 0.52 0.38 0.67 0.67 0.76 0.64 0.57
14.24 17.22 19.44 18.19 15.76 18.84 15.57 16.62 17.19 16.45 22.67 19.50 17.64
0.27 0.28 0.23 0.25 0.32 0.27 0.25 0.21 0.27 0.27 0.18 0.29 0.26
SimpleNeRF[71] 0.71 0.73 0.78 0.75 0.72 0.76 0.78 0.88 0.82 0.80 0.84 0.81 0.78
11.81 12.95 14.72 12.71 10.42 11.67 14.12 18.84 14.05 14.43 16.87 14.23 13.90
0.27 0.35 0.31 0.28 0.27 0.27 0.37 0.16 0.43 0.42 0.28 0.18 0.30
VGOS[75] 0.73 0.69 0.71 0.74 0.76 0.78 0.64 0.90 0.66 0.66 0.75 0.85 0.74
11.09 9.53 10.57 11.15 9.12 10.00 8.10 19.53 6.55 7.14 12.69 15.65 10.93
0.16 0.14 0.15 0.21 0.21 0.14 0.10 0.09 0.04 0.05 0.09 0.06 0.12
SparseNeRF[81] 0.72 0.80 0.85 0.74 0.80 0.86 0.86 0.88 0.95 0.95 0.93 0.93 0.86
18.60 20.99 25.87 20.92 19.45 24.81 22.15 26.37 26.20 26.72 28.10 28.19 24.03
0.43 0.32 0.28 0.44 0.28 0.25 0.20 0.29 0.17 0.14 0.26 0.32 0.28
ZeroRF[66] 0.36 0.62 0.66 0.47 0.68 0.73 0.73 0.67 0.87 0.87 0.72 0.62 0.67
11.75 13.48 16.47 13.53 16.87 17.26 16.48 15.92 19.33 19.12 15.18 13.36 15.73
0.17 0.12 0.16 0.17 0.19 0.12 0.12 0.12 0.05 0.04 0.07 0.10 0.12
FrugalNeRF(Ours) 0.73 0.81 0.81 0.79 0.81 0.85 0.85 0.89 0.95 0.95 0.93 0.92 0.86
19.21 21.84 24.99 23.08 19.47 25.64 21.59 27.31 26.27 27.26 29.27 28.21 24.51
0.17 0.12 0.15 0.17 0.19 0.12 0.11 0.12 0.05 0.03 0.07 0.09 0.12
FrugalNeRFw/mono.depth(Ours) 0.73 0.81 0.82 0.80 0.82 0.86 0.86 0.90 0.96 0.95 0.93 0.92 0.86
19.07 21.65 25.82 23.13 18.96 25.55 22.21 28.02 26.87 28.28 29.27 28.92 24.81
Table12.QuantitativeresultsontheRealEstate-10K[100]dataset.ForSimpleNeRF[71]andViP-NeRF[70],wecalculatemetricsusing
testingdataprovidedintheirrespectiveclouds.Asforothermodels,werelyonthescoresprovidedintheSimpleNeRFpaper.
Learned 2-view 3-view 4-view Training
Method Venue priors PSNR‚Üë SSIM‚Üë LPIPS‚Üì PSNR‚Üë SSIM‚Üë LPIPS‚Üì PSNR‚Üë SSIM‚Üë LPIPS‚Üì time‚Üì
RegNeRF[53] CVPR2022 normalizingflow 16.87 0.59 0.45 17.73 0.61 0.44 18.25 0.62 0.44 2.35hrs
DS-NeRF[21] CVPR2022 - 25.44 0.79 0.32 25.94 0.79 0.32 26.28 0.79 0.33 3.5hrs
DDP-NeRF[61] CVPR2022 depthcompletion 26.15 0.85 0.15 25.92 0.85 0.16 26.48 0.86 0.16 3.5hrs
FreeNeRF[91] CVPR2023 - 14.50 0.54 0.55 15.12 0.57 0.54 16.25 0.60 0.54 1.5hrs
ViP-NeRF[70] SIGGRAPH2023 - 29.55 0.87 0.09 29.75 0.88 0.11 30.47 0.88 0.11 13.5hrs
SimpleNeRF[71] SIGGRAPHAsia2023 - 30.30 0.88 0.07 31.40 0.89 0.08 31.73 0.89 0.09 9.5hrs
FrugalNeRF(Ours) - - 30.12 0.87 0.07 31.04 0.89 0.06 31.78 0.90 0.06 20mins
22Table13.QuantitativeresultsontheRealEstate-10K[100]datasetwithtwoinputviews.ThethreerowsshowLPIPS,SSIM,and
PSNRscores,respectively.
Scene 0 1 3 4 6 Average
Method
0.35 0.32 0.49 0.54 0.54 0.45
RegNeRF[53] 0.60 0.83 0.30 0.61 0.59 0.59
16.51 21.04 13.88 17.13 15.79 16.87
0.26 0.27 0.51 0.24 0.31 0.32
DS-NeRF[21] 0.81 0.91 0.50 0.88 0.83 0.79
24.68 27.93 19.24 29.18 26.18 25.44
0.11 0.12 0.34 0.06 0.11 0.15
DDP-NeRF[61] 0.89 0.95 0.56 0.94 0.92 0.85
25.90 25.87 18.97 32.01 28.00 26.15
0.45 0.50 0.64 0.67 0.48 0.55
FreeNeRF[91] 0.54 0.77 0.28 0.49 0.58 0.53
15.00 17.00 12.15 12.84 15.50 14.50
0.05 0.05 0.22 0.04 0.08 0.09
ViP-NeRF[70] 0.94 0.97 0.56 0.95 0.93 0.87
30.41 32.03 18.96 34.74 31.61 29.55
0.04 0.04 0.21 0.03 0.05 0.07
SimpleNeRF[71] 0.95 0.97 0.56 0.95 0.96 0.88
31.89 33.8 18.65 34.93 32.24 30.30
0.04 0.04 0.20 0.04 0.05 0.07
FrugalNeRF(Ours) 0.94 0.97 0.56 0.95 0.95 0.87
30.13 34.69 18.35 35.00 32.45 30.12
Table14.QuantitativeresultsontheRealEstate-10K[100]datasetwiththreeinputviews.ThethreerowsshowLPIPS,SSIM,and
PSNRscores,respectively.
Scene 0 1 3 4 6 Average
Method
0.40 0.32 0.53 0.56 0.37 0.44
RegNeRF[53] 0.60 0.82 0.29 0.62 0.71 0.61
15.99 20.89 13.87 17.60 20.28 17.73
0.24 0.26 0.53 0.26 0.31 0.32
DS-NeRF[21] 0.83 0.91 0.49 0.87 0.85 0.79
25.24 28.68 19.14 29.08 27.58 25.94
0.11 0.11 0.38 0.06 0.13 0.16
DDP-NeRF[61] 0.89 0.96 0.55 0.94 0.92 0.85
25.27 26.67 18.81 31.84 26.99 25.92
0.54 0.51 0.64 0.59 0.42 0.54
FreeNeRF[91] 0.53 0.75 0.29 0.61 0.66 0.57
13.79 15.59 12.45 15.72 18.05 15.12
0.06 0.10 0.26 0.04 0.08 0.11
ViP-NeRF[70] 0.94 0.95 0.60 0.95 0.95 0.88
30.66 29.89 19.59 35.17 33.43 29.75
0.04 0.04 0.23 0.03 0.08 0.08
SimpleNeRF[71] 0.95 0.98 0.61 0.95 0.95 0.89
32.23 36.44 19.65 35.85 32.81 31.40
0.04 0.03 0.18 0.03 0.04 0.06
FrugalNeRF(Ours) 0.95 0.98 0.61 0.95 0.96 0.89
31.11 35.39 18.85 35.78 34.07 31.04
23Table15.QuantitativeresultsontheRealEstate-10K[100]datasetwithfourinputviews.ThethreerowsshowLPIPS,SSIM,and
PSNRscores,respectively.
Scene 0 1 3 4 6 Average
Method
0.43 0.35 0.59 0.56 0.27 0.44
RegNeRF[53] 0.59 0.83 0.29 0.65 0.75 0.62
16.09 20.98 13.91 18.48 21.78 18.25
0.27 0.26 0.56 0.25 0.31 0.33
DS-NeRF[21] 0.82 0.92 0.50 0.87 0.85 0.79
25.40 29.40 19.64 29.26 27.69 26.28
0.12 0.08 0.39 0.06 0.13 0.16
DDP-NeRF[61] 0.89 0.96 0.58 0.93 0.91 0.86
25.14 28.57 19.57 31.73 27.36 26.48
0.56 0.48 0.65 0.58 0.39 0.53
FreeNeRF[91] 0.53 0.80 0.31 0.66 0.69 0.60
13.84 17.93 12.69 17.29 19.48 16.25
0.06 0.08 0.27 0.05 0.09 0.11
ViP-NeRF[70] 0.94 0.96 0.62 0.94 0.95 0.88
31.64 32.24 20.35 34.84 33.28 30.47
0.04 0.05 0.24 0.03 0.09 0.09
SimpleNeRF[71] 0.96 0.97 0.64 0.95 0.94 0.89
32.95 36.44 20.52 35.97 32.77 31.73
0.04 0.03 0.17 0.03 0.05 0.06
FrugalNeRF(Ours) 0.96 0.98 0.64 0.95 0.96 0.90
32.29 36.06 19.81 36.54 34.22 31.78
24FrugalNeRF (Ours) SimpleNeRF SparseNeRF FSGS
Figure10.MorequalitativecomparisonsontheLLFF[48]datasetwithtwoinputviews.FrugalNeRFachievesbettersynthesisquality
indifferentscenes.
25FrugalNeRF(Ours) FrugalNeRF(w/ mono. depth) FreeNeRF SimpleNeRF SparseNeRF(w/ mono. depth) ZeroRF FSGS(w/ mono. depth) Ground Truth
Figure11.MorequalitativecomparisonsontheDTU[32]datasetwithtwoinputviews.FrugalNeRFachievesbettersynthesisqualityin
differentscenes.
FrugalNeRF(Ours) ViP-NeRF SimpleNeRF Ground Truth
Figure12.QualitativecomparisonsontheRealEstate-10K[100]datasetwithtwoinputviews.ComparedtoVip-NeRF[70]and
SimpleNeRF[71],ourFrugalNeRFrenderssharperdetailsinthescene.
26FrugalNeRF(Ours) ViP-NeRF SimpleNeRF Ground Truth
Figure13.MorequalitativecomparisonsontheRealEstate-10K[100]datasetwithtwoinputviews.FrugalNeRFachievessynthesis
qualitycomparabletothestate-of-the-artmethods.
27