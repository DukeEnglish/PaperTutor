Preprint
SAM2LONG: ENHANCING SAM 2 FOR LONG VIDEO
SEGMENTATION WITH A TRAINING-FREE MEMORY
TREE
ShuangruiDing1 RuiQian1 XiaoyiDong1,2 PanZhang2
YuhangZang2 YuhangCao2 YuweiGuo1 DahuaLin1 JiaqiWang2
1TheChineseUniversityofHongKong 2ShanghaiArtificialIntelligenceLaboratory
ABSTRACT
TheSegmentAnythingModel2(SAM2)hasemergedasapowerfulfoundation
model for object segmentation in both images and videos, paving the way for
variousdownstreamvideoapplications. ThecrucialdesignofSAM2forvideo
segmentationisitsmemorymodule,whichpromptsobject-awarememoriesfrom
previousframesforcurrentframeprediction. However,itsgreedy-selectionmem-
orydesignsuffersfromthe“erroraccumulation”problem, whereanerroredor
missedmaskwillcascadeandinfluencethesegmentationofthesubsequentframes,
whichlimitstheperformanceofSAM2towardcomplexlong-termvideos. Tothis
end,weintroduceSAM2Long,animprovedtraining-freevideoobjectsegmen-
tationstrategy,whichconsidersthesegmentationuncertaintywithineachframe
andchoosesthevideo-leveloptimalresultsfrommultiplesegmentationpathways
inaconstrainedtreesearchmanner. Inpractice,wemaintainafixednumberof
segmentationpathwaysthroughoutthevideo. Foreachframe,multiplemasksare
proposedbasedontheexistingpathways,creatingvariouscandidatebranches. We
thenselectthesamefixednumberofbrancheswithhighercumulativescoresasthe
newpathwaysforthenextframe. Afterprocessingthefinalframe,thepathway
withthehighestcumulativescoreischosenasthefinalsegmentationresult. Bene-
fitingfromitsheuristicsearchdesign,SAM2Longisrobusttowardocclusionsand
objectreappearances,andcaneffectivelysegmentandtrackobjectsforcomplex
long-termvideos. Withoutintroducinganyadditionalparametersorfurthertrain-
ing,SAM2LongsignificantlyandconsistentlyoutperformsSAM2onfiveVOS
benchmarks. Notably,SAM2Longachievesanaverageimprovementof3.0points
acrossall24head-to-headcomparisons,withgainsofupto5.3pointsinJ&F on
long-termvideoobjectsegmentationbenchmarkssuchasSA-VandLVOS.The
codeisreleasedathttps://github.com/Mark12Ding/SAM2Long.
1 INTRODUCTION
TheSegmentAnythingModel2(SAM2)hasgainedsignificantattentionasaunifiedfoundational
modelforpromptableobjectsegmentationinbothimagesandvideos. Notably,SAM2(Ravietal.,
2024) has achieved state-of-the-art performance across various video object segmentation tasks,
significantlysurpassingpreviousmethods. BuildingupontheoriginalSAM(Kirillovetal.,2023),
SAM2incorporatesamemorymodulethatenablesittogeneratemaskletpredictionsusingstored
memorycontextsfrompreviouslyobservedframes. ThismoduleallowsSAM2toseamlesslyextend
SAMintothevideodomain,processingvideoframessequentially,attendingtothepriormemoriesof
thetargetobject,andmaintainingobjectcoherenceovertime.
While SAM 2 demonstrates strong performance in video segmentation, its greedy segmentation
strategystrugglestohandlecomplexvideoscenarioswithfrequentocclusionsandobjectreappearance.
Indetail, SAM2confidentlyandaccuratelysegmentsframeswhenclearvisualcuesarepresent.
However,inscenarioswithocclusionsorreappearingobjects,itcanproducemaskproposalsthat
arehighlyvariableanduncertain. Regardlessoftheframe’scomplexity,auniformgreedyselection
strategyisappliedtobothscenarios:themaskwiththehighestpredictedIoUisselected. Suchgreedy
1
4202
tcO
12
]VC.sc[
1v86261.0142:viXraPreprint
Time Flow
SAM2’s Prediction Wrong Tracking Error Accumulates
⚠ Occlusion Occurs
SAM2Long’s Prediction Occlusion Solved Object Reappears
Elapsed Time (seconds)
(a) Comparison in handling object occlusion over time. (b) Per-frame performance comparison across three benchmarks.
Figure 1: Comparison of occlusion handling and long-term compatibility between SAM 2 and
SAM2Long. (a) When an occlusion occurs, SAM 2 may lose track or follow the wrong object,
leadingtoaccumulatederrors. Incontrast,SAM2Longutilizesmemorytreesearchtorecoverwhen
theobjectreappears. (b)Theper-frameJ&F scoresofthepredictedmasksareplottedatspecific
timestampsontheLVOSandSA-Vdatasets. SAM2Longdemonstratesgreaterresiliencetoelapsed
timecomparedtoSAM2,maintainingsuperiorperformanceoverlongerperiods.
choiceworkswellfortheeasycasesbutraisestheerrorpotentialforthechallengingframes. Oncean
incorrectmaskisselectedintomemory,itisuncorrectableandwillmisleadthesegmentationofthe
subsequentframes. Weshowsuchan“erroraccumulation”probleminFigure1bothqualitativelyand
quantitatively. TheperformanceofSAM2progressivelydeterioratesasthepropagationextendsinto
thelatertemporalsegment,highlightingitslimitationsinmaintainingaccuratetrackingovertime.
To this end, we redesign the memory module of SAM 2 to enhance its long-term compatibility
androbustnessagainstocclusionsanderrorpropagation. Ourimprovementiscompletelyfreeof
additionaltraininganddoesnotintroduceanyexternalparameters,butsimplyunleashesthepotential
of SAM 2 itself. Our approach is motivated by the observation that the SAM 2 mask decoder
generates multiple diverse masks, accompanied by predicted IoU scores and an occlusion score
whenhandlingchallengingandambiguouscases. However,SAM2onlyselectsasinglemaskas
memory, sometimes disregarding the correct one. To address this, we aim to equip SAM 2 with
multiplememorypathways,allowingvariousmaskstobestoredasmemoryateachtimestep,thereby
improvingpredictionsforsubsequentframes.
In particular, we introduce a novel constrained tree memory structure, which maintains a fixed
numberofmemorypathwaysovertimetoexploremultiplesegmentationhypotheseswithefficiently
managedcomputationalresources. Ateachtimestep, basedonasetofmemorypathways, each
withitsownmemorybankandcumulativescore(accumulatedlogarithmofthepredictedIoUscores
acrossthepathway),weproducemultiplecandidatebranchesforthecurrentframe. Then,among
allthebranches,weselectthesamefixednumberofbrancheswithhighercumulativescoresand
pruneotherbranches,therebyconstrainingthetree’sgrowth. Afterprocessingthefinalframe,the
pathwaywiththehighestcumulativescoreisselectedasthefinalsegmentationresult. Moreover,to
preventprematureconvergenceonincorrectpredictions,weselecthypotheseswithdistinctpredicted
masks when their occlusion scores indicate uncertainty, in order to maintain diversity in the tree
branches. Thistree-likememorystructureaugmentsSAM2’sabilitytoeffectivelyovercomeerror
accumulation.
Withineachpathway,weconstructanobject-awarememorybankthatselectivelyincludesframes
with confidently detected objects and high-quality segmentation masks, based on the predicted
occlusionscoresandIoUscores. InsteadofsimplystoringthenearestframesasSAM2does,we
filter out frames where the object may be occluded or poorly segmented. This ensures that the
memorybankprovideseffectiveobjectcuesforthecurrentframe’ssegmentation. Additionally,we
modulatethememoryattentioncalculationbyweightingmemoryentriesaccordingtotheirocclusion
scores,emphasizingmorereliableentriesduringcross-attention. ThesestrategieshelpSAM2focus
onreliableobjectcluesandimprovesegmentationaccuracywithnegligiblecomputationaloverhead.
AsevidencedinFigure1(a),ourapproachsuccessfullyresolvesocclusionsandre-trackstherecurring
balloon,whereSAM2fails.
2
)erocSF&J(
ecnamrofrePPreprint
WeprovideacomprehensiveevaluationdemonstratingthatSAM2Longconsistentlyoutperforms
SAM2acrosssixVOSbenchmarks,particularlyexcellinginlong-termandocclusion-heavyscenarios.
Forinstance,onthechallengingSA-Vtestset,SAM2Long-LimprovestheJ&F scoreby5.3points,
andSAM2Long-Sshowsanimpressive4.7-pointgainoverthesamesizeSAM2modelonSA-V
val set. Similar trends are observed on the LVOS validation set, where SAM2Long-S surpasses
SAM 2-S by 3.5 points. These consistent improvements across different model sizes, including
bothSAM2andthemorerecentSAM2.1modelweights,clearlydemonstratetheeffectivenessof
ourproposedmethod. Furthermore, asillustratedinFigure1(b), theper-frameperformancegap
betweenSAM2LongandSAM2widensovertime, showcasingSAM2Longexcelsinlong-term
trackingscenarios. Withtheseresults,webelieveSAM2Longsetsanewstandardforvideoobject
segmentationbasedonSAM2incomplex,real-worldapplications,deliveringsuperiorperformance
withoutanyadditionaltrainingorexternalparameters.
2 RELATED WORK
2.1 VIDEOOBJECTSEGMENTATION
Perceiving the environment in terms of objects is a fundamental cognitive ability of humans. In
computervision,VideoObjectSegmentation(VOS)tasksaimtoreplicatethiscapabilitybyrequiring
models to segment and track specified objects within video sequences. A substantial amount of
researchhasbeenconductedonvideoobjectsegmentationinrecentdecades(Fanetal.,2019;Oh
etal.,2019;Huetal.,2018a;Ohetal.,2018;Perazzietal.,2017;Wangetal.,2019;Huetal.,2018b;
Li&Loy,2018;Baoetal.,2018;Zhangetal.,2019;Lietal.,2020;Johnanderetal.,2019;Zhang
etal.,2023;Venturaetal.,2019;Lietal.,2022;Wuetal.,2023;Wangetal.,2023;Qianetal.,2023;
2025;Dingetal.,2022;2023b).
There are two main protocolsfor evaluating VOS models(Pont-Tuset et al., 2017; Perazzi et al.,
2016): semi-supervised and unsupervised video object segmentation. In semi-supervised VOS,
the first-frame mask of the objects of interest is provided, and the model tracks these objects in
subsequentframes. InunsupervisedVOS,themodeldirectlysegmentsthemostsalientobjectsfrom
thebackgroundwithoutanyreference. Itisimportanttonotethattheseprotocolsaredefinedinthe
inferencephase,andVOSmethodscanleveragegroundtruthannotationsduringthetrainingstage.
In this paper, we explore SAM 2 (Ravi et al., 2024), for its application in semi-supervised VOS.
WeenhancethememorydesignofSAM2,significantlyimprovingmaskpropagationperformance
withoutrequiringanyadditionaltraining.
2.2 MEMORY-BASEDVOS
Videoobjectsegmentationremainsanunsolvedchallengeduetotheinherentcomplexityofvideo
scenes. Objects in videos can undergo deformation (Tokmakov et al., 2023), exhibit dynamic
motion(Brox&Malik,2010),reappearoverlongdurations(Hongetal.,2024;2023),andexperience
occlusion(Dingetal.,2023a),amongotherchallenges. Toaddresstheabovechallenges,adoptinga
memoryarchitecturetostoretheobjectinformationfrompastframesisindispensableforaccurately
trackingobjectsinvideo. Previousmethods(Bhatetal.,2020;Caellesetal.,2017;Maninisetal.,
2018; Robinson et al., 2020; Voigtlaender & Leibe, 2017) treat VOS as an online learning task,
where networks are test-time tuned on the first-frame annotation. However, this approach was
time-consumingduetotest-timefine-tuning. Othertechniques(Chenetal.,2018;Huetal.,2018b;
Voigtlaenderetal.,2019;Yangetal.,2018;2020;2021b)usetemplatematching,buttheylackthe
capabilityoftrackingunderocclusion.
More recent approaches have introduced efficient memory reading mechanisms, utilizing either
pixel-levelattention(Chengetal.,2023;Zhouetal.,2024;Dukeetal.,2021;Liangetal.,2020;Oh
etal.,2018;Seongetal.,2020;Cheng&Schwing,2022;Xieetal.,2021;Yang&Yang,2022;Yang
etal.,2021a)orobject-levelattention(Atharetal.,2023;2022;Chengetal.,2024). Aprominent
exampleisXMem(Cheng&Schwing,2022),whichleveragesahierarchicalmemorystructurefor
pixel-levelmemoryreadingcombined. BuildingonXMem’sframework,Cutie(Chengetal.,2024)
further improves segmentation accuracy by processing pixel features at the object level to better
handlecomplexscenarios.
3Preprint
ThelatestSAM2(Ravietal.,2024)incorporatesasimplememorymoduleontopoftheimage-based
SAM(Kirillovetal.,2023),enablingittofunctionforVOStasks. However,byselectingonlythe
temporallynearestframesasmemory,SAM2struggleswithchallengingcasesinvolvinglong-term
reappearing objects and confusingly similar objects. we redesign SAM 2’s memory to maintain
multiplepotentialcorrectmasks,makingthemodelmoreobject-awareandrobust.
2.3 SEGMENTANYTHINGMODEL
SegmentAnythingModel(SAM)(Kirillovetal.,2023)isrecognizedasamilestonevisionfoundation
modelthatcansegmentanyobjectinanimageusinginteractiveprompts. Itsimpressivezero-shot
transfer performance has shown great versatility in various vision tasks, including segmentation
applications(Lietal.,2023;Maetal.,2024;Xuetal.,2024),imageediting(Gaoetal.,2023)and
objectreconstruction(Linetal.,2024).
BuildingonSAM,SAM2(Ravietal.,2024)extendsitsfunctionalitytovideosegmentationthrough
amemory-basedtransformerarchitectureforreal-timevideoprocessing. SAM2’smemorystores
informationaboutobjectsandpastinteractions,enablingittogeneratesegmentationmasksacross
videoframesmoreaccuratelyandefficientlythanpreviousmethods. TofurtherenhanceSAM2,we
introduceaconstrainedmemorytreestructure.Thistraining-freedesignleveragestheSAM2’sability
togeneratemultiplecandidatemaskproposalswithpredictedIoUandocclusionscore,mitigating
erroraccumulationduringsegmentation.
3 METHOD
3.1 PRELIMINARYONSAM2
SAM2(Ravietal.,2024)beginswithanimageencoderthatencodeseachinputframeintoem-
beddings. In contrast to SAM, where frame embeddings are fed directly into the mask decoder,
SAM2incorporatesamemorymodulethatconditionsthecurrentframe’sfeaturesonbothprevious
andpromptedframes. Specifically,forsemi-supervisedvideoobjectsegmentationtasks,SAM2
maintainsamemorybankateachtimestept≥1:
M
=(cid:8)
M
∈RK×C(cid:9)
,
t τ τ∈I
whereK isthenumberofmemorytokensperframe,C isthechanneldimension,andI isthesetof
frameindicesincludedinthememory. InSAM2,memorysetI storesuptoN ofthemostrecent
frames,alongwiththeinitialmask,usingaFirst-In-First-Out(FIFO)queuemechanism.
Eachmemoryentryconsistsoftwocomponents: (1)thespatialembeddingfusedwiththepredicted
mask(generatedbythememoryencoder),and(2)theobject-levelpointer(generatedbythemask
decoder).Aftercross-attendingtothememory,thecurrentframe’sfeaturesintegratebothfine-grained
correspondencesandobject-levelsemanticinformation.1
The mask decoder, which is lightweight and retains the efficiency of SAM, then generates three
predicted masks for the current frame. Each mask is accompanied by a predicted Intersection
over Union (IoU) score IoU ≥ 0 and an output mask token. Additionally, the mask decoder
t
predictsasingleocclusionscoreo fortheframe,whereo > 0indicatesobjectpresence,o < 0
t t t
indicatesabsence,andtheabsolutevalue|o |depictsthemodel’sconfidence. Themaskwiththe
t
highestpredictedIoUscoreisselectedasthefinalprediction,anditscorrespondingoutputtokenis
transformedintotheobjectpointerforuseasthememory.
3.2 CONSTRAINEDTREEMEMORYWITHUNCERTAINTYHANDLING
ToenhanceSAM2’srobustnesstowardslong-termandambiguouscases,weproposeaconstrained
treememorystructurethatenablesthemodeltoexplorevariousobjectstatesovertimewithminimal
computationaloverhead. Weshowthehigh-levelpipelineinFigure2. Thistree-basedapproachmain-
tainsmultipleplausiblepathwaysandmitigatestheeffectsofocclusionsanderroneouspredictions.
1Inpractice,SAM2storesmoreobjectpointersthanspatialembeddings,aspointersarelighter.Weassume
equalnumbersofbothcomponentssolelyforillustrativepurposes,withoutalteringtheactualimplementation.
4Preprint
𝑆"𝑡−1=0.9 𝑆","𝑡 =1.5 𝑆"𝑡 =1.6 𝑆","𝑡+1=2.0 𝑆"𝑡+1=2.4 If Certain
Memory Bank 1 Memory Bank1 Memory Bank 1 Occlusion Score 𝑜"=9>𝛿’()* 𝑜!=8>𝛿’()*
Cumulative
⋮ DeM ca os dk er
𝑆",!𝑡 =1.6
M upe dm ao tery ⋮ DeM ca os dk er
𝑆",!𝑡+1=2.1
⋮ ⋯
MaIo skU
c
s ac no dre
id
𝑆 a% t, e&
s
1.5 1.6 1.5 1.6 1.4 1.5
Frame t
𝑆",$ M𝑡 a= sk1 .3
Frame t + 1
𝑆",$𝑡 M+ a1
sk
=2.2
Mem uor py date High-scoring Mask
If Uncertain
Selection Selection
𝑆!,"𝑡 =1.5 𝑆!,"𝑡+1=2.4 Occlusion Score 𝑜"=1<𝛿’()* 𝑜!=0.6<𝛿’()*
Cumulative
IoU score 𝑆%,& 1.4 1.7 1.6 1.5 1.7 1.5
⋮ Mask
𝑆!,!𝑡 =1.6
Memory ⋮ Mask
𝑆!,!𝑡+1=2.3
Memory ⋮ ⋯ Mask candidates
Decoder update Decoder update
Memory Bank 2 𝑆!,$𝑡 =1.4 Memory Bank 2 𝑆!,$𝑡+1=2.1 Memory Bank 2
𝑆!𝑡−1=0.9 𝑆!𝑡 =1.6 𝑆!𝑡+1=2.3 Distinct-shaped Mask
(a) The Pipeline of ConstrainedMemory Tree (b) Mask Selection with Uncertainty Handling
Figure2: (a)Thepipelineofconstrainedmemorytree: Ateachtimestept,wemaintainmultiple
memorypathways,eachcontainingamemorybankandacumulativescoreS [t]. Theinputframe
p
is processed through the mask decoder conditioned on the memory bank, generating three mask
candidates for each pathway. The candidates with the highest updated cumulative scores S [t]
p,k
arecarriedforwardtothenexttimestep. (b)Maskselectionwithuncertaintyhandling: Whenthe
maximumabsoluteocclusionscoreexceedsthethresholdδ (Certain),thehigh-scoringmaskis
conf
selected. Otherwise(Uncertain),distinctmaskcandidatesarepickedtoavoidincorrectconvergence.
Specifically, ateachtimestept, wemaintainasetofP memorypathways, eachwithamemory
bankMpandacumulativescoreS [t],representingapossiblesegmentationhypothesisuptoframe
t p
t. Conditionedonthememorybankofeachpathwayp,theSAM2decoderheadgeneratesthree
maskcandidatesalongwiththeirpredictedIoUscores,denotedasIoUp,1,IoUp,2,andIoUp,3. This
t t t
processexpandsthetreebybranchingeachexistingpathwayintothreenewcandidates. Asaresult,
thereareatotalof3P possiblepathwaysateachtimestep. Wethencalculatethecumulativescores
foreachpossiblepathwaybyaddingthelogarithmofitsIoUscoretothepathway’spreviousscore:
S [t]=S [t−1]+log(IoUp,k+ϵ), fork =1,2,3,
p,k p t
whereϵisasmallconstanttopreventthelogarithmofzero.
However,continuouslytriplingthepathwayswouldleadtounacceptablecomputationalandmemory
costs. Therefore,tomanagecomputationalcomplexityandmemoryusage,weimplementapruning
strategythatselectsthetopP pathwayswiththehighestcumulativescorestocarryforwardtothe
next time step. This selection not only retains the most promising segmentation hypotheses but
alsoconstrainsthetree-basedmemory,ensuringcomputationalefficiency. Finally,weoutputthe
segmentationpathwaywiththehighestcumulativescoreastheultimateresult.
Compared to SAM 2, our approach introduces additional computation mainly by increasing the
numberofpassesthroughthemaskdecoderandmemorymodule. Notably,thesecomponentsare
lightweightrelativetotheimageencoder. Forinstance,theimageencoderofSAM2-Largeconsists
of 212M parameters while the total parameter of SAM 2-Large is 224M. Since we process the
imageencoderonlyoncejustasSAM2does, theintroductionofamemorytreeaddsnegligible
computationalcostwhilesignificantlyenhancingSAM2’srobustnessagainsterror-pronecases.
UncertaintyHandling. Unfortunately,therearetimeswhenallpathwaysareuncertain. Toprevent
themodelfromimproperlyconvergingonincorrectpredictions,weimplementastrategytomaintain
diversity among the pathways by deliberately selecting distinct masks. That is, if the maximum
absoluteocclusionscoreacrossallpathwaysattimet, max({|op|}P ), islessthanapredefined
t p=1
uncertaintythresholdδ ,weenforcethemodeltoselectmaskcandidateswithuniqueIoUvalues.
conf
This is inspired by the observation that, within the same frame, different IoU scores typically
correspondtodistinctmasks. Inpractice,weroundeachIoUscoreIoUp,k totwodecimalplacesand
t
onlyselectthosehypotheseswithdistinctroundedvalues.
Overall, the integration of constrained tree memory with uncertainty handling offers a balanced
strategythatleveragesmultiplesegmentationhypothesestoenhancerobustnesstowardthelong-term
5Preprint
complex video and achieve more accurate and reliable segmentation performance by effectively
mitigatingerroraccumulation.
3.3 OBJECT-AWAREMEMORYBANKCONSTRUCTION
Ineachmemorypathway,wedeviseobject-awarememoryselectiontoretrieveframeswithdiscrimi-
nativeobjects. Meanwhile,wemodulatethememoryattentioncalculationtofurtherstrengthenthe
model’sfocusonthetargetobjects.
MemoryFrameSelection. Toconstructamemorybankthatprovideseffectiveobjectcues, we
selectively choose frames from previous time steps based on the predicted object presence and
segmentationquality. Startingfromtheframeimmediatelybeforethecurrentframet,weiterate
backward through the prior frames i = {t−1,t−2,...,1} in sequence. For each frame i, we
retrieveitspredictedocclusionscoreo andIoUscoreIoU asreference. Weincludeframeiinthe
i i
memorybankifitsatisfiesthefollowingcriteria:
IoU >δ and o >0,
i IoU i
whereδ isapredefinedIoUthreshold. Thisensuresthatonlyframeswithconfidentlydetected
IoU
objects and reasonable segmentation masks contribute to the memory. We continue this process
untilwehaveselecteduptoN frames. IncontrasttoSAM2,whichdirectlypicksthenearestN
framesasthememoryentries,thisselectionprocesseffectivelyfiltersoutframeswheretheobject
maybeoccluded,absent,orpoorlysegmented,therebyprovidingmorerobustobjectcuesforthe
segmentationofthecurrentframe.
MemoryAttentionModulation. Tofurtheremphasizemorereliablememoryentriesduringthe
cross-attentioncomputation,weutilizetheassociatedocclusionscoreo tomodulatethecontribution
t
ofeachmemoryentry. AssumingthememorysetconsistsofN framesplustheinitialframe,totaling
N +1masks, wedefineasetofstandardweightsWstd thatarelinearlyspacedbetweenalower
boundw andanupperboundw :
low high
(cid:26)
i−1
(cid:27)N+1
Wstd = w + (w −w ) .
low N high low
i=1
Next,wesorttheocclusionscoresinascendingordertoobtainsortedindicesI′ ={I }N+1 such
i i=1
that:
o ≤o ≤···≤o .
I1 I2 IN+1
Wethenassignthestandardweightstothememoryentriesbasedonthesesortedindices:
w =Wstd, fori=1,2,...,N +1.
Ii i
Thisassignmentensuresthatmemoryentrieswithhigherocclusionscores,whichindicateobject
presencewithhigherconfidence,receivehigherweights. Then,welinearlyscaletheoriginalkeys
M withtheircorrespondingweights:
τ
M(cid:102)τ =w
τ
·M τ, forτ ∈I.
Finally, the modulated memory keys M(cid:102)t = {M(cid:102)τ}
τ∈I
are used in the memory module’s cross-
attentionmechanismtoupdatethecurrentframe’sfeatures. Byusingtheavailableocclusionscoresas
indicators,weeffectivelyemphasizememoryentrieswithmorereliableobjectcueswhileintroducing
minimalcomputationaloverhead.
4 EXPERIMENTS
4.1 DATASETS
Toevaluateourmethod,weselect6standardVOSbenchmarksandreportthefollowingmetrics: J
(regionsimilarity),F (contouraccuracy),andthecombinedJ&F. Allevaluationsareconductedin
asemi-supervisedsetting,wherethefirst-framemaskisprovided. Thedatasetsusedfortestingare
detailedasfollows:
6Preprint
SA-V(Ravietal.,2024)isalarge-scalevideosegmentationdatasetdesignedforpromptablevisual
segmentationacrossdiversescenarios. Itencompasses50.9Kvideoclips,aggregatingto642.6K
masklets with 35.5M meticulously annotated masks. The dataset presents a challenge with its
inclusionofsmall,occluded,andreappearingobjectsthroughoutthevideos. Thedatasetisdivided
intotraining,validation,andtestingsets,withmostvideosallocatedtothetrainingsetforrobust
modeltraining. Thevalidationsethas293maskletsacross155videosformodeltuning,whilethe
testingsetincludes278maskletsacross150videosforcomprehensiveevaluation.
LVOSv1(Hongetal.,2023)isaVOSbenchmarkforlong-termvideoobjectsegmentationinrealistic
scenarios.Itcomprises720videoclipswith296,401framesand407,945annotations,withanaverage
videodurationofover60seconds. LVOSintroduceschallengingelementssuchaslong-termobject
reappearanceandcross-temporalsimilarobjects. InLVOSv1,thedatasetincludes120videosfor
training,50forvalidation,and50fortesting.
LVOS v2 (Hong et al., 2024) expends LVOS v1 and provides 420 videos for training, 140 for
validation,and160fortesting. Thispaperprimarilyutilizesv2,asitalreadyincludesthesequences
presentinv1. Thedatasetspans44categories,capturingtypicaleverydayscenarios,with12ofthese
categoriesdeliberatelyleftunseentoevaluateandbetterassessthegeneralizationcapabilitiesofVOS
models.
MOSE(Dingetal.,2023a)isachallengingVOSdatasettargetedoncomplex,real-worldscenarios,
featuring2,149videoclipswith431,725high-qualitysegmentationmasks. Thesevideosaresplit
into1,507trainingvideos,311validationvideos,and331testingvideos.
VOST (Tokmakov et al., 2023) is a semi-supervised video object segmentation benchmark that
emphasizes complex object transformations. Unlike other datasets, VOST includes objects that
arebroken,torn,orreshaped,significantlyalteringtheirappearance. Itcomprisesmorethan700
high-resolution videos, captured in diverse settings, with an average duration of 21 seconds, all
denselylabeledwithinstancemasks.
PUMaVOS(Bekuzarovetal.,2023)isanovelvideodatasetdesignedforbenchmarkingchallenging
segmentationtasks. Itincludes24videoclips,eachrangingfrom13.5to60seconds(28.7seconds
onaverage)at480presolutionwithvaryingaspectratios. PUMaVOSfocusesondifficultscenarios
whereannotationboundariesdonotalignwithclearvisualcues,suchashalffaces,necks,tattoos,
andpimples,commonlyencounteredinvideoproduction.
4.2 MAINRESULTS
SAM2LongconsistentlyimprovesSAM2overallmodelsizesanddatasets. Table1presents
anoverallcomparisonbetweenSAM2andSAM2LongacrossvariousmodelsizesontheSA-V
validationandtestsets,aswellastheLVOSv2validationset. Intotal,thetableincludes8model
variations,coveringSAM2andthelatestSAM2.1acrossfourmodelsizes. Theaverageperformance
across24experimentsyieldsaJ&F scoreof3.0. TheseresultsconfirmthatSAM2Longconsistently
outperformstheSAM2baselinebyalargemargin. Forinstance,forSAM2Long-Large,achievesan
improvementof4.5and5.3overSAM2onSA-Vvalandtestsets. Thispatternisalsoevidentinthe
LVOSvalidationset,whereSAM2LongdeliversnotableperformancegainsoverSAM2foreach
modelsize. Theseresultsshowcasetheeffectivenessofthetraining-freememorytreeinlong-time
videosegmentationscenarios.
SAM2Longoutperformspreviousmethodsandexcelsinunseencategories. Wealsocompareour
proposedmethod,SAM2Long,withvariousstate-of-the-artVOSmethodsonboththeSA-V(Ravi
etal.,2024)andLVOS(Hongetal.,2023;2024)datasets, asshowninTable2and3. Although
SAM2.1alreadysurpassespreviousmethodsbyalargemargin,SAM2.1Longpushestheselimits
evenfurther. Specifically,ourmethodachievesaJ&F scoreof81.1ontheSA-Vvalidationset,a
2.5-pointimprovementoverSAM2.1. ForLVOS,SAM2.1LongattainsJ&F scoresof83.4and
85.9onthev1andv2subsets,respectively,outperformingSAM2.1by3.2and1.8points. Notably,
SAM2Longparticularlyexcelsinunseencategories,achievingJ andF scoresof79.1and86.2.
Thesignificantimprovementsof7.5and5.1pointsoverSAM2highlightitsrobustgeneralization
capabilities.
SAM2Longdemonstratesversatilitywhenhandlingvideoswithvariouschallenges. Inaddition
totheSA-VandLVOSdatasets,weevaluateourproposedSAM2LongonotherVOSbenchmarks
7Preprint
inTable4. OntheMOSEdataset(Dingetal.,2023a),whichinvolvescomplexreal-worldscenes,
SAM2.1Long achieves a J&F score of 75.2, surpassing SAM 2.1’s score of 74.5. Given that
the stronger SAM 2.1-L shows no improvement over the SAM 2-L model on the MOSE bench-
mark, our performance gain with SAM2.1Long is particularly notable. Similarly, on the VOST
dataset (Tokmakov et al., 2023), which focuses on objects undergoing extreme transformations,
SAM2.1LongshowsimprovementwithaJ&F scoreof54.0,anearly1-pointgainoverSAM2.1.
OnthePUMaVOSdataset(Bekuzarovetal.,2023),whichchallengesmodelswithdifficultvisual
cues, SAM2.1LongoutperformsSAM2.1withascoreof82.4comparedto81.1, demonstrating
itsenhancedabilitytohandlesubtleandambiguoussegmentationtasks. Theseresultsunderscore
thatwehavepreservedthefundamentalsegmentationcapabilitiesofSAM2whileenhancingits
long-termabilities,demonstratingtherobustnessandversatilityofSAM2Longacrossarangeof
VOSbenchmarks.
Table1: PerformancecomparisononSA-V(Ravietal.,2024)andLVOSv2(Hongetal.,2024)
datasets between SAM 2 and SAM2Long across all model sizes. † We report the re-produced
performanceofSAM2usingitsopen-sourcecodeandcheckpoint.
SA-Vval SA-Vtest LVOSv2val
Method
J&F J F J&F J F J&F J F
SAM2-T† 73.5 70.1 76.9 74.6 71.1 78.0 77.8 74.5 81.2
SAM2Long-T 77.0(3.5↑) 73.2 80.7 78.7(4.1↑) 74.6 82.7 81.4(3.6↑) 77.7 85.0
SAM2.1-T† 75.1 71.6 78.6 76.3 72.7 79.8 81.6 77.9 85.2
SAM2.1Long-T 78.9(3.8↑) 75.2 82.7 79.0(2.7↑) 75.2 82.9 82.4(0.8↑) 78.8 85.9
SAM2-S† 73.0 69.7 76.3 74.6 71.0 78.1 79.7 76.2 83.3
SAM2Long-S 77.7(4.7↑) 73.9 81.5 78.1(3.5↑) 74.1 82.0 83.2(3.5↑) 79.5 86.8
SAM2.1-S† 76.9 73.5 80.3 76.9 73.3 80.5 82.1 78.6 85.6
SAM2.1Long-S 79.6(2.7↑) 75.9 83.3 80.4(3.5↑) 76.6 84.1 84.3(2.2↑) 80.7 88.0
SAM2-B+† 75.4 71.9 78.8 74.6 71.2 78.1 80.2 76.8 83.6
SAM2Long-B+ 78.4(3.0↑) 74.7 82.1 78.5(3.9↑) 74.7 82.2 82.3(2.1↑) 78.8 85.9
SAM2.1-B+† 78.0 74.6 81.5 77.7 74.2 81.2 83.1 79.6 86.5
SAM2.1Long-B+ 80.5(2.5↑) 76.8 84.2 80.8(3.1↑) 77.1 84.5 85.2(2.1↑) 81.5 88.9
SAM2-L† 76.3 73.0 79.5 75.5 72.2 78.9 83.0 79.6 86.4
SAM2Long-L 80.8(4.5↑) 77.1 84.5 80.8(5.3↑) 76.8 84.7 85.2(2.2↑) 81.8 88.7
SAM2.1-L† 78.6 75.1 82.0 79.6 76.1 83.2 84.0 80.7 87.4
SAM2.1Long-L 81.1(2.5↑) 77.5 84.7 81.2(1.6↑) 77.6 84.9 85.3(1.3↑) 81.9 88.8
Table2: Performancecomparisonwiththe-state-of-the-artsmethodsonSA-Vdataset.
SA-Vval SA-Vtest
Method
J&F J F J&F J F
STCN(Chengetal.,2021) 61.0 57.4 64.5 62.5 59.0 66.0
RDE(Lietal.,2022) 51.8 48.4 55.2 53.9 50.5 57.3
SwinB-AOT(Yangetal.,2021a) 51.1 46.4 55.7 50.3 46.0 54.6
SwinB-DeAOT(Yang&Yang,2022) 61.4 56.6 66.2 61.8 57.2 66.3
XMem(Cheng&Schwing,2022) 60.1 56.3 63.9 62.3 58.9 65.8
DEVA(Chengetal.,2023) 55.4 51.5 59.2 56.2 52.4 60.1
Cutie-base+Chengetal.(2024) 61.3 58.3 64.4 62.8 59.8 65.8
SAM2(Ravietal.,2024) 76.1 72.9 79.2 76.0 72.6 79.3
SAM2.1†(Ravietal.,2024) 78.6 75.1 82.0 79.6 76.1 83.2
SAM2Long(ours) 79.7 74.7 84.7 80.8 76.8 84.7
SAM2.1Long(ours) 81.1 77.5 84.7 81.2 77.6 84.9
4.3 ABLATIONSTUDY
WeconductaseriesofablationstudiesonthevalidationsplitofSA-VdatasetanduseSAM2-Large
asdefaultmodelsize.
8Preprint
Table3: Performancecomparisonwithstate-of-the-artmethodsonvalidationsetofLVOSdataset.
Subscriptsandudenotescoresinseenandunseencategories. UnliketheresultspresentedinTable1,
weusetheevaluationcodefromtheLVOSofficialrepository.
LVOSv1 LVOSv2
Method
J&F J F J&F J F J F
s s u u
LWL(Bhatetal.,2020) 56.4 51.8 60.9 60.6 58.0 64.3 57.2 62.9
CFBI(Yangetal.,2020) 51.5 46.2 56.7 55.0 52.9 59.2 51.7 56.2
STCN(Chengetal.,2021) 48.9 43.9 54.0 60.6 57.2 64.0 57.5 63.8
RDE(Lietal.,2022) 53.7 48.3 59.2 62.2 56.7 64.1 60.8 67.2
DeAOT(Yangetal.,2021a) - - - 63.9 61.5 69.0 58.4 66.6
XMem(Cheng&Schwing,2022) 52.9 48.1 57.7 64.5 62.6 69.1 60.6 65.6
DDMemory(Hongetal.,2023) 60.7 55.0 66.3 - - - - -
SAM2(Ravietal.,2024) 77.9 73.1 82.7 79.8 80.0 86.6 71.6 81.1
SAM2.1†(Ravietal.,2024) 80.2 75.4 84.9 84.1 80.7 87.4 80.6 87.7
SAM2Long(ours) 81.3 76.4 86.2 84.2 82.3 89.2 79.1 86.2
SAM2.1Long(ours) 83.4 78.4 88.5 85.9 81.7 88.6 83.0 90.5
Table4: TheperformancecomparisonsbetweenSAM2andSAM2LongonotherVOSbenchmarks.
AllexperimentsuseSAM2.1-Largemodel.
SAM2.1† SAM2.1Long
Dataset
J&F J F J&F J F
MOSE(Dingetal.,2023a) 74.5 70.6 78.4 75.2 71.1 79.3
VOST(Tokmakovetal.,2023) 53.1 47.8 58.3 54.0 48.4 59.6
PUMaVOS(Bekuzarovetal.,2023) 81.1 78.5 83.7 82.4 79.6 85.1
NumberofMemoryPathwaysP. Weablatethenumberofmemorypathwaystoassesstheirimpact
onSAM2LonginTable5. NotethatsettingP = 1revertstotheSAM2baseline. Increasingthe
numberofmemorypathwaystoP = 2yieldsanotableimprovement,raisingtheJ&F scoreto
80.1. Thisresultdemonstratesthattheproposedmemorytreeeffectivelybooststhemodel’sability
totrackthecorrectobjectwhilereducingtheimpactofocclusion. Furtherincreasingthenumber
ofmemorypathwaystoP = 3achievesthebestperformance. However,usingP = 4showsno
additionalgains,suggestingthatthreepathwaysstriketheoptimalbalancebetweenaccuracyand
computationalefficiencyfortheSAM2model.
Intermsofspeed,sincewemaintainafixednumberofmemorypathwaysateverytimestep,the
processingspeedremainsefficient. Usingthreememorypathwaysslowsdownthemodelbyonly
18%,whileyieldinga4.5-pointincreaseinperformance.
IouThresholdδ . ThechoiceoftheIoUthresholdδ iscrucialforselectingframeswithreliable
iou iou
objectcues. AsshowninTable6,settingδ =0.3yieldsthehighestJ&F,indicatinganeffective
iou
trade-offbetweenfilteringoutpoor-qualityframesandretainingvaluablesegmentationinformation.
Incontrast,havingnorequirementonmaskqualityandfeedingallmaskscontainingobjectsinto
memory(δ =0)decreasesthescoreto80.0,showingthatunreliableframeswithpoorsegmentation
iou
harmtheSAM2model. Meanwhile,anoverlystrictselection(δ = 0.9)degradesperformance
iou
evenmoreseverelyto77.8,asitexcludestoomanypotentiallyimportantneighboringframes,causing
themodeltorelyonframesthataretoofarawayfromthecurrentframeasmemory.
UncertaintyThresholdδ . Theuncertaintythresholdδ controlstheselectionofhypotheses
conf conf
underuncertainconditions. OurresultsinTable7indicatethatsettingδ to2providesthehighest
conf
J&F score,indicatinganoptimallevelforuncertaintyhandling. Lowervalues(e.g.,0.5)resultin
suboptimalperformance,astheymayprematurelycommittoincorrectsegmentationhypotheses,
leadingtosignificantperformancedropsduetoerrorpropagation. Ontheotherhand,highervalues
(e.g.,5)donotfurtherimproveperformance,suggestingthatbeyondacertainthreshold,themodel
doesnotbenefitfromadditionalmaskdiversityandcanefficientlyrelyonthetop-scoringmasks
whenthesegmentationisconfident.
9Preprint
Table5: AblationonnumberofpathwaysP. Table6: AblationonIoUthresholdδ .
iou
P J&F J F Speed δ J&F J F
iou
1 76.3 73.0 79.5 1× 0 80.0 76.6 83.4
2 80.1 76.7 83.5 0.93× 0.3 80.8 77.1 84.5
3 80.8 77.1 84.5 0.82× 0.7 80.2 76.6 83.8
4 80.7 77.0 84.5 0.75× 0.9 77.8 74.3 81.3
Table7:Ablationonuncertaintythresholdδ . Table8: Ablationonmodulation[w ,w ].
conf low high
δ J&F J F [w ,w ] J&F J F
conf low high
0.5 80.4 76.7 83.7 [1,1] 80.2 76.5 83.8
2 80.8 77.1 84.5 [0.95,1.05] 80.8 77.1 84.5
5 80.5 76.9 84.1 [0.9,1.1] 80.5 76.9 84.1
MemoryAttentionModulation[w ,w ]. Weexploretheeffectofmodulatingtheattention
low high
weights for memory entries using different ranges in Table 8. The configuration [1,1] means no
modulationisapplied. Wefindthattheconfigurationof[0.95,1.05]achievesthebestperformance
while increasing the modulation range to ([0.9,1.1]) slightly decreases performance. This result
indicatesthatslightmodulationsufficientlyemphasizesreliablememoryentries.
4.4 VISUALIZATION
We present a qualitative comparison between SAM 2 and SAM2Long in Figure 3. SAM2Long
demonstratesasignificantreductioninsegmentationerrors,maintainingmoreaccurateandconsistent
trackingofobjectsacrossvariousframes.
Forexample,inthesecondsequenceofthesecondrow,SAM2immediatelylosestrackoftheman
ofinterestwhenocclusionhappens. AlthoughSAM2Longalsolosestrackinitially,itsmemorytree
withmultiplepathwaysenablesittosuccessfullyre-trackthecorrectmanlateron. Inanothercase,
depictedinthethirdrowwhereagroupofpeopleisdancing,SAM2initiallytracksthecorrectperson.
However,whenocclusionoccurs,SAM2mistakenlyswitchestotrackingadifferent,misleading
individual. Incontrast,SAM2Longhandlesthisambiguityeffectively. Evenduringtheocclusion,
SAM2Longmanagestoresistthetrackingerrorandcorrectlyresumestrackingtheoriginaldancer
whenshereappears.
Inconclusion,SAM2LongsignificantlyimprovesSAM2’sabilitytohandleobjectocclusionand
reappearance,therebyenhancingitsperformanceinlong-termvideosegmentation.
5 CONCLUSION
In this paper, we introduce SAM2Long, a training-free enhancement to SAM 2 that alleviates
itslimitationsinlong-termvideoobjectsegmentation. Byemployingaconstrainedtreememory
structurewithobject-awarememorymodulation,SAM2Longeffectivelymitigateserroraccumulation
andimprovesrobustnessagainstocclusions,resultinginamorereliablesegmentationprocessover
extended periods. Extensive evaluations on five VOS benchmarks demonstrate that SAM2Long
consistently outperforms SAM 2, especially in complex video scenarios. Notably, SAM2Long
achievesuptoa5-pointimprovementinJ&F scoresonchallenginglong-termvideobenchmarks
suchSA-VandLVOSwithoutrequiringadditionaltrainingorexternalparameters.
AlthoughSAM2Longintroducessignificantimprovements,thereisstillroomforfurtherenhancement.
Future work could include fine-tuning the model on occlusion-heavy datasets using the memory
architecture of SAM2Long. Additionally, exploring the semantic interactions between multiple
objectswithinthesameframemayoffervaluableinsightsformoreaccuratesegmentation,asthe
currentmethoddoesnotaccountformulti-objectinteractionsinsuchscenarios.
10Preprint
Time Flow Time Flow
Figure3:QualitativecomparisonbetweenSAM2andSAM2Long,withGT(GroundTruth)provided
forreference. Ablueboxisusedtohighlightincorrectlysegmentedobjects,whilearedboxindicates
missingobjects. Bestviewedwhenzoomedin.
11
2MAS
TG
gnoL2MAS
2MAS
TG
gnoL2MAS
2MAS
TG
gnoL2MAS
2MAS
TG
gnoL2MAS
2MAS
TG
gnoL2MASPreprint
REFERENCES
AliAthar,JonathonLuiten,AlexanderHermans,DevaRamanan,andBastianLeibe. Hodor: High-
levelobjectdescriptorsforobjectre-segmentationinvideolearnedfromstaticimages. InProceed-
ingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.3022–3031,
2022.
Ali Athar, Alexander Hermans, Jonathon Luiten, Deva Ramanan, and Bastian Leibe. Tarvis: A
unifiedapproachfortarget-basedvideosegmentation.InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pp.18738–18748,2023.
LinchaoBao,BaoyuanWu,andWeiLiu. Cnninmrf: Videoobjectsegmentationviainferenceina
cnn-basedhigher-orderspatio-temporalmrf. InProceedingsoftheIEEEconferenceoncomputer
visionandpatternrecognition,pp.5977–5986,2018.
MaksymBekuzarov,ArianaBermudez,Joon-YoungLee,andHaoLi. Xmem++: Production-level
videosegmentationfromfewannotatedframes. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pp.635–644,2023.
GoutamBhat,FelixJa¨remoLawin,MartinDanelljan,AndreasRobinson,MichaelFelsberg,Luc
VanGool,andRaduTimofte. Learningwhattolearnforvideoobjectsegmentation. InComputer
Vision–ECCV2020: 16thEuropeanConference,Glasgow,UK,August23–28,2020,Proceedings,
PartII16,pp.777–794.Springer,2020.
ThomasBroxandJitendraMalik. Objectsegmentationbylongtermanalysisofpointtrajectories. In
Europeanconferenceoncomputervision,pp.282–295.Springer,2010.
Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taixe´, Daniel Cremers, and
LucVanGool. One-shotvideoobjectsegmentation. InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition,pp.221–230,2017.
Yuhua Chen, Jordi Pont-Tuset, Alberto Montes, and Luc Van Gool. Blazingly fast video object
segmentationwithpixel-wisemetriclearning. InProceedingsoftheIEEEconferenceoncomputer
visionandpatternrecognition,pp.1189–1198,2018.
HoKeiChengandAlexanderGSchwing. Xmem: Long-termvideoobjectsegmentationwithan
atkinson-shiffrin memory model. In European Conference on Computer Vision, pp. 640–658.
Springer,2022.
HoKeiCheng,Yu-WingTai,andChi-KeungTang. Rethinkingspace-timenetworkswithimproved
memory coverage for efficient video object segmentation. Advances in Neural Information
ProcessingSystems,34:11781–11794,2021.
HoKeiCheng,SeoungWugOh,BrianPrice,AlexanderSchwing,andJoon-YoungLee. Tracking
anything with decoupled video segmentation. In Proceedings of the IEEE/CVF International
ConferenceonComputerVision,pp.1316–1326,2023.
HoKeiCheng,SeoungWugOh,BrianPrice,Joon-YoungLee,andAlexanderSchwing. Putting
theobjectbackintovideoobjectsegmentation. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pp.3151–3161,2024.
HenghuiDing,ChangLiu,ShutingHe,XudongJiang,PhilipHSTorr,andSongBai. MOSE:Anew
datasetforvideoobjectsegmentationincomplexscenes. InICCV,2023a.
ShuangruiDing,WeidiXie,YaboChen,RuiQian,XiaopengZhang,HongkaiXiong,andQiTian.
Motion-inductiveself-supervisedobjectdiscoveryinvideos. arXivpreprintarXiv:2210.00221,
2022.
ShuangruiDing,RuiQian,HaohangXu,DahuaLin,andHongkaiXiong. Betrayedbyattention:
Asimpleyeteffectiveapproachforself-supervisedvideoobjectsegmentation. arXivpreprint
arXiv:2311.17893,2023b.
BrendanDuke, AbdallaAhmed, ChristianWolf, ParhamAarabi, andGrahamWTaylor. Sstvos:
Sparsespatiotemporaltransformersforvideoobjectsegmentation.InProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition,pp.5912–5921,2021.
12Preprint
Deng-PingFan,WenguanWang,Ming-MingCheng,andJianbingShen. Shiftingmoreattentionto
videosalientobjectdetection. InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pp.8554–8564,2019.
ShanghuaGao,ZhijieLin,XingyuXie,PanZhou,Ming-MingCheng,andShuichengYan. Editany-
thing: Empoweringunparalleledflexibilityinimageeditingandgeneration. InProceedingsofthe
31stACMInternationalConferenceonMultimedia,pp.9414–9416,2023.
LingyiHong,WenchaoChen,ZhongyingLiu,WeiZhang,PinxueGuo,ZhaoyuChen,andWenqiang
Zhang. Lvos: A benchmark for long-term video object segmentation. In Proceedings of the
IEEE/CVFInternationalConferenceonComputerVision,pp.13480–13492,2023.
LingyiHong,ZhongyingLiu,WenchaoChen,ChenzhiTan,YuangFeng,XinyuZhou,PinxueGuo,
JinglunLi,ZhaoyuChen,ShuyongGao,etal. Lvos: Abenchmarkforlarge-scalelong-termvideo
objectsegmentation. arXivpreprintarXiv:2404.19326,2024.
PingHu, GangWang, XiangfeiKong, JasonKuen, andYap-PengTan. Motion-guidedcascaded
refinementnetworkforvideoobjectsegmentation. InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition,pp.1400–1409,2018a.
Yuan-TingHu,Jia-BinHuang,andAlexanderGSchwing. Videomatch: Matchingbasedvideoobject
segmentation. InProceedingsoftheEuropeanconferenceoncomputervision(ECCV),pp.54–70,
2018b.
JoakimJohnander,MartinDanelljan,EmilBrissman,FahadShahbazKhan,andMichaelFelsberg.
Agenerativeappearancemodelforend-to-endvideoobjectsegmentation. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.8953–8962,2019.
AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,Tete
Xiao,SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal.Segmentanything.InProceedings
oftheIEEE/CVFInternationalConferenceonComputerVision,pp.4015–4026,2023.
FengLi,HaoZhang,PeizeSun,XueyanZou,ShilongLiu,JianweiYang,ChunyuanLi,LeiZhang,
and Jianfeng Gao. Semantic-sam: Segment and recognize anything at any granularity. arXiv
preprintarXiv:2307.04767,2023.
Mingxing Li, Li Hu, Zhiwei Xiong, Bang Zhang, Pan Pan, and Dong Liu. Recurrent dynamic
embedding for video object segmentation. In Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition,pp.1332–1341,2022.
Xiaoxiao Li and Chen Change Loy. Video object segmentation with joint re-identification and
attention-awaremaskpropagation. InProceedingsoftheEuropeanconferenceoncomputervision
(ECCV),pp.90–105,2018.
YuLi,ZhuoranShen,andYingShan. Fastvideoobjectsegmentationusingtheglobalcontextmodule.
InComputerVision–ECCV2020: 16thEuropeanConference,Glasgow,UK,August23–28,2020,
Proceedings,PartX16,pp.735–750.Springer,2020.
YongqingLiang,XinLi,NavidJafari,andJimChen.Videoobjectsegmentationwithadaptivefeature
bankanduncertain-regionrefinement. AdvancesinNeuralInformationProcessingSystems,33:
3430–3441,2020.
JiehongLin,LihuaLiu,DekunLu,andKuiJia. Sam-6d: Segmentanythingmodelmeetszero-shot
6dobjectposeestimation. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pp.27906–27916,2024.
JunMa,YutingHe,FeifeiLi,LinHan,ChenyuYou,andBoWang. Segmentanythinginmedical
images. NatureCommunications,15(1):654,2024.
K-KManinis,SergiCaelles,YuhuaChen,JordiPont-Tuset,LauraLeal-Taixe´,DanielCremers,and
LucVanGool. Videoobjectsegmentationwithouttemporalinformation. IEEEtransactionson
patternanalysisandmachineintelligence,41(6):1515–1530,2018.
13Preprint
Seoung Wug Oh, Joon-Young Lee, Kalyan Sunkavalli, and Seon Joo Kim. Fast video object
segmentationbyreference-guidedmaskpropagation. InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition,pp.7376–7385,2018.
SeoungWugOh,Joon-YoungLee,NingXu,andSeonJooKim. Videoobjectsegmentationusing
space-time memory networks. In Proceedings of the IEEE/CVF International Conference on
ComputerVision,pp.9226–9235,2019.
FedericoPerazzi,JordiPont-Tuset,BrianMcWilliams,LucVanGool,MarkusGross,andAlexander
Sorkine-Hornung.Abenchmarkdatasetandevaluationmethodologyforvideoobjectsegmentation.
InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pp.724–732,
2016.
FedericoPerazzi,AnnaKhoreva,RodrigoBenenson,BerntSchiele,andAlexanderSorkine-Hornung.
Learningvideoobjectsegmentationfromstaticimages. InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition,pp.2663–2672,2017.
Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbela´ez, Alex Sorkine-Hornung, and
Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint
arXiv:1704.00675,2017.
RuiQian,ShuangruiDing,XianLiu,andDahuaLin. Semanticsmeetstemporalcorrespondence:
Self-supervisedobject-centriclearninginvideos. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pp.16675–16687,2023.
RuiQian,ShuangruiDing,andDahuaLin. Rethinkingimage-to-videoadaptation: Anobject-centric
perspective. InEuropeanConferenceonComputerVision,pp.329–348.Springer,2025.
NikhilaRavi,ValentinGabeur,Yuan-TingHu,RonghangHu,ChaitanyaRyali,TengyuMa,Haitham
Khedr,RomanRa¨dle,ChloeRolland,LauraGustafson,etal. Sam2: Segmentanythinginimages
andvideos. arXivpreprintarXiv:2408.00714,2024.
Andreas Robinson, Felix Jaremo Lawin, Martin Danelljan, Fahad Shahbaz Khan, and Michael
Felsberg. Learningfastandrobusttargetmodelsforvideoobjectsegmentation. InProceedingsof
theIEEE/CVFconferenceoncomputervisionandpatternrecognition,pp.7406–7415,2020.
Hongje Seong, Junhyuk Hyun, and Euntai Kim. Kernelized memory network for video object
segmentation. InComputerVision–ECCV2020: 16thEuropeanConference,Glasgow,UK,August
23–28,2020,Proceedings,PartXXII16,pp.629–645.Springer,2020.
PavelTokmakov,JieLi,andAdrienGaidon. Breakingthe“object”invideoobjectsegmentation. In
CVPR,2023.
CarlesVentura,MiriamBellver,AndreuGirbau,AmaiaSalvador,FerranMarques,andXavierGiro-i
Nieto. Rvos: End-to-endrecurrentnetworkforvideoobjectsegmentation. InProceedingsofthe
IEEE/CVFconferenceoncomputervisionandpatternrecognition,pp.5277–5286,2019.
PaulVoigtlaenderandBastianLeibe. Onlineadaptationofconvolutionalneuralnetworksforvideo
objectsegmentation. arXivpreprintarXiv:1706.09364,2017.
PaulVoigtlaender,YuningChai,FlorianSchroff,HartwigAdam,BastianLeibe,andLiang-Chieh
Chen. Feelvos: Fastend-to-endembeddinglearningforvideoobjectsegmentation. InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.9481–9490,2019.
JunkeWang,DongdongChen,ZuxuanWu,ChongLuo,ChuanxinTang,XiyangDai,YuchengZhao,
YujiaXie,LuYuan,andYu-GangJiang. Lookbeforeyoumatch: Instanceunderstandingmatters
invideoobjectsegmentation. InProceedingsoftheIEEE/CVFconferenceoncomputervisionand
patternrecognition,pp.2268–2278,2023.
QiangWang,LiZhang,LucaBertinetto,WeimingHu,andPhilipHSTorr. Fastonlineobjecttracking
andsegmentation:Aunifyingapproach. InProceedingsoftheIEEE/CVFconferenceonComputer
VisionandPatternRecognition,pp.1328–1338,2019.
14Preprint
QiangqiangWu,TianyuYang,WeiWu,andAntoniBChan. Scalablevideoobjectsegmentationwith
simplifiedframework. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision,pp.13879–13889,2023.
HaozheXie,HongxunYao,ShangchenZhou,ShengpingZhang,andWenxiuSun. Efficientregional
memorynetworkforvideoobjectsegmentation. InProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,pp.1286–1295,2021.
XiuweiXu,HuangxingChen,LinqingZhao,ZiweiWang,JieZhou,andJiwenLu. Embodiedsam:
Onlinesegmentany3dthinginrealtime. arXivpreprintarXiv:2408.11811,2024.
LinjieYang,YanranWang,XuehanXiong,JianchaoYang,andAggelosKKatsaggelos. Efficient
videoobjectsegmentationvianetworkmodulation. InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition,pp.6499–6507,2018.
Zongxin Yang and Yi Yang. Decoupling features in hierarchical propagation for video object
segmentation. AdvancesinNeuralInformationProcessingSystems,35:36324–36336,2022.
ZongxinYang,YunchaoWei,andYiYang. Collaborativevideoobjectsegmentationbyforeground-
backgroundintegration. InEuropeanConferenceonComputerVision, pp.332–348.Springer,
2020.
ZongxinYang,YunchaoWei,andYiYang. Associatingobjectswithtransformersforvideoobject
segmentation. AdvancesinNeuralInformationProcessingSystems,34:2491–2502,2021a.
Zongxin Yang, Yunchao Wei, and Yi Yang. Collaborative video object segmentation by multi-
scaleforeground-backgroundintegration. IEEETransactionsonPatternAnalysisandMachine
Intelligence,44(9):4701–4712,2021b.
JiamingZhang,YutaoCui,GangshanWu,andLiminWang. Jointmodelingoffeature,correspon-
dence,andacompressedmemoryforvideoobjectsegmentation. arXivpreprintarXiv:2308.13505,
2023.
LuZhang, ZheLin, JianmingZhang, HuchuanLu, andYouHe. Fastvideoobjectsegmentation
viadynamictargetingnetwork. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pp.5582–5591,2019.
JunbaoZhou,ZiqiPang,andYu-XiongWang. Rmem: Restrictedmemorybanksimprovevideo
objectsegmentation. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pp.18602–18611,2024.
15