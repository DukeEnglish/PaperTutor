1–16
MoRE: Multi-Modal Contrastive Pre-training with Transformers
on X-Rays, ECGs, and Diagnostic Report
Samrajya Thapa1, Koushik Howlader2, Subhankar Bhattacharjee3, and Wei Le4
1Iowa State University, Email: sthapa@iastate.edu
2Iowa State University, Email: howlader@iastate.edu
3Iowa State University, Email: s7bhat@iastate.edu
4Iowa State University, Email: weile@iastate.edu
Abstract 1. Introduction
Self-supervised and multimodal pre-training Wang
In this paper, we introduce a novel Multi-
et al. (2023) are emerging research fields in Natural
Modal Contrastive Pre-training Framework
LanguageProcessing(NLP),ComputerVision(CV),
that synergistically combines X-rays, electro-
cardiograms (ECGs), and radiology/cardiology and the medical domain Lin et al. (2023). These
reports. Our approach leverages transformers methods use different types of data like images, text,
to encode these diverse modalities into a uni- audio, and signals to improve learning. In multi-
fied representation space, aiming to enhance modalpre-training,wecombinethesedatatypesfrom
diagnostic accuracy and facilitate comprehen- thesamesubjecttoenhancetaskperformance. There
sive patient assessments. We utilize LoRA- are two main types of pre-training: supervised and
Peft to significantly reduce trainable parame- self-supervised. Supervised pre-training uses labeled
ters in the LLM and incorporate recent lin-
data to train models from start to finish, ensuring
ear attention dropping strategy in the Vision
they learn specific responses. Self-supervised pre-
Transformer(ViT)forsmootherattention. Fur-
training, on the other hand, relies on large amounts
thermore, we provide novel multimodal atten-
of unlabeled data, allowing the model to learn pat-
tion explanations and retrieval for our model.
To the best of our knowledge, we are the terns and features on its own. In the medical field,
first to propose an integrated model that com- particularly in radiology, various diagnostic modali-
bines X-ray, ECG, and Radiology/Cardiology ties are employed to assess conditions affecting the
Report with this approach. By utilizing con- heart, lungs, brain, and more. Common radiologi-
trastiveloss,MoREeffectivelyalignsmodality- cal tools include X-rays, MRI, and CT scans, while
specific features into a coherent embedding, cardiological assessments might use ECG/EKG and
which supports various downstream tasks such
echocardiograms. Typically, a clinician might start
as zero-shot classification and multimodal re-
with a less expensive and more accessible modality
trieval. Employing our proposed methodol-
like an X-ray, and progressively use more detailed
ogy, we achieve state-of-the-art (SOTA) on the
and costly tools such as MRI and CT scans depend-
Mimic-IV, CheXpert, Edema Severity, and Pt-
ing on the initial findings. Given the varying cost
bXl downstream datasets, surpassing existing
multimodal approaches. Our proposed frame- andavailabilityofthesetechnologies,itraisesseveral
workshowssignificantimprovementsincaptur- pertinent research questions: Can we leverage more
ing intricate inter-modal relationships and its readily available and less expensive diagnostic tools
robustnessinmedicaldiagnosisthatestablishes effectively? How can we harness the rich, embedded
a framework for future research in multimodal information across these multiple modalities for en-
learning in the healthcare sector. You can find hanced diagnosis? Furthermore, understanding the
thecodeforourexperimentsat: github/MoRE.
generalization capabilities of integrating multiple di-
agnostic methods is crucial. This leads us to explore
Keywords: Multimodality, LLM, Transform- whether the decisions derived from such multimodal
ers, Interpretability, Self-Supervised Learning diagnostic strategies are reliable and how we might
.
4202
tcO
12
]IA.sc[
1v93261.0142:viXraMoRE:Multi-ModalContrastivePre-trainingwithTransformersonX-Rays,ECGs,andDiagnosticReport
expand the use of available modalities to fully utilize 2. Related Work
all accessible information for diagnosis.
Building on this foundation, we introduce our sim- In this section we give background on our work, and
ple and effective, Multi-Modal Contrastive Pre- introduce some of the recent works.
Training Framework for X-Rays, ECGs, and Radi-
ology/Cardiology Report (MoRE). Our framework
2.1. Vision Transformers
leverages tri-modal pre-training by combining image
data (X-rays), signal data (ECGs), and textual data
The Vision Transformer (ViT) has recently set new
(diagnostic reports) from the same patients. Recent
benchmarks in several key areas, achieving state-of-
studies (Kim et al. (2023) Yang et al. (2022) Zhang
the-art results in image classification on ImageNet,
etal.(2022))haveshowntheeffectivenessofincreas-
object detection on COCO, segmentation, and other
ingthenumberofmodalitiesinresearch, andweaim
tasks Yin et al. (2023). Traditionally, convolutional
to extend this by integrating the two most common
neuralnetworks(CNNs)havebeenthego-toforsuch
and accessible diagnostic tools for chest-related con-
tasks because of their ability to handle increasingly
ditions: X-raysandECGs. Thesemodalitiescomple-
complex patterns through larger kernels or recep-
ment each other, as the information missing in one
tive fields. The introduction of residual connections
is often present in the other. Inspired by the recent
Ebrahimi and Abadi (2018) has allowed CNNs to
work ImageBind Girdhar et al. (2023), which con-
grow significantly deeper, enhancing their ability to
nects different modalities through a common modal-
capture more complex information. Vision Trans-
ity, our framework seeks to link the X-ray and ECG
formers operate on a global scale using self-attention
modalities via the textual modality of diagnostic re-
mechanisms. Althoughlackingtheinductivebiasesof
ports. This makes it a unique tri-modal approach:
CNNs, given sufficient data, ViTs can learn intricate
X-ray, ECG, and Diagnosis Report, marking it the
relationships within the data, offering a comprehen-
first initiative in the medical field to integrate these
sive understanding of the input.
three modalities.
As we expand our framework to include the text
modality,weencountersignificantchallengesinmem- 2.2. Self-Supervised Training
ory management due to the high memory consump-
tion of large language models (LLMs). Operating Self-supervisedpretrainingLiuetal.(2023),hasnat-
threedistinctmodelsonasingleGPUposessubstan- urally gained popularity as the volume of available
tialdifficulties. Tomitigatetheseissues,weadoptthe dataincreaseswhileannotationsorgroundtruthsre-
LoRa - PEFT strategy Hu et al. (2023), which effec- main scarce. Recent advancements have shown that
tively reduces the trainable parameters of our LLM self-supervised learning not only reduces the depen-
to just 0.6% of its original size. This reduction not dency on labeled data but also improves the general-
only facilitates more efficient memory usage but also izability of models.
improves model performance. Crucially, the results
from our LoRa-pretrained LLM indicate a reduced
2.3. Multi-Modal Pre-training
susceptibility to catastrophic forgetting, further bol-
stering its utility in our multimodal approach. Our
Multimodal pretraining Wang et al. (2023), estab-
contribution in this work can be noted as:
lishes a unified framework that significantly en-
hances model generalization across various down-
• Implementaunifiedtri-modalframeworkthatis stream tasks, such as classification, visual question
capableofdiversedownstreamtaskandperform answering (VQA), and segmentation. By capturing
at a high accuracy for all modality. the fundamental features across different modalities
during the pretraining phase, these models can, in
• Show the generalization capability of the pre-
some cases, outperform fully supervised models in
trained model through Zero-shot Classification
downstream tasks. Moreover, the multimodal frame-
• Provide multimodal explainability with work offers the flexibility to leverage all available
Gradient-based Attention Visualization and modalities or select individual modalities for specific
multimodal retrieval. tasks, enhancing adaptability and application poten-
tial across a broader range of scenarios.
2MoRE:Multi-ModalContrastivePre-trainingwithTransformersonX-Rays,ECGs,andDiagnosticReport
Figure 1: MultiModal Pretraining Framework. We join the diagnostic report of both modalities as a single
input and align the modalities with contrastive loss. We employ DropToken algorithm in our ViT
encodersandcustompatchembeddingforECGsignalmodality. TheLLMisonlyfine-tunedwith
LoRA PEFT effectively training 0.6% of its total parameters.
2.3.1. Vison Language Pretraining in ablemodalitiesfordiagnosis. RecentworkHanetal.
Medical Domain: (2024)integratesX-ray,ECG,anddiagnosticreports
but primarily addresses the challenges of missing
modalities and modality collapse. In contrast, our
Recent developments in Vision Language Pretrain-
work emphasizes the alignment of modalities while
ing (VLP) within the medical domain primarily uti-
maintaining the capability to operate with single
lizes contrastive learning, integrating image and text
modalities. Similarly, Wu et al. (2024) also tackles
modalities. ConVIRT Zhang et al. (2020) effectively
the issue of missing modalities but focuses on ICU
pairsX-rayimageswithradiologyreports,employing
data rather than downstream classification or zero-
contrastive learning as its pretraining objective. In-
shot learning. Our paper proposes to integrate ECG
novations have continued with GLoRIA Huang et al.
dataalongsideitsassociatedcardiologyreport—with
(2021), which not only uses X-ray and radiology re-
existing X-ray data. This tri-modal approach—X-
ports but also introduces a novel architecture. This
ray, ECG, and Radiology/Cardiology Report—aims
architecture captures both global and local features
to enhance the pretraining process, leveraging three
of each modality, utilizing ‘GloRIA Loss’, a variation
modalitiestoimprovebothgeneralizationandperfor-
ofcontrastiveloss,andtraditionalcontrastivelossfor
mance indownstream tasksandshowssuperiorzero-
pretraining. Furthermore,MedKLIPWuetal.(2023)
shot performance.
refines data processing by extracting a triplet of ‘En-
tity, Position, Exist’ from radiology reports to elimi-
3. Method
nate irrelevant information and align textual entities
with image patches spatially. In the context of ECG
Inthissection,weintroduceourarchitecture,explain
data,FrozenSSLLietal.(2023)demonstratestheef-
the different components, and the objective task.
fectivenessofusingapre-trained, frozenClinicalBert
LLM with contrastive loss, avoiding additional fine-
3.1. Architecture
tuning for text modality integration.
Despite recent advancements, a significant gap re- In our method, we utilize the Vision Transformer
mains in fully leveraging the broad range of avail- (ViT) as the backbone encoder for both the X-ray
3MoRE:Multi-ModalContrastivePre-trainingwithTransformersonX-Rays,ECGs,andDiagnosticReport
and ECG modalities. To enhance training stability The loss function can be formulated as follows:
andaccelerateconvergence,weinitializetheViTwith
(cid:34) (cid:35)
pretrained ImageNet weights. The overall architec- exp(sim(z ,z )/τ)
L =− log i j (1)
ture is illustrated in Figure 1. InfoNCE (cid:80)N
exp(sim(z ,z )/τ)
k=1 i k
where:
3.2. Modality Encoder
• sim(z ,z ) represents the similarity between the
i j
We implement a custom patch embedding to encode
representations z and z , measured using the
i j
the ECG modality shown in Appendix A fig 8.
cosine similarity.
We adopt DropKey Li et al. (2022) strategy in the • τ denotes a temperature scaling parameter that
ViTself-attentionshowninFigure8. Insteadofdrop- controls the separation of the distribution.
pingtheattentinweights,werandomlymaskthekey • N is the batch size
with a linear rate over the layers. This allows for a
smoother attention plot and better robustness. For We ensure that the loss is symmetric, i.e. we max-
textual modality, we use an extended version Lewis imize and minimize distance bi-directional for both
et al. (2020) of Clinical Bert Alsentzer et al. (2019), modalities. We make the temperature parameter
which is a Roberta base pretrained on Pubmed and learnable and allow the model to find the best value
Mimicreports. LLMscanbecostlytotrain,specially to fit our data. The loss function for our framework
in multimodal setting, so we fine-tune the LLM with can be defined as:
LoRA PEFT strategy Hu et al. (2021). Addition-
ally,weconcatenatethediagnosticreportsfromboth 1
L= (L +L ) (2)
modalities of the same patient into a input, utiliz- 2 Text-Xray Text-ECG
ing the existing separator token in the LLM’s tok-
enizer. This enables the LLM to encode text from where:
both modalities into a unified input. We provide ad-
(cid:32)
ditional details on our encoders in Appendix B L
Text-Xray
=−1
2
loge (cid:80)xp e( xs pim (s( iz mT (e zxt,z X ,r zay )) // ττ ))
Text k
(3)
(cid:33)
exp(sim(z ,z )/τ)
3.3. Contrastive Loss
+log(cid:80) exp(simX (r zay T ,e zxt
)/τ)
Xray k
The pretraining framework’s objective centers on
(cid:32)
using contrastive loss to effectively manage rela- 1 exp(sim(z ,z )/τ)
tionships between modalities. Following established
L
Text-Ecg
=−
2
log(cid:80) exp(simT (e zxt E ,c zg
)/τ)
Text k
contrastive learning practices, we project the out- (4)
(cid:33)
puts from the modality-specific encoders into a low- +log(cid:80)exp(sim(z Ecg,z Text)/τ)
dimensional shared space. Specifically, we em- exp(sim(z Ecg,z k)/τ)
ploy the InfoNCE loss van den Oord et al. (2018),
which aims to minimize the distance between fea-
tures of the textual modality and those of the X- • L represents the InfoNCE loss for text-
Text-Xray
ray modality from the same patient, and similarly image pairs in both directions.
between the textual modality and the ECG modal- • L represents the InfoNCE loss for text-
Text-ECG
ity. Conversely, it maximizes the distance between signal pairs in both directions.
non-corresponding/negative, pairs. By concatenat- • z represents the representation of a negative
k
ing texts from both modalities with a separator to- sample for both modalities
ken during tokenization, the model learns to discern
which portion of text corresponds to which modal-
ity. We assess this capability through retrieval tasks, 4. Evaluation
confirmingthemodel’seffectivenessindistinguishing
andcorrectlyassociatingthetextualinputswiththeir In this section, we introduce our research questions,
respective modalities. experimental setup, and the results.
4MoRE:Multi-ModalContrastivePre-trainingwithTransformersonX-Rays,ECGs,andDiagnosticReport
4.1. Experimental design which has 21k ECG data. For X-Ray images we
test our model on the labels: Atelectasis (AT), Car-
Our experimental framework is meticulously struc-
diomegaly (CM), Edema (ED), and Pleural Effusion
tured to address each research question (RQ), ensur-
(PE). For ECG images we test on superclass la-
ing a comprehensive evaluation of our model:
bels: Normal (NORM), Hypertrophy (HYP), ST/ T
• RQ1: Is MoRE able to effectively learn the Changes (STTC), and Myocardial Infarction (MI).
representation for ECG and X-Ray? We use these labels as our baselines have worked on
Weassessthezero-shotclassificationcapabilities the same. For Zero-shot we use CheXpert 5x200,
ofourmodelforbothX-rayandECGmodalities. andMimicZero-Shotsubsetscreatedfromtheirorig-
Additionally,wepresentt-SNEplotstovisualize inal datasets. We provide additional details on our
the features of our model compared to baseline datasets in Appendix C
models, providing a clear graphical representa-
tion of its performance.
• RQ2: Can MoRE be fine-tuned to per- 4.3. Implementation
form downstream classification tasks ac-
curately? Our model employs the ‘ViT-Base-patch16-224’ pre-
Our pre-trained model is fine-tuned on down- trained on ImageNet, utilizing the Timm library for
stream datasets. The results of this fine-tuning its adaptability. The base encoder features 12 trans-
are presented in a tabular format, allowing for former layers and 12 multi-head self-attention heads.
direct comparison of performance metrics across WeapplyacustompatchembeddingforECGmodal-
different datasets. ities and handle diagnostic reports with Clinical-
• RQ3: Is Multimodal of X-ray and ECG Bert’s tokenizer. The model uses a projection layer
withtwolinearlayersandleveragestheInfoNCEloss,
applicable in medical domain ?
configured with a learnable temperature parameter.
We evaluate the retrieval performance of our
TrainingisoptimizedwiththeAdamWoptimizerand
model by comparing it with baseline models.
Automatic Mixed Precision, using gradient accumu-
Thiscomparisonhelpsillustratetheeffectiveness
lationtomanagelargebatchprocessing. Detailedim-
of our model in retrieving relevant medical im-
plementation specifics, including parameter settings
ages and data based on query inputs.
and architecture modifications, are provided in the
• RQ4: How does MoRE compare to single
Appendix E
model pre-training ?
We compare our multimodal approach against
single modality models to demonstrate the ben-
4.4. Baselines
efits of integrating multiple types of data. This
comparison aims to highlight the enhanced per-
We compare our Multi-Modal Contrastive Pre-
formance and utility that multimodality brings
training Framework (MoRE) with several state-of-
to medical image analysis.
the-art multimodal pretraining frameworks in the
medical domain, including GLoRIA Huang et al.
4.2. Experimental setup (2021), MedKlip Wu et al. (2023), ECG AdvMask-
ing Bo et al. (2022), and FrozenSSL Li et al. (2023).
4.2.1. Datasets
For a fair comparison, we pretrain GLoRIA on the
For pretraining, we use the matched subset of Mimic Mimic-IV Dataset, aligning with the dataset used
CXR and Mimic ECG dataset. We find about 45k for our own pretraining and that of MedKLIP. Sim-
matching patients, with combination of about 800k ilarly, we pretrain ECG AdvMask and FrozenSSL
X-Ray and ECG data. We provide further details on the Mimic-IV ECG 800k dataset. Notably, since
on the matching of the pretraining dataset in Ap- FrozenSSL does not specify a normalization process
pendix B. For downstream tasks, we test our model for ECG, we adopt the same normalization approach
on the CheXpert Irvin et al. (2019) dataset which used in our work. ECG AdvMask being a single
contains 192k frontal X-Ray images, Edema Sever- modality framework employs an autoencoder to gen-
ity Liao et al. (2021) which has 7k data with sever- erate masks for ECG during pretraining, we follow
ity level as classification, PtbXl PTB (2022) dataset their outlined process.
5MoRE:Multi-ModalContrastivePre-trainingwithTransformersonX-Rays,ECGs,andDiagnosticReport
4.5. Metrics select a portion of the training data that contains
images each uniquely labeled with one of the condi-
In the medical domain, relying solely on accuracy to
tions, ensuring clarity in our visual analysis. This
evaluate model performance can be misleading. Ac-
avoids any label conflicts in the plot. The conditions
curacytypicallyinvolvesafixedthreshold(often0.5)
are labeled as follows: 0 for Atelectasis, 1 for Car-
for classification, which does not account for the un-
diomegaly, 2 for Edema, and 3 for Pleural Effusion.
even distribution of classes, the varying difficulty of
These t-SNE plots allow us to assess the distinctive-
diagnosing certain conditions, or the prevalence of
ness of the model’s feature representations for each
different conditions within the dataset. Instead, we
pathology across the two datasets.
utilize the Area Under the Receiver Operating Char-
acteristic (AUROC) as our primary metric. AUROC Discussion: MoREconsistentlyoutperformsGLoRIA
measuresthemodel’sabilitytodiscriminatebetween and MedKlip in zero-shot classification for all labels
classesatvariousthresholdsettings,providingamore exceptEdema. Notably,GLoRIAdemonstratessupe-
comprehensive assessment of performance across dif- riorperformancespecificallyforEdema,asdetailedin
ferent clinical scenarios Ling et al. (2003). For eval- Table 1. The lower number of datapoints for Edema
uating the retrieval experiment, we use Precision@K in the dataset may be a contributing factor to this
metric and evaluate the text retrieved as correct if it performancegap. MoREoutperformsFrozenSSLinall
falls in the same class label as the original text. but one label by a small margin.
4.6.2. RQ2: Fine-tuning on downstream
4.6. Results and Discussion
dataset
4.6.1. RQ1: Zero-shot Classification
We perform fine-tuning on our downstream datasets
The zero-shot process is described in Appendix F and report the results in tables 3, 4, and 5
AC:Atelectasis,CM:Cardiomegaly,ED:Edema,PE:Pleural
Effusion MimicIV CheXpert
Model
AC CM ED PE AC CM ED PE
NORM:Normal,HYP:Hypertrophy,STTC:ST/TChanges,
GLoRIA 0.73 0.73 0.81 0.84 0.76 0.77 0.87 0.90
MI: Myocardial Infarction
MedKLIP 0.71 0.67 0.77 0.82 0.75 0.78 0.92 0.92
MoRE 0.74 0.78 0.83 0.86 0.77 0.80 0.92 0.91
CheXpert Mimic-IV
Model
AC CM ED PE AC CM ED PE Table 3: X-ray Inference Results (AUC) in 100%
GloRIA 0.66 0.79 0.71 0.73 0.63 0.68 0.81 0.72 data
MedKLIP 0.62 0.62 0.62 0.58 0.53 0.70 0.67 0.61
MoRE 0.75 0.85 0.65 0.83 0.79 0.72 0.68 0.80
Table 1: Zero-Shot for X-Ray Edema Severity
Model
0 1 2 3
PtbXl GLoRIA 0.83 0.62 0.73 0.92
Method
MedKLIP 0.85 0.66 0.76 0.88
NORM STTC MI HYP CD
MoRE 0.82 0.78 0.75 0.92
FrozenSSL 0.50 0.40 0.42 0.58 0.51
MoRE 0.77 0.56 0.50 0.56 0.52 Table 4: Edema Severity Results (AUC)
Table 2: Zero-Shot for ECG (Superclass)
Model PtbXL
Wealsovisualizethet-sneplotofthemodelsfeatures Norm STTC HYP MI CD
for X-Ray images shown in fig 2, which helps to il- AdvMask 0.89 0.87 0.85 0.81 0.84
lustrate the feature separations and clusters formed FrozenSSL 0.90 0.86 0.78 0.77 0.88
by different pathological labels. For the CheXpert MoRE 0.91 0.87 0.87 0.82 0.89
dataset, we specifically utilize the subset designated
as CheXpert 5x200. For the Mimic-IV dataset, we Table 5: PtbXL Superclass Results (AUC)
6MoRE:Multi-ModalContrastivePre-trainingwithTransformersonX-Rays,ECGs,andDiagnosticReport
Figure 2: t-SNEplotofX-rayfeaturesofdatasetChexpert(top)andMimic(bottom)fromModelsa: MoRE,
b: GLoRIA, and c: MedKLIP.
Discussion: MoRE outperforms both GLoRIA and 4.6.3. RQ3: Retrieval
MedKLIP on the Mimic-IV and CheXpert datasets
Weconductretrievaltaskstofurthervalidatetherep-
across all labels but one. Notably, all models, in-
resentations learned by our framework and demon-
cluding MoRE , GLoRIA, and MedKlip, were trained
strate its application in medical learning. We uti-
exclusively on the Mimic-IV dataset. This context
lize the CheXpert 5x200 dataset and a subset of the
highlightstherobustnessandgeneralizabilityof MoRE
Mimic-IV dataset, where each X-ray is uniquely la-
inhandlingdiversemedicalimagingdataundersimi-
beled with a diagnosis. This approach not only tests
lartrainingconditions. Wealsoconductdownstream
the effectiveness of the learned representations but
fine-tuning on Edema severity, to demonstrate that
also showcases how the framework can be applied in
after fine-tuning, our framework achieves improved
practical medical settings.
performanceonthefine-grainedclassificationtaskfor
Edema. Initially, Edema had shown lower perfor- Mimic-IV CheXpert
Model
manceinzero-shotclassification,asdetailedinTable Prec@5 Prec@10 Prec@100 Prec@5 Prec@10 Prec@100
GLoRIA 27.0 26.2 26.7 27.7 27.7 26.7
1. Thisfine-tuninghighlightsourframework’sadapt- MoRE 52.9 51.0 44.2 55.8 55.6 50.0
abilityandeffectivenessinenhancingperformanceon
specificconditionsthatinitiallyposedchallenges. Fi- Table 6: Precision@k Results for X-ray-to-Text
nally,MoREsignificantlyoutperformedbothAdvMask
and FrozenSSL as seen in Table 5, even though these Discussion: We were unable to test for MedKLIP
models were trained on the entirety of the MImic- for this task because their approach does not in-
IVECGdataset. Incontrast,MoREwastrainedusing volve training their LLM model; instead, they use it
onlyamatchedsubsetoftheMimic-IVECGdataset. solely to encode the triplet extracted from the med-
This underscores MoRE’s efficiency and robustness in ical notes. We find our model performs better in re-
achieving high performance with less training data. trieval for all K.
7MoRE:Multi-ModalContrastivePre-trainingwithTransformersonX-Rays,ECGs,andDiagnosticReport
Text-to-Xray Retrieval We demonstrate that, us- images display fluid accumulation in both lung areas
ing a query text, our system can successfully retrieve andatthebaseofthelungs,signalingthepresenceof
X-rays associated with that query. The retrieval re- edema and pleural effusion, as illustrated in Fig. 3,
sults display a variety of X-ray images that are rele- 4. Thisvisualcomparisonhighlightsthetool’sability
vanttothetext. Webelievethiscapabilitymakesour toeffectivelydifferentiateanddisplayspecificmedical
tool highly valuable for educational purposes, help- conditions critical for diagnostic purposes.
ingusersunderstandhowaparticularmedicalcondi-
MultiModalRetrievalWefurtherdemonstratethe
tioncanappearindifferentpatients,itsvariouslevels
capability of MoRE to retrieve both X-ray and ECG
of severity, and the presence of comorbid conditions.
data using a common text query in fig 5, showing
This functionality could enhance learning and diag-
thatthetextmodalityeffectivelybindstheX-rayand
nostic training in medical education.
ECGmodalitiestogether. Thisintegrationhighlights
Example Query1: ”Cardiomegaly is severe”, Query2:
MoRE’s ability to synthesize information across differ-
”There is presence of Edema and Effusion”
ent types of medical data, offering a cohesive view
that can be crucial for comprehensive diagnostic as-
sessments.
Example Query: ”Cardiomegaly is present”
Discussion: For the given query, the retrieved
X-rays specifically mention Cardiomegaly in their
original descriptions. Correspondingly, the re-
trieved ECGs show abnormalities such as ”Arrhyth-
mia, Bradycardia, Premature Ventricular Contrac-
tions(PVC)”—irregularitiesinheartbeat,slowheart-
beat, and skipped or extra heartbeats, respectively,
which are indicative of heart abnormalities. These
results illustrate MoRE’s capability to align relevant
diagnostic findings across modalities, enhancing the
comprehensiveness of medical interpretations.
Figure 3: Retrieved X-ray of Query1: ”Car-
diomegaly is severe”
Figure 4: Retrieved X-ray of Query2: ”There is
presence of Edema and Effusion”
Figure 5: X-ray-ECG Retrieval with its original as-
Discussion: In the retrieved images, the left X-ray sociated Text
images show a visibly enlarged heart, indicative of
severe cardiac conditions. On the right, the X-ray
8MoRE:Multi-ModalContrastivePre-trainingwithTransformersonX-Rays,ECGs,andDiagnosticReport
Gradient Based LRP Attention Visualization: cating how our multimodal approach stands against
We employ a modified version of TransLRP Byun single modality training.
and Lee (2023), which utilizes Layerwise Relevance
Propagation (LRP) to aggregate backward gradient Mimic IV
Model
flow for deriving explanations. Our modification al- AC CM ED PE
ViT-Base-IM(FS) 0.70 0.76 0.82 0.83
lows TransLRP to accept multimodal inputs, specif-
SimCLR 0.71 0.74 0.80 0.81
ically X-ray and ECG, enabling us to visualize at-
MoRE 0.74 0.78 0.83 0.86
tention maps through the backward gradient flow.
We present examples from the test set of the Mimic
Table 7: Single Model Pretraining Comparison with
dataset unseen by the model, illustrating the atten-
MoRE . IM: Imagenet, FS: Fully-Supervised
tion focused on different diagnostic classes. This vi-
sualizationhelpsclarifyhowthemodelprioritizesdif-
ferent aspects of the input data in making diagnostic
Discussion: We observe that MoRE outperforms
decisions. We visualize the rollout attention of the
single-modality methods, such as its standalone ViT
transformerblocksbasedonthebackwardflowofthe
encoder when fully supervised and the ViT encoder
gradientfromtheclassificationhead,whichallowsus
pre-trained with the SimCLR framework. This high-
to visualize the prominent attention for each class.
lights the superior fine-tuning capability of our pre-
In fig 6, we are able to plot the attention on multi-
trained framework.
modal input of X-Ray and ECG when we use both
modality for inference. For the given example, the 5. Ablation Study
condition is Cardiomegaly. We also show attention
Westudytheuseofincorporatingallavailablemodal-
plot for X-Ray image with multi-class label in fig 7.
ity for inference. Since Mimic IV dataset has a
Discussion: In our analysis of the X-rays, we ob-
matchedsubsetofX-RayandECGdata,weperform
serve that the model directs high attention to ar-
inference with just X-Ray and X-Ray plus ECG. For
eas of concern, specifically, the heart region for car-
the mulitmodal input, we ensure the study date of
diomegaly and the base of the lungs for pleural ef-
the X-Ray and ECG are no more than 3 days apart.
fusion. For the ECGs, while the attention distribu-
tionismorecomplextodecipher,itisnoticeablycon-
Mimic IV
centrated around the QRS complex, P Wave, and T Modality
AC CM ED PE
Wave. Theseareasarecrucialforidentifyingirregular
X-Ray Only 0.74 0.78 0.83 0.86
heartbeats,whichareprominentindicatorsofcardiac
X-Ray + ECG 0.73 0.74 0.76 0.81
issues. This focused attention could serve as a valu-
able tool for clinical experts to validate the model’s Table 8: AUROC scores of MoRE on Mimic IV
accuracy and relevance in real-world diagnostics, as
demonstrated in Fig. 6 and 7.
Mimic IV
Modality
4.6.4. RQ4 Results: Multimodal pretraining AC CM ED PE
against Single modalities X-Ray Only 0.46 0.51 0.53 0.71
X-Ray + ECG 0.50 0.54 0.65 0.71
We compare our model to single modality pre-
training frameworks in Table 7 We employ our base Table 9: AUPRC scores of MoRE on Mimic IV
encoder, the ViT model, pre-trained on Mimic-IV
datausingcontrastivelearningwithaugmenteddata, Discussion Our results indicate that the AUROC
similar to the approach used in SimCLR Chen et al. scores are higher when utilizing only the X-ray
(2020) with ImageNet initialization. We also bench- modality. However,theAURPCscoresimprovewhen
mark against a fully supervised base ViT model to both the X-ray and ECG modalities are combined,
provide a comprehensive performance comparison. suggesting that the model may be more robust to
ThesemodelsareevaluatedusingMimic-IVdata. For false negatives when incorporating additional infor-
ECG, our baseline ECG AdvMask Bo et al. (2022) is mation from the ECG data. Further investigation
a single modality framework. Previous evaluations, is needed to explore whether leveraging all available
as shown in Table 5, detail these comparisons, indi- modalities can enhance diagnostic accuracy and im-
prove overall model performance.
9MoRE:Multi-ModalContrastivePre-trainingwithTransformersonX-Rays,ECGs,andDiagnosticReport
Figure 6: Left: Attn Plot on X-ray Right: Attn plot on ECG with Condition: Cardiomegaly
Figure 7: Left: X-ray with Cardiomegaly and Effusion Right: X-ray with Edema and Atelectasis
Limitations perior performance on various benchmarks, estab-
Our research effectively integrates multimodal data lishing a new standard for multimodal learning in
sources like X-rays, ECGs, and Diagnostic report healthcare. Lookingahead,ourfutureworkwillfocus
but faces several limitations that affect its broader on overcoming the current limitations by expanding
usability and effectiveness. Although the model our evaluations to additional baseline datasets. We
shows strong performance on specific datasets such also plan to explore the use of large language models
as Mimic-IV and CheXpert which, its generalizabil- (LLMs) that could further extend the applicability
itytootherdatasetsneedstobefurthertested. While and accessibility of our approach in different health-
LoRA PEFT strategy significantly reduces trainable care contexts.
parameters, we still cannot use larger available lan-
guagemodelsinamultimodalsetting. Consequently,
we were unable to utilize large language models
(LLMs) like MEDITRON-7B and MEDITRON-70B
Chenetal.(2023)duetoGPUmemoryconstraints.
6. Conclusion
This study successfully demonstrates the potential
of our Multi-Modal Contrastive Pre-training Frame-
work (MoRE) in enhancing diagnostic accuracy by
integrating X-rays, ECGs, and clinical notes. Uti-
lizing state-of-the-art transformer architectures and
contrastive loss techniques, our model has shown su-
10MoRE:Multi-ModalContrastivePre-trainingwithTransformersonX-Rays,ECGs,andDiagnosticReport
References Steven Horng. Mimic-iv-ecg: Diagnostic electro-
cardiogram matched subset (version 1.0). Phys-
Ptb-xl, alargepubliclyavailableelectrocardiography
ioNet, 2023.
dataset (version 1.0.3), 2022. URL https://doi.
org/10.13026/kfzx-aw45. Xing Han, Huy Nguyen, Carl Harris, Nhat Ho, and
Suchi Saria. Fusemoe: Mixture-of-experts trans-
Emily Alsentzer, John R Murphy, Willie Boag,
formers for fleximodal fusion, 2024. URL https:
Wei-Hung Weng, Di Jin, Tristan Naumann,
//arxiv.org/abs/2402.03226.
and Matthew McDermott. Publicly avail-
able clinical bert embeddings. arXiv preprint KaimingHe,HaoqiFan,YuxinWu,SainingXie,and
arXiv:1904.03323, 2019. Ross Girshick. Momentum contrast for unsuper-
vised visual representation learning. 2020.
Jessica Y. Bo, Hen-Wei Huang, Alvin Chan, and
Giovanni Traverso. Pretraining ecg data with Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan
adversarial masking improves model generaliz- Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
ability for data-scarce tasks. arXiv preprint and Weizhu Chen. Lora: Low-rank adapta-
arXiv:2211.07889, 2022. tion of large language models. arXiv preprint
arXiv:2106.09685, 2021.
Seok-Yong Byun and Wonju Lee. Vit-reciprocam:
Gradientandattention-freevisualexplanationsfor Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-
vision transformer. 2023. Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria,
and Roy Ka-Wei Lee. Llm-adapters: An adapter
Ting Chen, Simon Kornblith, Mohammad Norouzi,
family for parameter-efficient fine-tuning of large
and Geoffrey Hinton. A simple framework for con-
language models. 2023.
trastive learning of visual representations. 2020.
ZongweiHuang,XiaoxiaoLi,EricPoirion,ZiyueZhu,
Zeming Chen, Alejandro Hern´andez Cano, An- Yuyin Zhou, Xuefei Li, Bing Li, and Ronald M.
gelika Romanou, Antoine Bonnet, Kyle Ma- Summers. Gloria: A multimodal global-local rep-
toba, Francesco Salvi, Matteo Pagliardini, Simin resentation learning framework for label-efficient
Fan, Andreas K¨opf, Amirkeivan Mohtashami, medical image recognition. 2021.
et al. Meditron-70b: Scaling medical pretrain-
ing for large language models. arXiv preprint Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yi-
arXiv:2311.16079, 2023. fan Yu, Silviana Ciurea-Ilcus, Chris Chute, Hen-
rikMarklund,BehzadHaghgoo,RobynBall,Katie
Mohammad Sadegh Ebrahimi and Hossein Karkeh Shpanskaya, et al. Chexpert: A large chest radio-
Abadi. Studyofresidualnetworksforimagerecog- graph dataset with uncertainty labels and expert
nition. 2018. comparison, 2019.
Luyu Gao, Yunyi Zhang, Jiawei Han, and Jamie Alistair Johnson, Tom Pollard, Roger Mark, Seth
Callan. Scaling deep contrastive learning batch Berkowitz,andStevenHorng. Mimic-cxrdatabase.
size under memory limited setup. arXiv preprint PhysioNet, 2019.
arXiv:YourArXivNumberHere, 2023.
Alistair Johnson, Lucas Bulgarelli, Tom Pollard,
Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, StevenHorng,LeoAnthonyCeli,andRogerMark.
Mannat Singh, Kalyan Vasudev Alwala, Armand Mimic-iv (version 2.2). PhysioNet, 2023.
Joulin, and Ishan Misra. Imagebind: One embed-
ding space to bind them all. 2023. Alistair Johnson, Matthew Lungren, Yifan Peng,
Zhiyong Lu, Roger Mark, Seth Berkowitz, and
Brian Gow, Tom Pollard, Larry A Nathanson, Steven Horng. Mimic-cxr-jpg - chest radiographs
Alistair Johnson, Benjamin Moody, Chrystinne with structured labels (version 2.1.0). PhysioNet,
Fernandes, Nathaniel Greenbaum, Jonathan W 2024.
Waks, Parastou Eslami, Tanner Carbonati,
Ashish Chaudhari, Elizabeth Herbst, Dana Minsu Kim, Jee-weon Jung, Hyeongseop Rha, Soumi
Moukheiber, Seth Berkowitz, Roger Mark, and Maiti, Siddhant Arora, Xuankai Chang, Shinji
Watanabe, and Yong Man Ro. Tmt: Tri-modal
11MoRE:Multi-ModalContrastivePre-trainingwithTransformersonX-Rays,ECGs,andDiagnosticReport
translation between speech, image, and text by Aaron van den Oord, Yazhe Li, and Oriol Vinyals.
processing different modalities as different lan- Representationlearningwithcontrastivepredictive
guages. 2023. coding. 2018.
Patrick Lewis, Myle Ott, Jingfei Du, and Veselin RogierVanderSluijs,NanditaBhaskhar,DanielRu-
Stoyanov.Pretrainedlanguagemodelsforbiomedi- bin, Curtis Langlotz, and Akshay S Chaudhari.
calandclinicaltasks: understandingandextending Exploring image augmentations for siamese rep-
the state-of-the-art. In Proceedings of the 3rd clin- resentation learning with chest x-rays. In Med-
ical natural language processing workshop, pages ical Imaging with Deep Learning, pages 444–467.
146–157, 2020. PMLR, 2024.
Bonan Li, Yinhan Hu, Xuecheng Nie, Congying Xiao Wang, Guangyao Chen, Guangwu Qian,
Han,XiangjianJiang,TiandeGuo,andLuoqiLiu. Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang,
Dropkey. 2022. Yonghong Tian, and Wen Gao. Large-scale multi-
modal pre-trained models: A comprehensive sur-
Jun Li, Che Liu, Sibo Cheng, Rossella Arcucci, vey. Machine Intelligence Research,20(4):447–482,
and Shenda Hong. Frozen language model 2023.
helps ecg zero-shot learning. arXiv preprint
Ross Wightman. Pytorch image models, 2019.
arXiv:2303.12311, 2023.
Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng
RuizhiLiao,GeetickaChauhan,PolinaGolland,Seth
Wang, and Weidi Xie. Medklip: Medical knowl-
Berkowitz, and Steven Horng. Pulmonary edema
edge enhanced language-image pre-training. 2023.
severity grades based on mimic-cxr, 2021.
ZhenbangWu,AnantDadu,NicholasTustison,Brian
ZhihongLin,DonghaoZhang,QingyiTao,DanliShi,
Avants,MikeNalls,JimengSun,andFarazFaghri.
Gholamreza Haffari, Qi Wu, Mingguang He, and
Multimodal patient representation learning with
Zongyuan Ge. Medical visual question answering:
missing modalities and labels. In The Twelfth
A survey. Artificial Intelligence in Medicine, page
International Conference on Learning Represen-
102611, 2023.
tations, 2024. URL https://openreview.net/
CharlesXLing,JinHuang,andHarryZhang. Auc: a forum?id=Je5SHCKpPa.
better measure than accuracy in comparing learn-
Yang Xu, Mingzhang Luo, Tao Li, and Gangbing
ing algorithms. In Advances in Artificial Intelli-
Song. Ecg signal de-noising and baseline wander
gence: 16th Conference of the Canadian Society
correction based on ceemdan and wavelet thresh-
forComputationalStudiesofIntelligence,AI2003,
old. 2017.
Halifax, Canada, June 11–13, 2003, Proceedings
16, pages 329–341. Springer, 2003. DongchaoYang,JianweiYu,HelinWang,WenWang,
Chao Weng, Yuexian Zou, and Dong Yu. Diff-
Shiqi Liu and Shengkui Dai. Adaptive histogram
sound: Discrete diffusion model for text-to-sound
equalization framework based on new visual prior
generation. 2022.
and optimization model, 2023.
Hongxu Yin, Arash Vahdat, Jose M. Alvarez, Arun
Xiaoqian Liu, Jianbin Jiao, and Junge Zhang. Mallya, Jan Kautz, and Pavlo Molchanov. A-vit:
Self-supervised pretraining for decision foundation Adaptive tokens for efficient vision transformer.
model: Formulation,pipelineandchallenges. 2023. 2023.
Paulius Micikevicius, Sharan Narang, Jonah Alben, Susan Zhang, Stephen Roller, Naman Goyal, et al.
Greg Diamos, Erich Elsen, David Garcia, Boris Opt: Openpre-trainedtransformerlanguagemod-
Ginsburg, Michael Houston, Oleksii Kuchaiev, els. 2022.
Ganesh Venkatesh, and Hao Wu. Mixed precision
Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christo-
training. arXiv preprint arXiv:1710.03740, 2017.
pher D. Manning, and Curtis P. Langlotz. Con-
Aniruddh Raghu, Divya Shanmugam, Eugene trastive learning of medical visual representations
Pomerantsev, John Guttag, and Collin M. Stultz. from paired images and text. arXiv preprint
Data augmentation for electrocardiograms. 2022. arXiv:2010.00747, 2020.
12MoRE:Multi-ModalContrastivePre-trainingwithTransformersonX-Rays,ECGs,andDiagnosticReport
Zhenxun Zhuang, Mingrui Liu, Ashok Cutkosky, patients. The ECG dataset contains no labels but
and Francesco Orabona. Understanding adamw includes clinical text.
through proximal methods and scale-freeness.
2022.
Matched Subset:
An individual Xray and ECG data item can be
identified by some id’s. To get the matched subset
Appendix A. DropKey and Custom of data we identify an Xray and ECG through the
Patch Embedding patient’s Sujbect ID, and Study ID. The subject id
is a identifier of a patient, we find about little above
We use a custom patch embedding with 2 convo- 45k patients that have both Xray and ECG data
lution layers each with ReLU activation and Batch present in the matched subset. There are multiple
Normalization to encode the ECG data before feed- studies of the same patient taken in multiple dates,
ing it to the transformer model. We adopt DropKey so we find a total of little more than 300k data
in our transformer model which uses a linear drop points (Xray + ECG) creating this matched dataset
rate to mask the Key of the transformer layers. The of Xray, and ECG. We also add the clinical texts
lower layers have a higher mask rate which linearly of the Xray and ECG that are available. For Xray
decreases to have the least masking rate at the last data that did not have clinical notes associated with
level. Thisisdonetopreservethehigh-levelinforma- it, we create its note through its diagnosis label,
tion in the final layers. e.g: F¨inding of {diagnosis}, Uncertain Finding
of {diagnosis}”We follow this format because the
clinical notes are in format of ”Impression:” and
Appendix B. Pretraining Data Detail
”Finding:”. Weadd”uncertain finding”fortheXray
For Pre-training, we use two Mimic-IV dataset data points that have ”-1” in their label diagnosis,
Johnson et al. (2023), Mimic-CXR v2 Johnson et al. whichisreportedasuncertainfindinginthedataset.
(2019) and Mimic-IV ECG Gow et al. (2023). We
use the permutation of the matched subset of Xray
Clinical Notes:
and ECG data from Mimic-IV and Mimic-CXR
For the clinical notes of Xray, we use text under the
dataset. The total data we use for pre-training is
headingsinformatof”Impression:”,and”Findings:”.
about 800k where each data point is an Xray, ECG
Wefilteroutthetextfromtheseheadingsandremove
data, and Clinical Note from the same patient taken
anyspecialcharactersandredundantspacesfromthe
within60days. Weensureweonlypretrainon‘train’
text. For ECG, there are multiple reports for a ECG
fold of the dataset and leave out the validation and
data, we merge the first 7 reports as we find them
test set for downstream evaluation.
to be available for most ECGs and contain the most
information. We then remove any special characters
X-Ray Dataset:
and redundant spaces from the text.
The Mimic-CXR-JPG dataset v2.0.0 Johnson et al.
Duringtokenizationoftheclinicalnotes, wetokenize
(2024) was released in Physionet. The dataset
the Xray and ECG note together with a separator
includes JPG images of chest Xray along with
token existing in the tokenizer of the LLM. During
associatedlabelanddiagnostictext. Thedatasethas
matching Xray and ECG data of the same patient
227,827 Chest Xray images. The major reason for
we carefully follow the following steps:
choosing this dataset is having matching ECG data
of the patients in Mimic-IV ECG dataset Gow et al. 1. Find all permutations of Xray and ECG studies
(2023) for the purposes of multimodal pretraining. of same patient
2. Filterthedatapointsifthede-identifieddatesof
ECG Dataset: Xray and ECG are within 60days of each other.
The Mimic-IV ECG Matched Subset dataset Gow
3. CreateNoteofXraydatathatdonothaveclini-
et al. (2023) also released in Physionet. This dataset
calnotewithitsassociateddiagnosis. i.e. ”Find-
is derived from the larger Mimic-IV Clinical dataset
ing of {diagnosis}”
which is also the parent of the Mimic-CXR dataset.
The dataset includes 800,000 ECG from 160,000
13MoRE:Multi-ModalContrastivePre-trainingwithTransformersonX-Rays,ECGs,andDiagnosticReport
Figure 8: Left: Custom Patch Embedding for ECG Right: ViT with DropKey
Appendix C. Datasets oneuniquediagnosislabel. Weutilizethisdatasetfor
Zero-Shot classification, precision@k, and retrieval
Pre-trainingDataForpretraining,weusematched
tasks to prove the representation learned from our
subsetfromMimic-CXRv2Johnsonetal.(2019)and
framework.
Mimic-IV ECG Gow et al. (2023). Mimic-CXR v2
has about 224k frontal chest X-ray images and Ra- Mimic-Zero-Shot Mimic-Zero-Shot is taken from
diology Report, and Mimic-IV ECG has about 800k theMimicdatasetsuchthateachdatapointhasonly
ECG signals with short Cardiology Report. There oneuniquediagnosislabel. Weutilizethisdatasetfor
areabout45kmatchedsubsetofX-rayandECGfrom Zero-Shot classification, precision@k, and retrieval
the same patients. tasks.
Datasets for Downstream Tasks Edema Severity Liao et al. (2021) This dataset
Mimic-CXR Johnson et al. (2019) We use the comes from the Mimic-CXR dataset. It contains
Mimic-CXR v2 dataset for evaluating the pretrained about 7k data of which 6.6k are of training data, 520
representations on the test fold that is suggested by are validation, and 140 additional for test. This split
the dataset. The dataset has13 labels but we choose is as given in the dataset. The validation and test
to work on CM: Cardiomegaly, AT: Atelectasis, ED: set are validated by multiple domain experts. The
Edema, and PF: Pleural Effusion. We choose these severity goes from 0, 1,2, and 3. 0, none; 1, vascu-
labels as our baselines have also worked on them. lar congestion; 2, interstitial edema; and 3, alveolar
EachX-raydatacanhavemultiplediagnosis,making edema
it a multi-label classification task.
PtbXl ECG PTB (2022) This dataset that has 21k
CheXpert Irvin et al. (2019) We use the CheXpert ECGdatafrom18kpatients. EachECGisassociated
dataset that has 192k X-ray data from 65k patients. with a diagnositc superclass label, namely: NORM
WeonlyutilizethefrontalX-rayviewsandrandomly : Normal ECG, HYP: Hypertrophy, STTC: ST/T
sample 5% of the training data for validation, and changes, MI: Myocardial Infarction
use the validation set of 202 X-ray images for test-
ing. This dataset does not come from the Mimic-IV
Appendix D. Data Pre-processing and
parent corpus so this will be an outside dataset for
our evaluation. We measure the AUROC which is Transformations
presented in Table 3
Pre-processing is vital to any deep learning model
CheXpert5x200CheXpert5x200istakenfromthe training as it can significantly impact the training
CheXpertdatasetsuchthateachdatapointhasonly times, convergence, and outcome. Especially for
14MoRE:Multi-ModalContrastivePre-trainingwithTransformersonX-Rays,ECGs,andDiagnosticReport
medical data, it is key to ensure no important fea- customization compared to the Hugging Face im-
turesarelostandconversely,highlighttheimportant plementation. Our ViT base encoder consists of 12
features. transformer layers and 12 multi-head self-attention
heads. We do not include any bias for query, key,
value (qkv) during the linear projection of patches
Xray:
due to batch normalization after each layer. For
For pre-processing we use Adaptive Histogram
the ECG modality, we employ a custom patch
Equalization Liu and Dai (2023) to bring out the
embeddingstrategy usingtwo1D-convolutionlayers,
contrast and separation in the features. We find this
each followed by batch normalization and ReLU
step particularly important since the Xray images
activation. The remainder of the ViT architecture
are greyscale and sometimes the quality of Xray
is unchanged. The diagnostic reports are processed
varies introducing noise in the image and blending
using ClinicalBert’s tokenizer with the parameter
thefeatures. Afterhistogramequalization,thepixels
‘addspecialtokens’=Truetoinsertseparatortokens
are defined sharper and bring out features visibly.
between texts from the two modalities. We set ‘max
Wethenfindthemeanandstandarddeviationofthe
length’ = 512 and ‘truncation’ = True. Our projec-
training dataset and use it for Normalization after
tion layer comprises two linear layers with a hidden
transformations to improve training stability and
dimension of 768 and an output dimension of 128.
performance. To bring some variability in the data,
The first linear layer is bias-free, followed by batch
wefollowworkofVanderSluijsetal.(2024)anduse
normalizationandReLUactivation,whilethesecond
RandomResizedScaling with less stronger scaling
layer includes a bias. We configure the temperature
of 0.6-0.9 with 0.8 probability, RandomColorJit-
parameter of the InfoNCE loss van den Oord et al.
ter with brightness and contrast values of upto 0.4
(2018) at 0.1, making it learnable to optimize
with 0.8 probability, and RandomGaussianBlur
performance. The model is trained over 50 epochs
with kernel size of 7, 23 and with 0.5 probability.
with an early stopping criterion set at a patience
of 10. We use the AdamW optimizer Zhuang et al.
ECG:
(2022),withaweightdecayof0.1forpretrainingand
We follow a few key pre-processing steps for ECG. 0.02 for fine-tuning. We fine-tune only the projector
1.Re-Sampling: We resample the ECG data from layer and classifier head for linear evaluation and
500Hzto100Hz,changingitschanneldimensionfrom fine-tune the ’qkv’ weight of the last few transformer
5000 to 1000, making it a matrix of 12,1000. 2.Re- self attention layers for downstream tasks. Training
move Nan: We find there are NA values present in leverages Automatic Mixed Precision (AMP) in
the data so we swap any NA values with 0. 3.Base- PyTorchforenhancedspeedandefficiencywithmin-
line Wander: Baseline Wander is one of the pre- imalaccuracyloss Micikeviciusetal.(2017). Instead
processing steps that absolutely cannot be missed. of using MoCo He et al. (2020) for larger batch sizes,
Xu et al. (2017) Baseline Wander is a low-frequency we implement gradient accumulation over 4 steps,
noise that is caused by the movement of the ECG which has been shown to be effective in contrastive
leads. If not addressed can cause the model to de- learning by simulating larger batch sizes Gao et al.
viate from the understanding of the data. Finally, (2023). The initial batch size is set at 100, and the
we do 4.Per Lead Normalization with MinMax modelistrainedonasingleA100-SXM4-80GBGPU.
Scaling to bring the range of each lead to -1 and 1.
For Transformation, we follow work of Raghu et al.
(2022) and utilize two augmentations for variability,
Appendix F. Zero-Shot Processs
Time Warping: warp 4 segments of ECG by fac-
torof0.25, andRandom Permutation: permute4 The Zero-Shot Classfication process is described be-
segments of ECG in random order. low in the figure.
Appendix E. Implementation Detail
Our implementation utilizes the ‘ViT-Base-patch16-
224’model, pre-trainedonImageNetfromtheTimm
library Wightman (2019), chosen for its ease of
15MoRE:Multi-ModalContrastivePre-trainingwithTransformersonX-Rays,ECGs,andDiagnosticReport
Figure 9: Zero Shot Classification Process
16