Analyzing Closed-loop Training Techniques for Realistic Traffic Agent
Models in Autonomous Highway Driving Simulations
Matthias Bitzer1, Reinis Cimurs2, Benjamin Coors2, Johannes Goth1, Sebastian Ziesche1,
Philipp Geiger1, Maximilian Naumann1
Abstract—Simulation plays a crucial role in the rapid enables the policy to reason about the future consequences
development and safe deployment of autonomous vehicles. of a given action. While many of the aforementioned works
Realistic traffic agent models are indispensable for bridging
propose new training methods and/or architectures, a sys-
the gap between simulation and the real world. Many existing
tematic, comparative, empirical study of high-level training approachesforimitatinghumanbehaviorarebasedonlearning
from demonstration. However, these approaches are often con- principles is often missing. But such a study is important to
strainedbyfocusingonindividualtrainingstrategies.Therefore, understand the sim-to-real gap induced by different training
to foster a broader understanding of realistic traffic agent paradigms, in particular when using them for safety testing.
modeling, in this paper, we provide an extensive comparative
Our goal in this paper is to systematically compare and
analysisofdifferenttrainingprinciples,withafocusonclosed-
analyze different training paradigms for multi-agent driver
loop methods for highway driving simulation. We experi-
mentally compare (i) open-loop vs. closed-loop multi-agent models, with a particular focus on closed-loop methods
training, (ii) adversarial vs. deterministic supervised training, in highway scenarios. The analysis includes the following
(iii) the impact of reinforcement losses, and (iv) the impact dimensions:
of training alongside log-replayed agents to identify suitable
training techniques for realistic agent modeling. Furthermore, • Closed-loop vs. open-loop: While it is already evident
we identify promising combinations of different closed-loop thatclosed-looptrainingisbeneficial,wereevaluatethis
training methods. claim over a larger set of training methods.
• Deterministic supervised learning vs. probabilistic ad-
I. INTRODUCTION
versarial learning: Recent methods used Model-based
Modeling the behavior of traffic participants is a cru- Generative Adversarial Imitation Learning (MGAIL)
cial component in the development process of autonomous [16] to train a probabilistic driver model. However,
driving systems. Multi-agent driver models are utilized, for also training purely deterministically in a closed-loop
example, in simulation [1], [2] to benchmark planners or in supervised fashion is possible [5], [6], [7]. It is unclear
planningsystemsthemselvestoreasonaboutthebehaviorof how both compare.
other traffic participants [3]. However, most deployed driver • Pure Imitation vs. additional reinforcement learning
models are rule-based and are not able to capture behavior signal: There is some evidence that training a policy
outside their manually specified rules. Data-driven driver with imitation combined with reinforcement learning is
models offer an alternative that is able to learn behavior beneficial[15].Thus,weincludeareinforcementsignal
directly from real-world data. in our analysis of closed-loop trainings.
Many general learning methods have been proposed to • Log-replay vs. multi-agent training: Most methods ei-
learn multi-agent driving policies, including simple one-step ther propose a single-agent training alongside log-
supervised learning [4], [2], closed-loop deterministic imita- replayed agents or a multi-agent learning scheme. The
tion learning [5], [6], [7], adversarial imitation learning [8], comparison of the two schemes is infrequent.
[9], [10] and combinations of imitation with Reinforcement
Each training method has its theoretical benefits and short-
Learning (RL) [5], [11], [12], [13], [14], [15]. Most recent
comings. Policies trained via deterministic supervised train-
methodshaveincommonthatthetrainingisexecutedclosed-
ing might lack diverse behavior, but can be trained in a
loop, meaning that the model directly executes a sequence
stable way. Adversarial training might, in theory, be able
of actions with a differentiable forward model in the loop
to match state distributions perfectly but are hard to train
(see Figure 1), instead of predicting the next action given
anditisunclearifthediscriminatormatchesthedistribution
a ground-truth state (which we refer to as open-loop). This
on features that are actually relevant for the driving task.
Furthermore, one can enforce driving properties that are
1Matthias Bitzer, Johannes Goth, Sebastian Ziesche, Philipp
Geiger and Maximilian Naumann are with Bosch Center for important for the driving task, such as collision avoidance,
Artificial Intelligence (BCAI), Robert-Bosch-Campus 1, 71272 via a reinforcement learning signal but it is unclear how
Renningen, Germany {matthias.bitzer, johannes.goth,
much this impacts other realism properties.
sebastian.ziesche, philipp.geiger,
maximillian.naumann}@de.bosch.com To execute our study, we propose an intuitive multi-agent
2 Reinis Cimurs and Benjamin Coors are with Robert Bosch policy parameterization that can be employed in all training
GmbH, Crossdomain Computing division, Wienerstrasse 42-46,
methodswithminimaladjustments.Thearchitectureisbased
70469 Stuttgart-Feuerbach, Germany {reinis.cimurs,
benjamin.coors}@de.bosch.com on a multi-agent Graph Neural Net (GNN) encoder that
4202
tcO
12
]OR.sc[
1v78951.0142:viXraFig.1. Ourproblemsetting-closed-loopmulti-agentpolicylearningforautonomousdriving.
is invariant to rotation and translation of the scene and Driver modeling with Reinforcement Learning. In RL
only operates on differentiable features. The decoder can be implementations[32],[33],[34]thedrivingpolicyistrained
a stochastic head (for MGAIL), a deterministic head (for to maximize a pre-specified reward obtained through the
deterministic supervised learning) or a head that maps to interaction with the environment. Typically, the environment
discriminator scores. Our experiments are executed on the dynamicsarenon-differentiableorevenunknown.Designing
exiD dataset [17] consisting of sixteen hours of real-world a reward function that captures realistic motion behavior is
driving data that focuses on agent-to-agent interaction on extremely difficult due to the subtle intricacies of human
highway on-ramp and off-ramp scenarios. We identify the decision making. Assuming that the reward is a function
following useful strategies for training realistic traffic agent purely defined by the state and is dynamics invariant [35],
models on these scenarios: its function can be inferred through Inverse Reinforcement
1) All investigated closed-loop training strategies show Learning (IRL) [36]. Yet, IRL is expensive to run and
superior performance over their open-loop counter- difficult to scale [37].
parts.
Driver Modeling with Imitation Learning. Imitation
2) A reinforcement signal improves crucial metrics such
Learning (IL) is used to train a policy solely from expert
as collision rate, but easily deteriorates other realism
demonstrations. Classically, Behavioral Cloning (BC) can
metrics.
be used in an open-loop setting to obtain a trained policy
3) Deterministic supervised learning can be competitive
directly from ground truth demonstrations [4], [2], [38].
with probabilistic adversarial learning in highway sce-
However, these methods suffer from distribution shift in
narios.
long tail rollouts [39]. This can be alleviated with ap-
4) Combiningdifferentclosed-looppolicylearningstrate-
plying augmentations [2] or training in closed-loop where
gies improves crucial metrics such as collision rate
the training is performed directly in a sequential decision-
while maintaining realism.
making manner. Here, differential simulation accumulates
II. RELATEDWORK the loss over multiple steps [1], [6], [11], [10], [7]. This
requires either augmenting the input data if working with
Rule-based driver models. The driver modeling task is
rasterizedrepresentationsofenvironment[11]orusingvector
oftensolvedbyemployingrule-basedmethodsforcreatinga
representations [40] to have fully differentiable features.
decision-making policy. Generally, rule-based driver models
However,pureILcanbeinsufficienttolearnsafeandreliable
express the driving task as a set of parameterized functions,
policies due to a rareness of critical scenarios in the ground
such as the Intelligent Driver Model (IDM) [18] and its
truth data [15]. Therefore, loss in IL is augmented with RL
extensions [19], [20], [21]. Their popularity stems from the
rewards or common sense losses. In [5], [11], [12], [13],
simple implementation and parameterization based on ego
[14] a level of infraction is used to penalize the collisions
agent’s velocity, distance to other vehicles, and the velocity
and road departures in the loss.
difference. To obtain the best performing parameters for this
method,datadrivenapproacheshavebeenusedin[22],[23], Driver Modeling with Generative Adversarial Learn-
[24]. Other works [25], [26], [27], [28], [29] parameterize ing. Adversarial learning is an alternative approach for
car following models based on surrounding vehicle features. imitating expert behavior. Here, the loss function is replaced
While rule-based models are highly interpretable and com- withalearneddiscriminator,andthedrivingpolicyistrained
putationally efficient, they lack expressiveness w.r.t. realism to fool the discriminator. Foundational work that combined
of the driver behavior and suffer from poor generalization adversarial and policy learning was done with Generative
[30], [31]. To address these issues, different learning-based Adversarial IL (GAIL) [8], [9], [41], [16]. These methods
methods have come to the forefront in autonomous driving typically need an interplay between discriminator optimiza-
research. tion and solving the RL problem. Importantly, when theenvironment dynamics is known and differentiable, GAIL directly to the 2D action space via a weight-shared MLP,
can be replaced with model-based generative adversarial IL
a =[MLP(h ),...,MLP(h )]=Decoder(h ). (3)
(MGAIL), which can be used for closed-loop training [41]. t 1,t M,t t
In the light of driver modeling, it was used for single-agent
oreachMLPmapstotheparametersθ ofa2Ddistribution
planner learning [16] and multi-agent driver-modeling for i,t
like a Gaussian or Gaussian Mixture (GMM) which defines
simulations[10].Whilethosemethodsalleviatethenecessity
an agent-specific distribution over our action space.
tospecifyaloss,theyaredifficulttotrain[42],[43]andcan
Node features. The initial node features of the GNN
still suffer from a lack of realism in feature distribution if
contain information about the agents’ poses and kinematics,
not addressed in the training process [44].
the agents’ local map, and further information about the
III. PROBLEMFORMULATIONANDBASEPOLICY agent,liketheagenttypeanditsdimensions.First,weembed
poses, kinematics, agent type and dimensions into a single
In this section, we phrase our problem formulation and
embedding vector h via an MLP. The map is represented
i,t
introduce our base policy parameterization. In Section IV
as a sequence of line strings in the local coordinate system
we outline the different training paradigms for analysis.
of the agent. Those line strings are associated with the
Problem Formulation. Our driver modeling goal is
boundary lines of each lane segment. The line strings are
to learn a multi-agent policy π (a |s ), where a =
θ t t t embedded with Multi-Head Attention (MHA) resulting in a
(a 1,t,...,a M,t)denotestheactionsofM trafficparticipants sequenceoflineembeddings[l ]J .Theinitialembedding
at time step t and s denotes the states of all agents, which i,j j=1
t totheGNNiscomputedbyfusingmapinformationviacross
contain information like the position, speed etc. , and the attention (with residual) from h to [l ]J . All features
local map, at time step t. Given a set of ground truth i,t i,j j=1
are computed in the local reference frame of each agent and
trajectories D = {τ }N with τ = (s(i),a(i),...,s(i),a(i))
i i=1 i 0 1 T T are thus rotation and translation invariant.
the goal is to learn the parameters θ of the policy such
Edge features. The edge features capture relevant prop-
that it induces a distribution over trajectories p(τ) =
erties for interaction between source and target agents. The
p(s
)(cid:81)T
p(s |a ,s )π (a |s ) that resembles the distri-
0 t=0 t+1 t t θ t t edge features therefore consist of the distance and velocity
bution of trajectories in D. Here, we assume a known, de-
differencebetweensourceandtargetagents,relativeposition
terministicanddifferentiabletransitionmodelp(s |a ,s ),
t+1 t t historybetweensourceandtargetagentfort−2:tintarget
which is a common assumption in driver modeling [5], [7]. agents’ coordinate frame, heading difference1, and the time
In particular, we use position delta actions for each agent
to collision between the source and target agent, clipped at
(detailed in differential update step). The main challenge of
a maximum value of 10 s. All these features are invariant to
driver modeling consists of modeling the underlying policy
rotations and translations of the agent pair.
π and choosing the learning method to fit the generated
θ Message-Passing. Intuitively, our GNN has the inductive
trajectories to the ground truth trajectories. In the following
bias that the agents are the nodes in the graph, their interac-
paragraphs we propose our policy parameterization, that
tionismodeledviaedges,andthereasoningoverothersand
induces an intuitive inductive bias for multi-agent driving.
theresultingbehavioriscomputedwiththemessage-passing
Policy architecture. Our multi-agent policy π (a |s )
θ t t steps, as principally proposed by [45]. Concretely, our mes-
follows an encoder-decoder architecture. The encoder takes
sagepassingmoduleemploysanedgemodelthatupdatesthe
i an get nh te :multi-agent state s t and returns an encoding for each edgefeaturesviae( i→−k+ j1) =MLP([h( ik),h( jk),e( i→−k) j])anduses
cross-attention (with residual) between h( jk) and [e( i→−k+ j1)]M
i=1
h
t
:=[h 1,t,...,h M,t]=Encoder(s t) (1) togeth(k+1).Inthisway,eachtargetagentcanfocusonthe
j
relevant agents that might interact with it.
For multi-agent scenarios it is crucial that the Encoder can
Differentiable update step. A crucial component to en-
deal with different number of agents in the scene. Here, it is
abling closed-loop training is a differentiable forward step,
natural to define a graph over agents and use a GNN. The
that enables propagating gradients through the steps of the
agentsarethenodesinalocallyconnectedgraph.Foragiven
trajectory. As actions a , we use the position deltas in the
multi-agent state s we extract initial node features n(0) = i,t
t local reference frame of each agent, i.e., for i’s local 2D
[n( 10),...,n( M0)] and initial edge features e(0) = [e( i→−0) j]
i̸=j positionattimet+1,wehave(x ,y )=(x ,y )+
and process them via repeating message-passing layers with i,t+1 i,t+1 i,t i,t
a . After transforming this to the global reference frame,
index k =0,...,K−1 i,t
the position and the heading of each agent are updated (see
n(k+1),e(k+1) =MessagePassing(n(k),e(k)) (2) Figure 1). The new multi-agent state s t+1 is calculated via
differentiable transformations of all features given the new
The output of the encoder is given via h t = n(K). We locations and headings. Crucially, this enables MGAIL and
elaborate on the details of the initial node and edge features differentiable simulation training to propagate future error
as well as the message passing algorithm in the paragraphs through time to earlier actions. We investigate the impact of
below. different variants of closed-loop training in our experiments.
Depending on the training method (see Section IV) the
decoder of the policy either maps the agent-wise encoding 1Usingcosandsinofthedifference,toensurecontinuityofthefeature.IV. COMPAREDTRAININGAPPROACHES additional RL-like losses can induce unrealistic trajectories.
To investigate the impact of RL-losses we consider a dif-
We introduce the different training paradigms we analyse
ferentiable collision loss L (θ), as proposed in [1],
Collision
for learning realistic driver models.
that we use as an auxiliary loss in addition to the data-based
Behavioral Cloning. The first intuitive training method
losses.
is supervised one-step imitation learning, also called be-
Combination of Methods. In our experiments, we com-
havioral cloning. Here, we minimize an imitation loss over
bine different methods, and denote this by “+” signs.
the one-step state-action distribution. For example, one
For example, we can combine differentiable simulation
might minimize the expected negative log likelihood of
and MGAIL via minimizing the loss L(θ) = L (θ) +
DS
the policy under the one-step data distribution, L (θ) =
BC L (θ)forthegeneratorandtrainthediscriminatorasin
E [−log π (a|s)]. While it appears to be an intuitive MGAIL
s,a∼D θ MGAIL.Similarly,wecancombinedifferentiablesimulation
principle to train a policy, it has been shown repeatedly
with the collision loss, which can be seen as a closed-loop
[39], [46] that it can easily lead to compounding errors and
version of the method in [15]. Mixing different methods can
unrealistic distributions over (multi-step) trajectories p(τ).
combinebenefitsofbothmethodsandalleviateproblemsthat
However, we use this method for comparison purposes, as
onetrainingprinciplehaswhenusedonitsown.Importantly,
pretraining (see Section V) and as regularizer (see MGAIL).
oneneedstoconsiderproperweightingofthelossfunctions.
Differentiable Simulation. Since our forward model
is differentiable, we can alleviate the compounding error
V. EXPERIMENTS&RESULTS
problem via training π through differentiable simulation, Inthefollowing,wepresentourablationstudyonthedif-
θ
aka propagating gradients through time. Here, we consider ferent training methods for modeling highway traffic agents.
the policy to be a deterministic mapping from states to First, we introduce our experimental setup and show the
actions a = π (s ) rather than a probability distribu- results. In Section VI, we summarize the high-level findings
t θ t
tion. Given some initial state s and a generated trajec- of our experiments.
0
tory τ˜ = (s ,a˜ ,˜s ,...,˜s ), we use as loss L (θ) = Dataset. We evaluate on the exiD dataset [17], a real-
0 0 1 1 DS
E [(cid:80)T d(s ,˜s )|θ],whered(s ,˜s )isaweightedmean world trajectory dataset that contains drone-recorded driving
s0∼D t=1 t t t t
squared error (MSE) loss between the (x,y) positions in the data from highway entries and exits in Germany. We extract
states averaged over all agents. When training via differen- training and evaluation data by cutting the exiD recordings
tiablesimulationwepretraintheweightswithBC,wherewe into snippets of 10 second length, which we downsample
also replace the log likelihood loss with the weighted MSE to a frequency of 2 Hz. In total, our dataset consists of
loss. 5750recordingsnippets,whichwerefertoasrollouts,where
MGAIL.Recentmethods[8],[9],[16]utilizedgenerative all rollouts of one recording are assigned to one split. The
adversarialnetworkstolearndrivermodels.Here,wetraina datasetisorganizedintoatrain,validationandtestsplitwith
discriminator D in addition to the policy π . The discrim- 4461, 737 and 552 rollouts, respectively. Furthermore, we
ψ θ
inator is trained to classify states into the ones that come ensurethateachsplitcontainsrolloutsfromeachoftheseven
from ground truth and the ones that are generated via the exiD recording locations.
π . It maps from states to probability scores for each agent, Simulation setup. We simulate for the full 10 seconds
θ
thus D (s) ∈ [0,1]M, and is trained via the cross entropy of our rollouts at 2 Hz. In order to enable the computation
ψ
loss L(ψ)=E [−logD (s)]+E [−log(1−D (s))] of the node features, which include the agent’s speed and
s∼D ψ s∼πθ ψ
(here the expectation includes an additional averaging along acceleration, we only start controlling an agent after 3 steps
the time dimension, over all states s of the individual of it being present in the rollout. This means that an agent
trajectories). We parameterize our discriminator in the same effectively performs three initial log-replay steps before it is
way as the policy, except that the MLP in the decoder maps controlled by the model.
to [0,1] instead of the 2D action space. Evaluated Methods. We compare the methods pre-
The policy/generator is trained to fool the discriminator sented in Section IV along with their combinations. Con-
and thus minimizes the loss L (θ) = E [log(1− cretely, for open-loop training, we evaluate BC train-
D (s))]. It is important to notM eG thA aI tL gradientss∼ iπ nθ this loss, ing with maximum likelihood (BC-LL) using a Gaussian
ψ
can also propagate to previous time points, because of the head (BC Gaussian-LL) as well as a Gaussian mix-
differentiableforwardmodel.Here,thedecoderofthepolicy ture head (BC GMM-LL). We consider training BC with
maps to the parameters of a proper probability distribution weighted MSE combined with orientation loss (BC wMSE
and is either parameterized via a Gaussian or Gaussian mix- + Orientation). Here, BC wMSE refers to an MSE
turedistribution.Thelosscanbeapproximatedviasampling loss where different weights are applied for x and y axis
with the reparameterization trick. deviations, since lateral motion is predominantly smaller in
highway scenarios. Orientation is a loss expressed as:
Differentiable Collision Loss. Combining data-based
losses with reinforcement learning (RL) losses has been M
shown to be beneficial for learning planner policies [15]. d Orientation(aˆ,a):= M1 (cid:88) d MSE((δˆ x,i,δˆ y,i),(δ x,i,δ y,i)),
Also, for driver modeling, enforcing certain aspects, like i=1
(4)
preventing collisions, is crucial. However, directly applyingwhere (δ ,δ ) is the ground-truth normalized heading of We also report the Average Displacement Error (ADE)
x,i y,i
the next time step for agent i and (δˆ ,δˆ ) is the heading which is the L2 distance between the ground-truth and
x,i y,i
thatwouldresultfromthechosenaction.M isthenumberof generated trajectories for a fixed horizon of 5 seconds. This
vehicles. For closed-loop training, we consider deterministic metricmeasureshowcloselythegeneratedtrajectoriesfollow
training through differentiable simulation with a weighted the ground-truth trajectories.
MSE loss (DiffSim wMSE) and an unweighted MSE loss We evaluate the distributional realism of the models with
(DiffSim MSE). Furthermore, MGAIL is trained in com- metrics that compute the Jensen-Shannon Divergence (JSD)
bination with a BC loss (MGAIL + BC-LL) and in com- between the ground-truth and generated distributions of the
bination with differential simulation (MGAIL + DiffSim agent’s speed, acceleration and number of lane changes
wMSE), using a Gaussian or GMM head. To investigate N . The JSD is computed between histograms with 100
LC
the impact of an additional reinforcement loss, we consider equisized bins for speed, acceleration and number of lane
addingthedifferentiablecollisionlosstodifferentiablesimu- changes in a range that covers the minimum and maximum
lation (DiffSim wMSE + Collision) and to the com- of the generated and ground-truth metric values.
bination of MGAIL and differentiable simulation (MGAIL Results. Table I presents our results for all mentioned
+ DiffSim wMSE + Collision). For all closed-loop methods when trained in a combined multi-agent way (non
methods, we execute a combined multi-agent training and log-replay). Here, as baseline we compare the results to
training along log-replay agents. IDM [18] with MOBIL [29] to allow for lane changes
Training setup. We train each method for 100 epochs. in highway and on-ramping/off-ramping scenarios. In Table
Hyperparameters for each training method are determined II, we show the results of the closed-loop methods when
withhyperparametersweepsbasedonresultsfromevaluation trained alongside log-replay agents. Videos of ground-truth
onthevalidationset.TrainingofBCmethodsisperformedat and generated scenarios can be found in the supplementary
everystepoftherollout,usingthemethod’srespectivelosses. material.
Differential simulation methods are initialized from a pre-
trained BC wMSE + Orientation model. Execution is
VI. ANALYSISOFRESULTS
simulated for the full length of the rollout. MGAIL methods We analyze the experimental results and draw high-level
areinitializedwithweightsfromapre-trainedBC-LLmodel conclusions for driver model training:
withrespectivelog-likelihoodloss.Insingle-agentlog-replay Closed-loop can be beneficial over open-loop training:
trainings, we randomly select one controlled agent in the Theoretical and experimental findings have been established
rollout,withaprobabilityproportionaltotheagent’snumber in the past regarding the benefit of closed-loop multi-agent
of time steps in the scene for each batch element. training over simpler, more open-loop approaches, e.g., the
Evaluationsetup.Allmodelsareevaluatedinclosed-loop “compounding error” phenomenon of BC training [46],
simulation on the test split over all the time steps in the [47], [39]. Nonetheless, open-loop approaches are still often
rollout. In our evaluation, we compare two control settings used due to their simplicity [38], [10]. Our experimental
of the agents. First, the setting of controlling all agents at findings confirm the case made for closed-loop training.
once.ForthetestsplitoftheexiDdataset,thismeansthatwe From Table I, we see that the two closed-loop paradigms
control, on average, about 26 agents per scene. Since model - differentiable simulation and MGAIL - significantly out-
training is not deterministic, we evaluate their performance perform open-loop BC methods. This is evident when com-
variance by training the base BC models with 5 different paring DiffSim to BC wMSE + Orientation as well
seed values, then use these model weights to initialize the as MGAIL + BC-LL to the respective BC-LL method with
respective differential simulation and MGAIL methods. The the same head. In all cases, the closed-loop method is better
results are shown in Table I and Table II which report the or equal on (almost) every metric. Additionally, single-agent
mean and standard deviation of the respective 5 evaluations. closed-loop training with log replay of surrounding agents
Second, the setting of only controlling one agent, with all (Table II) can be seen as “less closed-loop” than full multi-
other agents being log-replayed. Here, the controlled agent agent closed-loop training (Table I). Also, here, the former
in the scene is selected deterministically as the agent with outperforms the latter. Furthermore, we can see that the
the most timesteps in the rollout. closedloopmethodsleadtomorerealisticscenariosinterms
Metrics. We employ a variety of metrics to judge a ofJSDmetricsandandalmostalwaystolowercollisionrates
model’s quality and realism. Metrics are computed on the than an IDM model.
test split for agents and frames, where an agent is controlled Reinforcementlosscanharmrealism:Unrealisticallyhigh
in a respective frame by the model. collision rates remain one of the biggest open challenges
A first set of metrics evaluates the model’s infractions. in learned driver models, and are a key indicator of where
Here, a collision rate is defined as the percentage of realism is still limited [16], [10]. As a natural remedy,
agents with at least one collision in a given rollout. It is various works [1], [48] have added a “common sense
implemented as a polygon intersection check between the loss”, also called “reinforcement loss”. Our experiments
bounding boxes of the agents. An off-road rate computes show that such a reinforcement learning aspect can in-
the percentage of frames at which a controlled agent drives deed significantly bring down the collision rate (e.g., com-
off the road, i.e., outside the highway lanes. pare collision rate metric between DiffSim wMSE andTABLEI
LOSSABLATIONSTUDYFORMULTI-STEPCLOSED-LOOPTRAININGSWHENTRAINEDONCONTROLLINGALLAGENTSANDEVALUATEDWHEN
CONTROLLINGALLAGENTS.
Method C (%ol ). O (%ff ). A (mD )E JSDS ×pe 1ed 0−2 JSDA ×c 1c. 0−2 JSDN ×L 1C 0−2
IDM 0.8±0.0 0.00±0.0 5.31±0.0 4.0±0.0 17.64±0.0 0.23±0.0
BCGaussian-LL 2.67±0.11 6.42±0.42 2.22±0.17 0.63±0.41 8.02±4.6 2.28±0.16
BCGMM-LL 3.1±0.62 7.05±0.78 3.42±0.41 2.41±0.66 23.16±6.52 2.23±0.33
BCwMSE+Orientation 3.1±0.65 4.36±0.73 4.15±0.34 3.51±0.8 12.54±4.33 1.5±0.22
DiffSimMSE 1.1±0.74 3.05±2.11 1.62±0.77 0.28±0.3 1.82±1.57 0.92±1.03
DiffSimwMSE 0.51±0.11 0.68±0.31 1.25±0.13 0.2±0.09 1.05±0.38 0.48±0.09
DiffSimwMSE+Collision 0.39±0.42 2.52±1.15 3.57±3.31 3.24±5.78 2.5±1.7 0.66±0.4
MGAIL+BC-LL(Gauss.) 1.7±0.58 2.95±1.2 1.95±0.24 0.42±0.18 1.82±0.52 0.6±0.56
MGAIL+BC-LL(GMM) 2.87±2.42 2.77±0.52 7.8±8.31 9.34±10.7 18.55±10.69 0.19±0.13
MGAIL+DiffSimwMSE(Gauss.) 0.39±0.06 0.59±0.35 0.93±0.03 0.08±0.03 0.86±0.15 0.32±0.15
MGAIL+DiffSimwMSE(GMM) 0.35±0.19 0.36±0.32 1.0±0.18 0.11±0.1 1.03±0.71 0.31±0.15
MGAIL+Diff.wMSE+Col.(Gauss.) 0.17±0.09 2.28±0.84 1.47±0.09 0.23±0.05 2.35±0.48 0.26±0.21
MGAIL+Diff.wMSE+Col.(GMM) 0.17±0.11 1.49±0.42 1.48±0.16 0.26±0.06 1.86±0.85 0.35±0.42
TABLEII
LOSSABLATIONSTUDYFORMULTI-STEPCLOSED-LOOPTRAININGSWHENTRAINEDONCONTROLLINGONEAGENTANDEVALUATEDWHEN
CONTROLLINGALLAGENTS.
Method C (%ol ). O (%ff ). A (mD )E JSDS ×pe 1ed 0−2 JSDA ×c 1c. 0−2 JSDN ×L 1C 0−2
DiffSimMSE 1.21±0.64 2.71±1.41 1.69±0.41 0.33±0.27 2.22±0.8 0.45±0.32
DiffSimwMSE 0.63±0.16 1.46±0.38 1.69±0.17 0.27±0.06 1.57±0.45 0.63±0.36
DiffSimwMSE+Collision 0.39±0.17 0.75±0.39 1.83±0.18 0.45±0.2 4.79±2.81 0.88±0.36
MGAIL+BC-LL(Gauss.) 2.65±0.27 4.57±0.67 2.14±0.12 0.42±0.16 2.54±0.65 1.62±0.43
MGAIL+BC-LL(GMM) 1.88±0.25 3.74±0.16 2.16±0.13 0.8±0.22 4.35±1.23 0.64±0.25
MGAIL+DiffSimwMSE(Gauss.) 0.65±0.21 1.59±0.5 1.16±0.02 0.18±0.05 3.1±1.52 0.14±0.09
MGAIL+DiffSimwMSE(GMM) 0.44±0.31 1.12±0.59 1.14±0.09 0.11±0.0 1.11±0.25 0.2±0.14
MGAIL+Diff.wMSE+Col.(Gauss.) 1.28±0.13 2.23±0.47 1.59±0.12 0.41±0.11 2.24±0.23 0.8±0.18
MGAIL+Diff.wMSE+Col.(GMM) 0.54±0.37 1.34±0.5 1.46±0.16 0.18±0.04 1.41±0.5 0.4±0.33
DiffSim wMSE + Collision or between the respec- very multi-modal and thus a deterministic method is well
tive MGAIL methods). However, this can come at the sub- suited.
stantial cost of losing realism in other aspects of the model. Combinations of methods can be beneficial: One key in-
For example, all three JSD metrics (speed, acceleration, sightofourexperimentsisthatcombinationsofmethodscan
N ) are worse (or equal) when comparing DiffSim counteractindividualweaknesses.Wealreadyobservedthata
LC
wMSE + Collisionvs.DiffSim wMSEorconsidering collisionlosscanreducecollisionrate,atthepriceofhaving
MGAIL + DiffSim wMSE + Collision vs. MGAIL worse distributional realism. However, adding adversarial
+ DiffSim wMSE.Here,thecollisionlossnotonlyweak- learning helps mitigate this effect, as can be seen when
ens the JSD metrics, but also has a negative effect on the comparing DiffSim wMSE + Collision with MGAIL
off-road rate. + DiffSim wMSE + Collision, where distributional
realismisimprovedoverallmetrics.Furthermore,MGAIL +
Deterministicsupervisedlearningcanbecompetitivewith
DiffSim wMSEshowedtobethebestperformingmodelin
MGAIL: Recent work [10], [48] increasingly utilized ad-
all metrics except of collision rate. This holds for combined
versarial learning to train driver models. It has the consid-
as well as log-replay training. We think that the supervised
erable advantage that this allows for probabilistic policies,
learning signal helps to stabilize the adversarial training.
and the loss is learned instead of being defined manually.
However,adversarialmethodscomewithtrainingchallenges VII. CONCLUSION
and are usually less stable to train compared to supervised To summarize, we conducted a systematic analysis study
or variational inference methods [49]. In our experiments, of closed-loop imitation training principles for realistic traf-
we observe that methods employing DiffSim outperform fic agent models for highway scenarios. We utilized the
MGAIL + BC-LL on all metrics, except of the accelera- same GNN-based driving policy under different training
tion JSD. While both methods are trained closed-loop and paradigms, and reported quantitative experimental results as
improve upon their open-loop BC pretrainings, we observe wellashigh-levelinsightswithqualitativeresultsgiveninthe
a stronger improvement in the metrics of DiffSim MSE, supplementarymaterial.Wefindthateachmethodonitsown
DiffSim wMSE, and DiffSim wMSE + Collision. comes with individual weaknesses, and combining different
However, we believe that this statement is dependent on the methods can counteract them. In particular, we find that
tuning of the MGAIL method as well as the probabilistic closed-loop training has significant advantages over open-
characteristics of the dataset, and should therefore be taken loop training, that a reinforcement signal can destroy real-
withcaution. Inparticular, itmightbe thecase thathighway ism and that combinations of different closed-loop learning
scenarios,includingon-rampandoff-rampsituations,arenot principles can improve overall performance.REFERENCES on Intelligent Robots and Systems (IROS), pages 8652–8659. IEEE,
2022.
[1] Simon Suo, Sebastian Regalado, Sergio Casas, and Raquel Urtasun.
[17] TobiasMoers,LennartVater,RobertKrajewski,JulianBock,Adrian
Trafficsim: Learning to simulate realistic multi-agent behaviors. In
Zlocki, and Lutz Eckstein. The exid dataset: A real-world trajectory
IEEEConferenceonComputerVisionandPatternRecognition,CVPR
dataset of highly interactive highway scenarios in germany. In 2022
2021,virtual,June19-25,2021,pages10400–10409.ComputerVision
IEEEIntelligentVehiclesSymposium(IV),pages958–964,2022.
Foundation/IEEE,2021.
[18] MartinTreiber,AnsgarHennecke,andDirkHelbing.Congestedtraffic
[2] Luca Bergamini, Yawei Ye, Oliver Scheel, Long Chen, Chih Hu,
statesinempiricalobservationsandmicroscopicsimulations.Physical
LucaDelPero,Błaz˙ejOsin´ski,HugoGrimmet,andPeterOndruska.
reviewE,62(2):1805,2000.
Simnet: Learning reactive self-driving simulations from real-world
[19] ArneKesting,MartinTreiber,andDirkHelbing. Enhancedintelligent
observations. In 2021 IEEE International Conference on Robotics
drivermodeltoaccesstheimpactofdrivingstrategiesontrafficcapac-
andAutomation(ICRA),pages–.IEEE,2021.
ity. PhilosophicalTransactionsoftheRoyalSocietyA:Mathematical,
[3] KatherineDriggs-Campbell,VijayGovindarajan,andRuzenaBajcsy.
PhysicalandEngineeringSciences,368(1928):4585–4605,2010.
Integrating intuitive driver models in autonomous planning for inter-
active maneuvers. IEEE Transactions on Intelligent Transportation [20] MofanZhou,XiaoboQu,andShengJin.Ontheimpactofcooperative
Systems,18(12):3461–3472,2017. autonomousvehiclesinimprovingfreewaymerging:amodifiedintel-
[4] LanFeng,QuanyiLi,ZhenghaoPeng,ShuhanTan,andBoleiZhou. ligentdrivermodel-basedapproach. IEEETransactionsonIntelligent
Trafficgen:Learningtogeneratediverseandrealistictrafficscenarios.
TransportationSystems,18(6):1422–1428,2016.
In2023IEEEInternationalConferenceonRoboticsandAutomation [21] MN Sharath and Nagendra R Velaga. Enhanced intelligent driver
(ICRA),pages3567–3575.IEEE,2023. model for two-dimensional motion planning in mixed traffic. Trans-
[5] OliverScheel,LucaBergamini,MaciejWolczyk,BlazejOsinski,and portationResearchPartC:EmergingTechnologies,120:102780,2020.
Peter Ondruska. Urban driver: Learning to drive from real-world [22] Venkatesan Kanagaraj, Gowri Asaithambi, CH Naveen Kumar,
demonstrations using policy gradients. In Aleksandra Faust, David Karthik K Srinivasan, and R Sivanandan. Evaluation of different
Hsu,andGerhardNeumann,editors,ConferenceonRobotLearning, vehicle following models under mixed traffic conditions. Procedia-
8-11 November 2021, London, UK, volume 164 of Proceedings of SocialandBehavioralSciences,104:390–401,2013.
MachineLearningResearch,pages718–728.PMLR,2021. [23] Mitra Pourabdollah, Eric Bja¨rkvik, Florian Fu¨rer, Bjo¨rn Lindenberg,
[6] Peter Karkus, Boris Ivanovic, Shie Mannor, and Marco Pavone. and Klaas Burgdorf. Calibration and evaluation of car following
Diffstack:Adifferentiableandmodularcontrolstackforautonomous modelsusingreal-worlddrivingdata.In2017IEEE20thInternational
vehicles. In Karen Liu, Dana Kulic, and Jeff Ichnowski, editors, conference on intelligent transportation systems (ITSC), pages 1–6.
ProceedingsofThe6thConferenceonRobotLearning,volume205of IEEE,2017.
ProceedingsofMachineLearningResearch,pages2170–2180.PMLR, [24] DuoZhang,XiaoyunChen,JunhuaWang,YinhaiWang,andJianSun.
14–18Dec2023. A comprehensive comparison study of four classical car-following
[7] SimonSuo,KelvinWong,JustinXu,JamesTu,AlexanderCui,Sergio modelsbasedonthelarge-scalenaturalisticdrivingexperiment. Sim-
Casas, and Raquel Urtasun. Mixsim: A hierarchical framework for ulationModellingPracticeandTheory,113:102383,2021.
mixed reality traffic simulation. In Proceedings of the IEEE/CVF [25] RainerWiedemann. Simulationdesstrassenverkehrsflusses. 1974.
Conference on Computer Vision and Pattern Recognition (CVPR), [26] Partha Chakroborty and Shinya Kikuchi. Evaluation of the general
pages9622–9631,June2023. motors based car-following models and a proposed fuzzy inference
[8] AlexKuefler,JeremyMorton,TimWheeler,andMykelKochenderfer. model. Transportation Research Part C: Emerging Technologies,
Imitatingdriverbehaviorwithgenerativeadversarialnetworks.In2017 7(4):209–235,1999.
IEEEintelligentvehiclessymposium(IV),pages204–211.IEEE,2017. [27] StefanKrauß,PeterWagner,andChristianGawron. Metastablestates
[9] MahanTabatabaie,SuiningHe,andKangGShin. Interaction-aware inamicroscopicmodeloftrafficflow.PhysicalReviewE,55(5):5597,
and hierarchically-explainable heterogeneous graph-based imitation 1997.
learning for autonomous driving simulation. In 2023 IEEE/RSJ
[28] PeterGGipps. Abehaviouralcar-followingmodelforcomputersim-
International Conference on Intelligent Robots and Systems (IROS),
ulation. TransportationResearchPartB:Methodological,15(2):105–
pages3576–3581.IEEE,2023.
111,1981.
[10] Maximilian Igl, Daewoo Kim, Alex Kuefler, Paul Mougin, Punit
[29] Arne Kesting, Martin Treiber, and Dirk Helbing. General lane-
Shah,KyriacosShiarlis,DragomirAnguelov,MarkPalatucci,Brandyn
changing model mobil for car-following models. Transportation
White,andShimonWhiteson. Symphony:Learningrealisticanddi-
ResearchRecord,1999(1):86–94,2007.
verseagentsforautonomousdrivingsimulation.In2022International
[30] Siyu Teng, Xuemin Hu, Peng Deng, Bai Li, Yuchen Li, Yunfeng
Conference on Robotics and Automation (ICRA), pages 2445–2451.
Ai,DongshengYang,LingxiLi,ZheXuanyuan,FenghuaZhu,etal.
IEEE,2022.
[11] Adam S´cibior, Vasileios Lioutas, Daniele Reda, Peyman Bateni, and Motion planning for autonomous driving: The state of the art and
futureperspectives. IEEETransactionsonIntelligentVehicles,2023.
Frank Wood. Imagining the road ahead: Multi-agent trajectory
[31] DiChen,MeixinZhu,HaoYang,XuesongWang,andYinhaiWang.
predictionviadifferentiablesimulation.In24thIEEEInternationalIn-
Data-driventrafficsimulation:Acomprehensivereview. IEEETrans-
telligentTransportationSystemsConference,ITSC2021,Indianapolis,
actionsonIntelligentVehicles,2024.
IN,USA,September19-22,2021,pages720–725.IEEE,2021.
[12] VasileiosLioutas,AdamScibior,andFrankWood. Titrated:Learned [32] ShinyaShiroshita,ShirouMaruyama,DaisukeNishiyama,MarioYno-
human driving behavior without infractions via amortized inference. cente Castro, Karim Hamzaoui, Guy Rosman, Jonathan DeCastro,
TransactionsonMachineLearningResearch,2022. Kuan-Hui Lee, and Adrien Gaidon. Behaviorally diverse traffic
[13] ChrisZhang,JamesTu,LunjunZhang,KelvinWong,SimonSuo,and
simulationviareinforcementlearning.In2020IEEE/RSJInternational
RaquelUrtasun.Learningrealistictrafficagentsinclosed-loop.In7th Conference on Intelligent Robots and Systems (IROS), pages 2103–
AnnualConferenceonRobotLearning,2023. 2110.IEEE,2020.
[14] Danfei Xu, Yuxiao Chen, Boris Ivanovic, and Marco Pavone. Bits: [33] DianChen,VladlenKoltun,andPhilippKra¨henbu¨hl.Learningtodrive
Bi-level imitation for traffic simulation. In 2023 IEEE International fromaworldonrails. InProceedingsoftheIEEE/CVFInternational
Conference on Robotics and Automation (ICRA), pages 2929–2936. ConferenceonComputerVision(ICCV),pages15590–15599,October
IEEE,2023. 2021.
[15] YirenLu,JustinFu,GeorgeTucker,XinleiPan,EliBronstein,Rebecca [34] MarinToromanoff,EmilieWirbel,andFabienMoutarde. End-to-end
Roelofs, Benjamin Sapp, Brandyn White, Aleksandra Faust, Shimon model-freereinforcementlearningforurbandrivingusingimplicitaf-
Whiteson,etal. Imitationisnotenough:Robustifyingimitationwith fordances. InProceedingsoftheIEEE/CVFConferenceonComputer
reinforcement learning for challenging driving scenarios. In 2023 VisionandPatternRecognition(CVPR),June2020.
IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems [35] Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards
(IROS),pages7553–7560.IEEE,2023. with adversarial inverse reinforcement learning. arXiv preprint
[16] Eli Bronstein, Mark Palatucci, Dominik Notz, Brandyn White, Alex arXiv:1710.11248,2017.
Kuefler,YirenLu,SupratikPaul,PayamNikdel,PaulMougin,Hongge [36] Guanjie Zheng, Hanyang Liu, Kai Xu, and Zhenhui Li. Objective-
Chen,etal. Hierarchicalmodel-basedimitationlearningforplanning aware traffic simulation via inverse reinforcement learning. arXiv
in autonomous driving. In 2022 IEEE/RSJ International Conference preprintarXiv:2105.09560,2021.[37] Zheng Wu, Liting Sun, Wei Zhan, Chenyu Yang, and Masayoshi Tocomputetheactionforeachagentinagivenscene,we
Tomizuka. Efficient sampling-based maximum entropy inverse rein- employaSceneEncoder,followedbyaGNN,followedbya
forcement learning with application to autonomous driving. IEEE
Behavior Decoder. As can be seen in Figure 2, the input to
RoboticsandAutomationLetters,5(4):5355–5362,2020.
[38] Mayank Bansal, Alex Krizhevsky, and Abhijit Ogale. Chauffeurnet: the network is a scene at a certain point in time. It contains
Learning to drive by imitating the best and synthesizing the worst. a map, which consists of lane graph information and lane
arXivpreprintarXiv:1812.03079,2018.
boundaries and it contains features for each agent like the
[39] Ste´phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction
of imitation learning and structured prediction to no-regret online position, bounding box, orientation, velocity, acceleration.
learning.InProceedingsofthefourteenthinternationalconferenceon Basedonthesefeatureswedetermine,foreachpairofagents,
artificialintelligenceandstatistics,pages627–635.JMLRWorkshop
whether one is relevant for the action of the other. If that is
andConferenceProceedings,2011.
[40] Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir Anguelov, the case, we introduce a directed edge between them in our
CongcongLi,andCordeliaSchmid.Vectornet:Encodinghdmapsand GNN.
agentdynamicsfromvectorizedrepresentation. InProceedingsofthe
Based on this input, the Scene Encoder computes node
IEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages11525–11533,2020. and edge features (like the relative position or velocity of
[41] Jiaming Song, Hongyu Ren, Dorsa Sadigh, and Stefano Ermon. two agents) and embeds them in latent space. Moreover, for
Multi-agent generative adversarial imitation learning. In S. Bengio,
each agent, the lines in the relevant part of the map are
H.Wallach,H.Larochelle,K.Grauman,N.Cesa-Bianchi,andR.Gar-
nett, editors, Advances in Neural Information Processing Systems, extracted, transformed into ego coordinates and embedded
volume31.CurranAssociates,Inc.,2018. intolatentspace.Afterthat,acrossattentionmodule(which
[42] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which
also takes the node embedding into account) computes a
training methods for GANs do actually converge? In Jennifer Dy
and Andreas Krause, editors, Proceedings of the 35th International map embedding from the line embeddings (see Figure 3 for
Conference on Machine Learning, volume 80 of Proceedings of details).Themapembeddingisaddedtothenodeembedding
Machine Learning Research, pages 3481–3490. PMLR, 10–15 Jul
as the last step of the Scene Encoder. Hence, the output of
2018.
[43] Robert Dadashi, Le´onard Hussenot, Matthieu Geist, and Olivier the Scene Encoder is an embedding for each agent and an
Pietquin. Primal wasserstein imitation learning. arXiv preprint embedding for each directed edge between two agents.
arXiv:2006.04678,2020.
These embeddings enter the GNN that computes one up-
[44] XintaoYan,ZhengxiaZou,ShuoFeng,HaojieZhu,HaoweiSun,and
HenryXLiu.Learningnaturalisticdrivingenvironmentwithstatistical date of the edge embedding by taking into account the node
realism. Naturecommunications,14(1):2037,2023. embeddings of the source and the target node. Afterwards,
[45] S. Casas, C. Gulino, R. Liao, and R. Urtasun. Spagnn: Spatially- each node is updated by taking into account all embeddings
awaregraphneuralnetworksforrelationalbehaviorforecastingfrom
sensordata. In2020IEEEInternationalConferenceonRoboticsand of incoming edges. This yields a final embedding for each
Automation(ICRA),pages9491–9497,2020. agentthatispassedtotheBehaviorDecoder.ThisisanMLP
[46] Jonathan Ho and Stefano Ermon. Generative adversarial imitation that generates the action in the desired output format (e.g.,
learning. Advances in neural information processing systems, 29,
means and covariances of the Gaussians or next waypoints).
2016.
[47] Ste´phane Ross and Drew Bagnell. Efficient reductions for imitation 1) Polyline representation.: To embed the map informa-
learning. InProceedingsofthethirteenthinternationalconferenceon tion, we use a similar method as in [40]. Road lines are
artificialintelligenceandstatistics,pages661–668.JMLRWorkshop
extracted from Lanelet2 [50] map representation and split
andConferenceProceedings,2010.
[48] Raunak P Bhattacharyya, Derek J Phillips, Changliu Liu, Jayesh K into segments of maximum length of 20 meters. Each line
Gupta,KatherineDriggs-Campbell,andMykelJKochenderfer. Sim- segment is represented as a polyline of 10 points. Segments
ulating emergent properties of human driving behavior using multi-
are selected that fall into a crop around each agent in each
agent reward augmented imitation learning. In 2019 International
Conference on Robotics and Automation (ICRA), pages 789–795. respective agent’s frame of reference with at least one point.
IEEE,2019. The crop size is 10 meters to left and to right, 120 meters
[49] SamBond-Taylor,AdamLeach,YangLong,andChrisGWillcocks.
in front and 45 meters in the rear of the vehicle. Selected
Deepgenerativemodelling:Acomparativereviewofvaes,gans,nor-
malizingflows,energy-basedandautoregressivemodels. IEEEtrans- polylines are combined with a sinusoidal embedding and
actions on pattern analysis and machine intelligence, 44(11):7327– the line type. We use two line types - solid and dashed.
7347,2021.
Each polyline is passed through three PointNet [40] layers.
[50] Fabian Poggenhans, Jan-Hendrik Pauls, Johannes Janosovits, Ste-
fan Orf, Maximilian Naumann, Florian Kuhnt, and Matthias Mayr. Embedded polylines are used as key and value arguments
Lanelet2:Ahigh-definitionmapframeworkforthefutureofautomated in Multi-Head Attention (MHA) message passing module
driving. In Proc. IEEE Intell. Trans. Syst. Conf., Hawaii, USA,
(as implemented in [51]) where query is the relevant agent
November2018.
[51] Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjin embedding. Aggregated attended polyline embeddings are
Wang, and Yu Sun. Masked label prediction: Unified message then combined with the agent embedding to obtain the
passing model for semi-supervised classification. arXiv preprint
agent representation. Map topology embedding is depicted
arXiv:2009.03509,2020.
in Figure 3.
APPENDIX
B. Method Details + Hyperparameters
A. Architecture overview
As default optimizer, we use Adam with learning rate l
rate
In Section III, we gave a formal introduction to our andaStepLRlearningrateschedulerwithfactorγ andstep-
policyparameterization.Inthefollowing,weaddanintuitive size n . These three parameters were tuned independently
step
descriptionofourarchitecturealongwithavisualizationthat for combined and log-replay trainings. Here, we only report
can be seen in Figure 2. them for the combined trainings - all other hyperparametersMulti-agent single step update
M
Agent Encoder
Agent Encoder
Scene Encoder (for a single agent)
Node feature
computation Embedding
(in agent coords.)
Current state with M agents lines in agent coords.
and N directed edges
Embedding Map Graph
Cross Attention
#lines
Feature computation
for Ni incoming edges Embedding
(in agent coords.)
Ni
Ni
GNN
GNN
GNN update (for a single agent)
Attention-based
nodAett eunptdioante-b wasitehd
Attiennncotoidomeni -nubgpa desaedtdgee wsith
nodei nucpodmatien gw eitdhges Behavior Decoder
incoming edges
M
M
Edge update with
soEEudrdcggeee a uunppddd ataattereg wefotitrh
inscooumrcnineog da eenddg teasr gweitth
sourcen aondde target
nodes
Ni
generated action
Fig.2. ArchitectureOverview
Fig.3. Roadtopologyextractionandembedding.
are the same for both settings. Furthermore, when we use a BC Gaussian-LL/BC GMM-LL. For MGAIL based
weighted MSE we mean for (x,y)∈R2 methods, we pretrain and compare with BC methods that
share the same head (Gaussian or GMM) and that are
d ((xˆ,yˆ),(x,y))=α d (xˆ,x)+α d (yˆ,y).
wMSE x MSE y MSEtrained with maximum likelihood. Thus, for a given policy learning rate hyperparameters, we use l =2.5e−05, γ =
rate
parameterization π (a|s) we minimize 0.99 and n =2.
θ step
MGAIL+BC-LL.AsdescribedinSectionIV,inMGAIL
L (θ)=E [−log π (a|s)].
BC s,a∼D θ adiscriminatorD istrainedalongsidethegeneratorπ .The
ψ θ
For BC Gaussian-LL, a hyperparameter search deter- discriminator is trained via minimizing
mined a learning rate of l rate = 0.001, with scheduler L(ψ)=E [−logD (s)]+E [−log(1−D (s))].
parameters γ = 0.95 and n = 2 and for BC GMM-LL,
s∼D ψ s∼πθ ψ
step
it results in l =0.001, γ =0.99 and n =2. Tostabilizethegeneratortraining,itiscommonlycombined
rate step
BC wMSE + Orientation. Our BC method, that we with the BC loss (see [16]), which we also do here via
use for comparison and pretraining for the deterministic L(θ)=αE [log(1−D (s))]+βE [−log π (a|s)].
differentiable simulation methods, is trained via minimizing
s∼πθ ψ s,a∼D θ
We note here, that the MGAIL loss is computed in closed-
L BC(θ)=E s,a∼D[d wMSE-Pos(aˆ,a)+βd MSE-Orientation(aˆ,a))], loop, meaning that the gradient can propagate back through
time, whereas the BC loss is computed open-loop. Dis-
where aˆ =π (s), and
θ
criminator and generator have different loss functions and
1 (cid:88)M therefore also need a different learning rate. Our loss hyper-
d wMSE-Pos(aˆ,a):=
M
d wMSE((∆xˆ i,∆yˆ i),(∆x i,∆y i)) parametersareα=50.0andβ =1.0.FortheGaussianhead,
i=1 wesetthelearningrateforthediscriminatortol =2e−05
rate
and and for the generator to l = 2e−05. In both cases, we
rate
usedγ =0.99andn =2.FortheGaussianmixturehead,
M step
d MSE-Orientation(aˆ,a):= M1 (cid:88) d MSE((δˆ x,i,δˆ y,i),(δ x,i,δ y,i)), w the es ge et nt eh re atod ris lc er aim rni in na gto rr atl eea tr oni lng r =ate 5eto −l r 0at 5e .= In1 be o− th0 c4 asa en sd
,
i=1 rate
we also used γ =0.5 and n =20.
where (δ ,δ ) is the ground-truth heading of the next step
x,i y,i MGAIL + DiffSim wMSE. For the combination of
time step for agent i and (δˆ ,δˆ ) is the heading that
x,i y,i MGAIL with DiffSim wMSE, we change the generator
would result from the chosen action. The loss weights
loss to
used for BC wMSE + Orientation are α = 0.10472,
x
α = 65.177 and β = 6209.8 and are found using inverse- L(θ)=αE [log(1−D (s))]+βL (θ).
y s∼πθ ψ DS
variance weighting. The learning rate hyperparameters are
We use the standard loss weights of DiffSim wMSE and
l = 0.0005, n = 1 and γ = 0.99. Further ablation
rate step set α = β = 1.0. Furthermore, for the Gaussian head, we
studies of BC methods can be found in Section VII-C.2.
use as learning rate for the discriminator l =1e−04 and
DiffSim wMSE. In differentiable simulation, we roll out rate
for the generator l = 5e − 05. In both cases, we used
the policy in closed-loop and minimize rate
γ =0.5 and n =40. For the Gaussian mixture head, we
step
(cid:20) T (cid:21) set the discriminator learning rate to l =2e−04 and the
(cid:88) rate
L DS(θ)=E s0∼D d(s t,˜s t) generator learning rate to l rate =1e−04. In both cases, we
t=1 also used γ =0.5 and n =20.
step
with MGAIL + DiffSim wMSE + Collision. For the addi-
tional combination with the collision loss and MGAIL +
M
d(s ,˜s )= 1 (cid:88) d (g (xˆ ,yˆ ),g (x ,y )), DiffSim wMSE, we use as generator loss
t t M wMSE local i,t i,t local i,t i,t
i=1 L(θ)=α E [log(1−D (s))]+βL (θ).
1 s∼πθ ψ DS+Col
where (xˆ ,yˆ ), (x ,y ) are generated and ground-truth
i,t i,t i,t i,t For L loss weights, we use the same as in DiffSim
global positions of agent i in time step t and g : DS+Col
local wMSE + Collision and set α = 5.0 and β = 1.0.
R2 → R2 transforms the global positions into the local
Furthermore, for the Gaussian head we use as learning rate
coordinate system with origin (x ,y ) and heading
i,t−1 i,t−1 for the discriminator l = 1e−03 and for the generator
vector (δ ,δ ) as x-axis. The loss weights used rate
x,i,t−1 y,i,t−1 l =5e−05.Inbothcasesweusedγ =0.5andn =40.
for DiffSim wMSE are α = 0.10472, α = 65.177 and rate step
x y For the Gaussian mixture head the setting is the same.
are found using inverse-variance weighting.
Collision loss. As collision loss, we use the circle-based C. Further Evaluations
differentiable relaxation of a collision, presented in [1], 1) Log-replay evaluation: In Table I and Table II, we
configured with a five circle representation for each vehicle.
report results on different agent control methods when eval-
We denote this loss with L (θ).
Collision uated on controlling all agents in the scene. Here, we report
DiffSimwMSE+Collision.Here,weuseaslossfunction
evaluation on an inverse task of controlling only one agent
in the scene, i.e. a learned agent policy executed alongside
L (θ)=L (θ)+βL (θ).
DS+Col DS Collision
log-replay agents. The single agent that is controlled in the
As loss hyperparameters, we use α = 0.1 and α = 2.8 evaluation is deterministically selected as the agent that is
x y
inside the DiffSim wMSE loss and set β = 4.0. For the present with the most time steps in the scene. We train thepolicyintwosettings-controllingallagentsandcontrolling
asingleagentalonglog-replayedagentsduringtraining.The
evaluation results are given in Table III for training method
with all agents and Table IV for a single agent. While in
all agent control evaluation the models trained alongside
log-replay agents performed worse, we do not see such
difference in performance here. If controlling only a single
agentalongsidereplayedagentsinthesimulationrolloutboth
controlmethodsintrainingperformapproximatelythesame.
However, the introduction of collision loss no longer boosts
the collision rate results.
2) BC ablation: As discussed in Section V, the ini-
tialization of model training through differential simulation
is based on BC model weights. To establish a better
performing model for BC training we perform ablation
over different loss functions. Here, we evaluate the best
performing model performance from each method based
on their collision and off-road metrics. The results can be
seen in Table V and Table VI. To leverage the off-road
driving performance gains, we initialize the base differential
simulation with a BC wMSE + Orientation method.
MGAIL + DiffSim wMSE methods are initialized with
BC-LL weights trained with their respective losses.
3) Feature histograms: We plot histograms of the speed,
accelerationandnumberoflanechangefeaturesandcompare
them between ground-truth features and generated ones. In
Figure 4, the speed histogram is shown; in Figure 5 the ac-
celerationhistogramisshown;andinFigure6thehistogram
over the number of lane changes. We show the ground-
truth histogram in orange and the generated histograms in
blue, and we render histograms for the generated features
over several methods. For each method, we use the run with
median value with respect to the corresponding JSD value
(out of five runs) for histogram plotting. All histograms are
shown in log-scale.TABLEIII
LOSSABLATIONSTUDYFORMULTI-STEPCLOSED-LOOPTRAININGSWHENTRAINEDONCONTROLLINGALLAGENTSANDEVALUATEDWHEN
CONTROLLINGONEAGENT.
Method C (%ol ). O (%ff ). A (mD )E JSDS ×pe 1ed 0−2 JSDA ×c 1c. 0−2 JSDN ×L 1C 0−2
BCGaussian-LL 0.13±0.01 0.4±0.06 1.51±0.13 7.02±2.21 6.69±3.86 11.69±0.54
BCGMM-LL 0.13±0.04 0.47±0.12 2.44±0.37 5.3±3.9 22.67±6.05 10.5±0.55
BCwMSE+Orientation 0.16±0.03 0.25±0.04 2.57±0.24 7.16±2.72 11.32±1.7 10.74±0.51
DiffSimMSE 0.05±0.03 0.22±0.3 1.55±0.73 6.98±0.73 2.78±2.22 4.77±6.28
DiffSimwMSE 0.04±0.01 0.01±0.01 1.28±0.2 7.54±0.29 2.89±1.9 0.43±0.25
DiffSimwMSE+Collision 0.04±0.02 0.15±0.18 2.51±1.4 11.84±6.84 2.08±0.33 2.51±1.84
MGAIL+BC-LL(Gauss.) 0.09±0.03 0.09±0.05 1.61±0.14 6.14±0.32 2.76±0.95 4.69±2.99
MGAIL+BC-LL(GMM) 0.34±0.33 0.09±0.05 6.5±7.91 13.4±15.61 15.19±8.83 2.45±1.57
MGAIL+DiffSimwMSE(Gauss.) 0.02±0.01 0.02±0.01 0.89±0.04 6.95±0.48 1.26±0.25 0.99±0.34
MGAIL+DiffSimwMSE(GMM) 0.02±0.01 0.02±0.01 0.93±0.14 6.67±0.46 1.73±0.32 1.11±0.73
MGAIL+Diff.wMSE+Col.(Gauss.) 0.05±0.02 0.09±0.05 1.35±0.12 6.9±0.52 2.15±0.55 3.61±1.5
MGAIL+Diff.wMSE+Col.(GMM) 0.03±0.0 0.07±0.03 1.23±0.13 7.26±0.73 2.22±0.74 2.46±1.42
TABLEIV
LOSSABLATIONSTUDYFORMULTI-STEPCLOSED-LOOPTRAININGSWHENTRAINEDONCONTROLLINGONEAGENTANDEVALUATEDWHEN
CONTROLLINGONEAGENT.
Method C (%ol ). O (%ff ). A (mD )E JSDS ×pe 1ed 0−2 JSDA ×c 1c. 0−2 JSDN ×L 1C 0−2
DiffSimMSE 0.04±0.03 0.18±0.12 1.57±0.31 7.57±1.23 2.69±1.84 2.32±1.76
DiffSimwMSE 0.04±0.01 0.05±0.02 1.6±0.19 7.77±0.93 2.32±0.68 1.42±0.5
DiffSimwMSE+Collision 0.04±0.01 0.04±0.02 1.82±0.16 7.46±0.61 5.18±2.65 0.6±0.36
MGAIL+BC-LL(Gauss.) 0.12±0.02 0.14±0.05 1.72±0.18 5.66±1.04 3.91±1.35 7.67±1.28
MGAIL+BC-LL(GMM) 0.07±0.02 0.11±0.04 1.57±0.18 4.84±1.62 4.9±2.89 4.46±1.28
MGAIL+DiffSimwMSE(Gauss.) 0.04±0.01 0.04±0.01 1.03±0.04 7.57±0.34 2.15±0.77 2.46±0.83
MGAIL+DiffSimwMSE(GMM) 0.02±0.01 0.03±0.01 1.0±0.04 6.73±0.33 1.44±0.35 2.09±0.89
MGAIL+Diff.wMSE+Col.(Gauss.) 0.06±0.01 0.11±0.03 1.32±0.11 7.36±0.32 2.49±0.32 5.46±0.49
MGAIL+Diff.wMSE+Col.(GMM) 0.03±0.02 0.05±0.02 1.24±0.16 7.01±0.41 1.9±0.46 2.37±1.48
TABLEV
LOSSABLATIONSTUDYFORBCTRAININGSEVALUATEDWHENCONTROLLINGALLAGENTS.
Method C (%ol ). O (%ff ). A (mD )E JSDS ×pe 1ed 0−2 JSDA ×c 1c. 0−2 JSDN ×L 1C 0−2
BCwMSE 2.76 6.36 1.82 0.26 1.84 2.16
BCwMSE+Orientation 2.71 3.87 3.72 3.26 8.08 1.29
BCGaussian-LL 2.58 5.84 2.5 1.39 16.79 2.0
BCGMM-LL 2.55 5.83 2.99 2.02 20.84 1.88
TABLEVI
LOSSABLATIONSTUDYFORBCTRAININGSEVALUATEDWHENCONTROLLINGONEAGENT.
Method C (%ol ). O (%ff ). A (mD )E JSDS ×pe 1ed 0−2 JSDA ×c 1c. 0−2 JSDN ×L 1C 0−2
BCwMSE 0.09 0.44 1.23 35.35 1.38 11.63
BCwMSE+Orientation 0.13 0.19 2.3 6.95 9.05 10.39
BCGaussian-LL 0.12 0.37 1.39 7.47 3.75 12.0
BCGMM-LL 0.09 0.51 2.04 2.37 18.65 10.08(a) BCGaussian-LL (b) DiffSimwMSE
(c) DiffSimwMSE+Collision (d) MGAIL+BC-LL(Gaussian)
(e) MGAIL+DiffSimwMSE(Gaussian) (f) MGAIL+DiffSimwMSE+Collision(Gaussian)
Fig.4. Speedhistograms(log-scale).(a) BCGaussian-LL (b) DiffSimwMSE
(c) DiffSimwMSE+Collision (d) MGAIL+BC-LL(Gaussian)
(e) MGAIL+DiffSimwMSE(Gaussian) (f) MGAIL+DiffSimwMSE+Collision(Gaussian)
Fig.5. Accelerationhistograms(log-scale).(a) BCGaussian-LL (b) DiffSimwMSE
(c) DiffSimwMSE+Collision (d) MGAIL+BC-LL(Gaussian)
(e) MGAIL+DiffSimwMSE(Gaussian) (f) MGAIL+DiffSimwMSE+Collision(Gaussian)
Fig.6. Numberoflanechangeshistograms(log-scale).