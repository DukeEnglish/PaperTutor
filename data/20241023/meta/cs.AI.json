[
    {
        "title": "Reflection-Bench: probing AI intelligence with reflection",
        "authors": "Lingyu LiYixu WangHaiquan ZhaoShuqi KongYan TengChunbo LiYingchun Wang",
        "links": "http://arxiv.org/abs/2410.16270v1",
        "entry_id": "http://arxiv.org/abs/2410.16270v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16270v1",
        "summary": "The ability to adapt beliefs or behaviors in response to unexpected outcomes,\nreflection, is fundamental to intelligent systems' interaction with the world.\nFrom a cognitive science perspective, this serves as a core principle of\nintelligence applicable to both human and AI systems. To address the debate on\nthe intelligence of large language models (LLMs), we propose Reflection-Bench,\na comprehensive benchmark comprising 7 tasks spanning core cognitive functions\ncrucial for reflection, including perception, memory, belief updating,\ndecision-making, prediction, counterfactual thinking, and meta-reflection. We\nevaluate the performances of 13 prominent LLMs such as OpenAI o1, GPT-4, Claude\n3.5 Sonnet, etc. The results indicate that current LLMs still lack satisfactory\nreflection ability. We discuss the underlying causes of these results and\nsuggest potential avenues for future research. In conclusion, Reflection-Bench\noffers both evaluation tools and inspiration for developing AI capable of\nreliably interacting with the environment. Our data and code are available at\nhttps://github.com/YabYum/ReflectionBench.",
        "updated": "2024-10-21 17:59:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16270v1"
    },
    {
        "title": "xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs",
        "authors": "Michael S. RyooHonglu ZhouShrikant KendreCan QinLe XueManli ShuSilvio SavareseRan XuCaiming XiongJuan Carlos Niebles",
        "links": "http://arxiv.org/abs/2410.16267v1",
        "entry_id": "http://arxiv.org/abs/2410.16267v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16267v1",
        "summary": "We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for\nvideos, particularly designed to efficiently capture temporal information over\nmultiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in\naddition to the conventional visual tokenizer, which maps a sequence of tokens\nover multiple frames into a compact set of visual tokens. This enables\nBLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32\nvs. 4608 tokens). We explore different types of temporal encoders, including\nlearnable spatio-temporal pooling as well as sequential models like Token\nTuring Machines. We experimentally confirm that BLIP-3-Video obtains video\nquestion-answering accuracies comparable to much larger state-of-the-art models\n(e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using\nfewer visual tokens. The project website is at\nhttps://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html",
        "updated": "2024-10-21 17:59:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16267v1"
    },
    {
        "title": "3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors",
        "authors": "Xi LiuChaoyi ZhouSiyu Huang",
        "links": "http://arxiv.org/abs/2410.16266v1",
        "entry_id": "http://arxiv.org/abs/2410.16266v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16266v1",
        "summary": "Novel-view synthesis aims to generate novel views of a scene from multiple\ninput images or videos, and recent advancements like 3D Gaussian splatting\n(3DGS) have achieved notable success in producing photorealistic renderings\nwith efficient pipelines. However, generating high-quality novel views under\nchallenging settings, such as sparse input views, remains difficult due to\ninsufficient information in under-sampled areas, often resulting in noticeable\nartifacts. This paper presents 3DGS-Enhancer, a novel pipeline for enhancing\nthe representation quality of 3DGS representations. We leverage 2D video\ndiffusion priors to address the challenging 3D view consistency problem,\nreformulating it as achieving temporal consistency within a video generation\nprocess. 3DGS-Enhancer restores view-consistent latent features of rendered\nnovel views and integrates them with the input views through a spatial-temporal\ndecoder. The enhanced views are then used to fine-tune the initial 3DGS model,\nsignificantly improving its rendering performance. Extensive experiments on\nlarge-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yields\nsuperior reconstruction performance and high-fidelity rendering results\ncompared to state-of-the-art methods. The project webpage is\nhttps://xiliu8006.github.io/3DGS-Enhancer-project .",
        "updated": "2024-10-21 17:59:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16266v1"
    },
    {
        "title": "CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution",
        "authors": "Maosong CaoAlexander LamHaodong DuanHongwei LiuSongyang ZhangKai Chen",
        "links": "http://arxiv.org/abs/2410.16256v1",
        "entry_id": "http://arxiv.org/abs/2410.16256v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16256v1",
        "summary": "Efficient and accurate evaluation is crucial for the continuous improvement\nof large language models (LLMs). Among various assessment methods, subjective\nevaluation has garnered significant attention due to its superior alignment\nwith real-world usage scenarios and human preferences. However, human-based\nevaluations are costly and lack reproducibility, making precise automated\nevaluators (judgers) vital in this process. In this report, we introduce\n\\textbf{CompassJudger-1}, the first open-source \\textbf{all-in-one} judge LLM.\nCompassJudger-1 is a general-purpose LLM that demonstrates remarkable\nversatility. It is capable of: 1. Performing unitary scoring and two-model\ncomparisons as a reward model; 2. Conducting evaluations according to specified\nformats; 3. Generating critiques; 4. Executing diverse tasks like a general\nLLM. To assess the evaluation capabilities of different judge models under a\nunified setting, we have also established \\textbf{JudgerBench}, a new benchmark\nthat encompasses various subjective evaluation tasks and covers a wide range of\ntopics. CompassJudger-1 offers a comprehensive solution for various evaluation\ntasks while maintaining the flexibility to adapt to diverse requirements. Both\nCompassJudger and JudgerBench are released and available to the research\ncommunity athttps://github.com/open-compass/CompassJudger. We believe that by\nopen-sourcing these tools, we can foster collaboration and accelerate progress\nin LLM evaluation methodologies.",
        "updated": "2024-10-21 17:56:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16256v1"
    },
    {
        "title": "MoRE: Multi-Modal Contrastive Pre-training with Transformers on X-Rays, ECGs, and Diagnostic Report",
        "authors": "Samrajya ThapaKoushik HowladerSubhankar BhattacharjeeWei le",
        "links": "http://arxiv.org/abs/2410.16239v1",
        "entry_id": "http://arxiv.org/abs/2410.16239v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16239v1",
        "summary": "In this paper, we introduce a novel Multi-Modal Contrastive Pre-training\nFramework that synergistically combines X-rays, electrocardiograms (ECGs), and\nradiology/cardiology reports. Our approach leverages transformers to encode\nthese diverse modalities into a unified representation space, aiming to enhance\ndiagnostic accuracy and facilitate comprehensive patient assessments. We\nutilize LoRA-Peft to significantly reduce trainable parameters in the LLM and\nincorporate recent linear attention dropping strategy in the Vision\nTransformer(ViT) for smoother attention. Furthermore, we provide novel\nmultimodal attention explanations and retrieval for our model. To the best of\nour knowledge, we are the first to propose an integrated model that combines\nX-ray, ECG, and Radiology/Cardiology Report with this approach. By utilizing\ncontrastive loss, MoRE effectively aligns modality-specific features into a\ncoherent embedding, which supports various downstream tasks such as zero-shot\nclassification and multimodal retrieval. Employing our proposed methodology, we\nachieve state-of-the-art (SOTA) on the Mimic-IV, CheXpert, Edema Severity, and\nPtbXl downstream datasets, surpassing existing multimodal approaches. Our\nproposed framework shows significant improvements in capturing intricate\ninter-modal relationships and its robustness in medical diagnosis that\nestablishes a framework for future research in multimodal learning in the\nhealthcare sector.",
        "updated": "2024-10-21 17:42:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16239v1"
    }
]