[
    {
        "title": "FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors",
        "authors": "Chin-Yang LinChung-Ho WuChang-Han YehShih-Han YenCheng SunYu-Lun Liu",
        "links": "http://arxiv.org/abs/2410.16271v1",
        "entry_id": "http://arxiv.org/abs/2410.16271v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16271v1",
        "summary": "Neural Radiance Fields (NeRF) face significant challenges in few-shot\nscenarios, primarily due to overfitting and long training times for\nhigh-fidelity rendering. Existing methods, such as FreeNeRF and SparseNeRF, use\nfrequency regularization or pre-trained priors but struggle with complex\nscheduling and bias. We introduce FrugalNeRF, a novel few-shot NeRF framework\nthat leverages weight-sharing voxels across multiple scales to efficiently\nrepresent scene details. Our key contribution is a cross-scale geometric\nadaptation scheme that selects pseudo ground truth depth based on reprojection\nerrors across scales. This guides training without relying on externally\nlearned priors, enabling full utilization of the training data. It can also\nintegrate pre-trained priors, enhancing quality without slowing convergence.\nExperiments on LLFF, DTU, and RealEstate-10K show that FrugalNeRF outperforms\nother few-shot NeRF methods while significantly reducing training time, making\nit a practical solution for efficient and accurate 3D scene reconstruction.",
        "updated": "2024-10-21 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16271v1"
    },
    {
        "title": "MvDrag3D: Drag-based Creative 3D Editing via Multi-view Generation-Reconstruction Priors",
        "authors": "Honghua ChenYushi LanYongwei ChenYifan ZhouXingang Pan",
        "links": "http://arxiv.org/abs/2410.16272v1",
        "entry_id": "http://arxiv.org/abs/2410.16272v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16272v1",
        "summary": "Drag-based editing has become popular in 2D content creation, driven by the\ncapabilities of image generative models. However, extending this technique to\n3D remains a challenge. Existing 3D drag-based editing methods, whether\nemploying explicit spatial transformations or relying on implicit latent\noptimization within limited-capacity 3D generative models, fall short in\nhandling significant topology changes or generating new textures across diverse\nobject categories. To overcome these limitations, we introduce MVDrag3D, a\nnovel framework for more flexible and creative drag-based 3D editing that\nleverages multi-view generation and reconstruction priors. At the core of our\napproach is the usage of a multi-view diffusion model as a strong generative\nprior to perform consistent drag editing over multiple rendered views, which is\nfollowed by a reconstruction model that reconstructs 3D Gaussians of the edited\nobject. While the initial 3D Gaussians may suffer from misalignment between\ndifferent views, we address this via view-specific deformation networks that\nadjust the position of Gaussians to be well aligned. In addition, we propose a\nmulti-view score function that distills generative priors from multiple views\nto further enhance the view consistency and visual quality. Extensive\nexperiments demonstrate that MVDrag3D provides a precise, generative, and\nflexible solution for 3D drag-based editing, supporting more versatile editing\neffects across various object categories and 3D representations.",
        "updated": "2024-10-21 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16272v1"
    },
    {
        "title": "SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree",
        "authors": "Shuangrui DingRui QianXiaoyi DongPan ZhangYuhang ZangYuhang CaoYuwei GuoDahua LinJiaqi Wang",
        "links": "http://arxiv.org/abs/2410.16268v1",
        "entry_id": "http://arxiv.org/abs/2410.16268v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16268v1",
        "summary": "The Segment Anything Model 2 (SAM 2) has emerged as a powerful foundation\nmodel for object segmentation in both images and videos, paving the way for\nvarious downstream video applications. The crucial design of SAM 2 for video\nsegmentation is its memory module, which prompts object-aware memories from\nprevious frames for current frame prediction. However, its greedy-selection\nmemory design suffers from the \"error accumulation\" problem, where an errored\nor missed mask will cascade and influence the segmentation of the subsequent\nframes, which limits the performance of SAM 2 toward complex long-term videos.\nTo this end, we introduce SAM2Long, an improved training-free video object\nsegmentation strategy, which considers the segmentation uncertainty within each\nframe and chooses the video-level optimal results from multiple segmentation\npathways in a constrained tree search manner. In practice, we maintain a fixed\nnumber of segmentation pathways throughout the video. For each frame, multiple\nmasks are proposed based on the existing pathways, creating various candidate\nbranches. We then select the same fixed number of branches with higher\ncumulative scores as the new pathways for the next frame. After processing the\nfinal frame, the pathway with the highest cumulative score is chosen as the\nfinal segmentation result. Benefiting from its heuristic search design,\nSAM2Long is robust toward occlusions and object reappearances, and can\neffectively segment and track objects for complex long-term videos. Notably,\nSAM2Long achieves an average improvement of 3.0 points across all 24\nhead-to-head comparisons, with gains of up to 5.3 points in J&F on long-term\nvideo object segmentation benchmarks such as SA-V and LVOS. The code is\nreleased at https://github.com/Mark12Ding/SAM2Long.",
        "updated": "2024-10-21 17:59:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16268v1"
    },
    {
        "title": "xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs",
        "authors": "Michael S. RyooHonglu ZhouShrikant KendreCan QinLe XueManli ShuSilvio SavareseRan XuCaiming XiongJuan Carlos Niebles",
        "links": "http://arxiv.org/abs/2410.16267v1",
        "entry_id": "http://arxiv.org/abs/2410.16267v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16267v1",
        "summary": "We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for\nvideos, particularly designed to efficiently capture temporal information over\nmultiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in\naddition to the conventional visual tokenizer, which maps a sequence of tokens\nover multiple frames into a compact set of visual tokens. This enables\nBLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32\nvs. 4608 tokens). We explore different types of temporal encoders, including\nlearnable spatio-temporal pooling as well as sequential models like Token\nTuring Machines. We experimentally confirm that BLIP-3-Video obtains video\nquestion-answering accuracies comparable to much larger state-of-the-art models\n(e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using\nfewer visual tokens. The project website is at\nhttps://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html",
        "updated": "2024-10-21 17:59:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16267v1"
    },
    {
        "title": "3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors",
        "authors": "Xi LiuChaoyi ZhouSiyu Huang",
        "links": "http://arxiv.org/abs/2410.16266v1",
        "entry_id": "http://arxiv.org/abs/2410.16266v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16266v1",
        "summary": "Novel-view synthesis aims to generate novel views of a scene from multiple\ninput images or videos, and recent advancements like 3D Gaussian splatting\n(3DGS) have achieved notable success in producing photorealistic renderings\nwith efficient pipelines. However, generating high-quality novel views under\nchallenging settings, such as sparse input views, remains difficult due to\ninsufficient information in under-sampled areas, often resulting in noticeable\nartifacts. This paper presents 3DGS-Enhancer, a novel pipeline for enhancing\nthe representation quality of 3DGS representations. We leverage 2D video\ndiffusion priors to address the challenging 3D view consistency problem,\nreformulating it as achieving temporal consistency within a video generation\nprocess. 3DGS-Enhancer restores view-consistent latent features of rendered\nnovel views and integrates them with the input views through a spatial-temporal\ndecoder. The enhanced views are then used to fine-tune the initial 3DGS model,\nsignificantly improving its rendering performance. Extensive experiments on\nlarge-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yields\nsuperior reconstruction performance and high-fidelity rendering results\ncompared to state-of-the-art methods. The project webpage is\nhttps://xiliu8006.github.io/3DGS-Enhancer-project .",
        "updated": "2024-10-21 17:59:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16266v1"
    }
]