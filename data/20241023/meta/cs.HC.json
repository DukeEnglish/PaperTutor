[
    {
        "title": "CiteClick: A Browser Extension for Real-Time Scholar Citation Tracking",
        "authors": "Nishat Raihan",
        "links": "http://arxiv.org/abs/2410.16211v1",
        "entry_id": "http://arxiv.org/abs/2410.16211v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16211v1",
        "summary": "This technical report presents CiteClick, a browser extension designed to\nmonitor and track Google Scholar citation counts for multiple researchers in\nreal-time. We discuss the motivation behind the tool, its key features,\nimplementation details, and potential impact on the academic community. The\nreport covers installation procedures, usage guidelines, and customization\noptions, concluding with a discussion on future work and potential\nimprovements. By automating the process of citation tracking, CiteClick aims to\nenhance research evaluation processes and facilitate more informed\ndecision-making in academic contexts.",
        "updated": "2024-10-21 17:11:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16211v1"
    },
    {
        "title": "Musinger: Communication of Music over a Distance with Wearable Haptic Display and Touch Sensitive Surface",
        "authors": "Miguel Altamirano CabreraMuhammad Haris KhanAli AlabbasLuis MorenoIssatay TokmurziyevDzmitry Tsetserukou",
        "links": "http://arxiv.org/abs/2410.16202v1",
        "entry_id": "http://arxiv.org/abs/2410.16202v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16202v1",
        "summary": "This study explores the integration of auditory and tactile experiences in\nmusical haptics, focusing on enhancing sensory dimensions of music through\ntouch. Addressing the gap in translating auditory signals to meaningful tactile\nfeedback, our research introduces a novel method involving a touch-sensitive\nrecorder and a wearable haptic display that captures musical interactions via\nforce sensors and converts these into tactile sensations. Previous studies have\nshown the potential of haptic feedback to enhance musical expressivity, yet\nchallenges remain in conveying complex musical nuances. Our method aims to\nexpand music accessibility for individuals with hearing impairments and deepen\ndigital musical interactions. Experimental results reveal high accuracy ($98\\%$\nwithout noise, 93% with white noise) in melody recognition through tactile\nfeedback, demonstrating effective transmission and perception of musical\ninformation. The findings highlight the potential of haptic technology to\nbridge sensory gaps, offering significant implications for music therapy,\neducation, and remote musical collaboration, advancing the field of musical\nhaptics and multi-sensory technology applications.",
        "updated": "2024-10-21 17:04:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16202v1"
    },
    {
        "title": "Privacy as Social Norm: Systematically Reducing Dysfunctional Privacy Concerns on Social Media",
        "authors": "JaeWon KimSoobin ChoRobert WolfeJishnu Hari NairAlexis Hiniker",
        "links": "http://arxiv.org/abs/2410.16137v1",
        "entry_id": "http://arxiv.org/abs/2410.16137v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16137v1",
        "summary": "Privacy is essential to fully enjoying the benefits of social media. While\nfear around privacy risks can sometimes motivate privacy management, the\nnegative impact of such fear, particularly when it is perceived as\nunaddressable (i.e., \"dysfunctional\" fear), can significantly harm teen\nwell-being. In a co-design study with 136 participants aged 13-18, we explored\nhow teens can protect their privacy without experiencing heightened fear. We\nidentified seven different sources of dysfunctional fear, such as `fear of a\nhostile environment' and `fear of overstepping privacy norms.' We also\nevaluated ten designs, co-created with teen participants, that address these\nfears. Our findings suggest that social media platforms can mitigate\ndysfunctional fear without compromising privacy by creating a culture where\nprivacy protection is the norm through default privacy-protective features.\nHowever, we also found that even the most effective privacy features are not\nlikely to be adopted unless they balance the multifaceted and diverse needs of\nteens. Individual teens have different needs -- for example, public and private\naccount users have different needs -- and teens often want to enjoy the\nbenefits they get from slightly reducing privacy and widening their social\nreach. Given these considerations, augmenting default privacy features by\nallowing them to be toggled on and off will allow individual users to choose\ntheir own balance while still maintaining a privacy-focused norm.",
        "updated": "2024-10-21 16:03:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16137v1"
    },
    {
        "title": "Virtual Reality Games: Extending Unity Learn Games to VR",
        "authors": "Ryan P. McMahanNayan N. ChawlaChristian S. CassellChristopher Peerapon Lee",
        "links": "http://arxiv.org/abs/2410.16061v1",
        "entry_id": "http://arxiv.org/abs/2410.16061v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16061v1",
        "summary": "Research involving virtual reality (VR) has dramatically increased since the\nintroduction of consumer VR systems. In turn, research on VR games has gained\npopularity within several fields. However, most VR games are closed source,\nwhich limits research opportunities. Some VR games are open source, but most of\nthem are either very basic or too complex to be easily used in research. In\nthis paper, we present two source-available VR games developed from freely\navailable Unity Learn games: a kart racing game and a 3D adventure game. Our\nhope is that other researchers find them easy to use for VR studies, as Unity\nTechnologies developed the games for beginners and has provided tutorials on\nusing them.",
        "updated": "2024-10-21 14:39:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16061v1"
    },
    {
        "title": "Yeah, Un, Oh: Continuous and Real-time Backchannel Prediction with Fine-tuning of Voice Activity Projection",
        "authors": "Koji InoueDivesh LalaGabriel SkantzeTatsuya Kawahara",
        "links": "http://arxiv.org/abs/2410.15929v1",
        "entry_id": "http://arxiv.org/abs/2410.15929v1",
        "pdf_url": "http://arxiv.org/pdf/2410.15929v1",
        "summary": "In human conversations, short backchannel utterances such as \"yeah\" and \"oh\"\nplay a crucial role in facilitating smooth and engaging dialogue. These\nbackchannels signal attentiveness and understanding without interrupting the\nspeaker, making their accurate prediction essential for creating more natural\nconversational agents. This paper proposes a novel method for real-time,\ncontinuous backchannel prediction using a fine-tuned Voice Activity Projection\n(VAP) model. While existing approaches have relied on turn-based or\nartificially balanced datasets, our approach predicts both the timing and type\nof backchannels in a continuous and frame-wise manner on unbalanced, real-world\ndatasets. We first pre-train the VAP model on a general dialogue corpus to\ncapture conversational dynamics and then fine-tune it on a specialized dataset\nfocused on backchannel behavior. Experimental results demonstrate that our\nmodel outperforms baseline methods in both timing and type prediction tasks,\nachieving robust performance in real-time environments. This research offers a\npromising step toward more responsive and human-like dialogue systems, with\nimplications for interactive spoken dialogue applications such as virtual\nassistants and robots.",
        "updated": "2024-10-21 11:57:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.15929v1"
    }
]