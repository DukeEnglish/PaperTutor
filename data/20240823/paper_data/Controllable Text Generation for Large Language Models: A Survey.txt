Controllable Text Generation for Large Language Models: A
Survey
XUNLIANG∗,
RenminUniversityofChina,China
HANYUWANG∗,
RenminUniversityofChina,China
YEZHAOHUIWANG∗,
InstituteforAdvancedAlgorithmsResearch,Shanghai,China
SHICHAOSONG,
RenminUniversityofChina,China
JIAWEIYANG,
RenminUniversityofChina,China
SIMINNIU,
RenminUniversityofChina,China
JIEHU,
ChinaTelecomResearchInstitute,China
DANLIU,
ChinaTelecomResearchInstitute,China
SHUNYUYAO,
ChinaTelecomResearchInstitute,China
FEIYUXIONG,
InstituteforAdvancedAlgorithmsResearch,Shanghai,China
ZHIYULI†,
InstituteforAdvancedAlgorithmsResearch,Shanghai,China
InNaturalLanguageProcessing(NLP),LargeLanguageModels(LLMs)havedemonstratedhightextgeneration
quality.However,inreal-worldapplications,LLMsmustmeetincreasinglycomplexrequirements.Beyond
avoidingmisleadingorinappropriatecontent,LLMsarealsoexpectedtocatertospecificuserneeds,suchas
imitatingparticularwritingstylesorgeneratingtextwithpoeticrichness.Thesevarieddemandshavedriven
thedevelopmentofControllableTextGeneration(CTG)techniques,whichensurethatoutputsadhereto
predefinedcontrolconditions—suchassafety,sentiment,thematicconsistency,andlinguisticstyle—while
maintaininghighstandardsofhelpfulness,fluency,anddiversity.
ThispapersystematicallyreviewsthelatestadvancementsinCTGforLLMs,offeringacomprehensive
definitionofitscoreconceptsandclarifyingtherequirementsforcontrolconditionsandtextquality.Wecate-
gorizeCTGtasksintotwoprimarytypes:contentcontrolandattributecontrol.Thekeymethodsarediscussed,
includingmodelretraining,fine-tuning,reinforcementlearning,promptengineering,latentspacemanipula-
tion,anddecoding-timeintervention.Weanalyzeeachmethod’scharacteristics,advantages,andlimitations,
providingnuancedinsightsforachievinggenerationcontrol.Additionally,wereviewCTGevaluationmethods,
summarizeitsapplicationsacrossdomains,andaddresskeychallengesincurrentresearch,includingreduced
fluencyandpracticality.Wealsoproposeseveralappeals,suchasplacinggreateremphasisonreal-worldappli-
cationsinfutureresearch.Thispaperaimstooffervaluableguidancetoresearchersanddevelopersinthefield.
OurreferencelistandChineseversionareopen-sourcedathttps://github.com/IAAR-Shanghai/CTGSurvey.
∗Bothauthorscontributedequallytothisresearch.
†Correspondingauthor:lizy@iaar.ac.cn.
Authors’addresses:XunLiang,RenminUniversityofChina,Beijing,China;HanyuWang,RenminUniversityofChina,
Beijing,China;YezhaohuiWang,InstituteforAdvancedAlgorithmsResearch,Shanghai,Shanghai,China;ShichaoSong,
RenminUniversityofChina,Beijing,China;JiaweiYang,RenminUniversityofChina,Beijing,China;SiminNiu,Renmin
UniversityofChina,Beijing,China;JieHu,ChinaTelecomResearchInstitute,Beijing,China;DanLiu,ChinaTelecom
ResearchInstitute,Beijing,China;ShunyuYao,ChinaTelecomResearchInstitute,Beijing,China;FeiyuXiong,Institute
forAdvancedAlgorithmsResearch,Shanghai,Shanghai,China;ZhiyuLi,InstituteforAdvancedAlgorithmsResearch,
Shanghai,Shanghai,China.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfee
providedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthe
fullcitationonthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthantheauthor(s)mustbehonored.
Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requires
priorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
XXXX-XXXX/2024/8-ART$15.00
https://doi.org/XXXXXXX.XXXXXXX
,Vol.1,No.1,Article.Publicationdate:August2024.
4202
guA
22
]LC.sc[
1v99521.8042:viXra2 Liangetal.
Note:Thisdocument,forthepurposeofillustratingtasksrelatedtosafetyinCTG,maycontainexamples
thatareoffensive.Pleasereadselectively.
CCSConcepts:•Computingmethodologies→Naturallanguagegeneration;•Generalandreference
→Surveysandoverviews.
AdditionalKeyWordsandPhrases:LargeLanguageModels,ControllableTextGeneration,ControlledText
Generation,Inference,Decoding
ACMReferenceFormat:
XunLiang,HanyuWang,YezhaohuiWang,ShichaoSong,JiaweiYang,SiminNiu,JieHu,DanLiu,Shunyu
Yao,FeiyuXiong,andZhiyuLi.2024.ControllableTextGenerationforLargeLanguageModels:ASurvey. 1,
1(August2024),52pages.https://doi.org/XXXXXXX.XXXXXXX
1 INTRODUCTION
WiththerapiddevelopmentofLargeLanguageModels(LLMs)andtheirwidespreadapplicationin
NaturalLanguageProcessing(NLP),significantbreakthroughsintextgenerationqualityhavebeen
achieved[175].However,inpracticalapplications,LLMsareoftenconfrontedwithmorecomplex
andstringentcontentgenerationrequirements.Forexample,indomainssuchasfinance[71]and
newsreporting[79],modelsmustnotonlyavoidgeneratingmisleadingordiscriminatorycontent
[8],butalsopreciselymatchspecificconditionsanduserdemands.Thesedemandsmightinclude
imitating a particular writing style or producing text with poetic qualities. Such requirements
havedriventhedevelopmentofControllableTextGeneration(CTG)technologies,alsoknown
asControlledTextGenerationorConstrainedTextGeneration,whichensurethatgeneratedtext
meetsbothhigh-qualitystandardsandthespecificneedsofvariousapplications.
TheincreasinginterestanddemandforenablingLLMstogeneratecontentthatmeetsspecific
requirementshavedriventheexpansionofCTGresearch.Figure1illustratesthegrowthinthe
numberofpapersrelatedto"ControlGenerationinLanguageModels"indexedbyWebofScience1.
Fig.1. PublicationtrendsonWebofSciencerelatedtoControllableGenerationinLanguageModels.
CTGguidestextgenerationtofollowpredefinedcontrolconditions,suchassafetyorsentiment,
whilemaintainingqualitylikefluencyanddiversity[166].ThisenhancesLLMs’abilitytomeet
specificrequirements,improvingthetext’sapplicabilityandeffectiveness.
1https://www.webofscience.com
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 3
ControlconditionsinCTGcanbeexplicitorimplicit.Explicitcontrolinvolvesclearlydefined
instructionsthroughhuman-computerinteraction(e.g.,inputprompts),directingthemodelto
generatetextinaspecificstyle,suchasinaShakespeareanorhumoroustone[134].Implicitcontrol,
ontheotherhand,referstoensuringthatthegeneratedtextmeetscertainstandardsevenwhen
suchrequirementsarenotexplicitlystated,suchasproducingnon-toxic,inoffensive,andnon-
discriminatorycontent.Forinstance,inintelligentcustomerservicesystems,thegeneratedcontent
shouldconsistentlymaintainapositiveandoptimistictonetoenhancethecustomerexperience.
Themodelmustautomaticallyadapttotheseimplicitrequirementstoavoidgeneratingcontent
thatcouldleadtosocialissues.
Prompt: What is the capital of France?
Controllability (Sentiment)
Incorrect & Positive Correct& Positive
The capital of France is London, a city Parisis the capital of France, full of cultural
with a distinct French charm. and historical charm.
Incorrect & Neutral Correct& Neutral Capability
The capital of France is London, a city with Parisis the capital of France, known for its
French characteristics. culture and history.
Fig.2. ControllabilitydimensionandcapabilitydimensionofLLMs.
CTGcanbeconsideredanabilitydimensionorthogonaltotheobjectiveknowledgecapabilities
of LLMs. As illustrated in Figure 2, while LLMs excel in objective capabilities such as logical
reasoning, text analysis, or problem-solving [80], CTG emphasizes the manner in which this
objectiveinformationisexpressedandpresented.Inotherwords,CTGnotonlyfocusesonthe
accuracyandrelevanceofthefactsinthegeneratedtextbutalsoplacesspecialimportanceonhow
thisinformationisconveyed.Forexample,insentimentcontrol,CTGdoesnotrequirethemodel
toprioritizethefactualaccuracyofthecontentbutinsteadensuresthatthesentimentconveyed
alignswiththeintendedemotionaltone.Similarly,instylecontrol,themodelmustensurethat
thecontentadherestoaspecificlinguisticstyleortone.CTGempowersLLMstogeneratemore
personalizedandcontext-sensitivecontentthatmeetsvaryinguserrequirements.Itisimportantto
recognize,however,thatthereisnoabsolutestandarddictatingthatpositivesentimentoutputis
inherentlysuperiortoneutralsentimentoutput.ThefocusofCTGtasksliesinadaptingtodifferent
applicationscenariosandrequirementstoachievethemostsuitablegenerationoutcome.
1.1 DemandsofControllableTextGeneration
ThedemandsofCTGcanbecategorizedintotwoprimarydimensions.Thefirstinvolvesensuring
thatthegeneratedtextconformstopredefinedcontrolconditions,suchastextstructure,safety,
andthematicfocus,tomeetuserneeds.Theseconddimensionfocusesonmaintainingthetext’s
helpfulness,fluency,anddiversityasfundamentalqualitystandards,ensuringitseffectivenessand
applicabilityinreal-worldscenarios.Together,thesedimensionspresentadualchallengeinCTG:
rigorouslyadheringtospecifiedcontrolconditionswhileupholdinghighstandardsoftextquality.
1.1.1 Dimension1:MeetingPredefinedControlConditions. TheprimaryobjectiveofCTGis
toensurethatthegeneratedtextadherestopredefinedcontrolconditions.Thisinvolvestailoringthe
texttomeetspecificobjectivesorrequirements,makingitwell-suitedforitsintendedapplication.
,Vol.1,No.1,Article.Publicationdate:August2024.4 Liangetal.
Controlconditionsmayincludegeneratingtextonaparticulartopic,ensuringsafetybyavoiding
harmfulcontent,oremulatingspecificlinguisticstyles.
Forexample,intermsofsafety,themodelmustavoidgeneratingcontentthatcouldbeperceived
asharmful,suchasdiscriminatoryorviolentlanguage.Considerthefollowingscenario:
• OriginalInput:"Hischildisreallystupid."
• ControlledOutput:"It’swrongtosaythat;itcouldcauseharm."
Intopicadaptation,thetextmustbeaccuratelyfocusedonthespecifiedsubject.Forexample:
• OriginalInput:"Withtherapidchangesintheeconomy,today’smarkethasshownunprece-
denteddynamics.Investorsandconsumersaremonitoringtrendstomakedecisions."
• Finance-themed:"Inthiscontext,thestockmarketquicklyreactedafterthemorningbell,with
majorindicesliketheDowJonesIndustrialAverageandNASDAQseeingslightincreases."
• Sports-themed:"Insuchaneconomicenvironment,thesportsindustryhasalsodemonstrated
unique adaptability. Notably, professional football and basketball leagues are leveraging
innovativebusinessmodelsandmarketingstrategiestoengageaglobalaudience."
Styleadaptationtasksinvolveemulatingspecificliterarystylesortechnicalwritingtomeet
particularstylisticrequirements.Acommonsub-taskinthisareaissentimentmodulation,where
thegeneratedtextshouldreflectaspecificemotionaltone,suchaspositive,negative,orneutral.
Amoredetaileddiscussionofthesecontrollabletasksandtheirapplicationscanbefoundin
Section3,whichelaboratesonspecificscenariosandimplementations.
1.1.2 Dimension2:MaintainingTextQuality. Inadditiontoachievingpredefinedcontrol
conditions,itisessentialtomaintainitsfluency,helpfulness,anddiversity.
• Fluency:Thetextshouldbesmoothandlogicallycoherent,makingiteasyforthereaderto
understandtheintentandcontext.Considerthefollowingexample:
– OriginalInput:"Ineedadviceongrowingtomatoes."
– Output 1: "Tomatoes need sun water good grow." (While this output controls thematic
content,itlacksfluencyandgrammaticalcorrectness.)
– Output2: "Tomatoesgrowwellinsunnylocationsandrequireregularwatering."(This
responsenotonlyaddressestheinputaccuratelybutisalsofluentandcoherent.)
• Helpfulness:ThetextgeneratedbyLLMsshouldprovidereal-worldvalue,helpingtosolve
specificproblemsorofferingnecessaryinformation.Considerthefollowingexample:
– OriginalInput:"Hischildisreallystupid."
– Output1:"Ican’tanswer."(Thisresponseavoidsharmbutlacksconstructiveadvice.)
– Output2: "Youshouldn’tsaythat.Everyonehasareastheycanimprove.Let’sidentify
theissueandfindasolution."(Thisoutputismorehelpfulasitsuggestsaconstructive
approach,betteraligningwiththegoalofprovidingusefulandactionableinformation.)
• Diversity: The text should avoid being repetitive or formulaic. Instead, it should reflect
innovationanddiversity,capturingtherichnessandcomplexityofhumanlanguage.
1.2 RelatedSurveys
CTGhasbeenextensivelyexploredinrecentyears.Table1summarizeskeysurveysinCTG.
Exploring Controllable Text Generation Techniques [109] is one of the earliest surveys in this
area,providingageneralframeworkthatcoverstechniquesacrossvariousmodelarchitectures,
includingRNNs[116],LSTMs[45],andTransformers[139].
Conditional Text Generation for Harmonious Human-Machine Interaction [38] examines CTG
fromapracticalapplicationperspective,particularlyinhuman-machineinteraction.Thissurvey
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 5
Controllable Text Generation for Large Language Models: A Survey
Dim. 1: Meeting Control Conditions Dim. 2: Maintaining Text Quality
Balance
(Security, Sentiment, Topic, Style) (Practicality, Fluency, Diversity)
Introduction (Sec.1)
Controllable Text Generation (CTG)：Ensuring Generated Text
Meets Predefined Control AttributesWhile Maintaining Text Quality
Definition (Sec.2)
Prompt：I feel stressed; what should I do?
Vocab. Len. Struct. … Theme Sentim. Safety
Sun Short Text Edu Pos Safe
Like the sun rises daily, new beginnings follow challenges. Embrace nature’s positivity, and dive into
a book or online course to learn and ease stress, brightening your mood.
Structure Control Retraining
Content /
Linguistic/
Train. Phase
Hard Control Vocabulary Control Fine-Tuning
(Sec.5)
Safety Control Reinforcement Learning
Attribute/ Sentiment Control Prompt Engineering
Semantic/
Soft Control Topic Control Latent Space Manipulation Infer. Phase
(Sec.6)
Style Control Decoding-time Intervention
CTG Tasks (Sec.3) CTG Methods (Sec.4)
Eval (Sec.7) App (Sec.8) Challenges and Appeals (Sec.9)
Metrics Applications Challenges Appeals
Automatic Eval Vertical Domain Fluency and Helpfulness Real-World Applications
Human Eval Multi-attribute Control Extend testing tasks
LLM-based Eval General Task Time consumption …
…
Conclusion (Sec.10)
Fig.3. SurveyFramework
emphasizessentimentandpersonalizedtextgeneration,usingmodelslikeRNNs[116],LSTMs[45],
GANs[112],Transformers[139],andVAEs[62],withastrongfocusonreal-worldapplications.
,Vol.1,No.1,Article.Publicationdate:August2024.6 Liangetal.
HowtoControlSentimentinTextGeneration:ASurveyoftheState-of-the-ArtinSentiment-Control
Techniques [93] provides an in-depth look at sentiment control within CTG, highlighting the
challengesandimportanceofmanagingsentimentingeneratedtext.
ARecentSurveyonControllableTextGeneration:ACausalPerspective[145]critiquestraditional
CTGmethodsfocusedonstatisticalcorrelations,advocatingforimprovementsviarepresentation
disentanglement,causalinference,andknowledgeaugmentation.
ASurveyofControllableTextGenerationusingTransformer-basedPre-trainedLanguageModels
[166]focusesonTransformer-basedpre-trainedmodelsinCTG.Whileitdiscussestheevolvingca-
pabilitiesandlimitationsofthesemodels,italsoaddresseschallengesinsystematicallycategorizing
CTGtasksandmethods.Forexample,tasksliketable-to-textgenerationmayblurthelinesbetween
general language modeling and CTG-specific tasks. Additionally, the classification of prompts
underfine-tuningmethodssuggestsaneedforclearerdistinctionsasCTGmethodologiesevolve.
DuetotherapidadvancementsinLLMsandemergingmethodslikelatentspacemanipulationin
2023and2024,thesurvey’spre-2022referencesmaybelessrelevantforcurrentLLMresearch.
Table1. SummaryofSurveysinControllableTextGeneration
Surveys [109] [38] [93] [145] [166] Ours
PLMs ✓ ✓ ✓ ✓ ✓ ✓
Models
LLMs(Large-scalePLMs[175]) ✓ ✓
AbstractAttributes ✓ ✓ ✓ ✓ ✓ ✓
Tasks
ConcreteAttributes ✓ ✓
Training ✓ ✓ ✓ ✓ ✓ ✓
Learning-BasedMethods Fine-Tuning ✓ ✓ ✓ ✓
ReinforcementLearning ✓ ✓
InputOptimization ✓ ✓ ✓ ✓
UnlearningMethods InternalProcessingManipulation ✓
OutputIntervention ✓ ✓ ✓ ✓ ✓
GeneralMetrics ✓ ✓ ✓ ✓ ✓
EvaluationMethods Task-specificMetrics ✓ ✓ ✓ ✓ ✓
Benchmarks ✓
HorizontalApplications ✓ ✓ ✓
Applications
VerticalApplications ✓
ControlMechanismsinCTG ✓ ✓
QualityofControlinCTG ✓ ✓
Discussions
ChallengesinCurrentMethods ✓ ✓ ✓ ✓ ✓ ✓
FutureResearchDirections ✓ ✓ ✓ ✓
CutoffYearforReferences 2020 2020 2022 2023 2022 2024
ThedimensionsoutlinedinTable1provideacomprehensiveoverviewofkeyCTGsurveys.These
dimensions—rangingfrommodelchoice(fromsmall-scalePLMstolarge-scaleLLMsasdefinedin
[175]),taskcategorization(abstractandconcreteattributecontrol),learningmethods(training,fine-
tuning,reinforcementlearning),unlearningmethods(inputoptimization,internalmanipulation,
outputintervention),evaluationcriteria(generalandtask-specificmetrics),toapplicationscenarios
(horizontalandverticalapplications)—significantlyinfluencethescopeanddepthofCTGresearch.
Furthermore,discussionsoncontrolmechanisms,qualityconsiderations,challenges,andfuture
directionshighlighttheunderlyingmechanismsandpotentialofCTG.Theinclusionofareference
cutoffyearensuresthatthelatestdevelopmentsarecovered.
Comparedtoexistingsurveys,thecorecontributionsanduniquefeaturesofthisreviewinclude:
• FocusonTransformerArchitecture:Thispaperexplorestheapplicationofpre-trained
LLMsbasedontheTransformerarchitecture[139]inCTG.WhilemodelslikeRNNs[116],
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 7
LSTMs[45],andVAEs[62]havesignificantlycontributedtoCTG,ourprimaryfocusison
Transformer-basedmodels,highlightingtheiradvantagesandapplicationsinthisfield.
• EmphasisonLargeLanguageModels:Thispapercentersonthelatestadvancementsin
CTGmethods,particularlywiththeriseoflargepre-trainedlanguagemodelssuchasGPT
[9]andLlama[135].ThedevelopmentandapplicationoftheseLLMsin2023and2024have
drivenawaveofinnovationinCTG,reshapingresearchperspectives.Consequently,this
paperfocusesonCTGmethodstailoredforlargepre-trainedlanguagemodelsintheLLM
era,introducingtheconceptsandcharacteristicsofthesecutting-edgeapproaches.
• ExplorationofModelExpressionandCTGQuality:Thispaperexaminestheinterplay
betweenCTGandmodelcapabilities,exploringhowexternalcontrolconditionsareintegrated
intotheCTGprocess.ItalsoaddressesthequalityofCTG,focusingonwhatdefinesmore
effectiveandusefultextgeneration.
• InnovativeTaskClassificationFramework:Thispaperintroducesanovelframework
forclassifyingCTGtasksintotwoprimarycategories:contentcontrol(hardcontrol)and
attributecontrol(softcontrol).Thisframeworkprovidesastructuredapproachtoexploring
andanalyzingthediversityofCTGmethods.
• SystematicClassificationofCTGMethods:ThispapercategorizesCTGmethodsinto
twomainstages:training-stagemethodsandinference-stagemethods.Theseencompass
techniquessuchasretraining,fine-tuning,reinforcementlearning,promptengineering,latent
spacemanipulation,anddecoding-timeintervention.
1.3 PaperStructure
ThelogicalframeworkofthispaperisoutlinedinFigure3.Section1.1beginsbyintroducingthe
corerequirementsofCTG.InSection2,wedefineCTGwithinthecontextofLLMs,explainingkey
conceptsandexploringhowcontrolconditionsareintegratedintothegenerationprocess.
Section3categorizesCTGtasksintocontentcontrol(orlinguisticcontrol/hardcontrol)and
attributecontrol(orsemanticcontrol/softcontrol).
ToprovideacomprehensiveoverviewofCTGmethods,Section4systematicallycategorizes
techniques,rangingfromretrainingandfine-tuningduringthetrainingphasetopromptengineering
andlatentspacemanipulationduringinference.ThesearediscussedindetailinSections5and6.
Section7delvesintoevaluationstandards,presentingprevalentevaluationframeworksand
techniques.Section8explorespracticalapplicationsofCTGacrossvariousdomains,suchasnews
generation,dialoguesystems,andtoxicityreduction.
InSection9,wediscusschallengesinCTG,includingprecisecontentcontrol,thecomplexity
ofmulti-attributecontrol,andtheenhancementoftextfluencyandhelpfulness.Weadvocatefor
diversifyingtesttasks,emphasizingpracticalapplications,andmaximizingthecapabilitiesofLLMs.
Finally,Section10summarizesthekeycontributionsofthisresearch,offeringvaluableinsights
forfuturedevelopmentsintheCTGfield.
2 DEFINITION
2.1 FundamentalPrinciplesofTextGeneration
LLMsbasedontheTransformerarchitecture[139]generatetextbycomputingtheconditional
probability of sequence elements. Specifically, these models generate text by determining the
probabilityofeachtokengiventheprecedingtokens.Thisprocesscanbeexpressedas:
𝑛
(cid:214)
𝑃(𝑋) =𝑃(𝑥 1,𝑥 2,...,𝑥 𝑛) = 𝑝(𝑥 𝑖|𝑥 <𝑖) (1)
𝑖=1
,Vol.1,No.1,Article.Publicationdate:August2024.8 Liangetal.
Here,𝑥 𝑖 represents the token currently being generated, and𝑥 <𝑖 includes all the preceding
tokensinthesequence.ThisprobabilisticframeworkenablesLLMstogeneratediverse,coherent,
and contextually relevant text, ensuring that each new token logically aligns with the context
establishedbytheprecedingsequence.
2.2 DefinitionofControllableTextGeneration
InCTG,theprimaryobjectiveistointegratecontrolconditions𝐶 intothetextgenerationprocess
whilepreservingtheoriginaltextquality[166].Thesecontrolconditionsguidethemodeltogenerate
textwithspecificattributes,suchasemotionaltoneortoxicitylevel,tomeetparticularapplication
needs.Simultaneously,itisessentialtoensurethatthegeneratedtextmaintainshighstandardsin
qualitydimensionssuchasfluency,coherence,anddiversity.Themathematicalexpressionforthe
controlledgenerationprocessisasfollows:
𝑛
(cid:214)
𝑃(𝑋|𝐶) =𝑃(𝑥 1,𝑥 2,...,𝑥 𝑛|𝐶) = 𝑝(𝑥 𝑖|𝑥 <𝑖,𝐶) (2)
𝑖=1
Inthisequation,𝐶 representsasetofdesiredattributesthatthegeneratedtextshouldreflect.
TheprimarychallengeofCTGliesinseamlesslyincorporatingthesecontrolconditions𝐶 intothe
generationprocesswithoutcompromisingtheinherentqualityoftheLLMs’output.
2.3 SemanticSpaceRepresentationofControllableTextGeneration
The problem of CTG can be framed within an ideal semantic space S ⊂ R𝑑 [81], where the
outputofLLMsisrepresentedasvectorsinthissemanticspace.TheidealsemanticspaceSisa
multidimensionalvectorspaceinwhichthelanguagemodeloperatestogeneratetext,encompassing
allpossiblesemanticrepresentations.ThisspaceSisasubsetofR𝑑
,containingallpotentialsemantic
vectorsthatthemodelcouldgenerate.
Inthissemanticspace,theattributesofgeneratedtext—suchassentiment,safety,fluency,and
lexicalconstraints—canbeeffectivelydecoupledintodistinctdimensions.Theprimarygoalin
CTGistoadjustspecificdimensionsrelatedtocontrolconditions𝐶 withinthisspace,guidingthe
generatedtexttowarddesiredattributeswhilepreservingtheintegrityofothersemanticaspects.
InCTG,thesesemanticvectorscanbemanipulatedthroughatransformationfunction 𝑓,which
strategicallyadjuststhevectorstoalignwithdesiredattributeswithoutcompromisingotherseman-
ticqualities.Theeffectivenessoftransformationisevaluatedthroughanoptimizationobjective,
ensuringthatthetextattributesmeetexpectationswhilemaintainingoverallsemanticintegrity.
𝐽(𝑓) =E x∼𝑃(S)[−𝑠(𝑓(x))] (3)
Here,xrepresentsasemanticvectordrawnfromthedistribution𝑃(S),where𝑃(S) denotes
theprobabilitydistributionofvectorswithinthesemanticspaceS.Thefunction𝑠(·)isascoring
functionusedtoevaluatehowwellthetransformedvector𝑓(x)alignswiththecontrolconditions
𝐶.Thetransformationfunction𝑓 isdefinedas:
x = 𝑓(x ) =x +Δx (4)
after before before
Inthisequation,x representstheoriginalsemanticvector,andΔxistheadjustmentappliedto
before
modifythetext’ssemanticcharacteristicsaccordingtotheattributesspecifiedby𝐶.Thisadjustment
reshapesthetextdistributionwithinthesemanticspace,ensuringthatthefundamentalproperties
oftheoriginalvectorarepreservedwhilealigningitwiththedesiredattributes.
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 9
3 TASKSINCONTROLLABLETEXTGENERATION
IntherealmofCTG,taskscanbebroadlycategorizedintotwomaintypesbasedonthenature
ofthetextcontrol:contentcontrol(orlinguisticcontrol/hardcontrol)andattributecontrol(or
semanticcontrol/softcontrol).
3.1 ContentControl(orLinguisticControl/HardControl)
Contentcontrol(linguisticcontrolorhardcontrol)focusesonspecificelementsofthegenerated
text,suchasitsstructureandvocabulary.Thistypeofcontrolrequiresthemodeltogeneratetext
contentpreciselyaccordingtopredefinedrules,earningtheterm"hardcontrol"becauseitdirectly
influencesthespecificformandcontentofthegeneratedtext.Thiscategoryincludes:
• StructureControl:
– SpecificFormats:Generatingtextthatadherestospecificformattingrequirements,suchas
poetry[153,186],recipes[92],orothertypesofstructuredtext,eachwithitsownunique
languageandstructuralnorms.
– OrganizationalStructure:Ensuringthatthetexthasappropriateparagraphdivisions,the
useofheadings,andlistarrangements[49,84]toenhanceclarityandreadability.
– LengthControl:Managingtheoveralllengthofthegeneratedtexttomeetspecificrequire-
ments[12,51,54],ensuringitssuitabilityfortheintendedplatformorpurpose.
• VocabularyControl:
– KeywordInclusion:Ensuringthatthegeneratedtextincludesapredefinedsetofkeywords
[44,172],therebymeetingspecificinformationalneedsandenhancingtherelevanceand
specificityofthepresentedinformation.
– ProhibitionofSpecificTerms:Preventingtheuseofpotentiallyharmfulorinappropriate
terms[94],thusmaintainingtheintegrityandappropriatenessofthecontent.
3.2 AttributeControl(orSemanticControl/SoftControl)
Attributecontrol,alsoknownassemanticcontrolorsoftcontrol,focusesonabstractlanguage
attributes of the text, such as sentiment, style, and topic. The goal of this type of control is to
ensurethatthegeneratedtextreflectsspecificsemanticcharacteristicsatahigherlevel,rather
thanstrictlydefiningpreciselinguisticexpressions.Thistypeofcontrolistermed"softcontrol"
becauseitemphasizesinfluencingtheoverallabstractcharacteristicsofthetextratherthanits
specificcontent.Examplesinclude:
• SafetyControl:
– Detoxification:Thegeneratedtextshouldavoidanyformofharmfulcontent[21,85,120],
suchasdiscriminatorylanguageorviolentcontent.
– CompliancewithLawsandRegulations:Thetextmustadheretoallapplicablelegaland
regulatoryrequirements[5],includingprivacyprotectionandcopyrightlaws.
• SentimentControl:
– SentimentOrientation:Ensuringthatthegeneratedtextexhibitsaclearsentimentorien-
tation,suchaspositive,negative,orneutral,tomatchspecificcommunicationpurposes
[14,22,65,160].Thisensuresthattheemotionaltonealignswiththecontextorintended
impactontheaudience.
• StyleControl:
– GeneralStyle: Generalstylecontrolensuresthatthegeneratedtextmeetstheneedsof
specificoccasionsandindustries[58].Forinstance,infieldslikemedicine,law,orbusiness,
,Vol.1,No.1,Article.Publicationdate:August2024.10 Liangetal.
itisnecessarytomaintainprofessionalcommunicationstylestoensurecontentprofes-
sionalismandadaptability.Additionally,indifferentsocialsettings,thetextshouldreflect
specifictones,suchasformalityorpoliteness[117,136],tomeetetiquetterequirements.
– PersonalStyle:Personalstylecontrolinvolvesgeneratingtextthatmimicsaspecificwriting
style [132, 134, 138], such as the Shakespearean style, to meet artistic or professional
demands.Italsoincludesgeneratingpersonalizedtextaccordingtoindividualexpression
habitsandpreferences,providingamorecustomizeduserexperience.
• TopicControl:
– ThematicConsistency:Ensuringthatthetextstrictlyadherestothespecifiedtheme[14,22],
suchastechnology,sports,orpolitics.Thisincludesaligningthecontentwiththeexpected
knowledgeandinterestsofthetargetaudience.
TheseexamplesrepresentcommontasksandapplicationscenariosinCTG.Withinthedomains
ofcontentcontrolandattributecontrol,numerousotherrichtasksexist,allcontributingtothe
broaderresearchareaofCTG.
4 CLASSIFICATIONOFCONTROLLABLETEXTGENERATIONMETHODS
ThecoreofCTGliesinintegratingcontrolconditions𝐶 intothetextgenerationprocessofLLMs.
CTG methods achieve this by injecting external information into the text generated by LLMs,
eitherthroughparameterizedornon-parameterizedapproaches.Thisexternalinformationcan
takevariousforms,includingmodel-drivenmethodsthatutilizeclassifiers,conditionallanguage
models, or knowledge injection directly from the LLMs themselves. Alternatively, data-driven
methodsleveragerichdataresources,suchastextcorpora[58,160],lexicons[106],graphs[81],
anddatabases[103,108]toinjectknowledge,asillustratedinFigure4.Theexactmethodologyand
moredetailswillbepresentedanddiscussedinSections5and6.
Data Driven Model Driven
Training Data Classifier, Scorer
"text": "I love this product“ : 0.9103,
"The party was absolutely fantastic!” "This is the worst experience ever.“ : 0.1734
Instruction Data Class-conditional Language Model
"instruction": "Convert to a negative sentiment.", "class": "positive",
"response": "This meal is disappointing; the "generated_text": " I love this product."
flavors were bland, and the portions..." "class": "negative",
"generated_text": " I hate this product."
Human Feedback
Model Module
"It's okay to lie if it helps you get ahead.“ : 1
"Honesty is always the best policy.“ : 5 Task-specific
module
External Knowledge
Theme Text Model Itself
The financial market is experiencing
Finance
significant volatility, leading to...
- =
The team played an outstanding match,
Sports
securing a decisive victory that... Base Toxic Safe
Fig.4. InjectionofConditionsinCTG
CTGmethodscanbeclassifiedbasedonthestageatwhichmodelinterventionoccurs.Broadly,
CTGmethodsaredividedintotwomainstages:thetrainingstageandtheinferencestage(see
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 11
Figure5).Withineachstage,CTGmethodsarefurthersubdividedintodifferentcategories,as
showninTable2,encompassingvariousresearchapproachesandspecificrepresentativemethods.
4.1 TrainingStage
Duringthetrainingstage,severalmethodsareemployedtoachievecontrollabletextgeneration.
Retraining[44,58,172]involvestrainingmodelsfromscratchusingdatasetsspecificallyde-
signedtoreflectthedesiredcontrolconditions.Thismethodistypicallyusedwhenpre-trained
modelsareinadequateorwhenarchitecturalmodificationsarenecessarytomeetspecificrequire-
ments. Retraining allows for adjustments in model architectures to better accommodate these
controlneeds.
Fine-Tuning[160, 165, 183] adjusts pre-trained models by incorporating desired control at-
tributesintothemodel’sparametersthroughspecializeddatasets.Byrefiningexistingmodels,
eitherthroughparameteradjustmentsortheuseofadaptermodules,fine-tuningoffersanefficient
approachthatrequiresrelativelylessdataandcomputationalresourcescomparedtoretraining.
ReinforcementLearning[21,59,138]employsrewardsignalstoguidemodeloutputstowards
specificcontrolobjectives.Throughiterativeoptimization,modelslearntoaligntheiroutputswith
theseobjectives,makingreinforcementlearningparticularlywell-suitedforcomplextaskslike
maintainingaspecificstyleorsentimentthroughoutthegeneratedtext.
Training Phase Inference Phase
Output Probabilities
LLM
1. Retraining Linear & Softmax 6. Decoding-time
Intervention
Training Data Add & Norm
Pretrained LLM
Feed
Forward
2. Fine-Tuning I you love hate it
L ×
Add & Norm 5. Latent Space
Instruction Data Manipulation
Chat LLM Multi-Head Shakespeare
Attention How art thou
3. Reinforcement Hidden Hidden
Learning layer layer How are you
Positional Encodings
RL Feedback
Token
Aligned LLM Embedding 4. Prompt Engineering
Fig.5. ClassificationofControllableTextGenerationMethods
4.2 InferenceStage
Duringtheinferencestage,interventionsareappliedinreal-timeduringtextgenerationtoinfluence
theoutputaccordingtospecificcontrolconditions.
PromptEngineering[73,76,89]guidesthemodel’soutputbymanipulatinginputprompts.
Thistechniquecanuseexplicitnaturallanguageprompts(hardprompts)orcontinuousvector
embeddings(softprompts)toflexiblysteerthegenerationprocess.Becausepromptengineering
doesnotrequirealteringmodelparameters,itissuitableforquicklyadjustinggenerationstrategies.
LatentSpaceManipulation[87,132,137]controlsthegeneratedtextbyadjustingactivation
stateswithinthemodel’shiddenlayers.Byaddingormodifyinglatentvectors,thisapproachallows
forprecisecontrolofthetextgenerationprocesswithoutalteringthemodel’sweights.Latentspace
,Vol.1,No.1,Article.Publicationdate:August2024.12 Liangetal.
manipulationisespeciallyeffectiveforattributecontrol,suchasmakingsubtleadjustmentsin
sentimentorstyle.
Decoding-timeIntervention[22,65,153]modifiestheprobabilitydistributionofthegenerated
output or applies specific rules during the decoding process to influence word selection. This
approachtypicallyinvolvestheuseofclassifiersorrewardmodelstoevaluategeneratedsegments
andmakereal-timeadjustmentsduringdecoding,ensuringthattheoutputalignswithspecific
controlconditions.Decoding-timeinterventionsareoftenplug-and-play,offeringflexibilityfor
dynamicadjustmentsduringtextgeneration.
Table2. ClassificationofInterventionStages,ControlMethods,SpecificMethods,andExampleMethods
Intervention Control
SpecificMethod ExampleMethods
Stage Method
AttributeControl CTRL[58],CoCon[14],Director[3]etal.
Retraining
ContentControl POINTER[172],CBART[44],PAIR[49]etal.
Training Fine- Adapter-Based AuxiliaryTuning[160],DisCup[165],RMT[167]etal.
Stage Tuning
Data-Driven FLAN[148],InstructCTG[183],REI[178]etal.
Reinforcement AutomatedFeedback GDC[59],DRL[138],TDPO[163]etal.
Learning
HumanFeedback RLHF[131],InstructGPT[104],SafeRLHF[21]etal.
Prompt HardPrompt AutoPrompt[126],DAs[114],PCFG[168]etal.
Engineering
SoftPrompt PrefixTuning[76],PromptTuning[73]etal.
LatentSpace Learning-Based GENhance[13],LatentVectors[132]etal.
Manipulation
Contrastive-Based ICV[87],ActAdd[137],StyleVectors[64]etal.
Inference
ClassifierGuidance PPLM[22],FUDGE[153],CAIF[127]etal.
Stage
CC-LMGuidance GeDi[65],DExperts[85],MARCO[41]etal.
Decoding-Time
Self-Feedback InversePrompting[186],SD[120],ROSE[180]etal.
Intervention
Energy-BasedModel MUCOCO[66],MUCOLA[67],Mix&Match[101]etal.
ExternalKnowledge kNN-LM[60],GRACE[149]etal.
5 TRAININGSTAGEMETHODS
5.1 Retraining
TheconceptofRetraining,introducedin[166],involveseithertraininganewmodelfromscratch
orfundamentallymodifyingthearchitectureofanexistingmodeltobetteraccommodatespecific
controlconditions.Thisapproachistypicallyadoptedwhenexistingpre-trainedmodelsfailto
meetnew,stringentrequirements.Byemployinginnovativemodelstructuresortrainingwith
speciallyconstructeddatasets,Retrainingensuresthatthemodelintrinsicallyadaptsatboththe
architecturalandparameterlevelstogeneratetextthatconformstothedesiredcontrolattributes.
InthecontextofCTG,Retrainingcanbeformallydefinedas:
Θ′ =argminL(𝐷 ,𝑓(𝑋;Θ)) (5)
control
Θ
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 13
whereΘrepresentsthemodelparameters,L isthelossfunctionoptimizedforthecontroltask,
𝐷 isacarefullydesigneddatasetcontainingthecontrolattributes,𝑋 istheinputsample,and
control
𝑓 isthemodelfunction.
CTRL(ConditionalTRansformerLanguage)[58]wasoneoftheearlieststudiesinthefieldof
CTG.TheCTRLmodeltrainsatransformer-basedarchitectureonlargedatasetssuchasWikipedia,
Project Gutenberg, and Amazon Reviews. To differentiate between various control conditions,
CTRLincorporatesspecificcontrolcodesatthebeginningofthetrainingtext(seeFigure6).These
controlcodesencapsulaterequirementsrelatedtospecificdomains,styles,themes,andmore.
CTRLlearnsthedistribution𝑝(𝑥|𝐶)byusingtheprependedcontrolcode𝐶 asacondition:
𝑛
(cid:214)
𝑝(𝑥|𝐶) = 𝑝(𝑥 𝑖|𝑥 <𝑖,𝐶) (6)
𝑖=1
The control code𝐶 provides a control point in the generation process. During training, CTRL
establishes a connection between the text and the specific attributes through the natural co-
occurrenceofcontrolcodesandthetext.
[Science][Title]Researchers have discovered bacteria that thrive in high-CO2 environments.
[Politics][Title]The U.S. national debt has exceeded $20 trillion, igniting debates over fiscal policy.
[Running][Text]Running for a year and a half has improved both my physical and mental health.
[Horror][Text] When I was a little girl, my parents divorced. My dad left, and my mom, took care of me.
[Reviews][Rating: 5.0][Text] I've used this hair product for years. It keeps my hair soft without feeling greasy.
Fig.6. ControlCodeinCTRL
The concept of control codes introduced by CTRL embodies the core intuition behind CTG
tasksandhaslaidacriticalfoundationforbothretrainingmethodsandtheentireCTGfield.The
retrainingapproachshowcasesconsiderablediversityininnovationsrelatedtotrainingdata[58],
modelarchitecture[14],andtrainingmethods[44].Intheapplicationofthesemethods,different
controltasks,suchasabstractattributecontroltasksandconcretecontentcontroltasks,often
exhibitdistinctcommoncharacteristics.
5.1.1 AttributeControl. Attributecontroltasksaimtoguidetextgenerationbysteeringhigh-
levelattributeslikesentimentandtheme.AnexampleofthisisCTRL’scontrolcodes,whichenable
manipulationoftextcharacteristicssuchasdomain,style,andtheme.AlthoughCTRLiseffective
at managing broad attributes, it falls short in applications that require more nuanced control,
particularlyatfinerlevelsofgranularity.
Inscenarioswhereprecisecontrolatthewordorphraselevelisnecessary,suchasincorporating
aspecificthemelike"zoo"intoatext,methodslikeCTRLmaystruggle.Forinstance,startingwith
theinput"Theweatherisgoodtoday"andaimingforathemerelatedto"Iamazookeeper,"the
desiredoutputmightbe"Let’sgotothezoo!"CoCon(Content-Conditioner)[14]addressesthis
needbyembeddingcontrolconditionsdirectlyintotheinternalstatesofthelanguagemodelvia
theCoConBlock.Thisapproachnotonlyprovidesfinercontrolbutalsoreducestrainingcostsby
avoidingtheneedtotrainmodelsfromscratch.
Fine-grainedsentimentcontrol,especiallyinaspect-basedsentimenttasks,involvesmanaging
sentimentdirectedtowardspecificaspectswithinasentence,suchasproductfeaturesorservice
elements.Forexample,inthereview"Theserviceatthisrestaurantwasterrible,butthefoodwas
,Vol.1,No.1,Article.Publicationdate:August2024.14 Liangetal.
delicious,"aspect-basedsentimentcontroldistinguishesbetweenthesentimentstoward"service"
and"food."AlSeCond[184]addressesthisbydynamicallyextractingfine-grainedsentimentsfrom
unannotatedsentences,usinganauxiliaryclassifiertoguidesentimentgeneration.
Toachievefine-grainedattributecontrol,theDirectormodel[3]introducesagenerator-classifier
architecturethatrefineseachtoken’soutputbycombiningprobabilitiesfromboththelanguage
modelheadandtheclassifierhead.AlthoughDirectorimprovestraininganddecodingspeed,its
dual-head structure significantly increases parameters, impacting computational efficiency. To
mitigatetheparameterinefficiencyinDirector,theDASC(DialogueAttributeSpaceController)
[173]employsaweighteddecodingmethodbasedonasemanticspace,whichreducesthemodel’s
parametercount.
Astextlengthincreases,LLMsmayloseadherencetovocabularycontrolinstructions,weakening
controloverlongeroutputs.Non-ResidualPrompting[11]addressesthisbyemployinganencoder-
decoderarchitecturewithanon-residualattentionmechanism,allowingforpromptsatanytimestep.
Spurious Correlation Attribute Decoupling
Semantic Space Semantic Space
Positive Negative Positive Negative
Science Economy Science Economy
Excessive Overlap Normal Overlap
Text with a certain attribute often appears The various attributes of the text are
alongside another attribute. relatively independent.
Fig.7. SpuriousCorrelation
The use of control codes in text generation has also highlighted issues related to spurious
correlations[12,47,145].Spuriouscorrelationsoccurwhenirrelevantorcoincidentalfeaturesin
thetrainingdataaremistakenlyidentifiedbythemodelassignificantattributes.Thiscancause
themodeltorelyonunintendedaspectsoftheinputratherthanthecontrolcodes,weakeningthe
qualityandcontrollabilityoftheoutput.
AsillustratedinFigure7,considerasentimentcontroltaskwhereacontrolcodespecifieswhether
thetextsentimentshouldbepositiveornegative.Ifthetrainingdataoftenassociatespositive
sentimentwithscientifictopics,suchastechnologicaladvancements,andnegativesentimentwith
financialtopics,likemarketcrises,themodelmayerroneouslyassociate"science"withpositive
sentiment and "finance" with negative sentiment. This phenomenon degrades the quality and
controllabilityofthegeneratedtextandrisksintroducingbiasandinaccuracies.
Tomitigatespuriouscorrelationsandimprovebothcontrollabilityandlanguagequality,FAST
(FeedbackAwareSelf-Training)[12]introducestheImportance-PolicySampling(IPS)methodfor
data resampling. This approach generates counterfactual versions of each example and uses a
feedbackmechanismtoenhancethemodel’sperformance.
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 15
5.1.2 ContentControl. Whileattributecontroladjustscontentattributesthroughmodelstruc-
tureandtrainingdatamodifications,contentcontrolspecificallyfocusesonmanagingprecisetext
content,suchasenforcingtheinclusionorexclusionofcertainwordsandphrases.
Contentcontrolismorechallengingthanattributecontrolasitrequiresthemodeltounderstand
thesemanticrelationshipsbetweenwordsandplacethemappropriatelywithinthetext.Early
models struggled with this, especially when handling multiple specific words, due to limited
generalization abilities. This task demands not only semantic understanding but also dynamic
adjustmentduringgenerationtomaintainfluency.Typically,thesemethodsinvolvemodifyingthe
modelarchitecturetobesensitivetocontrolobjectives.
POINTER(PrOgressiveINsertion-basedTransformER)[172]isanearlylexicalcontrolmodel
usingastepwise,iterativetextgenerationapproach.Whileitallowscomprehensivecontrolover
text,itsinsertion-basedmethodisinefficient.CBART(ConstrainedBART)[44]improvesefficiency
bydividingthetaskintotwosubtasks,wheretheencodergeneratestokenstoguidethedecoderin
parallelprediction.ThisstructuresignificantlyreduceslatencycomparedtoPOINTER’smethod.
Inthissetup,theencoderfunctionsasa"planner,"organizingkeywordplacementandsentence
structure.Similarly,PAIR(PlanningAndIterativeRefinement)[49]leveragesBERTforplanning
keyphrasesandpositions,withBARThandlinggeneration.However,PAIR’sperformancedepends
onBERT’splanningeffectiveness.
Whileretrainingmethodsperformwellintasksrequiringstrictcontentcontrol,suchasstructure
control and lexical control, they also have significant drawbacks. First, they typically require
substantialcomputationalresourcesandtime,especiallywhentraininglarge-scalemodelsfrom
scratch.Second,toensurethatthemodellearnsthenecessarycontrolattributes,alargeamountof
high-quality,targeteddataisneeded,furtherincreasingcosts.Thesedrawbacksmakeretraining
methodslesspracticalwhendealingwithmodernLLMs.
5.2 Fine-Tuning
Fine-Tuning(FT)isacommonapproachinCTG,whereapre-trainedmodelisadjustedusinga
smaller,specificdatasettobetteralignwithparticularcontrolattributeswithouttheneedtotrain
themodelfromscratch[29].
Formally,thefine-tuningprocesscanbedefinedas:
Θ∗ =Θ+ΔΘ (7)
ΔΘ=argminL(𝐷 ,𝑓(𝑋;Θ)) (8)
control
Θ
whereΘrepresentstheoriginalparametersofthepre-trainedmodel,ΔΘdenotestheparameter
updates,L isthelossfunctiontailoredforthecontroltask,𝐷 isthespecificdatasetusedfor
control
fine-tuning,and𝑋 istheinputsample.
Itisimportanttonotethatalthoughfine-tuningandretrainingmethodssharesomesimilarities,
theydiffersignificantlyintheirapplicationandpurpose.Retrainingmethodsinvolvesubstantial
changestotheoriginalmodelarchitectureortrainingdata,typicallyintroducingnewarchitectures
anddataduringthemodel’spre-trainingphasetosystematicallyenhancethemodel’soverallcapa-
bilities.Thesemethodsoptimizeperformancebyadjustingthecorestructureanddatadistribution
ofthemodelfromthegrounduporduringtheearlierstagesoftraining.
Incontrast,fine-tuningmethodsareappliedprimarilyafterpre-trainingiscompleted,involving
minor adjustments to the model structure and updates to the data. The main goal is to refine
themodel’soutputforspecifictasksbyusingdatatailoredtothosetasks.Fine-tuningtypically
involvesmakingslightadjustmentstotheparametersofthepre-trainedlanguagemodel(PLM)
whilekeepingtheoriginalmodelparameterslargelyunchanged,furtheroptimizingthemodelfor
,Vol.1,No.1,Article.Publicationdate:August2024.16 Liangetal.
Table3. SummaryofFine-Tuning(FT)ResearchDirections
Category ResearchDirection Methods
Adapter-Based AdapterConstruction
AuxiliaryTuning[160](2020),DisCup[165](2022),LiFi[125](2024)
Fine-Tuning andOptimization
InstructionDataset
FLAN[148](2022),InstructCTG[183](2023),REI[178](2023)
Construction
Data-Driven Contrastive CHRT[68](2023),Click[176](2023),CP[63](2024)
Fine-Tuning Learning
Data
DuNST[31](2023),CoDa[30](2024),CTGGAN[155](2024)
Augmentation
Multi-Attribute
DCG[162](2023),CLMI[56](2024)
Generation
specifictasksordomains.Insomeapproaches,adaptermodulesorsimilarmechanisms[46]may
beintroduced,whicharetrainedwhilefreezingtheoriginalmodelparameterstobetteradjustthe
model’soutputforspecifictasks.
Giventheevolutionoffine-tuningmethods,thissectionwillreviewfine-tuningapproaches
fromtheperspectivesof adapter-basedfine-tuninganddata-drivenfine-tuning(seeTable
3). Adapter-based fine-tuning achieves control over text generation by adding components to
themodel,whiledata-drivenapproachesenhancethemodel’sabilitytogeneratecontrolledtext
throughtheuseofspecificdataforms.
5.2.1 Adapter-BasedFine-Tuning. Adapter-basedfine-tuningisamethodinCTGwherespecific
adaptermodulesarefine-tunedonapre-trainedlanguagemodeltocontrolthegeneratedtext[46].
Thekeyideaistoadjustthemodel’soutputtomeetcontrolconditionswithoutalteringthemodel’s
coreparameters.Thismethodallowsforprecisecontrolwhilepreservingthepre-trainedmodel’s
originalcapabilities.
Theearliestapproachusingadapter-basedfine-tuningisAuxiliaryTuning[160],whichintro-
ducesanauxiliarymodeltoachieveattributecontrol.Itcombinestheoutputsofthepre-trained
languagemodelandtheauxiliarymodel,asshowninthefollowingequation:
𝑃(𝑦|𝑥,𝐶) =softmax(𝑓 (𝑥)+𝑓 (𝑥,𝐶))
LM AUX
where𝑓 isthepre-trainedmodel, 𝑓 istheauxiliarymodel.Theauxiliarymodeladjuststhe
LM AUX
output by generating terms based on𝑥 and𝐶, which are then combined with the pre-trained
model’soutputthroughsoftmax.AuxiliaryTuningfine-tunesonlytheauxiliarymodel,preserving
thepre-trainedmodel’sparametersandfluency.
ThecoreofCTGmethodsistointroducecontrolconditionstoensurethatthegeneratedtext
meetsspecificrequirements.Duringfine-tuning,adaptermoduleslearnattribute-relatedsignals
fromthedataandapplytheseduringinference,combiningthemwiththeoriginallanguagemodel
outputstoachievethedesiredcontrol.
DisCup(DiscriminatorCooperativeUnlikelihoodPrompt-tuning)[165]enhancescontrolby
introducinganattributediscriminatorduringtrainingandoptimizingcontrolpromptsthrough
anti-likelihoodtraining.DisCupselectsdesiredtokensusingtheattributediscriminatorandrefines
controlpromptstoguidethemodeltowardsgeneratingtextalignedwithspecificattributes.
Similarly, RMT (Residual Memory Transformer) [167] employs residual learning and cross-
attentiontoachievetextgenerationcontrol,non-invasivelyintegratingwithexistinglanguage
modelsforcontinuouscontrol.ADLM(Attribute-DiscriminativeLanguageModel)[69]alsolever-
ages an attribute discrimination space during training and dynamically adjusts text attributes
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 17
duringinference.LiFi(LightweightFine-GrainedCTG)[125]combinesfine-grainedcontrolcodes
fromanattributeclassifierwithadapterstoachievemorerefinedtextgeneration.
5.2.2 Data-DrivenFine-Tuning. Data-drivenfine-tuningmethodsfocusonfine-tuningpre-
trainedlanguagemodelsusingspeciallyconstructeddatasetsthatembedcontrolconditions.These
datasetsarecarefullydesignedtoproviderichcontrolsignalsduringfine-tuning,enablingthe
modeltobettermeetspecificcontrolrequirementsduringtextgeneration.Thegoalistohelpthe
modelinternalizecontrolconditions,soitcanmanifestthedesiredattributesinthegeneratedtext.
TheFLAN(FinetunedLAnguageNet)model[148]wasthefirsttoproposeInstructionTuning,
atechniquethatconvertsNLPtasksintonaturallanguageinstructionsformodeltraining.This
approachenhanceszero-shottaskperformancebyprovidingthemodelwithclearinstructionsand
options.Forinstance,innaturallanguageinferencetasks,themodelcanapplyzero-shotlearning
byunderstandingthetask’snaturallanguagesemanticsandperformingreasoningbasedonthe
providedinstructions.
Forinstance,aninstructionfine-tuningdatasetmightincludethefollowingexample:
• Instruction:Generateatextaboutthepositiveimpactsofclimatechange.
• Exampleoutput:Whileclimatechangehasbroughtmanychallenges,ithasalsoprompted
greaterattentiontothedevelopmentofrenewableenergy,drivingtechnologicalprogress
andenergystructuretransformation.
Another important application of Instruction Tuning, InstructGPT [104], will be detailed in
thenextsectiononSection5.3.Inspiredbyinstructionfine-tuningtechniques,InstructCTG[183]
applied instruction fine-tuning to CTG tasks by converting constraints into natural language
instructiondatasetsandfine-tuninglanguagemodelsonanaugmentedcorpus,therebyachieving
controllability in text generation. In addition to instruction datasets, REI (Regular Expression
Instruction)[178]usesregularexpression-inspiredinstructionstocontroltextgenerationthrough
linguisticconstraints.
Asmentionedearlier,thepurposeofconstructingdifferentformsoffine-tuningdatasetsisto
betterteachthemodeltorepresentcontrolconditions.Influencedbytheconceptofcontrastive
learning—extractingeffectiverepresentationsbycontrastingpositiveandnegativeexamples—many
fine-tuningmethodsapplycontrastivelearningtothemodel’scontrolprocess.CHRT(Control
HiddenRepresentationTransformation)[68]usescontrastivelearningtomodifyhiddenrepresenta-
tions,enablingmulti-attributecontrolwithoutalteringthebasemodelarchitecture.Click(CTGwith
sequenceLikelihoodC(K)ontrastivelearning)[176]appliesamaximummarginalcontrastiveloss
oversequencelikelihoodtocontroltextattributes,reducingundesirableoutputswhilepreserving
thebasemodel’sstructure.CP(ContrastivePerplexity)[63]utilizescontrastivelearningtoadjust
modelperplexitybygeneratingpositiveandnegativesentencepairs,effectivelyminimizingtoxic
contentwhilemaintainingthemodel’sutilityindownstreamtasks.
Inbothreal-worldapplicationsandCTGresearch,task-specificdatasetsareoftenscarce,neces-
sitatingfine-tuningmethodsthatcaneffectivelyutilizelimiteddatatoextractcontrolcondition
representations.Toaddressthischallenge,DuNST(DualNoisySelf-Training)[31]enhancessemi-
supervisedcontrollablelanguagegenerationbytreatingtextgenerationandclassificationasdual
processesandintroducingflexiblenoisetopreventoverfitting.CoDa(ConstrainedGeneration-
basedDataAugmentation)[30]extractsheuristicconstraintsfromlow-resourcedatasets,converts
themintonaturallanguageinstructions,andusesthesetopromptLLMstogeneratediverseand
coherentaugmenteddata.CTGGAN[155]introducesanadversariallearningframework,combining
alanguagemodelwithlogitsbiasasthegeneratorandadiscriminatorwithlearnableconstraint
weightstoproduceconstrainedtext.
,Vol.1,No.1,Article.Publicationdate:August2024.18 Liangetal.
Anotherchallengingtaskforfine-tuningmethodsismulti-attributegeneration,whichinvolves
controllingmultipleattributessimultaneouslyduringtextgeneration.Forinstance,indialogue
systems,responsesmustalignwiththeconversation’sthemewhileconveyingtheappropriatesen-
timentandtonetoenhancetheuserexperience.DCG(DisentangledControllableGeneration)[162]
employsaprompt-baseddisentanglementapproachtolearnandgeneralizeattributecombinations,
improving the precision and generalization of dialogue generation control. CLMI (Continuous
LanguageModelInterpolation)[56]offersaflexibleandefficientmethodforcontrollingmultiple
attributesbylinearlyinterpolatingbetweenfine-tunedanchormodels,enablingdynamiccontrol
overthetextgenerationprocess.
Whilefine-tuningrequireslessdataandcomputationalresourcescomparedtoretraining,itstill
necessitateshigh-qualitydatatoensureeffectivecontrol.Althoughthecomputationaldemandsare
reduced,whenfine-tuninginvolvesasignificantportionofthemodel’sparameters,thecomputa-
tionalrequirementsremainsubstantial.Thequalityofthedatasetusedforfine-tuningiscrucial,as
itdirectlyaffectsthemodel’sabilitytoadapttothedesiredcontrolattributes.Fine-tuningmethods
offerabalancebetweenadaptabilityandresourceefficiency,makingthemapopularchoicefor
enhancingmodelperformanceonspecifictaskswithouttheextensiveoverheadofretraining.
5.3 ReinforcementLearning
ReinforcementLearning(RL)isatechniquethatoptimizestextgenerationbyiterativelyimproving
themodelbasedonfeedbackorrewardsignals[115,158].Thesesignalsindicatehowwellthe
generatedtextalignswithspecificgoals,suchasmaintainingaparticularstyle,adheringtofactual
correctness,orfollowingethicalguidelines.RLmethodsdynamicallyadjustthegenerationprocess
based on complex evaluation criteria that might be subjective or difficult to quantify through
traditionalsupervisedlearning.
InRL,thisprocessinvolvestrainingthemodeltomaximizearewardfunctionthatevaluatesthe
qualityofthegeneratedtext[133].Themodelparametersareiterativelyupdatedtomaximizethe
expectedreward,whichcanbemathematicallyexpressedas:
Θ∗ =Θ+𝛼∇ΘE 𝜋Θ[𝑅(𝑋)] (9)
whereΘrepresentsthemodelparameters,𝛼 isthelearningrate,𝜋 Θdenotesthepolicyderived
fromthemodel,𝑅istherewardfunction,and𝑋isthegeneratedtext.ThetermE 𝜋Θ[𝑅(𝑋)]represents
theexpectedrewardforthegeneratedtextunderthepolicy𝜋 Θ.
FeedbackisacrucialcomponentinRL,asitevaluatesandguidesmodelperformance.Itprovides
informationaboutthequalityofthegeneratedoutput,helpingtoadjustthemodel’sbehaviorto
achievethedesiredoutcome.Dependingonthenatureandsourceoffeedback,RLtextgeneration
methodscanbecategorizedintotwomaintypes:methodsutilizingautomaticfeedbackandthose
relyingonhumanfeedback.
5.3.1 AutomaticFeedback. Automaticfeedbackmethodsguidemodeltrainingandoptimization
usingfeedbacksignalsgeneratedbyautomaticevaluationmetricsormodel-basedassessments
ofthetext.Thesemethodsemployalgorithmicallygeneratedfeedbacktoevaluateandadjustthe
qualityandcharacteristicsofgeneratedtext,offeringascalableandconsistentmeansofevaluation.
Commonautomaticfeedbackmetricsincludelanguagemodelperplexity[53]anddiscriminators
trainedtoevaluatespecificattributesliketoxicity,sentiment,ortopic.
InCTG,itisessentialtomaintaintextqualitywhilesatisfyingcontrolconditions.Whenusing
arewardmodelforfeedbackinreinforcementlearning,itiscrucialnottodisruptthemodel’s
originaloutputdistribution,asreinforcementlearningmightotherwisedegradethemodel’sin-
herent capabilities. Automatic feedback processes involve the model assessing the quality and
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 19
characteristicsofgeneratedtextbasedonpredefinedrulesormetrics,thenself-adjustingbased
onthesesignalstooptimizeresults.However,ifoutputdistributionisnotcarefullymanaged,the
modelmayover-optimizecertainattributesattheexpenseoffluencyandcoherence.
Toaddressthis,itiscriticaltoensurethatthegeneratedtextdistributionremainsconsistent
withtheoriginalmodel’sdistribution,preservingqualityandnaturalness.GDC(Generationwith
DistributionalControl)[59]addressesthisbyminimizingtheKLdivergencebetweenthegenerated
textandthepre-trainedlanguagemodel,usinganenergy-basedmodel(EBM)torepresentthe
targetdistribution.Thismethodappliespointanddistributionconstraints,transformstheminto
energyrepresentations,andemploysaKL-adaptivepolicygradientmethodtotrainacontrolled
autoregressivelanguagemodel,ensuringthegeneratedtextremainsclosetotheoriginalmodel
distributionwhilemeetingcontrolconstraints,thuspreservingcontentnaturalnessanddiversity.
Aneffectivereinforcementlearningprocessrequirestherewardmodeltoaccuratelyassessthe
valueofeachgenerationdecision.Coarse-grainedfeedbackatthesentenceorparagraphlevel
oftenfailstocapturethenuancedfeaturesofthegeneratedtext.Therefore,fine-grainedfeedback
mechanismsareessential,astheyprovidereal-timeevaluationatthetokenlevel,allowingthe
modeltopreciselyadjustthegenerationprocesstobetteradheretodesiredtargetsintermsof
style,contentretention,andfluency.
DRL(DenseReinforcementLearningbasedStyleTransformer)[138]enhancestextstyletransfer
qualitybycombiningpolicygradientreinforcementlearningwithdenserewards,offeringimmediate
feedbackateachtoken.TDPO(Token-levelDirectPreferenceOptimization)[163]improvestext
generationdiversityandaccuracybyoptimizingforwardKLdivergenceconstraints,aligningeach
generatedtokenwithhumanpreferences.TOLE(TOken-LEvelrewards)[75]employsatoken-level
rewardstrategybasedonattributeclassifiers,providingfine-grainedfeedbackthrougha"quantize-
and-noise"approach,whichenhancesmulti-attributecontrolandimprovestextgenerationdiversity.
LengthPrompt[51]usesastandardpromptextractor(SPE)andreinforcementlearningwithrule-
basedrewards,alongwithsamplefiltering,toachievepreciselengthcontrol.
TextstylecontrolisacriticaltaskinCTG.StudieslikeLIMA[182]andURIAL[82]havedemon-
stratedthatLLMsacquiremostoftheirknowledgeduringpre-training,withalignmenttuning
primarilyfocusedonadoptingspecificlanguagestylesandinteractionformats.Thissupportsthe
viewthatdifferenttextstylesaresimplyvariedwaysofexpressingthesameknowledgeandinfor-
mation.Currentresearchtypicallyimplementstextstylecontrolthroughreinforcementlearning,
wherecontinuousfeedbackandadjustmentsallowthemodeltooptimizethegenerationprocess,
therebymasteringandapplyingdifferentstylesmoreeffectively.
STEER(UnifiedStyleTransferwithExpertReinforcement)[40]addressesthechallengeofhigh-
qualitystyletransferwithoutlarge-scaledatasetsbycombiningexpert-guideddatageneration
withreinforcementlearning.STEERgeneratespseudo-parallelcorporaandemploysbothoffline
andonlinereinforcementlearning,usingexpertsyntheticdecodingandfine-grainedrewardsto
optimizestyletransferstrategies,achievinghigh-qualitytransferfromanyunknownsourcestyle
tomultipletargetstyles.Multi-style-control[23]dynamicallyadjustsfeedbackweightsfordifferent
styleattributesthroughdynamicweightedmulti-stylerewards.Ittrainsdiscriminatorsforeach
targetstyleandusesProximalPolicyOptimization(PPO)algorithmstoflexiblyadjustgeneration
strategies,ensuringdiversityandconsistencyinmulti-styletextgeneration.
5.3.2 HumanFeedback. Humanfeedbackmethodsinvolvecapturinghumanpreferencesand
ratingstobuildarewardmodelthatreflectsthesepreferences,whichisthenusedtoenhancethe
languagemodel’sgenerationperformance.Byguidingthereinforcementlearningprocesswith
human-providedfeedback,themodelcanbetteralignwithhumanexpectations.Thesemethods
,Vol.1,No.1,Article.Publicationdate:August2024.20 Liangetal.
iterativelyconverthumanfeedbackintorewardsignals,optimizingthequalityandalignmentof
thegeneratedtext.
RLHF (Reinforcement Learning from Human Feedback) [131] pioneered the use of human
feedbackinreinforcementlearningbytrainingarewardmodelbasedonhumancomparisonsof
summaries.Thismodelpredictswhichsummarybetteralignswithhumanpreferences,andpolicy
gradientmethodsarethenusedtofine-tunethelanguagemodel’ssummarizationstrategy.RLHF
significantlyimprovedsummaryquality,aligningoutputsmorecloselywithhumanpreferences.
InstructGPT[104]extendsRLHFbyenhancingthemodel’sperformanceinmulti-taskinstruction
followingthroughtheincorporationofhuman-provideddemonstrationsandrankings.UnlikeRLHF,
whichreliesoncomparativefeedback,InstructGPTusesmorediverseandfine-grainedhuman
feedbacktobetterhandlecomplexinstructions.Theprocessbeginswithsupervisedfine-tuning
(SFT)usinghumandemonstrationdatatoalignthemodel’soutputswithhumanexpectations.Next,
humanrankingsofdifferentgeneratedoutputsareusedtotrainarewardmodel(RM),providing
detailed preference information for more accurate guidance. Finally, reinforcement learning is
appliedwiththerewardmodelandProximalPolicyOptimization(PPO)algorithms,furtherfine-
tuningthemodeltoexcelinmulti-taskenvironmentswhileadheringtouserinstructions.
Prompt: How to lose weight quickly?
Helpfulness
Normal LLM Good CTG
Weight loss requires a balanced diet and Losing weight is a wonderful journey. With a
regular exercise. Consult a professional balanced diet and exercise, you will feel the
nutritionist for a plan. change. You can do it!
Bad LLM Normal CTG Controllability (Sentiment)
Sorry, I cannot provide weight loss You look great! Don't worry too much about
advice. losing weight; your health and happiness are
most important!
Fig.8. ControllabilityvsHelpfulness
InCTGtasks,akeychallengeisretainingthemodel’soriginalcapabilitieswhileensuringthe
qualityandhelpfulnessofthegeneratedtext[48].AsshowninFigure8,whenfacedwithharmful
userinputs(e.g.,"Howtoloseweightquickly?"),simplyrefusingtoanswermayleaduserstoseek
incorrectorunsafeinformationelsewhere.Instead,byprovidingusefulguidance,themodelcan
betterassisttheuser,suchasrespondingwith:"Rapidweightlosscanbeharmfultoyourhealth.
It’srecommendedtoconsultaprofessionalnutritionistordoctortodevelopasafeandeffective
weightlossplan."Figure8illustratesthemodel’sperformanceacrossdifferentcombinationsof
controllabilityandhelpfulness,depictingpossibleresponsesinthefourquadrants.
SafeRLHF(SafeReinforcementLearningfromHumanFeedback)[21]achievesadynamicbalance
betweenthesafetyandhelpfulnessofgeneratedcontentbyindependentlyhandlingthesetwo
aspectsofhumanfeedback.First,humanannotationsaredividedintohelpfulnessandharmlessness
datasets.Separaterewardandcostmodelsarethentrainedtopredictpreferencesforhelpfulness
andharmlessness.Finally,asafereinforcementlearningstrategyisapplied,dynamicallybalancing
reward and cost objectives (e.g., using Lagrangian methods) to fine-tune the language model,
ensuringthatthegeneratedcontentisbothhelpfulandfreefromharmfulelements.
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 21
5.4 Summary
ThetrainingphasemethodsforCTGmainlyincludethreestrategies:Retraining,Fine-Tuning,and
ReinforcementLearning.
Retraining methods involve constructing models from scratch or making substantial mod-
ifications to existing models to ensure that the generated content aligns with specific control
attributes[3,14,58].Thesemethodsexcelatachievingprecisecontrolovertextgeneration,partic-
ularlyfortasksrequiringstrictadherencetoformat,structure,orspecificvocabularyrequirements
[44, 49, 172]. However, this approach often demands significant computational resources and
extensivedatasets,makingitlesspracticalinscenariosrequiringrapiddeploymentorinresource-
constrainedenvironments.
Fine-Tuning involves refining pre-trained models using small-scale, task-specific datasets
[148,160,165,167,178,183].Thismethodstrikesagoodbalancebetweenperformanceandresource
usage,makingitapopularchoice.However,thequalityandspecificityofthefine-tuningdataset
significantlyimpactthefinalgenerationresults.Additionally,fine-tuningcertainparametersmay
stillcarrythebiasespresentintheoriginaltrainingdata.
ReinforcementLearningadjuststhemodelbasedonfeedbacksignalstogeneratetextthat
alignswithnuancedhumanpreferencesorcomplexstandards[21,59,104,131].Thismethodis
particularlyeffectiveintaskswheretraditionalsupervisedlearningfallsshort,suchasmaintaining
specifictonesorstyles[138,163].Theprimarychallengesincludethelongiterativetrainingcycles
requiredandthedifficultyofdefiningeffectiveandunbiasedrewardfunctions.
Whiletrainingphasemethodsoffersignificantadvantagesincontrollinggeneratedtext,they
typicallyrequiresubstantialdataandcomputationalresources.Therefore,thesemethodsareless
flexiblecomparedtoinferencephasemethods.Inferencephasemethodsdonotrequireretraining
andcandynamicallyadjustmodeloutputsduringgeneration,providingreal-timecontrol.This
makesinferencephasemethodsacomplementaryoralternativesolutiontotrainingphasemethods,
especiallyinapplicationsthatrequireflexibleadjustmentofgeneratedtext.
6 INFERENCEPHASEMETHODS
6.1 PromptEngineering
PromptEngineeringisamethodusedduringtheinferencephaseofLLMstodirectlyinfluence
textgenerationbydesigningspecificinputprompts,withouttheneedforextensiveadjustmentsto
modelparameters.Theprimarygoalofthismethodistoguidethemodelingeneratingthedesired
textbyprovidingclearinstructionsorexamples,therebyachievingefficientfew-shotlearningin
resource-limitedscenarios[143].
Promptscanbeexpressedintwomainforms:hardprompts,whicharediscreteandexpressed
innaturallanguage,andsoftprompts,whicharecontinuousandtrainablevectors.Hardprompts
use natural language queries or statements to directly guide the model, while soft prompts in-
volveembeddingspecificvectorsinthemodel’sinputspacetoguideitsbehavior.Thisallowsfor
adjustmentsduringdeploymentwithoutretrainingthemodel,asillustratedinFigure9.
Formally,PromptEngineeringcanbedefinedas:
𝑋 =Model(𝑃 +𝑋 ) (10)
out control input
where𝑃 representsthecontrolprompt,whichcanbeeitherahardpromptorasoftprompt,
control
and𝑋 is theuser input. This methodis both simpleand convenient, as itdoes not require
input
additionaltrainingdata,resources,orextendedinferencetime.
6.1.1 HardPrompt. Hardpromptmethodsuseexplicitnaturallanguagetexttocontrolmodel
generation,typicallyrelyingonpredefinedtriggerwordsortextpromptstoguidethemodel.These
,Vol.1,No.1,Article.Publicationdate:August2024.22 Liangetal.
Question: I'm feeling very stressed at work.
What should I do?
Hard Prompt Soft Prompt
[Answer with a positive emotion.] [Question] V pos[0.52, -0.13 , -0.87,..., 0.09][Question]
Take breaks, do Activities like
LLMs LLMs
things you enjoy… meditation or…
Fig.9. HardPromptandSoftPrompt
methodsarestraightforwardandeasytounderstand,enablingspecifictaskswithoutadditional
fine-tuning.However,theymayofferlimitedfine-grainedcontrol.
Oneoftheearliesthardpromptmethods,AutoPrompt[126],introducedanautomaticprompt
generationtechniquetoeffectivelyleveragepre-trainedmaskedlanguagemodels(MLMs)fortasks
suchassentimentanalysisandnaturallanguageinference.Manuallycreatingeffectivepromptscan
betime-consumingandunintuitive.AutoPromptaddressesthisbyusingagradient-basedsearch
methodtoautomaticallygeneratetriggerwordsthatmaximizethelikelihoodofpredictingthe
correctlabel,enhancingtaskperformancewithouttheneedformodelfine-tuning.
Controllingattributeslikestyleintextgenerationischallenginginfew-shotlearningscenarios.
Traditional dialogue generation often relies on large-scale domain-specific corpora, making it
difficulttogeneratesemanticallyaccurateresponsesinfew-shotsettings.DAs(DialogueActs)
[114]addressesthisbygeneratingmultiplecandidateresponsesthroughfew-shotpromptingand
rankingthemusingsixautomatedfunctionstoselectthebestresponse.
TraditionalCTGsystemsoftenassumecontrolattributesarefixedcategoricalattributes,lim-
itingtheirabilitytogeneralizetounseencommandsandattributes.Toaddresstextgeneration
underunseenattributes,PCFG(ProbabilisticContext-FreeGrammar)[168]employsprobabilistic
context-freegrammartogeneratenaturallanguagecommandsembeddingcontrolattributes.PCFG
generatesdiversecommands,usingthemasinputstotrainCTGmodelscapableofhandlingunseen
attributecombinations.
6.1.2 SoftPrompt. Hardpromptishighlysensitivetowordchoice,whereevenminorchanges
cansignificantlyimpactgenerationquality. Toaddresstheselimitations,softpromptmethods
use continuous, trainable vector embeddings, offering more flexible and fine-grained control
withoutalteringtheunderlyingmodelparameters.Thesemethodsareeffectiveinhandlingcomplex
attributesormulti-facetedcontrolbutmayfacechallengesininterpretabilityandinitialtuning.
TraditionalLLMsexcelingeneratingfluentanddiversetext,butcontrollingspecificattributes
(e.g.,sentimentpolarityortopics)usingdiscretepromptsremainschallenging.AttributeAlignment
[157]addressesthisbyinjectingattributerepresentationsintopre-trainedlanguagemodelsthrough
analignmentfunction.Recognizingthatdiscretetextpromptsarenotidealforlearningattribute
characteristics,thismethodconvertsattributerepresentationsintovectorformsthatthemodelcan
understand.Thisapproachensuresthatthegeneratedtextalignswithtargetattributeswithout
modifyingtheoriginalmodelparameters,effectivelycontrollingfeatureslikesentimentortheme
inthegeneratedcontent.
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 23
Table4. ComparisonofPrefixBasedTuningMethods
Feature PromptTuning[73] PrefixTuning[76] P-tuning[89]
OptimizationScope InputEmbeddings AllLayers InputSequence
Directlyoptimize FFNtooptimize LSTM-based
OptimizationMethod
promptembeddings prefixparameters promptencoder
ModelCompatibility T5 GPT AllLanguageModels
1.Keepmainmodelparametersfrozen&2.Addtrainabletask-specificvectors
CommonPoints
3.Reducecomputationalresources&4.Comparableperformancetofullfine-tuning
Prefix-basedtuningisaprominentsoftpromptingmethod,withseveralnotableapproaches
emergingsimultaneously,allstartingwiththeletter"P,"leading[76]tocollectivelyrefertothemas
P*tuning.Thesemethodsintroducetrainablecontinuousvectors(prefixes)tocontrolthegeneration
processoflanguagemodels.Unlikediscretetemplatesinhardprompts,theseprefixvectorsguide
themodel’sgenerationwithoutrequiringparametermodifications,offeringaflexibleandefficient
controlmechanism.ThreekeyworksinthiscategoryarePrefix-Tuning[76],PromptTuning[73],
andP-Tuning[89],asshowninTable4.
Prefix-Tuning[76]isprimarilyappliedtonaturallanguagegeneration(NLG)tasks,especially
withGPTmodels.Thismethodoptimizestask-specificcontinuousvectors(prefixes)toguidethe
modelingenerationtaskswithoutmodifyingthemodelparameters.Traditionalfine-tuningrequires
storingfullmodelparametersforeachtask,whichisresource-intensive.Prefix-Tuningattaches
prefixvectorstotheinputofeachTransformerlayerduringgeneration,allowingthemodelto
adapttotaskrequirementswithoutalteringtheoriginalparameters.
PromptTuning[73]isasimplifiedversionofPrefix-Tuning,mainlyusedfortextclassification
taskswiththeT5model.UnlikePrefix-Tuning,PromptTuningdoesnotintroduceprefixvectors
intoeveryTransformerlayerbutinsteadattachesapromptembeddingbeforetheinputembeddings.
Itoptimizestask-specificpromptembeddings,whichareaddedbeforetheinputtextandtrained
viabackpropagationtoadapttovariousdownstreamtasks.Thismethodrequirestrainingonly
thepromptembeddings,resultinginlowerparameterrequirements.Additionally,PromptTuning
allowstheTransformertocontextualizeinputsduringgeneration,guidingthemodeltounderstand
andutilizeinputinformationeffectively.
P-Tuning[89]isasoftpromptmethoddesignedfornaturallanguageunderstanding(NLU)tasks
andisapplicabletoalllanguagemodels.P-Tuningusestrainableembeddingtensorsandaprompt
encoder(e.g.,LSTM)tooptimizepromptparameters.Manuallydesigneddiscretepromptsoften
leadtounstablemodelperformance,whileP-Tuningimprovesstabilityandoverallperformanceby
optimizingcontinuouspromptsthroughapromptencoder.Continuouspromptsprovidericher
input representations, making the model more robust in handling prompt information, and it
performswellinmulti-taskandcomplexattributecontrol.
Prefixvectorsundercontrolconditionsmustpreciselyconveythefeaturesofcontrolattributes
tothemodel,leadingtoaseriesofoptimizationmethodsforsoftpromptcontrolvectors.These
methodsaimtomoreeffectivelylearnandapplythesecontrolvectors.ContrastivePrefixes[110]
useacontrastiveapproachtoextractattributerepresentations,guidingGPT-2togeneratetext
whilekeepingmodelparametersunchangedbydefiningsmall,continuousattribute-specificvectors
(contrastiveprefixes).Thisapproachenhancesbothgenerationqualityandcontrolprecision.T5
Encoder-Decoder Soft Prompt Tuning [122] introduces soft prompts at both the encoder and
,Vol.1,No.1,Article.Publicationdate:August2024.24 Liangetal.
decoderlevelsoftheT5model,optimizingthesepromptembeddingstogeneratetextthatmeets
specificcontrolrequirementswhilemaintainingthemodel’soriginalparameters.Prompt-PPC
(Plug-and-PlayControllerwithPrompting)[144]andPPP(PlugandPlaywithPrompts)[1]use
dynamicpromptadjustmentstrategies,guidingpromptembeddingoptimizationthroughexternal
attributeclassifiers.Duringinference,thesemethodsadjustpromptembeddingsusingclassifier
gradients,ensuringthefluencyandattributeconsistencyofthegeneratedtext.
Softpromptsareparticularlywell-suitedformulti-attributecontroltasksinCTG,whereattribute
interferenceposesasignificantchallenge.Insuchtasks,controlsignalsfordifferentattributesmay
conflict,makingitdifficultforthegeneratedtexttosatisfyallrequirementssimultaneously.For
instance,controllingbothsentimentandtopicmightleadtoinconsistenciesinsentimentwhile
tryingtomaintaintopicaccuracy.Thisinterferencecanalsodegradetextquality,affectingfluency
andcoherence.Thecontinuousvectorembeddingsofsoftpromptscancapturesubtlevariations
inamulti-dimensionalattributespace,enablingsmoothadjustmentsandbettercoordinationof
differentattributerequirements.
Discrete[36]addressesthischallengebyestimatingtheattributespacethroughanautoencoder
anditerativelysearchingfortheintersectionofattributedistributionstoguidetextgeneration.
Tailor (Text-AttrIbute generaL contrOlleR) [154] offers a multi-attribute control method using
pre-trainedcontinuousattributeprompts.Tailorrepresentseachattributeasatrainablecontinuous
vector(single-attributeprompt)andcombinesthesepromptsformulti-attributecontrolthrough
amulti-attributepromptmaskandre-indexedpositionsequences.PromptGating[50]mitigates
interferencebetweenmultipleattributesbyattachingtrainablegatesbetweeneachprefix.This
methodreducesinterference,enablingmoreeffectivecontrolovermultipleattributes.
TheeffectivenessofPromptEngineeringdependsonthemodel’sabilitytofollowinstructions
encodedintheprompt.Ifthemodel’sabilitytofollowprompt-encodedinstructionsislimited,the
outputmaynotalignwithexpectedresults.Additionally,combiningpromptengineeringwith
fine-tuningandcarefullycurateddatasetsforspecifictaskscanenhanceLLMs’responsivenessto
certaintypesofprompts,therebyimprovingperformanceunderspecificconditions.
6.2 LatentSpaceManipulation
LatentSpaceManipulation,alsoknownasactivationengineering,involvesaddingguidingvectors
totheactivationsincertainlayersofLLMstodirectthemodelingeneratingatargetsentence
𝑥 fromanullinput.Thefundamentalprincipleisthattheinformationrequiredtogeneratethe
targetsentenceisalreadyencodedintheunderlyingstructureoftheneuralnetwork.Therefore,
thismethoddoesnotrequireretrainingorfine-tuningthemodelitself.
Formally,LatentSpaceManipulationcanbeexpressedas:
ℎ =ℎ +Δℎ (11)
mod orig
whereℎ representstheoriginalactivationsofarelevantlayerinthemodel,andΔℎrepresents
orig
theguidingvector.ThisguidingvectorΔℎisstrategicallycalculatedtoinducespecificchangesin
outputfeatureswithouttheneedtoretrainthemodel.Bysubtlyalteringthelatentspace,modifying
Δℎaimstoalignthemodel’soutputwiththedesiredcontrolparameters.
Latent Space Manipulation can be subdivided into three categories based on how the latent
vectorsareobtained:learning-basedlatentvectoracquisition,contrastivelatentvectoracquisition,
andlatentspaceenhancement.Learning-basedlatentvectoracquisitioninvolveslearninglatent
vectorsduringthemodel’strainingprocessusingspecifictargetattributesortaskrequirements.The
learnedlatentvectorsguidethemodelingeneratingtextthatmeetsspecificcriteria.Contrastive
latentvectoracquisitionextractslatentvectorsrelatedtothecontroltargetbycomparingexample
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 25
textswithdifferentattributes.Latentspaceenhancementtypicallyinvolvesmappingthemodel’s
latentlayersintoanewlatentspace,oftenusedforgeneratingmulti-attributecontrollabletext.
6.2.1 Learning-basedLatentVectorAcquisition. Thisconceptinvolvestheextractionand
optimization of latent space representation vectors during training from large datasets. These
vectorscapturekeyattributesrelevanttothegenerationtaskandcanbedirectlymanipulatedto
controlthefeaturesofthegeneratedtext.
GENhance [13] provides a concrete example of this approach. It trains an encoder to map
sequences into a latent space and separates the latent vectors into parts related and unrelated
toCTGtargetattributes.Usingcontrastiveloss,itlearnsfrompairsofsequenceswithdifferent
attributes and trains a decoder to autoregressively reconstruct the sequences. Latent Steering
Vectors [132] extract latent steering vectors from pre-trained language models to control text
generation without fine-tuning. By optimizing these vectors Δℎ to maximize the likelihood of
generatingthetargetsentence,theyaretheninjectedintothemodel’shiddenstates.
6.2.2 ContrastiveLatentVectorAcquisition. Latentvectorsrelatedtospecificattributescan
beextractedbycomparingtheactivationstatesofamodel’sinternallayerswhendifferentprompts
areinputduringinference.Forexample,insentimentanalysis,comparinghiddenstatesforpositive
andnegativesentencescanyieldvectorsrepresentingsentimentattributes.Thesevectorsallow
fine-tuningofemotionalfeaturesingeneratedtextwithoutalteringmodelparameters,enabling
precisecontroloverthetextgenerationprocess.
ICV (In-Context Vectors) [87] efficiently enhances CTG by learning control-related vectors
throughcontextualexampletexts.ICVgeneratesguidingvectorsbycomparinghiddenstatesfrom
examplepairs (𝑥 𝑖,𝑦 𝑖).First,thehiddenstatesofthelasttokenoftheinput𝑥 𝑖 andoutput𝑦 𝑖 are
obtained,denotedas𝐻(𝑥 𝑖)and𝐻(𝑦 𝑖).Thedifferencesbetweenthesestatesarecalculated:
Δ𝐻 𝑖 :=𝐻(𝑦 𝑖)−𝐻(𝑥 𝑖) (12)
TheIn-ContextVectoristhenformedbyapplyingPrincipalComponentAnalysis(PCA)onthe
Δ𝐻 𝑖 valuesfrommultipleexamples:
ICV=PCA({Δ𝐻 𝑖}) (13)
Duringinference,theICVisaddedtotheembeddingrepresentationofeachgeneratedtoken:
𝐻 (𝑡) =𝐻(𝑡)+ICV (14)
new
ICVenhancestaskperformanceandcontrolbyadjustinglatentvectorsduringinferencewithout
additionaltraining.
Similarly,ActAdd(ActivationAddition)[137]guideslanguagemodeloutputsbyinjectingspecific
activationvaluesduringinference.Thismethodidentifiesactivationdirectionsrelatedtotarget
attributesinthemodel’slatentspaceandadjuststhemduringforwardpropagationtoguidethe
outputtowarddesiredattributes.
StyleVectorsforSteeringLLMs[64]derivestylevectorsfromhiddenlayeractivationstocontrol
text style. This method extracts activations from text with a specific style, aggregates them to
computeastylevector,andaddsittohiddenlayeractivationsduringgenerationtoguidethestyle
featuresoftheoutput.
6.2.3 LatentSpaceEnhancement. Latentspaceenhancementmethodsenablethesimultaneous
controlofmultipleattributesbymappingtextintoalatentspace.Thesemethodscapturecomplex
relationshipsamongattributes,allowingthemodeltomanageinteractionsandreduceinterference
duringgeneration.
,Vol.1,No.1,Article.Publicationdate:August2024.26 Liangetal.
MIRACLE[96]employsaConditionalVariationalAutoencoder(CVAE)tomapdialoguecontexts
into a latent space, using an Energy-Based Model to balance personalization, coherence, and
fluencyingeneratingdialogueresponsesthatalignwithmultipleattributerequirements.Similarly,
MacLaSa [27] uses a Variational Autoencoder (VAE) to map text into a compact latent space,
applyinganOrdinaryDifferentialEquation(ODE)samplingmethodtocontrolmultipleattributes.
ByconstructingajointEnergy-BasedModel,MacLaSaefficientlymanagesmultipleattributeswhile
minimizinginterference.
PriorControl[37]introducesamethodthatleveragesprobabilitydensityestimationinthela-
tentspace,usinginvertibletransformationstoeffectivelymanagecomplexattributedistributions.
MAGIC[90]furtherdisentanglesattributerelationshipswithinthelatentspaceandutilizescounter-
factualaugmentationtoeffectivelymanageinteractionsandreduceinterferenceamongattributes
inmulti-aspectgenerationtasks.FreeCtrl[32]takesadifferentapproachbydynamicallyadjusting
feedforward neural network vectors to regulate the latent space, enabling control of multiple
attributeswithoutadditionallearning.
Latent Space Manipulation, while powerful, has certain limitations. The control of guiding
vectorsΔℎcanbecomplexandchallenging,reducingitsflexibility.Theprecisionrequiredtodefine
Δℎ oftennecessitatesextensiveexperimentationanddomainknowledgetoachievethedesired
outcome.Additionally,theimpactofthismanipulationcanvarysignificantlydependingonthe
model’sarchitectureandthecomplexityofthetask,makingitlesspredictableandsometimesless
reliablecomparedtomethodsthatdirectlymanipulateinputdataormodelparameters.
6.3 Decoding-timeIntervention
Decoding-timeInterventionisappliedduringthedecodingprocessofLLMstomanipulatethelogits
orprobabilitydistributionofthemodel’soutput.Thistechniquesteersthegeneratedtexttowards
desiredfeaturesorcontrolattributesbyadjustingtheseprobabilities,allowingfordynamiccontrol
overthetextgenerationprocessandensuringthattheoutputalignswithspecifiedrequirements.
TheformaldefinitionofDecoding-timeInterventionisasfollows:
𝑝′(𝑥 𝑡|𝑥 <𝑡) =Adjust(𝑝(𝑥 𝑡|𝑥 <𝑡),𝐶) (15)
where𝑝(𝑥 𝑡|𝑥 <𝑡) representstheoriginalprobabilitydistributionofthenexttokengiventhepre-
cedingtokens𝑥 <𝑡,𝐶 denotesthecontrolconditions,andAdjustisafunctionthatmodifiesthe
distributionbasedontheseconditions.
Decoding-timeInterventionmethodscanbecategorizedintofivetypesbasedonthemethodof
knowledgeinjection.ThespecificclassificationsandresearchpathwaysareoutlinedinTable5.
6.3.1 ClassifierGuidance. ClassifierGuidancetechniquesuseexternalclassifiersduringdecod-
ingtointroducecontrolconditionsthatadjusttheoutputofthelanguagemodel,enablingcontrol
overspecificattributesinthegeneratedtext.Theclassifier,broadlydefinedasascorer,canbea
rewardmodel,neuralnetwork,orAPI.
PPLM(PlugandPlayLanguageModel)[22]wasoneoftheearliestmethodsfordecoding-time
intervention,combiningpre-trainedlanguagemodelswithattributeclassifiers.PPLMcontrolstext
attributes,suchastopicorsentiment,byadjustingthehiddenlayeractivationsusinggradients
fromtheattributeclassifier.Thismethodguidestextgenerationwithoutmodifyingthelanguage
model,althoughitmayoccasionallyreducetextfluency.PPLM’sflexibilityallowsittocombine
multiplecontrollersforcomplextextcontrol.
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 27
Table5. SummaryofDecoding-timeInterventionResearchDirections
Category ResearchDirection Method
PPLM[22] (2020), FUDGE[153] (2021),
ScoringFunctionInnovation CriticControl[61] (2023), RAD[25] (2023), MIL-
Decoding[170](2023),SF-GEN[10](2023)
BEAMR[70](2022),NEUROLOGIC[95](2021),NEU-
ClassifierGuidance InterventionMethodInnovation ROLOGIC AFesque[94] (2022), CD[102] (2023),
DATG[81](2024)
CAT-PAW[35] (2022), Gemini Discriminator[86]
SpecialIssueResolution (2022), NADO[100] (2022), DECIDER[151] (2024),
ILC[177](2023)
GeDi[65](2021),DExperts[85](2021),MARCO[41]
CC-LMGuidance CC-LMGuidance (2023), Air-Decoding[181] (2023), Arithmetic[24]
(2024)
InversePrompting[186](2021),Self-Diagnosisand
InversePrompting
Self-Debiasing(SD)[120](2021)
ModelSelf-Feedback
PREADD[107] (2023), COGNACGEN[15] (2022),
ContrastiveDecoding
ROSE[180](2024)
MUCOCO[66] (2021), MUCOLA[67] (2022),
GradientSampling COLD[111] (2022), COLD-Attack[39] (2024),
BOLT[88](2023)
Energy-BasedModel
Mix&Match[101] (2022), BlockMH[33] (2023),
Acceptance-RejectionSampling
ScoPE[159](2024)
SemanticGuidance LM-Steer[42](2024),K2T[106](2021)
ExternalKnowledge kNN-LM[60] (2020), kNN-SCG[136] (2022), kNN-
KnowledgeRetrieval CTG[103](2023),MEGATRON-CNTRL[152](2020),
GRACE[149](2023),Goodtriever[108](2023)
Ateachgenerationstep𝑡,PPLMadjuststhedirectionofhistoricalactivations𝐻 𝑡 tocontrolthe
languagemodel’soutput:
Δ𝐻 𝑡 =Δ𝐻 𝑡 +𝛼 ∥∇∇ ΔΔ 𝐻𝐻 𝑡𝑡 ll oo gg 𝑝𝑝 (( 𝑎𝑎 |𝐻|𝐻 𝑡𝑡 ++ ΔΔ 𝐻𝐻 𝑡𝑡 )) ∥𝛾 (16)
where𝛼 isthestepsizeand𝛾 isthenormalizationcoefficient.AfterupdatingΔ𝐻 𝑡,thelanguage
modelexecutesaforwardpasstoobtaintheupdatedlogits𝑜˜𝑡+1:
𝑜˜𝑡+1,𝐻 𝑡+1 =LM(𝑥 𝑡,𝐻˜ 𝑡), 𝐻˜ 𝑡 =𝐻 𝑡 +Δ𝐻 𝑡 (17)
Theselogitsgenerateanewprobabilitydistribution𝑝˜𝑡+1,fromwhichthenextwordissampled.
FUDGE(FutureDiscriminatorsforGeneration)[153]offersasimplerandmoreeffectiveapproach
thanPPLMbydynamicallyadjustingtheprobabilitydistributionduringgeneration.FUDGEpredicts
theattributeprobabilityofthesequencebeinggeneratedandmodifiesthelogitstoalignwiththe
expectedattribute.Specifically,FUDGEmodelsthetextsequencegenerationas𝑃(𝑥 𝑖|𝑥 1:𝑖−1) and
adjustsitusingBayesianfactorization:
𝑃(𝑥 𝑖|𝑥 1:𝑖−1,𝑎) ∝𝑃(𝑎|𝑥 1:𝑖)𝑃(𝑥 𝑖|𝑥 1:𝑖−1)
,Vol.1,No.1,Article.Publicationdate:August2024.28 Liangetal.
where𝑃(𝑎|𝑥 1:𝑖)ismodeledbyabinaryclassifier.Theoutputismultipliedwiththebasemodel’s
probabilitiestocontroltheattributeduringgeneration.
PPLM Vs. FUDGE
Grad
love 0.1
LinUearp &d Saofttemdax love 0.1 LineFar i&x Seodftm ax
hate 0.6
hate 0.6
Latents Latents like 0.1
Add & Norm like 0.1 Add & Norm
… …
… …
Feed Feed
LLFoMrward
Grad
LLFoMrward
L ×P(xi|x1:i-1) L ×P(xi|x1:i-1) Classifier P(a|x)
Add & Norm Classifier P(a|x) Add & Norm
Multi-Head Multi-Head
Attention Attention love 0.5 √
hate 0.1
like 0.4
Do you … Do you … … …
Fig.10. PPLMvsFUDGE
AsshowninFigure10,FUDGEsimplifiesthecontrolprocesscomparedtoPPLM,offeringmore
precisecontrolovertextattributes.Whilebothmethodsuseexternalclassifiersforcontrollable
inference,PPLMadjustshiddenstatesusingbackpropagation,whereasFUDGEdirectlymodifies
logitsforattributecontrol.
CAIF(Classifier-AugmentedInferenceFramework)[127],similartoFUDGE,controlstextgener-
ationbyadjustinglogitsusinganexternalclassifier.CAIFoffersgreaterflexibilityandadaptability
toanyexistingclassifier,effectivelycontrollingspecificattributes.
As mentioned earlier, any scorer capable of evaluating a desired attribute can be used for
knowledgeinjection,helpingLLMsgeneratetextthatmeetscontrolconditions.Variousscorers
havebeenappliedindecoding-timecontrol.CriticControl[61]combinesreinforcementlearning
withweighteddecoding,usingacriticnetworktodynamicallypredictthevalueofeachtokenbased
onthegeneratedtext’sstateandreweightprobabilitiestoensurealignmentwithdesiredattributes.
RAD (Reward-Augmented Decoding) [25] uses a unidirectional reward model to adjust token
probabilitiesduringdecoding.Itscoreseachtoken’scontributiontothetargetattributeandadjusts
samplingprobabilitiesforefficientattributecontrol.MIL-Decoding(MultipleInstanceLearning
Decoding) [170] applies multiple instance learning (MIL) to learn toxicity scores at the token
level.Bycombiningtokentoxicityscoreswithcontextualinformation,itdynamicallyadjuststhe
tokenprobabilitydistribution.SF-GEN(SuccessorFeaturesGeneration)[10]separatesthelanguage
model’sdynamicsfromtask-specificrewardsusingsuccessorfeatures,enablingmulti-agentcontrol
withasingletensormultiplication,significantlyreducingcomputationaloverhead.
Theaforementionedmethodsprimarilyinnovateatthescoringmodellevel,oftenusingweighted
decodingforknowledgeinjection.However,otherapproachesemploydiversedecodingtechniques
tocontroltextgeneration.BEAMR(BeamReweighing)[70]adjustsbeamsearchcandidatesby
reweightingthembasedonscoresfromanattributeclassifier,whichareusedtomodifygeneration
probabilities.NEUROLOGIC[95]andNEUROLOGICAFesque[94]useheuristicsearchtoguide
textgenerationundercomplexlexicalconstraints.CD(ControlledDecoding)[102]controlstext
generationwithaprefixscoringmethod.Ittrainstheprefixscorerofflineusingpolicyoptimization,
guidinggenerationduringinferencebasedontheexpectedrewardofpartiallydecodedsequences.
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 29
DATG(DynamicATtributeGraphs-basedCTG)[81]employsdynamicattributegraphstoadjust
theoccurrenceofattribute-relatedkeywords,therebyachievingcontrolovertextgeneration.
Severalmethodshavebeenoptimizedtoaddressspecificchallengesindecoding-stagecontrol.
Forexample,CAT-PAW[35]introducesalightweightregulatorthatdynamicallyadjustscontrol
signalsatdifferentdecodingpositions,mitigatingissuesofincoherenceandrepetitionwhencontrol
strengthincreases.Gemini[86]usesfeatureextractionandattribute-drivenkernelsamplingto
addressinconsistenciesbetweentrainingandinferencefeatures,ensuringthequalityofgenerated
text.NADO(NeurAlly-DecomposedOracle)[100]focusesoncomplexconstraintsbydecomposing
sequence-levelconstraintsintotoken-levelguidance,enablingfine-grainedcontrol.DECIDER[151]
enhanceslogicalityandscientificaccuracybycombininglanguagemodelprobabilitydistributions
withlogicalreasoningvectorsusingfirst-orderlogicrules.ILC(InvariantLearningCharacterization)
[177]leveragesinvariantlearningtoimprovethegeneralizationofattributepredictionsacross
differentdistributions,ensuringconsistencyinmulti-domaingeneration.
6.3.2 Class-ConditionedLanguageModelGuidance. Class-ConditionedLanguageModels
(CC-LMs)usepre-trainedorfine-tunedmodelsduringdecodingtocontroltheattributesofgenerated
text.CC-LMsaretrainedwithspecificlabelsorclassinformation,enablingthemtogeneratetext
that reflects predefined attributes, such as sentiment or theme. However, directly using these
modelsoftenyieldssuboptimalresults.Toenhancecontrol,thelogitsfromCC-LMs,whichcontain
attributeinformation,areusedasguidanceduringthedecodingprocess,improvingthecontrolled
generationofLLMs.
GeDi(GenerativeDiscriminator)[65]isamethodthatusesclass-conditionedlanguagemodels
fortextgenerationcontrol.Itfine-tunesaCC-LMusingcontrolcodes,allowingittodistinguish
andgeneratetextwithdesiredattributes.
GeDiappliesBayes’ruleduringdecodingbycombiningtheoutputsofabaselanguagemodel
(LM)andaCC-LMtocalculatetheprobabilityofgeneratingthenexttoken:
𝑃(𝑥 𝑡|𝑥 <𝑡,𝑐) ∝𝑃 LM(𝑥 𝑡|𝑥 <𝑡)𝑃 𝜃(𝑐|𝑥 𝑡,𝑥 <𝑡)𝜔, (18)
where𝑃 LM(𝑥 𝑡|𝑥 <𝑡)isthegenerationprobabilityfromthebaseLM,and𝑃 𝜃(𝑐|𝑥 𝑡,𝑥 <𝑡)istheclassifica-
tionprobabilitythatthetext,aftergenerating𝑥 𝑡,belongstothecontrolcondition𝑐.Theparameter
𝜔 adjuststhebiastowardsthetargetattribute.
GeDienhancescontrolprecisionbycalculatingandnormalizingtheprobabilitiesofthenext
tokenunderdesiredandundesiredattributes:
𝑃 𝜃(𝑐|𝑥 1:𝑡) =
(cid:205)
𝑐′∈{𝑃 𝑐,𝑐( ¯𝑐 }) 𝑃(cid:206) (𝑐𝑡 ′𝑗= )1 (cid:206)𝑃 𝜃
𝑡
𝑗=( 1𝑥 𝑃𝑗| 𝜃𝑥 (< 𝑥𝑗 𝑗, |𝑐 𝑥) <𝑗,𝑐′). (19)
ThisguidesthebaseLM’soutputtoalignbetterwiththetargetattribute.
DExperts(Decoding-timeExperts)[85]offersamorestraightforwardcontrastivedecodingap-
proachbymodifyingapre-trainedLM’spredictionsusingexpertandanti-expertmodels.DExperts
operatesonapre-trainedLM𝑀,withanexpertmodel𝑀′ andananti-expertmodel𝑀′′,which
modeltextwithandwithoutthetargetattribute,respectively.Attimestep𝑡,thesemodelsproduce
logits𝑧 𝑡,𝑧 𝑡′,and𝑧 𝑡′′:
𝑃˜(𝑥 𝑡|𝑥 <𝑡) =softmax(𝑧 𝑡 +𝛼(𝑧 𝑡′ −𝑧 𝑡′′)), (20)
where𝛼 controlsthestrengthofthemodification.DExpertsadjuststhelogitsfromthebaseLM
usingtheexpertmodeltoalignwiththetargetattribute,whiletheanti-expertmodelattenuates
unwantedattributes.Figure11illustratesthedifferencesbetweenGeDi,DExperts,andtheself-
feedbackguidancemethodPREADD(Prefix-AdaptiveDecoding)[107].
,Vol.1,No.1,Article.Publicationdate:August2024.30 Liangetal.
GeDi Vs. Dexperts Vs. PreAdd
<negative> The party was The party was
The party was
<negative> The party was
<positive> The party was
M Au tl tt ei n-H tioe nad M Au tl tt ei n-H tioe nad Multi-Head
Multi-Head Attention Attention
L × C Lla as ns g- uC ao FA gen d eed dd & F oM i N rwt o ai rm roo ddn ee ld L CP ×o CAsdd -i & Lt iNv Morme L N C×e CAgdd -a & Lt Ni Movrme L × LA Ldd M & Norm
P(xi| Ac d, dx &1 N:io-r1m) P(x Fi| oFx re we1ad r: di-1) P(x Fi| oFx re we1ad r: di-1) P(xFe i|exd F 1o :r i-w 1a)rd
Linear & Softmax
Add & Norm Add & Norm Add & Norm
amazing awful …cinematic okay amazing awful … cinematic okay
0.01 0.09 … 0.2 0.05 0.09 0.01 … 0.2 0.05
Linear & Softmax Linear & Softmax Linear & Softmax
pos
pos
𝑷𝑷neg 𝒙𝒙𝟏𝟏:𝒕𝒕 pos mi-nus
𝑷𝑷 amazin𝒙𝒙 g𝟏𝟏:𝒕𝒕 a= wf𝑷𝑷ul𝒙𝒙𝟏𝟏…:𝒕𝒕 cinem+a𝑷𝑷tic𝒙𝒙𝟏𝟏:𝒕𝒕okay mi -nus
0.9 0.1 … 0.2 0.05
LLM Final logits:
LLM
Fig.11. GeDivsDExpertsvsPREADD
MARCO(MaskandReplacewithContext)[41]focusesoncorrectingtextratherthangenerating
it.MARCOtrainsexpertandanti-expertmodelstodetectandreplacetoxiccomponentsduringtext
generation.Arithmetic[24]usesmodelarithmetictechniquesforpreciseattributecontrolintext
generation.Itcombinesmultiplemodelsandattributes,includingclassifiersandclass-conditioned
language models, through weighted linear combinations and joint operators to optimize and
integratedifferentinputdistributions.
Air-Decoding [181] addresses "attribute collapse," where strong attribute control can impair
fluency.Air-Decodingreconstructstheattributedistributionduringgeneration,adjustingtoken
weightsusingattributedistributionsfromprefixtuning.Thisbalancesattribute-specificandnon-
attributewords,ensuringthetextmeetsattributerequirementswhilemaintainingfluency.
6.3.3 Self-FeedbackGuidance. Self-FeedbackGuidanceleveragestheinternalknowledgeof
pre-trainedlanguagemodelstocontrolandguidetextgeneration[80].Thepremiseisthatwhile
themodelhastheknowledgetosolveatask,itmayfailtoachieveCTGduetoinadequateprompts
oroutputlimitations.Thesemethodsadjustthegeneratedtextduringdecodingbytappingintothe
model’sinherentknowledge,ensuringalignmentwithdesiredattributes.
InversePrompting[186]enhancesconsistencyintextgenerationbyusingthegeneratedtextto
inverselypredictthepromptduringthegenerationprocess.Itcalculatestheconditionalprobability
oftheoriginalpromptundertheinverseprompttoensurehighconsistencybetweenthegenerated
textandtheinitialprompt.
Forexample,atraditionalmodelmightgenerateananswerintheformat“Question:$Question
Description:$DescriptionAnswer:$Answer.”InInversePrompting,thegeneratedanswerisused
asaprompttoinverselypredictthequestion,forminganinversepromptlike"$Answeranswered
thequestion$Question."Theprocessinvolves:
• Thebaselanguagemodelfirstgeneratesananswer,e.g.,for"Whatisinverseprompting?",it
mightgenerate"Inversepromptingisamethodofusinggeneratedtexttopredictprompts."
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 31
• The answer is then recombined with the question to form the inverse prompt: "Inverse
promptingisamethodofusinggeneratedtexttopredictprompts.Itansweredthequestion
’Whatisinverseprompting?’"
• Theconditionalprobabilityoftheoriginalpromptundertheinverseprompt,𝑃(𝑐 𝑝′|𝑐 𝑔′),where
𝑐 𝑝′ istheoriginalpromptand𝑐 𝑔′ istheinverseprompt,iscalculatedtoadjustthescoresof
generatedcandidates.
Beamsearchtechniquesareusedduringdecodingtosynthesizecandidatescores,allowingthe
selectionofthegeneratedtextthatbestmatchestheinitialprompt,therebyenhancingconsistency
betweenthegeneratedcontentandcontrolattributes.
SD (Self-Diagnosis and Self-Debiasing) [120] leverages the model’s ability to self-diagnose
andself-debiastoidentifyandreducebiasesingeneratedtext.Duringdecoding,SDadjustsword
probabilitydistributionstominimizebiasedcontent.Theself-diagnosisprocessinSDisconceptually
similartoInversePrompting,anditsself-debiasingapproachwasoneoftheearliestapplicationsof
contrastivedecodingfordetoxificationcontrol.
ContrastiveDecodingApproachesplayasignificantroleinself-feedbackguidancebycomparing
the logits generated for different prompts during decoding, enabling flexible control over text
generation attributes. These methods often involve designing prompts that induce the model
to generate text with attributes opposite to those desired, using this comparison to guide the
generationoftextthatalignswiththeintendedattributes.
PREADD(Prefix-AdaptiveDecoding)[107]controlstextgenerationattributesbycomparingand
adjustingthelogitsgeneratedbydifferentprompts.DuringthegenerationprocessofmodelG,
PREADDpre-addsaprefix𝑟 1:𝑘 andadjuststheoutputbycomparingthelogitdifference𝑑 between
theprefixedandnon-prefixedoutputs:
𝑑 :=log𝑃(𝑥 𝑖+1|𝑟 1:𝑘,𝑥 1:𝑖)−log𝑃(𝑥 𝑖+1|𝑥 1:𝑖) (21)
Thisdifference𝑑 isappliedwithamultiplier𝛼 tocontroltheoutputintensity,allowingthemodel
toadjustattributecontrolflexibly.Thefinalprobabilitymodelis:
𝑃(𝑥 𝑖+1|𝑟 1:𝑘,𝑥 1:𝑖)𝛼𝑃(𝑥 𝑖+1|𝑥 1:𝑖)1−𝛼 (22)
For example, in detoxification tasks, PREADD uses a static prefix 𝑒 1:𝑚 that encourages the
generationoftoxictext,suchas:"Thefollowingtextperpetuatesnegativestereotypes,isthreatening
orsexuallyexplicit,orcontainsprofanelanguage."Bycalculatingthelogitdifferencesbetween
theprefixedandnon-prefixedpromptsateachgenerationstep,PREADDeffectivelyadjuststhe
attributesofthegeneratedtext.
COGNACGEN[15]andROSE(ReversePromptContrastiveDecoding)[180]sharesimilarideas
with SD and PREADD. COGNACGEN adjusts token generation by generating guiding words
thatalignwithcomplexconstraints,integratingthisguidancethroughprefixadjustment.ROSE
usesreversepromptstoinduceharmfulresponses,applyingthemduringinferencetosuppress
undesirablecontent,enhancingoutputsafety.
Asdiscussedearlier,spuriouscorrelationsoccurwhenmodelsmistakenlyidentifyunrelated
featuresasimportant,leadingtobiasedattributeselectionintextgeneration.Thisissuealsoaffects
CTGduringdecoding.SCM(StructuralCausalModel)[47]reducesbiasbyincorporatingcausal
reasoningintotextgeneration,allowingforattributemodificationwhilepreservingotherfeatures
throughcounterfactualinference.FPT(FocusedPrefixTuning)[97]addressesinterferencefrom
implicitattributesbyusingspecificandgeneralprefixes,trainingthemseparatelyandcombining
theirlogitstoenhancecontroloverexplicitattributes.
,Vol.1,No.1,Article.Publicationdate:August2024.32 Liangetal.
6.3.4 Energy-BasedModelGuidance. Energy-BasedModel(EBM)Guidancemethodscontrol
theattributesofgeneratedtextbyoptimizinganenergyfunctionduringthegenerationprocess.
Thesemethodsassignlowerenergyvalueswhenspecificconstraintsaremet,guidingthetextto
alignwithdesiredattributes.EBMsareoftenusedtobalancemultipleattributes,searchingfor
decodingstrategiesthatsatisfytheseconstraintswithintheenergyspace.
EBM-guidedgenerationreliesonthesamplingmethod.Whensamplingfromthejointdistribution
of multiple control attributes, the key is to select an optimal sampling method that efficiently
identifiesthebesttokenwithintheenergymodelspace.Somemethodsusegradientinformation
fromtheenergymodeltoachievetextconstraintcontrolbysamplinginthesolutionspace.
MUCOCO(Multi-ConstraintControlledOptimization)[66]wasoneoftheearliestenergy-based
CTGmethods,treatingdecodingasacontinuousoptimizationproblemwithmultipledifferentiable
constraints. It combines gradient descent and Lagrange multipliers for multi-attribute control.
MUCOLA(MultipleConstraintsusingLangevinDynamics)[67]improvesuponMUCOCObyinte-
gratingthelanguagemodel’slog-likelihoodwithuser-definedconstraintsintoanenergyfunction,
usingLangevindynamicsfornon-autoregressivesampling.COLD(ConstrainedDecodingwith
LangevinDynamics)[111]alsoemploysLangevindynamics,iterativelyupdatingtogeneratetext
thatmeetsspecificconstraints.COLD-Attack[39]extendsCOLDbygeneratingadversarialprompts
through energy-constrained decoding. To improve sampling efficiency, BOLT (Bias-Optimized
LogitTuning)[88]addsadjustablebiasestopredictedlogitsateachdecodingstep,optimizingthem
viagradientdescenttominimizeoverallenergyandensurecompliancewithspecifiedconstraints.
AnotherclassofEBMsamplingmethodsusesacceptance-rejectionmechanisms,suchasMetropolis-
HastingsandGibbssampling,tocontroltextattributeswithoutrelyingongradientinformation,
allowingtheuseofblack-boxscorers.
Mix&Match [101] combines scores from pre-trained black-box models (e.g., fluency, control
attributes,contextfidelity)intoaunifiedenergyfunctionandusesMetropolis-Hastingssamplingto
generatetextthatmeetsdesiredattributes.Duringgeneration,Mix&Matchincrementallyproposes
tokenreplacements,acceptingchangesthatlowertheenergy.BlockMH(BlockMetropolis-Hastings
Sampler)[33]enhancesefficiencyandoutputqualitybyintroducingablock-levelproposalsampler
thatiterativelyrewritesthesequence.ScoPE(Score-basedProgressiveEditor)[159]integratesthe
energymodelwiththeeditingprocess,progressivelyeditingintermediatetokenstoalignwith
targetattributesandguidingtheblack-boxmodeltogeneratethedesiredtext.
6.3.5 ExternalKnowledgeGuidance. ExternalKnowledgeGuidanceenhancestextgeneration
byintegratinginformationfromexternalknowledgebasesorretrievalmechanisms.Thesemethods
introduce relevant knowledge dynamically, improving coherence and alignment with desired
attributes.Theycanbecategorizedintotwotypes:semanticguidanceandknowledgeretrieval.
SemanticGuidancemethodsincorporateexternalsemanticinformationandcontext-relevant
informationtomodulatethemodel’soutput.
K2T(KeywordtoText)[106]ensurestheinclusionofspecifickeywordsbyadjustinglogproba-
bilitiesbasedoncosinesimilaritybetweenwordsandkeywordsateachgenerationstep.LM-Steer
[42]enablesflexibleandinterpretablecontroloverlanguagemodelgenerationstylesbyapplyinga
learnablelineartransformationtooutputwordembeddings.
Knowledgeretrievalmethodsenhancecoherence,accuracy,andcontrolbyretrievingrelevant
informationfromexternalsourcesduringgeneration.kNN-LM[60]isanearlyretrieval-augmented
method,buildingakey-valuestorefromtrainingdataandretrievingtheknearestneighborsusing
contextembeddings,interpolatingthisinformationintopredictions.kNN-SCG[136]andkNN-CTG
[103]extendkNN-LMbycombiningretrievaltechniqueswithCTG,enhancingcontrolthrough
relevantexampleretrieval.Anothernotablemethod,MEGATRON-CNTRL[152],enhancesstory
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 33
generationbydynamicallyintegratingkeywordsandretrievingtherelevantknowledge.GRACE
[149]combinesgenerativeandcontrastivelearningtoadjusttherelevanceanddiversityofretrieved
content.Goodtriever[108]integratestoxicandnon-toxicdatastores,combiningstoreoutputwith
modellogitsforadaptivetoxicitymitigation.
WhileDecoding-timeInterventionofferssignificantflexibilityandallowsforreal-timeadjust-
mentsduringthetextgenerationprocess,ittypicallyreliesonexternalmodelsorcomponents
toinjectthedesiredcontrolconditions.Thisdependencycanincreaseinferencetimeduetothe
additionalcomputationneededtoadjusttheoutput.Moreover,directlymanipulatingthemodel’s
outputprobabilitiesmaydisruptthenaturalfluencyandcoherenceofthegeneratedtext,asthese
adjustments might force the model to select less likely tokens that fit the control conditions,
potentiallyimpactingthetext’ssmoothness.
6.4 Summary
Inference-stagemethodsprovideprecisecontrolinCTGbydynamicallyadjustingthegeneration
process.ThesemethodsincludePromptEngineering,LatentSpaceManipulation,Decoding-time
Intervention,andvariousguidancetechniques,eachofferingdistinctadvantagesandchallenges.
PromptEngineeringmethodsexertcontroldirectlyattheinputlevelthroughhardprompts
[114,126,168]andsoftprompts[73,76,89],withoutrequiringadditionalmodeltraining,making
themsuitableforquicklyadjustinggenerationstrategies.Hardpromptsrelyonexplicitnatural
languageinstructions,whilesoftpromptsusetrainablevectorsformoregranularcontrol.Although
flexibleandresource-efficient,theeffectivenessofthisapproachdependsonthemodel’ssensitivity
toandaccuracyininterpretingtheprompts.
LatentSpaceManipulationinvolvesintroducingcontrolvectorsintothemodel’slatentspace
toadjustthecharacteristicsofthegeneratedtext[13,64,87,132,137].Bydirectlymanipulatingthe
model’sactivationstates,thismethodallowsforprecisecontrol,especiallyinmulti-attributetasks.
Decoding-timeInterventionusesdynamicadjustmentsduringthedecodingprocesstocontrol
the generated output, including classifier guidance [22, 127, 153], class-conditioned language
models[41,65,85],energy-basedmodels[66,67,101],modelself-feedback[120,180],andexternal
knowledge[103,108].Adjustingoutputprobabilitiesduringgenerationenablescomplexattribute
controlbutmayimpacttextnaturalnessandcoherence,andaddscomputationalcomplexitydueto
relianceonexternalmodels.
Overall,inference-stagemethodsprovideflexibleanddynamictextcontrolcapabilities,enabling
highlycustomizedtextgenerationwithoutalteringtheoriginalmodelstructure.However,they
oftenrelyonexternalresourcesandmodels,whichmayposechallengesintermsoffluencyand
consistency.Nevertheless,thesemethodsexcelinscenariosrequiringattributecontrol.
7 EVALUATION
EvaluationmetricsforCTGtaskscanbebroadlycategorizedintothreetypes:automaticevaluation,
humanevaluation,andLLM-basedevaluationmethods,asshowninTable6.
7.1 AutomaticEvaluation
Automaticevaluationusesspecificmetricsormodelsandcanbedividedintogeneralandtask-
specificevaluations.GeneralmetricsassessoveralltextqualityacrossvariousCTGtasks,while
task-specificevaluationsfocusonqualitybasedonspecificattributes.
7.1.1 GeneralMetrics. Dependingonhowtheyarecalculated,generalmetricscanbedivided
inton-gramoverlap-basedmetrics,languagemodel-basedmetrics,distance-basedmetrics,etc.
,Vol.1,No.1,Article.Publicationdate:August2024.34 Liangetal.
Table6. SummaryofEvaluationMethodsandMetrics
EvaluationType Aspect Description
N-gram Overlap-based: BLEU[105], ROUGE[83], METEOR[6],
NIST[28],Distinct-n[74],Repetition-n[123],Self-BLEU[185]
Language Model-based: Perplexity, BertScore[169],
GeneralMetrics
MoverScore[174],BLEURT[121]
Automatic
Evaluation Distance-based:TER[128]
Other:CIDEr[140],SPICE[2]
Task-specificMetrics ClassifiersorAPIforspecificattributes[81,181]
EvaluationMetrics Fluency,Coherence,Topicality,GeneralQuality,AttributeRelevance
Human
Evaluation EvaluationMethods A/Btest,N-pointLikert-likescale
LLM-based Approach UsingLLMforEvaluation[21,39,78,87,146,150,180]
N-gramOverlap-BasedMetrics:Thesemetricsconverttextintosetsofn-gramunitsandfocus
onthesimilarityofn-gramdistributions,typicallybycomparinggeneratedtexttoreferencetext.
BLEU[105]:BLEUisacommonevaluationmetricthatmeasuresthesimilaritybetweengenerated
textandreferencetext,focusingonprecision.Itcalculatestheproportionofn-gramunitsinthe
generatedtextthatappearinthereferencetext,withtheformulaasfollows:
(cid:205) 𝑐∈𝐶(cid:205) 𝑔∈𝑐Count clip(𝑔)
BLEU-n= (23)
(cid:205) 𝑐′∈𝐶(cid:205) 𝑔′∈𝑐′Count(𝑔′)
where𝐶 isthesetofcandidatetextsand𝑔isann-gram.Count (𝑔)isthen-gram’scountinthe
clip
referencetext,cappedbyitscountinthecandidate.Count(𝑔′) isthetotaln-gramcountinthe
candidate.Ahighervalueindicatesgreatersimilaritybetweenthegeneratedandreferencetexts.
ROUGE[83]:ROUGEisconceptuallysimilartoBLEUbutcalculatestheproportionofn-grams
inthereferencetextthatappearinthegeneratedtext,focusingonrecallratherthanprecision.
(cid:205) 𝑟∈𝑅(cid:205) 𝑔∈𝑟Count match(𝑔)
ROUGE-n= (24)
(cid:205)𝑔 ∈𝑟Count(𝑔)
where𝑅denotesthesetofreferencetexts,𝑟 representsareferencetext,and𝑔denotesann-gram.
Count (𝑔) representsthenumberofmatchingn-gramsinthegeneratedtext,andCount(𝑔)
match
representsthetotalcountofn-gramsinthereferencetext.Thehigherthisvalue,thegreaterthe
similaritybetweenthegeneratedandreferencetexts.
METEOR[6]: BLEU focuses on precision, and ROUGE on recall, but both have limitations.
METEORaddressesthisbycombiningthemintoan"F1score"withthefollowingformula:
10𝑃𝑅
𝐹 𝑚𝑒𝑎𝑛 = 𝑅+9𝑃 (25)
where𝑃 representsprecision,and𝑅representsrecall.
UnlikeBLEU,whichconsidersonlyexactn-grammatches,METEORincorporatesadditional
mechanismslikesynonymmatchingandstemming,usingresourceslikeWordNet.Forexample,
"journey"and"tour"wouldbematchedassynonyms,improvingevaluationaccuracy.
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 35
Additionally,METEORconsidersn-gramalignmentbetweengeneratedandreferencetexts.It
introducestheconceptof"chunks,"whicharecontinuoussequencesofmatchedn-grams.Apenalty
isappliedfordiscontinuitiesinthematchingsequences:
(cid:18) (cid:19)3
chunks
𝑃𝑒𝑛𝑎𝑙𝑡𝑦 =0.5 (26)
unigramsmatched
wherechunksrepresentsthenumberofdiscontinuousmatchedsequences,andunigramsmatched
representsthenumberofmatchedwords.Thispenaltyreducesthescoreforexcessivediscontinu-
ities.Thefinalscoreiscalculatedasfollows:
𝑆𝑐𝑜𝑟𝑒 =𝐹 𝑚𝑒𝑎𝑛(1−𝑃𝑒𝑛𝑎𝑙𝑡𝑦) (27)
where𝑆𝑐𝑜𝑟𝑒 representsthefinalMETEORscore.Morechunksresultinahigherpenaltyanda
lowerMETEORscore.Thismethodbetteraccountsforwordorderandcoherence,offeringamore
detailedandaccurateevaluationthann-gram-basedmetrics.
NIST[28]:NISTbuildsonBLEUbyintroducingtheconceptofinformationweight:
Info(𝑤 1...𝑤 𝑛) =log 2(cid:18) C Co ou un nt( t𝑤 (𝑤1 1.. .. .𝑤 .𝑤𝑛 𝑛− )1)(cid:19) (28)
whereCount(𝑤 1...𝑤 𝑛−1)representstheoccurrencecountofthefirst𝑛−1words,andCount(𝑤 1...𝑤 𝑛)
representstheoccurrencecountofthefulln-gram.Raren-gramsaregivenhigherweight.
NIST assigns varying weights to each n-gram, averaging them for a final score that better
evaluatessimilaritybyaccountingforraren-grams.
Distinct-n[74]:Distinct-nmeasuresthediversityofgeneratedtextbycalculatingtheratioof
uniquen-gramstototaln-grams:
Count(uniquen-gram)
Distinct-n= (29)
Count(n-gram)
whereCount(uniquen-gram)representsthenumberofuniquen-gramsinthegeneratedtext,and
Count(n-gram)representsthetotalnumberofn-grams.
Repetition-n[123]:Repetition-nindirectlyevaluatesthediversityofgeneratedtextbycalculat-
ingtheratioofn-gramsthatoccurmorethanoncetothetotalnumberofn-grams:
Count(repeatedn-gram)
Repetition-n= (30)
Count(n-gram)
whereCount(repeatedn-gram)representsthenumberofrepeatedn-gramsinthegeneratedtext,
andCount(n-gram)representsthetotalnumberofn-grams.Thisratioassessestherepetitionlevel
ofthegeneratedtext,reflectingitsdiversity.
Self-BLEU[185]:Self-BLEUmeasuresdiversitybycalculatingBLEUscoresbetweengenerated
texts,notagainstreferences.ItaveragesBLEUscoresacrossgeneratedtextsandlowerSelf-BLEU
scoresindicatehigherdiversityamongthegeneratedtexts.
LanguageModel-BasedMetrics:
Perplexity[53]: Perplexity measures the model’s ability to predict test data, indicating the
model’suncertaintyinitspredictions.InNLPtasks,perplexityrepresentsthemodel’saccuracyin
predictingwordsequencesinatestset.Itiscalculatedasfollows:
(cid:32) 𝑛 (cid:33) 𝑛1
(cid:214) 1
PPL= (31)
𝑖=1
𝑝(𝑤 𝑖|𝑤 1,𝑤 2,...,𝑤 𝑖−1)
,Vol.1,No.1,Article.Publicationdate:August2024.36 Liangetal.
Inpractice,aproxymodel(e.g.,GPT-2)isoftenusedtocalculatetheperplexityofthegenerated
text.LowerPPLindicateshigherfluencyofthegeneratedtext.
BertScore[169]:BertScoreisalanguagegenerationevaluationmetricbasedonpre-trainedBERT
contextualembeddingsItcomputesthesimilarityoftwosentencesasasumofcosinesimilarities
between their tokens’ embeddings. Unlike n-gram-based metrics, BertScore captures semantic
information,offeringamoreaccurateevaluation.
MoverScore[174]:MoverScorecombineswordembeddingswithEarthMover’sDistance(EMD).
UnlikeBertScore,whichconsiderseachword’sindependentsimilarity,MoverScoretreatstextasa
distributionofwordembeddingsandcalculatesthedistancebetweenthesedistributions,capturing
contextualinformationandwordrelationshipsformoreaccurateevaluation.
BLEURT[121]:BLEURTimprovesuponBertScorebytrainingBERTonsyntheticdatagenerated
byaddingrandomperturbationstoWikipediasentences.Thisallowsthemetrictobemorerobust
todomainandqualitydrift,providinghigherevaluationaccuracy.
Distance-BasedMetrics:
TER[128]: TER evaluates the quality of generated text by comparing it with reference text,
calculatingthenumberofeditoperations(insertion,deletion,substitution,andshiftofwords)
neededtotransformthegeneratedtextintothereferencetext.Theformulais:
NumberofEdits
TER= (32)
AverageNumberofReferenceWords
LowerTERindicateshighersimilarityandqualityofthegeneratedtext.
OtherMetrics:
CIDEr[140]: CIDEr evaluates the quality of generated text by comparing it with multiple
referencetexts,incorporatingTF-IDFweightingtoassigndifferentweightstodifferentn-grams.
This highlights important n-grams and reduces the influence of common ones, capturing key
contentandimportantinformationforamorenuancedevaluation.
SPICE[2]:SPICEisasemanticsimilaritymetricthatusesaprobabilisticcontext-freegrammar
(PCFG) dependency parser to parse generated and reference texts into syntactic dependency
trees.Thesearethenmappedtoscenegraphs,includingentities,attributes,andrelations,and
thesimilarityscoreiscalculatedbasedonthematchingbetweenthescenegraphs.Comparedto
n-gram-basedmetrics,SPICEbettercapturessemanticinformation.
7.1.2 Task-specificMetrics. Toevaluatewhetherthegeneratedtextmeetsthespecifiedattributes
inCTGtasks,aclassifierisoftenused.Thisclassifiercanbeobtainedbytrainingabasemodel(e.g.,
BERT)onaspecificdataset(e.g.,IMDB).Table7listscommonlyuseddatasetsandbasemodels.
Alternatively, existing models can be directly used, often sourced from HuggingFace, such as
DistilBERT-base-uncased-finetuned-SST-2foremotiontasks2,tweet-topic-21-multifortopictasks3,
andthePerspectiveAPIfortoxicitytasks4.
7.2 HumanEvaluation
While automated evaluation meets most evaluation requirements, considering the diversity of
CTGtasksandthelimitationsofautomatedevaluation,humanevaluationcanserveasavaluable
supplement,providingcustomizedassessmentandmoreaccurateresults.Thissectionintroduces
themetricsandmethodsusedinhumanevaluation.
2https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english
3https://huggingface.co/cardiffnlp/tweet-topic-21-multi
4PerspectiveAPI
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 37
Table7. CommonBaseModelsandDatasetsforTrainingClassifiers
Attribute BaseModel Dataset
Emotion BERT[26], RoBERTa[91], DeBERTa[43], IMDB[98], AMAZON-5[99], SST-5[129], SST-2[129],
distilBERT[118],MacBERT[20] Yelp[171],Twittersentiment[7],DailyDialog[77]
Topic BERT[26],RoBERTa[91] AG-NEWS[171],DBpedia[171]
Toxicity RoBERTa[91],DeBERTa[43] Jigsaw Toxic Comment Classification Challenge[19],
RealToxicityPrompts[34]
7.2.1 Metric. Commonhumanevaluationmetricsinclude:
Fluency:Fluencymeasureswhetherthegeneratedtextisgrammaticallycorrect,easytounder-
stand,andfreefromrepetition.
Coherence:Assesseswhetherthetextmaintainsalinguisticstyle,exhibitscausalandtemporal
dependencybetweensentences,andwhethertheinformationislogicallyorganized.
Topicality:Measuresconsistencywiththecontextofthegivenprompt.
Generalquality:Unlikethemoreholisticmetricsmentionedabove,thisclassofmetricsis
morespecific,evaluatingparticularaspectsofthegeneratedtext,suchascommonsense,logical
consistency,diversityofexpression,lexicalrichness,andgrammaticalcorrectness.
Attributerelevance:Similartothemetricsinautomatedevaluation,thismetricjudgeswhether
thegeneratedtextmeetsthegivenattribute(e.g.,emotion,topic,lexicalfeatures).
7.2.2 Method. CommonhumanevaluationmethodsincludeA/BtestingandLikertscales.
A/Btest:A/Btestingisacomparison-basedevaluationmethodwherehumanannotatorsare
askedtoselectthetextthatbettermeetstherequirementsfromtwo(ormore)generatedtexts
basedonagivenquestion(e.g.,whichsentenceismorelogical?).
N-pointLikert-likescale:TheN-pointLikert-likescaleisaquantitativeevaluationmethod
wherehumanannotatorsratethegeneratedtextaccordingtopredefinedscoringstandards(usually
discrete),suchas0representinglowqualityand3representinghighquality.
7.3 LLM-basedEvaluation
WiththeadventofpowerfullanguagemodelslikeChatGPT,LLM-basedevaluationmethodsare
becomingincreasinglypopular[21,39,78,87,146,150,180].Theseevaluationmethodsonlyrequire
theconstructionofspecificprompts,allowingthemodeltoevaluatethegeneratedtext.Comparedto
traditionalautomatedevaluationmethods,LLM-basedmethodsaremorediverse,meetingspecific
evaluationneedsandreturningricherevaluationresults.Comparedtohumanevaluationmethods,
LLM-basedmethodsaremorepractical,significantlyreducingevaluationcosts(e.g.,labor,time,
money)whilealsoreducingtheimpactofhumanannotators’subjectivebiasestosomeextent.
7.4 Benchmarks
SeveralbenchmarkshavebeenproposedintheresearchofCTGevaluationtoassesstheperformance
ofgenerationmodelsunderdifferenttasksandconditions.
• CTRLEval[57]introducesanunsupervised,reference-freemetrictoevaluatecontrolledtext
generationquality,usingtextinfillingwithapre-trainedmodel(e.g.,PEGASUS)toassess
coherence,consistency,andattributerelevance.
• ConGenBench[4]benchmarkscontrollablegenerationmethodsbygeneratingconstrained
datasetswithinstruction-tunedLLMs,showcasingtheirpotential,particularlyinstyletasks.
,Vol.1,No.1,Article.Publicationdate:August2024.38 Liangetal.
• CoDI-Eval[17]integratesdiverseinstructionsbyexpandinghuman-writtenseeds,introduc-
ingnewtasksandstandardsfortestingLLMs’controllablegenerationincomplexsettings.
• FOFO[150]isabenchmarkdevelopedthroughAI-humancollaboration,coveringavariety
ofreal-worldformatsandinstructionstoevaluateLLMs’formatadherencecapabilities.
8 APPLICATIONS
CTGtechnologyhasdevelopeddiversecontrolgenerationmethodstomeetvariousgeneration
needsacrossdifferentfields.Thesemethodscanbecategorizedintoverticaldomainapplications
andgeneraltaskapplications.Verticaldomainapplicationsaretailoredtospecifictaskswithin
particular industries, focusing on specialization and precision, while general task applications
addresscross-domainneeds,offeringhighversatility.Thefollowingsectionsprovideanoverview
andanalysisofCTGtechnologyindifferentapplicationscenarios.
8.1 VerticalDomainApplications
CTGhasshownstrongadaptabilityinspecializedfields,effectivelyaddressinguniquegeneration
needsindomainssuchasnewsreporting,scientificliterature,andeducationalcontentcreation.By
employingspecializedmodelsandmethods,CTGenhancesthequalityandrelevanceofgenerated
text,makingitmoretargetedandprofessional.
Innewsgeneration,DeepPress[113]integratespre-trainedmodelstoproducetopic-awarenews
content,enhancingobjectivityandcoherence,whileSeqCTG[130]ensureslogicalconsistencyin
articlesusinglocalcontrolcodes.Forscientifictexts,MReD[124]utilizesstructureddatasetsto
improvethedomainspecificityofgeneratedcontent.
In education, CE (Complexity Embedding)[52] leverages complexity embeddings to control
lexicalcomplexity,enablingthecreationofcustomizedlearningmaterialsforlanguagelearners.
Formultilingualgeneration,SweCTRL-Mini[55]appliescontrolcodesinSwedishtextgeneration,
whileCollocation2Text[142]guidesRussiantextgenerationthroughspecifiedphrases.
CTG also enhances internet text generation. PCTG-X[156] uses text prompts and attribute
labelstocontrolthestanceandstyleofsocialmediacontent,whileCounterGeDi[117]suppresses
unwantedattributestocounterhatespeech.InChinesecontent,CAT-LLM[134]facilitatesstyle
transformationusingLLMsandtextstylemodules.
Innicheapplicationslikerecipegeneration,RecipeWithPlans[92]combinescontentplanning
withsequencegenerationtoproducecoherentandlogicallystructuredrecipes.
8.2 GeneralTaskApplications
Generaltaskapplicationsaddresscross-domainchallengesliketoxicityremoval,dialoguegenera-
tion,andstorycreation,makingthesemethodsapplicableacrossvariousscenarios.
Intoxicitycontrol,SRDT[72]manipulatesattentionlayerstoreducetoxiccontent,whileDESTEIN[78]
andInferAligner[146]adjustactivationstatestolowerthelikelihoodofgeneratingharmfulcontent.
Additionally, UncertaintyAttack[161] exploits changes in the probability distribution of model
outputlogitstocarryoutsecurityattacks,highlightingthethreatthatimproperapplicationof
CTGposestothereliabilityofLLMs.
Fordialoguegeneration,Personalized-Dialogue[179]enhancespersonalizationbyincorporating
user data, and MultiT-C-Dialog[164] employs multi-task learning to improve dialogue quality.
ECCRG[16]enhancesemotionalexpressionandcoherencethroughemotionandcontentcontrol.
Instorygeneration,Plug-and-Blend[84]offersfinecontrolovermultiplethemes,whileCHAE[147]
allowsdetailedcustomizationofcharactersandemotions.SCSC[18]ensuresconsistencyanddiver-
sityinstorytelling,andPMCSG[141]generatesnarrativesthatmeetkeyplotpointsbyselecting
pathswithminimalperplexity.
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 39
Inkeyword-controlledgeneration,KeywordPosition[119]enhancesalignmentwithuserintent
bycontrollingkeywordplacement,makingitsuitablefortaskslikeautomatedsummarygeneration.
9 CHALLENGESANDAPPEALS
9.1 Challenges
9.1.1 ReducedFluencyandPracticality. DespitetheremarkableprogressinLLMslikeGPT-3
andBERT,challengesremaininachievingfluencyandpracticalityingeneratedtext.Issuessuchas
incoherence,semanticambiguity,orredundancyoftenarise,particularlyincomplextasksorwhen
preciseresponsesarerequired.Theseshortcomingscansignificantlydiminishthepracticalvalue
ofthegeneratedcontent[81,181].Therefore,enhancingthefluencyandpracticalapplicationof
generatedtextremainsacriticalchallenge.
9.1.2 ComplexityofMulti-AttributeControl. Controllingmultipleattributessimultaneously,
suchasemotion,style,andtopic,posesasignificantchallengeduetothecomplexinterdependencies
andconstraintsamongtheseattributes.Whilecurrentresearchmainlyfocusesonsingle-attribute
control,multi-attributecontrolisstillinitsearlystages[36].Theabilitytopreciselycontrolmultiple
attributeswhilemaintainingthequalityofgeneratedtextisanunresolvedissuethatwouldgreatly
enhancethecustomizationandutilityofAI-generatedcontent.
9.1.3 IncompleteAttributeDecoupling. Attributedecoupling,theabilitytocontroloneat-
tributewithoutaffectingothers,remainsanongoingchallengeduetothepresenceofspurious
correlations.Currentmethodsstruggletoachievecompleteattributedecouplinginpractice[47].
Forexample,alteringthesentimentofatextmightinadvertentlyshiftitsfocustoaparticular
topic,suchaspolitics.Achievingcompletedecouplingtoensuretheindependenceandstabilityof
multi-attributecontrolisakeyresearchdirection.
9.1.4 DecodingTimeOptimization. Decodingtime,orthetimerequiredforamodeltogenerate
text,isacrucialperformanceindicatorforthepracticalapplicationofAI-generatedcontent.The
largeparametersizesofcurrentLLMsoftenresultinatime-consuminggenerationprocess,affecting
their feasibility in real-time applications. This issue is particularly relevant when generating
longtextsorrequiringmultipleiterations.Thus,significantlyreducingdecodingtimewithout
compromising text quality is a major challenge that necessitates in-depth research into model
architectureoptimizationandimprovementsindecodingalgorithms.
9.1.5 LackofPrecisioninContentControl. Achievingprecisecontentcontrol,orhardcontrol,
inCTGremainschallenging.Whileexistingmodelscangeneratetextthatmeetsexpectationsto
someextent,theyoftenfallshortinaccuracy.Forinstance,intasksrequiringstrictlexicalcontrol,
modelperformanceisoftenunsatisfactory[4].
9.2 Appeals
9.2.1 ResearchShouldBeMoreOrientedTowardsReal-WorldApplications. Manydecoding-
phasemethodsfacelimitationsinpracticality,particularlyinbalancingtimeefficiencywitheffec-
tiveness.Futureresearchshouldprioritizepracticalapplicationneeds,aimingtostrikeanoptimal
balancebetweenthesefactors.Forexample,asnotedby[4],promptsremaineffectiveinmany
cases,suggestingthatprompt-basedmethodsshouldnotbeoverlooked.Whileinnovativemethods
involvinglatentspacemanipulationanddecoding-phaseinterventionsarepromising,theultimate
criterionshouldbetheireffectiveness.Researchersshouldselectthemostsuitablemethodbased
onspecificapplicationscenariostoachievethebestgenerationoutcomes.
,Vol.1,No.1,Article.Publicationdate:August2024.40 Liangetal.
9.2.2 Expanding the Diversity of Testing Tasks. Current testing tasks primarily focus on
aspectssuchastoxicity,emotion,topics,andlexicon,withrelativelylimitedevaluationsofstyleand
form.Futureresearchshouldbroadenthediversityoftestingtaskstoincludeaspectslikelinguistic
style,narrativestructure,andpragmaticfunctions.Introducingthesevariedtestingtaskswould
allowforamorecomprehensiveevaluationoftheperformanceandpracticalityofCTGmodels.
9.2.3 MaximizingLLMCapabilitiesWhenComparingBaselines. Whenconductingexper-
imentaltesting,researchersshouldnotlimitthemselvestotraditionalCTGmethods.Withthe
advancementofLLMtechnology,itisessentialtoactivelyincorporatevariousexistingprompt-
basedtechniquestofullyleveragetheirCTGcapabilities.Thisapproachwillhelpinthoroughly
evaluating the effectiveness of different methods, ensuring that the chosen baselines are more
representativeandpractical,therebyidentifyingtheoptimalsolution.
10 CONCLUSION
ThispaperreviewsthelatestresearchadvancesinthefieldofControllableTextGeneration(CTG)
forLargeLanguageModels(LLMs)andsystematicallydefinesthebasicconcepts,addressingboth
controlconditionsandtextqualityrequirements.Thepaperintroducesanewtaskclassification
approach, categorizing CTG tasks into content control (or linguistic control/hard control) and
attributecontrol(orsemanticcontrol/softcontrol).
ThepaperprovidesadetailedreviewofvariousCTGmethods.Duringthetrainingphase,key
methodsincluderetrainingorfine-tuningpre-trainedmodelsandemployingreinforcementlearning
strategiestooptimizegenerationqualityandcontrolprecision.Intheinferencephase,commonly
usedtechniquesinvolveguidinggenerationthroughpromptengineering,manipulatingthelatent
spaceforprecisecontrol,andinterveningduringdecodingtoadjusttheoutputtext.
ThepaperalsoexploresvariousevaluationmethodsforCTGandhighlightsthewideapplication
ofCTGtechnologyacrossmultipleverticaldomainsandgeneraltasks.Thechallengesfacedby
theCTGfield,includingimprovingquality,optimizingcontrolprecision,andenhancinginference
efficiency,arediscussed,alongwithfutureresearchdirectionsandappeals.
Inconclusion,thispaperprovidesacomprehensivereviewofthecoreconcepts,technicalmeth-
ods,evaluationapproaches,andpracticalapplicationsinthefieldofcontrollabletextgeneration,
identifyingcurrentresearchchallengesandproposingfuturedevelopmentdirections.Itaimsto
serveasasystematicreferenceandguideforresearchexplorationincontrollabletextgeneration.
ACKNOWLEDGMENTS
ThisworkwassupportedbytheNationalNaturalScienceFoundationofChina(GrantsNo.62072463
and71531012),theNationalSocialScienceFoundationofChina(GrantNo.18ZDA309),theResearch
SeedFundsoftheSchoolofInterdisciplinaryStudiesatRenminUniversityofChina,andtheOpening
ProjectoftheStateKeyLaboratoryofDigitalPublishingTechnologyatFounderGroup.
REFERENCES
[1] RohanDeepakAjwani,ZiningZhu,JonathanRose,andFrankRudzicz.2024.PlugandPlaywithPrompts:APrompt
TuningApproachforControllingTextGeneration. arXiv:2404.05143[cs.CL] https://arxiv.org/abs/2404.05143
[2] PeterAnderson,BasuraFernando,MarkJohnson,andStephenGould.2016.SPICE:SemanticPropositionalImage
CaptionEvaluation.InComputerVision–ECCV2016,BastianLeibe,JiriMatas,NicuSebe,andMaxWelling(Eds.).
SpringerInternationalPublishing,Cham,382–398.
[3] KushalArora,KurtShuster,SainbayarSukhbaatar,andJasonWeston.2022. Director:Generator-ClassifiersFor
SupervisedLanguageModeling.InProceedingsofthe2ndConferenceoftheAsia-PacificChapteroftheAssociation
forComputationalLinguisticsandthe12thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:
LongPapers),YulanHe,HengJi,SujianLi,YangLiu,andChua-HuiChang(Eds.).AssociationforComputational
Linguistics,Onlineonly,512–526. https://aclanthology.org/2022.aacl-main.39
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 41
[4] Dhananjay Ashok and Barnabas Poczos. 2024. Controllable Text Generation in the Instruction-Tuning Era.
arXiv:2405.01490[cs.CL] https://arxiv.org/abs/2405.01490
[5] YuntaoBai,SauravKadavath,SandipanKundu,AmandaAskell,JacksonKernion,AndyJones,AnnaChen,AnnaGoldie,
AzaliaMirhoseini,CameronMcKinnon,CarolChen,CatherineOlsson,ChristopherOlah,DannyHernandez,Dawn
Drain,DeepGanguli,DustinLi,EliTran-Johnson,EthanPerez,JamieKerr,JaredMueller,JeffreyLadish,JoshuaLandau,
KamalNdousse,KamileLukosuite,LianeLovitt,MichaelSellitto,NelsonElhage,NicholasSchiefer,NoemiMercado,
NovaDasSarma,RobertLasenby,RobinLarson,SamRinger,ScottJohnston,ShaunaKravec,SheerElShowk,Stanislav
Fort,TameraLanham,TimothyTelleen-Lawton,TomConerly,TomHenighan,TristanHume,SamuelR.Bowman,
ZacHatfield-Dodds,BenMann,DarioAmodei,NicholasJoseph,SamMcCandlish,TomBrown,andJaredKaplan.
2022.ConstitutionalAI:HarmlessnessfromAIFeedback. arXiv:2212.08073[cs.CL] https://arxiv.org/abs/2212.08073
[6] SatanjeevBanerjeeandAlonLavie.2005. METEOR:AnAutomaticMetricforMTEvaluationwithImproved
CorrelationwithHumanJudgments.InProceedingsoftheACLWorkshoponIntrinsicandExtrinsicEvaluationMeasures
forMachineTranslationand/orSummarization,JadeGoldstein,AlonLavie,Chin-YewLin,andClareVoss(Eds.).
AssociationforComputationalLinguistics,AnnArbor,Michigan,65–72. https://aclanthology.org/W05-0909
[7] FrancescoBarbieri,JoseCamacho-Collados,LuisEspinosaAnke,andLeonardoNeves.2020. TweetEval:Unified
BenchmarkandComparativeEvaluationforTweetClassification.InFindingsoftheAssociationforComputational
Linguistics:EMNLP2020,TrevorCohn,YulanHe,andYangLiu(Eds.).AssociationforComputationalLinguistics,
Online,1644–1650. https://doi.org/10.18653/v1/2020.findings-emnlp.148
[8] EmilyMBender,TimnitGebru,AngelinaMcMillan-Major,andShmargaretShmitchell.2021. Onthedangers
ofstochasticparrots:Canlanguagemodelsbetoobig?.InProceedingsofthe2021ACMconferenceonfairness,
accountability,andtransparency.610–623.
[9] TomB.Brown,BenjaminMann,NickRyder,MelanieSubbiah,JaredKaplan,PrafullaDhariwal,ArvindNeelakantan,
PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,
RewonChild,AdityaRamesh,DanielM.Ziegler,JeffreyWu,ClemensWinter,ChristopherHesse,MarkChen,Eric
Sigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRadford,
IlyaSutskever,andDarioAmodei.2020.Languagemodelsarefew-shotlearners.InProceedingsofthe34thInternational
ConferenceonNeuralInformationProcessingSystems(Vancouver,BC,Canada)(NIPS’20).CurranAssociatesInc.,Red
Hook,NY,USA,Article159,25pages.
[10] MengCao,MehdiFatemi,JackieChiKitCheung,andSamiraShabanian.2023. SuccessorFeaturesforEfficient
MultisubjectControlledTextGeneration. arXiv:2311.04921[cs.CL] https://arxiv.org/abs/2311.04921
[11] FredrikCarlsson,JoeyÖhman,FangyuLiu,SeverineVerlinden,JoakimNivre,andMagnusSahlgren.2022. Fine-
GrainedControllableTextGenerationUsingNon-ResidualPrompting.InProceedingsofthe60thAnnualMeetingof
theAssociationforComputationalLinguistics(Volume1:LongPapers),SmarandaMuresan,PreslavNakov,andAline
Villavicencio(Eds.).AssociationforComputationalLinguistics,Dublin,Ireland,6837–6857. https://doi.org/10.18653/
v1/2022.acl-long.471
[12] JunyiChai,ReidPryzant,VictorYeDong,KonstantinGolobokov,ChenguangZhu,andYiLiu.2022.FAST:Improving
ControllabilityforTextGenerationwithFeedbackAwareSelf-Training. arXiv:2210.03167[cs.CL] https://arxiv.org/
abs/2210.03167
[13] AlvinChan,AliMadani,BenKrause,andNikhilNaik.2021.DeepExtrapolationforAttribute-EnhancedGeneration.
InAdvancesinNeuralInformationProcessingSystems,A.Beygelzimer,Y.Dauphin,P.Liang,andJ.WortmanVaughan
(Eds.). https://openreview.net/forum?id=NCDMYD2y5kK
[14] AlvinChan,Yew-SoonOng,BillPung,AstonZhang,andJieFu.2021. CoCon:ASelf-SupervisedApproachfor
ControlledTextGeneration.InInternationalConferenceonLearningRepresentations. https://openreview.net/forum?
id=VD_ozqvBy4W
[15] HowardChen,HuihanLi,DanqiChen,andKarthikNarasimhan.2022.ControllableTextGenerationwithLanguage
Constraints. arXiv:2212.10466[cs.CL] https://arxiv.org/abs/2212.10466
[16] HuiChen,BoWang,KeYang,andYiSong.2024.ECCRG:AEmotion-andContent-ControllableResponseGeneration
Model.InCollaborativeComputing:Networking,ApplicationsandWorksharing,HonghaoGao,XinhengWang,and
NikolaosVoros(Eds.).SpringerNatureSwitzerland,Cham,115–130.
[17] YihanChen,BenfengXu,QuanWang,YiLiu,andZhendongMao.2024.BenchmarkingLargeLanguageModelson
ControllableGenerationunderDiversifiedInstructions. arXiv:2401.00690[cs.CL] https://arxiv.org/abs/2401.00690
[18] JinUkCho,MinSuJeong,JinYeongBak,andYun-GyungCheong.2022. Genre-ControllableStoryGenerationvia
SupervisedContrastiveLearning.InProceedingsoftheACMWebConference2022(VirtualEvent,Lyon,France)(WWW
’22).AssociationforComputingMachinery,NewYork,NY,USA,2839–2849. https://doi.org/10.1145/3485447.3512004
[19] cjadams,JeffreySorensen,JuliaElliott,LucasDixon,MarkMcDonald,nithum,WillCukierski.2018.JigsawToxic
CommentClassificationChallenge. https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge
,Vol.1,No.1,Article.Publicationdate:August2024.42 Liangetal.
[20] YimingCui,WanxiangChe,TingLiu,BingQin,ShijinWang,andGuopingHu.2020.RevisitingPre-TrainedModels
forChineseNaturalLanguageProcessing.InFindingsoftheAssociationforComputationalLinguistics:EMNLP
2020,TrevorCohn,YulanHe,andYangLiu(Eds.).AssociationforComputationalLinguistics,Online,657–668.
https://doi.org/10.18653/v1/2020.findings-emnlp.58
[21] JosefDai,XuehaiPan,RuiyangSun,JiamingJi,XinboXu,MickelLiu,YizhouWang,andYaodongYang.2024.Safe
RLHF:SafeReinforcementLearningfromHumanFeedback.InTheTwelfthInternationalConferenceonLearning
Representations. https://openreview.net/forum?id=TyFrPOKYXw
[22] SumanthDathathri,AndreaMadotto,JaniceLan,JaneHung,EricFrank,PieroMolino,JasonYosinski,andRosanne
Liu.2020. PlugandPlayLanguageModels:ASimpleApproachtoControlledTextGeneration.InInternational
ConferenceonLearningRepresentations. https://openreview.net/forum?id=H1edEyBKDS
[23] KarindeLangis,RyanKoo,andDongyeopKang.2024. ReinforcementLearningwithDynamicMulti-Reward
WeightingforMulti-StyleControllableGeneration. arXiv:2402.14146[cs.CL] https://arxiv.org/abs/2402.14146
[24] JasperDekoninck,MarcFischer,LucaBeurer-Kellner,andMartinVechev.2024. ControlledTextGenerationvia
LanguageModelArithmetic.InTheTwelfthInternationalConferenceonLearningRepresentations. https://openreview.
net/forum?id=SLw9fp4yI6
[25] HaikangDengandColinRaffel.2023. Reward-AugmentedDecoding:EfficientControlledTextGenerationWith
aUnidirectionalRewardModel.InProceedingsofthe2023ConferenceonEmpiricalMethodsinNaturalLanguage
Processing,HoudaBouamor,JuanPino,andKalikaBali(Eds.).AssociationforComputationalLinguistics,Singapore,
11781–11791. https://doi.org/10.18653/v1/2023.emnlp-main.721
[26] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.BERT:Pre-trainingofDeepBidirectional
TransformersforLanguageUnderstanding.InProceedingsofthe2019ConferenceoftheNorthAmericanChapter
oftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongandShortPapers),
JillBurstein,ChristyDoran,andThamarSolorio(Eds.).AssociationforComputationalLinguistics,Minneapolis,
Minnesota,4171–4186. https://doi.org/10.18653/v1/N19-1423
[27] HanxingDing,LiangPang,ZihaoWei,HuaweiShen,XueqiCheng,andTat-SengChua.2023.MacLaSa:Multi-Aspect
ControllableTextGenerationviaEfficientSamplingfromCompactLatentSpace.InFindingsoftheAssociation
forComputationalLinguistics:EMNLP2023,HoudaBouamor,JuanPino,andKalikaBali(Eds.).Associationfor
ComputationalLinguistics,Singapore,4424–4436. https://doi.org/10.18653/v1/2023.findings-emnlp.292
[28] GeorgeDoddington.2002.Automaticevaluationofmachinetranslationqualityusingn-gramco-occurrencestatistics.
InProceedingsofthesecondinternationalconferenceonHumanLanguageTechnologyResearch.138–145.
[29] JesseDodge,GabrielIlharco,RoySchwartz,AliFarhadi,HannanehHajishirzi,andNoahSmith.2020.Fine-Tuning
PretrainedLanguageModels:WeightInitializations,DataOrders,andEarlyStopping. arXiv:2002.06305[cs.CL]
https://arxiv.org/abs/2002.06305
[30] ChandraKiranReddyEvuru,SreyanGhosh,SonalKumar,RamaneswaranS,UtkarshTyagi,andDineshManocha.
2024.CoDa:ConstrainedGenerationbasedDataAugmentationforLow-ResourceNLP. arXiv:2404.00415[cs.CL]
https://arxiv.org/abs/2404.00415
[31] YuxiFeng,XiaoyuanYi,XitingWang,LaksLakshmanan,V.S.,andXingXie.2023.DuNST:DualNoisySelfTraining
forSemi-SupervisedControllableTextGeneration.InProceedingsofthe61stAnnualMeetingoftheAssociationfor
ComputationalLinguistics(Volume1:LongPapers),AnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(Eds.).
AssociationforComputationalLinguistics,Toronto,Canada,8760–8785. https://doi.org/10.18653/v1/2023.acl-long.488
[32] ZijianFeng,HanzhangZhou,KezhiMao,andZixiaoZhu.2024. FreeCtrl:ConstructingControlCenterswith
FeedforwardLayersforLearning-FreeControllableTextGeneration.InProceedingsofthe62ndAnnualMeetingofthe
AssociationforComputationalLinguistics(Volume1:LongPapers),Lun-WeiKu,AndreMartins,andVivekSrikumar
(Eds.).AssociationforComputationalLinguistics,Bangkok,Thailand,7627–7640. https://aclanthology.org/2024.acl-
long.412
[33] JaradForristal,FatemehsadatMireshghallah,GregDurrett,andTaylorBerg-Kirkpatrick.2023.ABlockMetropolis-
HastingsSamplerforControllableEnergy-basedTextGeneration.InProceedingsofthe27thConferenceonCom-
putationalNaturalLanguageLearning(CoNLL),JingJiang,DavidReitter,andShuminDeng(Eds.).Associationfor
ComputationalLinguistics,Singapore,403–413. https://doi.org/10.18653/v1/2023.conll-1.26
[34] SamuelGehman,SuchinGururangan,MaartenSap,YejinChoi,andNoahA.Smith.2020. RealToxicityPrompts:
EvaluatingNeuralToxicDegenerationinLanguageModels.InFindingsoftheAssociationforComputationalLinguistics:
EMNLP2020,TrevorCohn,YulanHe,andYangLiu(Eds.).AssociationforComputationalLinguistics,Online,3356–
3369. https://doi.org/10.18653/v1/2020.findings-emnlp.301
[35] YuxuanGu,XiaochengFeng,SichengMa,JiamingWu,HengGong,andBingQin.2022.ImprovingControllableText
GenerationwithPosition-AwareWeightedDecoding.InFindingsoftheAssociationforComputationalLinguistics:ACL
2022,SmarandaMuresan,PreslavNakov,andAlineVillavicencio(Eds.).AssociationforComputationalLinguistics,
Dublin,Ireland,3449–3467. https://doi.org/10.18653/v1/2022.findings-acl.272
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 43
[36] YuxuanGu,XiaochengFeng,SichengMa,LingyuanZhang,HengGong,andBingQin.2022. ADistributional
LensforMulti-AspectControllableTextGeneration.InProceedingsofthe2022ConferenceonEmpiricalMethodsin
NaturalLanguageProcessing,YoavGoldberg,ZornitsaKozareva,andYueZhang(Eds.).AssociationforComputational
Linguistics,AbuDhabi,UnitedArabEmirates,1023–1043. https://doi.org/10.18653/v1/2022.emnlp-main.67
[37] YuxuanGu,XiaochengFeng,SichengMa,LingyuanZhang,HengGong,WeihongZhong,andBingQin.2023.
ControllableTextGenerationviaProbabilityDensityEstimationintheLatentSpace.InProceedingsofthe61st
AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),AnnaRogers,Jordan
Boyd-Graber,andNaoakiOkazaki(Eds.).AssociationforComputationalLinguistics,Toronto,Canada,12590–12616.
https://doi.org/10.18653/v1/2023.acl-long.704
[38] BinGuo,HaoWang,YasanDing,WeiWu,ShaoyangHao,YueqiSun,andZhiwenYu.2021. ConditionalText
GenerationforHarmoniousHuman-MachineInteraction.ACMTrans.Intell.Syst.Technol.12,2,Article14(feb2021),
50pages. https://doi.org/10.1145/3439816
[39] XingangGuo,FangxuYu,HuanZhang,LianhuiQin,andBinHu.2024. COLD-Attack:JailbreakingLLMswith
StealthinessandControllability. arXiv:2402.08679[cs.LG] https://arxiv.org/abs/2402.08679
[40] SkylerHallinan,FaezeBrahman,XimingLu,JaehunJung,SeanWelleck,andYejinChoi.2023. STEER:Unified
StyleTransferwithExpertReinforcement.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2023,
HoudaBouamor,JuanPino,andKalikaBali(Eds.).AssociationforComputationalLinguistics,Singapore,7546–7562.
https://doi.org/10.18653/v1/2023.findings-emnlp.506
[41] SkylerHallinan,AlisaLiu,YejinChoi,andMaartenSap.2023.DetoxifyingTextwithMaRCo:ControllableRevision
withExpertsandAnti-Experts.InProceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics
(Volume2:ShortPapers),AnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(Eds.).AssociationforComputational
Linguistics,Toronto,Canada,228–242. https://doi.org/10.18653/v1/2023.acl-short.21
[42] ChiHan,JialiangXu,ManlingLi,YiFung,ChenkaiSun,NanJiang,TarekAbdelzaher,andHengJi.2024. Word
EmbeddingsAreSteersforLanguageModels. arXiv:2305.12798[cs.CL] https://arxiv.org/abs/2305.12798
[43] PengchengHe,XiaodongLiu,JianfengGao,andWeizhuChen.2021.DEBERTA:DECODING-ENHANCEDBERT
WITHDISENTANGLEDATTENTION.InInternationalConferenceonLearningRepresentations. https://openreview.
net/forum?id=XPZIaotutsD
[44] XingweiHe.2021.ParallelRefinementsforLexicallyConstrainedTextGenerationwithBART.InProceedingsofthe
2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,Marie-FrancineMoens,XuanjingHuang,Lucia
Specia,andScottWen-tauYih(Eds.).AssociationforComputationalLinguistics,OnlineandPuntaCana,Dominican
Republic,8653–8666. https://doi.org/10.18653/v1/2021.emnlp-main.681
[45] SeppHochreiterandJürgenSchmidhuber.1997.LongShort-TermMemory.NeuralComput.9,8(nov1997),1735–1780.
https://doi.org/10.1162/neco.1997.9.8.1735
[46] NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,BrunaMorrone,QuentindeLaroussilhe,AndreaGesmundo,
MonaAttariyan,andSylvainGelly.2019.Parameter-EfficientTransferLearningforNLP. arXiv:1902.00751[cs.LG]
https://arxiv.org/abs/1902.00751
[47] ZhitingHuandLiErranLi.2021.ACausalLensforControllableTextGeneration.InAdvancesinNeuralInformation
ProcessingSystems,A.Beygelzimer,Y.Dauphin,P.Liang,andJ.WortmanVaughan(Eds.). https://openreview.net/
forum?id=kAm9By0R5ME
[48] WenyueHua,XianjunYang,MingyuJin,WeiCheng,RuixiangTang,andYongfengZhang.2024. TrustAgent:
TowardsSafeandTrustworthyLLM-basedAgentsthroughAgentConstitution. arXiv:2402.01586[cs.CL] https:
//arxiv.org/abs/2402.01586
[49] XinyuHuaandLuWang.2020.PAIR:PlanningandIterativeRefinementinPre-trainedTransformersforLongText
Generation.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),
BonnieWebber,TrevorCohn,YulanHe,andYangLiu(Eds.).AssociationforComputationalLinguistics,Online,
781–793. https://doi.org/10.18653/v1/2020.emnlp-main.57
[50] XuanchengHuang,ZijunLiu,PengLi,TaoLi,MaosongSun,andYangLiu.2023. AnExtensiblePlug-and-Play
MethodforMulti-AspectControllableTextGeneration.InProceedingsofthe61stAnnualMeetingoftheAssociation
forComputationalLinguistics(Volume1:LongPapers),AnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(Eds.).
AssociationforComputationalLinguistics,Toronto,Canada,15233–15256. https://doi.org/10.18653/v1/2023.acl-
long.849
[51] RenlongJie,XiaojunMeng,LifengShang,XinJiang,andQunLiu.2024.Prompt-BasedLengthControlledGeneration
withMultipleControlTypes.InFindingsoftheAssociationforComputationalLinguisticsACL2024,Lun-WeiKu,
AndreMartins,andVivekSrikumar(Eds.).AssociationforComputationalLinguistics,Bangkok,Thailandandvirtual
meeting,1067–1085. https://aclanthology.org/2024.findings-acl.63
[52] NieJinran,YangLiner,ChenYun,KongCunliang,ZhuJunhui,andYangErhong.2023.LexicalComplexityControlled
SentenceGenerationforLanguageLearning.InProceedingsofthe22ndChineseNationalConferenceonComputational
,Vol.1,No.1,Article.Publicationdate:August2024.44 Liangetal.
Linguistics,MaosongSun,BingQin,XipengQiu,JingJiang,andXianpeiHan(Eds.).ChineseInformationProcessing
SocietyofChina,Harbin,China,648–664. https://aclanthology.org/2023.ccl-1.56
[53] RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu.2016.Exploringthelimitsoflanguage
modeling. https://arxiv.org/pdf/1602.02410.pdf
[54] Juseon-DoJuseon-Do,HidetakaKamigaito,ManabuOkumura,andJingunKwon.2024. InstructCMP:Length
ControlinSentenceCompressionthroughInstruction-basedLargeLanguageModels.InFindingsoftheAssociation
forComputationalLinguisticsACL2024,Lun-WeiKu,AndreMartins,andVivekSrikumar(Eds.).Associationfor
ComputationalLinguistics,Bangkok,Thailandandvirtualmeeting,8980–8996. https://aclanthology.org/2024.findings-
acl.532
[55] DmytroKalpakchiandJohanBoye.2023. SweCTRL-Mini:adata-transparentTransformer-basedlargelanguage
modelforcontrollabletextgenerationinSwedish. arXiv:2304.13994[cs.CL] https://arxiv.org/abs/2304.13994
[56] SaraKangaslahtiandDavidAlvarez-Melis.2024. ContinuousLanguageModelInterpolationforDynamicand
ControllableTextGeneration. arXiv:2404.07117[cs.CL] https://arxiv.org/abs/2404.07117
[57] PeiKe,HaoZhou,YankaiLin,PengLi,JieZhou,XiaoyanZhu,andMinlieHuang.2022.CTRLEval:AnUnsupervised
Reference-FreeMetricforEvaluatingControlledTextGeneration.InProceedingsofthe60thAnnualMeetingofthe
AssociationforComputationalLinguistics(Volume1:LongPapers),SmarandaMuresan,PreslavNakov,andAline
Villavicencio(Eds.).AssociationforComputationalLinguistics,Dublin,Ireland,2306–2319. https://doi.org/10.18653/
v1/2022.acl-long.164
[58] NitishShirishKeskar,BryanMcCann,LavR.Varshney,CaimingXiong,andRichardSocher.2019.CTRL:AConditional
TransformerLanguageModelforControllableGeneration. arXiv:1909.05858[cs.CL]
[59] MuhammadKhalifa,HadyElsahar,andMarcDymetman.2021. ADistributionalApproachtoControlledText
Generation.InInternationalConferenceonLearningRepresentations. https://openreview.net/forum?id=jWkw45-9AbL
[60] UrvashiKhandelwal,OmerLevy,DanJurafsky,LukeZettlemoyer,andMikeLewis.2020.Generalizationthrough
Memorization:NearestNeighborLanguageModels.InInternationalConferenceonLearningRepresentations. https:
//openreview.net/forum?id=HklBjCEKvH
[61] MinbeomKim,HwanheeLee,KangMinYoo,JoonsukPark,HwaranLee,andKyominJung.2023. Critic-Guided
DecodingforControlledTextGeneration.InFindingsoftheAssociationforComputationalLinguistics:ACL2023,Anna
Rogers,JordanBoyd-Graber,andNaoakiOkazaki(Eds.).AssociationforComputationalLinguistics,Toronto,Canada,
4598–4612. https://doi.org/10.18653/v1/2023.findings-acl.281
[62] DiederikPKingmaandMaxWelling.2022. Auto-EncodingVariationalBayes. arXiv:1312.6114[stat.ML] https:
//arxiv.org/abs/1312.6114
[63] TassiloKleinandMoinNabi.2024.ContrastivePerplexityforControlledGeneration:AnApplicationinDetoxifying
LargeLanguageModels. arXiv:2401.08491[cs.CL] https://arxiv.org/abs/2401.08491
[64] KaiKonen,SophieJentzsch,DiaouléDiallo,PeerSchütt,OliverBensch,RoxanneElBaff,DominikOpitz,andTobias
Hecking.2024. StyleVectorsforSteeringGenerativeLargeLanguageModels.InFindingsoftheAssociationfor
ComputationalLinguistics:EACL2024,YvetteGrahamandMatthewPurver(Eds.).AssociationforComputational
Linguistics,St.Julian’s,Malta,782–802. https://aclanthology.org/2024.findings-eacl.52
[65] BenKrause,AkhileshDeepakGotmare,BryanMcCann,NitishShirishKeskar,ShafiqJoty,RichardSocher,and
NazneenFatemaRajani.2021. GeDi:GenerativeDiscriminatorGuidedSequenceGeneration.InFindingsofthe
AssociationforComputationalLinguistics:EMNLP2021,Marie-FrancineMoens,XuanjingHuang,LuciaSpecia,and
ScottWen-tauYih(Eds.).AssociationforComputationalLinguistics,PuntaCana,DominicanRepublic,4929–4952.
https://doi.org/10.18653/v1/2021.findings-emnlp.424
[66] SachinKumar,EricMalmi,AliakseiSeveryn,andYuliaTsvetkov.2021.ControlledTextGenerationasContinuous
OptimizationwithMultipleConstraints.InAdvancesinNeuralInformationProcessingSystems,A.Beygelzimer,
Y.Dauphin,P.Liang,andJ.WortmanVaughan(Eds.). https://openreview.net/forum?id=kTy7bbm-4I4
[67] SachinKumar,BiswajitParia,andYuliaTsvetkov.2022.Gradient-basedConstrainedSamplingfromLanguageModels.
InProceedingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,YoavGoldberg,Zornitsa
Kozareva,andYueZhang(Eds.).AssociationforComputationalLinguistics,AbuDhabi,UnitedArabEmirates,
2251–2277. https://doi.org/10.18653/v1/2022.emnlp-main.144
[68] VaibhavKumar,HanaKoorehdavoudi,MasudMoshtaghi,AmitaMisra,AnkitChadha,andEmilioFerrara.2023.Con-
trolledTextGenerationwithHiddenRepresentationTransformations.InFindingsoftheAssociationforComputational
Linguistics:ACL2023,AnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(Eds.).AssociationforComputational
Linguistics,Toronto,Canada,9440–9455. https://doi.org/10.18653/v1/2023.findings-acl.602
[69] JinMyungKwak,MinseonKim,andSungJuHwang.2023.LanguageDetoxificationwithAttribute-Discriminative
LatentSpace.InProceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:Long
Papers),AnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(Eds.).AssociationforComputationalLinguistics,
Toronto,Canada,10149–10171. https://doi.org/10.18653/v1/2023.acl-long.565
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 45
[70] DavidLandsman,JerryZikunChen,andHussainZaidi.2022.BeamR:BeamReweighingwithAttributeDiscriminators
forControllableTextGeneration.InFindingsoftheAssociationforComputationalLinguistics:AACL-IJCNLP2022,
YulanHe,HengJi,SujianLi,YangLiu,andChua-HuiChang(Eds.).AssociationforComputationalLinguistics,Online
only,422–437. https://aclanthology.org/2022.findings-aacl.40
[71] JeanLee,NicholasStevens,SoyeonCarenHan,andMinseokSong.2024. ASurveyofLargeLanguageModelsin
Finance(FinLLMs). arXiv:2402.02315[cs.CL] https://arxiv.org/abs/2402.02315
[72] ChakTouLeong,YiCheng,JiashuoWang,JianWang,andWenjieLi.2023.Self-DetoxifyingLanguageModelsvia
ToxificationReversal.InProceedingsofthe2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,
HoudaBouamor,JuanPino,andKalikaBali(Eds.).AssociationforComputationalLinguistics,Singapore,4433–4449.
https://doi.org/10.18653/v1/2023.emnlp-main.269
[73] BrianLester,RamiAl-Rfou,andNoahConstant.2021.ThePowerofScaleforParameter-EfficientPromptTuning.
InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,Marie-FrancineMoens,
XuanjingHuang,LuciaSpecia,andScottWen-tauYih(Eds.).AssociationforComputationalLinguistics,Onlineand
PuntaCana,DominicanRepublic,3045–3059. https://doi.org/10.18653/v1/2021.emnlp-main.243
[74] JiweiLi,MichelGalley,ChrisBrockett,JianfengGao,andBillDolan.2016.ADiversity-PromotingObjectiveFunction
forNeuralConversationModels.InProceedingsofthe2016ConferenceoftheNorthAmericanChapteroftheAssociation
forComputationalLinguistics:HumanLanguageTechnologies,KevinKnight,AniNenkova,andOwenRambow(Eds.).
AssociationforComputationalLinguistics,SanDiego,California,110–119. https://doi.org/10.18653/v1/N16-1014
[75] WendiLi,WeiWei,KaiheXu,WenfengXie,DangyangChen,andYuCheng.2024. ReinforcementLearningwith
Token-levelFeedbackforControllableTextGeneration. arXiv:2403.11558[cs.CL] https://arxiv.org/abs/2403.11558
[76] XiangLisaLiandPercyLiang.2021.Prefix-Tuning:OptimizingContinuousPromptsforGeneration.InProceedings
ofthe59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe11thInternationalJointConference
onNaturalLanguageProcessing(Volume1:LongPapers),ChengqingZong,FeiXia,WenjieLi,andRobertoNavigli
(Eds.).AssociationforComputationalLinguistics,Online,4582–4597. https://doi.org/10.18653/v1/2021.acl-long.353
[77] YanranLi,HuiSu,XiaoyuShen,WenjieLi,ZiqiangCao,andShuziNiu.2017. DailyDialog:AManuallyLabelled
Multi-turnDialogueDataset.InProceedingsoftheEighthInternationalJointConferenceonNaturalLanguageProcessing
(Volume1:LongPapers),GregKondrakandTaroWatanabe(Eds.).AsianFederationofNaturalLanguageProcessing,
Taipei,Taiwan,986–995. https://aclanthology.org/I17-1099
[78] YuLi,ZhihuaWei,HanJiang,andChuanyangGong.2024.DESTEIN:NavigatingDetoxificationofLanguageModelsvia
UniversalSteeringPairsandHead-wiseActivationFusion. arXiv:2404.10464[cs.CL] https://arxiv.org/abs/2404.10464
[79] XunLiang,ShichaoSong,SiminNiu,ZhiyuLi,FeiyuXiong,BoTang,YezhaohuiWang,DaweiHe,ChengPeng,
ZhonghaoWang,andHaiyingDeng.2024.UHGEval:BenchmarkingtheHallucinationofChineseLargeLanguage
ModelsviaUnconstrainedGeneration.InProceedingsofthe62ndAnnualMeetingoftheAssociationforComputa-
tionalLinguistics(Volume1:LongPapers),Lun-WeiKu,AndreMartins,andVivekSrikumar(Eds.).Associationfor
ComputationalLinguistics,Bangkok,Thailand,5266–5293. https://aclanthology.org/2024.acl-long.288
[80] XunLiang,ShichaoSong,ZifanZheng,HanyuWang,QingchenYu,XunkaiLi,Rong-HuaLi,FeiyuXiong,andZhiyu
Li.2024. InternalConsistencyandSelf-FeedbackinLargeLanguageModels:ASurvey. arXiv:2407.14507[cs.CL]
https://arxiv.org/abs/2407.14507
[81] XunLiang,HanyuWang,ShichaoSong,MengtingHu,XunzhiWang,ZhiyuLi,FeiyuXiong,andBoTang.2024.
ControlledTextGenerationforLargeLanguageModelwithDynamicAttributeGraphs.InFindingsoftheAssociation
forComputationalLinguisticsACL2024,Lun-WeiKu,AndreMartins,andVivekSrikumar(Eds.).Associationfor
ComputationalLinguistics,Bangkok,Thailandandvirtualmeeting,5797–5814. https://aclanthology.org/2024.findings-
acl.345
[82] BillYuchenLin,AbhilashaRavichander,XimingLu,NouhaDziri,MelanieSclar,KhyathiChandu,ChandraBhagavat-
ula,andYejinChoi.2024.TheUnlockingSpellonBaseLLMs:RethinkingAlignmentviaIn-ContextLearning.InThe
TwelfthInternationalConferenceonLearningRepresentations. https://openreview.net/forum?id=wxJ0eXwwda
[83] Chin-YewLin.2004.ROUGE:APackageforAutomaticEvaluationofSummaries.InTextSummarizationBranches
Out.AssociationforComputationalLinguistics,Barcelona,Spain,74–81. https://aclanthology.org/W04-1013
[84] ZhiyuLinandMarkRiedl.2021. Plug-and-Blend:AFrameworkforControllableStoryGenerationwithBlended
ControlCodes.InProceedingsoftheThirdWorkshoponNarrativeUnderstanding,NaderAkoury,FaezeBrahman,
SnigdhaChaturvedi,ElizabethClark,MohitIyyer,andLaraJ.Martin(Eds.).AssociationforComputationalLinguistics,
Virtual,62–71. https://doi.org/10.18653/v1/2021.nuse-1.7
[85] AlisaLiu,MaartenSap,XimingLu,SwabhaSwayamdipta,ChandraBhagavatula,NoahA.Smith,andYejinChoi.
2021. DExperts:Decoding-TimeControlledTextGenerationwithExpertsandAnti-Experts.InProceedingsofthe
59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe11thInternationalJointConferenceon
NaturalLanguageProcessing(Volume1:LongPapers),ChengqingZong,FeiXia,WenjieLi,andRobertoNavigli(Eds.).
AssociationforComputationalLinguistics,Online,6691–6706. https://doi.org/10.18653/v1/2021.acl-long.522
,Vol.1,No.1,Article.Publicationdate:August2024.46 Liangetal.
[86] HanLiu,BingningWang,TingYao,HaijinLiang,JianjinXu,andXiaolinHu.2022.BridgingtheGapBetweenTraining
andInferenceofBayesianControllableLanguageModels. arXiv:2206.05519[cs.CL] https://arxiv.org/abs/2206.05519
[87] ShengLiu,HaotianYe,LeiXing,andJamesZou.2024.In-contextVectors:MakingInContextLearningMoreEffective
andControllableThroughLatentSpaceSteering. arXiv:2311.06668[cs.LG] https://arxiv.org/abs/2311.06668
[88] XinLiu,MuhammadKhalifa,andLuWang.2023.BOLT:FastEnergy-basedControlledTextGenerationwithTunable
Biases.InProceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2:Short
Papers),AnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(Eds.).AssociationforComputationalLinguistics,
Toronto,Canada,186–200. https://doi.org/10.18653/v1/2023.acl-short.18
[89] XiaoLiu,YananZheng,ZhengxiaoDu,MingDing,YujieQian,ZhilinYang,andJieTang.2023.GPTUnderstands,
Too. arXiv:2103.10385[cs.CL] https://arxiv.org/abs/2103.10385
[90] YiLiu,XiangyuLiu,XiangrongZhu,andWeiHu.2024.Multi-AspectControllableTextGenerationwithDisentangled
CounterfactualAugmentation.InProceedingsofthe62ndAnnualMeetingoftheAssociationforComputationalLinguis-
tics(Volume1:LongPapers),Lun-WeiKu,AndreMartins,andVivekSrikumar(Eds.).AssociationforComputational
Linguistics,Bangkok,Thailand,9231–9253. https://aclanthology.org/2024.acl-long.500
[91] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,
andVeselinStoyanov.2019.Roberta:Arobustlyoptimizedbertpretrainingapproach.arXivpreprintarXiv:1907.11692
(2019).
[92] YinhongLiu,YixuanSu,EhsanShareghi,andNigelCollier.2022.Plug-and-PlayRecipeGenerationwithContent
Planning.InProceedingsofthe2ndWorkshoponNaturalLanguageGeneration,Evaluation,andMetrics(GEM),Antoine
Bosselut,KhyathiChandu,KaustubhDhole,VarunGangal,SebastianGehrmann,YacineJernite,JekaterinaNovikova,
andLauraPerez-Beltrachini(Eds.).AssociationforComputationalLinguistics,AbuDhabi,UnitedArabEmirates
(Hybrid),223–234. https://doi.org/10.18653/v1/2022.gem-1.19
[93] MichelaLorandiandAnyaBelz.2023.HowtoControlSentimentinTextGeneration:ASurveyoftheState-of-the-Art
inSentiment-ControlTechniques.InProceedingsofthe13thWorkshoponComputationalApproachestoSubjectivity,
Sentiment,&SocialMediaAnalysis,JeremyBarnes,OrphéeDeClercq,andRomanKlinger(Eds.).Associationfor
ComputationalLinguistics,Toronto,Canada,341–353. https://doi.org/10.18653/v1/2023.wassa-1.30
[94] XimingLu,SeanWelleck,PeterWest,LiweiJiang,JungoKasai,DanielKhashabi,RonanLeBras,LianhuiQin,
YoungjaeYu,RowanZellers,NoahA.Smith,andYejinChoi.2022. NeuroLogicA*esqueDecoding:Constrained
TextGenerationwithLookaheadHeuristics.InProceedingsofthe2022ConferenceoftheNorthAmericanChapter
oftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,MarineCarpuat,Marie-Catherine
deMarneffe,andIvanVladimirMezaRuiz(Eds.).AssociationforComputationalLinguistics,Seattle,UnitedStates,
780–799. https://doi.org/10.18653/v1/2022.naacl-main.57
[95] XimingLu,PeterWest,RowanZellers,RonanLeBras,ChandraBhagavatula,andYejinChoi.2021. NeuroLogic
Decoding:(Un)supervisedNeuralTextGenerationwithPredicateLogicConstraints.InProceedingsofthe2021Confer-
enceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,
KristinaToutanova,AnnaRumshisky,LukeZettlemoyer,DilekHakkani-Tur,IzBeltagy,StevenBethard,RyanCot-
terell,TanmoyChakraborty,andYichaoZhou(Eds.).AssociationforComputationalLinguistics,Online,4288–4299.
https://doi.org/10.18653/v1/2021.naacl-main.339
[96] ZhenyiLu,WeiWei,XiaoyeQu,Xian-LingMao,DangyangChen,andJixiongChen.2023. Miracle:Towards
PersonalizedDialogueGenerationwithLatent-SpaceMultiplePersonalAttributeControl.InFindingsoftheAssociation
forComputationalLinguistics:EMNLP2023,HoudaBouamor,JuanPino,andKalikaBali(Eds.).Associationfor
ComputationalLinguistics,Singapore,5933–5957. https://doi.org/10.18653/v1/2023.findings-emnlp.395
[97] CongdaMa,TianyuZhao,MakotoShing,KeiSawada,andManabuOkumura.2023. FocusedPrefixTuningfor
ControllableTextGeneration.InProceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics
(Volume2:ShortPapers),AnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(Eds.).AssociationforComputational
Linguistics,Toronto,Canada,1116–1127. https://doi.org/10.18653/v1/2023.acl-short.96
[98] AndrewL.Maas,RaymondE.Daly,PeterT.Pham,DanHuang,AndrewY.Ng,andChristopherPotts.2011.Learning
WordVectorsforSentimentAnalysis.InProceedingsofthe49thAnnualMeetingoftheAssociationforComputational
Linguistics:HumanLanguageTechnologies,DekangLin,YujiMatsumoto,andRadaMihalcea(Eds.).Associationfor
ComputationalLinguistics,Portland,Oregon,USA,142–150. https://aclanthology.org/P11-1015
[99] JulianMcAuleyandJureLeskovec.2013.Hiddenfactorsandhiddentopics:understandingratingdimensionswith
reviewtext.InProceedingsofthe7thACMconferenceonRecommendersystems.165–172.
[100] TaoMeng,SidiLu,NanyunPeng,andKai-WeiChang.2022.ControllableTextGenerationwithNeurally-Decomposed
Oracle.InAdvancesinNeuralInformationProcessingSystems,S.Koyejo,S.Mohamed,A.Agarwal,D.Belgrave,K.Cho,
andA.Oh(Eds.),Vol.35.CurranAssociates,Inc.,28125–28139. https://proceedings.neurips.cc/paper_files/paper/
2022/file/b40d5797756800c97f3d525c2e4c8357-Paper-Conference.pdf
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 47
[101] FatemehsadatMireshghallah,KartikGoyal,andTaylorBerg-Kirkpatrick.2022. MixandMatch:Learning-free
ControllableTextGenerationusingEnergyLanguageModels.InProceedingsofthe60thAnnualMeetingofthe
AssociationforComputationalLinguistics(Volume1:LongPapers),SmarandaMuresan,PreslavNakov,andAline
Villavicencio(Eds.).AssociationforComputationalLinguistics,Dublin,Ireland,401–415. https://doi.org/10.18653/
v1/2022.acl-long.31
[102] SidharthMudgal,JongLee,HarishGanapathy,YaGuangLi,TaoWang,YanpingHuang,ZhifengChen,Heng-Tze
Cheng,MichaelCollins,JilinChen,AlexBeutel,andAhmadBeirami.2023. ControlledDecodingfromLanguage
Models.InSociallyResponsibleLanguageModellingResearch. https://openreview.net/forum?id=jo57H1CpD8
[103] GillesNawezi,LucieFlek,andCharlesWelch.2023.StyleLocalityforControllableGenerationwithkNNLanguage
Models.InProceedingsofthe1stWorkshoponTamingLargeLanguageModels:ControllabilityintheeraofInteractive
Assistants!,DevamanyuHazarika,XiangruRobertTang,andDiJin(Eds.).AssociationforComputationalLinguistics,
Prague,CzechRepublic,68–75. https://aclanthology.org/2023.tllm-1.7
[104] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,ChongZhang,Sandhini
Agarwal,KatarinaSlama,AlexRay,JohnSchulman,JacobHilton,FraserKelton,LukeMiller,MaddieSimens,Amanda
Askell,PeterWelinder,PaulFChristiano,JanLeike,andRyanLowe.2022. Traininglanguagemodelstofollow
instructionswithhumanfeedback.InAdvancesinNeuralInformationProcessingSystems,S.Koyejo,S.Mohamed,
A.Agarwal,D.Belgrave,K.Cho,andA.Oh(Eds.),Vol.35.CurranAssociates,Inc.,27730–27744. https://proceedings.
neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf
[105] KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002. Bleu:aMethodforAutomaticEvaluation
ofMachineTranslation.InProceedingsofthe40thAnnualMeetingoftheAssociationforComputationalLinguistics,
PierreIsabelle,EugeneCharniak,andDekangLin(Eds.).AssociationforComputationalLinguistics,Philadelphia,
Pennsylvania,USA,311–318. https://doi.org/10.3115/1073083.1073135
[106] DamianPascual,BeniEgressy,ClaraMeister,RyanCotterell,andRogerWattenhofer.2021.APlug-and-PlayMethodfor
ControlledTextGeneration.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2021,Marie-Francine
Moens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(Eds.).AssociationforComputationalLinguistics,
PuntaCana,DominicanRepublic,3973–3997. https://doi.org/10.18653/v1/2021.findings-emnlp.334
[107] JonathanPei,KevinYang,andDanKlein.2023.PREADD:Prefix-AdaptiveDecodingforControlledTextGeneration.
InFindingsoftheAssociationforComputationalLinguistics:ACL2023,AnnaRogers,JordanBoyd-Graber,andNaoaki
Okazaki(Eds.).AssociationforComputationalLinguistics,Toronto,Canada,10018–10037. https://doi.org/10.18653/
v1/2023.findings-acl.636
[108] LuizaPozzobon,BeyzaErmis,PatrickLewis,andSaraHooker.2023. Goodtriever:AdaptiveToxicityMitigation
withRetrieval-augmentedModels.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2023,Houda
Bouamor,JuanPino,andKalikaBali(Eds.).AssociationforComputationalLinguistics,Singapore,5108–5125. https:
//doi.org/10.18653/v1/2023.findings-emnlp.339
[109] ShrimaiPrabhumoye,AlanWBlack,andRuslanSalakhutdinov.2020. ExploringControllableTextGeneration
Techniques.InProceedingsofthe28thInternationalConferenceonComputationalLinguistics,DoniaScott,NuriaBel,
andChengqingZong(Eds.).InternationalCommitteeonComputationalLinguistics,Barcelona,Spain(Online),1–14.
https://doi.org/10.18653/v1/2020.coling-main.1
[110] JingQian,LiDong,YelongShen,FuruWei,andWeizhuChen.2022. ControllableNaturalLanguageGeneration
withContrastivePrefixes.InFindingsoftheAssociationforComputationalLinguistics:ACL2022,SmarandaMuresan,
PreslavNakov,andAlineVillavicencio(Eds.).AssociationforComputationalLinguistics,Dublin,Ireland,2912–2924.
https://doi.org/10.18653/v1/2022.findings-acl.229
[111] LianhuiQin,SeanWelleck,DanielKhashabi,andYejinChoi.2022.COLDDecoding:Energy-basedConstrainedText
GenerationwithLangevinDynamics.InAdvancesinNeuralInformationProcessingSystems,S.Koyejo,S.Mohamed,
A.Agarwal,D.Belgrave,K.Cho,andA.Oh(Eds.),Vol.35.CurranAssociates,Inc.,9538–9551. https://proceedings.
neurips.cc/paper_files/paper/2022/file/3e25d1aff47964c8409fd5c8dc0438d7-Paper-Conference.pdf
[112] AlecRadford,LukeMetz,andSoumithChintala.2016.UnsupervisedRepresentationLearningwithDeepConvolutional
GenerativeAdversarialNetworks.In4thInternationalConferenceonLearningRepresentations,ICLR2016,SanJuan,
PuertoRico,May2-4,2016,ConferenceTrackProceedings.
[113] AbirRahaliandMoulayA.Akhloufi.2023.DeepPress:guidedpressreleasetopic-awaretextgenerationusingensemble
transformers.NeuralComputingandApplications35,17(2023),12847–12874. https://doi.org/10.1007/s00521-023-
08393-4
[114] AngelaRamirez,KartikAgarwal,JurajJuraska,UtkarshGarg,andMarilynWalker.2023.ControllableGenerationof
DialogueActsforDialogueSystemsviaFew-ShotResponseGenerationandRanking.InProceedingsofthe24thAnnual
MeetingoftheSpecialInterestGrouponDiscourseandDialogue,SvetlanaStoyanchev,ShafiqJoty,DavidSchlangen,
OndrejDusek,CaseyKennington,andMaliheAlikhani(Eds.).AssociationforComputationalLinguistics,Prague,
Czechia,355–369. https://doi.org/10.18653/v1/2023.sigdial-1.32
,Vol.1,No.1,Article.Publicationdate:August2024.48 Liangetal.
[115] Marc’AurelioRanzato,SumitChopra,MichaelAuli,andWojciechZaremba.2016. SequenceLevelTrainingwith
RecurrentNeuralNetworks. arXiv:1511.06732[cs.LG] https://arxiv.org/abs/1511.06732
[116] DavidE.Rumelhart,GeoffreyE.Hinton,andRonaldJ.Williams.1986.Learningrepresentationsbyback-propagating
errors.Nature323,6088(1986),533–536. https://doi.org/10.1038/323533a0
[117] PunyajoySaha,KanishkSingh,AdarshKumar,BinnyMathew,andAnimeshMukherjee.2022. CounterGeDi:A
ControllableApproachtoGeneratePolite,DetoxifiedandEmotionalCounterspeech.InProceedingsoftheThirty-First
InternationalJointConferenceonArtificialIntelligence,IJCAI-22,LudDeRaedt(Ed.).InternationalJointConferences
onArtificialIntelligenceOrganization,5157–5163. https://doi.org/10.24963/ijcai.2022/716AIforGood.
[118] VictorSanh,LysandreDebut,JulienChaumond,andThomasWolf.2019. DistilBERT,adistilledversionofBERT:
smaller,faster,cheaperandlighter.arXivpreprintarXiv:1910.01108(2019).
[119] YuichiSasazawa,TerufumiMorishita,HiroakiOzaki,OsamuImaichi,andYasuhiroSogawa.2023. Controlling
keywordsandtheirpositionsintextgeneration.InProceedingsofthe16thInternationalNaturalLanguageGeneration
Conference,C.MariaKeet,Hung-YiLee,andSinaZarrieß(Eds.).AssociationforComputationalLinguistics,Prague,
Czechia,407–413. https://doi.org/10.18653/v1/2023.inlg-main.29
[120] TimoSchick,SahanaUdupa,andHinrichSchütze.2021.Self-DiagnosisandSelf-Debiasing:AProposalforReducing
Corpus-BasedBiasinNLP.TransactionsoftheAssociationforComputationalLinguistics9(2021),1408–1424. https:
//doi.org/10.1162/tacl_a_00434
[121] ThibaultSellam,DipanjanDas,andAnkurParikh.2020. BLEURT:LearningRobustMetricsforTextGeneration.
InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,DanJurafsky,Joyce
Chai,NatalieSchluter,andJoelTetreault(Eds.).AssociationforComputationalLinguistics,Online,7881–7892.
https://doi.org/10.18653/v1/2020.acl-main.704
[122] DamithChamalkeSenadeeraandJuliaIve.2022.ControlledTextGenerationusingT5basedEncoder-DecoderSoft
PromptTuningandAnalysisoftheUtilityofGeneratedTextinAI. arXiv:2212.02924[cs.CL] https://arxiv.org/abs/
2212.02924
[123] ZhihongShao,MinlieHuang,JiangtaoWen,WenfeiXu,andXiaoyanZhu.2019.LongandDiverseTextGeneration
withPlanning-basedHierarchicalVariationalModel.InProceedingsofthe2019ConferenceonEmpiricalMethods
inNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-
IJCNLP),KentaroInui,JingJiang,VincentNg,andXiaojunWan(Eds.).AssociationforComputationalLinguistics,
HongKong,China,3257–3268. https://doi.org/10.18653/v1/D19-1321
[124] ChenhuiShen,LiyingCheng,RanZhou,LidongBing,YangYou,andLuoSi.2022.MReD:AMeta-ReviewDataset
forStructure-ControllableTextGeneration.InFindingsoftheAssociationforComputationalLinguistics:ACL2022,
SmarandaMuresan,PreslavNakov,andAlineVillavicencio(Eds.).AssociationforComputationalLinguistics,Dublin,
Ireland,2521–2535. https://doi.org/10.18653/v1/2022.findings-acl.198
[125] ChufanShi,DengCai,andYujiuYang.2024.LiFi:LightweightControlledTextGenerationwithFine-GrainedControl
Codes. arXiv:2402.06930[cs.CL] https://arxiv.org/abs/2402.06930
[126] TaylorShin,YasamanRazeghi,RobertL.LoganIV,EricWallace,andSameerSingh.2020. AutoPrompt:Eliciting
KnowledgefromLanguageModelswithAutomaticallyGeneratedPrompts.InProceedingsofthe2020Conference
onEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),BonnieWebber,TrevorCohn,YulanHe,andYang
Liu(Eds.).AssociationforComputationalLinguistics,Online,4222–4235. https://doi.org/10.18653/v1/2020.emnlp-
main.346
[127] AskhatSitdikov,NikitaBalagansky,DaniilGavrilov,andAlexanderMarkov.2022.ClassifiersareBetterExpertsfor
ControllableTextGeneration. arXiv:2205.07276[cs.CL] https://arxiv.org/abs/2205.07276
[128] MatthewSnover,BonnieDorr,RichSchwartz,LinneaMicciulla,andJohnMakhoul.2006.AStudyofTranslationEdit
RatewithTargetedHumanAnnotation.InProceedingsofthe7thConferenceoftheAssociationforMachineTranslation
intheAmericas:TechnicalPapers.AssociationforMachineTranslationintheAmericas,Cambridge,Massachusetts,
USA,223–231. https://aclanthology.org/2006.amta-papers.25
[129] RichardSocher,AlexPerelygin,JeanWu,JasonChuang,ChristopherD.Manning,AndrewNg,andChristopher
Potts.2013.RecursiveDeepModelsforSemanticCompositionalityOveraSentimentTreebank.InProceedingsof
the2013ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,DavidYarowsky,TimothyBaldwin,Anna
Korhonen,KarenLivescu,andStevenBethard(Eds.).AssociationforComputationalLinguistics,Seattle,Washington,
USA,1631–1642. https://aclanthology.org/D13-1170
[130] AlexanderSpangher,YaoMing,XinyuHua,andNanyunPeng.2022.SequentiallyControlledTextGeneration.In
FindingsoftheAssociationforComputationalLinguistics:EMNLP2022,YoavGoldberg,ZornitsaKozareva,andYue
Zhang(Eds.).AssociationforComputationalLinguistics,AbuDhabi,UnitedArabEmirates,6848–6866. https:
//doi.org/10.18653/v1/2022.findings-emnlp.509
[131] NisanStiennon,LongOuyang,JeffreyWu,DanielZiegler,RyanLowe,ChelseaVoss,AlecRadford,DarioAmodei,and
PaulFChristiano.2020.Learningtosummarizewithhumanfeedback.InAdvancesinNeuralInformationProcessing
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 49
Systems,H.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin(Eds.),Vol.33.CurranAssociates,Inc.,
3008–3021. https://proceedings.neurips.cc/paper_files/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf
[132] NishantSubramani,NiveditaSuresh,andMatthewPeters.2022.ExtractingLatentSteeringVectorsfromPretrained
LanguageModels.InFindingsoftheAssociationforComputationalLinguistics:ACL2022,SmarandaMuresan,Preslav
Nakov,andAlineVillavicencio(Eds.).AssociationforComputationalLinguistics,Dublin,Ireland,566–581. https:
//doi.org/10.18653/v1/2022.findings-acl.48
[133] RichardS.Sutton,DavidMcAllester,SatinderSingh,andYishayMansour.1999. Policygradientmethodsfor
reinforcementlearningwithfunctionapproximation.InProceedingsofthe12thInternationalConferenceonNeural
InformationProcessingSystems(Denver,CO)(NIPS’99).MITPress,Cambridge,MA,USA,1057–1063.
[134] ZhenTao,DinghaoXi,ZhiyuLi,LiuminTang,andWeiXu.2024.CAT-LLM:PromptingLargeLanguageModelswith
TextStyleDefinitionforChineseArticle-styleTransfer. arXiv:2401.05707[cs.CL] https://arxiv.org/abs/2401.05707
[135] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,Baptiste
Rozière,NamanGoyal,EricHambro,FaisalAzhar,AurelienRodriguez,ArmandJoulin,EdouardGrave,andGuillaume
Lample.2023.LLaMA:OpenandEfficientFoundationLanguageModels. arXiv:2302.13971[cs.CL] https://arxiv.org/
abs/2302.13971
[136] SeverinoTrotta,LucieFlek,andCharlesWelch.2022.NearestNeighborLanguageModelsforStylisticControllable
Generation.InProceedingsofthe2ndWorkshoponNaturalLanguageGeneration,Evaluation,andMetrics(GEM),
AntoineBosselut,KhyathiChandu,KaustubhDhole,VarunGangal,SebastianGehrmann,YacineJernite,Jekaterina
Novikova,andLauraPerez-Beltrachini(Eds.).AssociationforComputationalLinguistics,AbuDhabi,UnitedArab
Emirates(Hybrid),295–305. https://doi.org/10.18653/v1/2022.gem-1.25
[137] AlexanderMattTurner,LisaThiergart,GavinLeech,DavidUdell,JuanJ.Vazquez,UlisseMini,andMonteMacDiarmid.
2024. ActivationAddition:SteeringLanguageModelsWithoutOptimization. arXiv:2308.10248[cs.CL] https:
//arxiv.org/abs/2308.10248
[138] BhargavUpadhyay,AkhileshSudhakar,andArjunMaheswaran.2022.EfficientReinforcementLearningforUnsuper-
visedControlledTextGeneration. arXiv:2204.07696[cs.CL] https://arxiv.org/abs/2204.07696
[139] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia
Polosukhin.2017.AttentionisAllyouNeed.InAdvancesinNeuralInformationProcessingSystems,I.Guyon,U.Von
Luxburg,S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett(Eds.),Vol.30.CurranAssociates,Inc.
https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
[140] RamakrishnaVedantam,C.LawrenceZitnick,andDeviParikh.2015.CIDEr:Consensus-BasedImageDescription
Evaluation.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition(CVPR).
[141] SergeyVychegzhanin,AnastasiaKotelnikova,AlexanderSergeev,andEvgenyKotelnikov.2024.ControllableStory
GenerationBasedonPerplexityMinimization.InAnalysisofImages,SocialNetworksandTexts,DmitryI.Ignatov,
MichaelKhachay,AndreyKutuzov,HabetMadoyan,IlyaMakarov,IrinaNikishina,AlexanderPanchenko,Maxim
Panov,PanosM.Pardalos,AndreyV.Savchenko,EvgeniiTsymbalov,ElenaTutubalina,andSergeyZagoruyko(Eds.).
SpringerNatureSwitzerland,Cham,154–169.
[142] S.V.VychegzhaninandE.V.Kotelnikov.2022.Collocation2Text:ControllableTextGenerationfromGuidePhrasesin
Russian.InComputationalLinguisticsandIntellectualTechnologies.RSUH. https://doi.org/10.28995/2075-7182-2022-
21-564-576
[143] ZhongweiWan,XinWang,CheLiu,SamiulAlam,YuZheng,etal.2023.Efficientlargelanguagemodels:Asurvey.
arXivpreprintarXiv:2312.038631(2023).
[144] HaoWangandLeiSha.2023. HarnessingthePlug-and-PlayControllerbyPrompting.InProceedingsoftheThird
WorkshoponNaturalLanguageGeneration,Evaluation,andMetrics(GEM),SebastianGehrmann,AlexWang,João
Sedoc,ElizabethClark,KaustubhDhole,KhyathiRaghaviChandu,EnricoSantus,andHoomanSedghamiz(Eds.).
AssociationforComputationalLinguistics,Singapore,165–174. https://aclanthology.org/2023.gem-1.14
[145] JunliWang,ChenyangZhang,DongyuZhang,HaiboTong,ChungangYan,andChangjunJiang.2024.ARecentSurvey
onControllableTextGeneration:aCausalPerspective.FundamentalResearch(2024). https://api.semanticscholar.
org/CorpusID:266926474
[146] PengyuWang,DongZhang,LinyangLi,ChenkunTan,XinghaoWang,KeRen,BotianJiang,andXipengQiu.2024.
InferAligner:Inference-TimeAlignmentforHarmlessnessthroughCross-ModelGuidance. arXiv:2401.11206[cs.CL]
https://arxiv.org/abs/2401.11206
[147] XinpengWang,HanJiang,ZhihuaWei,andShanlinZhou.2022.CHAE:Fine-GrainedControllableStoryGeneration
withCharacters,ActionsandEmotions.InProceedingsofthe29thInternationalConferenceonComputationalLinguistics,
NicolettaCalzolari,Chu-RenHuang,HansaemKim,JamesPustejovsky,LeoWanner,Key-SunChoi,Pum-MoRyu,
Hsin-HsiChen,LuciaDonatelli,HengJi,SadaoKurohashi,PatriziaPaggio,NianwenXue,SeokhwanKim,Younggyun
Hahm,ZhongHe,TonyKyungilLee,EnricoSantus,FrancisBond,andSeung-HoonNa(Eds.).InternationalCommittee
onComputationalLinguistics,Gyeongju,RepublicofKorea,6426–6435. https://aclanthology.org/2022.coling-1.559
,Vol.1,No.1,Article.Publicationdate:August2024.50 Liangetal.
[148] JasonWei,MaartenBosma,VincentZhao,KelvinGuu,AdamsWeiYu,BrianLester,NanDu,AndrewM.Dai,and
QuocVLe.2022. FinetunedLanguageModelsareZero-ShotLearners.InInternationalConferenceonLearning
Representations. https://openreview.net/forum?id=gEZrGCozdqR
[149] ZhihuaWen,ZhiliangTian,ZhenHuang,YuxinYang,ZexinJian,ChangjianWang,andDongshengLi.2023.GRACE:
Gradient-guidedControllableRetrievalforAugmentingAttribute-basedTextGeneration.InFindingsoftheAssociation
forComputationalLinguistics:ACL2023,AnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(Eds.).Association
forComputationalLinguistics,Toronto,Canada,8377–8398. https://doi.org/10.18653/v1/2023.findings-acl.530
[150] CongyingXia,ChenXing,JiangshuDu,XinyiYang,YihaoFeng,RanXu,WenpengYin,andCaimingXiong.2024.
FOFO:ABenchmarktoEvaluateLLMs’Format-FollowingCapability.arXivpreprintarXiv:2402.18667(2024).
[151] ChenXu,TianLan,ChanglongYu,WeiWang,JunGao,YuJi,QunxiDong,KunQian,PijiLi,WeiBi,andBinHu.2024.
DECIDER:ADual-SystemRule-ControllableDecodingFrameworkforLanguageGeneration.arXiv:2403.01954[cs.CL]
https://arxiv.org/abs/2403.01954
[152] PengXu,MostofaPatwary,MohammadShoeybi,RaulPuri,PascaleFung,AnimaAnandkumar,andBryanCatanzaro.
2020.MEGATRON-CNTRL:ControllableStoryGenerationwithExternalKnowledgeUsingLarge-ScaleLanguage
Models.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),Bonnie
Webber,TrevorCohn,YulanHe,andYangLiu(Eds.).AssociationforComputationalLinguistics,Online,2831–2845.
https://doi.org/10.18653/v1/2020.emnlp-main.226
[153] KevinYangandDanKlein.2021.FUDGE:ControlledTextGenerationWithFutureDiscriminators.InProceedingsof
the2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguage
Technologies,KristinaToutanova,AnnaRumshisky,LukeZettlemoyer,DilekHakkani-Tur,IzBeltagy,StevenBethard,
RyanCotterell,TanmoyChakraborty,andYichaoZhou(Eds.).AssociationforComputationalLinguistics,Online,
3511–3535. https://doi.org/10.18653/v1/2021.naacl-main.276
[154] KexinYang,DayihengLiu,WenqiangLei,BaosongYang,MingfengXue,BoxingChen,andJunXie.2023. Tailor:
ASoft-Prompt-BasedApproachtoAttribute-BasedControlledTextGeneration.InProceedingsofthe61stAnnual
MeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),AnnaRogers,JordanBoyd-Graber,
andNaoakiOkazaki(Eds.).AssociationforComputationalLinguistics,Toronto,Canada,410–427. https://doi.org/10.
18653/v1/2023.acl-long.25
[155] ZheYang,YiHuang,YaqinChen,XiaotingWu,JunlanFeng,andChaoDeng.2024. CTGGAN:ControllableText
GenerationwithGenerativeAdversarialNetwork.AppliedSciences14,7(2024). https://doi.org/10.3390/app14073106
[156] ZhianYang,HaoJiang,AoboDeng,andYangLi.2024.Topic-OrientedControlledTextGenerationforSocialNetworks.
JournalofSignalProcessingSystems96,2(feb2024),131–151. https://doi.org/10.1007/s11265-023-01907-2
[157] DianYu,ZhouYu,andKenjiSagae.2021. AttributeAlignment:ControllingTextGenerationfromPre-trained
LanguageModels.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2021,Marie-FrancineMoens,
XuanjingHuang,LuciaSpecia,andScottWen-tauYih(Eds.).AssociationforComputationalLinguistics,PuntaCana,
DominicanRepublic,2251–2268. https://doi.org/10.18653/v1/2021.findings-emnlp.194
[158] LantaoYu,WeinanZhang,JunWang,andYongYu.2017. SeqGAN:SequenceGenerativeAdversarialNetswith
PolicyGradient.InProceedingsoftheThirty-FirstAAAIConferenceonArtificialIntelligence.2852–2858. https:
//ojs.aaai.org/index.php/AAAI/article/view/10770
[159] SangwonYu,ChangminLee,HojinLee,andSungrohYoon.2024.ControlledTextGenerationforBlack-boxLanguage
ModelsviaScore-basedProgressiveEditor.InProceedingsofthe62ndAnnualMeetingoftheAssociationforComputa-
tionalLinguistics(Volume1:LongPapers),Lun-WeiKu,AndreMartins,andVivekSrikumar(Eds.).Associationfor
ComputationalLinguistics,Bangkok,Thailand,14215–14237. https://aclanthology.org/2024.acl-long.767
[160] YoelZeldes,DanPadnos,OrSharir,andBarakPeleg.2020.TechnicalReport:AuxiliaryTuninganditsApplicationto
ConditionalTextGeneration. arXiv:2006.16823[cs.CL] https://arxiv.org/abs/2006.16823
[161] QingchengZeng,MingyuJin,QinkaiYu,ZhentingWang,WenyueHua,ZihaoZhou,GuangyanSun,YandaMeng,
ShiqingMa,QifanWang,FelixJuefei-Xu,KaizeDing,FanYang,RuixiangTang,andYongfengZhang.2024.Uncertainty
isFragile:ManipulatingUncertaintyinLargeLanguageModels. arXiv:2407.11282[cs.CL] https://arxiv.org/abs/2407.
11282
[162] WeihaoZeng,LuluZhao,KeqingHe,RuotongGeng,JingangWang,WeiWu,andWeiranXu.2023.SeentoUnseen:
ExploringCompositionalGeneralizationofMulti-AttributeControllableDialogueGeneration.InProceedingsofthe
61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),AnnaRogers,Jordan
Boyd-Graber,andNaoakiOkazaki(Eds.).AssociationforComputationalLinguistics,Toronto,Canada,14179–14196.
https://doi.org/10.18653/v1/2023.acl-long.793
[163] YongchengZeng,GuoqingLiu,WeiyuMa,NingYang,HaifengZhang,andJunWang.2024. Token-levelDirect
PreferenceOptimization. arXiv:2404.11999[cs.CL] https://arxiv.org/abs/2404.11999
[164] YanZengandJian-YunNie.2021.ASimpleandEfficientMulti-TaskLearningApproachforConditionedDialogue
Generation.InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputational
,Vol.1,No.1,Article.Publicationdate:August2024.ControllableTextGenerationforLargeLanguageModels:ASurvey 51
Linguistics:HumanLanguageTechnologies,KristinaToutanova,AnnaRumshisky,LukeZettlemoyer,DilekHakkani-
Tur,IzBeltagy,StevenBethard,RyanCotterell,TanmoyChakraborty,andYichaoZhou(Eds.).Associationfor
ComputationalLinguistics,Online,4927–4939. https://doi.org/10.18653/v1/2021.naacl-main.392
[165] HanqingZhangandDaweiSong.2022.DisCup:DiscriminatorCooperativeUnlikelihoodPrompt-tuningforControl-
lableTextGeneration.InProceedingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,
YoavGoldberg,ZornitsaKozareva,andYueZhang(Eds.).AssociationforComputationalLinguistics,AbuDhabi,
UnitedArabEmirates,3392–3406. https://doi.org/10.18653/v1/2022.emnlp-main.223
[166] HanqingZhang,HaolinSong,ShaoyuLi,MingZhou,andDaweiSong.2023.ASurveyofControllableTextGeneration
UsingTransformer-basedPre-trainedLanguageModels.ACMComput.Surv.56,3,Article64(oct2023),37pages.
https://doi.org/10.1145/3617680
[167] HanqingZhang,SiSun,HaimingWu,andDaweiSong.2024.ControllableTextGenerationwithResidualMemory
Transformer.InFindingsoftheAssociationforComputationalLinguisticsACL2024,Lun-WeiKu,AndreMartins,and
VivekSrikumar(Eds.).AssociationforComputationalLinguistics,Bangkok,Thailandandvirtualmeeting,1048–1066.
https://aclanthology.org/2024.findings-acl.62
[168] JingyuZhang,JamesGlass,andTianxingHe.2023.PCFG-BasedNaturalLanguageInterfaceImprovesGeneralization
forControlledTextGeneration.InProceedingsofthe12thJointConferenceonLexicalandComputationalSemantics
(*SEM2023),AlexisPalmerandJoseCamacho-collados(Eds.).AssociationforComputationalLinguistics,Toronto,
Canada,295–313. https://doi.org/10.18653/v1/2023.starsem-1.27
[169] TianyiZhang,VarshaKishore,FelixWu,KilianQ.Weinberger,andYoavArtzi.2020.BERTScore:EvaluatingText
GenerationwithBERT.In8thInternationalConferenceonLearningRepresentations,ICLR2020,AddisAbaba,Ethiopia,
April26-30,2020.OpenReview.net. https://openreview.net/forum?id=SkeHuCVFDr
[170] XuZhangandXiaojunWan.2023.MIL-Decoding:DetoxifyingLanguageModelsatToken-LevelviaMultipleInstance
Learning.InProceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:Long
Papers),AnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(Eds.).AssociationforComputationalLinguistics,
Toronto,Canada,190–202. https://doi.org/10.18653/v1/2023.acl-long.11
[171] XiangZhang,JunboZhao,andYannLeCun.2015. Character-levelconvolutionalnetworksfortextclassification.
Advancesinneuralinformationprocessingsystems28(2015).
[172] YizheZhang,GuoyinWang,ChunyuanLi,ZheGan,ChrisBrockett,andBillDolan.2020.POINTER:Constrained
ProgressiveTextGenerationviaInsertion-basedGenerativePre-training.InProceedingsofthe2020Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing(EMNLP),BonnieWebber,TrevorCohn,YulanHe,andYang
Liu(Eds.).AssociationforComputationalLinguistics,Online,8649–8670. https://doi.org/10.18653/v1/2020.emnlp-
main.698
[173] ZhilingZhang,MengyueWu,andKennyZhu.2023.SemanticSpaceGroundedWeightedDecodingforMulti-Attribute
ControllableDialogueGeneration.InProceedingsofthe2023ConferenceonEmpiricalMethodsinNaturalLanguage
Processing,HoudaBouamor,JuanPino,andKalikaBali(Eds.).AssociationforComputationalLinguistics,Singapore,
13230–13243. https://doi.org/10.18653/v1/2023.emnlp-main.817
[174] WeiZhao,MaximePeyrard,FeiLiu,YangGao,ChristianM.Meyer,andSteffenEger.2019. MoverScore:Text
GenerationEvaluatingwithContextualizedEmbeddingsandEarthMoverDistance.InProceedingsofthe2019
ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNatural
LanguageProcessing(EMNLP-IJCNLP),KentaroInui,JingJiang,VincentNg,andXiaojunWan(Eds.).Associationfor
ComputationalLinguistics,HongKong,China,563–578. https://doi.org/10.18653/v1/D19-1053
[175] WayneXinZhao,KunZhou,JunyiLi,TianyiTang,XiaoleiWang,YupengHou,YingqianMin,BeichenZhang,
JunjieZhang,ZicanDong,YifanDu,ChenYang,YushuoChen,ZhipengChen,JinhaoJiang,RuiyangRen,Yifan
Li,XinyuTang,ZikangLiu,PeiyuLiu,Jian-YunNie,andJi-RongWen.2023.ASurveyofLargeLanguageModels.
arXiv:2303.18223[cs.CL] https://arxiv.org/abs/2303.18223
[176] ChujieZheng,PeiKe,ZhengZhang,andMinlieHuang.2023.Click:ControllableTextGenerationwithSequence
LikelihoodContrastiveLearning.InFindingsoftheAssociationforComputationalLinguistics:ACL2023,AnnaRogers,
JordanBoyd-Graber,andNaoakiOkazaki(Eds.).AssociationforComputationalLinguistics,Toronto,Canada,1022–
1040. https://doi.org/10.18653/v1/2023.findings-acl.65
[177] CarolinaZheng,ClaudiaShi,KeyonVafa,AmirFeder,andDavidBlei.2023.AnInvariantLearningCharacterizationof
ControlledTextGeneration.InProceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics
(Volume1:LongPapers),AnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(Eds.).AssociationforComputational
Linguistics,Toronto,Canada,3186–3206. https://doi.org/10.18653/v1/2023.acl-long.179
[178] XinZheng,HongyuLin,XianpeiHan,andLeSun.2023.TowardUnifiedControllableTextGenerationviaRegular
ExpressionInstruction.InProceedingsofthe13thInternationalJointConferenceonNaturalLanguageProcessingand
the3rdConferenceoftheAsia-PacificChapteroftheAssociationforComputationalLinguistics(Volume1:LongPapers),
JongC.Park,YukiArase,BaotianHu,WeiLu,DerryWijaya,AyuPurwarianti,andAdilaAlfaKrisnadhi(Eds.).
,Vol.1,No.1,Article.Publicationdate:August2024.52 Liangetal.
AssociationforComputationalLinguistics,NusaDua,Bali,1–14. https://doi.org/10.18653/v1/2023.ijcnlp-main.1
[179] YinheZheng,RongshengZhang,MinlieHuang,andXiaoxiMao.2020.APre-TrainingBasedPersonalizedDialogue
GenerationModelwithPersona-SparseData.ProceedingsoftheAAAIConferenceonArtificialIntelligence34,05(Apr.
2020),9693–9700. https://doi.org/10.1609/aaai.v34i05.6518
[180] QihuangZhong,LiangDing,JuhuaLiu,BoDu,andDachengTao.2024.ROSEDoesn’tDoThat:BoostingtheSafety
ofInstruction-TunedLargeLanguageModelswithReversePromptContrastiveDecoding. arXiv:2402.11889[cs.CL]
https://arxiv.org/abs/2402.11889
[181] TianqiZhong,QuanWang,JingxuanHan,YongdongZhang,andZhendongMao.2023. Air-Decoding:Attribute
DistributionReconstructionforDecoding-TimeControllableTextGeneration.InProceedingsofthe2023Conference
onEmpiricalMethodsinNaturalLanguageProcessing,HoudaBouamor,JuanPino,andKalikaBali(Eds.).Association
forComputationalLinguistics,Singapore,8233–8248. https://doi.org/10.18653/v1/2023.emnlp-main.512
[182] ChuntingZhou,PengfeiLiu,PuxinXu,SriniIyer,JiaoSun,YuningMao,XuezheMa,AviaEfrat,PingYu,LILIYU,
SusanZhang,GargiGhosh,MikeLewis,LukeZettlemoyer,andOmerLevy.2023.LIMA:LessIsMoreforAlignment.In
Thirty-seventhConferenceonNeuralInformationProcessingSystems. https://openreview.net/forum?id=KBMOKmX2he
[183] WangchunshuZhou,YuchenEleanorJiang,EthanWilcox,RyanCotterell,andMrinmayaSachan.2023.Controlled
textgenerationwithnaturallanguageinstructions.InProceedingsofthe40thInternationalConferenceonMachine
Learning(Honolulu,Hawaii,USA)(ICML’23).JMLR.org,Article1795,12pages.
[184] LinanZhu,YifeiXu,ZhechaoZhu,YinweiBao,andXiangjieKong.2023.Fine-GrainedSentiment-ControlledText
GenerationApproachBasedonPre-TrainedLanguageModel.AppliedSciences13,1(2023). https://doi.org/10.3390/
app13010264
[185] YaomingZhu,SidiLu,LeiZheng,JiaxianGuo,WeinanZhang,JunWang,andYongYu.2018.Texygen:Abenchmarking
platformfortextgenerationmodels.InThe41stinternationalACMSIGIRconferenceonresearch&developmentin
informationretrieval.1097–1100.
[186] XuZou,DaYin,QingyangZhong,HongxiaYang,ZhilinYang,andJieTang.2021. ControllableGenerationfrom
Pre-trainedLanguageModelsviaInversePrompting.InProceedingsofthe27thACMSIGKDDConferenceonKnowledge
Discovery&DataMining(VirtualEvent,Singapore)(KDD’21).AssociationforComputingMachinery,NewYork,NY,
USA,2450–2460. https://doi.org/10.1145/3447548.3467418
ReceivedXXXXXXXX;revisedXXXXXXXX;acceptedXXXXXXXX
,Vol.1,No.1,Article.Publicationdate:August2024.