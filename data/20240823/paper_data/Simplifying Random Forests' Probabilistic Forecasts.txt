Simplifying Random Forests’
Probabilistic Forecasts∗
Nils Koster Fabian Krüger
Karlsruhe Institute of Technology, Karlsruhe Institute of Technology
Broad Institute of MIT and Harvard fabian.krueger@kit.edu
nils.koster@kit.edu
August 23, 2024
Abstract
Since their introduction by Breiman (2001), Random Forests (RFs) have proven
to be useful for both classification and regression tasks. The RF prediction of a
previously unseen observation can be represented as a weighted sum of all training
sampleobservations. Thisnearest-neighbor-typerepresentationisuseful,amongother
things, for constructing forecast distributions (Meinshausen, 2006). In this paper, we
consider simplifying RF-based forecast distributions by sparsifying them. That is, we
focus on a small subset of nearest neighbors while setting the remaining weights to
zero. This sparsification step greatly improves the interpretability of RF predictions.
It can be applied to any forecasting task without re-training existing RF models. In
empirical experiments, we document that the simplified predictions can be similar
to or exceed the original ones in terms of forecasting performance. We explore the
statistical sources of this finding via a stylized analytical model of RFs. The model
suggests that simplification is particularly promising if the unknown true forecast
distribution contains many small weights that are estimated imprecisely.
1 Introduction
Many statisticians agree that forecast distributions are preferable to mere point fore-
casts. Correspondingly, an active literature is concerned with making statistical forecast
distributions in meteorology (e.g. Rasp and Lerch, 2018), economics (e.g. Krüger et al.,
2017), energy (e.g. Taieb et al., 2021), epidemiology (e.g. Cramer et al., 2022), and other
fields. Nevertheless, point predictions still dominate in many practical settings in policy,
business, and society. As argued by Raftery (2016), the cognitive load that forecast
distributions impose upon their users may be an important bottleneck impeding their
adoption. Motivated by this possibility, we consider simplifying the forecast distributions
∗We thank conference participants at the International Symposium on Forecasting (Dijon, 2024) as
well as Andreas Eberl for helpful comments.
1
4202
guA
22
]PA.tats[
1v23321.8042:viXraFigure 1: Summary of proposed method. This schematic description summarizes the
method proposed in this paper. First, a standard RF is trained. Second, from the trained
RF, we compute a weight vector for each new test case. Collecting the weight vectors for
multiple test cases results in a matrix, with rows corresponding to test cases and columns
corresponding to training cases. Next, we select the top k (in this case k = 3) weights
for each test case, re-normalize such that the weights again sum up to one and set the
remaining weights to zero. These weights can now be used for prediction, as illustrated
here for the first and fourth test cases.
produced by a popular method called Quantile Regression Forests (QRFs) (Meinshausen,
2006), which extend the well-known Random Forests (RFs) proposed by Breiman (2001),
and study how simplification affects statistical forecast performance. While our main focus
is on probabilistic forecasting, the method we propose can also be used to simplify point
forecasts for the mean. More specifically, we approximate an RF forecast distribution
for a continuous scalar outcome by a discrete distribution with k support points, where
k ∈ {1,2,...,n} is a user-determined parameter and n denotes the number of training
samples. Figure 1 provides a schematic description. Discrete distributions of this type can
effectively be communicated to non-statisticians. See, for example, Abbas and Howard
(2015, Chapter 35) for a decision analysis perspective, and Altig et al. (2022) for a survey
design perspective. To illustrate the representation, consider constructing a forecast
distribution for the sale price of a house, and let k = 3. The discrete distribution’s support
points correspond to three price scenarios (‘low’, ‘medium’ and ‘high’) with associated
probabilities. Larger values of k correspond to a larger number of scenarios. This increases
the complexity of the forecast distribution, but may be beneficial in terms of forecasting
performance, as measured by accuracy measures for distributions (proper scoring rules).
2Our approximation builds upon the fact that RFs can be cast as a nearest-neighbor-like
method (Lin and Jeon, 2006; Meinshausen, 2006): The forecast distribution for a test
sample observation with feature vector x is a discrete distribution with support points
0
given by the training sample outcomes (y )n and corresponding probability weights
i i=1
(w (x ))n . As detailed in Section 2.1, the weight w (x ) reflects the similarity between
i 0 i=1 i 0
x and x , the feature vector of the i-th training sample observations.
0 i
To describe our method formally, consider a given test case x and the set I (x )
0 k 0
containing the indices of the k largest weights for this test case, where k ∈ {1,2,...,n}.
We then set
w˜ i(x 0) =
(cid:40) (cid:80) j∈Ikw (i x( 0x )0 w) j(x0) if i ∈ I k(x 0)
0 else
That is, we retain only the k << n largest weights, and re-scale them such that they
sum to one. All other weights are set to zero. Setting k = n recovers the weights of the
initial RF’s mean and the QRF’s distribution forecast, which is described in more detail
in Section 2.1 To illustrate, consider a toy example with k = 3, n = 10 and weights as
given in Table 1. In this example, the weights in the index set I (x ) = {5,7,9} sum to
3 0
0.75 = 3/4. Hence the transformed weights w˜ (x ) are given by w (x )·4/3 if i ∈ I (x ),
i 0 i 0 3 0
and by w (x ) = 0 otherwise.
i 0
Table 1: Toy example with n = 10 training sample cases, original weights (w (x ),
i 0
second row) and transformed weights (w˜ (x ), third row). Numbers in third row are
i 0
rounded to two digits; empty cells correspond to zero weights.
i 1 2 3 4 5 6 7 8 9 10
w (x ) 0.03 0.02 0.10 0.04 0.21 0.01 0.32 0.04 0.22 0.01
i 0
w˜ (x ) 0.28 0.43 0.29
i 0
WeapproachtheinterpretabilityofaRFfromadifferentanglecomparedtomostother
existingliteratureonthetopic: InBreiman’soriginalworkonRFs,featurepermutationwas
introduced, effectively quantifying the influence of each feature on the overall performance
of the model. This method is still widely used and implemented in standard software
packages (Pedregosa et al., 2011). Biau and Scornet (2016, Section 5) provide further
discussion. This perspective on interpretation is also commonly used for other forecasting
models, such as neural networks, e.g. Lundberg et al. (2017). Visualizations of RFs have
also been proposed (Zhao et al., 2019; Haddouchi and Berrado, 2019). In contrast to
approaches based on variable importance, the simplification we consider does not address
the relation between the RF and the features. Instead, we seek to represent the RF
forecast distribution via a small number of support points, which can be interpreted as
‘scenarios’. Moreover, we can compare the likely scenarios’ features to the test point’s
features in order to validate the forecast.
Many extensions and modifications of Breiman’s original RF have been proposed in the
literature, typically with the aim of improving statistical performance (either empirically,
1Prediction as in Equations 4 and 5 is unaltered, except for the substitution of w (x ) by w˜ (x ).
i 0 i 0
3or in terms of theoretical properties). See Biau and Scornet (2016) for a survey, and Beck
et al. (2023) and Cevid et al. (2022) for illustrative recent contributions. By contrast, we
focus on a standard RF implementation, and post-process its forecast distribution in a
way that makes it easier to interpret. The procedure we propose can easily be applied to
other RF variants (or even to prediction methods other than RFs), provided that they
can be represented as discrete distributions of the type described above.
We study the performance of our ‘Topk’ simplification in a series of empirical ex-
periments based on 18 data sets for which RFs have been found to perform well, as
compared to deep neural network models (Grinsztajn et al., 2022). Our findings indicate
that already for k = {5,10}, the simplified RFs can perform on a similar level compared
to the full counterpart for both probabilistic and point forecasts, depending on the data
set. Considering probabilistic forecasts, for k = 20, the median performance across all
datasets is equivalent to the full RFs and considering k = 50 even increases the median
performance slightly. For point (mean) forecasts, k = 20 even increases performance
slightly. Even though very sparse choices like k ∈ {3,5} may come with a significant
performance decrease, we argue that they may be worthwhile if ease of communication
is a main concern. We further show that our results are qualitatively robust to different
hyperparameter choices.
In order to rationalize the empirical results, we then consider a detailed analytical
example that models the weights estimatedby RFs as a randomdraw from a Dirichlet-type
distribution. The example also features a true vector of weights that may be either similar
to, or different from, the estimated weights. This setup is useful to study how a simplifying
approximation similar to Topk affects statistical forecasting performance. In a nutshell,
the amount of noise in the estimated weights determines whether or not the simplification
comes at a high cost in terms of performance. Perhaps surprisingly, one can construct
examples in which simplification improves performance: this result arises when the largest
weights are estimated precisely, whereas smaller weights are more noisy. Focusing on the
largest weights then constitutes a beneficial form of shrinkage. When the small weights
are estimated precisely, however, simplification is harmful in terms of performance.
The remainder of this paper is structured as follows: Section 2 introduces our method-
ological setup, including RFs and methods for evaluating forecast distributions. Section 3
presentsempiricalexperiments, whereasSection4coversthestylizedanalyticalmodel. Sec-
tion 5 concludes. The appendix contains further empirical results and detailed derivations
for Section 4.
2 Methodological Setup
In this section, we describe RF based forecasting as well as the forecast evaluation methods
we consider.
2.1 Forecasting Methods
Here we describe Random Forests and their probabilistic cousins, Quantile Regression
Forests. We follow Lin and Jeon (2006) and Meinshausen (2006) who emphasize the
4perspective of RF predictions as a weighted sum over training observations. We refer to
Hastie et al. (2009) for a textbook presentation of RFs.
Our goal is to fit a univariate forecasting model. That is, we have some data set
D = (x ,y )n of training set size n, where x is a p-dimensional vector of features, and
i i i=1 i
y ∈ R is a real-valued outcome. We use this data set to train our model fˆ, such that
i
some choice of loss function L is minimized:
n
n
1 (cid:88)
minL (fˆ) = L(fˆ(x ),y )
n i i
fˆ n
i
In a typical regression context, the function fˆ: Rp → R maps x to the real line in order
i
to estimate the conditional expectation functional. A popular loss function is given by
squared error, with L(z,y) = (y−z)2. Let further B ⊆ Rp denote the feature space, i.e.,
the space in which the individual input samples x exist.
i
Random Forests An RF is an ensemble of individual regression trees, each denoted
by T(ξ), where ξ describes the configuration of the tree. At each node, a single tree
greedily splits B and rectangular subspaces thereof into two further rectangular subspaces,
such that the loss L is minimized. Each resulting subspace corresponds to a leaf
n
R ⊆ B, l = 1,...,L where L is the total number of leaves. Each sample x can only
l i
occur in one leaf or, put differently, when dropping a sample x down the tree, it can only
i
fall into one leaf. This leaf is denoted ℓ(x ,ξ) ∈ {1,...,L} for tree T(ξ). For a single
i
tree T(ξ), a prediction µˆ (x ) for a new sample x is obtained by taking the mean of all
T 0 0
training samples within leaf ℓ(x ,ξ). This can be expressed as
0
n
(cid:88)
µˆ (x ) = w (x ,ξ)y , (1)
T 0 i 0 i
i=1
where the weight w (x ,ξ) is equal to zero for all training samples i that fall into leaves
i 0
other than ℓ(x ,ξ), and is equal to one over the leaf size for all training samples that fall
0
into ℓ(x ,ξ):
0
1{x ∈ R }
w (x ,ξ) = i ℓ(x0,ξ) . (2)
i 0 (cid:80)n 1{x ∈ R }
j=1 j ℓ(x0,ξ)
Motivated by the lack of stability and tendency to overfit of individual trees, RFs build B
trees(T(ξ ))B , basedonB bootstrapsamplesofD, andconsidertheiraverageprediction.
b b=1
Moreover, ineachsplitwithineachtree, itiscommontoconsideronlyarandomsubsample
of p˜ out of p regressors. This step aims to diversify the ensemble of trees by avoiding
√
excessive use of the same regressors for splitting. Common choices for p˜are ⌊ p⌋ or ⌊p⌋
3
(Probst et al., 2019), where ⌊z⌋ floors the real number z to the nearest integer. The RF
mean prediction can thus be expressed as
B n
1 (cid:88)(cid:88)
µˆ (x ) = w (x ,ξ )y
RF 0 i 0 b i
B
b=1 i=1
n
(cid:88)
= w (x )y , (3)
i 0 i
i=1
5where
B
1 (cid:88)
w (x ) = w (x ,ξ ) (4)
i 0 i 0 b
B
b=1
is the weight for training sample i, averaged across all B trees. By construction, the
weights (w (x ))n are non-negative and sum to one. Thus, w (x ) be interpreted as the
i 0 i=1 i 0
empirically estimated probability that the new test sample observation is equal to y .
i
Quantile Regression Forest Conceptually, µˆ (x ) is an estimate of the conditional
RF 0
mean E[Y|X = x ]. As described above, it is obtained as a weighted sum over all training
0
observations. Meinshausen (2006) extends this framework to estimating the cumulative
distribution function (CDF) of Y, which is given by E[1(Y ≤ t)|X = x ] = P(Y ≤ t|X =
0
x ), where t ∈ R is a threshold value. The similarity to RFs becomes apparent in the last
0
expression. Utilizing the weights from Equation 4, one can approximate the CDF by the
weighted mean over the binary observations 1(y ≤ t):
i
n
(cid:88)
P (cid:98)(Y ≤ t|X = x 0) = w i(x 0)1(y
i
≤ t). (5)
i=1
That is, QRFs estimate the CDF of Y via the weighted empirical CDF of the training
sample outcomes (y )n , using the weights (w (x ))n produced by RFs. This estimator
i i=1 i 0 i=1
ispracticallyappealingasitarisesasabyproductofstandardRFsoftwareimplementation.
Furthermore, its representation in terms of a weighted empirical CDF enables a theoretical
understanding of its properties by leveraging tools from nonparametric statistics (Lin and
Jeon, 2006; Meinshausen, 2006). In this paper, we consider the standard variant of QRFs
which uses squared error as a criterion for finding splits (and thus growing the forest’s
individual trees). Various other splitting criteria have been analyzed in the literature. In
particular, Cevid et al. (2022) propose to use a splitting criterion based on distibutional
similarity. Since their RF variant retains the weighted empirical CDF representation (see
their Section 2.2), our Topk method can be applied to it as well.
2.2 Forecast Evaluation
Since we generate probabilistic forecasts, we need a tool to evaluate them. For this, we use
the Continuous Ranked Probability Score (CRPS), a strictly proper scoring rule. Scoring
rules are loss functions for probabilistic forecasts. We use them in negative orientation, so
that smaller scores indicate better forecasts. When evaluated using a proper scoring rule,
a forecaster minimizes their expected score by stating what they think is the true forecast
distribution. Under a strictly proper scoring rule, this minimum is unique within a suitable
classofforecastdistributions. Conceptually, strictlyproperscoringrulesincentivizecareful
and honest forecasting. See Gneiting and Raftery (2007) for a comprehensive technical
treatment, and Winkler (1996) and Gneiting and Katzfuss (2014) for further discussion
and illustration. The CRPS (Matheson and Winkler, 1976) is defined as
(cid:90) ∞ (cid:16) (cid:17)2
CRPS(Fˆ,y) = Fˆ(z)−1{z ≥ y} dz (6)
−∞
6where Fˆ denotes the CDF implied by the forecast distribution and y denotes the true
outcome. For various forms of Fˆ(z), closed-form expressions of the CRPS are available, so
that there is no need for costly numerical evaluation of the integral in Equation 6.
The CRPS allows for very general types of forecast distributions Fˆ. In particular, the
forecast distribution may be discrete, that is, it need not possess a density. This allows
for evaluating forecast distributions based on (weighted) empirical CDFs, which arise in
the case of QRFs. In the special case that the forecast distribution is deterministic, i.e., it
places point mass on a single outcome, the CRPS reduces to the Absolute Error (AE).
Thus, numerical values of the AE and CRPS can meaningfully be compared to each other.
Jordan et al. (2019) provide an efficient implementation of the CRPS for weighted
empirical distributions in their R-package scoringRules. Let (y )n denote the response
i i=1
values from the training data, and denote by y their ith ordered value, with y ≤
(i) (1)
y ≤ ... ≤ y . Furthermore, let w denote the weight corresponding to y . Then the
(2) (n) (i) (i)
CRPS for a realization y ∈ R is given by
   
n i
CRPS(Fˆ,y) = 2(cid:88) w (i)(y (i)−y)1{y < y (i)}−(cid:88) w (j)+ w (i) , (7)
2
i=1 j=1
where we dropped the dependence of w on a vector x of covariates at this point for
(i) 0
ease of notation. Equation 7 extends Jordan et al.’s Equation 3 to the case of non-equal
weights, based on their implementation in the function crps_sample. In the case of sparse
weights, one may omit the indices i with w = 0 from the sum at (7) in order to speed
(i)
up the computation.
Additionally to the CRPS, we also report results for the squared error (SE). In the
present context, the SE is given by
n
(cid:88)
SE(Fˆ,y) = (y− w y )2. (8)
i i
i=1
Hence, the SE depends on the forecast distribution Fˆ via its mean (cid:80)n w y only.
i=1 i i
3 Experimental Results
In order to assess the statistical performance of the simplified forecast distributions, we
conduct experiments on 18 data sets considered by Grinsztajn et al. (2022) in the context
of numerical regression. The authors demonstrate that tree-based methods compare
favorably to neural networks for these data sets. Their selection of data sets aims to
represent real-world, ‘clean’ data sets with medium size as well as heterogeneous data
types and fields of applications. If deemed necessary, some basic preprocessing was applied
by Grinsztajn et al. (2022). Details can be found in Section 3.5 and Appendix A.1 in their
paper, and in Table 5 of Appendix A below. The data set delays_zurich_transport
contains about 5.6 million data points in its original form. For computational reasons, we
reduced the size of this dataset through random subsampling, to approximately 1.1 million
7data points (20% of the original observations). We did not apply further preprocessing of
any of the data sets in order to retain comparability. We allocate 70% of each data set for
training our models and reserve the remaining 30% for testing. For each data set, we train
a RF, and then evaluate its performance by computing the average CRPS and SE, as
introduced in Equations 7 and 8 over the test data set. Our main interest is in studying
the impact of the parameter k which governs the number of support points of the sparsified
forecast distribution. We consider a grid of choices k = 1,...,50 and denote these sparse
RFs as ‘Topk’. Our standard choice of RF hyperparameters is a combination of default
values of the machine learning software packages scikit-learn (Pedregosa et al., 2011)
and ranger (Wright and Ziegler, 2017) which are popular choices in the Python (van
Rossum et al., 2011) and R (R Core Team, 2022) programming languages, respectively. In
√
summary, we consider a random selection of p out of p possible features at each split
point, do not impose any form of regularization in terms of tree growth restriction, and
set the number of trees to 1000 in order to obtain a large and stable ensemble. These
choices, which are also listed in the first row (entitled ‘standard’) of Table 7 in Appendix
A, are used for the analysis in Sections 3.1 and 3.2. In Section 3.3, we further consider
the effects of tuning hyperparameters.
3.1 Probabilistic Forecasts
Table 2 presents the CRPS for the full RF and the relative CRPS of Topk, for k ∈
{3,5,10,20,50}, compared to the full RF. For example, a relative CRPS of 1.5 indicates
a 50% larger CRPS for the Topk version compared to the full RF. The three smallest
values for k seem especially attractive in terms of simplicity and ease of communication.
While the two larger choices k ∈ {20,50} are less attractive in terms of simplicity, we also
consider them in order to assess the trade-off between simplicity and performance.
Using only k = 3 support points performs worse than the full RF, with a median per-
formance cost of 25% across data sets. While this result is unsurprising from a qualitative
perspective, its magnitude is interesting, and gives a first indication of the performance
cost of using a rather drastic simplification of the original forecast distribution. For
each single data set, we find that the performance of Topk improves monotonically when
increasing k from 3 to 5, from 5 to 10, and for all data sets but one when increasing k
from 10 to 20. This pattern is plausible, given that we move from a drastic simplification
(k = 3) to less drastic versions. Compared to the full RF, Top5 implies a median loss
increase of 14%, whereas Top10 yields a median loss increase of 5%. Top20 performs
equally well as the full RF in the median. For most data sets, Top50 slightly enhances
predictive performance compared to the full RF. Whether k = 50 support points remain
worth interpreting depends on the application at hand. Only for a few data sets, Top50 is
not sufficient to reach the performance of the full RF, but performance costs are small
even for these data sets.
In order to contextualize the magnitude of our presented results (such as Top3’s
median CRPS increase of 25% compared to the full RF), we next present results on two
8Table 2: Results for forecast distributions. The table reports the CRPS of the full RF
as well as the CRPS for Top{3,5,10,20,50} relative to the full RF, i.e., CRPS Topk/CRPS Full.
A value smaller than 1 means that Topk outperforms the full RF. The last row lists the
median relative CRPS across data sets.
Absolute CRPS CRPS relative to Full
Data set Full Top3 Top5 Top10 Top20 Top50
cpu_act 1.2508 1.21 1.08 1.00 0.96 0.95
pol 1.4203 1.50 1.28 1.10 1.01 0.96
elevators 0.0014 1.22 1.10 1.01 0.96 0.95
wine_quality 0.2565 1.35 1.20 1.09 1.02 0.99
Ailerons 0.0001 1.23 1.11 1.01 0.97 0.96
houses 0.1229 1.19 1.08 0.99 0.96 0.95
house_16H 0.1984 1.32 1.17 1.06 1.01 0.98
diamonds 0.1320 1.34 1.21 1.11 1.05 1.01
Brazilian_houses 0.0256 0.88 0.82 0.78 0.79 0.85
Bike_Sharing_Demand 46.0292 1.26 1.15 1.06 1.02 1.00
nyc-taxi-green-dec-2016 0.1505 1.39 1.24 1.12 1.06 1.02
house_sales 0.0995 1.25 1.14 1.04 0.99 0.97
sulfur 0.0123 1.05 0.97 0.94 0.95 0.97
medical_charges 0.0376 1.35 1.21 1.11 1.05 1.01
MiamiHousing2016 0.0778 1.20 1.10 1.02 0.98 0.97
superconduct 3.6341 1.14 1.08 1.02 1.00 0.99
yprop_4_1 0.0140 1.38 1.25 1.14 1.08 1.03
delays_zurich_transport 1.6174 1.33 1.19 1.10 1.05 1.02
Median - 1.25 1.14 1.05 1.00 0.98
simple benchmark methods.2 First, we consider a deterministic point forecast which
assumes that the full RF’s median forecast materializes with probability one.3 This is a
very optimistic point (rather than probabilistic) forecast, containing no uncertainty. As
noted earlier, its CRPS is the same as its AE. We consider this benchmark in order to
quantify the costs of ignoring uncertainty altogether. We clearly expect these costs to be
positive, i.e., we expect the point forecast to perform worse than the full RF. Second, we
use the CRPS of the unconditional empirical distribution of the response variable in the
training sample. This distribution places a uniform weight of 1/n on each training sample
observation, in contrast to the QRF weights w(x ) that depend on the feature vector x .
0 0
The unconditional distribution is a very conservative forecast, as no information about
the features is used whatsoever. Qualitatively, we clearly expect this forecast to perform
worse than the full RF. Quantitatively, the difference in performance of the unconditional
versus conditional forecast distributions captures the predictive content of the features
2WerefertoGrinsztajnetal.(2022)fordetailedcomparisonsofRFpointforecaststoothertree-based
models and neural networks.
3Formally, this forecast is characterized by the CDF Fˆ (z)=1(z ≥med(Fˆ (z)), where med(Fˆ )
δ Full full
denotes the median implied by the CDF of the full RF’s forecast distribution. That is, the CDF Fˆ is
δ
a step function with a single jump point at the median forecast of the full RF. We choose the median
functional here because the latter is the optimal point forecast under absolute error loss, to which the
CRPS reduces in the case of a deterministic forecast.
915.0
10.0
5.0 Point Forecast
Unconditional
Top3
2.0
1.5
1.0
Dataset
Figure 2: Benchmarking performance. Blue bars: Relative CRPS of a deterministic
point forecast (median) as a benchmark that ignores uncertainty. Ochre bars: Relative
CRPS of the unconditional forecast distribution, yielding a conservative benchmark that
ignores features. Green bars: Relative CRPS of Top3, as listed in Table 2. In each case,
relative CRPS results are compared to the full RF. That is, a relative CRPS smaller than
1 (represented by horizontal line) indicates that the method performs better than the full
RF.
(see e.g. Gneiting and Resin, 2023, Section 2.5). Both benchmarks are visualized jointly
with Top3 results in Figure 2.
As expected, the relative CRPS of the point forecast (shown in blue) exceeds one for
most data sets, indicating that it is generally inferior to the full RF model. Notably, an
exception is observed for Brazilian_houses, where the point forecast’s relative CRPS is
smaller than one. This result appears to be due to prediction uncertainty being very small
for this data set; see Figure 13 in the supplemental material for Grinsztajn et al. (2022).
Indeed, for this data set, the response variable seems to mostly be a linear combination of
a subset of the features. Furthermore, Top3 (shown in green) performs similar to or better
than the point forecast for all data sets, demonstrating the usefulness of incorporating
additional uncertainty in the forecast. Note that the point forecast has access to the full
RF forecast distribution, using the median of this distribution as a point forecast. By
contrast, Top3 only has access to the three most important support points of the RF
forecast distribution.
The relative CRPS of the unconditional forecast distribution (displayed in ochre in
Figure 2) exceeds two for most data sets, indicating that the features are generally very
10
SPRC
evitaleR
tca
upc
lop srotavele ...lauq
eniw
snoreliA sesuoh H61
esuoh
sdnomaid ...nailizarB ...rahS
ekiB
...-ixat-cyn ...las
esuoh
ruflus ...c
lacidem
...suoHimaiM ...dnocrepus 1
4
porpy
...uz
syaleduseful for prediction. An exception occurs for the last two data sets (yprop_4_1 and
delays_zurich_transport). In these cases, the RF appears unable to learn meaningful
connections between the features and the target variable. Figure 13 in the online supple-
ment of Grinsztajn et al. (2022) supports this interpretation, reporting low predictability
(in terms of low out-of-sample R2) for these data sets. In this situation, we cannot
expect a Topk-model to perform well compared to the full RF. To see this, consider the
stylized case of the features being entirely uninformative. Subsequently, the unconditional
distribution (placing a weight of 1/n on all training sample responses) is the best possible
forecast, which is in sharp contrast to Topk (for which k weights are non-zero and large
by construction, whereas the remaining n−k weights are forced to zero).
Table 3: Topk weight sums. Average sum of un-normalized Top{3,5,10,20,50} weights.
The last column reports the number of observations in the training set.
Average Sum
Data set Top3 Top5 Top10 Top20 Top50 n
cpu_act 0.094 0.135 0.212 0.318 0.504 5734
pol 0.073 0.104 0.159 0.233 0.359 10500
elevators 0.119 0.166 0.252 0.366 0.555 11619
wine_quality 0.150 0.189 0.258 0.350 0.513 4547
Ailerons 0.119 0.168 0.257 0.374 0.566 9625
houses 0.143 0.201 0.304 0.435 0.634 14447
house_16H 0.071 0.102 0.162 0.247 0.402 15948
diamonds 0.257 0.348 0.494 0.656 0.851 37758
Brazilian_houses 0.202 0.278 0.406 0.558 0.763 7484
Bike_Sharing_Demand 0.248 0.334 0.473 0.628 0.821 12165
nyc-taxi-green-dec-2016 0.172 0.234 0.337 0.461 0.642 407284
house_sales 0.116 0.160 0.240 0.345 0.522 15129
sulfur 0.212 0.296 0.438 0.602 0.811 7056
medical_charges 0.223 0.304 0.438 0.590 0.789 114145
MiamiHousing2016 0.196 0.267 0.385 0.520 0.704 9752
superconduct 0.390 0.497 0.617 0.708 0.805 14884
yprop_4_1 0.111 0.149 0.216 0.304 0.456 6219
delays_zurich_transport 0.015 0.024 0.048 0.095 0.230 765180
In order to further study the properties of Topk forecast distributions, Table 3 reports
the average weight sums across test cases, for different values of k. The weight sums are
computed before our normalization step (see Section 1) which re-scales all weight sums
to one. For a given choice of k, the unnormalized weight sums can vary in magnitude,
both across data sets and from test case to test case. By construction, the weight sums
increase with k. Interestingly, many data sets yield a large weight sum for Top3, exceeding
10% for all but four data sets. For Top50, the average weight sum exceeds 50% for most
data sets. These numbers are remarkable, given that the data sets include thousands of
training samples (n, see rightmost column of Table 3) that could potentially be used as
11support points for the RF forecast distributions. If the weights were uniform, we would
hence observe Topk weight sums of k/n. This is in sharp contrast to our empirical finding
that a few large weights dominate for most data sets. Lin and Jeon (2006) find similar
results in their work, where they consider RFs as adaptive nearest-neighbor methods
and investigate the influence of the minimum number of samples per leaf. Figures 1c,d
and 5 show few large weights for synthetic data sets. The presence of a small number
of important weights explains why the simplification pursued by Topk often results in
modest (if any) performance costs as compared to the full RF.
3.2 Mean Forecasts
Let us turn our attention towards conditional mean forecasts, for which results in terms
of squared error are shown in Table 4. We notice a similar pattern as in the probabilistic
scenario: apart from Top3 outperforming the full RF for one data set (sulfur), Top3
performs worse than the full RF for the remaining data sets, with a maximum performance
cost of 61% for pol. This results in a median SE increase of 22% for Top3. Top5 still
Table 4: Results for conditional mean forecasts. The table reports the SE of the full
RF as well as the SE for Top{3,5,10,20,50} relative to the full RF, i.e., SE Topk/SE Full. The
last row lists the median relative SE for each choice of k across data sets.
Absolute SE SE relative to Full
Data set Full Top3 Top5 Top10 Top20 Top50
cpu_act 6.5359 1.11 0.99 0.93 0.91 0.91
pol 38.5033 1.61 1.37 1.16 1.03 0.95
elevators 9.22e-6 1.09 0.98 0.89 0.85 0.86
wine_quality 0.3485 1.42 1.24 1.11 1.03 1.00
Ailerons 3.22e-8 1.15 1.02 0.94 0.91 0.91
houses 0.0590 1.16 1.03 0.94 0.91 0.92
house_16H 0.3011 1.49 1.31 1.15 1.06 1.00
diamonds 0.0563 1.37 1.24 1.12 1.06 1.02
Brazilian_houses 0.0072 1.05 1.12 0.99 0.94 0.94
Bike_Sharing_Demand 10126.7983 1.22 1.11 1.03 0.99 0.99
nyc-taxi-green-dec-2016 0.1528 1.38 1.23 1.12 1.05 1.02
house_sales 0.0382 1.22 1.11 1.02 0.96 0.94
sulfur 0.0015 0.69 0.69 0.73 0.81 0.90
medical_charges 0.0073 1.36 1.22 1.11 1.05 1.01
MiamiHousing2016 0.0243 1.15 1.05 0.97 0.94 0.94
superconduct 87.2592 1.05 1.02 0.97 0.95 0.95
yprop_4_1 0.0010 1.26 1.16 1.10 1.05 1.02
delays_zurich_transport 9.3282 1.33 1.19 1.10 1.05 1.02
Median - 1.22 1.12 1.02 0.98 0.95
shows a 12% median increase and for Top10, the median performance almost matches
the full RF’s performance. Top20 and Top50 even outperform the full RF, yielding
median improvements of 2% and 5%, respectively. Compared to the results for forecast
distributions (Table 2), the results in Table 4 indicate that the performance costs of
12simplicity are comparatively lower in the case of mean forecasts, with median relative
losses being somewhat smaller for a given value of k.
3.3 Varying Hyperparameters
Compared to other modeling algorithms, RFs have relatively few hyperparameters. Never-
theless, tuning its hyperparameters can improve the performance of RFs (Probst et al.,
2019). The most important hyperparameters control the depth of each individual tree, as
well as the number of randomly selected regressors considered for splitting. The depth of
a tree can be restricted directly (‘maximum depth’) or indirectly by restricting leaf and
split sizes (‘minimum leaf size’ and ‘minimum split size’, respectively). The number of
regressors considered for splitting is often denoted as ‘max features’ or ‘mtry’ (the latter
term is used, e.g. in the R packages randomForest (Liaw and Wiener, 2002) and ranger
(Wright and Ziegler, 2017)). In what follows, we study how different hyperparameter sets
influence the Topk prediction and whether a hyperparameter set that optimizes the full
RF is also beneficial for Topk. We therefore investigate the influence of ‘max features’
and one of the depth-regularizing hyperparameters, ‘minimum leaf size’, on the Topk
approach. To do so, we consider a grid search for both, the full RF and Top3, with 5-fold
cross-validation on the training set of each data set. Due to the size of medical_charges,
nyc-taxi-green-dec-2016anddelays_zurich_transport,weuseavalidationsetwhich
contains 25% of the training set instead. Further, the latter two are down-sampled to
30% and 15% of their original training set size, respectively. For brevity, our analysis of
hyperparameter tuning focusses on the case k = 3, which is the most drastic simplifcation
we consider.
Figure 3 visualizes the CRPS of Top3 relative to the full RF’s CRPS for three different
hyperparameter sets. In each case, we consider the same hyperparameter set for Top3 and
the full RF. The green bars show the results with standard hyperparameters, as listed in
Table2. Theredbarsindicatetherelativeperformancewiththerespectivehyperparameter
set that optimizes the full RF, while the blue bars visualize the relative performance with
the hyperparameters that optimize the CRPS of Top3. When hyperparameters are tuned
on the full RF, Top3 performance tends to slightly decrease overall. Across the datasets,
the median relative CRPS is 1.31, compared to 1.25 for the standard setting. Conversely,
if hyperparameters are tuned on Top3, the relative performance of Top3 is mostly better
than in the standard version, reaching a median relative CRPS of 1.20.
Figure 4 presents results for point forecasting performance, which are similar to the
probabilistic case. Tuning on the full RF hurts Top3, with a median relative SE of 1.32,
compared to 1.22 in the standard case. By contrast, tuning on Top3 benefits Top3, with a
median relative SE of 1.13. In a small minority of cases, tuning is not beneficial in terms
of the relative loss.
Hence, despite its simplicity, Top3 can perform similar to the full RF given suitable
hyperparameter choices. Overall, the relative performance of Top3 obtained under the
standard setting is between the performance obtained under the two other hyperparameter
settings. Furthermore, the full RF and Top3 require different sets of hyperparameters in
order to perform well. Ideally, users of the Topk method should thus choose hyperparam-
131.5
Untuned
1.0
Tuned on Full
Tuned on Top3
0.5
0.0
Dataset
Figure 3: Hyperparameter tuning performance (probabilistic forecasts). The
figure shows the relative CRPS of Top3 compared to the full RF, for different hyperpa-
rameter settings. Green bars: standard hyperparameters, as listed in Table 2. Red bars:
hyperparameters that optimize the full RF. Blue bars: hyperparameters that optimize
Top3. Hyperparameter tuning is based on CRPS.
eters based on the performance of Topk itself, rather than the performance of the full
RF.
4 Stylized Analytical Model
In this section, we construct a stylized analytical framework which helps explain our
experimental findings presented in Section 3: For many data sets, simplified RFs perform
similar to full RFs even for relatively small choices of k. This finding, and especially the
possibilitythatsimplifiedRFsmayevenoutperformfullRFs, deservesfurtherinvestigation.
Motivated by the structure of RF forecast distributions (see Section 2), we consider a
model in which the true forecast distribution is discrete with support points u = (u )n
i i=1
and corresponding (true) probabilities ω∗ = (ω∗)n that are positive and sum to one.
i i=1
Specifically, we let
(cid:40)
θ∗/k if i ∈ I
ω∗ = (9)
i (1−θ∗)/(n−k) if i ∈/ I,
where I ⊆ {1,2,...,n} is a subset of ‘important’ indexes with |I| = k. The corresponding
‘important’ probabilities (ω∗) sum to θ∗ ∈ [0,1], whereas the other, ‘unimportant’,
i i∈I
14
SPRC
evitaleR
tca
upc
lop srotavele ...lauq
eniw
snoreliA sesuoh H61
esuoh
sdnomaid ...nailizarB ...rahS
ekiB
...-ixat-cyn ...las
esuoh
ruflus ...c
lacidem
...suoHimaiM ...dnocrepus 1
4
porpy
...uz
syaled4
3
Untuned
Tuned on Full
2
Tuned on Top3
1
0
Dataset
Figure 4: Hyperparameter tuning performance (point forecasts). The figure shows
the relative SE of Top3 compared to the full RF, for different hyperparameter settings.
Green bars: standard hyperparameters, as listed in Table 2. Red bars: hyperparameters
that optimize the full RF. Blue bars: hyperparameters that optimize Top3. Hyperparame-
ter tuning is based on SE.
probabilities sum to 1−θ∗. To justify the notion of ‘important’ probabilities, we will
focus on choices of θ∗ and k that satisfy θ∗/k > (1−θ∗)/(n−k).
In addition to the true forecast distribution just described, we consider an estimated
forecastdistributionthatusesthesamesupportpointsu,togetherwithpossiblyincorrectly
estimated probabilities ω = (ω )n . We assume that the estimated probabilities can be
i i=1
described by the following model:
(cid:40)
θ Z if i ∈ I
ω = 1,i (10)
i (1−θ) Z if i ∈/ I,
2,i
where θ ∈ [0,1], Z is a draw from a Dirichlet distribution with k-dimensional parameter
1
vector (d ,...,d ), with d > 0, such that each element of Z has expected value 1/k and
1 1 1 1
variance (k−1)/(k2(kd +1)). Similarly, Z is a draw from another, independent Dirichlet
1 2
distribution with (n−k)-dimensional parameter vector (d ,...,d ), where d > 0. This
2 2 2
means that the expected probabilities are given by
(cid:40)
θ/k if i ∈ I
E[ω ] = (11)
i (1−θ)/(n−k) if i ∈/ I.
15
ES
evitaleR
tca
upc
lop srotavele ...lauq
eniw
snoreliA sesuoh H61
esuoh
sdnomaid ...nailizarB ...rahS
ekiB
...-ixat-cyn ...las
esuoh
ruflus ...c
lacidem
...suoHimaiM ...dnocrepus 1
4
porpy
...uz
syaledθ=0.8, d =d =1 θ=0.4, d =d =10
1 2 1 2
0.5 0.5
0.4 0.4
0.3 0.3
0.2 0.2
0.1 0.1
0.0 0.0
1 3 5 7 9 11 13 15 17 19 1 3 5 7 9 11 13 15 17 19
i i
Figure 5: Simulated probability estimates. In both panels, we set n = 20,k = 5, and
θ∗ = 0.8. The other parameters (θ,d and d ) are as indicated in the header. Horizontal
1 2
segments represent true probabilities, dots represent estimated probabilities.
In the following, we assume that 2 ≤ k ≤ n−2, which ensures that there are at least two
‘important’ and ‘unimportant’ support points, respectively. This restriction ensures that
weight estimation within both sets is a non-trivial problem.4
Thus, if θ ̸= θ∗, the forecast model’s expected probabilities differ from the true
ones in Equation 9. The parameters d and d represent the precision of the forecast
1 2
model’s probabilities around their expected values. Small values for d ,d indicate noisy
1 2
probabilities, whereas large values for d ,d correspond to probabilities close to their
1 2
expected values. This is a property of the variance of Dirichlet-distributed random
variables as noted above for the case of Z . Conceptually, the above model provides a
1
stylized probabilistic description of the estimated probabilities ω produced by a forecasting
method like RFs. Thereby, the model does not aim to specify the mechanism by which
the forecasting method generates these probabilities.
Figure 5 illustrates this setup, with n = 20 and the important indexes given by
I = {1,2,3,4,5}. The left panel shows a situation in which the estimated probabilities
are quite noisy (d = d = 1) but are correct in expectation (θ = θ∗). In the right panel,
1 2
the estimated probabilities are less noisy (d = d = 10) but are false in expectation
1 2
(θ = 0.4 ̸= 0.8 = θ∗).
In the special case that θ = θ∗,d → ∞,d → ∞, the forecast model coincides with
1 2
the true model. Furthermore, in the case θ = 1, the forecast model is very similar to the
‘Topk’ strategy (retaining the k most important probabilities, rescaling them to sum to
one, and setting all other probabilities to zero). The somewhat subtle difference between
the analytical model and our practical implementation of Topk is that the indexes of the
important weights are fixed in the analytical model (given by the set I), whereas they
are chosen as the k largest empirical weights in practice. Exact modeling of our practical
procedure would seem to complicate the analysis substantially without necessarily yielding
4If there was only one important support point, for example, the probability of this support point
would necessarily be equal to θ, rendering weight estimation trivial.
16
ω i ω ifurther insights. While stylized, the analytical model described above is flexible enough to
cover various situations of applied interest. For example, the relation between ‘important’
versus ‘unimportant’ values of the true probabilities can be governed flexibly via the
parameters n,k and θ∗. While we assume that the set I of important indexes is known to
the forecast model, the possibility of a poor forecasting model can be represented by a
value θ that differs substantially from θ∗, and/or small values of d ,d that correspond to
1 2
noisy estimates. Thus, the forecast model could even yield estimates of the ‘unimportant’
probabilities that greatly exceed those of the ‘important’ ones.5 For given support points
u and estimated probabilities ω, the expected squared error and expected CRPS implied
by the analytical framework are given by
n n
(cid:88) (cid:88)
E[SE(ω,u)] = ω∗(u − ω u )2, (12)
i i j j
i=1 j=1
n n
(cid:88)(cid:88)
E[CRPS(ω,u)] = ω (ω∗−ω /2) |u −u |; (13)
i j j i j
i=1 j=1
the expression for the CRPS follows from adapting the representation in Equation 2
of Jordan et al. (2019). In both cases, the expected value is computed with respect to
the discrete distribution with support points u and associated true probabilities ω∗. As
noted in Equation 10, we cast the predicted probabilities ω as scaled draws from two
Dirichlet distributions. We further assume that the support points u are n draws from
a standard normal distribution; these are mutually independent and independent of ω.
Using these assumptions, we obtain the following expressions for the (unconditionally)
expected squared error and CRPS:
(cid:90) (cid:90)
E[SE] = E[SE(ω,u)]dF (ω)dF (u)
ω u
(cid:26) θ∗θ (1−θ∗)(1−θ)(cid:27)
= 1−2 + +
k n−k
θ2 (1−θ)2 θ2(k−1) (1−θ)2(n−k−1)
+ + + , (14)
k n−k k(d k+1) (n−k)(d (n−k)+1)
1 2
(cid:90) (cid:90)
E[CRPS] = E[CRPS(ω,u)]dF (ω)dF (u)
ω u
1
= √ E[SE], (15)
π
where F is the distribution of the estimated probabilities that is implied by our model
ω
setup, and F is the joint distribution of n independent standard normal variables. The
u
proof can be found in Appendix B. The result that the expressions for E[SE] and E[CRPS]
√
are identical up to a factor of π is a somewhat idiosyncratic implication of our model
setup.
5In practice, where the set of important indexes I is not known, this situation corresponds to one in
which the largest empirical weights are not helpful for predicting new test sample cases.
17In order to interpret the implications of these formulas, we compare a forecasting
method with θ < 1 (representing standard RFs) to a method with θ = 1 (representing
Topk) in the following.
For given values of n,k and θ∗, both E[SE] and E[CRPS] attain their theoretical
minimum at θ = θ∗,d → ∞ and d → ∞.6 This result is unsurprising: Under the stated
1 2
conditions, the forecast model coincides with the true model, i.e., ω = ω∗ with probability
one. Since the squared error is strictly consistent for the mean (and, similarly, the CRPS
is a strictly proper scoring rule), the true model must yield the smallest possible expected
score.7 As both expected score functions are continuous in θ,d and d , this implies that
1 2
if θ is sufficiently close to θ∗, and d ,d are sufficiently large, then the standard approach
1 2
will outperform the Topk method.
Conversely,thefollowingconditionsfavortheTopk methodoverthestandardapproach:
• |θ−θ∗| > |1−θ∗|, i.e. the standard method’s implicit assumption that θ∗ = θ is
worse than Topk’s implicit assumption that θ∗ = 1
• d is large, i.e. important probabilities are estimated precisely
1
• d is small, i.e. estimates of unimportant probabilities are noisy
2
If these conditions, or an appropriate combination thereof, hold, then the Topk approach
can be expected to perform well.
Figure 6 illustrates the above discussion. In the left panel (with d = 1000), the
2
‘unimportant’ probabilities are estimated very precisely. Here the Topk method is superior
only to values θ ≤ 0.6 that are clearly smaller than the true parameter θ∗ = 0.8. In the
right panel (d = 0.01), the estimates of the unimportant probabilities are very noisy.
2
Hence it is beneficial to focus on the important probabilities which are estimated precisely
(since d = 1000). Accordingly, the Topk method - which focuses on the important
1
probabilities exclusively - is superior to a wide range of values for θ. Interestingly, this
range includes the true parameter θ = θ∗, i.e., the Topk method can be beneficial even if
the probability estimates are correct in expectation.
5 Conclusion
This paper has considered simplified Random Forest forecast distributions that consist of
a small number k of support points, in contrast to thousands of support points (possibly
equal to n, the size of the training set) of the original forecast distribution. The Topk
forecastdistributioncanbeviewedasacollectionofk scenarioswithattachedprobabilities.
Ithencesimplifiescommunicationandimprovesinterpretabilityoftheprobabilisticforecast.
Our empirical results in Tables 2 and 4 imply that simplified distributions using five or
6Proof: ∂E[SE] <0 for i=1,2; this holds for all values of θ,θ∗,n,k,d and d . It is hence optimal to
let d ,d go
t∂ odi
infinity. Next consider the limiting expression of E[SE]
a1
s d
,d2
→∞. Minimizing this
1 2 1 2
expression with respect to θ yields the solution θ=θ∗.
7While the possibility of exactly matching the true model is unrealistic in practice, the requirement
thatthetruemodelperformbestisconceptuallyplausible,andisthemainideabehindforecastevaluation
via proper scoring rules and related tools.
18d =1000 d =0.01
2 2
0.56
0.8
0.54
0.7
0.52
0.6
0.50
0.5
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
θ θ
Figure 6: Expected CRPS as a function of θ. The left panel refers to d = 1000 (i.e.,
2
preciseestimatesof‘unimportant’probabilities), whereastherightpanelassumesd = 0.01
2
(i.e., noisy estimates). The other parameters are set as follows: n = 100,k = 5,θ∗ =
0.8,d = 1000. Solid vertical line marks θ∗, dashed vertical line marks best value for θ.
1
Shaded area marks range of values for θ that perform worse than θ = 1 (corresponding to
the Topk method).
ten support points often attain similar performance as the original forecast distribution,
while larger choices of k, e.g., 20 or 50, even increase performance slightly in many cases.
Our analytical framework in Section 4 offers a theoretical rationale for these results. Our
empirical analysis further shows that when tuning hyperparameters to the target value for
k, even k = 3 can yield very good results.
While we have focused on the trade-off between simplicity (as measured by k) and
statistical performance, the optimal choice of k depends on the subjective preferences
of the audience to which forecast uncertainty is communicated. In order to choose k in
practice, we recommend that communicators first assess the statistical performance of
various choices of k for their particular data set. In a second step, communicators may
then interview potential users about their perceived cognitive costs of various choices of k.
For example, Altig et al. (2022) argue that k = 5 resonates well with participants of an
online survey on firm performance.
References
Abbas, A. E. and Howard, R. A. (2015). Foundations of Decision Analysis. Pearson.
Altig, D., Barrero, J. M., Bloom, N., Davis, S. J., Meyer, B., and Parker, N. (2022).
Surveying business uncertainty. Journal of Econometrics, 231:282–303.
Beck, E., Kozbur, D., and Wolf, M. (2023). Hedging forecast combinations with an
application to the random forest. Preprint, arXiv:2308.15384.
Biau, G. and Scornet, E. (2016). A random forest guided tour. Test, 25:197–227.
19
SPRCdetcepxE SPRCdetcepxEBreiman, L. (2001). Random forests. Machine Learning, 45:5–32.
Cevid, D., Michel, L., Näf, J., Bühlmann, P., and Meinshausen, N. (2022). Distributional
random forests: Heterogeneity adjustment and multivariate distributional regression.
Journal of Machine Learning Research, 23:1–79.
Cramer, E. Y., Ray, E. L., Lopez, V. K., Bracher, J., et al. (2022). Evaluation of
individual and ensemble probabilistic forecasts of covid-19 mortality in the united states.
Proceedings of the National Academy of Sciences, 119:e2113561119.
Gneiting,T.andKatzfuss,M.(2014). Probabilisticforecasting. Annual Review of Statistics
and Its Application, 1:125–151.
Gneiting, T. and Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and
estimation. Journal of the American Statistical Association, 102:359–378.
Gneiting, T. and Resin, J. (2023). Regression diagnostics meets forecast evaluation:
Conditional calibration, reliability diagrams, and coefficient of determination. Electronic
Journal of Statistics, 17:3226–3286.
Grinsztajn, L., Oyallon, E., and Varoquaux, G. (2022). Why do tree-based models still
outperform deep learning on typical tabular data? Advances in Neural Information
Processing Systems, 35:507–520.
Haddouchi,M.andBerrado,A.(2019). Asurveyofmethodsandtoolsusedforinterpreting
random forest. ICSSD 2019 - International Conference on Smart Systems and Data
Science.
Hastie, T., Tibshirani, R., and Friedman, J. (2009). The Elements of Statistical Learning
Data Mining, Inference, and Prediction. Springer, 2 edition.
Jordan, A., Krüger, F., and Lerch, S. (2019). Evaluating probabilistic forecasts with
scoringRules. Journal of Statistical Software, 90:1–37.
Krüger, F., Clark, T. E., and Ravazzolo, F. (2017). Using entropic tilting to combine
BVAR forecasts with external nowcasts. Journal of Business & Economic Statistics,
35:470–485.
Liaw, A. and Wiener, M. (2002). Classification and regression by randomForest. R News,
2:18–22.
Lin, Y. and Jeon, Y. (2006). Random forests and adaptive nearest neighbors. Journal of
the American Statistical Association, 101:578–590.
Lundberg, S. M., Allen, P. G., and Lee, S.-I. (2017). A unified approach to interpreting
model predictions. 31st Conference on Neural Information Processing Systems (NIPS
2017).
Matheson, J. E. and Winkler, R. L. (1976). Scoring rules for continuous probability
distributions. Management Science, 22:1087–1096.
20Meinshausen,N.(2006). Quantileregressionforests. Journal of Machine Learning Research,
7:983–999.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel,
M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau,
D., Brucher, M., Perrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research, 12:2825–2830.
Probst, P., Wright, M. N., and Boulesteix, A. L. (2019). Hyperparameters and tun-
ing strategies for random forest. Wiley Interdisciplinary Reviews: Data Mining and
Knowledge Discovery, 9:e1301.
R Core Team (2022). R: A Language and Environment for Statistical Computing. R
Foundation for Statistical Computing, Vienna, Austria. URL: https://www.R-project.
org/ (last accessed: February 4, 2024).
Raftery, A. E. (2016). Use and communication of probabilistic forecasts. Statistical
Analysis and Data Mining: The ASA Data Science Journal, 9:397–410.
Rasp, S. and Lerch, S. (2018). Neural networks for postprocessing ensemble weather
forecasts. Monthly Weather Review, 146:3885–3900.
Taieb, S. B., Taylor, J. W., and Hyndman, R. J. (2021). Hierarchical probabilistic
forecasting of electricity demand with smart meter data. Journal of the American
Statistical Association, 116:27–43.
van Rossum, G. et al. (2011). Python programming language. URL: https://www.python.
org (last accessed: February 4, 2024).
Winkler, R. L. (1996). Scoring rules and the evaluation of probabilities. Test, 5:1–26.
Wright, M. N. and Ziegler, A. (2017). ranger: A fast implementation of random forests
for high dimensional data in C++ and R. Journal of Statistical Software, 77:1–17.
Zhao, X., Wu, Y., Lee, D. L., and Cui, W. (2019). Iforest: Interpreting random forests
via visual analytics. IEEE Transactions on Visualization and Computer Graphics,
25:407–416.
21A Details on Empirical Experiments
Here we present further details on the empirical experiments of Section 3.
Table 5 lists the data sets used in the experiments, following the analysis of Grinsztajn
et al. (2022). The data sets cover a wide spectrum of size, number of covariates and
domains. They can be easily downloaded using the URLs listed in the table. In the
case of delays_zurich_transport, we subsampled the data set to 20% of its original
size (which is shown in the table) due to computational reasons, resulting in roughly
1.1 million observations. Table 6 describes the grid of hyperparameter values we consid-
ered for the analysis in Section 3.3, and Table 7 presents the best choices selected via
cross-validation. For a given data set, we use a grid search with 5-fold cross-validation.
For computational reasons, we use simplified procedures for the three largest data sets
(medical_charges, nyc-taxi-green-dec-2016 and delays_zurich_transport), where
we use a single holdout set which consists of 25% of the training data. Furthermore, we
subsample the nyc-taxi-green-dec-2016 and delays_zurich_transport data sets to
30% and 15% of their training set size. Table 7 shows that hyperparameter choices are
often the same across both loss functions (CRPS and SE), whereas differences between
‘Full’ and ‘Top3’ are more pronounced. Hence users of simplified RFs (such as Top3)
should consider tuning hyperparameters to optimize the performance of these simplified
RFs directly, instead of optimizing the performance of full RFs.
Table 5: Data sets tested. Following Grinsztajn et al. (2022), this table lists all tested
data sets, their respective sizes (number of observations and regressors), name of the
target variable, and URL.
NameofDataSet Numberof Numberof TargetVariable URL
Observations Regressors
cpu_act 8192 21 usr https://www.openml.org/d/44132
pol 15000 26 foo https://www.openml.org/d/44133
elevators 16599 16 Goal https://www.openml.org/d/44134
wine_quality 6497 11 quality https://www.openml.org/d/44136
Ailerons 13750 33 goal https://www.openml.org/d/44137
houses 20640 8 medianhousevalue https://www.openml.org/d/44138
house_16H 22784 16 price https://www.openml.org/d/44139
diamonds 53940 6 price https://www.openml.org/d/44140
Brazilian_houses 10692 8 totalBRL https://www.openml.org/d/44141
Bike_Sharing_Demand 17379 6 count https://www.openml.org/d/44142
nyc-taxi-green-dec-2016 581835 9 tipamount https://www.openml.org/d/44143
house_sales 21613 15 price https://www.openml.org/d/44144
sulfur 10081 6 y1 https://www.openml.org/d/44145
medical_charges 163065 3 AverageTotalPayments https://www.openml.org/d/44146
MiamiHousing2016 13932 13 SALEPRC https://www.openml.org/d/44147
superconduct 21263 79 criticaltemp https://www.openml.org/d/44148
yprop_4_1 8885 42 oz252 https://www.openml.org/d/45032
delays_zurich_transport 5465575×0.2 8 delay https://www.openml.org/d/45034
22Table 6: Hyperparameter search grid. The listed values are possible values for each
hyperparameter for the hyperparameter tuning. Explanations of the hyperparameters can
be found in the caption of Table 7. This results in 44 different hyperparameter combina-
tions. Hyperparameters indicated with an asterisk were not used for medical_charges,
nyc-taxi-green-dec-2016anddelays_zurich_transporttoreducecomputationalover-
head.
Hyperparameter Possible Values
min_samples_leaf [1, 2, 4, 6, 8, 10, 15, 20, 30*, 40*, 50]
max_features [0.333, ‘sqrt’, 0.5, 1.0]
23Table 7: Hyperparameters selected via cross-validation. The table presents the
optimal hyperparameters according to 5-fold cross-validation (with exceptions for the
three largest data sets, see text for details). The table lists the best choices for both
hyperparameters (max_features and min_samples_leaf), two loss functions (CRPS and
SE) and depending on whether we consider the full RF or its simplified Top3 variant.
The first row represents our standard hyperparameter choice (considered in Sections 3.1
and 3.2) which is the same for each data set. min_samples_leaf is the minimum number
of samples a leaf must contain and max_features (also called mtry in some software
packages) denotes the number of features considered in each split, where ‘sqrt’ denotes the
floored square root of the number of total features and real numbers correspond to the
floored fraction of total features. The grid of candidate values for each hyperparameter is
listed in Table 6. The number of trees (bootstrap iterations) is set to 1000 for all data
sets, the depth remains unrestricted and the minimum number of samples required to be
considered for another split is fixed to 5.
max_features min_samples_leaf
Tuned on: Full Top3 Full Top3
Dataset CRPS SE CRPS SE CRPS SE CRPS SE
standard sqrt sqrt sqrt sqrt 1 1 1 1
cpu_act 0.5 0.5 0.333 0.333 1 1 4 4
pol 0.5 0.5 0.333 0.333 1 1 20 20
elevators 1.0 1.0 0.5 0.5 2 1 4 4
wine_quality 0.333 0.333 0.333 0.333 1 1 8 8
Ailerons 1.0 1.0 0.5 0.5 2 2 20 20
houses 1.0 1.0 0.5 0.5 2 2 4 4
house_16H 0.5 0.5 sqrt sqrt 1 1 8 8
diamonds sqrt sqrt 0.333 0.333 10 8 50 50
Brazilian_houses 1.0 1.0 1.0 0.5 1 1 1 1
Bike_Sharing_Demand 0.5 1.0 sqrt sqrt 6 10 15 15
nyc-taxi-green-dec-2016 1.0 1.0 1.0 1.0 4 4 8 8
house_sales 0.5 0.5 0.5 0.5 1 1 6 10
sulfur 1.0 sqrt sqrt 0.333 1 1 2 2
medical_charges 1.0 1.0 0.333 0.333 50 50 15 15
MiamiHousing2016 0.5 0.333 sqrt sqrt 1 1 8 2
superconduct 0.333 0.333 0.333 sqrt 2 1 6 2
yprop_4_1 0.333 sqrt sqrt sqrt 6 2 50 40
delays_zurich_transport 0.333 0.333 0.5 0.5 50 50 4 8
24B Details on Section 4
Here we derive the results for E[SE] and E[CRPS] stated in Equations 14 and 15.
B.1 SE
We start with the derivation for the squared error, which is given by
(cid:90) (cid:90)
E[SE] = E[SE(ω,u)] dF (ω)dF (u).
ω u
First, we rewrite Equation 12:
n n
(cid:88) (cid:88)
E[SE(ω,u)] = ω∗(u − ω u )2
i i j j
i=1 j=1
n n n n n
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
= ω∗u2−2 ω∗u ω u +( ω u )2 ω∗.
i i i i j j j j i
i=1 i=1 j=1 j=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125)
=1
We next change the order of integration and calculate the expectation with respect to the
support points, i.e., we consider the expected value with respect to u:
(cid:90)
E [E[SE(ω,u]] = E[SE(ω,u)] dF (u)
u u
n n n n
(cid:88) (cid:88) (cid:88) (cid:88)
= E [ ω∗u2]−2 E [ ω∗u ω u ]+E [( ω u )2]
u i i u i i j j u i i
i=1 i=1 j=1 i=1
n n
(cid:88) (cid:88)
= 1−2 ω∗ω + ω2,
i i i
i=1 i=1
where we have used the assumption that the elements of u are independently standard
normal. Next, recall that
(cid:40)
θ/k if i ∈ I
E[ω ] =
i (1−θ)/(n−k) if i ∈/ I
(cid:40)
θ2 Var[Z ] if i ∈ I
1
Var[ω ] =
i (1−θ)2 Var[Z ] if i ∈/ I
2
Furthermore, from the variance of a Dirichlet distributed random variable, we obtain
(cid:40)
k−1 if j = 1
Var[Z ] =
k2(kd1+1)
j n−k−1 if j = 2
(n−k)2((n−k)d2+1
25We are now ready to calculate the expectation with respect to ω:
(cid:90) (cid:90)
E [E [E[SE(ω,u)]]] = E[SE(ω,u)] dF (u)dF (ω)
ω u u ω
 =θ∗ =1−θ∗ 
k n−k
= 1−2 (cid:88)(cid:122) ω(cid:125)(cid:124) i∗(cid:123) E ω[ω i]+(cid:88) (cid:122) ω(cid:125)(cid:124) i∗(cid:123) E ω[ω i] +
 
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
i∈I i∈/I
=θ =1−θ
k n−k
(cid:88) (cid:88)
( E [ω2]+ E [ω ]2)
ω i ω i
i∈I i∈/I
(cid:26) θ∗θ (1−θ∗)(1−θ)(cid:27)
= 1−2 + +
k n−k
θ2 (1−θ)2 θ2(k−1) (1−θ)2(n−k−1)
+ + + .
k n−k k(kd +1) (n−k)((n−k)d +1)
1 2
B.2 CRPS
We seek to evaluate the following integral:
(cid:90) (cid:90)
E[CRPS] = E[CRPS(ω,u)] dF (ω)dF (u),
ω u
whereE[CRPS(ω,u)]isgiveninEquation13. NotethatarandomvariableW := |W −W |
1 2
with W 1,W
2
∼ N(0,1) follows a folded normal distribution with mean µ
Y
= 2/√ π. Using
this fact and Equation 13, the expected value with respect to u is given by
n
E [E[CRPS(ω,u)]] =
√2 (cid:88)(cid:88)
ω
(cid:16)
ω∗−
ω j(cid:17)
.
u i j
π 2
i=1 j̸=i
To simplify notation, we define c := 2/√ π. In order to compute the expected value with
respect to ω, we differentiate between four cases:
Case (1) i,j ∈ I. It holds that Cov(ω ,ω ) = − θ2 . We hence obtain
i j k2(kd1+1)
E [E [E[CRPS(ω,u)]]] =
c(cid:88)(cid:88)(cid:18)
E [ω ]ω∗−
E ω[ω iω j](cid:19)
ω u ω i j
2
i∈I j∈/I
(cid:18) θ∗ θd (cid:19)
1
= c(k−1)θ − .
k 2(kd +1)
1
26Case (2) i,j ∈/ I. Here we have Cov(ω ,ω ) = − (1−θ)2
i j (n−k)2((n−k)d2+1)
E [E [E[CRPS(ω,u)]]] =
c(cid:88)(cid:88)(cid:18)
E [ω ]ω∗−
E ω[ω iω j](cid:19)
ω u ω i j
2
i∈/I j∈/I
(cid:18) (1−θ∗) (1−θ)d (cid:19)
2
= c(n−k−1)(1−θ) − .
n−k 2((n−k)d +1)
2
Case (3) i ∈ I, j ∈/ I. With Cov(ω ,ω ) = 0, we obtain
i j
E [E [E[CRPS(ω,u)]]] =
c(cid:88)(cid:88)(cid:18)
E [ω ]ω∗−
E ω[ω iω j](cid:19)
ω u ω i j
2
i∈I j∈/I
(cid:18) θ(1−θ∗) θ(1−θ) (cid:19)
= ck(n−k) −
k(n−k) 2k(n−k)
(cid:18) (cid:19)
1
= c θ−θθ∗− (θ−θ2) .
2
Case (4) j ∈/ I, j ∈ I. As in Case (3), it holds that Cov(ω ,ω ) = 0.
i j
(cid:18) (cid:19)
1
E [E [E[CRPS(ω,u)]]] = c θ−θθ∗− (θ−θ2) .
ω u
2
Summarizing all four cases, we obtain
2 (cid:26) (cid:20) θ∗ d θ (cid:21)(cid:27)
E[CRPS] = √ θ(k−1) − 1 +
π k 2(kd +1)
1
2 (cid:26) (cid:20) 1−θ∗ (1−θ)d (cid:21)(cid:27)
2
√ (1−θ)(n−k−1) − +
π n−k 2((n−k)d +1)
2
√2 (cid:8) θ∗+θ2−2θ∗θ(cid:9)
.
π
√
The result that E[CRPS] = E[SE]/ π then follows from tedious yet straightforward
algebra. We are happy to provide detailed notes upon request.
27