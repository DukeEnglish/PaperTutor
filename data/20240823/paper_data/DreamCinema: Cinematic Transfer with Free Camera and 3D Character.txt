DreamCinema: Cinematic Transfer with Free Camera and 3D Character
WeiliangChen FangfuLiu* DiankunWu HaowenSun HaixuSong YueqiDuan†
TsinghuaUniversity
{cwl24, liuff23, wdk21, shw22, shx22}@mails.tsinghua.edu.cn;
duanyueqi@tsinghua.edu.cn
Abstract difficulty[49,63],extensivetimerequirements[10,13],and
considerablecosts[50,51],asfilmmakershavetofindap-
Wearelivinginaflourishingeraofdigitalmedia,where propriatecharactersanddesignintricatecinematographyto
everyonehasthepotentialtobecomeapersonalfilmmaker. enhanceexpressiveeffectsandcraftcompellingnarratives.
Currentresearchoncinematictransferempowersfilmmak- Therefore,creatorsareeagerlypursuinginnovativetechnolo-
ers to reproduce and manipulate the visual elements (e.g. giestoenableefficientandwallet-friendlyfilmproduction.
cinematographyandcharacterbehaviors)fromclassicshots. Withrecentcompellingsuccessinlargediffusionmod-
However, characters in the reimagined films still rely on els [14, 20, 67, 84], video generation [19, 26, 47, 82, 90]
manualcrafting,whichinvolvessignificanttechnicalcom- suggests a potential avenue for efficient film creation, as
plexityandhighcosts,makingitunattainableforordinary demonstratedbySora[8]andVidu[3],whichcanproduce
users.Furthermore,theirestimatedcinematographylacks visuallyappealing,attention-grabbingvideos.However,the
smoothnessduetoinadequatecapturingofinter-framemo- videosgeneratedbythesemethodsoftenfailtomaintainvi-
tionandmodelingofphysicaltrajectories.Fortunately,the sualconsistency[41,68]anddefyphysicalintuition[12,42]
remarkable success of 2D and 3D AIGC has opened up due to the lack of decoupling of visual elements such as
thepossibilityofefficientlygeneratingcharacterstailored cameramovementsandcharactermotions,aswellasinsuffi-
to users’ needs, diversifying cinematography. In this pa- cient3Dcharactermodeling,ultimatelyfailingtofullyim-
per, we propose DreamCinema, a novel cinematic trans- merseviewersinthevideocontent.Additionally,becausethe
fer framework that pioneers generative AI into the film promptsbasedontextorimageslackrichcinematicknowl-
production paradigm, aiming at facilitating user-friendly edge,videogenerationmodelsstruggletocreatevideoswith
film creation. Specifically, we first extract cinematic el- cinematicquality.Therefore,efficientlyconstructingandin-
ements (i.e. human and camera pose) and optimize the tegratingvisualelementsinfilmproductionremainsacrucial
camera trajectory. Then, we apply a character genera- challenge.
tor to efficiently create 3D high-quality characters with
Drawinginspirationfromtheprevalentwatch-and-learn
a human structure prior. Finally, we develop a structure-
paradigm in film production, recent works [25, 75] have
guided motion transfer strategy to incorporate generated
focused on cinema behavior transfer, attempting to ex-
charactersintofilmcreationandtransferitvia3Dgraph-
tract cinematic knowledge from movie scenes and apply
ics engines smoothly. Extensive experiments demonstrate
ittosubsequentfilmmaking.Leveragingadvancementsin
the effectiveness of our method for creating high-quality
camera pose estimation [16, 66, 81, 92] and human mo-
films with free camera and 3D characters. Project page:
tionestimation[56,80]technologies,thesestudiesutilize
https://liuff19.github.io/DreamCinema/.
NeRF [52, 55] to extract visual elements such as camera
trajectoriesandSMPL[44]tracksfromclassicfilmclipsfor
visualhomagesandclassicrecreations.However,thecharac-
1.Introduction
tersusedinsubsequentfilmreinventionaremanuallycrafted
bydesigners,whichisextremelytime-consumingandcostly.
Withtheevolutionofdigitalmedia,awidespreadandflour-
Moreover,itisdifficulttocreatehigh-qualitycharactersthat
ishing need arises for efficiently creating personal, high-
bothmeetuserrequirementsandseamlesslyintegratewith
quality,cinematic-levelvideos[77,84,88].However,film
theextractedcinematicelements.Furthermore,simplyap-
creationhasalwaysbeenaprocessmarkedbyhightechnical
plyingtheextractedbutnotsmoothcameramovementsto
the new content limits overall creative flexibility and im-
*ProjectLeader
†CorrespondingAuthor pact.Sincegenerativemodels[17,34,38]havedemonstrated
1
4202
guA
22
]VC.sc[
1v10621.8042:viXraT
Reference Films Selected Film
Generated Characters Selected Characters Camera and Pose Estimation Transferred Film
Figure1.DreamCinemaisauser-friendlycinematictransferframeworkthatfacilitatespersonalmoviecreationwithfreecameraanddesired
characters.DreamCinemafirstextractscinematicelementsfromreferenceshotsandgenerates3Dcharacterstailoredtouserpreferences.
Thenitintegratesthegeneratedcharactersintofilmcreationviaastructure-guidedmotiontransferstrategy.
remarkable efficiency [69, 93], high quality [36, 43] and Although existing SLAM [16, 92] systems could be
customizability[37,61,65]acrossmanyfields,itnaturally utilizedtoextractcameraparametersandthusobtainworld-
promptsustoexplorehowtheycanbeharnessedtoenhance groundedhumanmotiontrajectories,theyoftenstrugglein
thefilmproductionparadigm. dynamic scenes. To address this challenge, GLAMR [83]
Totackletheabovechallenges,weproposeDreamCin- integratesalocalmotionextractor,adeepgenerativemotion
ema,anovelcinematictransferframework(showninFig- infiller,andaglobaltrajectorypredictortooptimizehuman
ure1),whichisuser-friendlyforfilmproduction.Thisframe- trajectories and camera poses simultaneously. D&d [35]
workincludesasmoothercinematicelementsextractor,anef- enhances dynamic motion estimation with an inertial
ficientandhigh-qualitycharactergenerator,andaneffective force control module for videos with moving cameras.
cinematictransferoptimizationstrategy.Specifically,givena SLAHMR[80]feedscameramotioninitializedbyDROID-
selectedfilm,wefirstextractcinematicelements(i.e.human SLAM [70] and human motion tracked by PHALP [62]
andcamerapose)andoptimizethecameratrajectorywith intoajointoptimizationsystemtosolveforcamerascale,
motion-awareguidanceandphysicalmodeling(i.e.Bezier groundplane,andhumantrajectoriesintheworldcoordinate
curve).Next,poweredbygenerativemodels,weemploya system. JAWS [75] and CineTrans [25] model sequence
3Dcharactergeneratorwithstructurepriors(e.g.SMPL[44]) camera parameters in canonical exponential coordinate
toproducehigh-qualitycharacterstailoredtouserpreference. systemanduseafixeddynamicNeRF[59]tooptimizethem.
Finally,wedeviseanefficientcinematictransferstrategyto
facilitatesmoothbehaviortransferandtonalharmonyforthe 3D Avatar Generation. In the field of 3D avatar
generatedcharactersinfilmcreations.Extensiveexperiments generation,parametricmodelssuchasSMPL[44],SMPL-
verifytheeffectivenessofourframeworkincreatingnovel X[56],andimGHUM[1]areextensivelyused,providing
filmswithhighqualityanduser-preferencealignment.We accurate human-body priors for various 3D generation
believeourDreamCinemacanserveasapotentialcinematic methods. EVA3D [21] combines a GAN backbone with
transferframeworkforfuturefilmproduction. a pose-guided sampling technique to create high-quality
3D avatars. AvatarCLIP [22] first achieves zero-shot
text-driven avatar generation by leveraging CLIP [22]
2.RelatedWork
scores to optimize NeuS [73], which is initialized by the
World-groundedHumanMeshRecovery.Inrecentyears, SMPL[44]model.DreamAvatar[9]andAvatarCraft[24]
therecoveryofhumanshapeandmotionfromvideoshas enhance this process by distilling pre-trained 2D latent
garneredsignificantinterestfromresearchers.Earlyworks diffusionmodels.DreamHuman[32]adoptsImGHUM[1]
primarilyfocusedonrecoveringhumanwithinthecamera asbodypriorsandintroducesafocusrenderingmechanism
coordinatesystem,commonlyadoptingeitheroptimization- to better reconstruct geometry. DreamWaltz [23] uses
based approaches [6, 33, 72] or regression-based ap- occlusion-awareSDSandskeletonconditioningtechnique
proaches[27,31,85].Basedon3Dhumanreconstruction to maintain 3D consistency and reduce artifacts during
fromHMRmodels[27],PHALP[62]demonstratesstate-of- optimization.HumanGaussian[39]incorporatesSDSwith
the-artperformanceintrackingpeopleacrossframeswithin state-of-the-art3DGS[30]representationformoreefficient
thecameracoordinatesystem.However,thesemethodsdo text-driven3Dhumanavatars.
not account for camera motion and fail to recover global
humantrajectories. AI-Based Film Creation. The rapid development of
2artificial intelligence has propelled researchers to explore In the inference, the model can use the reverse diffusion
its applications in the film creation process [49]. Some process,conditionedonc,torecovertheoriginaldatadistri-
works utilize artificial intelligence to assist with specific butionfromGaussiannoise.
task within the traditional film creation pipeline [2]. For OptimizationofCameraParameters.Recentworks[81]
example, ScriptWriter [91] integrates an attention-based applyNeRF[52]traininginaninversemannertoestimate
model and a matching-updating mechanism to generate the camera pose by optimizing the pose to match the ren-
movie scripts from narratives. Meanwhile, powered by deredviewI withareferenceviewI∗.Toensuretheesti-
the strong capabilities of latent diffusion models [5, 57], matedposestillliesintheSE(3)manifold,theoptimization
manyresearchersattempttoachievefilmcreationthrough processisperformedinthecanonicalexponentialcoordinate
video generation techniques [3, 8]. However, since these system[45]basedonaninitialcameraposeT(0)as:
modelsdon’tmodel3Dphysicalworld,theyoftenstruggle
with issues such as handling causal-effect relationships, Tˆ(θ)=e[ξ]θT(0), wheree[ξ]θ =(cid:20) e[w]θ K(θ,ω,v)(cid:21) ,
simulatingphysicalinteractions,andmaintainingproportion 0 1
consistency[11,77,88].Recently,followingthewatch-and- (cid:40)
vθ ifω =0
learnparadigm,JAWS[75]andCineTrans[25]utilizethe withK(θ,ω,v)= .
(I−e[w]θ)[w]v+ωω⊤vθ
otherwise
robusttransferofvisualcinematicfeaturesfromreference ∥ω∥
clipstocreatenewclips,aprocessreferredtoascinematic (3)
transfer.
where[ξ]isatwistmatrix,(ω,v)aretwistcoordinates,
and θ is a magnitude. The optimization problem can be
3.Method
formulatedas:(θˆ ,ωˆ ,vˆ )=argmin L.
k k k θk,ωk,vk
In this section, we introduce our DreamCinema, a user-
3.2.CinematicElementExtraction
friendlycinematictransferframeworkwithafreecameraand
3Dcharacters.Ourgoalistocreatenovelfilmswithgener- Human pose and Camera Estimation. To reproduce a
atedelementstailoredtouserpreferencepoweredbyAIGC. novel cinematic film with AI-generated components, the
First,weextractcinematicelements(i.e.humanandcamera firstthingistoanalyzeandextractthecinematicelements
pose)fromtheselectedfilmandoptimizethecameratrajec- (i.e.cameratrajectoryandcharactermotion).Poweredby
tory(Sec.3.2).Then,wedesigna3Dcharactergeneratorto recent advancements in human tracking [18, 48, 80], we
producehigh-qualitycharacterstailoredtouserpreference firstapplyparametrichumanmodels(e.g.SMPL[44]and
(Sec.3.3).Finally,wedeviseanefficientcinematictransfer SMPL-X[56])tothevideo,decouplingthecorresponding
strategytotoseamlesslyincorporategeneratedcharacters charactermotionandcameratrajectory.Formally,givena
intofilmcreation.BeforeintroducingourDreamCinemain set of cinematic frames V = {I }T , we predict SMPL
t t=0
detail,wefirstreviewsomepreliminaries.Anoverviewof tracks S = {S }N,T and the camera trajectory
w n,t n=1,t=1
ourframeworkisdepictedinFigure2. C = {C }T in the world coordinate system by a joint
w t t=1
learningframeworkof4Dhumanreconstruction[80],where
3.1.Preliminaries
T istheframenumberandN representsthenumberofchar-
acters.However,thisjointoptimization[80]methodcannot
DiffusionModel.Diffusionmodels[20,67]generatesam-
perfectlyestimatethecameraduetotheinherententangle-
plesfromaGaussiandistributionwithtwoprocesses:(a)a
mentofC andS .Toaddressthis,weutilizeaNeRF-based
forwarddiffusionprocessthataddsnoisetothedata;(b)are- w w
cameraestimation[81]whichfixesS andonlyoptimizes
versediffusionprocessthatremovesthenoisetorecoverthe w
C viadifferentialrenderingformoreaccuratecameraesti-
originaldatadistribution.Letx ∼p(x)bethesampleddata, w
0
mation.Specifically,wetrainadynamicNeRFf (θ,t)to
andcrefertotheadditionalcondition(e.g.textorimage).In D
representtheSMPLtracksS intheworldcoordinates.The
training,themodeladdsnoisetox overT timestepsusing w
0
anoisingscheduleα ∈(0,1),withαˆ
=(cid:81)t
α .Thisis
cameraoptimizationcanthenbeviewedasaninvertedNeRF
t t s=1 s
training,formulatedas:
formulatedas:
(cid:112) (cid:112) cˆ =arg min L(c |Iˆ,θ), (4)
x = αˆ x + 1−αˆ ϵ, (1) t t t
t t 0 t cˆt∈SE(3)
whereϵ∼N(0,I)istheaddednoise.Themodelthenlearns where Iˆ is the t-step predicted mask of SMPL from the
t
toestimatethenoisegivenaconditioncbyminimizingthe videoframeI .
t
followingobjective: CameraTrajectoryOptimization.AstheNeRFistrained
on SMPL tracks in world coordinates, it is impractical to
L =E (cid:2) ∥ϵ−ϵ (x ,t,c)∥2(cid:3) . (2) directly use RGB images from the video to supervise the
simple x0,t,ϵ,c θ t
3Cinematic Elements Extraction Camera Optimization
T Ground Truth
T
Render
Reference Flims Selected Flim   (  ,  )
Instance-aware Loss
SPML Tracks Differentiable
I like this film and that
Renderer  
character!
Motion
Character Generation
Transfer
Semantic-aware Loss
… SPML BP  
Initialized
Mesh
… Motion
Transfer
Selected Generated
Character Character
Character Portraits Muti-view Assets 3D Engine Motion-aware loss Loss

Figure2.TheoverallframeworkofDreamCinema.Wefirstextractthecinematicelements(i.e.cinematographyandcharactermotion)
fromthereferenceshotsandoptimizethecameratrajectory.Next,wegenerate3Dcharactersthatmeetuserpreferences.Finally,wetransfer
themotiontothegeneratedcharactersusingastructure-guidedmotiontransferstrategy.Ourframeworkcancreatenovelfilmswithgenerated
elementstailoredtouserpreferences.
cameraoptimization.Weadopttwostaticlosses:aninstance- formulatedasminimizingthecombinedloss:
aware loss L and a semantic-aware loss L , inspired by
I S
recent cinematic transfer works [25, 75]. Additionally, to L all =λ IL I +λ SL S +λ ML M, (5)
guidethemodelincapturingdynamicinformationbetween
whereλ ,λ ,andλ aretheweightsforthelossesrespec-
frames,wedesignanovelmotion-awarelossusingthemo- I S M
tively. After optimizing the camera trajectory, we further
tion flow. Specifically, we compute them respectively as
refineitbyfittingitwithalearnablesmoothcurvetoachieve
follows: (a)Instance-awareloss:WefittheSMPLtracks
amorepolishedandcontinuouspath.Specifically,weem-
backtothevideotoobtainthecharacterinstancemaskI ,
thelossbetweentherenderedimageIˆandthemaskiscam
l-
ployanN-degreeBeziercurve[86]tofitthesampledpoints
culatedasL =MSE(Iˆ,I ). (b)Semantic-awareloss:To Cˆ =f β(t).TheN-degreeBeziercurveisgivenby:
I m
alignthesemanticinformationbetweentherenderedimage
N (cid:18) (cid:19)
(cid:88) N
andtheSMPLtracks,wereprojectthejointsinSMPLtracks B(t)= (1−t)N−itic∗. (6)
tocameracoordinatesandsupervisethemwiththe2Dkey- i i
i=0
pointsdetectedbyanexistingmaturemodel,Vit-Pose[78].
Here,cˆ andcˆ areusedasthefirstandlastcontrolpoints
L =MSE(Π(J ),J ).whereJ arethe3Djointpo- 1 T
S 3D 2D 3D
x andx ,respectively.Ourgoalistolearntheintermedi-
sitionsfromtheSMPLmodel,Πistheprojectionfunction 0 T
ate control points {c∗}T−1. The optimization objective is
from 3D to 2D camera coordinates, and J 2D are the 2D t t=1
formulatedas:
keypointsdetectedbyViT-Pose. (c)Motion-awareloss:we
computethemotionbetweenjointsintheconsecutiveframes
T−1 (cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
(cid:88) t t
andsupervisethenetworktocapturethismotionflow,which {c∗}T−1 =argmin MSE B ,f .
inturnstrengthenstheconnectionbetweencamerasacross t t=1 ct T β T
t=2
differentframes,asformulatedL =MSE(∆J ,∆J ). (7)
M pred gt
where∆J representsthepredictedjointflow,and∆J
pred gt 3.3.3DCharacterGeneration
representsthegroundtruthjointflow.
Ascharactergenerationinfilmapplicationsrequiresintri-
catedetails,weadoptamulti-levelupscalestrategytoim-
Overall,werepresenttheoptimizedcameraposewithan provetheresolutionofourgeneratedresults.Givenaninput
MLP(θˆ ,ωˆ ,vˆ )=f (t).Ouroptimizedobjectivecanbe characterimageordescriptionofacharacter(wewilluse
k k k β
4
weiv-itluM
ledoM
noisuffiDa text-to-image [60] diffusion process), we first employ a deformationformula:
multi-view diffusion model to generate four orthographic
(cid:88) (cid:88)
multi-views.Thenweprocessthemwithanormaldiffusion v i′ = w ikT kv i s.t. w ik =1, (10)
model to generate their corresponding normal maps. The k k
multi-viewandnormaldiffusionmodelsarebothfine-tuned
whereT isthetransformationmatrixforboneb .Given
fromStableDiffusion[64].Finally,weemployasingle-view k k
thatourcharactersarecustom-generatedtomeetuserspeci-
super-resolutionmodel[74]toquadrupletheresolutionof
fications,thereareoftendiscrepanciesbetweenthecharacter
multi-viewcolorimagesandnormalmaps,whichenables
andtheskeletonusedformotion(e.g.differencesinscale
ustorecovermoreintricategeometricdetailsforcharacter
andrestpose),makingthebindingprocesschallenging.To
generationsettings.Despitehigh-qualityresultsachievedby
address this, we employ advanced rigging techniques [4]
recent3Dcharactergenerationframework[22,23,43,46]
tobindtheskeletonandassignweightsbasedonstructural
thatoptimizeanimplicitfunctiontoobtain3Dresults,they
guidance(e.g.wristsandknees)andadaptivelyadjustthe
usuallysufferfromlow-resolutionandcoarsemeshresults.
character’sposeandscaletoalignwiththemotionsequence.
Thissignificantlylimitstheirapplicationinfilmproduction.
Finally,thecharactercanbeseamlesslyboundtothemotion
Incontrast,weadoptanefficientandhigh-fidelitymeshgen-
usingauto-riggingtechnology[15]ina3Dengine,ensuring
eration strategy Unique3D [76] for 3D characters, which
effectivemotiontransfer.
canbeeasiertointegratethepopulargraphicsengines[54].
Transfervia3DGraphicsEngines.Withtheriggedchar-
AsSMPL[44]iswidelyusedforhumanmodeling,wefirst
acter and camera trajectory, we first render the pure char-
utilizeitsstructuretoinitializeacoarse3Dmeshwithhu-
acter films F = {I }T without a background. Then,
manshapeprior.Givenmulti-viewcolorimageswiththeir c c,t t=1
weremovethecharactersfromtheoriginalvideousingan
correspondingnormalmapsofacharacter,wethenformal-
advancedvideoinpaintingmethod[89]toobtaintheback-
izeanexplicitoptimizationstrategythatfurtherunleashes
groundfilmsF ={I }T .Wethenmergethetwofilms
thestructureinformationinmulti-viewimages.Theoverall b b,t t=1
to create a new movie tailored to user requirements. Ad-
objectlossfunctionisdefinedas:
ditionally, there exist some RGB color space distribution
differencessincethegeneratedcharactersandtheoriginal
L recon =(cid:88) M i⊙(cid:13) (cid:13) (cid:13)Iˆ i−I iT(cid:13) (cid:13) (cid:13)2 , (8) charactersmayhaveasignificantdomaingap(e.g.instyle
2
i andhue).Toaddressthis,wedesignaharmonizationalgo-
rithmasformulated:
whereM isthepredictedrenderedmaskunderviewi.Iˆ
i i
referstogeneratedmulti-views(i.e.RGBornormalmaps). I∗ =F−1(clip(F (I s×τ clipped),L min,L max))
IT istheexplicitoptimizationtargetthatensemblesmulti- (cid:18) (cid:18) L (cid:19)(cid:19)
i where τ =max τ ,min t ,τ
viewinformation,computedas: clipped min L +ϵ max
s
(11)
(cid:80) V(v,i)σ(v,i)2Col(P(v,i),I(i))
IT = i∈I m , (9)
i (cid:80) V(v,i) Here,L s,L t arethevisualfeature(e.g.luminance)ofthe
i∈I
source and target image, ϵ is a small constant to avoid di-
visionbyzero,τ ,τ ,L andL aretheadjustable
whereV(v,i)denotesthevertexvisibilityinviewi,P(v,i) min max min max
ratioboundsandtheboundsforthefeaturespace.F andF−1
is a projection function that projects spatial coordinates
denotesthefeaturespaceconversionandinverseconversion.
into 2D image of view i, and Col(·) denote the point
color.Moreover,weemploytheweightingfactorσ(v,i)=
4.Experiments
cos(n ,n(view))tomeasurethesimilaritydistancefromver-
v i
texnormaln ofvtoviewdirectionn(view)ofviewi.No- 4.1.ImplementationDetails
v i
tably, the overall mesh reconstruction takes no more than
OurimplementationisbasedonPytorchandOpencv[7].In
10sandweprovidemoredetailsinoursupplementary.
thecameratrajectoryoptimization,weuseSLAHMR[80]
for4D human recovery,D-NeRF[58]for differentialren-
3.4.CinematicTransferOptimization.
dering and VitPose [78] for joints prediction. For the 3D
Structure-GuidedMotionTransfer.Givena3Dcharacter charactergenerator,wechooseanopen-sourcedimage-to-
and a motion sequence, the conventional method [28, 29, 3DgenerationframeworkUnique3D[76]formoreintricate
53,79]involvesbindingaskeletonwithbones{b }tothe details. In the cinematic transfer, we use Mixamo [4] for
k
3D character and assigning skinning weights w to each rest-posetransferandblenderauto-rigging[15]formotion
ik
vertexv ofthemesh.Themotionisthentransferredtothe transfer.Moreimplementationdetailscanbefoundinour
i
skeleton,suchthatthemovementofeachvertexfollowsthe Appendix.
5(a) Original Shot (b) SMPL Visualization (c) Character Transfer Results (d) Character Transfer Results
Figure3.Examplesofcinematictransferresults.(a)Theoriginalshots.Wepresentthreecommonshottypes:Arc,Track,andPush
In.(b)SMPLvisualizationofourmethod.WerendertheextractedSMPLtrackswiththeoptimizedcameratrajectoriestovisualizeour
cinematicelementsextraction.(c)-(d)Thecinematictransferresults.Wegeneratediversecharacterswithhighqualityandalignmenttouser
preferencesandselectivelytransferthesecinematicbehavior(e.g.cinematographyandcharactermotions)tothenewcharacters.
6Figure4.AlignmentComparison.Wepresentthealignmentvisualizationresults,combiningoriginalshotwiththeextractedcinematic
elements.
(i-a) Original Shot (ii-a) Original Shot
(i-b) Desired Character Transfer (single character) (ii-b) 3D Cinematic Transfer (only camera)
(i-c) Desired Character Transfer (all character) (ii-c) 3D Cinematic Transfer (char. & camera)
Figure5.MoreApplication.WeshowtheflexibleapplicationsofDreamCinema:(i)thereferenceshotsrestoration(ii)newfilmcreation
withextractedandgeneratedelements.
4.2.QualitativeResults motiontransferandharmoniouscinematicintegration.
ComparisionwithSOTAincinematicelementsextrac-
QualitativeresultsofDreamCinema.Figure3showcases tion.Figure4comparesthealignmentofcinematicelements
the results of our cinematic transfer applied to generated and the original shot with DROID-SLAM [71] and Cine-
characters. Our cinematic transfer framework can handle Trans[25].InFigure4(b),AsDROID-SLAMdirectlyesti-
a wide range of shots and accommodate both single and matesthecameratrajectoryinthepixelcoordinates,itfails
multiplecharacters.Additionally,ourframeworkcangener- todecoupletheobjectandcameramotions,resultinginin-
atehigh-qualitycharacterstailoredtouserpreferences,and accuratecameratrajectoryestimationinworldcoordinates.
seamlesslyintegratethemintothecinematransfer.Specifi- InFigure4(c),itisevidentthatthetrajectoriesestimatedby
cally,figure3highlightsthreekeyaspects:(a)theoriginal CineTransfailtoaligntheSMPLtracks(asshownontheleft)
shots,whichrepresentcommoncinematicmovementssuch andsufferfromsignificantjitter(asshownontheright).Our
asArc,Track,andPush-In;(b)avisualizationofSMPLout- methodachievesthebestalignmentresults,whichdemon-
putsusingourapproach,whichillustratestherobustnessof stratestheeffectivenessofourcinematicelementextraction.
ourcinematicelementsextractionacrossdifferentshottypes; MoreApplications.Figure5showsvariousapplicationsof
and(c)–(d)thecinematictransferresults,whereourmethod ourcinematictransferframework.AsshowninFigure5(i-a)
successfullygeneratesdiversecharactersandtransferscine- and(i-b),ourframeworktransferssingleormultiplecharac-
maticbehavior,includingsubtleaspectslikecinematography tersasspecifiedbytheuser.Therestoredshotsarehighly
andcharactermotion,intothesenewlygeneratedcharacters. consistentwiththereferenceshotsincharactermotions,cin-
Grounded by diverse characters that fit into different shot ematography,andvisualaesthetics,enabledbyoursmooth
types,wedemonstratethatourDreamCinemaisbothuni- and accurate cinematic elements extraction and structure-
versalanduser-friendly,featuringseamlessstructure-guided guidedcinematictransferoptimization.Figure5(ii)shows
7
lanigirO
)a(
MALS-DIORD
)b(
snarTeniC
)c(
sruO
)d(Table1.Comparisonwiththestate-of-the-artcameraposeestimationmethodsondifferentshotmovementtypes.Ourapproach
outperformstheotherbaselinesacrossallmetrics.
ShotMove. PUSH-IN PULL-OUT PAN
Methods PA↑ IoU↑ MPJPE↓ PA↑ IoU↑ MPJPE↓ PA↑ IoU↑ MPJPE↓
DROID-SLAM[70] 86.2 85.8 404.9 86.0 85.3 356.2 91.9 89.6 40.9
iNeRF[81] 89.0 88.2 292.6 92.8 91.5 83.9 83.9 81.0 109.6
CineTrans[25] 89.9 88.5 59.6 94.8 94.0 23.8 93.4 91.4 21.4
Ours 90.3 93.7 58.3 94.9 94.5 22.5 94.8 93.7 20.2
ShotMove. TRACK FOLLOW ARC
Methods PA↑ IoU↑ MPJPE↓ PA↑ IoU↑ MPJPE↓ PA↑ IoU↑ MPJP↓
DROID-SLAM[70] 89.3 88.3 109.2 73.3 70.5 1046.9 92.7 92.6 145.2
iNeRF[81] 90.1 89.2 58.5 85.3 85.1 267.5 90.8 90.5 116.3
CineTrans[25] 94.5 93.8 21.8 91.3 90.5 130.9 94.8 94.5 47.9
Ours 95.0 94.6 19.3 92.4 90.7 117.9 95.2 94.9 40.6
ourflexibleapplicationincreatingnewfilmsviaa3Den- Table2.AbalationStudyofCinematicElementExtraction.We
gine.InFigure5(ii-b)and(ii-c),Wetransferourestimated performedanablationstudyonthethreeguidanceofcinematic
elementextractionsandtestediton30filmshots.
cameraandincorporatethecharacterstoanewsceneforfilm
Methods PA↑ IoU↑ MPJPE↓
creation.Asuserscanmanipulatetheelementsextractedor
DreamCinema 93.8 93.7 46.5
generatedbyourframework,itdemonstratesthatouruser-
w/oInstance-awareloss 11.6 11.8 146.3
friendlyframeworkhasthepotentialtomakeeveryonetobe
w/oSemantic-awareloss 88.2 89.3 104.5
theirownfilmmaker.
w/oMotion-awareloss 88.9 87.5 64.9
4.3.QuantitativeResults
MPJPEbutalsoshowsimprovementsinPAandIoU.The
Weconductedtwoquantitativeexperimentstodetermineif Motion-awarelosscontributestoimprovementsacrossall
ourcinematictransfermeetstwocriteria.Oneistoevaluate metrics by guiding the model to focus on the continuous
theconsistencybetweentheoriginalandreconstructedshots. movementofjoints,therebyenhancingtheeffectivenessof
Theotheristodeterminewhetherthenewlycreatedmovies cinematicelementextractionindynamicvideos.
usingcinematictransfermeetuserpreferences.
ComparisionwithSOTAonCinematicSceneRestoration. 5.Conclusion
Wefirstcomparedourresultswiththestate-of-the-artcamera
poseestimationmethodacrosssixtypesofshots,asshown Inthispaper,wehaveintroducedDreamCinema,apioneer-
inTab.1.Wetestedonover80classicshotscollectedfrom ingcinematictransferframeworkdesignedtosimplifyand
the internet. As shown in Table 1, i) DROID-SLAM [71] make film creation more accessible. First,we extract cine-
andiNeRF[81]failedtoreconstructthescenes,particularly maticelements(i.e.humanandcamerapose)andoptimize
performingpoorlyonMPJPE.Thisismainlybecausethey thecameratrajectorywithmotion-awareguidanceandphysi-
didnotdecouplethecomplexforegroundmotionandcamera calmodeling(i.e.Beziercurve).Next,wedesignacharacter
motion.ii)Ourmethodachievedthebestresults,showing generator with human structure prior, capable of creating
improvements over CineTrans [25], primarily due to the high-qualitycharacterstailoredtouserpreferences.Tothe
introductionofmotion-awareguidanceandexplicitphysical bestofourknowledge,thisisthefirstattempttointegrate
modelingofthecameratrajectory. generativemodelsintoacinematictransferframework.Fi-
AbalationStudy.Tofurtherinvestigatethethreeguidanceof nally,wedevelopastructure-guidedmotiontransferstrategy
cinematicelementextraction,weconductedablationstudies toseamlesslyincorporatethegeneratedcharactersintofilms
on each, as shown in Tab. 2. The results indicate that the andtransferthemvia3Dgraphicsengines.WithourDream-
Instance-awarelossplaysadecisiverolebecauseitprovides Cinemaframework,userscaneffortlesslyextractcinematic
pixel-level supervision, enabling precise control over the elementsfromclassicshotsandgeneratetheirdesiredchar-
camaraparametersoptimization.TheSemantic-awareloss acters,empoweringthemtobecometheirownfilmmakers.
effectivelyimprovesMPJPE,asitguidesthemodeltofocus LimitationsandFutureWork.AlthoughourDreamCin-
onthejointsintheSMPLtracks,whichnotonlyenhances emaprovidesawayforuserstoefficientlycreatetheirown
8movies,thecinematicelementsextractionmethodwebuilt [12] Joseph Cho, Fachrina Dewi Puspitasari, Sheng Zheng,
onisspecificallydesignedforcharacter-focusedshots,which JingyaoZheng,Lik-HangLee,Tae-HoKim,ChoongSeon
somewhatlimitsthescopeofourapplication.Welookfor- Hong,andChaoningZhang. Soraasanagiworldmodel?a
wardtointegratingamoregeneralcameraposeestimation completesurveyontext-to-videogeneration. arXivpreprint
arXiv:2403.05131,2024. 1
methodandmotionextractionapproachsuitableforawider
[13] BastianCleve´. Filmproductionmanagement:Howtobudget,
rangeofobjectswithinourframework.
organizeandsuccessfullyshootyourfilm. Routledge,2017.
1
References
[14] Florinel-AlinCroitoru,VladHondru,RaduTudorIonescu,
[1] Thiemo Alldieck, Hongyi Xu, and Cristian Sminchisescu. andMubarakShah. Diffusionmodelsinvision:Asurvey.
imghum:Implicitgenerativemodelsof3dhumanshapeand IEEETransactionsonPatternAnalysisandMachineIntelli-
articulatedpose. InProceedingsoftheIEEE/CVFInterna- gence,2023. 1
tional Conference on Computer Vision, pages 5461–5470, [15] RyanCushman. OpensourcerigginginBlender:Amodular
2021. 2 approach. PhDthesis,ClemsonUniversity,2011. 5
[2] AdrianAzzarelli,NantheeraAnantrasirichai,andDavidR [16] HughDurrant-WhyteandTimBailey. Simultaneouslocal-
Bull. Reviewing intelligent cinematography: Ai re- izationandmapping:parti. IEEErobotics&automation
searchforcamera-basedvideoproduction. arXivpreprint magazine,13(2):99–110,2006. 1,2
arXiv:2405.05039,2024. 3 [17] Lin Geng Foo, Hossein Rahmani, and Jun Liu. Aigc
[3] FanBao,ChendongXiang,GangYue,GuandeHe,Hongzhou for various data modalities: A survey. arXiv preprint
Zhu,KaiwenZheng,MinZhao,ShilongLiu,YaoleWang, arXiv:2308.14177,2023. 1
andJunZhu. Vidu:ahighlyconsistent,dynamicandskilled [18] ShubhamGoel,GeorgiosPavlakos,JathushanRajasegaran,
text-to-videogeneratorwithdiffusionmodels. arXivpreprint AngjooKanazawa,andJitendraMalik.Humansin4d:Recon-
arXiv:2405.04233,2024. 1,3 structingandtrackinghumanswithtransformers. InProceed-
[4] SueBlackmanandSueBlackman. Riggingwithmixamo. ingsoftheIEEE/CVFInternationalConferenceonComputer
UnityforAbsoluteBeginners,pages565–573,2014. 5,14 Vision,pages14783–14794,2023. 3
[5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel [19] RobertoHenschel,LevonKhachatryan,DaniilHayrapetyan,
Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang,
Zion English, Vikram Voleti, Adam Letts, et al. Stable ShantNavasardyan,andHumphreyShi. Streamingt2v:Con-
videodiffusion:Scalinglatentvideodiffusionmodelstolarge sistent,dynamic,andextendablelongvideogenerationfrom
datasets. arXivpreprintarXiv:2311.15127,2023. 3 text. arXivpreprintarXiv:2403.14773,2024. 1
[6] FedericaBogo,AngjooKanazawa,ChristophLassner,Peter [20] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu-
Gehler,JavierRomero,andMichaelJBlack. Keepitsmpl: sionprobabilisticmodels. Advancesinneuralinformation
Automaticestimationof3dhumanposeandshapefromasin- processingsystems,33:6840–6851,2020. 1,3
gleimage. InComputerVision–ECCV2016:14thEuropean [21] FangzhouHong,ZhaoxiChen,YushiLan,LiangPan,and
Conference, Amsterdam, The Netherlands, October 11-14, ZiweiLiu. Eva3d:Compositional3dhumangenerationfrom
2016,Proceedings,PartV14,pages561–578.Springer,2016. 2dimagecollections.arXivpreprintarXiv:2210.04888,2022.
2 2
[7] Gary Bradski. The opencv library. Dr. Dobb’s Journal: [22] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang
SoftwareToolsfortheProfessionalProgrammer,25(11):120– Cai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-
123,2000. 5 drivengenerationandanimationof3davatars. arXivpreprint
[8] TimBrooks,BillPeebles,ConnorHolmes,WillDePue,Yufei arXiv:2205.08535,2022. 2,5
Guo,LiJing,DavidSchnurr,JoeTaylor,TroyLuhman,Eric [23] YukunHuang,JiananWang,AilingZeng,HeCao,Xianbiao
Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Qi,YukaiShi,Zheng-JunZha,andLeiZhang. Dreamwaltz:
Videogenerationmodelsasworldsimulators. 2024. 1,3 Makeascenewithcomplex3danimatableavatars. Advances
[9] YukangCao,Yan-PeiCao,KaiHan,YingShan,andKwan- inNeuralInformationProcessingSystems,36,2024. 2,5
YeeKWong. Dreamavatar:Text-and-shapeguided3dhu- [24] Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai,
manavatargenerationviadiffusionmodels. arXivpreprint MingmingHe,DongdongChen,andJingLiao. Avatarcraft:
arXiv:2304.00916,2023. 2 Transformingtextintoneuralhumanavatarswithparameter-
[10] JingqiangChen. Budgetingandcostcontrolinfilmproduc- izedshapeandposecontrol.InProceedingsoftheIEEE/CVF
tion:Balancingcreativityandfinancialviability. Highlights InternationalConferenceonComputerVision,pages14371–
inBusiness,EconomicsandManagement,22:187–192,2023. 14382,2023. 2
1 [25] XuekunJiang,AnyiRao,JingboWang,DahuaLin,andBo
[11] Joseph Cho, Fachrina Dewi Puspitasari, Sheng Zheng, Dai.Cinematicbehaviortransfervianerf-baseddifferentiable
JingyaoZheng,Lik-HangLee,Tae-HoKim,ChoongSeon filming. arXivpreprintarXiv:2311.17754,2023. 1,2,3,4,7,
Hong,andChaoningZhang. Soraasanagiworldmodel?a 8,14
completesurveyontext-to-videogeneration. arXivpreprint [26] Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si,
arXiv:2403.05131,2024. 3 DahuaLin,YuQiao,ChenChangeLoy,andZiweiLiu.Video-
9booth:Diffusion-basedvideogenerationwithimageprompts. [40] YuanLiu,ChengLin,ZijiaoZeng,XiaoxiaoLong,Lingjie
arXivpreprintarXiv:2312.00777,2023. 1 Liu,TakuKomura,andWenpingWang.Syncdreamer:Gener-
[27] AngjooKanazawa,MichaelJBlack,DavidWJacobs,and atingmultiview-consistentimagesfromasingle-viewimage.
Jitendra Malik. End-to-end recovery of human shape and arXivpreprintarXiv:2309.03453,2023. 13
pose. InProceedingsoftheIEEEconferenceoncomputer [41] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao,
visionandpatternrecognition,pages7122–7131,2018. 2 RuoxiChen,ZhengqingYuan,YueHuang,HanchiSun,Jian-
[28] LadislavKavanandJiˇr´ıZˇa´ra. Sphericalblendskinning:a fengGao,etal. Sora:Areviewonbackground,technology,
real-timedeformationofarticulatedmodels. InProceedings limitations,andopportunitiesoflargevisionmodels. arXiv
ofthe2005symposiumonInteractive3Dgraphicsandgames, preprintarXiv:2402.17177,2024. 1
pages9–16,2005. 5 [42] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao,
[29] Ladislav Kavan, Steven Collins, Jiˇr´ı Zˇa´ra, and Carol RuoxiChen,ZhengqingYuan,YueHuang,HanchiSun,Jian-
O’Sullivan. Skinning with dual quaternions. In Proceed- fengGao,etal. Sora:Areviewonbackground,technology,
ingsofthe2007symposiumonInteractive3Dgraphicsand limitations,andopportunitiesoflargevisionmodels. arXiv
games,pages39–46,2007. 5 preprintarXiv:2402.17177,2024. 1
[30] BernhardKerbl,GeorgiosKopanas,ThomasLeimku¨hler,and [43] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,
GeorgeDrettakis.3dgaussiansplattingforreal-timeradiance ZhiyangDou,LingjieLiu,YuexinMa,Song-HaiZhang,Marc
fieldrendering. ACMTransactionsonGraphics,42(4):1–14, Habermann, Christian Theobalt, et al. Wonder3d: Single
2023. 2 imageto3dusingcross-domaindiffusion. arXivpreprint
[31] NikosKolotouros,GeorgiosPavlakos,MichaelJBlack,and arXiv:2310.15008,2023. 2,5,13
KostasDaniilidis. Learningtoreconstruct3dhumanpose [44] MatthewLoper,NaureenMahmood,JavierRomero,Gerard
andshapeviamodel-fittingintheloop. InProceedingsof Pons-Moll, and Michael J Black. Smpl: A skinned multi-
theIEEE/CVFinternationalconferenceoncomputervision, personlinearmodel. InSeminalGraphicsPapers:Pushing
pages2252–2261,2019. 2 theBoundaries,Volume2,pages851–866.2023. 1,2,3,5,
[32] Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Ed- 13,14
uard Bazavan, Mihai Fieraru, and Cristian Sminchisescu. [45] YiMa,StefanoSoatto,JanaKosˇecka´,andShankarSastry.
Dreamhuman:Animatable3davatarsfromtext. Advancesin Aninvitationto3-dvision:fromimagestogeometricmodels.
NeuralInformationProcessingSystems,36,2024. 2 Springer,2004. 3
[33] ChristophLassner,JavierRomero,MartinKiefel,Federica [46] YiweiMa,ZhekaiLin,JiayiJi,YijunFan,XiaoshuaiSun,
Bogo,MichaelJBlack,andPeterVGehler. Unitethepeople: andRongrongJi. X-oscar:Aprogressiveframeworkforhigh-
Closingtheloopbetween3dand2dhumanrepresentations. qualitytext-guided3danimatableavatargeneration. arXiv
InProceedingsoftheIEEEconferenceoncomputervision preprintarXiv:2405.00954,2024. 5
andpatternrecognition,pages6050–6059,2017. 2 [47] ZeMa,DaquanZhou,Chun-HsiaoYeh,Xue-SheWang,Xi-
[34] ChenghaoLi,ChaoningZhang,AtishWaghwase,Lik-Hang uyuLi,HuanruiYang,ZhenDong,KurtKeutzer,andJiashi
Lee, Francois Rameau, Yang Yang, Sung-Ho Bae, and Feng.Magic-me:Identity-specificvideocustomizeddiffusion.
ChoongSeonHong. Generativeaimeets3d:Asurveyon arXivpreprintarXiv:2402.09368,2024. 1
text-to-3dinaigcera.arXivpreprintarXiv:2305.06131,2023. [48] SeyedMojtabaMarvasti-Zadeh,LiCheng,HosseinGhanei-
1 Yakhdan, and Shohreh Kasaei. Deep learning for visual
[35] JiefengLi,SiyuanBian,ChaoXu,GangLiu,GangYu,and tracking:Acomprehensivesurvey. IEEETransactionson
CewuLu. D&d:Learninghumandynamicsfromdynamic IntelligentTransportationSystems,23(5):3943–3968,2021.
camera. InEuropeanConferenceonComputerVision,pages 3
479–496.Springer,2022. 2 [49] JohnMateer. Digitalcinematography:evolutionofcraftor
[36] FangfuLiu,DiankunWu,YiWei,YongmingRao,andYueqi revolutioninproduction? JournalofFilmandVideo,66(2):
Duan. Sherpa3d:Boostinghigh-fidelitytext-to-3dgeneration 3–14,2014. 1,3
viacoarse3dprior. arXivpreprintarXiv:2312.06655,2023. [50] Jordi McKenzie. The economics of movies: A literature
2 survey. JournalofEconomicSurveys,26(1):42–70,2012. 1
[37] Fangfu Liu, Hanyang Wang, Weiliang Chen, Haowen [51] Jordi McKenzie. The economics of movies (revisited): A
Sun, and Yueqi Duan. Make-your-3d: Fast and consis- surveyofrecentliterature. JournalofEconomicSurveys,37
tent subject-driven 3d content generation. arXiv preprint (2):480–525,2023. 1
arXiv:2403.09625,2024. 2 [52] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
[38] JianLiu,XiaoshuiHuang,TianyuHuang,LuChen,Yuenan JonathanTBarron,RaviRamamoorthi,andRenNg. Nerf:
Hou,ShixiangTang,ZiweiLiu,WanliOuyang,Wangmeng Representingscenesasneuralradiancefieldsforviewsyn-
Zuo, Junjun Jiang, et al. A comprehensive survey on 3d thesis. CommunicationsoftheACM,65(1):99–106,2021. 1,
contentgeneration. arXivpreprintarXiv:2402.01166,2024. 3
1 [53] Albert Mosella-Montoro and Javier Ruiz-Hidalgo. Skin-
[39] XianLiu,XiaohangZhan,JiaxiangTang,YingShan,Gang ningnet:Two-streamgraphconvolutionalneuralnetworkfor
Zeng,DahuaLin,XihuiLiu,andZiweiLiu. Humangaus- skinningpredictionofsyntheticcharacters. InProceedingsof
sian:Text-driven3dhumangenerationwithgaussiansplatting. theIEEE/CVFConferenceonComputerVisionandPattern
arXivpreprintarXiv:2311.17061,2023. 2 Recognition,pages18593–18602,2022. 5
10[54] TonyMullen. Masteringblender. JohnWiley&Sons,2011. [67] JiamingSong,ChenlinMeng,andStefanoErmon. Denoising
5 diffusionimplicitmodels. arXivpreprintarXiv:2010.02502,
[55] ThomasMu¨ller,AlexEvans,ChristophSchied,andAlexander 2020. 1,3
Keller. Instantneuralgraphicsprimitiveswithamultiresolu- [68] RuiSun,YuminZhang,TejalShah,JiaohaoSun,Shuoying
tionhashencoding. ACMtransactionsongraphics(TOG), Zhang,WenqiLi,HaoranDuan,andBoWei.Fromsorawhat
41(4):1–15,2022. 1 wecansee:Asurveyoftext-to-videogeneration. 1
[56] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, [69] JiaxiangTang,ZhaoxiChen,XiaokangChen,TengfeiWang,
TimoBolkart,AhmedAAOsman,DimitriosTzionas,and GangZeng,andZiweiLiu. Lgm:Largemulti-viewgaussian
MichaelJBlack.Expressivebodycapture:3dhands,face,and modelforhigh-resolution3dcontentcreation. arXivpreprint
bodyfromasingleimage. InProceedingsoftheIEEE/CVF arXiv:2402.05054,2024. 2
conferenceoncomputervisionandpatternrecognition,pages [70] ZacharyTeedandJiaDeng. Droid-slam:Deepvisualslam
10975–10985,2019. 1,2,3 formonocular,stereo,andrgb-dcameras.Advancesinneural
[57] WilliamPeeblesandSainingXie. Scalablediffusionmodels informationprocessingsystems,34:16558–16569,2021. 2,8
withtransformers. InProceedingsoftheIEEE/CVFInter- [71] ZacharyTeedandJiaDeng. Droid-slam:Deepvisualslam
nationalConferenceonComputerVision,pages4195–4205, formonocular,stereo,andrgb-dcameras.Advancesinneural
2023. 3 informationprocessingsystems,34:16558–16569,2021. 7,8,
14
[58] GerardPons-Moll,FrancescMoreno-Noguer,EnricCorona,
[72] GarvitaTiwari,DimitrijeAntic´,JanEricLenssen,Nikolaos
andAlbertPumarola. D-nerf:Neuralradiancefieldsfordy-
Sarafianos, Tony Tung, and Gerard Pons-Moll. Pose-ndf:
namicscenes. In2021IEEE/CVFConferenceonComputer
Modelinghumanposemanifoldswithneuraldistancefields.
VisionandPatternRecognition(CVPR).IEEE,2021. 5,13
InEuropeanConferenceonComputerVision,pages572–589.
[59] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and
Springer,2022. 2
Francesc Moreno-Noguer. D-nerf: Neural radiance fields
[73] PengWang,LingjieLiu,YuanLiu,ChristianTheobalt,Taku
fordynamicscenes. InProceedingsoftheIEEE/CVFCon-
Komura,andWenpingWang. Neus:Learningneuralimplicit
ferenceonComputerVisionandPatternRecognition,pages
surfacesbyvolumerenderingformulti-viewreconstruction.
10318–10327,2021. 2
arXivpreprintarXiv:2106.10689,2021. 2
[60] ArMoheshRadhakrishnan. Ismidjourney-aithenewanti-
[74] XintaoWang,LiangbinXie,ChaoDong,andYingShan.Real-
heroofarchitecturalimagery&creativity? GSJ,11(1):94–
esrgan:Trainingreal-worldblindsuper-resolutionwithpure
104,2023. 5
syntheticdata.InProceedingsoftheIEEE/CVFinternational
[61] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,
conferenceoncomputervision,pages1905–1914,2021. 5,
NatanielRuiz,BenMildenhall,ShiranZada,KfirAberman,
13
MichaelRubinstein,JonathanBarron,etal. Dreambooth3d:
[75] XiWang,RobinCourant,JingleiShi,EricMarchand,and
Subject-driventext-to-3dgeneration. InProceedingsofthe
MarcChristie. Jaws:justawildshotforcinematictransfer
IEEE/CVF International Conference on Computer Vision,
inneuralradiancefields. InProceedingsoftheIEEE/CVF
pages2349–2359,2023. 2
Conference on Computer Vision and Pattern Recognition,
[62] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo
pages16933–16942,2023. 1,2,3,4
Kanazawa,andJitendraMalik.Trackingpeoplebypredicting
[76] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang
3d appearance, location and pose. In Proceedings of the
Wang,YatingHu,YueqiDuan,andKaishengMa. Unique3d:
IEEE/CVF Conference on Computer Vision and Pattern
High-qualityandefficient3dmeshgenerationfromasingle
Recognition,pages2740–2749,2022. 2
image. arXivpreprintarXiv:2405.20343,2024. 5,13
[63] RMTG Udula Rathnayake. An analytical study of sergei [77] ZhenXing,QijunFeng,HaoranChen,QiDai,HanHu,Hang
eisenstein’smontagetheoryanditsapplicationinsrilankan Xu, Zuxuan Wu, and Yu-Gang Jiang. A survey on video
cinema. 1 diffusionmodels. arXivpreprintarXiv:2310.10647,2023. 1,
[64] Robin Rombach, Andreas Blattmann, Dominik Lorenz, 3
Patrick Esser, and Bjo¨rn Ommer. High-resolution image [78] YufeiXu,JingZhang,QimingZhang,andDachengTao. Vit-
synthesis with latent diffusion models. In Proceedings of pose:Simplevisiontransformerbaselinesforhumanpose
theIEEE/CVFconferenceoncomputervisionandpattern estimation. AdvancesinNeuralInformationProcessingSys-
recognition,pages10684–10695,2022. 5,13 tems,35:38571–38584,2022. 4,5
[65] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, [79] Zhan Xu, Yang Zhou, Evangelos Kalogerakis, Chris Lan-
MichaelRubinstein,andKfirAberman. Dreambooth:Fine dreth,andKaranSingh.Rignet:Neuralriggingforarticulated
tuningtext-to-imagediffusionmodelsforsubject-drivengen- characters. arXivpreprintarXiv:2005.00559,2020. 5
eration.InProceedingsoftheIEEE/CVFConferenceonCom- [80] VickieYe,GeorgiosPavlakos,JitendraMalik,andAngjoo
puterVisionandPatternRecognition,pages22500–22510, Kanazawa. Decoupling human and camera motion from
2023. 2 videos in the wild. In Proceedings of the IEEE/CVF con-
[66] JohannesLSchonbergerandJan-MichaelFrahm. Structure- ferenceoncomputervisionandpatternrecognition,pages
from-motionrevisited. InProceedingsoftheIEEEconfer- 21222–21232,2023. 1,2,3,5,13
enceoncomputervisionandpatternrecognition,pages4104– [81] LinYen-Chen,PeteFlorence,JonathanTBarron,Alberto
4113,2016. 1 Rodriguez,PhillipIsola,andTsung-YiLin. inerf:Inverting
11neuralradiancefieldsforposeestimation. In2021IEEE/RSJ
InternationalConferenceonIntelligentRobotsandSystems
(IROS),pages1323–1330.IEEE,2021. 1,3,8
[82] ShenghaiYuan,JinfaHuang,YujunShi,YongqiXu,Ruijie
Zhu,BinLin,XinhuaCheng,LiYuan,andJieboLuo. Mag-
ictime:Time-lapsevideogenerationmodelsasmetamorphic
simulators. arXivpreprintarXiv:2404.05014,2024. 1
[83] YeYuan,UmarIqbal,PavloMolchanov,KrisKitani,andJan
Kautz. Glamr:Globalocclusion-awarehumanmeshrecovery
with dynamic cameras. In Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages
11038–11049,2022. 2
[84] ChenshuangZhang,ChaoningZhang,MengchunZhang,and
InSoKweon. Text-to-imagediffusionmodelingenerativeai:
Asurvey. arXivpreprintarXiv:2303.07909,2023. 1
[85] HongwenZhang,YatingTian,XinchiZhou,WanliOuyang,
YebinLiu,LiminWang,andZhenanSun. Pymaf:3dhuman
poseandshaperegressionwithpyramidalmeshalignment
feedbackloop.InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pages11446–11456,2021.
2
[86] JiwenZhang.C-be´ziercurvesandsurfaces.GraphicalModels
andImageProcessing,61(1):2–15,1999. 4,13
[87] LvminZhang,AnyiRao,andManeeshAgrawala. Adding
conditional control to text-to-image diffusion models. In
ProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages3836–3847,2023. 13
[88] PengyuanZhou,LinWang,ZhiLiu,YanbinHao,PanHui,
SasuTarkoma,andJussiKangasharju.Asurveyongenerative
aiandllmforvideogeneration,understanding,andstreaming.
arXivpreprintarXiv:2404.16038,2024. 1,3
[89] Shangchen Zhou, Chongyi Li, Kelvin CK Chan, and
ChenChangeLoy. Propainter:Improvingpropagationand
transformer for video inpainting. In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pages10477–10486,2023. 5
[90] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui
Xu,XunCao,YaoYao,HaoZhu,andSiyuZhu. Champ:
Controllableandconsistenthumanimageanimationwith3d
parametricguidance. arXivpreprintarXiv:2403.14781,2024.
1
[91] YutaoZhu,RuihuaSong,ZhichengDou,Jian-YunNie,and
JinZhou. Scriptwriter:Narrative-guidedscriptgeneration.
arXivpreprintarXiv:2005.10331,2020. 3
[92] ZihanZhu,SongyouPeng,ViktorLarsson,WeiweiXu,Hujun
Bao,ZhaopengCui,MartinROswald,andMarcPollefeys.
Nice-slam:Neuralimplicitscalableencodingforslam. In
ProceedingsoftheIEEE/CVFConferenceonComputerVi-
sionandPatternRecognition,pages12786–12796,2022. 1,
2
[93] Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li,
Ding Liang, Yan-Pei Cao, and Song-Hai Zhang. Triplane
meets gaussian splatting: Fast and generalizable single-
view 3d reconstruction with transformers. arXiv preprint
arXiv:2312.09147,2023. 2
12Appendix generate the multi-view images from a single wild image
tailored touser preference. Then, wegenerate theircorre-
A.MoreImplementationDetails
spondingnormalmapsfromanormaldiffusion.Finally,we
apply our efficient mesh reconstruction algorithm to pro-
Inthissection,weprovidemoreimplementationdetailsof
ducehigh-qualitycharactermeshes.Nowweintroducethe
ourmethod.
processindetail.Westartwiththeinitializationofthepre-
Cinematic Element Extraction. We employ the
trained2DdiffusionmodelusingthecheckpointofStable
SLAMHR [80] method for 4D human reconstruction
Diffusion [64] and fine-tune it to a multi-view diffusion
toobtaintheSMPLtracks.Wethenrandomlysamplethe
model like [43] to generate four orthographic multi-view
camera positions within a sphere of radius 1 to train the
imageswith256resolution.Thenwefinetuneanormaldiffu-
D-NeRF [58] model. For each video, we train the NeRF
sionmodelfollowingconventionaldiffusiontrainingprocess,
for30,000iterations.Wedevelopauserinterface(UI)that
whichiscapableofgeneratingcorrespondingnormalmaps
allows the user to adjust the initial camera pose. This UI
with multi-view RGB images. Next, we finetune a multi-
enablesselectiveadjustmentoftheinitialcameraposeby
viewawareControlNet[87]tolift256resolutionto512with
matching the rendered image with the initial SMPL track
moreprecisemulti-viewresults.Givenmulti-viewcolorim-
(although this step is optional). We optimize the initial
ageswiththeirnormalmapsat512resolution,weemploy
camera pose for 300 steps using instance-aware loss and
asingle-viewsuper-resolutionmodelReal-ESRGAN[74]
semantic-aware loss, as described in Sec. 3.2. Given the
toachieve2048resolutionwithouthurtingthemulti-view
initialpose,weapproximatethecameraparametersusing
consistency.Wefinetuneourmodelvia8NVIDIAA6000
two-layerMLPs,formulatedas
(48GB)GPUsfor4days.
v,w=f (t),f (t). (12) Incontrasttotheconventionalmeshreconstructionmeth-
θ β
odsfromimplicitfieldsusedbyrecentimage-to-3Dframe-
Both f (t) and f (t) are two-layer MLPs with an input
θ β worklike[40,43]whicharelimitedbylowresolution.We
dimension of 1 and an output dimension of 3. For each
optimizethemeshexplicitlyfromthecolorandnormalim-
timestep,wetraintheMLPfor80iterations,usinginstance-
ages.Tofullyleveragethehumaninformation,wechoose
awareloss,semantic-awareloss,andmotion-awareloss,with
thehumanstructureinSMPL[44]asourcoarsemeshini-
allweightssetto1.Wethenmodeltheoptimizedcamera
tialization. Then we iteratively optimize the mesh model
trajectorywithaBeziercurve[86],formulatedasfollows:
tominimizethelossfunctionwhichconsistsoftwolosses:
masklossandnormalloss.Wecomputemasklossas:
N (cid:18) (cid:19)
(cid:88) N
B(t)= (1−t)N−itic∗. (13)
i=0
i i
L mask
=(cid:88)(cid:13)
(cid:13) (cid:13)Mˆ i−M
ipred(cid:13)
(cid:13)
(cid:13)2
, (16)
2
Specifically,weuseaquarticBe´ziercurve[86],formulated i
asfollows:
whereMˆ istherenderedmaskunderviewiandMpred is
i i
C(t)=(1−t)4P +4(1−t)3tP +6(1−t)2t2P thepredictedmaskfromprevioussubsectionunderviewi.
0 1 2
+4(1−t)t3P +t4P , Themask-basedlossregulatesthemeshcontour.Addition-
3 4
ally,wecomputenormallossas:
(14)
where C(t) represents the camera position at time t, L =(cid:88) Mpred⊙(cid:13) (cid:13)Nˆ −Npred(cid:13) (cid:13)2 . (17)
normal i (cid:13) i i (cid:13)
P ,P ,P ,P ,P are the control points that define the 2
0 1 2 3 4 i
curve,trangesfrom0to1.Specifically,weusethebeginand
theendpointoftheoptimizedcameratrajectory{C t}T t=1, WedenotetherenderednormalmapasNˆ ioftheobjectand
C 0,C T asthefirstandthelastcontrolpoints.Thenweset thepredictednormalmapasN ipred,optimizingthenormal
thecontrolpointsP 1,P 2,andP 3 asthelearnablecontrol directioninthevisibleareas,where⊙denoteselement-wise
points,andoptimizethemwith{C t}T t=− 21.Wesettheopti- production.Duetotherequirementsoftheintricatedetails
mizationprocessfor300epochs.Theoptimizationobjective (e.g. face and body) from character, we devise a explicit
isformulatedas: mesh reconstruction algorithm as depicted in Equation 9
withacorrespondingrefiningoptimizationloss:
T−1 (cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
(cid:88) t t
{c∗}T−1 =argmin MSE B ,f .
t t=1 ct
t=2
T β T L
refine
=L recon+L mask. (18)
(15)
3DCharacterGeneration.Inour3Dcharactergeneration, Suchareconstructionissuitableforcharacterreconstruction,
wechooseUnique3D[76]asourbaseframework.Wefirst especiallyindetailedtexturemodeling.
13CinematicTransferOptimization.Themotionoftheani- Table3.UserStudy.Comparisononnewfilmscreationinseven-
matablecharacterwithlinearblendskinningcanbeformu- pointLikertscale(lowest-highest:1-7).
latedasfollows:
Methods SLAM+SMPL CinematicTrans Ours
(cid:88) (cid:88)
v′ = w T v s.t. w =1, (19) cameramov. 4.7±1.3 5.9±0.8 6.1±0.4
i ik k i ik
char.motion 5.5±1.0 5.7±0.7 5.9±0.6
k k
char.quality 3.3±0.5 3.4±0.4 6.3±0.5
wherev irepresentstheoriginalvertexpositions,T kdenotes char.cam.harmony 3.8±1.3 4.2±0.9 6.2±0.6
thetransformationmatricesforeachbone,andw arethe
ik
skinningweights.Theweightedsumensuresthatthenew
vertexpositionsv′ arecorrectlyinfluencedbythetransfor- methodsincharacterquality,demonstratingthenecessityof
i
mations, maintaining the realistic motion of the character. integratingacharactergenerativemodelintoourcinematic
For the skeletal binding, we utilized Mixamo to bind the transferframework.
skeletonandadjustittotheT-pose,whilescalingthemesh
E.SocialImpact
tomatchtheskeletoninthemotionsequenceaccordingto
thestructure.Forthetonalharmony,wesettheτ to0.97
min Positive Impacts: The DreamCinema framework can
andtheτ to1.03.Allourexperimentswereperformedon
max achieve efficient and effective cinematic transfer, making
asingleA6000.
it easier for artists to produce high-quality cinematic con-
tentwithlesseffort.Thiscanleadtoincreasedinnovation
B.Baselines
andasurgeincreativeapplicationsacrossvariousindustries
In the User Study, for the baseline methods, DROID- includingfilm,gaming,andvirtualreality.
SLAM[71]+SMPL[44]andCineTrans[25],sincetheylack Negative Impacts: On the flip side, the ease of creating
thecapabilitytogeneratecharactersbasedonuserrequire- high-qualitycinematiccontentraisesconcernsaboutpoten-
ments,weutilizedSMPL[44]andcharacterscollectedfrom tialmisuse,suchascreatingmisleadingmediaordeepfakes.
onlinesources(e.g.Mixamo[4])tocreatenewvideosfor Additionally,thiscouldleadtojobdisplacementfortradi-
them,followingthesettinginCineTrans[25]. tionalfilmmakersandeditors.Theremayalsobechallenges
relatedtointellectualpropertyandprivacyifthetechnology
C.Metrics isusedirresponsibly.
Weusedthreemetricstoevaluatethecinemascenerestora-
tion: a) Pixel Accuracy (PA): The percentage of pixels in
thebinarysegmentationimagethatareaccuratelyclassified.
b) Intersection over Union (IoU): The area of overlap be-
tween the predicted characters segmentation map and the
groundtruth,dividedbytheareaoftheirunion.c)MeanPer
JointPositionError(MPJPE):ThemeanEuclideandistance
betweenthepredictedjointsandthegroundtruth.
Ourstudyfocusedonfouraspects:a)CameraMovement
Alignment.b)CharacterMotionSmoothness.c)Character
Qualityd)CharacterCinematographyHarmony.
D.MoreResults
MoreQualitativeResultsWeshowmorequalitativeresults
ofourDreamCinemainFigure6.
User Study. To further investigate whether our cinematic
transfermeetsuserpreferences,weconductedauserstudy
with30volunteersusing32newlycreatedshotsfeaturing
50characters.AsshowninTab.3,theresultsareasfollows:
i)Fortherestorationofcinematicelements(cameramove-
mentandcharactermotion),CineTrans[25]andourmethod
outperformtheSLAM[71]+SMPL[44]method,mainlydue
tothedecouplingoftheforegroundmotionfromthecam-
eramotion.ii)Ourmethodsignificantlyoutperformsother
14(a) Original shot (b) SPML Visualization (c) Character Transfer Results (d) Character Transfer Results
Figure6.Moreexamplesofcinematictransferresults.
15