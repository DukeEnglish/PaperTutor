Identifying the Best Arm
in the Presence of Global Environment Shifts
PhurinutSrisawad,JuergenBranke andLongTran-Thanh
UniversityofWarwick,UnitedKingdom
Abstract. This paper formulates a new Best-Arm Identification suchasChristmasapproaching,theproductbeingdiscussedinatalk
problem in the non-stationary stochastic bandits setting, where the show, or a celebrity wearing the product. This is confirmed by re-
meansofallarmsareshiftedinthesamewayduetoaglobalinflu- centlypublishedexamplesofdailyempiricalmeansfrommarketing
enceoftheenvironment.Theaimistoidentifytheuniquebestarm experimentswithuniformlycollecteddata[18].Thetrendofempir-
across environmental change given a fixed total budget. While this icalmeansofallarmsispositivelycorrelated,andtheirrelativegaps
setting can be regarded as a special case of Adversarial Bandits or arequitewell-behaved.
CorruptedBandits,wedemonstratethatexistingsolutionstailoredto Identifying the best arm under such settings is challenging, be-
thosesettingsdonotfullyutilisethenatureofthisglobalinfluence, causeanarmevaluatedmoreofteninmorefavourableenvironments
andthus,donotworkwellinpractice(despitetheirtheoreticalguar- (positiveoffsets )mayappearbetterthananarmthatwasevaluated
j
antees).Toovercomethisissue,inthispaperwedevelopanovelse- more often in less favourable environments, even though the latter
lectionpolicythatisconsistentandrobustindealingwithglobalen- isbetteraccordingtotheunderlying(environment-independent)ex-
vironmentalshifts.Wethenproposeanallocationpolicy,LinLUCB, pectedrewardµ .Notethatthissettingcanbeconsideredasaspecial
i
whichexploitsinformationaboutglobalshiftsacrossallarmsineach caseofCorruptedBandits[39]andAdversarialBandits[1]wherethe
environment.Empiricaltestsdepictasignificantimprovementinour adversarycanonlycorruptrewardsofallarmswiththesamecon-
policiesagainstotherexistingmethods. stants ,andtheagentcanonlyobservewhentheadversaryattacks
j
thebandits,andotherwisejustreceivesthecorruptedfeedback.As
1 Introduction such,intheory,existingrobustBAIalgorithmsdesignedforadver-
sarial environments can be applied to our setting. However, as we
AMulti-ArmedBandit(MAB)isanabstractconceptofadecision will show later in this paper, those algorithms can be less efficient
problem,whereadecisionmakerhasachoicebetweendifferentac- comparedtoaround-robinexplorationsincetheydonotexploitin-
tions(arms),andselectinganactionyieldsastochasticreward.Best- formationaboutglobalattacksandthenoticeofcorruption.Assuch,
armidentification(BAI)[31],asub-probleminMAB,aimsatiden- weposethequestionwhetheronecandesignefficientalgorithmsthat
tifyingthebestamongalldesigns/armswithoutcaringaboutaccu- workwellinundersuchglobalenvironmentshiftsandperformbetter
mulatingregretduringtheexploration.Thestandardassumptionfor thanthetrivialround-robinpolicy.
BAI is that each arm has an underlying reward distribution that is Againstthisbackground,thispaperproposesanovelmethodthat
stationary.However,inpractice,therewarddistributionsmaychange takesadvantageofthisspecialsettingbyestimatingtheglobalshift
overtime.Onepossibleobjectiveinsuchanon-stationarysettingis from rewards across different arms and uses it to design a suitable
to track the best arm or minimise the cumulative regret over time, statisticforanalgorithmdesign.
adapting to the environmental changes, and this has been explored Our contribution and organisation: As far as we are aware, this
extensivelyintheliterature[20,3,13,11,21]. is the first paper to consider MAB in the presence of global envi-
In this paper, we consider a different problem of identifying the ronment shifts. In Section 2, we provide a formal definition of the
designthatworksbestinexpectation,acrossenvironments.Wefur- consideredproblem,thendiscussrelatedwork.ToaddresstheMAB
thermore assume that environmental changes affect the underlying problem with global environment shifts, we transform it into a re-
reward distributions of all arms in the same additive way, i.e., the gressionprobleminSection3andexplainwhyitssolutionisagood
meanoftherewarddistributionofarmiinenvironmentjcanbede- choiceforthebest-armpredictor.InSection4,weproposetheLin-
scribedasµ ij =µ i+s j.WecallthisproblemMulti-ArmedBandits LUCBallocationpolicywhichappliestheconfidenceboundbased
inthePresenceofGlobalEnvironmentShifts. on a regression estimator. Numerical experiments in Section 5 are
This is motivated by the fact that environmental changes often conducted to understand the effectiveness of the proposed shift es-
influence the reward of different actions in the same way. For ex- timatorindifferentallocationpoliciesandtoexaminehowourpro-
ample, when aiming to identify the best pricing strategy for a taxi posedLinLUCBalgorithmperformsinvariousproblemsettings.Fi-
application, customer’s willingness to pay may differ from day to nally,asummaryandideasforfutureworkwillbeprovidedinSec-
day,basedonweatherorspecificeventssuchasconcertsorfootball tion6
matches,whichmaysimilarlyinfluencetheachievableprofitforall Notation:Vectorsaredenotedbylowercaseboldfacelettersandma-
consideredpricingstrategies.Orconsideradvertisingonsocialme- tricesbyuppercaseboldfaceletters.Ingeneral,weuseasuperscript
dia, where the click-through rate of different adverts may increase oftork torefertoitsvalueattimesteptoritskth value,respec-
anddecreasesynchronouslyovertimedependingonexternaleffects
4202
guA
22
]GL.sc[
1v18521.8042:viXratively.ForanyintegerK,[K]denotes{1,..,K}.Astandardbasis arms[K].Figure1illustratesanallocationpolicyinsuchasetting
ofRdisgivenby{e (d)fori=1,..,d}wheretheithcoordinateof wheretheproblemhas5Gaussianarmswithdifferentmeansandthe
i
vectore (d)is1,otherwise0.ForamatrixA,wedenoteitstrans- samevariance.Weusetheprobabilityofincorrectselection(PICS)
i
posebyA′.Anidentitymatrixwithasizeofd×disdenotedby asaperformancemeasuretoassesstheefficiencyofpolicies.Since
I .AprobabilitymeasureisdenotedbyP.WeuseE[·]toreferto the best arm maintains its rank over environmental changes in our
d
the expectation of uni- or multi-variate random variables. V[·] and setting, the PICS is simply defined by the expectation of 0-1 loss
Cov(·,·)denotethevarianceofarandomvariableandthecovariance functionL (·,·)asfollows
0,1
betweentworandomvariables.Foramultivariaterandomvariable,
(cid:16) (cid:17)
Cov[·] denotes its covariance matrix. Denote 1[E] as an indicator PICS:=E[L (ˆi,i∗)]=P ˆi̸=i∗
0,1
functionofeventE.Wedenoteadiscreteuniformdistributionanda
continuousuniformdistributionwithparametersofminimumaand whereˆi is the arm recommended by the selection policy, and i∗ is
maximumbasU˜(a,b)andU(a,b),respectively.
thetruebestarm.Therealsoisaso-calledfixed-confidencesetting
aiming to minimise the amount of budget to achieve the specified
2 ProblemFormulation&RelatedWork PICS,butwedonotconsidersuchasettinginthispaper.
Inthissection,weformallydefinethenewK-armedstochasticbandit
probleminthepresenceofglobalenvironmentshifts.Wethenreview μi+sj 1st Environment ! 2nd Environment ! … Jth Environment
literaturerelatedtooursettingandshowhowglobalenvironmental
5.5
shiftsnegativelyaffectexistingalgorithmsforfindingthebestarm.
5
4.5
4 4
2.1 BAIwithGlobalEnvironmentShifts 3.5 s2−s1 3.5
3
G univ de en rta hefi jn ti hte ed ni vs ic rr oe nte ms ee nt to isf aa nrm i.s i.d[K ra] n, dth oe mr vew ara iard bler ,ij cof nro sm istia nr gm oi
f
AA rr mm 45 12
.5 s2−s1
2 2.5 sJ−s1
threecomponents: Arm3 1 sJ−s1
Arm2 0.5
r ij =µ i+s j+ϵ Arm1 0
whereµ ∈Risthetruequalityofarmi,s representsaglobalshift 0 1 2 3 4 5 …cp1 cp1+1cp1+2 … cp2 …cpJ−1+1cpJ−1+2 … T Round
i j
on the reward of all arms that depends on the environment j, and
Figure1:ExampleofapolicysamplingfromarmsonaBAIproblemwith
noiseisnormallydistributed,ϵ∼N(0,σ2).
globalenvironmentshifts.
Weassumethatanagentcanonlyobservetwothings:
2.2 RelatedWork
1. therewardr ifarmiischosenduringenvironmentj,and
ij
2. thetimeofanenvironmentalchange. Our problem formulation shares similarities with (but is different
from) many papers on the problem of identifying the best arm in
Notethatnoinformationabouttheenvironmentisavailable,inpar- anon-stationarysetting.Furthermore,inSection2.3,wewillexplain
ticularwecannotdirectlyobserveitsshifts j,nordowehavefeatures thatexistingalgorithmsforstationarysettingscanidentifythebest
thatdescribetheenvironmentandthatcouldbeused,e.g.,incontex- arm in the long run with a high probability under some shift con-
tualMAB.Inaddition,wesupposethatshiftsandnoiseareindepen- ditions.Therefore,wealsoreviewsomeliteratureonstationaryset-
dentanddonotmakeanyspecificassumptionsaboutthestructureof tings.
environmentshifts. BAI in a stationary setting: There is a rich literature on BAI al-
The best arm is defined as the arm with the largest expected re- gorithms which assume rewards are i.i.d, drawn from a stationary
ward,i∗ = argmax iµ i,whichisindependentoftheenvironment. distributionandmostlybounded.Infixed-confidencesettings,most
Everyenvironmentjisassumedtoremainvalidfromtimet=cp j−1 algorithms are either elimination-based or confidence-bound-based
tot = cp j,i.e.,theenvironmentispiecewisestationary.Thedura- [24], such as Exponential Gap Elimination [29], LUCB [28], and
tion during which an environment is valid may be stochastic, and Lil’UCB[26].Asimpleandefficientalgorithmforthefixed-budget
wedonotneedtoassumeanunderlyingdistributionforthelength setting is Sequential Halving (SH), which divides the total budget
ofenvironments.Note,however,thatsincethereisnopriorknowl- intomultiplephasesandhalvesthenumberofcandidatearmsaftera
edgeabouts j foreachenvironment,asingleobservedrewardunder samplingphaseends[29].Therearealsosomevariationsoftheup-
anenvironmentcannotprovideanystatisticalinformationaboutthe perconfidencebound(UCB)algorithmappliedtothistask[12,7].
arm. Such extremely short environment durations would thus have Ranking&Selection (R&S): R&S [23] is a problem class in the
tobeignoredinpractice.Instead,wehereassumethatthelengthof stochastic simulation literature, and it is closely related to BAI in
environment∆cp j :=cp j−cp j−1 ≥2forallj. MAB, although usually Gaussian reward distributions are assumed
Ineachtimestept,wecanfirstobservewhethertheenvironment [6].Inafixed-budgetsetting,mostalgorithmsarederivedeitherfrom
haschanged,andthenallocateonesampletooneofthearms.Our anequivalentproblemofthePICSminimisationwithabudgetcon-
aimistodesignanallocationpolicythatdecideswhicharmtosam- straintordynamicprogramming[35],suchastheOCBAprocedure
plenext,givenallthehistoricalinformation,andaselectionpolicy [14],0-1procedure[15]andKnowledge-Gradientpolicy[19].These
thatwillrecommendthebestarmattheendofsampling,afterhav- adaptivepoliciesallowbatchsamplinginmultiplephasesandwork
ing exhausted the available budget of T samples, i.e., we consider wellinpractice.
afixedbudgetsetting.Apolicyπ isdefinedasamappingfromse- Adversarial Bandits & Corrupted Bandits: In general, adversar-
quencesofaction-rewardinformation,includingtheenvironmentor- ialsettingsassumethatasequenceofrewards{rt}T foreacharm
i t=1
dinal, It := (j1,i1,r i1j1,...,jt−1,it−1,r it−1jt−1,jt), to a set of isdeterminedbyanadversary[9],whichresultsinarewardthatisnotarandomvariable.Therearesomevariantsofadversarialban- LinearBandits:InSection3,ourrewardmodelwillbevectorised
ditswhichmergeastochasticstructureintotheproblemformulation. asalinearfunctionoftheindexofthearmandoftheenvironment,
Forinstance,corruptedbanditsassumerewarddistributionscanbe whichiscloselyrelatedtothelinearrelationshipoffeatureandre-
attacked by an adversary which strives to trick an agent by inject- wardoflinearbandits.ForBAIinlinearbanditssetting,eacharmi
ingcontaminatedinformation[27,33].Onecantreatoursettingas isrepresentedbyaknownfeaturevectorx ∈ X ⊂ Rd,|X| = K.
i
a special case of corrupted bandits where an adversary can instead At time t, a noisy reward rt is assumed to be a linear function of
chooseasequenceofglobalshiftstofoolalearnerinadvance.Few anunknownmodelparameterθt ∈ Rd;rt = xtθt′+ϵt.Infixed-
papers consider BAI tasks in this formulation. For a fixed-budget budget settings, most of the works assume the unknown parameter
setting, [39] assume an agent can only observe corrupted rewards isfixed,θt = θ∗ forallt;therefore,thebestarmisdefinedbythe
rt =µ +st+ϵtwhereanadversaryhasaboundedtotalcorruption highestexpectedrewardmean,i∗ = argmax x θ∗′.[10]devel-
i i i i LB i i
budget,(cid:80)T max |st| ≤ S forsomeconstantS andcorrupted opstheGSEalgorithmforwhichthetotalbudgetisevenlysplitinto
i i∈[K] i
rewardsarebounded.TheyproposethePSSalgorithm,whichisan multiplephases,andaspecifiednumberofarmsiseliminatedafter
extensionoftheSHalgorithm[29]withuniformrandomisation.Be- eachphaseends.TheGSEalgorithmappliesanadaptivesamplingin
sides,BAIinacorruptedmodelisstudiedinamoregeneralwayfor eachphaseandusestheleastsquareestimatorofθ∗torankthearms
fixed-confidence settings without any strict assumptions of true re- foreliminationoftheworst.[38]proposestheOD-LinBAIalgorithm
warddistributionandcontaminateddistributionby[5].Inadversarial whichcombinestheideasoftheSHalgorithmandG-optimaldesign
bandits,theuniquebestarmoverthetotaltimehorizonT ispossibly [31]. [2] propose a variant of the SH algorithm equipped with the
undefinedwithoutrigorousassumptions.[1]assumestheuniquebest leastsquareestimatorwhichisrobusttomoderatelevelsofmisspec-
armwithrespecttothehighestcumulativerewardsexistsandstudies ification from the linear bandits model. A recent paper [36] gener-
BAIfortheBest-of-Both-worldproblem.Theyproposeanalgorithm alisestheassumptionofastaticmodelparametertoanon-stationary
P1, in which the probability of sampling each arm pt is generated setting. The goal is to find the optimal arm i∗ over the average
i
fromarankingoftheinverse-propensity-score(IPS)estimator,and model parameter θ¯T = (cid:80)T θt/T at the specified time horizon
t=1
thefinalrecommendationisanarmwiththehighestIPSestimator. T;i∗ =argmax x θ¯T.TheauthorsproposetheG-BAIalgorithm,
i i
NotethattheIPSestimatorr¯˜T :=(1/T)(cid:80)T rt/pt·1[it =i]isan whichsamplesthenextallocationbasedonG-optimaldesignandes-
i t=1 i i
unbiasedestimatoroftheaveragerewarduptotimeT.Anotherway timatesθtfromaninverse-propensityscoreestimator.FromtheBAI
todefinethebestarmisbyassumingtheconvergenceofthereward inlinearbanditsliterature,amajordifferencetolinearbanditsfrom
sequenceorlim rtexists[25,34]andtheuniversalbestarmis ourstudyisthatthedimensionalitydisfixed,whereasinoursetting,
t→∞ i
definedbythehighestlimit.Tothebestofourknowledge,noBAI thenumberofdimensions(environmentsencountered)keepsgrow-
studyinadversarialbanditsconsiderstheglobalstructureofchange, ing. In order to apply linear bandit algorithms in our setting, since
andinSection2.3,wewillempiricallyshowthatwithoutexploiting thereisnofeatureabouttheenvironmentapartfromagrowingindex
suchastructure,thesealgorithmsdonotworkwellinoursetting. ofenvironment,atabularapproachandanapproachofaveragingthe
Piecewise-stationaryBandits:Thistypeofbanditproblemisquite modelparameterwillnotbeveryeffective.
relevanttooursettingsinceitallowsmeanµt oftherewarddistri-
i
butiontoremainstationarywithinacertaintimehorizon∆cp for
j 2.3 EffectofEnvironmentChangeonExistingPolicies
j ∈[J]whereJ isthenumberofenvironmentalchangesuptotime
T. Similar to adversarial settings, the task of minimising regret is Inoursetting,theglobalshiftcanaffectpoliciesintwomajorways:
more natural to study. When environments do not change too fre-
quently,andthechangeisabrupt,therearethreegeneralapproaches 1. thebehaviouroftheadaptiveallocationpolicy,and
totacklethissetting[20,3,13,11,21]: 2. theselectionofthebest-predictedarm.
1. Resetstrategyifdriftisdetected We consider a sample mean of reward, which is one of the most
2. Discountedfactorstoreducetheimportanceofrewardsreceived commonlyusedstatisticsinBAIalgorithmssuchasSH,UCB,and
longago LUCB,includingthecriteriaoftheselectionpolicyofround-robin
3. Sliding window to only evaluate rewards from a desirable time sampling.Denoter¯ :=(cid:80)J ((cid:80)nij rk)/(cid:80)J n asthesample
i j=1 k=1 ij j=1 ij
window. meanofarmiwhereJisthelatestenvironmentduringsampling,rk
ij
isthekthrewardorarmiinenvironmentj,andn isthenumberof
Someworksalsointroducedanevolutionaryalgorithmandanadap- ij
samplesonarmiunderthejth environment.Underoursettingthe
tive allocation strategy to track the best arm under abrupt changes
differenceofsamplemeansbetweenarmi andi ,r¯ −r¯ contains
[30].WeareawareofonlyonestudyofBAIforpiecewisestationary (cid:16) 1 2 i1 i2 (cid:17)
bandits [4]. Their setting is a generalisation of adversarial settings
thetermof(cid:80)J
j=1s j n
i1j/(cid:80)J
j=1n i1j−n
i2j/(cid:80)J
j=1n i2j .From
where an adversary chooses a sequence of reward distributions in- suchacalculation,theinfluenceoftheenvironmentcanleadtobi-
steadofasequenceofrewards.Somedistributionspossiblyhavezero asedsamplemeansandbiaseddifferencesifthenumbersofsamples
variances.Thebestarmisdefinedbyi∗ = argmax (cid:80)T µt. ofeacharmundereachenvironmentaredifferent.Forexample,in
PWS i t=1 i
TheyproposetheSER3algorithmthatcombinesasuccessiveelimi- thecaseofonlyoneenvironmentchangehappeningorJ = 2,sup-
nationmechanismwithrandomisedround-robinsampling,utilisinga poseaninferiorarmi suchthatµ −µ < 0hasmoresamples
1 i1 i2
criterionderivedfromHoeffding’sinequalitytoeliminatepotentially thanasuperiorarmi inthesecondenvironmentn ≥n mean-
2 i12 i22
inferiorarmsuntilonlyonebest-predictedarmremains.Inourpaper, whileforthefirstenvironmenttheyhaveanequalnumberofsamples
driftdetectionisnotrequiredsinceweassumetheagentknowswhen n =n .Ifs issufficientlylargerthans thendecision-makers
i11 i21 2 1
thechangeoccurs.Besides,ourstudyisafixed-budgetsetting,dif- mayselectaninferiorarmduetor¯ −r¯ >0.
i1 i2
ferent to the fixed-confidence setting of [4]. But most importantly, Suchacalculationisamainissueforthesample-mean-basedfinal
weassumeglobalshiftsthataffectallarmsinthesameway,whereas selectionifanadaptiveallocationpolicyisused.Thisphenomenon
thisisnotthecaseintheotherpublications. canalsooccurinelimination-basedalgorithms,evenwhenuniformsampling is used, since the change cannot be controlled. We may
1.0
deducethatthesamplemeanisnotasuitablestatisticforbothallo-
cationpolicyandselectionpolicyifnoknowledgeabouttheshiftis 0.8
provided.However,iftheshiftsatisfiestheconditionsinCorollary
0.6
2of[17],suchasshiftisauniformrandomvariable,existingBAI
algorithmsthatsampleallarmssufficientlyunderdifferentenviron- 0.4
mentswillbeabletoidentifythebestarmwithahighprobability.
0.2
The main reason is the shift term in the sample-mean calculation
(cid:80)J
s
(cid:16)
n
/(cid:80)J
n −n
/(cid:80)J
n
(cid:17)
→ 0asJ → ∞
0.0
j=1 j i1j j=1 i1j i2j j=1 i2j 250 500 750 1000 1250 1500 1750 2000
SampleAverageE[T]
foralli∈[K],j ∈[J].
AnotherapproachistousetheIPSestimator,whichisanunbiased Round-Robin LUCB-normal EXP4P(mean) SER3 PSS
estimatorforrandomisation-basedalgorithmsinadversarialsettings.
0 −11 UCB-normal EXP4P(IPS) P1 SR
However,withthesamereasonassamplemeancalculation,insuffi-
Figure2:PICSofexistingalgorithmsfrom105 replicationsontheGaus-
sianconfigurationof5armswherethegapsoforderedarms(δ = 0.5)are
cientsamplingforsomearmsinsomeenvironmentscanstillcause equallydistributedandarmshaveequalvariance(σ = 1).Thelengthsof
abiasforrankingtheIPSestimatorsinceaprobability-weightedre- environmentsjareuniformlydistributed,∆cpj ∼U˜(2,50)andtheshiftis
wardinafavourableenvironmentcanbeexcessivewhenitiscom- arandomvariable,sj ∼U(0,20).
paredtotheoneinalessfavourableenvironment.Lastly,implement-
3 LinearRegressionforTheSelectionPolicy
ingrobustBAIalgorithmsincontaminatedbanditscouldalleviatethe
estimatorproblem,butwithoutexploitingtheglobalshiftstructure, AsexplainedinSection2.3,evenifs isbounded,asamplemean
j
thatalgorithmstillneedshighbudgetstoidentifythebestarm. ofrewardsmaynotbeanappropriatestatisticforpredictingthebest
Figure2depictshowdifferentexistingpoliciesperformunderthe arm since different arms may have been evaluated under different
presenceofglobalshiftswhentheshiftisrelativelybigincompar- environmentshifts.Inthefollowing,wederiveapointestimateby
ison to the gap between optimal arm and suboptimal arm. On the formulatingaregressionproblem.
horizontalaxis,thesampleaveragereferstothegivenbudgetT for
eachpolicyexcepttheSER3algorithm,whereitmeanstheaverage
3.1 OrdinaryLeastSquare(OLS)Estimator
oftherequirednumberofsamplestoachievedifferentPICSs.The
round-robin sampling is executed as a simple baseline. For UCB- Sinceweareonlyinterestedinidentifyingthebestarm,withoutloss
based algorithms, LUCB [24] with the sample-mean-based recom- ofgenerality,weassumethats 1 = 0.Consideringthestatedprob-
mendationandUCB[8]forminimisingcumulativeregretwiththe lemasaregressionmodel,arewardmatrix,givenatotalnumberof
most-frequency-based recommendation are implemented by using evaluationsN acrossJ environments,canberewrittenintwoways
thenormalconfidenceboundin[8].Foranalgorithminadversarial asfollows
settings, the P1 algorithm and the EXP4P algorithm [37] with dif- r=Aµ+Bs+ϵ (1)
ferentfinalrecommendationsareexecuted;oneisthesamplemean,
r=Xθ+ϵ (2)
andanotheristheIPSestimator.ForBAIincontaminatedbandits,
where
weapplythePSS(2)algorithmwithaslightmodificationbyusing
a randomised round-robin instead of uniform randomisation to en- • r := [r1 r2 ... rN]′ isacolumnvectorcontainingallrewards
sure each arm is sampled equally. In addition, we mimic such an obtainedfromN evaluations
ideabytestingtheSuccessiveRejects(SR)algorithm[7]witharan- • ϵ := [ϵ1 ϵ2 ... ϵN]′ isacorrespondingnoisevectorwhereϵ ∼
domisedround-robinsampling.Wealsoimplementthe0-1 1 proce- MN(0,σ2I N).
dure[16]fromR&Sliteraturewhichworkswellinpracticewiththe • Aisacoefficientmatrixinwhicheachrowa isavectorofthe
t
Gaussian distribution assumption. The PICS plot of these adaptive standard basis of RK referring to the chosen arm it. i.e., at =
policiesdecreasessignificantlyslowercomparedtotheround-robin e′ (it)
K
samplingwhenthebudgetishigher.SRalgorithmperformsslightly • Similarly,B isacoefficientmatrixreferringtotheenvironment
worsethanround-robinsamplingassample-mean-basedelimination ordinaljt,i.e.,eachrowb =e′ (jt−1)forj ≥2.
t J−1
criteriahavemoreriskinthissetting.Interestingly,thePICSofthe • µ := [µ ... µ ]′ isaK−dimensionalcolumnvectorcontain-
1 K
PSS algorithms show a significant difference even if they use the ingtheactualmeansofallarms.
samesamplingpolicy.Twomajorreasonsarethatfirst,eliminating • s:=[s ... s ]′isa(J −1)−dimensionalcolumnvectorcon-
2 J
halfofthecandidatearmsinthefirstphasebyusingasamplemean tainingactualshiftsrelativetothefirstenvironment.
has a higher risk of excluding the optimal arm than one-arm elim- • X =[AB]isacoefficientblockmatrix.Similarly,the(K+J−
ination,andsecondthesamplemeaninthePSSalgorithmiscom- 1)−dimensionaljointparametervectorθ=[µ′s′]′.
puted from rewards in one particular phase which is not sufficient
toreducetheinfluenceofshiftinthesamplemeancalculation.The Note that the dimensions of J −1 and K +J −1 are due to the
best policy is the SER3 algorithm, which is quite robust to global zero-valued shift s 1 assumption; therefore, such a shift will not be
change,eventhoughtheeliminationcriteriaarebuiltonthebounded estimated. Model (1) is a hybrid linear model similar to the model
rewardassumption.However,thispolicyisnotquitesuitableforuse in[32].Onedifferenceisthedimensionofourparameterss,which
in fixed-budget settings since we need to tune the hyperparameter growsby1whentransitioningtoanewenvironment,butthevalues
oftheprobabilityofselectingthebesttomatchthelimitedbudget. ofparametersinpreviousenvironmentsareunchanged.
Thisresult,hence,raisesthequestionofwhetherthereisabetteres- Tofindthesolutiontotheregressionproblem,thesecondmodel
timatorandadaptivepolicycomparedtouniform-exploration-based (2)iseasiertosolve.Basedonaleastsquaresmethod,wecanderive
sampling. auniquesolution;
θˆ=(X′X)−1X′r.
SCIPNote that such an estimator is unbiased (E[θˆ] = θ) which means sameenvironmentorthelossfunctioncanbepartitionedandopti-
that the estimated mean and shift are also unbiased; E[µˆ ] = misedseparatelysincemeanestimatorsarenotcomparable.There-
i
µ ,E[sˆ ] = s . In addition, the distribution of OLS estimators is fore,ingeneral,anyallocationpolicythatappliesregressionandis
i j j
θˆ∼MN(θ,σ2(X′X)−1),providedthatXisfixed,duetoGaus- notawareofenvironmentalchangecannotbedirectlyimplemented.
siannoiseassumption.Byusingblockmatrixinversion,wecansep- In addition, evaluating only one arm in one environment can lead
aratethesolutionforeachparameterasfollows to unchanged mean estimators since an estimated shift in such an
environment can be varied arbitrarily. To ensure the existence and
µˆ = (cid:0) A′(I−H )A(cid:1)−1A′(I−H )r uniquenessoftheregressionsolution,thereareafewrequirements
B B
sˆ = (cid:0) B′(I−H )B(cid:1)−1B′(I−H )r forallocationpolicies.
A A
whereH :=
A(cid:0) A′A(cid:1)−1A′
andH :=
B(cid:0) B′B(cid:1)−1B′.Inad-
InitialisationforRegression
A B
dition,thecovarianceofbothestimatorscanbecomputedby
Disconnectedevaluationsacrossdifferentenvironmentscancausean
Cov[µˆ] =σ2[A′(I−2H +H H H )A]−1 ill-posedoptimisationproblem.Forinstance,givena5-armssetting,
B B A B
Cov[sˆ] =σ2[B′(I−2H +H H H )B]−1. if a policy evaluates arms {1,2} under the first environment, arms
A A B A
{1,2,3} under the second environment and arms {4,5} under the
Inthecasethatacommonvarianceσ2isnotknown,anunbiased thirdenvironment,thenparametersofthelossfunctionwillbesepa-
estimatorforsuchvariancecanbecalculatedfromthefollowingfor- ratedintotwopartitionsforarms1,2,3andarms4,5.Theinforma-
mula tionshareofregressionparameters,infact,canberepresentedbya
(cid:80)J (cid:80)K (cid:80)nij (cid:0) rk −µˆ −sˆ (cid:1)2 graphwheretheverticesarearms,andtheundirectededgebetween
σˆ2 = j=1 i=1 k=1 ij i j . twoverticesexistsifthecorrespondingarmshavebeensampledun-
N −(K+J−1)
der the same environments. From the mentioned example, we can
representitwithtwosub-graphswhereoneis1−2−3−1,andan-
Consistencyofmeanestimator
otheris4−5asinthetoprowofFigure3.Sotheestimatorsofarm
The key challenge of this work is whether the mean estimator can 1andarm4arenotcomparable.Theregressionapproachrequiresa
guaranteethecorrectrankinginthelongrunsincethedimensionof connectedgraphconnectingallarmstofullyshareinformation-if
the parameter keeps growing. Due to our assumption that s = 0, thegraphisdisconnected,itisimpossibletoranksolutionsfromdif-
1
the correlation between estimated parameters is likely not to van- ferentarmsrelativetoeachother.Theinitialisationphaseiscrucial
ish,leadingtoaninconsistentmeanestimator.Nevertheless,theesti- foreveryallocationpolicytogenerateatleastatreestructure.
matedrankingismorecrucialtoidentifythebestarm;wetherefore
consider the difference between two mean estimators (ranking) in-
1 2 1 2 1 2
stead.
5 3 5 3 5 3
Theorem1. ForanypolicyunderwhichtheOLSestimatorisvalid
andallarmsaresampledinfinitelyoften,orN
:=(cid:80)J
n →∞
4 4 4
foralli,assumethatJ →∞andthereexistci onstantj s= v1 ∗,i wj ∗such 1→ 2 2→ 1→ 3 5→ 4
that0<v∗ ≤V[sˆ ],Cov(sˆ ,sˆ )≤w∗ <∞forallj ̸=m.
j j m 1 2 1 2 1 2
1) Themeanestimatorµˆ isnotconsistent.
i 5 3 5 3 5 3
2) IfJ ∈o(N)andN ,N ∈Θ(N),thedifferenceinmeanestima-
1 2 4 4 4
torsbetweenthosetwoarms,µˆ −µˆ ,isconsistent.
1 2
1→ 2 2→ 3→ 5 5→ 4
The above theorem implies that when all arms are sampled in- 1st environment 2nd environment 3rd environment
finitelyoften,themeanestimatordoesnotconvergetoitstruevalue,
Figure3:Representativegraphstructureillustrateshowanallocationpolicy
producestheevolutionofthegraphattheendofeachenvironment.
butitcanbeusedtoidentifythebestarmsincetherankingstillcon-
vergestotheactualonewhentheenvironmentchangegrowssublin- Fortheinitialisationphase,therandomisedround-robinsampling
early,andthenumberofsamplesforeacharmgrowslinearly.Inad- is modified to evaluate the last arm chosen under the previous en-
dition,theconsistencyofdifferenceholdsempiricallywithoutsuch vironmentatthestartofthenextenvironmentifallarmscannotbe
additionalassumptions.Besides,themeanestimatorandthediffer- observedwithinoneenvironment.ThepseudocodeisprovidedinAl-
enceestimatorbenefitfromrobustnessagainstenvironmentalshifts gorithm 1. For example, sequentially evaluate arm 1 → 2 (in the
sincetheirconsistencydoesnotdependontheshiftmagnitude.Due 1st environment) → 2 → 3 → 5 (in the 2nd environment), and
tothepagelimit,theproofanddiscussionareprovidedinSupple- → 5 → 4(inthe3rd environment)asinthebottomrowofFigure
mentaryMaterialsB. 3.Withsuchinitialisation,theestimatorsofarm1andarm4canbe
quantitativelycomparedthrougharm2andthenarm5.
3.2 RequirementsforRegression
Evaluatingtwofirstdistinctarmswhentheenvironment
MerelymerginganOLSestimatorwithanallocationpolicymaylead
changes
toanill-posedproblemduetoasingularityofmatrixXTX.Inlinear
bandits,thesingularityproblemisalleviatedbye.g.addingaregu- Even if an environment length is relatively short, some allocation
larisationtermintheregressionlossfunction[32,22]orapplyinga policies may evaluate only one arm under the same environment.
dimensionalityreductiontechnique[38,10].However,inoursetting, Thiswouldnotprovideanyvaluableinformationsincetheestimated
theseapproachesarenothelpfulifallarmsarenotobservedinthe shiftundersuchanenvironmentcanbeanyarbitraryvaluesubjectto(cid:113)
Algorithm1Randomisedround-robinsamplingforinitialisation γt = 16ln(t)/(cid:80)J n asanexplorationrate.Finally,these-
i j=1 ij
Require: Numberofinitialsamplesperarmn 0 ≥2 lectionpolicychoosesthehighestOLSmeanestimatorasthebest-
1: Settheinitialordinalofenvironmentj =1 predicted arm. The pseudocode of LinLUCB policy is provided in
2: SetaninitialarmsetS =[K]andshuffle. SupplementaryMaterialsA.
3: Setarmi1asthefirstindexedarminS.
4: fort=1,...,n 0Kdo
5 EmpiricalEvaluation
5: ifEnvChange==Truethen
6: Settheordinaloftheenvironment:j ←j+1 Inordertounderstandhowenvironmentalchangeinfluencesdiffer-
7: ifBuildTreeSuccess==Falsethen entpoliciesonvariousconfigurations,weconductnumericalexper-
8: Playarmit ←it−1 iments for the proposed algorithm and modified versions of some
9: else existingpolicies.Wechosetheexaminedproblemsettingsfrom[16]
10: PlayarmitfromSinanorderfollowingit−1 sinceitwasaseminalpaperdevelopingapolicyforPICSminimisa-
11: endif tionforGaussianrewards.Twoconfigurationsaremonotonedecreas-
12: else ing means (MDM) configuration and slippage configuration (SC),
13: PlayarmitfromSinanorderfollowingit−1 withamodificationbyaddingrandomshiftss ∼U(0,20).Forthe
j
14: endif MDMconfiguration,rewardsforalternativesi=1,...,Kare
15: Obtainarewardrt
16: RemovearmitfromSifitsnumberofsamplesN it =n 0 r ij
∼N(cid:0)
δ(i−1)+s
j,σ2(cid:1)
,
17: ifThelastindexedarminSisplayedthen
18: ShuffleS whilefortheSCconfiguration,rewardsare
19: BuildTreeSuccess←True
r
∼N(cid:0)
s
,σ2(cid:1)
for1≤i<K, r
∼N(cid:0)
δ+s
,σ2(cid:1)
.
20: endif ij j Kj j
21: endfor
WeusePICSastheperformancemeasureestimatedbythefraction
ofreplicationsselectingthetruebestalternativecorrectly.Forafair
thevalueoftheestimatedmean.Inotherwords,therearenoupdates
comparison, all procedures in all time steps share the same set of
inestimatorvaluesifonlyonearmisevaluatedinoneenvironment.
potentialobservationsbycontrollingrandomseeds.ThePICScon-
Inordertoavoidsuchanissue,evaluatingatleasttwodistinctarms
vergenceplotsbelowaregeneratedusing105replications.Wesetthe
oncetheenvironmentchangesisimperative.
valueofparametersintheproblemasδ=0.5andσ=1.
4 LinLUCBAllocationPolicy
5.1 Comparisonagainststandardpolicies
GivenanormaldistributionofOLSestimatorfortheactualmeansµˆ
FromtheproofofTheorem1,theuncertaintyoftheestimatedshift
attimet,theupperconfidenceboundoftheactualmeanofarmican
plays a vital role in the convergence of the mean OLS estimator.
bedefinedas
Sincetheenvironmentlengthhasasignificantinfluenceontheshift
(cid:113)
UCBt =µˆ +γt at′σ2[A′(I−2H +H H H )A]−1at estimation,wetesttheperformanceofourproposedLinUCBpolicy
i i i B B A B
againstotherexistingpoliciesinthefollowingenvironmentalchange
whereγt isanexplorationrateatatimestept.Weproposeanew scenarioswith5arms,additionalresultsondifferentscenarioscanbe
foundintheSupplementarymaterialsC.
variant of the LUCB algorithm modified from [28] for our linear
modelinAlgorithm2.TheLUCBalgorithmwasoriginallydesigned • General scenario, where ∆cp ∼ U˜(2,10K): The duration of
j
foraPACsubsetselectioninafixed-confidencesettingwheresam-
thestationaryphaseoftheenvironmentmayvaryfromveryshort
pling two arms every time step is allowed. However, we found its
torelativelylong,leadingtodifferentchallengesinestimating.
potentialtobeimplementedinafixed-budgetsetting,especiallyin • Cannot-sample-all-armsscenario,where∆cp ∼U˜(2,K−1):
j
our setting. The LUCB algorithm ensures that at least two arms
Theenvironmentisveryshortandpoliciescannotexploreallarms
are evaluated in every environment, allows for an adaptive budget
inoneenvironment
T, and is optimal in a two-armed setting with the worst environ-
mentlengthof2.ThealgorithmstartsbyexecutingAlgorithm1for Thefollowinglistdescribesthetestedpolicies:
theinitialisationphaseandthenalternatingsamples,thegreedyarm
andthemostpotentiallybestarmfromtherest,whileguaranteeing • Round-robin:round-robinsamplingwithr¯ asaselectionpolicy
i
that the two first samples in the new environment are distinct. At • 0-1 :procedureproposedin[16]andr¯ asaselectionpolicy
1 i
time t, the greedy arm is indexed based on the highest mean es- • SER3: the elimination-based algorithm from [4] for fixed-
timator lt := argmax i∈[K]µˆ i, in which ties are broken arbitrar- confidence piecewise-stationary bandits where prior knowledge
ily, then the rest of arms are filtered to find the highest UCB arm abouttheoptimalgapµ −max µ isprovided
1 i̸=1 i
ut := argmax UCBt.Inthispart,sinceoursettingdoes • LinLUCB:Ourproposedmethod(Section4)
i∈[K]\{lt} i
not allow the sampling of two arms in one time step, we mimic
the batch sampling by sequentially selecting lt and ut instead of Following [16], 0-1 and LinLUCB first perform an initialisation
1
using the interleaving strategy. If there is an environment change phase with n = 6 samples with a round-bin sampling and Al-
0
and the choice of second sampling in such a new environment is gorithm 1, respectively. As shown in Figure 4, LinLUCB signifi-
thesameasthechoiceoffirstsampling,wecanswapthesampling cantlyoutperformsotherpoliciesinallconfigurations.Withshorten-
order of lt and ut to ensure two first choices of sampling are dif- vironmentdurations(Cannot-sample-all-arms),shiftestimationhas
ferent.MotivatedbytheUCB1-normalalgorithmfrom[8],weuse more uncertainty, and consequently we observe a slower decayingPICScomparedtotheGeneralsettingforbothMDMandSCcon- and,forcomparison,underidealisedconditionswithoutanyenviron-
figurations. For the 0-1 policy, the General setting seems actually mental shifts. Note that for LinLUCB in a stationary environment,
1
(cid:113)
moredifficultbecauseanimbalanceofsamplesperenvironmentcan thecorrespondingUCB becomesr¯ + 16ln(t)σ˜2/((cid:80)J n )2
strongly bias the sampling strategy and the selection policy. Sam- i i j=1 ij
whereσ˜2 = (cid:80)K (cid:80)ni1 (cid:0) rk −r¯(cid:1)2 /(N −K)isanunbiasedes-
plingfromseveralenvironmentscanreducethedominatingeffectof i=1 k=1 i1 i
timatorforσ2.Meanwhile,forLinLUCBwiththeReduce-to-MAB
afewenvironmentalshifts,resultinginbetterPICSinquicklychang-
strategy,thecalculationofr¯ andσ˜2 forUCB isinsteadcomputed
ingenvironments(compareFigure4bwith4aandFigure4dwith4c i i
fromrewardswiththeshiftestimatorssubtracted.Wealsoexecuted
includingtheinitialworseninginallcases).ButevenintheCannot-
theproposedLinLUCB(Section4)toinvestigatethebenefitofin-
sample-all-armsscenario,0-1 performsworsethanRound-Robin.
1
cluding the uncertainty of the OLS estimator in the UCB compu-
tation. The relatively small gaps in Figure 5 between the policies
0.6 andtheirrespectiveperformanceinastationaryenvironmentdemon-
0.4
0.4 stratetheeffectivenessofshiftestimation.Notsurprisingly,thegaps
0.2 0.2 are smaller in long-duration environments (General) than in short-
durationenvironments(Cannot-sample-all-arms).Round-robinsam-
0.0 0.0
50 100 150 200 250 300 50 75 100 125 150 175 200 plingshowsthesmallestgapbetweenitsvariantswithandwithout
SampleAverageE[T] SampleAverageE[T]
(a)MDM,General (b)MDM,Cannot-sample-all-arms theOLSestimator.Thismaybebecausetheadaptivesamplingstrat-
egy of LinLUCB is susceptible to estimation errors of the shifts,
0.6 0.6 whereasround-robinsamplingisnotaffected.Comparingthevari-
0.4 0.4 antsofLinLUCB,asmalladvantageofReduce-to-MABstrategycan
0.2 0.2 onlybeobservedforaverysmallbudget,asmaybeseeninFigure5b
and5d.Thisphenomenonoccursbecausethevalueoftheexploration
0.0 0.0
50 100 150 Sampl2 e0 A0 verage25 E0 [T] 300 350 400 50 100 Samp1 le50 AverageE2 [T00 ] 250 300 terminUCBoftheReduce-to-MABvariantdropsfasterthanofthe
(c)SC,General (d)SC,Cannot-sample-all-arms proposedLinLUCBduetothedenominator.Moreover,thevariance
estimatorσ˜2 underestimatesitstruevalueinanon-stationaryenvi-
0 −11 linLUCB Round-Robin SER3
ronmentandleadstolessexplorationoftheReduce-to-MABone.
Figure4:TheperformanceofLinLUCBandbenchmarkpolicies
100
10−1 6 ConclusionandDiscussion
10−2 10−2
10−3 Inthispaper,weformulateanewsettingforfixed-budgetbest-arm
10−4 10−4 identificationinwhichanenvironmentcangloballyshifttherewards
10−5
50 100 150 200 250 300 50 75 100 125 150 175 200 of all arms in the same way. A selection policy based on ordinary
SampleAverageE[T] SampleAverageE[T]
linear regression is proposed to ensure an unbiased and consistent
(a)MDM,General (b)MDM,Cannot-sample-all-arms
100 best-armpredictorwherethenumberofenvironmentskeepsincreas-
10−1 10−1 ing. We also propose LinLUCB, an algorithm which integrates an
error from the mean and shift estimator into the sample allocation
10−2 10−2
decision,constructingaconfidenceboundthatnaturallyarisesfrom
10−3 10−3 the covariance matrix of the OLS estimator. Empirically, the Lin-
50 100 150 Sampl2 e0 A0 verage25 E0 [T] 300 350 400 50 100 Samp1 le50 AverageE2 [T00 ] 250 300 LUCBalgorithmiseffectiveindealingwithpiecewisestationaryen-
(c)SC,General (d)SC,Cannot-sample-all-arms vironments with global shifts. Besides, our numerical experiments
demonstratethebenefitsofexploitingtheOLSestimatorandhowthe
Round-Robin+noshift linLUCB+noshift linLUCB
Round-Robin+reduce-to-MAB linLUCB+reduce-to-MAB distributionofthedurationofstationaryperiodsoftheenvironment
Figure5:ComparisonofReduce-to-MABstrategiesandthecorresponding affectstheperformanceofpolicies.Simplyusingtheshiftestimates
performancesinastationaryenvironment producedbyourOLSestimatortoreducetheproblemtoastandard
MABsettingworkswellifthelengthofenvironmentsissufficiently
5.2 Reduce-to-MABstrategy long.Still,LinLUCBworksatleastasgoodandmostlybetterinall
testedcases.
Inordertogaugethebenefitofshiftestimation,wetestanalternative Thepaperopensseveralinterestingavenuesforfuturework.For
approachbyapplyinganyexistingpolicydesignedforastationary instance, the strong assumption of global environmental influence
environment,andonceachangeoccurs,wesimplysubtracttheOLS mayberelaxed.Also,inreal-worldcontexts,environmentalchanges
shiftestimatorsfromtherespectiverewards(r in jew =r ij−sˆ j)toap- are often continuous with smooth transitions rather than piecewise
proximatelyreducetheproblemtoastandardMABproblemwithout stationary,makingitsignificantlyhardertodetectthechangingpoint.
shifts(Reduce-to-MABstrategy).Onecansupposethatallmodified Lastly,theextensiontoheterogeneousnoisefordifferentarmsand
rewardsareGaussianwiththeexpectationofE[r ij −sˆ j] = µ i.All differentenvironmentswillbeusefulforamoregeneralstudy.
requirements for regression, however, are applied, and all required
statistics in any such policies are replaced by statistics calculated
from all subtracted rewards instead of original rewards. Note that Acknowledgements
thesamplemeanofsuchmodifiedrewardsisequivalenttotheOLS
meanestimator. Phurinut would like to acknowledge the Royal Thai Government
WeimplementedRound-RobinandLinLUCBwiththeReduce- scholarshipsponsoredbyTheInstituteforthePromotionofTeaching
to-MABstrategyandcomparedthesepoliciesinourtestscenarios ScienceandTechnology.
SCIP
SCIP
SCIP
SCIP
SCIP
SCIP
SCIP
SCIPReferences andhyperparameteroptimization. InArtificialintelligenceandstatis-
tics,pages240–248.PMLR,2016.
[26] K.Jamieson,M.Malloy,R.Nowak,andS.Bubeck. lil’ucb:Anopti-
[1] Y.Abbasi-Yadkori,P.Bartlett,V.Gabillon,A.Malek,andM.Valko. malexplorationalgorithmformulti-armedbandits. InConferenceon
Bestofbothworlds:Stochastic&adversarialbest-armidentification. LearningTheory,pages423–439.PMLR,2014.
InConferenceonlearningtheory,pages918–949.PMLR,2018. [27] K.-S.Jun,L.Li,Y.Ma,andJ.Zhu. Adversarialattacksonstochastic
[2] A.Alieva,A.Cutkosky,andA.Das. Robustpureexplorationinlinear bandits.Advancesinneuralinformationprocessingsystems,31,2018.
banditswithlimitedbudget. InInternationalConferenceonMachine [28] S.Kalyanakrishnan,A.Tewari,P.Auer,andP.Stone. Pacsubsetse-
Learning,pages187–195.PMLR,2021. lectioninstochasticmulti-armedbandits. InICML,volume12,pages
[3] R.AllesiardoandR.Féraud. Exp3withdriftdetectionfortheswitch- 655–662,2012.
ingbanditproblem. In2015IEEEInternationalConferenceonData [29] Z.Karnin,T.Koren,andO.Somekh. Almostoptimalexplorationin
ScienceandAdvancedAnalytics(DSAA),pages1–7.IEEE,2015. multi-armedbandits.InInternationalconferenceonmachinelearning,
[4] R. Allesiardo, R. Féraud, and O.-A. Maillard. The non-stationary pages1238–1246.PMLR,2013.
stochasticmulti-armedbanditproblem. InternationalJournalofData [30] D.E.KoulouriotisandA.Xanthopoulos. Reinforcementlearningand
ScienceandAnalytics,3:267–283,2017. evolutionary algorithms for non-stationary multi-armed bandit prob-
[5] J.Altschuler,V.-E.Brunel,andA.Malek. Bestarmidentificationfor lems.AppliedMathematicsandComputation,196(2):913–922,2008.
contaminatedbandits. JournalofMachineLearningResearch,20(91): [31] T.LattimoreandC.Szepesvári.Banditalgorithms.CambridgeUniver-
1–39,2019. sityPress,2020.
[6] S.Amaran,N.V.Sahinidis,B.Sharda,andS.J.Bury.Simulationopti- [32] L.Li,W.Chu,J.Langford,andR.E.Schapire. Acontextual-bandit
mization:areviewofalgorithmsandapplications.AnnalsofOperations approachtopersonalizednewsarticlerecommendation.InProceedings
Research,240:351–380,2016. ofthe19thinternationalconferenceonWorldwideweb,pages661–670,
[7] J.-Y.Audibert,S.Bubeck,andR.Munos. Bestarmidentificationin 2010.
multi-armedbandits.InCOLT,pages41–53,2010. [33] T.Lykouris,V.Mirrokni,andR.PaesLeme. Stochasticbanditsrobust
[8] P.Auer,N.Cesa-Bianchi,andP.Fischer. Finite-timeanalysisofthe to adversarial corruptions. In Proceedings of the 50th Annual ACM
multiarmedbanditproblem.Machinelearning,47:235–256,2002. SIGACTSymposiumonTheoryofComputing,pages114–122,2018.
[9] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The non- [34] C.Shen.Universalbestarmidentification.IEEETransactionsonSignal
stochasticmultiarmedbanditproblem.SIAMjournaloncomputing,32 Processing,67(17):4464–4478,2019.
(1):48–77,2002. [35] H.Shen,L.J.Hong,andX.Zhang.Rankingandselectionwithcovari-
[10] M. Azizi, B. Kveton, and M. Ghavamzadeh. Fixed-budget best-arm atesforpersonalizeddecisionmaking. INFORMSJournalonComput-
identificationinstructuredbandits. InL.D.Raedt,editor,Proceedings ing,33(4):1500–1519,2021.
oftheThirty-FirstInternationalJointConferenceonArtificialIntelli- [36] Z.Xiong,R.Camilleri,M.Fazel,L.Jain,andK.Jamieson.A/btesting
gence,IJCAI-22,pages2798–2804.InternationalJointConferenceson andbest-armidentificationforlinearbanditswithrobustnesstonon-
ArtificialIntelligenceOrganization,2022. stationarity. InInternationalConferenceonArtificialIntelligenceand
[11] L. Besson, E. Kaufmann, O.-A. Maillard, and J. Seznec. Efficient Statistics,pages1585–1593.PMLR,2024.
change-pointdetectionfortacklingpiecewise-stationarybandits. The [37] M.XuandD.Klabjan. Regretboundsandreinforcementlearningex-
JournalofMachineLearningResearch,23(1):3337–3376,2022. plorationofexp-basedalgorithms. arXivpreprintarXiv:2009.09538,
[12] S.Bubeck,R.Munos,andG.Stoltz. Pureexplorationinmulti-armed 2020.
banditsproblems. InAlgorithmicLearningTheory:20thInternational [38] J.YangandV.Tan. Minimaxoptimalfixed-budgetbestarmidentifi-
Conference,ALT2009,Porto,Portugal,October3-5,2009.Proceed- cationinlinearbandits. AdvancesinNeuralInformationProcessing
ings20,pages23–37.Springer,2009. Systems,35:12253–12266,2022.
[13] E.Cavenaghi,G.Sottocornola,F.Stella,andM.Zanker.Nonstationary [39] Z.Zhong,W.C.Cheung,andV.Tan.Probabilisticsequentialshrinking:
multi-armedbandit:Empiricalevaluationofanewconceptdrift-aware Abestarmidentificationalgorithmforstochasticbanditswithcorrup-
algorithm.Entropy,23(3):380,2021. tions.InInternationalConferenceonMachineLearning,pages12772–
[14] C.-H.Chen,J.Lin,E.Yücesan,andS.E.Chick. Simulationbudget 12781.PMLR,2021.
allocationforfurtherenhancingtheefficiencyofordinaloptimization.
DiscreteEventDynamicSystems,10:251–270,2000.
[15] S.E.ChickandK.Inoue. Newtwo-stageandsequentialprocedures
forselectingthebestsimulatedsystem. OperationsResearch,49(5):
732–743,2001.
[16] S.E.Chick,J.Branke,andC.Schmidt.Sequentialsamplingtomyopi-
callymaximizetheexpectedvalueofinformation. INFORMSJournal
onComputing,22(1):71–80,2010.
[17] N.Etemadi. Stabilityofsumsofweightednonnegativerandomvari-
ables.JournalofMultivariateAnalysis,13(2):361–365,1983.
[18] T.Fiez,H.Nassif,Y.-C.Chen,S.Gamez,andL.Jain. Bestofthree
worlds:Adaptiveexperimentationfordigitalmarketinginpractice. In
ProceedingsoftheACMonWebConference2024,pages3586–3597,
2024.
[19] P.I.Frazier,W.B.Powell,andS.Dayanik.Aknowledge-gradientpol-
icyforsequentialinformationcollection.SIAMJournalonControland
Optimization,47(5):2410–2439,2008.
[20] A.GarivierandE.Moulines. Onupper-confidenceboundpoliciesfor
switchingbanditproblems.InInternationalConferenceonAlgorithmic
LearningTheory,pages174–188.Springer,2011.
[21] C.Hartland,S.Gelly,N.Baskiotis,O.Teytaud,andM.Sebag. Multi-
armedbandit,dynamicenvironmentsandmeta-bandits.2006.
[22] M.Hoffman,B.Shahriari,andN.Freitas. Oncorrelationandbudget
constraintsinmodel-basedbanditoptimizationwithapplicationtoau-
tomaticmachinelearning.InArtificialIntelligenceandStatistics,pages
365–374.PMLR,2014.
[23] L.J.Hong,W.Fan,andJ.Luo. Reviewonrankingandselection:A
newperspective.FrontiersofEngineeringManagement,8(3):321–343,
2021.
[24] K. Jamieson and R. Nowak. Best-arm identification algorithms for
multi-armedbanditsinthefixedconfidencesetting. In201448thAn-
nual Conference on Information Sciences and Systems (CISS), pages
1–6.IEEE,2014.
[25] K.JamiesonandA.Talwalkar. Non-stochasticbestarmidentificationSupplementaryMaterials 2. (Lines 14-22) If there is a change in one step before a round of
alternatingsamplingandthegreedyarmlt issimilartothearm
A PseudocodeofLinLUCBpolicy sampledpreviously,sampleutandthenlt,
3. (Lines24-31)Otherwise,sampleltandthenut.
Algorithm2LinLUCBwithOLSestimator
Require: TotalbudgetT,n B AnalysisofConsistencyoftheOLSestimator
0
1: ExecuteAlgorithm1untiltimet=n 0K
2: Setthecurrentenvironmentordinalj =J 0. ProofofTheorem1
3: FitregressionmodelbyEq.(2)
4: RecentChange←False
5: whilet<=T do Proof. DenotethenumberofsamplesofarmiasN i
:=(cid:80)J
j=1n ij.
6: t←t+1 An estimator αˆ of α is called weakly consistent if for all ϵ > 0,
7: ifEnvChangethen lim Ni→∞P[|αˆ − α| > ϵ] = 0. Since shifts are independent
8: Increasetheordinaloftheenvironment:j ←j+1 of noise, we consider the variance of the OLS estimator condi-
9: Playarmit ←lt−1 tionalonasequencenumberofevaluationsforanalternativeunder
10: t←t+1 eachenvironment{n ij}J
j=1
andasequenceofshifts{s j}J j=1,i.e.,
11: Playarmit ←ut−2 V[µˆ i|{n ij},{s j}] and V[µˆ i1 −µˆ i2|{n i1j},{n i2j},{s j}] (simply
12: RecentChange←False writeV[µˆ i]andV[µˆ i1 −µˆ i2],respectively).
13: else DuetotheGaussiandistributionoftheOLSestimator,itsuffices
14: ifRecentChange∧(lt−1 =it−1)then toshowthatV[µˆ 1]↛0andV[µˆ 1−µˆ 2]→0withoutlossofgeneral-
15: RecentChange←False ity.Insteadofinvestigatingthematrixformulaofthemeanestimator
16: Playarmit ←ut−1 variance,weconsidertheestimatedmeanderivedfromminimising
17: t←t+1 thefollowinglossfunction,whichcorrespondstothestatedregres-
18: ifEnvChangethen sionproblem;
19: Increasetheordinaloftheenv.:j ←j+1
2 20 1:
:
endR ifecentChange←True L(µ˜,s˜)=(cid:88)N (cid:34) (cid:88)ni1
(r ik 1−µ˜
i)2+(cid:88)ni2
(r ik 2−µ˜ i−s˜ 2)2+...
22: Playarmit ←lt−2 i=1 k=1 k=1
23: else +(cid:88)niJ
(rk −µ˜ −s˜
)2(cid:35)
.
24: RecentChange←False iJ i J
25: Playarmit ←lt−1 k=1
26: t←t+1
Settingthegradientequaltozero,themeanestimatorisequivalent
27: ifEnvChangethen
tothefollowingformulagiventheshiftestimatorsˆ,
28: Increasetheordinaloftheenv.:j ←j+1
29: RecentChange←True
n r¯ +n (r¯ −sˆ )+...+n (r¯ −sˆ )
30: endif µˆ i = i1 i1 i2 i2 2 N iJ iJ J
31: Playarmit ←ut−2 i
J (cid:18) (cid:19)
32: endif =r¯ −(cid:88) n ij sˆ
33: endif i N i j
j=2
34: endwhile
returnBestpredictedarmˆi=argmax iµˆ i.
wherer¯
∼N(cid:32)
µ +
(cid:80)J j=1n ijs
j
,
σ2(cid:33)
.
i i N N
i i
LinLUCB algorithm (Algorithm 2) starts by executing Algo-
ThisimpliesthatCov(r¯,sˆ )→0asJ →∞foralli∈[K],j ∈[J]
i j
rithm1fortheinitialisationphaseandthenalternatingsamples,the duetoV[r¯]→0asN →∞
i i
greedy arm and the most potentially best arm from the rest, while
guaranteeingthatthetwofirstsamplesinthenewenvironmentare
To prove the convergence of variance, we suppose
distinct. After initialisation phase, at time t, the greedy arm is in- 0 < v∗ ≤ V[sˆ ],Cov(sˆ ,sˆ ) ≤ w∗ < ∞ for some con-
dexedbasedonthehighestmeanestimatorlt := argmax i∈[K]µˆ i, stantv∗,w∗ andalj lj ̸= m.j Impm licitly,weassumethatthevariance
inwhichtiesarebrokenarbitrarily,thentherestofarmsarefilteredto
of the shift estimator sˆ and the covariance of any two shift esti-
findthehighestUCBarmut :=argmax UCBt.Tomimic j
i∈[K]\{lt} i mators cannot vanish. Besides, all shift estimators are positively
abatchsamplingoftheoriginalLUCBpolicy[28],withoutlossof
correlated.
generality,wegenerallyselectlt andthenut inthenexttimestep.
After exhausting the total budget T, the highest mean estimator is
1)Thevarianceofthemeanestimatorcanbecomputedas
returned.
meI nn tA chlg ano gri eth sm in2 th, eE sn yv sC teh man .g Te or ge uf ae rr as nt to eew thh eet fihe rsr ta twc our sr ae mnt pe len svi fr ro on m-
V[µˆ
1]=(cid:88)J V(cid:20) n
N1jsˆ
j(cid:21) −(cid:88)J 2Cov(cid:18)
r¯
1,n
N1jsˆ
j(cid:19)
i i
differentarms,wecanseparatetheconditionsintothreescenarios: j=2 j=2
J−1 J (cid:18) (cid:19)
1. (Lines7-12)Ifthecurrentenvironmentchangesbeforearoundof +(cid:88) (cid:88) 2Cov n 1jsˆ ,n 1msˆ +V[r¯ ]
N j N m 1
alternatingsampling,sampleltandthenut,
j=2m>j
i iV[µˆ
1]=(cid:88)J n N2
1 2jV[sˆ
j]−(cid:88)J 2n
N1jCov(r¯ 1,sˆ j)
( ao nr dS Si Cmp cl oe nr fie gg ur re at) tioo nf sth we its ham Ke =pol 5ici ae ns da Ks in =Se 1c 0t .io Dn u5 rino gn tM heD sM
e-
j=2 i j=2 i quentialallocationpolicy,thejthenvironmentwilllastfor∆cp j ∼
+J (cid:88)−1 (cid:88)J 2n
N1j
n
N1mCov(sˆ j,sˆ m)+V[r¯ 1]
U p˜ a( rc ap mm ei tn er, scp cm
p
max in)t aim nde cs pte mp as x. ,D wep ee sn pd li in tg tho en et xh pe ee rn imvi er no tn sm ine tn otl 4en sg ct eh
-
j=2m>j i i nariosasfollows:
≥v∗(cid:34) (cid:88)J n2 1j +J (cid:88)−1 (cid:88)J 2n 1j n 1m(cid:35) + σ2 1. Worst-casescenario(cp min =cp max =2)
j=2 N i2 j=2m>j N i N i N 1 2. Cannot-sample-all-armsscenario(cp min =2,cp max =N −1)
3. Sample-1-to-10-per-armscenario(cp =N,cp =10N)
=v∗(cid:34) (cid:88)J n 1j(cid:35)2
+
σ2 4. Generalscenario(cp
min
=2,cp maxm =in 10N). max
N N
j=2 i 1 Overall, LinLUCB outperforms other policies in all configura-
tions.For0-1 policy,itperformsbetterinshortenvironmentsthan
AsJ →
∞,wehave(cid:20)
(cid:80)J n
1j(cid:21)2
→ 1.Hencethelowerbound
longenvironm1
ents,i.e.,Worst-caseandCannot-sample-all-armssce-
j=2 N
1 narios,sincesamplesofeacharmfromseveralenvironmentscanre-
ofV[µˆ ]→v∗ >0.
1 ducetheshiftinfluenceinchoicesofallocationandselection.Espe-
ciallyintheWorst-casescenario,theperformanceof0-1 policyis
1
2)Thevarianceofthedifferencebetweentwomeanestimatorscan
quitecompetitivetoround-robinsampling.FortheEOCmeasures,
becomputedas;V[µˆ −µˆ ]
1 2 all results are more or less similar to the PICS measures, and Lin-
J (cid:20)(cid:18) (cid:19) (cid:21)
=V[r¯ ]+V[r¯ ]+(cid:88) V n 1j − n 2j sˆ LUCBstilloutperformstheothers.
1 2 N N j
1 2
j=2
J (cid:18) (cid:18) (cid:19) (cid:19) C.1 MeasureofPICS
−(cid:88) 2Cov r¯ , n 1j − n 2j sˆ
1 N N j
1 2
j=2
J−1 J (cid:18)(cid:18) (cid:19) (cid:18) (cid:19) (cid:19)
0 −11 linLUCB Round-Robin SER3
+(cid:88) (cid:88) 2Cov n N1j − n N2j sˆ j, n N1m − n N2m sˆ m 0.6 0.6
1 2 1 2
j=2m>j
0.4 0.4
= σ2 + σ2 +(cid:88)J (cid:18) n 1j − n 2j(cid:19)2 V[sˆ ] 0.2 0.2
N 1 N 2 N 1 N 2 j 0.0 0.0
j=2 50 75 100 125 150 175 200 50 75 100 125 150 175 200
SampleAverageE[T] SampleAverageE[T]
−(cid:88)J (cid:18) n
N1j −
n N2j(cid:19)
2Cov(r¯ 1,sˆ j) 0.6
(a)Worst-case (b)Cannot-sample-all-arms
1 2
j=2
0.4 0.4
J−1 J (cid:18) (cid:19)(cid:18) (cid:19) +(cid:88) (cid:88) n N1j − n N2j n N1m − n N2m 2Cov(sˆ j,sˆ m) 0.2 0.2
1 2 1 2
j=2m>j
0.0 0.0
≤ σ2 + σ2 +w∗(cid:32) (cid:88)J (cid:12) (cid:12) (cid:12)n 1j − n 2j(cid:12) (cid:12) (cid:12)(cid:33)2 50 (c)10 S0 amSa pmp l1 l ee50 A -v 1er ta oge 1E2 [ 0T00 ] -per-a25 r0 m 300 50 100 S (am dp1 )le50 A Gve erag ne eE2 [ rT0 a0 ] l 250 300
N 1 N 2 j=2(cid:12)N 1 N 2(cid:12) Figure6:MDMconfigurationswith5arms
Toguaranteetheabsolutesumconvergencegoestozero,wesuppose
0.8
the number of environments not to grow too fast J ∈ o(N), and 0.6
0.6
the number of samples for each arm to be quite similar N 1,N 2 ∈ 0.4 0.4
Θ(N).Somealgorithms,suchasround-robinsampling,cansatisfy
(cid:12) (cid:12) 0.2 0.2
such an assumption. Therefore (cid:80)J j=2(cid:12) (cid:12) (cid:12)n N1 1j − n N2 2j(cid:12) (cid:12) (cid:12) → 0. Hence 0.0 100 S1 a5 m0 pleAverag2 e0 E0 [T] 250 300 0.0 100 S1 a5 m0 pleAverag2 e0 E0 [T] 250 300
V[µˆ −µˆ ]→0. (a)Worst-case (b)Cannot-sample-all-arms
1 2
0.6 0.6
Apart from the theoretical results, we found that, empirically,
such additional assumptions for the consistency of the ranking 0.4 0.4
can be lifted. From our empirical studies, we conjecture that 0.2 0.2
the value of Cov(sˆ ,sˆ ) converges to one constant, specifically,
Cov(sˆ j,sˆ
m)→1/(cid:80)j
K
i=m
1n i1forallj ̸=m.Providedthatthecon-
0.0
100 150 Sam20 p0 leAvera2 g5 e0 E[T]300 350 400
0.0
100 150 Sam20 p0 leAvera2 g5 e0 E[T]300 350 400
jecture holds, we can simply prove that V[µˆ 1 −µˆ 2] → 0 without (c)Sample-1to10-per-arm (d)General
additionalassumptions.Besides,thesameconvergenceoccurswith Figure7:MDMconfigurationswith10arms
thevarianceofmeanestimators,i.e.,V[µˆ ] → 1/(cid:80)K n forall
i i=1 i1
i∈[K].
C.2 MeasureofEOC
C AdditionalEmpiricalResults TheEOCisdefinedbytheexpectationoflinearlossfunctionasfol-
lows
The code is available on github.com/S-Phurinut/ABtesing-with-
warning.WereportthePICSandExpectedOpportunityCost(EOC) EOC:=E[µ i∗ −µ ˆi]
SCIP
SCIP
SCIP
SCIP
SCIP
SCIP
SCIP
SCIP0.8
100 100
0.6 0.6
0.4 0.4 10−1 10−1
0.2 0.2 10−2 10−2
0.0 0.0
50 100 Samp1 le50 AverageE2 [T00 ] 250 300 50 100 Samp1 le50 AverageE2 [T00 ] 250 300 10−3 50 75 100 125 150 175 200 10−3 50 75 100 125 150 175 200
(a)Worst-case (b)Cannot-sample-all-arms SampleAverageE[T] SampleAverageE[T]
(a)Worst-case (b)Cannot-sample-all-arms
0.6 0.6 100 100
0.4 0.4 10−2 10−2
0.2 0.2
10−4 10−4
0.0 0.0
50 100 150 200 250 300 350 400 50 100 150 200 250 300 350 400
SampleAverageE[T] SampleAverageE[T] 50 100 150 200 250 300 50 100 150 200 250 300
(c)Sample-1to10-per-arm (d)General SampleAverageE[T] SampleAverageE[T]
(c)Sample-1to10-per-arm (d)General
Figure8:SCconfigurationswith5arms
Figure11:MDMconfigurationswith5arms
0.8 0.8
0.6 0.6 100 100
000 ... 024
100 200 Samp3 le00 AverageE4 [T00
]
500 600
000 ... 024
100 200 Samp3 le00 AverageE4 [T00
]
500 600
111 000 −−− 321 111 000 −−− 321
(a)Worst-case (b)Cannot-sample-all-arms
100 150 200 250 300
10−4
100 150 200 250 300
SampleAverageE[T] SampleAverageE[T]
0.8 0.8
(a)Worst-case (b)Cannot-sample-all-arms
0.6 0.6
100 100
0.4 0.4
0.2 0.2 10−2 10−2
0.0 0.0
100 200 300 Sampl4 e0 A0 verage50 E0
[T]
600 700 800 100 200 300 Sampl4 e0 A0 verage50 E0
[T]
600 700 800 10−4 10−4
(c)Sample-1to10-per-arm (d)General
100 150 200 250 300 350 400 100 150 200 250 300 350 400
SampleAverageE[T] SampleAverageE[T]
Figure9:SCconfigurationswith10arms
(c)Sample-1to10-per-arm (d)General
whereˆiisthearmrecommendedbytheselectionpolicy,andi∗isthe Figure12:MDMconfigurationswith10arms
truebestarm.TheEOCvaluesinallfiguresareapproximatedbythe
samplemeanoflinearlossinstead.
10−1 10−1
2.0
1.5
10−2
50 100 150 200 250 300
10−2
50 100 150 200 250 300
SampleAverageE[T] SampleAverageE[T]
(a)Worst-case (b)Cannot-sample-all-arms
1.0
10−1 10−1
0.5
10−2 10−2
0.0
250 500 750 1000 1250 1500 1750 2000 10−3 10−3
SampleAverageE[T] 50 100 150 Sampl2 e0 A0 verage25 E0 [T] 300 350 400 50 100 150 Sampl2 e0 A0 verage25 E0 [T] 300 350 400
Round-Robin LUCB-normal EXP4P(mean) SER3 PSS (c)Sample-1to10-per-arm (d)General
0 −11 UCB-normal EXP4P(IPS) P1 SR Figure13:SCconfigurationswith5arms
Figure10:EOCofexistingalgorithmsfrom105replicationsontheGaussian
configurationof5armswherethegapsoforderedarms(δ=0.5)areequally
distributedandarmshaveequalvariance(σ=1).Thelengthofenvironment
jtharerandomuniformly,∆cpj ∼U˜(2,50)andtheshiftisarandomvari-
10−1
10−1
able,sj ∼U(0,20).
10−2
100 200 300 400 500 600 100 200 300 400 500 600
SampleAverageE[T] SampleAverageE[T]
(a)Worst-case (b)Cannot-sample-all-arms
10−1 10−1
10−2 10−2
10−3 10−3
100 200 300 400 500 600 700 800 100 200 300 400 500 600 700 800
SampleAverageE[T] SampleAverageE[T]
(c)Sample-1to10-per-arm (d)General
Figure14:SCconfigurationswith10arms
COE
SCIP
SCIP
SCIP
SCIP
SCIP
SCIP
SCIP
SCIP
COE
COE
COE
COE
COE
COE
COE
COE
COE
COE
COE
COE
COE
COE
COE
COE