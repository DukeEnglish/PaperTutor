MARLadona - Towards Cooperative Team Play Using Multi-Agent
Reinforcement Learning
Zichong Li, Filip Bjelonic, Victor Klemm, and Marco Hutter
A
Abstract—Robot soccer, in its full complexity, poses an
unsolved research challenge. Current solutions heavily rely
on engineered heuristic strategies, which lack robustness and
adaptability.Deepreinforcementlearninghasgainedsignificant
traction in various complex robotics tasks such as locomotion,
manipulation,andcompetitivegames(e.g.,AlphaZero,OpenAI
Five), making it a promising solution to the robot soccer prob-
lem.ThispaperintroducesMARLadona.Adecentralizedmulti- B C D
agentreinforcementlearning(MARL)trainingpipelinecapable
of producing agents with sophisticated team play behavior,
bridging the shortcomings of heuristic methods. Further, we
created an open-source multi-agent soccer environment based
onIsaacGym.UtilizingourMARLframeworkandamodified
a global entity encoder as our core architecture, our approach
achieves a 66.8% win rate against HELIOS agent, which
employs a state-of-the-art heuristic strategy. Furthermore, we
provided an in-depth analysis of the policy behavior and
interpreted the agent’s intention using the critic network.
Fig. 1. An illustration of our MARL environment (A) and various top-
I. INTRODUCTION
downviewsofa5v5game(B-D).(B)Thetrajectoryvisualizerdepictsthe
Recentadvancementsindeepreinforcementlearning(RL) generalgamedynamic.(C)Thecorrespondingdefaulttop-downview.(D)
Thecorrespondingballpositioncriticvalueheatmap.
have made it possible to solve some of the most challenging
robotic tasks such as object manipulation [1], locomotion
[2],andhigh-levelnavigationtasks[3].However,manyreal-
MoJoCo[10]andtheRCSimulation2Dserver[4].However,
world problems require cooperation among multiple robots,
besides lacking GPU support, many existing MARL soccer
requiring RL agents to learn optimal behavior while coex-
environmentsarekinematicsimulators[9],[5],[4].Thelack
isting with other agents, with either conflicting or common
of physics creates an additional sim-to-real gap, making
objectives requiring negotiation and communication. As a
transferring the learned strategies to real robotic platforms
result, this intricate interplay of strategies introduces addi-
harder. On the other hand, MARL environments like [8]
tional complexity, making multi-agent system (MAS) more
are great for comparing novel algorithms by providing a
challenging than single-agent systems.
standardizedframework.Still,itsgame-inspiredactions,i.e.,
As one of the most popular MAS, robot soccer has been
abstracted actions like “long pass”, make it even less ap-
thegrandgoalofroboticandAIresearch.Competitionssuch
plicable. Most RC teams also have their own customized
as RoboCup (RC) [4] and IEEE Very Small Size Soccer
multi-agent soccer simulators [11], [12], [13] tailored and
[5] have inspired generations of researchers to push the
fine-tuned for their specific robotic platforms.
state-of-the-art (SOTA) in robotic hardware and algorithms.
Traditionally, the high-level strategies of soccer agents are While existing approaches for MARL soccer often tackle
often hierarchically separated from the rest and tackled with the multi-agent strategy as a stand-alone MARL problem
sophisticated decision trees crafted by human experts. But [9], [14], [15]. Recent works from DeepMind [6], [7] have
robot soccer as a pure learning problem has gained signif- demonstratedanapproachthatsolvesthefullsoccerproblem
icant attention in recent years [6], [7]. Existing literature end-to-endina3Denvironment.Bothoftheseworksutilized
in soccer MARL can be roughly categorized into creating a combination of imitation learning and individual skill
environments and proposing training approaches. training to obtain low to mid-level motor skills. Then, they
trained with MARL in a self-play setting to get the final
A. Related Work policy. While emerging team play was observed in [6],
they only demonstrated within a 3v3 game setting. For
Several soccer environments have already been pro-
games with more agents, it is more common to simplify the
posed by literature such as Google research football [8],
soccer problem into 2D and train high-level policies [14],
MARL2DSoccer [9], rSoccer [5], Humanoid football in
[15], [9], [5], [16] which assume the existence of motion
This work has been submitted to the IEEE for possible publication. interfaces such as walking velocity and kicking commands.
Copyrightmaybetransferredwithoutnotice,afterwhichthisversionmay Existing work commonly trains decentralized policies [9],
nolongerbeaccessible.
[17] to avoid the exploding state space with growing agent
AllauthorsarewiththeRoboticSystemsLab,ETHZürich,8092Zürich,
Switzerland. number. For instance, Smit et al. [9] used an attention
4202
peS
03
]AM.sc[
1v62302.9042:viXraFig. 2. An overview of our system. (A) The ego perspective observation from the opponents (red), teammates (blue), and local observation. (B) The
soccer environment. (C) Various curricula we adopted during training. (D) The architecture of our policy network. (D1) Encoders with shared weights.
(D2)Policynetwork.(E)Thedistributionweusedforactionsampling.(F)Theactionmodelofoursocceragent.
mechanism and utilized their customized MARL2DSoccer iorandbenchmarkeditsperformanceagainstHELIOS1,
environment for simulation. Lin et al. [17] used Google a SOTA scripted soccer agent.
research football directly and presented the TiZero, which
II. METHOD
utilized a combination of observation encoder and LSTM
mechanism with a multi-stage self-play curriculum. They A. Overview
achievedSOTAperformance,buttheirgame-inspiredactions
Our work aims to train a generic decentralized soccer
make their policy incompatible with actual robotic systems.
policythatdemonstrateseffectiveanddynamicteamplay.To
For many MAS, permutation invariant with respect to achieve this, we designed a multi-agent soccer environment
neighboring agents is a desirable property. An indirect way on top of Isaac Gym to leverage its realistic physics and
to achieve this is to augment the data with the permutated GPUaccelerationandutilizedthisframeworkincombination
state,similartolearningsymmetry[18].However,itismore with a novel approach (Fig. 2) to train the soccer policy. We
desirable to have it directly as a network property. Various address the sparse reward problem of soccer MARL with
formsofpermutationinvariantnetworkshavebeenproposed differentcurricula(C)andadoptedamodifiedversionofthe
in the literature, e.g., Attention mechanism [19], PointNet GEE architecture [14] as our actor (D) to effectively handle
[20], deep sets [21] etc. Based on the PointNet, An et al. permutationandnumberchangesamongbothteammatesand
[14] proposed the GEE in which set inputs of the same opponents. Moreover, our approach aims to include only the
type, called entities, are passed through a shared multilayer minimum for observation states and rewards. By using the
perceptron (MLP) allowing a list of local feature encodings PPO algorithm [23], we trained our lightweight network on
to be obtained. The max pooling is then conducted over the a consumer desktop (single RTX 2060 GPU) and achieved
set of local feature encodings and the resulting global entity quality team play within just a few hours of training.
feature, which is permutation invariant for set inputs.
B. Markov Game
B. Contributions Different from traditional single-agent RL problems,
MARL the problem is often modeled as a Markov game
While existing training approaches often demonstrate im-
(MG) [24] which is a generalization of the markov decision
proved quantitative results against some heuristics [9], [5],
process (MDP) to multiple agents. A MG can be formally
[16], none of them, to our best knowledge, demonstrated
defined as a tuple:
qualityteamplayinMARLsoccerformorethan3v3games.
To address this open research problem, we introduce MAR- ⟨I,S,A,P,R,γ⟩ (1)
Ladona, a novel framework for training high-level multi-
WhereI denotesthesetofagents,S istheenvironmentstate,
agent policies using end-to-end RL.
A={a }N isthejointactionofallagents,P :S×A→S
Our key contributions include: i i=1
thetransitionprobability,R={r }N withr :S×A×S →
i i=1 i
• A new customizable open-source 2D MARL soccer R is the reward function of agent i and γ as the discount
environment based on Isaac Gym. factor.Ateachtimestept,thestates∈S evolvesaccording
• Introduced an improved GEE for training policies that to the transition probability P and the joint action a ∈ A.
achieve effective team play for games up to 11v11.
• Providedacomprehensiveanalysisofthepolicybehav- 1Forclarity,weuseHELIOStoreferto[22]Each agent i then receives their respective reward r from A B
i
the environment as feedback.
Often in MARL, the global environment states are not
fully observable, leading to the partial observable Markov
game (POMG) [25]. The additional O is used to denote the
observablestatesoftheenvironment.Incaseallagentsshare
the same reward function, then a cooperative POMG can be
considered as a decentralized partially observable Markov
decision process (Dec-POMDP) [26].
For this paper, we adopted the centralized training and
decentralized execution (CTDE) paradigm due to its scala- Fig. 3. Initial position curriculum depended on the current policy
bilityandrobustnesstovaryingagentnumbersandaccessto performance.Theball’sinitialdistributionisadjustedtowardtheblueside
for lower levels to enhance trainees’ chances of gaining ball procession.
privileged information. In CTDE, each agent has its policy
Theagent’sinitialdistribution,ontheotherhand,iskeptconstant.
π : ω (O) → a , which maps its local observation to its
i i i
actiona .ω istheobservationfunctionofagenti.Denoting
i i
the additional set of observation function as Ω = {ω i}N i=1, [28],weaddeddifferentfieldandgoalsizelevelstoincrease
ourfinalproblemcanbemodeledasadecentralizedpartially the initial scoring chance. Similar to [28], the environment
observableMarkovgame(Dec-POMG)definedbythetuple: level is moved up or down depending on the outcome of the
previous game.
⟨I,S,A,O,Ω,P,R,γ⟩ (2)
Self-play is another approach commonly applied in com-
C. Environments and Agent Modeling petitive RL problems [29], [7], [8], [9], [30]. It allows our
trainees to adapt to different adversaries and incentivizes
Our MARL soccer environment (Fig. 2 B) is built on
them to learn more generalized strategies. Various forms of
top of Isaac Gym [27], allowing us to simulate thousands
implementationcanbefoundintheliterature,suchasLeague
of games in parallel. Each environment contains a ball and
training,Polyakaveraging[9],orsamplingfrompastpolicies
two teams with a random number of agents, and all actors
[7]. We adopted a replay mechanism that buffers up to 8
areuniformlyspawnedwithintheirdesignatedarea.Agame
policies. During training, the current trainee policy is added
terminates when a goal is scored or after 30 seconds, which
totheadversarybufferwhenitachievesan75%averagewin
is empirically chosen to give the agents enough time to
rateagainstallofitsadversaries.Theadversariesthensample
score. The ball respawns above the borderline upon leaving
fromthisbufferandrunthepolicyininferencemode.During
the playing field, and a physical wall is placed at a short
sampling,weapplythesamepolicyperteam,ascooperation
distance around the playing field to prevent escaping agents.
quality might deteriorate due to strategy mix-ups.
Furthermore,theenvironmentregisterseventssuchaspasses,
Unlike many other MARL problems, the initial position
ownership2 losses, etc. Only the blue agents are used for
of the agents and ball is critical as being closer to the ball
training, while the red agents are controlled by bot or old
directly implies ownership, which is a significant advantage
policies,dependingonthescenario.Forclarity,wewillrefer
in soccer. Therefore, we adopted different curriculum levels
to the blue agent as trainee and the red agent as adversary.
totheinitialpositionaswell.Inourapproach,agentsareun-
Afloatingbasewithacylinderasitscollisionbodyisused
affected by the curriculum level and just spawned uniformly
for the agent modeling. At each simulation step, each agent
within their designated area with direction sampled from
receives an action command of the form (v ,v ,v ,k ,k )
x y θ x y (π,−π].Theballisbiaseddeeperintothebluesideforlower
(Fig. 2 F). All actions represent velocity commands3 which
curriculum levels to increase ball ownership probability for
isthentrackedbyaPDcontrolleroutputtingacorresponding
the trainee team (Fig. 3).
forceandtorqueforthesimulator.Thexandycomponentof
Finally, we also want the trainees to learn and adapt their
thetranslationcommands(base:v ,v )and(kick:k ,k )are
x y x y strategies accordingly, depending on the team sizes. Since
remapped using Eq. 3 to map the rectangular control space
training directly with the maximum agent number can be
into the unitdisk. Note, the agent’s kickcommands are only
counterproductive,weimplementedaresamplingmechanism
in effect when the ball is within its kickable area K.
that allows the environment to dynamically re-configuration
its team composition before an episode begins. This enables
(cid:114) (cid:114)
y2 x2 us to adapt the maximum number of players according to
x =x· 1− , y =y· 1− (3)
new 2 new 2 the policy performance. Due to computational constraints,
we limited our training to a maximum of 3v3 games.
D. Curricula E. Observations and Rewards
Toreducetheinitialexplorationcomplexityandaccelerate An overview of the observation is summarized in Tab. I.
thelearningprocess,weincludedvarioustypesofcurriculum Rows 1-6 are local observations, and rows 7 and 8 show the
during training (Fig. 2 C). Inspired by terrain curriculum neighbor observations. Both actor and critic use the same
observation states but slightly different noise configurations.
2An agent is assigned with ball ownership if it is within 0.25m, the Note that all observation types are transformed into the
closestfromhisteam,andnoopponentfulfillsthefirstcondition.
perspectiveoftheegoagent.Theoptioncolumnprovidesad-
3Velocitycommandsasitiswidelyusedtointerfacewithmanyrobotic
platforms. ditional configuration details. (W) indicates the observationFig.4. Anoverviewofourevaluationresultsconductedfora3v3gameforthreedifferentscenarios(Offensive,Equal,Defensive)againstthreedifferent
adversaries(RL,Bot,HELIOS).Thecollectedaveragestatistics(gameoutcome(%),teamballownership(%),thenumberofsuccessfulpassesandball
ownershiplosses,andgameduration)aredepictedindifferentrows.Ourtraineepolicy(Blue)achievedcleardominanceagainstalladversaries(besides
itself)inallscenarios.Thetraineeswon66.8%(averagedoverallthreescenarios)ofallgamesagainstHELIOS.
TABLEI TABLEII
OBSERVATIONOVERVIEW. REWARDOVERVIEW.
Observationname Noiseactor/critic Dimensions Options Rewardname Description Scale Shared
LocalObservations SparseRewards
1)Basepose 0.002/0 4 W,E,N 1)Score +1ifwins,-1ifloses 100 True
2)Basevelocity 0.005/0 3 W 2)Balloutsidefield -1ifballleavesthefield. 1 True
3)Ballposition 0.002/0 2 W,N 3)Collision -1ifcollision(exceptball) 1
4)Ballvelocity 0.005/0 2 W DenseRewards (Removedlater)
5)Fieldinfo 0/0 5 N 4)Ball2goalvelocity ∥v ball2goal∥ 2 True
6)#activeagents 0/0 2 5)Base2ballvelocity ∥v base2goal∥ifballisfar 0.5
NeighborObservations 6)Balldirection exp(−(θ base2ball/0.4)2) 0.025
7)Teammateposes 0.002/0 4·H·NT W,E,N
8)Opponentposes 0.002/0 4·H·NO W,E,N
4), bring the ball closer to the goal (reward 5), and keep the
ballwithinitskickableareaK(reward6).Thesharedcolumn
is in the world frame, (E) denotes the rotation component θ indicates which of them is shared with the whole team.
isexpandedintosin(θ)andcos(θ).(N)meanswenormalize
the observation with the field length and width. And H F. Network Architecture
showsthenumberofincludedpasttimesteps.Thefieldinfo MASoftenrequiresagentstohaveinvariantstrategieswith
contains auxiliary information about the x and y position respect to neighboring agents while dynamically adapting to
of the field line border and goal width. For the neighbor changes in agent number. We achieved this using a similar
observations,theobservationfunctionΩ isimplementedasa architecture as the GEE [14] (Fig. 2 D). One key difference
filtermaskthatpassesthe N max closestagents’state,which to the original GEE architecture is the expanded observation
iskeptatthreeduringtraining.Fortestingwithhigheragent formed by concatenating each neighboring agent’s observa-
numbers, e.g., 5v5 and above, we use N max = 5 and also tion with the local observation. This expanded observation
limit the value of observation 6 in Tab. I to three active is then forward fed into the encoder network (D1), allowing
agents, avoiding scalability issues. the shared encoder to extract information with additional
Tab. II provides an overview of our rewards. Rows 1-3 context of the ego agent. The most prominent feature of
are our main sparse rewards, and rows 4-6 show the dense the resulting local entity feature is max pooled across each
rewards that get permanently removed the first time the agent type. The final input for the policy network (D2) can
trainee policy reaches 75% win rate against its adversaries. be obtained by merging the fixed-size global entity feature
Thedenserewardsareonlyaddedtoprovideinitialguidance of the teammates and opponents, which can be concatenated
by incentivizing the trainees to approach the ball if neither with the local observation. The layer size configuration of
theegoagentnorhisteammateshaveballownership(reward boththeencoderandpolicynetworkaresummarizedinTab.C
Base Position’s Critic Value Ball Position’s Critic Value
Blue 1’s Perspective Blue 2’s Perspective Blue 1’s Perspective Blue 2’s Perspective
A1 A2 B1 B2
1
2
Fig. 5. An illustration of the critic values from a 2v2 game as a heat map (Res. 80 × 80). The plots are acquired by varying the base positions of
thetrainees(A1,A2)andtheballposition(B1,B2)overthewholefieldwhilekeepingtheotherobservationsfixed.Thepositionofthetrainees(blue),
adversaries(red),andball(white)arerepresentedbytheirrespectivecoloreddots,andthelargeblackcircleindicateswhichofthetraineestheheatmap
belongsto.Furthermore,Cprovidesthecorrespondingdefaulttop-downviewoverlaidwithmotiontrajectoriestoprovideadditionalinformationaboutthe
currentgamedynamic.Theblueareasontheseheatmapsindicatewheretheagentswantthemselvesandtheballtobe.
TABLEIII A B C
NETWORKCONFIGURATION.
Input Hidden Output Activation
Encodernetwork 27 64,32 16 ELU A1
1
Policynetwork 35 128,128,128 10 ELU 1 1
2
3 B1
2 2
III. Similar to [14], we use beta distribution to adhere to 3 C1
the constraint of the velocity command, which requires the 3
policy net to predict a (α,β) value pair per action.
A2 A3 B2 B3 C2 C3
III. EXPERIMENTALSETUP
A. Evaluation Methodology
We validated our trainee policy against three adversaries Fig. 6. A top-down view sequence depicting some typical trainee (blue)
behaviors in a 3v3 game setting. A, B, and C provide an overview of the
using three scenarios in a 3v3 setting. Over 600 seconds
global behavior, such as positioning, while A1-3, B1-3, and C1-3 provide
of simulated gameplay (6 ∼ 20 game sets), we tracked azoom-inviewshowcasingnumerouslocalball-handlingskills.
various performance metrics, including game outcome, ball
procession, the average number of passes, ball losses, and
game duration. Sec. III-B will provide additional details currentlyverydifficultduetothelackofunifiedMARLsoc-
about the adversaries. Identical to the training setup, we cerframeworks.MostexistingMARLpoliciesareembedded
spawn agents uniformly in their designated areas. While the in other custom simulators, which lack interoperability. For
spawning distribution is identical between the two teams in our benchmark, we created an interface server using the
all scenarios, we biased the ball spawn location deep into RC Simulation2D protocol [31], allowing us to compete
the blue side for Offensive, into the red side for Defensive against algorithms from other Simulation2D teams within
and kept them in the field center for Equal scenario. our Isaac Gym environment. Therefore, we chose HELIOS
Besidesthequantitativeanalyses,wealsoprovideadditional as our benchmark adversary since it is currently the best-
insight into the trainee’s behavior directly with trajectory performing framework in the RC Simulation2D, winning a
rollouts (Fig. 6 and 7) and value function of the base and total of 6 world championships in the past. To reduce the
ball position (Fig. 5). handicap introduced by the sim-to-sim gap, we replicated
the movement and kick model inside Isaac Gym to the best
B. Adversaries of our ability.
For the adversaries, we have the trainee itself (RL), a
IV. RESULTSANDANALYSES
simple heuristic bot (Bot), and lastly the HELIOS agent as
A. Quantitative Results
a SOTA benchmark (HELIOS).
The heuristic bot is a role-based scripted bot that assigns Fig. 4 provides an overview of the evaluation result
the closest agent with greedy soccer logic, i.e., approaching wherethetraineepolicyperformsagainstdifferentadversary
the ball and immediately kicking toward the goal once the implementations.
ballbecomeskickable.Italsohasagoalkeeperanddefender Against itself (RL), all outcomes are roughly mirrored.
logic. It is a simple yet effective adversary but far from Spawning the ball in one team’s half directly leads to
sufficientasabenchmark.Properbenchmarking,however,is dominance, which can be seen from both game outcomes1v1 2v2 3v3 7v7 11v11
1v2 2v3
Fig. 7. Recorded motion trajectories from various games illustrating generalized cooperative behavior for diverse team composition. The big colored
dots represent the current position of our trainees (blue), adversaries (red), and the ball. The last few motion frames are additionally emphasized by the
fadingdottrails.Thedotedtrajectoriesvisualizethefullgamedynamic.Theball(white)trajectory,inparticular,isusefulforunderstandinghowtheball
ispassedbetweentraineesbeforetheyscorethegoalattheend.
and ball ownership statistics. Once they have the ball, they architecture,curricula,andself-play,thetraineescanquickly
perform numerous passes with a very high success rate. exploreandadoptintelligentbehavioragainstpreviouslyun-
Againsttheheuristicbot(Bot),thetraineeexcelledacross seen team composition (up to 11v11) and adversaries. Most
allscenarios,especiallyintheoffensive.Whenaveragedover of the sophisticated cooperative behavior emerges after the
all scenarios, the trainees won about 80%, drew 16%, and initial exploration, including the removal of dense rewards.
lostinonly4%ofallgamesandowningtheballmostofthe With only the sparse rewards, the policy can demonstrate
time. Although, more back and forth can be observed based clear role assignment (Blue 1 in Fig. 6 A) and strategic
on the pass and game duration statistics. positioning (Blue 2 and 3 in A, B). Our trainees have also
Finally, our trainees also prevailed against our SOTA acquired various ball-handling skills such as dribbling (A1-
benchmarkagent(HELIOS)inallscenarios.Comparedwith 3), passing (A3, B3), and goal shooting (C3). Furthermore,
HELIOS,ourtraineesachievedmoreconsistentpassesacross thevaluefunction(Fig.5)alsoclearlyillustratesmanyofthe
allscenarios.Fromthesignificantballownershiplossesfrom intended strategies, such as ball assignment and positioning
both teams, we can conclude that both parties were actively (A1, A2), opportunities for a pass, and goal (prominent
engaging the ball. In total, our trainee won 66.8%, drew blue funnel in B1, B2). We strongly encourage interested
4.2% and lost 29% of all games against HELIOS. readers to check out the supplementary video for better
understanding.
B. Qualitative Results
We depict some common trainee behaviors in Fig. 6, Onelimitationofourapproachiscornercasehandling,for
which provides a top-down view of a 3v3 game at different instance, when the ball is stuck at the border or when the
times. (A, B, and C) showcase global behavior and (A1- opponents have the ball and are too far away. Our trainees
3, B1-3, C1-3) illustrate various local ball handling skills. oftenstartdriftingaimlesslyinthesescenarios,makingthem
Additionally, Fig. 7 showcases some common behaviors in poor defenders. Furthermore, a higher agent number in our
games with other team compositions. Despite encountering environmentfrequentlyresultsinanagentcluster,sometimes
onlyteamsofupto3v3agentsduringtraining,thetraineehas stallsthegameentirely.Astheultimategoalistodeploysuch
learnedtogeneralizehisbehaviortoafull-scale11v11game, policies onto a robotic system, we conduct preliminary tests
showcasing various quality passes despite the clustering and ofthepolicyinsidetheNomadZ-NGframework[32],which
chaos. simulatessoccergamesforNAOrobots.Whilethemovement
Besides the policy behavior, Fig. 5 also depicts the andglobalpositioningexhibitcomparablebehavior,thekick
trainees’ value functions taken during a pass. It is worth does not map well onto humanoid robots due to the lack of
noting that the ball position heat map (B1, B2) is nearly continuous ball handling.
identical from both trainees’ perspectives. This similarity is
consistent with our intuition since, different from the base Future works should address these limitations. As our
position, the ball’s state is directly intertwined with the environment is already embedded in 3D, we can explore
team’s common goal, which should be identical regardless additional 3D actions such as high kicks or even directly
of perspective. replace our agents with fully articulated humanoid robots,
possibly using our work as a hierarchical or pre-training
V. CONCLUSION
component.Moreover,ourworkfocusesonperfectinforma-
Our proposed approach allows effective end-to-end learn- tion games where information about all agents is available
ing of cooperative behaviors in a multi-agent soccer envi- without delay. Handling additional complexity stemming
ronment and achieves an overall 66.8% win rate against the from limited field of view, occlusion, and communication
SOTAbenchmarkHELIOS.ByemployinganimprovedGEE delay could also be an exciting research direction.REFERENCES [24] M.L.Littman,“Markovgamesasaframeworkformulti-agentrein-
forcementlearning,”inMachinelearningproceedings1994. Elsevier,
[1] F. Wang and K. Hauser, “Stable bin packing of non-convex 3d
1994,pp.157–163.
objects with a robot manipulator,” in 2019 International Conference
[25] S.GronauerandK.Diepold,“Multi-agentdeepreinforcementlearn-
onRoboticsandAutomation(ICRA). IEEE,2019,pp.8698–8704.
ing:asurvey,”ArtificialIntelligenceReview,vol.55,no.2,pp.895–
[2] J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter,
943,2022.
“Learningquadrupedallocomotionoverchallengingterrain,”Science
[26] E.A.Hansen,D.S.Bernstein,andS.Zilberstein,“Dynamicprogram-
robotics,vol.5,no.47,p.eabc5986,2020.
mingforpartiallyobservablestochasticgames,”inAAAI,vol.4,2004,
[3] J.Lee,M.Bjelonic,A.Reske,L.Wellhausen,T.Miki,andM.Hutter,
pp.709–715.
“Learningrobustautonomousnavigationandlocomotionforwheeled-
[27] V.Makoviychuk,L.Wawrzyniak,Y.Guo,M.Lu,K.Storey,M.Mack-
leggedrobots,”ScienceRobotics,vol.9,no.89,p.eadi9641,2024.
lin,D.Hoeller,N.Rudin,A.Allshire,A.Handa,etal.,“Isaacgym:
[4] H.Kitano,RoboCup-97:robotsoccerworldcupI. SpringerScience
High performance gpu-based physics simulation for robot learning,”
&BusinessMedia,1998,vol.1395.
arXivpreprintarXiv:2108.10470,2021.
[5] F.B.Martins,M.G.Machado,H.F.Bassani,P.H.Braga,andE.S.
[28] N. Rudin, D. Hoeller, P. Reist, and M. Hutter, “Learning to walk
Barros,“rsoccer:Aframeworkforstudyingreinforcementlearningin
in minutes using massively parallel deep reinforcement learning,” in
smallandverysmallsizerobotsoccer,”inRobotWorldCup. Springer,
ConferenceonRobotLearning. PMLR,2022,pp.91–100.
2021,pp.165–176.
[29] D.Silver,T.Hubert,J.Schrittwieser,I.Antonoglou,M.Lai,A.Guez,
[6] S.Liu,G.Lever,Z.Wang,J.Merel,S.A.Eslami,D.Hennes,W.M.
M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al., “A general
Czarnecki, Y. Tassa, S. Omidshafiei, A. Abdolmaleki, et al., “From
reinforcement learning algorithm that masters chess, shogi, and go
motorcontroltoteamplayinsimulatedhumanoidfootball,”Science
throughself-play,”Science,vol.362,no.6419,pp.1140–1144,2018.
Robotics,vol.7,no.69,p.eabo0235,2022.
[30] B.Brandão,T.W.DeLima,A.Soares,L.Melo,andM.R.Maximo,
[7] T.Haarnoja,B.Moran,G.Lever,S.H.Huang,D.Tirumala,J.Hump-
“Multiagentreinforcementlearningforstrategicdecisionmakingand
lik, M. Wulfmeier, S. Tunyasuvunakool, N. Y. Siegel, R. Hafner,
controlinroboticsoccerthroughself-play,”IEEEAccess,vol.10,pp.
et al., “Learning agile soccer skills for a bipedal robot with deep
72628–72642,2022.
reinforcementlearning,”ScienceRobotics,vol.9,no.89,p.eadi8022,
[31] “Soccer server protocols,” 2024. [Online]. Available: https:
2024.
//rcsoccersim.readthedocs.io/en/latest/soccerserver.html#protocols
[8] K. Kurach, A. Raichuk, P. Stan´czyk, M. Zaja˛c, O. Bachem, L. Es-
[32] NomadZ Team, “NomadZ Code Release,” Oct. 2024. [Online].
peholt, C. Riquelme, D. Vincent, M. Michalski, O. Bousquet, et al.,
Available:https://github.com/nomadz-ethz/nomadz-ng
“Google research football: A novel reinforcement learning environ-
ment,”inProceedingsoftheAAAIconferenceonartificialintelligence,
vol.34,no.04,2020,pp.4501–4510.
[9] A. Smit, H. A. Engelbrecht, W. Brink, and A. Pretorius, “Scal-
ing multi-agent reinforcement learning to full 11 versus 11 simu-
latedroboticfootball,”AutonomousAgentsandMulti-AgentSystems,
vol.37,no.1,p.20,2023.
[10] S.Liu,G.Lever,J.Merel,S.Tunyasuvunakool,N.Heess,andT.Grae-
pel, “Emergent coordination through competition,” arXiv preprint
arXiv:1902.07151,2019.
[11] B-Human Team, “B-Human Code Release,” Oct. 2023. [Online].
Available:https://github.com/bhuman/BHumanCodeRelease
[12] A. Aggarwal, M. Van Der Burgh, P. Van Dooren, L. Messing,
R.M.Núnez,S.Narla,G.C.Cardenas,J.Lunenburg,L.VanBeek,
L.Janssen,etal.,“Techunitedeindhoven@home2023teamdescrip-
tionpaper,”Tech.Rep,Tech.Rep.,2023.
[13] M.Ratzel,M.Geiger,andA.Ryll,“Robocup2023sslchampiontigers
mannheim-improved ball interception trajectories,” in Robot World
Cup. Springer,2023,pp.407–415.
[14] T. An, J. Lee, M. Bjelonic, F. De Vincenti, and M. Hutter, “Solv-
ing multi-entity robotic problems using permutation invariant neural
networks,”arXivpreprintarXiv:2402.18345,2024.
[15] J.M.CatacoraOcana,F.Riccio,R.Capobianco,andD.Nardi,“Co-
operativemulti-agentdeepreinforcementlearninginsoccerdomains,”
in Proceedings of the 18th International Conference on Autonomous
AgentsandMultiAgentSystems,2019,pp.1865–1867.
[16] H. Zhong, H. Zhu, and X. Li, “Development of a simulation envi-
ronmentforrobotsoccergamewithdeepreinforcementlearningand
roleassignment,”in2023WRCSymposiumonAdvancedRoboticsand
Automation(WRCSARA). IEEE,2023,pp.213–218.
[17] F. Lin, S. Huang, T. Pearce, W. Chen, and W.-W. Tu, “Tizero:
Masteringmulti-agentfootballwithcurriculumlearningandself-play,”
arXivpreprintarXiv:2302.07515,2023.
[18] M.Mittal,N.Rudin,V.Klemm,A.Allshire,andM.Hutter,“Symme-
try considerations for learning task symmetric robot policies,” arXiv
preprintarXiv:2403.04359,2024.
[19] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
Advancesinneuralinformationprocessingsystems,vol.30,2017.
[20] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning
onpointsetsfor3dclassificationandsegmentation,”inProceedings
of the IEEE conference on computer vision and pattern recognition,
2017,pp.652–660.
[21] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdi-
nov, and A. J. Smola, “Deep sets,” Advances in neural information
processingsystems,vol.30,2017.
[22] H.AkiyamaandT.Nakashima,“Heliosbase:Anopensourcepackage
fortherobocupsoccer2dsimulation,”inRoboCup2013:RobotWorld
CupXVII17. Springer,2014,pp.528–535.
[23] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
“Proximal policy optimization algorithms,” arXiv preprint
arXiv:1707.06347,2017.