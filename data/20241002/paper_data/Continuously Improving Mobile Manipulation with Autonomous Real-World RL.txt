Continuously Improving Mobile Manipulation
with Autonomous Real-World RL
RussellMendonca1,EmmanuelPanov2,BernadetteBucher2,JiuguangWang2,DeepakPathak1
1CarnegieMellonUniversity,2AIInstitute
Figure1: ContinualAutonomousLearning: Weenablealeggedmobilemanipulatortolearnavarietyof
taskssuchasmovingchairs(top,leftandright),rightingadustpan(top,middle),andsweeping(bottom)via
practiceintherealworldwithminimalhumanintervention.
Abstract: Wepresentafullyautonomousreal-worldRLframeworkformobile
manipulationthatcanlearnpolicieswithoutextensiveinstrumentationorhuman
supervision. Thisisenabledby1)task-relevantautonomy,whichguidesexplo-
ration towards object interactions and prevents stagnation near goal states, 2)
efficient policy learning by leveraging basic task knowledge in behavior priors,
and3)formulatinggenericrewardsthatcombinehuman-interpretablesemantic
informationwithlow-level,fine-grainedobservations. Wedemonstratethatour
approachallowsSpotrobotstocontinuallyimprovetheirperformanceonasetof
fourchallengingmobilemanipulationtasks,obtaininganaveragesuccessrateof
80%acrosstasks,a3-4×improvementoverexistingapproaches. Videoscanbe
foundat https://continual-mobile-manip.github.io/
Keywords: ContinualLearning,MobileManipulation,ReinforcementLearning
1 Introduction
Howdowebuildgeneralistsystemscapableofexecutingawidearrayoftasksacrossdiverseenvi-
ronments,withminimalhumaninvolvement? Whilevisuomotorpoliciestrainedwithreinforcement
learning(RL)havedemonstratedsignificantpotentialtobringrobotsintoopen-worldenvironments,
theyoftenfirstrequiretraininginsimulation[1,2,3,4,5,6]. However,itischallengingtobuild
simulations that capture the unbounded diversity of real-life tasks, especially involving complex
manipulation. Whatiflearninginsteadoccursthroughdirectengagementwiththerealworld,without
extensiveenvironmentinstrumentationorhumansupervision?
8thConferenceonRobotLearning(CoRL2024),Munich,Germany.
4202
peS
03
]OR.sc[
1v86502.9042:viXraPrior work on real-world RL for learning new skills has been shown for locomotion [7, 8], and
inmanipulationforpick-place[9,10,11,12]ordexterousin-handtasks[13,14,15]instationary
setups. Consideracomplex,high-dimensionalsystemlikealeggedmobilemanipulatorlearningin
openspaces. Thefeasiblespaceofexplorationismuchlargerthaninconstrainedtabletopsetups.
Autonomousoperationofsuchacomplex,high-dimensionalrobotsoftendoesnotresultindata
thathasusefullearningsignal. Forexample, wewouldliketoavoidtherobotsimplywavingits
armintheairwithoutinteractingwithobjects. Furthermore,evenaftermakingsomeprogresson
thetask,therobotshouldnotstagnateneargoalstates. Whilepriorworkhasexploredusinggoal
cycles [16,13,17]tohelpmaintainstatediversity,thishasnotbeenshownformobilesystems. Such
systemsalsoneedtolearnmorecomplexskills,involvingconstrainedmanipulationoflargerobjects
and moving beyond pick and place, making sample-efficient learning critical. Finally, reward
supervisionusingcurrentRLapproachesoftenrequiresphysicalinstrumentationusingspecialized
sensors[18,19]orhumansintheloop[20,21,22,23],whichisdifficulttoscaletodifferenttasks.
Ourapproachtackleseachoftheseissuesofautonomy,efficientpolicylearning,andrewardspecifi-
cation. Weenablehigher-qualitydatacollectionbyguidingexplorationtowardobjectinteractions
usingoff-the-shelfvisualmodels. Thisleadstherobottosearchfor,navigateto,andgraspobjects
beforelearninghowtomanipulatethem. Wepreservestatediversitytopreventrobotstagnationby
extendingtheapproachofgoal-cyclestomobiletasksandwithmulti-robotsystems. Forsample
efficientpolicylearning,wecombineRLwithbehaviorpriorsthatcontainbasictaskknowledge.
Thesepriorscanbeplannerswithasimplifiedincompletemodel,orprocedurallygeneratedmotions.
Forrewardswithoutinstrumentationorhumaninvolvement,wecombinesemanticinformationfrom
detectionandsegmentationmodelswithlow-leveldepthobservationsforobjectstateestimation.
Themaincontributionofthisworkisageneralapproachforcontinuouslylearningmobilemanipula-
tionskillsdirectlyintherealworldwithautonomousRL.Themaincomponentsofourapproach
involve: (1) task-relevant autonomy for collecting data with useful learning signals, (2) efficient
controlbyintegratingpriorswithlearningpolicies,and(3)flexiblerewardspecificationcombining
high-levelvisual-textsemanticswithlow-leveldepthobservations.OurapproachenablesaSpotrobot
tocontinuallyimproveinperformanceonasetof4challengingmobilemanipulationtasks,including
movingachairtoagoalwiththetableinthecornerorcenteroftheplaypen,pickingupandvertically
balancingalong-handleddustpan,andsweepingapaperbagtoatargetregion. Ourexperiments
showthatourapproachgetsanaveragesuccessrateofabout80%acrosstasks,a4×improvement
overusingeitherRLorthebehaviorpriorindividuallywithourtask-relevantautonomycomponent.
2 RelatedWork
AutonomousReal-WorldRL: Previousworkforreal-worldRLmostlyinvolveseithermanip-
ulationfortable-toppick-placesettings[9,10,8],in-handdexterousmanipulation[15,13,14]or
locomotion behavior [24, 25, 8]. Approaches for automated resets needed for continual practice
includeinstrumentedenvironments[9,10],forward-backwardpolicies[26],graphstructureofsub-
tasks that serve as resets for one another [13, 14], or pre-trained, reliable reset policies [7]. For
mobilemanipulation,real-worldRLhasbeenlimitedtopickandplacetasks[11,12]. Inourwork,
weextendtheRLframeworktolearnchallengingmanipulationskillssuchassweepingandmoving
chairsforamobilesystem. Autonomousmobilesystemsshouldleveragetheabilityoftherobotto
movearoundtoextendtheeffectivereachoftherobotandattemptmanipulationtaskswithlarge
objectsthatarenotpossibleonatable-topsetup. Forefficientlearningonthesecomplextasks,we
leveragebehaviorpriors,whichhavesomebasictaskknowledge. Moreover,taskspecificationisa
bigchallenge[27]forreal-worldlearning. Currentapproachesoftenrequirephysicalinstrumentation
usingspecializedsensors[18,19]orhumansintheloop[20,21,22,23],whichisdifficulttoscaleto
differenttasks. Therehasbeensomeworkoncompletelyself-supervisedlearningsystemswithsome
extensionstorobotics[28,29],buttheseapproachesarechallengingtodeployoncomplextasksdue
tointractability,underspecification,andmisalignment. Weextendtheapproachofusinglanguage
goals and combining these with large-scale visual models [30], conditioned on open-vocabulary
prediction[31,32,33],toobtainobjectstates,whichcanbeusedtocomputereward.
2Task-relevant Autonomy Efficient Control Flexible Supervision
Detic
Dino
Goal-cycles Multi-goal
SAM
Prior + Policy
Text + Segment + Depth
Auto-grasp Multi-robot
Figure2: MethodOverview:Themaincomponentsofourapproachforrobotstocontinuallypracticetasks
intherealworld.Left:Task-relevantautonomytoensurecollectionofusefuldataviaobjectinteraction,and
maintainingstatediversityviaautomatedresetsusingmulti-goalandmulti-robotsetups. Center: Efficient
controlbyaidingpolicylearningwithbasictaskknowledgepresentinbehaviorpriorsintheformofplanners
withasimplifiedmodelorautomatedbehaviors. Right: Flexiblerewardsupervisionthatcombineshuman-
interpretablesemanticdetection-segmentationinformationwithlow-level,fine-graineddepthobservation.
MobileManipulation Inthe2015DARPARoboticsChallengeFinals,mobilemanipulationso-
lutionsprimarilyreliedonpre-builtobjectmodelsandtask-specificengineeringtoenablemobile
manipulation[34]. Morerecentworkmodularizingtasksintoskillprimitivesandinteractingwith
thoseprimitivesusingflexibleplanners,includinglargelanguagemodels,hasenabledmoregen-
eralization outside of pre-coded tasks [35, 36, 11, 37]. Imitation learning approaches to mobile
manipulationenablejointreasoningovermanipulationandnavigationactionsandgeneralizeacross
broadsetsoftasks[38,39,40,41,42]. However,imitationlearningrequiresanexpensivecollection
ofexperttrajectories. Incontrast,RLmethodscanlearnfromexperiencewithoutrequiringextra
humanlaborforeachnewtask. DecomposingtheactionspaceoverwhichtheRLpolicyoperatesen-
ablesmoretractableandefficientlearningoflong-horizonmobilemanipulationskills[43,44,45,46].
Inourwork,wemovebeyondtasksthatinvolvepickingandplacingtoinsteadlearnskillsthatrequire
coordinationbetweenthelegsandarms,e.g.,movingchairsorsweeping.
3 ContinuouslyImprovingMobileManipulationviaReal-worldRL
We design our approach to allow robots
Algorithm1AutonomousRLforMobileManipulation
to autonomously practice and efficiently
learn new skills without task demonstra- Require: Detection-segmentationmodelsM(.)
Require: BehaviorpriorP(.)
tionsorsimulationmodeling,andwithmin-
imal human involvement. The overview 1: InitializeDatabufferD,RLpolicyπ θ
of the approach we use is presented in 2: InitializetaskgoalG T withgoalobjectstateg T
3: InitializetrajectoriespertaskK,horizonH
Alg.1. Ourapproachhasthreecomponents,
4: whiletrainingdo
as depicted in Fig 2: task-relevant auton-
5: fortrajectory1:Kdo
omy, efficient control using behavior pri- 6: ApproachobjectusingAuto-grasp/nav
ors,andflexiblerewardspecification. The 7: fortimestep1:Hdo
firstensuresthedatacollectedislikelyto 8: Usepolicyπ θ(.)andpriorP(.)forseparate,
have learning signal, the second utilizes sequentialorresidualcontrol
signalfromdatatocollectevenbetterdata 9: Computerewardr tusingM(o t)
toquicklyimprovethecontroller,andthe 10: Add(o t,a t,o t+1,r t)(cid:55)→D
11: Samplebatchβ ∼DtoupdateπviaRL
thirddescribeshowtodefinelearningsig-
12: endfor
nal for tasks. This allows learning diffi-
cultmanipulationtasks,includingtooluse
13: (optional)Ifdistance(x,g T)≤ϵ,break
14: endfor
andconstrainedmanipulationoflargeand
15: SwitchtaskgoalG T
heavyobjects. Next,wedescribeeachof 16: endwhile
thesecomponentsinfurtherdetail.
3(a) (b) (c) (d)
(e) (f) (g) (h)
Figure3: TaskGoals:Statesthatdefinegoal-cyclesforour4tasks-(a-b):ChairMovingwithacornertable,
(c-d):ChairMovingwithamiddletable,(e-f):LongHandledDustpanStandup,(g-h):Sweeping
3.1 Task-RelevantAutonomy
Auto-Grasp/Auto-Nav: Forsafeautonomousoperation,wefirstcreateamapbywalkingtherobot
aroundtheenvironment. Thismapisusedbytherobottoavoidcollisionsduringitsautonomous
learningprocess. Toensuredatacollectedinvolvesobjectinteraction, everyepisodebeginswith
therobotestimating,movingto,and/orgraspingtheobjectofinterestforthetask. Theobjectstate
isestimatedusingdetectionandsegmentationmodelsalongwithdepthobservations,asdescribed
insection3.3. TherobotthennavigatestowardstheobjectpositionusingRRT*toplaninSE(2)
spaceusingthecollisionmap,andoptionallydeploysthegraspingskillfromtheBostonDynamics
SpotSDKdependingonthetask. Thisgraspisgeneratedviaageometricalgorithmthatfitsagrasp
locationwithageometricmodelofthegripper,scoresdifferentpossiblegrasps,andpicksthebest
one. Wedonotconstrainthegrasptype,oronwhichportionoftheobjectthegraspisperformed.
Thisallowstherobottokeeppracticingregardlessofwhichpositionororientationtheobjectmight
endupinasaresultofcontinualinteraction.
Goal-Cycles: To prevent robot stagnation near goal states, we set up ’goal-cycles’ within tasks,
whichserveasautomatedtaskresets. Weshowthedifferentgoalstatesusedineachofthe4tasks
weconsiderinFig.3. Inthecaseofthechairmovingtasks(Fig.3: a-d),therobotalternatesbetween
goalsthatarefarapartinthex-yplane,andforthedustpanstand-uptask(Fig.3e,f),therobotneeds
topickupthefallendustpanandverticallyorientandbalanceit. Forthesweepingtask(Fig.3: g-h),
weuseamulti-robotsetupforthegoalcycle,whereonerobotholdsthebroomandneedstosweep
thepaperbagintothetargetregion(denotedbythebluebox),whiletheotherneedstopickupthe
bag and drop it back into the region where it can be swept. Since we only need learning for the
sweepingskill,therobotthatpicksupthebagrunsthepreviouslydescribedauto-graspprocedure.
3.2 Prior-guidedPolicyLearning
IncorporatingPriors: Weenableefficientlearningbyleveragingbehaviorpriorsthatutilizebasic
knowledge of the task. This removes the burden from the learning algorithm from having to
rediscover this knowledge and instead focus on learning additional behavior needed to solve the
task. Forexample,anRRT*plannerwithasimplified2Dmodelcanhelpanagentmovebetween
twopointsinthex-yplanewhileavoidingobstacles. Startingwiththisprior,usingRLcanhelpthe
robotlearntorecoverfromcollisionsanddealwithdynamicconstraintsnotrepresentedinthemodel.
Concretely,thepriorisafunctionP(.)thattakesinanobservationo andproducesanactiona ,
t t
similartoapolicyπ(a |o ). Wecandeploythepriorandthepolicyinthefollowingways:
t t
41. Separate: Trajectories are collected independently using either the prior
{P(a |o ),...,P(a |o )} or the policy {π(a |o ),...,π(a |o )}. Instead of learning en-
0 0 T T 0 0 T T
tirelyfromscratch,weincorporatethe(potentially)suboptimaldatafromthepriorintotherobot’s
data buffer to bootstrap learning. Intuitively, the prior is likely to see a higher reward than a
completelyrandomlyinitializedpolicy,especiallyforsparserewardtasks. Wemakenoassumptions
ontheoptimalityoftheprior,andbootstraplearningviaincorporatingitsdata. Inpractice,wefirst
collecttrajectoriesusingtheprior,toinitializethedatabufferfortrainingtheonlineRLpolicyπ(.).
2. Sequential: Inadditiontoprovidingdatawithbettersignaltothelearningprocess, priorscan
reliablymakereasonableprogressonatask. Thisisbecausetheyoftengeneralizewell,forexample,
anSE(2)plannerwillmakereasonableprogressinmovingarobotbetweenanytwopointsinthe
x-yplane,evenwhenitperformsconstrainedmanipulation. Wewouldneedtosamplemanytimes
fromthepriortodistillthisinformationpurelyviathedatabuffer. Hence,amoredirectapproachis
toutilizetheprioralongwiththepolicyforcontrol. Wedothisbysequentiallyexecutingtheprior,
followedbythepolicy. Thatis,trajectoriescollectedinthismannertaketheform:
{P(a |o ),..,P(a |o ),π(a |o ),..,π(a |o ).} (1)
0 0 L L L+1 L+1 T T
Thus, the prior structures the policy’s initial state distribution, making learning easier. The data
collectedbythepriorisaddedtothedatabuffer,allowingthepolicytolearnfromthesetransitions.
3. Residual: Incertaincases,thepriormightnotberobustenoughtodeploydirectlybutnonetheless
providereasonableboundsonwhatactionsshouldbeexecuted. Forexample,forsweepinganobject,
therobot’sbaseshouldroughlybeinthevicinityofthetrashbeingswept,butthisdoesnotprescribe
whatexactactionstotake. Suchapriorcanbeusedresidually,whereapolicyadjuststheactionsof
thepriorateverytimestepbeforebeingexecuted. Thesetrajectoriestaketheform:
{P(a |o )+π(a |o ),...,P(a |o )+π(a |o )} (2)
0 0 0 0 T T T T
RLPolicyTraining: TheRLobjectiveislearnparametersθofapolicyπ tomaximizetheexpected
θ
discountedsumofrewardsR(s ,a ):
t t
(cid:34) T (cid:35)
(cid:88)
J(π θ)=E s0∼p0 γtR(s t,a t), (3)
at∼πθ(at|st)
t=0
st+1∼P(st+1|st,at)
wherep istheinitialstatedistribution,P isthetransitionfunctionandγ isthediscountfactor. For
0
sampleefficientlearningthateffectivelyincorporatespriordata,weusethestate-of-the-artmodel-free
RLalgorithmRLPD[47]. RLPDisanoff-policymethodbasedonSoft-ActorCritic(SAC)[48],
whichsamplesfromamixtureofdatasourcesforonlinelearning. LikeREDQ[49],RLPDuses
a large ensemble of critics and in-target minimization over a random subset of the ensemble to
mitigateover-estimationcommoninTD-Learning. Sinceourobservationsconsistofrawimages,we
incorporatetheimageaugmentationsaddedbyDrQ[50]tothebaseRLalgorithm.
3.3 FlexibleSupervisionviaText-PromptedSegmentation
For flexible reward supervision, we combine semantic high-level information from vision and
languagemodelswithlow-leveldepthobservations. Eachtaskisdefinedbyadesiredconfiguration
ofsomeobjectofinterest,sowederivearewardfunctionbycomparingtheestimatedstateofthe
objectatagiventimetothisdesiredstate(seeSection4fortask-specificdetails). Toestimatethe
stateoftheobject,westartbyusinganopen-vocabularydetectionmodelDetic[51]toobtainthe
boundingboxcorrespondingtotheobjectofinterest. Wethenobtainthecorrespondingobjectmask
byconditioningasegmentationmodel,Segment-Anything[30],ontheboundingbox. Finally,using
depthobservationsandthecalibratedcamerasystemforeithertheegocentricorfixedthird-person
cameras, we get a point cloud. Although this estimation is noisy, we find it sufficient to enable
learningeffectivecontrolpoliciesviareal-worldRL.Thissystemisflexibleenoughtohandledifferent
objectsofinterest,suchasthechair,longhandleddustpanforverticalorientation,orthepaperbag
forsweeping. Fulldetailsontheprompts,detectionandsegmentationmodels,andrewardfunctions
foreachtaskinthesupplementalmaterials.
54 ExperimentalSetup
Forourexperiments,weruncontinualautonomousRLusingtheSpotrobotandarmsystemina
playpenofabout6×5meters,enclosedwithmetalrailingsforsafety. Theplaypenismappedbefore
autonomousoperationtoensuretherobotstayswithinboundsanddoesn’tcollidewiththerailings.
Thenavigationaspectoftaskautonomyinvolvessearchingforobjectsofinterest. Sincethemain
focusofthisworkisonlearningcomplexmanipulationskills,wedonotuselearningforthesearch
problem; instead, we rely on a fixed camera in the scene. In addition to this, we also use the 5
egocentricbodycamerasoftheSpotwhilesearchingforobjects.
The chair-moving task requires
the robot to grasp a chair and Prior Policymode Reward Sparse
moveitbetweengoallocations.
Weconsidertwovariants,chair- Chair-tablecorner RRT* Sequential Chair-goaldistance False
Chair-tablemiddle RRT* Sequential Chair-goaldistance False
tablecorner(Fig.3a-b)andchair-
DustpanStandup Scripted Separate Handleheight True
tablemiddle(Fig.3c-d). Thelat- Sweeping Distanceconstraint Residual Bag-goaldistance False
terismorechallengingsincecol-
Table1: Welistthechoiceofprior,howitiscombinedwiththepolicy,
lisionsbetweenthechairandta-
howrewardrelatestotheobjectstate,andwhethertherewardissparse.
blebasearemuchmorefrequent
andtherobothastooperateinamuchtighterspace. Thedustpanstanduptaskinvolvesliftingupthe
longhandleofadust-pan(Fig.3-e),andthenverticallybalancingitsothatitcanstayuprightonits
base(Fig.3-f). Sweepinginvolvestworobots,whereoneoftherobotsholdsabroominitsgripper
andneedstouseittosweepapaperbagintoagoalregion(Fig.3-g). Theotherrobotdoesnotuse
learning,insteadusingtheauto-graspproceduretoresetthepaperbagbypickingitupanddropping
itclosetotheinitialposition(Fig.3-h). Foreachtask,wespecifysuccesscriteriafortaskcompletion,
whichcorrespondstoreachingthegoalstatesinFig.3. Welistthechoiceoftheprior,itscombination
withthepolicy,thestatemeasurementsusedforreward,andrewardsparsityinTable1.
TheobservationspaceforRLpolicytrainingforalltasksconsistsofthree128X128RGBimage
sources: the fixed, third-person camera and two egocentric cameras on the front of the robot.
Additionally, we use the body position, hand position, and target goal. The action space for the
chair and sweeping tasks is 5 dimensional, with base (x,y,θ) control and (x,y) control for the
handrelativetothebase. Thedustpanstand-uptaskis3dimensional,consistingof(z,yaw,gripper)
commandsforthehand, wherethegripperopenactionterminatestheepisode. Weusethesame
networkarchitecturesforimageprocessing,criticfunctions,policy,etc.,forallcomparisons. Please
seesupplementarymaterialsformoredetailsonthefullrewardfunctions,successcriteria,procedural
functionsforpriors,hyper-parametersforlearning,andnetworkdetails.
5 Results
Ourreal-worldexperimentstestwhetherautonomousreal-worldRLcanenablerobotstocontinuously
improvemobilemanipulationskillsforperformingvarioustasks. Specifically,weseektoanswerthe
followingquestions: 1)Canarealrobotlearntoperformtasksthatrequirebothmanipulationand
mobilityinanefficientmanner? 2)Doesperformancecontinuallyimproveastherobotcollectsmore
data? 3)HowdoestheapproachofstructuredexplorationusingpriorsalongwithRL,compareto
solelyusingtheprior,orusingonlyRL?4)Howdoesthepolicylearnedviaautonomoustraining
performwhenevaluatedintestsettings?
Task-relevant Autonomy: Running the robot without auto-grasp or goal-cycles, with the full
actionspacecomprisingbaseandarmmovementtoanypositionintheplaypendoesnotleadto
anymeaningfulchangeintaskprogressevenoverlongperiodsoftime. Further,suchoperationis
unsafesincetherobotarmcangetstuckintheenclosurerailings,orstrikethewallinanoutstretched
configuration. Hence, all the experiments we conduct, including those for baselines, utilize the
task-relevantautonomycomponentsothattherobotcanmakesomeprogressonthetask.
6Chair Move - Table Corner Chair Move - Table Middle Long Handled Dustpan Sweeping
0.8 0.8 0.6 0.4
0.6 0.6 0.3
0.4
0.4 0.4 0.2
0.2 0.2 0.2 0.1
0.0 0.0 0.0 0.0
0 1000 2000 3000 0 1000 2000 3000 0 200 400 600 800 0 1000 2000
Steps Steps Steps Steps
RL + Prior (Ours) RL (without prior) Prior
Figure 4: Continualtrainingimprovement: Successratevsnumberofsamplesforours, onlyRLand
onlyprior.Notethatweuseourtask-relevantautonomyapproachwithallmethods.Weseethatourapproach
continuouslyimproveswithexperienceacrosstasks,learningmuchfasterthanRLwithoutpriors,andattaining
significantlyhigherperformancethanjustusingtheprior.
ContinualImprovementviaPractice: Givenourtaskautonomyprocedure,howeffectiveisour
proposedapproachofcombiningrealworldRLwithbehaviorpriors,asopposedtousingeitheronly
thepriororRL?FromFig.4, weseethatourapproachlearnssignificantlyfasterthanusingonly
RL,andattainsmuchsuperiorperformancethantheprior,foreachofthetasks. Ontheespecially
challengingsweepingtaskwhichinvolvestooluseofthebroomwithadeformablepaperbag,using
onlythepriororonlyRLleadstoalmostnoprogress,whileourmethodisabletolearnthetask. Each
robottrainingruntakesaround8-15hours,withthevariationintimeowingtodifferentgoalreset
strategiesacrosstasksandvarianceinhowoftentherobotretriesgraspingobjectsfortask-relevant
autonomy. Hence,forfaircomparisonsacrossmethods,weusethenumberofreal-worldsamples
collectedtomeasureefficiency. Thesystemalsoneedstoberobusttomanydifferentfactorsinorder
tolearnthesetasks. Thetrainingareaisexposedtosunlight,andtherobotkeepscollectingdata
andlearningthroughoutthedaywithvaryingdegreesofillumination. Objectstartingpositionsand
graspscanvarywidely,whichaffectstheresultingobjectdynamicswhenpracticingthetask.
RLwithoutPrior: Forsometasks,usingRL
withoutthepriordoesimproveinperformance,
Chair Move - Table Corner Chair Move - Table Middle
butatamuchslowerratethanourmethod.With- 1
outtheprior,RLoftenspendssamplesexploring 0
0
partsofthestatethatarefarfromthegoal. To
1
illustratethis,weplottheaveragerewardover 1
eachtrajectoryforthechairtasks(Fig.5). The 2
reward for this task is of the form −x+e−x, 2
0 1000 2000 3000 0 1000 2000 3000
wherexisthedistanceofthechairtothegoal Steps Steps
positionofthechair. Thenegativemeanreward RL + Prior (Ours) RL (without prior) Prior
for RL without the prior implies that the dis-
Figure5: Trainingmeanreward: Meanrewardvs
tancextothegoalisquitelarge,meaningthat
number of samples for the chair moving tasks. The
therobotisoftenfarfromthegoal. Ontheother negativeaveragerewardforRLwithoutpriorsindicates
hand,sinceourmethodexecutesthepriorand thattherobotisoftenfarfromthegoallocation.
policysequentiallyforthechairtask,ourpolicyalwaysstartsoutreasonablyclosetothegoal,andcan
thuscanpickuponhighrewardsignalmoreoften,leadingtofasterlearning. Weobserveasimilar
patternforthesweepingtask,whereusingonlyRLleadstherobottowanderaroundtheplaypen,
greatlydecreasingthelikelihoodofinteractingwiththepaperbagandobtaininghighreward.
PriorwithoutRL:Whilethebehaviorpriorsareeffectiveatbootstrappinglearning,theyarenot
sufficientontheirown. Thisisbecausetheydonotadaptorlearnfromexperience, andsokeep
repeating the same mistakes without improvement over time. We illustrate a qualitative failure
exampleofthebehaviorpriorforthechairmovingtaskinFig.6,wheretherobotfollowingtheRRT*
plannerrunsintoacollisionstateduetothesimplifiedmodelbeingused. Incontrast,ourapproach
adaptsthepolicybasedonitsexperiencetoimproveitsperformance,avoidingsuchcollisions. For
sometaskslikesweepingthebehaviorpriorismuchsimpler,onlyprovidingaconstraintnottomove
toofarawayfromthepaperbag,whichdoesnotspecifyhowtherobotshouldsweep.
7
etaR
sseccuS
etaR
sseccuS
etaR
sseccuS
draweR
naeM
draweR
naeM
etaR
sseccuSFigure6: Left:Theprior(RRT*withincompletemodel)getsstuckinacollisionwiththetableandisunable
torecoverastheplannerdoesnothaveamodelofchair-tableinteractiondynamics. Right: Ourapproach
effectivelyrecoversfromcollisionstocompletethetask.
Final Policy Evaluation: We evaluate the final policies obtained after autonomous, contin-
ual practice and find that our approach obtains an average success rate of 80% across tasks
from Table 2. For comparisons between our method and using only RL, we evaluate models
obtained with the same number of real world samples. For evaluation, we use the determin-
istic policy instead of sampling from the stochastic distribution, which is used during training.
Further,wesettheinitialstateofthe
objectstobeclosetotheoppositegoal Ours OnlyRL OnlyPrior OfflineRL
inthegoalcycle. Forinstance,inthe
sweepingtask,weinitializethepaper Chair-tablecorner 100% 20% 22% 10%
bagroughlyinthelocationshownin Chair-tablemiddle 80% 50% 38% 20%
Fig.3-h. Thisisdifferentfromtrain- DustpanStandup 60% 20% 18% 60%
ing,wherethepaperbagcouldendup Sweeping 80% 0% 5% 10%
inanylocation,andsuccessiscontinu-
Table2: EvaluationComparison:Thesuccessrateofthefinal
allyevaluated.Wenotethatonthepar-
policyevaluatedondifferenttasks.Forevaluation,weusethedeter-
ticularlyhardtaskofsweeping,none
ministicpolicyinsteadofsamplingfromthestochasticdistribution
of the other methods are successful, likeintraining.Ourapproachgetsanaveragesuccessrateof80%,
whileourapproachgets80%success. about4×improvementoverusingonlythepriororonlyRL.
PriorDataQuality: Thebehaviorpriorhelpsourapproachintwoways,bystructuringexploration
foronlinelearning,andalsobyprovidinghigherqualitydatathanrandomsearch,containinghigher
reward. Totestthequalityofthedataobtainedbytheprior,werunofflineRLonthedatasetcollected
by the prior. This utilizes the reward of transitions to learn a policy, without any online rollouts.
FromTable2,weseethatonthechairandsweepingtasks,thebehaviorpriordataqualityismuch
worse,withanaveragesuccessrateof13%. ThecaseofdustpanstandupisnotablesinceofflineRL
performsonparwithourmethod,gettingabout60%success. Whilethenumericalperformanceis
similar,thereisaconsiderablequalitativedifferenceinthebehaviorlearned. Ourapproachlearns
strategiesthatareverydifferentfromthebehaviorprior,throughexploration. Thisinvolvesraising
therobot’sarmanddroppingthedustpan,suchthatitlandsupright. Ontheotherhand,offlineRL
sticksclosetothesuccessfulexamplesfromthebehaviorpriorgenerations.
6 DiscussionandLimitations
Wehavepresentedanapproachforcontinuouslylearningnewmobilemanipulationskills. Thisis
enabledusingtask-relevantautonomy,efficientreal-worldcontrolusingbehaviorpriors,andflexible
rewarddefinition. Thecurrentapproachuseslearningprimarilyforacquiringlow-levelmanipulation
skillsafterobjectsaregrasped. Usingautomatedproceduresfornavigationandsearchmakinguse
of a fixed third-person camera is a current limitation. This can be addressed by adding learning
forthehigher-levelsearchproblemtoo,whichwouldallowtherobottorelyjustonitsegocentric
observations. Thiswouldallowlearninginmoreunstructured,open-endedenvironments.
87 Acknowledgements
We thank Laura Smith, Murtaza Dalal, Ananye Agrawal, Kaiyu Zheng and Farzad Niroui for
thoughtfuldiscussionsandtheirvaluablefeedback.ThisworkwassupportedbytheDARPAMachine
CommonsenseProgram,GoogleResearchAwardandONRN00014-22-1-2096.
References
[1] M.Cutler,T.J.Walsh,andJ.P.How. ReinforcementLearningwithMulti-FidelitySimulators.
InICRA,pages3888–3895.IEEE,2014.
[2] J.Tobin,R.Fong,A.Ray,J.Schneider,W.Zaremba,andP.Abbeel. DomainRandomization
forTransferringDeepNeuralNetworksfromSimulationtotheRealWorld. InIROS,pages
23–30.IEEE,2017.
[3] I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino,
M. Plappert, G. Powell, R. Ribas, et al. Solving Rubik’s Cube with a Robot Hand. arXiv
preprintarXiv:1910.07113,2019.
[4] T. Haarnoja, B. Moran, G. Lever, S. H. Huang, D. Tirumala, M. Wulfmeier, J. Humplik,
S.Tunyasuvunakool,N.Y.Siegel,R.Hafner,etal. LearningAgileSoccerSkillsforaBipedal
RobotwithDeepReinforcementLearning. arXivpreprintarXiv:2304.13653,2023.
[5] R. Yang, Y. Kim, A. Kembhavi, X. Wang, and K. Ehsani. Harmonic Mobile Manipulation.
arXivpreprintarXiv:2312.06639,2023.
[6] X.Cheng,K.Shi,A.Agarwal,andD.Pathak. ExtremeParkourwithLeggedRobots. arXiv
preprintarXiv:2309.14341,2023.
[7] L.Smith,J.C.Kew,X.B.Peng,S.Ha,J.Tan,andS.Levine. LeggedRobotsthatKeepon
Learning: Fine-TuningLocomotionPoliciesintheRealWorld. InICRA,pages1593–1599.
IEEE,2022.
[8] P.Wu,A.Escontrela,D.Hafner,K.Goldberg,andP.Abbeel. DayDreamer: WorldModelsfor
PhysicalRobotLearning. InCoRL,2023.
[9] D.Kalashnikov,A.Irpan,P.Pastor,J.Ibarz,A.Herzog,E.Jang,D.Quillen,E.Holly,M.Kalakr-
ishnan,V.Vanhoucke,etal. QT-Opt: ScalableDeepReinforcementLearningforVision-Based
RoboticManipulation. arXivpreprintarXiv:1806.10293,2018.
[10] D.Kalashnikov,J.Varley,Y.Chebotar,B.Swanson,R.Jonschkowski,C.Finn,S.Levine,and
K.Hausman. MT-Opt: ContinuousMulti-TaskRoboticReinforcementLearningatScale. arXiv
preprintarXiv:2104.08212,2021.
[11] C. Sun, J. Orbik, C. M. Devin, B. H. Yang, A. Gupta, G. Berseth, and S. Levine. Fully
AutonomousReal-WorldReinforcementLearningwithApplicationstoMobileManipulation.
InCoRL,pages308–319.PMLR,2022.
[12] A.Herzog,K.Rao,K.Hausman,Y.Lu,P.Wohlhart,M.Yan,J.Lin,M.G.Arenas,T.Xiao,
D. Kappler, et al. Deep rl at scale: Sorting waste in office buildings with a fleet of mobile
manipulators. arXivpreprintarXiv:2305.03270,2023.
[13] A.Gupta,J.Yu,T.Z.Zhao,V.Kumar,A.Rovinsky,K.Xu,T.Devlin,andS.Levine.Reset-Free
ReinforcementLearningviaMulti-TaskLearning: LearningDexterousManipulationBehaviors
withoutHumanIntervention. InICRA,pages6664–6671.IEEE,2021.
[14] K. Xu, Z. Hu, R. Doshi, A. Rovinsky, V. Kumar, A. Gupta, and S. Levine. Dexterous Ma-
nipulationfromImages: AutonomousReal-WorldRLviaSubstepGuidance. InICRA,pages
5938–5945.IEEE,2023.
9[15] A.Nagabandi,K.Konolige,S.Levine,andV.Kumar. DeepDynamicsModelsforLearning
DexterousManipulation. InCoRL,pages1101–1112.PMLR,2020.
[16] W.Han,S.Levine,andP.Abbeel. LearningCompoundMulti-StepControllersunderUnknown
Dynamics. InIROS,2015.
[17] A. Gupta, C. Lynch, B. Kinman, G. Peake, S. Levine, and K. Hausman. Demonstration-
BootstrappedAutonomousPracticingviaMulti-TaskReinforcementLearning. InICRA,pages
5020–5026.IEEE,2023.
[18] A.Yahya,A.Li,M.Kalakrishnan,Y.Chebotar,andS.Levine. CollectiveRobotReinforcement
LearningwithDistributedAsynchronousGuidedPolicySearch. InIROS,2017.
[19] C.SchenckandD.Fox. VisualClosed-LoopControlforPouringLiquids. InICRA,2017.
[20] J.Fu,A.Singh,D.Ghosh,L.Yang,andS.Levine. VariationalInverseControlwithEvents: A
GeneralFrameworkforData-DrivenRewardDefinition. InNeurIPS,volume31,2018.
[21] A.Singh,L.Yang,K.Hartikainen,C.Finn,andS.Levine. End-to-EndRoboticReinforcement
LearningwithoutRewardEngineering. InRSS,2019.
[22] H. Liu, S. Nasiriany, L. Zhang, Z. Bao, and Y. Zhu. Robot Learning on the Job: Human-
in-the-LoopAutonomyandLearningDuringDeployment. arXivpreprintarXiv:2211.08416,
2022.
[23] L.Smith,N.Dhawan,M.Zhang,P.Abbeel,andS.Levine. Avid: Learningmulti-stagetasks
viapixel-leveltranslationofhumanvideos. arXivpreprintarXiv:1912.04443,2019.
[24] S.Ha,P.Xu,Z.Tan,S.Levine,andJ.Tan. LearningtoWalkintheRealWorldwithMinimal
HumanEffort. InCoRL,2020.
[25] L.Smith,I.Kostrikov,andS.Levine. DemonstratingaWalkinthePark: LearningtoWalkin
20MinutesWithModel-FreeReinforcementLearning. InRSS,2022.
[26] A. Sharma, A. M. Ahmed, R. Ahmad, and C. Finn. Self-Improving Robots: End-to-End
AutonomousVisuomotorReinforcementLearning. arXivpreprintarXiv:2303.01488,2023.
[27] H.Zhu,J.Yu,A.Gupta,D.Shah,K.Hartikainen,A.Singh,V.Kumar,andS.Levine. The
IngredientsofReal-WorldRoboticReinforcementLearning. InICLR,2020.
[28] V. H. Pong, M. Dalal, S. Lin, A. Nair, S. Bahl, and S. Levine. Skew-Fit: State-Covering
Self-SupervisedReinforcementLearning. InICML,2020.
[29] R.Mendonca,S.Bahl,andD.Pathak. ALAN:AutonomouslyExploringRoboticAgentsinthe
RealWorld. InICRA,2023.
[30] A.Kirillov,E.Mintun,N.Ravi,H.Mao,C.Rolland,L.Gustafson,T.Xiao,S.Whitehead,A.C.
Berg,W.-Y.Lo,etal. SegmentAnything. arXivpreprintarXiv:2304.02643,2023.
[31] W. Yu, N. Gileadi, C. Fu, S. Kirmani, K.-H. Lee, M. G. Arenas, H.-T. L. Chiang, T. Erez,
L.Hasenclever,J.Humplik,etal. LanguagetoRewardsforRoboticSkillSynthesis. arXiv
preprintarXiv:2306.08647,2023.
[32] H.Li,X.Yang,Z.Wang,X.Zhu,J.Zhou,Y.Qiao,X.Wang,H.Li,L.Lu,andJ.Dai. Auto
MC-Reward: AutomatedDenseRewardDesignwithLargeLanguageModelsforMinecraft.
arXivpreprintarXiv:2312.09238,2023.
[33] K.Baumli,S.Baveja,F.Behbahani,H.Chan,G.Comanici,S.Flennerhag,M.Gazeau,K.Hol-
sheimer,D.Horgan,M.Laskin,etal. Vision-LanguageModelsasaSourceofRewards. arXiv
preprintarXiv:2312.09187,2023.
10[34] E. Krotkov, D. Hackett, L. Jackel, M. Perschbacher, J. Pippine, J. Strauss, G. Pratt, and
C.Orlowski. TheDARPARoboticsChallengeFinals: ResultsandPerspectives. Journalof
FieldRobotics,34(2):229–240,22017. doi:10.1002/rob.21683.
[35] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg, S. Rusinkiewicz, and
T.Funkhouser. TidyBot: PersonalizedRobotAssistancewithLargeLanguageModels. Au-
tonomousRobots,2023.
[36] B.Wu,R.Martin-Martin,andL.Fei-Fei. M-EMBER:TacklingLong-HorizonMobileManipu-
lationviaFactorizedDomainTransfer. InICRA,2023.
[37] M.Bajracharya,J.Borders,D.Helmick,T.Kollar,M.Laskey,J.Leichty,J.Ma,U.Nagarajan,
A.Ochiai,J.Petersen,etal. AMobileManipulationSystemforOne-ShotTeachingofComplex
TasksinHomes. InICRA,pages11039–11045.IEEE,2020.
[38] A. Gupta, A. Murali, D. P. Gandhi, and L. Pinto. Robot Learning in Homes: Improving
GeneralizationandReducingDatasetBias. InNeurIPS,volume31,2018.
[39] A.Brohan,N.Brown,J.Carbajal,Y.Chebotar,J.Dabis,C.Finn,K.Gopalakrishnan,K.Haus-
man,A.Herzog,J.Hsu,etal. RT-1: RoboticsTransformerforReal-WorldControlatScale. In
arXivpreprintarXiv:2212.06817,2022.
[40] M.Ahn,A.Brohan,N.Brown,Y.Chebotar,O.Cortes,B.David,C.Finn,C.Fu,K.Gopalakr-
ishnan,K.Hausman,A.Herzog,D.Ho,J.Hsu,J.Ibarz,B.Ichter,A.Irpan,E.Jang,R.J.Ruano,
K.Jeffrey,S.Jesmonth,N.Joshi,R.Julian,D.Kalashnikov,Y.Kuang,K.-H.Lee,S.Levine,
Y.Lu,L.Luu,C.Parada,P.Pastor,J.Quiambao,K.Rao,J.Rettinghouse,D.Reyes,P.Sermanet,
N.Sievers,C.Tan,A.Toshev,V.Vanhoucke,F.Xia,T.Xiao,P.Xu,S.Xu,M.Yan,andA.Zeng.
DoAsICanandNotAsISay: GroundingLanguageinRoboticAffordances. InarXivpreprint
arXiv:2204.01691,2022.
[41] N.M.M.Shafiullah,A.Rai,H.Etukuru,Y.Liu,I.Misra,S.Chintala,andL.Pinto.OnBringing
RobotsHome. arXivpreprintarXiv:2311.16098,2023.
[42] Z.Fu,T.Z.Zhao,andC.Finn. MobileALOHA:LearningBimanualMobileManipulationwith
Low-CostWhole-BodyTeleoperation. InarXiv,2024.
[43] F.Xia,C.Li,R.Mart´ın-Mart´ın,O.Litany,A.Toshev,andS.Savarese. ReLMoGen: Integrating
Motion Generation in Reinforcement Learning for Mobile Manipulation. In ICRA, pages
4583–4590.IEEE,2021.
[44] J. Gu, D. S. Chaplot, H. Su, and J. Malik. Multi-Skill Mobile Manipulation for Object
Rearrangement. InICLR,2023.
[45] Y.Ma,F.Farshidian,T.Miki,J.Lee,andM.Hutter. CombiningLearning-BasedLocomotion
PolicyWithModel-BasedManipulationforLeggedMobileManipulators. IEEERoboticsand
AutomationLetters,7(2):2377–2384,2022.
[46] N.Yokoyama,A.Clegg,J.Truong,E.Undersander,T.-Y.Yang,S.Arnaud,S.Ha,D.Batra,and
A.Rai. ASC:AdaptiveSkillCoordinationforRoboticMobileManipulation. IEEERobotics
andAutomationLetters,9(1):779–786,2024. doi:10.1109/LRA.2023.3336109.
[47] P.J.Ball,L.Smith,I.Kostrikov,andS.Levine. EfficientOnlineReinforcementLearningwith
OfflineData. InICML,2023.
[48] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft Actor-Critic: Off-Policy Maximum
EntropyDeepReinforcementLearningwithaStochasticActor. InICML,pages1861–1870,
2018.
[49] X. Chen, C. Wang, Z. Zhou, and K. Ross. Randomized Ensembled Double Q-Learning:
LearningFastWithoutaModel. InICLR,2021.
11[50] D.Yarats,R.Fergus,andI.Kostrikov. ImageAugmentationIsAllYouNeed: Regularizing
DeepReinforcementLearningfromPixels. InICLR,2021.
[51] X. Zhou, R. Girdhar, A. Joulin, P. Kra¨henbu¨hl, and I. Misra. Detecting Twenty-Thousand
ClassesUsingImage-LevelSupervision. InECCV,2022.
[52] K. Stachowicz, D. Shah, A. Bhorkar, I. Kostrikov, and S. Levine. FastRLAP: A System
forLearningHigh-SpeedDrivingviaDeepRLandAutonomousPracticing. arXivpreprint
arXiv:2304.09831,2023.
[53] S.Liu,Z.Zeng,T.Ren,F.Li,H.Zhang,J.Yang,C.Li,J.Yang,H.Su,J.Zhu,etal. Grounding
DINO:MarryingDINOwithGroundedPre-TrainingforOpen-SetObjectDetection. arXiv
preprintarXiv:2303.05499,2023.
[54] T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan,P.Dolla´r,andC.L.Zitnick.
Microsoftcoco: Commonobjectsincontext. InComputerVision–ECCV2014: 13thEuropean
Conference,Zurich,Switzerland,September6-12,2014,Proceedings,PartV13,pages740–755.
Springer,2014.
12Appendix
A Videos
Themainvideosummarizingourresultscanbefoundinresult video.mp4inthezipfolder. This
depicts the robot performing each of the tasks we consider - moving the chair 1) with a table in
thecornerintheplaypen,2)withatableinthemiddleoftheplaypen,3)pickingupadustpanand
verticallyorientingitsuchthatitcanstandup,4)sweepingapaperbagintoatargetregion. Wealso
includetimelapsevideoswhichshowhowourapproachadaptsbehaviorovertime.
B PolicyTraining
For our experiments we run DrQ implemented in the official RLPD codebase open-sourced by
Ball et al. [47]. Since we run image-based real robot experiments, we use learning algorithm
hyperparameters(includingfortheimageencoders)from Stachowiczetal.[52],whichdeployed
RLPDforracecardriving. Theobservationsarefirstencodedintoalatentspace(separatelyforthe
actorandcritic),andtheprocessedlatentisusedbythecriticensembleortheactor. Detailsofthe
architectureforeachofthese,inadditiontohyperparametersfortrainingisprovidedinTable3.
Weusebothimageandvectorobservationsforlearning. Eachoftheseisprocessedbyanimage
encoderora1-layerdenseencodingforvectorobservations,andthecorrespondinglatentsareall
concatenatedtogetherandthenusedasinputfortheactororcritic.Notethatweuseseparateencoders
forthecriticandthecritic. Weusethearchitecturefrom Stachowiczetal.[52]forencodingeach
imagesource,withoutusinganypre-trainedembeddings,thenetworkisretrainedfromscratchfor
eachnewexperiment. Thereare4RGBimagesources. Thenetworkencodersareprovidedwith
thelast3framesforeachimagesource,exceptforthegoalimage,sincethisremainsfixedforthe
episode. Theimagesourcesare-
• Egocentricfront-leftimage
• Egocentricfront-rightimage
• Third-personfixed-camcurrentimage
• Third-personfixed-camgoalimage
Weuse(128,128)spatialresolutionfortheegocentricimages,and(256,256)fortheimagesfromthe
thirdpersoncamera. Thelatterusesahigherresolutionsinceitisfurtherawayfromthesceneand
objectsappearsmaller/lessclear.
Inaddition,wehavetwovectorobservations-
• Bodypose-Wecomputethe(x,y,θ)positionoftherobotbodyintheSE(2)planerelativeto
thecalibratedplaypenframe(calibrationdetailsinsectionD).Theinputtothenetworkis4
dimensional,consistingof(x,y,cos(θ),sin(θ)). Weusesin,costransformsfortheangleto
avoiddiscontinuitiesininput,since−πandπrepresentthesameorientation.
• Handpose-6-dofendeffectororientationofthehandrelativetothebaseposition.
Therearecertainlearningparametersthataretunedseparatelyforeachenvironment,whichwelistin
Table4. Thiswasmainlytobalancetheexploration-exploitationtrade-offforlearningnewbehavior,
andpertaintotheweightplacedonentropymaximizationinDrQ(temperatureandtargetentropy),
ortohandlesparserewards(numberofminQfunctions). Weuseamaximumepisodelengthof16
forthechairandsweepingtasks,and8forthedustpantask,sinceithassparsereward.
13Table3: Hyperparametersusedintheexperiments
Category Hyperparameter Value
Training Batchsize 256
UpdatetoSampleRatio 4
Actor/Critic Actorlearningrate 3e-4
Criticlearningrate 3e-4
Actornetworkarchitecture 2x256
Criticnetworkarchitecture 2x256
Criticensemblesize 10
ImageEncoder Layercount 4
Convolutionsize 3x3
Stride 2
Hiddenchannels 32
Outputlatentdim 50
Table4: Environment-tunedHyperparameters
Env #MinQ TempLR InitTemp TargetEntropy
Chair 2 1e-4 0.5 -2
Dustpan 1 1e-3 0.1 -2
Sweeping 2 1e-4 0.1 -4
C Rewards
C.1 Detection-Segmentation
For each task, there is an object of
interest,thestateofwhichisusedto
computethereward. Wespecifythe
object using a text prompt, which is
used by the detection model to ob-
tain a bounding box. This is then
used to condition the Segment Any-
thing [30] model to obtain a 2D ob-
ject mask, as shown in Fig.7. For
Figure 7: GroundedSAM/DeticVisualization: Visualization
text-based detection we use either
of the object masks obtained from Segment Anything for chair
moving(left)andsweeping(right). Grounding-Dino [53] or Detic [51].
ForGrounding-Dino,weappendthe
task-specificprompttothelistofclassnamesinCOCO[54](toavoidcasesoffalsepositivede-
tection), and we use Detic with objects365 vocabulary class names. The task-specific text
prompts we use are ’chair’ for the chair tasks, ’red broom’ for the dustpan standup task, and
’box.bag.poster.signboard.envelope.tag.clipboard.street sign’forthesweepingtask. Theobjectof
interestinthesweepingtaskisapaperbagbeingsweptandweusemanydifferentpossiblematching
textdescriptionssinceitisdetectedasdifferentclassesduetoitsdeformablenature. Welistthe
detectionmodelandtheconfidencethresholdforadetectiontobeacceptedforeachtaskinTable 5.
Once we obtain object masks, we can obtain the corresponding object point-cloud using depth
observations. Somedetectionsarerejectedbasedonestimatedposition,eg: ifthereisadetection
ofanobjectoutsidetheplaypen. Thisfilteringisessentialsincetherobotoftenpicksuponknown
infeasibleobjects,eg: theboxinthemiddleoftheplaypen,orsomechairsoutsidetherailings.
14Table5: DetectionSettings
Env DetectionModel ConfidenceThreshold
Chair Grounding-Dino 0.4
Dustpan Grounding-Dino 0.2
Sweeping Detic 0.1
C.2 RewardFunction
Chair-movingtasks: Forthistask,wecomputerewardateverytimestepoftheepisode. Giventhe
estimatedchairpointcloudusingthedetection-segmentationsystemalongwithdepthobservations,
weestimatethecenterofmassx andtheyawrotationw . Giventhegoalpositiongandorientation
t t
g (extractedfromthegoalimage),wecomputepositionx andyawdifferencew norms. Then
w diff diff
therewardisgivenby:
r =−x +e(−xdiff)+e(−10·xdiff)
position diff
r =e(−wdiff)+e(−10·wdiff)
ori
TotalReward=r +r
position ori
DustpanStandupInthistask,itisdifficulttoproviderewardwhentherobotisinteractingwiththe
dustpan,sincethedetectionmodelfailstopickuponthedustpanfromthethirdpersonoregocentric
imageobservations. Wecanmeasurerewardattheendoftheepisode(whentherobothasreleased
itsgrasp)todetectthedustpanandestimatethecenterofthehandlex ,andprovidealargebonus
T
iftheheightofthehandle(zcomponentofx )isaboveasetthreshold. Toprioritizefastertask
T
completion,weuseanalivepenaltyof-0.1. Therobotcanterminatetheepisodeearlierbyreleasing
itsgripperandlettinggoofthehandle.
r =−0.1
penalty
r =10ifx height≥thresh
bonus t
(cid:26)
r , iftimestept<T
TotalReward= penalty
r , ifendofepisode,timestepT
bonus
Sweeping: Similar to the chair task, we compute reward at every timestep of the episode. We
estimatethepointcloudofthepaperbag,letitscenterofmassbedenotedbyx . Thetargetregion
t
is a rectangle, denoted by G . Let d(x,G ) denote the distance from position x to the closest
r r
correspondingpointontherectanglegivenbyG . Thentherewardisgivenby:
r
r
distance
=−0.2·d(x t,G r)+e(−10·xdiff)
r =10·max(0,d(x ,G )−d(x ,G ))
progress t−1 r t r
(cid:26)
10, ifd(x ,G )=0
r = t r
bonus 0, else
TotalReward=r +r +r
distance progress bonus
C.3 SuccessCriteria
Theresultsweshowforcontinualimprovementduringtraining,aswellastheevaluationofthefinal
policiesreportsuccessrate. Successisdefinedforanepisodeinthefollowingmanner:
• Chairtasks: Maxrewardinepisodeisabove1,implyingthechairisveryclosetoitstarget.
15• DustpanStandup: Episodeendswitharewardof10(indicatingthedustpanisstandingup).
• Sweeping: Episodeendswitharewardof10(paperbagissweptintothegoalregion).
C.4 Priors
For the chair moving tasks we use RRT*
Algorithm2PriorgenerationforDustpanStandup
forplanningapathinSE(2)spacewitha
simplified model that only has 2D occu- 1: InitializePriordatabufferD
2: InitializeUniformnoisedistributionU withlimits
pancyofthetopsurfaceofthetable,and
:
isnotawareofthechair,orrobot-chairor
(−0.1,−0.1,−1)→(0.1,0.1,1)
chair-table interactions. This generates a
3: forN =1toNumberofepisodesdo
setofway-pointsforthetargetpositionof
4: InitializeactionlistA=[]
the center of mass of the robot in SE(2) 5: Setyawhandrotationωtoeither+0.5or-0.5
space,inglobalcoordinates. Weusecoor- 6: fort=1toepisodelendo
dinatetransformstoconvertthesetargetsto 7: Setverticalhandactionztobeeither+0.2or
beintherobot’sbodyframeinordertouse -0.2
the same action space as the reactive RL 8: Add(z,ω,0)+(n∼U)toA
policy. We are able to perform this com- 9: endfor
putation since we know the robot’s body 10: Add(−0.2,ω,0)+(n∼U)toA
11: ExecuteAontherobot,recordobservations,add
positioninglobalcoordinates. Specifically,
toD
we have W = W ∗ T−1, where
body global 12: endfor
W denotestheway-pointwithrespectto
f 13: return PriordatabufferD
framef andT isthematrixtransformof
therobotbodycenterofmasswithrespect
totheglobalcoordinates. Forsweeping,thepriorissimplytostaywithin0.5mofthelastdetected
location of the paper bag. For dustpan standup we use a simple procedural function to generate
trajectoriestocreateapriordataset,whichwedetailinAlgorithm2
D MapCalibration
WeusetheGraphNavfunctionalityprovidedintheSpotSDKbyBoston
Dynamics for Spot robots for generating a map of the playpen. This
involveswalkingtherobotaroundwithsomefiducials(weuse5)inthe
arena. This needs to be performed only once, and is used to obtain a
referenceframetolocalizetherobot,whichisusefultorecordbodypose
informationandalsotoimplementsafetycheckstomakesuretherobot
isnotexecutingactionsthatcollidewiththeplaypenrailings. WhileSpot
Figure8: Collisionmapof hasinbuiltcollisionavoidanceweimplementanadditionalsafetylayer
theplaypenusedforsafety
usingthemaptoclipunsafeactionsthatwouldmovetherobottooclose
and navigation. The table
to the playpen railings. For navigation we use RRT* to plan in SE(2)
is added to this map when
includedinexperiments. spacegiventheobstacles,usingthecollisionmapoftheplaypenasshown
inFig.8. Theredregiondenotestheestimateoftherobot’spositionin
thex-yplane,withthebluemarkingdenotingitsheading.
E SystemOverview
WeuseaworkstationwithasingleA5000GPUtorunRLPDonline,whichrequiresabout20GB
GPUmemory,mostlyowingtoalltheimageinputsthatneedtobeprocessed. Thedetectionand
segmentation models are run on cloud compute on a single A100 GPU. The fixed third person
cameraimagesfromtherealsensearestreamedtoalocallaptop. Communicationbetweenthelaptop,
workstationandcloudserverisfacilitatedviaGRPCservers,andthemainprogramscriptisrunon
theworkstation,whichalsocontrolstherobot. Commandsareissuedtotherobotoverwifiusingthe
SpotSDKprovidedbyBostonDynamics.
16