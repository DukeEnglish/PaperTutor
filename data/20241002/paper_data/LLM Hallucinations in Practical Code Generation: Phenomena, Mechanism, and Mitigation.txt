LLM Hallucinations in Practical Code Generation:
Phenomena, Mechanism, and Mitigation
Ziyao Zhang1, Yanlin Wang1∗, Chong Wang2, Jiachi Chen1, Zibin Zheng1
1Sun Yat-sen University, Zhuhai, China
2Nanyang Technological University, Singapore
Abstract—Codegenerationaimstoautomaticallygeneratecode developmentscenarios.Evaluationsbasedonthesebenchmarks
from input requirements, significantly enhancing development have revealed that LLMs face challenges in generating non-
efficiency.Recentlargelanguagemodels(LLMs)basedapproaches standalone functions with contextual dependencies, such as
have shown promising results and revolutionized code generation
callstouser-definedfunctionsandproject-defineddataprotocol.
task. Despite the promising performance, LLMs often generate
contents with hallucinations, especially for the code generation While these benchmarks provide valuable insights into the
scenario requiring the handling of complex contextual dependen- effectiveness of LLMs in practical code generation, they pri-
cies in practical development process. Although previous study marilyfocusonfunctionalcorrectnessasmeasuredbytestcase
hasanalyzedhallucinationsinLLM-poweredcodegeneration,the
pass rates and lack a thorough analysis of underlying failure
studyislimitedtostandalonefunctiongeneration.Inthispaper,we
causes. To bridge this gap, this work aims to systematically
conduct an empirical study to study the phenomena, mechanism,
and mitigation of LLM hallucinations within more practical investigateissuesinpracticalLLM-basedcodegenerationfrom
and complex development contexts in repository-level generation the perspective of hallucinations.
scenario. First, we manually examine the code generation results Hallucination is a significant issue for state-of-the-art gener-
fromsixmainstreamLLMstoestablishahallucinationtaxonomy ative LLMs [7]. For general natural language tasks, LLM
ofLLM-generatedcode.Next,weelaborateonthephenomenonof
hallucinations have been explored to a certain extent [5],
hallucinations, analyze their distribution across different models.
[7], [8], [69] and are typically categorized into three types:
We then analyze causes of hallucinations and identify four
potentialfactorscontributingtohallucinations.Finally,wepropose Input-Conflicting Hallucination, Fact-Conflicting Hallucination,
an RAG-based mitigation method, which demonstrates consistent and Context-Conflicting Hallucination [69]. In the domain of
effectiveness in all studied LLMs. The replication package code generation, Liu et al. [75] conducted a study to analyze
includingcode,data,andexperimentalresultsisavailableathttps:
hallucinationsinLLM-poweredcodegenerationandestablished
//github.com/DeepSoftwareAnalytics/LLMCodingHallucination.
a taxonomy that aligns with these three categories. While
I. INTRODUCTION Liu et al.’s study provided insightful findings, it is based on
benchmarks (i.e., HumanEval [15] and DS-1000 [76]) for stan-
Code generation is an automation technology aimed at effi-
dalone function/script generation instead. Our work, however,
ciently producing code from specifications described in natural
focuses on hallucinations within more practical and complex
language. This process significantly reduces the manual coding
development contexts in repository-level generation scenarios.
workload for developers [6], [9], [10], allowing them to focus
Additionally, their study primarily categorized hallucinations
moreonsolvingadvancedtechnicalchallengesandengagingin
fromaproblem-presentationperspectivetouncoverfine-grained
innovativetasks.Recentdevelopmentshaveintroducedavariety
code-semanticissues,resultingincategoriessuchasDeadCode
of large language models (LLMs) [14]–[16], [18]–[25], [37],
andRepetition.Inthiswork,weinvestigatehallucinationsfrom
[44], [51] built upon the Transformer architecture [1]. These
a holistic perspective in terms of phenomena, mechanism, and
models, trained on extensive code corpora, can automatically
mitigation. We believe that our study can complement the
generate code from natural language inputs and have shown
findings by Liu et al., providing a broader understanding of
high efficacy in code generation. For example, GPT-4 has
hallucinations in LLM-based code generation.
achieved state-of-the-art results on evaluation benchmarks
In this work, we conduct an empirical study to uncover the
such as HumanEval [15] and MBPP [17], demonstrating high
status quo and root causes of hallucinations in LLM-based
functional correctness, particularly in generating standalone
code generation within real-world projects. The study aims at
functions based on detailed specifications.
answering the following research questions (RQs):
However, in practical development scenarios, the require-
ments for code generation are more complex than sim- • RQ1 (Hallucination Taxonomy): What are the specific
manifestationsofhallucinationsinpracticalcodegeneration,
ply generating standalone functions from detailed specifica-
and how are they distributed?
tions [59]. To address this complexity, new benchmarks, such
as CoderEval [59], ClassEval [60], and EvoCodeBench [61], • RQ2 (LLM Comparison): How do different LLMs
compareintermsofhallucinationoccurrencesandpatterns?
have been proposed to better reflect real-world repository-level
• RQ3 (Root Causes): What are the root causes of halluci-
∗ Correspondingauthor. nations in practical LLM-based code generation?
4202
peS
03
]ES.sc[
1v05502.9042:viXraTo answer the questions, we experiment on six main- II. BACKGROUND&RELATEDWORK
stream LLMs (ChatGPT [55], CodeGen [16], PanGu-α [44],
A. LLM-based Code Generation
StarCoder2 [3], DeepSeekCoder [51], and CodeLlama [14])
with the CoderEval dataset [59]. To obtain the hallucina- Fordevelopers,arealisticscenarioistouseacoderepository
tion taxonomy of practical LLM-based code generation, we to write code, which is very common in practice [66]. For
manually perform open coding [65] on the LLM-generated example, due to security and functionality considerations,
code. Specifically, we first extract 10% of the coding tasks companies often only build code warehouses internally. The
from the CoderEval dataset in the initial stage. Then, from code repository provides many private APIs that are not seen
the initial annotation and discussion, we obtain preliminary by the language model and are not public on any code hosting
taxonomy. Finally, we obtain the fully hallucination taxonomy platform. Therefore, it is worth exploring whether pre-trained
with iterative labelling the remaining 90% coding tasks and language models can adapt to real development needs and
continuously refining the taxonomy in the process. After generate correct and efficient code. In real-world development
obtaining the taxonomy, we conduct extensive analysis based scenarios, the development of a function not only relies on
on the research questions aforementioned. the text description and function signature of the function,
but also requires calling a custom API in the code repository.
Findings. Our study reveals the following findings. ⃝1
Such non-independent functions are commonly found in real-
LLM hallucinations in code generation can be divided into
world generation scenarios. By analyzing the 100 most popular
three major categories (Task Requirement Conflicts, Factual
projects written in Java and Python on GitHub [59], previous
Knowledge Conflicts, and Project Context Conflicts) with
work found that dependent functions account for more than
eight subcategories: Functional Requirement Violation, Non-
70% of the functions in open source projects. In order to
Functional Requirement Violation, Background Knowledge
better simulate real development scenarios and to check
Conflicts, Library knowledge Conflicts, API Knowledge Con-
the correctness of LLMs, CoderEval [59], ClassEval [60],
flicts, Environment Conflicts, Dependency Conflicts, and Non-
and EvoCodeBench [61] collected code snippets and text
code Resource Conflicts. ⃝2 We analyze the hallucination
descriptions from real code repositories and used test cases
distribution in different LLMs and find that Task Requirement
to check the correctness of the code repositories in their
Conflicts are the most prevalent type of hallucination across
corresponding environments.However, the performance of the
all models. ⃝3 We identify four potential factors that cause
model on these benchmarks is extremely poor. LLMs cannot
hallucinations: training data quality, intention understanding
generate correct code based on the problem description, and
capacity, knowledge acquisition capacity, and repository-level
the model prefers to generate independent code segments
context awareness.
rather than using existing functions in the current development
Mitigation. Based on the findings, we explore a lightweight
scenario.
mitigation approach based on retrieval augmented generation
(RAG) and evaluate its effectiveness. In this approach, we
B. Hallucinations in LLMs
construct a retrieval library based on the repository in the
In the field of natural language processing (NLP), hallu-
development scenario of each generation task and obtain the
cination refers specifically to situations where the content
code snippet that is beneficial to the current generation task
produced by a language model in the process of generating
as a prompt through the similarity detection between the task
text is inconsistent with the given input or expected output
description in the generation task and the code snippet in the
environment, lacks meaning, or violates the facts [68]. This
retrievallibrary.Experimentalresultsshowthatthislightweight
kindofphenomenonisparticularlyprominentintextgeneration
mitigation can consistently improve the performance of all
models, especially in tasks such as text completion, summary
studied LLMs.
generation, and machine translation. The output of the model
In summary, this paper makes the following contributions:
must maintain a high degree of consistency and authenticity to
• We conduct an empirical study to analyze the types ensure its practicality and reliability. Hallucination phenomena
hallucinations in LLM code generation in real development can be divided into the following categories according to their
scenarios and establish a hallucination taxonomy in LLM- nature [69]: (1) Input-Conflicting Hallucinations: When the
based code generation. text generated by the model deviates from the original input
• Weelaborateonthephenomenonofhallucinations,analyze source,input-conflictinghallucinationswilloccur.Thisillusion
the distribution of hallucinations on different models. may result from the model’s incorrect parsing or inaccurate
• We further analyze causes of hallucinations and identify internal representation of the input information, causing the
four possible factors. output content to deviate from the intent and context of the
• WeproposeaRAG-basedmitigationapproachbasedonthe source input. (2) Context-Conflicting Hallucinations: This type
causes of hallucinations and experiment on various LLMs of hallucination occurs when the text generated by the model
to study its effectiveness. is contradictory or inconsistent with its previously generated
• We make the replication package available at https://github. content. Contextual conflict hallucinations reflect the model’s
com/DeepSoftwareAnalytics/LLMCodingHallucination, to challenges in maintaining textual coherence and consistency,
support further studies in this field. which may be due to the model’s insufficient processing ofcontextualinformationorlimitationsofitsmemorymechanism. versions. To better accomplish the generation task, we use
(3)Fact-ConflictingHallucinations:Whenthecontentgenerated the CodeGen-350M-Mono model.
by LLM is inconsistent with established knowledge or facts • PanGu-α[44]:PanGu-αcanperformcodegenerationtasks
in the real world, fact-conflicting hallucinations will occur. in multiple languages. We use the PanGu-α-2.6B model.
This illusion reveals the model’s inadequacy in understanding • DeepSeekCoder [51]: DeepSeekCoder performs well in
and applying knowledge about the external world, and may be opensourcemodelsacrossmultipleprogramminglanguages
causedbylimitationsinmodeltrainingdata,lagsinknowledge and various benchmarks. We use the DeepSeekCoder-6.7B
updates, or limitations in the model’s reasoning capabilities. base model.
However, there is a lack of research on hallucination • CodeLlama [14]: CodeLlama is a set of pre-trained and
phenomenainthefieldofcodegeneration.Althoughtherehave fine-tuned generative text models ranging in size from 7 to
been a large number of LLM-based methods to optimize code 34billionparameters.WeusetheCodeLlama-7b-Python-hf
generation tasks, these works do not have a clear definition of model.
the code generation illusion. The presence of hallucination • StarCoder2 [18]: StarCoder2 is a family of open code-
problems can be detrimental to the overall quality of the oriented models for large languages, providing three scales
generated code. This may not only affect the performance and of models, we use the StarCoder2-7B model.
maintainability of the code, but may also lead to unexpected
For each task, we use the LLMs to generate 10 code
errors and security vulnerabilities, thus posing a threat to the
snippetsbyemployingthenuclearsamplingstrategyandsetting
stabilityandsecurityofthesoftware.Inordertomakeupforthe
temperature to 0.6, following the same setting as CoderEval.
gaps in the definition of hallucination problems, there has been
worktodefinehallucinationsforLLMsincodegenerationtasks.
This work [8] defined new hallucination standards for LLMs C. Taxonomy Annotation
in code generation tasks and divided hallucinations into five
In order to analyze the hallucination types in the LLM-
maintypes,butthisworkignoresthatLLMsinreal-worldcode
generated code, we manually perform open coding [65] on the
generation tasks will involve relevant knowledge unique to the
generated code to obtain the hallucination taxonomy.
software engineering field such as development environment,
(1) Initial Open Coding. Firstly, in the initial open-coding
system resources, external constraints, code warehouses, etc.
stage, we select 10% of the 230 coding tasks in CoderEval
These factors often cause LLMs to fail in actual development.
Pythondatasetforpreliminaryanalysis.Werandomlycollect23
Problems such as low usability and low accuracy. In order
generative tasks from CoderEval, we employ CodeGen, Pangu-
to better explore the illusions that exist in LLMs in real
α, ChatGPT, DeepSeekCoder, CodeLlama, and StarCoder2,
development scenarios, our work obtained data sets in real
with each model generating ten code snippets for each code
development scenarios for empirical study, and defined new
generation task, culminating in a total of 1,380 code snippets
types of illusions, which opened up new ideas for subsequent
tobeanalysedforhallucinationtaxonomyframework.Foreach
research on illusions.
code snippet, we test it in the actual development environment
III. EVALUATIONSETUP corresponding to the task to determine its correctness. On this
basis,thetwoauthorswillcomparethedifferencesbetweenthe
A. Dataset
ground-truth and LLMs generated code snippets and discuss
To better simulate practical development scenarios, we use a and record possible hallucination phenomena.
set of coding tasks from real-world Python repositories based
(2) Preliminary Taxonomy Construction. Secondly, we
on the CoderEval benchmark [59]. CoderEval comprises 230
document possible hallucinations in the generated code and
Python code generation tasks, extracted from a diverse set of
the location of the hallucination content. Several different
Python repositories. Each task consists of a natural language
hallucinations may occur within a single code snippet. All
description,aground-truthcodesnippet,andasetoftestcases,
annotators are required to discuss the codes and define the
along with the project environment context associated with the
code’s hallucinatory taxonomy. In this process, we classify
task.
similar hallucination to create a preliminary taxonomy that
illustrates the various hallucination types and their meanings
B. Studied LLMs
in the code generated by LLMs.
We utilize several mainstream LLMs to perform code (3) Full Taxonomy Construction. Finally, after obtaining
generationforthestudiedprogrammingtasks.TheLLMsbeing the categorisation criteria, the remaining code snippets will be
used cover both open-source and closed-source models and independentlyannotatedbythreenewlyinvitedvolunteerswith
span various parameter sizes, listed as follows. extensivePythonprogrammingexperience,twowithmorethan
• ChatGPT [55]: ChatGPT is a versatile text generation tenyearsofexperienceandonewithfouryearsofprogramming
model for multilingualism with powerful code generation experience. If new types of hallucinations arise that are not
capabilities, we use the GPT-3.5-Turbo in our experiments. covered by the current taxonomy, annotators are required
• CodeGen [53]: CodeGen is a family of auto-regressive lan- to write descriptions of the hallucinations to allow further
guage models for program synthesis with several different discussion to establish new types and enhance the taxonomy.FunctionalRequirementViolation Example:WrongFunctionality,MissingFunctionality
TaskRequirementConflicts(§IV-A1)
Non-functionalRequirementViolation Example:Security,Performance,Style,CodeSmell
BackgroundKnowledgeConflicts
FactualKnowledgeConflicts(§IV-A2) LibraryKnowledgeConflicts
Example:Parameter,GuardConditions
APIKnowledgeConflicts
Similar-but-wrongAPIs,ExceptionHandling
EnvironmentConflicts
ProjectContextConflicts(§IV-A3) DependencyConflicts Example:UndefinedMethods,APIVersionConflict
Non-codeResourceConflicts Example:Data,Config,Assert,Connection
Fig.1. TaxonomyofHallucinationsinLLM-basedCodeGeneration
IV. EVALUATIONRESULTS
In this section, we present the evaluation results and answer
the three aforementioned research questions.
A. RQ1: Hallucination Taxonomy
TheoverallLLMcodinghallucinationtaxonomyweobtained
from Section III-C is presented in Figure 1. Through manual
annotation, we identify three primary hallucination categories:
Task Requirement Conflicts, Factual Knowledge Conflicts, and
Project Context Conflicts, which can be further divided into
eightspecifictypes.Notethatourthreeprimarycategoriesalign
well with the hallucination types in the general domain [68].
Task requirement conflicts correspond to input-conflicting
hallucinations in the general domain, indicating that the
generated code does not meet the functional or non-functional
requirements of the coding tasks. Factual knowledge conflicts Fig.2. HallucinationDistribution
correspond to knowledge-conflicting hallucinations in the gen-
eraldomain,indicatingthatthegeneratedcodedoesnotcomply
Docstring
with background knowledge, library/framework knowledge, or # similarity_filter.py
APIknowledge.Projectcontextconflictscorrespondtocontext- Hydratorfor`Time`and`LocalTime`values.
:paramnanoseconds:
conflictinghallucinationsinthegeneraldomain,indicatingthat :paramtz:
:return:Time
the generated code incorrectly uses project contexts, including Ground-truth
environments, dependencies, and resources. In the following, defhydrate_time(nanoseconds,tz=None):
frompytzimportFixedOffset
we present the detailed hallucination types in our taxonomy. seconds,nanoseconds=map(int,divmod(nanoseconds,1000000000))
minutes,seconds=map(int,divmod(seconds,60))
Figure 2 shows the distribution of the hallucination types. hours,minutes=map(int,divmod(minutes,60))
t=Time(hours,minutes,seconds,nanoseconds)
1) Task Requirement Conflicts (43.53%): In the general iftzisNone:
domain, input-conflicting hallucinations occur when the an- returnt Handle `LocalTime`
tz_offset_minutes,tz_offset_seconds=divmod(tz,60) in functional
swers generated by LLMs deviate from the original intentions zone=FixedOffset(tz_offset_minutes) requirement
returnzone.localize(t)
of user inputs [7]. In the context of code generation tasks, LLM Generation
the primary intentions of inputs typically revolve around
defhydrate_time(nanoseconds,tz=None): Overlook
the functional and non-functional requirements of the coding from.timeimportTime `LocalTime`
returnTime.from_nanoseconds(nanoseconds,tz)
tasks. When the code generated by LLMs does not align with
these requirements, hallucinations related to Task Requirement Fig.3. Example:FunctionalRequirementViolation
Conflicts occur. Specifically, these conflicts can be categorized
into two types: Functional Requirement Violation and Non-
functional Requirement Violation. executionbehaviors)orruntimeerrors(suchastheKeyError
Functional Requirement Violation (36.66%). Functional re- during dictionary access). More specifically, the functional
quirements are typically expressed in natural language and requirementmismatchcanbesubdividedintotwotypicaltypes:
describe the desired functionality of the generated code. When Wrong Functionality and Missing Functionality. For example,
these requirements are not correctly and comprehensively as illustrated in Figure 3, the functional requirement involves
understood, the resulting code may fail to meet expected handling LocalTime based on the specific timezone tz. In
functionality, leading to logic bugs (such as unexpected the ground-truth code, this requirement is addressed by the
noitanicullaH
noitareneG
edoCDocstring Docstring
validatesthatRegistryYAMLcontenthasallrequiredfields validatesthatRegistryYAMLcontenthasallrequiredfields
:paramfile_content:contentoftheRegistryYAMLfile :paramfile_content:contentoftheRegistryYAMLfile
:raiseIRValidatorException:whenmandatorydataismissinginRegistry :raiseIRValidatorException:whenmandatorydataismissinginRegistry
:return:DictionarywithdataloadedfromaRegistryYAMLfile :return:DictionarywithdataloadedfromaRegistryYAMLfile
Ground-truth Ground-truth
defvalidate_from_content(cls,file_content=None): definitialize(self):
iffile_contentisNone: (parent,root_dir)=fs.path.split(self.root)
raiseIRValidatorException( parent_fs=open_fs(parent)
"RegistryYAMLcontentismissing") ifparent_fs.exists(root_dir):
registry_dict=yaml.safe_load(file_content) Avoiding safety selfr .a ri os oe t_S ft sore =Ex pc ae rp et ni to _n f( s." mO aC kF eL dis rt (o rr oa og te _dr io ro )t%salreadyexists,aborting!"%(self.root))
ifnotisinstance(registry_dict,dict): hazards ifself.N da im sa ps ot se i( td i= o0 n,c io snt ne on tt= Ns oe nl ef :.declaration_tvalue).write(pyfs=self.root_fs)
raiseIRValidatorException( withself.root_fs.open(self.layout_file,'w')asfh:
"Registryfileisemptyorcorrupted:{}".format(file_content)) layout={'extension':self.disposition, Create an OCFL
try: 'description’:"..."} storage root
jsonschema.validate(registry_dict, json.dump(layout,fh,sort_keys=True,indent=2)
cls.SCHEMA_REGISTRY) LLM Generation
exceptjsonschema.exceptions.ValidationErroraserror:
raiseI "R {V }al ii nda ft io lr eE :x \c ne {p }t "i .o fn o( rmat(error.message,file_content)) d e f i s sn e ei l lt f fi . .a r rl o oi o oz t te . o( = p s ee F nl i (f l )e) (: self.path, “ro”, "ocfl_storage.json") M OCis Fu Ln d ae nr ds tood
self.root.create_child("ocfl_storage", "ocfl_storage") generatedwrong
returnregistry_dict self.root.create_child("ocfl_storage ", "ocfl_storage.json")
LLM Generation ...//omitted file structure
self.root.create_child("ocfl_storage", "ocfl_storage.tar.bz2")
defvalidate_from_content(cls,file_content=None):
if file _contentisNone:
raiseIRValidatorException('filecontentismissing’) Fig.5. Example:BackgroundKnowledgeConflicts
file_data=yaml.load(spec_content) Leading to system
validate_data(cls, file_data) security risks
return file_data
2) Factual Knowledge Conflicts (31.91%): In the field of
Fig.4. Example:Non-functionalRequirementViolation NLP, the term “factual conflicts” refers to content generated
by LLMs that does not align with established knowledge or
facts about the real world. Practical software development
lines highlighted in the green rectangle. However, the code similarlyreliesonvarioustypesandlevelsoffactualknowledge
generated by PanGu-α overlooks this requirement, resulting in to produce correct code. Consequently, when LLMs fail to
a hallucination of Functional Requirement Violation. accuratelyunderstandandapplybackgroundknowledge[67],li-
brary/framework knowledge, or API knowledge, hallucinations
Non-functional Requirement Violation (6.86%). Besides
on Factual Knowledge Conflicts arise. We further divide this
functional requirements, developers often have non-functional
hallucinationcategoryintothreetypes:BackgroundKnowledge
requirements for the generated code, such as security
Conflicts, Library Knowledge Conflicts, and API Knowledge
concerns or performance considerations. These non-functional
Conflicts.
requirements are usually more implicit than functional
Background Knowledge Conflicts (8.82%). Background
requirements and are not described in the input natural
Knowledge Conflicts are a common issue when using large
language descriptions. Our open coding annotation reveals that
language models. These conflicts refer to the situation that
non-functional requirements in coding tasks can be mainly
the generated code is inconsistent with existing domain-
divided into the following aspects: Security, Performance,
specific knowledge, potentially rendering the code invalid or
Style, and Code Smell. Generated code that violates these
introducing logic bugs and risks. For instance, in automotive
non-functional requirements may introduce safety risks or
software development, if the generated code fails to adhere to
increase the maintenance complexity of the corresponding
certain industry standards (e.g., AUTOSAR1), it can result in
project.
significant compliance issues or safety risks.
Specifically, on the security side, the generated code may
Background knowledge typically includes Domain Concepts
introduce vulnerabilities such as unsanitized inputs, which can
(e.g., specific data formats or protocols) and related Standards
lead to insecure deserialization or SQL injection attacks. As
and Specifications (e.g., standard parameters or configurations).
showninFigure4,theground-truthcodeusesthesafe_load
For example, Figure 5 shows an example about OCFL (Oxford
function to safely read YAML files. In contrast, the LLM-
Common File Layout), a specification for data storage and
generated code utilizes the load function, thereby introducing
transformation.Accordingtotheofficialdescription2,anOCFL
a potential security risk. Regarding performance, the generated
storage root must contain a “Root Conformance Declaration”
code may lack optimization for execution efficiency, for
following the “NAMASTE” specification and may include a
example, by using inefficient loop structures that lead to
file named ocfl_layout.json to describe the root layout
unnecessary overhead in computing and memory resources.
arrangement. While the ground-truth code aligns with these
Style violations often occur when the generated code fails to
specifications when initializing the OCFL storage root, the
follow established programming conventions or style guides,
generated code might incorrectly focus on other OCFL aspects
such as inconsistent naming conventions or inappropriate
that are irrelevant to creating the storage root.
code layout, which can negatively affect code readability and
Library Knowledge Conflicts (2.68%). In modern software
maintainability. Code smell violations include issues such as
development, developers frequently employ frameworks or
overly complex functions or excessive use of global variables,
which increase the complexity and potential risks associated 1https://en.wikipedia.org/wiki/AUTOSAR
with future maintenance. 2https://ocfl.io/1.1/spec/#storage-rootDocstring Docstring
Decorator to wrap a function with a memoizing callable that saves up to `maxsize` Given a frequency string with a number and a unit of time, return a corresponding
results based on a Least Recently Used (LRU) algorithm with a per-item time-to-live datetime.timedelta instance or None if the frequency is None or “always”.For instance,
(TTL) value. given "3 weeks", return datetime.timedelta(weeks=3)
:parammaxsize ttl timer typed Raise ValueError if the given frequency cannot be parsed.
:type int int object boolean Ground-truth
:return Decorator
Ground-truth def parse_frequency(frequency):
if not frequency:
defttl_cache(maxsize=128,ttl=600,timer=time.monotonic,typed=False): if no… t … t/ i/ mo em _i ut nt ie td .endswith('s'):
ifmaxsizeisNone: time_unit += 's'
return_cache(_UnboundTTLCache(ttl,timer),typed) Correctly handle if time_unit == 'months':
elifcallable(maxsize): the decorator number *= 30
return_cache(TTLCache(128,ttl,timer),typed)(maxsize) time_unit = 'days’ Correct use of parameter 'day'
else: elif time_unit == 'years': in datetime.timedelta()
return_cache(TTLCache(maxsize,ttl,timer),typed) number *= 365
time_unit = 'days'
LLM Generation try:
return datetime.timedelta(**{time_unit: number})
defttl_cache(maxsize=128,ttl=600,timer=time.monotonic,typed=False):
timer=timeror(lambda:perf_counter()) except TypeError:
raise ValueError(f"Could not parse consistency check frequency '{frequency}'")
is_async=asyncio.iscoroutinefunction Incorrectlyadopt the LLM Generation
is_async_gen=asyncio.iscoroutine asyncio framework
get_loop=asyncio.get_event_loop def parse_frequency(frequency):
ifis_async(func): if frequency == "always":
return None Incorrect use of a non-existent
return_cache(TTLCache(maxsize,ttl,timer),typed)(maxsize) elif frequency == "year": parameter 'year' in
return datetime.timedelta(year=1) datetime.timedelta()
Fig.6. Example:LibraryKnowledgeConflicts ... //omitted
else:
raise ValueError("Unknown frequency: '%s'", frequency)
third-party libraries (e.g., Django3 for web applications) to Fig.7. Example:APIKnowledgeConflicts
expedite the development process by reusing the features
or functionalities that these frameworks or libraries provide.
We present an example in Figure 7. In this generation
When utilizing these frameworks or libraries, LLMs may
task, CodeGen correctly identifies the task intent and utilizes
encounter factual errors that lead to unexpected behaviors
the datetime.timedelta() function. However, the code
or even security risks. For example, As depicted in Figure 6,
snippet generated by CodeGen uses a non-existing parameter
the task requires the model to generate a decorator that caches
year.
the return value of the function upon each invocation. In
3) Project Context Conflicts (24.56%): Project Context
the code generated by the DeepSeekCoder model, the APIs
Conflicthallucinationreferstothephenomenonwherethecode
from the asyncio framework are utilized. This framework is
generatedbyLLMsisinconsistentwiththespecificcontextofa
designed for asynchronous processing, and the model’s misuse
givenproject.Inasense,thistypeofhallucinationisalsoatype
of the asynchronous processing framework poses unexpected
offactualconflict,wherefactswithinthecurrentprojectcontext
behaviors to the developed application.
are violated. The key difference is that Factual Knowledge
API Knowledge Conflicts (20.41%). API Knowledge Con-
Conflicts involve common facts (e.g., libraries and APIs) that
flicts are a common hallucination in LLM-generated code
are publicly accessible, while Project Context Conflicts pertain
caused by various types of API misuses, such as pa-
to facts that are specific to the corresponding project, which
rameter errors, improper guard conditions, similar-but-
are generally unavailable for public access. Project Context
incorrect/deprecated API usage, and improper exception han-
Conflicts are often caused by LLMs not aware of such project-
dling. For example, parameter errors can occur when inappro-
specific facts when generating code. This hallucination can be
priateparametertypesorvaluesareusedinthegeneratedcode,
divided into Environment Conflicts, Dependency Conflicts, and
causing API calls to fail or return unexpected results. This
Non-code Resource Conflicts.
case is especially common in dynamically typed programming
Environment Conflicts (0.94%). In the process of software
language such as Python [29]. Improper guard conditions
development, conflicts between the generated code and the
mean that the generated code does not correctly implement
development environment are common, especially regarding
pre-condition checks. If the validity of the pre-conditions of
version differences in platforms, operating systems, drivers,
certain APIs is not verified before calling them (e.g., file
languages, compilers/interpreters, frameworks, and libraries.
existence), runtime errors may occur. In terms of similar-but-
When generating code, such environmental concerns are often
wrong/deprecated API usage, LLMs may mistakenly choose
not considered, leading to problematic code if there are
APIs with similar functions but different applicable scenar-
environment-sensitiveoperations.Forexample,ifthegenerated
ios. Although this choice is syntactically correct, it cannot
code uses language features (e.g., f-string expressions) from
meet actual application needs. Improper exception handling
higher Python versions that are not supported by the current
involves generating code that fails to properly handle potential
development environment, a conflict arises. For example,
exceptions, which can cause the program to crash or behave
Figure 8 shows a code snippet generated by CodeGen that
abnormally when faced with an error condition. This kind of
attempts to use the package _lfu_cache, which does not
API knowledge conflict will not only directly lead to program
exist in the current environment.
functionalerrors,butmayalsoaffectthestabilityofthesystem
Dependency Conflicts (11.26%). Dependency Conflicts
and the usability of the code.
arise when the generated code relies on undefined or
3https://www.djangoproject.com/ unimported dependencies, such as user-defined attributesDocstring Docstring
Given an url and a destination path, retrieve and extract.tar.gz archive which contains
Decorator to wrap a function with a memoizing callable that saves up to `maxsize` results based on a Least 'desc' file for each package
Frequently Used (LFU) algorithm. Args:
url: url of the.tar.gz archive to download
Ground-truth destination_path: the path on disk where to extract archive
Returns:
def lfu_cache(maxsize=128, typed=False): path where the archive is extracted to
if maxsize is None: Ground-truth
return _cache(_UnboundCache(), typed)
elif callable(maxsize): def get_repo_archive(self, url: str, destination_path: Path) -> Path:
return _cache(LFUCache(128), typed)(maxsize) Use the current res = self.http_request(url)
else: development destination_path.parent.mkdir(parents=True, exist_ok=True) Successfully fetched the
return _cache(LFUCache(maxsize), typed) environment’s classes d ee xs tt ri an ca tt _i to on _ =p a Pt ah t. hw (r si tt re (_ db ey st te is n( ar te is o. nc _o pn at te hn )t .) split(".tar.gz")[0])file with the correct path
and filename
tar = tarfile.open(destination_path)
LLM Generation tar.extractall(path=extract_to)
def lfu_cache(maxsize=128, typed=False): Using a non-existent t ra er t. uc rl no s ee x( t) ract_to
import _lfu_cache package in the current LLM Generation
return _lfu_cache.lfu_cache(maxsize, typed) environment
def get_repo_archive(self, url: str, destination_path: Path) -> Path:
archive_name = url.split('/')[-1]
archive_path = os.path.join(os.getcwd(), "archive", archive_name) Fail to fetch the
if not os.path.exists(archive_path): file due to the use
Fig.8. Example:EnvironmentConflicts os.makedirs(archive_path) of a non-existent
tar = tarfile.open(archive_path, "w:gz")
file path
for i in range(len(url.split('/'))-1):
tar.add(url.split('/')[i+1], arcname=url.split('/')[i+1])
tar.close()
Docstring return archive_path
Generate the default observer schema for each Kubernetes resource present
in``spec.manifest`` for which a custom observer schema hasn't been specified. Fig.10. Example:Non-codeResourceConflicts
:param app (krake.data.kubernetes.Application): The application for which to generate
a default observer schema
:return
Ground-truth
defgenerate_default_observer_schema(app): code incorrectly parses a data file or attempts to access a
observer_schema={}
resources=app.spec.manifest Proper use of the non-existent data field, it can lead to runtime errors or data
forresourceinresources: generate_default_
ifresourcenotinapp.observer_schemas: schema function inconsistencies. Config conflicts arise from incorrect settings
observer_schema[resource]=generate_default_schema()
returnobserver_schema or options in configuration files. This might include using
LLM Generation undefined configuration fields or options, which can prevent
defgenerate_default_observer_schema(app):
app.status.mangled_observer_schema=deepcopy(app.observer_schema) the generated code from properly applying the configuration
observer_schema={}
forresource_manifestinapp.spec.manifest: and affect system behavior. Asset conflicts here involve
try:
ifresource_manifestnotinapp.observer_schemas: improper handling of asset files and their properties. For
observer_schema[resource]=generate_default_observer_schema_dict()
exceptIndexError: instance, if the generated code fails to set the correct size
app.status.mangled_observer_schema.append( Error using non-existing
generate_default_observer_schema_dict( generate_default_observer_ and resolution for images or videos, it can result in display
resource_manifest,
first_level=True)) schema_dict function. issues or severe bugs, such as application crashes. Connection
returnobserver_schema
conflicts relate to wrong settings of various connection
Fig.9. Example:DependencyConflicts resources, such as incorrect IP addresses, port numbers, or
database tables. These issues often lead to failed connections
oroperationsbeingperformedonthewrongserverordatabase,
and functions. This often results in errors such as undefined
potentially causing data leaks or security incidents.
variables or no-member errors. In practical software
For example, Figure 10 illustrates the generation task hopes
development, 70% of functions are non-standalone and depend
that LLM can generate a function for a given URL and
on entities defined elsewhere in the project or imported from
target path to retrieve and extract the tar.gz compressed
third-party libraries [59]. Due to the inability of LLMs to
package containing each package’s “description” file. However,
access the entire project context, they often resort to using
in the code snippet generated by the model, the model adds
non-existent APIs, functions, attributes, and variables when
“archive” as a path in the target path, which causes the code
dealing with non-standalone functions.
snippet to point to a non-existent file path. This will not allow
For example, Figure 9 illustrates a sce-
the tar.gz compressed package to be correctly obtained,
nario involving a user-defined function
resulting in a program error.
generate_default_observer_schema_dict(). In
this case, the PanGu-α erroneously uses a function with a sim- RQ1 Summary: We have established a hallucination
ilar but incorrect name, generate_default_schema(), taxonomy in LLM-based code generation, comprising three
whichdoesnotexistintheproject.ThisleadstoaDependency main categories (i.e., Task Requirement Conflicts, Factual
Conflict, as the code fails to execute correctly due to the Knowledge Conflicts, and Project Context Conflicts) with
missing definition. eight subtypes. Among these, Task Requirement Conflicts
Non-code Resource Conflicts (12.36%). Non-code are the most frequently occurring category.
Resource Conflicts can be further categorized into four
B. RQ2: LLM Comparison
main types: Data, Configs, Assets, and Connections. Each
type of conflict can undermine the correctness and reliability Based on the obtained hallucination taxonomy for LLM-
of the system. Data conflicts often involve mishandling of based code generation, we further analyze the hallucination
data formats, fields, or content. For example, if the generated distribution comparison across different models. Figure 111600 CodeGen such corpora, they may unintentionally incorporate these flaws
1400 PanGu-α into their knowledge base, leading to hallucinations in code
ChatGPT generation.AsshowninFigure4,LLMsmaygeneratecodethat
1200 DeepSeekCoder uses unsafe APIs, reflecting problematic patterns commonly
1000 CodeLlama found in the training data. This indicates that the model may
StarCoder2
800 have been affected by low-quality data during the training
600 phase. Most hallucinations associated with Task Requirement
Conflicts and Factual Knowledge Conflicts can be, to a certain
400
extent, attributed to data quality issues in the training corpora.
200
This highlights the importance of building a high-quality code-
0
relatedtrainingdatatoreducehallucinationsincodegeneration.
Task Requirement Conflicts Factual Knowledge Conflicts Project Context Conflicts
Fig.11. Hallucinationdistributionofdifferentmodels 2) Intention Understanding Capacity: Although LLMs
have shown great potential in code generation, they still face
challengesinaccuratelycapturingandinterpretingspecificuser
shows the distribution of the number of hallucinations of intentionsandneeds[49].Thislimitationcanresultingenerated
different models based on the breakdown analysis of the three codethatisfunctionallyornon-functionallyinaccurate,thereby
hallucination types. We find that Task Requirement Conflicts affecting the overall effectiveness and trustworthiness of LLM-
are the most common hallucination type for all models, while based code generation [48]. The core advantage of LLMs
Factual Knowledge Conflicts and Project Context Conflicts lies in their excellent pattern recognition capabilities, but
remain at approximately the same frequency. Additionally, we this is also the source of their limitations. LLMs tend to
find that CodeGen and StarCoder2 exhibit a notably higher fre- generate code based on common patterns observed in the
quencyofhallucinationsrelatedtoTaskRequirementConflicts, training data rather than from a deep understanding of the
whereas DeepSeekCoder and CodeLlama demonstrates the specific requirements context. As shown in Figure 3, the
lowestoccurrence.Thisvariationmayberelatedtothemodels’ task description requires the LLM to handle LocalTime,
ability to understand task requirements, potentially influenced which is ignored in the LLM-generated code. This example
by factors such as the model size or the training corpora. highlights LLMs’ inadequacy in comprehensively interpreting
For instance, DeepSeekCoder and CodeLlama are trained on the intentions behind requirements. Furthermore, LLMs also
diverse corpora including both extensive code and text data, show limitations in handling subtle requirements involving
while CodeGen and StarCoder2 are primarily trained on code- complex logic or multi-step operations [47]. Due to a poor
related data. In terms of factual knowledge conflicts, PanGu-α understanding of the overall scope and potential limitations of
demonstrates the highest frequency of factual hallucinations. the task, LLM-generated code may only address part of the
This can be attributed to its extensive training on Chinese requirements or perform poorly in handling edge cases. This
corpora,whichmayhaveledtoarelativelylimitedexposureto can result in generated code snippets that seem correct on the
factualknowledge,suchasspecificdomainconcepts,expressed surface but fail to meet specific business logic or functional
in English. requirements in practice.
3) Knowledge Acquisition Capacity: LLMs may learn
RQ2 Summary: Task Requirement Conflicts are the most
incorrect knowledge and miss certain domain-specific knowl-
prevalent type of hallucination across all models, with
edge due to the aforementioned training data quality issues.
CodeGenandStarCoder2showinganotablyhigherfrequency
Moreover, as software development techniques evolve, such
of this type compared to others.
as library updates, relevant knowledge developed after model
training period cannot be acquired by LLMs. Unlike human
C. RQ3: Root Cause Analysis
developers who can continuously learn and integrate latest
In this research question, we conduct further analysis on the information during development, LLMs are limited to the
possible root causes of the hallucinations in practical LLM- knowledge available at the time of training. For example, as
based code generation. shown in Figure 5, the task description needs a piece of code
1) Training Data Quality: The quality of the training for generating a data format that satisfies the OCFL storage
data is a crucial factor in the development of LLMs, as it specification, but LLM generates incorrect code, possibly due
significantly affects models’ inference capabilities. Recent to its lack of the OCFL-related knowledge during inference.
LLMs are often trained on large-scale code corpora typically This limitation in LLMs’ knowledge acquisition capacity
collected from open-source repositories. However, the quality leads to hallucinations related to incorrect or outdated factual
of these repositories is not always assured, leading to the informationinthegeneratedcode.Thishighlightstheneedfora
inclusion of low-quality data in the training corpora. Such knowledgeacquisitionmechanism,suchasretrievalaugmented
issues include mismatches between docstrings and code [77], generation (RAG), to allow LLMs to update, correct, and
inefficient or insecure code implementations [46], misused supplement the knowledge they have learned.
API calls, outdated library documentation and usage [50], 4) Repository-level Context Awareness: Feeding all project
and a lack of domain diversity. When LLMs are trained on contexts, including code, documents, and non-code resources,intoanLLMforrepository-levelcodegenerationischallenging TABLEI
and impractical. This is because LLMs, typically based on the EXPERIMENTALRESULTSOFMITIGATIONMETHODUNDERPASS@1.
Transformer architecture [1], have token number limits (e.g. 8k
Model Raw Method RAG-based Mitigation
or 12k tokens) and experience quadratic computation growth
as the number of tokens increases. Additionally, including all CodeGen 1.30% 2.61% (↑ 1.31%)
PanGu-α 0.04% 1.74% (↑ 1.70%)
projectcontextscanintroduceasignificantamountofirrelevant
DeepSeekCoder 3.04% 3.91% (↑ 0.87%)
information, hindering LLMs’ ability to focus on the most
CodeLlama 2.17% 5.22% (↑ 3.05%)
relevant context for code generation. Therefore, it is crucial StarCoder2 0.04% 2.61% (↑ 2.57%)
to develop methods that make LLMs aware of the project ChatGPT 10.40% 12.61% (↑ 2.21%)
contexts (project-specific memory) that are precisely related to
thecurrentcodingtask.Recentworksattempttointegratestatic
analysis tools [78] or apply retrieval-augmented generation We employ a sparse bag-of-words (BOW) model for our
(RAG) based on repository-level retrieval corpora [62] to retrieval mechanism, which simplifies gauging similarity be-
address such context awareness issues. tween textual data. This model transmutes both the query and
the candidate code snippets into sets of tokens, which are
RQ3 Summary: By further analyzing the causes of
comparedusingtheJaccardindex.TheJaccardindexmeasures
hallucinations, we identify four possible contributing fac-
the similarity between two sets by dividing the size of their
tors: training data quality, intention understanding capacity,
intersection by the size of their union, we choose the code
knowledge acquisition capacity, and repository-level context
snippet that retrieves the top ten scores each time to return as
awareness. Deficiencies in any of these factors can lead to
the prompt for the LLMs.
hallucinations in practical development scenarios.
V. MITIGATIONAPPROACH C. Evaluation
A. Motivation We evaluate the effectiveness of the RAG-based mitigation
The aforementioned root causes of hallucinations in code method with the six LLMs: CodeGen, PanGu-α, ChatGPT,
generated by LLMs can be traced back to three main factors at DeepSeekCoder, CodeLlama, and StarCoder2 on the CodeEval
the inference stage: incorrect or insufficient understanding for dataset. We compared our RAG-based mitigation method with
task requirements, the lack of factual knowledge pertinent to the Raw method. In the Raw method, we only provide LLMs
the generation tasks, and the inability to access the necessary basic docstrings and function signatures. In the RAG-based
code and non-code resources from the repository. These mitigation, when providing docstrings and function signatures,
limitations create substantial challenges for LLMs in code we will obtain ten related code snippets from the above-
generation in practical development settings. There are many constructed retrieval library through a similarity algorithm
previousworksinvestingLLM-basedcodegeneration[26]–[28], as prompts and provide them to LLMs. We use the Pass@1
[28], [30], [31], [33]–[35], we draw inspiration from existing metric to assess the functionality correctness of the generated
work [62] on repository-level code generation and explore the code snippets according to test cases. As shown in Table I, the
feasibility of applying retrieval-augmented generation (RAG) Pass@1scoresofallsixmodelsareconsistentlyimprovedwith
to mitigate hallucinations.The idea is that by providing LLMs the RAG-based mitigation method. Note that the performance
with code snippets relevant to the current task, they can better improvement in our experiments is modest, as the mitigation
understand the requirements and gain awareness of specific methodweexploredispreliminary.Weconsiderthisexperiment
factual knowledge and project contexts. as an pilot study to explore the potential effectiveness of RAG-
basedmitigation.Infuturework,therearemoremethodsworth
B. RAG-based Mitigation studying,suchasmodelfine-tuningandmulti-agentframework
To implement the RAG method, we first collect all code with tool using, etc.
repositories from the CoderEval dataset and follow Re- To further illustrate the effectiveness of the hallucination
poCoder’s method [62] to construct the retrieval corpora. mitigation,weconducttwocasestudies.AsshowninFigure12,
Specifically, for each repository, we apply a sliding window to in the Raw method, which only provides a docstring and a
scan all the source files in it. This scanning process extracts function signature, CodeGen incorrectly uses the replace
consecutive lines of code based on a predefined window size. function and fails to convert scripts to one-line commands.
The sliding window moves by a fixed number of lines (slicing In contrast, with the RAG-based method, CodeGen correctly
step) at each iteration to ensure complete coverage of the code. uses the splitlines function, aligning with the ground-
We adhere to RepoCoder’s parameter settings, with a window truth and successfully addressing the requirement. In addition,
sizeof20linesandaslidingstepof2lines.Topreventanswer the RAG-based method can also effectively mitigate Project
leakage, code lines containing or following the ground-truth Context Conflicts. As shown in Figure 13, in the Raw method,
code are excluded from the scanning process. Once all files ChatGPT attempts to use the self.items.popitem()
are processed, a retrieval corpus of code snippets is generated API, which does not exist in the repository, leading to
for the repository. hallucinated generation. In contrast, with the RAG-basedDocstring work, researchers may consider developing more effective
convertsascripttoonelinecommand.THisisusefultorunasinglesshcommandandp techniques to quickly and precisely identify and localize
assaonelinescript.
:paramscript: hallucinations in LLM-generated code.
:return:
Ground-truth Developing more effective hallucination mitigation tech-
niques: In Section V, we explore the feasibility of applying a
defoneline(script,separator=’&&’):
returnseperator.join(textwrap.dedent(script).strip().splitlines()) lightweight RAG-based method to mitigate hallucinations in
Raw Method LLM-based code generation. While the method demonstrates
defoneline(script,separator=’&&'): Task Requirement effectiveness in mitigating hallucinations such as undefined
returnscript.replace(’\t',seperator) Conflicts attributes, the potentials of RAG need to be further explored.
RAG-based Mitigation For example, we only construct retrieval corpus using current
defoneline(script,separator=’&&’): Correctly convert scripts to code repository, leading to the augmented information is
returnseperator.join(script.splitlines()) one-line commands
insufficienttomitigatemanyhallucinationssuchasbackground
Fig.12. Example:hallucinationmitigationinTaskRequirementConflicts knowledge conflicts. In the future, we can integrate more
comprehensive knowledge sources like online search engines,
Docstring
API documents, and StackOverflow discussions. In addition
Removeandreturnthe`(key,value)`pairfirstinserted. to RAG techniques, other methods such as input query
Ground-truth refinement [4], [49] and multi-agent systems [73] can also
defpopitem(self): be leveraged to achieve an iterative process of (i) clarifying
try:
key=next(iter(self.__order)) task requirements, (ii) generating code, (iii) running test cases,
exceptStopIteration:
raiseKeyError('%sisempty'%type(self).__name__)fromNone and (iv) mitigating hallucinations. To achieve this, we need to
else:
return(key,self.pop(key)) designtheappropriateinteractionprotocolsbetweenagentsand
Raw Method relevant tools (e.g., search engines and static analysis tools)
defpopitem(self): AttributeError: 'self ' and apply suitable prompting strategies.
object has no
returnself.items.popitem( )
attribute 'items'
RAG-based Mitigation VII. THREATSTOVALIDITY
defpopitem(self): External Validity. Threats to external validity mainly con-
try:
key=next(iter(self.__order)) cernthegeneralizabilityofourfindings.WefocusedonPython
exceptStopIteration:
raiseKeyError('%sisempty'%type(self).__name__)fromNone whenexploringthetaxonomyandrootcausesofhallucinations
else: Proper use of the
return(key,self.pop(key)) repository's internal API in LLM-based code generation due to its simplicity and
ease of use. Constructing hallucination taxonomies for other
Fig.13. Example:hallucinationmitigationinProjectContextConflicts
programming languages and comparing them with our current
taxonomy is a valuable future direction. Another potential
mitigation, ChatGPT correctly implements the requirement threat is the limited scale of the adopted CoderEval dataset,
using the self.pop() function. which contains only 230 coding tasks. To mitigate this, we
selected six LLMs and had each generate 10 code snippets for
VI. DISCUSSION
each task to ensure a sufficient number of annotations.
We provide implications for future research on the halluci- Internal Validity. Threats to internal validity primarily
nations in practical LLM-based code generation. concern the manual annotation process in taxonomy con-
Developing hallucination identification techniques: struction. A key issue is the absence of formal inter-rater
Throughourstudy,wefind3majorcategoriesofhallucinations reliability measure for annotating hallucinations. To address
intheLLM-basedcodegeneration.Somehallucinationscanbe this, discrepancies were discussed and resolved in annotator
detected by using static analysis (e.g., undefined variables) or meetings to ensure a consistent annotation protocol, with each
dynamictestexecution(runtimeerrorsortestfailures),making identified hallucination receiving a mutually agreed-upon label.
it relatively easy for developers to recognize and locate the Additionally, to ensure consistency in our findings, one author
relevant code issues. However, certain hallucinations, such as reviewed all labeled data. Another potential threat is model
incomplete functionality and security issues, are very difficult bias during the annotation process. To mitigate this, we mixed
for developers to detect and correct, as they can likely pass the generation results of the six models before annotation.
static checks and all test cases. As a result, LLM-generated Construct Validity. Threats to construct validity are related
code containing these hallucinations may be introduced into toevaluatingourhallucinationmitigationapproach.Toalleviate
development projects and even real production environments, these threats, we conducted experiments on six models using
leadingtounreliablesoftwaresystemsandseveresecurityrisks. testcasesavailableintheCoderEvaldataset,astandardmethod
Existing hallucination localization approaches [71], [72] based for evaluating the correctness of generated code.
on LLM self-feedback methods can detect hallucinations to a
certain extent. However, these approaches heavily rely on the
VIII. CONCLUSION
currentmodel’scapabilitiesandcannotaddressthefundamental In this paper, we conduct an empirical study on code-
limitationsimposedbythetrainingcorpora.Therefore,infuture generated hallucinations of large models in the practicaldevelopment scenarios and through a full manual analysis, [14] B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan,
we construct a taxonomy of hallucinations and follow up with Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov,
J. Bitton, M. Bhatt, C. Canton-Ferrer, A. Grattafiori, W. Xiong,
furtherhallucinationclassifications.Basedonthehallucinations
A. Défossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier,
found, we provide a deeper discussion of the causes of halluci- T. Scialom, and G. Synnaeve, “Code llama: Open foundation models
nationsandthedistributionofhallucinationsindifferentLLMs. for code,” CoRR, vol. abs/2308.12950, 2023. [Online]. Available:
https://doi.org/10.48550/arXiv.2308.12950
At last, we implement a RAG-based approach for hallucination
[15] M.Chen,J.Tworek,H.Jun,Q.Yuan,H.P.deOliveiraPinto,J.Kaplan,
mitigationandfurtherdiscusspotentialhallucinationmitigation H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri,
approaches. G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan,
S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian,
C.Winter,P.Tillet,F.P.Such,D.Cummings,M.Plappert,F.Chantzis,
REFERENCES E.Barnes,A.Herbert-Voss,W.H.Guss,A.Nichol,A.Paino,N.Tezak,
J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,
[1] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford,
L.Kaiser,andI.Polosukhin,“Attentionisallyouneed,”inAdvances M.Knight,M.Brundage,M.Murati,K.Mayer,P.Welinder,B.McGrew,
inNeuralInformation Processing Systems30: AnnualConferenceon D.Amodei,S.McCandlish,I.Sutskever,andW.Zaremba,“Evaluating
Neural Information Processing Systems 2017, December 4-9, 2017, large language models trained on code,” CoRR, vol. abs/2107.03374,
Long Beach, CA, USA, I. Guyon, U. von Luxburg, S. Bengio, H. M. 2021.[Online].Available:https://arxiv.org/abs/2107.03374
Wallach,R.Fergus,S.V.N.Vishwanathan,andR.Garnett,Eds.,2017, [16] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou,
pp.5998–6008.[Online].Available:https://proceedings.neurips.cc/paper/ S. Savarese, and C. Xiong, “Codegen: An open large language
2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html model for code with multi-turn program synthesis,” in The Eleventh
[2] A.M.Dakhel,V.Majdinasab,A.Nikanjam,F.Khomh,M.C.Desmarais, International Conference on Learning Representations, ICLR 2023,
andZ.M.J.Jiang,“Githubcopilotaipairprogrammer:Assetorliability?” Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [Online].
JournalofSystemsandSoftware,vol.203,p.111734,2023. Available:https://openreview.net/pdf?id=iaYcJKpY2B_
[3] A. Lozhkov, R. Li, L. B. Allal, F. Cassano, J. Lamy-Poirier, N. Tazi, [17] J.Austin,A.Odena,M.I.Nye,M.Bosma,H.Michalewski,D.Dohan,
A.Tang,D.Pykhtar,J.Liu,Y.Weietal.,“Starcoder2andthestack E. Jiang, C. J. Cai, M. Terry, Q. V. Le, and C. Sutton, “Program
v2:Thenextgeneration,”arXivpreprintarXiv:2402.19173,2024. synthesis with large language models,” CoRR, vol. abs/2108.07732,
[4] K. D. Dhole, R. Chandradevan, and E. Agichtein, “An interactive 2021.[Online].Available:https://arxiv.org/abs/2108.07732
query generation assistant using llm-based prompt modification and [18] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou,
userfeedback,”arXivpreprintarXiv:2311.11226,2023. M. Marone, C. Akiki, J. Li, J. Chim, Q. Liu, E. Zheltonozhskii,
[5] H. Ye, T. Liu, A. Zhang, W. Hua, and W. Jia, “Cognitive mirage: T. Y. Zhuo, T. Wang, O. Dehaene, M. Davaadorj, J. Lamy-Poirier,
A review of hallucinations in large language models,” arXiv preprint J.Monteiro,O.Shliazhko,N.Gontier,N.Meade,A.Zebaze,M.Yee,
arXiv:2309.06794,2023. L.K.Umapathi,J.Zhu,B.Lipkin,M.Oblokulov,Z.Wang,R.M.V,
[6] Z.Zhang,C.Chen,B.Liu,C.Liao,Z.Gong,H.Yu,J.Li,andR.Wang, J.Stillerman,S.S.Patel,D.Abulkhanov,M.Zocca,M.Dey,Z.Zhang,
“Unifyingtheperspectivesofnlpandsoftwareengineering:Asurveyon N.Moustafa-Fahmy,U.Bhattacharyya,W.Yu,S.Singh,S.Luccioni,
languagemodelsforcode,”arXivpreprintarXiv:2311.07989,2023. P. Villegas, M. Kunakov, F. Zhdanov, M. Romero, T. Lee, N. Timor,
[7] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, J. Ding, C. Schlesinger, H. Schoelkopf, J. Ebert, T. Dao, M. Mishra,
W. Peng, X. Feng, B. Qin et al., “A survey on hallucination in large A. Gu, J. Robinson, C. J. Anderson, B. Dolan-Gavitt, D. Contractor,
languagemodels:Principles,taxonomy,challenges,andopenquestions,” S.Reddy,D.Fried,D.Bahdanau,Y.Jernite,C.M.Ferrandis,S.Hughes,
arXivpreprintarXiv:2311.05232,2023. T. Wolf, A. Guha, L. von Werra, and H. de Vries, “Starcoder: may
[8] S.Tonmoy,S.Zaman,V.Jain,A.Rani,V.Rawte,A.Chadha,andA.Das, the source be with you!” CoRR, vol. abs/2305.06161, 2023. [Online].
“Acomprehensivesurveyofhallucinationmitigationtechniquesinlarge Available:https://doi.org/10.48550/arXiv.2305.06161
languagemodels,”arXivpreprintarXiv:2401.01313,2024. [19] S. Black, L. Gao, P. Wang, C. Leahy, and S. Biderman, “GPT-Neo:
[9] S.Barke,M.B.James,andN.Polikarpova,“Groundedcopilot:How LargeScaleAutoregressiveLanguageModelingwithMesh-Tensorflow,”
programmersinteractwithcode-generatingmodels,”Proceedingsofthe Mar.2021,Ifyouusethissoftware,pleaseciteitusingthesemetadata.
ACMonProgrammingLanguages,vol.7,no.OOPSLA1,pp.85–111, [Online].Available:https://doi.org/10.5281/zenodo.5297715
2023. [20] B. Wang and A. Komatsuzaki, “GPT-J-6B: A 6 Billion Parame-
[10] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, ter Autoregressive Language Model,” https://github.com/kingoflolz/
H.Edwards,Y.Burda,N.Joseph,G.Brockmanetal.,“Evaluatinglarge mesh-transformer-jax,May2021.
language models trained on code,” arXiv preprint arXiv:2107.03374, [21] Y. Wang, W. Wang, S. R. Joty, and S. C. H. Hoi, “Codet5:
2021. Identifier-aware unified pre-trained encoder-decoder models for code
[11] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, understandingandgeneration,”inProceedingsofthe2021Conference
L.Zettlemoyer,N.Cancedda,andT.Scialom,“Toolformer:Language onEmpiricalMethodsinNaturalLanguageProcessing,EMNLP2021,
modelscanteachthemselvestousetools,”CoRR,vol.abs/2302.04761, VirtualEvent/PuntaCana,DominicanRepublic,7-11November,2021,
2023.[Online].Available:https://doi.org/10.48550/arXiv.2302.04761 M.Moens,X.Huang,L.Specia,andS.W.Yih,Eds. Associationfor
[12] B.Paranjape,S.M.Lundberg,S.Singh,H.Hajishirzi,L.Zettlemoyer, ComputationalLinguistics,2021,pp.8696–8708.[Online].Available:
andM.T.Ribeiro,“ART:automaticmulti-stepreasoningandtool-use https://doi.org/10.18653/v1/2021.emnlp-main.685
forlargelanguagemodels,”CoRR,vol.abs/2303.09014,2023.[Online]. [22] Y. Wang, H. Le, A. Gotmare, N. D. Q. Bui, J. Li, and S. C. H. Hoi,
Available:https://doi.org/10.48550/arXiv.2303.09014 “Codet5+: Open code large language models for code understanding
[13] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, andgeneration,”inProceedingsofthe2023ConferenceonEmpirical
N.Bashlykov,S.Batra,P.Bhargava,S.Bhosale,D.Bikel,L.Blecher, Methods in Natural Language Processing, EMNLP 2023, Singapore,
C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, December 6-10, 2023, H. Bouamor, J. Pino, and K. Bali, Eds.
J.Fu,W.Fu,B.Fuller,C.Gao,V.Goswami,N.Goyal,A.Hartshorn, Association for Computational Linguistics, 2023, pp. 1069–1088.
S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, [Online].Available:https://aclanthology.org/2023.emnlp-main.68
I.Kloumann,A.Korenev,P.S.Koura,M.Lachaux,T.Lavril,J.Lee, [23] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi,
D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, R. Zhong, S. Yih, L. Zettlemoyer, and M. Lewis, “Incoder: A
I.Molybog,Y.Nie,A.Poulton,J.Reizenstein,R.Rungta,K.Saladi, generative model for code infilling and synthesis,” in The Eleventh
A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, International Conference on Learning Representations, ICLR 2023,
B.Tang,R.Taylor,A.Williams,J.X.Kuan,P.Xu,Z.Yan,I.Zarov, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [Online].
Y.Zhang,A.Fan,M.Kambadur,S.Narang,A.Rodriguez,R.Stojnic, Available:https://openreview.net/pdf?id=hQwb-lbM6EL
S.Edunov,andT.Scialom,“Llama2:Openfoundationandfine-tuned [24] Y.Li,D.H.Choi,J.Chung,N.Kushman,J.Schrittwieser,R.Leblond,
chat models,” CoRR, vol. abs/2307.09288, 2023. [Online]. Available: T. Eccles, J. Keeling, F. Gimeno, A. D. Lago, T. Hubert, P. Choy,
https://doi.org/10.48550/arXiv.2307.09288 C.deMassond’Autume,I.Babuschkin,X.Chen,P.Huang,J.Welbl,S.Gowal,A.Cherepanov,J.Molloy,D.J.Mankowitz,E.S.Robson, ComputationalLinguistics,2020,pp.1536–1547.[Online].Available:
P.Kohli,N.deFreitas,K.Kavukcuoglu,andO.Vinyals,“Competition- https://doi.org/10.18653/v1/2020.findings-emnlp.139
level code generation with alphacode,” CoRR, vol. abs/2203.07814, [40] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan,
2022.[Online].Available:https://doi.org/10.48550/arXiv.2203.07814 A. Svyatkovskiy, S. Fu, M. Tufano, S. K. Deng, C. B. Clement,
[25] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, D.Drain,N.Sundaresan,J.Yin,D.Jiang,andM.Zhou,“Graphcodebert:
L. Golding, H. He, C. Leahy, K. McDonell, J. Phang, M. Pieler, Pre-trainingcoderepresentationswithdataflow,”in9thInternational
U. S. Prashanth, S. Purohit, L. Reynolds, J. Tow, B. Wang, Conference on Learning Representations, ICLR 2021, Virtual Event,
and S. Weinbach, “Gpt-neox-20b: An open-source autoregressive Austria, May 3-7, 2021. OpenReview.net, 2021. [Online]. Available:
languagemodel,”CoRR,vol.abs/2204.06745,2022.[Online].Available: https://openreview.net/forum?id=jLoC4ez43PZ
https://doi.org/10.48550/arXiv.2204.06745 [41] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. B.
[26] L. Guo, Y. Wang, E. Shi, W. Zhong, H. Zhang, J. Chen, R. Zhang, Clement, D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou,
Y.Ma,andZ.Zheng,“Whentostop?towardsefficientcodegeneration L.Zhou,M.Tufano,M.Gong,M.Zhou,N.Duan,N.Sundaresan,S.K.
inllmswithexcesstokenprevention,”inProceedingsofthe33rdACM Deng,S.Fu,andS.Liu,“Codexglue:Amachinelearningbenchmark
SIGSOFTInternationalSymposiumonSoftwareTestingandAnalysis, dataset for code understanding and generation,” in Proceedings of
2024,pp.1073–1085. the Neural Information Processing Systems Track on Datasets and
[27] Y.Wang,T.Jiang,M.Liu,J.Chen,andZ.Zheng,“Beyondfunctional Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December
correctness:Investigatingcodingstyleinconsistenciesinlargelanguage 2021, virtual, J. Vanschoren and S. Yeung, Eds., 2021. [Online].
models,”arXivpreprintarXiv:2407.00456,2024. Available: https://datasets-benchmarks-proceedings.neurips.cc/paper/
[28] Z.Zheng,K.Ning,J.Chen,Y.Wang,W.Chen,L.Guo,andW.Wang, 2021/hash/c16a5320fa475530d9583c34fd356ef5-Abstract-round1.html
“Towards an understanding of large language models in software [42] D.Guo,S.Lu,N.Duan,Y.Wang,M.Zhou,andJ.Yin,“Unixcoder:
engineeringtasks,”arXivpreprintarXiv:2308.11396,2023. Unifiedcross-modalpre-trainingforcoderepresentation,”inProceedings
of the 60th Annual Meeting of the Association for Computational
[29] C.Wang,J.Zhang,Y.Lou,M.Liu,W.Sun,Y.Liu,andX.Peng,“Tiger:
Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland,
Agenerating-then-rankingframeworkforpracticalpythontypeinference,”
May 22-27, 2022, S. Muresan, P. Nakov, and A. Villavicencio, Eds.
arXivpreprintarXiv:2407.02095,2024.
Association for Computational Linguistics, 2022, pp. 7212–7225.
[30] Y.Wang,Y.Wang,D.Guo,J.Chen,R.Zhang,Y.Ma,andZ.Zheng,
[Online].Available:https://doi.org/10.18653/v1/2022.acl-long.499
“Rlcoder:Reinforcementlearningforrepository-levelcodecompletion,”
[43] K. Zhang, G. Li, J. Li, Z. Li, and Z. Jin, “Toolcoder: Teach code
arXivpreprintarXiv:2407.19487,2024.
generation models to use apis with search tools,” arXiv preprint
[31] Y.Li,E.Shi,D.Zheng,K.Duan,J.Chen,andY.Wang,“Repomincoder:
arXiv:2305.04032,2023.
Improvingrepository-levelcodegenerationbasedoninformationloss
[44] W. Zeng, X. Ren, T. Su, H. Wang, Y. Liao, Z. Wang, X. Jiang,
screening,” in Proceedings of the 15th Asia-Pacific Symposium on
Z.Yang,K.Wang,X.Zhangetal.,“Pangu-α:Large-scaleautoregressive
Internetware,2024,pp.229–238.
pretrained chinese language models with auto-parallel computation,”
[32] C.Wang,Y.Lou,X.Peng,J.Liu,andB.Zou,“Miningresource-operation
arXivpreprintarXiv:2104.12369,2021.
knowledgetosupportresourceleakdetection,”inProceedingsofthe31st
[45] K.WashioandY.Miyao,“Codegenerationforunknownlibrariesvia
ACMJointEuropeanSoftwareEngineeringConferenceandSymposium
readingapidocumentations,”arXivpreprintarXiv:2202.07806,2022.
ontheFoundationsofSoftwareEngineering,2023,pp.986–998.
[46] J.H.Klemmer,S.A.Horstmann,N.Patnaik,C.Ludden,C.BurtonJr,
[33] Z.Zheng,K.Ning,Y.Wang,J.Zhang,D.Zheng,M.Ye,andJ.Chen, C.Powers,F.Massacci,A.Rahman,D.Votipka,H.R.Lipfordetal.,
“Asurveyoflargelanguagemodelsforcode:Evolution,benchmarking, “Using ai assistants in software development: A qualitative study on
andfuturetrends,”arXivpreprintarXiv:2311.10372,2023. securitypracticesandconcerns,”arXivpreprintarXiv:2405.06371,2024.
[34] Y.WangandH.Li,“Codecompletionbymodelingflattenedabstract [47] D. Zan, B. Chen, Z. Lin, B. Guan, Y. Wang, and J.-G. Lou, “When
syntax trees as graphs,” in Proceedings of the AAAI conference on languagemodelmeetsprivatelibrary,”arXivpreprintarXiv:2210.17236,
artificialintelligence,vol.35,no.16,2021,pp.14015–14023. 2022.
[35] L. Nie, J. Sun, Y. Wang, L. Du, S. Han, D. Zhang, L. Hou, J. Li, [48] C. Spiess, D. Gros, K. S. Pai, M. Pradel, M. R. I. Rabin, S. Jha,
andJ.Zhai,“Unveilingtheblackboxofplmswithsemanticanchors: P.Devanbu,andT.Ahmed,“Qualityandtrustinllm-generatedcode,”
towards interpretable neural semantic parsing,” in Proceedings of the arXivpreprintarXiv:2402.02047,2024.
AAAI Conference on Artificial Intelligence, vol. 37, no. 11, 2023, pp. [49] F. Mu, L. Shi, S. Wang, Z. Yu, B. Zhang, C. Wang, S. Liu, and
13400–13408. Q. Wang, “Clarifygpt: Empowering llm-based code generation with
[36] Y.Wang,L.Guo,E.Shi,W.Chen,J.Chen,W.Zhong,M.Wang,H.Li, intentionclarification,”arXivpreprintarXiv:2310.10996,2023.
H.Zhang,Z.Lyuetal.,“Youaugmentme:Exploringchatgpt-baseddata [50] L.ZhongandZ.Wang,“Canllmreplacestackoverflow?astudyon
augmentation for semantic code search,” in 2023 IEEE International robustnessandreliabilityoflargelanguagemodelcodegeneration,”in
ConferenceonSoftwareMaintenanceandEvolution(ICSME). IEEE, ProceedingsoftheAAAIConferenceonArtificialIntelligence,vol.38,
2023,pp.14–25. no.19,2024,pp.21841–21849.
[37] L. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M. Ferrandis, [51] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen,
N. Muennighoff, M. Mishra, A. Gu, M. Dey, L. K. Umapathi, X.Bi,Y.Wu,Y.Lietal.,“Deepseek-coder:Whenthelargelanguage
C. J. Anderson, Y. Zi, J. Lamy-Poirier, H. Schoelkopf, S. Troshin, modelmeetsprogramming–theriseofcodeintelligence,”arXivpreprint
D. Abulkhanov, M. Romero, M. Lappert, F. D. Toni, B. G. del Río, arXiv:2401.14196,2024.
Q. Liu, S. Bose, U. Bhattacharyya, T. Y. Zhuo, I. Yu, P. Villegas, [52] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou,
M. Zocca, S. Mangrulkar, D. Lansky, H. Nguyen, D. Contractor, M.Marone,C.Akiki,J.Li,J.Chimetal.,“Starcoder:maythesource
L. Villa, J. Li, D. Bahdanau, Y. Jernite, S. Hughes, D. Fried, bewithyou!”arXivpreprintarXiv:2305.06161,2023.
A. Guha, H. de Vries, and L. von Werra, “Santacoder: don’t reach [53] E.Nijkamp,B.Pang,H.Hayashi,L.Tu,H.Wang,Y.Zhou,S.Savarese,
for the stars!” CoRR, vol. abs/2301.03988, 2023. [Online]. Available: andC.Xiong,“Codegen:Anopenlargelanguagemodelforcodewith
https://doi.org/10.48550/arXiv.2301.03988 multi-turnprogramsynthesis,”arXivpreprintarXiv:2203.13474,2022.
[38] F.Christopoulou,G.Lampouras,M.Gritta,G.Zhang,Y.Guo,Z.Li, [54] B.Roziere,J.Gehring,F.Gloeckle,S.Sootla,I.Gat,X.E.Tan,Y.Adi,
Q.Zhang,M.Xiao,B.Shen,L.Li,H.Yu,L.Yan,P.Zhou,X.Wang, J.Liu,T.Remez,J.Rapinetal.,“Codellama:Openfoundationmodels
Y.Ma,I.Iacobacci,Y.Wang,G.Liang,J.Wei,X.Jiang,Q.Wang,and forcode,”arXivpreprintarXiv:2308.12950,2023.
Q.Liu,“Pangu-coder:Programsynthesiswithfunction-levellanguage [55] OpenAI, “Chatgpt: Optimizing language models for dialogue,” https:
modeling,” CoRR, vol. abs/2207.11280, 2022. [Online]. Available: //openai.com/blog/chatgpt,2022.
https://doi.org/10.48550/arXiv.2207.11280 [56] Z. Tu, Z. Su, and P. Devanbu, “On the localness of software,” in
[39] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, Proceedingsofthe22ndACMSIGSOFTInternationalSymposiumon
B. Qin, T. Liu, D. Jiang, and M. Zhou, “Codebert: A pre-trained FoundationsofSoftwareEngineering,2014,pp.269–280.
model for programming and natural languages,” in Findings of the [57] W.Zou,J.Xuan,X.Xie,Z.Chen,andB.Xu,“Howdoescodestyle
Association for Computational Linguistics: EMNLP 2020, Online inconsistencyaffectpullrequestintegration?anexploratorystudyon117
Event, 16-20 November 2020, ser. Findings of ACL, T. Cohn, githubprojects,”EmpiricalSoftwareEngineering,vol.24,pp.3871–3903,
Y. He, and Y. Liu, Eds., vol. EMNLP 2020. Association for 2019.[58] S. Bajracharya, J. Ossher, and C. Lopes, “Sourcerer: An internet- [78] C.Wang,J.Zhang,Y.Feng,T.Li,W.Sun,Y.Liu,andX.Peng,“Teaching
scalesoftwarerepository,”in2009ICSEWorkshoponSearch-Driven codellmstouseautocompletiontoolsinrepository-levelcodegeneration,”
Development-Users,Infrastructure,ToolsandEvaluation. IEEE,2009, arXivpreprintarXiv:2401.06391,2024.
pp.1–4. [79] R.Bommasani,D.A.Hudson,E.Adeli,R.Altman,S.Arora,S.von
[59] H.Yu,B.Shen,D.Ran,J.Zhang,Q.Zhang,Y.Ma,G.Liang,Y.Li, Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill et al., “On
Q. Wang, and T. Xie, “Codereval: A benchmark of pragmatic code the opportunities and risks of foundation models,” arXiv preprint
generationwithgenerativepre-trainedmodels,”inProceedingsofthe arXiv:2108.07258,2021.
46thIEEE/ACMInternationalConferenceonSoftwareEngineering,2024,
pp.1–12.
[60] X.Du,M.Liu,K.Wang,H.Wang,J.Liu,Y.Chen,J.Feng,C.Sha,
X. Peng, and Y. Lou, “Classeval: A manually-crafted benchmark
for evaluating llms on class-level code generation,” arXiv preprint
arXiv:2308.01861,2023.
[61] J.Li,G.Li,X.Zhang,Y.Dong,andZ.Jin,“Evocodebench:Anevolving
codegenerationbenchmarkalignedwithreal-worldcoderepositories,”
arXivpreprintarXiv:2404.00599,2024.
[62] F.Zhang,B.Chen,Y.Zhang,J.Keung,J.Liu,D.Zan,Y.Mao,J.-G.Lou,
andW.Chen,“Repocoder:Repository-levelcodecompletionthrough
iterativeretrievalandgeneration,”arXivpreprintarXiv:2303.12570,2023.
[63] M.Liu,T.Yang,Y.Lou,X.Du,Y.Wang,andX.Peng,“Codegen4libs:
Atwo-stageapproachforlibrary-orientedcodegeneration,”in202338th
IEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering
(ASE). IEEE,2023,pp.434–445.
[64] D.Liao,S.Pan,Q.Huang,X.Ren,Z.Xing,H.Jin,andQ.Li,“Context-
awarecodegenerationframeworkforcoderepositories:Local,global,
and third-party library awareness,” arXiv preprint arXiv:2312.05772,
2023.
[65] S.H.Khandkar,“Opencoding,”UniversityofCalgary,vol.23,no.2009,
2009.
[66] M.Grechanik,C.McMillan,L.DeFerrari,M.Comi,S.Crespi,D.Poshy-
vanyk,C.Fu,Q.Xie,andC.Ghezzi,“Anempiricalinvestigationintoa
large-scalejavaopensourcecoderepository,”inProceedingsofthe2010
ACM-IEEEInternationalSymposiumonEmpiricalSoftwareEngineering
andMeasurement,2010,pp.1–10.
[67] C. Wang, X. Peng, Z. Xing, and X. Meng, “Beyond literal meaning:
Uncoverandexplainimplicitknowledgeincodethroughwikipedia-based
conceptlinking,”IEEETransactionsonSoftwareEngineering,vol.49,
no.5,pp.3226–3240,2023.
[68] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang,
A.Madotto,andP.Fung,“Surveyofhallucinationinnaturallanguage
generation,”ACMComputingSurveys,vol.55,no.12,pp.1–38,2023.
[69] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,
Y. Zhang, Y. Chen et al., “Siren’s song in the ai ocean: a survey on
hallucinationinlargelanguagemodels,”arXivpreprintarXiv:2309.01219,
2023.
[70] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,
H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel et al., “Retrieval-
augmented generation for knowledge-intensive nlp tasks,” Advances
inNeuralInformationProcessingSystems,vol.33,pp.9459–9474,2020.
[71] A. Agrawal, M. Suzgun, L. Mackey, and A. T. Kalai, “Do language
models know when they’re hallucinating references?” arXiv preprint
arXiv:2305.18248,2023.
[72] P.Manakul,A.Liusie,andM.J.Gales,“Selfcheckgpt:Zero-resource
black-boxhallucinationdetectionforgenerativelargelanguagemodels,”
arXivpreprintarXiv:2303.08896,2023.
[73] T.Guo,X.Chen,Y.Wang,R.Chang,S.Pei,N.V.Chawla,O.Wiest,
andX.Zhang,“Largelanguagemodelbasedmulti-agents:Asurveyof
progressandchallenges,”arXivpreprintarXiv:2402.01680,2024.
[74] F.Shi,X.Chen,K.Misra,N.Scales,D.Dohan,E.H.Chi,N.Schärli,and
D.Zhou,“Largelanguagemodelscanbeeasilydistractedbyirrelevant
context,”inInternationalConferenceonMachineLearning. PMLR,
2023,pp.31210–31227.
[75] F. Liu, Y. Liu, L. Shi, H. Huang, R. Wang, Z. Yang, and L. Zhang,
“Exploringandevaluatinghallucinationsinllm-poweredcodegeneration,”
arXivpreprintarXiv:2404.00971,2024.
[76] Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-t.
Yih, D. Fried, S. Wang, and T. Yu, “Ds-1000: A natural and reliable
benchmarkfordatasciencecodegeneration,”inInternationalConference
onMachineLearning. PMLR,2023,pp.18319–18345.
[77] Z.Sun,L.Li,Y.Liu,X.Du,andL.Li,“Ontheimportanceofbuilding
high-qualitytrainingdatasetsforneuralcodesearch,”inProceedingsof
the44thInternationalConferenceonSoftwareEngineering,2022,pp.
1609–1620.