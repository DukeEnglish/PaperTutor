DressRecon: Freeform 4D Human Reconstruction from Monocular Video
JeffTan,DonglaiXiang,ShubhamTulsiani,DevaRamanan,GengshanYang*
CarnegieMellonUniversity,USA
Image-based Priors
NormalFeature Mask View 1
DressRecon
View 2
Body Clothing
Gaussians Gaussians
+
Hierarchical Extreme-View
Input: Monocular Video Output: Time-Consistent Body Model
Motion Field Rendering
Figure1. Givenasingleinputvideoofahuman,DressReconreconstructsatime-consistent4Dbodymodel,includingshape,appearance,
time-varyingbodyarticulations,aswellasextremelylooseclothingdeformationoraccessoryobjects. Weproposeahierarchicalbag-of-
bonesdeformationmodelthatallowsbodyandclothingmotiontobeseparated.Weleverageimage-basedpriorssuchashumanbodypose,
surfacenormals,andopticalflowtomakeoptimizationmoretractable. Theresultingneuralfieldscanbeextractedintotime-consistent
meshes,orfurtheroptimizedasexplicit3DGaussiansforhigh-fidelityinteractiverendering.
Abstract and optical flow during optimization. The resulting neu-
ral fields can be extracted into time-consistent meshes, or
We present a method to reconstruct time-consistent hu- furtheroptimizedasexplicit3DGaussiansforhigh-fidelity
man body models from monocular videos, focusing on interactive rendering. On datasets with highly challeng-
extremely loose clothing or handheld object interactions. ing clothing deformations and object interactions, Dress-
Priorworkinhumanreconstructioniseitherlimitedtotight Recon yields higher-fidelity 3D reconstructions than prior
clothingwithnoobjectinteractions,orrequirescalibrated art. Projectpage: https://jefftan969.github.io/dressrecon/
multi-view captures or personalized template scans which
are costly to collect at scale. Our key insight for high-
quality yet flexible reconstruction is the careful combina- 1.Introduction
tionofgenerichumanpriorsaboutarticulatedbodyshape
We aim to reconstruct animatable dynamic human avatars
(learnedfromlarge-scaletrainingdata)withvideo-specific
from videos of people wearing loose clothing or interact-
articulated “bag-of-bones” deformation (fit to a single
ing with objects, such as in-the-wild monocular videos
video via test-time optimization). We accomplish this by
recordedonaphoneorfromtheInternet. High-qualityre-
learninganeuralimplicitmodelthatdisentanglesbodyver-
constructions in this setting traditionally require calibrated
susclothingdeformationsasseparatemotionmodellayers.
multi-viewcaptures[40,63],whicharecostlytoobtain.
Tocapturesubtlegeometryofclothing,weleverageimage-
From only a single viewpoint, recovering freely-
based priors such as human body pose, surface normals,
deforminghumanswitharbitrarytopologyishighlyunder-
*Correspondingauthor:jefftan@andrew.cmu.edu constrained, and thus prior works often rely on domain-
4202
peS
03
]VC.sc[
1v36502.9042:viXraspecific constraints which struggle to support loose cloth- Table 1. Related work in monocular 3D body reconstruction.
ing. Template-basedhumanreconstruction[18,19,62]re- (1)Methods based on human body and pose models. (2)General
methods for humans and animals. Dense: Dense deformation
quires personalized scanned templates, which works well
fields. Bob: Bag-of-bones. H:Humanbodyandposepriors. F:
for a single instance but cannot reconstruct unseen cloth-
Opticalflow. N:Surfacenormal. ϕ: Features. Ourmethodcom-
ing and body shapes and clothing. Methods that regress
bines the best of human-specific and general methods by fitting
3Dsurfacesfromasingleimage[60,61]canproducehigh-
a flexible motion model initialized from off-the-shelf 3D human
qualitygeometryatobservedregions,buttheresultsarein-
poses,usingdenseimage-basedpriors.
consistentacrossframesandsometimesfailtoproduceco-
herent body shapes. Human-specific methods [16, 24, 54]
Method Motionmodel Prior Input
can achieve high quality on tight clothing, but often use a
ECON[61] N.A. H,N Image
fixedhumanskeletonorparametricbodytemplateandthus
NeuMan[24] Skeleton H Video
cannothandleextremedeformationsoutsidethebody.More (1) Vid2Avatar[16] Skeleton H Video
broadly, genericmethodsforhumansandanimals[65,66] SelfRecon[22] Skeleton+Dense H,N Video
HumanNeRF[54] Skeleton+Dense H Video
cansupportarbitrarydeformations,butoftenproducelower
qualityresultsthanhuman-specificmethods. MagicPony[56] Skeleton ϕ Image
LASR[65] Bob F Video
This paper presents DressRecon, which reconstructs (2)
BANMo[67] Bob F,ϕ Video
freeform4Dhumanswithlooseclothingandhandheldob-
RAC[68] Skeleton+Dense F,ϕ Video
jectsfrommonocularvideos. Ourkeyinsightisthecareful
DressRecon(Ours) HierarchicalBob H,F,N,ϕ Video
combinationofgenerichuman-levelpriorsaboutarticulated
body shape (learned from large-scale training data) with
video-specific articulated “bag-of-bones” clothing models ered. Trained on ground truth 3D scans, pixel-aligned im-
(fittoasinglevideoviatest-timeoptimization). Weaccom- plicitfunctions[34,47,61]regressclothedhumansurfaces
plish this by learning a neural implicit model that disen- fromamonocularimage, buttheiroutputonavideotends
tanglesbodyandclothingdeformationsasseparatemotion to be less temporally coherent. Another line of work aims
layers. Tocapturesubtlegeometryofclothing,weleverage toreconstructdynamichumanshapesfromvideoinput,us-
image-basedpriorssuchasmasks,normals,andbodypose ingadeformablehumanmodel[16,22,54]orpre-scanned
during optimization. When the goal is shape reconstruc- personalizedtemplates[19,25,62]andoftenachievingsig-
tion,weextracttime-consistentmeshesfromtheoptimized nificantspeedups[20,23,33]. Generichumanmodels(e.g.
neuralfields. Otherwise, toenablehigh-qualityinteractive SMPL) help resolve monocular 3D ambiguity, but with-
rendering, weproposearefinementstagethatconvertsour out a personalized clothed template, few works can han-
implicit neural body into 3D Gaussians while maintaining dledynamicclothingthatdoesnotcloselyfollowbodymo-
the motionfield design. On datasets withhighly challeng- tion. Ourmethodintroducesanovelrepresentationthatnot
ing clothing and object deformations, DressRecon yields onlyleverageshuman-specificmodelpriors,butalsosimul-
higher-fidelity3Dreconstructionsthanpriorart. taneously enjoys the flexibility to handle loose garments.
ReLoo [17] is a concurrent work that applies a two layer
2.RelatedWork
deformationmodeltoaccountforloosegarments.
Humans from multi-view or depth. With sufficient in- Monocularnonrigid3Dreconstruction. Non-rigidstruc-
formation as input, multi-view methods [9, 13, 26, 37, 40, ture from motion (NRSfM) methods [4] reconstruct non-
45, 58] can reconstruct human shape and appearance of rigid 3D shapes from 2D point trajectories in a class-
very high fidelity, but the reliance on a dense capture stu- agnostic way. However, due to the oversimplified mo-
dio limits their applicability at a consumer level. Depth- tion model and the difficulties in estimating long-range
basedmethods[10,59,72]followtheseminalDynamicFu- correspondences [49], they do not work well for videos
sionwork[43]tointegratehumanshapefromamonocular with challenging deformations. Recent work applies dif-
depth stream into a canonical space with the help of a de- ferentiablerenderingtoreconstructarticulatedobjectsfrom
formation model. However, their application scenarios are videos[46,55,65,66]orimages[14,29,56,71]. However,
alsolimitedbecausetheyrequirespecializeddepthsensors. they cannot reconstruct challenging body articulations and
Monocular human reconstruction. Monocular RGB- large deformations beyond the body, due to the lack of a
basedreconstructionischallengingduetothe3Dambigu- flexible motion representation and sufficient measurement
ity of a monocular input. Early work [2, 15, 28, 57] aims signals. As shown in Tab. 1, we introduce a hierarchical
to reconstruct 3D human keypoints or skeletal poses us- bag-of-bonesmotionmodelthatiscapableofrepresenting
ing a deformable human model [27, 39]. Compared with the deformation of loose garments and accessories, fitted
sparse keypoints, reconstructing dense human surfaces is usingrichsignalsfrompretrainedvisionmodelssuchashu-
evenmorechallenging,especiallywhenclothingisconsid- manbodypose,surfacenormals,andopticalflow.Canonical Shape Hierarchical Gaussian Supervision:
(Sec. 3.1) Motion Fields (Sec. 3.2) Image-based Priors (Sec. 3.3)
Initialized from
X
3D Pose Prior t
Normal Feature Mask
+
...
X
Clothing Body Time
Gaussians Gaussians Input video Camera
Figure2. MethodOverview: Werepresent3Dhumansinlooseclothingastemporallyconsistent4Dneuralfields(Sec.3.1). Centralto
ourapproachisaflexiblemotionrepresentationthatcapturesfine-grainedclothingdeformationsaswellaslimbmotions,whileeffectively
utilizingdomain-specificpriorssuchas3Dhumanbodypose(Sec.3.2). Weperformvideo-specificoptimizationthatfitsthismodelto
denseimage-basedpriorsviadifferentiablerendering(Sec.3.3). Afteroptimization, ourneuralimplicitsurfacecanbeextractedintoa
time-consistentmeshviamarchingcubes,orconvertedintoexplicit3DGaussiansforhigh-fidelityinteractiverendering(Sec.3.4).
3.Method Canonical Clothing Cloth+Body Reference
Shape Deformation Deformation Image
Clothing Body Rendering
Ourgoalistoreconstructtime-varying3Dhumansinloose Gaussians Gaussians Loss
clothing from in-the-wild monocular videos (Fig. 2). We
representhumanswithclothingas4Dneuralfieldsandper-
form per-video optimization with differentiable rendering
Camera
(Sec. 3.1). Key to our approach is a hierarchical motion
model (Sec. 3.2) capable of representing large limb mo-
Figure3.Visualizationoftwo-layerdeformation.Thebodyand
tionsaswellasclothingandobjectdeformations. Welever-
clothingdeformationlayerseachcontributeseparatetypesofmo-
age image-based priors (Sec. 3.3) such as body pose, sur-
tion.Inthissequence,theclothingGaussiansdeformthewoman’s
face normals, and optical flow to make optimization more
dresstobelarger, whilethebodyGaussiansmoveherrightarm
stable and tractable. The resulting neural fields can be ex- forward. During forward warping, we start from the canonical
tractedintotime-consistentmeshesviamarchingcubes,or shape(left),andfirstapplytheforwardwarpdescribedbyclothing
convertedintoexplicit3DGaussiansforhigh-fidelityinter- Gaussians, then the forward warp described by body Gaussians.
activerendering(Sec.3.4). Thesameprocesshappensinreverseduringbackwardwarping.
3.1.Preliminary: Consistent4DNeuralFields
backtothecanonicalspaceusingabackwarddeformation
field W(t)− : X → X. We use a 3D cycle loss L to
Torepresentatime-varying3Dhuman,weconstructatime- t cyc
ensurethatW(t)+◦W(t)−isclosetoidentity[35,67].
invariant canonical shape that is warped by a time-varying
Volumerendering. Neuralfieldscanbeoptimizedviadif-
deformationfield.
ferentiable volume rendering [41], which renders images
Canonicalshape. Werepresentthebodyshapeasaneural
andminimizesreconstructionerrors(e.g.photometricloss).
signed distance field in the canonical space, with the fol-
Toprovideadditionalsupervisionongeometryandmotion,
lowingproperties: signeddistanced,colorc,anduniversal
we augment the training data with additional signals ob-
featuresϕ. Thecanonicalfieldsaredefinedas
tainedfromoff-the-shelfnetworks,detailedinSec.3.3.
(d,ϕ)=MLP (X), (1)
SDF
3.2.HierarchicalGaussianMotionFields
c =MLP (X,ω ), (2)
t color t
In monocular 4D reconstruction, it is challenging to find a
whereXisa3Dpointincanonicalspaceandω isatime- motion representation that is both sufficiently flexible and
t
varyingappearancecodespecifictoeachframe. easy to optimize. Recent methods are either not flexible
Space-time warpings. We represent time-varying motion enoughtomodeldynamicstructuresoutsidethebody[22],
using continuous 3D deformation fields. A forward defor- orstruggletorobustlyreconstructdynamicmotionsathigh
mationfieldW(t)+ : X → X mapsacanonical3Dpoint quality [65]. We introduce hierarchical motion fields to
t
totimet.Duringvolumerendering,raysattimetaretraced strikeabalancebetweenflexibilityandrobustness.Bag-of-bonesskinningdeformation.Ourmotionmodelis tominimizetheimpactofclothingGaussians:
inspired by deformation graphs and its extension to Gaus-
s mia on tib ol nen od fBski bn on ni en sg (m deo fid ne els d[ a3 s, 35 D0, G65 a] u. sT sih ae ni sd ,e tya pis ict ao llu ys Beth =e L
cl
=(cid:13) (cid:13)W c+ loth(X,t)−X(cid:13) (cid:13)2 (5)
25) to drive the canonical geometry’s motion. Each Gaus-
Compositional two-layer deformation. The final defor-
sian maintains a time-varying trajectory of its 3D centers
µ ∈RT×3 andorientationsV ∈RT×3 overT frames,as mationfieldsarethecompositionofbodyandclothinglayer
wt ellasaxis-alignedscalesΛ ∈t R3 thataretime-invariant. deformations(Fig. 3),eachwithabout25Gaussianbones.
Duringforwardwarpingweapplytheclothingdeformation
Giventhe3DGaussians,adenseforwarddeformationfield
beforethebodydeformation,andduringbackwardwarping
canbecomputedbyblendingtheSE(3)transformationsof
weperformthereverse:
GaussianswithforwardskinningweightsW+. Similarly,a
densebackwarddeformationfieldisproducedbyblending
withbackwardskinningweightsW t−: W+(t)=W b+ ody(t)◦W c+ loth(t) (6)
W−(t)=W− (t)◦W− (t) (7)
(cid:32) B (cid:33) cloth body
X =W+(X,t)= (cid:88) W+,bGb(cid:0) Gb(cid:1)−1 X (3)
t t
We optimize the body and clothing Gaussians jointly.
b=1
(cid:32) B (cid:33) To encourage body and clothing Gaussians to be well-
X=W−(X ,t)= (cid:88) W−,bGb(cid:0) Gb(cid:1)−1 X (4) distributedin3Dspace,weuseaSinkhorndivergenceloss
t t t t
L [12] to match the spatial distribution of Gaussians
b=1 sink
withthebodyshape. TheSinkhorndivergenceiscomputed
Here G and G are the SE(3) transformations of the between1krandompointsonthecanonicalrestsurface,and
t
canonicalandtimetGaussians,respectively.Forwardskin- 3DpointsontheGaussiansofeachdeformationlayer.
ning weights W+ ∈ Rb are computed using the Maha- With proper initialization and regularization, body and
lanobisdistancefromXtoeachcanonicalGaussianG. We clothingmotioncanbeproperlydisentangled. InFig. 3,the
useacoordinateMLPtorefinetheweights(similarto[67]), clothingGaussiansdeformthedresswhilethebodyGaus-
anduseanegativesoftmaxsuchthatfartherGaussiansare siansdeformthewoman’sarm.Onthesupplementaryweb-
assignedalowerweight. Inthesameway,backwardskin- page, we show video examples where body and clothing
ning weights W t− ∈ RT×b are computed using the Ma- motionisproperlydecomposedbytwo-layerdeformation.
halanobis distance from X to each time t Gaussian G ,
t t
followedbyMLPrefinement.
3.3.OptimizationwithImage-BasedPriors
This bag-of-bones representation can represent large
non-rigiddeformationsduetoitsflexibility,butcanbechal- Optimizing time-varying 3D geometry from monocular
lengingtooptimize. Forexample, mostGaussianscanget videos is challenging due to its under-constrained nature.
concentratedinalocalregion,whichlimitstheabilitytode- Recent advances in surface normals [11], optical flow
formtheotherpartsofthetarget. Carefullyinitializingthe [51,64],imagefeatures[5,44],andzero-shotsegmentation
Gaussiansandspatiallydistributingthemduringoptimiza- [32] provide additional interpretations of raw pixel values.
tioncanhelpavoidsuchbadlocalminima. Ourkeyideais This knowledge is not only generic, but also highly corre-
todividetheGaussiansintobodyandclothinglayers,which latedwiththegeometryandmotionoftheunderlyingscene,
canbeinitializedandregularizedseparately. makingitsuitableforourreconstructiontask. Weintroduce
BodyGaussiansareintendedtorepresentskeletalmotions anoptimizationroutinethatusesfoundationalimage-based
of the target. With recent advances in human and animal priorsassupervisiontomaketheproblemtractable.
body pose [15, 42], 3D joint locations can be robustly es- Surface normals. Without multi-view inputs, it is chal-
timated from images and used to initialize the body Gaus- lenging to distinguish shape from appearance. For exam-
siantrajectories. ThisallowsbodyGaussianstostartfrom ple,detailedstructuressuchasclothingwrinklescanjustas
aclose-to-optimalsolutionandgetlocallyrefinedthrough- easilybepaintedascolorsonaflatsurface,leadingtoinac-
outdifferentiablerendering. TheresultingbodyGaussians curatesurfacegeometry. Tocounteractthis,weusenormal
exhibit less temporal jitter than the single-frame predictor, estimators[31]trainedonlargedatasetstoprovideasignal
andarebetteralignedtophysicalbonelocations. toimprovethegeometry. Wecantakespatialderivativesof
ClothingGaussiansareintendedtorepresentfree-formde- signeddistancedwithrespecttoX tocomputethesurface
t
formationsnotexplainedbybodyGaussians,suchascloth normalofapointX indeformedspace. Wenormalizethe
t
deformationandthemotionofhandheldobjects.Toencour- renderedandestimatedsurfacenormalsandcomputeanor-
agethatclothingGaussiansonlydeformstructuresoutside mal loss as L error between them. Similar to prior work
2
thescopeofbodyGaussians, weaddaregularizationterm on neural surface reconstruction [70], we also compute aneikonallossL toregularizetheneuralsurface. to tracking [69] to predict accurate silhouettes of humans
eik
with clothing and accessories. We pass different prompts
n=normalize(∇d(W−(X ,t))) (8)
t accordingtodifferentscenariosweaimtoreconstruct,such
L =∥n−n∗∥2 =2−2⟨n,n∗⟩ (9) as“humanwearingcloth”and“humanholdinganobject”.
n
L eik =(cid:13) (cid:13)norm(∇d(W−(X t,t)))−1(cid:13) (cid:13) (10) W dee rec do am np du et se tis mil ah to eu de st it le holo us es ttea ss ,t Lhe =L 2 ∥ser −ro sr ∗b ∥e 2t .ween ren-
s
Normalswithnumericalgradients. Mostpriorworkuses Losses. Ourfinallossisaweightedsumofreconstruction
analytical gradients (e.g. auto-diff) to compute normals andregularizationterms. Lossweightsλaresearchedonce
of signed distance fields. However, these are computed andkeptacrossallexperiments.
within an infinitesimally small neighborhood of X and
t L =λ L +λ L +λ L +λ L +λ L (12)
suffer from noise in both the estimated backward warping rec c c f f n n ϕ ϕ s s
fieldsandsigneddistances[7]. Thisleadstounstableopti- L reg =λ eikL eik+λ cycL cyc+λ sinkL sink+λ clL cl (13)
mization when dealing with deformable objects. To avoid
3.4.Refinementwith3DGaussians
this,wecomputenormalsbynumericalgradients[36]with
afixed1mmstepsizeduringoptimization. Representation. NeuralSDFsareidealforextractingsur-
Normals with eikonal filtering. Although numerical nor- faces,butcanbedifficulttooptimizeasaddingnewgeom-
mal computation works well on static scenes, it is more etryrequiresmakingglobalchanges. Inlightofthis,wein-
challengingindeformablesceneswherethewarpingfield’s troduce a refinement procedure that replaces the canonical
influencecancause∥X +δ−X ∥tobeverydifferentfrom shape representation with 3D Gaussians [30] while keep-
t t
∥W−(X + δ) − W−(X )∥. For example, the hand and ingthetwo-layermotionmodelasis. Torenderanimage,
t t
waistmightbecloseindeformedspacebutfarincanonical wewarpGaussiansforwardfromcanonicalspacetotimet
space, causing exploding gradients due to a large change (Eq.17)andcallthedifferentiableGaussianrasterizer.
in signeddistance gradient overa small neighborhood. To Initialization. We use 40k Gaussians, each parameterized
avoid this problem, we clip the normal direction to 0 af- by 14 values, including its opacity, RGB color, center lo-
ter Eq. 8 whenever the gradient magnitude exceeds some cation, orientation, and axis-aligned scales. Gaussians are
threshold,inourcase∥∇d∥>10. initialized on the surface of the neural SDF with isotropic
Optical flow. We use optical flow [51, 64] to learn the scaling. ToinitializethecolorofeachGaussian, wequery
non-rigid deformation and relative camera transform be- thecanonicalcolorMLP(Eq.2)atitscenter.
tween two frames. We compute 3D scene flow vectors by Optimization. Weupdateboththecanonical3DGaussian
backwardwarpingdeformedpointstocanonicalspace,then parametersandthemotionfieldsbyminimizing
forward-warping to another timestamp. We use the cam-
eramatrixtoproject3Dflowvectorsinto2D,andcompute L rec =λ cL c+λ fL f +λ sL s (14)
L error between rendered flow f and estimated flow f∗, L =λ L . (15)
2 reg sink sink
L =∥f−f∗∥. Here,|t′−t|={1,2,4,8}:
f
Notably, the 3D cycle loss L can be dropped since ras-
cyc
f (X ,t→t′)=W+(W−(X ,t),t′)−X (11) terizationdoesnotrequirecomputingbackwardwarps.
3D t t t
Universalfeatures.Deepneuralfeaturesareusefulforreg- 4.Experiments
isteringpixelstoa3Dmodel[38,66], whileallowingbet-
terconvergenceattexturelessregionsorunderdeformation. We evaluate DressRecon’s ability to reconstruct both 3D
Prior work relies on category-specific image features, but shapeandappearancegivenchallengingmonocularvideos.
wefindDINOv2[44])tobearobustanduniversalfeature Videoresultsareavailableonthesupplementarywebpage.
descriptorthatworkswellforclothingandaccessories. We
4.1.Datasets
choose the small DINOv2 model with registers, as it pro-
duces fewer peaky feature artifacts [8]. We obtain pixel- Dynamic clothing and accessories. To evaluate Dress-
level features from DINOv2’s patch descriptors by eval- Recon’s ability to reconstruct dynamic clothing and ob-
uating DINOv2 on an image pyramid, averaging features jects,weselect14sequencesfromDNA-Rendering[6]with
acrosspyramidlevels,andreducingthedimensionto16via challengingclothdeformationand/orhandheldobjects(e.g.
PCA[1]. WecomputefeaturelossasL errorbetweenren- playing a cello, swinging a cloth, waving a brush). As
2
deredandestimatedfeatures,L =∥ϕ−ϕ∗∥2 . DNA-Renderingdoesnotprovideground-truthmeshes,we
ϕ
Zero-shot segmentation. Inspired by shape-from- compute pseudo-ground-truth 3D meshes by using all 48
silhouette [52], we use image segmentation to carve out available cameras to optimize a separate NeuS2 [53] in-
the 3D boundary of the target. We leverage the founda- stanceateachtimestep. Toovercomethelimitedviewpoint
tional2DsegmentationmodelSAM[32]anditsextension range of each individual camera, we assemble turntablemonocular videos by rendering these per-frame NeuS2 in- Input DressRecon Vid2Avatar BANMo RAC ECON
stancesalongasmooth360-degreecameratrajectory.
Avatars from casual videos. We also evaluate DressRe-
con’sabilitytorecoverhigh-fidelityhumanavatarsfromca-
sualturntablevideos.WeevaluateourmethodonActorsHQ
[21] and select subsets of the first 4 sequences for evalua-
tion, each about 200 frames. As ActorsHQ cameras have
smallfieldsofviewandoftendonotcoverthewholebody,
we colorize the provided ground-truth meshes and render
turntablemonocularvideoswith360◦ofcamerarotation.
4.2.Results
Reconstructingdynamicclothingandaccessories.Tab.2
reportsthe3Dchamferdistance(cm, ↓)forreconstructing
dynamicclothingandhandheldobjects,evaluatedacross14
DNA-Rendering sequences. We compare with Vid2Avatar
[16], BANMo [67], RAC [68], and ECON [61], and show
qualitativeresultsinFig. 4. Theprojectpagecontainscor-
respondingvideoresults.DressReconreconstructsfinerde-
tailsandmoreaccuratebodyshapethanpriorart,andisable
tohandlechallengingscenariossuchasthetipofthecello
(image1),thehairtassels(image2),andthedetailedcloth
wrinklesonthemartialartsuniform(image4).
Reconstructing avatars from casual videos. Tab. 4 re-
ports3Dchamferdistance(cm,↓)andF-scoreat{1,2,5}-
cmthresholdsforrecoveringavatarsfromturntablevideos,
evaluatedacross4ActorsHQsequences. Wecomparewith
Vid2Avatar [16] and show qualitative results in Fig. 5.
DressReconperformsonparwithVid2Avatarintightcloth-
ingscenarios,andreconstructshigher-fidelitygeometryon
sequenceswithchallengingclothingsuchasdresses.
Renderingdynamicclothingandaccessories. Tab. 3re-
ports the RGB PSNR (↑), SSIM (↑), LPIPS (↓), and mask
IoU (↑) on test views by holding out every 8-th training
view. WecompareagainstVid2Avatar[16],BANMo[67],
andRAC[68], andshowqualitativeresultsinFig. 7. The
project page contains extensive video results. DressRecon
producesmoreaccuraterenderingsthanpriorart.
4.3.Diagnostics Figure 4. 3D reconstruction results on DNA-Rendering. We
demonstrate DressRecon’s ability to reconstruct challenging se-
3DGaussianrefinement. InTab. 6,weshowresultsfrom quences with large cloth deformation. DressRecon’s predictions
optimizinganeuralimplicitmodelfromscratch,a3DGaus- alignwellwiththeimageevidence,eveninthepresenceofrapid
sianmodelfromscratch, anda3DGaussianmodelinitial- clothingandobjectdeformations.Vid2Avataroftenoutputsspuri-
izedfromaneuralimplicitmodel. Thesamecomputational ousshapeartifactsandisunabletoreconstructchallengingstruc-
tures, suchasthewhitecloth(row2), brownbrush(row3), and
budget is allocated to all three experiments. The highest
detailedsleeves(row4). BANMoandRACproducehollowcel-
renderingqualityisachievedwithneuralimplicitoptimiza-
losonthefirstrow,andtendtooutputover-smoothedsurfacesfor
tion followed by 3D Gaussian refinement. This suggests
the other cases. ECON produces highly detailed textures, but it
thattheneuralimplicitmodelhelpsproduceagoodinitial-
performstheworstnumerically(Tab. 2)astheoutputsoftenhave
izationofshapeanddeformation,makingiteasierfor3DGS
anincorrectoverallshape(e.g. Row1). Weencouragereadersto
toconvergetobetterlocaloptima. viewthevideoresultsonthesupplementarywebpage.
Choice of deformation model. In Tab. 5, we swap our
hierarchical two-layer deformation model with several al- warpingfield[22,54],skeletonalone[68],orbag-of-bones
ternatives in the literature. Swapping to a skeleton+dense alone[67]reducesthegeometryquality. Alternativedefor-Input DressRecon Vid2Avatar Table 2. 3D reconstruction metrics on DNA-Rendering se-
quences. We evaluate 3D chamfer distance (cm, ↓) on fourteen
DNA-Renderingsequenceswithchallengingclothingdeformation
orhandheldobjects.DressReconoutperformsallbaselines,andis
thebestorsecond-bestmethodonallsequences.
Sequence DressRecon Vid2Avatar BANMo RAC ECON
(Ours) [16] [67] [68] [61]
0008 01 7.300 6.786 7.768 8.112 9.420
0047 01 8.542 11.216 7.808 9.106 17.102
0047 12 6.064 9.334 6.914 6.618 7.541
0102 02 5.421 7.812 5.131 7.278 10.181
0113 06 6.872 8.517 8.362 8.391 11.282
0121 02 5.520 6.478 7.453 6.926 9.334
0123 02 6.725 8.343 7.418 9.683 12.108
0128 04 7.803 8.184 8.913 10.005 11.569
GT DR, 48k DR, 192k V2A, 200k
0133 07 6.194 6.314 7.017 7.449 7.320
0152 01 6.437 7.465 6.815 6.170 8.735
0166 04 4.356 4.608 5.969 6.286 6.562
0188 02 5.403 5.887 6.341 5.829 9.026
0206 04 7.555 8.392 8.404 9.644 9.987
0239 01 5.559 5.503 6.503 7.831 8.026
Image DR w/o normal w/o feat w/o flow w/o masks Average 6.411 7.489 7.201 7.809 9.871
Figure5. 3DreconstructionresultsonActorsHQ.DressRecon
isonparwithVid2Avatarforstandardclothing(Rows2and4), Table3.RenderingmetricsonDNA-Renderingsequences.We
andhigherfidelitythanVid2Avatarforlooseclothing(Rows1and evaluateRGBPSNR(↑),SSIM(↑),LPIPS(↓),andmaskIoU(↑),
3).Vid2Avatar’sreconstructedskirtsoftencontainshapeartifacts. averaged across fourteen DNA-Rendering sequences with chal-
We attribute DressRecon’s improved performance to its flexible lenging clothing deformation or handheld objects. DressRecon
shapeanddeformationrepresentation,whichiscapableofrepre- outperformsallbaselines, particularlywhen3DGaussianrefine-
sentingnon-standardgeometryanddeformation. mentisusedtoimprovetherenderingquality.
Image skel+bob skel+dense bob-only skel-only Image numerical analytical w/o normal
Method PSNR SSIM LPIPS MaskIoU
DressRecon 22.03 0.9375 0.1059 0.9544
w/3DGSrefinement 22.27 0.9506 0.0860 0.9786
Vid2Avatar[16] 19.61 0.8948 0.1167 0.7931
BANMo[67] 19.78 0.9341 0.1257 0.8988
RAC[68] 19.89 0.9351 0.1157 0.9044
Figure6. Qualitativeablationofnumericalnormals. Weshow
thedifferencebetweenoptimizingwithnumericalandanalytical
Input DressRecon Input DressRecon Input nDorremssaRlesc.onUsinganalyticalnormalscausestrainingtobeunstable, Table 4. 3D reconstruction metrics on ActorsHQ sequences.
Weevaluate3Dchamferdistance(cm,↓)andF-scoreat{1,2,5}-
resultinginaflatshapewithnosurfacedetail. Thequalityofsur-
cmthresholds(%, ↑)onthefourActorsHQsequencesshownin
facedetailsisreducedwhennormallossisdisabled(Tab.5).
Fig.5.DressReconoutperformsVid2Avataronmostsequences.
mation models are also less interpretable, as skeleton-only
DressRecon Vid2Avatar
andbag-of-bonesdonotseparatebodyandclothingmotion.
Sequence CD F@5 F@2 F@1 CD F@5 F@2 F@1
Choiceofimage-basedpriors. InTab. 5,weruntheopti-
a1s1 3.212 94.74 72.46 48.99 3.204 98.22 69.69 39.25
mizationroutineandremoveoneoftheimage-basedpriors
a2s1 1.838 99.96 92.72 62.14 2.891 97.59 79.35 40.65
each time. Without mask loss, the surface geometry has a3s1 2.647 97.15 79.91 51.38 4.376 90.93 56.83 30.33
anincorrectoverallstructure. Withoutnormalloss, there- a4s1 2.247 98.88 85.02 57.00 2.039 98.82 89.26 63.84
constructedsurfacehaslowerdetail. Withoutflowloss,the Average 2.486 97.68 82.53 54.88 3.128 96.39 73.78 43.52
shapeislesssensibleandcameraoptimizationislessstable.
V2A DressRecon V2A DressRecon V2A CDhreossiRceecoonf normal supervision. In Fig. 6, we show the
benefitofusingnormallosswithnumericalgradients. With analyticalgradients,shapeoptimizationbecomesunstable.DressRecon DressRecon DressRecon DressRecon
Image Vid2Avatar Image Vid2Avatar
w/o refine +3dgs refine w/o refine +3dgs refine
Input
View
90-deg
Novel View
Input
View
90-deg
Novel View
Figure7. RGBrenderingresultsonDNA-Rendering. Foreachsequence,weshowDressRecon’sandVid2Avatar’srenderingsatboth
theinputviewanda90-degreenovelview.DressRecon’srenderingsareshownwithandwithout3DGaussianrefinement.Wefind(similar
toTab.3)thatrefinementsignificantlyimprovesthetextures,especiallytheflowersontheyellowdancer’ssleeve.Vid2Avatar’srenderings
arelessdetailed,andfailtoaccuratelydepictstructuresthatsubstantiallydeviatefromthebody,suchasthecelloandwhitestool.
Table5.Ablationstudyfor3Dreconstruction.Weablatetheim- Table 6. Ablation study for Gaussian refinement. We ablate
portanceofmotionfieldrepresentationandchoiceofimage-based theimpactof3DGaussianrefinement,byevaluatingRGBPSNR
priors,byevaluating3Dchamferdistance(cm,↓)andF-scoreat (↑),SSIM(↑),LPIPS(↓),andmaskIoU(↑)on14DNA-Rendering
{1,2,5}-cmthresholds(%,↑)on14DNA-Renderingsequences. sequences.WeperformexperimentswhereonlyanimplicitSDFis
DressRecon performs worse after switching motion representa- optimized,where3DGaussiansareoptimizedwithoutinitializing
tions(skeleton-only[16],bag-of-bones[67],skeleton+dense[22]) from an SDF, and where a neural SDF is used to initialize 3D
andafterremovinganyimage-basedprior. Gaussians. Thebestrenderingqualityisobtainedbyinitializing
3DGaussiansfromanSDF.
Sequence CD F@5 F@2 F@1
Sequence PSNR SSIM LPIPS MaskIoU
DressRecon 6.411 81.16 47.66 25.62
Implicit-only 22.03 0.9375 0.1059 0.9544
skeleton-only 7.340 78.10 44.55 22.67
3DGS-only 21.31 0.9455 0.0939 0.9737
bag-of-bones 6.942 80.19 47.21 25.49
Implicit→3DGS 22.27 0.9506 0.0860 0.9786
skeleton+dense 7.526 76.33 41.58 21.91
w/omask 10.647 65.73 33.61 17.42
w/onormal 7.206 77.70 43.46 23.53 normals to make optimization more tractable. To improve
w/oflow 7.094 79.03 45.72 24.41 the rendering quality, we introduce a refinement stage that
w/opose 6.938 78.87 46.21 25.11 convertstheimplicitneuralbodyinto3DGaussians.
w/ofeat 6.829 79.25 46.75 25.34 Limitations. DressReconrequiressufficientviewcoverage
toreconstructacompletehuman,andcannothallucinateun-
5.Discussion observedbodyparts. Italsohasnounderstandingofcloth
deformationphysics. Asaresult,clothingmaydeformun-
We present DressRecon, which reconstructs humans with naturallyifwereanimatewithnovelbodymotion.Weleave
loose clothing and accessory objects from monocular reanimatinghuman-clothandhuman-objectinteractionsas
videos. DressRecon uses hierarchical bag-of-bones defor- futurework.Moreover,specifyinginaccuratesegmentation,
mationtomodelclothingandbodydeformationseparately, e.g.bypassingthewrongprompttoSAM[32],couldresult
andleveragesoff-the-shelfpriorssuchasmasksandsurface infailuretoreconstructsomedetails.References [13] ChenGeng,SidaPeng,ZhenXu,HujunBao,andXiaowei
Zhou. Learning neural volumetric representations of dy-
[1] ShirAmir,YossiGandelsman,ShaiBagon,andTaliDekel.
namichumansinminutes. InProceedingsoftheIEEE/CVF
Deepvitfeaturesasdensevisualdescriptors. arXivpreprint
Conference on Computer Vision and Pattern Recognition,
arXiv:2112.05814,2021. 5
pages8759–8770,2023. 2
[2] FedericaBogo,AngjooKanazawa,ChristophLassner,Peter
[14] Shubham Goel, Angjoo Kanazawa, and Jitendra Malik.
Gehler,JavierRomero,andMichaelJBlack. Keepitsmpl:
Shape and viewpoints without keypoints. In ECCV, 2020.
Automatic estimation of 3d human pose and shape from a
2
singleimage. InComputerVision–ECCV2016: 14thEuro-
[15] ShubhamGoel, GeorgiosPavlakos, JathushanRajasegaran,
peanConference,Amsterdam,TheNetherlands,October11-
Angjoo Kanazawa*, and Jitendra Malik*. Humans in 4D:
14,2016,Proceedings,PartV14,pages561–578.Springer,
Reconstructing and tracking humans with transformers. In
2016. 2
ICCV,2023. 2,4
[3] AljazˇBozˇicˇ,PabloPalafox,MichaelZollho¨fer,JustusThies,
[16] Chen Guo, Tianjian Jiang, Xu Chen, Jie Song, and Ot-
Angela Dai, and Matthias Nießner. Neural deforma-
mar Hilliges. Vid2Avatar: 3D Avatar Reconstruction from
tiongraphsforglobally-consistentnon-rigidreconstruction.
Videos in the Wild via Self-supervised Scene Decomposi-
CVPR,2021. 4
tion. CVPR,2023. 2,6,7,8
[4] Christoph Bregler, Aaron Hertzmann, and Henning Bier-
[17] Chen Guo, Tianjian Jiang, Manuel Kaufmann, Chengwei
mann. Recoveringnon-rigid3dshapefromimagestreams.
Zheng,JulienValentin,JieSong,andOtmarHilliges.Reloo:
InCVPR,2000. 2
Reconstructing humans dressed in loose garments from
[5] MathildeCaron,HugoTouvron,IshanMisra,Herve´ Je´gou,
monocular video in the wild. In European conference on
JulienMairal,PiotrBojanowski,andArmandJoulin.Emerg-
computervision(ECCV),2024. 2
ing properties in self-supervised vision transformers. In
[18] Marc Habermann, Weipeng Xu, Michael Zollhoefer, Ger-
ICCV,pages9650–9660,2021. 4
ardPons-Moll,andChristianTheobalt. LiveCap: Real-time
[6] WeiCheng, RuixiangChen, WanqiYin, SimingFan, Keyu
HumanPerformanceCapturefromMonocularVideo. ACM
Chen, Honglin He, Huiwen Luo, Zhongang Cai, Jingbo
TOG,2019. 2
Wang, Yang Gao, Zhengming Yu, Zhengyu Lin, Daxuan
[19] Marc Habermann, Weipeng Xu, Michael Zollhoefer, Ger-
Ren, Lei Yang, Ziwei Liu, Chen Change Loy, Chen Qian,
ardPons-Moll,andChristianTheobalt. DeepCap: Monoc-
WayneWu,DahuaLin,BoDai,andKwan-YeeLin. DNA-
ularHumanPerformanceCaptureUsingWeakSupervision.
Rendering: A Diverse Neural Actor Repository for High-
CVPR,2020. 2
FidelityHuman-centricRendering. arXiv,2023. 5
[20] Liangxiao Hu, Hongwen Zhang, Yuxiang Zhang, Boyao
[7] Aditya Chetan, Guandao Yang, Zichen Wang, Steve
Zhou, Boning Liu, Shengping Zhang, and Liqiang Nie.
Marschner, and Bharath Hariharan. Accurate differen-
Gaussianavatar: Towards realistic human avatar modeling
tial operators for hybrid neural fields. arXiv preprint
fromasinglevideoviaanimatable3dgaussians. 2024. 2
arXiv:2312.05984,2023. 5
[21] Mustafa Is¸ık, Martin Ru¨nz, Markos Georgopoulos, Taras
[8] Timothe´e Darcet, Maxime Oquab, Julien Mairal, and Piotr
Khakhulin,JonathanStarck,LourdesAgapito,andMatthias
Bojanowski. Visiontransformersneedregisters,2023. 5
Nießner. HumanRF:High-FidelityNeuralRadianceFields
[9] Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter
forHumansinMotion. ACMTOG,2023. 6
Duiker, Westley Sarokin, and Mark Sagar. Acquiring the
reflectance field of a human face. In Proceedings of the [22] BoyiJiang,YangHong,HujunBao,andJuyongZhang.Sel-
27thannualconferenceonComputergraphicsandinterac- frecon: Selfreconstructionyourdigitalavatarfrommonoc-
tivetechniques,pages145–156,2000. 2 ularvideo. InCVPR,2022. 2,3,6,8
[10] Mingsong Dou, Sameh Khamis, Yury Degtyarev, Philip [23] TianjianJiang,XuChen,JieSong,andOtmarHilliges. In-
Davidson,SeanRyanFanello,AdarshKowdle,SergioOrts stantavatar: Learning avatars from monocular video in 60
Escolano,ChristophRhemann,DavidKim,JonathanTaylor, seconds. arXiv,2022. 2
etal. Fusion4d:Real-timeperformancecaptureofchalleng- [24] WeiJiang, KwangMooYi, GolnooshSamei, OncelTuzel,
ing scenes. ACM Transactions on Graphics (ToG), 35(4): andAnuragRanjan. Neuman: Neuralhumanradiancefield
1–13,2016. 2 from a single video. In ECCV, pages 402–418. Springer,
[11] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir 2022. 2
Zamir. Omnidata: A scalable pipeline for making multi- [25] YueJiang,MarcHabermann,VladislavGolyanik,andChris-
taskmid-levelvisiondatasetsfrom3dscans.InICCV,pages tian Theobalt. Hifecap: Monocular high-fidelity and ex-
10786–10796,2021. 4 pressive capture of human performances. arXiv preprint
[12] Jean Feydy, Thibault Se´journe´, Franc¸ois-Xavier Vialard, arXiv:2210.05665,2022. 2
Shun-ichi Amari, Alain Trouve, and Gabriel Peyre´. Inter- [26] HanbyulJoo, TomasSimon, XulongLi, HaoLiu, LeiTan,
polatingbetweenoptimaltransportandmmdusingsinkhorn LinGui,SeanBanerjee,TimothyGodisart,BartNabbe,Iain
divergences. In The 22nd International Conference on Ar- Matthews, et al. Panoptic studio: A massively multiview
tificial Intelligence and Statistics, pages 2681–2690, 2019. system for social interaction capture. TPAMI, 41(1):190–
4 204,2017. 2[27] Hanbyul Joo, Tomas Simon, and Yaser Sheikh. Total cap- [43] Richard A Newcombe, Dieter Fox, and Steven M Seitz.
ture:A3ddeformationmodelfortrackingfaces,hands,and Dynamicfusion: Reconstruction and tracking of non-rigid
bodies. InCVPR,2018. 2 scenesinreal-time. InCVPR,pages343–352,2015. 2
[28] Angjoo Kanazawa, Michael J Black, David W Jacobs, and [44] MaximeOquab,Timothe´eDarcet,TheoMoutakanni,HuyV.
Jitendra Malik. End-to-end recovery of human shape and Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
pose. InCVPR,2018. 2 DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,Rus-
[29] AngjooKanazawa,ShubhamTulsiani,AlexeiA.Efros,and sell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-
JitendraMalik. Learningcategory-specificmeshreconstruc- WenLi,WojciechGaluba,MikeRabbat,MidoAssran,Nico-
tionfromimagecollections. InECCV,2018. 2 las Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou,
[30] Bernhard Kerbl, Georgios Kopanas, Thomas Leimku¨hler, JulienMairal,PatrickLabatut,ArmandJoulin,andPiotrBo-
and George Drettakis. 3d gaussian splatting for real-time janowski. Dinov2: Learningrobustvisualfeatureswithout
radiancefieldrendering.ACMTransactionsonGraphics,42 supervision,2023. 4,5,2
(4),2023. 5 [45] SidaPeng, YuanqingZhang, YinghaoXu, QianqianWang,
[31] RawalKhirodkar, TimurBagautdinov, JulietaMartinez, Su Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:
Zhaoen,AustinJames,PeterSelednik,StuartAnderson,and Implicit neural representations with structured latent codes
ShunsukeSaito.Sapiens:Foundationforhumanvisionmod- for novel view synthesis of dynamic humans. In CVPR,
els,2024. 4,2 2021. 2
[32] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,
[46] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and
ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite-
FrancescMoreno-Noguer.D-NeRF:NeuralRadianceFields
head,AlexanderCBerg,Wan-YenLo,etal. Segmentany-
forDynamicScenes. InCVPR,2020. 2
thing. arXivpreprintarXiv:2304.02643,2023. 4,5,8,2
[47] ShunsukeSaito,ZengHuang,RyotaNatsume,ShigeoMor-
[33] JiahuiLei,YufuWang,GeorgiosPavlakos,LingjieLiu,and
ishima,AngjooKanazawa,andHaoLi. Pifu: Pixel-aligned
KostasDaniilidis. Gaussianavatar: Towardsrealistichuman
implicitfunctionforhigh-resolutionclothedhumandigitiza-
avatarmodelingfromasinglevideoviaanimatable3dgaus-
tion. InICCV,pages2304–2314,2019. 2
sians. 2024. 2
[48] Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J.
[34] RuilongLi,YuliangXiu,ShunsukeSaito,ZengHuang,Kyle
Black. SCANimate: Weaklysupervisedlearningofskinned
Olszewski,andHaoLi.Monocularreal-timevolumetricper-
clothedavatarnetworks. InCVPR,2021. 1
formance capture. In Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020, [49] PeterSandandSethTeller. Particlevideo: Long-rangemo-
Proceedings,PartXXIII16,pages49–67.Springer,2020. 2 tionestimationusingpointtrajectories. InIJCV,2008. 2
[35] ZhengqiLi,SimonNiklaus,NoahSnavely,andOliverWang. [50] RobertWSumner,JohannesSchmid,andMarkPauly. Em-
Neuralsceneflowfieldsforspace-timeviewsynthesisofdy- beddeddeformationforshapemanipulation. InACMSIG-
namicscenes. InCVPR,2021. 3,1 GRAPH2007papers.2007. 4
[36] Zhaoshuo Li, Thomas Mu¨ller, Alex Evans, Russell H Tay- [51] ZacharyTeedandJiaDeng. RAFT:Recurrentall-pairsfield
lor, MathiasUnberath, Ming-YuLiu, andChen-HsuanLin. transformsforopticalflow. InECCV,2020. 4,5
Neuralangelo:High-fidelityneuralsurfacereconstruction.In
[52] Daniel Vlasic, Ilya Baran, Wojciech Matusik, and Jovan
CVPR,pages8456–8465,2023. 5
Popovic´. Articulated mesh animation from multi-view sil-
[37] ZheLi,ZerongZheng,LizhenWang,andYebinLiu. Ani- houettes. InSIGGRAPH2008.2008. 5
matablegaussians: Learningpose-dependentgaussianmaps
[53] Yiming Wang, Qin Han, Marc Habermann, Kostas Dani-
for high-fidelity human avatar modeling. arXiv preprint
ilidis, Christian Theobalt, and Lingjie Liu. Neus2: Fast
arXiv:2311.16096,2023. 2
learning of neural implicit surfaces for multi-view recon-
[38] PhilippLindenberger,Paul-EdouardSarlin,ViktorLarsson,
struction. In Proceedings of the IEEE/CVF International
and Marc Pollefeys. Pixel-Perfect Structure-from-Motion
ConferenceonComputerVision(ICCV),2023. 5
withFeaturemetricRefinement. InICCV,2021. 5
[54] Chung-Yi Weng, Brian Curless, Pratul P Srinivasan,
[39] MatthewLoper,NaureenMahmood,JavierRomero,Gerard
JonathanTBarron,andIraKemelmacher-Shlizerman. Hu-
Pons-Moll,andMichaelJ.Black. SMPL:Askinnedmulti-
mannerf: Free-viewpointrenderingofmovingpeoplefrom
personlinearmodel. SIGGRAPHAsia,2015. 2
monocularvideo. InCVPR,pages16210–16220,2022. 2,6
[40] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and
[55] ShangzheWu, TomasJakab, ChristianRupprecht, andAn-
DevaRamanan. Dynamic3DGaussians: TrackingbyPer-
sistentDynamicViewSynthesis. 3DV,2024. 1,2 drea Vedaldi. Dove: Learning deformable 3d objects by
watching videos. arXiv preprint arXiv:2107.10844, 2021.
[41] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
2
JonathanTBarron,RaviRamamoorthi,andRenNg. Nerf:
Representingscenesasneuralradiancefieldsforviewsyn- [56] Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rup-
thesis. InECCV,2020. 3 precht, and Andrea Vedaldi. MagicPony: Learning articu-
[42] Tanmay Nath*, Alexander Mathis*, An Chi Chen, Amir lated3danimalsinthewild. 2023. 2
Patel, Matthias Bethge, and Mackenzie W Mathis. Using [57] DonglaiXiang,HanbyulJoo,andYaserSheikh. Monocular
deeplabcutfor3dmarkerlessposeestimationacrossspecies totalcapture: Posingface, body, andhandsinthewild. In
andbehaviors. NatureProtocols,2019. 4 CVPR,2019. 2[58] DonglaiXiang,FabianPrada,TimurBagautdinov,Weipeng
Xu,YuanDong,HeWen,JessicaHodgins,andChengleiWu.
Modelingclothingasaseparatelayerforananimatablehu-
manavatar. ACMTransactionsonGraphics(TOG),40(6):
1–15,2021. 2
[59] DonglaiXiang,FabianPrada,ZheCao,KaiwenGuo,Chen-
glei Wu, Jessica Hodgins, and Timur Bagautdinov. Driv-
ableavatarclothing:Faithfulfull-bodytelepresencewithdy-
namicclothingdrivenbysparsergb-dinput. arXivpreprint
arXiv:2310.05917,2023. 2
[60] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and
MichaelJ.Black. ICON:ImplicitClothedhumansObtained
fromNormals. CVPR,2022. 2
[61] YuliangXiu,JinlongYang,XuCao,DimitriosTzionas,and
Michael J. Black. ECON: Explicit Clothed humans Opti-
mizedviaNormalintegration. CVPR,2023. 2,6,7
[62] WeipengXu,AvishekChatterjee,MichaelZollho¨fer,Helge
Rhodin, DushyantMehta, Hans-PeterSeidel, andChristian
Theobalt.MonoPerfCap:HumanPerformanceCapturefrom
MonocularVideo. ACMTOG,2018. 2
[63] ZhenXu,SidaPeng,HaotongLin,GuangzhaoHe,Jiaming
Sun, Yujun Shen, Hujun Bao, and Xiaowei Zhou. 4K4D:
Real-Time 4D View Synthesis at 4K Resolution. arXiv,
2023. 1
[64] GengshanYangandDevaRamanan. Volumetriccorrespon-
dence networks for optical flow. In NeurIPS, 2019. 4, 5,
2
[65] GengshanYang,DeqingSun,VarunJampani,DanielVlasic,
Forrester Cole, Huiwen Chang, Deva Ramanan, William T
Freeman, and Ce Liu. LASR: Learning articulated shape
reconstructionfromamonocularvideo. InCVPR,2021. 2,
3,4,1
[66] GengshanYang,DeqingSun,VarunJampani,DanielVlasic,
ForresterCole, CeLiu, andDevaRamanan. Viser: Video-
specificsurfaceembeddingsforarticulated3dshaperecon-
struction. InNeurIPS,2021. 2,5
[67] Gengshan Yang, Minh Vo, Neverova Natalia, Deva Ra-
manan,AndreaVedaldi,andHanbyulJoo.Banmo:Building
animatable 3d neural models from many casual videos. In
CVPR,2022. 2,3,4,6,7,8
[68] Gengshan Yang, Chaoyang Wang, N Dinesh Reddy, and
DevaRamanan.ReconstructingAnimatableCategoriesfrom
Videos. CVPR,2023. 2,6,7,1
[69] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing
Wang,andFengZheng. Trackanything: Segmentanything
meetsvideos,2023. 5
[70] LiorYariv,JiataoGu,YoniKasten,andYaronLipman. Vol-
umerenderingofneuralimplicitsurfaces.NeurIPS,2021.4,
1
[71] Yufei Ye, Shubham Tulsiani, and Abhinav Gupta. Shelf-
supervisedmeshpredictioninthewild. InCVPR,2021. 2
[72] TaoYu,KaiwenGuo,FengXu,YuanDong,ZhaoqiSu,Jian-
huiZhao,JianguoLi,QionghaiDai,andYebinLiu. Body-
fusion: Real-timecaptureofhumanmotionandsurfacege-
ometryusingasingledepthcamera. InProceedingsofthe
IEEE International Conference on Computer Vision, pages
910–919,2017. 2DressRecon: Freeform 4D Human Reconstruction from Monocular Video
Supplementary Material
6.VideoResults computes the normalized influence of each Gaussian bone
onacanonical3Dpoint.Atacoarselevel,skinningweights
Pleaseseetheattachedwebpageforvideoresults.
are defined as the Mahalanobis distance from X to the
canonicalGaussians:
7.ImplementationDetails
7.1.Consistent4DNeuralFields W σ+ =(X−µ)⊤Q(X−µ), (19)
Signed distance fields. We initialize canonical signed whereµ∈RB×3arecanonicalbonecenters,Q=V⊤ΛV
distance fields as a sphere with radius 0.1m. Following arecanonicalboneprecisionmatrices,V ∈ RB×SO(3) are
standard practice, we apply positional encodings to all 3D canonicalboneorientations,andΛB×3×3aretime-invariant
points(L xyz = 10)andtimestamps(L t = 6)beforepass- axis-aligneddiagonalscalematrices.
ingintoMLPs. Theappearancecodeω thas32channels. In addition to a coarse component, we find it helpful to
AfterMLP SDF computesthesigneddistancedata3D use delta skinning weights to model fine geometry. Delta
point, we convert the signed distance to a volumetric den- skinningweightsarecomputedbyacoordinateMLP:
sity σ ∈ [0,1] for volume rendering. Similar to VolSDF
[70],thisisdoneusingthecumulativeLaplacedistribution W ∆+ =MLP ∆,+(X,t)∈RB (20)
σ = Γ (d),whereβ isagloballearnablescalarparameter
β
that controls the solidness of the object, approaching zero The final skinning function is a normalized sum of coarse
forsolidobjects. Thisrepresentationallowsustoextracta andfinecomponents:
meshasthezerolevel-setoftheSDF.
W+ =S+(X,t)=softmax(−W+−W+), (21)
Cycleconsistencyregularization. Givenaforwardwarp- σ ∆
ing field W+(t) : X → X and abackward warping field
t where the negative sign ensures that faraway Gaussian
W−(t) : X → X,weintroduceacycleconsistencyterm,
t bones (which have a larger Mahalanobis distance) are as-
similar to NSFF [35]. A sampled 3D point in camera co-
signedalowerskinningweightaftersoftmax.
ordinatesshouldreturntoitsoriginallocationafterpassing
Backward skinning weights are computed analogously
throughabackwardandforwardwarping:
with the time t Gaussians, which have center µ , orienta-
t
L
=(cid:88)
∥W+(W−(X ,t),t)−X ∥2 (16)
tionV t,andtime-invariantscaleΛ.Wealsoneedthetrans-
cyc t t 2 formation G− from each time t Gaussian to the canonical
t
Xt Gaussian,aswellasthebackwardskinningMLP−.
∆
7.2.HierarchicalGaussianMotionFields
7.3.Optimization
Bag-of-bones skinning deformation. Our motion model
Sampling. Due to the expensive per-ray computation in
usesthemotionofB bones(definedas3DGaussians,typ-
volumerendering,optimizationwithbatchgradientdescent
icallyB = 25)todrivethemotionofcanonicalgeometry.
is challenging. As a result, previous methods randomly
Given 3D Gaussians, we compute dense 3D motion fields
sample entire images [68] to compute the reconstruction
byblendingtheSE(3)transformationsofcanonicalGaus-
terms, leading to small batch sizes (typically 16 images
sianswithskinningweightsW:
per batch) and noisy gradients. We implement an effi-
cient data-loading pipeline with memory-mapping that al-
(cid:32) B (cid:33)
X =W+(X,t)= (cid:88) W+,bGb(cid:0) Gb(cid:1)−1 X (17) lowsper-pixelmeasurements(e.g.,RGB,flow,features)to
t t load directly from disk without accessing the full image.
b=1
This allows loading pixels from significantly more images
(cid:32) B (cid:33)
X=W−(X ,t)= (cid:88) W−,bGb(cid:0) Gb(cid:1)−1 X (18) inasinglebatch(e.g. 256imagesonaGPU).
t t t t Hyperparameters.WeusetheAdamoptimizerwithlearn-
b=1
ing rate 0.0005. We use 48k iterations of optimization for
where G+ are forward warps from canonical to time t allexperiments. OnasingleRTX4090GPU,ittakesabout
t
Gaussians,G− arebackwardwarpsfromtimettocanoni- 8hourstooptimizetheneuralimplicitbodymodeland15
t
calGaussians,andW+areforwardskinningweights. seconds to render each frame. 3D Gaussian refinement is
Similar to SCANimate [48] and LASR [65], we define performed for another 48k iterations of optimization, tak-
aforwardskinningweightfunctionS+ : X → RB which ingabout8hourstooptimizeand0.1secondstorendereachTable 7. Summary of losses and loss weights. Our final loss is a weighted sum of reconstruction terms (color, optical flow, normal,
feature,andsegmentation)andregularizationterms(eikonal,cycle-consistency,gaussianconsistency,cameraprior,andjointprior).
Loss Weight Description
L λ =0.1 L2loss,renderedRGBvs. theinputimage
c c
L λ =0.5 L2loss,rendered2Dflowvs. computedflowfromVCNPlus[64]
f f
L λ =0.03 L2loss,renderednormalsvs. computednormalsfromSapiens[31]
n n
L λ =0.01 L2loss,renderedfeaturesvs. computedfeaturesfromDINOv2[44]
ϕ ϕ
L λ =0.1 L2loss,renderedmasksvs. computedmasksfromSAM[32]
s s
L λ =0.01 EncouragenumericalgradientsofcanonicalSDFtohaveunitnorm
eik eik
L λ =0.05 Encouragebackwardandforwardwarpingfieldstobeinverses
cyc cyc
L λ =0.2 Sinkhorndivergencebetweencanonical3DGaussiansandSDF
gauss gauss
L λ =0.1 Minimizethemagnitudeofclothingdeformation
cloth cloth
frame. OurlossweightsaredescribedinTab. 7. Ateachit-
eration,wesample72imagesandtake16pixelsamplesper
image. For training efficiency, input images are cropped
to a tight bounding box around the object and resized to
256x256. To prevent floater artifacts from appearing out-
side the tight crop, 90% of pixel samples are taken from
thetightboundingboxand10%ofpixelsamplesaretaken
fromthefullun-croppedimage.