Optimism in the Face of Ambiguity Principle
for Multi-Armed Bandits
Mengmeng Li∗ Daniel Kuhn† Bahar Taşkesen‡
October 1, 2024
Abstract
Follow-The-Regularized-Leader (FTRL) algorithms often enjoy optimal regret for adversarial
as well as stochastic bandit problems and allow for a streamlined analysis. Nonetheless, FTRL
algorithms require the solution of an optimization problem in every iteration and are thus
computationally challenging. In contrast, Follow-The-Perturbed-Leader (FTPL) algorithms
achieve computational efficiency by perturbing the estimates of the rewards of the arms, but
their regret analysis is cumbersome. We propose a new FTPL algorithm that generates optimal
policies for both adversarial and stochastic multi-armed bandits. Like FTRL, our algorithm
admits a unified regret analysis, and similar to FTPL, it offers low computational costs. Unlike
existing FTPL algorithms that rely on independent additive disturbances governed by a known
distribution, we allow for disturbances governed by an ambiguous distribution that is only
known to belong to a given set and propose a principle of optimism in the face of ambiguity.
Consequently, our framework generalizes existing FTPL algorithms. It also encapsulates a broad
range of FTRL methods as special cases, including several optimal ones, which appears to
be impossible with current FTPL methods. Finally, we use techniques from discrete choice
theory to devise an efficient bisection algorithm for computing the optimistic arm sampling
probabilities. This algorithm is up to 104 times faster than standard FTRL algorithms that
solve an optimization problem in every iteration. Our results not only settle existing conjectures
but also provide new insights into the impact of perturbations by mapping FTRL to FTPL.
1 Introduction
We consider multi-armed bandit problems where a learner sequentially interacts with an environment
for T rounds. In each round, the learner selects one of the K arms, observes and receives its reward.
The goal of the learner is to minimize regret, which measures the absolute difference between the
total reward received and the total reward that could have been received with perfect knowledge of
the reward distribution. When the rewards received in each round are drawn independently from a
fixed unknown reward distribution, the upper confidence bound (UCB) algorithm [Auer et al., 2002a]
or the Thompson sampling algorithm [Thompson, 1933] achieve optimal regret O(logT) [Bubeck
and Cesa-Bianchi, 2012]. However, in an adversarial setting, where rewards are chosen strategically
∗Risk Analytics and Optimization Chair, EPFL mengmeng.li@epfl.ch
†Risk Analytics and Optimization Chair, EPFL daniel.kuhn@epfl.ch
‡Booth School of Business, University of Chicago bahar.taskesen@chicagobooth.edu
1
4202
peS
03
]GL.sc[
1v04402.9042:viXraby an adversary, these methods suffer from linear regret [Zimmert and Seldin, 2021]. Conversely, in
an adversarial environment, the Follow-the-Regularized-Leader (FTRL) algorithm, introduced by
√
Gordon [1999] as “generalized gradient descent”, often achieves optimal regret O( KT) [Bubeck
and Cesa-Bianchi, 2012].
Prior knowledge regarding the nature of the environment is typically unavailable. Therefore, an
algorithm that simultaneously achieves optimal regret in both stochastic and adversarial settings is
highly desirable. Recently, Zimmert and Seldin [2021] proved that an FTRL algorithm with Tsallis
√
entropy regularizer can simultaneously achieve the optimal regret bound O( KT) in adversarial
settings and the optimal regret bound O(logT) in stochastic settings, without the need for parameter
tuning. Algorithms of this kind are often said to display the “best-of-both-worlds” (BOBW)
capability [Bubeck and Slivkins, 2012]. The results in [Zimmert and Seldin, 2021] have been extended
along various directions, aimed at identifying the abstract properties of the regularizers that lead
to the BOBW capability [Jin et al., 2024]. Nevertheless, FTRL algorithms require the solution of
an expensive optimization problem in each round to compute the arm-sampling distribution. On
the other hand, the Follow-the-Perturbed-Leader algorithms [Hannan, 1957] perturb the cumulative
reward estimates with noise sampled from a given distribution and select the arm with the maximum
perturbed reward estimate. They are popular for their superior computational efficiency [Abernethy
et al., 2014; Lattimore and Szepesvári, 2020] compared to the FTRL algorithms. Recently, it was
shown that FTPL with Fréchet perturbations has BOBW capability [Honda et al., 2023]. However,
this analysis heavily relies on the specific form of the Fréchet distribution with a particular shape.
A generalized analysis of the FTPL algorithms was later provided by Lee et al. [2024], further
showcasing the strength of FTPL approaches. Nevertheless, this analysis still relies significantly on
extreme value theory and shares no significant commonality with the FTRL-based analysis.
NotethatanyFTPLpolicycanbeexpressedasanFTRLpolicy[Abernethyetal.,2016;Hofbauer
and Sandholm, 2002]. That being said, the reverse direction does not hold in general [Hofbauer and
Sandholm, 2002, Proposition 2.2]. Establishing a one-to-one correspondence between meaningful
subclasses of FTPL and FTRL policies remains open [Abernethy et al., 2016]. The Gradient-Based
Prediction Algorithm (GBPA) framework [Abernethy et al., 2015] encompasses FTRL and FTPL as
special cases, but whether the algorithm itself is FTRL or FTPL still demands specialized regret
analysis. Moreover, an open question is posed by Kim and Tewari [2019] on whether there exists a
noisedistributionthatmatchestheFTRLpolicywiththeTsallisentropyregularizer. KimandTewari
[2019] even showed the impossibility of recovering Tsallis-entropy-regularized FTRL using FTPL
with independent noise distributions across the arms. Constructing perturbations that exactly match
the FTRL algorithm with BOBW capability is crucial to understanding the effects of regularization
through perturbation. Answering this open question will also lead to the unification of FTRL and
FTPL regret analysis.
In this paper, we bridge this gap by studying ambiguous noise-sampling distributions that allow
for correlation across the arms. In addition, we introduce an “optimism in the face of ambiguity”
principle, whereby arms are selected under the most advantageous noise distribution. This is in stark
contrast to standard FTPL algorithms, which assume that the noise distribution is fixed. Using
techniques from discrete choice theory, which is traditionally studied in economics and psychology,
we show that the arm-sampling probabilities under the best noise distribution can be computed
highly efficiently using bisection. Unlike standard FTRL algorithms that require the solution of an
2expensive optimization problem in every round, our approach is far more computationally efficient.
As a result, our algorithm admits a unified regret analysis similar to FTRL and a computationally
efficient implementation similar to FTPL. It also encompasses a broad range of FTRL methods
as special cases, including several optimal ones, such as those with Tsallis entropy and hybrid
regularizers. Notably, while it previously appeared impossible to unify these FTRL methods within
the traditional FTPL framework, our approach successfully achieves this integration.
Related work. Werelaxthei.i.d.noiseassumptioninourpaper,generalizingthetraditionalFTPL
methods. The i.i.d. noise assumption underlying most FTPL algorithms is also relaxed in [Melo
and Müller, 2023] by interpreting the arm-sampling probabilities as choice probabilities of a nested
logit model commonly studied in discrete choice theory. However, the noise distribution considered
in [Melo and Müller, 2023] must be a generalized extreme-value distribution, and the resulting
algorithm does not have BOBW capability. In contrast, we work with a family of distributions and
use ideas from discrete choice theory to devise a highly efficient bisection algorithm for computing the
arm-sampling probabilities under the most advantageous noise distribution. This general framework
encompasses several algorithms that enjoy BOBW regret bounds. The FTPL algorithm with i.i.d.
Fréchet-distributed noise is also known to have the BOBW capability [Honda et al., 2023], but
their regret analysis is tailored to the Fréchet distribution. The results in [Honda et al., 2023]
are generalized to other noise distributions by Lee et al. [2024], but the regret analysis remains
cumbersome. Meanwhile, our perturbation-based algorithm achieves BOBW regret bounds by
exploiting the exact equivalence with FTRL algorithms that have the BOBW capability.
Notation. We denote by [K] = {1,...,K} the set of all integers up to K ∈ N. The probability
simplexover[K]isdefinedas∆K = {p ∈ RK : (cid:80)K p = 1}. Weusee withi ∈ [d]todenotetheith
+ k=1 k i
standardbasisvectorofthed-dimensionalEuclideanspace. TheBregmandivergencefunctioninduced
byadifferentiablefunctionϕ : Rd → RcanbeexpressedasD (x,y) = ϕ(x)−ϕ(y)−⟨x−y,∇ϕ(y)⟩.
ϕ
2 Multi-Armed Bandits
We study the multi-armed bandit (MAB) problem, where a learner is given a fixed set of arms [K]
and interacts with an environment over T ∈ N rounds. In each round t ∈ [T], the learner selects an
action a ∈ [K], and the environment generates a reward vector r = (r ,r ,...,r ) ∈ [−1,0]K.
t t t,1 t,2 t,K
The learner observes and receives the reward associated with the chosen arm, r , and receives
t,at
no information as to the values r for k ̸= a . The learner’s objective is to minimize its (pseudo)
t,k t
regret, which measures the difference between the expected reward of the best arm in hindsight and
the learner’s expected cumulative reward
(cid:34) T (cid:35) (cid:34) T (cid:35)
(cid:88) (cid:88)
R(T) = max E r −E r ,
t,k t,at
k∈[K]
t=1 t=1
where the expectation is taken over the internal randomization of the algorithm and the stochastic
nature of the environment. The nature of the learning experience depends on the reward generation
paradigms employed by the environment. In the adversarial paradigm, the environment can choose
the reward vectors arbitrarily, potentially using the history of the learner’s actions to influence
3future rewards. In the stochastic paradigm, the rewards are sampled i.i.d. from a fixed distribution.
There are also other regimes characterized by the varying levels of adversarial power exerted by the
environment.
For clarity in presenting our results and to capture various learning paradigms, we adopt
adversarial regime with a self-bounding constraint from [Zimmert and Seldin, 2021]. In this regime,
for some ∆ ∈ [0,1]K and C ≥ 0, the adversary selects rewards such that at time T, the learner’s
regret of any policy satisfies
T K
(cid:88)(cid:88)
R(T) ≥ ∆ P(a = k)−C. (1)
k t
t=1k=1
Note that the above condition must be satisfied at time T, but it is not required to hold for all times
t < T. The stochastic bandit setting, where the rewards r are drawn i.i.d. from distributions
t,at
with fixed means E[r |a = k] = µ , is an instance of the adversarial regime with a self-bounding
t,at t k
constraint. The pseudo-regret in the stochastic regime can be written as
T K (cid:18) (cid:19)
(cid:88)(cid:88)
R(T) = maxE[r ]−E[r | a = k] P(a = k),
t,j t,at t t
j∈[K]
t=1k=1
which satisfies (1) with ∆ = max E[r ]−E[r ] for all k ∈ [K] and C = 0. The adversarial
k j∈[K] t,j t,k
regime with a self-bounding constraint also encompasses several other paradigms, including the
stochastically constrained adversarial [Wei and Luo, 2018] and adversarially corrupted stochas-
tic [Lykouris et al., 2018] settings. Finally, if the learner’s regret for any policy is not required to
satisfy the self-bounding constraint (1), then the learner operates within the adversarial setting.
Algorithm 1 Gradient-based prediction algorithm (GBPA) for MAB
Require: Differentiable and convex function ϕ with ∇ ϕ(u) ∈ ∆K
u
1: uˆ 0,k ← 0 ∀k ∈ [K]
2: for round t = 1,...,T do
3: Environment chooses a reward vector r t ∈ [−1,0]K
4: Learner chooses a t ∼ p t = ∇ uϕ(u)| u=uˆt−1
5: Learner receives r t,at
6: Learner estimates single-round reward vector rˆ t = (r t,at/p t,at)e at
7: uˆ t ← uˆ t−1+rˆ t
8: end for
We study the Gradient Based Prediction Algorithm (GBPA) [Abernethy et al., 2014, 2012, 2015;
Kim and Tewari, 2019] for multi-armed bandits presented in Algorithm 1. At each round t ∈ [T] of
GBPA, the learner maintains an unbiased estimate of the cumulative reward uˆ , updates uˆ by
t−1 t−1
adding a single round estimate rˆ , and uses the gradient of a convex potential function ϕ : RK → R
t
evaluated at uˆ as an arm sampling distribution p from which the learner samples arm a .
t−1 t t
Although this framework might seem restrictive, it has proven foundational for several MAB
algorithms, including but not limited to [Auer et al., 2002b; Kujala and Elomaa, 2005; Neu and
Bartók, 2013]. Additionally, it encompasses various follow-the-leader type algorithms widely used in
sequential decision-making processes with full information, differing primarily in the choice of the
convex function ϕ used as an input to GBPA. In the following sections, we explain the policies of
the learners in each setting given a cumulative reward estimate uˆ .
t
4Follow-the-leader (FTL). In the full information setting, GBPA with ϕ(u) = max p⊤u is
p∈∆K
known as FTL algorithm. The learner chooses the arm with the highest cumulative reward estimate,
which can be equivalently written as a ∼ p ∈ argmax p⊤uˆ .
t+1 t p∈∆K t
Despite the simplicity of implementing Follow-the-Leader (FTL), it is well known that the regret of
FTL can grow linearly with T even for the simple case of K = 2 when the adversary chooses the
reward sequence to be r = {−1/2,0}, r = {−1,0} when t > 1 is odd and r = {0,−1} when t is
1 t t
even [Hazan, 2016, Chapter 5].
Follow-the-regularized-leader (FTRL). One of the most prominent approaches to stabilize the
FTL algorithm is regularizing the linear objective p⊤u with some convex function ψ : RK → R. In
this case, the learner samples the next arm according to p , where p ∈ argmax p⊤uˆ −ψ(p).
t t p∈∆K t
Then, GBPA with ΦR(u;ψ) = max p⊤u − ψ(p) is known as the FTRL algorithm in the
p∈∆K
full information setting. In the adversarial regime, the FTRL method with Tsallis entropy, i.e.,
√
GBPA(ΦR(·;ηψT
)), achieves the minimax optimal regret of O( KT) [Abernethy et al., 2015,
α
(cid:112) T
Corollary 3.2], where η = T(1−α)/(2α) is the learning rate and ψ is the Tsallis entropy with
α
parameter α ∈ (0,1)
1−(cid:80)K pα
ψT (p) = k=1 k ∀p ∈ RK. (2)
α 1−α
Moreover, when the potential function is allowed to be adaptive, GBPA(ΦR(·;η ψT )) achieves
t α
optimal regret of O(logT) in the stochastic setting [Ito, 2021, Theorem 2]. An FTRL method using
a hybrid regularizer combining Shannon entropy and Tsallis entropy is known to achieve optimal
regret in both adversarial and stochastic settings [Zimmert et al., 2019].
Despite the widespread use of the FTRL framework with various choices of regularization,
including optimal ones, computing arm sampling distributions at each iteration involves solving a
convex optimization problem, making it computationally challenging.
Follow-the-perturbed-leader (FTPL). A promising candidate to circumvent the computational
limitations of FTRL while maintaining the stability of FTRL is achieved by injecting stochastic
noise z ∼ Q into the cumulative reward estimate. In that case, the learner samples the next
arm according to p
t
= E z∼Q[e k⋆(z)], where k⋆(z) ∈ argmax j∈[K]uˆ
t,j
+ z j. Then, GBPA with
ΦP(u;Q) = E z∼Q[max
p∈∆K
p⊤(u+z)] is known as the FTPL algorithm in the full information
setting.
While FTPL algorithms achieve computational efficiency by avoiding the need to solve complex
optimization problems, their analysis is more cumbersome due to the perturbations introduced
compared to the straightforward analysis of FTRL algorithms. Even though FTPL algorithms
have shown BOBW capability [Honda et al., 2023; Lee et al., 2024], it is unclear whether it is
possible to obtain a computationally efficient algorithm that simultaneously inherits the streamlined
analysis of FTRL algorithms. One prominent approach to achieving this goal involves systematically
identifying the perturbations for FTPL that coincide with the arm sampling distributions of FTRL,
a task generally perceived as challenging [Honda et al., 2023]. Therefore, the following has been an
important unresolved open problem seeking the existence of a bridge between regularization and
perturbation-based algorithms.
5Open Question: For some convex ψ : RK → R, is there a perturbation model with distribution Q
that satisfies ∇ ΦP(u;Q) = ∇ ΦR(u;ψ) for all u ∈ RK?
u u
Since FTRL with Tsallis entropy regularizer can achieve the minimax optimal rate in adversarial
bandits, a simpler version of the above open problem is posed by Kim and Tewari [2019] seeking the
existence of an FTPL algorithm with the same arm sampling probability distribution as the FTRL
algorithm with Tsallis entropy. Later, Kim and Tewari [2019, Theorem 8] shows that there is no
stochastic perturbation that yields the same arm sampling probability distribution as the Tsallis
entropy regularizer when the additive perturbations are mutually independent.
In the following section, we identify a general framework for constructing Q that positively
answerstheaforementionedopenquestion. Thiswaspreviouslyconsidereddifficultorevenimpossible
in the FTPL/GBPA literature [Honda et al., 2023; Kim and Tewari, 2019]. Our approach achieves
this by studying ambiguous noise-sampling distributions that allow for correlation across the arms.
3 Distributionally Optimistic Perturbations
We now define the smooth potential function Φ as a best-case expected utility of the type studied in
semi-parametric discrete choice theory, that is,
(cid:20) (cid:21)
Φ(u;B) = supE z∼Q max(u k +z k) , (3)
Q∈B k∈[K]
where z represents a random vector of perturbations that are independent of u. Specifically,
we assume that z is governed by a Borel probability measure Q from within some ambiguity
set B ⊆ P(RK). Note that if B is a singleton that contains only the Dirac measure at the origin
of RK, then Algorithm 1 with Φ coincides with FTL. If we denote by Q⋆ an optimizer of (3), then
Φ(u;{Q⋆}) coincides with ΦP(u;Q⋆) for all u ∈ RK. Hence, GBPA with the potential function
Φ(·;B) generalizes traditional FTPL that injects i.i.d. noise into cumulative reward estimates. In
particular, when B is a singleton joint probability measure with independent fixed marginals, GBPA
with the potential function Φ(·;B) is equivalent to conventional FTPL.
Remark 1 (Conventional FTPL as a special case). Fix any Q¯ ∈ (cid:81)K P(R). If B = {Q ∈
k=1
(cid:81)K P(R) : Q[z ≤ s] = Q¯[z ≤ s] ∀k ∈ [K]}, then we have Φ(u;B) = ΦP(u;Q¯).
k=1 k k
As a notable example within the FTPL family, our method also naturally encompasses the Exp3
algorithm [Auer et al., 1995]. This insight is detailed in the following remark.
Remark 2 (Exp3 algorithm as a special case). If B consists of a singleton distribution described
by a Gumbel distribution, i.e., B = {Q} where coordinates of z ∼ Q follow independent Gumbel
distributions with means log(K)/η and variances π2/(6η2), for some η ∈ R . Then, the smooth
++
potential Φ(u;B) reduces to Φ(u;B) = log((cid:80)K exp(ηu ))/η, which follows from [Taşkesen et al.,
k=1 k
2023, Proposition 3.4] and [McFadden, 1981, Theorem 5.2]. In this case, the arm-sampling probability
vector p⋆(u) ∈ ∆K admits a closed-form expression through p⋆(u) = exp(ηu )/((cid:80)K exp(ηu )),
k k j=1 j
and GBPA(Φ(·;B)) recovers the celebrated Exp3 algorithm [Auer et al., 1995].
In the rest of our paper, we relax the i.i.d. noise assumption commonly adopted in the FTPL
method and focus our attention on marginal ambiguity sets, also referred to as Fréchet ambiguity
6sets [Fréchet, 1951]1. Marginal ambiguity sets completely specify the marginal distributions of the
components of the random vector z but do not impose any constraints on their dependence structure.
This relaxation allows us to recover many optimal FTRL methods in a systematic way.
Definition 1 (Marginal ambiguity set). For given cumulative distribution functions {F }K , the
k k=1
marginal ambiguity set is defined as
B = {Q ∈ P(RK) : Q[z ≤ s] = F (s) ∀s ∈ R,∀k ∈ [K]}. (4)
k k
In the following, we will argue that marginal ambiguity sets explain most known as well as several
new regularization methods in FTRL. As a first step, we state a known result initially established
for discrete choice models that reformulates (3) as a regularized optimization problem, originally
appeared in [Natarajan et al., 2009, Theorem 1] with an alternative proof provided in [Taşkesen
et al., 2023, Proposition 3.6].
Lemma 3.1 ([Natarajan et al., 2009, Theorem 1]). If B is a marginal ambiguity set of the form (4),
and if the underlying cumulative distribution functions F ,k ∈ [K], are continuous, then the smooth
k
potential function (3) can be equivalently expressed as
K K (cid:90) 1
(cid:88) (cid:88)
Φ(u;B) = max u p + F−1(t)dt (5)
k k k
p∈∆K 1−p
k=1 k=1 k
for all u ∈ RK. In addition, Φ(u;B) is convex and differentiable with respect to u ∈ RK, and
∇ Φ(u;B) represents the unique solution of the convex program (5).
u
Note that the right-hand side of (5) is a sum of K strictly concave and differentiable functions
u p +(cid:82)1 F−1(t)dt. Indeed,thederivativeofthekth functionwithrespecttop isu +F−1(1−p ),
k k 1−p k k k k k
k
which is strictly decreasing in p because F (s) is increasing in s and 1−p is strictly decreasing
k k k
in p .
k
We are now positioned to formally address the open question presented in Section 2 with
Corollary 3.2, which bridges the gap between regularization-based and perturbation-based algorithms
for MAB problems.
Corollary 3.2. Suppose that ψ : RK → R is of the form ψ(p) = (cid:80)K ψ (p ) where each ψ is
k=1 k k k
strictly convex, differentiable and satisfies ψ (0) = 0. If B is a marginal ambiguity set of the form (4)
k
with cumulative distribution functions F satisfying −(cid:82)1 F−1(t)dt = ψ (p ) for all k ∈ [K], then
k 1−p k k k
k
Φ(u;B) = ΦR(u;ψ(p)) for all u ∈ RK.
Todifferentiateourapproachfromtraditionalalgorithms,werefertoouralgorithmGBPA(Φ(·;B))
prescribed by ambiguity set B, as the Distributionally Optimistic Perturbation Algorithm (DOPA).
The performance of DOPA, in terms of regret, varies with different ambiguity sets. This variation
will be discussed in Section 4.
1Note that to the best of our knowledge, Fréchet ambiguity sets have no obvious relationship with Fréchet
distributions.
74 Regret of DOPA
Afundamentalprincipleinalgorithmdesignisstability,whichdictatesthatsmallperturbationsinthe
input should not dramatically alter the algorithm’s output. For GBPA with a convex differentiable
potential function ϕ, the output corresponds to ∇ ϕ(u), and stability is reflected through the
u
Lipschitz continuity of the gradient. Unfortunately, not every regularizer leads to a convergent regret
bound. Abernethy et al. [2014] demonstrated that a uniform bound on ∇2ϕ(u) ensures a regret
u
guarantee for GBPA(ϕ) in the full information setting. However, this condition does not directly
transfer to the bandit setting, where the inverse scaling with respect to the arm sampling probability
affects the cumulative reward estimation. Hence, an additional regularity condition on the potential
function Φ, known as differential consistency [Abernethy et al., 2015], is required to ensure sublinear
regret for DOPA.
Definition 2 (Differential consistency). For γ,B > 0, a function g : RK → R is (γ,B)-differentially-
consistent if for all u ∈ RK
(∇2g(u)) ≤ B(∇ g(u))γ ∀k ∈ [K].
u kk u k
The following lemma translates the differential consistency condition on the potential func-
tion Φ(·;B) into requirements on the marginal cumulative distribution functions that prescribe the
ambiguity set B.
Lemma 4.1. Suppose B is a marginal ambiguity set of the form (4) where the marginal cumulative
distribution functions F , k ∈ [K] are twice differentiable on the interior of their respective supports.
k
The potential function Φ(·;B) is (γ,B)-differentially-consistent if
F′(F−1(1−p)) ≤ Bpγ ∀p ∈ (0,1), ∀k ∈ [K]. (6)
k k
Equipped with Lemma 4.1, we are now prepared to present an upper bound on the expected
regret for GBPA(Φ(·;B)).
Theorem 4.2. Suppose B is a marginal ambiguity set of the form (4) where the marginal cumulative
distribution functions F , k ∈ [K] are twice differentiable on the interior of their respective supports.
k
IfadditionallyB encompassesdistributionswithzeromeanandF is(γ,B(T))-differentiallyconsistent
k
for all k ∈ [K] with γ ∈ (1,2), then the regret of GBPA(Φ(·;B)) ensures
(cid:88)K (cid:90) 1 1
R(T) ≤ F−1(t)dt+ B(T)TK2−γ.
k 2
1−p
k=1 0,k
5 Optimal Ambiguity Sets
This section identifies the instances of the marginal ambiguity sets B that allow GBPA(Φ(·;B))
to recover FTRL algorithms that achieve optimal regret bounds in designated regimes. Within
these specific settings, the unresolved conjectures that DOPA addresses become particularly relevant
concerning the recoverability of FTRL algorithms through the application of FTPL methods. First,
we introduce a structured approach to defining the marginal cumulative distribution functions that
prescribe ambiguity sets B and systematically demonstrate the corresponding forms of regularization.
8Theorem 5.1 (Fréchet regularization). Suppose that B is a marginal ambiguity set of the form (4)
and that the marginal cumulative distribution functions are defined through
F (s) = min{1,max{0,1−F(−s/η )}} (7)
k k
for some vector η ∈ RK and strictly increasing function F : R → R with (cid:82)1 F−1(t)dt = 0. Then,
++ 0
Φ(u;B) is equivalent to ΦR(u;ψ), where ψ(p) = (cid:80)K η f(p ) and f(s) = (cid:82)s F−1(t)dt.
k=1 k k 0
The function f(s) introduced in Theorem 5.1 is smooth and convex because its derivative
df(s)/ds = F−1(s) is strictly increasing. From now on we will refer to F as the marginal generating
function. We now tailor the regret upper bound presented in Theorem 4.2 to cases where marginal
cumulative functions are generated by F.
Corollary 5.2 (Regret bound for marginal ambiguity sets of the Fréchet form). Suppose B is a
marginal ambiguity set of the form (7) for some vector η = η1 ∈ RK encompassing distributions
++
with zero mean and Φ(·;B) is (γ,B)-differentially-consistent. Then, GBPA(Φ(·;B)) ensures
BTK2−γ
R(T) ≤ −ηKf(1/K)+ .
2η
√
In the adversarial setting, the optimal regret is established as O( KT) [Audibert and Bubeck,
2009, 2010], and is achievable by FTRL methods using Tsallis entropy [Abernethy et al., 2012].
Recently, there has been significant interest in determining whether perturbation-based methods
can match the efficacy of FTRL techniques. Kim and Tewari [2019] has shown that no stochastic
perturbation can reproduce the choice probability function of the Tsallis entropy regularizer when
the additional random noise in each arm utility is independent. Despite this, it was conjectured
√
that O( KT) regret might be attainable through FTPL with Fréchet-type perturbations. More
recently, Honda et al. [2023] demonstrated that an FTPL with a Fréchet perturbation indeed
√
achieves O( KT) regret. Nevertheless, whether an FTRL algorithm with Tsallis entropy, achieving
this optimal rate, can be replicated by a perturbation-based algorithm has remained open until
now [Honda et al., 2023; Kim and Tewari, 2019] and is resolved by the following theorem.
Theorem 5.3. Suppose that B is a marginal ambiguity set with (shifted) Pareto distributed marginals
of the form (7) induced by the marginal generating function F(s) = (s(α−1)/α+1/α)α−1 1 with
α ∈ (0,1). Then, GBPA(Φ(·;B)) with η k = (cid:112) (T(1−α))/(2α)Kα−1 2 for all k ∈ [K] √satisfies
R(T) ≤ (cid:112) KT/(α(1−α)). In particular, when α = 1/2, GBPA(Φ(·;B)) ensures R(T) ≤ 2 2KT.
Note that the Exp3 algorithm can be realized as a special case of DOPA when B is a marginal
ambiguity set with shifted exponential marginals. Therefore, the Exp3 algorithm is not only induced
by a singleton distribution as in Remark 2, but also induced by marginal ambiguity sets of the
form (4) with exponential marginals.
Remark 3 (Exp3 algorithm revisited). Suppose that B is a marginal ambiguity set of the form (7),
where η = η1 ∈ RK and F(s) = exp(−s−1). Then, DOPA with B is equivalent to FTRL with
++
ψ(p) = (cid:80)K p log(p ) [Abernethy et al., 2014, Section 3].
k=1 k k
Due to the mathematical equivalence between employing FTRL with Tsallis regularization and
using DOPA with shifted Pareto marginals, the attractive BOBW capability of FTRL can be directly
9extended to DOPA. Additionally, the algorithm exhibits anytime properties; it does not require
knowledge of the time horizon T nor the use of doubling schemes. This relationship is detailed in
Theorem 5.4.
Algorithm 2 Anytime GBPA for MAB
Require: (ϕ ) with ∇ ϕ (u) ∈ ∆K
t t=1,2,... u t
1: uˆ 0,k ← 0 ∀k ∈ [K]
2: for round t = 1,... do
3: A reward vector r t ∈ [−1,0]K is chosen by the environment
4: Learner chooses a t ∼ p t = ∇ uϕ t(u)| u=uˆt−1
5: Learner receives r t,at
6: Learner estimates single-round reward vector rˆ t = (r t,at/p t,at)e at
7: uˆ t ← uˆ t−1+rˆ t
8: end for
Theorem 5.4 (Anytime BOBW algorithm with adaptive perturbations). Suppose that B , t ∈ Z
t +
is a marginal ambiguity set of the form
B = {Q ∈ P(RK) : Q[z ≤ s] = F (s) ∀s ∈ R,∀k ∈ [K]}
t k t,k
with (shifted) Pareto-distributed marginals,
F (s) = min{1,max{0,1−(s/η +2)−2}}.
t,k t
√ √
Then, for any T ∈ N, GBPA(Φ(·;B )) with η = 2 t ensures R(T ) ≤ 4 KT +1 always, and simul-
0 t t 0 0
taneouslythefollowingregretboundiftheadversarysatisfies (1): R(T ) ≤ O((cid:80) log(T )/∆ ).
0 k∈[K]:∆ >0 0 k
k
In addition to replicating FTRL using a regularization function derived from a single marginal
generator function, DOPA also effectively replicates hybrid regularizers. These are systematically
derived from two marginal generator functions, as detailed in Corollary 5.5.
Corollary 5.5 (Hybrid Fréchet regularization). Suppose that B is a marginal ambiguity set of the
form (4), and fix γ,η ∈ RK. Suppose further that the marginal cumulative distribution functions are
+
defined through
F (s) = min{1,max{0,1−(γ G−1+η G−1)−1(−s)}} (8)
k k 1 k 2
where G ,G : R → R are strictly increasing functions with (cid:82)1 G−1(t)dt = (cid:82)1 G−1(t)dt = 0. Then,
1 2 0 1 0 2
Φ(u;B) is equivalent to ΦR(u;ψ), where ψ(p) = (cid:80)K (η f(p )+γ g(p )), f(s) = (cid:82)s G−1(t)dt, and
k=1 k k k k 0 1
g(s) = (cid:82)s G−1(t)dt.
0 2
Remark 4 (Generalization to N regularization functions). All existing theoretically optimal algo-
rithms incorporate a hybrid regularizer that combines only two regularization terms [Jin et al., 2024;
Zimmert et al., 2019]. However, it is worth noting that instead of having two generating functions
G ,G , one can define marginal cumulative distribution functions F through N generating functions
1 2 k
G ,...,G and obtain a regularizer as a sum of N integrals of quantile functions G−1 for i ∈ [N].
1 N i
10The following corollary demonstrates that our generalized mixed perturbation formulation can
indeed achieve theoretically optimal BOBW results through its equivalent formulation as hybrid
regularizedFTRL.Tothebestofourknowledge, ithasnotbeenknownwhetheranyFTPLalgorithm
could recover an FTRL algorithm with hybrid regularizers.
Corollary 5.6 (Hybrid adaptive regularizers for bandits). Suppose that B , t ∈ Z is a marginal
t +
ambiguity set of the form
B = {Q ∈ P(RK) : Q[z ≤ s] = F (s) ∀s ∈ R,∀k ∈ [K]},
t k t,k
with
F (s) = min{1,max{0,1−(γ G−1+η G−1)−1(−s)}}.
t,k t,k 1 t,k 2
√
If G (s) = 1−exp(−(s+1)), G (s) = (−2s)−2, with γ = η = t for all t ∈ [T] and k ∈ [K],
1 2 √ t,k t,k
then, GBPA(Φ(·;B )) ensures R(T) ≤ O( KT) always, and simultaneously the following regret
t
bound if the optimal arm is unique and the adversary satisfies (1): R(T) ≤ O((cid:80) logT/∆ )+
k̸=k⋆ k
O((cid:80) (logK)2/∆ ).
k̸=k⋆ k
OuralgorithmicframeworkextendsbeyondtheK-armedbanditsettingwhilemaintainingBOBW
capability. Notable examples include the decoupled-exploitation-exploration setting [Jin et al., 2024],
where the learner can choose to receive a reward from one arm while obtaining information about the
reward from another arm. Another example where our algorithm can be applied and achieve BOBW
regret bound is the dueling bandit setting [Zimmert and Seldin, 2021], where, in each round, two
arms are chosen to “duel” and feedback is received for the arm with the higher reward. Additionally,
our framework recovers the hybrid Tsallis entropy regularizers used in an FTRL-type algorithm with
BOBW capability [Ito et al., 2024] for both K-armed bandit and linear bandit problems.
6 Numerical Experiments
FTPL-type algorithms have been popular because the arm sampling probability distributions
appearing in Line 4 of Algorithm 1 can be computed efficiently when the perturbations z are i.i.d.
k
[Neu and Bartók, 2016]. On the other hand, FTRL-type algorithms fall short of this computational
benefitbecauseanoptimizationproblemateachroundhastobesolvedtocomputethearm-sampling
probabilities. This section discusses how the arm-sampling probabilities of DOPA admit an efficient
computation even when we relax the usual assumption in FTPL that the additive noise components
are independent.
Surprisingly, onecanstillapplyacomputationallyefficientbisectionalgorithmtofindtheoptimal
choice probabilities [Taşkesen et al., 2023, Algorithm 2]. This bisection method uses techniques
from discrete choice theory to exploit the structure of the marginal ambiguity set. As a result, it is
inherently faster than solving an expensive optimization problem. Algorithm 3 enjoys the following
convergence guarantee.
Theorem 6.1 (Convergence guarantee of Algorithm 3 [Taşkesen et al., 2023, Theorem 4.9]). If B is
a marginal ambiguity set of the form (4) and the cumulative distribution function F is continuous
k
for every k ∈ [K], then, for any u ∈ RK and ε > 0, Algorithm 3 outputs p ∈ RK with (cid:80)K p ≤ 1
+ k=1 k
and ∥∇ Φ(u,B)−p∥ ≤ ε.
u
11102
Algorithm 3 Bisection method to approxi-
mate ∇ Φ(u;B)
u
Require: error tolerance ε, utility vector u,
100
marginal ambiguity set B
1: Set τ¯ ← max k∈[K]{−u k −F k−1(1−1/K)} 10-2
2: Set τ ← min k∈[K]{−u k −F k−1(1−1/K)}
3: Evaluate δ √(ε)=min k∈[K]{max δ{δ :|F k(t 1)− 10-4
F (t )|≤ε/ K ∀t ,t ∈R with |t −t | ≤ δ}}
k 2 1 2 1 2
10-6
100 101 102 103 104
4: for k = 1,2,...,⌈log ((τ¯−τ)/δ(ε))⌉ do
2
5: Set τ ← (τ +τ)/2 Figure 1: Runtime of computing arm-sampling
6: Set p k ← 1−F k(−u k −τ) for k ∈ [K] probabilitiesusingFTRL(gray)andDOPA(pur-
(cid:80)
7: if k∈[K]p k > 1 then τ¯ ← τ ple) over 10 simulation runs (solid lines show
8: else τ ← τ end if the mean and the shaded areas correspond to 1
9: end for standard deviation) as a function of number of
10: returnpwithp k = 1−F k(−u k −τ),k ∈ [K] arms K.
In the following, we empirically demonstrate the computational efficiency of DOPA relative to
FTRL by evaluating the runtimes required to compute arm-sampling probabilities. All experiments
are run on an Intel i7-8700 CPU (3.2 GHz) computer with 16GB RAM. The optimization problems
are modelled in MATLAB via YALMIP [McCormick, 1976]. The code is publicly available at
https://github.com/RAO-EPFL/DOPA.
For DOPA, we choose B of the form (7) with F(s) = (−s + 2)−2 and η = 1. In the case
of FTRL, we utilize Tsallis regularization and establish that the arm sampling distributions of
both algorithms are equivalent, such that ∇ Φ(u;B) = ∇ ΦR(u;ηψT ) for all u ∈ RK. We
u u 1/2
calculate ∇
ΦR(u;ψT
) = argmax
p⊤u−ψT
(p) by solving the corresponding second-order-
u 1/2 p∈∆K 1/2
cone program using MOSEK [Mosek ApS, 2019]. We employ Algorithm 3 to approximate ∇ Φ(u;B)
u
with an error tolerance of ε = 10−8, matching the optimality tolerance used by MOSEK for conic
problems. Figure 1 highlights that DOPA achieves running times that are uniformly lower than
FTRL with Tsallis entropy across all numbers of arms and is, in fact, up to 104 times faster.
7 Concluding Remarks and Limitations
We introduce a distributional “optimism in the face of ambiguity” principle to determine the noise
distribution for FTPL-type algorithms in multi-armed bandit problems. This principle allows us
to establish a one-to-one correspondence between FTRL algorithms with separable strictly convex
regularizers and FTPL algorithms. Hence, our algorithm bypasses the difficulties in analyzing
FTPL-type algorithms and lifts the computational burden of FTRL by devising an efficient bisection
algorithm using ideas from modern discrete choice theory. DOPA aims to provide a unified regret
analysis for perturbation-based methods through FTRL and opens doors to the discovery of new
algorithms. We find it promising to study other types of ambiguity sets or other types of regularizers
12induced by marginal ambiguity sets, such as hyperbolic perturbations [Taşkesen et al., 2023, Example
3.11].
At the same time, we acknowledge the limitations of this work. First, certain types of regularizers
cannot be recovered by marginal ambiguity sets B of the form (4), with a notable example of the
log-barrier regularizer as considered in [Jin et al., 2024]. While whether the log-barrier regularizer is
essential in showing the BOBW guarantee of FTRL algorithms remains unclear [Jin et al., 2024],
[Hofbauer and Sandholm, 2002, Proposition 2.2] demonstrates that it is impossible to recover
the FTRL algorithm with the log-barrier regularizer using any FTPL algorithm with a stochastic
perturbation whose distribution is independent of the underlying utilities. Second, the bisection
algorithm presented in Algorithm 3 is efficient as long as the computation of the marginal cumulative
distributions F and the quantile function F−1 are efficient. However, for hybrid regularizers, the
k k
computationofF couldbecumbersome. Asaresult, bisectionmethodmightnotbecomputationally
k
efficient for some choices of hybrid marginal generating functions G and G .
1 2
Acknowledgements
ML and DK are supported by the Swiss National Science Foundation under the NCCR Automation,
grant agreement 51NF40_180545.
References
J. Abernethy, C. Lee, A. Sinha, and A. Tewari. Online linear optimization via smoothing. Conference
on Learning Theory, 2014.
J. Abernethy, C. Lee, and A. Tewari. Perturbation techniques in online learning and optimization.
Perturbations, Optimization, and Statistics, 233:17, 2016.
J. D. Abernethy, E. Hazan, and A. Rakhlin. Interior-point methods for full-information and bandit
online learning. IEEE Transactions on Information Theory, 58(7):4164–4175, 2012.
J. D. Abernethy, C. Lee, and A. Tewari. Fighting bandits with a new kind of smoothness. Advances
in Neural Information Processing Systems, 2015.
J.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. Conference
on Learning Theory, 2009.
J.-Y. Audibert and S. Bubeck. Regret bounds and minimax policies under partial monitoring.
Journal of Machine Learning Research, 11:2785–2836, 2010.
P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. Gambling in a rigged casino: The
adversarial multi-armed bandit problem. IEEE 36th Annual Foundations of Computer Science,
1995.
P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem.
Machine learning, 47:235–256, 2002a.
13P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit
problem. SIAM Journal on Computing, 32(1):48–77, 2002b.
S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit
problems. Foundations and Trends® in Machine Learning, 5(1):1–122, 2012.
S. Bubeck and A. Slivkins. The best of both worlds: Stochastic and adversarial bandits. Conference
on Learning Theory, 2012.
M. Fréchet. Sur les tableaux de corrélation dont les marges sont données. Annales de l’Université de
Lyon, Sciences, 4(1/2):13–84, 1951.
G. J. Gordon. Regret bounds for prediction problems. Conference on Computational Learning
Theory, 1999.
J. Hannan. Approximation to Bayes risk in repeated play. Contributions to the Theory of Games, 3
(2):97–139, 1957.
E. Hazan. Introduction to online convex optimization. Foundations and Trends® in Optimization,
2(3-4):157–325, 2016.
J. Hofbauer and W. H. Sandholm. On the global convergence of stochastic fictitious play. Economet-
rica, 70(6):2265–2294, 2002.
J. Honda, S. Ito, and T. Tsuchiya. Follow-the-Perturbed-Leader Achieves Best-of-Both-Worlds for
Bandit Problems. International Conference on Algorithmic Learning Theory, 2023.
S. Ito. Parameter-free multi-armed bandit algorithms with hybrid data-dependent regret bounds.
Conference on Learning Theory, 2021.
S. Ito, T. Tsuchiya, and J. Honda. Adaptive learning rate for follow-the-regularized-leader: Competi-
tive ratio analysis and best-of-both-worlds. arXiv:2403.00715, 2024.
T. Jin, J. Liu, and H. Luo. Improved best-of-both-worlds guarantees for multi-armed bandits: FTRL
with general regularizers and multiple optimal arms. Advances in Neural Information Processing
Systems, 2024.
B. Kim and A. Tewari. On the optimality of perturbations in stochastic and adversarial multi-armed
bandit problems. Advances in Neural Information Processing Systems, 2019.
J. Kujala and T. Elomaa. On following the perturbed leader in the bandit setting. International
Conference on Algorithmic Learning Theory, 2005.
T. Lattimore and C. Szepesvári. Bandit algorithms. Cambridge University Press, 2020.
J. Lee, J. Honda, S. Ito, and M.-h. Oh. Follow-the-perturbed-leader with Fréchet-type tail distribu-
tions: Optimality in adversarial bandits and best-of-both-worlds. arXiv:2403.05134, 2024.
T. Lykouris, V. Mirrokni, and R. Paes Leme. Stochastic bandits robust to adversarial corruptions.
ACM Symposium on Theory of Computing, 2018.
14G. P. McCormick. Computability of global solutions to factorable nonconvex programs: Part
I—Convex underestimating problems. Mathematical programming, 10(1):147–175, 1976.
D. McFadden. Econometric models of probabilistic choice. Structural analysis of discrete data with
econometric applications, 1981.
E. Melo and D. Müller. Discrete choice multi-armed bandits. arXiv:2310.00562, 2023.
Mosek ApS. Mosek optimization toolbox for matlab. User’s Guide and Reference Manual, Version,
4(1), 2019.
K. Natarajan, M. Song, and C.-P. Teo. Persistency model and its applications in choice modeling.
Management Science, 55(3):453–469, 2009.
G. Neu and G. Bartók. An efficient algorithm for learning with semi-bandit feedback. International
Conference on Algorithmic Learning Theory, 2013.
G. Neu and G. Bartók. Importance weighting without importance weights: An efficient algorithm
for combinatorial semi-bandits. Journal of Machine Learning Research, 17(154):1–21, 2016.
T. Sun and Q. Tran-Dinh. Generalized self-concordant functions: A recipe for Newton-type methods.
Mathematical Programming, 178(1):145–213, 2019.
B. Taşkesen, S. Shafieezadeh-Abadeh, and D. Kuhn. Semi-discrete optimal transport: Hardness,
regularization and numerical solution. Mathematical Programming, 199(1-2):1033–1106, 2023.
W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the
evidence of two samples. Biometrika, 25(3-4):285–294, 1933.
C.-Y. Wei and H. Luo. More adaptive algorithms for adversarial bandits. Conference on Learning
Theory, 2018.
J. Zimmert and Y. Seldin. Tsallis-inf: An optimal algorithm for stochastic and adversarial bandits.
Journal of Machine Learning Research, 22(28):1–49, 2021.
J. Zimmert, H. Luo, and C.-Y. Wei. Beating stochastic and adversarial semi-bandits optimally and
simultaneously. International Conference on Machine Learning, 2019.
15A Auxiliary Results
The following corollary sheds light on the condition of the properties of the cumulative distribution
functions F that induce a strongly convex regularizer. Its proof follows from the smoothness/strong
k
convexity duality and specifically the proof of [Taşkesen et al., 2023, Proposition 4.8]. For complete-
ness, we include the full proof here.
Corollary A.1. If B is a marginal ambiguity set of the form (4), and if the cumulative distribution
functions F ,k ∈ [K], are Lipschitz continuous with Lipschitz constant L, then (cid:80)K (cid:82)1 F−1(t)dt
k k=1 1−p k
k
is L-strongly concave.
Proof of Corollary A.1. We aim to show that −(cid:80)K (cid:82)1 F−1(t)dt−p2/(2L) is convex. By the
k=1 1−p k k
k
assumed L-Lipschitz continuity of F , we have
k
|F (s )−F (s )| F (s )−F (s ) p −q
k 1 k 2 k 1 k 2 k k
L ≥ sup = sup ≥ sup
s1,s2∈R |s 1−s 2| s1,s2∈R s 1−s 2 p k,q k∈(0,1) F k−1(p k)−F k−1(q k)
s1̸=s2 s1>s2 p k>q k
where the second inequality follows from restricting s and s to the preimage of (0,1) with respect
1 2
to F . Rearranging terms in the above inequality then yields
k
q p
−F−1(1−q )− k ≤ −F−1(1−p )− k
k k L k k L
for all p ,q ∈ (0,1) such that q < p . Consequently, the function −F−1(1−p )−p /L is non-
k k k k k k k
decreasing and its primitive −(cid:82)1 F−1(t)dt−p2/(2L) is convex in p on (0,1). The claim then
1−p k k k
k
follows because convexity is preserved under summation.
We use E [·] as a shorthand for the conditional expectation E[· | F ], where F is the σ-algebra
t t−1 t
σ(a ,r ,...,a ,r ) generated by the history of actions and rewards.
1 1 t t
Lemma A.2 (Regret bound for GBPA [Abernethy et al., 2015, Lemma 2.1]). For some convex
potential function ϕ : RK → R, the expected regret of GBPA(ϕ) enjoys the following upper bound
expressed through
(cid:34) T (cid:35)
(cid:88)
R(T) ≤ ϕ(0)+E max uˆ −ϕ(uˆ )+ E [D (uˆ ,uˆ )] .
T,k T t ϕ t t−1
k∈[K]
t=1
Theorem A.3 (Regretboundformarginalambiguitysets). Suppose B is a marginal ambiguity set of
the form (4) where the marginal cumulative distribution functions F ,k ∈ [K] are twice differentiable
k
on the interior of their respective supports. Define p⋆(u) = ∇ Φ(u;B) for any u ∈ RK. The regret
u
of GBPA(Φ(·;B)) satisfies
K (cid:90) 1 (cid:20) (cid:21)
(cid:88)
R(T) ≤ F−1(t)dt+E max uˆ −Φ(uˆ ;B)
k T,k T
1−p⋆ k∈[K]
k=1 0,k
  (9)
(cid:88)T (cid:88)K (cid:90) ∥rˆt∥(cid:90) x
+E  p t,k F k′(F k−1(1−p⋆(uˆ t−1−se at) k)) ds dx.
0 0
t=1k=1, p >0
t,k
16Proof of Theorem A.3. By Lemma 3.1 and Lemma A.2, we have
K (cid:90) 1 (cid:34) T (cid:35)
(cid:88) (cid:88)
R(T) ≤ F−1(t)dt+E max uˆ −Φ(uˆ ;B)+ E [D (uˆ ,uˆ )] .
k T,k T t Φ t t−1
1−p k∈[K]
k=1 0,k t=1
In what follows, we will establish an upper bound on the term E [D (uˆ ,uˆ )]. For any t ∈ [T],
t Φ t t−1
conditioning on the event that arm a ∈ [K] is chosen, we define h : R → R as
t +
h(s) = D (uˆ +srˆ /∥rˆ ∥,uˆ ).
Φ t−1 t t t−1
A direct calculation reveals that the second derivative of h satisfies
h′′(s) = (rˆ /∥rˆ ∥)⊤(∇2Φ(u;B)| )(rˆ /∥rˆ ∥)
t t u u=uˆt−1+srˆt/∥rˆt∥ t t
= e⊤(∇2Φ(u;B)| )e ,
at u u=uˆt−1−seat at
where the second equality follows because r ∈ [−1,0]K and thus e = −rˆ /∥rˆ ∥. Then, we have
t at t t
(cid:88)K (cid:90) ∥rˆt∥(cid:90) x
E [D (uˆ ,uˆ )] = p h′′(s) ds dx
t Φ t t−1 t,k
0 0
k=1, p >0
t,k
(cid:88)K (cid:90) ∥rˆt∥(cid:90) x
= p e⊤(∇2Φ(u;B)| )e ds dx
t,k at u u=uˆt−1−seat at
0 0
k=1, p >0
t,k
(cid:88)K (cid:90) ∥rˆt∥(cid:90) x
= p F′(F−1(1−p⋆(uˆ −se ) )) ds dx,
t,k k k t−1 at k
0 0
k=1, p >0
t,k
where the first equality holds thanks to the fundamental theorem of calculus, and the third equality
follows from [Sun and Tran-Dinh, 2019, Proposition 6].
Similar to Lemma 4.1, we can translate the differential consistency condition into requirements
on the noise distribution.
Lemma A.4 (Differential consistency condition for marginal ambiguity sets of the Fréchet form).
Suppose B is a marginal ambiguity set of the form (7) for some vector η = η1 ∈ RK where the
++
marginal generating function F is twice differentiable on the interior of its support. The corresponding
potential function Φ(·;B) is (γ,B)-differentially-consistent if f(s) = (cid:82)s F−1(t)dt < ∞ satisfies
0
(ηf′′(p))−1 ≤ Bpγ ∀p ∈ (0,1). (10)
Proof of Lemma A.4. Denote by p⋆(u) = ∇ Φ(u;B) which represents the unique solution of the
u
optimization problem (5) by Lemma 3.1. By [Sun and Tran-Dinh, 2019, Proposition 6], the Hessian
of Φ(·;B) can be expressed through the Hessian of its convex conjugate. We then have
(∇2Φ(u;B)) = ((∇2Φ∗(p;B)| )−1)
u kk p p=p⋆(u) kk
1
= ≤ B(p⋆(u) )γ = (∇ (Φ(u;B)) )γ,
ηf′′(p⋆(u) ) k u k
k
where the second equality follows from Theorem 5.1, and the inequality holds because f satisfies (10).
Hence, the claim follows.
17B Omitted Proofs
Proof of Corollary 3.2. By the strict convexity of ψ , we deduce that ψ′(s) is strictly increasing in s.
k k
Hence, there exist strictly increasing functions G such that G (s) = −ψ′(1−s) for any s ∈ [0,1].
k k k
(cid:82)1
In addition, we have − G (t)dt = ψ (p ) thanks to the fundamental theorem of calculus and
1−p k k k
k
the assumption that ψ (0) = 0. Choosing F (s) = min{1,max{0,G−1(s)}} gives F−1(s) = G (s)
k k k k k
for all s ∈ (0,1) and the desired relation −(cid:82)1 F−1(t)dt = −(cid:82)1 G (t)dt = ψ (p ). Applying
1−p k 1−p k k k
k k
Lemma 3.1 concludes the proof.
Proof of Lemma 4.1. Denote by p⋆(u) = ∇ Φ(u;B) which represents the unique solution of the
u
optimization problem (5) by Lemma 3.1. By [Sun and Tran-Dinh, 2019, Proposition 6], the Hessian
of Φ(·;B) can be expressed through the Hessian of its convex conjugate, and thus we have
(∇2Φ(u;B)) = ((∇2Φ∗(p;B)| )−1)
u kk p p=p⋆(u) kk
= F′(F−1(1−p⋆(u) ))
k k k
≤ B(p⋆(u) )γ = B(∇ (Φ(u;B)) )γ,
k u k
where the second equality follows by the inverse function theorem together with the fact that
Φ∗(p;B) = −(cid:82)1 F−1(t)dt, and the third equality holds because p⋆(u) = (∇ Φ(u;B)) by
1−p k k u k
k
Lemma 3.1. Finally, the inequality holds because F ’s satisfy (6). This observation concludes our
k
proof.
Proof of Theorem 4.2. Note that B encompasses distributions of zero mean, and thus any Q ∈ B
satisfies E z∼Q[z] = 0. Then, for any u ∈ RK, we have
(cid:20) (cid:21)
max u k = max E z∼Q[u k +z k] ≤ E z∼Q max u k +z k ≤ Φ(u T;B),
k∈[K] k∈[K] k∈[K]
where the first inequality follows by Jensen’s inequality. The above inequality implies
(cid:20) (cid:21)
E max uˆ −Φ(uˆ ;B) ≤ 0.
T,k T
k∈[K]
Therefore, the second term in the regret bound in (9) is upper bounded by 0.
As for the third term in (9), we have
(cid:88)K (cid:90) ∥rˆt∥(cid:90) x
p F′(F−1(1−p⋆(uˆ −se ) )) ds dx
t,k k k t−1 at k
0 0
k=1,p >0
t,k
(cid:88)K (cid:90) ∥rˆt∥(cid:90) x
≤ B p (p⋆(uˆ −se ) )γds dx
t,k t−1 at k
0 0
k=1,p >0
t,k
≤ B
(cid:88)K
p
(cid:90) ∥rˆt∥(cid:90) x
p⋆(uˆ )γds dx
t,k t−1 k
0 0
k=1,p >0
t,k
(cid:88)K (cid:90) ∥rˆt∥(cid:90) x B (cid:88)K
= B p 1+γ ds dx = p γ−1r2 ,
t,k 2 t,k t,at
0 0
k=1,p >0 k=1
t,k
18where the first inequality follows as Φ(·;B) is (γ,B)-differentially consistent and the second inequality
follows because p⋆(u) is non-decreasing and s ≥ 0.
k
Note that 1/(2−γ)-norm and 1/(γ−1)-norm are duals for γ ∈ (1,2). Then, Hölder’s inequality
yields
K K (cid:32) K (cid:33)γ−1(cid:32) K (cid:33)2−γ
(cid:88) p t,kγ−1 = (cid:88) p t,kγ−1·1 ≤ (cid:88) p t,kγ γ− −1 1 (cid:88) 12−1 γ = (1)γ−1K2−γ = K2−γ.
k=1 k=1 k=1 k=1
This observation together with the assumption that r2 ∈ [0,1] completes our proof.
t,at
Proof of Theorem 5.1. By Lemma 3.1, Φ(u;B) is equivalent to
K K (cid:90) 1
(cid:88) (cid:88)
Φ(u;B) = max u p + F−1(t)dt
k k k
p∈∆K 1−p
k=1 k=1 k
As F is strictly increasing, we have F−1(s) = −F−1(1−s)η for all s ∈ (0,1). Thus, we find
k k
(cid:90) s (cid:90) 1−s 1 (cid:90) 1
f(s) = F−1(t)dt = − F−1(1−x)dx = − F−1(x)dx,
η k
0 1 k 1−s
wherethesecondequalityfollowsfromthevariablesubstitutionx ← 1−t. Thisintegralrepresentation
of f(s) then allows us to reformulate Φ(u;B) as
K K
(cid:88) (cid:88)
Φ(u;B) = max u p − η f(p ).
k k k k
p∈∆K
k=1 k=1
This concludes our proof.
Proof of Corollary 5.2. By Lemma A.3 and Lemma A.4, we have
(cid:88)K BTK2−γ
R(T) ≤ −η f(p )+ , (11)
0,k
2η
k=1
where p = ∇ Φ(u;B)| by Line 4 of Algorithm 1. We now show that p = [1/K,...,1/K],
0 u u=0 0
which coincides with the unique optimizer of
K
(cid:88)
max −η f(p )
k
p∈∆K
k=1
Denote by H(p) =
−(cid:80)K
f(p ) and Π(p ) the set of all permuted copies of p . Take K ele-
k=1 k 0 0
ments {x(i)}K ⊆ Π(p ) by cyclic permutation, i.e., let x(1) = p , x(K) = [p ,p ,...,p ],
i=1 0 0 0,K 0,1 0,K−1
and x(i) = [p ,...,p ,p ,...,p ] ∈ RK for i = 2,...,K − 1. Observe as well that p′ =
0,i 0,K 0,1 0,i−1
[1/K,...,1/K] ∈ ∆K can be represented as p′ = (cid:80)K x(i)/K. We then have
i=1
(cid:18) (cid:19) (cid:32) K (cid:33) K K K
1 1 (cid:88) 1 (cid:88) (cid:88) (cid:88)
−Kf = H(p′) = H x(i) ≥ H(x(i)) = − f(p ) = max− f(p ),
0,k k
K K K p∈∆K
i=1 i=1 k=1 k=1
19where the inequality is due to Jensen, and the third equality holds because H is a permutation
invariant function, i.e., H(x(i)) = −(cid:80)K f(x(i) ) = −(cid:80)K f(p ) for any x(i) ∈ Π(p ). By
k=1 k k=1 0,k 0
Corollary 3.1, (11) admits a unique maximizer p . As p′ ∈ ∆K is feasible in (11), the upper bound
0
above is in fact tight and p = p′. This observation concludes our proof.
0
Proof of Theorem 5.3. We denote η = η within this proof. Thanks to Theorem 5.1 we have
k
Φ(u;B) = ΦR(u;ηψT ) for all u ∈ RK. The claim follows as GBPA(ΦR(·;ηψT )) has R(T) ≤
α α
(cid:112)
KT/(α(1−α)) by [Abernethy et al., 2015, Theorem 3.1].
Proof of Theorem 5.4. Thanks to Theorem 5.1 for any α ∈ (0,1), we have Φ(u;B ) = ΦR(u;η ψT )
t √ t 1/2
for all u ∈ RK. The first part of the claim then follows because when η = 2 t, the regret of
√ t
GBPA(ΦR(·;η ψT )) in the adversarial setting is upper bounded by 4 KT +1 thanks to [Zimmert
t 1/2 0
and Seldin, 2021, Theorem 1]. By the refined analysis of Tsallis-INF algorithm in [Ito, 2021,
(cid:80)
Theorem 2], we further have that R(T ) ≤ O( log(T )/∆ ) if adversary satisfies (1).
0 k∈[K]:∆ >0 0 k
k
Proof of Corollary 5.5. By Lemma 3.1, Φ(u;B) is equivalent to
K K (cid:90) 1
(cid:88) (cid:88)
Φ(u;B) = max u p + F−1(t)dt.
k k k
p∈∆K 1−p
k=1 k=1 k
By construction for all k ∈ [K], we have
(cid:90) s (cid:90) s
η f(s)+γ g(s) = η G−1(t)dt+γ G−1(t)dt
k k k 1 k 2
0 0
(cid:90) 1−s (cid:90) 1
= − (η G−1+γ G−1)(1−x)dx = − F−1(x)dx,
k 1 k 2 k
1 1−s
where the second equality follows from the variable substitution x ← 1−t and last equality follows
by construction of F . The final representation of f (s) above then allows us to reformulate Φ(u;B)
k k
as
K K
(cid:88) (cid:88)
Φ(u;B) = max u p − (η f(p )+γ g(p )).
k k k k k k
p∈∆K
k=1 k=1
This observation concludes our proof.
Proof of Corollary 5.6. Note first that by Corollary 5.5, for every t ∈ [T] we have Φ(u;B ) =
t
ΦR(u;ψ ), where ψ(p) = (cid:80)K (η f(p )+γ g(p )), f(s) = (cid:82)s G−1(t)dt, and g(s) = (cid:82)s G−1(t)dt.
t k=1 t,k k t,k k 0 1 0√ 2
Moreover, thanks to our choice of G and G , we have that η f(s)+γ g(s) = −η ( s+(s−
1 2 t,k t,k t,k
1)log(1−s)). The claim then follows from [Zimmert et al., 2019, Theorem 3].
20