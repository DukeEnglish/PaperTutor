Can We Break the Curse of Multiagency
in Robust Multi-Agent Reinforcement Learning?
Laixi Shi∗† Jingchu Gai∗‡ Eric Mazumdar† Yuejie Chi§ Adam Wierman†
Caltech PKU Caltech CMU Caltech
October 1, 2024
Abstract
Standard multi-agent reinforcement learning (MARL)algorithms are vulnerable to sim-to-real gaps.
To address this, distributionally robust Markov games (RMGs) have been proposed to enhance robust-
nessin MARLbyoptimizingtheworst-case performance whengame dynamicsshift withinaprescribed
uncertainty set. Solving RMGs remains under-explored, from problem formulation to the development
ofsample-efficient algorithms. Anotorious yet openchallenge is if RMGscan escape thecurseof multi-
agency, where the sample complexity scales exponentially with the number of agents. In this work, we
propose a natural class of RMGs where the uncertainty set of each agent is shaped by both the envi-
ronment and other agents’ strategies in a best-response manner. We first establish the well-posedness
of these RMGs by proving the existence of game-theoretic solutions such as robust Nash equilibria and
coarsecorrelated equilibria(CCE).Assumingaccesstoagenerativemodel, wethenintroduceasample-
efficient algorithm for learning the CCE whose sample complexity scales polynomially with all relevant
parameters. To the best of our knowledge, this is the first algorithm to break the curse of multiagency
for RMGs.
Keywords: multi-agent reinforcement learning, robust Markov games, game theory, distribution shift
Contents
1 Introduction 2
1.1 The curse of multiagency in robust MARL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.3 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Preliminaries 5
2.1 Standard Markov games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.2 Distributionally robust Markov games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3 Robust Markov Games with Fictitious Uncertainty Sets 7
3.1 A novel uncertainty set definition in RMGs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.2 Properties of RMGs with fictitious rectangular uncertainty set . . . . . . . . . . . . . . . . . 9
4 Sample-Efficient Learning: Algorithm and Theory 10
4.1 Problem setting and goal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.2 Algorithm design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.3 Theoretical guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
∗Thefirsttwoauthors contributed equally.
†Department ofComputingMathematicalSciences, CaliforniaInstitute ofTechnology, CA91125, USA.
‡SchoolofMathematicalSciences, PekingUniversity,Beijing,100871, China.
§Department ofElectrical andComputerEngineering, CarnegieMellonUniversity,Pittsburgh, PA15213, USA.
1
4202
peS
03
]GL.sc[
1v76002.9042:viXra5 Conclusion 14
A Preliminaries 19
A.1 Additional matrix and vector notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.2 Preliminary facts about FTRL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B Proof for Section 3 21
B.1 Proof of Theorem 1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
B.2 Proof of auxiliary facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
C Proof of Theorem 2 28
C.1 Proof pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
C.2 Controlling B: adversarialonline learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
C.3 Controlling terms A and C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
C.4 Proof of auxiliary lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
1 Introduction
A flurry of problems naturally involve decision-making among multiple players with strategic objectives.
Multi-agent reinforcement learning (MARL) serves as a powerful framework to address these challenges,
demonstrating potential in various applications such as social dilemmas (Baker, 2020; Leibo et al., 2017;
Zhang et al., 2024), autonomous driving (Lillicrap et al., 2015), robotics (Kober et al., 2013; Rusu et al.,
2017),andgames(Mnih et al.,2015;Vinyals et al.,2019). DespitetherecentsuccessofstandardMARL,its
transition from prototypes to reliable production is hindered by robustness concerns due to the complexity
and variability of both the real-world environment and human behaviors. Specifically, environmental un-
certainty can arise from sim-to-real gaps (Tobin et al., 2017), unexpected disturbance (Pinto et al., 2017),
system noise, and adversarial attacks (Mahmood et al., 2018); agents’ behaviors are subject to unknown
bounded rationality and variability (Tversky and Kahneman, 1974). The solution learned at training time
can fail catastrophically when faced with a slightly shifted MARL problem during testing, resulting in a
significant drop in overall outcomes and each agent’s individual payoff (Balaji et al., 2019; Shi et al., 2024;
Slumbers et al., 2023; Yeh et al., 2021; Zeng et al., 2022; Zhang et al., 2020a).
To address robustness challenges, a promising framework is (distributionally) robust Markov games
(RMGs) (Littman, 1994; Shapley, 1953). It is a robust counterpart to the common playground of stan-
dard MARL problems — Markov games (MGs) (Kardeş et al., 2011; Zhang et al., 2020c). In standard
MGs, agents consider (competitive) personal objectives and simultaneously interact with each other within
a shared unknown environment. The goal is to learn some solution concepts called equilibria, which are
joint strategies/policiesof agents that all of them stick with rationally with other agents fixed; for instance,
Nashequilibria(NE)(Nash,1951;Shapley,1953),correlatedequilibria(CE),andcoarsecorrelatedequilibria
(CCE) (Aumann, 1987; Moulin and Vial, 1978). To promote robustness, RMGs differ from standard MGs
by defining each agent’s payoff (objective) as its worst-case performance when the dynamics of the game
shift within a prescribed uncertainty set centered around a nominal environment.
1.1 The curse of multiagency in robust MARL
Sample efficiency is a crucial metric for MARL due to the limited availability of data relative to the high
dimensionality of the problem. In MARL, agents strive to learn a rationally optimal solution (equilibrium)
through interactions with an unknown environment (Achiam et al., 2023; Silver et al., 2016; Vinyals et al.,
2019). In contemporary applications, the environment is often extremely large-scale,while data acquisition
can be prohibitively limited by high costs and stakes. As such, a notable challenge in terms of scalability
for sample efficiency in MARL is known as the curse of multiagency — the sample complexity requirement
scalesexponentiallywiththenumberofagents(inducedbytheexponentiallygrowingsizeofthejointaction
space). This issue has been recognized and studied in extensive MARL problems, but remains open for
robust MARL. We concentrate on learning finite-horizon multi-player general-sum Markov games with a
2generative model (Kearns and Singh, 1999), where the number of agents is n, the episode length is H, the
size of the state space is S, and the size of the i-th agent’s action space is A , for 1 i n.
i
≤ ≤
• Breaking the curse of multiagency in standard MARL. A line of pioneering work (Bai and Jin, 2020;
Jin et al., 2021; Li et al., 2023; Song et al., 2021) has recently introduced a new suite of algorithms
usingadaptivesamplingthat provablybreakthe curseofmultiagencyinstandardMGs. Inparticular,
to find an ε-approximate CCE, Li et al. (2023) requires a sample complexity no more than
H4S n A
O i=1 i (1)
ε2
(cid:18) P (cid:19)
uptologarithmicfactors,whichdependsoenlyonthesumofindividualactions,ratherthanthenumber
of joint actions.
• The persistent curse of multiagency in robust MARL. The development of provable sample-efficient
algorithms for RMGs is largely underexplored, with only a few recent studies (Blanchet et al., 2023;
Kardeş et al.,2011;Ma et al.,2023;Shi et al.,2024;Zhang et al.,2020c). FocusingonaclassofRMGs
with uncertainty sets satisfying the (s,a)-rectangularity condition, existing works all suffer from the
curse of multiagency, significantly limiting their scalability. For example, using the total variation
(TV) distance as the divergence function, the state-of-the-art (Shi et al., 2024), using non-adaptive
sampling, finds an ε-approximate robust CCE with a sample complexity no more than
H3S n A 1
O i=1 i min H, (2)
ε2 min σ
(cid:18) Q n 1≤i≤n i o(cid:19)
e
up to logarithmic factors, where σ [0,1) is the uncertainty level for the i-th agent. As a result, the
i
∈
sample size requirement becomes prohibitive when the number of agents is large.
Consequently,there is a significantdesire to explore paths that could break throughthe curse of multia-
gencyinRMGs,whichismuchmoreinvolvedthanitsstandardcounterpartduetocomplicatednon-linearity
introducedbyplanningforworst-caseperformances. Nevertheless,thefamilyofRMGsisamuchricherclass
ofproblems because ofthe flexibility in choosingthe uncertainty sets to capture differentrobustdesigncon-
siderations. While convenient, the (s,a)-rectangularity condition prevalent in current approaches can be
overly restricted in practice, as each agent’s uncertainty set is assumed to be independent of other agents’
strategiesandcanbe decoupledintoindependent subsetsfor eachstate-jointactionpair(s,a),suggestingit
might be challenging to break the curse of multiagency in the existing framework. Given these limitations,
we are motivated to develop new classes of RMGs that can provide robust solutions applicable to more
realistic MARL problems with sample-efficient algorithms. This raises an open question:
Can we design RMGs with practically-meaningful uncertainty sets that come with sample complexity
guarantees breaking the curse of multiagency?
1.2 Contributions
We propose a new class of RMGs with a fictitious uncertainty set that explicitly captures uncertainties in
the environmentin view of other agents’strategies,making it suitable for complex real-worldscenarios. We
begin by verifying the game-theoretic properties of the proposed class of RMGs to ensure the existence of
robust variants of well-known standard equilibria notions, robust NE and robust CCE. Next, due to the
generalintractabilityoflearningNE,wefocusondesigningalgorithmsthatcanprovablyovercomethecurse
ofmultiagencyinlearninganapproximaterobustCCE,referringtoajointpolicywherenoagentcanimprove
their benefit by more than ε through rational deviations.. Specifically, for sampling mechanisms to explore
the unknown environment, we assume access to a generative model that can only draw samples from the
nominal environment (Shi et al., 2024). The main contributions are summarized as follows.
• We introduce a new class of robust Markovgames using fictitious uncertainty sets with policy-induced
(s,a )-rectangularity condition (see Section 2.2 for details), which is a natural adaptation from robust
i
3Algorithm Uncertainty set Equilibria Sample complexity
P2MPO
(s,a)-rectangularity robust NE S4( n A )3 H4/ε2
(Blanchet et al., 2024) i=1 i
DR-NVI (s,a)-rectangularity robust NE/CE/CCE SH3 n i=1AiQ min H, 1
(Shi et al., 2024) Qε2 min1≤i≤nσi
n o
Robust-Q-FTRL fictitious robust CCE SH6 1≤i≤nAi min H, 1
(this work) (s,a i)-rectangularity Pε4 min1≤i≤nσi
n o
Table 1: Comparisons between our results and prior art for finding an ε-approximate equilibrium in finite-
horizon multi-agent general-sum robust Markov games. We omit all logarithmic factors in the sample
complexitieshere. Ourresultistheonlycomputationallytractablealgorithmthatprovablybreaksthecurse
of multiagency.
single-agent RL to robust MARL. The uncertainty set for each agent i can be decomposed into inde-
pendent subsets over each state and its own action tuple (s,a ), where each subset is a “ball” around
i
theexpectednominaltransitiondeterminedbyotheragents’policiesandthenominaltransitionkernel,
adistancefunction ρ,andthe radius/uncertaintylevelσ . We verifyseveralessentialfactsofthis class
i
of RMGs: the existence of the desired equilibrium — robust NE and robust CCE for this new class of
RMGsusinggame-theoreticaltoolssuchasfixed-pointtheorem;the existenceofbest-responsepolicies
and robust Bellman equations.
• We consider the total variation (TV) distance as the distance metric ρ for uncertainty sets due to its
popularity in both theory (Blanchet et al., 2023; Panaganti and Kalathil, 2022; Shi et al., 2023, 2024)
and practice (Lee et al., 2021; Pan et al., 2023; Szita et al., 2003). We propose Robust-Q-FTRL that
canprovably find ε-approximaterobustCCE with high probability, as long as the sample size exceeds
SH6 n A 1
O i=1 i min H, (3)
ε4 min σ
(cid:18) P n 1≤i≤n i o(cid:19)
e
uptologarithmicfactors,whereσ (0,1]istheuncertaintylevelforthei-thagent. Tothebestofour
i
∈
knowledge,thisisthefirstalgorithmtobreakthecurseofmultiagencyinsamplecomplexityofRMGs.
It provably finds an ε-approximate robust CCE using a sample size that is polynomial to all salient
parameters. Table1 providesadetailedcomparisonto priorworksinrobustMARL,where ourresults
significantly improve upon prior art (2) (Shi et al., 2024) by reducing the exponential dependency
on the size of each agent’s action space to a linear dependency. To achieve this, we utilize adaptive
sampling and online adversarial learning tools, coupled by a tailored design and analysis for robust
MARL due to the nonlinearity of the robust value function, which contrasts with the linear payoff
functions in standard MARL with respect to the transition kernel.
Notation. In this paper, we denote [T] := 1,2,...,T for any positive integer T > 0. We define ∆( )
{ } S
as the simplex over a set . For any policy π and function Q() defined over a domain , the variance of Q
vu en cd te or rπ thi as tg riv epen reb sey ntV sa vr π a( lS uQ e) s:=
for
Peaa c∈ hB sπ t( aa t) e[ -Q ac(a ti) o−
n
pE aπ i[ rQ
,
] a]2 n. dW x· e =d [e xfi (n se ,ax i)=
]
(s,[ ax i( )s ∈, Sa ×) A] ( is,a ∈)B ∈ RS S× AA i∈ asR aS nA ya vs eca tn oy
r
representing agent-wise state-action values. Similarly, we denote x = [x(s)] as any vector representing
s∈S
values foreachstate. For :=( , A ,H, σ ,1,1), letf( )=O(g( )) denote that thereexists
X S { i }i∈[n] { i }i∈[n] ε δ X X
a universal constant C >0 such that f C g. Furthermore, the notation O() is defined similarly to O()
1 1
≤ · ·
but hides logarithmic factors.
e
1.3 Related work
Breaking curse of multiagency for standard Markov games. Breaking the curse of multia-
gency is a major and prevalent challenge in sequential games. In standard multi-agent general-sum
4MGs, it has been shown that learning a Nash equilibrium requires an exponential sample complex-
ity (Bai and Jin, 2020; Rubinstein, 2017; Song et al., 2021). However, for other types of equilibria,
such as CE and CCE, many works have successfully broken the curse of multiagency. Specifically,
for finite-horizon general-sum MGs in the tabular setting with finite state and action spaces, Jin et al.
(2021) developed the V-learning algorithm for learning CE and CCE with the sample complexity of
O(H6S(max A )2/ǫ2)andO(H6Smax A /ǫ2),respectively;Daskalakis et al.(2023)achievedasam-
i∈[n] i i∈[n] i
ple complexity of O(H11S3max A /ǫ3) for learning a CCE. Beyondtabular settings, Wang et al.(2023)
i∈[n] i
aendCui et al.(2023)extendedetheseresultstolinearfunctionapproximation,achievingsamplecomplexities
of O(d4H6 max e A5 /ǫ2) and O(H10d4log max A /ǫ4), respectively, where d is the dimension of
i∈[n] i i∈[n] i
thelinearfeatures. ForMarkovpotentialgames,asubclassofMGs,Song et al.(2021)providedacentralized
(cid:0) (cid:1) (cid:0) (cid:1)
algoerithm that learns a NE with a esample complexity of O(H4S2max A /ǫ3).
i∈[n] i
Finite-sample analysis for distributionally robusteMarkov games. Robust Markov games under
environmental uncertainty are largely underexplored, with only a few provable algorithms (Blanchet et al.,
2023;Kardeş et al.,2011;Ma et al.,2023;Shi et al.,2024;Zhang et al.,2020a). Existingsample complexity
analyses all suffer from the daunting curse of multiagency issues, or impose an extremely restricted uncer-
tainty level that can fail to deliver the desired robustness (Blanchet et al., 2024; Ma et al., 2023; Shi et al.,
2024). Specifically, they all consider a class of RMGs with the (s,a)-rectangularity condition, where the
uncertainty sets for each agent can be decomposed into independent sets over each (s,a) pair. Shi et al.
(2024)consideredthegenerativemodelwithanuncertaintysetmeasuredbytheTVdistance,Blanchet et al.
(2023)treatedadifferentsamplingmechanismwithofflinedataforboththeTVdistanceandKLdivergence.
In addition, Ma et al. (2023) required the uncertainty level be much smaller than the accuracy-leveland an
instance-dependent parameter (i.e., σ max ε ,pmin for all i [n]). This can thus fail to maintain the
i ≤ {SH2 H } ∈
desired robustness, especially when the accuracy requirement is high (i.e., ε 0) or the RMG has small
→
minimal positive transition probabilities (i.e., pmin 0).
→
Robust MARL. Standard MARL algorithms may overfit the training environment and could fail dra-
matically due to the perturbations and variability of both agents’ behaviors and the shared environment,
leading to performance drop and large deviation from the equilibrium. To address this, this work consid-
ers a robust variant of MARL adopting the distributionally robust optimization (DRO) framework that
has primarily been investigated in supervised learning (Bertsimas et al., 2018; Blanchet and Murthy, 2019;
Duchi and Namkoong,2018;Gao,2020;Rahimian and Mehrotra,2019)andhasattractedalotofattentionin
promotingrobustnessinsingle-agentRL(Badrinath and Kalathil,2021;Iyengar,2005;Nilim and El Ghaoui,
2005;Shi and Chi,2024;Shi et al.,2023;Wang et al.,2024;Zhou et al.,2021). BeyondtheRMGframework
consideredin this work,recentresearchhas advancedthe robustness ofMARL algorithmsfromvarious per-
spectives, including resilience to uncertainties or attacks on states (Han et al., 2022; Zhou and Liu, 2023),
the type of agents (Zhang et al., 2021), other agents’ policies (Kannan et al., 2023; Li et al., 2019), offline
datapoisoning(McMahan et al.,2024;Wu et al.,2024),andnonstationaryenvironment(Szita et al.,2003).
A recent review can be found in Vial et al. (2022).
2 Preliminaries
In this section, we begin with some backgroundon multi-agent general-sumstandard Markovgames (MGs)
in finite-horizonsettings, followedby a generalframeworkof a robustvariantof standardMGs —- distribu-
tionally robust Markov games.
2.1 Standard Markov games
A finite-horizon multi-agent general-sum Markov game (MG) is characterized by the tuple
= , ,P,r,H .
i 1≤i≤n
MG S {A }
Thissetupfeaturesnagentseachstrivingtom(cid:8)aximizetheirindividua(cid:9)llong-termcumulativerewardswithina
sharedenvironment. Ateachtimestep,allagentsobservethesamestateoverthestatespace = 1, ,S
S { ··· }
5within the shared environment. For each agent i (i [n]), = 1, ,A denotes its action space
i i
∈ A { ··· }
containing A possible actions. The joint action space for all agents (resp. the subset excluding the i-th
i
agent) is defined as := (resp. := for any i [n]). We use the notation
a (resp. a A )A t1 o× de· n· o· t× e A an joint actioA n− pi rofile ij n6= vi oA lvj ing all agen∈ ts (resp. all except the i-th
−i −i
∈ A ∈ A Q
agent). In addition, the probability transition kernel P = P , with each P : ∆( ),
h 1≤h≤H h
describes the dynamics of the game: P (s′ s,a) is the pro{ babi} lity of transitioning fromS s× tatA e s7→ S to
h
state s′ at time step h when agents choo| se the joint action profile a . The reward function∈ ofS the
∈ S ∈ A
game is r = r , with each r : [0,1] normalized to the unit interval. For any
i,h 1≤i≤n,1≤h≤H i,h
(i,h,s,a) [n{ ] [} H] , r (s,a) representsS th× eA im7→ mediate rewardreceivedby the i-th agentin state
i,h
s when the∈ join× t actio× nS pr× ofiA le a is taken. Last but not least, H >0 represents the horizon length.
Markov policies and value functions. In this work,we concentrateon Markovpolicies that the action
selection rule depends only on the current state s, independent from previous trajectory. Namely, the i-th
(i [n]) agent chooses actions according to π = π : ∆( ) . Here, π (a s) represents the
i i,h i 1≤h≤H i,h
∈ { S 7→ A } |
probabilityofselectingactiona instatesattime steph. Assuch,the jointMarkovpolicyofallagents
i
∈A
can be denoted as π = (π ,...,π ) : [H] ∆( ), i.e., given any s and h [H], the joint action
1 n
profile a of all agents is chosen foS llo× wing t7→ he disA tribution π ( s)=(∈ π S ,π ..∈ .,π )( s) ∆( ).
h 1,h 2,h n,h
∈A ·| ·| ∈ A
To continue, for any given joint policy π and transition kernel P of a , the i-th agent’s long-term
cumulativerewardcanbecharacterizedbythevaluefunctionVπ,P : R(M resG p.Q-functionQπ,P :
R) as below: for all (h,s,a) [H] , i,h S 7→ i,h S×A7→
∈ ×S×A
H H
Vπ,P(s):=E r s ,a s =s , Qπ,P(s,a):=E r s ,a s =s,a =a . (4)
i,h π,P " i,t t t | h # i,h π,P " i,t t t | h h #
t=h t=h
X (cid:0) (cid:1) X (cid:0) (cid:1)
In this context, the expectation is calculated over the trajectory (s ,a ) produced by following the
t t h≤t≤H
{ }
joint policy π under the transition kernel P.
2.2 Distributionally robust Markov games
A general distributionally robust Markov game (RMG) is represented by the tuple
= , , σi(P0) ,r,H .
RMG S {Ai }1≤i≤n {Uρ }1≤i≤n
Here, , ,r,H are definedin (cid:8)the samemanner as those instandard(cid:9)MGs (see Section2.1). RMGs
i 1≤i≤n
S {A }
differfromstandardMGs: foreachagenti(1 i n),the transitionkernelisnotfixedbutcanvarywithin
≤ ≤
its own prescribed uncertainty set σi(P0) centered around some nominal kernel P0 : ∆( ) that
Uρ S ×A 7→ S
represents a reference (such as the training environment). The shape and the size of the uncertainty set
Uρσi(P0)
i∈[n]
are further specified by a divergence function ρ and the uncertainty levels {σ
i
}i∈[n], serving
as the “distance” metric and the radius respectively.
(cid:8) (cid:9)
Various choices of the divergence function have been considered in the literature of robust RL, including
butnotlimitedtof-divergence(suchastotalvariation,χ2divergence,andKullback-Leibler(KL)divergence)
(Lu et al., 2024; Shi and Chi, 2024; Wang et al., 2024; Yang et al., 2022; Zhou et al., 2021) andWasserstein
distance (Xu et al., 2023). Adopting uncertainty sets with different structures leads to distinct RMGs, as
they address distinct types of uncertainty and game-theoretical solutions. This paper focuses on variability
in environmental dynamics (transition kernels), though uncertainty in agents’ reward functions could also
be considered similarly but is omitted for brevity.
Robust value functions and best-response policies. For any RMG, eachagentseeksto maximize its
worst-case performance in the presence of other agents’ behaviors despite perturbations in the environment
dynamics,aslongasthekerneltransitionsremainwithinitsprescribeduncertaintyset σi(P0). Mathemat-
Uρ
ically,givenany jointpolicy π : [H] ∆( ), the worst-caseperformanceofany agenti is characterized
by the robust value function Vπ,Sσi× and t7→ he robA ust Q-function Qπ,σi: for all (i,h,s,a ) [n] [H] ,
i,h i,h i ∈ × ×S×Ai
Vπ,σi(s):= inf Vπ,P(s) and Qπ,σi(s,a ):= inf Qπ,P(s,a ). (5)
i,h P∈Uρσi(P0) i,h i,h i P∈Uρσi(P0) i,h i
6Notethatdifferentfrom(4),heretheQ-functionforanyi-thagentisdefinedonlyoveritsownactiona
i i
rather than the joint action a . ∈A
∈A
To continue, we denote π as the policy for all agents except for the i-th agent. By optimizing the i-th
−i
agent’s policy π′ : [H] ∆( ) (independent from π ), we define the maximum of the robust value
i S × → Ai −i
function as
V⋆,π−i,σi(s):=
max
Vπ i′×π−i,σi(s)=
max inf
Vπ i′×π−i,P
(s) (6)
i,h π i′:S×[H]7→∆(Ai) i,h π i′:S×[H]7→∆(Ai)P∈Uρσi(P0) i,h
for all (i,h,s) [n] [H] . The policy that achieves the maximum of the robust value function for all
∈ × ×S
(i,h,s) [n] [H] is called a robust best-response policy.
∈ × ×S
Solution concepts for robust Markov games. In view of the conflicting objectives between agents,
establishing equilibrium becomes the goal of solving RMGs. As such, we introduce two kinds of solution
concepts — robust NE and robust CCE — robust variants of standard NE and CCE (usually considered in
standard MGs) specified to the form of RMGs.
• Robust NE. A productpolicy π =π π π : [H] n ∆( ) is saidto be a robust NE
1 × 2 ×···× n S× 7→ i=1 Ai
if
Vπ,σi(s)=V⋆,π−i,σi(s),
(s,i)
Q
[n]. (7)
i,1 i,1 ∀ ∈S×
Given the strategies of the other agents π , when each agent wants to optimize its worst-case per-
−i
formance when the environment and other agents’policy stay within its ownuncertainty set σi(P0),
Uρ
robust NE means that no player can benefit by unilaterally diverging from its present strategy.
• RobustCCE.Adistributionoverthejointproductpolicyξ := ξ :[H] ∆( ∆( ))
{ h }h∈[H] 7→ S 7→ i∈[n] Ai
is said to be a robust CCE if it holds that
Q
E Vπ,σi(s) E V⋆,π−i,σi(s) , (i,s) [n] . (8)
π∼ξ i,1 ≥ π∼ξ i,1 ∀ ∈ ×S
Considering all agents follow(cid:2) the pol(cid:3)icy drawn(cid:2) from the d(cid:3)istribution ξ, i.e., π ξ for all h [H],
h h
∼ ∈
when the distribution of all agents but the i-th agent’s policy is fixed as the marginal distribution of
ξ, robust CCE indicates that no agent can benefit from deviating from its current policy.
Notethat,forstandardMGs,CCEisdefinedasapossiblycorrelatedjointpolicyπCCE : [H] ∆( )
S× 7→ A
(Aumann, 1987; Moulin and Vial, 1978) if it holds that
VπCCE,P(s)
max
Vπ i′×π −CC iE,P
(s), (s,i) [n]. (9)
i,1 ≥π i′:S×[H]→∆(Ai) i,1 ∀ ∈S×
This correlated policy πCCE can also be viewed as a distribution ξ over the product policy space since each
joint action a can be seen as a deterministic product policy. Careful readers may note that the definition
(9) of CCE in standard MGs is in a different form from the one (8) in RMGs, as the latter does not include
the expectation operator E [] with respect to the policy distribution (ξ) over the value function. We
π∼ξ
·
emphasize that the definition with the expectation operator outside of the value (or cost) function with
respect to a distribution of product pure strategies in (8) is a natural formulation originating from game
theory(Moulin et al.,2014;Moulin and Vial,1978). InstandardMARLandpreviousrobustMARLstudies,
the definition in(9) is typicallyusedbecause (9)and(8)areidenticalin thosesituations,as the expectation
operator and the corresponding value functions are linear with respect to the joint policy, allowing them to
be interchanged (Li et al., 2023; Shi et al., 2024).
3 Robust Markov Games with Fictitious Uncertainty Sets
Given the definition of general RMGs, a natural question arises: what kinds of uncertainty sets should we
consider to achieve the desired robustness in our solutions? To address this, we focus on a specific class of
RMGs characterizedby a type of natural yet powerful uncertainty sets.
73.1 A novel uncertainty set definition in RMGs
Weproposeanewclassofuncertaintysets,namedfictitiousuncertaintysets,whichcountintheuncertainty
induced by both the environment and agents’ behaviors in a correlated manner. Before introducing the
uncertainty sets, we provide some auxiliary notations as below. We denote a vector of any transition kernel
P : ∆( ) or P0 : ∆( ) respectively as
S×A7→ S S×A7→ S
∀(s,a) ∈S×A: P h,s,a :=P h( ·|s,a) ∈R1×S, P h0 ,s,a :=P h0( ·|s,a) ∈R1×S. (10)
For any (possibly correlated) joint Markov policy (defined in section 2.1) π : [H] ∆( ), we define
S × 7→ A
the expected nominal transitionkernel conditioned on the situation that the i-th agent chooses some action
a and other agents play according to the conditional policy (i.e., a π ( s,a )) given s and
i i −i h i
∈ A ∼ ·| ∈ S
a as below: for each time step h [H],
i
∈
π (a ,a s)
∀(h,s,a i) ∈[H] ×S×Ai : P hπ ,− s,i ai =E a∼πh(·|s,ai) (cid:2)P h0 ,s,a (cid:3)=
a −Xi∈A−i
h π i,hi (a i− |i s| ) (cid:2)P h0 ,s,a (cid:3). (11)
Armed with the above definitions, now we are in a position to define the fictitious uncertainty sets,
denoted as Uρσi(P0, ·) i∈[n], which satisfy a policy-induced (s,a i)-rectangularity condition.
Definition(cid:8)1. For any(cid:9)joint policy π : [H] ∆( ), divergence function ρ : ∆( ) ∆( ) R+ and
S × 7→ A S × S 7→
accessible uncertainty levels σ
i
≥0 for all i ∈[n], the fictitious uncertainty sets Uρσi(P0,π)
i∈[n]
satisfy the
policy-induced (s,a )-rectangularity condition:
i (cid:8) (cid:9)
i [n]: σi(P0,π):= σi Pπ−i ,
∀ ∈ Uρ ⊗Uρ h,s,ai
s.t. (h,s,a ) [H] : σi Pπ−i := P ∆(cid:16) ( ):ρ(cid:17) P,Pπ−i σ , (12)
∀ i ∈ ×S×Ai Uρ h,s,ai ∈ S h,s,ai ≤ i
(cid:16) (cid:17) n (cid:16) (cid:17) o
where represents the Cartesian product.
⊗
In words, conditioned on a fixed joint policy π, the uncertainty set σi(P0,π) for each i-th agent can
Uρ
be decomposed into a Cartesian product of subsets over each state and agent-action pair (s,a ). Each
i
uncertainty subset Uρσi(P hπ ,− s,i ai) over (s,a i) is defined as a “ball” arounda reference — the expected nominal
transition kernel
Pπ−i
conditioned on both transition kernel and agents’ behavior π.
h,s,ai
Further discussions of fictitious uncertainty sets. It is in order to remark on the proposed type of
uncertainty sets, in comparison with prior works.
• A natural adaptation from single-agent robust RL.Whenagentsfollowsomejointpolicy π : [H]
S× 7→
∆( ), fixing other agents’policy π , from the perspective of eachindividual agenti, RMGs with our
−i
A
policy-induced (s,a )-rectangularity condition will degrade to a single-agent robust RL problem with
i
thewidelyused(s,a )-rectangularityconditioninthesingle-agentliterature(Iyengar,2005;Zhou et al.,
i
2021). Namely, from any agent i’s viewpoint, in a RMG, it has an "overall environment" player that
can not only manipulate the environmental dynamics but also other players’policy π .
−i
• Allowing uncertainty from both the environment and agents’ behaviors in a correlated manner. One
essential feature of our proposed uncertainty set is that it is shaped by both the environment and
agents’ strategies in a (possibly) correlatedmanner. Specifically, for any agent i and a given policy π,
any uncertainty subset Uρσi P hπ ,− s,i
ai
(over any (h,s,a i)) is constructed as a neighborhood around a
nominal center Pπ−i (see ((cid:16)11)) tha(cid:17)t depends on both the nominal environment P0 and other agents’
h,s,ai
conditional strategies π ( s,a ).
h i
·|
• Comparisons to prior works. Prior works on provable sample-efficient algorithms have focused on a
different type of uncertainty sets with (s,a)-rectangularity condition (Blanchet et al., 2023; Ma et al.,
2023; Shi et al., 2024). This class of uncertainty sets decouples the uncertainty into independent
subsets for each state-joint action pair (s,a), accounting for the uncertainty of the environment and
agents’ strategies independently. In comparison, the proposed uncertainty set lifts this independence
assumption across subsets over different (s,a ,a ) for any a , enabling the environment and
i −i −i i
∈ A
agents’ strategies to shape the uncertainty set in a correlated manner.
83.2 Properties of RMGs with fictitious rectangular uncertainty set
Throughout the paper, we focus on the class of RMGs with the above proposed fictitious uncertainty sets,
represented as
= , , σi(P0, ) ,r,H
RMGπ S {Ai }1≤i≤n {Uρ · }1≤i≤n
andabbreviatedasfictitiousRMGsinth(cid:8)eremainingofthepaper. Inthissecti(cid:9)on,wepresentkeyfactsabout
fictitiousRMGsrelatedtobest-responsepolicies,equilibria,andthecorrespondingone-steplookaheadrobust
Bellman equations. The proofs are postponed to Appendix B.
First,weintroducethefollowinglemma,whichverifiestheexistenceofarobustbest-responsepolicythat
achieves the maximum robust value function (cf. (6)).
Lemma1. Foranyi [n],givenπ : [H] ∆( ),thereexistsatleastonepolicyπ : [H] ∆( )
−i i i i
∈ S× 7→ A S× → A
for the i-th agent that can simultaneously attain
Vπi×π−i,σi(s)=V⋆,π−i,σi(s)
for all s and h [H]. We
i,h i,h ∈S ∈
refer this policy as the robust best-response policy.e e
Existence of robust NE and robust CCE. fictitious RMGs can be viewed as hierarchicalgames with
n+nS n A agents. This includes the original n agents and n additional sets of S n A independent
i=1 i i=1 i
adversaries, each determining the worst-case transitions for one agent over a state plus agent-wise-action
P P
pair. Considering the solution concepts — robust NE and robust CCE — introduced in Section 2.2, the
followingtheoremverifiestheexistenceofthemforanyfictitiousRMGsusingKakutani’sfixed-pointtheorem
(Kakutani, 1941), focusing on robust NE firstly.
Theorem 1 (Existence of robust NE). For any
RMGπ
= S,
{Ai
}1≤i≤n, {Uρσi(P0, ·) }1≤i≤n, r,H with an
uncertainty set defined in Definition 1, there exists at least one robust NE.
(cid:8) (cid:9)
Analogous to standard Markov games, since robust NE robust CCE , Theorem 1 indicates the
{ } ⊆ { }
existence of robust CCEs directly.
Robust Bellman equations. Fortunately, the class of fictitious RMGs feature a robust counterpart of
the Bellman equation — robust Bellman equation. Specifically, for any joint policy π : [H] ∆( ), the
S× 7→ A
robust value function can be expressed as
H
V iπ ,h,σi(s)= Uρσii (n Pf 0,π)E
"
Xt=hr i(s t,a t) |s
h
=s #=E a∼πh(s)[r i,h(s,a)]+E
ai∼πi,h(s)
(cid:20)Uρσi
(cid:16)i Pn hπf
,− s,i
ai(cid:17)PV iπ ,h,σ +i
1
(cid:21).
(13)
It can be verified directly by definition. The robust Bellman equation described above is intrinsically linked
to the policy-induced (s,a )-rectangularity condition (cf. (12)) of the uncertainty set. This condition leads
i
to a well-posedandcomputationally-tractableclassofRMGs by allowingthe decompositionfromanoverall
uncertainty set to independent subsets across different agents, time steps, and each state-action pair (s,a ).
i
Note that the specified robust Bellman equation is different for a joint correlated policy and a joint
productpolicy,inducedbydifferentexpectednominaltransitionkernels. Inparticular,foranyjointproduct
policy π : [H] ∆( ), the expected nominal transition kernel conditioned on the i-th agent’s
S × 7→ i∈[n] Ai
action a , current state s , and the policy π can be expressed by
i i
∈A Q ∈S
P hπ ,− s,i ai =E a∼πh(·|s,ai) P h0 ,s,a =E a −i∼π−i,h(·|s) P h0 ,s,(ai,a −i) (14)
(cid:2) (cid:3) h i
for any (i,h,s,a ) [n] [H] , where the last equality holds since the policy π is a product policy,
i i
and the
distributio∈
n of
a× is× inS de× peA
ndent of a . It is observed that the expected nominal transition kernel
−i i
Pπ−i
for a product policy π is independent of the i-th agent’s policy given (s,a ). This differs from (11)
h,s,ai i
for a possibly correlated policy, where (11) can generally depend on the i-th agent’s policy.
The robust Bellman equation described above is intrinsically linked to the policy-induced (s,a )-
i
rectangularity condition (cf. (12)) of the uncertainty set. This condition leads to a well-posed and
computationally-tractable class of RMGs by allowing the decomposition from an overall uncertainty set
to independent subsets across different agents, time steps, and each state-action pair (s,a ).
i
94 Sample-Efficient Learning: Algorithm and Theory
In this section, we focus on designing sample-efficient algorithms for solving fictitious RMGs when agents
need to collect data by interacting with the unknown shared environment in order to learn the equilibria.
To proceed, we shall first specify the data collection mechanism and the divergence function for the uncer-
tainty set. Then we propose a sample-efficient algorithm Robust-Q-FTRLthat leveragesa carefully-designed
adaptive sampling strategy to break the curse of multiagency.
4.1 Problem setting and goal
Recall that the uncertainty sets are constructed by specifying a divergence function ρ and the uncertainty
leveltocontrolitsshapeandsize. Inthiswork,wefocusonusingtheTVdistanceasthedivergencefunction
ρ for the uncertainty set, following Lee et al. (2021); Pan et al. (2023); Shi et al. (2023, 2024); Szita et al.
(2003), defined by
1
P,P′ ∆( ): ρ (P,P′):= P P′ . (15)
∀ ∈ S TV 2k − k1
For convenience, throughout the paper, we abbreviate σi():= σi () when there is no ambiguity.
U · UρTV ·
Data collection mechanism: a generative model. We assume the agents interact with the environ-
ment through a generative model (simulator) (Kearns and Singh, 1999), which is a widely used sampling
mechanism in both single-agent RL and MARL (Li et al., 2022; Zhang et al., 2020b). Specifically, at any
timesteph,wecancollectanarbitrarynumberofindependentsamplesfromanystateandjointactiontuple
(s,a) , generated based on the true nominal transition kernel P0:
∈S×A
si i.i.d P0( s,a), i=1,2,... (16)
h,s,a ∼ h ·|
Goal. Consider any fictitious RMGs
π
= ,
i
1≤i≤n, σi(P0) 1≤i≤n,r,H . While learning
RMG S {A } {U }
exact robust equilibria is computationally challenging and may not be necessary in practice, instead in
(cid:8) (cid:9)
this work, we focus on finding an approximate robust CCE (defined in (8)). Namely, a distribution ξ :=
ξ :[H] ∆( ∆( )) is said to be an ε-robust CCE if
{ h }h∈[H] 7→ S 7→ i∈[n] Ai
gap Q (ξ):= max E V⋆,π−i,σi(s) E Vπ,σi(s) ε. (17)
CCE s∈S,1≤i≤n π∼ξ i,1 − π∼ξ i,1 ≤
(cid:8) (cid:2) (cid:3) (cid:2) (cid:3)(cid:9)
Armed with a generative model of the nominal environment, the goalis to learn a robust CCE using as few
samples as possible.
4.2 Algorithm design
With the sampling mechanism over a generative model in hand, we propose an algorithm called Robust-Q-
FTRL to learn an ε-robust CCE in a sample-efficient manner, summarized in Algorithm 2 in the appendix.
Robust-Q-FTRL draws inspiration from Q-FTRL developed in the standard MG literature (Li et al., 2022),
but empowers tailored designs for learning in fictitious RMGs to achieve a robust equilibrium and to tackle
statistical challenges arising from agents’ nonlinear objectives. Overall, Robust-Q-FTRL takes a single pass
to learn recursively from the final time step h=H to h=1. At each time step h [H], an online learning
∈
process with K iterations will be executed. Before introducing the algorithm, we first concentrate on two
essential steps customized for learning in fictitious RMGs.
Constructing the empirical model via N-sample estimation. For each time step h, we denote πk
i,h
as the current learning policy of the i-th agent before the beginning of the k-th iteration for any k [K].
∈
And we denote the joint product policy as πk = (πk , ,πk ). During each iteration k, for each agent
h 1,h ··· n,h
i [n], we require to generate N independent samples from the generative model over each (s,a )
i i
∈ ∈S×A
to obtain an empirical model, detailed in Algorithm 1. It includes an empirical rewardfunction represented
10Algorithm 1: N-sample estimation π = π ,i,h .
h j,h j∈[n]
{ }
1 Initialization: the reward r =0 R (cid:0)SAi and the transition(cid:1) model P =0 RSAi×S.
∈ ∈
2 for (s,a ) do
i i
∈S×A
3 for t=1 to N do b b
4 Sample at(s,a )=[a (s,a )] constructed by independent actions drawn from policy:
i j i 1≤j≤n
ind.
a (s,a ) π ( s) (j =i) and a (s,a )=a . (18)
j i j,h i i i
∼ ·| 6
5 Sample from the generative model:
rt (s,a )=r (s,at(s,a )), st P s,at(s,a ) . (19)
i,h i i,h i s,ai ∼ h · | i
(cid:0) (cid:1)
6 Set r(s,a )= 1 rt (s,a ) and P s′ s,a = 1 1 st =s′ .
i N t∈[N] i,h i | i N t∈[N] s,ai
7 Return: empirical mPodel r,P . (cid:0) (cid:1) P (cid:8) (cid:9)
b b
(cid:0) (cid:1)
b b
byrk RSAi andtransitionkernelsdenotedbyPk RSAi×S. NotethatdifferentfromstandardMGs,we
i,h ∈ i,h ∈
need to generate N samples instead of 1 sample per iteration to handle the additional statistical challenges
induced by the non-linear objective of agents (N will be specified momentarily).
Estimating robust Q-function of the current policy πk. We denote V RS as the estimation of
h i,h ∈
thei-thagent’srobustvaluefunctionattimesteph. Foranyagenti,withtheempiricalrewardfunctionrk ,
i,h
empirical kernel Pk , and the estimated robust value function V at thebnext step in hand, the robust
i,h i,h+1
Q-function qk of current policy πk can be estimated as:
{ i,h} h
b
(i,h,s,a ) [n] [H] A : qk (s,a )=rk (s,a )+ inf V . (20)
∀ i ∈ × ×S× i i,h i i,h i P∈Uσi(Pk )P i,h+1
i,h,s,ai
b
Unlikethelinearfunctionw.r.t. Pk instandardMGs,(20)lacksaclosedformandintroducesanadditional
i,h
inneroptimizationproblem. Solving(20)directlyiscomputationallychallengingduetotheneedtooptimize
over an S-dimensional probability simplex, with complexity growing exponentially with the state space size
S. Fortunately,byapplyingstrongduality,wecansolve(20)equivalentlyviaitsdualproblemwithtractable
computation (Iyengar, 2005):
qk (s,a )=rk (s,a )+ max Pk V σ α min V (s′) , (21)
i,h i i,h i α∈[minsVi,h+1(s),maxsVi,h+1(s)]
n
i,h
h
i,h+1 iα− i
(cid:16)
− s′
h
i,h+1 iα
(cid:17)o
b b b b
where [V] denotes the clipped version of any vector V RS determined by some level α 0, namely,
α
∈ ≥
α, if V(s)>α,
[V] (s):= (22)
α
(V(s), otherwise.
The above modules are key components of Robust-Q-FTRL, serving for constructing nonlinear robust
objectives in the online learning process and ensuring the desired statistical accuracy.
Overall pipeline of Robust-Q-FTRL. With these technical modules in place, we introduce Robust-Q-
FTRL, which follows a similar online learning procedure as Q-FTRL for standard MGs (Li et al., 2022).
The complete procedure is summarized in Algorithm 2. We denote Qk RSAi as the estimated robust
i,h ∈
Q-function of the equilibrium for the i-th agent at the k-th iteration of time step h. To begin with, Robust-
Q-FTRL initialize the robust value function, robust Q-function V (s) = Q0 (s,a ) = 0, and the policy
i,H+1 i,h i
π1 (a s)=1/A for all i [n]. Then subsequently from the final time step h= H to h= 1, for each step
i,h i | i ∈
h, a K iterations online learning process will be executed. At eabch k-th iteration, given current policy πk,
h
11as described above, an empirical model ( rk and Pk ) is constructed by N-sample estimation
{
i,h}i∈[n]
{
i,h}i∈[n]
(cf. algorithm 1). Then the robust Q-function qk of the current policy πk is estimated by (21).
{ i,h}i∈[n] h
Now we are ready to specify the loss objective and proceed the online learning procedure. With the
currentone-stepupdate qk , we update the Q-estimateas Qk =(1 α )Qk−1+α qk . Here, α
{ i,h} i,h − k i,h k i,h { k }k∈[K]
is a series of rescaled linear learning rates with some c 24,
α
≥
c logK α n (1 α ), if 0<k<n K
k [K]: α = α and αn = k i=k+1 − i ≤ . (23)
∀ ∈ k k −1+c αlogK k (α
nQ
if k =n
LettheQ-estimatebetheonlinelearninglossobjectiveatthismoment,weapplytheFollow-the-Regularized-
Leader strategy (Li et al., 2022; Shalev-Shwartz, 2012) to update the corresponding policy as below:
πk+1(a s)=
exp η k+1Qk i,h(s,a i)
with η =
logK
, k =1,2,...
i,h i | exp η Qk (s,a′) k+1 α H
a′ (cid:0) k+1 i,h (cid:1) r k
This is a widely used adapPtive sam(cid:0)pling and learni(cid:1)ng procedure for MARL problems.
After completing K iterationsfortime steph, wefinalize the robustvalue function estimationby setting
it to its confidence upper bound, incorporating carefully designed optimistic bonus terms β as: for all
i,h
{ }
(i,h,s) [n] [H] ,
∈ × ×S
log3(KS n i=1Ai) K
β (s)=c Pδ αK Var qk (s, ) +H , (24)
i,h bs KH k π ik ,h(·|s) i,h ·
Xk=1 n (cid:0) (cid:1) o
where c denotes some absolute constant, δ (0,1) is the high probability threshold, Finally, after the
b
∈
recursive learning process ends for all time steps h = H,H 1, ,1, we output a distribution of product
− ···
policyξ = ξ overallthepolicies πk =(πk πk ) occursduring theprocessthat
{ h }h∈[H] { h 1,h×···× n,h }h∈[H],k∈[K]
defined as
b b
(h,k) [H] [K]: ξ (πk):=α . (25)
∀ ∈ × h h k
4.3 Theoretical guarantees
In this section, we provide the theoretical guarantees for the sample complexity of our proposed algorithm
Robust-Q-FTRL, shown as below:
Theorem 2 (Upper bound). Using the TV uncertainty set defined in (15). Consider any δ (0,1) and any
∈
fictitious RMGs
π
= ,
i
1≤i≤n, σi(P0, ) 1≤i≤n,r,H with σ
i
(0,1] for all i [n]. For any
RMG S {A } {U · } ∈ ∈
ε min H, 1 , Algorithm 2 can output an ε-robust CCE ξ, i.e.,
≤ min1≤i≤nσi (cid:8) (cid:9)
q (cid:8) (cid:9)
gap (ξ):= max E V⋆,π−i,σi(s) E b Vπ,σi(s) ε
CCE s∈S,1≤i≤n π∼ξ i,1 − π∼ξ i,1 ≤
n b(cid:2) (cid:3) b(cid:2) (cid:3)o
with probability at least 1 bδ, as long as
−
C H2 1 C H3
1 1
N min ,H , K . (27)
≥ ǫ2 min σ ≥ ǫ2
(cid:26) 1≤i≤n i (cid:27)
Here C is some universal large enough constant. Namely, it is sufficient if the total number of samples
1
acquired in the learning process obeys
N :=HKNS A
(C 1)2H6S 1≤i≤nA i
min H,
1
.
all i ≥ ε4 min σ
1≤ Xi≤n P n 1≤i≤n i o
Before we jump into more discussions of the above theorem, in addition, we introduce the information-
theoretic minimax lower bound for this problem as well.
12Algorithm 2: Robust-Q-FTRL
1 Input: learning rates α and η , number of iterations K per time step, and number of
k k+1
{ } { }
samples N per iteration.
2 Initialization: V (s)=Q0 (s,a )=0 and π1 (a s)=1/A for all i [n] and then all
i,H+1 i,h i i,h i | i ∈
(h,s,a ) [H] .
i i
∈ ×S×A
// start recubrsive learning process.
3 for h=H,H 1, ,1 do
− ···
4 for k =1,2, ,K do
···
5 for i=1,2, ,n do
···
// construct empirical models and estimate current robust Q-function
6 rk ,Pk N-sample estimation πk = πk ,i,h . (Algorithm 1)
i,h i,h ← h { j,h}j∈[n]
7 Estimate the robust Q-function qk of current πk according to (21).
(cid:0) (cid:1) i,h(cid:0) h (cid:1)
// Online learning procedure
8 Update the Q-estimate Qk =(1 α )Qk−1+α qk and apply FTRL:
i,h − k i,h k i,h
(s,a ) : πk+1(a s)= exp ηk+1Qk i,h(s,ai) .
∀ i ∈S×Ai i,h i | a′ex (cid:0)p ηk+1Qk i,h(s,a (cid:1)′)
P
// set the final robust value estimate at t(cid:0)ime step h(cid:1).
9 for i=1,2, ,n do
···
10 For all s : set β (s) to be the optimistic bonus term in (24) and
i,h
∈S
K
V (s)=min αK πk ( s), qk (s, ) +β (s), H h+1 , (26)
i,h k i,h ·| i,h · i,h −
nXk=1 (cid:10) (cid:11) o
b
11 Output: a set of policies πk =(πk πk ) and a distribution ξ = ξ over
{ h 1,h×···× n,h }k∈[K],h∈[H] { h }h∈[H]
them. For any time step h, ξ is the distribution over πk so that ξ (πk)=αK.
h { h}k∈[K] h h b k b
b b
Lower bound for learning in fictitious RMGs. Considering the instances of fictitious RMGs that
the action space for all the agents except the i-th agent contains only a single action, i.e., A = 1 for all
j
j = i. As such, all the agents j = i will take a fixed action and the game reduces to a single-agent robust
6 6
MDP with (s,a)-rectangularity condition (Zhou et al., 2021). So the goal of finding the robust equilibrium
— robust NE/CCE also degrades to finding the optimal policy of the i-th agent. Invoking the results from
Shi et al. (2024, Theorem 2), the lower bound for the class of fictitious RMGs is achieved directly: consider
any tuple S, A , σ ,H obeying σ (0,1 c ] with 0 < c 1 being any small enough
{ i }1≤i≤n { i }1≤i≤n i ∈ − 0 0 ≤ 4
positive constant, and H >16log2. Let
(cid:8) (cid:9)
c1, if σ c1 ,
ε H i ≤ 2H (28)
≤(1 otherwise
for any c 1. We can construct a set of fictitious RMGs = , such that for any dataset
1 ≤ 4 M {RMGi }i∈[I]
generated from the nominal environment with in total N independent samples over all state-action pairs,
all
we have
1
inf max P gap (ξ)>ε , (29)
ξ∈[H]7→∆(S7→ Qn i=1Ai) MGi∈M
n
MGi
(cid:0)
CCE (cid:1)o≥ 8
provided that b b
C SH3max A 1
2 1≤i≤n i
N min H, . (30)
all ≤ ε2 min σ
1≤i≤n i
n o
Here, the infimum is taken over all estimators ξ, P denotes the probability when the game is for
RMGi MGi
all , and C is some small enough constant.
i 2
MG ∈M
b
13Armed with both the upper bound (Theorem 2) and lower bound in (30), we are now ready to discuss
the implications of our sample complexity results.
Breaking the curse ofmultiagencyinthe samplecomplexityforRMGs. Theorem2demonstrates
that for any fictitious RMGs, Robust-Q-FTRL algorithm finds an ǫ-robust CCE when the total number of
samples exceeds
O
SH6 1≤i≤nA i
min H,
1
. (31)
ǫ4 min σ
P (cid:26) 1≤i≤n i(cid:27)!
e
To the best of our knowledge, Robust-Q-FTRL with the above sample complexity in (31) is the first algo-
rithm for RMGs breaking the curse of multiagency, regardless of the types of uncertainty sets. Our sample
complexitydependslinearlyonthesumofeachagent’sactions n A ratherthantheirproduct n A —
i=1 i i=1 i
making the algorithm highly scalable as the number of agents increases. Nonetheless, there still exist gaps
P Q
betweenourupper bound andthe lowerbound—especiallyin termso the dependency onthe horizonlength
H and the accuracy level ε—an interesting direction to investigate in the future.
Comparisons with prior works. All prior works focus on learning equilibria for a different kind of
robustMGswith(s,a)-rectangularuncertaintysets(Blanchet et al.,2023;Ma et al.,2023;Shi et al.,2024).
However, the state-of-the-art sample complexity O SH3 n i=1Ai min H, 1 (Shi et al., 2024)
Qε2 min1≤i≤nσi
still suffers from the curse of multiagency with an ex(cid:16)ponential dependnency on the nuom(cid:17)ber of agents when
all agents have equal action spaces, which uses noneadaptive sampling. Our work circumvents the curse of
multiagency by resorting to a tailored adaptive sampling and online learning procedure, together with the
introduction of a new class of fictitious RMGs, providing a fresh perspective to learning RMGs.
Technical insights. For sample complexity analysis, while previous works have addressed the curse of
multiagency in sequential games like standard Markov games (MGs) and Markov potential games, these
methods are not directly applicable to RMGs. Prior approaches assume a linear relationship between the
valuefunctionandthetransitionkernel,allowingstatisticalerrorsacrossK iterationstocancelout. However,
in RMGs, the robust value function, due to its distributionally robust requirement, is highly nonlinear and
oftenlacksaclosedform,makingitimpossibletolinearlyaggregatestatisticalerrors. Totacklethenonlinear
challenges in RMGs, we design a variance-style bonus term through non-trivial decomposition and control
of auxiliary statistical errors caused by nonlinearity, resulting in a tight upper bound on regret during the
online learning process.
5 Conclusion
Robustness in MARL presents greater challenges than in single-agent RL due to the strategic interactions
between agents in a game-theoretic setting. This work proposes a new class of RMGs with fictitious
uncertainty sets that naturally extends from robust single-agent RL and addresses more realistic scenarios
where each agent’s uncertainty is influenced by both the environment and the behavior of others. We then
propose Robust-Q-FTRL, the first algorithm to break the curse of multiagency in robust Markov games
regardless of the uncertainty set definitions, with sample complexity scaling polynomially with all key
parameters. This opens up new researchdirections in MARL, such as uncertainty set selection, equilibrium
refinement, and sample-efficient algorithm design.
Acknowledgements
The work of Y. Chi is supported in part by the grants NSF CCF-2106778 and CNS-2148212, and by funds
fromfederalagencyandindustrypartnersasspecifiedintheResilient&IntelligentNextGSystems(RINGS)
program. TheworkofL.ShiissupportedinpartbytheResnickInstituteandComputing,Data,andSociety
Postdoctoral Fellowship at California Institute of Technology. The work of E. Mazumdar is supported in
14partfromNSF-2240110. TheworkofA.WiermanissupportedinpartfromtheNSFthroughCNS-2146814,
CPS-2136197,CNS-2106403,NGSDI-2105648.
References
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J.,
Altman, S., Anadkat, S., et al. (2023). Gpt-4 technical report. arXiv preprint arXiv:2303.08774.
Aumann, R. J. (1987). Correlated equilibrium as an expression of Bayesian rationality. Econometrica:
Journal of the Econometric Society, pages 1–18.
Badrinath,K.P. andKalathil,D. (2021). Robustreinforcementlearningusing leastsquarespolicy iteration
with provableperformanceguarantees. In International Conference on Machine Learning, pages511–520.
PMLR.
Bai, Y. and Jin, C. (2020). Provable self-play algorithms for competitive reinforcement learning. In Inter-
national Conference on Machine Learning, pages 551–560.PMLR.
Baker, B. (2020). Emergent reciprocity and team formation from randomized uncertain social preferences.
Advances in neural information processing systems, 33:15786–15799.
Balaji, B., Mallya, S., Genc, S., Gupta, S., Dirac, L., Khare, V., Roy, G., Sun, T., Tao, Y., Townsend,
B., et al. (2019). Deepracer: Educational autonomous racing platform for experimentation with sim2real
reinforcement learning. arXiv preprint arXiv:1911.01562.
Bertsimas, D., Gupta, V., and Kallus, N. (2018). Data-driven robust optimization. Mathematical Program-
ming, 167(2):235–292.
Blanchet, J., Lu, M., Zhang, T., and Zhong, H. (2023). Double pessimism is provably efficient for distri-
butionally robust offline reinforcement learning: Generic algorithm and robust partial coverage. arXiv
preprint arXiv:2305.09659.
Blanchet, J., Lu, M., Zhang, T., and Zhong, H. (2024). Double pessimism is provably efficient for distribu-
tionallyrobustoffline reinforcementlearning: Genericalgorithmandrobustpartialcoverage. Advances in
Neural Information Processing Systems, 36.
Blanchet, J. and Murthy, K. (2019). Quantifying distributional model risk via optimal transport. Mathe-
matics of Operations Research, 44(2):565–600.
Cui, Q., Zhang, K., and Du, S. (2023). Breaking the curse of multiagents in a large state space: Rl in
markov games with independent linear function approximation. In The Thirty Sixth Annual Conference
on Learning Theory, pages 2651–2652.PMLR.
Daskalakis, C., Golowich, N., and Zhang, K. (2023). The complexity of markov equilibrium in stochastic
games. In The Thirty Sixth Annual Conference on Learning Theory, pages 4180–4234.PMLR.
Duchi, J. and Namkoong,H. (2018). Learning models with uniform performance via distributionally robust
optimization. arXiv preprint arXiv:1810.08750.
Gao,R. (2020). Finite-sample guaranteesfor wassersteindistributionally robustoptimization: Breakingthe
curse of dimensionality. arXiv preprint arXiv:2009.04382.
Han, S., Su, S., He, S., Han, S., Yang, H., and Miao, F. (2022). What is the solution for state adversarial
multi-agent reinforcement learning? arXiv preprint arXiv:2212.02705.
Iyengar,G. N. (2005). Robust dynamic programming. Mathematics of Operations Research, 30(2):257–280.
Jin, C., Liu, Q., Wang, Y., and Yu, T. (2021). V-learning–a simple, efficient, decentralized algorithm for
multiagent RL. arXiv preprint arXiv:2110.14555.
15Kakutani, S. (1941). A generalizationof brouwer’s fixed point theorem.
Kannan,S.S.,Venkatesh,V.L.,andMin,B.-C.(2023). Smart-LLM:Smartmulti-agentrobottaskplanning
using large language models. arXiv preprint arXiv:2309.10062.
Kardeş,E., Ordóñez,F., and Hall, R. W. (2011). Discounted robuststochastic games and an applicationto
queueing control. Operations research, 59(2):365–382.
Kearns,M.J.andSingh,S.P.(1999).Finite-sampleconvergenceratesforQ-learningandindirectalgorithms.
In Advances in neural information processing systems, pages 996–1002.
Kober,J., Bagnell,J. A., and Peters, J. (2013). Reinforcement learning in robotics: A survey. The Interna-
tional Journal of Robotics Research, 32(11):1238–1274.
Lattimore, T. and Szepesvári, C. (2020). Bandit algorithms. Cambridge University Press.
Lee, J., Jeon, W., Lee, B., Pineau, J., and Kim, K.-E. (2021). Optidice: Offline policy optimization via
stationary distribution correction estimation. In International Conference on Machine Learning, pages
6120–6130.PMLR.
Leibo, J. Z., Zambaldi, V., Lanctot, M., Marecki, J., and Graepel, T. (2017). Multi-agent reinforcement
learning in sequential social dilemmas. arXiv preprint arXiv:1702.03037.
Li, G., Cai, C., Chen, Y., Wei, Y., and Chi, Y. (2024). Is Q-learning minimax optimal? a tight sample
complexity analysis. Operations Research, 72(1):222–236.
Li, G., Chi, Y., Wei, Y., and Chen, Y. (2022). Minimax-optimal multi-agent RL in Markov games with a
generative model. Advances in Neural Information Processing Systems, 35:15353–15367.
Li,G.,Yan,Y.,Chen,Y.,andFan,J.(2023). Minimax-optimalreward-agnosticexplorationinreinforcement
learning. arXiv preprint arXiv:2304.07278.
Li,S.,Wu,Y.,Cui,X.,Dong,H.,Fang,F.,andRussell,S.(2019).Robustmulti-agentreinforcementlearning
via minimax deep deterministic policy gradient. In Proceedings of the AAAI conference on artificial
intelligence, volume 33, pages 4213–4220.
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. (2015).
Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.
Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In Machine
learning proceedings 1994, pages 157–163.Elsevier.
Lu, M., Zhong, H., Zhang, T., and Blanchet, J. (2024). Distributionally robust reinforcement learning
with interactive data collection: Fundamental hardness and near-optimal algorithm. arXiv preprint
arXiv:2404.03578.
Ma, S., Chen, Z., Zou, S., and Zhou, Y. (2023). Decentralized robust v-learning for solving markov games
with model uncertainty. Journal of Machine Learning Research, 24(371):1–40.
Mahmood,A.R.,Korenkevych,D.,Vasan,G.,Ma,W.,andBergstra,J.(2018).Benchmarkingreinforcement
learning algorithms on real-worldrobots. In Conference on robot learning, pages 561–591.PMLR.
McMahan, J., Artiglio, G., and Xie, Q. (2024). Roping in uncertainty: Robustness and regularization in
markov games. arXiv preprint arXiv:2406.08847.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller,
M.,Fidjeland,A.K.,andOstrovski,G.(2015). Human-levelcontrolthroughdeepreinforcementlearning.
Nature, 518(7540):529–533.
Moulin, H., Ray, I., and Gupta, S. S. (2014). Coarse correlated equilibria in an abatement game. Technical
report, Cardiff Economics Working Papers.
16Moulin,H.andVial,J.-P.(1978). Strategicallyzero-sumgames: the classofgameswhosecompletelymixed
equilibria cannot be improved upon. International Journal of Game Theory, 7(3):201–221.
Nash, J. (1951). Non-cooperative games. Annals of mathematics, pages 286–295.
Nilim, A. and El Ghaoui, L. (2005). Robust control of Markov decision processes with uncertain transition
matrices. Operations Research, 53(5):780–798.
Pan, Y., Chen, Y., and Lin, F. (2023). Adjustable robust reinforcement learning for online 3d bin packing.
arXiv preprint arXiv:2310.04323.
Panaganti,K.andKalathil,D.(2022). Samplecomplexityofrobustreinforcementlearningwithagenerative
model. In International Conference on Artificial Intelligence and Statistics, pages 9582–9602.PMLR.
Pinto, L., Davidson, J., Sukthankar, R., and Gupta, A. (2017). Robust adversarial reinforcement learning.
In International Conference on Machine Learning, pages 2817–2826.PMLR.
Rahimian, H. and Mehrotra, S. (2019). Distributionally robust optimization: A review. arXiv preprint
arXiv:1908.05659.
Rubinstein, A. (2017). Settling the complexity of computing approximate two-player nash equilibria. ACM
SIGecom Exchanges, 15(2):45–49.
Rusu, A. A., Večerík, M., Rothörl, T., Heess, N., Pascanu, R., and Hadsell, R. (2017). Sim-to-real robot
learning from pixels with progressive nets. In Conference on robot learning, pages 262–270.PMLR.
Shalev-Shwartz, S. (2007). Online learning: Theory, algorithms, and applications.
Shalev-Shwartz, S. (2012). Online learning and online convex optimization. Foundations and Trends® in
Machine Learning, 4(2):107–194.
Shalev-Shwartz,S. and Singer,Y. (2007). A primal-dualperspective of online learning algorithms. Machine
Learning, 69(2):115–142.
Shapley, L. S. (1953). Stochastic games. Proceedings of the national academy of sciences, 39(10):1095–1100.
Shi, L. and Chi, Y. (2024). Distributionally robust model-based offline reinforcement learning with near-
optimal sample complexity. Journal of Machine Learning Research, 25(200):1–91.
Shi, L., Li, G., Wei, Y., Chen, Y., Geist, M., and Chi, Y. (2023). The curious price of distributional
robustness in reinforcement learning with a generative model. In Proceedings of the 37th International
Conference on Neural Information Processing Systems, pages 79903–79917.
Shi, L., Mazumdar,E., Chi, Y., and Wierman, A. (2024). Sample-efficientrobustmulti-agentreinforcement
learning in the face of environmental uncertainty. In Forty-first International Conference on Machine
Learning.
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J.,
Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016). Mastering the game of go with deep
neural networks and tree search. nature, 529(7587):484–489.
Slumbers, O., Mguni, D. H., Blumberg, S. B., Mcaleer, S. M., Yang, Y., and Wang, J. (2023). A game-
theoretic framework for managing risk in multi-agent systems. In International Conference on Machine
Learning, pages 32059–32087.PMLR.
Song, Z., Mei, S., and Bai, Y. (2021). When can we learn general-sum Markov games with a large number
of players sample-efficiently? arXiv preprint arXiv:2110.04184.
Szita,I.,Takács,B.,andLorincz,A.(2003). ε–mdps: Learninginvaryingenvironments. JournalofMachine
Learning Research, 3(1).
17Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel, P. (2017). Domain randomization
for transferringdeep neuralnetworks from simulationto the realworld. In 2017 IEEE/RSJ international
conference on intelligent robots and systems (IROS), pages 23–30. IEEE.
Tversky, A. and Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases: Biases in
judgments reveal some heuristics of thinking under uncertainty. science, 185(4157):1124–1131.
Vial, D., Shakkottai, S., and Srikant, R. (2022). Robust multi-agent bandits over undirected graphs. Pro-
ceedings of the ACM on Measurement and Analysis of Computing Systems, 6(3):1–57.
Vinyals,O.,Babuschkin,I.,Czarnecki,W.M., Mathieu, M.,Dudzik, A., Chung,J.,Choi,D.H., Powell,R.,
Ewalds, T., Georgiev, P., et al. (2019). Grandmaster level in starcraft ii using multi-agent reinforcement
learning. Nature, 575(7782):350–354.
Wang, H., Shi, L., and Chi, Y. (2024). Sample complexity of offline distributionally robust linear Markov
decision processes. arXiv preprint arXiv:2403.12946.
Wang, Y., Liu, Q., Bai, Y., and Jin, C. (2023). Breaking the curse of multiagency: Provably efficient
decentralized multi-agent RL with function approximation. In The Thirty Sixth Annual Conference on
Learning Theory, pages 2793–2848.PMLR.
Wu, Y., McMahan, J., Zhu, X., and Xie, Q. (2024). Data poisoning to fake a nash equilibria for markov
games. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 15979–15987.
Xu, Z., Panaganti, K., and Kalathil, D. (2023). Improved sample complexity bounds for distributionally
robust reinforcement learning. arXiv preprint arXiv:2303.02783.
Yang, W., Zhang, L., and Zhang, Z. (2022). Toward theoretical understandings of robust Markov decision
processes: Sample complexity and asymptotics. The Annals of Statistics, 50(6):3223–3248.
Yeh, C., Meng, C., Wang, S., Driscoll, A., Rozi, E., Liu, P., Lee, J., Burke, M., Lobell, D. B., and Ermon,
S. (2021). Sustainbench: Benchmarks for monitoring the sustainable development goals with machine
learning. arXiv preprint arXiv:2111.04724.
Zeng, L., Qiu, D., and Sun, M. (2022). Resilience enhancement of multi-agent reinforcement learning-based
demand response against adversarialattacks. Applied Energy, 324:119688.
Zhang, H., Chen, H., Boning, D., and Hsieh, C.-J. (2021). Robust reinforcement learning on state observa-
tions with learned optimal adversary. arXiv preprint arXiv:2101.08452.
Zhang,H., Chen, H., Xiao, C., Li,B., Liu, M., Boning,D., andHsieh, C.-J.(2020a). Robustdeep reinforce-
ment learning against adversarial perturbations on state observations. Advances in Neural Information
Processing Systems, 33:21024–21037.
Zhang, K., Kakade, S., Basar, T., and Yang, L. (2020b). Model-based multi-agent RL in zero-sum Markov
gameswithnear-optimalsamplecomplexity.AdvancesinNeuralInformationProcessingSystems,33:1166–
1178.
Zhang, K., Sun, T., Tao,Y., Genc, S., Mallya,S., and Basar,T. (2020c). Robust multi-agent reinforcement
learning with model uncertainty. Advances in neural information processing systems, 33:10571–10583.
Zhang, R., Shamma, J., and Li, N. (2024). Equilibrium selection for multi-agent reinforcement learning: A
unified framework. arXiv preprint arXiv:2406.08844.
Zhou, Z., Bai, Q., Zhou, Z., Qiu, L., Blanchet, J., and Glynn, P. (2021). Finite-sample regret bound for
distributionally robust offline tabular reinforcement learning. In International Conference on Artificial
Intelligence and Statistics, pages 3331–3339.PMLR.
Zhou,Z.andLiu, G.(2023). Robustnesstesting formulti-agentreinforcementlearning: State perturbations
on critical agents. arXiv preprint arXiv:2306.06136.
18A Preliminaries
Denoting the vectors x =[x ] and y = [y ] , we use the notation x y (or x y) to signify that
i 1≤i≤n i 1≤i≤n
x y (or x y ) for every 1 i n. The Hadamardproduct of two vector≤ s x and y≥ in RS is denoted as
i i i i
≤ ≥ ≤ ≤
x y = x(s) y(s) . In addition, for any series of vectors x , diag(x ,x , ,x ) denote a block
◦ · s∈S { i }i∈[S 1 2 ··· S
diagonalmatrixbyplacingeachgivenvectorx alongthediagonal,withzerosfillingtheoff-diagonalblocks.0
i
(or 1) re(cid:2) presents th(cid:3) e all-zero (or all-one) vector, while e RS denotes a basis vector of dimension S with 1
i
∈
in the i-th position and 0 elsewhere.
A.1 Additional matrix and vector notation
Before continuing, we introduce or recall some matrix and vector notation that will be used throughout the
paper. In particular, for any joint policy π : [H] ∆( ) and any (i,h) [n] [H]:
S× 7→ A ∈ ×
Matrices for policy. We introduce three matrices associated with π, i.e., Ππ
h ∈
RS×S Qn i=1Ai, Π hπ−i
∈
RS×S j6=iAj, and Ππi RS×SAi, which are defined as block diagonal matrices that adhere to the following
Q h ∈
properties:
• The matrix Ππ is given by diag π (1)⊤,π (2)⊤,...,π⊤(S) , where π (s) = [π (a s)] ∆( ) for
h h h h h h | a∈A ∈ A
each s represents the joint policy vectors across all agents.
∈S (cid:0) (cid:1)
• The matrix Ππ−i can be expressed as diag π (1)⊤,π (2)⊤,...,π⊤ (S) , where π (s) =
h −i,h −i,h −i,h −i,h
[π h(a
−i
|s)]
a −i∈A−i ∈
∆( A−i) for all s
∈ S
(cid:16)denotes the joint policy vectors fr(cid:17)om all agents except
agent i.
• The matrix Ππi is defined as diag π (1)⊤,π (2)⊤,...,π⊤ (S) , where π (s) = [π (a s)]
h i,h i,h i,h i,h i,h i | ai∈Ai ∈
∆( i) for each s represents th(cid:16)e policy of the i-th agent. (cid:17)
A ∈S
Reward vectors. Werecallthedefinitionofr andintroducetherewardvectorsrπ andrπ−i asfollows:
i,h i,h i,h
• Letr i,h =[r i,h(s,a)] (s,a)∈S×A RS n i=1Ai representthe rewardfunctionfor the i-thplayerattime step
∈ Q
h, where is the state space and is the action space.
S A
• The reward vector rπ RS corresponds to the joint policy π = π at time step h. Specifically,
for each s
∈
S, r iπ ,h(i s,h )∈ = E a∼πh(s)[r i,h(s,a)], where the expectat{ ionh } ish∈ t[ aH k] en over the actions a drawn
from policy π in state s.
h
• The reward vector r iπ ,− hi
∈
RSAi corresponds to the joint policy π
−i
= {π
−i,h }h∈[H]
at time step h,
ee xx pcl eu cd ti an tg ioa ng ie sn ot vi e. rS tp he eci afi cc ta ioll ny s, f aor a dll rs aw∈ nS fra on md ta hi e∈ joA ini, tr piπ , o− h li ic( ys, πa i)= fE oa r− ai∼ llπ a− gi e,h n( ts s)[r ei x,h ce(s p, ta a) g], ew nthe i.re the
−i −i,h
Matrices for transition variants. We firstintroduce the followingnotations relatedto transitions asso-
ciated with the nominal transition kernel and the policy π:
• iD caefi lln y,e fP orh0
a∈
nyRS
(sQ
,n i a= )1Ai×S, the
,
m P0atrix re Rp 1r ×es Sen rt ei pn rg est eh ne tsno tm hein ra ol wtr ca on rs ri et sio pn onk de ir nn gel toat tt hi em se tast te ep -ach t. ioS npe pc ai if r-
(s,a). ∈ S ×A
h,s,a
∈
• Define Pπ−i RSAi×S, the matrix representing the nominal transition kernel at time step h, associated
with theh join∈ tpolicy π −i. Specifically, foralls,s′
∈S
anda i ∈Ai, P hπ ,− s,i ai(s′)=E a −i∼π−i,h(s)[P h0 ,s,a(s′)].
Here, Pπ−i R1×S represents the row corresponding to the state-action pair (s,a ).
h,s,ai ∈ i
• LetPπ−i RSAi×S denotetheempiricaltransitionkernelmatrixattimesteph,associatedwiththejoint
i,h ∈
policy π and agent i. Similarly, Pπ−i R1×S represents the row corresponding to the state-action
pairb(s,a−i
).
h,s,ai ∈
i
• Define Pπ RS×S as Pπ :=ΠπP0,b where Ππ is the policy matrix at time step h under joint policy π.
h ∈ h h h h
• Define Pπ RS×S as Pπ := ΠπiPπ−i, where Ππi denotes the policy matrix at time step h under
i,h ∈ i,h h i,h h
policy π .
i
b b b
19We introduce matrix notations for transitions that are associated not only with the nominal transition
and policy π, but also with value functions:
• For time step h [H], joint policy π, and a value vector V RS, we define Pπ−i,V RSAi×S as the
∈ ∈ i,h ∈
matrix representing the worst-case transition probability kernel within the uncertainty set for agent i,
centered around the nominal kernel. The row corresponding to the state-action pair (s,a ) in
Pπ−i,V
,
i i,h
denoted as Pπ−i,V RS, is given by:
i,h,s,ai ∈
P iπ ,h− ,i s, ,V a−i =argmin P∈Uρσi(P hπ ,− s,i ai)PV. (32a)
We also define the transition matrices for specific value vectors as:
P iπ ,h,V :=P iπ ,h−i,V iπ ,h,σ +i 1 and P iπ ,h,V ,s,ai :=P iπ ,h− ,i s, ,V ai iπ ,h,σ +i 1 =argmin P∈Uρσi(P hπ ,− s,i ai)PV iπ ,h,σ +i 1. (32b)
Finally, we define square matrices Pπ,V RS×S as: Pπ,V :=ΠπiPπ−i,V .
i,h ∈ i,h h i,h
• Byreplacingthenominaltransitionkernelwiththeempiricaltransitionkernel,wesimilarlydefinePπ−i,V
i,h
as the worst-case probability transition kernel within the uncertainty set for agent i, centered around
the empirical kernel Pπ−i. The row corresponding to the state-action pair (s,a ) in Pπ−i,V is debnoted
i,h i i,h
as Pπ−i,V RS and is defined as:
i,h,s,ai ∈ b b
b P iπ ,h− ,i s, ,V a−i =argmin P∈Uρσi(P iπ ,h− ,i s,ai)PV. (32c)
b
The transition matrices Pπ,V for sb pecific value vectors are defined as:
i,h
P iπ ,h,V :=P iπ ,h−i,bV iπ ,h,σ +i 1 and P iπ ,h,V ,s,ai :=P iπ ,h− ,i s, ,V ai iπ ,h,σ +i 1 =argmin P∈Uρσi(P iπ ,h− ,i s,ai)PV iπ ,h,σ +i 1, (32d)
b
Additionallb y, we defib ne square matricesb Pπ,V RSb ×S as: Pπ,V :=ΠπiPπ−i,V .
i,h ∈ i,h h i,h
Variance. We now introduce notations fobr variance correspbonding to abspecific probability distribution.
For a probability vector P R1×S and a vector V RS, we denote the variance of V with respect to P as
∈ ∈
Var (V), defined as:
P
Var (V):=P(V V) (PV) (PV), (33)
P
◦ − ◦
Additionally, for a transition kernel Pπ−i ∈RSAi×S and a vector V
∈
RS, we define Var Pπ−i(V) ∈RSAi as
a vector of variances. The entry corresponding to (s,a i) in Var Pπ−i(V) is given by:
Var Pπ−i(s,a i):=Var Psπ ,− aii(V), (34)
where
Pπ−i
denotes the row of the transition matrix corresponding to state s and action a .
s,ai i
A.2 Preliminary facts about FTRL
Our proposed algorithm (see Algorithm 2) is inspired by online adversarial learning. In this section, we
introduce the formulation of online learning and review key aspects of a widely-used algorithm, the Follow-
the-Regularized-Leader (FTRL) algorithm.
Problemsetting: onlinelearningforweightedaverage loss. Weconsideranonlinelearningproblem
overK steps,commonlyfoundinadversariallearningsettings(Lattimore and Szepesvári,2020). Thelearner
is presented with an action set , and loss functions f ,...,f : R are provided for each step. At
1 K ≥0
A A →
each time step k, the learner selects a distribution over the action set, π ∆( ), and observes the loss
k
∈ A
function f (π ). The goal of the learner is to minimize the weighted averageloss over the K steps, which is
k k
20defined as: LK = K αKf (π ). To evaluate the learner’s performance, the regret for the online learning
k=1 k k k
process is defined as:
P
K K
RK = αKf (π ) min αKf (π) . (35)
k k k −"π∈∆(A) k k #
k=1 k=1
X X
FTRL and its regret bound. A widely-used method for solving the online learning problem de-
scribedaboveistheFollow-the-Regularized-Leader(FTRL)algorithm,introducedbyShalev-Shwartz(2007);
Shalev-Shwartz and Singer (2007). At each step k+1, the learner selects a soft-greedy action by solving:
k
π =arg min αkf (π)+F (π) , k =1,2,..., (36)
k+1 i i k
π∈∆(A)" #
i=1
X
where F (π) represents a convex regularization function. The following theorem provides a refined regret
k
bound for the FTRL algorithm when the loss function is linear with respect to the policy.
Theorem 3 (Theorem 3 in Li et al. (2022)). For all k [K] and policy π, the loss function is defined as
f (π) = π ,l , where l R|A| represents a loss vecto∈ r. The learner’s choice π in episode k +1 is
k k k k k+1
h i ∈
updated according to the FTRL algorithm:
exp η L (a)
k+1 k
π (a)=arg min π,L +F (π) = − , for all a , (37)
k+1 π∈∆(A){h k i k } a′∈Ae (cid:0)xp −η k+1L k(cid:1)(a′) ∈A
where the regularization function is given by F (π)
=P 1(cid:0) π(a)log(π(a)(cid:1)
). Suppose 0 < α 1 and
k a∈A ηk+1 1 ≤
η =η (1 α ), and for all k 2, assume 0<α <1 and 0<η (1 α ) η . Define:
1 2 − 1 ≥ k P k+1 − k ≤ k
η , if k =1,
2
η := (38)
k ( ηk , if k >1.
1−αk
b
Then, the regret of the FTRL algorithm is bounded by:
K K
R max αK π ,l αKl (a)
n ≤ a∈A" k h k k i− k k #
k=1 k=1
X X
n n
5 logA 1
αnη α Var (l )+ +3 αnη2α2 l 3 I η α l > . (39)
≤ 3 k k k πk k η k k kk k k∞ k k k k k∞ 3
k=1 n+1 k=1 (cid:18) (cid:19)
X X
b b b
B Proof for Section 3
B.1 Proof of Theorem 1
Step 1: preliminaries. First, we introduce some useful definition and existing facts that are standardin
real analysis and game theory literature.
Definition2(Uppersemi-continuous). Apoint-to-setmappingx φ(x) isuppersemi-continuous
∈X 7→ ∈Y
if lim xn =x ,yn φ(xn),lim yn =y imply that y0 φ(x ).
n→∞ 0 n→∞ 0 0
∈ ∈
Theorem 4 (Kakutani’s fixed point Theorem (Kakutani, 1941)). If X is a closed, bounded, and convex set
in a Euclidean space, and φ is a upper semi-continuous correspondence mapping X into the family of all
closed convex subsets of X, then there exists x X so that x φ(x).
∈ ∈
21Step 2: constructing an auxiliary single-step game. Focusing on finite-horizon RMG =
rob
MG
S,
{Ai
}1≤i≤n, {Uρσi(P0) }1≤i≤n,r,H , we shall verify the theorem by firstly consider a one-step game and
then apply the results recursively to the sequential Markov games.
(cid:8) (cid:9)
Without loss of generality, we focus on any of the steps h [H] and construct an auxiliary one-step
game. Towards this, we first introduce a fixed value function V ∈ RS with 0 V H for the i-th
i,h+1 i,h+1
∈ ≤ ≤
agent, representing the possible value function obtained at the next time step h+1. Focusing on time step
h, for any joint product policy π : ∆( ), we abuse the notation defined in (14) to denote the
S 7→ i∈[n] Ai
expected nominal transition kernel over each (s,a ) as:
Q i
P hπ ,− s,i ai =E π(a −i|s,ai) P h0 ,s,(ai,a −i) =E π−i(a −i|s) P h0 ,s,(ai,a −i) . (40)
h i h i
Armedwiththis,foranyjointproductpolicyπ : ∆( ),wecandefinethepayoffstomaximize
S 7→ i∈[n] Ai
for the players as below:
Q
∀s
∈S
: f i,s(π i(s),π −i(s);V i,h+1)=E a∼π(s)[r i,h(s,a)]+E
ai∼πi(s)
Uσi
i Pn πf
−i
PV
i,h+1
, (41)
(cid:16)
h,s,ai(cid:17)
 
whichisdefinedanalogoustotherobustBellmanequation(cf.(13))byreplacingarealrobustvaluefunction
vector (associated with some policy) to some fixed vector V .
i,h+1
Now we are ready to introduce the following useful mapping: for any π : ∆( ),
S 7→ i∈[n] Ai
φ(π):= u u (s) argmax f (π′(s),π (s);V ), (i,s)Q [n] . (42)
| i ∈ π i′(s)∈∆(Ai) i,s i −i i,h+1 ∀ ∈ ×S
n o
Step 3: the existence of NE of the auxiliary game. To apply Theorem 4, there are three required
conditions. First, we know that the space of product policy is X = π : ∆(A ) is a closed,
{ S 7→ i∈[n] i }
bounded and convex set in Euclidean space.
Q
• Verifyingthatφ(π)isanuppersemi-continuouscorrespondence. Beforestarting,weintroduce
the following two useful lemmas with the proof postponed to Appendix B.2.2 and B.2.3.
Lemma 2. The set of function f (π′(s),π (s);V ),0 V ) H is equicontinuous with
{ i,s i −i i,h+1 ≤ i,h+1 ≤ }
respect to π′(s),π (s) for all (i,s) [n] .
i −i ∈ ×S
Lemma 3. For any i [n] and then x : ∆( ), the functions
∈ −i S 7→ j6=i,j∈[n] Aj
∀s
∈S
: g i,s(x −i(s),V i,h+1):=Qmax π i′∈∆(S) f i,s(π i′(s),x −i(s);V i,h+1) (43)
are continuous with respect to x (s) and the set g (,V)V RS,0 V H is equicontinuous.
−i i,s
{ · | ∈ ≤ ≤ }
Armed with above lemmas, we are in the position to prove this condition. We suppose there are
two sequence lim xn = x0,yn φ(xn),lim yn = y0. Recall the definition of a upper semi-
n→∞ n→∞
∈
continuous correspondence (cf. Definition 2), we are supposed to show that y0 φ(x0), i.e.,
∈
∀(i,s) ∈[n]
×S
: f i,s(y i0(s),x0 −i(s);V i,h+1)=max π i′∈∆(S) f i,s(π i′(s),x0 −i(s);V i,h+1). (44)
Towards this, we have
f (y0(s),x0 (s);V ) g (x0 (s),V )
| i,s i −i i,h+1 − i,s −i i,h+1 |
f (y0(s),x0 (s);V ) f (yn(s),xn (s);V )
≤| i,s i −i i,h+1 − i,s i −i i,h+1 |
+ f (yn(s),xn (s);V ) g (x0 (s),V )
| i,s i −i i,h+1 − i,s −i i,h+1 |
( =i) f (y0(s),x0 (s);V ) f (yn(s),xn (s);V ) + g (xn (s),V ) g (x0 (s),V )
| i,s i −i i,h+1 − i,s i −i i,h+1 | | i,s −i i,h+1 − i,s −i i,h+1 |
0 as n , (45)
→ →∞
wherethefirstinequalityfollowsfromthetriangleinequality,(i)holdsbytheassumptionyn φ(xn)so
∈
thatf i,s(y in(s),xn −i(s);V i,h+1)=max π i′∈∆(S) f i,s(π i′(s),xn −i(s);V i,h+1), andthe lastline canbe verified
by the continuity implied by Lemma 2 and Lemma 3.
22• Verifying φ(π) is convex for any π X. Finally, we gonna work on the convexity of φ(π) for any
∈
π X. To begin with, by the definition of φ(π) in (42), we know that φ(π) X and the maximum of
∈ ⊆
thecontinuousfunction f (π (s),π (s);V )(cf.Lemma2)onacompactsetexists,i.e.,φ(x)= .
i,s i −i i,h+1
6 ∅
SupposethereexiststwoNashequilibriumz : ∆( ),v : ∆( )andz,v φ(π).
S 7→ i∈[n] Ai S 7→ i∈[n] Ai ∈
Then we have that for any (i,s) [n] ,
∈ ×S Q Q
f (z (s),π (s);V )=f (v (s),π (s);V )= max f (u (s),π (s);V ). (46)
i,s i −i i,h+1 i,s i −i i,h+1 i,s i −i i,h+1
ui(s)∈∆(Ai)
To continue, for any 0 λ 1, one has
≤ ≤
max f (u (s),π (s);V )=λf (z (s),π (s);V )+(1 λ)f (v (s),π (s);V )
i,s i −i i,h+1 i,s i −i i,h+1 i,s i −i i,h+1
ui(s)∈∆(Ai) −
=λ E rπ−i(s,a ) +E inf PV
(cid:18)
ai∼zi(s)
h
i,h i
i
ai∼zi(s)
(cid:20)Uσi (cid:16)P hπ ,− s,i ai(cid:17)
i,h+1
(cid:21)(cid:19)
+(1 λ) E rπ−i(s,a ) +E inf PV (47)
− (cid:18) ai∼vi(s) h i,h i i ai∼vi(s) (cid:20)Uσi (cid:16)P hπ ,− s,i ai(cid:17) i,h+1 (cid:21)(cid:19)
=E rπ−i(s,a ) +E inf PV
ai∼[λzi(s)+(1−λ)vi(s)]
h
i,h i
i
ai∼[λzi(s)+(1−λ)vi(s)]
(cid:20)Uσi (cid:16)P hπ ,− s,i ai(cid:17)
i,h+1
(cid:21)
=f (λz (s)+(1 λ)v (s),π (s);V ). (48)
i,s i i −i i,h+1
−
where we denote r iπ ,− hi(s,a i) := E a −i∼π−i(s)[r i,h(s,(a i,a −i))]. Hence, we show that λz i(s) + (1
−
λ)v (s) φ(π) for all (i,s) [n] and 0 λ 1, thus verify that φ(π) is convex for any π X.
i
∈ ∈ ×S ≤ ≤ ∈
Step 4: the existence of robust NE in RMGs. Armed with above results, now we consider a general
form to show that there exists a policy π :[H] ∆( ) that satisfies
×S 7→ i∈[n] Ai
(i,h,s) [n] [H] :
Q Vπ,σi(s)=V⋆,π−i,σi(s).
(49)
∀ ∈ × ×S i,h i,h
We shall prove this by induction.
• The base case. Starting with the final step h=H, we recall that by definition,
(i,s) [n] : Vπ,σi (s)=0. (50)
∀ ∈ ×S i,H+1
To apply the results in the one-step game constructed in Step 2, we consider the one-step game at
h=H and using the payoff function (cf. (41))
∀s
∈S
: f i,s(π i(s),π −i(s);V iπ ,H,σ +i 1)=E a∼π(s)[r i,h(s,a)]. (51)
We know that there exists a policy π so that
(i,s) [n] :
Vπ,σi(s)=V⋆,π−i,σi(s)
(52)
∀ ∈ ×S i,H i,H
by setting π as the NE of the one-step game.
H
• Induction. Assuming that there exists a policy π so that for subsequent steps h+1, ,H,
···
(i,h,s) [n] h+1, ,H :
Vπ,σi(s)=V⋆,π−i,σi(s),
(53)
∀ ∈ ×{ ··· }×S i,h i,h
whichareachievedbydeterminingcertainpoliciesfor π ,π , ,π . Wearesupposedtoprove
h+1 h+2 H
{ ··· }
that at time step h, we can ensure our policy π satisfying
(i,s) [n] :
Vπ,σi(s)=V⋆,π−i,σi(s)
(54)
∀ ∈ ×S i,h i,h
23by choosing a proper policy π at the time step h.
h
Towards this, it is observed that
V⋆,π−i,σi(s)=
max
Vπ i′×π−i,σi(s)
i,h π i′:S×[H]7→∆(Ai) i,h
=
π
i′:S×[m Ha ]7→x ∆(Ai)E a∼π i′ ,h(s)×π−i,h(s)[r i,h(s,a)]+E ai∼π i′ ,h(s) (cid:20)P∈Uσii (cid:16)n Pf
hπ ,− s,i
ai(cid:17)PV iπ ,hi′ +× 1π−i,σi
(cid:21)
=
π i′
,h(m s)∈a ∆x (Ai)E a∼π i′ ,h(s)×π−i,h(s)[r i,h(s,a)]
+
π i′
,h(m s)∈a ∆x (Ai)E ai∼π i′ ,h(s)
π i′
,h+:S×m ha +x
7→∆(Ai)
(cid:20)P∈Uσii (cid:16)n Pf
hπ ,− s,i
ai(cid:17)PV iπ ,hi′ +× 1π−i,σi
(cid:21)
=
π i′
,h(m s)∈a ∆x (Ai)E a∼π i′ ,h(s)×π−i,h(s)[r i,h(s,a)]+E ai∼π i′ ,h(s) P∈Uσii (cid:16)n Pf
hπ ,− s,i
ai(cid:17)PV i⋆ ,h,π +− 1i,σi .
  (55)
where we denote h+ = h+1,h+2, ,H as the set that includes all the time steps after h until
{ ··· }
the end of the episode, and the last equality follows from the fact
max inf
PVπ i′×π−i,σi
= inf P max
Vπ i′×π−i,σi
π i′ ,h+:S×h+7→∆(Ai) (cid:20)Uσi (cid:16)P hπ ,− s,i ai(cid:17) i,h+1 (cid:21) Uσi (cid:16)P hπ ,− s,i ai(cid:17) π i′ ,h+:S×h+7→∆(Ai) i,h+1 (cid:21)
= inf
PV⋆,π−i,σi,
(56)
i,h+1
Uσi Pπ−i
(cid:16)
h,s,ai(cid:17)
which holds by the definition of V⋆,π−i,σi. Now invoking the results in the auxiliary one-step game
i,h+1
with V
=V⋆,π−i,σi,
one has that there exists a policy with π that satisfies
i,h+1 i,h+1 h
(i,s) [n] :
Vπ,σi(s)=V⋆,π−i,σi(s).
(57)
∀ ∈ ×S i,h i,h
Combining the results in the base case and induction, we complete the proof by recursively choosing
π : ∆( ) for h = H,H 1, ,1 as the NE of the corresponding one-step game at time step
h S 7→ i∈[n] Ai − ···
h and arrive at
Q
(i,s) [n] :
Vπ,σi(s)=V⋆,π−i,σi(s).
(58)
∀ ∈ ×S i,1 i,1
B.2 Proof of auxiliary facts
B.2.1 Proof of Lemma 1
The proof is obtained by recursively showing that for each (h,s), there exist a policy. Then the product
policy of them will be that final policy
Without loss of generality, we consider any i [n] with the other agents’ policy π : [H] ∆( )
i i
∈ S × 7→ A
fixed. We shall prove this lemma by induction.
• The base case. Considerthebasecaseh=H. Conditionedontheotheragents’policyπ : [H]
i
S× 7→
∆( ), the maximum of the robust value function of the i-th agent can be expressed by
−i
A
s :
V⋆,π−i,σi(s)=
max
Vπ i′×π−i,σi(s)
∀ ∈S i,H π i′:S×[H]7→∆(Ai) i,H
=
π
i′:S×[m Ha ]7→x ∆(Ai)E ai∼π i′ ,H(s) E a −i∼π−i,H(s)[r i,H(s,a)]
=
π i′
,H(m s)∼ax ∆(Ai)E ai∼π i′ ,H(s) E(cid:2) a −i∼π−i,H(s)[r i,H(s,a)] .(cid:3) (59)
(cid:2) (cid:3)
24Since the maximum of the continuousfunction E ai∼π i′ ,H(s) E a −i∼π−i,H(s)[r i,H(s,a)] on a compactset
∆( ) exists, by setting
Ai (cid:2) (cid:3)
∀s ∈S : π i,H(s)=argmax π i′ ,H(s)∼∆(Ai)E ai∼π i′ ,H(s) E a −i∼π−i,H(s)[r i,H(s,a)] , (60)
(cid:2) (cid:3)
we arrive at
e
s :
Vπi×π−i,σi(s)=V⋆,π−i,σi(s).
(61)
∀ ∈S i,H i,H
e
This complete the proof for the base case.
• Induction. Assuming that for t=h+1,h+2, ,H, we have
···
s :
Vπi×π−i,σi(s)=V⋆,π−i,σi(s).
(62)
∀ ∈S i,t i,t
e
Then, we want to prove for the step h, where the maximum of the robust value function of the i-th
agent can be expressed as: for all s ,
∈S
V⋆,π−i,σi(s)
i,h
= max
Vπ i′×π−i,σi(s)
π i′:S×[H]7→∆(Ai) i,h
= π i′:S×[m Ha ]7→x ∆(Ai)E ai∼π i′ ,h(s) (cid:2)E a −i∼π−i,h(s)[r i,h(s,a)] (cid:3)+E ai∼πi,h(s) Uρσi (cid:16)i Pn hπf ,− s,i ai(cid:17)PV i⋆ ,h,π +− 1i,σi 
 
( =i) π i′:S×[m Ha ]7→x ∆(Ai)E ai∼π i′ ,h(s) (cid:2)E a −i∼π−i,h(s)[r i,h(s,a)] (cid:3)+E ai∼πi,h(s) Uρσi (cid:16)i Pn hπf ,− s,i ai(cid:17)PV iπ e,hi +× 1π−i,σi 
 
=
π i′
,h(m s)∼a ∆x (Ai)E ai∼π i′ ,h(s) (cid:2)E a −i∼π−i,h(s)[r i,h(s,a)] (cid:3)+E ai∼πi,h(s)
Uρσi
(cid:16)i Pn hπf
,− s,i
ai(cid:17)PV iπ e,hi +× 1π−i,σi . (63)
 
where (i) holds by the induction assumption in (62). Similarly to the base case, the maximum of the
continuous function E ai∼π i′ ,h(s) E a −i∼π−i,h(s)[r i,h(s,a)] +E ai∼πi,h(s) (cid:20)inf Uρσi (cid:16)P hπ ,− s,i ai(cid:17)PV iπ e,hi +× 1π−i,σi
(cid:21)
on
a compact set ∆( i) exists. So(cid:2)without conflict, for all(cid:3)s , we can set
A ∈S
π (s)
i,h
e=argmax π i′ ,h(s)∼∆(Ai)E ai∼π i′ ,h(s) (cid:2)E a −i∼π−i,h(s)[r i,h(s,a)] (cid:3)+E ai∼πi,h(s)
Uρσi
(cid:16)i Pn hπf
,− s,i
ai(cid:17)PV iπ e,hi +× 1π−i,σi ,
 (64)
sincethefunctioninf
PVπi×π−i,σi andespeciallyVπi×π−i,σi
areindependentfromthepolicy
Uρσi (cid:16)P hπ ,− s,i
ai(cid:17)
i e,h+1 i e,h+1
in the first h steps ( π (s) ).
i,t s∈S,t∈[h]
{ }
Consequently, (64) directly implies that
e
s :
Vπi×π−i,σi(s)=V⋆,π−i,σi(s).
(65)
∀ ∈S i,h i,h
e
Combining the results in base case and the induction, we complete the proof by showing that
(h,s) [H] :
Vπi×π−i,σi(s)=V⋆,π−i,σi(s).
(66)
∀ ∈ ×S i,h i,h
e
25B.2.2 Proof of Lemma 2
First, we define the distance between any two policy π,π′ X = π : ∆(A ) as below:
∈ { S 7→ i∈[n] i }
d(π,π′):=max max π (a s) π′(a sQ ). (67)
i∈[n](s,ai)∈S×Ai| i i | − i i | |
To prove the continuity, given any ǫ>0, we want to show that there exists δ(ǫ)>0 such that if
d(π,π′)<δ(ǫ), (68)
then
f (π (s),π (s);V ) f (π′(s),π′ (s);V ) <ǫ (69)
i,s i −i i,h+1 − i,s i −i i,h+1
(cid:12) (cid:12)
for any fixed V i,h+1 i∈[n(cid:12)] with 0 V i,h+1 H for all i [n]. Towards this, w(cid:12)e observe that
{ } ≤ ≤ ∈
f (π (s),π (s);V ) f (π′(s),π′ (s);V )
i,s i −i i,h+1 − i,s i −i i,h+1
(cid:12) (cid:12)=
(cid:12)
(cid:12)E a∼π(s)[r i,h(s,a)]+E ai∼πi(s)
(cid:20)Uσi
(cid:16)i Pn hπf
,− s,i
ai(cid:17)PV i,h+(cid:12) (cid:12)1
(cid:21)
(cid:12)
−(cid:12) E a∼π′(s)[r i,h(s,a)]+E ai∼π i′(s)
(cid:20)Uσi
i Pn πf
−′
i
PV i,h+1
(cid:21)(cid:12)
h,s,ai (cid:12)
E a∼π(s)[r i,h(s,a)] E a∼π′(s)[r i,h(s,a (cid:0))] (cid:1) (cid:12) (cid:12)
≤ −
+(cid:12) (cid:12)
(cid:12)
(cid:12)E ai∼πi(s)
(cid:20)Uσi
(cid:16)i Pn hπf
,− s,i
ai(cid:17)PV i,h+1 (cid:21)−E ai(cid:12) (cid:12)∼π i′(s)
(cid:20)Uσi
i Pn hπf
,−′ s,i
ai
PV i,h+1
(cid:21)(cid:12)
(cid:12). (70)
(cid:12) (cid:12)
(cid:12) (cid:0) (cid:1) (cid:12)
The first term can be bounded by
E a∼π(s)[r i,h(s,a)] E a∼π′(s)[r i,h(s,a)]
−
(cid:12) (cid:12)
≤
π i(a i |s)
−
π i′(a i |s) (s,(cid:12) (cid:12) am )∈a Sx ×Ar i,h(s,a)
a X∈A(cid:12) (cid:12)iY∈[n] iY∈[n] (cid:12)
(cid:12)
(cid:12) (cid:12)
(cid:12) π (a s) π′(a s)(cid:12) , (71)
≤ i i | − i i |
a X∈A(cid:12) (cid:12)iY∈[n] iY∈[n] (cid:12)
(cid:12)
(cid:12) (cid:12)
where the last inequality holds (cid:12)by the definition of reward (cid:12)function max (s,a)∈S×Ar i,h(s,a) 1 for all
≤
(i,h) [n] [H]. To continue, we first define the difference between δ (s,a ) := π′(a s) π (a s).
∈ × i i i i | − i i |
Therefore, we have
π (a s) π′(a s) = π (a s) (π (a s)+δ (s,a ))
i i | − i i | i i | − i i | i i
(cid:12) (cid:12)iY∈[n] iY∈[n] (cid:12)
(cid:12)
(cid:12) (cid:12)iY∈[n] iY∈[n] (cid:12)
(cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
= δ (s,a ) π (a s)
i i i i
(cid:12) (cid:12)|Y|≥X1,Y⊆[n] i Y∈Y !· i Y∈Yc | !(cid:12)
(cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
δ (s,a ) π (a s) (2n 1)δ(ǫ), (72)
i i i i
≤ |Y|≥X1,Y⊆[n](cid:12)
(cid:12)
i Y∈Y !· i Y∈Yc | !(cid:12) (cid:12)≤ −
(cid:12) (cid:12)
where the last inequality holds by (68). Plugging(cid:12) (72) back to (71) indicates that (cid:12)
E a∼π(s)[r i,h(s,a)] E a∼π′(s)[r i,h(s,a)] A i(2n 1)δ(ǫ). (73)
− ≤ −
(cid:12) (cid:12)
iY∈[n]
(cid:12) (cid:12)
26For the second term in (70), we observe that
(cid:12)
(cid:12)E ai∼πi(s) (cid:20)P∈Uσii (cid:16)n Pf
hπ ,− s,i
ai(cid:17)PV i,h+1 (cid:21)−E ai∼π i′(s) (cid:20)P∈Uσiin Pf
hπ ,−′ s,i
ai
PV i,h+1
(cid:21)(cid:12)
(cid:12)
(cid:12) (cid:12)
(cid:12) E inf PV E (cid:0) inf (cid:1) PV (cid:12)
≤ (cid:12) (cid:12)
ai∼πi(s)
(cid:20)P∈Uσi (cid:16)P hπ ,− s,i ai(cid:17)
i,h+1
(cid:21)−
ai∼πi(s)
(cid:20)P∈Uσi (cid:18)P hπ ,−′ s,i ai(cid:19)
i,h+1
(cid:21)(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
+ (cid:12)E ai∼πi(s) (cid:20)P∈Uσiin Pf
π−′
i
PV i,h+1 (cid:21)−E ai∼π i′(s) (cid:20)P∈Uσiin Pf
π−′
i
PV i,h+1
(cid:21)(cid:12)
(cid:12) (cid:18) h,s,ai(cid:19) h,s,ai (cid:12)
(cid:12) (cid:12)
(i) (cid:12) (cid:0) (cid:1) (cid:12)
≤ E ai∼πi(s) (cid:20)α∈[minsVi,h+1m (sa ),x maxsVi,h+1(s)]
(cid:12)
(cid:12)E π−i(a −i|s) hP h0 ,s,(ai,a −i) i[V i,h+1] α
−E π −′ i(a −i|s) hP h0 ,s,(ai,a −i) i[V i,h+1] α (cid:12) (cid:12)(cid:21)(cid:12) (cid:12)+ a Xi∈Ai(cid:12)π i′(a i |s) −π i(a i |s) (cid:12)P∈Uσiin Pf hπ ,−′ s,i
ai
PV i,h+1
(cid:12) (cid:12) (cid:12)
(ii) (cid:12) (cid:0) (cid:1)
π (a s) π′(a s) H +HA ∆(ǫ)
≤ j j | − j j | i
a −Xi∈Ai(cid:12) (cid:12)Yj6=i Yj6=i (cid:12)
(cid:12)
(iii) (cid:12) (cid:12)
H (cid:12) A (2n−1 1)δ(ǫ)+HA(cid:12) ∆(ǫ) 2H A (2n 1) δ(ǫ), (74)
j i i
≤ − ≤ − ·
j6=i,j∈[n] i∈[n]
Y Y
wherethe firstinequalityholds by the triangleinequality,and(i) followsfromapplying the dualformofTV
distance
inf V = max P[V] σ α min[V] (s′) , (75)
P∈Uσi(P)P α∈[minsV(s),maxsV(s)] α− i − s′ α
n (cid:16) (cid:17)o
and the maximum operator is 1-Lipschitz, (ii) arises from the fact that V H, and (iii) can be
i,h+1 ∞
k k ≤
verified by following the same pipeline of (72). Combining (73) and (74), one has
f (π (s),π (s);V ) f (π′(s),π′ (s);V ) 3H A (2n 1) δ(ε). (76)
i,s i −i i,h+1 − i,s i −i i,h+1 ≤ i − ·
(cid:12) (cid:12)
iY∈[n]
(cid:12) (cid:12)
Consequently, letting δ (ǫ)= min{ǫ,1} , we have when d(π,π′)<δ (ǫ),
1 3H i∈[n]Ai(2n−1) 1
Q
f (π (s),π (s);V ) f (π′(s),π′ (s);V ) <ǫ.
i,s i −i i,h+1 − i,s i −i i,h+1
(cid:12) (cid:12)
B.2.3 Proof of Lemm(cid:12)a 3 (cid:12)
Without loss of generality, we consider any i [n]. Consider x : ∆( ) and y :
∈ −i S 7→ j6=i,j∈[n] Aj −i S 7→
∆( ). Before continuing, for all s , we denote
j6=i,j∈[n] Aj ∈S Q
Q u⋆ :=argmax f (π′(s),x (s);V ),
i,s π′∈∆(S) i,s i −i i,h+1
i
v⋆ :=argmax f (π′(s),y (s);V ). (77)
i,s π′∈∆(S) i,s i −i i,h+1
i
Then we have for any s ,
∈S
g (x (s),V ) g (y (s),V )
i,s −i i,h+1 i,s −i i,h+1
−
=max π i′∈∆(S) f i,s(π i′(s),x −i(s);V i,h+1) −max π i′∈∆(S) f i,s(π i′(s),y −i(s);V i,h+1)
=f (u⋆ ,x (s);V ) f (v⋆ ,y (s);V )
i,s i,s −i i,h+1 − i,s i,s −i i,h+1
f (u⋆ ,x (s);V ) f (u⋆ ,y (s);V ) 0 as y (s) x (s), (78)
≤ i,s i,s −i i,h+1 − i,s i,s −i i,h+1 → −i → −i
27where the last line holds by Lemma (2) which shows that the function f is continuous. Similarly, one has
i,s
g (x (s),V ) g (y (s),V )
i,s −i i,h+1 i,s −i i,h+1
−
f (v⋆ ,x (s);V ) f (u⋆ ,y (s);V ) 0 as y (s) x (s). (79)
≥ i,s i,s −i i,h+1 − i,s i,s −i i,h+1 → −i → −i
We complete the proof by showing that
g (x (s),V ) g (y (s),V ) 0 as y (s) x (s). (80)
i,s −i i,h+1 i,s −i i,h+1 −i −i
| − |→ →
C Proof of Theorem 2
We will present the proof of Theorem 2 by first outlining the proof structure, followed by a step-by-step
explanation of the key components. Auxiliary proofs will be provided at the end of this section.
C.1 Proof pipeline
To proof Theorem 2, recall the goal is to show that
(i,s) [n] : E V⋆,π−i,σi(s) E Vπ,σi(s) ε, (81)
∀ ∈ ×S π∼ξ i,1 − π∼ξ i,1 ≤
b(cid:2) (cid:3) b(cid:2) (cid:3)
where ξ = ξ is the output distribution over the set of policies πk = (πk πk )
{ h }h∈[H] { h 1,h×···× n,h }k∈[K],h∈[H]
from Algorithm 2. Namely, π ξ means
b b ∼
∀hb ∈[H]: π
h
∼ξ h, where ξ h(π hk)=αK
k
. (82)
We first introduce the best-response policy fobr player i: b
π˜⋆ =[π˜⋆ ] :=arg max E Vπ i′,π−i .
i i,h h∈[H] π i′:S×[H]→∆(Ai) π∼ξ i,1
bh i
RecallthatvaluefunctionE Vπ,σi satisfiesthefollowingBellmanequationforall(i,s,h) [n] [H]:
π∼ξ i,h ∈ ×S×
bh i
E Vπ,σi (s) =0,
π∼ξ i,H+1
bh i
E Vπ,σi(s) =E π (a s)r (s,a)+E inf E Vπ,σi ,
π∼ξ bh i,h i π∼ξ b a X∈A h | i,h ai∼πi,h P∈U iσi (cid:16)P hπ ,− s,i ai(cid:17)P π∼ξ bh i,h+1 i 
 
K  K 
= αKπk(a s)r (s,a)+ αKE inf E Vπ,σi ,
Xk=1a X∈A k h | i,h k X=1 k ai∼π ik ,h P∈U iσi (cid:18)P hπ ,−k s,i ai(cid:19)P π∼ξ bh i,h+1 i

 
πk
where P −i is defined as:
h,s,ai
P hπ ,−k s,i ai =E a −i∼π −k i,h(·|s) P h0 ,s,(ai,a −i) = π −k i,h(a −i |s) P h0 ,s,(ai,a −i) .
h i a −Xi∈A−i h i
We decompose the error in the value functions as follows:
E V⋆,π−i E Vπ
π∼ξ i,h − π∼ξ i,h
Ebh V⋆,πi −i Eb(cid:2) V(cid:3)π˜ i⋆,π−i +E V⋆,π−i E Vπ +E Vπ E Vπ . (83)
≤ π∼ξ i,h − π∼ξ i,h π∼ξ i,h − π∼ξ i,h π∼ξ i,h − π∼ξ i,h
bh i A bh i bh i B bh i bh i C b(cid:2) (cid:3)
| {z } | {z } | {z }
28We define the following auxiliary value functions for all s :
∈S
K K
E Vπ (s) = αKE rk (s,a ) + αKE inf E Vπ ,
π∼ξ
bh
i,h
i Xk=1
k ai∼π ik ,h(s)
(cid:2)
i,h i
(cid:3) k X=1
k ai∼π ik ,h(s)
P∈Uσi (cid:16)P ik
,h,s,ai(cid:17)P π∼ξ
bh
i,h+1 i
  (84a)
K K
E π∼ξ V iπ˜ ,i⋆ h,π−i(s) = αK k E ai∼π˜ i⋆ ,h(s) r ik ,h(s,a i) + αK k E ai∼π˜ i⋆ ,h(s) inf PE π∼ξ V iπ˜ ,i⋆ h, +π 1−i ,
bh i Xk=1 (cid:2) (cid:3) Xk=1 P∈Uσi (cid:16)P ik ,h,s,ai(cid:17) bh i
 (84b)
K
E V⋆,π−i(s) = max αK rk (s,a )+ inf E V⋆,π−i , (84c)
π∼ξ bh i,h i ai∈Ai Xk=1 k  i,h i  P∈Uσi (cid:16)P ik ,h,s,ai(cid:17)P π∼ξ bh i,h+1 i
  
where for all s , we also have
∈S
E Vπ (s) =E Vπ˜ i⋆,π−i(s) =E V⋆,π−i (s) =0
π∼ξ i,H+1 π∼ξ i,H+1 π∼ξ i,H+1
bh i bh i bh i
Here, we use the fact that E V⋆,π−i E Vπ˜ i⋆,π−i(s) . Using the error decomposition in (83), we
π∼ξ i,h ≥ π∼ξ i,h
will now individually bound the bthhree terims, A, B bh, and C, inithe following sections.
C.2 Controlling B: adversarial online learning
C.2.1 Step 1: showing that V is an entry-wise upper bound on E V⋆,π−i
i,h π∼ξ i,h
th
bh i
The following lemma demonstratebs that the value estimate V
i,h
for the i player serves as an optimistic
estimate of the auxiliary value E V⋆,π−i , as defined in (84).
π∼ξ i,h
b
bh i
Lemma 4. With probability at least 1 δ, it holds that
−
V E V⋆,π−i , for all (i,h) [n] [H].
i,h ≥ π∼ξ i,h ∈ ×
bh i
Proof. See Appendix C.4.1 b
th
The following lemma demonstrates that the value estimate V for the i player serves as an optimistic
i,h
estimate of the auxiliary value E V⋆,π−i , as defined in (84).
π∼ξ i,h
b
bh i
Lemma 5. For value vector V and E Vπ , it holds that
i,h π∼ξ i,h
bh i
Vb E Vπ , for all (i,h) [n] [H].
i,h ≥ π∼ξ i,h ∈ ×
bh i
Proof. See Appendix C.4.2 b
C.2.2 Step 2: constructing recursion
To begin with, according to the definition of V (s) and E Vπ (s) , we have
i,h π∼ξ i,h
bh i
V (s) E Vπ (s) b
i,h − π∼ξ i,h
K
bh i
b
=min αKE rk (s,a )+ inf V +β (s),H h+1

Xk=1
k ai∼π ik ,h i,h i
P∈Uσi (cid:16)P ik
,h,s,ai(cid:17)P i,h+1  i,h − 

 b 
 
29K
αKE rk (s,a )+ inf E Vπ
−
Xk=1
k ai∼π ik ,h i,h i
P∈Uσi (cid:16)P ik
,h,s,ai(cid:17)P π∼ξ
bh
i,h+1 i
 
K
αKE rk (s,a )+ inf V +β (s)
≤
Xk=1
k ai∼π ik ,h i,h i
P∈Uσi (cid:16)P ik
,h,s,ai(cid:17)P i,h+1  i,h
 b 
K
αKE rk (s,a )+ inf E Vπ
−
Xk=1
k ai∼π ik ,h i,h i
P∈Uσi (cid:16)P ik
,h,s,ai(cid:17)P π∼ξ
bh
i,h+1 i
 
K K
= αKE inf V +β (s) αKE inf E Vπ
Xk=1
k ai∼π ik ,h
P∈Uσi (cid:16)P ik
,h,s,ai(cid:17)P i,h+1  i,h −
Xk=1
k ai∼π ik ,h
P∈Uσi (cid:16)P ik
,h,s,ai(cid:17)P π∼ξ
bh
i,h+1 i
 b   (85)
To simplify the notations, we define transition kernel associated estimated value function similarly as (32).
For all k [K], we define matrix notations
Pπk,V
and
Pπˆk,V
as:
∈ i,h i,h
b
Pπk,V :=Pπ −k i,Vi,h+1
and
Pπk,V :=Pπb−k i,Vi,h+1 =ab
rgmin V ,
i,h b i,h b i,h,s b,ai i,h,s, bai P∈Uρσi (cid:18)P iπ ,h−k ,i s,ai(cid:19)P i,h+1
Pb πk,V :=Pbπ −k i,E π∼ξ[Vπ i,h+1] ab nd Pπk,Vb :=Pπ −k i,E π∼ξ[Vπ i,h+1] =argb min b E Vπ .
i,h i,h b i,h,s,ai i,h,s,ai b P∈Uρσi (cid:18)P iπ ,h−k ,i s,ai(cid:19)P π∼ξ
bh
i,h+1
i
b b b b b
Additionally, we define square matrices Pπk,V RS×S and Pπk,V RS×S as: Pπk,V := Ππ ik Pπ −k i,V and
i,h ∈ i,h b ∈ i,h h i,h
Pπk,V :=Ππ ik Pπ −k i,V
. We rewrite the result of (85) in a vector form, we can obtain that
i,h b h i,h b b b b b
b V b E Vπ
i,h − π∼ξ i,h
bh i
b K K
αKΠπi inf V +β αKΠπi inf Vπ
≤ Xk=1 k h  P∈Uσi (cid:18)P iπ ,h−k ,i s,ai(cid:19)P bi,h+1

i,h − k X=1 k h  P∈Uσi (cid:18)P iπ ,h−k ,i s,ai(cid:19)P i,h+1

 b   b 
K K
=
αKPπk,V
V +β
αKPπk,VE Vπ
k i,h b i,h+1 i,h − k i,h π∼ξ i,h+1
Xk=1 Xk=1 bh i
K b b b
αKPπk,V V E Vπ +β .
≤ k i,h i,h+1 − π∼ξ i,h+1 i,h
Xk=1 (cid:16) bh i(cid:17)
b b
To continue, we first introduce an lemma of the upper bound for bonus vector β .
i,h
Lemma 6. The bonus vector β is bounded by the following inequality:
i,h
log3(KS n i=1Ai) K
β 3c Pδ H 1+ αKVar V
i,h ≤ bs KH ·
k=1
k Pπ i,k h,V
b
i,h+1 !
X
b
b
Proof. See Appendix C.4.3
To proceed, we introduce some notations for convenience. Let e denote the S-dimensional standard
s
basis vector, with support on the s-th element. Additionally, we define:
j−1 K
bh =e and bj =e⊤
αKPπk,V
, j =h+1,...,H. (86)
h s h s " k i,r !# ∀
r=h k=1
Y X
b
30Armed with above notations and fact, for any s , we have
∈S
H
V (s) E Vπ (s) = e ,V E Vπ = bj,β
i,h − π∼ξ i,h s i,h − π∼ξ i,h h i,j
bh i D bh iE Xj=hD E
b b
H log3(KS n i=1Ai) H K log3(KS n i=1Ai)
bj,3c H Pδ 1 + αK bj,3c Pδ Var V
≤ j=h* h b s KH +
j=hk=1
k * h bs KH Pπ i,k j,V
b
i,j+1 +
X XX
b
b
H3log3(KS n i=1Ai) log3(KS n i=1Ai) H K
=3c Pδ +3c Pδ αK bj,Var V . (87)
bs K bs KH k h Pπk,V i,j+1
j=hk=1 (cid:28) i,j b (cid:29)
XX
b
b
With elementary inequality Var (V +V′) Var (V)+ Var (V′) for any transition kernel P RS
P P P
and vector V,V′ RS, we further decompose≤ (87) as ∈
∈ p p p
V (s) E Vπ (s)
i,h − π∼ξ i,h
b H3lob gh 3(KS n ii =1Ai) log3(KS n i=1Ai) H K
3c Pδ +3c Pδ αK bj,Var V
≤ bs K bs KH k h Pπk,V i,j+1
j=hk=1 (cid:28) i,j b (cid:29)
XX
b
b
3c
H3log3(KS Pδn i=1Ai)
+3c
log3(KS Pδn i=1Ai) K
αK
H
bj,Var V E Vπ
≤ bs K bs KH k h Pπk,V i,j+1 − π∼ξ i,j+1
k X=1 j X=h(cid:28) bi,j b (cid:16) bh i(cid:17)(cid:29)
b
+3c
log3(KS Pδn i=1Ai) K
αK
H
bj,Var E Vπ
bs KH k h Pπk,V π∼ξ i,j+1
Xk=1 Xj=h(cid:28) bi,j b (cid:16) bh i(cid:17)(cid:29)
3c
H3log3(KS Pδn i=1Ai)
+3c
log3(KS Pδn i=1Ai) H K
αK bj,Var V E Vπ
≤ bs K bs KH k h Pπk,V i,j+1 − π∼ξ i,j+1
j X=h Xk=1 (cid:28) bi,j b (cid:16) bh i(cid:17)(cid:29)
b
D1
+3c
log3(KS Pδn i=1Ai) H |K
αK bj,Var E Vπ
{z
Var E Vπ
}
bs KH k h Pπk,V π∼ξ i,j+1 − Pπk,V π∼ξ i,j+1
Xj=hk X=1 (cid:28) bi,j b (cid:16) bh i(cid:17) bi,j (cid:16) bh i(cid:17)(cid:29)
D2
+3|
c
log3(KS Pδn i=1Ai) H K
αK bj,Var
{z
E Vπ .
}
bs KH k h Pπk,V π∼ξ i,j+1
Xj=hk X=1 (cid:28) bi,j (cid:16) bh i(cid:17)(cid:29)
D3
We no|w control the three terms , {z, separately. }
1 2 3
D D D
Controlling . We can directly obtain the following upper bound on :
1 1
D D
=3c
log3(KS Pδn i=1Ai) H K
αK bj,Var V E Vπ
D1 bs KH k h Pπk,V i,j+1 − π∼ξ i,j+1
j X=h Xk=1 (cid:28) bi,h b (cid:16) bh i(cid:17)(cid:29)
b
3c
log3(KS Pδn i=1Ai) H K
αK bj, Var V E Vπ 1
≤ bs KH k h Pπk,V i,j+1 − π∼ξ i,j+1 ·
j X=h Xk=1 (cid:28) (cid:13) (cid:13) bi,h b (cid:16) bh i(cid:17)(cid:13) (cid:13)∞ (cid:29)
(cid:13) b (cid:13)
3c log3(KS Pδn i=1Ai) H K αK bj,(cid:13) V E Vπ 2 1 (cid:13)
≤ bs KH k h i,j+1 − π∼ξ i,j+1 ∞·
j X=h Xk=1 (cid:28) (cid:13) bh i(cid:13) (cid:29)
(cid:13) (cid:13)b (cid:13) (cid:13)
31(i)
3c
Hlog3(KS Pδn i=1Ai) H K
αK bj, V E Vπ 1
≤ bs K k h i,j+1 − π∼ξ i,j+1 ∞·
Xj=hk X=1 D (cid:13) bh i(cid:13) E
3c
H3log3(KS Pδn i=1Ai)
max V
(cid:13) (cid:13)b
E Vπ
(cid:13)
(cid:13)
(88)
≤ bs K h≤j≤H i,j+1 − π∼ξ i,j+1 ∞
(cid:13) bh i(cid:13)
where (i) follows from the elementary upper boun(cid:13) (cid:13) db V H, E (cid:13) (cid:13) Vπ H for all h j
i,j+1 ∞ ≤ π∼ξ i,j+1 ∞ ≤ ≤ ≤
H. (cid:13) (cid:13) (cid:13) bh i(cid:13)
Before deriving the upper bounds for the terms(cid:13) (cid:13)b
2
an(cid:13) (cid:13)d 3, we(cid:13) (cid:13)first introduce(cid:13) (cid:13)the following auxiliary
D D
lemmas, which will be instrumental in the subsequent derivation.
Lemma 7. For all (i,h) [n] [H], the estimated robust value function E Vπ satisfies the following
∈ × π∼ξ i,h
inequality: bh i
maxE Vπ (s) minE Vπ (s) min 1 ,H h+1 .
s∈S π∼ξ bh i,h i− s∈S π∼ξ bh i,h i≤ (cid:26)σ i − (cid:27)
Proof. See Appendix C.4.4.
With Lemma 7, we have the following lemma on variance base on different transition probability in the
same uncertainty set, and we leave the proof to Appendix C.4.5.
Lemma 8. For a transition kernel P′ RS and any P RS such that P σi(P′), the following bound
∈ ∈ ∈ U
holds for all (i,h,) [n] [H]:
∈ ×
e e
Var E Vπ Var E Vπ min 1 ,H h+1 . (89a)
P′ π∼ξ i,h − P π∼ξ i,h ≤ σ −
(cid:12) (cid:16) bh i(cid:17) e(cid:16) bh i(cid:17)(cid:12) (cid:26) i (cid:27)
(cid:12) (cid:12)
(cid:12) (cid:12)
Controlling . We can directly apply Lemma 8 and arrive at
2
D
Var E Vπ Var E Vπ
Pπk,V π∼ξ i,h+1 − Pπk,V π∼ξ i,h+1
(cid:12) (cid:12) (cid:12)
(cid:12) ≤
Vb ai r,h Pπb k(cid:16)
,V
Eb π∼h
ξ
Vπ i,hi +(cid:17)
1
−Vb ai r,h Pπk(cid:16) E π∼b ξh Vπ i,h+1i(cid:17)(cid:12) (cid:12) (cid:12)
(cid:12)
+(cid:12) (cid:12) (cid:12)
(cid:12)
Vab ri, Ph πb k(cid:16) E π∼b ξh Vπ i,h+1i(cid:17) −Varb Pi π,h k,(cid:16)
V
E πb ∼h
ξ
Vπ i,h+i 1(cid:17)(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12) i,h i,h (cid:12)
(cid:12) b 1 (cid:16) bh i(cid:17) b (cid:16) bh i(cid:17)(cid:12)
(cid:12) (cid:12)
2m(cid:12)in ,H . (cid:12)
≤ σ
(cid:26) i (cid:27)
We insert (90) back to the expression of , and we can obtain that
2
D
=3c
log3(KS Pδn i=1Ai) H K
αK bj,Var E Vπ Var E Vπ
D2 bs KH k h Pπk,V π∼ξ i,j+1 − Pπk,V π∼ξ i,j+1
Xj=hk X=1 (cid:28) bi,j b (cid:16) bh i(cid:17) bi,j (cid:16) bh i(cid:17)(cid:29)
log3(KS n i=1Ai) H
1
3c Pδ bj,2min ,H 1
≤ bs KH h σ
j=h(cid:28) (cid:26) i (cid:27) (cid:29)
X
Hlog3(KS n i=1Ai)
1
=6c Pδ min ,H . (90)
bs
K σ
(cid:26) i (cid:27)
32Controlling . We first apply Lemma 12, and we can directly deduce that
3
D
=3c
log3(KS Pδn i=1Ai) H K
αK bj,Var E Vπ
D3 bs KH k h Pπk,V π∼ξ i,j+1
j X=h Xk=1 (cid:28) bi,j (cid:16) bh i(cid:17)(cid:29)
3c
log3(KS Pδn i=1Ai) H
bj,Var E Vπ
≤ bs KH h K αKPπk,V π∼ξ i,j+1
j X=h(cid:28) Pk=1 k bi,j (cid:16) bh i(cid:17)(cid:29)
Wenowintroducethefollowinglemmaon H bj,Var E Vπ ,whichisanempirical-
j=h h K αKPπk,V π∼ξ i,j+1
transition version of Lemma 16. P (cid:28) Pk=1 k bi,j bh i(cid:29)
Lemma9. Letδ (0,1). Withprobabilityatleast1 δ,thefollowingconditionholdsforall(h,i) [H] [n]:
∈ − ∈ ×
H
bj,Var E Vπ 3H maxE Vπ (s) minE Vπ (s) . (91)
j X=h(cid:28)
h PK k=1αK kP bπ i,k j,V
(cid:16)
π∼ξ
bh
i,j+1 i(cid:17)(cid:29)≤ (cid:18)s∈S π∼ξ
bh
i,h i− s∈S π∼ξ
bh
i,h
i(cid:19)
Proof. See Appendix C.4.6.
Therefore, we can further achieve the following upper bound of by applying Lemma 9:
3
D
3c
log3(KS Pδn i=1Ai) H
bj,Var E Vπ
D3 ≤ bs KH h K αKPπk,V π∼ξ i,j+1
j X=h(cid:28) Pk=1 k bi,j (cid:16) bh i(cid:17)(cid:29)
9c
Hlog3(KS Pδn i=1Ai)
maxE Vπ (s) minE Vπ (s)
≤ bs K s∈S π∼ξ i,h − s∈S π∼ξ i,h
(cid:18) bh i bh i(cid:19)
(i)
Hlog3(KS n i=1Ai)
1
9c Pδ min ,H (92)
≤
bs
K σ
(cid:26) i (cid:27)
where (i) holds due to Lemma 8.
C.2.3 Step 3: summing up the result
We combine the result of (88), (90), (92), yielding
V E Vπ 3c
H3log3(KS Pδn i=1Ai)
1+ + +
i,h − π∼ξ i,h ≤ bs K D1 D2 D3
bh i
b Hlog3(KS n i=1Ai)
1
c Pδ 3H +15min ,H 1
≤
bs
K σ
(cid:18) (cid:26) i (cid:27)(cid:19)
+3c
H3log3(KS Pδn i=1Ai)
max V E Vπ 1.
bs K h≤j≤H i,j+1 − π∼ξ i,j+1 ∞
(cid:13) bh i(cid:13)
Moreover,Lemma 5 implies that V E Vπ = V (cid:13) (cid:13)b E Vπ , which(cid:13) (cid:13)indicates that
i,h − π∼ξ i,h i,h − π∼ξ i,h
bh i (cid:12) bh i(cid:12)
max V b E Vπ (cid:12) (cid:12)b (cid:12) (cid:12)
h∈[H] i,h −− π∼ξ i,h ∞
(cid:13) bh i(cid:13)
(cid:13) (cid:13)b Hlog3(KS n i=1Ai)(cid:13) (cid:13) 1
c Pδ 3H +15min ,H
≤
bs
K σ
(cid:18) (cid:26) i (cid:27)(cid:19)
+3c
H3log3(KS Pδn i=1Ai)
max V E Vπ
bs K h≤j≤H i,j+1 − π∼ξ i,j+1 ∞
(cid:13) bh i(cid:13)
(cid:13) (cid:13)b (cid:13)
(cid:13)
33(i) 18c
H3log3(KS Pδn i=1Ai)
+ 1 max V E Vπ
≤ bs K 2h∈[H] i,h − π∼ξ i,h ∞
(cid:13) bh i(cid:13)
H3log3(KS n i=1Ai) (cid:13) (cid:13)b (cid:13) (cid:13)
36c Pδ
≤
bs
K
where (i) holds by taking K
12c2H3log3(KS n i=1Ai),
and involving the basic facts that V =
≥ b Pδ i,H+1
E Vπ =0. Eventually, we can achieve the following upper bound of term B:
π∼ξ i,H+1 b
bh i
E V⋆,π−i E Vπ 36c
H3log3(KS Pδn i=1Ai)
1. (93)
π∼ξ i,h − π∼ξ i,h ≤ bs K
bh i bh i
C.3 Controlling terms A and C
Inthissection,wederiveanupperboundforthedifferencebetweenthetruevaluefunctionandtheestimated
value function. We consider a more general case involving a given set of policies πk , where
h (h,k)∈[H]×[K]
eitherπk =πk forall(h,k) [H] [K],orπk =π˜⋆ πk forall(h,k) [H] [K]. Additionally,wedefine
h h ∈ × h i × −i,h ∈ × (cid:8) (cid:9)
adistributionoverthesetofpoliciesζ := ζ ,withζ :[H] ∆( ∆b( )),whereζ πk =
{ h }h∈[H] h 7→ S 7→ i∈[n] Ai h h
αK k forb all(h,k) ∈[H] ×[K]. Ourobjectiveib stoderiveanupperboundfor E πQ∼ζ V iπ ,h(s) −E π∼ζ Vπ i,(cid:0)h b(s)(cid:1) ,
where for all s ∈S, E π∼ζ Vπ i,h(s) is defined as (cid:12) (cid:12)
(cid:12)
h i h i(cid:12) (cid:12)
(cid:12)
h i
K K
E Vπ (s) = αKE [rk (s,a )]+ αKE inf E Vπ ,
π∼ζ h i,h i Xk=1 k ai∼π bik ,h(s) i,h i k X=1 k ai∼π bik ,h(s) P∈Uσi (cid:18)P iπ b,h−k ,i s,ai(cid:19)P π∼ζ h i,h+1 i

 b 
with E Vπ (s) = 0. Here, rk (s,a ) represents the empirical estimation of rπ −k i(s,a ), and Pπ −k i
π∼ζ i,H+1 i,h i i,h i i,h,s,ai
b b
h i πk
denotes the empirical estimation of P −i for all (h,s,a ,k) [H] [K]. For notational clarity,
h,s,ai i ∈ ×S ×Ai × b
we define the empirical reward vectorb rπk RS, such that rπk (s)=E [rk (s,a )] for all s .
i,h ∈ i,h ai∼π ik ,h(s) i,h i ∈S
We first introduce the following twoblemmas in terms of ebstimation errorof transitionmodel and reward
b
function:
Lemma 10. Let δ (0,1) and consider any (h,i,k) [H] [n] [K]. With a probability of at least 1 δ,
for any fixed value v∈ ector V RS, where 0 V(s) ∈ H for× all s× , the following inequality holds: −
∈ ≤ ≤ ∈S
K log
18S n i=1AiNHK
log
18S n i=1AiNHK
(cid:12)
(cid:12)P iπ b,h−k i,V V −P iπ b,h−k i,V V
(cid:12)
(cid:12)≤2 Xk=1αK k v u
u
t
(cid:16) P Nδ (cid:17) rVar
P
hπ b−k i(V)+ (cid:16) P Nδ (cid:17)1
(cid:12) b (cid:12)
(cid:12) (cid:12) H2log 18S n i=1AiNKH
P δ
3v 1,
≤ u (cid:16) N (cid:17)
u
t
where Var () is as defined in (34).
Pπ−k
i ·
hb
Proof. See Appendix C.4.7.
Lemma 11. There exists a constant c such that for any fixed pair (h,i) [H] [n], with probability at
r
∈ ×
least 1 δ, the following inequality holds:
−
K K log KS
αKrπk αKrπk c δ 1.
(cid:12) (cid:12)Xk=1 k i b,h− Xk=1 k i b,h (cid:12) (cid:12)≤ r s (cid:0)K (cid:1)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
34Proof. See Appendix C.4.8.
For clarity of presentation, we extend the definitions in (32) and introduce additional notations related
to transitions associated with the estimated value function. With a slight abuse of notation, we define the
matrix notations
Pπk,V
and
Pπk,V
as follows for all (i,h,k) [n] [H] [K]:
i,h i,h ∈ × ×
b b b
Pπk,V :=Pπ −k i,E πb∼ζ[Vπ i,h+1] b and Pπk,V :=Pπ −k i,E π∼ζ[Vπ i,h+1] =argmin E Vπ ,
i b,h i b,h i b,h,s,ai i b,h,s,ai P∈Uρσi (cid:18)P hπ b,−k s,i ai(cid:19)P π∼ζ
h
i,h+1
i
Pπk,V :=Pπ −k i,E π∼ζ[Vπ i,h+1] and Pπk,V :=Pπ −k i,E π∼ζ[Vπ i,h+1] =argmin E Vπ .
i b,h i b,h i b,h,s,ai i b,h,s,ai P∈Uρσi (cid:18)P iπ b,h−k ,i s,ai(cid:19)P π∼ζ
h
i,h+1
i
b b b b b
Additionally, we define the square matrices Pπk,V RS×S and Pπk,V RS×S as:
i b,h ∈ i b,h ∈
Pπk,V :=Ππ ik Pπ −kb i,V
,
Pπk,V :=Ππ ik Pπ −k i,V
.
i,h h i,h i,h h i,h
b b b b b b
At any time step h [H], we habve b
∈
E Vπ E Vπ
π∼ζ i,h − π∼ζ i,h
(cid:2) (cid:3) h i
K K
( =i) αKrπk + αKΠπ ik ,h inf E Vπ
k X=1 k i b,h k X=1 k h b  P∈Uσi (cid:18)P hπ b,−k s,i ai(cid:19)P π∼ζ
(cid:2)
i,h+1 (cid:3)

 
K K
αKrπk αKΠπ ik ,h inf E Vπ
− Xk=1 k i b,h− k X=1 k h b  P∈Uσi (cid:18)P hπ b,−k s,i ai(cid:19)P π∼ζ h i,h+1 i

 b 
K K K K
( =ii) αKrπk + αKPπk,VE Vπ αKrπk αKPπk,VE Vπ (94)
k X=1
k i b,h
k X=1
k i b,h π∼ζ
(cid:2)
i,h+1 (cid:3)−
k X=1
k i b,h−
Xk=1
k i b,h π∼ζ
h
i,h+1
i
b
where (i) holds by the robust Bellman equation in (13) with matrix notation in (32), (ii) arises from
the definition in (84). Moreover, through simple observation, we directly have Pπk,VE Vπ
i,h π∼ζ i,h+1 ≤
Pπk,VE Vπ for all (h,k) [H] [K]. Thus, we further control (94) as b h i
i,h π∼ζ i,h+1 ∈ ×
b
h i
E Vπ E Vπ
π∼ζ i,h − π∼ζ i,h
= K(cid:2) αKr(cid:3) πk + K h αKPi πk,VE Vπ K αKrπk K αKPπk,VE Vπ
Xk=1
k i b,h
k X=1
k i b,h π∼ζ
(cid:2)
i,h+1 (cid:3)−
k X=1
k i b,h−
Xk=1
k i b,h π∼ζ
h
i,h+1
i
K b
= αK rπk rπk + Pπk,VE [Vπ ] Pπk,VE Vπ
k i,h− i,h i,h π∼ζ i,h+1 − i,h π∼ζ i,h+1
Xk=1 (cid:20)(cid:16) b b (cid:17) (cid:16) b b h i(cid:17)
+
Pπk,VE Vπ Pπk,VE Vπ
(cid:18)
i b,h π∼ζ
h
i,h+1 i− i b,h π∼ζ
h
i,h+1
i(cid:19)(cid:21)
K
αK Pπk,VE Vπ b Pπk,VE Vπ
≤ k i,h π∼ζ i,h+1 − i,h π∼ζ i,h+1
Xk=1 (cid:16) b (cid:2) (cid:3) b h i(cid:17)
K
+ αK rπk rπk + Pπk,VE Vπ Pπk,VE Vπ . (95)
Xk=1
k
(cid:20)(cid:12) (cid:12) (cid:12)
i b,h− i b,h
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
i b,h π :∼ =ζ
aζh
i,h+1 i− bi b,h π∼ζ
h
i,h+1
i(cid:12) (cid:12) (cid:12) (cid:12)(cid:21)
i,h
| {z }
35Applying (95) recursively leads to
H j−1 K
E Vπ E Vπ αKPπk,V aζ , (96)
π∼ζ
(cid:2)
i,h (cid:3)− π∼ζ
h
i,h i≤ Xj=h"
r Y=h Xk=1
k i b,r !# i,j
where the inequality holds by adopting the following notations:
h−1 K
αKPπk,V
=I,
k i,r
" r=h k=1 b !#
Y X
j−1 K K K K
αKPπk,V
=
αKPπk,V αKPπk,V αKPπk,V
.
"
r=h k=1
k i b,r !#
k=1
k i b,h !·
k=1
k i b,h+1 !···
k=1
k i b,j−1 !
Y X X X X
Next, similar to (95), we can achieve that
E Vπ E Vπ
π∼ζ i,h − π∼ζ i,h
K h i K (cid:2) (cid:3) K K
( =i) αKrπk + αKPπk,VE Vπ αKrπk αKPπk,VE Vπ
k X=1
k i b,h
Xk=1
k i b,h π∼ζ
h
i,h+1 i−
k X=1
k i b,h−
Xk=1
k i b,h π∼ζ
(cid:2)
i,h+1
(cid:3)
K b
= αK rπk rπk + Pπk,VE Vπ Pπk,VE Vπ
k X=1 k " (cid:16) i b,h− i b,h (cid:17) (cid:18) i b,h π∼ζ h i,h+1 i− i b,h π∼ζ h i,h+1 i(cid:19)
b
+ Pπk,VE Vπ Pπk,VE Vπ
i b,h π∼ζ i,h+1 − i b,h π∼ζ i,h+1 #
(cid:16) h i
(cid:2)
(cid:3)(cid:17)
K
(ii) αK Pπk,VE Vπ Pπk,VE Vπ
≤ k i,h π∼ζ i,h+1 − i,h π∼ζ i,h+1
k X=1 (cid:16) b h i b (cid:2) (cid:3)(cid:17)
K
+ αK rπk rπk + Pπk,VE Vπ Pπk,VE Vπ ,
where (i) holds dXk u= e1 tok (cid:20) ro(cid:12) (cid:12) (cid:12) bi b u,h st− Bi b e,h ll(cid:12) (cid:12) (cid:12) man(cid:12) (cid:12) (cid:12) (cid:12) ei b, qh uatiπ o∼ nζ ,h ani d,h+ (1 iii )− hob li b d,h s duπ e∼ζ toh ti h,h e+1 di i(cid:12) (cid:12) (cid:12) (cid:12)r(cid:21) ect observation that
Pπk,VE Vπ Pπk,VE Vπ
. Then following the routine of achieving (96), we can obtain
i,h π∼ζ i,h+1 ≤ i,h π∼ζ i,h+1
thbat
h i
b
h i
H j−1 K
E Vπ E Vπ αKPπk,V aζ . (97)
π∼ζ
h
i,h i− π∼ζ
(cid:2)
i,h (cid:3)≤ Xj=h"
r Y=h Xk=1
k i b,r !# i,j
Summing up (97) and (96), one has
E Vπ E Vπ max E Vπ E Vπ ,E Vπ E Vπ
π∼ζ i,h − π∼ζ i,h ≤ π∼ζ i,h − π∼ζ i,h π∼ζ i,h − π∼ζ i,h
(cid:12) (cid:12) h i (cid:2) (cid:3)(cid:12) (cid:12) n H (cid:2)j−1 (cid:3) K h i h H j−i 1 K (cid:2) (cid:3)o
(cid:12) (cid:12) max αKPπk,V aζ , αKPπk,V aζ . ,
≤  Xj=h"
r Y=h Xk=1
k i b,r !# i,j
j
X=h"
r Y=h k X=1
k i b,r !# i,j 

(98)
 
where the max operator is taken entry-wise for vectors. To continue, we apply Lemma 10 and Lemma 11,
and we can obtain the following upper bound on aζ for all (i,j) [n] [H]:
i,j ∈ ×
K
aζ = αK rπk rπk + Pπk,VE Vπ Pπk,VE Vπ
i,h
k X=1
k
(cid:20)(cid:12) (cid:12) (cid:12)
i b,h− i b,h
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
i b,h π∼ζ
h
i,h+1 i− bi b,h π∼ζ
h
i,h+1
i(cid:12) (cid:12) (cid:12) (cid:12)(cid:21)
36K log
18S n i=1AiKNnH
≤2 Xk=1αK k v u
u
t
(cid:16) P Nδ (cid:17) rVar Pπ
h
bk (cid:16)E π∼ζ hVπ i,h+1
i(cid:17)
+
log
18S Pn
i=1
δAiKNnH
1+c
log(KS δnH)
1,
(cid:16) N (cid:17) r s K
holds with probability at least 1 δ.
−
C.3.1 Controlling the first term in (98)
To simplify notation,let us introduce some additionalsymbols. Recallthat e represents the standardbasis
s
vector in S-dimensional space associated with the s-th component. We define
j−1 K
dh =e and dj =e⊤ αKPπk,V for j =h+1,...,H. (99)
h s h s k i,r
" r=h k=1 b !#
Y X
With these notations in place, for any s , we consider
∈S
H
E Vπ (s) E Vπ (s) = e ,E Vπ E Vπ = dj,aζ .
π∼ζ i,h − π∼ζ i,h s π∼ζ i,h − π∼ζ i,h h i,j
(cid:2) (cid:3) h i D (cid:2) (cid:3) h iE j X=hD E
Applying Lemma 10, we obtain
E Vπ (s) E Vπ (s)
π∼ζ i,h − π∼ζ i,h
(cid:2) (cid:3) h i
H K log
18S n i=1AiKNH
≤ Xj=h*dj h,2 Xk=1αK k v u u
t
(cid:16) P Nδ (cid:17) rVar Pπ j bk (cid:16)E π∼ζ hVπ i,j+1 i(cid:17)+
+
log
18S Pn
i=1
δAiKNH
+c
log KS δnH
(cid:16) N (cid:17) r s K
(cid:0) (cid:1)
Hlog
18S Pn
i=1
δAiKNH
+c
H2log KS δnH
≤ (cid:16) N (cid:17) r s K
(cid:0) (cid:1)
H K log
18S n i=1AiKNH
+
j
X=h*dj h,2 Xk=1αK k v u
u
t
(cid:16) P Nδ (cid:17) rVar Pπ
j
bk (cid:16)E π∼ζ hVπ i,j+1 i(cid:17)+. (100)
By applying the triangle inequality, we can further decompose the term of interest as follows:
E Vπ (s) E Vπ (s)
π∼ζ i,h − π∼ζ i,h
H(cid:2)
log
18S(cid:3) Pn
i=1
δAiKhNH +i
c
H2log KS δnH
≤ (cid:16) N (cid:17) r s K
(cid:0) (cid:1)
H K log
18S n i=1AiKNH
+ j X=h*dj h,2 Xk=1αK k v u u
t
(cid:16) P Nδ (cid:17) rVar P iπ b,k j,V (cid:16)E π∼ζ hVπ i,j+1 i(cid:17)+
B1
H|
K log
18S n i=1AiKNH {z }
+ j X=h2 Xk=1αK k v u u
t
(cid:16) P Nδ (cid:17) *dj h, s (cid:12) (cid:12)Var Pπ i b,k j,V (cid:16)E π∼ζ hVπ i,j+1 i(cid:17)−Var Pπ j bk (cid:16)E π∼ζ hVπ i,j+1 i(cid:17)(cid:12) (cid:12)+.
(cid:12) (cid:12)
(cid:12) B2 (cid:12)
(101)
| {z }
37We then analyze the bounds for the terms and separately.
1 2
B B
Controlling . First,weintroducethefollowinglemmaandcorrespondinginequalitytoestablishcontrol
1
B
over the term K αK Var E Vπ :
k=1 k Pπk,V π∼ζ i,j+1
r i b,j (cid:16) h i(cid:17)
P
Lemma 12. For any transition kernels P ,...,P RS, and any weight a ,...,a [0,1] with a +...+
1 m 1 m 1
∈ ∈
a =1, one has
m
m
a i Var Pi(V)
≤
Var m i=1aiPi(V),
Xi=1 p q P
where V denote any fixed value vector V RS with 0 V(s) H for all s .
∈ ≤ ≤ ∈S
Proof. Initially, since f(x)=√x is a concave function, we have
m m
a Var (V)= a Var (V).
i Pi
v
i Pi
i=1 ui=1
X p uX
t
Moreover,according to the definition of variance in (33), we obtain that
m m m m m
a Var (V)= a (E (V V) (E V E V)) a E (V V) a E V a E V ,
i Pi i Pi
◦ −
Pi
◦
Pi
≤
i Pi
◦ −
i Pi
!◦
i Pi
!
i=1 i=1 i=1 i=1 i=1
X X X X X
where the last inequality holds due to the elementary fact that f(x) = x2 is a convex function. Therefore,
we have proven the result of the lemma.
With Lemma 12, we can further control with
1
B
H K log
18S n i=1AiKNnH
B1 =
j
X=h*dj h,2 Xk=1αK k v u
u
t
(cid:16) P Nδ (cid:17) rVar Pπ
i
b,k j,V (cid:16)E π∼ζ hVπ i,j+1
i(cid:17)+
log
18S n i=1AiKNnH
H
≤2v
u (cid:16)
P Nδ
(cid:17)
dj h, Var
K αKPπk,V
E
π∼ζ
Vπ
i,j+1
u t Xj=h(cid:28) r Pk=1 k i b,j (cid:16) h i(cid:17)(cid:29)
log
18S n i=1AiKNnH
H
≤2v
u (cid:16)
P Nδ (cid:17)vH dj h,Var
K αKPπk,V
E
π∼ζ
Vπ
i,j+1
. (102)
u t u u Xj=h(cid:28) Pk=1 k i b,j (cid:16) h i(cid:17)(cid:29)
t
The last inequality holds due to Cauchy-Schwartzinequality. To further achieve the upper bound of , we
1
B
introduce the following lemma of H dj,Var E Vπ :
j=h h K αKPπk,V π∼ζ i,j+1
P
(cid:28) Pk=1 k i b,j (cid:16) h i(cid:17)(cid:29)
Lemma 13. Consider any δ (0,1). With probability at least 1 δ, one has
∈ −
H
(h,i) [H] [n]: dj,Var E Vπ
∀ ∈ × h K αKPπk,V π∼ζ i,j+1
Xj=h(cid:28) Pk=1 k i b,j (cid:16) h i(cid:17)(cid:29)
3H maxE Vπ (s) minE Vπ (s) 1+2H
log(18S Pn
i=1
δAinKNH
) .
≤ s∈S π∼ζ i,h+1 − s∈S π∼ζ i,h+1  s N 
(cid:18) h i h i(cid:19)
 (103)
Proof. See Appendix C.4.9.
38Lemma 14. For all (i,h) [n] [H], the estimated robust value function E Vπ satisfies the following
∈ × π∼ζ i,h
inequality: h i
maxE Vπ (s) minE Vπ (s) min 1 ,H h+1 .
s∈S π∼ζ h i,h i− s∈S π∼ζ h i,h i≤ (cid:26)σ i − (cid:27)
Proof. The proof of Lemma 14 closely parallels that of Lemma 7. Therefore, we omit the details here for
brevity and clarity.
Apply Lemma 13 to (102), we arrive at
log
18S n i=1AiKNnH
H
B1
≤2v
u (cid:16)
P Nδ (cid:17)vH dj h,Var
K αKPπk,V
E
π∼ζ
Vπ
i,j+1
u t u u Xj=h(cid:28) Pk=1 k i b,j (cid:16) h i(cid:17)(cid:29)
t
3H2 maxE Vπ (s) minE Vπ (s)
≤s s∈S π∼ζ i,h+1 − s∈S π∼ζ i,h+1
(cid:18) h i h i(cid:19)
2vlog 18S Pn i=1 δAiKNnH
1+2H
log(18S Pn i=1 δAinKNH )
· u (cid:16) N (cid:17) · s N 
u
u
t  
(i)
2vlog 18S Pn i=1A δinKNnH
v3H2min
1
,H h+1 1+2H
log(18S Pn
i=1
δAinKNH
)
≤ u
u
(cid:16) N (cid:17)u
u
(cid:26)σ
i
− (cid:27) s N 
t u
t  
H2min {1/σ i,H }log 18S Pn i=1 δAiKNnH
6v , (104)
≤ u N(cid:16) (cid:17)
u
t
where (i) holds by applying Lemma 7 and Lemma 14, and the final inequality follows by taking N
≥
4H2log 18S n i=1AiKnNH .
P δ
(cid:16) (cid:17)
Controlling . Initially, with similar analysis as Lemma 8, we have the following lemma:
2
B
Lemma 15. For transition kernel P′ RS and any P RS such that P σi(P′), the following bounds
∈ ∈ ∈ U
are established for all (i,h) [n] [H]:
∈ ×
e e
Var E Vπ Var E Vπ min 1 ,H h+1 .
P′ π∼ζ i,h − P π∼ζ i,h ≤ σ −
(cid:12) (cid:16) h i(cid:17) e(cid:16) h i(cid:17)(cid:12) (cid:26) i (cid:27)
(cid:12) (cid:12)
With Lemma 15(cid:12), we observe that (cid:12)
Var E Vπ Var E Vπ
(cid:12)
Pπ jk π∼ζ i,j+1 − Pπ i,k j,V π∼ζ i,j+1
(cid:12)
(cid:12) b (cid:16) h i(cid:17) b (cid:16) h i(cid:17)(cid:12)
(cid:12) (cid:12)( =i) Ππi Var E Vπ Var E (cid:12) (cid:12)Vπ
(cid:12)
(cid:12)
j P jπ b−k i
(cid:16)
π∼ζ
h
i,j+1 i(cid:17)− P iπ b,jk,V
(cid:16)
π∼ζ
h
i,j+1 i(cid:17)!(cid:12)
(cid:12)
(cid:12) (cid:12)
(ii)(cid:12) (cid:12) Var E Vπ Var E Vπ 1 (cid:12) (cid:12)
≤ (cid:13)
(cid:13)
P jπ b−k i
(cid:16)
π∼ζ
h
i,j+1 i(cid:17)− P iπ b,jk,V
(cid:16)
π∼ζ
h
i,j+1 i(cid:17)(cid:13)
(cid:13)∞
(cid:13) (cid:13)
(cid:13) 1 (cid:13)
m(cid:13)in ,H h+1 1, (cid:13) (105)
≤ σ −
(cid:26) i (cid:27)
where (i) and (ii) follows from the definition of matrix notations Ππ (cf A.1) and Pπk ,Pπk,V (cf A.1), and
j j i,j
the last inequality holds by applying Lemma 15 with P′ =Pπ −k i ,P =Pπk,V for allb (s,ab ) .
j,s,ai i,j,s,ai i ∈S×Ai
b
e
39Plugging back (105) to (101), it can be verified that
H K log
18S n i=1AiKNnH
B2 = j X=h2 Xk=1αK k v u u
t
(cid:16) P Nδ (cid:17) *dj h, s (cid:12) (cid:12)Var Pπ i b,k j,V (cid:16)E π∼ζ hVπ i,j+1 i(cid:17)−Var Pπ j bk (cid:16)E π∼ζ hVπ i,j+1 i(cid:17)(cid:12) (cid:12)+
(cid:12) (cid:12)
H
2
K αKvlog
18S Pn
i=1
δAiKNnH
dj,
(cid:12)
min
1
,H 1
(cid:12)
≤ j=h k=1 k u u (cid:16) N (cid:17) * h s (cid:26)σ i (cid:27) +
X X t
H2min 1 ,H log 18S n i=1AiKNnH
2v
σi P δ
(106)
≤ u n o N(cid:16) (cid:17)
u
t
Consequently, combining (104) and (106), (101) can be bounded by
E Vπ (s) E Vπ (s)
π∼ζ i,h − π∼ζ i,h
H(cid:2)
log
18S(cid:3) Pn
i=1
δAiKhNnH +i
c
H2log(KS δnH)
+ +
≤ (cid:16) N (cid:17) r s K B1 B2
≤
Hlog (cid:16)18S
P
Nn i=1 δAiKNnH
(cid:17)
+c
r
sH2log K(KS δnH)
+8v
uH2min nσ1 i,H olog N(cid:16)18S Pn i=1 δAiKNnH
(cid:17)
u
t
≤c
r
sH2log K(KS δnH)
+12v
uH2min nσ1 i,H olog N(cid:16)18S Pn i=1 δAiKNnH
(cid:17), (107)
u
t
where the last inequality holds by taking N 4H2log 18S n i=1AiKNnH .
≥ P δ
(cid:16) (cid:17)
C.3.2 Controlling the second term in (98)
To do so, similar to (99), we define
j−1 K
wh =e and wj =e⊤ αKPπk,V j =h+1, ,H. (108)
h s h s "
r=h k=1
k i b,r !# ∀ ···
Y X
With the above notations in mind, following the routine of (100) gives: for any s ,
∈S
E Vπ (s) E Vπ (s)
π∼ζ i,h − π∼ζ i,h
(cid:2) (cid:3) h i
H K log
18S n i=1AiKNnH
≤ Xj=h(cid:28)w hj,2 Xk=1αK k v u u
t
(cid:16) P Nδ (cid:17) rVar Pπ j bk (cid:16)E π∼ζ hVπ i,j+1
i(cid:17)
+
log
18S Pn
i=1
δAiKNnH
1+c
log(KS δnH)
1
(cid:16) N (cid:17) r s K
(cid:29)
Hlog
18S Pn
i=1
δAiKNnH
+c
H2log(KS δnH)
≤ (cid:16) N (cid:17) r s K
H K log
18S n i=1AiKNnH
+
j
X=h*w hj,2 Xk=1αK k v u
u
t
(cid:16) P Nδ (cid:17) rVar Pπ
j
bk (cid:16)E π∼ζ hVπ i,j+1 i(cid:17)+. (109)
40Furthermore, following the routine established in (101), we can decompose the expression as follows:
E Vπ (s) E Vπ (s)
π∼ζ i,h − π∼ζ i,h
H(cid:2)
log
18S(cid:3) Pn
i=1
δAiKhNnH +i
c
H2log(KS δnH)
≤ (cid:16) N (cid:17) r s K
H K log
18S n i=1AiKNnH
+
j
X=h*w hj,2 Xk=1αK k v u
u
t
(cid:16) P Nδ (cid:17) rVar Pπ
j
bk (cid:16)E π∼ζ hVπ i,j+1
i(cid:17)+
(i) Hlog
18S Pn
i=1
δAiKNnH
+c
H2log(KS δnH)
≤ (cid:16) N (cid:17) r s K
H K log
18S n i=1AiKNnH
+
j
X=h*w hj,2 Xk=1αK k v u
u
t
(cid:16) P Nδ (cid:17) rVar Pπ
i
b,k j,V (cid:0)E π∼ζ (cid:2)V iπ ,j+1
(cid:3)(cid:1)+
B3
|
H K log
18S n i=1{Az iKNnH }
+ j X=h*w hj,2 Xk=1αK k v u u
t
(cid:16) P Nδ (cid:17) s (cid:12) (cid:12)Var Pπ j bk (cid:0)E π∼ζ (cid:2)V iπ ,j+1 (cid:3)(cid:1)−Var Pπ i b,k j,V (cid:0)E π∼ζ (cid:2)V iπ ,j+1 (cid:3)(cid:1)(cid:12) (cid:12)+
(cid:12) (cid:12)
(cid:12) B4 (cid:12)
|
H K log
18S n i=1AiKNnH {z }
+
j
X=h*w hj,2 Xk=1αK k v u
u
t
(cid:16) P Nδ (cid:17) rVar Pπ
j
bk (cid:16)E π∼ζ hVπ i,j+1 i−E π∼ζ (cid:2)V iπ ,j+1
(cid:3)(cid:17)+
(110)
B5
where(|i)holdsduetothetriangleinequalityandth{zefundamentalinequality Var (V +V′) } Var (V)+
P P
≤
Var (V′) for any transition kernel P RS and vectors V,V′ RS.
P ∈ ∈ p p
Next, we will control the three main terms , , in (110) separately as outlined below:
p B3 B4 B5
Controlling . Initially, we apply Lemma 12, and we can obtain the following upper bound of :
3 3
B B
H K log
18S n i=1AiKNnH
B3 =
j
X=h*w hj,2 Xk=1αK k v u
u
t
(cid:16) P Nδ (cid:17) rVar Pπ
i
b,k j,V (cid:0)E π∼ζ (cid:2)V iπ ,j+1
(cid:3)(cid:1)+
log
18S n i=1AiKNnH
H
≤2v
u
u
t
(cid:16)
P Nδ
(cid:17)
Xj=h*w hj, rVar
PK k=1αK kPπ
i
b,k j,V
(cid:0)E
π∼ζ
(cid:2)V iπ
,j+1
(cid:3)(cid:1)+.
We further apply Cauchy-Schwartzinequality, and we can obtain that
log
18S n i=1AiKNnH
H
B3
≤2v
u
u
t
(cid:16)
P Nδ
(cid:17)
Xj=h*w hj, rVar
PK k=1αK kPπ
i
b,k j,V
(cid:0)E
π∼ζ
(cid:2)V iπ
,j+1
(cid:3)(cid:1)+
log
18S n i=1AiKNnH
H
≤2v
u (cid:16)
P Nδ (cid:17)vH w hj,Var
K αKPπk,V
E
π∼ζ
V iπ
,j+1
u
t
u
u
j X=h(cid:28) Pk=1 k i b,j
(cid:0) (cid:2)
(cid:3)(cid:1)(cid:29)
t
In addition, to further bound the term of interest, we introduce the following lemma and inequalities for
H wj,Var E Vπ .
j=h h K αKPπk,V π∼ζ i,j+1
P
(cid:28) Pk=1 k i b,j
(cid:0) (cid:2)
(cid:3)(cid:1)(cid:29)
41Lemma 16. For any joint policy π, we have for all (h,i) [H] [n]:
∈ ×
H
wj,Var E Vπ 3H maxE Vπ (s) minE Vπ (s) . (111)
Xj=h(cid:28)
h PK k=1αK kP iπ b,k j,V
(cid:0)
π∼ζ
(cid:2)
i,j+1 (cid:3)(cid:1)(cid:29)≤ (cid:18)s∈S π∼ζ
(cid:2)
i,h (cid:3)− s∈S π∼ζ
(cid:2)
i,h
(cid:3)(cid:19)
Proof. See Appendix C.4.10.
Lemma 17. For all (i,h) [n] [H], the estimated robust value function E Vπ satisfies the following
∈ × π∼ζ i,h
inequality: h i
1
maxE Vπ (s) minE Vπ (s) min ,H h+1 .
s∈S π∼ζ i,h − s∈S π∼ζ i,h ≤ (cid:26)σ i − (cid:27)
(cid:2) (cid:3) (cid:2) (cid:3)
Proof. The proof of Lemma 17 closely parallels that of Lemma 7. Therefore, we omit the details here for
brevity and clarity.
Then applying Lemma 16 and Lemma 17 yields
log
18S n i=1AiKNnH
H
B3
≤2v
u (cid:16)
P Nδ (cid:17)vH w hj,Var
K αKPπk,V
E
π∼ζ
V iπ
,j+1
u
t
u
u
Xj=h(cid:28) Pk=1 k i b,j
(cid:0) (cid:2)
(cid:3)(cid:1)(cid:29)
t
log
18S n i=1AiKNnH
≤2v
u (cid:16)
P Nδ
(cid:17)
s3H2 m s∈a SxE
π∼ζ
V iπ ,h,σi(s) −m s∈i SnE
π∼ζ
V iπ ,h,σi(s)
u t (cid:18) h i h (cid:17)(cid:21)
H2min 1/σ ,H log 18S n i=1AiKNnH
{ i } P δ
4v , (112)
≤ u N(cid:16) (cid:17)
u
t
where the last inequality follows from Lemma 17.
Controlling and With similar analysis as Lemma 8, we have the following lemma:
4 5
B B
Lemma 18. For any joint policy π, transition kernel P′ RS, and any P RS such that P σi(P′),
∈ ∈ ∈ U
the following bounds are established for all (i,h) [n] [H]:
∈ ×
e e
1
Var P′ E π∼ζ V iπ ,h −Var P E π∼ζ V iπ ,h ≤min σ ,H −h+1 .
(cid:26) i (cid:27)
(cid:12) (cid:0) (cid:2) (cid:3)(cid:1) e(cid:0) (cid:2) (cid:3)(cid:1)(cid:12)
We apply Lemma(cid:12) 18, and we can directly obtain the follow(cid:12) ing upper bound of :
4
B
H K log
18S n i=1AiKNnH
B4 = j X=h*w hj,2 Xk=1αK k v u u
t
(cid:16) P Nδ (cid:17) s (cid:12) (cid:12)Var Pπ j bk (cid:0)E π∼ζ (cid:2)V iπ ,j+1 (cid:3)(cid:1)−Var Pπ i b,k j,V (cid:0)E π∼ζ (cid:2)V iπ ,j+1 (cid:3)(cid:1)(cid:12) (cid:12)+
(cid:12) (cid:12)
H
wj,2
K αKvlog
18S Pn
i=1
δAiKNnH (cid:12)
min
1
,H 1
(cid:12)
≤ j=h* h k=1 k u u (cid:16) N (cid:17) s (cid:26)σ i (cid:27) +
X X t
H2min 1/σ ,H log 18S n i=1AiKNnH
{ i } P δ
=2v (113)
u N(cid:16) (cid:17)
u
t
Then the remainder of the proof shall focus on . Recalling the definition in (110), one has
5
B
H K log
18S n i=1AiKNnH
B5 = Xj=h*w hj,2 Xk=1αK k v u
u
t
(cid:16) P Nδ (cid:17) rVar Pπ
j
bk (cid:16)E π∼ζ hVπ i,j+1 i−E π∼ζ (cid:2)V iπ ,j+1
(cid:3)(cid:17)+
42H K log
18S n i=1AiKNnH
≤ Xj=h*w hj,2 Xk=1αK k v u u
t
(cid:16) P Nδ (cid:17) r(cid:13) (cid:13)Var Pπ j bk (cid:16)E π∼ζ hVπ i,j+1 i−E π∼ζ (cid:2)V iπ ,j+1
(cid:3)(cid:17)(cid:13)
(cid:13)∞1 +
log
18S n i=1AiKNnH
H
(cid:13) (cid:13)
≤2v
u (cid:16)
P Nδ
(cid:17)
w hj, E
π∼ζ
Vπ
i,j+1
−E
π∼ζ
V iπ
,j+1
∞1
u t Xj=hD (cid:13) (cid:13) h i (cid:2) (cid:3)(cid:13) (cid:13) E
H2log 18S n i=1AiKNnH (cid:13) (cid:13)
≤2v
u (cid:16)
P
N
δ
(cid:17)
hm ≤ja ≤x
H
E
π∼ζ
Vπ
i,j+1
−E
π∼ζ
V iπ
,j+1
∞. (114)
u
t (cid:13) (cid:13) h i (cid:2) (cid:3)(cid:13) (cid:13)
Summing up (112), (113), and (114) and inserti(cid:13)ng back to (110), we conclude (cid:13)
E Vπ (s) E Vπ (s)
π∼ζ i,h − π∼ζ i,h
H(cid:2)
log
18S(cid:3) Pn
i=1
δAiKhNnH +i
c
H2log KS δnH
+ + +
≤ (cid:16) N (cid:17) r s K B3 B4 B5
(cid:0) (cid:1)
≤
Hlog (cid:16)18S
P
Nn i=1 δAiKNnH
(cid:17)
+c
r
sH2log KKS δnH
+6v
uH2min {1/σ i,H }lo Ng (cid:16)18S Pn i=1 δAiKNnH
(cid:17)
(cid:0) (cid:1) u
t
H2log 18S n i=1AiKNnH
+2v
u (cid:16)
P
N
δ
(cid:17)
hm ≤ja ≤x
H
E
π∼ζ
Vπ
i,j+1
−E
π∼ζ
V iπ
,j+1 ∞
(115)
u t (cid:13) (cid:13) h i (cid:2) (cid:3)(cid:13) (cid:13)
(cid:13) (cid:13)
C.3.3 Summing up the results: upper bound for term A and C
Inserting (107) and (115) back into (98), we observe that
E Vπ E Vπ
π∼ζ i,h − π∼ζ i,h
(cid:12) (cid:12) max(cid:2) E(cid:3) Vπ h E i(cid:12) (cid:12) Vπ ,E Vπ E Vπ
(cid:12)≤ π∼ζ i,h − π∼(cid:12)ζ i,h π∼ζ i,h − π∼ζ i,h
n (cid:2) (cid:3) h i h i (cid:2) (cid:3)o
≤max (c
r
sH2log
K
(cid:0)KS δnH
(cid:1)1+12v
u
uH2min nσ1 i,H olog N(cid:16)18S Pn i=1 δAiKNnH (cid:17)1,Hlog (cid:16)18S
P
Nn i=1 δAiKNnH
(cid:17)1
t
+2v
u
u
tH2log (cid:16)18S
P
Nn i=1 δAiKNnH
(cid:17)
hm ≤ja ≤x
H
(cid:13)
(cid:13)E
π∼ζ
hVπ
i,j+1
i−E
π∼ζ
(cid:2)V iπ
,j+1
(cid:3)(cid:13)
(cid:13)∞1+c
r
sH2log
K
(cid:0)KS δnH
(cid:1)1
H2min 1 ,H log 18S n i=1AiKN(cid:13)nH (cid:13)
+6v
{σi } P δ
1 ,
u N(cid:16) (cid:17)
u )
t
which indicates that
max E Vπ E Vπ (116)
h∈[H] π∼ζ i,h − π∼ζ i,h ∞
(cid:13)
(cid:13) (cid:2) (cid:3)
h i(cid:13)
(cid:13)
≤c
r
s(cid:13) H2log K(KS δnH)
+12v
uH2(cid:13)min nσ1 i,H olog N(cid:16)18S Pn i=1 δAiKNnH
(cid:17)
+
Hlog (cid:16)18S
P
Nn i=1 δAiKNnH
(cid:17)
u
t
H2log 18S n i=1AiKNnH
+2v
u (cid:16)
P
N
δ
(cid:17)
hm ∈a [Hx
]
E
π∼ζ
Vπ
i,h+1
−E
π∼ζ
V iπ
,h+1 ∞
u t (cid:13) (cid:13) h i (cid:2) (cid:3)(cid:13) (cid:13)
(cid:13) (cid:13)
43( ≤i)
c
r
sH2log K(KS δnH)
+12v
uH2min nσ1 i,H olog N(cid:16)18S Pn i=1 δAiKNnH
(cid:17)
u
+
Hlog
18S Pn
i=1
δAiKNnt H
+ 1 max E Vπ E Vπ
(cid:16) N (cid:17) 2h∈[H] π∼ζ i,h − π∼ζ i,h ∞
(cid:13) (cid:13) h i (cid:2) (cid:3)(cid:13) (cid:13)
≤2c
r
sH2log K(KS δnH)
+24v
uH2min nσ1(cid:13) i,H olog N(cid:16)18S Pn i=1 δAiKNnH(cid:13)
(cid:17)
+
2Hlog (cid:16)18S
P
Nn i=1 δAiKNnH
(cid:17)
u
t (117)
where (i) holds when N 4H2log 18S n i=1AiNnH , and invoking the basic fact that E Vπ =
≥ P δ π∼ζ i,H+1
E Vπ =0. With (117), we c(cid:16)an achieve the fo(cid:17)llowing upper bound on term A and termhC: i
π∼ζ i,H+1
E (cid:2) V⋆,π−(cid:3) i E Vπ i⋆,π−i
π∼ξ i,h − π∼ξ i,h
e
bh i bh i
≤2c
r
sH2log K(KS δnH)
1+24v
uH2min nσ1 i,H olog N(cid:16)18S Pn i=1 δAiKNnH
(cid:17)1+
2Hlog (cid:16)18S
P
Nn i=1 δAiKNnH
(cid:17)1
u
t (118)
E Vπ E Vπ
π∼ξ i,h − π∼ξ i,h
bh i b(cid:2) (cid:3)
≤2c
r
sH2log K(KS δnH)
1+24v
uH2min nσ1 i,H olog N(cid:16)18S Pn i=1 δAiKNnH
(cid:17)1+
2Hlog (cid:16)18S
P
Nn i=1 δAiKNnH
(cid:17)1
u
t (119)
C.3.4 Summing up the results
Summing up the results in (118), (93), (119), we can achieve the upper bound of our target:
H3log3(KS n i=1Ai) H2log(KSnH)
E V⋆,π−i E Vπ 36c Pδ 1+4c δ 1
π∼ξ i,1 − π∼ξ i,1 ≤ bs K r s K
b(cid:2) (cid:3) b(cid:2) (cid:3)
4Hlog 18S n i=1AiKNnH H2min 1 ,H log 18S n i=1AiKNnH
+
P δ
1+48v
σi P δ
1.
(cid:16) N (cid:17) u n o N(cid:16) (cid:17)
u
t
Therefore, there exists a constant C, such that when N and K satisfies:
1 18S n A KNnH 1 KS n A H 1
N CH2min ,H log i=1 i , K CH3log3 i=1 i
≥ min σ δ ǫ2 ≥ δ ǫ2
(cid:26) 1≤i≤n i (cid:27) (cid:18) P (cid:19) (cid:18) P (cid:19)
we achieve max E V⋆,π−i E Vπ ǫ 1 with probability at least 1 δ. Therefore, the total
i∈[n] π∼ξ i,1 − π∼ξ i,1 ≤ · −
number of samples we need is at least
b(cid:2) (cid:3) b(cid:2) (cid:3)
n Smax A H6 1
N =HS KN = ˜ 1≤i≤n i min ,H .
all O ǫ4 min σ
i=1 (cid:18) (cid:26) 1≤i≤n i (cid:27)(cid:19)
X
Thus, we finish the proof of Theorem 2.
C.4 Proof of auxiliary lemmas
C.4.1 Proof of Lemma 4
Before proving Lemma 4, we first introduce the following lemma regarding the properties of the learning
rate.
44Lemma 19 (Li et al. (2023, Lemma 1)). For any k 1, one has
≥
k
2c logK
α =1, αk =1, max αk α . (120a)
1 i 1≤i≤k i ≤ k
i=1
X
In addition, if k c logK+1 and c 24, then one has
α α
≥ ≥
1
max αk . (120b)
1≤i≤k/2 i ≤ K6
Wewillnowprovethe lemmawithinductionargument. Initially,thebasestepH+1triviallyholdstrue,
since we have
V =E V⋆,π−i =0.
i,H+1 π∼ξ i,H+1
bh i
Next, we assume that the lemma holdsbfor step h+1, namely
V E V⋆,π−i
i,h+1 ≥ π∼ξ i,h+1
bh i
and attempt to justify the validity of Lembma 4 for step h. Let l denote l = qk (s, ), k 1, then the
k k − i,h · ∀ ≥
update rule of Algorithm 2 can be viewed as the FTRL algorithm applied to the loss vectors l .
k k∈[K]
{ }
According to the definition of η and α , we have
k k∈[K] k k∈[K]
{ } { }
2
η α k 2+c logK k 1
k = k = − α − =1 α >(1 α )2. (121)
k k
η α k 1+c logK ≥ k 1+c logK − −
(cid:18) k+1(cid:19) k−1 − α − α
This property (121) permits us to invoke Theorem 3 to obtain
K K
max αKqk (s,a ) αK πk ,qk (s, )
ai∈Ai k i,h i − k i,h i,h ·
k=1 k=1
X X (cid:10) (cid:11)
K K
= max αK πk (s),l αKl (a )
ai∈Ai( k i,h k − k k i )
k=1 k=1
X (cid:10) (cid:11) X
K
5 η α logA
αK k k Var qk (s, ) + i +τ
≤ 3 k 1 α
k
π ik ,h(s) i,h · η
K+1
i,h
k X=2 − (cid:16) (cid:17)
where τ is defined as
i,h
5 K η2α2 η α 1
τ := αKη q1 2 + 3 αK k k qk 3 1 k k qk > +3αKη2 q1 3 . (122)
i,h 3 1 2 i,h ∞ k (1 α )2 i,h ∞ 1 α i,h ∞ 3 1 2 i,h ∞
( k=2 − k (cid:18) − k (cid:19))
(cid:13) (cid:13) X (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
According to the definition of α K and η K , we have the following fact:
{ k }k=1 { k }k=1
c logK 1 cαlogK = 1 1 , if k 2,
1 α =1 α − 1+cαlogK 1+cαlogK ≥ 2cαlogK ≥ (123a)
− k − k −1+c αlogK ≥(1
−
K/c 2α +l co αg lK
ogK
= K+2cK
αlogK ≥
21, if k ≥K/2+1,
logK logK α logK 2c log2K
k α
η α = α α = . (123b)
k k k k
sα k−1H · ≤ rα kH ·
r
H ≤s kH
Therefore, we can re-control max K αKqk (s,a ) K αK πk ,qk (s, ) with
ai∈Ai k=1 k i,h i − k=1 k i,h i,h ·
D E
P P
K K
max αKqk (s,a ) αK πk ,qk (s, ) (124)
ai∈Ai k i,h i − k i,h i,h ·
k=1 k=1
X X (cid:10) (cid:11)
45K
5 η α logA
αK k k Var qk (s, ) + i +τ
≤ 3 k 1 α
k
π ik ,h(s) i,h · η
K+1
i,h
k X=2 − (cid:16) (cid:17)
(i) 5K/2 2c 1.5 log2K
α αKVar qk (s, )
≤ 3 √kH k π ik ,h(s) i,h ·
Xk=2 (cid:0) (cid:1) (cid:16) (cid:17)
C1
| 20 K c{zlog2K } logA
+ αK α Var qk (s, ) + i + τ , (125)
3 k s KH π ik ,h(s) i,h · η
K+1
i,h
k=XK/2+1 (cid:16) (cid:17) C3
C2
|{z}
Now we separately control the four terms , , in (125). | {z }
1 2 3
C C C
• For term , we have
1
C
K/2 αKlog2K K/2 log2K
k Var qk (s, ) Var qk (s, )
√kH π ik ,h(s) i,h · ≤ K6√kH π ik ,h(s) i,h ·
Xk=2 (cid:16) (cid:17) Xk=2 (cid:16) (cid:17)
K/2 log2K qk (s, ) 2 H3/2log2K K/2 1
≤ K6√kH i,h · ∞ ≤ K6 √k
k=2 k=2
X (cid:13) (cid:13) X
2H3/2log2K(cid:13) (cid:13) 2H3/2log2K
K/2 , (126)
≤ K6 · ≤ K5
p
where the third inequality holds due to the elementary bound qk (s, ) H.
i,h · ∞ ≤
• For term 2, we have (cid:13) (cid:13)
C (cid:13) (cid:13)
logA α H 2c Hlog2A
i K α i
=logA , (127)
i
η
K+1
slogK ≤s K
where the first equality holds due to the definition of η .
K+1
• For term , we initially have
3
C
η α
2cαlog2K
8c3Hlog4K
k k qk kH H = α . (128)
1 α i,h ∞ ≤ q 1 · s k
− k 2cαlogK
(cid:13) (cid:13)
(cid:13) (cid:13)
Clearly,theright-handsideof (128)isupperboundedby1/3forallk obeyingk c H2log4 K forsome
≥ 9 δ
large enough constant c >0. Consequently, one can derive
9
τ = 5 αKη q1 2 + 3 K αK η k2α2 k qk 3 1 η kα k qk > 1 +3αKη2 q1 3
i,h 3 1 2 i,h ∞ k (1 α )2 i,h ∞ 1 α i,h ∞ 3 1 2 i,h ∞
( k=2 − k (cid:18) − k (cid:19))
(cid:13) (cid:13) X (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
5
(cid:13) logK(cid:13)
q1 2 + 2c αlogK
2 (cid:13) 3c9H(cid:13)2log4K
δ
η2α2(cid:13)
qk
(cid:13)
3 + 3 logK
q1(cid:13)
3
(cid:13)
≤ 3K6 H i,h ∞ K6  k k i,h ∞ K6 H i,h ∞
r (cid:13) (cid:13) (cid:0) (cid:1)  Xk=2 (cid:13) (cid:13)  (cid:13) (cid:13)
24c3 αlog4K
(cid:13)K (cid:13)
1 H3 
(cid:13) (cid:13)

(cid:13) (cid:13)
≤ K6H ( k )
k=1
X
24c3H3log5K 1
α , (129)
≤ K6 ≤ K4
where the second line comes from (123) and the fact that K/2>c Hlog4 K .
9 δ
46Combining previous three items, we can obtain that
K K
max αKqk (s,a ) αK πk ,qk (s, )
ai∈Ai k i,h i − k i,h i,h ·
k=1 k=1
X X (cid:10) (cid:11)
5(2c )1.5 2H3/2log2K 20 c log2K K 2c Hlog2A 1
α + α αKVar qk (s, ) + α i +
≤ 3 · K5 3 s KH k π ik ,h(s) i,h · s K K4
k=KX/2+1 (cid:16) (cid:17)
c log3(KA ) K c Hlog3(KA )
10 α i αKVar qk (s, ) +2 α i , (130)
≤ s KH k π ik ,h(s) i,h · s K
k X=1 (cid:16) (cid:17)
According to the definition of qk (s,a ) in the update rule of Algorithm 2, we have
i,h i
K K
max αK rk (s,a )+ inf V αKE rk (s,a )+ inf V
ai∈Ai
k X=1
k " i,h i P∈Uσi(P ik ,h,s,ai)P i,h+1 #−
k X=1
k ai∼π ik ,h(s) " i,h i P∈Uσi(P ik ,h,s,ai)P i,h+1 #
K K b b
= max αKqk (s,a ) αK πk ,qk (s, )
ai∈Ai k i,h i − k i,h i,h ·
k=1 k=1
X X (cid:10) (cid:11)
c log3(KA ) K c Hlog3(KA )
10 α i αKVar qk (s, ) +2 α i =β (s)
≤ s KH k π ik ,h(s) i,h · s K i,h
Xk=1 (cid:16) (cid:17)
Moreover,according to the induction hypothesis, we have for all s
∈S
K
E V⋆,π−i(s) = max αK rk (s,a )+ inf E V⋆,π−i
π∼ξ
bh
i,h
i
ai∈Ai
Xk=1
k " i,h i P∈Uσi(P ik ,h,s,ai)P π∼ξ
bh
i,h+1 i#
K
max αK rk (s,a )+ inf V
≤ai∈Ai
Xk=1
k " i,h i P∈Uσi(P ik ,h,s,ai)P i,h+1 #
K b
αKE rk (s,a )+ inf V +β (s)
≤ k ai∼π ik ,h(s) " i,h i P∈Uσi(Pk )P i,h+1 # i,h
k X=1 i,h,s,ai
V (s). b
i,h
≤
Thus, we finished the proof of thbe lemma.
C.4.2 Proof of Lemma 5
We will prove the lemma with induction argument. Initially, the base step H +1 trivially holds true, since
we have
V =E Vπ =0.
i,H+1 π∼ξ i,H+1
bh i
Next, we assume that the lemma holdsbfor step h+1, namely
V E Vπ .
i,h+1 ≥ π∼ξ i,h+1
bh i
According to the definition of V and E b Vπ , we have
i,h π∼ξ i,h
bh i
bK
E Vπ (s) = αKE rk (s,a )+ inf E Vπ
π∼ξ i,h k ai∼π ik ,h(s) " i,h i P∈Uσi(Pk )P π∼ξ i,h+1 #
bh i Xk=1 i,h,s,ai bh i
47K
αKE rk (s,a )+ inf V
≤ k ai∼π ik ,h(s) " i,h i P∈Uσi(Pk )P i,h+1 #
Xk=1 i,h,s,ai
K b
αKE rk (s,a )+ inf V +β (s).
≤ k ai∼π ik ,h(s) " i,h i P∈Uσi(Pk )P i,h+1 # i,h
Xk=1 i,h,s,ai
b
Since, we also trivially have E Vπ (s) H h+1, we can deduce that for all s
π∼ξ i,h ≤ − ∈S
bh i
E Vπ (s)
π∼ξ i,h
bh K i
min αKE rk (s,a )+ inf V +β (s),H h+1 =V (s).
≤ ( k ai∼π ik ,h(s) " i,h i P∈Uσi(Pk )P i,h+1 # i,h − ) i,h
k X=1 i,h,s,ai
b b
Thus, we finished the proof of the lemma.
C.4.3 Proof of Lemma 6
Recall that for all s , bonus term β (s) is defined as
i,h
∈S
log3(KS n i=1Ai) K
β (s)=c Pδ αK Var qk (s, ) +H . (131)
i,h bs KH k π ik ,h(·|s) i,h ·
Xk=1 n (cid:0) (cid:1) o
For any k [K], we have the following inequality for Var qk (s, ) :
∈ π ik ,h(·|s) i,h ·
(cid:16) (cid:17)
Var qk (s, )
π ik ,h(·|s) i,h ·
2Var (cid:0) rk ((cid:1) s, ) +2Var Pπ −k i,V (s′ s, )V (s′)
≤ π ik ,h(·|s) i,h · π ik ,h(·|s) i,h b | · i,h+1 !
s′
(cid:0) (cid:1) X
b b 2
(i) 2+2 πk (a s)Pπ −k i,V ( s,a ) V V πk (a s)Pπ −k i,V ( s,a )V
≤  i,h i | i,h b ·| i i,h+1 ◦ i,h+1 − i,h i | i,h b ·| i i,h+1 ! 
a Xi∈Ai (cid:16) (cid:17) a Xi∈Ai
 b b b b b 
=2+2 e ,Var V . (132)
s πk,V i,h+1
P
(cid:28) i,h b (cid:29)
b
where e denotes an S-dbimensional standard basis supported on the s-th element, and (i) holds due to the
s
elementary fact that rk (s,a ) 1 and Pπ −k i,V (s′ s,a ) 1 for all s,s′ ,a . We insert the
i,h i ≤ i,h b | i ≤ ∈ S i ∈ Ai
result of (132) back to(cid:12) (cid:12) (131), an(cid:12) (cid:12)d rewrite t(cid:12) (cid:12)he result in vector(cid:12) (cid:12) form, we can achieve that
(cid:12) (cid:12) (cid:12)b (cid:12)
(cid:12) (cid:12)
log3(KS n i=1Ai)
β 3c Pδ H 1+Var V
i,h
≤
bs
KH ·
Pπk,V i,h+1
(cid:18) i,h b (cid:19)
b
b
C.4.4 Proof of Lemma 7
To prove Lemma 7, we start by analyzing the value function of policy π under uncertainty set σ . We first
i
establish bounds on min E Vπ (s) :
s∈S π∼ξ i,h
bh i
minE Vπ (s)
s∈S π∼ξ i,h
bh i
K
=min αK E rk (s,a ) +E inf E Vπ (s)
s∈S Xk=1 k  ai∼π ik ,h(s) (cid:2) i,h i (cid:3) ai∼π ik ,h(s) P∈Uσi (cid:16)P ik ,h,s,ai(cid:17)P π∼ξ bh i,h+1 i
minE
Vπ
(s) .
 
≥ s∈S π∼ξ i,h+1
bh i
48This follows from the robust Bellman equation (13).
Next, we examine max E Vπ (s) :
s∈S π∼ξ i,h
bh i
maxE Vπ (s)
s∈S π∼ξ i,h
bh i
K
=max αK E rk (s,a ) +E inf E Vπ (s)
s∈S Xk=1 k  ai∼π ik ,h(s) (cid:2) i,h i (cid:3) ai∼π ik ,h(s) P∈Uσi (cid:16)P ik ,h,s,ai(cid:17)P π∼ξ bh i,h+1 i
  
K
1+ αK max inf E Vπ (s) . (133)
≤ Xk=1 k (s,ai)∈S×Ai P∈Uσi (cid:16)P hπ ,− s,i ai(cid:17)P π∼ξ bh i,h+1 i
 
We now construct an auxiliary distribution vector P′ RS by strictly reducing some elements of
h,s,ai ∈
Pπ−i
such that:
h,s,ai
0 P′ Pk and Pk (s′) P′ (s′)= P′ Pk =σ . (134)
≤ h,s,ai ≤ i,h,s,ai i,h,s,ai − h,s,ai h,s,ai − i,h,s,ai 1 i
s′∈S
X (cid:13) (cid:13)
(cid:13) (cid:13)
Let e s⋆
i,h
denote the standard basis vector supported on s⋆ i,h. We can show:
1 ⊤ 1 1 ⊤
2 P h′ ,s,ai +σ i e s⋆ i,h −P ik ,h,s,ai ≤ 2 P h′ ,s,ai −P ik ,h,s,ai 1+ 2 σ i e s⋆ i,h ≤σ i, (135)
(cid:13) (cid:13)1 (cid:13) (cid:13)1
(cid:13) h i (cid:13) (cid:13) (cid:13) (cid:13) h i (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
where the firs(cid:13)t inequality follows from the tr(cid:13)iangle inequality of the total var(cid:13)iation distan(cid:13)ce.
From (135), we conclude that:
inf E Vπ
P π∼ξ i,h+1
P∈Uσi Pk
(cid:16) i,h,s,ai(cid:17) bh i
≤ P h′ ,s,ai +σ i e s⋆ i,h ⊤ E π∼ξ Vπ i,h+1 (136)
(cid:18)
P′
Eh Vi π(cid:19) bh
+σ E
i
Vπ (s⋆ )
≤ h,s,ai 1 π∼ξ i,h+1 ∞ i π∼ξ i,h+1 i,h+1
≤(cid:13) (cid:13)(1 −σ i)(cid:13) (cid:13)m s∈a(cid:13) (cid:13)
(cid:13)
SxE π∼b ξh Vπ i,h+i 1(cid:13) (cid:13) (cid:13)(s) +σ im s∈i Snbh E
π∼ξ
Vπ i,h+1(s)i .
bh i bh i
Substituting (136) into (133) yields:
maxE Vπ (s) 1+(1 σ )maxE Vπ (s) +σ minE Vπ (s) . (137)
s∈S π∼ξ i,h ≤ − i s∈S π∼ξ i,h+1 i s∈S π∼ξ i,h+1
bh i bh i bh i
Combining (133) and (137) gives:
maxE Vπ (s) minE Vπ (s)
s∈S π∼ξ i,h − s∈S π∼ξ i,h
bh i bh i
1+(1 σ ) maxE Vπ (s) minE Vπ (s)
≤ − i s∈S π∼ξ i,h+1 − s∈S π∼ξ i,h+1
(cid:18) bh i bh i(cid:19)
1+(1 σ ) 1+(1 σ ) maxE Vπ (s) minE Vπ (s)
≤ − i − i s∈S π∼ξ i,h+2 − s∈S π∼ξ i,h+2
1
(1(cid:20)
σ )H−h
(cid:18)
1
bh i bh i(cid:19)(cid:21)
i
− − . (138)
≤···≤ σ ≤ σ
i i
Combiningthis withthe basicfactthatmax E Vπ (s) min E Vπ (s) H h+1,we
s∈S π∼ξ i,h − s∈S π∼ξ i,h ≤ −
complete the proof. bh i bh i
49C.4.5 Proof for Lemma 8
We introduce the following notation for the value function at time h:
h [H], Vspan :=E Vπ (s) minE Vπ (s′) , (139)
∀ ∈ i,h π∼ξ i,h −s′∈S π∼ξ i,h
bh i bh i
π
which normalizes the value function V . This definition leads to the following bound:
i,h
span 1
V min ,H h+1 , (140)
(cid:13) i,h (cid:13)∞ ≤ (cid:26)σ i − (cid:27)
(cid:13) (cid:13)
a result derived using Lemma 7. W(cid:13)ith th(cid:13)is notation established, we now consider any transition kernel
P′ RS andanyP RS suchthatP σi(P′). Forall(i,h) [n] [H],weanalyzethevariancedifference
∈ ∈ ∈U ∈ ×
between the value functions under these kernels:
e e
Var E Vπ Var E Vπ = Var Vspan Var Vspan
P′ π∼ξ i,h − P π∼ξ i,h P′ i,h − P i,h
(cid:12) (cid:12) (cid:12) (cid:16) bh i(cid:17) e(cid:16) bh i(cid:17)(cid:12) (cid:12) (cid:12)≤(cid:12) (cid:12)P −P(cid:16) ′ 1 V(cid:17)s i,p han ∞e(cid:16) (cid:17)(cid:12) (cid:12)
(cid:13) (cid:13) 1(cid:13) (cid:13) (cid:13) (cid:13) 2 1
σ(cid:13)e min(cid:13) (cid:13),H (cid:13)h+1 min ,H h+1 .
i
≤ σ − ≤ σ −
(cid:18) (cid:26) i (cid:27)(cid:19) (cid:26) i (cid:27)
(141)
C.4.6 Proof of Lemma 9
Analogous to Appendix C.4.9, we introduce some auxiliary values and rewardfunctions to control
H
bj,Var E Vπ
h K αKPπk,V π∼ξ i,j+1
Xj=h(cid:28) Pk=1 k bi,j
(cid:16) bh
i(cid:17)(cid:29)
as below for any time step h and agent i.
Definition 3. For any time step h [H] and the i-th agent, we denote Vmin := min E Vπ (s) as
∈ h s∈S π∼ξ i,h
the minimum value of all the entries in vector E Vπ . We further define V′ :=E Vπ bh Vmini 1 as
π∼ξ i,h h π∼ξ i,h − h
the truncated value function. Eventually for rewardbhfunctiion, we define rm i,hin = K k=1αK
k
E bh
ai∼π ik
,i hr ik ,h( ·,a i)+
min min
V V 1 as the truncated reward function.. P
h+1− h
(cid:16) (cid:17)
Then applying the robust Bellman’s consistency equation in (13) gives
K K
V′ =E Vπ Vmin 1= αKE rk (,a )+ αKPπk,VE Vπ Vmin 1
h π∼ξ i,h − h k ai∼π ik ,h i,h · i k i,h π∼ξ i,h+1 − h
bh i k X=1 Xk=1 bh i
K b K
= αKE rk (,a )+ Vmin Vmin 1+ αKPπk,V V′ (142)
k ai∼π ik ,h i,h · i h+1− h k i,h h+1
k X=1 (cid:16) (cid:17) k X=1
K b
=rmin+ αKPπk,V V′ . (143)
i,h k i,h h+1
k=1
X
b
The above fact leads to
Var E Vπ
K αKPπk,V π∼ξ i,h+1
k=1 k i,h
(i) P b (cid:16) b ′h i(cid:17)
= Var V
K αKPπk,V h+1
k=1 k i,h
P (cid:16) (cid:17)
b
50K K K
=
αKPπk,V V′ V′ αKPπk,V V′ αKPπk,V V′
k i,h h+1◦ h+1 − k i,h h+1 ◦ k i,h h+1
Xk=1 (cid:16) (cid:17) (cid:0)Xk=1 (cid:1) (cid:0)Xk=1 (cid:1)
b b b
where (i) follows from the fact that Var (V b1) = Var (V) for any value vector
K αKPπk,V − K αKPπk,V
k=1 k i,h k=1 k i,h
V RS and scalar b. According to (143)Pand (13b), we have P b
∈
K
Var E Vπ = αKPπk,V V′ V′ V′ rmin ◦2
K αKPπk,V π∼ξ i,h+1 k i,h h+1◦ h+1 − h− i,h
Pk=1 k bi,h (cid:16) bh i(cid:17) k X=1 (cid:16) (cid:17) (cid:16) (cid:17)
K b
= αKPπk,V V′ V′ V′ V′ +2V′ rmin rmin rmin
k i,h h+1◦ h+1 − h◦ h h◦ i,h − i,h ◦ i,h
k X=1 (cid:16) (cid:17)
K b
αKPπk,V V′ V′ V′ V′
+2
V′
1,
≤ k i,h h+1◦ h+1 − h◦ h k hk∞
k X=1 (cid:16) (cid:17)
b
where the last inequality arises from rmin K αKE rk (,a ) 1 since Vmin Vmin 0 by
i,h ≤ k=1 k ai∼πk i,h · i ≤ h+1 − h ≤
definition.
Consequently, combining (155) and the definP ition of bj in (99), we arrive at
h
H
bj,Var E Vπ
h K αKPπk,V π∼ξ i,j+1
Xj=hD Pk=1 k bi,h (cid:16) bh i(cid:17)E
H K
= bj ⊤ αKPπk,V V′ V′ V′ V′ +2 V′ 1
h k i,h j+1◦ j+1 − j ◦ j k hk∞ !
j X=h(cid:16) (cid:17) k X=1 (cid:16) (cid:17)
H K b
(i) bj ⊤ αKPπk,V V′ V′ V′ V′ +2H V′
≤ " h k i,h j+1◦ j+1 − j ◦ j !# h ∞
j X=h (cid:16) (cid:17) Xk=1 (cid:16) (cid:17) (cid:13) (cid:13)
(cid:13) (cid:13)
b (cid:13) (cid:13)
′ ′ ′
where (i) and the last inequality hold by the fact V V V . Further according
h ∞ ≥ h+1 ∞ ≥ ··· ≥ H ∞
to basic calculus, we have (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
H
bj,Var E Vπ
h K αKPπk,V π∼ξ i,j+1
j X=hD Pk=1 k bi,h (cid:16) bh i(cid:17)E
H
= bj+1 ⊤ V′ V′ (bj)⊤ V′ V′ +2H V′
h j+1◦ j+1 − h j ◦ j h ∞
j X=h(cid:20)(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)(cid:21) (cid:13) (cid:13)
(cid:13) (cid:13)
bH+1 V′ V′ +2H V′ (cid:13) (cid:13)
≤ h 1 H+1◦ H+1 ∞ h ∞
≤3(cid:13) (cid:13)H V(cid:13) (cid:13)′
h
(cid:13) (cid:13)
(cid:13)
∞. (cid:13) (cid:13)
(cid:13)
(cid:13) (cid:13)
(cid:13)
(cid:13) (cid:13)
(cid:13)
(144)
(cid:13) (cid:13)
(cid:13) (cid:13)
C.4.7 Proof of Lemma(cid:13)10(cid:13)
πk ,V πk ,V
To prove the inequality involving P −i V and P −i V, we start by analyzing the absolute difference
i,h i,h
between these terms: b b
b
πk ,V πk ,V
P −i V P −i V = inf V inf V
(cid:12) (cid:12) (cid:12) (cid:12) i b,h,s,ai − bi b,h,s,ai (cid:12) (cid:12) (cid:12) (cid:12) ( =i)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)P E∈ aiU ∼σ π bi (cid:18) ik ,hP hπ αb,−k s ∈,i [a mi(cid:19) inP sVm (sa− ),x mP a∈ xsU Vσi ((cid:18) s)P b ]i (cid:20)π b,h−k P,i s hπ b,a ,−k si ,i(cid:19) aiP [V] α(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) −σ i (cid:16)α −m si ′n[V] α(s′)
(cid:17)(cid:21)
(cid:12)
(cid:12)
51E max Pπ −k i [V] σ α min[V] (s′)
≤E
ai∼−
π ik
,ha αi ∈∼ [mπ bik i, nh sα Vm∈ ([ sm
a
),xi mns axV s( Vs) (, sm )]a (cid:12)x PsV
hπ
b,−k( ss ,i) a]
i(cid:20)
[Vbi ]b, αh, −s,a Pi
iπ b,h−k ,i
sα ,a− i[Vi
]
α(cid:16)
(cid:12),− s′ α
(cid:17) ((cid:21) 1(cid:12) (cid:12)
(cid:12) (cid:12)45)
b (cid:12) (cid:12)
where (i) follows from applying the robust Bellman equatio(cid:12)n (13), and thbe last inequ(cid:12)ality uses the fact that
(cid:12) (cid:12)
the maximum operator is 1-Lipschitz.
πk πk
Next, we apply Bernstein’s inequality to bound the difference between P −i [V] and P −i [V] for
h,s,ai α i,h,s,ai α
fixed α, k, and (s,a ). With probability at least 1 δ, we have: b b
i
−
b
Pπ −k i [V] Pπ −k i [V] 2log 2 δ Var ([V] )+ 2Hlog 2 δ . (146)
(cid:12)
(cid:12)
h b,s,ai α − i b,h,s,ai α (cid:12) (cid:12)≤s N (cid:0) (cid:1)
r
P hπ b,−k s,i ai α 3N (cid:0) (cid:1)
(cid:12) b (cid:12)
Toextendthis(cid:12) boundtoall(s,a ),weus(cid:12)eauniformboundoveranε -netforα. Thenetsize N 3H
i 1 | ε1|≤ ε1
allows us to apply the union bound:
πk πk πk πk
P −i [V] P −i [V] max P −i [V] P −i [V] +ε
(cid:12)
h b,s,ai α − i b,h,s,ai α (cid:12)≤α∈Nε1(cid:12) h b,s,ai α − i b,h,s,ai α
(cid:12)
1
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)
b (cid:12)
(cid:12)
2log(cid:12)
(cid:12)
2S n i=1Ai|Nεb 1|Kn (cid:12)
(cid:12) 2Hlog
2S n i=1Ai|Nε1|Kn
P δ P δ
≤v u u
t
(cid:16) N (cid:17) rVar P hπ b,−k s,i ai(V)+ (cid:16) 3N (cid:17) +ε 1
2log
2S n i=1AiNKn
Hlog
2S n i=1AiNKn
P δ P δ
≤v u u
t
(cid:16) N (cid:17) rVar P hπ b,−k s,i ai(V)+ (cid:16) N (cid:17), (147)
Hlog
2S n i=1AiNKn
where the last steps use that ε 1 = (cid:16) P 3Nδ (cid:17) and |N ε1|≤9N.
Inserting this back into (145) gives:
2log
2S n i=1AiNKn
Hlog
2S n i=1AiNKn
πk ,V πk ,V P δ P δ
(cid:12) (cid:12)P i b,h−i V −P i b,h−i V (cid:12) (cid:12)≤v u u
t
(cid:16) N (cid:17) rVar P hπ b−k i(V)+ (cid:16) N (cid:17)1
(cid:12) b (cid:12)
(cid:12) (cid:12) H2log 2S n i=1AiNKn
P δ
3v 1.
≤ u (cid:16) N (cid:17)
u
t
This completes the proof by showing that the bound holds uniformly over all (s,a ) .
i i
∈S×A
C.4.8 Proof of Lemma 11
BeforeprovingLemma11,wefirststateamodifiedversionoftheFreedmaninequalityformartingales,which
is crucial for our analysis.
Theorem 5. Suppose Y = n X R, where X is a real-valued scalar sequence such that
n k=1 k ∈ { k }
X P R and E[X X ]=0 for all k 1
k k j j<k
| |≤ |{ } ≥
for some constant R>0. Define
n
W := E [X2],
n k−1 k
k=1
X
where E denotes the conditional expectation given X . For any κ>0, with probability at least 1 δ,
k−1 j j<k
{ } −
the following holds:
3n 3n 2 3n
Y 8W log +5Rlog κW + +5R log . (148)
n n n
| |≤ δ δ ≤ κ δ
r (cid:18) (cid:19)
52Proof. Suppose deterministically that W σ2 for some σ2. According to Li et al. (2024), with probability
n
≤
at least 1 δ, we have
−
σ2 2K 4 2K
Y 8max W , log + Rlog .
| n |≤s n 2K δ 3 δ
(cid:26) (cid:27)
for any positive integer K 1. Utilizing the trivial bound W nR2, set σ2 =nR2 and K =log n. Then:
≥ n ≤ 2
4log n 4 4log n
Y 8max W ,R2 log 2 + Rlog 2
n n
| |≤ { } δ 3 δ
r
3n 3n 4 3n
8W log + 8R2log + Rlog
n
≤ δ δ 3 δ
r r
3n 3n
8W log +5Rlog ,
n
≤ δ δ
r
where we used 4log n 3n for any integer n 1. This establishes the firstinequality in (148). The second
2 ≤ ≥
inequality follows from the elementary inequality 2ab a2+b2.
≤
To prove Lemma 11, we apply Lemma 5. Define
2c logK
R:= max αK πk (s),rk (s, ) max αK max πk (s) rk α ,
k∈[K] k i,h i,h · ≤ k∈[K] k k∈[K]k i,h k1 k i,hk∞ ≤ K
(cid:26) (cid:27)(cid:26) (cid:27)
(cid:12) (cid:10) (cid:11)(cid:12)
where the first line u(cid:12)ses Lbemma 19. We fu(cid:12)rther define b
K
W = (αK)2Var πk (s),rk (s, )
K k h,k−1 i,h i,h ·
k=1
X (cid:0)(cid:10) (cid:11)(cid:1)
K b
max αK αKVar πk (s),rk (s, )
≤ (cid:26)k∈[K] k (cid:27)(
k=1
k h,k−1 i,h i,h · )
X (cid:0)(cid:10) (cid:11)(cid:1)
2c logK K b
α αKVar (rk (s)),
≤ K k h,k−1 i,h
k=1
X
where we use variance operator Var [] to denote the variance conditional on what happens before the
h,k−1
·
beginning of the k-th round of data collection for step h. Applying Freedman’s inequality (Lemma 5) with
κ = Klog(K/δ), we obtain
1
p K K
αKE rk (s,a ) αKE rπ −k i(s,a )
(cid:12) k ai∼π ik ,h i,h i − k ai∼π ik ,h i b,h i (cid:12)
(cid:12) (cid:12)k X=1
b
k X=1
b
(cid:12)
(cid:12)
(cid:12) 2 3K (cid:12)
(cid:12) κ 1W K + +5R 1 log (cid:12)
≤ κ δ
(cid:18) 1 (cid:19) (cid:18) (cid:19)
log3 K K 1 10c logK 3K
2c δ αKVar rk (s) + 2 + α log
≤ α s K (cid:0) (cid:1)k=1 k h,k−1 i,h sKlog K δ K ! (cid:18) δ (cid:19)
X (cid:0) (cid:1)
log3 K K log 3K(cid:0) (cid:1)
2c δ αKVar rk (s) +4 δ ,
≤ α s K k h,k−1 i,h s K
(cid:0) (cid:1)k=1 (cid:0) (cid:1)
X (cid:0) (cid:1)
with probability at least 1 δ. Taking a union bound over all s , there exists an absolute constant c
r
− ∈ S
such that
K
log(KS/δ)
αK rπk rπk c 1.
k i,h− i,h ≤ r K
Xk=1 (cid:12) b b (cid:12) r
(cid:12) (cid:12)
(cid:12) (cid:12)
53C.4.9 Proof of Lemma 13
In this section, we want to take the accessible range of the robust value function E Vπ into consid-
π∼ζ i,j+1
eration when controlling H dj,Var E Vπ . Towards thish , we ini troduce some
j=h h K αKPπk,V π∼ζ i,j+1
auxiliary values and rewaPrdfunc(cid:28) tions asPbk e= lo1 wk . i b,j (cid:16) h i(cid:17)(cid:29)
Definition 4. For any time step h [H] and the i-th agent, we denote Vmin := min E Vπ (s) as
∈ h s∈S π∼ζ i,h
the minimum value of all the entries in vector E Vπ . Wefurther define V′ :=E Vπ h Vmini 1 as
π∼ζ i,h h π∼ζ i,h − h
the truncated value function. Eventually for rewardhfunctiion, we define rmin = K αKEh irk (,a )+
i,h k=1 k ai∼π ik ,h i,h · i
min min
V V 1 as the truncated reward function.. P b
h+1− h
(cid:16) (cid:17)
′
With above notation, we introduce the following fact of V :
h
V′ =E Vπ Vmin 1
h π∼ζ i,h − h
K h i K
( =i) αKE rk (,a )+ αKPπk,VE Vπ Vmin 1
k ai∼π ik ,h i,h · i k i b,h π∼ζ i,h+1 − h
k X=1
b
Xk=1 h i
K K b
= αKE rk (,a )+ αKPπk,VE Vπ
k ai∼π ik ,h i,h · i k i,h π∼ζ i,h+1
k X=1
b
Xk=1 b h i
K K
+ αKPπk,V αKPπk,V E Vπ Vmin 1
(cid:16)k X=1
k i b,h −
k X=1
k i b,h
(cid:17)
π∼ζ
h
i,h+1 i− h
K b
= αKE rk (,a )+ Vmin Vmin 1
k ai∼π ik ,h i,h · i h+1− h
k X=1
b
(cid:16) (cid:17)
K K K
+ αKPπk,VV′ + αKPπk,V αKPπk,V E Vπ
Xk=1
k i b,h h+1
(cid:16)k X=1
k i b,h −
k X=1
k i b,h
(cid:17)
π∼ζ
h
i,h+1
i
K Kb K
=rmin+ αKPπk,VV′ + αKPπk,V αKPπk,V E Vπ , (149)
i,h
Xk=1
k i b,h h+1
(cid:16)Xk=1
k i b,h −
Xk=1
k i b,h
(cid:17)
π∼ζ
h
i,h+1
i
b
where (i) holds by the robust Bellman’s consistency equation of E Vπ . With the above fact in hand,
π∼ζ i,h
we control Var E Vπ as follows: h i
K αKPπk,V π∼ζ i,h+1
k=1 k i,h
P b (cid:16) h i(cid:17)
Var E Vπ
K αKPπk,V π∼ζ i,h+1
k=1 k i,h
P b (cid:16) h i(cid:17)
(i) ′
= Var V (150)
K αKPπk,V h+1
k=1 k i,h
P b (cid:16) (cid:17)
K K K
=
αKPπk,V V′ V′ αKPπk,VV′ αKPπk,VV′
,
k i,h h+1◦ h+1 − k i,h h+1 ◦ k i,h h+1
k X=1 b (cid:16) (cid:17) (cid:0)k X=1 b (cid:1) (cid:0)Xk=1 b (cid:1)
where (i) follows from the fact that Var (V b1) = Var (V) for any value vector
K αKPπk,V − K αKPπk,V
k=1 k i,h k=1 k i,h
V RS and scalar b, Additionally accordPing to (1b49), we have P b
∈
Var E Vπ
K αKPπk,V π∼ζ i,h+1
k=1 k i,h
P b (cid:16) h i(cid:17)
K K K
= αKPπk,V V′ V′ V′ rmin αKPπk,V αKPπk,V E Vπ ◦2
Xk=1
k i b,h
(cid:16)
h+1◦ h+1 (cid:17)−
(cid:16)
h− i,h −
(cid:16)Xk=1
k i b,h −
Xk=1
k i b,h
(cid:17)
π∼ζ
h
i,h+1
i(cid:17)
b
54K K K
= αKPπk,V V′ V′ V′ V′ +2V′ rmin+ αKPπk,V αKPπk,V E Vπ
Xk=1
k i b,h
(cid:16)
h+1◦ h+1 (cid:17)− h◦ h h◦
(cid:16)
i,h
(cid:16)Xk=1
k i b,h −
Xk=1
k i b,h
(cid:17)
π∼ζ
h
i,h+1
i(cid:17)
K K b
rmin+ αKPπk,V αKPπk,V E Vπ ◦2
−
(cid:16)
i,h
(cid:16)Xk=1
k i b,h −
Xk=1
k i b,h
(cid:17)
π∼ζ
h
i,h+1
i(cid:17)
b
Furthermore, we have
Var E Vπ
K αKPπk,V π∼ζ i,h+1
k=1 k i,h
P b (cid:16) h i(cid:17)
K K K
= αKPπk,V V′ V′ V′ V′ +2V′ rmin+ αKPπk,V αKPπk,V E Vπ
Xk=1
k i b,h
(cid:16)
h+1◦ h+1 (cid:17)− h◦ h h◦
(cid:16)
i,h
(cid:16)Xk=1
k i b,h −
Xk=1
k i b,h
(cid:17)
π∼ζ
h
i,h+1
i(cid:17)
K K b
rmin+ αKPπk,V αKPπk,V E Vπ ◦2
−
(cid:16)
i,h
(cid:16)Xk=1
k i b,h −
Xk=1
k i b,h
(cid:17)
π∼ζ
h
i,h+1
i(cid:17)
K b K K
(i) αKPπk,V V′ V′ V′ V′ +2 V′ 1+ αKPπk,V αKPπk,V E Vπ
≤
Xk=1
k i b,h
(cid:16)
h+1◦ h+1 (cid:17)− h◦ h
(cid:13) (cid:13)
h
(cid:13)
(cid:13)∞
(cid:16) (cid:12) (cid:12) (cid:12)(cid:16)k X=1
k bi b,h −
k X=1
k i b,h
(cid:17)
π∼ζ
h
i,h (1+ 51
1i )(cid:12) (cid:12) (cid:12)(cid:17)
K H2log 18S n i=1AinHNK
≤
αK
k
Pπ i,k h,V V′ h+1◦V′
h+1
−V′ h◦V′ h+2 V′
h
∞1+6 kV h′ k∞v
u (cid:16)
P
N
δ (cid:17)1, (152)
Xk=1 b (cid:16) (cid:17) (cid:13) (cid:13) u t
(cid:13) (cid:13)
holds with probability at least 1 δ, where (i) arises from rmin K αKE rk (,a ) 1 since
− i,h ≤ k=1 k ai∼π ik ,h i,h · i ≤
Vmin Vmin 0 by definition, and the last inequality holds by Lemma 10. Finally, combining (152) and
h+1 − h ≤ P b
the definition of dj in (99), the term of interest can be controlled as
h
H
dj,Var E Vπ
h K αKPπk,V π∼ζ i,j+1
Xj=h(cid:28) Pk=1 k i b,j
(cid:16) h
i(cid:17)(cid:29)
H K H2log 18S n i=1AinHNK
= (dj h)⊤ αK
k
P iπ ,k j,V V′ j+1◦V′
j+1
−V′
j
◦V′
j
+2 kV′ jk∞1+6 kV′ jk∞v
u (cid:16)
P
N
δ (cid:17)1
Xj=h Xk=1 b (cid:16) (cid:17) u t 
 
H  K 
(i) (dj)⊤ αKPπk,V V′ V′ V′ V′ +2H V′
≤ Xj=h" h
k X=1
k i b,j
(cid:16)
j+1◦ j+1 (cid:17)− j ◦ j !# k hk∞
log
18S n i=1AinHNK
+6H2 kV′ hk∞v
u (cid:16)
P Nδ
(cid:17)
u
t
′ ′ ′
where (i) holds by the fact V V V . With further basic calculus, we can finally
k
hk∞
≥k
h+1k∞
≥···≥k
Hk∞
obtain that
H
dj,Var E Vπ
h K αKPπk,V π∼ζ i,j+1
j X=h(cid:28) Pk=1 k i b,j (cid:16) h i(cid:17)(cid:29)
H log
18S n i=1AinHNK
= (dj h+1)⊤ V′ j+1◦V′
j+1
−(dj h)⊤ V′
j
◦V′
j
+2H kV′ hk∞+6H2 kV′ hk∞v
u (cid:16)
P Nδ
(cid:17)
j X=hh (cid:16) (cid:17) (cid:16) (cid:17)i u t
log
18S n i=1AinHNK
≤
dH h+1
1
V′ H+1◦V′
H+1
∞+2H kV′ hk∞+6H2 kV′ hk∞v
u (cid:16)
P Nδ
(cid:17)
u
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) t
55log
18S n i=1AinHNK
≤3H kV′ hk∞+6H2 kV′ hk∞v
u (cid:16)
P Nδ
(cid:17)
u
t
log
18S n i=1AinHNK
′ P δ
=3H kV hk∞1+2Hv
u (cid:16) N
(cid:17),
u
 t 
 
 
C.4.10 Proof of Lemma 16
Analogous to Appendix C.4.9, we introduce some auxiliary values and rewardfunctions to control
H
wj,Var E Vπ
h K αKPπk,V π∼ζ i,j+1
Xj=h(cid:28) Pk=1 k i b,j
(cid:0) (cid:2)
(cid:3)(cid:1)(cid:29)
as below: for any time step h and the i-th agent
Definition 5. For any time step h [H] and the i-th agent, we denote Vmin := min E Vπ (s) as
∈ h s∈S π∼ζ i,h
the minimum value of all the entries in vector E Vπ . We further define V′ := Cπ Vhmin1 asi the
π∼ζ i,h h i,h − h
truncated value function. Eventually for reward funch tion,i we define rmin = K αKE rπ −k i(,a )+
i,h k=1 k ai∼π ik ,h i b,h · i
Vmin Vmin 1 as the truncated reward function..
h+1 − h P
(cid:0) Then apply(cid:1)ing the robust Bellman’s consistency equation in (13) gives
K K
V′ =E Vπ,σi Vmin1= αKrπk + αKPπk,VE Vπ,σi Vmin1
h π∼ζ i,h − h k i,h k i,h π∼ζ i,h+1 − h
h i Xk=1 b Xk=1 b h i
K K
= αKrπk + Vmin Vmin 1+ αKPπk,VV′ (153)
k i,h h+1 − h k i,h h+1
k=1 b k=1 b
X (cid:0) (cid:1) X
K
=rmin+ αKPπk,VV′ . (154)
i,h k i,h h+1
k=1 b
X
The above fact leads to
Var E Vπ,σi
K αKPπk,V π∼ζ i,h+1
k=1 k i,h
P b (cid:16) h i(cid:17)
( =i) Var (V′ )
K αKPπk,V h+1
k=1 k i,h
P b
K K K
= αKPπk,V V′ V′ αKPπk,VV′ αKPπk,VV′
k i,h h+1◦ h+1 − k i,h h+1 ◦ k i,h h+1
k=1 b k=1 b k=1 b
X (cid:0) (cid:1) (cid:0)X (cid:1) (cid:0)X (cid:1)
K
( =ii) αKPπk,V V′ V′ V′ rmin ◦2
k i,h h+1◦ h+1 − h− i,h
Xk=1 b (cid:0) (cid:1) (cid:16) (cid:17)
K
= αKPπk,V V′ V′ V′ V′+2V′ rmin rmin rmin
k i,h h+1◦ h+1 − h◦ h h◦ i,h − i,h ◦ i,h
k=1 b
X (cid:0) (cid:1)
K
αKPπk,V V′ V′ V′ V′+2 V′ 1, (155)
≤ k i,h h+1◦ h+1 − h◦ h k hk∞
k=1 b
X (cid:0) (cid:1)
where (i) follows from the fact that Var (V b1) = Var (V) for any value vector
K αKPπk,V − K αKPπk,V
k=1 k i,h k=1 k i,h
V RS and scalar b, (ii) holds by (154)Pand (13),band the last inequPality arisebs from rmin rπ 1 since
∈ i,h ≤ i,h ≤
Vmin Vmin 0 by definition.
h+1 − h ≤
56Consequently, combining (155) and the definition of wj in (108), we arrive at
h
H
wj,Var E Vπ,σi
h K αKPπk,V π∼ζ i,j+1
k=1 k i,h
j X=hD P b (cid:0) (cid:2) (cid:3)(cid:1)E
H K
(wj)⊤ αKPπk,V V′ V′ V′ V′+2 V′ 1
≤
j=h
h
k=1
k i b,j j+1◦ j+1 − j ◦ j k hk∞ !
X X (cid:0) (cid:1)
H K
(i) (wj)⊤ αKPπk,V V′ V′ V′ V′ +2H V′
≤ j=h" h
k=1
k i b,j j+1◦ j+1 − j ◦ j !# k hk∞
X X (cid:0) (cid:1)
H
= (wj+1)⊤ V′ V′ (wj)⊤ V′ V′ +2H V′
h j+1◦ j+1 − h j ◦ j k hk∞
j X=hh
(cid:0) (cid:1) (cid:0)
(cid:1)i
wH+1 V′ V′ +2H V′
≤k h k1 H+1◦ H+1 ∞ k hk∞
≤3H kV h′ k∞(cid:13) (cid:13), (cid:13)
(cid:13)
(156)
where (i) and the last inequality hold by the fact V′ V′ V′ and basic calculus.
k
hk∞
≥k
h+1k∞
≥···≥k
Hk∞
57