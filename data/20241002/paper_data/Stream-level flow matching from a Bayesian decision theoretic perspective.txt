Stream-level flow matching from a Bayesian
decision theoretic perspective
Ganchao Wei1 and Li Ma1
1Department of Statistical Science, Duke University
{ganchao.wei,li.ma}@duke.edu
Abstract
Flow matching (FM) is a family of training algorithms for fitting
continuous normalizing flows (CNFs). A standard approach to FM, called
conditional flow matching (CFM), exploits the fact that the marginal
vector field of a CNF can be learned by fitting least-square regression
to the so-called conditional vector field specified given one or both ends
of the flow path. We show that viewing CFM training from a Bayesian
decision theoretic perspective on parameter estimation opens the door to
generalizations of CFM algorithms. We propose one such extension by
introducing a CFM algorithm based on defining conditional probability
paths given what we refer to as “streams”, instances of latent stochastic
pathsthatconnectpairsofnoiseandobserveddata. Further,weadvocates
themodelingoftheselatentstreamsusingGaussianprocesses(GPs). The
uniquedistributionalpropertiesofGPs,andinparticularthefactthatthe
velocities of a GP is still a GP, allows drawing samples from the resulting
stream-augmented conditional probability path without simulating the
actualstreams,andhencethe“simulation-free”natureofCFMtrainingis
preserved. We show that this generalization of the CFM can substantially
reduce the variance in the estimated marginal vector field at a moderate
computationalcost,therebyimprovingthequalityofthegeneratedsamples
under common metrics. Additionally, we show that adopting the GP on
thestreamsallowsforflexiblylinkingmultiplerelatedtrainingdatapoints
(e.g., time series) and incorporating additional prior information. We
empirically validate our claim through both simulations and applications
to two hand-written image datasets.
1 Introduction
Deep generative models aim to estimate and sample from an unknown proba-
bility distribution. Continuous normalizing flows (CNFs, Chen et al. (2018))
construct an invertible and differentiable mapping, using neural ordinary differ-
ential equation (ODE), between a source and the target distribution. However,
traditionally it has been difficult to scale CNF training to large datasets (Chen
1
4202
peS
03
]LM.tats[
1v32402.9042:viXraet al., 2018; Grathwohl et al., 2019; Onken et al., 2021). Recently, Lipman
et al. (2023); Albergo and Vanden-Eijnden (2023); Liu et al. (2023b) showed
that CNFs can be trained via a regression objective, and proposed the flow
matching (FM) algorithm. The FM exploits the fact that the marginal vector
field inducing a desired CNF can be learned through a regression formulation,
approximating per-sample conditional vector fields using a smoother such as a
deep neural network (Lipman et al., 2023). In the original FM approach, the
training objective is conditioned on samples from the target distribution, and
the source distribution has to be Gaussian. This limitation was later relaxed,
allowingthetargetdistributiontobesupportedonmanifolds(ChenandLipman,
2024) and the source distribution to be non-Gaussian (Pooladian et al., 2023).
Tongetal.(2024a)providedaunifyingframeworkwitharbitrarytransportmaps
by conditioning on both ends. While their framework is in principle general,
it does require the induced conditional probability paths be readily sampled
from, and as such they considered a few Gaussian probability paths. Moreover,
existing FM methods only consider the inclusion of two endpoints, and hence
cannot accommodate data involving multiple related observations such as time
series and other data with a grouping structure.
In this paper, we first view FM from the perspective of Bayesian estimation
under squared error loss, which motivates us to go one level deeper in Bayesian
hierarchical modeling and specify distributional assumptions on streams, which
are latent stochastic paths connecting the two endpoints. This leads to a class
of CFM algorithms that conditions at the “stream” level, which broadens the
range of conditional probability paths allowed in CFM training. By endowing
the streams with Gaussian process (GP) distributions, these algorithms provide
wider sampling coverage over the support of the marginal vector field, leading to
reduced variance in the estimated vector field and improved synthetic samples
from the target distribution. Furthermore, conditioning on GP streams allows
for flexible integration of related observations through placing them along the
streamsbetweentwoendpointsandforincorporatingadditionalpriorinformation,
all while maintaining analytical tractability and computation efficiency of CFM
algorithms.
In summary, the main contributions of this paper are:
1. We present a Bayesian decision theoretic perspective on FM algorithms,
which provides an additional justification for FM algorithms beyond gradi-
entmatchingandservesasthefoundationforextensionstothesealgorithms
by latent variable modeling on the streams.
2. Wegeneralize CFMtrainingbyaugmentingthespecificationofconditional
probabilitypathsthroughlatentvariablemodelingonthestreams. Weshow
that streams endowed with GP distributions lead to a simple stream-level
CFM algorithm that preserves the “simulation-free” training.
3. We demonstrate that adjusting the GP streams can reduce the variance of
the estimated marginal vector field with moderate computational cost.
24. We demonstrate how to use GP streams to integrate related observations,
thereby taking advantage of the correlation among related samples to
enhance the quality of the generated samples from target distributions.
5. These benefits are illustrated by simulations and two hand-written image
datasets (MNIST and HWD+), with code for Python implementation
available at https://github.com/weigcdsb/GP-CFM.
2 A Bayesian Decision Theoretic Perspective on
Flow Matching
WestartbyviewingFMfromaBayesiandecisiontheoreticperspective. Consider
i.i.d. training observations from an unknown population distribution q over Rd.
1
The prime objective is to generate new samples from q based on the training
1
data. A CNF is a time-dependent differomorphic map ϕ that transforms a
t
random variable x ∈Rd from a source distribution q into a random variable
0 0
from q . The CNF induces a distribution of x =ϕ (x ) at each time t, which is
1 t t 0
denotedbyp ,therebyformingaso-calledprobabilitypath{p :0≤t≤1}. This
t t
probability path should (at least approximately) satisfy the boundary conditions
p =q andp =q . Itisrelatedtotheflowmapthroughthechange-of-variable
0 0 1 1
formula or the push-forward equation
p =[ϕ ] p .
t t ∗ 0
Instead of directly learning a flow ϕ , FM aims at learning the corresponding
t
vector field u (x), which induces the probability path over time by satisfying the
t
continuity equation (Villani, 2008).
The key observation underlying FM algorithms is that the vector field u (x)
t
can be written as a conditional expectation involving a conditional vector field
u (x|z), which induces a conditional probability path p (·|z) corresponding to
t t
the conditional distribution of ϕ (x) given z. Here, z is the conditioning latent
t
variable, which can be the target sample x (e.g. Ho et al. (2020); Song et al.
1
(2021); Lipman et al. (2023),), or can be a pair of (x ,x ) on source and target
0 1
distribution(e.g. Liuetal.(2023b);Tongetal.(2024a)). Specifically, Tongetal.
(2024a), generalizing the result from Lipman et al. (2023), showed that
(cid:90) p (x|z)q(z)
u (x)= u (x|z) t dz =E(u (x|z)|x =x),
t t p (x) t t
t
where the expectation is taken over z, which one can recognize is the conditional
expectation of u (x|z) conditional on the event that x =x. The integral is the
t t
with respect to the conditional distribution of z given x =x.
t
It is well-known in Bayesian decision theory (Berger, 1985) that under squared
error loss, the Bayesian estimator, which minimizes both the posterior expected
loss (which conditions on the data and integrates out the parameters) and the
3marginalloss(whichintegratesoutboththeparametersandthedata),isexactly
the posterior expectation of that parameter. This implies immediately that if
one considers the conditional vector field u (x|z) as the target of “estimation”,
t
and the corresponding “data” being the event that x =x, i.e., that the path
t
goes through x at time t, then the corresponding Bayes estimate for u (x|z)
t
will exactly be the marginal vector field u (x) as it is now the “posterior mean”
t
of u (x|z). We emphasize again that here the “data” differs from the actual
t
trainingandthegeneratednoiseobservations, whichinfacthelpformthe“prior”
distribution.
The FM algorithm is motivated from the goal of approximating the marginal
vector field u (x) through a smoother vθ (typically a neural network), via the
t t
flow matching (FM) objective
L (θ)=E ||vθ(x)−u (x)||2,
FM t,pt(x) t t
which is not identifiable due to the non-uniqueness of the marginal vector fields
that satisfy the boundary conditions without further constraints. FM algorithms
address this by fitting vθ to the conditional vector field u (x|z) after further
t t
specifying the distribution of q(z) along with the conditional probability path
p (x|z), through minimizing the finite-sample version of the marginal squared
t
errorloss. Thisapproachwasreferredtoastheconditionalflowmatching(CFM)
objective
L (θ)=E ||vθ(x)−u (x|z)||2.
CFM t,q(z),pt(x|z) t t
Traditionally, optimizing the CFM objective is justified because it has the same
gradients w.r.t. θ to the corresponding FM loss (Lipman et al., 2023; Tong
et al., 2024a). The Bayesian decision-theoretic perspective provides a further
validation because approximating the conditional vector field by minimizing the
marginal squared error loss can be interpreted as approximating the “posterior
expectation” of u (x|z), which is exactly u (x).
t t
Moreover,thisistrueforanycoherentlyspecifiedprobabilitymodelq(z). Solong
astheconditionalprobabilitypathp (x|z)istractable,asuitableCFMalgorithm
t
canbedesigned. Thereforeonecanenrichthespecificationofq(z)usingBayesian
latentvariablemodelingstrategies. ThismotivatesustogeneralizeCFMtraining
to the stream level, which we describe in the next section.
3 Stream-level Flow Matching
3.1 A per-stream perspective on flow matching
Define a stream s to be a complete path that connects one end x to the other
0
x over time. That is, s={x :0≤t≤1}. From here on, s will take the space
1 t
of the latent quantity z. To avoid confusion, we shall use the notation x (s) to
t
represent the value of a given stream s at time t.
4Instead of defining a conditional probability path and vector field given one
endpoint at t=1 (Lipman et al., 2023) or two endpoints at t=0 and 1 (Tong
et al., 2024a), we shall consider defining it given the whole stream connecting
the two ends. To this end, we need to specify a probability model for s. This
can be separated into two parts—the marginal model on the endpoints π(x ,x ),
0 1
which has been considered in Tong et al. (2024a), and the conditional model for
s given the two ends. That is
(x ,x )∼π and s|x ,x ∼p (·|x ,x ).
0 1 0 1 s 0 1
Our model and algorithm will generally apply to any choice of π that satisfies
the boundary condition, which includes all of the examples considered in Tong
et al. (2024a). We defer the description of specific choices of p (·|x ,x ) to the
s 0 1
next section and for now focus on the general framework.
Given a stream s, we define a “per-stream” vector field u (x|s), which is the
t
“velocity” (or derivative) of the stream at time t, conditional on the event that
x (s)=x, i.e, the stream s passes through x at time t. Specifically, assuming
t
that the stream is differentiable with respect to time, the per-stream vector field
is
u (x|s):=x˙ (s)=dx (s)/dt,
t t t
which is defined only for all pairs of (t,x) that satisfy x (s) = x. The per-
t
streamviewextendspreviousCFMconditioningonendpointsandprovidesmore
flexibility. See Appendix A for more detailed discussion on how the per-stream
perspective relates to the per-sample perspective on FM.
While the endpoint of the stream x is an actual observation in the training
1
data, for the task of learning the marginal vector field u (x), one can think of
t
our “data” as the event that a stream s passes through a point x at time t, that
isx (s)=x. Underthesquarederrorloss, theBayesestimatefortheper-stream
t
conditional vector field u (x|s) will be the “posterior” expectation given the
t
“data”, which is exactly the marginal vector field
u (x)=E(u (x|s)|x (s)=x)=E(x˙ (s)|x (s)=x). (1)
t t t t t
Following Theorem 3.1 in Tong et al. (2024a), we can show that the marginal
vector u (x) indeed generates the probability path p (x). (See the proof in the
t t
Appendix F.) The essence of the proof is to check the continuity equation for
the (degenerate) conditional probability path p (x|s).
t
A general stream-level CFM loss for learning u (x) is then
t
L (θ)=E ∥vθ(x)−u (x|s)∥2=E ∥vθ(x (s))−x˙ (s)∥2
sCFM t,q(s),δ(x−xt(s)) t t t,q(s) t t t
whereq(s)isthemarginaldistributionofsinducedbyπ(x ,x )andp (·|x ,x );
0 1 s 0 1
δ(x−c) is the indicator function for x = c. As in previous research such as
Lipman et al. (2023); Tong et al. (2024a), we can also show that the gradient of
L equalsthat ofthe L withdetails ofproof intheAppendix G.However,
sCFM FM
5we wish to emphasize again that stream-level CFM can be justified from a
Bayesian decision theoretic perspective without gradient matching (Section 2).
Because the (population-level) minimizer for the sCFM loss is exactly u (x),
t
minimizing the sCFM loss will provide a reasonable estimate for the marginal
vector field u (x). To see this more directly, we rewrite the sCFM loss by the
t
law of iterated expectation as
L (θ)=E E (cid:0) ||vθ(x (s))−x˙ (s)||2|t(cid:1) .
sCFM t s t t t
The inner expectation can be further written as
E (cid:0) ||vθ(x (s))−x˙ (s)||2|t(cid:1) =E E (cid:0) ||vθ(x (s))−x˙ (s)||2|t,x (s)(cid:1) .
s t t t xt(s) s t t t t
Foranyx,E (cid:0) ||vθ(x (s))−x˙ (s)||2|t,x (s)=x(cid:1) =E (cid:0) ||vθ(x)−x˙ (s)||2|t,x (s)=x(cid:1) ,
s t t t t s t t t
whose minimizer is the conditional expectation of x˙ (s) given x (s)=x, which
t t
is exactly u (x). Hence, one can estimate u (x) by minimizing L (θ).
t t sCFM
3.2 Choice of the stream model
Next, we specify the conditional model for the stream given the endpoints
p (·|x ,x ). The key desiderata for this model is that it should emit streams
s 0 1
differentiable with respect to time, with readily available velocity (either ana-
lytically or easily computable). Previous methods such as optimal transport
(OT) conditional path (Liu et al., 2023b; Lipman et al., 2023; Tong et al., 2024a)
can provide rather poor coverage of the (t,x) space, resulting in extensive ex-
trapolation of the estimated vector field v (x). It is thus desirable to consider
t
stochastic models for the streams that ensure the smoothness while allowing
streams to diverge and provide more spread-out coverage of the (t,x) space.
Previous research suggested that, compared to ODEs, SDEs (Ho et al., 2020;
Song et al., 2021) can be more robust in high-dimensional spaces (Tong et al.,
2024b; Shi et al., 2023; Liu et al., 2023a), which we believe is partly due to the
robustnessarisingfromtheregularizationinducedbytheadditionalstochasticity.
Injecting such stochasticity to achieve desirable regularization is an additional
consideration in our choice of the stream model.
To preserve the ”simulation-free” nature of CFM, we consider models where the
joint distribution of the stream and its velocity is available in closed form. In
particular, we further explore the streams following Gaussian processes (GPs).
A desirable property of GP is that its velocity is also a GP, with mean and
covariance directly derived from original GP (Rasmussen and Williams, 2005).
This enables efficient joint sampling of (x (s),x˙ (s)) given observations from a
t t
GP in stream-level CFM training. By adjusting covariance kernels for the joint
GP, one can fine-tune the variance level to control the level of regularization,
thereby further improving the estimation of the marginal vector field u (x)
t
(Section 4.1). The prior path constraints can also be incorporated into the
kerneldesign. Additionally, GPconditioningontheeventthatthestreampasses
through a finite number of intermediate locations between two endpoints again
6leads to a GP with analytic mean and covariance kernel (Section 4.2). This is
particularly useful for incorporating multiple related observations.
Specifically,givenM observationsx (s)=(x (s),...,x (s))witht =0and
obs t1 tM 1
t =1, we construct a conditional (multioutput) GP for s that (approximately)
M
satisfies the boundary conditions, with differentiable mean function m and
covariance kernel k . Note that this contains the special case of conditioning
11
on two endpoints (i.e., M = 2) described in Section 3.1. We consider a more
general construction for M ≥ 2 because later we will use this to incorporate
multiplerelatedobservations. SincethederivativeofaGPisalsoaGP,thejoint
distributionofsandcorrespondingvelocitys˙ :={x˙ (s):t∈[0,1]}givenx (s)
t obs
isalsoaGP,withthemeanfunctionfors˙ bem˙(t)=dm(t)/dtandkernelsdefined
by derivatives of k . Constructing this GP by incorporating x (s) into mean
11 obs
and covariance design directly would be difficult for multiple observations and
is not flexible for stream design. To facilitate such a construction, we consider
an auxiliary GP on s with differentiable mean function ξ and covariance kernel
c . Using the property that the conditional distribution of Gaussian remains
11
Gaussian, we can obtain a joint GP model on (s,s˙)|x (s), which satisfies the
obs
boundary conditions. For computational efficiency and ease of implementation,
we assume independence of the GP across dimensions of x, and models the
marginal GP for each dimension separately. Notably, while we are modeling
streams conditionally given x (s) as a GP, the marginal distributions of x are
obs t
all allowed to be non-Gaussian, which is necessary for satisfying the boundary
condition. The detailedderivation canbe foundinAppendix B,andthe training
algorithm for GP-CFM is summarized in Algorithm 1.
Algorithm 1: Gaussian Process Conditional Flow Matching (GP-CFM)
Input :samples of observations x =(x ,...,x ) from distribution
obs 0 1
π(x ), and a GP defining the conditional distribution
obs
(x ,x˙ )|x ∼N(µ˜ ,Σ˜ ), for t∈[0,1].
t t obs t t
Output:fitted vector field vθ(x)
t
while Training do
x ∼π(x ); t∼U(0,1)
obs obs
(x ,x˙ )|x ∼N(µ˜ ,Σ˜ )
t t obs t t
L (θ)←∥vθ(x )−x˙ ∥2
sCFM t t t
θ ← update (θ,∇ L (θ))
θ sCFM
end
Several conditional probability paths considered in previous works are special
cases of the general GP representation. For example, if we set m(t) = tx +
1
(1−t)x (therefore, m˙(t)=x −x ) and k (t,t′)=σ2I , the path reduces to
0 1 0 11 d
the OT conditional path used in OT-CFM with constant variance (Tong et al.,
2024a). The OT-CFM path can also be induced by conditional GP construction
(Appendix B) using a linear kernel for c , with more details in Appendix C. In
11
the following, we set ξ(t)=0 and use squared exponential (SE) kernel for c
11
7for each dimension (may with additional terms such as in Figure2). The details
of SE kernel can be found in Appendix D.
Probabilitypathswithtime-varyingvariance,suchasSongandErmon(2019);Ho
et al. (2020); Lipman et al. (2023), also motivate the adoption of non-stationary
GPs whose covariance kernel could vary over t. For example, to encourage
samples that display larger deviation from those in the training set (and hence
more regularization), one could consider using a kernel producing larger variance
as t approaches to ends with finite training samples (Figure 2 and 7). Moreover,
because the GP model for s is specified given the two endpoints, both its mean
and covariance kernel can be specified as functions of (x ,x ). For example, if
0 1
x is an outlier of the training data, e.g., from a tail region of q , then one may
1 1
incorporate a more variable covariance kernel for p (·|x ,x ) to account for the
s 0 1
uncertainty in the “optimal” transport path from x to x .
0 1
4 Numerical experiments
In this section, we demonstrate the benefits of GP stream models by several
simulation examples. Specifically, we show that using GP stream models can
substantiallyimprovethegeneratedsamplequalityatamoderatecostoftraining
time, through tweaking the variance function to reduce sampling variance of
the estimated vector field. Moreover, the GP stream model makes it easy to
integrate multiple related observations along the time scale.
4.1 Adjusting GP variance for high quality samples
We first show that one can reduce the variance in estimating u (x) by incor-
t
porating additional stochasticity in the sampling of streams with appropriate
GP kernels. As illustrated in Figure 1A, for estimating 2-Gaussian mixtures
from standard Gaussian noise, the straight conditional stream used in I-CFM
covers a relatively narrow region (gray). For points outside the searching region,
there are no “data” and the neural network vθ(x) must be extrapolated. In the
t
sampling stage, this can lead to potential “leaky” or outlying samples that are
far from the training observations.
For constructing GP conditional streams, we condition on the endpoints but
expand the coverage region (red) by tweaking the kernel function (e.g. decrease
the SE bandwidth in this case). This provides a layer of protection against
extrapolationerrors. WethentraintheI-CFMandGP-I-CFM100timesusinga
2-hidden layer multi-layer perceptron (MLP) with 100 training samples at t=1,
and calculate 2-Wasserstein (W2) distance between generated and test samples.
For fair comparison, we set σ =0 for I-CFM and use noise-free GP-I-CFM. The
results are summarized in Table 1B. Empirically, the GP-I-CFM has smaller W2
distance than I-CFM. We further generate 1000 samples and streams for I-CFM
and GP-I-CFM with largest W2 distance in Figure 1C, starting with the same
points from standard Gaussian. In this example, several outliers are generated
8from I-CFM.
Figure 1: GP streams reduce extrapolation by expanding coverage
area. We use a 2-Gaussian mixture distribution as an example. The training
observations are in red. The generated samples in orange and the samples
from noise source are in black. A. FM with straight conditional stream (e.g.
I-CFM) may generate “leaky” or outlier samples due to extrapolation errors.
The FM method with GP conditional stream can have a broader coverage area.
B. We train models with I-CFM and GP-I-CFM for 100 times and calculate
2-Wasserstein (W2) distance between generated and test samples for each. The
results of 100 seeds are summarized by mean and standard error. C. Among
these 100 trained models, generate 1000 samples (orange) and streams (blue) for
I-CFM and GP-I-CFM with largest W2 distance.
We can further change the variance scheme over time by modifying the GP
variance function, to more efficiently achieve high quality generated samples.
Here, we again consider the task of estimating and sampling from a 2-Gaussian
mixtures, with 100 training samples at t=1. When constructing the conditional
GP from auxiliary GP, we add diagonal (white) noise to the SE kernel for
constant noise. For varying noise, we add a non-stationary dot product kernel
to the SE kernel. Specifically, denote the kernel for auxiliary GP on s in
dimension i as ci , for i=1,...,d. Let ci (t,t′)=cSE(t,t′)+αtt′ for increasing
11 11 11
variance and ci (t,t′) = cSE(t,t′)+α(t−1)(t′ −1) for decreasing variance,
11 11
(cid:16) (cid:17)
where {t,t′} ∈ [0,1] and cSE(t,t′) = σ2exp −(t−t′)2 . (See Appendix B for
11 2l2
additional details.) Some examples of the streams connecting two endpoints
under different variance schemes are shown in Figure 2A. We train models 100
times and calculate 2-Wasserstein (W2) distance between generated and test
samples, and the results of these 100 seeds are summarized in Table 2B. In
this example, we have infinite samples from standard Gaussian at t = 0 but
finite samples at t=1 (100 training samples). Therefore, further injecting noise
(i.e., adding regularization) at beginning (t=0) does not help and even makes
estimation worse. However, when approaching the target distribution at the
9end (t = 1), further injecting noise can help with estimation. Since in this
case, we have relative small samples (100), which is common for many scientific
applications, and the estimation variance can be relatively high. Adding more
noise when approaching to end perturbs the limited data, and it encourages
the algorithm to explore wider regions and add more regularization, which can
reduce the estimation error.
Figure 2: Change variance over time by tweaking the covariance kernel.
Here, we again consider a 2-Gaussian mixture distribution as an example. A.
Examples of conditional stream between two points, under different variance
change scheme. B. We then train models under each variance scheme for 100
times and calculate 2-Wasserstein (W2) distance between generated and test
samplesforeach. Theresultsof100seedsaresummarizedbymeanandstandard
error.
Wefurtherconsiderthetransformationfroma2-Gaussiantoanother2-Gaussian
distribution, with finite (100) samples at both ends. The results are shown in
the Appendix E. In this case, we have finite samples at both ends, and further
injecting noise when approaching either end can help with estimation.
4.2 Incorporating multiple related training observations
Next we show that using GP streams allow us to flexibly include related obser-
vations along the same stream over time. This is especially useful for generating
related samples such as time series (e.g. videos), where the correlation of ob-
servations can help borrowing information across observations. Incorporating
the “temporal” correlation can potentially improve the estimation for each time
point.
Toillustratethemainidea,weconsider100pairedobservationsandplacethetwo
observationsineachpairatt=0.5andt=1respectively(Figure3A)whilet=0
still corresponds to a source distribution. Here, we show the generated samples
(at t=0.5 and t=1) and the corresponding streams for GP-I-CFM and I-CFM.
10Again, 2-hidden layer MLP is used in this case. The I-CFM strategy employs
two separate models with I-CFM algorithms (Figure 3B), whereas GP-I-CFM
offers a single unifying model for all observations, resulting in a smooth stream
across all time points (Figure 3C).
Figure 3: GP streams can include related points flexibly. A. Paired
data with observations on t = 0.5 (red) and t = 1 (orange). B. The generated
samples (red for t = 0.5 and orange for t = 1) and streams (blue) for I-CFMs.
The I-CFMs contain two separate models trained by I-CFM, t = 0 (standard
Gaussian noise) to t = 0.5 and t = 0.5 to t = 1. C. The generated samples for
GP-I-CFM.
In some cases the GP streams may not be well separated and thus may confuse
the training of the vector field at crossing points. In Figure 4, we show a time
series dataset over 3 time points, where training data at t = 0 and t = 1 on
one horizontal side while points at t=0.5 are on the opposite side (Figure 4A).
Therefore, these streams have two crossing regions (marked with blue boxes
in Figure 4A), where the training of vector field is deteriorated when simply
using the GP-I-CFM (Figure 4B). One easy solution is to further condition
the neural net vθ(x) on subject label c, such that the optimizing objective is
t
L =E ∥vθ(x,c)−x˙ (s)∥2, where q(s|c) represents the distribution of
cCFM t,q(s|c) t t
s given label c. The validity for approximating the covariate-dependent vector
field using the above optimizing objective is shown in the Appendix H. In this
example, similar subjects have close starting points at t=0, and we let c=x .
0
By conditioning on c (labeled model), the neural net are separated for different
subjects, and hence the training of vector field will not be confused (Figure 4C).
5 Applications
We apply our GP-based CFM methods to two hand-written image datasets
(MNISTandHWD+), toillustratehowGP-basedalgorithms1)reducesampling
variance and 2) flexibly incorporate multiple related observations, and generate
smooth transformation across different time points.
11Figure4: Further conditioning on the starting points helps with stream
generation. A. Paired data with observations on three time points: t = 0
(black), t = 0.5 (red) and t = 1 (orange). The two stream cross regions are
marked with light blue square. B. The generated samples and streams for
unlabeled GP-I-CFM, where the initial points at t=0 are generated from noise
using a separate I-CFM. C. The generated samples and streams for labeled
GP-I-CFM using the same starting points, where the neural network is further
conditioning on data at t=0.
5.1 Application to MNIST database
We explore the empirical benefits of variance reduction by using FM with GP
conditional streams on the MNIST database (Deng, 2012). Four algorithms are
considered: two linear stream models (I-CFM, OT-CFM) and two GP stream
models (GP-I-CFM, GP-OT-CFM). For a fair comparison, we set σ = 0 for
linear stream models and use noise-free GP stream models. For all models,
U-Nets (Ronneberger et al., 2015) with 32 channels and 1 residual block are
used. Figure 5A shows the 10 generated images for each trained model, starting
from the same standard Gaussian noise. Compared to I-CFM, the OT version
jointly samples two endpoints by 2-Wasserstein optimal transport (OT) map
π (Tong et al., 2024a). Here, we demonstrate how much the GP stream-level
CFM can further improve the estimation. We train each algorithm 100 times,
and calculate the kernel inception distance (KID) (Bin´kowski et al., 2018) and
Fr´echet inception distance (FID) (Heusel et al., 2017). The histograms in Figure
5B show distribution of these 100 KIDs and FIDs, with results summarized in
Figure 5C. According to KID and FID, the independent sampling algorithms
(I-algorithms) are comparable to optimal transport sampling algorithms (OT-
algorithms). However, algorithms using GP conditional stream exhibit lower
standard error and fewer extreme values for KID and FID, thereby reducing the
occurrence of outlier samples, as illustrated in Figure 1).
12Figure 5: Application to MNIST dataset. We compare the performance
of four algorithms (I-CFM, OT-CFM, GP-I-CFM and GP-OT-CFM) on fitting
MNIST dataset. A. The 10 images generated from each trained model. Fit the
models 100 times for each, and evaluate the quality of the samples by KID and
FID. B. The histograms of KID and FID. C. The mean and standard error for
KID and FID.
5.2 Application to HWD+ dataset
Finally, we demonstrate how our GP stream-level CFM can flexibly incorporate
related observations (between two endpoints at t=0 and t=1) into one single
model and provide smooth transformation across different time points, using the
HWD+ dataset (Beaulac and Rosenthal, 2022). The HWD+ dataset contains
images of handwritten digits along with writer IDs and characteristics, which
are not available in MNIST dataset used in Section5.1.
Here, weconsiderthetaskoftransformingfrom“0”(att=0)to“8”(att=0.5),
and then to “6” (at t = 1). The intermediate image, “8”, is placed at t = 0.5
(artificialtime)for“symmetric”transformations. Allthreeimageshavethesame
numberofsamples,totaling1,358samples(1,086fortrainingand272fortesting)
from 97 subjects. Again, U-Nets with 32 channels and 1 residual block are used.
Bothunlabeledandlabeledmodels(bystartingimages, topreservethegrouping
effectasinFigure4C)areconsidered. EachmodelistrainedbothbyI-CFMand
GP-I-CFM. The I-CFM transformation contains two separate models trained
by I-CFM (“0” to “8” and “8” to “6”). Noise-free GP-I-CFM and I-CFM with
σ = 0 are used for fair comparisons. In each training iteration, we randomly
select samples within each writer, to preserve the grouping structure of data.
The traces for 10 generate samples from each algorithm are shown in Figure 6A,
wherethestartingimages(“0”inthefirstrows)aregeneratedbyanI-CFMfrom
standard Gaussian noise. Visually, the GP-based algorithms generate higher
quality images and smoother transformation compared algorithms using linear
conditional stream (I-CFM), highlighting the benefit of including correlations
across different time points. Additionally, the transformation generally looks
13smoother when the CFM training is further conditioned on the subject label.
We then quantify the performance of different algorithms by calculating the FID
for “0”, “8” and “6”, and plot them over time for each (Figure 6B). For all FIDs,
the GP-based algorithms (green & red) outperform their straight connection (I-)
counterparts (blue & orange), which is consistent with the visual examination
above,especiallyfortheFIDfor“8”att=0.5andtheFIDto“6”att=1. This
also holds for the FID for “0”, as the GP-based algorithms are unified and the
information is shared across all time points. However, for the I-algorithms, the
conditional version (orange) performs worse than unconditional one (blue), as
conditioning on the starting images makes the stream more separated, requiring
moredatatoachievecomparableperformance. Incontrast,thedatainGP-based
algorithms is more efficiently utilized, as correlations across time points for the
same subject are integrate into one model. Therefore, explicitly accounting for
the grouping effect by conditioning on starting images (red) further improves
performance.
Figure6: Application to HWD+ dataset. Wefitmodelsfortransforming“0”
to “8” and then to “6”. Both labeled and unlabeled (by starting images) models
are considered, and each model is fitted by both I-CFM and GP-I-CFM. The
I-CFM transformation consists of two separate models trained by I-CFM (“0”
to “8” and “8” to “6”). A. 10 sample traces for the four trained models. The
starting images (“0”s in the first row) are generated by an I-CFM from standard
Gaussian noise, and all four trained models use the same starting images. B.
The corresponding FID to “0”, “8” and “6” for these four trained models over
time.
146 Conclusion
We have presented a Bayesian decision theoretic perspective to CFM training,
which motivates an extension to CFM algorithms based on latent variable
modeling. In particular, we adopt GP models on the latent streams. Our
GP-CFM algorithm preserves the “simulation-free” feature of CFM training by
exploiting distributional properties of GPs. This generalization not only reduces
the sampling variance by expanding coverage of the sampling space in CFM
training, but also allows easy integration of multiple related observations to
achieve borrowing of strength.
There are some potential improvements either under GP-CFM frameworks or
generally motivated by Bayesian decision theoretic perspective. For example,
under GP-CFM framework, current implementations require the complete obser-
vations for all time points, which can be rare in time series applications. To deal
with the missingness as well as the potential high-dimensionality of the training
data, we may fit the GP-CFM in some latent space as in latent diffusion models
(Rombach et al., 2022) and latent flow matching (Dao et al., 2023).
We believe that the Bayesian decision theoretic perspective and GP-CFM gener-
alization so motivated open the door to various further improvements of CFM
training of CNFs.
Acknowledgments
This research is funded by NIGMS grant R01-GM135440.
References
Albergo, M. S. and Vanden-Eijnden, E. (2023). Building normalizing flows with
stochasticinterpolants. InThe Eleventh International Conference on Learning
Representations.
Beaulac, C. and Rosenthal, J. S. (2022). Introducing a New High-Resolution
Handwritten Digits Data Set with Writer Characteristics. SN Computer
Science, 4(1):66.
Berger, J. (1985). Statistical Decision Theory and Bayesian Analysis. Springer
Series in Statistics. Springer.
Bin´kowski, M., Sutherland, D. J., Arbel, M., and Gretton, A. (2018). Demysti-
fying MMD GANs. In International Conference on Learning Representations.
Chen, R. T. Q. and Lipman, Y. (2024). Flow matching on general geometries.
In The Twelfth International Conference on Learning Representations.
Chen, R. T. Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. (2018).
Neural ordinary differential equations. In Bengio, S., Wallach, H., Larochelle,
15H., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in
Neural Information Processing Systems, volume 31. Curran Associates, Inc.
Dao, Q., Phung, H., Nguyen, B., and Tran, A. (2023). Flow matching in latent
space.
Deng, L. (2012). The mnist database of handwritten digit images for machine
learning research. IEEE Signal Processing Magazine, 29(6):141–142.
Grathwohl, W., Chen, R. T. Q., Bettencourt, J., and Duvenaud, D. (2019).
Scalable reversible generative models with free-form continuous dynamics. In
International Conference on Learning Representations.
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S.
(2017). Gans trained by a two time-scale update rule converge to a local nash
equilibrium. In Proceedings of the 31st International Conference on Neural
Information Processing Systems, NIPS’17, page 6629–6640, Red Hook, NY,
USA. Curran Associates Inc.
Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models.
In Proceedings of the 34th International Conference on Neural Information
Processing Systems, NIPS ’20, Red Hook, NY, USA. Curran Associates Inc.
Lipman, Y., Chen, R.T.Q., Ben-Hamu, H., Nickel, M., andLe, M.(2023). Flow
matching for generative modeling. In The Eleventh International Conference
on Learning Representations.
Liu, G.-H., Vahdat, A., Huang, D.-A., Theodorou, E., Nie, W., and Anandku-
mar, A. (2023a). I2sb: Image-to-image schr¨odinger bridge. In International
Conference on Machine Learning (ICML).
Liu, X., Gong, C., and Liu, Q. (2023b). Flow straight and fast: Learning to
generate and transfer data with rectified flow. In The Eleventh International
Conference on Learning Representations.
Onken, D., Fung, S. W., Li, X., and Ruthotto, L. (2021). OT-Flow: Fast
and accurate continuous normalizing flows via optimal transport. In AAAI
Conference on Artificial Intelligence, volume 35, pages 9223–9232.
Pooladian, A.-A., Ben-Hamu, H., Domingo-Enrich, C., Amos, B., Lipman, Y.,
and Chen, R. T. Q. (2023). Multisample flow matching: straightening flows
with minibatch couplings. In Proceedings of the 40th International Conference
on Machine Learning, ICML’23. JMLR.org.
Rasmussen,C.E.andWilliams,C.K.I.(2005). Gaussian Processes for Machine
Learning. The MIT Press.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022).
High-resolutionimagesynthesiswithlatentdiffusionmodels. InProceedings of
the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
16Ronneberger,O.,Fischer,P.,andBrox,T.(2015).U-net: Convolutionalnetworks
forbiomedicalimagesegmentation. InNavab,N.,Hornegger,J.,Wells,W.M.,
and Frangi, A. F., editors, Medical Image Computing and Computer-Assisted
Intervention – MICCAI 2015, pages 234–241, Cham. Springer International
Publishing.
Shi, Y., Bortoli, V. D., Campbell, A., and Doucet, A. (2023). Diffusion
schr¨odinger bridge matching. In Thirty-seventh Conference on Neural Infor-
mation Processing Systems.
Song, Y. and Ermon, S. (2019). Generative modeling by estimating gradients of
the data distribution. Curran Associates Inc., Red Hook, NY, USA.
Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole,
B. (2021). Score-based generative modeling through stochastic differential
equations. In International Conference on Learning Representations.
Tong,A.,Fatras,K.,Malkin,N.,Huguet,G.,Zhang,Y.,Rector-Brooks,J.,Wolf,
G., and Bengio, Y. (2024a). Improving and generalizing flow-based generative
models with minibatch optimal transport. Transactions on Machine Learning
Research. Expert Certification.
Tong, A. Y., Malkin, N., Fatras, K., Atanackovic, L., Zhang, Y., Huguet, G.,
Wolf, G., and Bengio, Y. (2024b). Simulation-free Schr¨odinger bridges via
score and flow matching. In Dasgupta, S., Mandt, S., and Li, Y., editors,
Proceedings of The 27th International Conference on Artificial Intelligence
and Statistics, volume 238 of Proceedings of Machine Learning Research, pages
1279–1287. PMLR.
Villani, C. (2008). Optimal Transport: Old and New. Grundlehren der mathe-
matischen Wissenschaften. Springer Berlin Heidelberg.
Appendices
A Discussion on Per-stream Perspective on Flow
Matching
Itishelpfultorecognizetherelationshipbetweentheper-streamvectorfieldand
the conditional vector field given one or both endpoints introduced previously in
the literature. Specifically, the per-sample vector field in Lipman et al. (2023)
corresponds to marginalizing out s given the end point x , that is u (x|x )=
1 t 1
E(u (x|s)|x (s)=x,x (s)=x ). Similarly,theconditionalvectorfieldofTong
t t 1 1
et al. (2024a), corresponds to marginalizing out s given both x and x , that
0 1
is u (x|x ,x ) = E(u (x|s)|x (s)=x,x (s)=x ,x (s)=x ). Furthermore,
t 0 1 t t 0 0 1 1
when p (·|x ,x ) is simply a unit-point mass (Dirac) concentrated on the
s 0 1
17optimal transport (OT) path, i.e., a straight line that connects two endpoints x
0
and x , then u (x|s)=u (x|x )=u (x|x ,x ) for all (s,t,x) tuples that satisfy
1 t t 1 t 0 1
x (s)=x ,x (s)=x ,x (s)=x. Intuitively, when the stream connecting two
0 0 1 1 t
ends is unique, conditioning on the two ends is equivalent to conditioning on
the corresponding stream s in which case our stream-level FM algorithm (to
be described later) coincides with those previous algorithms. More generally,
however, this equivalence does not hold when p (·|x ,x ) is non-degenerate.
s 0 1
The per-stream view affords additional modeling flexibility and alleviates the
practitionerfromtheburdenofdirectlysamplingfromtheconditionalprobability
paths given one (Lipman et al., 2023) or both endpoints (Tong et al., 2024a).
While the per-stream vector field induces a degenerate unit-point mass condi-
tional probability path, we will attain non-degenerate marginal and conditional
probability paths that satisfy the boundary conditions after marginalizing out
the streams. Sampling the streams in essence provides a data-augmented Monte
Carlo alternative to sampling directly from the conditional probability paths,
which can then allow estimation of the marginal vector field u (x) when direct
t
sampling from the conditional probability path is challenging. Additionally, as
we will demonstrate later, by approaching FM at the stream level, one could
more readily incorporate prior knowledge or other external features into the
design of the stream distribution p (·|x ,x ).
s 0 1
B Derivation of joint conditional mean and co-
variance
Forcomputationalefficiencyandeaseofimplementation,weassumeindependent
GPs across dimensions and present the derivation dimension-wise throughout
Appendices. We use xi(s) to denote the location of stream s at t in dimension
t
i, for i = 1,...,d. Suppose each dimension of stream s follows a Gaussian
process with a differentiable mean function ξi and covariance kernel ci . Then
11
the joint distribution of xi (s) = (xi (s),...,xi (s))′ and x˙i (s) =
t1,...,tg t1 tg t1,...,tg
(x˙i (s),...,x˙i (s))′ on g time points is
t1 tg
(cid:18) xi (s)(cid:19) (cid:18)(cid:18)ξi (cid:19) (cid:18) Σi Σi (cid:19)(cid:19)
x˙it1,...,tg
(s)
∼N ξ˙t i1,...,tg , Σi11⊺ Σ1 i2 , (2)
t1,...,tg t1,...,tg 12 22
where ξi = ξi(t), ξ˙i = dξi/dt, ξi = (ξi ,...,ξi )′, ξ˙i = (ξ˙i ,...,ξ˙i )′
t t t t1,...,tg t1 tg t1,...,tg t1 tg
and covariance Σi is determined by kernel ci . The kernel function for the
jl jl
covariance between s and s˙ in dimension i is ci (t,t′)= ∂ci 11(t,t′), and the kernel
12 ∂t′
defining covariance of s˙ is ci = ∂2ci 11(t,t′) (Rasmussen and Williams (2005)
22 ∂t∂t′
Chapter 9.4). The conditional distribution of (s,s˙) in dimension i given M
observations xi (s) is also a (bivariate) Gaussian process. In particular, for
obs
18t∈[0,1], let µi =(ξi,ξ˙i)′ and µi =(ξi ,...,ξi ), the joint distribution is
t t t obs t1 ts
(cid:16)
xi(s),x˙i(s),xi
(s)′(cid:17)′
∼N
(cid:18)(cid:18) µi
t
(cid:19) ,(cid:18) Σ ⊺i
t
Σi t,obs(cid:19)(cid:19)
,
t t obs µi Σi Σi
obs t,obs obs
where Σi = Cov(xi(s),x˙i(s)) and Σi = Cov(xi (s)). Accordingly, the
t t t obs obs
conditional distribution (xi(s),x˙i(s))|xi (s) ∼ N(µ˜i,Σ˜i), where µ˜i = µi +
t t obs t t t t
Σi Σi −1 (xi (s)−µi ) and Σ˜i =Σi−Σi Σi −1 Σi⊺ .
t,obs obs obs obs t t t,obs obs t,obs
C Induce optimal transport path from Condi-
tional GP Construction
In this section, we show how to derive the path in OT-CFM (Tong et al., 2024a)
fromtheconditionalGPconstruction(AppendixB)usinglinearkernel. Without
loss of generality, we show the derivation of “noise-free” path with σ2 =0 (i.e.,
the rectified flow, Liu et al. (2023b)).
Let xi =(xi,xi)′, ξi =ξ˙i =0 and ci (t,t′)=σ2+σ2(t−1)(t′−1), such that
obs 0 1 t t 11 a b
(cid:18) σ2+σ2(t−1)2 σ2(t−1)(cid:19) (cid:18) σ2−σ2(t−1) σ2(cid:19)
Σi = a b b , Σi = a b a ,
t σ2(t−1) σ2 t,obs −σ2 0
b b b
Σi
obs
=(cid:18) σ a2 σ+ 2σ b2 σσ a2 2(cid:19) , Σi obs−1 = σ1
2
(cid:32) −1
1
1+−1
σ
b2(cid:33) .
a a b σ2
a
Therefore,
µ˜i =Σi Σi
−1(cid:18) xi 0(cid:19) =(cid:18) 1−t t(cid:19)(cid:18) xi 0(cid:19) =(cid:18) (1−t)xi 0+txi 1(cid:19)
,
t t,obs obs xi −1 1 xi xi −xi
1 1 1 0
Σ˜i =Σi−Σi Σi −1 Σi⊺ =O
t t t,obs obs t,obs
D Covariance under Squared Exponential kernel
Throughoutthispaper,weadoptedthesquaredexponential(SE)kernel,withthe
same hyper-parameters for each dimension. The kernel defining block covariance
for s, (s,s˙) and s˙ in dimension i from Equation 2 are as follows:
(cid:18) (t−t′)2(cid:19) σ2 (cid:18) (t−t′)2(cid:19)
ci (t,t′)=σ2exp − ci (t,t′)= (t−t′)exp −
11 2l2 12 l2 2l2
ci (t,t′)=−ci (t,t′) ci (t,t′)=
σ2
(cid:2) l2−(t−t′)2(cid:3)
exp(cid:18) −(t−t′)2(cid:19)
.
21 12 22 l4 2l2
E A Supplementary Example for Variance Chang-
ing over Time
Here, instead of generating data from standard Gaussian noise, we consider 100
training (unpaired) samples from a 2-Gaussian to another 2-Gaussian (Figure
197A).Theexamplestreamsconnectingtwopointsunderdifferentvariancescheme
are shown in Figure 7B, again using additional nugget noise for constant noise,
and dot product kernel for decreasing and increasing noise, as in Section 4.1. We
then fit 100 independent models, and calculate W2 distance between generated
and test samples at t=1. The results are summarized in Figure 7C. Now, since
both ends have finite samples, injecting noise (a.k.a. adding regularization) in
both ends helps.
Figure 7: Supplementary Example for Variance Change over Time.A.
The 100 observations in training data at t = 0 and t = 1. B. Examples of
streams between two points, under different variance change scheme. C. Train
models 100 times and calculate 2-Wasserstein (W2) distance between generated
and test samples for each. The results of these 100 seeds are summarized by
mean and standard error.
F Proof for conditional FM on stream
Theorem 1. The marginal vector field over stream u (x) generates the marginal
t
probability path p (x) from initial condition p (x).
t 0
(cid:82)
Proof. Denote probability over stream as q(s)= p(s|x ,x )π(x ,x )d(x ,x )
0 1 0 1 0 1
and p (x|s)=δ(x−x (s)), then
t t
d d (cid:90)
p (x)= p (x|s)q(s)ds
dt t dt t
Assume the integrand is dominated by an integrable function (e.g. is bounded
for most application), such that we can exchange limit, derivative and integral
(dominated convergence theorem, DCT). Therefore,
(cid:90) d
= p (x|s)q(s)ds
dt t
20To handle the derivative on zero measure, define x (s)-centered Gaussian condi-
t
tional path and corresponding flow map as
p (x|s):=N(x|x (s),σ2I)
σ,t t
ψ (z |s):=σz+x (s)
σ,t t
, for z ∼N(0,I), such that lim p (x|s)=p (x|s). Then by Theorem 3
σ→0 σ,t t
of Lipman et al. (2023), the unique vector field defining ψ (z |s) (and hence
σ,t
generating p (x|s)) is u∗(x|s)=dx (s)/dt=u (x (s)|s), for all (t,x). Note
σ,t t t t t
that u∗(x | s) extends u (x | s) by defining on all x, and they are equivalent
t t
when x=x (s). Since u∗(·|s) generates p (·|s), by continuity equation,
t t σ,t
d (cid:90) d
p (x)= lim p (x|s)q(s)ds
dt t dtσ→0 σ,t
(cid:90)
= − lim div(u∗(x|s)p (x|s))q(s)ds
t σ,t
σ→0
Then by DCT,
(cid:18)(cid:90) (cid:19)
=− lim div u∗(x|s)p (x|s)q(s)ds
t σ,t
σ→0
(cid:18)(cid:90) (cid:19)
=−div u∗(x|s) lim p (x|s)q(s)ds
t σ,t
σ→0
=−div(E(u (x|s)|x (s)=x)p (x))
t t t
By definition in equation 1,
=−div(u (x)p (x))
t t
, which shows that p (·) and u (·) satisfy the continuity equation, and hence
t t
u (x) generates p (x).
t t
G Proof for gradient equivalence on stream
Recall
L (θ)=E ∥vθ(x)−u (x)∥2,
FM t,pt(x) t t
L (θ)=E ∥vθ(x)−u (x|s)∥2,
sCFM t,q(s),δ(x−xt(s)) t t
(cid:82)
where q(s)= p(s|x ,x )π(x ,x )d(x ,x ).
0 1 0 1 0 1
Theorem 2. ∇ L (θ)=∇ L (θ).
θ FM θ sCFM
Proof. To ensure existence of all integrals and to allow the changes of integral
(Fubini’s Theorem), we assume that q(s) are decreasing to zero at a sufficient
21speedas∥s∥→∞andthatu , v , ∇ v arebounded. Tosavenotations, further
t t θ t
let p (x|s)=δ(x−x (s)).
t t
The L-2 error in the expectation ca be re-written as
∥vθ(x)−u (x)∥2 =∥vθ(x)∥2+∥u (x)∥2−2⟨vθ(x),u (x)⟩
t t t t t t
∥vθ(x)−u (x|s)∥2 =∥vθ(x)∥2+∥u (x|s)∥2−2⟨vθ(x),u (x|s)⟩
t t t t t t
Thus, it’s sufficient to prove the result by showing the expectations of terms
including θ are equivalent.
First,
(cid:90)
E ∥vθ(x)∥2 = ∥vθ(x)∥2p (x)dx
pt(x) t t t
(cid:90)
= ∥vθ(x)∥2p (x|s)q(s)dxds
t t
(cid:90)
=E ∥vθ(x)∥2δ(x−x (s))dx
q(s) t t
=E ∥vθ(x)∥2
q(s),δ(x−xt(s)) t
Second,
(cid:90)
E ⟨vθ(x),u (x)⟩= ⟨vθ(x),u (x)⟩p (x)dx
pt(x) t t t t t
(cid:90) (cid:82) u (x|s)p (x|s)q(s)ds
= ⟨vθ(x), t t ⟩p (x)dx
t p (x) t
t
(cid:90) (cid:90)
= ⟨vθ(x), u (x|s)p (x|s)q(s)ds⟩dx
t t t
(cid:90) (cid:90)
= ⟨vθ(x),u (x|s)⟩δ(x−x (s))q(s)dsdx
t t t
=E ⟨vθ(x),u (x|s)⟩
q(s),δ(x−xt(s)) t t
These two holds for all t, and hence ∇ L (θ)=∇ L (θ)
θ FM θ sCFM
H Proof for gradient equivalence conditioning
on covariates
Letxberesponse,cbecovariates,andsbethestreamconnectingtwoendpoints
(x ,x ). Given covariate c, denote the conditional distribution of s as q(s|c)=
0 1
(cid:82)
p(s|x ,x ,c)π(x ,x )d(x ,x ) and marginal conditional probability path as
0 1 0 1 0 1
p (x|c). Further, let
t
L (θ)=E ∥vθ(x,c)−u (x|c)∥2
cFM t,pt(x|c) t t
L (θ)=E ∥vθ(x,c)−u (x|s)∥2
cCFM t,q(s|c),δ(x−xt(s)) t t
22Theorem 3. ∇ L (θ)=∇ L (θ).
θ cFM θ cCFM
Proof. To ensure existence of all integrals and to allow the changes of integral
(Fubini’s Theorem), we assume that q(· | c) decreases to zero at a sufficient
speed as ∥s∥ → ∞ and that vθ, u , ∇ vθ are bounded. To save notations, let
t t θ t
p (x|s)=δ(x−x (s)).
t t
The L-2 error in the expectation ca be re-written as
∥vθ(x,c)−u (x|c)∥2 =∥vθ(x,c)∥2+∥u (x|c)∥2−2⟨vθ(x,c),u (x|c)⟩
t t t t t t
∥vθ(x,c)−u (x|s)∥2 =∥vθ(x,c)∥2+∥u (x|s)∥2−2⟨vθ(x,c),u (x|s)⟩
t t t t t t
Thus, it’s sufficient to prove the result by showing the expectations of terms
including θ are equivalent.
First,
(cid:90)
E ∥vθ(x,c)∥2 = ∥vθ(x,c)∥2p (x|c)dx
pt(x|c) t t t
(cid:90)
= ∥vθ(x,c)∥2p (x|s)q(s|c)dxds
t t
(cid:90)
=E ∥vθ(x,c)∥2δ(x−x (s))dx
q(s|c) t t
=E ∥vθ(x,c)∥2
q(s|c),δ(x−xt(s)) t
Second,
(cid:90)
E ⟨vθ(x,c),u (x|c)⟩= ⟨vθ(x,c),u (x|c)⟩p (x|c)dx
pt(x|c) t t t t t
(cid:90) (cid:82) u (x|s)p (x|s)q(s|c)ds
= ⟨vθ(x,c), t t ⟩p (x|c)dx
t p (x|c) t
t
(cid:90) (cid:90)
= ⟨vθ(x,c), u (x|s)p (x|s)q(s|c)ds⟩dx
t t t
(cid:90) (cid:90)
= ⟨vθ(x,c),u (x|s)⟩δ(x−x (s))q(s|c)dsdx
t t t
=E ⟨vθ(x,c),u (x|s)⟩
q(s|c),δ(x−xt(s)) t t
These two holds for all t, and hence ∇ L (θ)=∇ L (θ).
θ cFM θ cCFM
23