MM1.5: Methods, Analysis & Insights from
Multimodal LLM Fine-tuning
HaotianZhang◦,MingfeiGao◦,ZheGan◦,PhilippDufter⋆,NinaWenzel⋆,
ForrestHuang⋆,DhrutiShah⋆,XianzhiDu⋆,BowenZhang⋆,YanghaoLi⋆,
SamDodge,KeenYou,ZhenYang,AlekseiTimofeev,MingzeXu,
Hong-YouChen,Jean-PhilippeFauconnier,ZhengfengLai,HaoxuanYou,
ZiruiWang,AfshinDehghan,PeterGrasch⋆,YinfeiYang†
Apple
{haotian.zhang2,mgao22,zhe.gan,yinfeiy}@apple.com
◦Firstauthors;⋆Coreauthors;†Projectlead
Abstract
WepresentMM1.5,anewfamilyofmultimodallargelanguagemodels(MLLMs)
designedtoenhancecapabilitiesintext-richimageunderstanding,visualreferring
andgrounding,andmulti-imagereasoning. BuildingupontheMM1architecture,
MM1.5adoptsadata-centricapproachtomodeltraining,systematicallyexploring
theimpactofdiversedatamixturesacrosstheentiremodeltraininglifecycle. This
includeshigh-qualityOCRdataandsyntheticcaptionsforcontinualpre-training,
aswellasanoptimizedvisualinstruction-tuningdatamixtureforsupervisedfine-
tuning. Ourmodelsrangefrom1Bto30Bparameters,encompassingbothdense
andmixture-of-experts(MoE)variants,anddemonstratethatcarefuldatacuration
andtrainingstrategiescanyieldstrongperformanceevenatsmallscales(1Band
3B).Additionally,weintroducetwospecializedvariants: MM1.5-Video,designed
for video understanding, and MM1.5-UI, tailored for mobile UI understanding.
Throughextensiveempiricalstudiesandablations,weprovidedetailedinsights
intothetrainingprocessesanddecisionsthatinformourfinaldesigns, offering
valuableguidanceforfutureresearchinMLLMdevelopment.
1 Introduction
MultimodalLargeLanguageModels(MLLMs)haveemergedasanincreasinglyactiveresearchtopic
inrecentyears. Closed-sourcemodels,suchasGPT-4o[51],GPT-4V[125],Gemini-1.5[149,130],
andClaude-3.5[5],havedemonstratedremarkablecapabilitiesinadvancedmultimodalunderstand-
ing. Meanwhile, open-sourcemodels, suchastheLLaVAseriesofwork[102,100,101,74], In-
ternVL2[21],Cambrian-1[151]andQwen2-VL[9,150],arerapidlynarrowingtheperformancegap.
Therehasalsobeengrowinginterestindevelopingmodelscapableofunderstandingsingle-image,
multi-image,andvideodatausingasinglesetofmodelweights[74].
BuildinguponthesuccessofMM1[118],weintroduceMM1.5,anewfamilyofMLLMscarefully
designedtoenhanceasetofcorecapabilities. Specifically,wefocusonthefollowingaspects.
• OCR.BuildinguponrecenttrendsindevelopingMLLMswithhigh-resolutionimagecomprehen-
sion[182,21],MM1.5supportsarbitraryimageaspectratiosandresolutionsofupto4Megapixels.
By incorporating carefully selected OCR data to enhance text comprehension across different
trainingstages,MM1.5excelsatunderstandingtext-richimages.
• Visualreferringandgrounding. MM1.5offersrobust,fine-grainedimageunderstanding,extend-
ingbeyondtextpromptstointerpretvisualpromptssuchaspointsandboundingboxes. Moreover,
Preprint.
4202
peS
03
]VC.sc[
1v66502.9042:viXraLanguage Response
Decoder Only LLM
Coordinate tokens
Image Encoder Language Instruction
Image patches
Dynamic Splitting Dynamic Splitting
Single Image Multi Image
Figure1: Theoverviewofmodelarchitecture. MM1.5excelsat(i)text-richimageunderstanding
withdynamicimagesplitting,(ii)visualreferringandgroundingwithcoordinatetokens,and(iii)
multi-imagereasoning.
MM1.5cangenerategroundedresponsesbygroundingtextoutputwithimageboundingboxes.
Thiscapabilityisnotablyunder-exploredinmostopen-sourcemodels(e.g.,LLaVA-OneVision[74]
andPhi-3-Vision[3]),andeveninstrongproprietarymodelslikeGPT-4o,whichrelyonset-of-mark
(SoM)prompting[167]toreferenceimageregions.
• Multi-imagereasoningandin-contextlearning. MM1.5benefitsfromlarge-scaleinterleaved
pre-training,resultinginstrongin-contextlearningandmulti-imagereasoningcapabilitiesright
outofthebox. Wefurtherimproveitscapabilitiesviasupervisedfine-tuning(SFT)onadditional
high-qualitymulti-imagedata,similartomethodsexploredin[53,77].
Ourprimaryfocusisonthemostefficientmodelscales,1Band3B,anddemonstratesthatevenrela-
tivelysmallMLLMscanachievecompetitiveperformanceonvariousdownstreamtasks. Specifically,
wepresenttwotypesofmodelsinthisregime.
• Densemodels:Availablein1Band3Bsizes,thesemodelsarecompactenoughforeasydeployment
onmobiledevicesyetpowerfulenoughtooutperformlargeropen-sourcemodels.
• Mixture-of-Experts(MoE)models: TheMoEmodels,alsoofferedin1Band3Bvariantswith64
experts,enhanceperformancewhilemaintainingaconstantnumberofactivatedparametersduring
inference.
Beyond the smaller model scales, we further demonstrate that the MM1.5 recipe exhibits strong
scalingbehaviorallthewayto30Bparameters,achievingcompetitiveperformanceacrossawide
rangeofbenchmarks.
MM1.5isageneral-purposemodel;however,thereareinstanceswherespecializedmodelsareneeded
forspecificdownstreamapplications. Tothisend,wedeveloptwoadditionalvariants:
• MM1.5-Video,avariantforvideounderstanding. Weexplorebothtraining-freemethodsusing
MM1.5trainedsolelyonimagedata,aswellassupervisedfine-tuningonvideo-specificdata.
• MM1.5-UI, a tailored version of MM1.5 focused on mobile UI understanding (e.g., iPhone
screens)[44,171],wherevisualreferringandgroundingplayacriticalrole.
BuildingperformantMLLMsisahighlyempiricalendeavor. Whiletheoverarchinggoalandthe
high-leveltrainingprocedurearewell-defined,thefinerdetailsoftheirexecutionremainunclear. In
developingMM1.5,wechoosetoretainthesamemodelarchitectureasMM1[118],enablingusto
2Stage-1: Large- Stage-2: High-res Stage-3: High-res
scale Pre-training Cont. Pre-training SFT
The impact of image- The impact of OCR
The exact impact of The optimal data
text and text-only data and synthetic
each data category mixing ratio
pre-training data captions
Figure 2: Recipe for building MM1.5. Model training contains three stages: (i) large-scale pre-
trainingwithlow-resolutionimages(378×378),(ii)continualpre-trainingwithhigh-resolution(up
to4Megapixels)OCRdataandsyntheticcaptions,and(iii)supervisedfine-tuning(SFT).Ateach
stage,weaimtoidentifytheoptimaldatamixandassesstheimpactofeachdatatype.
focusonrefiningandinvestigatingtheintricaciesofourdata-centrictrainingrecipes. Ourattentionis
centeredonthefollowingkeyaspects:
• ContinualPre-training. Weintroduceanadditionalhigh-resolutioncontinualpre-trainingstage
preceding the SFT stage, which we found crucial for boosting text-rich image understanding
performance. Weablatetheimpactoftwokindsofhigh-qualitydataforthisstage:
– Weexploredtext-richOCRdataforcontinualpre-training,focusingondetailedtranscriptionof
textwithinimages[46,68].
– Wealsoexperimentedwithhigh-qualitysyntheticimagecaptions,whichareeitherpublicdaAtapple Confidential–Internal Use Only
orgeneratedusinganimagecaptionerbasedonapreviouslytrainedMM1model.
• SFT.WhileconsiderablepriorworkdiscussesSFTdataforMLLMs,thereisstilllimitedexploration
intohoweachcategoryofSFTdatainthemixturecanaffectthefinalmodel’sperformance. In
particular,theimpactofdatasupportingeachcapabilityonothercapabilitiesisunderstudied. We
conductextensiveablationstoidentifytrade-offsandsynergies,ultimatelyconstructingamixture
frompublicdatasetsthatcontributestowell-balancedperformanceacrossawidesetofcapabilities
inMM1.5.
• Dynamic High-resolution. Furthermore, for high-resolution image encoding, we follow the
popularany-resolutionapproach,dynamicallydividingtheimageintosub-images[99,93,182],
andconductthoroughablationstorefinekeydetailsinourdesign.
Unlike most open-source models focusing solely on SFT [102, 100, 101], MM1 demonstrated
strongzero-shotandfew-shotlearningcapabilitiesthroughlarge-scalepre-training. Indeveloping
MM1.5,weaimtoretainthesestrengthsandmoreeffectivelytransferthemtotheSFTstage. To
achievethis,wefurtherextendMM1’spre-trainingbyexploringtheimpactoftext-onlydataand
optimizingtheratioofdifferentpre-trainingdatatypes. Thisapproachimprovesperformanceon
knowledge-intensivebenchmarksandenhancesoverallmultimodalunderstandingcapabilities.
Ourmaincontributionsaresummarizedasfollows: (i)WeintroduceMM1.5,afamilyofMLLMs
thatincludebothdensemodels(rangingfrom1Bto30B)andMoEvariants. MM1.5representsa
significantupgradeoverMM1[118],excellinginhandlingawiderangeofmultimodaltasks,from
general-domaintotext-richimageunderstanding,coarse-tofine-grainedunderstanding,andsingle-
tomulti-imagereasoning. (ii)Wepresenttwospecializedvariants:MM1.5-Video,designedforvideo
understanding,andMM1.5-UI,tailoredformobileUIunderstanding. (iii)Weconductathorough
empiricalstudydetailingtheprocessanddecisionsleadingtoourfinaldesignchoices.
2 RelatedWork
MultimodalLargeLanguageModels(MLLMs)[125,51,149,76,49]haverecentlyemergedasa
significantareaofresearchfocus. ThedevelopmentofMLLMscanbetracedbacktoFrozen[153]
andFlamingo[4,6],withmorerecentadvancementssuchasLLaVA[102]andMiniGPT-4[191]
introducingtheconceptofvisualinstructiontuning. Thepastyearhaswitnessedaboomofopen-
sourceMLLMs,someofwhichclaimtorivalGPT-4ooncertainbenchmarks. Notableexamples
3Refer & Ground
"
What can I make with these ingredients <111,24,315,229> <308,235,460,314> <383,372,571,485>?
!
The ingredients <111,24,315,229> <308,235,460,314> <383,372,571,485> can be used to make a delicious cake. The
first ingredient is cake flour, the second one is eggs, and the third one is butter. These are all essential components for
baking a cake.
"
Should I add this stuff <101,306> to my cake too?
!
Yes, you should definitely add the canned cherries <54,211,165,388> to your cake. They will add a sweet and tangy flavor
to the cake, enhancing its taste.
"
If we want to make a cake, choose the best ingredients in the image.
!
To make a cake, you would need flour, sugar, eggs, butter, and baking soda from the image.
"
Can you show me their coordinates?
!
Sure! The flour <112,26,316,239>, sugar <306,19,484,251>, eggs <312,239,392,312>
<386,243,461,316>, butter <383,371,571,481>, and baking soda <457,155,567,308> are the
ingredients you would need for a cake.
Multi-Image Math/Reasoning
"
" Try to reason about the question step by step.
Help me generate a Twitter post providing these images. !
! To find the skier's speed at the bottom, we can use the conservation of
Step 1: Identify the main subject of each image. energy principle. At the start, the skier has potential energy, and at the
end, the skier has kinetic energy. Since the energy is conserved, we can
•Image 1: The main subject is a crowd of people at a concert. set up the following equation:
•Image 2: The main subject is the Sagrada Familia, a large Gothic-style church Start: E=mgh End: E=(1/2)mv2
in Barcelona.
where E is the energy, m is the mass of the skier, g is the acceleration
•Image 3: The main subject is a cup of coffee with a heart-shaped foam design. due to gravity (9.81 m/s2), h is the height, and v is the final velocity.
Step 2: Determine the tone and theme of each image. We are given the initial height (40 m) and the final height (0 m) since
•Image 1: The tone is energetic and lively, with a focus on the vibrant the skier reaches the ground. We can plug these values into the
atmosphere of a concert. equation and solve for the final velocity (v):
•Image 2: The tone is historical and architectural, showcasing the grandeur of (1/2)mv2=mgh
the Sagrada Familia. Divide both sides by m:
•I em nja og ye
in
3 g: aT ch ue
p
t o on
f
e
c
ois
ff
ec eo .z y and inviting, with a focus on the comforting act of (1/2)v2=gh
Step 3: Craft a Twitter post that incorporates the main subject and tone of Now, plug in the given values for g and h:
each image. (1/2)v2=(9.81m/s2)(40m)
"Experience the energy of a live concert at [Concert Venue] or take a moment to (1/2)v2=392.4J
appreciate the architectural beauty of [Sagrada Familia] with [Tourist Multiply both sides by 2:
Attraction]. Enjoy a warm cup of coffee with a heart-shaped foam design from
[Coffee Shop]." v2=784.8J
Now, take the square root of both sides to find the final velocity (v):
v= 784.8J
v≈28m/s
So, the skier's speed at the bottom is approximately 28 m/s.
Text Rich
"
Read out the abstract of this paper.
!
In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study
the importance of various architecture components and data choices. Through careful and comprehensive ablations of
the image encoder, the vision language connector, and various pre-training data choices, we identify several crucial
design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of
image-captioning, the interleaved image-text and text-only data is crucial for achieving state-of-the-art (SOTA) few-
shot results across multiple benchmarks, compared to other published multimodal pre-training results. Further, we
show that the image encoder together with image resolution and the image token count has substantial impact, while
the vision-language connector design is of comparable negligible importance. By scaling up the presented recipe, we
build MM1, a family of multimodal models, including both dense variants up to 30B and mixture-of-experts (MoE)
variants up to 64B, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-
tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing
properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought
prompting.
Figure3: ExamplesofMM1.5capabilities. Theexampleswedemonstratedaregeneratedbythe
MM1.5-3Bmodel. MoresamplescanbefoundinAppendixA.8.
4includeEmu2[144,143],VILA[97],Idefics2/3[68,66],Cambrian-1[151],InternLM-XComposer-
2.5 [26, 182], InternVL2 [22, 21], MiniCPM-V [169], CogVLM2 [156, 43], BLIP-3 [82, 166],
LLaVA-OneVision[77],Llama3.1-V[29],andthelatestQwen2-VL[9].
ResearchinMLLMshasexpandedacrossseveralfronts: (i)scalingupthepre-trainingdata[97,
118, 166, 7, 87] and supervised instruction-tuning data [47, 148, 68, 151]; (ii) enhancing high-
resolutionimagecomprehension[99,93,101,26,37,38,17,185,165,91];(iii)exploringvarious
vision encoders [152, 135] and vision-language connectors [14, 168, 88, 13]; (iv) using mixture-
of-experts[95,80];(v)extendingLLaVA-likearchitecturestoregion-level[157,187,179,127,16,
184,170,181]andpixel-level[65,129,176,131]understanding,multi-imagereasoning[53,77],UI
comprehension[171,44],andvideounderstanding[96,163,164],amongothers.
Among the extensive body of literature on MLLMs, MM1.5 distinguishes itself as a significant
upgrade over its predecessor, MM1 [118]. The MM1.5 model family integrates a diverse set of
core capabilities, including text-rich image understanding, visual referring and grounding, and
multi-imagereasoning. Incontrast,recentgeneral-purposeMLLMssuchasCambrian-1[151]and
LLaVA-OneVision[77]haveshownlesssatisfactoryperformanceinhandlingreferringandgrounding
tasks,andGPT-4ohastorelyonset-of-mark(SoM)prompting[167]tounderstandimageregions.
Whileseveralrecentworkshaveopen-sourceddetailedSFTdatamixturesforpublicuse[68,151],
thepreciseimpactofeachdatacategoryandthebestrecipetocombinethemremainunder-explored.
Thisisparticularlytrueformodelsrequiringdiversecapabilities. MM1.5standsoutbyprovidinga
comprehensiveempiricalstudythatpresentsmaturerecipesforbuildingperformantMLLMs. The
extensionofMM1.5tomobileUIunderstandingfurtherenhancestheuniquenessofthiswork.
AnotheremergingtrendinthefieldisthedevelopmentoflightweightMLLMsforpotentialedge
deployment[55,48,11,104,42,91,190,41]. InMM1.5,modelswith1Band3Bparametersare
offered,whichoutperformsimilar-sizedmodels,suchasPhi-3-Vision[3]andMiniCPM-V[169].
3 RecipeforBuildingMM1.5
DevelopingandimprovingMLLMsisahighlyempiricalpractice. Inthiswork,beyondincluding
pre-training and SFT stages as in MM1 [118], we introduce a continual pre-training stage with
high-qualityOCRdataandsyntheticcaptions. AsoutlinedinFigure2,toobtainthebestdatarecipe,
• WefirstpresentcomprehensiveablationsofourSFTdatamixture(Section3.2). Wecategorizethe
SFTdataintomultiplegroupsbasedonthecapabilitiestheyaimtosupport. Wecarefullyevaluate
theimpactofdatasetsfromeachcategoryandadjusttheratioofeachcategoryinourfinalmixture
tobalancedifferentcorecapabilities.
• Tofurtherenhancemodelperformance,especiallyfortext-richimageunderstanding,wefurther
ablatethedatachoicesforcontinualpre-training(Section3.3). Thisincludes45millionrichOCR
data and 7 million high-quality image captions generated by a previously trained MM1-based
imagecaptioner. SimilarideashavealsobeenexploredinVILA2[30]andLLaVA-OneVision[74].
• Finally,toenhanceperformanceonknowledge-heavybenchmarkslikeMMMU[177],wefurther
studytheimpactofpre-trainingdata(Section3.4). Weretainthesameimage-captionandinter-
leavedimage-textdatafromMM1[118],updatethetext-onlydata,andcarefullyadjustthedata
mixratio,resultinginasignificantlyrefinedfinaldatacomposition.
Besides data ablation, we also provide detailed ablation regarding dynamic image splitting, also
knownasAnyRes[101](Section3.5,alsoseeFigure1),forhigh-resolutionimagecomprehension.
3.1 EmpiricalSetupforAblations
Unlessotherwisenoted,wefollowthedefaultsettingsbelowinourablationstudies.
Modelarchitectureanddatapreprocessing. WeusethesamearchitectureasMM1[118],focusing
onthe3Bdensemodelforalltheablationstudiesinthissection. Specifically,
• Staticimagesplitting[99]isenabledwith4sub-imagesplits(plusanoverviewimage),andeach
sub-imageisresizedto672×672resolutionviapositionembeddinginterpolation. Notethatwedid
notusedynamicimagesplittingduringablationforfasteriterationofexperiments.
5General LLaVA1.5conversation[100](56.7k)
LLaVAv1.5VQAv2OKVQA[114,100](91.8k) LLaVAv1.5GQA[50,100](72.1k)
LLaVAv1.5A-OKVQA[133](66.2k) CocoCaptions[20](82.8k)
LLaVAComplexReasoning[102](76.6k) ShareGPT-4V[18](96.1k)
Referring& Multi TextRich TextCaps[137](22k)
Grounding
Image Synthdog-En[62](500k) ArxivQA[86](100k)
Text ScreenQA[45](80.8k) WikiSQL[189](75k)
Only WikiTQ[126](38.2k) Chart2Text[124](27k)
Knowledge SFT TabMWP[108](22.7k) TextVQA[138](34.6k)
Mixture ST-VQA[12](17.2k) RenderedText[1](10k)
General VisText[147](10k) FinQA[23](5.3k)
DeepForm[145](7k) TAT-QA[192](2.2k)
HiTab[25](2.5k) DVQA[57](200k)
TextRich
ChartQA[115](66.7k) DocVQA[117](128.5k)
InfoVQA[116](52.3k) WikiTable(35.8k)[126]
TabFact[19](91.6k) VisualMRC[146](24.5k)
KleisterCharity[140](27.7k) OCRVQA[121](80k)
TextVQA[138](34.6k)
Knowledge(Math/Science/Code) CLEVR[56](70k) IconQA[109](27.3k)
RAVEN[180](42k) Inter-GPS[106](1.3k) GeomVerse[58](9.3k)
AI2D[60](3.2k) ScienceQA[107](5k) WebSight[69](10k)
DaTikZ[10](48k) Design2Code[136](0.5k)
Referring&Grounding GRIT-VisualGenome[170,64](267.6k) GRIT-Regionreasoning[170](76.6k)
GRIT-Flickr30k[170,128](200.8k) GRIT-Refcoco[170,98,173,123](210k) GRIT-SpatialNegativeMining[170,134](213.6k)
Multi-image Birds-to-Words[53,31](2.6k) Coinstruct[53,159](132.3k)
DreamSim[53,34](15.9k) IconQA[53,109](64.5k) MultiVQA[53](5k)
NLVR2[53,141](86.4k) Spot-the-diff[53,52](8k) NExT-QA[53,161](3.9k)
Star[53,158](3k) ICL-Instruct†(0.4k) Cocoinstructinterleaved†(2.1k)
Text OpenOrca[94](994k) MathInstruct[178](262k)
OrcaMath[122](200k) WizardCoder[110](43k) OpenCodeInterpreter[188](66k)
Dolly[120](11k)
Figure4:Ahigh-qualitydatamixtureusedforMM1.5supervisedfine-tuning,including(i)single-imagedata
forenhancedmath/sciencereasoning,text-richimageunderstanding,andvisualreferringandgrounding,(ii)
multi-imagedata,and(iii)text-onlydata.(†)denotesin-housedatasetswithcurationdetailsinAppendixA.3.
• Astotheencodingofmulti-imagedata,weenableimagesplittingonlywhenthecurrenttraining
samplecontainsfewer1thanthreeimagestoavoidexcessivelylongsequencelengths.
• SimilartocapabilitiesintroducedinFerret[170],MM1.5directlysupportsreferringandgrounding.
Whenrequested,MM1.5canproduceboundingboxesinitstextualoutputtogrounditsresponses.
Additionally,themodelcaninterpretreferencestopointsandregionsintheinputimageintheform
ofreferringcoordinatesandboundingboxes(seeFigure1).
• AsinMM1,theCLIPimageencoderandtheLLMbackbonearebasedonourin-housemodels,
withC-Abstractor[14]servingasthevision-languageconnector.
Modeloptimization. Forbothcontinualpre-trainingandSFT,wesetthebatchsizeas256. Weuse
theAdaFactoroptimizerwithapeaklearningrateof1e-5andacosinedecayof0. Forcontinual
pre-training,wetrainamaximumof30ksteps. DuringSFT,allmodelsareoptimizedforoneepoch.
Continualpre-training. ModelsareinitializedwiththeMM1pre-trainedcheckpoint. Bydefault,we
conductcontinualpre-trainingon45Mhigh-resolutionOCRdata(includingPDFA,IDL,Rendered-
text[68]andDocStruct-4M[46]1)atthisstage. Ineachtrainingbatch,dataisequallysampledfrom
thosefourdatasets. SimilartotheSFTstage,weusestaticimagesplitting,dividingeachimageinto
fivesub-images,witheachsub-imageresizedto672×672resolution.Wefindthatthishigh-resolution
setupisessentialforcontinualpre-training.
SFTdatacategorization. Groupingdatasetsintocategoriescanbehelpfulfordatabalancingand
simplifyingtheanalysis [68,151]. Atahighlevel,weclusterdatasetsintosingle-image,multi-image,
andtext-onlycategoriesbasedonthenumberofimagespresentedineachexample. Forthesingle-
imagegroup,wefurtherclassifyeachdatasetintothefollowingsub-categories: general,text-rich,
refer&ground,science,mathandcode. SeeTable13inAppendixA.2forthedetailsofeachcategory
usedfortheablationstudy,andFigure4foranoverviewofthegroupcategories.
Evaluationbenchmarks. Wegroupourbenchmarksintocategoriesbasedonwhatcapabilitiesa
benchmarkprimarilymeasures. Ourbenchmarkgroupsincludegeneral,text-rich, refer&ground,
knowledge,andmulti-image. SeeTable14inAppendixA.4formoredetails. WeproposeCategory
1Weexcludethemulti-grainedtextlocalizationsplitfromDocStruct-4M,asitdoesnotshowperformance
improvementsinourexperiments.
6General General+Text-rich General+Math
General+Science General+Code General+Refer&ground
80
60
40
20
TextRich Knowledge General Refer&Ground
Figure 5: Impact of different SFT data categories to different model capabilities (general, text-
rich,knowledge,andrefer&ground). Text-richdatasignificantlyimprovestext-richandknowledge
benchmarksonaverage. Sciencedataimprovesknowledgeaveragescore. Referringandgrounding
dataenablesthiscapability.
Average Score, the average score of all benchmark numbers for each sub-category, to represent
theaverageperformanceonthatcapability. Wefocusonthecategoriesofgeneral, text-rich, and
knowledge,asthesecapabilitiesarewidelyconsideredessentialforMLLMs. Toevaluateamodel’s
impactonthesecapabilities,werefertoaMMBasescore,definedastheaveragescoresongeneral,
text-rich,andknowledgecategories. DetailsoftheevaluationmetricsareprovidedinAppendixA.4.
3.2 SFTAblations
To determine the optimal SFT recipe, we first study the impact of different data categories in
Section3.2.1,followedbyinvestigatinghowtobestmixallthedatainSection3.2.2.
3.2.1 ImpactofDifferentDataCategories
Inthissubsection,wefocusonevaluatingthesingle-imagedatacategory. Webeginbyassessing
thegeneraldatacategoryandthenprogressivelyevaluatetheimpactofaddingothersub-categories
individually. Duringtraining,wemixdatafromdifferentsub-categoriesandconstructeachtraining
batchbyrandomlysamplingdatafromthecorrespondingmixture. Wecomparemodelsusingeach
capabilityusingtheCategoryAverageScore.
Our results are summarized in Figure 5. We observe that adding text-rich data can significantly
improvetheperformanceontext-richandknowledgebenchmarks.Theinclusionofmathdatafollows
asimilartrend,thoughweobservealesserdegreeofimprovementinthetext-richaveragescore.
Whensciencedataisadded,weobservetheexpectedimprovementintheknowledgebenchmarks,
alongsideaminorimprovementintext-richperformance. Addingthecodecategoryyieldsaslight
increaseinthetext-richaveragescore,whiletheperformanceonotherbenchmarksdoesnotimprove.
Includingtherefer&grounddatainstillsthemodelwithreferringandgroundingcapability,butwe
alsoobserveslightregressioninallothercapabilitycategories.
3.2.2 DataMixtureRatioStudy
Wefirststudythemixingratiowithinthesingle-imagecategories. Sincedirectlymixingthegeneral
andtext-richdatabasedontheirdatasizesshowsstrongresultsacrossavarietyofbenchmarks(see
Figure5),weusethiscombinationasthestartingpointtostudyhowtomixothercategoriestothis
set. Then,wecombinetheentiresingle-imagesetwithmulti-imageandtext-onlysetswithsampling
weightsofw ,w andw ,respectively,wherew +w +w =1.
single multi text single multi text
Mixtureofsingle-imagedata. Directlymixingalldatasetsfromdifferentcategoriesmaynotbe
idealduetoimbalancednumbersofdatasamplesacrossdifferentsub-categories. Forexample,the
sizeofthegeneraldatacategoryisaround68×thesizeofthesciencedatacategory. Inthisstudy,we
7
erocSegarevA
68.04
85.16
14.44 41.24 4.24 96.04 10.54 97.64 73.64 88.74 36.34 2.24
9.36 62.36 71.46 28.36 80.26 62.16
10.81 99.71
9.71
51.81 28.71
64.3759
60
58
58
57
56 56
0 5·10−2 0.1 0.15 0.2 0 0.2 0.4 0.6 0.8 1
α α
(a)ScienceMixture. (b)MathMixture.
58 80
57 60
56 40
55 20
0 0.1 0.2 0.3 0.4 0 0.5 1 1.5 2
α α MMBaseScore
Refer&GroundAverageScore
(c)CodeMixture. (d)Refer&GroundMixture.
Figure6: Impactofαfordifferentdatacategoriestoamodel’sdifferentcapabilities. Theselected
ratioismarkedwithred“x”. αdenotesthedataratioofthetargetcategory(science,math,code,
refer&ground)whencomparedwiththegeneralcategory.
MMBaseScore Multi-imageAverageScore
60 60
58
55
56
50
54
45
52
50 40
0 5·10−2 0.1 0.15 0.2 0 0.1 0.2 0.3 0.4 0.5
Text-onlydatamixtureratio Multi-imagedatamixtureratio
Figure7: Impactofthemixingratiofortext-onlyandmulti-imageSFTdata. Theselectedratiois
markedwithred“x”.
usethegeneraldatacategoryasthereference,andupsample/downsampledatafromatargetcategory,
suchthatineachtrainingbatch,thedataratiofromthegeneralandtargetcategoryis1:α.
Tomeasuretheaverageimpactofα,weproposeMMBasescore,anaverageovergeneral,text-rich,
and knowledge average scores, for model comparison. As shown in Figure 6, we vary the α for
differentdatacategories. Forscience,math,andcodecategories,wefindthebestratioofαtobe
0.1,0.5,and0.2,respectively. AsshowninSection3.2.1,therefer&grounddataisthemaindriver
forimprovingreferringandgroundingbenchmarks. Therefore,besidestheMMBasescore,wealso
includetheRefer&Groundaveragescoreasanothermetricfortheαselection. Assummarizedin
Figure6(d),theMMBasescorewilldropslightly,whiletheRefer&Groundaveragescoreincreases
significantly. Withthat,weselectα=2.0asagoodtrade-off.
Mixture of single-image, multi-image, and text-only data. Now, we study the mixture ratios,
w ,w andw . Enumeratingallcombinationsbetweenthethreeratioswillincursignificant
single multi text
computationalcost. Therefore,weinsteadseparatelyablatew andw fortext-onlyandmulti-
text multi
imagedata, respectively, toevaluatehowsensitiveourmodelistotheseratios. Finally, w is
single
determinedby1−w −w .
text multi
Similartothesingle-imagemixturestudy,wealsostartwiththecombinationofgeneralandtext-rich
dataandenumeratedifferentvaluesforw andw . Fortext-onlydata,wetestedw from0to
multi text text
0.2. Figure7(left)showsthatvaryingdifferentvaluesforw hasminoreffectsonthemodel’sbase
text
8BaseMixture SingleImageMixture AllMixture
80
60
40
20
Text-rich Knowledge General Refer& Multi- Average
Ground image
Figure8: AblationstudyofmixingalltheSFTdata. BaseMixturedenotesgeneral,text-richand
knowledge(science,mathandcode). The“Average”columnrepresentstheperformanceaveraged
acrosstheprecedingfivebenchmarkcategories.
capabilitiesingeneral. Weselectw = 0.1toallocateahigherweightforsingle-imagedatafor
text
potentialperformanceimprovements.
Formulti-imagedata,weusethemulti-imageaveragescore(evaluatedonmulti-imagebenchmarksin
Table14)asanadditionalmetrictoassessamodel’scapabilityofhandlingmulti-imagetasks. Results
aresummarizedinFigure7(right). Weobservethatincreasingthesamplingratioofmulti-imagedata
wouldintroduceaperformancedropofthebasecapabilitiesasindicatedbythedecreasednumberof
theMMBasescore,whilethemulti-imageaveragescoreincreases. Weselectw =0.1sinceit
multi
introducesasurgeinthemulti-imageaveragescore.
Mixingmultiplecategories.Basedonthestudiesabove,wepresentthreemixtures,theBasemixture,
Single-imagemixture,andAllmixture,andanalyzetheirtrade-offs. TheBasemixtureincludesthe
general,text-rich,science(α = 0.1),math(α = 0.5)andcode(α = 0.2)datagroups.
science math code
TheSingle-imagemixtureadditionallyaddsrefer&grounddata(α =2.0)totheBasemixture. All
rg
mixture includes all data from single-image, multi-image, and text-only data, with w = 0.8,
single
w =0.1,andw =0.1.
multi text
OurresultsaresummarizedinFigure8. Thefirstthreecolumnsindicatethatincludingrefer&ground
andmulti-imagedataslightlyreducesaverageperformanceontext-rich, knowledge, andgeneral
benchmarks. Thefourthcolumnshowsthataddingrefer&grounddatasignificantlyboostsreferring
andgroundingperformance,whilethefifthcolumnhighlightsthataddingmulti-imagedatagreatly
improvesmulti-imagebenchmarks. Thefinalcolumnrevealsthatouroptimizedmixtureachievesthe
bestoverallperformance,balancingallcapabilitiesacrossbenchmarks.
3.3 ContinualPre-trainingAblations
Unless otherwise specified, we use OCR data (45M in total), including PDFA, IDL, Rendered-
text[68]andDocStruct-4M[46]inahigh-resolutionsetting(1344×1344)forcontinualpre-training.
DuringtheSFTstage,allcontinualpre-trainedmodelsinthissectionarefine-tunedwithdatafrom
theBaseMixtureincludinggeneral,text-rich,knowledge(science,math,andcode)withtheirselected
mixtureratiosasdescribedinSection3.2.2.
Impactofimageresolution. Intuitively,higher-resolutionimagesarepreferablewhentrainingwith
OCRdata. Wefirstablatetheimpactofimageresolutionduringthisstagebysettinguptwobaselines,
continual pre-training with 378×378 and 756×756 resolutions, respectively. For the former, we
disabledbothimagesplittingandpositionembeddinginterpolation(ourCLIPimageencodernatively
supports image resolution of 378×378). For the latter, we enabled image splitting and turn-off
positionembeddinginterpolation. TheresultsareshowninFigure9(a). NotethatthefinalSFTstage
alwaysusesimageresolution1344×1344acrosstheseexperiments,sothetrainingonlydifferswith
respecttotheimageresolutionusedincontinualpre-training.
9
erocSegarevA
32.26 79.06 21.95
51.35 45.25 39.15
14.56 60.46 54.46
20.81
97.47 88.47
12.54 95.34
87.55
18.84
91.95 32.1661 61
60 60
59 59
58 58
No
Cont.PT 378×378 756×756 1344×1344
No
Cont.PT LO LC aR VARe Sc ha ap reGP +LT4 LaV VAR +e Sc ha ap reGPT4V
R R
C C
O O
(a)Impactofinputresolution.OCRdataisusedforall (b) Impact of data source. Continual pre-training is
thecontinualpre-trainingexperiments. conductedinthehigh-resolution(1344×1344)setting.
Figure9: Ablationstudyofcontinualpre-training. AverageScoreindicatestheMMBasescore. Cont.
PTdenotescontinualpre-training.
Wecanclearlyseethatusingasettingof1344×1344imageresolutionforcontinualpre-training
achievesthebestoverallperformance. Decreasingresolutionconsistentlyleadstolowerfinalscores.
Inparticular, continualpre-trainingwith378×378resolutioncanunderperformamodelwithout
continualpre-training. Wehypothesizethisisduetoinsufficientvisibledetailatlowerresolutions,
whichmayhinderthemodel’sabilitytoeffectivelylearnfromthedocument-basedOCRdatainthe
continualpre-trainingmixture.
ImpactofOCRdataandsyntheticcaptions. BesidesOCRdata, high-qualitysyntheticimage
captions[18,71]arealsowidelyconsideredusefulforpre-training. Tostudyitsimpact,weuseour
defaultsettingexceptforthedatausedincontinualpre-training. Westudytwosyntheticcaption
datasets: LLaVA-Recap-3M[71]andShareGPT4V-PT[18],andtheircombinationwithourOCR
data. When we combine ShareGPT4V-PT or LLaVA-Recap-3M with our OCR data, we equally
sampledatafromindividualdatasetsineachtrainingbatch. ResultsarepresentedinFigure9(b).
Weobservethatallcontinualpre-trainedmodelsperformbetterthanthebaselinewithoutcontinual
pre-training. However,wedidnotfindconclusiveevidencethatthesehigh-qualitysyntheticcaptions
improvedperformanceoverthearguablysimplerOCRdata. Whilepriorstudies[74]showsynthetic
captionsboostperformance,ourresultsindicatefurtherinvestigationintotheirexactimpactisneeded.
Therefore,wefurtherinvestigatetheimpactofsyntheticcaptionsgeneratedthroughself-trainingfor
evenlargerscales(upto7M)andmorecontrollablestyles,usingapre-trainedMM1modelfine-tuned
onhuman-annotatedcaptions, similarto[30]. Thisnewdatasetshowedsomepromiseincertain
settings,seeAppendixA.1fordetails. Wedeferfurtherstudyintothistopictofuturework.
3.4 Pre-trainingAblations
BeyondtheSFTandcontinualpre-training,weemphasizetheimportanceoflarge-scale,task-specific
data used during pre-training in establishing robust foundations for models to effectively handle
diversetasks. Forknowledge-heavybenchmarkslikeMMMU[177],wefoundthatmodelperfor-
manceishighlysensitivetoitstextcomprehensioncapabilities. TheLLM’sabilitytounderstandand
processtextualcontentispivotalinaddressingthecomplexreasoningandknowledge-representation
challengesposedbythesebenchmarks,asalsoobservedinCambrian-1[151].
Weincorporatedahigher-qualityandmorediversesetoftext-onlydatasets,referredtoasHQ-Text,
introducedby[39],duringthepre-trainingphase. Thesedatasetswerespecificallycuratedtoenhance
themodel’slanguagecapabilitiesbyprovidingdeeperandmorevariedtextualcontexts,withafocus
ongeneralknowledge,mathematics,andcoding. Thisupdateaimstostrengthenthemodel’sability
inlanguage-basedreasoning.
10
erocSegarevA
8.85
82.85
14.95
62.06
erocSegarevA
8.85
62.06
84.95
95
77.95
15.95settingsinMM1(45:45:10) HQ-Text(45:45:10) HQ-Text(50:10:40)
80
70
60
50
Text-rich Knowledge General Refer& Multi- Average
Ground image
Figure10: Performancecomparisonofallcategoriesacrossdifferenttext-onlydataandpre-training
dataratio. ThefigurehighlightstheperformanceimprovementwhenreplacingwithHQ-Textdata
andtheadditionalgainsachievedbyadjustingtheratioto50:10:40. Notethatthedefaultsettingfor
continualpre-training(OCR)andAllMixtureforSFTareusedforallmodels.
As shown in Figure 10, by simply replacing with the new data, the average score on knowledge
improvesby0.85point.
Inconjunctionwiththetext-onlydatasetsandthelatestSFTrecipesdiscussedinSection3.2,we
furtherrefinedourpre-trainingdatacomposition. TheoriginaldataratioproposedinMM1[118]
was45:45:10forimage-caption,interleavedimage-text,andtext-onlydata,respectively. Further
experimentsrevealedthatdecreasingtheamountofinterleavedpre-trainingdataand,respectively,
increasingtheweightoftext-onlydatatoaratioof50:10:40resultedinimprovedperformanceacross
most tasks after SFT. We note that in contrast to pre-training ablations in MM1, for MM1.5, we
conductevaluationsondownstreambenchmarkspostSFTtoselectourfinalpre-trainingmixture.
We hypothesize that relying primarily on few-shot pre-training metrics may not be ideal, as the
improvementsonsuchevaluationsmaynoteffectivelytransfertodownstreamperformance. Our
newlyoptimizeddatamixforMM1.5notonlyenhancesmultimodalcapabilitiesbutalsostrengthens
languageunderstanding,leadingtosuperioroverallperformanceacrossbenchmarks.
Withtheupdatedmixture,performanceontext-richaverageincreasedby0.85,knowledgeaverage
by0.99,andrefer&groundtasksbyaround1.4,asshowninFigure10. Althoughtherewasaslight
decreaseof0.05onmulti-imagedatasetsduetothelowerweightingofinterleaveddata,weconsider
thistrade-offreasonableformaintainingstrongperformanceacrossalltasks.
3.5 DynamicImageSplittingAblations
Toeffectivelyprocessimagesofvariableaspectratiosandresolutions,weintroduceadynamicimage
splittingmethodforhigh-resolutionimageencoding. Wealsodetailtheablationsettingsandthe
correspondingresultsforthisproposedsplittingmethod.
Dynamicimagesplitting. Processinghigh-resolutionimagesisessentialfortext-richimageunder-
standing. Instaticimagesplitting[99],imagesaresplitintomultiplesub-imagesandindividually
encodedbythevisionencoder.TheLLMthenhasaccesstomultipletilesofthesameimage,resulting
inahighereffectiveresolution.However,splittingeachimageintoarigid2×2gridisofteninefficient.
Low-resolutionimagesaresplittedwithoutanyneed,andimageswithnon-squareaspectratioscan
resultinsub-imagesbeingpaddingonly. Therefore,weadoptadynamicimagesplittingapproach,
whichiscommonintheliterature [71,27,46,99,165,185],forMM1.5.
Given a minimum and maximum number of sub-images, n and n , consider the set of all
min max
candidategridsG={(n ,n )∈N|n ≤n ·n ≤n }. Further,considertheresolutionof
h w min h w max
thevisionencoderr,andaninputimageresolution(h,w). Ifthereisagridthatcancovertheimage,
wechoosethegridthatminimizestheamountofpaddingafterlongersideresizingtothegrid,i.e.,
g∗ =arg min n n r2−h w , (1)
h w g g
(nh,nw)=g∈G
11
erocSegarevA
51.26 46.26 94.36
44.65 92.75
82.85
55.56 21.56 42.56
14.67
21.87 5.97
42.75 06
59.95
65.36
46.46 92.56padding
resolution loss due to downscaling
Figure11: Illustrationofimagegridselectionusedindynamicimagesplittingforhigh-resolution
imageencoding. (Left)Ifthegridcancoverthefullimagewithoutscalingdown,wechoosethegrid
thatminimizespadding. (Right)Otherwise,wechoosethegridthatminimizestheresolutionlossdue
toscalingdown.
subjectton r ≥h ≥handn r ≥w ≥w,whereh ,w denotetheimageheightandwidthafter
h g w g g g
longersideresizingthecandidategrid. Ifnosuchgridexists,wechoosetheonethatminimizesthe
resolutionlossduetoscalingtheimagedownandfullycoversthelongersideresizedimage.
Apple Confidential–Internal Use Only
Figure 11 visualizes which areas are minimized for the two scenarios. Assume we al-
low up to 4 sub-images. With a static image splitting approach, all images use the
grid (2,2). The dynamic splitting approach instead allows for the following grids:
{(1,1),(1,2),(2,1),(1,3),(3,1),(1,4),(4,1),(2,2)}.
Global-Local Format. In addition to the sub-images, we always feed the original image with a
longersideresizedtotheencoderresolutionrtothemodel. Thisensuresthatthemodelhasaglobal
understanding of the image. If the grid is (1,1), we omit the overview image. We consider two
variants: (i)before: theoverviewimageisputbeforethesub-images;(ii)after: theoverviewimage
isputafterthesub-images. Thesevariantsyielddifferentresultsbecauseanautoregressivemaskis
usedintheLLMdecoder,andassuch,thechoicedetermineswhetherthedecodercanattendtothe
overviewimagewhenprocessingthesub-images(i)orattendtothesub-imageswhenprocessingthe
overviewimage(ii).
Sub-imagepositionindicator. Giventhataninputimageisdynamicallysplitintomultiplesub-
images,weexplorewhetheritishelpfultoindicatethepositionofeachsub-imageintheoriginalhigh-
resolutionimagetoensurethemodelcanunderstandtheoriginal2Dimagestructure. Specifically,
weconsidertwomethods.
• Index. A tuple of (k,i,j) is used to represent sub-image position information, where k is the
zero-indexedimagenumberintheexample(assumingtherecanbemultipleimagesinatraining
sample),iandjaretheone-indexrowandcolumnid,e.g.,(0,0,0)istheoverviewimageofimage
0,and(0,2,1)isthesub-imageinthesecondrowandfirstcolumn,forimage0.
• Seps. Insteadofusingindexes,weusethreetexttokens. Specifically,‘:’ istheoverviewimage
indicator, ‘,’ isthecolumnseparator, and‘<n>’istherowseparator. Thelattertwotokensare
insertedbetweenthesetofimagetokenscorrespondingtoeachsub-imagesothattheoriginal2D
imagestructurecanberecoveredfromtheflattenedimagetokensequence.
Inference for higher resolution. The tuple (n ,n ) is used to decide the dynamic image
min max
splittingconfigurationformodeltraining. Duringinference,itispossibletosupportevenhigher-
resolution image processing simply by increasing these parameters. For example, we explore
trainingat(n ,n ) = (4,9)tosavemodeltrainingcompute,whileduringinference,weuse
min max
(n′ ,n′ )=(4,16)toprocessimagesatevenhighereffectiveresolutions.
min max
3.5.1 AblationResults
Inthissection,weusethefinalSingle-imageMixtureasourdefaultexperimentsetting,including
general,text-rich,knowledge(science,mathandcode),andrefer&grounddata. Forfastiterationof
experiments,allthemodelsareinitializedwiththeMM1pre-trainedcheckpointwithoutcontinual
12Row #imagetokens ImageEnc. Effective Refer&
Mode n Text-rich Knowledge General Average
# (persub-img/total) Resolution Resolution Ground
1 1 144/144 672×672 0.45MP 49.4 53.6 62.6 71.3 59.2
Static
2 5 144/720 672×672 1.8MP 57.7 53.8 64.4 74.8 62.7
3 5 144/720 672×672 1.8MP 58.6 53.7 64.1 74.0 62.5
4 10 81/810 378×378 1.3MP 57.6 53.3 62.9 74.0 62.0
5 Dynamic 10 81/810 672×672 4.1MP 58.3 53.8 64.3 74.9 62.8
6 10 144/1440 378×378 1.3MP 58.5 54.0 63.2 74.5 62.6
7 10 144/1440 672×672 4.1MP 59.8 54.0 64.5 75.2 63.3
Table1: Ablationontheimageresolutionandthenumberofimagetokensusedindynamicimage
splitting. n denotes the total number of sub-images. Row 3: (n ,n ) = (4,4); Row 4-7:
min max
(n ,n )=(9,9).Imageencoderresolution:(i)378×378:nopositionembeddinginterpolation;
min max
(ii)672×672: withpositionembeddinginterpolation.
Row (n min,n max)
DocVQA InfoVQA Text-rich Knowledge General
Refer&
Average
# Train Inference Ground
3BModelComparison
1 (4,4) (4,4) 73.2 48.3 58.6 53.3 64.1 74.0 62.5
2 (4,9) (4,9) 75.7 53.8 60.0 54.0 63.9 74.6 63.1
3 (4,16) (4,16) 76.3 55.2 60.7 53.4 64.0 73.8 63.0
4 (1,9) (1,9) 76.2 54.1 60.4 53.7 62.5 71.5 62.0
5 (4,4) (4,9) 73.4 52.9 59.7 53.5 63.8 74.0 62.8
6 (4,4) (4,16) 72.3 53.5 59.6 53.8 63.5 74.0 62.7
7 (4,4) (1,9) 73.5 52.7 59.8 50.7 62.6 24.5 49.4
7BModelComparison
8 (4,4) (4,4) 77.0 54.3 64.5 61.1 66.8 77.7 67.5
9 (4,9) (4,9) 81.7 62.1 67.4 60.1 66.6 78.0 68.0
10 (4,16) (4,16) 83.3 64.1 68.0 58.7 67.7 77.2 67.9
Table2: Ablationontheimagegridconfiguration(n ,n )usedindynamicimagesplitting.
min max
pre-training. FollowingFigure5,wereporttheaverageperformanceontext-rich,knowledge,general,
andrefer&groundbenchmarks. Ourfindingsaresummarizedasfollows.
Impactofimageresolutionandthenumberofimagetokens(Table1). Dynamicimagesplitting
achievesabettertext-richperformancethanstaticimagesplitting(rows2vs.3)evenwhenboth
usethesamemaximumnumberof5sub-images. Weobservethattext-richtasksaresensitiveto
bothimageresolutionandthenumberofsub-images,whileothertasksarelessaffected. Notethat
increasingthenumberofsub-imagesimpliesanincreaseintotalnumberofimagetokens.Specifically,
withthesameeffectiveresolution,moreimagetokensimprovetext-richperformance(rows4vs.6
and5vs.7). Conversely,withthesamenumberofimagetokens,ahighereffectiveresolutionyields
bettertext-richperformance(rows4vs.5,and6vs.7). Overall,usingupto10sub-imageswith
animageencoderresolutionof672×672using144tokenspersub-image(row7)achievesthebest
performance.
Impactofimagegridconfiguration(Table2). Dynamicimagesplittingusingalargern is
max
especiallywellsuitedforunusualaspectratiossuchasdocumentandinfographicsunderstanding.
ItimprovesDocVQAandInfoVQAperformanceby3.1and6.9points,respectively,viachanging
n from4to16(rows1to3). Itisalsopossibletoboostperformanceviaonlyincreasingthe
max
numberofsub-imagesduringinference,buttrainingnativelyforityieldsbetterresults(rows2vs.5,
3vs.6,and4vs.7). Groundingperformanceishighlysensitivetochangesintheminimumgridsize
suchaschangingtheminimumnumberofsub-imagesfrom4to1duringinferenceonly(row7),as
thisaffectstheconversionfromlocaltoglobalcoordinatesforalargesubsetofthedata. Last,we
observethatperformanceimprovementsaregreaterwithlargerLLMbackbones. Specifically,with
the7Bsize,weobservea6.3and9.8pointsincreaseonDocVQAandInfoVQA,respectively(rows
8vs.10). Incontrast,the3Bsizemodelshowsa3.1and6.9pointsimprovement(rows1vs.3).
Impactofsub-imagepositionindicatorandoverviewimageposition(Table3). Wefindthat
position indicators are not strictly necessary (rows 1 to 3). Previous ablations, such as in [27],
showedthiscanbebeneficial,particularlyforDocVQAandInfoVQA,whichalignswithourfindings.
However,onaverage,wedonotseeasignificantimpactontext-richtasks. Indexpositionindicators
seemtoaidwithreferringandgroundingwhichisexpectedasspatialunderstandingisessentialfor
13Row Sub-img Overview Refer&
DocVQA InfoVQA Text-rich Knowledge General Average
# pos.indicator imagepos. Ground
1 none before 73.2 48.3 58.6 53.5 64.1 74.0 62.5
2 seps before 74.3 49.7 58.8 53.0 63.8 74.5 62.5
3 index before 73.4 48.6 58.6 52.7 63.4 74.8 62.4
4 none after 73.3 49.7 59.2 54.3 64.1 73.8 62.8
Table3: Ablationonthesub-imagepositionindicatorandthepositionoftheoverviewimage. Weset
(n ,n )=(4,4)forexperiments.
min max
thesetasks. Placingtheoverviewimageafterthesub-imagesslightlyimprovesperformance(rows1
vs.4),asthedecoderattentionmaskallowstheoverviewimagetoattendtoallsub-images.
Efficiency. Whileapossibleexplanationfordynamicimagesplittingoutperformingstaticsplitting
istradingoffadditionalcomputeforperformance,henceallowingmoretotalsub-imagesforhigh-
resolutioninputs,thisisn’tnecessarilythecaseonaverage. Inarandomsampleof100kexamples
takenfromthesingle-imagetrainingdatamixturedescribedinAppendixA.2,staticsplittinggenerates
atotalof500ksub-images. Incontrast,dynamicsplittingwith(n ,n )=(4,9)producesbarely
min max
more,only539kimagesintotal.
4 FinalModelandTrainingRecipe
WecollecttheresultsfromthepreviousablationstodeterminethefinalrecipeforMM1.5training:
• Architecture. WeusethesamemodelarchitectureasMM1[118].
• Dataandtrainingpipeline. AssummarizedinFigure2,MM1.5istrainedinthreestages:
– Pre-training. Thepre-trainingdatacomprisesthreeparts: (i)2Bimage-textpairs,(ii)600M
interleavedimage-textdocumentswith1Bimagesintotal,and(iii)text-onlydatawith2Ttokens.
Exceptfortheupdatedtext-onlydata,thedataremainsunchangedfromMM1[118]. However,
thedataratiohasbeenadjustedfrom45:45:10to50:10:40, significantlydownweightingthe
interleaveddata(from45%to10%)whileincreasingtheproportionoftext-onlydata(from10%
to40%)asdiscussedinSection3.4.
– Continual Pre-training. We use 45M OCR data to enhance text-rich image understanding.
Notably,wedonotincludeadditionalsyntheticimagecaptionsbasedonempiricalresults.
– SFT.WeusethedataillustratedinFigure4andadoptthemixingratiosstudiedinSection3.2.2.
Ourfinalmixtureconsistsof80%single-imagedata,10%multi-imagedata,and10%text-only
SFTdata. Thesingle-imagedatacanbefurthercategorizedinto37.2%text-richdata,22.5%
refer&grounddata(visualQAdataenrichedwithboundingboxesand/orpointcoordinates),
11.3%generaldata,5.6%mathdata,2.3%codedata,and1.1%sciencedata,totaling80%ofall
useddata.
• Dynamichigh-resolution. Wesettheimagegridconfiguration(n ,n )=(4,9),usingan
min max
indexforthesub-imagepositionindicatorandplacingtheoverviewimageafterthesub-images.
Dynamicimagesplittingisonlyenabledwhenthecurrenttrainingsamplehasfewerthanthree
images. Thesupportedresolutionreachesupto4Megapixels(approximately2016×2016fora
squareimage,or6048×672foralongimage).
WekeeptheimageencoderandtheLLMbackboneunfrozenduringallthemodeltrainingstages. For
pre-training,wefollowtheexactsamelearningratescheduleasinMM1[118]and200ktraining
stepswithsequencelength4096. Forcontinualpre-training, weuseapeaklearningrateof1e-5
withthecosinedecayand30ktrainingstepsforallthemodels(from1Bto30B).ForSFT,weusea
peaklearningrateof2e-5and23ktrainingstepsforallthemodels. Allmodelsaretrainedusingthe
AXLearnframework.2
Mixture-of-Experts(MoE).Intheseexperiments,wescalethedensemodelbyaddingmoreexperts
totheFFNlayersofthelanguagemodel,followingGShard[70]andST-MoE[194]. Weusetop-2
gatingwitha0.01loadbalancelosstoencourageabetterexpertloadbalanceanda0.001router
z-lossfortrainingstability. AsinMM1[118],weconvertthedensemodeltoMoEbyreplacingonly
thedenselanguagedecoder,keepingtheimageencoderandvision-languageconnectorunchanged.
2https://github.com/apple/axlearn
14Capability Benchmark MM1.5 MM1.5 MM1.5 MiniCPM-V2 Phi-3-Vision InternVL2
1B 1B(MoE) 3B 3B 4B 2B
MME[32](SUM) 1611.4 1873.0 1798.0 1808.2 1761.6 1864.3
Multi-discip
SeedBench[75](image) 70.2% 71.4% 72.4% 67.1% 71.8% 70.9%
Multi-discip;Large-scale
POPE[92] 88.1% 88.6% 88.1% 87.8% 85.8% 85.2%
GeneralVQA Obj.Hallu
LLaVAW[102] 71.6 75.5 73.0 69.2 71.6 60.0
OODGeneral
MM-Vet[174] 37.4% 39.8% 41.0% 38.2% 46.2% 39.7%
Multi-discip
RealworldQA[160] 53.3% 57.8% 56.9% 55.8% 59.4% 57.4%
RealwoldQA
†WTQ[126] 34.1% 38.9% 41.8% 24.2% 47.4% 35.8%
Wiki-tableQuestions
†TabFact[19] 66.1% 71.4% 72.9% 58.2% 67.8% 56.7%
TableFactVerification
OCRBench[103] 60.5% 62.6% 65.7% 60.5% 63.7% 78.1%
OCR;Multi-discip
†ChartQA[115] 67.2% 73.7% 74.2% 59.8% 81.4% 76.2%
Text-rich ChartUnderstanding
†TextVQA[138] 72.5% 76.1% 76.5% 74.1% 70.1% 73.4%
OCR;Reason
†DocVQA[117](test) 81.0% 84.8% 87.7% 71.9% 83.3% 86.9%
DocumentUnderstanding
†InfoVQA[116](test) 50.5% 55.9% 58.5% 37.6% 49.0% 58.9%
InfographicUnderstanding
†AI2D[61] 59.3% 67.1% 65.7% 62.9% 76.7% 74.1%
ScienceDiagrams
†ScienceQA[107] 82.1% 87.6% 85.8% 80.7% 90.8% 94.1%
High-schoolScience
Knowledge
MMMU[177](val,w/oCoT) 35.8% 41.2% 37.1% 38.2% 40.4% 36.3%
College-levelMulti-discip
MathVista[105](testmini) 37.2% 42.9% 44.4% 38.7% 44.5% 46.0%
GeneralMathUnderstanding
†RefCOCO[59](avg) 81.4% 83.9% 85.6% – 38.1% 77.7%
VisualGround
†Flickr30k[172](test) 83.0% 85.4% 85.9% – 27.1% 51.6%
PhraseGround
Refer&Ground
LVIS-Ref[170](avg) 62.2% 64.1% 67.9% 48.0% 54.2% 51.1%
Obj.Refer
Ferret-Bench[170] 67.4 69.6 69.5 22.1 32.2 34.9
ReferReason
†Q-Bench2[186] 66.4% 70.9% 73.2% – 56.8% 52.0%
Low-levelpercep
Mantis[53] 50.7% 51.2% 54.8% – 47.9% 53.0%
Multi-imageintheWild
†NLVR2[141] 79.0% 83.2% 83.8% – 53.6% 67.4%
Multi-image VisualReason
MVBench[85] 45.8% 48.3% 47.7% – 46.7% 60.2%
Multi-discip
BLINK[35] 46.3% 43.7% 46.8% 41.2% 44.2% 42.8%
UnusualVisualScenarios
MuirBench[155] 34.7% 40.9% 44.3% – 38.0% 23.1%
ComprehensiveMulti-image
In-context VL-ICL[193](avg) 51.0% 56.0% 56.3% – 19.5% 18.5%
Learning MultimodalIn-context
Table4: ComparisonwithSOTAmobile-friendlymodelsacrossdiversebenchmarks. (†)indicates
thatthetrainingsethasbeenobservedinourdatamixture. MiniCPM-V2[169]andInternVL2[21]
usebeamsearchdecoding,whilePhi-3-Vision[3]andourMM1.5modelsusegreedydecoding. For
allmultiple-choicequestion(MCQ)benchmarks(e.g.,AI2D,OCRBench),ourmodeloutputs
arenotpost-processedbyChatGPT,keepingorder,punctuation,andcasesensitivityintact.
15KnowledgeBenchmarks GeneralBenchmarks
Model AI2D SQA MMMU MathV MME SEEDI POPE LLaVAW MM-Vet RealWorldQA
(test) (test) (val) (testmini) (P/C)
1BModelComparison
LLaVAOneVision-0.5B[74] 57.1 67.2 31.4 34.8 1238.0/240.0 65.5 – – 29.1 55.6
SPHINX-Tiny[37] 24.6 21.5 – 26.4 1261.2/242.1 – 82.2 52.3 23.8 –
DeepSeek-VL[104] – – 32.2 31.1 – – 87.6 – 34.8 –
TinyLLaVA[190] – 60.1 – – – – 86.1 60.8 25.8 –
GeminiNano-1[149] 37.9 – 26.3 27.3 – – – – – –
IntenVL2-2B[21] 74.1 94.1 36.3 46.0 1864.3† 70.9 85.2 60.0 39.7 57.4
MM1-1B[118] 57.7 62.3 33.2 31.1 1393.2/217.1 65.6 87.4 67.5 39.4 51.2
MM1.5-1B 59.3 82.1 35.8 37.2 1365.7/245.7 70.2 88.1 71.6 37.4 53.3
MM1.5-1B-MoE 67.1 87.6 41.2 42.9 15119/361.1 71.4 88.6 75.5 39.8 57.8
3BModelComparison
MiniCPM-V2.0-3B[169] 62.9 80.7 38.2 38.7 1808.2† 67.1 87.8 69.2 38.2 55.8
VILA1.5-3B[97] – 69.0 33.3 – 1442.4/– 67.9 85.9 – – –
TinyLLaVA[190] – 69.1 – – 1464.9/– – 86.4 75.8 32.0 –
GeminiNano-2[149] 51.0 – 32.6 30.6 – – – – – –
Bunny[41] – 78.3 41.4 – 1581.5/361.1 72.5 87.2 – – –
BLIP-3[166] – 88.3 41.1 39.6 – 72.2 87.0 – – 60.5
Phi-3-Vision-4B[2] 76.7 90.8 40.4 44.5 1441.6/320.0 71.8 85.8 71.6 46.2 59.4
MM1-3B[118] 62.4 69.4 33.9 32.0 1482.5/279.3 68.8 87.4 72.1 43.7 55.8
MM1.5-3B 65.7 85.8 37.1 44.4 1478.4/319.6 72.4 88.1 73.0 41.0 56.9
MM1.5-3B-MoE 69.9 89.8 42.9 46.9 1591.4/365.7 73.3 87.2 76.1 43.7 60.7
7BModelComparison
LLaVA-NeXT-7B[101] – 70.1 35.8 34.6 1519.0/332.0 70.2 86.5 81.6 43.9 –
Idefics2-8B[68] – – 43.0 51.4 – – – – – –
MM1-7B[118] 66.0 72.6 37.0 35.9 1529.3/328.9 69.9 86.6 81.5 42.1 55.7
MM1.5-7B 72.2 89.6 41.8 47.6 1514.9/346.4 73.4 88.6 74.2 42.2 62.5
30BModelComparison
LLaVA-NeXT-34B[101] – 81.8 51.1 46.5 1631.0/397.0 75.9 87.7 89.6 57.4 –
Cambrian-34B[151] 79.7 85.6 49.7 53.2 1689.3/– 75.3 – – – 67.8
MM1-30B[118] 73.3 81.0 44.7 39.4 1637.6/431.4 72.1 87.6 89.3 48.7 59.4
MM1.5-30B 77.2 91.9 47.4 55.6 1646.2/405.7 75.0 88.6 80.4 52.0 69.0
Gemini-1.5-Pro[130] 79.1 85.7 60.6 57.7 2110.6† – 88.2 95.3 64.0 64.1
GPT-4V[125] 75.9 82.1 53.8 48.7 1771.5† 71.6 75.4 93.1 56.8 56.5
GPT-4o[51] 84.6 90.7 69.2 61.3 2310.3† 77.1 85.6 102.0 69.1 75.4
Table5: ComparisonwithSOTAmodelsonknowledgeandgeneralbenchmarks. (†)Thescoreisthe
summationofperceptionandcognitionscores. Gemini-1.5-Pro,GPT-4VandGPT-4onumbersare
fromOpenVLMLeaderboard.
WeintroducetwoMoEmodels,a1B-MoEanda3B-MoE,with64expertsreplacingdenselayers
everytwolayers. Weusedthesamehyperparametersasthoseappliedtothedensemodelsforboth
the1Band3Bscales.
4.1 Results
WeevaluateourMM1.5modelsacross35multimodalbenchmarksusinganinternalforkoflm-eval-
harness[36],coveringtaskcategoriesrangingfromgeneralmultimodalunderstanding,knowledge,
text-rich,referringandgrounding,multi-imagereasoning,toin-contextlearning. Forafaircompari-
sonwithtopMLLMs,wereportresultsfromoriginalpapersorconductevaluationsusingconsistent
settings when unavailable. All results use zero-shot settings and greedy decoding unless stated
otherwise. Forexample,MiniCPM-V2[169]andInternVL2[21]usebeamsearchdecodinginstead.
For mobile-scale models, we provide a detailed comparison with leading small MLLMs across
allbenchmarksinTable4. Detailedresultsforeachcapabilityatvariousmodelsizesarefurther
summarizedinTable5,6,7,8,and9,respectively. Below,wehighlightafewkeyobservations.
MM1.5representsamajorupgradeoverMM1. Itdeliversimprovementsacrossallmodelsizes
and nearly all benchmarks, often by a substantial margin. For instance, MM1.5-30B boosts the
MathVistascorefrom39.4to55.6,DocVQAfrom75.8to91.4,andInfoVQAfrom47.3to67.3.
Notably,italsooffersmuchenhancedmulti-imagereasoningcapability,e.g.,improvingMuirBench
from36.7to58.2. Additionally,itintroducesnewcapabilitiesnotpresentinMM1,suchasvisual
referringandgrounding. Weintentionallypresentresultsacrossidenticalmodelscalesanduse
thesameLLMsasinMM1[118]toisolatetheimpactofthenovelcontributionsofMM1.5.
BothDenseandMoEmodelscalingareeffective. First,scalingthedensemodelfrom1Bto30B
consistentlyimprovesperformance,withbenchmarkslikeAI2Dincreasingfrom59.3to77.2.Second,
16Text-richBenchmarks
Model WTQ TabFact OCRBench ChartQA TextVQA DocVQA InfoVQA
(test) (test) (test) (test) (val) (test) (test)
1BModelComparison
LLaVAOneVision-0.5B[74] – – – 61.4 – 70.0 41.8
SPHINX-Tiny[37] 15.3 51.1 – 34.1 57.8 53.0 26.3
DeepSeek-VL[104] – – 40.9 – – – –
TinyLLaVA[190] – – – – 51.7 – –
GeminiNano-1[149] – – – 53.6 62.5 72.2 51.1
InternVL2-2B[21] 35.8 56.7 78.1 76.2 73.4 86.9 58.9
MM1-1B[118] 19.9 49.8 56.6 61.8 68.2 68.4 38.5
MM1.5-1B 34.1 66.1 60.5 67.2 72.5 81.0 50.5
MM1.5-1B-MoE 38.9 71.4 62.6 73.7 76.1 84.8 55.9
3BModelComparison
MiniCPM-V2.0-3B[169] 24.2 58.2 60.5 59.8 74.1 71.9 37.6
TinyLLaVA[190] – – – – 59.1 – –
GeminiNano-2[149] – – – 51.9 65.9 74.3 54.5
BLIP-3-4B[166] – – – – 71.0 – –
Phi-3-Vision-4B[3] 47.4 67.8 63.7 81.4 70.1 83.3 49.0
MM1-3B[118] 23.6 52.9 57.0 66.8 71.9 75.2 44.7
MM1.5-3B 41.8 72.9 65.7 74.2 76.5 87.7 58.5
MM1.5-3B-MoE 39.1 73.1 63.8 73.6 76.8 85.0 53.6
7BModelComparison
LLaVA-NeXT-7B[101] – – – – 64.9 – –
Idefics2-8B[68] – – – – 73.0 74.0 –
DocOwl-1.5-Chat[46] 40.6 80.2 – 70.2 68.6 82.2 50.7
MM1-7B[118] 28.8 55.5 62.6 72.6 72.80 76.8 45.5
MM1.5-7B 46.0 75.9 63.5 78.6 76.5 88.1 59.5
30BModelComparison
LLaVA-NeXT-34B[101] – – – – 69.5 – –
Cambrian-34B[151] – – 60.0 75.6 76.7 75.5 –
MM1-30B[118] 33.3 58.9 60.6 76.9 73.5 75.8 47.3
MM1.5-30B 54.1 84.0 65.8 83.6 79.2 91.4 67.3
Gemini-1.5-Pro[130] – – 75.4 87.2 78.7 93.1 81.0
GPT-4V[125] – – 64.5 78.5† – 88.4† –
GPT-4o[51] – – 73.6 85.7† – 92.8† –
Table6: ComparisonwithSOTAmodelsontext-richbenchmarks. Numbersmarkedwith(†)are
obtainedfrom[74].
boththe1Band3BMoEmodelsoutperformtheirdensecounterparts. Notably,theMM1.5-3B-MoE
modelcanevensurpasstheMM1.5-7Bmodelinknowledge,general,visualreferringandgrounding,
andmulti-imagebenchmarks,thoughitfallsslightlybehindontext-richbenchmarks. Thissuggests
thatMoEmodelsshowstrongpotentialinintegratingdiversecapabilitiescomparedtodensemodels.
MM1.5-1Bisthestate-of-the-artmodelatthe1Bscale.Whilefewmodelsareavailableatthisscale,
MM1.5-1BclearlyoutperformscomparablemodelssuchasSPHINX-Tiny[37],DeepSeek-VL[104],
andTinyLLaVA[190]. Forreference, MM1.5-1BalsosignificantlysurpassesLLaVAOneVision-
0.5B[74](e.g.,ScienceQA:67.2vs. 82.1,DocVQA:70.0vs. 81.0),butitshouldbestressedthatthis
isofcourseanevensmallermodelandassuchcannotbedirectlycompared.
MM1.5-3BoutperformsMiniCPM-V2.0andiscompetitivewithInternVL2andPhi-3-Vision.
Asedgedeploymentbecomesincreasinglyimportant,moremodelsareemergingatthe3Bscale,
includingMiniCPM-V2.0[169],InternVL2-2B[21],VILA1.5-3B[97],Bunny[41],andtherecent
BLIP-3[166]. UsingMiniCPM-V2.0asarepresentativeexample,MM1.5-3Bdemonstratessuperior
performanceacrossbenchmarks(e.g.,MathVista: 38.7vs. 44.4,DocVQA:71.9vs. 87.7). Further-
more,MM1.5-3Bsupportsvisualreferringandgrounding—capabilitiesabsentinMiniCPM-V2.0.
MM1.5-3BalsoachievesoverallbetterperformancethanInternVL2-2B[21]ongeneralVQA.
WhilePhi-3-Vision3 reportsresultsononlyasubsetofbenchmarkswefocusoninthiswork,we
conductedacomprehensivecomparisonbyevaluatingtheirmodelonallthebenchmarksnotcovered
bytheoriginalpaper(SeeAppendixA.7formethodology). AlthoughMM1.5-3BlagsbehindPhi-3-
Visiononcertainknowledge-basedbenchmarkslikeAI2DandMMMU—likelyduetoPhi-3-Vision’s
3https://huggingface.co/microsoft/Phi-3-vision-128k-instruct
17ReferandGroundBenchmarks
Model RefCOCO RefCOCO+ RefCOCOg Flickr30k LVIS-Ref Ferret-Bench
(testA/B) (testA/B) (test) (test) (box/point) (avg.)
1BModelComparison
SPHINX-Tiny[37] 86.9/77.9 78.5/63.7 78.9 – – –
MM1-1B[118] 0/0 0/0 0 0 51.4/51.6 47.3
MM1.5-1B 89.3/81.9 83.7/69.3 82.8 83.0 69.7/54.7 67.4
MM1.5-1B-MoE 91.0/84.8 86.0/73.0 84.7 85.4 71.4/56.7 69.6
3BModelComparison
MiniCPM-v2-3B[169] – – – – 48.2/47.7 22.1
Phi-3-Vision-4B[3] 46.3/36.1 42.0/28.8 37.6 27.12 53.8/54.5 32.2
InternVL2[21] 88.2/75.9 82.8/63.3 78.3 51.6 51.0/51.1 35.0
MM1-3B[118] 0/0 0/0 0 0 52.9/53.9 46.3
MM1.5-3B 92.0/86.1 87.7/75.9 86.4 85.9 76.3/59.5 69.5
MM1.5-3B-MoE 92.6/86.4 88.0/77.8 86.4 85.8 79.3/54.5 72.2
7BModelComparison
Qwen-VL-7B[9] 92.3/84.5 88.6/76.8 86.3 – – –
MiniGPT-v2-7B[15] 91.3/84.3 85.5/73.3 84.3 – – –
LLaVA-OneVision-7B[3] 80.0/61.6 76.9/56.2 70.0 50.1 51.2/51.4 38.4
Ferret-7B[170] 91.4/82.5 87.4/73.1 84.8 82.2 79.4/67.9 64.5
Ferret-V2-7B[181] 94.7/88.7 92.8/79.3 89.3 85.8 86.6/74.6 75.6
MM1-7B[118] 0/0 0/0 0 0 53.1/53.3 48.5
MM1.5-7B 92.5/86.7 88.7/77.8 87.1 85.3 79.4/53.4 72.6
Larger(>13B)ModelComparison
Ferret-13B[170] 92.4/84.4 88.1/75.2 86.3 84.8 80.5/68.4 66.3
Ferret-V2-13B[181] 95.0/88.9 92.8/81.4 90.0 86.3 87.7/75.1 74.9
MM1-30B[118] 0/0 0/0 0 0 53.4/52.7 50.9
MM1.5-30B 94.9/89.5 92.4/83.5 90.0 87.5 84.9/61.4 77.1
Table7: ComparisonwithSOTAmodelsonreferringandgroundingbenchmarks.
VL-ICLBenchmark
Model Operator
Matching Open Operator
CLEVR induction TextOCR Avg.
MiniImageNet MiniImageNet induction
interleaved
1BModelComparison
MM1-1B[118] 25.0 49.3 73.0 16.7 8.3 33.5 34.3
MM1.5-1B 39.0 52.0 84.0 60.0 36.7 34.0 51.0
MM1.5-1B-MoE 33.0 56.5 89.0 56.7 56.7 44.0 56.0
3BModelComparison
Phi-3-Vision-4B[3] 17.0 50.0 1.0 26.7 8.3 14.0 19.5
MM1-3B[118] 27.5 50.0 79.0 18.3 13.3 34.0 37.0
MM1.5-3B 33.5 59.0 88.0 48.3 66.7 42.5 56.3
MM1.5-3B-MoE 32.0 58.0 92.0 63.3 65.0 47.5 59.6
7BModelComparison
OpenFlamingo-9B[6] 18.8 50.0 51.2 2.8 2.8 0.0 20.9
Idefics-9B[67] 27.7 50.0 53.8 7.8 6.1 22.8 28.0
Otter-9B[73] 8.2 50.4 28.5 12.2 7.2 0.8 17.9
InternLM-XComposer2-7B[26] 20.0 50.1 49.0 39.4 11.1 16.0 30.9
Qwen-VL-Chat-7B[9] 26.8 56.4 58.0 18.9 8.9 22.3 31.9
LLaVA-NeXT-7B[101] 17.8 50.0 0.0 3.3 5.0 0.0 12.7
MM1-7B[118] 33.0 69.5 97.5 40.0 45.0 32.0 52.8
MM1.5-7B 25.5 52.8 98.5 68.3 60.0 31.0 56.0
Larger(>30B)ModelComparison
Idefics-80B-[67] 31.5 50.0 52.5 21.7 28.3 29.5 35.6
Emu2-Chat-37B[142] 14.8 50.0 28.2 21.7 10.0 36.5 26.9
MM1-30B[118] 25.0 63.0 98.5 51.7 38.3 36.0 52.1
MM1.5-30B 46.5 66.5 100.0 65.0 80.0 44.5 77.6
GPT-4V[125] 42.0 81.0 56.0 92.0 74.0 50.0 65.8
Table8: ComparisonwithSOTAmodelsonVL-ICLbenchmark[193]formultimodalin-context
learning. 4-shotaccuracyreportedforeachsubtask.
18Multi-imageBenchmarks
Model QBench2 Mantis NLVR2 BLINK Muirbench
MVBench
(val) (test) (val) (val) (test)
1BModelComparison
LLaVA-NeXT-Interleave-0.5B[78] 52.0 45.6 67.8 45.6 39.2 –
LLaVAOneVision-0.5B[74] 48.8 39.6 63.4 45.5 52.1 25.5
MM1-1B[118] 43.4 41.5 50.9 43.8 40.3 30.7
MM1.5-1B 66.4 50.7 79.0 45.8 46.3 34.7
MM1.5-1B-MoE 70.9 51.2 83.2 48.3 43.7 40.9
3BModelComparison
BLIP-3-4B[166] 75.1 56.7 – – 49.7 –
Phi-3-Vision-4B[3] 56.8 47.9 53.6 46.7 44.2 38.0
MM1-3B[118] 41.4 45.2 51.7 44.8 41.5 28.0
MM1.5-3B 73.2 54.8 83.8 47.7 46.8 44.3
MM1.5-3B-MoE 73.8 54.4 86.0 50.3 49.8 45.6
7BModelComparison
LLaVA-v1.5-7B[100] 49.3 31.3 53.9 36.0 37.1 23.5
LLaVA-NeXT-Interleave-7B[78] 74.2 62.7 88.8 53.1 52.6 38.9
Idefics2-8B[68] 57.0 48.9 86.9 29.7 45.2 26.1
Mantis-Idefics2-8B[53] 75.2 57.1 89.7 51.4 49.1 44.5
MM1-7B[118] 43.6 51.6 59.9 45.3 40.0 30.4
MM1.5-7B 73.2 57.6 86.9 48.3 48.2 49.1
Larger(>14B)ModelComparison
LLaVA-NeXT-Interleave-14B[78] 76.7 66.4 91.1 54.9 52.1 –
Emu2-Chat-37B[142] 50.1 37.8 58.2 39.7 36.2 33.6
MM1-30B[118] 42.8 52.5 63.1 47.1 43.5 36.7
MM1.5-30B 79.3 64.6 90.6 54.0 50.2 58.2
GPT-4V[125] 76.5 62.7 88.8 43.5 51.1 68.0†
Table9: ComparisonwithSOTAmodelsonmulti-imagebenchmarks. Theresultwithmark(†)inthe
rowofGPT-4VisfromGPT-4o. MVBench[84]istreatedasamulti-imagebenchmarktotestthe
zero-shottransfercapabilityofMM1.5tovideounderstandingtasks.
largermodelsize(4.2B)—MM1.5-3Bgenerallyexcelsontext-richbenchmarks(e.g.,DocVQA:83.3
vs. 87.7,InfoVQA:49.0vs. 58.5). Moreover,MM1.5-3BsignificantlyoutperformsPhi-3-Visionon
referringandgroundingtasks(seeTable7)aswellasin-contextlearningbenchmarks(seeTable8).
MM1.5-30B is a stronger generalist model than Cambrian-34B. Some notable models at the
30BscaleincludeLLaVA-NeXT-34B[101]andCambrian-34B[151]. MM1.5-30Bsignificantly
surpassesCambrian-34Bontext-richbenchmarks(e.g.,DocVQA:75.5vs. 91.4,ChartQA:75.6vs.
83.6),overallon-parongeneralandknowledgebenchmarks. Additionally,Cambrian-34Blacksthe
capabilitiesforreferringandgrounding,anditalsodoesnotsupportmulti-imagereasoning,asitis
exclusivelytrainedonsingle-imagedata.
MM1.5excelsinvisualreferringandgrounding. WhilemostSOTAmodelsfocusonimproving
performanceacrossgeneral,knowledge,andtext-richbenchmarks,fewhaveintegratedfine-grained
imagegroundingandreferringabilityintotheirdesign. EvenGPT-4oreliesonset-of-markprompting
todemonstratevisualgroundingcapabilities. AsshowninTable7,MM1.5-3BoutperformsFerret-7B
andisonparwithFerret-13B,bothofwhicharefine-tunedspecificallyforreferringandgrounding
tasks. Notably,ourmodelinherentlypossessesthesecapabilitieswhilestillexcellinginotherareas.
MM1.5excelsinmulti-imagereasoningandin-contextlearning.AsshowninTable9,theMM1.5-
1BmodeloutperformsLLaVAOneVision-0.5Batthe1Bscale. Similarly,atthe3Bscale,MM1.5-3B
significantlysurpassesPhi-3-Vision. Additionally,weevaluateMM1.5’szero-shottransfercapability
forvideounderstandingusingMVBench[84],abenchmarkdesignedforvideotasks.InSection5,we
willfurtherintroduceMM1.5-Video,amodelvariantspecificallydesignedforvideounderstanding.
Moreover,weevaluateMM1.5’sabilityofmultimodalin-contextlearning,anemergentcapabilityin
MLLMsinducedbylarge-scalepre-training. WeusetheVL-ICLbenchmark[193],abenchmark
especiallycuratedtotestdiverseandchallengingICLcapabilitiesbyrequiringthemodeltofollow
non-trivialinstructionsexpressedviademonstrationspresentedasin-contextinterleavedimage-text
19pairs. AsshowninTable8,ourmodelsoutperformothersinin-contextlearning(e.g.,Phi-3-Vision
vs. MM1.5-3B:19.5vs. 56.3;Ideficsvs. MM1.5-30B:35.6vs. 77.6).
5 MM1.5-Video
Themulti-imagereasoningcapabilityshowninMM1.5naturallyleadsustodevelopMM1.5-Video
for video understanding. It takes a video and an instruction as input and generates the response.
Fortheinputs,weuniformlysampleN framesfromthevideoatanarbitrarylengthandfeedthem
intothemodelasmulti-imageinputswithoutspecialframeassembly. Duetothetokenlimits,we
disablethedynamicimagesplittingforeachframe,andthevisionencodergeneratesthefeaturemaps
frame-by-frameindependently. Specifically,wesample24framesforeachvideo,andeachframeis
representedby144tokens.
WeintroducetwovariantsforMM1.5-Video. First,webuildMM1.5-Videoasatraining-freemodel,
whichisachievedbydirectlyadoptingthepre-trainedMM1.5imagemodelstovideotaskswithout
being fine-tuned on any video data. This saves a lot of computation resources and demonstrates
MM1.5’scapabilityoftransferringknowledgetonewdomains.
Second, weintroducethesupervisedfine-tuning(SFT)modelwherewefine-tuneMM1.5image
modelsonvideoinstruction-tuningdatasetstoimproveitstemporalmodelingcapabilityforvideo
tasks.WeuseamixtureofpublicvideodatasetsfromShareGPTVideo[183](556K),VideoChat2[83]
(225K),andActivityNet-QA[175](31.5K).Thesedatasetscontainavarietyofvideostypes,spanning
differenttasks(e.g.,open-endedandmultiplechoicequestions),viewpoints(e.g.,first-andthird-
personviews),andlengths(e.g.,videosfromafewsecondstotensofminutes).
5.1 BenchmarksandMetrics
Wecompareourvideotraining-freeandSFTmodelswithstate-of-the-artmethodsonmultiplevideo
question-answering(VideoQA)tasksandbenchmarks.
Open-Ended Benchmarks evaluate the performance of a model to answer questions in a free-
form style. For this task, we include ActivityNet-QA [175] and VCGBench [112]. Following
priorwork[164],weuseGPT-3.5-Turbo-0125toassesstheaccuracyandscorefortheprediction.
Consideringthatthelabeledanswersofthesetwodatasetsaretypicallyshort(e.g.,onewordorphrase),
wealsoevaluateontheLLaVA-Hound[183],whichrequiresthemodeltogeneratemoredetailed
answers. Thisisusefulforassessingperformanceontasksinvolvingdetailedvideounderstanding.
WefollowtheiroriginalsettingtoreportthescorefromGPT-3.5-Turbo-0301andconsiderascore
value≥3ascorrectforaccuracycalculation.
MultipleChoiceBenchmarksrequirethemodeltopickthecorrectanswerfrommultiplechoices.
For this evaluation, we include VideoMME [33], EgoSchema [113], NExTQA [162], and Inten-
tQA[81]. VideoMMEisacomprehensiveevaluationdatasetcontainingvideofromafewseconds
toonehourinlength. EgoSchemaconsistsofegocentricvideosandinvolvescomplexlong-form
temporalunderstandingandreasoning. NExTQAandIntentQAarecollectedfromthesamevideo
source,butIntentQAfocusesonpredictingintentsindailysocialactivities. Forallthesedatasets,the
accuracyofselectingthecorrectanswerfromtheoptionsisusedastheevaluationmetric.
5.2 Results
Training-freeresultsareshowninTable10and11.MM1.5-Videodemonstratesgreatercapabilityon
MultipleChoiceVideoQA,whereMM1.5-Video-3Balreadyoutperformsstate-of-the-arttraining-free
7Bmodelsonallbenchmarks. WealsofindthatMM1.5-Videocanfollowtheinstructiontoprecisely
outputthepredictedoption; however, mostexistingmethods[84]usestructuredanswerprompts
(e.g,"Best Option:(")toguidetheirmodelstogenerateanswersinadesirableformat. Onthe
otherhand,MM1.5-Videoachievesonlyon-parperformancecomparedtoSlowFast-LLaVAonthe
open-endedbenchmarks. Wehypothesizethatthisisbecauseourmulti-imageSFTdatasetscontain
primarilymultiplechoicetasks,makingsuchataskformulationmostsimilartothetrainingdata.
SFTresultsarealsoshowninTable10and11. First,weobservethatfine-tuningMM1.5-Videoon
videodatasetscanimproveitsperformanceonalltasks. Second,onbothopen-endedandmultiple
choicebenchmarks,oursmallmodel,MM1.5-Video-1B,significantlyoutperformsLLaVAOneVision-
20Open-EndedBenchmarks MultipleChoiceBenchmarks
Video
Model
Data ActivityNet-QA VCGBench VideoMME EgoSchema NExTQA IntentQA
(test) (test) (w/osubs) (subset) (val) (val)
Training-FreeModelComparison
DeepStack-L-7B[119] ✘ 49.3 – – 38.4 61.0 –
IG-VLM-7B(LLaVA-v1.6)[63] ✘ 54.3 3.03 – 35.8 63.1 60.3
SlowFast-LLaVA-7B[164] ✘ 55.5 3.04 40.7 47.2 64.2 60.1
MM1.5-Video-1B(Training-free) ✘ 46.8 2.86 45.6 45.4 70.0 67.8
MM1.5-Video-3B(Training-free) ✘ 50.9 3.04 48.4 48.4 72.8 72.7
MM1.5-Video-7B(Training-free) ✘ 52.5 3.05 52.4 49.6 76.1 76.7
SFTModelComparison
VideoChatGPT-7B[112] ✔ 35.2 2.42 – – – –
Video-LLaVA-7B[96] ✔ 45.3 2.84 39.9 – – –
Vista-LLaMA-7B[111] ✔ 48.3 – – – 60.7 –
MovieChat+-7B[139] ✔ 48.1 – – – 54.8 –
VideoChat2-7B[84] ✔ 49.1 2.98 – – 68.6 81.9
Video-LLaMA2-7B[24] ✔ 50.2 3.13 47.9 51.7 – –
PLLaVA-7B[163] ✔ 56.3 – – – – –
LLaVA-NeXT-Interleave-0.5B[78] ✔ 48.0 3.07 – – 59.5 –
LLaVA-NeXT-Interleave-7B[78] ✔ 55.3 3.42 – – 78.2 –
LLaVAOneVision-0.5B[74] ✔ 50.5 3.12 44.0 26.8 57.2 –
LLaVAOneVision-7B[74] ✔ 56.6 3.51 58.2 60.1 79.4 –
MM1.5-Video-1B(SFT) ✔ 56.1 3.14 45.7 51.0 71.8 74.2
MM1.5-Video-3B(SFT) ✔ 57.9 3.17 49.5 52.4 74.7 81.2
MM1.5-Video-7B(SFT) ✔ 60.9 3.22 53.5 57.2 76.9 86.6
Table10: ComparisonwithSOTAmodelsonOpen-EndedandMultipleChoicebenchmarks.
In-domainBenchmarks Out-of-domainBenchmarks
Model
ActivityNet-QA VIDAL-QA WebVid-QA MSVD-QA MSRVTT-QA TGIF-QA SSV2-QA
Video-ChatGPT-7B[112] 34.2 29.4 38.9 34.1 25.7 31.4 19.4
LLaMA-VID-7B[90] 36.5 30.6 37.0 34.1 25.0 27.2 22.2
Chat-UniVi-7B[54] 39.4 31.4 40.1 35.6 25.9 33.2 20.6
Video-LLaVA-7B[96] 41.4 34.3 42.5 39.5 30.8 33.0 24.3
LLAVA-HOUND-SFT-7B† 62.8 56.3 66.8 62.2 52.6 61.1 35.4
MM1.5-Video-1B(Training-free) 49.0 42.6 55.8 49.8 43.3 47.6 27.2
MM1.5-Video-3B(Training-free) 51.5 45.4 58.5 51.1 46.0 49.2 28.2
MM1.5-Video-7B(Training-free) 52.8 48.7 58.5 52.9 48.1 49.8 30.4
MM1.5-Video-1B(SFT) 65.7 60.6 68.7 65.0 55.3 64.0 34.0
MM1.5-Video-3B(SFT) 67.8 63.4 71.1 65.2 57.2 64.9 35.2
MM1.5-Video-7B(SFT) 68.5 68.5 71.5 67.2 59.3 65.5 37.9
Table11: ComparisonwithSOTAmodelsonLLaVA-Houndbenchmarks. (†)indicatesthepublished
versionreleasedathttps://huggingface.co/ShareGPTVideo/LLaVA-Hound-SFT.
0.5B(e.g.,24.2%onEgoSchemaand14.6%onNExTQA)andachievesthestate-of-the-artresults.
Third,our7Bmodelachievesstate-of-the-artperformanceonActivityNet-QA(e.g.,outperforming
LLaVAOneVision-7Bby4.3%)andverystrongresults(mostlyrunner-up)onotherbenchmarksby
usingonlypublicvideodatasets. WeareimpressedbythesuperiorresultsofLLaVAOneVision-7B,
especiallyonlong-formvideobenchmarkssuchasVideoMMEandEgoSchema. Wehypothesizethis
canbeduetothat(i)itistrainedontheirre-annotatedvideodatasetswithbetterlabelingquality,
(ii)ittakesmorevideoframesasinputs(i.e., 32vs. 24), (iii)itusesmultipletrainingstageson
joint image and video datasets. We will explore these directions to improve our model in future
work. Lastly,MM1.5-Videoachievesstate-of-the-artperformanceontheLLaVA-Houndbenchmarks,
whichdemonstratesourcapabilityfordetailedvideounderstanding.
6 MM1.5-UI
OneofthemostpromisingapplicationsofMLLMsthathasrecentlygainedpopularityisusingthem
tounderstandandactonuserinterfaces(UIs)onbehalforalongsideusers[44,8,171,79],which
couldsignificantlyboostusers’productivityandefficiencywheninteractingwithdigitaldevices. This
applicationtypicallyinvolvesprovidingamodelinputof: (i)animageofthegraphicaluserinterface
(GUI)ofadevice(i.e.,phoneorcomputer)screen;and(ii)instructionsoneitherknowledgegrounded
oncertainareasortheentiretyofthescreen(e.g.,Isthiselementat<x1,x2,y1,y2>clickable?),or
askingittorefertocertainareasofthescreenthatfitthequestions’criterion(e.g.,Whereisthetext
‘login’onthescreen?). Beyondreferringandgroundingabilities,excellingonUItasksalsorequires
21Figure12: IllustrationoftheUIunderstandingcapabilityshowninMM1.5-UI.Oursinglemodelis
abletoperformavarietyofreferringandgroundingtasksandestablishnewstate-of-the-arts. More-
over,itcansummarizethefunctionsoftheUIscreenandengagewithusersthroughconversations.
text-richimageunderstandingabilitytounderstandtext-denseUIs,andbackgroundknowledgeabout
typicaluserinteractionsondevices,whichmakesMM1.5aperfectcandidatetobedevelopedintoa
highlycapableUIunderstandingmodel.
Towardsthisgoal,wedevelopedMM1.5-UI,anMM1.5modelvariantfurtherfine-tunedspecifically
onUIdatathatachievescompetitiveperformanceonUIunderstandingtasksandestablishesnew
state-of-the-artperformanceinvariousbenchmarks. Figure12illustratesasingleMM1.5-UImodel’s
widerangeofUIunderstandingcapabilitiesonaniPhonescreenshot. Themodelcanfindcertaintext
(“BRIGHTNESS”)ontheleftside(box2),correctlyidentifythesettingsiconatthetopleft(box1),
classifyaUIelementontherightasacheckbox(box0),andmaintainamulti-turnconversationabout
the“NightShift”function(box3)intheUI.
6.1 BenchmarksandMetrics
WetrainandevaluateMM1.5-UIonavarietyofpublicandelementaryUIunderstandingtasksused
inFerret-UI[171]. Thesetasksareestablishedbenchmarksinliteraturethatcovermultipleaspectsof
UIunderstanding,andallowustofairlycompareMM1.5-UIagainstpriorwork:
• Public Benchmarks include screen2words [154]: a screen-level captioning task; widget cap-
tions[89]: awidget-levelcaptioningtask;andtaperception[132]: predictingthetapabilityofa
certainwidgetontheUI.
• Ferret-UIelementarytasksaresplitintotwocategories:Grounding(Grd-*)arequestionsquerying
foracertainareaonthescreen,suchasfindinganicon;andReferring(Ref-*)arequestionsgiven
acertainareaonthescreen, suchasrecognizingtextwithinascreenarea(i.e., OCR).Eachof
thesetasksalsohasaniOS(*-i)andAndroid(*-A)version,formingfourcategoriesoftasks(e.g.,
GroundingtaskonAndroidisGrd-A).
22PublicBenchmarks Ferret-UIElementaryTasks
Model
S2W WiC TaP Ref-i Ref-A Grd-i Grd-A
Spotlight[79] 106.7 141.8 88.4 - - - -
PaliGemma-3B†[11] 119.6 148.4 - - - - -
Ferret-UI-13B[171] 113.4 142.0 78.4 80.5 82.4 79.4 83.5
MM1.5-UI-1B 103.0 144.4 79.3 90.0 88.6 86.5 88.2
MM1.5-UI-3B 103.3 145.0 80.4 90.8 89.2 87.3 88.8
MM1.5-UI-7B 100.6 149.7 80.3 91.2 89.2 87.2 88.6
MM1.5-UI-30B 106.0 145.9 80.6 91.8 89.7 88.2 89.1
AblationonMM1.5SFTonUItasks
MM1.5-UI-3B(1ep.) 103.9 145.2 77.4 88.6 87.7 86.0 87.9
MM1.5-UI-3B(1ep.,w/oMM1.5SFT) 103.8 139.5 75.3 88.2 87.4 85.5 87.1
Table12: ComparisonwithSOTAmodelsonUIbenchmarks. S2W:screen2words,WiC:widget
captioning,TaP:taperception. (†)denotesper-taskfine-tuning. 1ep. means1epochmodeltraining.
MoredetailsofthebenchmarkscanbefoundinAppendixA.5andtheoriginalFerret-UIpaper[171].
6.2 Results
MM1.5-UI models are trained by further fine-tuning the final MM1.5 models on the Ferret-UI
datamixture[171],whichincludestrainingdatacorrespondingtotheaboveelementaryUItasks
andadditionalGPT-4-generatedconversationsaboutfunctionalitiesanddescriptionsabouttheUIs’
functionalityandlayouts. Thereare801Ksamplesintotal. Allmodelsaretrainedwiththesame
batchsizeandlearningrateastheoriginalMM1.5model.
ComparisonwithPriorArt.ResultsaresummarizedinTable12.OurMM1.5-UImodelsoutperform
priorbestmodelsinnearlyallbenchmarksexceptScreen2words. Inparticular,evenour1Bmodelis
abletooutperformtheFerret-UImodelinitsproposedelementarytasksbyawidemargindespite
beingtentimessmaller. TheperformancedifferenceismostsignificantoniOStasksat9.1pointson
average. ThisdemonstratesthattheabilitieslearnedbyMM1.5arerelevantandusefulforUItasks.
Whencomparingtheperformanceacrossindividualbenchmarks,MM1.5-UIdemonstratesaclear
hierarchyofdifficultiesamongtasksthatfocusondifferenttypesofUIelements,similartoFerret-
UI[171]. Tasksfocusedontextarethemostchallenging,followedbythoseinvolvingicons,while
widget-basedtasksaretheeasiest. Thistrendholdsforbothreferringandgroundingtasks. However,
MM1.5-UIshowsanotableperformanceimprovementinicon-basedtasks,significantlynarrowing
thegapbetweeniconandwidgettasks. Ferret-UIhighlightedtheimportanceofresolutionfortasks
involvingsmallerelementslikeicons. Thehigherresolutionanddynamicimagesplittingusedin
MM1.5-UIfurtherconfirmthatresolutionisparticularlybeneficialforenhancingperformancein
icon-relatedtasks.
ImpactofMM1.5SFT onUItasks. TohighlighttheeffectivenessoftheMM1.5SFTmixture
ondownstreamUItasks(i.e.,inMM1.5-UI),wecomparetheperformanceofthefullMM1.5-UI
modelwithabaselineUImodelfine-tunedwithUIdataonthepre-trainingcheckpointthatMM1.5
wastrainedon. BothmodelsaretrainedforoneepochusingtheFerret-UIdataset,andtheirresults
arepresentedinTable12. ThefinalMM1.5-UImodel,whichunderwentSFTforgeneraldomain,
text-rich, and refer&ground tasks, achieves superior UI performance within the same number of
trainingsteps. ThisdemonstratesthestrongtransfercapabilityofMM1.5forUIapplicationsand
contributestoitsperformanceimprovementoverpriorSOTAmodels.
Impactofmodelscaling. Weobserveoverallperformanceimprovementsasmodelsscale,though
gainsinallmetricsremainmodest,suggestingthatlargermodelsmaybeconstrainedbyfactorssuch
asdatadiversity,imageresolution,oroverfitting. Forinstance,inthemostchallengingOCRtasks,
47.8%ofincorrectresponsescontainthegroundtruthasastrictsubstringofthegeneratedresponse,
orviceversa. Thissuggeststhemodelaccuratelyrecognizedthetextbutfailedtotrimorinclude
thecorrectamount. Additionally,performanceofthe7Band30Bmodelsappearstohaveplateaued,
indicatingthatlarger,morediversedatasetsandjointSFTofUIandcorecapabilitiescouldfurther
improvetheperformance.
237 Conclusion
In this work, we build on the insights of MM1 [118] and introduce MM1.5, a family of highly
performantgeneralistMLLMs. WhereMM1providedextensivestudyonkeypre-trainingchoices,
thisworkcomplementsthatbyfocusingonhowtofurtherimproveperformanceafterpre-training,
beyondthestrongbaselinessetbyMM1. Specifically,wefocusonhoningtechniquesforcontinual
pre-training,dynamichigh-resolutionimageprocessing,andcarefulcurationofoursupervisedfine-
tuningdatasets. Weofferextensiveablationsandjustificationsandshowthatourchoicesenablethe
MM1.5modelfamilytoachievestrongresultsacrossarangeofcorecapabilities,includingtext-rich
imageunderstanding,visualreferringandgrounding,andmulti-imagereasoning. Additionally,we
showhowourgeneralistmodelcanbefurtherfine-tunedforvideoandUIunderstanding.Futurework
willaimtounifythesecapabilitiesintoanevenstrongergeneralist. Wehopetheseinsightsbenefit
thecommunitybyhelpingthembuildstrongmodelsbeyondanyspecificarchitectureorcodebase.
References
[1] wendlerc/renderedtext.
[2] MarahAbdin,SamAdeJacobs,AmmarAhmadAwan,JyotiAneja,AhmedAwadallah,HanyAwadalla,
NguyenBach,AmitBahree,ArashBakhtiari,JianminBao,HarkiratBehl,AlonBenhaim,MishaBilenko,
JohanBjorck,SébastienBubeck,QinCai,MartinCai,CaioCésarTeodoroMendes,WeizhuChen,Vishrav
Chaudhary,DongChen,DongdongChen,Yen-ChunChen,Yi-LingChen,ParulChopra,XiyangDai,
AllieDelGiorno,GustavodeRosa,MatthewDixon,RonenEldan,VictorFragoso,DanIter,MeiGao,
MinGao,JianfengGao,AmitGarg,AbhishekGoswami,SuriyaGunasekar,EmmanHaider,JunhengHao,
RussellJ.Hewett,JamieHuynh,MojanJavaheripi,XinJin,PieroKauffmann,NikosKarampatziakis,
DongwooKim,MahoudKhademi,LevKurilenko,JamesR.Lee,YinTatLee,YuanzhiLi,YunshengLi,
ChenLiang,LarsLiden,CeLiu,MengchenLiu,WeishungLiu,EricLin,ZeqiLin,ChongLuo,Piyush
Madan,MattMazzola,ArindamMitra,HardikModi,AnhNguyen,BrandonNorick,BarunPatra,Daniel
Perez-Becker,ThomasPortet,ReidPryzant,HeyangQin,MarkoRadmilac,CorbyRosset,Sambudha
Roy,OlatunjiRuwase,OlliSaarikivi,AminSaied,AdilSalim,MichaelSantacroce,ShitalShah,Ning
Shang,HiteshiSharma,SwadheenShukla,XiaSong,MasahiroTanaka,AndreaTupini,XinWang,Lijuan
Wang,ChunyuWang,YuWang,RachelWard,GuanhuaWang,PhilippWitte,HaipingWu,Michael
Wyatt,BinXiao,CanXu,JiahangXu,WeijianXu,SonaliYadav,FanYang,JianweiYang,ZiyiYang,
YifanYang,DonghanYu,LuYuan,ChengruidongZhang,CyrilZhang,JianwenZhang,LiLynaZhang,
YiZhang,YueZhang,YunanZhang,andXirenZhou. Phi-3technicalreport:Ahighlycapablelanguage
modellocallyonyourphone,2024.
[3] MarahAbdin,SamAdeJacobs,AmmarAhmadAwan,JyotiAneja,AhmedAwadallah,HanyAwadalla,
NguyenBach,AmitBahree,ArashBakhtiari,HarkiratBehl,etal. Phi-3technicalreport: Ahighly
capablelanguagemodellocallyonyourphone. arXivpreprintarXiv:2404.14219,2024.
[4] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,
ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo:avisuallanguagemodelfor
few-shotlearning. NeurIPS,2022.
[5] Anthropic. Claude3modelcard,2023. Accessed:2024-08-13.
[6] AnasAwadalla,IrenaGao,JoshGardner,JackHessel,YusufHanafy,WanrongZhu,KalyaniMarathe,
YonatanBitton,SamirGadre,ShioriSagawa,etal.Openflamingo:Anopen-sourceframeworkfortraining
largeautoregressivevision-languagemodels. arXivpreprintarXiv:2308.01390,2023.
[7] AnasAwadalla,LeXue,OscarLo,ManliShu,HannahLee,EtashKumarGuha,MattJordan,Sheng
Shen,MohamedAwadalla,SilvioSavarese,etal. Mint-1t:Scalingopen-sourcemultimodaldataby10x:
Amultimodaldatasetwithonetrilliontokens. arXivpreprintarXiv:2406.11271,2024.
[8] GillesBaechler,SrinivasSunkara,MariaWang,FedirZubach,HassanMansoor,VincentEtter,Victor
Carbune,JasonLin,JindongChen,andAbhanshuSharma. Screenai:Avision-languagemodelforuiand
infographicsunderstanding. InIJCAI,2024.
[9] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,ChangZhou,and
JingrenZhou. Qwen-vl:Afrontierlargevision-languagemodelwithversatileabilities. arXivpreprint
arXiv:2308.12966,2023.
[10] JonasBelouadi,AnneLauscher,andSteffenEger. Automatikz:Text-guidedsynthesisofscientificvector
graphicswithtikz. arXivpreprintarXiv:2310.00367,2023.
[11] LucasBeyer,AndreasSteiner,AndréSusanoPinto,AlexanderKolesnikov,XiaoWang,DanielSalz,
MaximNeumann,IbrahimAlabdulmohsin,MichaelTschannen,EmanueleBugliarello,etal. Paligemma:
Aversatile3bvlmfortransfer. arXivpreprintarXiv:2407.07726,2024.
24[12] AliFurkanBiten,RubenTito,AndresMafla,LluisGomez,MarçalRusinol,ErnestValveny,CVJawahar,
andDimosthenisKaratzas. Scenetextvisualquestionanswering. InICCV,2019.
[13] MuCai,JianweiYang,JianfengGao,andYongJaeLee. Matryoshkamultimodalmodels. arXivpreprint
arXiv:2405.17430,2024.
[14] JunbumCha,WooyoungKang,JonghwanMun,andByungseokRoh. Honeybee: Locality-enhanced
projectorformultimodalllm. InCVPR,2024.
[15] JunChen,DeyaoZhu,XiaoqianShen,XiangLi,ZechunLiu,PengchuanZhang,RaghuramanKrish-
namoorthi,VikasChandra,YunyangXiong,andMohamedElhoseiny. Minigpt-v2:largelanguagemodel
asaunifiedinterfaceforvision-languagemulti-tasklearning. arXivpreprintarXiv:2310.09478,2023.
[16] KeqinChen,ZhaoZhang,WeiliZeng,RichongZhang,FengZhu,andRuiZhao. Shikra: Unleashing
multimodalllm’sreferentialdialoguemagic. arXivpreprintarXiv:2306.15195,2023.
[17] KezhenChen,RahulThapa,RahulChalamala,BenAthiwaratkun,ShuaiwenLeonSong,andJames
Zou. Dragonfly: Multi-resolution zoom supercharges large visual-language model. arXiv preprint
arXiv:2406.00977,2024.
[18] LinChen,JisongLi,XiaoyiDong,PanZhang,ConghuiHe,JiaqiWang,FengZhao,andDahuaLin.
Sharegpt4v:Improvinglargemulti-modalmodelswithbettercaptions. arXivpreprintarXiv:2311.12793,
2023.
[19] WenhuChen,HongminWang,JianshuChen,YunkaiZhang,HongWang,ShiyangLi,XiyouZhou,and
WilliamYangWang. Tabfact: Alarge-scaledatasetfortable-basedfactverification. arXivpreprint
arXiv:1909.02164,2019.
[20] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and
CLawrenceZitnick. Microsoftcococaptions: Datacollectionandevaluationserver. arXivpreprint
arXiv:1504.00325,2015.
[21] ZheChen,WeiyunWang,HaoTian,ShenglongYe,ZhangweiGao,ErfeiCui,WenwenTong,Kongzhi
Hu,JiapengLuo,ZhengMa,etal. Howfararewetogpt-4v?closingthegaptocommercialmultimodal
modelswithopen-sourcesuites. arXivpreprintarXiv:2404.16821,2024.
[22] ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,SenXing,MuyanZhong,QinglongZhang,
XizhouZhu,LeweiLu,etal. Internvl: Scalingupvisionfoundationmodelsandaligningforgeneric
visual-linguistictasks. InCVPR,2024.
[23] ZhiyuChen,WenhuChen,ChareseSmiley,SameenaShah,IanaBorova,DylanLangdon,ReemaMoussa,
MattBeane,Ting-HaoHuang,BryanRoutledge,etal. Finqa: Adatasetofnumericalreasoningover
financialdata. arXivpreprintarXiv:2109.00122,2021.
[24] ZesenCheng,SicongLeng,HangZhang,YifeiXin,XinLi,GuanzhengChen,YongxinZhu,Wenqi
Zhang,ZiyangLuo,DeliZhao,andLidongBing. VideoLLaMA2:Advancingspatial-temporalmodeling
andaudiounderstandinginvideo-llms. arXivpreprintarXiv:2406.07476,2024.
[25] ZhoujunCheng,HaoyuDong,ZhiruoWang,RanJia,JiaqiGuo,YanGao,ShiHan,Jian-GuangLou,
andDongmeiZhang. Hitab:Ahierarchicaltabledatasetforquestionansweringandnaturallanguage
generation. arXivpreprintarXiv:2108.06712,2021.
[26] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang,
Haodong Duan, Wenwei Zhang, Yining Li, et al. Internlm-xcomposer2-4khd: A pioneering large
vision-languagemodelhandlingresolutionsfrom336pixelsto4khd. arXivpreprintarXiv:2404.06512,
2024.
[27] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang,
Haodong Duan, Wenwei Zhang, Yining Li, et al. Internlm-xcomposer2-4khd: A pioneering large
vision-languagemodelhandlingresolutionsfrom336pixelsto4khd. arXivpreprintarXiv:2404.06512,
2024.
[28] HaodongDuan,JunmingYang,YuxuanQiao,XinyuFang,LinChen,YuanLiu,XiaoyiDong,Yuhang
Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An open-source toolkit for
evaluatinglargemulti-modalitymodels,2024.
[29] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha
Letman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3herdofmodels. arXiv
preprintarXiv:2407.21783,2024.
[30] YunhaoFang,LigengZhu,YaoLu,YanWang,PavloMolchanov,JangHyunCho,MarcoPavone,Song
Han,andHongxuYin. vila2:Vilaaugmentedvila. arXivpreprintarXiv:2407.17453,2024.
[31] MaxwellForbes,ChristineKaeser-Chen,PiyushSharma,andSergeBelongie. Neuralnaturalist:Generat-
ingfine-grainedimagecomparisons. arXivpreprintarXiv:1909.04101,2019.
25[32] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,JinruiYang,Xiawu
Zheng,KeLi,XingSun,YunshengWu,andRongrongJi. Mme:Acomprehensiveevaluationbenchmark
formultimodallargelanguagemodels. arXivpreprintarXiv:2306.13394,2024.
[33] ChaoyouFu,YuhanDai,YondongLuo,LeiLi,ShuhuaiRen,RenruiZhang,ZihanWang,ChenyuZhou,
YunhangShen,MengdanZhang,etal. Video-MME:Thefirst-evercomprehensiveevaluationbenchmark
ofmulti-modalllmsinvideoanalysis. arXivpreprintarXiv:2405.21075,2024.
[34] StephanieFu*,NetanelTamir*,ShobhitaSundaram*,LucyChai,RichardZhang,TaliDekel,andPhillip
Isola. Dreamsim: Learningnewdimensionsofhumanvisualsimilarityusingsyntheticdata. arXiv
preprintarXiv:2306.09344,2023.
[35] XingyuFu,YushiHu,BangzhengLi,YuFeng,HaoyuWang,XudongLin,DanRoth,NoahASmith,
Wei-ChiuMa,andRanjayKrishna. Blink:Multimodallargelanguagemodelscanseebutnotperceive.
arXivpreprintarXiv:2404.12390,2024.
[36] LeoGao,JonathanTow,BaberAbbasi,StellaBiderman,SidBlack,AnthonyDiPofi,CharlesFoster,
LaurenceGolding, JeffreyHsu, AlainLeNoac’h, HaonanLi, KyleMcDonell, NiklasMuennighoff,
ChrisOciepa,JasonPhang,LariaReynolds,HaileySchoelkopf,AviyaSkowron,LintangSutawika,Eric
Tang,AnishThite,BenWang,KevinWang,andAndyZou. Aframeworkforfew-shotlanguagemodel
evaluation,122023.
[37] PengGao,RenruiZhang,ChrisLiu,LongtianQiu,SiyuanHuang,WeifengLin,ShitianZhao,Shijie
Geng,ZiyiLin,PengJin,etal. Sphinx-x:Scalingdataandparametersforafamilyofmulti-modallarge
languagemodels. arXivpreprintarXiv:2402.05935,2024.
[38] ChunjiangGe,SijieCheng,ZimingWang,JialeYuan,YuanGao,JunSong,ShijiSong,GaoHuang,and
BoZheng. Convllava: Hierarchicalbackbonesasvisualencoderforlargemultimodalmodels. arXiv
preprintarXiv:2405.15738,2024.
[39] TomGunter,ZiruiWang,ChongWang,RuomingPang,AndyNarayanan,AonanZhang,BowenZhang,
ChenChen,Chung-ChengChiu,DavidQiu,etal. Appleintelligencefoundationlanguagemodels. arXiv
preprintarXiv:2407.21075,2024.
[40] Agrim Gupta, Piotr Dollár, and Ross B. Girshick. LVIS: A dataset for large vocabulary instance
segmentation. InCVPR,2019.
[41] MuyangHe,YexinLiu,BoyaWu,JianhaoYuan,YuezeWang,TiejunHuang,andBoZhao. Efficient
multimodallearningfromdata-centricperspective. arXivpreprintarXiv:2402.11530,2024.
[42] Musashi Hinck, Matthew L Olson, David Cobbley, Shao-Yen Tseng, and Vasudev Lal. Llava-
gemma:Acceleratingmultimodalfoundationmodelswithacompactlanguagemodel. arXivpreprint
arXiv:2404.01331,2024.
[43] WenyiHong,WeihanWang,MingDing,WenmengYu,QingsongLv,YanWang,YeanCheng,Shiyu
Huang,JunhuiJi,ZhaoXue,etal. Cogvlm2:Visuallanguagemodelsforimageandvideounderstanding.
arXivpreprintarXiv:2408.16500,2024.
[44] WenyiHong,WeihanWang,QingsongLv,JiazhengXu,WenmengYu,JunhuiJi,YanWang,ZihanWang,
YuxiaoDong,MingDing,etal. Cogagent:Avisuallanguagemodelforguiagents. InCVPR,2024.
[45] Yu-ChungHsiao,FedirZubach,MariaWang,andJindongChen. Screenqa:Large-scalequestion-answer
pairsovermobileappscreenshots. arXivpreprintarXiv:2209.08199,2022.
[46] AnwenHu,HaiyangXu,JiaboYe,MingYan,LiangZhang,BoZhang,ChenLi,JiZhang,QinJin,Fei
Huang,etal. mplug-docowl1.5:Unifiedstructurelearningforocr-freedocumentunderstanding. arXiv
preprintarXiv:2403.12895,2024.
[47] AnwenHu,HaiyangXu,JiaboYe,MingYan,LiangZhang,BoZhang,ChenLi,JiZhang,QinJin,Fei
Huang,etal. mplug-docowl1.5:Unifiedstructurelearningforocr-freedocumentunderstanding. arXiv
preprintarXiv:2403.12895,2024.
[48] MingxinHuang,YuliangLiu,DingkangLiang,LianwenJin,andXiangBai. Mini-monkey:Alleviatethe
sawtootheffectbymulti-scaleadaptivecropping. arXivpreprintarXiv:2408.02034,2024.
[49] ShaohanHuang,LiDong,WenhuiWang,YaruHao,SakshamSinghal,ShumingMa,TengchaoLv,Lei
Cui,OwaisKhanMohammed,BarunPatra,etal. Languageisnotallyouneed:Aligningperceptionwith
languagemodels. NeurIPS,2023.
[50] DrewAHudsonandChristopherDManning. Gqa:Anewdatasetforreal-worldvisualreasoningand
compositionalquestionanswering. CVPR,2019.
[51] RaisaIslamandOwanaMarziaMoushi. Gpt-4o: Thecutting-edgeadvancementinmultimodalllm.
AuthoreaPreprints,2024.
[52] HarshJhamtaniandTaylorBerg-Kirkpatrick. Learningtodescribedifferencesbetweenpairsofsimilar
images. arXivpreprintarXiv:1808.10584,2018.
26[53] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis:
Interleavedmulti-imageinstructiontuning. arXivpreprintarXiv:2405.01483,2024.
[54] PengJin,RyuichiTakanobu,WancaiZhang,XiaochunCao,andLiYuan. Chat-univi: Unifiedvisual
representationempowerslargelanguagemodelswithimageandvideounderstanding. InCVPR,2024.
[55] Yizhang Jin, Jian Li, Yexin Liu, Tianjun Gu, Kai Wu, Zhengkai Jiang, Muyang He, Bo Zhao, Xin
Tan, Zhenye Gan, et al. Efficient multimodal large language models: A survey. arXiv preprint
arXiv:2405.10739,2024.
[56] JustinJohnson,BharathHariharan,LaurensVanDerMaaten,LiFei-Fei,CLawrenceZitnick,andRoss
Girshick. Clevr:Adiagnosticdatasetforcompositionallanguageandelementaryvisualreasoning. In
CVPR,2017.
[57] KushalKafle,BrianPrice,ScottCohen,andChristopherKanan.Dvqa:Understandingdatavisualizations
viaquestionanswering. InCVPR,2018.
[58] MehranKazemi,HamidrezaAlvari,AnkitAnand,JialinWu,XiChen,andRaduSoricut. Geomverse:A
systematicevaluationoflargemodelsforgeometricreasoning. arXivpreprintarXiv:2312.12241,2023.
[59] SaharKazemzadeh,VicenteOrdonez,MarkMatten,andTamaraBerg. Referitgame:Referringtoobjects
inphotographsofnaturalscenes. InEMNLP,2014.
[60] AniruddhaKembhavi,MikeSalvato,EricKolve,MinjoonSeo,HannanehHajishirzi,andAliFarhadi. A
diagramisworthadozenimages. InECCV,2016.
[61] AniruddhaKembhavi,MikeSalvato,EricKolve,MinjoonSeo,HannanehHajishirzi,andAliFarhadi. A
diagramisworthadozenimages. InECCV,2016.
[62] GeewookKim,TeakgyuHong,MoonbinYim,JeongYeonNam,JinyoungPark,JinyeongYim,Won-
seokHwang,SangdooYun,DongyoonHan,andSeunghyunPark. Ocr-freedocumentunderstanding
transformer. InECCV,2022.
[63] WonkyunKim,ChanginChoi,WonseokLee,andWonjongRhee. Animagegridcanbeworthavideo:
Zero-shotvideoquestionansweringusingavlm. arXivpreprintarXiv:2403.18406,2024.
[64] RanjayKrishna,YukeZhu,OliverGroth,JustinJohnson,KenjiHata,JoshuaKravitz,StephanieChen,
YannisKalantidis,Li-JiaLi,DavidAShamma,etal. Visualgenome:Connectinglanguageandvision
usingcrowdsourceddenseimageannotations. IJCV,2017.
[65] XinLai,ZhuotaoTian,YukangChen,YanweiLi,YuhuiYuan,ShuLiu,andJiayaJia. Lisa:Reasoning
segmentationvialargelanguagemodel. InCVPR,2024.
[66] HugoLaurençon,AndrésMarafioti,VictorSanh,andLéoTronchon. Buildingandbetterunderstanding
vision-languagemodels:insightsandfuturedirections. arXivpreprintarXiv:2408.12637,2024.
[67] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov,
ThomasWang,SiddharthKaramcheti,AlexanderRush,DouweKiela,etal. Obelics:Anopenweb-scale
filtereddatasetofinterleavedimage-textdocuments. NeurIPS,2024.
[68] HugoLaurençon,LéoTronchon,MatthieuCord,andVictorSanh. Whatmatterswhenbuildingvision-
languagemodels? arXivpreprintarXiv:2405.02246,2024.
[69] HugoLaurençon,LéoTronchon,andVictorSanh. Unlockingtheconversionofwebscreenshotsinto
htmlcodewiththewebsightdataset. arXivpreprintarXiv:2403.09029,2024.
[70] DmitryLepikhin,HyoukJoongLee,YuanzhongXu,DehaoChen,OrhanFirat,YanpingHuang,Maxim
Krikun,NoamShazeer,andZhifengChen.{GS}hard:Scalinggiantmodelswithconditionalcomputation
andautomaticsharding. InICLR,2021.
[71] BoLi,HaoZhang,KaichenZhang,DongGuo,YuanhanZhang,RenruiZhang,FengLi,ZiweiLiu,and
ChunyuanLi. Llava-next:Whatelseinfluencesvisualinstructiontuningbeyonddata?,May2024.
[72] BoLi,PeiyuanZhang,KaichenZhang,FanyiPu,XinrunDu,YuhaoDong,HaotianLiu,YuanhanZhang,
GeZhang,ChunyuanLi,andZiweiLiu. Lmms-eval:Acceleratingthedevelopmentoflargemultimodal
models,March2024.
[73] BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,FanyiPu,JingkangYang,ChunyuanLi,andZiwei
Liu. Mimic-it:Multi-modalin-contextinstructiontuning. arXivpreprintarXiv:2306.05425,2023.
[74] BoLi,YuanhanZhang,DongGuo,RenruiZhang,FengLi,HaoZhang,KaichenZhang,YanweiLi,Ziwei
Liu,andChunyuanLi. Llava-onevision: Easyvisualtasktransfer. arXivpreprintarXiv:2408.03326,
2024.
[75] BohaoLi,RuiWang,GuangzhiWang,YuyingGe,YixiaoGe,andYingShan.Seed-bench:Benchmarking
multimodalllmswithgenerativecomprehension. arXivpreprintarXiv:2307.16125,2023.
[76] ChunyuanLi,ZheGan,ZhengyuanYang,JianweiYang,LinjieLi,LijuanWang,JianfengGao,etal.
Multimodalfoundationmodels:Fromspecialiststogeneral-purposeassistants. FoundationsandTrends®
inComputerGraphicsandVision,16(1-2):1–214,2024.
27[77] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li.
Llava-next-interleave:Tacklingmulti-image,video,and3dinlargemultimodalmodels. arXivpreprint
arXiv:2407.07895,2024.
[78] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li.
Llava-next:Tacklingmulti-image,video,and3dinlargemultimodalmodels,June2024.
[79] GangLiandYangLi. Spotlight:MobileUIunderstandingusingvision-languagemodelswithafocus. In
ICLR,2023.
[80] JiachenLi,XinyaoWang,SijieZhu,Chia-WenKuo,LuXu,FanChen,JiteshJain,HumphreyShi,and
LongyinWen. Cumo: Scalingmultimodalllmwithco-upcycledmixture-of-experts. arXivpreprint
arXiv:2405.05949,2024.
[81] JiapengLi,PingWei,WenjuanHan,andLifengFan. IntentQA:Context-awarevideointentreasoning. In
ICCV,2023.
[82] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-2: Bootstrappinglanguage-imagepre-
trainingwithfrozenimageencodersandlargelanguagemodels. InICML,2023.
[83] KunChangLi,YinanHe,YiWang,YizhuoLi,WenhaiWang,PingLuo,YaliWang,LiminWang,and
YuQiao. Videochat:Chat-centricvideounderstanding. arXivpreprintarXiv:2305.06355,2023.
[84] KunchangLi,YaliWang,YinanHe,YizhuoLi,YiWang,YiLiu,ZunWang,JilanXu,GuoChen,Ping
Luo,etal. Mvbench:Acomprehensivemulti-modalvideounderstandingbenchmark. InCVPR,2024.
[85] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen,
PingLuo,LiminWang,andYuQiao. Mvbench: Acomprehensivemulti-modalvideounderstanding
benchmark. arXivpreprintarXiv:2311.17005,2023.
[86] LeiLi,YuqiWang,RunxinXu,PeiyiWang,XiachongFeng,LingpengKong,andQiLiu. Multimodal
arxiv:Adatasetforimprovingscientificcomprehensionoflargevision-languagemodels. arXivpreprint
arXiv:2403.00231,2024.
[87] QingyunLi,ZheChen,WeiyunWang,WenhaiWang,ShenglongYe,ZhenjiangJin,GuanzhouChen,
YinanHe,ZhangweiGao,ErfeiCui,etal. Omnicorpus:Anunifiedmultimodalcorpusof10billion-level
imagesinterleavedwithtext. arXivpreprintarXiv:2406.08418,2024.
[88] WentongLi,YuqianYuan,JianLiu,DongqiTang,SongWang,JiankeZhu,andLeiZhang. Tokenpacker:
Efficientvisualprojectorformultimodalllm. arXivpreprintarXiv:2407.02392,2024.
[89] YangLi,GangLi,LuhengHe,JingjieZheng,HongLi,andZhiweiGuan. Widgetcaptioning:Generating
naturallanguagedescriptionformobileuserinterfaceelements. InBonnieWebber,TrevorCohn,Yulan
He,andYangLiu,editors,EMNLP,2020.
[90] YanweiLi,ChengyaoWang,andJiayaJia. LLaMA-VID:Animageisworth2tokensinlargelanguage
models. arXivpreprintarXiv:2311.17043,2023.
[91] YanweiLi,YuechenZhang,ChengyaoWang,ZhishengZhong,YixinChen,RuihangChu,Shaoteng
Liu,andJiayaJia. Mini-gemini:Miningthepotentialofmulti-modalityvisionlanguagemodels. arXiv
preprintarXiv:2403.18814,2024.
[92] YifanLi,YifanDu,KunZhou,JinpengWang,WayneXinZhao,andJi-RongWen. Evaluatingobject
hallucinationinlargevision-languagemodels. arXivpreprintarXiv:2305.10355,2023.
[93] ZhangLi,BiaoYang,QiangLiu,ZhiyinMa,ShuoZhang,JingxuYang,YaboSun,YuliangLiu,and
XiangBai. Monkey:Imageresolutionandtextlabelareimportantthingsforlargemulti-modalmodels.
arXivpreprintarXiv:2311.06607,2023.
[94] WingLian,BleysGoodson,EugenePentland,AustinCook,ChanvichetVong,and"Teknium". Openorca:
An open dataset of gpt augmented flan reasoning traces. https://https://huggingface.co/
Open-Orca/OpenOrca,2023.
[95] BinLin,ZhenyuTang,YangYe,JiaxiCui,BinZhu,PengJin,JunwuZhang,MunanNing,andLiYuan.
Moe-llava:Mixtureofexpertsforlargevision-languagemodels. arXivpreprintarXiv:2401.15947,2024.
[96] BinLin,BinZhu,YangYe,MunanNing,PengJin,andLiYuan. Video-llava:Learningunitedvisual
representationbyalignmentbeforeprojection. arXivpreprintarXiv:2311.10122,2023.
[97] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On
pre-trainingforvisuallanguagemodels. InCVPR,2024.
[98] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDollár,
andCLawrenceZitnick. Microsoftcoco:Commonobjectsincontext. InECCV,2014.
[99] ZiyiLin,ChrisLiu,RenruiZhang,PengGao,LongtianQiu,HanXiao,HanQiu,ChenLin,WenqiShao,
KeqinChen,etal. Sphinx:Thejointmixingofweights,tasks,andvisualembeddingsformulti-modal
largelanguagemodels. arXivpreprintarXiv:2311.07575,2023.
28[100] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Llava-1.5: Improvedbaselineswithvisual
instructiontuning. arXivpreprintarXiv:2310.03744,2023.
[101] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee.Llava-next:
Improvedreasoning,ocr,andworldknowledge,2024.
[102] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. arXivpreprint
arXiv:2304.08485,2023.
[103] YuliangLiu,ZhangLi,MingxinHuang,BiaoYang,WenwenYu,ChunyuanLi,XuchengYin,Cheng
linLiu,LianwenJin,andXiangBai. Onthehiddenmysteryofocrinlargemultimodalmodels. arXiv
preprintarXiv:2305.07895,2024.
[104] HaoyuLu,WenLiu,BoZhang,BingxuanWang,KaiDong,BoLiu,JingxiangSun,TongzhengRen,
Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan.
Deepseek-vl: Towardsreal-worldvision-languageunderstanding. arXivpreprintarXiv:2403.05525,
2024.
[105] PanLu,HritikBansal,TonyXia,JiachengLiu,ChunyuanLi,HannanehHajishirzi,HaoCheng,Kai-Wei
Chang,MichelGalley,andJianfengGao. Mathvista:Evaluatingmathematicalreasoningoffoundation
modelsinvisualcontexts. arXivpreprintarXiv:2310.02255,2023.
[106] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu.
Inter-gps:Interpretablegeometryproblemsolvingwithformallanguageandsymbolicreasoning. InACL,
2021.
[107] PanLu,SwaroopMishra,TonyXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,OyvindTafjord,Peter
Clark, andAshwinKalyan. Learntoexplain: Multimodalreasoningviathoughtchainsforscience
questionanswering. InNeurIPS,2022.
[108] PanLu,LiangQiu,Kai-WeiChang,YingNianWu,Song-ChunZhu,TanmayRajpurohit,PeterClark,
andAshwinKalyan. Dynamicpromptlearningviapolicygradientforsemi-structuredmathematical
reasoning. InICLR,2023.
[109] PanLu, LiangQiu, JiaqiChen, TonyXia, YizhouZhao, WeiZhang, ZhouYu, XiaodanLiang, and
Song-ChunZhu. Iconqa: Anewbenchmarkforabstractdiagramunderstandingandvisuallanguage
reasoning. InNeurIPSTrackonDatasetsandBenchmarks,2021.
[110] ZiyangLuo,CanXu,PuZhao,QingfengSun,XiuboGeng,WenxiangHu,ChongyangTao,JingMa,
QingweiLin,andDaxinJiang. Wizardcoder:Empoweringcodelargelanguagemodelswithevol-instruct.
arXivpreprintarXiv:2306.08568,2023.
[111] Fan Ma, Xiaojie Jin, Heng Wang, Yuchen Xian, Jiashi Feng, and Yi Yang. Vista-llama: Reducing
hallucinationinvideolanguagemodelsviaequaldistancetovisualtokens. InCVPR,2024.
[112] MuhammadMaaz,HanoonaRasheed,SalmanKhan,andFahadShahbazKhan.Video-ChatGPT:Towards
detailedvideounderstandingvialargevisionandlanguagemodels. InACL,2024.
[113] KarttikeyaMangalam,RaiymbekAkshulakov,andJitendraMalik. Egoschema:Adiagnosticbenchmark
forverylong-formvideolanguageunderstanding. NeurIPS,2024.
[114] KennethMarino,MohammadRastegari,AliFarhadi,andRoozbehMottaghi. Ok-vqa:Avisualquestion
answeringbenchmarkrequiringexternalknowledge. InCVPR,2019.
[115] AhmedMasry,DoXuanLong,JiaQingTan,ShafiqJoty,andEnamulHoque. Chartqa:Abenchmark
forquestionansweringaboutchartswithvisualandlogicalreasoning. arXivpreprintarXiv:2203.10244,
2022.
[116] Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar.
Infographicvqa. InWACV,2022.
[117] MineshMathew,DimosthenisKaratzas,andCVJawahar.Docvqa:Adatasetforvqaondocumentimages.
InWACV,2021.
[118] BrandonMcKinzie,ZheGan,Jean-PhilippeFauconnier,SamDodge,BowenZhang,PhilippDufter,
DhrutiShah,XianzhiDu,FutangPeng,FlorisWeers,etal. Mm1: Methods,analysis&insightsfrom
multimodalllmpre-training. arXivpreprintarXiv:2403.09611,2024.
[119] LingchenMeng,JianweiYang,RuiTian,XiyangDai,ZuxuanWu,JianfengGao,andYu-GangJiang.
Deepstack:Deeplystackingvisualtokensissurprisinglysimpleandeffectiveforlmms. arXivpreprint
arXiv:2406.04334,2024.
[120] CMike,HMatt,MAnkit,XJianwei,WJun,SSam,GAli,WPatrick,ZMatei,andXReynold. Free
dolly:Introducingtheworld’sfirsttrulyopeninstruction-tunedllm,2023.
[121] AnandMishra, ShashankShekhar, AjeetKumarSingh, andAnirbanChakraborty. Ocr-vqa: Visual
questionansweringbyreadingtextinimages. InICDAR,2019.
29[122] ArindamMitra,HamedKhanpour,CorbyRosset,andAhmedAwadallah. Orca-math: Unlockingthe
potentialofslmsingradeschoolmath. arXivpreprintarXiv:2402.14830,2024.
[123] VarunKNagaraja,VladIMorariu,andLarrySDavis. Modelingcontextbetweenobjectsforreferring
expressionunderstanding. InECCV,2016.
[124] JasonObeidandEnamulHoque. Chart-to-text:Generatingnaturallanguagedescriptionsforchartsby
adaptingthetransformermodel. arXivpreprintarXiv:2010.09142,2020.
[125] OpenAI. Gpt-4vision. OpenAI,2024. https://openai.com/research/gpt-4-vision.
[126] PanupongPasupatandPercyLiang. Compositionalsemanticparsingonsemi-structuredtables. arXiv
preprintarXiv:1508.00305,2015.
[127] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,ShaohanHuang,ShumingMa,andFuruWei.Kosmos-
2:Groundingmultimodallargelanguagemodelstotheworld. arXivpreprintarXiv:2306.14824,2023.
[128] BryanAPlummer,LiweiWang,ChrisMCervantes,JuanCCaicedo,JuliaHockenmaier,andSvetlana
Lazebnik. Flickr30kentities:Collectingregion-to-phrasecorrespondencesforricherimage-to-sentence
models. InICCV,2015.
[129] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham
Cholakkal,RaoMAnwer,EricXing,Ming-HsuanYang,andFahadSKhan. Glamm:Pixelgrounding
largemultimodalmodel. InCVPR,2024.
[130] MachelReid,NikolaySavinov,DenisTeplyashin,DmitryLepikhin,TimothyLillicrap,Jean-baptiste
Alayrac,RaduSoricut,AngelikiLazaridou,OrhanFirat,JulianSchrittwieser,etal.Gemini1.5:Unlocking
multimodalunderstandingacrossmillionsoftokensofcontext. arXivpreprintarXiv:2403.05530,2024.
[131] ZhongweiRen,ZhichengHuang,YunchaoWei,YaoZhao,DongmeiFu,JiashiFeng,andXiaojieJin.
Pixellm:Pixelreasoningwithlargemultimodalmodel. InCVPR,2024.
[132] EldonSchoop,XinZhou,GangLi,ZhourongChen,BjoernHartmann,andYangLi. Predictingand
explainingmobileuitappabilitywithvisionmodelingandsaliencyanalysis. InProceedingsofthe2022
CHIConferenceonHumanFactorsinComputingSystems.AssociationforComputingMachinery,2022.
[133] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi.
A-okvqa:Abenchmarkforvisualquestionansweringusingworldknowledge. InECCV,2022.
[134] ShuaiShao,ZemingLi,TianyuanZhang,ChaoPeng,GangYu,XiangyuZhang,JingLi,andJianSun.
Objects365:Alarge-scale,high-qualitydatasetforobjectdetection. InICCV,2019.
[135] MinShi,FuxiaoLiu,ShihaoWang,ShijiaLiao,SubhashreeRadhakrishnan,De-AnHuang,HongxuYin,
KaranSapra,YaserYacoob,HumphreyShi,etal. Eagle:Exploringthedesignspaceformultimodalllms
withmixtureofencoders. arXivpreprintarXiv:2408.15998,2024.
[136] ChengleiSi,YanzheZhang,ZhengyuanYang,RuiboLiu,andDiyiYang. Design2code:Howfararewe
fromautomatingfront-endengineering? arXivpreprintarXiv:2403.03163,2024.
[137] OleksiiSidorov,RonghangHu,MarcusRohrbach,andAmanpreetSingh. Textcaps:adatasetforimage
captioningwithreadingcomprehension. InECCV,2020.
[138] AmanpreetSingh,VivekNatarajan,MeetShah,YuJiang,XinleiChen,DhruvBatra,DeviParikh,and
MarcusRohrbach. Towardsvqamodelsthatcanread. InCVPR,2019.
[139] EnxinSong,WenhaoChai,TianYe,Jenq-NengHwang,XiLi,andGaoangWang.Moviechat+:Question-
awaresparsememoryforlongvideoquestionanswering. arXivpreprintarXiv:2404.17176,2024.
[140] TomaszStanisławek,FilipGralin´ski,AnnaWróblewska,DawidLipin´ski,AgnieszkaKaliska,Paulina
Rosalska, Bartosz Topolski, and Przemysław Biecek. Kleister: key information extraction datasets
involvinglongdocumentswithcomplexlayouts. InInternationalConferenceonDocumentAnalysisand
Recognition,2021.
[141] AlaneSuhr,StephanieZhou,AllyZhang,IrisZhang,HuajunBai,andYoavArtzi.Acorpusforreasoning
aboutnaturallanguagegroundedinphotographs. arXivpreprintarXiv:1811.00491,2018.
[142] QuanSun,YufengCui,XiaosongZhang,FanZhang,QiyingYu,ZhengxiongLuo,YuezeWang,Yongming
Rao,JingjingLiu,TiejunHuang,andXinlongWang. Generativemultimodalmodelsarein-context
learners. arXivpreprintarXiv:2312.13286,2023.
[143] QuanSun,YufengCui,XiaosongZhang,FanZhang,QiyingYu,YuezeWang,YongmingRao,Jingjing
Liu,TiejunHuang,andXinlongWang. Generativemultimodalmodelsarein-contextlearners. InCVPR,
2024.
[144] QuanSun,QiyingYu,YufengCui,FanZhang,XiaosongZhang,YuezeWang,HongchengGao,Jingjing
Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv preprint
arXiv:2307.05222,2023.
[145] StaceySvetlichnaya. Deepform:Understandstructureddocumentsatscale,2020.
30[146] RyotaTanaka,KyosukeNishida,andSenYoshida. Visualmrc: Machinereadingcomprehensionon
documentimages. InAAAI,2021.
[147] BennyJ.Tang,AngieBoggust,andArvindSatyanarayan. VisText:ABenchmarkforSemanticallyRich
ChartCaptioning. InACL,2023.
[148] Jingqun Tang, Chunhui Lin, Zhen Zhao, Shu Wei, Binghong Wu, Qi Liu, Hao Feng, Yang Li, Siqi
Wang, LeiLiao, etal. Textsquare: Scalinguptext-centricvisualinstructiontuning. arXivpreprint
arXiv:2404.12803,2024.
[149] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu
Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable
multimodalmodels. arXivpreprintarXiv:2312.11805,2023.
[150] Qwenteam. Qwen2-vl,August2024.
[151] ShengbangTong,EllisBrown,PenghaoWu,SanghyunWoo,ManojMiddepogu,SaiCharithaAkula,
JihanYang,ShushengYang,AdithyaIyer,XichenPan,AustinWang,RobFergus,YannLeCun,and
SainingXie. Cambrian-1:Afullyopen,vision-centricexplorationofmultimodalllms. arXivpreprint
arXiv:2406.16860,2024.
[152] ShengbangTong,ZhuangLiu,YuexiangZhai,YiMa,YannLeCun,andSainingXie. Eyeswideshut?
exploringthevisualshortcomingsofmultimodalllms. InCVPR,2024.
[153] MariaTsimpoukelli,JacobLMenick,SerkanCabi,SMEslami,OriolVinyals,andFelixHill.Multimodal
few-shotlearningwithfrozenlanguagemodels. NeurIPS,2021.
[154] Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. Screen2words:
Automaticmobileuisummarizationwithmultimodallearning. InThe34thAnnualACMSymposiumon
UserInterfaceSoftwareandTechnology.AssociationforComputingMachinery,2021.
[155] FeiWang, XingyuFu, JamesYHuang, ZekunLi, QinLiu, XiaogengLiu, MingyuDerekMa, Nan
Xu,WenxuanZhou,KaiZhang,etal. Muirbench:Acomprehensivebenchmarkforrobustmulti-image
understanding. arXivpreprintarXiv:2406.09411,2024.
[156] WeihanWang,QingsongLv,WenmengYu,WenyiHong,JiQi,YanWang,JunhuiJi,ZhuoyiYang,
LeiZhao,XixuanSong,etal. Cogvlm: Visualexpertforpretrainedlanguagemodels. arXivpreprint
arXiv:2311.03079,2023.
[157] WenhaiWang,ZheChen,XiaokangChen,JiannanWu,XizhouZhu,GangZeng,PingLuo,TongLu,Jie
Zhou,YuQiao,etal. Visionllm:Largelanguagemodelisalsoanopen-endeddecoderforvision-centric
tasks. arXivpreprintarXiv:2305.11175,2023.
[158] BoWu,ShoubinYu,ZhenfangChen,JoshuaBTenenbaum,andChuangGan. Star:Abenchmarkfor
situatedreasoninginreal-worldvideos. arXivpreprintarXiv:2405.09711,2024.
[159] HaoningWu,HanweiZhu,ZichengZhang,ErliZhang,ChaofengChen,LiangLiao,ChunyiLi,Annan
Wang,WenxiuSun,QiongYan,etal. Towardsopen-endedvisualqualitycomparison. arXivpreprint
arXiv:2402.16641,2024.
[160] x.ai. Grok-1.5visionpreview.
[161] JunbinXiao,XindiShang,AngelaYao,andTat-SengChua. Next-qa:Nextphaseofquestion-answering
toexplainingtemporalactions. InCVPR,2021.
[162] JunbinXiao,XindiShang,AngelaYao,andTat-SengChua.NExT-QA:Nextphaseofquestion-answering
toexplainingtemporalactions. InCVPR,2021.
[163] LinXu,YilinZhao,DaquanZhou,ZhijieLin,SeeKiongNg,andJiashiFeng. Pllava:Parameter-free
llavaextensionfromimagestovideosforvideodensecaptioning. arXivpreprintarXiv:2404.16994,
2024.
[164] MingzeXu,MingfeiGao,ZheGan,Hong-YouChen,ZhengfengLai,HaimingGang,KaiKang,and
AfshinDehghan. SlowFast-LLaVA:Astrongtraining-freebaselineforvideolargelanguagemodels.
arXivpreprintarXiv:2407.15841,2024.
[165] RuyiXu,YuanYao,ZonghaoGuo,JunboCui,ZanlinNi,ChunjiangGe,Tat-SengChua,ZhiyuanLiu,
MaosongSun,andGaoHuang. Llava-uhd: anlmmperceivinganyaspectratioandhigh-resolution
images. arXivpreprintarXiv:2403.11703,2024.
[166] LeXue,ManliShu,AnasAwadalla,JunWang,AnYan,SenthilPurushwalkam,HongluZhou,Viraj
Prabhu,YutongDai,MichaelSRyoo,etal.xgen-mm(blip-3):Afamilyofopenlargemultimodalmodels.
arXivpreprintarXiv:2408.08872,2024.
[167] JianweiYang,HaoZhang,FengLi,XueyanZou,ChunyuanLi,andJianfengGao.Set-of-markprompting
unleashesextraordinaryvisualgroundingingpt-4v. arXivpreprintarXiv:2310.11441,2023.
31[168] Huanjin Yao, Wenhao Wu, Taojiannan Yang, YuXin Song, Mengxi Zhang, Haocheng Feng, Yifan
Sun, Zhiheng Li, Wanli Ouyang, and Jingdong Wang. Dense connector for mllms. arXiv preprint
arXiv:2405.13800,2024.
[169] YuanYao,TianyuYu,AoZhang,ChongyiWang,JunboCui,HongjiZhu,TianchiCai,HaoyuLi,Weilin
Zhao,ZhihuiHe,etal.Minicpm-v:Agpt-4vlevelmllmonyourphone.arXivpreprintarXiv:2408.01800,
2024.
[170] HaoxuanYou,HaotianZhang,ZheGan,XianzhiDu,BowenZhang,ZiruiWang,LiangliangCao,Shih-Fu
Chang,andYinfeiYang. Ferret:Referandgroundanythinganywhereatanygranularity. arXivpreprint
arXiv:2310.07704,2023.
[171] KeenYou,HaotianZhang,EldonSchoop,FlorisWeers,AmandaSwearngin,JeffreyNichols,Yinfei
Yang,andZheGan. Ferret-ui:Groundedmobileuiunderstandingwithmultimodalllms. arXivpreprint
arXiv:2404.05719,2024.
[172] PeterYoung, AliceLai, MicahHodosh, andJuliaHockenmaier. Fromimagedescriptionstovisual
denotations:Newsimilaritymetricsforsemanticinferenceovereventdescriptions. TACL,2014.
[173] LichengYu,PatrickPoirson,ShanYang,AlexanderCBerg,andTamaraLBerg. Modelingcontextin
referringexpressions. InECCV,2016.
[174] WeihaoYu,ZhengyuanYang,LinjieLi,JianfengWang,KevinLin,ZichengLiu,XinchaoWang,and
LijuanWang. Mm-vet:Evaluatinglargemultimodalmodelsforintegratedcapabilities. arXivpreprint
arXiv:2308.02490,2023.
[175] ZhouYu,DejingXu,JunYu,TingYu,ZhouZhao,YuetingZhuang,andDachengTao. ActivityNet-QA:
Adatasetforunderstandingcomplexwebvideosviaquestionanswering. InAAAI,2019.
[176] YuqianYuan,WentongLi,JianLiu,DongqiTang,XinjieLuo,ChiQin,LeiZhang,andJiankeZhu.
Osprey:Pixelunderstandingwithvisualinstructiontuning. InCVPR,2024.
[177] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,RuoqiLiu,GeZhang,SamuelStevens,Dongfu
Jiang,WeimingRen,YuxuanSun,etal. Mmmu:Amassivemulti-disciplinemultimodalunderstanding
andreasoningbenchmarkforexpertagi. arXivpreprintarXiv:2311.16502,2023.
[178] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.
Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint
arXiv:2309.05653,2023.
[179] YuhangZang,WeiLi,JunHan,KaiyangZhou,andChenChangeLoy. Contextualobjectdetectionwith
multimodallargelanguagemodels. arXivpreprintarXiv:2305.18279,2023.
[180] ChiZhang,FengGao,BaoxiongJia,YixinZhu,andSong-ChunZhu. Raven:Adatasetforrelationaland
analogicalvisualreasoning. InCVPR,2019.
[181] HaotianZhang,HaoxuanYou,PhilippDufter,BowenZhang,ChenChen,Hong-YouChen,Tsu-JuiFu,
WilliamYangWang,Shih-FuChang,ZheGan,etal. Ferret-v2:Animprovedbaselineforreferringand
groundingwithlargelanguagemodels. arXivpreprintarXiv:2404.07973,2024.
[182] PanZhang,XiaoyiDong,YuhangZang,YuhangCao,RuiQian,LinChen,QipengGuo,HaodongDuan,
Bin Wang, Linke Ouyang, et al. Internlm-xcomposer-2.5: A versatile large vision language model
supportinglong-contextualinputandoutput. arXivpreprintarXiv:2407.03320,2024.
[183] RuohongZhang,LiangkeGui,ZhiqingSun,YihaoFeng,KeyangXu,YuanhanZhang,DiFu,Chunyuan
Li,AlexanderHauptmann,YonatanBisk,etal. Directpreferenceoptimizationofvideolargemultimodal
modelsfromlanguagemodelreward. arXivpreprintarXiv:2404.01258,2024.
[184] ShilongZhang,PeizeSun,ShoufaChen,MinXiao,WenqiShao,WenweiZhang,KaiChen,andPingLuo.
Gpt4roi:Instructiontuninglargelanguagemodelonregion-of-interest. arXivpreprintarXiv:2307.03601,
2023.
[185] Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, Liang Wang, and Rong Jin.
Beyondllava-hd:Divingintohigh-resolutionlargemultimodalmodels. arXivpreprintarXiv:2406.08487,
2024.
[186] ZichengZhang,HaoningWu,ErliZhang,GuangtaoZhai,andWeisiLin. Abenchmarkformulti-modal
foundationmodelsonlow-levelvision:fromsingleimagestopairs. arXivpreprintarXiv:2402.07116,
2024.
[187] YangZhao,ZhijieLin,DaquanZhou,ZilongHuang,JiashiFeng,andBingyiKang. Bubogpt:Enabling
visualgroundinginmulti-modalllms. arXivpreprintarXiv:2307.08581,2023.
[188] TianyuZheng,GeZhang,TianhaoShen,XuelingLiu,BillYuchenLin,JieFu,WenhuChen,andXiang
Yue. Opencodeinterpreter:Integratingcodegenerationwithexecutionandrefinement. arXivpreprint
arXiv:2402.14658,2024.
32[189] VictorZhong,CaimingXiong,andRichardSocher. Seq2sql:Generatingstructuredqueriesfromnatural
languageusingreinforcementlearning. arXivpreprintarXiv:1709.00103,2017.
[190] BaichuanZhou,YingHu,XiWeng,JunlongJia,JieLuo,XienLiu,JiWu,andLeiHuang. Tinyllava:A
frameworkofsmall-scalelargemultimodalmodels. arXivpreprintarXiv:2402.14289,2024.
[191] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing
vision-languageunderstandingwithadvancedlargelanguagemodels. arXivpreprintarXiv:2304.10592,
2023.
[192] FengbinZhu,WenqiangLei,YouchengHuang,ChaoWang,ShuoZhang,JianchengLv,FuliFeng,and
Tat-SengChua. TAT-QA:Aquestionansweringbenchmarkonahybridoftabularandtextualcontentin
finance. InACL,2021.
[193] YongshuoZong,OndrejBohdal,andTimothyHospedales. Vl-iclbench: Thedevilinthedetailsof
benchmarkingmultimodalin-contextlearning. arXivpreprintarXiv:2403.13164,2024.
[194] BarretZoph, IrwanBello, SameerKumar, NanDu, YanpingHuang, JeffDean, NoamShazeer, and
William Fedus. St-moe: Designing stable and transferable sparse expert models. arXiv preprint
arXiv:2202.08906,2022.
33A Appendix
A.1 ImpactofSyntheticCaptionsbySelf-Training
Figure13: Impactofsyntheticcaptionsforcontinualpre-trainingthe3Bmodel,buildingontopof
thefinalMM1.5strategyintroducedinthemaintext,i.e.,includingtheOCRcontinualpre-training
stage. Wereporttheimpactofaddingincrementallymoresyntheticcaptions,upto7Mintotal.
Besides using public captioning data mentioned in Section 3.3, we also follow [30] to study the
effectofself-trainingusingsyntheticcaptions. Thisisparticularlyimportantbecausecaptioning
datageneratedbyblack-boxcommercialmodelscansometimesbedifficulttoscaleanddatafrom
openmodels,suchasfromtheLLaVA-NeXTfamilymaybeconstrainedbythesemodel’sinherent
limitations. Wedevelopaself-augmentedimage-captiondataenginebuildingonourpreviouswork,
MM1 [118] with a goal to provide high quality captions in a computationally efficient manner.
Specifically,wefine-tuneapre-trained3BMM1-stylemodelonamixofsyntheticandapproximately
8khuman-annotatedparagraph-lengthimagecaptions(approximately70tokensonaverage). We
thenapplythiscaptioneratscaleto290millionweb-crawledimageswithresolutionsrangingfrom
512to1024px. Followingtheapproachin[101],weperformconceptfilteringbasedonthegenerated
captions,resultinginadatasetof7millionhigh-qualitycaptions,whichincludesnumeroustext-rich
examplesandalt-text-derivedknowledge.
Buildingonthis,weinvestigatetheimpactofthevolumeofsyntheticcaptiondata. Thesynthetic
captions,rangingfrom1.4millionto7million,showconsistentimprovementsinmodelperformance
acrossvariousmetrics,asillustratedinFigure13. Specifically,aratioof0onthex-axisindicatesthat
noimage-captiondataisaddedtotheoriginalOCRdataset,whileratiosrangingfrom0.2to1.0on
thex-axisrepresenttheproportionofthetotal7Mdatasetincludedinthetraining. Forexample,a
ratioof0.2correspondstoapproximately1.4Mimage-captionpairsaddedwiththeoriginalOCR
datawhile1.0denotesall7Mareaddedintotraining.
In contrast to the ShareGPT4V-PT and LLaVA-Recap-3M captions as explored in Section 3.3,
wefindthataddingourin-housesyntheticcaptionstotheOCRdatamixturecanleadtoconsistent
improvementforcontinualpre-training.4 Weobserveimprovementsinknowledge-relatedbenchmarks
andtheaggregatedMMBasescoresthatfurtherscalewithincreaseddatavolume. Thisisespecially
notable,sinceourin-housecaptionerisusingonlya3BmodelwhileLLaVA-Recap,forexample,
useda34Bmodelforcaptioning.
OurresultssuggestthatwhileacomparativelysimpleOCRmixturerepresentsastrongbaselinefor
continualpre-trainingdata,high-qualitycaptionscanstillleadtofurtherimprovements. However,the
quality,distribution,perhapsevenstyleandlengthofthegeneratedcaptionsseemcrucialtorealize
gains. Whileourin-housecaptionsempiricallyoutperformedpubliclyavailabledatainourspecific
setting,furtherresearchisnecessaryastowhat,specifically,theseimprovementsareattributableto
andwhetherfurtherimprovementscanbeachieved. Thisinvestigationgoesbeyondthescopeofthis
paperandweaimtofurtherstudysyntheticcaptioninginfuturework.
A.2 DetailsofSFTDataforAblation
AspresentedinTable13,weuseasubsetofourfinalSFTdatawhenconductingablationstudy.
4ExperimentsinthissectionarebasedonthefinalrecipefromSection4,withslightlydifferentsettings
comparedtothoseinSection3.4.
34Datacategory Sub-category Datasets #QA
LLaVAComplexReasoning,LLaVAConversation,
General ShareGPT-4v,CocoCaption,LLaVAv1.5VQAv2OKVQA, 542K
LLaVAv1.5GQA,LLaVAv1.5A-OKVQA
OCRVQA,Synthdog-En,TextCaps,TextVQA,DVQA,
TextRich ChartQA,DocVQA,InfoVQA,VisualMRC, 1.3M
WikiTQ,DeepForm,KleisterCharity,TabFact
Single-image
GRIT-VisualGenome,GRIT-Regionreasoning,GRIT-Flickr30k,
Refer&Ground 1.08M
GRIT-Refcoco,GRIT-SpatialNegativeMining
Science AI2D,ScienceQA 8K
Math GeomVerse,CLEVER,IconQA,RAVEN,Inter-GPS 150K
Code WebSight,DaTikZ,Design2Code 58K
DreamSim,NLVR2,Star,Birds-to-Words,IconQA,
Multi-image – Spot-the-diff,ICL-instruct,Coinstruct,MultiVQA, 324K
NExT-QA,CocoInstructInterleaved
Dolly,OpenOrca,MathInstruct,
Text-only – 1.71M
WizardCoder,OrcaMath,OpenCodeInterpreter
Table13: OverviewoftheSFTdatausedinablationstudy.
A.3 MM1.5In-HouseMulti-ImageSFTData
Wegeneratedtwosetsofin-houseSFTdataforthemulti-imagecapabilitiesofourmodel: coco-
instruct-interleavedandicl-instruct.
Coco-instruct-interleavedwasgeneratedusinganLLaVa-styleapproachusingourtext-onlyLLM.
WeinputCOCOimagecaptionsfortwoimages,alongwithapromptrequiringtheLLMtogenerate
aconversationbetweenauserandanassistant,withthetwoimagesinsertedincontext. Thisdataset
helpsourmodellearnconversationalmulti-imagecapabilities. Anexamplefromthisdatasetisshown
below. Thisdatasetcontains~2kexamples,with2imagesperexample.
User: <image1>Whatdoyouseeinthefollowingpicture?
Assistant: I see an open beige toilet with a monitoring device attached to the
seat. Thedeviceappearstobedisplayingvariousmetrics,suchastemperatureand
pressure,anditislikelyusedformedicalpurposes.
User: <image2>Whataboutthispicture?
Assistant: Iseearowofparkingmeterssittingnexttoeachothernearastreet.
Themetersaretypicalofthoseusedtoenforceparkingregulationsinmanycities.
User: Whatisthedifferencebetweenthetwopictures?
Assistant: Thefirstpicturedepictsatoiletwithamonitoringdevice, whilethe
secondpictureshowsarowofparkingmeters. Thesearetwoverydifferentobjects
indifferentsettings. Thefirstpictureislikelyusedformedicalpurposes,whilethe
secondpictureisusedtoregulateparkingincities.
Icl-instructwasgeneratedmanuallytohelpthemodelretainin-contextlearningabilitiesfrompre-
trainingalongwithmorenuancedin-contextlearningtaskswhichrequiremulti-stepreasoningfrom
themodel. Thisdatasetcontainsdifferentvarietiesofin-contextlearningtasks,someofwhichare
similartothosefoundintheVL-ICLbenchmark[193]. Wefoundthatincludingthisdataset,which
contains~500examples,greatlybooststhemodels’in-contextlearningperformance.
A.4 MM1.5BenchmarkDetails
Benchmarks used for MM1.5 evaluation are summarized in Table 14, where for each Category
AverageScore,wedirectlycalculatetheaverageofeachmetricnumberwithinthatcapabilitycategory
asfollows,withdetailedevaluationmetricslistedinTable14.
35Category Benchmark Metric
MME[32] NormalizedAccuracy
SEED[75] Seed-IMG
POPE[92] Averageofrandom,popularandadversarial
General
LLaVA-Bench(Wild)[102] GPT-assistedscore
MM-Vet[174] GPT-assistedscore
RealWorldQA[160] Accuracy
WTQ[126] Accuracy
TabFact[19] Accuracy
OCRBench[103] Accuracy
Text-rich ChartQA[115] Accuracy
TextVQA[138] VQAOpenFlamingoAccuracy
DocVQA[117] ANLSScore
InfoVQA[116] ANLSScore
Flickr30K[172] Recall(IoU>0.5,anyprotocol)
LVIS_Ferret[40,170] Accuracy
Refcoco[59] Recall@1(IoU>0.5)
Refer&Ground
Refcoco+[59] Recall@1(IoU>0.5)
Refcocog[59] Recall@1(IoU>0.5)
Ferret-Bench†[170] GPT-assistedscore
AI2D[61] Accuracy
ScienceQA[107] Accuracy-IMG
Knowledge(Math/Science/Code)
MathVista[105] GPT-assistedscore
MMMU[177] Accuracy
Qbench2[186] Accuracy
Mantis[53] Accuracy
NLVR2[141] Accuracy
Multi-image
BLINK[35] Accuracy
MVbench[85] Accuracy
Muirbench†[155] Accuracy
Table 14: Details of benchmarks and their metrics used in MM1.5 ablation study. Benchmarks
markedwith(†)areexcludedfromthecategoryaverage.
• GeneralAverageScore: averagescoreofthecorrespondingmetricscoresfromMME-Normalize5,
Seed-IMG,POPE,LLaVAW,MM-VetandRealWorldQA.
• Text-richAverageScore: averagescoreofthecorrespondingmetricscoresfromWTQ,TabFact,
OCRBench6,ChartQA7,TextVQA,DocVQAandInfoVQA.
• Refer&groundAverageScore: averageofthescoresofFlickr30k,RefCOCOavg. andLVISavg.,
whereRefCOCOavg. istheaverageofRefCOCOA,RefCOCOB,RefCOCO+A,RefCOCO+B
andRefCOCOg,andLVISavg. istheaverageofpointandboxmetrics.
• KnowledgeAverageScore: averagescoreofthecorrespondingmetricscoresfromAI2D,Sci-
enceQA,MathVistaandMMMU.
• Multi-imageaveragescore: averageofQbench,Mantis,NLVR2,BLINKandMVBenchmetric
scores.
• MMBaseScore: averagescoreoftheGeneralAverageScore,Text-richAverageScoreandKnowl-
edge Average Score. This aggregated metric is used in Section 3 to measure the impact of the
general,text-rich,andknowledgecapabilitiesofamodel.
A.5 MM1.5-UIBenchmarkDetails
ThepublicbenchmarktasksandmetricsforevaluatingMM1.5-UIare:
• Screen2words is a captioning task where each complete screen is paired with 5 ground-truth
high-levelsummariesundertenwords. Thegeneratedsummaries’qualityismeasuredbyCIDEr
scorebetweentheground-truthandgeneratedsummaries.
• WidgetCaptioningisacaptioningtaskwhereacertainscreenareathatcorrespondstoawidget
(e.g.,button,listitem)ispairedwith3ground-truthcaptions. Thegeneratedsummaries’qualityis
measuredbyCIDErscoresimilartoScreen2words.
5MME-Normalizeis(MME-Perception+MME-Cognition)/2800×100%.
6TheaccuracyofOCRbenchisthetotalscorenormalizedby1000×100%.
7Averageofhumanpartaccuracyandaugmentedpartaccuracy.
36• Taperceptionisabinaryclassificationtaskwhereacertainscreenareathatcorrespondtoawidget
(e.g., button, list item) is paired with a ground-truth binary label of whether the screen area is
’tappable’(i.e.,clickablebyusers). Thegeneratedlabels’qualityismeasuredbyF1score.
TheFerret-UIElementarytaskbenchmarksusedtoevaluateMM1.5-UI,organizedbycapability
categories,are:
• Ferret-UIGrounding(Grd-i/A)isasetofthreegrounding-basedUItasksintroducedinFerret-
UI[171]. Thesetasksqueryforcertainareasofscreensthatmeetcertaincriteria. Theyinclude
findingawidgetgivenatextdescription,findinganicongiventheclassoftheicon,andfindinga
textlocationonscreen. Theexpectedresponsefromthemodelisaboundingbox,andthequality
oftheboundingboxismeasuredbyRecallwithIoU>0.5.
• Ferret-UI Referring (Ref-i/A) is a set of three referring-based UI tasks introduced in Ferret-
UI[171]. Thesetasksqueryaboutknowledgeorcharacteristicsthatcorrespondtocertainareasof
thescreens. Theyincludeclassifyingthetypeofwidgetsinthegivenareas,recognizingthetypeof
iconsinthegivenareas,andrecognizingtextsinthegivenareas. Thequalityoftheresponsesis
measuredbyexactmatchaccuracies.
EachofthesetwosetsFerret-UItasksfurtherhavetwovariantswithscreenshotsfromtwotypes
ofoperatingsystems(iOS/Android),ofwhichAndroidtasksaredenotedas-A(e.g.,Grd-A),and
iPhonetasksas-i,whichresultsin12tasksintotal.
A.6 DynamicvsStaticImageSplitting
DetailedablationstudyofMM1.5withdifferentimagesplittingstrategiesisshowninTable15,16,
17and18. Allmodelsareusingthefinalsettingexceptfortheimagesplitting(dynamicvs. static).
KnowledgeBenchmarks GeneralBenchmarks
Model AI2D SQA MMMU MathV MME SEEDI POPE LLaVAW MM-Vet RealWorldQA
(test) (test) (val) (testmini) (P/C)
1BModelComparison
MM1.5-1B(S) 59.5 83.9 36.1 37.4 1393.4/244.9 69.99 87.9 67.9 34.2 51.8
MM1.5-1B(D) 59.3 82.1 35.8 37.2 1365.7/245.7 70.2 88.1 71.6 37.4 53.3
MM1.5-1B-MoE(S) 66.4 86.8 41.2 42.2 1481.4/293.6 71.3 89.5 76.5 41.8 60.1
MM1.5-1B-MoE(D) 67.1 87.6 41.2 42.9 1511.9/361.1 71.4 88.6 75.5 39.8 57.8
3BModelComparison
MM1.5-3B(S) 66.1 87.2 36.8 43.1 1439.8/297.5 71.9 88.3 72.1 38.3 58.8
MM1.5-3B(D) 65.7 85.8 37.1 44.4 1478.4/319.6 72.4 88.1 73.0 41.0 56.9
MM1.5-3B-MoE(S) 66.3 89.3 41.9 43.1 1527.8/342.5 72.4 88.4 78.5 41.4 59.2
MM1.5-3B-MoE(D) 69.9 89.8 42.9 46.9 1591.4/365.7 73.3 87.2 76.1 43.7 60.7
7BModelComparison
MM1.5-7B(S) 72.2 89.6 44.1 49.1 1531.3/366.4 73.5 88.6 77.2 43.3 57.0
MM1.5-7B(D) 72.2 89.6 41.8 47.6 1514.9/346.4 73.4 88.6 74.2 42.2 62.5
30BModelComparison
MM1.5-30B(S) 75.4 92.8 46.8 56.0 1605.2/402.1 74.1 89.0 79.5 49.4 68.0
MM1.5-30B(D) 77.2 91.9 47.4 55.6 1646.2/405.7 75.0 88.6 80.4 52.0 69.0
Table15: Comparisonofourmodelswhenusingdynamicvs. staticimagesplitting. Wefollowour
finalsettingsforallmodels. (S)and(D)indicatestaticanddynamicsplitting,respectively.
A.7 MethodologyforRunningCompetitorModels
ThissectioncoversthemethodologyusedtoreportresultsforPhi-3-Vision[3],LLaVA-OneVision
[74],InternVL2[21]andMiniCPM-V2[169]. Whenavailable,wereportedtheresultspublishedby
theoriginalauthors,eitherintheirtechnicalreportsoronpublicleaderboards8. Whennotavailable,
weimplementedinferencerunnersusingpubliclyreleasedcheckpoints. Commonly,wefollowed
[72]’simplementationsthatweadaptedonourowninternalforkoflm-eval-harness[36,118]. To
verify the validity of our inference implementations, we ensured we could reproduce previously
publishedresultswithinstandarddeviation. Below,wesharedetailsforeachmodelimplementation:
8https://huggingface.co/spaces/opencompass/open_vlm_leaderboard
37Text-richBenchmarks
Model WTQ TabFact OCRBench ChartQA TextVQA DocVQA InfoVQA
(test) (test) (test) (test) (val) (test) (test)
1BModelComparison
MM1.5-1B(S) 31.0 65.4 60.4 67.5 72.8 79.7 40.8
MM1.5-1B(D) 34.1 66.1 60.5 67.2 72.5 81.0 50.5
MM1.5-1B-MoE(S) 34.1 69.6 58.0 72.7 75.8 82.5 46.0
MM1.5-1B-MoE(D) 38.9 71.4 62.6 73.7 76.1 84.8 55.9
3BModelComparison
MM1.5-3B(S) 36.3 71.0 61.1 74.3 75.2 84.0 45.8
MM1.5-3B(D) 41.8 72.9 65.7 74.2 76.5 87.7 58.5
MM1.5-3B-MoE(S) 32.5 70.1 60.2 73.0 75.9 81.0 44.2
MM1.5-3B-MoE(D) 39.1 73.1 63.8 73.6 76.8 85.0 53.6
7BModelComparison
MM1.5-7B(S) 38.4 73.7 59.7 77.9 76.1 84.5 47.3
MM1.5-7B(D) 46.0 75.9 63.5 78.6 76.5 88.1 59.5
30BModelComparison
MM1.5-30B(S) 46.0 81.0 64.5 82.4 78.7 88.5 53.2
MM1.5-30B(D) 54.1 84.0 65.8 83.6 79.2 91.4 67.3
Table16: Comparisonofourmodelswhenusingdynamicvs. staticimagesplitting. Wefollowour
finalsettingsforallmodels. (S)and(D)indicatestaticanddynamicsplitting,respectively.
ReferandGroundBenchmarks
Model Flickr30k
RefCOCOavg. LVISavg. Ferret-Benchavg.
(test)
1BModelComparison
MM1.5-1B(S) 82.0 82.7 62.4 69.7
MM1.5-1B(D) 81.4 83.0 62.2 67.4
MM1.5-1B-MoE(S) 79.3 80.9 63.9 73.4
MM1.5-1B-MoE(D) 84.8 85.4 64.6 69.6
3BModelComparison
MM1.5-3B(S) 85.1 85.3 68.1 71.2
MM1.5-3B(D) 85.6 85.9 67.9 69.5
MM1.5-3B-MoE(S) 82.8 82.6 67.5 70.8
MM1.5-3B-MoE(D) 86.2 85.8 66.9 72.2
7BModelComparison
MM1.5-7B(S) 87.2 86.0 68.8 71.2
MM1.5-7B(D) 86.6 85.3 66.4 72.6
30BModelComparison
MM1.5-30B(S) 90.1 87.7 73.2 75.6
MM1.5-30B(D) 90.1 87.5 73.1 77.1
Table17: Comparisonofourmodelswhenusingdynamicvs. staticimagesplitting. Wefollowour
finalsettingsforallmodels. (S)and(D)indicatestaticanddynamicsplitting,respectively.
Phi-3-Vision. WeusedthepublicPhi-3-Visioncheckpoint9 andranitonourfamiliesofbenchmarks.
Forgeneral,text-rich,knowledgeandrefer&groundbenchmarks,whenthepositionoftheimage
isnotdeterminedbythetask,weprependtheimagetothetextinputfollowingtheexamplesgiven
inthePhi-3-Visioncookbook10. Forgroundingbenchmarks,weintroducedthefollowingprompt:
“Question: {question}<b>Answerthisquestionbylistingtherequestedentitiesandtheirbounding
boxes. The bounding boxes are formatted as follows: <x1,y1,x2,y2>, each value is between 0-
9https://huggingface.co/microsoft/Phi-3-vision-128k-instruct
10https://github.com/microsoft/Phi-3CookBook/blob/main/md/03.Inference/Vision_Inference.md#3-
comparison-of-multiple-images
38Multi-imageBenchmarks
Model QBench2 Mantis NLVR2 BLINK Muirbench
MVBench
(val) (test) (val) (val) (test)
1BModelComparison
MM1.5-1B(S) 65.8 48.4 78.6 46.1 41.9 34.0
MM1.5-1B(D) 66.4 50.7 79.0 45.8 46.3 34.7
MM1.5-1B-MoE(S) 70.2 52.1 83.0 47.4 44.8 42.5
MM1.5-1B-MoE(D) 70.9 51.2 83.2 48.3 43.7 40.9
3BModelComparison
MM1.5-3B(S) 72.0 53.5 83.9 47.8 42.5 44.5
MM1.5-3B(D) 73.2 54.8 83.8 47.7 46.8 44.3
MM1.5-3B-MoE(S) 70.4 54.4 85.3 47.2 47.1 44.2
MM1.5-3B-MoE(D) 73.8 54.4 86.0 50.3 49.8 45.6
7BModelComparison
MM1.5-7B(S) 73.0 56.7 87.2 49.7 47.6 53.8
MM1.5-7B(D) 73.2 57.6 86.9 48.3 48.2 49.1
30BModelComparison
MM1.5-30B(S) 77.0 64.5 90.2 49.9 48.4 60.1
MM1.5-30B(D) 79.3 64.6 90.6 54.0 50.2 58.2
Table18: Comparisonofourmodelswhenusingdynamicvs. staticimagesplitting. Wefollowour
finalsettingsforallmodels. (S)and(D)indicatestaticanddynamicsplitting,respectively.
{upper_bound}.<n>Answer:”. Forbothreferringandgrounding,weexperimentedwithavarietyof
upperboundboundingboxes. Throughourexperiments,wenoticedthatanupperboxof1yieldedto
betterresults,inlinewiththeanswersproducedbyPhi-3-Vision.ForFlickr30k,weslightlysimplified
thebenchmarkandaskedthemodeltogroundoneentityperprompt,asgroundingmultipleentities
jointlydidnotleadtosatisfactoryresults.
LLaVA-OneVision. We used the public checkpoint LLaVA-OneVision 7B11 and we followed
closelytheLLaVAdocumentation12. Whennotbakeddirectlyintothebenchmarks,weusedthe
original LlaVA prompts specified in [100, 74] for all families of benchmarks. In particular, for
groundingbenchmarks,weusedthepromptintroducedinTable18of[74]: “Providethebounding
boxcoordinateoftheregionthissentencedescribes”. OnFlickr30k,wefollowedthesingle-entity
approachoutlinedabove.
InternVL2. TheInternVL2authorsprovidedalreadyacomprehensivesetofbenchmarksongeneral,
text-rich,knowledgeandrefer&groundfamilies13,whichwereportedfirst. Forthefewremaining
benchmarks, we used the 2B public checkpoint released. InternVL2 code base relies on both
VLMEvalKit[28]andacustominternalevaluation14. Wecarefullyreviewedthelogicimplemented15,
especiallyregardingthedecodingparametersandpromptsused. Forgrounding,weusedtheprompt
shared by the authors: “Please provide the bounding box coordinates of the region this sentence
describes: <ref>{question}</ref>”. OnFlickr30k,wefollowedthesingle-entityapproachoutlined
above.
MiniCPM-V2. WeusedthepubliclyreleasedMiniCPM-V22.8Bcheckpoint16. SimilarlytoIn-
ternVL2,MiniCPM-V2codebasereliesonbothVLMEvalKitandacustominternalimplementation17,
whichwereviewedcarefullytoreproducedecodingparametersandprompts. Forrefer&ground
benchmarks,wenoticedthatregardlessofthepromptused,MiniCPM-V2couldnotproducesatisfac-
11https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-ov
12https://github.com/LLaVA-VL/LLaVA-NeXT/blob/main/docs/LLaVA_OneVision_Tutorials.
ipynb
13https://huggingface.co/OpenGVLab/InternVL2-2B
14https://internvl.readthedocs.io/en/latest/internvl2.0/evaluation.html
15https://github.com/OpenGVLab/InternVL
16https://huggingface.co/openbmb/MiniCPM-V-2
17https://github.com/OpenBMB/MiniCPM-V/tree/main/eval_mm
39toryresultsonRefCOCOandFlickr30k18. Wedecidednottoincludethoseresults. Formulti-image
andin-contextlearningbenchmarks,wefoundthatMiniCPM-V2doesnotacceptmultipleimages
asinputinitsHuggingFaceimplementation. WeacknowledgeMiniCPM-V2’sauthorsusedacustom
imageconcatenatingapproachforBLINK19,butwecouldnotreproduceeasilythesameapproachon
otherbenchmarks. WedecidedtoonlyincludetheBLINKscorereportedbytheauthors20.
18Responsescommonlyproducedincompleteboundingboxes,preventinganyparsingforafairevaluation.
19https://github.com/OpenBMB/MiniCPM-V/blob/a209258d851f404485e5ae25864417dff3bb74ca/
eval_mm/vlmevalkit/vlmeval/api/base.py#L260
20https://huggingface.co/spaces/opencompass/open_vlm_leaderboard
40A.8 QualitativeExamples
A.8.1 Text-RichImageUnderstanding
"
Could you summarize every article on this newspaper
page?
!
The newspaper page is from The New York Times dated
November 11, 1918. The main headline announces the
signing of the Armistice, marking the end of World War
I. Other articles cover the revolution in Germany, with
Berlin being seized by revolutionists and the abdication
of Kaiser Wilhelm II. The page also includes a call for
the abdication of the German Emperor by the socialist
Chancellor, Friedrich Ebert. Additionally, there are
reports on the ousting of Kaiser Wilhelm II and his
flight to the Netherlands, as well as the resignation of
the German General Staff. The page also features a
report on the armistice being signed in France, marking
the end of the war.
"
Could you summarize the articles in this newspaper
page?
!
The newspaper page features several articles. The
main headline reports on the tragic event of 60
people dying in blasts at US embassies. Another
article discusses President Clinton consoling the
victims' families. There's also a piece about
Ojukwu foreclosing on a presidential ambition.
Additionally, the page includes a special sales
advertisement for Samsonite/Carlton luggage and
high-quality crystals.
41"
What percent of growth is expected in the primary
technician jobs in the U.S.?
!
32%
"
How much is the top 10% income earned by medical
assistants in the U.S.?
!
$40,190
"
What is the expected growth rate of physical therapist jobs
in the U.S.?
!
39%
"
Can you give me a table of games and sales numbers, but only show games
released before 2010
!
Sure, here is a table of games released before 2010 and their sales numbers:
Title Sales Series Platform(s) Initial release date
Wii Sports 82,900,000Wii Wii November 19, 2006
Super Mario Bros. 58,000,000Super Mario Multi-platform September 13, 1985
T Ne Etr Sis ) (Game Boy / 48,000,000Tetris Game Boy / NES June 14, 1989
P Go rek eé nm /o Bn luR eed / 47,520,000Pokémon Multi-platform February 27, 1996
Pac-Man 42,000,000Pac-Man Multi-platform May 22, 1980
42A.8.2 VisualReferringandGrounding
"
Show me the guidelines for selecting the Ripe and Sweet melon
!
Uniform Size & Heavy: Pick a watermelon that is uniform in size and feels heavy for its size.
This indicates that it is full of juice.
Larger "Webbing": Choose a watermelon with larger webbing, as it is likely to be sweeter.
Dark & Dull: A ripe watermelon will have a dark and dull exterior, which means it's ready
to be enjoyed.
Field Spot: Look for a watermelon with a field spot, which is a yellow or orange spot on the
bottom where it was resting on the ground. This indicates that it was allowed to ripen on
the vine.
Orange Field Spot: The ideal watermelon will have an orange field spot, which means it was
harvested at the perfect time.
Shiny: A ripe watermelon will not be shiny, so avoid any that are. By following these
guidelines, you can select a ripe and sweet watermelon every time.
"
Based on the guidelines, which one is the ripe and sweet melon? Also show me the
coordinates of the things that you mention.
!
The ripe and sweet melon <114,453,227,595> would be the one with a dark and dull
exterior, an orange field spot, and larger webbing.
"
Which place should I go to if I want to buy lemons?
!
Fruits and Vegetables
"
Ok. Is this the correct place?
!
No, you are in the fresh meat area. The correct place to buy lemons is in the
Vegetables department as shown in the supermarket image.
"
Okay, here is the place. Can you help me find the lemons? Also show me the
coordinates of the things that you mention.
!
Sure! The lemons <236,218,646,448> are located in the middle of the image,
nestled between the tomatoes and the peppers.
43A.8.3 Multi-ImageReasoning
"
In which of these situations can I drive on ahead?
!
In the second image, you can drive on ahead because the traffic light is green.
"
What is the bike experiencing in image 1? Which one fits best, image 2 or image 3, and why?
!
The bike in image 1 is experiencing rust and wear, as indicated by the rusted metal parts and the bent
handlebars. Image 2, with its focus on the bicycle wheel, fits best as it provides a close-up view of the
bike's wheel, which is the part most directly related to the bike's condition.
"
I have these ingredients in my fridge. What can I make using all of these? Explain.
!
You can make a banana bread using all of these ingredients. The ripe bananas from image
3 can be mashed and combined with the flour from image 1 and the eggs from image 4 to
create a moist and flavorful banana bread. The chocolate chips from image 2 can be mixed
into the batter for added texture and sweetness.
44A.8.4 VideoUnderstanding
"
Here are two videos. Please provide the analysis on their differences.
!
The first video and the second video are similar in that they both depict a person
gardening. However, there are some differences between the two videos.
In the first video, the person is seen picking berries from a bush, while in the second video,
the person is seen planting flowers in a garden. This difference in activity suggests that the
two videos may have been taken at different times or in different locations.
Additionally, the first video is taken from a bird's-eye view, while the second video is taken
from a lower angle. This difference in perspective gives the two videos a different feel and
may affect the viewer's experience. Overall, while the two videos share some similarities,
they also have some differences in terms of the activity being performed and the perspective
from which the video is taken.
"
Describe the player highlighted in this video.
!
The highlighted player is a long jumper, who is wearing a green and black uniform and is
participating a long jump competition.
"
Why this player run very fast in the beginning of the video?
!
The player runs very fast at the beginning of the video to gain momentum and speed for
their jump, which will help them achieve a longer distance.
45"
Describe the player highlighted in this video.
!
The player highlighted in this video is a gymnast. He is wearing a blue leotard and is
shown performing on the parallel bars. The video captures his routine, which includes a
variety of swings and movements on the apparatus. The gymnast's form and technique are
on display as he navigates the parallel bars with precision and grace. The video also
provides a close-up view of the gymnast's face, allowing viewers to see the concentration
and determination he exhibits during his performance. Overall, the video showcases the
gymnast's skill and dedication to his sport.
"
Do you think the player and his coach are satisfied with his performance?
!
The player and his coach are embracing each other in a sports hall or gym suggests a
positive outcome or a sense of accomplishment, which could indicate satisfaction with the
player's performance.
"
What was the primary objective that this person was trying to accomplish and what
tool did he use repeatedly to check the accuracy of his work?
!
The primary objective that this person was trying to accomplish was installing or
repairing a cable channel. He used a level repeatedly to check the accuracy of his work.
"
What activities did the man do after he washed the black pan?
!
He cleaned the kitchen counter and the sink with a white towel.
46"
Please identify the most significant actions in the video. What conclusions can be drawn
from these actions?
!
The character is seen lying on the bed, holding a black shirt. They then place the black
shirt on the bed. The character is seen holding a gray shirt and places it on the bed as
well. They then hold a pair of black socks and place them on the bed. The character is
seen holding a pair of red socks and places them on the bed. The character is seen holding
a pair of black shoes and places them on the bed.
"
Please describe the video in details.
!
In the video, a person is engaged in the act of painting a wedding scene on a canvas. The
canvas is propped up on a table, which is cluttered with various art supplies. The person
is holding a paintbrush, dipping it into a palette filled with different shades of paint. They
are carefully applying the paint to the canvas, bringing the wedding scene to life.
The table around the canvas is a testament to the artist's process. It's covered in art
supplies - brushes of different sizes and shapes, jars of paint in various colors, and a
sketchbook filled with preliminary sketches. The sketchbook, in particular, provides a
glimpse into the artist's thought process, with sketches of the wedding scene visible on its
pages.
The background of the video is a wall adorned with shelves. The shelves are lined with
more art supplies, indicating a well-stocked workspace. The overall scene suggests a
dedicated artist immersed in their work, bringing a beautiful wedding scene to life on
canvas.
47A.8.5 UIUnderstanding
$ "
List all UI elements on the screen.
!
“UI widgets present in this screen include Text displaying "Projects"
<16,48,103,81>, "more" Icon <273,48,303,79>, Text displaying "No
Projects" <106,259,204,287>, Text displaying "Start New Project"
<85,352,225,380>, "Magic Movie, Choose media from Photos and let iMovie,
create a movie for you." Button <0,413,310,482>, "Storyboard, Use beautiful
templates to easily create a, video or movie trailer." Button
<0,487,310,558>, "Movie, Make a movie from scratch using media, from
Photos and your camera." Button <0,568,310,637>.”
$ "
What is the type of icon at <257,118,299,153> ?
!
microphone
"
What happens it I click on it?
!
voice search
"
Is there another way to search?
!
Yes, you can also use the search field <10,118,303,155> at the top of the
screen.
48$ "
Please provide a brief description of the screenshot.
!
page displaying various books
"
Where can I find the current best booksellers?
!
You can find the current best booksellers by clicking on the 'Current
Bestsellers' text <20,130,212,160>.
"
Where should I tap to sign in to my account?
!
You can sign in to your account by tapping on the 'person' icon
<255,70,294,111>.
"
What will happen if I tap <66,603,118,650>?
!
Tapping on the 'Library' tab <66,603,118,650> will take you to your
personal library where you can view all the books you have added.
49B AuthorContributionsandAcknowledgements
FirstAuthors
HaotianZhang,MingfeiGao,ZheGan: Ledtheoverallprojectdirection,experimentation,andpaper
writing.
CoreAuthors
PhilippDufter: LedMM1.5dynamichighresolutionmodeling.
NinaWenzel,ForrestHuang: LedMM1.5referringandgrounding.
DhrutiShah: LedMM1.5multi-imagereasoningandin-contextlearning.
XianzhiDu: LedMM1.5MoEexperimentation,underlyingLLMtrainingandhigh-qualitytextdata.
BowenZhang,YanghaoLi,PeterGrasch: Researchdiscussionandrecipevalidation.
FurtherAuthors
SamDodge: Assistedwithpre-trainingandSFTexperimentation,andpaperwriting.
ForrestHuang,KeenYou,ZhenYang: LedMM1.5UIunderstanding.
MingzeXu,AlekseiTimofeev,Hong-YouChen: LedMM1.5videounderstanding.
Jean-PhilippeFauconnier: Ledevaluationtoreproducemetricsandfillmissingbenchmarks.
HaoxuanYou: Assistedwithmultimodalevaluationinfrastructures.
ZhengfengLai: Ledhigh-qualitysyntheticimagecaptionsforcontinualpre-training.
ZiruiWang,AfshinDehghan: Researchdiscussion.
LastAuthor
YinfeiYang: Ledtheoverallprojectdirection,experimentation,andpaperwriting.
Acknowledgements
TheauthorswouldliketothankChenChen,ElmiraAmirloo,ErikDaxberger,JiamingHu,JuanLao
Tebar,LiangchenSong,QibinChen,Tsu-JuiFu,VasileiosSaveris,YusuQian,AlexanderToshev,
JeffreyNichols,RuomingPang,YangZhaoforvaluableguidance,suggestions,andfeedback.
50