VARIATIONAL AUTO-ENCODER BASED SOLUTIONS TO
INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS
APREPRINT
YinghuiPan BiyangMa
NationalEngineeringLaboratoryforBigDataSystemComputingTechnology SchoolofComputerScience
ShenzhenUniversity MinnanNormalUniversity
Shenzhen,China Zhangzhou,China
panyinghui@szu.edu.cn mby@mnnu.edu.cn
HanyiZhang
NationalEngineeringLaboratoryforBigDataSystemComputingTechnology
ShenzhenUniversity
Shenzhen,China
YifengZeng
DepartmentofComputerandInformationSciences
NorthumbriaUniversity,Newcastle,UK
yifeng.zeng@northumbria.ac.uk
October1,2024
ABSTRACT
Addressing multiagent decision problems in AI, especially those involving collaborative or com-
petitive agents acting concurrently in a partially observable and stochastic environment, remains
a formidable challenge. While Interactive Dynamic Influence Diagrams (I-DIDs) have offered a
promisingdecisionframeworkforsuchproblems,theyencounterlimitationswhenthesubjectagent
encountersunknownbehaviorsexhibitedbyotheragentsthatarenotexplicitlymodeledwithinthe
I-DID.Thiscanleadtosub-optimalresponsesfromthesubjectagent. Inthispaper,weproposea
noveldata-drivenapproachthatutilizesanencoder-decoderarchitecture,particularlyavariational
autoencoder,toenhanceI-DIDsolutions. Byintegratingaperplexity-basedtreelossfunctioninto
theoptimizationalgorithmofthevariationalautoencoder,coupledwiththeadvantagesofZig-Zag
One-Hotencodinganddecoding,wegeneratepotentialbehaviorsofotheragentswithintheI-DID
thataremorelikelytocontaintheirtruebehaviors,evenfromlimitedinteractions. Thisnewapproach
enablesthesubjectagenttorespondmoreappropriatelytounknownbehaviors,thusimprovingits
decisionquality. Weempiricallydemonstratetheeffectivenessoftheproposedapproachintwowell-
establishedproblemdomains,highlightingitspotentialforhandlingmulti-agentdecisionproblems
withunknownbehaviors. Thisworkisthefirsttimeofusingneuralnetworksbasedapproachesto
dealwiththeI-DIDchallengeinagentplanningandlearningproblems.
Keywords Decision-making Multi-agentSysterm VariationalAuto-encoder
· ·
1 Introduction
Interactionsbetweenintelligentagentsoperatinginasharedanduncertainenvironmentamplifytheoverallsystem’s
uncertainty,posingformidablechallengesforefficientmodelinganddecision-makinginmulti-agentsystems. Conse-
quently,theactionsoftheseagentsmutuallyinfluenceeachother,necessitatingthecomprehensiveconsiderationof
bothenvironmentaldynamicsandpotentialactionsequencesofotheragentsinordertomakeoptimaldecisions[39].
Thisissueiscommonlyreferredtoasmulti-agentsequentialdecisionmakingunderuncertainty[28].
4202
peS
03
]AM.sc[
1v56991.9042:viXraVariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
Overtheyears,variousdecisionmodelshavebeenproposedtotacklemulti-agentsequentialdecisionmaking(MSDM)
problems,includingdecentralizedpartiallyobservableMarkovdecisionprocesses(POMDPs)[36],interactivePOMDPs
[35],andinteractivedynamicinfluencediagrams(I-DIDs)[34]. Amongthese,I-DIDsstandoutfortheirabilityto
modelbothcollaborativeandcompetitiveagents,aswellastheircomputationaladvantages[32,33]. Furthermore,their
probabilisticgraphicalmodelstructuremakestheminherentlyexplainableinreasoningaboutthebehaviorsofother
agents.
TosolveI-DIDs,knowledge-basedapproacheshavebeenexploredtoprovidethesubjectagentwithmoreinformation
aboutotheragents[25,26,24]. However,theseapproachesmaybelimitedbyenvironmentalcomplexity,incomplete
data,anduncertainty,resultingininaccurateorlimitedinformation. Oneapproachtoaddressthisisbyincreasing
the number of candidate models for other agents, but this can lead to a prohibitively large model set, increasing
computationalcomplexity[37,29]. Asanalternative,data-drivenmethodshavebeenproposedthatleveragehistorical
behavioraldataofotheragentstolearntheirpotentialdecisionmodels,enhancingtheadaptabilityandinterpret-ability
ofthedecisionmodels[7].
However,data-drivendecisionmodelingconfrontstwosignificantchallenges: scarcityorconstraintsindataavailability
andinadequacyorbiasindataquality[43].Limitedorbiasedhistoricaldataoriginatingfromotheragentsfrequentlyfails
toaccuratelycapturetheirgenuinemodels.Moreover,inadequateorrestricteddatasamplinghindersthedevelopmentof
comprehensiveintentmodels,ultimatelyleadingtoalossofinformationfromtheoriginalhistoricaldecisionsequences.
Consequently,thesefactorscancontributetosub-optimalornon-generalizabledecisionmodelsforthesubjectagent.
Inthispaper,weproposeanoveldata-drivenframeworktolearnthetruepolicymodelofotheragentsfromlimited
historicalinteractiondata. Weextracthistoricalbehaviorcharacteristicstolearnanewsetofincompletebehavior
models from interaction sequences. To enhance modeling accuracy, we adapt the Variational Autoencoder (VAE)
[44,18]togenerateacollectionofmodelsthatcanencompassthetruemodelsofotheragentsfromanincomplete
setofpolicytrees. Thisnewapproachleveragesinformationfromincompletepolicytreesthatareoftendiscardedor
approximatedbytraditionalmethods. Wedevelopaperplexity-basedmetrictoquantifythelikelihoodofincludingtrue
behaviormodelsandselectoptimizedtop-K behaviorsfromthelargecandidateset. Ourcontributionsaresummarized
asfollows:
• WeproposetheVAE-basedalgorithmtoaddresschallengesindata-drivenI-DIDs,resultinginVAE-enabled
behaviors. ThispavesthewayforfutureresearchonneuralnetworksinMSDMproblems.
• WedeveloptheZig-ZagOne-Hot(ZZOH)encodinganddecodingtechnique,tailoredspecificallyforpolicy
trees,whichempowerstheVAEtoefficientlyhandlebothcompleteandincompletepolicytreesasinputdata,
thusenhancingitsversatility.
• We analyze the quality of I-DID solutions using a novel perplexity-based metric, providing insights for
algorithmfine-tuningandconfidenceintheperformance.
• Weconductempiricalcomparisonstostate-of-the-artI-DIDsolutionsintwoproblemdomainsandinvestigate
thepotentialofthenovelalgorithms.
Theremainderofthispaperisorganizedasfollows. Section2reviewsrelatedworksonsolvingI-DIDs. Section3
providesbackgroundknowledgeonI-DIDsandVAE.Section4presentsourapproachtogeneratingbehaviorsofother
agentsinI-DIDs. Section5showstheexperimentalresultsbycomparingvariousI-DIDsolutions. Finally,weconclude
theresearchanddiscussfutureworkinSection6.
2 RelatedWorks
Thissectionreviewsthecurrentresearchandemergingtrendsinthreekeyareas: neuralcomputing-baseddecision-
makingmodeling,modelingofunknownagentbehaviors,andI-DIDmodels,alongwithrelateddata-drivendecision-
makingapproaches.
Neuralcomputingtechniques,particularlydeepreinforcementlearning(DRL),haveattainedsignificantprogressin
addressingmulti-agentdecision-makingchallenges[42,14,15].DRL,whichcombinesdeeplearningandreinforcement
learning,facilitatesend-to-endlearningcontrolbutencounterslimitationsinsparserewards,limitedsamples,andmulti-
agentenvironments. Recentadvancementsinhierarchical,multi-agent,andimitationlearning,alongwithmaximum
entropy-basedmethods,offerpromisingresearchdirections[17]. InvestigationssuchasKononovandMaslennikov’s
recurrentneuralnetworkstrainedthroughreinforcementlearning[16]andZhangetal.’smethodforgeneratingnatural
languageexplanationsforintelligentagents’behavior[11],demonstratethepotentialofDRL.Additionally,MO-MIX
andotherDRLapproachesenablemulti-agentcooperationacrossdiversedomains[19,20,21,22,40].
2VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
To emulate human-level intelligence and handle unexpected agent behaviors, neuro-symbolic multi-agent systems
areemerging. Thesesystemsleverageneuralnetworks’abilitytoextractsymbolicfeaturesfromrawdata,combined
with sophisticated symbolic reasoning mechanisms. Techniques like agent-based models (ABMs) [13, 12], data-
drivendecision-making[4,3], andlargelanguagemodels(LLMs)[10]arebeingexplored. Anetal.[5]proposed
areinforcementlearningandCNN-basedapproachforagentstoself-learnbehaviorrulesfromdata,promotingthe
integrationofABMswithdatascienceandAI.Wangetal.[41]presentedoptimaldecision-makingandpathplanning
formulti-agentsystemsincomplexenvironmentsusingmean-fieldmodelingandreinforcementlearningforunmanned
aerialvehicles. Wasonetal.[9]andNascimentoetal.[8]exploredintegratinglargelanguagemodelsintomulti-agent
systems,highlightingtheirhuman-likecapabilitiesbutalsotheirdistinctiveness,agentautonomy,anddecision-making
indynamicenvironments.
Similarly, several I-DID solutions have relied solely on behavioral and value equivalences to constrain the model
space for other agents, presuming that the true behaviors of those agents fall within the subject agent’s modeling
capabilities[32,33]. However,thisapproachfallsshortinfullyaccountingforunexpectedornovelbehaviors. Pan
etal.[6]proposedageneticalgorithm-basedframeworkthatincorporatesrandomnessintoopponentmodeling,thus
generatingnovelbehaviorsforagents,therebydemonstratingthesignificantpotentialofevolutionaryapproaches.
In the context of data-driven I-DIDs [7], the objective is to optimize multi-agent decision-making in uncertain
environments,asignificantchallengeinAIresearch. Ourworkinthisarticlepioneersanovelapproachtomodeling
otheragentsinI-DIDs,exploringneuralcomputing-baseddata-drivenmethodstoapproximaterealagentbehavior
modelsfromhistoricaldatainagentplanningresearch.
3 PreliminaryKnowledge
AswewilladapttheVAE-baseddatagenerationmethodstoaugmenttheI-DIDmodelwithnovelmetricsforother
agentsandgeneratenovelbehaviors,weprovidebackgroundknowledgeonI-DIDandVAE.
3.1 BackgroundknowledgeonI-DIDs
Thetraditionalinfluencediagram,designedforsingle-agentdecisions,hasevolvedintotheI-DIDframework. This
frameworkisaprobabilisticgraphtailoredforinteractivemulti-agentdecision-makingunderpartialobservability. From
theagenti’sperspective,theI-DIDpredictsagentj’sactions,aidingagentiinoptimizingitsdecisions. Wefocuson
agentiandintegrateagentj’spotentialbehaviorsintothedecisionframework.
Weintroducethedynamicinfluencediagram(DID)forasingleagentinFig.1(a). ThisDIDrepresentstheagent’s
decisionprocessoverthreetimesteps,withsolutionsshowninFig.1(b). IntheDID,ovalsrepresenteitherchance
nodes for environmental states (S) or observations (O); rectangles are decision nodes for the agent’s actions (A);
diamondsdenoteutilitynodescapturingrewards(R). Attimet,theagent’sdecision(At)isinfluencedbythecurrent
observation(Ot)andthepreviousdecision(At−1). Theobservation(Ot)dependsonthepreviousdecision(At−1)and
thecurrentstate(St). Thestates(St)arepartiallyobservableandinfluencedbythepreviousstates(St−1)anddecision
(At−1). Rewards(Rt−1)aredeterminedbyautilityfunctionconsideringboththestatesanddecisions. Arcsmodel
conditionalprobabilitiesamongtheconnectednodes. Forexample,ifOtisinfluencedbyAt−1andSt,thearrowsfrom
At−1andSttoOtindicatethisdependency. Similarly,ifStisaffectedbyAt−1andtransitionsfromSt−1,thearrows
fromAt−1andSt−1toStshowthestatetransition. TheDIDdemonstrateshowtheagentoptimizesitsdecisionsin
theresponsetothechangingenvironment. Afterdefiningthetransition, observation, andutilityfunctions, weuse
traditionalinferencealgorithmstosolvethemodelandgettheagent’soptimalpolicy. Thispolicyisrepresentedbya
policytree(seeFig.1(b)). Attimet=1,theagenttakesactiona andthesubsequentactionsbasedonobservations.
1
Thepathsinthetreecorrespondtotheobservations,whilenodesrepresenttheagent’schosenactions. Indifferent
domains,thenumberofthepathsandnodetypesmayvary. Thepolicytree,asaDIDsolution,encapsulatesthemodel’s
optimalpolicy(behavioralmodel). It’safullk-arytree,wherekisthenumberofpossibleobservations.
IntheI-DIDmodel, ahexagonalnode, knownasthemodelnodeM , dynamicallyextendsthebehavioralmodels
j
ofotheragentsintheinfluencediagram. Thisnodecontainspotentialbehavioralmodelsofagentj,whicharethen
providedtothesubjectagenti,transformingthecomplexI-DIDproblemintoaconventionalDID.However,asthe
numberoftimeslicesincreases,managinganextensivesetofcandidatemodelsbecomesintractable. Compression
andpruningofmodelnodesareoftennecessarybeforeintegratingthemintotheDID,butthiscanresultinnodeswith
similarbehavioralmodels,leadingtoalimitedsetofoptimaldecisionsforagenti.
Inthisarticle,toaddressthechallengesposedbyalargenumberoftimeslicesandselectingvastcandidatemodels,
weuseavariationalautoencoder(VAE)tolearnfromagentj’shistoricaltrajectory. Thenewmethodgeneratesaset
3VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
𝒕=𝟏 𝒕=𝟐 𝒕=𝟑 𝒕=𝟏
𝑹 𝑺𝟏,𝑨𝟏 𝑹𝟏 𝑹 𝑺𝟐,𝑨𝟐 𝑹𝟐 𝑹 𝑺𝟑,𝑨𝟑 𝑹𝟑 𝒂
𝟏
𝒐 𝟏 𝒐 𝟐
𝑨𝟏 𝑨𝟐 𝑨𝟑
𝒕=𝟐
𝒂 𝒂
𝟑 𝟏
𝒐 𝒐 𝒐 𝒐
𝑺𝟏 𝑺𝟐 𝑺𝟑 𝟏 𝟐 𝟏 𝟐
𝑷𝒓 𝑺𝟏 Pr 𝑺𝟐𝑺𝟏,𝑨𝟏 Pr 𝑺𝟑𝑺𝟐,𝑨𝟐
Pr 𝑶𝟏𝑺𝟏 Pr 𝑶𝟐𝑺𝟐,𝑨𝟏 Pr 𝑶𝟑𝑺𝟑,𝑨𝟐 𝒂 𝟐 𝒂 𝟑 𝒂 𝟏
𝑶𝟏 𝑶𝟐 𝑶𝟑
𝒕=𝟑
𝒂 𝑏
Figure1: Adynamicinfluencediagramanditssolutions:(a)theleftisthedynamicinfluencediagramwiththreetime
stepsand(b)therightisitssolutionrepresentedasapolicytree. Theblockswiththesamecolorin(a)and(b)belongto
thesametimeslice.
𝑅(cid:3047)(cid:2879)(cid:2869) 𝑅(cid:3047) 𝑅(cid:3047)(cid:2878)(cid:2869)
(cid:3036) (cid:3036) (cid:3036)
𝐴(cid:3047)(cid:2879)(cid:2869) 𝐴(cid:3047) 𝐴(cid:3047)(cid:2878)(cid:2869)
(cid:3036) (cid:3036) (cid:3036)
𝑆(cid:3047)(cid:2879)(cid:2869) 𝑆(cid:3047) 𝑆(cid:3047)(cid:2878)(cid:2869)
𝑂(cid:3047)(cid:2879)(cid:2869) 𝑂(cid:3047) 𝑂(cid:3047)(cid:2878)(cid:2869)
(cid:3036) (cid:3036) (cid:3036)
𝐴(cid:3047)(cid:2879)(cid:2869) 𝐴(cid:3047) 𝐴(cid:3047)(cid:2878)(cid:2869)
(cid:3037) (cid:3037) (cid:3037)
Figure2: ByextendingDIDmodel(thebluepart),theagentioptimisesitsdecisionintheI-DIDmodelswiththeblue
partwhichmodelsagentj’sdecisionmakingprocess.
ofhighlyreliablepolicytrees,capturingkeyfeaturesofthetrajectory. Byembeddingthepotentiallytruebehavioral
modelofagentj intotheI-DID,weenableagenti’soptimalpolicytoconsiderthemostprobablebehaviorsofagentj.
3.2 BackgroundknowledgeonVAE
VariationalAutoencoders(VAEs)[31,38]areatypeofunsupervisedlearningmodelthatcombinesthecapabilitiesof
autoencoderswithprobabilisticgenerativemodels. Unliketraditionalautoencoders[30],whichencodeinputsintoa
fixedlatentrepresentation,VAEsmaptheinputstoaprobabilitydistributioninthelatentspace. Thekeyideabehind
VAEsistoencodeinputdataintotheparametersofalatentdistribution,typicallyaGaussiandistribution. Theencoder
networkpredictsthemean(µ )andvariance(σ2)ofthisdistribution, whilethedecodernetworkreconstructsthe
ϕ ϕ
originalinputfromlatentsamples. Toenabledifferentiablesamplingfromthelatentdistribution,VAEsemploythe
reparameterization trick. This involves sampling random noise from a standard normal distribution and scaling it
accordingtothepredictedmeanandvariancetoobtainalatentsample(z Rl).
∈
TheVAE’sobjectivefunctionconsistsoftwoparts:areconstructionlossthatmeasuresthesimilaritybetweentheoriginal
inputandthereconstructedoutput,andaKullback–Leibler(KL)divergencethatregularizesthelatentdistributiontobe
closetoapriordistribution(e.g.,astandardnormaldistribution). Optimizingthisobjectivefunctionleadstoencoder
anddecodernetworksthatcanencodemeaningfulrepresentationsoftheinputdataintothelatentspace. Formally,fora
giveninputxfromdataset ,theVAElossfunctionisdefinedas:
D
vae(x)= KL(P (z x),P (z))+ recon(x) (1)
Lϕ,θ Lϕ,θ ϕ | θ Lϕ,θ
where KL istheKLdivergencebetweentheposteriordistributionP (z x)andthepriordistributionP (z),and recon
Lϕ,θ ϕ | θ Lϕ,θ
isthereconstructionloss. Overtheentiredataset ,theVAElossfunctionisgivenby:
D
(cid:88)
vae( )= vae(x) (2)
Lϕ,θ D Lϕ,θ
x∈D
Byminimizingthelossfunctionthroughthetechniqueslikeastochasticgradientdescent,theVAElearnstoencode
inputsintoalatentspacethatcapturestheiressentialfeatureswhileenablingthegenerationofnewdatasamplesthat
followthedistributionoftheoriginalinputdata,wherewecanapplystochasticgradientdescent(SGD)[38]tofindthe
optimizedparametersϕ∗andθ∗.
4VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
4 PolicyTreeGenerationBasedonVAE
Policytreegeneration,oftenreferredtoaslearningagents’behavioralmodels,involvestheautomatedconstructionofa
behavioralrepresentationintheformofapolicytreefromextensivemulti-agentinteractiondata. Thebehavioralmodel
aimstopredicttheactionsanagentwilltakegivenobservationsfromtheenvironment,whichistypicallyrepresentedas
acompletemulti-forktree,asdepictedinFig.1.
Fromtheperspectiveofagenti,itisnotnecessarytounderstandhowagentjoptimizesitsbehaviorbutrathertopredict
howagentjwillact. IntheI-DIDmodel,agentiisprovidedwithmpolicytreesrepresentingagentj’sbehavior,where
thesetreesshallbecomplete. However,twochallengesarise:
• Manypolicytreesgenerateddirectlyfromhistoricalbehaviorsequencesarenotfullydeveloped-theyare
incomplete.
• Thegeneratedpolicytreesmaynotaccuratelyreflectthetruebehavioralmodelofagentj.
Addressing these issues is crucial for the effective utilization of the policy trees in predicting and understanding
multi-agentinteractions. Toaddressthefirstissue,therearetwotraditionalmethods,butbothhavetheirlimitations[2].
Discardingpolicytreeswithmissingnodesisproblematicbecauseitrisksexcludingsignificantpolicysequencesthat
maycontainvaluableinformationfromtheoriginalhistoricaldata. Thisapproachisonlyfeasiblewhendataisabundant,
butinmostcases,dataislimited. Thealternativemethod,basedonstatisticalanalysis,estimatestheprobabilityofeach
actioninthecurrenttimesliceusingtheactionsfromtheprevioustimesliceandthecurrentobservations. Then,a
roulettewheelselectionprocessisemployedtorandomlyfillinthemissingactionsinthepolicytree. However,this
approachhasitsshortcomingsaswell. Theactionsatdifferentlevelsinthepolicytreerepresentdistinctcontexts. For
instance,theactiona4attimeslicet=4isinfluencednotonlybytheactiona3anditscorrespondingobservation,but
alsobytheactionsandobservationsatearliertimeslicessuchast=1andt=2. Consequently,thehighconfidencein
takingactiona2att=2doesnotnecessarilytranslatetothehighprobabilityoftakingthesameactionatt=4.
Totacklethesecondissueofpotentialincompletenessinmodelingagentj’sbehaviors,weendeavortogatherabroader
andmorediversecollectionofbehavioralmodels. Thegreaterthediversitywithinthisset,thehighertheprobability
thatitwillcapturethefullspectrumofagentj’sactualbehaviorpatterns. Thisapproachensuresthatourmodelsare
comprehensiveandrepresentative,increasingtheirreliabilityandadaptability.
Tothatend,wedevelopanewframeworkcapableofgeneratingacomprehensiveensembleofpolicytreesfromagents’
interactiondata. Thisframeworknotonlyreconstructspolicytreesaccuratelybutalsoguaranteesthediversitywithin
theresultingset. Totacklethesechallenges,wedevelopapolicygenerationmethodleveragingvariationalautoencoder
techniques.
4.1 ReconstructinganIncompletePolicyTree
Giventhelimitationthatagentsoftencannotstorealargenumberofpolicytreestonavigatecomplexmulti-agent
interactiveenvironments,theagentmustrepeatedlytraversethepolicyfromtherootdownduringinteractions. Ina
simpleterm,fromthesubjectagent’sperspective,wecanconstructpotentiallyincompletepolicytreesforotheragents
solelybasedonasinglesequenceofinteractiondata.
To obtain a long sequence of agent j’s action-observations, we control the interaction between agent i and the
environment. ThissequenceisdenotedashL =(a1o1,a2o2,...,atot,...,aLoL),whereat Aandot Ωrepresent
∈ ∈
analternatingseriesofactionsandobservationsattimeslicet. TheparameterListhetotalnumberoftimeslicesin
thisinteraction.
GiventheinteractiondatahL,thedepthofthepolicytree(denotedasT),andthenumberofagentj’smodels(denoted
asm),wecanreconstructasetofpossibleincompletepolicytrees(cid:83) T ofthefixeddepthT (T L)usingfour
H ≪
operators: split,union,roulette,andgraphing. AsillustratedinFig.3,theseoperatorsenableustogeneratethe
desiredsetofthepolicytrees,andtheimplementationoftheseoperatorsisdescribedinAlg.3oftheAppendix.
Utilizingthesplitoperator(denotedas inFig.3 1),webeginatthestartingpointandextractsequencesofactions
S ⃝
withafixedlengthT,therebyformingpolicypathsthatcommencewithaspecificactiona. Werefertosuchapathas
hT. Ifthispolicypathisalignedwithasequenceofobservationso,itcanbedenotedashT . Concurrently,wecompute
a ao
theprobabilityofeachpolicypath,denotedasP(hT) #(hT) T/L. Here,#(hT)representingthenumberoftimes
hT appearsintheentiresequencehL. Werepeattha is← operatioa nu· ntiltheendofthea sequence,ultimatelyformingaset
a
ofpolicypathsHT denotedasHT (hL).
T
←S
ToobtainasetofpolicytreesfromthesetofpolicypathsHT,weusetheunionoperator(denotedas inFig.3
2). First, we define a subset of policy paths that begin with the same action a and observations oU as HT =
⃝ ao
5VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
(cid:83) (hT ,P(hT )). Thissubsetcontainsallthepolicypathsthatstartwiththesameactionaandfollowa
(hT,P(hT))∈HT ao ao
ao ao
specificsequenceofobservationso. Wethencomputetheprobabilityofthissubsetbysumminguptheprobabilities
ofallthepolicypathswithinit: P(HT) = (cid:80) P(hT ). AftercollectingallthesubsetsHT forevery
ao (hT,P(hT))∈HT ao ao
ao ao
possiblesequenceoftheobservationso ΩT−1followingactiona,weconstructalargersetHTthatcontainsallthe
policypathsbeginningwithactiona. Th∈ issetisdefinedasHT =(cid:83) (HT,P(HT))andia tsoverallprobability
a o∈ΩT−1 ao ao
iscomputedasP(HT) = (cid:80) P(HT). Finally, foreveryactiona A, wecombinethesetsHT toformthe
completesetofpolica ypathsHo T∈Ω ,wT− h1 ichisa do enotedasHT =(cid:83) (HT,P(H∈ T)). Thissetrepresentsallpa ossiblepolicy
a∈A a a
treesofdepthT thatcanbeconstructedfromtheoriginalinteractionsequencehL.
Togenerateasetofincompletepolicytrees T containingmsamples,wefirstapplytherouletteoperator(denotedas
,Fig.3 3)onthesetofpolicytreesetsHDT. ThisrandomselectionprocessyieldsasinglepolicytreesetHTfora
R particular⃝ actiona,representedasHT HT. Next,foreachpossiblesequenceofobservationso ΩT−1fola lowing
a ←R ∈
actiona,weapplytherouletteoperatoragaintoselectasubsetofpolicypathsfromthecorrespondingsubsetHT .This
ao
resultsinacollectionofsubsets,whichisapossiblepolicytree(denotedas T),definedas T (cid:83) HT .
Ha Ha ← o∈ΩT−1R ao
Byrepeatedlyapplyingtherouletteoperatorinthismanner,wegeneratemincompletepolicytreecollections, T .
m
Thesetisdenotedas T andrepresentsadiversesetofpartiallyconstructedpolicytrees,eachofwhichstart{ sH wit} ha
D
randomlyselectedactionaandcontainsrandomlysampledpolicypathsforeachpossiblesequenceofobservations.
When applying the graphing operator (denoted as , Fig. 3 4) to the set of policy paths T for the purpose of
visualizingthecorrespondingpolicytree,werepresentG thistrans⃝ formationas T T. NotH ea that,althoughweuse
thesamenotation T torefertoboththepolicytreeandthesetofpolicypatH hsa ro← oteG dH ata awithalengthoftime-slice
Ha
T, thevisualizationprocessactuallycreatesarepresentationofthepolicytreeinagraphicalform. Toclarifythis
distinction,wecanexplicitlystatethat T representsboththeabstractpolicytreestructureandtheconcretesetof
policypaths,while T representsthesH ama epolicytreebutinagraphicalrepresentationgeneratedbythegraphing
GHa
operator .
G
AsdepictedinFig.3,theincompletepolicytree T isreconstructedfromthehistoricalbehaviorsequencehLthrough
Ha1
the application of the four operators: split, union, roulette, and graphing (Fig. 3 1-4). Additionally, Fig. 1
⃝ ⃝
illustratesascenariowhere,afteragentj observeso inthefirsttimesliceandtakesactiona ,itisunabletoselect
2 1
asubsequentactionifitencounterstheobservationo . Notably,someincompletepolicytreesareinevitablycreated
1
duringthegenerationofagentj’spolicytreesfromhistoricaldata. Theseincompletepolicytrees,thoughaspecial
typeofbehaviormodel,differfromcompletepolicytreesinthattheylackspecifiedactionsforcertainobservations
incertaintimeslices. However,theI-DIDmodelrequiresacompletepolicytreemodelforitssolutionmethodology.
Therefore,anincompletepolicytreecannotbedirectlyutilizedinI-DID.Thus,weproposeaVAE-basedmethodto
convertincompletepolicytreesintotheircompletecounterparts,therebyenablingtheirutilizationwithintheI-DID.
4.2 StandardPolicyTreeGeneralization
Asmentionedearlier, discardingorrandomlyprocessingincompletepolicytreescanresultinasignificantlossof
crucialinformationfromtheoriginalhistoricaldecisionsequences. Toaddressthis,wedevelopaVAE-basedapproach
forgeneratingpolicytrees. Thismethodeffectivelyleveragesbothcompleteandincompletepolicytrees,enablingthe
productionofnumerousnewpolicytreesfromjustafewexamples. Bydoingso,itmaximizesthecoverageofagentj’s
truebehaviors.
Inthismanner,ourapproachsolvestwokeyproblemsfacedbytraditionalmethodsinmodelingagentj’sbehaviors.
AsillustratedinFig.4, aftertrainingtheVAEwithaselectionofincompletepolicytrees, themodeliscapableof
generatingpolicytreeswithvaryingdegreesofdeviation,adheringcloselytothedistributionofthehistoricaldata. This
approachsignificantlyenhancesthediversityoftheoverallpolicytreecollection,leadingtoamorecomprehensive
representationofagentj’sbehaviors.
Here,weintroducetheZig-ZagOne-Hot(ZZOH)encodinganddecodingtechniquespecificallydesignedforpolicy
trees. ThistechniqueenablestheVAEtoeffectivelyhandleboththecompleteandincompletepolicytreesasinput
andoutputdata. SinceZZOHencodingprioritizesserializingthepolicytreestructure,nodesclosertotheroothavea
greaterimpactontheoveralldecision-makingeffectivenessoftheentirepolicytree.
Therefore,weadaptthelossfunctionoftheVAEnetworktoprioritizelearningbehaviorsfromnodesassociatedwith
earliertimeslices. Thisapproachhelpscapturethestructuralinformationembeddedintheoriginalpolicytree. After
reconstructingandfilteringalargenumberofpolicytreesusingtheVAE,weutilizetheMeasurementofdiversitywith
frames(MDF)andinformationconfusiondegree(IDF)toselectthetop-K policytreesfromthegeneratedset.
6VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
ℎ(cid:3013):𝑎(cid:2869)𝑜(cid:2869),𝑎(cid:2871)𝑜(cid:2869),𝑎(cid:2871)𝑜(cid:2870),𝑎(cid:2870)𝑜(cid:2869),𝑎(cid:2871)𝑜(cid:2869),𝑎(cid:2871)𝑜(cid:2870),⋯,𝑎(cid:2871)𝑜(cid:2869),𝑎(cid:2871)𝑜(cid:2869),𝑎(cid:2869)𝑜
(cid:2869)
1 split𝒮
𝐻(cid:3021)←𝒮(cid:3021)ℎ(cid:3013): (cid:4666)𝑎(cid:2869)𝑜(cid:2869),𝑎(cid:2871)𝑜(cid:2869),𝑎(cid:2870)𝑜(cid:2870),0.03(cid:4667) (cid:4666)𝑎(cid:2870)𝑜(cid:2869),𝑎(cid:2871)𝑜(cid:2869),𝑎(cid:2871)𝑜(cid:2870),0.03(cid:4667) ⋯(cid:4667)
2 Union 𝒰
H(cid:3021)←𝒰𝐻(cid:3021) 𝐻(cid:3021): (cid:4666)𝑎(cid:2869)𝑜(cid:2869),𝑎(cid:2871)𝑜(cid:2869),𝑎(cid:2870)𝑜(cid:2870),0.03(cid:4667) ⋯ ARoule 𝑎tte ←ℛ
ℛA
𝑎𝒐←𝑎(cid:2869)𝑜(cid:2869)𝑜(cid:2869) 𝑎0.2
𝐻(cid:3028)(cid:3021) 𝒐← (cid:3032)(cid:3276)𝒐←(cid:3035)(cid:3276)(cid:3269)𝒐(cid:4651)
,ℙ(cid:3035)(cid:3276)(cid:3269)𝒐
∈(cid:3009)(cid:3269)𝑒(cid:3028)𝒐 𝑜 𝑜(cid:2869) (cid:2869)𝑜 𝑜(cid:2869) (cid:2869): :𝑎 𝑎(cid:2869) (cid:2869)𝑎 𝑎(cid:2871) (cid:2871)𝑎 𝑎(cid:2870)
(cid:2871)
0 0. .0 03
2
𝑏 𝑐0 0. .2 10 𝑎.2
𝑎
𝑏ℛ
𝑐
𝑎
H𝒐 (cid:3028)(cid:3021)∈ ←Ω(cid:3021) (cid:4651)(cid:2879)(cid:2869) 𝐻ℙ (cid:3028)(cid:3021)𝐻 𝒐(cid:3028) ,(cid:3021) ℙ𝒐 ← 𝐻(cid:3028)(cid:3021)(cid:3533)
𝒐
(cid:3035)(cid:3276)(cid:3269)𝒐,ℙ(cid:3035)(cid:3276)(cid:3269)𝒐 ∈(cid:3009)(cid:3276)(cid:3269)𝒐ℙℎ(cid:3028)(cid:3021) 𝒐
𝐻𝐻(cid:3028) (cid:3028)(cid:3021) (cid:3021)(cid:3117) (cid:3117)(cid:3042) (cid:3042)(cid:3117) (cid:3117)(cid:3042) (cid:3042)(cid:3117)
(cid:3118)
0 0. .0 05
1
ℋ(cid:3028)(cid:3021)0 (cid:3117).4
←
𝑎0 (cid:2869). 𝒢8 ℋ1
(cid:3028)(cid:3021) (cid:3117)
𝒐∈(cid:2960)(cid:3269)(cid:3127)(cid:3117) 𝐻(cid:3028)(cid:3021) (cid:3117)(cid:3042)(cid:3118)(cid:3042)(cid:3118) 0.04 𝑜(cid:2869) 𝑜(cid:2870)
𝑎 H∈ (cid:3021)𝐴
←(cid:4651)
Hℙ (cid:3028)(cid:3021),H ℙ(cid:3028)(cid:3021) H← (cid:3028)(cid:3021)(cid:3533) (cid:3009)(cid:3276)(cid:3269)𝒐,ℙ(cid:3009)(cid:3276)(cid:3269)𝒐 ∈(cid:2892)(cid:3276)(cid:3269)ℙ𝐻(cid:3028)(cid:3021) 𝒐 HH(cid:3028) (cid:3028)(cid:3021)
(cid:3021)(cid:3117)
(cid:3118)
0 0. .5
3
𝑜(cid:2869)𝑎(cid:2871) 𝑜(cid:2870)𝑜(cid:2869)𝑎(cid:2869)
𝑜(cid:2870)
(cid:3028)∈(cid:3002) H(cid:3028)(cid:3021)
(cid:3119)
0.2 𝑎(cid:2870) 𝑎(cid:2871) 𝑎(cid:2869)
3 Rouletteℛ 4 graphing 𝒢
H HH(cid:3028) (cid:3028) (cid:3028)(cid:3021) (cid:3021) (cid:3021)(cid:3117) (cid:3118)
(cid:3119)
0 0 0. . .5 3
2
0 ℛ.2 𝐻 𝐻𝐻(cid:3028) (cid:3028) (cid:3028)(cid:3021) (cid:3021) (cid:3021)(cid:3117) (cid:3117) (cid:3117)(cid:3042) (cid:3042) (cid:3042)(cid:3117) (cid:3117) (cid:3118)(cid:3042) (cid:3042) (cid:3042)(cid:3117) (cid:3118)
(cid:3118)
0 0 0. . .0 0 05 1
4
𝑜 𝑜 𝑜 𝑜(cid:2869) (cid:2869) (cid:2869) (cid:2870)𝑜 𝑜 𝑜 𝑜(cid:2869) (cid:2869) (cid:2870) (cid:2870): : : :𝑎 𝑎 𝑎 𝑎(cid:2869) (cid:2869) (cid:2869) (cid:2869)𝑎 𝑎 𝑎 𝑎(cid:2871) (cid:2871) (cid:2871)
(cid:2869)
⋮𝑎 𝑎 𝑎 𝑎(cid:2870) (cid:2871) (cid:2871)
(cid:2869)
0 0 0 0. . . .0 0 0 03 2 1 2𝒐∈ 0 0ℛ. .4 1Ω 0(cid:3021) .3(cid:2879) (cid:2869) 𝑎𝑎 𝑎(cid:2869) (cid:2869) (cid:2869)→ →(cid:3042) (cid:3042) →(cid:3042)(cid:3117) (cid:3117) (cid:3118)𝑎 𝑎 𝑎(cid:2871) (cid:2871) (cid:2869)→ → →(cid:3042) (cid:3042) (cid:3042)(cid:3117) (cid:3118) (cid:3118)𝑎 𝑎 𝑎(cid:2869)(cid:2870) (cid:2871)
H(cid:3021) H(cid:3028)(cid:3021) (cid:3117)←ℛH(cid:3021) 𝒎𝐻(cid:3028)(cid:3021) (cid:3117)𝒐 ℋ(cid:3028)(cid:3021) (cid:3117)←∪ 𝒐∈(cid:2960)(cid:3269)(cid:3127)(cid:3117)ℛ𝐻(cid:3028)(cid:3021) (cid:3117)𝒐
𝒟(cid:3021) (cid:3404) ℋ(cid:3021) (cid:3040)
Figure3: Reconstructingmincompletepolicytreesviafouroperators(split,union,rouletteandgraphying)from
behaviorsequences.
𝑎 𝑎
𝑜 ! 𝑜 𝑜 ! 𝑜
! " ! "
𝑎 𝑎 𝑎 𝑎
# ! # "
𝑜 𝑜 𝑜 𝑜 𝑜 𝑜 𝑜 𝑜
! " ! " ! " ! "
𝑎 𝑎 𝑎 𝑎 𝑎 𝑎 𝑎 𝑎
" # " " # # " !
𝑎 𝑎
𝑜 ! 𝑜 𝑜 ! 𝑜
! " ! "
𝑎 𝑎 𝑎 𝑎
# ! # "
𝑜 𝑜 𝑜 𝑜 𝑜 𝑜 𝑜 𝑜
! " ! " ! " ! "
𝑎 𝑎 𝑎 𝑎 𝑎 𝑎 𝑎 𝑎
! # ! ! " # " #
Decoder
noise
𝜺~𝒩(𝟎,𝐈)
compressed space
Encoder
𝑎 𝑎
𝑜 ! 𝑜 𝑜 ! 𝑜
! " ! "
𝑎 𝑎 𝑎 𝑎
# ! # "
𝑜 𝑜 𝑜 𝑜 𝑜 𝑜 𝑜 𝑜
! " ! " ! " ! "
𝑎 𝑎 𝑎 𝑎 𝑎 𝑎
! # " " # !
Incomplete policy trees
Figure4: TheprincipleofaVAE-basedapproachthatleveragesincompletepolicytreestogeneratediversenewtrees,
maximizingcoverageofagentj’struebehaviorsandenhancingthediversityoftheoverallpolicytreecollection.
7VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
Empirically,weverifythatthebehaviormodelofagentj generatedusingtheVAEcloselyalignswiththedistribution
ofagentj’shistoricaldata. Furthermore,wecomparethediversityandcredibilityofthetop-K policytrees. Wehave
describedthedetailedprocessofgeneratingmorediversepolicytreesusingtheVAEmodelinFig.5.
𝒟(cid:3021) (cid:3404) ℋ(cid:2904)
(cid:3040)
ℒ(cid:3087),(cid:3109)x(cid:3404)ℒ(cid:3109)(cid:3012)(cid:3013)x(cid:3397)ℒ(cid:3087)(cid:3045)(cid:3032) ,(cid:3109)(cid:3030)(cid:3042)(cid:3041)x
𝑎𝑜 (cid:2871)𝑎 𝐴(cid:2869) 𝑎𝑜 (cid:2870)𝑜(cid:2869) (cid:2869) (cid:3404)(cid:2869)𝑎 (cid:2870)𝑎𝑎(cid:2871)𝑜 𝑎𝑜(cid:2871)(cid:2869)𝑜 𝑎 (cid:2869)(cid:2870)𝑜𝑜(cid:2869) 𝑜 ,𝑎(cid:2871)(cid:2870)(cid:2869)𝑎 𝑎(cid:2870) 𝑎(cid:2871)(cid:2869)𝑎 (cid:2871)𝑎 (cid:2870)𝑎(cid:2869) 𝑜 ,(cid:2869) (cid:2870)𝑜 𝑎 𝑎(cid:2869)𝑜(cid:2870)𝑜 𝑜𝑜 (cid:2870)(cid:2869) (cid:2871)(cid:2870) (cid:2869)𝑎(cid:2870) (cid:2870)𝑎𝑎(cid:2870)𝑜(cid:2869)(cid:2870)𝑜𝑜(cid:2870) 𝑎(cid:2870) (cid:2869) ℤ 1ℋ(cid:2904) 𝐱 1 0 0 0 0 0 1 0 1 0 0 0 0. . . 𝜙← 𝑔E (cid:3109)𝜙 n :(cid:3397) c ℙ𝛼 o (cid:3109)∇ d (cid:4666)(cid:3109) e 𝐳ℒ(cid:3087) |r 𝐱,(cid:3109) (cid:4667)x 𝐳~𝒩(cid:4666)2 𝝁 (cid:3109),T V 𝝈ra A (cid:3109)in (cid:4667)i Eng via 𝐳 SGD 𝜃← 𝑓𝜃 D (cid:3087)(cid:3397) e :ℙ𝛼 c∇ (cid:3087)o(cid:3087) ℒ d 𝐱(cid:3087)(cid:3045) e(cid:3032) ,(cid:3109)(cid:3030) 𝐳r(cid:3042)(cid:3041)x .. .. .. 0 0 . . 0𝐱 0(cid:3556) . . .81 19 41 5 1. . . I 𝐱(cid:3556) 𝐱 1 0 0 0 0 0 1 0 0 1 0 0(cid:3540) . . .
Ω(cid:3404) 𝑜 ,𝑜 0 𝐳(cid:3404)𝝁 (cid:3397)𝝈 ⨀𝜺 .1 0
(cid:2869) (cid:2870) 0 (cid:3109) (cid:3109) .8 1
𝑇(cid:3404)3 1 𝛘 𝜺~𝒩(cid:4666)𝟎,𝐈(cid:4667) 0 .1. 0 0
Zig‐ZagOne‐Hot (ZZOH)encoding & decoding 𝐱
ℤ𝐱(cid:3540) (cid:2897)3 Generating
1 Top‐K argmax 𝑑(cid:4666) ℋ(cid:3553)(cid:2904) (cid:4667) 𝑑(cid:3014)(cid:3005)(cid:3007)
ℋ(cid:2904) ℋ(cid:2904)⇔ℤ 𝐱 1 0 0 0 0 ℋ(cid:3553)(cid:3152) (cid:3143)⊂ℤ𝐱(cid:3540)(cid:3145) (cid:2895) 𝑑(cid:3010)(cid:3004)(cid:3005)
ℎ=3 𝑜 (cid:2869) 𝑎 (cid:2869) 𝑜 (cid:2870) en dco ecd oin dg in g 0 0 1 𝑜0 0 (cid:2869) 𝑎 (cid:2869) 𝑜 (cid:2870) 1 0 0 0 0 1 0 1 0 𝒟(cid:3561)(cid:3021) (cid:3404)4 ℋ(cid:3553)(cid:2904)ℋ(cid:3553) (cid:2895)(cid:2904) (cid:2895) 𝑜 (cid:2869)𝑜𝑜 (cid:2869)(cid:2869)𝑎𝑎(cid:2869)𝑎 (cid:2869)(cid:2869)𝑜𝑜(cid:2870)𝑜 (cid:2870)(cid:2870)
ℎ=2 𝑜 (cid:2869) 𝑎 (cid:2871)𝑜 (cid:2870) 𝑜 (cid:2869) 𝑎 (cid:2869) 𝑜 (cid:2870) 𝑎 1 0(cid:2869)𝑎 0 1(cid:2870)𝑎 0 0(cid:2871) 0 0 1 0 0 𝑜0 (cid:2869) 𝑎 (cid:2871) 0 0 1𝑜 (cid:2870) 𝑜 (cid:2869)0 1 0𝑎 (cid:2869) 𝑜0 (cid:2870) 0 0 0 0 0 . . . 𝑜 (cid:2869)𝑜𝑜 (cid:2869)(cid:2869)𝑎𝑎(cid:2870)𝑎 (cid:2871)(cid:2871)𝑜𝑜(cid:2870)𝑜 (cid:2870)(cid:2870) 𝑜 (cid:2869)𝑜𝑜 (cid:2869)(cid:2869)𝑎𝑎(cid:2870)𝑎 (cid:2870)(cid:2869)𝑜𝑜(cid:2870)𝑜 (cid:2870)(cid:2870)
ℎ=1 𝑎 (cid:2869) 𝑎 (cid:2871) 𝑎 (cid:2870) 0 0 0 0 1 0 0 1 0 𝑎 (cid:2869) 0 𝑎 (cid:2871) 𝑎 (cid:2870) 0 1 0 0
0 1 𝑎𝑎(cid:2871)𝑎 (cid:2870)(cid:2870) 𝑎𝑎(cid:2871)𝑎 (cid:2871)(cid:2871)𝑎 (cid:2870)𝑎 (cid:2870) 𝑎𝑎 (cid:2869)(cid:2870)
Figure5: TheVAEnetworkcreatesnewpolicytrees. WiththeZig-ZagOne-Hotencoding-decodingmethoddesigned
forpolicytrees,VAEcanhandlebothcompleteandincompletetrees,emphasizinglearningfromearliernodes. By
usingMDFandIDF,wepickthemostdiverseandreliabletop-K trees,matchinghistoricalagentpatternsclosely.
4.2.1 Zig-ZagOne-Hotencoding&decoding
ToserializethepolicytreeandmakeitcompatiblewiththeVAEnetwork,weintroducetheZig-ZagOne-Hot(ZZOH)
encodinganddecodingtechnique. Thisapproacheffectivelytransformsbothcompleteandincompletepolicytreesinto
columnvectorsthatcanbeutilizedbytheVAEnetwork.
Considering some empty nodes in incomplete policy tree, we enlarge the action space A = a ,a ,...a of
j
{
1 2 |Aj|
}
agentj withemptyactiona ,denotedasA˜ = a ,a ,a ,...a . Then,weuseone-hotencodingtoencodeeach
0 j
{
0 1 2 |Aj|
}
actionoftheenlargedactionspaceA˜ intoabinarycodewiththelength A˜ andgenerateanencodingsetofaction
j j
space, denoted as A˜c. For example, given the action space A = a ,a| ,a| , we have enlarged the action space
j j { 1 2 3 }
A˜ = a ,a ,a ,a ,andtheone-hotencodingsetofenlargedactionspace
j 0 1 2 3
{ }
a a a a
0 1 2 3
1 0 0 0
A˜c = 0 1 0 0
j
0 0 1 0
0 0 0 1
, andwehaveA˜c[a ] = [1;0;0;0]. Byencodingtheactionspace, wedefineanoperator forencoding/decoding
j 0 Z
policytreesto/frombinarycodes. Thislosslesstransformationintoavectorrepresentationpreservesthetree’sstructural
information,enablingefficientprocessingandanalysis.
Thus,forgivenapolicytree T,weencodeeachactionnodeintoabinarycodeusingtheencodingsetofenlarged
actionspace,andthenconcateH natethebinarycodesintoacolumnvectorxinazigzagorder,denotedas T. Wherein
ZH
the numerical information indicates the node action information, and the node position information indicates the
sequenceinformationinfrontofthenodeandthecurrentobservationresult. Werepeatthisoperationtoencodethe
policytreeinthegivenpolicytreeset T andformthedatasetX fortrainingandtestingtheVAEnetwork,whichis
denotedas T,asshowninFig.5 1D . WealsopresenttheimplementationoftheoperatorsinAlg.5oftheAppendix.
ZD ⃝
8VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
4.2.2 TreeLossFunctioninVAE
Theintricatestructureofpolicytreesencapsulatescriticalinformation,includingactionvalues,sequentialorder,and
theirrelativeimportance. Actionnodesclosertotherootexercisegreaterinfluenceontheoveralldecision-making
effectivenessofthetree,shapingthetrajectorytoalltheleafnodes. However,thisstructuralinformationisnotfully
capturedinserializedtreerepresentations.
ToleverageserializedpolicytreedataforVAEtrainingandbettercapturestructuralnuances,weproposeamodified
VAElossfunction. Thisrefinementprioritizesthebehaviorlearningfromnodeswithsmallertimeslices,emphasizing
thestructuralinformationembeddedintheoriginalpolicytree. Effectively,thisapproachassignshigherweightsto
actionnodesclosertotheroot,ensuringthattheirsignificanceisreflectedintheVAElearning.
GiventhepriordistributionoverlatentvariablesasacenteredisotropicmultivariateGaussian: P (z)= (z;0,I),we
θ
definethelikelihoodfunctionP (x z)asamultivariateBernoullidistribution(forbinarydata)wherethN edistribution
θ
|
parametersaredeterminedfromzusingafully-connectedneuralnetworkwithasinglehiddenlayer. Sincethetrue
posterior P (z x) is often intractable, we employ a variational approximate posterior P (z x) to approximate it.
θ ϕ
| |
Specifically,weletthevariationalapproximateposteriorbeamultivariateGaussianwithadiagonalcovariancestructure:
P (z x) = (z;µ ,σ2I). whereµ andσ2 arethemeanandvariancevectorspredictedbytheencodernetwork
ϕ | N ϕ ϕ ϕ ϕ
parameterizedbyϕ. Toenabledifferentiablesamplingfromthisvariationalposterior,weutilizethereparameterization
trick. Specifically,wesamplearandomnoisevectorεfromastandardnormaldistribution (0,I)andthentransform
N
itaccordingtothepredictedmeanandvariance: z=µ +σ ε. Thisre-parameterizationallowsgradientstoflow
ϕ ϕ ⊙
throughthesamplingprocess,enablingtheuseofgradient-basedoptimizationtechniquestotraintheVAE.
µ ,σ =Encoder (x)
ϕ ϕ ϕ
ε P(ε)= (ε;0,I) (3)
∼ N
z=µ +σ ε
ϕ ϕ ⊙
WehaveathedecoderofVAEforBernoullidata:
˜x=Decoder (z)
θ
|x|
(cid:88)
logP (x z)= logP (x z)
θ θ k
| |
k=1
|x|
(cid:88)
= Bernoulli(x ;˜x ) (4)
k k
k=1
|x|
(cid:88)
= x log˜x +(1 x )log(1 ˜x )
k k k k
− −
k=1
=ℓ(x,˜x)
whereℓ(x,˜x)denotestheBinaryCross-EntropyLoss(BCELoss)[38]duetotheVAEdecoderforBernoullidata,aka
thebasicconstructionerror.
ToutilizeserializeddatafromthepolicytreetotrainaVAEnetworkandbetterrepresentthestructuralinformationand
datadistributioncharacteristicsoftheoriginalpolicytree,weproposethedecoderofVAEforBernoullidatawithtree
weights.
|x|
(cid:88)
ℓtree(x,˜x)= w(k)(x log˜x +(1 x )log(1 ˜x )) (5)
k k k k
− −
k=1
wherew(k)=log(1+h( k−1 +1)),N = x/(A +1), ˜x ˜x:0 ˜x 1. h(n)istheheightofanodeinthe
⌊ N ⌋ | | | j | ∀ i ∈ ≤ i ≤
policytree,whichcanbecalculatedas:
Ωc−1 1 Ωc 1
h(n)=T c+1,if n [| | − +1,| | − ],c R+ (6)
− ∈ Ω 1 Ω 1 ∈
| |− | |−
ThenwehavethelossfunctionofVAE:
Lr θe ,c ϕon(x)= −E z∼P ϕ(z|x)log(P θ(x |z))
= E log(P (x µ +σ ε))
− ε∼N(ε;0,I) θ | ϕ ϕ ⊙ (7)
1
(cid:88)ns
= ℓtree(x,˜x(l))
−2n
s
l=1
9VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
KL(x;ϕ)= (P (z x),P (z))
Lθ,ϕ LKL ϕ | θ
=D ( (z;µ ,σ2I) (z;0,I))
KL N ϕ ϕ ||N
=
1| (cid:88)µ ϕ|
(cid:0) 1+logσ2 µ2 σ2 (cid:1)
(8)
2 ϕ,k− ϕ,k− ϕ,k
k=1
= KL(x;ϕ)
Lϕ
(x)= recon(x)+ KL(x) (9)
Lθ,ϕ Lθ,ϕ Lϕ
wheren isthebatchlearningsamplingsize,˜x=f (g (x))istheoutputvectorofVAEnetwork,theoperator isa
s θ ϕ
⊙
Algorithm1:Learningatree-lossbasedVAEnetworkthroughSGD(@VAE)
Data: Dataset = x ,learningrateα,batchsizen ,samplingsizen .
m b s
Result: LearneX dnetw{ o} rkNetvae()withparametersθandϕ
θ,ϕ ·
1 Initialize: Variationalnetworkparametersθandϕrandomly
2 repeat
3 Sampleabatchofn bdata X˜ = {x }nb from
X
4 forx ˜do
∈X
5 µ,σ Encoder ϕ(x)
←
6 forl 1,2,...,n s do
∈{ }
7 Generatenoiseε (0,I)
∼N
8 z µ+σ ε
← ⊙
9 ˜x(l) Decoder θ(z)
←
10 end
11 Computethelossfunction:
12 Lr θe ,c ϕon(x) ←−2n1
s
(cid:80) (cid:16)n l=s 1ℓtree(x,˜x(l))
(cid:17)
13 LK ϕL(x) ← 1 2(cid:80)| kµ =ϕ 1| 1+logσ2 ϕ,k−µ2 ϕ,k−σ2 ϕ,k
14 end
15 Comput (cid:16)ethegradientsofthe (cid:17)lossfunctionwithrespecttothenetworkparameters: ∇θ(cid:80) xLr θe ,c ϕon(x)
16
∇ϕ(cid:80)
x
Lr θe ,c ϕon(x)+ LK ϕL(x)
17 Updateparametersusinggradientdescent:
18 θ ←θ −α ∇θ(cid:80) xL(cid:16)r θe ,c ϕon(x)
(cid:17)
19 ϕ ←ϕ −α ∇ϕ(cid:80) x Lr θe ,c ϕon(x)+ LK ϕL(x)
20 untilConvergence;
element-wiseproductoftwovectorandremovesallthezero-elements.
Subsequentlyweapplystochasticgradientdescentalgorithm(SGD)withthebatchlearningtotraintheVAEnetwork,
minimizingthemodifiedreconstructionloss. First,wecomputethegradientsofthelossfunctionwithrespecttothe
(cid:16) (cid:17)
networkparameters: (cid:80) recon(x)and (cid:80) recon(x)+ KL(x) . Then,weupdateparametersusinggradient
∇θ xLθ,ϕ ∇ϕ x Lθ,ϕ Lϕ
(cid:16) (cid:17)
descent:θ θ α (cid:80) recon(x) and ϕ ϕ α (cid:80) recon(x)+ KL(x) . After the training process is
← − ∇θ xLθ,ϕ ← − ∇ϕ x Lθ,ϕ Lϕ
completed,theoriginalpolicytreeisreconstructedtoexpandandobtainthecompletepolicytree,asshowninFig. 5.
DuringthetrainingphaseofVAE,wewilltraintheVAEnetworkparameterstolearnthedistributionofthebehaviors
fromhistoricalpolicytreesthatmayhavesomeincompletepolicytrees. Inthetesting,wereconstructtheinputpolicy
treetoobtainacompletepolicytree,includingtheselectionprobabilitiesforeachnode.
We outline the VAE training in Alg. 1. We initialize the VAE parameters θ and ϕ and choose a data batch of size
n (lines 1-2). Then we encode each datum to get the compressed variable statistics and decode with the noise to
b
reconstruct(lines5-10). SubsequentlyweadjustandcomputetheVAEloss(lines11-13),calculatethegradients,update
theparameters,andrepeattheproceduresuntiltheparametervaluesbecomestable(lines15-20). Afterthenetwork
parametersarefinalized,weinputthegivenpolicytrees T intothetrainedVAEnetwork. Thisgeneratesanewvector
H
xˇ representingapolicytreeasoutputs. ApplyingtheOne-hotencodingoperator (x˜)transformsaprobabilityvector
I
toone-hotbinaryvectorforeachactionnode,whichmeansselectingtheactionwiththehighestprobabilityinthe
10VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
correspondingchosenvectorforthenode,presentedinAlg.4. Then,weapplytheZOOHdecodingoperatortoproduce
apolicytree xˇ,ensuringthattheresultingpolicytreeiscompleteandvalid. WerepeatthisprocessuntilweobtainM
completepolZ icytrees(Fig.5 3),denotedas ˇT = xˇ .
M
⃝ D {Z }
4.3 EvaluationIndex
Afterreconstructingasignificantnumberofpolicytrees,weneedtofilteroutthosethatdonotaccuratelyrepresent
agentj’struebehavior. Toachievethis,weleveragethemeasurementofdiversitywithframes(MDF)[6]andintroduce
anovelmetriccalledinformationconfusiondegree(ICD).Thesemetricsaretoselectthetop-K policytreesfromthe
generatedset.
MDFevaluatesthediversityacrossboththeverticalandhorizontaldimensions: theverticalrefersthevariationsin
behaviorsequenceswithineachpolicytree,andthehorizontalexploresthesequencedifferencesacrossallpotential
observationsatatimestep.
T
d(cid:16)
ˆT
(cid:17)MDF =(cid:88)Diff(h t)+Diff(H t)
(10)
{H }K Ω t−1
t=1 | j |
whereDiff(h )andDiff(H )countthedifferentsequencesh andsub-trees(frames)H respectivelyin T =
t t t t DK
ˆT . Ω denotesthenumberofagentj’sobservations.
K j
{H } | |
Additionally, we introduce ICD to evaluate the reliability of the policy tree data. This metric guarantees a high
confidenceforeachnodeinthereconstructedtree. IntheVAE-generatedtree,nodesactasclassifiers,choosingthemost
probableaction. Asmallmarginbetweenthetopaction’sprobabilityandtherestleadstohighinformationconfusion,
indicatingunclearorunreliabledecisions. WewanttheICDvalueofthepolicytreetoreachthemaximumvalue,and
selectthetop-K treeastheoutput.
N
d(cid:16) ˆT (cid:17)ICD = (cid:88) (cid:88) log(1+h(n)) p log(p ) (11)
K n n
{H } − ∗
HˆT∈{HˆT}Kn=1
wherep istheprobabilityofactionundercorrespondingobservationandN isthenumberofactionnodes.
n
Byselectingthetop-K policytrees,weaimforthemaximumdiversityorminimalinformationconfusionamongthe
generatedtrees. SinceMDFcapturesthediversityoftheoverallbehaviorsandtheICDofthesetreesreflectstheirtrue
distributioninthehistoricaldata,weconductthetop-K selectionusingbothMDFandICD.
(cid:16) (cid:17)
ˆT = argmax d ˆT (12)
K K
{H } {H }
{HˆT}K⊂DˇT
whered()iscalculatebyeitherEq.11orEq.10.
·
4.3.1 TheFrameworkoftheVAE-basedPolicyTreeGeneration
WeelaboratethegenerationofthediversepolicytreesusingtheVAEinAlg.2. AsdepictedinFig.3andFig.5,thekey
stepsareasfollows.
Initially, the process begins by reconstructing a set of incomplete policy trees, denoted as T, from an agent j’s
interactivehistoryhL. ThisreconstructionisachievedthroughaseriesofoperationsincludingD split,union,roulette,
andgraphying,whichservetoextractandorganizepolicytreesfromthehistoricaldata(lines1-9,Fig.3 1-4).
⃝ ⃝
Subsequently,aZZOHencodingoperatorisappliedtoeachpolicytreein T,convertingthemintoabinaryvector
D
representation. Thisencodeddataset,denotedasX,isthenusedtotrainandtesttheVAEnetwork(line10,Fig.5 1).
⃝
Utilizingstochasticgradientdescent(SGD)andatree-specificlossfunction,theVAEnetworklearnstocapturethe
latentrepresentationsofthepolicytrees(line11,Fig.5 2).
⃝
OncetheVAEnetworkhasbeentrained,itisreadytogeneratemorecompletepolicytrees(lines12-18,Fig.5 3).
⃝
Thisisachievedbyrandomlyselectingabinaryvectorfromtheoriginaldataset andfeedingitintotheVAEnetwork.
X
Thenetworkthengeneratesanewbinaryvector,whichisdecodedusingOne-hotencodingandtheZZOHdecoding
operatortoreconstructacompletepolicytree(lines14-17). Thisprocessisrepeateduntiladesirednumberofcomplete
policytreesareobtained(lines13-18).
Finally,fromthegeneratedsetofpolicytrees ˇT,themostdiverseorreliablebehaviorsareselectedusingmetricssuch
D
astheMDFandICD.ThesemetricsallowustoidentifytheK policytreesthatexhibitthehighestdegreeofdiversity
orreliability,enablingtheselectionofbehaviorsthatarethebestsuitedforagiventaskorscenario(line19,Fig.5 4).
⃝
11VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
Algorithm2:VAE-Enabledbehaviors(@VEB)
Data: agentj’sinteractivehistoryhL,learningrateα,actionsetAandobservationsetΩ,thenumberofpolicy
treesm,thenumberofgeneratedpolicytreesM,theevaluationindexfunctiond(),theparameteroftop-K
·
selectionfunctionK
Result: NewK policytrees ˆT
K
1 HT ThL.◁Applyingspl{ itH op} erator;
←S
2
HT HT.◁Applyingunionoperator;
←U
T
3
D ←∅
4 fork 1,2, m do
5 HT a∈ ←{ RH·T· .· ◁A} pplyingrouletteoperator;
6 HaT ←∪o∈ΩT−1 RHT ao.◁Applyingrouletteoperator;
7
HaT ←GHaT.◁Applyinggraphingoperator;
T T T
8 D ←D ∪Ha
9 end
10 HT∈DT
T.◁ApplyingZZOHoperator;
11 NX e← tv θ,a ϕe∪ ( ·) ←VAZ EH ( X).◁LearningVAEwithtreeloss;
ˇT
12
D ←∅
13 fork 1,2, M do
∈{ ··· }
14 x .◁Applyingrouletteoperator;
15
˜x← ←NR eX tv θ,a ϕe(x).◁GeneratedataviaVAE;
16 ˇx (˜x).◁ApplyingOne-hotencoding;
17 ˇ← T I ˇT ˇx ◁ApplyingZZOHoperator;
D ←D ∪Z
18 end
(cid:16) (cid:17)
19 {HˆT }K ←argmax {HˆT}K⊂DˇT d {HˆT }K ◁Applyingtop-K operator;
5 ExperimentalResults
WeimprovetheI-DIDbyincorporatingtheVAE-basedmethodtogenerateandselectthediverseandrepresentative
behaviorsforagentj. Weconducttheexperimentsintwowell-studiedmulti-agentproblemdomains: themulti-agent
tigerproblem(Tiger)andthemulti-agentunmannedaerialvehicle(UAV)problem[46,45,47,29]. Alltheexperiments
areperformedonaWindows10systemwithan11-thGenIntelCorei7-6700CPU@3.40GHz(4cores)and24GB
RAM.
Inthemulti-agentcontext, wefocusonageneralscenariowithtwoagents, consideringagentiasthesubjectand
constructinganI-DIDmodelfortheproblemdomains. TheM nodesinthismodelrepresentthebehavioralmodelsof
j
agentj,asshowninFig.2. OurproposedapproachoptimizestheI-DIDmodelbyprovidingagentiwithasetofagent
j’srepresentativebehaviors. WecomparesevenalgorithmsforsolvingtheI-DIDmodelintheexperiments:
• TheclassicI-DIDalgorithm,whichreliessolelyonknownmodelsM toexpandthemodelnodes,assuming
thetruebehaviorofagentj lieswithinthesenodes[47].
• Thegeneticalgorithmbasedalgorithm(IDID-GA)generatesagentj’sbehaviorsthroughgeneticoperations[48,
6].
• TheIDID-MDF[6]andIDID-VAE-MDFalgorithms,whichutilizeVAEmethodstogeneratenewdatafrom
agentj’shistoricaldata,andthenemploytheMDFmetrictoselectthetop-kbehaviorsofagentj.
• TheIDID-Random,IDID-VAE-MDF,andIDID-VAE-ICDalgorithms,whichdifferinhowtheyselectthe
top-K behaviorsfromthepolicytreesgeneratedbyVAE.IDID-Randomselectsthebehaviorsrandomly,while
IDID-VAE-MDFandIDID-VAE-ICDutilizeMDFandICDrespectivelyandIDID-VAE-BCELossreplaces
theproposedtreelosswiththetraditionalBCELoss.
Allsevenalgorithms(IDID,IDID-MDF,IDID-VAE-MDF,IDID-GA,IDID-Random,IDID-VAE-ICDandIDID-VAE-
BCELoss)usethesameunderlyingalgorithmtosolvetheI-DIDmodel,differingonlyinhowtheyexpandthemodel
nodeswithcandidatemodelsforagentj.
Toevaluatethealgorithms,weconsidertheaveragerewardsreceivedbyagentiduringinteractionswithagentj. We
randomlyselectonebehaviormodelofagentj asitstruemodelfromthesetofM nodes. Agentithenexecutesits
j
12VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
GR(Growls from Right) GR(Growls from Right)
L(Listen) OL(Open Left)
Figure6: Amulti-agenttigerproblem,beingsimplifiedtothetwo-agentversion,wheremodelingagentj’sbehavioris
tooptimizeagenti’sdecision. Theproblemspecificationfollows: S =2,A = A =3,Ω =6,and Ω =2.
i j i j
| | | | | | | | | |
optimalI-DIDpolicy,whileagentj followstheselectedbehavior. Ineachinteraction,agentiaccumulatesrewards
basedontheactionresultsateachtimestepovertheentireplanninghorizonT. Werepeattheinteraction50timesand
computetheaveragerewardforagenti.
5.1 Multi-agenttigerproblems
Thetwo-agenttigerproblemservesasacanonicalbenchmarkforassessingtheperformanceofmulti-agentplanning
models. InFig.6,agentiandagentj mustmakeachoicebetweenopeningtheright/leftdoor(OR/OL)orlistening(L)
todeterminethetiger’sposition,givenlimitedvisibility. Ifanagentopensthedoorwithgoldbehindit,itwilltakethe
gold;however,ifthetigerisbehindthedoor,theagentwillbeeaten. Meanwhile,ifbothagentssimultaneouslyopen
thedoorwiththegold,theywillsharetherewardequally. Theagents’decisionsareinfluencedbytheirobservations,
whichmaynotalwaysbeaccurate. Forinstance,asqueakemanatingfromadoorcouldbemistakenforanotheragent’s
voice,thetiger’sgrowl,oramisinterpretationofthesound.
5.1.1 DiversityandMeasurements
We first explore how the top-K selection algorithm impacts the diversity of the policy tree set and investigate the
relationshipbetweentheselectioncriteriaofthepolicytree. Toevaluatethediversityofthetop-K policytreesetfor
agentj generatedbyIDID-VAE-MDFandIDID-VAE-ICDintheTigerproblem,weconductedtheexperimentsonthe
correlationbetweentheK valuesandthediversity(MDFevaluationmetric). AsshownintheFig.7,thereisasmall
diversitygapbetweenthetop-K policytreesselectedusingtheICDindexandthoseselectedusingthediversityindex
MDF.ThisverifiesthatthepolicytreesgeneratedbyVAEgenerallyhavegooddiversity,whichmakesitdifficultfor
MDFtodistinguishbetweenpolicytreesandselectpossiblytruebehaviors.
IDID-VAE-MDF 18 IDID-VAE-MDF
22.5 IDID-VAE-ICD IDID-VAE-ICD
20.0 16
17.5 14
15.0 12
12.5 10
10.0 8
7.5 6
5.0 4
2.5
1 2 3 4 5 6 7 8 9 10 11 1 2 3 4 5 6 7 8 9 10 11
K K
(a) T=3 (b) T=4
Figure7: For(a)T =3and(b)T =4,givendifferentK values,thediversityofthetop-K policytreesgeneratedby
IDID-VAE-MDFandIDID-VAE-ICDrespectively.
To explore the correlation between the average reward obtained by the subject agent i and the corresponding met-
rics(MDFandICD),weselecttheobtainedpolicytreesetusingvariousevaluationmetricstodecidethetop-K set. The
13
ytisreviD ytisreviDVariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
selectedmetricsarethennormalized. Finally,wemeasuretheaveragerewardthatagenticanachieveusingthispolicy
treeset. AsshowninFig.8,therewardsobtainedbyagentiincreaseasthecorrespondingmetricsrise. Thex-axiswith
thelabeld¯representsthenormalizedvaluesofICDandMDFmetrics,processedusingmin-maxnormalization. Under
thepolicytreesselectedbyMDF,theaveragerewardsofagentiincreasewithMDF,butthereseemstobeanupper
limit. ThepolicytreesselectedbyICDappeartobelessstable. AttimesliceT =4,thereisanoverallupwardtrend.
IDID-VAE-MDF IDID-VAE-MDF
2 IDID-VAE-ICD 2 IDID-VAE-ICD
0 0
-2
-2
-4
-4
-6
-6
-8
-8
-10
-10
-12
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
d d
(a) T=3 (b) T=4
Figure 8: For (a) T = 3 and (b) T = 4, the correlation between the average rewards obtained by agent i and the
correspondingmetrics(MDFandICD)referstotherelationshipbetweentherewardsobtainedusingdifferentmodels,
namelyIDID-VAE-MDFandIDID-VAE-ICD.
5.1.2 ComparisonResultsofMultipleI-DIDAlgorithms
Weinvestigatethequalityofthemodel,specificallytheaveragerewards,andcomparemultipleI-DIDalgorithms. To
optimizeitsdecision-making,agentimustanticipateagentj’sactionsconcurrently. WeconstructanI-DIDmodel
tailoredforagentianddevisethemodelspaceM foragentj throughmultipleI-DIDalgorithms.
j
100
IDID 100 IDID
IDID-MDF IDID-MDF
50 IDID-VAE-MDF 50 IDID-VAE-MDF
IDID-RANDOM IDID-RANDOM
0
0
50
50
100
100
150
1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10
Agent j's Behaviours Agent j's Behaviours
(a) T=3 (b) T=4
Figure9: Theaveragerewardsarereceivedbythesubjectagentiwithagentj’sbehaviorsmodels(generatedbyIDID,
IDID-MDF,IDID-VAE-MDFandIDID-Random)for(a)T =3and(b)T =4.
InFigs.9and 10, theresultsshowtheaveragerewardsforagentiusingvariousI-DIDmethodsforT =3andT
= 4. The original I-DID gives agent j six historical models (M = 6). We compared IDID-MDF, IDID-Random,
IDID-VAE-MDF,IDID-GA,IDID-VAE-ICD,andIDID-VAE-BCELoss,pickingthetop10modelsforagentj. Wefind
thatIDID-MDF,IDID-VAE-MDF,IDID-GA,andIDID-VAE-ICDallbeattheoriginalI-DID.Notably,IDID-VAE-ICD
14
sdraweR
egarevA
sdraweR
egarevA
sdraweR
egarevA
sdraweR
egarevAVariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
60
IDID-VAE-MDF 40 IDID-VAE-MDF
IDID-VAE-ICD IDID-VAE-ICD
40 IDID-GA 30 IDID-GA
IDID-VAE-BCELoss IDID-VAE-BCELoss
20 20
10 0
0
20
10
40
20
1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10
Agent j's Behaviours Agent j's Behaviours
(a) T=3 (b) T=4
Figure10: Theaveragerewardsarereceivedbythesubjectagentiwithagentj’sbehaviorsmodels(generatedby
IDID-VAE-MDF,IDID-VAE-ICD,IDID-GAandIDID-VAE-BCELoss)for(a)T =3and(b)T =4.
andIDID-MDFdosimilarlywell,probablybecausethetigerproblemisn’ttoocomplex. Theseimprovedmethods
arebetteratmodelingandpredictingagentj’struebehavior,aidingagenti’sdecisions. Inthetests,IDID-VAE-ICD
outperformedIDID-VAE-ICD-BCELoss. ThisisbecauseIDID-VAE-ICDusesaspecializedlossbasedonpolicytree
nodes’importance,whereasIDID-VAE-ICD-BCELossreliesonthestandardbinarycross-entropyloss. Inaddition,the
VAE-basedI-DIDsolutionsoutperformtheIDID-GAandshowtheirpotentialingeneratingagentj’struebehaviors
fromtheknownmodels.
5.2 Multi-agentUAVproblems
agent j
agent i
Figure11: Inatwo-agentUAVproblem,agentiaimstocaptureagentj beforeagentj reachesthesafehouse. The
problemspecificationfollows: S =81, A = A =5,and Ω = Ω =4.
i j i j
| | | | | | | | | |
Themulti-agentUnmannedAerialVehicle(UAV)problemposesasignificantchallengeintherealmofmulti-agent
planning.AsdepictedinFig.11,bothUAVs,referredtoasagents,havetheoptiontomoveinfourdifferentdirectionsor
remainstationary. Inlinewithrealisticscenarios,theUAVsareunabletoascertaintheprecisepositionsofotheragents
andcanonlyreceivesignalsrelativetoeachother. Here,wedesignateagentiasthechaser,taskedwithintercepting
thefleeingagentj,whileagentj aimstoreachasafehouse. Sincebothagentsoperateconcurrently,agentirequires
anaccurateestimationofagentj’sbehaviorinordertosuccessfullyachieveitsgoal. Agentiwillberewardedifit
successfullyinterceptsagentj beforeitreachesthesafehouse. WeletchaseriuseanI-DIDmodel,providingitwith
potentialtruebehaviormodelsofagentj.
5.2.1 DiversityandMeasurements
Initially, weexplorehowthetop-K selectionalgorithmaffectsthediversityofthepolicytreeset, delvingintothe
correlationbetweenpolicytreeselectioncriteria. AsillustratedinFig.12,bothcurvesdisplaynotablefluctuationsat
smallvaluesofK,buttheygraduallyriseandstabilizeasK increases,ultimatelyresultinginaslightdivergence. By
examiningthetop-K selectionfunctionalongsideexperimentaldatafromtwodistinctproblemdomains,itbecomes
evidentthatasmallK valuecorrespondstoawiderangeofpotentialpolicytreesetcombinations,therebyaugmenting
15
sdraweR
egarevA
sdraweR
egarevAVariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
the diversity within these sets. This effect may not be immediately obvious in less complex problem domains.
Nonetheless,asthecomplexityofthedomainincreases,sodoesthepotentialdecisiontreespace. Hence,whenselecting
onlyafewpolicytreesfromthisexpandedspace,thereisasubstantialchanceofencounteringsignificantdisparities
among them. According to the diversity function, smaller K values are associated with a limited number of sets,
restrictingthepotentialpolicytreepathsandsub-treesthatcanbeuncovered. Thislimitationsuggeststhatthediversity
valuelinkedtosmallerK willnotsurpassthatoflargerK values. AsK increases,thefinalconvergencevalueof
IDID-VAE-ICD is lower than that of IDID-VAE-MDF, indicating its superiority in facilitating the selection of an
appropriatepolicytreeset. Inessence,continuouslyaugmentingthenumberofpotentialpolicytreesdoesnotinherently
leadtogreaterdiversity,nordoesitensureahigherlikelihoodofdiscoveringthetruebehaviorofagentj.
IDID-VAE-MDF IDID-VAE-MDF
9 IDID-VAE-ICD IDID-VAE-ICD
12
8
7 10
6 8
5
6
4
4
3
2 2
1 2 3 4 5 6 7 8 9 10 11 1 2 3 4 5 6 7 8 9 10 11
K K
(a) T=3 (b) T=4
Figure12: For(a)T =3and(b)T =4,givendifferentK values,thediversityofthetop-K policytreesgeneratedby
IDID-VAE-MDFandIDID-VAE-ICDrespectively.
Westudythecorrelationbetweenagenti’saveragerewardandtwometrics: MDFandICD.Aftercarefullyselecting
thetop-K policytreesfromtheavailablesetusingvariousmetricsandnormalizingthemviamin-maxnormalization
(d¯),wediscoveradirectcorrelationbetweenthesemetricsandagenti’saveragereward,asillustratedinFig.13. Inthe
Tigerproblem(refertoFig.8),whiletheICDmetricdoesnotstronglycorrelatewiththeaveragerewardcompared
totheMDFmetric,bothstillensurerespectableaveragerewardsforthechosenpolicytrees. However,intheUAV
problem,theICDmetricdemonstratesastrongercorrelationwithaveragerewardsthantheMDFmetric. Thissuggests
thattheMDFmetricmaynotconsistentlyidentifythetruepolicytree,particularlygiventhevarietyoftreesgenerated
byVAE.Incontrast,theICDmetricensuresbothdiversityandacloseralignmentwiththeactualbehaviormodel. This
reinforcesourobservationthatVAE-generatedpolicytreesexhibitdiversity,posingachallengeforMDFinrecognizing
andselectingpotentialrealbehaviormodels. Whenpresentedwithabroadarrayofpolicytrees,theMDFmetricfinds
itdifficulttodiscernwhichonescloselyalignwiththetruedistributionofagentj’sbehaviors.
5.2.2 ComparisonResultsofMultipleI-DIDAlgorithms
Weassessmodelperformancebycomparingtheaveragerewardsacrossmultiplemodels. Foragentitoimproveits
decisions,itmustpredictagentj’sactions. WearetailoringanI-DIDmodelforagentiandbuildingamodelspaceM
j
foragentj usingvariousI-DIDtechniques. Theobjectiveistomeasuretheaccuracyofthesemethodsinmodelingand
predictingagentj’sbehavior,therebyinfluencingagenti’stargetinterceptionsuccess.
Figs. 14 and 15 compare the average rewards in a multi-agent UAV setting. Among the various methods tested,
includingmultipleIDIDtechniques,IDID-VAE-ICDstandsout,outperformingtheoriginalI-DIDandotherapproaches.
InFigs.14(T=3)and 15(T=3),IDID-G,IDID-VAE-BCELoss,andIDID-VAE-ICDsurpassothermodelsdueto
theircapabilitytogeneratediversepolicytrees,comprehensivelycoveringthetruemodel. Notably,inFigs.14(T=
4)and 15(T=4),unlikeIDID-GAandIDID-VAE-BCELoss,IDID-VAE-ICDshinesinpolicytreegenerationand
selection. Thissuccessispartlyattributedtothelossfunctionweproposed,whichassignsgreaterweighttotheearly
nodesofthepolicytreeinlong-termdecision-making,andpartlyattributedtotheICD,theconstructionindexofthe
policytreeset. TheVAEmodel,combinedwiththeICDindexandourpolicytreelossfunction,effectivelyconstructs
theauthenticsubspacewithinthevastpolicytreespace,demonstratingitsabilitytoaccuratelygenerateagentj’strue
behaviorsfromhistoricalinteractivedata.
16
ytisreviD ytisreviDVariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
70 IDID-VAE-MDF IDID-VAE-MDF
IDID-VAE-ICD 60 IDID-VAE-ICD
60
50
50
40
40
30
30
20
20
10
10
0
0
-10
-10
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
d d
(a) T=3 (b) T=4
Figure 13: For (a) T = 3 and (b) T = 4, the correlation between the average rewards obtained by agent i and the
correspondingmetrics(MDFandICD)referstotherelationshipbetweentherewardsobtainedusingdifferentmodels,
namelyIDID-VAE-MDFandIDID-VAE-ICD.
30
IDID IDID
IDID-MDF 60 IDID-MDF
20
IDID-VAE-MDF IDID-VAE-MDF
IDID-RANDOM IDID-RANDOM
10 40
0
20
10
0
20
1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10
Agent j's Behaviours Agent j's Behaviours
(a) T=3 (b) T=4
Figure14: Theaveragerewardsarereceivedbythesubjectagentiwithagentj’sbehaviorsmodels(generatedbyIDID,
IDID-MDF,IDID-VAE-MDFandIDID-Random)for(a)T =3and(b)T =4.
5.3 ExperimentalSummaryandDiscussions
AftercomparingIDID-VAE-ICDandIDID-VAE-MDF,wefindthatwithintheVAEframework,evaluatingpolicy
tree diversity using MDF is less effective. This is because VAE can generate more novel paths, maintaining high
diversityamongnewtrees,whichisafeatureabsentinthepreviousmethods. ICDleveragestheinformationfrom
theVAE-basedpolicytreegenerationprocess,allowingforanintuitivecomparisonofdifferentpolicytrees’abilityto
representhistoricalbehaviorcharacteristics.
OurexperimentsrevealthattheVAEframeworkoffersalinearrelationshipbetweentreegenerationspeedandplanning
range,providingthespeedincreaseoverthepreviousmethods,asdepictedinTable1. AlthoughIDID-MDF,IDID-VAE-
ICD,andIDID-GAtakelongerthanthebasicIDIDduetotheiterationsforthenewmodelgeneration,IDID-VAE-ICD
provesmoreefficientthanIDID-GA.However,sinceallthesealgorithmsoperateoffline,theirefficiencydoesn’thinder
theoverallperformance. Additionally,IDID-VAE-ICDoutperformsIDID-VAE-ICD-BCELossthankstoitscustomized
lossfunctiondesignedforpolicytreenodeimportancedistribution.
ComprehensiveexperimentsshowthatbothIDID-MDFandIDID-VAE-ICDoutperformIDID-GAandIDIDinmost
cases. IDID-VAE-ICDconsistentlyperformswellduetotheVAE’sabilitytobalancecoherenceandindividualityin
17
sdraweR
egarevA
sdraweR
egarevA
sdraweR
egarevA
sdraweR
egarevAVariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
250
IDID-VAE-MDF IDID-VAE-MDF
80
IDID-VAE-ICD IDID-VAE-ICD
IDID-GA 200 IDID-GA 60
IDID-VAE-BCELoss IDID-VAE-BCELoss
150
40
100
20
50
0
0
20
1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10
Agent j's Behaviours Agent j's Behaviours
(a) T=3 (b) T=4
Figure15: Theaveragerewardsarereceivedbythesubjectagentiwithagentj’sbehaviorsmodels(generatedby
IDID-VAE-MDF,IDID-VAE-ICD,IDID-GAandIDID-VAE-BCELoss)for(a)T =3and(b)T =4.
behaviors,whileIDID-MDFtendstoproducemonotonicbehaviors. Althoughfindingoptimalparametervaluesremains
crucial,theVAE-basedI-DIDsolutionshavedemonstratedpotentialinmodelingunexpectedbehaviorsthatclassicalAI
approachescan’tachieve. DespitetherandomnessintheVAEnetwork,experimentalresultsshowstableperformance
intermsofaveragerewards,supportedbyouranalysis. Overall,IDID-VAE-ICDexhibitsstrongadaptabilitytocomplex
behavioralpatterns,maintainingstableperformanceinuncertainenvironments,pavingthewayforfutureintelligent
decision-makingsystems.
Table1: Runningtimes (sec)forallthefourmethodsinthetwoproblemdomainsgenerating,specificallyfocusingon
∼
generatingM =10policytrees.
Domain Tiger UAV
T 3 4 3 4
IDID 0.26 32.41 8.16 406.57
IDID-GA 8.8 45.1 27 637.18
IDID-MDF 442.94 573.85 81.96 97.40
IDID-VAE-ICD 2.09 5.36 9.23 41.84
6 Conclusion
Weexploredneuralcomputing-basedI-DIDmethodstoaddressunpredictableagentbehaviorsinplanningresearch.
Using a VAE approach, we overcame the challenge of reusing incomplete policy trees from interactive data. The
VAEmodelnotonlygeneratesnumerousnewpolicytrees,bothcompleteandincomplete,fromalimitedsetofinitial
examples,butalsooffersflexibilityinproducingvaryingdegreesofdeviation. Thisallowsforthecreationofabroader
rangeofpolicytrees. Furthermore,theintegrationofanovelperplexity-basedmetricwithinourVAE-basedapproach
hassignificantlyenhancedthediversityoftheoverallpolicytreeensemble. Bymaximizingthecoverageofagentj’s
truepolicymodels,thenewmethoddemonstratespromisingpotentialforaccuratelyapproximatingrealagentbehavior
modelsfromhistoricaldata. FollowingtheVAEtrainingwithaselectionofincompletepolicytrees,thisdata-driven
approachhasthecapabilitytobeappliedtoincreasinglydiverseandcomplexdomains,highlightingitsadaptabilityand
robustness.
Thisworkrepresentsasignificantcontributiontothefieldofagentplanningresearch,providingavaluableframework
forhandlingtheunpredictablebehaviorsofotheragentsinmulti-agentsystems. Italsoopensthedoortoexploitneural
networksbasedapproachesforaddressingtheI-DIDchallenge,whichhasbeenaddressedthroughtraditionalBayesian
approachinthepastdecade. Inthefuture,wewillimprovetheefficiency,accuracy,andinterpret-abilityofourmodels,
thereby increasing their adaptability to the uncertainties of online interactive environments. This will be achieved
throughadvancementsindeeplearningarchitecturesandtheincorporationofexplainableAItechniques.
18
sdraweR
egarevA
sdraweR
egarevAVariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
7 Acknowledgments
ThisworkissupportedinpartbytheNationalNaturalScienceFoundationofChina(GrantsNo.62176225,62276168
and61836005)andtheNaturalScienceFoundationofFujianProvince,China(GrantNo. 2022J05176)andGuangdong
Province,China(GrantNo. 2023A1515010869).
Appendix
A:[TheFourOperatorsinReconstructinganIncompletePolicyTree]Wepresentthepseudo-codetoimplement
thefouroperatorsinreconstructinganincompletepolicytree. TheoperatorsaredescribedinSection4.1andinAlg.2.
AsdepictedinAlg.3,thekeystepsofeachoperatorareasfollows:
• SplitOperator: Thesplitoperator dividesasequenceintomultiplesub-sequencesbasedonafixedlength,
S
andstoresthemalongwiththeirprobabilitiesintheformofaset.
– Initially,anemptysetHT isinitializedtostorepolicypathsandtheirassociatedprobabilities(line4).
– Thegivenaction-observationsequenceisthensegmentedintomultiplesub-sequencesoffixedlength(line
5).
– Thesesub-sequences, alongwiththeirprobabilities, arestoredinthesetHT (lines6-9), providinga
structuredrepresentationofpolicypaths.
• UnionOperator: Theunionoperator combinestheelementsofthegivensetofpolicypathsintoasetof
U
setsofpolicytreesbasedonthegroupingoftheirrootnodeactionsandobservationpairs.
– PolicypathsubsetsHT aredefinedandtheirprobabilitiescomputed,whereeachsubsetbeginswitha
ao
specificactionaandobservationsequenceo(lines15-16).
– For every possible observation sequence o that follows action a, the corresponding subsets HT are
ao
collected(lines14-16).
– SetsHTarethenconstructed,encompassingallpolicypathsthatstartwithaparticularactiona(line17).
a
– TheoverallprobabilityP(HT)ofeachHTisdeterminedbysummingtheprobabilitiesofitsconstituent
a a
subsetsHT (line18).
ao
– CombiningthesetsHTandtheirprobabilitiesforallactionsa Ayieldsthecomprehensivesetofpolicy
pathsHT(lines13-19a
).
∈
– ThecomprehensivesetHTrepresentsallpotentialpolicytreesofdepthT thatcanbederivedfromthe
originalinteractionsequencehL(line20).
• RouletteOperator: Therouletteoperator randomlyselectsanelementfromagivenset.
R
– Given a set A of elements (a,p) where p is the probability associated with a, a random number p
r
between0and1isgenerated(line24).
– Arunningsump ismaintainedtotrackthecumulativeprobabilitiesencounteredwhileiteratingthrough
c
thesetA(line25).
– Assoonasp exceedsorequalsp ,thecorrespondingelementaisreturned,effectivelyselectingan
c r
elementbasedonitsassociatedprobability(lines26-31).
• GraphingOperator:Thegraphingoperator convertsthesetofpathsfromthepolicytreeintoatreestructure
G
withinagraphmodel.
– AnedgesetE isconstructedtostorethesub-pathswithinthepolicytree(lines35-41).
– DuplicatesareremovedfromE,ensuringthatedgeswiththesametimeslice,nodevalue(action),and
edgeweight(observation)arenotrepresentedmultipletimes(line42).
– UniquenodesfromthededuplicatededgesetE arethenextractedandcompiledintoanodesetV (line
44).
– Finally,adirectedgraphG(E,V)isgeneratedandplotted,representingthepolicytreeconstructedfrom
theedgeandnodesets(line45).
B:[TheOperatorsinencodinganddecodinganIncompletePolicyTree]TheZZOHencoderanddecoderoperators
aredescribedinSection4.2.1andinAlg.2. Theoperatorsareusedtoperformazigzagtransformationontheactionsof
agivenpolicytree,encodingthemintoabinaryformatusingone-hotencodingtoformacolumnvector. Conversely,it
canalsodecodeagivencolumnvectorfromone-hotencodingbackintoactions,subsequentlyreconstructingthepolicy
tree. AsdepictedinAlg.5,thekeystepsofeachoperatorareasfollows:
19VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
Algorithm3:TheFourOperatorsinReconstructinganIncompletePolicyTree
1 ◁splitoperator
2 Function (hL,T) ◁HT ThL:
3
a1o1,aS 2o2...at,ot,...aLo← L S hL
←
4
HT
←∅
5 {hT }←{a(l−1)T+1o(l−1)T+1,...,alTolT }l⌊ =L T 1⌋
6 forhT hT do
7 P(h∈T){ ←} #(hT a)T/L
8
◁#(hT a)indicatesthenumberoftimeshTappearsinthesequencecountthetimesofhL
HT HT(cid:83) (hT,P )
hT
←
9 end
10
returnHT
11 ◁unionoperator
12 Function (HT) ◁HT HT:
U ←U
13 fora Ado
14 fo∈ ro ΩT−1do
15 H∈ aT o ←(cid:83) (hT ao,P(hT ao))∈HT(hT ao,P(hT ao))P(H aT o) ←(cid:80) (hT ao,P(hT ao))∈HTP(hT ao)
16 end
1 17
8
H P(T a H←
T
a)(cid:83) ←o∈ (cid:80)ΩT o− ∈1 Ω( TH −T a 1o P,P (H(H
T
aoT a )o))
19 end
20 HT ←(cid:83) a∈A(HT a,P(HT a))
21
returnHT
22 ◁rouletteoperator
23 Function (A) ◁a A:
R ←R
24 p r random()
←
25 p c 0
←
26 for(a,p) Ado
∈
27 p c p c+p
←
28 ifp r <=p cthen
29 return a
30 end
31 end
32 returna
33 ◁Graphingoperator
34 Function G( HaT) ◁Tree ←GHaT:
35 E
←∅
36 for(hT,P(hT)) T do
37
a1o1,a2o2..∈ .atH ,ot,...aToT hT
←
38 fort 1,2, T 1 do
39
E∈{ E(cid:83)· (· o·t :a−t,a}t+1)
←
40 end
41 end
42 E Deduplicate(E)
←
43 ◁deduplicatetheedgewithsametimeslice,samenodevalue(action)andedgeweight(observation)
44 V Node(E))
←
45 Tree G(E,V)
←
46 returnTree
20VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
Algorithm4:One-HotEncodingOperator
1 ◁One-hotencoding
2 Function (˜x) ◁x (˜x):
I ←I
3 x
R(|Aj|+1)|Ω |Ω|T |−− 11
∈
4 forl 1,2,
|Ω|T−1
do
∈{ ··· |Ω|−1 }
5 v ˜x[(l 1)(A j +1)+1:(A j +1)l]
← − | | | |
6 k ←argmax k∈{1,2,···|Aj|+1}v[k]
7 x[(l 1)(A j +1)+1:(A j +1)l] A˜ j[k]
− | | | | ←
8 end
9 returnx
• ZZOHEncoderOperator:
– Initially,weextracttheactionsequencefromthegivenpolicytree T (lines3-12).
H
– Subsequently,foreachactioninthesequence,wegenerateacorrespondingone-hotbinarycoderepre-
sentationp(lines13-16). Thisone-hotencodingisdesignedspecificallyforpolicytrees,utilizingthe
Zig-Zagencodingtechnique.
• ZZOHDecoderOperator:
– Initially,givenaone-hotbinarycoderepresentationx,wedecodeitbackintoanactionsequencep(lines
20-26).ThedecodingprocessreversestheZig-Zagencoding,accuratelyreconstructingtheoriginalaction
sequence.
– Subsequently,usingthedecodedactionsequencep,wereconstructthepolicytree T (lines27-40). This
H
reconstructionensuresthatthegeneratedtreecloselymatchestheoriginaltree,capturingitsstructureand
behavior.
TheOne-hotencodingoperatorisdescribedinSection4.3.1andinAlg.2. Theone-hotoperator usedtoconvertthe
I
summaryvectoroutputbytheVAEnetworkintoabinaryencodingformatthatmatchestheone-hotencodingofthe
policytree. AsdepictedinAlg.4,thekeystepsofeachoperatorareasfollows: Initializeavectorxtorepresentthe
one-hotencodingofthepolicytree(line3). Then,foreachnodeinthepolicytree(lines4-8).Locatetheindexwiththe
maximumvalueinthecorrespondingsub-binaryrepresentation(line6). Representthatsub-binaryencodingusingthe
enlargedactionspaceA˜ (line7).
j
21VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
Algorithm5:ZZOHEncoder&DecoderOperators
1 ◁ZZOHencoderoperator
2 Function ( T) ◁x T:
3 ◁genZ eratH eactionse← quZ enH ceoftree T
H
4 x
R(|Aj|+1)|Ω |Ω|T |−− 11
∈
5 p ←{a l |a l ←0, ∀l ∈{1,2, ···|Ω |Ω|T |−− 11
}}
6 for(hT,i) T do
7
a1o1,a2∈ o2H ...at,ot,...aToT hT
←
8 fort 1,2, T do
∈{ ··· }
9 l |Ω|(t−1)−1 + i−1 +1
← |Ω|−1 ⌊|Ω|(t−1)⌋
10 p[l] at
←
11 end
12 end
13 ◁generateone-hotbinarycodeofactionsequencep
14 forl 1,2,
|Ω|T−1
do
∈{ ··· |Ω|−1 }
15 x[(l −1)( |A j |+1)+1:( |A j |+1)l] ←A˜c j[p[l]]
16 end
17 returnx
18 ◁ZZOHdecoderoperator
19 Function (x) ◁ T x:
Z H ←Z
20 ◁generateactionsequencepofone-hotbinarycodex
21 p ←{a l |a l ←0, ∀l ∈{1,2, ···|Ω |Ω|T |−− 11
}}
22 forl 1,2,
|Ω|T−1
do
∈{ ··· |Ω|−1 }
23 v ←A˜c j ·x[(l −1)( |A j |+1)+1:( |A j |+1)l]
24 k ←argmax k∈{1,2,···|Aj|+1}v[k]
25 p[l] A˜ j[k]
←
26 end
27 ◁generatepolicytreefromactionsequencep
28
T (cid:83) hT
H ←
29 for(hT,i) T do
30
a1o1,a2∈ o2H ...at,ot,...aToT hT
←
31 fort 1,2, T do
∈{ ··· }
32 l |Ω|(t−1)−1 + i−1 +1
← |Ω|−1 ⌊|Ω|(t−1)⌋
33 at p[l]
←
34 ift=T then
35
̸
k
(i−1)|Ω|t
+1
←⌊|Ω|(T−1) ⌋
36 ot+1 o k
←
37 end
38 end
39
hT a1o1,a2o2...at,ot,...aToT
←
40 end
41 return T
H
22VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
References
[1] Pan,Yinghui,Zhang,Hanyi,Zeng,Yifeng,Ma,Biyang,Tang,Jing,andMing,Zhong."DiversifyingAgent’s
BehaviorsinInteractiveDecisionModels."Int.J.Intell.Syst.37,no.12(2022): 12035-12056.
[2] Ross Conroy, Yifeng Zeng, Marc Cavazza, and Yingke Chen. "Learning Behaviors in Agents Systems with
InteractiveDynamicInfluenceDiagrams."InIJCAI,2015,pp.39–45.
[3] Wang, Xindi, Liu, Hao, andGao, Qing."Data-Driven DecisionMakingand Near-Optimal PathPlanning for
MultiagentSysteminGames."IEEE-J-MASS4,no.3(2023): 320-328.
[4] RonaldBuijsse,MartijnWillemsen,andChrisSnijders."Data-DrivenDecision-Making."InDataSciencefor
Entrepreneurship: PrinciplesandMethodsforDataEngineering,Analytics,Entrepreneurship,andtheSociety.
SpringerInternationalPublishing,Cham,2023,pp.239–277.
[5] LiAn,VolkerGrimm,YuBai,AbigailSullivan,B.L.Turner,NicolasMalleson,AlisonHeppenstall,Christian
Vincenot,DerekRobinson,XinyueYe,JianguoLiu,EmilieLindkvist,andWenwuTang."Modelingagentdecision
andbehaviorinthelightofdatascienceandartificialintelligence."Environ.Modell.Softw.166(2023): 105713.
[6] Pan,Yinghui,Ma,Biyang,Zeng,Yifeng,Tang,Jing,Zeng,Buxin,andMing,Zhong."AnEvolutionaryFramework
forModellingUnknownBehavioursofOtherAgents."IEEETrans.EmergingTop.Comput.Intell.7,no.4(2023):
1276-1289.
[7] YinghuiPan,JingTang,BiyangMa,Yi-fengZeng,andZhongMing."Towarddata-drivensolutionstointeractive
dynamicinfluencediagrams."Knowl.Inf.Syst.63,no.9(2021): 2431-2453.
[8] Nascimento,Nathalia,PauloAlencar,andDonaldCowan."Self-AdaptiveLargeLanguageModel(LLM)-Based
MultiagentSystems."In2023IEEEACSOS-C,2023,pp.104-109.
[9] Wason, Ritika, ParulArora, DevanshArora, JasleenKaur, SunilPratapSingh, andM.N.Hoda."Appraising
SuccessofLLM-basedDialogueAgents."In2024IEEEINDIACom,2024,pp.1570-1573.
[10] Lu, Jiaying, Bo Pan, Jieyi Chen, Yingchaojie Feng, Jingyuan Hu, Yuchen Peng, and Wei Chen. "AgentLens:
VisualAnalysisforAgentBehaviorsinLLM-basedAutonomousSystems."IEEETrans.VisualComput.Graphics,
2024,pp.1-17.
[11] Zhang,Xijia,YueGuo,SimonStepputtis,KatiaSycara,andJosephCampbell."ExplainingAgentBehaviorwith
LargeLanguageModels."arXiv,2023,eprint2309.10346.
[12] Berger,Uta,AndrewBell,C.MichaelBarton,EmileChappin,GunnarDreßler,TatianaFilatova,ThibaultFronville,
AllenLee,EmielvanLoon,IrisLorscheid,MatthiasMeyer,BirgitMüller,CyrilPiou,ViktoriiaRadchuk,Nicholas
Roxburgh,LennartSchüler,ChristianTroost,NandaWijermans,TimG.Williams,Marie-ChristinWimmler,and
VolkerGrimm."Towardsreusablebuildingblocksforagent-basedmodellingandtheorydevelopment."Environ.
Modell.Softw.,vol.175,2024,p.106003.
[13] Zhang, Wei, Andrea Valencia, and Ni-Bin Chang. "Synergistic Integration Between Machine Learning and
Agent-BasedModeling: AMultidisciplinaryReview."IEEETrans.NeuralNetworksLearn.Syst.,vol.34,no.5,
2023,pp.2170-2190.
[14] Hazra, Rishi, andLucDeRaedt."DeepExplainableRelationalReinforcementLearning: ANeuro-Symbolic
Approach."InMachineLearningandKnowledgeDiscoveryinDatabases: ResearchTrack,2023,pp.213–229.
Cham: SpringerInternationalPublishing.
[15] Belle, Vaishak, Michael Fisher, Alessandra Russo, Ekaterina Komendantskaya, and Alistair Nottle. "Neuro-
Symbolic AI + Agent Systems: A First Reflection on Trends, Opportunities and Challenges." In AAMAS
Workshops,2024,pp.180–200.Cham: SpringerNatureSwitzerland.
[16] Kononov, Roman, andOlegMaslennikov."Performingdecision-makingtasksthroughdynamicsofrecurrent
neuralnetworkstrainedwithreinforcementlearning."InProc.2023DCNA,2023,pp.144-147.
[17] Wang, Xu, Sen Wang, Xingxing Liang, Dawei Zhao, Jincai Huang, Xin Xu, Bin Dai, and Qiguang Miao.
"DeepReinforcementLearning: ASurvey."IEEETrans.NeuralNetworksLearn.Syst.,vol.35,no.4,2024,pp.
5064-5078.
[18] Almaeen,Manal,YasirAlanazi,NobuoSato,W.Melnitchouk,MichelleP.Kuchera,andYaohangLi."Variational
AutoencoderInverseMapper: AnEnd-to-EndDeepLearningFrameworkforInverseProblems."InProc.2021
IJCNN,2021,pp.1-8.
[19] Hu,Tianmeng,BiaoLuo,ChunhuaYang,andTingwenHuang."MO-MIX:Multi-ObjectiveMulti-AgentCooper-
ativeDecision-MakingWithDeepReinforcementLearning."IEEETrans.PatternAnal.Mach.Intell.,vol.45,no.
10,Oct.2023,pp.12098–12112.
23VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
[20] SenYang,YiZhang,XinzhengLu,WeiGuo,andHuiquanMiao."Multi-agentdeepreinforcementlearningbased
decisionsupportmodelforresilientcommunitypost-hazardrecovery."Reliab.Eng.Syst.Safe.,vol.242,2024,p.
109754.
[21] Le,Ngan,VidhiwarSinghRathour,KashuYamazaki,KhoaLuu,andMariosSavvides."Deepreinforcement
learningincomputervision: acomprehensivesurvey."Artif.Intell.Rev.,2022,pp.1–87.
[22] Zhu,Kai,andTaoZhang."Deepreinforcementlearningbasedmobilerobotnavigation: Areview."Tsinghua
ScienceandTechnology,vol.26,no.5,2021,pp.674–691.
[23] Wang,Xiangjun,JunxiaoSong,PenghuiQi,PengPeng,ZhenkunTang,WeiZhang,WeiminLi,XiongjunPi,
JujieHe,ChaoGao,andothers."SCC:Anefficientdeepreinforcementlearningagentmasteringthegameof
StarCraftII."InProc.2021ICML,2021,pp.10905–10915.
[24] Zeng,Yifeng,andPrashantDoshi."ExploitingModelEquivalencesforSolvingInteractiveDynamicInfluence
Diagrams."J.Artif.Int.Res.,vol.43,no.1,Jan.2012,pp.211–255.
[25] Pan, Yinghui, YifengZeng, YanpingXiang, LeSun, andXuefengChen."Time-CriticalInteractiveDynamic
InfluenceDiagram."Int.J.Approx.Reasoning,vol.57,no.C,Feb.2015,pp.44–63.
[26] Pan, Yinghui, Yifeng Zeng, and Hua Mao. "Learning Agents’ Relations in Interactive Multiagent Dynamic
InfluenceDiagrams."InADMI2014.LNCS,2015,pp.1–11.Cham: SpringerInternationalPublishing.
[27] StefanoV.AlbrechtandPeterStone."Autonomousagentsmodellingotheragents: Acomprehensivesurveyand
openproblems."arXiv,2017,vol.abs/1709.08071.
[28] MasoumehTabaehIzadi."SequentialDecisionMakingUnderUncertainty."InAbstraction,Reformulationand
Approximation,2005,pp.360–361.Berlin,Heidelberg: SpringerBerlinHeidelberg.
[29] Zeng, Yifeng, Prashant Doshi, Yingke Chen, Yinghui Pan, Hua Mao, and Muthukumaran Chandrasekaran.
"ApproximatingBehavioralEquivalenceforScalingSolutionsofI-DIDs."Knowl.Inf.Syst.,vol.49,no.2,Nov.
2016,pp.511–552.
[30] GeoffreyE.HintonandRuslanR.Salakhutdinov."ReducingtheDimensionalityofDatawithNeuralNetworks."
Science,vol.313,no.5786,2006,pp.504–507.
[31] DiederikP.KingmaandMaxWelling."Auto-EncodingVariationalBayes."CoRR,abs/1312.6114,2013.
[32] Zeng,Yifeng,andPrashantDoshi."ExploitingModelEquivalencesforSolvingInteractiveDynamicInfluence
Diagrams."J.Artif.Intell.Res.,vol.43,2012,pp.211-255.
[33] PrashantDoshi,PiotrJ.Gmytrasiewicz,andEdmundH.Durfee."Recursivelymodelingotheragentsfordecision
making: Aresearchperspective."Artif.Intell.,vol.279,2020.
[34] PrashantDoshi,YifengZeng,andQiongyuChen."GraphicalModelsforInteractivePOMDPs: Representations
andSolutions."Auton.AgentMulti-ag.,vol.18,no.3,2009,pp.376-416.
[35] GmytrasiewiczP.J.andDoshiP."AFrameworkforSequentialPlanninginMultiagentSettings."J.Artif.INntell.
Res.,vol.24,2005,pp.49-79.
[36] Seuken,S.,andS.Zilberstein."Formalmodelsandalgorithmsfordecentralizeddecisionmakingunderuncer-
tainty."Auton.AgentMulti-ag.,vol.17,no.2,2008,pp.190–250.
[37] Chen,Yingke,PrashantDoshi,andYifengZeng."IterativeOnlinePlanninginMultiagentSettingswithLimited
ModelSpacesandPACGuarantees."InAAMAS’15,2015,pp.1161–1169.Richland,SC:AAAIPress.
[38] MingDing."TheroadfromMLEtoEMtoVAE:Abrieftutorial."AIOpen,vol.3,2021,pp.29-34.
[39] Rizk,Yara,MarietteAwad,andEdwardW.Tunstel."DecisionMakinginMultiagentSystems: ASurvey."IEEE
Trans.Cognit.Dev.Syst.,vol.10,no.3,2018,pp.514-529.
[40] Chen,Wenbin,GuoXie,WenjiangJi,RongFei,XinhongHei,SiyuLi,andJialinMa."DecisionMakingfor
OvertakingofUnmannedVehicleBasedonDeepQ-learning."In2021IEEEDDCLS,2021,pp.350-353.
[41] Wang, Xindi, Hao Liu, and Qing Gao. "Data-Driven Decision Making and Near-Optimal Path Planning for
MultiagentSysteminGames."IEEEJ.MiniaturizationAirSpaceSyst.,vol.4,no.3,2023,pp.320-328.
[42] Sun,Changyin,WenzhangLiu,andLuDong."ReinforcementLearningWithTaskDecompositionforCooperative
MultiagentSystems."IEEETrans.NeuralNetworksLearn.Syst.,vol.32,no.5,2021,pp.2054-2065.
[43] Habib, Maki K., Samuel A. Ayankoso, and Fusaomi Nagata. "Data-Driven Modeling: Concept, Techniques,
ChallengesandaCaseStudy."In2021IEEEICMA,2021,pp.1000-1007.
24VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
[44] Abroshan,Mahed,KaiHouYip,CemTekin,andMihaelavanderSchaar."ConservativePolicyConstruction
UsingVariationalAutoencodersforLoggedDataWithMissingValues."IEEETrans.NeuralNetworksLearn.
Syst.,vol.34,no.9,2023,pp.6368-6378.
[45] Conroy, Ross, Yifeng Zeng, Marc Cavazza, and Yingke Chen. "Learning Behaviors in Agents Systems with
InteractiveDynamicInfluenceDiagrams."InProc.2015IJCAI,2015,pp.39–45.BuenosAires,Argentina: AAAI
Press.
[46] Zeng,Yifeng,HuaMao,YinghuiPan,andJianLuo."ImprovedUseofPartialPoliciesforIdentifyingBehavioral
Equivalence."InProc.2012AAMAS,vol.2,2012,pp.1015–1022.Richland,SC:AAAIPress.
[47] Doshi,Prashant,YifengZeng,andQiongyuChen."GraphicalmodelsforinteractivePOMDPs: representations
andsolutions."AutonomousAgentsandMulti-AgentSystems,vol.18,no.3,June2009,pp.376–416.
[48] Zeng, Yifeng, Ran, Qiang, Ma, Biyang, and Pan, Yinghui. "Modelling other agents through evolutionary be-
haviours."Memet.Comput.14,no.1(2022): 19–30.
25