[
    {
        "title": "Continuously Improving Mobile Manipulation with Autonomous Real-World RL",
        "authors": "Russell MendoncaEmmanuel PanovBernadette BucherJiuguang WangDeepak Pathak",
        "links": "http://arxiv.org/abs/2409.20568v1",
        "entry_id": "http://arxiv.org/abs/2409.20568v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20568v1",
        "summary": "We present a fully autonomous real-world RL framework for mobile manipulation\nthat can learn policies without extensive instrumentation or human supervision.\nThis is enabled by 1) task-relevant autonomy, which guides exploration towards\nobject interactions and prevents stagnation near goal states, 2) efficient\npolicy learning by leveraging basic task knowledge in behavior priors, and 3)\nformulating generic rewards that combine human-interpretable semantic\ninformation with low-level, fine-grained observations. We demonstrate that our\napproach allows Spot robots to continually improve their performance on a set\nof four challenging mobile manipulation tasks, obtaining an average success\nrate of 80% across tasks, a 3-4 improvement over existing approaches. Videos\ncan be found at https://continual-mobile-manip.github.io/",
        "updated": "2024-09-30 17:59:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20568v1"
    },
    {
        "title": "MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning",
        "authors": "Haotian ZhangMingfei GaoZhe GanPhilipp DufterNina WenzelForrest HuangDhruti ShahXianzhi DuBowen ZhangYanghao LiSam DodgeKeen YouZhen YangAleksei TimofeevMingze XuHong-You ChenJean-Philippe FauconnierZhengfeng LaiHaoxuan YouZirui WangAfshin DehghanPeter GraschYinfei Yang",
        "links": "http://arxiv.org/abs/2409.20566v1",
        "entry_id": "http://arxiv.org/abs/2409.20566v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20566v1",
        "summary": "We present MM1.5, a new family of multimodal large language models (MLLMs)\ndesigned to enhance capabilities in text-rich image understanding, visual\nreferring and grounding, and multi-image reasoning. Building upon the MM1\narchitecture, MM1.5 adopts a data-centric approach to model training,\nsystematically exploring the impact of diverse data mixtures across the entire\nmodel training lifecycle. This includes high-quality OCR data and synthetic\ncaptions for continual pre-training, as well as an optimized visual\ninstruction-tuning data mixture for supervised fine-tuning. Our models range\nfrom 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE)\nvariants, and demonstrate that careful data curation and training strategies\ncan yield strong performance even at small scales (1B and 3B). Additionally, we\nintroduce two specialized variants: MM1.5-Video, designed for video\nunderstanding, and MM1.5-UI, tailored for mobile UI understanding. Through\nextensive empirical studies and ablations, we provide detailed insights into\nthe training processes and decisions that inform our final designs, offering\nvaluable guidance for future research in MLLM development.",
        "updated": "2024-09-30 17:59:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20566v1"
    },
    {
        "title": "DressRecon: Freeform 4D Human Reconstruction from Monocular Video",
        "authors": "Jeff TanDonglai XiangShubham TulsianiDeva RamananGengshan Yang",
        "links": "http://arxiv.org/abs/2409.20563v1",
        "entry_id": "http://arxiv.org/abs/2409.20563v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20563v1",
        "summary": "We present a method to reconstruct time-consistent human body models from\nmonocular videos, focusing on extremely loose clothing or handheld object\ninteractions. Prior work in human reconstruction is either limited to tight\nclothing with no object interactions, or requires calibrated multi-view\ncaptures or personalized template scans which are costly to collect at scale.\nOur key insight for high-quality yet flexible reconstruction is the careful\ncombination of generic human priors about articulated body shape (learned from\nlarge-scale training data) with video-specific articulated \"bag-of-bones\"\ndeformation (fit to a single video via test-time optimization). We accomplish\nthis by learning a neural implicit model that disentangles body versus clothing\ndeformations as separate motion model layers. To capture subtle geometry of\nclothing, we leverage image-based priors such as human body pose, surface\nnormals, and optical flow during optimization. The resulting neural fields can\nbe extracted into time-consistent meshes, or further optimized as explicit 3D\nGaussians for high-fidelity interactive rendering. On datasets with highly\nchallenging clothing deformations and object interactions, DressRecon yields\nhigher-fidelity 3D reconstructions than prior art. Project page:\nhttps://jefftan969.github.io/dressrecon/",
        "updated": "2024-09-30 17:59:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20563v1"
    },
    {
        "title": "SpaceMesh: A Continuous Representation for Learning Manifold Surface Meshes",
        "authors": "Tianchang ShenZhaoshuo LiMarc LawMatan AtzmonSanja FidlerJames LucasJun GaoNicholas Sharp",
        "links": "http://dx.doi.org/10.1145/3680528.3687634",
        "entry_id": "http://arxiv.org/abs/2409.20562v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20562v1",
        "summary": "Meshes are ubiquitous in visual computing and simulation, yet most existing\nmachine learning techniques represent meshes only indirectly, e.g. as the level\nset of a scalar field or deformation of a template, or as a disordered triangle\nsoup lacking local structure. This work presents a scheme to directly generate\nmanifold, polygonal meshes of complex connectivity as the output of a neural\nnetwork. Our key innovation is to define a continuous latent connectivity space\nat each mesh vertex, which implies the discrete mesh. In particular, our vertex\nembeddings generate cyclic neighbor relationships in a halfedge mesh\nrepresentation, which gives a guarantee of edge-manifoldness and the ability to\nrepresent general polygonal meshes. This representation is well-suited to\nmachine learning and stochastic optimization, without restriction on\nconnectivity or topology. We first explore the basic properties of this\nrepresentation, then use it to fit distributions of meshes from large datasets.\nThe resulting models generate diverse meshes with tessellation structure\nlearned from the dataset population, with concise details and high-quality mesh\nelements. In applications, this approach not only yields high-quality outputs\nfrom generative models, but also enables directly learning challenging geometry\nprocessing tasks such as mesh repair.",
        "updated": "2024-09-30 17:59:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20562v1"
    },
    {
        "title": "LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and Planning with LM-Driven PDDL Planner",
        "authors": "Xiaopan ZhangHao QinFuquan WangYue DongJiachen Li",
        "links": "http://arxiv.org/abs/2409.20560v1",
        "entry_id": "http://arxiv.org/abs/2409.20560v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20560v1",
        "summary": "Language models (LMs) possess a strong capability to comprehend natural\nlanguage, making them effective in translating human instructions into detailed\nplans for simple robot tasks. Nevertheless, it remains a significant challenge\nto handle long-horizon tasks, especially in subtask identification and\nallocation for cooperative heterogeneous robot teams. To address this issue, we\npropose a Language Model-Driven Multi-Agent PDDL Planner (LaMMA-P), a novel\nmulti-agent task planning framework that achieves state-of-the-art performance\non long-horizon tasks. LaMMA-P integrates the strengths of the LMs' reasoning\ncapability and the traditional heuristic search planner to achieve a high\nsuccess rate and efficiency while demonstrating strong generalization across\ntasks. Additionally, we create MAT-THOR, a comprehensive benchmark that\nfeatures household tasks with two different levels of complexity based on the\nAI2-THOR environment. The experimental results demonstrate that LaMMA-P\nachieves a 105% higher success rate and 36% higher efficiency than existing\nLM-based multi-agent planners. The experimental videos, code, and datasets of\nthis work as well as the detailed prompts used in each module are available at\nhttps://lamma-p.github.io.",
        "updated": "2024-09-30 17:58:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20560v1"
    }
]