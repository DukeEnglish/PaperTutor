TechnicalReport
Qwen2.5-Coder Technical Report
BinyuanHui* JianYang* ZeyuCui* JiaxiYang*
DayihengLiu LeiZhang TianyuLiu JiajunZhang BowenYu KaiDang AnYang
RuiMen FeiHuang XingzhangRen XuanchengRen JingrenZhou JunyangLin†
QwenTeam AlibabaGroup
https://hf.co/Qwen/Qwen2.5-Coder-7B-Instruct
https://github.com/QwenLM/Qwen2.5-Coder
Abstract
In this report, we introduce the Qwen2.5-Coder series, a significant up-
gradefromitspredecessor,CodeQwen1.5. Thisseriesincludestwomodels:
Qwen2.5-Coder-1.5B and Qwen2.5-Coder-7B. As a code-specific model,
Qwen2.5-CoderisbuiltupontheQwen2.5architectureandcontinuespre-
trainedonavastcorpusofover5.5trilliontokens. Throughmeticulous
datacleaning,scalablesyntheticdatageneration,andbalanceddatamix-
ing,Qwen2.5-Coderdemonstratesimpressivecodegenerationcapabilities
whileretaininggeneralversatility. Themodelhasbeenevaluatedonawide
rangeofcode-relatedtasks,achievingstate-of-the-art(SOTA)performance
acrossmorethan10benchmarks,includingcodegeneration,completion,
reasoning,andrepair,consistentlyoutperforminglargermodelsofthesame
modelsize. WebelievethatthereleaseoftheQwen2.5-Coderserieswillnot
onlypushtheboundariesofresearchincodeintelligencebutalso,through
its permissive licensing, encourage broader adoption by developers in
real-worldapplications.
8
8
.4
81.9
65 6. 29
.2
55 23 ..
80818 . 422.
38 33
.7
9. 75
5 3.1
.3
..CR .U .X
MEval....MBP78 P. .1 ..7 .8 ..6 ..79 H.3
u8 1
m.1 anEval. ..72.6 Ev7 a3.5
lPlu74.9 s....76.8
Aider.3 ..4 .... L..i.6 ve3
C
od5
he cB
ne.3
20.5
24 48. .39 249 7. .6 750. 34
2. 39
5.9
76.5 73.2
70.2
69.2
66.1ultiP
L-E...... McEvalBigCodeBenc.h...BIRD-SQL......S 3pi
9de .r
8
70.0
77 743 6..
.6
68
60.3
54.7
54.3
50.5 46.0
2
8
.12 4
.5
46.24 41 5. .6
6
82.0
4.8
33.1 51.1
3
Qwen2.5-Coder7B-Instruct DeepSeek-Coder33B-Instruct CodeStral-22B
DeepSeek-Coder-V2Lite-Instruct DeepSeek-Coder6.7B-Instruct
∗Equalcorecontribution,†Correspondingaut1hor
1
4202
peS
81
]LC.sc[
1v68121.9042:viXra
5.23TechnicalReport
Contents
1 Introduction 3
2 ModelArchitecture 3
3 Pre-training 4
3.1 PretrainingData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.1 DataComposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.2 DataMixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.2 TrainingPolicy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.2.1 File-LevelPretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.2.2 Repo-LevelPretraining . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Post-training 7
4.1 ARecipeforInstructionData . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.2 TrainingPolicy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
5 Decontamination 9
6 EvaluationonBaseModels 9
6.1 CodeGeneration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
6.2 CodeCompletion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
6.3 CodeReasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
6.4 MathReasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
6.5 GeneralNaturalLanguage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
6.6 Long-ContextEvaluation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
7 EvaluationonInstructModels 14
7.1 CodeGeneration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
7.2 CodeReasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
7.3 CodeEditing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
7.4 Text-to-SQL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
7.5 MathReasoningandGeneralNaturalLanguage . . . . . . . . . . . . . . . . 20
8 Conclusion 20
2TechnicalReport
1 Introduction
Withtherapiddevelopmentoflargelanguagemodels(LLMs)(Brown,2020;Achiametal.,
2023;Touvronetal.,2023;Dubeyetal.,2024;Jiangetal.,2023;Baietal.,2023b;Yangetal.,
2024;Anthropic,2024;OpenAI,2024),code-specificlanguagemodelshavegarneredsig-
nificantattentioninthecommunity. BuiltuponpretrainedLLMs,codeLLMssuchasthe
StarCoder series (Li et al., 2023; Lozhkov et al., 2024), CodeLlama series (Roziere et al.,
2023),DeepSeek-Coderseries(Guoetal.,2024),CodeQwen1.5(Baietal.,2023a),andCode-
Stral(team,2024),havedemonstratedsuperiorperformanceincodingevaluations(Chen
etal.,2021;Austinetal.,2021;Cassanoetal.,2022;Jainetal.,2024). However,incomparison
withtherecentlystate-of-the-artproprietaryLLMs,Claude-3.5-Sonnet(Anthropic,2024)
andGPT-4o(OpenAI,2024),thecodeLLMsarestillfallingbehind,eitheropen-sourceor
proprietarymodels.
Buildinguponourpreviouswork,CodeQwen1.5,weareexcitedtointroduceQwen2.5-
Coder,anewseriesoflanguagemodelsdesignedtoachievetop-tierperformanceincoding
tasksatvariousmodelsizes. Qwen2.5-CodermodelsarederivedfromtheQwen2.5LLMs,
inheritingtheiradvancedarchitectureandtokenizer. Thesemodelsarepretrainedonexten-
sivedatasetsandfurtherfine-tunedoncarefullycuratedinstructiondatasetsspecifically
designedforcodingtasks. Theseriesincludesmodelswith1.5Band7Bparameters,along
withtheirinstruction-tunedvariants.Wearecommittedtofosteringresearchandinnovation
inthefieldofcodeLLMs,codingagents,andcodingassistantapplications. Therefore,we
areopen-sourcingtheQwen2.5-Codermodelstothecommunitytosupportandaccelerate
advancementsintheseareas.
Significanteffortshavebeendedicatedtoconstructingalarge-scale,coding-specificpretrain-
ingdatasetcomprisingover5.5trilliontokens. Thisdatasetissourcedfromabroadrangeof
publiccoderepositories,suchasthoseonGitHub,aswellaslarge-scaleweb-crawleddata
containingcode-relatedtexts. Wehaveimplementedsophisticatedprocedurestorecalland
cleanpotentialcodedataandfilteroutlow-qualitycontentusingweakmodelbasedclassi-
fiersandscorers. Ourapproachencompassesbothfile-levelandrepository-levelpretraining
toensurecomprehensivecoverage. Tooptimizeperformanceandbalancecodingexpertise
withgenerallanguageunderstanding, wehavecarefullycuratedadatamixturethatin-
cludescode,mathematics,andgeneraltexts. Totransformmodelsintocodingassistantsfor
downstreamapplications,wehavedevelopedawell-designedinstruction-tuningdataset.
Thisdatasetincludesawiderangeofcoding-relatedproblemsandsolutions,sourcedfrom
real-worldapplicationsandsyntheticdatageneratedbycode-focusedLLMs,coveringa
broadspectrumofcodingtasks.
ThisreportintroducestheQwen2.5-Coderseries,anupgradedversionofCodeQwen1.5,
featuringtwomodels:Qwen2.5-Coder-1.5BandQwen2.5-Coder-7B.BuiltontheQwen2.5ar-
chitectureandpretrainedonover5.5trilliontokens,thesecode-specificmodelsdemonstrate
exceptionalcodegenerationcapabilitieswhile maintaining generalversatility. Through
rigorousdataprocessingandtrainingtechniques,Qwen2.5-Coderachievesstate-of-the-art
performanceacrossmorethan10code-relatedbenchmarks,outperforminglargermodels
invarioustasks. Thereleaseofthesemodelsaimstoadvancecodeintelligenceresearch
and promote widespread adoption in real-world applications, facilitated by permissive
licensing.
2 ModelArchitecture
Architecture ThearchitectureofQwen2.5-CoderisthesameasQwen2.5. Table1shows
thearchitectureofQwen2.5-Coderfortwodifferentmodelsizes: 1.5Band7Bparameters.
Bothsizessharethesamearchitectureintermsoflayers,having28layersandaheadsizeof
128. However,theydifferinseveralkeyaspects. The1.5Bmodelhasahiddensizeof1,536,
whilethe7Bmodelhasamuchlargerhiddensizeof3,584. The1.5Bmodeluses12query
headsand2key-valueheads,whereasthe7Bmodeluses28queryheadsand4key-value
heads,reflectingitslargercapacity. Theintermediatesizealsoscaleswithmodelsize,being
8,960forthe1.5Bmodeland18,944forthe7Bmodel. Additionally,the1.5Bmodelemploys
3TechnicalReport
embeddingtying,whilethe7Bmodeldoesnot. Bothmodelssharethesamevocabulary
sizeof151,646tokensandhavebeentrainedon5.5trilliontokens.
Tokenization Qwen2.5-Coder inherits the vocabulary from Qwen2.5 but introduces
several special tokens to help the model better understand code. Table 2 presents an
overview of the special tokens added during training to better capture different forms
of code data. These tokens serve specific purposes in the code-processing pipeline. For
instance, <|endoftext|> marks the end of a text or sequence, while the <|fim_prefix|>,
<|fim_middle|>,and<|fim_suffix|>tokensareusedtoimplementtheFill-in-the-Middle
(FIM)(Bavarianetal.,2022)technique,whereamodelpredictsthemissingpartsofacode
block. Additionally,<|fim_pad|>isusedforpaddingduringFIMoperations. Othertokens
include <|repo_name|>, which identifies repository names, and <|file_sep|>, used as a
fileseparatortobettermanagerepository-levelinformation. Thesetokensareessentialin
helpingthemodellearnfromdiversecodestructuresandenableittohandlelongerand
morecomplexcontextsduringbothfile-levelandrepo-levelpretraining.
Configuration Qwen2.5-Coder1.5B Qwen2.5-Coder7B
HiddenSize 1,536 3,584
#Layers 28 28
#QueryHeads 12 28
#KVHeads 2 4
HeadSize 128 128
IntermediateSize 8,960 18,944
EmbeddingTying True False
VocabularySize 151,646 151,646
#TrainedTokens 5.5T 5.5T
Table1: ArchitectureofQwen2.5-Coder.
Token TokenID Description
<|endoftext|> 151643 endoftext/sequence
<|fim_prefix|> 151659 FIMprefix
<|fim_middle|> 151660 FIMmiddle
<|fim_suffix|> 151661 FIMsuffix
<|fim_pad|> 151662 FIMpad
<|repo_name|> 151663 repositoryname
<|file_sep|> 151664 fileseparator
Table2: Overviewofthespecialtokens.
3 Pre-training
3.1 PretrainingData
Large-scale,high-quality,anddiversedataformsthefoundationofpre-trainedmodels. To
thisend,weconstructedadatasetnamedQwen2.5-Coder-Data. Thisdatasetcomprises
fivekeydatatypes: SourceCodeData,Text-CodeGroundingData,SyntheticData,Math
Data,andTextData. Inthissection,weprovideabriefoverviewofthesourcesandcleaning
methodsappliedtothesedatasets.
3.1.1 DataComposition
SourceCode WecollectedpublicrepositoriesfromGitHubcreatedbeforeFebruary2024,
spanning 92 programming languages. Similar to StarCoder2 (Lozhkov et al., 2024) and
DS-Coder(Guoetal.,2024),weappliedaseriesofrule-basedfilteringmethods. Inaddition
4TechnicalReport
torawcode,wealsocollecteddatafromPullRequests,Commits,JupyterNotebooks,and
Kaggledatasets,allofwhichweresubjectedtosimilarrule-basedcleaningtechniques.
Tokens(B) AveragePerformance
600
582 50
500
48
400
45
370
300 42
200 40
147
100 118 38
0 35
Stage1 Stage2 Stage3 Stage4
Figure1: Numberofdatatokensacrossdiffer1entcc-stages,andthevalidationeffectiveness
oftrainingQwen2.5-Coderusingcorrespondingdata.
Text-CodeGroundingData Wecuratedalarge-scaleandhigh-qualitytext-codemixed
datasetfromCommonCrawl,whichincludescode-relateddocumentation,tutorials,blogs,
andmore. InsteadoftheconventionalURL-basedmulti-stagerecallmethod,wedeveloped
acoarse-to-finehierarchicalfilteringapproachforrawdata. Thismethodofferstwokey
advantages:
1. Itenablesprecisecontrolovereachfilter’sresponsibility,ensuringcomprehensive
handlingofeachdimension.
2. Itnaturallyassignsqualityscorestothedataset,withdataretainedinthefinalstage
beingofhigherquality,providingvaluableinsightsforquality-drivendatamixing.
WedesignedacleaningpipelinefortheText-CodeGroundingData,whereeachfilterlevelis
builtusingsmallermodels,suchasfastText. Althoughweexperimentedwithlargermodels,
theydidnotyieldsignificantbenefits. Alikelyexplanationisthatsmallermodelsfocus
moreonsurface-levelfeatures,avoidingunnecessarysemanticcomplexity.
InQwen2.5-Coder,weappliedthisprocessiteratively. AsshowninFigure1,eachiteration
resultedinimprovement. Through4-stagefiltering,theaveragescoresonHumanEvaland
MBPPincreasedfrom41.6%to46.8%comparedtothebaseline,demonstratingthevalueof
high-qualityText-CodeGroundingDataforcodegeneration.
SyntheticData Syntheticdataoffersapromisingwaytoaddresstheanticipatedscarcity
oftrainingdata. WeusedCodeQwen1.5,thepredecessorofQwen2.5-Coder,togenerate
large-scalesyntheticdatasets. Tomitigatetheriskofhallucinationsduringthisprocess,we
introducedanexecutorforvalidation,ensuringthatonlyexecutablecodewasretained.
MathData ToenhancethemathematicalcapabilitiesofQwen2.5-Coder,weintegrated
thepre-trainingcorpusfromQwen2.5-MathintotheQwen2.5-Coderdataset. Importantly,
theinclusionofmathematicaldatadidnotnegativelyimpactthemodel’sperformanceon
codetasks. Forfurtherdetailsonthecollectionandcleaningprocess,pleaserefertothe
Qwen2.5-Mathtechnicalreport.
TextData SimilartotheMathData,weincludedhigh-qualitygeneralnaturallanguage
data from the pre-training corpus of the Qwen2.5 model to preserve Qwen2.5-Coder’s
general capabilities. This data had already passed stringent quality checks during the
5
)B(snekoT
ecnamrofrePTechnicalReport
cleaningphaseofQwen2.5’sdataset,sonofurtherprocessingwasapplied. However,all
codesegmentswereremovedfromthegeneralTextdatatoavoidoverlapwithourcode
data,ensuringtheindependenceofdifferentdatasources.
3.1.2 DataMixture
BalancingCode,Math,andTextdataiscrucialforbuildingarobustfoundationalmodel.
Althoughtheresearchcommunityhasexploredthisbalancebefore,thereislimitedevidence
regardingitsscalabilitytolargedatasets. Toaddressthis,weconductedempiricalexperi-
mentswithdifferentratiosofCode,Math,andTextdata,designingmultipleexperimentsto
identifyanoptimalcombinationrapidly. Specifically,asshowninTable3,wecompared
threedifferentCode: Textratios—100:0:0,85:10:5,and70:20:10.
Interestingly,wefoundthatthe7:2:1ratiooutperformedtheothers,evensurpassingthe
performanceofgroupswithahigherproportionofcode. Apossibleexplanationisthat
MathandTextdatamaypositivelycontributetocodeperformance,butonlywhentheir
concentrationreachesaspecificthreshold. Infuturework,weplantoexploremoreefficient
ratiomechanismsandinvestigatetheunderlyingcausesofthisphenomenon. Ultimately,
weselectedafinalmixtureof70%Code,20%Text,and10%Math. Thefinaltrainingdataset
comprises5.2trilliontokens.
TokenRatio Coding Math General
Average
Code Text Math Common BCB MATH GSM8K MMLU CEval HellaSwag
100 0 0 49.8 40.3 10.3 23.8 42.8 35.9 58.3 31.3
85 15 5 43.3 36.2 26.1 52.5 56.8 57.1 70.0 48.9
70 20 10 48.3 38.3 33.2 64.5 62.9 64.0 73.5 55.0
Table3: TheperformanceofQwen2.5-Codertrainingondifferentdatamixturepolicy.
3.2 TrainingPolicy
Qwen2.5 File-Level Pretrain Repo-Level Qwen2.5-Code-Base Qwen2.5-Code-Instruct
Pretrain Code SFT
(1.5B, 7B) 5.2T Tokens 300B Tokens (1.5B, 7B) (1.5B, 7B)
① ② ③
Figure2: Thethree-stagetrainingpipelineforQwen2.5-Coder.
As shown in 2, we employed a three-stage training approach to train Qwen2.5-Coder,
includingfile-levelpretraining,repo-levelpretraining,andinstructiontuning.
3.2.1 File-LevelPretraining
File-level pretraining focuses on learning from individual code files. In this stage, the
maximumtrainingsequencelengthissetto8,192tokens,covering5.2Tofhigh-qualitydata.
Thetrainingobjectivesincludenexttokenpredictionandfill-in-the-middle(FIM)(Bavarian
etal.,2022). ThespecificFIMformatisshowninFigure3.
File-LevelFIMformat.
<|fim_prefix|>{code_pre}<|fim_suffix|>{code_suf}<|fim_middle|>{code_mid}<|endoftext|>
Figure3: File-LevelFIMformat.
6TechnicalReport
3.2.2 Repo-LevelPretraining
Afterfile-levelpretraining,weturntorepo-levelpretraining,aimedatenhancingthemodel’s
long-contextcapabilities. Inthisstage,thecontextlengthisextendedfrom8,192tokensto
32,768tokens,andRoPE’sbasefrequencyisadjustedfrom10,000to1,000,000. Tofurther
leveragethemodel’sextrapolationpotential,weappliedtheYARNmechanism(Pengetal.,
2023),enablingthemodeltohandlesequencesupto131,072(132K)tokens.
Inthisstage,weusedalargeamountofhigh-quality,long-codedata(≈300B)andextended
file-levelFIMtotherepo-levelFIMfollowedbymethodsdescribedinLozhkovetal.(2024),
withthespecificformatshowninFigure4.
Repo-LevelFIMformat.
<|repo_name|>{repo_name}
<|file_sep|>{file_path1}
{file_content1}
<|file_sep|>{file_path2}
{file_content2}
<|file_sep|>{file_path3}
<|fim_prefix|>{code_pre}<|fim_suffix|>{code_suf}<|fim_middle|>{code_fim}<|endoftext|>
Figure4: Repo-LevelFIMformat.
4 Post-training
4.1 ARecipeforInstructionData
MultilingualProgrammingCodeIdentification Wefine-tuneaCodeBERTtoperform
thelanguageidentificationmodeltocategorizedocumentsintonearly100programming
languages. Wekeeptheinstructiondataofthemainstreamprogramminglanguagesand
randomlydiscardaportionoftheinstructiondataofthelong-taillanguages. Ifagiven
samplecontainsverylittlecodedataorevennocodesnippets,thesamplewillpossiblybe
classifiedinto“NoProgrammingLanguage”tag. Weremovemostofthesampleswithout
codesnippetstokeepthecodegenerationcapabilityofourinstructionmodel.
InstructionSynthesisfromGitHub Fortheunsuperviseddata(codesnippets)massively
existing in many websites (e.g. GitHub), we try to construct the supervised instruction
dataset. Specifically,weusetheLLMtogeneratetheinstructionfromthecodesnippets
within1024tokensandthenweusethecodeLLMtogeneratetheresponseSunetal.(2024).
Finally,weusetheLLMscorertofilterthelow-qualityonestoobtainthefinalpair.Giventhe
codesnippetsofdifferentprogramminglanguages,weconstructaninstructiondatasetfrom
thecodesnippets. Toincreasethediversityoftheinstructiondataset. Conversely,wefirst
generatetheanswersfromthecode. ThenweusetheLLMscorertofilterthelow-quality
to obtain the final triplet. Similarly, given the code snippets of different programming
languages,wecanconstructaninstructiondatasetwiththeuniversalcodefromthecode
snippets. Tofullyunleashthepotentialofourproposedmethod,wealsoincludetheopen-
sourceinstructiondatasetintheseedinstructiondataset. Finally,wecombinethreeparts
instructiondatasetforsupervisedfine-tuning.
MultilingualCodeInstructionData Tobridgethegapamongdifferentprogramming
languages,weproposeamultilingualmulti-agentcollaborativeframeworktosynthesize
themultilingualinstructioncorpora. Weintroducelanguage-specificagents,whereasetof
specializedagentsarecreatedandeachdedicatedtoaparticularprogramminglanguage.
Theseagentsareinitializedwithlanguage-specificinstructiondataderivedfromthelimited
7TechnicalReport
existingmultilingualinstructioncorpora. Themultilingualdatagenerationprocesscanbe
splitinto: (1)Language-SpecificIntelligentAgents: Wecreateasetofspecializedagents,
eachdedicatedtoaparticularprogramminglanguage. Theseagentsareinitializedwith
language-specificinstructiondataderivedfromcuratedcodesnippets. (2)Collaborative
Discussion Protocol: Multiple language-specific agents engage in a structured dialogue
toformulatenewinstructionsandsolutions. Thisprocesscanresultineitherenhancing
existinglanguagecapabilitiesorgeneratinginstructionsforanovelprogramminglanguage.
(3)AdaptiveMemorySystem: Eachagentmaintainsadynamicmemorybankthatstoresits
generationhistorytoavoidgeneratingthesimilarsamples. (4)Cross-LingualDiscussion:
Weimplementanovelknowledgedistillationtechniquethatallowsagentstoshareinsights
andpatternsacrosslanguageboundaries,fosteringamorecomprehensiveunderstanding
of programming concepts. (5) Synergy Evaluation Metric: We develop a new metric to
quantifythedegreeofknowledgesharingandsynergybetweendifferentprogramming
languageswithinthemodel. (6)AdaptiveInstructionGeneration: Theframeworkincludes
amechanismtodynamicallygeneratenewinstructionsbasedonidentifiedknowledgegaps
acrosslanguages.
Checklist-basedScoringforInstructionData Tocompletelyevaluatethequalityofthe
created instruction pair, we introduce several scoring points for each sample: (1) Ques-
tion&AnswerConsistency: WhetherQ&Aareconsistentandcorrectforfine-tuning. (2)
Question&AnswerRelevance: WhetherQ&Aarerelatedtothecomputerfield. (3)Ques-
tion&AnswerDifficulty:WhetherQ&Aaresufficientlychallenging.(4)CodeExist:Whether
the code is provied in question or answer. (5) Code Correctness: Evaluate whether the
providedcodeisfreefromsyntaxerrorsandlogicalflaws. (6)Considerfactorslikeproper
variablenaming,codeindentation,andadherencetobestpractices. (7)CodeClarity: Assess
howclearandunderstandablethecodeis. Evaluateifitusesmeaningfulvariablenames,
propercomments, andfollowsaconsistentcodingstyle. (8)CodeComments: Evaluate
thepresenceofcommentsandtheirusefulnessinexplainingthecode’sfunctionality. (9)
Easy to Learn: determine its educational value for a student whose goal is to learn ba-
sic coding concepts. After gaining all scores (s ,...,s ), we can get the final score with
1 n
s = w s +···+w s ,where(w ,...,w )areaseriesofpre-definedweights.
1 1 n n 1 n
Amultilingualsandboxforcodeverification Tofurtherverifythecorrectnessofthecode
syntax, we use the code static checking for all extracted code snippets of programming
languages(e.g. Python,Java,andC++). Weparsethecodesnippetintotheabstractsyntax
treeandfilteroutthecodesnippet,wheretheparsednodesincodesnippethaveparsing
errors. Wecreateamultilingualsandboxtosupportthecodestaticcheckingforthemain
programminglanguage. Further,themultilingualsandboxisacomprehensiveplatform
designedtovalidatecodesnippetsacrossmultipleprogramminglanguages.Itautomatesthe
processofgeneratingrelevantunittestsbasedonlanguage-specificsamplesandevaluates
whethertheprovidedcodesnippetscansuccessfullypassthesetests. Especially,onlythe
self-contained (e.g. algorithm problems) code snippet will be fed into the multilingual
sandbox. Themultilingualverificationsandboxismainlycomprisedoffiveparts:
1. LanguageSupportModule:
• Implementssupportformultiplelanguages(e.g.,Python,Java,C++,JavaScript)
• Maintainslanguage-specificparsingandexecutionenvironments
• Handlessyntaxandsemanticanalysisforeachsupportedlanguage
2. SampleCodeRepository:
• Storesadiversecollectionofcodesamplesforeachsupportedlanguage
• Organizessamplesbylanguage,difficultylevel,andprogrammingconcepts
• Regularlyupdatedandcuratedbylanguageexperts
3. UnitTestGenerator:
• Analyzessamplecodetoidentifykeyfunctionalitiesandedgecases
• Automaticallygeneratesunittestsbasedontheexpectedbehavior
• Producestestcasescoveringvariousinputscenariosandexpectedoutputs
8TechnicalReport
4. CodeExecutionEngine:
• Providesisolatedenvironmentsforexecutingcodesnippetssecurely
• Supportsparallelexecutionofmultipletestcases
• Handlesresourceallocationandtimeoutmechanisms
5. ResultAnalyzer:
• Comparestheoutputofcodesnippetsagainstexpectedresultsfromunittests
• Generatesdetailedreportsontestcasesuccessesandfailures
• Providessuggestionsforimprovementsbasedonfailedtestcases
4.2 TrainingPolicy
Coarse-to-fineFine-tuning Wefirstsynthesizedtensofmillionsoflow-qualitybutdiverse
instructionsamplestofine-tunethebasemodel. Inthesecondstage, weadoptmillions
ofhigh-qualityinstructionsamplestoimprovetheperformanceoftheinstructionmodel
withrejectionsamplingandsupervisedfine-tuning. Forthesamequery,weusetheLLM
togeneratemultiplecandidatesandthenusetheLLMtoscorethebestoneforsupervised
fine-tuning.
MixedTuning Sincemostinstructiondatahaveashortlength,weconstructtheinstruction
pairwiththeFIMformattokeepthelongcontextcapabilityofthebasemodel. Inspiredby
programminglanguagesyntaxrulesanduserhabitsinpracticalscenarios,weleveragethe
tree-sitter-languages1toparsethecodesnippetsandextractthebasiclogicblocksasthe
middlecodetoinfill. Forexample,theabstractsyntaxtree(AST)representsthestructureof
Pythoncodeinatreeformat,whereeachnodeinthetreerepresentsaconstructoccurring
inthesourcecode. Thetree’shierarchicalnaturereflectsthesyntacticnestingofconstructs
inthecodeandincludesvariouselementssuchasexpressions,statements,andfunctions.
BytraversingandmanipulatingtheAST,wecanrandomlyextractthenodesofmultiple
levelsandusethecodecontextofthesamefiletouncoverthemaskednode. Finally,we
optimizetheinstructionmodelwithamajorityofstandardSFTdataandasmallpartof
FIMinstructionsamples.
5 Decontamination
To ensure that Qwen2.5-Coder does not produce inflated results due to test set leakage,
weperformeddecontaminationonalldata,includingbothpre-trainingandpost-training
datasets. WeremovedkeydatasetssuchasHumanEval,MBPP,GSM8K,andMATH.The
filteringwasdoneusinga10-gramoverlapmethod,whereanytrainingdatawitha10-gram
string-leveloverlapwiththetestdatawasremoved.
6 EvaluationonBaseModels
Forthebasemodel,weconductedacomprehensiveandfairevaluationinsixkeyaspects,in-
cludingcodegeneration,codecompletion,codereasoning,mathematicalreasoning,general
naturallanguageunderstandingandlong-contextmodeling. Toensurethereproducibility
of all results, we made all evaluation codes publicly available2. For comparing models,
we chose the most popular and powerful open source language models, including the
StarCoder2andDeepSeek-Coderseries. Belowisthelistofartifactsusedintheevaluation
forthissection.
6.1 CodeGeneration
HumanEval and MBPP Code generation serves as a fundamental capability for code
modelstohandlemorecomplextasks.Weselectedtwopopularcodegenerationbenchmarks
1https://pypi.org/project/tree-sitter-languages/
2https://github.com/QwenLM/Qwen2.5-Coder
9TechnicalReport
Artifact Publiclink
Qwen2.5-Coder-1.5B https://hf.co/qwen/Qwen/Qwen2.5-Coder-1.5B
Qwen2.5-Coder-7B https://hf.co/qwen/Qwen/Qwen2.5-Coder-7B
CodeQwen1.5-7B-Base https://huggingface.co/Qwen/CodeQwen1.5-7B
StarCoder2-3B https://hf.co/bigcode/starcoder2-3b
StarCoder2-7B https://hf.co/bigcode/starcoder2-7b
StarCoder2-15B https://hf.co/bigcode/starcoder2-15b
DS-Coder-1.3B-Base https://hf.co/deepseek-ai/deepseek-coder-1.3b-base
DS-Coder-6.7B-Base https://hf.co/deepseek-ai/deepseek-coder-6.7b-base
DS-Coder-33B-Base https://hf.co/deepseek-ai/deepseek-coder-33b-base
DS-Coder-V2-Lite-Base https://hf.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Base
DS-Coder-V2-Base https://hf.co/deepseek-ai/DeepSeek-Coder-V2-Base
Table4: Allartifactsreleasedandusedinthissection.
HumanEval MBPP BigCodeBench
Model Size
HE HE+ MBPP MBPP+ 3-shot Full Hard
1B+Models
StarCoder2-3B 3B 31.7 27.4 60.2 49.1 - 21.4 4.7
DS-Coder-1.3B 1.3B 34.8 26.8 55.6 46.9 46.2 26.1 3.4
Qwen2.5-Coder-1.5B 1.5B 43.9 36.6 69.2 58.6 59.2 34.6 9.5
6B+Models
StarCoder2-7B 7B 35.4 29.9 54.4 45.6 - 27.7 8.8
StarCoder2-15B 15B 46.3 37.8 66.2 53.1 - 38.4 12.2
DS-Coder-6.7B-Base 6.7B 47.6 39.6 70.2 56.6 60.6 41.1 11.5
DS-Coder-V2-Lite-Base 2.4/16B 40.9 34.1 71.9 59.4 - 30.6 8.1
CodeQwen1.5-7B-Base 7B 51.8 45.7 72.2 60.2 61.8 45.6 15.6
Qwen2.5-Coder-7B-Base 7B 61.6 53.0 76.9 62.9 68.8 45.8 16.2
20B+Models
DS-Coder-33B-Base 33B 54.9 47.6 74.2 60.7 66.0 49.1 20.3
Table5: PerformanceofvariousmodelsonHumanEval,MBPPandthe“complete”taskof
BigCodeBench.
toevaluateQwen2.5-Coder,namelyHumanEval(Chenetal.,2021)andMBPP(Austinetal.,
2021). HumanEvalconsistsof164manuallywrittenprogrammingtasks,eachprovidinga
Pythonfunctionsignatureandadocstringasinputtothemodel. MBPP,ontheotherhand,
comprises974programmingproblemscreatedbycrowdsourcecontributors. Eachproblem
includesaproblemstatement(i.e.,adocstring),afunctionsignature,andthreetestcases.
Tofurtherensureaccurateevaluation,EvalPlus(Liuetal.,2023)extendsHumanEvalinto
HumanEval+byadding80timesmoreuniquetestcasesandcorrectinginaccurateground-
truthsolutionsinHumanEval. Similarly,MBPP+offers35timesmoretestcasesthanthe
originalMBPP.
Additionally, we should notice that MBPP 3-shot is particularly suitable for monitoring
modelconvergenceduringtraining. Earlyintheconvergenceprocess,themodeltendstobe
unstable,causingsignificantfluctuationinmetrics,andsimple3-shotexampleseffectively
mitigateit. Therefore,wealsoreporttheresultsofMBPP3-shotperformance.
AsshowninTable5,Qwen2.5-Coderhaveshownimpressiveperformanceinbasiccode
generation,achievingstate-of-the-artresultsamongopen-sourcemodelsofthesamesize
andsurpassingevenlargermodels. Inparticular,Qwen2.5-Coder-7B-Baseoutperformsthe
previousbestdensemodel,DS-Coder-33B-Base,acrossallfivemetrics.
BigCodeBench-Complete BigCodeBench(Zhuoetal.,2024)isarecentandmorechalleng-
ingbenchmarkforcodegeneration,primarilyaimedatevaluatingtheabilityoftool-use
andcomplexinstructionfollowing. Thebasemodelgeneratestheexpectedcodethrougha
10TechnicalReport
Model Size Python C++ Java PHP TS C# Bash JS Average
1B+Models
StarCoder2-3B 3B 31.7 30.4 29.8 32.9 39.6 34.8 13.9 35.4 31.1
DS-Coder-1.3B-Base 1.3B 34.8 31.1 32.3 24.2 28.9 36.7 10.1 28.6 28.3
Qwen2.5-Coder-1.5B 1.5B 42.1 42.9 38.6 41.0 49.1 46.2 20.3 49.1 41.1
6B+Models
StarCoder2-7B 7B 35.4 40.4 38.0 30.4 34.0 46.2 13.9 36.0 34.3
StarCoder2-15B 15B 46.3 47.2 46.2 39.1 42.1 53.2 15.8 43.5 41.7
DS-Coder-6.7B-Base 6.7B 49.4 50.3 43.0 38.5 49.7 50.0 28.5 48.4 44.7
DS-Coder-V2-Lite-Base 2.4/16B 40.9 45.9 34.8 47.2 48.4 41.7 19.6 44.7 40.4
CodeQwen1.5-7B-Base 7B 51.8 52.2 42.4 46.6 52.2 55.7 36.7 49.7 48.4
Qwen2.5-Coder-7B-Base 7B 61.6 62.1 53.2 59.0 64.2 60.8 38.6 60.3 57.5
20B+Models
DS-Coder-33B-Base 33B 56.1 58.4 51.9 44.1 52.8 51.3 32.3 55.3 50.3
Table6: PerformanceofdifferentmodelsonMultiPL-E.
completionmode,givenafunctionsignatureanddocumentation,whichisreferredtoas
BigCodeBench-Complete. Itconsistsoftwosubsets: thefullsetandthehardset. Compared
toHumanEvalandMBPP,BigCodeBenchissuitedforout-of-distribution(OOD)evaluation.
Table 5 illustrates that Qwen2.5-Coder continues to show strong performance on
BigCodeBench-Complete,underscoringthemodel’sgeneralizationpotential.
Multi-ProgrammingLanguage TheevaluationsmentionedabovefocusonthePython
language. However,weexpectastrongcodemodeltobenotonlyproficientinPythonbut
alsoversatileacrossmultipleprogramminglanguagestomeetthecomplexandevolving
demandsofsoftwaredevelopment. TomorecomprehensivelyevaluateQwen2.5-Coder’s
proficiencyinhandlingmultipleprogramminglanguages,weselectedtheMultiPL-E(Cas-
sanoetal.,2022)andchoosetoevaluateeightmainstreamlanguagesfromthesebenchmark,
includingPython,C++,Java,PHP,TypeScript,C#,BashandJavaScript.
Asshowninthetable6,Qwen2.5-Coderalsoachievedstate-of-the-artresultsinthemulti-
programminglanguageevaluation,withitscapabilitieswell-balancedacrossvariouslan-
guages. Itscoredover60%infiveoutoftheeightlanguages.
6.2 CodeCompletion
HumanEvalInfilling Manydeveloperaidtoolsrelyonthecapabilitytoautocompletecode
basedonprecedingandsucceedingcodesnippets. Qwen2.5-CoderutilizestheFill-In-the-
Middle(FIM)trainingstrategy,asintroducedinBavarianetal.(2022),enablingthemodel
togeneratecodethatiscontextuallycoherent. Toassessitscodecompletionproficiency,we
utilizetheHumanEvalInfillingbenchmarkAllaletal.(2023).Thisbenchmarkchallengesthe
modeltoaccuratelypredictmissingsectionsofcodewithintasksderivedfromHumanEval.
We use the single-line infilling settings across Python, Java, and JavaScript, focusing on
predictingasinglelineofcodewithingivencontexts. Performancewasmeasuredusingthe
ExactMatchmetric,whichdeterminestheproportionofthefirstgeneratedcodelinethat
preciselymatchthegroundtruth.
Thetable7illustratesthatQwen2.5-Codersurpassesalternativemodelsconcerningmodel
size. Specifically,Qwen2.5-Coder-1.5Bachievesanaverageperformanceimprovementof
3.7%,rivalingthemajorityofmodelsexceeding6billionparameters. Moreover,Qwen2.5-
Coder-7B-Basestandsastheleadingmodelamongthoseover6billionparameters,matching
theperformanceoftheformidable33billionparametermodel,DS-Coder-33B-Base. Notably,
weexcludedDS-Coder-v2-236Bfromcomparisonduetoitsdesignfocusnotbeingoncode
completiontasks.
11TechnicalReport
HumanEval-Infilling
Model Size
Python Java JavaScript Average
1B+Models
StarCoder2-3B 3B 70.9 84.4 81.8 79.0
DS-Coder-1.3B-Base 1.3B 72.8 84.3 81.7 79.6
Qwen2.5-Coder-1.5B 1.5B 77.0 85.6 85.0 82.5
6B+Models
StarCoder2-7B 7B 70.8 86.0 84.4 80.4
StarCoder2-15B 15B 78.1 87.4 84.1 81.3
DS-Coder-6.7B-Base 6.7B 78.1 87.4 84.1 83.2
DS-Coder-V2-Lite-Base 2.4/16B 78.7 87.8 85.9 84.1
CodeQwen1.5-7B-Base 7B 75.8 85.7 85.0 82.2
Qwen2.5-Coder-7B-Base 7B 79.7 88.5 87.6 85.3
20B+Models
DS-Coder-33B-Base 33B 80.1 89.0 86.8 85.3
Table7: PerformanceofdifferentapproachesontheHumanEvalInfillingTasks
6.3 CodeReasoning
Code is a highly abstract form of logical language, and reasoning based on code helps
us determine whether a model truly understands the reasoning flow behind the code.
We selected CRUXEval (Gu et al., 2024) as the benchmark, which includes 800 Python
functions along with corresponding input-output examples. It consists of two distinct
tasks: CRUXEval-I,wherethelargelanguagemodel(LLM)mustpredicttheoutputbased
onagiveninput;andCRUXEval-O,wherethemodelmustpredicttheinputbasedona
knownoutput. ForbothCRUXEval-IandCRUXEval-O,weusedachain-of-thought(CoT)
approach,requiringtheLLMtooutputstepssequentiallyduringsimulatedexecution.
AsshowninTable8,Qwen2.5-Coderdeliveredhighlypromisingresults,achievingascore
of56.5onCRUXEval-Iand56.0onCRUXEval-O,thankstoourfocusonexecutablequality
duringthecodecleaningprocess.
CRUXEval
Model Size
Input-CoT Output-CoT
1B+Models
StarCoder2-3B 3B 42.1 29.2
DS-Coder-1.3B-Base 1.3B 32.1 28.2
Qwen2.5-Coder-1.5B 1.5B 43.8 34.6
6B+Models
StarCoder2-7B 7B 39.5 35.1
StarCoder2-15B 15B 46.1 47.6
DS-Coder-6.7B-Base 6.7B 39.0 41.0
DS-Coder-V2-Lite-Base 2.4/16B 53.4 46.1
CodeQwen1.5-7B-Base 7B 44.8 40.1
Qwen2.5-Coder-7B-Base 7B 56.5 56.0
20B+Models
DS-Coder-33B-Base 33B 50.6 48.8
DS-Coder-V2-Base 21/236B 62.7 67.4
Table8: PerformanceofdifferentmodelsonCRUXEvalwithInput-CoT andOutput-CoT
settings.
12TechnicalReport
6.4 MathReasoning
Mathematicsandcodinghavealwaysbeencloselyintertwined. Mathematicsformsthe
foundational discipline for coding, while coding serves as a vital tool in mathematical
fields. Assuch,weexpectanopenandpowerfulcodemodeltoexhibitstrongmathematical
capabilitiesaswell. ToassessQwen2.5-Coder’smathematicalperformance,weselectedfive
popularbenchmarks,includingMATH(Hendrycksetal.,2021),GSM8K(Cobbeetal.,2021),
MMLU-STEM(Hendrycksetal.,2020)andTheoremQA(Chenetal.,2023). Asshownin
Table9,Table3highlightsQwen2.5-Coder’sstrengthsinmathematics,whichlikelystem
fromtwokeyfactors: first,themodel’sstrongfoundationbuiltonQwen2.5,andsecond,
thecarefulmixingofcodeandmathematicaldataduringtraining, whichhasensureda
well-balancedperformanceacrossthesedomains.
MATH GSM8K MMLUSTEM TheoremQA
Model Size
4-shot 4-shot 5-shot 5-shot
1B+Models
StarCoder2-3B 3B 10.8 21.6 34.9 12.1
DS-Coder-1.3B-Base 1.3B 4.6 4.4 24.5 8.9
Qwen2.5-Coder-1.5B 1.5B 30.9 65.8 49.0 21.4
6B+Models
StarCoder2-7B 7B 14.6 32.7 39.8 16.0
StarCoder2-15B 15B 23.7 57.7 49.2 20.5
DS-Coder-6.7B-Base 6.7B 10.3 21.3 34.2 13.6
DS-Coder-V2-Lite-Base 2.4/16B 39.0 67.1 58.5 29.3
CodeQwen1.5-7B-Base 7B 10.6 37.7 39.6 15.8
Qwen2.5-Coder-7B-Base 7B 46.6 83.9 67.6 34.0
20B+Models
DS-Coder-33B-Base 33B 14.4 35.4 39.5 17.5
Table9: Performanceofvariousmodelsonfourmathbenchmark,namedMATH,GSM8K,
MMLUSTEMandTheoremQArespectively.
MMLU
Model Size
Base Redux
1B+Models
StarCoder2-3B 3B 36.6 37.0
DS-Coder-1.3B-Base 1.3B 25.8 24.5
Qwen2.5-Coder-1.5B 1.5B 53.6 50.9
6B+Models
StarCoder2-7B 7B 38.8 38.6
StarCoder2-15B 15B 64.1 48.8
DS-Coder-6.7B-Base 6.7B 36.4 36.5
DS-Coder-V2-Lite-Base 2.4/16B 60.5 58.3
CodeQwen1.5-7B-Base 7B 40.5 41.2
Qwen2.5-Coder-7B-Base 7B 68.0 66.6
20B+Models
DS-Coder-33B-Base 33B 39.4 38.7
Table10: MMLUresultsofdifferentmodels,ageneralbenchmarkforcommonknowledge.
13TechnicalReport
Model Size ARC-Challenge TruthfulQA WinoGrande HellaSwag
1B+Models
StarCoder2-3B 3B 34.2 40.5 57.1 48.1
DS-Coder-1.3B-Base 1.3B 25.4 42.7 53.3 39.5
Qwen2.5-Coder-1.5B 1.5B 45.2 44.0 60.7 61.8
6B+Models
StarCoder2-7B 7B 38.7 42.0 57.1 52.4
StarCoder2-15B 15B 47.2 37.9 64.3 64.1
DS-Coder-6.7B-Base 6.7B 36.4 40.2 57.6 53.8
DS-Coder-V2-Lite-Base 2.4/16B 57.3 38.8 72.9 -
CodeQwen1.5-7B-Base 7B 35.7 42.2 59.8 56.0
Qwen2.5-Coder-7B-Base 7B 60.9 50.6 72.9 76.8
20B+Models
DS-Coder-33B-Base 33B 42.2 40.0 62.0 60.2
Table 11: General performance of different models on four popular general benchmark,
ARC-Challenge,TruthfulQA,WinoGrandeandHellaSwag.
6.5 GeneralNaturalLanguage
Inadditiontomathematicalability,weaimtoretainasmuchofthebasemodel’sgeneral-
purposecapabilitiesaspossible,suchasgeneralknowledge. Toevaluategeneralnatural
languageunderstanding,weselectedMMLU(Hendrycksetal.,2021)anditsvariantMMLU-
Redux(Gemaetal.,2024),alongwithfourotherbenchmarks: ARC-Challenge(Clarketal.,
2018),TruthfulQA(Linetal.,2021),WinoGrande(Sakaguchietal.,2019),andHellaSwag
(Zellersetal.,2019). Similartotheresultsinmathematics, Table11highlightsQwen2.5-
Coder’sadvantageingeneralnaturallanguagecapabilitiescomparedtoothercoders,further
validatingtheeffectivenessofQwen2.5-Coderdatamixingstrategy.
6.6 Long-ContextEvaluation
LongcontextcapabilityiscrucialforcodeLLMs,servingasthecoreskillforunderstanding
repository-levelcodeandbecomingacodeagent.However,mostofcurrentcodemodelsstill
haveverylimitedsupportforlength,whichhinderstheirpotentialforpracticalapplication.
Qwen2.5-Coderaimstofurtheradvancetheprogressofopen-sourcecodemodelsinlong
contextmodeling. Toachievethis,wehavecollectedandconstructedlongsequencecode
data at the repository level for pre-training. Through careful data proportioning and
organization,wehaveenabledittosupportinputlengthsofupto128Ktokens.
NeedleintheCode WecreatedasimplebutbasicsynthetictaskcalledNeedleintheCode,
inspiredbypopularlong-contextevaluationsinthetextdomain. Inthistask,weinserteda
verysimplecustomfunctionatvariouspositionswithinacoderepo(wechoseMegatron3to
honoritscontributionstoopen-sourceLLMs!) andtestedwhetherthemodelcouldreplicate
thisfunctionattheendofthecodebase. ThefigurebelowshowsthatQwen2.5-Coderis
capableofsuccessfullycompletingthistaskwithina128klengthrange.
7 EvaluationonInstructModels
Fortheevaluationoftheinstructmodels,werigorouslyassessedsixcoreareas: codegenera-
tion,codereasoning,codeediting,text-to-sql,mathematicalreasoningandgeneralnaturallanguage
understanding. Theevaluationwasstructuredtoensureafairandthoroughcomparison
acrossmodels. Allevaluationcodeispubliclyaccessibleforreproducibility4. Toensurea
broadcomparison,weincludedsomeofthemostpopularandwidely-usedopen-source
3https://github.com/NVIDIA/Megatron-LM
4https://github.com/QwenLM/Qwen2.5-Coder
14TechnicalReport
CodeQwen2.5-7B
Correct Incorrect
0%
11%
22%
33%
44%
56%
67%
78%
89%
100%
10k 23k 35k 48k 61k 74k 86k 99k 112k 128k
ContextLength
1
Figure5: ThelongcontextabilityofQwen2.5-Coder,evaluatedbyNeedleinCode.
instruction-tunedmodels,notablyversionsfromtheDeepSeek-CoderseriesandCodestral
models. Belowisalistofallartifactsreferencedinthissection.
Artifact Publiclink
Qwen2.5-Coder-1.5B-Instruct https://hf.co/qwen/Qwen/Qwen2.5-Coder-1.5B-Instruct
Qwen2.5-Coder-7B-Instruct https://hf.co/qwen/Qwen/Qwen2.5-Coder-7B-Instruct
CodeQwen1.5-7B-Chat https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat
CodeStral-22B https://hf.co/mistralai/Codestral-22B-v0.1
DS-Coder-1.3B-instruct https://hf.co/deepseek-ai/deepseek-coder-1.3b-instruct
DS-Coder-6.7B-instruct https://hf.co/deepseek-ai/deepseek-coder-6.7b-instruct
DS-Coder-33B-instruct https://hf.co/deepseek-ai/deepseek-coder-33b-instruct
DS-Coder-V2-Lite-Instruct https://hf.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct
DS-Coder-V2-Instruct https://hf.co/deepseek-ai/DeepSeek-Coder-V2-Instruct
Table12: Allartifactsreleasedandusedinthissection.
7.1 CodeGeneration
BuildingontheperformanceimprovementsoftheQwen2.5-Coderseriesbasemodels,our
Qwen2.5-Coderseriesinstructmodelssimilarlydemonstratedoutstandingperformancein
codegenerationtasks.
HumanEvalandMBPP WealsoevaluatedthecodegenerationcapabilitiesoftheQwen2.5-
CoderseriesinstructmodelsusingtheEvalPlus(Liuetal.,2023)dataset. Asillustratedby
theexperimentalresultsinTable13,ourQwen2.5-Coder-7B-Instructmodeldemonstrated
superioraccuracy,significantlyoutperformingothermodelswithacomparablenumberof
parameters. Remarkably,itevenexceededtheperformanceoflargermodelswithover20
billionparameters,suchasCodeStral-22BandDS-Coder-33B-Instruct. Notably,Qwen2.5-
Coder-7B-Instructwastheonlymodelinourevaluationtosurpassan80%accuracyrateon
HumanEval+,achievinganimpressive84.1%.
BigCodeBench-Instruct TheinstructsplitprovidedbyBigCodeBench(Zhuoetal.,2024)
isintendedforassessingthecodegenerationabilitiesofinstructmodels. Weassessedthe
Qwen2.5-CoderseriesinstructmodelsontheBigCodeBench-Instruct. AsshowninTable13,
theQwen2.5-Coder-7B-Instructoutperformedotherinstructmodelswithsimilarparameter
sizes,achievinghigheraccuracyscoresonboththefullandhardsubsets,reaching41.0%
onthefullsubsetand18.2%onthehardsubset,demonstratingtheQwen2.5-Coderseries
instructmodels’powerfulcodegenerationcapabilities.
LiveCodeBench LiveCodeBench(Jainetal.,2024)isacomprehensiveandcontamination-
free benchmark designed to evaluate the coding capabilities of LLMs. It continuously
15TechnicalReport
HumanEval MBPP BigCodeBench LiveCodeBench
Model Size
HE HE+ MBPP MBPP+ Full Hard Pass@1
1B+Models
DS-Coder-1.3B-Instruct 1.3B 65.2 61.6 61.6 52.6 22.8 3.4 9.3
Qwen2.5-Coder-1.5B-Instruct 1.5B 70.7 66.5 69.2 59.4 32.5 6.8 15.7
6B+Models
DS-Coder-6.7B-Instruct 6.7B 78.6 70.7 75.1 66.1 35.5 10.1 20.5
DS-Coder-V2-Lite-Instruct 2.4/16B 81.1 75.0 82.3 68.8 36.8 16.2 24.3
CodeQwen1.5-7B-Chat 7B 86.0 79.3 83.3 71.4 39.6 17.2 20.1
Qwen2.5-Coder-7B-Instruct 7B 88.4 84.1 83.5 71.7 41.0 18.2 37.6
20B+Models
CodeStral-22B 22B 78.1 74.4 73.3 68.2 47.1 20.6 32.9
DS-Coder-33B-Instruct 33B 79.3 68.9 81.2 70.1 46.5 17.6 27.7
Table13: TheperformanceofdifferentinstructmodelsoncodegenerationbyHumanEval,
MBPP,bigcodebenchandlivecodebench. Forbigcodebenchhere,wereport“instruct”tasks
score.
Model Size Python Java C++ C# TS JS PHP Bash Average
1B+Models
DS-Coder-1.3B-Instruct 1.3B 65.2 51.9 45.3 55.1 59.7 52.2 45.3 12.7 48.4
Qwen2.5-Coder-1.5B-Instruct 1.5B 71.2 55.7 50.9 64.6 61.0 62.1 59.0 29.1 56.7
6B+Models
DS-Coder-6.7B-Instruct 6.7B 78.6 68.4 63.4 72.8 67.2 72.7 68.9 36.7 66.1
DS-Coder-V2-Lite-Instruct 2.4/16B 81.1 76.6 75.8 76.6 80.5 77.6 74.5 43.0 73.2
CodeQwen1.5-7B-Chat 7B 83.5 70.9 72 75.9 76.7 77.6 73.9 41.8 71.6
Qwen2.5-Coder-7B-Instruct 7B 87.8 76.5 75.6 80.3 81.8 83.2 78.3 48.7 76.5
20B+Models
CodeStral-22B 22B 78.1 71.5 71.4 77.2 72.3 73.9 69.6 47.5 70.2
DS-Coder-33B-Instruct 33B 79.3 73.4 68.9 74.1 67.9 73.9 72.7 43.0 69.2
DS-Coder-V2-Instruct 21/236B 90.2 82.3 84.8 82.3 83 84.5 79.5 52.5 79.9
Table14: TheperformanceofdifferentapproachesoninstructformatMultiPL-E.
gathersnewproblemsfromleadingcompetitiveprogrammingplatformslikeLeetCode5,
AtCoder6,andCodeForces7,ensuringanup-to-dateanddiversesetofchallenges. Currently,
ithostsover600high-qualitycodingproblemspublishedbetweenMay2023andSeptember
2024.
Tobetterdemonstrateourmodel’seffectivenessonreal-worldcompetitiveprogramming
tasks, we conduct evaluation of the Qwen-2.5-Coder series instruct models on the Live-
CodeBench(2305-2409)dataset. AsillustratedinTable13,theQwen-2.5-Coder-7B-Instruct
modelachievedanimpressivePass@1accuracyof37.6%,significantlyoutperformingother
models of comparable parameter scales. Notably, it also surpassed larger models such
as CodeStral-22B and DS-Coder-33B-Instruct, underscoring the Qwen-2.5-Coder series’
exceptionalcapabilitiesinhandlingcomplexcodegenerationchallenges.
Multi-ProgrammingLanguage TheQwen2.5-Coderseriesinstructmodelshaveinherited
thehighperformanceofthebasemodelontheMulti-ProgrammingLanguage. Tofurther
evaluate their capabilities, we tested the instruct models on two specific benchmarks:
MultiPL-E(Cassanoetal.,2022)andMcEval(Chaietal.,2024).
MultiPL-E As demonstrated by the evaluation results in Table 14, Qwen2.5-Coder-7B-
Instructconsistentlyoutperformsothermodelswiththesamenumberofparameters,in-
cluding DS-Coder-V2-Lite-Instruct, in code generation tasks across eight programming
5https://leetcode.com
6https://atcoder.jp
7https://codeforces.com
16TechnicalReport
languages. Withanaverageaccuracyof76.5%,Qwen2.5-Coder-7B-Instructsurpasseseven
largermodelssuchasCodeStral-22BandDS-Coder-33B-Instruct(despitehavingover20
billionparameters),highlightingitspowerfulcodegenerationcapabilitiesinmultiplepro-
gramminglanguages.
McEvalPerformance
50
0
Average Coffee Groovy Swift Json C# Kotlin Power
50
0
Rust Java VB Haskell R Shell Julia PHP
50
25
0
Scheme F# Python JS Ruby Scala AWK C
50
25
0
C++ Go Lua Racket TS Clisp Elixir Fortran
50
25
0
Perl Elisp Pascal Tcl VimL Erlang Dart Html
Qwen2.5-Coder-7B-Chat CodeStral-20B DS-Coder-V1-6.7B-Instruct
DS-Coder-V2-Lite-Instruct DS-Coder-V1-33B-Instruct CodeQwen1.5-7B-Chat
Figure6: TheMcEvalPerformanceofQwen21.5-Coder-7B-Instructcomparedwithpopular
open-sourcelargecodemodelswithsimilarsize.
McEval TocomprehensivelyassessthecodegenerationcapabilitiesoftheQwen2.5-Coder
seriesmodelsacrossabroaderrangeofprogramminglanguages,weevaluatedthemonthe
McEvalbenchmark(Chaietal.,2024),whichspans40programminglanguagesandincludes
16,000testcases. AsshowninFigure6,theQwen2.5-Coder-7B-Instructmodelexcelswhen
comparedtootheropen-sourcemodelsontheMcEvalbenchmark, particularlyacrossa
widerangeofprogramminglanguages. Itsaverageaccuracynotonlyexceedsthatofmuch
largermodelssuchasDS-Coder-33B-InstructandCodeStral-22Bbutalsodemonstratesa
notableadvantageovermodelsofcomparableparametersize.
7.2 CodeReasoning
ToassessthecodereasoningcapabilitiesoftheQwen2.5-Coderseriesinstructmodels,we
performedanevaluationonCRUXEval(Guetal.,2024). Asillustratedbytheexperimental
resultsinTable15,theQwen2.5-Coder-7B-InstructmodelachievedInput-CoTandOutput-
CoTaccuraciesof65.8%and65.9%,respectively.Thisrepresentsanotableimprovementover
theDS-Coder-V2-Lite-Instructmodel,withgainsof12.8%inInput-CoTaccuracyand13.0%
inOutput-CoTaccuracy. Furthermore,theQwen2.5-Coder-7B-Instructmodeloutperformed
larger models, such as the CodeStral-22B and DS-Coder-33B-Instruct, underscoring its
superiorcodereasoningcapabilitiesdespiteitssmallersize.
Figure7illustratestherelationshipbetweenmodelsizesandcodereasoningcapabilities.
The Qwen2.5-Coder instruct models stand out for delivering superior code reasoning
performancewiththefewestparameters,surpassingtheresultsofotheropen-sourcelarge
language models by a significant margin. According to this trend, we expect that code
reasoningperformancecomparabletoGPT-4ocouldbeachievedwithamodelaroundthe
30billionparametersscale.
17TechnicalReport
CRUXEval
Model Size
Input-CoT Output-CoT
1B+Models
DS-Coder-1.3B-Instruct 1.3B 14.8 28.1
Qwen2.5-Coder-1.5B-Instruct 1.5B 45.4 37.5
6B+Models
DS-Coder-6.7B-Instruct 6.7B 42.6 45.1
DS-Coder-V2-Lite-Instruct 2.4/16B 53.0 52.9
CodeQwen1.5-7B-Chat 7B 42.1 38.5
Qwen2.5-Coder-7B-Instruct 7B 65.8 65.9
20B+Models
CodeStral-22B 22B 48.0 60.6
DS-Coder-33B-Instruct 33B 47.3 50.6
DS-Coder-V2-Instruct 21/236B 70.0 75.1
Table 15: The CRUXEval performance of different instruct models, with Input-CoT and
Output-CoTsettings.
CRUXEval-O (CoT)
90% Best performance/size ratio GPT-4o-0513
Claude-3-Opus
Qwen2.5-Coder-7B-Instruct DS-Coder-V2-Instruct
Llama3-Instruct
Codestral
DS-Coder-V2-Lite-Instruct
DS-Coder-33B-Instruct
45% Qwen2.5-Coder-1.5B-Instruct DS-Coder-6.7B-Instruct
CodeQwen1.5-7B-Chat
DS-Coder-1.3B-Instruct
# of Parameters (B)
0%
0 3B 7B 30B ???
Figure7: Therelationshipbetweenmodelsizesandcodereasoningcapabilities. Thex-axis
representstheparametersizesofdifferentmodels,andthey-axisindicatestheCRUXEval-O
(CoT)scoresrespectively.
7.3 CodeEditing
Aider8hascreatedacodeeditingbenchmarkdesignedtoquantitativelymeasureitscollab-
orationwithlargelanguagemodels(LLMs). Drawingfromasetof133Pythonexercises
sourcedfromExercism9,thebenchmarkteststheabilityofAiderandLLMstointerpret
naturallanguageprogrammingrequestsandtranslatethemintoexecutablecodethatsuc-
cessfullypassesunittests. Thisassessmentgoesbeyondevaluatingrawcodingproficiency;
italsoexamineshoweffectivelyLLMscaneditexistingcodeandformatthosemodifications
forseamlessintegrationwithAider’ssystem,ensuringthatlocalsourcefilescanbeupdated
withoutissues. Thecomprehensivenatureofthisbenchmarkreflectsboththetechnical
aptitudeoftheLLMsandtheirconsistencyintaskcompletion.
Table16highlightstheperformanceofseverallanguagemodelsintheCodeEditingtask.
Amongthem,Qwen2.5-Coder-7B-Instructdemonstratesoutstandingcoderepaircapabil-
ities. Despiteitsrelativelymodestsizeof7billionparameters,itachievesanimpressive
8https://github.com/paul-gauthier/aider
9https://github.com/exercism/python
18TechnicalReport
Aider
Model Size
Pass@1 Pass@2
1B+Models
DS-Coder-1.3B-Instruct 1.3B 17.3* 21.1*
Qwen2.5-Coder-1.5B-Instruct 1.5B 30.1 33.1
6B+Models
DS-Coder-6.7B-Instruct 6.7B 34.6* 42.1*
DS-Coder-V2-Lite-Instruct 2.4/16B 48.9* 55.6*
CodeQwen1.5-7B-Chat 7B 23.3 33.1
Qwen2.5-Coder-7B-Instruct 7B 50.4 57.1
20B+Models
CodeStral-22B 22B 35.3* 45.1*
DS-Coder-33B-Instruct 33B 49.6* 59.4*
DS-Coder-V2-Instruct 21/236B 73.7 -
Table16:ThecodeeditingabilityofdifferentinstructmodelsevaluatedbyAiderbenchmark.
*indicatesthattheexperimentalresultshavebeenreproducedinourexperiments,andthe
wholeedit-formatwasconsistentlyappliedacrossallexperiments.
PASS@1accuracyof50.4%,significantlyoutperformingcomparablemodels. Notably,italso
surpasseslargermodelssuchasCodeStral-22B(22billionparameters)andDS-Coder-33B-
Instruct(33billionparameters),showcasingitsremarkableefficiencyandeffectivenessin
codeeditingtasks.
7.4 Text-to-SQL
SQL is one of the essential tools in daily software development and production, but its
steeplearningcurveoftenhindersfreeinteractionbetweennon-programmingexpertsand
databases. Toaddressthisissue,theText-to-SQLtaskwasintroduced,aimingformodels
to automatically map natural language questions to structured SQL queries. Previous
improvements in Text-to-SQL focused primarily on structure-aware learning, domain-
specificpre-training,andsophisticatedpromptdesigns.
Thankstotheuseoffinelycraftedsyntheticdataduringbothpre-trainingandfine-tuning,
wesignificantlyenhancedQwen2.5-Coder’scapabilityinText-to-SQLtasks.Weselectedtwo
well-knownbenchmarks,Spider(Yuetal.,2018)andBIRD(Lietal.,2024),forcomprehensive
evaluation. ToensureafaircomparisonbetweenQwen2.5-Coderandotheropen-source
languagemodelsonthistask,weusedaunifiedprompttemplateasinput,followingthe
workofChang&Fosler-Lussier(2023). AsshowninFigure8,thepromptconsistsoftable
representations aligned with database instructions, examples of table content, optional
additionalknowledge,andnaturallanguagequestions. Thisstandardizedprompttemplate
minimizesbiasesthatmayarisefrompromptvariations. AsshowninFigure9,Qwen2.5-
CoderoutperformsothercodemodelsofthesamesizeontheText-to-SQLtask.
Model Size MATH GSM8K GaoKao2023en OlympiadBench CollegeMath AIME24
DS-Coder-V2-Lite-Instruct 2.4/16B 61.0 87.6 56.1 26.4 39.8 6.7
Qwen2.5-Coder-7B-Instruct 7B 66.8 86.7 60.5 29.8 43.5 10.0
Model Size AMC23 MMLU MMLU-Pro IFEval CEval GPQA
DS-Coder-V2-Lite-Instruct 2.4/16B 40.4 42.5 60.6 38.6 60.1 27.6
Qwen2.5-Coder-7B-Instruct 7B 42.5 45.6 68.7 58.6 61.4 35.6
Table17: TheperformanceofmathandGeneral.
19TechnicalReport
Prompttemplatefortext-to-SQLtasks.
CREATE TABLE "list" (
"LastName" TEXT,
"FirstName" TEXT,
"Grade" INTEGER,
"Classroom" INTEGER,
PRIMARY KEY(LastName, FirstName)
);
/* 3 example rows:
SELECT * FROM list LIMIT 3;
LastName FirstName Grade Classroom
CAR MAUDE 2 101
KRISTENSEN STORMY 6 112
VANDERWOUDE SHERWOOD 3 107
*/
(...other tables omitted...)
-- External Knowledge: ...
-- Using valid SQLite and understanding External Knowledge, answer the following
questions for the tables provided above.
Question: How many students are there?
SELECT count(*) FROM list;
Figure8: PrompttemplateofQwen2.5-Coderfortext-to-SQLtasks.
Qwen2.5-Coder-7B-Instruct 51.1 82.0
CodeStral-22B 46.2 76.6
DS-Coder-33B-Instruct 45.6 73.8
DS-Coder-V2-Lite-Instruct 41.6 74.6
DS-Coder-6.7B-Instruct 39.8 70.0
DS-Coder-1.3B-Instruct BBiirrdd 22.0 59.0 SSppiiddeerr
60 50 40 30 20 40 50 60 70 80 90
1
Figure9: Thetext-to-SQLevaluationonvariousinstructcodemodels.
7.5 MathReasoningandGeneralNaturalLanguage
Inthissection,wepresentacomparativeanalysisoftheperformancebetweenourQwen2.5-
Coder-7B-InstructmodelandtheDS-Coder-V2-Lite-Instructmodel,focusingonbothmath-
ematicalcomputationandgeneralnaturallanguageprocessingtasks. AsindicatedinTable
17,theQwen2.5-Coder-7B-InstructmodeloutperformstheDS-Coder-V2-Lite-Instructin
11 out of 12 tasks. This result underscores the model’s versatility, excelling not only in
complexcodingtasksbutalsoinsophisticatedgeneraltasks,thusdistinguishingitfromits
competitors.
8 Conclusion
ThisworkintroducesQwen2.5-Coder,thelatestadditiontotheQwenseries. Builtupon
Qwen2.5,atop-tieropen-sourceLLM,Qwen2.5-Coderhasbeendevelopedthroughexten-
sivepre-trainingandpost-trainingofQwen2.5-1.5BandQwen2.5-7Bonlarge-scaledatasets.
To ensure the quality of the pre-training data, we have curated a dataset by collecting
publiccodedataandextractinghigh-qualitycode-relatedcontentfromwebtexts,while
20TechnicalReport
filteringoutlow-qualitydatausingadvancedclassifiers. Additionally,wehaveconstructed
ameticulouslydesignedinstruction-tuningdatasettotransformthebasecodeLLMintoa
strongcodingassistant.
Lookingahead,ourresearchwillfocusonexploringtheimpactofscalingupcodeLLMs
intermsofbothdatasizeandmodelsize. Wewillalsocontinuetoenhancethereasoning
capabilitiesofthesemodels,aimingtopushtheboundariesofwhatcodeLLMscanachieve.
References
JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoni
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
Gpt-4technicalreport. arXivpreprintarXiv:2303.08774,2023.
LoubnaBenAllal,RaymondLi,DenisKocetkov,ChenghaoMou,ChristopherAkiki,Car-
losMunozFerrandis,NiklasMuennighoff,MayankMishra,AlexGu,MananDey,etal.
Santacoder: don’treachforthestars! arXivpreprintarXiv:2301.03988,2023.
Anthropic.Claude3.5sonnet.https://www.anthropic.com/news/claude-3-5-sonnet,2024.
2024.06.21.
JacobAustin,AugustusOdena,MaxwellNye,MaartenBosma,HenrykMichalewski,David
Dohan,EllenJiang,CarrieCai,MichaelTerry,QuocLe,etal. Programsynthesiswith
largelanguagemodels. arXivpreprintarXiv:2108.07732,2021.
JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,Wenbin
Ge, YuHan, FeiHuang, etal. Qwentechnicalreport. arXivpreprintarXiv:2309.16609,
2023a.
JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,Wenbin
Ge, YuHan, FeiHuang, etal. Qwentechnicalreport. arXivpreprintarXiv:2309.16609,
2023b.
MohammadBavarian,HeewooJun,NikolasTezak,JohnSchulman,ChristineMcLeavey,
JerryTworek,andMarkChen. Efficienttrainingoflanguagemodelstofillinthemiddle.
arXivpreprintarXiv:2207.14255,2022.
TomBBrown. Languagemodelsarefew-shotlearners. arXivpreprintarXiv:2005.14165,2020.
FedericoCassano, JohnGouwar, DanielNguyen, SydneyNguyen, LunaPhipps-Costin,
Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feld-
man,etal. Multipl-e: Ascalableandextensibleapproachtobenchmarkingneuralcode
generation. arXivpreprintarXiv:2208.08227,2022.
LinzhengChai,ShukaiLiu,JianYang,YuweiYin,KeJin,JiahengLiu,TaoSun,GeZhang,
ChangyuRen,HongchengGuo,etal. Mceval: Massivelymultilingualcodeevaluation.
arXivpreprintarXiv:2406.07436,2024.
Shuaichen Chang and Eric Fosler-Lussier. How to prompt llms for text-to-sql: A study
inzero-shot,single-domain,andcross-domainsettings. arXivpreprintarXiv:2305.11853,
2023.
MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondeDeOliveiraPinto,
JaredKaplan,HarriEdwards,YuriBurda,NicholasJoseph,GregBrockman,etal. Evalu-
atinglargelanguagemodelstrainedoncode. arXivpreprintarXiv:2107.03374,2021.
WenhuChen,MingYin,MaxKu,PanLu,YixinWan,XueguangMa,JianyuXu,XinyiWang,
andTonyXia. Theoremqa: Atheorem-drivenquestionansweringdataset. InProceedings
ofthe2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.7889–7901,
2023.
PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,
andOyvindTafjord.Thinkyouhavesolvedquestionanswering?tryarc,theai2reasoning
challenge. arXivpreprintarXiv:1803.05457,2018.
21TechnicalReport
KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,LukaszKaiser,
MatthiasPlappert,JerryTworek,JacobHilton,ReiichiroNakano,etal. Trainingverifiers
tosolvemathwordproblems. arXivpreprintarXiv:2110.14168,2021.
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,
AieshaLetman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3
herdofmodels. arXivpreprintarXiv:2407.21783,2024.
Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto
Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad
RezaGhasemiMadani,etal. Arewedonewithmmlu? arXivpreprintarXiv:2406.04127,
2024.
AlexGu,BaptisteRozie`re,HughLeather,ArmandoSolar-Lezama,GabrielSynnaeve,and
SidaIWang. Cruxeval: Abenchmarkforcodereasoning,understandingandexecution.
arXivpreprintarXiv:2401.03065,2024.
DayaGuo,QihaoZhu,DejianYang,ZhendaXie,KaiDong,WentaoZhang,GuantingChen,
XiaoBi, YuWu, YKLi, etal. Deepseek-coder: Whenthelargelanguagemodelmeets
programming–theriseofcodeintelligence. arXivpreprintarXiv:2401.14196,2024.
DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,DawnSong,and
JacobSteinhardt. Measuringmassivemultitasklanguageunderstanding. arXivpreprint
arXiv:2009.03300,2020.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang,
DawnSong,andJacobSteinhardt. Measuringmathematicalproblemsolvingwiththe
mathdataset. arXivpreprintarXiv:2103.03874,2021.
NamanJain,KingHan,AlexGu,Wen-DingLi,FanjiaYan,TianjunZhang,SidaWang,Ar-
mandoSolar-Lezama,KoushikSen,andIonStoica. Livecodebench: Holisticandcontami-
nationfreeevaluationoflargelanguagemodelsforcode. arXivpreprintarXiv:2403.07974,
2024.
AQJiang,ASablayrolles,AMensch,CBamford,DSChaplot,DdelasCasas,FBressand,
GLengyel,GLample,LSaulnier,etal. Mistral7b(2023). arXivpreprintarXiv:2310.06825,
2023.
JinyangLi,BinyuanHui,GeQu,JiaxiYang,BinhuaLi,BowenLi,BailinWang,BowenQin,
RuiyingGeng,NanHuo,etal. Canllmalreadyserveasadatabaseinterface? abigbench
forlarge-scaledatabasegroundedtext-to-sqls. AdvancesinNeuralInformationProcessing
Systems,36,2024.
RaymondLi,LoubnaBenAllal,YangtianZi,NiklasMuennighoff,DenisKocetkov,Cheng-
haoMou,MarcMarone,ChristopherAkiki,JiaLi,JennyChim,etal. Starcoder: maythe
sourcebewithyou! arXivpreprintarXiv:2305.06161,2023.
StephanieLin,JacobHilton,andOwainEvans. Truthfulqa: Measuringhowmodelsmimic
humanfalsehoods. arXivpreprintarXiv:2109.07958,2021.
J Liu, CS Xia, Y Wang, and L Zhang. Is your code generated by chatgpt really correct?
rigorousevaluationoflargelanguagemodelsforcodegeneration.arxivpreprintarxiv:
230501210.2023,2023.
Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier,
NouamaneTazi,AoTang,DmytroPykhtar,JiaweiLiu,YuxiangWei,etal. Starcoder2
andthestackv2: Thenextgeneration. arXivpreprintarXiv:2402.19173,2024.
OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o,2024. 2024.05.13.
BowenPeng,JeffreyQuesnelle,HongluFan,andEnricoShippole. Yarn: Efficientcontext
windowextensionoflargelanguagemodels. arXivpreprintarXiv:2309.00071,2023.
22TechnicalReport
BaptisteRoziere,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,XiaoqingEllenTan,
YossiAdi,JingyuLiu,RomainSauvestre,TalRemez,etal. Codellama: Openfoundation
modelsforcode. arXivpreprintarXiv:2308.12950,2023.
KeisukeSakaguchi,RonanLeBras,ChandraBhagavatula,andYejinChoi. Anadversarial
winogradschemachallengeatscale. arXivpreprintarXiv:1907.10641,2019.
TaoSun,LinzhengChai,JianYang,YuweiYin,HongchengGuo,JiahengLiu,BingWang,
LiqunYang,andZhoujunLi. Unicoder: Scalingcodelargelanguagemodelviauniversal
code. InLun-WeiKu,AndreMartins,andVivekSrikumar(eds.),Proceedingsofthe62nd
AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),ACL
2024,Bangkok,Thailand,August11-16,2024,pp.1812–1824.AssociationforComputational
Linguistics,2024. URLhttps://aclanthology.org/2024.acl-long.100.
MistralAIteam. Codestral. https://mistral.ai/news/codestral,2024. 2024.05.29.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:
Openfoundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
AnYang,BaosongYang,BinyuanHui,BoZheng,BowenYu,ChangZhou,ChengpengLi,
ChengyuanLi,DayihengLiu,FeiHuang,etal. Qwen2technicalreport. arXivpreprint
arXiv:2407.10671,2024.
Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma,
Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: A large-scale human-labeled
datasetforcomplexandcross-domainsemanticparsingandtext-to-sqltask.arXivpreprint
arXiv:1809.08887,2018.
RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi. Hellaswag: Cana
machinereallyfinishyoursentence? arXivpreprintarXiv:1905.07830,2019.
TerryYueZhuo,MinhChienVu,JennyChim,HanHu,WenhaoYu,RatnadiraWidyasari,
Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench:
Benchmarking code generation with diverse function calls and complex instructions.
arXivpreprintarXiv:2406.15877,2024.
23