[
    {
        "title": "FLAME: Learning to Navigate with Multimodal LLM in Urban Environments",
        "authors": "Yunzhe XuYiyuan PanZhe LiuHesheng Wang",
        "links": "http://arxiv.org/abs/2408.11051v1",
        "entry_id": "http://arxiv.org/abs/2408.11051v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11051v1",
        "summary": "Large Language Models (LLMs) have demonstrated potential in\nVision-and-Language Navigation (VLN) tasks, yet current applications face\nchallenges. While LLMs excel in general conversation scenarios, they struggle\nwith specialized navigation tasks, yielding suboptimal performance compared to\nspecialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied\nAgent), a novel Multimodal LLM-based agent and architecture designed for urban\nVLN tasks that efficiently handles multiple observations. Our approach\nimplements a three-phase tuning technique for effective adaptation to\nnavigation tasks, including single perception tuning for street view\ndescription, multiple perception tuning for trajectory summarization, and\nend-to-end training on VLN datasets. The augmented datasets are synthesized\nautomatically. Experimental results demonstrate FLAME's superiority over\nexisting methods, surpassing state-of-the-art methods by a 7.3% increase in\ntask completion rate on Touchdown dataset. This work showcases the potential of\nMultimodal LLMs (MLLMs) in complex navigation tasks, representing an\nadvancement towards practical applications of MLLMs in embodied AI. Project\npage: https://flame-sjtu.github.io",
        "updated": "2024-08-20 17:57:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11051v1"
    },
    {
        "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding",
        "authors": "Jian ChenVashisth TiwariRanajoy SadhukhanZhuoming ChenJinyuan ShiIan En-Hsu YenBeidi Chen",
        "links": "http://arxiv.org/abs/2408.11049v1",
        "entry_id": "http://arxiv.org/abs/2408.11049v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11049v1",
        "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.",
        "updated": "2024-08-20 17:57:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11049v1"
    },
    {
        "title": "Inside the Black Box: Detecting Data Leakage in Pre-trained Language Encoders",
        "authors": "Yuan XinZheng LiNing YuDingfan ChenMario FritzMichael BackesYang Zhang",
        "links": "http://arxiv.org/abs/2408.11046v1",
        "entry_id": "http://arxiv.org/abs/2408.11046v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11046v1",
        "summary": "Despite being prevalent in the general field of Natural Language Processing\n(NLP), pre-trained language models inherently carry privacy and copyright\nconcerns due to their nature of training on large-scale web-scraped data. In\nthis paper, we pioneer a systematic exploration of such risks associated with\npre-trained language encoders, specifically focusing on the membership leakage\nof pre-training data exposed through downstream models adapted from pre-trained\nlanguage encoders-an aspect largely overlooked in existing literature. Our\nstudy encompasses comprehensive experiments across four types of pre-trained\nencoder architectures, three representative downstream tasks, and five\nbenchmark datasets. Intriguingly, our evaluations reveal, for the first time,\nthe existence of membership leakage even when only the black-box output of the\ndownstream model is exposed, highlighting a privacy risk far greater than\npreviously assumed. Alongside, we present in-depth analysis and insights toward\nguiding future researchers and practitioners in addressing the privacy\nconsiderations in developing pre-trained language models.",
        "updated": "2024-08-20 17:55:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11046v1"
    },
    {
        "title": "Scaling Law with Learning Rate Annealing",
        "authors": "Howe TissueVenus WangLu Wang",
        "links": "http://arxiv.org/abs/2408.11029v1",
        "entry_id": "http://arxiv.org/abs/2408.11029v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11029v1",
        "summary": "We find that the cross-entropy loss curves of neural language models\nempirically adhere to a scaling law with learning rate (LR) annealing over\ntraining steps ($s$): $$L(s) = L_0 + A\\cdot S_1^{-\\alpha} - C\\cdot S_2$$ Where\n$S_1$ is forward area and $S_2$ is learning rate annealing area. This\nformulation takes into account two factors: (1) The forward scaling defined as\ntypical scaling law, and (2) the additional loss drop brought by LR annealing.\nTherefore, this formulation can describe the full loss curve at each step,\nrather than the single loss point at the end of training. Applying the scaling\nlaw with LR annealing and fitting only one or two training curves, we can\naccurately predict the loss of language model training at any given step and\nacross any learning rate scheduler (LRS). Furthermore, this equation accurately\ndescribes the dynamics during training process, and provides a theoretical\nverification and explanation for numerous experimental findings of previous\nstudies, particularly those focusing on LR schedule and LR annealing. The\nresulting insights, also serve as a guide for researchers to select critical\nLRS in advance by prediction using our equation. Most significantly, since all\nthe points in a full training curve follow the equation, we can achieve\naccurate loss prediction at any given step across any learning rate scheduler,\nwhile expending less than 1\\% of the computational cost required by the\nchinchilla scaling law to fit language modeling loss. This approach extremely\ndemocratizes scaling law fitting and predicting in developing large language\nmodels.",
        "updated": "2024-08-20 17:30:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11029v1"
    },
    {
        "title": "Athena: Safe Autonomous Agents with Verbal Contrastive Learning",
        "authors": "Tanmana SadhuAli PesaranghaderYanan ChenDong Hoon Yi",
        "links": "http://arxiv.org/abs/2408.11021v1",
        "entry_id": "http://arxiv.org/abs/2408.11021v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11021v1",
        "summary": "Due to emergent capabilities, large language models (LLMs) have been utilized\nas language-based agents to perform a variety of tasks and make decisions with\nan increasing degree of autonomy. These autonomous agents can understand\nhigh-level instructions, interact with their environments, and execute complex\ntasks using a selection of tools available to them. As the capabilities of the\nagents expand, ensuring their safety and trustworthiness becomes more\nimperative. In this study, we introduce the Athena framework which leverages\nthe concept of verbal contrastive learning where past safe and unsafe\ntrajectories are used as in-context (contrastive) examples to guide the agent\ntowards safety while fulfilling a given task. The framework also incorporates a\ncritiquing mechanism to guide the agent to prevent risky actions at every step.\nFurthermore, due to the lack of existing benchmarks on the safety reasoning\nability of LLM-based agents, we curate a set of 80 toolkits across 8 categories\nwith 180 scenarios to provide a safety evaluation benchmark. Our experimental\nevaluation, with both closed- and open-source LLMs, indicates verbal\ncontrastive learning and interaction-level critiquing improve the safety rate\nsignificantly.",
        "updated": "2024-08-20 17:21:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11021v1"
    }
]