[
    {
        "title": "Approximation Rates for Shallow ReLU$^k$ Neural Networks on Sobolev Spaces via the Radon Transform",
        "authors": "Tong MaoJonathan W. SiegelJinchao Xu",
        "links": "http://arxiv.org/abs/2408.10996v1",
        "entry_id": "http://arxiv.org/abs/2408.10996v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10996v1",
        "summary": "Let $\\Omega\\subset \\mathbb{R}^d$ be a bounded domain. We consider the problem\nof how efficiently shallow neural networks with the ReLU$^k$ activation\nfunction can approximate functions from Sobolev spaces $W^s(L_p(\\Omega))$ with\nerror measured in the $L_q(\\Omega)$-norm. Utilizing the Radon transform and\nrecent results from discrepancy theory, we provide a simple proof of nearly\noptimal approximation rates in a variety of cases, including when $q\\leq p$,\n$p\\geq 2$, and $s \\leq k + (d+1)/2$. The rates we derive are optimal up to\nlogarithmic factors, and significantly generalize existing results. An\ninteresting consequence is that the adaptivity of shallow ReLU$^k$ neural\nnetworks enables them to obtain optimal approximation rates for smoothness up\nto order $s = k + (d+1)/2$, even though they represent piecewise polynomials of\nfixed degree $k$.",
        "updated": "2024-08-20 16:43:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10996v1"
    },
    {
        "title": "Kernel-Based Differentiable Learning of Non-Parametric Directed Acyclic Graphical Models",
        "authors": "Yurou LiangOleksandr ZadorozhnyiMathias Drton",
        "links": "http://arxiv.org/abs/2408.10976v1",
        "entry_id": "http://arxiv.org/abs/2408.10976v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10976v1",
        "summary": "Causal discovery amounts to learning a directed acyclic graph (DAG) that\nencodes a causal model. This model selection problem can be challenging due to\nits large combinatorial search space, particularly when dealing with\nnon-parametric causal models. Recent research has sought to bypass the\ncombinatorial search by reformulating causal discovery as a continuous\noptimization problem, employing constraints that ensure the acyclicity of the\ngraph. In non-parametric settings, existing approaches typically rely on\nfinite-dimensional approximations of the relationships between nodes, resulting\nin a score-based continuous optimization problem with a smooth acyclicity\nconstraint. In this work, we develop an alternative approximation method by\nutilizing reproducing kernel Hilbert spaces (RKHS) and applying general\nsparsity-inducing regularization terms based on partial derivatives. Within\nthis framework, we introduce an extended RKHS representer theorem. To enforce\nacyclicity, we advocate the log-determinant formulation of the acyclicity\nconstraint and show its stability. Finally, we assess the performance of our\nproposed RKHS-DAGMA procedure through simulations and illustrative data\nanalyses.",
        "updated": "2024-08-20 16:09:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10976v1"
    },
    {
        "title": "Conformalized Interval Arithmetic with Symmetric Calibration",
        "authors": "Rui LuoZhixin Zhou",
        "links": "http://arxiv.org/abs/2408.10939v1",
        "entry_id": "http://arxiv.org/abs/2408.10939v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10939v1",
        "summary": "Uncertainty quantification is essential in decision-making, especially when\njoint distributions of random variables are involved. While conformal\nprediction provides distribution-free prediction sets with valid coverage\nguarantees, it traditionally focuses on single predictions. This paper\nintroduces novel conformal prediction methods for estimating the sum or average\nof unknown labels over specific index sets. We develop conformal prediction\nintervals for single target to the prediction interval for sum of multiple\ntargets. Under permutation invariant assumptions, we prove the validity of our\nproposed method. We also apply our algorithms on class average estimation and\npath cost prediction tasks, and we show that our method outperforms existing\nconformalized approaches as well as non-conformal approaches.",
        "updated": "2024-08-20 15:27:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10939v1"
    },
    {
        "title": "Feature Selection from Differentially Private Correlations",
        "authors": "Ryan SwopeAmol KhannaPhilip DoldoSaptarshi RoyEdward Raff",
        "links": "http://arxiv.org/abs/2408.10862v1",
        "entry_id": "http://arxiv.org/abs/2408.10862v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10862v1",
        "summary": "Data scientists often seek to identify the most important features in\nhigh-dimensional datasets. This can be done through $L_1$-regularized\nregression, but this can become inefficient for very high-dimensional datasets.\nAdditionally, high-dimensional regression can leak information about individual\ndatapoints in a dataset. In this paper, we empirically evaluate the established\nbaseline method for feature selection with differential privacy, the two-stage\nselection technique, and show that it is not stable under sparsity. This makes\nit perform poorly on real-world datasets, so we consider a different approach\nto private feature selection. We employ a correlations-based order statistic to\nchoose important features from a dataset and privatize them to ensure that the\nresults do not leak information about individual datapoints. We find that our\nmethod significantly outperforms the established baseline for private feature\nselection on many datasets.",
        "updated": "2024-08-20 13:54:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10862v1"
    },
    {
        "title": "Sparse Regression for Discovery of Constitutive Models from Oscillatory Shear Measurements",
        "authors": "Sachin ShanbhagGordon Erlebacher",
        "links": "http://arxiv.org/abs/2408.10762v1",
        "entry_id": "http://arxiv.org/abs/2408.10762v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10762v1",
        "summary": "We propose sparse regression as an alternative to neural networks for the\ndiscovery of parsimonious constitutive models (CMs) from oscillatory shear\nexperiments. Symmetry and frame-invariance are strictly imposed by using tensor\nbasis functions to isolate and describe unknown nonlinear terms in the CMs. We\ngenerate synthetic experimental data using the Giesekus and Phan-Thien Tanner\nCMs, and consider two different scenarios. In the complete information\nscenario, we assume that the shear stress, along with the first and second\nnormal stress differences, is measured. This leads to a sparse linear\nregression problem that can be solved efficiently using $l_1$ regularization.\nIn the partial information scenario, we assume that only shear stress data is\navailable. This leads to a more challenging sparse nonlinear regression\nproblem, for which we propose a greedy two-stage algorithm. In both scenarios,\nthe proposed methods fit and interpolate the training data remarkably well.\nPredictions of the inferred CMs extrapolate satisfactorily beyond the range of\ntraining data for oscillatory shear. They also extrapolate reasonably well to\nflow conditions like startup of steady and uniaxial extension that are not used\nin the identification of CMs. We discuss ramifications for experimental design,\npotential algorithmic improvements, and implications of the non-uniqueness of\nCMs inferred from partial information.",
        "updated": "2024-08-20 11:52:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10762v1"
    }
]