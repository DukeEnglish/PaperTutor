[
    {
        "title": "NeCo: Improving DINOv2's spatial representations in 19 GPU hours with Patch Neighbor Consistency",
        "authors": "Valentinos ParizaMohammadreza SalehiGertjan BurghoutsFrancesco LocatelloYuki M. Asano",
        "links": "http://arxiv.org/abs/2408.11054v1",
        "entry_id": "http://arxiv.org/abs/2408.11054v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11054v1",
        "summary": "We propose sorting patch representations across views as a novel\nself-supervised learning signal to improve pretrained representations. To this\nend, we introduce NeCo: Patch Neighbor Consistency, a novel training loss that\nenforces patch-level nearest neighbor consistency across a student and teacher\nmodel, relative to reference batches. Our method leverages a differentiable\nsorting method applied on top of pretrained representations, such as\nDINOv2-registers to bootstrap the learning signal and further improve upon\nthem. This dense post-pretraining leads to superior performance across various\nmodels and datasets, despite requiring only 19 hours on a single GPU. We\ndemonstrate that this method generates high-quality dense feature encoders and\nestablish several new state-of-the-art results: +5.5% and + 6% for\nnon-parametric in-context semantic segmentation on ADE20k and Pascal VOC, and\n+7.2% and +5.7% for linear segmentation evaluations on COCO-Things and -Stuff.",
        "updated": "2024-08-20 17:58:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11054v1"
    },
    {
        "title": "Revisiting VerilogEval: Newer LLMs, In-Context Learning, and Specification-to-RTL Tasks",
        "authors": "Nathaniel PinckneyChristopher BattenMingjie LiuHaoxing RenBrucek Khailany",
        "links": "http://arxiv.org/abs/2408.11053v1",
        "entry_id": "http://arxiv.org/abs/2408.11053v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11053v1",
        "summary": "The application of large-language models (LLMs) to digital hardware code\ngeneration is an emerging field. Most LLMs are primarily trained on natural\nlanguage and software code. Hardware code, such as Verilog, represents only a\nsmall portion of the training data and few hardware benchmarks exist. To\naddress this gap, the open-source VerilogEval benchmark was released in 2023,\nproviding a consistent evaluation framework for LLMs on code completion tasks.\nIt was tested on state-of-the-art models at the time including GPT-4. However,\nVerilogEval and other Verilog generation benchmarks lack failure analysis and,\nin present form, are not conducive to exploring prompting techniques. Also,\nsince VerilogEval's release, both commercial and open-source models have seen\ncontinued development.\n  In this work, we evaluate new commercial and open-source models of varying\nsizes against an improved VerilogEval benchmark suite. We enhance VerilogEval's\ninfrastructure and dataset by automatically classifying failures, introduce new\nprompts for supporting in-context learning (ICL) examples, and extend the\nsupported tasks to specification-to-RTL translation. We find a measurable\nimprovement in commercial state-of-the-art models, with GPT-4 Turbo achieving a\n59% pass rate on spec-to-RTL tasks. We also study the performance of\nopen-source and domain-specific models that have emerged, and demonstrate that\nmodels can benefit substantially from ICL. We find that recently-released Llama\n3.1 405B achieves a pass rate of 58%, effectively matching that of GPT-4 Turbo,\nand that the much smaller domain-specific RTL-Coder 6.7B models achieve an\nimpressive 37% pass rate. However, prompt engineering is key to achieving good\npass rates, and varies widely with model and task. A benchmark infrastructure\nthat allows for prompt engineering and failure analysis is key to continued\nmodel development and deployment.",
        "updated": "2024-08-20 17:58:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11053v1"
    },
    {
        "title": "Accelerating Goal-Conditioned RL Algorithms and Research",
        "authors": "Michał BortkiewiczWładek PałuckiVivek MyersTadeusz DziarmagaTomasz ArczewskiŁukasz KucińskiBenjamin Eysenbach",
        "links": "http://arxiv.org/abs/2408.11052v1",
        "entry_id": "http://arxiv.org/abs/2408.11052v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11052v1",
        "summary": "Self-supervision has the potential to transform reinforcement learning (RL),\nparalleling the breakthroughs it has enabled in other areas of machine\nlearning. While self-supervised learning in other domains aims to find patterns\nin a fixed dataset, self-supervised goal-conditioned reinforcement learning\n(GCRL) agents discover new behaviors by learning from the goals achieved during\nunstructured interaction with the environment. However, these methods have\nfailed to see similar success, both due to a lack of data from slow\nenvironments as well as a lack of stable algorithms. We take a step toward\naddressing both of these issues by releasing a high-performance codebase and\nbenchmark JaxGCRL for self-supervised GCRL, enabling researchers to train\nagents for millions of environment steps in minutes on a single GPU. The key to\nthis performance is a combination of GPU-accelerated environments and a stable,\nbatched version of the contrastive reinforcement learning algorithm, based on\nan infoNCE objective, that effectively makes use of this increased data\nthroughput. With this approach, we provide a foundation for future research in\nself-supervised GCRL, enabling researchers to quickly iterate on new ideas and\nevaluate them in a diverse set of challenging environments. Website + Code:\nhttps://github.com/MichalBortkiewicz/JaxGCRL",
        "updated": "2024-08-20 17:58:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11052v1"
    },
    {
        "title": "FLAME: Learning to Navigate with Multimodal LLM in Urban Environments",
        "authors": "Yunzhe XuYiyuan PanZhe LiuHesheng Wang",
        "links": "http://arxiv.org/abs/2408.11051v1",
        "entry_id": "http://arxiv.org/abs/2408.11051v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11051v1",
        "summary": "Large Language Models (LLMs) have demonstrated potential in\nVision-and-Language Navigation (VLN) tasks, yet current applications face\nchallenges. While LLMs excel in general conversation scenarios, they struggle\nwith specialized navigation tasks, yielding suboptimal performance compared to\nspecialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied\nAgent), a novel Multimodal LLM-based agent and architecture designed for urban\nVLN tasks that efficiently handles multiple observations. Our approach\nimplements a three-phase tuning technique for effective adaptation to\nnavigation tasks, including single perception tuning for street view\ndescription, multiple perception tuning for trajectory summarization, and\nend-to-end training on VLN datasets. The augmented datasets are synthesized\nautomatically. Experimental results demonstrate FLAME's superiority over\nexisting methods, surpassing state-of-the-art methods by a 7.3% increase in\ntask completion rate on Touchdown dataset. This work showcases the potential of\nMultimodal LLMs (MLLMs) in complex navigation tasks, representing an\nadvancement towards practical applications of MLLMs in embodied AI. Project\npage: https://flame-sjtu.github.io",
        "updated": "2024-08-20 17:57:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11051v1"
    },
    {
        "title": "RP1M: A Large-Scale Motion Dataset for Piano Playing with Bi-Manual Dexterous Robot Hands",
        "authors": "Yi ZhaoLe ChenJan SchneiderQuankai GaoJuho KannalaBernhard SchölkopfJoni PajarinenDieter Büchler",
        "links": "http://arxiv.org/abs/2408.11048v1",
        "entry_id": "http://arxiv.org/abs/2408.11048v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11048v1",
        "summary": "It has been a long-standing research goal to endow robot hands with\nhuman-level dexterity. Bi-manual robot piano playing constitutes a task that\ncombines challenges from dynamic tasks, such as generating fast while precise\nmotions, with slower but contact-rich manipulation problems. Although\nreinforcement learning based approaches have shown promising results in\nsingle-task performance, these methods struggle in a multi-song setting. Our\nwork aims to close this gap and, thereby, enable imitation learning approaches\nfor robot piano playing at scale. To this end, we introduce the Robot Piano 1\nMillion (RP1M) dataset, containing bi-manual robot piano playing motion data of\nmore than one million trajectories. We formulate finger placements as an\noptimal transport problem, thus, enabling automatic annotation of vast amounts\nof unlabeled songs. Benchmarking existing imitation learning approaches shows\nthat such approaches reach state-of-the-art robot piano playing performance by\nleveraging RP1M.",
        "updated": "2024-08-20 17:56:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11048v1"
    }
]