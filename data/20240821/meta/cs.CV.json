[
    {
        "title": "Prompt-Guided Image-Adaptive Neural Implicit Lookup Tables for Interpretable Image Enhancement",
        "authors": "Satoshi Kosugi",
        "links": "http://dx.doi.org/10.1145/3664647.3680743",
        "entry_id": "http://arxiv.org/abs/2408.11055v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11055v1",
        "summary": "In this paper, we delve into the concept of interpretable image enhancement,\na technique that enhances image quality by adjusting filter parameters with\neasily understandable names such as \"Exposure\" and \"Contrast\". Unlike using\npredefined image editing filters, our framework utilizes learnable filters that\nacquire interpretable names through training. Our contribution is two-fold.\nFirstly, we introduce a novel filter architecture called an image-adaptive\nneural implicit lookup table, which uses a multilayer perceptron to implicitly\ndefine the transformation from input feature space to output color space. By\nincorporating image-adaptive parameters directly into the input features, we\nachieve highly expressive filters. Secondly, we introduce a prompt guidance\nloss to assign interpretable names to each filter. We evaluate visual\nimpressions of enhancement results, such as exposure and contrast, using a\nvision and language model along with guiding prompts. We define a constraint to\nensure that each filter affects only the targeted visual impression without\ninfluencing other attributes, which allows us to obtain the desired filter\neffects. Experimental results show that our method outperforms existing\npredefined filter-based methods, thanks to the filters optimized to predict\ntarget results. Our source code is available at\nhttps://github.com/satoshi-kosugi/PG-IA-NILUT.",
        "updated": "2024-08-20 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11055v1"
    },
    {
        "title": "NeCo: Improving DINOv2's spatial representations in 19 GPU hours with Patch Neighbor Consistency",
        "authors": "Valentinos ParizaMohammadreza SalehiGertjan BurghoutsFrancesco LocatelloYuki M. Asano",
        "links": "http://arxiv.org/abs/2408.11054v1",
        "entry_id": "http://arxiv.org/abs/2408.11054v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11054v1",
        "summary": "We propose sorting patch representations across views as a novel\nself-supervised learning signal to improve pretrained representations. To this\nend, we introduce NeCo: Patch Neighbor Consistency, a novel training loss that\nenforces patch-level nearest neighbor consistency across a student and teacher\nmodel, relative to reference batches. Our method leverages a differentiable\nsorting method applied on top of pretrained representations, such as\nDINOv2-registers to bootstrap the learning signal and further improve upon\nthem. This dense post-pretraining leads to superior performance across various\nmodels and datasets, despite requiring only 19 hours on a single GPU. We\ndemonstrate that this method generates high-quality dense feature encoders and\nestablish several new state-of-the-art results: +5.5% and + 6% for\nnon-parametric in-context semantic segmentation on ADE20k and Pascal VOC, and\n+7.2% and +5.7% for linear segmentation evaluations on COCO-Things and -Stuff.",
        "updated": "2024-08-20 17:58:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11054v1"
    },
    {
        "title": "FLAME: Learning to Navigate with Multimodal LLM in Urban Environments",
        "authors": "Yunzhe XuYiyuan PanZhe LiuHesheng Wang",
        "links": "http://arxiv.org/abs/2408.11051v1",
        "entry_id": "http://arxiv.org/abs/2408.11051v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11051v1",
        "summary": "Large Language Models (LLMs) have demonstrated potential in\nVision-and-Language Navigation (VLN) tasks, yet current applications face\nchallenges. While LLMs excel in general conversation scenarios, they struggle\nwith specialized navigation tasks, yielding suboptimal performance compared to\nspecialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied\nAgent), a novel Multimodal LLM-based agent and architecture designed for urban\nVLN tasks that efficiently handles multiple observations. Our approach\nimplements a three-phase tuning technique for effective adaptation to\nnavigation tasks, including single perception tuning for street view\ndescription, multiple perception tuning for trajectory summarization, and\nend-to-end training on VLN datasets. The augmented datasets are synthesized\nautomatically. Experimental results demonstrate FLAME's superiority over\nexisting methods, surpassing state-of-the-art methods by a 7.3% increase in\ntask completion rate on Touchdown dataset. This work showcases the potential of\nMultimodal LLMs (MLLMs) in complex navigation tasks, representing an\nadvancement towards practical applications of MLLMs in embodied AI. Project\npage: https://flame-sjtu.github.io",
        "updated": "2024-08-20 17:57:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11051v1"
    },
    {
        "title": "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model",
        "authors": "Chunting ZhouLili YuArun BabuKushal TirumalaMichihiro YasunagaLeonid ShamisJacob KahnXuezhe MaLuke ZettlemoyerOmer Levy",
        "links": "http://arxiv.org/abs/2408.11039v1",
        "entry_id": "http://arxiv.org/abs/2408.11039v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11039v1",
        "summary": "We introduce Transfusion, a recipe for training a multi-modal model over\ndiscrete and continuous data. Transfusion combines the language modeling loss\nfunction (next token prediction) with diffusion to train a single transformer\nover mixed-modality sequences. We pretrain multiple Transfusion models up to 7B\nparameters from scratch on a mixture of text and image data, establishing\nscaling laws with respect to a variety of uni- and cross-modal benchmarks. Our\nexperiments show that Transfusion scales significantly better than quantizing\nimages and training a language model over discrete image tokens. By introducing\nmodality-specific encoding and decoding layers, we can further improve the\nperformance of Transfusion models, and even compress each image to just 16\npatches. We further demonstrate that scaling our Transfusion recipe to 7B\nparameters and 2T multi-modal tokens produces a model that can generate images\nand text on a par with similar scale diffusion models and language models,\nreaping the benefits of both worlds.",
        "updated": "2024-08-20 17:48:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11039v1"
    },
    {
        "title": "Atmospheric Transport Modeling of CO$_2$ with Neural Networks",
        "authors": "Vitus BensonAna BastosChristian ReimersAlexander J. WinklerFanny YangMarkus Reichstein",
        "links": "http://arxiv.org/abs/2408.11032v1",
        "entry_id": "http://arxiv.org/abs/2408.11032v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11032v1",
        "summary": "Accurately describing the distribution of CO$_2$ in the atmosphere with\natmospheric tracer transport models is essential for greenhouse gas monitoring\nand verification support systems to aid implementation of international climate\nagreements. Large deep neural networks are poised to revolutionize weather\nprediction, which requires 3D modeling of the atmosphere. While similar in this\nregard, atmospheric transport modeling is subject to new challenges. Both,\nstable predictions for longer time horizons and mass conservation throughout\nneed to be achieved, while IO plays a larger role compared to computational\ncosts. In this study we explore four different deep neural networks (UNet,\nGraphCast, Spherical Fourier Neural Operator and SwinTransformer) which have\nproven as state-of-the-art in weather prediction to assess their usefulness for\natmospheric tracer transport modeling. For this, we assemble the CarbonBench\ndataset, a systematic benchmark tailored for machine learning emulators of\nEulerian atmospheric transport. Through architectural adjustments, we decouple\nthe performance of our emulators from the distribution shift caused by a steady\nrise in atmospheric CO$_2$. More specifically, we center CO$_2$ input fields to\nzero mean and then use an explicit flux scheme and a mass fixer to assure mass\nbalance. This design enables stable and mass conserving transport for over 6\nmonths with all four neural network architectures. In our study, the\nSwinTransformer displays particularly strong emulation skill (90-day $R^2 >\n0.99$), with physically plausible emulation even for forward runs of multiple\nyears. This work paves the way forward towards high resolution forward and\ninverse modeling of inert trace gases with neural networks.",
        "updated": "2024-08-20 17:33:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11032v1"
    }
]