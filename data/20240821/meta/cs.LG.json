[
    {
        "title": "Accelerating Goal-Conditioned RL Algorithms and Research",
        "authors": "Michał BortkiewiczWładek PałuckiVivek MyersTadeusz DziarmagaTomasz ArczewskiŁukasz KucińskiBenjamin Eysenbach",
        "links": "http://arxiv.org/abs/2408.11052v1",
        "entry_id": "http://arxiv.org/abs/2408.11052v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11052v1",
        "summary": "Self-supervision has the potential to transform reinforcement learning (RL),\nparalleling the breakthroughs it has enabled in other areas of machine\nlearning. While self-supervised learning in other domains aims to find patterns\nin a fixed dataset, self-supervised goal-conditioned reinforcement learning\n(GCRL) agents discover new behaviors by learning from the goals achieved during\nunstructured interaction with the environment. However, these methods have\nfailed to see similar success, both due to a lack of data from slow\nenvironments as well as a lack of stable algorithms. We take a step toward\naddressing both of these issues by releasing a high-performance codebase and\nbenchmark JaxGCRL for self-supervised GCRL, enabling researchers to train\nagents for millions of environment steps in minutes on a single GPU. The key to\nthis performance is a combination of GPU-accelerated environments and a stable,\nbatched version of the contrastive reinforcement learning algorithm, based on\nan infoNCE objective, that effectively makes use of this increased data\nthroughput. With this approach, we provide a foundation for future research in\nself-supervised GCRL, enabling researchers to quickly iterate on new ideas and\nevaluate them in a diverse set of challenging environments. Website + Code:\nhttps://github.com/MichalBortkiewicz/JaxGCRL",
        "updated": "2024-08-20 17:58:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11052v1"
    },
    {
        "title": "RP1M: A Large-Scale Motion Dataset for Piano Playing with Bi-Manual Dexterous Robot Hands",
        "authors": "Yi ZhaoLe ChenJan SchneiderQuankai GaoJuho KannalaBernhard SchölkopfJoni PajarinenDieter Büchler",
        "links": "http://arxiv.org/abs/2408.11048v1",
        "entry_id": "http://arxiv.org/abs/2408.11048v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11048v1",
        "summary": "It has been a long-standing research goal to endow robot hands with\nhuman-level dexterity. Bi-manual robot piano playing constitutes a task that\ncombines challenges from dynamic tasks, such as generating fast while precise\nmotions, with slower but contact-rich manipulation problems. Although\nreinforcement learning based approaches have shown promising results in\nsingle-task performance, these methods struggle in a multi-song setting. Our\nwork aims to close this gap and, thereby, enable imitation learning approaches\nfor robot piano playing at scale. To this end, we introduce the Robot Piano 1\nMillion (RP1M) dataset, containing bi-manual robot piano playing motion data of\nmore than one million trajectories. We formulate finger placements as an\noptimal transport problem, thus, enabling automatic annotation of vast amounts\nof unlabeled songs. Benchmarking existing imitation learning approaches shows\nthat such approaches reach state-of-the-art robot piano playing performance by\nleveraging RP1M.",
        "updated": "2024-08-20 17:56:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11048v1"
    },
    {
        "title": "Atmospheric Transport Modeling of CO$_2$ with Neural Networks",
        "authors": "Vitus BensonAna BastosChristian ReimersAlexander J. WinklerFanny YangMarkus Reichstein",
        "links": "http://arxiv.org/abs/2408.11032v1",
        "entry_id": "http://arxiv.org/abs/2408.11032v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11032v1",
        "summary": "Accurately describing the distribution of CO$_2$ in the atmosphere with\natmospheric tracer transport models is essential for greenhouse gas monitoring\nand verification support systems to aid implementation of international climate\nagreements. Large deep neural networks are poised to revolutionize weather\nprediction, which requires 3D modeling of the atmosphere. While similar in this\nregard, atmospheric transport modeling is subject to new challenges. Both,\nstable predictions for longer time horizons and mass conservation throughout\nneed to be achieved, while IO plays a larger role compared to computational\ncosts. In this study we explore four different deep neural networks (UNet,\nGraphCast, Spherical Fourier Neural Operator and SwinTransformer) which have\nproven as state-of-the-art in weather prediction to assess their usefulness for\natmospheric tracer transport modeling. For this, we assemble the CarbonBench\ndataset, a systematic benchmark tailored for machine learning emulators of\nEulerian atmospheric transport. Through architectural adjustments, we decouple\nthe performance of our emulators from the distribution shift caused by a steady\nrise in atmospheric CO$_2$. More specifically, we center CO$_2$ input fields to\nzero mean and then use an explicit flux scheme and a mass fixer to assure mass\nbalance. This design enables stable and mass conserving transport for over 6\nmonths with all four neural network architectures. In our study, the\nSwinTransformer displays particularly strong emulation skill (90-day $R^2 >\n0.99$), with physically plausible emulation even for forward runs of multiple\nyears. This work paves the way forward towards high resolution forward and\ninverse modeling of inert trace gases with neural networks.",
        "updated": "2024-08-20 17:33:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11032v1"
    },
    {
        "title": "An Overlooked Role of Context-Sensitive Dendrites",
        "authors": "Mohsin RazaAhsan Adeel",
        "links": "http://arxiv.org/abs/2408.11019v1",
        "entry_id": "http://arxiv.org/abs/2408.11019v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11019v1",
        "summary": "To date, most dendritic studies have predominantly focused on the apical zone\nof pyramidal two-point neurons (TPNs) receiving only feedback (FB) connections\nfrom higher perceptual layers and using them for learning. Recent cellular\nneurophysiology and computational neuroscience studies suggests that the apical\ninput (context), coming from feedback and lateral connections, is multifaceted\nand far more diverse, with greater implications for ongoing learning and\nprocessing in the brain than previously realized. In addition to the FB, the\napical tuft receives signals from neighboring cells of the same network as\nproximal (P) context, other parts of the brain as distal (D) context, and\noverall coherent information across the network as universal (U) context. The\nintegrated context (C) amplifies and suppresses the transmission of coherent\nand conflicting feedforward (FF) signals, respectively. Specifically, we show\nthat complex context-sensitive (CS)-TPNs flexibly integrate C moment-by-moment\nwith the FF somatic current at the soma such that the somatic current is\namplified when both feedforward (FF) and C are coherent; otherwise, it is\nattenuated. This generates the event only when the FF and C currents are\ncoherent, which is then translated into a singlet or a burst based on the FB\ninformation. Spiking simulation results show that this flexible integration of\nsomatic and contextual currents enables the propagation of more coherent\nsignals (bursts), making learning faster with fewer neurons. Similar behavior\nis observed when this functioning is used in conventional artificial networks,\nwhere orders of magnitude fewer neurons are required to process vast amounts of\nheterogeneous real-world audio-visual (AV) data trained using backpropagation\n(BP). The computational findings presented here demonstrate the universality of\nCS-TPNs, suggesting a dendritic narrative that was previously overlooked.",
        "updated": "2024-08-20 17:18:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11019v1"
    },
    {
        "title": "Audio Match Cutting: Finding and Creating Matching Audio Transitions in Movies and Videos",
        "authors": "Dennis FedorishinLie LuSrirangaraj SetlurVenu Govindaraju",
        "links": "http://dx.doi.org/10.1109/ICASSP48485.2024.10447306",
        "entry_id": "http://arxiv.org/abs/2408.10998v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10998v1",
        "summary": "A \"match cut\" is a common video editing technique where a pair of shots that\nhave a similar composition transition fluidly from one to another. Although\nmatch cuts are often visual, certain match cuts involve the fluid transition of\naudio, where sounds from different sources merge into one indistinguishable\ntransition between two shots. In this paper, we explore the ability to\nautomatically find and create \"audio match cuts\" within videos and movies. We\ncreate a self-supervised audio representation for audio match cutting and\ndevelop a coarse-to-fine audio match pipeline that recommends matching shots\nand creates the blended audio. We further annotate a dataset for the proposed\naudio match cut task and compare the ability of multiple audio representations\nto find audio match cut candidates. Finally, we evaluate multiple methods to\nblend two matching audio candidates with the goal of creating a smooth\ntransition. Project page and examples are available at:\nhttps://denfed.github.io/audiomatchcut/",
        "updated": "2024-08-20 16:46:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10998v1"
    }
]