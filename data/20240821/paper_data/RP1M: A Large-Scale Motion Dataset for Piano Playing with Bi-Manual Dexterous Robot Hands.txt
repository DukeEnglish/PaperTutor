RP1M: A Large-Scale Motion Dataset for Piano
Playing with Bi-Manual Dexterous Robot Hands
YiZhao∗†1 LeChen∗2 JanSchneider2 QuankaiGao3
JuhoKannala1,4 BernhardScho¨lkopf2 JoniPajarinen1 DieterBu¨chler2
Abstract: Ithasbeenalong-standingresearchgoaltoendowrobothandswith
human-leveldexterity. Bi-manualrobotpianoplayingconstitutesataskthatcom-
bineschallengesfromdynamictasks,suchasgeneratingfastwhileprecisemotions,
withslowerbutcontact-richmanipulationproblems. Althoughreinforcementlearn-
ingbasedapproacheshaveshownpromisingresultsinsingle-taskperformance,
thesemethodsstruggleinamulti-songsetting. Ourworkaimstoclosethisgap
and,thereby,enableimitationlearningapproachesforrobotpianoplayingatscale.
Tothisend,weintroducetheRobotPiano1Million(RP1M)dataset,containing
bi-manualrobotpianoplayingmotiondataofmorethanonemilliontrajectories.
Weformulatefingerplacementsasanoptimaltransportproblem,thus,enabling
automaticannotationofvastamountsofunlabeledsongs. Benchmarkingexisting
imitationlearningapproachesshowsthatsuchapproachesreachstate-of-the-art
robotpianoplayingperformancebyleveragingRP1M⋄.
Keywords: Bi-manual dexterous robot hands, dataset for robot piano playing,
imitationlearning,robotlearningatscale
1 Introduction
Empoweringrobotswithhuman-leveldexterityisnotoriouslychallenging. Currentroboticsresearch
onhandandarmmotionsfocusesonmanipulationanddynamicathletictasks. Manipulation,such
as grasping or reorienting [1], requires continuous application of acceptable forces at moderate
speedstovariousobjectswithdistinctshapesandweightdistributions. Environmentalchanges,like
humidityortemperature,alterthealreadycomplexcontactdynamics,whichaddstothecomplexity
of manipulation tasks. Dynamic tasks, like juggling [2] and table tennis [3] involve making and
breakingcontact,demandinghighprecisionandtoleratinglessinaccuracyduetorarercontacts. High
speedsinthesetasksnecessitategreateraccelerationsandintroduceaprecision-speedtradeoff.
Robot piano playing combines various aspects of dynamic and manipulation tasks: the agent is
requiredtocoordinatemultiplefingerstopreciselypresskeysforarbitrarysongs,whichisahigh-
dimensionalandrichcontroltask. Atthesametime,thefingermotionshavetobehighlydynamic,
especiallyforsongswithfastrhythms. Well-practicedpianistscanplayarbitrarysongs,butthislevel
ofgeneralizationisextremelychallengingforrobots. Inthiswork,webuildthefoundationtodevelop
methodscapableofachievinghuman-levelbi-manualdexterityattheintersectionofmanipulation
anddynamictasks,whilereachingsuchgeneralizationcapabilitiesinmulti-taskenvironments.
Whilereinforcementlearning(RL)isapromisingdirection,traditionalRLapproachesoftenstruggle
toachieveexcellentperformanceinmulti-tasksettings[4]. Theadventofscalableimitationlearning
techniques [5] enables representing complex and multi-modal distributions. Such large models
are most effective when trained on massive datasets that combine the state evolution with the
∗Authorscontributedequallyandbothcanbeconsideredasfirstauthors.
†WorkdoneduringaninternshipatMaxPlanckInstituteforIntelligentSystems.
1 AaltoUniversity,Finland. 2 MaxPlanckInstituteforIntelligentSystems,Germany. 3 Universityof
SouthernCalifornia,USA.4UniversityofOulu,Finland.
⋄ProjectWebsite:https://rp1m.github.io/
4202
guA
02
]OR.sc[
1v84011.8042:viXraHands Piano
MIDI Fingering
Collect
Agent
Cost Matrix
TH FF MF RF LF
Action
Key 1 0.2 0.3 0.5 0.6 0.8
Key 2 0.4 0.4 0.5 0.7 0.8
RP1M Key 3 0.3 0.3 0.3 0.3 0.5
Figure1: OverviewofRP1M.(Left)RP1Misalarge-scalemotiondatasetforpianoplayingwith
bi-manualdexterousrobothands. Thedatasetincludes∼1Mexperttrajectoriescollectedby∼2kRL
specialistagents. (Right)Tocollectadiversemotiondatasetofplayingsheetmusicavailableonthe
Internet,welifttherequirementofhuman-annotatedfingeringbyformulatingthefingerplacementas
anoptimaltransportproblemsuchthattherobothandsplaypianoinanenergy-efficientway.
correspondingactioAnctiontrajectories. Sofar,creatinglargedatasetsforrobotpianoplayisproblematic
duetothetime-consumingfingeringannotations.Fingeringannotationsmapwhichfingerissupposed
to press a particular piano key at each time step. With fingering information, the reward is less
sparse,makingthetrainingsignificantlymoreeffective. Theselabelsusuallyrequireexperthuman
annotators[6],preventingtheagentfromleveragingthelargeamountsofunlabeledmusicpieceson
theinternet[7]. Besides,human-labeledfingeringmaybeinfeasibleforrobotswithmorphologies
differentfromhumanhands,suchasdifferentnumbersoffingersordistincthanddimensions.
Inthispaper,weproposetheRobotPiano1Milliondataset (RP1M).Thisdatasetcomprisesthe
motiondataofhigh-qualitybi-manualrobotpianoplay. Inparticular,wetrainRLagentsforeachof
the2ksongsandrollouteachpolicy500timeswithdifferentrandomseeds. Toenablethegeneration
ofRP1M,weintroduceamethodtolearnthefingeringautomaticallybyformulatingfingerplacement
asanoptimaltransport(OT)problem[8,9]. Intuitively,thefingersareplacedinawaysuchthat
thecorrectkeysarepressedwhiletheoverallmovingdistanceofthefingersisminimized. Agents
trainedusingourautomaticfingeringmatchtheperformanceofagentstrainedwithhuman-annotated
fingeringlabels. Besides,ourmethodiseasytoimplementwithalmostnoextratrainingtime. The
automaticfingeringalsoallowslearningpianoplayingwithdifferentembodiments,suchasrobots
withfourfingersonly. WithRP1M,itisnowpossibletotrainandtestimitationlearningapproaches
atscale. WebenchmarkvariousbehaviorcloningapproachesandfindthatbyusingRP1M,existing
methodsreachstate-of-the-artperformanceintermsofgeneralizationcapabilitiesinmulti-songpiano
play. Thisworkcontributesinvariousways:
• Tofacilitatetheresearchondexterousrobothands,wereleaseRP1M,adatasetofpianoplaying
motionsthatincludesmorethan2kmusicpieceswithexperttrajectoriesgeneratedbyouragents.
• Weformulatefingeringasanoptimaltransportproblem,enablingthegenerationofvastamounts
ofrobotpianodatawithRL,aswellasallowingvariationsintheembodiment.
• UsingRP1M,webenchmarkvariousapproachestorobotpianoplaying,wherebyexistingimitation
learningapproachesreachnewstate-of-the-artresultsinmotionsynthesisonnovelmusicpieces
duetoscalingwithRP1M.
2 RelatedWork
DexterousRobotHands Theresearchofdexterousrobothandsaimstoreplicatethedexterity
of human hands with robots. Many previous works [10, 11, 12, 13, 14, 15, 16, 17] use planning
tocomputeatrajectoryfollowedbyacontroller,thusrequireanaccuratemodeloftherobothand.
Closed-loopapproacheshavebeendevelopedbyincorporatingsensorfeedback[18]. Thesemethods
alsorequireanaccuratemodeloftherobothand,whichcanbedifficulttoobtaininpractice,especially
consideringthelargenumberofactivecontactsbetweenthehandandobjects.
Duetothedifficultyofactuallymodelingthedynamicsofthedexterousrobothand,recentmeth-
odsresorttolearning-basedapproaches,especiallyRL,whichhasachievedhugesuccessinboth
2Table1: Existingdatasetsondexterousorbi-manualroboticmanipulation.
Dexterous Dynamic Demonstra-
Dataset Task Bi-manual
hands tasks tions
DexGraspNet[48] grasping 1.3M
RealDex[50] grasping (cid:8) 2.6K
UniDexGrasp[29] grasping (cid:8) 1.1M
ALOHA[52] manipulation (cid:8) 825
Bi-DexHands[53] manipulation (cid:8) partially ∼20K
D4RL[51](Adroit) manipulation (cid:8) (cid:8) 30K
RP1M(ours) piano (cid:8) 1M
(cid:8) (cid:8) (cid:8)
robotics[19,20,1]andcomputergraphics[21]. Toeasethetrainingofdexterousrobothandswith
alargenumberofdegreesoffreedom(DoFs),demonstrationsarecommonlyused[22,23,24,25].
DuetotheadvanceofbothRLalgorithmsandsimulation, recentworkshowsimpressiveresults
on dexterous hand manipulation tasks without human demonstrations. Furthermore, the policy
trained in the simulator can further be deployed on real dexterous robot hands via sim-to-real
transfer[26,27,28,29,30,31].
PianoPlayingwithRobots Pianoplayingwithrobothandshasbeeninvestigatedfordecades. It
isachallengingtasksincebi-manualrobothandsshouldpreciselypresstherightkeysattheright
time,especiallyconsideringitshigh-dimensionalactionspace. Previousmethodsrequirespecific
robotdesigns[32,33,34,35,36,37]ortrajectorypre-programming[38,39]. Recentmethodsenable
pianoplayingwithdexteroushandsthroughplanning[40]orRL[41]butarelimitedtosimplemusic
pieces. RoboPianist[4]introducesabenchmarkforrobotpianoplayinganddemonstratesstrongRL
performance,butrequireshumanfingeringlabelsandperformsworseinmulti-tasklearning.
Human fingering informs the agent of the correspondence between fingers and pressed keys at
eachtimestep. Theselabelsrequireexpertannotatorsandare,therefore,expensivetoacquirein
practice. Several approaches learn fingering from human-annotated data with different machine
learningmethods[6,42,43]. Moryossefetal.[44]extractfingeringfromvideostoacquirefingering
labelscheaply. Ramonedaetal.[45]proposetotreatpianofingeringasasequentialdecision-making
problemanduseRLtocalculatefingeringbutwithoutconsideringthemodelofrobothands. Shi
etal.[46]automaticallyacquiresfingeringviadynamicprogramming,butthesolutionislimitedto
simpletasks. Inourpaper,wedonotintroduceaseparatefingeringmodel,instead,similartohuman
pianists,fingeringisdiscoveredautomaticallywhileplayingthepiano.
DatasetsforDexterousRobotHands Mostlarge-scaledatasetsofdexterousrobothandsfocuson
graspingvariousobjects. Togetsuitablegrasppositions,somemethodsutilizeplanners[47,48,49],
whileothersuselearnedgraspingpolicies[29],ortrackgraspingmotionsofhumansandimitate
thesemotionsonarobothand[50]. Comparedtotheabundanceofdatasetsforgrasping,thereexist
relativelyfewdatasetsforobjectmanipulationwithdexterousrobothands.TheD4RLbenchmark[51]
providessmallsetsofexperttrajectoriesforfoursuchtasks,consistingofhumandemonstrations
androlloutsoftrainedpolicies. Zhaoetal.[52]provideasmallobjectmanipulationdatasetthat
utilizesalow-costbi-manualplatformwithsimpleparallelgrippers. Chenetal.[53]collectoffline
datasetsfortwosimulatedbi-manualmanipulationtaskswithdexteroushands. Table1summarizes
thecharacteristicsoftheseexistingdatasets. Tothebestofourknowledge,ourRP1Mdatasetisthe
firstlarge-scaledatasetofdynamic,bi-manualmanipulationwithdexterousrobothands.
3 Background
TaskSetup Thesimulatedpiano-playingenvironmentisbuiltuponRoboPianist[4]. Itincludesa
robotpiano-playingsetup,anRL-basedagentforplayingpianowithsimulatedrobothands,anda
multi-tasklearner.Toavoidconfusion,werefertothesecomponentsasRoboPianist,RoboPianist-RL,
3andRoboPianist-MT,respectively. Thepianoplayingenvironmentfeaturesafull-sizekeyboardwith
88keysdrivenbylinearsprings,twoShadowrobothands[54],andapseudosustainpedal.
SheetmusicisrepresentedbyMusicalInstrumentDigitalInterface(MIDI)transcription. Eachtime
stepintheMIDIfilespecifieswhichpianokeystopress(activekeys). Thegoalofapiano-playing
agentistopressactivekeysandavoidinactivekeysunderspaceandtimeconstraints. Thisrequires
theagenttocoordinateitsfingersandplacethemproperlyinahighlydynamicscenariosuchthat
targetkeys,atnotonlythecurrenttimestepbutalsothefuturetimesteps,canbepressedaccurately
andtimely. TheoriginalRoboPianistusesMIDIfilesfromthePIGdataset[6]whichincludeshuman
fingeringinformationannotatedbyexperts. However,asmentionedearlier,thislimitstheagentto
onlyplayhuman-labeledmusicpieces,andthehumanannotationmaynotbesuitableforrobotsdue
tothedifferentmorphologies.
Theobservationincludesthestateofthetworobothands,fingertippositions,pianosustainstate,
pianokeystates,andagoalvector,resultinginan1144-dimensionalobservationspace. Thegoal
includes10-stepactivekeysand10-steptargetsustainstatesobtainedfromtheMIDIfile,represented
byabinaryvector. RoboPianstfurtherincludes10-stephuman-labeledfingeringintheobservation
spacebutweremovethisobservationinourmethodsincewedonotneedhuman-labeledfingering.
Fortheactionspace,weremovetheDoFsthatdonotexistinthehumanhandorareusedinmost
songs, resulting in a 39-dimensional action space, consisting of the joint positions of the robot
hands,thepositionsofforearms,andasustainpedal. Weevaluatetheperformanceofthetrained
agentwithanaverageF1scorecalculatedbyF = 2· precision·recall . Forpianoplaying,recalland
1 precision+recall
precisionmeasuretheagent’sperformanceonpressingtheactivekeysandavoidinginactivekeys
respectively[4].
PlayingPianowithRL WeuseRLtotrainspecialistagentspersongtocontrolthebi-manual
dexterous robot hands to play the piano. We frame the piano playing task as a finite Markov
Decision Process (MDP). At time step t, the agent π (a |s ), parameterized by θ, receives state
θ t t
s andtakesactiona tointeractwiththeenvironmentandreceivesnewstates andrewardr .
t t t+1 t
Thestateandactionspacesaredescribedaboveandtherewardr givesanimmediateevaluation
t
of the agent’s behavior. We will introduce reward terms used for training in Section 4.1. The
agent’sgoalistomaximizetheexpectedcumulativerewardsoveranepisodeoflengthH,definedas
(cid:104) (cid:105)
J =E (cid:80)H γtr (s ,a ) ,whereγ isadiscountfactorrangingfrom0to1.
πθ t=0 t t t
4 Large-ScaleMotionDatasetCollection
Inthissection,wedescribeourRP1Mdatasetindetail. Wefirstintroducehowtotrainaspecialist
piano-playingagentwithouthumanfingeringlabels. Removingtherequirementofhumanfingering
labelsallowstheagenttoplayanysheetmusicavailableontheInternet(undercopyrightlicense). We
thenanalyzetheperformanceofourspecialistRLagentaswellasthelearnedfingering. Lastly,we
introduceourcollectedlarge-scalemotiondataset,RP1M,whichincludes∼1Mexperttrajectories
forrobotpianoplaying,covering∼2kpiecesofmusic.
4.1 PianoPlayingwithoutHumanFingeringLabels
Tomitigatethehardexplorationproblemposedbythesparserewards,RoboPianist-RLaddsdense
rewardsignalsbyusinghumanfingeringlabels. Fingeringinformstheagentofthe“ground-truth”
fingertippositions,andtheagentminimizestheEuclideandistancebetweenthecurrentfingertipposi-
tionsandthe“ground-truth”positions. WenowdiscussourOT-basedmethodtolifttherequirement
ofhumanfingering.
Althoughfingeringishighlypersonalized,generallyspeaking,ithelpspianiststopresskeystimely
andefficiently. Motivatedbythis,apartfrommaximizingthekeypressingrewards,wealsoaimto
minimizethemovingdistancesoffingers. Specifically,attimestept,forthei-thkeykitopress,we
usethej-thfingerfj topressthiskeysuchthattheoverallmovingcostisminimized. Wedefinethe
4Piano sonata no.232 mov French suite no.5 sarabande Intermezzo in b minor op.117 no.2 Fleurs de varsovie op.57
1.00 1.00 0.8
0.75 OT (Ours) 0.75 0.75 0.6
0.50 RoboPianist­RL 0.50 0.50 0.4
No Fingering
0.25 0.25 0.25
0.2
0.00 0.00 0.00
0.0 2.5 5.0 7.5 0.0 2.5 5.0 7.5 0.0 2.5 5.0 7.5 0.0 2.5 5.0 7.5
Environment Steps (1e6) Environment Steps (1e6) Environment Steps (1e6) Environment Steps (1e6)
Figure2: ComparisonoftheRLperformancewithourOTfingering,human-annotatedfingering,and
nofingering. OurmethodmatchestheperformanceofRoboPianist-RL,whichistrainedwithhuman
fingering. Wealsooutperformsthebaselinewithoutanyfingeringinformationbyalargemargin. The
plotsshowthemeanover3randomseedsandtheshadedareasrepresentthe95%confidenceinterval.
minimizedcumulativemovingdistancebetweenfingersandtargetkeysasdOT ∈R+,givenby
t
(cid:88) (cid:88)
dOT =min w (ki,fj)·c (ki,fj), s.t., i) w (ki,fj)=1, for i∈K ,
t t t t t
wt
(i,j)∈Kt×F j∈F
(1)
(cid:88)
ii) w (ki,fj)≤1, for j ∈F, iii) w (ki,fj)∈{0,1}, for (i,j)∈K ×F.
t t t
i∈Kt
K representsthesetofkeystopressattimesteptandF representsthefingersoftherobothands.
t
c (ki,fj)representsthecostofmovingfingerfj topianokeykiattimesteptcalculatedbytheir
t
Euclidean distance. w (ki,fj) is a boolean weight. In our case, it enforces that each key in K
t t
willbepressedbyonlyonefingerinF,andeachfingerpressesatmostonekey. Theconstrained
optimization problem in Eq. (1) is an optimal transport problem. Intuitively, it tries to find the
best”transport”strategysuchthattheoverallcostofmoving(asubsetof)fingersF tokeysK is
t
minimized. WesolvethisoptimizationproblemwithamodifiedJonker-Volgenantalgorithm[55]
fromSciPy[56]andusetheoptimalcombinations(i∗,j∗)asthefingeringfortheagent.
WedefinearewardrOTbasedondOTtoencouragetheagenttomovethefingersclosetothekeysK ,
t t t
whichisdefinedas:
(cid:26) exp(c·(dOT−0.01)2) if dOT ≥0.01,
rOT = t t (2)
t 1.0 if dOT <0.01.
t
cisaconstantscalevalueandweusethesamevalueasTassaetal.[57]. Theoverallrewardfunction
isdefinedas:
r =rOT+rPress+rSustain+α ·rCollision+α ·rEnergy (3)
t t t t 1 t 2 t
rPress andrSustain representtherewardforcorrectlypressingthetargetkeysandthesustainpedal.
t
rCollisionencouragestheagenttoavoidcollisionbetweenforearmsandrEnergyprefersenergy-saving
t t
behaviors. α andα arecoefficientterms,andα =0.5andα =5·10−3areadopted. Ourmethod
1 2 1 2
iscompatiblewithanyRLmethods,andweuseDroQ[58]inourpaper. DuringRLtraining,the
generatedfingeringwillkeepupdatingaccordingtothestateofhands. Aftergivinganinitialguessat
thebeginning,theOT-basedfingeringwillberevisediftheagentdiscoversabetterfingeringsolution.
4.2 AnalysisofSpecialistRLAgents
TheperformanceofthespecialistRLagentsdecidesthequalityofourdataset. Inthissection,we
investigatetheperformanceofourspecialistRLagents. Weareinterestedini)howtheproposedOT-
basedfingerplacementhelpslearning,ii)howthefingeringdiscoveredbytheagentitselfcompares
tohumanfingeringlabels,andiii)howourmethodtransferstootherembodiments.
Results InFig.2, wecompareourmethodwithRoboPianist-RLbothwithandwithouthuman
fingering. We use the same DroQ algorithm with the same hyperparameters for all experiments.
RoboPianist-RLincludeshumanfingeringinitsinputs,andthefingeringinformationisalsousedin
therewardfunctiontoforcetheagenttofollowthisfingering. Ourmethod,markedasOT,removes
the fingering from the observation space and uses OT-based finger placement to guide the agent
5
erocS
1F
erocS
1F
erocS
1F
erocS
1FFive-finger
Hands
OT
Human
Four-finger
Hands
OT
Human
OT
HumTahnumb First Finger Middle Finger Ring Finger Little Finger
Thumb First Finger Middle Finger Ring Finger Little Finger
Figure3: Comparisonoffingeringdiscoveredbytheagentitselfandhumanannotations.
to discoveOrTits own fingering. We also include a baseline, called No Fingering, that removes the
fingeringHeunmtiarenly. ThefirsttwocolumnsofFig.2showthatourmethodwithouthuman-annotated
fingeringmatchesRoboPianst-RL’sperformanceontwodifferentsongs. Ourmethodoutperforms
thebaselinewithouthumanfingeringbyalargemargin,showingthattheproposedOT-basedfinger
placementbooststheagentlearning. Theproposedmethodworkswellevenonchallengingsongs.
WetestourmethodonFlightoftheBumblebeeandachieve0.79F1scoreafter3Mtrainingsteps. To
thebestofourknowledge,wearethefirsttoplaythechallengingsongFlightoftheBumblebeewith
general-purposebi-manualdexterousrobothands.
AnalysisoftheLearnedFingering Wenowcomparethefingeringdiscoveredbytheagentitself
andthehumanannotations. InFig.3,wevisualizethesampletrajectoryofplayingFrenchSuite
No.5Sarabandeandthecorrespondingfingering. Wefoundthatalthoughtheagentachievesstrong
performanceforthissong(thesecondplotinFig.2),ouragentdiscoversdifferentfingeringcompared
tohumans. Forexample,fortherighthand,humansmainlyusethemiddleandringfingers,whileour
agentusesthethumbandfirstfinger. Furthermore,insomecases,humanannotationsarenotsuitable
fortherobothandduetodifferentmorphologies. Forexample,inthesecondtimestepofFig.3,the
humanusesthefirstfingerandringfinger. However,duetothemechanicallimitationoftherobot
hand,itcannotpresskeysthatfarapartwiththesetwofingers,thusmimickinghumanfingeringwill
missonekey. Instead,ouragentdiscoveredtousethethumbandlittlefinger,whichsatisfiesthe
hardwarelimitationandaccuratelypressesthetargetkeys.
CrossEmboidments Labsusuallyhavedifferentrobotplatforms,thushavingamethodthatworks
fordifferentembodimentsishighlydesirable. Wetestourmethodonadifferentembodiment. To
simplifytheexperiment,wedisablethelittlefingeroftheShadowrobothandandobtainafour-finger
robot hand, which has a similar morphology to Allegro [59] and LEAP Hand [60]. We evaluate
themodifiedrobothandonthesongFrenchSuiteNo.5Sarabande(first550timesteps),whereour
methodachievesa0.95F1score,similartothe0.96achievedwiththeoriginalrobothands. Inthe
bottomrowof Fig.3,wevisualizethelearnedfingeringwithfour-fingerhands. Theagentdiscovers
differentfingeringcomparedtohumansandtheoriginalhandsbutstillaccuratelypressesactivekeys,
meaningourmethodiscompatiblewithdifferentembodiments.
4.3 RP1MDataset
Tofacilitatetheresearchondexterousrobothands,wecollectandreleasealarge-scalemotiondataset
forpianoplaying. Ourdatasetincludes∼1Mexperttrajectoriescovering∼2kmusicalpieces. For
eachmusicalpiece,wetrainanindividualDroQagentwiththemethodintroducedinSection4.1
for8millionenvironmentstepsandcollect500experttrajectorieswiththetrainedagent. Wechunk
eachsheetmusicevery550timesteps,correspondingto27.5seconds,sothateachrunhasthesame
6Histogram of Pressed Keys
75000 Key Color
white
50000 black
25000
0
A0A#0B0C1C#1D1D#1E1F1F#1G1G#1A1A#1B1C2C#2D2D#2E2F2F#2G2G#2A2A#2B2C3C#3D3D#3E3F3F#3G3G#3A3A#3B3C4C#4D4D#4E4F4F#4G4G#4A4A#4B4C5C#5D5D#5E5F5F#5G5G#5A5A#5B5C6C#6D6D#6E6F6F#6G6G#6A6A#6B6C7C#7D7D#7E7F7F#7G7G#7A7A#7B7C8
Key
Num of Active Keys over All Time Steps Histogram of F1 Scores
1.00
60
75 0.75
40
50 0.50
20 25 0.25
0 0 0.00
1000 2000 3000 4000 5000 6000 7000 0.5 0.6 0.7 0.8 0.9 1.0
Num of Keys F1 Score
Figure4: StatisticsofourRP1Mdataset. (Top)HistogramofpressedkeysinourRP1Mdataset.
(Bottom Left) Distribution of the number of active keys over all time steps. (Bottom Right)
DistributionofF1scoresinourdataset.
episodelength. ThesheetmusicusedfortrainingisfromthePIGdataset[6]andasubset(1788
pieces)oftheGiantMIDI-Pianodataset[7]. ThestoredepisodesareconvertedtoRLDSformat[61].
InFig.4,weshowthestatisticsofourcollectedmotiondataset. Thetopplotshowsthehistogramof
thepressedkeys. Wefoundthatkeysclosetothecenteraremorefrequentlypressedthankeysatthe
corner. Also,whitekeys,taking65.7%,aremorelikelytobepressedthanblackkeys. Inthebottom
leftplot,weshowthedistributionofthenumberofactivekeysoveralltimesteps. Itroughlyfollows
aGaussiandistribution,and90.70%musicalpiecesinourdatasetinclude1000-4000activekeys. We
alsoincludethedistributionofF1scoresoftrainedagentsusedforcollectingdata. Wefoundmost
agents(79.00%)achieveF1scoreslargerthan0.75,and99.89%oftheagents’F1scoresarelarger
than0.5. ThedistributionofF1scoresreflectsthequalityofthecollecteddataset. Weempirically
foundagentswithF1score≥0.75arecapableofplayingsheetmusicreasonablywellwithonly
minorerrors. Agentswith≤0.5F1scoresusuallyhavenotableerrorsduetothedifficultyofsongs
orthemechanicallimitationsoftheShadowrobothand. WealsoincludetheF1scoresforeachpiece
inourdatasetsouserscanfilterthedatasetaccordingtotheirneeds.
5 BenchmarkingResults
Theanalysisintheprevioussectionhighlightedthediversityofhighlydynamicpiano-playingmotions
inthe RP1Mdataset. Inthissection, weassess themulti-taskimitation learningperformanceof
several widely used methods on our benchmark. To be specific, the objective is to train a single
multi-taskpolicycapableofplayingvariousmusicpiecesonthepiano. Wetrainthepolicyona
portionoftheRP1Mdatasetandevaluateitsin-distributionperformance(F1scoresonsongsincluded
inthetrainingdata)anditsgeneralizationability(F1scoresonsongsnotpresentinthetrainingdata).
Baselines WeevaluatedBehaviorCloning(BC)[62],ImplicitBehavioralCloning(IBC)[63],BC
withaRecurrentNeuralNetworkpolicy(BC-RNN)[64],andDiffusionPolicy[5]. BCdirectlylearns
apolicybyusingsupervisedlearningonobservation-actionpairsfromexpertdemonstrations. IBC
learnsanimplicitpolicyasanenergy-basedmodelconditionedonobservationandaction. BC-RNN
usesanRNNasthepolicynetworktoencodeahistoryofobservations. DiffusionPolicylearnsto
modeltheactiondistributionbyinvertingaprocessthatgraduallyaddsnoisetoasampledaction
sequence. WeusedaCNN-basedDiffusionPolicywithDDIM[65]asthesampler. Weusethesame
codeandhyperparametersasChietal.[5].
ExperimentSetup Wefirsttrainthepolicieswith3differentsizesofexpertdata: 50,150,and300
songs. Wethenevaluatethetrainedpolicieson3differentgroupsofmusicpieces. (1)In-distribution
songs: musicpiecesthatoverlapwiththetrainingsets. Itshowsthemultitaskingperformanceofthe
trainedpolicies. (2)Easyout-of-distribution(OOD)songs: simplemusicpiecesthatdonotoverlap
withthetrainingsongs. Thosepiecesareeasytoplay,withonlyslowmotionsandshorthorizons.
7
tnuoC
sserP
yeK
seceiP
lacisuM
fo
muN
seceiP
lacisuM
fo
muN
FDCTable2: Comparisonresultsofmulti-taskimitationlearning.
BC
Task #music IBC BC-RNN DiffusionPolicy
256 1024 4096
50 0.086 0.621 0.200 0.120 0.174 0.706
In-Dist. 150 0.124 0.245 0.176 0.088 0.121 0.578
300 0.084 0.249 0.102 0.083 0.025 0.596
50 0.06 0.278 0.109 0.128 0.204 0.446
EasyOOD 150 0.085 0.257 0.172 0.058 0.050 0.465
300 0.056 0.236 0.112 0.056 0.039 0.486
50 0.141 0.244 0.217 0.183 0.221 0.303
HardOOD 150 0.155 0.275 0.229 0.148 0.105 0.432
300 0.145 0.253 0.181 0.144 0.081 0.440
(3)Hardout-of-distributionsongs: difficultmusicpiecesthatdonotoverlapwiththetrainingsongs.
Theycontainmorediversemotionsandlongerhorizons. Theout-of-distributionevaluationassesses
thezero-shotgeneralizationabilityofthetrainedpolicies. WereporttheaverageF1scoresofeach
groupofmusicpiecesforpoliciestrainedwitheachbaselinemethod.
DiscussionAsshowninTable2,mostbaselinesincludingBC(256),IBC,andBC-RNNhaveworse
performance. This is because of the limited model capacity. We increase the model capacity of
theBCbaselinebyincreasingthehiddendimensionofa3-layerMLPfrom256to1024andwe
observeclearimprovementinbothin-distributionevaluationandOODevaluation. However,when
furtherincreasingthemodelsizetoa6-layerMLPwith4096hiddendimensions,aperformance
dropisobserved,meaningdirectlyincreasingthemodelcapacityofMLPcausesissuesformodel
training. Amongthebaselineswehaveevaluated,DiffusionPolicyperformsthebestinallcases,
demonstratingthatitisastrongbaselineforthepiano-playingtask. Furthermore,whenincreasing
thesizeofthetrainingset,theOODperformancegraduallyincreases. However,forin-distribution
evaluation, Diffusion Policy still has a lower F1 score than our RL specialist and we observed a
performancedropwhenincreasingthedatasetsize,whichindicatestheDiffusionPolicyhasissues
fittingourdatasetwell. Thiscanbecausedbymultiplereasons,e.g.,thelimitedmodelcapacityor
improperhyperparametersbutweleaveitasfuturework.
6 Limitations&Conclusion
Limitations Ourpaperhaslimitationsinseveralaspects. Firstly, althoughourmethodliftsthe
requirementofhuman-annotatedfingering,enablingRLtrainingondiversesongs,ourmethodstill
failstoachievestrongperformanceonchallengingsongsduetofastrhythmsandmechanicallimi-
tationsoftherobothands. ThiscouldbesolvedbyproposingabetterRLmethodandimproving
thehardwaredesignoftherobothands. Secondly,ourdatasetonlyincludesproprioceptiveobser-
vations. However, humans play piano with multi-modal inputs, including vision, tactile sensing,
andauditoryinformation. Enablingtheagenttoplaythepianofromsuchrichinputsourcesisan
intriguingdirection. Lastly,althoughwedemonstratebetterzero-shotgeneralizationperformance
thanRoboPianist-MT[4],thereisstillagapbetweenourbestmulti-taskagentandRLspecialists,
whichrequiresfutureinvestigation.
Conclusion Inthispaper,weproposealarge-scalemotiondatasetnamedRP1Mforpianoplaying
withbi-manualdexterousrobothands. RP1Mincludes1millionexperttrajectoriesforplaying2k
musical pieces. To collect such a diverse dataset for piano playing, we lift the need for human-
annotatedfingeringinthepreviousmethodbyintroducinganovelautomaticfingeringannotation
approach based on optimal transport. On single songs, our method matches the baselines with
human-annotated fingering and can be adopted across different embodiments. Furthermore, we
benchmarkvariousimitationlearningapproachesformulti-songplaying. Wereportnewstate-of-the-
artresultsinmotionsynthesisfornovelmusicpiecesandidentifythegaptoachievinghuman-level
piano-playingability. WebelievetheRP1Mdataset,withitsscaleandquality,formsasolidstep
towardsempoweringrobotswithhuman-leveldexterity.
8Acknowledgments
WethankthesupportoftheMaxPlanckInstituteforIntelligentSystems,Tu¨bingen(Germany). We
acknowledgeCSC–ITCenterforScience,Finland,forawardingthisprojectaccesstotheLUMI
supercomputer,ownedbytheEuroHPCJointUndertaking,hostedbyCSC(Finland)andtheLUMI
consortiumthroughCSC.YiZhao,JuhoKannala,andJoniPajarinenacknowledgefundingbythe
ResearchCouncilofFinland(345521353138,327911). WethankYuxinHouandWenyanYangfor
theinsightfuldiscussion.
References
[1] OpenAI,M.Andrychowicz,B.Baker,M.Chociej,R.Jo´zefowicz,B.McGrew,J.Pachocki,
A.Petron,M.Plappert,G.Powell,A.Ray,J.Schneider,S.Sidor,J.Tobin,P.Welinder,L.Weng,
and W. Zaremba. Learning dexterous in-hand manipulation. The International Journal of
RoboticsResearch,39(1):3–20,2020.
[2] K.Ploeger,M.Lutter,andJ.Peters. Highaccelerationreinforcementlearningforreal-world
jugglingwithbinaryrewards. InConferenceonRobotLearning,pages642–653.PMLR,2021.
[3] D.Bu¨chler,S.Guist,R.Calandra,V.Berenz,B.Scho¨lkopf,andJ.Peters. Learningtoplaytable
tennisfromscratchusingmuscularrobots. IEEETransactionsonRobotics,38(6):3850–3860,
2022.
[4] K.Zakka,P.Wu,L.Smith,N.Gileadi,T.Howell,X.B.Peng,S.Singh,Y.Tassa,P.Florence,
A.Zeng,etal. RoboPianist: Dexterouspianoplayingwithdeepreinforcementlearning. In7th
AnnualConferenceonRobotLearning,2023.
[5] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song. Diffusion policy:
Visuomotorpolicylearningviaactiondiffusion. arXivpreprintarXiv:2303.04137,2023.
[6] E.Nakamura,Y.Saito,andK.Yoshii. Statisticallearningandestimationofpianofingering.
InformationSciences,517:68–85,2020.
[7] Q.Kong,B.Li,J.Chen,andY.Wang.GiantMIDI-Piano:Alarge-scalemididatasetforclassical
pianomusic. arXivpreprintarXiv:2010.07061,2020.
[8] C.Villanietal. Optimaltransport: Oldandnew,volume338. Springer,2009.
[9] G.Peyre´,M.Cuturi,etal. Computationaloptimaltransport: Withapplicationstodatascience.
FoundationsandTrends®inMachineLearning,11(5-6):355–607,2019.
[10] D.Rus. In-handdexterousmanipulationofpiecewise-smooth3-dobjects. TheInternational
JournalofRoboticsResearch,18(4):355–381,1999.
[11] A.BicchiandR.Sorrentino. Dexterousmanipulationthroughrolling. InProceedingsof1995
IEEEInternationalConferenceonRoboticsandAutomation,volume1,pages452–457.IEEE,
1995.
[12] L.HanandJ.C.Trinkle. Dextrousmanipulationbyrollingandfingergaiting. InProceedings.
1998 IEEE International Conference on Robotics and Automation (Cat. No. 98CH36146),
volume1,pages730–735.IEEE,1998.
[13] Y. Bai and C. K. Liu. Dexterous manipulation using both palm and fingers. In 2014 IEEE
InternationalConferenceonRoboticsandAutomation(ICRA),pages1560–1565.IEEE,2014.
[14] I.Mordatch,Z.Popovic´,andE.Todorov. Contact-invariantoptimizationforhandmanipulation.
In Proceedings of the ACM SIGGRAPH/Eurographics symposium on computer animation,
pages137–144,2012.
9[15] N.C.Dafle, A.Rodriguez, R.Paolini, B.Tang, S.S.Srinivasa, M.Erdmann, M.T.Mason,
I. Lundberg, H. Staab, and T. Fuhlbrigge. Extrinsic dexterity: In-hand manipulation with
externalforces. In2014IEEEInternationalConferenceonRoboticsandAutomation(ICRA),
pages1578–1585.IEEE,2014.
[16] N.Chavan-DafleandA.Rodriguez. Sampling-basedplanningofin-handmanipulationwith
externalpushes.InRoboticsResearch:The18thInternationalSymposiumISRR,pages523–539.
Springer,2020.
[17] V.Kumar,Y.Tassa,T.Erez,andE.Todorov. Real-timebehavioursynthesisfordynamichand-
manipulation. In2014IEEEInternationalConferenceonRoboticsandAutomation(ICRA),
pages6808–6815.IEEE,2014.
[18] M.Li,Y.Bekiroglu,D.Kragic,andA.Billard. Learningofgraspadaptationthroughexperience
and tactile sensing. In 2014 IEEE/RSJ International Conference on Intelligent Robots and
Systems,pages3339–3346.Ieee,2014.
[19] J.Lee,J.Hwangbo,L.Wellhausen,V.Koltun,andM.Hutter. Learningquadrupedallocomotion
overchallengingterrain. Sciencerobotics,5(47):eabc5986,2020.
[20] I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino,
M.Plappert,G.Powell,R.Ribas,etal. SolvingRubik’scubewitharobothand. arXivpreprint
arXiv:1910.07113,2019.
[21] X.B.Peng,P.Abbeel,S.Levine,andM.VandePanne. Deepmimic: Example-guideddeep
reinforcement learning of physics-based character skills. ACM Transactions On Graphics
(TOG),37(4):1–14,2018.
[22] V.Kumar,A.Gupta,E.Todorov,andS.Levine. Learningdexterousmanipulationpoliciesfrom
experienceandimitation. arXivpreprintarXiv:1611.05095,2016.
[23] A. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani, J. Schulman, E. Todorov, and S. Levine.
Learningcomplexdexterousmanipulationwithdeepreinforcementlearninganddemonstrations.
arXivpreprintarXiv:1709.10087,2017.
[24] I.Radosavovic,X.Wang,L.Pinto,andJ.Malik. State-onlyimitationlearningfordexterous
manipulation.in2021ieee. InRSJInternationalConferenceonIntelligentRobotsandSystems
(IROS),pages7865–7871.
[25] R.Jeong,J.T.Springenberg,J.Kay,D.Zheng,Y.Zhou,A.Galashov,N.Heess,andF.Nori.
Learningdexterousmanipulationfromsuboptimalexperts. arXivpreprintarXiv:2010.08587,
2020.
[26] T. Chen, J. Xu, and P. Agrawal. A system for general in-hand object re-orientation. In
ConferenceonRobotLearning,pages297–307.PMLR,2022.
[27] Z.Yang,K.Yin,andL.Liu. Learningtousechopsticksindiversegrippingstyles. 2022.
[28] Y.Chen,T.Wu,S.Wang,X.Feng,J.Jiang,Z.Lu,S.McAleer,H.Dong,S.-C.Zhu,andY.Yang.
Towardshuman-levelbimanualdexterousmanipulationwithreinforcementlearning. Advances
inNeuralInformationProcessingSystems,35:5150–5163,2022.
[29] Y.Xu,W.Wan,J.Zhang,H.Liu,Z.Shan,H.Shen,R.Wang,H.Geng,Y.Weng,J.Chen,etal.
UniDexGrasp: Universalroboticdexterousgraspingvialearningdiverseproposalgeneration
andgoal-conditionedpolicy. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pages4737–4746,2023.
[30] A.Allshire,M.MittaI,V.Lodaya,V.Makoviychuk,D.Makoviichuk,F.Widmaier,M.Wu¨thrich,
S.Bauer,A.Handa,andA.Garg. Transferringdexterousmanipulationfromgpusimulationto
aremotereal-worldtrifinger. In2022IEEE/RSJInternationalConferenceonIntelligentRobots
andSystems(IROS),pages11802–11809.IEEE,2022.
10[31] Y.Qin,B.Huang,Z.-H.Yin,H.Su,andX.Wang. DexPoint: Generalizablepointcloudrein-
forcementlearningforsim-to-realdexterousmanipulation. InConferenceonRobotLearning,
pages594–605.PMLR,2023.
[32] I.Kato,S.Ohteru,K.Shirai,T.Matsushima,S.Narita,S.Sugano,T.Kobayashi,andE.Fujisawa.
Therobotmusician‘wabot-2’(wasedarobot-2). Robotics,3(2):143–155,1987.
[33] J.-C.Lin,H.-H.Huang,Y.-F.Li,J.-C.Tai,andL.-W.Liu. Electronicpianoplayingrobot. In
2010InternationalSymposiumonComputer,Communication,ControlandAutomation(3CA),
volume2,pages353–356.IEEE,2010.
[34] A.Topper,T.Maloney,S.Barton,andX.Kong. Piano-playingroboticarm. WorcesterMA,
pages01609–2280,2019.
[35] J.Hughes,P.Maiolino,andF.Iida. Ananthropomorphicsoftskeletonhandexploitingcondi-
tionalmodelsforpianoplaying. ScienceRobotics,3(25):eaau3098,2018.
[36] R.CastroOrnelas. Roboticfingerhardwareandcontrolsdesignfordynamicpianoplaying,
2022.
[37] D.Zhang,J.Lei,B.Li,D.Lau,andC.Cameron. Designandanalysisofapianoplayingrobot.
In2009InternationalConferenceonInformationandAutomation,pages757–761.IEEE,2009.
[38] Y.-F.LiandL.-L.Chuang. Controllerdesignformusicplayingrobot—appliedtotheanthropo-
morphicpianorobot. In2013IEEE10thInternationalConferenceonPowerElectronicsand
DriveSystems(PEDS),pages968–973.IEEE,2013.
[39] A.Zhang,M.Malhotra,andY.Matsuoka. MusicalpianoperformancebytheACThand. In
2011 IEEE international conference on robotics and automation, pages 3536–3541. IEEE,
2011.
[40] B.Scholz. Playingpianowithashadowdexteroushand,2019.
[41] H. Xu, Y. Luo, S. Wang, T. Darrell, and R. Calandra. Towards learning to play piano with
dexteroushandsandtouch. In2022IEEE/RSJInternationalConferenceonIntelligentRobots
andSystems(IROS),pages10410–10416.IEEE,2022.
[42] P.Ramoneda,D.Jeong,E.Nakamura,X.Serra,andM.Miron. Automaticpianofingeringfrom
partiallyannotatedscoresusingautoregressiveneuralnetworks. InProceedingsofthe30th
ACMInternationalConferenceonMultimedia,pages6502–6510,2022.
[43] D.A.Randolph,B.DiEugenio,andJ.Badgerow. Modelingpianofingeringdecisionswith
conditionalrandomfields. 2023.
[44] A. Moryossef, Y. Elazar, and Y. Goldberg. At your fingertips: Extracting piano fingering
instructionsfromvideos. arXivpreprintarXiv:2303.03745,2023.
[45] P.Ramoneda, M.Miron, andX.Serra. Pianofingeringwithreinforcementlearning. arXiv
preprintarXiv:2111.08009,2021.
[46] W. Shi, Y. Li, Y. Guan, X. Chen, S. Yang, and S. Mo. Optimized fingering planning for
automaticpianoplayingusingdual-armrobotsystem. In2022IEEEInternationalConference
onRoboticsandBiomimetics(ROBIO),pages933–938.IEEE,2022.
[47] M.Liu,Z.Pan,K.Xu,K.Ganguly,andD.Manocha. Deepdifferentiablegraspplannerfor
high-dofgrippers. arXivpreprintarXiv:2002.01530,2020.
[48] R. Wang, J. Zhang, J. Chen, Y. Xu, P. Li, T. Liu, and H. Wang. DexGraspNet: A large-
scaleroboticdexterousgraspdatasetforgeneralobjectsbasedonsimulation. In2023IEEE
International Conference on Robotics and Automation (ICRA), pages 11359–11366. IEEE,
2023.
11[49] L. F. C. Murrilo, N. Khargonkar, B. Prabhakaran, and Y. Xiang. MultiGripperGrasp: A
dataset for robotic grasping from parallel jaw grippers to dexterous hands. arXiv preprint
arXiv:2403.09841,2024.
[50] Y.Liu,Y.Yang,Y.Wang,X.Wu,J.Wang,Y.Yao,S.Schwertfeger,S.Yang,W.Wang,J.Yu,
et al. RealDex: Towards human-like grasping for robotic dexterous hand. arXiv preprint
arXiv:2402.13853,2024.
[51] J.Fu,A.Kumar,O.Nachum,G.Tucker,andS.Levine. D4RL:Datasetsfordeepdata-driven
reinforcementlearning. arXivpreprintarXiv:2004.07219,2020.
[52] T.Z.Zhao,V.Kumar,S.Levine,andC.Finn. Learningfine-grainedbimanualmanipulation
withlow-costhardware. arXivpreprintarXiv:2304.13705,2023.
[53] Y.Chen,Y.Geng,F.Zhong,J.Ji,J.Jiang,Z.Lu,H.Dong,andY.Yang. Bi-DexHands:Towards
human-levelbimanualdexterousmanipulation. IEEETransactionsonPatternAnalysisand
MachineIntelligence,2023.
[54] ShadowRobot. ShadowRobot Dexterous Hand. https://www.shadowrobot.com/
products/dexterous-hand/,2005.
[55] D.F.Crouse. Onimplementing2drectangularassignmentalgorithms. IEEETransactionson
AerospaceandElectronicSystems,52(4):1679–1696,2016.
[56] P.Virtanen,R.Gommers,T.E.Oliphant,M.Haberland,T.Reddy,D.Cournapeau,E.Burovski,
P.Peterson,W.Weckesser,J.Bright,etal. Scipy1.0: fundamentalalgorithmsforscientific
computinginpython. Naturemethods,17(3):261–272,2020.
[57] Y.Tassa, Y.Doron, A.Muldal, T. Erez, Y. Li, D. d. L.Casas, D.Budden, A.Abdolmaleki,
J.Merel,A.Lefrancq,etal. Deepmindcontrolsuite. arXivpreprintarXiv:1801.00690,2018.
[58] T.Hiraoka,T.Imagawa,T.Hashimoto,T.Onishi,andY.Tsuruoka. DropoutQ-functionsfor
doublyefficientreinforcementlearning. arXivpreprintarXiv:2110.02034,2021.
[59] Allegro. https://www.wonikrobotics.com/research-robot-hand.
[60] K.Shaw,A.Agarwal,andD.Pathak. Leaphand: Low-cost,efficient,andanthropomorphic
handforrobotlearning. arXivpreprintarXiv:2309.06440,2023.
[61] S. Ramos, S. Girgin, L. Hussenot, D. Vincent, H. Yakubovich, D. Toyama, A. Gergely,
P. Stanczyk, R. Marinier, J. Harmsen, et al. RLDS: an ecosystem to generate, share and
usedatasetsinreinforcementlearning. arXivpreprintarXiv:2111.02767,2021.
[62] D.A.Pomerleau. Alvinn: Anautonomouslandvehicleinaneuralnetwork. Advancesinneural
informationprocessingsystems,1,1988.
[63] P. Florence, C. Lynch, A. Zeng, O. A. Ramirez, A. Wahid, L. Downs, A. Wong, J. Lee,
I.Mordatch,andJ.Tompson. Implicitbehavioralcloning. InConferenceonRobotLearning,
pages158–168.PMLR,2022.
[64] A.Mandlekar,D.Xu,J.Wong,S.Nasiriany,C.Wang,R.Kulkarni,L.Fei-Fei,S.Savarese,
Y.Zhu,andR.Mart´ın-Mart´ın. Whatmattersinlearningfromofflinehumandemonstrations
forrobotmanipulation. InA.Faust,D.Hsu,andG.Neumann,editors,Proceedingsofthe5th
ConferenceonRobotLearning,volume164ofProceedingsofMachineLearningResearch,
pages 1678–1690. PMLR, 08–11 Nov 2022. URL https://proceedings.mlr.press/
v164/mandlekar22a.html.
[65] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502,2020.
12[66] A.v.d.Oord,Y.Li,andO.Vinyals. Representationlearningwithcontrastivepredictivecoding.
arXivpreprintarXiv:1807.03748,2018.
[67] R.Rombach,A.Blattmann,D.Lorenz,P.Esser,andB.Ommer.High-resolutionimagesynthesis
withlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pages10684–10695,2022.
[68] J.Ho,T.Salimans,A.Gritsenko,W.Chan,M.Norouzi,andD.J.Fleet. Videodiffusionmodels.
AdvancesinNeuralInformationProcessingSystems,35:8633–8646,2022.
[69] J.Ho,W.Chan,C.Saharia,J.Whang,R.Gao,A.Gritsenko,D.P.Kingma,B.Poole,M.Norouzi,
D.J.Fleet,etal. Imagenvideo: Highdefinitionvideogenerationwithdiffusionmodels. arXiv
preprintarXiv:2210.02303,2022.
[70] B.Poole,A.Jain,J.T.Barron,andB.Mildenhall. DreamFusion: Text-to-3dusing2ddiffusion.
arXivpreprintarXiv:2209.14988,2022.
[71] Z. Liu, Y. Feng, M. J. Black, D. Nowrouzezahrai, L. Paull, and W. Liu. MeshDiffusion:
Score-basedgenerative3dmeshmodeling. arXivpreprintarXiv:2303.08133,2023.
[72] H.Ha,P.Florence,andS.Song. Scalingupanddistillingdown: Language-guidedrobotskill
acquisition. InConferenceonRobotLearning,pages3766–3777.PMLR,2023.
[73] M.Reuss,M.Li,X.Jia,andR.Lioutikov.Goal-conditionedimitationlearningusingscore-based
diffusionpolicies. arXivpreprintarXiv:2304.02532,2023.
[74] O.M.Team,D.Ghosh,H.Walke,K.Pertsch,K.Black,O.Mees,S.Dasari,J.Hejna,T.Kreiman,
C.Xu,etal. Octo: Anopen-sourcegeneralistrobotpolicy. arXivpreprintarXiv:2405.12213,
2024.
[75] X.Huang,Y.Chi,R.Wang,Z.Li,X.B.Peng,S.Shao,B.Nikolic,andK.Sreenath.Diffuseloco:
Real-time legged locomotion control with diffusion from offline datasets. arXiv preprint
arXiv:2404.19264,2024.
[76] A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany,
M. K. Srirama, L. Y. Chen, K. Ellis, P. D. Fagan, J. Hejna, M. Itkina, M. Lepert, Y. J. Ma,
P.T.Miller,J.Wu,S.Belkhale,S.Dass,H.Ha,A.Jain,A.Lee,Y.Lee,M.Memmel,S.Park,
I.Radosavovic, K.Wang, A.Zhan, K.Black, C.Chi, K.B.Hatch, S.Lin, J.Lu, J.Mercat,
A.Rehman,P.R.Sanketi,A.Sharma,C.Simpson,Q.Vuong,H.R.Walke,B.Wulfe,T.Xiao,
J.H.Yang,A.Yavary,T.Z.Zhao,C.Agia,R.Baijal,M.G.Castro,D.Chen,Q.Chen,T.Chung,
J.Drake,E.P.Foster,J.Gao,D.A.Herrera,M.Heo,K.Hsu,J.Hu,D.Jackson,C.Le,Y.Li,
K. Lin, R. Lin, Z. Ma, A. Maddukuri, S. Mirchandani, D. Morton, T. Nguyen, A. O’Neill,
R. Scalise, D. Seale, V. Son, S. Tian, E. Tran, A. E. Wang, Y. Wu, A. Xie, J. Yang, P. Yin,
Y.Zhang,O.Bastani,G.Berseth,J.Bohg,K.Goldberg,A.Gupta,A.Gupta,D.Jayaraman,
J.J.Lim,J.Malik,R.Mart´ın-Mart´ın,S.Ramamoorthy,D.Sadigh,S.Song,J.Wu,M.C.Yip,
Y.Zhu,T.Kollar,S.Levine,andC.Finn. Droid: Alarge-scalein-the-wildrobotmanipulation
dataset. 2024.
13Appendix
A RP1MDatasetCollectionDetails
A.1 Rewardformulation
InEq.(3),wegivetheoverallrewardfunctionusedinourpaper. Wenowgivedetailsofeachterm.
rPressindicateswhethertheactivekeysarecorrectlypressedandinactivekeysarenotpressed. We
t
usethesameimplementationas[4],givenas: rPress =0.5·(1 (cid:80)Kg(||ki −1|| ))+0.5·(1−1 ).
t K t s 2 fp
Kisthenumberofactivekeys,kiisthenormalizedkeystateswithrange[0,1],where0meansthe
t
i-thkeyisnotpressedand1meansthekeyispressed. gistolerancefromTassaetal.[57],whichis
similartotheoneusedinEquation(2). 1 indicateswhethertheinactivekeysarepressed,which
fp
encouragestheagenttoavoidpressingkeysthatshouldnotbepressed. rSustainencouragestheagent
t
topressthepseudosustainpedalattherighttime,givenasrSustain =g(s −starget). s andstargetare
t t t t t
thestateofcurrentandtargetsustainpedalrespectively. rCollisionpenalizestheagentfromcollision,
t
definedasrCollision = 1−1 ,where1 is1ifcollisionhappensand0otherwise. rEnergy
t collision collision t
prioritizesenergy-savingbehavior. ItisdefinedasrEnergy =|τ |⊺|v |. τ andv arejoint
t joints joints joints joints
torquesandjointvelocitiesrespectively.
A.2 Trainingdetails
ObservationSpace Our1144-dimensionalobservationspaceincludestheproprioceptivestateof
dexterousrobothandsandthepianoaswellasL-stepgoalstatesobtainedfromtheMIDIfile. Inour
case,weincludethecurrentgoaland10-stepfuturegoalsintheobservationspace(L=11). Ateach
timestep,an89-dimensionalbinaryvectorisusedtorepresentthegoal,where88dimensionsarefor
keystatesandthelastdimensionisforthesustainpedal. Thedimensionofeachcomponentinthe
observationspaceisgiveninTable3.
Table3: Observationspace.
Observations Dim
Pianogoalstate L·88
Sustaingoalstate L·1
Pianokeyjoints 88
Pianosustainstate 1
Fingertipposition 30
Handstate 46
TrainingAlgorithm&Hyperparameters Althoughourproposedmethodiscompatiblewithany
reinforcementlearningmethod,wechoosetheDroQ[58]asZakkaetal.[4]forfaircomparison.
DroQisamodel-freeRLmethod,whichusesDropoutandLayernormalizationintheQfunctionto
improvesampleefficiency. WelistthemainhyperparametersusedinourRLtraininginTable4.
A.3 Computationalresources
WetrainourRLagentsontheclusterequippedwithAMDMI250XGPUs,64coresAMDEPYC
“Trento”CPUs,and64GBsDDR4memory. Eachagenttakes 21hourstotrain. Theoveralldata
collectioncostisroughly21hours*2089agents=43,869GPUhours.
A.4 MuJoCoXLAImplementation
To speed up training, we re-implement the RoboPianist environment with MuJoCo XLA (MJX),
whichsupportssimulationinparallelwithGPUs. MJXhasaslowperformancewithcomplexscenes
withmanycontacts. Toimprovethesimulationperformance,wemadethefollowingmodifications:
14Table4: HyperparametersusedinourRLagent.
Hyperparameter Value
Trainingsteps 8M
Episodelength 550
Actionrepeat 1
Warm-upsteps 5k
Buffersize 1M
Batchsize 256
Updateinterval 2
Pianoenvironment
Lookaheadsteps 10
Gravitycompensation True
Controltimestep 0.05
Stretchfactor 1.25
Trimslience True
Agent
MLPs [256,256,256]
Num. Q 2
Activation GeLU
DropoutRate 0.01
EMAmomentum 0.05
Discountfactor 0.88
Learnabletemperature True
Optimization
Optimizer Adam
Learningrate 3e-4
β 0.9
1
β 0.999
2
eps 1e-8
• Wedisablemostofthecontactsbutonlykeepthecontactsbetweenfingersandpianokeys
aswellasthecontactbetweenforearms.
• Primitivecontacttypesareusedwheneverpossible.
• Thedimensionalityofthecontactspaceissetto3.
• Themaximalcontactpointsaresetto20.
• WeuseNewtonsolverwithiterations=2andls iterations=6.
Aftertheabovemodifications,with1024parallelenvironments,thetotalstepspersecondis159,376.
WeusePPOimplementationimplementedwithJaxtofullyutilizetheparalleledsimulation. ThePPO
withMJXimplementationismuchfasterthantheDroQimplementation,whichonlytakes2hoursand
7minutesfor40MenvironmentstepsontheTwinkleTwinkleLittleStarsongwhileasacomparison,
DroQneedsroughly21hoursfor8Menvironmentsteps. However,thePPOimplementationfailsto
achieveacomparableF1scoreastheDroQimplementationasshowninFig.5. Therefore,weuse
theDroQimplementwiththeCPUversionoftheRoboPianistenvironment.
15Twinkle Twinkle Little Star
1.00
0.75
0.50
0.25 DroQ
PPO+MJX
0.00
0 20 40
Environment Steps (1e6)
Figure5: ComparisonoftheRLperformancebetweenDroQandPPOwiththeMJXimplementation
oftheRoboPianistenvironment. PPO+MJXisfastertorunbuthasaworseperformancethanDroQ.
WeuseDroQwiththeCPU-versionRoboPianistenvironmentwhentrainingourRLagents.
B MultitaskBenchmarkingDetails
Asinglemulti-taskpolicycapableofplayingvarioussongsishighlydesirable. However,playing
differentmusicpiecesonthepianoresultsindiversebehaviors,creatingacomplexactiondistribution,
particularlyfordexterousrobothandswithalargenumberofdegreesoffreedom(DoFs).Thissection
introducesthebaselinemethodswehavecomparedandthehyperparameterswehaveused. Wealso
talkaboutthedetailsofourmultitasktrainingandevaluation.
B.1 Baselinesandhyperparameters
B.1.1 BC
BehaviorCloning(BC)[62]directlylearnsapolicybyusingsupervisedlearningonobservation-
actionpairsfromexpertdemonstrations,whichisoneofthesimplestmethodstoacquireroboticskills.
Duetoitsstraightforwardapproachandprovenefficacy,BCispopularacrossmultiplefields. The
methodemploysaMulti-LayerPerceptron(MLP)asthepolicynetwork. Givenexperttrajectories,
thepolicynetworklearnstoreplicateexpertbehaviorbyminimizingtheMeanSquaredError(MSE)
betweenpredictedandactualexpertactions. Despiteitsadvantages,BCtendstoperformpoorlyin
generalizingtounseenstatesfromtheexpertdemonstrations. Inourstudy,weevaluatedthreeMLP
modelswithvaryinghiddendimensions—256,1024,and4096. Thefirsttwomodelsfeaturethree
layers,whilethemodelwith4096hiddendimensionsisdesignedwithsixlayers.
Table5: BC
Hyperparameter Value
BatchSize 256
Optimizer Adam
LearningRate 3e-4
Activation GELU
TrainingSteps 1M
ObservationHorizon 1
PredictionHorizon 1
ActionHorizon 1
16
erocS
1FB.1.2 IBC
ImplicitBehavioralCloning(IBC)[63]adoptsanovelangleonbehaviorcloningbyreformulating
supervisedimitationlearningasaconditionalenergy-basedmodelingproblem. Ittrainsanimplicit
policy represented by an energy function that is conditioned on both the action and observation,
utilizingtheInfoNCEloss[66]. Thismethoddemonstratesimprovedgeneralizationovertraditional
BC.However,itencounterstypicaldifficultiesassociatedwithtrainingenergy-basedmodels,andthe
needforintensiveactionsamplingandoptimizationatinferencetime,whichmaynotscalewellto
high-dimensionalactionspaces.
Table6: IBC
Hyperparameter Value
BatchSize 256
Optimizer AdamW
LearningRate 1e-4
LearningRateScheduler cosine
TrainingSteps 1M
WeightDecay 1e-6
PredictionNumofIteration 5
PredictionNumofSample 1024
ObservationHorizon 2
PredictionHorizon 2
ActionHorizon 1
B.1.3 BC-RNN
BC-RNN[64]isavariantofBCthatincorporatesaRecurrentNeuralNetworkasthepolicynetwork
to capture a sequence of past observations. It is the best-performing baseline in the Robomimic
paper[64].
Table7: BC-RNN
Hyperparameter Value
BatchSize 256
Optimizer AdamW
LearningRate 1e-4
LearningRateScheduler linear
TrainingSteps 1M
ObservationHorizon 1
PredictionHorizon 4
ActionHorizon 1
B.1.4 DiffusionPolicy
Diffusionmodelshaveachievedmanystate-of-the-artresultsacrossimage,video,and3Dcontent
generation [67, 68, 69, 70, 71]. In the context of robotics, diffusion models have been used as
policynetworksforimitationlearninginbothmanipulation[5,72,73,74]andlocomotiontasks[75],
showingremarkableperformanceacrossvariousrobotictasks. DiffusionPolicy[5]proposedtolearn
animitationlearningpolicywithaconditionaldiffusionmodel. Itmodelstheactiondistributionby
invertingaprocessthatgraduallyaddsnoisetoasampledactionsequence,conditioningonastateand
asamplednoisevector. WeusedaCNN-basedDiffusionPolicywithDDIM[65]asthesamplerto
17diffuseoutactiontrajectoriesforimprovedefficiency. Webuildourdiffusionpolicytrainingpipeline
basedontheRobomimic[64]andDROID[76],whichprovidehigh-qualityimplementations.
Table8: DiffusionPolicy
Hyperparameter Value
BatchSize 128
Optimizer Adam
LearningRate 1e-4
LearningRateScheduler Linear
TrainingSteps 1M
DiffusionMethod DDIM
EMAPower 0.75
U-NetHiddenLayerSizes [256,512,1024]
ObservationHorizon 2
PredictionHorizon 4
ActionHorizon 1
B.2 Trainingandevaluation
We train the policies with 3 different sizes of expert data: 50, 150, and 300 songs, respectively.
Subsequently,weassessthetrainedpoliciesusingthreedistinctcategoriesofmusicalpieces. The
firstcategory,in-distributionsongs,includespiecesthatarepartofthetrainingdatasets. Evaluating
within-distributionsongsteststhemultitaskingabilitiesofthepoliciesandchecksifapolicycan
accuratelyrecallthesongsonwhichitwastrained. Thesecondgroupofsongsforevaluationare
easyout-of-distribution(OOD)songs: thosemusicpiecesdonotoverlapwiththetrainingsongs
but they are easy to play. They only contain slow motions and short horizons. The third group
ofevaluationsongsarehardout-of-distributionsongs: thosearedifficultmusicpiecesthatdonot
overlapwiththetrainingsongs. Theycontainmorediversemotionsandlongerhorizons. Thisout-of-
distributionevaluationmeasuresthezero-shotgeneralizationcapabilitiesofthepolicies. Analogous
toanexperiencedhumanpianistwhocanplaynewpiecesatfirstsight,weaimtodetermineifitis
feasibletodevelopageneralistagentcapableofplayingthepianoundervariousconditions.
Additionally,ourframeworkisdesignedwithflexibilityinmind,allowinguserstoselectsongsnot
includedinourdatasetforeithertrainingdatacollectionorevaluation. Furthermore,usershavethe
optiontoassesstheirpoliciesonspecificsegmentsofasongratherthantheentirepiece.
Table9: In-distributionsongs
RoboPianist-etude-12-FrenchSuiteNo1Allemande-v0
RoboPianist-etude-12-FrenchSuiteNo5Sarabande-v0
RoboPianist-etude-12-PianoSonataD8451StMov-v0
RoboPianist-etude-12-PartitaNo26-v0
RoboPianist-etude-12-WaltzOp64No1-v0
RoboPianist-etude-12-BagatelleOp3No4-v0
RoboPianist-etude-12-KreislerianaOp16No8-v0
RoboPianist-etude-12-FrenchSuiteNo5Gavotte-v0
RoboPianist-etude-12-PianoSonataNo232NdMov-v0
RoboPianist-etude-12-GolliwoggsCakewalk-v0
RoboPianist-etude-12-PianoSonataNo21StMov-v0
RoboPianist-etude-12-PianoSonataK279InCMajor1StMov-v0
18Table10: Easyout-of-distributionsongs
RoboPianist-debug-TwinkleTwinkleLittleStar-v0
RoboPianist-debug-CMajorChordProgressionTwoHands-v0
RoboPianist-debug-TwinkleTwinkleRousseau-v0
RoboPianist-debug-NocturneRousseau-v0
RoboPianist-debug-NocturneRousseau-v0
Table11: Hardout-of-distributionsongs
GP-AkimenkoTheodoreAuCoinDuFeuOp28-v0
GP-AgnewRoy2PianoPieces-v0
GP-AlbaAntonioElEnsuenoOp16-v0
GP-AlbaAntonioSensitiva-v0
GP-MinotAdolfMisterioso-v0
19