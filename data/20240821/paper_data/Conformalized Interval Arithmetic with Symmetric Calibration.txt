CONFORMALIZED INTERVAL ARITHMETIC WITH SYMMETRIC
CALIBRATION
Rui Luo∗1 and Zhixin Zhou†1
1City University of Hong Kong
Abstract. Uncertainty quantification is essential in decision-making, especially when joint distribu-
tionsofrandomvariablesareinvolved. Whileconformalpredictionprovidesdistribution-freeprediction
setswithvalidcoverageguarantees,ittraditionallyfocusesonsinglepredictions. Thispaperintroduces
novelconformalpredictionmethodsforestimatingthesumoraverageofunknownlabelsoverspecific
indexsets. Wedevelopconformalpredictionintervalsforsingletargettothepredictionintervalforsum
of multiple targets. Under permutation invariant assumptions, we prove the validity of our proposed
method. We also apply our algorithms on class average estimation and path cost prediction tasks,
andweshowthatourmethodoutperformsexistingconformalizedapproachesaswellasnon-conformal
approaches.
1. Introduction
Conformal prediction [35] has gained popularity as a method for uncertainty quantification due to its
validity guarantee. It plays a crucial role in modern decision-making processes, particularly in domains
where understanding the joint distribution of random variables is essential. Under the exchangeability
assumptions on the calibration and test samples, conformal prediction can provide prediction sets with
validcoverageguarantees. Existingworkshavestudiedpredictionintervalsforsingletargets[16,29,31].
Given a black box algorithm to estimate the target label, current conformalized algorithms can provide
predictionintervalsforthetargetwithreliablecoverageprobability. However,althoughnumerousstudies
have developed the methodology of conformal prediction, they are limited to predicting a single label,
leaving a gap in finding the prediction set for the sum of labels.
This gap is particularly relevant in applications such as transductive conformal prediction on traffic
networks. For example, existing Graph Neural Network (GNN) methods can predict the label of each
road, where the label can be considered as the cost of traversing that road. This problem has been
studied in [11, 21, 19]. Conformalized GNN can output a prediction set of each edge’s label, but the
cost of a route, which is the sum of the labels of the edges on the route, cannot be directly obtained by
applying conformal prediction to individual edges. This challenge arises from two main aspects. First,
the sum or average of random variables involves the convolution of the density function, and simple
interval arithmetic, such as adding up the lower and upper bounds, cannot provide a confidence interval
with the desired coverage. Second, the coverage of each confidence interval is in a marginal sense, so the
coverage of two labels from two confidence intervals are dependent events. Consequently, it is difficult
to use the conformal prediction set for a single label to devise a prediction set for multiple labels. To
address this critical gap in the current literature on conformal prediction and expand its applicability
to a broader range of uncertainty quantification problems, we introduce the method Conformal Interval
Arithmetic (CIA),specificallydesignedtoestimatetheaverageorothersymmetricfunctionsofunknown
labelsoveracertainindexset, demonstratingtheusefulnessofourproblemsettinginmanyapplications
where people are interested in obtaining estimates about multiple labels.
Our main proposed method can be described as follows. The core idea is to establish a confidence
interval using the exchangeability of the groups of indices. This method involves finding the absolute
value of the difference between the sum of labels in a group and its prediction, or absolute residual,
and using this as the score function for split conformal prediction. Assuming the groups of indices are
exchangeable, we can provide prediction sets with valid marginal coverage. However, this approach
cannothandlethecasewhenthecalibrationsamplesandthetestsamplesbelongtodifferentgroups. To
address this issue, we introduce a new split conformal prediction method called symmetric calibration,
which creates a scenario in which the calibration set and the test set play symmetric roles. At the group
level,eachindexwithinthesamegrouphasequalchanceofbeingassignedtoeitheracalibrationsample
∗ruiluo@cityu.edu.hk
†
zhixzhou@cityu.edu.hk
1
4202
guA
02
]GL.sc[
1v93901.8042:viXra2 CONFORMALIZED INTERVAL ARITHMETIC WITH SYMMETRIC CALIBRATION
oratestsample. Thisensuresthattheresidualfromthesumsofcalibrationsamplesandthesumsoftest
samples have identical distributions. By leveraging the exchangeability of the groups of samples in the
calibration set, we can devise a conformal prediction set for the sum of residuals of calibration samples.
Since both the calibration samples and the test samples share the same distribution, this conformal
prediction set is also valid for the sum of the test samples.
Inadditiontothemethodologyofsymmetriccalibrationandthevalidityguaranteediscussedabove,we
will introduce some important extensions of our method to make it more widely applicable and efficient.
First, symmetric calibration is easy to understand when the groups of indices are disjoint. We relax this
condition to allow for small amounts of overlap between groups. Second, given prior knowledge of the
number of unknown labels being summed, we can stratify the prediction into different levels. Finally,
we can incorporate other conformal prediction methods, such as conformalized quantile regression [29],
to improve prediction efficiency.
To summarize the contribution of this paper:
(1) We introduce the idea of conformalized interval arithmetic (CIA) to provide confidence interval
for sum of multiple unknown labels.
(2) We develop the technique of symmetric calibration to address the issue that unknown label can
appear in different groups. The proposed method has valid coverage guarantee theoretically and
empirically.
(3) We apply our methods on datasets in [30], and show the reliability of our method. The code of
our method will be published as open source.
2. Preliminary and Problem Setup
We will first provide an overview of conformal prediction in the literature. Then, we will study
conformalized interval arithmetic (CIA) in a simple setting, where the calibration set and test set are
exchangeable at the group level. In this case, we can use the residuals of the sums to perform split
conformal prediction on the test group.
2.1. Split Conformal Prediction. Conformal prediction has been applied to the following types of
problems. Suppose a dataset with an index set I is partitioned into training, calibration, and test sets,
with index sets Itrain, Ical, and Itest, respectively. Firstly, we apply a black-box model to the training
data{(x i,y i)}
i∈Itrain
andobtainafunctionf(cid:98)suchthaty
(cid:98)i
=f(cid:98)(x i). Wedefineaconformityscorefunction
s(x,y), which depends on f(cid:98). A larger value of the score function indicates that y is less likely to be a
true label. A typical choice of the score function is
s =s(x ,y )=|y −y |.
i i i i (cid:98)i
We will first focus on this choice and then extend it to other choices in later sections. For data in
{(x i,y i)} i∈Ical, we evaluate s
i
= |y i−y (cid:98)i| and let Q
1−α
be the ⌈(1+|Ical|)(1−α)⌉-th smallest value of
|y i−y (cid:98)i| for i∈Ical. Then, for i∈Itest, the prediction set of y
i
is defined as
C(i)={y :|y−y |≤Q }=[y −Q ,y +Q ].
(cid:98)i 1−α (cid:98)i 1−α (cid:98)i 1−α
Under the assumption of exchangeability of the samples in the calibration set and the test set, it can be
shownthatthisconformalpredictionsethascoverageofatleast1−α. Inpredictiontasks, theresulting
prediction set C(i) is usually an interval. Our present work will focus on the generalization of these
prediction intervals.
2.2. Problem Setup and a Naive Approach. In many applications, we are not just satisfied with
the prediction set for a single label y . Unlike existing works, this paper aims to provide the prediction
i
(cid:80)
set of the sum of variables, i.e., a prediction set C(S) for y . We will apply conformal prediction
i∈S i
techniques so that 1−α coverage is guaranteed under certain exchangeability assumption.
The choice of S ⊆ Itest can depend on fixed or random procedures. Some concrete examples will be
considered in Section 7. We can start with a simple example. Let S = {i,j} ⊆ Itest. Suppose a simple
split conformal prediction provides prediction intervals P(Y ∈ [L ,U ]) ≥ 1−α and P(Y ∈ [L ,U ]) ≥
i i i j j j
1−α, then the addition operation of interval arithmetic gives
P(Y +Y ∈[L +L ,U +U ])≥P(Y ∈[L ,U ] and Y ∈[L ,U ])≥1−2α.
i j i j i j i i i j j j
Hence, the addition operation of interval arithmetic of 1−α confidence intervals cannot guarantee 1−α
coverage. Onepossiblecorrectionistofirstfind1−α/2confidenceintervalsforY andY ,thenconstruct
i j
the prediction interval by adding the intervals. This can be generalized using Bonferroni correction to
predictthesumofanynumberofunknownlabels,withoutrequiringassumptionsaboutthedependenciesCONFORMALIZED INTERVAL ARITHMETIC WITH SYMMETRIC CALIBRATION 3
between variables. However, this method clearly lacks efficiency. Our method will be compared with
Bonferroni correction in empirical experiments in Section 7.
3. Conformal Interval Arithmetic
We first study a simple case, in which the problem can be reduced to existing conformal prediction
techniques. Suppose we have disjoint index sets S ,S ,...,S ,S and suppose we observe samples
1 2 K K+1
(x ,y )fori∈S ∪S ∪···∪S ,andx forj ∈S astestsamples. Wecanthencalculatetheresidual
i i 1 2 K j K+1
as the score function on the set level:
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)(cid:88) (cid:12)
s
k
:=s(z k):=(cid:12) y i−y (cid:98)i(cid:12),k ∈[K]
(cid:12) (cid:12)
(cid:12)i∈Sk (cid:12)
where K ={1,...,K} and z =(x ,y ) denotes a group of pairs of (x,y). The following procedure
k i i i∈Sk
followssimilarlyfromtheconformalpredictionfromtheprevioussection. LetQ bethe⌈(1+K)(1−
1−α
α)⌉-th smallest value of s ,k ∈[K]. We define the prediction set C(S ) as
k K+1
 
(cid:88) (cid:88)
(1)  y (cid:98)i−Q 1−α, y (cid:98)i+Q 1−α.
i∈SK+1 i∈SK+1
This procedure provides conformal prediction for the sum of unknown labels and is called Conformal
Interval Arithmetic (CIA) in this paper. Assuming group exchangeability, it can be guaranteed that the
coverage probability is valid.
Proposition 1. Suppose that Z is group exchangeable in the sense of for any permutation π on [K+1]:
k
(2) P(Z ,...,Z )=P(Z ,...,Z ),
1 K+1 π(x) π(K+1)
then the prediction set C(S ) in (1) satisfies
K+1
Ñ é
(cid:88)
P y ∈C(S ) ≥1−α.
i K+1
i∈SK+1
The idea of conformal interval arithmetic can be applied to the simple case where the test sample
with unknown labels belongs only to the same group, i.e., S , and y for i∈S ,...,S must be fully
K+1 i 1 K
observed. However, this assumption may not hold for many scenarios, such as cost estimation of edges
in a route, as discussed in the introduction. Consider a traffic network where we randomly sample two
nodes and find the shortest path between them. If a sufficient amount of edge weights are not observed,
then only a few routes will contain all edges with observed weights. Let S ,...,S be the set of edges
1 K
on a path, determined by the network structure and edge costs. For unobserved weights, they can be
estimated by their predictions. In this method, every shortest path can contain edges with unobserved
weights,andedgescanbelongtomultiplepaths. Theseissuesmakethesimplecasemethodinapplicable.
4. Symmetric Calibration
(cid:80)
Let us formally state the problem. We aim to provide a prediction set for y . Since y is not
(cid:80)
i∈Sk i i
observed only for i ∈ I, it is equivalent to finding the prediction set for y . In the previous
i∈Sk∩Itest i
section, we considered a simple case where S
K+1
= Itest. Here, we will consider a general case where
every S can contain indices of unobserved labels. To tackle this problem, we introduce the method of
k
symmetric calibration.
Before training the model f(cid:98), we first hold out a calibration set that has the same size as the test set,
i.e.,|Ical|=|Itest|. Notethatweassumethenumberofobservedlabelsmustbegreaterthanthenumber
of unobserved labels. Now, we focus on the calibration and test samples. Let us assume
S 1∪···∪S
K
∪S
K+1
=Ical∪Itest,
and let us define
cal test
(3) S
k
:=S k∩Ical and S
k
=S k∩Itest.
(cid:80)
Our task becomes finding the prediction set for y for every k ∈ [K +1]. However, since the
i∈Stest i
k (cid:80)
algorithm is equivalent, we can focus on finding the confidence interval for y . Using conformal
i∈Stest i
K+14 CONFORMALIZED INTERVAL ARITHMETIC WITH SYMMETRIC CALIBRATION
interval arithmetic, we define the score function similar to (3):
(cid:12) (cid:12)
(cid:12) (cid:12)
cal (cid:12) (cid:88) (cid:12)
(4) s
k
=(cid:12)
(cid:12)
y i−y (cid:98)i(cid:12)
(cid:12)
(cid:12)i∈Scal (cid:12)
k
Let Q be the ⌈(1+K)(1−α)⌉-th smallest value of scal,k =1,...,K. Then we define the prediction
1−α k
set for (cid:80) y , denoted by C(Stest ):
i∈Stest i K+1
K+1
 
(cid:88) (cid:88)
(5)  y (cid:98)i−Q 1−α, y (cid:98)i+Q 1−α.
i∈Stest i∈Stest
K+1 K+1
The above procedure is summarized in Algorithm 1.
Algorithm 1 CIA with Symmetric Calibration
1: Input: labeled samples {(x i,y i)} i∈I\Itest, unlabeled samples {x i} i∈Itest, a black box algorithm B,
disjoint index sets S ,...,S ,S ⊆I.
1 K K+1
2: Output: Prediction sets C(S ktest) for (cid:80) i∈Stest y i.
K+1
3: Hold out a calibration set Ical from I\Itest such that Ical has the same size as Itest.
4: f(cid:98)←B({(x i,y i)} i∈I\(Ical∪Itest).
5: y (cid:98)i ←f(cid:98)(x i) for i∈Ical∪Itest.
(cid:12) (cid:12)
6: sc kal =(cid:12) (cid:12)(cid:80) i∈Scaly i−f(cid:98)(x i)(cid:12) (cid:12) for k ∈[K]
k
7: Q 1−α ←⌈(1+K)(1−α)⌉-th smallest value of {sc kal} k∈[K].
(cid:104) (cid:105)
8: C(S Kte +st 1)← (cid:80) i∈Stest y (cid:98)i−Q 1−α,(cid:80) i∈Stest y (cid:98)i+Q 1−α .
K+1 K+1
9: Output: C(Stest ).
K+1
WewillanalyzethecoverageprobabilityofC(Stest ). Inthissection,weassumethatS ,...,S ,S
K+1 1 K K+1
are disjoint. We will introduce the reasoning behind this in two steps. Firstly, we assume that the score
functions scal,k ∈ [K +1], defined in (3) are exchangeable. scal has a uniform rank over [K +1], so
k K+1
scal ≤Q with probability at least 1−α. Equivalently, this suggests that the interval
K+1 1−α
 
(cid:88) (cid:88)
(6)  y (cid:98)i−Q 1−α, y (cid:98)i+Q 1−α
i∈Scal i∈Scal
K+1 K+1
(cid:80)
covers y with probability at least 1−α. This result comes from Proposition 1. However, we
i∈Scal i
K+1
are not interested in the sum over Scal . Instead, we want to show that the sum over Stest has the
K+1 K+1
same property. We need the following assumption: the index is randomly assigned to the calibration set
or test set with equal probability:
(7) P(i∈Ical)=P(i∈Itest)=0.5
for all i ∈ I\Itrain independently. In other words, Ical has a uniform distribution on the power set
2I\Itrain. We summarize the above argument in the following theorem.
Theorem 4.1. Suppose
(a) S ,...,S ,S are disjoint.
1 K K+1
(b) The groups are exchangeable in the sense of (2).
(c) The labels in I ∪I are assigned to I and I with equiprobability independently.
cal test cal test
Then the coverage probability satisfies
Ñ é
(cid:88)
P y ∈C(Stest ) ≥1−α.
i K+1
i∈Stest
K+1
The proof will appear in the appendix in the supplimentary material. In this theorem, we have only
shown the validity for Stest . However, since we assume the group exchangeability on [K+1], the result
K+1CONFORMALIZED INTERVAL ARITHMETIC WITH SYMMETRIC CALIBRATION 5
can also be stated as
Ñ é
P (cid:88) y ∈C(Stest ) ≥1−α,
i k
i∈Stest
k
for every k ∈[K +1] by changing the algorithm accordingly. To be more precise, to provide prediction
interval for sum over Stest, Q is ⌈(1+K)(1−α)⌉-th smallest value of scal for ℓ∈[K+1]\{k}. The
k 1−α ℓ
rest of the steps remain the same as in Algorithm 1.
Proposition 1 S S ··· S S
1 2 K K+1
Ical Itest
Theorem 1 S 1cal S 2cal ··· S KcalS Kca +l
1
Ical
S 1test S 2test ··· S KtestS Kte +st
1
Itest
Figure 1. Diagram illustrating the partition of data into calibration and test sets for
Proposition1andTheorem4.1. ThesumswithineachofS ,...,S (Top)serveasthe
1 K
calibration data to predict the sum within S , mirroring the calibration row for the
K+1
theorem’s diagram (Bottom). As Scal and Stest are exchangeable, their sums have
K+1 K+1
the same distribution. Therefore, the prediction set (6) can be derived similarly to (5).
Remark. The random splitting assumption (7) is slightly different from the procedure of holding out
a calibration set I with the same size as I in Algorithm 1. Theorem 4.1 suggests that it only
cal test
requires the calibration set and test set have roughly the same size. Suppose I and I are partitions
cal test
of I ∪I with equal size, this complicates the proof of the theorem, as we can only swap Scal and
cal test K+1
Stest when they have the same size.
K+1
5. Extensions
In this section, we will extend the applicability of the proposed method to handle overlapping sub-
sets. Furthermore, we will introduce a stratified technique and utilize Conformal Quantile Regression
(CQR) [29] to enhance the efficiency of our approach.
5.1. Overlapping Subsets. Algorithm 1 can be applied to overlapping subsets S ,...,S ,S , but
1 K K+1
the conclusion of Theorem 4.1 is valid only if the subsets are disjoint. There are applications where we
must assume that the subsets overlap. We will study these applications in Section 7.2. Theoretically,
overlapping subsets affect the validity of the method. The following theorem shows that as long as each
subset overlaps with only a small portion of the other subsets, the effect on the coverage probability is
acceptable.
Theorem 5.2. We still assume condition (b) and (c) in Theorem 4.1, and suppose that
1 (cid:88)
max 1{S ∩S ̸=∅}=δ,
ℓ∈[K+1]K+1 k ℓ
k̸=ℓ
Then the coverage probability satisfies
Ñ é
(cid:88)
P y ∈C(Stest ) ≥1−α−δ.
i K+1
i∈Stest
K+1
The proof will appear in the appendix in the supplimentary material. Similar as the result of Theo-
rem 4.1, the coverage probability guarantee can be generalized to any Stest for any k ∈[K+1].
k6 CONFORMALIZED INTERVAL ARITHMETIC WITH SYMMETRIC CALIBRATION
Algorithm 2 Stratified CIA with Symmetric Calibration
1: Input: labeled samples {(y i,y (cid:98)i)} i∈I\Itrain, disjoint index sets S 1,...,S K,S K+1 ⊆ I. Partition of
positive integers C ,C ,...
1 2
2: Output: Prediction sets C(S ktest) for (cid:80) i∈Stest y i.
K+1
3: Find C j such that |S Kte +st 1|∈C j.
(cid:12) (cid:12)
4: sc kal =(cid:12) (cid:12)(cid:80) i∈Scaly i−y (cid:98)i(cid:12) (cid:12) for k ∈[K].
k
5: n j ←|{k ∈[K]:|S kcal|∈C j}|.
6: Qj
1−α
←⌈(1+n j)(1−α)⌉-th smallest value of {sc kal :|S kcal|∈C j}.
(cid:104) (cid:105)
7: C(S Kte +st 1)← (cid:80) i∈Stest y (cid:98)i−Qj 1−α,(cid:80) i∈Stest y (cid:98)i+Qj 1−α .
K+1 K+1
8: Output: C(Stest ).
K+1
5.3. Stratified Conformal Prediction. Given the known number of unknown labels in each test set,
wecanleveragethisinformationtoenhancetheefficiencyofconformalprediction. Forinstance,consider
acasewhere|Stest |isrelativelysmall,suchas|Stest |=1. Inthiscase,itismoreappropriatetocompare
K+1 K+1
the residual with other residuals that have a smaller number of unknown labels in the calibration set.
By incorporating this idea, we can modify Algorithm 1 and derive a stratified version of the algorithm
that takes advantage of this additional information to improve its efficiency.
In practice, the stratified approach can often reduce the size of the prediction set. However, if the
size of C is too small, it may negatively impact the validity of the method. To mitigate this issue, it is
j
advisable to set lower bounds for n when defining the sets C . This ensures that each stratum has a
j j
sufficient number of samples to maintain the desired coverage probability while still benefiting from the
increased efficiency provided by the stratified technique.
5.4. Conformal Quantile Regression. From the theorems, we can observe that the form of the score
function can be modified. For instance, we can adopt Conformal Quantile Regression (CQR) [29]. The
score function can be defined as
 
scal =max (cid:88) yα/2−y , (cid:88) y −y1−α/2
k (cid:98)i i i (cid:98)i
 
i∈Scal i∈Scal
k k
for k ∈[K], where yα/2 and y1−α/2 are the outputs of quantile regression at quantiles α/2 and 1−α/2,
(cid:98)i (cid:98)i
respectively, to predict y . We can obtain Q in Algorithm 1 using the score functions of CQR. The
i 1−α
prediction set C(Stest ) is then given by
K+1
 
 (cid:88) y (cid:98)iα/2−Q 1−α, (cid:88) y (cid:98)i1−α/2+Q 1−α.
i∈Stest i∈Stest
K+1 K+1
CQR can also be employed in Algorithm 2, with similar modifications required in Step 4 and Step 7 of
the algorithm.
In CQR, if yα/2 and y1−α/2 are the true quantiles of the distribution of Y , then CQR can achieve
(cid:98)i (cid:98)i i
conditional 1−α coverage. However, after combining with CIA, we do not have conditional validity
because the sum of quantiles is not necessarily the quantile of the sum. Nevertheless, the upper and
lower quantiles can still help in measuring the variation of the estimation and improving the efficiency
of the conformal prediction.
6. Related Work
In this section, we will explore the relationships and distinctions between the present work and prior
research. Although we maintain that the ideas of Conformal Interval Arithmetic (CIA), symmetric
calibration, and stratified conformal prediction are original and groundbreaking, and that they are un-
paralleled among existing techniques for addressing the issues presented in this paper, there are studies
in the literature that are relevant to these concepts.
6.1. Multiple Test. Conformal prediction has been applied to multiple testing [10, 13, 5, 4, 2]. While
ourpresentworkalsodealwithmultiplerandomvariables,CIAprovidesonlyoneconfidenceintervalfor
the sum of these random variables, so our method is essentially different from these multiple test-based
approaches.CONFORMALIZED INTERVAL ARITHMETIC WITH SYMMETRIC CALIBRATION 7
6.2. Half Sampling. InTheorem4.1, theconclusioncanberestatedasfollows: the(1−α)-thquantile
ofthescorefunctionvaluesinthecalibrationsetislargerthanaproportionof1−αofthescorefunction
valuesinthetestset. Atahigherlevel,thisisacomparisonbetweenthe(1−α)-thquantileofhalfofthe
samples and the entire sample set. In this regard, symmetric calibration can be linked to the balanced
half-sample method [15, 32], which was popular from 1970 to 2000. The balanced half-sample method
shares the same idea as symmetric calibration, as it compares the statistic from the half-sample with
the entire sample. It is possible that further theory about symmetric calibration can be developed by
building upon classical half-sample theory.
6.3. Fair Conformal Prediction. Fairconformalprediction[18,17,36]andgroup-conditionalconfor-
malprediction[28,9,25]aimtoequalizecoverageacrossvariousgroupscharacterizedbygroupattributes
or auxiliary data. This idea is related to the stratified conformal prediction discussed in this paper. The
procedures in these methods are very similar. However, the motivation behind stratified conformal
prediction is to improve the efficiency of the conformal prediction, while group-conditional conformal
prediction is designed to ensure fairness among different groups.
6.4. Robust Route Planning. As mentioned in the introduction, our method can be applied to path
cost prediction. We will further discuss this application in Section 7.2. In [34, 26], the authors intro-
duce Conformal-Predict-Then-Optimize (CPO) for linear programming by predicting edge weights and
minimizing upper bounds of path costs for optimal robust solutions. While our method can generate
conformal predictions for algorithm-selected paths, choosing the path with the smallest upper bound
depends on the score function, compromising the guarantee of 1−α coverage. Additionally, applying
CPO to our problem is inefficient, as it only provides conformal predictions for the joint distribution
of edge weights. We are also aware of recent works [6, 14, 33] which address decision making under
uncertainty.
7. Application and Experiment
In this section, we will introduce two main applications of the proposed methods. Then we will
analyze the empirical performance of our algorithms on public datasets. The code is publicly available
at GitHub.
7.1. Application 1: Group Average Prediction. Suppose that in a dataset, x is the jth feature
ij
of the ith sample, and suppose that this feature is categorical. We can then define the subsets as
S
k
={i∈Ical∪Itest :x
ij
=k}
To predict the average value in class k, we can equivalently predict (cid:80) y , since Stest is the set of
i∈Stest i k
k
indices corresponding to the unknown labels in class k.
7.1.1. Datasets.
(1) Bike Sharing [8]: This dataset is used to investigate the factors influencing bike rental demand.
The grouping features are season, workingday, and weather, which have 272 unique value
combinations out of 10886 samples.
(2) Community Crime [27]: This dataset is used to predict the per capita violent crime rate of a
communityusingitsdemographicfeatures. Thegroupingfeaturesarestateandcounty,which
have 350 unique value combinations out of 1994 samples.
(3) Medical Expenditure Panel Survey [1]: This dataset is used to predict the utilization of medical
services based on various features. The grouping features are age, race, and marriage, which
have 302, 302, and 301 unique value combinations out of 15785, 17541, and 15656 samples for
the years 2019, 2020, and 2021, respectively.
Following similar procedures in [30], we standardize the response variables Y for all datasets. We use
70% of data for training the quantile regression model and 30% for calibration and testing.
7.2. Application 2: Path Cost Prediction. As discussed in the introduction, our method has a
valuable application in route planning problems. We consider a transductive setting in edge regression
problems, where I represents the set of edge indices. In the experiment, we assume that there are edges
with unknown labels, and we denote the set of these edges as Itest. Similar to Algorithm 1, we can hold
out a set of edges with known labels as the calibration set Ical. Utilizing node and/or edge features,
the network structure, and the observed edge labels (costs), we train a Graph Neural Network (GNN)
to predict the labels of the calibration set and test set. The transductive setting of GNN is introduced8 CONFORMALIZED INTERVAL ARITHMETIC WITH SYMMETRIC CALIBRATION
in [11], where they consider the prediction of node labels. However, more recent work [19] which focuses
on edge prediction is more suitable for this particular application set.
While the choice of S ’s can be quite general, the most interesting case in the traffic network data is
k
whenweusetheedgesontheshortestpathasthesetofedgesS . First, werandomlysampletwonodes
k
(s,t) as the source and target nodes, respectively. Then, we use y
i
for i∈Itrain and y
(cid:98)i
for i∈Ical∪Itest
as the cost of each edge. Next, we find the shortest path using Dijkstra’s algorithm. By sampling K+1
pairs of (s,t), the indices of edges on the path form an index set S . Given the index sets and the
(s,t)
predictions y for calibration samples and test samples, we can apply Algorithm 1 or Algorithm 2 to
(cid:98)i
obtain the conformal prediction.
In this setting, the index sets can overlap, meaning that different paths can contain the same edge.
However, in a large network without bottlenecks, two shortest paths with random sources and targets
share the same edge with a small probability. Therefore, Theorem 5.2 can guarantee that the coverage
of the prediction set is very close to 1−α.
Dataset: Road Traffic in Anaheim and Chicago [3]. This dataset is used to predict the traffic flow along
certain roads based on the traffic flow along other roads. The predicted traffic flow is then utilized as
edgecostforshortestpathplanning. Thisscenariocorrespondstothesituationofsubsetswithoverlaps,
where the edges (roads) in different subsets (shortest paths) may overlap with each other. We collect
2000 shortest paths by randomly sampling (s,t) pairs. The Anaheim dataset consists of 413 nodes and
858 edges, and the Chicago dataset consists of 541 nodes and 2150 edges. We adopt a similar procedure
from [12, 11], and allocate 50%, 10%, and 40% for training, validation, and calibration and testing.
7.3. Baseline. Despiteofthesignificanceofourmethodinapplication,therearefewmethodcanprovide
valid confidence interval for finding the intervals. We introduce three possible methods for comparison.
GroupSamplingConformalPrediction. ThismethodcorrespondstotheapproachdescribedinSection3.
(cid:80)
To predict y , we sample K groups of samples from the calibration sets, where each group
i∈Stest i
K+1
contains |Stest | samples. Let S ,...,S be the index sets of these groups. The prediction set is then
K+1 1 K
given by (1).
It is important to note that in this method, the indices of S ,...,S may not belong to the same
1 K
class, which means that this approach may not be appropriate in all cases. However, the method is
always applicable, regardless of the class composition of the sampled groups.
Normal Confidence Interval. This approach assumes that y −y follows an i.i.d. N(0,σ2) distribution.
(cid:98)i i
The predicted common variance is estimated as
1 (cid:88)
σ2 = (y −y )2.
(cid:98) |Ical|−1 (cid:98)i i
i∈Ical
(cid:80)
Then, the prediction set for y is given by
i∈Stest i
K+1
 
(cid:88) » (cid:88) »
 y (cid:98)i+zα
2
|S Kte +st 1|σ (cid:98), y (cid:98)i+z 1−α
2
|S Kte +st 1|σ (cid:98),
i∈Stest i∈Stest
K+1 K+1
where z and z are the α/2 and (1−α/2)-th quantile of the standard normal distribution. It is
α/2 1−α/2
important to note that this approach relies on the assumption of normality, which is unlikely to hold in
most real-world datasets.
Bonferroni correction. The Bonferroni correction has been employed in various conformal methods [10,
13,7]. AsmentionedinSection2.2,itisalsoapplicabletoourproblem. WhiletheBonferronicorrection
is a valid approach, it often lacks efficiency in the context of our problem.
7.4. Results. We split the calibration and test sets equally as indicated by (7), repeating this process
100 times to record the average results and the standard deviation. Figure 2 presents the results for
constructing prediction sets for subsets without overlaps, specifically for the Bike Sharing, Community
Crime, and Medical Expenditure Panel Survey datasets. We compare the baseline methods with Al-
gorithm 2 using CQR, as described in Section 5.4. In these figures, we refer to our method as “CIA.”
The baseline methods from Section 7.3 are labeled as “Group,” “Normal,” and “Bonferroni,” respectively.
Additional results will appear in the appendix.
For these five datasets, the left plots present the desired miscoverage rate α versus the true coverage
rate. The closer the curve aligns with the line 1−α, the easier it is for the method to achieve the
desired coverage. Our proposed method, CIA, is very close to the line 1−α. There is a small gap in the
result of the Anaheim dataset. This result aligns with Theorem 5.2. Compared to the Chicago dataset,
Anaheim is a smaller traffic network. The probability of two shortest paths overlapping each other canCONFORMALIZED INTERVAL ARITHMETIC WITH SYMMETRIC CALIBRATION 9
bike community
1.00 30 1.00 6
0.98
25 0.95 5
0.96
0.94 20 0.90 4
0.92 15
0.85
0.90 10 3
0.88 0.80
0.86 5 2
0.75
0
0.02 0.04 0.06 0.08 0.10 0.8750.9000.9250.9500.9751.000 0.02 0.04 0.06 0.08 0.10 0.80 0.85 0.90 0.95 1.00
Coverage Coverage
CIA Group Normal Bonferroni CIA Group Normal Bonferroni
meps19 meps20
1.0 1.0
600 800
0.9 0.9
500
600
0.8 400 0.8
300 400
0.7 0.7
200
200
0.6 100 0.6
0.5 0 0.5 0
0.02 0.04 0.06 0.08 0.10 0.6 0.7 0.8 0.9 1.0 0.02 0.04 0.06 0.08 0.10 0.6 0.7 0.8 0.9 1.0
Coverage Coverage
CIA Group Normal Bonferroni CIA Group Normal Bonferroni
meps21
1.0
600
0.9 500
0.8 400
300
0.7
200
0.6
100
0.5 0
0.02 0.04 0.06 0.08 0.10 0.6 0.7 0.8 0.9 1.0
Coverage
CIA Group Normal Bonferroni
Figure2. Resultsforconstructingpredictionsetsforsubsetswithoutoverlaps(Section
7.1). CIA achieves guaranteed 1−α coverage as well as optimal efficiency on various
datasets.
Anaheim Chicago
1.00 27 1.00
31
26 0.95
0.95 30
25 0.90 29
0.90
24 0.85 28
0.85 23 0.80 27
22 26
0.80 0.75
21 25
0.70
0.02 0.04 0.06 0.08 0.10 0.80 0.85 0.90 0.95 1.00 0.02 0.04 0.06 0.08 0.10 0.75 0.80 0.85 0.90 0.95 1.00
Coverage Coverage
CIA Group Normal Bonferroni CIA Group Normal Bonferroni
Figure 3. Results for constructing prediction sets for subsets with overlaps (Section
7.2). CIA achieves close to 1−α coverage and optimal efficiency on two road traffic
datasets.
affectthevalidity. ThisissueismuchlessobviousinalargertrafficnetworklikeChicago. Recallthatthe
group-sample conformal method is proposed in Section 3. This is also a conformal prediction method,
so its validity is also close to the desired line 1−α in the results of Figure 2. However, the validity
is much less reliable in the results of Figure 3. This is because edge weights from random samples are
not exchangeable with the edge weights on the shortest path. The method of Bonferroni correction has
good validity in the example of the community dataset. The reason is that under the current setting of
calibration and test set ratio, each subset contains one or two samples, which is very close to the case
egarevoC
egarevoC
egarevoC
eziS
eziS
eziS
egarevoC
eziS
egarevoC
egarevoC
egarevoC
eziS
eziS
eziS10 CONFORMALIZED INTERVAL ARITHMETIC WITH SYMMETRIC CALIBRATION
of classical conformal prediction. In other datasets, Bonferroni correction has too high coverage. For
normal confidence intervals, since the assumption is nottrue in general, the coverage is much lower than
the desired one. In conclusion, our proposed method has the most reliable validity in all datasets that
we have studied.
Underthechoicesofαontheleftplots,therightplotscomparethecoverageversusthepredictionset
size. The lower the curve, the more efficient and informative the prediction set provided by the method.
IntheplotsofFigure2,CIAisthemostefficientinalldatasets. IntheplotsofFigure3,CIAhassimilar
efficiency to the Group and Normal methods. In conclusion, CIA is the most efficient method with valid
coverage.
8. Conclusion
In this paper, we have introduced conformalized interval arithmetic with symmetric calibration, to
tackle the challenges of conformal prediction for interdependent data. Our approach offers method-
ological contributions and theoretical guarantees for achieving the desired coverage under reasonable
exchangeability assumptions. Extensive experiments on real-world datasets demonstrate the validity
and efficiency of our method, outperforming others even when they fail to ensure valid coverage.
Future works include using novel conformal score functions [20, 22, 23] and weighted aggregation [24]
to improve the efficiency. We hope our core ideas and techniques will foster the development of more
robust conformal prediction methods across various domains.
References
[1] AfHRaQ. Medical expenditure panel survey. https://meps.ahrq.gov/mepsweb/data_stats/data_overview.jsp,
2021.
[2] Angelopoulos, A. N., Bates, S., Fisch, A., Lei, L., and Schuster, T.Conformalriskcontrol.InThe Twelfth
International Conference on Learning Representations (2024).
[3] Bar-Gera, H., Stabler, B., and Sall, E. Transportation networks for research core team. Transportation Net-
work Test Problems. Available online: https://github.com/bstabler/TransportationNetworks (accessed on 10
September 2023) (2023).
[4] Bashari, M., Epstein, A., Romano, Y., and Sesia, M. Derandomized novelty detection with fdr control via
conformale-values.Advances in Neural Information Processing Systems 36 (2024).
[5] Bates, S., Candès, E., Lei, L., Romano, Y., and Sesia, M. Testing for outliers with conformal p-values. The
Annals of Statistics 51,1(2023),149–178.
[6] Chassein, A., Dokka, T., and Goerigk, M.Algorithmsanduncertaintysetsfordata-drivenrobustshortestpath
problems.European Journal of Operational Research 274,2(2019),671–686.
[7] Cleaveland, M., Lee, I., Pappas, G. J., and Lindemann, L. Conformal prediction regions for time series using
linearcomplementarityprogramming.InProceedingsoftheAAAIConferenceonArtificialIntelligence(2024),vol.38,
pp.20984–20992.
[8] Fanaee-T, H. Bike Sharing Dataset. UCI Machine Learning Repository, 2013. DOI:
https://doi.org/10.24432/C5W894.
[9] Feldman, S., Bates, S., and Romano, Y. Improving conditional coverage via orthogonal quantile regression.
Advances in neural information processing systems 34 (2021),2060–2071.
[10] Fisch,A.,Schuster,T.,Jaakkola,T.S.,andBarzilay,R.Efficientconformalpredictionviacascadedinference
withexpandedadmission.InInternational Conference on Learning Representations (2021).
[11] Huang, K., Jin, Y., Candes, E., and Leskovec, J. Uncertainty quantification over graph with conformalized
graphneuralnetworks.Advances in Neural Information Processing Systems 36 (2024).
[12] Jia,J.,andBenson,A.R.Residualcorrelationingraphneuralnetworkregression.InProceedingsofthe26thACM
SIGKDD international conference on knowledge discovery & data mining (2020),pp.588–598.
[13] Jin, Y., and Candès, E. J.Selectionbypredictionwithconformalp-values.JournalofMachineLearningResearch
24,244(2023),1–41.
[14] Johnstone, C., and Cox, B. Conformal uncertainty sets for robust optimization. In Conformal and Probabilistic
Prediction and Applications (2021),PMLR,pp.72–90.
[15] Kish,L.,andFrankel,M.R.Balancedrepeatedreplicationsforstandarderrors.JournaloftheAmericanStatistical
Association 65,331(1970),1071–1094.
[16] Lei, J., G’Sell, M., Rinaldo, A., Tibshirani, R. J., and Wasserman, L.Distribution-freepredictiveinference
forregression.Journal of the American Statistical Association 113,523(2018),1094–1111.
[17] Liu, M., Ding, L., Yu, D., Liu, W., Kong, L., and Jiang, B. Conformalized fairness via quantile regression.
Advances in Neural Information Processing Systems 35 (2022),11561–11572.
[18] Lu,C.,Lemay,A.,Chang,K.,Höbel,K.,andKalpathy-Cramer,J.Fairconformalpredictorsforapplications
inmedicalimaging.InProceedingsoftheAAAIConferenceonArtificialIntelligence(2022),vol.36,pp.12008–12016.
[19] Luo, R., and Colombo, N. Conformal load prediction with transductive graph autoencoders. arXiv preprint
arXiv:2406.08281(2024).
[20] Luo, R., and Colombo, N.Entropyreweightedconformalclassification.arXiv preprint arXiv:2407.17377(2024).
[21] Luo,R.,Nettasinghe,B.,andKrishnamurthy,V.Anomalousedgedetectioninedgeexchangeablesocialnetwork
models.InConformal and Probabilistic Prediction with Applications (2023),PMLR,pp.287–310.CONFORMALIZED INTERVAL ARITHMETIC WITH SYMMETRIC CALIBRATION 11
[22] Luo, R., and Zhou, Z. Conformal thresholded intervals for efficient regression. arXiv preprint arXiv:2407.14495
(2024).
[23] Luo, R., and Zhou, Z. Trustworthy classification through rank-based conformal prediction sets. arXiv preprint
arXiv:2407.04407(2024).
[24] Luo,R.,andZhou,Z.Weightedaggregationofconformityscoresforclassification.arXivpreprintarXiv:2407.10230
(2024).
[25] Melki, P., Bombrun, L., Diallo, B., Dias, J., and Da Costa, J.-P. Group-conditional conformal prediction
via quantile regression calibration for crop and weed classification. In Proceedings of the IEEE/CVF International
Conference on Computer Vision (2023),pp.614–623.
[26] Patel, Y. P., Rayan, S., and Tewari, A.Conformalcontextualrobustoptimization.InInternational Conference
on Artificial Intelligence and Statistics (2024),PMLR,pp.2485–2493.
[27] Redmond, M. Communities and Crime. UCI Machine Learning Repository, 2009. DOI:
https://doi.org/10.24432/C53W3X.
[28] Romano, Y., Barber, R. F., Sabatti, C., and Candès, E.Withmalicetowardnone: Assessinguncertaintyvia
equalizedcoverage.Harvard Data Science Review 2,2(2020),4.
[29] Romano, Y., Patterson, E., and Candes, E.Conformalizedquantileregression.Advances in neural information
processing systems 32 (2019).
[30] Sesia, M., and Candès, E. J.Acomparisonofsomeconformalquantileregressionmethods.Stat9,1(2020),e261.
[31] Sesia, M., and Romano, Y. Conformal prediction using conditional histograms. Advances in Neural Information
Processing Systems 34 (2021),6304–6315.
[32] Shao, J., and Wu, C.Asymptoticpropertiesofthebalancedrepeatedreplicationmethodforsamplequantiles.The
Annals of Statistics (1992),1571–1593.
[33] Stanton, S., Maddox, W., and Wilson, A. G.Bayesianoptimizationwithconformalpredictionsets.InInterna-
tional Conference on Artificial Intelligence and Statistics (2023),PMLR,pp.959–986.
[34] Sun, C., Liu, L., and Li, X.Predict-then-calibrate: Anewperspectiveofrobustcontextuallp.AdvancesinNeural
Information Processing Systems 36 (2023),17713–17741.
[35] Vovk, V., Gammerman, A., and Shafer, G.Algorithmic learning in a random world,vol.29.Springer,2005.
[36] Wang,F.,Cheng,L.,Guo,R.,Liu,K.,andYu,P.S.Equalopportunityofcoverageinfairregression.Advances
in Neural Information Processing Systems 36 (2024).
Appendix A. Proof of Theorem 1
We first consider a fixed permutation and a fixed partition of calibration and test set. Let rcal be
K+1
the rank of scal in scal for k ∈ [K +1]. We exchange the calibration set and test set in S , i.e.,
K+1 k K+1
we swap Stest and Scal . Under the symmetry assumption on the calibration indices and test indices,
K+1 K+1
this swapping procedure corresponds two possible partitions with the same probability. Let rtest be the
K+1
rank of stest in {scal :k =1,...,K}∪{stest } after swapping. Under the assumption of Theorem 1, the
K+1 k K+1
scores of the calibration set remains the same. Hence rtest =rcal . rtest has the same distribution as
K+1 K+1 K+1
rcal . Hence,
K+1
P(stest
≤Q
)=P(rtest
≤⌈(1+K)(1−α)⌉)
K+1 1−α K+1
=P(rcal
≤⌈(1+K)(1−α)⌉)
K+1
≥1−α.
The event rtest ≤⌈(1+K)(1−α) is equivalent to (cid:80) y ∈C Stest ) using the definition of Q .
K+1 i∈Stest i ( K+1 1−α
K+1
This completes the proof.
Appendix B. Proof of Theorem 2
The assumption of the theorem means there are most (K +1)δ many subsets overlaps with S .
K+1
When swapping the Scal and Stest , it affects at most (K +1)δ many scores in scal,k ∈ [K]. Thus,
K+1 K+1 k
rtest ≤rcal +(K+1)δ.
K+1 K+1
P(stest
≤Q )
K+1 1−α
=P(rtest
≤⌈(1+K)(1−α)⌉)
K+1
≥P(rcal
+(K+1)δ ≤⌈(1+K)(1−α)⌉)
K+1
=P(rcal
≤⌈(1+K)(1−α−δ)⌉)
K+1
≥1−α−δ.
The proof is complete.12 CONFORMALIZED INTERVAL ARITHMETIC WITH SYMMETRIC CALIBRATION
Anaheim Chicago
0.020
0.030
0.015 0.025
0.020
0.010
0.015
0.010
0.005
0.005
0.000 0.000
0.009 0.010 0.011 0.012 0.013 0.014 0.006 0.008 0.010
CIA (Split) CIA (CQR) CIA (Split) Stratified CIA (CQR) Stratified
Figure 4. The figure illustrates the positive correlation between the subsets overlap
ratio δ and the coverage gap (the difference between the coverage probability and the
desired 1−α coverage).
Appendix C. Analysis of CIA on Overlapping Subsets
In the main text, we have demonstrated the extension of the proposed CIA method to overlapping
subsets with an acceptable compromise on the coverage. In particular, Theorem 2 shows that as long as
eachsubsetoverlapswithonlyasmallportionoftheothersubsets,theeffectonthecoverageprobability
is acceptable.
Here, we conduct an extensive analysis to further investigate the performance of CIA on overlap-
ping subsets. We vary the degree of overlap between subsets and evaluate the impact on the coverage
probability and interval width.
ForbothAnaheimandChicagodatasets,theshortestpathsrepresentthesubsetsofedgescorrespond-
ing to individual road segments. The overlap ratio of the subsets is computed as follows:
δ =
1 (cid:88) |S k∩S ℓ|
,
(cid:0)K+1(cid:1) |S ∪S |
k ℓ
2 k̸=ℓ,k,ℓ∈[K+1]
which serves as an averaged version of the δ defined in Theorem 2 of the main text for easier numerical
analysis. Theterm |Sk∩Sℓ| representstheoverlapratiobetweensubsetsS andS , calculatedasthesize
|Sk∪Sℓ| k ℓ
of their intersection divided by the size of their union.
We alternate the subsets overlap ratio by setting varying lower bounds on the length of the shortest
paths. Withincreasinglowerboundsonthelengthoftheshortestpaths, theretendstobeanincreasing
overlap between different shortest paths. In Figure 4, we show there is a positive correlation between
the subsets overlap ratio δ and the coverage gap, i.e., the difference between the coverage probability
and the desired 1−α coverage. We demonstrate the results for all four variants of the proposed CIA
method.
The analysis reveals that CIA maintains good coverage probability even when there is moderate
overlap between subsets. The interval width remains relatively stable across different overlap scenarios,
indicating the robustness of CIA in handling overlapping subsets.
Overall, the experimental results support the theoretical findings and demonstrate the effectiveness
of CIA in providing valid confidence intervals for overlapping subsets.
Appendix D. Summary of Methods and Results
Due to space limitations, we present one variant of the proposed CIA method using CQR to compute
scores, along with three representative baseline methods in the main text. Here, we provide a compre-
hensive summary of the various CIA method variants and additional baseline methods for comparison.
D.1. Methods.
D.1.1. CIA Variants.
(1) CIA (Split) Nonstratified. This variant of CIA uses a split conformal prediction approach
without stratification. The score function is
(cid:12) (cid:12)
(cid:12) (cid:12)
cal (cid:12) (cid:88) (cid:12)
(8) s
k
:=(cid:12)
(cid:12)
y i−y (cid:98)i(cid:12) (cid:12),k ∈[K].
(cid:12)i∈Scal (cid:12)
k
pag
egarevoC
pag
egarevoCCONFORMALIZED INTERVAL ARITHMETIC WITH SYMMETRIC CALIBRATION 13
(2) CIA (CQR) Nonstratified. This variant of CIA employs conformal quantile regression (CQR)
without stratification. The score function is
 
(9) scal =max (cid:88) yα/2−y , (cid:88) y −y1−α/2 ,k ∈[K].
k (cid:98)i i i (cid:98)i
 
i∈Scal i∈Scal
k k
(3) CIA (Split) Stratified. This variant of CIA utilizes the same score function (8) as the nonstrat-
ified version. It stratifies the calibration set based on the number of samples in each group |Scal|
k
when computing the quantile of the scores to determine the prediction set threshold.
(4) CIA(CQR)Stratified. ThisCIAvariantisastratifiedversionwhichusesthesamescorefunction
as in (9).
D.1.2. Baseline Using Conformal Prediction.
(1) Group Sampling Conformal Prediction (Split). Thisbaselinemethodusesconformalpredic-
tion, sampling groups of size |Stest | without replacement, as described in Section 7.3 in the main
K+1
text. It employs the same score function as (8).
(2) Group Sampling Conformal Prediction (CQR). This baseline method combines the CQR
score function (9) with group sampling.
(3) Bonferroni Correction (Split). This baseline method applies Bonferroni correction to the split
conformalpredictionintervalswiththesamescorefunction(8). Thecorrectedintervalsaccountfor
varying sizes of |Stest|.
k
(4) Bonferroni Correction (CQR).ThisbaselinemethodappliesBonferronicorrectiontotheCQR
intervals computed with score function (9).
D.1.3. Baseline Using Normal Approximation.
(1) Normal Confidence Interval with Homoscedasticity Assumption. This baseline method,
introducedinSection7.3inthemaintext,constructsconfidenceintervalsassumingani.i.d. normal
distribution N(0,σ2) for all y −y . The predicted common variance is estimated as
(cid:98)i i
1 (cid:88)
σ2 = (y −y )2.
(cid:98) |Ical|−1 (cid:98)i i
i∈Ical
(2) Normal Confidence Interval Using IQR.Thisbaselinemethodconstructsconfidenceintervals
assuming independentzero-mean normaldistributions withvaryingstandard deviationsfor y −y .
(cid:98)i i
The standard deviation is estimated using the interquartile range (IQR) for each sample, predicted
via a quantile regression model:
q (x )−q (x )
σ = (cid:98)0.75 i (cid:98)0.25 i ,
(cid:98)i z −z
0.75 0.25
where z is the β quantile of the standard normal distribution. This formulation enables het-
β
eroscedasticity.
D.2. Results. Wepresentthecomparisonresultsoftheaforementionedmethodsacrossvariousdatasets
as follows. The results demonstrate the performance of each method in terms of coverage probability
and set size.14 CONFORMALIZED INTERVAL ARITHMETIC WITH SYMMETRIC CALIBRATION
bike meps21
1.00 35 1.0 700
0.98 30 600
0.9
0.96 25 500
0.94 20 0.8 400
0.92 15 0.7 300
0.90
10 200
0.88 0.6
5 100
0.86
0 0.5 0
0.02 0.04 0.06 0.08 0.10 0.88 0.90 0.92 0.94 0.96 0.98 1.00 0.02 0.04 0.06 0.08 0.10 0.6 0.7 0.8 0.9 1.0
Coverage Coverage
CIA (Split) Group (Split) Normal (Homo) CIA (Split) Group (Split) Normal (Homo)
CIA (CQR) Group (CQR) Bonferroni (Split) CIA (CQR) Group (CQR) Bonferroni (Split)
CIA (Split) Stratified Normal (Hetero) Bonferroni (CQR) CIA (Split) Stratified Normal (Hetero) Bonferroni (CQR)
CIA (CQR) Stratified CIA (CQR) Stratified
community Anaheim
1.00 1.00 30
6
0.95 0.95 28
5
0.90 0.90 26
4
0.85
3 0.85 24
0.80
2 0.80 22
0.75
0.02 0.04 0.06 0.08 0.10 0.80 0.85 0.90 0.95 1.00 0.02 0.04 0.06 0.08 0.10 0.80 0.85 0.90 0.95 1.00
Coverage Coverage
CIA (Split) Group (Split) Normal (Homo) CIA (Split) Group (Split) Normal (Homo)
CIA (CQR) Group (CQR) Bonferroni (Split) CIA (CQR) Group (CQR) Bonferroni (Split)
CIA (Split) Stratified Normal (Hetero) Bonferroni (CQR) CIA (Split) Stratified Normal (Hetero) Bonferroni (CQR)
CIA (CQR) Stratified CIA (CQR) Stratified
meps19 Chicago
1.0 1.00 36
600 0.95 34
0.9
500
0.90 32
0.8 400
0.85 30
300
0.7
200 0.80 28
0.6 100 0.75 26
0.5 0 0.70
0.02 0.04 0.06 0.08 0.10 0.6 0.7 0.8 0.9 1.0 0.02 0.04 0.06 0.08 0.10 0.75 0.80 0.85 0.90 0.95 1.00
Coverage Coverage
CIA (Split) Group (Split) Normal (Homo) CIA (Split) Group (Split) Normal (Homo)
CIA (CQR) Group (CQR) Bonferroni (Split) CIA (CQR) Group (CQR) Bonferroni (Split)
CIA (Split) Stratified Normal (Hetero) Bonferroni (CQR) CIA (Split) Stratified Normal (Hetero) Bonferroni (CQR)
CIA (CQR) Stratified CIA (CQR) Stratified
meps20
1.0
800
0.9
600
0.8
400
0.7
200
0.6
0.5 0
0.02 0.04 0.06 0.08 0.10 0.6 0.7 0.8 0.9 1.0
Coverage
CIA (Split) Group (Split) Normal (Homo)
CIA (CQR) Group (CQR) Bonferroni (Split)
CIA (Split) Stratified Normal (Hetero) Bonferroni (CQR)
CIA (CQR) Stratified
Figure5. Completeresultsforpredictionsets. AllCIAvariantsachieve1−αcoverage,
with CIA (CQR) Stratified showing optimal efficiency.
egarevoC
egarevoC
egarevoC
egarevoC
eziS
eziS
eziS
eziS
egarevoC
egarevoC
egarevoC
eziS
eziS
eziSCONFORMALIZED INTERVAL ARITHMETIC WITH SYMMETRIC CALIBRATION 15
Table 1. Coverage and Size Results for Different Methods Across Datasets at α=0.1
CIA(Split) CIA(CQR) CIA(Split)Stratified CIA(CQR)Stratified Group(Split)
Dataset Coverage Size Coverage Size Coverage Size Coverage Size Coverage Size
Anaheim 0.89(0.04) 23.86(0.78) 0.89(0.04) 22.31(1.04) 0.90(0.04) 23.71(0.97) 0.89(0.04) 22.57(1.16) 0.83(0.04) 22.38(0.90)
bike 0.90(0.02) 3.89(0.41) 0.90(0.03) 5.56(0.16) 0.90(0.03) 4.00(0.40) 0.90(0.03) 0.42(0.19) 0.89(0.02) 3.28(0.18)
Chicago 0.90(0.04) 28.80(1.04) 0.90(0.03) 26.60(1.49) 0.90(0.03) 27.40(1.47) 0.90(0.03) 26.67(1.55) 0.75(0.04) 25.76(1.35)
community 0.90(0.03) 2.53(0.18) 0.90(0.03) 2.27(0.17) 0.90(0.02) 2.63(0.19) 0.90(0.03) 1.48(0.20) 0.88(0.04) 2.47(0.30)
meps_19 0.90(0.02) 22.87(2.61) 0.90(0.02) 22.76(2.94) 0.90(0.02) 25.83(3.35) 0.90(0.03) 20.83(4.00) 0.86(0.03) 23.33(2.90)
meps_20 0.89(0.02) 28.75(3.45) 0.90(0.02) 33.11(3.51) 0.90(0.02) 33.83(4.88) 0.90(0.02) 30.56(4.78) 0.86(0.03) 31.61(4.04)
meps_21 0.90(0.02) 21.39(2.57) 0.90(0.02) 22.61(2.82) 0.90(0.02) 25.06(3.18) 0.90(0.02) 20.50(3.48) 0.87(0.02) 24.19(3.22)
Group(CQR) Normal(Hetero) Normal(Homo) Bonferroni(Split) Bonferroni(CQR)
Dataset Coverage Size Coverage Size Coverage Size Coverage Size Coverage Size
Anaheim 0.88(0.03) 21.98(0.91) 0.80(0.03) 20.84(0.82) 0.83(0.03) 21.19(0.81) 0.99(0.01) 26.69(1.17) 0.98(0.01) 25.79(1.13)
bike 0.90(0.02) 4.13(0.19) 0.87(0.02) 4.37(0.09) 0.89(0.02) 3.18(0.09) 0.96(0.01) 25.84(1.06) 0.96(0.01) 24.27(0.94)
Chicago 0.79(0.04) 25.16(1.35) 0.74(0.03) 24.64(1.31) 0.76(0.03) 24.72(1.31) 0.99(0.00) 31.62(1.95) 0.97(0.01) 29.22(1.78)
community 0.89(0.03) 2.20(0.27) 0.76(0.02) 1.81(0.03) 0.88(0.02) 2.36(0.12) 0.91(0.02) 3.46(0.22) 0.91(0.02) 3.04(0.25)
meps19 0.88(0.02) 25.67(2.84) 0.54(0.02) 6.42(0.08) 0.92(0.02) 28.01(2.32) 0.95(0.01) 250.03(30.56) 0.96(0.01) 240.15(31.88)
meps20 0.88(0.02) 35.55(3.67) 0.54(0.02) 7.44(0.08) 0.91(0.02) 36.45(2.59) 0.96(0.01) 371.84(45.07) 0.97(0.01) 361.34(44.02)
meps21 0.89(0.02) 25.40(2.67) 0.53(0.02) 6.15(0.06) 0.92(0.02) 29.99(2.44) 0.96(0.01) 254.63(31.54) 0.96(0.01) 224.75(31.47)
Table 2. CoverageandSizeResultsforDifferentMethodsAcrossDatasetsatα=0.05
CIA(Split) CIA(CQR) CIA(Split)Stratified CIA(CQR)Stratified Group(Split)
Dataset Coverage Size Coverage Size Coverage Size Coverage Size Coverage Size
Anaheim 0.94(0.03) 25.48(0.85) 0.94(0.03) 23.65(1.08) 0.94(0.03) 24.89(0.98) 0.94(0.02) 23.92(1.10) 0.88(0.03) 23.25(0.90)
bike 0.95(0.02) 6.52(0.92) 0.95(0.02) 6.04(0.18) 0.95(0.02) 5.24(0.47) 0.95(0.02) 1.09(0.21) 0.94(0.02) 4.25(0.23)
Chicago 0.95(0.03) 30.40(1.06) 0.95(0.02) 27.39(1.46) 0.95(0.02) 28.14(1.53) 0.95(0.02) 27.40(1.53) 0.84(0.04) 26.57(1.36)
community 0.95(0.02) 3.38(0.33) 0.95(0.02) 2.98(0.26) 0.95(0.02) 3.44(0.24) 0.95(0.02) 2.21(0.26) 0.94(0.03) 3.27(0.45)
meps_19 0.95(0.02) 42.86(6.70) 0.95(0.02) 44.27(8.25) 0.95(0.02) 49.28(6.82) 0.95(0.02) 45.37(6.96) 0.92(0.02) 40.41(5.42)
meps_20 0.95(0.02) 61.36(10.31) 0.95(0.02) 63.27(8.80) 0.95(0.02) 68.88(8.32) 0.95(0.02) 61.65(7.37) 0.92(0.02) 55.06(6.05)
meps_21 0.95(0.02) 40.78(5.89) 0.95(0.02) 44.86(7.31) 0.95(0.02) 49.85(7.13) 0.95(0.02) 46.45(7.08) 0.93(0.02) 42.24(5.38)
Group(CQR) Normal(Hetero) Normal(Homo) Bonferroni(Split) Bonferroni(CQR)
Dataset Coverage Size Coverage Size Coverage Size Coverage Size Coverage Size
Anaheim 0.92(0.02) 22.85(0.91) 0.86(0.03) 21.76(0.84) 0.90(0.02) 22.21(0.82) 0.99(0.00) 27.59(1.30) 0.99(0.01) 26.24(1.16)
bike 0.95(0.02) 5.06(0.21) 0.90(0.02) 5.21(0.11) 0.93(0.01) 3.79(0.10) 0.99(0.01) 28.90(1.21) 0.99(0.00) 26.89(1.09)
Chicago 0.86(0.03) 25.92(1.39) 0.82(0.03) 25.60(1.34) 0.84(0.03) 25.71(1.33) 1.00(0.00) 32.74(2.02) 0.99(0.01) 30.06(1.81)
community 0.94(0.03) 2.96(0.36) 0.81(0.02) 2.15(0.03) 0.92(0.02) 2.81(0.14) 0.95(0.02) 4.44(0.33) 0.95(0.02) 3.89(0.27)
meps_19 0.93(0.02) 43.71(5.20) 0.60(0.02) 7.66(0.09) 0.93(0.01) 33.38(2.76) 0.98(0.01) 389.89(40.47) 0.98(0.01) 389.91(42.76)
meps_20 0.93(0.02) 58.36(6.39) 0.60(0.02) 8.86(0.10) 0.93(0.01) 43.43(3.09) 0.98(0.01) 578.71(54.66) 0.98(0.01) 563.85(50.91)
meps_21 0.93(0.02) 43.32(5.23) 0.60(0.02) 7.33(0.07) 0.94(0.01) 35.73(2.90) 0.98(0.01) 400.18(42.43) 0.98(0.01) 370.19(44.47)
Table 3. CoverageandSizeResultsforDifferentMethodsAcrossDatasetsatα=0.01
CIA(Split) CIA(CQR) CIA(Split)Stratified CIA(CQR)Stratified Group(Split)
Dataset Coverage Size Coverage Size Coverage Size Coverage Size Coverage Size
Anaheim 0.99(0.01) 29.68(1.95) 0.99(0.01) 27.11(1.61) 0.98(0.02) 26.22(1.04) 0.98(0.01) 25.31(1.06) 0.95(0.02) 25.03(1.08)
bike 0.99(0.01) 13.83(2.03) 0.99(0.01) 7.40(0.63) 0.98(0.01) 7.04(0.76) 0.98(0.01) 2.18(0.47) 0.99(0.01) 6.63(0.39)
Chicago 0.99(0.01) 33.53(1.14) 0.99(0.01) 29.03(1.49) 0.99(0.01) 29.54(1.57) 0.99(0.01) 28.78(1.54) 0.95(0.02) 28.50(1.58)
community 0.99(0.01) 5.19(0.60) 0.99(0.01) 4.89(1.07) 0.99(0.01) 4.85(0.37) 0.99(0.01) 4.02(0.79) 0.99(0.01) 5.37(0.77)
meps_19 0.99(0.01) 101.43(11.42) 0.99(0.01) 102.07(10.24) 0.98(0.01) 104.16(16.15) 0.98(0.01) 97.98(15.43) 0.98(0.01) 88.53(9.06)
meps_20 0.99(0.01) 128.68(16.50) 0.99(0.01) 119.37(14.50) 0.98(0.01) 124.92(16.46) 0.98(0.01) 104.97(11.72) 0.98(0.01) 114.05(9.08)
meps_21 0.99(0.01) 104.26(14.87) 0.99(0.01) 101.38(11.80) 0.98(0.01) 99.60(17.51) 0.98(0.01) 90.37(14.42) 0.98(0.01) 94.13(10.31)
Group(CQR) Normal(Hetero) Normal(Homo) Bonferroni(Split) Bonferroni(CQR)
Dataset Coverage Size Coverage Size Coverage Size Coverage Size Coverage Size
Anaheim 0.97(0.02) 24.39(1.01) 0.93(0.02) 23.49(0.88) 0.96(0.02) 24.13(0.86) 1.00(0.00) 29.88(1.78) 0.99(0.00) 26.97(1.20)
bike 0.99(0.01) 7.22(0.36) 0.93(0.02) 6.85(0.15) 0.97(0.01) 4.98(0.13) 1.00(0.00) 33.97(1.57) 1.00(0.00) 31.50(1.57)
Chicago 0.95(0.02) 27.48(1.48) 0.93(0.02) 27.41(1.39) 0.95(0.02) 27.57(1.39) 1.00(0.00) 35.64(2.38) 0.99(0.00) 31.57(1.87)
community 0.99(0.01) 5.42(1.30) 0.86(0.02) 2.83(0.04) 0.96(0.02) 3.69(0.18) 0.99(0.01) 6.48(0.50) 0.99(0.01) 5.93(0.89)
meps_19 0.98(0.01) 92.53(9.94) 0.69(0.02) 10.06(0.12) 0.95(0.01) 43.87(3.63) 0.99(0.01) 662.98(68.44) 0.99(0.01) 665.74(62.79)
meps_20 0.98(0.01) 113.42(9.68) 0.68(0.02) 11.65(0.13) 0.95(0.01) 57.07(4.06) 0.99(0.01) 892.21(54.55) 0.99(0.01) 856.86(52.43)
meps_21 0.98(0.01) 92.66(10.70) 0.70(0.02) 9.64(0.09) 0.95(0.01) 46.96(3.81) 0.99(0.01) 682.08(59.95) 0.99(0.01) 653.63(58.24)