AUDIOMATCHCUTTING:FINDINGANDCREATINGMATCHING
AUDIOTRANSITIONSINMOVIESANDVIDEOS
DennisFedorishin1,2⋆,LieLu1,SrirangarajSetlur2,VenuGovindaraju2
1DolbyLaboratories,2UniversityatBuffalo
ABSTRACT
A “match cut” is a common video editing technique where a
pairofshotsthathaveasimilarcompositiontransitionfluidlyfrom
onetoanother. Althoughmatchcutsareoftenvisual,certainmatch
cuts involve the fluid transition of audio, where sounds from dif-
ferent sources merge into one indistinguishable transition between
twoshots. Inthispaper,weexploretheabilitytoautomaticallyfind
and create “audio match cuts” within videos and movies. We cre-
ate a self-supervised audio representation for audio match cutting
anddevelopacoarse-to-fineaudiomatchpipelinethatrecommends
matchingshotsandcreatestheblendedaudio.Wefurtherannotatea
datasetfortheproposedaudiomatchcuttaskandcomparetheability
ofmultipleaudiorepresentationstofindaudiomatchcutcandidates.
Finally,weevaluatemultiplemethodstoblendtwomatchingaudio
candidateswiththegoalofcreatingasmoothtransition.Projectpage
andexamplesareavailableat:https://denfed.github.io/
audiomatchcut/
Fig.1. Examplematchcutsinmovies. In2001: ASpaceOdyssey
Index Terms— Self-Supervised Learning, Match Cuts, Audio [12](top),twodifferentvisualstransitionfluidlybasedonthesimilar
Transitions,AudioRetrieval,SimilarityMatching sizeandshapeoftheobjects.InTheChroniclesofNarnia:TheLion,
the Witch and the Wardrobe [13] (bottom), The sound of a sword
1. INTRODUCTION clinkingwithinitssheathmatchedtothestrikeofahammerinthe
nextscene,creatingaseamlessaudiomatchacrossscenes.
Inmoviesandvideos,the“cut”isafoundationaleditingtechnique
thatisusedtotransitionfromonesceneorshottothenext[1]. The
ofediting. Recentworksfocusonimprovingtheunderstandingof
precise use of cuts often crafts the story being portrayed, whether
movies,fromdetectingeventsandobjectswithinthem,likespeakers
itcontrolspacing,highlightsemotions,orconnectsdisparatescenes
[6], video attributes like shot angles, sequences and locations [7],
intoacohesivestory[2]. Therearemanyvariationsofcutsthatare
andunderstandingvariouscuts[8]. Beyondunderstandingvideos,
usedacrossthefilmindustry,includingsmashcuts,reactioncuts,J-
full editing tools have been proposed including shot sequence or-
cuts,L-cuts,andothers. Onespecificcutisthe“matchcut”,which
dering[7],automaticscenecutting[9],trailergeneration[6],video
is a transition between a pair of shots that uses similar framing,
transition creation [10], and audio beat matching [11]. Recently,
composition, or action to fluidly bring the viewer from one scene
[5] proposed a framework to automatically find frame and motion
to thenext. Match cutsoften match visuals across eachscene, ei-
matchcutsinmovies.[5]collectsalarge-scaledatasetofmatchcuts
therthroughsimilarobjectsandtheirplacement,colors,orcamera
foundinmoviesandfurthertrainsaclassificationnetworktoretrieve
movements[3]. However,matchcutscanalsomatchsound across
match cut candidates, aiding video editors in finding and creating
scenes, wheresoundbetweentwoscenestransitionseamlesslybe-
thesematchcuts. However,[5]onlyfocusesonvisualmatchcuts.
tweeneachother.Theseaudiomatchcuts(alsoreferredtoas“sound
In our work, we expand upon this area and focus on the ability to
bridges”)eitherblendtogethersoundsorcarrysimilarsoundacross
automaticallyfindcreateaudiomatchcuts.
scenes, often from different sound sources, to create a fluid audio
At its core, creating audio match cuts involves retrieving can-
transitionbetweenthem[4].Figure1showsexamplesofvisualand
didate audio clips that are able to create high-quality match cuts.
audiomatchcutsfoundinmovies.
Retrieving similar audio samples have been explored in the music
Alongwithcutting,videoeditingasawholeisatime-consuming
domain with music information retrieval systems that retrieve full
processthatofteninvolvesateamofexperteditorstocreatehigh-
songs based on small song snippets, using signal processing tech-
qualityvideosandmovies. Whenperformingtaskslikematchcuts,
niques[14,15]andmorerecentlydeeplearning[16,17]. Similarly,
it often involves a manual search across a collection of recorded
performing audio transitions in the music domain is an often-used
content to find strong candidates to transition to, which becomes
technique, bothinmusicmixingandliveDJperformances. Inlit-
a time-consuming and tedious manual process [5]. As a result,
erature, both signal processing [18] and deep-learning-based [19]
AI-assisted video editing has emerged as a promising area of re-
techniqueshavebeenintroducedtoautomaticallycreatethesetran-
search,withthegoalofaidingeditorsimprovethespeedandquality
sitions. However, finding and creating matching audio transitions
⋆DoneduringinternshipatDolbyLaboratories has been unexplored in the context of movies and videos across a
4202
guA
02
]DS.sc[
1v89901.8042:viXraFig.2. a)ProposedFramework. Givenaqueryvideo,weretrieveanaudiomatchcutcandidatefromavideogalleryandfindtheoptimal
transition point using a sub-spectrogram similarity search. Using the variance of the created similarity matrix, we adaptively select the
crossfadelengthtoblendboththequeryandmatchaudiointoafluidaudiomatchcut.b)Proposed“Split-and-Contrast”contrastiveobjective.
Eachaudiosampleissplitatarandomlyselectedframe,thentheadjacentframesofthesplitarecontrastedtowardseachother.
diversesetofsoundsbeyondmusic. Weselectedtoperformretrievalover1-secondpairstobalancebe-
In this paper, we explore this problem and propose a self- tweengranularityandsearchcomplexity.
supervised retrieval-and-transition framework, shown in Figure 2, Next, we collect a query set of samples of a variety of natu-
toautomaticallyfindandcreatehigh-qualityaudiomatchcuts. Our ralsoundsandsoundeffects,includingsoundslikeenginesrevving,
contributionsinthispaperareasfollows: impulsivesoundslikeahammerstriking,doorbells,campfires,and
otheruniquesoundsseeninvideosandmovies. Foreachquery,we
• Weintroducetheproblemofautomaticaudiomatchcutgen-
labelasetofmatchcandidatesbasedontwocriteriathatconstitute
erationacrossdiversesoundsandcreatetwodatasetsforeval-
apositiveaudiomatch: i)thepairmustsoundplausibleiftheaudio
uatingautomaticaudiomatchcuttingmethods.
isswappedbetweenthequeryandmatchimages. ii)thepairmust
• We propose a framework where a coarse-to-fine audio re- soundperceptuallysimilarintermsofpitch,rhythm,timbre,etc.
trievalpipelinefirstrecommendsmatchedclips,thenafine- Aslabelingrandompairsacrossallsamplesresultsinanunfea-
grainedtransitionmethodcreatesaudiomatchcutsthatout- siblesearchspace(over4trillionpairs),weuseexistingaudiorepre-
performmultiplebaselines. sentationstohelpgeneratecandidateaudiomatches.Wehypothesize
thatsincethemaincharacteristicofaudiomatchcutsisthattheau-
2. METHOD dioof bothscenes areperceptually-similar, widely-availableaudio
representationsmaybeusedastheyoftenaretrainedwiththegoal
2.1. ProblemDefinition
of similar audio samples having high similarity. We use two sim-
Tomodelthereal-worldtaskofcreatingaudiomatchcuts,wefor- plerepresentations,theMFCCandMel-Spectrogram,andtwodeep
mulate our proposed audio match cut problem as a unimodal au- representations,theaudioencodersfromCLAP[21]andImageBind
dio retrieval task. Specifically, given a query video clip V q and a [22]. ForMFCCandMel-Spectrogram, weuseawindowof2048
collection of n other video clips G = {i 1,i 2,...,i n}, the goal is samplesandhoplengthof1024samples. Weflattenbothrepresen-
toretrieveavideoclipG i andcreateanaudiotransitionsuchthat tationsalongthetimestepsandusetheresultingfeaturevectorsfor
V q ⇒ G i creates a high-quality audio match cut. We formulate retrieval. ForCLAP[21]andImageBind[22],weusetheirrespec-
the retrieval as a maximum inner-product search (MIPS) between tivespectrogramgenerationparametersandusetheresultingaudio
extracted L2 normalized feature representations of the query and encoderfeaturevectorsforretrieval.WeusetheMIPSoperationde-
a gallery of the audio of video clips, z Vq,z Gi ∈ Zd, denoted by scribedinSection2.1tocreatetheaudiomatchcutcandidatesfor
z∗ = argmax (zT z ). Afterretrievingthetop-khighest-similar labeling.Allaudiousedinthisworkissampledat48kHz.
Gi i Vq Gi
gallery clips {G∗}k , we perform a processing operation f to Sinceweusedaudiorepresentationstocollectaudiomatchcan-
i i=1 p
blend the query and retrieved clips to create the final audio match didate pairs and label only those pairs, our evaluation set tends to
cuts{f (V ,G∗)}k ,whereauserselectswhichmatchcutstouse favorthehighest-similarcandidatesofeachrepresentation. Toad-
p q i i=1
outofkrecommendations. dressthisbiasandcreateamorecomprehensiveevaluation,weran-
domlysample100negativematchesforeachqueryintheAudioset
2.2. DataCollection andMovieclipsdataset. Byrandomlyselectingsamplesoutofmil-
lionsof1-secondsamples,thereisaveryunlikelychancethatthese
Astheaudiomatchcutproblemisunexplored,wedevelopedeval- samplesinfactarepositiveaudiomatches. TheresultingAudioset
uationsetsbasedonsubsetsofpubliclyavailabledatasets,Audioset evaluationsethasagalleryof12,350labeledsamplesspreadacross
[20]andMovieclips1,toevaluateaudiomatchcutgenerationmeth-
102queries,andtheMovieclipsevaluationsethasagalleryof8,289
ods. Audioset contains user-generated videos from YouTube and labeledsamplesspreadacross66queries.Eachqueryhasanaverage
Movieclipscontainshigh-qualitymoviesnippets. Foreachdataset, of123labeledsamplesand10positivematches.
wespliteachvideointo1-secondnon-overlappingimage-audiopairs
where the image is the middle frame of the respective second of 2.3. AudioMatchCutRepresentationLearning
video,resultinginover2MAudiosetand800kMovieclipssamples.
InSection2.2,weutilizeexistingaudiorepresentationstogenerate
1MovieclipsiscollectedfromtheMovieclipsYouTubeChannel audiomatchcutcandidates.However,existingaudiorepresentationsarenotdirectlyalignedfortheaudiomatchcuttask,whichaimsto
retrieve perceptually-similar audio from different scenes, differing
fromexistingretrievaltasks. Asaresult,existingaudiorepresenta-
tionsmayproducesub-optimalaudiomatchcutcandidates.
Lackinglabeleddataforaudiomatchcutting,weproposeaself-
supervisedlearningobjectivetocreateanaudiorepresentationthat
effectively retrieves high-quality audio match cut candidates. Our
objective leverages already-edited videos based on the notion that
given a query audio frame of a video, an audio frame that results
inahigh-qualitymatchcutisthenextsuccessiveframeinthesame
video, as the entire video has been previously edited to have con-
tinuousaudio.Wemodelthischaracteristicas“Split-and-Contrast”,
shown in Figure 2b, where adjacent audio frames in two splits of Fig.3. Examplesub-spectrogramsimilaritiesofaudiomatchcuts:
avideoaretrainedtohavehighsimilarity, whilecontrastingaway Aforginghammerstrikingmatchedwithaknifechopping(left)ex-
othernon-adjacentaudioframes. hibitshighsimilarityoneachstrikeoccurrence. Ablendermatched
GivenabatchofN audiosamplesthathavenaudioframes,for withamotorcyclerevving(right)showsasmoothersimilarityma-
everysample,weextractafeaturerepresentationzfromeachframe, trix,allowingforplausibletransitionsacrossmultipletimesteps.
{z }n ∈ Zd×n, where d is the feature representation size. We
k k=1
then randomly select an index to split the N sets of features into Since we perform retrieval using 1-second audio clips, the
left/rightsectionsz
α
∈ Zd×nα andz
β
∈ Zd×nβ,oflengthn
α
and matchedclipsmaybeoverallstrongcandidatesforanaudiomatch
n , suchthatn +n = n. Foreachleft/rightsectioninN, we cut,buttheexactbordersofeachstillmaynotalignwellforadirect
β α β
denotetheadjacentframesasz = z andz = z , corre- transition. Therefore, we propose an operation to find an optimal
spondingtothelastframeinthekl leftseα cn tiα on,andk fir rstfraβ m0 einthe transition point between the query and matched clip at the spec-
right section, respectively. Then, we define a contrastive learning trogramtime-step-level,named“MaxSub-Spectrogram”similarity
formulationforabatchofN samplestolearnarepresentationthat search,showninFigure2a.
produceshighsimilarityforonlytheadjacentframesinthesplitsec- Given a Mel-spectrogram representation of the query audio
tions,andlowsimilarityforallotherpairs: S Q ∈ Rf×t and matched audio S M ∈ Rf×t, where f and t de-
note the frequency bins and time steps, respectively, we calculate
(cid:32) (cid:80)N exp(zTz /τ) (cid:33) theinnerproductoftwospectrogramsacrosstimesteps,yieldinga
L S&C(N)=−log
(cid:80)N i=·
1nαk (cid:80)=1
N j=·n
1βek xl p(k zr
αT iz βj/τ)
(1) s si im mi il la ar rit ty imm eat sr ti ex pM pai=
r,
S arQT gS
mM ax
i∈ ,j(R Mt× ).t. W We et hh ye pn ofi thn ed sit zh eeh thig athe ts ht e-
highest-similartimesteppairinMyieldsastrongpointtotransition
z aTz b denotestheinnerproductofL2 normalizedvectors, and betweenthequeryandmatchastheaudiospectraaremostaligned.
τ denotes a temperature parameter for softmax. This formulation Afterfindingthetransitionpoint,weperformacrossfadetofur-
issimilartoInfoNCE[23]andN-Pair[24]loss, modifiedtoallow ther blend together the query and match audio clip. However, as
multiplepositivesinasinglelosscomputation. Bymaximizingthe previouslymentioned,certaintypesofaudiomaybenefitfromdif-
similarityoftwoadjacentframesinasplitaudiosample,weexpect ferentlengthcrossfades[28]. Figure3showstwoexamplesofau-
themodeltolearntoretrieveperceptually-similaraudioframesthat dio matches that require different crossfades. When matching the
resultinhigh-qualitytransitions. WeusethepretrainedCLAP[21] strikesofahammerandknife,long-durationcrossfadesresultinthe
audioencoder,basedontheHTSAT[25]architecture,andtheCLAP impactsoverlappingeachotherandresultinginablurry,low-quality
[21]linearprojectionlayers. Wealsousethespectrogramcreation transition. Whenmatchingablenderandmotorcycle,theaudioex-
andpreprocessingstepsdefinedin[21], usinganaudioframesize hibitsmorenoisethroughoutthesample, thatbenefitsfromlonger
of1-second. Wefoundfine-tuningtheCLAP[21]projectionlayers crossfadesasbothnoisysamplesblendintoeachotherslowly.
withafrozenencoderworksbetterthanend-to-endfinetuning,sug- Wemodelthischaracteristicbasedonthevarianceofthespec-
gestingthat“Split-and-Contrast”isbettersuitedforaligningexisting trogram similarity matrix based on the hypothesis that audio pairs
audiofeaturerepresentationsfortheaudiomatchcuttask. that exhibit high variance in their similarity matrix (e.g. impul-
Wetraintheprojectionlayersusing200krandomAudiosetsam- sivesounds)requirelittle-to-nocrossfading, whileaudiopairsthat
plesfor20epochsusingtheAdam[26]optimizer,learningrateof exhibit low variance in their similarity matrix (e.g. noisy, static
10−4,batchsizeof2048,andtemperatureτof0.1.Eachsamplehas
sounds)benefitfromlongercrossfades,astheyhaveplausibletran-
ten1-secondaudioframes,suchthatn α+n β =10. sitionpointsacrossmultipletimesteps.Weusetheinverse-variance
ofthecomputedpairsimilaritymatrixtoadaptivelydeterminecross-
2.4. AudioTransition fadelength,named“AdaptiveCrossfade”:
O t oh un e se lc yc roo fasm dsfm ia ndo gen t, hm w ee h st eeh cro e od nto dhf e ct lfir ia prn s its ni a ,ti u ro edn sii o un lg tc il nb ip get ifw nade ae e sn s motw ou oto tw hau h td ri ali eo nsss iia tm im ou np ll t [e a 2s n 7ei ]s -
.
l crossfade = Var(1 M)ϕ;M = ||SS QQT i|i |S ||SM Mj j||∀i,j ∈{1,...,t} (2)
However,creatinghigh-qualitytransitionsusingcrossfadeoftenre- Here,ϕcontrolsthescalingoftherelationshipofthesimilarityma-
quires manual tuning of the crossfade length, based on the audio trixvariancetothecrossfadelength. Weuseavalueofϕ = 8. For
characteristics [28]. In this section, we describe our audio transi- thecrossfade,weuseasquare-rootwindowforfade-inandfade-out,
tionmethodthatimprovesuponsimplecrossfadebyi)firstfindinga withlengthandoverlapofl seconds. WeusethesameMel-
crossfade
specifictransitionpointwithinthe1-secondclip,andii)adaptively SpectrogramparametersdescribedinSection2.2. Noteweusedot
selectingamoreoptimalcrossfadelengthbasedontheaudiochar- producttofindthemostsimilartimesteppairandusecosinesim-
acteristics. ilarityincalculatingthematrixvariancetokeepvaluesboundedinRetrievalMethods Dataset R-mAP HR@1 HR@2 HR@5 P@5 P@10
Random .1093 .0392 .1373 .3235 .0804 .0794
MFCC .4111 .3725 .5196 .6961 .3510 .3206
Mel-Spectrogram .3318 .3529 .5392 .6569 .3157 .2882
AudioSet[20]
ImageBind[22] .5623 .6471 .7745 .9314 .5137 .4696
CLAP[21] .7225 .7843 .9314 .9608 .6765 .5990
Split-and-Contrast(Ours) .7656 .8333 .9608 .9804 .7216 .6069
Random .1176 .1061 .1364 .3030 .0727 .0742
MFCC .3266 .3636 .5000 .6667 .2576 .2197
Mel-Spectrogram .3337 .3485 .5606 .7273 .3758 .3258
MovieClips
ImageBind[22] .5209 .4697 .6212 .7576 .4939 .4955
CLAP[21] .7729 .7424 .8939 .9848 .7636 .7136
Split-and-Contrast(Ours) .7995 .8788 .9394 1.000 .7758 .7227
Table1.AudioretrievalresultsonthelabeledaudiomatchcutevaluationsetfromAudioSet[20]andMovieClips.
TransitionMethods TransitionScore(0-3) weseethatour“Split-and-Contrast”schemeoutperformsCLAP[21]
Concatenation 0.821 andallothermethodsacrossallretrievalmetrics,showingourself-
Crossfade(0.25s) 1.750 supervisedobjectiveiseffectiveforbetteraligningaudiorepresenta-
Crossfade(0.5s) 1.714 tionsfortheaudiomatchcuttask.
Max-Sub-Spectrogram(Max-SS)(Ours) 1.107
3.3. TransitionEvaluation
Max-SS+AdaptiveCrossfade(Ours) 2.143
To evaluate the quality of transition methods once an audio match
Table2.Transitionscoresonaudiotransitionmethods.
is retrieved, we score the transition quality of 27 Audioset and 41
a defined range. We found that using dot product takes the spec- Movieclipspositivematches. Table2showstheaveragetransition
trogrammagnitudeintoaccount(viaun-normalizedfeatures)andas scores for multiple baseline transition methods and our proposed
a result the transition point often occurs on time steps with higher method,withandwithoutcrossfading. Simpleconcatenationofthe
energies, likestrikesandimpactsratherthanquietportions, which queryandmatchaudiooftenresultsinartifactsandaudiblediscon-
alignswellwithmanyreal-worldaudiomatchcuts. tinuities, which degrade the transition quality as the exact borders
ofaudiomaynotbeperfectlyaligned. Whenperformingcrossfad-
3. EXPERIMENTS ingatmultipletimelengths,significantlyhigher-qualitymatchcuts
areproducedasdiscontinutiesandslightdifferencesinspectraare
3.1. EvaluationMetrics blendedawaywiththecrossfade. Whencomparingourmethodof
selectingaspecifictransitionpoint, named“Max-SS”, weseethat
To evaluate audio retrieval performance, we use multiple stan-
itoutperformsconcatenation,showingthatselectingamoreoptimal
dard metrics that are widely used across various retrieval tasks.
transition point within the 1-second query and match often results
Specifically,wemeasureretrievalmeanaverageprecision(R-mAP),
inahigherqualitytransition. Whenaddingourproposedadaptive
hit-rate@K, and precision@K metrics. These metrics align well
crossfading, we see the best transition performance, showing that
withthereal-worldusecaseofourproposedframework,wherean
theadditionofselectingtheoptimaltransitionpointandadaptively
editorisprovidedKaudiomatchcutstochoosefrom,withthegoal
fadingbasedontheaudiocharacteristicsoutperformseachbaseline.
oftheKrecommendationsbeinghigh-qualityaudiomatchcuts.
We highlight that the performance of the transition methods
Forevaluatingtransitionquality, weconstructcriteriatograde
areoftenafunctionofhowperceptually-similartheretrievedaudio
theoverallqualityoftheaudiotransitionofapositiveaudiomatch
match is. The more the query and retrieved audio match, the less
pair.Wecreatefourcriteria,rangingfrom0−3ofincreasingtransi-
theneedforadvancedtransitionmethodsastheyalreadytransition
tionquality:0)Transitionispooranddirectlynoticeable.1)Transi-
fromoneanotherwell. Forveryhigh-qualitymatchretrievals,sim-
tionisnoticeablebutisstillafluidtransition. 2)Transitionishigh-
pletransitionsmayresultinaudiomatchcutsofsimilarperceptual
qualitythatstronglymatcheseitherrhythmortimbre/pitch.3)Tran-
quality to our proposed method. However, our method allows for
sitionisimperceptible,thetransitionpointcannotbedirectlyheard.
thealignmentofthecutonspecificsoundevents,likeimpactsand
3.2. RetrievalEvaluation strikes. Therefore, the specific transition method is left for user
choice,dependingonthetypeofaudiomatchcutthatisdesired.
Table1showsqualitativeaudiomatchcutretrievalperformanceof
multiplebaselinemethodsagainstourproposedmethod. Asshown, 4. CONCLUSION
both the MFCC and Mel-Spectrogram representations are able to
outperformrandomselectionofaudiomatches,showingthatsimple In this paper, we introduce a framework to automatically find and
non-learnable representations are able to effectively retrieve audio createaudiomatchcuts,anadvancedvideoeditingtechniqueusedin
match cut candidates. However, when comparing large-scale deep videosandmovies.Analogoustovisualmatchcutting[5],thiswork
audiorepresentationsImageBind[22]andCLAP[21], weseethat canbeusedtoaidintheautomaticcreationoftrailers,edits,mon-
theysignificantlyoutperformthenon-learnablerepresentations,with tages, and other videos by creating high-quality audio match cuts
CLAPoutperformingImageBind[22]acrossallmetrics. Although thatareinterestingandappealingtoviewers. Inthefuture,wehope
models like CLAP are trained for other tasks like language-audio toexploremoreadvancedaudioblendingmethodsbeyondcrossfad-
alignment,thelearnedrepresentationsstillareeffectiveinaudio-to- ing, in addition creating audio-visual match cuts by incorporating
audioretrievalasthehighly-similarsamplesareoftenalsoperceptu- thevisualmodality,withtheabilitytocontrolspecificaudio-visual
allysimilar,themaincriteriaforcreatingaudiomatchcuts. Finally, characteristicsofthedesiredmatchcut.5. REFERENCES contrastive learning,” in ICASSP 2021-2021 IEEE Interna-
tionalConferenceonAcoustics,SpeechandSignalProcessing
[1] JamesECutting, “Theevolutionofpaceinpopularmovies,” (ICASSP).IEEE,2021,pp.3025–3029.
Cognitiveresearch:principlesandimplications,vol.1,pp.1–
[18] LenVandeVeireandTijlDeBie, “Fromrawaudiotoaseam-
21,2016.
lessmix:creatinganautomateddjsystemfordrumandbass,”
[2] AntonKarlKozlovic, “Anatomyoffilm,” Kinema: AJournal EURASIP Journal on Audio, Speech, and Music Processing,
forFilmandAudiovisualMedia,vol.1,2007. vol.2018,no.1,pp.1–21,2018.
[3] JohnSDouglassandGlennPHarnden, “Theartoftechnique: [19] Bo-Yu Chen, Wei-Han Hsu, Wei-Hsiang Liao, Marco
Anaestheticapproachtofilmandvideoproduction,” 1996. AMart´ınezRam´ırez,YukiMitsufuji,andYi-HsuanYang,“Au-
[4] Roy Thompson and Christopher J Bowen, Grammar of the tomaticdjtransitionswithdifferentiableaudioeffectsandgen-
Edit, Taylor&Francis,2013. erativeadversarialnetworks,” inICASSP2022-2022IEEEIn-
ternationalConferenceonAcoustics,SpeechandSignalPro-
[5] Boris Chen, Amir Ziai, Rebecca S Tucker, and Yuchen Xie,
cessing(ICASSP).IEEE,2022,pp.466–470.
“Matchcutting: Findingcutswithsmoothvisualtransitions,”
inProceedingsoftheIEEE/CVFWinterConferenceonAppli- [20] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren
cationsofComputerVision,2023,pp.2115–2125. Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal,
andMarvinRitter,“Audioset:Anontologyandhuman-labeled
[6] Go Irie, Takashi Satou, Akira Kojima, Toshihiko Yamasaki,
dataset for audio events,” in 2017 IEEE international con-
andKiyoharuAizawa, “Automatictrailergeneration,” inPro-
ferenceonacoustics,speechandsignalprocessing(ICASSP).
ceedingsofthe18thACMinternationalconferenceonMulti-
IEEE,2017,pp.776–780.
media,2010,pp.839–842.
[21] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor
[7] DawitMurejaArgaw,FabianCabaHeilbron,Joon-YoungLee,
Berg-Kirkpatrick, and Shlomo Dubnov, “Large-scale con-
Markus Woodson, and In So Kweon, “The anatomy of
trastive language-audio pretraining with feature fusion and
video editing: a dataset and benchmark suite for ai-assisted
keyword-to-caption augmentation,” in ICASSP 2023-2023
videoediting,” inEuropeanConferenceonComputerVision.
IEEEInternationalConferenceonAcoustics,SpeechandSig-
Springer,2022,pp.201–218.
nalProcessing(ICASSP).IEEE,2023,pp.1–5.
[8] Alejandro Pardo, Fabian Caba Heilbron, Juan Leo´n Alca´zar,
[22] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat
AliThabet,andBernardGhanem, “Moviecuts:Anewdataset
Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan
andbenchmarkforcuttyperecognition,” inEuropeanConfer-
Misra,“Imagebind:Oneembeddingspacetobindthemall,”in
enceonComputerVision.Springer,2022,pp.668–685.
ProceedingsoftheIEEE/CVFConferenceonComputerVision
[9] AlejandroPardo,FabianCaba,JuanLeo´nAlca´zar,AliKTha- andPatternRecognition,2023,pp.15180–15190.
bet, and Bernard Ghanem, “Learning to cut by watching
[23] Aaron van den Oord, Yazhe Li, and Oriol Vinyals, “Repre-
movies,” inProceedingsoftheIEEE/CVFInternationalCon-
sentation learning with contrastive predictive coding,” arXiv
ferenceonComputerVision,2021,pp.6858–6868.
preprintarXiv:1807.03748,2018.
[10] YaojieShen,LiboZhang,KaiXu,andXiaojieJin, “Autotran-
[24] KihyukSohn,“Improveddeepmetriclearningwithmulti-class
sition: Learning to recommend video transition effects,” in
EuropeanConferenceonComputerVision.Springer,2022,pp.
n-pairlossobjective,”Advancesinneuralinformationprocess-
ingsystems,vol.29,2016.
285–300.
[11] Sen Pei, Jingya Yu, Qi Chen, and Wozhou He, “Au- [25] Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-
tomatch: A large-scale audio beat matching benchmark for Kirkpatrick, and Shlomo Dubnov, “Hts-at: A hierarchical
boostingdeeplearningassistantvideoediting,” arXivpreprint token-semanticaudiotransformerforsoundclassificationand
arXiv:2303.01884,2023. detection,” inICASSP2022-2022IEEEInternationalConfer-
ence on Acoustics, Speech and Signal Processing (ICASSP).
[12] Stanley Kubrick and Arthur C. Clarke, “2001: A space
IEEE,2022,pp.646–650.
odyssey,”1968.
[26] Diederik P Kingma and Jimmy Ba, “Adam: A method for
[13] Andrew Adamson, “The chronicles of narnia: The lion, the
stochastic optimization,” arXiv preprint arXiv:1412.6980,
witchandthewardrobe,”2005.
2014.
[14] Joren Six and Marc Leman, “Panako: a scalable acoustic
[27] FitzgeraldJArchibald,“Crossfadeofdigitalaudiostreams,”.
fingerprintingsystemhandlingtime-scaleandpitchmodifica-
tion,” in15thInternationalsocietyformusicinformationre- [28] LucianLupsa-Tataru, “Audiofade-outprofileshapingforin-
trievalconference(ISMIR-2014),2014. teractivemultimedia,” 2020.
[15] Se´bastienFenet,Gae¨lRichard,YvesGrenier,etal.,“Ascalable
audiofingerprintmethodwithrobustnesstopitch-shifting.,”in
ISMIR,2011,pp.121–126.
[16] AdhirajBanerjeeandVipulArora, “wav2tok: Deepsequence
tokenizer for audio retrieval,” in The Eleventh International
ConferenceonLearningRepresentations,2022.
[17] Sungkyun Chang, Donmoon Lee, Jeongsoo Park, Hyungui
Lim, Kyogu Lee, Karam Ko, and Yoonchang Han, “Neu-
ralaudiofingerprintforhigh-specificaudioretrievalbasedon