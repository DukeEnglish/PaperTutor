APPROXIMATION RATES FOR SHALLOW RELUk NEURAL
NETWORKS ON SOBOLEV SPACES VIA THE RADON TRANSFORM
TongMao JonathanW.Siegel
Computer,ElectricalandMathematicalScienceandEngineeringDivision DepartmentofMathematics
KingAbdullahUniversityofScienceandTechnology TexasA&MUniversity
Thuwal23955,SaudiArabia CollegeStation,TX77843
tong.mao@kaust.edu.sa jwsiegel@tamu.edu
JinchaoXu
Computer,ElectricalandMathematicalScienceandEngineeringDivision
KingAbdullahUniversityofScienceandTechnology
Thuwal23955,SaudiArabia
jinchao.xu@kaust.edu.sa
August21,2024
ABSTRACT
Let Ω ⊂ Rd be a bounded domain. We consider the problem of how efficiently shallow neu-
ral networks with the ReLUk activation function can approximate functions from Sobolev spaces
Ws(L (Ω)) with errormeasured in the L (Ω)-norm. Utilizing the Radon transform and recentre-
p q
sultsfromdiscrepancytheory,weprovideasimpleproofofnearlyoptimalapproximationratesina
varietyofcases,includingwhenq≤p,p≥2,ands≤k+(d+1)/2.Therateswederiveareoptimal
uptologarithmicfactors,andsignificantlygeneralizeexistingresults. Aninterestingconsequence
is that the adaptivity of shallow ReLUk neural networks enables them to obtain optimal approxi-
mation rates for smoothness up to order s=k+(d+1)/2, even though they represent piecewise
polynomialsoffixeddegreek.
1 Introduction
We consider the problemof approximatinga targetfunction f :Ω→R, defined on a boundeddomain Ω⊂Rd, by
shallowReLUk neuralnetworksofwidthn,i.e. byanelementfromtheset
n
Σk(Rd):= ∑aσ(ω·x+b), a,b ∈R,ω ∈Rd , (1.1)
n i k i i i i i
(i=1 )
wheretheReLUk activationfunctionσ isdefinedby
k
0 x≤0
σ(x)= (1.2)
k xk x>0.
(cid:26)
We remark that when d =1, the class of shallow ReLUk neural networks is equivalent to the set of variable knot
splinesofdegreek. Forthisreason,shallowReLUk neuralnetworksarealsocalledridgesplinesandformahigher
dimensionalgeneralizationofvariableknotsplines. TheapproximationtheoryofshallowReLUkneuralnetworkshas
beenheavilystudiedduetotheirrelationshipwithneuralnetworksandtheirsuccessinmachinelearningandscientific
computing(seeforinstance[2,3,7,11,19,23,25,29,40,47,54]andthereferencestherein). Despitethiseffort,many
importantproblemsremainunsolved.Notably,adeterminationofsharpapproximationratesforshallowReLUkneural
networksonclassicalsmoothnessspaces,inparticularSobolevandBesovspaces,hasnotbeencompletedexceptwhen
d=1(thetheoryofvariableknotsplinesinonedimensioniswellknownandcanbefoundin[10,22],forinstance).
4202
guA
02
]LM.tats[
1v69901.8042:viXraA PREPRINT - AUGUST21,2024
LetΩ⊂Rd beaboundeddomain.Tosimplifythepresentation,wewillonlyconsiderthecasewhere
Ω:={x:|x|<1} (1.3)
is the unitballin Rd, althoughwe remarkthat ourtechniquesgive the same resultsfor anydomainΩ with smooth
boundarybyutilizingappropriateSobolevandBesovextensiontheorems[1,10,20,56].
WedefinetheSobolevspacesWs(L (Ω))forintegralsviathenorm
q
kfk Ws(Lq(Ω))=kfk Lq(Ω)+ ∑ kf(s)k Lq(Ω), (1.4)
|α|=s
wherethesumisovermulti-indicesαwithweights. Whensisnotaninteger,wewrites=k+θwithk aninteger
and0<θ<1,anddefinethefractionalSobolevspaces(seeforinstance[15])via
|Dαf(x)−Dαf(y)|q
|f|q := ∑ dxdy (1.5)
Ws(Lq(Ω))
|α|=kZΩ×Ω
|x−y|d+θq
and
kfkq :=kfkq +|f|q , (1.6)
Ws(Lq(Ω)) Lq(Ω) Ws(Lq(Ω))
withthestandardmodificationswhenq=∞.SobolevspacesarecentralobjectsinanalysisandthetheoryofPDEs(see
forinstance[20]). Weremarkalsothatwhenq=2andΩ=Rd,theSobolevnormcanbeconvenientlycharacterized
viatheFouriertransform,specifically
|f|2 h |ξ|2s|fˆ(ξ)|dξ, (1.7)
Ws(L2(Rd))
Rd
Z
where fˆdenotestheFouriertransformof f definedby(see[?,1])
fˆ(ξ):= eiξ·xf(x)dx. (1.8)
Rd
Z
TheBesovspacesmaybedefinedusingthemodulusofsmoothness(seeforinstance[10,13,59]),whichforafunction
f ∈L (Ω)isgivenby
q
ω(f,t) = supk∆kfk . (1.9)
k q h Lq(Ωkh)
|h|≤t
Here∆kf isthek-thorderfinitedifferenceinthedirectionhandΩ ={x∈Ω, x+kh∈Ω},whichguaranteesthat
h kh
alltermsofthefinitedifferencelieinΩ. Fors>0and1≤r,q≤∞theBesovnormisdefinedby
∞ω(f,t)r 1/r
k q
|f|
Bs
r(Lq(Ω)):=
0
tsr+1
dt (1.10)
(cid:18)Z (cid:19)
whenr<∞andby
|f| Bs ∞(Lq(Ω)):=supt−sω k(f,t) q, (1.11)
t>0
when r = ∞. The Besov spaces are closely related to approximation by trigonometric polynomials, splines, and
wavelets,andhavenumerousapplicationsinapproximationtheory,harmonicanalysis,signalprocessing,andstatistics
(see for instance [6,9,16–18]). We remark that it is knownthat the Sobolev spacesfor non-integralvalues of s are
equivalenttoBesovspaces(see[12,13]),specifically
kfk Ws(Lq(Ω))hkfk
Bs q(Lq(Ω))
(1.12)
foranappropriatelysmoothdomain.
An importanttheoreticalquestionis to determineoptimalapproximationratesforΣk(Rd) onthe classes ofSobolev
n
andBesovfunctions.Specifically,wewishtodetermineminimaxapproximationrates
sup inf kf−f k and sup inf kf−f k (1.13)
kfkWs(Lq(Ω))≤1fn∈Σk n(Rd)
n Lp(Ω)
kfkBs r(Lq(Ω))≤1fn∈Σk n(Rd)
n Lp(Ω)
fordifferentvaluesof theparameterss,p,q,r andk. Whend =1, the setofshallow neuralnetworksΣk(R) simply
n
correspondstothesetofvariableknotsplineswithatmostnbreakpoints.Inthiscaseacompletetheoryfollowsfrom
knownresultsonapproximationbyvariableknotsplines[7,8,46]. Whend>1,thisproblembecomesconsiderably
moredifficultandonlyafewpartialresultsareknown.
2A PREPRINT - AUGUST21,2024
We remark that when approximating functions from a Sobolev space Ws(L (Ω)) or a Besov space Bs(L (Ω)) in
q r q
L there is a significant difference dependingupon whether q≥ p or q< p. In the former case, linear methods of
p
approximationareabletoachieveanoptimalapproximationrate,whilewhenq< pnon-linearmethodsarerequired
[8,30]. For shallow ReLUk neural networks, existing approximation results have exclusively been obtained in the
linearregimewhenq≥ p. Fullyunderstandingapproximationbyshallow ReLUk neuralnetworksin thenon-linear
regimewhenq<pappearstobeaverydifficultopenproblem.
In this work, we study approximation rates for shallow ReLUk neural networks on Sobolev spaces using existing
approximationresults on variation spaces (we leave the more technical case of Besov spaces to future work). The
variationspacecorrespondingtoReLUk neuralnetworksisdefinedasfollows.LetΩ⊂Rd beaboundeddomainand
considerthedictionary,i.e. set,offunctions
Pd :={σ(ω·x+b),ω∈Sd−1, b∈[c,d]}, (1.14)
k k
wheretheinterval[c,d]dependsuponthedomainΩ(see[54,55]fordetailsandintuitionbehindthisdefinition). The
setPd consistsofthepossibleoutputsofeachneurongivenaboundontheinnerweights.Theunitballofthevariation
k
spaceistheclosedsymmetricconvexhullofthisdictionary,i.e.
n n
B (Pd)= ∑ad, d ∈Pd, ∑|a|≤1 , (1.15)
1 k i i i k i
(i=1 i=1 )
wheretheclosureistakenforinstanceinL (itisknownthattheclosureisthesamewhentakenindifferentnormsas
2
well[51,62]). Giventheunitball,wemaydefinethevariationspacenormvia
kfk =inf{c>0: f ∈cB (Pd)}. (1.16)
K 1(Pd k) 1 k
Thevariationspacewillbedenoted
K (Pd):={f ∈X : kfk <∞}. (1.17)
1 k K 1(Pd k)
We remarkthatthe variationspace canbe definedfora generaldictionary,i.e. boundedset offunctions,D (see for
instance [8,27,28,41,42,54]). This space plays an important role in non-linear dictionary approximation and the
convergencetheoryofgreedyalgorithms[14,50,57,58]. Inaddition,thevariationspacesK (Pd)playanimportant
1 k
roleinthetheoryofshallowneuralnetworksandhavebeenextensivelystudiedindifferentformsrecently[2,19,44,
45,55].
AnimportantquestionregardingthevariationspacesistodetermineoptimalapproximationratesforshallowReLUk
networksonthespaceK (Pd).Thisproblemhasbeenstudiedinaseriesofworks[2,3,23,31,36,37],withthe(nearly)
1 k
optimalrateofapproximation,
fn∈i Σn
k
nf (Rd)kf−f nk≤Ckfk
K 1(Pd
k)n− 21−2k 2+ d1 , (1.18)
recently being obtained for the L -norm in [54] and in the L -norm in [51]. To be precise, this rate is optimal up
2 ∞
to logarithmicfactors, which is shown in [54] under a mild restriction on the weights, while the general optimality
followsfromtheresultsprovedinthiswork.
ApromisingapproachtoobtainingapproximationratesforReLUkneuralnetworksonSobolevandBesovspacesisto
usetheapproximationrate(1.18)obtainedonthevariationspacetoobtainratesonSobolevandBesovspacesviaan
interpolationargument,i.e. byapproximatingthetargetfunction f firstbyanelementofthevariationspaceandthen
approximatingviaashallowneuralnetwork. Thistypeofargumentwasappliedin[2],whereanapproximationrate
of
n −1/d
inf kf−f k ≤Ckfk (1.19)
fn∈Σ1 n(Rd) n L∞(Ω) W1(L∞(Ω)) (cid:18)logn
(cid:19)
wasprovedfortheclassofLipschitzfunctionsW1(L (Ω)). Weremarkthat,duetoaminorerror,theproofin[2]is
∞
onlycorrectwhend≥4. Thisapproachwasextendedin[63](seealso[38,62])tolargervaluesofthesmoothnesss
andthelogarithmicfactorwasremoved,whichgivestheapproximationrate
fn∈i Σn
k
nf (Rd)kf−f nk L∞(Ω)≤Ckfk Ws(L∞(Ω))n−s/d (1.20)
for all s<(d+2k+1)/2. Up to logarithmic factors, this rate is optimal, which solves the problem (1.13) when
p=q=∞. Indeed,lowerboundsontheapproximationrates(1.13)canbeobtainedusingeithertheVC-dimensionor
pseudodimensionoftheclassofshallowneuralnetworksΣk(Rd)(see[4,25,35,50]). Thisgivesalowerboundof
n
sup inf kf−f k ≥C(nlog(n))−s/d (1.21)
kfkWs(Lq(Ω))fn∈Σk n(Rd)
n Lp(Ω)
3A PREPRINT - AUGUST21,2024
foralls,d,k,pandq. Removingtheremaininglogarithmicgaphereappearstobeaverydifficultproblem.
Weremarkthattherearealsootherapproacheswhichdonotutilizethevariationspace,suchasthemethoddeveloped
in[11,47],whereitisprovedforSobolevspacesthat
fn∈Σin
k
nf (Rd)kf−f nk L2(Ω)≤Ckfk Ws(L2(Ω))n−s/d (1.22)
for s≤ (d+2k+1)/2. Again, this rate is optimal up to logarithmic factors, giving the solution to (1.13) when
p=q=∞.
Inthiswork,weutilizeapproximationratesforthevariationspaceandaninterpolationargumenttoextendtheapprox-
imationratesderivedinpreviousworktoavarietyofnewcases. Thekeycomponentofouranalysisisthefollowing
embeddingtheorem,whichisprovedusingaRadonspacecharacterizationofthevariationspace[43–45].
Theorem1. Lets=(d+2k+1)/2.Thenwehavetheembedding
Ws(L (Ω))⊂K (Pd). (1.23)
2 1 k
This result shows that the L -Sobolev space with a certain amount of smoothness embeds into the variation space
2
K (Pd), and has quite a few importantconsequences. First, combiningthis with the approximationrate (1.18), we
1 k
obtainthefollowingcorollary.
Corollary1. Lets=(d+2k+1)/2.Thenwehavetheapproximationrate
fn∈Σin
k
nf (Rd)kf−f nk L∞(Ω)≤Ckfk Ws(L2(Ω))n−s/d. (1.24)
Notethatin(1.24)wehaveerrormeasuredinL with p=∞andsmoothnessmeasuredinL withq=2. Inparticular,
p q
this result gives to the best of our knowledgethe first approximationrate for ridge splines in the non-linearregime
when q< p. However,this onlyappliesto one particularvalueof s, p and q, and it is an interestingopen question
whetherthiscanbeextendedmoregenerally.
Tounderstandtheimplicationsforthelinearregime,wenotethatitfollowsfromCorollary1that
fn∈i Σn
k
nf (Rd)kf−f nk Lp(Ω)≤Ckfk Ws(Lp(Ω))n−s/d (1.25)
forany2≤p≤∞withs=(d+2k+1)/2.Standardinterpolationargumentscannowbeusedtogiveapproximation
ratesforSobolevspacesintheregimewhen p=qand p≥2.
Corollary2. Supposethat2≤p≤∞and0<s≤k+d+1. Thenwehave
2
fni ∈n Σf
k
nkf−f nk Lp(Ω)≤Ckfk Ws(Lp(Ω))n−s/d. (1.26)
Corollary2extendstheapproximationratesobtainedin[2,38,47,62,63]toall p≥2. Weremarkthatusinginterpo-
lationananalogousresultcanbeprovedforBesovspaces,butforsimplicitywewillnotdiscussthismoretechnical
resultinthispaper.
NotethatinCorollary2, werequiredtheindex p≥2. Whend=1,i.e. inthecaseofone-dimensionalsplines,itis
well-knownthatthesameratealsoholdswhen p<2. Inthiscase,Theorem1canactuallybeimprovedto(see[55],
Theorem3)
Ws(L (Ω))⊂K (Pd) (1.27)
1 1 k
fors=k+1(thecased=1inTheorem1),andapproximationratesforall1≤ p≤∞easilyfollowfromthisinan
analogousmanner. However,weremarkthatthismethodofprooffailswhend>1,sincetheembedding(1.27)fails
inthiscase forthevalueofsinTheorem1, whichis requiredtoobtaintheapproximationrateinCorollary2. This
canbeseenbynotingthat
K (Pd)⊂L (Ω),
1 k ∞
andthusif(1.27)holds,thenwemusthaveWs(L (Ω))⊂L (Ω)whichbytheSobolevembeddingtheoryimpliesthat
1 ∞
s≥d,whichisnotcompatiblewithTheorem1unless
(d+2k+1)/2≥d,
i.e. k≥(d−1)/2. Forthisreasonthecurrentmethodofproofcannotgivethesameapproximationrateswhend>1
forallvaluesof1≤p<2andk≥0.Resolvingthesecasesisaninterestingopenproblem,whichwillrequiremethods
thatgobeyondthevariationspacesK (Pd),forinstancebygeneralizingtheanalysisin[11,47].
1 k
4A PREPRINT - AUGUST21,2024
Let us also remark that the embedding given in Theorem 1 is sharp in the sense of metric entropy. Recall that the
metricentropynumbersofacompactsetK⊂X inaBanachspaceX isdefinedby
ε(K) =inf{ε>0: Kiscoveredby2n ballsofradiusε}. (1.28)
n X
ThisconceptwasfirstintroducedbyKolmogorov[24]andgivesameasureofthesizeofcompactsetK⊂X. Roughly
speaking,itgivesthesmallestpossiblediscretizationerrorifthesetKisdiscretizedusingn-bitsofinformation.Ithas
beenprovedin[54]thatthemetricentropyoftheunitballB (Pd)satisfies
1 k
ε n(B 1(Pd k)) L2(Ω)hn− 21−2k 2+ d1 . (1.29)
Moreover,theresultsin[31,51]implythatthemetricentropydecaysatthesamerateinallL (Ω)-spacesfor1≤p≤∞
p
(potentiallyup to logarithmicfactors). By the Birman-Solomyaktheorem [5], this matchesthe rate of decay of the
metric entropywith respectto L (Ω) ofthe unitballofthe SobolevspaceWs(L (Ω)) fors=(d+2k+1)/2. This
p 2
meansthatbothspacesinTheorem1haveroughlythesamesizeinL (Ω).
p
Finally,letuserelatetheseresultstotheexistingliteratureonridgeapproximation.Ridgeapproximationisconcerned
withapproximatingatargetfunction f byanelementfromtheset
n
R := ∑ f(ω·x), f :R→R, ω ∈Sd−1 , (1.30)
n i i i i
(i=1 )
Here the functions f can be arbitraryone-dimensionalfunctionsandthe directionω lie onthe sphere Sd−1. There
i i
isafairlyextensiveliteratureontheproblemofridgeapproximation(seeforinstance[25,48]foranoverviewofthe
literature). In thelinear regimeoptimalapproximationrates areknownforSobolevandBesov spaces(see [32,34])
andwehaveforinstance
fni ∈n Rf nkf−f nk Lp(Ω)≤Ckfk Ws(Lp(Ω))n− d−s 1 (1.31)
forall1≤p≤∞. Thisresultisprovedbyfirstapproximating f bya(multivariate)polynomialofdegreem,andthen
representingthispolynomialasasuperpositionofmd−1polynomialridgefunctions.Thisconstructionappliestoneural
networksprovidedweuseanexoticactivationfunctionσwhosetranslatesaredenseinC([−1,1])(see[33]). Using
anarbitrarysmoothnon-polynomialactivationfunctionwecanalsoreproducepolynomialsusingfinitedifferencesto
obtainanapproximationrateofO(n−s/d)(see[39]).
Ontheotherhand,shallowReLUkneuralnetworksalwaysrepresentpiecewisepolynomialsoffixeddegreek,andour
resultsdonotproceedbyapproximatingwithahigh-degreepolynomial. Onewouldexpectthatsuchamethodcould
onlycapturesmoothnessuptoorderk+1.Interestingly,asshowninCorollary2,thenon-linearnatureofReLUkneu-
ralnetworksallowustocapturesmoothnessuptodegreek+(d+1)/2. Thisshowsthatinhighdimensions,suitably
adaptivepiecewisepolynomialscancaptureveryhighsmoothnesswithafixedlowdegree,providingaSobolevspace
analogueoftheresultsobtainedin[53]. WeremarkthatthisisapotentialadvantageofshallowReLUk networksfor
applicationssuchassolvingPDEs[52,61].
Thepaperisorganizedasfollows.InSection2wegiveanoverviewoftherelevantfactsregardingtheRadontransform
[49] thatwe will use later. Then, in Section 3 we providethe proofof Theorem1. Finally, in Section 4 we deduce
Corollary2.
2 The Radon Transform
InthisSection, werecallthedefinitionandseveralimportantfactsaboutthe Radontransformthatwewilluse later.
The study of the Radon transform is a large and active area of research and we necessarily only cover a few basic
facts which will be importantin our later analysis. For more detailed information on the Radon transform, see for
instance[21,26,60]. We alsoremarkthattheRadontransformhasrecentlybeenextensivelyappliedtothestudyof
shallowneuralnetworksin[43,44].
GivenaSchwartzfunction f ∈S(Rd)definedonRd,wedefinetheRadontransformof f as
R(f)(ω,b)= f(x)dx, (2.1)
Zω·x=b
wheretheaboveintegralisoverthehyerplaneω·x=b. ThedomainoftheRadontransformisSd−1×R,i.e. |ω|=1
andb∈R.
5A PREPRINT - AUGUST21,2024
UsingFubini’stheorem,weeasilyseethatforeachω∈Sd−1wehave
kR(f)(ω,·)k L1(R)= ZR|R(f)(ω,b)|db=
ZR (cid:12)Zω·x=b
f(x)dx (cid:12)db≤
ZR
Zω·x=b|f(x)|dxdb=kfk L1(Rd). (2.2)
(cid:12) (cid:12)
IntegratingthisoverthesphereSd−1weget (cid:12) (cid:12)
(cid:12) (cid:12)
kR(f)k L1(Sd−1×R)≤ω d−1kfk L1(Rd),
whereω denotesthesurfaceareaofthesphereSd−1,sothattheRadontransformextendstoaboundedmapfrom
d−1
L (Rd)→L (Sd−1×R). Infact,thebound(2.2)givesevenmoreinformation.
1 1
AfundamentalresultrelatingtheRadontransformtotheFouriertransformistheFourierslicetheorem(seeforinstance
Theorem5.10in[26]).
Theorem2(FourierSliceTheorem). Let f ∈L (Rd)andω∈Sd−1. Letg (b)=R(f)(ω,b). Thenforeacht ∈R
1 ω
wehave
g (t)= fˆ(ωt). (2.3)
ω
Notethatby(2.2)wehaveg ∈L (R)andsotheFouriertransforminTheorem2iswell-defined. Forcompleteness,
ω 1
c
wegivethesimpleproof.
Proof. ExpandingoutthedefinitionoftheFouriertransformandRadontransformsandusingFubinigives
g (t)= e−itb f(x)dxdb= e−itω·xf(x)dxdb= e−itω·xf(x)dx= fˆ(ωt), (2.4)
ω
ZR Zω·x=b ZR Zω·x=b ZRd
sinceω·x=b.
c
Utilizing the Fourier slice theorem and Fourier inversion, we can invert the Radon transform as follows (see for
instanceSection5.7in[26]).
1 1 ∞
f(x)= fˆ(ξ)eiξ·xdξ= fˆ(ωt)|t|d−1eitω·xdtdω
(2π)d ZRd 2(2π)d ZSd−1 Z−∞
(2.5)
1 ∞
= g (t)|t|d−1eitω·xdtdω.
2(2π)d ZSd−1 Z−∞ ω
The inner integralabove is the inverse Fourier transform of g ω(t)|t|d−1 cevaluated atω·x. This gives the inversion
formula
f(x)= H dRfc(ω,ω·x)dω, (2.6)
Sd−1
Z
wheretheoperatorH actsontheb-coordinateandisdefinedbythe(one-dimensional)Fouriermultiplier
d
1
H g(t)= |t|d−1gˆ(t). (2.7)
d 2(2π)d
Theinversionformula(2.6)is typicallycalleddthe filteredback-projectionoperatorandisoftenappliedto invertthe
Radontransforminmedicalimagingapplications(seeforinstance[26]).ItisvalidprovidedthattheFourierinversion
formulaisvalid,forinstancewhenever f isaSchwartzfunction.
3 Embeddings ofSobolevSpaces into ReLUk VariationSpaces
OurgoalinthissectionistoproveTheorem1ontheembeddingofSobolevspacesintotheneuralnetworkvariation
space.
ProofofTheorem1. ByastandarddensityargumentandtheSobolevextensiontheory(seeforinstance[?,1,20,56])
itsufficestoprovethat
kfk
K 1(Pd
k)≤Ckfk
Ws(L2(Rd))
(3.1)
fors=(d+2k+1)/2andeveryfunction f ∈C∞(Bd). Herethenormontheleft-handsideisthevariationnormof
c 2
f restrictedtoΩ,theconstantCisindependentof f,andBd denotestheballofradius2inRd (anyboundeddomain
2
containingΩwillalsodo).
6A PREPRINT - AUGUST21,2024
Since f isaSchwartzfunction,wemayusetheRadoninversionformula(2.6)towrite
f(x)= F (ω·x)dω, (3.2)
ω
ZSd−1
whereF (t)=H Rf(ω,t). Weremarkalsothatsince f ∈C∞(Bd),wehaveF ∈C∞(R)foreachω∈Sd−1(itisnot
ω d c 2 ω
necessarilycompactlysupportedduetotheHilberttransforminthefilteredback-projectionoperator).
Next,weusethePeanokernelformulatorewrite(3.2)forxintheunitballas
1 ω·x
f(x)=p(x)+ F(k+1) (b)(ω·x−b)kdbdω
k! ZSd−1 Z−1 ω
(3.3)
1 1
=p(x)+
F(k+1)
(b)σ(ω·x−b)dbdω,
k! ZSd−1 Z−1 ω k
where p(x)isapolynomialofdegreeatmostkgivenby
k F(j) (−1)
p(x)= ∑ ω (ω·x+1)jdω. (3.4)
ZSd−1
j=0
j!
NowHölder’sinequalityimpliesthat
1 1 1/2 1/2
|F(k+1) (b)|dbdω≤C |F(k+1) (b)|2db dω≤C |F(k+1) (b)|2db dω
ω ω ω
ZSd−1 Z−1 ZSd−1 (cid:18)Z−1
(cid:19)
ZSd−1 (cid:18)ZR
(cid:19) (3.5)
1/2
=C |tk+1Fˆ (t)|2dt dω.
ω
Sd−1 R
Z (cid:18)Z (cid:19)
UtilizingtheFourierslicetheorem,thedefinitionofthefilteredback-projectionoperatorH ,andJensen’sinequality,
d
weobtainthebound
1 1/2
|F(k+1) (b)|dbdω≤C |tk+1Fˆ (t)|2dt dω
ω ω
ZSd−1 Z−1 ZSd−1 (cid:18)ZR
(cid:19)
=C ∞ |t|2s+d−1|R[ (f)(ω,t)|2dt 1/2 dω
ZSd−1 (cid:18)Z−∞
(cid:19) (3.6)
≤C ∞ |t|2s+d−1|R[ (f)(ω,t)|2dtdω 1/2
(cid:18)ZSd−1 Z−∞
(cid:19)
1/2
=C 2 Rd|ξ|2s|fˆ(ξ)|2dξ =C|f| Ws(L2(Rd)).
(cid:18) Z (cid:19)
Setting
1 1
g(x):=
F(k+1)
(b)σ(ω·x−b)dbdω (3.7)
k! ZSd−1 Z−1 ω k
thebound(3.6)impliesthat(seeforinstanceLemma3in[55])
1
kgk
K 1(Pd
k)≤
ZSd−1
Z−1|F
ω(k+1)
(b)|dbdω≤C|f| Ws(L2(Rd)). (3.8)
Italsoimmediatelyfollowsfrom(3.6)that
1
kgk L2(Ω)≤C
ZSd−1
Z−1|F
ω(k+1)
(b)|dbdω≤C|f| Ws(L2(Rd)), (3.9)
sincetheelementsofthedictionaryPd areuniformlyboundedinL . Thisimpliesthat
k 2
kpk L2(Ω)=kf−gk L2(Ω)≤kfk L2(Ω)+kgk L2(Ω)≤Ckfk Ws(L2(Rd)). (3.10)
Sinceallnormsonthefinitedimensionalspaceofpolynomialsofdegreeatmostkareequivalent,wethusobtain
kpk
K 1(Pd
k)≤Ckfk Ws(L2(Rd)), (3.11)
whichcombinedwith(3.8)giveskfk
K 1(Pd
k)≤Ckfk Ws(L2(Rd))asdesired.
7A PREPRINT - AUGUST21,2024
4 ApproximationUpper Bounds forSobolev Spaces
Inthissection,wededucetheapproximationratesinCorollary2fromTheorem1andCorollary1. Thisresultfollows
easilyfromtheinterpolationtheorycharacterizingtheinterpolationspacesbetweentheSobolevspaceWs(L (Ω))and
p
L (Ω)(seeforinstance[10],Chapter6fortheonedimensionalcase),butforthereader’sconveniencewegiveasimple
p
direct proof(which contains the essential interpolationargument). We remark that a similar, but more complicated
interpolationargumentcanbeusedtoobtainapproximationratesforBesovspacesaswell.
ProofofCorollary2. The first step in the proof is to note that by the Sobolev extension theorems(see for instance
[?,10,20,56])wemayassumethat f isdefinedonallofRd, f issupportedontheballofradius(say)2(orsomeother
domaincontainingΩ),and
kfk Ws(Lp(Rd))≤Ckfk
Ws(Lp(Ω))
(4.1)
foraconstantC=C(Ω).
Letφ:Rd →[0,∞)beasmoothradiallysymmetricbumpfunctionsupportedintheunitballandsatisfying
φ(x)dx=1.
Rd
Z
Forε>0,wedefineφ :Rd →Rd by
ε
φ(x)=ε−dφ(x/ε)
ε
andformtheapproximant
ρ ρ
f (x)=∑ (−1)t−1 φ(y)f(x−ty)dy, (4.2)
ε ε
t Rd
t=1(cid:18) (cid:19) Z
whereρ>sisaninteger.Usingthat φ (y)dy=1,weestimatetheerrorkf−f k by
ε ε Lp
R ρ ρ
kf−f εk Lp(Rd)≤
(cid:13)
(cid:13)ZRdφ ε(y) t∑
=0(cid:18)t
(cid:19)(−1)tf(x−ty) !dy
(cid:13)
(cid:13)Lp(dx). (4.3)
(cid:13) (cid:13)
Now,φ issupportedonaballofradiusεa(cid:13)nd (cid:13)
ε (cid:13) (cid:13)
ρ ρ
∑
t
(−1)tf(x−ty) dy ≤ω ρ(f,|y|) p≤C|y|skfk
Ws(Lp(Ω))
(4.4)
(cid:13) (cid:13) t=0(cid:18) (cid:19) ! (cid:13) (cid:13)Lp(dx)
(cid:13) (cid:13)
foraconstantC=C(s(cid:13) (cid:13),p,d)bythedefinitionoftheB(cid:13) (cid:13)esovspaceBs ∞(L p(Rd))(hereω ρ(f,r)
p
denotestheρ-thorder
modulusofsmoothness).Thusthetriangleinequalityimpliesthat
ρ ρ
kf−f εk Lp(Rd)≤ ZRdφ ε(y)
(cid:13)
(cid:13)t∑
=0(cid:18)t
(cid:19)(−1)tf(x−ty)
(cid:13)
(cid:13)Lp(dx)dy≤Cεskfk Ws(Lp(Rd)), (4.5)
(cid:13) (cid:13)
sincekφ εk L1(Rd)=1. (cid:13)
(cid:13)
(cid:13)
(cid:13)
The next step is to bound the Wα(L (Rd))-norm of f , where α=(d+2k+1)/2. Observe that since ρ is fixed
2 ε
dependingupons,itsufficestobound
φ (y)f(x−ty)dy (4.6)
ε
(cid:13)ZRd (cid:13)Wα(L2(Rd,dx))
(cid:13) (cid:13)
foreachfixedintegert≥1. Todothis,w(cid:13)efirstmakeachange(cid:13)ofvariablestorewrite
(cid:13) (cid:13)
1 y
f (x):= φ(y)f(x−ty)dy= φ f(x−y)dy= φ (y)f(x−y)dy. (4.7)
ε,t Rd ε td Rd ε t Rd tε
Z Z Z
(cid:16) (cid:17)
TakingtheFouriertransform,wethusobtain
fˆ (ξ)= fˆ(ξ)φˆ(tεξ). (4.8)
ε,t
WenowestimatetheWα(L (Rd))-normof f asfollows
2 ε,t
|f |2 h |ξ|2α|fˆ (ξ)|2dξ= |ξ|2α|fˆ(ξ)|2|φˆ(tεξ)|2dξ. (4.9)
ε,t Wα(L2(Rd))
Rd
ε,t
Rd
Z Z
8A PREPRINT - AUGUST21,2024
Notethatsince f issupportedonaballofradius2,wehave(recallthat p≥2)
|ξ|2s|fˆ(ξ)|2dξh|f|2 ≤Ckfk2 . (4.10)
Rd
Ws(L2(Rd)) Ws(Lp(Rd))
Z
ThusHölder’sinequalityimpliesthat
|f |2 ≤ |ξ|2s|fˆ(ξ)|2dξ sup |ξ|2(α−s)|φˆ(tεξ)|
ε,t Wα(L2(Rd))
(cid:18)ZRd (cid:19) ξ∈Rd !
(4.11)
≤Ckfk2 sup |ξ|2(α−s)|φˆ(tεξ)| .
Ws(Lp(Rd))
ξ∈Rd !
Bychangingvariables,weseethat
sup |ξ|2(α−s)|φˆ(tεξ)| =(tε)−2(α−s) sup |ξ|2(α−s)|φˆ(ξ)| ≤Cε−2(α−s), (4.12)
ξ∈Rd ! ξ∈Rd !
sincethesupremumaboveisfinite(φisaSchwartzfunction).Hence,weget
|f ε,t| Wα(L2(Rd))≤Ckfk Ws(Lp(Rd))ε−(α−s). (4.13)
Inaddition,weclearlyhavefromthetriangleinequalitythat
kf ε,tk L2(Rd)≤kfk L2(Rd)≤kfk W2(L2(Rd)), (4.14)
sothatifε≤1weobtain(applyingthisforallt uptoρ)
kf εk Wα(L2(Rd))≤Ckfk Ws(Lp(Rd))ε−(α−s) (4.15)
WenowapplyCorollary1toobtainan f ∈Σk(Rd)suchthat
n n
kf n−f εk Lp(Ω)≤Ckfk Ws(Lp(Rd))ε−(α−s)n−α. (4.16)
Combiningthiswiththebound(4.5),weget
kf−f nk Lp(Ω)≤Ckfk Ws(Lp(Rd)) εs+n−αε−(α−s) . (4.17)
(cid:16) (cid:17)
Finally,choosingε=n−1/d andrecallingthatα=(d+2k+1)/2completestheproof.
5 Acknowledgements
WewouldliketothankRonaldDeVore,RobertNowak,RahulParhi,andHrushikeshMhaskarforhelpfuldiscussions
during the preparation of this manuscript. Jonathan W. Siegel was supported by the National Science Foundation
(DMS-2424305andCCF-2205004)aswellastheMURIONRgrantN00014-20-1-2787. TongMaoandJinchaoXu
aresupportedbytheKAUSTBaselineResearchFund.
References
[1] RobertAAdamsandJohnJFFournier,Sobolevspaces,Elsevier,2003.
[2] FrancisBach,Breakingthecurseofdimensionalitywithconvexneuralnetworks,TheJournalofMachineLearn-
ingResearch18(2017),no.1,629–681.
[3] AndrewR Barron, Universalapproximationboundsfor superpositionsofa sigmoidalfunction, IEEETransac-
tionsonInformationtheory39(1993),no.3,930–945.
[4] PeterLBartlett,NickHarvey,ChristopherLiaw,andAbbasMehrabian,Nearly-tightvc-dimensionandpseudodi-
mension bounds for piecewise linear neural networks, The Journal of Machine Learning Research 20 (2019),
no.1,2285–2301.
[5] MikhailShlemovichBirmanandMikhailZakharovichSolomyak,Piecewise-polynomialapproximationsoffunc-
tionsoftheclassesWα,MatematicheskiiSbornik115(1967),no.3,331–355.
p
9A PREPRINT - AUGUST21,2024
[6] AntoninChambolle,RonaldADeVore,Nam-YongLee,andBradleyJLucier,Nonlinearwaveletimageprocess-
ing: variationalproblems, compression, and noise removal through wavelet shrinkage, IEEE Transactions on
ImageProcessing7(1998),no.3,319–335.
[7] RonaldDeVore,BorisHanin,andGuerganaPetrova,Neuralnetworkapproximation,ActaNumerica30(2021),
327–444.
[8] RonaldADeVore,Nonlinearapproximation,Actanumerica7(1998),51–150.
[9] RonaldADeVore,BjörnJawerth,andBradleyJLucier,Imagecompressionthroughwavelettransformcoding,
IEEETransactionsonInformationTheory38(1992),no.2,719–746.
[10] RonaldA DeVoreand GeorgeG Lorentz, Constructive approximation,vol. 303, SpringerScience & Business
Media,1993.
[11] RonaldADeVore,KonstantinIOskolkov,andPenchoPPetrushev,Approximationbyfeed-forwardneuralnet-
works,AnnalsofNumericalMathematics4(1996),261–288.
[12] RonaldADeVoreandVasilAPopov,Interpolationofbesovspaces,TransactionsoftheAmericanMathematical
Society305(1988),no.1,397–414.
[13] RonaldADeVoreandRobertCSharpley,BesovspacesondomainsinRd,TransactionsoftheAmericanMathe-
maticalSociety335(1993),no.2,843–864.
[14] RonaldADeVoreandVladimirNTemlyakov,Someremarksongreedyalgorithms,Advancesincomputational
Mathematics5(1996),no.1,173–187.
[15] Eleonora Di Nezza, Giampiero Palatucci, and Enrico Valdinoci, Hitchhiker’s guide to the fractional sobolev
spaces,BulletindesSciencesMathématiques136(2012),no.5,521–573.
[16] DavidLDonohoandIainMJohnstone,Adaptingtounknownsmoothnessviawaveletshrinkage,Journalofthe
AmericanStatisticalAssociation90(1995),no.432,1200–1224.
[17] ,Minimaxestimationviawaveletshrinkage,TheAnnalsofStatistics26(1998),no.3,879–921.
[18] DavidL.Donoho,MartinVetterli,RonaldA.DeVore,andIngridDaubechies,Datacompressionandharmonic
analysis,IEEETransactionsonInformationTheory44(1998),no.6,2435–2476.
[19] Weinan E, Chao Ma, and Lei Wu, The barron space and the flow-inducedfunction spaces for neural network
models,ConstructiveApproximation55(2022),no.1,369–406.
[20] LawrenceCEvans,Partialdifferentialequations,vol.19,AmericanMathematicalSoc.,2010.
[21] SigurdurHelgasonandSHelgason,Theradontransform,vol.2,Springer,1980.
[22] Jean-PierreKahane,Teoriaconstructivadefunciones,CourseNotes(1961).
[23] JasonMKlusowskiandAndrewRBarron,Approximationbycombinationsofreluandsquaredreluridgefunc-
tionswithℓ1andℓ0controls,IEEETransactionsonInformationTheory64(2018),no.12,7649–7656.
[24] Andrei Nikolaevich Kolmogorov, On linear dimensionality of topological vector spaces, Doklady Akademii
Nauk,vol.120,RussianAcademyofSciences,1958,pp.239–241.
[25] SergeiVladimirovichKonyagin,AleksandrAndreevichKuleshov,andVitaliiEvgen’evichMaiorov,Someprob-
lemsinthetheoryofridgefunctions,ProceedingsoftheSteklovInstituteofMathematics301(2018),144–169.
[26] PeterKuchment,TheRadonTransformandMedicalImaging,SIAM,2013.
[27] Vera Kurkováand Marcello Sanguineti, Boundson rates of variable-basisand neural-networkapproximation,
IEEETransactionsonInformationTheory47(2001),no.6,2659–2665.
[28] , Comparisonof worst case errors in linearand neuralnetwork approximation,IEEE Transactionson
InformationTheory48(2002),no.1,264–275.
[29] YannLeCun,YoshuaBengio,andGeoffreyHinton,Deeplearning,Nature521(2015),no.7553,436–444.
[30] GeorgeGLorentz,ManfredvonGolitschek,andYulyMakovoz,Constructiveapproximation: Advancedprob-
lems,vol.304,Springer,1996.
[31] Limin Ma, Jonathan W Siegel, and Jinchao Xu, Uniform approximation rates and metric entropy of shallow
neuralnetworks,ResearchintheMathematicalSciences9(2022),no.3,46.
[32] VE Maiorov, Best approximationby ridge functions in l p-spaces, Ukrainian MathematicalJournal 62 (2010),
452–466.
[33] VitalyMaiorovandAllanPinkus,Lowerboundsforapproximationbymlpneuralnetworks,Neurocomputing25
(1999),no.1-3,81–91.
10A PREPRINT - AUGUST21,2024
[34] VitalyEMaiorov,Onbestapproximationbyridgefunctions,JournalofApproximationTheory99(1999),no.1,
68–94.
[35] VitalyEMaiorovandRonMeir,Onthenearoptimalityofthestochasticapproximationofsmoothfunctionsby
neuralnetworks,AdvancesinComputationalMathematics13(2000),79–103.
[36] Y Makovoz, Uniform approximation by neural networks, Journal of Approximation Theory 95 (1998), no. 2,
215–228.
[37] YulyMakovoz,Randomapproximantsandneuralnetworks,JournalofApproximationTheory85(1996),no.1,
98–109.
[38] TongMaoandDing-XuanZhou,Ratesofapproximationbyrelushallowneuralnetworks,JournalofComplexity
79(2023),101784.
[39] Hrushikesh N Mhaskar, Neural networks for optimal approximation of smooth and analytic functions, Neural
computation8(1996),no.1,164–177.
[40] ,Kernel-basedanalysisofmassivedata,FrontiersinAppliedMathematicsandStatistics6(2020),30.
[41] HrushikeshN.MhaskarandTongMao,Tractabilityofapproximationbygeneralshallownetworks,Analysisand
Applications22(2024),no.03,535–568.
[42] Hrushikesh Narhar Mhaskar, On the tractability of multivariate integration and approximation by neural net-
works,JournalofComplexity20(2004),no.4,561–590.
[43] GregOngie,RebeccaWillett,DanielSoudry,andNathanSrebro,Afunctionspaceviewofboundednorminfinite
widthrelunets: Themultivariatecase,arXivpreprintarXiv:1910.01635(2019).
[44] Rahul Parhi and Robert D Nowak, Banach space representer theorems for neural networks and ridge splines,
TheJournalofMachineLearningResearch22(2021),no.1,1960–1999.
[45] ,Whatkindsoffunctionsdodeepneuralnetworkslearn? insightsfromvariationalsplinetheory,SIAM
JournalonMathematicsofDataScience4(2022),no.2,464–489.
[46] Pencho P Petrushev, Direct and converse theorems for spline and rational approximation and besov spaces,
FunctionSpacesandApplications:ProceedingsoftheUS-SwedishSeminarheldinLund,Sweden,June15–21,
1986,Springer,1988,pp.363–377.
[47] , Approximationby ridge functions and neuralnetworks, SIAM Journal on Mathematical Analysis 30
(1998),no.1,155–189.
[48] AllanPinkus,Approximationtheoryofthemlpmodelinneuralnetworks,Actanumerica8(1999),143–195.
[49] Johann Radon, 1.1 über die bestimmung von funktionen durch ihre integralwerte längs gewisser mannig-
faltigkeiten,Classicpapersinmoderndiagnosticradiology5(2005),no.21,124.
[50] JonathanWSiegel,Optimalapproximationratesfordeepreluneuralnetworksonsobolevspaces,arXivpreprint
arXiv:2211.14400(2022).
[51] , Optimal approximation of zonoids and uniform approximation by shallow neural networks, arXiv
preprintarXiv:2307.15285(2023).
[52] JonathanW Siegel, QingguoHong, Xianlin Jin, Wenrui Hao, and JinchaoXu, Greedy trainingalgorithmsfor
neuralnetworksandapplicationstopdes,JournalofComputationalPhysics484(2023),112084.
[53] JonathanWSiegelandJinchaoXu,High-orderapproximationratesforshallowneuralnetworkswithcosineand
ReLUk activationfunctions,AppliedandComputationalHarmonicAnalysis58(2022),1–26.
[54] , Sharp bounds on the approximation rates, metric entropy, and n-widths of shallow neural networks,
FoundationsofComputationalMathematics(2022),1–57.
[55] , Characterizationofthe variationspacescorrespondingtoshallowneuralnetworks, ConstructiveAp-
proximation57(2023),no.3,1109–1132.
[56] EliasMStein,Singularintegralsanddifferentiabilitypropertiesoffunctions,Princetonuniversitypress,1970.
[57] VladimirTemlyakov,Greedyapproximation,vol.20,CambridgeUniversityPress,2011.
[58] VladimirNTemlyakov,Greedyapproximation,ActaNumerica17(2008),235–409.
[59] HansTriebel,Theoryoffunctionspaces,BirkhäuserVerlag,Basel,1983.
[60] MichaelUnser, Ridges, neuralnetworks, andthe Radontransform, Journalof Machine LearningResearch 24
(2023),no.37,1–33.
11A PREPRINT - AUGUST21,2024
[61] Jinchao Xu, Finite neuron method and convergence analysis, Communications in Computational Physics 28
(2020),no.5,1707–1745.
[62] Yunfei Yang and Ding-Xuan Zhou, Nonparametric regression using over-parameterized shallow relu neural
networks,JournalofMachineLearningResearch25(2024),1–35.
[63] ,Optimalratesofapproximationbyshallowrelukneuralnetworksandapplicationstononparametric
regression,ConstructiveApproximation(2024),1–32.
12