An Overlooked Role of Context-Sensitive Dendrites
Mohsin Raza1,2, Ahsan Adeel1,3*
Abstract—To date, most dendritic studies have predominantly [16]. To uncover the true computational potential of TPNs, it
focused on the apical zone of pyramidal two-point neurons is critical to emphasize the importance of understanding and
(TPNs) receiving only feedback (FB) connections from higher
defining the roles of different kinds of contexts arriving at the
perceptual layers and using them for learning. Recent cellular
apicaltuft[13].Hence,dissectionofcontextualfield(CF)into
neurophysiologyandcomputationalneurosciencestudiessuggests
thattheapicalinput(context),comingfromfeedbackandlateral sub-CFs is imperative to better understand the amplification
connections, is multifaceted and far more diverse, with greater andsuppressionofrelevantandirrelevantsignals,respectively
implications for ongoing learning and processing in the brain [14]. Specifically: (i) what kinds of information arrive at
thanpreviouslyrealized.InadditiontotheFB,theapicaltuftre-
the apical tuft? (ii) how are they formed? (iii) how do they
ceivessignalsfromneighboringcellsofthesamenetworkasprox-
influence the cell’s response to the FF signals? [14]
imal (P) context, other parts of the brain as distal (D) context,
andoverallcoherentinformationacrossthenetworkasuniversal The Conscious Multisensory Integration (CMI) theory [13]–
(U) context. The integrated context (C) amplifies and suppresses [15] suggested that the apical tuft of the TPN receives mod-
the transmission of coherent and conflicting feedforward (FF) ulatory sensory signals coming from: the neighbouring cells
signals, respectively. Specifically, we show that complex context-
of the same network e.g., audio as P, other parts of the brain,
sensitive(CS)-TPNsflexiblyintegrateCmoment-by-momentwith
in principle from anywhere in space-time e.g., visuals as D,
theFFsomaticcurrentatthesomasuchthatthesomaticcurrent
is amplified when both feedforward (FF) and C are coherent; and the background information/overall coherent information
otherwise, it is attenuated. This generates the event only when across the multisensory (MS) network as U. These contextual
the FF and C currents are coherent, which is then translated signals play a decisive role in precisely selecting whether
into a singlet or a burst based on the FB information. Spiking
to amplify/suppress the transmission of relevant/irrelevant FF
simulation results show that this flexible integration of somatic
signals, without changing the content e.g., which information
andcontextualcurrentsenablesthepropagationofmorecoherent
signals (bursts), making learning faster with fewer neurons. isworthpayingmoreattentionto?This,asopposedto,uncon-
Similar behavior is observed when this functioning is used in ditionalexcitatoryandinhibitoryactivity,iscalledconditional
conventionalartificialnetworks,whereordersofmagnitudefewer amplification and suppression [13].
neurons are required to process vast amounts of heterogeneous
This view of context-sensitivity is also called cooperative
real-worldaudio-visual(AV)datatrainedusingbackpropagation
context-sensitive computing [4], [14], [15] whose processing
(BP).Thecomputationalfindingspresentedheredemonstratethe
universality of CS-TPNs, suggesting a dendritic narrative that and learning capabilities are shown to be well-matched to the
was previously overlooked. capabilities of the mammalian neocortex [10], [17]. In this
approach,CS-TPNsreceiveP,D,andUfieldstoconditionally
I. INTRODUCTION
segregate relevant and irrelevant FF signals or transmit their
Going beyond long-standing integrate-and-fire pyramidal FF message only when it is coherently related to the overall
PointNeurons(PNs)[1]—uponwhichcurrentdeeplearningis activity of the network. Individual neurons extract synergistic
based [2]—recent breakthroughs in cellular neurophysiology FF components as U by first conditionally segregating the
[3] have revealed that pyramidal neurons in the mammalian coherentandincoherentmultisensoryinformationstreamsand
neocortex possess two major points of integration: apical and then recombining only the coherent multistream. The U is
basal, termed TPNs. [4]. To date, most studies, including broadcasted to other brain areas which are received by other
the latest burst-dependent synaptic plasticity (BDSP) [5] and neurons along with the current local context (P and D) [14],
single-phase deep learning in cortico-cortical networks [6] [18], [19]. These complex CS-TPNs when used in artificial
have predominantly used FB information at the apical zone neural network approaches enabled faster learning (beating
of TPNs for learning (aka. credit assignment) [7]–[12]. How- Transformer—the backbone of ChatGPT) [20] and consumed
ever, the apical input, coming from the feedback and lateral significantlyfewerresourcescomparedtoconvolutionalneural
connections, is far more diverse with far greater implications nets (CNNs) [14], [15] in some experimental settings.
for ongoing learning and processing in the brain [13]–[15]. Building on these recent findings, the goal of this work is
The apical zone receives input from diverse cortical and to find the biologically plausible validation for why deep
subcortical sources as a context that selectively amplifies and neural network (DNN) approaches with CS-TPNs processing
suppresses the transmission of coherent and conflicting FF and standard backpropagation-based learning [14], [15], [20],
signals, respectively received at the basal zone [13], [14], managetoenablefasterlearningandconsumefewerresources
in some experimental settings compared to PNs-based DNNs.
01CMILab,UniversityofStirling,2UniversityofWolverhampton,Wolver- The main contributions of this paper are as follows:
hampton. 3Oxford Computational Neuroscience, Nuffield Department of
SurgicalSciences,UniversityofOxford,Oxford.
Email:ahsan.adeel@deepci.org i. WeintegratethefeaturesofCMI-inspiredCSTPNs[13]–
4202
guA
02
]CN.oib-q[
1v91011.8042:viXra[15] into a spiking TPNs-inspired local BDSP rule [5], (transmitting coherent information) than singlets (transmitting
demonstrating accelerated ‘local’ and ‘online’ learning incoherent information), hence they learn faster.
compared to BDSP approach alone. This validates the Thelocalweightsareupdatedbasedonlocalburstfiringinfor-
efficient and effective information processing capabilities mation.Thebursting(y)iscontrolledbyanovelasynchronous
of CS-TPNs, paving the way toward ‘local’, ‘on-the-fly’, modulatory function (MOD) [14], [15] that ensures neurons
‘online’ training and processing on neuromorphic chips. burst only when R is coherent with C, leading to the suppres-
ii. Addressed the limitations raised in the BDSP rule [5] by sion of conflicting information (singlets), and simultaneous
incorporating a thalamic circuitry proposed in [13], [14], faster processing and learning [14], [15]. The updated BDSP
[21], [22] into our spiking model. The thalamic circuitry learningruleisdetailedinTable1andcomparedwithstandard
(termed as U) stores coherent information across the BDSP and backpropagation (BP). Adapted from [5], somatic
sensory hierarchy and extracts synergistic information. membrane potential dynamics and the apical dendrite dynam-
The thalamic inputs are projected to the apical dendrites ics of CS-TPNs are represented by the following simplified
of the CS-TPNs. We show one of the ways how U (as a differential equations (see Eqs: 1 - 14):
Gate)canmediatethesignallingbetweenapicalandbasal
1 1 1
inputs, depending on their strength, thereby selectively 𝑉˙ = (𝑉 −𝐸 )+ 𝑀𝑜𝑑(𝐼 ,𝐼 )− 𝑤 (1)
strengtheningeithershort-termfacilitation(STF)orshort- 𝑠 𝜏 𝑠 𝑠 𝐿 𝐶 𝑠 𝑠 𝑐 𝐶 𝑠 𝑠
term depression (STD). We show that the incorporation
1
of this phenomenon, when integrated into a hierarchical 𝑤˙ =− (𝑤 )+𝑏𝑆(𝑡) (2)
𝑠 𝜏 𝑠
circuit, helps the network learn faster. 𝑤𝑠
iii. We show that the spiking neural network composed of The MOD function is capturing the complex interactions
CS-TPNs significantly reduces the required number of betweensomaticanddendriticcurrentsmoment-by-momentat
events, including both singlets and bursts, for the task at the soma, enabling intrinsic adaptation mechanism, allowing
hand compared to simple TPNs. nuanced control over the neuron’s firing rate and pattern.
iv. We scaled up the two-layer CS TPNs-driven CNN [14], 𝐼 ,including𝐼 ,𝐼 ,and𝐼 ,issolelyintegratedattheapical
𝐶 𝑃 𝐷 𝑈
[15] to a 50-layer deep CNN for AV speech processing, site using the TPN dynamics defined in [5]. The network
solely to demonstrate the scalability and information evolvesonthesametimescale,ascoincidentaldetectioniskey
processing efficiency of CS-TPNs in larger networks. In in two-point neuron operations [3], [24]. Since 𝐼 is the most
𝑈
some cases, it even surpasses the generalization capabili- reliable contextual field, constituting the coherent/synergistic
tiesofPNs-inspiredCNNs.Thisshowstheuniversalityof signalsacrosstheMSnetworkandacquiredbyfirstcondition-
ourcontext-sensitiveTPNs-inspiredprocessingregardless ally segregating the coherent and incoherent MS information
of the learning mechanism. streams and then recombining only the coherent multistream,
it has been awarded the maximum weight in the integration
II. CMI-INSPIREDCOOPERATIVECSTPNS+BDSP
function. To make it equivalent to G (coupling gate between
In the standard BDSP [5], the simple TPNs (Figs 1a and apical and basal sites) (Aru et al., TICS, 2020), its influence
1b) integrate the FF information at the soma to create an is embedded within the MOD function at the soma, such that
event (spike or burst) disregarding the contextual input. Once terms with Iu dominate the overall coupling between 𝐼 and
𝑆
the event is generated, the feedback information (context) at 𝐼 . The third term tends to zero in the absence of 𝐼 . The
𝑆 𝑈
the apical dendrites decides whether this already generated offset value of 2 is empirically calculated. Nevertheless, there
event should be a singlet (incoherent information) or a burst could certainly be a better way to model the whole dynamics.
(coherentinformation).AlthoughBDSPusestheapicalzoneto TheMODfunctionsareexplicitlydefinedinthelatersections.
solvethe‘online’creditassignmentproblem,theprocessingis Where 𝑉 represents the membrane potential of the so-
𝑠
stilldrivenbyPNs.Inshort,thelearningisinspiredbyTPNs, matic compartment (soma), 𝜏 = 16𝑚𝑠 is the membrane
𝑠
but the processing is not [14], [15], [22]. time constant, 𝐶 =370pF is the membrane capacitance, 𝐸 =-
𝑠 𝐿
In contrast, our approach shows that complex CS-TPNs (Figs 70mV is leak reversal potential, and 𝑤 is an adaptation
𝑠
1c and 1d) at the soma flexibly integrate moment-by-moment variable. The total current applied to the soma is represented
rich contextual current (including FB, proximal, distal, and by 𝑀𝑜𝑑(𝐼 ,𝐼 ,𝐼 ) that is the basal current 𝐼 modulated
𝑠 𝑐 𝑢 𝑠
thalamus (universal)) with the FF somatic current such that with the contextual current 𝐼 and the synergistic components
𝑐
the somatic current is amplified when both FF and context 𝐼 . The basal current here is the integrated synaptic inputs
𝑢
are coherent; otherwise, it is attenuated. This generates the (i.e., receptive excitatory and inhibitory inputs) along with
event only when the somatic and apical currents are coherent, basal noise. The contextual current is the accumulated effect
which is then translated into a singlet or a burst based on of proximal signal, distal signal, and feedback error with
the FB information. Simulation results show that this flexible dendriticnoise.Whereas,thesynergisticcomponents𝐼 isthe
𝑢
integration of somatic and contextual currents requires a extracted synergistic FF components. The adaptation variable
reduced number of overall events (both singlets and bursts) in 𝑤 is defined by the (2) where 𝑆(𝑡) is the spike train of
𝑠
the system to generate external stimuli (Fig 1d). Raster plots the neurons, and 𝑏 = 200 is the strength of spike-triggered
shown in the later sections show that CS-TPNs burst far more adaptation. A spike occurs, every time 𝑉 crosses a dynamic
𝑠Fig. 1. Signal propagation and burst control in TPNs combined with short-term plasticity. Here, we show that the spiking neural network composed of
CS-TPNssignificantlyreducestherequirednumberofevents,includingbothsingletsandbursts,forthetaskathandcomparedtosimpleTPNs.aContext-
insensitiveTPNswithBDSP.Thecontextualcurrent(𝐼𝑐)doesnotinfluencethesomaticpotential;therefore,thelearningisinspiredbyTPNs,buttheprocessing
is not, and is driven by point neuron conception. (b) Context-insensitive TPNs signal propagation and burst control [5]. The circuit has two populations of
neurons(Pop1,bottomandPop2,top),4000neuronseach.TheneuronsinPop1receiveexternalinputassomaticcurrent𝐼𝑠 andthoseinPop2receive
dendriticcurrent𝐼 𝑑.Thefeedforwardpass(somatosoma)isfromPop1toPop2throughshort-termdepression(STD)synapsessimilarto[5].Theoutput
fromPop1isalsoprojectedtoapopulationprovidingdisynapticinhibition(disk).Thefeed-backwardpass(somatodendriticcompartment)isfromPop2
toPop1throughtheshort-termfacilitaion(STF)synapse.Similarly,theoutputfromPop2somaisconnectedtoapopulationprovidingdisynapticinhibition
(square).cContext-insensitiveTPNswithBDSP,receivingFBaswellasP,D,andU.Bothlearningandprocessingareinfluencedbythecontextualcurrent.
Thecontextualcurrentcontrolsthemembranepotentialinawaythateventsoccuronlywhenthecontextualcurrentandsomaticcurrentarecoherent.Thisis
duetodensecoordinationandcooperationamongTPNs,amplifyingthetransmissionofcoherentinformationandsuppressingthetransmissionofincoherent
information. (d) Context-sensitive TPNs signal propagation and burst control. Neurons in both populations receive P, however, the D and U signals are set
tozeroforsimplicity.Thegoalhereistotestthecontext-sensitiveoperationandforthatPissufficient.Specifically,Pisthecontextualinformationcoming
fromtheneighbouringneuronswithinthesamepopulation.PleasenotethatbothpopulationshavethesameCS-TPNsdynamics.However,P,D,andUare
included in the XOR simulations in Fig 2. It can be seen that the response of BP and ER at apical and soma, respectively, for both standard TPNs and
CS-TPNs,afterinjecting𝐼𝑠inPop1,and𝐼 𝑑inPop2showdifferentbehaviour.Specifically,theCS-TPNsrequirefewerevents,includingsingletsandbursts,
torepresenttheinputsinusoidalsignal.
threshold(i.e.,−50𝑚𝑉).Subsequently,thethresholdincreases
1 1 1
by2𝑚𝑉 immediatelyfollowingaspikeandreturnsto−50𝑚𝑉 𝐸 =𝑉˙ = (𝑉 −𝐸 )+ 𝐼 − 𝑤 (3)
𝑒 𝜏 𝑒 𝑒 𝐶 𝑒 𝐶 𝑒
with a time constant of 27𝑚𝑠. After a spike occurs, the 𝑉 is 𝑒 𝑒 𝑒
𝑠
reset to a resting voltage 𝑉 = −70𝑚𝑉. These values of the 1 1
𝑟 𝑤˙ =− 𝑤 + 𝑎 (𝑉 −𝐸 ) (4)
parameters are adopted from the [5]. 𝑒 𝜏 𝑒 𝜏 𝑤 𝑒 𝐿
𝑤𝑒 𝑤𝑒
TheCS-TPNsmodelreceivesthecontextualsignals(i.e.,FB
1 1 1
(E),P,D,andU)attheapicaldendriticsiteandintegratesthem 𝑃 =𝑉˙ = (𝑉 −𝐸 )+ 𝐼 − 𝑤 (5)
𝑝 𝜏 𝑝 𝑝 𝐶 𝑝 𝐶 𝑝
individually. Their respective membrane potentials, termed 𝑝 𝑝 𝑝
𝑉 , 𝑉 , 𝑉 , and 𝑉 are defined by the following simplified 1 1
𝑒 𝑝 𝑑 𝑢
𝑤˙ =− 𝑤 + 𝑎 (𝑉 −𝐸 ) (6)
differential equations adopted from [5]: 𝑝 𝜏 𝑝 𝜏 𝑤 𝑝 𝐿
𝑤𝑝 𝑤𝑝Fig. 2. (a) CMI-inspired cooperative CS-TPNs + BDSP for XOR problem. Individual TPNs receive FB, P, D, and U inputs to conditionally segregate the
coherent and incoherent FF signals, respectively. In terms of their sequence, first, coherent and incoherent signals are segregated by the TPNs. Then, these
coherentsignalsarerecombinedbyPNs,extractingsynergisticFFcomponentsfromallthecoherentmultistreams.Thishappenswiththehelpofanadditional
ensemble representing U with a population of 50 PNs. U is broadcasted to TPNs along with the current local context [13], [18], [19]. (b) Impact of this
informationprocessingmechanism: anincreasedspeedin ‘local’and‘online’ learningandprocessingcan beobservedwhenP andDare integrated inCS
TPNs and when P, D, and U are integrated in CS TPNs, compared to BDSP alone (c) BDSP XOR output reconstruction in 250 epochs (d) BDSP + P +
DXORoutputin250epochs(e)BDSP+P+D+UXORoutputreconstructionin250epochs.NotethatcooperativeCSTPNs+BDSPcrossthetarget
thresholdfasterwiththesamenumberofneurons.
TABLEI
TRAINING:SUMMARYOFTHEMAINEQUATIONS.INTHEFIRSTCOLUMN,𝑟0ISTHERECEPTIVEINPUTAND𝑦 𝑙ISTHEOUTPUTOFTHETPNSWHICHIS
MODULATEDWITHCONTEXT𝑐 𝑙THROUGHTHENON-LINEARFUNCTION.THEBDSPALGORITHMADOPTSTHEBURSTINGRATEDEPENDINGONTHE
DIFFERENCEBETWEENTHECURRENTBURSTINGPROBABILITY𝑃 𝑙(𝑡)ANDTHEPREVIOUSPROBABILITY𝑃 𝑙(𝑡−1).ALTHOUGHTHELEARNING
EQUATIONSREMAINTHESAMEASSTANDARDBDSP,THEBURSTINGPROBABILITYANDBURSTINGRATESUSECOOPERATIVEOUTPUTHENCETHE
CHANGESARESENSITIVETOTHECONTEXTUALMODULATION.NOTETHATTHEOUTPUTNEURONSARENOTCONTEXT-SENSITIVEBUTONLYTHE
HIDDEN-LAYERNEURONS.MOREOVER,𝑌 𝑏𝑙 ISTHEFEEDBACKWEIGHTMATRIX.
CS-TPNs + BDSP TPNs + BDSP [23] PNs + BP [2]
𝑟 =𝑥 𝑒 =𝑥 𝑎 =𝑥
0 0 0
𝑦
𝑙
=𝑟 𝑙+𝑐 𝑙⊙(0.1+|𝑟 𝑙|)+𝑐 𝑙𝐼𝑢⊙(2+|𝑟 𝑙|) 𝑒
𝑙
=𝑓 𝑙(𝑊 𝑙𝑒 𝑙−1) 𝑎
𝑙
=𝑓 𝑙(𝑊 𝑙𝑎 𝑙−1)
where 𝑐
𝑙
is integrated context and 𝐼𝑢 is the synergistic component
𝑦 =𝑓 (𝑊 𝑦 ) 𝑒 =𝑓 (𝑊 𝑒 ) 𝑎 =𝑓 (𝑊 𝑎 )
𝐿 𝐿 𝐿 (𝐿−1) 𝐿 𝑙 𝐿 𝐿−1 𝐿 𝐿 𝐿 𝐿−1
𝑝
𝐿
=𝑝( 𝐿0)(1,1,...,1)𝑇 𝑝
𝐿
=𝑝( 𝐿0)(1,1,...,1)𝑇 𝑔
𝐿
=𝑓 𝑙′(𝑉 𝐿)⊙∇𝑎𝐿ℒ
𝑝
𝐿
=𝜁(𝑝 𝐿−ℎ(𝑦 𝐿))⊙∇𝑦𝐿ℒ 𝑝
𝐿
=𝜁(𝑝 𝐿−ℎ(𝑒 𝐿))⊙∇𝑒𝐿ℒ 𝑔
𝑙
=𝑓 𝑙′(𝑉 𝑙)⊙(𝑊 𝑙𝑇 +1𝑔 𝑙+1)
𝑢 =ℎ(𝑦 )⊙(𝑌 𝑏 ) 𝑢 =ℎ(𝑒 )⊙(𝑌 𝑏 )
𝑙 𝑙 𝑙 𝑙+1 𝑙 𝑙 𝑙 𝑙+1
𝑝 =𝜎(𝛽𝑢 +𝛼) 𝑝 =𝜎(𝛽𝑢 +𝛼)
𝑙 𝑙 𝑙 𝑙
𝑏 =𝑝 ⊙𝑦 𝑏 =𝑝 ⊙𝑒
𝑙 𝑙 𝑙 𝑙 𝑙 𝑙
𝑢 =ℎ(𝑦 )⊙(𝑌 ) 𝑢 =ℎ(𝑒 )⊙(𝑌 )
𝑙 𝑙 𝑏𝑙+1 𝑙 𝑙 𝑏𝑙+1
𝑝 =𝜎(𝛽𝑢 +𝛼) 𝑝 =𝜎(𝛽𝑢 +𝛼)
𝑙 𝑙 𝑙 𝑙
𝑏 =𝑝 ⊙𝑦 𝑏 =𝑝 ⊙𝑒
𝑙 𝑙 𝑙 𝑙 𝑙 𝑙
Δ𝑊 =𝜂𝛿𝑏 𝑦𝑇 Δ𝑊 =𝜂𝛿𝑏 𝑒𝑇 Δ𝑊 =−𝜂𝑔 𝑎𝑇
𝑙 𝑙 𝑙−1 𝑙 𝑙 𝑙−1 𝑙 𝑙 𝑙−1
1 1
𝑤˙ =− 𝑤 + 𝑎 (𝑉 −𝐸 ) (8)
𝐷 =𝑉˙
𝑑
= 𝜏1 (𝑉 𝑑−𝐸 𝑑)+ 𝐶1 𝐼 𝑑− 𝐶1 𝑤
𝑑
(7) 𝑑 𝜏 𝑤𝑑 𝑑 𝜏 𝑤𝑑 𝑤 𝑑 𝐿
𝑑 𝑑 𝑑must respond with a high output if only one input is active,
𝑔
𝐼 = (𝑓(𝑉 )+𝑓(𝑉 )−𝑓(𝑉 )) (9) and a low output if neither or both input pools are active.
𝑐 𝐶 𝑝 𝑒 𝑑
TheXORisaclassicalnon-linearexampletodemonstratethe
1 1 1 capability of a network. The network is initialized such that
𝑈 =𝑉˙ = (𝑉 −𝐸 )+ 𝐼− 𝑤 (10)
𝑢 𝜏 𝑢 𝑢 𝐶 𝐶 𝑢 the output ensemble treats any input combination as roughly
𝑢 𝑢 𝑢
equivalent.
1 1
𝑤˙ =− 𝑤 + 𝑎 (𝑉 −𝐸 ) (11) Foranalysis,thenetworkperformanceandneuralactivityof
𝑢 𝜏 𝑢 𝜏 𝑤 𝑢 𝐿
𝑤𝑢 𝑤𝑢 CS-TPNs is compared to a similar network but with context-
𝑔 insensitive TPNs. The cost function for the network is binary
𝐼 = 𝑓(𝑉 ) (12)
𝑢 𝐶 𝑢 cross entropy. The population size in each ensemble for
In the aforementioned equations, 𝜏 , 𝜏 , 𝜏 are membrane BDSP only and for CS-TPNs+BDSP network is 175 neurons.
𝑑 𝑔 𝑐
time constants (7𝑚𝑠 each). 𝐶 , 𝐶 , and 𝐶 are membrane The total population of these two networks is equal (i.e.,
𝑑 𝑔 𝑐
capacitance (170𝑝𝐹 each). Similarly, the 𝜏 , 𝜏 , and 𝜏 875 neurons). The CS-TPNs+BDSP network with U (CSM-
are adaptation time constants (30𝑚𝑠 each𝑤 )𝑑 . 𝛼𝑤𝑔 = 13𝑛𝑤 𝑆𝑐 TPNs+BDSP)hasanadditionalensemblerepresentingUwith
𝑤
is sub-threshold adaptation. 𝑔 = 1,300𝑝𝐴 in the eq(9) is a population of 50 PNs. To retain the network size, the input,
hidden and output ensemble population size is reduced to 150
the dendrosomatic coupling. The simulation of the proposed
neurons each. Thus the total population size of the CSM-
model uses the same settings as [5] including but not limited
TPNs+BDSPnetworkis800.Fig.2bshowstheconvergenceof
to dendrite-targeting inhibition, perisomatic inhibitions, noise,
the cost function over 250 epochs for each network. The cost
and synapses. The top-down effect of the apical compartment
is 𝐼 , represented by eq(9) and 𝐼 in eq(12) is the gating functions are compared to demonstrate the learning speed and
𝑐 𝑢
convergenceofBDSPalone(blue),CS-TPNs+BDSP(orange),
current referring to synergistic components.
andCSM-TPNs+BDSP(green).ItcanbeseenthatBDSPwith
Contrary to the multiplexing approach [25] used in BDSP
[5], the contextual modulatory function 𝑀𝑜𝑑(𝐼 ,𝐼 ) detailed CS TPNs learns faster than the BDSP, whereas, the BDSP
𝑠 𝑐
with CSM-TPNs learns even quicker. The network output
in [13], [19] is scaled as follows to suit the spiking neural net
is shown in Fig. 2c,d,e. See the region where the mean of
(SNN) in the CS-TPNs circuit.
the function at (0, 1) and (1,1) inputs is distant from the
𝑀𝑜𝑑(𝐼 ,𝐼 )=𝐼 +𝐼 (0.1+|𝐼 |) (13) thresholdinCS-TPNs+BDSPandCSM-TPNs+BDSP(Fig.2d,
𝑠 𝑐 𝑠 𝑐 𝑠
e)comparedtoBDSPalone(Fig.2c).Fig.3ashowstheraster
The scaling is necessary to align with the working range plots of standard BDSP and CS-TPNs+BDSP. The blue dots
valuesandthescalingiscalculatedempirically.Thishowever, represent the event rate and the red dots represent the bursts.
keeps the essence of the contextual field (C) overruling the Higher bursts are associated with the high output whereas,
typical dominance of the receptive field (R), and therefore the lesser events and bursts represent low output. It can be
discourages and encourages amplification of neural activity seen that the network with CS-TPNs distinguishes between
when C is weak and strong, respectively [13], [19]. As the high and the low outputs more clearly than BDSP alone
discussed earlier, to incorporate the synergistic components CS-TPNs.Specifically,itistobeobservedthatCS-TPNstend
U as the Gate, the modulatory function is further modified toremainlargelysilentwheninformationislessrelevant(close
(see Eq 14). to zero) but become active (bursting) when information is
relevant (close to one). It also implies that CS-TPNs burst
more when apical and basal, both the inputs are stronger,
𝑀𝑜𝑑(𝐼 ,𝐼 ,𝐼 )=𝐼 +𝐼 (0.1+|𝐼 |)+𝐼 𝐼 (2+|𝐼 |) (14)
𝑠 𝑐 𝑢 𝑠 𝑐 𝑠 𝑐 𝑢 𝑠
thus amplifying and suppressing the transmission of coherent
In the Eq 14, the gating current is 𝐼 𝑢. In this modified and conflicting information, respectively. This is because the
modulatory function, the third term incorporates both ‘U’ and neighbouring proximal TPNs and distal TPNs in the same
the integrated context ‘C’ with higher weight than the second and other ensembles influence the perception of each TPN in
term.InthepresenceofU,thethirdtermdominatestheoverall each ensemble. They are sharing their perceptions with each
results, whereas the third term tends to zero in the absence of otherwiththegoaltominimisetheconflictinginformationand
U. The offset value of 2 is empirically calculated. maximisethecoherentinformation,achievingharmonyacross
thenetwork.Additionally,itisnotablethatcontext-insensitive
III. RESULTS
TPNsinBDSPburstmorefrequentlythanCS-TPNs,implying
A. Shallow spiking XOR
that neurons are not sharing information and burst even when
To simulate the spiking CS-TPNs trained using BDSP, we the received information is not very relevant to the task
adopta3-layernetworkstructurepresentedin[5]withsimilar at hand. Fig. 3b shows the development of the membrane
settings for exclusive or (XOR) tasks (Fig. 2a). The proposed potentialofasinglerandomlyselectedneuron.Duringagiven
networkconsistsoftwoneuralpools(termedensemble)atthe time frame, the context-insensitive TPNs in BDSP fire three
input, two ensembles in the hidden layer and one ensemble at times more than CS-TPNs. The CS-TPNs fire only when the
the output. Each ensemble has a fixed population of intercon- proximal TPNs and the distal TPNs emphasize the need to
nected CS-TPNs. To perform an XOR function, the network fire, thus quickly distinguishing between the irrelevant andFig. 3. (a) Raster plots with 150 neurons: TPNs + BDSP (i) cooperative CS-TPNs + BDSP (ii). CS-TPNs tend to remain largely silent when information
is less relevant but become active (bursting) when information is relevant, compared to BDSP alone. Note the clear distinction between coherent (burst)
and conflicting signals (singlet). It implies that CS-TPNs are bursting far more (transmitting coherent information) than singlets (transmitting incoherent
information),hencelearnfaster.Thebottombluelinereflecteventrates.(b)itisnotablethatTPNsfiremorefrequentlythanCS-TPNs.(c)A50-layerdeep
CNN composed of CS-TPNs requires significantly fewer neurons at any time during training with better generalization capability (Table II) compared to a
deepCNNcomposedofPNs.ThisrevealstheuniversalapplicabilityofCS-TPN-drivenefficientinformationprocessingregardlessofthelearningmechanism
type.See:http://cmilab.org/research/
relevant events for the task at hand. It can be convincingly B. Deep AV speech processing with 50-layered CNN com-
concludedthatCS-TPNsaremorerobustandefficient,capable posed of CS-TPNs
of learning with a smaller population of neurons compared to
1) AV Dataset for Speech Enhancement [19]: The AV
the context-insensitive TPNs in BDSP.
ChiME3datasetwascreatedbyblendingthecleanGridvideos
TheFig1ashowstheresultsreproducedfromtheBDSPwhich
[26]withChiME3backgroundnoises[27](suchascafe,street
demonstratesthattheBDSPwithshort-termplasticitysupports
junction, public transport (BUS), and pedestrian areas) across
multiplexing of FF and FB signals. The graphs also reflect
SNR levels from -6 to 6dB, using a 3dB increment. The
the signal-processing behaviour of the neuronal population.
preprocessing steps include adding prior visual frames to the
Fig. 1a shows a conventional TPNs circuit and Fig. 1b is a
data. To capture temporal dynamics, the system utilizes six
circuitconsistingofCS-TPNs.Thecircuitarchitectureremains
preceding frames of both audio and visual data, enhancing
similar in both cases, having two populations of TPNs (Pop 1
the correlation between visual and auditory features. The
andPop2)andtwopopulationsofPNs(diskandsquare).The
Grid corpus includes recordings from 34 speakers, with each
PNs population provides inhibition. The external input to pop
speaker delivering 1000 sentences. From this group, a subset
1 is somatic current (𝐼 ) and that to pop 2 is dendritic current
𝑠 of 4 speakers was chosen (comprising two white females and
(𝐼 ). The output of Pop 1 is somatic input for Pop 2 whereas,
𝑑 two white males), each contributing 900 command sentences.
thePop2outputisfedtotheapicalatPop1asfeedback.For
This selection aims to maintain speaker diversity. Additional
simplicity,intheCS-TPNsnetwork(Fig.1b),theDandUare
information is detailed in [14], [28]. For training and eval-
settozeroandonlyPisconnected.ThetoptwographsinFig.
uation, the data is divided into a 75%-25% split, with one
1a, b reflect the activity in Pop 2, i.e., the burst probability
sample reserved from both the training and testing portions as
of the Pop 2 neurons against the applied current 𝐼 and event
𝑑 a representative proxy.
rate against the FF input from Pop 1, respectively. Similarly,
2) NetworkArchitecture:: Thetwo-layeredCNNcomposed
thetwobottomgraphsinFigs1aand1brepresenttheactivity
ofCS-TPNs[14]isscaledtoa50-layeredCNN.EachTPNin
at Pop 1. These graphs show the burst probability at pop 1
eachconvolutionallayerusedthenovelMODblock.TheMod
due to the feedback from pop 2 and the event rate against
block utilizes a structure and a modulatory transfer function
the applied current 𝐼 . The modulatory function of CS-TPNs
𝑠 as described in [14], [19], adjusting its scale based on the
resultsindynamicandvariableneuralactivitywithanincrease
input values range. The multi-modal AV network architecture
in burst probability and event rate compared to conventional
integratestwoparallelstreamsforaudioandvisualmodalities.
TPNs.Thissuggestsahigherresponsivesysteminfluencedby
The audio modality processes the magnitude of the noisy
contextual inputs while retaining the multiplexing behaviour
speech’s Short-Time Fourier Transform (STFT), while the
of BDSP.
visual modality processes a series of images through 2D
convolutionlayerstoalignwiththeaudiostream’sdimension.
These deep models are tasked with reconstructing a clean
STFT of the audio signal from both noisy audio and visualTABLEII
NEURALACTIVITY,PESQ,ANDSTOIRESULTS:THEIDEALCOLUMNREPRESENTSTHEREFERENCEVALUETHATDOESNOTAPPLYTONEURAL
ACTIVITY.
SNR Neural Activity PESQ STOI
Ideal Baseline Proposed Ideal Baseline Proposed Ideal Baseline Proposed
-6dB 0.34316 0.00102 1.92 1.23 1.33 0.78 0.56 0.63
0dB 0.31665 0.00102 2.09 1.40 1.58 0.83 0.63 0.71
6dB 0.33646 0.00102 2.28 1.52 1.77 0.87 0.71 0.78
TABLEIII
SPARSITYRATIO(SR)ANDMACSCOMPARISON:PNSVS.TPNS.GIVENANON-ZEROPROPAGATIONONLY,THENEURALACTIVITYCANBE
INTERPRETEDINTERMSOFTHEREDUCTIONOFMULTIPLICATIONSANDACCUMULATIONS(MACS)WHICHRESULTSINFLOATINGPOINTOPERATIONS
(FLOPS).THEMACSARECALCULATEDBASEDONTHENUMBEROFCONVOLUTIONSUSEDANDTHEMODULATIONFUNCTIONADOPTEDIN
CONTEXT-SENSITIVENETWORKS.THESPARSITYRATIOISCHOSENFROM0DBSNR.THEAFTER-TRAININGMACSARETHEINFERENCEMACS.THE
CONTEXT-SENSITIVENETWORKHAS24%MOREPARAMETERSDUETOCONTEXT-EXTRACTINGLAYERSANDMODULATION.THISISVISIBLEWITHMORE
MACS(I.E.,1.394TIMESMOREMACSFORCONTEXT-SENSITIVENETWORKS).HOWEVER,THEOVERALLREDUCTIONINMACSAFTERTRAININGIS
980TIMESFORCONTEXT-SENSITIVENETWORKSCOMPAREDTO3.16FORTHESTANDARDNETWORK
Model Sparsity Ratio (SR) @ 0dB MACs After training MACs MACs Reduction (times)
Baseline-(PNs) 0.31665 36827136 1161313 3.16
CS-TPNs 0.00102 51342336 52369 980.39
TABLEIV
FLOPSCOMPARISON:PNSVS.CS-TPNS
Model @ 0dB FLOPs After training FLOPs FLOPs Reduction (times)
Baseline-(PNs) 0.31665 73654272 23322625 3.16
CS-TPNs 0.00102 102684672 104738 980.39
inputs as mentioned in [29]. In each stream, the data passes model is detailed in [14] The MOD function is expressed as:
through convolution units that serve as feature extraction
𝑀𝑜𝑑(𝑅,𝐶)=𝜁[𝑅*𝐶*(𝑅2+2*𝑅+𝐶*(1+|𝑅|))] (15)
layers, each followed by a ReLU unit, a contextual modu-
lation, constituting a network block. Each modality employs Where 𝜁 is the activation function, ReLU6 in this case.
12 such blocks, with each containing two convolution units The Loss function is expressed as:
and following the aforementioned sequence including context
modulation. The context is derived exclusively from P and 𝐿𝑜𝑠𝑠=𝛽𝐸[𝑆𝐸(𝑍,𝑍^)]+𝛾𝐸[𝜖] (16)
D, where the audio stream, D is represented by visuals
The 𝑆𝐸 is the squared error between the clean speech 𝑍
and P by the audio itself, and vice versa for the visual
and the predicted speech 𝑍^. However, the second term with 𝜖
stream.Throughouttheseblocks,bothaudioandvisualsignals
is a differentiable approximation for the number of non-zero
undergo frequency domain reduction while maintaining the
activations’. The coefficients of the loss function are adjusted
time domain integrity. The outputs from the final blocks of
to make the secondary objective significantly less important
both streams are then averaged, pooled, concatenated, and fed
thanthemaingoal;inparticular,𝛾 issettoareallysmallvalue
into a dense layer followed by a sigmoid activation function.
inallexperiments.Themodelestimatesthecleanspectrogram
The network employs convolution layers with 16 filters and
whenthenoisyspectrogramandvisualimagesarefedasinput.
a kernel size of 3, and it is trained end-to-end using back-
Table1comparesthestandardPNs-inspiredCNN(baseline)
propagation. The specific loss function employed for this
with the proposed CS-TPNs inspired CNN in terms of neuralFig. 4. A 50-layer deep CNN composed of CS-TPNs requires significantly fewer neurons at any time during training with better generalization capability
(TableII)comparedtoadeepCNNcomposedofPNs.ThisrevealstheuniversalapplicabilityofCS-TPN-drivenefficientinformationprocessingregardless
ofthelearningmechanismtype.
activity, perceptual evaluation of speech quality (PESQ), and V. ACKNOWLEDGMENTS
short-time objective intelligibility (STOI). It can be seen (see
This research was supported by the UK Engineering and
Fig.4)thattheCS-TPNsinspiredCNNgeneralisesbetterthan
Physical Sciences Research Council (EPSRC) Grant Ref.
baseline with up to 330x fewer neurons for all SNRs. Same
EP/T021063/1. We would like to acknowledge Professor Bill
proportion could be observed in required number of MACs
Phillips and Professor Leslie Smith from the University
(Multiply-AccumulateOperations)andFLOPs(FloatingPoint
of Stirling, Professor Newton Howard from Oxford
Operations).
Computational Neuroscience, Professor Heiko Neumann
from the Institute of Neural Information Processing, Ulm
University, Professor Panayiota Poirazi from IMBB-FORTH
IV. CONCLUSION and Dr. Michalis Pagkalos from the University of Crete,
FoundationforResearchandTechnologyHellasfortheirhelp
The reliance on PNs and BP continues. Despite their and support in several different ways, including reviewing
remarkable performance improvements in a range of real- this work, appreciation, and encouragement.
world applications, including large language models (LLMs),
current AI technology based on them is rapidly becoming Code availability: The data and code supporting the
economically, technically, and environmentally unsustainable. findings of this study are available upon request.
The limitations of current AI systems therefore persist: issues
with scalability, slow and unstable training, processing of ir-
REFERENCES
relevantinformation,andcatastrophicforgetting—allrequiring [1] M.Ha¨usser,“Synapticfunction:dendriticdemocracy,”CurrentBiology,
a massive number of electronic components, which leads to vol.11,no.1,pp.R10–R12,2001.
trade-offs between cost, speed, power, size, accuracy, gener- [2] Y.LeCun,Y.Bengio,andG.Hinton,“Deeplearning,”nature,vol.521,
no.7553,pp.436–444,2015.
alization, and inter-chip congestion. CMI-inspired CS-TPNs
[3] M.E.Larkum,J.J.Zhu,andB.Sakmann,“Anewcellularmechanism
drivendeepnetworksproposedherehaveshownthecapability forcouplinginputsarrivingatdifferentcorticallayers,”Nature,vol.398,
of processing large amounts of heterogeneous real-world AV no.6725,pp.338–341,1999.
[4] W.A.Phillips,TheCooperativeNeuron:CellularFoundationsofMental
datausingasignificantlysmallernumberofneuronscompared
Life. OxfordUniversityPress,2023.
to standard PNs-inspired deep nets, with better generalization [5] A.Payeur,J.Guerguiev,F.Zenke,B.A.Richards,andR.Naud,“Burst-
in some cases. The demonstration of biologically plausible dependent synaptic plasticity can coordinate learning in hierarchical
circuits,”Natureneuroscience,vol.24,no.7,pp.1010–1019,2021.
CS-TPNs simulation with BDSP shows that this efficient
[6] W.Greedy,H.W.Zhu,J.Pemberton,J.Mellor,andR.P.Costa,“Single-
information processing approach is universal regardless of the phase deep learning in cortico-cortical networks,” Advances in Neural
learning algorithm. Future work includes further scaling up InformationProcessingSystems,2022.
[7] T. P. Lillicrap, A. Santoro, L. Marris, C. J. Akerman, and G. Hinton,
the proposed artificial and spiking TPNs inspired neural nets
“Backpropagationandthebrain,”NatureReviewsNeuroscience,vol.21,
for a range of different real-world problems. no.6,pp.335–346,2020.[8] J.Guerguiev,T.P.Lillicrap,andB.A.Richards,“Towardsdeeplearning
withsegregateddendrites,”Elife,vol.6,p.e22901,2017.
[9] J.Sacramento,R.PonteCosta,Y.Bengio,andW.Senn,“Dendriticcorti-
calmicrocircuitsapproximatethebackpropagationalgorithm,”Advances
inneuralinformationprocessingsystems,vol.31,2018.
[10] S. G. Sarwat, T. Moraitis, C. D. Wright, and H. Bhaskaran, “Chalco-
genide optomemristors for multi-factor neuromorphic computation,”
Naturecommunications,vol.13,no.1,pp.1–9,2022.
[11] Y. Zhang, S. Xiang, X. Cao, S. Zhao, X. Guo, A. Wen, and Y. Hao,
“Experimentaldemonstrationofpyramidalneuron-likedynamicsdomi-
natedbydendriticactionpotentialsbasedonavcselforall-opticalxor
classification task,” Photonics Research, vol. 9, no. 6, pp. 1055–1061,
2021.
[12] M. Ward and O. Rhodes, “Beyond lif neurons on neuromorphic hard-
ware,”FrontiersinNeuroscience,vol.16,2022.
[13] A.Adeel,“Consciousmultisensoryintegration:Introducingauniversal
contextual field in biological and deep artificial neural networks,”
FrontiersinComputationalNeuroscience,vol.14,p.15,2020.
[14] A. Adeel, M. Franco, M. Raza, and K. Ahmed, “Context-sensitive
neocorticalneuronstransformtheeffectivenessandefficiencyofneural
informationprocessing,”arXiv:2207.07338,2022.
[15] A. Adeel, A. Adetomi, K. Ahmed, A. Hussain, T. Arslan,
and W. Phillips, “Unlocking the potential of two-point cells
for energy-efficient training of deep nets,” The IEEE Transac-
tions on Emerging Topics in Computational Intelligence (in-press);
https://arxiv.org/abs/2211.01950,2022.
[16] W. A. Phillips, “Cognitive functions of intracellular mechanisms for
contextual amplification,” Brain and Cognition, vol. 112, pp. 39–53,
2017.
[17] M.Pagkalos,R.Makarov,andP.Poirazi,“Leveragingdendriticproper-
tiestoadvancemachinelearningandneuro-inspiredcomputing,”arXiv
preprintarXiv:2306.08007,2023.
[18] L. e. a. Muckli*, “The cortical microcircuitry of predictions and
contextaˆC“amulti-scaleperspective,”Sep.2023.[Online].Available:
https://doi.org/10.5281/zenodo.8380094
[19] A. Adeel, A. Adetomi, K. Ahmed, A. Hussain, T. Arslan, and
W. Phillips, “Unlocking the potential of two-point cells for energy-
efficient and resilient training of deep nets,” IEEE Transactions on
EmergingTopicsinComputationalIntelligence,2023.
[20] A.Adeel,J.Muzaffar,K.Ahmed,andM.Raza,“Cooperationisallyou
need,”arXivpreprintarXiv:2305.10449,2023.
[21] J.Aru,M.Suzuki,andM.Larkum,“Cellularmechanismsofconscious
processing,”TrendsinCognitiveSciences,vol.25,102021.
[22] A. Adeel, “Cellular foundations of constrained rationality,” European
MolecularBiologyOrganization(EMBO)Workshop:PrinciplesofDen-
driticFunctionandComputation,21–24May2024,2024.
[23] M. I. Belghazi, A. Baratin, S. Rajeshwar, S. Ozair, Y. Bengio,
A. Courville, and D. Hjelm, “Mutual information neural estimation,”
inInternationalConferenceonMachineLearning. PMLR,2018,pp.
531–540.
[24] J.Aru,F.Siclari,W.A.Phillips,andJ.F.Storm,“Apicaldrive—acel-
lularmechanismofdreaming?”Neuroscience&BiobehavioralReviews,
vol.119,pp.440–455,2020.
[25] R.NaudandH.Sprekeler,“Sparseburstsoptimizeinformationtransmis-
sioninamultiplexedneuralcode,”ProceedingsoftheNationalAcademy
ofSciences,vol.115,no.27,pp.E6329–E6338,2018.
[26] M. Cooke, J. Barker, S. Cunningham, and X. Shao, “An audio-visual
corpusforspeechperceptionandautomaticspeechrecognition(l),”The
JournaloftheAcousticalSocietyofAmerica,vol.120,pp.2421–4,12
2006.
[27] J. Barker, R. Marxer, E. Vincent, and S. Watanabe, “The third
‘chime’speech separation and recognition challenge: Analysis and out-
comes,”ComputerSpeech&Language,vol.46,pp.605–626,2017.
[28] A. Adeel, M. Gogate, and A. Hussain, “Contextual deep learning-
based audio-visual switching for speech enhancement in real-world
environments,”InformationFusion,vol.59,pp.163–170,2021.
[29] M. Gogate, K. Dashtipour, A. Adeel, and A. Hussain, “Cochleanet: A
robust language-independent audio-visual model for real-time speech
enhancement,”InformationFusion,vol.63,pp.273–285,2020.
VI. SUPPLEMENTARYMATERIALFig.5. Effects of different time scales on the XOR task for the population size of 200 neurons on Standard BDSP and CS-TPNs+BDSP.aandc,
Comparison of costs for different duration of examples T (in s). T = 8s (green line) is the duration used in Fig. 2. b and d, Output event rate (ER) after
learningforthecasesina,cclassifyingbetween’true(1)’and’false(o)’fortheXOR.Thecostsforthestandardnetwork(a)reachaminimumof1.5starting
atepoch250.Thecontext-sensitivenetworkattainsthecostof1.0startingatepoch120.ItisevidentfromthegraphsthattheFFcontext-sensitivenetwork
learnsfasterandbetter.