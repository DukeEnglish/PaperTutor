Inside the Black Box: Detecting Data Leakage in
Pre-trained Language Encoders
YuanXina,ZhengLia,NingYub,DingfanChena,*,MarioFritza,MichaelBackesa andYangZhanga
aCISPAHelmholtzCenterforInformationSecurity
bNetflixEyelineStudios
Abstract. Despite being prevalent in the general field of Natural require direct access to target models that are perfectly trained for
LanguageProcessing(NLP),pre-trainedlanguagemodelsinherently thedatatheattackeraimtoinfer,withnoobserveddiscrepancy.
carryprivacyandcopyrightconcernsduetotheirnatureoftraining Tofillthisgap,weinitiatethefirstsystematicstudyoftheinfor-
onlarge-scaleweb-scrapeddata.Inthispaper,wepioneerasystem- mationleakagerisksinherentinPLEsbyinvestigatingthevulnera-
atic exploration of such risks associated with pre-trained language bilityofPLEframeworkstoadversariesattemptingtoinfertheirpre-
encoders, specifically focusing on the membership leakage of pre- training data. Specifically, we introduce an attack pipeline for the
trainingdataexposedthroughdownstreammodelsadaptedfrompre- most realistic scenario, where service providers build downstream
trained language encoders—an aspect largely overlooked in exist- modelsbyintegratingPLEsinternallyandonlygrantaccesstothe
ing literature. Our study encompasses comprehensive experiments downstreammodels(insteadofdirectaccesstothePLEs)inablack-
acrossfourtypesofpre-trainedencoderarchitectures,threerepresen- boxmanner.
tativedownstreamtasks,andfivebenchmarkdatasets.Intriguingly, To systematically explore the potential risks associated with
our evaluations reveal, for the first time, the existence of member- the usage of PLEs, we perform an extensive experimental inves-
shipleakageevenwhenonlytheblack-boxoutputofthedownstream tigation spanning four distinct PLE architectures and five down-
modelisexposed,highlightingaprivacyriskfargreaterthanprevi- stream datasets across three representative downstream tasks. In-
ouslyassumed.Alongside,wepresentin-depthanalysisandinsights triguingly, our evaluations uncover considerable data membership
towardguidingfutureresearchersandpractitionersinaddressingthe leakagewithinPLEs,evenwhenonlytheoutputofthefinaldown-
privacyconsiderationsindevelopingpre-trainedlanguagemodels. stream model is exposed. This leakage persists irrespective of the
PLE architecture and the type of the downstream tasks, indicating
amoresevereriskthanpreviouslyanticipated,especiallyconsider-
1 Introduction
ingthatthischallengingsettinghasbeenostensiblyviewedas“safe”
Pre-trained language encoders (PLEs), exemplified by BERT [7], forusage.Ourstudyisfurtherenrichedbyacomprehensiveanalysis
underpin the recent advancements in the general field of natural thatofferskeyinsightsintotheprimaryfactorsassociatedwiththe
language processing and have found widespread use across vari- privacy risk of PLEs, with which we aim to increase model devel-
ousapplicationscenarios[39,38,23].Commonly,PLEsaretrained opers’awarenessofPLEvulnerabilitiesandtomotivatetheincorpo-
on large-scale text corpora to encapsulate general linguistic pat- ration of privacy considerations into model design and training for
terns,subsequentlyfine-tunedtorefinetheirinternalrepresentations privacy-preservingdownstreamusage.
forspecificdownstreamtasks[34,30].Typically,modeldevelopers
leveragePLEsbyintegratingafewlayers,explicitlydesignedforthe
2 Relatedwork
intendeddownstreamtasks,tothesepre-trainedencoders,followed
byfine-tuningthemodelondownstreamdata.Thisprocessenables Pre-trainedLanguageEncoders(PLEs). TheuseofPLEsisper-
themodelstobecustomizedforabroadrangeoftasks,suchastext vasive in the NLP domain due to their ability to capture generic
classification,namedentityrecognition(NER),andquestionanswer- linguisticcharacteristicsofnaturallanguages,whichareuniversally
ing(Q&A). beneficial for various downstream tasks that rely on the semantics
Despite the extensive application of PLEs, the inherent risks as- of the representation [7, 3, 16]. In particular, PLEs stand out for
sociated with information leakage and copyright infringement of theirrelativelylightweightusageandtheflexibilitytheyofferbypro-
thepre-trainingdatathroughtheuseofPLEsremainlargelyunder- vidingsemantic-awareembeddings,despitetherecentdevelopment
explored.Specifically,itremainsunclearwhetherwecandetermine oflarge-scaledecoder-basedpre-trainedmodelslikeGPT-4[2]and
ifaPLE-basedlanguagemodel,givenapieceoftextandblack-box LAMMA[36].WefocusonthemostpredominantinstanceofPLEs,
access,hasbeenpre-trainedontheprovidedtext.Whilethisproblem specifically,BERT[7],alongwithitsprevalentvariants[14,19,40].
canbeformulatedasaninstanceofMembershipInferenceAttacks
Downstream Tasks. PLEs are typically adopted within the pre-
(MIAs), existing MIAs typically necessitate assumptions about the
train and fine-tune paradigm, where PLEs are pre-trained on large
attackers’ access that may not align with practical usage scenarios
corpora through self-supervised learning (e.g., masked language
involving PLEs [21, 12, 29]. In particular, all prior attack surfaces
modelingandnextsentenceprediction),andaresubsequentlyinte-
∗CorrespondingAuthor.Email:dingfan.chen@outlook.com grated into downstream models while being fine-tuned to achieve
4202
guA
02
]LC.sc[
1v64011.8042:viXraPre-training Data Fine-tuning Data
Pre-trained Encoder Downstream Model
pre-training BERT, ALBERT, RoBERTa, XLNet fine-tuning
Model Training & Deployment
Local Member Downstream Model
Attack Model
1: member
Local Non-Member
0: non-member
Attack Training
Downstream Model
Attack Model
Test Data
1/0？
query member/non-member?
Attack Inference
Figure1. Overviewoftheworkflow.
specific objectives for various tasks [27]. In Section 4, we intro- pre-training dataset D ={x }Npre with self-supervision objec-
pre i i=1
duce our attack pipeline, designed for seamless applicability to di- tives. The downstream model M is constructed by appending lay-
versedownstreamtasks.Forourexperimentalevaluation,wefocus
ers,whichmaptheembeddingstothefinaloutputpredictionspace,
ontherepresentativetasks,namelytextclassification,namedentity atopthePLEs.Specifically,M(x)=g (cid:0) f (x)(cid:1) ,whereg denotes
ϕ θ ϕ
recognition(NER),andquestion&answering(Q&A). thetask-specificdownstreamlayerswithparametersϕ.Thedown-
DataLeakageofPre-trainingModels. Pre-trainingonlarge-scale stream model M is fine-tuned on the labelled fine-tuning dataset
web-scraped data offers substantial advantages in learning generic D fine={(x i,y i)}N i=fi 1newithy idenotingthetask-specificlabels,e.g.,
linguisticrepresentations,butmayraiseprivacyandcopyrightcon- classindexfortextclassificationtasks.Duringthefine-tuningpro-
cerns [15]. This is particularly relevant in the context of stringent cess,theparametersforboththeencoderθandthedownstreamlay-
legal regulations, such as the General Data Protection Regulation ersϕareupdated.Ifnecessaryforclarity,wemaydenotetheupdated
(GDPR). Recent studies have highlighted these concerns, demon- encoder parameters as θ′ to distinguish them from the pre-trained
strating privacy leakage of the training data for encoder-decoder model’sparameters.ThedatasetsizeN fineistypicallymuchsmaller
modelstrainedfromscratch[33]anddeployeddecodermodels[5], thanN pre duetothehigherdifficultyandworkloadinvolvedinob-
aswellasofthefine-tuningdatausedinthepre-trainingfine-tuning taininglabelleddata.
pipeline[22].Inparticular,existingstudiesonpre-trained(encoder)
models[21,12,35]assumesdirectaccessoftheattackertotheen- 3.2 MembershipInferenceAttacks
coder.This,however,maynotbeapracticalpresumptionasPLEsare
typicallyintegratedinternallyintothedownstreamservicemodels. A membership inference attack refers to a privacy attack where an
Inourwork,wedelveintoamorerealisticandchallengingsce- adversaryattemptstodeterminewhetheraparticularsamplewaspart
nario:ourstudydoesnotrelyondirectaccesstotheoriginalPLEs ofthetrainingsetusedtotrainatargetmachinelearningmodel[11,
thathavebeenpre-trainedwiththedataweaimtoinfer.Thismore 4,41].Inthiscontext,alltrainingdataareconsideredas“members”
closelymirrorspracticalusagescenariosbutimpliesacertainlevel whileanydatanotincludedinthetrainingset(i.e.,theunseendata)
ofdiscrepancybetweenthetargetPLEsandthefinaloutputwehave areregardedas“non-members”.
accessto,whichthenraisestheneedforamorerefinedattackdesign WeformalizetheattackerasabinaryclassificationmodelA,which
toeffectivelyextractprivateinformation.Moreover,thisdiscrepancy receivesaquerysamplexandthecorrespondingoutputfromatarget
isfurtheramplifiedbythefine-tuningprocess,whichadjuststhepa- modelM(x).Inthiswork,theattacker’sgoalistoinferwhetherthe
rametersofthePLEstobettersuitthenewdownstreamtask.These givensampleisinsidethepre-trainingsetofthetargetmodel,i.e.,
inconsistencies complicate the assessment of a model’s vulnerabil- A(x,M(x))=1(cid:2) x∈D pre(cid:3) with1denotingtheindicatorfunction
itytoattacks,potentiallyleadingtoprevioushastyclaimsregarding andD prerepresentingthepre-trainingdataset.
the “safe” usage of PLEs, while we question such claims with our
comprehensiveevaluation. 4 AttackMethod
4.1 ThreatModel
3 Formulation
3.1 TargetModels Attacker’sGoal.Inthiswork,wefocusontheprivacyleakagesof
thepre-trainingdataofPLEs.Specifically,theattackeraimstoin-
WedenoteaPLEasf θ(parametrizedbyθ)whichmapsagiventex- ferwhetheragivensamplewaspartofthepre-trainingdatasetused
tual input x to its embedding f θ(x). PLE is trained on unlabelled totrainthePLE.Theattackerhasaccesstothedownstreammodel,pre-training pre-training pre-training
unseen unseen unseen
(a)Pre-trainedBERT (b)Fine-tunedBERT (c)Downstreammodel
Figure2. t-SNEvisualizationofBERTembeddings.Thepre-trainingandunseensamplesareplottedasredandbluedots,respectively.(a)Embeddings
directlyobtainedfromPLEs,i.e.,f θ(x).(b)Embeddingsobtainedfromtheencoderafterfine-tuning,whichcorrespondstof θ′(x)withθ′̸=θ.(c)Embeddings
obtainedfromthedownstreammodel,i.e.,g ϕ(f θ′(x)).Fine-tuningisconductedontheAG’sNewsdataset.
denotedasM(·)=g ϕ(cid:0) f θ′(·)(cid:1) ,wheref θ′ representsthefine-tuned billionsoflocalsamplestoconstructa“shadowmodel”thatisoften
PLE,andg isthedownstreamtask-specifichead.Thekeychallenge adoptedinpreviousstudiesformembershipinference[31,28].
ϕ
fortheattackerliesinthefactthatthePLEmayhaveundergonefine-
tuning,meaningitsparametershavebeenupdatedfromtheiroriginal
4.2 Intuition
pre-trainedstate,i.e.,θ′ ̸= θ.Despitethis,theattackerseekstoin-
ferthemembershipstatusofthepre-trainingdatathatinfluencedthe
The main underlying principle of MIAs is the strategic use of the
initialtrainingofthePLE,evenafterfine-tuninghasaltereditspa-
memorizationeffectofmemberdataintargetmodels[31,28,9,18].
rameters.
Modern deep learning models, while exhibiting substantial expres-
Attacker’s Background Knowledge Typically, the background sivecapacityduetotheirlargenumberofparameters,arealsosus-
knowledgeconsideredforMIAsfallsintotwodimensions:(1)thear- ceptibletoinherentgeneralizationissues.Thisisprimarilyattributed
chitectureoftargetmodels;(2)thedistributionoftargetpre-training totheempiricalriskminimizationformulation,whichtendstopro-
dataset. moteoverlearning/overfittingandmemorizationoftrainingdata.As
Alongthefirstdimension,weconsiderthemostgeneralandrealis- a result, models typically display distinct behaviors when queried
ticsettingwhere(1)theattackerhasnoknowledgeofthearchitecture withbothmemberandnon-memberdata,whichcanbeexploitedby
ofthetargetPLEs;(2)theattackerhasonlyblack-boxaccesstothe potentialattackerstodifferentiatebetweenthetargetmodel’straining
downstreammodelM,whichimpliesitcanonlyinputqueriesand andunseendata[4,25,9,21].
receivepredictionswithoutknowingtheinternals;(3)theaccessible
downstreammodelMhasbeenfine-tunedtoadapttodownstream
4.3 AttackPipeline
task,resultinginparameterupdatesofthePLEsandapotentialin-
formation loss regarding the membership of its pre-training data.
Training.Theconstructionoftheattacktrainingdatasetisachieved
This scenario closely mirrors real-world usage of PLEs, as service
through pairing the black-box output from the target downstream
providerstypicallysharetask-specificmodelsadaptedfromPLEsto
model M when queried using the local data (Section 4.1), with
thepublicaspartof“machinelearningasaservice”,leadingtheat-
thebinarymembershipindicators,where“1”representsknownpre-
tackerstoaccesstheoutputofthedownstreammodelMratherthan
trainingsamplesand“0”denotesunseensamples.Theseindicators
direct access to the target PLEs. Notably, such scenario is largely
actasthetargetpredictionlabelsfortheattacker.
under-explored in the general field of trustworthy learning, poten-
Formally, the attack model A (represent as a neural network) is
tiallygivingrisetoafalsesenseofprivacypreservationwithinthis (cid:8) (cid:12) (cid:9) (cid:8) (cid:12) (cid:9)
trained on (M(x),1)(cid:12)x ∈ S
pre
∪ (M(x),0)(cid:12)x ∈ S
non
usecasescenario.
withS andS beingthelocalpre-trainingandunseensample
pre non
Regarding the second dimension, in line with previous stud-
set(Section4.1),respectively.Specifically,theattackmodelisgiven
ies [28], we presume that the attacker has access to a very small
themodelresponsesM(x)asinput,andtrainedwiththebinarycross
subset of the pre-training data, denoted as S (i.e., S ⊂ D
pre pre pre entropyobjectivewhiletakingthemembershipindicatorasthetarget
and|S | ≪ |D |).Theattackeralsohastheabilitytocompilea
pre pre predictionlabels.
smallsetoflocalnon-memberdata,denotedasS .Thesedatasets
non
are subsequently used to train the attack model A (refer to Sec- Inference. Afterbeingtrained(onthelocaldataset)todistinguish
tion4.3)whilewedetailtheinvestigation(andpotentialrelaxation) systematic disparities within the model’s responses (see Figure 2
oftheconstructionofsuchdatasetinSection5.2.Suchanassump- for a visual illustration) for its pre-training versus unseen data, the
tionmaynotbeimplausibleinreal-worldscenarios,consideringthat attacker is then able to process a query text input and determine
PLEstypicallyutilizebillionsofweb-scrapeddatasamplesforpre- itsmembershipstatus.Specifically,ifthequeryinputmoreclosely
training.Itisconceivablethatanattackermightmanagetogathera alignswiththehigherconfidencescoreforthe“pre-training”class
verysmallfractionofsuchvoluminouspre-trainingdata.Moreover, asopposedtothe“unseen”class,theattackerwillpredictitasapre-
thisassumptionismorefeasiblethanthedauntingtaskofcollecting trainingsample.1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 DownstreamTasks 0.4 DownstreamTasks 0.4 DownstreamTasks
Classification(SST) Classification(SST) Classification(SST)
Classification(AG’sNews) Classification(AG’sNews) Classification(AG’sNews)
0.2 Classification(YelpReviewFull) 0.2 Classification(YelpReviewFull) 0.2 Classification(YelpReviewFull)
NER NER NER
Q&A Q&A Q&A
0.0 0.0 0.0
BERT ALBERT RoBERTa XLNet BERT ALBERT RoBERTa XLNet BERT ALBERT RoBERTa XLNet
(a)Accuracy (b)Precision (c)Recall
Figure3. AttackperformancefordifferentPLEarchitectures(BERT,ALBERT,RoBERTa,XLNet)ontextclassification,NERandQ&Atasks.
Precision Recall Precision Recall
1.0 1.0 1.0 1.0
0.9 0.9 0.9 0.9
0.9000 0.8713 0.8707 0.9100 0.9143 0.9410 0.8787 0.8333 0.8472 0.7610 0.8150 0.7990
0.8 0.8 0.8 0.8
0.7 0.7 0.7 0.7
0.8926 0.8831 0.8795 0.6 0.7873 0.9143 0.8787 0.6 0.9866 0.9940 0.9175 0.6 0.9610 0.9940 0.9900 0.6
0.5 0.5 0.5 0.5
0.4 0.4 0.4 0.4 0.9000 0.8870 0.9013 0.9256 0.6573 0.7733 0.9239 0.9506 0.8878 0.9470 0.9250 0.9340
0.3 0.3 0.3 0.3
Wiki Books Mixture 0.2 Wiki Books Mixture 0.2 IMDB AX CoLA 0.2 IMDB AX CoLA 0.2
(a)Relaxation-I (b)Relaxation-II
Figure4. Attackperformancewithrelaxationofpre-trainingdatasets(Relaxation-I)andrelaxationofnon-memberdatasets(Relaxation-II)onNERdown-
streamtask:Wiki(Wikipedia),Books(BooksCorpus),Mixture(Wikepedia+BooksCorpus).X-axis:attacktrainingdataset.Y-axis:attacktestingdataset.
Evaluation. Theattackperformanceisevaluatedonunseentesting andBooksCorpus[7]—asthesearecommonlyusedacrossallfour
samplesthataredistinctfromtheattacktrainingset.Whileusingar- PLEs,andwecategorizethemasD fortheevaluation.
pre
bitrarynon-membertestingdatacouldbehelpfulforcertainpractical
Downstream Models and Fine-tuning Data. As elaborated in
scenarios[20],weoptfornon-membertestingsamplesthatfollows
Section4.1,weevaluatedthemostchallengingandrealisticscenario
a similar distribution as the pre-training set, for a rigorous evalu-
inwhichtheattackercanonlyaccesstheoutputsfromdownstream
ation of MIA. Specifically, we adopt general-purpose natural texts
modelsthathavebeenadaptedfromPLEs,whilethesedownstream
from the GLUE benchmark as the non-member samples. We also
models may be utilized for any given tasks. For extensiveness, we
explicitly test the setting in which we enforce the (semantic) simi-
consideredsixbenchmarkdatasetsforD ,referringtothreerepre-
laritybetweenthememberandnon-memberinstances.Thisisdone fine
sentativeNLPtopics.Fortextclassification,weadopttheSST,AG’s
byrephrasingthepre-trainingsamplesusingathird-partylanguage
News, and Yelp Review Full [32, 42] datasets. The CoNLL2003
model(e.g.,GPT-3.5)togeneratethenon-membersamplesfortest-
dataset was used for the NER studies [17], while the SQuADv1.0
ing(SeeSection5.2.5fordetailedinformation).
datasetwerechosenforthe Q&Atask[1].
AttackModelandAttackTraining&EvaluationData. Forcon-
5 Experiments
structing the attacker’s local training dataset, we randomly chose
5.1 ExperimentalSetup 30000 entries from the pre-training data used across all PLEs,
which represents a tiny fraction of the total dataset: the total size
PLEs and Pre-training Data. In our experiments, we investi- of Wikipedia and BooksCorpus are 16GB, (and we investigate the
gate four state-of-the-art architectures of PLEs: BERT [7], AL- possibilitytofurtherreducesuchfractionin5).Werefertothissub-
BERT [14], RoBERTa [19], XLNet [40]. We adopt well-trained set as the attacker’s local members set, denoted as S pre. We then
PLEs, which are publicly available online, as the targets for our opted for third-party datasets to constitute theattacker’s local non-
attacker1. This is notably more realistic than the majority of exist- memberdatasets,denotedasS non.Thesedatasetsaredistinctfrom
ingworkthatisconductedinlaboratoryenvironments,whichgen-
boththepre-trainingandfine-tuningdatasets.Morespecifically,the
erallyinvolvesre-trainingthetargetmodelfromscratchusingare- localnon-memberdatasetsconsistof15,000randomsamplesfrom
ducedsetoftrainingdatasamples.WhiledifferentPLEsmayemploy IMDB,CoLA,andAXdatasets,whichserveaspartoftheGLUE
their own pre-training data, we focus on two datasets—Wikipedia benchmarkdataset[7].Forenhancedgeneralization,weuseamix-
tureofthesethreedatasetsasnon-members.
1ThePLEsusedinthispaperaredownloadedfromhttps://huggingface.co/ For attack model, we constructed a three-layer Multilayer Per-
models ceptron(MLP)asthemodelarchitecture,whichtakestheoutputof
ycaruccA
erutxiM
skooB
ikiW
erutxiM
skooB
ikiW
noisicerP
ALoC
XA
BDMI
llaceR
ALoC
XA
BDMIdownstreammodelsM(x)asinputandpredictthebinarymember- Embeddings(t-SNE)2 asseeninFigure2(a).Followingthis,wein-
ship indicator variable. Given the variety of downstream tasks, the tegratethepre-trainedBERTintoadownstreammodelandconduct
dimension of the weight parameters in the first layer of the attack fine-tuningonAG’sNewsdatasets.Similarly,weplotthet-SNEvi-
modelisadjustedtosuitdifferenttasks.Theattackmodelistrained sualizations of the fine-tuned BERT embedding (i.e., f θ′(x)), and
for100epochswithalearningrateof1e-2,usingtheAdam[13]op- thedownstreammodeloutputs(i.e.,g ϕ(f θ′(x)))inFigure2(b)and
timizer.Bydefault,weseta5:1ratioforpartitioningattacktraining 2(c),respectively.
and evaluation data, and adopt a balanced partition (maintaining a Our findings lead to several key observations together with in-
1:1ratio)forthememberandnon-memberevaluationset.Addition- sights that can be beneficial for future development of pre-trained
ally,weconductadetailedinvestigationintotheimpactofthesize models:
andtypeoftheattacker’slocaldatasetontheattackperformancein • Original PLEs display a notable difference when being queried
Figure4.
with its pre-training and unseen data samples, inevitably leav-
ingcuesforattackerstoinferthemembershipinformationofthe
EvaluationMetrics. Inlinewithpreviousstudies[31,28,9],we
pre-trainingdatasamples.Thisnecessitatesprivacyconsiderations
regardtheMIAasabinaryclassificationtaskandevaluatetheattack
andcarefulcensorshipofthepre-trainingdatasetbeforePLEsare
performanceacrossstandardmetricsincludingaccuracy,precision,
madepubliclyavailable.
recall,andF1-score,whilehighervaluesofthesemetricssuggesta
moreeffectiveattackandconsequentlyahigherriskofdataleakage • Intriguingly,PLEsconsistentlyshowadisparityintheirresponse
fromthePLEs. tothepre-trainingandunseendata,evenpostfine-tuning.Thisin-
dicatesthefeasibilityofanattackandtheneedforconsidersuch
vulnerabilityunderpracticalusagescenariosofPLEs.Moreover,
5.2 ExperimentalResults thisdisparityremainsevidentintheoutputsofdownstreammod-
els,underscoringtheplausibilityofourproposedattackscenario.
5.2.1 AttackPerformance • After fine-tuning on D (which is disjoint from both the pre-
fine
trainingandnon-membersetintheevaluation),thedifferencebe-
Firstly, we present the attack performance across standard metrics tween the target model‘s responses on pre-training and unseen
suchasattackaccuracy,precision,andrecallinFigure3.Ourexper- datatendstodiminish.Thistrendcouldbeattributedtothemodel
imentalresultshighlightthatourattackgenerallydemonstrateshigh forgettingeffect[8].However,acertainlevelofdisparitystillre-
performance, as evidenced by diverse metrics, across various PLE mains, as the general representation learned on the pre-training
architecturesandmultipledownstreamtasks.Withinthecontextof datasetmayretainitsutilityfordownstreamtasksandthusbepre-
MIAliterature,thiscouldbedeemedasuccessfulattack,giventhat servedduringfine-tuning.
itsignificantlysurpassestherandomguessingbaselineof0.5.Taking
• Inlinewiththedataprocessinginequalityprinciple—thatis,the
theBERTmodelasanexample,theattackexecutedacrossalldiffer-
membership information is fully contained in the PLEs and any
entdownstreammodelsconsistentlydemonstratesahighdegreeof
additionaldownstreamlayerswillonlydecreasetheavailablein-
effectiveness: The precision for the classification, NER, and Q&A
formation—thefinaloutputsfromthedownstreammodelsexhibit
tasks are around 0.77, 0.87, and 0.87 respectively. Meanwhile, the
less divergence between samples from D and D than the
recallforthesetasksarearound0.79,0.86,and0.87.Notably,forall pre non
intermediate responses provided by the PLEs. This observation
the investigated attack performance metrics, a value exceeding 0.6
suggestsamorechallenging(yetrealistic)scenarioconsideredin
isgenerallyconsideredeffective,whereasavaluesurpassing0.8is
ourstudy,incomparisontopreviousstudieswherethefine-tuned
deemedsignificant.
encoderisdirectlyaccessiblebypotentialattackers.
On one hand, such findings suggest that membership leakage in
PLEs is indeed a prevalent issue, irrespective of the type of down-
In summary, our visualizations qualitatively show that member-
stream task, presenting considerably more severe potential privacy
shipleakageinPLEs’pre-traineddatapersists,perhapssurprisingly,
risksofPLEsthanpreviouslybelieved.Ontheotherhand,suchre-
even when such PLEs have undergone the fine-tuning process and
sultsmayhavebroaderimplicationsinreal-worldscenariosfortasks
only indirect access through the black-box output of downstream
such as privacy risk auditing and copyright authentication. For in-
modelsisavailabletopotentialadversaries.
stance, our findings suggest the feasibility of leveraging our attack
pipeline to detect potential data misuse during pre-training, requir-
5.2.3 EffectofAttackSettings
ingonlyblack-boxaccesstothefinalcommercialmodels.
Attack Training Dataset Relaxation. While our standard ap-
proachinvolvesutilizingamixtureofavailabledataforrobustper-
5.2.2 EmbeddingVisualization
formance evaluation (as described in Section 5.1), we also explore
a more demanding scenario for the adversary, where we relax the
Whilethequantitativeresultsabovedemonstratethevulnerabilityof
assumption about the adversary’s accessibility to D and D .
pre non
PLEs to membership leakage, we delve deeper into the underlying
Specifically,weconsidertheadversaryonlyhasaccesstoalimited
reasonsbyvisualizingtheembeddings,whichrevealspotentialsys-
numberofsamplesfromasinglepre-training(termedRelaxation-I)
tematicdisparitiesbetweenmembersandnon-membersthatcanbe
ornon-member(termedRelaxation-II)datasetfortrainingtheattack
exploitedbyanattacker.AsillustratedinFigure2,weanalyzePLEs’
model.Subsequently,theattackperformanceisevaluatedusingother
behaviorinresponsetomemberandnon-membersamplesfordiffer-
unseenpre-training/non-memberdatasets.Suchrelaxationssuggest
entscenarioswithinthepre-trainingfine-tuningframework.
thattheadversarypossessesonlypartialknowledge,potentiallyfail-
Firstly,wedirectlyfeed1000pre-trainingand1000unseensam-
ingtoaccuratelyreflectthecompletedatadistribution.Thisincom-
plesintotheoriginalpre-trainedBERTandembedthemodeloutput
(i.e.,f θ(x))intoa2Dspaceusingt-DistributedStochasticNeighbor 2https://github.com/DmitryUlyanov/Multicore-TSNE1.0 1.0 1.0
0.9 0.9 0.9
0.8 0.8 0.8
0.7 Model 0.7 Model 0.7 Model
BERT RoBERTa BERT RoBERTa BERT RoBERTa
ALBERT XLNet ALBERT XLNet ALBERT XLNet
0.6 0.6 0.6
1k 2k 4k 8k 16k 32k 64k 128k 256k 1k 2k 4k 8k 16k 32k 64k 128k 256k 1k 2k 4k 8k 16k 32k 64k 128k 256k
Size Size Size
(a)Accuracy (b)Precision (c)Recall
Figure5. AttackperformancewhenvaryingthesizeoftheSpreusedfortrainingtheattackmodel,withAG’sNewsbeingthefine-tuningdataset.
1.0 1.0 1.0
0.9 0.9 0.9
0.8 0.8 0.8
0.7 0.7 0.7
Model Model Model
0.6 0.6 0.6
BERT RoBERTa BERT RoBERTa BERT RoBERTa
ALBERT XLNet ALBERT XLNet ALBERT XLNet
0.5 0.5 0.5
0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32
Epoch Epoch Epoch
(a)Accuracy (b)Precision (c)Recall
Figure6. Attackperformancewhenvaryingthefine-tuningepochs,withAG’sNewsbeingthefine-tuninigdataset.
1.0 1.0 1.0
0.9 0.9 0.9
0.8 0.8 0.8
0.7 Model 0.7 Model 0.7 Model
BERT BERT BERT
ALBERT ALBERT ALBERT
0.6 0.6 0.6
RoBERTa RoBERTa RoBERTa
XLNet XLNet XLNet
0.5 0.5 0.5
1k 4k 16k 64k 256k 1024k 3000k 1k 4k 16k 64k 256k 1024k 3000k 1k 4k 16k 64k 256k 1024k 3000k
Size Size Size
(a)Accuracy (b)Precision (c)Recall
Figure7. Attackperformancewhenvaryingthefine-tuningdatasetsize,withAmazonbeingthefine-tuiningdataset.
pleterepresentationcouldintroducedisparitiesbetweenthetraining lacks,orhasskewed,knowledgeaboutthedistributionofthetraining
andtestingdatadistributionsfortheattack,therebyaddingextradif- data.
ficultiestoitsgeneralization. Additionally,theresultsunderscorethemajorroleoftheknowl-
WereporttheattackperformanceforRelaxation-IandRelaxation- edgeaboutthepre-trainingdatadistributionsontheattack’ssuccess.
II in Figure 4. Firstly, it is noteworthy that the attack performance As these distributions serve as the primary targets of the attacker,
remainsconsistentlyhigh,withbothprecisionandrecallexceeding theirknowledgeiscrucialtotheinferenceprocess.Conversely,non-
0.7. This high level of performance provides a clear indication of memberdatasetsplayasupportingrole,aidingthedecision-making
the inherent differences in the target model’s responses to the pre- processbutnotbeingtheprimaryfocus.
trainingversusunseensamples,whilesuchdifferentialbehaviorcan
EffectofAttackTrainingSetSize. Wefurtherinvestigatetheef-
generallybedelineatedbyadecisionboundaryderivedfrompartial
fectsofthesizeofS collectedbytheadversaryforitstraining.As
knowledgeofthedistribution. pre
illustratedinFigure5,thereexistsacleartrendofincreasingattack
Moreover,whilethereexistsanoticeabledecreaseinattackrecall
effectiveness as the attack training set size increases. These results
whenthepre-trainingdatasetsdifferfortrainingandtestingtheat-
alignwiththeexpectationthatalargerquantityoftrainingdatacon-
tackmodel,ourresultsstillindicateaneffectiveattack,asaMIAis
tributestohigherperformanceinanMLmodel.Interestingly,wefind
deemed useful as long as the attack can accurately infer a portion
thatevenwhenusingonly1kpre-trainingsamplesforS ,theat-
ofthemembers[4](i.e.,maintaininghighprecision).Specifically,it pre
tackperformanceremainseffective,yieldingmorethan0.7interms
maybeimpracticaltoexpecttheattacktoinferallthemembers(i.e.,
of precision, recall, and accuracy. Notably, such a training set rep-
aimingforhighrecall).Thisisparticularlythecasewhentheattack
resentsonly0.0077%ofWikipediaand0.00067%ofBooksCorpus,
ycaruccA
ycaruccA
ycaruccA
noisicerP
noisicerP
noisicerP
llaceR
llaceR
llaceRTable1. AttackperformanceevaluatedonGPT-3.5generatednon-memberdata.Wereportaccuracy(A),precision(P),recall(R),andF1-score(F)foreach
PLEmodelunderdifferentdatasetsandtasks.
SST-2 AG’sNews YelpReviewFull CoNLL2003 SQuADv1.0
Models
A P R F A P R F A P R F A P R F A P R F
BERT 0.56 0.57 0.51 0.53 0.61 0.63 0.54 0.58 0.60 0.59 0.65 0.62 0.83 0.83 0.82 0.83 0.83 0.91 0.74 0.81
ALBERT 0.60 0.63 0.80 0.70 0.58 0.59 0.55 0.57 0.60 0.59 0.64 0.61 0.75 0.74 0.80 0.76 0.94 0.94 0.95 0.95
RoBERTa 0.57 0.56 0.64 0.60 0.71 0.72 0.70 0.71 0.59 0.58 0.63 0.61 0.88 0.86 0.92 0.88 0.85 0.83 0.88 0.86
XLNet 0.55 0.54 0.79 0.64 0.82 0.81 0.84 0.82 0.80 0.80 0.80 0.80 0.73 0.69 0.83 0.75 0.93 0.94 0.92 0.93
Table2. Examplesofpre-trainingdatasamplesandtheircorrespondingGPT-3.5generatednon-memberdata.Thepromptissettobe"paraphrasethe
sentencewiththesamestyle".Theexamplesarecategorizedbasedontheattacker’sprediction:“correct”whentheattackercorrectlyidentifiespre-trainingdata,
“incorrect”whentheattackererroneouslyidentifiesthepre-trainingdatasampleasunseen.
Type Pre-trainingdata Non-memberdata
ingeneralthemoremassiveastaristheshorteritslifespanonthe Asarule,heavierstarsspendlesstimeinthemainsequencephase.
mainsequenceafterthehydrogenfuelatthecorehasbeenconsumed Oncetheyburnthroughtheheliumattheircore,theymoveoffthe
thestarevolvesawayfromthemainsequenceonthehrdiagramthe mainsequencetrackontheHRdiagram.Thefutureofthesestarsis
behaviorofastarnowdependsonitsmasswithstarsbelow023m largelydefinedbytheirsize,withthosebelow0.23solarmassesoften
becomingwhitedwarfs. morphingintoredgiants.
eventhoughtheydidn’tspeak,megandidn’tfeelawkwardaround notspeakanything,Meganwasuncomfortablearoundhimafterwhat
himafterwhattheyhaddone. theyhaddone.
theOregonShakespeareFestival,whichoriginallybeganasasummer OriginallylaunchedasasummerindooreventinMedford,Oregon
correct
outdoorseriesinAshlandduringthe1930sandlatermovedtoTalent, duringthe1930s,theOregonShakespeareFestivallatermovedtoTal-
Oregon, to specialize in battery electric motorcycles, has grown to ent,Oregon,whereitshifteditsfocustobatteryelectricmotorcycles.
spanaseasonfromFebruarytoOctoberandincorporatebothShake- ItnowoffersabroadarrayofperformancesfromMarchtoNovem-
speareanandnon-Shakespeareanworks. ber,featuringbothShakespeareanandcontemporaryworks.
april1774louisxvfellillaftercontractingsmallpoxanddiedthefol- InApril1774,LouisXVcontractedsmallpoxandsuccumbedtothe
lowing10maythedauphinlouisaugustesucceededhisgrandfather illnessonMay10th.Hisgrandson,LouisAuguste,ascendedtothe
askinglouisxviaseldestbrotherofthekinglouisstanislasreceived throneasKingLouisXVI.Astheking’seldestbrother,LouisStanis-
thetitlemonsieurlouisstanislaslongedforpoliticalinfluenceheat- laswasbestowedthetitle"Monsieur."Eagerforpoliticalclout,he
temptedtogainadmittancetothekingscouncilin1774speculation triedtosecureaspotintheking’scouncilthatsameyear,amidrising
soared speculation.
universityinithacanewyorkhewantedtostudythehumanitiesorbe- AtauniversityinIthaca,NewYork,hewasinclinedtopursuehuman-
comeanarchitectlikehisfatherbuthisfatherandbrotherascientist itiesorfollowinhisfather’sfootstepsasanarchitect.However,under
urgedhimtostudyausefuldisciplineasaresultvonnegutmajoredin pressurefromhisfatherandhisbrother,whowasascientist,hewas
biochemistrybuthehadlittleproficiencyintheareaandwasindiffer- persuadedtostudya"practical"field.Consequently,Vonnegutended
incorrect
enttowardshisstudies upmajoringinbiochemistry
peshsighedandreleasedherhand. Peshletoutasighandremovedherhand.
resignedly,hefollowedherintotheroom. Reluctantly,hetrailedherintotheroom.
atleastnotwithaman. Notwithaman,attheveryleast.
indicatingacautionarysignalthatMIAagainstPLEsmaybemuch Table3. Comparisonoffine-tuningstrategiesonBERTfortheYelpReview
easiertocarryoutthanpreviouslythought. Fulldownstreamtask.
UpdatedLayers Accuracy Precision Recall F1-score Utility
Embedding 0.86 0.84 0.89 0.86 0.56
5.2.4 EffectofFine-tuningSettings
Embedding+Classifier 0.84 0.84 0.83 0.85 0.61
Classifier 0.83 0.82 0.84 0.83 0.47
EffectofFine-tuningEpochs. Wereporttheperformanceoffine- AllLayers(Default) 0.82 0.85 0.79 0.82 0.65
tuningdifferentepochsinFigure6.Generally,asthenumberoffine-
tuning epochs increases, a slight decrease in attack performance is
observed.Thisisprimarilyduetothemoresignificantalterationsin erationlargelypreservesthelearnedsemanticinformationinthere-
the model parameters of PLEs during the fine-tuning phase under maininglayers,makingiteasierfortheattacktoextractinformation
theseconditions.Asaresult,theinformationaboutthepre-training aboutthepre-trainingdataset.
datatendstobecomeprogressivelyobfuscated.However,itmaystill
remainvulnerabletopotentialattacks,asthegenericrepresentations
derivedfromthepre-trainingdataoftenretaintheiruniversalutility
EffectoftheFine-tuningDatasetSize. Weillustratetheimpact
andarelargelypreserved.
of fine-tuning dataset size in Figure 7, where we vary the size of
EffectofFine-tuningStrategies. Wepresenttheperformanceof thefine-tuningtrainingsetbyrandomlysamplingfromtheAmazon
fine-tuning different parts of a BERT model in Table 3, reflecting Reviewdatasetandadoptthisdatatofine-tuneallPLEsexaminedin
variouscommonfine-tuningstrategiesadoptedinpractice.Thelast thisstudy.AsdepictedinFigure7,itappearsthatattackperformance
column(utility)representsthedownstreammodel’sclassificationac- generally diminishes as the size of the fine-tuning data increases.
curacyontheYelpdataset.Ascanbeobserved,ourdefaultsetting This can be attributed to the fact that introducing more fine-tuning
(i.e., fine-tuning all layers) results in the best downstream utility, dataisakintoaugmentingthePLEswithadditionalknowledge.Con-
which confirms our default configuration is appropriate. Moreover, sequently, the PLEs will undergo more substantial changes, poten-
the other configurations yield similar or even better attack perfor- tially obscuring their knowledge about the pre-training data during
mance.Notably,onlyupdatingthewordembeddingyieldsthehigh- thefine-tuningprocess,therebyleadingtoadecreaseinattackper-
est attack performance. This aligns with our intuition: such an op- formance.5.2.5 DistributionSimilarityComparison tance (FID) 5 and Maximum Mean Discrepancy (MMD) 6. For
BERTScoreandRougeScore,highervaluessignifygreatersimilar-
Toalleviatethepotentialdiscrepancybetweenthepre-trainingdata
ity. Conversely, for FID and MMD, lower values indicate higher
and unseen data distribution during evaluation, we further conduct
similarity.Wequantifiedthedistributionsimilaritybetweenthepre-
experimentsby:
training(Wikipedia)andtwodifferenttypesofnon-memberdata:
• selecting an existing dataset as the unseen data (i.e., STORIES)
• Random Subset: random selections from the same pre-training
that shares similar semantic styles with the pre-training dataset
dataset(i.e.,Wikipedia).
(i.e.,BookCorpus);
• GPTNon-member:datageneratedbyrephrasingthepre-training
• adoptingathird-partyGPT-3.5torephraseeachpre-trainingdata
datausingaGPT-3.5model.
intoacorrespondingnon-memberdatasentence,generatingatotal
of 10k non-member sentences while maintaining the same style Bothtypesarecomparedagainstthesamereferencepre-trainingset
andsemantics. forfaircomparison.Notably,Table5showsthattheGPT-generated
non-memberdataachievearemarkablelevelofdistributionsimilar-
STORIES as the Non-member Data. To address concerns that
ity with pre-training data, even surpassing that of random subsets
theattackmightonlydistinguishbetweendatadistributions,specif-
withinthesamepre-trainingdataset.Suchfindingsunderscorethat
icallythevariationsinlanguagestyleandsemanticsacrossdatasets,
our successful detection of pre-training data leakage is most likely
weselectedanadditionaldataset,i.e.,STORIES,asthenon-member
not merely due to divergent distributions between pre-training and
datasetforevaluation.ThestyleofSTORIES[37]closelyresembles
non-memberdata.
thatoftheBookCorpuspre-trainingdatasets:theSTORIESdataset
wasdevelopedbyextractingstory-likesectionsfromasubsetofthe Table5. Quantificationofdistributionsimilarity.
CommonCrawldataset,whileBookCorpusconsistsmostlyoffiction SimilarityMetrics RandomSubset GPT-3.5Generated
books from unpublished authors. Therefore, the two datasets share
BERTScore(↑) 0.80 0.91
similarnarrativestylesandstory-likecontent.AsobservedinTable4,
RougeScore(↑) 0.15 0.48
theattackperformanceisconsistentwiththatshowninFigure3.This FID(↓) 9.39 3.45
further verifies that the success of our data leakage attack against MMD(↓) 192 67
PLEs is not solely attributable to distribution differences between
pre-trainingdataandunseendata.
Table4. AttackperformanceevaluatedonSTORIESnon-memberdataset. 6 Conclusion
DownstreamTask Accuracy Precision Recall F1-score
Inthiswork,wepioneerthesystematicstudyofpotentialdataleak-
SST-2 0.64 0.63 0.69 0.66
age associated with PLEs. Specifically, we consider a realistic and
AGNews 0.92 0.95 0.90 0.92
challenging scenario where the adversary can only gain access to
YelpReviewFull 0.80 0.85 0.74 0.79
NER 0.84 0.81 0.88 0.84 the output of downstream models adapted from PLEs. We conduct
QA 0.92 0.95 0.89 0.92 extensive and rigorous evaluations that span a variety of PLE ar-
chitectures, different types of downstream tasks, and a number of
importantfactorsthataffectmembershipleakagefromdifferentan-
GPT-3.5GeneratedNon-memberData. Table1showsthatthe
gles. Our experimental results yield intriguing findings, suggesting
attack remains generally effective, despite a slight drop in perfor-
thatwhatappearstobeasafeusagescenariomightindeedbeprob-
mancecomparedtothedefaultsettingthatdoesnotcontrolthese-
lematic,presentingaprivacythreattoPLEsthatisfargreaterthan
manticsofnon-memberdata.Whilecomparingdifferentdatasetsand
previouslybelieved.Lastly,ourin-depthanalysis,togetherwithkey
tasks, we observe that tasks resulting in higher dimensionality of
insights,raisescriticalconsiderationsforfuturemodeldevelopers.
downstream models’ outputs tend to make attacks more effective.
Thisalignswiththeintuitionthathigher-dimensionaloutputscause
downstream models to leak more information about their training
data,therebyincreasingthethreatofattacks(seeAppendixforaddi-
tionalexperiments).Additionally,thequalitativeexamplespresented
inTable2revealsomepotentiallyinterestingtrends.Ingeneral,the
attackdemonstrateshighereffectivenesswithmoreconfidentpredic-
tionsforlongersentencescontainingmeaningfulentitiesthatprovide
morefine-grainedinformationandareindeedmoreprivacy-sensitive.
Conversely,samplesforwhichtheattackisuncertainoftheirmem-
bershipstatustendtofeature’neutral’textthatcouldgenerallyoc-
curineverydaylife.Theseobservationsindicatethepotentialthreat
andhighlighttheneedforcarefulsanitizationof(theinformativeor
sensitiveentitiescontainedwithin)pre-trainingdatawhendeploying
PLEstoensurecompliancewithprivacyregulations.
Quantitative Measure of Distribution Similarity. We further
quantify the distribution similarity using common standard met-
ricsincluding:BERTScore3,RougeScore4,Fréchetinception dis-
5https://pytorch.org/ignite/generated/ignite.metrics.FID.html
3https://huggingface.co/spaces/evaluate-metric/bertscore 6https://lightning.ai/docs/torchmetrics/stable/image/kernel_inception_
4https://huggingface.co/spaces/evaluate-metric/rouge distance.htmlEthicsStatement L. Zettlemoyer, and V. Stoyanov. RoBERTa: A Robustly Optimized
BERTPretrainingApproach.CoRRabs/1907.11692,2019.
[20] P.Maini,M.Yaghini,andN.Papernot. Datasetinference:Ownership
Ourworkcontributestoabetterunderstandingofthepotentialthreats
resolutioninmachinelearning.Iniclr,2021.
associatedwiththeusageofpre-trainedlanguageencoders,provid- [21] F. Mireshghallah, K. Goyal, A. Uniyal, T. Berg-Kirkpatrick, and
inginsightsthatraiseawarenessandanticipatepositivesocietalim- R.Shokri.QuantifyingPrivacyRisksofMaskedLanguageModelsUs-
pacts.Wearenotawareofanyadditionalnegativesocietalimpacts
ingMembershipInferenceAttacks.CoRRabs/2203.03929,2022.
[22] F. Mireshghallah, A. Uniyal, T. Wang, D. Evans, and T. Berg-
beyondthegenericrisksofMLtechnology.
Kirkpatrick. Memorizationinnlpfine-tuningmethods. arXivpreprint
arXiv:2205.12506,2022.
References [23] M.Munikar,S.Shakya,andA.Shrestha.Fine-grainedSentimentClas-
sificationusingBERT.CoRRabs/1910.03474,2019.
[1] A.Bordes,S.Chopra,andJ.Weston. QuestionAnsweringwithSub- [24] B. Pang and L. Lee. Seeing stars: Exploiting class relationships for
graphEmbeddings. InConferenceonEmpiricalMethodsinNatural sentimentcategorizationwithrespecttoratingscales. arXivpreprint
LanguageProcessing(EMNLP),pages615–620.ACL,2014. cs/0506075,2005.
[2] T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal, [25] S.Rahimian,T.Orekondy,andM.Fritz. SamplingAttacks:Amplifi-
A.Neelakantan,P.Shyam,G.Sastry,A.Askell,etal.Languagemodels cationofMembershipInferenceAttacksbyRepeatedQueries. CoRR
arefew-shotlearners.nips,33:1877–1901,2020. abs/2009.00395,2020.
[3] T.B.Brown,B.Mann,N.Ryder,M.Subbiah,J.Kaplan,P.Dhariwal, [26] P.Rajpurkar,J.Zhang,K.Lopyrev,andP.Liang. SQuAD:100,000+
A.Neelakantan,P.Shyam,G.Sastry,A.Askell,S.Agarwal,A.Herbert- Questions for Machine Comprehension of Text. arXiv e-prints, art.
Voss,G.Krueger,T.Henighan,R.Child,A.Ramesh,D.M.Ziegler, arXiv:1606.05250,2016.
J.Wu,C.Winter,C.Hesse,M.Chen,E.Sigler,M.Litwin,S.Gray, [27] A.RamponiandB.Plank. Neuralunsuperviseddomainadaptationin
B.Chess,J.Clark,C.Berner,S.McCandlish,A.Radford,I.Sutskever, nlp—asurvey.arXivpreprintarXiv:2006.00632,2020.
and D. Amodei. Language Models are Few-Shot Learners. In An- [28] A.Salem,Y.Zhang,M.Humbert,P.Berrang,M.Fritz,andM.Backes.
nualConferenceonNeuralInformationProcessingSystems(NeurIPS). ML-Leaks: Model and Data Independent Membership Inference At-
NeurIPS,2020. tacks and Defenses on Machine Learning Models. In ndss. Internet
[4] N.Carlini,S.Chien,M.Nasr,S.Song,A.Terzis,andF.Tramèr.Mem- Society,2019.
bershipInferenceAttacksFromFirstPrinciples.CoRRabs/2112.03570, [29] V.Shejwalkar,H.A.Inan,A.Houmansadr,andR.Sim. Membership
2021. InferenceAttacksAgainstNLPClassificationModels.InPriMLWork-
[5] N.Carlini,F.Tramer,E.Wallace,M.Jagielski,A.Herbert-Voss,K.Lee, shop(PriML).NeurIPS,2021.
A.Roberts,T.B.Brown,D.Song,U.Erlingsson,etal.Extractingtrain- [30] L.Shen,S.Ji,X.Zhang,J.Li,J.Chen,J.Shi,C.Fang,J.Yin,and
ingdatafromlargelanguagemodels. InUSENIXSecuritySymposium, T. Wang. Backdoor Pre-trained Models Can Transfer to All. In
volume6,2021. ACMSIGSACConferenceonComputerandCommunicationsSecurity
[6] C. A. C. Choo, F. Tramèr, N. Carlini, and N. Papernot. Label-Only (CCS),pages3141–3158.ACM,2021.
MembershipInferenceAttacks. InInternationalConferenceonMa- [31] R.Shokri,M.Stronati,C.Song,andV.Shmatikov. MembershipInfer-
chineLearning(ICML),pages1964–1974.PMLR,2021. enceAttacksAgainstMachineLearningModels. InIEEESymposium
[7] J.Devlin,M.Chang,K.Lee,andK.Toutanova. BERT:Pre-training onSecurityandPrivacy(S&P),pages3–18.IEEE,2017.
ofDeepBidirectionalTransformersforLanguageUnderstanding. In [32] R.Socher,A.Perelygin,J.Wu,J.Chuang,C.D.Manning,A.Y.Ng,and
ConferenceoftheNorthAmericanChapteroftheAssociationforCom- C.Potts. RecursiveDeepModelsforSemanticCompositionalityOver
putationalLinguistics:HumanLanguageTechnologies(NAACL-HLT), aSentimentTreebank.InConferenceonEmpiricalMethodsinNatural
pages4171–4186.ACL,2019. LanguageProcessing(EMNLP),pages1631–1642.ACL,2013.
[8] I.J.Goodfellow,M.Mirza,D.Xiao,A.Courville,andY.Bengio. An [33] C. Song and V. Shmatikov. Auditing Data Provenance in Text-
empiricalinvestigationofcatastrophicforgettingingradient-basedneu- GenerationModels. InACMConferenceonKnowledgeDiscoveryand
ralnetworks.arXivpreprintarXiv:1312.6211,2013. DataMining(KDD),pages196–206.ACM,2019.
[9] X.He,Z.Li,W.Xu,C.Cornelius,andY.Zhang.Membership-Doctor: [34] C.Sun,X.Qiu,Y.Xu,andX.Huang. HowtoFine-TuneBERTfor
ComprehensiveAssessmentofMembershipInferenceAgainstMachine TextClassification?InChinaNationalConferenceonChineseCompu-
LearningModels.CoRRabs/2208.10445,2022. tationalLinguistics(CCL),pages194–206.Springer,2019.
[10] X.He,H.Liu,N.Z.Gong,andY.Zhang.Semi-Leak:MembershipIn- [35] K.Tirumala,A.Markosyan,L.Zettlemoyer,andA.Aghajanyan.Mem-
ferenceAttacksAgainstSemi-supervisedLearning. InEuropeanCon- orizationwithoutoverfitting:Analyzingthetrainingdynamicsoflarge
ferenceonComputerVision(ECCV).Springer,2022. languagemodels.nips,35:38274–38290,2022.
[11] H.Hu,Z.Salcic,L.Sun,G.Dobbie,P.S.Yu,andX.Zhang. Member- [36] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
shipInferenceAttacksonMachineLearning:ASurvey.ACMComput- T.Lacroix,B.Rozière,N.Goyal,E.Hambro,F.Azhar,etal. Llama:
ingSurveys,2021. Open and efficient foundation language models. arXiv preprint
[12] A.Jagannatha,B.P.S.Rawat,andH.Yu.MembershipInferenceAttack arXiv:2302.13971,2023.
Susceptibility of Clinical Language Models. CoRR abs/2104.08305, [37] T.H.TrinhandQ.V.Le.Asimplemethodforcommonsensereasoning.
2021. arXivpreprintarXiv:1806.02847,2018.
[13] D.P.KingmaandJ.Ba.Adam:AMethodforStochasticOptimization. [38] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
InInternationalConferenceonLearningRepresentations(ICLR),2015. Gomez,L.Kaiser,andI.Polosukhin. AttentionisAllyouNeed. In
[14] Z.Lan,M.Chen,S.Goodman,K.Gimpel,P.Sharma,andR.Soricut. AnnualConferenceonNeuralInformationProcessingSystems(NIPS),
ALBERT:ALiteBERTforSelf-supervisedLearningofLanguageRep- pages5998–6008.NIPS,2017.
resentations. InInternationalConferenceonLearningRepresentations [39] T.Wolf,L.Debut,V.Sanh,J.Chaumond,C.Delangue,A.Moi,P.Cis-
(ICLR),2020. tac, T. Rault, R. Louf, M. Funtowicz, and J. Brew. HuggingFace’s
[15] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang. Transformers: State-of-the-art Natural Language Processing. CoRR
BioBERT:apre-trainedbiomedicallanguagerepresentationmodelfor abs/1910.03771,2019.
biomedicaltextmining.Bioinformatics,2020. [40] Z.Yang,Z.Dai,Y.Yang,J.G.Carbonell,R.Salakhutdinov,andQ.V.
[16] M.Lewis,Y.Liu,N.Goyal,M.Ghazvininejad,A.Mohamed,O.Levy, Le. XLNet:GeneralizedAutoregressivePretrainingforLanguageUn-
V. Stoyanov, and L. Zettlemoyer. BART: Denoising Sequence-to- derstanding. InAnnualConferenceonNeuralInformationProcessing
Sequence Pre-training for Natural Language Generation, Translation, Systems(NeurIPS).NeurIPS,2019.
andComprehension.InAnnualMeetingoftheAssociationforCompu- [41] J.Ye,A.Maddi,S.K.Murakonda,andR.Shokri. EnhancedMem-
tationalLinguistics(ACL),pages7871–7880.ACL,2020. bership Inference Attacks against Machine Learning Models. CoRR
[17] X.Li,J.Feng,Y.Meng,Q.Han,F.Wu,andJ.Li. AUnifiedMRC abs/2111.09679,2021.
FrameworkforNamedEntityRecognition. InAnnualMeetingofthe [42] X.Zhang,J.Zhao,andY.LeCun. Character-levelConvolutionalNet-
Association for Computational Linguistics (ACL), pages 5849–5859. worksforTextClassification. InAnnualConferenceonNeuralInfor-
ACL,2020. mationProcessingSystems(NIPS),pages649–657.NIPS,2015.
[18] Z. Li, Y. Liu, X. He, N. Yu, M. Backes, and Y. Zhang. Auditing
MembershipLeakagesofMulti-ExitNetworks.CoRRabs/2208.11180,
2022.
[19] Y.Liu,M.Ott,N.Goyal,J.Du,M.Joshi,D.Chen,O.Levy,M.Lewis,Appendix posedbycrowdworkersonasetofwikipediaarticles,wherethean-
swer to every question is a segment of text, or span, from the cor-
respondingreadingpassage,orthequestionmightbeunanswerable.
A ExperimentSetupDetails Theprimarytaskinvolvesprovidingapieceofcontextandaques-
tion,theanswertowhichiscontainedwithinthegivencontext.The
A.1 DownstreamTasks&Datasets objectiveistopredictthe“start”and“end”locationsfortheanswer
inthegivencontext.Thisistypicallymodeledbypredictingthestart
We list below the detailed description for the benchmark datasets andendprobabilitiesforeachtokenwithinthegivencontext.Subse-
adoptedinthisworkandsummarizesthekeypropertiesinTable6. quently,theanswerispinpointedwithinthetwotokensthatexhibit
WeusethetrainingsplitofthedatasetforPLEs’finetuningunless thehighestpredicted“start”and“end”probabilities.Thedatasetof-
otherwisespecified. fers87,600sentencesfortrainingandanadditional10,600sentences
SST-27: The Stanford Sentiment Treebank (SST) is a corpus with fortesting.
fully labeled parse trees that allows for a complete analysis of the
Table6. Summaryofdifferentdatasets.
compositionaleffectsofsentimentinlanguage.Thecorpusisbased
Dataset #Sentences Topic Domain
onthedatasetintroducedby[24]andconsistsof11,855singlesen-
tencesextractedfrommoviereviews.ItwasparsedusingtheStan- SST-2 70k classification moviereview
AG’sNews 127.6k classification newsarticle
fordparser,yieldingacollectionof215,154uniquephrasesderived
YelpReview 650k classification reviewsfromYelp
from the parsed trees. The corpus is often used in binary classifi- AmazonReview 9100k classification amazonwebsitereview
cation experiments, where full sentences are categorized as either CoNLL2003 20.7k NER newswirearticles
“negative/somewhatnegative”or“positive/somewhatpositive”,with SQuADv1.0 98.2k Q&A wikipediaarticles
neutralsentencesbeingdiscarded.Thisbinarydatasetiscommonly
referredtoasSST-2orSSTbinary.Itencapsulatesatotalof70,000
sentences,ofwhich67,300areusedfortraining.
B ImplementationDetails
AG’sNews8:TheAG’sNewsdatasetservesasabenchmarkdataset
for topic classification tasks. It is an extensive collection of over a The attack model is implemented as a three-layer fully connected
millionnewsarticles,amassedfrommorethan2,000newssources network,i.e.,3-layerMLP,withReLUactivationfunctionsafterthe
bytheacademicnewssearchengine,ComeToMyHead,overayear- firsttwolayersandafinalsoftmaxlayertoproduceoutputprobabili-
long period. The dataset has been utilized as a text classification ties.AndweusetheAdamoptimizerwithalearningrate0.01totrain
benchmark in research, notably in the study by Zhang et al. [42]. theattackmodel,followingthesettingofpreviousworks[10,6].The
AG’sNewsisstructuredasafour-classclassificationframework.In input dimension of the attack model varies acorss different down-
total,itencompasses127,600sentences,ofwhich120,000fromthe streamtasks:
trainingsplitaretypicallyusedformodelfinetuning. • For classification tasks the dimension equals to number of label
AmazonReview9:TheAmazonreviewsdatasetconsistsofreviews classesinthecorrespondingtask:2forSST-2,4forAG’sNews,
and5forbothYelpandAmazonReviewdataset.
from amazon [42]. The data span a period of 18 years, including
around35millionreviewsuptoMarch2013.Reviewsincludeprod- • TheNERisconductedasatokenclassificationtaskwheretheclas-
uctanduserinformation,ratings,andaplaintextreview.TheAma- sificationoutputare9NERtagswithIOB2taggingscheme,sothe
zonreviewspolaritydatasetisisdevisedbydesignatingreviewswith inputdimensionforattackmodelis9.
scores1and2asnegative(class1)andthosewithscores4and5as • FortheQ&AtaskconductedontheSQuADv1.0dataset,thefine-
positive (class 2). Reviews with a neutral score of 3 are excluded. tuned downstream model will output two logits representing the
Thedatasetisbalancedwitheachclasscontaining1.8milliontrain- “start”and“end”indicator,enablingtheidentificationofanswer
ingsamplesand20,000testingsamples. positions.Asaresult,theinputdimensionfortheattackmodelis
Yelp Review Full10: The Yelp reviews dataset is comprised of re- twicethesequencelength.
viewscollectedfromYelp,originallyextractedfromtheYelpDataset
Challengeconductedin2015.Itwasfirstintroducedasatextclassi- C AdditionalResults
ficationbenchmarkbyZhangetal.[42].Thedatasetcontains650k
trainigsamplesintotaland50ksamplesreservedfortesting. Effect of Downstream Task Configuration. We provide addi-
CoNLL200311: CoNLL2003 is used for named entity recogni- tional results on the relationship between attack performance and
the configuration of downstream tasks, specifically, the number of
tion (NER) adopting IOB2 tagging scheme, which includes 9
classesintheclassificationtask.WeconductexperimentsontheYelp
classesforeachtoken:{’O’,’B-PER’,’I-PER’,’B-ORG’,’I-ORG’,’B-
ReviewFulldataset,whichconsistsoffivelabelclassesintotal.We
LOC’,’I-LOC’, ’B-MISC’,’I-MISC’}. It contains totally 20.7k sen-
thenconstruct4-class,3-class,and2-classsubsetbydroppingsam-
tenceswiththesizeoftrainingsetbeing14k.
plesfromone(randomly)selectedlabelclass,andweadditionallyfix
SQuADv1.012:SQuADv1.0isareadingcomprehensiondatasetused
thetotalnumberoftrainingsamplestobethesame(i.e.,120,000),in
for Question Answering(Q&A) [26], which consists of questions ordertoeliminatetheeffectofdatasetsize.
AsillustratedinFigure8,thereisapositivecorrelationbetween
7https://huggingface.co/datasets/sst2
attackperformanceandthenumberofclasses.Thissuggeststhatan
8https://huggingface.co/datasets/ag_news
increaseinthenumberofclassesmayleadtoahigheramountofpre-
9https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews
10https://huggingface.co/datasets/yelp_review_full trainingdatainformationbeingleakedbythedownstreammodel.A
11https://huggingface.co/datasets/conll2003 simple explanation for this is that the dimensionality of the down-
12https://huggingface.co/datasets/squad streammodel’soutputgrowsinlinewiththenumberofclasses.This1.0 1.0 1.0
0.9 0.9 0.9
0.8 0.8 0.8
0.7 Model 0.7 Model 0.7 Model
BERT BERT BERT
ALBERT ALBERT ALBERT
0.6 0.6 0.6
RoBERTa RoBERTa RoBERTa
XLNet XLNet XLNet
0.5 0.5 0.5
2 3 4 5 2 3 4 5 2 3 4 5
NumberofClasses NumberofClasses NumberofClasses
(a)Accuracy (b)Precision (c)Recall
Figure8. Attackperformancewhenvaryingthenumberofdownstreamtask’slabelclasses,conductedonYelpReviewFulldataset.
providesmoreinformationregardingthetaskbutalsoregardingthe
membership, consequently leading to an enhanced performance in
detectingpre-trainingdata’sleakage.
ycaruccA noisicerP
llaceR