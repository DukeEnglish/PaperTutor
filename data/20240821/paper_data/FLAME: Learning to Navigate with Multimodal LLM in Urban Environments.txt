FLAME: Learning to Navigate with Multimodal LLM in Urban Environments
YunzheXu,YiyuanPan,ZheLiu,HeshengWang
ShanghaiJiaotongUniversity
{xyz9911,pyy030406,liuzhesjtu,wanghesheng}@sjtu.edu.cn
Abstract Casual Chat Navigation Scenario
What do these pictures … According to the instruction,
LargeLanguageModels(LLMs)havedemonstratedpotential have in common? what’s the next move?
inVision-and-LanguageNavigation(VLN)tasks,yetcurrent LLM All these rp ei dct cu or le os r .contain a LLM Ba ts he ed p o rn o pt eh re ao cb ts ie or nv ia st tio on ,
applications face challenges. While LLMs excel in general (a) go straight.
conversationscenarios,theystrugglewithspecializednaviga-
Street View Description (b)
tiontasks,yieldingsuboptimalperformancecomparedtospe- Tuning
There are two traffic
cialized VLN models. We introduce FLAME (FLAMingo- lightsin the image, and one Phase I
Architected Embodied Agent), a novel Multimodal LLM- caris visible on the road.
based agent and architecture designed for urban VLN tasks
Trajectory Summarization
thatefficientlyhandlesmultipleobservations.Ourapproach Tuning Bypassing a hotel on the left, the
implementsathree-phasetuningtechniqueforeffectiveadap- Phase II agent reaches the next light with
a shopping mall, located on the
tation to navigation tasks, including single perception tun- far rightcorner.
ingforstreetviewdescription,multipleperceptiontuningfor
trajectory summarization, and end-to-end training on VLN
Instruction
datasets. The augmented datasets are synthesized automati-
cally. Experimental results demonstrate FLAME’s superior- Flamingo-Architected Embodied Agent
ityoverexistingmethods,surpassingstate-of-the-artmethods
by a 7.3% increase in task completion rate on Touchdown I should Head Head Head Head I'm supposed
turn … forward forward forward forward to take …
dataset. This work showcases the potential of Multimodal
LLMs (MLLMs) in complex navigation tasks, representing … a right at the
intersection and see
anadvancementtowardspracticalapplicationsofMLLMsin
stores on my right.
embodiedAI.Projectpage:https://flame-sjtu.github.io I'll TURN RIGHT.
Action: Right
1 Introduction
Figure1:LLM-basedagentsexcelinconversationbutoften
LargeLanguageModels(LLMs)(Achiametal.2023;Tou-
falter in specialized navigation tasks. Our agent, powered
vron et al. 2023) have revolutionized the field of embodied
solely by a Multimodal LLM, demonstrates proficiency in
intelligence. Vision-and-Language Navigation (VLN) (An-
navigation skills, adapting to navigation-specific scenarios
derson et al. 2018), a fundamental task in embodied AI,
throughtargetedfinetuningphases.
challengesagentstonavigatetoagoalfollowinghumanin-
structionsinindoororoutdoorenvironments.Thistaskde-
mands sophisticated abilities in instruction comprehension,
environmental understanding, and decision-making, which ahugeperformancegapcomparedtoVLN-specializedmod-
can be effectively managed by LLMs. Recent approaches els. For Multimodal LLMs, while partially addressing the
have integrated LLMs into VLN methods, either by trans- limitations of text-only LLMs, they often struggle to inter-
latingvisualdataintolanguage(Zhou,Hong,andWu2024; actwithinnavigation-specificscenarios.Recentattemptsto
Panetal.2024;Qiaoetal.2023;Linetal.2024a;Chenetal. incorporateMLLMsfornavigationfocusonintegrationwith
2024)orbyemployingMultimodalLLMs(MLLMs)(Zhang down-streammodels(Zhouetal.2024)orintensivetraining
etal.2024;Zhouetal.2024)forenvironmentalperception. on navigation videos (Zhang et al. 2024), rather than mak-
However, the incorporation of general-purpose LLMs in ingefficientadaptationstonavigation-specificscenariothat
VLNstillfacescriticalchallenges,primarilystemmingfrom wouldharnesstheMLLMs’inherentcapabilitiesinhandling
theirinherentlimitationsinnavigation-specificscenarios,as interleaved textual and visual inputs, leading to suboptimal
Figure 1(a) depicts. For text-only LLMs, translating visual performance.
dataintolanguageusingvisualfoundationmodelscanlead Moreover, the application of MLLM in urban VLN re-
toinformationloss(Zhou,Hong,andWu2024),resultingin mains unexplored, despite its importance alongside indoor
4202
guA
02
]VC.sc[
1v15011.8042:viXraVLN.Outdoornavigationpresentsuniquechallengesforin- Linetal.2024a)anddialoguecapabilities(Qiaoetal.2023;
troducingMLLM,includinglongertrajectorylengths(upto Longetal.2024)ofLLMs.Theseapproacheseitherconvert
55 iterations) and increased difficulty (40% lower success visual data to text (Zhou, Hong, and Wu 2024) or employ
ratecomparedtoindoornavigationtasks). MultimodalLLMs(MLLMs)withintensivetraining(Zhang
To address these challenges, we introduce FLAME etal.2024).Giventherelativeunderexplorationofoutdoor
(FLAMingo-Architected Embodied Agent), the first VLN,ourworkaddressesthisgapbyintroducinganeffec-
MLLM-based agent designed for urban VLN tasks, as tivelyadaptedMLLM-basedagentforurbanVLNtasks.
shown in Figure 1(b). Based on Flamingo (Alayrac et al.
2022), FLAME operates autoregressively and efficiently 2.2 MultimodalLargeLanguageModels
handles multiple perceptions without increasing context The emergence of Multimodal Large Language Models
length, ensuring efficiency in end-to-end training and (MLLMs) (Liu et al. 2024; Wang et al. 2023a; Zhu et al.
inference. We propose a three-phase tuning technique to 2024; Alayrac et al. 2022; Awadalla et al. 2023) has ex-
adapt Flamingo model (Alayrac et al. 2022) to navigation pandedtheirapplicationtovarioustasks,includingcaption-
tasks using augmented data: 1) Single perception tuning: ing(Lietal.2023b),grounding(Pengetal.2023),andgen-
Learning to describe street views. 2) Multiple perception eralinstructionfollowing(Daietal.2023;Baietal.2023).
tuning: Learning to summarize agent trajectories. 3) End- Thesemodelsdemonstratemultimodalreasoningskills(Lu
to-EndtrainingandevaluationonVLNdatasets.Tosupport etal.2022;Zengetal.2023;Luetal.2024)inchatconversa-
thefirsttwotuningphases,weutilizeGPT-4(Achiametal. tions,handlingsingle-turn,in-context(Sunetal.2024),and
2023) to synthesize captions and route summaries for the interleaved text and image inputs (Laurenc¸on et al. 2024a;
Touchdown environment (Chen et al. 2019). Additionally, Jiangetal.2024).MLLMshavebenefitedfromvision-and-
wesynthesizenavigationrationalesforurbanVLNdatasets language tuning (Laurenc¸on et al. 2024b), enabling them
(Chen et al. 2019; Schumann and Riezler 2021) to validate to process diverse modalities (Zhan et al. 2024a; Wu et al.
FLAME’sreasoningcapability(Weietal.2022). 2023). However, the general pretraining is insufficient for
Experimental results demonstrate FLAME’s superiority expert navigation tasks. Our work addresses this gap by
over existing methods on two urban VLN datasets: Touch- adapting an MLLM from general scenarios to navigation
down(Chenetal.2019)andMap2seq(SchumannandRie- throughaspecializedtuningapproach.
zler 2021). Our approach significantly outperforms current
state-of-the-art(SOTA)methodsby7.3%TCinTouchdown 2.3 DataAugmentationinVision-and-Language
and 3.74% TC in Map2seq, establishing new standards for Navigation
urban VLN tasks. Our work not only benefits the field of
To overcome data scarcity in navigation, various data aug-
urban VLN but also showcases the potential of MLLMs in
mentation techniques (Chen et al. 2022a; Ku et al. 2020;
navigationwithincomplexenvironmentsandtasks.
Zhaoetal.2021;Huangetal.2019b,a)havebeenproposed.
Insummary,ourcontributionsarethreefold:
Theseincludeutilizingaspeakermodule(Friedetal.2018;
• WeintroduceFLAME,toourknowledge,thefirstagent Dou and Peng 2022) to generate synthetic instructions,
based on Multimodal LLM (MLLM) for urban Vision- leveraging multilingual data (Li, Tan, and Bansal 2022a),
and-LanguageNavigation(VLN)tasks. incorporatingcounterfactualinformation(Wangetal.2022;
Parvaneh et al. 2020; Fu et al. 2020), and altering environ-
• We propose a tailored three-phase tuning technique for
ments(Li,Tan,andBansal2022b;Liuetal.2021).Forout-
adapting Flamingo into navigation scenarios using syn-
door VLN, previous studies have explored training agents
theticdata,fullyunleashingMLLM’spower.
on instructions with different styles (Zhu et al. 2021), pre-
• Experiments show the superiority of FLAME over cur-
trainingonauxiliarytasks(Armitage,Impett,andSennrich
rentSOTAs.FLAME’sperformanceprovesthatMLLMs
2023)andusingdrivingvideos(Lietal.2024).However,the
cansignificantlyoutperformspecializedmodels,opening
use of auxiliary training data to tailor MLLMs for outdoor
newavenuesforresearchinembodiedAI.
navigationremainslargelyunexplored.Ourworkaddresses
thisgapbyusingsyntheticdatatoadaptMLLMsforurban
2 RelatedWorks
VLNtasks.
2.1 Vision-and-LanguageNavigation
3 Method
Vision-and-Language Navigation (VLN) (Anderson et al.
2018) encompasses indoor (Qi et al. 2020; Krantz et al. We present FLAME (Flamingo-Architected Embodied
2020) and outdoor scenarios (Chen et al. 2019; Schumann Agent),anagentforurbanVision-and-LanguageNavigation
andRiezler2021),withadvancementsinmultimodalunder- (VLN).Ourmethodcomprisesthreekeycomponents:1)An
standing(Tan,Yu,andBansal2019;Hongetal.2021;Zhu architecture for urban navigation. 2) A three-phase tuning
etal.2020)andactionstrategies(Qietal.2020;Chenetal. technique.3)Asyntheticdatacreationprocess.
2022b),primarilyforindoorenvironments.TraditionalVLN
TaskFormulation
agentsoftenlackadvanceddecision-makingskills,prompt-
ingtheintegrationofLargeLanguageModels(LLMs)(Lin WeformalizeurbanVLNasfollows:Givenanavigationin-
et al. 2024b,a; Zhan et al. 2024b; Schumann et al. 2023), struction I = {w ,w ,...,w }, an agent starts in an ini-
1 2 n
leveragingthereasoning(Panetal.2024;Chenetal.2024; tialstateS .Ateachtimestept,theagentselectsanaction
0(b) (c)
LM BLOCK With
Masked Rationale?
Y N
Non-masked At Key
Tuned Layer Location?
STRIDED GATED XATTN
Key Location t 1 Rationale Generation
Visited Location Perceiver t 2
Current Location Resampler Rationale
…
AsIwalkdownthestreet,Ipassthe
t n (a) CLIP traffic light without any turns,
… lang-x keeping an eye out for the fourth
light.Atthethirdlight,IseeChase
Bankontheleftcorner…
t 2 LM BLOCK
t
1
Decision
Instruction History
“Go past the 1st Turn around.
light. At the 2nd Head forward. Action
light with Chase …. Head Forward.
at the left… _____________
Next Step Autoregressive Update
Figure 2: Overview of FLAME’s navigation process at time step t . The model architecture, based on Flamingo, integrates
n
vision modules for observation processing and decoder blocks for instruction and history handling. The finetuned STRIDED
GATEDXATTNlayersprioritizerecentobservationsincross-attentioncomputation.Atkeylocations,FLAMEcanengagein
reasoningbeforedecision-makingorproceeddirectlytoactionselection.Thenavigationprocessisautoregressive.
a ∈ AbasedonthecurrentobservationO andinstruction where CMA denotes cross-modal attention, W , Wv,
t t Q K
I, where A = {Forward,Left,Right,Stop,TurnAround}. Wv are learnable parameters, and xv refers to the j-th vi-
V j
Theenvironment’sstatetransitionfunctionT :S×A→S sualtoken.Thisapproachisaimedatprioritizingrecentob-
updatestheagent’sstatetoS .Thisprocesscontinuesun- servations, thereby augmenting the system’s proficiency in
t+1
tiltheagentselectstheStopaction.Navigationissuccessful identifyingandrespondingtosignificantfeaturesintheen-
iftheagentstopsatthetargetnodeorwithinonestepofit. vironmentdynamically.
Figure 2(a) provides an illustration of agent’s observations
Action Prediction As Figure 2(c) shows, FLAME oper-
andtrajectory.
atesautoregressively,predictingactionsbasedontheinitial
instructionI,currentobservationO ,historyofobservations
3.1 FLAMEArchitecture t
O andpreviousactions.Bydefault,ateachtimestept,
≤t−1
FLAME builds upon the Flamingo architecture (Alayrac
thenextactiona isobtainedby:
t
etal.2022),leveragingcross-attentionforprocessingofvi-
sualandtextualinputs,withoutextendingcontextlengthand
a =MLLM(I,O ,a ,...,O ,a ,O ). (2)
t 1 1 t−1 t−1 t
improves efficiency. We introduce two key adaptations to
Optionally, to make agent’s thought process transparent
FlamingoforurbanVLN:
and understandable, FLAME generate rationales at key lo-
Strided Cross-Attention To handle the large number of cations(e.g.,intersections)beforeactionprediction,similar
observations in urban VLN, we implement strided cross- toReAct(Yaoetal.2023).LetR denotethegeneratedra-
t
attention (Child et al. 2019) in the cross attention layer, as tionale,thenextactioncanbeobtainedby:
depicted in Figure 2(b). The Perceiver Resampler module
transformsCLIP(Radfordetal.2021)featuresintoacom-
a =MLLM(I,O ,a ,...,O ,a ,O ,R ). (3)
pact set of visual tokens, with N tokens per observation. t 1 1 t−1 t−1 t t
r
Let Xv ∈ RN×d represent flattened visual tokens up to
t 3.2 Three-PhaseTuningforNavigation
timestep t, where N = N · t. The LM block outputs a
r
word embedding matrix X, with X as the the segment of ToadaptFlamingoforurbanVLNtasks,weproposeathree-
t
context corresponds to t, and l as the stride length. Pattern phasetuningparadigm,asdepictedinFigure3(a).Thisap-
S ={S ,...,S }definesvisualtokenindicesforattentionat proach progressively builds the model’s capabilities from
1 t
t,withS ={k,k+1,...,t·N }andk =max(0,(t−l)·N ). understandingenvironmenttocomplexdecision-making.
t r r
Thestridedcross-attentionscoreisgivenby:
Single Perception Tuning In the first phase, we train
FLAMEonastreetviewcaptioningtasktodevelopitsfea-
A(X t,S t)=CMA(X tW Q,(xv jW K) j∈St,(xv jW V) j∈St), turerecognitionabilities.GivenadatasetD p1 = {τ(i)}N i=1,
(1) where each instance τ(i) = {(P(i),O(i),c(i))} consists of
stride(a) (b)
1. Single Perception Tuning 2. Multiple Perception Tuning Observation Landmark Graph Node 2: traffic signals (ahead)
I've aligned Node 3: clothing store(right)
Image traT fh fie cr le i gis h ta in Sub-routes fw loi wth a t nh de mtr oa vff eic d …
Caption the image … summary forward… Node 6: McDonalds (left)
Large Multimodal Model Large Language Model
Flamingo Model Flamingo Model (GPT-4V) (GPT-4)
6
5
4
Question Simple … A prominent traffic light is 3 I crossed the intersection
Instruction visible in the image, along 2 with clothing store on my
3. Navigation Tuning Forward Left Stop
tt
hrw rai oft
f
uh
ic
g
s hle
i
ogv uhe tr
t
sa
th
l
s
es
c
m
a st
ca
t
eel nl re eer
…d
Node
2-6
1
wri ig th ht M, n co Dw
o
c
nI
o
a
rr ne lda esc
r
.h
o
nt h te
h
eli g leh ft
t
…
FLAME Navigation Instruction (From node 1 to 6):
Urban-VLN Move into the intersection. You’ll see a clothing store on your
Dataset InsT tra us ck ti on … CI am pa tig oe n right. Go all the wa Ly a d rgo ew Ln a t no g uth ae g ee n Md o o df e t lhis block and stop. S Su ub m-r mou at re ys
(GPT-4)
(c)
Navigation Tuning I've seen… Now that…
(with rationale) Forward Left Stop Now I’ve reached the second light. I see a prominent traffic light
… McDonalds on the left. STOP.
Ur Db aa tn a- sV eL tN FLAME RS ay tn ioth ne at li ec s If all coherent InO str ri ugi cn ta iol n R Ra at ti io on na al le e a at t N No od de e 2 6
(subset)
Task … If flawed, drop
Instruction
Figure3:Illustrationofthethree-phasetuningfornavigationandsyntheticdatagenerationprocess.(a)Thefirstphasetrainsthe
modelonsingle-perceptiontasks.Thesecondphaseescalatestohandlingmulti-perceptualinput.Finally,themodelundergoes
anend-to-endfinetuning.(b)WeutilizeLLMstogeneratestreetviewcaptionsandroutesummariestoaidthetrainingofthe
firsttwophase.(c)WefurthersynthesizerationalesfortovalidatethereasoningcapabilityofFLAME.
ataskpromptP(i),anobservationO(i),andagroundtruth 3.3 SyntheticDataGeneration
captionc(i),thetrainingobjectiveisdefinedas:
To support the finetuning of our agent, we leverage LLMs
(cid:88)N toautomaticallysynthesizestreetviewcaptions,routesum-
L =− logp(c(i)|P(i),O(i);θ), (4)
p1 maries(Figure3(b)),andnavigationrationales(Figure3(c)).
i=1
whereθrepresentslearnableparametersofthemodel. Street View Caption Generation We focus on generat-
ingcaptionsforstreetviewsatkeylocations,whicharecru-
Multiple Perception Tuning The second phase focuses
cial for successful navigation. These images are processed
on synthesizing information from sequential observations,
byGPT-4V(Achiametal.2023)toproducecorresponding
crucial for navigation understanding. Using a trajectory
descriptions.Toensurediversity,werandomlyselectmeta-
summary dataset D = {τ(i)}N , where τ(i) =
p2 i=1 prompts from acurated list, guiding the caption generation
{(P(i),O(i),...,O(i) ,s(i))},weminimize:
process.
1 T(i)
N
L =−(cid:88) logp(s(i)|P(i),O(i),...,O(i) ;θ). (5) Trajectory Summary Generation We construct a com-
p2 1 T(i) prehensive knowledge graph of landmarks by combining
i=1
placesofinterestsfromOpenStreetMapwithadditionalmap
Here,s(i)isthegroundtruthsummary,andT(i)isthenum-
information,followingthemethodologyof(Schumannand
berofimagesinthei-thinstance.
Riezler2021).Usingthisgraphasinput,weemployGPT-4
End-to-EndNavigationTuning Thefinalphasefinetunes togeneratedetailedroutesummariesbetweenkeylocations.
FLAMEontheVLNdatasetD ={τ(i)}N ,whereeach
vln i=1 RationaleGenerationforVLNDatasets Tovalidaterea-
instanceτ(i) = {(I(i),O(i),a(i),...,O(i) ,a(i) )}contains
1 1 T(i) T(i) soning capabilities of our model, we generate synthetic ra-
an instruction I(i), observations O(i), and ground truth ac- tionales for the VLN datasets. The process involves seg-
t
tionsa(i).Weminimize: menting each VLN trajectory into sub-routes between key
t
locations, then retrieving the corresponding image caption
L vln
=−(cid:88)N (cid:88)T(i)
logp(a( ti)|I(i),O ≤(i) t,a( ≤i) t−1;θ). (6)
a lin zd es Gu Pb T-r -o 4u tt oe gsu em nem raa tr ey afo syr ne ta hc eh ticke ry atl io oc na at li eon a. tW eae chth ke en yu lt oi-
-
i=1 t=1 cation,basedontheretrievedinformation.Tomaintaindata
This multi-phase approach progressively builds FLAME’s quality,wediscardinstancescontainingrationalesthatcon-
capabilities,frombasicurbanenvironmentunderstandingto tradictgroundtruthactions.Thisresultsinaalteredsubsetof
complexdecision-making,enablingittoeffectivelyprocess theVLNdatasetaugmentedwithsyntheticrationales,which
navigationalinstructionsandenvironmentalcues. facilitatesend-to-endtrainingwithreasoningcapabilities.Touchdown Map2seq
DevSet TestSet DevSet TestSet
Model TC↑ SPD↓ nDTW↑ TC↑ SPD↓ nDTW↑ TC↑ SPD↓ nDTW↑ TC↑ SPD↓ nDTW↑
RCONCAT(2019) 10.60 20.4 22.50 11.80 20.40 22.90 17.10 - 30.70 14.70 - 27.70
GA(2019) 12.00 18.70 25.20 11.9 19.00 24.90 18.20 - 33.00 17.00 - 30.10
VLN-Trans(2021) 15.00 20.30 27.00 16.20 20.80 27.80 18.60 - 31.10 17.00 - 29.50
ARC+L2S(2020) 19.48 17.05 - 16.68 18.84 - - - - - - -
ORAR(2022) 30.05 11.12 45.50 29.60 11.79 45.30 49.88 5.87 62.70 47.75 6.53 62.10
VELMA(2023) 29.83 14.67 43.44 27.38 15.03 41.93 52.75 6.78 66.45 48.70 6.80 62.37
VLN-Video(2024) 34.50 9.60 - 31.70 11.2 - - - - - - -
Loc4Plan(2024) 34.50 10.50 - 32.90 11.50 - 48.00 7.00 - 45.30 7.20 -
RCONCAT*(2019) 9.27 19.14 16.40 8.34 21.02 15.70 8.25 38.71 12.51 7.63 41.22 12.03
GA*(2019) 7.12 10.97 13.27 6.63 11.02 12.44 6.50 38.74 11.67 6.07 41.25 10.88
ORAR*(2022) 20.99 20.91 30.78 21.34 21.76 32.44 49.88 6.74 62.96 48.53 7.22 61.91
FLAME 41.28 9.14 55.96 40.20 9.53 54.56 56.95 5.95 71.36 52.44 5.91 67.72
Table1:Comparisonwithstate-of-the-artmodelsonTouchdownandMap2seqdatasets.Modelsdenotedby(*)utilizeimage
featuresconsistentwithFLAME’simplementation.Boldvaluesindicatebestperformance.
4 Experiments Here,I isthei-thinstruction,Rj istheagent’srationale,
i i
4.1 ExperimentSetup Aj is the action, and γ(Rj) is the ground truth rationale at
i i
thej-thkeylocation.M andK arethenumberofkeyloca-
Datasets WeevaluateourapproachontwourbanVision- i i
tionoverlapsandvisitedkeylocations,respectively.N isthe
and-Language Navigation (VLN) datasets: Touchdown
totalnumberofinstances.FunctionsCFR (·)andCFR (·)
(Chen et al. 2019) and Map2seq (Schumann and Riezler rc ra
useGPT-4(Achiametal.2023)toevaluaterationaleconsis-
2021), both set in the StreetLearn environment (Mirowski
tencyandactionalignment(trueorfalse),respectively.
et al. 2018). Touchdown contains 9,326 instruction-
trajectorypairs,whileMap2seqcomprises7,672pairs.The ImplementationDetails OuragentisbuiltuponOtterand
augmenteddatasetforthefirsttwotrainingphasescontains OpenFlamingo(Lietal.2023a;Awadallaetal.2023),inte-
2,354and4,674instances,respectively.Ouragentisbench- grating CLIP (Radford et al. 2021) and LLaMA (Touvron
markedagainstothersontheoriginaldatasets. et al. 2023), trained on a single A100 GPU. To accommo-
Wecollect6,518(outof9,326)and6,291(outof7,672) date CLIP’s input size, we center-crop and resize panora-
pairs grounded with rationales at key locations using GPT- mas, which differs from the broader visual context used in
4forTouchdownandMap2seq,respectively.Weformulate existingmodelsbutalignsbetterwithMLLM’snature.
synthetic pairs to create a dataset for evaluating reasoning
capabilities.Thereasoningperformanceisevaluatedexclu- 4.2 ComparisonwithSOTAs
sivelyonthesubsetcontainingsyntheticrationalestoensure
Inthesection,wecompareFLAMEwithpreviousstate-of-
afaircomparison.
the-art (SOTA) approaches. As shown in Table 1, FLAME
Metrics For the original VLN datasets, we employ three establishesnewSOTAperformanceonbothTouchdownand
metricsforperformanceevaluation:TaskCompletion(TC), Map2seq datasets. On Touchdown test split, FLAME sur-
Shortest-Path Distance (SPD) and Normalized Dynamic passes the previous SOTA, Loc4Plan (Tian et al. 2024),
TimeWarping(nDTW).Specifically,TCrepresentstheper- by 7.3% in TC and %1.97 in SPD. This demonstrates the
centage of successful navigation. SPD calculates the mini- superiority of our MLLM-based approach in comprehend-
mumdistancefromthestoplocationtothegoal.nDTWas- ingnavigationalinstructionsandenvironmentalcues,result-
sessestheoverlapbetweentheagent’sandthegroundtruth ing in higher success rates and better path adherence. For
trajectories. Map2seq, FLAME outperforms VELMA (Schumann et al.
Tofurtherevaluatetheagent’sreasoningcapabilitieswith 2023),anLLM-basedmethod,witha3.74%increaseinTC
syntheticrationales,weintroducetwonewmetrics: anda5.35%improvementinnDTW.Thishighlightsthead-
vantage of Multimodal LLMs in capturing comprehensive
• RationaleCoherence(RC):
information compared to text-only LLM, which may intro-
RC=
(cid:80)N i=1(cid:80)M j=i 1CFR rc(I i,γ(R ij),R ij)
(7)
duceinformationlosswhentranslatingvisualdata.
(cid:80)N
M
Furthermore, we evaluated open-sourced methods in our
i=1 i visualsetting,whichfeaturesarestrictedfieldofviewsimi-
• Rationale-ActionAlignment(RA): lartohumanvision.Theresults,markedwithanasterisk(*)
inTable1,showasignificantperformancedropforbaseline
(cid:80)N (cid:80)Ki CFR (Rj,Aj)
RA= i=1 j=1 ra i i (8) models, especially on the Touchdown dataset. This under-
(cid:80)N K scores the dependence of baseline methods on panoramic
i=1 iTouchdown(subset) Map2seq(subset)
DevSet TestSet DevSet TestSet
Param TC↑ RC↑ RA↑ TC↑ RC↑ RA↑ TC↑ RC↑ RA↑ TC↑ RC↑ RA↑
T =0.0,P =1 37.45 84.73 97.41 37.73 85.96 99.44 49.42 82.54 97.28 45.60 84.38 96.61
T =0.7,P =4 38.27 82.07 96.62 36.77 82.37 97.40 50.19 83.97 97.07 48.92 85.86 96.21
T =1.0,P =4 37.27 80.90 97.47 36.96 84.12 97.77 46.33 81.34 96.63 46.58 86.97 97.74
T =0.7,P =8 37.18 84.83 98.60 37.54 83.71 97.71 52.32 82.38 96.92 45.60 85.32 97.97
T =1.0,P =8 39.17 86.48 97.48 37.82 86.80 96.91 48.84 86.61 96.98 49.51 88.00 99.25
Table 2: Performance comparison of the agent’s reasoning capability on urban VLN dataset subsets under varying decoding
temperaturesandpaths.ThetemperatureT regulatestherandomnessinsampling,andthenumberofdecodingpathsP referto
thenumberoftimestheagentsamplesrationale-to-actionpairs,withthefinalactiondeterminedthroughavotingprocess.
Touchdown Map2seq Touchdown Touchdown (subset) Map2seq Map2seq (subset)
Method
RC RA RC RA Task Completion Normalized Dynamic Time Warping
70
55
Human 82.88 100.00 85.29 97.64
65
Vanilla 92.79 99.10 88.82 95.88 50
Calibrated 81.08 100.00 84.12 97.06 60
45
55
40
Table3:Comparisonofmetriccalculationmethods.
50
35
1 2 3 4 1 2 3 4
Stride Size Stride Size
environmental perception. In contrast, FLAME excels at
Figure4:TheeffectofvaryingstridesonTCandnDTW.
navigating with limited visual input, more closely approx-
imatinghumannavigationcapabilities.
manassessments.However,ourcalibratedapproachsignif-
4.3 ReasoningPerformance
icantlyreducedthesedisparities,demonstratingthehuman-
We evaluated FLAME’s reasoning capabilities during nav- comparable reliability of the automatic evaluation method.
igation using the self-consistency approach (Wang et al. Consequently, we employ this calibrated evaluation tech-
2023b),exploringvariousdecodingpathsandtemperatures. nique as default. Specifically, We calibrate the outputs of
The results are presented in Table 2. Rationale Coherence CFR andCFR asfollows:
rc ra
(RC)andRationale-ActionAlignment(RA)consistentlyre-
• When action at key location is incorrect: If CFR = 1
mainedabove80%and95%respectively,indicatingrobust ra
butCFR =1,weforceCFR to0.
rationalegenerationandstrongconsistency.Highertemper- rc rc
aturesledtoperformancefluctuations,especiallywithfewer • Whenactionatkeylocationiscorrect:IfCFR rc = 1but
decodingpaths.However,whenweincreasethenumberof CFR ra =0,weforceCFR rato1.
decodingpathsto8,weseemorepronouncedimprovements
4.4 Analyses
inbothTCandRC.OnTouchdown,FLAMEachievedopti-
malTCperformancewithatemperatureof1.0and8decod- This section presents a comprehensive analysis of our ap-
ing paths, outperforming greedy decoding (T=0.0, P=1) by proach, evaluating the strided cross-attention module, the
1.72%inTCand1.75%inRC.ForMap2seq,with8decod- three-phase tuning technique, and providing qualitative in-
ingpaths,FLAMEsurpassedgreedydecodingby3.91%in sightsofnavigationdetails.
TC and 3.62% in RC on the test set. These results demon-
Effect of Strided Cross Attention To investigate the ef-
strate that increased sampling diversity and a larger decod-
fectofthestridedcrossattentionmodule,weconductexper-
ing budget enable FLAME to generate diverse rationales
imentsondifferentstridesizesonbothoriginaldatasetsand
and effectively ensemble reasoning results, leading to im-
subsets in Figure 4. We observe that increasing stride size
proved decision-making. This provides strong evidence for
generallycorrelateswithadecreaseintaskcompletionrates,
thereasoningcapabilitiesdevelopedthroughsyntheticratio-
highlightingtheimportanceofprioritizingcurrentobserva-
nale tuning and the advanced decision-making capacity of
tionsoverlongerhistoryindecision-makingprocesses.Pay-
FLAME’sarchitecture.
ingfullattentiontonumerousobservationsappearstoover-
MetricCalculation Tovalidatethereliabilityofourauto- whelmtheagent,negativelyimpactingperformance.Based
maticmetriccalculations(Eqs.7and8),weconductedhu- onthesefindings,wesetthedefaultstridesizeto1inourim-
manevaluationson50instanceseachfromtheTouchdown plementation. However, nDTW scores for Map2seq fluctu-
and Map2seq datasets. The results, presented in Table 3, ateratherthandeclineconsistently.Thissuggestsstridesize
revealed discrepancies between the vanilla method and hu- maybelesscriticalforpath-relatedperformanceintasksless
)%(
etaR
erocSInstruction:From the starting position, face the closest intersection1. You'll see a bus on your right. Take a right4at the
intersection, and you'll see a bunch of stores on your right. Continue straight until you see a traffic light. At the trafficlight, turn
right16. You'll pass a set of blue bikes on your left. Go straight until the first traffic light. Turn right26at the light, and you'll go down
a narrow one-way street. Continue straight past the scaffoldinguntil the traffic light. Turn right40at the light, and stop42as at the
level of the first blue bike on your left.
1 4 16 26 40 42
ORAR(baseline):
Failed
FLAME:
Turn around. Turn right. Turn right. Turn right. Turn right. Stop.
FLAME (with rationale):
I need to face the I'm supposed to take I need to continue After passing blue I need to continue
closest intersection a right at the straight until I see a bikes on my left, I straight past the
and see a bus on my intersection and see traffic light and then need to go to the scaffoldingand then
Stop.
right. I should adjust a bunch of stores on turn right. I've arriv first light and then t turn right. I’ve
my orientation. my right. I can see ed at the traffic light, urn right. I've reach reached the traffic
Turn around. the stores. Turn right. ed the traffic light, light,
Turn right. Turn right. Turn right.
Figure5:QualitativeanalysisofFLAME’snavigationperformance.Superscriptsonwordsandnumbersinthetopleftcorner
ofeachimageindicatethecountofviewpointsencounteredbytheagent.Theground-truthactionsarerepresentedbycolored
arrows:redcirculararrowsforturnaround,blueforturnright,andacircleforstop.Keywordandlandmarkalignmentishigh-
lightedbymatchingcolorsinresponsesandinstructions.Thetoprowshowsactionstakenbythebaselinemethod(Schumann
andRiezler2022),whilesubsequentrowsdisplayFLAME’sresponses.
Touchdown Map2seq QualitativeAnalysisofNavigation Toillustratethenavi-
P1 P2
TC↑ SPD↓ nDTW↑ TC↑ SPD↓ nDTW↑ gationcapabilitiesofFLAMEandcompareitwiththebase-
line method (Schumann and Riezler 2022), we present a
× × 40.16 9.02 55.31 53.57 6.55 66.70
✓ × 41.15 9.19 55.26 54.69 6.10 69.70 qualitativeexampleinFigure5.Inthisinstance,ORARfails
✓ ✓ 41.28 9.14 55.96 56.95 5.95 71.36 atthefourthintersection,whileFLAMEsuccessfullycom-
pletesthenavigationtask.Theagent’sresponses,accompa-
niedbyrationales,accuratelyidentifykeylandmarkssuchas
Table4:Ablationstudyofthree-phasetuningfornavigation.
“trafficlight”and“scaffolding”,whichaligncloselywiththe
giveninstructions.FLAMEdemonstratesproficiencyinfol-
lowinginstructionsandcapturingsalientenvironmentalde-
dependent on visual information. The lower TC on subset
tails.ThisexamplehighlightstheMLLM-basedapproach’s
canbeattributedtofewertraininginstances.
effectiveness in correlating specific environmental features
EffectivenessofThree-PhaseTuningTechnique Table4 withverbalnavigationinstructions.
presentsourinvestigationintotheimpactofthree-phasetun-
ingonnavigationperformance.Thefirstrow,whereboththe
firstphase(P1)andsecondphase(P2)areomitted,demon-
5 Conclusion
strates sub-optimal navigation performance. Implementing
thefirstphasetuningyieldsincreasesof0.99%and1.12%in
TCfordevsplitsofTouchdownandMap2seq,respectively, In this paper, we introduced FLAME, a Multimodal LLM-
suggestingimprovedenvironmentalfamiliarity.Thesecond based agent for urban Vision-and-Language Navigation
phase of tuning leads to optimal performance across both tasks. By adapting the architecture through a novel three-
datasets, indicating the importance of tuning with sequen- phasetuningtechniqueandsyntheticdata,FLAMEachieves
tial observations. These results underscore the crucial role state-of-the-artperformanceinurbanVLN.Thecomparison
of phased learning in equipping the MLLM with advanced results and reasoning performance demonstrate FLAME’s
navigationalskills,fromsingleperceptiontosequentialtra- superior ability to integrate verbal and environmental cues
jectory understanding, facilitating adaptation from general fordecision-making.Theeffectivenessofourproposedtun-
scenarios. Notably, even the non-phased training surpasses ing technique and other components is validated through
currentstate-of-the-artresults,furthervalidatingtheefficacy comprehensiveanalyses.Thesefindingshighlightthepoten-
ofFLAME’sarchitecture. tialofMultimodalLLMsincomplexnavigationtasks.References Advances in Neural Information Processing Systems, vol-
ume36,49250–49267.
Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.;
Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.; Dou, Z. Y.; and Peng, N. 2022. Foam: A follower-aware
Anadkat, S.; et al. 2023. Gpt-4 technical report. arXiv speakermodelForvision-and-languagenavigation. InPro-
preprintarXiv:2303.08774. ceedingsoftheConferenceoftheNorthAmericanChapter
Alayrac,J.B.;Donahue,J.;Luc,P.;Miech,A.;Barr,I.;Has- of the Association for Computational Linguistics: Human
son,Y.;Lenc,K.;Mensch,A.;Millican,K.;Reynolds,M.; LanguageTechnologies,4332–4340.
et al. 2022. Flamingo: a visual language model for few-
Fried, D.; Hu, R.; Cirik, V.; Rohrbach, A.; Andreas, J.;
shotlearning. InAdvancesinNeuralInformationProcess-
Morency,L.P.;BergKirkpatrick,T.;Saenko,K.;Klein,D.;
ingSystems,volume35,23716–23736.
and Darrell, T. 2018. Speaker-follower models for vision-
Anderson, P.; Wu, Q.; Teney, D.; Bruce, J.; Johnson, M.; and-language navigation. In Advances in Neural Informa-
Su¨nderhauf, N.; Reid, I.; Gould, S.; and Van Den Hen- tionProcessingSystems,volume31.
gel,A.2018. Vision-and-languagenavigation:Interpreting
Fu,T.J.;Wang,X.E.;Peterson,M.F.;Grafton,S.T.;Eck-
visually-grounded navigation instructions in real environ-
stein,M.P.;andWang,W.Y.2020. Counterfactualvision-
ments.InProceedingsoftheIEEEConferenceonComputer
and-languagenavigationviaadversarialpathsampler.InEu-
VisionandPatternRecognition,3674–3683.
ropeanConferenceonComputerVision,71–86.
Armitage, J.; Impett, L.; and Sennrich, R. 2023. A pri-
Hong,Y.;Wu,Q.;Qi,Y.;RodriguezOpazo,C.;andGould,
ority map for vision-and-language navigation with trajec-
S.2021. Vlnbert:Arecurrentvision-and-languagebertfor
toryplansandfeature-locationcues. InProceedingsofthe
navigation. InProceedingsoftheIEEE/CVFConferenceon
IEEE/CVFWinterConferenceonApplicationsofComputer
ComputerVisionandPatternRecognition,1643–1653.
Vision,1094–1103.
Awadalla, A.; Gao, I.; Gardner, J.; Hessel, J.; Hanafy, Y.; Huang, H.; Jain, V.; Mehta, H.; Baldridge, J.; and Ie, E.
Zhu, W.; Marathe, K.; Bitton, Y.; Gadre, S.; Sagawa, S.; 2019a. Multi-modal Discriminative Model for Vision-
et al. 2023. Openflamingo: An open-source framework for and-Language Navigation. In Proceedings of the Com-
traininglargeautoregressivevision-languagemodels. arXiv bined Workshop on Spatial Language Understanding and
preprintarXiv:2308.01390. GroundedCommunicationforRobotics,40–49.
Bai, J.; Bai, S.; Yang, S.; Wang, S.; Tan, S.; Wang, P.; Huang, H.; Jain, V.; Mehta, H.; Ku, A.; Magalhaes, G.;
Lin, J.; Zhou, C.; and Zhou, J. 2023. Qwen-vl: A frontier Baldridge,J.;andIe,E.2019b. Transferablerepresentation
large vision-language model with versatile abilities. arXiv learninginvision-and-languagenavigation. InProceedings
preprintarXiv:2308.12966. oftheIEEE/CVFInternationalConferenceonComputerVi-
sion,7404–7413.
Chen, H.; Suhr, A.; Misra, D.; Snavely, N.; and Artzi, Y.
2019. Touchdown:Naturallanguagenavigationandspatial Jiang, D.; He, X.; Zeng, H.; Wei, C.; Ku, M.; Liu, Q.; and
reasoning in visual street environments. In Proceedings of Chen,W.2024. Mantis:Interleavedmulti-imageinstruction
theIEEE/CVFConferenceonComputerVisionandPattern tuning. arXivpreprintarXiv:2405.01483.
Recognition,12538–12547.
Krantz,J.;Wijmans,E.;Majumdar,A.;Batra,D.;andLee,
Chen,J.;Lin,B.;Xu,R.;Chai,Z.;Liang,X.;andWong,K.
S.2020. Beyondthenav-graph:Vision-and-languagenavi-
Y. K. 2024. Mapgpt: map-guided prompting with adaptive
gationincontinuousenvironments.InEuropeanConference
path planning for vision-and-language navigation. In Pro-
onComputerVision,104–120.
ceedingsofthe62ndAnnualMeetingoftheAssociationfor
ComputationalLinguistics(Volume1:LongPapers),9796– Ku, A.; Anderson, P.; Patel, R.; Ie, E.; and Baldridge,
9810. J. 2020. Room-across-room: Multilingual vision-and-
language navigation with dense spatiotemporal grounding.
Chen,S.;Guhur,P.L.;Tapaswi,M.;Schmid,C.;andLaptev,
InProceedingsoftheConferenceonEmpiricalMethodsin
I. 2022a. Learning from unlabeled 3d environments for
NaturalLanguageProcessing,4392–4412.
vision-and-language navigation. In European Conference
onComputerVision,638–655. Laurenc¸on, H.; Saulnier, L.; Tronchon, L.; Bekman, S.;
Singh,A.;Lozhkov,A.;Wang,T.;Karamcheti,S.;Rush,A.;
Chen,S.;Guhur,P.L.;Tapaswi,M.;Schmid,C.;andLaptev,
Kiela,D.;etal.2024a. Obelics:Anopenweb-scalefiltered
I. 2022b. Think global, act local: Dual-scale graph trans-
datasetofinterleavedimage-textdocuments.InAdvancesin
formerforvision-and-languagenavigation. InProceedings
NeuralInformationProcessingSystems,volume36,71683–
oftheIEEE/CVF ConferenceonComputerVisionand Pat-
71702.
ternRecognition,16537–16547.
Child, R.; Gray, S.; Radford, A.; and Sutskever, I. 2019. Laurenc¸on,H.;Tronchon,L.;Cord,M.;andSanh,V.2024b.
Generatinglongsequenceswithsparsetransformers. arXiv Whatmatterswhenbuildingvision-languagemodels?arXiv
preprintarXiv:1904.10509. preprintarXiv:2405.02246.
Dai,W.;Li,J.;Li,D.;Tiong,A.;Zhao,J.;Wang,W.;Li,B.; Li,B.;Zhang,Y.;Chen,L.;Wang,J.;Yang,J.;andLiu,Z.
Fung, P.; and Hoi, S. 2023. Instructblip: Towards general- 2023a. Otter:Amulti-modalmodelwithin-contextinstruc-
purposevision-languagemodelswithinstructiontuning. In tiontuning. arXivpreprintarXiv:2305.03726.Li, J.; Li, D.; Savarese, S.; and Hoi, S. 2023b. Blip-2: Neural Information Processing Systems, volume 33, 5296–
Bootstrappinglanguage-imagepre-trainingwithfrozenim- 5307.
age encoders and large language models. In International Peng, Z.; Wang, W.; Dong, L.; Hao, Y.; Huang, S.; Ma,
ConferenceonMachineLearning,19730–19742. S.; and Wei, F. 2023. Kosmos-2: Grounding multimodal
Li,J.;Padmakumar,A.;Sukhatme,G.;andBansal,M.2024. large language models to the world. arXiv preprint
Vln-video:Utilizingdrivingvideosforoutdoorvision-and- arXiv:2306.14824.
language navigation. In Proceedings of the AAAI Confer- Qi,Y.;Wu,Q.;Anderson,P.;Wang,X.;Wang,W.Y.;Shen,
enceonArtificialIntelligence,volume38,18517–18526. C.; and Hengel, A. v. d. 2020. Reverie: Remote embodied
Li, J.; Tan, H.; and Bansal, M. 2022a. Clear: Improving visual referring expression in real indoor environments. In
vision-languagenavigationwithcross-lingual,environment- ProceedingsoftheIEEE/CVFConferenceonComputerVi-
agnosticrepresentations. InFindingsoftheAssociationfor sionandPatternRecognition,9982–9991.
ComputationalLinguistics:NAACL,633–649. Qiao,Y.;Qi,Y.;Yu,Z.;Liu,J.;andWu,Q.2023. Marchin
Li,J.;Tan,H.;andBansal,M.2022b. Envedit:Environment chat: Interactive prompting for remote embodied referring
editingforvision-and-languagenavigation. InProceedings expression. InProceedingsoftheIEEE/CVF International
oftheIEEE/CVF ConferenceonComputerVisionand Pat- ConferenceonComputerVision,15758–15767.
ternRecognition,15407–15417. Radford,A.;Kim,J.W.;Hallacy,C.;Ramesh,A.;Goh,G.;
Lin,B.;Nie,Y.;Wei,Z.;Chen,J.;Ma,S.;Han,J.;Xu,H.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;
Chang, X.; and Liang, X. 2024a. Navcot: Boosting llm- et al. 2021. Learning transferable visual models from nat-
basedvision-and-languagenavigationvialearningdisentan- ural language supervision. In International Conference on
gledreasoning. arXivpreprintarXiv:2403.07376. MachineLearning,8748–8763.
Lin,B.;Nie,Y.;Wei,Z.;Zhu,Y.;Xu,H.;Ma,S.;Liu,J.;and Schumann, R.; and Riezler, S. 2021. Generating landmark
Liang,X.2024b. Correctablelandmarkdiscoveryvialarge navigation instructions from maps as a graph-to-text prob-
modelsfor vision-languagenavigation. IEEETransactions lem. InProceedingsofthe59thAnnualMeetingoftheAs-
onPatternAnalysisandMachineIntelligence,1–14. sociationforComputationalLinguisticsandthe11thInter-
Liu, C.; Zhu, F.; Chang, X.; Liang, X.; Ge, Z.; and Shen, nationalJointConferenceonNaturalLanguageProcessing
Y.D.2021. Vision-languagenavigationwithrandomenvi- (Volume1:LongPapers),489–502.
ronmentalmixup. InProceedingsoftheIEEE/CVFInterna- Schumann,R.;andRiezler,S.2022. Analyzinggeneraliza-
tionalConferenceonComputerVision,1644–1654. tion of vision and language navigation to unseen outdoor
Liu,H.;Li,C.;Wu,Q.;andLee,Y.J.2024. Visualinstruc- areas. In Proceedings of the 60th Annual Meeting of the
tiontuning. InAdvancesinNeuralInformationProcessing AssociationforComputationalLinguistics(Volume1:Long
Systems,volume36,34892–34916. Papers),7519–7532.
Long,Y.;Li,X.;Cai,W.;andDong,H.2024.Discussbefore Schumann,R.;Zhu,W.;Feng,W.;Fu,T.J.;Riezler,S.;and
moving:Visuallanguagenavigationviamulti-expertdiscus- Wang, W. Y. 2023. Velma: Verbalization embodiment of
sions. In IEEE International Conference on Robotics and llmagentsforvisionandlanguagenavigationinstreetview.
Automation,17380–17387. arXivpreprintarXiv:2307.06082.
Lu,P.;Mishra,S.;Xia,T.;Qiu,L.;Chang,K.W.;Zhu,S.C.; Sun, Q.; Cui, Y.; Zhang, X.; Zhang, F.; Yu, Q.; Wang, Y.;
Tafjord, O.; Clark, P.; and Kalyan, A. 2022. Learn to ex- Rao,Y.;Liu,J.;Huang,T.;andWang,X.2024. Generative
plain:Multimodalreasoning viathoughtchainsfor science multimodalmodelsarein-contextlearners. InProceedings
questionanswering.InAdvancesinNeuralInformationPro- oftheIEEE/CVF ConferenceonComputerVisionand Pat-
cessingSystems,volume35,2507–2521. ternRecognition,14398–14409.
Lu,P.;Peng,B.;Cheng,H.;Galley,M.;Chang,K.W.;Wu, Tan, H.; Yu, L.; and Bansal, M. 2019. Learning to navi-
Y.N.;Zhu,S.C.;andGao,J.2024. Chameleon:Plug-and- gate unseen environments: Back translation with environ-
play compositional reasoning with large language models. mental dropout. In Proceedings of the Conference of the
InAdvancesinNeuralInformationProcessingSystems,vol- North American Chapter of the Association for Computa-
ume36,43447–43478. tional Linguistics: Human Language Technologies, Volume
Mirowski, P.; Grimes, M.; Malinowski, M.; Hermann, 1(LongandShortPapers),2610–2621.
K. M.; Anderson, K.; Teplyashin, D.; Simonyan, K.; Zis- Tian, H.; Meng, J.; Zheng, W. S.; Li, Y. M.; Yan, J.; and
serman, A.; Hadsell, R.; et al. 2018. Learning to navigate Zhang, Y. 2024. Loc4plan: Locating before planning for
incitieswithoutamap. InAdvancesinNeuralInformation outdoor vision and language navigation. In ACM Interna-
ProcessingSystems,volume31. tionalConferenceonMultimedia.
Pan,B.;Panda,R.;Jin,S.;Feris,R.;Oliva,A.;Isola,P.;and Touvron,H.;Lavril,T.;Izacard,G.;Martinet,X.;Lachaux,
Kim, Y. 2024. LangNav: Language as a Perceptual Repre- M. A.; Lacroix, T.; Rozie`re, B.; Goyal, N.; Hambro, E.;
sentationforNavigation. InFindingsoftheAssociationfor Azhar, F.; et al. 2023. Llama: Open and efficient founda-
ComputationalLinguistics:NAACL,950–974. tionlanguagemodels. arXivpreprintarXiv:2302.13971.
Parvaneh, A.; Abbasnejad, E.; Teney, D.; Shi, J. Q.; and Wang,H.;Liang,W.;Shen,J.;VanGool,L.;andWang,W.
Van den Hengel, A. 2020. Counterfactual vision-and- 2022. Counterfactual cycle-consistent learning for instruc-
languagenavigation:Unravellingtheunseen.InAdvancesin tionfollowingandgenerationinvision-languagenavigation.In Proceedings of the IEEE/CVF Conference on Computer Zhu,D.;Chen,J.;Shen,X.;Li,X.;andElhoseiny,M.2024.
VisionandPatternRecognition,15471–15481. Minigpt-4: Enhancing vision-language understanding with
Wang, W.; Lv, Q.; Yu, W.; Hong, W.; Qi, J.; Wang, Y.; Ji, advanced large language models. In The Twelfth Interna-
J.;Yang,Z.;Zhao,L.;Song,X.;etal.2023a. Cogvlm:Vi- tionalConferenceonLearningRepresentations.
sual expert for pretrained language models. arXiv preprint Zhu, W.; Wang, X.; Fu, T. J.; Yan, A.; Narayana, P.; Sone,
arXiv:2311.03079. K.;Basu,S.;andWang,W.Y.2021. Multimodaltextstyle
Wang, X.; Wei, J.; Schuurmans, D.; Le, Q. V.; Chi, E. H.;
transferforoutdoorvision-and-languagenavigation.InPro-
ceedings of the 16th Conference of the European Chapter
Narang, S.; Chowdhery, A.; and Zhou, D. 2023b. Self-
oftheAssociationforComputationalLinguistics:MainVol-
consistency improves chain of thought reasoning in lan-
guage models. In The Eleventh International Conference ume,1207–1221.
onLearningRepresentations. Zhu, Y.; Zhu, F.; Zhan, Z.; Lin, B.; Jiao, J.; Chang, X.;
and Liang, X. 2020. Vision-dialog navigation by explor-
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;
ingcross-modalmemory. InProceedingsoftheIEEE/CVF
Chi,E.;Le,Q.V.;Zhou,D.;etal.2022. Chain-of-thought
Conference on Computer vision and Pattern Recognition,
prompting elicits reasoning in large language models. In
Advances in Neural Information Processing Systems, vol- 10730–10739.
ume35,24824–24837.
Wu, S.; Fei, H.; Qu, L.; Ji, W.; and Chua, T. S. 2023.
Next-gpt: Any-to-any multimodal llm. arXiv preprint
arXiv:2309.05519.
Xiang, J.; Wang, X.; and Wang, W. Y. 2020. Learning
to stop: A simple yet effective approach to urban vision-
language navigation. In Findings of the Association for
ComputationalLinguistics:EMNLP,699–707.
Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan,
K.R.;andCao,Y.2023. React:Synergizingreasoningand
acting in language models. In The Eleventh International
ConferenceonLearningRepresentations.
Zeng,A.;Attarian,M.;Ichter,B.;Choromanski,K.;Wong,
A.; Welker, S.; Tombari, F.; Purohit, A.; Ryoo, M.; Sind-
hwani, V.; et al. 2023. Socratic models: Composing zero-
shotmultimodalreasoningwithlanguage. InTheEleventh
InternationalConferenceonLearningRepresentations.
Zhan,J.;Dai,J.;Ye,J.;Zhou,Y.;Zhang,D.;Liu,Z.;Zhang,
X.;Yuan,R.;Zhang,G.;Li,L.;etal.2024a. Anygpt:Uni-
fiedmultimodalllmwithdiscretesequencemodeling. arXiv
preprintarXiv:2402.12226.
Zhan, Z.; Yu, L.; Yu, S.; and Tan, G. 2024b. Mc-gpt: Em-
poweringvision-and-languagenavigationwithmemorymap
andreasoningchains. arXivpreprintarXiv:2405.10620.
Zhang,J.;Wang,K.;Xu,R.;Zhou,G.;Hong,Y.;Fang,X.;
Wu, Q.; Zhang, Z.; and He, W. 2024. Navid: Video-based
vlmplansthenextstepforvision-and-languagenavigation.
arXivpreprintarXiv:2402.15852.
Zhao, M.; Anderson, P.; Jain, V.; Wang, S.; Ku, A.;
Baldridge,J.;andIe,E.2021. Ontheevaluationofvision-
and-languagenavigationinstructions. InProceedingsofthe
16thConferenceoftheEuropeanChapteroftheAssociation
forComputationalLinguistics:MainVolume,1302–1316.
Zhou, G.; Hong, Y.; Wang, Z.; Wang, X. E.; and Wu, Q.
2024. Navgpt-2: Unleashing navigational reasoning ca-
pability for large vision-language models. arXiv preprint
arXiv:2407.12366.
Zhou, G.; Hong, Y.; and Wu, Q. 2024. Navgpt: Explicit
reasoninginvision-and-languagenavigationwithlargelan-
guage models. In Proceedings of the AAAI Conference on
ArtificialIntelligence,volume38,7641–7649.