Revisiting VerilogEval: Newer LLMs, In-Context
Learning, and Specification-to-RTL Tasks
Nathaniel Pinckney∗†, Christopher Batten‡∗, Mingjie Liu∗, Haoxing Ren∗ and Brucek Khailany∗
∗NVIDIA Corporation, Santa Clara, CA
†Email: npinckney@nvidia.com
‡Cornell, Ithaca, NY
Abstract—The application of large-language models (LLMs) model’s generation performance using in-context learning [9]
to digital hardware code generation is an emerging field. Most examples nor do they provide a detailed way to inspect the
LLMs are primarily trained on natural language and software
reasons for a model’s failure.
code. Hardware code, such as Verilog, represents only a small
This work aims to address these limitations by extending
portionofthetrainingdataandfewhardwarebenchmarksexist.
VerilogEval [5] to support specification-to-RTL tasks in addi-
Toaddressthisgap,theopen-sourceVerilogEvalbenchmarkwas
released in 2023, providing a consistent evaluation framework tion to the original code completion task. We also incorporate
for LLMs on code completion tasks. It was tested on state- a variable number of in-context learning prompts, and pro-
of-the-art models at the time including GPT-3.5, GPT-4, and vide a robust failure classification mechanism, to provide a
codegen-16b-verilog-sft. However, VerilogEval and other Verilog
more comprehensive evaluation framework for Verilog code
generationbenchmarkslackfailureanalysisand,inpresentform,
generation tasks. The significance of this work lies in its
arenotconducivetoexploringpromptingtechniques.Also,since
VerilogEval’s release, both commercial and open-source models potential to push LLM development forward for hardware
have seen continued development. design, through offering insights into model performance and
In this work, we evaluate new commercial and open-source the efficacy of prompt tuning, and to point out differences
models of varying sizes (GPT-4 Turbo, Llama 3.1 8B/70B/405B,
in generation quality across tasks. Even with similar problem
Llama 3 70B, Mistral Large, Deepseek Coder 33B and 6.7B,
statementsandin-contextlearningexamples,wefinddivergent
CodeGemma 7B, and RTL-Coder) against an improved Verilo-
gEvalbenchmarksuite.WeenhanceVerilogEval’sinfrastructure responsesbylarge-languagemodels.Thisvariabilityhighlights
and dataset by automatically classifying failures, introduce new theimportanceofunderstandinghowdifferentmodelsrespond
prompts for supporting in-context learning (ICL) examples, and to various prompts and contexts through the use of the
extend the supported tasks to specification-to-RTL translation.
benchmarks providing granular failure feedback.
We find a measurable improvement in commercial state-of-the-
Moreover, we evaluate newer large-language models than
art models, with GPT-4 Turbo achieving a 59% pass rate on
specification-to-RTL tasks. We also study the performance of those tested in the original VerilogEval paper, including GPT-
open-sourceanddomain-specificmodelsthathaveemergedsince 4 Turbo [10], open-source models like Llama 3.1 [11], and
theoriginalreleaseofVerilogEval,anddemonstratethatmodels domain-specific models such as RTL-Coder [12]. In short, we
canbenefitsubstantiallyfromICL.Wefindthatrecently-released
assess the latest state-of-the-art language models to determine
Llama3.1405Bachievesapassrateof58%,effectivelymatching
the current frontier of LLM-based Verilog code generation
thatofGPT-4Turbo,andthatthemuchsmallerdomain-specific
RTL-Coder 6.7B models achieve an impressive 37% pass rate. while also evaluating the impact of prompt tuning. We find
However, prompt engineering is key to achieving good pass that recent open-source models are becoming competitive
rates, and varies widely with model and task. A benchmark with last year’s closed models, and that prompt tuning varies
infrastructure that allows for prompt engineering and failure
considerably across models.
analysisiskeytocontinuedmodeldevelopmentanddeployment.
The following new features are part of the proposed bench-
IndexTerms—LargeLanguageModels,HardwareDescription mark infrastructure:
Languages, Verilog RTL Generation, Benchmark 1) Specification-to-RTLtasksupport:VerilogEvalonlysup-
ported code completion tasks, such as used in CoPilot
I. INTRODUCTION
[1], while many models are tuned and deployed as
Applications of large-language models (LLMs) to software instruction-tunedmodels[13],withquestionandanswer
coding have reached wide deployment, with examples such prompting.
as Github CoPilot [1]. Yet, applications of LLMs to hardware 2) In-context learning examples: No in-context learning
design are still in their infancy [2], [3]. Only a small handful (ICL)[9]examplesweresupportedaspartoftheprompt
of Verilog code design benchmarks exist in the literature, in VerilogEval.
including RTLLM [4], VerilogEval [5], VeriGen [6], [7], 3) Failure classification: VerilogEval only reported pass/-
and most recently RTL-Repo [8]. While RTLLM bench- fail results of a benchmark problem, and did not give
markedconversationalspecification-to-RTLgenerationperfor- fine-grained feedback on failures.
mance, VerilogEval, VeriGen, RTL-Repo are code completion 4) Makefile-based evaluation environment: The original
benchmarks. Additionally, none of the benchmarks explore a VerilogEval benchmark [5] used a monolithic dataset,
4202
guA
02
]ES.sc[
1v35011.8042:viXrawhereas the proposed infrastructure uses a textfile ap- // binary numbers.
proach. This allows for easier scaling while sweeping module TopModule
(
evaluationsettingsacrossmoremodelsthantheoriginal
input logic [7:0] in ,
benchmark, and easier human inspection of the dataset.
output logic [7:0] out
The improved VerilogEval benchmark is available publicly );
at https://github.com/NVlabs/verilog-eval. // Combinational logic
assign out = in + 1;
endmodule
II. BENCHMARKIMPROVEMENTS
A. Specification-to-RTL Task Support Listing2
THE1-SHOTICLEXAMPLEFORSPECIFICATION-TO-RTLTASKS.THE
The proposed benchmark supports both code comple-
HIGHLIGHTEDCODEISTHEPROMPTSTYLE.
tion and specification-to-RTL tasks to better match the
Question:
instruction-tuning[13]ofrecentmodels.Thefull156-problem
Implement a hardware module named TopModule
dataset from VerilogEval is converted into specification-to-
with the following interface . All input and
RTL prompting in this work. Code completion has the prob- output ports are one bit unless otherwise
lem description in Verilog-compatible comments and always specified .
appends the module interface declaration to the end of the
− input in (8 bits)
prompt.Ontheotherhand,specification-to-RTL’spromptstyle
− output out (8 bits)
is as a chat bot, with well-defined ”Question” and ”Answer”
sections. The specification-to-RTL prompting is implemented The module should implement an incrementer
in a manner similar to the Mostly Basic Python Problems which increments the input by one and writes
(MBPP) benchmark [14] with [BEGIN] and [DONE] tags the result to the output. Assume all values
are encoded as two’s complement binary
surrounding code blocks. Examples of these two styles can
numbers.
be found in listings 1 and 2 with only the highlighted code
indicating the prompt styles. Enclose your code with [BEGIN] and [DONE].
Only output the code snippet and do NOT output
B. Support for In-Context Learning Examples
anything else .
In-context learning (ICL) was proposed by [9] to add
Answer:
examples of task questions and desired responses into the
[BEGIN]
prompt context, so that an LLM can better respond to a
module TopModule
given task. ICL is implemented through simple Verilog code (
examples, tailored for both code completion (Listing 1) and input logic [7:0] in ,
specification-to-RTL tasks (Listing 2). The listings contain output logic [7:0] out
);
the 1-shot examples used for both tasks, except line width
// Combinational logic
and whitespace was adjusted for printing. The examples were
assign out = in + 1;
selected to be short and simple, while including a full module endmodule
(from declaration to endmodule). Two additional examples [DONE]
for each task are added: a sequential incrementer similar to
the first 1-shot example, and a basic finite-state machine for
C. Support for Failure Classification
the third example. The number of shots is parameterized and
can easily be swept to determine sensitivity of a model’s pass FailuresofLLM-generatedresponsesareautomaticallyclas-
rateasICLexamplesareaddedtotheprompt.1-shotincludes sified by broad reasons for failure, both Verilog compile time
onlythecombinationalincrementer,2-shotaddsthesequential errorsandsimulationruntimeerrors,suchasincorrectlyusing
incrementer, and 3-shot includes all three examples in the a wire as a register, incorrect bit widths, and missing mod-
context prompt. ule interface definitions. This classification feature provides
insight into the most common reasons for failures and how
Listing1
THE1-SHOTIN-CONTEXTLEARNINGEXAMPLEFORCODECOMPLETION to mitigate poor code generation through prompt tuning. The
TASKS.THEHIGHLIGHTEDCODEISTHEPROMPTSTYLE. classification is dependent on specific warnings and errors
given by Icarus Verilog or the test harness. The failures are
// Implement the Verilog module based on the
// following description . Assume that sigals classified in Table I.
// are positive clock/clk triggered unless Classifications were developed by human inspection of
// otherwise stated . commonfailuremodesacrossthecodecompletionbenchmark.
//
For example, LLMs were observed frequently mixing up the
// The module should implement an incrementer
use of registers and wires. Solutions in prompt tuning could
// which increments the input by one and
// writes the result to the output. Assume vary: from adding prompt rules to only use wires on ports
// all values are encoded as two’s complement to suggesting the use of SystemVerilog logic port types,obviating the immediate type confusion, to allowing the LLM • Meta Llama 3.1 70B [11]
to generate the interface entirely on its own (as in the case of • Meta Llama 3 70B [11]
specification-to-RTLgeneration,ratherthancodecompletion). • Meta CodeLlama 70B [19]
Byclassifyingfailures,theimpactofpromptchangesoncode • Google CodeGemma 7B [20]
generation performance can be directly observed and guided. • DeepSeek Coder 33B and 6.7B [21]
• Meta Llama 3.1 8B [11]
TABLEI • RTL-Coder DeepSeek v1.1 6.7B [12].
TYPESOFFAILURESSUPPORTEDBYAUTOMATICFAILURE The models are comprised of a range of closed and open
CLASSIFICATION.
source, parameter sizes, and general-purpose to specialized.
FailureType Example Model results were captured as both a 20-sample (n=20) high
Compile-TimeFailures temperature(T=0.85,top p=0.95)setand1-sample(n=1)low
UnabletoBind Clk is missing in interface ports list, such as if a
temperature (T=0.0, top p=0.01) set. The 20-sample set is
Wire/Reg’clk’ code completion task does not specify a clock to
beusedyettheLLMuseditinthegeneratedcode. similar to the model parameters from VerilogEval [5] which
UnabletoBind Otherportrelatedbindproblems. had a 20-sample set with temperature T=0.80.
Wire/Reg The graph in Figure 1 illustrates the performance of vari-
ExplicitCast A datatype problem occurred, often with use of
ous large-language models (LLMs) on code completion and
Required enums.
ModuleMissing Typically indicates the modular declaration is specification-to-RTL translation tasks, as measured by the
missingfromthegeneratedcode. benchmark pass rate (pass@1 in [5]). Models are arranged
Sensitivity Sensitivitylistsforalwaysblocksarenotdefined
along the x-axis by model size, with undisclosed model sizes
Problem properly.
RegDeclaredas Awireisassignedtoasareg. ontheright.Theevaluationcomparesmodelswithandwithout
Wire 1-shot in-context learning (ICL) examples, represented by ar-
SyntaxError Generalsyntaxerrorsingeneratedcode.
rowsindicatingthechangeinperformanceas1-shotexamples
GeneralCompiler Other compiler errors without specific classifica-
Error tion. are added. For code completion tasks, GPT-4 Turbo achieves
Run-TimeFailures the the highest pass rate at approximately 48%, surpassing
ResetIssue Resetshouldbesynchronousbutisasynchronous. the previously established state-of-the-art frontier of 43% for
Timeout The simulation did not complete in reasonable
0-shot by GPT-4 [5]. Further adding an ICL example in the
time, indicating a sequential block does not have
acorrectimplementation. 1-shot result leads to to the highest performance yet at 58%.
GeneralRuntime Otherruntimeerrorsthatarenotclassified,includ- ThishighlightsGPT-4Turbo’srobustimprovementoverGPT-
Error ingmismatchedoutputs. 4 for RTL generation tasks despite being a general-purpose
model.
D. Other Infrastructural Improvements Llama 3.1 405B demonstrates that open models have
matched closed models by scoring 57% in the 0-shot code
TheoriginalVerilogEvalbenchmarkcontainedallproblems
completion task, exceeding both GPT-4 and GPT-4 Turbo,
in a monolithic jsonl format. This was efficient to run,
while Llama 3.1 70B nearly matching GPT-4 despite being
but inefficient to inspect manually using a text editor. In the
much smaller in size. While Llama 3.1 generally improves
improved benchmark each problem was split into its own set
with in-context learning examples, Llama 3 70B declines in
of files, with problem prompts, module interfaces, and test
passratewhenthe1-shotICLexampleisaddedtotheprompt,
benches. Autoconf [15] and GNU Make [16] were employed
which will be discussed in detail in the next section. Among
totargetamodelevaluationbuilddirectorytoaspecificevalu-
thesmallerspecializedmodels,RTL-Codershowedsignificant
ation target, including the LLM itself to run, number of shots,
improvements with 1-shot ICL examples, reaching pass rates
number of samples, task to complete, and other parameters.
of around 35%, while being much smaller than general-
For each problem, a resulting problem evaluation directory
purpose models. RTL-Coder when originally sampled did not
is created containing a log of the LLM prompt/responses,
properly insert whitespace after endmodule statements and
generated Verilog file, and the Icarus Verilog output log. This
wouldoftenrepeatcodeblocks.Wemodifiedourpost-process
infrastructure allows for scalable sweeps through the use of
script that extract the Verilog code form the response to
Make’s parallel run feature, helps continue an evaluation run
match the post-processing in RTL-Coder’s evaluation scripts
if it is interrupted, and allows for easy human inspection of
[22], and Figure 1’s RTL-Coder results are shown using their
the resulting collateral.
corrected extraction.
Specification-to-RTL task results showed generally similar
III. BENCHMARKEVALUATION
pass rate performance compared to code completion. GPT-4
We evaluate eight publicly available large-language models Turbo showed noticeable pass rate improvement in spec-to-
on the proposed benchmark: RTL 0-shot tasks, but similar pass rates for 1-shot. Mistral
• OpenAI GPT-4 Turbo [10] (gpt-4-1106-preview) Large showed the opposite trend, with measurable improve-
• OpenAI GPT-4 [17] (gpt-4-0613) ment in 1-shot results and Llama 3 and Llama 3.1 70B saw
• Mistral AI Mistral Large [18] improvementinboth,asdidLlama3.18B.InLlama3.1405B
• Meta Llama 3.1 405B [11] across both tasks, adding an ICL example made little differ-Code Completion Pass Rate (n=20, temperature=0.85, top_p=0.95)
Arrows indicate change from 0-shot to 1-shot ICL examples
70%
GPT-4 Turbo has pushed the
RTL-Coder 6.7B has nearly the Llama3.1 405B frontier. GPT-4 was the prior
60% same pass rate as DeepSeek best result from VerilogEval.
33B, despite being much The open Llama3.1 405B
smaller. performs simiarily to GPT-4 GPT-4 Turbo
Turbo.
50%
40% DeepSeek Coder 33B
GPT-4
RTL-Coder (corr.)
Llama3.1 70B
30%
Mistral Large
20% DeepSeek Coder 6.7B
CodeGemma 7B CodeLlama 70B
10% Llama3.1 8B is worse with
Llama3.1 8B 1-shot, while 70B and
405B see improvements.
0%
5 50 500 Undisclosed Size 5,000
Model Parameters (Billions)
Lower resource cost Higher resource cost
(a) CodeCompletionTask
Specification-to-RTL Pass Rate (n=20, temperature=0.85, top_p=0.95)
Arrows indicate change from 0-shot to 1-shot ICL examples
70%
GPT-4 Turbo slightly degrades GPT-4 Turbo
60% with 1-shot.
50% DeepSeek Coder 33B Llama3.1 405B
GPT-4
Llama3.1 70B
RTL-Coder performs
40% well with 1-shot.
RTL-Coder (corr.)
30% DeepSeek Mistral Large
Coder 6.7B
20% Llama3.1 8B
CodeLlama 70B
CodeGemma 7B
10%
RTL-Coder fails at spec-to-RTL with 0-shot when T=0.85.
0%
5 50 500 Undisclosed Size 5,000
Model Parameters (Billions)
Lower resource cost Higher resource cost
(b) Specification-to-RTLTask
Fig.1. Passrateacrossrecentlarge-languagemodels.Greenmodelsareclosedgeneral-purposemodels,orangeareopengeneral-purposemodels,darkblue
arecoding-specificmodels,andlightblueisanRTL-specificmodel.
ence in pass rate. Interestingly, RTL-Coder initially fails at anear-zero(1.6%)passratein0-shotspecification-to-RTL,but
the specification-to-RTL task with 0-shot but recovers with 1- does have a respectable pass rate (37%) when temperature=0
shot examples. This variability underscores the importance of (n=1). Inspection of the RTL-Coder responses with high
tailored prompt tuning and the potential of ICL to enhance temperature show that it tries to do code completion instead
code generation performance in certain models. of specification-to-RTL in 0-shot and often omits the modular
ThefullresultsareshowninTableIIandincludebothn=20 declaration. Adding an ICL example in 1-shot corrects this
(20 samples, temperature=0.85, top p=0.95) from Figure 1 behavior.
along with deterministic n=1 (1 sample, temperature=0.0, Overall, larger models generally achieve higher pass rates,
top p=0.01). RTL-Coder results are shown for both the cor- though resource costs and model-specific responses to ICL
rected and original extraction methods, with the original examplesvarysignificantly.WithinthecontextofVerilogEval,
methodalsoappliedtotheothermodels.Asmentionedabove, GPT-4 Turbo and Llama 3.1 405B have become clear leaders
RTL-Coderathightemperatures(temperature=0.85,n=20)has for the highest achieved pass rates, demonstrating that open
ycarucca
rehgiH
)1@ssap(
etaR
ssaP
kramhcneB
ycarucca
rewoL
ycarucca
rehgiH
)1@ssap(
etaR
ssaP
kramhcneB
ycarucca
rewoLTABLEII
VERILOGEVALPASSRATESOFRECENTLARGE-LANGUAGEMODELS.TEMPERATUREIS0.85WHENn=20AND0WHENn=1.
ModelName ModelSize License Type Task:CodeCompletion Task:Specification-to-RTL
In-ContextLearningExamples: 0-Shot 1-Shot 0-Shot 1-Shot
NumberofSamples: n=1 n=20 n=1 n=20 n=1 n=20 n=1 n=20
GPT-4Turbo[10] Undisclosed Closed General 52.6% 48.4% 59.6% 58.2% 60.3% 59.1% 58.3% 56.2%
GPT-4[17] Undisclosed Closed General 42.7% 39.6% 48.1% 48.0% 44.2% 37.4% 50.6% 49.1%
MistralLarge[18] Undisclosed Closed General 34.0% 31.3% 41.0% 41.8% 35.3% 33.8% 48.7% 45.7%
Llama3.1[11] 405B Open General 56.5% 59.6% 58.9% 57.7% 55.8% 56.1% 57.7% 58.3%
Llama3.1[11] 70B Open General 41.0% 33.5% 44.2% 35.7% 37.8% 37.9% 48.7% 50.7%
Llama3[11] 70B Open General 36.5% 36.7% 24.4% 28.8% 41.7% 42.8% 37.2% 38.6%
CodeLlama[19] 70B Open Coding 39.1% 25.7% 38.5% 23.9% 34.0% 22.0% 41.0% 23.9%
DeepSeekCoder[21] 33B Open Coding 19.9% 24.2% 32.7% 32.4% 17.3% 18.7% 37.2% 38.3%
Llama3.1[11] 8B Open General 10.9% 9.7% 15.4% 15.8% 20.5% 23.4% 26.9% 23.1%
CodeGemma[20] 7B Open Coding 7.1% 8.3% 21.2% 14.7% 9.0% 8.9% 23.1% 21.0%
DeepSeekCoder[21] 6.7B Open Coding 26.3% 19.2% 20.5% 20.3% 28.2% 21.4% 26.9% 25.5%
RTL-Coder(corr.)[12] 6.7B Open VerilogRTL 35.3% 29.3% 37.2% 31.5% 37.2% 1.6% 35.3% 33.0%
RTL-Coder(orig.)[12] 6.7B Open VerilogRTL 11.5% 12.8% 21.8% 19.0% 36.5% 1.1% 35.3% 32.6%
models (Llama 3.1 405B) have reached parity with closed Code Completion and Spec-to-RTL versus # of In-Context Learning Examples
models.Additionally,smaller(70B)openmodelshavebecome (n=20, temperature=0.85, top_p=0.95)
70%
competitive last year’s larger closed models. Domain-specific GPT-4 Turbo GPT-4 Turbo sees ICLs helping code completion but Llama3.1 70B
Spec-to-RTL hindering spec-to-RTL. Spec-to-RTL
models (RTL-Coder) are also competitive in some scenarios 60%
at a much smaller size.
50%
Llama3 70B
40% CG oP dT e-4 C T ou mrb po . L Cla om dea 3 C. o1 m 70 pB . Spec-to-RTL
IV. IMPACTOFICLONPASSRATESANDFAILURES
30% A. Higher-Shot ICL Results
RTL-Coder (corr.)
20% Code Comp. Llama3 70B degrades as ICL examples are added to
Asdemonstratedfromtheprevioussection,in-contextlearn- code completion and 1-shot spec-to-RTL. In contrast, Llama3 70B
Llama3.1 70B sees improvements with ICLs in both Code Comp.
ing examples improve model generation accuracy in some 10% cases, including sigificantly in spec-to-RTL.
RTL-Coder (corr.) RTL-Coder stabilizes after 1-shot, with spec-to-RTL
conditions but degrade accuracy in others. ICL impact bears Spec-to-RTL performing better than code completion.
0%
further investigation. Higher-shot ICL runs were conducted 0-Shot 1-Shot 2-Shot 3-Shot
for four models across parameter size classes: GPT-4 Turbo, Smaller context size Number of In-Context Learning Examples Larger context size
Llama 3 70B, Llama 3.1 70B, and RTL-Coder. The second
Fig. 2. Pass rate of three models for code completion and specification-to-
ICL example was similar to the first example but requested RTLtaskswith0-shotto3-shotin-contextlearningexamples.Solidlinesare
a sequential (flopped) incrementer instead of a combinational codecompletionanddashedlinesarespec-to-RTL.
incrementer. The third example involved designing a finite-
state machine.
B. Failure Analysis
Pass rates across three models for the two tasks across
0-shot to 3-shots is shown in Figure 2. Notably, GPT-4 Figure 3 employs the new failure classification feature of
Turbo exhibits stable and high performance across all ICL theimprovedbenchmarkinfrastructuretoillustratethenumber
example counts of at least 1-shot, maintaining a pass rate and types of failures encountered by different models across
of 55% to 60%. In contrast, Llama 3 70B demonstrates various numbers of in-context learning (ICL) examples. The
divergent trends; its spec-to-RTL performance improves from y-axis represents the number of failures, with lower values
40% to nearly 50% with more ICL examples, whereas its indicating better pass rates. Each bar is segmented to show
codecompletionperformancedeclinesfrom35%tojustabove different categories of errors, with orange shades representing
20%. Llama 3.1 70B is similar to Llama 3 for spec-to- compiler errors and blue shades representing runtime errors.
RTL,anddoesn’tdemonstratedegradationincodecompletion. The figure is divided into three sections for the three models
RTL-Coder shows significant variability, with its spec-to-RTL from Figure 2, highlighting the numbers and types of failures
performanceimprovingdramaticallyfromaround5%at0-shot across 0-shot to 3-shot ICL examples. As compiler errors will
toalmost35%at3-shot.Asmentionedintheprevioussection, be flagged and mask runtime errors, the bars on the graph are
RTL-Coder drops to a very lower pass rate at high temper- best read from bottom to top. A reduction in runtime errors
ature with 0-shot spec-to-RTL because it omits the module for the same total bar height indicates that compiler errors
declaration, but recovers to a nominal pass rate once ICL have displaced runtime errors. This layering effect should be
examples are added. This graph highlights the varying impact considered when interpreting the improvements or degrada-
of ICL examples on different models and tasks, emphasizing tions in model performance as additional ICL examples are
the potential benefits of task-specific tuning and the necessity introduced.
of providing contextual examples to enhance model outputs. For RTL-Coder, the model shows notable improvement
ycarucca
rehgiH
)1@ssap(
etaR
ssaP
kramhcneB
ycarucca
rewoLFailure Classification Across Models and In-Context Learning Examples
RTL-Coder (Corr.) 6.7b Llama3 70b GPT-4 Turbo
3500
RTL-Coder improves with Llama3 code
ICLs until 2-shot. 3-shot completion degrades
has roughly the same RTL-Coder tries to do with ICLs added Llama3 spec-to-rtl
3000 code completion for adding ICLs
pass rate as 2-shot. because "endmodule"
spec-to-RTL when elimates reg
temperature is high. It is frequently missing. declared as wire
2500 does not include the errors, but adds GPT-4 Turbo code
module declaration. completion
other compiler
improves with ICLs GPT-4 Turbo spec-
errors. added, fewer to-RTL degrades
2000 compiler errors. with more compiler
errors as ICLs are
added.
1500
1000
500
0
0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot
Code Completion Specification-to-RTL Code Completion Specification-to-RTL Code Completion Specification-to-RTL
C (General Compiler Error) S (Syntax Error) c (Unable to Bind Wire/Reg `clk`) e (Explicit Cast Required)
m (Module Missing) n (Sensitivity Problem) p (Unable to Bind Wire/Reg) w (Reg Declared as Wire)
r (Reset Issue) R (General Runtime Error) T (Timeout) Runtime Errors Compiler Errors
Fig.3. FailureclassificationforRTL-Coder,Llama370B,andGPT-4Turbomodelswith0-shotto3-shotICLexamplesacrossthetwotasks.Orangecoloring
indicatescompilererrorswhileblueindicatesruntimeissues.
in both tasks up to 2-shot ICL examples, after which the by-problem basis within a run. This granular analysis helps
performance stabilizes. The primary source of failure in 0- identify whether specific problems or categories of problems
shot examples are compile-time errors, but with the addi- havesystematictypesoffailures.Suchinsightscanguidemore
tion of ICL examples, these errors decrease significantly. As careful tuning of prompts across the benchmark, leading to
mentioned previously, in the specification-to-RTL task, the more effective and targeted improvements in model perfor-
modelattemptscodecompletionwhenthetemperatureishigh, mance. A careful analysis of the problem categories within
leading to a high number of “module missing” errors, which VerilogEval and comparative failure counts could help find
are reduced with the introduction of ICL examples. the best ICL examples to use for a given model.
Llama3exhibitsadifferentpattern,wherecodecompletion
performance degrades with the addition of ICL examples V. CONCLUSIONS
due to frequent endmodule errors. In contrast, for the
The enhanced VerilogEval benchmark provides a more
specification-to-RTL task, adding ICL examples mitigates er-
robust framework for evaluating the performance of large-
rors related to wires being declared as registers but introduces
language models (LLMs) on digital hardware code generation
other compiler errors.
tasks.OurfindingsdemonstratethatbothLlama3.1405Band
GPT-4TurboshowsamixedresponsetoICLexamples.For GPT-4Turbopushthefrontierofperformancewitha60%and
code completion, the model benefits from ICL examples, as 48% 0-shot pass rate on code completion tasks, respectively,
indicated by a reduction in compiler errors across the board. surpassing the previously established 43% pass rate by GPT-4
However, in the specification-to-RTL task, the performance (non-Turbo)[5].Open-sourcegeneral-purposemodels,namely
slightly degrades with more ICL examples, resulting in an Llama 3 70B, and domain-specific models, RTL-Coder, show
increase in compiler errors. favorable pass rates compared to last year’s closed models,
The results emphasize the need for careful tuning of ICL 37% and 32%, respectively. The addition of specification-
examples to optimize results. While ICL can help correct to-RTL task support in the improved VerilogEval benchmark
certain types of mistakes, it can also introduce new issues reveals even better model capabilities. GPT-4 Turbo achieves
leading to similar or even worse performance. In addition to an impressive 59% pass rate in specification-to-RTL tasks,
the failure classification feature capturing high-level counts exceeding Llama 3.1 405B at 56%, while Llama 3 70B and
of types of failures across different models and prompting RTL-Coder6.7Balsodemonstratestrongcompetitivenesswith
settings, it also allows for detailed inspection on a problem- pass rates of 42% and 37%, respectively. Adding in-context
seruliaF
fo
rebmuN
retteb
si
rewoLlearning examples led to notable improvements (GPT-4 Turbo [9] T.B.Brown,B.Mann,N.Ryder,M.Subbiah,J.Kaplan,P.Dhariwal,
nearly achieving the same pass rate for code completion A.Neelakantan,P.Shyam,G.Sastry,A.Askell,S.Agarwal,A.Herbert-
Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler,
as spec-to-RTL), although the impact varies widely across
J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,
different models and tasks. This variability underscores the B.Chess,J.Clark,C.Berner,S.McCandlish,A.Radford,I.Sutskever,
importance of task-specific tuning to optimize performance. andD.Amodei,“Languagemodelsarefew-shotlearners,”2020.
[10] New models and developer products announced
The improved benchmark infrastructure, including the new
at DevDay. [Online]. Available: https://openai.com/index/
failure classification feature, provides deeper insights into new-models-and-developer-products-announced-at-devday/
the types of errors encountered by different models. For [11] meta-llama/llama-models. Original-date: 2024-06-27T22:14:09Z. [On-
line].Available:https://github.com/meta-llama/llama-models
example, Llama 3 70B frequently encounters endmodule
[12] S. Liu, W. Fang, Y. Lu, Q. Zhang, H. Zhang, and Z. Xie, “Rtlcoder:
missing errors during code completion, which careful prompt Outperforming gpt-3.5 in design rtl generation with our open-source
tuning or model alignment may be able to fix. The ability to datasetandlightweightsolution,”2024.
[13] Z. Yuan, J. Liu, Q. Zi, M. Liu, X. Peng, and Y. Lou, “Evaluating
classify and inspect failures on a problem-by-problem basis is
instruction-tuned large language models on code comprehension and
criticalforunderstandingandmitigatingpoorcodegeneration, generation,”2023.
leading to more effective and targeted improvements in LLM [14] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,
E. Jiang, C. Cai, M. Terry, Q. Le, and C. Sutton, “Program synthesis
performance for digital hardware code generation.
withlargelanguagemodels,”2021.
In the future, the research community would benefit from [15] Autoconf-GNUproject-freesoftwarefoundation.[Online].Available:
https://www.gnu.org/software/autoconf/
digitalhardwarebenchmarksfurtherexpandedtoincludemore
[16] Make - GNU project - free software foundation. [Online]. Available:
tasksbeyondRTLcodegenerationrepresentativeofthedigital https://www.gnu.org/software/make/
hardware design flow. Some examples include verification- [17] OpenAI,“Gpt-4technicalreport,”2023.
[18] M. AI. Au large. Section: news. [Online]. Available: https://mistral.ai/
related tasks [23]–[25], testbench stimulus generation [26],
news/mistral-large/
alongwithmanymore[27].TheenhancedVerilogEvalbench- [19] meta-llama/CodeLlama-70b-instruct-hf · hugging face. [Online]. Avail-
mark in this work is meant to be a step towards facilitating able:https://huggingface.co/meta-llama/CodeLlama-70b-Instruct-hf
[20] google/codegemma-7b · hugging face. [Online]. Available: https:
additional task support on top of a common set of design
//huggingface.co/google/codegemma-7b
problems that allows for a more comprehensive assessment of [21] D.Guo,Q.Zhu,D.Yang,Z.Xie,K.Dong,W.Zhang,G.Chen,X.Bi,
model performance for hardware design. Y. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang, “Deepseek-coder:
Whenthelargelanguagemodelmeetsprogramming–theriseofcode
intelligence,”2024.
[22] hkust zhiyao. hkust-zhiyao/RTL-coder. Original-date: 2023-11-
ACKNOWLEDGMENT 20T13:01:13Z. [Online]. Available: https://github.com/hkust-zhiyao/
RTL-Coder
[23] Y.-D.Tsai,M.Liu,andH.Ren,“Rtlfixer:Automaticallyfixingrtlsyntax
This paper would not have been possible without the errors with large language models,” Computing Research Repository
generous help of NVIDIA Applied Deep Learning Research (CoRR),vol.arXivv:2311.16543,Nov2023.
[24] M. Orenes-Vera, A. Manocha, D. Wentzlaff, and M. Martonosi, “Au-
(ADLR), especially Teodor-Dumitru Ene, and the NVIDIA
tosva: Democratizing formal verification of rtl module interactions,”
Inference Microservices (NIM) teams. DesignAutomationConf.(DAC),Dec2021.
[25] S. Thakur, J. Blocklove, H. Pearce, B. Tan, S. Garg, and R. Karri,
“Autochip:Automatinghdlgenerationusingllmfeedback,”Computing
REFERENCES ResearchRepository(CoRR),vol.arXiv:2311.04887,Nov2023.
[26] Z.Zhang,G.Chadwick,H.McNally,Y.Zhao,andR.Mullins,“Llm4dv:
Using large language models for hardware test stimuli generation,”
[1] GitHub copilot · your AI pair programmer. [Online]. Available: Computing Research Repository (CoRR), vol. arXiv:2310.04535, Oct
https://github.com/features/copilot/ 2023.
[2] J. Blocklove, S. Garg, R. Karri, and H. Pearce, “Chip-chat: [27] Y. Fu, Y. Zhang, Z. Yu, S. Li, Z. Ye, C. Li, C. Wan, and Y. C. Lin,
Challenges and opportunities in conversational hardware design,” “Gpt4aichip:Towardsnext-generationaiacceleratordesignautomation
in 2023 ACM/IEEE 5th Workshop on Machine Learning for via large language models,” Int’l Conf. on Computer-Aided Design
CAD (MLCAD). IEEE, Sep 2023. [Online]. Available: http: (ICCAD),Nov2023.
//dx.doi.org/10.1109/MLCAD58807.2023.10299874
[3] K. Chang, Y. Wang, H. Ren, M. Wang, S. Liang, Y. Han, H. Li, and
X. Li, “Chipgpt: How far are we from natural language hardware de-
sign,”ComputingResearchRepository(CoRR),vol.arXiv:2305.14019,
May2023.
[4] Y.Lu,S.Liu,Q.Zhang,andZ.Xie,“Rtllm:Anopen-sourcebenchmark
fordesignrtlgenerationwithlargelanguagemodel,”2023.
[5] M.Liu,N.Pinckney,B.Khailany,andH.Ren,“VerilogEval:Evaluating
largelanguagemodelsforVerilogcodegeneration,”2023.
[6] S. Thakur, B. Ahmad, Z. Fan, H. Pearce, B. Tan, R. Karri, B. Dolan-
Gavitt, and S. Garg, “Benchmarking large language models for auto-
mated verilog rtl code generation,” Design, Automation, and Test in
Europe(DATE),Apr2023.
[7] S.Thakur,B.Ahmad,H.Pearce,B.Tan,B.Dolan-Gavitt,R.Karri,and
S.Garg,“Verigen:Alargelanguagemodelforverilogcodegeneration,”
ACM Trans. on Design Automation of Electronic Systems (TODAES),
vol.29,no.3,pp.1–31,Apr2024.
[8] A. Allam and M. Shalan, “Rtl-repo: A benchmark for evaluating llms
onlarge-scalertldesignprojects,”2024.