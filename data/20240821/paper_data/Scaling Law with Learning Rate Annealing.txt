SCALING LAW WITH LEARNING RATE ANNEALING
HoweTissue VenusWang
h-sun20@tsinghua.org.cn wangxxing12@gmail.com
LuWang
wangluloveslezhi@gmail.com
ABSTRACT
Wefindthatthecross-entropylosscurvesofneurallanguagemodelsempirically
adheretoascalinglawwithlearningrate(LR)annealingovertrainingsteps(s):
L(s)=L +A·S−α−C·S
0 1 2
WhereS isforwardareaandS islearningrateannealingarea. Thisformulation
1 2
takesintoaccounttwofactors: (1)Theforwardscalingdefinedastypicalscaling
law, and (2) the additional loss drop brought by LR annealing. Therefore, this
formulationcandescribethefulllosscurveateachstep,ratherthanthesingleloss
pointattheendoftraining. ApplyingthescalinglawwithLRannealingandfit-
tingonlyoneortwotrainingcurves,wecanaccuratelypredictthelossoflanguage
modeltrainingatanygivenstepandacrossanylearningratescheduler(LRS).Fur-
thermore,thisequationaccuratelydescribesthedynamicsduringtrainingprocess,
andprovidesatheoreticalverificationandexplanationfornumerousexperimental
findings of previous studies, particularly those focusing on LR schedule and LR
annealing. The resulting insights, also serve as a guide for researchers to select
criticalLRSinadvancebypredictionusingourequation.Mostsignificantly,since
allthepointsinafulltrainingcurvefollowtheequation,wecanachieveaccurate
losspredictionatanygivenstepacrossanylearningratescheduler,whileexpend-
inglessthan1%ofthecomputationalcostrequiredbythechinchillascalinglaw
tofitlanguagemodelingloss. Thisapproachextremelydemocratizesscalinglaw
fittingandpredictingindevelopinglargelanguagemodels.
1 INTRODUCTION
Inrecentyears,thespotlightofacademicandindustrialinteresthasbeenincreasinglycapturedby
largelanguagemodels. Theintriguingnotionofthescalinglawpositsthatthecross-entropylossof
languagemodelsadherestoapower-lawpatternasscalingupofmodelsizeanddatasize(Kaplan
et al., 2020; Hoffmann et al., 2022). This law serves as a powerful tool, affording researchers
the ability to precisely forecast the performance of large language models by fitting the loss at a
smallerscale. Thepotentialimplicationsofscalinglawsarevastandcompelling,sparkingasurge
ofexplorationinthisburgeoningfield(Bahrietal.,2021;Michaudetal.,2023;DeepSeek-AI,2024).
However, typicalscalinglawsdescribetheperformanceofthemodelattheendoftraining, rather
than every step during the training process. Essentially, the middle points with different degrees
of LR annealing fail to follow typical scaling laws, which do not consider local loss drop brought
by LR annealing. Consequently, these laws are unable to fit or predict a full loss curve. Till this
work,wedonothaveanappropriateformulationthataccuratelydescribesthedynamicsduringthe
trainingprocess,whichiscrucialtodeeplyunderstandandimprovethetrainingprocess.
Thelearningrateschedule(LRS)isasignificantvariablethatgreatlyinfluencesthelossduringthe
entire training process. Several previous studies have concluded that the rate of decrease in loss
acceleratesasLRannealingtakesplace(Loshchilov&Hutter,2016;Ibrahimetal.,2024;Huetal.,
2024;DeepSeek-AI,2024). However,thesestudiesgenerallyonlyprovideaqualitativedescription
ofhowlosschangesduringLRannealing.Instead,inthepresentstudy,webuilduponthechinchilla
scaling law (Hoffmann et al., 2022) by introducing an additional LR annealing term. This term
1
4202
guA
02
]LC.sc[
1v92011.8042:viXra21 1.. .09 908 508
1
11 198
7
1.891
16
1.809
15
1.707
14
1.588
13
1.454
1.309 S 2 (Annealing Area) 12
11
1.156
10
1.000
9
0.844
8
0.691
0.546 S 1 (Forward Area) 7
6
0.412
5
0.293
4
0.191
3
0.109 2
0.049 1
00..001020
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Step
Figure 1: The definition of S and S , taking a 20-step cosine learning rate schedule as a toy ex-
1 2
ample. S (forwardarea)istheareaenclosedbythelearningratecurveandtheX-axisstep,which
1
canbeapproximatelyregardedasthetotalamountofmovementofneuralnetworkparameters;S
2
isthedecayedannealingareaofthelearningratecurve(thelighterthecolor,thegreaterthedegree
ofdecay),thatis,theweightedsumoftheareasofthesmallbluesquares. BoththeincreaseofS
1
andS canhelptoreducetheloss,buttheyexistinadelicatebalanceortrade-offrelationship. To
2
minimizethefinalloss,it’snecessarytodiscoveroptimalbalancebetweenS andS .
1 2
allowsustoaccuratelypredicthowlossdecreasesduringLRannealing. Overall, wediscoverthat
thelossvalueatanystepofafixed-sizemodelisdeterminedbytwofactors: forwardareaandthe
degreeofLRannealing. Formally,theexpectationoflossLatstep1sofalanguagemodelfollows:
L(s)=L +A·S−α−C·S
0 1 2
s
(cid:88)
S = η
1 i
(1)
i=1
s i
(cid:88)(cid:88)
S = (η −η )·λi−k
2 k−1 k
i=1k=1
WhereS isforwardarea2andS islearningrateannealingarea. η isthelearningrateatstepi. λ
1 2 i
isahyper-parametertonotatethedecayfactorinLRannealingmomentum(introducedinSec.3in
detail),whichtypicallyrangesfrom0.99to0.999. L ,A,C,αareundeterminedpositiveconstants.
0
AvisualizationtopresentthedefinitionsofS andS isshownasFig.1.
1 2
InEq.1,thetermL +A·S−αisarectifiedscalinglaw,whichstatesthattheexpectedlossdecreases
0 1
asapower-lawfunctionofthenumberoftrainingsteps. Theterm−C ·S isthenewlyintroduced
2
learningrateannealingterm. Thistermrepresentstheadditionaldecreaseinlossthatoccursasthe
learning rate is reduced during training. This universal equation can fit the validation loss of any
stepacrossanyLRS,andthenhelppredictthevalidationlossofanystepacrossanyLRS.Assome
examples,wefittheequationonthelosscurveofconstantandcosineLRSin20Ktotalsteps(shown
asFig.2),thenweareabletoemploytheuniversalscalinglawwithLRannealingtopredictthefull
losscurveacrossvariousLRSinlongertotalsteps(e.g. 60K)(shownasFig.3).
InSec.3,wedemonstratethederivationofthescalinglawwithLRannealingandelucidatethepo-
tentialtheoryunderpinningourequation. Wealsoextendourequationtoaformulationwithmodel
sizeN. InSec.4,weapplyourequationtoverifyandexplaintheconclusionsdrawnfromamul-
titudeofpreviousstudies. Theseconclusions,whichpreviouslyrequiredsubstantialcomputational
1Inthispaper, wetaketrainingstepsasdataamountinthescalinglawequation, whichhasnoessential
differencebecausedataamount=trainingsteps×batchsizeandbatchsizeisfixed.
2S isa.k.a.summedlearningrate,mentionedinOpenAI’sscalinglawpaper(Kaplanetal.,2020).
1
2
4
01×
etaR
gninraeL4.0 2.90
2.00 3.9 c co os si in ne e L FiR ttS in, gG r Co uu rn vd e Truth Loss 2.89
1.75 33 .. 78 c co on ns st ta an nt
t
L FiR ttS in, gG r Co uu rn vd
e
Truth Loss 2.88
1.50 3.6 2.87
1.25 3.5 2.86 1.00 33 .. 34 F Ri 2tt =in 0g
.
9C 9u 9rv 6e 4: L = 2.628 + 0.429*S1^(-0.550) - 0.412*S2 22 .. 88 45
0.75 3.2
00 .. 25 50
C Co on sis nt ea n Lt
e
L ae rna ir nn gin Rg
a
R tea t Ce
u
C rvu erve
233 ... 901 222 ... 888 123 c c
c
co o
o
os s
n
ni i
s
sn n
t
te e
a
a
n
nL F
t
tiR t
L
FtS i iRn,
t
tSgG
in,
r C gGo u u
r
Cr on v uud e
rn
vT
d
er u Tt rh
u
tL ho Lss
oss
0.00 0 2500 5000 7500 10000 12500 15000 17500 20000 2.8 0 2500 5000 7500 10000 12500 15000 17500 20000 2.8 10 0000 12000 14000 16000 18000 20000
Step Step Step
(a)Learningrate. (b)Zoomed-outlossfitting. (c)Zoomed-inlossfitting.
Figure2: Fulllosscurvefittingoncosine(20kstepstoη = 0)andconstantLRS.Thefigures
min
omitthewarmupinthefirst500steps. Afterfitting,wegetauniversallossequationL=2.628+
0.429·S−0.550−0.412·S .
1 2
resources,cannowbeconsolidatedinacost-freemannerusingourequation. Thisnotonlystream-
linestheprocessbutalsobringsusclosertounderstandingthecruxofeverythingaboutlossdrop,
LRschedule, andLRannealing(i.e., theartofbalancebetweenforwardareaandannealingarea).
Moreover, these key insights serve as a guide for researchers to select critical LRS in advance by
prediction using our equation. In Sec. 5, we compare our equation with typical chinchilla scaling
law (Hoffmann et al., 2022). We show that chinchilla scaling law actually describes special cases
(the endpoint of one full loss curve) of our equation, which means that our equation is a general-
izedformofchinchillascalinglaw. Furthermore,wecomparethecomputationalcostsrequiredby
thechinchillascalinglawandourequation,revealingthatourequationofferssignificantsavingsin
computationalcosts. Thisgreatlydemocratizesthefittingandpredictionofscalinglaws.
2 PRELIMINARY
2.1 SCALINGLAWS
Cross-entropylossoflanguagemodelsisagoodindicatorfortheperformanceofalmostalldown-
stream tasks (Caballero et al., 2022; Du et al., 2024). Kaplan et al. (2020) empirically discovers
a power-law relationship between validation loss and three key factors: model size, dataset size,
and the computational resources utilized for training. As an application of scaling law, Hoffmann
etal.(2022)trainsacompute-optimallargelanguagemodelnamedchinchillabybalancingmodel
size and dataset size. Moreover, chinchilla scaling law adopts a simpler and intuitive equation to
describetherelationshipbetweenthefinalvalidationloss(L)andthenumberofparameters(N)as
wellastheamountofdata(D)asfollows:
L(D,N)=L +A·D−α+B·N−β (2)
0
whereL ,A,B,α,β areallundeterminedpositiveconstants. Notethatthetypicalscalinglawsfit
0
thelossattheendpointoftraininginsteadoffulllosscurve, thusaexperimentwithnewdatasize
requires another training launch. Previous works have conducted some preliminary studies on the
impact of the learning rate on the scaling laws. For example, OpenAI and chinchilla scaling laws
bothfindthatthechoiceoflearningratescheduledoesnotinfluencethepower-lawformat(Kaplan
et al., 2020; Hoffmann et al., 2022). Also, OpenAI’s experiments suggest that the specific choice
of learning rate schedule has minimal impact on the final validation loss, provided that the total
summedlearningrateisadequatelylargeandthescheduleincorporatesbothawarmupstageanda
finalannealingstage,reducingthelearningratetonearlyzero(Kaplanetal.,2020).
2.2 WSDLEARNINGRATESCHEDULER
Huetal.(2024)proposesawarmup-stable-decay(WSD)LRSincludingthreelearningratestages,
whichcouldhelpgetalowervalidationlosscomparedtothetypicalcosineLRS.Theformatislike

s η , s≤T
 Twarmup max warmup
WSD(s)= η , T <s≤T (3)
max warmup stable

f(s−T )η , T <s≤T
stable max stable total
3
401×
etaR gninraeL ssoL ssoL2.00 33 .. 89 G Prr eo du in cd ti oT nru Cth u rL vo ess 22 .. 77 89 G Prr eo du in cd ti oT nru Cth u rL vo ess
1.75 3.7 2.77
1.50 33 .. 56 22 .. 77 56
1.25 3.4 2.74 1.00 33 .. 23 222 ... 777 123
0.75 33 .. 01 22 .. 67 90 Mean Prediction Error = 0.146%
0.50 2.9 2.68
0.25 2.8 2.67
Learning Rate Curve 2.7 2.66
0.00 0 10000 20000 30000 40000 50000 60000 2.6 0 10000 20000 30000 40000 50000 60000 2.6 35 5000 40000 45000 50000 55000 60000
Step Step Step
(a)FullcurvepredictionofconstantLRS.
2.00 Learning Rate Curve 33 .. 89 G Prr eo du in cd ti oT nru Cth u rL vo ess 22 .. 77 89 G Prr eo du in cd ti oT nru Cth u rL vo ess
1.75 3.7 2.77
1.50 33 .. 56 22 .. 77 56
1.25 3.4 2.74 1.00 33 .. 23 222 ... 777 123
0.75 33 .. 01 22 .. 67 90 Mean Prediction Error = 0.159%
0.50 2.9 2.68
0.25 2.8 2.67
2.7 2.66
0.00 0 10000 20000 30000 40000 50000 60000 2.6 0 10000 20000 30000 40000 50000 60000 2.6 35 5000 40000 45000 50000 55000 60000
Step Step Step
(b)FullcurvepredictionofcosineLRS(60Kstepstoη =0.1·η ).
min max
2.00 Learning Rate Curve 33 .. 89 G Prr eo du in cd ti oT nru Cth u rL vo ess 22 .. 77 89 G Prr eo du in cd ti oT nru Cth u rL vo ess
1.75 3.7 2.77
1.50 33 .. 56 22 .. 77 56
1.25 3.4 2.74 1.00 33 .. 23 222 ... 777 123
0.75 33 .. 01 22 .. 67 90 Mean Prediction Error = 0.176%
0.50 2.9 2.68
0.25 2.8 2.67
2.7 2.66
0.00 0 10000 20000 30000 40000 50000 60000 2.6 0 10000 20000 30000 40000 50000 60000 2.6 35 5000 40000 45000 50000 55000 60000
Step Step Step
(c)Fullcurvepredictionofmulti-stepcosineLRS(80%+10%+10%)(DeepSeek-AI,2024)
2.00 33 .. 89 G Prr eo du in cd ti oT nru Cth u rL vo ess 22 .. 77 89 G Prr eo du in cd ti oT nru Cth u rL vo ess
1.75 3.7 2.77
1.50 33 .. 56 22 .. 77 56
1.25 3.4 2.74 1.00 33 .. 23 222 ... 777 123
0.75 33 .. 01 22 .. 67 90 Mean Prediction Error = 0.235%
0.50 2.9 2.68
0.25 2.8 2.67
Learning Rate Curve 2.7 2.66
0.00 0 10000 20000 30000 40000 50000 2.6 0 10000 20000 30000 40000 50000 2.65 36000 38000 40000 42000 44000 46000 48000 50000
Step Step Step
(d)FullcurvepredictionofWSDLRS(20%cosineannealingtoη =0)(Huetal.,2024).
min
2.00 Learning Rate Curve 33 .. 89 G Prr eo du in cd ti oT nru Cth u rL vo ess 22 .. 88 34 G Prr eo du in cd ti oT nru Cth u rL vo ess
1.75 3.7 2.82
1.50 33 .. 56 22 .. 88 01
1.25 3.4 2.79 1.00 33 .. 23 222 ... 777 678
0.75 3.1 2.75
3.0 2.74
0.50 2.9 2.73
0.25 2.8 2.72 Mean Prediction Error = 0.322%
2.7 2.71
0.00 0 10000 20000 30000 40000 50000 60000 2.6 0 10000 20000 30000 40000 50000 60000 2.7 30 0000 35000 40000 45000 50000 55000
Step Step Step
(e)FullcurvepredictionofevenstrangeLRS:periodicallyrepeatinglinearannealingandlinearre-warmup.
Figure3: Fulllosscurveprediction(60Ksteps)bytheuniversallosscurveequationacrossvarious
LRS. The left, the medium, and the right figures in each row are learning rate curve, zoomed-
outlossprediction, zoomed-inlossprediction, respectively. Theredrectanglemeansthezoomed-
in zone. The figures omit the warmup in the first 500 steps. The universal loss curve is L =
2.628+0.429·S−0.550−0.412·S ,fittedasinFig.2. Theequationcanaccuratelypredictvarious
1 2
formsofunseenLRS,anditishighlypreciseinpredictingthetrendoflosschangesasLRvaries.
Please note that these are predictive results, which means that none of the points in this figure are
involvedinthefittingprocess. ThemeanpredictionerrorsacrossvariousLRSarelowto∼0.2%.
4
401×
etaR gninraeL
401×
etaR gninraeL
401×
etaR gninraeL
401×
etaR gninraeL
401×
etaR gninraeL
ssoL noitadilaV
ssoL noitadilaV
ssoL noitadilaV
ssoL noitadilaV
ssoL noitadilaV
ssoL noitadilaV
ssoL noitadilaV
ssoL noitadilaV
ssoL noitadilaV
ssoL noitadilaV2.00
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00
0 10000 20000 30000 40000 50000 60000
Step
0.50
0.45
0.40
0.35
0.30
0.25
0.20
0.15
0.100 10000 20000 30000 40000 50000 60000
Step
3.00
2.95
2.90
2.85
2.80
2.75
2.70
0 10000 20000 30000 40000 50000 60000
Step
Figure 4: The curve shapes of learning rate (top), gradient norm (medium), and validation loss
(bottom)holdaquitehighsimilarityacrossvariousLRS(labeledasdifferentcolors).
Where0≤f(s−T )≤1isadecreasingfunctionaboutsteps,andη isthemaximallearn-
stable max
ing rate. Ha¨gele et al. (2024) consolidates the effectiveness of WSD scheduler by many empirical
experiments. Moreover,Ha¨geleetal.(2024)alsofindsthatusing1-sqrtannealingandamoderate
annealingratio(e.g. 20%)canfurtherdecreasethefinalloss.
2.3 LEARNINGRATEANNEALING
Learningrateannealingisacommontechniqueusedintrainingneuralnetworks,wherethelearning
rateisreducedovertrainingiterationsaccordingtoapre-definedlearningrateschedule.Thismethod
isoftenemployedtoimprovetheperformanceandstabilityofthemodelduringtraining(Loshchilov
&Hutter,2016). Forexample,thewidely-usedcosineLRScouldbeperceivedasaprocesswhere
the learning rate is cosine curve-like annealing over full steps. Conversely, WSD LRS (Hu et al.,
2024)maintainsastablelearningrateforthemajorityofthesteps,implementingannealingonlyin
thefinal(e.g. 10% ∼ 20%)ofthesteps. DuringthetrainingofLLMs,ithasbeenwidelyobserved
thatamorepronounceddecreaseinthelearningrateoftenresultsinamoreprecipitousdropinthe
validation loss (Loshchilov & Hutter, 2016; Ibrahim et al., 2024; DeepSeek-AI, 2024; Hu et al.,
2024). However,tothebestofourknowledge,allpreviousstudiesendwithprovidingaroughand
qualitativedescriptionofhowlosschangesduringLRannealing, whileweprovideanequationto
formulatethelosschangesduringLRannealinginthiswork.
3 THEORY
Inthissection,weelaborateindetailontheintuition,theorigin,step-by-stepderivation,andtheex-
perimentalbasisofourtheory. Wethenprovetheeffectivenessofourformulathroughexperiments.
3.1 SIMILARITYBETWEENLEARNINGRATE,GRADIENTNORM,ANDLOSS
AsshowninFig.4,thefirstkeyobservationisthattheshapesofLRcurve,gradientnormcurve,and
validationlosscurvearequitesimilaracrossvariousLRS.Thisgivesacluethattheremightexistan
implicitconnectionbetweenlearningrateandloss,wheregradientnormcouldbethebridge.
5
401×
etaR
gninraeL
mroN
tneidarG
ssoL
noitadilaV2.77 2.75
Learning Rate Ground-Truth Loss-Annealing-0.1K 2.0 2.0
Turning Point of LR Ground-Truth Loss-Annealing-0.5K
2.76 Ground-Truth Loss-Annealing-1K
Ground-Truth Loss-Annealing-2K 2.74 0.3K Delay Steps
2.75 Turning Point of Loss 1.5 0.3K Delay Steps 1.5
0.1K Delay Steps 0.1K Delay Steps 2.73
2.74
1.0 2.73 2.72 1.0
2.72 0.5 2.71
0.5
2.71
2.70 1.1K 0 .D 8Kel a Dy e lS at ye p Ss teps0.3K Delay Steps 0.0 2.70 G G Gr r ro o ou u un n nd d d- - -T T Tr r ru u ut t th h h L L Lo o os s ss s s- - -R R Re e e- - -W W Wa a ar r rm m mu u up p p- - -0 1 0. K .1 5K K
0.4K Delay Steps Turning Point of LR Ground-Truth Loss-Re-Warmup-2K 0.0
Learning Rate Turning Point of Loss
2.69 2.69
39000 40000 41000 42000 43000 44000 45000 46500 47000 47500 48000 48500 49000 49500
Steps Steps
(a)Thedifferentdelaystepsintheannealingprocess (b) The different delay steps in the re-warmup pro-
with different annealing steps (0.1K, 0.5K, 1K and cesswithdifferentre-warmupsteps(0.1K,0.5K,1K
2K)andthesameminlearningrate. and2K)andthesamemaxlearningrate.
Figure5: Thedelayphenomenonbetweenthelearningrateandvalidationloss. Thisphenomenon
suggeststhatlearningrateannealing(re-warmup)hasmomentum.
Scaling Laws for Constant LRS. Constant LRS could be seen as a special LRS, in which any
step could be as the endpoint of the LRS. Memorize that chinchilla scaling law (Hoffmann et al.,
2022)exactlydescribestheendpointoffulllosscurves. Thatistosay,theexpectationofvalidation
lossofallstepsinconstantLRSadheretochinchillascalinglaw.
ExtraLossChangesinLRAnnealing. Differently,LRannealing(orre-warmup)bringsdrastic
losschangesinalocalrange(e.g. Fig.4), whichmakesthefulllosscurvenomorefollowtypical
scaling laws. Consider that the LRS without annealing, or constant LRS, follows typical scaling
laws. Based on that, we can easily induce that, in a full loss curve, the loss L at step s can be
rectifiedbyanaddedη-relateditem,andmayfollowtheformatas
L(s)=L +A·s−α−f(η) (4)
0
wherethebluepartisthesameaschinchillascalinglaw(Hoffmannetal.,2022),whiletheredpart,
f(η) denotes the extra loss change brought by LR annealing. Recall the curve similarity between
learningrateandloss.Itcouldleadtoonenaiveguess:f(η)=C·η ,whereCisapositiveconstant.
s
TrainingDiscountinAnnealing. TheformofEq.4isstillimperfect. Notethatthegradientgof
parametersdecreasesalongwithLR(showninFig.4). Comparedwiththestagesbeforeannealing,
the amount of parameter changes (nearly η ·|g| of each step) in the neural network optimization
process will bring out an almost quadratic faster decline during LR annealing. These clues show
that during LR annealing, the loss drop brought by power law (the blue part in Eq. 4) should also
decrease. Therefore,wemodifytothefollowingformat:
L(s)=L +A·S−α−f(η)
0 1
(cid:88)s (5)
S = η
1 i
i=1
WherewedefineS asforwardarea. S isexactlytheareaenclosedbytheLRScurveandtheX-
1 1
axisstep,whichcouldbeapproximatelyregardedasthetotalamountofneuralnetworkparameter
changes. Fig.1givesamoreintuitivevisualizationforthedefinitionofS .
1
3.2 LRANNEALINGMOMENTUM
Another key observation is that LR annealing has momentum. For more information about the
propertyoff(η),WedesignonespecialLRSwherethelearningratelinearlydecreasesfromη to
max
η thenincreases.Wemaintaintheslopeoftheincreasingstage(always5Ksteps)andchangethe
min
slopeinthedecreasingstage. Thedecreasingstepsinclude0.1K,0.5K,1K,and2K.Symmetrically,
we design another special LRS where the learning rate linearly increases from η to η then
min max
6
ssoL
noitadilaV
401×
etaR
gninraeL
ssoL
noitadilaV
401×
etaR
gninraeLdecreases. We observe how the validation loss changes with LR increasing or decreasing. The
learningrateandlosscurvesareshowninFig.5.
We discover the delay phenomenon between learning rate and validation loss. Firstly, the turning
point of validation loss curve is always later than the turning point of learning rate curve in both
schedulers. Thatmeansthatthevalidationlossstillmaintainsitsprevioustrajectoryforafewsteps
even after the learning rate changes direction. Secondly, the steeper the slope of the learning rate
annealing(orre-warmup),themorepronouncedthedelaybetweenthetwoturningpoints. Thirdly,
giventhesameLRslope,theleftfigure(LRdecreasingthenincreasing)alwayshasalongerdelay
stepsthantherightfigure(LRdecreasingthenincreasing).
Interestingly,theentireexperimentalphenomenonisstrikinglysimilartothephysicalexperimentof
asmallballfallingalongaslope.Thelargertheangleoftheslope,thefastertheballfalls.Whenthe
balllands,theaccumulatedmomentumcausestheballtocontinueslidingforsomedistance. Based
onthedelayphenomenon, weposeahypothesisthatf(η), thelossdropbroughtbyLRannealing
hascumulativehistoricalformationsothatthepastchangeoflearningratewillaffectthefollowing
losscurveforafewsteps. Insummary,learningrateannealinghasmomentum.
Thuswedefinef(η)=C·S asfollows:
2
m =λ·m +(η −η )
i i−1 i−1 i
s
(cid:88)
S = m
2 i
(6)
i=1
s i
(cid:88)(cid:88)
= (η −η )·λi−k
k−1 k
i=1k=1
Wherem istheLRannealingmomentumatstepi,and∆η =η −η denotestheLRannealing
i i−1 i
amount. λisthedecayfactor,whichsignifiesthedegreetowhichhistoricalinformationisretained.
Wefindthatλempiricallyrangesfrom0.99to0.999. Infact,ifLRannealinghadnomomentum,
λ = 0wouldinducethatf(η) = C ·η ,whichisexactlythenaiveformatmentionedabove. Note
s
thatS isnotonlyapplicabletoLRannealing(S > 0),butalsotoLRre-warmup(S < 0). This
2 2 2
meansthatourequationcanbealsoappliedincontinualpre-training,whereLRre-warmupserves
asanimportantfactorforbetteroutcomes. Moreintuitively,thedefinitionofS canbevisualized
2
inFig.1,astheweightedsumoftheareasofthesmallbluesquares.
3.3 FINALFORMAT
Definition. Giventhesametrainingandvalidationdataset,thesamemodelsize,thesametraining
hyper-parameterssuchasmaxlearningrateη andbatchsize,thelanguagemodelinglossatstep
max
s
sinafulllosscurveempiricallyfollowasL(s) = L +A·S−α−C ·S ,whereS = (cid:80) η and
0 1 2 1 i
i=1
S aredefinedasEq.6. L ,A,C,αareallundeterminedpositiveconstantstobefitted.
2 0
It is feasible to apply the formulation to universally describe the loss of each step across diverse
learningrateschedulers. Thisformulationsupportsvalidationlosspredictioninamorecomplicated
LRSoflongertotalsteps,fittedfromasimplerLRSofshortertotalsteps.
BalancebetweenS andS . Noticethat ∂L < 0and ∂L < 0alwaysholdtrue,whichmeans
1 2 ∂S1 ∂S2
boththeincreaseofS1andS2canhelptoreducetheloss. However,asshownintuitivelyinFig.1,
there exists a delicate balance between S and S . Once the learning rate begins to anneal and
1 2
S startstoincrease, theforwardareaS ofincomingstepsstartstodiminish. Ourequationaptly
2 1
describesthisdelicatebalance. InSec.4,weelaborateindetailonthisissue.
3.4 EXPERIMENTS
LRWarmup. Differentwarmupstepscanresultindifferentlosscurvesintrainingfromscratch.
During the warmup stage, neural networks are prone to random optimization, resulting in unpre-
dictable outcomes in the very beginning stage (Hestness et al., 2017). One evidence is about the
highgradientnormintheLRwarmupstage. Fig.4showsthatattheverybeginningoftraining,the
7gradientnorminitiallyconvergesfromarelativelyhighvaluetoalowerone,exhibitingapatterndis-
tinctfromthesubsequentsteps. Fromanotherperspective,LRwarmupisalsonecessary. Ourpilot
experiment(refertoAppendixA)showsthatwarmupindeedsignificantlyacceleratesconvergence,
a finding also noted by Liu et al. (2020); Kosson et al. (2024). Consequently, in all experiments
conductedforthispaper, weemploya500-steplearningratelinearwarmuptoreachthemaxLR,
whereS andS arecomputedyetasconstantmaxLRinthewarmupstage3.
1 2
Experimental Setups. In this paper, we adopt standard experimental setups in large language
model pre-training. The training dataset is Fineweb (Penedo et al., 2024) and the validation
datasetisRedPajama-CC(Computer,2023). Wetraina594Mnon-embeddingparametersLLAMA
architecture-likemodel(Meta,2024)fromscratch. WeuseAdamWoptimizer(Loshchilov&Hut-
ter,2017)withβ = 0.9andβ = 0.95. Theweightdecayissetas0.1andgradientclipissetas
1 2
1.0. Wesetmaximallearningrateas2×10−4 andbatchsizeas4Mtokens. Weusethetokenizer
ofLLAMA-3(Meta,2024). Weadoptthedecayfactoroflearningrateannealingλ=0.999andwe
discusstheimpactofλinSec.6.
Tovalidatethatourformulationworksacrossdifferentexperimentalsettings, wealsoconductour
experimentsonanotherexperimentalsetups.PleaserefertoAppendixBforfullexperimentalsetups.
Fitting Details. Given a learning rate scheduler, we can easily compute out S and S of each
1 2
stepinadvance. Toestimate(L ,A,C,α), weadoptasimilarfittingmethodaschinchillascaling
0
law(Hoffmannetal.,2022). Specifically, weminimizetheHuberloss(Huber,1964)betweenthe
predictedandtheobservedloglossusingtheL-BFGSalgorithm(Nocedal,1980):
min
(cid:88)
Huber
(cid:16) logLˆ(i)−logL(i)(cid:17)
δ (7)
L0,A,C,α
Stepi
Weimplementthisbytheutilizationofminimizeinscipylibrary. Huberlossistoenhanceto
robustnessofthefittingresultsandwesetδ ofHuberlossas10−3. Wemitigatethepotentialissue
oflocalminimaoffittingbychoosingtheoptimalfitfromarangeofinitialconditions. Notethatin
practice,wecanalsofitthefulllosscurvesusingmultipleLRSwithasingletupleof(L ,A,C,α).
0
Inthissituation,wesumtheHuberlossesinEq.7ofallfittedLRS.
Fitting and Prediction Results. We employ the full loss curves from constant and cosine LRS,
bothcontaining20Ktotalsteps,asourfittingdatapoints(refertoFig.2).Wethenproceedtopredict
thefulllosscurvesforvariousunseenLRSwith60Ktotalsteps(refertoFig.3). Thecoefficientof
determination(R2)inthefittingprocessislargerthan0.999,nearlyperfect,whichunderscoresthe
robustcapabilityofourequationtofitlosscurvesacrossdiverseLRSusingasingleparametertuple.
AsforpredictionshowninFig.3,ourequationdemonstratesbroadapplicabilityandrobustgeneral-
izationacrossfivedistincttypesofLRS,includingfourpreviouslyunseenLRS.Thisisevidencedby
thelowmeanpredictionerror,whichisapproximately0.2%. Remarkably,ourequationiscapable
toaccuratelypredictlosseseveninaverycomplicatedLRSwithmultipleLRre-warmupstages,as
illustrated in Fig. 3e, despite the absence of any LR re-warmup stage in the fitting LRS. We also
fitandpredictonthecurvesadoptinganotherexperimentalsetups(showninAppendixC),andthe
resultsaresimilarlygood,whichprovesthatEq.1holdsacrossdifferentexperimentalsetups.
3.5 EXTENSIONTOMODELSIZESCALING
LossDropinAnnealingStageScaleswithModelSizeN. First,weresearchwhetherthemodel
size N impacts on the amount of loss drop in the annealing stages. We compare the difference of
final losses between constant LRS and WSD LRS (10% cosine annealing to η = 0), thus to
min
getthelossgapbroughtbyLRannealing. Weconductthisexperimentondifferenttotalstepsand
differentmodelsizes. TheexperimentalresultsareshowninFig.6a. Itsuggeststhatthelossdrop
broughtbyLRannealingisscalingwithbothannealingstepsandmodelsize,whichmeansthatthe
annealingareaS inourequationgetslargerwhenmodelsizeN becomeslarge. Wesupposethere
2
isasimplerelationshipofS ∝Nγ whereγ isapositiveconstanttobefitted.
2
3NoteLRwarmupintrainingfromscratchisdifferentfromLRre-warmupincontinualtraining,wherewe
donotregardre-warmupstepsasahyper-parameterandwillshowhowtoapplyourequationtofindoptimal
re-warmupstepsinSec.4.
80.0500
N=84.95M 3.9 Ground-truth Loss
00 .. 00 44 57 05 N N= =1 56 91 4. .9 6M M 333 ... 678 F + Ri 2t 3t =i .n 0 0g 2
.
98C 9*u N 8rv 8^e 9(: - 0L .o 1s 7s 9 = ) -1 0.6 .26 74 7 + *S 0 2. *4 N3 ^2 (* 0S .1 0^ 57(- )0.552) N N
N
N= =
=
=8 1
5
94 6
9
0. 1
4
39 .
.
.5 9
6
7M M
M
M
3.5 N=1214.4M 0.0425 3.4
3.3 0.0400
3.2
3.1
0.0375
3.0
0.0350 2.9
2.8
0.0325 2.7
2.6
0.0300 2.5
10000 15000 20000 25000 30000 35000 40000 0 10000 20000 30000 40000 50000 60000
Step Steps
(a)ThelossdropbroughtbyLRannealingofdiffer- (b)CurvefittingoncosineLRS(60Kstepstoη =
min
entmodelsizes.Thedashedlinesrepresentthetrend 0.1·η ) of many model sizes using our scaling
max
oversteps. ItshowsthatthelossgapbroughtbyLR lawextendedtomodelsize. Bysubstitutingspecific
annealingalsoscaleswithdatasizeandmodelsize. valuesofN,theobtainedequationsimplifiestoEq.1.
Figure 6: The loss drop brought by LR annealing (on the left) and the N-extended full loss curve
fitting(ontheright).ItsuggeststhatS ∝Nγ isreasonablefrombothactualexperimentalphenom-
2
enaandcurvefitting.
The Format with Model Size. According to the experiments and analysis above, based on the
typicalscalinglaw(Eq.2),wecanextendourproposedequationtomodelsizescalingasfollows:
L(s,N)=L +A·S−α+B·N−β −C·S ·Nγ (8)
0 1 2
whereN denotesthenumberofnon-embeddingmodelparameters(M),andB,β,γ areN-related
undeterminedpositiveconstants.
FittingwithModelSize. WeutilizetheEq.8tofitfulllosscurvesofdifferentmodelsizes. The
fittingresultisshowninFig.6b,whereR2 >0.999isverycloseto1,suggestingthatthelosscurves
of different model sizes indeed follow our proposed N-extended equation. We also fit the curves
adopting another experimental setups (shown in Appendix D) and get similar results, proving that
Eq.8holdsacrossdifferentexperimentalsetups.
4 TAKEAWAYS: EXPERIMENTAL FINDINGS VERIFICATION AND
EXPLANATION
Ourderivedequationdescribesthetrainingdynamicsoflanguagemodels. Inthissection,weapply
the equation to give theoretical verification and explanation for many existing experimental find-
ings. These key insights also serve as a guide for researchers to select critical LRS in advance by
predictionviaourequation,withlittlecomputationalcost. Aninterestingsummaryisthat
The art of learning rate schedule lies in the delicate balancing act between
forwardareaandannealingarea.
4.1 ITVERIFIESANDEXPLAINSWHYLOSSDROPSMORESHARPLYWHENLRANNEALS.
WeadoptourequationtohelpresearchersunderstandwhylossdropsmoresharplywhenLRanneals,
whichhasbeenwidelyobservedinmanypreviousstudies. Wesubstitutethefittedparameters(see
Fig.2)toourequationasaninstance. WedrawhowtheS -item(A·S−α)andthenegativeS -item
1 1 2
(−C·S )impactsthelossalongwithaWSDscheduler. Fig.7suggeststhatstartingfromannealing
2
stage,negativeS -itemhasamuchmoresignificantimpactontheoveralllossthanS -item,which
2 1
makes loss drop more sharply compared with the stable LR stage. In conclusion, LR annealing
bringsoutquickincreaseoftheannealingarea,resultinginadrasticdecreaseinvalidationloss.
9
)gnilaennA
oN .s.v
gnilaennA(
porD
ssoL
ssoL
noitadilaV0.30 2.00
0.25 1.75
0.20 1.50
0.15 S1-item 1.25
Negative S2-item
0.10 Annealing Start 1.00
Learning Rate Curve
0.05 0.75
0.00 0.50
0.25
0.05
0.00
10000 12000 14000 16000 18000 20000
Step
Figure 7: How S1-item and negative S -item changes in a WSD scheduler. Gray area means the
2
amountoflossdropbroughtbyS andS inannealingstage.
1 2
4.2 ITVERIFIESANDEXPLAINSWHENWEUSECOSINELRS,WESHOULDSETTHECOSINE
CYCLELENGTHT ASTHETOTALSTEPSS,ANDSETMINLRAS0TOGETTHEOPTIMAL
LOSS.
3.00 Prediction loss curve of T=0.6S (min_lr=0.1 max_lr)
Prediction loss curve of T=1.4S (min_lr=0.1 max_lr) 2.0
2.95 P Pr re ed di ic ct ti io on n l lo os ss s c cu ur rv ve e o of f T T= =S S ( (m mi in n_ _l lr r= =0 0. )1 max_lr)
LR of T=0.6S (min_lr=0.1 max_lr)
2.90 L LR R o of f T T= =1 S . 4 (S m ( inm _i ln r=_l 0r= .10 . m1 am x_a lx r)_lr) 1.5
LR of T=S (min_lr=0)
2.85
2.80 1.0
2.75
0.5
2.70
2.65
0.0
2.60
0 100000 200000 300000 400000 500000
Step
Figure 8: The predicted loss curves of different cycle length T and min LR in cosine LRS, well
alignedwithpreviousstudies.
ManypapershavefoundthatinLLMpre-trainingusingcosineLRS,settingthecosinecyclelength
T as the total steps S, and setting min LR as nearly 0 (rather than 10% max LR) can lead to the
optimal loss (Hoffmann et al., 2022; Hu et al., 2024; Parmar et al., 2024). Actually, the settings
abovehavebeenafactualstandardinLLMpre-trainingusingcosineLRS.Wetheoreticallyverify
and explain the finding by our equation, presented as Fig. 8. The prediction curve convincingly
demonstratesthatthelosscurve,withtheconfigurationT = S andaminLRof0,indeedachieves
theoptimallossintheend.
Moreover,ourequationgivesaquiteintuitiveexplanation. T >S leadstoanincompleteannealing
while T < S leads to a small forward area due to early annealing. Thus, it is optimal to set T as
thetotalstepsS. Similarly,settingminLRas0canmakelargerannealingamountandthuslarger
annealingareaS ,whichenableslowerfinalloss.
2
4.3 ITVERIFIESANDEXPLAINSTHEPHENOMENON,WHERECONSTANTLRSGETSALOWER
LOSSTHANCOSINELRSIFSETTINGSMALLTOTALSTEPS,ANDVICEVERSA.
Intheexperiments,wefindthatifwesetsmalltotalsteps,thefinallossofconstantLRScouldbe
evenlowerthancosineLRS,andviceversa.Refertotheground-truthlossinFig.2(20Ksteps),and
theground-truthlossinFig.3a,3b(60Ksteps). Tovalidatethisphenomenon,weuseourequation
to draw the prediction loss curve of 10K total steps and 100K total steps in Fig. 9. It shows that
ourproposedequationcanverifywellthatthebetterLRSchangesoverthetotalsteps. Moreover,
10
ssoL
ssoL
noitadilaV
401×
etaR
gninraeL
401×
etaR
gninraeL3.5 2.00 3.00 2.00 3.00 The final loss of Cosine LRS
3.4 P Pr re ed di ic ct ti io on n l lo os ss s o of f C Co os ni sn te a nL tR LS RS 1.75 2.95 P Pr re ed di ic ct ti io on n l lo os ss s o of f C Co os ni sn te a nL tR LS RS 1.75 2.95 The final loss of Constant LRS
3.3 C Co os ni sn te a nL tR L ( R1 0 (1K 0 s Kt e sp tes p) s) 1.50 2.90 C Co os ni sn te a nL tR L ( R1 0 (10 0K 0 s Kt e sp tes p) s) 1.50 2.90
3.2 1.25 2.85 1.25 2.85
3.1 1.00 2.80 1.00 2.80
0.75 0.75
3.0 2.75 2.75
0.50 0.50
2.9 0.25 2.70 0.25 2.70
2.8 0 2000 4000 6000 8000 10000 0.00 2.650 20000 40000 60000 80000 1000000.00 2.6510000 20000 30000 40000 50000 60000
Total Steps Total Steps Total Steps
(a)Thepredictedlosscurveof10K (b) The predicted loss curve of (c) The predicted final loss of
steps. 100Ksteps differenttotalsteps.
Figure9: Comparisonofconstantandcosineindifferentsteps.
Fig.9cshowsthepredictedfinallossofdifferenttotalstepsusingconstantandcosineLRS.Itfurther
convincinglysuggeststhatconstantLRSindeedgetsalowerlossifsettingsmalltotalsteps,butthe
scalingslopeissmallerthancosineLRS’s,resultinginhigherlossinmoresteps.
From a more essential and comprehensive perspective, |∂L| is a power-law decreasing function
∂S1
while |∂L| is stable over training steps. In the early stages, |∂L| is large when S is small, thus
∂S2 ∂S1 1
increasingS bymaintaininglargeLR(e.g.constantLRS)intheearlystagescangreatlyhelpreduce
1
theloss. Thatis,S playsadominantroleoverS . Inthelaterstages,|∂L|ismuchsmallerwhen
1 2 ∂S1
S becomeslarge,thusincreasingS inthelaterstagesdoesnotsignificantlyhelpreducetheloss.
1 1
Thatis,S playsadominantroleoverS . Atthisstage,ItistimetostartLRannealingtoincrease
2 1
S . Interestingly,thisperspectivealignsdirectlywiththeideaofWSDLRS(Huetal.,2024):Inthe
2
earlystages,theneuralnetworkisexploringgloballyanditisasuitabletimetousealargerLR;In
thelaterstages,theneuralnetworkisexploringlocallyanditisasuitabletimetouseasmallerLR.
WewilldelvefurtherintoWSDLRSinthefollowingsubsections.
4.4 ITVERIFIESANDEXPLAINSWSDANDMULTI-STEPCOSINELRSHAVEMATCHEDOR
EVENLOWERLOSSTHANCOSINELRS.
0.30
2.00
0.25
1.75
0.20
1.50
0.15 S1-item of Cosine
1.25 Negative S2-item of Cosine
S1-item of WSD
1.00 0.10 Negative S2-item of WSD
S1-item of Multi-stage Cosine
0.75 0.05 Negative S2-item of Multi-stage Cosine
Cosine LRS
0.50 WSD LRS 0.00
Multi-step Cosine LRS
0.25
0.05
0.00
0.10
0 10000 20000 30000 40000 50000 60000 0 10000 20000 30000 40000 50000 60000
Step Step
(a)LearningratecurvesofthreetypesofLRS. (b)S -itemandnegativeS -itemofdifferentLRS.
1 2
Figure10: ThecomparisonbetweenS -itemandnegativeS -itemindifferentLRS.
1 2
Recently, it has been demonstrated that WSD LRS (Hu et al., 2024) and multi-step cosine
LRS (DeepSeek-AI, 2024) yield a lower loss compared to the typical cosine LRS. We also prove
thisbytheexperiments(refertotheground-truthlossinFig.3b,3c,3d). Wevalidateandelucidate
this finding using our proposed scaling law with LR annealing. In Fig. 10, we depict the learning
rate(ontheleft)andthepredictedlossdrop(ontheright)fordifferentLRSusingourderivedequa-
tion. Thefiguressuggestthatformulti-stepcosineorWSDLRS,thenegativeS -item(−C·S )is
2 2
slightlyhigherthanthatofcosineLRS.However,theS -item(A·S−α)issignificantlylowerthan
1 1
thatincosineLRS.Moresimplytosay,bothWSDLRSandmulti-stepcosineLRSaimtoemploya
11
ssoL noitailaV
401×
etaR
gninraeL
401×
etaR
gninraeL
ssoL noitailaV
ssoL
401×
etaR
gninraeL
ssoL noitadilaV2.85 2.85
20k Steps 20k Steps
Cosine 40k Steps
40k Steps 60k Steps
60k Steps 80k Steps
2.80 80k Steps 2.80 100k Steps
100k Steps
2.75 2.75
2.70 2.70
2.65 2.65
0.0 0.2 0.4 0.6 0.8 1.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
Ratio of Annealing Steps Forward Area S1
(a)Therelationshipbetweenthepredictedfinalloss (b)Therelationshipbetweenpredictedfinallossand
and the ratio of annealing steps under the condition theforwardareaS ofdifferenttotalsteps. Different
1
ofdifferenttotalsteps. pointsdenotedifferentannealingratios.
Figure11:Illustrationofthepredictedlossinrelationtotheratioofannealingstepsandtheforward
areainWSDLRS(cosineannealing),presentingparabola-likecurves,withadistinctoptimalloss.
strategythatmarginallyreducesS butsubstantiallyincreasesS ,leadingtoanoveralldecreasein
2 1
validationloss.
4.5 ITVERIFIESANDEXPLAINSTHATAMODERATEWSDANNEALINGRATIOCOULDGET
THELOWESTLOSS.
InthecaseofWSDlearningrateschedule,itiscrucialtoascertaintheoptimalannealingratiofor
training steps. Prior research by Ha¨gele et al. (2024) has established the existence of an optimal
annealing ratio within the WSD scheduler. As illustrated in experimental results of their study,
excessivelyhighorlowannealingratiosleadtosub-optimalmodelperformance. Thisphenomenon
canbefurtherelucidatedthroughourproposedequation.
Theoretically,ahighannealingratioresultsinasignificantreductionoftheforwardtrainingareaS ,
1
whileonlymarginallyincreasingtheannealingareaS . Conversely, anexcessivelylowannealing
2
ratio leads to under-annealing, characterized by a diminutive S and consequently get a high final
2
loss. Our scaling law function builds a trade-off relationship between the forward area S and
1
annealingareaS abouttheannealingratio.
2
AsdepictedinFig.11,weutilizethefittedscalinglawfunctiontodrawthefinallossacrossvarious
annealing ratios and total training steps. The prediction results present parabola-like curves, and
align well with the actual experimental outcomes by previous works. It suggests that a moderate
WSD annealing ratio is optimal which could make a moderate S , and then maximize S and S
1 1 2
fromaglobalview,therebyminimizingtheoverallvalidationloss. Moreover,ourequationdirectly
helppredictanoptimalannealingratiofordifferenttotalstepswithoutexperiments,whichsavesa
lotofresources.
4.6 ITVERIFIESANDEXPLAINSTHATTHEOPTIMALANNEALINGFUNCTIONINWSDLRS
DEPENDSONTHEANNEALINGRATIO.
InthecontextoftheWSDLRS,theselectionoftheannealingmethodintheannealingstageisalso
pivotaltooptimizethetrainingprocess.Ha¨geleetal.(2024)concludethatthe1-sqrtannealing(refer
toAppendixEfor1-sqrtfunctionandcurve)yieldsalowerfinallosscomparedtotheotherannealing
methods(e.g. cosine). Theyclaimthattheconclusionholdstrueacrossdifferentannealingratios.
However, as we predict using our equation (Fig. 12a), it indicates that 1-sqrt annealing does get a
lowerlossthancosineannealingin10%and20%annealingratios,butperformsmuchworsethan
cosineannealingin50%annealingratio.
12
ssoL
noitadilaV
ssoL
noitadilaV2.850 2.700 2.850 2.715
Constant Constant
10% Cosine Annealing 10% Cosine Annealing
2.825 10% 1-Sqrt Annealing 2.825 10% 1-Sqrt Annealing
20% Cosine Annealing 20% Cosine Annealing Ratio
20% 1-Sqrt Annealing 2.695 20% 1-Sqrt Annealing Ratio 2.710
2.800 5 50 0% % C 1-o Ss qin rte AA nn nn ee aa lil nin gg 2.800 5 50 0% % C 1-o Ss qin rte AA nn nn ee aa lil nin gg RR aa tt ioio
2.775 2.690 2.775 2.705
2.750 2.750
2.725 2.685 2.725 2.700
2.700 2.700
2.680 2.695
2.675 2.675
2.65100000 15000 20000 25000 30000 35000 40000 45000 50000 550002.675 2.65100000 15000 20000 25000 30000 35000 40000 45000 50000 550002.690
Steps Steps
(a)Thepredictedlosscurveofcosineand1-sqrtan- (b)Thetruelosscurveofdifferentannealingratios
nealingmethodofdifferentannealingratio. withcosineand1-sqrtannealingmethods.
Figure12:Thepredicted(left)andtrueloss(right)ofcosineand1-sqrtannealingmethodatdifferent
annealingratios. Experimentalresults(right),alignedwithourprediction(left),refutetheprevious
finding“theorderandresultsofdifferentannealingholdacrosssettings”(Ha¨geleetal.,2024).
Toverifywhetherthepredictionsfromourequationareaccurate,weconductexperimentsbytrain-
ingmodelsusingdifferentannealingmethodsandratioswithinafixed50Ktotalsteps.Asillustrated
in Fig. 12b, at a 10% or 20% annealing ratio, the 1-sqrt method outperforms the cosine method,
whereasata50%annealingratio,thelattermethodexhibitsalowerfinalloss. Thetrueexperimen-
talresultsalignquitewellwithourprediction,whichalsooverturnssomeoftheconclusionsmade
bypreviousworks. WeconcludethattheoptimalannealingfunctioninWSDLRSdependsonthe
annealingratio.
Ourscalinglawfunctionprovidesanexplanatoryframeworkfortheseobservations.WedrawtheLR
curvesof1-sqrtandcosineannealinginAppendixE.At10%annealingratio,althoughtheforward
area S of the cosine method is slightly larger thanthat of the 1-sqrt method, the larger annealing
1
areaS ofthe1-sqrtmethodplaysamorecriticalroleinreducingtheoverallfinalloss. However,
2
as the annealing ratio increases, the difference of S between two LRS gradually becomes larger
1
andlarger,tillbreakingthedelicatebalancebetweenS andS at50%annealingratio,resultingin
1 2
alowerfinallossforthecosinemethod. Thisrelationshipunderscorestheimportanceofcarefully
selecting the annealing strategy to optimize model training outcomes within the WSD scheduler.
Still, our equation can help predict a better annealing method without experiments, which saves a
lotofresources.
4.7 ITVERIFIESANDEXPLAINSTHATINCONTINUALPRE-TRAINING,THEHIGHERMAX
LEARNINGRATETORE-WARMUP,THEHIGHERTHEINITIALPEAKLOSSWILLBE,AND
THENTHEMORESHARPLYITWILLDECREASE.
In continual pre-training (CPT), the learning rate scheduler is usually set as re-warmup to a new
maxLRatthebeginning. Bymanyexperiments,Guptaetal.(2023)concludesthatthehighermax
learningratetore-warmup,thehighertheinitialpeaklosswillbe,andthenthemoresharplyitwill
decrease.
Accordingtoourscalinglawfunction4,inthere-warmupprocess,theannealingareaS willreduce
2
toanegativevalue(S <0)andthusthevalidationlossincreases.ThehighermaxLRinre-warmup,
2
theannealingareaS becomesmorenegativeandthustherewouldbeahigherpeakloss. Butstill,
2
highermaxLRcouldmaketheforwardareaS growfasterandthelossdecreasesmoresharplyafter
1
re-warmup. We use the fitted equation to predict the continual pre-training process with different
maxLRasshowninFig.13a. Thepredictedlosscurvesreproduceaquitesimilarphenomenonwith
previousworks(Guptaetal.,2023).
4Strictlyspeaking,continualpre-trainingprocessoftenincludeLRre-warmupaswellasdatadistribution
shift. Hereweprimarilyresearchontheconditionwherethereisnodistributionshiftbetweentwotraining
stages.TheconclusionstransferacrossmostcasesbecausethelosschangebroughtbyLRre-warmupissignif-
icantlylargerthanthelosschangebroughtbydatadistributionshift(Guptaetal.,2023;Ibrahimetal.,2024).
13
ssoL noitadilaV
detciderP
ssoL
noitadilaV
eurT2.85 2.85
Pre-training Pre-training
CPT Constant 2e-5 Warmup Steps: 500
CPT Cosine Max LR: 2e-5 Warmup Steps: 5000
CPT Cosine Max LR: 6e-5 Warmup Steps: 10000
2.80 CPT Cosine Max LR: 1e-4 2.80
CPT Cosine Max LR: 2e-4
2.75 2.75
2.70 2.70
Pre-training final learning rate is 2e-5 Pre-training final learning rate is 2e-5
2.65 2.65
2.60 2.60
0 10K20K30K40K50K60K70K80K90K100K 0 10K20K30K40K50K60K70K80K90K100K
Continual Pre-training Steps Continual Pre-training Steps
(a) The predicted validation loss with different re- (b) The predicted validation loss with different re-
warmup max LR in the continual pre-training pro- warmup max learning rate in the continual pre-
cess.Allthere-warmupstepsare500steps. trainingprocess.
Figure13: Thepredictedvalidationlosswithdifferentre-warmupmaxlearningrateandre-warmup
stepsinthecontinualpre-trainingprocess. TheLRSofcontinualpre-trainingiscosine(T=100K)
andtheminlearningrateis0.
ThereisamoreprofoundstrategyusingourequationinCPT.AsshowninFig.13a,afterensuring
totalstepsduringCPT,wecanapplyourequationtopredictabettermaxLRandschedulertoget
thelowestfinallosswithoutexperiments,whichsavesalotofresources.
4.8 ITVERIFIESANDEXPLAINSTHATINCONTINUALPRE-TRAINING,THESTEPSOF
RE-WARMUPHAVELITTLEIMPACTONTHEFINALLOSS.
Meanwhile,howmanystepstore-warmupisanotherimportantissueinthecontinualpre-training.
Gupta et al. (2023) find that the longer re-warmup steps could smooth the transition of loss curve
butthenumberofre-warmupstepsdoesnotsignificantlyinfluencethefinalvalidationloss. Weuse
thefittedequationtopredictedthecontinualpre-trainingdynamicswithdifferentre-warmupsteps.
Theresults,showninFig.13b,presentagoodalignmentwithpreviousworks(Guptaetal.,2023).
Basedonourtheory, giventhefixedmaxLR,whenthere-warmupstepsarelonger, theannealing
areadecreasesmoreslowlyandthelosscurverisesmoresmoothly,butbothfinalS andS arequite
1 2
stable across different re-warmup steps. First, the annealing area S of different re-warmup steps
2
areverycloseduetothesamemaxLRandthesameminLR.Besides,thoughdifferentre-warmup
stepsbringintemporarydistinctlosses,re-warmuponlycoverasmallpercentagecomparedwithall
trainingsteps. Thus,theforwardareaS isalsocloseacrossdifferentre-warmupsteps,resultingin
1
thecloseoveralllossacrossdifferentstepsofre-warmup.
5 COMPARISON WITH CHINCHILLA SCALING LAW
5.1 REDUCTIONTOCHINCHILLASCALINGLAW
Our scaling law function can predict the full loss curve across any given learning rate scheduler.
Inthissection,weshowthatourequationhasnocontradictionwithtypicalscalinglaw,anditisa
generalizedformofthechinchillascalinglaw(Hoffmannetal.,2022). Thatistosay, allthefinal
loss points of different total training steps following our equation should also follow a power-law
relationship. Weprovethisbydividingintotwoconditions: (1)constantLRS,and(2)otherLRS.
Constant LRS. In the case of constant learning rate scheduler, the annealing area S is always
2
zeroandtheforwardareaS =η ·s,wheresisthestep,andη isthemaximallearningrate
1 max max
whichisconstantoversteps. Thus,thewholetrainlosscurvesbecomes:
L(s)=L +(A·η−α )·s−α =L +A′·s−α (9)
0 max 0
whichcanbeeasilyreducedtothechinchillascalinglawequation.
14
ssoL
noitadilaV
ssoL
noitadilaV3.00 3.00
Predicted Loss - Total Step: 20k Predicted Loss - Total Step: 45k Predicted Loss - Total Step: 20k Predicted Loss - Total Step: 45k
Predicted Loss - Total Step: 25k Predicted Loss - Total Step: 50k Predicted Loss - Total Step: 25k Predicted Loss - Total Step: 50k
Predicted Loss - Total Step: 30k Predicted Loss - Total Step: 55k Predicted Loss - Total Step: 30k Predicted Loss - Total Step: 55k
2.95 P Pr re ed di ic ct te ed d L Lo os ss s - - T To ot ta al l S St te ep p: : 3 45 0k k P Fir te ted dic -t Ce hd i nL co hs is ll a- Total Step: 60k 2.95 P Pr re ed di ic ct te ed d L Lo os ss s - - T To ot ta al l S St te ep p: : 3 45 0k k P Fir te ted dic -t Ce hd i nL co hs is ll a- Total Step: 60k
2.90 2.90
Fitted Chinchilla Curve: Fitted Chinchilla Curve:
Loss = 2.210 + 4.905*Step^(-0.207) Loss = 2.261 + 4.420*Step^(-0.216)
2.85 R2 = 0.996 2.85 R2 = 0.996
2.80 2.80
2.75 2.75
2.70 2.70
2.65 2.65
10000 20000 30000 40000 50000 60000 10000 20000 30000 40000 50000 60000
Steps Steps
(a)Thepredictedlossofdifferenttotalstepswithco- (b) The predicted loss of different total steps with
sineLRSandthefittedchinchillacurve. WSDLRSandthefittedchinchillacurve.
Figure14:Chinchillascalinglawfitswellthevalidationlossendpointspredictedbyourformulation,
takingcosineLRS(ontheleft)andWSDLRS(ontheright)asexamples.
Other LRS. For other learning rate schedulers, we adopt a method based on statistics to show
that our scaling law function can be reduced to the chinchilla scaling law. Specifically, we check
whether chinchilla scaling law fits well the endpoints of loss curves predicted and generated by
our scaling law. The parameter tuple of our equation is (L ,A,C,α). We then randomly sample
0
1000 sets of parameter tuples in some uniform distributions: L ∼ U(1,3), A ∼ U(0.3,0.5),
0
C ∼U(0.2,0.6),α ∼U(−0.6,−0.4). Eachparametertuplecouldbeseenasthefittingresultofa
distinctsetofexperimentalsetups5. (e.g. dataset,batchsize,modelsize,etc.). Foreachgenerated
parametertuple,weapplyourequationtopredictthefinallossofdifferenttotaltrainingstepsrange
from 5K steps to 60K steps. We conduct the prediction on two LRS including cosine and WSD
(10%annealingratio). Thepredictedfinallosspointsareusedtofitthechinchillaequationthrough
minimizingtheHuberloss. FittingexamplesareshowninFig.14.
LRScheduler mean(R2)↑ std(R2)↓ Huberloss↓
Cosine 0.972 0.056 0.00017
WSD 0.979 0.053 0.00013
Table 1: Mean and standard deviation of R2, as well as the mean Huber loss of 1000 randomly
generatedparametertuples.
WecalculatethemeanandstandarddeviationofR2andHuberlossamong1000randomparameter
tuples. As shown in Table 1, the random selected parameter tuples have low Huber loss and high
R2 value. The experimental results demonstrate that chinchilla scaling law fits well on the data
predictedandgeneratedbyourscalinglawfunction. Thatis,ourscalinglawwithLRannealingcan
bereducedtochinchillascalinglaw.
5.2 SCALINGLAWFITTINGDEMOCRATIZATION
Weextremelydemocratizethefittingandpredictionofscalinglawbygreatlyenlargingitsscopeof
applicability.AdoptingourscalinglawwithLRannealing,wecancollectthousandsoffittingpoints
inonefulllosscurveduringtraining,whiletypicalscalinglawcanonlycollectoneendpointfroma
fulllosscurve.
Foramoredirectcomparisonwithchinchillascalinglaw(Hoffmannetal.,2022),wesupposeasce-
nariowhereitrequires100fittingpointstogettheparametersofscalinglaws. Weassumethedis-
5It’sworthnotingthatsomeofthesesampledparametertuplesmightnotbereasonableorlikelytohappen
inreal-worldscenarios,butwechoosetokeepthemnonetheless.
15
ssoL
noitadilaV
ssoL
noitadilaVEquation LRS Computationalcost ApplicabletootherLRS?
Chinchilla Cosine 100% No
Chinchilla WSD(20%annealing) 21.6% No
Chinchilla WSD(10%annealing) 11.8% No
Ours Any(exceptconstant) <1.0% Yes
Table2:Thecomparisonofcomputationalcostforfitting,assuming100fittingpointsasanexample.
tancebetweeneachpointasK. Wecomputetherequiredtrainingstepsusingdifferentapproaches
asfollows:
• Adopting chinchilla scaling law, typical cosine LRS requires total steps of at least 1K +
2K+3K+···+100K =5050K.
• Adopting chinchilla scaling law, WSD LRS (notating annealing ratio as r) requires total
stepsofatleast(1K+2K+3K+···+100K)r+100K(1−r)=(100+4950r)K.
• Adoptingourscalinglaw,allweneedisonlyonetrainingcurvewithmoderatetotalsteps
(andthenumberoffittingpointsisfarmorethan100),suchasonecurvewith50Ksteps6.
WepresentacomparisonofthecomputationalcostsassociatedwithdifferentlawsandLRSinTa-
ble 2. Our findings indicate that our proposed equation enables us to fit loss with less than 1% of
thecomputationalcostrequiredbythechinchillascalinglaw. Furthermore,ourscalinglawwithLR
annealing, can be universally applicable to other LRS by fitting any given LRS (except constant),
therebyconservingadditionalcomputationalresources. Thisapproachextremelydemocratizesthe
processoffittingandpredictingscalinglawsinLLMpre-training,pavingthewayforamoreenvi-
ronmentallyfriendlyandcarbon-efficientmethodology.
6 DISCUSSION
6.1 THEIMPACTOFDECAYFACTORλ
2.850 2.720 2.850 2.720
WSD 1-Sqrt Annealing and Re-warmup Fitting Curve WSD 1-Sqrt Annealing and Re-warmup Fitting Curve
2.00 2.825 W CoS nD s t1 a- nS tq Fu ita tr ie n gA n Cn ue rva eling and Re-warmup Fitting Curve 2.715 2.825 W CoS nD s t1 a- nS tq Fu ita tr ie n gA n Cn ue rva eling and Re-warmup Fitting Curve 2.715
1.75
1.50 2.800 2.710 2.800 2.710
2.775 2.775 1.25 2.705 2.705 1.00 2.750 2.700 2.750 2.700
0.75 2.725 2.725
0.50 2.700 2.695 2.700 2.695
0.25 WSD 1-Square Annealing and Re-warmup LR 2.675 2.690 2.675 2.690
WSD 1-Sqrt Annealing and Re-warmup LR
0.0200000 25000 30000 35000 4 S0 t0 e0 p0 45000 50000 55000 60000 2.65300000 35000 40000 4S5t0e0p0 50000 55000 600002.685 2.65300000 35000 40000 4S5t0e0p0 50000 55000 600002.685
(a)Learningratecurves. (b)Thelosscurveofλ=0.99. (c)Thelosscurveofλ=0.999.
Figure15: Thecomparisonoffittingeffectofdifferentdecayfactorλ.
Thedecayfactorλinourequationplaysacrucialroletoindicatetheinformationretainingdegree
inLRannealing. Wesetλas0.999inourallexperiments. Weexplorethedifferencefromanother
decayfactorλ = 0.99. Wefitandgetdifferentequationsfordifferentλ. Wecomparethem(1)on
the predicted loss curves for 1-square and 1-sqrt annealing methods, and (2) on the predicted loss
curvesindifferentannealingratiosofWSDLRS(cosineannealing).
Theresults,illustratedinFig.15and16,revealseveralkeyinsightsintotheimpactofdecayfactor:
DelaySteps. Alargerdecayfactorresultsinlongerdelaysteps.ComparingFig.15bandFig.15c,
λ = 0.999 introduces a more obvious delay phenomenon, which is consistent across both the 1-
6The empirical rule that more fitting points always achieve better fitting results always holds true. Our
equationcanalsousemorepointsandLRSforfitting,suchas30K constant+70K cosine. Nevertheless,we
cancollectfarmorefittingpointsthanthetypicalscalinglawwithsignificantlyfewertrainingsteps.
16
401×
etaR gninraeL ssoL noitadilaV ssoL noitadilaV2.85
2.80
=0.999 20k Steps =0.99 20k Steps
2.75 =0.999 Cosine =0.99 Cosine
=0.999 60k Steps =0.99 60k Steps
=0.999 100k Steps =0.99 100k Steps
2.70
2.65
0.0 0.2 0.4 0.6 0.8 1.0
Ratio of Annealing Steps
Figure16:ThepredictedlossindifferentannealingratiosofWSDLRSforλ=0.99andλ=0.999.
square and 1-sqrt annealing methods. The root reason is simple: larger λ can retain more LR
historicalmomentum,causinglongerdelaystepsafterLRfinishannealing.
OptimalAnnealingRatio. alargerdecayfactortendstofavorahigherannealingratio.Asshown
inFig.16, Theoptimalannealingratioofλ = 0.999islargerthanthatofλ = 0.99. Meanwhile,
duetothesimilarreason,λ=0.999favors1-sqrtannealingmethodwhileλ=0.99favors1-square
annealingmethod,asshowninFig.15.
BalancePointbetweenS andS . Moreessentially,theselectionofλdecidesthebalancepoint
1 2
of S and S . For example, λ = 0.999 means that, LR annealing only retain the information of
1 2
previousapproximately 1 = 1000steps,whichcanbeseenasthewindowsizeofLRannealing
1−λ
momentum. The window size could be very close to the optimal annealing steps. After reaching
windowsize,S increasesveryslowly,withthecostoflargedecreaseofS .
2 1
The analyses above underscore the sensitivity of our scaling law function to the choice of decay
factor and highlights the importance of selecting a decay factor that aligns closely with empirical
datatoensuretheaccuracyofpredictions. Werecommendthatthefuturedeveloperstrydifferentλ
fortheirownsetups7.
6.2 POSSIBLEROOTREASONSOFDELAYPHENOMENONINLEARNINGRATEANNEALING
InSec.3,wediscoverthedelayphenomenon,whichprovesthatLRannealinghasmomentum. We
discusspossiblerootreasonsofthephenomenoninthissection.
AdamOptimizer? No. WenoticethatAdamoptimizer(Kingma&Ba,2015)alsohasthefirst-
ordermomentumdecayfactorβ andthesecond-ordermomentumdecayfactorβ ,whichpresents
1 2
thepossibleconnectiontothethedelayphenomenon.
Wekeepβ = 0.9, andconductdelayexperimentsondifferentβ ∈ {0.95,0.99,0.999}(default:
1 2
0.95) of AdamW optimizer (Loshchilov & Hutter, 2017) to observe whether larger β causes a
2
more longer delay steps. The learning rate and ground-true loss curve are shown in Fig. 17a. It
suggeststhattheground-truthlosscurvesofdifferentβ almostcoincidewitheachother,andtheir
2
delaystepsare alsothesame. Therefore, webelievethatAdamoptimizer haslittletodowith the
delayphenomenon,despiteitsmomentumformseemingveryrelatedtoourexperiments. Speaking
of which, we even once tried to mimic the form of Adam Optimizer to describe LR annealing
momentum,attemptingtodiscoveraconnectionbetweenthem,butthefittingresultswereamess.
7Actually,λcanbefittedasaparameter,insteadofahyper-parameterrequiringmanualtuning. Weregard
λas ahyper-parameter becauseλ = 0.999performswell in ourmost experiments. Besides, fittingwith λ
couldbringinadditionaltimecomplexityduetotherecomputationofS .
2
17
ssoL
noitadilaV2.77 2.77
Constant 2.0 L Tue ra nr in ni gn g P oR ia nt te of LR G Gr ro ou un nd d- -T Tr ru ut th h L Lo os ss s- -R Re ew wa ar rm mu up p- -5 1K 0K 2.0
2.76 2=0.95 2.76 Turning Point of Loss
2=0.99
2.75 2=0.999 1.5 2.75 1.5
0.5K Delay Steps
2.74 2.74 1.0
1.0
2.73 2.73
0.3K Delay Steps 0.5
2.72 0.5 2.72
2.71 2.71 0.0
Learning Rate 0.0
2.70 2.70
38000 39000 40000 41000 42000 43000 44000 45000 39000 40000 41000 42000 43000 44000 45000 46000
Steps Steps
(a)Thecomparisonoftruelosscurvewithsettingdif- (b) The comparison of delay steps of different re-
ferentβ ofAdamoptimizer. warmupsteps(andthusdifferentS ).
2 1
Figure17:Thepossiblerootreasonanalysis(left:Adamoptimizer,right:S )ofdelayphenomenon.
1
ForwardAreaS ? NotReally. NomatterhowLRchanges,S isalwaysincreasingoversteps,
1 1
resultinginconsistentlyreducingthevalidationlossbroughtfromS . Therefore,theforwardarea,
1
S would lengthen delay steps in LR annealing then re-warmup, but would shorten delay steps in
1
LRre-warmupthenannealing. ThedelayphenomenonisindeedrelatedtoS .
2
Butstill,S isnotallthereasonsofdelayphenomenon.WeprovethisbyFig.5b,whichsuggeststhat
1
even though in LR re-warmup then annealing, the delay phenomenon, while not that pronounced,
stillexists. Moreover,weconductdelayexperimentsbyadjustingtheslopeofLRaftertuningpoint
ofLR.AsshowninFig.17b, WefindthatmoresmoothslopeofLRre-warmup, withsmallerS ,
1
but still causes longer delay steps. Therefore, we conclude that S indeed influences the specific
1
delaylength,butisnottherootreason.
OtherPossibleReasons? Thedelayphenomenoncouldbeintuitiveinsomecases. Forexample,
supposethatlearningratedecreasesdirectlyfrom2e-4to2e-5inonestep,andmaintains2e-5. In
thiscase, althoughthelosswoulddecreasetoalowervaluebuttheparameterchangesinonestep
is too small in neural networks. Given a sudden low LR, neural networks still require some steps
to gradually optimize to a local minimum, incurring delay phenomenon. But still, analysis above
stillendswitharoughdescription,andwehavenotfiguredouttherootreasonsandlookforwardto
completingthispartinfuturework.
6.3 OTHERPOSSIBLESCALINGLAWFORMATSWITHLRANNEALING
AddingaLR-weightedCoefficienttoS ? ImaginethatwhenLRannealstonearly0,theneural
2
network’sparametersalmostdonotchangeandthevalidationlossshouldnotchange,either. How-
ever, as defined in our equation, Eq. 1, S still has historical momentum even if LR is nearly 0,
2
makingthelosscontinuetodecreaseandmisalignwithobservedtrainingdynamics.
To cover this corner case, we try a revision to our equation and add a LR-weighted coefficient to
S . Specifically, weadjustS tomoreapproach0whenη iscloseto0, counteractingtheoriginal
2 2
formulation’stendencytooverestimatelossreductionwhenη ≈0.
TherevisedequationfortheannealingareaS inourscalinglawfunctionisasfollows:
2
m =λ·m +(η −η )
i i−1 k−1 k
i
(cid:88)
= (η −η )·λi−k
k−1 k
(10)
k=1
s
(cid:88)
S = m ·ηϵ
2 i i
i=1
WheretheredpartistheaddedLR-weightedcoefficientandϵisaundeterminedpositiveconstant.
ϵcouldbeverysmallinpractice.
18
ssoL
noitadilaV
401×
etaR
gninraeL
ssoL
noitadilaV
401×
etaR
gninraeL12 .. 70 50 23 .. 90 50 C C C Co o o os s n ni i s sn n t te e a a n nG F t tir t G Fo ti iu n r to tn g iu nd C n g T u d r Cr u Tv ut reh ru v tL eho Lss oss 23 .. 90 50 C C C Co o o os s n ni i s sn n t te e a a n nG F t tir t G Fo ti iu n r to tn g iu nd C n g T u d r Cr u Tv ut reh ru v tL eho Lss oss
2.90 2.90
11 .. 25 50 2.85 Loss = R 2 2. =61 00 . 9+ 9 90 6.449*S1^(-0.528) - 0.347*S2 2.85 Loss = R 2 2. =61 04 . 9+ 9 90 7.445*S1^(-0.536) - 0.436*S2^1.125
1.00 2.80 2.80
0.75 2.75 2.75
0.50 2.70 2.70
0.25 C Co on sis nt ea n Lt e L ae rna ir nn gin Rg a R tea t Ce u C rvu erve 2.65 MM ee aa nn CC oo sn is nt ea n Ft it F tii ntt gin Eg r rE or rr o =r = 0. 00 5.0 74 %7% 2.65 MM ee aa nn CC oo sn is nt ea n Ft it F tii ntt gin Eg r rE or rr o =r = 0. 00 4.0 04 %9%
0.00 0 10000 20000 3 S0 t0 e0 p0 40000 50000 60000 2.6 10 0000 20000 30000 Step40000 50000 60000 2.6 10 0000 20000 30000 Step40000 50000 60000
(a)LRofcosineandconstant. (b)ThefittingcurveofL∝S (c)ThefittingcurveofL∝Sζ.
2 2
Figure18: ThecomparisonoffittingeffectbetweenL∝Sζ withL∝S .
2 2
Wehavetriedtherevisedfunctiontofitdata. Wefindthatthefittingresultsarequitesimilarandϵ
isverycloseto0,showinglittleuseinpracticaleffect. Hence,weadopttheoriginalformatinour
experiments8.
L∝Sζ ratherthanL∝S ? Actually,allweknowisthatLandS haveapositivecorrelation.
2 2 2
Thus L ∝ Sζ rather than L ∝ S might be a more reasonable assumption. That is, our equation
2 2
would be changed to L(s) = L +A·S−α −C ·Sζ. Theoretically, the introduction of ζ as an
0 1 2
additionalfittingparameterisexpectedtoprovideamorenuancedcontroloverhowchangesinthe
learning rate annealing affect validation loss, potentially leading to improve the accuracy of our
equation.
However,theempiricalresults,asdepictedinFig.18,demonstratethatthefittingimprovementwith
the inclusion of ζ is quite marginal when compared to the version without this parameter. This
slight enhancement does not justify the additional complexity introduced by managing negative
valuesofS . Furthermore,theempiricalobservationthatζ tendstoconvergecloseto1(e.g. 1.125
2
in Fig. 18c) reinforces the notion that the original formulation of the function, without the power
term ζ, is adequately robust. This finding suggests that the direct influence of the learning rate
annealing area, as initially modeled, sufficiently captures the essential dynamics without the need
forthisadditionalcomplexity9.
7 FUTURE WORKS
7.1 TUNINGSCALINGLAWFORMAT
Wehavetestedavarietyofequationformatstoenhancetheaccuracyoftheentiretrainingprocess.
As a result, the final equation format, as presented in Eq. 1, proves to be optimal so far across a
rangeofscenarios. Andtheequationhasachievedalevelofpracticalitythatenablestheprediction
offuturelosswhenscalingtrainingstepsandmodelsizes. Wewillpersistinrefiningtheequation
formattoforecastthelosswiththehighestpossibleaccuracy.
7.2 EXTENSIONTOPOST-TRAINING
In this work, we research primarily on the scope of pre-training of LLM. We also show how to
applyourequationtoguidetheLRre-warmupstrategyincontinualpre-training. Wewillcontinue
researching on how to extend our equation to post-training, which might include data distribution
shift,datamixture,modelalignment,andspecificdownstreamevaluations.
8 CONCLUSION
In conclusion, we discover that the loss curves of neural language models empirically conform to
a scaling law with learning rate annealing over training steps (s), which is expressed as L(s) =
8Westillrecommendfuturedeveloperstotrythisformatifpossible.
9AnotheradditionalcomplexityliesinthatSζ becomesincalculablewhenS <0inLRre-warmup.
2 2
19
401×
etaR gninraeL
ssoL
noitadilaV
ssoL
noitadilaVL +A·S−α−C·S . Thisequationpointsouttheartofbalancebetweenforwardareaandanneal-
0 1 2
ingareaoflearningrateschedulers, offeringtheoreticalverificationandexplanationfornumerous
experimentalfindingsofpriorstudies. Wepresenttheunderlyingtheoryofourequationincluding
curve similarity and LR annealing momentum. We extend our equation to the format with model
sizeN. Moreover,weprovethatourequationisageneralizedformoftypicalscalinglaws,further
showingthesuperiorityofourequation.
Compared to typical scaling laws which predict the endpoint of one full loss curve, our equation
canaccuratelyforecastthelossoflanguagemodeltrainingatanygivenstepandacrossanylearning
rate scheduler. As for required computation resources, our equation expends less than 1% of the
computational cost required by typical scaling laws to fit language modeling loss. Our proposed
approachgreatlydemocratizesthefittingandpredictionofscalinglawsinthedevelopmentoflarge
languagemodels.
REFERENCES
YasamanBahri,EthanDyer,J.Kaplan,JaehoonLee,andUtkarshSharma.Explainingneuralscaling
laws. Proceedings of the National Academy of Sciences of the United States of America, 2021.
doi: 10.1073/pnas.2311878121.
EthanCaballero, KshitijGupta, I.Rish, andDavidKrueger. Brokenneuralscalinglaws. Interna-
tionalConferenceonLearningRepresentations,2022. doi: 10.48550/arXiv.2210.14891.
Together Computer. Redpajama: an open dataset for training large language models, 2023. URL
https://github.com/togethercomputer/RedPajama-Data.
DeepSeek-AI. Deepseek llm: Scaling open-source language models with longtermism. arXiv
preprintarXiv: 2401.02954,2024.
Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang. Understanding emergent abilities of
languagemodelsfromthelossperspective. arXivpreprintarXiv: 2403.15796,2024.
Kshitij Gupta, Benjamin The´rien, Adam Ibrahim, Mats L. Richter, Quentin Anthony, Eugene
Belilovsky, Irina Rish, and Timothe´e Lesort. Continual pre-training of large language models:
Howto(re)warmyourmodel?,2023. URLhttps://arxiv.org/abs/2308.04014.
JoelHestness,SharanNarang,NewshaArdalani,GregoryDiamos,HeewooJun,HassanKianinejad,
Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,
empirically. arXivpreprintarXiv: 1712.00409,2017.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,TomHen-
nigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy,
Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre.
Trainingcompute-optimallargelanguagemodels. arXivpreprintarXiv: 2203.15556,2022.
ShengdingHu,YugeTu,XuHan,ChaoqunHe,GanquCui,XiangLong,ZhiZheng,YeweiFang,
YuxiangHuang,WeilinZhao,XinrongZhang,ZhengLengThai,KaihuoZhang,ChongyiWang,
Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang
Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm: Unveiling the potential of small
languagemodelswithscalabletrainingstrategies. arXivpreprintarXiv: 2404.06395,2024.
PeterJ.Huber. RobustEstimationofaLocationParameter. TheAnnalsofMathematicalStatistics,
35(1):73–101,1964.doi:10.1214/aoms/1177703732.URLhttps://doi.org/10.1214/
aoms/1177703732.
AlexanderHa¨gele,ElieBakouch,AtliKosson,LoubnaBenAllal,LeandroVonWerra,andMartin
Jaggi. Scalinglawsandcompute-optimaltrainingbeyondfixedtrainingdurations. arXivpreprint
arXiv: 2405.18392,2024.
AdamIbrahim, BenjaminThe´rien, KshitijGupta, MatsL.Richter, QuentinGregoryAnthony, Eu-
gene Belilovsky, Timothe´e Lesort, and Irina Rish. Simple and scalable strategies to continu-
ally pre-train large language models. Trans. Mach. Learn. Res., 2024, 2024. URL https:
//openreview.net/forum?id=DimPeeCxKO.
20Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models. arXivpreprintarXiv: 2001.08361,2020.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
BengioandYannLeCun(eds.),3rdInternationalConferenceonLearningRepresentations,ICLR
2015,SanDiego,CA,USA,May7-9,2015,ConferenceTrackProceedings,2015. URLhttp:
//arxiv.org/abs/1412.6980.
AtliKosson,BettinaMessmer,andMartinJaggi. Analyzing&eliminatinglearningratewarmupin
GPT pre-training. In High-dimensional Learning Dynamics 2024: The Emergence of Structure
andReasoning,2024. URLhttps://openreview.net/forum?id=RveSp5oESA.
LiyuanLiu,HaomingJiang,PengchengHe,WeizhuChen,XiaodongLiu,JianfengGao,andJiawei
Han. Onthevarianceoftheadaptivelearningrateandbeyond. In8thInternationalConference
on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-
view.net,2020. URLhttps://openreview.net/forum?id=rkgz2aEKDr.
I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm restarts. International
ConferenceonLearningRepresentations,2016.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:
1711.05101,2017.
Meta. Llama 2: Open foundationand fine-tunedchat models. arXiv preprintarXiv: 2307.09288,
2023.
Meta. Thellama3herdofmodels. arXivpreprintarXiv: 2407.21783,2024.
Eric J. Michaud, Ziming Liu, Uzay Girit, and Max Tegmark. The quantization model of neural
scaling. InAliceOh,TristanNaumann,AmirGloberson,KateSaenko,MoritzHardt,andSergey
Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on
NeuralInformationProcessingSystems2023,NeurIPS2023,NewOrleans,LA,USA,December
10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/
hash/5b6346a05a537d4cdb2f50323452a9fe-Abstract-Conference.html.
JorgeNocedal. Updatingquasinewtonmatriceswithlimitedstorage. MathematicsofComputation,
35(151):951–958,July1980. ISSN0025-5718. doi: 10.1090/S0025-5718-1980-0572855-7.
Jupinder Parmar, Sanjev Satheesh, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro.
Reuse, don’t retrain: A recipe for continued pretraining of language models. arXiv preprint
arXiv: 2407.07263,2024.
GuilhermePenedo, HynekKydl´ıcˇek, LoubnaBenallal, AntonLozhkov, MargaretMitchell, Colin
Raffel,LeandroVonWerra,andThomasWolf. Thefinewebdatasets: Decantingthewebforthe
finesttextdataatscale. arXivpreprintarXiv: 2406.17557,2024.
21A IMPACT OF WARMUP STEPS
Weconductexperimentsontheimapctoflearningratewarmupsteps. AsshowninFig.19,wefind
that500warmupstepscangetthelowestvalidationlosscomparedto100ornowarmupsteps. The
experimentalresultsalsoguideustochoose500warmupstepsinourexperimentsofthiswork.
0 Steps Warmup Ground Truth Loss
3.9
100 Steps Warmup Ground Truth Loss
3.8
500 Steps Warmup Ground Truth Loss
3.7
3.6
3.5
3.4
3.3
3.2
3.1
3.0
2.9
2.8
2.7
2.6
0 2500 5000 7500 10000 12500 15000 17500 20000
Step
Figure 19: The comparison of the true loss curve of different warmup steps. We experiment on
cosineLRSwith20Ktotalsteps.
B EXPERIMENTAL SETUPS
Inthiswork,weusemultiplesetsofexperimentalsetups,inordertovalidatethatourequationcan
workacrossdifferentexperimentalsetups. Forclarification,wepresenttheexperimentalsetuplist
asshowninTable3.
Inourmostexperiments(includingourtakeawaysections),weusethemainsettingA.Inourfitting
and prediction experiment, we repeat experiments and consolidate our conclusion using another
setting, setting B, and the experimental results are shown in Fig. 21. In our N-extended scaling
lawfittingexperiment,werepeatexperimentsandconsolidateourconclusionusinganothersetting,
settingC,andtheexperimentalresultsareshowninFig.22.
Setups SettingA(mainly) SettingB SettingC
ModelSize(Non-embedding) 594M 293M multiple
TrainDataset Fineweb Finweb Mixture-train*
ValDataset RedPajama-CC RedPajama-CC Mixture-valid*
TotalSteps 60K 120K 143K
MaximalLearningRate 2×10−4 2×10−4 1.381×10−3
WarmupSteps 500 100 500
BatchSize(tokens) 4M 2M 4M
Tokenizer LLAMA-3’s LLAMA-3’s ExtendedLLAMA-2’s
β ,β inAdamOptimizer 0.9,0.95 0.9,0.95 0.9,0.95
1 2
WeightDecay 0.1 0.1 0.1
GradientClip 1.0 1.0 1.0
Table 3: Experimental settings adopted in this work. * denotes pre-training multilingual dataset
includingmixtureofsourcessuchascommoncrawls,books,arxiv,code,etc. “ExtendedLLAMA-
2’s”isextendedfromLLAMA-2’stokenizer (Meta,2023)byaddingvocabulary.
22
ssoL
noitadilaV2.00 3.9 C Pro en ds it ca tin ot n L CR uS r G veround Truth Loss 3.04 C Pro en ds it ca tin ot n L CR uS r G veround Truth Loss
11 .. 57 05 33 .. 78 C Pro es din ice t iL oR nS C G urr vo eund Truth Loss 33 .. 00 23 C Pro es din ice t iL oR nS C G urr vo eund Truth Loss
1.25 33 .. 56 Fitti Rn 2g = C 0u .r 9v 9e 8: 5L = 2.761 + 0.517*S1^(-0.491) - 0.458*S2 3.01 3.00 1.00 3.4
0.75 3.3 2.99
3.2 2.98
0.50 3.1 2.97 Mean Prediction Error = 0.084%
0.25 C Co on sis nt ea n Lt e L ae rna ir nn gin Rg a R tea t Ce u C rvu erve 3.0 2.96
0.00 0 5000 10000 15000 20000 25000 30000 2.9 0 5000 10000 15000 20000 25000 30000 2.9 25 0000 22000 24000 26000 28000 30000
Step Step Step
(a)Learningrate. (b)Zoomed-outlossfitting. (c)Zoomed-inlossfitting.
Figure20: Fulllosscurvefittingoncosine(30Kstepstoη =0)andconstantLRS.Thefigures
min
omitthewarmupinthefirst100steps. Afterfitting,wegetauniversallossequationL=2.761+
0.517·S−0.491−0.458·S . RefertosettingBinTable3forexperimentalsetups.
1 2
C OUR SCALING LAW ON ANOTHER EXPERIMENTS SETUPS
Fig.2andFig. 3showthatourequationcanworkvery wellonourmainexperimentalsetup. For
proving that our scaling law with LR annealing can apply to different (but given) experimental
settings, we change the setting from A to B (refer to Table 3) and observe whether our equation
can still work or not. The fitting results are shown in Fig. 20. The prediction results are shown
in Fig. 21. The results suggest that our scaling law with LR annealing can still work well across
differentexperimentalsetups.
D OUR N-EXTENDED SCALING LAW ON ANOTHER EXPERIMENTS SETUPS
Fig. 6b show that our N-extended equation can work very well on our main experimental setup.
Similarly,forprovingthatourN-extendeccalinglawcanapplytodifferent(butgiven)experimental
settings,wechangethesettingfromAtoC (refertoTable3)andobservewhetherourequationcan
stillworkornot. ThefittingresultsareshowninFig.22. TheresultssuggestthatourN-extended
scalinglawwithLRannealingcanstillworkwellacrossdifferentexperimentalsetups.
E 1-SQRT AND 1-SQUARE ANNEALING
The1-sqrtannealingisproposedbyHa¨geleetal.(2024),whoselearningrateatstepsinannealing
stageisdefinedas:
(cid:114)
s−T
f(s)=1− stable
T −T (11)
total stable
η =η +f(s)·(η −η )
s min max min
WedrawthelearningratecurveofWSD(20%and50%1-sqrtannealing)inFig.23,comparedwith
cosineannealing.Otherthan1-sqrtannealing,Ha¨geleetal.(2024)alsomentions1-squareannealing
methoddefinedas:
(cid:18)
s−T
(cid:19)2
f(s)=1− stable
T −T (12)
total stable
η =η +f(s)·(η −η )
s min max min
23
401× etaR gninraeL ssoL ssoL2.93
2.00 33 .. 89 G Prr eo du in cd ti oT nru Cth u rL vo ess 22 .. 99 12 G Prr eo du in cd ti oT nru Cth u rL vo ess
1.75 3.7 2.90
1.50 33 .. 56 22 .. 88 89
1.25 3.4 2.87 3.3 2.86 1.00 3.2 22 .. 88 45
00 .. 57 05 233 ... 901 222 ... 888 123 Mean Prediction Error = 0.047%
0.25 2.8 2.80
Constant Learning Rate Curve 2.7 2.79
0.00 0 20000 40000 60000 80000 100000 120000 2.6 0 20000 40000 60000 80000 100000 120000 2.7 18 00000102500105000107500110000112500115000117500120000
Step Step Step
(a)FullcurvepredictionofconstantLRS.
2.93
2.00 33 .. 89 G Prr eo du in cd ti oT nru Cth u rL vo ess 22 .. 99 12 G Prr eo du in cd ti oT nru Cth u rL vo ess
1.75 3.7 2.90
1.50 33 .. 56 22 .. 88 89
1.25 3.4 2.87 3.3 2.86 1.00 3.2 22 .. 88 45
0.75 3.1 2.83
0.50 23 .. 90 22 .. 88 12 Mean Prediction Error = 0.253%
0.25 2.8 2.80
WSD Learning Rate Curve 2.7 2.79
0.00 0 20000 40000 60000 80000 2.6 0 10000 20000 30000 40000 50000 60000 70000 80000 90000 2.7 88 0000 82000 84000 86000 88000 90000
Step Step Step
(b)FullcurvepredictionofWSDLRS(90Ktotalsteps;10%cosineannealingtoη =0).
min
2.00 33 .. 89 G Prr eo du in cd ti oT nru Cth u rL vo ess 22 .. 88 89 G Prr eo du in cd ti oT nru Cth u rL vo ess
1.75 3.7 2.87
1.50 33 .. 56 22 .. 88 56
1.25 3.4 2.84 1.00 33 .. 23 222 ... 888 123
0.75 33 .. 01 22 .. 78 90 Mean Prediction Error = 0.255%
0.50 2.9 2.78
0.25 2.8 2.77
WSD Learning Rate Curve 2.7 2.76
0.00 0 20000 40000 60000 80000 100000 2.6 0 20000 40000 60000 80000 100000 2.7 15 00000 102000 104000 106000 108000 110000
Step Step Step
(c)FullcurvepredictionofWSDLRS(110Ktotalsteps;10%cosineannealingtoη =0).
min
Figure 21: Full loss curve prediction (120K steps) by the universal loss curve equation across
various LRS. The left, the medium, and the right figures in each row are learning rate curve,
zoomed-outlossprediction, andzoomed-inlossprediction, respectively. The red rectanglemeans
thezoomed-inzone. Thefiguresomitthewarmupinthefirst100steps. Theuniversallosscurveis
L=2.761+0.517·S−0.491−0.458·S ,fittedasinFig.3b. Theequationcanaccuratelypredict
1 2
variousformsofunseenLRS,anditishighlypreciseinpredictingthetrendoflosschangesasLR
varies.Pleasenotethatthesearepredictiveresults,whichmeansthatnoneofthepointsinthisfigure
(exceptconstantLRS)areinvolvedinthefittingprocess. Themeanpredictionerrorsacrossvarious
LRSarelowto∼0.2%. RefertosettingBinTable3forexperimentalsetups.
24
401×
etaR gninraeL
401×
etaR gninraeL
401×
etaR gninraeL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL3.9 Ground-truth Loss
3.8 N=18.68M
3.7 Fitting Curve: Loss = 1.412 + 0.921*S1^(-0.761) N=48.34M
+ 2.562*N^(-0.137) - 0.069*S2*N^(0.117)
3.6 N=106.2M
R2= 0.99837
3.5 N=161.9M
3.4 N=293M
3.3 N=401M
3.2 N=594M
3.1
N=1720M
3.0
2.9
2.8
2.7
2.6
2.5
2.4
2.3
2.2
2.1
0 20000 40000 60000 80000 100000 120000 140000
Steps
Figure 22: Curve fitting on cosineLRS (143K steps to η = 0) of many model sizes usingour
min
scalinglawextendedtomodelsizeN. RefertosettingCinTable3forexperimentalsetups.
2.00 2.00
1.75 1.75
1.50 1.50
1.25 1.25
1.00 1.00
0.75 0.75
0.50 0.50
20% WSD Cosine Annealing 50% WSD Cosine Annealing
0.25 0.25
20% WSD Sqrt Annealing 50% WSD Sqrt Annealing
0.000 10000 20000 30000 40000 50000 0.000 10000 20000 30000 40000 50000
Step Step
(a)LRcurveofWSD(20%1-sqrt/cosineannealing). (b)LRcurveofWSD(50%1-sqrt/cosineannealing).
Figure23:Thelearningratecurvesof20%(left)and50%(right)annealingratioinWSDLRS,with
cosineand1-sqrtannealingmethod.
25
401×
etaR
gninraeL
ssoL
noitadilaV
401×
etaR
gninraeL