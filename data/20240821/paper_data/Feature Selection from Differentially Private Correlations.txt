Feature Selection from Differentially Private Correlations
RyanSwope∗ AmolKhanna∗ PhilipDoldo∗
Swope_Ryan@bah.com Khanna_Amol@bah.com Doldo_Philip@bah.com
BoozAllenHamilton BoozAllenHamilton BoozAllenHamilton
Philadelphia,Pennsylvania,USA Boston,Massachusetts,USA Baltimore,Maryland,USA
SaptarshiRoy EdwardRaff
roysapta@umich.edu Raff_Edward@bah.com
UniversityofMichigan BoozAllenHamilton
AnnArbor,Michigan,USA UniversityofMaryland,
BaltimoreCounty
Syracuse,NewYork,USA
ABSTRACT 1 INTRODUCTION
Datascientistsoftenseektoidentifythemostimportantfeaturesin Linearmodelsremainoneofthemostcommonandwidelyused
high-dimensionaldatasets.Thiscanbedonethrough𝐿1-regularized techniquesinpracticeandresearchtoday.Inparticular,linearre-
regression,butthiscanbecomeinefficientforveryhigh-dimensional gressionandlogisticregressionarestraightforwardtosolveus-
datasets.Additionally,high-dimensionalregressioncanleakinfor- ingeithergeneral-purposeconvexsolverslikeLimited-memory
mationaboutindividualdatapointsinadataset.Inthispaper,we BFGS[28]orbespokeoptimizerslikethoseprovidedinLIBLINEAR
empiricallyevaluatetheestablishedbaselinemethodforfeature andotherlibraries[14,34].Inhigh-dimensionalsituations,where
selectionwithdifferentialprivacy,thetwo-stageselectiontech- thenumberoffeatures𝑑 isgreaterthanthenumberofsamples
nique,andshowthatitisnotstableundersparsity.Thismakesit 𝑁,theneedtoperformfeatureselectiontoavoidover-determined
performpoorlyonreal-worlddatasets,soweconsideradifferent systems is particularly pertinent. While classic approaches like
approachtoprivatefeatureselection.Weemployacorrelations- forward-backwardselection[2]andmutual-information[38,49]
basedorderstatistictochooseimportantfeaturesfromadataset arestillstudied,theincorporationofsparsity-inducingpenalties
andprivatizethemtoensurethattheresultsdonotleakinformation hasbecomethepredominantapproach.
aboutindividualdatapoints.Wefindthatourmethodsignificantly Theinductionofsparsityinthesolutionisusefulfromapure
outperformstheestablishedbaselineforprivatefeatureselection engineering,practicaldeployment,andanalyticalunderstanding
onmanydatasets. sincethe𝐿1penaltywasintroducedviathe“LASSO”regularizer
byTibshirani[47].The𝐿1penaltyfurtherhasprovableadvantages
CCSCONCEPTS inhighdimensionalsettings[32]thathaveledtosignificanteffort
incustomsolvers[15,16,56]andthebroadpreferenceformaking
• Security and privacy → Formal methods and theory of
featureselectionajointprocesswiththeregressionitself[21].
security;Databaseandstoragesecurity.
However,thewidespreadsuccessof𝐿1basedoptimizationfor
jointsolvingoffeatureselectionandmodelweightsisnotsoclear
KEYWORDS
cutwhenweareconcernedwiththeprivacyofthedatausedtobuild
Differential Privacy, Feature Selection, Model Selection, Sparse,
themodel.Insuchacase,thereisaneedtointroduceanadditional
Correlations,LinearRegression
frameworktoprotectdataprivacy.Specifically,differentialprivacy
ACMReferenceFormat: isastatisticalframeworkthatguaranteesdataprivacyinanalgo-
Ryan Swope, Amol Khanna, Philip Doldo, Saptarshi Roy, and Edward rithm[30].Givenparameters𝜖and𝛿,onanytwodatasets𝐷and
Raff.2024.FeatureSelectionfromDifferentiallyPrivateCorrelations.In 𝐷′differingononeexample,anapproximatedifferentiallyprivate
ProceedingsofProceedingsofthe17thACMWorkshoponArtificialIntelli- algorithmAsatisfiesPr[A(𝐷) ∈𝑂] ≤exp{𝜖}Pr[A(𝐷′) ∈𝑂]+𝛿
genceandSecurity(AISec’24).ACM,NewYork,NY,USA,12pages.https: forany𝑂 ⊆ image(A).Notethatlowervaluesof𝜖 and𝛿 corre-
//doi.org/XXXXXXX.XXXXXXX
spondtostrongerprivacy.Differentialprivacyistypicallyachieved
byaddingcalibratedamountsofnoiseinthemechanismortothe
∗Equalcontribution.
outputofA.
Differentiallyprivateregressionalgorithmsensurethattheweight
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
ofaregressiondoesnotrevealsignificantinformationaboutits
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation training data. This is especially important in high-dimensional
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe models,wheretheratioofparameterstotrainingdatapointsis
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission higher, and thus, the parameters can encode more information
and/orafee.Requestpermissionsfrompermissions@acm.org. aboutthedatapoints.Forthisreason,severaldifferentiallypri-
AISec’24,October18,2024,SaltLakeCity,UT vatehigh-dimensionalregressionalgorithmshavebeendeveloped.
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
ACMISBN978-1-4503-XXXX-X/18/06
https://doi.org/XXXXXXX.XXXXXXX
4202
guA
02
]GL.sc[
1v26801.8042:viXraAISec’24,October18,2024,SaltLakeCity,UT RyanSwope,AmolKhanna,PhilipDoldo,SaptarshiRoy,andEdwardRaff
However,thesealgorithmshavetoaddnoisescaledbythedi- 2 RELATEDWORK
mensionalityofthedata,whichcanquicklyoverwhelmthe Inthissection,wewilldescriberelatedworksonhigh-dimensional
signal,anddestroyallsparsityofthesolution[25].Thenoise regressioninthenon-privateandprivatesettings.Weaimtopro-
addedbydifferentialprivacyprovestobeasignificantroadblock videanoverviewofthespacewhilefocusingoncorrelation-based
tosuccessfulDPapproachesthatinducehard-zerosasthefeature selectionstrategiesinthenon-privatesetting.Wemakenotethat
selectionstepbecausethenoisecontinuallyknockssolutionsaway thenotionsof𝐿1basedfeatureselectionasaprocessindependent
fromexactzerocomponents.Forthisreason,wechoosetoexplore oftheoptimizationprocesshasbeendevelopedinthenon-private
differentiallyprivatefeatureselectionstrategieswhichreducethe settingvia“screeningrules”[27,29,33,37,50]thatattempttoiden-
dimensionalityofthedatasetpriortousingprivateregressionalgo- tifycoefficientswhichwillhaveazerovalueoncetheoptimization
rithms. isdone,andsocanbediscardedearlyforcomputationalefficiency.
Fewpastworkshaveconsidereddifferentiallyprivatefeature Unfortunately,negativeresultshavebeenshownforconverting
selection,andofthosethathave,mostproducecomputationally screeningrulesintoadifferentiallyprivateform[23].Itisforthis
infeasibleprocedures[39,45].Kiferetal.[26]developa“two-stage” reasonthatwelookbacktotheolderapproachofaseparatefeature
methodwhichiscomputationallyfeasiblebutstillintensiveasit selectionprocessfrommodeltrainingtoseeifimprovedresults
requirestrainingmultiplemodels.Itisinspiredbythe𝐿1penalty canbeobtained.Below,wewillreviewthepertinentnon-private
asitinvolvestrainingmultiple𝐿1-regularizedmodels,butakey andprivatehigh-dimensionalregressionliterature.
insightweempiricallydemonstrateisthat𝐿1penalizationisnot
algorithmicallystable,leadingtoinconsistentperformance 2.1 Non-privateHigh-dimensionalRegression
underdifferentialprivacy.
Statisticianstypicallyconstrainthestructureofhigh-dimensional
Asimpleyeteffectivenon-privatefeatureselectionalgorithm
regressions.Onesuchconstraintiscoefficientsparsity,whichwe
isSureIndependenceScreening(SIS),whichselectsthe𝑘features
willfocusoninthispaper.
withthehighestabsolutecorrelationtothetargetvector[13].This
methodisalsoflexible,allowinguserstoswitchoutcorrelation
Constrainingsparsityisequivalentto𝐿0-constrainedorpenal-
izedregression.Indeed,itisnotdifficulttoseethatforvarying𝜆,
withanyothermetrictheybelievetobebettersuitedtoidentifying
bothof
importantvariables.Wecomparethenon-privateSISalgorithmto
𝑁
anon-privateversionof[26]todisambiguatetheimpactofnoise ∑︁
vsalgorithmicinstabilityof[26].ThistestshowsthatSISperforms
argmin ℓ(x𝑖,𝑦 𝑖;w) (1)
better,andthusisolatestheinstabilityof𝐿1-regularizationasa
w∈R𝑑:∥w∥0≤𝜆𝑖=1
𝑁
majorfactorinthelowerperformanceof[26].Wethenprovide ∑︁
intuitionforwhytheSISalgorithmwillperformbetterinthepri-
argmin ℓ(x𝑖,𝑦 𝑖;w)+𝜆∥w∥0 (2)
vatesettingthanthetwo-stageapproachandproceedtoprivatize
w∈R𝑑 𝑖=1
SIS.OurexperimentsshowthatprivateSISperformsbetter
willproducesolutionsofvaryingsparsity.However,𝐿0-constrained
orpenalizedregressionisNP-hard,makingitimpossibletofind
thantwo-stageonavarietyofhigh-dimensionaldatasets,
solutionstotheseproblemsinpolynomialtime[48].
achievingsimilarorimprovedaccuraciesfor𝜖 inausable
rangeof[1,10],whilebeingeasytoreasonandprove.
Tomakeoptimizationfeasible,𝐿1-constrainedorregularized
regressioncanbeused.Here,theoptimizationfunctions
Here,weprovideanoverviewofthefollowingsections.Insec-
tion2,wewillreviewrelatedworksonnon-privateandprivate 𝑁
∑︁
featureselectionstrategies.Insection3,wewillcompareanon- argmin ℓ(x𝑖,𝑦 𝑖;w) (3)
privateversionoftwo-stagewithasimplecorrelation-basedse- w∈R𝑑:∥w∥1≤𝜆𝑖=1
lectionapproachandshowthatthebaselineissignificantlyless 𝑁
∑︁
stableundersparsitythanthesimpleapproach.Insection4,we argmin ℓ(x𝑖,𝑦 𝑖;w)+𝜆∥w∥1 (4)
willdescribehowtomakethiscorrelation-basedselectionprivate, w∈R𝑑 𝑖=1
andwillprovidemathematicalargumentshighlightingwhenour are convex and feasible. However, employing 𝐿1 constraints or
methodwillperformwell.section5willdetailexperimentsand penaltiescreatesabiasawayfromthe𝐿0 solution,meaningthe
theirresults,withsection6concludingthepaper.Wemakespecial optimalwinEquations(3)and(4)willnotequalthoseinEquations
notethattoimplementausefulandprivateSISfeatureselector, (1)or(2)[48].Nevertheless,DonohoandHuoandDonohoand
aprivatetop-𝑘 selectionstepwasneeded.Theonlymethodfor Eladshowedthatthesupportsetsofpenalized𝐿0solutionscanbe
top-𝑘selectionthatimprovedoverthebaselinewasthecanonical foundthroughoptimizationwith𝐿1penaltieswhenwissufficiently
Lipschitzmechanism[40],whichinitsoriginalpresentationisdif- sparse[9,10].
ficulttoproveandunderstand.Asanadditionalcontributionto However,evenifconvexoptimizationisfeasible,itcanbein-
thiswork,were-stateandsimplifytheexpositionofthisvaluable creasingly difficult on very high-dimensional datasets. For this
techniqueinAppendixAtoimproveitsutility. reason,statisticianshavedevelopedavarietyoffeatureselection
mechanismstoremoveirrelevantfeaturespriortooptimization.
Wedescribeasubsetofthesehere.
Oneofthefirstmethodsforfeatureselectionwhichdoesnot
optimize a variant of Equations (3) or (4) is Sure Independence
Screening(SIS)[13].ThismethodtreatseachfeatureasindependentFeatureSelectionfromDifferentiallyPrivateCorrelations AISec’24,October18,2024,SaltLakeCity,UT
andmeasuresthecorrelationbetweeneachfeatureandthetarget inthispaper.Instead,wecommentthatsimilartothenon-private
variable.Thetop-𝑘absolutelycorrelatedfeaturesareretained,with case,thesetechniquesbecomeincreasinglyinefficientwhenthe
theothersbeingscreenedoutpriortooptimization.Experiments dimensionalityofthedataisveryhigh,andamethodtoreducethe
withSISdemonstratethatitworksparticularlywellondatawith supportsetpriortooptimizationwouldbeuseful[25].Additionally,
independentfeaturesbutcanalsoperformreasonablywellwhen theutilityofeachofthesemethodsreliesonthedimensionality
featuresaremildlycorrelated. oftheirinputdata,andasthedimensionalityincreases,theirper-
SimilartoSISarescreeningrules,whichseektoboundthedual formancewilldecrease[36].Ifinsteadthesealgorithmsreceived
solutions of linear optimization problems within a compact set asupportsetofreasonablesizeafterfeatureselection,theycould
[52].Thiscompactsetcanidentifyfeatureswhicharesurelynot operatewithbetterexpectedutility.
partofthefeaturesetselectedby𝐿1-regularizedestimators,but Giventheusefulnessofaprivatefeatureselectionmechanism,
candosowithoutperformingoptimization.Inthisway,screening wechoosetostudytheeffectivenessoffeatureselectionfrompri-
rulescansafelyremovefeaturespriortooptimizationtomakethe vatecorrelations.Weempiricallycomparetheperformanceofour
optimizationproblemmorefeasible. methodtothefeatureselectionstageofthetwo-stageapproach,as
Thecompactsetswhichscreeningrulesusecanbespheresor thisistheonlyprivatefeatureselectionstrategywhichiscomputa-
spheresincombinationwithhalfspaces.Asthenumberofhalfs- tionallyefficient.
pacesemployedgrows,thecomputationalcomplexityofscreening
grows,andoftenwithmarginalbenefits.Asaresult,sphericalsets 3 EVALUATINGTHEINSTABILITYOFTHE
areoftenused.Interestingly,inthecaseofsphericalsets,there
TWO-STAGEAPPROACH
existsanequivalencebetweenSISandscreeningrulesforsome
Alineofrecentworksonprivatehigh-dimensionalregressionhas
regularizationvalueof𝜆[52].Thisequivalencedemonstratesthat
discussedtheroleofalgorithmicinstabilityinKiferet.al.’stwo-
itisreasonabletoselectfeaturesfor𝐿1-regularizedproblemsbased
stageapproach[23–25,36].However,noneoftheseworksseemto
oncorrelations.
haveevaluatedwhetherthisinstabilityaffectsactualperformance
throughanempiricallens.
2.2 PrivateHigh-dimensionalRegression Inthissection,weruntwosimpleexperimentstoshowthateven
inthenon-privatesetting,thetwo-stageapproachsuffersfromin-
High-dimensionalregressionhasalsobeenconsideredindiffer-
stabilitycomparedtothesimpleSISbaseline.Inthefirstexperiment,
entialprivacy.Wereviewsomekeyworkshere,butseethecited
surveybyKhannaetal.foramorecomprehensiveview[25].
wegenerate100datapointsX∼N(0,I100)whereeachx𝑖 ∈R100.
Kiferetal.firstconsideredhigh-dimensionaldifferentiallypri- Wethenuseaweightvectorw1= (cid:2)1 1 1 1 1 0 ··· 0(cid:3)⊤
withvalue1incomponents1through5andvalue0incomponents
vateregressionbybuildingatwo-stageprocedure[26].Thefirst
stageofthisprocedureprivatelychoosesasupportsetofsize𝑘by
6through100.Finally,wegeneratetargets𝑦
𝑖
=x𝑖⊤w1+𝜖 𝑖,where
splittingthedatainto√
𝑁
blocksofsize√
𝑁 andprivatelycomput-
𝜖
𝑖
∼N(0,0.1).
Withthisdataset,weemployedthefirststageofKiferet.al.’s
ingwhichfeaturesaremostconsistentlyincludedinthesupportsof
two-stagemethodwithanon-privateselectionsteptoidentifya
regressionestimatorsbuiltoneachblock.Oncethealgorithmfinds √
supportset.Wedidthisbysplittingthedatainto 𝑁 blocksofsize
asupportset,ittrainsafinalmodelrelyingononlythefeatures √
inthesupportset.Tothebestofourknowledge,thisalgorithmis 𝑁 andidentifyingthe5featu √reswhichweremostconsistently
theonlycomputationallyefficientalgorithmwhichemploysfea- includedinthesupportsofthe 𝑁 regressionestimators.Wealso
tureselectiontobuildhigh-dimensionalregressionestimatorswith employedthenon-privateSISalgorithmonthisdataset.SISchose
differentialprivacy. asupportsetbyidentifyingwhich5featuresinthedatasetwere
Heuristically,weseetwochallengeswithKiferetal.’sapproach. mostcorrelatedwiththetargetvariable.
First,theiralgorithmiscomputationallyintensive:itrequiresbuild- Thisexperimentwasrepeated1000times,andresultsarein-
√
ing 𝑁 +1high-dimensionalregressionestimators,whichmay cludedasexperimentW1inFigure1.ItisclearthatSISchoosesthe
notbepossibleifthedimensionalityofthedataisveryhigh.Sec- truenonzerofeaturesmuchmoreoftenthanthetwo-stagemethod,
ond,theiralgorithmreliesonthealgorithmicstabilityofsparse makingitabetterfeature-selectionmechanismforthisproblem.
estimators:theyneeddisjointpartitionsofthedatasettoagreeon Theaboveexperimentdemonstratesthatanintuitivemethod
whichfeaturesshouldbeincludedinthesupportset.Thismaybe likeSIScanoutperformthetwo-stagemethodonasimpledataset.
anunreasonableassumption,aseveninthenon-privatesetting, However,conditionsofreal-datasetsarerarelysoideal.Tosim-
sparseestimatorsarenotalgorithmicallystable[53].Combining ulatehowoutliersaffectthetwo-stageandSISmethods,weem-
theirrequirementforalgorithmicallystablesparsesupportselec- ploythefollowingexperiment.Wegenerate100datapointsX ∼
tionwithaneedforprivate(noisy)supportselectionmayrender N(0,I100) where each x𝑖 ∈ R100, like above. We then use two
thefinalsupportsetineffective.Thisheuristicanalysisisthereason weightvectors:w1= (cid:2)1 1 1 1 1 0 ··· 0(cid:3)⊤andw2=
whywechosetostudyaprocedurewhichdidnotrequirebuilding (cid:2)0 ··· 0 1 1 1 1 1(cid:3)⊤.w1 thesamevectordescribed
regressionestimatorsforsupportselection. inthepreviousexperiment,whereasw2hasvalue0incomponents
Othermethodsfordifferentiallyprivatehigh-dimensionalre- 1through95andvalue1incomponents96through100.Finally,
gressionexist,andtheytypicallyrelyonprivateoptimizationtech- usingthefirst90datapoints,wegeneratey1=X[1:90,:]w1+𝝐[1:90]
niques.Wedonotprovidedetailsontheseoptimizationtechniques and y2 = X[91:100,:]w2 +𝝐[91:100], where𝜖 𝑖 ∼ N(0,0.1). When
here,sincethisisout-of-scopeforthemethodsandexperiments constructingourfinaldataset,wechoosetorepeatedlyintersperseAISec’24,October18,2024,SaltLakeCity,UT RyanSwope,AmolKhanna,PhilipDoldo,SaptarshiRoy,andEdwardRaff
correlationbetweeneachfeatureandthetargetvariable.Thetop-𝑘
absolutelycorrelatedfeaturesareretained.Toprivatelyfindthetop-
𝑘highestabsolutelycorrelatedfeatureswithatargetvariable,we
needtocomputeprivatecorrelations.Todothis,weneedtobound
thesensitivityofthedotproductbetweenafeatureandthetarget
vector.Unlikethenon-privateSISalgorithm,whichcentersand
normalizeseachfeaturetoavarianceof1,weboundthesensitivity
bycenteringandnormalizingeachfeaturetoamaximumabsolute
valueof1.OnereasonwechoosetoprivatizeSISisbecauseitis
easytounderstandandcanbeeasilyadapted.
Inthefollowingsteps,X∈R𝑁×𝑑 isthedesignmatrix,withx(𝑖)
representingthe𝑖thcolumn(orfeature)ofX.y∈R𝑁 isthevector
oftargets.DP-SISworksasfollowsinAlgorithm1:
Algorithm1DP-SIS
Require:
DesignmatrixX∈R𝑁×𝑑,targetvectory∈R𝑁,privacy
parameter𝜖.
1: CentereachcolumninXandcentery.Thisensuresthateach
x⊤ yrepresentsacorrelation.
(𝑖)
2: ScaleeachcolumninXsothatthemaximumabsolutevalue
ofanyelementineachcolumnvectoris1.Scaleysothatthe
maximumabsolutevalueofanyelementinthevectoris1.This
ensuresthatthesensitivityof|x⊤ y|is1.
(𝑖)
3: Employaprivatetop-𝑘 selectionstrategywithparameter𝜖
giventhatthesensitivityof|x⊤ y|is1.The“score”or“quality”
(𝑖)
offeature𝑖 ismeasuredwiththeabsolutecorrelationofthe
featurewiththetarget,namely|x⊤ y|.Theprivatetop-𝑘algo-
(𝑖)
rithmreturns𝑘indices,correspondingtotheselectedfeatures.
Figure1:Occurrencesofselectingfeature𝑛inthetwo-stage
Thisalgorithmissimpletounderstandandimplement,butwe
and SIS algorithms. An ideal result for both experiments
believethatitmayoutperformKiferetal.’stwo-stageapproach
wouldselectfeatures1through51000timesandallother
sinceitemploysallthedataanddoesnotrelyonthealgorithmic
featureszerotimes.SISisclosertotheidealresultthantwo-
stabilityofsparseestimators.Inthefollowingsection,weprovide
stage.
atheoreticalunderstandingofthisalgorithm.
NotethatforDP-SIStoworkitisnecessarytouseahigh-quality
privatetop-𝑘selectionalgorithm.Weinitiallytestedmechanisms
9datapointsgeneratedfromw1with1datapointgeneratedfrom
in[35],[17],and[11],butfoundthattheyaddedtoomuchnoise
w2sothateachdisjointblockofthetwo-stagealgorithmreceives
toyieldfavorableresults.However,wefoundthatthecanonical
oneoutlier.
LipschtizalgorithmbyShekelyan&Loukidesdoeswork,thoughit
Thisexperimentwasrepeated1000times,andtheresultsare
isnoteasilyaccessibleinitsoriginalpresentation.Asasignificant
includedinFigure1asW1/W2.Itisclearthateveninthepresence
componentofourwork,were-derivedandformalizedthecanonical
oftheseoutliers,SISstillchoosesthenonzerocomponentscorre-
Lipschtizmechanism,presentingitinamoreaccessibleway.This
spondingw1 muchmorefrequentlythanthetwo-stagemethod.
explanationisplacedintheappendix,highlightingthatwearenot
Thisisdesirableifwebelievethatdatapointsgeneratedwithw2
claimingShekelyan&Loukidesinnovation,butwebelievethat
should be attributed to noise, and it demonstrates that SIS can
ourexplanationwasnecessaryandwillsupportotherworksinthe
outperformthetwo-stagemethodinthepresenceofsuchnoise.
future.
4 PRIVATIZINGSIS
4.2 TheoreticalAnalysis
Inthissection,wewilldetailourapproachtoprivatefeaturese-
Tobetterunderstandwhenthismethodworkswell,weprovidethe
lection.Wewillbeginbydescribingouralgorithmandgoonto
followinganalysis.WeseektoidentifytheprobabilitythatDP-SIS
provideatheoreticalanalysis.
identifiesthesamefeaturesasnon-privateSIS.
4.1 FeatureSelectionfromPrivateCorrelations Theorem4.1. LetXandybethedesignmatrixandtargetvector
GiventheperformanceofSISintheprevioussection,ouralgorithm after transformations in steps 1 and 2. Then |(X⊤y)𝑖| = |x⊤ (𝑖)y|.
employsaprivatizedversionofSIS.Asareminder,SISmeasuresthe Denote (cid:2) |X⊤y|(cid:3) tobethe𝑗thlargestelementin|X⊤y|.
𝑗FeatureSelectionfromDifferentiallyPrivateCorrelations AISec’24,October18,2024,SaltLakeCity,UT
Wearetryingtofindthetop-𝑘highestabsolutelycorrelatedfeatures Dataset n p R2
withthetargetvariable.Assume (cid:2) |X⊤y|(cid:3) −(cid:2) |X⊤y|(cid:3) =𝜉.Then
𝑘 𝑘+1 Alon 62 2000 0.4822
theprobabilitythatDP-SISexactlyidentifiesthetruetop-𝑘features
Borovecki 31 22283 0.9011
isatleast
Burczynski 127 22283 0.2175
(cid:26) (cid:18)𝑑(cid:19) 𝜉𝛾𝜖(cid:27)
1−exp 𝑘log
𝑘
+log(cid:0)𝑐 𝑑,𝑘(cid:1)−
2
, C Ch hi ia nretti 1 12 18
8
1 22 26 22 15
5
0 0. .2 57 80 75
9
where𝑐 𝑑,𝑘 = (cid:0)𝑑 𝑘(cid:1)/𝑑 𝑘𝑘𝑘 ≤ 𝑘 and𝛾 isahyperparameterofcanonical C Ch ho riw std ea nr sy en 1 20 14 7 2 12 42 18 33 0 0. .8 73 83 78 0
Lipschitzwhichmustbebetween0and1.
Golub 72 7129 0.6819
Proof. ThisstatementfollowsdirectlyfromtheoremsA.19and Gordon 181 12533 0.2432
A.20byShekelyan&Loukides[40].1Thistheoremprovidesahigh- Gravier 168 2905 0.0689
probabilityboundforthedifferencebetweenthesmallestprivately Khan 63 2308 0.6961
selectedelementandthetrue𝑘thlargestelement.Ifthisdifference Shipp 77 7129 0.4204
islessthan𝜉,weknowthatthesmallestprivatelyselectedelement Singh 102 12600 0.6449
isthetrue𝑘thlargestelement,andthusthesetsoverlapexactly. Sorlie 85 457 0.9300
Giventhis,wemustsolve Su 102 5564 0.9602
Subramanian 50 10100 0.5340
2 (cid:18) (cid:18)𝑑(cid:19) (cid:19)
𝛾𝜖
𝑘log
𝑘
−log𝛼+(cid:0)𝑐 𝑑,𝑘(cid:1) <𝜉 Tian 173 12625 0.0080
West 49 7129 0.7044
for𝛼.Rearrangingthisinequality,wefind Yeoh 248 12625 0.4611
𝛼 >exp(cid:26) 𝑘log(cid:18)𝑑 𝑘(cid:19) +log(cid:0)𝑐 𝑑,𝑘(cid:1)− 𝜉𝛾 2𝜖(cid:27) . T vaa tb ele L1 A: SS Su Om rm ega rr ey sso if ond sat was ie thts. 𝜆R =2 0a .1re
.
Tin hc elu Rd 2e id nf do icr an teon hp or wi-
TheoremA.20,weknowthatwithprobabilityatleast1−𝛼,the “sparselylinear”adatasetis.
smallestprivatelyselectedelementislessthan𝜉farfromthetrue
𝑘thlargestelement.Choosingthesmallest𝛼 fromtheinequality
aboveproducestheresult. □
Theweightvectorwwasgeneratedbyrandomlyselectingeight
Althoughitislikelydifficulttoidentifythevalueof𝜉privately, indicestoserveasthenon-zerovaluesintheweightvector.The
thisanalysisisusefultounderstandhowthehyperparametersof valueofanon-zeroindex𝑤 𝑖 isgivenbyEquation6.
DP-SISimpactitsperformance.As𝑘 and𝑑 increase,itbecomes
lesslikelyfortheDP-SIStoexactlyidentifythetop-𝑘scores.This 𝑤
𝑖
=(−1)𝑢 (𝑎+|𝑧|) (6)
makessense-as𝑘 and𝑑 increase,thealgorithmmustaddmore
where
noisewhichproducesahigherlikelihoodforrandomvariations
inselectedfeatures.Next,as𝜉,𝛾,and𝜖 increase,DP-SISwillbe 𝑢 ∼Bernoulli(0.4)
moreconsistentwiththenon-privatealgorithm.Thisalsomatches
𝑧∼N(0,1)
intuition-higher𝜉meansgreaterseparationbetweenthe𝑘-and
log𝑛
𝑘+1-thscoresandhigher𝜖meansweakerprivacyandlessnoise. 𝑎=4 √
Althoughouranalysisindicatesthathigher𝛾 wouldalsoproduce 𝑛
betterresults,thetop-𝑘simulationsdoneinShekelyan&Loukides Thesyntheticdatasetservedasausefulbenchmarkasitprovided
indicatesthatfortheaveragecase,𝛾 = 1 performsbetterthan amodelwithaknownsolutionandthereforeaknownsetoftop-k
2
𝛾 =1,soweuse𝛾 = 1 inourexperiments[40]. features.Fortherealdatasets,weusedusedAlon[1],Borovecki[3],
2
Burczynski[4],Chiaretti[5],Chin[6],Chowdary[7],Christensen
5 EXPERIMENTS [8],Golub[18],Gordon[19],Gravier[20],Khan[22],Shipp[41],
WeusedDP-SISandthetwo-stageapproachtoperformdifferen- Singh[42],Sorlie[43],Subramanian[44],Tian[46],West[51],and
tiallyprivatetop-𝑘 featureselectiononnineteencommonhigh- Yeoh[54]datasets.ThesearesummarizedinTable1.
dimensionaldatasetsandanadditionalsyntheticdataset.Thesyn- Threehyperparameterswereusedinourexperiments:𝜖,𝑘,and
theticdatawasgeneratedusingthetechniquedescribedinsection 𝜆.𝜖controlledtheprivacybudget:smallervaluesof𝜖corresponded
3.3.1of[12]with(𝑛,𝑑)=(100,2000).Weusetheusuallinearmodel, tohigherlevelsofprivacy.𝜖valuesbetween0.1and20weretested.
foundinEquation5. 𝑘correspondedtothenumberoffeaturestobeselected.Finally,𝜆
wasusedas𝐿1regularizertoperformnon-privateLASSOregres-
y=Xw+𝝐 siononthedatasets.Thisisnecessaryinthetwo-stageprocedure
X∼N(0,I𝑑) (5) butwealsousedthetop-𝑘componentsofthenon-privateLASSO
regressionasaproxyforthetruetop-𝑘features,sinceitiscompu-
𝝐 ∼N(0,1.5I𝑑)
tationallyinfeasibletoidentifytheoptimalfeaturestoa𝑘-sparse
1TheoremA.20oftheirpaperhasatypo:thepositionsof𝑥(cid:174)[𝑇]and𝑥(cid:174)[𝑘]shouldbe constrainedregressionproblemondatasetswithmanyfeatures.
switched.ThisisclearwhenreadingthederivationofTheoremsA.19andA.20. Inallpresentedresults,𝜆 wassetto0.1becausewhenrunningAISec’24,October18,2024,SaltLakeCity,UT RyanSwope,AmolKhanna,PhilipDoldo,SaptarshiRoy,andEdwardRaff
Top-5 Accuracy for Christensen Top-5 Accuracy for Sorlie
DP SIS DP SIS
0.6 two-stage 0.4 two-stage
0.5
0.3
0.4
0.3 0.2
0.2
0.1
0.1
0.0 0.0
10−1 100 101 10−1 100 101
Epsilon Epsilon
Top-7 Accuracy for Christensen Top-6 Accuracy for Sorlie
DP SIS DP SIS
0.5 two-stage 0.35 two-stage
0.30
0.4
0.25
0.3 0.20
0.15
0.2
0.10
0.1
0.05
0.0 0.00
10−1 100 101 10−1 100 101
Epsilon Epsilon
Figure2:Top-𝑘 accuracyofmodelsfitonfeaturesselected Figure3:Top-𝑘 accuracyofmodelsfitonfeaturesselected
fromontheChristensendataset.DP-SISoutperformsthe fromontheSorliedataset.DP-SISoutperformsthetwo-stage
two-stagemechanismon𝜖valuesbetween100 and101
,which
mechanismon𝜖valuesgreaterthan100
,whicharecommonly
arecommonlyusedforprivatecomputation[31]. usedforprivatecomputation[31].
experimentswithdifferent𝜆valueswefoundlittledifferencein
results.NotethattheR2scoresforthenon-privateLASSOarelisted
inTable1. Figure2,Figure3,Figure4,andFigure5displaytheperformance
Forthesyntheticdatasetweselected𝑘 ∈ {5,8},astherewere of DP-SIS and two-stage for the Christensen, Sorlie, Yeoh, and
only8significantfeaturesinthissyntheticdataset.Fortheother syntheticdatasets,respectively.Itisexpectedthatforsmallvalues
datasets,wetested𝑘 ∈ {5,⌊log(𝑑)⌋}.Wechosetouse5sincein of𝜖like0.1,methodswillperformpoorlyduetothedifficultyof
manycasesdataanalystsseektoidentifyasmallsubsetofimportant featureselection,andDPingeneral,underhighprivacyconstraints.
featuresinadataset,andweusedlog(𝑑)sincetheoreticalliterature Inallplots,wecanseethatatverysmallvaluesof𝜖,bothDP-SIS
onfeatureselectionoftenfocusesoneffectiverecoveryoflog(𝑑) andtwo-stagehavepooraccuracyasexpected.
features. However,oneachdataset,DP-SISoutperformsthetwo-stage
Foreach(𝑘,𝜖)pairweperformedprivatefeatureselectionus- mechanism for a range of intermediate𝜖 values. This range of
ingtheDP-SISmechanismandthetwo-stagetechniqueforone intermediatevaluesisinlinewithreal-world𝜖usage,indicating
hundredtrialsforeachdataset.Toscoreprivatefeatureselection, thatourmethodisin-lineandpotentiallyusefulforpracticalmodel
wecomparedthenumberofcorrectlychosentop-𝑘featuresto𝑘, building[31].ThisindicatesthatDP-SISisamoreusefulmethod
producinganaccuracyscore.Experimentswererunonanhigh- thanthetwo-stageapproachsinceitcanselectcorrectfeaturesat
performancecomputingclusterparallelizedacrossseveralnodesat lower𝜖valuesthanthetwo-stagemechanism,meaningthatitcan
atimeusingtheSLURMHPCresourcemanager[55].Eachnode produceusefulresultswhilemaintainingtheprivacyofindividuals
had40CPUsand512GBofRAM. inthedatasets.
ycaruccA
k-poT
ycaruccA
k-poT
ycaruccA
k-poT
ycaruccA
k-poTFeatureSelectionfromDifferentiallyPrivateCorrelations AISec’24,October18,2024,SaltLakeCity,UT
Top-5 Accuracy for Yeoh Top-5 Accuracy for Synth
0.5 DP SIS DP SIS
two-stage 1.0 two-stage
0.4
0.8
0.3
0.6
0.2
0.4
0.1 0.2
0.0 0.0
10−1 100 101 10−1 100 101
Epsilon Epsilon
Top-9 Accuracy for Yeoh Top-8 Accuracy for Synth
0.35 DP SIS 0.8 DP SIS
two-stage two-stage
0.30 0.7
0.6
0.25
0.5
0.20
0.4
0.15
0.3
0.10
0.2
0.05 0.1
0.00 0.0
10−1 100 101 10−1 100 101
Epsilon Epsilon
Figure4:Top-𝑘 accuracyofmodelsfitonfeaturesselected Figure5:Top-𝑘 accuracyofmodelsfitonfeaturesselected
fromontheYeohdataset.DP-SISoutperformsthetwo-stage fromontheSynthdataset.DP-SISoutperformsthetwo-stage
mechanismon𝜖 valuesgreaterthan2×100 ,whichcanbe mechanismon𝜖valuesgreaterthan101 .Althoughsuchhigh
usedinprivatecomputation[31]. 𝜖valuesaretypicallynotusedforprivatecomputation,this
resultstilldemonstratesthatDP-SIShasbetterresultsthan
thetwo-stagebaseline.
Figure6andFigure7showthatthistrendholdsformoredatasets.
Theseheatmapsshowthatthetwo-stagemethoddoesnotsignifi-
cantlyoutperformDP-SISonanydataset,andformanydatasets, Wedemonstratethiswhenexaminingthe𝑇𝑂𝑃,𝐺𝑅𝐸𝐴𝑇,and𝐺𝑂𝑂𝐷
DP-SIShasmuchbetteraccuracythanthetwo-stagemethodfor performanceofDP-SIS.Thesemetricsaredefinedin[40]as:
some𝜖values.However,somedatasetslikeAlonandBoroveckido (1) 𝑇𝑂𝑃:DP-SISselectsall𝑘ofthetruetop-korderstatistics
notshowadifferencebetweenDP-SISandtwo-stageforany𝜖value. (2) 𝐺𝑅𝐸𝐴𝑇:DP-SISselectsallofthetop 𝑘 orderstatisticsand
10
Thisislikelybecausethesedatasetsarehigherdimensionaland therestcomefromthetop 11𝑘
havelessstableselectedfeaturesthanotherdatasets.Indeed,we 10
(3) 𝐺𝑅𝐸𝐴𝑇:DP-SISselectsallofthetop 𝑘 orderstatisticsand
foundthatDP-SISoutperformsthetwo-stagemethodtoagreater 100
therestcomefromthetop 3𝑘
degreewhenthereisastrongerlinearrelationshipbetweenthe 2
featuresandthetarget.Forexample,DP-SISoutperformstwo-stage Forexample,if𝑘 =200,DP-SISperforms𝐺𝑅𝐸𝐴𝑇 ifitselectsallof
around𝜖 =5ontheChristensendatasetbutnotonAlondespitethe thetop20truetop-korderstatistics,andtheremaining180features
factthattheyhavecomparablesizes((217,1413)forChristensen areselectedfromthetop220.
and(62,2000)forAlon).However,linearregressionachievesan𝑅2 DP-SISonlyusestheorderstatisticsduringitstop-𝑘selection
of0.7870onChristensen,butonly0.4822forAlon. process,anditcanperformwellinselecting𝐺𝑅𝐸𝐴𝑇 and𝐺𝑂𝑂𝐷
Wealsonotethatevenwhentheaccuracyofselectedfeaturesis featuresonhigh-dimensionaldatasets.Figure8givesanexample
poor,DP-SISachievesahighlevelofaccuracyontheorderstatistics ofthisontheChindataset.Whileitstop-10accuracyisaround20%
themselvessinceitisbuiltonthecanonicalLipschitzmechanism. at𝜖 = 20,thefiguredemonstratesthatitperformsverywellon
ycaruccA
k-poT
ycaruccA
k-poT
ycaruccA
k-poT
ycaruccA
k-poTAISec’24,October18,2024,SaltLakeCity,UT RyanSwope,AmolKhanna,PhilipDoldo,SaptarshiRoy,andEdwardRaff
DP-SIS - two-stage Top-5 Accuracy Difference DP-SIS Metrics for Chin with k=10
alon 1.0 Top
christensen
khan Great
gravier 0.8 Good
golub 0.8
sorlie
shipp
0.6
borovecki
subramanian 0.6
chiaretti
singh
0.4
west 0.4
synth
gordon
chowdary
0.2
chin 0.2
tian
burczynski
yeoh 0.0 0.0
10−1 100 101
Epsilon Epsilon
Figure6:DifferencebetweenDP-SISandtwo-stagetop-𝑘for Figure8:PerformanceofDP-SISon𝑇𝑂𝑃,𝐺𝑅𝐸𝐴𝑇,and𝐺𝑂𝑂𝐷
𝑘 =5.Onnodatasetand𝜖valuedoestwo-stagesignificantly metrics.ThisgraphdemonstratesthattheDP-SISselection
outperform DP-SIS, but DP-SIS significantly outperforms procedureisaccuratewithrespecttoorderstatistics,mean-
two-stageonrangesof𝜖valuestypicallygreaterthan1.0. ingthatifabetterrepresentationoffeatureimportantthan
correlationcanbeengineered,theprivateselectionproce-
durecanbeappliedwithexpectationsofgoodperformance.
DP-SIS - two-stage Top-log(d) Accuracy Difference
alon 0.7 √
christensen 𝑁 estimatorsondisjointpartitionsofadatasettoidentifythe
khan 0.6 mostcommonlyselectedfeaturesintheseestimators.Whilethisis
gravier
golub computationallyfeasible,itisstilldifficultsinceitrequiresbuild-
sorlie 0.5 ingmanyhigh-dimensionalestimators.Additionally,itrequires
shipp
borovecki high-dimensionalsparseregressionestimatorstobealgorithmically
0.4
subramanian stable,whichtheyarenot.
chiaretti
singh 0.3 SISisanalternativemethodforhigh-dimensionalfeatureselec-
west
tion.Itselectsthe𝑘 featureswhicharemostcorrelatedwiththe
synth
gordon 0.2 targetvector.BymakingSISprivatewithadifferentiallyprivate
chowdary
top-𝑘selector,wecandevelopadifferentiallyprivatefeatureselec-
chin
0.1
tian torbasedonSIS.WechoosetouseDP-SISbasedonitssuperior
burczynski
performance.
yeoh 0.0
SISoutperformsthetwo-stagemethodonmostdatasetsinrea-
sonablerangesof𝜖.Additionally,DP-SIScanbeusedwithany
Epsilon
featureselectionmetric,makingitflexibletoimprovedmetricsfor
Figure7:DifferencebetweenDP-SISandtwo-stagetop-𝑘ac- featureselectiondevelopedinthefuture.
curacyfor𝑘 =log𝑑.Onnodatasetand𝜖valuedoestwo-stage Finally,weendwithafinalcomment.Thispaperexploresem-
ployingasimplemetricforfeatureselectionindifferentialprivacy,
significantlyoutperformDP-SIS,butDP-SISsignificantlyout-
performstwo-stageonrangesof𝜖 valuestypicallygreater andpitsitagainstamorecomplicatedmechanism.Thesimpler
metricworksbetterdespiteitnotbeingabletoidentifycasesin
than1.0.
whichsubsetsoffeaturesareindividuallyweaklycorrelatedwith
thetargetbutjointlystronglycorrelatedwiththetarget.Thisis
the𝑇𝑂𝑃,𝐺𝑅𝐸𝐴𝑇,and𝐺𝑂𝑂𝐷metrics.Thisindicatesthatthelower becausewhenusingdifferentialprivacy,thereisaconstanttug-
accuracyisafunctionofthecorrelationmetricusedinDP-SIS,and of-warbetweenemployingmoreexpressivemethodswithmore
ifanothermetricwhichbetterpredictsfeatureimportanceisused, noiseandsimplermethodswithlessnoise.Inthecasepresentedin
DP-SIScanbeusedeffectivelyforprivatetop-𝑘selection. thispaper,featureselectionhadhigheraccuracywhenasimpler
methodwasselectedwhichwasmorestableandrequiredlessnoise.
6 CONCLUSION
Thispaperseekstoimproveuponthestate-of-the-artincompu- REFERENCES
tationallyfeasibledifferentiallyprivatefeatureselectionforhigh- [1] U.Alon,N.Barkai,D.A.Notterman,K.Gish,S.Ybarra,D.Mack,andA.J.Levine.
dimensionallinearregression.Weidentifythatthecurrentcomputa- 1999.Broadpatternsofgeneexpressionrevealedbyclusteringanalysisoftumor
andnormalcolontissuesprobedbyoligonucleotidearrays.Proceedingsofthe
tionallyfeasiblemethod,thetwo-stageapproach,requiresbuilding NationalAcademyofSciences96,12(1999),6745–6750.
tesataD
tesataD
1.0
1.0
21.0
21.0
61.0
61.0
91.0
91.0
42.0
42.0
3.0
3.0
83.0
83.0
74.0
74.0
85.0
85.0
37.0
37.0
19.0
19.0
0.1
0.1
31.1
31.1
14.1
14.1
67.1
67.1
2.2
2.2
47.2
47.2
24.3
24.3
62.4
62.4
23.5
23.5
36.6
36.6
72.8
72.8
0.01
0.01
13.01
13.01
68.21
68.21
40.61
40.61
0.02
0.02
dooGFeatureSelectionfromDifferentiallyPrivateCorrelations AISec’24,October18,2024,SaltLakeCity,UT
[2] GiorgosBorboudakisandIoannisTsamardinos.2019.Forward-BackwardSelec- [21] TrevorHastie,RobertTibshirani,andRyanTibshirani.2020.BestSubset,Forward
tionwithEarlyDropping. JournalofMachineLearningResearch20,8(2019), StepwiseorLasso?AnalysisandRecommendationsBasedonExtensiveCompar-
1–39. http://jmlr.org/papers/v20/17-334.html isons.Statist.Sci.35,4(Nov.2020),579–592. https://doi.org/10.1214/19-STS733
[3] F.Borovecki,L.Lovrecic,J.Zhou,H.Jeong,F.Then,H.D.Rosas,S.M.Hersch,P. Publisher:InstituteofMathematicalStatistics.
Hogarth,B.Bouzou,R.V.Jensen,andD.Krainc.2005.Genome-wideexpression [22] J.Khan,J.S.Wei,M.Ringner,L.H.Saal,M.Ladanyi,F.Westermann,F.Berthold,
profilingofhumanbloodrevealsbiomarkersforHuntington’sdisease.Proceedings M.Schwab,C.R.Antonescu,C.Peterson,andP.S.Meltzer.2001.Classification
oftheNationalAcademyofSciences102,31(2005),11023–11028. anddiagnosticpredictionofcancersusinggeneexpressionprofilingandartificial
[4] M.E.Burczynski,R.L.Peterson,N.C.Twine,K.A.Zuberek,B.J.Brodeur,L.Cas- neuralnetworks.NatureMedicine7,6(2001),673–679.
ciotti,V.Maganti,H.M.Sackett,N.Novoradovskaya,B.Cook,P.S.Reddy,A.Strahs, [23] AmolKhanna,FredLu,andEdwardRaff.2023.Thechallengeofdifferentially
M.L.Clawson,R.M.Goldschmidt,S.Chaturvedi,A.M.Slager,L.A.Marshall,and privatescreeningrules.arXivpreprintarXiv:2303.10303(2023).
B.Renault.2006.MolecularclassificationofCrohn’sdiseaseandulcerativecolitis [24] AmolKhanna,FredLu,EdwardRaff,andBrianTesta.2023.DifferentiallyPri-
patientsusingtranscriptionalprofilesinperipheralbloodmononuclearcells.The vateLogisticRegressionwithSparseSolutions.InProceedingsofthe16thACM
JournalofMolecularDiagnostics8,1(2006),51–61. WorkshoponArtificialIntelligenceandSecurity.1–9.
[5] S.Chiaretti,X.Li,R.Gentleman,A.Vitale,M.Vignetti,F.Mandelli,J.Ritz,andR. [25] AmolKhanna,EdwardRaff,andNathanInkawhich.2024. SoK:AReviewof
Foa.2004.GeneexpressionprofileofadultT-cellacutelymphocyticleukemia DifferentiallyPrivateLinearModelsForHigh-DimensionalData.In2024IEEE
identifiesdistinctsubsetsofpatientswithdifferentresponsetotherapyand ConferenceonSecureandTrustworthyMachineLearning(SaTML).IEEE,57–77.
survival.Blood103,7(2004),2771–2778. [26] DanielKifer,AdamSmith,andAbhradeepThakurta.2012.Privateconvexempiri-
[6] KoeiChin,SanjeevDeVries,JaneFridlyand,PaulT.Spellman,RajikaRoydasgupta, calriskminimizationandhigh-dimensionalregression.InConferenceonLearning
Wen-LinKuo,AnnaLapuk,RichardM.Neve,ZhenQian,TimRyder,NoraBayani, Theory.JMLRWorkshopandConferenceProceedings,25–1.
JonathanBrock,KimberlyMontgomery,DavidGinzinger,DanSeah,WeiKuo, [27] JohanLarsson.2021.Look-AheadScreeningRulesfortheLasso.(2021). http:
JeffreyL.Stilwell,DanielPinkel,DonnaG.Albertson,FredericM.Waldman, //arxiv.org/abs/2105.05648arXiv:2105.05648.
AnilN.Jain,ColinCollins,andJoeW.Gray.2006.Genomicandtranscriptional [28] DongCLiuandJorgeNocedal.1989. OnthelimitedmemoryBFGSmethod
aberrationslinkedtobreastcancerpathophysiologies.CancerCell10,6(2006), forlargescaleoptimization.MathematicalProgramming45,1(1989),503–528.
529–541. https://doi.org/10.1007/BF01589116
[7] D.R.Chowdary,J.Lathrop,J.Skelton,K.Curtin,T.Briggs,L.Zhang,A.Rashidi,S. [29] EugeneNdiaye,OlivierFercoq,Alex,ReGramfort,andJosephSalmon.2017.
White,D.Curtis,D.D.VonHoff,andD.Kravitz.2006.Prognosticgeneexpression GapSafeScreeningRulesforSparsityEnforcingPenalties.JournalofMachine
signaturescanbemeasuredintissuescollectedinRNAlaterpreservative.The LearningResearch18,128(2017),1–33. http://jmlr.org/papers/v18/16-577.html
JournalofMolecularDiagnostics8,1(2006),31–39. [30] JosephPNearandChikéAbuah.2021.Programmingdifferentialprivacy.URL:
[8] BrockCChristensen,EAndresHouseman,CarmenJMarsit,ShichunZheng, https://uvm(2021).
MargaretRWrensch,JosephLWiemels,HeatherHNelson,MargaretRKaragas, [31] JosephPNear,DavidDarais,NaomiLefkovitz,GaryHowarth,etal.2023.Guide-
JamesFPadbury,RaphaelBueno,DavidJSugarbaker,Ru-FangYeh,JohnK linesforEvaluatingDifferentialPrivacyGuarantees.TechnicalReport.National
Wiencke,andKarlTKelsey.2009.AgingandEnvironmentalExposuresAlter InstituteofStandardsandTechnology.
Tissue-SpecificDNAMethylationDependentuponCpGIslandContext.PLOS [32] AndrewY.Ng.2004.Featureselection,L1vs.L2regularization,androtational
Genetics5,8(Aug.2009),e1000602. invariance.Twenty-firstinternationalconferenceonMachinelearning-ICML’04
[9] DLDonohoandMElad.2003.MaximalsparsityrepresentationviaL1minimiza- (2004),78. https://doi.org/10.1145/1015330.1015435Publisher:ACMPressPlace:
tion.ProceedingsofNationalAcademyofSciences100(2003),2197–2202. NewYork,NewYork,USAISBN:1581138285.
[10] DavidLDonoho,XiaomingHuo,etal.2001.Uncertaintyprinciplesandideal [33] KoheiOgawa,YoshikiSuzuki,andIchiroTakeuchi.2013.Safescreeningofnon-
atomicdecomposition. IEEEtransactionsoninformationtheory47,7(2001), supportvectorsinpathwiseSVMcomputation.ICML(2013). http://jmlr.org/
2845–2862. proceedings/papers/v28/ogawa13b.html
[11] DavidDurfeeandRyanMRogers.2019. Practicaldifferentiallyprivatetop-k [34] FPedregosa,GVaroquaux,AGramfort,VMichel,BThirion,OGrisel,MBlondel,
selectionwithpay-what-you-getcomposition.AdvancesinNeuralInformation PPrettenhofer,RWeiss,VDubourg,JVanderplas,APassos,DCournapeau,
ProcessingSystems32(2019). MBrucher,MPerrot,andEDuchesnay.2011.Scikit-learn:MachineLearning
[12] JianqingFanandJinchiLv.2006.Sureindependencescreeningforultrahighdi- inPython. JournalofMachineLearningResearch12(2011),2825–2830. http:
mensionalfeaturespace.JournaloftheRoyalStatisticalSociety:SeriesB(Statistical //jmlr.csail.mit.edu/papers/v12/pedregosa11a.html
Methodology)70(2006). https://api.semanticscholar.org/CorpusID:5001358 [35] GangQiao,WeijieSu,andLiZhang.2021.Oneshotdifferentiallyprivatetop-k
[13] JianqingFanandJinchiLv.2008.Sureindependencescreeningforultrahighdi- selection.InInternationalConferenceonMachineLearning.PMLR,8672–8681.
mensionalfeaturespace.JournaloftheRoyalStatisticalSocietySeriesB:Statistical [36] EdwardRaff,AmolKhanna,andFredLu.2024. ScalingUpDifferentiallyPri-
Methodology70,5(2008),849–911. vateLASSORegularizedLogisticRegressionviaFasterFrank-WolfeIterations.
[14] Rong-EnFan,Kai-WeiChang,Cho-JuiHsieh,Xiang-RuiWang,andChih-JenLin. AdvancesinNeuralInformationProcessingSystems36(2024).
2008.LIBLINEAR:ALibraryforLargeLinearClassification.J.Mach.Learn.Res. [37] AlainRakotomamonjy,GillesGasso,andJosephSalmon.2019. Screening
9(jun2008),1871–1874. rulesforLassowithnon-convexSparseRegularizers.InProceedingsofthe
[15] EmanueleFrandi,RicardoÑanculef,StefanoLodi,ClaudioSartori,andJohan 36thInternationalConferenceonMachineLearning.PMLR,5341–5350. https:
A.K.Suykens.2016.FastandscalableLassoviastochasticFrank–Wolfemethods //proceedings.mlr.press/v97/rakotomamonjy19a.htmlISSN:2640-3498.
withaconvergenceguarantee.MachineLearning104,2(Sept.2016),195–221. [38] BrianC.Ross.2014.Mutualinformationbetweendiscreteandcontinuousdata
https://doi.org/10.1007/s10994-016-5578-4 sets.PLoSONE9,2(2014). https://doi.org/10.1371/journal.pone.0087357
[16] JeromeFriedman,TrevorHastie,andRobTibshirani.2010.RegularizationPaths [39] SaptarshiRoyandAmbujTewari.2023.OntheComputationalComplexityof
forGeneralizedLinearModelsviaCoordinateDescent. JournalofStatistical PrivateHigh-dimensionalModelSelectionviatheExponentialMechanism.arXiv
Software33,1(2010),1–22. arXiv:1501.0228ISBN:9781439811870. preprintarXiv:2310.07852(2023).
[17] JenniferGillenwater,MatthewJoseph,AndresMunoz,andMonicaRiberoDiaz. [40] MichaelShekelyanandGrigoriosLoukides.2022.DifferentiallyPrivateTop-k
2022.AJointExponentialMechanismForDifferentiallyPrivateTop-𝑘.InInter- SelectionviaCanonicalLipschitzMechanism.arXivpreprintarXiv:2201.13376
nationalConferenceonMachineLearning.PMLR,7570–7582. (2022).
[18] ToddR.Golub,DonnaK.Slonim,PabloTamayo,ChristineHuard,Michelle [41] MargaretA.Shipp,KennethN.Ross,PabloTamayo,AlexP.Weng,JefferyL.
Gaasenbeek,JillP.Mesirov,HilaryColler,MignonL.Loh,JamesR.Downing, Kutok,RicardoC.T.Aguiar,MichelleGaasenbeek,MariaAngelo,MargaretReich,
MichaelA.Caligiuri,ClaraD.Bloomfield,andEricS.Lander.1999.Molecular GeraldineS.Pinkus,ThomasS.Ray,MichaelA.Koval,KennethW.Last,Andrew
classificationofcancer:classdiscoveryandclasspredictionbygeneexpression Norton,T.AndrewLister,JillMesirov,DonnaS.Neuberg,EricS.Lander,JonC.
monitoring.Science286,5439(1999),531–537. Aster,andToddR.Golub.2002.DiffuselargeB-celllymphomaoutcomeprediction
[19] G.J.Gordon,R.V.Jensen,L.L.Hsiao,S.R.Gullans,J.E.Blumenstock,S.Ramaswamy, bygene-expressionprofilingandsupervisedmachinelearning.NatureMedicine
W.G.Richards,D.J.Sugarbaker,andR.Bueno.2002.Translationofmicroarray 8,1(2002),68–74.
dataintoclinicallyrelevantcancerdiagnostictestsusinggeneexpressionratios [42] DineshSingh,PhillipG.Febbo,KennethRoss,DouglasG.Jackson,JudithManola,
inlungcancerandmesothelioma.CancerResearch62,17(2002),4963–4967. CarolLadd,PabloTamayo,AndrewA.Renshaw,AnthonyV.D’Amico,JeromeP.
[20] Gravier,Eleonore,GaellePierron,AnneVincent-Salomon,Nadegegruel,Virginie Richie,EricS.Lander,MassimoLoda,PhilipW.Kantoff,ToddR.Golub,and
Raynal,AlexiaSavignoni,YannDeRycke,Jean-YvesPierga,CarloLucchesi, WilliamR.Sellers.2002.Geneexpressioncorrelatesofclinicalprostatecancer
FabienReyal,AlainFourquet,SergioRoman-Roman,FrancoisRadvanyi,Xavier behavior.CancerCell1,2(2002),203–209.
Sastre-Garau,BernardAsselain,andOlivierDelattre.2010.AprognosticDNA [43] ThereseSørlie,CharlesMPerou,RobertTibshirani,TuridAas,StephanieGeisler,
signatureforT1T2node-negativebreastcancerpatients.Genes,Chromosomes HildeJohnsen,TrevorHastie,MichaelBEisen,MattvandeRijn,StefanieSJeffrey,
andCancer49,12(Sept.2010),1125–1125. ThorThorsen,HanneQuist,JohnCMatese,PatrickOBrown,DavidBotstein,
PerEysteinLønning,andAnne-LiseBørresen-Dale.2001.Geneexpressionpat-
ternsofbreastcarcinomasdistinguishtumorsubclasseswithclinicalimplications.AISec’24,October18,2024,SaltLakeCity,UT RyanSwope,AmolKhanna,PhilipDoldo,SaptarshiRoy,andEdwardRaff
ProceedingsoftheNationalAcademyofSciences98(Sept.2001),10869–10874. suchthattheselectedsetis“close”tothetruesetof𝑘largestscores,
[44] AravindSubramanian,PabloTamayo,VamsiK.Mootha,SayanMukherjee,Ben- insomesense,withreasonablyhighprobability.
jaminL.Ebert,MichaelA.Gillette,AmandaPaulovich,ScottL.Pomeroy,ToddR.
ThecanonicalLipschitzmechanismaimstosolvethisproblemby
Golub,EricS.Lander,andJillP.Mesirov.2005.Genesetenrichmentanalysis:
Aknowledge-basedapproachforinterpretinggenome-wideexpressionprofiles. consideringall(cid:0)𝑑(cid:1)possible𝑘-subsetsofthescoreindices{1,...,𝑑},
ProceedingsoftheNationalAcademyofSciences102,43(2005),15545–15550. assigningavalue𝑘
toeachsubset,andreturningthesubsetwiththe
[45] AbhradeepGuhaThakurtaandAdamSmith.2013.Differentiallyprivatefeature
selectionviastabilityarguments,andtherobustnessofthelasso.InConference largestvalue.Thevalueassignedtoa𝑘-subsetisthesumoftwo
onLearningTheory.PMLR,819–850. terms:adeterministicutilityterm(whichisanegatedlossterm)and
[46] EugeneTian,JamesR.Sawyer,AmeliaH.Ligon,AnandS.Lagoo,SherrylL.
arandomizednoiseterm.Theutilityandnoisearescaledinspecific
Hubbard,KathyL.Myers,SusanG.Hilsenbeck,RonaldJ.Berenson,DavidO.
Dixon,JenniferR.Sawyer,BartBarlogie,andJohnD.Shaughnessy.2003.High- wayssuchthat𝜖-differentialprivacyisachieved.Theintuitionis
resolutionfluorescenceinsituhybridizationmappingofrecurrentbreakpoint thattheutilitytermwillbelarger(i.e.lessnegative)forindexsets
regionsinmultiplemyelomatranslocations.CancerResearch63,2(2003),532–
539. thatare“closer”tothetruetop-𝑘indexsetandeachnoisetermis
[47] RobertTibshirani.1994. RegressionShrinkageandSelectionViatheLasso. largeenoughsuchthatdifferentialprivacyismaintained.
JournaloftheRoyalStatisticalSociety,SeriesB58,1(1994),267–288.
[48] MartinJWainwright.2019.High-dimensionalstatistics:Anon-asymptoticview-
point.Vol.48.Cambridgeuniversitypress. A.2 Top-𝑘
[49] ChiWangandBailuDing.2019.FastApproximationofEmpiricalEntropyviaSub-
sampling.In25THACMSIGKDDCONFERENCEONKNOWLEDGEDISCOVERY Supposewehaveadataset𝑥ˆ ∈ Xandascorefunction 𝑓 : X →
ANDDATAMINING. R𝑑 whereeachcomponentofthescorefunctionhassensitivity
[50] JieWang,JiayuZhou,PeterWonka,andJiepingYe.2013. LassoScreening
RulesviaDualPolytopeProjection.InAdvancesinNeuralInformationProcessing Δ 𝑓 meaningthat |𝑓 𝑖(𝑥ˆ 1) − 𝑓 𝑖(𝑥ˆ 2)| ≤ Δ 𝑓 for𝑖 ∈ {1,...,𝑑} forall
Systems,Vol.26.CurranAssociates,Inc. https://papers.nips.cc/paper/2013/hash/ 𝑥ˆ 1,𝑥ˆ 2 ∈ Xthatdifferbyexactlyoneindividual.Let𝑥 ∈ R𝑑 bea
8b16ebc056e613024c057be590b542eb-Abstract.html
[51] MikeWest,ChristianBlanchette,HollyDressman,ErichHuang,SueIshida,
vectorofnormalizedscoressothat𝑥
𝑖
=𝑓 𝑖(𝑥ˆ)/Δ
𝑓
for𝑖 ∈{1,...,𝑑}.
RainerSpang,HannahZuzan,JosephA.Olson,JeffreyR.Marks,andJosephR. Thescorevector𝑥 hasdescendingorderstatistics𝑥 [1] ≥...≥𝑥 [𝑑]
Nevins.2001.Predictingtheclinicalstatusofhumanbreastcancerbyusinggene and,justasin[40],welet𝑗1,...,𝑗
𝑑
∈{1,...,𝑑}betheindicessuch
1ex 14p 6re 2s –s 1io 1n 46p 7r .ofiles.ProceedingsoftheNationalAcademyofSciences98,20(2001),
that𝑥
𝑗1
=𝑥 [1],...,𝑥
𝑗𝑑
=𝑥 [𝑑].Theindexset{𝑗1,...,𝑗 𝑘}isthetrue
[52] ZhenJamesXiang,YunWang,andPeterJRamadge.2016.Screeningtestsfor top-𝑘indexset.
lassoproblems.IEEEtransactionsonpatternanalysisandmachineintelligence39, LetYbethesetofallsize-𝑘subsetsof{1,...,𝑑}.Thisisthesetof
5(2016),1008–1027.
[53] HuanXu,ConstantineCaramanis,andShieMannor.2011.Sparsealgorithmsare size-𝑘indexsetsandultimatelywewillusethecanonicalLipschitz
notstable:Ano-free-lunchtheorem.IEEEtransactionsonpatternanalysisand mechanismtorandomlyselectanindexsetthatapproximatesthe
machineintelligence34,1(2011),187–193.
top-𝑘indexsetbyassigningavaluetoevery𝑦 ∈Y.Wenotethat
[54] Eng-JuhYeoh,MeganE.Ross,SheilaA.Shurtleff,WilliamK.Williams,Divya
Patel,RamyMahfouz,FrederickG.Behm,SusanaC.Raimondi,MaryV.Relling, the“canonical”Lipschitzmechanismisaspecialcaseofthemore
AshaPatel,ChengCheng,DarioCampana,DavidWilkins,XiaoweiZhou,Jia generalLipschitzmechanismthatusesaspecific“canonical“loss
Li,HanLiu,Ching-HonPui,WilliamE.Evans,CliffordNaeve,LawrenceWong,
function.BelowwedescribetheLipschitzmechanismfortop-𝑘
andJamesR.Downing.2002.Classification,subtypediscovery,andpredictionof
outcomeinpediatricacutelymphoblasticleukemiabygeneexpressionprofiling. selection,butadditionallynotethattheLipschitzmechanismfor
CancerCell1,2(2002),133–143. top-𝑘isaspecialcaseofthegeneralLipschitzmechanismdescribed
[55] AndyBYoo,MorrisAJette,andMarkGrondona.2003. Slurm:Simplelinux
utilityforresourcemanagement.InWorkshoponjobschedulingstrategiesfor in[40]whichcontainsanadditionalparameter𝜅(nottobeconfused
parallelprocessing.Springer,44–60. with𝑘)andreturnsthetop-𝜅 highestvaluedobjects.Inthecase
[56] Guo-xunYuan,Chia-HuaHo,andChih-jenLin.2012.AnimprovedGLMNET
oftop-𝑘 selection,weuse𝜅 =1aseachobjectweareconcerned
forL1-regularizedlogisticregression.JournalofMachineLearningResearch13
(2012),1999–2030. https://doi.org/10.1145/2020408.2020421 Publisher:ACM withisasize-𝑘 indexset.With𝜅 > 1,theLipschitzmechanism
PressPlace:NewYork,NewYork,USAISBN:9781450308137. would(abitconfusingly)returnthetop-𝜅size-𝑘subsetsthathave
thehighestvalue(inotherwords,thetop-𝜅differentiallyprivate
approximationsofthetop-𝑘indexset).
A APPENDIX
A.1 CanonicalLipschitzMechanismforTop-𝑘 DefinitionA.1(LipschitzMechanism,𝜅 = 1). Let𝐹 beacumu-
lativedistribituionfunctionsuchthatlog(1−𝐹(𝑥))is1-Lipschitz
Selection
continuousandlet𝐹−1bethecorrespondinginversecumulative
TheoriginalpaperintroducingthecanonicalLipschitzmechanism
distributionfunction.LetYbethedomainthemechanismselects
[40]hasreceivedlittleattentiondespiteitsstrongresults.Webelieve
itsoutputfromandlet𝑈 𝑦 ∼ 𝑈𝑛𝑖𝑓(0,1) foreach𝑦 ∈ Ybei.i.d.
thatthisisinpartduetothepaper’spresentation.Inthisappendix,
Let𝑥ˆ ∈ Xbeadatasetand𝑓 :X→R𝑑 beascorefunctionwith
wewillattempttoreviewandclarifythemainideasbehindusing
component-wisesensitivityΔ sothatwehavethe(normalized)
thecanonicalLipschitzmechanismfor𝜖-differentiallyprivatetop-𝑘 𝑓
selection. scorevector𝑥 ∈R𝑑 suchthat𝑥 𝑖 = 𝑓 𝑖(𝑥ˆ)/Δ 𝑓 for𝑖 ∈ {1,...,𝑑}.Let
Supposewehaveadatasetcontaininginformationaboutsome LOSS(·|·):Y×R𝑑 →R ≥0bealossfunctionwithsensitivityΔ LOSS.
individuals.Forexample,intheregressioncontextwecanimagine Let𝜖 >0betheprivacylossparameter.TheoutputoftheLipschitz
thatourdesignmatrix𝑋 ∈R𝑁×𝑑 isadatasetcontaininginforma- mechanismis
tionabout𝑁 individuals,witheachrowcorrespondingtoasingle (cid:18) 𝜖 (cid:19)
individual.Givenadataset𝑥ˆ ∈Xwecandefineascoringfunction 𝑌 =argmax −
2Δ
LOSS(𝑦|𝑥)+𝐹−1 (𝑈 𝑦) .
𝑓 :X→R𝑑 whichreturnsavectorof𝑑scoresgivensomedataset 𝑦∈Y LOSS
inX.Thetop-𝑘problemisconcernedwithselectingasetof𝑘 ∈Z+ Inthetop-𝑘 setting,theselectiondomainYwillbethesetof
scoresfromasetof𝑑 >𝑘scoresinadifferentiallyprivatemanner size-𝑘 subsetsoftheindexset {1,...,𝑑}.However,noticethatYFeatureSelectionfromDifferentiallyPrivateCorrelations AISec’24,October18,2024,SaltLakeCity,UT
isexponentiallylarge,containing(cid:0)𝑑(cid:1)elements,eachofwhichwe generalizedlossfunctionalsohasasensitivityof1(provideda
𝑘
wouldneedtocomputetheutilityandnoisetermsofifwewere normalizedscorevector)and[40]consideradditionaloptimizations
tonaivelyimplementtheLipschitzmechanism,whichwouldbe thatcanbemadetothealgorithminthe𝛾 =1case,butweonly
prohibitively expensive. The authors of the original paper [40] discussthe𝛾 ∈ [0,1)caseforsimplicity.
cleverlygetaroundthisbymakingaspecificchoiceoflossfunction
suchthatmanyelementsofYsharethesamelossvalueandYcan
A.4 Implementation
ultimatelybepartitionedintoonly𝑂(𝑑𝑘)utilityclasses.Usingthe
TheLipschitzmechanisminthetop-𝑘 settingreturnsthesize-𝑘
factthat𝑈 01/𝑚 isequalindistributiontomax{𝑈1,...,𝑈 𝑚}fori.i.d.
indexsetwhichmaximizes,overall𝑦 ∈ Y,avaluewhichisthe
standard uniformvariables{𝑈 𝑖}𝑚 𝑖=0 alongwiththefactthat𝐹−1 sumofadeterministicutilitytermandarandomnoiseterm.To
isanincreasingfunction,weonlyneedtogenerate𝐹−1(𝑈1/𝑚)to efficientlyimplementthecanonicalLipschitzmechanism,weonly
findthemaximalnoiseterm(whichisallwecareaboutsinceall needtoiteratethrougheveryutilityclassCℎ,𝑡 andcomputethe
elementsintheutilityclasssharethesamelossvalue)ratherthan maximalnoisetermforeachutilityclassbecauseevery𝑦 ∈Cℎ,𝑡 has
generating𝑚differentsamplesandthentakingthemaximum. thesameutilityandtheadditivenoisesarei.i.d.soevery𝑦 ∈Cℎ,𝑡
isequallylikelytoreceivethelargestnoiseterm.Oncewehavethe
A.3 CanonicalLossFunction 𝑦 ∈Ywiththelargestvalueforeachclass,wesimplyreturnthe
Let𝑦 ∈Ybesomesize-𝑘indexsetand𝑥 ∈R𝑑 besomenormalized onewiththelargestvalueacrossallclasses.Thatis,foreachCℎ,𝑡
scorevectorcorrespondingtosomescorefunction𝑓 anddataset wecompute
𝑥ˆ ∈ X such that𝑥 𝑖 = 𝑓 𝑖(𝑥ˆ)/Δ 𝑓 for𝑖 ∈ {1,...,𝑑}. The so-called
canonicallossfunctionthat[40]introducesisgivenby 𝑌 ℎ,𝑡 =argmax(cid:18) − 2Δ𝜖 LOSS(𝑦|𝑥)+𝐹−1 (𝑈 𝑦)(cid:19)
LOSS(𝑦|𝑥)= min ∥𝑥−𝑣∥∞ 𝑦∈Cℎ,𝑡 LOSS
𝑣∈OPT−1(𝑦)
andthecanonicalLipschitzmechanismultimatelyreturnsthe𝑌
where𝑂𝑃𝑇−1(𝑦) isthesetofall𝑣 ∈ R𝑑 whose𝑘 largestvalues ℎ,𝑡
withthelargestvalueacrossallpossibleℎand𝑡.Thereareonly
havetheindices𝑦 = {𝑦1,...,𝑦 𝑘}.Wenotethatthislossfunction 𝑘(𝑑−𝑘)+1=𝑑𝑘−𝑘2+1=𝑂(𝑑𝑘)differentutilityclasses,butsome
implicitlydependsonthedataset𝑥ˆthroughitsexplicitdependence
utilityclassescancontainmanyelements.Itiseasytoseethatthe
onthescorevector𝑥 anditcanbeshown(see[40])thatthisloss
sizeofautilityclassisdeterminedbythenumberofpossiblebodies
functionhasasensitivityofΔ LOSS=1,aresultwhichreliesonthe
itcanhaveandeachbodycontains|B|=𝑘−ℎ−1elementsand
factthatthescorevectorwasappropriatelynormalizedbyΔ .
𝑓 thereare(𝑡−1)−(ℎ+1)+1=𝑡−ℎ−1possiblevaluesthatcan
Thiscanonicallossfunctionisspecialbecauseitallowsusto
partitionYintodisjointutilityclasseswhereallelementsinagiven
bechosenfromtoformagivenbody,thus|Cℎ,𝑡| = (cid:0) 𝑘𝑡− −ℎ ℎ− −1 1(cid:1).This
meansthatnaivelywewouldhavetogenerateexponentiallymany
utilityclasshavethesamecanonicallossvalue.Thereasonthis
partitioncanbemadeisbecauseitcanbeshownthat
standarduniformnoisetermsforasingleutilityclasswhen|Cℎ,𝑡|=
(cid:0)𝑡−ℎ−1(cid:1) islarge,butaswenotedearlierwecangetaroundthis
LOSS(𝑦|𝑥)= 𝑣∈Om PTi −n 1(𝑦)∥𝑥−𝑣∥∞=
𝑥 [ℎ+1] 2−𝑥 [𝑡]
b
d𝑘
iy
s− toℎ
rn
i− bl1
uy ts ioa nm tp oli tn hg e𝑈 m1 a/ x| iC mℎ, u𝑡| mw oi vth er𝑈 all∼ o𝑈 ft𝑛 h𝑖 e𝑓 s( t0 a, n1 d) aa rs dt uh ni is fois rmeq nu oa il si en
s
whereℎisthelargestintegerstrictlylessthan𝑘 suchthatallof and𝐹−1,whichisappliedtothenoise,isanincreasingfunction.
{𝑗1,...,𝑗 ℎ}arein𝑦and𝑡isthesmallestintegergreaterthanorequal ThisallowsustoperformthecanonicalLipschitzmechanismfor
to𝑘suchthatallof{𝑗 𝑡+1,...,𝑗 𝑑}arenotincludedin𝑦.Eachutility top-𝑘selectioninonly𝑂(𝑑𝑘)time,whichisadramaticreduction
classCℎ,𝑡 isparameterizedbytwointegers:ℎ∈{0,1,...,𝑘−1}and fromthenaiveexponentialcomplexity,notingthatwecancompute
𝑡 ∈ {𝑘,...,𝑑}where𝑡 =𝑘 isonlyallowedifℎ =𝑘−1inorderto thebinomialcoefficients|Cℎ,𝑡|efficientlybyupdatingthemaswe
ensurethateachelementofCℎ,𝑡,definedbelow,containsatotalof iteratethroughℎand𝑡usingthefactthat(cid:0)𝑛+1(cid:1) = (cid:0)𝑛(cid:1)·𝑛+1 forsome
𝑘indices. 𝑗+1 𝑗 𝑗+1
0≤ 𝑗 ≤𝑛,seeAlgorithm2.
DefinitionA.2(UtilityClass). TheutilityclassCℎ,𝑡 isthesetofall
subsetsoftheform{𝑗1,...,𝑗 ℎ}∪B∪{𝑗 𝑡}withB ⊆{𝑗 ℎ+1,...,𝑗 𝑡−1} A.5 StandardExponentialNoiseGeneration
andℎ+|B|+1 =𝑘.Ifℎ = 0,thenwedefine{𝑗1,...,𝑗 ℎ}tobethe Theoriginalpaper[40]achievedthebestresultswhenusingthein-
emptyset.
versecumulativedistributionfunctioncorrespondingtoastandard
Borrowingterminologyfrom[40],each𝑦 ∈ Cℎ,𝑡 comprisesof exponentialdistributionsothat𝐹−1 = −log(1−𝑥).Wediscuss
a“head”{𝑗1,...,𝑗 ℎ},a“body”B,anda“tail”{𝑗 𝑡}.Every𝑦 ∈ Cℎ,𝑡 someimplementationdetailswhenusingthischoiceof𝐹−1 and
containsthetruetop-ℎindicesandtheremaining𝑘−ℎindicesareall alsodiscusssomeusefulpropertiesofthenoisedistributionthat
withinthetruetop-𝑡indices,so𝑗 isthebestindexnotincludedin weobserved.
ℎ+1
𝑦and𝑗
𝑡
istheworstindexincludedin𝑦.Consequently,weseethat Asthebinomialcoefficients|Cℎ,𝑡|canbeverylarge,caremustbe
thecanonicallossLOSS(𝑦|𝑥)isconstantacrossall𝑦 ∈Cℎ,𝑡 because takenwhencomputingthenoiseterm𝐹−1(𝑈1/|Cℎ,𝑡|)=−log(1−
the loss only depends on the best missing score𝑥
[ℎ+1]
and the 𝑈1/|Cℎ,𝑡|)numerically.Forlarge|Cℎ,𝑡|,thequantity𝑈1/|Cℎ,𝑡| will
worstincludedscore𝑥 .Wenotethatthecanonicallossfunction usuallybeverycloseto1andsotheargumentofthelogarithm
[𝑡]
canbegeneralizedtotheformLOSS(𝑦|𝑥)=(1−𝛾)𝑥 [ℎ+1]−𝛾𝑥
[𝑡]
willbeveryclosetozeroandnumericalprecisionbecomesim-
for𝛾 ∈ [0,1],whereabovewehadthecasewhere𝛾 = 1/2.This portantsothatweavoidaddinganartificiallylargenoisetermAISec’24,October18,2024,SaltLakeCity,UT RyanSwope,AmolKhanna,PhilipDoldo,SaptarshiRoy,andEdwardRaff
Algorithm2CanonicalLipschitzMechanismforTop-𝑘 Theorem A.3. The expected value of𝑋 𝑚 = −log(1−𝑈1/𝑚)
1: Input: dataset 𝑥ˆ ∈ X, score function 𝑓 : X → R𝑑 with with𝑈 ∼ 𝑈𝑛𝑖𝑓(0,1) and𝑚 ∈ Z+ is the𝑚th harmonic number
component-wisesensitivityΔ 𝑓,subsetsize𝑘 ∈ {1,...,𝑑−1},
𝐻
𝑚
=1+1/2+1/3+···+1/𝑚.
privacyloss𝜖 ≥0,inverseCDF𝐹−1,lossparameter𝛾 ∈ [0,1) Proof. Itisclearthat
2: Let𝑥 𝑖 =𝑓 𝑖(𝑥ˆ)/Δ 𝑓 forall𝑖 ∈{1,...,𝑑} ⊲Definenormalized ∫ 1 1−𝑥𝑚
scorevector𝑥 𝐻 𝑚 = 𝑑𝑥
0
1−𝑥
3: C ino 𝑂m (p 𝑑u lt oe gd 𝑑e )sc te imnd eingorderstatistics𝑥 [1] ≥...≥𝑥 [𝑑] ⊲Sort𝑥 because 1 1− −𝑥 𝑥𝑚 = 1+𝑥 +𝑥2+···𝑥𝑚−1 for𝑥 ≠ 1.Integratingby
parts,wesee
4: Let𝑈 ℎ,𝑡 ∼𝑈𝑛𝑖𝑓(0,1)forallℎ ∈ {0,...,𝑘−1},𝑡 ∈ {𝑘,...,𝑑}be
5:
i L.i e. td.
𝜖1=(1−𝛾)𝜖and𝜖2=𝛾𝜖 ⊲Notethat𝜖1+𝜖2=𝜖
𝐻
𝑚
=∫ 01 1 1− −𝑥 𝑥𝑚 𝑑𝑥 =−(1−𝑥𝑚 )log(1−𝑥)(cid:12) (cid:12)
(cid:12)
(cid:12)1 0−∫ 01 log(1−𝑥)𝑚𝑥𝑚−1𝑑𝑥
6: Initialize𝐻 =𝑘−1,𝑇 =𝑘 ⊲InitializetheutilityclassC𝐻,𝑇 to
wherethefirsttermcanbeshowntobezerowithanapplicationof
onlyincludethetruetop-𝑘
L’Hôpital’srule.Itisalsotruethat
7: 𝐿 𝐻,𝑇 = 𝜖2− 2𝜖1𝑥 [𝑘] ⊲Initialutility:− 2ΔL𝜖 OSSLOSS(𝑦|𝑥)for
∫ 1 ∫ 1
𝑦 ∈C𝑘−1,𝑘. E[𝑋 𝑚] =− log(1−𝑢1/𝑚 )𝑑𝑢 =− log(1−𝑥)𝑚𝑥𝑚−1𝑑𝑥
8: 𝑋 𝐻,𝑇 =𝐹−1(𝑈 𝐻,𝑇) ⊲RandomnoisecorrespondingtoC𝑘−1,𝑘 0 0
9: Initialize𝑣 =𝐿 𝐻,𝑇 +𝑋 𝐻,𝑇 ⊲ValueforC𝑘−1,𝑘,value=utility+ usingasubstitutionof𝑥 =𝑢1/𝑚.ItfollowsthatE[𝑋 𝑚] =𝐻 𝑚. □
randomnoise
10: for𝑡 ∈{𝑘+1,...,𝑑}do Thisfactisusefulbecauseitallowsustoquantifytheaverage
11: Initializeℎ=𝑘−1 magnitudeofthemaximalnoiseaddedtoeachutilityclasswhich
12: Initialize𝑚=1 ⊲Initializesizeofutilityclass givesusaroughideaofhowlargeourutilitytermsneedtobein
𝑚=|C𝑘−1,𝑡|= (cid:0)𝑡− 0𝑘(cid:1)=1 wor hd ee tr ht eo rc oo rm np oe tt ae cw hi oth set nhe scn oo ri ese f. uT nh ctis ioi nnf wor im lla yt ii eo ldn gca on odhe relp suin ltf so fr om
r
13: whileℎ ≥0do
14: ifℎ<𝑘−1then
somedataset.Forexample,thelargestutilityclassC0,𝑑 willhave
15: Update𝑚=𝑚· 𝑡−ℎ−1 ⊲Efficientlyupdate thelargestexpectednoisetermandonecaneasilycompute𝐻 𝑚
binomialcoefficient 𝑘−ℎ−1 with𝑚 = (cid:0)𝑑 𝑘−− 11(cid:1) togetaroughsenseofhowlargethenoiseis.If
theutilitytermistoosmall,thenthenoisewilldominateandthe
16: endif
17: 𝐿 ℎ,𝑡 = 𝜖 22𝑥 [𝑡]−𝜖 21𝑥 [ℎ+1] ⊲UtilitytermforCℎ,𝑡 quality of the returned index set will be worse on average. We
18: 𝑋 ℎ,𝑡 =𝐹−1(𝑈 ℎ1 ,/ 𝑡𝑚 ) ⊲RandomnoisetermforCℎ,𝑡 n effiot ce ieth na tlt yv ce or my pg uo to ed da up sp inro gx wim ela lt -i ko nn os wo nfh aa syrm mo pn toic tin cu em xpb ae nr ss ioca nn s.be
19: 𝑣′ =𝐿 ℎ,𝑡 +𝑋 ℎ,𝑡 ⊲ValueforCℎ,𝑡
20: if𝑣′ >𝑣then Received20February2007;revised12March2009;accepted5June2009
21: Update𝑣 =𝑣′,𝐻 =ℎ,𝑇 =𝑡 ⊲UpdateC𝐻,𝑇 tobe
Cℎ,𝑡 ifCℎ,𝑡 hasalargervalue
22: endif
23: Updateℎ=ℎ−1
24: endwhile
25: endfor
26: ReturnarandomindexsetfromC𝐻,𝑇
andconsequentlyreturnbadapproximationsofthetruetop-𝑘in-
dex set. To address the issues of numerical stability, we Taylor
expanded the function𝑔(𝑦) = 1−𝑢𝑦 for𝑢 ∈ [0,1] and𝑦 ≥ 0
about𝑦 = 0andkeptatleastaquadraticapproximationsothat
𝑔(𝑦) ≈ −𝑦log(𝑥)− 1𝑦2log2 (𝑥).Giventhattheapproximationis
2
onlygoodwhen𝑦issufficientlyclosetozero,weonlyuseditwhen
𝑦 ≤10−8andsimplyused1−𝑢𝑦 otherwise,whichworkedwellin
practice.
A useful property of the noise term𝑋 ℎ,𝑡 = 𝐹−1(𝑈1/|Cℎ,𝑡|) =
−log(1−𝑈1/|Cℎ,𝑡|)with𝑈 ∼𝑈𝑛𝑖𝑓(0,1)isthatitsexpectedvalue
isaharmonicnumber.Inparticular,
| ∑︁Cℎ,𝑡|
1
E[𝑋 ℎ,𝑡] = 𝑘.
𝑘=1