Enhancing End-to-End Autonomous Driving Systems Through
Synchronized Human Behavior Data
YiqunDuan ZhuoliZhuang JinzhaoZhou
yiqun.duan-1@uts.edu.au zhuoli.zhuang@uts.edu.au jinzhao.zhou@student.uts.edu.au
HAICentre,AAII,Universityof UniversityofTechnologySydney UniversityofTechnologySydney
TechnologySydney Sydney,NewSouthWales,Australia Broadway,NewSouthWales
Sydney,NSW,AU Australia
Yu-ChengChang Yu-KaiWang Chin-TengLin
fred.chang@uts.edu.au yukai.wang@uts.edu.au chin-teng.lin@uts.edu.au
UniversityofTechnologySydney UniversityofTechnologySydney UniversityofTechnologySydney
Broadway,NewSouthWales Broadway,NewSouthWales Broadway,NewSouthWales
Australia Australia Australia
ABSTRACT 1 INTRODUCTION
Thispaperpresentsapioneeringexplorationintotheintegration Thispositionpaperaimstopioneertheexplorationofintegrat-
offine-grainedhumansupervisionwithintheautonomousdriving inggranularhumansupervisionintotheburgeoningfieldofau-
domaintoenhancesystemperformance.Thecurrentadvancesin tonomousdrivingtoenhanceitsperformance.Presently,thema-
End-to-Endautonomousdrivingnormallyaredata-drivenandrely jorityofautonomousdrivingapproaches,whetherend-to-endor
ongivenexperttrials.However,thisreliancelimitsthesystems‚Äôgen- pipeline-based,relyheavilyonexperttrials.Suchrelianceisinade-
eralizabilityandtheirabilitytoearnhumantrust.Addressingthis quateforconsiderationssuchasgeneralizabilityandearninghuman
gap,ourresearchintroducesanovelapproachbysynchronously trust.Thispapercollectssynchronousdatafrombothhumanand
collectingdatafromhumanandmachinedriversunderidentical machinedrivingscenariostoinvestigatethisaspect.
drivingscenarios,focusingoneye-trackingandbrainwavedata Thecurrenttrendinautonomousdrivingsystemscouldbecat-
toguidemachineperceptionanddecision-makingprocesses.This egorizedintopipelineformations[14,17,28,35]andEnd-to-End
paperutilizestheCarlasimulationtoevaluatetheimpactbrought (E2E)approaches[10,23,31,40].Thesesystemsprimarilytransform
byhumanbehaviorguidance.Experimentalresultsshowthatusing rawsensoryinputsintomachine-readablerepresentationsusinga
humanattentiontoguidemachineattentioncouldbringasignifi- varietyoffeaturefusiontechniques,suchasBEVorrangeview.The
cantimprovementindrivingperformance.However,guidanceby goalistodevelopautonomousdrivingmodelsthatcancomplete
humanintentionstillremainsachallenge.Thispaperpioneersa predeterminedrouteswhilesafelynavigatingdynamicenviron-
promising direction and potential for utilizing human behavior mentsandcomplyingwithtrafficrules,allthroughobservational
guidancetoenhanceautonomoussystems. data-drivenpolicies.
These approaches mentioned above are predominantly data-
KEYWORDS driven,relyingheavilyonmodelslearningfromexpertexamples.
However,thepuredata-drivenformationalsobringsstrongreli-
Human-GuidedAutonomousDriving;Brain-ComputerInterfaces
abilitytothedatadistribution.Wesuggestanaugmentationof
thismethodologybyincorporatingfine-grained,immediatehuman
ACMReferenceFormat:
labelsasfine-tuningfeedbacktofurtherenhancetherobustness
YiqunDuan,ZhuoliZhuang,JinzhaoZhou,Yu-ChengChang,Yu-KaiWang,
ofthedrivingprocess.Ourproposalinvolvesanovelapproach
andChin-TengLin.2024.EnhancingEnd-to-EndAutonomousDrivingSys-
where humans and machines share the same driving scenarios.
temsThroughSynchronizedHumanBehaviorData.InProceedingsofthe1st
InternationalWorkshoponBrain-ComputerInterfaces(BCI)forMultimedia Duringthesesharedexperiences,weaimtogatherdatafromboth
Understanding(BCIMM‚Äô24),October28-November1,2024,Melbourne,VIC, humanandmachinedriversconcurrently.Thisdual-datacollection
AustraliaProceedingsofthe32ndACMInternationalConferenceonMultime- focusesoneye-gazingmarkersandbrainwavedata,offeringacom-
dia(MM‚Äô24),October28-November1,2024,Melbourne,Australia.ACM,New prehensiveviewofthedrivingenvironmentanddecision-making
York,NY,USA,8pages.https://doi.org/10.1145/3688862.3689108 processes.Incorporatinghuman-guideddataintotheautonomous
systemisintuitivelyrightashumansstillperformbetterability
while dealing with a lot of scenarios. The utilization of human
guidancecouldleveragehumansuperiorintelligenceincomplex
Permissiontomakedigitalorhardcopiesofpartorallofthisworkforpersonalor
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed scenarios(eg.correctattentionincomplexscenarios)intuitively
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation benefitingfromexpertguidanceinthelearningprocess.
onthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored.
Forallotheruses,contacttheowner/author(s). Introducinghumanguidanceintothelearningloopisafeasible
BCIMM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia andeffectivestrategy,asevidencedbypreviousstudies[5,22,29,
¬©2024Copyrightheldbytheowner/author(s). 39].However,thisareahasverylimitedresearch.Followingthe
ACMISBN979-8-4007-1189-3/24/10
autonomousdrivingscenariosmentionedabove,weproposethe
https://doi.org/10.1145/3688862.3689108
4202
guA
02
]OR.sc[
1v80901.8042:viXraBCIMM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia YiqunDuan,ZhuoliZhuang,JinzhaoZhou,Yu-ChengChang,Yu-KaiWang,andChin-TengLin
Human State
Representation
Human Brain Cognition State
+ Human driving behaviour
data
Human and Machine Perform
Same Task at the Same Time
Simulation Platform +
Camera & VR Headset+ EEG
Vision sensors Eye Tracking Attention
Lidar
Radar
HD Maps 1. Eye-Tracking Guided
driving attention
Driving 2. Decision Enhance with
States Cognition and driving
behaviour data feed back.
Sensory Input
Machine Perception Framework
Figure1:Visualschemaofhumanenhancedautonomousdriving.Thismodelsubjectsboththemachineandthehumanto
identicaldrivingscenariosalongthesameroute.Throughoutthisprocess,wecollectsynchronousdataonhumanbehavior,
includingeye-trackingmetrics,brainwavepatterns,andbrakesignaling,capturingtheseelementsinunison.
drivingframeworkwheretheoverallstructureofourexplorationis humaneye-trackingdataasanauxiliaryfeedbackmechanismin
illustratedinFig1.Itdelineatestheresearchintotwomainaspects imitationlearninghasyieldedpositiveimprovementsindriving
ofexploringhowhumanguidancecouldenhancetheautonomous performance.Specifically,thisapproachenhancedthedrivingscore
drivingsystem:1)eyetrackingattentionduringhumandrivingand ontheCarlaLong-Set6datasetfrom50.63to51.29.Conversely,the
2)humancognitiondatageneratedthroughthedrivingprocedure. integrationofcognitiondatadidnotresultinasignificantdirect
Weinitiallyexploretheconceptof‚Äúobservinglikeahuman" enhancementofthemachine‚Äôsdrivingscore.Atthepositionpaper
inSection3.5.1.Thisinvolvesanalyzingeyemovementsduring stage,wethinkthisissueisreasonablebecauseoftwoissues:1)
drivingtouncoverdeeperinsightsintothecognitiveprocessesand Therecognitionaccuracyofthehumanbraindataisstilllimited.
attentionalfocusofhumandrivers.Leveraginghumanattention Thislimitedtheguidanceaccuracyofhumanintentionfordriv-
asguidance,thisapproachseekstoaddressoverlookedaspectsby ing.2)Thedataqualityisstillrelativelylow,wherethehuman
machines.Itallowsforthecorrectionofinstanceswhereobjects driver‚Äôsreactionspeedsvary,andtheirdecision-makingcapability
ofpotentialsignificanceareignoredbyautomatedsystems.This varieswhenfacingdangeroussituations.Thisoutcomesuggests
human-guidedattentionisinstrumentalinhighlightingobjectsthat thattheeffectiveapplicationofcognitiondatainautonomousdriv-
alignwithhumanempiricalskillsbutmaynotberepresentedin ingremainsanarearipeforfurtherinvestigation.However,the
experttrainingdatasets.Suchguidanceisinvaluableformachines, effectivenessofhuman-observingdatastilloutlinesfutureresearch
as it aids in recognizing and responding to critical elements in integrating human guidance in a collaborative human-machine
variousdrivingscenarios,includingsuddenhazardsorintricate drivingcontext.Thecontributionofthispapercouldbecategorized
trafficsituations.Byintegratingthesehuman-centricinsights,we intofourfolds.
cansignificantlyenhancethemachine‚Äôsperceptualanddecision-
‚Ä¢ Thispositionpaperpioneersenhancingmachinedrivingby
makingabilitiesindynamicdrivingenvironments.
twoaspects1)observinglikehumandrivers,and2)making
Secondly,weexplorehowthehumancognitionorbehaviordata
decisionslikehumandrivers.
couldhelpguidetheautonomousdrivingmodelinSection3.5.2.
‚Ä¢ Thispapercollectstheparallelhumancognitionandbehav-
Weexplorethehumancognitiondatatotrainanadditionalreward
iordataofsimultaneousmachineandhumandriving.
criticizertoprovideadditionalfeedbackforadditionalimitation
‚Ä¢ Experimentalresultssuggesteffectivenessofintroducing
learning.Torealizethisgoal,wefirsttrainasimpleEEGwaveclas-
human-guidanceintoautonomousdriving.
sifiertorecognizewhetherhumanswilldecidetoemergencybreak
accordingtothecurrentsituation.Then,wefeedthesimultaneously
2 RELATEDWORKS
collectedEEGwaveswhilehumandrivingtothisclassifieranduse
thebreaksignalastheadditionalfeedbacktothereinforcement End-to-EndAutonomousDriving. Althoughrapidlydeveloping,
fine-tuningofthedrivingmodel. autonomousdriving(AD)technology[24],itfaceschallengesin
ExperimentalresultsareconductedinSection4,wherewere- generalizationandsafety.Toremedythisissue,anumberofworks
spectivelyexplorehoweye-trackingguidanceandhumancogni- haveexploredimitationlearning(IL)andreinforcementlearning
tiondataguidanceinSection3.5.1.Theresultssuggestleveraging methodstoleveragehumanguidanceforenhancingtrainingeffi-
ciencyandthesafetyofthelearnedofdrivingpolicy[5,8,9,25,30].
Transformer
DecisionEnhancingEnd-to-EndAutonomousDrivingSystemsThroughSynchronizedHumanBehaviorData BCIMM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia
Currentmethodologiesforintegratinghumanguidanceintoau- denotestheexperttrajectoryofwaypoints.Here,ùë• ùë°,ùë¶ ùë° denotes
tonomousdriving(AD)systemsprimarilycategorizebasedontheir the2Dcoordinatesinego-vehicle(BEV)space.Thus,thelearning
impactontheADsystemintotwodistinctapproaches:1)leveraging targetcouldbedefinedasinEq.1.
humanexpertknowledgetotrainreliabledrivingpolicies,and2)in- argmin E (X,W)‚àºD (cid:2) Lùë§ùëù(W,ùúã(X))(cid:3) (1)
corporatinghumanoversightduringmodelexecution.Techniques ùúã
suchasConditionalImitationLearning(CIL)[4],andGenerative whereLùë§ùëù isthewaypointlossdefinedinEq.4,andùúã(X)isthe
AdversarialImitationLearning(GAIL)[13]relyonhumanexpert predictedwaypointsgivenobservationXthroughpolicyùúã tobe
trajectoriesfortrainingthedrivingpolicymodel.However,these learned.
methodsfacesignificantchallenges,includingtheinefficiencyof Formation:Inthispaper,thepolicyùúã(X) isrealizedbythe
datacollectionanddistributionshifts.Alternatively,adifferentset combinationofahybridfusionnetwork(Sec.3.2)andthedecision
ofapproachesfocusesoninterpretinghumanbiosignalsduringthe transformer(Sec.3.3),wherethefusionnetworktransfermulti-
evaluationphase,enablingthemodeltomaneuverthecarmore modalitysensoryinputsXintosemantictokensFùë†,anddecision
safelyunderhumanguidanceorfeedback[11].Subsequentresearch transformerpredictsfuturegoalpointsWgivenFùë†.Thehuman
hasproposedstrategiesforemployingreal-timehumanguidance guidanceisinjectedbyThenaPIDControllerisappliedonthe
toenhancethesafetyandperformanceofthehuman-AIco-driving waypointsWdecisionanddecomposedintopracticalcontrol,ie.,
system[12,15,36,37].Thesestrategiesinvolvedynamiccontrol steer,throttle,andbrake.
transferbetweenhumandriversandAIagentstofacilitatetimely
intervention,therebyimprovingtheco-drivingexperience.This 3.2 HybridFusionTransformerEncoder
evolutioninapproachunderscorestheimportanceofhuman-AIcol-
ForbasicsensorfusionweutilizeMaskFuser[8]asourmainourk.
laborationinenhancingtheeffectivenessandsafetyofautonomous
MaskFuserproposedahybridnetworkshowninthelowerpartof
drivingsystems.Tocombinethemeritsoftheabovemethods,our
Fig.2thatcombinestheadvantagesofearlyfusionandlatefusion.
proposedmethodsalsomakeuseofhumanbiosignalstoimprove
Thenetworkconsistsoftwostages.
thetrainingefficiencyofAImodelswhilealsobeingusedindriving
EarlyFusion:Atthefirststage,weapplytwoseparateCNN
interventions.
branchestoextractshallowfeaturesrespectivelyfrommonotonic
imageandLiDARinputs.Fortheimagebranch,MaskFusercon-
Real-TimeHumanGuidanceforAutonomoussystems. Current
catenatesthreefrontviewcamerainputseachwith60Fovintoa
methodsforintroducingreal-timehumanguidanceinautonomous
monotonicviewandreshapedintoshape3√ó160√ó704.Forthe
drivingscenariospredominantlyrelyonheadmovement[20]and
LiDARbranch,MaskFuserreprocessestherawLiDARinputwith
eye-trackingsignals[1,41].However,theseapproaches,focuspri-
PointPillar[16]intoBEVfeaturewithshape33√ó256√ó256.Sincethe
marilyonexternalindicatorsofattentionandintentwithoutcon-
lower-levelfeaturesstillretainstronggeometricrelations,thesep-
sideringthecriticalroleofcognitiveprocessesindriving.Thereare
aratedencodercouldextracttightlocalfeaturerepresentationwith
onlylimitedworksexploringthereal-timemonitoringofthecogni-
fewerdistractions.Anovelmonotonic-to-BEVtranslation(MBT)
tiveprocessinthecontextofdrivingsafety[2,3,26,34],however,
attentionisappliedtoenricheachmodalitywithcross-modality
theydidnotconsiderusingthecognitivebrainsignalasguidance
assistance.MBTattentiontranslatesbothimagesandLiDARfea-
totheADsystem.Toaddressthisgap,weproposeaninnovativere-
turesintoBEVfeaturespaceandperformsamoreprecisespatial
searchdirectionthatintegrateshumancognitivesignalsalongside
featurealignmentcomparedtopreviouselement-wiseapproaches.
eye-trackingdatatoprovideamorecomprehensiveunderstanding
LateFusion:Atthesecondstage,thenetworkrespectivelytok-
ofdriverintentions.Bycombiningthesesourcesofinformation,we
enizes[6]featuremapsfromImageandLiDARstreamintosemantic
aimtoachieveamoreseamlessintegrationofthedriver‚Äôsobjectives
tokens,respectivelydenotedbygreenandblueinlowerpartof
andsafetyconsiderationswiththeperformanceoftheautonomous
Fig.2.Thelatefusionisperformedbydirectlyapplyingashared
driving(AD)system.Thisholisticapproachpromisestoenhance
transformerencoderovertheconcatenatedtokenrepresentation.
thesymbiosisbetweenhumandriversandADtechnologies,paving
Thesharedencoderwithpositionembeddingcouldforcethetokens
thewayforadvancementsindrivingsafetyandefficiency.
fromvariousmodalitiesalignedintoaunifiedsemanticspace.Also,
bytreatingmulti-sensoryobservationassemantictokens,wecould
3 METHODOLOGY
furtherintroducemaskedauto-encodertrainingmentionedbelow.
3.1 Overview
3.2.1 Monotonic-to-BEVTranslation(MBT). MBTattentionper-
ProblemSetup:Wefollowthepreviouswidelyacceptedsettingof formscross-modalityattentionmorepreciselybyintroducinghu-
E2Edriving([10,32,40])thatthegoalistocompleteagivenroute manpriorknowledge(BEVtransformation).InspiredbyMonotonic-
whilesafelyreactingtootherdynamicagents,trafficrules,and Translation[21],wemodelthetranslationasasequence-to-sequence
environmentalconditions.Thus,thegoalistolearnapolicyùúãgiven processwithacameraintrinsicmatrix.Thedetailedstructureof
observation.WechoosetheImitationLearning(IL)approach MBTattentionsisshowninFig.3,wherethefeaturemapfrom
tolearnthepolicy.Thegoalistoobtainpolicyùúã byimitatingthe imagestreamwithshapeFùëñùëö ‚àà RùëÅ√óùê∂√óùêª√óùëä isreshapedalong
behaviorofanexpertùúã‚àó.Givenanexpert,thelearningdatasetD = widthdimensionùëä intoimagecolumnsFùëê ‚àà RùëÅùëä√ó{ùêª√óùê∂}.The
{(Xùëñ,Wùëñ)}couldbecollectedbylettingtheexpertperformsimilar
columnvectorsareprojectedintoasetofmediateencoding{hùëñ ‚àà
r so enu ste os r, yw oh be sere rvX atùëñ io= ns{ o( fx tùëñ ùëñùëö he,x cùëñ ùêø uùëñ r) rùë° e} nùëá ùë°= t1 std ae ten ,o at ne ds i Wmag =e {a (ùë•n ùë°d ,ùë¶L ùë°i )D }ùëáA ùë°=R
1
WRùêª e√ó tùê∂ re} aùëñùëÅ t=ùëä 1 met dh ir ao tu eg eh na ct or da in ns gfo or fm mer ol na oy te or nw icith vim ewult hi- ùëñh wea hd ics helf p- ra ott jeen ctt sion.BCIMM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia YiqunDuan,ZhuoliZhuang,JinzhaoZhou,Yu-ChengChang,Yu-KaiWang,andChin-TengLin
Waypoint Prediction
Status Status
Prediction Prediction
Velocity Eye-Tracking Human Intention Label
wH ayis pt oo ir ny t s GRU H Oea ffd sein tg HumanM AA tE te nL to ioss n Attention T Sr ta offi p c S iL gig n ht Cross Entropy
BBox Prediction Junction Break Signal
Decision Transformer
‚Ä¶
waypoints queries Hidden State Tokens Traffic Query Cognition Query
Nx80x384x10 Nx40x176x216 MLP Layer
8
Range Decode
Patch r Head
Depth
Vision with Monotonic View Head
MBT MBT Seg
Head
Image
Nx256x256x
3
BEV
PointPill BEV Head
ar Patch
Decode
r Head
LiDAR
Nx128x128x108 Nx64x64x216 LiDAR
Figure2:Frameworkofinjectinghumanguidanceintotheautonomoussystem,takingautonomousdrivingasanexample.
ThelowerpartisthehybridfusiontransformerencoderdefinedinSection3.2.Thelearnedmachinestateisfedintoadecision
transformertocomeoutwiththefinaldrivingwaypointprediction.Thedecisiontransformerissupervisedbywaypoints
predictionGTandothersafetyconstraints.Thehumanguidanceisinjectedbyaddingtwohuman-behaviordatasupervision
branches.Thedecisiontransformerisrequiredtoreconstructhumanbehavior(eye-trackingattentionandbrakeintention)
jointlywithothertargets.
Image
Block
featuremapFùëöùëèùë°.
K,V Transfromer ùëÑ(gùëñ)ùêæ(hùëó)ùëá
s(gi,hj)= ‚àö , ùëÑ(gùëñ)=gùëñùëä ùëÑ
Q ùê∑
(3)
NWx{HxC} Query Emb
Grid
Sampling Fùëöùëñùëë =‚àëÔ∏Å
ùëñ
(cid:205)e ùëóx ep x( ps (( sg (i g,h i,j h)) j))ùëâ(hùëñ)
Position Query
wheres(gi,hj)isthescaleddotproduct[33]regularizedbydimen-
sionùê∑.Yet,sincethemonotonicviewonlysuggestsinformation
LiDAR BEV
insidecertainFOV(Fieldofview),weapplyasamplingprocess
LiDAR
Block Fùëöùëèùë° = P(F mid) tosamplepointsinsideFOVdecidedbycamera
intrinsicmatrixintoBEVfeaturemapFùëöùëèùë°.Thetranslatedfeature
Figure3:ThestructureoftheMBTattentionmodule,where mapFùëöùëèùë° andfeaturemapfromLiDarFùêøùëñ isreshapedbyflatten
thefeaturesfrommonotonicviewareprojectedintoBEV alongwidthùëä anddepthùëü intovectorsandconcatenateintose-
spacethroughasequence-to-sequenceformation. quenceFùëñùëõ = cat(F mbt,FLi).Thetransformerlayerisappliedon
Fùëñùëõ
‚ààRùëÅ‚òÖ√óùê∂
toperformselfmulti-headattention([33])between
eachtokeninùëÅ‚òÖ
dimension.
thekeyandvaluerepresentingtherangeviewinformationtobe
translated.
3.3 DecisionTransformer
ùêæ(hùëñ)=hùëñùëä ùêæ, ùëâ(hùëñ)=hùëñùëä ùëâ (2)
InspiredbythepreviousworkInterFuser[23],weuseasimilar
Agridmatrixisgeneratedindicatingthedesiredshapeofthetarget transformer decoder as the decision layer. The decoder follows
BEVspace.Wegeneratepositionencoding{ùëî ùëñ ‚ààRùëü√óùê∂} ùëñùëÅ =ùëä
1
along thestandardtransformerarchitecture,transformingsomequery
sideeachradiusdirectioninsidethegridwithdepthùëü.Thegrid embeddingsofsizeùëëusingùêæ layersofmulti-headedself-attention
positionùëî ùëñistokenizedintoqueryembeddingandquerythehùëñwith mechanisms.Fivetypesofinputaredesigned:{ùë§ ùëñ} ùëñùë°
=1
previous
radiusdirections1asdefinedinEq.3andgeneratethetranslated waypointsquery,ùëÖ2densitymapqueries(toquerycurrentvehi-
clestatus),humanattentionquery,onetrafficrulequery,andone
humanintentionquery.Thesequeriesareconcatenatedintoase-
1Theradiuscoordinatesarecalculatedbygivencameraintrinsicmatrix,FOV,and
prefixeddepthlength.
quenceoftokenswithshapeq‚ààRùëë√óùëÅ
whichwiththesameshape
Transfromer
Multi-Head
Attention
Encoding
Unified
Position
Transformer
Encoder
Transformer
Decoder
ClassificationEnhancingEnd-to-EndAutonomousDrivingSystemsThroughSynchronizedHumanBehaviorData BCIMM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia
withz.Toindicatethequeryorderweaddastandardpositional 3.5 Human-GuidanceHeaders
embeddingintothequeryqwhendoingtheconcatenation.Ineach 3.5.1 HumanEye-TrackingAttentionPrediction. Wedesigntheeye
decoderlayer,weemploythesequeriestoinquireaboutthede-
trackingattentionquerieswithshapeQeye ‚àà
Rùêª 16√óùëä 16√óùëë
,where
siredinformationfromthemulti-modalmulti-viewfeaturesvia ùêª ùëä
the √ó denotesthe16timesdown-sampledrange-viewratio
theattentionmechanism.Morespecifically,ineachtransformer 16 16
ofthecamera,whichhastheexactlysameratiowiththehuman
layer,wetreatperceptionstatezaskeyKandvalueV,andwe
treatthementionedqueriesqasthequeryQ.Thisstructureal-
attentiongroundtruthmapwithratioùêª √óùëä.ThequeryEeyeis
ùêª ùëä
fedintothedecisiontransformerbyflattening √ó intoafixed
lowsthedecisiontransformerqueryperceptionstateaccordingly 16 16
lengthandoutputsthepredictionembeddingthesameshape.Then
withdifferentqueriesinsidethesequence,resultingintheoutput
weuseatransposeconvolutiontoupsampletheembeddingfrom
independentlydecodedintowaypoints,onedensitymap,human
attentionprediction,trafficstatus,andthehumanintentionbythe shape 1ùêª 6√óùëä 16 √óùëëintohumanattentionpredictionEeyewithshape
followingpredictionheaders. ùêª √óùëä √ó1.Wedefinetheeyetrackingpredictionloss L eye by
calculatingreconstructionlossbetweenpredictionEeyeandground
3.4 BasicPredictionHeaders
truthhumanattentionE¬Ø eye:
The transformer decoder is followed by five parallel prediction L eye=‚à•Eeye‚àíE¬Ø eye‚à•2, (6)
modulestopredictthewaypoints,theobjectdensitymap,human
whereweusemeansquareerror(MSE)losstosupervisethehuman
attention,trafficrule,andhumanintentionrespectively.
attentionpredictorhead.
3.4.1 WaypointsPrediction. Forthewaypointsprediction,follow- 3.5.2 Human Intention Prediction. Given the synchronized col-
ingthementionedwaypointspredictionnetworkdefined[24],we lecteddatahumanintentionEEGdata,weuseapre-trainedclassi-
takeasinglelayerGRUtoauto-regressivelypredictasequenceof fier[7,42,43]todeterminewhetherthehumanhastheintentionto
threefuturewaypoints{ùë§ ùë°+ùëô} ùëô3 =1.TheGRUpredictstheùë°+1-thway- breakthecarwerespectivelyrepresenttheEEGclassifiedlabelas
pointsbytakinginthehiddenstatefromtheùë°-thdecodedwaypoint Iùê∏ùê∏ùê∫.Meanwhilegiventhebreakbehaviorsignalfromthehuman
embeddingfromthetransformerdecoderrelatedtothewaypoints driver‚ÄôspedalasIùëèùëüùëéùëòùëí,weproposetoletthetransformerjointly
queries,andpreviousinputsfromtherecordedùë§ ùë°‚àí2,ùë§ ùë°‚àí1,ùë§ ùë°.Also predictthehumanintentionscore.Similartotrafficruleprediction,
toinformthewaypointsGRUpredictoroftheegovehicle‚Äôsgoal weutilizeacombinedbinarycross-entropylosstosupervisethe
location,weconcatenatetheGPScoordinatesofthegoallocation decisiontransformer.Thelossfunctionisgivenbelow:
atthebeginningoftheinputsequence.Morespecifically,theloss
functionisdefinedinEquation4asfollows: L hb=ùúÜ EEGL EEG+ùúÜ bL brake, (7)
ùëá wherethehumanintentionissupervisedbycalculatingthebinary
Lùë§ùëù =‚àëÔ∏Å(cid:13) (cid:13) (cid:13)wùë° ‚àíwùëî ùë°ùë°(cid:13) (cid:13)
(cid:13)
(4) cross-entropycombination.
ùë°=1 1
3.6 TrainingLossCombination
ùëîùë°
wherethewùë° isthegroundtruthfromtheexpertroute. Thelossfunctionisdesignedtosimultaneouslypredictmultiple
targetsincluding,waypoints(Lùëùùë°),objectdensitymap(Lùëöùëéùëù),hu-
3.4.2 DensityMapStatusPrediction. Thedensitymapprediction manattention(Lùëíùë¶ùëí),trafficrule(Lùë°ùëì),andhumanbreakintention
forcesthemodeltolearntopredictthecurrentvehiclestatuson (L‚Ñéùëè).
adensitymap.WebasicallyfollowtheoriginalsettingofInter-
Fuser[23]asthisisnotourresearchtarget.Pleaserefertoappen-
ùêø=ùúÜ ptL pt+ùúÜ mapL map+ùúÜ mapL map+ùúÜ tfL tf+ùúÜ hbL hb, (8)
dixAfordetaileddefinitionsoflossLùëöùëéùëù. whereùúÜbalancesthethreelossterms.Here,thewaypoints(Lùëùùë°),
objectdensitymap(Lùëöùëéùëù),andtrafficrule(Lùë°ùëì)consistsofthe
3.4.3 TrafficRulePrediction. Fortrafficruleprediction,thecor-
puredatadrivingsupervisionforautonomousdrivingsimilarInter-
responding embedding from the transformer decoder is passed
Fuser[23]andthetwoadditionallossitemshumanattention(Lùëíùë¶ùëí)
throughasinglelinearlayertopredictthestateofthetrafficlight
andhumanbreakintention(L‚Ñéùëè)indicatesthehumanguidance.
ahead,whetherthereisastopsignahead,andwhethertheego
vehicleisatanintersection.Whenpredictingthetrafficinformation 4 EXPERIMENTS
L tf,weexpecttorecognizethetrafficlightstatusLùëô,stopsignLùë†,
andwhetherthevehicleisatajunctionofroadsLùëó.Allthesethree 4.1 DataCollection
statusesarerepresentedasaone-hot0-1labelacquiredfromthe Thedataarecollectedbycollectingdatasimultaneouslyfromhu-
CARLAsimulator.Theselossesaresimplycalculatedusingcross mansandmachinesdrivingunderthesamerouteandsimultane-
entropybetweenpredictionandgroundtruthasbelow: ouslycollectingthehumandataincludinghumaneye-tracking,
humanbrainwaves,andbrakingbehaviors.Thesimultaneoushu-
L tf=ùúÜ ùëôLùëô +ùúÜ ùë†Lùë† +ùúÜ ùëóLùëó, (5) mancollectionprocessisrealizedbylettinghumansubjectsdrive
usingtheLogitechG920drivingforcewheels,pedals,anddriving
whereùúÜbalancesthelossterms,whicharecalculatedbybinary chair.Thehumanvisualattentioniscollectedbylettinghumans
cross-entropyloss. wearaHTCViveProEyevirtualrealitygear.WeusethewidelyBCIMM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia YiqunDuan,ZhuoliZhuang,JinzhaoZhou,Yu-ChengChang,Yu-KaiWang,andChin-TengLin
Figure4:Visualizationexampleofthecollectedcombined
humaneye-trackingattentiondata.
Figure5:Exampleofthestatisticaldistributionofleftand
righteye‚Äôspupilpositionanddiametersizechangeduring
usedCARLAsimulator2 version0.9.13asthedrivingsimulator oneepisode.
Thedrivingscenariosareprojectedintoembodiedperspectivefor
thehumandriver.ThankstothepreviousworkDReyeVR3[27],
wedirectlyutilizetheproposedmodifiedCARLAsimulatortocom- withcorrespondinglabeladjustments.Inter-modalbridgingtrans-
pletethedatacollection.Tovisualizethehumaneye‚Äôsattention,we formers(MBTs)integratefeaturespost-initialconvolutions,atres-
provideexamplesofcollectedeye-trackingdatainFigure4.Simul- olutions(ùê∂ 1,40,176)and(ùê∂ 2,20,88),whereùê∂ 1=72andùê∂ 2=216.
taneously,wecollecttheEEGdatathrougha64-channelcollection TheMBTcomprisestwotransformerlayerswith512hiddendi-
device,whereexcludingtheground,reference,CB1,andCB2,the mensionsand4heads.Followingestablishedpolarraygridsam-
availablecountshouldbe60outof64.Wecollect12humansub- pling[21],MBTattendstovaryingdepthranges,translatingtoa
jectsusingthedefaultroutesunderTown4andTown7forhuman real-worldcoverageofupto30.5meters.Post-MBT,featuremaps
guidance. of20√ó88√ó512and32√ó32√ó512aresectionedinto4√ó4patches,
Forthemachineperceptiondataset,weemployedarule-based combinedintoatokensequenceforaunifiedViTencoderwithfour
expertagentthatadherestothemethodologiesoutlinedinTrans- layersand4attentionheads.Thisresultsinasemanticsequenceof
Fuser[10]andInterFuser[23].Thisagentwasdeployedacrossa 174tokens,maintainingthe512-dimensionalityfromearlyfusion.
diverserangeofeighturbanlayoutsandvaryingweathercondi- Forthedecisiontransformer,thenumberoflayersùêæinthetrans-
tions,operatingatafrequencyof2framespersecond.Through formerdecoderandthetransformerencoderis6,andthefeature
thisprocess,weamassedanextensiveexpertdatasetconsistingof dimensionùëë is256.Thedimensionofsemantictokensequence
3millionframes,whichequatestoapproximately410hoursofdata. outputfromtheHybridNetworkisprojectedinto256andfedinto
Thisdatasetservedasthefoundationfortheinitialpretraining thedecisiontransformer.WetrainourmodelsusingtheAdamW
phaseofourmachineperceptionmodel. optimizer[19]withacosinelearningratescheduler[18].Theini-
Theeye-trackingisprojectedinto2Dpositionswhiletraining. tiallearningrateissetto5√ó10‚àí4√ó BatchSize forthetransformer
512
Forfurtherillustrationofthecollecteddatadistribution,wefurther encoder&decoder,and2√ó10‚àí4√ó BatchSize fortheencoders.The
visualizethestatisticaldistributionoftheeyedatainFigure5. 512
weightdecayforallmodelsis0.07.Allthemodelsaretrainedfora
maximumof35epochswiththefirst5epochsforwarm-up[18].
4.2 ImplementationDetails
Fordataaugmentation,weusedrandomscalingfrom0.9to1.1and
Themodeltrainingisdividedintotwostages,wherestage1follows colorjittering.
thenormalautonomousmodeltraining,andstage2utilizesthe Human-GuidedFinetuneUponcompletionofthepretraining
human-guideddatatofurtherfinetunethedecisionmodel. withthe3millionframesderivedsolelyfrommachine-generated
Machine State Pretraining: For the feature extractor part, data,wetransitionedtoaphaseoftargetedrefinement.Inthisstage,
we follow the setting proposed in Section 4.2, the Hybrid Net- theperceptionmodel‚Äôsparameterswerefixed,andweexclusively
work,whichemployscamerasandLiDARasdualmodalities.Cam- fine-tunedthedecision-makingmoduleofthetransformer.Forthe
erainputsaremergedintoa120-degreeFOVandreformattedto EEGwaveclassifier,wepre-traina‚Äúhumanintentiontobrake"
(160,704),whileLiDARdataistransformedtoa256√ó256BEVfor- classifierbasedonthesimultaneouslycollectedEEGwaveandthe
mat[16].Forfeatureextraction,ImageNet-trainedRegNet-32[38]is humanbehaviortobrakelabel.Thisrefinementutilizedasmaller,
utilizedonbothimageandBEVLiDARdata.Trainingincludesangu- yethighlynuanceddatasetcomprising12,000framesofhuman-
laraugmentationby¬±20degreesonLiDAR,akintoTransfuser[10], deriveddata.Theobjectivewastointegratehumandecision-making
nuancesintothemachineperceptionmodel,therebyenhancingits
2https://github.com/carla-simulator/carla abilitytointerpretcomplexdrivingscenarios.Whenweconduct
3https://github.com/HARPLab/DReyeVR thehuman-guideddrivingtraining,weusetheinitiallearningrateEnhancingEnd-to-EndAutonomousDrivingSystemsThroughSynchronizedHumanBehaviorData BCIMM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia
Table1:DrivingEvaluationonLongSet6withHumanGuidance,whereDTdenotesdecisiontransformer.
HumanGuidance DT Network DS‚Üë RC‚Üë(%) IS‚Üë
√ó √ó TransFuser 46.95¬±5.81 89.64¬±2.08 0.52¬±0.08
√ó √ó MaskFuser 49.05¬±6.02 92.85¬±0.82√ó 0.56¬±0.07
√ó ‚úì InterFuser 49.86¬±4.37 91.05¬±1.92 0.60¬±0.08
√ó ‚úì MaskFuser+DT 50.63¬±5.98 92.89¬±0.80 0.62¬±0.08
Eye-Attention ‚úì MaskFuser+DT 51.39¬±5.33 92.01¬±0.97 0.65¬±0.09
Intention ‚úì MaskFuser+DT 50.06¬±5.81 90.97¬±1.80 0.63¬±0.09
Eye-Attention+Intention ‚úì MaskFuser+DT 50.59¬±6.12 91.39¬±0.80 0.63¬±0.09
Eye-Attention+FakeIntention(GT) ‚úì MaskFuser+DT 51.28¬±6.17 92.03¬±0.98 0.64¬±0.08
Expert 75.83¬±2.45 89.82¬±0.59 0.85¬±0.03
as1√ó10‚àí4√ó BatchSize forthedecisiontransformer.Theweight changingsituations.However,theimprovementbroughtbythe
512
decayiskeptthesameas0.07. humanattentionguidanceunderscoresthepotentialofleveraging
nuancedhumanbehavioralcuestoenhanceautonomousdriving
4.3 EvaluationwithHumanGuidance systems.
EvaluationBenchmark:Wedirectlykeeptheexperimentalset-
5 LIMITATION
tingsthesameasthepuremachinedrivingevaluationonCARLA
LongSet6[10].Weconductourdetailedablationandcomparison Intheexperimentalsection,althoughusingeye-trackingdatahasa
basedontheLongeset6BenchmarkproposedbyTransFuser[10], positiveimpactonautonomousdrivingperformance,usinghuman-
whichchoosesthe6longestroutespertownfromtheofficially intentiondatadoesnot.Weattributethisissuetotwomainfactors:
releasedroutesfromtheCARLAChallenge2019andsharesquitea First, human intention relies on a pre-trained EEG recognition
similaritywiththeofficialevaluation.
model,whichcanonlyachievea60‚àí70%accuracyrateinidenti-
QuantativeEvaluationForbothonlineevaluationandofflineeval- fyingpeople‚Äôsdangerorbrakingintentions,therebyintroducing
uation,wefollowtheofficialevaluationmetricstocalculatethree noise.Second,comparedtotheabundantmachine-generatedau-
mainmetrics,RouteCompletion(RC),InfractionScore(IS), tonomousdrivingdata,collectingdriverintentiondataisrelatively
andDrivingScore(DS).TheRCscoreisthepercentageofroutedis- costly.Weutilizeddrivingdatafromonly12individuals,which
tancecompleted.GivenùëÖ ùëñasthecompletionbytheagentinrouteùëñ, maynotbesufficienttosignificantlyinfluencethetrainingoflarge
RCiscalculatedbyaveragingthecompletionrateùëÖùê∂ = ùëÅ1 (cid:205) ùëñùëÅùëÖ ùëñ. models.Moreover,intentiondata,comparedtohumanattention,
TheISiscalculatedbyaccumulatingtheinfractionsùëÉ ùëñ incurred providessparsersupervision,necessitatingmoresuperviseddata
fortraining.Thisaspectwarrantsfurtherdiscussioninfuturework.
bytheagentduringcompletingtheroutes.Thedrivingscoreis
calculatedbyaccumulatingroutecompletionùëÖ ùëñ withinfraction
multiplierùëÉ ùëñ asùê∑ùëÜ = ùëÅ1 (cid:205) ùëñùëÅùëÖ ùëñùëÉ ùëñ.Wealsocalculatethedetailedin- 6 CONCLUSION
fractionstatisticaldetailsaccordingtotheofficialcodes.Wereport Inthispaper,wedelveintotheutilizationofhumanbehavioraldata
theevaluationwiththehuman-guidedfine-tuningmodelsonthese toimproveautonomousdrivingperformance.Weexploreharness-
metricsinTable1. inginsightsfromhumandriverstoenhancethedrivingsystem‚Äôs
ItisobservedfromTable1thattheintegrationofhuman-guided capabilitiesbytwoaspects1)observinglikeahuman,and2)deci-
fine-tuningwiththeMaskFuserdecisiontransformer(DT)leads sionlikeahuman.Toachievethis,wecollectedeye-trackingand
toanoticeableimprovementinDrivingScore(DS).Theimprove- brake&cognitiondatafrom12humansubjectsbylettingmachines
mentismainlybroughtbyhumanattentionguidance,yetforthe andhumansdrivethesameroute.Theexperimentalresultsindicate
humanintentionguidance,wedidn‚Äôtobserveaclearimprovement. thatguidingmachineattentionwithhumanattentioncanleadto
Here,theEye-Attentionguidancecombinedwiththedecisiontrans- aclearimprovementinperformance.However,theexperiments
formerachievesthehighestmeandrivingscoreof51.39,indicating didnotdemonstratethathumancognitiondatacouldsignificantly
theeffectivenessofincorporatinghumanattentiontoguidethe enhanceoutcomes.Integratinggranularhumansupervisioninto
drivingmodel.Furthermore,theadditionofintentiondataslightly machine driving merits further in-depth investigation. Such an
reducesthedrivingscoreunderbothsettings.Thisphenomenon approachisbeneficialforincreasingthemachine‚Äôstrustworthi-
isstillreasonablebecauseoftworeasons.1)Thehumanintention nesstohumanswhilemakingitsdecision-makingprocessesmore
recognitionisnotasaccurateatthisstageasthesuperviselabel. anthropomorphic.
Duringourexperiments,thehumanintentionclassifierhasanaccu-
racybetween60%‚àí70%whichisstillcomparedlowatthisstage.2) REFERENCES
Along-existingproblemappearsthatthesimultaneouslycollected [1] YamanAlbadawi,MaenTakruri,andMohammedAwad.2022.Areviewofrecent
developmentsindriverdrowsinessdetectionsystems.Sensors22,5(2022),2069.
datahassmalltimeshiftsbetweenthehumantimestampandthe
[2] EmadAlyan,StefanArnau,JulianEliasReiser,StephanGetzmann,Melanie
machinetimestamp,thiswillleadtothewronglabelsinrapidly Karthaus,andEdmundWascher.2023. Blink-relatedEEGactivitymeasuresBCIMM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia YiqunDuan,ZhuoliZhuang,JinzhaoZhou,Yu-ChengChang,Yu-KaiWang,andChin-TengLin
cognitiveloadduringproactiveandreactivedriving. ScientificReports13,1 [28] HaoranSong,WenchaoDing,YuxuanChen,ShaojieShen,MichaelYuWang,and
(2023),19379. QifengChen.2020.Pip:Planning-informedtrajectorypredictionforautonomous
[3] ZehongCao,Chun-HsiangChuang,Jung-KaiKing,andChin-TengLin.2019. driving.InEuropeanConferenceonComputerVision.Springer,598‚Äì614.
Multi-channelEEGrecordingsduringasustained-attentiondrivingtask.Scientific [29] JingkaiSun,QiangZhang,YiqunDuan,XiaoyangJiang,ChongCheng,and
data6,1(2019),19. RenjingXu.2024. Prompt,plan,perform:Llm-basedhumanoidcontrolvia
[4] FelipeCodevilla,MatthiasM√ºller,AntonioL√≥pez,VladlenKoltun,andAlexey quantizedimitationlearning.In2024IEEEInternationalConferenceonRobotics
Dosovitskiy.2018.End-to-enddrivingviaconditionalimitationlearning.InIEEE andAutomation(ICRA).IEEE,16236‚Äì16242.
InternationalConferenceonRoboticsandAutomation.IEEE,4693‚Äì4700. [30] LeiTai,JingweiZhang,MingLiu,andWolframBurgard.2018.Sociallycompli-
[5] FelipeCodevilla,EderSantana,AntonioML√≥pez,andAdrienGaidon.2019.Ex- antnavigationthroughrawdepthinputswithgenerativeadversarialimitation
ploringthelimitationsofbehaviorcloningforautonomousdriving.InProceedings learning.In2018IEEEinternationalconferenceonroboticsandautomation(ICRA).
oftheIEEE/CVFInternationalConferenceonComputerVision.9329‚Äì9338. IEEE,1111‚Äì1117.
[6] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,Xi- [31] ArdiTampuu,TambetMatiisen,MaksymSemikin,DmytroFishman,andNaveed
aohuaZhai,ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,Georg Muhammad.2020.Asurveyofend-to-enddriving:Architecturesandtraining
Heigold,SylvainGelly,etal.2020.Animageisworth16x16words:Transformers methods.IEEETransactionsonNeuralNetworksandLearningSystems(2020).
forimagerecognitionatscale.arXivpreprintarXiv:2010.11929(2020). [32] MarinToromanoff,EmilieWirbel,andFabienMoutarde.2020.End-to-endmodel-
[7] YiqunDuan,CharlesChau,ZhenWang,Yu-KaiWang,andChin-tengLin.2024. freereinforcementlearningforurbandrivingusingimplicitaffordances.In
Dewave:Discreteencodingofeegwavesforeegtotexttranslation.Advancesin ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
NeuralInformationProcessingSystems36(2024). 7153‚Äì7162.
[8] YiqunDuan,XiandaGuo,ZhengZhu,ZhenWang,Yu-KaiWang,andChin-Teng [33] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,
Lin.2024. MaskFuser:MaskedFusionofJointMulti-ModalTokenizationfor AidanNGomez,≈ÅukaszKaiser,andIlliaPolosukhin.2017. Attentionisall
End-to-EndAutonomousDriving.arXivpreprintarXiv:2405.07573(2024). youneed.InAdvancesinNeuralInformationProcessingSystems.5998‚Äì6008.
[9] YiqunDuan,QiangZhang,andRenjingXu.2024.PromptingMulti-ModalTokens [34] Yu-KaiWang,Tzyy-PingJung,andChin-TengLin.2015.EEG-basedattention
toEnhanceEnd-to-EndAutonomousDrivingImitationLearningwithLLMs. trackingduringdistracteddriving. IEEEtransactionsonneuralsystemsand
arXivpreprintarXiv:2404.04869(2024). rehabilitationengineering23,6(2015),1085‚Äì1094.
[10] Prakashetal.[n.d.]. Multi-ModalFusionTransformerforEnd-to-EndAu- [35] MingyunWen,JisunPark,andKyungeunCho.2020. Ascenariogeneration
tonomousDriving.InCVPR2021.7077‚Äì7087. pipelineforautonomousvehiclesimulators. Human-centricComputingand
[11] DeepakGopinath,GuyRosman,SimonStent,KatsuyaTerahata,LukeFletcher, InformationSciences10,1(2020),1‚Äì15.
BrennaArgall,andJohnLeonard.2021.Maad:Amodelanddatasetfor"attended [36] JingdaWu,ZhiyuHuang,ZhongxuHu,andChenLv.2023. Towardhuman-
awareness"indriving.InProceedingsoftheIEEE/CVFInternationalConferenceon in-the-loopAI:Enhancingdeepreinforcementlearningviareal-timehuman
ComputerVision.3426‚Äì3436. guidanceforautonomousdriving.Engineering21(2023),75‚Äì91.
[12] ShaneGriffith,KaushikSubramanian,JonathanScholz,CharlesLIsbell,and [37] JingdaWu,ZhiyuHuang,WenhuiHuang,andChenLv.2022. Prioritized
AndreaLThomaz.2013. Policyshaping:Integratinghumanfeedbackwith experience-basedreinforcementlearningwithhumanguidanceforautonomous
reinforcementlearning. Advancesinneuralinformationprocessingsystems26 driving.IEEETransactionsonNeuralNetworksandLearningSystems(2022).
(2013). [38] JingXu,YuPan,XinglinPan,StevenHoi,ZhangYi,andZenglinXu.2022.
[13] JonathanHoandStefanoErmon.2016.Generativeadversarialimitationlearning. RegNet:self-regulatednetworkforimageclassification. IEEETransactionson
Advancesinneuralinformationprocessingsystems29(2016). NeuralNetworksandLearningSystems(2022).
[14] JunjieHuang,GuanHuang,ZhengZhu,andDalongDu.2021. Bevdet:High- [39] QiangZhang,PeterCui,DavidYan,JingkaiSun,YiqunDuan,ArthurZhang,
performancemulti-camera3dobjectdetectioninbird-eye-view.arXivpreprint andRenjingXu.2024. Whole-bodyhumanoidrobotlocomotionwithhuman
arXiv:2112.11790(2021). reference.arXivpreprintarXiv:2402.18294(2024).
[15] WBradleyKnoxandPeterStone.2011. Augmentingreinforcementlearning [40] QingwenZhang,MingkaiTang,RuoyuGeng,FeiyiChen,RenXin,andLujia
withhumanfeedback.InICML2011WorkshoponNewDevelopmentsinImitation Wang.2022. MMFN:Multi-Modal-Fusion-NetforEnd-to-EndDriving. arXiv
Learning(July2011),Vol.855.3. preprintarXiv:2207.00186(2022).
[16] AlexHLang,SourabhVora,HolgerCaesar,LubingZhou,JiongYang,andOs- [41] FengZhou,XJessieYang,andJoostCFDeWinter.2021. Usingeye-tracking
carBeijbom.2019. Pointpillars:Fastencodersforobjectdetectionfrompoint datatopredictsituationawarenessinrealtimeduringtakeovertransitionsin
clouds.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern conditionallyautomateddriving.IEEEtransactionsonintelligenttransportation
Recognition.12697‚Äì12705. systems23,3(2021),2284‚Äì2295.
[17] ZhijianLiu,HaotianTang,AlexanderAmini,XinyuYang,HuiziMao,Daniela [42] JinzhaoZhou,YiqunDuan,ZiyiZhao,Yu-ChengChang,Yu-KaiWang,Thomas
Rus,andSongHan.2022. BEVFusion:Multi-TaskMulti-SensorFusionwith Do,andChin-TengLin.2024.TowardsLinguisticNeuralRepresentationLearning
UnifiedBird‚Äôs-EyeViewRepresentation.arXivpreprintarXiv:2205.13542(2022). andSentenceRetrievalfromElectroencephalogramRecordings.arXivpreprint
[18] IlyaLoshchilovandFrankHutter.2016.Sgdr:Stochasticgradientdescentwith arXiv:2408.04679(2024).
warmrestarts.arXivpreprintarXiv:1608.03983(2016). [43] JinzhaoZhou,JustinSia,YiqunDuan,Yu-ChengChang,Yu-KaiWang,andChin-
[19] IlyaLoshchilovandFrank.Hutter.2017.Decoupledweightdecayregularization. TengLin.2024.MaskedEEGModelingforDrivingIntentionPrediction.arXiv
arXivpreprintarXiv:1711.05101(2017). preprintarXiv:2408.07083(2024).
[20] MahdiRezaeiandReinhardKlette.2014.Lookatthedriver,lookattheroad:No
distraction!noaccident!.InProceedingsoftheIEEEconferenceoncomputervision
andpatternrecognition.129‚Äì136.
[21] AvishkarSaha,OscarMendez,ChrisRussell,andRichardBowden.2022.Translat-
ingimagesintomaps.In2022InternationalConferenceonRoboticsandAutomation
(ICRA).IEEE,9200‚Äì9206.
[22] StefanSchaal.1999.Isimitationlearningtheroutetohumanoidrobots?Trends
incognitivesciences3,6(1999),233‚Äì242.
[23] HaoShao,LetianWang,RuobingChen,HongshengLi,andYuLiu.2022.Safety-
enhancedautonomousdrivingusinginterpretablesensorfusiontransformer.
arXivpreprintarXiv:2207.14024(2022).
[24] HaoShao,LetianWang,RuobingChen,HongshengLi,andYuLiu.2023.Safety-
enhancedautonomousdrivingusinginterpretablesensorfusiontransformer.In
ConferenceonRobotLearning.PMLR,726‚Äì737.
[25] JiaminShi,TangyikeZhang,JunxiangZhan,ShitaoChen,JingminXin,andNan-
ningZheng.2023.EfficientLane-changingBehaviorPlanningviaReinforcement
LearningwithImitationLearningInitialization.In2023IEEEIntelligentVehicles
Symposium(IV).IEEE,1‚Äì8.
[26] JustinSia,Yu-ChengChang,Chin-TengLin,andYu-KaiWang.2023. EEG-
BasedTNNforDriverVigilanceMonitoring.In2023IEEESymposiumSerieson
ComputationalIntelligence(SSCI).IEEE,53‚Äì57.
[27] GustavoSilvera,AbhijatBiswas,andHennyAdmoni.2022.DReyeVR:Democra-
tizingVirtualRealityDrivingSimulationforBehavioural&InteractionResearch.
InProceedingsofthe2022ACM/IEEEInternationalConferenceonHuman-Robot
Interaction.639‚Äì643.