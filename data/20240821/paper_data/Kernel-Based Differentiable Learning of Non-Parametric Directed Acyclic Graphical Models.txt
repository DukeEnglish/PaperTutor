ProceedingsofMachineLearningResearch246:1–20,2024 ProbabilisticGraphicalModels(PGM)
Kernel-Based Differentiable Learning of Non-Parametric Directed
Acyclic Graphical Models
YurouLiang YUROU.LIANG@TUM.DE
OleksandrZadorozhnyi OLEKSANDR.ZADOROZHNYI@TUM.DE
MathiasDrton MATHIAS.DRTON@TUM.DE
TechnicalUniversityofMunichandMunichCenterforMachineLearning,Germany
Editors:J.H.P.Kwisthout&S.Renooij
Abstract
Causaldiscoveryamountstolearningadirectedacyclicgraph(DAG)thatencodesacausalmodel.
Thismodelselectionproblemcanbechallengingduetoitslargecombinatorialsearchspace,par-
ticularly when dealing with non-parametric causal models. Recent research hassought to bypass
thecombinatorialsearchbyreformulatingcausaldiscoveryasacontinuousoptimizationproblem,
employingconstraintsthatensuretheacyclicityofthegraph. Innon-parametricsettings,existing
approachestypicallyrelyonfinite-dimensionalapproximationsoftherelationshipsbetweennodes,
resultinginascore-basedcontinuousoptimizationproblemwithasmoothacyclicityconstraint. In
thiswork,wedevelopanalternativeapproximationmethodbyutilizingreproducingkernelHilbert
spaces(RKHS)andapplyinggeneralsparsity-inducingregularizationtermsbasedonpartialderiva-
tives. Within this framework, we introduce an extended RKHS representer theorem. To enforce
acyclicity, we advocate the log-determinant formulation of the acyclicity constraint and show its
stability. Finally, weassesstheperformanceofourproposedRKHS-DAGMAprocedurethrough
simulationsandillustrativedataanalyses.
Keywords:Causaldiscovery;graphicalmodel;kernelmethods;RKHS;structuralequationmodel.
1. Introduction
Structural equation models (SEMs) based on directed acyclic graphs (DAGs) have found wide-
spread applications ranging from computational biology (Zhang et al., 2023) to manufacturing
(Go¨bler et al., 2024) and finance (Ji et al., 2018). To represent the joint dependence structure,
each variable is modeled as a function of a subset of the other variables and noise. In this setting,
DAG-based models assume the absence of causal feedback loops. Although this assumption can
berestrictive,itiscrucialfordefiningnon-linearmodels. Indeed,itisoftenunclearwhethercyclic
systemsofstructuralequationshaveauniquesolutionoriftheyadmitasolutionatall.
Inmanyapplications,theunderlyingDAGisunknown,andmethodsforcausaldiscovery,which
learn the DAG from data, offer useful insights. Numerous algorithms have been proposed for
causal discovery; see, e.g., Drton and Maathuis (2017) or Spirtes and Zhang (2019). A classical
constraint-basedapproachreliesontestingconditionalindependences(SpirtesandGlymour,1991;
Tsamardinosetal.,2003;MargaritisandThrun,1999). Anotherprominentapproachisscore-based
algorithms (Heckerman et al., 1995; Chickering, 2002). In this work, we focus on a score-based
approach. Specifically,wewilltakeuparecentthemethataimstofindaDAGminimizingamodel
selection score through a continuous optimization problem with a continuous acyclicity constraint
appliedtoaweightedadjacencymatrixW. ThisapproachwasinitiatedintheNOTEARSalgorithm
(Zheng et al., 2018), which assumes a linear SEM and uses an exponential acyclicity constraint
transformingthecombinatorialoptimizationproblemintoacontinuousonethatissolvedusingan
augmented Lagrangian scheme. Several follow-up works have proposed alternative characteriza-
tionsofacyclicity(Yuetal.,2019;Nazaretetal.,2023;Ngetal.,2020). Onesuchworkintroduced
©2024Y.Liang,O.Zadorozhnyi&M.Drton.
4202
guA
02
]LM.tats[
1v67901.8042:viXraLIANGZADOROZHNYIDRTON
theDAGMAalgorithm(Belloetal.,2022), whichisgenerallyfasterthanNOTEARS.Inlinewith
relatedliterature,werefertothisbroadapproachasdifferentiablecausaldiscovery.
Zhengetal.(2020)extendedthismethodologytonon-parametricsettings. Specifically,theau-
thorsmodeledeachvariableasanon-parametricfunctionoftheothervariablesandnoise,proposing
to approximate the non-parametric functions by multi-layer perceptions or via a (truncated) basis
expansionintheoriginalfunctionalspace—i.e.,thespaceoffunctionswhosederivativesaresquare-
integrableoverthedomain(anexpansionisthenpossibleviathetrigonometricbasisoffunctions).
Theythenminimizethecorrespondingresiduallosssubjecttotheexponentialacyclicityconstraint.
Inourwork,webuildonthisapproachunderadditionalassumptionsthatthenon-parametricfunc-
tionsarecontinuouslydifferentiable,takingupaperspectiveof(Gaussian)RKHS.
IntheMLPframeworkofZhengetal.(2020),theentryW oftheweightedadjacencymatrix
kj
isdefinedastheL normofthekthcolumnoftheweightmatrixinthefirsthiddenlayerofthejth
2
MLP.Whenapproximatingbybasisexpansions,thegeneralnon-parametricmodelisassumedtobe
anadditivemodel. TheentriesoftheweightedadjacencymatrixaredefinedastheL normofthe
2
coefficientscorrespondingtothebasisapproximation.
BoththeMLPandthebasisexpansionmethodsinZhengetal.(2020)leadtofinite-dimensional
optimization problems in terms of neural network weights or basis coefficients. The current MLP
approximation is sensitive to the number of hidden units: while increasing the size of the hidden
layersincreasestheflexibilityofMLPfunctions,largernetworksrequiremoresamplesforaccurate
estimation (Zheng et al., 2020). Moreover, the MLP approximation relies on random initialization
of weights, which introduces randomness in results (see Figure 2 in Waxman et al., 2024). Fine-
tuningthearchitectureofaneuralnetworkis,thus,anon-trivialtask. Ontheotherhand,thecurrent
basisexpansionapproximationsarerestrictedthroughafocusonadditivemodels.
Contributions. In this work, we present a novel kernel-based methodology for differentiable
causaldiscoveryinnon-parametricsettings. Ourcontributionscanbesummarizedasfollows:
• We approximate each non-parametric function that represents the dependency structure be-
tweenrandomvariablesusinganRKHSwithadifferentiablekernelk. Weestablishaversion
ofanRKHSRepresenterTheoremforanempiricalacyclicity-constrainedoptimizationprob-
lem, similar to that of Rosasco et al. 2013 in the statistical learning context. Given data
(xi)n , thisleadstooptimizingfunctionsthatarecombinationsofevaluationsofthekernel
i=1
anditspartialderivatives:
n n d (cid:12)
(cid:88)
α
ik(x,xi)+(cid:88)(cid:88)
β
ai∂k ∂(x sa,s)(cid:12)
(cid:12)
(cid:12)
. (1)
i=1 i=1 a=1 s=xi
• Letf representthedependencystructurebetweenj-thrandomvariableandtheotherrandom
j
variables. If x (cid:55)→ f (x) is continuously differentiable for all x in a connected and compact
j
sample space X, then
∂fj
= 0 implies that f does not depend on x . Thus, we define
∂x j k
k
the weighted adjacency matrix directly via the partial derivatives of the functions f . This
j
approachismodel-agnosticinthesensethattheweightsdonotrefertoapproximationstof .
j
• We base our optimization on the DAGMA method (Bello et al., 2022) and adopt the log-
determinantacyclicityconstrainth ,forwhichwedemonstratestableoptimizationbehavior
ldet
ontheboundaryofthedomain.
• Weexplorethebehaviorofkernel-baseddifferentiablecausaldiscoveryinsimulationexper-
iments as well as the collection of cause-effect datasets from Mooij et al. (2016). The code
forourexperimentsisavailableonthefirstauthor’sGitHubsite.1
1.https://github.com/yurou-liang/RKHS-DAGMA
2LEARNINGNON-PARAMETRICDIRECTEDACYCLICGRAPHICALMODELS
Outline. Section 2 sets up notation, reviews the DAG learning problem, and presents basic facts
aboutRKHS.Section3summarizesexistingacyclicityconstraintsanddiscussesthestabilityofthe
log-determinantacyclicityconstraint,onwhichourworkisbased. Section4introducesthesparsity
regularizerandexaminestheextendedRKHSrepresentertheoremfortheconstrainedoptimization
problem with acyclicity constraint. Section 5 outlines the resulting constrained optimization prob-
lem and introduces the RKHS-DAGMA algorithm for learning sparse nonparametric graphs. In
Section6,wecomparetheperformanceofRKHS-DAGMAwithdifferentversionsofnonparamet-
ricNOTEARS(Zhengetal.,2020)innumericalexperiments. Additionaldetailsontheexperiments
andproofsaregivenintheAppendix.
2. Background
2.1. StructuralEquationModelandDifferentiableCausalDiscovery
Let X = (X ,...,X ) be a random vector taking values in X ⊂ Rd defined on some proba-
1 d
bility space (Ω,F,P). We assume X is a bounded connected non-empty open set. Let [d] :=
{1,2,...,d}. Consider a directed acyclic graph G = (V,E) with vertex set V = [d] and edge set
E ⊂ V ×V. Asusual,pa(i) = {j ∈ V : (j,i) ∈ E}isthesetofparentsofavertexi ∈ V. Fora
subsetA ⊂ [d],letX = (X ) . WhenA = ∅,wesetX ≡ 0.
A i i∈A A
In a graphical model, each variable X exhibits a constrained stochastic relationship with the
j
other coordinates of random vector X := (X )d (Maathuis et al., 2019). Presenting the model
j j=1
throughstructuralequationsandassumingadditivenoise,themodelforDAGGpostulatesthat
X = f (X)+ε , j = 1,...,d, (2)
j j j
where each measurable functionf : Rd (cid:55)→ Rdepends onlyon thesubvectorX , and(ε )
j pa(i) j j∈[d]
are mutually independent stochastic error terms. In this model, the conditional expectations are
E[X |X ] = f (X),wherewenoteagainthatf : X → RdoesnotdependonX ifk ∈/ pa(j).
j pa(j) j j k
The DAG learning problem for the considered models may be formulated as follows. Let
X ∈ Rn×d be a data matrix whose rows (xi)n represent n i.i.d. observations. Let xi be the
i=1 j
j-th coordinate of the i-th observation. We denote the loss function by ℓ : Xn × Yn (cid:55)→ R ,
+
whereY ⊂ Ristheimagespaceforpredictions. Thetypicallossfunctionistheleastsquaresloss
ℓ(y,y) = ∥y−y∥2. Thegoalistoestimatef = (f ,...,f )byminimizingthescorefunction
(cid:98) (cid:98) 2 1 d
d
1 (cid:88)
L(f) := ℓ(X ,f (X))subjecttothedependenciesinf correspondingtoaDAG, (3)
j j
2n
j=1
where,inslightabuseofnotation,f (X) = (cid:0) f (cid:0) x1(cid:1) ,...,f (xn)(cid:1) . Weassumethateachf iscon-
j j j j
tinuouslydifferentiableandthatf anditsderivativearebothsquare-integrable. AsinWaxmanetal.
j
(cid:13) (cid:13)
(2024), we define the weighted adjacency matrix W ∈ Rd×d with entries W(f) := (cid:13)∂fj(·)(cid:13) .
kj (cid:13) ∂x (cid:13)
k L2
Thisdefinitionismodel-agnosticinthatitdoesnotrefertoanyparticularfamilyofapproximation
modelsforthefunctionf (aswasdone,e.g.,intheMLPsetupofZhengetal.,2020).
j
In the sequel, P denotes the joint distribution of the random vector X. We write C1(X) for
X
thespaceofcontinuouslydifferentiablefunctionsoverX andL (X)forthespaceof(equivalence
2
classes of) square-integrable functions on X, i.e., functions f : X → R with (cid:82) f2(x)P (dx) <
X X
∞. WewillmostlydropdependenceonthedomainandontheunderlyingdistributionP andsim-
X
plywriteL := L (X)orL := L (Rd)whenthedomainisclearfromthecontext. Furthermore,
2 2 2 2
weuse∥·∥ todenotetheessentialsupremumnormw.r.t.Lebesquemeasureλona(subset)X ⊂ R.
∞
Forg : X (cid:55)→ RinC1(X),wewrite ∂ g(x)foritspartialderivative(asamapx (cid:55)→ ∂ g(x))and
∂x ∂x
k k
3LIANGZADOROZHNYIDRTON
∂ g(x)| for the derivative’s value at point x = s. Finally, we denote the i−th smallest eigen-
∂x x=s
k
value of a real matrix A ∈ Rd×d by λ (A); so, |λ (A)| ≤ |λ (A)| ≤ ··· ≤ |λ (A)|. The spectral
i 1 2 d
radiusρ(A) = |λ (A)|isthelargesteigenvalueofAinmagnitude.
d
2.2. KernelsandRKHS
To estimate the structural equation model in (2), we use the toolbox of kernel-based algorithms
(Steinwart and Christmann, 2008, Chap. 4). For a given vector space X, let k : X × X (cid:55)→ R
be a symmetric function such that for any n ∈ N, {xi}n ∈ Xn, and c ∈ Rn, it holds that
i=1
c⊤Kc ≥ 0, where K = (cid:0) k(cid:0) xi,xj(cid:1)(cid:1)n ∈ Rn×n is the kernel matrix. Such a function k(·,·)
i,j=1
is termed a kernel. Its values can be represented as an inner product in a Hilbert space H , i.e.,
0
k(cid:0) xi,xj(cid:1)
=
(cid:10) ϕ(xi),ϕ(xj)(cid:11)
, where ϕ : X (cid:55)→ H is the feature map. To every kernel k, we can
associateareproducingkernH e0 lHilbertspace(RKHS0 )H,whichisaspaceoffunctionsf : X (cid:55)→ R
forwhichthereproducingpropertyholds: f(x) = ⟨f,k(·,x)⟩ forallx ∈ X,f ∈ H.
H
LetH beanRKHS,andletk beingitscorrespondentreproducingkernelk : X ×X (cid:55)→ R. Let
R (f ) := 1 (cid:80)n E(xi,f (xi)) be the empirical risk for convex loss-function E : R×Y (cid:55)→
E,D j n i=1 j j
R . Consider the empirical risk with function complexity regularizer based on an arbitrary non-
+
decreasingfunctionJ : R (cid:55)→ R :
+
(cid:0) (cid:1)
R (f )+J ∥f ∥ . (4)
E,D j j H
Then(see,e.g.Scho¨lkopfetal.,2001, Thm1),theminimizerof(4)canbewrittenas
n
f(cid:98)j(·) = (cid:88) α ijk(·,xi),
i=1
whereαj ∈ Rn. Furthermore,thereproducingpropertyoftheRKHSdirectlyimpliesthatf(cid:98)j(x) =
(cid:80)n αjk(x,xi)forallx ∈ X.
i=1 i
3. AcyclicityConstraints
The earliest work that transfers the discrete acyclicity constraint in Problem (3) to a continuous
differentiable constraint is NOTEARS (Zheng et al., 2018). It introduced the following acyclicity
constraintbasedonthetrace-exponentialoftheweightedadjacencymatrixW ∈ Rd×d:
h (W) = Tr(exp(W ◦W))−d,
exp
where◦istheHadamardproduct. SinceW ◦W hasnonnegativeentries,wemayconsiderh also
exp
asafunctionofnonnegativematricesA ∈ Rd×d,A = W ◦W. Usingthefactthatx (cid:55)→ exp(x)can
≥0
be represented by uniformly over R convergent power series and by the linearity of the trace one
can generalize the acyclicityconstraint to the so-calledPower Series Trace Family(PST) (see also
Nazaretetal.,2023,Def. 1). Themembersofthefamilycanbeexpressedasfollows:
∞
(cid:88)
h(A) = a Tr[Ak],
k
k=1
where(a k) k∈N∗ ∈ RN ≥∗ 0,andA ∈ Rd ≥× 0d. Table1listssomeexamples.
4LEARNINGNON-PARAMETRICDIRECTEDACYCLICGRAPHICALMODELS
Name α h
k a
h expm k1 ! Trexp(A)−d Name h a
h 1 −logdet(I −A)
log k d h spectral |λ d(A)|
h inv 1 (cid:16) (cid:17) Tr(I d−A)−1 h ldet −logdet(sI d−A)+dlogs
h d Tr(I +A)d−d
binom k d
Table2: Spectral-basedacyclicityconstraints
Table1: PSTacyclicityconstraints
Anotherclassofconstraints,thespectral-basedacyclicityconstraints,wasdevelopedbasedon
thefactthatthespectralradiusofanon-negativeweightedadjacencymatrixAiszeroifandonlyif
AcorrespondstoaDAG(Nazaretetal.,2023;Belloetal.,2022). Table2givesexamples.
Nazaret et al. (2023) introduced three criteria for constraints to ensure stable optimization be-
haviorandshowedthatthePTSacyclicityconstraintsarenotstableduringtheoptimizationprocess.
Belloetal.(2022)studiedtheacyclicityconstrainth ,basedonthelog-determinantoftheweight-
ldet
matrix A. More precisely, for s > 0, consider space of matrices Ws := {A ∈ Rd×d : s > ρ(A)}
anddefinehs : Ws → Ras
ldet
hs (A) := −logdet(sI −A)+dlogs. (5)
ldet d
Bello et al. (2022) show favorable performance of h in comparison to the constraints h and
ldet exp
h . Wewilladopttheconstrainth inthesequel. Weexplainourchoicefromtheoptimization
poly ldet
perspectiveinthefollowingtheorem.
Theorem1 Theconstraintfunctionhs (·)from(5)satisfiesthefollowingstabilityproperties:
ldet
ForallA ∈ Rd×d itholds:
≥0
• V-stableIfhs (A) ̸= 0,thenforε → 0+,hs (εA) ≥ cεforsomepositiveconstantc.
ldet ldet
• D-stable hs (A) and ∇hs (A) are well-defined where ▽hs (A) = (sI − A)−T, with
ldet ldet ldet d
▽hs (A) = 0ifandonlyifAisaDAG.
ldet
Finally,hs isanacyclicityconstraintinthesensethat
ldet
hs (A) ≥ 0,withhs (A) = 0ifandonlyifgraphgeneratedbymatrixAisaDAG.
ldet ldet
NoPSTconstraintisV-stableford ≥ 2,i.e.,thereexistsA ∈ Rd×d,h(εA) = O(cid:0) εd(cid:1) asε → 0,
ε > 0 (Nazaret et al., 2023, Theorem 2). In contrast, V-stability ensures hs does not vanish
ldet
rapidly to 0, and D-stability ensures that hs and its gradient exists. Although h is both V-
ldet spectral
stable and D-stable (Nazaret et al., 2023), we observe that in practice, h (W) and hs (W)
spectral ldet
operate on distinctly different scales, with h (W) being substantially larger than hs (W).
spectral ldet
This considerable difference in magnitude raises challenges in assessing whether h (W) can
spectral
beregardedassufficientlysmalltobeconsideredzero. Thus,wechoosehs (W).
ldet
4. OverallLearningObjective
4.1. SparsityRegularizer
In applications, we often expect the random variables X to have only a few parent variables and,
j
thus, invoke a sparsity regularizer. Recall that we assume that the functions f : X → R are in the
j
5LIANGZADOROZHNYIDRTON
classC1(X),andthefunctionsf andtheirderivativesarebothsquare-integrable. Weconsiderthe
j
regularizer(comparealsoRosascoetal.2013)
(cid:115)
Ω 1(f j) = (cid:88)d (cid:13) (cid:13) (cid:13)∂f j(·)(cid:13) (cid:13) (cid:13) = (cid:88)d (cid:90) (cid:18) ∂f j(x)(cid:19)2 P X(dx). (6)
(cid:13) ∂x (cid:13) ∂x
k=1 k L2 k=1 X k
Todevelopadata-baseddecisionruleweneedtoconsideranempiricalcounterpartforthe∥·∥
L2
norm of the derivative. Thus, we form an empirical estimate based on the data X = {xi}n , by
i=1
setting
(cid:118)
(cid:13)
(cid:13)∂f
j(·)(cid:13)
(cid:13)
(cid:117)
(cid:117)1
(cid:88)n (cid:18)
∂f
j(xi)(cid:19)2
(cid:13) (cid:13) := (cid:116) .
(cid:13) ∂x (cid:13) n ∂x
k n i=1 k
Thentheempiricalestimateof(6)is
d (cid:13) (cid:13)
ΩD
1
(f j) =
(cid:88)(cid:13)
(cid:13)
(cid:13)∂ ∂f xj(·)(cid:13)
(cid:13)
(cid:13)
.
k=1 k n
Similarly,theempiricalestimateofthecoefficientW oftheweightedadjacencymatrixis
jk
(cid:13) (cid:13)
WD = (cid:13) (cid:13)∂f j(·)(cid:13) (cid:13) .
kj (cid:13) ∂x (cid:13)
k n
4.2. ConstrainedEmpiricalOptimizationProblemSolvedbyKernelMethods
Foreachj ∈ [d],wefurtherassumef : X → R,j ∈ [d]tobeinareproducingkernelHilbertspace
j
(RKHS) H generated by a bounded continuously-differentiable kernel k on X and use the term
λ∥f ∥2 topenalizefunctioncomplexity. Thenweaimtominimizethefollowinglossfunction:
j H
d
(cid:88) 1
ℓ(Xj,f (X))+τ(2ΩD(f )+λ∥f ∥2 )s.t. hs (WD) = 0, (7)
2n j 1 j j H ldet
j=1
where τ,λ are positive numbers and s > 0 is some fixed number (typically set to 1). Following
Rosasco et al. (2013), we show a version of the RKHS Representer Theorem for problem (7) with
the log-determinant acyclicity constraint. The main point of the result below is to show that the
solutionofthelog-determinantconstrainedempiricalminimizationproblem(7)admitstheformof
afinitelinearcombination,forwhichwemaythenoptimizetheweights.
Theorem2 Let X be a bounded connected non-empty open set in Rd, k(·,·) be a bounded coun-
tiniouslydifferentiablekernel. Thentheconstrainedminimizerof (7)canbewrittenas
n n d (cid:12)
f(cid:98)jτ (x) = (cid:88) α ijk(x,xi)+(cid:88)(cid:88) β aj i∂k ∂(x s,s)(cid:12) (cid:12)
(cid:12)
, x ∈ X, (8)
i=1 i=1 a=1 a s=xi
whereαj,(βj )n ∈ Rn anda,j ∈ [d]. Then,
ai i=1
(cid:13) (cid:13) (cid:13)f(cid:98)jτ(cid:13) (cid:13) (cid:13)2
H
= i(cid:88) ,ln =1α ijα ljk(xi,xl)+2 i(cid:88) ,ln =1(cid:88) a=d 1α ijβ aj l∂k( ∂x xi
l
a,xl) + i(cid:88) ,ln =1a(cid:88) ,bd =1β aj iβ bj l∂ ∂k x(x
i
a∂i, xx
l
bl) . (9)
6LEARNINGNON-PARAMETRICDIRECTEDACYCLICGRAPHICALMODELS
ViaTheorem2,wemayestimateeveryfunctionf byakernelestimator. Asnorandomvariable
j
shouldcauseitself,f shouldnotdependonx asitsinput. Thus,toestimatef wereplacekin(8)
j j j
witharestrictedkernelk−j thatdependsonlyonthesubvector(x ) andusetherepresentation
k k̸=j
f(cid:98)j(x) =
(cid:88)n
α
ijk−j(x,xi)+(cid:88)n (cid:88)d
β aj
i∂k− ∂j s(x,s)(cid:12)
(cid:12) (cid:12)
(cid:12)
.
i=1 i=1 a=1 a s=xi
Let θ = {αj,βj : a ∈ [d],i ∈ [n]} denote the parameters for each f , and let θ = (θ ,...,θ ).
j ai j 1 d
Thenthelossfunctionisconstructedasfollows:
d
(cid:88) 21 nℓ(Xj,f(cid:98)jθ (X))+τ[2ΩD
1
(f(cid:98)jθ )+λ(cid:13) (cid:13)f(cid:98)jθ(cid:13) (cid:13)2 H]. (10)
j=1
Toevaluatetheacyclicityconstraintonthedataset,usingtheRepresenterTheoremforthefunction
f(cid:98)j,weconsequentlyobtainforeveryk,j ∈ [d]:
(cid:13) θ (cid:13)
W kD j(f(cid:98)jθ ) = WD(θ)
kj
= (cid:13) (cid:13) (cid:13)∂f(cid:98) ∂j x(x)(cid:13) (cid:13)
(cid:13)
(cid:13) k (cid:13)
n
= (cid:26) 1 (cid:88)n (cid:34) (cid:88)n αj∂k−j(xi,xl) +(cid:88)n (cid:88)d βj ∂k−j(xi,xl)(cid:35)2 (cid:27)1 2 , (11)
n l ∂xi al ∂xi∂xl
i=1 l=1 k l=1 a=1 k a
andconsequently
(cid:118)
d (cid:117) n (cid:32) θ (cid:33)2 d
ΩD
1
(f(cid:98)jθ ) = (cid:88)(cid:117) (cid:116) n1 (cid:88) ∂f(cid:98) ∂j x(xi) = (cid:88) WD(θ) kj. (12)
k
k=1 i=1 k=1
As a default, we base our algorithm on the Gaussian kernel k (cid:0) x,x′(cid:1) = exp(− 1 ∥x−x′∥2),
γ γ2
based on the Euclidean distance between x and x′. For γ > 0, j ∈ [d], we define the restricted
Gaussiankernelk−j : Rd−1 (cid:55)→ Ras
γ
(cid:18) d (cid:19)
1 (cid:88)
k−j(x,x′) := exp − (x −x′)2 . (13)
γ γ2 i i
i̸=j
ItcorrespondstotheGaussiankernelk
(cid:0) x,x′(cid:1)
whenfixingthej-thcoordinatetoaconstant.
γ
Remark3 Wewantthedecisionrulereturnedbyourestimationalgorithmtobelongtothespaceof
continuouslydifferentiablefunctions,asthisguaranteesthatavanishingpartialderivativeimplies
that the function does not depend on the considered coordinate. To this end, we claim that any
functionthatbelongstoGaussianRKHSH (X)alsobelongstoC1(X). Toseethis,letH bethe
γ γ
RKHSofthereal-valuedGaussianRBFkernelk forγ > 0. Sincek (·,·)is(infinitelymanytimes)
γ γ
differentiable, Theorem 10.45 in Wendland (2004) implies that the associated RKHS H (X) is a
γ
subset of the space of continuously differentiable functions. Therefore, H (X) ⊂ C1(X), which
γ
impliesthat∥f∥ < ∞foreveryf ∈ H .
C1(X) γ
7LIANGZADOROZHNYIDRTON
5. Optimization
CombiningEquations(10),(11),(12)togetherwithleastsquareslossandlog-determinantacyclicity
constraint,weobtainthefollowingconstrainedempiricaloptimizationproblem:
m θin(cid:88)d (cid:26) 21 n(cid:13) (cid:13) (cid:13)Xj −f(cid:98)jθ (X)(cid:13) (cid:13) (cid:13)2 2+τ[2ΩD
1
(f(cid:98)jθ )+λ(cid:13) (cid:13)f(cid:98)jθ(cid:13) (cid:13)2 H](cid:27)
j=1
s.t. −logdet(sI −WD(θ)◦WD(θ))+dlog(s) = 0.
d
As in DAGMA, we use a central path method to solve the constrained optimization problem. We
giveourmethodinAlgorithm1andrefertoitasRKHS-DAGMA.Notethatline3ofAlgorithm1
meansthatstartingatθ = θ(t),θ(t+1) isobtainedbytheADAMoptimizer(KingmaandBa,2015).
Algorithm1:RKHS-DAGMA
Input: DatamatrixX,initialcoefficient(learningstep)µ(0) (e.g.,1),decayfactor
α ∈ (0,1)(e.g.,0.1),sparsityparameterτ(e.g.,1×10−4),functioncomplexity
parameterλ(e.g.,1×10−3),log-detparameters > 0(e.g.,1),numberofiterations
T (e.g.,6),thresholdω (e.g.,0.1).
Output: W(cid:99),theestimatedweightedadjacencymatrix.
1 Initializeθ(0) sothatWD(θ(0)) ∈ Ws.
2 fort ← 0toT −1do
3 Startingatθ(t),solveθ(t+1) =
argmin θµ(t)((cid:80)d j=1{ 21 n(cid:13) (cid:13)Xj−f(cid:98)jθ (X)(cid:13) (cid:13)2 2+τ[2ΩD
1
(f(cid:98)jθ )+λ(cid:13) (cid:13)f(cid:98)jθ(cid:13) (cid:13)2 H]})+hs ldet(WD(θ)).
4 Setµ(t+1) = αµ(t).
5 end
6 ThresholdmatrixW(cid:99) = WD(θ(T))·1(WD(θ(T)) > ω).
6. Experiments
This section is divided into three parts. First, we analyze the performance of RKHS-DAGMA in
a simple bivariate prediction setting, distinguishing cause-effect within artificially constructed toy
models. Second, we evaluate and compare the properties of RKHS-DAGMA with non-parametric
NOTEARS algorithms, including NOTEARS-MLP and NOTEARS-SOB, as well as the score-
basedFGESalgorithm(Ramsey,2015)onsampleddirectedErdo˝s-Re´nyigraphsofgrowingdimen-
sion. Finally,weassesstheperformanceofRKHS-DAGMAagainstNOTEARS-MLP,NOTEARS-
SOB,andFGESonreal-worldbivariatedatasets(Mooijetal.,2016).
6.1. ToyExample
WeillustratetheperformanceofRKHS-DAGMAbytwosimplesimulationswithd = 2nodesand
n = 100datapoints. Figure1plotsthegroundtruthdatapoints(blue)andestimatedfunctionvalues
obtained by RKHS-DAGMA (red) in the bivariate causal models (a) Y = X2 +ε, X ∼ U[0,10]
and (b) Y = 10sin(X)+ε, X ∼ U[−3,3]. Let W be the estimated weighted adjacency matrix
est
without any thresholding. The results of Figure 1(a) and Figure 1(b) correspond to the estimated
matrices
(cid:16) 0 10.35(cid:17) (cid:16) 0 4.91(cid:17)
W = , W = ,
est 6.22×10−4 0 est 8.49×10−4 0
8LEARNINGNON-PARAMETRICDIRECTEDACYCLICGRAPHICALMODELS
(a)quadraticrelationship (b)sinerelationship
Figure1: IllustrationsofRKHS-DAGMAbytoyexampleswithtwonodes.
respectively. In both cases, W is large, whereas W is small enough to be ignored after thresh-
12 21
olding,indicatingthatRKHS-DAGMAfindscorrectcausalrelationships.
6.2. StructureLearning
Next, we compare RKHS-DAGMA to non-parametric NOTEARS methods by comparing the es-
timated DAG with the ground truth, generated as Erdo˝s-Re´nyi (ER) directed graphs with given
topological ordering. Zheng et al. (2020) consider several graph models, including ER and scale-
free graphs. As one of the hardest settings (Zheng et al., 2020, Fig. 4), we take up ER4 graphs for
which NOTEARS algorithms are less competitive compared to algorithms like fast greedy equiva-
lencesearch(Ramseyetal.,2017),DAG-GNN(Yuetal.,2019),orgreedyequivalencesearchwith
generalizedscores(Huangetal.,2018). ERm(m = 4)denotesanERgraphwithm×dedges.
Given the ground truth, we simulate from an SEM with X = g (X ) + ε with ε ∼
j j pa(j) j j
N(0,1). We consider the following functional relationships for g : Additive models with Gaus-
j
sian processes, Gaussian processes, and MLPs according to the procedure of Zheng et al. (2020).
We also add a simulation type called the combinatorial model, which is an additive model with
non-linear relationships random picked from the following common non-linear functions: g(x) =
exp(−|x|), g(x) = 0.05·x2, g(x) = sin(x). We refer to Appendix B for comprehensive details
onthesimulations.
As noted in Bello et al. (2022), the initial point W(θ(0)) is required to be inside Ws. Since
the zero matrix is always inside Ws for any s > 0, we set the parameters θ(0) be 0. Given that
our approximation method and sparsity regularizer fundamentally differ from those in NOTEARS
algorithms, the hyperparameters λ and τ are tuned via grid search. In RKHS-DAGMA, we take
sparsity parameter τ = 1 × 10−4, function complexity parameter λ = 1 × 10−3, and threshold
ω = 0.1. Additionally, we take µ(0) = 1 and the default value T = 6; if the resulting weighted
adjacency matrix is not a DAG, we enhance T to 7. We set γ = 0.4d for the Gaussian kernel (see
Appendix B.2 for an explanation of this choice). Due to the explicit computation of derivatives
and the Hessian of the kernel function, we limit the maximum number of iterations of the ADAM
optimizer to 10% of corresponding values in DAGMA to compensate for the additional cost. For
the NOTEARS algorithms, we choose the default hyperparameters as described in Waxman et al.
(2024)andZhengetal.(2020).
To evaluate model performance, we use the structural Hamming distance (SHD), which mea-
sures the total number of edge additions, deletions, and reversals needed to convert the estimated
graphintothetruegraph. AlowerSHDindicatesbettermodelperformance.
9LIANGZADOROZHNYIDRTON
(a) (b)
(c) (d)
Figure2: ComparisonbetweenRKHS-DAGMAandNOTEARS-MLPbySHD(lowerisbetter)for
random data generated from 2(a) the ER-4 GP model, 2(b) the ER-4 GP-additive model, 2(c) the
ER-4 MLP model, 2(d) the model with combination of functions. Boxplots show the median and
quartilesacross10differentsimulationsforeachsimulationmodel.
OurresultsindicatethatRKHS-DAGMAconsistentlyoutperformsNOTEARS-SOBintermsof
structuredHammingdistance(SHD).Additionally,comparedtoNOTEARS-MLP,RKHS-DAGMA
shows superior performance in simulations based on Gaussian processes (GP), additive GP, and
combinatorialmodels,whilemaintainingcompetitiveresultsinMLPexperiments(seeFigure2-3).
We also conduct a comparative analysis between RKHS-DAGMA and the FGES algorithm,
implementedthroughthepy-causalpackage2. Figure4indicatesthatRKHS-DAGMAsignifi-
cantlyoutperformstheFGESalgorithminsimulationsbasedonGP,additiveGP,andcombinatorial
modelsford = 10. Itachievescomparableperformanced ≥ 20. However,intheMLPsimulations,
theperformanceofRKHS-DAGMAclearlyfallsbehindford ≥ 20.
6.3. RealData
Finally,wecomparetheperformanceofRKHS-DAGMAwithNOTEARS-MLP,NOTEARS-SOB,
andtheFGESonabenchmarkcollectionofdatasetsfeaturingcause-effectpairs(Mooijetal.,2016).
Thesedatasetsarebivariate,eachconsistingofonepairofstatisticallydependentvariables. Weex-
cludedsixdatasetscontainingmulti-dimensionalrandomvariablesandstandardizedtheremaining
2.https://github.com/bd2kccd/py-causal
10LEARNINGNON-PARAMETRICDIRECTEDACYCLICGRAPHICALMODELS
(a) (b)
(c) (d)
Figure3: ComparisonbetweenRKHS-DAGMAandNOTEARS-SOBbySHD(lowerisbetter)for
random data generated from 3(a) the ER-4 GP model, 3(b) the ER-4 GP-additive model, 3(c) the
ER-4 MLP model, 3(d) the model with combination of functions. Boxplots show the median and
quartilesacross10differentsimulationsforeachsimulationmodel.
datasets. To reduce computational costs, datasets with more than 400 samples were organized in
ascendingorderaccordingtothefirstvariable. Thesedatasetswerethenpartitionedinto300grids
accordingtothefirstvariable,withthemedianvalueofthefirstvariableineachgridandthecorre-
spondingvalueofthesecondvariableusedformodelevaluation.
RKHS-DAGMAachievesthebestaccuracyof55.88%amongtheremaining102datasets,while
NOTEARS-SOBandNOTEARS-MLPachieveanaccuracyof45.10%and0.98%correspondingly.
We attribute the bad performance of NOTEARS-MLP to the small sample size with a relatively
largenumberofhiddenunitscomparedtothenumberofnodes,andtothespecificdefinitionofthe
weighted adjacency matrix, which depends on the weights of the first hidden layer and may differ
significantly from those defined by derivatives (Waxman et al., 2024). Furthermore, the FGES
algorithm yields only undirected edges across all bivariate datasets, as it employs a conditional
independence test to eliminate unnecessary edges. In the context of bivariate data, where each
variablepairlacksadditionalconditioningvariables,thealgorithm’scapacitytoextractinformation
isinherentlylimited.
11LIANGZADOROZHNYIDRTON
(a) (b)
(c) (d)
Figure 4: Comparison between RKHS-DAGMA and FGES by SHD (lower is better) for random
datageneratedfrom4(a)theER-4GPmodel,4(b)theER-4GP-additivemodel,4(c)theER-4MLP
model, 4(d) the model with combination of functions. Boxplots show the median and quartiles
across10differentsimulationsforeachsimulationmodel.
7. Conclusion
In this work, we addressed the non-parametric DAG learning problem using a procedure that ex-
ploits the machinery of infinite-dimensional (Gaussian) RKHS. We showed in Theorem 2 that the
RKHS-DAGMA Algorithm, which solves a (combined) constrained empirical optimization prob-
lem with log-determinant acyclicity constraint, admits an explicit solution as a finite-dimensional
representationofthekernelelementsofthedataandtheirderivatives. Furthermore,thissolutioncan
becomputedusingcentralpathmethodssimilartothoseinthe DAGMA algorithm. Wecompared
theaccuracyofRKHS-DAGMAwiththeknownbaselinesundernon-parametricstructuralequation
modeling,suchasNOTEARS-MLPandNOTEARS-SOB(seeex. Zhengetal.,2020),aswellas
thescore-basedFGESalgorithm(Ramsey,2015)undersettingscomparabletothoseinZhengetal.
(2020). The RKHS-DAGMA Algorithm demonstrates utility across both simulated and empirical
datasets.
Acknowledgments
YurouLiangissupportedbytheDAADprogrammeKonradZuseSchoolsofExcellenceinArtificial
Intelligence,sponsoredbytheFederalMinistryofEducationandResearch.
12LEARNINGNON-PARAMETRICDIRECTEDACYCLICGRAPHICALMODELS
References
K. Bello, B. Aragam, and P. Ravikumar. DAGMA: Learning DAGs via m-matrices and a log-
determinantacyclicitycharacterization. AdvancesinNeuralInformationProcessingSystems,35:
8226–8239,2022.
D.M.Chickering.Optimalstructureidentificationwithgreedysearch.JournalofMachineLearning
Research,3:507–554,2002.
M.DrtonandM.H.Maathuis. Structurelearningingraphicalmodeling. Annu.Rev.Stat.Appl.,4:
365–393,2017.
C.Giraud. Introductiontohigh-dimensionalstatistics. ChapmanandHall/CRC,2021.
K. Go¨bler, T. Windisch, M. Drton, T. Pychynski, M. Roth, and S. Sonntag. causalAssembly:
Generating realistic production data for benchmarking causal discovery. In Proceedings of the
Third Conference on Causal Learning and Reasoning, volume 236 of Proceedings of Machine
LearningResearch,pages609–642.PMLR,01–03Apr2024.
L.Gyo¨rfi,M.Kohler,A.Krzyz˙ak,andH.Walk. Adistribution-freetheoryofnonparametricregres-
sion. SpringerSeriesinStatistics.Springer-Verlag,NewYork,2002.
D. Heckerman, D. Geiger, and D. M. Chickering. Learning Bayesian networks: The combination
ofknowledgeandstatisticaldata. MachineLearning,20:197–243,1995.
B.Huang,K.Zhang,Y.Lin,B.Scho¨lkopf,andC.Glymour. Generalizedscorefunctionsforcausal
discovery. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge
Discovery&DataMining,pages1551–1560,2018.
Q. Ji, E. Bouri, R. Gupta, and D. Roubaud. Network causality structures among bitcoin and other
financial assets: A directed acyclic graph approach. The Quarterly Review of Economics and
Finance,70:203–213,2018.
D.P.KingmaandJ.Ba. Adam: Amethodforstochasticoptimization. InY.BengioandY.LeCun,
editors, 3rdInternationalConferenceonLearningRepresentations, ICLR2015, SanDiego, CA,
USA,May7-9,2015,ConferenceTrackProceedings,2015.
M.Maathuis,M.Drton,S.Lauritzen,andM.Wainwright,editors. Handbookofgraphicalmodels.
Chapman&Hall/CRCHandbooksofModernStatisticalMethods.CRCPress, BocaRaton, FL,
2019.
D. Margaritis and S. Thrun. Bayesian network induction via local neighborhoods. Advances in
NeuralInformationProcessingSystems,12:505–511,1999.
J.M.Mooij,J.Peters,D.Janzing,J.Zscheischler,andB.Scho¨lkopf. Distinguishingcausefromef-
fectusingobservationaldata: Methodsandbenchmarks. JournalofMachineLearningResearch,
17(32):1–102,2016.
A. Nazaret, J. Hong, E. Azizi, and D. Blei. Stable differentiable causal discovery. arXiv preprint
arXiv:2311.10263,2023.
I.Ng,A.Ghassami,andK.Zhang. OntheroleofsparsityandDAGconstraintsforlearninglinear
DAGs. AdvancesinNeuralInformationProcessingSystems,33:17943–17954,2020.
13LIANGZADOROZHNYIDRTON
J. Ramsey, M. Glymour, R. Sanchez-Romero, and C. Glymour. A million variables and more: the
fastgreedyequivalencesearchalgorithmforlearninghigh-dimensionalgraphicalcausalmodels,
with an application to functional magnetic resonance images. International Journal of Data
ScienceandAnalytics,3:121–129,2017.
J. D. Ramsey. Scaling up greedy equivalence search for continuous variables. CoRR,
abs/1507.07749,2015. URLhttp://arxiv.org/abs/1507.07749.
L.A.Rosasco,S.Villa,S.Mosci,M.Santoro,andA.Verri. Nonparametricsparsityandregulariza-
tion. JournalofMachineLearningResearch,14(1):1665–1714,2013.
W.Rudin. Functionalanalysis,volume2. McGraw-Hil,1991.
B. Scho¨lkopf, R. Herbrich, and A. J. Smola. A generalized representer theorem. In D. P. Helm-
boldandR.C.Williamson,editors,ComputationalLearningTheory,14thAnnualConferenceon
Computational Learning Theory, COLT 2001 and 5th European Conference on Computational
LearningTheory,EuroCOLT2001,Amsterdam,TheNetherlands,July16-19,2001,Proceedings,
volume2111ofLectureNotesinComputerScience,pages416–426.Springer,2001.
P. Spirtes and C. Glymour. An algorithm for fast recovery of sparse causal graphs. Social Science
ComputerReview,9(1):62–72,1991.
P.SpirtesandK.Zhang. Searchforcausalmodels. InHandbookofgraphicalmodels,Chapman&
Hall/CRCHandb.Mod.Stat.Methods,pages439–469.CRCPress,BocaRaton,FL,2019.
I. Steinwart and A. Christmann. Support vector machines. Springer Science & Business Media,
2008.
I. Tsamardinos, C. F. Aliferis, and A. R. Statnikov. Algorithms for large scale Markov blanket
discovery. In I. Russell and S. M. Haller, editors, Proceedings of the Sixteenth International
Florida Artificial Intelligence Research Society Conference, May 12-14, 2003, St. Augustine,
Florida,USA,pages376–381.AAAIPress,2003.
D.Waxman,K.Butler,andP.M.Djuric´. Dagma-DCE:Interpretable,non-parametricdifferentiable
causaldiscovery. IEEEOpenJournalofSignalProcessing,2024.
H.Wendland. Scattereddataapproximation,volume17. Cambridgeuniversitypress,2004.
Y.Yu,J.Chen,T.Gao,andM.Yu. DAG-GNN:DAGstructurelearningwithgraphneuralnetworks.
InInternationalConferenceonMachineLearning,pages7154–7163.PMLR,2019.
J. Zhang, L. Cammarata, C. Squires, T. P. Sapsis, and C. Uhler. Active learning for optimal inter-
ventiondesignincausalmodels. NatureMachineIntelligence,5(10):1066–1075,2023.
X. Zheng, B. Aragam, P. K. Ravikumar, and E. P. Xing. DAGs with NO TEARS: Continuous
optimization for structure learning. Advances in Neural Information Processing Systems, 31:
9492–9503,2018.
X. Zheng, C. Dan, B. Aragam, P. Ravikumar, and E. Xing. Learning sparse nonparametric DAGs.
In International Conference on Artificial Intelligence and Statistics, pages 3414–3425. PMLR,
2020.
D.-X. Zhou. Derivative reproducing properties for kernel methods in learning theory. Journal of
ComputationalandAppliedMathematics,220(1-2):456–463,2008.
14LEARNINGNON-PARAMETRICDIRECTEDACYCLICGRAPHICALMODELS
AppendixA. DetailedProofs
A.1. ProofofTheorem1
• D-stable: FollowsfromTheorem1inBelloetal.(2022).
• V-stable: Notethat
logdet(sI −A)−dlogs = log(sddet(I −s−1A))−dlogs = logdet(I −s−1A).
Since s > ρ(A) is equivalent to 1 > ρ(s−1A), we consider the case s = 1 w.l.o.g. For any
ε > 0withε|ρ(A)| ≤ 1,itholdsthat
d d
(cid:89) (cid:88)
hs (εA) = −logdet(I −εA) = −log( λ (I −εA)) = −log(1−ελ (A)).
ldet i i
i=1 i=1
WeprovethestatementoftheTheoremundertheassumptionthattheeigenvalues{λ (A)}
i i∈[d]
ofAarecomplexnumbersandundertheadditionalassumptionthat(cid:80)d
λ (A) ̸= 0.
i=1 i
ApplyingtheCauchy-integralformulatotheprincipalbranchofthecomplexlogarithmfunc-
tionz (cid:55)→ log(1−z),whichisanalyticwithindomain|z| < 1,weget:
ε2λ2(A) (cid:90) log(1−w)
−log(1−ελ (A)) = ελ (A)+ i dw,
i i 2πi (ελ (A)−w)w2
η0 i
whereη isanyclosedcircleofradiusr withε|λ (A)| < r < 1. Therefore,summingover
0 0 i 0
all complex values λ (A) and using triangle inequality |a−b| ≥ |a|−|b| for a,b ∈ C, we
i
deducethat
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)(cid:88)d (cid:12) (cid:12)(cid:88)d (cid:12) 1 (cid:12)(cid:88)d (cid:90) log(1−w) (cid:12)
(cid:12) −log(1−ελ (A))(cid:12) ≥ ε(cid:12) λ (A)(cid:12)− (cid:12) ε2λ2(A) dw(cid:12).
(cid:12) i (cid:12) (cid:12) i (cid:12) 2π(cid:12) i (w−ελ (A))w2 (cid:12)
(cid:12)
i=1
(cid:12) (cid:12)
i=1
(cid:12) (cid:12)
i=1
η0 i (cid:12)
log(1−w)
Sincew (cid:55)→ iscontinuous,WeierstrassTheoremimpliesthatthefunctionisbounded
w−ελi(A)
(cid:12) (cid:12)
ontheboundeddomainη
,yieldingthat(cid:12)log(1−w)(cid:12)
≤ K forsomeK > 0. Since
ε|λi(A)|
< 1
0 (cid:12)w−ελi(A)(cid:12) r0
andη isacircleofradiusr ,theMLinequalityforthecomplexintegralgives
0 0
(cid:12) (cid:90) (cid:12)
(cid:12) (cid:12)ε2λ2(A) log(1−w) dw(cid:12) (cid:12) ≤ 2πε2λ2(A)K ,
(cid:12) i w−ελ (A)w2 (cid:12) i r
η0 i 0
whichinturnimpliesthat
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
d d d
(cid:12)(cid:88) (cid:12) (cid:12)(cid:88) (cid:12) K (cid:12)(cid:88) (cid:12)
(cid:12) −log(1−ελ (A))(cid:12) ≥ ε(cid:12) λ (A)(cid:12)− ε2(cid:12) λ2(A)(cid:12)
(cid:12) i (cid:12) (cid:12) i (cid:12) r (cid:12) i (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) 0 (cid:12) (cid:12)
i=1 i=1 i=1
(cid:12) (cid:12)
d d
(cid:12)(cid:88) (cid:12) K (cid:88)
= ε(cid:12) λ (A)(cid:12)− ε2 λ2(A)
(cid:12) i (cid:12) r i
(cid:12) (cid:12) 0
i=1 i=1
(cid:12) (cid:12)
(cid:12)(cid:88)d (cid:12) Kε2∥A∥2
= ε(cid:12) λ (A)(cid:12)− 2
(cid:12) i (cid:12) r
(cid:12) (cid:12) 0
i=1
(cid:12) (cid:12)
(cid:12)(cid:80)d λ (A)(cid:12)
(cid:12) i=1 i (cid:12)
≥ ε ,
2
15LIANGZADOROZHNYIDRTON
wherethelastinequalityholdsprovidedεischosensmallenough,with
(cid:12) (cid:12)
(cid:26) r (cid:12)(cid:80)d λ (A)(cid:12)(cid:27)
1 0(cid:12) i=1 i (cid:12)
ε ≤ min , .
|ρ(A)| 2K∥A∥2
2
Thus,weprovedtheclaim.
Finally,noticethatBelloetal.(2022)showedthaths isanacyclicityconstraintandtheinstability
ldet
ofPSTacyclicityconstraintisshowninNazaretetal.(2023).
A.2. ProofofTheorem2
Proof (8): Consider arbitrary elements f,g ∈ H. Since H is complete, there exists a sequence
(g)∞ ,g ∈ H,thatconvergestog inthenormofHilbertspaceH. Furthermore,foranyn ∈ N,
n=1 n
Cauchy-Schwarz
⟨f,g ⟩ −⟨f,g⟩ ≤ |⟨f,g ⟩ −⟨f,g⟩ | = |⟨f,g −g⟩ | ≤ ∥f∥ ·∥g −g∥ .
n H H n H H n H H n H
Since the kernel k is bounded, elements f ∈ H are bounded (Steinwart and Christmann, 2008,
Lemma4.23). Furthermore,sinceg → g inthenormofthespaceH wehavethat
n
lim ⟨f,g ⟩ −⟨f,g⟩ = 0,
n H
n→∞
whichimpliesthatmapx (cid:55)→ ⟨·,x⟩ iscontinuous. Similarly,onecanprovethatthescalarproduct
H
iscontinuousinthefirstcoordinate.
For coherence of the remainder of the proof, we first refine the proof that for the open set
X ⊂ Rd, we have that ∂ k(·,x) ∈ H for every a ∈ [d] and x ∈ X, and that a “differential
∂xa
reproducingproperty”holds,whichstatesthat
(cid:28) (cid:12) (cid:29)
∂ ∂ (cid:12)
f(x) = f, k(·,s)(cid:12) , ∀x ∈ X, f ∈ H.
∂x ∂s (cid:12)
a a s=x H
Specifically, we refine the proof of Theorem 1 a)-b) in Zhou (2008) in case α = 0 to show that
the “derivative element” exists in H. We deviate from that proof in the part that establishes the
differential reproducing property, the difference being the use of completeness of H and the fact
thatweakconvergenceandpointwiseconvergenceareequivalentinRKHSH.
Consider arbitrary x ∈ X. As X is open, there exists r > 0 such that the ball {x + y,y ∈
Rd,∥y∥ ≤ r} ⊂ X. For a ∈ [d], let e be the a-th orthonormal vector in the standard Euclidean
2 a
basis in Rd. Let h˜ : X (cid:55)→ R be the function given by h˜ (y) = ∂ k(s,y)| for all y ∈ X.
x,a x,a ∂sa s=x
BydefinitionoftheRKHS,k(·,x) ∈ H,andthesetoffunctionsH (cid:55)→ Rgivenby
(cid:26) (cid:27)
1
(k(·,x+te )−k(·,x)) : |t| ≤ r (14)
a
t
satisfiesthatforeveryt,|t| ≤ r:
(cid:13) (cid:13)k(·,x+te a)−k(·,x)(cid:13) (cid:13)2 1(cid:16) (cid:17)
(cid:13)
(cid:13) t
(cid:13)
(cid:13)
=
t2
k(x+te a,x+te a)−k(x,x+te a)−k(x+te a,x)+k(x,x)
H
(cid:13)
(cid:13)
∂2 (cid:13) (cid:13)2
≤ (cid:13) k(·,·)(cid:13) ,
(cid:13)∂x ∂x (cid:13)
a a ∞
16LEARNINGNON-PARAMETRICDIRECTEDACYCLICGRAPHICALMODELS
where the last inequality follows from the fact that k(·,·) is a continuously differentiable function
in every coordinate and an application of the Mean Value Theorem (twice, once in every coordi-
(cid:13) (cid:13)
nate). The latter inequality implies that set (14) lies within a ball of radius (cid:13) ∂2 k(·,·)(cid:13) in
(cid:13)∂xa∂xa (cid:13)
∞
Hilbert space H. It is known (since ∗-weak convergence is equivalent to weak convergence in
Hilbert spaces) that the ball in the Hilbert space is weakly sequentially compact; see, e.g., (Rudin,
1991, Chap. 3). Thus, there exists a sequence (t ) such that lim t = 0 and the sequence
n n→∞ n
1 (k(·,x+t e )−k(·,x))convergesweaklytoanelementh ∈ H. Thelattermeansthat
tn n a x
(cid:28) (cid:29)
1
lim (k(·,x+t e )−k(·,x)),f = ⟨h ,f⟩ ∀f ∈ H. (15)
n→∞ t n n a H x H
Considerf(·) = k(·,y), wherey ∈ X isarbitrary. Sincek(·,y)isdifferentiable(asafunction
ofthefirstcoordinate),theleft-handsideof(15)satisfies
(cid:28) (cid:29)
1 1
lim (k(·,x+t e )−k(·,x)),k(·,y) = lim (k(x+t e ,y)−k(x,y))
n a n a
n→∞ t n H n→∞ t n
(cid:12)
= ∂ k(s,y)(cid:12) (cid:12) = h˜ x,a(y).
∂s (cid:12)
a s=x
Butfortheright-handsideof(15),itholdsthat
(cid:28) (cid:29)
1
lim (k(·,x+t e )−k(·,x)),k(·,y) = ⟨h ,k(y,·)⟩ = h (y) ∀y ∈ X.
n a x x
n→∞ t n H
We conclude that h˜ = h as a map X (cid:55)→ R, and since h is in H, so is h˜ . By identifying
x,a x x x
(cid:68) (cid:69)
∂ k(·,x) := h ,theexistenceofanelementy with ∂ k(x,y) = ∂ k(x,·),k(·,y) follows.
∂xa x ∂xa ∂xa
Now we show the differentiable reproducing property, that is, the convergence to the limit
∂ (k(·,x)) is pointwise (and not only as a weak limit) and we can exchange the differential and
∂xa
innerproductsign. Thelatterisequivalenttothefolklorefactthatweakconvergenceisequivalentto
pointwiseconvergencewhentheunderlyingspaceisRKHS.Indeed,consideranysequenceg that
n
convergesweaklytoanelementg ∈ H,whichmeansthatlim ⟨g ,f⟩ = ⟨g,f⟩forallf ∈ H.
n→∞ n
In particular, the latter holds for all k(x,·), x ∈ X yielding the necessity. To show the sufficiency,
iflim g (x) = g(x)forallx ∈ X thenlim ⟨g ,f⟩ = ⟨g,f⟩forallf ∈ span{k(x,·)},by
n→∞ n n→∞ n
linearityoftheinnerproductanditscontinuity. TheclaimthenfollowssinceH iscomplete. From
thisstatement,wededucethatthelimitin(15)isactuallyapointwiselimit. Thus,foreveryx ∈ X,
we have lim 1(k(·,x+te )−k(·,x)) = ∂ k(·,x) and, moreover, it holds for every f ∈ H
t→0 t a ∂xa
that
(cid:28) (cid:29) (cid:28) (cid:29)
∂ 1 f(x+te )−f(x) ∂f(x)
a
k(·,x),f = lim (k(·,x+te )−k(·,x)),f = lim = ,
a
∂x a H t→0 t t→0 t ∂x a
where we used continuity of inner product and the reproducing property in the second equality.
Hence,thederivativeexists,andthedifferentialreproducingpropertyholds.
LetX′ := {xi : i = 1,...,n},andlet
(cid:26) (cid:27)
∂k(·,s)
H| := span k(·,xi), | : i = 1,...,n, a = 1,...,d .
X′
∂s
s=xi
a
Furthermore, let H⊥ be the orthogonal complement of H| in H; it exists and is well-defined
|X′ X′
as every element in the span exists and is well-defined. By the Hilbert Projection Theorem, every
17LIANGZADOROZHNYIDRTON
f ∈ H,j ∈ [d]canbeuniquelydecomposedasf = f∥ +f⊥,wheref∥ ∈ H| andf⊥ ∈ H⊥ .
j j j j j X′ j |X′
Lete ∈ Rd bethea-thvectorofthestandardEuclideanbasisinRd. Bythereproducingproperty
a
andthedefinitionofH⊥ inH,itholdsthat
|X′
(cid:68) (cid:69)
f⊥(xi) = f⊥,k(·,xi) = 0.
j j
H
Using the fact (proved above) that the differentiation reproducing property holds for f ∈ H, to-
j
getherwiththeorthogonalpropertyweget:
∂f j⊥ (cid:0) xi(cid:1) = (cid:28) f⊥, ∂k(·,s) | (cid:29) f j⊥∈ =H |⊥ X′ 0.
∂x j ∂s s=xi
a a H
BythereproducingpropertyinH,wededucethatforeveryj ∈ [d],itholds
1 ℓ(cid:0)Xj,f (X)(cid:1)2 = 1 ℓ(cid:16) Xj,f ∥(X)(cid:17)2 ,
j j
n n
whereasthedifferentialreproducingpropertyimpliesthatitholdsthat
(cid:16) (cid:17)2
WD(f) =
1 (cid:88)n ∂f j2(xi)
=
1 (cid:88)n ∂f j∥ (xi)
= WD(f∥)
aj n ∂xi n ∂xi aj
i=1 a i=1 a
Noticefurtherthat
(cid:13) (cid:13) (cid:13)∂f j(x)(cid:13) (cid:13)
(cid:13) =
(cid:13) (cid:13) (cid:13)∂f j∥ (x)(cid:13) (cid:13)
(cid:13) ,
(cid:13) ∂x (cid:13) (cid:13) ∂x (cid:13)
a n (cid:13) a (cid:13)
n
whichinturnimpliesthatΩD(f ) = ΩD(f∥ ). Thus,bydenoting
1 j 1 j
d (cid:26) (cid:27)
R L,D(f)+τ(2ΩD
1
(f)+λ∥f∥2 Hd) = (cid:88) 21 n(cid:13) (cid:13)Xj −f j(X)(cid:13) (cid:13)2 +τ[2ΩD
1
(f j)+λ(cid:13) (cid:13)f j(cid:13) (cid:13)2 H] ,
j=1
for f = (f ,...,f ) ∈ H⊗d, where H⊗d denotes the direct product of d copies of H, we get that
1 d
overtheacyclicityconstraintthefollowingchainoftheequalitiesholds:
inf R (f)+τ(2ΩD(f)+λ∥f∥2 )
L,D 1 Hd
f∈Hd,hs (WD(f))=0
ldet
= inf R L,D(f∥)+τ(2ΩD
1
(f∥)+λ(cid:13) (cid:13)f∥(cid:13) (cid:13)2
Hd
+λ(cid:13) (cid:13)f⊥(cid:13) (cid:13)2 Hd)
f∈Hd,f=f∥+f⊥,
f∥∈H|d ,hs (WD(f∥))=0
X′ ldet
= inf R L,D(f∥)+τ(2ΩD
1
(f∥)+λ(cid:13) (cid:13)f∥(cid:13) (cid:13)2 Hd)
f∈Hd,f=f∥+f⊥,f∥∈H|d ,
X′
∥f⊥∥ =0,hs (WD(f∥))=0
Hd ldet
= inf R (f)+τ(2ΩD(f)+λ∥f∥2 ).
L,D 1 Hd
f∈H|d ,hs (WD(f))=0
X′ ldet
Thus,weshowedthat
inf R (f)+τ(2ΩD(f)+λ∥f∥2 ) = inf R (f)+τ(2ΩD(f)+λ∥f∥2 ),
L,D 1 Hd L,D 1 Hd
f∈Hd, f∈H|d ,
X′
hs ldet(WD(f))=0 hs ldet(WD(f))=0
18LEARNINGNON-PARAMETRICDIRECTEDACYCLICGRAPHICALMODELS
establishingthefirstclaimoftheTheoremholds.
Toprove(9),wefirstnotethat,bythedifferentialreproducingproperty,
∂k(·,x) ∂k(·,y) ∂2k(x,y)
⟨ , ⟩ = . (16)
H
∂x ∂y ∂x ∂y
a b a b
Pluggingintheformulaforthesolutionoftheconstrainedminimizationproblemandusingrepro-
ducinganddifferentialreproducingpropertiesweobtain:
(cid:13) (cid:13) (cid:13)f(cid:98)jτ(cid:13) (cid:13) (cid:13)2
H
= (cid:42) (cid:88) i=n 1α ijk(cid:0) ·,xi(cid:1) +(cid:88) i=n 1(cid:88) a=d 1β aj i∂k ∂(· x,
i
axi) ,(cid:88) i=n 1α ijk(cid:0) ·,xi(cid:1) +(cid:88) i=n 1(cid:88) a=d 1β aj i∂k ∂(· x,
i
axi)(cid:43)
H
=
(cid:88)n
αjαjk(cid:16) xi,xl(cid:17)
+2
(cid:88)n (cid:88)d
αjβj
(cid:42) ∂k(cid:0) ·,xl(cid:1)
,k(cid:0)
·,xi(cid:1)(cid:43)
i l i a l ∂xl
i,l=1 i,l=1a=1 a H
+
(cid:88)n (cid:88)d
βj
βj(cid:42) ∂k(cid:0) ·,xi(cid:1)
,
∂k(cid:0) ·,xl(cid:1)(cid:43)
ai bl ∂xi ∂xl
i,l=1a,b=1 a b H
=
(cid:88)n
αjαjk(cid:16) xi,xl(cid:17)
+2
(cid:88)n (cid:88)d
αjβj
∂k(cid:0) xi,xl(cid:1)
+
(cid:88)n (cid:88)d
βj βj
∂k(cid:0) xi,xl(cid:1)
,
i l i al ∂xl ai bl ∂xi∂xl
i,l=1 i,l=1a=1 a i,l=1a,b=1 a b
(17)
whichfinalizestheproof.
AppendixB. DetailsofNumericalExperimentsandIntuitionfortheChoiceofthe
BandwidthoftheGaussianKernelγ.
B.1. Simulations.
WesimulateER4DAGsfollowingtheprocedureoutlinedbyZhengetal.(2020)withthefunctional
relationshipestablishedinthreedistinctwaysandaddanadditionalsimulationtype.
1. ThefirstwayemploystheGaussianprocess:
f (X) = g (X )+ε ∀ j ∈ [d],
j j pa(j) j
where g is sampled from RBF GP with lengthscale 1. In detail, for the sub-data matrix
j
X ∈ Rn×p wherep ≤ d,wehaveg (X ) ∼ N(0,K(X ,X ))with
pa(j) j pa(j) pa(j) pa(j)
(cid:32) (cid:13) (cid:13)xa−xb(cid:13) (cid:13)2(cid:33)
K(X ,X ) = exp − 2 .
pa(j) pa(j) a,b 2l2
Here,xa,xbdenotethea-thandb-throwofX ,respectively,andthelengthscaleisl = 1.
pa(j)
Moreover,ε ∼ N(0,I )isastandardGaussiannoise.
j n
2. ThesecondwayisreferredtoastheadditiveGaussianprocess(additiveGP).Itsets
(cid:88)
f (X) = g (X )+ε ,
j kj k j
k∈pa(j)
whereeachg issampledfromRBFGPwithlengthscale1.
kj
19LIANGZADOROZHNYIDRTON
3. ThethirdwayinvolvesusinganMLPnetworkwithahiddenlayersizeof100andasigmoid
activationfunction,whereallweightsaresampledfromU((−2.0,−0.5)∪(0.5,2.0)).
4. Moreover,weintroduceanadditionalsimulationtypecalledthecombinatorialmodel,where
thenon-linearrelationshipisalinearcombinationofvariouscommonnon-linearfunctions:
(cid:88)
f (X) = g (X )+ε ,
j kj k j
k∈pa(j)
whereg israndomlypickedfromfollowingnon-linearfunctions:
kj
g(x) = exp(−|x|), g(x) = 0.05x2, g(x) = sin(x).
B.2. Hyperparameterchoice
WhileconsideringtheexperimentalsettingwithdifferentdimensionsoftheunderlyingErdo˝s-Re´nyi
graphs, we notice that the the complexity of the decision rule increases with the grows of the di-
mension. Thus, one observes a “classical” phenomenon of the curse of dimensionality (see for
example Giraud, 2021; Gyo¨rfi et al., 2002). In a nutshell, high-dimensional i.i.d. observations are
“essentially” equidistant from each other, while the distance between the points grows with the
growingdimension,whichposesaproblemforhigh-dimensionalmetric-basedmethods. Tohandle
this problem, we seek to reduce the estimation error for the signal by decision rules with higher
regularity.
In our case, we employ the machinery of RKHS rules based on the Gaussian kernel (13) with
parameter γ. Let H be the Gaussian RKHS with reproducing kernel k(x,·) := k (x,·) =
γ γ
(cid:16) ∥x−·∥2(cid:17)
exp − . It then holds that H ⊂ H for γ > γ > 0 (see Steinwart and Christmann,
γ2 γ2 γ1 2 1
2008, Proposition4.46). Hence, higherregularityresultsfromalargerchoiceofγ. Noticethatfor
bounded domains X, the same effect (i.e., restricting to the spaces of larger smoothness) can be
ensuredbyconsideringthere-scaleddomainwithparameter 1.
γ
20