Prompt-Guided Image-Adaptive Neural Implicit Lookup Tables
for Interpretable Image Enhancement
SatoshiKosugi
TokyoInstituteofTechnology
Yokohama,Kanagawa,Japan
kosugi.s.aa@m.titech.ac.jp
Abstract adjustingtheirbrightnessandcolor.Thisprocesssignificantlyin-
Inthispaper,wedelveintotheconceptofinterpretableimageen- creasesanimage’sutilityacrossvariousapplications.Thispaper
hancement,atechniquethatenhancesimagequalitybyadjusting focusesonimageenhancementtechniques,examiningtheirscope
filterparameterswitheasilyunderstandablenamessuchas“Expo- andpotentialindetail.Especially,wedelveintotheconceptofin-
sure”and“Contrast”.Unlikeusingpredefinedimageeditingfilters, terpretableimageenhancement,atechniquethatimprovesimages
ourframeworkutilizeslearnablefiltersthatacquireinterpretable throughtheadjustmentoffilterparameterswitheasilyunderstand-
namesthroughtraining.Ourcontributionistwo-fold.Firstly,we ablenames,suchas“Exposure”,“Contrast”,and“Saturation”.This
introduceanovelfilterarchitecturecalledanimage-adaptiveneural approachallowstheusertoadjusttheenhancementresultsaccord-
implicitlookuptable,whichusesamultilayerperceptrontoimplic- ingtohisorherpreferenceandtolearnandmoreeffectivelyutilize
itlydefinethetransformationfrominputfeaturespacetooutput theimageenhancementprocessitself.Consequently,interpretable
colorspace.Byincorporatingimage-adaptiveparametersdirectly imageenhancementisanticipatedtosubstantiallyenhanceusers’
intotheinputfeatures,weachievehighlyexpressivefilters.Sec- comprehensionandmanipulationofimageprocessing.
ondly,weintroduceapromptguidancelosstoassigninterpretable Previousinterpretableimageenhancementmethods[8,12,19,
namestoeachfilter.Weevaluatevisualimpressionsofenhancement 20]employpredefinedimageeditingfilters,andconvolutionalneu-
results,suchasexposureandcontrast,usingavisionandlanguage ralnetworks(CNNs)aretrainedtodeterminetheoptimalparame-
modelalongwithguidingprompts.Wedefineaconstrainttoensure tersforthesefilters.Sincethesefiltersaredesignedinamannerthat
thateachfilteraffectsonlythetargetedvisualimpressionwithout isunderstandabletohumans,theyfacilitateinterpretableimage
influencingotherattributes,whichallowsustoobtainthedesired enhancement.However,theeffectivenessofenhancementmaybe
filtereffects.Experimentalresultsshowthatourmethodoutper- constrainedbythelimitationsinherentinthedesignofthesepre-
formsexistingpredefinedfilter-basedmethods,thankstothefilters definedfilters.Forinstance,the“Exposure”filtercanbedesigned
optimizedtopredicttargetresults.Oursourcecodeisavailableat invariousways,makingitchallengingtomanuallycraftanopti-
https://github.com/satoshi-kosugi/PG-IA-NILUT. malExposurefilterforachievingspecificresults.Incontrast,most
recentimageenhancementmethods[18,29,30,33,34]employ3D
CCSConcepts lookuptables(LUTs)[31],whicharetablesthatrecordinputRGB
valuesandcorrespondingoutputRGBvalues.Multiple3DLUTs
•Computingmethodologies→Imageprocessing;Computa-
areemployedtoapplyvariouseffects,andimage-adaptiveenhance-
tionalphotography.
mentisachievedbylinearlysummingthese3DLUTs,weightedby
image-adaptiveparameters.Unlikepredefinedimageeditingfilters,
Keywords
3DLUTsarelearnablefiltersoptimizedforpredictingenhancement
Imageenhancement,Lookuptable,Implicitneuralrepresentation, results,enablinghighqualityenhancement.However,thereare
Visionandlanguage,CLIP,Interpretability twonotableissuesassociatedwiththeuseof3DLUTs.Firstly,the
expressivepowerislimited.Thisisbecausethemultiple3DLUTs
ACMReferenceFormat:
SatoshiKosugi.2024.Prompt-GuidedImage-AdaptiveNeuralImplicitLook- aremerelysummedinalinearfashion,weightedbyimage-adaptive
upTablesforInterpretableImageEnhancement.InProceedingsofthe32nd parameters,whichmeanstheimage-adaptiveparameterscanonly
ACMInternationalConferenceonMultimedia(MM’24),October28-November adjusttheenhancementeffectinalinearmanner.Secondly,3D
1,2024,Melbourne,VIC,Australia.ACM,NewYork,NY,USA,14pages. LUTslackinterpretablenames.Sincetheyareoptimizedsolelyfor
https://doi.org/10.1145/3664647.3680743 predicting target enhancement results, their effects may not be
intuitivelyunderstoodbyhumans.
1 Introduction Toachievehigh-performingandinterpretableenhancement,we
proposelearnableandinterpretablefiltersnamedaPrompt-Guided
Imageenhancementhasbecomeanessentialtaskinmoderndig-
Image-AdaptiveNeuralImplicitLookupTable(PG-IA-NILUT).Our
italimageprocessing,enhancingthevisualqualityofimagesby
contributionistwofold.Firstly,weintroduceanovellearnablefil-
terarchitecturecalledanImage-AdaptiveNeuralImplicitLookup
MM’24,October28-November1,2024,Melbourne,VIC,Australia
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM. Table(IA-NILUT).Inspiredbyapreviousmethod[5],weutilizeim-
Thisistheauthor’sversionofthework.Itispostedhereforyourpersonaluse.Not plicitneuralrepresentations[23]foracolortransformation.While
forredistribution.ThedefinitiveVersionofRecordwaspublishedinProceedingsofthe
previousresearchershaveused3DLUTstoexplicitlyrecordinput-
32ndACMInternationalConferenceonMultimedia(MM’24),October28-November1,
2024,Melbourne,VIC,Australia,https://doi.org/10.1145/3664647.3680743. outputRGBvaluepairs,weemployamultilayerperceptron(MLP)
4202
guA
02
]VC.sc[
1v55011.8042:viXraMM’24,October28-November1,2024,Melbourne,VIC,Australia SatoshiKosugi
toimplicitlydefinethetransformationfrominputfeaturespaceto personalizedimageenhancementmodels[11,14].Becauseencoder-
outputcolorspace.Themostsignificantdistinctionfromthe3D decoder-based methods are computationally costly, filter-based
LUT-basedmethodsisthatweincorporateimage-adaptiveparam- approacheshaverecentlybecomemoreprevalent.
etersdirectlyintotheinputfeatures.SinceanMLPcanrepresent
nonlinearandcomplexrelationshipsbetweeninputsandoutputs, 2.2 PredefinedFilter-BasedMethods
ourapproachenablestheseimage-adaptiveparameterstoexerta
Predefinedfilter-basedmethodstrainCNNstopredicttheparame-
complexinfluenceontheoutputRGBvalues,therebyachieving
tersofpredefinedimageeditingfilters.Parketal.[20]employed
highlyexpressivefiltereffects.Additionally,toaddresstheproblem
reinforcementlearningtotrainanagentthatiterativelydetermines
ofhighcomputationalcostsofMLPs,weintroducethetechnique
theparameters.Huetal.[8]utilizedgenerativeadversarialnet-
calledLUTbypassing.InsteadofapplyingtheMLPdirectlytoeach
works(GANs)togeneratemorerealisticresults.KosugiandYa-
pixel,weconverttheMLPintoa3DLUT,whichisthenappliedto
masaki[12]reproducedPhotoshopfilters,enablingmoreefficient
eachpixel.Colortransformationthroughthe3DLUTiscomputa-
predictionofenhancementresults.Biancoetal.[3]andLietal.[16]
tionallyinexpensive,enablingcost-effectiveimageenhancement.
usedcolortransformationcurveforflexibleenhancement.Ouyang
Asasecondcontribution,weproposeapromptguidanceloss
etal.[19]achievedlocalenhancementwithregion-specificcolor
toassigninterpretablenamestoeachfilter.Thislossfunctionuti-
filters.Someresearchersproposedmethodsforcrowdworkersto
lizesCLIP[21],avisionandlanguagemodelcapableofembedding
adjustthefilterparameters[13,15].Thesemethodscanachievein-
imagesandtextwithinthesamefeaturespace.CLIPhasdemon-
terpretableenhancementsbecausethepredefinedfiltersarenamed
strateditsabilitytoquantifyimageimpressions[25].Forexample,
inawaythatisunderstandabletohumans,buttheenhancement
foranimpressionwordsuchas“Exposure,”wepreparepairsof
performancecanbelimitedbythedesignofthesepredefinedfilters.
positiveandnegativeprompts(e.g.,“Overexposed photo.”and
“Underexposed photo.”)andcalculatetheratioofthesimilarities
betweentheimagefeatureandeachpromptfeature.Thisallowsus 2.3 LearnableFilter-BasedMethods
toquantitativelyevaluatethe“Exposure”impressionconveyedby Learnablefilter-basedmethodsoptimizethefiltersusingtraining
theimage.Inthisstudy,weproposeusingthepairsofpositiveand data.Heetal.[6]successfullyreplicatedanimageeditingprocess
negativepromptsasguidingpromptstoguidethefilterstoward usinganMLP.Wangetal.[27]furtherenhancedtheseresultsby
achievingthedesiredeffects.Ourpromptguidancelossensures applyingsequentialimageretouching.
thatwhentheparameterassociatedwith“Exposure”isaltered,only Recentlearnablefilter-basedmethodslargelyuse3DLUTs,which
the“Exposure”scorechanges,whilethescoresforotherimpres- aretrainabletablesthatmapinputRGBvaluestocorresponding
sionsremainunaffected.Byminimizingthispromptguidanceloss outputvalues.Zengetal.[31]utilizedmultiple3DLUTs,combin-
inconjunctionwithareconstructionlossofthetargetresults,we ingthemwithimage-adaptiveweights.Wangetal.[26]introduced
achievehigh-performingandinterpretablefilters. spatial-aware3DLUTs.Yangetal.[29]madethesamplingpoints
Toevaluatetheproposedmethod,weperformexperimentswith of3DLUTsadapttoimages.Yangetal.[30]incorporateda1DLUT
theFiveK[4]andPPR10K[17]datasets.Weshowthattheproposed alongside3DLUTs.Zhangetal.[33]proposedacompressedrepre-
methodachievesinterpretablefilters,whichareunderstandableto sentationof3DLUTstoefficientlyincreasetheirnumber.Zhanget
humans.Inaddition,theproposedmethodachieveshigherperfor- al.[34]introducedhashingtechniquestoreduceparameters.Liuet
mancethanexistingpredefinedfilter-basedmethods. al.[18]defined4DLUTsforlocalenhancement.Shietal.[22]devel-
Thecontributionsofthispaperareasfollows: opedanetworkthatconsiderscrossattentionbetweenRGBvalues
andLUTs.Zhangetal.[32]combined3DLUTswithlocallaplacian
• Forinterpretableandlearnablefilters,wedeveloptheIA-
filters[2]foradvancedeffects.Despitethehighperformance,they
NILUT,ahighlyexpressivefilterarchitecture.
lackinterpretability,presentingachallengeforunderstandingthe
• Toassigninterpretablenamestoeachfilter,weintroduce
modificationstheymaketotheimages.
thepromptguidanceloss.
• The proposed method achieves higher performance than
3 Preliminary
existingpredefinedfilter-basedmethods.
Thissectiondescribesthekeyexistingmethod:image-adaptive
3DLUTs[31].3DLUTsarelearnabletablesthatrecordinputRGB
2 RelatedWorks
valuesandthecorrespondingoutputRGBvalues.Wedenotethe
2.1 Encoder-Decoder-BasedMethods matrixrepresentingthesamplingpointsbyI ∈ R𝑁3×3 andthe
EarlyCNN-basedimageenhancementmethodsutilizedencoder-
matrixrecordingthecorrespondingoutputvaluesbyO∈R𝑁3×3,
decoder-basedCNNs.Kimetal.[10]developedasequentialap- where𝑁 isthenumberofsamplingcoordinates.Givenaninput
proachtoimageenhancement,applyingglobalandlocaladjust- RGBvalueof[𝑟x,𝑔x,𝑏x],anindex𝑠issearchedforsuchthatthe
mentsinstages.Kimetal.[9]developedarepresentativecolor vectorinthe𝑠-throwofImatches[𝑟x,𝑔x,𝑏x];then,the𝑠-throwof
transformtechniqueforimprovedcoloraccuracy.Zhaoetal.[36] O,denotedas[𝑟y,𝑔y,𝑏y],isreturned.IftheinputRGBvalueisnot
exploredtheuseofinvertibleneuralnetworkstorestorecontent includedinI,aninterpolatedvalueisreturnedbasedonthesur-
accuratelywhileavoidingbias.Zhangetal.[35]leveragedTrans- roundingRGBvalues.Thisprocessisperformedonallpixels.LetX
former [24] for structure-aware enhancement. Recognizing the andYbeinputandoutputimages,respectively,thetransformation
diversityinuserpreferences,someresearchershavefocusedon isrepresentedasY=Lookup(X,{I,O}).Prompt-GuidedImage-AdaptiveNeuralImplicitLookupTablesforInterpretableImageEnhancement MM’24,October28-November1,2024,Melbourne,VIC,Australia
Reconstruction loss
Parameter Image-adaptive
predictor parameters w
Target image Guiding prompts
IA-NILUT
LUT bypassing Prompt
guidance
loss
Lookup
Input image Image editing filters Output image
Figure1:Overviewofourinterpretableimageenhancementmethod.Forahighlyexpressivefilterarchitecture,weproposean
IA-NILUT.ByemployingLUTbypassing,wecanexpeditethetransformationprocess.Additionally,weintroduceaprompt
guidancelosstoassigninterpretablenamestoeachfilter.Asourmethodprovidesaninterpretableandlearnableframework
forenhancement,itoutperformsotherpredefinedfilter-basedmethodsintermsofperformance.
Inimage-adaptive3DLUTs[31],multipleLUTs{I,O1},...,{I,O𝐽} 4.1 IA-NILUT
areemployedfordifferenteffects.Toachieveoptimalenhancement WeproposeanovelfilterarchitecturecalledanIA-NILUT.Inspired
foreachimage,image-adaptiveparametersw ∈ R𝐽 areusedto bytheexistingmethodknownasNILUTs[5],ourapproachemploys
weighteachLUT.Theenhancedresultisrepresentedas animplicitneuralrepresentation[23],whereinweimplicitlydefine
thetransformationfrominputspacetooutputspaceusinganMLP.
Y=Lookup(X,{I,O1})×𝑤 1+···+Lookup(X,{I,O𝐽})×𝑤 𝐽. (1) Wevisualizethedifferencebetweenthe3DLUTsandourIA-NILUT
inFigure2.Themostsignificantdistinctionbetweentheprevious
EachLookup(X,{I,O𝑗})canberegardedastheresultofapplying
image-adaptive3DLUTsandourIA-NILUTisthattheIA-NILUT
differentfilterstoX,andeach𝑤 𝑗worksasafilterparameterthatde-
incorporatestheimage-adaptiveparametersdirectlyintotheinput
terminesthestrengthofthefiltereffect.O1,...,O𝐽 canbeoptimized
features.GiventhatanMLPiscapableofcapturingnonlinearand
topredictenhancementresults,whichmakesthelookuptables
intricate relationships between input and output variables, our
asefficientimageeditingfilters.Becausepixelsaretransformed
methodallowstheimage-adaptiveparameterstointricatelyaffect
independently,Eq.(1)canbesimplifiedas
theoutputRGBvalues,therebyachievinghighlyexpressivefilter
effects.Wedefinethecolortransformationprocessasfollows,
Y=Lookup(X,{I,O1×𝑤 1+···+O𝐽 ×𝑤 𝐽}). (2)
[𝑟y,𝑔y,𝑏y] = [𝑟x,𝑔x,𝑏x]
The image-adaptive parameters w are predicted by CNN-based +e(cid:0)[𝑟x,𝑔x,𝑏x]⊕sort([𝑟x,𝑔x,𝑏x])⊕w(cid:1) (3)
parameterpredictorFasw = F(X).TheparameterpredictorF −e(cid:0)[𝑟x,𝑔x,𝑏x]⊕sort([𝑟x,𝑔x,𝑏x])⊕0(cid:1),
processesimagesthataredownscaledtoafixedsize,andLookup
functionoperatesquickly.Asaresult,thisframeworkenablesreal- whereerepresentstheMLP,and⊕denotesvectorconcatenation.
timeenhancementforimagesofanysize. Wemaketwoimprovementstothecolortransformation.First,we
Thisapproachfacestwomainissues.First,there’stheissueof usethesortedRGBvalues,whicharedenotedbysort([𝑟x,𝑔x,𝑏x]),
limitedexpressivepower.Theenhancementresultsaresummed becausetheyplayanimportantroleinfilterinterpretability.For
linearlyasshowninEq.(1),meaningthatimage-adaptiveparam- instance,intheHSVcolorspace,saturationisdeterminedbythe
eterscannotproducecomplexeffects.Second,the3DLUTslack maximumandminimumRGBvalues.Second,weaddthedifference
interpretablenames.Sincethe3DLUTsareoptimizedsolelyfor
betweene(cid:0)[𝑟x,𝑔x,𝑏x]⊕sort([𝑟x,𝑔x,𝑏x])⊕w(cid:1) ande(cid:0)[𝑟x,𝑔x,𝑏x]⊕
predictingtargetresults,there’snoassurancethattheireffectswill
sort([𝑟x,𝑔x,𝑏x])⊕0(cid:1)
intotheinputRGBvalues.Thisensuresthat
bemeaningfulorunderstandabletohumans.Weaddressthesechal- theoriginalRGBvaluesareretainedintheoutputwhenwisset
lengesbyintroducinghighlyexpressiveandinterpretablefilters. to0,acommoncharacteristicofimageeditingfilters.WedefineEˆ
asafunctionthatappliesEq.(3)toeachpixelofimageX,andthe
4 ProposedMethod imagetransformationprocessisrepresentedasfollows:
Toachievehigh-performingandinterpretableenhancement,we Y=Eˆ(X,w). (4)
maketwocontributions.First,weproposeanovelfilterarchitecture
calledanIA-NILUT.Second,weintroduceapromptguidanceloss LUTbypassing. SinceMLPsinvolvemultiplenonlineartrans-
togiveinterpretablenamestoeachfilter.Weshowtheoverviewin formations,thecomputationalcostissignificant,especiallywhen
Figure1anddescribethecontributionsinthefollowingsections. processinglargesizedimages.Toaddressthisissue,wepropose
CLIPscoreMM’24,October28-November1,2024,Melbourne,VIC,Australia SatoshiKosugi
0 0 0 0 0 0 0 0 0 136112 92 223171118 0 0 0 0 0 0
8 0 0 12 2 3 15 0 0 136112 92 223172122 8 0 0 9 1 2
16 0 0 23 5 9 23 1 1 137113 93 222170124 16 0 0 20 3 6
255255255 255255255 255255255 255255255 255255255
Sampling 82 71 63 151102 64 Sampling Output
points points values
IA-NILUT IA-NILUT
Lookup Lookup
Input Output Input Output Input Output
(a) 3D LUTs (b) IA-NILUT (c) IA-NILUT + LUT bypassing
(Low expressiveness, High efficiency) (High expressiveness, Low efficiency) (High expressiveness, High efficiency)
Figure2:Comparisonbetweenthe3DLUTs[31],theIA-NILUT,andtheIA-NILUTwiththeLUTbypassing.
LUTbypassing.InsteadofdirectlyapplyingtheMLPtoeverypixel visionandlanguagemodelthatembedsimagesandtextwithin
oftheimage,weconverttheMLPintoanLUTandapplythisLUTto thesamefeaturespace.CLIPhasdemonstrateditsabilitytoquan-
theimageasshowninFigure2(c).Eq.(4)istransformedasfollows, titativelyassessvisualimpressions[25].Whenevaluatinganim-
age’s“Exposure,”wecreatepairsofpromptsthatcontrastposi-
Y=E(X,w)=Lookup(X,{I,O}),
tiveandnegativeaspects,suchas“Overexposed photo.”versus
(5)
whereO=Eˆ(I,w). “Underexposed photo.”Wedenotethesimilaritiesbetweentheim-
agefeatureandeachpromptfeatureas𝑠+and𝑠−,respectively.The
ThesamplingpointsI ∈ R𝑁3×3areconsideredasanimagewith image’sExposureimpressioncanbeevaluatedusingtheformula
𝑁3 pixels.ThisisthenconvertedintoObytheMLP.Following exp(𝑠+)/(exp(𝑠+)+exp(𝑠−)).
thisconversion,theinputimageXistransformedusingthelookup Weproposeusingthepairsofpositiveandnegativeprompts
tablecomprisingpairsofIandO.Inourexperiment,weset𝑁 to asguidingpromptstoguidethefilterstowardachievingthede-
33,whichresultsinIbeingtreatedasanimagecomposedof35,937 siredeffects.WeillustrateourmotivationinFigure3.Weprepare
pixels.Forcomparison,a512×512imagecontains262,144pixels, 𝐽 filternamesalongwithpairsofcorrespondingguidingprompts,
indicatingthatIrepresentsarelativelysmallimage.Evenwhen assigningafilternametoeachdimensionofthe 𝐽-dimensional
processinglarge-sizedimages,theMLPisappliedonlytoI,which image-adaptiveparametersw.Duringthetrainingphase,weassess
meansthatthecomputationalcostoftheMLPremainsconstant. theimpressionsoftheenhancedresultswitheachguidingprompt.
LUTbypassingleveragestheexpressivepowerofMLPswhilealso Whenweassignthefiltername“Exposure”to𝑤 1,weexpectthata
benefitingfromthelowcomputationalcostassociatedwithLUTs. changein𝑤 1willonlyaffecttheExposurescore,withoutimpact-
ingotherscoressuchas“Contrast”or“Saturation”asshownin
ComparisonwithadvancedLUT-basedmethods. Recentre-
Figure3(b).IftheContrastandSaturationscoreschangeasshown
searchershavemadevariousimprovementstoLUTstoenhance
inFigure3(c),thiscouldbeconsideredundesiredbehaviorforthe
theirexpressiveness.Forexample,AdaInt[29]makesthesampling
Exposurefilter,potentiallyconfusingusers.Therefore,wepropose
pointsItobeimage-adaptive.CLUTNet[33]usesacompressed
aconstraintthatensuresonlyspecificscoresareaffectedwhen
representationof3DLUTs.Themostsignificantdifferencebetween
parametersarealtered,whileotherscoresremainunchanged.
ourmethodandtheseexistingmethodsliesinthenumberofimage-
Wedefinerandomlysampledweightsasw,anddenotethescores
adaptiveparameters.Theexistingmethodsimproveexpressiveness
c∈R𝐽 evaluatedon𝐽 promptpairsasfollows.
byincreasingthenumberofimage-adaptiveparameters;forexam-
ple,AdaIntandCLUTNetuse99and20image-adaptiveparameters, c=CLIPscore(cid:0) E(X,w)(cid:1). (6)
respectively.However,thisapproachmakesinterpretabilitymore
complex.Toomanyparameterscanmaketheimageeditingprocess Toensurethataspecificfiltereffectisappliedwhen𝑤 𝑗 isaltered,
confusingforusers.Incontrast,ourmethodboostsexpressiveness wedefinethepromptguidanceloss.InsteadofaddingΔ𝑤 𝑗 to𝑤 𝑗,
wedirectlyapplyaconstrainttothegradientinthefollowingway.
byusinganimplicitneuralrepresentation,withoutincreasingthe
numberofimage-adaptiveparameters.Inourexperiments,weuse ∑︁𝐽 (cid:18) (cid:12)𝜕𝑐 𝑗 (cid:12) ∑︁(cid:12)𝜕𝑐 𝑗′ (cid:12)(cid:19)
onlyfiveimage-adaptiveparameters.Thisresultsinafilterarchi- L PG= 𝜆 𝑗(cid:12)
(cid:12)𝜕𝑤
−1(cid:12) (cid:12)+𝜆 (cid:12)
(cid:12)𝜕𝑤
−0(cid:12)
(cid:12)
, (7)
𝑗 𝑗
tecturethat’seasiertounderstand. 𝑗=1 𝑗′≠𝑗
where𝜆
𝑗
and𝜆arehyperparameters.Thisconstraintguarantees
4.2 PromptGuidanceLoss that𝑤 𝑗 affects only the targeted score𝑐 𝑗, while the remaining
Weintroduceapromptguidancelossthatassignsinterpretable scores𝑐 𝑗′(𝑗′ ≠ 𝑗) are unaffected. By minimizing L PG, we can
namestoeachfilter.Inthislossfunction,weutilizeCLIP[21],a assigninterpretablenamestoeachfilter.
epahseR ReshapePrompt-GuidedImage-AdaptiveNeuralImplicitLookupTablesforInterpretableImageEnhancement MM’24,October28-November1,2024,Melbourne,VIC,Australia
: Exposure Table1:Guidingprompts.
: Saturation Filtername Positiveprompt Negativeprompt
Filter names Guiding prompts 𝑤 1 Exposure “Overexposed photo.” “Underexposed photo.”
(FiveK)Contrast “Clear photo.” “Unclear photo.”
Exposure: 𝑤 2 (PPR10K)Contrast “High contrast photo.” “Low contrast photo.”
Contrast:
𝑤 3 Saturation “Full color photo.” “No color photo.”
𝑤 4 Colortemperature “Yellow tinted photo.” “Blue tinted photo.”
Input Output Saturation: 𝑤 5 Tintcorrection “Magenta tinted photo.” “Green tinted photo.”
(a) Impression scoring with CLIP and guiding prompts
5 Experiments
Exposure:
5.1 DatasetsandImplementation
Contrast:
Weutilizetwowidelyuseddatasets:FiveK[4]andPPR10K[17].
Output Saturation: FiveKcontains5,000images,eachretouchedbyfiveexperts.Fol-
lowingthesettingofpreviouspapers[29,33],weuse4,500ofthese
(b) Desired Exposure filter
imagesfortrainingandtheremaining500fortesting,employingthe
imagesretouchedbyExpertCasthetargetimages.Weconductex-
Exposure: perimentsinboth480presolution(wheretheshortersideisresized
Contrast: to480pixels)andtheoriginal4Kresolution.Totrainefficiently,
weperformthetrainingat480presolutionandusetheoriginal4K
Output Saturation: resolutiononlyfortesting.PPR10Kincludes11,161portraitimages,
eachretouchedbythreeexperts.Weconductourexperimentsusing
(c) Undesired Exposure filter
theresultsretouchedbyExpertA.Accordingtotheofficialsetup,
wehave8,875pairsfortrainingand2,286pairsfortesting.Allim-
Figure3:Motivationforourpromptguidanceloss.
agesareusedinaresizedformatat360p.Weevaluateeachmethod
usingPSNR,SSIM[28],andtheL2-distanceinCIELABcolorspace
4.3 TrainingandTesting
(Δ𝐸 𝑎𝑏).Whenmeasuringruntime,weusetheNVIDIARTXA6000
GPU.OurexperimentsarebasedonMMEditingtoolbox[1].
Thepairsofinputandtargetimagesfortrainingaredenotedas
In the IA-NILUT, we employ an MLP consisting of five fully
{X1,T1},...,{X𝐼,T𝐼}.Wedividethetrainingstepsintothreestages.
connectedlayers.ThehiddenfeatureswithinthisMLPare256-
Inthefirsttrainingstage,onlythefiltersEaretrained,usingonly
dimensional,andweutilizethehyperbolictangentasouractivation
thepromptguidancelossL PG.
function.FortheparameterpredictorF,afive-layerCNNisused
E=argminL PG. (8) onFiveK,andResNet18[7]isappliedtoPPR10K,followingthe
E configurationsreportedinpreviousstudies[17,29].
Inthesecondstage,weintroduceimage-adaptiveparametersw1, InspiredbythebasicfiltersinAdobeLightroom,wedefinefive
...,w𝐼 forimagesX1,...,X𝐼.Thetrainingprocessisdefinedas filternamesandemployfivecorrespondingguidingpromptpairsas
outlinedinTable1.FortheContrastfilter,weusedifferentprompts
E,w1,...,w𝐼 = argmin L PG foreachdataset,tailoringthemtoachievethedesiredeffects.
E,w1,...,w𝐼
∑︁𝐼 ∑︁𝐼 (9) 5.2 VisualizationofFilterEffects
+ MSE(T𝑖,E(X𝑖,w𝑖))+ MSE(X𝑖,E(T𝑖,−w𝑖)),
𝑖=1 𝑖=1 Todemonstratethattheproposedmethodachievesinterpretable
filtereffects,wevisualizefiltereffectsinFigure4.Inthesevisual-
whereMSErepresentsthemeansquarederrorfunction.Thethird
izations,onlycertainparametersarevariedwhileothersareheld
termisaconstraintensuringthattheinputimageisreconstructed
constant at 0. These results indicate that each filter produces a
from the target image when the parameters w𝑖 are reversed, a
specificeffectassociatedwiththecorrespondingguidingprompts.
propertythatexistingfiltersalsopossess.Inthefinalstage,the
Figure 5 shows examples of sequential application of predicted
parameterpredictorFistrainedas
parameters,wheretheenhancementprocessisvisualizedinaway
𝐼 thatiseasyforhumanstounderstand.Thesequentialapplication
∑︁
F=argmin MSE(T𝑖,E(X𝑖,F(X𝑖))). (10) ofthefiltereffectsinFigure5isforvisualizationpurposesonly,
F 𝑖=1 andtheallfiltereffectsareappliedsimultaneouslyinpractice.
Attesttime,theenhancementresultsaregeneratedusingthe
trainedEandF,asY=E(X,F(X)).ThefiltersEcanachievefast 5.3 AblationStudies
transformationsthroughtheLUTbypassing,andtheparameter Filterarchitecture. WeusetheIA-NILUTforahighlyexpressive
predictor F resizes the input image to a fixed resolution before filterarchitecture.ToassessthesignificanceoftheIA-NILUT,we
processing,resultinginreal-timeenhancement. train 3D LUTs [31] instead of the IA-NILUT using the prompt
Filters
Filters
Filters
CLIPscore
CLIPscore
CLIPscoreMM’24,October28-November1,2024,Melbourne,VIC,Australia SatoshiKosugi
Exposure Exposure
Contrast Contrast
Saturation Saturation
Color temperature Color temperature
Tint correction Tint correction
Figure4:Visualizationoflearnedfiltereffects.Onlycertainparametersarevariedwhileothersareheldconstantat0.The
imagesontheleftandrightaresamplesfromFiveKandPPR10K,respectively.
Input +Exposure +Contrast +Saturation +Color temperature +Tint correction GGrroouunndd ttrruuthth
Input +Exposure +Contrast +Saturation +Color temperature +Tint correction GGrroouunndd ttrruuthth
Figure5:Sequentialapplicationofpredictedparameters.Thissequentialapplicationisforvisualizationpurposesonly,andthe
alleffectsareappliedsimultaneouslyinpractice.ThetopandbottomimagesaresamplesfromFiveKandPPR10K,respectively.
guidanceloss.Toensuretheoriginalimageispreservedwhenwis inTable3,wedividetheimageintofourpatchesandsequentially
setto0,wemodifyEq.(1)asfollows, applytheMLPtoeachpatch.GiventhatanMLPiscomputation-
Y=X+Lookup(X,{I,O1})×𝑤 1+··· allyintensive,theabsenceoftheLUTbypassingleadstoincreased
(11) computationalcosts,particularlywhenprocessinglarge-sizedim-
+Lookup(X,{I,O𝐽})×𝑤 𝐽.
ages.Incontrast,byemployingtheLUTbypassing,theMLPis
AsshowninTable2,theIA-NILUTachieveshigherperformance, onlyappliedtosamplingpoints,thesizeofwhichareindependent
indicatingthehigherexpressivepoweroftheIA-NILUT.Thefil- oftheoverallimagesize.Inaddition,theLUTbypassinghaslittle
tereffectsofthe3DLUTstrainedwiththepromptguidanceloss effectonthePSNR.Thisapproachleadstocomputationallyefficient
areshowninFigure6.Thedesiredfiltereffectsarenotachieved, enhancementthatisnearlyunaffectedbytheimagesize.
indicatingthattheIA-NILUTisessentialforinterpretablefilters.
Promptguidanceloss. Weemploythepromptguidanceloss
LUTbypassing. WeusetheLUTbypassingtoreducethecomputa- toassigninterpretablenamestoeachfilter.Todemonstratethe
tionalcost.TodemonstratetheeffectivenessoftheLUTbypassing, significanceofthepromptguidanceloss,wepresenttheeffectsof
wepresentacomparisonofPSNRandruntimeinTable3,andacom- filterswhentrainingtheIA-NILUTwithoutthislossinFigure8.Itis
parisonoftherequiredGPUmemoryinFigure7.Whenprocessing difficulttoassigninterpretablenamestothesefilters.Forinstance,
some4Kimagesthatrequiremorememorythantheavailablelimit thefirstfilterinfluencesbothexposureandcolorsimultaneously.Prompt-GuidedImage-AdaptiveNeuralImplicitLookupTablesforInterpretableImageEnhancement MM’24,October28-November1,2024,Melbourne,VIC,Australia
Table2:ComparisonoffilterarchitectureusingFiveK(480p).
35
w/o LUT bypassing
Method PSNR↑ SSIM↑ 𝚫𝑬 ↓ 30 w/ LUT bypassing
𝒂𝒃
3DLUTs[31]w/promptguidanceloss 24.92 0.924 8.23 25
IA-NILUTw/promptguidanceloss 25.22 0.930 7.76 20
15
10
5
0
Exposure
480 960 1440 1920 2400
Image resolution [p]
Figure7:RequiredGPUmemoryw/andw/oLUTbypassing.
Contrast
Saturation
Color temperature
Tint correction
Figure 6: Filter effects of 3DLUTs [31] trained with the
promptguidanceloss.
Table3:EffectivenessoftheLUTbypassingonFiveK.
Figure8:FiltereffectsoftheIA-NILUTwithouttheprompt
480p FullRes.(4K) guidanceloss.
Method
PSNR↑Runtime↓ PSNR↑Runtime↓
Oursw/oLUTbypassing 25.22 1.9ms 25.06 7.8ms
Oursw/LUTbypassing 25.22 1.9ms 25.05 2.0ms uninterpretablemethodsisprovidedsolelyforreference,asour
primaryfocusisoninterpretableimageenhancement.
WepresentquantitativecomparisonsinTable4andvisualcom-
Similarly,thesecondfilteraffectsexposureandsaturationtogether, parisons with otherinterpretable methods in Figure 9. Because
whilethefourthfilterimpactscolorandcontrastatthesametime. ourfiltersarelearnableandoptimizedtopredictthegroundtruth,
Boththethirdandfifthfiltersareabletomodifytheimage’scon- our method achieves better performance than other predefined
trast;ifbothfiltershadthesame“Contrast”name,userswould filter-basedmethods.WhiletheruntimeforExposure’sfiltersand
beconfused.Theseresultshighlightthepromptguidanceloss’s UIE’sfiltersislongduetotheircomplexcolortransformations,
criticalroletoassigninterpretablenamestoeachfilter. theruntimeofourmethodisalmostunaffectedbytheimagesize
thankstotheLUTbypassing.Ourmethodachievescomparable
5.4 ComparisonwiththeState-of-the-Arts performancetothatofuninterpretablemethodsonsomemetrics.
Theseresultshighlightthepotentialofourmethodtobridgethe
Weemployfourinterpretablemethods:D&R[20],Exposure[8],
gapbetweeninterpretabilityandhighperformance.
UIE[12],andRSFNet[19].Forafaircomparison,weutilizeonlythe
filtersadoptedinthesemethodsandapplythesameparameterpre-
5.5 VariousFilterEffects
dictorasours.ForthefiltersfromUIE,weexcludenon-differentiable
filters. Additionally, we include three uninterpretable methods: Byusingdifferentguidingprompts,wecanachievevariousfilter
ourbaselinemethod(3DLUTs[31]),andthetwostate-of-the-art effects.InadditiontotheguidingpromptslistedinTable1,we
methods(AdaInt[29]andCLUTNet[33]).Sincethepre-trained assignadditionalguidingpromptsto𝑤 6andthentrainthefilters
weightsforCLUTNetwithPPR10Karenotpubliclyavailable,we usingonlythepromptguidanceloss.Figure10displaysexamples
onlyshowtheperformanceonFiveK.Theperformanceofthese ofsomeguidingpromptsandtheircorrespondingfiltereffects.Our
]BG[
yromem
UPGMM’24,October28-November1,2024,Melbourne,VIC,Australia SatoshiKosugi
Table4:Quantitativecomparisonson(a)FiveKand(b)PPR10K.Thetopthreemethodsareuninterpretablemethods,whilethe
bottomfiveareinterpretablemethods.
(a)FiveK (b)PPR10K
480p FullResolution(4K) 360p
Method
PSNR↑ SSIM↑ 𝚫𝑬 ↓ Runtime↓ PSNR↑ SSIM↑ 𝚫𝑬 ↓ Runtime↓ PSNR↑ SSIM↑ 𝚫𝑬 ↓
𝒂𝒃 𝒂𝒃 𝒂𝒃
3DLUTs[31] 25.36 0.927 7.56 1.5ms 25.32 0.933 7.61 1.5ms 26.29 0.961 6.58
AdaInt[29] 25.50 0.930 7.47 1.5ms 25.50 0.935 7.46 1.5ms 26.29 0.961 6.59
CLUTNet[33] 25.55 0.931 7.50 1.9ms 25.50 0.935 7.53 2.1ms - - -
D&R’sfilters[20] 23.86 0.903 9.07 1.9ms 23.76 0.907 9.16 1.9ms 24.27 0.934 8.11
Exposure’sfilters[8] 25.04 0.920 7.83 4.3ms 24.91 0.924 7.92 15.9ms 25.53 0.954 7.55
UIE’sfilters[12] 24.74 0.923 8.06 5.0ms 24.61 0.928 8.14 58.9ms 25.45 0.956 7.53
RSFNet’sfilters[19] 24.86 0.924 7.89 2.8ms 24.82 0.928 7.96 2.8ms 25.41 0.946 7.48
PG-IA-NILUT(ours) 25.22 0.930 7.76 1.9ms 25.05 0.934 7.88 2.0ms 26.00 0.957 6.81
Input D&R’s filters Exposure’s filters UIE’s filters RSFNet’s filters Ours Ground truth
Figure9:Visualcomparisonsofinterpretivemethods,withthetopimagefromFiveKandthebottomfromPPR10K.
Table5:ImpactofthepromptguidancelossonFiveK.
Method PSNR↑ SSIM↑ 𝚫𝑬 ↓
𝒂𝒃
Oursw/opromptguidanceloss 25.46 0.930 7.60
Oursw/promptguidanceloss 25.22 0.930 7.76
guidancelossslightlydeterioratesperformanceasshowninTable5.
Apotentialapproachtoimproveperformancewhilepreservingin-
terpretabilityinvolvesrefiningtheselectionoftheguidingprompts.
WeselectedthepromptslistedinTable1heuristically;however,it
remainsuncertainwhethertheyareoptimalforbothinterpretabil-
ityandperformance.Thedevelopmentofanautomaticprompt
Figure10:Filtereffectsbyvariousguidingprompts.
selectionmechanismisidentifiedasanavenueforfutureresearch.
7 Conclusion
filterishighlyexpressive,enablingustoachievevariouseffectsand
Inthispaper,weexploredinterpretableimageenhancement.We
demonstratingitspracticalutilityforimageeditingapplications.
proposedahighlyexpressivefilterarchitecturenamedanIA-NILUT.
Additionally,weintroducedthepromptguidancelosstoassign
6 Limitation
interpretablenamestoeachfilter.Ourexperimentsdemonstrated
Althoughourmethodachievesinterpretableandhigh-performing thatourmethodnotonlyprovidesinterpretabilitybutalsoachieves
enhancement,itencountersadrawbackwheretheuseoftheprompt higherperformancecomparedtoexistinginterpretablefilters.Prompt-GuidedImage-AdaptiveNeuralImplicitLookupTablesforInterpretableImageEnhancement MM’24,October28-November1,2024,Melbourne,VIC,Australia
Acknowledgments
ColorFilters.InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision.12160–12169.
ApartofthisresearchwassupportedbyJSPSKAKENHIGrant
[20] JongchanPark,Joon-YoungLee,DonggeunYoo,andInSoKweon.2018.Distort-
Number23K19997. and-recover:Colorenhancementusingdeepreinforcementlearning.InProceed-
ingsoftheIEEEConferenceonComputerVisionandPatternRecognition.5928–
References 5936.
[21] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,
[1] 2020-08-31. https://github.com/open-mmlab/mmediting SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,
[2] MathieuAubry,SylvainParis,SamuelWHasinoff,JanKautz,andFrédoDurand. etal.2021.Learningtransferablevisualmodelsfromnaturallanguagesupervision.
2014.Fastlocallaplacianfilters:Theoryandapplications.ACMTransactionson InInternationalConferenceonMachineLearning.8748–8763.
Graphics33,5(2014),1–14. [22] TengfeiShi,ChenglizhaoChen,YuanboHe,WenfengSong,andAiminHao.2023.
[3] SimoneBianco,ClaudioCusano,FlavioPiccoli,andRaimondoSchettini.2020. RGBandLUTbasedCrossAttentionNetworkforImageEnhancement.InThe
Personalizedimageenhancementusingneuralsplinecolortransforms. IEEE BritishMachineVisionConference.
TransactionsonImageProcessing29(2020),6223–6236. [23] VincentSitzmann,JulienMartel,AlexanderBergman,DavidLindell,andGordon
[4] VladimirBychkovsky,SylvainParis,EricChan,andFrédoDurand.2011.Learning Wetzstein.2020.Implicitneuralrepresentationswithperiodicactivationfunc-
photographicglobaltonaladjustmentwithadatabaseofinput/outputimagepairs. tions.Advancesinneuralinformationprocessingsystems33(2020),7462–7473.
InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition. [24] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,
97–104. AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.2017. Attentionisall
[5] MarcosVConde,JavierVazquez-Corral,MichaelSBrown,andRaduTimofte. youneed.Advancesinneuralinformationprocessingsystems30(2017).
2024.Nilut:Conditionalneuralimplicit3dlookuptablesforimageenhancement. [25] JianyiWang,KelvinCKChan,andChenChangeLoy.2023.Exploringclipfor
InProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.38.1371–1379. assessingthelookandfeelofimages.InProceedingsoftheAAAIConferenceon
[6] JingwenHe,YihaoLiu,YuQiao,andChaoDong.2020.Conditionalsequential ArtificialIntelligence,Vol.37.2555–2563.
modulationforefficientglobalimageretouching.InProceedingsoftheEuropean [26] TaoWang,YongLi,JingyangPeng,YipengMa,XianWang,FenglongSong,
ConferenceonComputerVision.679–695. andYouliangYan.2021.Real-timeimageenhancervialearnablespatial-aware
[7] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016.Deepresidual 3dlookuptables.InProceedingsoftheIEEE/CVFInternationalConferenceon
learningforimagerecognition.InProceedingsoftheIEEEconferenceoncomputer ComputerVision.2471–2480.
visionandpatternrecognition.770–778. [27] YiliWang,XinLi,KunXu,DongliangHe,QiZhang,FuLi,andErruiDing.2022.
[8] YuanmingHu,HaoHe,ChenxiXu,BaoyuanWang,andStephenLin.2018. NeuralColorOperatorsforSequentialImageRetouching.InProceedingsofthe
Exposure:Awhite-boxphotopost-processingframework.ACMTransactionson EuropeanConferenceonComputerVision.38–55.
Graphics37,2(2018),1–17. [28] ZhouWang,AlanCBovik,HamidRSheikh,andEeroPSimoncelli.2004.Image
[9] HanulKim,Su-MinChoi,Chang-SuKim,andYeongJunKoh.2021.Represen- qualityassessment:fromerrorvisibilitytostructuralsimilarity.IEEEtransactions
tativeColorTransformforImageEnhancement.InProceedingsoftheIEEE/CVF onimageprocessing13,4(2004),600–612.
InternationalConferenceonComputerVision.4459–4468. [29] CanqianYang,MeiguangJin,XuJia,YiXu,andYingChen.2022.AdaInt:Learning
[10] Han-UlKim,YoungJunKoh,andChang-SuKim.2020.GlobalandLocalEnhance- AdaptiveIntervalsfor3DLookupTablesonReal-timeImageEnhancement.In
mentNetworksforPairedandUnpairedImageEnhancement.InProceedingsof ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
theEuropeanConferenceonComputerVision. 17522–17531.
[11] Han-UlKim,YoungJunKoh,andChang-SuKim.2020.PieNet:Personalizedimage [30] CanqianYang,MeiguangJin,YiXu,RuiZhang,YingChen,andHuaidaLiu.
enhancementnetwork.InProceedingsoftheEuropeanConferenceonComputer 2022.SepLUT:SeparableImage-AdaptiveLookupTablesforReal-TimeImage
Vision.374–390. Enhancement.InProceedingsoftheEuropeanConferenceonComputerVision.
[12] SatoshiKosugiandToshihikoYamasaki.2020.Unpairedimageenhancementfea- 201–217.
turingreinforcement-learning-controlledimageeditingsoftware.InProceedings [31] HuiZeng,JianruiCai,LidaLi,ZishengCao,andLeiZhang.2022. Learning
oftheAAAIConferenceonArtificialIntelligence,Vol.34.11296–11303. Image-Adaptive3DLookupTablesforHighPerformancePhotoEnhancementin
[13] SatoshiKosugiandToshihikoYamasaki.2023.Crowd-PoweredPhotoEnhance- Real-Time.IEEETransactionsonPatternAnalysis&MachineIntelligence44,04
mentFeaturinganActiveLearningBasedLocalFilter. IEEETransactionson (2022),2058–2073.
CircuitsandSystemsforVideoTechnology33,7(2023),3145–3158. [32] FengZhang,MingTian,ZhiqiangLi,BinXu,QingboLu,ChangxinGao,andNong
[14] SatoshiKosugiandToshihikoYamasaki.2024.PersonalizedImageEnhancement Sang.2024.LookupTablemeetsLocalLaplacianFilter:PyramidReconstruction
FeaturingMaskedStyleModeling.IEEETransactionsonCircuitsandSystemsfor NetworkforToneMapping.AdvancesinNeuralInformationProcessingSystems
VideoTechnology34,1(2024),140–152. 36(2024).
[15] YukiKoyama,IsseiSato,DaisukeSakamoto,andTakeoIgarashi.2017.Sequential [33] FengyiZhang,HuiZeng,TianjunZhang,andLinZhang.2022.Clut-net:Learning
linesearchforefficientvisualdesignoptimizationbycrowds.ACMTransactions adaptivelycompressedrepresentationsof3dlutsforlightweightimageenhance-
onGraphics(TOG)36,4(2017),1–11. ment.InProceedingsofthe30thACMInternationalConferenceonMultimedia.
[16] ChongyiLi,ChunleGuo,ShangchenZhou,QimingAi,RuichengFeng,and 6493–6501.
ChenChangeLoy.2023.FlexiCurve:FlexiblePiecewiseCurvesEstimationfor [34] FengyiZhang,LinZhang,TianjunZhang,andDongqingWang.2023. Adap-
PhotoRetouching.InProceedingsoftheIEEE/CVFConferenceonComputerVision tivelyHashing3DLUTsforLightweightReal-timeImageEnhancement.InIEEE
andPatternRecognitionWorkshops.1092–1101. InternationalConferenceonMultimediaandExpo.2771–2776.
[17] JieLiang,HuiZeng,MiaomiaoCui,XuansongXie,andLeiZhang.2021.Ppr10k: [35] ZhaoyangZhang,YitongJiang,JunJiang,XiaogangWang,PingLuo,andJinwei
Alarge-scaleportraitphotoretouchingdatasetwithhuman-regionmaskand Gu.2021. STAR:AStructure-AwareLightweightTransformerforReal-Time
group-levelconsistency.InProceedingsoftheIEEE/CVFConferenceonComputer ImageEnhancement.InProceedingsoftheIEEE/CVFInternationalConferenceon
VisionandPatternRecognition.653–661. ComputerVision.4106–4115.
[18] ChengxuLiu,HuanYang,JianlongFu,andXuemingQian.2023.4DLUT:learn- [36] LinZhao,Shao-PingLu,TaoChen,ZhengluYang,andArielShamir.2021.Deep
ablecontext-aware4dlookuptableforimageenhancement.IEEETransactions SymmetricNetworkforUnderexposedImageEnhancementwithRecurrent
onImageProcessing32(2023),4742–4756. AttentionalLearning.InProceedingsoftheIEEE/CVFInternationalConferenceon
[19] WenqiOuyang,YiDong,XiaoyangKang,PeiranRen,XinXu,andXuansongXie. ComputerVision.12075–12084.
2023.RSFNet:AWhite-BoxImageRetouchingApproachusingRegion-SpecificSupplementary Material
A ComparisonofNetworkParameters
TableApresentsacomparisonofthenumberofnetworkparam-
eters. Our method has a slightly higher number of parameters
Exposure
comparedtootherpredefinedfilter-basedmethodsduetotheinclu-
sionofanMLPinourIA-NILUT.However,ourmethodusesfewer
parametersthan3DLUTsandAdaInt,whichutilizemultipleLUTs
containingmoreparametersthanourMLP.
Contrast
B AblationStudyaboutSortedRGBValues
IntheIA-NILUT,weusethesortedRGBvaluesasdescribedinEq.
(3)ofthemainpaper.Todemonstratetheeffectivenessofthesorted
Saturation
RGBvalues,wepresentthefiltereffectswhenthesortedRGBvalues
arenotusedinFigureA.Thecolortemperaturefilterisexpected
toonlyaffectcolor;however,withoutthesortedRGBvalues,it
alsoimpactscontrast.Thisresulthighlightstheimportanceofthe
Color temperature
sortedRGBvaluesforeachfiltertoachievethedesiredeffects.
C AblationStudyaboutthePromptGuidance
Loss
Tint correction
Tofurtheranalyzethepromptguidanceloss,weexcludethecon-
FigureA:FiltereffectsoftheIA-NILUTwithoutthesorted
straintsontheuntargetedCLIPscoresasfollows,
RGBvalues.
L P′ G=∑︁𝐽 (cid:18) 𝜆 𝑗(cid:12) (cid:12) (cid:12)𝜕𝜕 𝑤𝑐 𝑗
𝑗
−1(cid:12) (cid:12) (cid:12)(cid:19) . (A)
𝑗=1
WepresenttheeffectsoffilterswhentrainingtheIA-NILUTwith
L′ inFigureB.WhenweuseL′ ,allCLIPscorescanbechanged; Exposure
PG PG
therefore,thedesiredeffectisnotachieved.Thisresulthighlights
thesignificanceofourdesignofthepromptguidanceloss.
Contrast
D AdditionalVisualizationsofFilterEffects
WepresentadditionalvisualizationsoffiltereffectsinFiguresC
andD,whereeachfilterproducesaspecificeffectassociatedwith
thecorrespondingguidingprompts.Weshowfurtherexamplesof
Saturation
sequentiallyapplyingpredictedparametersinFiguresEandF.The
enhancementprocesscanbevisualizedinawaythatiseasyfor
humanstounderstand.
Color temperature
TableA:Comparisonofnetworkparameters.
#Network
Method
parameters
Tint correction
3DLUTs[31] 593.5K FigureB:FiltereffectsoftheIA-NILUTwithL′ .
AdaInt[29] 619.7K PG
CLUTNet[33] 278.7K
D&R’sfilters[20] 248.6K
Exposure’sfilters[8] 266.0K
UIE’sfilters[12] 250.6K
RSFNet’sfilters[19] 250.6K
PG-IA-NILUT(ours) 449.3KPrompt-GuidedImage-AdaptiveNeuralImplicitLookupTablesforInterpretableImageEnhancement MM’24,October28-November1,2024,Melbourne,VIC,Australia
Exposure Exposure
Contrast Contrast
Saturation Saturation
Color temperature Color temperature
Tint correction Tint correction
Exposure Exposure
Contrast Contrast
Saturation Saturation
Color temperature Color temperature
Tint correction Tint correction
FigureC:Visualizationoflearnedfiltereffects.Onlycertainparametersarevariedwhileothersareheldconstantat0.The
imagesaresamplesfromFiveK.MM’24,October28-November1,2024,Melbourne,VIC,Australia SatoshiKosugi
Exposure Exposure
Contrast Contrast
Saturation Saturation
Color temperature Color temperature
Tint correction Tint correction
Exposure Exposure
Contrast Contrast
Saturation Saturation
Color temperature Color temperature
Tint correction Tint correction
FigureD:Visualizationoflearnedfiltereffects.Onlycertainparametersarevariedwhileothersareheldconstantat0.The
imagesaresamplesfromPPR10K.Prompt-GuidedImage-AdaptiveNeuralImplicitLookupTablesforInterpretableImageEnhancement MM’24,October28-November1,2024,Melbourne,VIC,Australia
Input +Exposure +Contrast +Saturation +Color temperature +Tint correction GGrroouunndd ttrruuthth
Input +Exposure +Contrast +Saturation +Color temperature +Tint correction GGrroouunndd ttrruuthth
Input +Exposure +Contrast +Saturation +Color temperature +Tint correction GGrroouunndd ttrruuthth
Input +Exposure +Contrast +Saturation +Color temperature +Tint correction GGrroouunndd ttrruuthth
FigureE:Sequentialapplicationofpredictedparameters.Thissequentialapplicationisforvisualizationpurposesonly,andthe
alleffectsareappliedsimultaneouslyinpractice.TheimagesaresamplesfromFiveK.MM’24,October28-November1,2024,Melbourne,VIC,Australia SatoshiKosugi
Input +Exposure +Contrast +Saturation +Color temperature +Tint correction GGrroouunndd ttrruuthth
Input +Exposure +Contrast +Saturation +Color temperature +Tint correction GGrroouunndd ttrruuthth
Input +Exposure +Contrast +Saturation +Color temperature +Tint correction GGrroouunndd ttrruuthth
Input +Exposure +Contrast +Saturation +Color temperature +Tint correction GGrroouunndd ttrruuthth
FigureF:Sequentialapplicationofpredictedparameters.Thissequentialapplicationisforvisualizationpurposesonly,andthe
alleffectsareappliedsimultaneouslyinpractice.TheimagesaresamplesfromPPR10K.