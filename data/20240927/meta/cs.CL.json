[
    {
        "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models",
        "authors": "Matt DeitkeChristopher ClarkSangho LeeRohun TripathiYue YangJae Sung ParkMohammadreza SalehiNiklas MuennighoffKyle LoLuca SoldainiJiasen LuTaira AndersonErin BransomKiana EhsaniHuong NgoYenSung ChenAjay PatelMark YatskarChris Callison-BurchAndrew HeadRose HendrixFavyen BastaniEli VanderBiltNathan LambertYvonne ChouArnavi ChhedaJenna SparksSam SkjonsbergMichael SchmitzAaron SarnatByron BischoffPete WalshChris NewellPiper WoltersTanmay GuptaKuo-Hao ZengJon BorchardtDirk GroeneveldJen DumasCrystal NamSophie LebrechtCaitlin WittlifCarissa SchoenickOscar MichelRanjay KrishnaLuca WeihsNoah A. SmithHannaneh HajishirziRoss GirshickAli FarhadiAniruddha Kembhavi",
        "links": "http://arxiv.org/abs/2409.17146v1",
        "entry_id": "http://arxiv.org/abs/2409.17146v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17146v1",
        "summary": "Today's most advanced multimodal models remain proprietary. The strongest\nopen-weight models rely heavily on synthetic data from proprietary VLMs to\nachieve good performance, effectively distilling these closed models into open\nones. As a result, the community is still missing foundational knowledge about\nhow to build performant VLMs from scratch. We present Molmo, a new family of\nVLMs that are state-of-the-art in their class of openness. Our key innovation\nis a novel, highly detailed image caption dataset collected entirely from human\nannotators using speech-based descriptions. To enable a wide array of user\ninteractions, we also introduce a diverse dataset mixture for fine-tuning that\nincludes in-the-wild Q&A and innovative 2D pointing data. The success of our\napproach relies on careful choices for the model architecture details, a\nwell-tuned training pipeline, and, most critically, the quality of our newly\ncollected datasets, all of which will be released. The best-in-class 72B model\nwithin the Molmo family not only outperforms others in the class of open weight\nand data models but also compares favorably against proprietary systems like\nGPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human\nevaluation.\n  We will be releasing all of our model weights, captioning and fine-tuning\ndata, and source code in the near future. Select model weights, inference code,\nand demo are available at https://molmo.allenai.org.",
        "updated": "2024-09-25 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17146v1"
    },
    {
        "title": "FineZip : Pushing the Limits of Large Language Models for Practical Lossless Text Compression",
        "authors": "Fazal MittuYihuan BuAkshat GuptaAshok DevireddyAlp Eren OzdarendeliAnant SinghGopala Anumanchipalli",
        "links": "http://arxiv.org/abs/2409.17141v1",
        "entry_id": "http://arxiv.org/abs/2409.17141v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17141v1",
        "summary": "While the language modeling objective has been shown to be deeply connected\nwith compression, it is surprising that modern LLMs are not employed in\npractical text compression systems. In this paper, we provide an in-depth\nanalysis of neural network and transformer-based compression techniques to\nanswer this question. We compare traditional text compression systems with\nneural network and LLM-based text compression methods. Although LLM-based\nsystems significantly outperform conventional compression methods, they are\nhighly impractical. Specifically, LLMZip, a recent text compression system\nusing Llama3-8B requires 9.5 days to compress just 10 MB of text, although with\nhuge improvements in compression ratios. To overcome this, we present FineZip -\na novel LLM-based text compression system that combines ideas of online\nmemorization and dynamic context to reduce the compression time immensely.\nFineZip can compress the above corpus in approximately 4 hours compared to 9.5\ndays, a 54 times improvement over LLMZip and comparable performance. FineZip\noutperforms traditional algorithmic compression methods with a large margin,\nimproving compression ratios by approximately 50\\%. With this work, we take the\nfirst step towards making lossless text compression with LLMs a reality. While\nFineZip presents a significant step in that direction, LLMs are still not a\nviable solution for large-scale text compression. We hope our work paves the\nway for future research and innovation to solve this problem.",
        "updated": "2024-09-25 17:58:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17141v1"
    },
    {
        "title": "Assessing the Level of Toxicity Against Distinct Groups in Bangla Social Media Comments: A Comprehensive Investigation",
        "authors": "Mukaffi Bin MoinPronay DebnathUsafa Akther RifaRijeet Bin Anis",
        "links": "http://arxiv.org/abs/2409.17130v1",
        "entry_id": "http://arxiv.org/abs/2409.17130v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17130v1",
        "summary": "Social media platforms have a vital role in the modern world, serving as\nconduits for communication, the exchange of ideas, and the establishment of\nnetworks. However, the misuse of these platforms through toxic comments, which\ncan range from offensive remarks to hate speech, is a concerning issue. This\nstudy focuses on identifying toxic comments in the Bengali language targeting\nthree specific groups: transgender people, indigenous people, and migrant\npeople, from multiple social media sources. The study delves into the intricate\nprocess of identifying and categorizing toxic language while considering the\nvarying degrees of toxicity: high, medium, and low. The methodology involves\ncreating a dataset, manual annotation, and employing pre-trained transformer\nmodels like Bangla-BERT, bangla-bert-base, distil-BERT, and\nBert-base-multilingual-cased for classification. Diverse assessment metrics\nsuch as accuracy, recall, precision, and F1-score are employed to evaluate the\nmodel's effectiveness. The experimental findings reveal that Bangla-BERT\nsurpasses alternative models, achieving an F1-score of 0.8903. This research\nexposes the complexity of toxicity in Bangla social media dialogues, revealing\nits differing impacts on diverse demographic groups.",
        "updated": "2024-09-25 17:48:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17130v1"
    },
    {
        "title": "Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Handy Appetizer",
        "authors": "Benji PengXuanhe PanYizhu WenZiqian BiKeyu ChenMing LiMing LiuQian NiuJunyu LiuJinlang WangSen ZhangJiawei XuPohsun Feng",
        "links": "http://arxiv.org/abs/2409.17120v1",
        "entry_id": "http://arxiv.org/abs/2409.17120v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17120v1",
        "summary": "This book explores the role of Artificial Intelligence (AI), Machine Learning\n(ML), and Deep Learning (DL) in driving the progress of big data analytics and\nmanagement. The book focuses on simplifying the complex mathematical concepts\nbehind deep learning, offering intuitive visualizations and practical case\nstudies to help readers understand how neural networks and technologies like\nConvolutional Neural Networks (CNNs) work. It introduces several classic models\nand technologies such as Transformers, GPT, ResNet, BERT, and YOLO,\nhighlighting their applications in fields like natural language processing,\nimage recognition, and autonomous driving. The book also emphasizes the\nimportance of pre-trained models and how they can enhance model performance and\naccuracy, with instructions on how to apply these models in various real-world\nscenarios. Additionally, it provides an overview of key big data management\ntechnologies like SQL and NoSQL databases, as well as distributed computing\nframeworks such as Apache Hadoop and Spark, explaining their importance in\nmanaging and processing vast amounts of data. Ultimately, the book underscores\nthe value of mastering deep learning and big data management skills as critical\ntools for the future workforce, making it an essential resource for both\nbeginners and experienced professionals.",
        "updated": "2024-09-25 17:31:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17120v1"
    },
    {
        "title": "Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale",
        "authors": "Fan ZhouZengzhi WangQian LiuJunlong LiPengfei Liu",
        "links": "http://arxiv.org/abs/2409.17115v1",
        "entry_id": "http://arxiv.org/abs/2409.17115v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17115v1",
        "summary": "Large language model pre-training has traditionally relied on human experts\nto craft heuristics for improving the corpora quality, resulting in numerous\nrules developed to date. However, these rules lack the flexibility to address\nthe unique characteristics of individual example effectively. Meanwhile,\napplying tailored rules to every example is impractical for human experts. In\nthis paper, we demonstrate that even small language models, with as few as 0.3B\nparameters, can exhibit substantial data refining capabilities comparable to\nthose of human experts. We introduce Programming Every Example (ProX), a novel\nframework that treats data refinement as a programming task, enabling models to\nrefine corpora by generating and executing fine-grained operations, such as\nstring normalization, for each individual example at scale. Experimental\nresults show that models pre-trained on ProX-curated data outperform either\noriginal data or data filtered by other selection methods by more than 2%\nacross various downstream benchmarks. Its effectiveness spans various model\nsizes and pre-training corpora, including C4, RedPajama-V2, and FineWeb.\nFurthermore, ProX exhibits significant potential in domain-specific continual\npre-training: without domain specific design, models trained on OpenWebMath\nrefined by ProX outperform human-crafted rule-based methods, improving average\naccuracy by 7.6% over Mistral-7B, with 14.6% for Llama-2-7B and 20.3% for\nCodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7B\ntrained on 200B tokens. Further analysis highlights that ProX significantly\nsaves training FLOPs, offering a promising path for efficient LLM\npre-training.We are open-sourcing ProX with >100B corpus, models, and sharing\nall training and implementation details for reproducible research and future\ninnovation. Code: https://github.com/GAIR-NLP/ProX",
        "updated": "2024-09-25 17:28:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17115v1"
    }
]