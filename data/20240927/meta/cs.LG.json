[
    {
        "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models",
        "authors": "Matt DeitkeChristopher ClarkSangho LeeRohun TripathiYue YangJae Sung ParkMohammadreza SalehiNiklas MuennighoffKyle LoLuca SoldainiJiasen LuTaira AndersonErin BransomKiana EhsaniHuong NgoYenSung ChenAjay PatelMark YatskarChris Callison-BurchAndrew HeadRose HendrixFavyen BastaniEli VanderBiltNathan LambertYvonne ChouArnavi ChhedaJenna SparksSam SkjonsbergMichael SchmitzAaron SarnatByron BischoffPete WalshChris NewellPiper WoltersTanmay GuptaKuo-Hao ZengJon BorchardtDirk GroeneveldJen DumasCrystal NamSophie LebrechtCaitlin WittlifCarissa SchoenickOscar MichelRanjay KrishnaLuca WeihsNoah A. SmithHannaneh HajishirziRoss GirshickAli FarhadiAniruddha Kembhavi",
        "links": "http://arxiv.org/abs/2409.17146v1",
        "entry_id": "http://arxiv.org/abs/2409.17146v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17146v1",
        "summary": "Today's most advanced multimodal models remain proprietary. The strongest\nopen-weight models rely heavily on synthetic data from proprietary VLMs to\nachieve good performance, effectively distilling these closed models into open\nones. As a result, the community is still missing foundational knowledge about\nhow to build performant VLMs from scratch. We present Molmo, a new family of\nVLMs that are state-of-the-art in their class of openness. Our key innovation\nis a novel, highly detailed image caption dataset collected entirely from human\nannotators using speech-based descriptions. To enable a wide array of user\ninteractions, we also introduce a diverse dataset mixture for fine-tuning that\nincludes in-the-wild Q&A and innovative 2D pointing data. The success of our\napproach relies on careful choices for the model architecture details, a\nwell-tuned training pipeline, and, most critically, the quality of our newly\ncollected datasets, all of which will be released. The best-in-class 72B model\nwithin the Molmo family not only outperforms others in the class of open weight\nand data models but also compares favorably against proprietary systems like\nGPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human\nevaluation.\n  We will be releasing all of our model weights, captioning and fine-tuning\ndata, and source code in the near future. Select model weights, inference code,\nand demo are available at https://molmo.allenai.org.",
        "updated": "2024-09-25 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17146v1"
    },
    {
        "title": "DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion",
        "authors": "Yukun HuangJianan WangAiling ZengZheng-Jun ZhaLei ZhangXihui Liu",
        "links": "http://arxiv.org/abs/2409.17145v1",
        "entry_id": "http://arxiv.org/abs/2409.17145v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17145v1",
        "summary": "Leveraging pretrained 2D diffusion models and score distillation sampling\n(SDS), recent methods have shown promising results for text-to-3D avatar\ngeneration. However, generating high-quality 3D avatars capable of expressive\nanimation remains challenging. In this work, we present DreamWaltz-G, a novel\nlearning framework for animatable 3D avatar generation from text. The core of\nthis framework lies in Skeleton-guided Score Distillation and Hybrid 3D\nGaussian Avatar representation. Specifically, the proposed skeleton-guided\nscore distillation integrates skeleton controls from 3D human templates into 2D\ndiffusion models, enhancing the consistency of SDS supervision in terms of view\nand human pose. This facilitates the generation of high-quality avatars,\nmitigating issues such as multiple faces, extra limbs, and blurring. The\nproposed hybrid 3D Gaussian avatar representation builds on the efficient 3D\nGaussians, combining neural implicit fields and parameterized 3D meshes to\nenable real-time rendering, stable SDS optimization, and expressive animation.\nExtensive experiments demonstrate that DreamWaltz-G is highly effective in\ngenerating and animating 3D avatars, outperforming existing methods in both\nvisual quality and animation expressiveness. Our framework further supports\ndiverse applications, including human video reenactment and multi-subject scene\ncomposition.",
        "updated": "2024-09-25 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17145v1"
    },
    {
        "title": "Differential Privacy Regularization: Protecting Training Data Through Loss Function Regularization",
        "authors": "Francisco Aguilera-MartínezFernando Berzal",
        "links": "http://arxiv.org/abs/2409.17144v1",
        "entry_id": "http://arxiv.org/abs/2409.17144v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17144v1",
        "summary": "Training machine learning models based on neural networks requires large\ndatasets, which may contain sensitive information. The models, however, should\nnot expose private information from these datasets. Differentially private SGD\n[DP-SGD] requires the modification of the standard stochastic gradient descent\n[SGD] algorithm for training new models. In this short paper, a novel\nregularization strategy is proposed to achieve the same goal in a more\nefficient manner.",
        "updated": "2024-09-25 17:59:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17144v1"
    },
    {
        "title": "FineZip : Pushing the Limits of Large Language Models for Practical Lossless Text Compression",
        "authors": "Fazal MittuYihuan BuAkshat GuptaAshok DevireddyAlp Eren OzdarendeliAnant SinghGopala Anumanchipalli",
        "links": "http://arxiv.org/abs/2409.17141v1",
        "entry_id": "http://arxiv.org/abs/2409.17141v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17141v1",
        "summary": "While the language modeling objective has been shown to be deeply connected\nwith compression, it is surprising that modern LLMs are not employed in\npractical text compression systems. In this paper, we provide an in-depth\nanalysis of neural network and transformer-based compression techniques to\nanswer this question. We compare traditional text compression systems with\nneural network and LLM-based text compression methods. Although LLM-based\nsystems significantly outperform conventional compression methods, they are\nhighly impractical. Specifically, LLMZip, a recent text compression system\nusing Llama3-8B requires 9.5 days to compress just 10 MB of text, although with\nhuge improvements in compression ratios. To overcome this, we present FineZip -\na novel LLM-based text compression system that combines ideas of online\nmemorization and dynamic context to reduce the compression time immensely.\nFineZip can compress the above corpus in approximately 4 hours compared to 9.5\ndays, a 54 times improvement over LLMZip and comparable performance. FineZip\noutperforms traditional algorithmic compression methods with a large margin,\nimproving compression ratios by approximately 50\\%. With this work, we take the\nfirst step towards making lossless text compression with LLMs a reality. While\nFineZip presents a significant step in that direction, LLMs are still not a\nviable solution for large-scale text compression. We hope our work paves the\nway for future research and innovation to solve this problem.",
        "updated": "2024-09-25 17:58:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17141v1"
    },
    {
        "title": "Learning with Dynamics: Autonomous Regulation of UAV Based Communication Networks with Dynamic UAV Crew",
        "authors": "Ran ZhangBowei LiLiyuan ZhangJiangXieMiao Wang",
        "links": "http://arxiv.org/abs/2409.17139v1",
        "entry_id": "http://arxiv.org/abs/2409.17139v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17139v1",
        "summary": "Unmanned Aerial Vehicle (UAV) based communication networks (UCNs) are a key\ncomponent in future mobile networking. To handle the dynamic environments in\nUCNs, reinforcement learning (RL) has been a promising solution attributed to\nits strong capability of adaptive decision-making free of the environment\nmodels. However, most existing RL-based research focus on control strategy\ndesign assuming a fixed set of UAVs. Few works have investigated how UCNs\nshould be adaptively regulated when the serving UAVs change dynamically. This\narticle discusses RL-based strategy design for adaptive UCN regulation given a\ndynamic UAV set, addressing both reactive strategies in general UCNs and\nproactive strategies in solar-powered UCNs. An overview of the UCN and the RL\nframework is first provided. Potential research directions with key challenges\nand possible solutions are then elaborated. Some of our recent works are\npresented as case studies to inspire innovative ways to handle dynamic UAV crew\nwith different RL algorithms.",
        "updated": "2024-09-25 17:57:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17139v1"
    }
]