[
    {
        "title": "Non-asymptotic convergence analysis of the stochastic gradient Hamiltonian Monte Carlo algorithm with discontinuous stochastic gradient with applications to training of ReLU neural networks",
        "authors": "Luxu LiangAriel NeufeldYing Zhang",
        "links": "http://arxiv.org/abs/2409.17107v1",
        "entry_id": "http://arxiv.org/abs/2409.17107v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17107v1",
        "summary": "In this paper, we provide a non-asymptotic analysis of the convergence of the\nstochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm to a target\nmeasure in Wasserstein-1 and Wasserstein-2 distance. Crucially, compared to the\nexisting literature on SGHMC, we allow its stochastic gradient to be\ndiscontinuous. This allows us to provide explicit upper bounds, which can be\ncontrolled to be arbitrarily small, for the expected excess risk of non-convex\nstochastic optimization problems with discontinuous stochastic gradients,\nincluding, among others, the training of neural networks with ReLU activation\nfunction. To illustrate the applicability of our main results, we consider\nnumerical experiments on quantile estimation and on several optimization\nproblems involving ReLU neural networks relevant in finance and artificial\nintelligence.",
        "updated": "2024-09-25 17:21:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17107v1"
    },
    {
        "title": "Parameter-efficient Bayesian Neural Networks for Uncertainty-aware Depth Estimation",
        "authors": "Richard D. PaulAlessio QuerciaVincent FortuinKatharina NöhHanno Scharr",
        "links": "http://arxiv.org/abs/2409.17085v1",
        "entry_id": "http://arxiv.org/abs/2409.17085v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17085v1",
        "summary": "State-of-the-art computer vision tasks, like monocular depth estimation\n(MDE), rely heavily on large, modern Transformer-based architectures. However,\ntheir application in safety-critical domains demands reliable predictive\nperformance and uncertainty quantification. While Bayesian neural networks\nprovide a conceptually simple approach to serve those requirements, they suffer\nfrom the high dimensionality of the parameter space. Parameter-efficient\nfine-tuning (PEFT) methods, in particular low-rank adaptations (LoRA), have\nemerged as a popular strategy for adapting large-scale models to down-stream\ntasks by performing parameter inference on lower-dimensional subspaces. In this\nwork, we investigate the suitability of PEFT methods for subspace Bayesian\ninference in large-scale Transformer-based vision models. We show that, indeed,\ncombining BitFit, DiffFit, LoRA, and CoLoRA, a novel LoRA-inspired PEFT method,\nwith Bayesian inference enables more robust and reliable predictive performance\nin MDE.",
        "updated": "2024-09-25 16:49:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17085v1"
    },
    {
        "title": "Dimension reduction and the gradient flow of relative entropy",
        "authors": "Ben Weinkove",
        "links": "http://arxiv.org/abs/2409.16963v1",
        "entry_id": "http://arxiv.org/abs/2409.16963v1",
        "pdf_url": "http://arxiv.org/pdf/2409.16963v1",
        "summary": "Dimension reduction, widely used in science, maps high-dimensional data into\nlow-dimensional space. We investigate a basic mathematical model underlying the\ntechniques of stochastic neighborhood embedding (SNE) and its popular variant\nt-SNE. Distances between points in high dimensions are used to define a\nprobability distribution on pairs of points, measuring how similar the points\nare. The aim is to map these points to low dimensions in an optimal way so that\nsimilar points are closer together. This is carried out by minimizing the\nrelative entropy between two probability distributions.\n  We consider the gradient flow of the relative entropy and analyze its\nlong-time behavior. This is a self-contained mathematical problem about the\nbehavior of a system of nonlinear ordinary differential equations. We find\noptimal bounds for the diameter of the evolving sets as time tends to infinity.\nIn particular, the diameter may blow up for the t-SNE version, but remains\nbounded for SNE.",
        "updated": "2024-09-25 14:23:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.16963v1"
    },
    {
        "title": "Revisiting Extragradient-Type Methods -- Part 1: Generalizations and Sublinear Convergence Rates",
        "authors": "Quoc Tran-DinhNghia Nguyen-Trung",
        "links": "http://arxiv.org/abs/2409.16859v1",
        "entry_id": "http://arxiv.org/abs/2409.16859v1",
        "pdf_url": "http://arxiv.org/pdf/2409.16859v1",
        "summary": "This paper presents a comprehensive analysis of the well-known extragradient\n(EG) method for solving both equations and inclusions. First, we unify and\ngeneralize EG for [non]linear equations to a wider class of algorithms,\nencompassing various existing schemes and potentially new variants. Next, we\nanalyze both sublinear ``best-iterate'' and ``last-iterate'' convergence rates\nfor the entire class of algorithms, and derive new convergence results for two\nwell-known instances. Second, we extend our EG framework above to ``monotone''\ninclusions, introducing a new class of algorithms and its corresponding\nconvergence results. Third, we also unify and generalize Tseng's\nforward-backward-forward splitting (FBFS) method to a broader class of\nalgorithms to solve [non]linear inclusions when a weak-Minty solution exists,\nand establish its ``best-iterate'' convergence rate. Fourth, to complete our\npicture, we also investigate sublinear rates of two other common variants of EG\nusing our EG analysis framework developed here: the reflected forward-backward\nsplitting and the golden ratio methods. Finally, we conduct an extensive\nnumerical experiment to validate our theoretical findings. Our results\ndemonstrate that several new variants of our proposed algorithms outperform\nexisting schemes in the majority of examples.",
        "updated": "2024-09-25 12:14:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.16859v1"
    },
    {
        "title": "Conditional Testing based on Localized Conformal p-values",
        "authors": "Xiaoyang WuLin LuZhaojun WangChangliang Zou",
        "links": "http://arxiv.org/abs/2409.16829v1",
        "entry_id": "http://arxiv.org/abs/2409.16829v1",
        "pdf_url": "http://arxiv.org/pdf/2409.16829v1",
        "summary": "In this paper, we address conditional testing problems through the conformal\ninference framework. We define the localized conformal p-values by inverting\nprediction intervals and prove their theoretical properties. These defined\np-values are then applied to several conditional testing problems to illustrate\ntheir practicality. Firstly, we propose a conditional outlier detection\nprocedure to test for outliers in the conditional distribution with\nfinite-sample false discovery rate (FDR) control. We also introduce a novel\nconditional label screening problem with the goal of screening multivariate\nresponse variables and propose a screening procedure to control the family-wise\nerror rate (FWER). Finally, we consider the two-sample conditional distribution\ntest and define a weighted U-statistic through the aggregation of localized\np-values. Numerical simulations and real-data examples validate the superior\nperformance of our proposed strategies.",
        "updated": "2024-09-25 11:30:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.16829v1"
    }
]