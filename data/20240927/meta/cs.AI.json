[
    {
        "title": "Differential Privacy Regularization: Protecting Training Data Through Loss Function Regularization",
        "authors": "Francisco Aguilera-MartínezFernando Berzal",
        "links": "http://arxiv.org/abs/2409.17144v1",
        "entry_id": "http://arxiv.org/abs/2409.17144v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17144v1",
        "summary": "Training machine learning models based on neural networks requires large\ndatasets, which may contain sensitive information. The models, however, should\nnot expose private information from these datasets. Differentially private SGD\n[DP-SGD] requires the modification of the standard stochastic gradient descent\n[SGD] algorithm for training new models. In this short paper, a novel\nregularization strategy is proposed to achieve the same goal in a more\nefficient manner.",
        "updated": "2024-09-25 17:59:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17144v1"
    },
    {
        "title": "Attention Prompting on Image for Large Vision-Language Models",
        "authors": "Runpeng YuWeihao YuXinchao Wang",
        "links": "http://arxiv.org/abs/2409.17143v1",
        "entry_id": "http://arxiv.org/abs/2409.17143v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17143v1",
        "summary": "Compared with Large Language Models (LLMs), Large Vision-Language Models\n(LVLMs) can also accept images as input, thus showcasing more interesting\nemergent capabilities and demonstrating impressive performance on various\nvision-language tasks. Motivated by text prompting in LLMs, visual prompting\nhas been explored to enhance LVLMs' capabilities of perceiving visual\ninformation. However, previous visual prompting techniques solely process\nvisual inputs without considering text queries, limiting the models' ability to\nfollow text instructions to complete tasks. To fill this gap, in this work, we\npropose a new prompting technique named Attention Prompting on Image, which\njust simply overlays a text-query-guided attention heatmap on the original\ninput image and effectively enhances LVLM on various tasks. Specifically, we\ngenerate an attention heatmap for the input image dependent on the text query\nwith an auxiliary model like CLIP. Then the heatmap simply multiplies the pixel\nvalues of the original image to obtain the actual input image for the LVLM.\nExtensive experiments on various vison-language benchmarks verify the\neffectiveness of our technique. For example, Attention Prompting on Image\nimproves LLaVA-1.5 by 3.8% and 2.9% on MM-Vet and LLaVA-Wild benchmarks,\nrespectively.",
        "updated": "2024-09-25 17:59:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17143v1"
    },
    {
        "title": "FineZip : Pushing the Limits of Large Language Models for Practical Lossless Text Compression",
        "authors": "Fazal MittuYihuan BuAkshat GuptaAshok DevireddyAlp Eren OzdarendeliAnant SinghGopala Anumanchipalli",
        "links": "http://arxiv.org/abs/2409.17141v1",
        "entry_id": "http://arxiv.org/abs/2409.17141v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17141v1",
        "summary": "While the language modeling objective has been shown to be deeply connected\nwith compression, it is surprising that modern LLMs are not employed in\npractical text compression systems. In this paper, we provide an in-depth\nanalysis of neural network and transformer-based compression techniques to\nanswer this question. We compare traditional text compression systems with\nneural network and LLM-based text compression methods. Although LLM-based\nsystems significantly outperform conventional compression methods, they are\nhighly impractical. Specifically, LLMZip, a recent text compression system\nusing Llama3-8B requires 9.5 days to compress just 10 MB of text, although with\nhuge improvements in compression ratios. To overcome this, we present FineZip -\na novel LLM-based text compression system that combines ideas of online\nmemorization and dynamic context to reduce the compression time immensely.\nFineZip can compress the above corpus in approximately 4 hours compared to 9.5\ndays, a 54 times improvement over LLMZip and comparable performance. FineZip\noutperforms traditional algorithmic compression methods with a large margin,\nimproving compression ratios by approximately 50\\%. With this work, we take the\nfirst step towards making lossless text compression with LLMs a reality. While\nFineZip presents a significant step in that direction, LLMs are still not a\nviable solution for large-scale text compression. We hope our work paves the\nway for future research and innovation to solve this problem.",
        "updated": "2024-09-25 17:58:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17141v1"
    },
    {
        "title": "Turn Every Application into an Agent: Towards Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents",
        "authors": "Junting LuZhiyang ZhangFangkai YangJue ZhangLu WangChao DuQingwei LinSaravan RajmohanDongmei ZhangQi Zhang",
        "links": "http://arxiv.org/abs/2409.17140v1",
        "entry_id": "http://arxiv.org/abs/2409.17140v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17140v1",
        "summary": "Multimodal large language models (MLLMs) have enabled LLM-based agents to\ndirectly interact with application user interfaces (UIs), enhancing agents'\nperformance in complex tasks. However, these agents often suffer from high\nlatency and low reliability due to the extensive sequential UI interactions. To\naddress this issue, we propose AXIS, a novel LLM-based agents framework\nprioritize actions through application programming interfaces (APIs) over UI\nactions. This framework also facilitates the creation and expansion of APIs\nthrough automated exploration of applications. Our experiments on Office Word\ndemonstrate that AXIS reduces task completion time by 65%-70% and cognitive\nworkload by 38%-53%, while maintaining accuracy of 97%-98% compare to humans.\nOur work contributes to a new human-agent-computer interaction (HACI) framework\nand a fresh UI design principle for application providers in the era of LLMs.\nIt also explores the possibility of turning every applications into agents,\npaving the way towards an agent-centric operating system (Agent OS).",
        "updated": "2024-09-25 17:58:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17140v1"
    },
    {
        "title": "Blox-Net: Generative Design-for-Robot-Assembly Using VLM Supervision, Physics Simulation, and a Robot with Reset",
        "authors": "Andrew GoldbergKavish KondapTianshuang QiuZehan MaLetian FuJustin KerrHuang HuangKaiyuan ChenKuan FangKen Goldberg",
        "links": "http://arxiv.org/abs/2409.17126v1",
        "entry_id": "http://arxiv.org/abs/2409.17126v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17126v1",
        "summary": "Generative AI systems have shown impressive capabilities in creating text,\ncode, and images. Inspired by the rich history of research in industrial\n''Design for Assembly'', we introduce a novel problem: Generative\nDesign-for-Robot-Assembly (GDfRA). The task is to generate an assembly based on\na natural language prompt (e.g., ''giraffe'') and an image of available\nphysical components, such as 3D-printed blocks. The output is an assembly, a\nspatial arrangement of these components, and instructions for a robot to build\nthis assembly. The output must 1) resemble the requested object and 2) be\nreliably assembled by a 6 DoF robot arm with a suction gripper. We then present\nBlox-Net, a GDfRA system that combines generative vision language models with\nwell-established methods in computer vision, simulation, perturbation analysis,\nmotion planning, and physical robot experimentation to solve a class of GDfRA\nproblems with minimal human supervision. Blox-Net achieved a Top-1 accuracy of\n63.5% in the ''recognizability'' of its designed assemblies (eg, resembling\ngiraffe as judged by a VLM). These designs, after automated perturbation\nredesign, were reliably assembled by a robot, achieving near-perfect success\nacross 10 consecutive assembly iterations with human intervention only during\nreset prior to assembly. Surprisingly, this entire design process from textual\nword (''giraffe'') to reliable physical assembly is performed with zero human\nintervention.",
        "updated": "2024-09-25 17:42:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17126v1"
    }
]