1
Learning with Dynamics: Autonomous Regulation
of UAV Based Communication Networks with
Dynamic UAV Crew
Ran Zhang, Bowei Li, Liyuan Zhang, Jiang (Linda) Xie, and Miao Wang
Abstract—UnmannedAerialVehicle(UAV)basedcommunica- time-sequential decisions in dynamic environments free of
tion networks (UCNs) are a key component in future mobile the environment models. RL can be either centralized or
networking. To handle the dynamic environments in UCNs,
distributed. Centralized RL features a single agent making
reinforcement learning (RL) has been a promising solution
decisions for all the UAVs based on the complete network
attributed to its strong capability of adaptive decision-making
free of the environment models. However, most existing RL- information [4]. While potentially yielding a better overall
basedresearchfocusoncontrolstrategydesignassumingafixed network performance, the computing power of the agent is
set of UAVs. Few works have investigated how UCNs should be significantly challenged. Constant communications between
adaptivelyregulatedwhentheservingUAVschangedynamically.
the agent and all the UAVs may incur unacceptable com-
ThisarticlediscussesRL-basedstrategydesignforadaptiveUCN
munication overhead and latency in large-scale UCNs. On
regulation given a dynamic UAV set, addressing both reactive
strategies in general UCNs and proactive strategies in solar- the contrast, distributed RL, i.e., multi-agent RL (MARL),
poweredUCNs.AnoverviewoftheUCNandtheRLframework distributes the training load across the UAVs [5]. Each UAV
isfirstprovided.Potentialresearchdirectionswithkeychallenges acts as an agent and trains its own policy based on the local
and possible solutions are then elaborated. Some of our recent
observationsandlimitedinformationsharingwithotherUAVs
works are presented as case studies to inspire innovative ways
for coordinated goals. Scalability can be achieved.
to handle dynamic UAV crew with different RL algorithms.
Nevertheless, existing RL-based studies on UCNs focus
IndexTerms—UnmannedAerialVehicles(UAV),dynamicUAV
mainly on control policy design given a fixed set of UAVs.
crew, reinforcement learning, UAV solar charging
Fewworkshavedabbledhowthenetworkshouldbeadaptively
regulated when the serving UAV crew dynamically change.
I. INTRODUCTION On one hand, the serving UAV crew may passively change at
Unmanned aerial vehicles (UAVs) have been demonstrating times: UAVs have to quit the network when they run out of
dazzlingpotentialsinnextgenerationwirelesscommunication batteries; supplementary UAVs can also join the serving crew
andnetworking.UAVsequippedwithwirelesstransceiverscan whenever needed. Fluctuations in the network performance
serve as mobile base stations (BSs) and interconnect to form will be inevitably incurred, thus calling for passively respon-
UAV based communication networks (UCNs). As reported sive regulation strategies when such changes happen. On the
in [1], the UAV market is anticipated to reach 166.7 billion other hand, the future UAV models are expected to be solar-
USD by 2031. Driven by the booming market, UCNs have powerrechargeable[6].Thisleavesthenetworkgreatinitiative
been extensively studied in various aspects such as emer- to proactively control UAVs’ quit and join-in rather than
gency rescue, network coverage enhancement and extension, performing a passive response strategy. When user demand
crowd/trafficsurveillance,cachedcontentdelivery,andmobile is low, certain UAVs can be scheduled for solar charging
edge computing [2]. even if they are not in bad need. They can be later called
Conventional approaches to UCN problems typically adopt back to replace other UAVs or meet increasing demands. In
alternativeoptimization,heuristicalgorithmsorstatisticalanal- this manner, a self-sustainable UCN can be established based
ysis[2].Thesemethodologiesarebetterfitswhenthenetwork on benignant charging-serving cycles, optimizing the target
parameters are fixed. In UCNs, parameters such as the net- network performance subject to the constraints of network
work size and topology, connectivity, channel conditions and sustainability and time-varying user demand.
service demands are usually dynamically changing, making To this end, we discuss in this article how, under the RL
such methods re-executed every time when the parameters framework, UCNs can responsively react to and proactively
are updated. In addition, for works that consider sequen- controlthedynamicchangeoftheservingUAVcrewineither
tial decision making in a slotted time horizon, computing acentralizedordistributedmanner.Specifically,thefollowing
complexity increases exponentially with the number of slots. two aspects are elaborated.
This significantly challenges the computing power of decision • Design of RL-Based Responsive Strategies to Dynamic
makers in cases of large network scale or long time horizon. UAV Crew in General UCNs. With such strategies, when
Recent advances in reinforcement learning (RL) [3] have any UAVs are about to quit or join, the strategy will
brought promising solutions to UCN problems. By actively directtheactiveUAVstotakeautonomousyetsynergetic
interacting with the environment and learning from the inter- actions to minimize (or maximize) the performance loss
action experiences, RL agents are strongly capable to make (or gain). As the UAVs serve as mobile BSs, the target
4202
peS
52
]YS.ssee[
1v93171.9042:viXra2
strategywillfeatureajointdesignofUAVradioresource clouds.Ontheotherhand,whenaUAVisatahighaltitude,its
management (RRM) and trajectory control. In addition, communication to the ground users will degrade significantly
although the strategy does not control the crew change, and higher interference may be incurred due to increased
it is expected to be capable of identifying the upcoming footprint.Therefore,suspendingcommunicationtotheground
change and actively regulating the UAVs ahead of the (i.e., temporally quitting the network) at high altitudes will
change rather than passively reacting after the change. considerably ease RRM of the UCN.
• Development of RL-Based Proactive Control Strategies
on UAV Crew in Solar-Powered Sustainable UCNs. Un-
like the above aspect where the network passively reacts
SOLAR PANEL
to the change of UAV crew, strategies can be developed
toproactivelycontrolthechangeinsolar-poweredUCNs.
BACKBONE
Leveraging the spatio-temporal variability of the user NETWORK
service demand, RL-based strategies will be designed
to optimize UAVs’ charging profiles. The optimization
aims to strike the balance between UAVs’ individual
benefits and overall network performance while fulfilling
thenetworksustainabilityandtime-varyinguserdemand.
In the following, an overview is first presented on models UAV GROUND USERS
UAV BATTERY BACKHAUL LINKS
of UCNs together with the RL framework. Potential research
problems, challenges, and approaches are then discussed,
Server/
followed by our recent related works as case studies. We Ground Overall
BACKBONE Station Policy π
expect the research outcomes to provide valuable inspirations NETWORK
Backhaul
and benchmarks to the autonomous management of UCNs Links CENTRALIZED
RL DOMAIN
under a dynamic network setup. UAV 1 UAV 2 Di.re c.t L.inks UAV N MARL
DOMAIN
II. OVERVIEWOFUCNSANDTHERLFRAMEWORK Policy π1 Policy π2 Policy πN
Agent 1 Agent 2 Agent N
The article will focus on developing regulation strategies
LOCAL LOCAL LOCAL
to responsively react to and proactively control the dynamic ENV. ENV. ENV.
NETWORK ENVIRONMENT
change in UAV crew under the RL framework. We study a
LOCAL ENVIRONMENT OBSERVATION
group of interconnected UAVs flying over a target region, ELO XC CA HL A U NA GV E C DO IN ND FI OTI RO MNS ATION FROM OTHER UAVs
providing communication services to ground users of various
types as shown in Fig.1. Each UAV concentrates its transmis- Fig. 1: Network model and the underlying RL framework for
sion power within a certain aperture angle, so its coverage UCNs.
mainly depends on the altitude and transmission power. A
Thetimingoftheentiresystemisslottedandsynchronized.
UAV can communicate with other UAVs within reach via di-
In every slot, each UAV updates its local observations in-
rectlinksorbackbonenetworks(e.g.,groundBSsorsatellites)
cluding local environment situations (e.g., number of served
indirectly. The UAV direct links and backhaul links employ
users, solar charging intensity), and local UAV conditions
disjoint spectrum from the UAV-ground links, thus avoiding
(e.g.,3Dcoordinatesandbatteryresiduals).IncentralizedRL,
mutual interference. The UAVs are battery-constrained. A
a centralized server or a ground control station will collect
servingUAVwillquitthenetworkwhenrunningoutofbattery,
local observations across UAVs, make RRM and movement
whileanewUAVmayjoinanytime.Accordingly,theexisting
decisions for each UAV, and return the decisions via backhaul
UAVswillbereconfigured(inRRMandpositions/trajectories)
links. In MARL, each UAV implements its own learning
to minimize (or maximize) the performance loss (or gain).
agentanditerativelyupdatesitspolicybasedonits(historical)
For solar-powered UAVs, they can elevate to high altitudes
local observations and optionally the shared information from
(1000m∼10000m) for high-rate solar charging when they can
other UAVs. Inter-UAV information sharing may improve the
be unoccupied from users and return later when needed. This
coordinated learning, yet adds communication overhead and
creates opportunities to optimally determine UAV charging
latency. The shared information can be integrated into the
profiles to enhance the network performance while meeting
local learning states, the reward function design, or the value
UCNsustainabilityanduserservicerequirements.Weconsider
function updating.
that when a UAV needs solar-charging, it quits the network
temporarily and elevates high above the clouds. On one
hand, the solar radiation is attenuated exponentially with the
III. RESPONSIVESTRATEGYDESIGNTODYNAMICUAV
thickness of clouds between the sun and solar panel [7]. The
CREWINGENERALUCNS
intensity falls to only ∼1/10 after the first 300m down from When UAV crew change, RL-based response strategies are
the upper cloud edge. Since it does not take long (e.g., one desired to identify the upcoming change and actively regulate
minute or two) for a UAV to move vertically up through 300 theRRMandmovementsoftheexistingUAVs.By“actively”,
meters, it is reasonable to charge the UAVs just above the we expect the strategy to take actions in advance when the3
change is about to happen rather than react passively after early simulations showed that such methods failed to prompt
the change. The goal is to minimize (maximize) the perfor- “active” reconfiguration ahead of the change or reactive adap-
mance loss (gain) during transition incurred by the change. tationevenafterthechange.Innovativemethodsaredesiredto
The actions need to include both RRM decisions (e.g., user fully explore the state-action space around the time of change
association, spectrum allocation, power control, etc.) and tra- todetermineifan“active”reconfigurationaheadofthechange
jectory design for the best regulation performance. It is much is beneficial.
more challenging to jointly optimize multi-UAV RRM and Tothisend,analternativetosimplyincreasingtherandom-
trajectory control under the RL framework given a dynamic ness is to reduce the correlation among the collected experi-
UAV crew. We identify the following research directions with encesusedfortrainingtheagentneuralnetworks,whichcanbe
key challenges, open issues and potential approaches. crucial in early training stages. Experiences from one episode
are more or less correlated, which may degrade the neural
A. Design of Key Elements in RL network training in early stages and eventually affect the final
trainingperformance.Toaddressthis,anasynchronousparallel
DynamicchangesinUAVcrewimposehigherrequirements
computing (APC) structure can be exploited [8], inspired by
ondesigningthestatespace,actionspaceandrewardfunction
the asynchronous advantage actor critic (A3C) algorithm [9].
of the learning agent. The state space needs to be elastic
As shown in Fig.2, in APC, one agent is composed of a host
in dimension to accommodate the change, meanwhile being
client and multiple parallel workers, each interacting with an
information-inclusive to help identify the upcoming change.
independent environment copy. Such copies are exactly the
In addition, when RRM is configurable, RRM decisions be-
same but the randomness. The host client maintains unified
come part of the actions. However, the action space of user
critic and actor networks and updates both networks using
association and bandwidth allocation is discrete, while that of
the collected experiences from all workers. Each parallel
powercontrolandUAVpositioningiscontinuous.Noneofthe
worker executes the up-to-date policies in its own copy of
classicalRLalgorithmscanhandlesuchamixedactionspace.
environment. Unlike A3C, these workers share the same set
For state space, in addition to the network performance
of policy parameters from the host client and upload their
metrics and UAV conditions, the “in/away” status of all the
own experiences to the same replay buffer for unified policy
potentiallyinvolvedUAVsshouldalsobeincorporatedtohelp
updating. The motivation is that the independent random-
identify whether a change in the instantaneous reward is due
nesses in different environment copies make the generated
to the change of crew or UAV positions. For the “in/away”
experiencesmutuallyindependent.Thiseffectivelyreducesthe
status, continuous variables such as battery residual and the
correlation among the sampled experiences for updating the
join-in countdown timer are preferred over binary indicators
neural networks, thus improving the learning performance.
so that agent(s) can foresee the upcoming change and make
proper regulations ahead of the change. Moreover, agent(s)
should maintain a “maximum” state space to accommodate
the maximum possible number of UAVs, but only activate the
dimensionsoftheUAVswhosestatusare“in”ortobechanged
during the learning.
To handle the mixed action space, the advantages of actor-
critic RL (AC-RL) and deep Q-learning (DQL) can be po-
tentially combined. AC-RL excels in continuous action space,
e.g.powercontrol andpositioning,whileDQL iseffectivefor
discrete action space, e.g., UAV RRM. One approach is to
Fig. 2: Asynchronous parallel computing (APC) diagram.
separate the policy into discrete and continuous components.
During the learning, two neural networks can be trained: a
critic network for the Q-value function and an actor network
C. Algorithm Design with Enhanced Robustness
forthecontinuouspolicy.Ineachiteration,thediscretepolicy
Another challenge lies in the algorithm robustness against
will be first derived from the updated critic network, and then
the uncertain number of active UAVs. As UAVs’ quit’s and
together with the Q-values be fed into the actor network to
join-in’s are hardly predictable in the design and training
determine the continuous policy. We expect this two-stage
stage, the algorithm needs to handle random quit and join-
method would address the challenge of hybrid action space.
in given arbitrary number of active UAVs below a maximum.
A conceptually simple idea is to train a series of agents to
B. Algorithm Design with Promoted Exploration
handle quit and join-in separately for different numbers of
During training, sufficient exploration in state-action space active UAVs. But that would lead to a series of trainings with
aroundthetimingofUAVcrewchangesiscrucialtooptimize prohibitively high training load that increases superlinearly
the network response. A straightforward way is to increase with the maximum number of UAVs. Novel training methods
random exploration around the time of change. This can be thatimposemuchlesstrainingloadyetcanstillaccommodate
achieved by increasing the probability of selecting a random arbitrary changes in UAV crew are much needed.
actionin(deep)Q-learning,ormagnifyingtherandomnoiseto The most challenging point to achieve this robustness is
theoutputactionoftheactornetworkinAC-RL.However,our minimizing the training load. Obtaining a robust algorithm4
with one single training is technically feasible, if sufficient ineachstep,subjecttotheoutputofthefirstsub-problemand
experiences concerning every possible change are collected the network sustainability. By conducting one DRL training
andusedtotraintheagent(s).Thisrequiresdeliberateepisode for each time step (the first sub-problem) and a single DRL
design to be inclusive yet concise enough while assuring fair trainingovertheentiretimehorizon(thesecondsub-problem),
observation among all possible changes. Instead of homo- the original problem can be tractable.
geneous episode design, heterogeneous design may be used,
where episodes with different situations of change can take B. Fusion of Game Theory in MARL Framework with Hybrid
place in turns. This simplifies the design of each episode and Cooperate-Compete Relationship
thus the exploration complexity.
When MARL is leveraged, existing studies on the UCNs
usually assume full cooperation among UAVs neglecting
IV. PROACTIVEUAVCONTROLSTRATEGYIN
UAVs’ own benefits. Yet in many occasions, UAVs may
SOLAR-POWEREDSELF-SUSTAINABLEUCNS
have a hybrid relationship, i.e., cooperate and compete at
Whenasolar-poweredUAVquitsforcharging,thequitmay the same time. Each UAV may want to maximize its own
lead to failure in satisfying user service demand. But if the battery residual at the end while working together to satisfy
UAV is not charged in time, the UCN cannot be sustained. the network constraints. Under such a hybrid relationship,
Thekeyoutofthisdilemmaliesinthetime-variabilityofuser simplyfollowingtheexistingdesignasdescribedinIII-Amay
servicedemands.Whenusersarespatiallyconcentratedoruser leadtounsatisfyingdistributedpolicies[10].Novelapproaches
servicedemandsarelow,fewerUAVsareneeded.SomeUAVs must be identified or developed to model the cooperative-
can be opportunistically scheduled for solar charging even if competitive interaction among UAVs and guide the value
they are not in immediate need. They can be later called back function updates for each UAV during training.
to meet the increased demand or replace other UAVs. Such To handle such a hybrid relationship, the learning rewards
proactive control on the serving UAV crew can balance the of each UAV need to be revised to include at least two
user service satisfaction and UCN self-sustainability, making parts: one linked to the overall network objectives, and the
UCNs free of charging facility with high energy utilization. other being offset from each other due to the contention for
This motivates the design of UAV solar-charging strategies, charging and idling opportunities. As the contention among
aimingtooptimizethetargetnetworkperformanceoveratime UAVsnaturallyformsagame,itcanbepromisingtointegrate
horizon, subject to the constraints of user service demand and game theory into MARL framework to guide the learning
network sustainability. evolution. The result will be a joint coordinated policy that
leadtoanequilibrium.TwogametheorybasedRLtechniques,
i.e., Nash Q-learning [11] and correlated Q-learning [12],
A. Algorithm Design and Problem Decomposition
can be potentially exploited to achieve Nash and Correlated
AstandarddesignoftheRLalgorithmsforproactivecontrol
equilibra, respectively. Nash Q-learning requires each UAV
will involve a state space including at least the UAV battery
agent to maintain a Q-function for its local state space and a
residual, the serving status (being idle, serving or charged),
joint action space for all UAVs. In each iteration, each agent
user service fulfillment conditions, UAV 3D positions, and
selects its own action independently, but needs to calculate a
time(tohelpcaptureenvironmentdynamics).Theactionspace
joint Nash equilibrium via quadratic programming and update
willatleastconsistofthemovingdirection(s)anddistance(s),
its Q-function based on the Nash Q-function. In correlated Q-
and instructions to UAVs on whether to go charging, serving,
learning, each agent maintains a Q-function for the joint state
or idle. The hard constraints will be the UAV sustainability
spaceandthejointactionspaceofallUAVs.Ineachiteration,
and user service requirements. In addition, the learning needs
different Nash Q-learning, each agent will agree on a joint
to consider the environmental dynamics such as dynamic
actionselectionbysolvingalinearprogrammingtoachievethe
solarradiationintensityandtime-varyinguserdistributionand
correlatedequilibrium.ComparedtothecorrelatedQ-learning,
service demand at different times of the day. However, such
Nash Q-learning requires each agent to maintain a smaller Q
a design will result in a high-dimension state space and a
memory but with more computation complexity to calculate
multi-time-scale action space (i.e., movements related actions
theequilibrium.Thetwotechniquescanbeevaluatedinterms
at a smaller time scale and charging decisions at a larger time
of convergence speed, memory and computation complexity,
scale), making the algorithm prohibitively complex.
battery residuals, and target network performance.
To tackle this issue, one potential way is to decompose the
original learning problem into two sub-problems. In the first
V. CASESTUDIES
sub-problem,giventheuserservicedemandineachtimestep,
Asillustrativeexamples,inthissection,weintroducehowto
the least number of UAVs and their optimal 3D positions to
applydifferentRLalgorithmstoregulateUCNswithdynamic
meet the demand will be determined via Deep RL (DRL).
UAV set.
The outputs will be step-specific and combined to form the
time-varying constraints for the second sub-problem. In the
A. Responsive Regulation with Centralized DRL in General
second sub-problem, given the initial UAV battery residuals
UCNs
andtime-varyingsolarradiationintensity,DRLalgorithmscan
be designed to decide the optimal charging profiles for each As shown in Fig. 1, we consider that a group of UAVs
UAV, i.e., whether a UAV should go idle, serving or charging flying over a target region to provide communication services5
to the ground users. A user is served only when its minimum statespaceincludesitsownposition,theaway/activestatusof
throughput is satisfied. We target at an optimal UAV control all the UAVs that may be involved, and time. The individual
policy to maximize the total number of served users over action space only considers its own movements. The reward
time. The policy responsively relocates the UAVs when i) a awards the total number of served users, and punishes the
UAV quits or joins the network, or ii) the underlying user coverage overlapping between this UAV and the others. Such
distribution changes [8]. penalty makes the UAVs more dispersed, thus potentially
A centralized DRL approach is designed using deep deter- reducing competition and serving more users.
ministic policy gradient (DDPG) algorithm to accommodate A distributed strategy is yielded which can handle arbitrary
thecontinuousstateandactionspace.Inthestatespace,allthe UAV quit’s and join-in’s by taking the following innovative
UAV positions are included. To identify the upcoming change measures. In each episode, UAVs randomly and sequentially
in the UAV set, UAV battery residuals (case of quit) and join- quit from the maximum possible number to only 1. Two
in countdown timer (case of join-in) are included. The time is environment copies with the same setting except the ran-
alsoincludedtocapturethedynamicsofuserdistribution.The domness are considered. When a UAV quits in one copy, it
action space includes UAVs’ flying directions and distances. automatically enters the other copy where its agent continues
Therewardawardsthetotalnumberofservedusersandpunish training with any existing UAVs for the same optimization
UAVs for going out of bound. To handle the change in the goals. The UAV sets in the two copies are complementary
UAV set, fixed-dimension raw state space and action space and their trainings take place simultaneously. The collected
are maintained. The raw space corresponds to a maximum experiences of the same UAV from two copies are combined
possible number of active UAVs that may be involved, but in a single buffer to update its Q-network. This ensures
onlythestatesandactionsoftheactiveUAVs(i.e.,theserving statistically fair traverse of all possible changes and thus the
and joining UAVs) will be updated and contribute to the robustness against the arbitrariness. Fig. 4 shows bird views
reward. Moreover, to promote the learning exploration, the of UAV trajectories in one example of random changes in
APC structure in Subsection III-B is exploited. UAVset.ThehollowandsoliddotsrepresenttheoptimalUAV
With the above design, the obtained strategy can identify positions before and after a change happens, respectively. It
any upcoming change in the UAV set and captures the can be seen that each time when a random change happens,
dynamics of the underlying user distribution. It adaptively the obtained strategy is able to relocate the active UAVs to
relocates the active UAVs when a change happens or even maximize the user coverage.
ahead of the change if necessary. Fig. 3 presents bird views
of UAV trajectories obtained by the strategy. The users are
C. Proactive UAV Control in Solar-Powered UCNs
mostly distributed in 4 clusters with centers moving towards
the region center. When a UAV quits, the remaining UAVs With solar-chargeable UAVs, solar charging strategies can
mostly(UAVs2,4,5)movetowardsthecenteralongwiththe be designed to optimize when each UAV goes serving, charg-
clusters, but adjust their trajectories towards the quitting UAV ing or idle (i.e., to the ground for energy saving). The change
(UAV 1) to cover the service hole as much as possible. When of the serving UAV set can then be proactively controlled for
a UAV joins in, the existing UAVs (UAVs 1, 3, 4, 5) initially betternetworkmanagementascomparedtoreactingpassively
move towards the center, but may turn around (i.e., UAVs 1 to uncontrollable changes in general UCNs (i.e., Subsections
and 3) when UAV 2 is integrated. V-A and V-B). To this end, our work [14] studies UAV
solar charging profile design, aiming to balance between
maximizing the total number of served users over time and
maximizing the total UAV residual energy at the end. In each
UAV Solid: Trajectory step, the design needs to meet the minimum overall user
Trajectories after joining in service percentage and the UCN sustainability requirements,
Traces of
Cluster i.e.,everyUAVneedstohavesufficientenergytogocharging
Centers
Dash: Trajectory attheendofeachstep.Thedesignalsoconsiderstime-varying
before joining in
solar radiation and user service demand at a day scale (Fig.
5), which makes the problem more realistic but challenging.
Case of UAV quit: Start with 5 UAVs, UAV Case of UAV join-in: Start with 4 UAVs, UAV We decouple the original problem into two sub-problems.
1 quits in the middle of the time horizon 2 joins in the middle of the time horizon
Sub-problem 1 calculates mappings between the number of
Fig. 3: UAV trajectories with dynamic user distribution in cases of serving UAVs and the maximum served users given hourly
UAV quit and join-in, respectively. user service demands. It can be resolved by reusing the al-
gorithms designed in Subsection V-A. Sub-problem 2 designs
UAV charging profiles based on the obtained mappings, solar
B. Distributed Regulation with MARL in General UCNs
radiation dynamics, UAV sustainability, and the minimum
This subsection considers the same problem and network overalluserservicepercentage.AcentralizedDDPGalgorithm
setup as Subsection V-A, but provides a multi-agent DQL isdesignedtosolvetheproblem,witharelaxationmechanism
basedapproachtoobtainadistributedregulationstrategy[13]. to handle the large discrete action space. Simulation results
Ineachstep,eachUAVsharesitsposition,away/activestatus, are shown in Fig. 6. Whole sets of 15 and 17 UAVs are
andthenumberofitsservedusersineachstep.Theindividual simulated, respectively, with different coeff values. A smaller6
Start with 3 UAVs 1 UAV joins in 1 UAV joins in 1 UAV quits 1 UAV quits
Fig. 4: Optimal coverage of active UAVs when UAVs randomly quit and join in sequentially.
strategies in solar-powered UCNs. Illustrative examples have
demonstrated that deep RL algorithms can well handle arbi-
trary changes in UAV set reactively and control the change
of the serving UAV set proactively via UAV solar-charging
profile design. However, there are still many open issues to
be addressed and opportunities to be explored, some of which
are highlighted below.
The number of users Min. number of
in each hour serving UAVs
needed A. Open Issues
Robustness against unknown user distribution. The ex-
isting works are mostly based on known or predicted user
distributions.Robustregulationstrategiesaredesiredtohandle
Fig. 5: Dynamics of solar radiation and user service demand in a random user distributions for unexplored environment. A po-
day. tential solution is to integrate Convolutional Neural Networks
(CNNs) into the RL framework, and analyze the live user
distribution map as part of the state space.
coeff favorsservingmoreusersoversavingmoreUAVenergy.
Hybrid relationship among UAVs. When designing the
ThebaselineistheminimumnumberofUAVsneededineach
UAV solar-charging profiles, a practical consideration is that
hour to meet the minimum overall user service percentage.
UAVsmaycontendagainsteachother(e.g.,forsolarcharging
With smaller coeff, more UAVs tend to be scheduled to serve,
opportunities) while cooperating to meet overall objectives.
with less chance of being charged. This leads to more served
ThishybridrelationshipmaymaketheexistingRLalgorithms
users over time but less residual energy at the end.
perform unsatisfactorily. Game theories may be introduced to
guide the learning to a better convergence by coordinating
UAVs’ action decisions in each step for equilibrium.
The number of
served UAVs in
B. Opportunities of Broader Scope
each hour
Integration of generative AI (GAI). GAI has achieved
great success in content creation. Its remarkable capability
Hours of a day of complex task processing can be exploited in regulating
Accumulated number of served users UCNswithdynamicUAVset.Forinstance,thelargelanguage
under (Total UAV number, coeff)
models (LLMs) can be utilized to make accurate time-series
prediction of the environment and UAV dynamics. Generative
Adversarial Networks (GANs) can be adopted to produce
synthetic experiences that resemble the realistic ones. This
may achieve learning expedition and better generalization to
Fig. 6: Performance demonstration under different parameters.
different change cases of the UAV set.
Wireless charging of UAVs. In addition to solar charging,
wireless UAV charging is another innovative technology for
VI. CONCLUSIONSANDFUTUREOUTLOOK
which proactive control of the UAV set can be investigated.
Inthisarticle,wehavediscussedhowtoadaptivelyregulate There are several forms of wireless charged UCNs. Ground
UCNsgivenadynamicsetofUAVsundertheRLframework. chargingdocksortowerscanbedeployedsuchthatUAVsmay
System overview, research directions, design challenges, po- quit the network to get charged while being landed (docks)
tential approaches and case studies have been provided from or on the fly (towers). Airships can also be exploited like
aspects of reactive strategies in general UCNs and proactive tankerplanessothatUAVscangetchargedovertheairwithout7
leavingthenetwork.Laserbeamingisanotheroptiontocharge Miao Wang (mwang25@charlotte.edu) [SM’21] is an Assistant
over the air but may be limited by the locations. Professor with Department of Electrical and Computer Engineering,
University of North Carolina at Charlotte, USA.
REFERENCES
[1] “Global drone market,” Available at https://www.skyquestt.com/
report/drone-market#:∼:text=Drone%20Market%20Insights,period%
20(2024%2D2031).,June2024.
[2] M. Mozaffari, W. Saad, M. Bennis, Y.-H. Nam, and M. Debbah, “A
tutorial on UAVs for wireless networks: Applications, challenges, and
open problems,” IEEE Communications Surveys & Tutorials, vol. 21,
no.3,pp.2334–2360,2019.
[3] Y.Bai,H.Zhao,X.Zhang,Z.Chang,R.Ja¨ntti,andK.Yang,“Towards
autonomous multi-UAV wireless network: A survey of reinforcement
learning-basedapproaches,”IEEECommunicationsSurveys&Tutorials,
2023.
[4] P.Luong,F.Gagnon,L.-N.Tran,andF.Labeau,“Deepreinforcement
learning-basedresourceallocationincooperativeUAV-assistedwireless
networks,” IEEE Transactions on Wireless Communications, vol. 20,
no.11,pp.7610–7625,2021.
[5] C. Dai, K. Zhu, and E. Hossain, “Multi-agent deep reinforcement
learningforjointdecoupleduserassociationandtrajectorydesigninfull-
duplexmulti-UAVnetworks,”IEEETransactionsonMobileComputing,
vol.22,no.10,pp.6056–6070,2022.
[6] S. Morton, R. D’Sa, and N. Papanikolopoulos, “Solar powered UAV:
Design and experiments,” in 2015 IEEE/RSJ international conference
onintelligentrobotsandsystems(IROS). IEEE,2015,pp.2460–2466.
[7] A.Kokhanovsky,“Opticalpropertiesofterrestrialclouds,”Earth-Science
Reviews,vol.64,no.3-4,pp.189–241,2004.
[8] R.Zhang,M.Wang,L.X.Cai,andX.Shen,“Learningtobeproactive:
Self-regulation of uav based networks with UAV and user dynamics,”
IEEE Transactions on Wireless Communications, vol. 20, no. 7, pp.
4406–4419,2021.
[9] P. Yu, Y. Ding, Z. Li, J. Tian, J. Zhang, Y. Liu, W. Li, and X. Qiu,
“Energy-efficient coverage and capacity enhancement with intelligent
UAV-BSs deployment in 6G edge networks,” IEEE Transactions on
IntelligentTransportationSystems,vol.24,no.7,pp.7664–7675,2022.
[10] H.X.Pham,H.M.La,D.Feil-Seifer,andA.Nefian,“Cooperativeand
distributed reinforcement learning of drones for field coverage,” arXiv
preprintarXiv:1803.07250,2018.
[11] P. Casgrain, B. Ning, and S. Jaimungal, “Deep Q-learning for nash
equilibria: Nash-DQN,” Applied MathematicalFinance, vol. 29,no. 1,
pp.62–78,2022.
[12] K.C.Tsai andZ.Han,“Achievingcorrelated equilibriumbystudying
opponent’sbehaviorthroughpolicy-baseddeepreinforcementlearning,”
IEEEAccess,vol.8,pp.199682–199695,2020.
[13] B.Li,S.Tripathi,S.Hosain,R.Zhang,J.L.Xie,andM.Wang,“When
learningmeetsdynamics:Distributeduserconnectivitymaximizationin
UAV-basedcommunicationnetworks,”arXivpreprintarXiv:2409.06010,
2024.
[14] L. Wang, S. Tripathi, R. Zhang, N. Cheng, and M. Wang, “Optimal
charging profile design for solar-powered sustainable UAV commu-
nication networks,” in ICC 2023-IEEE International Conference on
Communications. IEEE,2023,pp.4658–4663.
ACKNOWLEDGEMENTS
This work is supported by Natural Science Foundation
under Award 2412393.
BIOGRAPHY
Ran Zhang(rzhang8@charlott.edu)[SM’22]isanAssistantPro-
fessor with Department of Electrical and Computer Engineering,
University of North Carolina at Charlotte, USA.
BoweiLi(boweili@andrew.cmu.edu)isaMasterstudentwithDe-
partment of Electrical and Computer Engineering, Carnegie Mellon
University, USA.
Liyuan Zhang (lzhang51@charlotte.edu) is a PhD student with
Department of Electrical and Computer Engineering, University of
North Carolina at Charlotte, USA.
Jiang(Linda)Xie(jxie1@charlotte.edu)[F’20]isaProfessorwith
Department of Electrical and Computer Engineering, University of
North Carolina at Charlotte, USA.