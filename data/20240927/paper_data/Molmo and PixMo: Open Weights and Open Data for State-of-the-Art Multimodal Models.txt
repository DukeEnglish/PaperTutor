Molmo and PixMo:
Open Weights and Open Data
for State-of-the-Art Multimodal Models
MattDeitke∗†ψ ChristopherClark∗† SanghoLee† RohunTripathi† YueYang†
JaeSungParkψ MohammadrezaSalehiψ NiklasMuennighoff† KyleLo† LucaSoldaini†
JiasenLu† TairaAnderson† ErinBransom† KianaEhsani† HuongNgo†
YenSungChen† AjayPatel† MarkYatskar† ChrisCallison-Burch† AndrewHead†
RoseHendrix† FavyenBastani† EliVanderBilt† NathanLambert† YvonneChou†
ArnaviChheda† JennaSparks† SamSkjonsberg† MichaelSchmitz† AaronSarnat†
ByronBischoff† PeteWalsh† ChrisNewell† PiperWolters† TanmayGupta† Kuo-HaoZeng†
JonBorchardt† DirkGroeneveld† JenDumas† CrystalNam† SophieLebrecht†
CaitlinWittlif† CarissaSchoenick† OscarMichel† RanjayKrishna†ψ LucaWeihs†
NoahA.Smith†ψ HannanehHajishirzi†ψ RossGirshick†ψ AliFarhadi†ψ AniruddhaKembhavi†ψ
†AllenInstituteforAI ψUniversityofWashington
Abstract 1.Introduction
Today’s most advanced multimodal models remain pro- Extensions to large language models (LLMs) that process
prietary. The strongest open-weight models rely heavily imagesinadditiontotexthaveresultedinimpressivemul-
on synthetic data from proprietary VLMs to achieve good timodalcapabilities,suchasgeneratingcomprehensiveim-
performance, effectivelydistillingtheseclosedmodelsinto age descriptions and accurately answering complex visual
openones. Asaresult,thecommunityisstillmissingfoun- questions. The most performant of these vision-language
dational knowledge about how to build performant VLMs models (VLMs), however, remain proprietary with neither
fromscratch.WepresentMolmo,anewfamilyofVLMsthat modelweights,data,norcodebeingpubliclyreleased.
arestate-of-the-artintheirclassofopenness.Ourkeyinno- Withthegoaloffosteringscientificexploration,numer-
vationisanovel,highlydetailedimagecaptiondatasetcol- ous research efforts have attempted to reproduce similar
lected entirely from human annotators using speech-based capabilities in open models. Early works, exemplified by
descriptions. To enable a wide array of user interactions, LLaVA[15],producedfullyopenweightsandtrainingdata
wealsointroduceadiversedatasetmixtureforfine-tuning but now lag significantly behind the state-of-the-art. More
that includes in-the-wildQ&A and innovative 2D pointing recent, stronger open-weight models have trended towards
data. Thesuccessofourapproachreliesoncarefulchoices less open data: the training data may either be proprietary
for the model architecture details, a well-tuned training (e.g.,[5])or, incaseswhereitisreleased, thereisaheavy
pipeline, and, most critically, the quality of our newly col- relianceonsyntheticdatageneratedbyproprietarysystems,
lected datasets, all of which will be released. The best-in- e.g., models are trained on datasets like ShareGPT4V [7]
class72BmodelwithintheMolmofamilynotonlyoutper- whichusesGPT-4V[25]togeneratealargesetofdetailed
forms others in the class of open weight and data models image captions. The resulting VLMs, therefore, are effec-
but also compares favorably against proprietary systems tively distillations of proprietary VLMs, and the scientific
likeGPT-4o,Claude3.5,andGemini1.5onbothacademic community is still missing foundational knowledge about
benchmarksandhumanevaluation. howtobuildperformantVLMsfromscratch.
Wewillbereleasingallofourmodelweights,captioning
Inthiswork,wepresenttheMolmo(MultimodalOpen
andfine-tuningdata,andsourcecodeinthenearfuture.Se-
Language Model) family of state-of-the-art open VLMs
lectmodelweights,inferencecode,anddemoareavailable
with released model weights and released vision-language
athttps://molmo.allenai.org.
training data without any reliance on synthetic data from
other VLMs, including proprietary ones. This result is
∗Equalcontribution achieved with a simple training pipeline in which we con-
4202
peS
52
]VC.sc[
1v64171.9042:viXranect an independently pre-trained, off-the-shelf vision en-
coder and language model and jointly train the resulting
Molmo
VLM to generate captions from a newly collected dataset
of detailed, high-quality, dense image descriptions. After
joint training, we follow standard practice and use super- “Point to Mt Rainier” “Mt Rainier”
visedfine-tuningtoproduceaninstructionfollowingmodel.
<point x="63.5" y="44.5”alt="Mt
Unlike other contemporary open VLMs, we avoid multi-
Rainier">Mt Rainier</point>
plepre-trainingstagesthatinvolvefreezingvariouspartsof
themodelandrelyonlarge-scaleweaklypairedimage-text
data, oftenthreeordersofmagnitudelargerthanourhigh-
qualitydata(e.g., [4,5]). Thesuccessofourapproachre- Large Language Model
liesoncarefulchoicesforthemodelarchitecturedetails, a
well-tunedtrainingpipeline,andmostcritically,thequality
ofournewdatasets, collectivelynamedPixMo(Pixelsfor
Molmo),allofwhichwillbereleased.
Connector Connector Tokenizer
In practice, it is challenging to collect dense captioning
datasets from human annotators. If asked to write an im- CLIP CLIP Point
agedescription,theresultoftenonlymentionsafewsalient to Mt
Rainier
visualelements[8]. Ifaminimumwordcountisenforced,
annotatorswilleithertaketoolongtotype,makingcollec-
tion uneconomical, or copy-and-paste responses from pro-
prietaryVLMs,circumventingourgoalofavoidingdistilla- Figure1.TheMolmoarchitecturefollowsthesimpleandstandard
tion. As a result, the open research community has strug- designofcombiningalanguagemodelwithavisionencoder. Its
gled to create such datasets without relying on synthetic strongperformanceistheresultofawell-tunedtrainingpipeline
datafromproprietaryVLMs. Ourkeyinnovationisasim- andournewPixModata.
ple but effective data collection strategy that avoids these
us to rank models by user preference. Our most efficient
problems: we ask annotators to describe images in speech
model, MolmoE-1B, based on the OLMoE-1B-7B [24]
for 60 to 90 seconds rather than asking them to write de-
mixture-of-experts LLM, nearly matches the performance
scriptions.Weprompttheannotatorstodescribeeverything
of GPT-4V on both academic benchmarks and user
theyseeingreatdetail,includingdescriptionsofspatialpo-
preference. Molmo-7B-O and Molmo-7B-D, based on
sitioningandrelationships. Empirically,wefoundthatwith
OLMo-7B [10] and Qwen2 7B [33], respectively, perform
thismodalityswitching“trick”annotatorsprovidefarmore
comfortably between GPT-4V and GPT-4o on both aca-
detaileddescriptionsinlesstime,andforeachdescription,
demic benchmarks and user preference. Our best-in-class
wecollectanaudioreceipt(i.e., theannotator’srecording)
Molmo-72B model, based on Qwen2 72B, achieves the
provingthataVLMwasnotused.
highestacademicbenchmarkscoreandrankssecondbyhu-
Aftertrainingourmodelstogeneratedensecaptionswe
man preference, just behind GPT-4o. Our best model out-
fine-tune them on a broad range of use cases with super-
performsmanystate-of-the-artproprietarysystems,includ-
vised training data. This data mixture consists of stan-
ingGemini1.5ProandFlash,andClaude3.5Sonnet.
dard academic datasets as well as several newly collected
datasets,includingahighlydiversesetofquestionscaptur-
2.Architecture
ing what users in the wild might ask a model, document-
focused question and answer data, analog clock reading Our model architecture (Figure 1) follows the simple and
data,andauniquenewdatasourcethatgroundslanguagein standard design of combininga language model with a vi-
imageswith2Dpoints.Thisnovelpointingdataenablesour sion encoder (e.g., [15]). It consists of four components:
modelstoanswersomequestionsmorenaturallybypoint- (1)apre-processorthatconvertstheinputimageintoaset
ingtothepixelsthatsupporttheanswer,improvescounting of multiscale, multi-crop images, (2) a ViT image encoder
accuracy(themodelcountsbypointing),andwebelieveit thatindependentlymapseachoftheseimagesintoasetof
willopenupanimportantfuturedirectioninwhichVLMs visiontokens,(3)aconnectorthatprojectsthevisiontokens
enable agents (e.g., robots, web agents) to act by pointing tothelanguagemodel’sinputdimensionwithanMLPand
intheirenvironments, e.g., toanavigationwaypoint, toan thenpoolsthevisiontokenstoreducetheircount,and(4)a
objecttopickup,ortoauserinterfacebuttontopress. decoder-onlyTransformerLLM[26,30].
We evaluate the Molmo family of models on 11 aca- Fromthistemplate,weconstructafamilyofmodelsthat
demicbenchmarksandwithahumanevaluationthatallows isparameterizedbythechoiceofvisionencoderandLLM.Giventhesechoices,thesubsequenttrainingdataandrecipe OurtrainingprocessusesallfouroftheseimageLLM-
are the same for all models (aside from optimizer learning processedtranscripts,whenavailable,asaformofnatural-
rates). For the vision encoder, all of our released mod- isticdataaugmentation.Intotal,wetrainedon712kdistinct
elsuseOpenAI’sViT-L/14336pxCLIPmodel[27],which imageswith∼1.3Mcaptions(includingtheaugmentation).
provides consistently good results (while this model uses
Stage 2: Supervised fine-tuning. After training for cap-
closeddata,itcanbereproducedfromscratchasshownby
tioning, we fine-tune all model parameters on a mixture
MetaCLIP [32]; we use the model from OpenAI because
of supervised training data. This mixture includes com-
itwastrainedforhigherresolutionimages). FortheLLM,
mon academic datasets and several new PixMo datasets,
weofferavarietyofchoicesatdifferentscalesanddegrees
describednext.
of openness: fully open OLMo-7B-1024 (a pre-released
• PixMo-AskModelAnything: We collected this data
October, 2024 backbone, which will be released at a later
withthegoalofenablingthemodeltoansweradiverse
date),fullyopenOLMoE-1B-7B(ourmostefficientmodel),
setof questionscoveringwhat usersmightask itwhen
open-weightQwen27B,andopen-weightQwen272B(our
deployedinthewild. Tocreateimage-question-answer
best-performingmodel).
triplets, we had annotators work with a language-only
LLM.First,anannotatorwouldselectanimagefroma
3.DataandTraining
large pool and then write a question about it. We used
Starting from an independently pre-trained vision encoder ourstage1modeltogenerateadensecaptionfortheim-
and LLM, our training processing is simple and consists ageandpassedthatcaption, OCRoutputfortheimage
ofonlytwostages: (1)multimodalpre-trainingforcaption (from a non-VLM, off-the-shelf OCR model), and the
generation using PixMo-Cap, our newly collected caption question to a language-only LLM. The LLM provided
dataand(2)supervisedfine-tuningusingamixtureofaca- an answer (emphasizing again that it had no access to
demicdatasetsandournewlycollectedsupervisedPixMo-⋆ the image), which the annotator could either accept or
family of datasets. All model parameters are updated in reject. Ifrejected,theywoulddescribewhatwaswrong
bothstages. WedonotuseRLHF. withtheanswerandasktheLLMtofixit. Theannota-
toriteratedthisprocessuntiltheanswerwasacceptable.
Stage 1: Caption generation. In this stage, we join the
Forsomeofthedata, weaskedannotatorstoaskques-
visionencoderandLLMwithourrandomlyinitializedcon-
tionsfollowingaspecificprompt,includingunusualre-
nectorandtrainallmodelparametersonthetaskofcaption
questssuchasaskingfortheanswertobewrittenupside
generation. WecollectedthePixMo-Captrainingdatafor
down(whichispossiblewithUnicode).Thisdatasethas
thisstageasfollows.
162kquestion-answerpairsand73kimages.
We started by sourcing web images according to a di-
• PixMo-Points:Wecollectedpointingdatathatachieves
versesetof∼70high-leveltopics(e.g.,streetsigns,memes,
three goals: (1) enables the model to point to anything
food,drawings,websites,blurryphotos,etc.),andforeach
described by text, (2) enables the model to count by
image we asked three annotators to describe the image in
pointing, and (3) enables the model to use pointing as
detailbyspeakingforatleast60seconds(inlaterstagesof
a natural form of visual explanation when answering
collectionweincreasedthisto90secondsandusedasingle
questions. To collect data for the first two goals, we
annotatorperimage;wefoundthiswasmoreefficientwith-
askedhumanannotatorstopointatsomethinginanim-
outalossinquality). Theannotatorswerepromptedwitha
age, write a description of it, and then point to every
listofsimplequestionstoanswerintheirdescriptions:
instanceofitintheimage(makingthepointingexhuas-
• Whatistheimageatfirstglance?
tive). We also collected “not present” data so models
• Whataretheobjectsandtheircounts?
can learn to respond appropriately when asked about
• Whatdoesthetextsay? somethingnotintheimage. Thisdataalsonaturallyal-
• Whatarethepositionsoftheobjects?
lowsustotrainthemodeltoanswercountingquestions
• Whatsubtledetailsarenoticeable?
with points acting as a form of chain-of-thought. We
• Whatisinthebackground?
collected2.3Mquestion-pointpairsfrom428kimages.
• Whatisthestyleandcolor?
Toenablepointsasaformofexplanation,wefollowed
Theannotators’audiowasthentranscribedusinganoff- thePixMo-AskModelAnythingpipelinebutaugmented
the-shelf speech-to-text system, and then the transcribed itsothattheannotatorcouldpasstheLLMalistoftext-
textwasprocessedusingalanguage-onlyLLMtoimprove annotated points. The LLM was then prompted to use
the text quality (e.g., removing spoken artifacts, normaliz- these points, if appropriate, to support its answer. We
ing style). We also created a fourth image description by collected79kquestion-answerpairsfrom29kimages.
askingthelanguage-onlyLLMtosummarizethethreeorig- • PixMo-CapQA: We generated an additional 214k
inaltranscriptsintoasingledescription. question-answer pairs from 165k images by promptingFigure2. (Left)Averagescoresonthe11academicbenchmarks. SeeTable1forper-benchmarkresults. (Right)Eloratingsfromour
humanpreferenceevaluation.
alanguage-onlyLLMtoaskandanswerquestionsgiven 4.Evaluation
onlytheground-truthcaptionforanimage. Toincrease
Vision-languagemodelevaluationisevolvingrapidly,with
diversity,wecreatedalistofhigh-leveltopicsandstyles
new academic benchmarks constantly appearing. These
andaskedthemodeltousethem.
benchmarksworkwellforevaluatingspecificskills,butdo-
• PixMo-Docs: We prompted an LLM to generate code
ing well on them often requires answering questions in a
for255ktextandfigure-heavyimages,includingcharts,
benchmark-specific style. These answers are often short
documents,tables,anddiagrams. Wethenpromptedthe
anddonotworkwellinaconversationalsetting.Asaresult,
LLMtogenerate2.3Mquestion-answerpairsbasedon
academicbenchmarksprovideonlyapartialpictureofhow
privilegedaccesstothecode(theimageswerenotused).
a model performs. To complement these benchmarks, we
• PixMo-Clocks: We constructed a new dataset of syn-
perform a human evaluation that allows us to rank models
thetic analog clocks with questions and answers about
accordingtouserpreference.
thetime. Theimageswererenderedfrom∼50different
watchesandadiversesetof∼160krealisticwatchface Foracademicbenchmarking,weattemptedtocollectre-
styles featuring randomly chosen times. We collected sultsforallmodelsonasetof11commonlyusedacademic
826kexamples. benchmarks.1 We prioritized using numbers published by
• Academic datasets: VQA v2 train (COCO 2014 sub- theauthorsthemselveswhentheywereavailable,butmany
set) [9], TextVQA train [29], OK-VQA train [19], were missing. When results were not available, we at-
ChartQA train (human and augmented examples bal- tempted to find the best previously reported values from
anced equally) [20], DocVQA train [21], Infograph- other technical reports or from public leaderboards, such
icVQA train [22], AI2D train (transparent and opaque astheOpenVLMLeaderboard. Finally,ifavaluewasstill
label boxes) [13], A-OKVQA train [28], Android-
Control train [14], ScienceQA train [16], TabMWP 1AI2D test, ChartQA test, VQA v2 test, DocVQA test, Infograph-
icVQA test, TextVQA val, RealWorldQA [2], MMMU val [34], Math-
train[17],ST-VQAtrain[6],TallyQAtrain[3],DVQA
Vistatestmini[18],CountBenchQA[5],FlickrCount(wecollectedthis
train[11],FigureQAtrain[12],andPlotQAtrain[23]. newdatasetthatissignificantlyharderthanCountBenchQA).Figure3. VLMOpennessComparison. WecharacterizetheopennessofVLMsbasedontwoattributes(openweights,opendataand
code) across three model components (the VLM and its two pre-trained components, the LLM backbone and the vision encoder). In
additiontoopenvs. closed,weusethe”distilled”labeltoindicatethatthedatausedtotraintheVLMincludesimagesandtextgenerated
byadifferent,proprietaryVLM,meaningthatthemodelcannotbereproducedwithoutadependencyontheproprietaryVLM.
missing,wecomputeditourselves.Wenotethatcomputing claimed “zero-shot” performance (often reported for
resultsisdifficultinpractice. Forafixedmodel,resultson closed-data models) and the supervised performance of
agivenbenchmarkcanvarybyalargeamount(e.g.,10per- modelsthatexplicitlytrainonbenchmarktrainingsets. The
centagepoints)dependingonthedetailsofhowitwaseval- distinctionbetweensupervisedtrainingandzero-shottrans-
uated. Further complicating matters, in many cases, criti- ferisfuzzysinceonecancuratenewdatasourcesthatserve
cal evaluation details, such as what prompts were used or aseffectiveproxiesforanygivenbenchmark’sliteraltrain-
howthedatawasprocessed,maynotbeavailable,making ingdata. Whentrainingdataisnotdisclosed, thecommu-
itdifficulttoreproducepublishedresults. Theseissuesun- nityhasnomeansofevaluatingzero-shottransferclaims.
derscoretheimportanceofopenevaluation. Forourhumanevaluation, wecollectedadiversesetof
We also avoid making a strong distinction between 15kimageandtextpromptpairsandqueriedasetofVLMsni
mi A
Model
AI2Dtest
ChartQAtest
VQAv2testdev
DocVQAtest
Info.VQAtest
TextVQAval
RealWorldQA
MMMUval
MathVistatest CountBenchQ
FlickrCount
Average
APIcallonly
GPT-4V 89.4 78.1 77.2 87.2 75.1 78.0 61.4 63.1 58.1 69.9 45.0 71.1
GPT-4o-0513 94.2 85.7 78.7 92.8 79.2 77.4 75.4 69.1 63.8 87.9 59.6 78.5
Gemini1.5Flash 91.7 85.4 80.1 89.9 75.3 78.7 67.5 56.1 58.4 81.6 61.1 75.1
Gemini1.5Pro 94.4 87.2 80.2 93.1 81.0 78.7 70.4 62.2 63.9 85.8 64.3 78.3
Claude-3Haiku 86.7 81.7 68.4 88.8 56.1 67.3 45.5 50.2 46.4 83.0 43.9 65.3
Claude-3Opus 88.1 80.8 66.3 89.3 55.6 67.5 49.8 59.4 50.5 83.6 43.3 66.7
Claude-3.5Sonnet 94.7 90.8 70.7 95.2 74.3 74.1 60.1 68.3 67.7 89.7 58.3 76.7
Openweightsonly
PaliGemma-mix-3B 72.3 33.7 76.3 31.3 21.4 56.0 55.2 34.9 28.7 80.6 60.0 50.0
Phi3.5-Vision-4B 78.1 81.8 75.7 69.3 36.6 72.0 53.6 43.0 43.9 64.6 38.3 59.7
Qwen2-VL-7B 83.0 83.0 82.9 94.5 76.5 84.3 70.1 54.1 58.2 76.5 48.0 73.7
Qwen2-VL-72B 88.1 88.3 81.9 96.5 84.5 85.5 77.8 64.5 70.5 80.4 55.7 79.4
InternVL2-8B 83.8 83.3 76.7 91.6 74.8 77.4 64.2 51.2 58.3 57.8 43.9 69.4
InternVL2-LLaMa-3-76B 87.6 88.4 85.6 94.1 82.0 84.4 72.7 58.2 65.5 74.7 54.6 77.1
Pixtral-12B 79.0 81.8 80.2 90.7 50.8 75.7 65.4 52.5 58.0 78.8 51.7 69.5
Openweights+data (†distilled)
LLaVA-1.5-7B 55.5 17.8 78.5 28.1 25.8 58.2 54.8 35.7 25.6 40.1 27.6 40.7
LLaVA-1.5-13B 61.1 18.2 80.0 30.3 29.4 61.3 55.3 37.0 27.7 47.1 35.2 43.9
xGen-MM-interleave-4B† 74.2 60.0 81.5 61.4 31.5 71.0 61.2 41.1 40.5 81.9 50.2 59.5
Cambrian-1-8B† 73.0 73.3 81.2 77.8 41.6 71.7 64.2 42.7 49.0 76.4 46.6 63.4
Cambrian-1-34B† 79.7 75.6 83.8 75.5 46.0 76.7 67.8 49.7 53.2 75.6 50.7 66.8
LLaVAOneVision-7B† 81.4 80.0 84.0 87.5 68.8 78.3 66.3 48.8 63.2 78.8 54.4 72.0
LLaVAOneVision-72B† 85.6 83.7 85.2 91.3 74.9 80.5 71.9 56.8 67.5 84.3 60.7 76.6
TheMolmofamily:Openweights,Opendata,Opentrainingcode,Openevaluations
MolmoE-1B 86.4 78.0 83.9 77.7 53.9 78.8 60.4 34.9 34.0 87.2 79.6 68.6
Molmo-7B-O 90.7 80.4 85.3 90.8 70.0 80.4 67.5 39.3 44.5 89.0 83.3 74.6
Molmo-7B-D 93.2 84.1 85.6 92.2 72.6 81.7 70.7 45.3 51.6 88.5 84.8 77.3
Molmo-72B 96.3 87.3 86.5 93.5 81.9 83.1 75.2 54.1 58.6 91.2 85.2 81.2
Table1. Academicbenchmarkresultscoveringtencommonlyuseddatasetsplusonenewlycollectedcountingbenchmark,FlickrCount,
whichfocusesoncountinginmorechallengingnaturalimagesthanCountBenchQA.Weorganizemodelsintofourgroups:(top)proprietary
modelsthatcanonlybeaccessedthroughAPIcalls,(uppermiddle)modelswithreleasedweightsbutcloseddata,(lowermiddle)models
with released weights and released training data, noting that some of these distill (†) from other models by training on synthetic data
generatedbyproprietaryVLMs,and(bottom)theMolmofamilyofmodels.
forresponses. Wethensampledandpresentedtheresulting matchestheperformanceofGPT-4Vonbothacademic
image-text-responsetripletsforallVLMpairingstoasetof benchmarksandElo.
∼870humanannotatorswhogavepairwisepreferencerank- • OurOLMo-7B-1024andQwen27Bbasedmodelsper-
ings. Acrossallpairsofmodels, wecollectedgreaterthan formcomfortablybetweenGPT-4VandGPT-4oonboth
325k preference ratings (∼450 matches per model pair). academicbenchmarksandtheEloranking.
Fromthesepreferencerankings,wecalculatedanElorank- • Ourbest-in-classQwen272Bbasedmodelachievesthe
ingusingtheBradley-Terrymodelfollowingthemethodol- highestacademicbenchmarkscoreandrankssecondin
ogyofLMSYSOrg’sChatbotArena[1]. Elo,justbehindGPT-4o.
• Our best model outperforms many state-of-the-art pro-
Broadly speaking, the academic benchmark results and
prietary systems, including Gemini 1.5 Pro and Flash
human evaluation strongly agree, with the exception of
andClaude3.5Sonnet.
Qwen2-VL[31],whichperformsstronglyontheacademic
• To highlight Molmo’s potential for action we tested
benchmarksandcomparativelyunderperformsinthehuman
Molmo-72BonAndroidControl[14]whereitachieved
evaluation. Wehighlightafewkeyresults:
88.7% low-level accuracy and 69.0% high-level accu-
• Our most efficient model, MolmoE-1B, based on racy,comparingwelltotheresultsof83.2%and70.8%
the OLMoE-1B-7B mixture-of-experts LLM, nearly reportedin[14].1078.88 1076.42 1074.47
1068.51
1055.78 1054.20 1051.21 1050.22
1050 1040.53 1036.73
1031.74
1024.97 1023.55
1017.60 1015.97
999.33
1000
982.06 979.54
970.74
960.12
952.82 952.52 952.15 951.04
950
937.04
900
850 840.98
820.90
GPT 4o Molmo 72B Gemini 1. C5 l aPr uo de-3.5 Sonnet Molmo 7B- GD emini 1.5 Flash Molm Lo L A7 VB- A O OneVision 72B GPT 4V Qwen VL2 72B Molmo 1B-E Qwen V LL L2 A V7 AB One IVi ns ti eo rn n 7 VB L2 LLAMA 76B Pixtral 12 CB laude-3 Haik Pu hi3.5 Vision 4B BLIP- C3 laude-3 Opus LLAVA-1.5 13B Intern VL2 8 CB ambrian-1 34B Cambrian-1 8B LLAVA-1. P5 a li7 GB emma-mix 3 CB hameleon 30B Chameleon 7B
Figure4.OurElohumanpreferenceevaluationsused15kimageandtextpromptpairs.WequeriedeachVLMforresponses,andpresented
theresultingimage-text-responsetripletsforallVLMpairingstoasetof∼870humanannotatorswhogavepairwisepreferencerankings,
foratotalof325kpairwisecomparisonsacross27models,makingitthebiggesthumanpreferenceevaluationformultimodalmodelsto
date.Asareference,ourELOrankingsarebasedon3×morevotesthanChatbotArena(LMSYS)forvisionmodels.
5.ReleasePlan
Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran
Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko
Our first release on September 25, 2024 includes a demo, Bosˇnjak,XiChen,MatthiasMinderer,PaulVoigtlaender,IoanaBica,
inferencecode,andthefollowingmodelweights: Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier
Henaff, XiXiong, RaduSoricut, JeremiahHarmsen, andXiaohua
• MolmoE-1B using the fully open OLMoE-1B-7B Zhai. PaliGemma:Aversatile3BVLMfortransfer. arXivpreprint
mixture-of-expertsLLM arXiv:2407.07726,2024.1,2,4
• Molmo-7B-O using the fully open OLMo-7B-1024 [6] AliFurkanBiten,RubenTito,AndresMafla,LluisGomez,Marc¸al
Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas.
LLM(anOctober2024pre-release,tobepubliclater)
Scenetextvisualquestionanswering.InICCV,2019.4
• Molmo-7B-D, our demo model, using the open-weight
[7] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Ji-
Qwen27BLLM aqi Wang, Feng Zhao, and Dahua Lin. ShareGPT4V: Improv-
• Molmo-72B, our best performing model, using the inglargemulti-modalmodelswithbettercaptions. arXivpreprint
arXiv:2311.12793,2023.1
open-weightQwen272BLLM
[8] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam,
Buildinguponthiswork,soonwe’llbereleasing: Saurabh Gupta, Piotr Dolla´r, and C Lawrence Zitnick. Microsoft
cococaptions:Datacollectionandevaluationserver. arXivpreprint
• Amoredetailedversionofthistechnicalreport
arXiv:1504.00325,2015.2
• AllPixModatasets
[9] YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBatra,and
• Updatedmodelweights DeviParikh. MakingtheVinVQAmatter: Elevatingtheroleof
• Trainingandevaluationcode imageunderstandinginvisualquestionanswering. InCVPR,2017.
4
[10] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rod-
References
neyKinney, OyvindTafjord, A.Jha, HamishIvison, IanMagnus-
son, Yizhong Wang, Shane Arora, David Atkinson, Russell Au-
[1] Chatbot arena: New models and Elo system update. https:
thur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas,
//lmsys.org/blog/2023-12-07-leaderboard/. Ac-
YanaiElazar, YulingGu, JackHessel, TusharKhot, WilliamMer-
cessed:2024-09-24.6
rill,JacobDanielMorrison,NiklasMuennighoff,AakankshaNaik,
[2] RealWorldQA. https://huggingface.co/datasets/ Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha
xai-org/RealworldQA.Accessed:2024-09-24.4
Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma
[3] Manoj Acharya, Kushal Kafle, andChristopher Kanan. TallyQA: Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi,
Answeringcomplexcountingquestions.InAAAI,2019.4 NathanLambert,KyleRichardson,LukeZettlemoyer,JesseDodge,
[4] Meta AI. The llama 3 herd of models. arXiv preprint Kyle Lo, Luca Soldaini, Noah A. Smith, and Hanna Hajishirzi.
arXiv:2407.21783,2024.2 OLMo:Acceleratingthescienceoflanguagemodels.arXivpreprint
[5] Lucas Beyer, Andreas Steiner, Andre´ Susano Pinto, Alexander arXiv:2402.00838,2024.2
Kolesnikov,XiaoWang,DanielSalz,MaximNeumann,IbrahimAl- [11] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan.
abdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas DVQA:Understandingdatavisualizationsviaquestionanswering.
Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam InCVPR,2018.4
)snoitaulavE
ecnereferP
namuH(
gnitar
olE[12] SamiraEbrahimiKahou,VincentMichalski,AdamAtkinson,A´kos [31] PengWang,ShuaiBai,SinanTan,ShijieWang,ZhihaoFan,Jinze
Ka´da´r, Adam Trischler, and Yoshua Bengio. FigureQA: An Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al.
annotated figure dataset for visual reasoning. arXiv preprint Qwen2-vl: Enhancing vision-language model’s perception of the
arXiv:1710.07300,2017.4 worldatanyresolution.arXivpreprintarXiv:2409.12191,2024.6
[13] AniruddhaKembhavi,MikeSalvato,EricKolve,MinjoonSeo,Han- [32] HuXu,SainingXie,XiaoqingTan,Po-YaoHuang,RussellHowes,
nanehHajishirzi,andAliFarhadi. Adiagramisworthadozenim- VasuSharma,Shang-WenLi,GargiGhosh,LukeZettlemoyer,and
ages.InECCV,2016.4 ChristophFeichtenhofer.DemystifyingCLIPdata.InICML.3
[14] Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo [33] AnYang,BaosongYang,BinyuanHui,BoZheng,BowenYu,Chang
Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang,
effects of data scale on computer control agents. arXiv preprint GuantingDong,HaoranWei,HuanLin,JialongTang,JialinWang,
arXiv:2406.03679,2024.4,6 Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jin-
[15] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.Visual grenZhou,JinzeBai,JinzhengHe,JunyangLin,KaiDang,Keming
instructiontuning.InNeurIPS,2023.1,2 Lu, KeqinChen, KexinYang, MeiLi, MingfengXue, NaNi, Pei
Zhang,PengWang,RuPeng,RuiMen,RuizeGao,RunjiLin,Shi-
[16] PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-WeiChang,
jieWang,ShuaiBai,SinanTan,TianhangZhu,TianhaoLi,Tianyu
Song-ChunZhu,OyvindTafjord,PeterClark,andAshwinKalyan.
Liu,WenbinGe,XiaodongDeng,XiaohuanZhou,XingzhangRen,
Learntoexplain: Multimodalreasoningviathoughtchainsforsci-
Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao,
encequestionanswering.InNeurIPS,2022.4
Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui,
[17] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun
Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv
Zhu,TanmayRajpurohit,PeterClark,andAshwinKalyan.Dynamic
preprintarXiv:2407.10671,2024.2
promptlearningviapolicygradientforsemi-structuredmathematical
[34] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,RuoqiLiu,Ge
reasoning.InICLR,2023.4
Zhang,SamuelStevens,DongfuJiang,WeimingRen,YuxuanSun,
[18] PanLu,HritikBansal,TonyXia,JiachengLiu,ChunyuanLi,Han-
CongWei,BotaoYu,RuibinYuan,RenliangSun,MingYin,Boyuan
nanehHajishirzi,HaoCheng,Kai-WeiChang,MichelGalley,and
Zheng,ZhenzhuYang,YiboLiu,WenhaoHuang,HuanSun,YuSu,
Jianfeng Gao. MathVista: Evaluating mathematical reasoning of
andWenhuChen. MMMU:Amassivemulti-disciplinemultimodal
foundationmodelsinvisualcontexts.InICLR,2024.4
understandingandreasoningbenchmarkforexpertAGI. InCVPR,
[19] KennethMarino,MohammadRastegari,AliFarhadi,andRoozbeh
2024.4
Mottaghi. OK-VQA: A visual question answering benchmark re-
quiringexternalknowledge.InCVPR,2019.4
[20] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul
Hoque.ChartQA:Abenchmarkforquestionansweringaboutcharts
withvisualandlogicalreasoning.InACL,2022.4
[21] MineshMathew,DimosthenisKaratzas,andCVJawahar.DocVQA:
AdatasetforVQAondocumentimages.InWACV,2021.4
[22] Minesh Mathew, Viraj Bagal, Rube`n Tito, Dimosthenis Karatzas,
ErnestValveny,andCVJawahar.InfographicVQA.InWACV,2022.
4
[23] NiteshMethani,PrithaGanguly,MiteshMKhapra,andPratyushKu-
mar.PlotQA:Reasoningoverscientificplots.InWACV,2020.4
[24] NiklasMuennighoff,LucaSoldaini,DirkGroeneveld,KyleLo,Ja-
cobMorrison,SewonMin,WeijiaShi,PeteWalsh,OyvindTafjord,
NathanLambert,YulingGu,ShaneArora,AkshitaBhagia,Dustin
Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim
Dettmers,DouweKiela,AliFarhadi,NoahA.Smith,PangWeiKoh,
AmanpreetSingh,andHannanehHajishirzi.OLMoE:Openmixture-
of-expertslanguagemodels.arXivpreprintarXiv:2409.02060,2024.
2
[25] OpenAI. GPT-4technicalreport. arXivpreprintarXiv:2303.08774,
2023.1
[26] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya
Sutskever. Improving language understanding by generative pre-
training.OpenAIBlog,2018.2
[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,
PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever.
Learningtransferablevisualmodelsfromnaturallanguagesupervi-
sion.InICML,2021.3
[28] DustinSchwenk,ApoorvKhandelwal,ChristopherClark,Kenneth
Marino,andRoozbehMottaghi.A-OKVQA:Abenchmarkforvisual
questionansweringusingworldknowledge.InECCV,2022.4
[29] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei
Chen, DeviParikh, andMarcusRohrbach. TowardsVQAmodels
thatcanread.InCVPR,2019.4
[30] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,Llion
Jones,AidanN.Gomez,ŁukaszKaiser,andIlliaPolosukhin.Atten-
tionisallyouneed.InNeurIPS,2017.2