STREAMINGNEURALIMAGES
MarcosV.Conde†§ AndyBigos§ RaduTimofte†
† ComputerVisionLab,CAIDAS&IFI,UniversityofWu¨rzburg,Germany
§ VisualComputingGroup,FTG,SonyPlayStation
ABSTRACT
ImplicitNeuralRepresentations(INRs)areanovelparadigm
for signal representation that have attracted considerable in-
terestforimagecompression. INRsofferunprecedentedad-
vantagesinsignalresolutionandmemoryefficiency,enabling
new possibilities for compression techniques. However, the
existinglimitationsofINRsforimagecompressionhavenot
beensufficientlyaddressedintheliterature. Inthiswork,we Fig. 1: Exploring the behaviour of neural image representa-
explore the critical yet overlooked limiting factors of INRs, tions. (Left)Animageafterlosingonerandompixel. (Mid)
such as computational cost, unstable performance, and ro- The corresponding implicit neural representation (INR) [2].
bustness. Throughextensiveexperimentsandempiricalanal- (Right)TheINRnetworkafterlosingonerandomneuron.
ysis,weprovideadeeperandmorenuancedunderstandingof
implicit neural image compression methods such as Fourier
is that they are not tied to spatial resolution. Unlike con-
Feature Networks and Siren. Our work also offers valuable
ventional methods where image size is tied to the number
insightsforfutureresearchinthisarea.
of pixels, the memory needed for these representations only
Index Terms— Image Compression, Implicit Neural scaleswiththecomplexityoftheunderlyingsignal[1,2]. In
Representations,MachineLearning,NeuralNetworks essence,theyoffer“infiniteresolution”,meaningtheycanbe
sampledatanyspatialresolution[2]byupsamplingtheinput
domainX (eg.,[H,W]gridofcoordinates).
1. INTRODUCTION
Recent works such as COIN [3, 5] and ANI [6] demon-
ImplicitNeuralRepresentations(INRs)allowtheparameter- stratesthatwecanfit“large”images(720p)usingsmallneu-
ization of signals of all kinds and have emerged as a new ralnetworks(8kparameters)asINRs,whichimpliespromis-
paradigminthefieldofsignalprocessing,particularlyforim- ingcompressioncapabilities[3,5]. Theseseminalworks[3,
age compression [1, 2, 3, 4]. Differing from traditional dis- 4,6]showthatINRscanbeabetteroptionthanimagecodecs
creterepresentations(eg.,animageisadiscretegridofpixels, suchasJPEG[7]insomescenarios(eg.,atlowbit-rates).
audiosignalsarediscretesamplesofamplitudes),INRsusea
Considering this new paradigm, we must emphasize that
continuous function to describe the signal. Such a function
an image is no longer characterized as a set of RGB pixels,
maps the source domain X of the signal to its characteristic
but as a simple neural network (ie., an MLP). This concept
values Y. For instance, it can map 2D pixel coordinates to
posesopenquestionsforinstance,losingapixelinanimage
their corresponding RGB values in the image I[x,y]. This
iswell-understood,butwhatistheequivalentinINRs? What
function ϕ is approximated using neural networks, thus it is
happensifthenetworklosesoneneuron? –SeeFigure1.
continuousanddifferentiable. Wecanformulatethisas
In this work we explore in depth the limitations of INRs
ϕ:R2 (cid:55)→R3 x→ϕ(x)=y, (1) for image compression and streaming. We analyze major
where ϕ is the learned INR function, the domains X ∈ R2 limitations such as the volatility and stochastic nature of
andY ∈R3,theinputcoordinatesx=(x,y),andtheoutput theseneuralnetworks,theircomplexity,andgreatsensitivity
RGB value y = [r,g,b]. In summary, INRs are essentially to hyper-parameters. We also introduce a novel analysis of
simpleneuralnetworks(NN),oncethesenetworksϕ(over)fit therobustnessoftheseneuralnetworks,whichhasimportant
thesignal,theybecomeimplicitlythesignalitself. implicationsinthecontextofimagetransmission(Fig.2).
In the context of image compression, this method offers Our approach SPINR (Streaming Progressive INRs) en-
uniqueadaptabilitythankstoitscontinuousanddifferentiable ablestosolvemanyofthoseproblems,andrepresentsamore
nature[3,4,5]. OneofthemajoradvantagesofusingINRs reliableapproachforimplicitneuralimagecompressionand
4202
peS
52
]VC.sc[
1v43171.9042:viXrax→(200,205,140)
Fit Entropy Recover Decode
x=(x,y)∈[H,W] Q
Signal Coding INR Image
... ...
Transmission
... ...
x
Meta-learn Fit Recover Decode
... ...
Initialization Adapt INR Image
(a)Conventionalimage (b)Coordinate-basedMLP (c)DifferentvariantsofimagestreamingusingINRs[4,5]
Fig.2:Weillustratethegeneralconceptsaroundneuralimagerepresentations[1,2].Wealsoillustratethecommonframeworks
forstreamingimagesasINRs[5,4]. Thiscanbeextendedtoothersortofsignalssuchasaudioor3Drepresentations.
transmission. InFigure3,wecomparetwopossiblesolutions
forefficientimagetransmission[8]. 1101...
0110...
2. RELATEDWORK
Codec Codec
Inrecentyears,ImplicitNeuralRepresentations(INRs)have
Stream
become increasingly popular in image processing as a new
method for representing images [1, 2, 9]. These neural net-
INR INR
works, usually simple Multilayer Perceptrons (MLPs), are
also known as coordinate-based networks. We denote the Donot wait!
INRsasafunctionϕwithparametersθ,definedas:
Fig. 3: Image streaming using (top) traditional image repre-
sentations and codecs [7], (bot.) our method, SPINR, based
onimplicitneuralimagecompression[2,3]allowstodecode
ϕ(x)=W (ς ◦ς ◦...◦ς )(x)+b
n n−1 n−2 0 n (2) theimagewithouthavingthefullneuralnetwork.
ς (x )=α(W x +b ),
i i i i i
where ς are the layers of the network (considering their
i
correspondingweightmatrixWandbiasb),andαisanon- 2.1. ImageTransmission
linear activation eg., ReLU, Tanh, Sine [2], complex Gabor
wavelet[10]. Consideringthisformulation,theparametersof
theneuralnetworkθ isthesetofweightsandbiasesofeach StreamingimagesasINRsisanovelresearchproblem[4,5,
layer. WeillustratetheminFigure2b. 6]. In this context, it is fundamental to understand that the
Tancik et al. [1] introduced fourier features as input en- imageisnolongercharacterizedasasetofRGBpixels, but
codingsforthenetwork,enhancingtheircapabilitytomodel asasetofweightsandbiases(θie.,theneuralnetworkitself).
high-frequency functions in lower-dimensional domains.
Sitzmann et al. [2] presented SIREN, a periodic activation WeillustrateinFigure2cthedifferentapproaches: (top)
function for neural networks, specifically designed to bet- we train the neural network ϕ to fit the signal, next we can
ter model complex natural signals. Based on this work, applyquantization(Q)andencodetheparametersθ.Thenwe
COIN [3, 5] explored the early use of INRs for efficient can transmit the parameters, the client can recover the net-
imagecompression. work,andthusreconstructthenaturalRGBimage. (bot.) We
Wealsofindotherworksthattacklenewactivationfunc- useafixedmeta-INRasinitialization[14,15,16],andadjust
tions such as multiplicative filter networks (MFN) [9] and (fine-tune) the network to the desired image, next we trans-
Wire[10],andnovelINRrepresentations[11,12,13]. mit only a residual ∆θ = θ − θ∗. This allows to simplify
the transmitted information and make the process more effi-
Inthisworkwewillanalyzethemostpopular(andrecent)
cient[4].FinallytheclientrecoverstheINRknowing∆θand
INR models: FourierNets [1] (MLP with Positional Encod-
θ(meta-INR),andreconstructsthenaturalimage.
ing),SIREN[2],MFN[9],Wire[10]andDINER[11].3. COMPRESSIONEXPERIMENTALRESULTS
We study the different INR methods considering the follow-
ingfactorsrelatedtoimagecompressionandstreaming:
1. General theoretical limitations and comparison to tra-
ditionalCodecs(JPEG[7],JPEG2000[17]).
2. Unstabletrainingandhyper-parameterssensitivity.The
performance of INR methods highly varies depending
onhyper-parameters,andthetargetsignal.
3. ModelComplexity.Thedesignoftheneuralnetworkis
Fig. 4: Comparison of INRs with standard codecs [7, 17].
paramounttoensureapositiverate-distortiontradeoff.
ThebaselineINRsrepresentaconstantbpplevel.
4. Model Robustness. We understand noise and “losing
pixels” in classical images (Fig. 2a), however, we do
not find any reference on equivalent noise and “losing Efficiency&Speed: Traditionalcodecsareoptimizedfor
neurons”inINRs(eg.,Fig.1).Forthisreason,weintro- low computational overhead and can rapidly compress and
duceanovelanalysisontherobustnessoftheseneural decompressimages. INRs,ontheotherhand,requireimage-
networks—consideringalsothestreamingscenario. specific training, and forward passes through the neural net-
workforimagereconstruction,whichcanbecomputationally
Dataset In our study we use a common dataset in image intensive. Despite meta-learning [4, 14, 15, 16] can help to
processinganalysis(eg.,compression,super-resolution,etc),
speedupthetraining,itisastillalimitingfactor.
which consists of Set5 and Set14. The images offer wide
Explicit Frequency Handling: Traditional codecs use
varietyofcoloursandhigh-frequencies(eg.,Fig.1).
methods like Discrete Cosine Transform (DCT) to handle
Implementation Details We implement all the methods in frequency components explicitly. This ensures more precise
PyTorch, using the author’s implementations when avail- controloverrate-distortionratios[18]. INRslearnsuchhigh-
able. We train all the models using the same environ- frequencies,whichismoredifficultanderror-prone[2,21].
ment with the Adam optimizer, and we adapt the learn- Robustness: Traditionalcodecsaregenerallymorerobust
ing rate for each method’s requirements. We use four toimagevariations(eg.,noise). Thisaspectwasnotexplored
NVIDIARTX4090Ti. Werepeateveryexperiment10times in depth in the INRs literature. In Section 3.5 we provide a
with different seeds. In every experiment, each model is novelanalysisinthisdirection–alsorelatedtostreaming.
trained for 2000 steps (or equivalent time) using the L
2 Summary: while INRs offer exciting possibilities, tradi-
loss [1, 2] to minimize the RGB image reconstruction error
tionalcodecscurrentlyprovideamorereliable,efficient,ma-
(cid:80) ∥I[x,y]−ϕ(x,y)∥2, ∀(x,y)∈[H,W].
x,y 2 tureandstandardizedapproachforimagecompression.
WecompareINRswithcodecsinFigure4. Strumpleret
3.1. GeneralLimitations al. [4] also proved the limited performance of INRs in com-
parisontotraditionalcodecsusingotherdatasets.
In the domain of image compression, INRs have shown
promise but also exhibit fundamental limitations. Critically,
asalossycompressionmethod,theirabilitytocapturehigh- 3.3. ModelComplexity
frequencycomponentsofimagesisconstrainedbyShannon’s
We denote the width and depth of an INR as the number of
theory [18]. Many INR approaches, even those employing
hidden neurons (h) per layer, and the number of layers (l)
Fourierfeaturesorperiodicactivationfunctions,fallshortof
respectively. The complexity of an INR (ie., number of pa-
this,particularlyforimageswithcomplextextures[1,2,9].
rameters)dependsonitsdesign,handl. Thisisfundamental
Further, although INRs are theoretically independent of
becauselargemodelsdonotofferagoodcompressionalter-
resolution, practical image discretization introduces errors
nativetoJPEG—asprovedin[4]. Forinstance,weconclude
that escalate with increasing resolution. Finally, most INRs
approaches are signal-specific ie., the neural network fits a from our experiments that models with h ≥ 256 and l ≥ 3
arenotablyworstthanJPEGandJPEG2000intermsofrate-
particularimage. Thismightimply“long”trainingonGPUs.
distortiontrade-off—wecannotevenplottheminFig.4.
Letnbethenumberofpixelsintheimage. Thecomplex-
3.2. ComparisonwithtraditionalCodecs
ityofINRsbasedonMLPsdoesnotdependontheresolution
Traditional codecs like JPEG [7] and JPEG2000 [17], and oftheinputsignal. However,insomerecentmethodssuchas
other neural compression techniques [19, 20], have several DINER [11] -based on hash mappings- the complexity does
advantagesoverINRswhenitcomestoimagecompression: dependontheinputimagefollowinganorderO(n),wherenMethod Param.(K) PSNR↑ SSIM↑ CF↑
FourierNet*[1] 132.09 30.78±0.23 0.857 1.22
SIREN*[2] 133.12 31.68±2.08 0.866 1.22
MFN*[9] 136.96 33.98±0.18 0.909 1.17
FourierNet[1] 66.30 30.10±0.42 0.832 2.41
SIREN[2] 66.82 30.95±2.31 0.855 2.40
MFN[9] 70.27 32.95±0.47 0.895 2.25
Wire[10] 66.82 25.96±3.83 0.712 1.20
DINER[11] 545.55 27.34±15.5 0.751 0.32
Table 1: Comparison of different INR approaches for im-
age compression. We report the mean PSNR (± std.) and
SSIM [22] over 10 runs. We also show the average Com-
pressionFactor(CF= imagesize/modelsize). Wehighlight
DINER’shighvariabilityandnegativeCFie.,model>image.
Method Base L@1 L@5 L@10
FourierNet[1] 30.10 28.50 24.22 21.32
SIREN[2] 30.95 23.94 18.99 16.13
MFN[9] 32.95 29.94 25.98 23.74
Table 2: Robustness study to losing k-neurons (L@k). We
reportthemeanPSNR(dB)over10trialsforeachsetup. As
weshowinFig.6theperformancedecaysnotably.
Fig.5: TrainingevolutionofthedifferentINRmethods. We
observehightraininginstabilityforDINER[11]. Themodels
haveh=128,l=4. Wealsoshowthecorrespondingimage.
3.5. ModelRobustness
isthenumberofpixelsintheimage. Thismeansthatforan Inspired by adversarial attacks [23], we study how the INRs
imagewithresolutionH,W thenumberoflearntparameters behave under information loss. For instance, noise is well-
is(H×W ×2)+θ,whereθisonlythesetofparametersof understood in natural images, however, it is unexplored in
theMLP.Thus,theseapproachesoffernegativecompression neuralimagerepresentationsie.,applygaussiannoiseonthe
rateie.,theINRisbiggerthantheimageitself. parametersθ. WeprovideanexampleinFigure6,whichre-
In Table 1 we study models with h = 256,l = 2 (high- veals the different behaviour of noise in both classical and
lighted with “*”) and smaller models with h = 128,l = 4 implicit neural representations. These results are consistent
(ie., half width, double depth). We can conclude that only through the whole dataset. Note that for noise levels with
MLP-based approaches with “low” complexity can be good σ ≤1e−4thenetworkdoesnotsufferinformationloss.
image compressors, offering a positive compression factor
Althoughthisisaninterestingtheoreticalresult, itisnot
(CF).ForinstanceCF> 2indicatestheINRistwicesmaller
realistic.Forthisreason,wefocusonthe“lost-neuron”prob-
thanthenaturalimage. SeethesemodelsalsoinFig.4.
lem ie., losing randomly some neurons of the network. Be-
Furthermore,modelquantizationandpruningcanhelpto yondtheoreticalinterest,thisisdirectlyassociatedtopossible
reducethemodel’ssizewhilepreservinghighfidelity[4]. packetlossduringthetransmission.
WeprovideastudyinTable2whereweremoverandomly
(ie., set to 0) 1, 5, and 10 neurons from the INR, and evalu-
3.4. UnstableTrainingandSensitivity ate its reconstruction performance after the corruption. We
observe that losing a single neuron might imply losing most
WefindthatINRsareextremelysensitivetohyper-parameters, of the signal information. Noting that in the streaming use
speciallylearningrate,handl.WeshowthisvolatilityinFig- casepacketlosswouldimplyasignificantlylargernumberof
ure5(weplottheaverageandconfidenceintervalconsidering neuronsbeinglost. Thiswasnotconsideredinanyprevious
the10runs),andinTable1. Recentmethods,Wire[10]and work. WealsoshowinFigure6theimpactoflosingasingle
Diner[11],arespeciallyunstable. Thisbehaviouralsovaries neuron. Webelievetheseresultsserveasthefirstbaselineto
dependingonthetargetsignal,anditis(sofar)unpredictable. interpretabilityandadversarialrobustnessinINRs.Fig. 6: Illustration of robustness analysis. From left to right: noisy image (σ = 0.01), trained INR [2], INR with noise
(σ=0.001),withnoise(σ=0.01),andlost-neuronattack. Incontrasttoconventionalimages,INRsareverysensitivetonoise.
4. SPINR:STREAMABLEPROGRESSIVEINR Algorithm1SPINRMulti-stageTraining
Require: Modelϕwithn=4hiddenlayers(L ,...,L )
0 5
Consideringthepreviousanalysisofperformanceandlimita-
Require: Input x, Ground Truth y, Stages S = [1,n+1],
tions of INRs, we are interested in an INR method with the
NumberofoptimizationstepsN
following properties: (i) robustness against information loss
Require: ListofforwardpasslayersC =[L ,L ]
0 5
ie., the model allows to reconstruct the RGB image even if
1: foreachstagesinS do
partofthenetoworkθislost.(ii)progressiveimagerepresen-
2: Initializeallmodelparametersasfrozen
tationie.,wecanreconstructtheRGBimagebystages[24].
3: if(s=1)SetC astrainableelseL sastrainable
We propose SPINR, Streaming Progressive INRs, to 4: fori=1toN do
achieve these desired properties. As a baseline, we will use 5: yˆ←ForwardpropagatexthroughactivelayersC
SIREN[2]followingpreviousworks[3,4]. 6: Computeandbackpropagatelossbetweenyˆandy
WedefinetheneuralnetworkasaMLPwithnlayersand 7: UpdatetrainableparametersL susingoptimizer
hneuronsperlayer.Strumpleretal.[4]studiedthebehaviour 8: endfor
ofvaryingthehandntofindagoodcompressionrate. Fol- 9: AddL sintoC(Addnewlearnedconnection)
lowingthiswork,wechoosen=4andh=128. 10: endfor
For simplification, we refer to each layer (including its 11: return Trainedmodelϕ
weightsandbiases)asL . OurINRisthentheapplicationof
i
n = 4hiddenlayers,andtheinput-outputmappings(L and
0
L 5). Wecanformulateitasy=L 5◦···◦L 1◦L 0(x). RGBimage.Thismakesthemodelextremelyrobusttopossi-
First,thelayerL 0 projectsthe2-dimensionalcoordinates blecorruptionsduetopacketlossie.,losingparametersfrom
into the h-dimensional hidden space. At the end, L 5 maps layers. Thiscanbeseenalsoaslayer-wiseredundancyie.,if
from h to the 3-dimensional RGB values. These two layers a layer is corrupted, we can still reconstruct the RGB image
representthesmallestpossibleconnectionornetwork. from the other layers. Moreover, this allows us to recover
Our method consists on a multi-stage training to learn the image in a progressive manner ie., we can reconstruct it
meaningful image representations by stages. This is similar basedonthereceivedpartialθ,andupdateitwhenwereceive
todeepnetworkswithstochasticdepth[25]. Wedescribethe additionallayers. ThisprogressiveRGBreconstructionisil-
trainingprocessinAlgorithm1. lustratedinFigure7,wecanappreciatehowtheimagequality
Consideringn+1stages,instage1wetrainthesmallest improveswhenweusemorelayerconnections.
possible mapping: y = L 5 ◦L 0(x). In the next stage, we Finally, a consequence of this method is the ability to
freeze the previously trained layers (eg., [L 0,L 5]) and train adapt the bit-rate during the transmission, for instance, we
thenextconnectiony = L 5◦L 1◦L 0(x),updatingonlyL 1 can transmit more or less information (layers) depending on
from the loss backpropagation. We repeat this process until thebandwidthandinfluencethequalityoftheresultantRGB.
we train L n having as frozen layers [L 0,...,L (n − 1),L 5]. Notethatmodelquantizationandpruningcanhelptoim-
Thisallowsthemodeltolearnthex→ymappingconsider- provethecompressionratewhilepreservinghighfidelity[4].
ingdifferentconnections(forwardpass)withintheINR.
4.1. ExperimentalResults
RobustnessandProgressiveDecoding
Weusethewell-knownset14datasetusedinimageprocess-
Since we learn how to map from any layer L to the output, ing analysis. For each image, we fit different INR neural
i
wedonotrequireallthelayersinthemodeltoreconstructthe networks [1, 2, 9], and we report the average reconstructionS = 1 ( 30%) S = 2 (50%) S = 5 (100%)
...
Fig.7:WeillustrateourapproachSPINR(StreamingProgressiveINRs).ThemethodallowstotransmittheINRneuralnetwork
in stages, similar to BACON [21]. This allows to reconstruct the natural image without waiting for the complete network θ.
Moreover,iftheparametersofonelayerarecorrupted(eg.,duetopacketloss),themodelcanstillproduceanaccurateimage.
Method Params. PSNR L@5 L@10 4.2. Discussion
FourierNet[1] 66.30 29.80 23.12 20.24 Based on our experiments we can conclude that the perfor-
SIREN[2] 66.82 28.26 20.00 15.78 manceoftheINRmethodsishighlyvolatile,andalsovaries
depending on the target image. However, there is no theo-
SPINR(Ours) 66.82 28.32 24.01 21.67
reticalorpracticalwayofpredicting a-prioriwhichmethod
fits best the signal. This lack of predictability is significant
Table3: ComparisonofdifferentINRapproachesforimage
drawbackwhencomparedtotraditionalencodingmethods.
compression. WereporttheaveragePSNRover10runs. We
When are INRs a good option? As we previously dis-
also show the robustness to losing k-neurons (L@k). Our
cussed, only low-complexity INRs offer a competitive rate-
methodSPINRoffersconsistentperformanceandrobustness
distortion trade-off in comparison to traditional codecs [17].
Tiny models (eg., h ∈ {16,32,64,128}, l ∈ [1,4]) allow to
PSNRover10independentrunswithdifferentseeds. Were- fixthebitrate(information)whileoptimizingforhighfidelity.
porttheseresultsinTable3. WecanseethatSPINRachieves Consideringthis,training(offline)suchmodelsenoughit-
similarperformanceastheothermethods. erations(≥5000steps),andevenrepeatedtimes,wouldallow
toderiveanINRasanoptimizedimage-specificcompressor.
Followingtheseexperiments,andinspiredbyadversarial
Nevertheless,insomecasesJPEG2000[17]andadvanceneu-
attacks [23], we study how the INRs behave under informa-
ral compression [19, 20] will still be superior. Novel works
tionloss. Wefocusonthe“lost-neuron”problemie.,losing
suchasANI[6]alsoallowadaptivebit-rateINRs.
randomlysomeneuronsoftheMLP,whichisdirectlyassoci-
atedtopossiblepacketlossduringthetransmission.
5. CONCLUSION
We provide the results in Table 3 where we remove ran-
domly (ie., set to 0) five and ten neurons from the INR, WeprovideacompletereviewonImplicitNeuralRepresen-
and evaluate its reconstruction performance (also in terms tations(INRs)forimagecompressionandstreaming,andwe
of PSNR) after the corruption. We observe that losing a introduce a novel robustness analysis of INRs. Our method
few neurons might imply losing most of the signal infor- SPINR improves the robustness of the neural network to
mation. Noting that in real streaming scenarios, packet loss packet loss, allows progressive decoding of the compressed
wouldimplyasignificantlylargernumberofparametersbe- image, and adaptive bit-rate. Our work offers a more nu-
ing lost. Our model is more robust to losing information anced understanding of implicit neural image compression,
thanks to the proposed multi-stage training (see comparison providingvaluableinsightsforfutureresearchinthisfield.
with SIREN [2]). Even if various layers are corrupted, the AcknowledgmentsThisworkwaspartiallysupportedbythe
modelcanstillleveragetheinformationfromtheothers. HumboldtFoundation(AvH).
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...Fig.8: IllustrationofimagetransmissionusingtheSPINRframework. Fromlefttoright: originalimage,stages1to5. Each
stage adds more details into the image by incorporating the features from an additional layer in the network. This allows to
transmittheimageasanINRwithlayer-wiseredundancyforrobustness,andadaptivebit-rate(ie.,variablelayers).
6. REFERENCES [8] OrenRippelandLubomirBourdev,“Real-timeadaptiveimage
compression,”inInternationalConferenceonMachineLearn-
[1] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara ing.PMLR,2017,pp.2922–2930. 2
Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-
[9] RizalFathony,AnitKumarSahu,DevinWillmott,andJZico
mamoorthi, JonathanBarron, andRenNg, “Fourierfeatures
Kolter, “Multiplicativefilternetworks,” inInternationalCon-
letnetworkslearnhighfrequencyfunctionsinlowdimensional
ferenceonLearningRepresentations,2020. 2,3,4,5
domains,” Advances in Neural Information Processing Sys-
[10] Vishwanath Saragadam, Daniel LeJeune, Jasper Tan, Guha
tems,vol.33,pp.7537–7547,2020. 1,2,3,4,5,6
Balakrishnan,AshokVeeraraghavan,andRichardGBaraniuk,
[2] VincentSitzmann, JulienMartel, AlexanderBergman, David “Wire: Waveletimplicitneuralrepresentations,” inProceed-
Lindell, and Gordon Wetzstein, “Implicit neural representa- ingsoftheIEEE/CVFConferenceonComputerVisionandPat-
tionswithperiodicactivationfunctions,” Advancesinneural ternRecognition,2023,pp.18507–18516. 2,4
informationprocessingsystems,vol.33,pp.7462–7473,2020.
[11] ShaowenXie,HaoZhu,ZhenLiu,QiZhang,YouZhou,Xun
1,2,3,4,5,6
Cao,andZhanMa, “Diner: Disorder-invariantimplicitneural
[3] EmilienDupont,AdamGolin´ski,MiladAlizadeh,YeeWhye representation,” inProceedingsoftheIEEE/CVFConference
Teh, andArnaudDoucet, “Coin: Compressionwithimplicit onComputerVisionandPatternRecognition,2023,pp.6143–
neural representations,” arXiv preprint arXiv:2103.03123, 6152. 2,3,4
2021. 1,2,5 [12] ZekunHao,ArunMallya,SergeBelongie,andMing-YuLiu,
“Implicit neural representations with levels-of-experts,” Ad-
[4] Yannick Stru¨mpler, Janis Postels, Ren Yang, Luc Van Gool,
vancesinNeuralInformationProcessingSystems,vol.35,pp.
andFedericoTombari,“Implicitneuralrepresentationsforim-
2564–2576,2022. 2
agecompression,” inEuropeanConferenceonComputerVi-
sion.Springer,2022,pp.74–91. 1,2,3,4,5 [13] Vishwanath Saragadam, Jasper Tan, Guha Balakrishnan,
RichardGBaraniuk,andAshokVeeraraghavan,“Miner:Mul-
[5] Emilien Dupont, Hrushikesh Loya, Milad Alizadeh, Adam
tiscaleimplicitneuralrepresentation,”inEuropeanConference
Golin´ski, Yee Whye Teh, and Arnaud Doucet, “Coin++:
onComputerVision.Springer,2022,pp.318–333. 2
Neural compression across modalities,” arXiv preprint
arXiv:2201.12904,2022. 1,2 [14] Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi
Schmidt,PratulPSrinivasan,JonathanTBarron,andRenNg,
[6] Leo Hoshikawa, Marcos V Conde, Takeshi Ohashi, and At- “Learnedinitializationsforoptimizingcoordinate-basedneural
sushiIrie, “Extremecompressionofadaptiveneuralimages,” representations,” inProceedingsoftheIEEE/CVFConference
arXivpreprintarXiv:2405.16807,2024. 1,2,6 onComputerVisionandPatternRecognition,2021,pp.2846–
[7] WilliamBPennebakerandJoanLMitchell, JPEG:Stillim- 2855. 2,3
agedatacompressionstandard, SpringerScience&Business [15] Jaeho Lee, Jihoon Tack, Namhoon Lee, and Jinwoo Shin,
Media,1992. 1,2,3 “Meta-learning sparse implicit neural representations,” Ad-vancesinNeuralInformationProcessingSystems,vol.34,pp.
11769–11780,2021. 2,3
[16] Yinbo Chen and Xiaolong Wang, “Transformers as meta-
learnersforimplicitneuralrepresentations,”inEuropeanCon-
ferenceonComputerVision.Springer,2022,pp.170–187. 2,
3
[17] Athanassios Skodras, Charilaos Christopoulos, and Touradj
Ebrahimi, “Thejpeg2000stillimagecompressionstandard,”
IEEE Signal processing magazine, vol. 18, no. 5, pp. 36–58,
2001. 3,6
[18] ClaudeE.Shannon, CodingTheoremsforaDiscreteSource
WithaFidelityCriterion, 1959. 3
[19] Johannes Balle´, David Minnen, Saurabh Singh, Sung Jin
Hwang, andNickJohnston, “Variationalimagecompression
with a scale hyperprior,” arXiv preprint arXiv:1802.01436,
2018. 3,6
[20] FabianMentzer,GeorgeDToderici,MichaelTschannen,and
EirikurAgustsson, “High-fidelitygenerativeimagecompres-
sion,” Advances in Neural Information Processing Systems,
vol.33,pp.11913–11924,2020. 3,6
[21] DavidBLindell,DaveVanVeen,JeongJoonPark,andGordon
Wetzstein,“Bacon:Band-limitedcoordinatenetworksformul-
tiscalescenerepresentation,” inProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition,2022,
pp.16252–16262. 3,6
[22] ZhouWang,AlanCBovik,HamidRSheikh,andEeroPSi-
moncelli, “Imagequalityassessment: fromerrorvisibilityto
structuralsimilarity,” IEEEtransactionsonimageprocessing,
vol.13,no.4,pp.600–612,2004. 4
[23] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy,
“Explaining and harnessing adversarial examples,” arXiv
preprintarXiv:1412.6572,2014. 4,6
[24] JunwooCho,SeungtaeNam,DanielRho,JongHwanKo,and
EunbyungPark, “Streamableneuralfields,” inEuropeanCon-
ferenceonComputerVision.Springer,2022,pp.595–612. 5
[25] GaoHuang,YuSun,ZhuangLiu,DanielSedra,andKilianQ
Weinberger, “Deepnetworkswithstochasticdepth,” inCom-
puter Vision–ECCV 2016: 14th European Conference, Ams-
terdam,TheNetherlands,October11–14,2016,Proceedings,
PartIV14.Springer,2016,pp.646–661. 5