TurnEveryApplicationintoanAgent:TowardsEfficient
Human-Agent-ComputerInteractionwithAPI-FirstLLM-BasedAgents
JUNTINGLU∗†,PekingUniversity,China
ZHIYANGZHANG∗†,NanjingUniversity,China
FANGKAIYANG‡,Microsoft,China
JUEZHANG,Microsoft,China
LUWANG,Microsoft,China
CHAODU,Microsoft,China
QINGWEILIN,Microsoft,China
SARAVANRAJMOHAN,Microsoft,USA
DONGMEIZHANG,Microsoft,China
QIZHANG,Microsoft,China
Fig.1. Anillustrationcomparingtaskcompletionmethods:manualoperation,UIAgent,andourapproachAXIS.Manualoperation
riskswrongtrailsifusersareunfamiliarwiththeUI.TheUIAgentrequiresnumeroussequentialinteractions.OurAXISefficiently
completesthetaskwithasingleAPIcall.
∗Bothauthorscontributedequallytothisresearch.
†WorkisdoneduringaninternshipatMicrosoft.
‡Correspondingauthor.
Authors’ContactInformation:JuntingLu,PekingUniversity,China;ZhiyangZhang,NanjingUniversity,China;FangkaiYang,Microsoft,China;Jue
Zhang,Microsoft,China;LuWang,Microsoft,China;ChaoDu,Microsoft,China;QingweiLin,Microsoft,China;SaravanRajmohan,Microsoft,USA;
DongmeiZhang,Microsoft,China;QiZhang,Microsoft,China.
ManuscriptsubmittedtoACM 1
4202
peS
52
]IA.sc[
1v04171.9042:viXra2 Luetal.
Multimodallargelanguagemodels(MLLMs)haveenabledLLM-basedagentstodirectlyinteractwithapplicationuserinterfaces(UIs),
enhancingagents’performanceincomplextasks.However,theseagentsoftensufferfromhighlatencyandlowreliabilityduetothe
extensivesequentialUIinteractions.Toaddressthisissue,weproposeAXIS,anovelLLM-basedagentsframeworkprioritizeactions
throughapplicationprogramminginterfaces(APIs)overUIactions.ThisframeworkalsofacilitatesthecreationandexpansionofAPIs
throughautomatedexplorationofapplications.OurexperimentsonOfficeWorddemonstratethatAXISreducestaskcompletiontime
by65%-70%andcognitiveworkloadby38%-53%,whilemaintainingaccuracyof97%-98%comparetohumans.Ourworkcontributesto
anewhuman-agent-computerinteraction(HACI)frameworkandafreshUIdesignprincipleforapplicationprovidersintheeraof
LLMs.Italsoexploresthepossibilityofturningeveryapplicationsintoagents,pavingthewaytowardsanagent-centricoperating
system(AgentOS).
CCSConcepts:•Human-centeredcomputing→Naturallanguageinterfaces;•Computingmethodologies→Planningand
scheduling.
AdditionalKeyWordsandPhrases:largelanguagemodels(LLMs),human-agent-computerinteraction,LLM-basedagent,task
completion,userinterface(UI)
1 Introduction
Aspersonalcomputer,mobiledevices,andinternetbecomeanindispensablepartofeveryday’sworkandlife,application
industriesareundergreatpressuretorapidlyevolvetheirsoftwareapplicationswithmorefeaturesandfunctionalities
tomeetpeoples’growingdemand[1,38].Nonetheless,thosenewapplicationsalsodemandamuchhigherinvestment
inbothtimeandcognitiveeffortfromregularusers.Tolearnhowtouseanewapplicationeffectively,Usersgenerally
havetofirstlyspendsignificanttimejusttogetfamiliarwiththeuserinterface(UI)andcorrespondingfunctionalities.
Andtocompletevarioustaskswiththenewapplicationefficiently,usersneedtofurtherinvesttimeandefforttolearn
howtobreakcomplextasksintostepsandusetherightUIsandcommandstocompleteeachstep.Whileboththe
applicationprovidersandresearchcommunityarefullyawareofthispainpoint,existingeffortshavebeenfocusingon
providingdetailedtutorialsandestablishingengaginglearningplatforms,whichcouldonlyprovidelimitedsupportin
alleviatingusers’cognitiveburden[6,10,35,42].
Largelanguagemodels(LLMs)[2,13,34]hasdemonstratednear-humancapabilitiesinreasoning,planning,and
collaborationandarehighlypromisingincompletingcomplextasks[22,27,46].Sincethen,researchershasbeen
exploringhowLLMscanbeutilizedtoreduceusers’cognitiveburdeninlearningandoperatingsoftwareapplications.
Inparticular,multimodallargelanguagemodels(MLLMs)[14,53,56]expandtheusagescenariosofLLMstovarious
tasksthatrequirevisioncapability[47,60].Recentworks[43,54,55,60]utilizesMLLMstodesignLLM-basedUIagents
capableofservingasusers’delegates,translatinguserrequestsexpressedinnaturallanguage,anddirectlyinteracting
withtheUIofsoftwareapplicationstofulfillusers’needs.WiththehelpofLLM-basedUIagents,userscouldsimply
asktheapplicationtocompletetaskswithoutadeepunderstandingofapplication’sUIsandfunctionalities,which
significantlyreducesusers’cognitiveloadoflearningnewapplications.
However,justlikethetransitionfromsteam-poweredtoelectric-poweredindustrytookmuchmorethanreplacing
centralsteamengineswithelectricmotorsinthefactories,simplybuildingLLM-basedagentupontheUIsofapplications
cannotmagicallydeliverasatisfiedandworry-freeuserexperience.Inparticular,today’sapplicationUIsaredesigned
forhuman-computerinteraction(HCI)[7,25],whichofteninvolvesmultipleUIinteractionsforcompletingasingle
task.Forinstance,insertinga2×2tableinanOfficeWorddocumentrequiresasequenceofUIinteractions:“Insert
→Table→2×2Table”.AlthoughtheHCI-baseddesignsuitsthehabitsofhumans,trainingLLM-basedUIagentsto
emulatesuchinteractionswouldgeneratequiteafewchallengesthataredifficulttoovercome.
ManuscriptsubmittedtoACMTurnEveryApplicationintoanAgent:TowardsEfficientHuman-Agent-ComputerInteractionwithAPI-First
LLM-BasedAgents 3
ThefirstchallengefortheLLM-basedUIagentisthehighlatencyandlongresponsetime.EachindividualUI
interactionsteprequiresoneLLMcalltoreasonwhichUItointeractwith.AtaskinvolvesmultipleUIinteractionsteps
canthusincurconsiderabletimeandmonetarycost.TheLLMcalllatencyisalsopositivelycorrelatedwiththenumber
ofprocessedtokens[15,24,45].ToensurethattheLLMcanreturnhighqualityoutputs,theLLM-basedUIagentmust
passlargevolumeofUIinformationtopreciselydescribethecurrentstate,whichalsoincreasesthelatencyineach
call.Thesecondchallengeliesinthereliabilitydomain.StudieshaveshownthatLLMsarepronetohallucinationsin
generatingresponses[5,11,18,57].DuringthelongsequentialcallswithLLM-basedUIagents,thechanceoftakinga
wrongUIcontrolorhallucinatinganon-existingUIforinteractionincreaseswitheachreasoningstep.AsLLM-based
UIagentsoftenpassthepreviousinteractionhistoryasadditionalcontextwhenreasoningoncurrentUIinteraction
step[54],hallucinationsinearlierstepscouldalsoincreasethechanceofhallucinationinlatersteps.Thus,whenalong
chainofUIinteractionsisrequired,theUIagentsaremorelikelytosufferfromcompoundingerrorsandencounter
taskfailure[8,59].Lastly,theLLM-basedUIagentalsofacesthechallengeofUIgeneralization.Whilerecentworks
hadmadeadvancementsinUIgrounding[4,9,37],howtheLLM-basedUIagentshandleinteractionswithapplications
whoseUIsarenotincludedinthepretrainingstageofLLMsremainacriticalobstaclewithoutgoodsolutions.
Webelievethatanewhuman-agent-computerinteraction(HACI)paradigmisneededtoaddressthechallengesfaced
byLLM-basedUIagents.InHACIparadigm,API-firstLLM-basedagentswillreplaceUIagentstoprioritizeAPIcalls
overunnecessarymulti-stepUIinteractionsintaskcompletion.RegularUIinteractionsareonlycalledwhentherelated
APIsareunavailable.ComparedtotheUIagents,API-firstagentsrequirelesstokensandcanobtainmoreaccurate
code-formatedresponsesfromLLMs.Forinstance,inthetaskofinsertinga2×2tableinWorddocument,theAPIcall
onlyrequiresonelineofcode,i.e.,doc.Tables.Add(NumRows=2,NumColumns=2)tocompletethetask.
Inthispaper,weproposeAXIS:AgenteXploringAPIforSkillintegration,aself-explorationLLM-basedframework
capableofautomaticallyexploringexistingapplications,learninginsightsfromthesupportdocumentsandaction
trajectories,andconstructingnewAPIs1basedontheexistingAPIstoempowerAPI-firstLLM-basedagentswithlow
latencyandhighreliability.BasedonourexperimentonOfficeWord[30]tasks,AXIScansignificantlyimprovethe
taskcompletionrateandreducethecognitiveloadsofusers.Moreover,AXISprovidesapracticalapproachforthe
applicationproviderstoturnanapplicationintoagentbysimplywrappinguptheapplicationwithanAPIsetand
adoptingasimplerUIdesignsuitableforHACI.Thisapplication-as-an-agentparadigmalsopavesthewaytowardsthe
agentoperatingsystem(AgentOS)[28,48,58],wheretheusersonlyneedtocommunicatetheirintentioninnatural
languageandtheAgentOSthenautomaticallyformsactionableplans,distributesthesub-taskstorelevantapplications,
andoverseestheexecutionandcompletionofthetaskswithminimalinterventionsfromusers.
Ourworkmakesthefollowingcontributions:
• WeproposeanHACIparadigmalongwiththeimplementationframeworknamedAXISforcreatingAPI-first
LLM-basedagentscapableofexploringtheapplicationanditsavailableAPIsandconstructingnewAPIs.This
newparadigmprovidesapracticalapproachtoturneveryapplicationintoanagentandpavesthewayof
developingarealAgentOS.
• Weaddressthecognitiveloadandlearningeffortchallengebyreducingunnecessarymulti-stepUIinteractions
andsimplifyingtaskcompletionthroughAPIcalls.
• Weconductperformanceevaluationsandextensiveuserstudytofullyexaminetheefficiencyandreliabilityof
AXIS.
1ThenewAPIsarealsoreferredas“Skills”inSection3,andweusetheterm“API”looselyheretodifferentiatefromtheUI.
ManuscriptsubmittedtoACM4 Luetal.
2 RelatedWork
2.1 LLM-basedUIAgent
LLM-basedagentsaredesignedtoutilizetheadvancedcontextunderstandingandreasoningskillsofLLMstointeract
withandmanipulateenvironmentswithhuman-likecognitiveabilities[16,26,44,49].TheadventofMLLM[14,53,56],
includingGPT-4o[32]andGemini[41],furtherbroadensthescopeofLLMsinpracticalapplicationswiththecapability
ofprocessingmulti-modalinputs,includingtextandimages.SupportedbythevisionunderstandingcapabilityofMLLM,
newLLM-basedUIagentscanacquirecrucialabilitiesofnavigatingandcontrollingUIsinsoftwareapplicationsfor
completingcomplextasks.Assuch,studyonLLM-basedUIagentshasemergedasahotareafordevelopingintelligent
assistantsthatcanautomaticallyinteractwithapplicationsfollowingusers’commands.Inthemobileplatform,methods
such as MM-Navigator [52], AppAgent [55], and MobileAgent [43] leverage GPT-4V [33] to operate smartphone
applicationsthroughhuman-likeinteractions(tappingandswiping)withoutrequiringback-endaccess.Morebroadly,
UFO[54],SeeAct[60]andCradle[40]supportthenavigationandoperationofUIsinWindowsOSapplications,websites
andgamesrespectivelyfollowingcommandsinnaturallanguage.OthernotableexamplesincludeCogAgent[20]
andSeeClick[9],whichfocusonthetrainingandfinetuningofvisuallanguagemodelsforUIunderstandingand
navigationindownstreammobileanddesktoptasks.WhiletheseLLM-basedUIagentsaretrainedtocompletetasksin
ahuman-likemanner,UIsinexistingapplicationswereoriginallydesignedforHCIsratherthanforagent-computer
interactions.Consequently,emulatingUI-basedinteractionsdirectlycanresultinunnecessarytimecosts,especiallyin
complextasksthatrequirenumerousorrepeatedUIinteractionsteps,suchaschangingmultipletitlestothesame
format.Incontrast,applicationprovidersusuallyofferAPIsthatcanaccomplishsuchtaskswithasingleAPIcallthat
eliminatetheneedofmulti-stepUIinteractions.ToovercomethelimitationofexistingUIagents,reduceunnecessary
UIinteractions,andlowerhumans’learningcurveforapplications,wewillstudyhowtheapplicationAPIscanbe
leveragedforbuildingLLM-basedagentsandexplorethenewdesignprinciplesofUIsintheeraofLLMs.
2.2 AgentOperatingSystem
TheLLM-basedagentsdiscussedinSection2.1areusuallydesignedtoworkwithinanarrowenvironmentsuch
asaspecificapplicationorwebpagewhichlimitstheirapplicabilityingeneralcomputertasksthatofteninvolves
cross-applicationcollaboration.Forexample,asimpletask“CreateareportwithdocsunderthePresentationfolderand
sendittoJack.”requiresmultiplestepswithmultipleapplicationstocomplete:readalldocs,summarizekeycontexts,
composeareportwithawordprocessingapplicationlikeMicrosoftWord,anddraftanemailwithattachedreport
usinganemailclientlikeOutlook,andsentittotherecipientJack.Tosupportthecompletionofcomplextaskswith
minimalhumaninterventions,emergingworkshaveexploredthepossibilityofdevelopinganoperatingsystem(OS)
fullysupportedbyLLMs.WorkslikeAIOS[28]andOS-Copilot[48]proposeanOS-levelagentthatcaneffectively
interactwiththeOSandavastnumberofthird-partyapplicationsincompletingcomplextasks.OSWorld[51]and
AndroidWorld[36]alsoprovidebenchmarksforevaluatingtheperformanceofmultimodalagentswithdiversetasks
andcross-applicationworkflowsacrossvariousOS.Intheindustry,commercialAgentOSsuchasAppleIntelligence[3],
Copilot+PC[29],HarmonyOS[23],andMagicOS[21],areevolvingtobemoreaccessibleandproductiveforcustomers
withthepotentialofleadinganeweraofHCI.AcommonapproachadoptedbyexistingAgentOSistodividecomplex
intosub-tasksandassignthemtoindividualapplications.However,foreachsub-task,theLLM-basedagentsstillrelyon
human-likeinteractionssuchasUIclickingandswipingforcompletion,whichcanbeinefficientcomparedtoAPIcalls.
ManuscriptsubmittedtoACMTurnEveryApplicationintoanAgent:TowardsEfficientHuman-Agent-ComputerInteractionwithAPI-First
LLM-BasedAgents 5
Moreover,whentheLLM-basedUIisprocessingthetask,thecontrolistakenoverfromtheuserbytheLLM-based
agent.
2.3 UIdesigninLLMera
UIdesignisanessentialpartofHCIandrequireshighlyspecializedexpertisesalongwithiterativeroundsoffeedback
andrevision[39].WithLLMs,UIdesigncanbefurtherempoweredwithautomatedproceduresofdesign,feedbackand
evaluation.Duanetal.[12]useLLM-generatedfeedbacktoautomaticlyevcuateUImockups.Similiarly,SimUser[50]
leverageLLMstosimulateuserswithdifferentcharacterstogeneratefeedbackonusabilityandprovideinsightsinUI
design.MUD[17]utilizeLLMstomimichuman-likeexplorationtomineUIdatafromapplicationsandemploysnoise
filteringtoimprovequalityofUIdata.Still,existingUIdesignslargelyfollowthetraditionalhuman-machineinteraction
paradigmratherthanthehuman-agent-computerinteraction(HACI)paradigmthatcouldbecamethecentraldesign
principleinAgentOS.Inthispaper,wewillleverageAXISframeworktoexploreapplications,identifytheessentialUIs,
andexaminewhichpartofUIscanbereplacedbyAPIscallablebyLLM-basedagents.Wehopethatourexploration
wouldgenerateinsightsonhowtodesignUIsthataremoreeffectiveundertheHACIparadigm.
3 DesignofAXIS
WedevelopeAXISasaframeworkthatcanautomaticallyexplorewithinexistingapplications,learninsightsfrom
explorationtrajectories,andconsolidateavailableinsightsandlearnedknowledgeintoactionable“skills”.Skillisa
high-levelrepresentationofUI-andAPI-basedactionswithapriorityinAPIactions2,whichisgeneratedwithAXIS
explorationwithintheapplication.IllustratedbyFigure2,AXISsystemconsistsofthreemajormodules:theapp
environment,theskills,andtheworkflows.AXISemploysnumerousLLM-basedagentstoexploretheappenvironment,
usuallyasetofapplicationsrunningontheOS,throughaunifiedinterfacetoobtainthestateoftheenvironmentand
tointeractwithit.Theknowledgelearntduringthisprocesswillbeconsolidatedintoskillscontainingstructuredcode
segmentscapableofaccomplishingvarioustaskswithintheenvironment.Specificexecutionandvalidationmethods
willalsobedesignedtoimprovetheperformanceofthoseskills.Finally,weestablishtwoworkflows:theexplorer
workflowandthefollowerworkflowtofacilitatethelearningofskillsfromtheenvironment.
Inthesubsequentsections,wewilllayoutthedetailsofallthe3modulesanddiscusshowtheyworktogetherto
exploreanddiscovervaluableskillsfromtheapplications.
3.1 EnvironmentofApplication
In the context of AXIS, the environment of applications refers to the collection of interactive entities within the
explorationscopeoftheagents.Inthispaper,thoseentitiesmainlyconsistofasetofapplicationsrunningonthe
Windowsoperatingsystem.Applicationsintheenvironmentoftensharecommonelements,suchascontrols[54]and
XMLelementsobtainedafterunpacking.
AgentsinAXISnotonlyobservethestateoftheenvironment,butalsoactivelyinteractwiththeenvironment.To
facilitatetheobservationandinteractionbetweenagentsandtheenvironment,wehavedesignedtwogeneralinterfaces:
state()andstep().state()interfacereturnsthestateoftheenvironment,whichincludesdetailedinformationon
thecurrentelementsoftheentitieswithintheenvironment.TheenvironmentstateencompasseskeyUIinformation
includingthepositionofcontrols,thetypeofcontrols,andwhetheracontrolisselected.Forapplicationsthatcan
2IftheskillcanberepresentedwithUIorAPIactions,theskillisrepresentedinAPI-onlyactions.
ManuscriptsubmittedtoACM6 Luetal.
Fig.2. OverviewofAXISframework.AXISfirstexploreskillsbyFollower-drivenorExplorer-drivenmode,thentheexplorationlogs
willbeusedtogenerateskill,duringwhichtheskillcodewouldbetranslatedandvalidated.Thedashedboxesrefertotheinteraction
betweenagentsandapplicationenvironment.
beunpacked,theunpackedXMLcontentisalsoincludedasapartofthestate.Thecontroltypesintheapplication
environmentofAXISareconsistentwiththosein[54].step()interfaceincorporatesaskillexecutorthatallowsagents
toperformoperationswithintheenvironmentbyexecutingskills.Uponcompletion,thisinterfacealsoreturnsthe
resultsoftheseoperations.
3.2 SkillsinApplication
Askillcontainsskillcode,description,andusageexample,andisdesignedtoaccomplishonespecifiedtaskwithinthe
environment.
3.2.1 skillstructure.
• SkillCode:apieceofcodestructuredtobecompatiblewiththeexecutordescribedinthefollowingsection.Skill
codeincludesauniformsetofparametersandadherestothestandardPEP257documentation.Theinitialset
ofskillsisgeneratedbytherestructureofthefundamentalAPIsfromtheapplicationprovider.Basedonthese
initialskills,AXIScanexploreanddevelopadditionalnewskills.ItisworthnotingthatwhileAXISprioritizes
API-basedskills,itisstilldesignedforgeneralpurposeandcanincoporatebothUI-basedandAPI-basedskill
code.
• Description:AdescriptionofthefunctionalityofaskillforassistingtheLLMinselectingandinvokingthe
appropriateskilintheprocessoftaskexecution.
ManuscriptsubmittedtoACMTurnEveryApplicationintoanAgent:TowardsEfficientHuman-Agent-ComputerInteractionwithAPI-First
LLM-BasedAgents 7
• UsageExample:Oneormorecodeexamplesincludinganyspecificparameterstypicallyassociatedwiththecode
andthedescription.TheseexamplescanassisttheLLMinfillingouttheparameterfieldsinthecorrectformat
whentheskillisinvoked.
3.2.2 skill executor. As discussed in Section 3.1, our application environment incorporates a step() interface to
facilitatetheinteractionbetweenagentsandtheenvironment.Thisinterfacealsohoststheskillexecutorresponsible
forexecutingtheskillgeneratedorselectedbytheagents.Theskillexecutorkeepscachingofapplicationdocuments
andsimultaneouslysupportsmultiplefunctionalitiesincludinglocatingapplicationcontrols,invokingmethodson
thosecontrols,andcallingapplicationAPIs(independentofcontrols),toenabletheUIactionsandAPIactionsinthe
sametimeandserveasanefficientfoundationforskill-drivenoperations.
3.2.3 skilltypes. Followingaversatiledesignprinciple,theskillsinAXIScanbecategorizedintofourtypesbasedon
thecompositionoftheircodefragments:AtomicUISkill,AtomicAPISkill,CompositeUISkill,CompositeAPISkill,
andAPI-UIHybridSkill.
Table1. Comparisonof4typesofskill.
Type Description Example Featurecoverage
AtomicUIskill ComposedofonebasicUIaction.Asthemost click_input click on different UI
primitiveskills,AtomicUIskillsarestackedand controls
transformedduringtheexplorationprocessto
formnewskills.
AtomicAPIskill ComposedofonebasicAPIactions.UnlikeUI select_text select text content in
actionsthatdependonUIcontrolsforexecution, thecanvas.
APIactionscanbeexecutedwithouttheneed
ofinteractingwithanyUIelements.
CompositeUIskill ComposedofmultipleatomicUIactionsorcom- search_for_help clickingthesearchbox
positeUIactions.CompositeUIskillareformed andtheneditingtext.
byasimplestackingandcombinationofUIac-
tions.
CompositeAPIskill Composed of multiple atomic API actions or insert_header_footer insert header and
compositeAPIactions.Thistypeofskillsoften footer with specified
representsahigher-levelcombinationoffunc- contentsbyAPI,which
tions. is equal to sequential
UI actions "Insert-
>Header->footer
edit->Footer->footer
edit".
API-UIhybridskill ComposedofbothAPIactionsandUIactions. format_text_in_word combineselect_textand
API-UIhybridskillssometimesappearasinter- aseriesofUIactionsre-
mediatestatesduringskillexplorationandmay latedtotextstyling.
evolveintopureAPIactionsduringthelater
stageofexploration.
3.2.4 SkillHierarchy. Wedefine“skillhierarchy”asthenumberofskillcomponentsthatmakeupaskill.Asingle
basicskillthushasaskillhierarchyof1.Theskillhierarchyofinsert_header_footerskillmentionedintable1is2.
ManuscriptsubmittedtoACM8 Luetal.
3.3 WorkflowsofSkillExploration
AsshowninFigure2,theskillexplorationinAXISisguidedbytwodrivingmechanisms:follower-drivenskillexploration
andexplorer-drivenskillexploration.
3.3.1 Follower-drivenskillexploration. Follower-drivenskillexplorationreferstotheprocessofexploringskillsfrom
theapplications’helpdocuments,whichisprimarilyaccomplishedthroughthecollaborationofthefollowingagents:
• FollowerAgent.FollowerAgentutilizesaskilllibrarycomposedofasetofprimitiveactionsbasedonthestep-to-
stepinstructionsprovidedbythehelpdocument.Ateachstep,itselectsthemostappropriateactionaccording
tothecurrentstateoftheenvironment.
• MonitorAgent.MonitorAgentmonitorseveryactiontakenbytheFollowerAgentalongwiththeimpactsonthe
environment.ItalsotrackstheentiretrajectoryoftheFollowerAgent.Whendeemedappropriate,itplacesa
breakpointandsummarizestheobservedtrajectoryintoacompleteskillincludingthefunctionalsummaryof
theskillanditslogic.ForinitialskillscomposedofbasicUIactions,thesummarizedlogicisalsooftenclosely
tiedtotheUI.
• SkillGeneratorAgent.BasedonthesummariesandlogicaldescriptionsprovidedbytheMonitorAgent,Skill-
GeneratorAgentgeneratesthecode,functionaldescriptions,andusecasesfortheskillinaccordancewiththe
specificationsoutlinedinSection3.2.Thegeneratedskillcodeishighlycorrelatedwiththelogicaldescriptions
generatedbytheMonitorAgent,andthereforeofteninvolvesthestackingofbasicUIactions.
• APITranslatorAgent.BasedontheskillcodegeneratedbytheSkillGeneratorAgent,APITranslatorAgentqueries
therelevantAPIdocumentationtotranslatetheUI-basedactionsintheskillcodeintoAPIcalls,thuscompleting
theAPI-ificationofthecode.Finally,thegeneratedskillsarefurthervalidatedthroughaskillvalidationprocess.
ItisworthnotingthatboththeSkillGeneratorAgentandtheAPITranslatorAgentsearchforreusableskills
fromtheoriginalskillsetduringtheskillcodegeneration,therebyobtainingskillsatdifferenthierarchies.This
approachefficientlyfacilitatestheconstructionofskills.
3.3.2 Explorer-drivenskillexploration. UnlikeFollower-drivenskillexploration,theexplorer-drivenskillexploration
kicksofftheexplorationprocesswithadifferentinitializationmethod:thestep-to-stepinstructionsareautomatically
generatedratherthanextractingfromthehelpdocument.Duringeachstep,explorerproposesthenextactionbasedon
thecurrentenvironmentinformationandthehistoryofpreviouslyexploredsteps.Thesubsequentstepsutilizethe
sameagentsinthefollower-drivenmodetogenerateskillswithexploredtrajectories.Inthisprocess,applicationseed
fileswithvaryingpre-filledcontentareoftenrequiredtoobtaindifferentinitialenvironmentsfordiscoveringamore
diverserangeofskills.
3.3.3 SkillValidation. Tovalidatethenewskillsgeneratedfromexploration,wehaveimplementedtwoverification
methods:staticvalidationanddynamicvalidation.
• StaticValidation:Thisvalidationmethodutilizesstructuralmethodtoverifytheskillcode,includingchecking
whether the skill’s parameters contain the mandatory parameters (such as the executor instance and args
list),whetherthemethodsandpropertiesoftheexecutorarecorrectlyinvokedinthecode,andwhetherany
non-existentskillsareimportedwhenreusingtheskill.
• DynamicValidation:Thisvalidationmethodevaluatesaskill’sperformanceintheappenvironmentwiththe
helpoftwoagents:ValidatorAgentandEvaluateAgent.Whenaskillissubmittedforvalidation,ValidatorAgent
ManuscriptsubmittedtoACMTurnEveryApplicationintoanAgent:TowardsEfficientHuman-Agent-ComputerInteractionwithAPI-First
LLM-BasedAgents 9
firstlyproposesataskbasedontheskill’sfunctionaldescription.Itthenexecutestheskillwithinthesameapp
environmentusedduringtheinitialskillexplorationandrecordstheresponsesandenvironmentalchangesduring
execution.Uponthecompletion,theEvaluateAgentdetermineswhethertheskillhassuccessfullycompletedthe
taskornot.
4 FeasibilityStudy
TovalidatetheusabilityandeffectivenessofAXISframework,weconductedafeasibilitystudy.WefirstuseAXIS
toexploreMicrosoftOfficeWordanddiscover73skills.Thenweextracted50tasksfromthewikihow3page"Use
MicrosoftWord"andtheofficialMicrosoftWordwebsite4.ThesetaskswereexecutedusingbothAXISandUIAgent,
andtheresultswereanalyzedandcompared.
4.1 SKillExploration
Beforetheexploration,AXISwasprovidedwith6basicactions,asshowninTable2.Then,347seedfileswereusedfor
theAXIStoexplore.Aftertheexploration,AXISdiscovered73skillswithdifferenthierarchies.Majorityoftheskills
(44)discoveredhaveaskillhierarchy1.Therestiscomposedof24skillswithhierarchy2,3skillswithhierarchy3,and
2skillswithhierarchy4.Table3displaysseveralsuccessfullyvalidatedskillsdiscoveredduringtheexplorationprocess.
4.2 TaskCompletion
The50Word-relatedtaskscollectedabovewereusedtotestandcomparetheperformanceofUIAgentandAXISwith
theexploredskills.Inourexperiment,wechooseUFO[54]astherepresentativeofUIAgentconsideringitsgood
performanceonwordtasks.TheresultsarepresentedinTable4,whichincludestheaveragetimetakentocomplete
differenttasks,thesuccessrates,theaveragenumberofstepspertask,andthecorrespondingcostsofLLMbackend
(GPT-4o,version20240513)forbothagents.
Intermsofexecutiontime,AXISsignificantlyoutperformstheUIAgent,withanaveragetaskcompletiontimeof
29.9secondscomparedto59.5secondsfortheUIAgent.ThisresultshowsthatAXISisnearlytwiceasfastasthe
UIAgent.AXISalsoachievesahighersuccessrateincompletingthetasks.Moreover,thankstotheabstractionand
integrationofbasicactionsintohigher-levelskills,AXIScouldcompletetaskswithfewerstepsonaverage,whichalso
incurslowercostscomparedtotheUIAgentUFO.
TobetterunderstandthereasonsbehindAXIS’shigherefficiency,weanalyzedthenumberofUIandAPI-type
actionsinvokedbyAXISandtheUIAgentduringtaskexecutionandrecordedtheproportionofAPIandAdvanced
API(definedasskillswithahierarchylevelof2orhigher)callsmadebybothmethods.AsshowninTable5,AXIS
invokedsignificantlyfewerUIactionscomparedtotheUIAgentduringtaskexecution.Notably,thetotalnumberofUI
actionsperformedbyAXISacrossalltaskswasgreaterthanthenumberofinvokedAPIactions.Uponinspection,this
wasfoundthatAXIStendstouseasingleandintegratedAPIskilltocompleteawholetask,resultingalowoverallAPI
actionscount.WefurthercalculatedtheAPIusagerateandtheproportionofAdvancedAPIusageamongtheAPI
actions.ThedatashowsthatAXIS’sproportionofAPIactionsreached55.7%witha23.1%usagerateofadvancedAPI.
Incontrast,theAPIusagerateofUIAgentisonly8.1%.Basedontheaboveresults,weconcludethatAXISindeed
adoptsanAPI-firstapproachandtendstouseskillstocompletetaskswhenthematchingskillsareavailable.Andthe
integrationofskillsintoactionsalsosignificantlycontributestotheincreasedefficiencyofAXIS.
3https://www.wikihow.com/Use-Microsoft-Word
4https://support.microsoft.com/en-us/word
ManuscriptsubmittedtoACM10 Luetal.
Table2. ThebasicactionssupportedforAXISexploration.
Name Description Example
set_edit_text ThefunctiontoSetthe set_edit_text(executor,
edittextofthecontrol args_dict="control_id":’119’,
element,canusetoin- "control_name":"Edit",
putcontentonEdittype ’text’:"hithere")
controls.
select_text Afunctiontoselectthe select_text(executor,
textwiththespecified args_dict="text":"hello")
textcontent.
select_table Afunctiontoselectthe select_table(executor,
tablewiththespecified args_dict="number":1)
number.
type_keys A function to Type type_keys(executor,
in keys on control args_dict="control_id":’119’,
item.Used to enter "control_name":"Edit",
shortcutsandsoon. "text": "VK_CONTROL
down","newline":False)
click_input AfunctiontoClickthe click_input(executor,
controlelement.Usually args_dict="control_id":"12",
beusedtoswitchtodif- "con-
ferent ribbon,click the trol_name":"Border",
buttonsinmenu. ’but-
ton’:"left",’double’:False)
wheel_mouse_input A function for Wheel wheel_mouse_input(executor,
mouseinputonthecon- args_dict="control_id":"12",
trolelement. ’wheel_dist’:-20)
5 UserStudy
WecarryoutextensiveuserexperimenttoevaluatetheperformanceofAXIS.Theexperimentandtheassociated
evaluationmetricsweredesignedtoexplorethefollowingresearchquestions(RQs)regardingtherolesofLLM-based
agentsinworkandlifescenarios:
• RQ1:DoesLLM-basedagentlowerthecognitiveloadoftheusersandmakethemhavelessefforttolearn?
• RQ2:DoesLLM-basedagentenhancetheefficiencyofusers?
• RQ3:WhatarethedifferencesbetweenaUIAgentandanAPI-basedAgentinuserexperience?
Inouruserexperiment,participantswereaskedtocompletespecifiedtaskswithinanapplicationthroughthree
methods:manually,withtheassistanceofaUIAgent,andwiththeassistanceofAXISandrecordedtheentireprocess.
MicrosoftWordischosenastheexperimentalapplicationconsideringitspopularityinourdailyworkandlifeaswell
astherichAPIdocumentations[31]).MotivatedbytheRQs,wesetthreeobjectivesfortheuserexperiment:
(1)Toevaluatethecognitiveloadonparticipantswhencompletingtasksusingdifferentmethods.(2)Tocomparethe
efficiencyandreliabilityoftaskcompletionacrossthethreemethods.(3)Toassessuserpreferencesregardingtheuse
ofdifferentAgents.ThisstudyisapprovedbytheInstitutionalReviewBoard(IRB)ofPekingUniversity.
ManuscriptsubmittedtoACMTurnEveryApplicationintoanAgent:TowardsEfficientHuman-Agent-ComputerInteractionwithAPI-First
LLM-BasedAgents 11
Table3. SamplesofskillsindifferenthierarchyexploredbyAXIS.
Hierarchy Name Description Example
1 activate_dictation The function is to ac- activate_dictation(executor)
tivate dictation in Mi-
crosoftWord.Itisequal
totheDictatebuttonin
theVoicegrouptostart
dictation.
2 align_text The function aligns align_text(executor,
thetextinaMicrosoft args_dict="text":"hello",
Worddocument.Itfirst "alignment":"center")
selects the text, then
applies the desired
alignment (left, center,
right,justify)usingthe
WordAPI.
3 apply_text_style Afunctiontoeditatext apply_text_style(executor,
withspecifiedtext,font args_dict="text":"Hello",
size,fontname.Theti- "font_name":"Arial",
tleissetinthecenter. "font_size":13)
Table4. ComparisonoftheperformanceofUIAgentandAXISon50tasks.
Metric UIAgent AXIS PairwiseSignificance
Time(s) 59.5 29.9 u>a(p<0.001)
SuccessRate(%) 52.0 84.0 u<a(p<0.001)
Steps 3.2 2.0 u>a(p<0.01)
Cost($) 0.4 0.2 u>a(p<0.001)
Table5. ComparisonofhitUIactionsandAPIactionsofUIAgentandAXISon50tasks.
Metric UIAgent AXIS
TotalUIactions 103 48
TotalAPIactions 9 39
APIusagerate(%) 8.1 55.7
AdvancedAPIusagerate(%) - 23.1
5.1 ExperimentProcedure
Theentireuserexperimentlastedfor30minutes.Duringthepreparationphase,wesampledfivedifferenttasksin
MicrosoftWordfrombothofficialWorddocumentationandGPT-generatedresults.Thosetaskswerecategorizedinto
twolevelsofdifficulty:lowdifficulty(L1)andhighdifficulty(L2),basedonfactorssuchasthenumberofUIinteractions
required,thedepthoftheUIfunctions,andthenumberofribbonswitches.Ourexperimentalresultsalsoconfirmed
thattasksinL2areindeedmoredifficultthantasksinL1.Inthesubsequentdiscussion,wewillsimplyreferstasksin
differentcategoriesasL1tasksandL2tasks.Additionally,wedesignedauserinformationformtocollectparticipants’
backgroundinformation,includingtheirfamiliaritywithMicrosoftWord.
ManuscriptsubmittedtoACM12 Luetal.
Weprovideduserswithasimplewebinterfaceduringtheformalexperiment,whichconsistedoftwostages.In
Stage1,participantsreceivedapre-printedtasklistincludingbothL1andL2tasks.BasedonthetaskIDdisplayedon
thewebpage,participantswereaskedtoreadthetaskrequirements,clickthe"start"button,completethetaskinthe
automaticallyopenedWorddocument,andclick"Finish."upontaskcompletion.InStage2,participantswereinstructed
touseboththeUIAgentandAXIStoassistthemincompletingtheWordtasks.Thecorrespondingwebpagewere
featuredwithbothinputfieldsandbuttonsforactivatingthetwoAgents.Theparticipantsneedtoentertaskdescription
tocommandtheAgentstocompletethetasks.Throughouttheformalexperiment,alltaskexecutionprocesseswere
recordedforsubsequentanalysis.Aftercompletingallassignedtasksmanuallyorwiththeassistantofagents,four
differentpost-taskquestionnairesweredisplayedontheexperimentalwebpagetosurveyusers’subjectiveexperiences.
5.2 Participants
Werecruitedcandidatesbypostingonsocialmedia.20individualswererandomlyselectedasparticipantsforthe
experimentfromthelistofcandidateswhoconfirmedtheirwillingnesstoparticipate.Ourparticipantsrangedin
agefrom18to40yearswitheducationalbackgroundsspanningfromundergraduatetopostgraduatelevels.Their
occupationsincludedengineers,students,researchers,andfull-timehomemakers,amongothers.100%oftheparticipants
hadsomeexperiencewithMicrosoftWordwithvaryinglevelsofproficiencyanddifferentusagefrequencyranging
fromdailytomonthly.Theuserexperimentlasted30minutesonaverageperparticipantandeachparticipantreceived
50CNYascompensation.
5.3 SubjectiveMetricCollection
Asmentionedin5.1,weusedfourquestionnairestocollectusers’subjectiveexperenceforcompletingthetasksusing
differentmethods.Questionnaires1to4wereadministeredseparatelyaftercompletingL1tasksmanually,completing
L2tasksmanually,completingL1taskswiththeassistanceofanAgent,andcompletingL2taskswiththeassistanceof
anAgent,respectively.
TodeterminewhetherLLM-basedagentscanindeedreduceusers’cognitiveloadforcompletingtaskscomparedto
manualwork,inallthefourquestionnaires,weincludequestionsbasedontheNASATaskLoadIndex(NASA-TLX)[19]
andanadditionalquestionontherequiredlearningefforts.ForNASA-TLX,ourquestionscoverallthesixmetrics,
includingMentalDemand,PhysicalDemand,TemporalDemand(howhurriedorrushedofthetasks),performance
(feelingofsuccessincompletingthetask),frustrationlevel,andcompletioneffort(howhardtheusersneedtoworkon
thetasks).Lowervaluesinthosesixmetricsindicatehighercognitiveloads,betterfeelingofsuccess,lowerfrustration
level,andlesseffortsduringtaskcompletion.Forthelearningefforts,lowerscorealsoindicateslesslearningefforts.
Tofurthercompareusers’perceptionsontheUIAgentandAXIS,Questionnaires3and4containedquestionsonthe
ratingsonfluency,reliability,UIdependency,decisionconsistency,andperceivedspeedforbothAgents.Specifically,
UIdependencymeasuresthedegreeofusers’relianceontheUIwhileobservingtheAgentcompletetasks.Decision
consistencyassesseshowcloselythedecisionsmadebytheAgentalignwiththedecisionsusersmightmaketocomplete
thesametasksmanually(theexperimentalwebpagedisplayedallthedecisionsmadebytheAgentsinastep-by-step
manner).Perceivedspeedreferstousers’subjectiveperceptiononhowfasttheAgentscompletedthetasks.
5.4 ObjectiveMetricCollection
Werecordedexperimentallogsthroughouttheexperiment,includingscreenrecordingsofthemanualcompletion
oftasksbytheusers,screenrecordingsofthetwoAgents(UIagentandAIXS)performingtasks,decision-making
ManuscriptsubmittedtoACMTurnEveryApplicationintoanAgent:TowardsEfficientHuman-Agent-ComputerInteractionwithAPI-First
LLM-BasedAgents 13
processes,UIinteractionpaths,timetaken,andthecostoftheLLMbackend(GPT-4o,version20240513).Fromonthe
logs,weextractedobjectivemetricsincludingthetimeandsuccessrateoftaskcompletionacrossthethreemethods,
thedegreeofUIdependency,andcostforthetwoAgents.
6 Results
Inlinewithourresearchquestions,wedividedtheexperimentalresultsintothreepartsforanalysis.Firstly,We
investigatehowtheadoptionofagentreducescognitiveloadforusers.Secondly,Weanalyzeandcomparedifferent
agents’abilitiesofenhancingtaskefficiency.Finally,wediscussusers’preferencesbetweenthetraditionalUIagentand
ourAXISagent.
6.1 Cognitiveload
ToinvestigatethereductionofcognitiveloadbyLLM-basedAgents,weanalyzedtheNASA-TLXandlearningeffort
scoringcollectedfromusersduringthetaskexecutionprocessandsummarizedresultsinTable6andFigure3.Itis
worthynotingthat,inourexperiment,L2tasksgenerallyscoreshigherthanL1tasksacrossmultipledimensionsofthe
NASA-TLXscaleandlearningefforts,whichindicatesthatourtaskdifficultyclassificationisconsistentwiththeusers’
experience.
AsshowninTable6,atbothL1andL2difficultylevels,theagentbasedmethodshowssignificantimprovements
overthemanualbasedmethodinmostoftheNASA-TLXscales.Inparticular,theagentbasedmethodismuchless
mentallyandphysicallydemanding,generatelessfrustrationforusers,andrequireslesseffortincompletingtasks
thanthemanualbasedmethod.Thereductioninthecognitiveeffortbytheagentbasedmethodisalsogenerallymore
pronouncedforthemoredifficultyL2tasks.Fortheperformancemetric,whilethedifferencebetweenagentbased
methodandmanualmethodisinsignficantfortheeasyL1tasks,theagentbasedmethoddoesboosttheusers’feeling
ofsuccesssignificantly(p<0.05)forthedifficultL2tasks.InFigure3(b),wealsosummarizedtheaveragescoresacross
allthesixNASA-TLXscales.Thisfigureshowsthat,whenusingtheAgents,theusers’experiencesincompleting
L1andL2tasksaresimilar,whichdemonstratesthestabilityofLLM-basedagentsinaddressingtaskswithdifferent
complexities.Finally,Figure3(c)showsthattheLLM-basedagentscanalsosignificantlyreducedusers’learningefforts
incompletingthetaskcomparedtothemanualapproach.SimilartotheNASA-TLXscales,thereductionisalsobigger
atthehighertaskdifficultylevel.
Allthoseresultsclearlydemonstratethevalueofagentbasedmethodinhelpingusersincompletingvarioustasks
andanswerthefirstresearchquestion(RQ1):theLLM-basedagentdoesindeedlowerthecognitiveloadofusersand
reducestheirefforttolearn,especiallyformoredifficulttasks.
6.2 Efficiencyandreliability
Tocomparetheefficiencyandreliabilitybetweenthemanualmethod,UIagents,andAXIS,wealsocollectedmetrics
onthecompletiontime,successrate,aswellasthenumberofstepsandcostsforcompletingtasksinourexperiment.
ThoseinformationaresummarizedinTable7andTable8.
Intermsoftimeefficiency,AXISconsistentlytooksignificantlylesstimethanboththemanualandUIAgentmethods
forbothL1andL2tasks(p<0.001),withalargeradvantageforthemoredifficultL2tasks.ForL1tasks,themanual
methodwasactuallyfasterthantheUIAgentastheUIAgentgenerallytookmanystepstocompleteatask.
Fortheaccuracy,unsurprisinglythemanualmethodisthebestamongallthethreemethods.Still,AXIScanachieve
ahighlevelaccuracythatisonlyslightlyworsethanhumanperformance.Incontrast,theaccuracyoftheUIagentis
ManuscriptsubmittedtoACM14 Luetal.
Table6. ComparisonofNASA-TLXresultsofLevel1andLevel2tasks.(m:Manual,a:Agents)
Metric TaskLevel Manual Agents PairwiseSignificance
L1 21.3 2.5 L1:m>a(p<0.001)
MentalDemand(0-100)
L2 70.0 7.5 L2:m>a(p<0.001)
L1 31.3 5.0 L1:m>a(p<0.001)
PhysicalDemand(0-100)
L2 57.5 6.3 L2:m>a(p<0.001)
L1 52.5 28.8 L1:m>a(p<0.05)
TemporalDemand(0-100)
L2 37.5 35.0 L2:-
L1 21.2 21.2 L1:-
Performance(0-100)
L2 47.5 26.2 L2:m>a(p<0.05)
L1 31.3 7.5 L1:m>a(p<0.001)
FrustrationLevel(0-100)
L2 62.5 10.0 L2:m>a(p<0.001)
L1 12.5 17.5 L1:-
CompletionEffort(0-100)
L2 35.0 13.8 L2:m>a(p<0.01)
Fig.3. TheresultsofNASAWorkloadandlearneffortsonL1andL2tasksofuserstudy.Barsindicatestandarderrors(**:p<0.01,
***:p<0.001)
considerablylower,especiallyattheL2tasks.Uponreviewingthevideologs,wefoundthatincorrectpositioningofUI
elementsandtheinvisibilityofcertainUIcomponentswerethemajorcausesoftheerrorsmadebythetheUIagent.
Finally,asshowninTable8),UIagentsoftenneedtotakemorestepstofinishthetask,especiallytheL2taskswhere
wherethetargetUIelementswereburieddeeperintheinterface.Incontrast,empoweredbythestreamlinedtask
executionenabledbyAPIcalls,AXIStooksignificantlyfewersteps,andthusloweroverallcostsforcompletingtasksat
bothdifficultylevels.
Basedontheaboveanalysis,wecanaddressoursecondresearchquestion(RQ2).TheuseofaUIAgentprovidesa
slightimprovementinhumanefficiencyforspecificcomplextasks,butitsuffersfromreliabilityissues.Incontrast,
AXISconsistentlyenhanceshumanefficiencyanddemonstratesgreaterreliability.
6.3 Affinitypreference
ToexplorethedifferencesinuserexperiencewhenexecutingtaskswithUIagentsversusAXIS,Wealsoconducteda
subjectivepreferenceevaluationonfiveaspectsforbothL1andL2tasksandsummarizedtheresultsinFigure4.At
ManuscriptsubmittedtoACMTurnEveryApplicationintoanAgent:TowardsEfficientHuman-Agent-ComputerInteractionwithAPI-First
LLM-BasedAgents 15
Table7. ComparisonofMethodsonTimeandSuccessRateinL1andL2tasks.
Metric TaskLevel Manual UIAgent AXIS PairwiseSignificance
L1:m<u(p<0.001)
L1 61.8 104.6 18.2
Time(s) L1,L2:a<m(p<0.001)
L2 167.6 155.5 57.1
L1,L2:a<u(p<0.001)
L1 100.0 75.0 98.3 L1,L2:m>u(p<0.001)
SuccessRate(%)
L2 97.5 45.0 95.0 L1,L2:a>u(p<0.001)
Table8. ComparisonofMethodsonStepsandCostinL1andL2tasks.
Metric TaskLevel UIAgent AXIS PairwiseSignificance
L1 6.4 1.0 L1:a<u(p<0.001)
steps
L2 11.1 4.2 L2:a<u(p<0.001)
L1 0.6 0.07 L1:a<u(p<0.001)
cost($)
L2 0.9 0.3 L2:a<u(p<0.001)
bothdifficultylevels,participantsshowedageneralpreferenceforAXISoverUIagentsintermoftheperceivedspeed,
fluency,andreliability.
Fortheperceivedconsistencyofdecision,theresultsvariedbetweentheL1andL2tasks.IntheL1tasks,AXISusually
cancompletetaskswithoneorafewstepsduetothehighencapsulationofitsAPI,whichresultedinadecision-making
stylethatisquitedistantfromthethinkingpatternofhuman.However,incomplextasks,AXIS’sdecisionsbecome
morealignedwithhuman’sthoughtprocessesandwerethusperceivedbetterbyhumansinthisaspect.FortheUI
dependency,AXISismuchlessrelianttotheUIcomparedtotheUIAgent,whichneedstofrequentlyinteractwiththe
UIinterface.ThisreduceddependencyontheUIwasclearlyperceivedbyusersduringtheirexperience.
Insummary,forthethirdresearchquestion(RQ3):AXIS,comparedtoUIagents,tendstoleaveuserswithaperception
ofbetterefficiency,smoothness,andreliability.Astaskcomplexityincreases,usersarealsomoreinclinedtofavor
AXISfortaskresolution.Additionally,feedbackfromusersurveysindicatedadesireforgreatercontrolwhenusing
LLM-basedagents.UnlikeUIagents,whichofteninterruptusers’mouseactionsandoccupyscreenspace,AXIS’s
API-firstapproachaddressesthisissueandimprovesusers’experience.
Fig.4. TheresultsofsubjectivepreferenceonL1andL2tasksofuserstudy.
ManuscriptsubmittedtoACM16 Luetal.
7 Discussion
7.1 AXIShelptodigestunnecessaryApplicationUIs
TobuildnewAPIsontopofexistingAPIandUIfunctions,AXISleveragesaLLM-poweredself-explorationframework
toidentifyallcontrolelementswithinanapplicationthatcanbeconvertedintoAPIs.Thisexplorationprocedurehelps
uncoverpotentiallyunnecessaryUIelementsorredundantUIdesignsforimprovementundertheHACIparadigm.
Toillustratethisprocess,inFigure5,theUIhierarchicalrelationshipsbetweenUIsarerepresentedasatree,inwhich
eachnoderepresentsaUIelementwithhigher-levelUIelementsasparentnodesandlower-levelonesaschildnodes.
WefurtheruserednodestorepresentUIlocationsthatcanbeAPI-ifiedafterexploredbyAXIS,andusebluenodes
representgeneralUIelements.Inthisexample,therootnodethatrepresentsthe"Home"tabisabluenodeasnotallits
sub-UInodesarered(API-ified).However,thesecond-levelnode"HighlightColor"(node2-2)andallitsthird-level
childnodescanallbeAPI-ifiedandarecoloredinred.Generally,wedefineanodeN asnon-essentialifthisnodealong
withalltheirchildnodescanallbeAPI-ified:
NonEssential(𝑁)=True, if𝑁 andallitschildnodesarerednodes(API-enabled)
False, otherwise

UnliketheHCIparadigmthatemphasizestheinteractionsbetweenhumanandinterfaces,inthefutureAgentOS
poweredbyLLM-basedagents,non-essentialUIelementscanbesimplifiedoreveneliminatedfromtheapplication
interface,withtheiroriginalfunctionsreplacedbytheAPIcalls.BycategorizingUIelementsasessentialornon-essential,
AXIScanprovidevaluableinsightsonhowtheUImightbeimprovedandre-designedinanagent-basedsystemforthe
applicationproviders.
Fig.5. ThefigureillustratesruleofidentifyingtheUIcontrolsavailabletobecropped.Ontheleft,therelevantUIcomponents
fromtheoriginaldocumentstructurearedisplayed.Ontheright,thecorrespondingUItreeisshown,withnodesmatchingtheUI
componentsbynumberandposition,numbersindicatinghierarchylevels,andarrowsrepresentingparent-childrelationships.The
rednodesrepresentUIcontrolsthatcanbecropped.
ManuscriptsubmittedtoACMTurnEveryApplicationintoanAgent:TowardsEfficientHuman-Agent-ComputerInteractionwithAPI-First
LLM-BasedAgents 17
7.2 TurnAnApplicationintoanAgent
Intheexperimentsection,weuseMicrosoftWordtoillustratehowtoexploreandconstructnewAPIagentsusingthe
AXISframework.ItisworthynotingthattheAXISframeworkishighlyadaptableandscalable,andcanbeextendedto
anynewapplicationwithabasicAPIanddocumentationsupport.Specifically,toadaptAXIS,theapplicationprovider
needstosupplementoperationalmanualsontheapplicationsaswellasthefollowinginterfaces:
• EnvironmentStateInterfaceforobtaininginformationaboutthestateoftheenvironment.
• BasicActionInterfaceforsupportingbasicinteractionswiththeenvironment.
Startingfromthosebasicresources,AXIScanautomaticallyandcontinuouslyexploretheapplications,discovernew
skills,andextenditsfunctionalities.ThisadaptabilityalsomeansthatAXIScanbeintegratedintovarioussoftware
environmentstoenhancefunctionalityanduserexperiencewithAPI-driveninteractions.
8 Conclusion
Inthispaper,weintroduceAXIS,apioneeringframeworkdesignedtoenhancehuman-agent-computerinteraction
(HACI)andaddresstheinefficienciesandcognitiveburdensassociatedwithmultimodallargelanguagemodels(MLLMs)
incomplextaskexecutionbyprioritizingAPIcallsovertraditionalUIinteractions.Throughuserexperimentswithtasks
fromOfficeWord,AXIShasproventobehighlyeffective,reducingtaskcompletiontimeby65%-70%andcognitive
workloadby38%-53%,whilemaintainingahighlevelofaccuracycomparabletohumanperformance.Theseresults
underscorethepotentialofAPI-firstLLM-basedagentstostreamlineinteractions,minimizelatency,andenhance
reliabilityintaskexecution.
Ourresearchcontributestothebroaderfieldofhuman-agentinteractionbyhighlightingthelimitationsoftraditional
UI-basedapproachesandproposinganovelsolutionthatleveragesAPIcallstosimplifyandacceleratetaskcompletion.
ByenablingapplicationstoactasagentsthroughareducedsetofessentialUIsandenhancedAPIsets,AXISnotonly
improvesefficiencybutalsopavesthewaytowardsthedevelopmentofacomprehensiveAgentOS.Thisparadigmshift
suggeststhateveryapplicationhasthepotentialtotransformintoanintelligentagentcapableofexecutingtaskswith
minimaluserintervention.
Inconclusion,AXISrepresentsasignificantstepforwardinreducingcognitiveloadandenhancingtheefficiency
oftaskcompletionwithLLM-basedagents.Ourfindingsprovidevaluableinsightsforapplicationdevelopersand
researchers,encouragingthemtorethinkUIdesignsandexplorenewwaystointegrateAPI-driveninteractions.Inour
futurework,wewillfocusonextendingthisframeworktoabroaderrangeofapplications,exploringitsimpacton
varioususergroups,andadvancingthepotentialofLLMsincreatingamoreintuitiveandefficienthuman-computer
interface.
References
[1] PekkaAbrahamsson,OutiSalo,JussiRonkainen,andJuhaniWarsta.2017.Agilesoftwaredevelopmentmethods:Reviewandanalysis.arXivpreprint
arXiv:1709.08439(2017).
[2] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,DiogoAlmeida,JankoAltenschmidt,Sam
Altman,ShyamalAnadkat,etal.2023.Gpt-4technicalreport.arXivpreprintarXiv:2303.08774(2023).
[3] Apple.2024.AppleIntelligence.https://developer.apple.com/apple-intelligence/. Accessed:2024-08-28.
[4] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,ChangZhou,andJingrenZhou.2023.Qwen-VL:AVersatile
Vision-LanguageModelforUnderstanding,Localization,TextReading,andBeyond.arXivpreprintarXiv:2308.12966(2023).
[5] YejinBang,SamuelCahyawijaya,NayeonLee,WenliangDai,DanSu,BryanWilie,HolyLovenia,ZiweiJi,TiezhengYu,WillyChung,etal.2023.A
multitask,multilingual,multimodalevaluationofchatgptonreasoning,hallucination,andinteractivity.arXivpreprintarXiv:2302.04023(2023).
ManuscriptsubmittedtoACM18 Luetal.
[6] GautamBiswas,KrittayaLeelawong,DanielSchwartz,NancyVye,andTheTeachableAgentsGroupatVanderbilt.2005.Learningbyteaching:A
newagentparadigmforeducationalsoftware.AppliedArtificialIntelligence19,3-4(2005),363–392.
[7] JeffreyMBradshaw,PaulJFeltovich,andMatthewJohnson.2017.Human–agentinteraction.InThehandbookofhuman-machineinteraction.CRC
Press,283–300.
[8] LingjiaoChen,JaredQuincyDavis,BorisHanin,PeterBailis,IonStoica,MateiZaharia,andJamesZou.2024.Aremorellmcallsallyouneed?
towardsscalinglawsofcompoundinferencesystems.arXivpreprintarXiv:2403.02419(2024).
[9] KanzhiCheng,QiushiSun,YougangChu,FangzhiXu,YantaoLi,JianbingZhang,andZhiyongWu.2024.Seeclick:Harnessingguigroundingfor
advancedvisualguiagents.arXivpreprintarXiv:2401.10935(2024).
[10] AliDarejeh,SaraMashayekh,andNadineMarcus.2022.Cognitive-basedmethodstofacilitatelearningofsoftwareapplicationsviaE-learning
systems.CogentEducation9,1(2022),2082085.
[11] ShehzaadDhuliawala,MojtabaKomeili,JingXu,RobertaRaileanu,XianLi,AsliCelikyilmaz,andJasonWeston.2023.Chain-of-verificationreduces
hallucinationinlargelanguagemodels.arXivpreprintarXiv:2309.11495(2023).
[12] PeitongDuan,JeremyWarner,YangLi,andBjoernHartmann.2024.GeneratingAutomaticFeedbackonUIMockupswithLargeLanguageModels.
InProceedingsoftheCHIConferenceonHumanFactorsinComputingSystems.1–20.
[13] AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,AieshaLetman,AkhilMathur,AlanSchelten,AmyYang,
AngelaFan,etal.2024.Thellama3herdofmodels.arXivpreprintarXiv:2407.21783(2024).
[14] ZaneDurante,QiuyuanHuang,NaokiWake,RanGong,JaeSungPark,BidiptaSarkar,RohanTaori,YusukeNoda,DemetriTerzopoulos,YejinChoi,
etal.2024.Agentai:Surveyingthehorizonsofmultimodalinteraction.arXivpreprintarXiv:2401.03568(2024).
[15] VageEgiazarian,AndreiPanferov,DenisKuznedelev,EliasFrantar,ArtemBabenko,andDanAlistarh.2024.Extremecompressionoflargelanguage
modelsviaadditivequantization.arXivpreprintarXiv:2401.06118(2024).
[16] MetaFundamentalAIResearchDiplomacyTeam(FAIR)†,AntonBakhtin,NoamBrown,EmilyDinan,GabrieleFarina,ColinFlaherty,DanielFried,
AndrewGoff,JonathanGray,HengyuanHu,etal.2022.Human-levelplayinthegameofDiplomacybycombininglanguagemodelswithstrategic
reasoning.Science378,6624(2022),1067–1074.
[17] SidongFeng,SuyuMa,HanWang,DavidKong,andChunyangChen.2024.MUD:TowardsaLarge-ScaleandNoise-FilteredUIDatasetforModern
StyleUIModeling.InProceedingsoftheCHIConferenceonHumanFactorsinComputingSystems.1–14.
[18] YanchuGuan,DongWang,ZhixuanChu,ShiyuWang,FeiyueNi,RuihuaSong,andChenyiZhuang.2024.IntelligentAgentswithLLM-based
ProcessAutomation.InProceedingsofthe30thACMSIGKDDConferenceonKnowledgeDiscoveryandDataMining.5018–5027.
[19] SGHart.1988.DevelopmentofNASA-TLX(TaskLoadIndex):Resultsofempiricalandtheoreticalresearch.Humanmentalworkload/Elsevier(1988).
[20] WenyiHong,WeihanWang,QingsongLv,JiazhengXu,WenmengYu,JunhuiJi,YanWang,ZihanWang,YuxiaoDong,MingDing,etal.2024.
Cogagent:Avisuallanguagemodelforguiagents.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.14281–14290.
[21] Honor.2024.MagicOS.https://www.honor.com/global/magic-os/. Accessed:2024-08-28.
[22] JieHuangandKevinChen-ChuanChang.2022.Towardsreasoninginlargelanguagemodels:Asurvey.arXivpreprintarXiv:2212.10403(2022).
[23] Huawei.2024.HarmonyOS.https://www.harmonyos.com/en/. Accessed:2024-08-28.
[24] MoshLevy,AlonJacoby,andYoavGoldberg.2024. Sametask,moretokens:theimpactofinputlengthonthereasoningperformanceoflarge
languagemodels.arXivpreprintarXiv:2402.14848(2024).
[25] MichaelLewis.1998.Designingforhuman-agentinteraction.Aimagazine19,2(1998),67–67.
[26] XiaoLiu,HaoYu,HanchenZhang,YifanXu,XuanyuLei,HanyuLai,YuGu,HangliangDing,KaiwenMen,KejuanYang,etal.2023.Agentbench:
Evaluatingllmsasagents.arXivpreprintarXiv:2308.03688(2023).
[27] ZhaoMandi,ShreeyaJain,andShuranSong.2024.Roco:Dialecticmulti-robotcollaborationwithlargelanguagemodels.In2024IEEEInternational
ConferenceonRoboticsandAutomation(ICRA).IEEE,286–299.
[28] KaiMei,ZelongLi,ShuyuanXu,RuosongYe,YingqiangGe,andYongfengZhang.2024.AIOS:LLMagentoperatingsystem.arXive-prints,pp.
arXiv–2403(2024).
[29] Microsoft.2024. Copilot+PC. https://www.microsoft.com/en-us/surface/do-more-with-surface/advantages-of-copilot-plus-pcs. Accessed:
2024-08-28.
[30] Microsoft365.2024.Microsoft365Word.https://www.microsoft.com/en-us/microsoft-365/word. Accessed:2024-08-28.
[31] Microsoft365.2024.Microsoft365WordAPI.https://learn.microsoft.com/en-us/dotnet/api/microsoft.office.interop.word?view=word-pia. Accessed:
2024-08-28.
[32] OpenAI.2024.GPT-4o.https://platform.openai.com/docs/models/gpt-4o. Accessed:2024-08-28.
[33] OpenAI.2024.GPT-4V(ision).https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4. Accessed:2024-08-28.
[34] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,ChongZhang,SandhiniAgarwal,KatarinaSlama,Alex
Ray,etal.2022.Traininglanguagemodelstofollowinstructionswithhumanfeedback.Advancesinneuralinformationprocessingsystems35(2022),
27730–27744.
[35] JanLPlass,RoxanaMoreno,andRolandBrünken.2010.Cognitiveloadtheory.(2010).
[36] ChristopherRawles,SarahClinckemaillie,YifanChang,JonathanWaltz,GabrielleLau,MarybethFair,AliceLi,WilliamBishop,WeiLi,Folawiyo
Campbell-Ajala,etal.2024.AndroidWorld:Adynamicbenchmarkingenvironmentforautonomousagents.arXivpreprintarXiv:2405.14573(2024).
ManuscriptsubmittedtoACMTurnEveryApplicationintoanAgent:TowardsEfficientHuman-Agent-ComputerInteractionwithAPI-First
LLM-BasedAgents 19
[37] ChristopherRawles,AliceLi,DanielRodriguez,OrianaRiva,andTimothyLillicrap.2024.Androidinthewild:Alarge-scaledatasetforandroid
devicecontrol.AdvancesinNeuralInformationProcessingSystems36(2024).
[38] NayanBRuparelia.2010.Softwaredevelopmentlifecyclemodels.ACMSIGSOFTSoftwareEngineeringNotes35,3(2010),8–13.
[39] DebbieStone,CarolineJarrett,MarkWoodroffe,andShaileyMinocha.2005.Userinterfacedesignandevaluation.Elsevier.
[40] WeihaoTan,ZiluoDing,WentaoZhang,BoyuLi,BohanZhou,JunpengYue,HaochongXia,JiechuanJiang,LongtaoZheng,XinrunXu,etal.2024.
Towardsgeneralcomputercontrol:Amultimodalagentforreddeadredemptioniiasacasestudy.arXivpreprintarXiv:2403.03186(2024).
[41] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,RaduSoricut,JohanSchalkwyk,AndrewMDai,Anja
Hauth,etal.2023.Gemini:afamilyofhighlycapablemultimodalmodels.arXivpreprintarXiv:2312.11805(2023).
[42] JeroenJGVanMerrienboerandJohnSweller.2005. Cognitiveloadtheoryandcomplexlearning:Recentdevelopmentsandfuturedirections.
Educationalpsychologyreview17(2005),147–177.
[43] JunyangWang,HaiyangXu,JiaboYe,MingYan,WeizhouShen,JiZhang,FeiHuang,andJitaoSang.2024.Mobile-agent:Autonomousmulti-modal
mobiledeviceagentwithvisualperception.arXivpreprintarXiv:2401.16158(2024).
[44] LeiWang,ChenMa,XueyangFeng,ZeyuZhang,HaoYang,JingsenZhang,ZhiyuanChen,JiakaiTang,XuChen,YankaiLin,etal.2024.Asurvey
onlargelanguagemodelbasedautonomousagents.FrontiersofComputerScience18,6(2024),186345.
[45] WenxiaoWang,WeiChen,YicongLuo,YongliuLong,ZhengkaiLin,LiyeZhang,BinbinLin,DengCai,andXiaofeiHe.2024.Modelcompression
andefficientinferenceforlargelanguagemodels:Asurvey.arXivpreprintarXiv:2402.09748(2024).
[46] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,DennyZhou,etal.2022.Chain-of-thoughtprompting
elicitsreasoninginlargelanguagemodels.Advancesinneuralinformationprocessingsystems35(2022),24824–24837.
[47] ChaoyiWu,JiayuLei,QiaoyuZheng,WeikeZhao,WeixiongLin,XiaomanZhang,XiaoZhou,ZihengZhao,YaZhang,YanfengWang,etal.2023.
Cangpt-4v(ision)servemedicalapplications?casestudiesongpt-4vformultimodalmedicaldiagnosis.arXivpreprintarXiv:2310.09909(2023).
[48] ZhiyongWu,ChengchengHan,ZichenDing,ZhenminWeng,ZhoumianzeLiu,ShunyuYao,TaoYu,andLingpengKong.2024.Os-copilot:Towards
generalistcomputeragentswithself-improvement.arXivpreprintarXiv:2402.07456(2024).
[49] ZhihengXi,WenxiangChen,XinGuo,WeiHe,YiwenDing,BoyangHong,MingZhang,JunzheWang,SenjieJin,EnyuZhou,etal.2023.Therise
andpotentialoflargelanguagemodelbasedagents:Asurvey.arXivpreprintarXiv:2309.07864(2023).
[50] WeiXiang,HanfeiZhu,SuqiLou,XinliChen,ZhenghuaPan,YupingJin,ShiChen,andLingyunSun.2024.SimUser:GeneratingUsabilityFeedback
bySimulatingVariousUsersInteractingwithMobileApplications.InProceedingsoftheCHIConferenceonHumanFactorsinComputingSystems.
1–17.
[51] TianbaoXie,DanyangZhang,JixuanChen,XiaochuanLi,SihengZhao,RuishengCao,TohJingHua,ZhoujunCheng,DongchanShin,FangyuLei,
etal.2024.Osworld:Benchmarkingmultimodalagentsforopen-endedtasksinrealcomputerenvironments.arXivpreprintarXiv:2404.07972(2024).
[52] AnYan,ZhengyuanYang,WanrongZhu,KevinLin,LinjieLi,JianfengWang,JianweiYang,YiwuZhong,JulianMcAuley,JianfengGao,etal.2023.
Gpt-4vinwonderland:Largemultimodalmodelsforzero-shotsmartphoneguinavigation.arXivpreprintarXiv:2311.07562(2023).
[53] ShukangYin,ChaoyouFu,SiruiZhao,KeLi,XingSun,TongXu,andEnhongChen.2023.Asurveyonmultimodallargelanguagemodels.arXiv
preprintarXiv:2306.13549(2023).
[54] ChaoyunZhang,LiqunLi,ShilinHe,XuZhang,BoQiao,SiQin,MinghuaMa,YuKang,QingweiLin,SaravanRajmohan,etal.2024. Ufo:A
ui-focusedagentforwindowsosinteraction.arXivpreprintarXiv:2402.07939(2024).
[55] ChiZhang,ZhaoYang,JiaxuanLiu,YuchengHan,XinChen,ZebiaoHuang,BinFu,andGangYu.2023.AppAgent:MultimodalAgentsasSmartphone
Users.CoRRabs/2312.13771(2023). https://doi.org/10.48550/ARXIV.2312.13771arXiv:2312.13771
[56] DuzhenZhang,YahanYu,ChenxingLi,JiahuaDong,DanSu,ChenhuiChu,andDongYu.2024.Mm-llms:Recentadvancesinmultimodallarge
languagemodels.arXivpreprintarXiv:2401.13601(2024).
[57] YueZhang,YafuLi,LeyangCui,DengCai,LemaoLiu,TingchenFu,XintingHuang,EnboZhao,YuZhang,YulongChen,etal.2023.Siren’ssongin
theAIocean:asurveyonhallucinationinlargelanguagemodels.arXivpreprintarXiv:2309.01219(2023).
[58] ZhiyangZhang,FangkaiYang,XiaotingQin,JueZhang,QingweiLin,GongCheng,DongmeiZhang,SaravanRajmohan,andQiZhang.2024.The
VisionofAutonomicComputing:CanLLMsMakeItaReality?arXivpreprintarXiv:2407.14402(2024).
[59] ZiruiZhao,WeeSunLee,andDavidHsu.2024.Largelanguagemodelsascommonsenseknowledgeforlarge-scaletaskplanning.Advancesin
NeuralInformationProcessingSystems36(2024).
[60] BoyuanZheng,BoyuGou,JihyungKil,HuanSun,andYuSu.2024.Gpt-4v(ision)isageneralistwebagent,ifgrounded.arXivpreprintarXiv:2401.01614
(2024).
ManuscriptsubmittedtoACM20 Luetal.
A UserStudyWebInterface
Duringtheuserstudy,weprovidedparticipantswithawebinterfacetocontroltheuserstudyprocedure.Beloware
somescreenshotsofthewebinterface.
Fig.6. Thefigureofthelogininterfaceforuserstudy.Eachparticipantwasassignedanaccountandpassword.
Fig.7. Thefigureoftheintroductionpageofmanualmodeinuserstudy.Eachparticipantwasinstructedtofollowthestepstofinish
thetaskmanually.
Fig.8. Thefigureofthetaskpageofmanualmodeinuserstudy.Participantshouldcompletethetasksbasedontheguidelines
printedonpaper.
ManuscriptsubmittedtoACMTurnEveryApplicationintoanAgent:TowardsEfficientHuman-Agent-ComputerInteractionwithAPI-First
LLM-BasedAgents 21
Fig.9. Thefigureoftheintroductionpageofagentmodeinuserstudy.Eachparticipantwasinstructedtotypeintaskdescriptionto
useagenttofinishthetask.
Fig.10. Thefigureofthetaskpageofagentmodeinuserstudy.Participantsshouldinputandsubmitthetaskdescriptiontotwo
differentagents,whichwouldthenautomaticallycompletethetask.Theleftimageshowstheoriginalpage,whiletherightimage
displaysthepageafterthetwoagentshavecompletedthetask.Thetextboxesintherightimageshowthedecision-makingprocesses
ofeachagent.
B UserStudyTasks
WesampledfivetasksaboutMicrosoftWordinuserstudywhichwerecategorizedintolowdifficulty(L1)andhigh
difficulty(L2).Herearethedetailedtasksintable9.
ManuscriptsubmittedtoACM22 Luetal.
Fig.11. Thefigureofthequestionairepageinuserstudy,whichoccurredafterthecompletionoftasksindifferentdifficultylevels
withinbothManualModeandAgentMode.
Table9. Thesampledtasksintwolevelsofdifficultyforuserstudy.
Taskid Taskdescription Difficultylevel
1 Here is an article, type in a title "Im- L1
possibleFriendshipbetweenmouseand
cats"andsetthetitleinthecenterwith
"Arial"typeof20fontsize.
2 Insertaheadernamed"header"anda L1
footernamed"footer".
3 Changethetitlesstyleofeachsections L1
intoheading1style.
4 Iwanttomakeaspecialformatforcom- L2
pany:inserta2x2table,thenchangethe
papersizeinWordtoA4,changethe
textdirectiontoverticalandaddwater
markwithconfidential1type.
5 Insert2shapesintodocument:(1)Insert L2
arectanglewithawidthandheightof
1inch,andsetthefillcolortored.(2)
Insertacirclewithawidthandheight
of1inch,andsetthefillcolortoyellow.
C UserStudySurveyForm
Toobtainsubjectivemetricsandanalyzetheresultstoaddressourresearchquestions,weincludedseveralquestionnaires
intheuserstudywhicharelistedbelow:
C.1 Cognitiveloadrelatedforms
Thecognitiveload-relatedformsincludetheNASA-TLXsurveyandthelearningeffortsurvey,whichparticipantsfilled
outaftercompletingtasksinbothmanualmodeandagentmode.
ManuscriptsubmittedtoACMTurnEveryApplicationintoanAgent:TowardsEfficientHuman-Agent-ComputerInteractionwithAPI-First
LLM-BasedAgents 23
Fig.12. ThesurveyformforNASA-TLXofmanualmodeinuserstudy,whichwascollectedafterthecompletionoftasksmanually.
Fig.13. ThesurveyformforNASA-TLXofagentmodeinuserstudy,whichwascollectedafterthecompletionoftasksusingagents.
ManuscriptsubmittedtoACM24 Luetal.
Fig.14. Thesurveyformforlearningeffortsofusingdifferentmethodstofinishtasks,whichwascollectedaftermanualmodeand
agentmode.
C.2 HumanPreferencerelatedforms
Theformsrelatedtohumanpreferencesincludesurveysonperceivedspeed,fluency,reliability,decisionconsistency,
andUIdependency.
Fig.15. Thesurveyformforperceivedspeedofusingdifferentmethodstofinishtasks,whichwascollectedaftermanualmodeand
agentmode.
Fig.16. Thesurveyformforuidependencyofusingdifferentagentstofinishtasks,whichwascollectedafteragentmode.
D FeasibilityStudyTasks
Inthefeasibilitystudy,werandomlysampled50tasksfromtheWikiHowpage’UseMicrosoftWord’andtheofficial
MicrosoftWordwebsite.Toincreasethetaskdifficulty,someofthe50taskswerecomposedofsmallersub-tasks,thus
increasingthenumberofstepsrequiredforcompletion.Asthetaskssampledinuserstudy,the50taskswerealso
dividedinto2levelsofdifficulty:lowdifficulty(L1)andhighdifficulty(L2),basedonfactorssuchasthenumberof
UIinteractionsrequired,thedepthoftheUIfunctions,andthenumberofribbonswitches.Table10hasshownthe
distributionoftherequiredexecutionsteps(i.e.,thenumberofstepsahumanwouldtypicallyneedtoperform)ofthe
50tasks,alongwiththenumberoftasksindifferentdifficultylevels.
ManuscriptsubmittedtoACMTurnEveryApplicationintoanAgent:TowardsEfficientHuman-Agent-ComputerInteractionwithAPI-First
LLM-BasedAgents 25
Fig.17. Thesurveyformfordecisionconsistency,fluencyandreliabilityofusingdifferentagentstofinishtasks,whichwascollected
afteragentmode.
Table10. Thedistributionoftherequiredexecutionstepsanddifficultylevelofthe50tasksinfeasibilitystudy.
Steps TasksNumber DifficultyLevel
L1:3
1 3
L2:0
L1:9
2 9
L2:0
L1:14
3 23
L2:9
L1:0
4 12
L2:12
L1:0
5 1
L2:1
L1:0
8 1
L2:1
L1:0
10 1
L2:1
ManuscriptsubmittedtoACM