Generative AI Research
PROGRAMMING EVERY EXAMPLE: LIFTING PRE-
TRAINING DATA QUALITY LIKE EXPERTS AT SCALE
FanZhou∗αµ ZengzhiWang∗αµ QianLius JunlongLiα PengfeiLiu‡αµδ
αShanghaiJiaoTongUniversity δShanghaiArtificialIntelligenceLaboratory sSeaAILab
µGenerativeAIResearchLab(GAIR)
{zhoufan98,pengfei}@sjtu.edu.cn
ABSTRACT
Largelanguagemodelpre-traininghastraditionallyreliedonhumanexpertsto
craft heuristics for improving the corpora quality, resulting in numerous rules
developedtodate. However,theseruleslacktheflexibilitytoaddresstheunique
characteristics of individual example effectively. Meanwhile, applying tailored
rulestoeveryexampleisimpracticalforhumanexperts. Inthispaper,wedemon-
stratethatevensmalllanguagemodels,withasfewas0.3Bparameters,canexhibit
substantialdatarefiningcapabilitiescomparabletothoseofhumanexperts. We
introduceProgrammingEveryExample(PROX),anovelframeworkthattreats
datarefinementasaprogrammingtask,enablingmodelstorefinecorporabygener-
atingandexecutingfine-grainedoperations,suchasstringnormalization,foreach
individualexampleatscale. Experimentalresultsshowthatmodelspre-trained
on PROX-curated data outperform either original data or data filtered by other
selectionmethodsbymorethan2%acrossvariousdownstreambenchmarks. Its
effectivenessspansvariousmodelsizesandpre-trainingcorpora, includingC4,
RedPajama-V2,andFineWeb. Furthermore,PROXexhibitssignificantpotential
indomain-specificcontinualpre-training: withoutdomainspecificdesign,models
trainedonOpenWebMathrefinedbyPROXoutperformhuman-craftedrule-based
methods,improvingaverageaccuracyby7.6%overMISTRAL-7B,with14.6%
forLLAMA-2-7Band20.3%forCODELLAMA-7B,allwithin10Btokenstobe
comparabletomodelslikeLLEMMA-7Btrainedon200Btokens. Furtheranalysis
highlightsthatPROXsignificantlysavestrainingFLOPs,offeringapromisingpath
forefficientLLMpre-training. Weareopen-sourcing PROX with≥100Bcor-
pus,models,andsharingalltrainingandimplementationdetailsforreproducible
researchandfutureinnovation.
• HFRepo:https://huggingface.co/gair-prox
• Code:https://github.com/GAIR-NLP/ProX
56 Apply ProX on FineWeb Apply ProX on OpenWebMath
55
54 20× less training steps
50 52
30× less training steps +6.2%
50 45 34B
+2.4%
48 40
46 35
44 1.7B model trained on ProX OLMo-1B Cont. trained on ProX InternLM-MATH
1.7B model trained on Orig. Pythia-1.4B 30 Cont. trained on Orig. CodeLLaMA
42 TinyLLaMA-1.1B 7B Llemma-7B
0 70 140 210 1.9k 2k(F ×L 1O 01P 9s ) 0 1 2 3 4 50 80(F ×L 1O 02P 0s )
Figure1: TrainingFLOPsv.s. averagedownstreamperformance. Althoughthesecorporahavegone
throughexpert-craftedrules,applyingPROXstillyieldssignificantimprovementsoverthesebaseline
modelstrainedwithoriginaldatacorpus. Moreover,withmuchlesstrainingFLOPs,modeltrained
onPROXcurateddatashowcomparableperformancewithexistingmodels.
∗Equalcontribution.‡Correspondingauthor.
1
4202
peS
52
]LC.sc[
1v51171.9042:viXra
)%(
ecnamrofreP
egarevA
)%(
ecnamrofreP
htaMGenerative AI Research
1 INTRODUCTION
LargeLanguageModels(LLMs)havemadesignificantstridesincapabilities(Meta,2024;Achiam
etal.,2023;Anthropic,2024;Reidetal.,2024),excellingintaskssuchascreativewriting(Yuan
etal.,2022),complexreasoning(Weietal.,2022;Kojimaetal.,2022),andagentictaskplanningand
execution(Fanetal.,2022;Parketal.,2023).Behindthese,massive,high-qualitypre-trainingcorpora
formthebackboneofthesemodels, equippingthemwiththeessentialknowledgeandreasoning
abilitiescrucialforawiderangeofdownstreamtasks(Together,2023;Penedoetal.,2024a).
TheInternetoffersvastamountsofdata,butmuchofitisnoisyandunrefined,requiringextensive
cleaningandqualityenhancementbeforebeingappliedforpre-training. Previousworksfocusprimar-
ilyondesigningheuristic-basedpipelinestoliftdataquality,suchasdocumentfiltering(Raeetal.,
2021;Penedoetal.,2024a;Soldainietal.,2024)andperplexity-basedscoringmethods(Together,
2023),relyingheavilyonhumanexpertiseandmanualadjustments(Zhangetal.,2024a). While
widely adopted, these labor-intensive solutions are inherently limited by rule coverage and their
inabilitytoaddresseveryspecificcase. Recently,someeffortshaveexploredleveragingLLMsfor
high-qualitydataacquisition. Ontheonehand,languagemodelshavebeenappliedfordatafiltering
orselection(Xieetal.,2023;Wettigetal.,2024;Yuetal.,2024;Dubeyetal.,2024),buttheirrole
islargelylimitedtoidentifyinglow-qualitydocumentswithoutenablingfine-grainedrefinements
(e.g.,string-level). Ontheotherhand,LLMsarealsobeinguseddirectlygeneratinghigh-quality
data, i.e., data synthesis (Gunasekar et al., 2023; Li et al., 2023; Ben Allal et al., 2024). Unlike
filtering,synthesismethodsactivelycreateorrefinedatatoproducenewdocuments,buttheyrequire
substantialcomputationalresources,limitingscalability.Despitetheirsuccess,thesemethodscanalso
inheritissueslikehallucination(Mainietal.,2024),andassessingtheircorrectnessandcompleteness
inaninterpretablemannerremainsachallenge(Liuetal.,2024a).
Standingattheintersectionofdataprocessingefficiencyanddataqualityimprovement,inthiswork,
weproposePROX,amodel-basedframeworkforpre-trainingleveldatarefinement. PROXfocuses
onrefininglarge-scaledatawithrelativelysmallermodels,offeringamoreefficientalternative. As
shown in Figure 2, in practice, PROX first adapts a small base language model (less than 1B) to
datarefiningtasksviafine-tuningonseeddata. This PROX’srefiningmodelthendeterminesthe
appropriate operations for each example in the pre-training corpora through versatile programs,
including operations such as filtering, string normalization and noisy line removal. Finally, the
generatedprogramisexecutedbyapre-definedexecutor,producingrefinedcorpusreadyforpre-
training. Inthisway,PROXisempoweredwithlanguagemodelstoautonomouslyrefinepre-training
corpora,leveragingflexiblefunctioncallstoenhancedataquality.
ExperimentalresultsdemonstratethattheproposedPROXframeworkconsistentlyliftsdataquality
forpre-training. Specifically,PROXachievesanaverageimprovementof2.1%over10downstream
benchmarksandoutperformsstate-of-the-artdataselectionmethodsbyover2.0%. Furthermore,
PROXshowsbroadapplicabilityacrossmodelsizesfrom0.3Bto1.7Bandshowsconsistentperfor-
mancegainsacrossdiversepre-trainingcorporaofvaryingquality,includingRedPajama-V2(To-
gether, 2023), C4 (Raffel et al., 2020), and FineWeb (Penedo et al., 2024a). In domain-specific
continual pre-training, PROX yields an 11% gain over OpenWebMath (Paster et al., 2024) for
TINYLLAMA-1.1Band7.6%forMISTRAL-7Bacross9mathematicaltasks,withsimilarimprove-
mentsseenonLLAMA-2-7BandCODELLAMA-7B.Beyondperformancegains,resultsalsosuggest
thatpre-trainingontherefinedcorpussignificantlyboostspre-trainingefficiency,achievingsimilar
downstreamperformancewithupto20×lesscomputing. Webelieveitisworthwhiletoscaleup
computingFLOPsfordatarefinement,whichenablessimilarperformancewithmuchlesstraining
costandoffersapromisingpathforefficientLLMpre-training.
2 APPROACH: PROGRAMMING EVERY EXAMPLE
2.1 DATAREFINEMENTTASKFORMULATION
Givenanydocumentinthecorpusd∈D,suchasanHTMLextractoratextbook,wedefinedata
refinement as the process of transforming d into dˆ, where dˆexhibits higher quality. While it is
challengingtoformallydefine“higherquality”forpre-trainingdata,weassumeitcanbedescribed
2Generative AI Research
ProX: Programming Every EXample
Pre-trainingCorpora Refined Corpora
[000] Activities | Technical Reports | About W3C | … ProXprograms [001] A description of the
[001] A description of the type of authorized Base type of authorized
interactions a can have with a resource. Examples Model Chunk-level Program interactions a can have
include read, write, execute, add, modify, and normalize(‘ClickHere’, ‘’) with a resource. Examples
delete. ClickHere remove_lines(0, 0) include read, write,
… [10…
8]http://t.co/XUQZZRvn3i
!!
1 Adapt
remove_lines(108, 109) e dx ee lec tu et .e, add, modify, ✓and
[109]Search English terms starting with the letter: D keo ec p-l _e dv oe cl (P )rogram ……
Noisy & Mixed-quality Refining 2 3 Clean & High-quality
Model Generate Execute
Figure2: AnoverviewofPROXframework: (1)weadaptabaselanguagemodeltoperformdata
refinement;(2)PROXrefiningmodelsareabletogeneratecomplexprogramsforeachdocument,
including document level filtering and more fine-grained chunk level refining; (3) A Python
executorwillexecutetheprogramswiththedocs,producingtherefinedhigh-qualitycorpora.
throughqualitativeimprovements,suchastheremovalofadvertisements,meaninglessURLlinks,
random code gibberish, and content lacking educational value, just as shown on the left side of
Figure 2. Specifically, we formulate this refining process as the generation of a data processing
programZ,conditionedond. TherefineddocumentdˆisthenproducedbyexecutingprogramZ on
theoriginaldocumentd. Forinstance,the“stringnormalization”canbeaveryfine-grainedprocess
transformingnoisystringsintocleanoneswithexecutorE andprogramZ :
normalize
E(Z ,d)=(s′)|d| , wheres′ =normalize(s )ifs needsnormalizationelses (1)
normalize i i=1 i i i i
Here, d = (s ,s ,...,s ) is the original document represented as a sequence of strings, and
1 2 |d|
normalize()isournormalizationfunctionthatmapscertainstringstotheirnormalizedversions.
Moreover,thedocumentfilteringprocesscanberegardedasaspecialcaseofsuchrefiningtransfor-
mationwhereexecutingonZ willleadtoremovingthewholedocument,i.e.,E(Z ,d)=∅.
filter filter
Inthismanner,dataqualityimprovementoperations,suchasdatacleaningornormalizing,canbe
unifiedintothestandardizedfunctionthatappliesaspecifictransformationorcleaningprocessto
thedocument. Theseoperationscanberepresentedasvariousinstantiationsofthegeneralexecutor
E(Z,d),whereZ encodesthefunctioncallingsnippetsorheuristicsforthespecifictask.
2.2 PROXFRAMEWORK
Overview AsshowninFigure2,givenanydocumentdasinput,the PROX frameworkutilizes
thelanguagemodelitselfwithparameterθ togeneratethedatarefinementprogramZ = f(θ,d).
ThesnippetisexecutedwithintheexecutorE,producingtherefineddocumentdˆ= E(f(θ,d),d).
WeincludetwostagesinthePROXframework,aimingtorefinethedataprogressively,fromrough
tofine-grained. Thesetwostagesarereferredtoasdocument-levelprogrammingandchunk-level
programming, as illustrated in Figure 2. In each stage, the PROX refining model will generate
programsZ andZ thatrefinethecorporaatvaryinglevelsofgranularities.
doc chunk
PROX ProgramDesign Thedetailedprogramspacedesignisalsocrucialformaximizingthe
capabilitiesoflanguagemodels. Webelieveddesigningsuchmodel-basedoperationsshouldconsider
severalrealisticfactorswhenscalingtolargepre-trainingcorpora: (1)themodeldoesnotneedto
beverypowerfulorverylargetohandlethesetasks,itonlyneedstorecognizeseveralpatterns;(2)
thesolution,thoughrequiringmorecomputingbudgetcomparedtoheuristic-rule-basedpipelines,
stillneedstobesimpleandefficient. Undersuchconsideration,wesimplyletthelanguagemodels
generate function calls without detailed implementations. These design choices aim to balance
functionalitywiththelimitationsofsmalllanguagemodels,enablingeffectivedocumentmanipulation
whilemaintainingsimplicityandcoherence.
Themostfundamentaloperationsweaimtoperformonadocument,aredeletionandreplacement.
Weincorporatethesetypesofoperationsacrossdifferentprogrammingstagesaimingtorefinethe
corpus with different granularities in PROX: (1) In the document-level programming stage, we
simplydefinethefunctiondrop_doc()todeleteadocumentandkeep_doc()toretainit. (2)In
chunk-levelprogramming,wesplitthelengthydocumentsintosmallerchunksandapplyfine-grained
operationstothesechunks. Theseoperationsincludedeletingspecificlinesremove_lines()
3Generative AI Research
Table1: PROXprogramdesignofdocument-levelandchunk-levelrefiningstage. Forinput,docand
chunkwillbesentintothecorrespondingfunctionasstring-typeinputsforexecution.
Stage FunctionInterface Description
Document drop_doc()→<None> Deletethewholedoc.
Level keep_doc()→<str> Returntheorignaldoc.
remove_lines(line_start, line_end)→<str> Deletenoisylinesfromchunk;
▷line_start<int>,indexofthefirstlinetoberemoved Returnchunkafterremoval.
▷line_end<int>,indexofthelastlinetoberemoved
Chunk normalize(source_str, target_str)→<str> Replacestringswithnormalizedones;
Level ▷source_str<str>,thenoisystringpattern Returnchunkafterreplacement.
▷target_str<str>,thestringforreplacement
keep_chunk()→<str> Returntheorignalchunk.
andreplacingstringsnormalize(),providingflexibilityinmodifyingcontentratherthansimply
droppingthewholedocument. Alsoforhigh-qualitychunksthatdonotrequireanymodifications,
weusethekeep_chunk()functionforflagging. Wepresentthedetailedfunctiondefinitionin
Table1,whichisalsothegenerationspaceofPROX’srefiningmodels. Whiletheindividualfunctions
mayseemstraightforward,theirdesignspaceisflexibleandcapableofexpressingcomplexrules
previouslydevelopedbyhumanexpertsasshowninTable1. Infact,theserulescanbeprojectedinto
theprogramspaceofPROX,showcasingthatourapproachnotonlysimplifiesbutalsoenhancesthe
rule-creationprocess,offeringmoresystematicandscalablerefinementcapabilities.
PROXExecution Duringtheexecutionstage,thegeneratedprogramsnippetsZ willbeexecuted
bytheexecutorE torefinethedocument. Forsimplicityandflexibility,PROXintegratesPythonic
grammars,wrappingalloperationsintodifferentfunctioncallingwithparametersandimplements
these function in Python for later execution. For example, in Figure 2, the document contains
some noisy patterns including navigation bars, meaningless HTML links and page indexes. The
refining model will then generate programs to remove the corresponding lines and patterns. In
thedocument-levelandchunk-levelcleaningstage,PROXutilizesanindependentrefiningmodel
togenerateprogramswithvariousfunctioncallsdescribedinTable1. Webelievethissequential
approachensuresastructuredandeffectiverefinement,addressingthelargerdocumentnoisefirst,
andthenfocusingonfiner-grainedcleaning.
2.3 MODELADAPTATIONFORPROX
Seed Documents LLM Document-Program Pairs
Base Model Refining Model
DDoFc- lPevreol gPrroagmram Fine-tune
ddrroopp__ddooc(c)() Inference
[Scoring Critiques] Synthesize At Scale
[ dF eu ”S f dnc dc eo rt loi ero p ten e_ 1 dD do- oe5 c cfi ( n t fe rit oxio mt:n s] ct or) r:
pus”
Zero-shot / Few-shot
C nD doh rFu romn pPk a- _rl le io dzv eg oe (l c‘r ►Pa ()rmo ©g ’r ,a ’’m
)
Pr Ce otr ra pi on ri ang
pass
Figure3: TheillustrationofthemodeladaptationinPROX.WeemploypowerfulLLMs(LLAMA-3)
toannotaterandomseeddocumentswithvalidprograms,andusethisdoc-programpairstofine-tune
asmallbasemodel,obtainingtherefiningmodelsuitableforfine-graineddatarefiningtasks.
ItisgenerallydifficultforbasemodelstodirectlygeneratePROXprograms. Infact,evenforthe
mostpowerfulpost-trainedLLMs,generatingcustomAPIcallsisrelativelychallengingatthecurrent
stage (Zhuo et al., 2024). Thus, it will be necessary that we curate some seed data to adapt the
modelforthesescenarios. Undersuchconsideration, weemploystrongLLMstoannotatethese
operationsviazero-shotandfew-shotprompting,andthenadaptourbasemodeltothesetasksby
supervisedfine-tuning(SFT).Wefirstusetwoadditivescalescoringprompts(Yuanetal.,2024;
Penedoetal.,2024a)tosplitthecorpusintokeptdocumentsanddroppeddocuments. Andthen
weuselargemodelstoannotatefine-grainedprogramsbasedonkeptdocuments. Specifically,we
leveragetheLLAMA-3seriesofmodels(Dubeyetal.,2024)fordatacollectionandannotation. In
PROX,thisdatacollectionisperformedonlyonce,andallbasemodelsareadaptedwiththesame
4Generative AI Research
curateddata. Toensurethereliabilityofthecollecteddata,wealsoconductnecessarychecksfor
grammarcorrectnessandcontroltheremovalratiothreshold. Thedetailedprocedureforprogram
synthesisandpost-processingcanbefoundin§A.1.
Forsimplicity,wedirectlyuseasmalllanguagemodel(e.g.,0.3Bparameters)thatwehavetrained
onapproximately26Btokensoforiginalunrefineddataasthebasemodel,whichalsoservesasthe
comparisonbaselineinsubsequentexperiments. Theadaptedmodel’sperformanceisthenevaluated
usingtheF1scoreonthesplitvalidationdataset,ensuringarobustassessment. Weselectthehighest-
performingmodelcheckpointandemploythemodeltogenerateprogramsZ,foreachdocument
orchunkofthedataset. Theseprogramstogetherwiththedocumentsarethenexecutedusingthe
correspondingfunctionimplementation,resultinginthefinalprocessedcorpus. Pleaseseeappendix
formoretrainingdetails(§A.2),implementationforcalculatingtheF1score(§A.3),andlargescale
inference(§A.4).
3 EXPERIMENTS
Inthissection,wefirstdescribeourexperimentalsetup,thenverifytheeffectivenessofeachPROX
stageandcompareitwithexistingdataselectionmethodstailoredforpretrainingcorpus(§3.2). We
thenapplyPROXtovariousmodelsizesandcorporatodemonstrateitsbroadapplicability(§3.3).
Finally,weapplyPROXtothemathematicaldomain,demonstratingitssuperiorityanduniversality
indomain-specifictraining(§3.4).
3.1 EXPERIMENTSETUP
TrainingCorpora Weutilizevariouscorporaforbothgeneralandspecificdomaindatainour
experiments.Forgeneraldomaindata,webeginwithRedPajama-V2(Together,2023),apreprocessed
large-scaledatasetof30trilliontokensfromdiverseInternetsources, readyforpre-training. We
furtherapplyPROXontheC4corpus(Raffeletal.,2020)with198billiontokensandtheFineWeb
dataset(Penedoetal.,2024a)containing15trilliontokens,notedforhighdataquality. Forspecific
domainexperiments,weuseOpenWebMath(Pasteretal.,2024),amath-focuseddatasetwith15
billion tokens. Given the limitations in computational resources, we conduct experiments on a
randomlysampledsubsetoftheentirepre-trainingdataset. SeeTable7(§B.2)forsamplingdetails.
BaseModelSelection Ourpre-trainingexperimentsareconductedusingvarioussizesofdecoder-
onlylanguagemodels. Detailedspecificationsofthesemodelsandalltrainingrecipesareprovidedin
§B.3,especiallyinTable8andTable9.
1. Toverifydifferentstages’effectivenessofPROX,weemploya750MsizedmodelsharingLLAMA-
2architecture(Touvronetal.,2023a),denotedasTLM-S,usedforbothpre-trainingfromscratch
andrefining. Wealsocompare PROX withdataselectionmethodsusing PYTHIA-410M/1B’s
architecture(Bidermanetal.,2023),asthoseemployedinMATES(Yuetal.,2024).
2. ForfurtherevaluationofPROXusingdifferentrefiningandbasemodelsizes,wescalethemodel
sizesfrom350M(0.5×smaller,denotedasTLM-XS)and1.7B(2×larger,denotedasTLM-M),
allbasedontheLLAMA-2architecture.
3. Fordomain-specificcontinualpre-training,weselectTINYLLAMA-1.1B(Zhangetal.,2024b),
LLAMA-2(Touvronetal.,2023a),CODELLAMA(Rozièreetal.,2023)andMISTRAL-7B(Jiang
etal.,2023)asrepresentativebasemodelsfortheiradequatetrainingandsolidperformance.
Baselines Toensureafaircomparisonw.r.t. trainingcost,wekeepalltraininghyperparameters,
suchastrainingstepsandbatchsize, consistentacrossbaselines, withonlythedatarefiningand
selectionpipelinesdiffering. WecomparePROXtoaseriesofbaselines:
1. In§3.2,toverifyPROX’seffectiveness,wefirstcomparewithPROXwithregularpre-trainingover
therawRedPajama-V2data. WealsointroduceheuristicbaselinesusedtocuratetheFineWeb
corpora, which is the combination of three filtering strategies from C4 (Raffel et al., 2020),
Gopher(Raeetal.,2021),andnewlycraftedrules(asFineWebrules). Apartfromrule-basedbase-
lines,wealsointroduceexistingdataselectiontechniquesproposedinpreviousworks,including
5Generative AI Research
(1)importanceresampling: DSIR(Xieetal.,2023);(2)model-basedselection:DsDm(Engstrom
etal.,2024),MATES(Yuetal.,2024),andQuRating(Wettigetal.,2024).
2. In§3.3,totestPROXondifferentmodelsizesandtrainingcorpora,wefinallyscaletheTLM-M’s
training tokens to 50B over RedPajama-V2, C4, and FineWeb. To show PROX efficiency, we
thendirectlycomparewithmodelscoveringavarietyofpre-trainingapproachesincluding(1)
large-scalepre-training: TINYLLAMA-1.1B(Zhangetal.,2024b)trainedon3Ttokens;(2)model
pruningfromexistingmodels:(SHEADLLAMA (Xiaetal.,2024)prunedfrom LLAMA-2 and
trainedonextra50Btokens);(3)LLMsynthesis(INSTRUCTIONLM-1.3B(Chengetal.,2024)
trained on MISTRAL-7B generated data and COSMO-1.8B (Ben Allal et al., 2024) trained on
MISTRAL-8x7Bgenerateddata).
3. In § 3.4’s specific domain continual pre-training, apart from standard continual pre-training
on TINYLLAMA-1.1B, LLAMA-2-7B, CODELLAMA-7B,and MISTRAL-7B,weadditionally
introduce with well-known and strong baselines trained on public (or partially public) data,
includingRHO-1(Linetal.,2024),INTERNLM2-MATH(Yingetal.,2024),LLEMMA(Azerbayev
etal.,2024),andaninternalcheckpointreportedinDEEPSEEK-MATH(Shaoetal.,2024).
EvaluationSetup Wecomparethebasemodels’performanceoveravastofdatasetsforcompre-
hensiveevaluation: (1)Forgeneralpre-training,weevaluatetheperformanceacrosstenselected
tasksusinglighteval’simplementation(Fourrieretal.,2023),andreportthezero-shotaccuracy;we
havealsoincludedLM-eval-harness(Bidermanetal.,2024)forfaircomparisonwithdataselection
methods. (2)Fordomain-specificcontinualpre-trainingevaluation,i.e.,mathematicalrelatedbench-
marks,weusethesamenineimplementationandbenchmarksusedinRHO-1(Linetal.,2024)and
evaluateallthebasemodelswithfew-shotchain-of-thought(CoT)examples(Weietal.,2022). The
selectedevaluationbenchmarks,numberofevaluationexamples,andfulldetailscanbefoundin§C.
3.2 VERIFYINGPROX’SEFFECTIVENESS
VerifyingEffectivenessforEachPROXOperation Wefirstconductaseriesofexperimentsto
verifytheeffectivenessofeachPROXoperation. WebeginbytrainingTLM-SontheRedPajama-V2
rawdataforapproximately26Btokens(or12.5Ksteps)astheinitialbaseline. FollowingWettig
etal.(2024)andforconvenience,wethensequentiallyapplythedoc-levelandchunk-levelrefining
pipelinesbyfine-tuningthe0.7Bmodelitself. Wethenperformlarge-scaleprogramsynthesisand
executionusingtherefiningmodels,resultinginD andD .Such2-stagesynthesisrequires
Doc Doc+Chunk
approximately192A100-80GGPUhoursforprocessing60Btokensofdata. Theresultingzero-shot
downstreamperformanceispresentedinTable2,includingbasemodelstrainedonthedataproduced
byPROXrefinementmethodsanddifferentrule-basedfilteringmethods. Moreover,wevisualizethe
dynamicbenchmarkperformanceinFigure4,implyingtheconsistentimprovementofPROXoverall
baselines. See§D.1forfulldetailedresultsofallintermediatecheckpoints.
TheseresultsshowthatPROXishighlyeffective,outperformingtherawcorpuswithanaverageboost
of2.5%,includingsignificantimprovementssuchas7.6%onARC-E,3.3%onHellaSwag,and2.1%
onMMLU.Webelievesuchconsistentperformanceissignificantgiventhattheseimprovements
wereachievedevenonbenchmarksthataretypicallypronetoperformanceinstability,suchasSIQA,
WinoGrande,andCSQA.Bycontrast,rule-basedmethodsdemonstraterelativelymarginaloverall
improvement. Forinstance,Gopherrulesachieveonlya0.2%boost,whileC4showsamodest0.5%
improvement. Furthermore,combiningallthreerules(asisdoneinconstructingtheofficialFineWeb
corpus),doesnotleadtoanylargerenhancementinoverallperformance.
ComparingwithDataSelectionMethods Apartfromcomparingwithheuristicmethods,wealso
includeexistingrepresentativemodel-baseddataselectionmethodstailoredforpertainingcorpus
to verify PROX’s effectiveness in Table 3, where we report both 0-shot and 2-shot performance
underthesamesettingsusedinMATES(Yuetal.,2024). Whilewemerelyapplydocument-level
stage (i.e., PROX-D) which is indeed similar to data selection methods, we can see that PROX
outperformsthestrongestdataselectionmethodMATES,by2.2%and2.5%in0-shotand2-shot
averageperformancefor410Mmodel,andby1.0%and2.0%for1Bmodel. Additionally,PROX
achievesthebestperformanceon7outof8benchmarkstested,demonstratingitssuperiorityover
existingdataselectionmethods. FullevaluationresultsareprovidedinTable11(§D.2).
6Generative AI Research
Table2: Zero-shotperformanceon10selectedtasks. AllmodelsusethesameTLM-Sarchitecture
andaretrainedonRedPajama-V2. Thedoc-level(PROX-D)andchunk-level(PROX-C)refiningare
donebyfine-tuningtherawdatapre-trainedmodelasarefiningmodel. Boldedentriesrepresentthe
bestresults. #Winrepresentsthenumberoftaskswherethemethodachievedthebestperformance.
Method ARC-C ARC-E CSQA HellaS MMLU OBQA PIQA SIQA WinoG SciQ AVG #Win
Raw 26.1 44.3 29.7 39.1 27.3 29.2 66.9 39.0 52.0 67.4 42.1 0/10
Rule-basedfiltering:GO=Gopherrules,C4=C4rules,FW=FineWebrules.
GO 25.7 44.0 31.3 40.2 27.3 29.0 66.3 39.0 51.2 68.9 42.3 0/10
C4 25.0 46.0 31.0 40.5 27.1 29.2 68.5 40.5 51.7 66.6 42.6 2/10
FW 25.2 46.8 32.6 39.6 27.2 29.0 66.5 39.4 52.4 69.2 42.8 2/10
GO+C4+FW 25.2 43.9 30.0 41.9 27.5 31.0 67.0 39.9 51.9 65.3 42.3 0/10
PROX(ours):D=Doc-levelProgramming,C=Chunk-levelProgramming.
PROX-D 26.6 49.7 30.1 40.5 29.4 30.4 66.3 39.0 51.2 71.6 43.5 2/10
PROX-D+C 26.4 51.9 30.9 42.4 29.4 31.6 67.9 40.0 52.2 73.5 44.6 5/10
Table3:Comparisonwithdifferentdataselectionmethodson
44
8benchmarksusingtheC4corpusandPYTHIAarchitecture.
#Winrepresentsthecountofbestperformance. 42
40 Method 0-shot 2-shot #Win
ModelArchitecture: PYTHIA-410M
38 ProX-D+C
Random 42.7 43.8 0/8
ProX-D
36 DSIR(Xieetal.,2023) 42.5 43.7 1/8
Rule
DsDm(Engstrometal.,2024) 43.4 44.1 0/8
34 Raw QuRating(Wettigetal.,2024) 43.5 44.6 0/8
MATES(Yuetal.,2024) 44.0 45.0 0/8
0.0 2.5 5.0 7.5 10.0 12.5
Training Step (K) PROX(ours) 46.2 47.5 7/8
ModelArchitecture: PYTHIA-1B
Figure4: Downstreamzero-shotper-
formance w.r.t. different training Random 44.7 45.4 0/8
steps: first 0.5K, then evenly from MATES(Yuetal.,2024) 45.8 46.4 1/8
2.5K to 12.5K. Rule: the best per- PROX(ours) 46.8 48.4 7/8
formingFineWebruleinTable2.
3.3 APPLYINGPROXACROSSMODELSIZESANDPRETRAININGCORPORA
Inthissection,wedemonstratethatPROXcaneffectivelybenefitmodelsbeyondscaleandacross
differentcorpora,showingpotentialforiterativepre-trainingimprovements.
PROXworkswellacrossdifferentscales. Wetrainafamilyofmodelsfrom350Mto1.7B(i.e.,
TLM-XS, TLM-S,and TLM-M)onthesame26Btokensusedin§3.2,andthenfine-tunethese
modelsondoc-levelandchunk-leveltasks,obtainingrefiningmodelswithdifferentsizes. Wethen
applythesemodelsindoc-levelrefiningandchunk-levelrefiningstages,andusethecurateddata
forfrom-scratchpre-training. WereportinTable4theadaptationperformanceonrefiningtasksof
differentrefiningmodelsizes. Accordingtothevalidationperformance,adaptingPROXworkswell
acrossallmodelsizes,allachieving80%F1ondoc-levelrefinement,and75%F1onchunk-level
refinement. Wefurthertrainthesemodelsofdifferentsizesfromscratchusingdataproducedby
refiningmodelsofvaryingsizes. InFigure5,theresultsindicatethatrefiningmodelsofallsizes
helpimproveperformanceoverrawdata,withaconsistentabsolutegapof2%overallbasemodel
sizes. WhileinFigure5,TLM-XScurateddatashowsslightlybetterdownstreamperformance,it
hasasignificantlylowertoken-levelretentionratio(23.2%vs. 28.8%)comparedtolargermodels
as reflected in Table 4. This implies that moderately larger models suggest a favorable balance
betweendataqualityandquantity. Theseadditionaltokenslikelyprovidemoreknowledgeduring
7
)%(
ecnamrofreP
egarevAGenerative AI Research
Our From-Scratch Experiments Other Approaches
Table 4: Refining model’s perfor- 52
manceonvalidsetandtokenreten- Raw Data ProX LLM Synthesis Model Pruning
tionratiooforiginalcorpus. 50 49.8 49.7 50.3
TinyLlama-1.1B-3T: 50.1% 49.2
Size Doc-levelChunk-levelKeptRatio 48.4
48.0
48
TLM-XS 82.6 75.2 23.2% 47.4
TLM-S 81.3 75.6 25.6%
TLM-M 83.7 77.3 28.8% 46 46.0 45.5
Raw ProX-(xs)ProX-(s)ProX-(m)
44
46
39.6 42.3 41.9 41.9
42 44 RedPajama C4 FineWeb Inst-LM Cosmo S-Llama
42.5 43.9 44.6 43.5
42 Figure 6: Performance of original data and PROX curated
data trained models across different datasets using ≈ 50B
43.4 46.0 46.2 45.7
40 tokens and comparison with existing models trained using
different techniques. Inst-LM represents INSTRUCTIONLM-
Figure5: PROX’seffectoverdiffer-
1.3B; Cosmo represents COSMO-1.8B; S-Llama represents
entmodelsizes.
SHEAREDLLAMA-1.3B.
pre-trainingwithoutcompromisingdownstreambenchmarkperformance,showcasinganeffective
trade-offbetweendatarefinementandinformationpreservation.
PROXworkswellacrosspre-trainingcorpora. ToassesstheapplicabilityofPROXacrossvarious
pre-trainingcorpora,weextendourexperimentsbeyondRedPajama-V2toincludeC4(Raffeletal.,
2020),andtherecentlyreleased15-trillion-tokenpre-trainingcorpus,FineWeb(Penedoetal.,2024a).
Forconsistency,weapplyexactlythesamePROX-xsrefiningmodelsdetailedinTable4tothese
corporawithoutconstructingnewSFTdataforeachcorpus. Weconductedlarger-scaleexperiments
bytrainingourmodelonapproximately50billiontokens,againachievingnotableimprovements.
Ontendownstreambenchmarks,modelstrainedonourmethod’scurateddatashowedimprovements
of+2.0%onRedPajama-V2,+3.1%onC4,and+2.4%onFineWeb.
ProXtrainslanguagemodelswithmuchgreaterefficiency. Todemonstratethenon-trivialnature
oftheseresults,wecomparedmodelstrainedonPROXcurateddataagainstvariousmodelstrainedby
differentapproaches. TheseincludemodelslikeTINYLLAMA-1.1B-3T(traineddirectlyon3trillion
tokens,about60×ofourtrainingtokensand40×trainingFLOPs),SHEADLLAMA-1.3B(denoted
asS-Llama,aprunedversionofLLAMA-2-7B,withextratrainingon50billiontokens),andmodels
usingLLMdatasynthesis,suchasINSTRUCTIONLM-1.3B(denotedasInst-LM)andCOSMO-1.8B.
Ourresults,includingTLM-M(PROX)andTLM-M(Raw),arepresentedalongsideallthesebaselines
inFigure6. OnFineWeb,whichisrecognizedforitshigh-qualitydata,TLM-MusingPROX-refined
data performs comparably to pruned models like SHEADLLAMA-1.3B and TINYLLAMA-1.1B,
despitetheirrelianceonadditionalpruningtechniquesormuchlargerdatasets.Moreover,usingmuch
lessinference-timecomputingoverhead,ourmodelsurprisinglyoutperformsmodelsthatrelyheavily
onLLMdatasynthesis,underscoringPROX’sefficiency. Notably,modelslikeINSTRUCT-LM-1.3B,
trainedon100billiontokensleveragingafine-tunedMISTRAL-7Bsynthesizer,andCOSMO-1.8B,
trainedon180billiontokens(including25billiontokenssynthesizedbyMISTRAL-8x7B),require
significantlymorecomputationalresourcesthanPROX.
3.4 APPLYINGPROXTODOMAIN-SPECIFICCONTIUALPRERAINING
Wealsodemonstratethepotentialof PROX inthecontinualpre-trainingscenario,specifically,in
themathematicaldomain. Weapplytheverysamepipelineasingeneraldomainstothealready
cleanedOpenWebMathcorpus(Pasteretal.,2024),aimingtofurtherrefineandminethehighquality
and clean data from the vast web pages crawled in it. We then adapt and apply PROX-xs series,
whichwasinitiallytrainedongeneraltextasdescribedin§3.3,andfurtheradaptedonmathtext
forthedoc-levelandchunk-levelrefiningtasks. Finally,weobtainabout5.5Btokensleftafterthe
8
eziS
ledoM
esaB
sx
s
m
ecnamrofreP
)%(
ecnamrofreP
egarevAGenerative AI Research
Table 5: OpenWebMath Continual Pre-training (CPT) Results. All models are tested using few-
shot CoT prompts. LLEMMA and INTERNLM2-MATH are continual pre-trained models from
CODELLAMAandINTERNLM2(Team,2023)withpublicavailabledata,respectively. DEEPSEEK-
LLM denotes an internal DeepSeek model, and the model trained on OpenWebMath introduced
byShaoetal.(2024). Notethattheuniquetokensandtrainingtokensinthecolumnreferexclusively
tothetokennumbersfrommath-specificcorpora(calculatedbycorrespondingtokenizers). †: MQA
evaluationofINTERNLM2-BASEisbasedonanalternativepromptduetonon-predictionissueswith
theoriginalprompt. Theboldedentriesrepresentthebestresultswithinthesamebasemodel.
Uniq Train MMLU SAT
Model Size Method GSM8K MATH SVAMP ASDiv MAWPS TAB MQA AVG
Toks Toks STEM MATH
ExistingContinualPre-trainingforReference
1.3B - - - 2.9 3.0 - - - - - 19.5 15.6 -
DEEPSEEK-LLM
1.3B - 14B 150B 11.5 8.9 - - - - - 29.6 31.3 -
7B - - - 11.8 5.0 44.2 50.7 62.6 30.6 14.3 20.4 21.9 29.1
CODELLAMA(Base) 34B - - - 31.8 10.8 61.9 66.0 83.4 51.6 23.7 43.0 53.1 47.3
7B - 55B 200B 38.8 17.2 56.1 69.1 82.4 48.7 41.0 45.4 59.4 50.9(+21.8)
LLEMMA
34B - 55B 50B 54.2 23.0 67.9 75.7 90.1 57.9 49.8 54.7 68.8 60.1(+12.8)
7B - - - 27.0 6.6 49.0 59.3 74.8 40.1 20.9† 19.0 28.1 36.1
INTERNLM2-BASE
20B - - - 50.6 18.8 72.5 75.9 93.9 45.4 33.1 53.7 59.4 55.9
7B - 31B 125B 41.8 14.4 61.6 66.8 83.7 50.0 57.3 24.8 37.5 48.7(+12.6)
INTERNLM2-MATH
20B - 120B 500B 65.4 30.0 75.7 79.3 94.0 50.9 38.5 53.1 71.9 62.1(+6.2)
ApplyingDataRefinementApproaches
TINYLLAMA(Base) 1.1B - - - 2.8 3.2 10.9 18.0 20.2 12.5 14.6 16.4 21.9 14.7
1.1B - 15B 15B 6.2 4.8 22.3 36.2 47.6 19.3 11.6 20.7 25.0 21.5(+8.1)
1.1B RHO 15B 9B∗1 7.1 5.0 23.5 41.2 53.8 - 18.0 - - -
TINYLLAMA(CPT)
1.1B Rule 6.5B 15B 4.5 2.8 17.5 29.4 39.3 15.1 12.4 19.4 25.0 18.4(+3.7)
1.1B PROX 5B 15B 9.0 5.6 23.8 41.9 56.9 22.2 15.6 26.8 31.2 25.7(+11.0)
LLAMA-2(Base) 7B - - - 14.1 3.8 39.5 51.6 63.6 30.9 12.5 32.9 34.4 31.5
7B - 15B 10B 29.6 13.6 49.2 61.9 78.4 36.3 31.9 40.5 43.8 42.8(+11.3)
LLAMA-2(CPT) 7B PROX 5B 10B 30.6 16.8 50.2 63.7 79.3 37.3 40.1 43.8 53.1 46.1(+14.6)
CODELLAMA(Base) 7B - - - 11.8 5.0 44.2 50.7 62.6 30.6 14.3 20.4 21.9 29.1
7B - 15B 10B 31.1 14.8 51.4 62.1 81.2 33.6 30.4 40.5 43.8 43.2(+14.1)
CODELLAMA(CPT) 7B PROX 5B 10B 35.6 17.6 55.8 67.9 82.7 41.3 38.9 42.6 62.5 49.4(+20.3)
MISTRAL(Base) 7B - - - 40.6 11.4 65.4 68.5 87.0 52.9 32.3 50.0 56.2 51.6
7B - 15B 10B 44.4 19.2 65.2 69.6 88.4 46.6 43.1 50.8 65.6 54.8(+3.2)
MISTRAL(CPT) 7B PROX 4.7B 10B 51.0 22.4 64.9 72.9 89.2 49.8 53.0 54.2 75.0 59.2(+7.6)
document-levelcleaningstageandabout4.7Btokensleftafterthechunk-levelrefiningstage. We
presentthefinalmathematicalevaluationresultsofmodelstrainedontherefinedOpenWebMathin
Table5,withfullevaluationresultspresentedin§D.4.
PROXboostsmathcontinualpre-trainingefficiencyvastly. Withoutanydomain-specificdesign,
Table5showsthatpre-trainingonOpenWebMathrefinedbyPROXbrings11.0%averageperformance
improvementsforbaseTINYLLAMA-1.1B,14.6%forbaseLLAMA-2,20.3%forbaseCODELLAMA,
7.6%forbaseMISTRAL,whichclearlyexceedtheimprovementsofallbaselines,includingtheir
counterpartspre-trainedontheoriginalcorpus,underthesamesettings. Itisalsoworthnoticingthat,
applyingtherule-basedfilteringmethoddoesnotbringimprovements;instead,itleadstoa3.1%
performancedegradationcomparedtocontinualpre-trainingontheoriginalcorpus. Thisfinding
impliesthattherearenouniversalworkableheuristicsforalldomains,highlightingthedemandsfor
automatedpipelinesjustlikePROX.Moreover,comparedwithsomeexistingstate-of-the-artmath
continualpre-trainingmodelslikeLLEMMAandINTERNLM2-MATHtypicallyrequiringhundredsof
billionsoftokenscontinualpre-training,ourPROXdemonstratesremarkableefficiencygains.Amore
controlledcomparisonfurtherhighlightsthisefficiency: LLEMMA-7B,basedonCODELLAMA-7B,
wastrainedon200Btokens,whereasourPROX,alsostartingfromCODELLAMA-7B,reachessimilar
performance levels with just 10B tokens of training, indicating a 20 times reduction in training
computes. Theseresultssuggestthatourapproachmaycontributetomoreefficientandaccessible
development of LLMs and could offer a new perspective in domain-specific model adaptation,
potentiallyenhancinghowtoaddressspecializedLLMinresource-constrainedsettings.
1RHO-1onlycountstheselectedtokensthatareusedfortraining(losscalculation).
9Generative AI Research
4 ANALYSIS
4.1 IMPACTONTHEORIGINALDATA
WhatchangesoccurinthecorporaafterapplyingPROX?Wecomparethedocumentlengthdistribu-
tionoftheoriginalcorpuswiththatofthePROX-refinedcorpusinFigure7. Inthegeneraldomain
corpora(RedPajama-V2,C4,andFineWeb),thedatarefinedby PROX exhibitsanoticeableshift
intheaveragenumberoftokensperdocument. Forinstance, inRedPajama-V2, weobservethat
documentswithfewerthan100tokensmakeupasignificantportionofthecorpus. Afterapplying
the PROX, the majority of documents contain more than 200 tokens, with an average number of
tokensperdocumentincreasingfrom1217toover2000. Thissuggeststhatveryshortdocuments
maybenoisyandlacksufficientmeaningfulinformationtobesuitableforpre-training. Thisshift,
however,isnotobservedinOpenWebMath,wheretheaveragenumberoftokensperdocumentis
already larger. One possible reason for this outlier is that the OpenWebMath corpus is collected
mostlyfromsourcesdifferentfromthegeneraldomain,e.g.,onlineforumslikeStackExchange,and
academicpublisherwebsitessuchasarXiv. Thenoisesofthesesourcescanbequitedifferentfrom
generaldomains. Furthercasestudiesonthesedocumentsareprovidedin§E.1.
Original Data Original Data Original Data Original Data
ProX Refined Data ProX Refined Data ProX Refined Data ProX Refined Data
Avg.Toks: 1217.5 Avg.Toks: 472.3 Avg.Toks: 674.6 Avg.Toks: 1815.8
Avg.Toks:2004.8 Avg.Toks: 1027.2 Avg.Toks: 1253.4 Avg.Toks: 1734.9
101 102 103 104 (#toks) 101 102 103 104(#toks) 102 103 104(#toks) 102 103 104 (#toks)
RedPajama-V2 C4 FineWeb OpenWebMath
Figure7: Comparisonofdoc’stokenlengthdistributionsbetweenoriginalandPROX-refineddata.
4.2 COMPUTINGOVERHEADANALYSIS
Although PROX demonstratespromisingresultsindownstreamtasks, itisimportanttoacknowl-
edge that large-scale model inference still requires a substantial computing budget. For exam-
ple, as mentioned in § 3.2, and in Table 7, the RedPajama-V2 corpus used for training TLM-S
was refined from about 60B raw tokens. As calculated in § E.2, if we utilize PROX-XS for
both two refining stages, the additional computational overhead will amount to approximately
C = 5 × 1019 FLOPs, which is equivalent to training an additional 12B tokens on TLM-
S and 5B tokens on TLM-M. It is noteworthy that this overhead ratio keeps decreasing as
model size increases, meaning that the relative computational cost diminishes for larger models.
In Figure 8, we compare the FLOPs consumed by 2.26
Train FLOPs
checkpointswithsimilarperformance,bothwithand 2.0
withoutapplyingPROX,acrossthreedifferentmodel Infer FLOPs
sizes. As the model size increases, the proportion 1.5 1.35
ofinferenceFLOPsrequiredforapplyingPROXde-
1.0
creases. Forthe0.7Bmodel,thetotalFLOPswhenus-
0.72 0.69
ingPROXarealreadylowerthanwithoutit(6.3×1e19 0.5 0.42 0.43
vs. 6.7×1e19). Notably,forthelargest1.7Bmodel,
weachieveperformancecomparabletoamodelpre- 0.0
0.3 0.7 1.7
trainedontheoriginaldata,butwithonly58%ofthe Model Parameters (B)
totalFLOPs. Thisdemonstratesthatrefiningmethods
Figure8: FLOPscomparisonforcompara-
likePROXnotonlyenhancesdataqualitybutalsobe-
ble downstream performance with/without
comesmorecomputationallyefficientasmodelsizes
PROXrefining: 0.3B(Avg.Perf=40.5),0.7B
grow, reinforcing the value of allocating additional
(41.6),and1.7B(42.9).2
resourcestorefiningpre-trainingdata.
2ThetrainFLOPsforthebasemodel(approximately5.3×1019)usedtocreatetherefiningmodelare
excluded. Thisisbecauseanypre-trainedLLMcantheoreticallyserveasthebaseforrefinement. Thisalso
reflectsPROX’sflexibility.
10
ytisneD
)02e1×(
sPOLF
refnI
+
niarTGenerative AI Research
5 RELATED WORKS
Pre-trainingDataProcessing Rawdatacollectedfrompublicsources(e.g.,CommonCrawl)are
noisy,anddirectlyusingthesedatacangreatlyhurtmodelperformance;thus,ithasbeenacommon
practicetoexecuteextensivepre-processingbeforepre-training(Touvronetal.,2023b;Together,
2023;Penedoetal.,2024a). Thepipelineusuallystartswithdocumentpreparation,whichincludes
URLfiltering,textextraction,language-basedfiltering(Smithetal.,2022). Theremainingdocument
willthenundergoseveralqualitycheckswithheuristicruleslikeoveralllength,symbol-to-wordratio,
andothercriteriatodeterminewhetheritiskept,partiallyorfullyaborted(Zhangetal.,2024a;Dou
etal.,2024;Qiuetal.,2024). Finally,thesedocumentsarededuplicatedusingdifferentmatching
methods,e.g.,fuzzymatchlikeMinHash(Broder,1997),orexactsequencesmatches(Penedoetal.,
2024b). InPROX,weusesthelanguagemodelforfurtherdatarefining,outperformingheuristicrules
withacceptablecomputationaloverhead.
DataSelectionMethods Dataselection,slightlydistinctfromdataprocessing,ismorecommonly
applied in the later stages of large-scale data pre-processing. In supervised fine-tuning (SFT), it
typicallyinvolvesselectingamuchsmallersubsetofsamplestominimizetuningoverheadwhile
maintainingperformance(Liuetal.,2024b). Recenteffortshaveextendedtheseselectionstrategies
tothepre-trainingstage(Engstrometal.,2024;Xieetal.,2023;Ankneretal.,2024;Sachdevaetal.,
2024;Liuetal.,2024c). Forinstance,Wettigetal.(2024)trainaratermodeltoscoredocumentson
fourqualitycriteriainSlimPajama(Sobolevaetal.,2023)andconductpre-trainingonaresampled
subsetbasedonscores. MATES(Yuetal.,2024)applyasmallermodelforestimatingdatainfluence
duringpre-training,enablingdynamicdataselectionschema. Moreover,asmentionedinLLAMA-
3(Meta,2024),LLAMA-2models(Touvronetal.,2023a)wasusedastext-qualityclassifiersthat
underpin LLAMA-3’strainingdata. Insteadofmerelyselectingdocuments, PROX enablesmore
fine-grainedoperationswithindocuments,contributingtofurtherperformanceimprovements.
Model-based Data Synthesizing Another branch of research focuses on editing or rephrasing
existingdatawithmodelstoimprovethedataquality. Fanetal.(2024)useChatGPTtorephrase
severalinstruction-tuningdatasetsforaclearformatbasedonmassivescenario-basedcriteria. Yue
et al. (2024) use LLMs to extract and refine 5M QA pairs from web documents, obtaining 10M
instruction-responsepairs. Synthesistechniqueshavealsobeenappliedinthepre-trainingphasesuch
asthePHIseries(Gunasekaretal.,2023;Lietal.,2023). Recently,Mainietal.(2024)andCheng
etal.(2024)utilizeoff-the-shelfinstruction-tunedmodelstoparaphrasewebdocumentsinspecific
stylessuchasQA,andmixthesesyntheticrephraseswithrealdatainpre-training. BenAllaletal.
(2024)furthersynthesizefrommereseedtopics,bypromptingLLMstogeneratepre-trainingsamples
in a cleaner format like textbooks. However, despite its success, it typically requires substantial
computationtosynthesizeapre-training-scalecorpus,andmorecritically,itinevitablyinheritsflaws
fromtheadvancedmodel,alsosufferingfromhallucinationissues(Liuetal.,2024a). Inthiswork,
wefocusonleveraginglanguagemodelstoliftdataqualitythroughthesynthesisofexecutableand
interpretableprograms,ratherthandirectlygeneratingdata. WedemonstratethatPROXcouldclearly
improvedataqualityatscaleonlywithacceptableextracomputing.
InferenceTimeScaling Recenttrendsinlanguagemodelshavebeguntoexplorethepotentialof
allocatingadditionalcomputingatinferencetime,complementingtheextensivecomputationsalready
deviatedtothepre-trainingandpost-trainingphases. Severalstudieshavedemonstratedthepotential
ofthisapproach, showingthatsmallerlanguagemodelsequippedwithadditionalinference-time
computingcanperformcomparablyto,orevenoutperform,significantlylargermodels,evidenced
acrossvariousdomains,includingcodegeneration(Hassidetal.,2024;Brownetal.,2024),andmath
problem-solving(Snelletal.,2024;Wuetal.,2024). Thesignificanceofthisapproachhasbeen
furthercorroboratedbyOpenAI’slatesto1modelrelease(OpenAI,2024). Whilethesestudiesfocus
onscalingcomputingontesttime,ourworkdemonstratesanalternativeperspectiveoninference
computingscaling. Weadvocateforallocatingcomputingtorefinepre-trainingcorpora,particularly
giventhatInternet-basedcorporahavebeenextensivelyutilizedinlanguagemodelpre-training. Our
proposedPROXdemonstratesremarkablegainsinpre-trainingefficiencybyinvestingmoderately
additionalcomputeinthecorpusrefinement,facilitatingmoreefficientandaccessibledevelopment
ofLLMs.
11Generative AI Research
6 CONCLUSION
Weintroduced PROX,aframeworkthatuseslanguagemodelstorefinepre-trainingdataatscale
through program generation. Our extensive experiments show that PROX curated data improves
modelperformancebyover2%onvariousdownstreambenchmarksandiseffectiveacrossdifferent
modelsizesandpre-trainingdatasets. Fordomain-specificcontinualpre-training,modelstrainedon
PROXcuratedtokensalsoyieldsignificantimprovementsin20×fewertokens,andcomparableto
state-of-the-artmodelstrainedon200Btokens. FurtheranalysisalsoimpliesapplyingPROXcan
achievesimilarresultswithlesscomputingpowerforlarge-scaleLLMpre-training. Insummary,
theseresultsdemonstratePROX’spotentialforgreatlyimprovingdataqualityandreducingcostsin
languagemodeltraining.
7 IMPLICATIONS AND FUTURE DIRECTIONS
ThestrongresultsfromPROXhighlightthepotentialofautomateddatarefinementtosignificantly
improvemodelperformancewhilereducingcomputationalcosts. Byrefiningdatamoreeffectively,
PROXopensnewpossibilitiesforimprovingtrainingefficiencyandachievingbetterresultsacross
a range of benchmarks. Looking ahead, these results suggest several future directions. First,
incorporatingadditionalrefiningoperationslikereformattingandrephrasingcouldfurtherenhance
dataquality.Second,improvingefficiencybyreducingmodelsizeandapplyinginferenceacceleration
techniques is a key goal. Expanding PROX to domains like code and multilingual data is also
promising. Scalingupwithmorecomputationalresourceswillallowforathoroughevaluationofits
potential. Finally,webelievethatprioritizingdatarefinementbeforepre-trainingcangreatlyimprove
trainingefficiency,andweencouragecontinuedexplorationinthisarea.
ACKNOWLEDGEMENT
We extend our profound gratitude to Shanghai AI Lab and Sea AI Lab for generously providing
valuablecomputationalresources,whichwereinstrumentalintherealizationofthisproject. Our
sincerethanksalsogotoMingxuanWangandJiazeChenfromByteDancefortheircrucialsupport.
WearedeeplythankfultoEthanChernfromShanghaiJiaoTongUniversityandYuqingYangfrom
UniversityofSouthernCaliforniafortheirearlydiscussionsandinsightfulcontributions,andequally
gratefultoZhoujunChengfromUCSanDiego, YihengXuandTianbaoXiefromUniversityof
HongKong,andTerryYueZhuofromMonashUniversityfortheirvaluablefeedback,toGuilherme
PenedoandLoubnaBenAllalfromHuggingFacefortheirguidanceonhyper-parametertuning,to
ZhibinGoufromTsinghuaUniversityforprovidingadviseoncontinualpre-training,toLyumanshan
Ye for helping with illustrations and color scheme design. Finally, special thanks go to Peiyuan
ZhangfromUCSanDiego,representingtheTinyLlamateam,forprovidingagreatopenpre-training
frameworkandsupportingseriesofaccelerationoperators. Thesecollectivewisdomandunwavering
supporthavebeenpivotaltoourproject. ThisprojectissupportedbySJTUSEIEE-ByteDance
LargeLanguageModelJointLaboratory,ShanghaiArtificialIntelligenceLaboratory.
12Generative AI Research
REFERENCES
Meta. Introducing meta llama 3: The most capable openly available llm to date, 2024. URL
https://ai.meta.com/blog/meta-llama-3.
JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technicalreport.
arXivpreprintarXiv:2303.08774,2023.
AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-
3 Model Card, 2024. URL https://www-cdn.anthropic.com/
de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf.
MachelReid,NikolaySavinov,DenisTeplyashin,DmitryLepikhin,TimothyLillicrap,Jean-baptiste
Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini
1.5: Unlockingmultimodalunderstandingacrossmillionsoftokensofcontext. arXivpreprint
arXiv:2403.05530,2024.
AnnYuan, AndyCoenen, EmilyReif, andDaphneIppolito. Wordcraft: storywritingwithlarge
languagemodels. In27thInternationalConferenceonIntelligentUserInterfaces,pages841–852,
2022.
JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,Denny
Zhou,etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels. Advancesin
neuralinformationprocessingsystems,35:24824–24837,2022.
TakeshiKojima,ShixiangShaneGu,MachelReid,YutakaMatsuo,andYusukeIwasawa. Large
languagemodelsarezero-shotreasoners. Advancesinneuralinformationprocessingsystems,35:
22199–22213,2022.
LinxiFan,GuanzhiWang,YunfanJiang,AjayMandlekar,YuncongYang,HaoyiZhu,AndrewTang,
De-AnHuang,YukeZhu,andAnimaAnandkumar. Minedojo: Buildingopen-endedembodied
agentswithinternet-scaleknowledge. AdvancesinNeuralInformationProcessingSystems,35:
18343–18362,2022.
JoonSungPark,JosephO’Brien,CarrieJunCai,MeredithRingelMorris,PercyLiang,andMichaelS
Bernstein. Generativeagents: Interactivesimulacraofhumanbehavior. InProceedingsofthe36th
AnnualACMSymposiumonUserInterfaceSoftwareandTechnology,pages1–22,2023.
Together. Redpajama: an open dataset for training large language models, October 2023. URL
https://github.com/togethercomputer/RedPajama-Data.
Guilherme Penedo, Hynek Kydlícˇek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro
VonWerra,ThomasWolf,etal. Thefinewebdatasets: Decantingthewebforthefinesttextdataat
scale. arXivpreprintarXiv:2406.17557,2024a.
JackWRae,SebastianBorgeaud,TrevorCai,KatieMillican,JordanHoffmann,FrancisSong,John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:
Methods,analysis&insightsfromtraininggopher. arXivpreprintarXiv:2112.11446,2021.
LucaSoldaini,RodneyKinney,AkshitaBhagia,DustinSchwenk,DavidAtkinson,RussellAuthur,
Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Jha,
SachinKumar, LiLucy, XinxiLyu, NathanLambert, IanMagnusson, JacobMorrison, Niklas
Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Abhilasha Ravichander, Kyle
Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan Walsh,
LukeZettlemoyer,NoahSmith,HannanehHajishirzi,IzBeltagy,DirkGroeneveld,JesseDodge,
and Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining
research. InLun-WeiKu,AndreMartins,andVivekSrikumar,editors,Proceedingsofthe62nd
AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pages
15725–15788,Bangkok,Thailand,August2024.AssociationforComputationalLinguistics. URL
https://aclanthology.org/2024.acl-long.840.
13Generative AI Research
GeZhang,ScottQu,JiahengLiu,ChenchenZhang,ChenghuaLin,ChouLeuangYu,DannyPan,
EstherCheng,JieLiu,QunshuLin,etal. Map-neo: Highlycapableandtransparentbilinguallarge
languagemodelseries. arXivpreprintarXiv:2405.19327,2024a.
SangMichaelXie,ShibaniSanturkar,TengyuMa,andPercySLiang. Dataselectionforlanguage
models via importance resampling. Advances in Neural Information Processing Systems, 36:
34201–34227,2023.
AlexanderWettig,AatmikGupta,SaumyaMalik,andDanqiChen. QuRating: Selectinghigh-quality
datafortraininglanguagemodels. InInternationalConferenceonMachineLearning(ICML),
2024.
Zichun Yu, Spandan Das, and Chenyan Xiong. Mates: Model-aware data selection for efficient
pretrainingwithdatainfluencemodels. arXivpreprintarXiv:2406.06046,2024.
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,Aiesha
Letman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3herdofmodels.
arXivpreprintarXiv:2407.21783,2024.
SuriyaGunasekar,YiZhang,JyotiAneja,CaioCésarTeodoroMendes,AllieDelGiorno,Sivakanth
Gopi,MojanJavaheripi,PieroKauffmann,GustavodeRosa,OlliSaarikivi,etal. Textbooksareall
youneed. arXivpreprintarXiv:2306.11644,2023.
YuanzhiLi,SébastienBubeck,RonenEldan,AllieDelGiorno,SuriyaGunasekar,andYinTatLee.
Textbooksareallyouneedii: phi-1.5technicalreport. arXivpreprintarXiv:2309.05463,2023.
Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von
Werra. Cosmopedia, February 2024. URL https://huggingface.co/datasets/
HuggingFaceTB/cosmopedia.
PratyushMaini,SkylerSeto,HeBai,DavidGrangier,YizheZhang,andNavdeepJaitly. Rephras-
ing the web: A recipe for compute and data-efficient language modeling. arXiv preprint
arXiv:2401.16380,2024.
RuiboLiu,JerryWei,FangyuLiu,ChengleiSi,YanzheZhang,JinmengRao,StevenZheng,Daiyi
Peng, Diyi Yang, Denny Zhou, et al. Best practices and lessons learned on synthetic data for
languagemodels. arXivpreprintarXiv:2404.07503,2024a.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
Zhou,WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-text
transformer. Journalofmachinelearningresearch,21(140):1–67,2020.
KeiranPaster,MarcoDosSantos,ZhangirAzerbayev,andJimmyBa.Openwebmath:Anopendataset
of high-quality mathematical web text. In The Twelfth International Conference on Learning
Representations,2024. URLhttps://openreview.net/forum?id=jKHmjlpViu.
TerryYueZhuo,MinhChienVu,JennyChim,HanHu,WenhaoYu,RatnadiraWidyasari,Imam
NurBaniYusuf,HaolanZhan,JundaHe,IndraneilPaul,etal. Bigcodebench: Benchmarkingcode
generationwithdiversefunctioncallsandcomplexinstructions. arXivpreprintarXiv:2406.15877,
2024.
WeizheYuan,RichardYuanzhePang,KyunghyunCho,SainbayarSukhbaatar,JingXu,andJason
Weston. Self-rewardinglanguagemodels. arXivpreprintarXiv:2401.10020,2024.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundation
andfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023a.
StellaBiderman,HaileySchoelkopf,QuentinGregoryAnthony,HerbieBradley,KyleO’Brien,Eric
Hallahan,MohammadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,EdwardRaff,etal.
Pythia: Asuiteforanalyzinglargelanguagemodelsacrosstrainingandscaling. InInternational
ConferenceonMachineLearning,pages2397–2430.PMLR,2023.
14Generative AI Research
PeiyuanZhang, GuangtaoZeng, TianduoWang, andWeiLu. Tinyllama: Anopen-sourcesmall
languagemodel. arXivpreprintarXiv:2401.02385,2024b.
BaptisteRozière,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,XiaoqingEllenTan,Yossi
Adi,JingyuLiu,TalRemez,JérémyRapin,ArtyomKozhevnikov,IvanEvtimov,JoannaBitton,
ManishBhatt,CristianCanton-Ferrer,AaronGrattafiori,WenhanXiong,AlexandreDéfossez,Jade
Copet,FaisalAzhar,HugoTouvron,LouisMartin,NicolasUsunier,ThomasScialom,andGabriel
Synnaeve. Codellama: Openfoundationmodelsforcode. CoRR,abs/2308.12950,2023. doi: 10.
48550/ARXIV.2308.12950. URLhttps://doi.org/10.48550/arXiv.2308.12950.
AlbertQJiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,DevendraSinghChaplot,
DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,LucileSaulnier,etal.
Mistral7b. arXivpreprintarXiv:2310.06825,2023.
LoganEngstrom,AxelFeldmann,andAleksanderMadry. Dsdm: Model-awaredatasetselection
withdatamodels. arXivpreprintarXiv:2401.12926,2024.
MengzhouXia,TianyuGao,ZhiyuanZeng,andDanqiChen. Shearedllama: Acceleratinglanguage
modelpre-trainingviastructuredpruning. InTheTwelfthInternationalConferenceonLearning
Representations,2024.
DaixuanCheng,YuxianGu,ShaohanHuang,JunyuBi,MinlieHuang,andFuruWei. Instruction
pre-training:Languagemodelsaresupervisedmultitasklearners.arXivpreprintarXiv:2406.14491,
2024.
ZhenghaoLin,ZhibinGou,YeyunGong,XiaoLiu,YelongShen,RuochenXu,ChenLin,Yujiu
Yang, Jian Jiao, Nan Duan, et al. Rho-1: Not all tokens are what you need. arXiv preprint
arXiv:2404.07965,2024.
HuaiyuanYing,ShuoZhang,LinyangLi,ZhejianZhou,YunfanShao,ZhaoyeFei,YichuanMa,
JiaweiHong,KuikunLiu,ZiyiWang,etal. Internlm-math: Openmathlargelanguagemodels
towardverifiablereasoning. arXivpreprintarXiv:2402.06332,2024.
ZhangirAzerbayev,HaileySchoelkopf,KeiranPaster,MarcoDosSantos,StephenMarcusMcAleer,
AlbertQ.Jiang,JiaDeng,StellaBiderman,andSeanWelleck. Llemma: Anopenlanguagemodel
formathematics. InTheTwelfthInternationalConferenceonLearningRepresentations,2024.
URLhttps://openreview.net/forum?id=4WnqRR915j.
ZhihongShao,PeiyiWang,QihaoZhu,RunxinXu,JunxiaoSong,MingchuanZhang,YKLi,YuWu,
andDayaGuo. Deepseekmath: Pushingthelimitsofmathematicalreasoninginopenlanguage
models. arXivpreprintarXiv:2402.03300,2024.
ClémentineFourrier,NathanHabib,ThomasWolf,andLewisTunstall. Lighteval: Alightweight
framework for llm evaluation, 2023. URL https://github.com/huggingface/
lighteval.
Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi,
AlhamFikriAji,PawanSasankaAmmanamanchi,SidneyBlack,JordanClive,AnthonyDiPofi,
JulenEtxaniz,BenjaminFattori,JessicaZosaForde,CharlesFoster,MimansaJaiswal,WilsonY.
Lee, Haonan Li, Charles Lovering, Niklas Muennighoff, Ellie Pavlick, Jason Phang, Aviya
Skowron,SamsonTan,XiangruTang,KevinA.Wang,GentaIndraWinata,FrançoisYvon,and
AndyZou. Lessonsfromthetrenchesonreproducibleevaluationoflanguagemodels,2024.
InternLMTeam. Internlm: Amultilinguallanguagemodelwithprogressivelyenhancedcapabilities,
2023.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timothée
Lacroix, BaptisteRozière, NamanGoyal, EricHambro, FaisalAzhar, etal. Llama: Openand
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023b.
ShadenSmith,MostofaPatwary,BrandonNorick,PatrickLeGresley,SamyamRajbhandari,Jared
Casper,ZhunLiu,ShrimaiPrabhumoye,GeorgeZerveas,VijayKorthikanti,etal.Usingdeepspeed
andmegatrontotrainmegatron-turingnlg530b,alarge-scalegenerativelanguagemodel. arXiv
preprintarXiv:2201.11990,2022.
15Generative AI Research
LongxuDou,QianLiu,GuangtaoZeng,JiaGuo,JiahuiZhou,WeiLu,andMinLin. Sailor: Open
languagemodelsforsouth-eastasia. CoRR,abs/2404.03608,2024. doi: 10.48550/ARXIV.2404.
03608. URLhttps://doi.org/10.48550/arXiv.2404.03608.
JiantaoQiu,HaijunLv,ZhenjiangJin,RuiWang,WenchangNing,JiaYu,ChaoBinZhang,PeiChu,
YuanQu,RunyuPeng,etal. Wanjuan-cc: Asafeandhigh-qualityopen-sourcedenglishwebtext
dataset. arXivpreprintarXiv:2402.19282,2024.
AndreiZBroder. Ontheresemblanceandcontainmentofdocuments. InProceedings.Compression
andComplexityofSEQUENCES1997(Cat.No.97TB100171),pages21–29.IEEE,1997.
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli,
AlessandroCappelli,BaptistePannier,EbtesamAlmazrouei,andJulienLaunay. Therefinedweb
datasetforfalconllm: Outperformingcuratedcorporawithwebdataonly. AdvancesinNeural
InformationProcessingSystems,36,2024b.
Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. What makes good data for
alignment? acomprehensivestudyofautomaticdataselectionininstructiontuning. InTheTwelfth
InternationalConferenceonLearningRepresentations,2024b. URLhttps://openreview.
net/forum?id=BTKAeLqLMw.
ZacharyAnkner,CodyBlakeney,KartikSreenivasan,MaxMarion,MatthewLLeavitt,andMansheej
Paul. Perplexedbyperplexity: Perplexity-basedpruningwithsmallreferencemodels. InICLR
2024WorkshoponNavigatingandAddressingDataProblemsforFoundationModels,2024.
Noveen Sachdeva, Benjamin Coleman, Wang-Cheng Kang, Jianmo Ni, Lichan Hong, Ed H Chi,
James Caverlee, JulianMcAuley, and Derek ZhiyuanCheng. How to traindata-efficient llms.
arXivpreprintarXiv:2402.09668,2024.
QianLiu,XiaosenZheng,NiklasMuennighoff,GuangtaoZeng,LongxuDou,TianyuPang,Jing
Jiang,andMinLin. Regmix: Datamixtureasregressionforlanguagemodelpre-training. CoRR,
abs/2407.01492, 2024c. doi: 10.48550/ARXIV.2407.01492. URL https://doi.org/10.
48550/arXiv.2407.01492.
DariaSoboleva,FaisalAl-Khateeb,RobertMyers,JacobRSteeves,JoelHestness,andNolanDey.
SlimPajama: A627BtokencleanedanddeduplicatedversionofRedPajama,June2023. URL
https://huggingface.co/datasets/cerebras/SlimPajama-627B.
Run-ZeFan,XuefengLi,HaoyangZou,JunlongLi,ShwaiHe,EthanChern,JiewenHu,andPengfei
Liu. Reformattedalignment. arXivpreprintarXiv:2402.12219,2024.
XiangYue,TuneyZheng,GeZhang,andWenhuChen. Mammoth2: Scalinginstructionsfromthe
web. arXivpreprintarXiv:2405.03548,2024.
MichaelHassid,TalRemez,JonasGehring,RoySchwartz,andYossiAdi. Thelargerthebetter? im-
provedLLMcode-generationviabudgetreallocation. InFirstConferenceonLanguageModeling,
2024. URLhttps://openreview.net/forum?id=QJvfpWSpWm.
BradleyC.A.Brown,JordanJuravsky,RyanSaulEhrlich,RonaldClark,QuocV.Le,Christopher
Ré,andAzaliaMirhoseini. Largelanguagemonkeys: Scalinginferencecomputewithrepeated
sampling. CoRR, abs/2407.21787, 2024. doi: 10.48550/ARXIV.2407.21787. URL https:
//doi.org/10.48550/arXiv.2407.21787.
CharlieSnell,JaehoonLee,KelvinXu,andAviralKumar. ScalingLLMtest-timecomputeoptimally
canbemoreeffectivethanscalingmodelparameters. CoRR,abs/2408.03314,2024. doi:10.48550/
ARXIV.2408.03314. URLhttps://doi.org/10.48550/arXiv.2408.03314.
YangzhenWu,ZhiqingSun,ShandaLi,SeanWelleck,andYimingYang. Anempiricalanalysisof
compute-optimalinferenceforproblem-solvingwithlanguagemodels. CoRR,abs/2408.00724,
2024. doi: 10.48550/ARXIV.2408.00724. URL https://doi.org/10.48550/arXiv.
2408.00724.
OpenAI. Introducing openai o1-preview, 2024. URL https://openai.com/index/
introducing-openai-o1-preview.
16Generative AI Research
RistoLuukkonen,VilleKomulainen,JouniLuoma,AnniEskelinen,JennaKanerva,Hanna-Mari
Kupari,FilipGinter,VeronikaLaippala,NiklasMuennighoff,AleksandraPiktus,etal. Fingpt:
Largegenerativemodelsforasmalllanguage.InProceedingsofthe2023ConferenceonEmpirical
MethodsinNaturalLanguageProcessing,pages2710–2726,2023.
YaoweiZheng,RichongZhang,JunhaoZhang,YanhanYe,andZheyanLuo. Llamafactory: Unified
efficientfine-tuningof100+languagemodels. arXivpreprintarXiv:2403.13372,2024.
GuilhermePenedo,HynekKydlícˇek,AlessandroCappelli,MarioSasko,andThomasWolf. Data-
trove: large scale data processing, 2024c. URL https://github.com/huggingface/
datatrove.
WoosukKwon,ZhuohanLi,SiyuanZhuang,YingSheng,LianminZheng,CodyHaoYu,JosephE.
Gonzalez,HaoZhang,andIonStoica. Efficientmemorymanagementforlargelanguagemodel
servingwithpagedattention. InProceedingsoftheACMSIGOPS29thSymposiumonOperating
SystemsPrinciples,2023.
LightningAI. Litgpt. https://github.com/Lightning-AI/litgpt,2023.
Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In
InternationalConferenceonLearningRepresentations(ICLR),2024.
YanliZhao,AndrewGu,RohanVarma,LiangLuo,Chien-ChinHuang,MinXu,LessWright,Hamid
Shojanazeri,MyleOtt,SamShleifer,AlbanDesmaison,CanBalioglu,PritamDamania,Bernard
Nguyen,GeetaChauhan,YuchenHao,AjitMathews,andShenLi. Pytorchfsdp: Experienceson
scalingfullyshardeddataparallel. Proc.VLDBEndow.,16(12):3848–3860,aug2023. ISSN2150-
8097. doi: 10.14778/3611540.3611569. URLhttps://doi.org/10.14778/3611540.
3611569.
ShengdingHu,YugeTu,XuHan,ChaoqunHe,GanquCui,XiangLong,ZhiZheng,YeweiFang,
YuxiangHuang,WeilinZhao,etal. Minicpm: Unveilingthepotentialofsmalllanguagemodels
withscalabletrainingstrategies. arXivpreprintarXiv:2404.06395,2024.
JohannesWelbl,NelsonFLiu,andMattGardner. Crowdsourcingmultiplechoicesciencequestions.
arXivpreprintarXiv:1707.06209,2017.
SachinMehta,MohammadHosseinSekhavat,QingqingCao,MaxwellHorton,YanziJin,Chenfan
Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, et al. Openelm: An
efficientlanguagemodelfamilywithopen-sourcetrainingandinferenceframework.arXivpreprint
arXiv:2404.14619,2024.
PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,and
OyvindTafjord. Thinkyouhavesolvedquestionanswering? tryarc,theai2reasoningchallenge.
arXivpreprintarXiv:1803.05457,2018.
AlonTalmor,JonathanHerzig,NicholasLourie,andJonathanBerant. CommonsenseQA:Aquestion
answering challenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and
Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, Volume 1
(Long and Short Papers), pages 4149–4158, Minneapolis, Minnesota, June 2019. Association
forComputationalLinguistics. doi: 10.18653/v1/N19-1421. URLhttps://aclanthology.
org/N19-1421.
RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi. Hellaswag: Canamachine
reallyfinishyoursentence? arXivpreprintarXiv:1905.07830,2019.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn
Song,andJacobSteinhardt. Measuringmathematicalproblemsolvingwiththemathdataset. In
Thirty-fifthConferenceonNeuralInformationProcessingSystemsDatasetsandBenchmarksTrack
(Round2),2021.
17Generative AI Research
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? a new dataset for open book question answering. In Ellen Riloff, David Chiang,
JuliaHockenmaier,andJun’ichiTsujii,editors,Proceedingsofthe2018ConferenceonEmpir-
icalMethodsinNaturalLanguageProcessing,pages2381–2391,Brussels,Belgium,October-
November2018.AssociationforComputationalLinguistics. doi: 10.18653/v1/D18-1260. URL
https://aclanthology.org/D18-1260.
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical
commonsenseinnaturallanguage. InProceedingsoftheAAAIconferenceonartificialintelligence,
volume34,pages7432–7439,2020.
MaartenSap,HannahRashkin,DerekChen,RonanLeBras,andYejinChoi.Socialiqa:Commonsense
reasoningaboutsocialinteractions. arXivpreprintarXiv:1904.09728,2019.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An
adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106,
2021.
Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: A
challenge dataset for machine reading comprehension with logical reasoning. arXiv preprint
arXiv:2007.08124,2020.
ChristopherClark,KentonLee,Ming-WeiChang,TomKwiatkowski,MichaelCollins,andKristina
Toutanova. Boolq: Exploringthesurprisingdifficultyofnaturalyes/noquestions. InProceedings
of the 2019 Conference of the North American Chapter of the Association for Computational
Linguistics: HumanLanguageTechnologies,Volume1(LongandShortPapers),pages2924–2936,
2019.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
MatthiasPlappert,JerryTworek,JacobHilton,ReiichiroNakano,etal. Trainingverifierstosolve
mathwordproblems. arXivpreprintarXiv:2110.14168,2021.
ArkilPatel,SatwikBhattamishra,andNavinGoyal. Arenlpmodelsreallyabletosolvesimplemath
wordproblems? InProceedingsofthe2021ConferenceoftheNorthAmericanChapterofthe
AssociationforComputationalLinguistics: HumanLanguageTechnologies,pages2080–2094,
2021.
Shen-YunMiao,Chao-ChunLiang,andKeh-YihSu. Adiversecorpusforevaluatinganddeveloping
englishmathwordproblemsolvers. InProceedingsofthe58thAnnualMeetingoftheAssociation
forComputationalLinguistics,pages975–984,2020.
RikKoncel-Kedziorski,SubhroRoy,AidaAmini,NateKushman,andHannanehHajishirzi. Mawps:
Amathwordproblemrepository. InProceedingsofthe2016conferenceofthenorthamerican
chapteroftheassociationforcomputationallinguistics: humanlanguagetechnologies, pages
1152–1157,2016.
AidaAmini, SaadiaGabriel, ShanchuanLin, RikKoncel-Kedziorski, YejinChoi, andHannaneh
Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based
formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the
AssociationforComputationalLinguistics: HumanLanguageTechnologies,Volume1(Longand
ShortPapers),pages2357–2367,2019.
Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter
Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured
mathematicalreasoning. InInternationalConferenceonLearningRepresentations(ICLR),2023.
JaredKaplan,SamMcCandlish,TomHenighan,TomBBrown,BenjaminChess,RewonChild,Scott
Gray,AlecRadford,JeffreyWu,andDarioAmodei. Scalinglawsforneurallanguagemodels.
arXivpreprintarXiv:2001.08361,2020.
18Generative AI Research
APPENDIX
A PROXImplementationDetails 20
A.1 SupervisedFine-tuningDataCollection . . . . . . . . . . . . . . . . . . . . . . 20
A.2 SupervisedFine-tuningDetails . . . . . . . . . . . . . . . . . . . . . . . . . . 25
A.3 EvaluationMetricsforPROXRefiningTasks . . . . . . . . . . . . . . . . . . . 25
A.4 PROXInferenceatScale. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
B Pre-trainingDetails 27
B.1 TrainingInfrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
B.2 Pre-trainingCorpora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
B.3 ModelConfigurationandTrainingParameters . . . . . . . . . . . . . . . . . . 27
C DownstreamTasksEvaluation 29
C.1 GeneralPre-trainingEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . 29
C.2 ContinualPre-trainingEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . 30
D FullEvaluationResults 31
D.1 DetailedPerformanceon10BenchmarksinSec3.2 . . . . . . . . . . . . . . . 31
D.2 DetailedPerformanceon8BenchmarksUsedinDataSelectionExperiments . . 32
D.3 DetailedPerformanceinSec3.3. . . . . . . . . . . . . . . . . . . . . . . . . . 33
D.4 EvaluationResultsofContinualPre-traininginSec3.4 . . . . . . . . . . . . . . 37
E Analysis 43
E.1 CaseStudies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
E.2 ComputingOverheadAnalysis . . . . . . . . . . . . . . . . . . . . . . . . . . 43
19Generative AI Research
A PROX IMPLEMENTATION DETAILS
A.1 SUPERVISEDFINE-TUNINGDATACOLLECTION
Inthissection,weelaboratethedetailedpromptsusedtogeneratedtheSFTdataformodeladap-
tation. Inprinciple,Weapplythesamepromptsforgeneraldomaincorpora(includingC4(Raffel
etal.,2020),RedPajama-V2(Together,2023),FineWeb(Penedoetal.,2024a))andmathematical
corpus(OpenWebMath(Pasteretal.,2024)). Andallseeddataisrandomlysampledfromtheraw
corpora.
Document-LevelProgramming Weapplytwozero-shotscoringpromptstoevaluateandassign
acombinedscoretoeachwebdocumentbeforesynthesizingthe(doc, program)pair. Oneof
thepromptsisthesameastheoneusedinFineWeb-Edu,whichisaprompttoletthemodeldecide
theeducationalscore. AdditionallyinPROX,weaddanewformatscoringprompt,focusingonthe
formatandstructureofthedocument. BothpromptsfollowtheadditivestyleproposedbyYuan
etal.(2024). Giventheseprompts,thelanguagemodelsgenerateshortcritiquesandassignascore
between0and5.
InFineWeb-Edu,documentsareretainedonlyiftheeducationalscore(EduScore)isgreaterthan
2. However, this approach is too aggressive when attempting to preserve a larger portion of the
tokens. Forinstance,FineWeb-Eduretainsonly1.3trilliontokensoutoftheoriginal15trillioninthe
FineWebcorpus. Torecallmoredocuments,werelaxthefilteringcriteriabyincorporatingtheformat
scoreasfollows:

EduScore≥3, keepdocument;

FilteringCriteria= EduScore=2andFormatScore≥4, keepdocument; (2)
EduScore<2, dropdocument.
Finally,weuseLLAMA-3-70B-INSTRUCTtoannotate51Kdata,splitting5Kforvalidation3.
TheFineWeb-EdupromptandourformatscoringpromptsarepresentedinFigure9.
Chunk-levelProgramming Weapplychunk-levelprogrammingformorefine-grainedoperations.
Wefindthreeverypopularpatternsthatkeepoccurringinallcorpus: (1)menu,navigationbarsatthe
topofthedocument;(2)button,htmlelements,links;(3)footers.
In general, LLMs work well given within 5 few-shot examples. But to generate these program
snippetsmoreaccurately,weapplyfew-shotpromptingwithLLAMA-3-70B-INSTRUCTforeach
typeofnoise. Wemergetheseprogramsaimingtocleandifferenttypesofnoises,performsome
grammarchecking,andmakethemthefinaldatafortrainingandvalidationduringthechunk-level
refining stage. The annotated source comes from the same seed document used in the previous
documentfilteringstage,accumulatingtoabout57Kdata,ofwhich5Kissplitasvalidation.
AfterthereleaseofLLAMA-3.1-405B-INSTRUCT,Wealsotrytouseonlyonepromptaimingto
removeallthenoises. However,wefindsuchpracticesleadtoaggressiveremovaloftheoriginal
document, often making the document less coherent. Finally, we decide to only keep the head
partandtailpartoftheprogramgeneratedbyLLAMA-3.1-405B-INSTRUCT,whichispreviously
mentionedinFinGPT(Luukkonenetal.,2023),andmergewiththepreviousprogramsgeneratedby
LLAMA-3-70B-INSTRUCT.
Thefew-shotpromptsusedtogenerateprogramsnippetsarepresentedinFigure10,Figure11and
Figure12.
3Intheearlierstageofexperiments,wefoundthatadatasetofthousandsofdatapoints(i.e.,5K)isalso
sufficienttoequipthemodelwiththe“programming”abilities.Thisgenerallyholdstrueforbothdocument-level
andchunk-levelprogrammingtasks. Scalingthedatasetsizecouldenhancethemodel’srobustnessacross
variousdocuments.
20Generative AI Research
EduScoringPrompts(Penedoetal.,2024a)
Belowisanextractfromawebpage.Evaluatewhetherthepagehasahigheducationalvalueandcouldbeusefulinaneducationalsettingforteachingfrom
primaryschooltogradeschoollevelsusingtheadditive5-pointscoringsystemdescribedbelow.Pointsareaccumulatedbasedonthesatisfactionofeachcriterion:
-Add1pointiftheextractprovidessomebasicinformationrelevanttoeducationaltopics, evenifitincludessomeirrelevantornon-academic
contentlikeadvertisementsandpromotionalmaterial.-Addanotherpointiftheextractaddressescertainelementspertinenttoeducationbutdoesnotalign
closelywitheducationalstandards.Itmightmixeducationalcontentwithnon-educationalmaterial,offeringasuperficialoverviewofpotentiallyusefultopics,
orpresentinginformationinadisorganizedmannerandincoherentwritingstyle.-Awardathirdpointiftheextractisappropriateforeducationaluseand
introduceskeyconceptsrelevanttoschoolcurricula.Itiscoherentthoughitmaynotbecomprehensiveorcouldincludesomeextraneousinformation.Itmay
resembleanintroductorysectionofatextbookorabasictutorialthatissuitableforlearningbuthasnotablelimitationsliketreatingconceptsthataretoo
complexforgradeschoolstudents.
-Grantafourthpointiftheextracthighlyrelevantandbeneficialforeducationalpurposesforalevelnothigherthangradeschool,exhibitingaclearand
consistentwritingstyle.Itcouldbesimilartoachapterfromatextbookoratutorial,offeringsubstantialeducationalcontent,includingexercisesandsolutions,
withminimalirrelevantinformation,andtheconceptsaren’ttooadvancedforgradeschoolstudents.Thecontentiscoherent,focused,andvaluableforstructured
learning.
-Bestowafifthpointiftheextractisoutstandinginitseducationalvalue,perfectlysuitedforteachingeitheratprimaryschoolorgradeschool.Itfollows
detailedreasoning,thewritingstyleiseasytofollowandoffersprofoundandthoroughinsightsintothesubjectmatter,devoidofanynon-educationalor
complexcontent.
Theextract:
<EXAMPLE>.
Afterexaminingtheextract:
-Brieflyjustifyyourtotalscore,upto100words.
-Concludewiththescoreusingtheformat:“Educationalscore:<totalpoints>”
FormatScoringPrompts
Evaluatetheprovidedwebcontentextractionsample.Pointsareaccumulatedbasedonthesatisfactionofeachcriterion:
0.Startwith0points.
1.Add1pointiftheextractcontainssomereadablecontent,evenifitincludesasignificantamountofHTMLtags,navigationelements,orotherwebpage
artifacts.Themaincontentshouldbeidentifiable,albeitmixedwithnoise.
2.Addanotherpointiftheextractshowssignsofbasiccleaning.MostobviousHTMLtagshavebeenremoved,thoughsomemayremain.Thetextstructure
beginstoemerge,butnon-contentelements(e.g.,footerlinks,buttontext)maystillbepresent.Thewritingstylemaybedisjointedduetoremnantsofpage
structure.
3.AwardathirdpointiftheextractislargelycleanedofHTMLandmostnon-contentelements.Themainbodyofthecontentisintactandcoherent.Some
extraneousinformation(e.g.,isolatedURLs,timestamps,imagealttext)maypersist,butdoesn’tsignificantlyimpedereadability.Theextractresemblesarough
draftoftheoriginalcontent.
4.Grantafourthpointiftheextractishighlyrefined,withclearparagraphstructureandformatting.AlmostallHTMLtagsandnon-contentelementshavebeen
eliminated.Minimalnoiseremains.Thecontentflowswellandreadslikeanear-finaldraft,withconsistentformattingandstyle.
5.Bestowafifthpointiftheextractionisflawless.Thecontentisentirelyclean,preservingtheoriginalstructure(paragraphs,headings,lists)withoutany
HTMLtagsorwebpageelements.Noextraneousinformationispresent.Theextractreadsasifitwereaprofessionallyediteddocument,perfectlycapturing
theoriginalcontent.
Theextract:
<EXAMPLE>.
Afterexaminingtheextract:
-Brieflyjustifyyourtotalscore,upto100words.
-Concludewiththescoreusingtheformat:"ExtractionQualityScore:<totalpoints>"
Figure9: EduscoringpromptsusedinFineWeb(Penedoetal.,2024a)andnewlyproposed“format
scoring”promptsforPROX.
ComparisonwithFineWeb-Edu’sApproach ComparedwiththerecentlyreleasedFineWeb-Edu,
whichalsousesmodel-basedscoringbyapplyingaBERTmodeltoevaluatedocuments,wefind
thatourrelaxeddesignretainsmoretokenswithoutcompromisingoveralldataquality. Specifically,
FineWeb-Eduretainsabout1.3trilliontokensoutofa15trilliontokencorpus(lessthan9%),while
PROXcurationtypicallykeeps23%to28%,providingupto3×moreuniquetokensfortraining.
Moreover, we conducted a preliminary study by training 0.7 billion parameter models on these
data. Wefoundthatmodelstrainedonourcurateddataachievedsimilardownstreamperformance,
as shown in Table 6. Therefore, we believe our current strategy is more suitable for large scale
pre-training,asitiscapableofretainingmoretokenswhilemaintainingveryhighdataquality.
Table6: ComparingFineWeb-EduwithourstrategyonTLM-S.
Methods KeptRatio ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG #Win
FineWeb-Edu 8.6% 30.3 58.7 29.0 42.0 30.4 31.8 67.7 38.1 50.4 73.3 45.2 5/10
FineWeb-PROX 28.0% 27.7 55.7 30.4 44.2 29.5 31.0 68.8 39.3 52.2 72.8 45.2 5/10
21Generative AI Research
NavigationRemovalPrompts
You’retaskedwithgeneratingPythonprogramstocleanwebtextstringsbyremovingnavigationbars.Thewebtextwillbepresentedwithlinenumbersstarting
from‘[000]‘.Yourtaskistousethefollowingpre-definedfunctionstocleanthetext:
‘‘‘python
def untouch_doc():
"""leave the clean doc untouched, for tagging clean and high quality doc."""
def remove_lines(start: int, end: int):
"""remove noisy lines from ‘start‘ until ‘end‘, including ‘end‘."""
‘‘‘
Yourgoalistoidentifynavigationbarsormenuitemsatthebeginningofthetextandremovethemusingthe‘remove_lines()‘function.Ifthetext
doesn’tcontainanavigationbarormenuitems,usethe‘untouch_doc()‘functiontoindicatethatnocleaningisnecessary.Ifthelinecontainsothertext
otherthannavigation,alsocall‘untouch_doc‘toescapeoverkilling.
Herearesomeexamplestoguideyou:
Example1:
[doc]
[000] Home | Products | About Us | Contact
[001] Welcome to our website
[002] Here’s our main content...
[/doc]
Program:
‘‘‘python
remove_lines(start=0, end=0)
‘‘‘
Example2:
[doc]
341 US 479 Hoffman v. United States
341 US 479 Hoffman v. United States 341 U.S. 479
95 L.Ed. 1118
HOFFMANv.UNITED STATES.
Mr. William A. Gray, Philadelphia, Pa., for petitioner.
Mr. John F. Davis, Washington, D.C., for respondent.
......
[/doc]
Program:
‘‘‘python
untouch_doc()
‘‘‘
Example3:
[doc]
[000]Police Search Tunbridge Wells House Over Human Remains Tip Off
[001]Posted: 16/04/2012 10:44 Updated: 16/04/2012 10:44 reddit stumble
[002]Crime, Body Buried In House, Buried Body, Buried Remains, Tip-Off, Uk News, Uk Police,
[003]Detectives are searching the gardens of a house following information that human remains may be
buried there.
[/doc]
Program:
‘‘‘python
untouch_doc()
‘‘‘
Example4:
[doc]
[000]Home > Bollywood News > Bollywood Stars clash on Indian TV Bollywood Stars clash on Indian TV
[001]By Lekha Madhavan09:47 pm Betting big on the festive season, general entertainment channels (GECs)
are launching celebrity-driven shows, but media buyers are concerned about the audience split that is set
to happen.
[002]The fourth season of Bigg Boss on Colors is almost certain to clash with the fourth season of Kaun
Banega Crorepati (KBC) on Sony Entertainment Television (SET) in the second week of October.
[003]Another big property, Master Chef, to be hosted by Akshay Kumar, on STAR Plus, is also expected to go
on air in October. However, the channel is yet to disclose the launch date.
[004]Big-budget shows like these are often loss-making propositions for channels, as the operating cost is
very high and advertisement revenues do not suffice to cover the cost.
[005]Source: IBNS
[/doc]
Program:
‘‘‘python
untouch_doc()
‘‘‘
Foreachgivenwebtext,analyzethecontentanddetermineifthere’sanavigationbarormenuitemsatthebeginning.Ifpresent,use‘remove_lines()‘or
‘normalize()‘toremovethem.Ifnot,use‘untouch_doc()‘toindicatethatnocleaningisneeded.
Example:<EXAMPLE>.
Afterexaminingthewebtext:-Brieflydescribeifthewebextractcontainsnavigationbaratthebegining(10lines).
-Youmustnotmistakenlydecidethattitleofthepageisnavigationbarandremoveit.
-Whenthewholelineisnavigationbar,call‘remove_lines‘;ifthelinecontainsotherinformation,call‘normalize‘toremovepartofit.
-Giveyourprogramusingthesameformat:‘‘‘python[your code]‘‘‘
Figure10: Few-shotnavigationbarremovalprompts.
22Generative AI Research
URLRemovalPrompts
You’retaskedwithgeneratingPythonprogramstocleanwebtextstringsbyremovinghttplines.Thewebtextwillbepresentedwithlinenumbersstartingfrom
‘[000]‘.Yourtaskistousethefollowingpre-definedfunctionstocleanthetext:
‘‘‘python
def untouch_doc():
"""leave the clean doc untouched, for tagging clean and high quality doc."""
def remove_lines(start: int, end: int):
"""remove noisy lines from ‘start‘ until ‘end‘, including ‘end‘."""
def normalize(source_str: str, target_str: str=""):
"""turn noisy strings into normalized strings."""
‘‘‘
Yourgoalistoidentifyhttplinksfromthetextandremovethemusingthe‘remove_lines()‘or‘normalize()‘function.Ifthetextdoesn’tcontain
httplines,usethe‘untouch_doc()‘functiontoindicatethatnocleaningisnecessary.
Herearesomeexamplestoguideyou:
Example1:
[doc]
[013] http://groups.google.com/group/toowoombalinuxLast
[014] Breaking News: Major Event Unfolds
[015] http://code.google.com/p/inxi/
[/doc]
Program:
‘‘‘python
# the whole line-[013] is http, so remove the line-[013]
remove_lines(start=13, end=13)
# the whole line-[015] is http, so remove the line-[015]
remove_lines(start=15, end=15)
‘‘‘
Example2:
[doc]
[000] The Impact of Climate Change on Global Ecosystems
[001] By Dr. Jane Smith
[002] Climate change continues to be a pressing issue...
[/doc]
Program:
‘‘‘python
untouch_doc()
‘‘‘
Example3:
[doc]
[021]Bow-wow
[022]http://groups.google.com/group/toowoombalinuxLast edited by Puppyt on Mon 06 Jun 2011, 00:23; edited
1 time in total
[023]I would like to see something like Jitsi
[024]http://www.jitsi.org/. Plus some others incorporated into a puppy distro.
[/doc]
Program:
‘‘‘python
# the http link in line 22 and line 24 comes with other text, so use normalize to ONLY remove the link
without touching text.
normalize(source_str="http://groups.google.com/group/toowoombalinuxLast", target_str="")
normalize(source_str="http://www.jitsi.org/.", target_str="")
‘‘‘
Foreachgivenwebtext,analyzethecontentanddetermineifthere’sanavigationbarormenuitemsatthebeginning.Ifpresent,use‘remove_lines()‘or
‘normalize()‘toremovethem.Ifnot,use‘untouch_doc()‘toindicatethatnocleaningisneeded.
Example:<EXAMPLE>.
Afterexaminingthewebtext:-donotremovetexttogetherwithhttp.
-Brieflydescribeifthewebextractcontainshttplinks;andmakesureremovethemwillnotinfluencethemaincontent.
-Programonlycontainsequencesoffunctioncallingsandcomments,noothercodes.
-notelinenumberstartswith0.makeaccurateannotationsaboutlinenumber.puttheexactintlinenumberofthegivenline.donotadd1orminus1.
-Giveyourprogramusingthesameformat:‘‘‘python[your code]‘‘‘
Figure11: Few-shotURLremovalprompts.
23Generative AI Research
FooterRemovalPrompts
You’retaskedwithgeneratingPythonprogramstocleanwebtextstringsbyremovingfootersections,references.Thewebtextwillbepresentedwithline
numbersstartingfrom‘[000]‘.Yourtaskistousethefollowingpre-definedfunctionstocleanthetext:
‘‘‘python
def untouch_doc():
"""leave the clean doc untouched, for tagging clean and high quality doc."""
def remove_lines(start: int, end: int):
"""remove noisy lines from ‘start‘ until ‘end‘, including ‘end‘."""
def normalize(source_str: str, target_str: str=""):
"""turn noisy strings into normalized strings."""
‘‘‘
Yourgoalistoidentifyfootersectionsfromthetextandremovethemusingthe‘remove_lines()‘function.Footersandreferencestypicallyappearatthe
endofthetextandmaycontaininformationsuchascopyrightnotices,contactdetails,ornavigationlinks.Ifthetextdoesn’tcontainafootersectionorany
references,usethe‘untouch_doc()‘functiontoindicatethatnocleaningisnecessary.
Herearesomeexamplestoguideyou:
Example1:
[doc]
[013] In conclusion, the study demonstrates significant findings.
[014] © 2023 Research Institute. All rights reserved.
[015] Contact: info@research-institute.com
[016] Follow us on social media: @ResearchInst
[/doc]
Program:
‘‘‘python
# Remove the footer section starting from line 14
remove_lines(start=14, end=16)
‘‘‘
Example2:
[doc]
[000] The Impact of Climate Change on Global Ecosystems
[001] By Dr. Jane Smith
[002] Climate change continues to be a pressing issue...
[003] Further research is needed to fully understand its implications.
[/doc]
Program:
‘‘‘python
untouch_doc()
‘‘‘
Example3:
[doc]
[020] Thank you for reading our newsletter.
[021] Stay informed with our latest updates!
[022] ---
[023] Unsubscribe | Privacy Policy | Terms of Service
[024] NewsletterCo, 123 Main St, Anytown, USA
[/doc]
Program:
‘‘‘python
# Remove the footer section starting from the divider
remove_lines(start=22, end=24)
‘‘‘
Foreachgivenwebtext,analyzethecontentanddetermineifthereisafootersectionorreference.Ifpresent,use‘remove_lines()‘toremoveit.Ifnot,
use‘untouch_doc()‘toindicatethatnocleaningisneeded.
Example:<EXAMPLE>.
Afterexaminingthewebtext:
-Brieflydescribeifthewebextractcontainsafootersectionorreferences;ensurethatremovingitwillnotinfluencethemaincontent.Ifnot,simplycall
‘untouch_doc‘.
-Theprogramshouldonlycontainsequencesoffunctioncallsandcomments,noothercode.
-Notethatlinenumbersstartwith0.Makeaccurateannotationsaboutlinenumbers.Puttheexactintlinenumberofthegivenline.Donotadd1orsubtract1.
-Giveyourprogramusingthesameformat:‘‘‘python[your code]‘‘‘
Figure12: Few-shotfooterremovalprompts.
24Generative AI Research
A.2 SUPERVISEDFINE-TUNINGDETAILS
TrainingParameters Weusellama-factory(Zhengetal.,2024)asourmaincodebaseforAdapta-
tionStage. Weapplyfullparaemtersupervisedfine-tuningonourbasemodels: wetrainonthewhole
seeddatasetfor3to5epochs,withbatchsizeas64,andcosinelearningrateschedular(lrfrom1e-5
→1e-6). Also,wefindthatbasemodelconvergentquitefastonthesetasks,thuswedonotapplya
furthertuningoverhyper-parameters,andkeepthesametrainingconfigurationsforalltheadaptation
tasks.
A.3 EVALUATIONMETRICSFORPROXREFININGTASKS
Document-levelrefiningTask Thedocumentfilteringtaskisindeedequaltoabinaryclassification
problem,wheredocumentsareclassifiedaseithertobekept(1)ordropped(0). Weevaluatethe
performanceusingtheF1score,calculatedasfollows:
Precision·Recall
F1=2· (3)
Precision+Recall
where:
TP TP
Precision= , Recall= (4)
TP+FP TP+FN
The F1 score ranges from 0 to 1 and we assume higher F1 score indicates better classification
performance.
Chunk-levelRefiningTask Thistaskactuallycontainstwoparts: lineremovalandstringnor-
malization. However,wefinditisratherhardtoevaluatethenormalizationtask,soweusetheline
removalaccuracytoreflecttherefiningperformance. Weproposealine-wiseF1scoremetric:
TheF1scoreiscomputedbycomparingthepredictednoisylineswiththelabelednoisylines. First,
we extract the noisy line indexes from both the prediction and the label. Then, we calculate the
overlapbetweenthesetwosets. Thetruepositives(TP)arethenumberoflinesinthisoverlap. False
positives(FP)arethepredictednoisylinesthatarenotinthelabeledset,andfalsenegatives(FN)are
thelabelednoisylinesthatarenotinthepredictedset. Thecalculationisactuallysimple:
TP(TruePositives) = |PredictedNoisyLines∩ActualNoisyLines| (5)
FP(FalsePositives) = |PredictedNoisyLines\ActualNoisyLines| (6)
FN(FalseNegatives) = |ActualNoisyLines\PredictedNoisyLines| (7)
ThenweusesamecalculationofF1scorementionedbefore,i.e.,F1= 2·TP .
2·TP+FP+FN
25Generative AI Research
A.4 PROXINFERENCEATSCALE
Thanks to the Datatrove project (Penedo et al., 2024c), we are able to efficiently split, and load
thewholecorpustoeachworker(whichnormallyequalstothenumberoftheGPUssincesmall
modelsdonotrequiretensorparallelism). Weusethevllm(Kwonetal.,2023)toperformlargescale
inference.
Forchunk-wiseprogramming,wewillsplittheoriginaldocumentintoseveralchunks,controlling
thetokensofeachchunklessthanthecontextwindow. Inpractice,wenormallyreplacetokencount
processasawordcountprocessforsavingtime,andcontrolthewindowsizeas1,500. Thegeneral
algorithmisimplementedasbelow:
Algorithm1DocumentChunkSplittingAlgorithm
Require: DocumentD,contextwindowsizeW
Ensure: SetofchunksC
1: C ←∅,c←∅
2: foreachlinelinDdo
3: ifTokenCount(c+l)≤W then
4: c←c+l ▷Addlinetocurrentchunk
5: else
6: ifc̸=∅then
7: C ←C∪{c} ▷Savecurrentchunk
8: endif
9: ifTokenCount(l)≤W then
10: c←l ▷Startnewchunk
11: else
12: C ←C∪{FlagAsSkipped(l)} ▷Flaglongline
13: c←∅
14: endif
15: endif
16: endfor
17: ifc̸=∅then
18: C ←C∪{c} ▷Addthefinalchunk
19: endif
20: returnC
26Generative AI Research
B PRE-TRAINING DETAILS
B.1 TRAININGINFRASTRUCTURE
CodeBase Thankstolitgpt(AI,2023),andTinyLlaMA(Zhangetal.,2024b),weareabletoflexibly
trainallourbasemodels. WeinheritseveralfusedkernelsfromtheTinyLlaMA,whichisinstalled
from the FlashAttention (Dao, 2024) including fused rotary positional embedding (RoPE), layer
normalization,andcrossentropylosstohelpsavingmemory. WemainlyapplyFSDPstrategy(Zhao
etal.,2023)toenabletraininglargerscalemodelsonmultiplenodes.
B.2 PRE-TRAININGCORPORA
Duetocomputingconstraintsandfaircomparisonpurpose,wecannotexhaustivelytrainoverthe
wholecorpora. Thus,weapplyrandomsamplingforsomeofthepre-trainingcorporaandmakethem
asourpre-trainingdatapools.
• For RedPajama-V2, We randomly download 70 file shards, obtaining a total data pool
consistingabout500Btokens, weevenlyseparateitinto8dumps, witheachcontaining
about62.5Btokens;duetocomputingconstraints,weuseonly1dumpforverifyingeffec-
tiveness(Section3.2)anduse2dumpsforscalingthetrainingto50Btokens(Section3.3);
• ForC4,wedownloadthewholedataset,whichcontainsabout198Btokens;
• ForFineWeb,wedownloadtheofficial350Bsample4;
• ForOpenWebMath,wedownloadthewholedataset.
WereportthecorporadetailsappliedineachexperimentinTable7.
Table7: Thedetailedbreakdownforpre-trainingcorporainallexperiments.
Section Experiments Source DataDescription CorporaSize(B) EffectiveTrainTokens(B) Epoch
rawdatasize 62.5 0.42
afterrule-basedfiltering 31.5 0.83
Section3.2 Table2,Figure4 RedPajama-V2 afterPROX-D 19.0 26.2 1.38
afterPROX-D+C 16.0 1.64
random - -
Section3.2 Table3 C4 afterPROX-D 41.5(GPT-NeoX) 26.2 0.63
otherbaselines - -
rawdatasize 62.5 0.42
afterPROX-D+C(usingPROX-xs) 14.5 1.80
Section3.3 Figure5 RedPajama-V2 afterPROX-D+C(usingPROX-s) 16.0 26.2 1.64
afterPROX-D+C(usingPROX-m) 18.0 1.46
rawdatasize 198.0 0.53
C4 afterPROX-D+C(usingPROX-xs) 44.5 1.18
rawdatasize 123.5 0.42
Section3.3 Figure6 RedPajama-V2 afterPROX-D+C(usingPROX-xs) 29 52.4 1.81
rawdatasize 79.0 0.66
FineWeb afterPROX-D+C(usingPROX-xs) 18.0 2.91
rawdatasize 15.0 1.05
afterrule-basedfiltering 6.5 2.40
Section3.4 Table5,1.1Bmodel OpenWebMath afterPROX-D 5.5 15.7 2.85
afterPROX-D+C 4.7 3.49
rawdatasize 15.0 0.70
Section3.4 Table5,7Bmodel OpenWebMath afterPROX-D 5.5 10.5 1.91
afterPROX-D+C 4.7 2.23
B.3 MODELCONFIGURATIONANDTRAININGPARAMETERS
4https://huggingface.co/datasets/HuggingFaceFW/fineweb/tree/main/
sample/350BT
27Generative AI Research
Table8: Thedetailsofthepre-trainingexperiments’modelarchitecture.
Model HiddenSize IntermediateSize ContextLen Heads Layers VocabSize #Params(w/oembed)
TrainingFromScratch
TLM-XS 1,280 2,048 2,048 16 24 32,000 354,284,800(313,324,800)
TLM-S 1,536 4,864 2,048 24 24 32,000 758,982,144(709,830,144)
TLM-M 2,048 8,192 2,048 32 24 32,000 1,741,785,088(1,676,249,088)
PYTHIA-410M 1,024 4,096 1,024 16 24 50,304 405,334,016(353,822,720)
PYTHIA-1B 2,048 8,192 1,024 8 16 50,304 1,011,781,632(908,759,040)
ContinualPre-training
TINYLLAMA-1.1B 2,048 5,632 2,048 32 22 32,000 1,100,048,384(1,034,512,384)
LLAMA-2-7B 4,096 11,008 4,096 32 32 32,000 6,738,415,616(6,607,343,616)
CODELLAMA-7B 4,096 11,008 4,096 32 32 32,016 6,738,546,688(6,607,409,152)
MISTRAL-7B 4,096 14,336 4,096 32/8(GQA) 32 32,000 7,241,732,096(7,110,660,096)
Table9: Traininghyper-parametersofallbasemodels.
Context Warmup Weight LR
Model BatchSize MaxSteps Optimizer LR
Length Steps Decay Scheular
TrainingfromScratch
TLM-XS 1,024 2,048 12,500 500 0.1 AdamW cosine 5e-4→5e-5
TLM-S 1,024 2,048 12,500 500 0.1 AdamW cosine 5e-4→5e-6
TLM-M 1,024 2,048 12,500/2,5000 500 0.1 AdamW cosine 3e-4→3e-5
PYTHIA-410M 512 1,024 50,200 2,000 0.1 AdamW WSD 1e-3→6.25e-5
PYTHIA-1B 512 1,024 50,200 2,000 0.1 AdamW WSD 1e-3→6.25e-5
ContinualPre-training
TINYLLAMA-1.1B 2,048 1,024 7,500 0 0.1 AdamW cosine 8e-5→8e-6
LLAMA-2-7B 4096 256 15,000(earlystopat10,000) 0 0.1 AdamW cosine 8e-5→8e-6
CODELLAMA-7B 4096 1024 3,750(earlystopat2,500) 0 0.1 AdamW cosine 3e-4→3e-5
MISTRAL-7B 4,096 256 15,000(earlystopat10,000) 0 0.1 AdamW cosine 2e-5→2e-6
ModelArchitecture Themodelsweusedingeneralandcontinualpre-trainingarepresentedat
Table8withdetailedarchitectureconfiguration.
TrainingHyperparameterChoice Weprimarilyuseacosinelearningrateschedulerandfollow
establishedsettingsusedinZhangetal.(2024b)andLinetal.(2024). Thedefaultconfigurationsfor
eachexperimentcanbefoundbelowandweelaboratefulldetailsinTable9.
1. Forgeneralpre-trainingexperiments,wesetthelearningrateto5e-4forTLM-XSandTLM-S,
3e-4forTLM-M;themaximumsequencelengthsareuniformlysetto2048,andtheglobalbatch
sizeissetto2Mtokens.
2. Additionally,wealignallourhyper-parameterswiththoseusedinMATES(Yuetal.,2024)to
facilitateadirectcomparisonwiththeirexistingdataselectionmethods,aspreviouslyshownin
Table3. Inthiscase,weswitchtothewarmup-stable-decay(WSD)learningratescheduler(Hu
et al., 2024), as implemented in MATES. For fair comparison with baselines implemented in
MATES,weapplytheexactsameWSDSchedular(Huetal.,2024):
 t ·η, ift<W
W
lr(t)= η, ifW ≤t<S (8)
0.54·(t−S)/D·η, ifS ≤t<S+D
whereW equalsto2000,S equalsto50000,Dequalsto200.
3. Forcontinualpre-trainingexperiments,wesetdifferenthyperparametersfordifferentbasemodels,
asshowninTable9. Weapplyanearly-stopmechanismmentionedinINTERNLM2-MATH(Ying
et al., 2024) for 7B model experiments. We mainly refer these settings to the setup reported
inRho-1(Linetal.,2024)and LLEMMA (Azerbayevetal.,2024). Wedonotusewarmupin
continualpre-trainingexperiments.
28Generative AI Research
C DOWNSTREAM TASKS EVALUATION
C.1 GENERALPRE-TRAININGEVALUATION
LightevalConfigurations WemainlyborrowtheevaluationbenchmarksfromtheFineWeb’snine
selected“earlysignal”tasks(Penedoetal.,2024a),andusetheimplementationoflighteval(Fourrier
etal.,2023)totestallourbasemodels. WealsointroduceSciQ(Welbletal.,2017)whichiswidely
usedinpreviousworksandprovedagoodtestbed(Mehtaetal.,2024;Wettigetal.,2024). Bydefault,
wereportthenormalizedzero-shotaccuracy. Alltheninebenchmarksatlistedbelow:
• ARC(Clarketal.,2018): includingARC-Easy(ARC-E)andARC-Challenge(ARC-C)
• CommonSenseQA(Talmoretal.,2019)(CSQA)
• HellaSwag(Zellersetal.,2019)
• MMLU(Hendrycksetal.,2021)
• OpenBookQA(Mihaylovetal.,2018)(OBQA)
• PIQA(Bisketal.,2020)
• SocialIQA(Sapetal.,2019)(SIQA)
• WinoGrande(Sakaguchietal.,2021)(WinoG)
• SciQ(Welbletal.,2017)
Wefollowthelighteval’sconfiguration,whichrandomlypicks1,000samplesforeachdataset(for
MMLU,itselects1,000samplesforeachofthe57subsets),andreportthenormalizedaccuracy.
These average performance is calculated over the nine benchmarks, where ARC-C and ARC-E
are considered as two separate benchmarks, and MMLU is treated as a single benchmark. This
approachdiffersslightlyfromtheaggregationscorecalculationinFineWeb,aswebelieveMMLU’s
performanceisrelativelyunstable,andweaimtogiveequalweighttoallbenchmarks,preventing
MMLUfrombecomingadominantfactor. Fortheoriginallightevalscores,pleaserefertothe§D.1,
whereweincludeadynamicresultcurvethatclearlyillustratesthefluctuationsineachbenchmark.
WepresentzeroshotevaluationresultsinTable2,Figure4.
LM-EvalHarnessConfigurations Wealsoincludethelm-evel-harness(Bidermanetal.,2024)
forzero-shotandfew-shotperformance,forfaircomparisonwithdifferentdataselectionmethods
including DSIR (Xie et al., 2023),DsDm (Engstrom et al., 2024), Qurating (Wettig et al., 2024)
MATES(Yuetal.,2024). Similartolightevalconfiguration,weinclude:
• ARC:includingARC-EandARC-C
• HellaSwag
• LogiQA(Liuetal.,2020)
• OpenBookQA(OBQA)
• PIQA
• WinoGrande(WinoG)
• SciQ
WeexcludetheBoolQ(Clarketal.,2019)tasksfromMATES(Yuetal.,2024),leavingeighttasksin
total. ThisdecisionwasmadebecauseweobservedthattheBoolQbenchmarkperformanceexhibited
severefluctuationsandshowedanotabledecliningtrendintheearlystages. Therefore,wedecidedto
excludeitfromourevaluationset. SuchtrendisalsoobservedearlierintheOpenELMwork(Mehta
etal.,2024). Wereportbothzero-shotandtwo-shotperformance. Ifthemetricsincludenormalized
accuracy,weusethatmeasure;otherwise,weuseaccuracy.
29Generative AI Research
C.2 CONTINUALPRE-TRAININGEVALUATION
Weevaluateallbenchmarksimplementedinthemath-eval-harnessrepository,5including:
• Math(MATH)(Hendrycksetal.,2021)
• GSM8K(Cobbeetal.,2021)
• SVAMP(Pateletal.,2021)
• ASDiv(Miaoetal.,2020)
• MAWPS(Koncel-Kedziorskietal.,2016)
• MathQA(MQA)(Aminietal.,2019)
• TableMWP(TAB)(Luetal.,2023)
• SATMATH(Azerbayevetal.,2024)
We use few-shot CoT prompting (Wei et al., 2022) when evaluating these tasks, and report the
accuracyofeachtask.
5https://github.com/ZubinGou/math-evaluation-harness
30Generative AI Research
D FULL EVALUATION RESULTS
D.1 DETAILEDPERFORMANCEON10BENCHMARKSINSEC3.2
WereportfullevaluationresultsofcheckpointssavedatdifferenttrainingstepsinSection3.2. We
presenttheresultsfor0.7BmodelstrainedondatacuratedbydifferentmethodsinTable10,including
modelstrainedonrawdata,rule-basedfiltereddata,anddatacuratedbyPROX.
Table10: FullevaluationresultsonTLM-S.
Train
ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG
Steps
RawData
2500 22.1 39.0 27.6 31.6 25.9 26.6 61.2 37.3 48.9 59.1 37.9
5000 24.4 41.2 28.8 34.8 26.7 27.0 64.9 39.3 50.4 61.9 39.9
7500 26.5 43.9 29.5 37.2 27.2 29.0 64.8 38.7 50.8 68.2 41.6
10000 25.8 43.5 29.1 38.8 27.4 29.8 66.9 39.0 51.2 66.2 41.8
12500 26.1 44.3 29.7 39.1 27.3 29.2 66.9 39.0 52.0 67.4 42.1
Gopher
2500 22.3 39.4 26.6 31.3 25.6 27.0 61.1 38.9 51.3 58.6 38.2
5000 25.1 41.4 29.8 34.3 26.4 27.2 64.5 39.6 52.1 62.9 40.3
7500 26.5 43.0 30.5 38.5 27.2 28.8 65.7 38.2 53.7 66.4 41.8
10000 26.2 44.2 31.8 39.2 27.5 29.4 66.6 38.9 51.3 68.2 42.3
12500 25.7 44.0 31.3 40.2 27.3 29.0 66.3 39.0 51.2 68.9 42.3
C4
2500 22.6 40.6 28.8 31.3 26.2 27.4 61.7 39.3 51.2 57.1 38.6
5000 22.9 41.6 29.3 36.0 26.8 27.6 64.7 40.2 50.9 63.6 40.4
7500 24.2 44.2 29.5 39.2 27.2 28.4 66.2 40.9 51.6 63.8 41.5
10000 24.6 44.8 30.4 39.5 27.0 29.4 68.7 40.9 51.7 63.9 42.1
12500 25.0 46.0 31.0 40.5 27.1 29.2 68.5 40.5 51.7 66.6 42.6
FineWeb
2500 23.2 39.4 27.2 31.8 25.6 26.2 62.6 39.0 51.4 57.1 38.3
5000 24.2 42.3 29.8 36.2 27.0 28.4 64.3 38.9 51.4 61.4 40.4
7500 24.4 44.1 30.4 37.8 27.2 28.2 66.1 39.5 50.8 66.2 41.5
10000 23.6 46.6 32.0 39.6 27.0 27.8 66.3 39.2 53.1 70.5 42.6
12500 25.2 46.8 32.6 39.6 27.2 29.0 66.5 39.4 52.4 69.2 42.8
Gopher+C4+FineWeb
2500 23.6 39.3 27.6 32.1 25.8 26.0 61.7 39.8 50.9 55.4 38.2
5000 23.9 40.9 29.0 36.2 26.9 26.8 65.3 39.3 52.7 62.4 40.3
7500 25.6 42.2 30.7 39.7 27.0 28.4 66.0 40.2 51.8 60.9 41.2
10000 25.8 43.3 30.8 41.4 27.5 29.8 66.9 39.5 51.8 63.1 42.0
12500 25.0 43.9 30.0 41.9 27.5 31.0 67.0 39.9 51.9 65.3 42.3
PROX-D
2500 25.6 43.2 27.7 32.9 27.2 27.0 61.3 39.4 50.6 63.0 39.8
5000 25.4 46.2 28.4 35.7 28.1 28.8 64.7 39.3 53.3 64.2 41.4
7500 26.9 49.2 29.1 39.2 28.6 30.8 65.4 38.8 51.2 71.7 43.1
10000 26.7 48.2 30.5 39.9 28.6 28.6 66.2 39.7 51.9 71.2 43.2
12500 26.6 49.7 30.1 40.5 29.4 30.4 66.3 39.0 51.2 71.6 43.5
PROX-D+C
2500 24.9 43.4 27.3 32.1 26.9 28.2 60.9 38.8 51.2 60.8 39.5
5000 24.9 49.6 28.8 36.8 27.9 30.6 64.7 38.8 51.1 66.9 42.0
7500 25.5 51.2 30.8 38.8 28.4 31.2 67.3 40.2 50.3 71.7 43.5
10000 26.2 51.7 30.8 39.9 29.0 32.6 68.6 39.7 51.7 73.7 44.4
12500 26.4 51.9 30.9 42.4 29.4 31.6 67.9 40.0 52.2 73.5 44.6
31Generative AI Research
D.2 DETAILEDPERFORMANCEON8BENCHMARKSUSEDINDATASELECTION
EXPERIMENTS
Thefullbenchmarkperformanceusedindata-selectionmethodcomparisonexperimentsispresented
inTable11.
Table11: Detailedevaluationresultsfordifferentdataselectionmethods.
Method ARC-C ARC-E HellaSwag LogiQA OBQA PIQA WinoGrande SciQ AVG
PYTHIA-410M0-shot
Random 25.6 40.2 39.7 24.7 29.4 67.1 50.6 64.1 42.7
DSIR 23.8 39.9 39.6 27.0 28.4 66.8 51.5 63.1 42.5
DsDm 24.7 41.7 40.3 27.5 29 68.1 50.1 65.4 43.4
QuRating 25.4 42.0 40.7 25.3 30.2 67.5 52.1 64.8 43.5
MATES 25.0 41.8 41.0 25.7 30.8 68.7 52.7 66.0 44.0
PROX 27.2 48.9 43.1 26.9 31.8 68.4 54.1 69.5 46.2
PYTHIA-410M2-shot
Random 25.3 42.6 39.9 24.1 28.6 66.9 52.2 70.6 43.8
DSIR 23.6 42.0 39.8 26.1 28.6 66.1 51.6 71.4 43.7
DsDm 23.6 44.2 40.1 23.5 29.2 66.5 51.5 74 44.1
QuRating 23.6 43.9 40.4 26.1 30.2 67.4 51.4 74.1 44.6
MATES 25.3 43.8 40.6 24.9 30.6 67.1 53.4 74.1 45.0
PROX 27.0 52.7 42.6 23.7 32.8 68.2 53.9 78.9 47.5
PYTHIA-1B0-shot
Random 25.6 43.7 43.8 27.5 31.8 68.9 50.7 65.8 44.7
MATES 25.9 44.9 45.3 28.7 32.2 69.5 52.4 67.3 45.8
PROX 26.2 49.1 46.6 24.8 32.2 70.3 54.2 70.9 46.8
PYTHIA-1B2-shot
Random 25.5 45.1 42.9 24.6 30.0 68.3 52.1 74.6 45.4
MATES 26.8 46.1 44.8 25.2 30.6 68.7 51.6 75.7 46.2
PROX 27.3 54.5 46.2 26.6 32.2 69.0 53.9 77.4 48.4
ARC-C ARC-E CSQA HellaSwag MMLU
50 32 40 29
26
45 30 28
24 40 28 35 27
DF+LF DF+LF 26 DF+LF DF+LF 26 DF+LF
DF 35 DF DF 30 DF DF
22 Rule Rule 24 Rule Rule 25 Rule
Raw 30 Raw Raw Raw Raw
0 10 20 0 10 20 0 10 20 0 10 20 0 10 20
Training Tokens(B) Training Tokens(B) Training Tokens(B) Training Tokens(B) Training Tokens(B)
OBQA PiQA SIQA WinoG SciQ
32 40 54 70
65
30 39 52 60
28 60
DF+LF DF+LF 38 DF+LF 50 DF+LF 50 DF+LF
26 D RuF le 55 D RuF le D RuF le D RuF le D RuF le
24 Raw Raw 37 Raw 48 Raw 40 Raw
0 10 20 0 10 20 0 10 20 0 10 20 0 10 20
Training Tokens(B) Training Tokens(B) Training Tokens(B) Training Tokens(B) Training Tokens(B)
Figure13: Visualizationofdynamicperformanceontenbenchmarks.
32
ecnamrofreP
ecnamrofreP
ecnamrofreP
ecnamrofreP
ecnamrofreP
ecnamrofreP
ecnamrofreP
ecnamrofreP
ecnamrofreP
ecnamrofrePGenerative AI Research
D.3 DETAILEDPERFORMANCEINSEC3.3
In§3.3,wetestPROX’seffectivenessusingdifferentsizesofrefiningmodels,andalsotrainaseries
ofmodelsbyusingthesecurateddata. WereportthesedetailedresultsinTable12,Table13and
Table14.
Table12: FullevaluationresultsofTLM-XStrainedondifferentPROXmodelcurateddata.
Train
ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG
Steps
TLM-XStrainedonRawdata
2500 22.5 38.5 27.0 29.1 25.8 25.0 60.2 38.8 50.4 58.6 37.6
5000 23.6 39.2 28.7 33.1 26.1 26.6 62.2 39.5 49.9 66.2 39.5
7500 23.8 42.7 28.0 33.4 26.0 26.2 64.0 39.3 51.5 67.0 40.2
10000 23.8 41.2 27.8 35.0 26.6 28.0 65.3 40.9 50.1 65.9 40.5
12500 22.6 41.9 29.7 32.8 26.2 26.4 62.2 39.3 51.3 63.3 39.6
TLM-XStrainedonPROX-xsdata
2500 24.8 43.5 26.5 30.3 26.8 26.6 59.3 38.6 50.8 60.7 38.8
5000 23.7 44.3 28.1 33.8 27.3 28.8 61.3 38.9 50.9 70.2 40.7
7500 24.1 46.0 29.2 35.0 27.7 30.6 63.4 38.7 52.0 70.4 41.7
10000 25.3 46.1 28.3 35.7 28.1 29.2 64.4 38.5 51.2 70.6 41.7
12500 25.9 47.5 29.2 36.7 28.1 30.2 64.6 38.0 51.7 71.4 42.3
TLM-XStrainedonPROX-sdata
2500 23.5 41.9 24.9 30.4 26.6 27.6 62.0 37.8 49.3 61.4 38.5
5000 24.7 44.5 27.0 33.8 27.5 28.0 62.4 38.0 50.6 67.0 40.3
7500 25.3 45.3 27.3 34.0 27.9 29.2 63.4 37.7 52.9 68.7 41.2
10000 25.6 45.7 27.6 35.6 28.6 30.2 63.6 37.4 52.0 71.1 41.7
12500 26.4 46.7 27.5 37.2 28.1 29.8 62.8 37.8 52.2 70.1 41.9
TLM-XStrainedonPROX-mcurateddata
2500 22.9 41.3 26.5 31.1 26.9 27.0 62.2 37.6 50.6 62.4 38.9
5000 25.8 44.0 27.3 34.0 27.1 29.6 63.1 38.5 51.8 64.9 40.6
7500 26.0 45.3 28.5 36.6 27.7 29.8 63.6 39.4 51.3 68.5 41.7
10000 26.0 46.6 28.8 37.3 27.6 30.6 63.3 38.7 51.6 70.3 42.1
12500 26.5 46.4 29.1 37.6 28.1 29.4 64.1 38.7 51.5 68.0 41.9
Table13: FullevaluationresultsofTLM-StrainedondifferentPROXmodelcurateddata.
Train
ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG
Steps
TLM-StrainedonRawdata
2500 22.1 39.0 27.6 31.6 25.9 26.6 61.2 37.3 48.9 59.1 37.9
5000 24.4 41.2 28.8 34.8 26.7 27.0 64.9 39.3 50.4 61.9 39.9
7500 26.5 43.9 29.5 37.2 27.2 29.0 64.8 38.7 50.8 68.2 41.6
10000 25.8 43.5 29.1 38.8 27.4 29.8 66.9 39.0 51.2 66.2 41.8
12500 26.1 44.3 29.7 39.1 27.3 29.2 66.9 39.0 52.0 67.4 42.1
TLM-StrainedonPROX-xscurateddata
2500 23.8 44.1 26.5 33.5 26.9 29.4 60.7 38.9 50.6 62.1 39.6
5000 26.8 48.1 28.4 36.7 28.0 30.6 64.0 38.6 50.3 65.6 41.7
7500 26.9 49.0 30.6 39.5 28.2 29.6 65.3 39.6 52.2 69.6 43.0
10000 26.7 51.3 29.4 40.1 28.3 31.8 64.1 39.3 51.4 69.9 43.2
12500 26.8 52.1 30.2 41.8 28.5 31.6 65.5 39.5 51.9 70.8 43.9
TLM-StrainedonPROX-scurateddata
2500 24.9 43.4 27.3 32.1 26.9 28.2 60.9 38.8 51.2 60.8 39.5
5000 24.9 49.6 28.8 36.8 27.9 30.6 64.7 38.8 51.1 66.9 42.0
7500 25.5 51.2 30.8 38.8 28.4 31.2 67.3 40.2 50.3 71.7 43.5
10000 26.2 51.7 30.8 39.9 29.0 32.6 68.6 39.7 51.7 73.7 44.4
12500 26.4 51.9 30.9 42.4 29.4 31.6 67.9 40.0 52.2 73.5 44.6
TLM-StrainedonPROX-mcurateddata
2500 25.3 45.3 27.5 32.2 26.7 27.0 62.4 38.7 50.6 60.8 39.6
5000 26.1 45.4 28.6 37.2 27.4 27.8 65.7 38.9 50.9 65.6 41.4
7500 27.1 47.5 30.6 41.0 28.6 29.2 66.8 39.3 51.1 69.9 43.1
10000 26.7 50.5 30.7 41.5 28.4 30.2 67.0 40.1 49.9 70.9 43.6
12500 27.4 50.7 30.6 42.0 28.8 30.2 67.4 39.4 48.8 70.1 43.5
33Generative AI Research
Table14: FullevaluationresultsofTLM-MtrainedondifferentPROXmodelcurateddata.
Train
ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG
Steps
TLM-StrainedonRawdata
2500 23.5 41.5 27.5 32.9 26.4 25.2 62.1 39.4 51.5 65.1 39.5
5000 24.0 42.1 29.6 37.6 27.6 27.2 65.0 39.7 53.2 68.5 41.4
7500 24.3 44.9 28.9 39.3 27.8 27.6 66.4 40.4 51.3 69.2 42.0
10000 24.8 46.1 29.6 41.4 27.9 28.4 67.5 39.8 51.9 70.9 42.8
12500 26.3 46.8 29.0 43.2 28.3 27.8 68.2 40.5 50.7 72.5 43.3
TLM-MtrainedonPROX-xscurateddata
2500 24.9 49.6 26.5 34.0 27.3 30.4 61.8 37.9 51.3 65.1 40.9
5000 26.7 47.6 28.6 39.7 28.5 31.8 65.4 39.5 50.2 70.7 42.9
7500 27.5 52.1 30.4 41.8 29.6 31.8 67.6 39.6 51.7 75.2 44.7
10000 28.4 54.7 29.8 45.2 30.8 31.8 67.9 39.7 52.0 77.7 45.8
12500 28.8 54.2 29.7 46.5 30.9 31.8 68.2 39.9 51.3 78.3 46.0
TLM-MtrainedonPROX-scurateddata
2500 25.3 45.7 27.8 34.2 27.8 29.0 64.4 37.5 49.3 66.3 40.7
5000 26.1 49.0 28.8 40.2 29.2 30.8 65.6 39.0 50.5 71.2 43.0
7500 27.7 53.6 31.1 44.1 29.6 34.8 67.6 39.4 52.5 72.2 45.3
10000 27.2 54.0 31.5 45.1 30.3 33.8 67.7 39.7 52.9 74.2 45.6
12500 28.6 56.1 31.8 45.5 30.5 34.4 68.5 39.4 51.3 76.1 46.2
TLM-MtrainedonPROX-mcurateddata
2500 24.7 44.1 25.9 34.8 27.4 27.8 62.9 38.9 49.2 67.0 40.3
5000 27.7 48.0 26.8 40.5 28.5 30.6 67.4 39.4 50.3 69.1 42.8
7500 26.7 51.9 26.7 42.9 29.3 31.4 69.1 40.3 50.4 73.3 44.2
10000 28.4 52.4 27.9 45.0 29.7 32.0 70.2 40.0 51.9 75.4 45.3
12500 28.3 53.7 28.4 45.9 30.1 33.8 70.6 41.1 52.3 72.5 45.7
WealsofurtherscalePROXtoothertwopre-trainingcorpora,C4andFineWeb. Wealsoscaleour
trainingtoabout50Btokens,anddirectlycomparewithexistingwell-trainedmodelsdevelopedby
differentresearchgroups. WereportourdetailedresultsinTable15,Table16andTable17. Wealso
presentothermodels’resultsinTable18.
Table15: Fullevaluationresultsonscalingpre-trainingtoabout50BtokensonRedPajama-V2.
Train
ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG
Steps
TLM-MtrainedonRedPajama-V2rawdata.
2500 24.0 42.9 26.6 33.7 25.9 26.0 62.4 39.4 52.3 64.0 39.7
5000 24.3 45.9 26.4 37.4 27.0 27.6 64.1 39.7 49.5 66.2 40.8
7500 25.1 45.3 28.8 40.3 27.1 29.2 66.3 39.1 51.7 66.9 42.0
10000 25.8 49.3 31.5 42.5 28.0 28.8 66.7 39.6 51.5 74.0 43.8
12500 25.3 50.1 30.2 43.0 28.2 30.0 66.6 39.2 51.1 74.2 43.8
15000 26.2 50.3 31.2 44.3 28.8 28.4 68.2 39.8 51.7 76.2 44.5
17500 25.8 51.1 30.8 44.7 29.0 29.6 67.7 39.2 52.6 75.2 44.6
20000 26.7 52.5 31.7 47.2 28.6 30.4 69.0 39.6 53.0 78.2 45.7
22500 27.4 51.7 32.1 47.2 29.3 30.4 69.5 39.5 51.9 78.5 45.7
25000 26.9 51.4 32.4 47.3 29.3 32.2 69.7 39.6 52.1 79.1 46.0
TLM-MtrainedonPROXrefinedRedPajama-V2data.
2500 24.8 46.8 27.2 33.8 27.3 28.2 61.3 38.6 50.3 65.1 40.3
5000 26.9 49.3 28.5 40.1 28.0 30.6 66.2 39.7 50.2 70.1 43.0
7500 28.5 53.1 29.2 41.7 29.4 33.2 66.9 39.3 53.0 73.0 44.7
10000 28.2 53.5 30.1 43.6 29.8 31.6 68.4 39.6 52.0 75.3 45.2
12500 29.5 55.3 30.2 46.4 30.5 32.2 68.6 40.2 52.6 76.9 46.2
15000 30.0 57.1 30.2 47.6 30.9 33.0 69.5 39.8 52.2 77.8 46.8
17500 31.5 59.6 29.4 49.5 31.6 33.6 69.4 39.8 53.0 78.9 47.6
20000 31.2 61.2 29.4 50.4 31.4 35.2 70.6 40.1 53.7 79.6 48.3
22500 32.0 61.7 30.2 51.4 31.4 34.0 70.0 39.9 53.2 79.5 48.3
25000 31.1 60.7 29.8 51.0 31.7 33.2 70.9 39.2 53.3 79.1 48.0
34Generative AI Research
Table16: Fullevaluationresultsonscalingpre-trainingtoabout50BtokensonC4.
Train
ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG
Steps
TLM-MtrainedonC4rawdata.
2500 22.4 39.7 26.8 36.5 26.5 27.6 64.8 40.2 50.1 60.0 39.5
5000 23.9 42.9 27.5 42.3 27.1 29.6 68.2 39.6 50.3 66.6 41.8
7500 25.1 44.8 28.2 45.4 27.1 29.2 70.7 40.7 51.6 66.3 42.9
10000 25.5 46.0 32.3 48.2 27.9 31.6 71.1 39.7 52.3 67.6 44.2
12500 25.8 48.8 30.3 49.7 27.9 31.6 71.2 40.9 52.0 69.4 44.8
15000 26.9 48.0 28.2 50.5 28.5 31.4 71.9 41.1 51.4 69.7 44.8
17500 26.6 48.8 30.3 52.1 28.6 31.2 73.2 41.6 52.0 70.0 45.4
20000 26.3 50.1 29.7 52.5 28.5 32.6 72.3 41.7 52.3 71.0 45.7
22500 25.8 50.7 31.0 52.9 28.8 33.8 73.0 41.6 53.0 71.5 46.2
25000 25.3 48.8 30.1 52.4 28.8 32.2 72.0 40.6 53.6 71.7 45.5
TLM-MtrainedonPROXrefinedC4data.
2500 24.1 45.9 26.0 37.3 27.2 29.0 66.3 39.8 50.8 65.9 41.2
5000 27.3 50.0 26.6 42.4 28.6 33.8 68.1 40.5 53.0 71.9 44.2
7500 28.3 53.7 27.7 47.7 29.3 35.4 71.1 39.3 54.0 73.1 46.0
10000 30.0 54.3 28.1 50.9 30.0 33.6 71.2 40.6 52.0 74.2 46.5
12500 29.3 56.7 27.5 52.3 30.9 33.8 72.8 39.9 52.5 77.5 47.3
15000 29.6 55.9 28.3 53.9 30.6 35.0 72.9 41.0 53.8 75.8 47.7
17500 30.6 55.5 28.7 53.3 31.2 34.2 73.6 40.4 53.4 76.7 47.8
20000 30.0 57.6 28.3 54.9 31.1 37.2 74.6 40.7 53.6 79.4 48.7
22500 30.1 56.7 28.6 55.2 31.4 37.2 73.8 41.6 53.3 77.7 48.6
25000 31.1 56.0 28.4 55.2 31.1 36.2 74.0 41.0 54.1 76.8 48.4
Table17: Fullevaluationresultsonscalingpre-trainingtoabout50BtokensonFineWeb.
Train
ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG
Steps
TLM-MtrainedonFineWebrawdata.
2500 22.9 41.2 28.9 34.3 26.1 27.6 64.8 39.3 52.1 62.8 40.0
5000 25.5 44.5 30.4 39.8 26.9 32.0 68.4 39.2 52.1 67.2 42.6
7500 26.8 45.6 31.4 44.1 27.6 30.2 70.9 38.8 52.2 70.3 43.8
10000 27.2 46.2 31.3 47.2 28.3 31.6 72.1 38.8 53.4 69.0 44.5
12500 26.4 49.2 32.1 48.7 28.7 31.6 71.5 40.1 52.6 74.7 45.6
15000 27.1 49.6 32.8 49.5 28.9 31.0 72.7 39.0 52.3 77.1 46.0
17500 26.4 50.9 33.8 51.3 29.3 31.0 71.9 39.3 53.0 78.0 46.5
20000 27.1 53.1 33.2 51.2 29.6 32.2 73.4 39.7 52.3 76.3 46.8
22500 27.1 51.2 34.9 51.7 29.5 33.4 73.7 40.1 52.4 78.0 47.2
25000 28.5 52.6 33.9 53.2 29.8 32.6 72.9 40.2 53.0 77.1 47.4
TLM-MtrainedonPROXrefinedFineWebdata.
2500 25.8 46.8 27.4 36.1 27.7 28.8 63.9 39.3 51.9 69.1 41.7
5000 28.5 52.1 28.8 43.5 29.3 32.6 66.4 38.7 51.2 71.3 44.2
7500 28.2 52.0 30.6 45.9 29.9 33.0 69.3 39.5 51.7 71.8 45.2
10000 29.3 54.3 30.6 48.5 30.8 33.2 69.7 40.7 50.6 74.4 46.2
12500 28.7 57.8 30.7 48.1 31.1 32.6 72.0 40.4 52.7 77.4 47.2
15000 31.1 59.6 31.9 50.4 31.8 34.4 71.9 40.5 50.8 78.0 48.0
17500 32.6 60.9 31.9 51.5 32.2 33.8 72.3 39.7 52.5 78.9 48.6
20000 33.2 62.5 32.5 51.6 32.4 34.6 72.4 39.7 51.7 80.7 49.1
22500 34.7 63.6 32.9 53.3 32.9 34.8 73.1 40.3 54.2 80.5 50.0
25000 34.4 63.9 32.6 53.0 33.1 34.4 73.1 39.3 52.7 81.5 49.8
35Generative AI Research
Table18: Detailedevaluationresultsofexistingbasemodelstrainedondifferentcorporaandtrained
usingdifferenttechniques.
ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG
TINYLLAMA-1.1B(trainedon3Ttokens)
31.5 59.0 35.5 57.8 32.8 33.4 72.8 40.0 56.0 82.4 50.1
OLMO-1B(trainedon2Ttokens)
31.4 59.7 38.9 61.9 32.2 38.4 76.1 41.5 53.9 78.8 51.3
PYTHIA-1.4B
28.7 56.9 34.7 51.7 31.5 36.0 71.8 40.8 55.1 79.3 48.7
PYTHIA-2.8B
32.9 61.0 36.5 60.4 33.3 35.0 73.5 41.1 57.0 83.1 51.4
SHEAREDLLAMA-1.3B(prunedfromLLAMA-2-7B)
22.4 39.7 29.3 36.0 26.4 28.4 62.6 39.9 52.0 71.4 40.8
SHEAREDLLAMA-1.3B(prunedfromLLAMA-2-7B,andfurthertrainedon50Btokens)
29.0 58.3 34.8 59.6 32.0 35.0 74.6 41.0 56.3 82.3 50.3
INSTRUCTLM-1.3B(LLMdatasynthesis)
28.1 57.9 32.5 52.3 30.0 34.0 74.5 39.9 56.1 86.9 49.2
COSMO-1.8B(LLMdatasynthesis)
33.4 57.0 31.2 55.1 32.4 35.2 71.4 42.0 54.7 84.4 49.7
36Generative AI Research
D.4 EVALUATIONRESULTSOFCONTINUALPRE-TRAININGINSEC3.4
We provide full ablation results for each base model, as shown in Table 19. We can observe
that PROX-D+C consistently improves average performance over PROX-D across various base
models. AlthoughtheperformancegainfromPROX-D+CcomparedtoPROX-Dislesspronounced
thantheimprovementof PROX-Dovercontinualpre-trainingonrawOpenWebMath,thisisboth
understandable and expected. PROX-D+C does not significantly reduce the token count beyond
thereductionsachievedby PROX-Dalone. GiventhescaleoftheOpenWebMathcorpus,amore
aggressivetokenremovalstrategycouldpotentiallydiminishthediversityofuniquetokensbelow
thethresholdnecessaryforrobustpre-training. Thisobservationunderscoresthedelicatebalance
betweendatarefinementandmaintainingsufficientlinguisticvarietyforeffectivelanguagemodel
training,particularlywhenworkingwithlimited-scalecorpora.
Table19: FullablationresultsonOpenWebMathContinualPre-training(CPT).Allmodelsaretested
usingfew-shotCoTprompts. LLEMMAandINTERNLM2-MATHarecontinualpre-trainedmodels
from CODELLAMA (Rozière et al., 2023) and INTERNLM2 (Team, 2023) with public available
data,respectively. DEEPSEEK-LLMdenotesaninternalDeepSeekmodel,andthemodeltrained
onOpenWebMathintroducedbyShaoetal.(2024). Notethattheuniquetokensandtrainingtokens
in the column refer exclusively to the token numbers from math-specific corpora (calculated by
correspondingtokenizers). †: MQAevaluationof INTERNLM2-BASE isbasedonanalternative
promptduetonon-predictionissueswiththeoriginalprompt. Theboldedentriesrepresentthebest
resultswithinthesamebasemodelandCPTexperiments.
Uniq Train MMLU SAT
Model Size Method GSM8KMATHSVAMPASDivMAWPSTABMQA AVG
Toks Toks STEM MATH
ExistingContinualPre-trainingforReference
1.3B- - - 2.9 3.0 - - - - - 19.5 15.6 -
DEEPSEEK-LLM
1.3B- 14B 150B 11.5 8.9 - - - - - 29.6 31.3 -
7B - - - 11.8 5.0 44.2 50.7 62.6 30.6 14.3 20.4 21.9 29.1
CODELLAMA(Base) 34B - - - 31.8 10.8 61.9 66.0 83.4 51.6 23.7 43.0 53.1 47.3
7B - 55B 200B 38.8 17.2 56.1 69.1 82.4 48.7 41.0 45.4 59.4 50.9(+21.8)
LLEMMA
34B - 55B 50B 54.2 23.0 67.9 75.7 90.1 57.9 49.8 54.7 68.8 60.1(+12.8)
7B - - - 27.0 6.6 49.0 59.3 74.8 40.1 20.9† 19.0 28.1 36.1
INTERNLM2-BASE
20B - - - 50.6 18.8 72.5 75.9 93.9 45.4 33.1 53.7 59.4 55.9
7B - 31B 125B 41.8 14.4 61.6 66.8 83.7 50.0 57.3 24.8 37.5 48.7(+12.6)
INTERNLM2-MATH
20B - 120B 500B 65.4 30.0 75.7 79.3 94.0 50.9 38.5 53.1 71.9 62.1(+6.2)
ApplyingDataRefinementApproaches
TINYLLAMA(Base) 1.1B- - - 2.8 3.2 10.9 18.0 20.2 12.5 14.6 16.4 21.9 14.7
1.1B- 15B 15B 6.2 4.8 22.3 36.2 47.6 19.3 11.6 20.7 25.0 21.5(+8.1)
1.1B RHO 15B 9B∗6 7.1 5.0 23.5 41.2 53.8 - 18.0 - - -
TINYLLAMA(CPT)
1.1BRule 6.5B 15B 4.5 2.8 17.5 29.4 39.3 15.1 12.4 19.4 25.0 18.4(+3.7)
1.1B PROX-D 5.4B 15B 9.3 7.4 23.4 41.9 55.6 22.1 14.6 24.1 25.0 24.8(+10.1)
1.1B PROX-D+C 5B 15B 9.0 5.6 23.8 41.9 56.9 22.2 15.6 26.8 31.2 25.7(+11.0)
LLAMA-2(Base) 7B - - - 14.1 3.8 39.5 51.6 63.6 30.9 12.5 32.9 34.4 31.5
7B - 15B 10B 29.6 13.6 49.2 61.9 78.4 36.3 31.9 40.5 43.8 42.8(+11.3)
LLAMA-2(CPT) 7B PROX-D 5.4B 10B 30.3 16.0 54.2 63.8 79.5 37.3 37.2 44.2 46.9 45.5(+14.0)
7B PROX-D+C 5B 10B 30.6 16.8 50.2 63.7 79.3 37.3 40.1 43.8 53.1 46.1(+14.6)
CODELLAMA(Base) 7B - - - 11.8 5.0 44.2 50.7 62.6 30.6 14.3 20.4 21.9 29.1
7B - 15B 10B 31.1 14.8 51.4 62.1 81.2 33.6 30.4 40.5 43.8 43.2(+14.1)
CODELLAMA(CPT) 7B PROX-D 5.4B 10B 38.1 17.0 54.2 67.0 83.1 40.9 39.8 43.7 50.0 48.2(+19.1)
7B PROX-D+C 5B 10B 35.6 17.6 55.8 67.9 82.7 41.3 38.9 42.6 62.5 49.4(+20.3)
MISTRAL(Base) 7B - - - 40.6 11.4 65.4 68.5 87.0 52.9 32.3 50.0 56.2 51.6
7B - 15B 10B 44.4 19.2 65.2 69.6 88.4 46.6 43.1 50.8 65.6 54.8(+3.2)
MISTRAL(CPT) 7B PROX-D 5.5B 10B 47.8 24.8 63.5 72.4 88.9 48.3 48.2 54.1 62.5 56.4(+4.8)
7B PROX-D+C 4.7B 10B 51.0 22.4 64.9 72.9 89.2 49.8 53.0 54.2 75.0 59.2(+7.6)
Besides,wereportthedetaileddynamicevaluationresultsofourcontinualpre-trainingexperiments
onOpenWebMath:
• Tables20,21,22,and23presenttheevaluationresultsforTINYLLAMA-1.1B.
6RHO-1onlycountstheselectedtokensthatareusedfortraining(losscalculation).
37Generative AI Research
• Tables24,25,and26presenttheevaluationresultsforLLAMA-2.
• Tables27,28,29presenttheevaluationresultsforCODELLAMA.
• Tables30,31,and32showtheevaluationresultsforMISTRAL-7B.
Table20: Fullevaluationresultsof TINYLLAMA-1.1B continualpre-trainingonOpenWebMath
withrawdata. Notethatabout1Btokensaretrainedper500steps.
Train MMLU SAT
GSM8K MATH SVAMP ASDiv MAWPS TAB MQA AVG
Steps STEM MATH
0 2.8 3.2 10.9 18 20.2 12.5 14.6 16.4 21.9 14.7
500 1.9 3.4 16.3 23.9 30.3 13.9 10.3 14.8 18.8 14.8
1000 3.1 2.2 16.6 25.6 32.4 12.5 12.0 16.6 25.0 16.2
1500 2.7 3.0 17.6 28.5 34.5 13.9 8.7 14.1 15.6 15.4
2000 4.5 3.2 16.4 28.5 39.0 15.1 10.2 16.6 34.4 18.7
2500 4.9 3.4 19.3 31.0 39.2 16.0 12.1 18.6 9.4 17.1
3000 4.1 5.2 19.1 32.0 43.0 15.3 9.6 16.1 18.8 18.1
3500 4.9 3.6 19.7 31.4 40.4 18.1 11.3 19.6 15.6 18.3
4000 4.8 4.8 19.5 33.8 44.5 16.4 10.7 19.9 12.5 18.5
4500 5.4 4.8 20.2 35.0 45.2 17.9 12.7 21.0 18.8 20.1
5000 5.5 4.6 22.3 34.6 42.9 16.0 10.6 21.7 28.1 20.7
5500 4.9 5.8 23.6 35.2 44.0 20.4 11.0 21.1 21.9 20.9
6000 6.1 4.4 22.8 36.2 45.4 17.8 12.7 21.4 15.6 20.3
6500 6.3 3.6 23.2 37.3 48.0 19.7 10.3 21.0 18.8 20.9
7000 6.1 4.6 22.2 36.6 46.9 19.4 12.0 21.5 21.9 21.2
7500 6.2 4.8 22.3 36.2 47.6 19.3 11.6 20.7 25.0 21.5
Table21: Fullevaluationresultsof TINYLLAMA-1.1B continualpre-trainingonOpenWebMath
withdataafterrule-basedfiltering. Notethatabout1Btokensaretrainedper500steps.
Train MMLU SAT
GSM8K MATH SVAMP ASDiv MAWPS TAB MQA AVG
Steps STEM MATH
0 2.8 3.2 10.9 18 20.2 12.5 14.6 16.4 21.9 14.7
500 3.4 3.6 13.6 22.5 25.9 13.1 14.2 13.5 28.1 15.3
1000 3.0 2.8 14.1 22.5 27.8 11.4 11.0 16.4 12.5 13.5
1500 3.6 3.2 13.6 24.0 31.2 13.9 9.2 18.0 18.8 15.1
2000 3.5 2.4 15.0 25.1 33.0 12.5 10.6 13.9 15.6 14.6
2500 3.3 1.6 15.0 25.3 33.5 13.7 11.1 18.1 25.0 16.3
3000 3.5 3.0 16.4 25.5 33.4 14.1 10.2 18.4 18.8 15.9
3500 3.2 3.4 17.2 27.0 37.7 14.6 11.2 13.3 25.0 17.0
4000 3.5 3.6 15.6 26.2 36.5 13.4 12.1 15.9 18.8 16.2
4500 4.1 3.8 15.6 27.9 38.2 14.9 11.6 17.1 18.8 16.9
5000 4.2 3.6 18.6 28.7 37.7 14.3 12.7 17.5 21.9 17.7
5500 4.1 3.8 16.3 29.3 38.4 14.7 10.8 17.5 18.8 17.1
6000 4.3 3.6 16.0 28.7 39.1 13.5 12.8 19.5 21.9 17.7
6500 4.2 3.2 16.4 29.5 39.0 15.1 11.7 17.9 21.9 17.7
7000 4.0 4.0 16.2 29.6 37.9 16.0 13.8 17.8 21.9 17.9
7500 4.5 2.8 17.5 29.4 39.3 15.1 12.4 19.4 25.0 18.4
38Generative AI Research
Table22: Fullevaluationresultsof TINYLLAMA-1.1B continualpre-trainingonOpenWebMath
withdataafterPROX-D.Notethatabout1Btokensaretrainedper500steps.
Train MMLU SAT
GSM8K MATH SVAMP ASDiv MAWPS TAB MQA AVG
Steps STEM MATH
0 2.8 3.2 10.9 18 20.2 12.5 14.6 16.4 21.9 14.7
500 3.3 2.8 17.7 29.0 38.7 12.4 9.5 15.7 15.6 16.1
1000 4.6 4.0 18.1 31.6 41.9 15.9 11.9 18.2 25.0 19.0
1500 5.2 5.4 21.1 32.9 43.1 15.3 11.1 20.4 12.5 18.6
2000 6.8 5.8 20.2 33.5 46.6 18.2 10.7 20.3 12.5 19.4
2500 7.1 3.8 20.7 37.0 48.6 18.3 12.0 21.4 18.8 20.9
3000 7.4 4.4 22.9 37.1 50.5 18.3 12.3 21.2 25.0 22.1
3500 8.8 4.8 22.8 39.4 53.3 19.2 12.0 22.8 34.4 24.2
4000 8.6 4.6 24.0 38.7 51.4 18.8 14.8 24.4 18.8 22.7
4500 8.6 4.2 24.2 39.2 53.6 20.4 13.5 23.9 18.8 22.9
5000 8.9 5.2 24.0 40.0 52.6 20.0 13.6 23.9 18.8 23.0
5500 8.0 6.2 23.2 41.4 55.0 22.3 14.3 24.9 25.0 24.5
6000 8.3 5.2 22.2 39.8 54.0 24.3 12.6 25.1 31.2 24.7
6500 9.4 5.6 24.4 40.2 54.5 20.3 13.0 24.9 31.2 24.8
7000 9.2 5.8 25.8 40.6 55.3 22.5 12.5 24.5 21.9 24.2
7500 9.3 7.4 23.4 41.9 55.6 22.1 14.6 24.1 25.0 24.8
Table23: Fullevaluationresultsof TINYLLAMA-1.1B continualpre-trainingonOpenWebMath
withdataafterPROX-D+C.Notethatabout1Btokensaretrainedper500steps.
Train MMLU SAT
GSM8K MATH SVAMP ASDiv MAWPS TAB MQA AVG
Steps STEM MATH
0 2.8 3.2 10.9 18 20.2 12.5 14.6 16.4 21.9 14.7
500 4.3 5.0 16.4 28.8 36.4 15.3 11.4 18.5 15.6 16.9
1000 5.5 3.8 20.5 34.6 44.6 15.3 12.1 19.6 28.1 20.5
1500 5.2 4.4 21.4 34.5 44.7 16.1 11.2 21.4 34.4 21.5
2000 6.3 5.4 20.1 33.7 46.2 19.4 10.5 21.2 12.5 19.5
2500 7.8 5.4 22.1 37.0 49.5 17.9 13.3 22.9 21.9 22.0
3000 6.4 3.4 23.0 38.6 51.1 18.5 12.6 24.3 18.8 21.9
3500 8.5 4.6 24.1 40.2 53.8 22.1 12.5 23.1 25.0 23.8
4000 8.2 6.0 24.1 41.0 52.4 19.8 10.2 26.1 31.2 24.3
4500 8.3 5.4 24.1 41.3 54.4 20.6 15.2 24.2 28.1 24.6
5000 8.5 7.0 26.0 40.5 54.9 21.7 13.9 25.5 34.4 25.8
5500 8.7 4.0 23.2 41.1 54.8 20.5 14.4 26.5 21.9 23.9
6000 8.3 5.0 24.8 41.3 54.3 23.2 14.0 25.3 25.0 24.6
6500 8.6 6.4 24.5 41.6 55.1 22.2 14.4 26.5 25.0 24.9
7000 8.9 6.0 23.4 40.5 53.4 22.0 15.8 27.3 28.1 25.0
7500 9.0 4.4 23.8 41.9 56.4 22.2 15.6 26.8 31.2 25.7
39Generative AI Research
Table24:FullevaluationresultsofLLAMA-2continualpre-trainingonOpenWebMathwithrawdata.
Notethatabout1Btokensaretrainedper1000steps.
Train MMLU SAT
GSM8K MATH SVAMP ASDiv MAWPS TAB MQA AVG
Steps STEM MATH
0 14.1 3.8 39.5 51.6 63.6 30.9 12.5 32.9 34.4 31.5
1k 17.2 3.6 39.1 50.4 63.0 30.2 18.9 31.8 31.2 31.7
2k 19.7 6.0 43.9 55.5 68.3 32.9 19.0 33.0 37.5 35.1
3k 19.6 8.6 42.9 56.3 68.4 32.2 17.4 34.6 40.6 35.6
4k 21.8 8.8 44.6 57.3 72.0 28.9 23.6 35.8 40.6 37.0
5k 22.6 10.4 45.9 57.0 73.5 31.5 23.9 39.0 43.8 38.6
6k 24.5 10.0 44.9 57.6 73.7 35.5 25.8 36.1 43.8 39.1
7k 23.3 10.4 46.5 59.0 75.3 32.9 27.7 39.0 50.0 40.5
8k 29.0 12.4 46.4 59.7 77.0 33.1 30.2 38.8 50.0 41.8
9k 26.1 12.8 48.8 59.9 74.3 35.0 28.3 39.2 50.0 41.6
10k 29.6 13.6 49.2 61.9 78.4 36.3 31.9 40.5 43.8 42.8
Table25: FullevaluationresultsofLLAMA-2continualpre-trainingonOpenWebMathwithPROX-
D.Notethatabout1Btokensaretrainedper1000steps.
Train MMLU SAT
GSM8K MATH SVAMP ASDiv MAWPS TAB MQA AVG
Steps STEM MATH
0 14.1 3.8 39.5 51.6 63.6 30.9 12.5 32.9 34.4 31.5
1k 17.1 7.2 39.8 51.6 68.4 31.4 21.4 35.2 40.6 34.7
2k 21.9 9.2 43.2 57.0 72.8 33.1 24.0 37.6 56.2 39.4
3k 20.5 10.8 45.7 58.6 76.2 35.3 25.8 38.3 53.1 40.5
4k 27.2 11.8 45.7 58.7 76.6 35.9 29.2 41.0 31.2 39.7
5k 28.9 14.2 49.3 60.2 77.9 38.8 32.8 41.7 53.1 44.1
6k 31.9 15.0 51.5 62.0 79.0 39.2 33.3 41.4 68.8 46.9
7k 31.5 16.8 51.9 63.2 77.9 36.5 35.9 43.8 43.8 44.6
8k 30.3 13.8 51.9 63.7 80.6 38.3 36.1 41.3 59.4 46.2
9k 30.6 14.0 52.7 62.6 78.7 37.5 36.1 43.2 43.8 44.4
10k 30.3 16.0 54.2 63.8 79.5 37.3 37.2 44.2 46.9 45.5
Table26: FullevaluationresultsofLLAMA-2continualpre-trainingonOpenWebMathwithPROX-
D+C.Notethatabout1Btokensaretrainedper1000steps.
Train MMLU SAT
GSM8K MATH SVAMP ASDiv MAWPS TAB MQA AVG
Steps STEM MATH
0 14.1 3.8 39.5 51.6 63.6 30.9 12.5 32.9 34.4 31.5
1k 18.8 6.8 40.1 54.4 66.1 29.7 22.9 35.6 53.1 36.4
2k 23.1 8.6 45.7 56.5 72.7 30.7 25.1 35.6 46.9 38.3
3k 23.4 11.8 47.9 59.1 74.6 30.4 28.2 38.3 59.4 41.5
4k 25.2 14.2 49.0 57.8 72.7 32.8 33.1 40.7 40.6 40.7
5k 24.4 13.6 48.0 58.7 72.1 28.9 33.0 40.6 50.0 41.0
6k 29.6 12.8 46.1 63.4 75.6 33.7 31.6 42.8 53.1 43.2
7k 29.9 13.6 50.5 61.5 75.2 36.4 34.5 41.7 53.1 44.0
8k 30.2 15.8 50.8 63.7 77.1 37.7 36.3 43.4 43.8 44.3
9k 34.0 15.4 52.1 62.4 79.3 35.9 40.2 44.0 56.2 46.6
10k 30.6 16.8 50.2 63.7 79.3 37.3 40.1 43.8 53.1 46.1
40Generative AI Research
Table27: FullevaluationresultsofCODELLAMA-7Bcontinualpre-trainingonOpenWebMathwith
rawdata. Notethatabout1Btokensaretrainedper250steps.
Train MMLU SAT
GSM8K MATH SVAMP ASDiv MAWPS TAB MQA AVG
Steps STEM MATH
0 11.8 5.0 44.2 50.7 62.6 30.6 14.3 20.4 21.9 29.1
250 16.7 8.2 45.2 52.2 65.3 33.9 16.0 28.8 43.8 34.5
500 18.3 7.8 43.1 53.9 69.0 29.3 15.3 22.5 37.5 33.0
750 20.2 8.0 45.2 54.2 71.9 29.9 17.1 31.2 37.5 35.0
1000 24.7 9.8 40.6 58.6 72.7 29.3 20.7 31.9 34.4 35.9
1250 24.3 10.4 44.0 57.5 74.8 29.2 21.4 36.1 50.0 38.6
1500 26.2 13.2 48.4 58.8 75.4 29.4 28.1 34.9 50.0 40.5
1750 25.5 11.8 49.1 58.7 76.6 32.4 26.7 37.3 43.8 40.2
2000 28.0 13.6 46.3 61.7 80.0 33.8 29.4 37.2 50.0 42.2
2250 27.7 13.6 48.9 62.2 80.3 32.5 28.9 39.1 59.4 43.6
2500 31.1 14.8 51.4 62.1 81.2 33.6 30.4 40.5 43.8 43.2
Table 28: Full evaluation results of CODELLAMA continual pre-training on OpenWebMath with
PROX-D.Notethatabout1Btokensaretrainedper250steps.
Train MMLU SAT
GSM8K MATH SVAMP ASDiv MAWPS TAB MQA AVG
Steps STEM MATH
0 11.8 5.0 44.2 50.7 62.6 30.6 14.3 20.4 21.9 29.1
250 21.1 9.2 48.7 56.1 71.3 33.4 22.2 34.1 50.0 38.5
500 23.7 11.6 49.8 57.4 74.7 32.9 28.5 35.8 59.4 41.5
750 25.1 15.4 48.1 58.9 78.8 36.8 29.4 37.6 53.1 42.6
1000 28.4 14.2 50.9 61.2 79.8 36.7 27.7 37.6 50.0 42.9
1250 33.0 15.2 49.3 62.9 81.1 33.4 32.8 41.0 46.9 44.0
1500 36.0 15.0 54.2 65.0 81.0 39.3 34.1 42.0 62.5 47.7
1750 34.7 14.6 53.1 63.6 83.3 40.6 35.9 43.4 62.5 48.0
2000 35.7 17.6 53.3 65.4 83.5 42.4 37.1 42.4 56.2 48.2
2250 37.2 18.8 54.5 65.4 83.2 41.9 41.0 44.9 71.9 51.0
2500 38.1 17.0 54.2 67.0 83.1 40.9 39.8 43.7 50.0 48.2
Table 29: Full evaluation results of CODELLAMA continual pre-training on OpenWebMath with
PROX-D+C.Notethatabout1Btokensaretrainedper250steps.
Train MMLU SAT
GSM8K MATH SVAMP ASDiv MAWPS TAB MQA AVG
Steps STEM MATH
0 11.8 5.0 44.2 50.7 62.6 30.6 14.3 20.4 21.9 29.1
250 18.1 10.2 46.0 54.5 71.9 33.0 21.3 34.4 50.0 37.7
500 22.4 10.0 50.3 59.7 76.4 31.3 26.1 36.0 59.4 41.3
750 26.8 11.4 51.2 61.0 78.5 34.9 26.4 38.0 53.1 42.4
1000 29.0 14.4 54.1 62.8 80.1 36.9 34.2 40.4 62.5 46.0
1250 31.4 15.0 51.7 63.8 81.1 37.2 32.5 41.4 75.0 47.7
1500 31.5 17.4 53.4 64.4 80.7 39.6 35.4 41.6 71.9 48.4
1750 33.7 15.2 50.6 64.3 81.5 39.2 36.1 40.5 53.1 46.0
2000 36.2 16.0 54.7 65.1 83.1 39.9 39.1 43.4 71.9 49.9
2250 37.1 16.6 55.3 65.6 82.4 41.3 36.5 42.7 75.0 50.3
2500 35.6 17.6 55.8 67.9 82.7 41.3 38.9 42.6 62.5 49.4
41Generative AI Research
Table30: FullevaluationresultsofMISTRAL-7Bcontinualpre-trainingonOpenWebMathwithraw
data. Notethatabout1Btokensaretrainedper1000steps.
Train MMLU SAT
GSM8K MATH SVAMP ASDiv MAWPS TAB MQA AVG
Steps STEM MATH
0 40.6 11.4 65.4 68.5 87.0 52.9 32.3 50.0 56.2 51.6
1k 31.6 12.0 56.5 66.0 80.1 43.9 27.1 45.1 56.2 46.5
2k 32.4 10.8 54.7 63.5 82.6 40.8 31.6 45.7 59.4 46.8
3k 33.6 14.8 60.4 64.7 84.5 43.5 33.1 47.2 68.8 50.1
4k 35.1 14.8 58.7 65.2 84.4 41.2 38.5 47.3 62.5 49.7
5k 33.4 16.0 59.3 65.0 83.8 46.7 34.6 49.1 62.5 50.0
6k 38.7 16.6 61.5 68.1 86.1 47.4 35.3 48.5 37.5 48.9
7k 39.6 17.2 60.5 68.2 86.2 44.4 38.5 49.3 53.1 50.8
8k 44.0 16.4 64.5 69.8 88.7 45.5 41.3 50.6 59.4 53.4
9k 43.9 19.4 63.7 69.7 87.6 44.9 42.9 51.0 62.5 54.0
10k 44.4 19.2 65.2 69.6 88.4 46.6 43.1 50.8 65.6 54.8
Table 31: Full evaluation results of MISTRAL-7B continual pre-training on OpenWebMath with
PROX-D.Notethatabout1Btokensaretrainedper1000steps.
Train MMLU SAT
GSM8K MATH SVAMP ASDiv MAWPS TAB MQA AVG
Steps STEM MATH
0 40.6 11.4 65.4 68.5 87.0 52.9 32.3 50.0 56.2 51.6
1k 36.8 14.6 57.2 66.1 83.1 45.7 32.6 47.7 59.4 49.2
2k 38.5 17.0 57.9 69.0 86.3 44.7 33.6 49.2 56.2 50.3
3k 40.0 19.0 59.3 68.7 87.0 46.8 41.0 48.0 68.8 53.2
4k 38.5 20.4 59.3 66.2 85.1 42.6 42.8 49.5 68.8 52.6
5k 42.5 20.2 63.0 70.5 86.6 47.2 43.4 49.8 62.5 54.0
6k 46.8 17.8 62.5 72.7 88.2 51.2 47.7 51.3 56.2 54.9
7k 47.5 22.4 64.1 71.8 89.1 51.4 47.9 52.4 65.6 56.9
8k 44.6 23.8 63.2 70.8 87.7 47.6 49.1 54.1 65.6 56.3
9k 46.6 24.6 61.6 72.3 86.4 46.9 49.8 53.2 65.6 56.3
10k 46.7 22.6 63.5 72.4 88.9 48.3 48.2 54.1 62.5 56.4
Table32: FullevaluationresultsofMistral-7Bcontinualpre-trainingonOpenWebMathwithPROX-
D+C.Notethatabout1Btokensaretrainedper1000steps.
Train MMLU SAT
GSM8K MATH SVAMP ASDiv MAWPS TAB MQA AVG
Steps STEM MATH
0 40.6 11.4 65.4 68.5 87.0 52.9 32.3 50.0 56.2 51.6
1k 30.9 16.0 60.1 64.5 85.3 40.8 33.9 48.0 59.4 48.8
2k 40.3 17.6 63.0 66.3 86.2 48.0 33.9 48.7 53.1 50.8
3k 42.4 17.8 59.6 69.1 85.7 50.1 38.5 49.9 59.4 52.5
4k 43.8 20.4 63.7 69.3 88.2 46.2 46.3 50.9 65.6 54.9
5k 42.5 18.4 59.3 69.6 87.9 44.3 46.1 51.9 65.6 54.0
6k 47.7 21.8 62.7 71.7 89.2 47.9 48.4 54.0 68.8 56.9
7k 46.8 21.6 62.9 72.1 88.4 50.1 46.4 52.5 68.8 56.6
8k 48.4 21.6 65.0 72.7 89.2 51.1 49.4 52.9 65.6 57.3
9k 48.5 24.8 64.4 72.6 88.3 50.7 48.1 53.4 62.5 57.0
10k 51.0 22.4 64.9 72.9 89.2 49.8 53.0 54.2 75.0 59.2
42Generative AI Research
E ANALYSIS
E.1 CASESTUDIES
We provide several cases to qualitatively illustrate the refinement effect of PROX, as shown in
Tables33-34. Forthegeneraldomain,usingRedPajama-V2asanexample,weobservethatPROX
can drop low-information documents, remove meaningless content such as navigation bars, and
replaceURLlinks(seeTable33). Inthemathematicsdomain, PROX demonstratestheabilityto
eliminatedocumentswithminimalrelevancetomathematicalreasoningandremovelessimportant
elementslikefunctionalbuttons(seeTable34). Theserefinementsenhancethequalityandrelevance
oftheprocesseddataacrossdifferentdomains.
E.2 COMPUTINGOVERHEADANALYSIS
AccordingtoKaplanetal.(2020),bothtrainingandinferencecomputationalFLOPsforTransformer-
basedLanguageModels(denotedasC andC )canbeapproximatedastheproductofmodel
train inference
parameters(non-embeddingparameter)N andthenumberoftokensD. Thiscanbeexpressedas:
C ≈6·ND , (9)
train train
C ≈2·N(D +D ). (10)
inference prefill decode
In PROX, we go through two data refining stages before final training, which incurs additional
inference-timecomputationalFLOPs. Supposetherefiningmodelparameterforeachstageisdenoted
asN ,andtherawdatasizeintokensisD .
refine raw
Forthefirstdocument-levelstage,thecomputationalcostcanbeapproximatedas:
C ≈2·N (D +D )≈2·N D , (supposeD ≪D ) (11)
doc refine raw output refine raw output raw
resultinginanewpoolofdatasizedD .
doc
Similarly,forthesecondchunk-levelstage,thecomputationalcostis:
C ≈2·N (D +D )≈2·ND , (supposeD ≪D ) (12)
chunk r doc output r doc output doc
whichproducesthefinalrefineddatasizeofD .
ProX
Thus,thetotalcomputationaloverheadforPROXcanbecalculatedasthesumofthetwostages:
C =C +C ≈2·N D +2·N D . (13)
PROX doc chunk doc_refine raw chunk_refine doc
Ingeneral,weuserefiningmodelswithsamesizes,sothefinalinferenceoverheadcanbeestimated
as
C ≈2·N (D +D ). (14)
PROX refine raw doc
Additionally,weomittheFLOPsforfine-tuningsincetheyarenegligiblecomparedtothelarge-scale
pre-trainingandinferenceFLOPs.
43Generative AI Research
Table 33: Cases from RedPajama-V2 after applying PROX. Text in red indicates content to be
removedorreplaced. “...” denotesomittedcontentduetolimitedspace.
Case1
TagCollegeEducationJournalismWar
:MichaelLewis
ContributorMichaelLewis
MichaelLewisispossiblythemostentertainingnonfictionwriteralive. Ifthat’snottrueit’satleastclosetotrue.
Liar’sPoker,Moneyball,TheBlindSide,hisNYTarticleaboutJonathanLebed(Googleit):what’snottolove?
504:HowIGotIntoCollege
ActTwo:MyAmesisTrue
WriterMichaelLewistellsthestoryofamannamedEmirKamenica,whosepathtocollegestartedwithfleeingthe
warinBosniaandbecomingarefugeeintheUnitedStates.Thenhehadastrokeofluck:astudentteacherreadan
essayhe’dplagiarizedfromabookhe’dstolenfromalibrarybackinBosnia,andwassoimpressedthatshegothim
outofabadhighschoolandintoamuchbetterone.
ActThree
MichaelLewis’storycontinues,andhefiguresoutwhyEmirKamenicainsistsonremembering,andtelling,thestory
ofhislifethewayhedoes—evenwhenhefindsoutthatsomeofthefactsmaybewrong.
OutputbyPROX:
drop_doc()
Case2
Home>Staff>Staffsearch>DrTimOverton
DrTimOvertonBScPhD
SchoolofChemicalEngineeringSeniorLecturer
Telephone(+44)(0)1214145306Emailt.w.overton@bham.ac.uk
AddressSchoolofChemicalEngineeringUniversityofBirmingham
B152TT
DrTimOvertonisabiochemistandmolecularmicrobiologistwhoisinterestedinapplyingmolecularbiologyandsingle-
celltechniquestounderstandanddevelopbioprocesses.Heisactiveinmicrobialflowcytometryresearchandcollaborates
widelywithbioprocessengineers,molecularmicrobiologists,cellbiologistsandenvironmentalmicrobiologiststodevelop
newmethodsofansweringfundamentalquestionsonasingle-celllevel.
Hisresearchalsofocusesonusingbacteriatomakeusefulproductssuchasproteindrugsandsmallmolecules,andthe
bacterialresponsestostressencounteredinsuchprocesses. Currentandrecentresearchfundinghascomefromthe
BBSRC,TSBandEUFP7.HeisthedirectoroftheMScinBiochemicalEngineering.Pages:134
...
Googlescholar:http://scholar.google.co.uk/citations?user=tF_eBKEAAAAJ
...
OutputbyPROX:
keep_doc()
remove_lines(line_start=0, line_end=5)
normalize(source_str="http://scholar.google.co.uk/citations?user",
target_str="")
normalize(source_str="Pages: 1 3 4", target_str="")
...
44Generative AI Research
Table 34: Cases from OpenWebMath after applying PROX. Text in red indicates content to be
removedorreplaced. “...” denotesomittedcontentduetolimitedspace.
Case1
##unhybridizedpibonds
sp,sp2,sp3,dsp3,d2sp3
Tatiana4B
Posts:30
Joined:FriSep28,201812:28am
###unhybridizedpibonds
...
###Re:unhybridizedpibonds
Iamnottoosureinmyknowledgeaboutthis,butIthinkthatbothhavehybridizedorbitals.Sincehybridizationis
definedasthephenomenonofintermixingoftheorbitalssuchassp,sigmaandpibondsarejustdifferenttypesof
covalentbondsformeddependingonthewaytheatomicorbitalshybridizewitheachother.Sigmabondsarearesult
ofwhentheoverlapoforbitalsoftwoatomstakesplacealongthelinejoiningthetwoorbitals,whilepibondsare
whentwoatomsoverlapduetothesidewaysoverlapoftheir’p’orbitals.
HannahYates1K
Posts:59
Joined:FriSep28,201812:27am
###Re:unhybridizedpibonds
Iamalsonottoosureonmyanswer,butIamprettysurethatasigmabondhasjusthybridizedorbitals,butthereason
apibondcanformisbecauseofanextra(nothybridized)porbital.Thisallowsforadoubleandtriplebondtoform.
OutputbyPROX:
drop_doc()
Case2
Solution-TrigonometricIdentities
Account
Register
Share
BooksShortlist
ConceptTrigonometricIdentities
Question
Provethefollowingtrigonometricidentities:
(i) sinθ =cosecθ+cotθ
1−cosθ
Solution
Youneedtotoviewthesolution
Isthereanerrorinthisquestionorsolution?
ReferenceMaterial
Solutionforconcept:TrigonometricIdentities.ForthecourseCBSE
S
OutputbyPROX:
keep_doc()
remove_lines(line_start=0, line_end=7)
remove_lines(line_start=18, line_end=24)
45