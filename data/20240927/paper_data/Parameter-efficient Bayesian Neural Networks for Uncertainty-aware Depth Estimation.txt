UNCV-W2024ExtendedAbstract
Parameter-efficient Bayesian Neural Networks
for Uncertainty-aware Depth Estimation
RichardD.Paul1,2,∗ AlessioQuercia1,3,∗ VincentFortuin2,4 KatharinaNo¨h1 HannoScharr1
1 ForschungszentrumJu¨lich,Ju¨lich,Germany
2 HelmholtzAI,Munich,Germany
3 RWTHAachenUniversity,Aachen,Germany
4 TechnicalUniversityofMunich,Munich,Germany
{r.paul,a.quercia,k.noeh,h.scharr}@fz-juelich.de
vincent.fortuin@tum.de
Abstract quantify epistemic uncertainties, however, due to the large
numberofparameters,manyexistingmethodsbecomepro-
State-of-the-art computer vision tasks, like monocular hibitivelyexpensive[18].
depth estimation (MDE), rely heavily on large, modern Althoughthelargegrowthinmodelsizeovertherecent
Transformer-based architectures. However, their applica- years has boosted predictive accuracy in many domains, it
tion in safety-critical domains demands reliable predic- also introduced serious issues regarding the accessibility
tive performance and uncertainty quantification. While of such methods, as their training typically requires large
Bayesian neural networks provide a conceptually simple computing infrastructure. To this end, PEFT methods like
approach to serve those requirements, they suffer from the LoRA[9],BitFit[29],andDiffFit[26]havebeenproposed,
high dimensionality of the parameter space. Parameter- whichconstructparametersubspacesmuchsmallerthanthe
efficient fine-tuning (PEFT) methods, in particular low- originalparameterspaces,yetallowingforcompetitiveper-
rankadaptations(LoRA),haveemergedasapopularstrat- formance on downstream tasks while requiring much less
egy for adapting large-scale models to down-stream tasks computationalpower. Thus,thequestionariseswhetherthe
by performing parameter inference on lower-dimensional PEFT subspaces are also suitable for performing less ex-
subspaces. In this work, we investigate the suitability of pensive,yeteffective,uncertaintyestimation.
PEFT methods for subspace Bayesian inference in large- The applicability of PEFT subspaces for performing
scale Transformer-based vision models. We show that, Bayesian inference has so far only started to be investi-
indeed, combining BitFit, DiffFit, LoRA, and CoLoRA, a gated[16,27].Inparticular,Yangetal.[27]appliedLaplace
novel LoRA-inspired PEFT method, with Bayesian infer- approximationstothefine-tunedLoRAadapterstoachieve
ence enables more robust and reliable predictive perfor- improvedcalibrationinlarge-languagemodels. Onaletal.
manceinMDE. [16] investigate using Stochastic Weight Averaging Gaus-
sians(SWAG)[14]withLoRAandfinditbeingcompetitive
toLaplaceapproximations.
1.Introduction
1.1.Contribution
Recent years have seen the emergence of large-scale
self-supervised foundation models in various domains, es- In this work, we investigate the suitability of differ-
pecially in computer vision [13,17,28] and natural lan- ent PEFT subspaces for post-hoc Bayesian inference on
guage processing [2,19,24]. By leveraging large amounts thestate-of-the-artvisionfoundationmodelDepthAnything
ofunlabeleddata, thesemodelsexhibitremarkableperfor- [28]forMDEusingSWAGandcheckpointensembles[3].
mance,evenunderdistributionshifts[15,19].Nevertheless, Inparticular,weincorporatetheBitFit[29]andDiffFit[26]
theirapplicationinsafety-criticalenvironments,suchasau- PEFT methods into our analysis, which have not yet been
tonomous driving or healthcare, demands for uncertainty investigatedforsubspaceBayesianinference.
estimation in order to detect distribution shifts and, thus, Moreover, since the architecture at hand uses a
enhancethereliabilityofthemodel’spredictions. Bayesian convolution-basedpredictionheadontopofavisiontrans-
deeplearningprovidesaconceptuallyattractiveapproachto former backbone [28], we propose the construction of
1
4202
peS
52
]VC.sc[
1v58071.9042:viXraUNCV-W2024ExtendedAbstract
a parameter-efficient subspace for convolutional kernels, vectors θ ,...,θ with θ ∈ Rd, one may choose be-
1 n i
which we call CoLoRA (Section 3.1). CoLoRA mimics tweenadiagonalapproximation,inwhichcasetheapprox-
LoRA by applying a low-rank decomposed perturbation imate posterior is N(µ ,σ2I), where µ = 1 (cid:80)n θ
basedontheTuckerdecomposition[8,25]. and σ2 = (1 (cid:80)n θ2)θ −θ µ2, or a low-θ rank pn lus di i= a1 go-i
θ n i=1 i θ
We find that the PEFT methods under consideration nal approximation, in which case the approximate poste-
allow for parameter-efficient Bayesian inference in large- riorbecomesN(µ ,Σ ),whereΣ = 1 ·(σ2I +Σ )and
scalevisionmodelsforMDE. Σ = 1 (cid:80)n (θθ −θ µ )(θ −µθ )⊤,w2 hichθ islow-l rr ankif
lr n−1 i=1 i θ i θ
n<d.
2.Background Alternatively, instead of computing the moments of the
empirical distribution of parameter values and then sam-
Infine-tuning,weconsideratypicalsupervisedlearning pling from the corresponding normal distribution to com-
settingwithdataD ={(x i,y i)}. ForMDE,theinputsx i ∈ putetheMonte-CarloestimatefromEquation(4),onemay
R3×h×w are RBG images and the outputs y i ∈ R +h×w are directlytreatthecheckpointsassamplesfromtheposterior
the depth maps. Within this work, we consider the depth distribution. This method is known as checkpoint ensem-
mapstobeindisparityspace,insteadofmetricspace. The ble[3].
disparity of a pixel is obtained as the inverse metric depth
2.3.LoRA
ofsaidpixel. Yangetal.[28]performfine-tuningofapre-
trainedneuralnetworkf :(x,θ)(cid:55)→ywithparametersθby
LoRA[9]isaPEFTstrategy,whichaddslow-rankper-
minimizingtheaffine-invariantmeanabsoluteerror[21]
turbations∆W tolargeweightmatricesW oflinearlayers
inmodernneuralnetworkarchitectures. Thatis,foralinear
N (cid:12) (cid:12)
ℓ D(θ)= N1 (cid:88)(cid:12) (cid:12) (cid:12)yˆ i s− (yˆt( )yˆ i) − y i s− (yt( )y i)(cid:12) (cid:12) (cid:12), (1) w we ei cg oh mt m pua tt erix W ∈ Rdin×dout and bias vector b ∈ Rdout,
i=1 i i
y =xW +x∆W +b=xW +xAB+b, (5)
whereyˆ = f(x ,θ)isthenetworkprediction, t(y )isthe
i i i
spatialmedian,ands(y i)= h1 w (cid:80)h j=× 1w|y j −t(y)|. whereA ∈ Rdin×r,B ∈ Rr×dout arefactorsdecomposing
∆W. By choosing sufficiently small rank r for A and B,
2.1.BayesianDeepLearning
wethenobtainalow-rankapproximation∆W.Fine-tuning
InBayesiandeeplearning,predictionisperformedwith isthenperformedbyonlyoptimizingthefactorsAandB.
respecttotheparameterposteriordistribution
2.4.BitFit&DiffFit
π(θ|D)∝L(D|θ)p(θ), (2) BitFit [29] is an alternative PEFT strategy, which only
unfreezesthebiasesofapre-trainedmodelforfine-tuning.
whereL(D|θ)∝exp−ℓ D(θ)isthelikelihoodandp(θ)the It was shown to be competitive or sometimes even better
prior. Forpredictiononanewdatapointx∗ withtruelabel thanperformingfullfine-tuningonlanguagemodels[29].
y∗,weconsidertheposteriorpredictivedistribution
DiffFit [26] extends BitFit by adding additional scalar
factorstotheattentionandfeed-forwardblocksofatrans-
π∗(y∗|x∗,D)=E [L(y∗|θ,x∗)]. (3)
θ∼π former,aswellasunfreezingthelayernorms.Intheirpaper,
theauthorsdemonstrateimprovedperformanceoverBitFit,
As the posterior is usually intractable, samples from it
LoRA,andfullfine-tuningfordiffusiontransformers.
need to be approximated to perform Monte-Carlo estima-
tion 3.Method
n
1 (cid:88) 3.1.CoLoRA
E [h(θ)]≈ h(θ ), θ ∼q(θ)≈π(θ|D) (4)
θ∼π n i i
i=1 The convolution operation in CNNs consists of apply-
ing a convolutional kernel W, which is a tensor of size
forh(·)being,e.g.,thepredictivemeanorvariance.
c ×c ×k × ··· × k , to an input tensor x of size
out in 1 d
c ×h ×···×h ,beforeapplyingafurtheradditivebias
2.2. Stochastic Weight Averaging Gaussians & in 1 d
b ∈ Rcout. That is, for every output channel i, we obtain
CheckpointEnsembles
y = b
+(cid:80)cin
h(x,ρ) ∗ W , where ∗ is the cross-
i i j=1 j δ ij δ
SWAG [14] was introduced as a simple baseline for correlation operation with stride δ and h(x,ρ) is the input
Bayesian inference by computing Gaussian approximate signal after applying padding ρ to it. Leveraging the dis-
posteriors from the checkpoints of a standard SGD train- tributivity of cross-correlations, we mimic LoRA for con-
ing run. Given a set of checkpoints reshaped as flattened volutionsbyconsideringanadditiveperturbation∆W ona
2UNCV-W2024ExtendedAbstract
givenweightmatrixW as DeepEns CkptEns SWAG-D SWAG-LR
y i =b i+(cid:88) jc =in 1h(x,ρ) j ∗ δ(W ij+∆W ij) 00 .. 99 46 1 21 42 84 168 31 263264 121 42 84 168 31 263 62 464 121 42 84 168 31 263 62 464 1 2 4 1631 26 64 B F Lu oa l Rs le Aline
0.92 64 CoLoRA
=b
i+(cid:88)cin
h(x,ρ) j ∗ δW ij (6) 0.90
B Di it ffF Fi it
t
j=1
10−3 100 10−3 100 10−3 100 10−3 100
Fractionofactiveparameters
(cid:88)cin
+ h(x,ρ) j ∗ δ∆W ij. Figure1.Negativelog-likelihoodforallcombinationsofinference
j=1 and PEFT methods under consideration, evaluated on the NYU
dataset. ExceptSWAG-LR,allmethodsachieveimprovedNLL
Wethenobtainalow-rankdecompositionwithcoreC and
overthedeterministicbaseline. Errorbarsindicate95%intervals
factorsU(1),U(2) byapplyingtheTucker-2decomposition
across5replicateruns. Numbersinthedotsindicatetherankpa-
(crefsec:tucker) [8,25] on the channel dimensions of size
rameterused.
c andc ,asinpracticethekerneldimensionsk ,··· ,k
in out 1 d
aretypicallymuchsmaller.AsinLoRA,forfine-tuning,we
only consider the decomposition C,U(1),U(2). Moreover, the previous Section 3.2, instead of performing real fine-
weinitializethelow-rankfactorssuchthat∆W iszeroini- tuning, we rather act as if continuing fine-tuning, while
tially by setting U(1) to be all zeros and initialize C and recordingcheckpoints,inordertoconstructcheckpointen-
U(2)randomlyfromaGaussiandistributionwithzeromean
semblesandestimatethefirstandsecondmomentsrequired
andthevariancecomputedasintheGlorotinitialization[7]. for SWAG. Following Yang et al. [28], we fine-tune the
As presented by Kim et al. [11] and Astrid et al. [1], model on the popular NYU [22] and KITTI [5] data sets
convolutionswithTucker-2-decomposedkernelscanthem- ontheverysamedatasplitsandusingthelossfromEqua-
selves be decomposed into a sequence of convolutions, tion(1).
wheretheconvolutionalkernelsaregivenfromthedecom- We test the four PEFT methods LoRA, CoLoRA, Bit-
position. For∆W =C× 1U(1)× 2U(2),wedecompose Fit,andDiffFitusingfourdifferentposteriorapproximation
h(x,ρ)∗ ∆W =(h(x∗ U˜(2),ρ)∗ C)∗ U˜(1), (7) methods: DeepEnsembles(DeepEns),CheckpointEnsem-
δ 1 δ 1
bles(CkptEns),diagonalSWAG(SWAG-D),andlow-rank
where U˜(1),U˜(2) are the unsqueezed factors of U(1),U(2) plusdiagonalSWAG(SWAG-LR).Detailsonsamplingand
withsizec out×r×1···×1andc in×r×1···×1,respec- ranksunderconsiderationaregiveninAppendixA.Wefur-
tively. This way, the computation of h(x,ρ)∗ δ ∆W does therconsiderfullfine-tuningonallparameters. Asasbase-
notrequiretheallocationof∆W duringtraining. line, we consider the performance obtained from the pro-
vided,fine-tunedcheckpointsfromYangetal.[28]. Allof
3.2.Inference
thefollowingevaluationswereperformedonthesametest
WeperformBayesianinferenceusingSWAGandcheck- splits of the NYU and KITTI data sets consisting of 130
point ensembles on the parameter-efficient subspaces con- randomlydrawnimagesfromeachdataset.
structed by either BitFit, DiffFit, LoRA, or CoLoRA. The
lattertwomethodsprovideanadditionalrankparameter,for 4.1.PredictivePerformance
whichweconsiderdifferentrankparametersbetween1and
We measure predictive performance by evaluating the
64.Westartfromfine-tunedcheckpointsinordertodemon-
negative log-likelihood (NLL) on an unseen test split for
strate the applicability of SWAG and checkpoint ensem-
NYUandKITTI.WereporttheresultsinFigure1andFig-
blesforpost-hocBayesificationofanexistingpipelinewith-
ure 4a. We observe improvements when using Bayesian
out any needs for sacrifices in accuracy. From a Bayesian
inferenceoneitherthefullparameterspaceorjustaPEFT
perspective, such an already fine-tuned checkpoint can be
subspace. Performinginferenceonthefullparameterspace
considered as the MAP or MLE estimate, depending on
achievesthebestperformanceacrossallinferencemethods,
whether regularization was used during training. Besides,
except low-rank plus diagonal SWAG (SWAG-LR), where
starting the inference from such a high-density region is
theevaluationfailedduetonumericalissues. Forinference
recommended for many sampling algorithms, often called
onthePEFTsubspaces,weobservethemostimprovements
awarmstart[6].
usingCoLoRAwitharankofatleast16,afterwhichitbe-
ginstooutperformBitFitandDiffFit,whenusingDeepEns,
4.Experiments
CkptEns,orSWAG-D.Mostremarkably,CoLoRAseemsto
Weperformexperimentsbasedonthepipelineprovided interpolate between the deterministic baseline and the full
byYangetal.[28],fromwhichweextracttheDINOv2fea- parameter space, if the rank parameter is increased. For
tureencoder[17]andDPTdecoder[20]. Asmentionedin LoRA, we observe improvement over the baseline already
3
LLN
←UNCV-W2024ExtendedAbstract
DeepEns CkptEns SWAG-D SWAG-LR DeepEns CkptEns SWAG-D SWAG-LR
0.3
Baseline
Baseline
Q1LoRA
0.3 Full Q2LoRA
LoRA
Q3LoRA
CoLoRA
0.2 BitFit Q1CoLoRA
Q2CoLoRA
DiffFit 0.2 Q3CoLoRA
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 1 4 16 641 4 16 641 4 16 641 4 16 64
Quantile Rank
Figure2. Testlossperquantileofmostcertainpredictionsevalu- Figure3.Testlosson25%,50%,and75%quantilesforLoRAand
atedontheNYUdataset. ExceptDeepEns,allmethodsachieve CoLoRAagainsttherankparameter,evaluatedontheNYUdata
improved test loss on more certain pixels, suggesting good cali- set. No clear trend suggesting the usage of higher ranks can be
bration.Uncertaintywasestimatedusingpixelwisestandarddevi- identified. Shadedareasindicate95%intervalsacross5replicate
ation. ForLoRAandCoLoRA,onlytheresultsfortherankwith runs.
lowesttest lossonthe5% quantilearedepicted. Theprediction
using the publicly available checkpoint was used as a baseline.
Shadedareasindicate95%intervalsacross5replicateruns. ingSWAG,howeveronKITTI(c.f.,Figure4b),CoLoRAis
often the only method achieving test loss smaller than the
baseline.
forrank1. However,quitesurprisingly,increasingtherank
Furthermore, we analyze the influence of the rank pa-
seemstoyieldonlyasmallfurtherimprovement.
rameter on calibration by considering the test loss at the
In terms of inference methods, we observe that Deep-
25%,50%,and75%quantilesagainsttherankparameters.
Ens, although consisting only of 5 different parameter
Results are depicted in Figure 3 and Figure 4c. For both
samples, yields better performance than all other meth-
data sets, the results are fairly noisy and do not suggest a
ods, where prediction is performed using 100 parameter
clear trend favoring higher ranks and thus, more parame-
samples. Moreover, we observe little difference between
ters. Interestingly, rank r = 4 seems to work particularly
SWAG-DandCkptEns. ForSWAG-LR,wehadnumerical
wellforbothdatasetswhenusingCkptEns.
issues on the NYU data set, resulting in degraded perfor-
mance for CoLoRA at ranks above 16, as well as missing
5.Conclusion
evaluation for LoRA (except rank 16). For KITTI, we ob-
serve similar results for CkptEns and SWAG-D (c.f., Fig-
We demonstrated the applicability of PEFT subspaces
ure4a).
forBayesiandeeplearningonamodernTransformer-based
computer vision architecture for monocular depth estima-
4.2.Calibration
tion. Weshowthatsimplemethodslikecheckpointensem-
Wefurtherevaluatedthecalibrationoftheuncertainties blesandSWAGarecapableofimprovingpredictiveperfor-
obtained from Bayesian inference on PEFT subspaces by manceandprovidingwell-calibrateduncertaintyestimates.
evaluatingthetestlossonquantilesofmostcertainpixels, Moreover, we propose a novel approach for constructing
assuggestedin[4]. Asanuncertaintymetric, weconsider LoRA-likesubspacesinconvolutionallayers,termedCoL-
the pixel-wise standard deviation. Results are depicted in oRA, and demonstrate that it performs competitively with
Figure2andFigure4b. NotethatforLoRAandCoLoRA, the other PEFT methods. We hope that CoLoRA can
weonlyincludedthemethodswithsmallesttestlossonthe alsoservetomakeexisting,convolution-basedarchitectures
5% quantile of most certain pixels. Contrary to the previ- uncertainty-awareinaparameter-efficientmanner.
ous section, we observe the worst performance for Deep-
Ens, where the test loss decreases instead of increasing on Acknowledgements
themostcertainpixels.However,similartotheresultsfrom
the previous section, we observe the best performance us- RDP and AQ performed this work as part of the
ingDeepEnsonthefullparameterspace. ForCkptEnsand Helmholtz School for Data Science in Life, Earth and En-
SWAG-D, inference on the full parameter space performs ergy(HDS-LEE)andreceivedfundingfromtheHelmholtz
a bit worse than BitFit and DiffFit, especially on the most Association. RDP performed parts of this work as part of
certainpixels. SWAG-LRoverallachievesthesmallesttest the HIDA Trainee Network program and received funding
lossonapproximately50%ofthemostcertainpixelswhen fromtheHelmholtzInformation&DataScienceAcademy
using BitFit and DiffFit. Quite interestingly, we observe (HIDA). VF was supported by a Branco Weiss Fellow-
BitFittobeslightlybettercalibratedthanDiffFit,although ship. The authors gratefully acknowledge computing time
the latter uses more parameters. On NYU (c.f., Figure 2), onthesupercomputersJURECA[23]andJUWELS[10]at
CoLoRA is outperformed by the other methods when us- ForschungszentrumJu¨lich.
4
ssoLtseT
←
ssoLtseT
←UNCV-W2024ExtendedAbstract
References [10] StefanKesselheim,AndreasHerten,KaiKrajsek,Jan
Ebert, Jenia Jitsev, Mehdi Cherti, Michael Langguth,
[1] MarcellaAstridandSeung-IkLee.CP-decomposition
Bing Gong, Scarlet Stadtler, Amirpasha Mozaffari,
with Tensor Power Method for Convolutional Neural
etal. Juwelsbooster–asupercomputerforlarge-scale
NetworksCompression,Jan.2017. arXiv:1701.07148
airesearch. InInternationalConferenceonHighPer-
[cs]. 3
formanceComputing,pages453–468.Springer,2021.
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie 4
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
[11] Yong-DeokKim, EunhyeokPark, SungjooYoo, Tae-
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
lim Choi, Lu Yang, and Dongjun Shin. Compres-
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
sionofDeepConvolutionalNeuralNetworksforFast
Gretchen Krueger, Tom Henighan, Rewon Child,
and Low Power Mobile Applications, Feb. 2016.
AdityaRamesh,DanielZiegler,JeffreyWu,Clemens
arXiv:1511.06530[cs]. 3
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teuszLitwin,ScottGray,BenjaminChess,JackClark, [12] DiederikP.KingmaandJimmyBa. Adam: Amethod
Christopher Berner, Sam McCandlish, Alec Radford, for stochastic optimization. In Yoshua Bengio and
IlyaSutskever,andDarioAmodei. LanguageModels Yann LeCun, editors, 3rd International Conference
areFew-ShotLearners. InAdvancesinNeuralInfor- onLearningRepresentations,ICLR2015,SanDiego,
mation Processing Systems, volume 33, pages 1877– CA,USA,May7-9,2015,ConferenceTrackProceed-
1901.CurranAssociates,Inc.,2020. 1 ings,2015. 7
[3] Hugh Chen, Scott Lundberg, and Su-In Lee. Check- [13] AlexanderKirillov,EricMintun,NikhilaRavi,Hanzi
point Ensembles: Ensemble Methods from a Single Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,
Training Process, Oct. 2017. arXiv:1710.03282 [cs]. SpencerWhitehead,AlexanderC.Berg,Wan-YenLo,
1,2 Piotr Dollar, and Ross Girshick. Segment Anything.
[4] KamilCiosek,VincentFortuin,RyotaTomioka,Katja In Proceedings of the IEEE/CVF International Con-
Hofmann, and Richard Turner. Conservative Uncer- ferenceonComputerVision,pages4015–4026,2023.
tainty Estimation By Fitting Prior Networks. In In- 1
ternationalConference onLearning Representations, [14] Wesley J Maddox, Pavel Izmailov, Timur Garipov,
Apr.2020. 4 DmitryPVetrov,andAndrewGordonWilson.ASim-
[5] A Geiger, P Lenz, C Stiller, and R Urtasun. Vi- pleBaselineforBayesianUncertaintyinDeepLearn-
sion meets robotics: The KITTI dataset. The Inter- ing. In Advances in Neural Information Processing
nationalJournalofRoboticsResearch, 32(11):1231– Systems,volume32.CurranAssociates,Inc.,2019. 1,
1237,Sept.2013. Publisher: SAGEPublicationsLtd 2
STM. 3
[15] Prasanna Mayilvahanan, Thadda¨us Wiedemer, Evge-
[6] Andrew Gelman, Aki Vehtari, Daniel Simpson, nia Rusak, Matthias Bethge, and Wieland Brendel.
Charles C. Margossian, Bob Carpenter, Yuling DoesCLIP’sgeneralizationperformancemainlystem
Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian from high train-test similarity? In NeurIPS 2023
Bu¨rkner, and Martin Modra´k. Bayesian Workflow, Workshop on Distribution Shifts: New Frontiers with
Nov.2020. arXiv:2011.01808[stat]. 3 FoundationModels,2024. 1
[7] Xavier Glorot and Yoshua Bengio. Understanding [16] Emre Onal, Klemens Flo¨ge, Emma Caldwell, Ar-
thedifficultyoftrainingdeepfeedforwardneuralnet- sen Sheverdin, and Vincent Fortuin. Gaussian
works. In Proceedings of the Thirteenth Interna- StochasticWeightAveragingforBayesianLow-Rank
tionalConferenceonArtificialIntelligenceandStatis- Adaptation of Large Language Models, May 2024.
tics,pages249–256.JMLRWorkshopandConference arXiv:2405.03425[cs]. 1
Proceedings,Mar.2010. ISSN:1938-7228. 3
[17] MaximeOquab,Timothe´eDarcet,The´oMoutakanni,
[8] FrankL.Hitchcock. TheExpressionofaTensorora
Huy V. Vo, Marc Szafraniec, Vasil Khalidov,
PolyadicasaSumofProducts. JournalofMathemat-
Pierre Fernandez, Daniel Haziza, Francisco Massa,
icsandPhysics,6(1-4):164–189,1927. 2,3
Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas,
[9] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Wojciech Galuba, Russell Howes, Po-Yao Huang,
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu
WeizhuChen. LoRA:Low-RankAdaptationofLarge Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou,
LanguageModels,Oct.2021. arXiv:2106.09685[cs]. JulienMairal,PatrickLabatut,ArmandJoulin,andPi-
1,2 otr Bojanowski. DINOv2: Learning Robust Visual
5UNCV-W2024ExtendedAbstract
Features without Supervision. Transactions on Ma- LLaMA: Open and Efficient Foundation Language
chineLearningResearch,July2023. 1,3 Models,Feb.2023. arXiv:2302.13971[cs]. 1
[18] Theodore Papamarkou, Maria Skoularidou, Kon- [25] Ledyard R. Tucker. Some mathematical notes
stantina Palla, Laurence Aitchison, Julyan Arbel, on three-mode factor analysis. Psychometrika,
David Dunson, Maurizio Filippone, Vincent For- 31(3):279–311,Sept.1966. 2,3
tuin, Philipp Hennig, Jose Miguel Hernandez Lo-
[26] Enze Xie, Lewei Yao, Han Shi, Zhili Liu, Daquan
bato, Aliaksandr Hubin, Alexander Immer, Theofa-
Zhou, Zhaoqiang Liu, Jiawei Li, and Zhenguo Li.
nis Karaletsos, Mohammad Emtiyaz Khan, Agusti-
DiffFit: UnlockingTransferabilityofLargeDiffusion
nus Kristiadi, Yingzhen Li, Stephan Mandt, Christo-
Models via Simple Parameter-Efficient Fine-Tuning.
pherNemeth,MichaelA.Osborne,TimG.J.Rudner, In2023IEEE/CVFInternationalConferenceonCom-
David Ru¨gamer, Yee Whye Teh, Max Welling, An- puterVision(ICCV),pages4207–4216,Paris,France,
drewGordonWilson,andRuqiZhang.PositionPaper:
Oct.2023.IEEE. 1,2
BayesianDeepLearningintheAgeofLarge-ScaleAI,
[27] Adam X. Yang, Maxime Robeyns, Xi Wang, and
Feb.2024. arXiv:2402.00809[cs,stat]. 1
Laurence Aitchison. Bayesian Low-rank Adap-
[19] AlecRadford,JongWookKim,ChrisHallacy,Aditya
tation for Large Language Models, Feb. 2024.
Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas-
arXiv:2308.13111[cs]. 1
try, Amanda Askell, Pamela Mishkin, Jack Clark,
[28] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang
Gretchen Krueger, and Ilya Sutskever. Learning
Xu, JiashiFeng, andHengshuangZhao. DepthAny-
Transferable Visual Models From Natural Language
thing: Unleashing the Power of Large-Scale Unla-
Supervision. InProceedingsofthe38thInternational
beled Data, Apr. 2024. arXiv:2401.10891 [cs]. 1, 2,
Conference on Machine Learning, pages 8748–8763.
3,7
PMLR,July2021. ISSN:2640-3498. 1
[29] EladBenZaken,ShauliRavfogel,andYoavGoldberg.
[20] Rene Ranftl, Alexey Bochkovskiy, and Vladlen
BitFit: Simple Parameter-efficient Fine-tuning for
Koltun. Vision Transformers for Dense Prediction.
Transformer-based Masked Language-models, Sept.
In2021IEEE/CVFInternationalConferenceonCom-
2022. arXiv:2106.10199[cs]. 1,2
puter Vision (ICCV), pages 12159–12168, Montreal,
QC,Canada,Oct.2021.IEEE. 3
[21] Rene´ Ranftl, Katrin Lasinger, David Hafner, Kon-
rad Schindler, and Vladlen Koltun. Towards Ro-
bust Monocular Depth Estimation: Mixing Datasets
forZero-ShotCross-DatasetTransfer. IEEETransac-
tions on Pattern Analysis and Machine Intelligence,
44(3):1623–1637, Mar. 2022. Conference Name:
IEEE Transactions on Pattern Analysis and Machine
Intelligence. 2
[22] Nathan Silberman, Derek Hoiem, Pushmeet Kohli,
and Rob Fergus. Indoor Segmentation and Sup-
port Inference from RGBD Images. In Andrew
Fitzgibbon, SvetlanaLazebnik, PietroPerona, Yoichi
Sato, and Cordelia Schmid, editors, Computer Vision
– ECCV 2012, pages 746–760, Berlin, Heidelberg,
2012.Springer. 3
[23] Philipp Tho¨rnig. Jureca: Data centric and booster
modules implementing the modular supercomputing
architecture at ju¨lich supercomputing centre. Jour-
naloflarge-scaleresearchfacilitiesJLSRF,7:A182–
A182,2021. 4
[24] Hugo Touvron, Thibaut Lavril, Gautier Izacard,
Xavier Martinet, Marie-Anne Lachaux, Timothe´e
Lacroix, Baptiste Rozie`re, Naman Goyal, Eric Ham-
bro, Faisal Azhar, Aurelien Rodriguez, Armand
Joulin, Edouard Grave, and Guillaume Lample.
6UNCV-W2024ExtendedAbstract
DeepEns CkptEns SWAG-D SWAG-LR
DeepEns CkptEns SWAG-D SWAG-LR Baseline
1.0
Full
111 ... 445 050 1 2
1 42 84 168 31 263 62 464
12142 84 168 31 263 62 464 12142 84 168 31 263 62 464 12142 84 168 31 263 62 464 B
F
L
Cu
oa
ol
Rs Lle oAli Rn Ae 00 .. 68 L
C
B
Do
io
it
ffR
L
F
FoA
i
itR tA
BitFit 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
1.35 DiffFit Quantile
10−3 100 10−3 100 10−3 100 10−3 100 (b) Test loss per quantile of most certain predictions evaluated on the
Fractionofactiveparameters KITTIdataset. ExceptDeepEns,allmethodsachieveimprovedtestloss
(a)Negativelog-likelihoodandmeantestlossforallcombinationsofin- onmorecertainpixels,suggestinggoodcalibration.Uncertaintywasesti-
ferenceandPEFTmethodsunderconsideration,evaluatedontheKITTI matedeitherusingpixelwisestandarddeviation.ForLoRAandCoLoRA,
dataset.ExceptSWAG-LR,allmethodsachieveimprovedNLLoverthe onlythemethodswithlowesttestlossonthe5%quantilearedepicted.
deterministicbaseline.Errorbarsindicate95%intervalsacross5replicate Thepredictionusingthepubliclyavailablecheckpointwasusedasabase-
runs. line.Shadedareasindicate95%intervalsacross5replicateruns.
DeepEns CkptEns SWAG-D SWAG-LR
1.0 Baseline
Q1LoRA
Q2LoRA
0.8 Q3LoRA
Q1CoLoRA
Q2CoLoRA
0.6 Q3CoLoRA
1 4 16 641 4 16 641 4 16 641 4 16 64
Rank
(c)Testlosson25%,50%,and75%quantilesforLoRAandCoLoRA
against the rank parameter, evaluated on the KITTI data set. No clear
trendsuggestingtheusageofhigherrankscanbeidentified.Shadedareas
indicate95%intervalsacross5replicateruns.
Figure4.EvaluationsonKITTIdataset.
A.ExperimentDetails
Fine-tuning is performed for 20 more epochs, starting from the checkpoints provided by Yang et al. [28]. During fine-
tuningtake100equidistantcheckpoints.ForbothSWAGvariants,wealsodraw100samplesfromtheapproximateposterior.
For LoRA and CoLoRA, which admit a rank parameter, we test ranks 1, 2, 4, 8, 16, 32 and 64. For every combination of
posteriorapproximationandPEFTmethod,weperform5replicateexperimentsusingdifferentseeds. ForDeepEns,weuse
thelastcheckpointfromthefivereplicatestocompileanensemble. Forallexperiments,weuseAdam[12]withaconstant
learningrateof1e-7. Thebatchsizeforallmethodsissetto4.
B.ResultsonKITTI.
WeprovidefiguresfortheanalysesfromSection4ontheKITTIdatasetinFigure4.
C.TuckerDecomposition
For an n-tensor A of size h ×···×h , the Tucker decomposition returns a core tensor C ∈ Rr1×···×rn and n factor
1 n
matricesU(1),...,U(n) ∈ Rhi×ri,wherer ,...,r aretheranksalongeachofthentensordimensions. FromtheTucker
1 n
decomposition,Aisrecoveredas
a i1,...,in =
(cid:88)r1
···
(cid:88)rn
c j1,...,jn ·u( i11 ,) j1···u( inn ,) jn. (8)
j1=1 jn=1
Alow-rankapproximationofAcanbecomputedbychoosingtheranksr ,...,r ofthedecompositiontobelessthanthe
1 n
fullranks.Moreover,inthepartialTuckerdecomposition,wemaychoosetoomitdecompositionofcertaintensordimensions
oftheincomingAtensor,inwhichcasethecorematrixCsimplytakesfullsizer =h alongtherespectivetensordimension
i i
7
LLN
←
ssoLtseT
←
ssoLtseT
←UNCV-W2024ExtendedAbstract
iandthecorrespondingfactormatrixU(i) = I ∈ Rhi×hi becomesanidentitymatrix. Ifthefirstntensordimensionsare
chosenaslow-rank,thecorrespondingdecompositionisalsocommonlycalledTucker-ndecomposition.
8