FineZip : Pushing the Limits of Large Language Models for
Practical Lossless Text Compression
FazalMittu1,YihuanBu1,AkshatGupta1,AshokDevireddy1,AlpErenOzdarendeli1,
AnantSingh2,GopalaAnumanchipalli1
1UCBerkeley,2NYU
akshat.gupta@berkeley.edu
Abstract largelanguagemodels(LLMs)canbeusedtocom-
press data from various modalities. Huang et al.
While the language modeling objective has
(2024)followedupthisworkbyshowingthatin-
beenshowntobedeeplyconnectedwithcom-
creasingcompressionabilitiesofLLMsislinearly
pression,itissurprisingthatmodernLLMsare
correlatedtodownstreamtaskperformance.
notemployedinpracticaltextcompressionsys-
Previousworkshaveexploitedthisconnection
tems.Inthispaper,weprovideanin-depthanal-
ysisofneuralnetworkandtransformer-based for lossless text compression. Neural network
compressiontechniquestoanswerthisquestion. basedmodelshavebeenimplementedfortextcom-
Wecomparetraditionaltextcompressionsys- pression(SchmidhuberandHeil,1996;Mahoney,
temswithneuralnetworkandLLM-basedtext 2000;Goyaletal.,2018)andhavereachedbetter
compressionmethods. AlthoughLLM-based
compressionperformancethantraditionalalgorith-
systemssignificantlyoutperformconventional
miccompressorssuchasgzip. Morerecentmeth-
compressionmethods,theyarehighlyimprac-
ods have explored using LSTM and transformer
tical. Specifically,LLMZip,arecenttextcom-
pressionsystemusingLlama3-8Brequires9.5 models(Bellard,2019,2021). Thesemethodsfall
daystocompressjust10MBoftext,although under the "online" compressors category, where
with huge improvements in compression ra- a randomly initialized model is directly trained
tios. Toovercomethis,wepresentFineZip-a on the data being compressed. In this case, the
novelLLM-basedtextcompressionsystemthat
modelparametersalsobecomepartofthecompres-
combinesideasofonlinememorizationanddy-
sion. Arecenteffort,LLMZip(Valmeekametal.,
namiccontexttoreducethecompressiontime
2023),testedtheuseofLLMsforlosslesscompres-
immensely. FineZipcancompresstheabove
sion. Given an LLM’s ability to predict the next
corpusinapproximately4hourscomparedto
9.5days,a54timesimprovementoverLLMZip token provided a fixed-length context window, a
and comparable performance. FineZip out- tokenizedtextcanbestoredasprobabilisticranks
performstraditionalalgorithmiccompression produced by an LLM predicting the next token.
methodswithalargemargin,improvingcom- Thisisatypeof"offline"compression,withafixed
pression ratios by approximately 50%. With
systemusedforbothcompressionanddecompres-
thiswork,wetakethefirststeptowardsmaking
sionofallincomingtext.
losslesstextcompressionwithLLMsareality.
Inthispaper,webuildonpriorworkandintro-
While FineZip presents a significant step in
thatdirection,LLMsarestillnotaviablesolu-
duceFineZip,whichusesLLMsforlosslesstext
tionforlarge-scaletextcompression. Wehope compressionwithbothonlineandofflinecompo-
ourworkpavesthewayforfutureresearchand nents. FineZipcombinesan"online"component,
innovationtosolvethisproblem. whichmemorizesthedatabeingcompressed,with
an"offline"componentintheformofpre-trained
LLMs for compression. The "online" memoriza-
1 Introduction
tion is done by fine-tuning the model on the data
Whiletherelationshipbetweenlanguagemodeling beingcompressedinaparameter-efficientway(Hu
andcompressionhaslongbeenknown(Schmidhu- et al., 2021; Dettmers et al., 2023) with an addi-
berandHeil,1996;Mahoney,2000;Goyaletal., tionalconstantoverheadofthelearnedembeddings
2018;Bellard,2019),recentworks(Delétangetal., duringfine-tuning. The"offline"componentofthe
2024;Huangetal.,2024)havereinforcedthiscon- systemisthepre-trainedLLMwhichremainsfixed
nection. Delétang et al. (2024) recently showed acrossdifferentcorpora. Figure1depictsthesys-
4202
peS
52
]LC.sc[
1v14171.9042:viXraFigure1: SystemdiagramforFineZip.
temdiagramforFineZip. Withthisapproach,we acter in a word occupies 8 bits (1 byte in UTF-8
canleveragethebenefitsofonlinecompressionfor encoding), representing the word as a token, es-
improved performance without the drawback of sentially converting it into a number, will almost
requiringadditionalstorageformodelparameters. alwaysreducethenumberofbytesneededtorepre-
Additionally, with FineZip we allow for a dy- sentit. ThisconnectionwasalsoobservedinDelé-
namiccontextwhereeachtokenbeingcompressed tangetal.(2024). Asanextstep, wecanusethe
hasacontextsizeofequaltoitspositioninasen- predictive capabilities of LLMs for compression.
tence. This allows us to batch compression and This idea is used in LLMZip (Valmeekam et al.,
decompressionstepsusingLLMs,allowingforsig- 2023) where they use a pre-trained LLM for text
nificant speed-up. "Online memorization" using compression. The connection between language
PEFTmethodsalsoallowsthemodeltocompen- modelingandcompressionbecomesintuitivewhen
sateforlossofperformanceduetoadynamiccon- we take a deeper look at the language modeling
text,whileadynamiccontextallowsforbatching objective,implementedusingacross-entropyloss.
whichallowscompressionanddecompressionof Itaimstomakeeachtokeninthetrainingdatathe
manybatchesoftextinparallelwithinafixedcom- most probable token given the context preceding
pute budget. With FineZip, we can achieve 54 it,thusminimizingthenumberofbitsrequiredto
timesfastercompressiontimeswithminorlossof represent the rank of the token in the vocabulary
performancewhencomparedtoLLMZip,stillout- list,whenrankedindescendingorderaccordingto
performingtraditionaltext compression methods theirprobability. Followingthislineofthought,we
byahugemargin. Ourworkalsoshowsthatcom- propose an intuitive yet effective way of enhanc-
pressionratesofLLM-basedmethodsarestillnot ingthis-fine-tuningthemodelonthedatabeing
low enough for practical use cases, and although compressed.
FineZip pushes the limits of using LLMs loss- Achallengetowardsfine-tuningmodernLLMs
lesstextcompressioninpractice,muchworkstill isthattheyarememory-intensive. Additionally,if
needs to be done. The code for our work can be wefine-tunetheentiremodelonthetextbeingcom-
foundhere-https://github.com/fazalmittu/ pressed,thentheentireLLMbecomespartofthe
FineZip. compression,requiringanadditionalspaceequal
tothespacerequiredtostorethemodelfordecom-
2 IntroducingFineZip pression. Thus, we propose FineZip, a compres-
sion framework that involves parameter-efficient
ThemostbasicformofcompressionusingLLMs fine-tuning(PEFT)(Mangrulkaretal.,2022)onthe
wouldbetotokenizetheinputtext. Sinceeachchar- inputtextasan"online"steppriortocompression.Wecallthisthe"onlinememorization"stepwhich Method CompressionRatio Time(min)
zlib 0.3251 0.0083
makes the data being compressed more probable
gzip 0.3238 0.0141
fortheLLM.Thisfine-tuningisimplementedusing bzip2 0.2374 0.0437
LoRA(Huetal.,2021)andismuchfasterthanfull NNCP 0.15021 251
LLMZip(AC) 0.0795 13571
fine-tuning,requiresmuchlessGPUmemory,and
LLMZip 0.1163 13651
requiresaverysmallamountofadditionalstorage Finezip(AC) 0.0797 13118
forthetrainedembeddings. Theadditionalembed- Finezip 0.12799 250
Finezip-4bit 0.1445 67
dingstoragedoesnotscalewiththedatasetbeing
compressedandbecomesnegligibleatlargesizes
Table1:ComparisonofCompressionMethodson10mb
ofcorpora.
Another key difference between LLMZip and
FineZipisthatFineZipadoptsadynamiccontext ModificationstoLLMZip: LLMZiporiginally
sizeapproachratherthanmaintainingafixedslid- used Llama-1-7B (Touvron et al., 2023a) while
ing window. LLMZip uses a permanent sliding we leverage Llama-3-8B for both LLMZip and
window approach, where the rank of each token FineZip for uniform comparison. Additionally,
produced has a fixed context window of a preset LLMZipusedtwomethodsforcompression-one
context size (512 as chosen by original authors). usingarithmeticcoding(AC)andtheotherusing
Thisbydesignmakesthecompressionprocessex- a secondary compression methods on generated
tremelyautoregressiveandnon-parallelizable,asto ranks. LLMZipuseszlib(Jean-loupGailly,2024)
producetherankofatoken,youneedtheprevious as a secondary compression method over ranks
512tokens. whereasourexperimentsshowthatbzip2provides
amuchbettercompressionratio(Appendix: A.1).
FineZipovercomesthislimitationbyemployinga
Thus, we use bzip2 as our secondary compres-
two-stepdynamiccontextwindowtechnique:
sionmethodforLLMranksinbothLLMZipand
1. Dividethecorpusintochunksofapre-decided FineZip. We also refer to bzip2 as the baseline
windowlength. fortextcompressionusingtraditionalcompression
methods (Table 1). To offer a better comparison,
2. Produce the ranks of each token within the
wealsocreateaversionofFineZipthatincorpo-
windowsuchthattherankfortheith tokenis
ratesarithmeticcoding. Theprocessusesthelogits
producedbasedonthetokensprecedingit
that the LLM outputs for each new token as the
The dynamic context window gives a variable probability distribution update for the arithmetic
context size to each token in a chunk. For a uni- codingscheme.
formcomparison,weuseachunkingsizeof512in Weusedthefirst10mboftheenwik8(Marcus
FineZip,whichisthesameasthecontextwindow
Hutter, 2006) dataset which is a standard bench-
sizechosenbyLLMZip. InFineZip,theith token markforcompressiontasks. Thoughcompression
in a chunk has a context size of i−1, thus only ratio(ratioofcompressedfilesizeandoriginalfile
thefinaltokeninachunkhasaccesstofullcontext size) is the key metric, we are also interested in
lengthof512. Incontrast,everytokeninLLMZip measuring time taken by these compression sys-
has access to the full context length of 512. The temstoevaluatepracticality. Theresultsareshown
dynamiccontextleadstosomelossofperformance, in Table 1. The first key observation is that neu-
whichismadeupforbyonlinememorization. ralnetworkandLLMbasedcompressionmethods
have significantly better compression ratios than
3 Experiments
traditional text compression methods (zlib, gzip,
We begin by comparing FineZip with (i) tradi- bzip2), thus highlighting the potential impact of
tional text compression methods - bzip2 (Julian these methods for text compression. The second
Seward,2024),zlib(Jean-loupGailly,2024),and key observation is that neural network and LLM
gzip(Jean-loupGailly,1992),(ii)neuralnetwork basedmethodstakesalongtimetocompresseven
basedtextcompressionmethods-NNCP(Bellard, smallamountsoftext,thuspreventingtheirusein
2021),andthe(iii)recentLLM-basedtextcompres- practice. This is especially true when using AC
sion method called LLMZip. For both FineZip for compression in LLM-based methods, which
and LLMZip, we use Llama-3 8B (Dubey et al., producesexceptionalcompressionratiosbutalso
2024). requires unprecedentedly large amounts of time.Figure 3: Compressing 10mb dataset with LLama-3
Figure 2: FineZip ablations for different fine-tune 8Bloadedwith4, 8, 16, and32-bitprecision. Purple
epochs barshowscompressionratio,redlineshowstimetaken
to compress. Each batch size was chosen to max out
ForLLMZipwithAC,thetimetakentocompress memoryona48GBGPU.
10MB of data is approximately 9.5 days. Thus,
we do not explore AC-based LLM compression is able to mitigate the loss in performance. We
furtherandstrictlycompareonlyrank-basedLLM furtherpushthelimitsofcompressiontimeusing
baselines. quantization. We perform the memorization step
Table 1 shows that FineZip is able to achieve usingQLoRA(Dettmersetal.,2023)andperform
comparableorbettercompressionratiosthanboth compression using the quantized model. We do
NNCPandLLMZipwithamuchfastercompres- this using a fixed compute budget of 48GB GPU
sion time. Specifically, we see that FineZip has memoryonasingleNVIDIAA6000GPU.Lower
amuchbettercompressionratiothanNNCPwith precision models will allow us to increase batch
comparable amount of compression time, while sizeandinturn,decreasetimeneededtocompress
the 4-bit quantized FineZip is approximately 4 afilebyasizableamount. Figure3showsthatfine-
timesfasterthanNNCPandstillexhibitsabetter tuning/compressinga4bitmodelallowsustofita
compression ratio. FineZip compresses enwik8 batchsizeof70ononeA6000GPUandachievea
within 4 hours, compared to approximately 227 compressiontimeof67minutes. This4xspeedup
hours taken by LLMZip. This is a 54x improve- makesFineZipnotonlyacompetitivecompressor
mentoncompressiontimewithaminordropof1 out-performningtraditionaltextcompressionsys-
percentagepointincompressionratio. temsbyahugemargin,butalsothefastestneural
network/transformerbasedcompressioncurrently
3.1 FineZipAblations
available.
FineZip uses an "online memorization step" as
showninFigure1beforeperformingcompression.
4 Conclusion
ThisisdoneusingLow-RankAdaptation(LoRA)
(Hu et al., 2021). We compare the effect of fine-
In this paper we explore the possibility of using
tuningoncompressionusing3differentlanguage
LLMsforlosslesstextcompression. Weshowthat
models: GPT2-XL 1.3B (Radford et al., 2019),
while using neural network and LLM based text
LLama-27B(Touvronetal.,2023b),andLLama-
compression systems lead to significantly better
3 8B (Dubey et al., 2024). We see that for each
compression rates, they also require impractical
model,memorizationimprovestheabsolutecom-
amounts of compression time. To alleviate this,
pression ratio by at least 1 percentage point or a
we introduce FineZip - an LLM-based lossless
relative improvement of about 8% over its non-
textcompressionsystemwhichcompressestext54
fine-tuned baseline as shown in Figure 2. This is
timesfasterthanLLMZipwithaminorlossincom-
significantespeciallywhendealingwithsuchlow
pressionperformance. FineZipalsoimproveson
compressionrates. Itshouldbenotedthatthetime
thecompressionratiooftraditionaltextcompres-
takenformemorizationisnegligiblecomparedto
sionsystemsbyapproximately50%. Wealsoshow
compressiontimeandcanbeignored.
that while FineZip presents a significant step in
Quantization: WesawinTable1thatdynamic makingpracticaltextcompressionsystemsusing
context helps speed up the compression process LLMs,muchstillneedstobedone. Wehopeour
bysignificantamounts,whileonlinememorization workcanserveasasteppingstoneinthatdirection.5 Limitations JulianSeward.2024. bzip2-afreeandopen-sourcefile
compressionprogram. Accessed: 2024-06-01.
LLM-based text compression systems assume a
GPU being available in the host machine for lo- MatthewV.Mahoney.2000. Fasttextcompressionwith
neural networks. In Proceedings of the Thirteenth
cal compression. While this is not true for every
InternationalFloridaArtificialIntelligenceResearch
personalcomputer,thelandscapeisrapidlychang-
SocietyConference,pages230–234.AAAIPress.
ing. Manypersonallaptopsarenowequippedwith
Sourab Mangrulkar, Sylvain Gugger, Lysandre De-
GPUs and as compute becomes cheaper and the
but, Younes Belkada, Sayak Paul, and Benjamin
powerofLLMsgrow,weenvisionafuturewhere
Bossan. 2022. Peft: State-of-the-art parameter-
everypersonalcomputerwillbeequippedwithan efficient fine-tuning methods. https://github.
LLMrunninglocallyandperformingvarioustasks. com/huggingface/peft.
Marcus Hutter. 2006. enwik8. http://prize.
hutter1.net/index.htm. Accessed: 2024-08-15.
References
AlecRadford,JeffreyWu,RewonChild,DavidLuan,
FabriceBellard.2019. Losslessdatacompressionwith
DarioAmodei,IlyaSutskever,etal.2019. Language
neuralnetworks.
modelsareunsupervisedmultitasklearners. OpenAI
FabriceBellard.2021. Nncpv2: Losslessdatacompres- blog,1(8):9.
sionwithtransformer.
J. Schmidhuber and S. Heil. 1996. Sequential neural
J.ClearyandI.Witten.1984. Datacompressionusing textcompression. IEEETransactionsonNeuralNet-
adaptivecodingandpartialstringmatching. IEEE works,7(1):142–146.
TransactionsonCommunications,32(4):396–402.
HugoTouvron,ThibautLavril,GautierIzacard,Xavier
Grégoire Delétang, Anian Ruoss, Paul-Ambroise Martinet,Marie-AnneLachaux,TimothéeLacroix,
Duquenne, Elliot Catt, Tim Genewein, Christo- BaptisteRozière,NamanGoyal,EricHambro,Faisal
pherMattern,JordiGrau-Moya,LiKevinWenliang, Azhar, et al. 2023a. Llama: Open and effi-
Matthew Aitchison, Laurent Orseau, Marcus Hut- cient foundation language models. arXiv preprint
ter, and Joel Veness. 2024. Language modeling is arXiv:2302.13971.
compression. Preprint,arXiv:2309.10668.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and bert, Amjad Almahairi, Yasmine Babaei, Nikolay
LukeZettlemoyer.2023. Qlora: Efficientfinetuning Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
ofquantizedllms. Preprint,arXiv:2305.14314. Bhosale, et al. 2023b. Llama 2: Open foundation
andfine-tunedchatmodels,2023. URLhttps://arxiv.
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey, org/abs/2307.09288.
AbhishekKadian,AhmadAl-Dahle,AieshaLetman,
Akhil Mathur, Alan Schelten, Amy Yang, Angela Chandra Shekhara Kaushik Valmeekam, Krishna
Fan,etal.2024. Thellama3herdofmodels. arXiv Narayanan,DileepKalathil,Jean-FrancoisChamber-
preprintarXiv:2407.21783. land,andSrinivasShakkottai.2023. Llmzip: Loss-
lesstextcompressionusinglargelanguagemodels.
Google.2024. Brotlicompressionalgorithm. Accessed:
Preprint,arXiv:2306.04050.
2024-06-01.
Mohit Goyal, Kedar Tatwawadi, Shubham Chandak,
andIdoiaOchoa.2018. Deepzip: Losslessdatacom-
pressionusingrecurrentneuralnetworks. Preprint,
arXiv:1811.08162.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu,YuanzhiLi,SheanWang,LuWang,and
WeizhuChen.2021. Lora: Low-rankadaptationof
largelanguagemodels. Preprint,arXiv:2106.09685.
YuzhenHuang,JinghanZhang,ZifeiShan,andJunx-
ianHe.2024. Compressionrepresentsintelligence
linearly. Preprint,arXiv:2404.09937.
Jean-loupGailly.1992. Gzip. http://www.gzip.org.
Accessed: 2024-08-15.
Jean-loup Gailly. 2024. Zlib: A massively spiffy yet
delicately unobtrusive compression library. http:
//www.zlib.net. Accessed: 2024-08-15.A Appendix tionalcompressiontechniques(Brotli,BZ2,PPM)
to create a benchmark for ourselves. Figure 3
A.1 EvaluatingTraditionalCompression
shows that Brotli, BZ2, and PPM perform con-
Methods
sistently across varying input file sizes and that
Wefirstexperimentedwiththreetraditionalcom- PPMperformsthebestontextualdata,reachinga
pression methods - Brotli (Google, 2024), BZ2 compressionratioofapproximately0.25. Figure4
(JulianSeward,2024),andPPM(ClearyandWit- measuresthecompressionratiowhentwocompres-
ten,1984)-fortextcompressionasafunctionof sion techniques are stacked and serves as a more
increasingdatasetsize. WefindthatPPMperforms accuratebenchmarkforFineZipasitalsoemploys
bestfortextcompression,andthattheperformance twostepcompression. Throughthesesetofbase-
remainsrelativelyconstantwithrespecttodataset line experiments, we can see that a compression
size. TheresultscanbeseeninFigure4. ratioof0.25isthevaluetobeat.
Figure4: EvaluatingBaselineCompressionTechniques
Figure6: EvaluatingStackedCompressionwithBrotli,
Brotli,BZ2,andPPMonenwik8
BZ2,andPPMonenwik8
We then use these algorithms to compress the
ranksgeneratedbyLLMsinFineZip. Weseethat
A.3 ContextSize
BZ2hasthebestperformancesowechoseitasthe
traditionalcompressionmethodforFineZip. Todeterminethebestcontextwindowsizetouse,
we ran experiments with the LLama2-7B base
model(LLMZip)anddiscoveredthatalargercon-
textsizeresultsinabettercompressionratio. The
compressionratiobegantoplateauasthecontext
windowreached512sowedecidedtousethatfor
allofourexperimentation.
Figure5: TestingTraditionalCompressionTechniques
Brotli,BZ2,andPPMontheranksproducedbycom-
pressing enwik8 with LLama2-7B finetuned for 64
epochswithr=16
A.2 DoubleCompressionBenchmark
Figure 7: Evaluating Best Context Window for Com-
Prior to testing FineZip, we compressed the en-
pression
wik8 (Marcus Hutter, 2006) dataset using tradi-Figure8: Compressinginputfilesofsize1,10,and100
megabyteswithLLama-38Bfinetunedfor256epochs.
A.4 FineZipandDatasetSize
Thepreviousexperimentswereonlyusingadataset
size of 10mb and for this to be a viable com-
pression technique, it has to scale well for much
smaller and larger file sizes. Figure 8 shows that
LLama-3 8B (Dubey et al., 2024) fine-tuned for
256epochsmaintainsaconsistentcompressionra-
tio on dataset sizes of 1, 10, and 100mb. This
verifiesthatFineZipremainsviableregardlessof
datasetsize.