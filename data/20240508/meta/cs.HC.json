[
    {
        "title": "Unveiling Disparities in Web Task Handling Between Human and Web Agent",
        "authors": "Kihoon SonJinhyeon KwonDeEun ChoiTae Soo KimYoung-Ho KimSangdoo YunJuho Kim",
        "links": "http://arxiv.org/abs/2405.04497v1",
        "entry_id": "http://arxiv.org/abs/2405.04497v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04497v1",
        "summary": "With the advancement of Large-Language Models (LLMs) and Large\nVision-Language Models (LVMs), agents have shown significant capabilities in\nvarious tasks, such as data analysis, gaming, or code generation. Recently,\nthere has been a surge in research on web agents, capable of performing tasks\nwithin the web environment. However, the web poses unforeseeable scenarios,\nchallenging the generalizability of these agents. This study investigates the\ndisparities between human and web agents' performance in web tasks (e.g.,\ninformation search) by concentrating on planning, action, and reflection\naspects during task execution. We conducted a web task study with a think-aloud\nprotocol, revealing distinct cognitive actions and operations on websites\nemployed by humans. Comparative examination of existing agent structures and\nhuman behavior with thought processes highlighted differences in knowledge\nupdating and ambiguity handling when performing the task. Humans demonstrated a\npropensity for exploring and modifying plans based on additional information\nand investigating reasons for failure. These findings offer insights into\ndesigning planning, reflection, and information discovery modules for web\nagents and designing the capturing method for implicit human knowledge in a web\ntask.",
        "updated": "2024-05-07 17:10:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04497v1"
    },
    {
        "title": "Towards Geographic Inclusion in the Evaluation of Text-to-Image Models",
        "authors": "Melissa HallSamuel J. BellCandace RossAdina WilliamsMichal DrozdzalAdriana Romero Soriano",
        "links": "http://arxiv.org/abs/2405.04457v1",
        "entry_id": "http://arxiv.org/abs/2405.04457v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04457v1",
        "summary": "Rapid progress in text-to-image generative models coupled with their\ndeployment for visual content creation has magnified the importance of\nthoroughly evaluating their performance and identifying potential biases. In\npursuit of models that generate images that are realistic, diverse, visually\nappealing, and consistent with the given prompt, researchers and practitioners\noften turn to automated metrics to facilitate scalable and cost-effective\nperformance profiling. However, commonly-used metrics often fail to account for\nthe full diversity of human preference; often even in-depth human evaluations\nface challenges with subjectivity, especially as interpretations of evaluation\ncriteria vary across regions and cultures. In this work, we conduct a large,\ncross-cultural study to study how much annotators in Africa, Europe, and\nSoutheast Asia vary in their perception of geographic representation, visual\nappeal, and consistency in real and generated images from state-of-the art\npublic APIs. We collect over 65,000 image annotations and 20 survey responses.\nWe contrast human annotations with common automated metrics, finding that human\npreferences vary notably across geographic location and that current metrics do\nnot fully account for this diversity. For example, annotators in different\nlocations often disagree on whether exaggerated, stereotypical depictions of a\nregion are considered geographically representative. In addition, the utility\nof automatic evaluations is dependent on assumptions about their set-up, such\nas the alignment of feature extractors with human perception of object\nsimilarity or the definition of \"appeal\" captured in reference datasets used to\nground evaluations. We recommend steps for improved automatic and human\nevaluations.",
        "updated": "2024-05-07 16:23:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04457v1"
    },
    {
        "title": "Large Language Models Cannot Explain Themselves",
        "authors": "Advait Sarkar",
        "links": "http://arxiv.org/abs/2405.04382v1",
        "entry_id": "http://arxiv.org/abs/2405.04382v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04382v1",
        "summary": "Large language models can be prompted to produce text. They can also be\nprompted to produce \"explanations\" of their output. But these are not really\nexplanations, because they do not accurately reflect the mechanical process\nunderlying the prediction. The illusion that they reflect the reasoning process\ncan result in significant harms. These \"explanations\" can be valuable, but for\npromoting critical thinking rather than for understanding the model. I propose\na recontextualisation of these \"explanations\", using the term \"exoplanations\"\nto draw attention to their exogenous nature. I discuss some implications for\ndesign and technology, such as the inclusion of appropriate guardrails and\nresponses when models are prompted to generate explanations.",
        "updated": "2024-05-07 15:05:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04382v1"
    },
    {
        "title": "A General Model for Detecting Learner Engagement: Implementation and Evaluation",
        "authors": "Somayeh MalekshahiJavad M. KheyridoostOmid Fatemi",
        "links": "http://arxiv.org/abs/2405.04251v1",
        "entry_id": "http://arxiv.org/abs/2405.04251v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04251v1",
        "summary": "Considering learner engagement has a mutual benefit for both learners and\ninstructors. Instructors can help learners increase their attention,\ninvolvement, motivation, and interest. On the other hand, instructors can\nimprove their instructional performance by evaluating the cumulative results of\nall learners and upgrading their training programs. This paper proposes a\ngeneral, lightweight model for selecting and processing features to detect\nlearners' engagement levels while preserving the sequential temporal\nrelationship over time. During training and testing, we analyzed the videos\nfrom the publicly available DAiSEE dataset to capture the dynamic essence of\nlearner engagement. We have also proposed an adaptation policy to find new\nlabels that utilize the affective states of this dataset related to education,\nthereby improving the models' judgment. The suggested model achieves an\naccuracy of 68.57\\% in a specific implementation and outperforms the studied\nstate-of-the-art models detecting learners' engagement levels.",
        "updated": "2024-05-07 12:11:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04251v1"
    },
    {
        "title": "What Impacts the Quality of the User Answers when Asked about the Current Context?",
        "authors": "Ivano BisonHaonan ZhaoFausto Giunchiglia",
        "links": "http://arxiv.org/abs/2405.04054v1",
        "entry_id": "http://arxiv.org/abs/2405.04054v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04054v1",
        "summary": "Sensor data provide an objective view of reality but fail to capture the\nsubjective motivations behind an individual's behavior. This latter information\nis crucial for learning about the various dimensions of the personal context,\nthus increasing predictability. The main limitation is the human input, which\nis often not of the quality that is needed. The work so far has focused on the\nusually high number of missing answers. The focus of this paper is on\n\\textit{the number of mistakes} made when answering questions. Three are the\nmain contributions of this paper. First, we show that the user's reaction time,\ni.e., the time before starting to respond, is the main cause of a low answer\nquality, where its effects are both direct and indirect, the latter relating to\nits impact on the completion time, i.e., the time taken to compile the\nresponse. Second, we identify the specific exogenous (e.g., the situational or\ntemporal context) and endogenous (e.g., mood, personality traits) factors which\nhave an influence on the reaction time, as well as on the completion time.\nThird, we show how reaction and completion time compose their effects on the\nanswer quality. The paper concludes with a set of actionable recommendations.",
        "updated": "2024-05-07 06:55:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04054v1"
    }
]