[
    {
        "title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving",
        "authors": "Yujun LinHaotian TangShang YangZhekai ZhangGuangxuan XiaoChuang GanSong Han",
        "links": "http://arxiv.org/abs/2405.04532v1",
        "entry_id": "http://arxiv.org/abs/2405.04532v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04532v1",
        "summary": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.",
        "updated": "2024-05-07 17:59:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04532v1"
    },
    {
        "title": "xLSTM: Extended Long Short-Term Memory",
        "authors": "Maximilian BeckKorbinian PöppelMarkus SpanringAndreas AuerOleksandra PrudnikovaMichael KoppGünter KlambauerJohannes BrandstetterSepp Hochreiter",
        "links": "http://arxiv.org/abs/2405.04517v1",
        "entry_id": "http://arxiv.org/abs/2405.04517v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04517v1",
        "summary": "In the 1990s, the constant error carousel and gating were introduced as the\ncentral ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have\nstood the test of time and contributed to numerous deep learning success\nstories, in particular they constituted the first Large Language Models (LLMs).\nHowever, the advent of the Transformer technology with parallelizable\nself-attention at its core marked the dawn of a new era, outpacing LSTMs at\nscale. We now raise a simple question: How far do we get in language modeling\nwhen scaling LSTMs to billions of parameters, leveraging the latest techniques\nfrom modern LLMs, but mitigating known limitations of LSTMs? Firstly, we\nintroduce exponential gating with appropriate normalization and stabilization\ntechniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM\nwith a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that\nis fully parallelizable with a matrix memory and a covariance update rule.\nIntegrating these LSTM extensions into residual block backbones yields xLSTM\nblocks that are then residually stacked into xLSTM architectures. Exponential\ngating and modified memory structures boost xLSTM capabilities to perform\nfavorably when compared to state-of-the-art Transformers and State Space\nModels, both in performance and scaling.",
        "updated": "2024-05-07 17:50:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04517v1"
    },
    {
        "title": "Switchable Decision: Dynamic Neural Generation Networks",
        "authors": "Shujian ZhangKorawat TanwisuthChengyue GongPengcheng HeMingyuan Zhou",
        "links": "http://arxiv.org/abs/2405.04513v1",
        "entry_id": "http://arxiv.org/abs/2405.04513v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04513v1",
        "summary": "Auto-regressive generation models achieve competitive performance across many\ndifferent NLP tasks such as summarization, question answering, and\nclassifications. However, they are also known for being slow in inference,\nwhich makes them challenging to deploy in real-time applications. We propose a\nswitchable decision to accelerate inference by dynamically assigning\ncomputation resources for each data instance. Automatically making decisions on\nwhere to skip and how to balance quality and computation cost with constrained\noptimization, our dynamic neural generation networks enforce the efficient\ninference path and determine the optimized trade-off. Experiments across\nquestion answering, summarization, and classification benchmarks show that our\nmethod benefits from less computation cost during inference while keeping the\nsame accuracy. Extensive experiments and ablation studies demonstrate that our\nmethod can be general, effective, and beneficial for many NLP tasks.",
        "updated": "2024-05-07 17:44:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04513v1"
    },
    {
        "title": "Toward In-Context Teaching: Adapting Examples to Students' Misconceptions",
        "authors": "Alexis RossJacob Andreas",
        "links": "http://arxiv.org/abs/2405.04495v1",
        "entry_id": "http://arxiv.org/abs/2405.04495v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04495v1",
        "summary": "When a teacher provides examples for a student to study, these examples must\nbe informative, enabling a student to progress from their current state toward\na target concept or skill. Good teachers must therefore simultaneously infer\nwhat students already know and adapt their teaching to students' changing state\nof knowledge. There is increasing interest in using computational models,\nparticularly large language models, as pedagogical tools. As students, language\nmodels in particular have shown a remarkable ability to adapt to new tasks\ngiven small numbers of examples. But how effectively can these models adapt as\nteachers to students of different types? To study this question, we introduce a\nsuite of models and evaluation methods we call AdapT. AdapT has two components:\n(1) a collection of simulated Bayesian student models that can be used for\nevaluation of automated teaching methods; (2) a platform for evaluation with\nhuman students, to characterize the real-world effectiveness of these methods.\nWe additionally introduce (3) AToM, a new probabilistic model for adaptive\nteaching that jointly infers students' past beliefs and optimizes for the\ncorrectness of future beliefs. In evaluations of simulated students across\nthree learning domains (fraction arithmetic, English morphology, function\nlearning), AToM systematically outperforms LLM-based and standard Bayesian\nteaching models. In human experiments, both AToM and LLMs outperform\nnon-adaptive random example selection. Our results highlight both the\ndifficulty of the adaptive teaching task and the potential of learned adaptive\nmodels for solving it.",
        "updated": "2024-05-07 17:05:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04495v1"
    },
    {
        "title": "TorchDriveEnv: A Reinforcement Learning Benchmark for Autonomous Driving with Reactive, Realistic, and Diverse Non-Playable Characters",
        "authors": "Jonathan Wilder LavingtonKe ZhangVasileios LioutasMatthew NiedobaYunpeng LiuDylan GreenSaeid NaderipariziXiaoxuan LiangSetareh DabiriAdam ŚcibiorBerend ZwartsenbergFrank Wood",
        "links": "http://arxiv.org/abs/2405.04491v1",
        "entry_id": "http://arxiv.org/abs/2405.04491v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04491v1",
        "summary": "The training, testing, and deployment, of autonomous vehicles requires\nrealistic and efficient simulators. Moreover, because of the high variability\nbetween different problems presented in different autonomous systems, these\nsimulators need to be easy to use, and easy to modify. To address these\nproblems we introduce TorchDriveSim and its benchmark extension TorchDriveEnv.\nTorchDriveEnv is a lightweight reinforcement learning benchmark programmed\nentirely in Python, which can be modified to test a number of different factors\nin learned vehicle behavior, including the effect of varying kinematic models,\nagent types, and traffic control patterns. Most importantly unlike many replay\nbased simulation approaches, TorchDriveEnv is fully integrated with a state of\nthe art behavioral simulation API. This allows users to train and evaluate\ndriving models alongside data driven Non-Playable Characters (NPC) whose\ninitializations and driving behavior are reactive, realistic, and diverse. We\nillustrate the efficiency and simplicity of TorchDriveEnv by evaluating common\nreinforcement learning baselines in both training and validation environments.\nOur experiments show that TorchDriveEnv is easy to use, but difficult to solve.",
        "updated": "2024-05-07 17:02:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04491v1"
    }
]