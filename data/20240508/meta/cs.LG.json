[
    {
        "title": "ChatHuman: Language-driven 3D Human Understanding with Retrieval-Augmented Tool Reasoning",
        "authors": "Jing LinYao FengWeiyang LiuMichael J. Black",
        "links": "http://arxiv.org/abs/2405.04533v1",
        "entry_id": "http://arxiv.org/abs/2405.04533v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04533v1",
        "summary": "Numerous methods have been proposed to detect, estimate, and analyze\nproperties of people in images, including the estimation of 3D pose, shape,\ncontact, human-object interaction, emotion, and more. Each of these methods\nworks in isolation instead of synergistically. Here we address this problem and\nbuild a language-driven human understanding system -- ChatHuman, which combines\nand integrates the skills of many different methods. To do so, we finetune a\nLarge Language Model (LLM) to select and use a wide variety of existing tools\nin response to user inputs. In doing so, ChatHuman is able to combine\ninformation from multiple tools to solve problems more accurately than the\nindividual tools themselves and to leverage tool output to improve its ability\nto reason about humans. The novel features of ChatHuman include leveraging\nacademic publications to guide the application of 3D human-related tools,\nemploying a retrieval-augmented generation model to generate\nin-context-learning examples for handling new tools, and discriminating and\nintegrating tool results to enhance 3D human understanding. Our experiments\nshow that ChatHuman outperforms existing models in both tool selection accuracy\nand performance across multiple 3D human-related tasks. ChatHuman is a step\ntowards consolidating diverse methods for human analysis into a single,\npowerful, system for 3D human reasoning.",
        "updated": "2024-05-07 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04533v1"
    },
    {
        "title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving",
        "authors": "Yujun LinHaotian TangShang YangZhekai ZhangGuangxuan XiaoChuang GanSong Han",
        "links": "http://arxiv.org/abs/2405.04532v1",
        "entry_id": "http://arxiv.org/abs/2405.04532v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04532v1",
        "summary": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.",
        "updated": "2024-05-07 17:59:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04532v1"
    },
    {
        "title": "NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts",
        "authors": "Shudan ZhangHanlin ZhaoXiao LiuQinkai ZhengZehan QiXiaotao GuXiaohan ZhangYuxiao DongJie Tang",
        "links": "http://arxiv.org/abs/2405.04520v1",
        "entry_id": "http://arxiv.org/abs/2405.04520v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04520v1",
        "summary": "Large language models (LLMs) have manifested strong ability to generate codes\nfor productive activities. However, current benchmarks for code synthesis, such\nas HumanEval, MBPP, and DS-1000, are predominantly oriented towards\nintroductory tasks on algorithm and data science, insufficiently satisfying\nchallenging requirements prevalent in real-world coding. To fill this gap, we\npropose NaturalCodeBench (NCB), a challenging code benchmark designed to mirror\nthe complexity and variety of scenarios in real coding tasks. NCB comprises 402\nhigh-quality problems in Python and Java, meticulously selected from natural\nuser queries from online coding services, covering 6 different domains. Noting\nthe extraordinary difficulty in creating testing cases for real-world queries,\nwe also introduce a semi-automated pipeline to enhance the efficiency of test\ncase construction. Comparing with manual solutions, it achieves an efficiency\nincrease of more than 4 times. Our systematic experiments on 39 LLMs find that\nperformance gaps on NCB between models with close HumanEval scores could still\nbe significant, indicating a lack of focus on practical code synthesis\nscenarios or over-specified optimization on HumanEval. On the other hand, even\nthe best-performing GPT-4 is still far from satisfying on NCB. The evaluation\ntoolkit and development set are available at\nhttps://github.com/THUDM/NaturalCodeBench.",
        "updated": "2024-05-07 17:52:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04520v1"
    },
    {
        "title": "xLSTM: Extended Long Short-Term Memory",
        "authors": "Maximilian BeckKorbinian PöppelMarkus SpanringAndreas AuerOleksandra PrudnikovaMichael KoppGünter KlambauerJohannes BrandstetterSepp Hochreiter",
        "links": "http://arxiv.org/abs/2405.04517v1",
        "entry_id": "http://arxiv.org/abs/2405.04517v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04517v1",
        "summary": "In the 1990s, the constant error carousel and gating were introduced as the\ncentral ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have\nstood the test of time and contributed to numerous deep learning success\nstories, in particular they constituted the first Large Language Models (LLMs).\nHowever, the advent of the Transformer technology with parallelizable\nself-attention at its core marked the dawn of a new era, outpacing LSTMs at\nscale. We now raise a simple question: How far do we get in language modeling\nwhen scaling LSTMs to billions of parameters, leveraging the latest techniques\nfrom modern LLMs, but mitigating known limitations of LSTMs? Firstly, we\nintroduce exponential gating with appropriate normalization and stabilization\ntechniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM\nwith a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that\nis fully parallelizable with a matrix memory and a covariance update rule.\nIntegrating these LSTM extensions into residual block backbones yields xLSTM\nblocks that are then residually stacked into xLSTM architectures. Exponential\ngating and modified memory structures boost xLSTM capabilities to perform\nfavorably when compared to state-of-the-art Transformers and State Space\nModels, both in performance and scaling.",
        "updated": "2024-05-07 17:50:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04517v1"
    },
    {
        "title": "Switchable Decision: Dynamic Neural Generation Networks",
        "authors": "Shujian ZhangKorawat TanwisuthChengyue GongPengcheng HeMingyuan Zhou",
        "links": "http://arxiv.org/abs/2405.04513v1",
        "entry_id": "http://arxiv.org/abs/2405.04513v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04513v1",
        "summary": "Auto-regressive generation models achieve competitive performance across many\ndifferent NLP tasks such as summarization, question answering, and\nclassifications. However, they are also known for being slow in inference,\nwhich makes them challenging to deploy in real-time applications. We propose a\nswitchable decision to accelerate inference by dynamically assigning\ncomputation resources for each data instance. Automatically making decisions on\nwhere to skip and how to balance quality and computation cost with constrained\noptimization, our dynamic neural generation networks enforce the efficient\ninference path and determine the optimized trade-off. Experiments across\nquestion answering, summarization, and classification benchmarks show that our\nmethod benefits from less computation cost during inference while keeping the\nsame accuracy. Extensive experiments and ablation studies demonstrate that our\nmethod can be general, effective, and beneficial for many NLP tasks.",
        "updated": "2024-05-07 17:44:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04513v1"
    }
]