[
    {
        "title": "Tactile-Augmented Radiance Fields",
        "authors": "Yiming DouFengyu YangYi LiuAntonio LoquercioAndrew Owens",
        "links": "http://arxiv.org/abs/2405.04534v1",
        "entry_id": "http://arxiv.org/abs/2405.04534v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04534v1",
        "summary": "We present a scene representation, which we call a tactile-augmented radiance\nfield (TaRF), that brings vision and touch into a shared 3D space. This\nrepresentation can be used to estimate the visual and tactile signals for a\ngiven 3D position within a scene. We capture a scene's TaRF from a collection\nof photos and sparsely sampled touch probes. Our approach makes use of two\ninsights: (i) common vision-based touch sensors are built on ordinary cameras\nand thus can be registered to images using methods from multi-view geometry,\nand (ii) visually and structurally similar regions of a scene share the same\ntactile features. We use these insights to register touch signals to a captured\nvisual scene, and to train a conditional diffusion model that, provided with an\nRGB-D image rendered from a neural radiance field, generates its corresponding\ntactile signal. To evaluate our approach, we collect a dataset of TaRFs. This\ndataset contains more touch samples than previous real-world datasets, and it\nprovides spatially aligned visual signals for each captured touch signal. We\ndemonstrate the accuracy of our cross-modal generative model and the utility of\nthe captured visual-tactile data on several downstream tasks. Project page:\nhttps://dou-yiming.github.io/TaRF",
        "updated": "2024-05-07 17:59:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04534v1"
    },
    {
        "title": "ChatHuman: Language-driven 3D Human Understanding with Retrieval-Augmented Tool Reasoning",
        "authors": "Jing LinYao FengWeiyang LiuMichael J. Black",
        "links": "http://arxiv.org/abs/2405.04533v1",
        "entry_id": "http://arxiv.org/abs/2405.04533v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04533v1",
        "summary": "Numerous methods have been proposed to detect, estimate, and analyze\nproperties of people in images, including the estimation of 3D pose, shape,\ncontact, human-object interaction, emotion, and more. Each of these methods\nworks in isolation instead of synergistically. Here we address this problem and\nbuild a language-driven human understanding system -- ChatHuman, which combines\nand integrates the skills of many different methods. To do so, we finetune a\nLarge Language Model (LLM) to select and use a wide variety of existing tools\nin response to user inputs. In doing so, ChatHuman is able to combine\ninformation from multiple tools to solve problems more accurately than the\nindividual tools themselves and to leverage tool output to improve its ability\nto reason about humans. The novel features of ChatHuman include leveraging\nacademic publications to guide the application of 3D human-related tools,\nemploying a retrieval-augmented generation model to generate\nin-context-learning examples for handling new tools, and discriminating and\nintegrating tool results to enhance 3D human understanding. Our experiments\nshow that ChatHuman outperforms existing models in both tool selection accuracy\nand performance across multiple 3D human-related tasks. ChatHuman is a step\ntowards consolidating diverse methods for human analysis into a single,\npowerful, system for 3D human reasoning.",
        "updated": "2024-05-07 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04533v1"
    },
    {
        "title": "Edit-Your-Motion: Space-Time Diffusion Decoupling Learning for Video Motion Editing",
        "authors": "Yi ZuoLingling LiLicheng JiaoFang LiuXu LiuWenping MaShuyuan YangYuwei Guo",
        "links": "http://arxiv.org/abs/2405.04496v1",
        "entry_id": "http://arxiv.org/abs/2405.04496v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04496v1",
        "summary": "Existing diffusion-based video editing methods have achieved impressive\nresults in motion editing. Most of the existing methods focus on the motion\nalignment between the edited video and the reference video. However, these\nmethods do not constrain the background and object content of the video to\nremain unchanged, which makes it possible for users to generate unexpected\nvideos. In this paper, we propose a one-shot video motion editing method called\nEdit-Your-Motion that requires only a single text-video pair for training.\nSpecifically, we design the Detailed Prompt-Guided Learning Strategy (DPL) to\ndecouple spatio-temporal features in space-time diffusion models. DPL separates\nlearning object content and motion into two training stages. In the first\ntraining stage, we focus on learning the spatial features (the features of\nobject content) and breaking down the temporal relationships in the video\nframes by shuffling them. We further propose Recurrent-Causal Attention\n(RC-Attn) to learn the consistent content features of the object from unordered\nvideo frames. In the second training stage, we restore the temporal\nrelationship in video frames to learn the temporal feature (the features of the\nbackground and object's motion). We also adopt the Noise Constraint Loss to\nsmooth out inter-frame differences. Finally, in the inference stage, we inject\nthe content features of the source object into the editing branch through a\ntwo-branch structure (editing branch and reconstruction branch). With\nEdit-Your-Motion, users can edit the motion of objects in the source video to\ngenerate more exciting and diverse videos. Comprehensive qualitative\nexperiments, quantitative experiments and user preference studies demonstrate\nthat Edit-Your-Motion performs better than other methods.",
        "updated": "2024-05-07 17:06:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04496v1"
    },
    {
        "title": "S3Former: Self-supervised High-resolution Transformer for Solar PV Profiling",
        "authors": "Minh TranAdrian De LuisHaitao LiaoYing HuangRoy McCannAlan MantoothJack CothrenNgan Le",
        "links": "http://arxiv.org/abs/2405.04489v1",
        "entry_id": "http://arxiv.org/abs/2405.04489v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04489v1",
        "summary": "As the impact of climate change escalates, the global necessity to transition\nto sustainable energy sources becomes increasingly evident. Renewable energies\nhave emerged as a viable solution for users, with Photovoltaic energy being a\nfavored choice for small installations due to its reliability and efficiency.\nAccurate mapping of PV installations is crucial for understanding the extension\nof its adoption and informing energy policy. To meet this need, we introduce\nS3Former, designed to segment solar panels from aerial imagery and provide size\nand location information critical for analyzing the impact of such\ninstallations on the grid. Solar panel identification is challenging due to\nfactors such as varying weather conditions, roof characteristics, Ground\nSampling Distance variations and lack of appropriate initialization weights for\noptimized training. To tackle these complexities, S3Former features a Masked\nAttention Mask Transformer incorporating a self-supervised learning pretrained\nbackbone. Specifically, our model leverages low-level and high-level features\nextracted from the backbone and incorporates an instance query mechanism\nincorporated on the Transformer architecture to enhance the localization of\nsolar PV installations. We introduce a self-supervised learning phase (pretext\ntask) to improve the initialization weights on the backbone of S3Former. We\nevaluated S3Former using diverse datasets, demonstrate improvement\nstate-of-the-art models.",
        "updated": "2024-05-07 16:56:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04489v1"
    },
    {
        "title": "A Significantly Better Class of Activation Functions Than ReLU Like Activation Functions",
        "authors": "Mathew Mithra NoelYug Oswal",
        "links": "http://arxiv.org/abs/2405.04459v1",
        "entry_id": "http://arxiv.org/abs/2405.04459v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04459v1",
        "summary": "This paper introduces a significantly better class of activation functions\nthan the almost universally used ReLU like and Sigmoidal class of activation\nfunctions. Two new activation functions referred to as the Cone and\nParabolic-Cone that differ drastically from popular activation functions and\nsignificantly outperform these on the CIFAR-10 and Imagenette benchmmarks are\nproposed. The cone activation functions are positive only on a finite interval\nand are strictly negative except at the end-points of the interval, where they\nbecome zero. Thus the set of inputs that produce a positive output for a neuron\nwith cone activation functions is a hyperstrip and not a half-space as is the\nusual case. Since a hyper strip is the region between two parallel\nhyper-planes, it allows neurons to more finely divide the input feature space\ninto positive and negative classes than with infinitely wide half-spaces. In\nparticular the XOR function can be learn by a single neuron with cone-like\nactivation functions. Both the cone and parabolic-cone activation functions are\nshown to achieve higher accuracies with significantly fewer neurons on\nbenchmarks. The results presented in this paper indicate that many nonlinear\nreal-world datasets may be separated with fewer hyperstrips than half-spaces.\nThe Cone and Parabolic-Cone activation functions have larger derivatives than\nReLU and are shown to significantly speedup training.",
        "updated": "2024-05-07 16:24:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04459v1"
    }
]