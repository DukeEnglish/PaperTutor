[
    {
        "title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving",
        "authors": "Yujun LinHaotian TangShang YangZhekai ZhangGuangxuan XiaoChuang GanSong Han",
        "links": "http://arxiv.org/abs/2405.04532v1",
        "entry_id": "http://arxiv.org/abs/2405.04532v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04532v1",
        "summary": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.",
        "updated": "2024-05-07 17:59:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04532v1"
    },
    {
        "title": "NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts",
        "authors": "Shudan ZhangHanlin ZhaoXiao LiuQinkai ZhengZehan QiXiaotao GuXiaohan ZhangYuxiao DongJie Tang",
        "links": "http://arxiv.org/abs/2405.04520v1",
        "entry_id": "http://arxiv.org/abs/2405.04520v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04520v1",
        "summary": "Large language models (LLMs) have manifested strong ability to generate codes\nfor productive activities. However, current benchmarks for code synthesis, such\nas HumanEval, MBPP, and DS-1000, are predominantly oriented towards\nintroductory tasks on algorithm and data science, insufficiently satisfying\nchallenging requirements prevalent in real-world coding. To fill this gap, we\npropose NaturalCodeBench (NCB), a challenging code benchmark designed to mirror\nthe complexity and variety of scenarios in real coding tasks. NCB comprises 402\nhigh-quality problems in Python and Java, meticulously selected from natural\nuser queries from online coding services, covering 6 different domains. Noting\nthe extraordinary difficulty in creating testing cases for real-world queries,\nwe also introduce a semi-automated pipeline to enhance the efficiency of test\ncase construction. Comparing with manual solutions, it achieves an efficiency\nincrease of more than 4 times. Our systematic experiments on 39 LLMs find that\nperformance gaps on NCB between models with close HumanEval scores could still\nbe significant, indicating a lack of focus on practical code synthesis\nscenarios or over-specified optimization on HumanEval. On the other hand, even\nthe best-performing GPT-4 is still far from satisfying on NCB. The evaluation\ntoolkit and development set are available at\nhttps://github.com/THUDM/NaturalCodeBench.",
        "updated": "2024-05-07 17:52:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04520v1"
    },
    {
        "title": "A Transformer with Stack Attention",
        "authors": "Jiaoda LiJennifer C. WhiteMrinmaya SachanRyan Cotterell",
        "links": "http://arxiv.org/abs/2405.04515v1",
        "entry_id": "http://arxiv.org/abs/2405.04515v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04515v1",
        "summary": "Natural languages are believed to be (mildly) context-sensitive. Despite\nunderpinning remarkably capable large language models, transformers are unable\nto model many context-free language tasks. In an attempt to address this\nlimitation in the modeling power of transformer-based language models, we\npropose augmenting them with a differentiable, stack-based attention mechanism.\nOur stack-based attention mechanism can be incorporated into any\ntransformer-based language model and adds a level of interpretability to the\nmodel. We show that the addition of our stack-based attention mechanism enables\nthe transformer to model some, but not all, deterministic context-free\nlanguages.",
        "updated": "2024-05-07 17:47:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04515v1"
    },
    {
        "title": "Switchable Decision: Dynamic Neural Generation Networks",
        "authors": "Shujian ZhangKorawat TanwisuthChengyue GongPengcheng HeMingyuan Zhou",
        "links": "http://arxiv.org/abs/2405.04513v1",
        "entry_id": "http://arxiv.org/abs/2405.04513v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04513v1",
        "summary": "Auto-regressive generation models achieve competitive performance across many\ndifferent NLP tasks such as summarization, question answering, and\nclassifications. However, they are also known for being slow in inference,\nwhich makes them challenging to deploy in real-time applications. We propose a\nswitchable decision to accelerate inference by dynamically assigning\ncomputation resources for each data instance. Automatically making decisions on\nwhere to skip and how to balance quality and computation cost with constrained\noptimization, our dynamic neural generation networks enforce the efficient\ninference path and determine the optimized trade-off. Experiments across\nquestion answering, summarization, and classification benchmarks show that our\nmethod benefits from less computation cost during inference while keeping the\nsame accuracy. Extensive experiments and ablation studies demonstrate that our\nmethod can be general, effective, and beneficial for many NLP tasks.",
        "updated": "2024-05-07 17:44:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04513v1"
    },
    {
        "title": "Toward In-Context Teaching: Adapting Examples to Students' Misconceptions",
        "authors": "Alexis RossJacob Andreas",
        "links": "http://arxiv.org/abs/2405.04495v1",
        "entry_id": "http://arxiv.org/abs/2405.04495v1",
        "pdf_url": "http://arxiv.org/pdf/2405.04495v1",
        "summary": "When a teacher provides examples for a student to study, these examples must\nbe informative, enabling a student to progress from their current state toward\na target concept or skill. Good teachers must therefore simultaneously infer\nwhat students already know and adapt their teaching to students' changing state\nof knowledge. There is increasing interest in using computational models,\nparticularly large language models, as pedagogical tools. As students, language\nmodels in particular have shown a remarkable ability to adapt to new tasks\ngiven small numbers of examples. But how effectively can these models adapt as\nteachers to students of different types? To study this question, we introduce a\nsuite of models and evaluation methods we call AdapT. AdapT has two components:\n(1) a collection of simulated Bayesian student models that can be used for\nevaluation of automated teaching methods; (2) a platform for evaluation with\nhuman students, to characterize the real-world effectiveness of these methods.\nWe additionally introduce (3) AToM, a new probabilistic model for adaptive\nteaching that jointly infers students' past beliefs and optimizes for the\ncorrectness of future beliefs. In evaluations of simulated students across\nthree learning domains (fraction arithmetic, English morphology, function\nlearning), AToM systematically outperforms LLM-based and standard Bayesian\nteaching models. In human experiments, both AToM and LLMs outperform\nnon-adaptive random example selection. Our results highlight both the\ndifficulty of the adaptive teaching task and the potential of learned adaptive\nmodels for solving it.",
        "updated": "2024-05-07 17:05:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.04495v1"
    }
]