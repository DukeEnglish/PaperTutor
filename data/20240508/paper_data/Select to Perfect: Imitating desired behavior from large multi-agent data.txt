PublishedasaconferencepaperatICLR2024
SELECT TO PERFECT: IMITATING DESIRED BEHAVIOR
FROM LARGE MULTI-AGENT DATA
TimFranzmeyer∗ EdithElkind PhilipTorr JakobFoerster† Joa˜oF.Henriques†
UniversityofOxford
ABSTRACT
AIagentsarecommonlytrainedwithlargedatasetsofdemonstrationsofhuman
behavior. However,notallbehaviorsareequallysafeordesirable. Desiredchar-
acteristicsforanAIagentcanbeexpressedbyassigningdesirabilityscores,which
weassumearenotassignedtoindividualbehaviorsbuttocollectivetrajectories.
Forexample, inadatasetofvehicleinteractions, thesescoresmightrelatetothe
number of incidents that occurred. We first assess the effect of each individual
agent’sbehavioronthecollectivedesirabilityscore,e.g.,assessinghowlikelyan
agent is to cause incidents. This allows us to selectively imitate agents with a
positiveeffect,e.g.,onlyimitatingagentsthatareunlikelytocauseincidents. To
enable this, we propose the concept of an agent’s Exchange Value, which quan-
tifies an individual agent’s contribution to the collective desirability score. The
Exchange Value is the expected change in desirability score when substituting
the agent for a randomly selected agent. We propose additional methods for es-
timating Exchange Values from real-world datasets, enabling us to learn desired
imitationpoliciesthatoutperformrelevantbaselines. Theprojectwebsitecanbe
foundathttps://tinyurl.com/select-to-perfect.
1 INTRODUCTION
Imitating human behaviors from large datasets is a promising technique for achieving human-AI
andAI-AIinteractionsincomplexenvironments(Carrolletal.,2019;,FAIR;Heetal.,2023;Shih
etal.,2022). However,suchlargedatasetscancontainundesirablehumanbehaviors,makingdirect
imitation problematic. Rather than imitating all behaviors, it may be preferable to ensure that AI
agentsimitatebehaviorsthatalignwithpredefineddesirablecharacteristics.Inthiswork,weassume
that desirable characteristics are quantified as desirability scores given for each trajectory in the
dataset. This is commonly the case when the evaluation of the desirability of individual actions
is impractical or too expensive (Stiennon et al., 2020). Assigning desirability scores to collective
trajectories may be the only viable option for complex datasets that involve multiple interacting
agents. For instance, determining individual player contributions in a football match is difficult,
whilethefinalscoreisareadily-availablemeasureofteamperformance.
We develop an imitation learning method for multi-agent datasets that ensures alignment with de-
sirablecharacteristics–expressedthroughaDesiredValueFunction1 (DVF)thatassignsascoreto
eachcollectivetrajectory. Thisscenarioisapplicabletoseveralareasthatinvolvelearningbehavior
fromdataofhumangroups. Oneexampleisadatasetofvehicleinteractions,desirabilityscoresin-
dicatingthenumberofincidentsinacollectivetrajectory,andtheaimtoimitateonlybehaviorthat
is unlikely to result in incidents (e.g., aiming to imitate driving with foresight). Similarly – given
adatasetofsocialmediaconversationthreadsanddesirabilityscoresthatindicatewhetherathread
has gone awry – one may want to only imitate behavior that reduces the chance of conversations
goingawryChang&Danescu-Niculescu-Mizil(2019).
∗frtim@robots.ox.ac.uk †equalsupervision
1TheDVFitselfisnotsufficienttodescribedesiredbehaviorcompletely,asitpossiblyonlycoversasubset
ofbehavior,e.g.,safety-relevantaspects.Itiscomplementarytothemorecomplexandnuancedbehaviorsthat
areobtainedbyimitatinghumandemonstrations,providingguardrailsoradditionalguidance.
1
4202
yaM
6
]GL.sc[
1v53730.5042:viXraPublishedasaconferencepaperatICLR2024
Given: Dataset of collective Step 1: Compute Exchange Value (EV) of all agents from DVF scores
trajectories and desirability scores EV( ) = DVF( ; ; ) - DVF( ; ; )
:(#,% %,#,…) DVF( )=0 Step 2: Rank agents by computed Exchange Values
:(#,% %,#,…) DVF( )=-1 EV( ) > EV( ) > 0 > EV( ) > EV( )
:(#,% %,#,…) DVF( )=-5 Step 3: Only imitate behaviorof agents with high Exchange Values ( , )
… π∗∈ImitationLearning(s,aa,s,aa,s,aa,s,…)
Figure1: Wearegivenadatasetcomposedofmulti-agenttrajectoriesgeneratedbymanyindividual
agents,e.g.,adatasetofcarsdrivinginurbanenvironments. Inaddition,theDesiredValueFunction
(DVF) indicates the desirability score of a collective trajectory, e.g., the number of incidents that
occurred. WefirstcomputetheExchangeValue(EV)ofeachagent,whereapositiveEVindicates
thatanagentincreasesthedesirabilityscore(e.g. anagentdrivingsafely). Wereformulateimitation
learningtotakeintoaccountthecomputedEVs,andachieveanimitationpolicythatisalignedwith
theDVF(e.g. onlyimitatingthebehaviorofsafedrivers).
Assessingthedesirabilityofanindividualagent’sbehaviorinvolvesgaugingitsimpactonthecol-
lectivedesirabilityscore. Forinstance, itrequiresevaluatingwhetheradriver’sbehaviorincreases
thelikelihoodofaccidents, orwhetherauser’sbehaviorincreasesthelikelihoodofaconversation
going awry. This is termed the credit assignment problem (Shapley, 1953), akin to fairly dividing
the value produced by a group of players among the players themselves. The credit assignment
problemprovescomplexinreal-worldscenariosduetothreemainfactors(seeFigure2fordetails):
First,manyscenariosonlypermitspecificgroupsizes.ThismakesShapleyValues(Shapley,1953)
– a concept commonly used in Economics for credit assignment – inapplicable, as it relies on the
comparisonsofgroupsofdifferentsizes(e.g.,ShapleyValuesarenotapplicabletofootballplayers,
asfootballisagameof11playersandagroupof12isneverobserved.) Second,real-worlddatasets
for large groups are almost always incomplete, i.e., they do not contain trajectories for all (com-
binatorially many) possible groups of agents. Third, datasets of human interactions may be fully
anonymizedbyassigningone-time-useIDs. Inthiscase,ifanagentispresentintwotrajectories,it
willappearinthedatasetasifitistwodifferentagents,makingthecreditassignmentproblemde-
generate. Thisrequiresincorporatingindividualbehaviorinformationinadditiontotheinformation
aboutcollectiveoutcomes.
To address these challenges, we propose Exchange Values (EVs), akin to Shapley Values, which
quantifyanagent’scontributionastheexpectedchangeindesirabilitywhensubstitutingtheagent
randomly. TheEVofanagentcanbeunderstoodastheexpectedchangeinvaluewhensubstituting
the agent with another randomly selected agent – or as comparing the average value of all groups
that include the agent to that of all groups not including the agent (see Step 1 in Figure 1). EVs
areapplicabletoscenarioswithfixedgroupsizes, makingthemmoreversatile. WeintroduceEV-
Clustering that estimates EVs from incomplete datasets by maximizing inter-cluster variance. We
show a theoretical connection to clustering by unobserved individual contributions and adapt this
methodtofully-anonymizeddatasets,byconsideringlow-levelbehavioralinformation.
WeintroduceExchangeValuebasedBehaviorCloning(EV2BC),whichimitateslargedatasetsby
onlyimitatingthebehaviorofselectedagentswithEVshigherthanatuneablethreshold(seeFig-
ure 1). This approach allows learning from interactions with agents with all behaviors, without
necessarily imitating them. This is not possible when simply excluding all trajectories with a low
collectivedesirabilityscore,i.e.,selectivelyimitatingbasedoncollectivescoresinsteadofindividual
contributions.WefindthatEV2BCoutperformsstandardbehaviorcloning,offlineRL,andselective
imitationbasedoncollectivescoresinchallengingenvironments,suchastheStarCraftMulti-Agent
Challenge(Samvelyanetal.,2019). Ourworkmakesthefollowingcontributions:
• WeintroduceExchangeValues(Def.4.1)tocomputeanagent’sindividualcontributiontoacol-
lectivevaluefunctionandshowtheirrelationtoShapleyValues.
• WeproposeEV-Clustering(Def.4.4)toestimatecontributionsfromincompletedatasetsandshow
atheoreticalconnectiontoclusteringagentsbytheirunobservedindividualcontributions.
• WeempiricallydemonstratehowEVscanbeestimatedfromfully-anonymizeddataandemploy
EV2BC(Def.4.5)tolearnpoliciesalignedwiththeDVF,outperformingrelevantbaselines.
2PublishedasaconferencepaperatICLR2024
Dataset characteristics Shapley Exchange
Example observation dataset Exchange Value
Example Scenario All S G izr eo sup A vll a O tiobs ne sr- IdK A en ngo te iw n titn e s of agents , , , … ApV pa ll iu cae bs leApV pa ll iu cae bs le Computation
DVF( ) DVF( ) DVF( ) …
Ideal
All group sizes are permitted. ✓ ✓ ✓ DVF( ) DVF( ) DVF( ) … ✓ ✓ Exact
All possible combinations of agents (all DVF( ) …
groups) are observed.
…
Group-Limited
Only specific group sizes are permitted. ✗ ✓ ✓ DVF( ) DVF( ) DVF( ) … ✗ ✓ Exact
A football game has 11 players.
Low-Data
Not all permitted groups are observed. ✗ ✗ ✓ DVF( ) DVF( ) … ✗ ✓ E (ps oti tm ena tt ie ad lly with
Two football players might never play for EV-Clustering)
the same team.
D Aneg oe nn ye mr ia zt ee d with one-time-use IDs. ✗ ✗ ✗ DVF( ) Behaviour clusteriD ngVF( ) … ✗ ✓ E w ans it t di h m b E ea V hte a-d vC il ou us rt e r ing
New ID for a player in each game played. DVF( ) DVF( ) … information from !
Figure2: Overviewofdifferentcharacteristicsofreal-worlddatasets,andwhetherShapleyValues
andExchangeValues(EVs)areapplicabletocomputecontributionsofindividualagentstotheDVF.
2 RELATED WORK
Most previous work on aligning AI agents’ policies with desired value functions either relies on
simplehand-craftedrules(Xuetal.,2020;,FAIR),whichdonotscaletocomplexenvironments,or
performspostprocessingofimitationpolicieswithfine-tuning(Stiennonetal.,2020;Ouyangetal.,
2022;Glaeseetal.,2022;Baietal.,2022),whichrequiresaccesstotheenvironmentorasimulator.
In language modeling, Korbak et al. (2023) showed that accounting for the alignment of behavior
withtheDVFalreadyduringimitationlearningyieldsresultssuperiortofine-tuningafter-the-fact,
however,theirapproachconsidersanagent-specificvaluefunction.Incontrast,weconsiderlearning
apolicyalignedwithacollectivevaluefunction,andfromofflinedataalone.
Creditassignmentinmulti-agentsystemswasinitiallystudiedinEconomics(Shapley,1953). Sub-
sequently, Shapley Values (Shapley, 1953) and related concepts have been applied in multi-agent
reinforcement learning to distribute rewards among individual agents during the learning pro-
cess (Chang et al., 2003; Foerster et al., 2018; Nguyen et al., 2018; Wang et al., 2020; Li et al.,
2021;Wangetal.,2022). Outsideofpolicylearning,Heuilletetal.(2022)usedShapleyValuesto
analyzeagentcontributionsinmulti-agentenvironments,howeverthisrequiresprivilegedaccessto
asimulator,inordertoreplaceagentswithrandomly-actingagents. IncontrasttoShapleyValues,
theapplicabilityofEVstoallgroupsizesallowsustoomittheneedtosimulateinfeasiblecoalitions.
Incontrasttothiswork,existingworkinmulti-agentimitationlearningtypicallyassumesobserva-
tions to be generated by optimal agents, as well as simulator access (Le et al., 2017; Song et al.,
2018;Yuetal.,2019). Similartoourframework,offlinemulti-agentreinforcementlearning(Jiang
&Lu,2021;Tsengetal.,2022;Tianetal.,2022)involvespolicylearningfrommulti-agentdemon-
strationsusingofflinedataalone,however,itassumesadenserewardsignaltobegiven,whilethe
DVFassignsasinglescorepercollectivetrajectory.
Insingle-agentsettings,alargebodyofworkinvestigatesestimatingdemonstratorexpertisetoen-
hance imitation learning (Chen et al., 2021; Zhang et al., 2021; Cao & Sadigh, 2021; Sasaki &
Yamashina,2021;Beliaevetal.,2022;Yangetal.,2021). However,thesemethodsdonottranslate
tothemulti-agentsettingduetothechallengeofcreditassignment.
To the best of our knowledge, no prior work has considered the problem of imitating multi-agent
datasetscontainingmixedbehaviors,whileensuringalignmentwithacollectivevaluefunction.
3 BACKGROUND AND NOTATION
MarkovGame. WeconsiderMarkovGames(Littman,1994),whichgeneralizeMarkovDecision
Processes(MDPs)tomulti-agentscenarios. InaMarkovGame,agentsinteractinacommonenvi-
ronment.Attimestept,eachagent(theithofatotalofmagents)takestheactionatandtheenviron-
i
3PublishedasaconferencepaperatICLR2024
menttransitionsfromstatesttost+1. AreducedMarkovgame(withoutrewards)isthendefinedby
astatespace (st ),adistributionofinitialstatesη,theactionspace (at )ofeachagent
i,anenvironmS ents∈ taS tetransitionprobabilityP(st+1 st,a ,...,a )andA ti heei p∈ isoA di elengthT. We
1 m
|
denotethisMarkovGameas =( , ,P,T),withcollectivetrajectoriesτ =(s ,a ,...,s ).
0 0 T
M S A
Setofmulti-agentdemonstrationsgeneratedbymanyagents. WeconsideraMarkovgame
M
of m agents and a set of demonstrator agents N = 1,...,n where n m. The Markov Game
{ } ≥
is further assumed to be symmetric (we can change the ordering of players without changing the
game). Thedemonstrationset capturesinteractionsamongvariousgroupsofagentsin . Every
D M
entry = (s ,τ )containsatrajectoryτ foragroupofagentss N. Notably, τ contains
Di i si si i
⊆
si
thecollectivetrajectoryofallagentsinthegroups .
i
Shapley Values. We now define the concept of the Shapley Value of an agent (Shapley, 1953),
which is commonly used to evaluate the contributions of individual agents to a collective value
functioninacharacteristicfunctiongame.Definition3.2belowissomewhatunconventionalbutcan
beeasilyseentobeequivalenttothestandarddefinition.
Definition3.1(Characteristicfunctiongame). AcharacteristicfunctiongameGisgivenbyapair
(N,v),whereN = 1,...,n isafinite,non-emptysetofagentsandv :2N Risacharacteristic
{ } →
function,whichmapseachgroup(sometimesalsoreferredtoascoalition)C N toarealnumber
⊆
v(C);itisassumedthatv( )=0. Thenumberv(C)isreferredtoasthevalueofthegroupC.
∅
Given a characteristic function game G = (N,v), let Π denote the set of all permutations
N\{i}
ofN i , i.e., one-to-onemappingsfromN i toitself. Foreachpermutationπ Π , we
N\{i}
\{ } \{ } ∈
denotebyS (m)thesliceofπ upuntilandincludingpositionm;wethinkofS (m)asthesetof
π π
allagentsthatappearinthefirstmpositionsinπ(notethatS (0)= ). Themarginalcontribution
π
∅
ofanagentiwithrespecttoapermutationπandasliceminagameG=(N,v)isgivenby
∆G (i)=v(S (m) i ) v(S (m)).
m,π π ∪{ } − π
Thisquantitymeasurestheincreaseinthevalueofthegroupwhenagentijoinsthem. Wecannow
definetheShapleyValueofanagenti: itissimplytheagent’saveragemarginalcontribution,where
theaverageistakenoverallpermutationsofthesetofallotheragentsN i andallslices.
\{ }
Definition 3.2 (Shapley Value). Given a characteristic function game G = (N,v) with N = n,
| |
theShapleyValueofanagenti N isdenotedbySV (G)andisgivenby
i
∈
SV i(G)=1/n! ·(cid:80)n m− =1 0(cid:80) π∈ΠN\{i}∆G m,π(i). (1)
Def. 3.2 is important in the context of credit assignment, as a possible solution for distributing
collectivevaluetoindividualagents. Italsohasseveralconsistencyproperties(Shapley,1953).
4 METHODS
Problemsetting. Givenadataset oftrajectoriesgeneratedbygroupsofinteractingagentsand
D
a Desired Value Function (DVF), the goal of our work is to learn an imitation policy for a single
agentthatisalignedwiththeDVF.Weassumethatafractionofthedemonstratoragents’behavior
is undesirable; specifically, their presence in a group significantly reduces the DVF. Further, we
assumethatthenumberofdemonstratoragentsismuchlargerthanthegroupsize.
Overview of the methods section. To evaluate agents’ contributions in games that only permit
specificgroupsizes,wefirstdefinetheconceptofEVs(Def.4.1)forregularcharacteristicfunction
games (Def. 3.1). We then show that our definition extends naturally to characteristic function
gameswithconstraintsonpermittedgroupsizes. WefinallyderivemethodstoestimateEVsfrom
real-worlddatasetswithlimitedobservations(seeFigure2foranoverview).
4.1 EXCHANGEVALUESTOEVALUATEAGENTS’INDIVIDUALCONTRIBUTIONS
Note that each term of the Shapley Value, denoted ∆G (i), requires computing the difference in
m,π
valuesbetweentwogroupsofdifferentsizes,withandwithoutanagenti(seeDef. 3.2). Ifwewish
4PublishedasaconferencepaperatICLR2024
toonlycomparegroupswiththesamesize,thenanaturalalternativeistocomputethedifferencein
valueswhentheagentatpositionmisreplacedwithagenti:
ΓG (i)=v(S (m 1) i ) v(S (m)). (2)
m,π π − ∪{ } − π
Wecallthisquantitytheexchangecontributionofi,givenapermutationofagentsπslicedatposi-
tionm. Itrepresentstheaddedvalueofagentiinagroup. Importantlyitdoesnotrequirevaluesof
groupsofdifferentsizes.
WenowdefinetheEVanalogouslytotheShapleyValueastheaverageexchangecontributionover
allpermutationsofN i andallnon-emptyslices.
\{ }
Definition4.1(ExchangeValue). GivenacharacteristicfunctiongameG = (N,v)with N = n,
| |
theExchangeValueofanagenti N isdenotedbyEV (G)andisgivenby
i
∈
EV (G)=((n 1)! (n 1))−1 (cid:80)n−1 (cid:80) ΓG (i). (3)
i − · − · m=1 π∈ΠN\{i} m,π
Inwords,theEVofanagentcanhencebeunderstoodastheexpectedchangeinvaluewhensubsti-
tutingtheagentwithanotherrandomlyselectedagent,orascomparingthevalueofallgroupsthat
includetheagenttothatofallgroupsthatdonotincludetheagent(seeStep1inFigure1).
RelationshipbetweenShapleyValueandExchangeValue. ItcanbeshownthattheExchange
Values of all agents can be derived from their Shapley Values by a simple linear transformation:
we subtract a fraction of the value of the grand coalition N (group of all agents) and scale the
result by n/n−1: EV i(G) = n−n 1(SV i(G)
−
1/n
·
v(N)). The proof proceeds by computing the
coefficient of each term v(C), C N, in summations (1) and (3) (see Appendix A). Hence, the
⊆
Shapley Value and the Exchange Value order the agents in the same way. Now, recall that the
ShapleyValueischaracterizedbyfouraxioms,namely,dummy,efficiency,symmetry,andlinearity
(Shapley,1953).ThelattertwoarealsosatisfiedbytheExchangeValue:ifv(C i )=v(C j )
∪{ } ∪{ }
for all C N i,j , we have EV (G) = EV (G) (symmetry), and if we have two games G
i j 1
⊆ \{ }
andG withcharacteristicfunctionsv andv overthesamesetofagentsN,thenforthecombined
2 1 2
game G = (N,v) with the characteristic function v given by v(C) = v (C)+v (C) we have
1 2
EV (G) = EV (G ) + EV (G ) (linearity). The efficiency property of the Shapley Value, i.e.,
i i 1 i 2
(cid:80) (cid:80)
SV (G) = v(N) implies that EV (G) = 0. In words, the sum of all agents’ EV is
i∈N i i∈N i
zero.Thedummyaxiom,too,needstobemodified:ifanagentiisadummy,i.e.,v(C i )=v(C)
∪{ }
for every C N i then for the Shapley value we have SV (G) = 0 and hence EV (G) =
i i
⊆ \{ }
1/n−1 v(N),Ineachcase,theprooffollowsfromtherelationshipbetweentheShapleyValueand
− ·
theExchangeValueandthefactthattheShapleyValuesatisfiestheseaxioms(seeAppendixA).
4.1.1 COMPUTINGEXCHANGEVALUESIFONLYCERTAINGROUPSIZESAREPERMITTED
Foracharacteristicfunctiongame =(N,v)thevaluefunctionvcanbeevaluatedforeachpossible
G
groupC N. Wenowconsiderthecasewherethevaluefunctionv isonlydefinedforgroupsof
⊆
certainsizesm M,i.e. visonlydefinedforasubsetofgroupsofcertainsizes.
∈
Definition 4.2 (Constrained characteristic function game). A constrained characteristic function
gameG¯ isgivenbyatuple(N,v,M),whereN = 1,...,n isafinite,non-emptysetofagents,
M 0,...,n 1 is a set of feasible group size{ s and v } : C 2N : C M R is a
⊆ { − } { ∈ | | ∈ } →
characteristicfunction,whichmapseachgroupC N ofsize C M toarealnumberv(C).
⊆ | |∈
NotethatboththeShapleyValueandtheEVaregenerallyundefinedforconstrainedcharacteristic
functiongames,asthevaluefunctionisnotdefinedforgroupsC ofsize C / M. Thedefinition
| | ∈
of the Shapley Value cannot easily be adapted to constrained characteristic function games, as its
computationrequiresevaluatingvaluesofgroupsofdifferentsizes. Incontrast,thedefinitionofthe
EV can be straightforwardly adapted to constrained characteristic function games by limiting the
summation to slices of size m M+, where M+ = m M : m > 0 . Hence, we define the
∈ { ∈ }
ConstrainedEVastheaverageexchangecontributionoverallpermutationsofN i andoverall
slicesofsizem M+. \{ }
∈
Definition 4.3 (Constrained Exchange Value). Given a constrained characteristic function game
G¯ = (N,v,M) with N = n, the Constrained Exchange Value of an agent i N is denoted by
EV (G¯)andisgivenb| yE| V (G¯)=((n 1)! M+ )−1 (cid:80) (cid:80) ∈ ΓG¯ (i).
i i − ·| | · m∈M+ π∈ΠN\{i} m,π
5PublishedasaconferencepaperatICLR2024
WerefertotheConstrainedEVandEVinterchangeably,astheyareapplicabletodifferentsettings.
Ifsomegroupsarenotobserved,wecanachieveanunbiasedestimateoftheEVbysamplinggroups
uniformly at random. The expected EV is EV (G¯) = E (cid:2) ΓG¯ (i)(cid:3) . This
i m∼U(M+),π∼U(ΠN\{i}) m,π
expectationconvergestothetrueEVinthelimitofinfinitesamples.
AsoutlinedinStep1inFigure1, theEVofanagentisacomparisonofthevalueofagroupthat
includestheagentandagroupthatdoesnotincludetheagent,consideringallpermittedgroupsizes.
4.2 ESTIMATINGEXCHANGEVALUESFROMLIMITEDDATA
TheEVassessesthecontributionofanindividualagentandisapplicableundergroupsizelimita-
tionsinreal-worldscenarios(seeGroup-LimitedinFigure2).However,exactlycalculatingEVs
isalmostalwaysimpossibleasreal-worlddatasetslikelydonotcontainobservationsforall(combi-
natoriallymany)possiblegroups(Low-DatainFigure2).Wefirstshowasampling-basedestimate
(Section4.2)ofEVs,whichmayhaveahighvarianceforEVsofagentsthatarepartofonlyafew
trajectories(outcomes). Next, weintroduceanovelmethod, EV-Clustering(Section4.2.1), which
clustersandcanbeusedtoreducethevariance. Whendatasetsareanonymizedwithone-time-use
IDs,eachdemonstratorisonlyobservedaspartofonegroup(seeDegenerateinFigure2),ren-
deringcreditassignmentdegenerate,asexplainedinSection4.2.1.Weaddressthisbyincorporating
low-levelbehaviordatafromthetrajectoriesτ.
4.2.1 EV-CLUSTERINGIDENTIFIESSIMILARAGENTS
Inthecaseofveryfewagentobservations,theabove-introducedsamplingestimatehasahighvari-
ance.Onewaytoreducethevarianceisbyclustering:ifweknewthatsomeagentstendtocontribute
similarlytotheDVF,thenclusteringthemandestimatingoneEVpercluster(insteadofoneEVper
agent)willusemoresamplesandtherebyreducethevariance. Notethat, asourfocusisonaccu-
rately estimating EVs, we do not consider clustering agents by behavior here, as two agents may
exhibitdistinctbehaviorswhilestillcontributingequallytotheDVF.
We propose EV-Clustering, which clusters agents such that the variance in EVs across all agents
is maximized. In Appendix A we show that EV-Clustering is equivalent to clustering agents by
their unobserved individual contribution, under the approximation that the total value of a group
isthesumoftheparticipatingagents’individualcontributions,anassumptionfrequentlymadefor
theoreticalanalysis(Lundberg&Lee,2017;Covert&Lee,2021),asitrepresentsthesimplestnon-
trivial class of cooperative games. Intuitively, if we choose clusters that maximize the variance in
EVsacrossallagents,allclusters’EVswillbemaximallydistinct. Anexampleofpoorclustering
isarandompartition,whichwillhaveverysimilarEVsacrossclusters(havinglowvariance).
Specifically, we assign n agents to k n clusters K = 1,...,k 1 , with individual cluster
≤ { − }
assignments u = u ,...,u , where u K. We first combine the observations of all agents
0 n−1 i
{ } ∈
withinthesameclusterbydefiningaclusteredvaluefunctionv˜(C)thatassignsavaluetoagroup
of cluster-centroid agents C K by averaging over the combined observations, as v˜(C) = 1/η
(cid:80)n−1 (cid:80)
v(S (m))
1⊆
( u j S (m) = C), whereη isanormalizationconstant.
The·
m=0 π∈ΠN π · { j | ∈ π }
EV of an agent i is then given as EV (G˜), with G˜ = (K,v˜), thereby assigning equal EVs to all
i
agentswithinonecluster.
Definition4.4(EV-Clustering). Wedefinetheoptimalclusterassignmentsu∗suchthatthevariance
inEVsacrossallagentsismaximized:
u∗ argmax Var([EV (G˜),...,EV (G˜)]). (4)
∈ u 0 n−1
WeshowinAppendixB.1thatthisobjectiveisequivalenttoclusteringagentsbytheirunobserved
individualcontributions,undertheapproximationofanadditivevaluefunction.
4.2.2 DEGENERACYOFTHECREDITASSIGNMENTPROBLEMFORFULLY-ANONYMIZEDDATA
Iftwoagentsareobservedonlyonceinthedatasetandaspartofthesamegroup,equalcreditmust
beassignedtobothduetotheinabilitytoseparatetheircontributions. Analogously,whenallagents
areonlyobservedonce,creditcanonlybeassignedtogroups,resultinginthedegeneratescenario
6PublishedasaconferencepaperatICLR2024
Relation between [%] of observed data and EV estimation error
Cramped Room 0.75
Coordination Ring
Table 1: Resulting performance with re- 0.02 ToC - vfinal
spect to the DVF for different imitation ToC - vtotal 0.50
ToC - vmin
learningmethodsindifferentStarcraftsce-
0.01 narios. 0.25
Method 2s3z 3svs5z 6hvs8z 50% 30% 20% 10% 5% 3% Deg. Deg. clust.
BC 12.14 1.8 13.10 2.0 8.56 0.6
Group-BC 15.41±2.4 16.63±1.9 9.10±0.9 Figure3:MeanerrorinestimatingEVswithdecreas-
EV2BC(Ours) 17.38±1.6 20.31±2.4 10.0 ±0.91 ingnumberofobservations. ‘Deg.’ referstothefully
± ± ±
anonymizeddegeneratecase. Errordecreasessignif-
icantlyifagentsareclustered(green-shadedarea).
thatallagentsinagroupareassignedthesamecredit(e.g. areassignedequalEV).Wesolvethisby
combininglow-levelbehaviorinformationfromtrajectoriesτ withEV-Clustering(seeSec.5.1).
4.3 EXCHANGEVALUEBASEDBEHAVIORCLONING(EV2BC)
HavingdefinedtheEVofanindividualagentanddifferentmethodstoestimateit,wenowdefinea
variationofBehaviorCloning(Pomerleau,1991),whichtakesintoaccounteachagent’scontribution
to the desirability value function (DVF). We refer to this method as EV2BC. Essentially, EV2BC
imitatesonlyactionsofselectedagentsthathaveanEVlargerthanatunablethresholdparameter.
Definition 4.5 (EV based Behavior Cloning (EV2BC)). For a set of demonstrator agents N, a
dataset ,andaDVF,wedefinetheimitationlearninglossforEV2BCas
D
L (θ)= (cid:80) (cid:80) log(πθ(an s )) 1(EVDVF >c) (5)
EV2BC − n∈N (si,an i)∈D i| i · n
whereEVDVFdenotestheEVofagentnandwherecisatunablethresholdparameterthattrades
n
off between including data of agents with higher contributions to the DVF and reducing the total
amountoftrainingdataused.
5 EXPERIMENTS
Theenvironmentsthatweconsideronlypermitcertaingroupsizes,henceweuseconstrainedEVs
(seeDef.4.3).Werunallexperimentsforfiverandomseedsandreportmeanandstandarddeviation
where applicable. For more details on the implementation, please refer to the Appendix. In the
followingexperiments,wefirstevaluateEVsasameasureofanagent’scontributiontoagivenDVF.
WethenassesstheaverageestimationerrorforEVsasthenumberofobservationsinthedatasetD
decreasesandhowapplyingclusteringdecreasesthiserror. Welastlyevaluatetheperformanceof
Exchange Value based Behaviour Cloning (EV2BC, see Definition 4.5) for simulated and human
datasets and compare to relevant baselines, such as standard Behavior Cloning (Pomerleau, 1991)
andOfflineReinforcementLearning(Panetal.,2022).
InTragedyoftheCommons(Hardin,1968)(ToC)multipleindividualsdepleteasharedresource.It
isasocialdilemmascenariooftenstudiedtomodeltheoverexploitationofcommonresources(Di-
etz et al., 2003; Ostrom, 2009). We model ToC as a multi-agent environment and consider three
DVFsrepresentingdifferentmeasuresofsocialwelfare: thefinalpoolsizev ,thetotalresources
final
consumedv ,andtheminimumconsumptionamongagentsv .
total min
Overcooked(Carrolletal.,2019)isatwo-playergamesimulatingacooperativecookingtaskrequir-
ingcoordinationandisacommontestbedinmulti-agentresearch. WithinOvercooked,weconsider
the configurations Cramped Room and Coordination Ring (displayed in Figure 4). For each envi-
ronmentconfiguration,wegeneratetwodatasetsbysimulatingagentbehaviorsusinganear-optimal
planningalgorithm,whereweuseaparameterλtodetermineanagent’sbehavior. Forλ=1agents
act(near)-optimal,forλ= 1agentsactadversarially.Werefertoλastheagent’strait,asitactsas
−
aproxyfortheagent’sindividualcontributiontothecollectivevaluefunction. Eachdemonstration
dataset D is generated by n = 100 agents, and trajectories τ are of length 400. The adversarial
datasetDadv iscomprisedof25%adversarialagentswithλ = 1and75%(near)-optimalagents
−
7
rorrE
VE
-
dekoocrevO
rorrE
VE
- CoTPublishedasaconferencepaperatICLR2024
withλ = 1,whileforthedatasetDλ agentswereuniformlysampledbetweenλ = 1andλ = 1.
The Dhuman dataset was collected from humans playing the game (see Carroll et al− . (2019)); it is
fully anonymized with one-time-use agent identifiers, hence is a degenerate dataset (see Figure 2
bottom row). We consider the standard value function given for Overcooked as the DVF, i.e. the
numberofsoupspreparedbybothagentsoverthecourseofatrajectory.
The StarCraft Multi-Agent Challenge (Samvelyan et al., 2019) is a cooperative multi-agent en-
vironment that is partially observable, involves long-term planning, requires strong coordination,
andisheterogeneous. Weconsiderthesettings2s3z,3s_vs_5zand6h_vs_8z,whichinvolve
teamsof3-6agents. Foreach,wegenerateapoolof200agentswithvaryingcapabilitiesbyextract-
ingpoliciesatdifferentepochs, andfromtrainingwithdifferentseeds. Wegenerateadatasetthat
containssimulatedtrajectoriesof100randomlysampledgroups(outof 109 possiblegroups)and
usetheenvironment’sgroundtruthrewardfunctiontoassignDVFscoresaccordingtothecollective
performanceofagents.
ExchangeValuesassessanagent’sindividualcontributiontoacollectivevaluefunction. To
analyzeEVsasameasureforanagent’sindividualcontributiontoaDVF,weconsiderfulldatasets
that contain demonstrations of all possible groups, which allows us to estimate EVs accurately.
In ToC, we find that the ordering of agents broadly reflects our intuition: Taking more resources
negativelyimpactstheEVs,andagentsconsumingtheaverageofothershavelessextremeEVs.The
color-codedorderingofagentsunderdifferentDVFsinshowninFigure7inApp.C.InOvercooked,
weconsiderthetwosimulateddatasets(Dadv andDλ)butnotthehumandataset,astheindividual
contributionisunknownforhumanparticipants. WefindthatEVsofindividualagentsarestrongly
correlatedwiththeirtraitparameterλ,whichisaproxyfortheagent’sindividualcontribution,and
provideaplotthatshowstherelationshipbetweenλandEVinFigue5inApp.B.
5.1 ESTIMATINGEVSFROMINCOMPLETEDATA
Estimationerrorfordifferentdatasetsizes. Wenowturntorealisticsettingswithmissingdata,
where EVs must be estimated (Sec. 4.2). For both ToC and Overcooked, we compute the mean
estimation error in EVs if only a fraction of the possible groups is contained in the dataset. As
expected,weobserveinFig.3thatthemeanestimationerrorincreasesasthefractionofobserved
groupsdecreases,withthelargestestimationerrorforfullyanonymizeddatasets(seeFig.3–Deg.).
Estimating EVs from degenerate datasets with EV-Clustering. To estimate EVs from degen-
eratedatasets, wefirstobtainbehaviorembeddingsfromthelow-levelbehaviorinformationgiven
inthetrajectoriesτ inD. Specifically,inOvercookedandToC,weconcatenateactionfrequencies
in frequently observed states. In Starcraft, we use TF-IDF (Spa¨rck Jones, 1972) to obtain behav-
iorembeddings. Wethencomputealargenumberofpossibleclusterassignmentsforthebehavior
embeddingsusingdifferentmethodsandhyperparameters. InaccordancewiththeobjectiveofEV-
Clustering, we choose the cluster assignment with the highest variance in EVs. We observe in
Figure3thatclusteringsignificantlydecreasestheestimationerror(seeDeg. clustered).
5.2 IMITATINGDESIREDBEHAVIORBYUTILIZINGEVS
WenowevaluateEV2BCinalldomains. Inaccordancewiththequantityofavailabledata,weset
the threshold parameter such that only agents with EVs above the 90th, 67th, and 50th percentile
areimitatedinToC,Starcraft,andOvercooked,respectively. Wereplicatethesingle-agentEV2BC
policyforallagentsintheenvironmentandevaluatetheachievedcollectiveDVFscore. Asbase-
lines,weconsider(1)BC,whereBehaviorCloning(Pomerleau,1991)isdonewiththefulldataset
withoutcorrectingforEVs,(2)offlinemulti-agentreinforcementOMAR(Panetal.,2022)withthe
rewardatthelasttimestepsettotheDVF’sscoreforagiventrajectory(noper-steprewardisgiven
bytheDVF)and(3)GroupBC,forwhichonlycollectivetrajectorieswithaDVFscorelargerthan
therelevantpercentileareincluded. WhileEV2BCisbasedonindividualagents’contributions,this
lastbaselineselectivelyimitatesdatabasedongroupoutcomes. Forinstance,ifacollectivetrajec-
toryincludestwoalignedagentsandoneunalignedagent,thelatterbaselineislikelytoimitateall
threeagents. Incontrast,ourapproachwouldonlyimitatethetwoalignedagents.
8PublishedasaconferencepaperatICLR2024
Table2: ResultingperformancewithrespecttotheDVFfordifferentimitationlearningmethodsin
theOvercookedenvironmentsCrampedRoom(top)andCoordinationRing(bottom). InTragedyof
Commons: 12agentsexperimentatthetop,120agentsexperimentatthebottom.
Overcooked TragedyofCommons
Imitationmethod Dλ Dadv Dhuman vfinal vtotal vmin
BC(Pomerleau,1991) 10.8 2.14 40.8 12.7 153.34 11.5 2693.6 139.1 50.6 2.4 0.45
Group-BC 54.2±5.45 64.8±7.62 163.34±6.08 5324.2±210.8 100.01 ±20.08 4.60± 1.01
OMAR(Panetal.,2022) 6.4±3.2 25.6± 8.9 12.5 ±4.5 -± -± ±-
EV2BC(ours) 91.6 ±12.07 104.2 ±10.28 170.89± 6.8 10576.8 307.4 342.8 49.36 44.2 6.4
± ± ± ± ± ±
BC(Pomerleau,1991) 15.43 4.48 10.4 6.8 104.89 12.44 2028.8 60.9 38.9 10.4 1.8 0.4
Group-BC 24 ±4.69 14.6 ±2.48 102.2±6.19 3400.5 ±100.9 77.1±14.1 3.51± 1.6
OMAR(Panetal.,2022) 12.43± 3.35 9.5±3.5 12.4±6.0 -± ±- -±
EV2BC(ours) 30.2 ±6.91 12.4±2.65 114.89± 5.08 8123.4 600.8 270.0 50.0 33.1 7.1
± ± ± ± ± ±
ToC results. We imitate datasets of 12 agents and 120 agents, with group sizes of 3 and 10, re-
spectively,evaluatingperformanceforeachofthethreeDVFsdefinedfortheToCenvironment. We
donotconsidertheOMARbaselineaspoliciesarenotlearnedbutrule-based. Table1demonstrates
thatEV2BCoutperformsthebaselinesbyalargemargin.
Overcooked results. We now consider all datasets Dadv, Dλ and Dhuman in both Overcooked
environments. WeevaluatetheperformanceachievedbyagentswithrespecttotheDVF(theenvi-
ronmentvaluefunctionofmaximizingthenumberofsoups)whentrainedwithdifferentimitation
learningapproachesonthedifferentdatasets. EVsarecomputedasdetailedinSection5.1. Table1
showsthatEV2BCclearlyoutperformsthebaselineapproaches. WefurthernotethatEV2BCsig-
nificantlyoutperformsbaselineapproachesonthedatasetsofhuman-generatedbehavior,forwhich
EVs were estimated from a fully-anonymized real-world dataset. This demonstrates that BC on
datasets containing unaligned behavior carries the risk of learning wrong behavior, but it can be
alleviatedbyweightingthesamplesusingestimatedEVs.
StarcraftResults. WeobserveinTable1thatEV2BCoutperformsthebaselinesbyasubstantial
margin,underliningtheapplicabilityofourmethodtolargerandmorecomplexsettings.Weomitted
the OMAR baseline, which is implemented as offline MARL with the DVF as the final-timestep
reward,asitperformedsignificantlyworsethanBC.
6 CONCLUSION
Our work presents a method for training AI agents from diverse datasets of human interactions
whileensuringthattheresultingpolicyisalignedwithagivendesirabilityvaluefunction. However,
itmustbenotedthatquantifyingthisvaluefunctionisanactiveresearcharea. ShapleyValuesand
ExchangeValuesestimatethealignmentofanindividualwithagroupvaluefunction(whichmustbe
prescribedseparately)and,assuch,canbemisusediftheyareincludedinalargersystemthatisused
to judge those individuals in any way. Discrimination of individuals based on protected attributes
is generally unlawful, and care must be taken to avoid any discrimination by automated means.
Wedemonstratedanovelpositiveuseofthesemethodsbyusingthemtotrainaligned(beneficial)
agents,thatdonotimitatenegativebehaviorsinadataset. Weexpectthatthebenefitsofaddressing
theproblemofunsafebehaviorbyAIagentsoutweighthedownsidesofmisuseofShapleyValues
andExchangeValues,whicharecoveredbyexistinglaws.
Futureworkmayaddresstheassumptionthatindividualagentsbehavesimilarlyacrossmultipletra-
jectoriesanddevelopmethodsforamorefine-grainedassessmentofdesiredbehavior. Additionally,
exploring how our framework can more effectively utilize data on undesired behavior is an inter-
esting avenue for further investigation, e.g., developing policies that are constrained to not taking
undesirableactions. Lastly,futureworkmayinvestigateapplicationstoreal-worlddomains,suchas
multi-agentautonomyscenarios.
Reproducibility. Tohelpreproduceourwork,wepublishcodeontheprojectwebsiteathttps:
//tinyurl.com/select-to-perfect. Weprovidedetailedoverviewsforallstepsoftheexper-
imental evaluation in the Appendix, where we also link to the publicly available code repositories
thatourworkused. Wefurtherprovideinformationaboutcomputationalcomplexityattheendof
theAppendix.
9PublishedasaconferencepaperatICLR2024
REFERENCES
YuntaoBai,AndyJones,KamalNdousse,AmandaAskell,AnnaChen,NovaDasSarma,DawnDrain,Stanislav
Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement
learningfromhumanfeedback. arXivpreprintarXiv:2204.05862,2022.
MarkBeliaev,AndyShih,StefanoErmon,DorsaSadigh,andRamtinPedarsani.Imitationlearningbyestimat-
ingexpertiseofdemonstrators. InInternationalConferenceonMachineLearning,pp.1732–1748.PMLR,
2022.
ZhangjieCaoandDorsaSadigh. Learningfromimperfectdemonstrationsfromagentswithvaryingdynamics.
IEEERoboticsandAutomationLetters,6(3):5231–5238,2021.
MicahCarroll,RohinShah,MarkKHo,TomGriffiths,SanjitSeshia,PieterAbbeel,andAncaDragan. On
theutilityoflearningabouthumansforhuman-aicoordination. Advancesinneuralinformationprocessing
systems,32,2019.
Jonathan P. Chang and Cristian Danescu-Niculescu-Mizil. Conversations gone awry dataset [reddit cmv
version]. https://convokit.cornell.edu/documentation/awry_cmv.html, 2019. Ac-
cessed:2024-03-14.
Yu-HanChang,TraceyHo,andLeslieKaelbling. Alllearningislocal: Multi-agentlearninginglobalreward
games. Advancesinneuralinformationprocessingsystems,16,2003.
Letian Chen, Rohan Paleja, and Matthew Gombolay. Learning from suboptimal demonstration via self-
supervisedrewardregression. InConferenceonrobotlearning,pp.1262–1277.PMLR,2021.
IanCovertandSu-InLee.Improvingkernelshap:Practicalshapleyvalueestimationusinglinearregression.In
InternationalConferenceonArtificialIntelligenceandStatistics,pp.3457–3465.PMLR,2021.
ThomasDietz,ElinorOstrom,andPaulCStern. Thestruggletogovernthecommons. science,302(5652):
1907–1912,2003.
Meta Fundamental AI Research Diplomacy Team (FAIR)†, Anton Bakhtin, Noam Brown, Emily Dinan,
GabrieleFarina,ColinFlaherty,DanielFried,AndrewGoff,JonathanGray,HengyuanHu,etal. Human-
levelplayinthegameofdiplomacybycombininglanguagemodelswithstrategicreasoning. Science,378
(6624):1067–1074,2022.
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counter-
factualmulti-agentpolicygradients. InProceedingsoftheAAAIconferenceonartificialintelligence,vol-
ume32,2018.
Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,
Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via
targetedhumanjudgements. arXivpreprintarXiv:2209.14375,2022.
GarrettHardin. Thetragedyofthecommons: thepopulationproblemhasnotechnicalsolution;itrequiresa
fundamentalextensioninmorality. science,162(3859):1243–1248,1968.
Jerry Zhi-Yang He, Zackory Erickson, Daniel S Brown, Aditi Raghunathan, and Anca Dragan. Learning
representationsthatenablegeneralizationinassistivetasks. InConferenceonRobotLearning,pp.2105–
2114.PMLR,2023.
AlexandreHeuillet,FabienCouthouis,andNataliaD´ıaz-Rodr´ıguez. Collectiveexplainableai: Explainingco-
operativestrategiesandagentcontributioninmultiagentreinforcementlearningwithShapleyvalues. IEEE
ComputationalIntelligenceMagazine,17(1):59–71,2022.
Jiechuan Jiang and Zongqing Lu. Offline decentralized multi-agent reinforcement learning. arXiv preprint
arXiv:2108.01832,2021.
TomaszKorbak,KejianShi,AngelicaChen,RasikaBhalerao,ChristopherLBuckley,JasonPhang,SamuelR
Bowman, and Ethan Perez. Pretraining language models with human preferences. arXiv preprint
arXiv:2302.08582,2023.
Dieter Kraft. A software package for sequential quadratic programming. Forschungsbericht- Deutsche
Forschungs-undVersuchsanstaltfurLuft-undRaumfahrt,1988.
Hoang M Le, Yisong Yue, Peter Carr, and Patrick Lucey. Coordinated multi-agent imitation learning. In
InternationalConferenceonMachineLearning,pp.1995–2003.PMLR,2017.
10PublishedasaconferencepaperatICLR2024
JiahuiLi,KunKuang,BaoxiangWang,FuruiLiu,LongChen,FeiWu,andJunXiao. Shapleycounterfactual
creditsformulti-agentreinforcementlearning. InProceedingsofthe27thACMSIGKDDConferenceon
KnowledgeDiscovery&DataMining,pp.934–942,2021.
Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine
LearningProceedings1994.1994. doi:10.1016/b978-1-55860-335-6.50027-1.
ScottMLundbergandSu-InLee. Aunifiedapproachtointerpretingmodelpredictions. Advancesinneural
informationprocessingsystems,30,2017.
DucThienNguyen,AkshatKumar,andHoongChuinLau. Creditassignmentforcollectivemultiagentrlwith
globalrewards. Advancesinneuralinformationprocessingsystems,31,2018.
ElinorOstrom. Ageneralframeworkforanalyzingsustainabilityofsocial-ecologicalsystems. Science,325
(5939):419–422,2009.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,
SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelstofollowinstructionswith
humanfeedback. AdvancesinNeuralInformationProcessingSystems,35:27730–27744,2022.
LingPan,LongboHuang,TengyuMa,andHuazheXu. Planbetteramidconservatism:Offlinemulti-agentre-
inforcementlearningwithactorrectification.InInternationalConferenceonMachineLearning,pp.17221–
17237.PMLR,2022.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830,
2011.
Dean A. Pomerleau. Efficient Training of Artificial Neural Networks for Autonomous Navigation. Neural
Computation,3(1),1991. ISSN0899-7667. doi:10.1162/neco.1991.3.1.88.
MikayelSamvelyan,TabishRashid,ChristianSchroederDeWitt,GregoryFarquhar,NantasNardelli,TimGJ
Rudner,Chia-ManHung,PhilipHSTorr,JakobFoerster,andShimonWhiteson. Thestarcraftmulti-agent
challenge. arXivpreprintarXiv:1902.04043,2019.
FumihiroSasakiandRyotaYamashina. Behavioralcloningfromnoisydemonstrations. InInternationalCon-
ferenceonLearningRepresentations,2021.
LloydShapley. Avalueforn-persongames. ContributionstotheTheoryofGames,pp.307–317,1953.
AndyShih,StefanoErmon,andDorsaSadigh. Conditionalimitationlearningformulti-agentgames. In2022
17thACM/IEEEInternationalConferenceonHuman-RobotInteraction(HRI),pp.166–175.IEEE,2022.
JiamingSong,HongyuRen,DorsaSadigh,andStefanoErmon. Multi-agentgenerativeadversarialimitation
learning. Advancesinneuralinformationprocessingsystems,31,2018.
KarenSpa¨rckJones. Astatisticalinterpretationoftermspecificityanditsapplicationinretrieval. Journalof
Documentation,28(1):11–21,1972.
NisanStiennon, LongOuyang, JeffreyWu, DanielZiegler, RyanLowe, ChelseaVoss, AlecRadford, Dario
Amodei,andPaulFChristiano. Learningtosummarizewithhumanfeedback. AdvancesinNeuralInfor-
mationProcessingSystems,33:3008–3021,2020.
RobertThorndike. Whobelongsinthefamily? Psychometrika,18(4):267–276,1953.
QiTian,KunKuang,FuruiLiu,andBaoxiangWang. Learningfromgoodtrajectoriesinofflinemulti-agent
reinforcementlearning. arXivpreprintarXiv:2211.15612,2022.
Wei-Cheng Tseng, Tsun-Hsuan Johnson Wang, Yen-Chen Lin, and Phillip Isola. Offline multi-agent rein-
forcementlearningwithknowledgedistillation. AdvancesinNeuralInformationProcessingSystems, 35:
226–237,2022.
JianhongWang, YuanZhang, Tae-KyunKim, andYunjieGu. Shapleyq-value: Alocalrewardapproachto
solveglobalrewardgames. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,
pp.7285–7292,2020.
JianhongWang,YuanZhang,YunjieGu,andTae-KyunKim. Shaq: Incorporatingshapleyvaluetheoryinto
multi-agentq-learning. AdvancesinNeuralInformationProcessingSystems,35:5941–5954,2022.
11PublishedasaconferencepaperatICLR2024
JingXu, DaJu, MargaretLi, Y-LanBoureau, JasonWeston, andEmilyDinan. Recipesforsafetyinopen-
domainchatbots. arXivpreprintarXiv:2010.07079,2020.
MengjiaoYang,SergeyLevine,andOfirNachum.Trail:Near-optimalimitationlearningwithsuboptimaldata.
arXivpreprintarXiv:2110.14770,2021.
Lantao Yu, Jiaming Song, and Stefano Ermon. Multi-agent adversarial inverse reinforcement learning. In
InternationalConferenceonMachineLearning,pp.7194–7201.PMLR,2019.
Songyuan Zhang, Zhangjie Cao, Dorsa Sadigh, and Yanan Sui. Confidence-aware imitation learning from
demonstrationswithvaryingoptimality. AdvancesinNeuralInformationProcessingSystems, 34:12340–
12350,2021.
12PublishedasaconferencepaperatICLR2024
A APPENDIX TO METHODS
A.1 AXIOMATICPROPERTIESOFTHEEXCHANGEVALUEANDITSRELATIONSHIPWITHTHE
SHAPLEYVALUE
FixacharacteristicfunctiongameGwithasetofplayersN. Itiswell-knownthattheShapleyValuesatisfies
thefollowingaxioms(Shapley,1953):
(1) Dummy:ifanagentisatisfiesv(C∪{i})=v(C)forallC ⊆N \{i}thenSV (G)=0;
i
(2) Efficiency: the sum of all agents’ Shapley Values equals to the value of the grand coalition, i.e.,
(cid:80)
SV (G)=v(N);
i∈N i
(3) Symmetry:foreverypairofdistinctagentsi,j ∈N withv(C∪{i})=v(C∪{j})forallC ⊆N\{i,j}
wehaveSV (G)=SV (G);
i j
(4) Linearity: foranypairofgamesG =(N,v )andG =(N,v )withthesamesetofagentsN,thegame
1 1 2 2
G = (N,v)whosecharacteristicfuncitonv isgivenbyv(C) = v (C)+v (C)forallC ⊆ N satisfies
1 2
SV (G)=SV (G )+SV (G )foralli∈N.
i i 1 i 2
Indeed,theShapleyValueistheonlyvalueforcharacteristicfunctiongamesthatsatisfiestheseaxioms(Shap-
ley, 1953). It is then natural to ask which of these axioms (or their variants) are satisfied by the Exchange
Value.
Toanswerthisquestion,wefirstestablisharelationshipbetweentheShapleyValueandtheExchangeValue.
PropositionA.1. ForanycharacteristicfunctiongameG=(N,v)andeveryagenti∈N wehave
(cid:18) (cid:19)
n 1
EV (G)= SV (G)− ·v(N) . (6)
i n−1 i n
Proof. Fixanagentiandconsideranarbitrarynon-emptycoalitionC ⊊N \{i}.
IntheexpressionfortheShapleyValueofithecoefficientofv(C)is
1
− (|C|)!(n−1−|C|)!:
n!
wesubtractthefractionofpermutationsofN wheretheagentsinCappearinthefirst|C|positions,followed
byi.Bythesameargument,thecoefficientofv(C∪{i})is
1
(|C|)!(n−1−|C|)!.
n!
Similarly,intheexpressionfortheExchangeValueofithecoefficientofv(C)is
1
− (|C|)!(n−1−|C|)!:
(n−1)!(n−1)
eachpermutationofN \{i}whereagentsinC appearinthefirst|C|positionscontributeswithcoefficient
− 1 .Bythesameargument,thecoefficientofv(C∪{i})is
(n−1)!(n−1)
1
(|C|)!(n−1−|C|)!
(n−1)!(n−1)
Now, if C = N \{i}, in the expression for SV (G) the coefficient of v(C) is −1 and the coefficient of
i n
v(C∪{i})=v(N)is 1.Incontrast,intheexpressionforEV (G)thecoefficientofv(C)is− 1 :foreach
n i n−1
ofthe(n−1)!permutationsofN \{i}wesubtractv(C)withcoefficient 1 whenwereplacethe
(n−1)!(n−1)
lastagentinthatpermutationbyi.Ontheotherhand,v(N)neverappears.
Itfollowsthat,foreverycoalitionC ⊊N,ifthevaluev(C)appearsintheexpressionforSH (G)withweight
i
ωthenitappearsintheexpressionforEV (G)withweight n ·ω.Hence
i n−1
(cid:18) (cid:19)
n 1
EV (G)= SH (G)− ·v(N)
i n−1 i n
ExampleA.2. ConsideracharacteristicfunctiongameG = (N,v), whereN = {1,2}andv isgivenby
v({1})=2,v({2})=4,v({1,2})=10.Wehave
SH (G)=(2+(10−4))/2=4, SH (G)=(4+(10−2))/2=6
1 2
and
EV (G)=2−4=−2, EV (G)=4−2=2.
1 2
NotethatEV (G)=2(SH (G)− 1v(N)),EV (G)=2(SH (G)− 1v(N)).
1 1 2 2 2 2
13PublishedasaconferencepaperatICLR2024
WecanusePropositionA.1toshowthattheExchangeValuesatisfiestwooftheaxiomssatisfiedbytheShapley
Value,namely,linearityandsymmetry.
PropositionA.3. TheExchangeValuesatisfiessymmetryandlinearityaxioms.
Proof. For the symmetry axiom, fix a characteristic function game G = (N,v) and consider two agents
i,j ∈N withv(C∪{i})=v(C∪{j})forallC ⊆N \{i,j}.Wehave
(cid:18) (cid:19) (cid:18) (cid:19)
n 1 n 1
EV (G)= SV (G)− ·v(N) = SV (G)− ·v(N) =EV (G),
i n−1 i n n−1 j n j
wherethefirstandthethirdequalityfollowbyPropositionA.1,andthesecondequalityfollowsbecausethe
ShapleyValuesatisfiessymmetry.
Forthelinearityaxiom,considerapairofgamesG =(N,v )andG =(N,v )withthesamesetofagents
1 1 2 2
N and the game G = (N,v) whose characteristic funciton v is given by v(C) = v (C)+v (C) for all
1 2
C ⊆N.Fixanagenti∈N.Wehave
(cid:18) (cid:19)
n 1
EV (G)= SV (G)− ·(v (N)+v (N))
i n−1 i n 1 2
(cid:18) (cid:19) (cid:18) (cid:19)
n 1 n 1
= SV (G )− ·v (N) + SV (G )− ·v (N)
n−1 i 1 n 1 n−1 i 2 n 2
=EV (G )+EV (G ).
i 1 i 2
Again,thefirstandthethirdequalityfollowbyPropositionA.1,andthesecondequalityfollowsbecausethe
ShapleyValuesatisfieslinearity.
WhiletheExchangeValuedoesnotsatisfythedummyaxiomortheefficiencyaxiom,itsatisfiesappropriately
modifiedversionsoftheseaxioms.
(cid:80)
PropositionA.4. ForeverycharacteristicfunctiongameGitholdsthat EV (G)=0.Moreover,ifiis
i∈N i
adummyagent,i.e.,v(C∪{i})=V(C)forallC ⊆N \{i}thenEV (G)=−v(N).
i n−1
Proof. Wehave
(cid:18) (cid:19)
(cid:88) (cid:88) n 1 (cid:88) n n
EV (G)= SV (G)− ·v(N) = SV (G)− ·v(N)
i n−1 i n n−1 i n−1
i∈N i∈N i∈N
n n
= ·v(N)− ·v(N)=0,
n−1 n−1
whereweusePropositionA.1andthefactthattheShapleyValuesatisfiestheefficiencyaxiom.
Now,fixadummyagenti.Wehave
(cid:18) (cid:19)
n 1 1
EV (G)= SV (G)− ·v(N) =− ·v(N);
i n−1 i n n−1
again,weusePropositionA.1andthefactthattheShapleyValuesatisfiesthedummyaxiom.
A.2 DERIVATIONOFCLUSTERINGOBJECTIVESTATEDINEQ.4
InessentialgamesandEVs. TheassumptionofaninessentialgameiscommonlymadetocomputeShap-
leyValuesmoreefficiently2. Inaninessentialgame,thevalueofagroupisgivenbythesumoftheindividual
(cid:80)
contributions of its members, denoted as v(C) = v , where v is an individual agent’s unobserved
i∈C i i
contributionv .TheEV(seeDefinition4.1)ofanindividualagentiinaninessentialgameisgivenas
i
(cid:88) (cid:88)
EV i(G)=v i−1/|N|−1· v
j
=(1+1/|N|−1)·v i−1/|N|−1· v j,
j∈N\{i} j∈N
Thisexpressionrepresentsthedifferencebetweentheindividualcontributionofagenti, v , andtheaverage
i
individualcontributionofallotheragents.Thesecondtermisindependentofiandremainsconstantacrossall
agents.
2see,e.g.,Covert,I.andLee,S.I.,2020.Improvingkernelshap:Practicalshapleyvalueestimationvialinear
regression.arXivpreprintarXiv:2012.01536.
14PublishedasaconferencepaperatICLR2024
Figure 4: In the Overcooked environments Cramped Room (left) and Coordination Ring (right),
agentsmustcooperatetocookanddeliverasmanysoupsaspossiblewithinagiventime.
Derivationofequivalentclusteringobjective. Wenowconsidertheoptimizationproblemdefinedby
Equation4,whichdefinesoptimalclusterassignmentsu∗suchthatthevarianceinEVsismaximised
u∗ ∈argmax Var([E˜V (G˜),...,E˜V (G˜)]).
u 0 n−1
Further,theclusteredvaluefunctionisdefinedas
v˜(C)=1/η·(cid:80)n m− =1 0(cid:80) π∈ΠNv(S π(m))·1({u
j
|j ∈S π(m)}=C),
wherethenormalisationconstantisdefinedasη=(cid:80)n−1 (cid:80) 1({u |j ∈S (m)}=C).Wedenoteby
m=0 π∈ΠN j π
k theindividualcontributionoftheagentthatrepresentstheagentsinclusteri. Thevaluek isdefinedasthe
i i
averageindividualcontributionofallagentsassignedtothecluster,i.e.k
i
=1/ϵ·(cid:80) j∈Nv j·1(u(i)=u(j)).
Here,thenormalizationconstantisgivenasϵ=(cid:80) 1(u(i)=u(j)).
j∈N
Usingtheconceptoftheclusteredvaluefunctionv˜,wecanexpresstheEVforallagentsassignedclusterias
EV i(G˜)=(1+1/|K|−1)·k i−1/|K|−1·(cid:88) k j.
j∈K
The second term, which is cluster-independent, can be omitted when computing the variance
Var([EV (G˜),...,EV (G˜)]),asthevarianceisagnostictoashiftinthedatadistribution. Wewillomit
0 n−1
thescalingfactor(1+1/|K|−1)fromhereonwards.
Letn denotethenumberofagentsassignedtoclusterj ∈
K,with(cid:80)K−1n
= N. BysimplifyingEqua-
j i=0 i
tion4,weobtain:
K−1 (cid:18) (cid:19)2
Var([EV 0(G˜),...,EV n−1(G˜)])= (cid:88) n i· k i−(cid:80)K j=− 01nj·kj/N .
i=0
ThisallowsustoexpresstheobjectivestatedinEquation4as
u∗ ∈argmax Var([k ,...,k ]).
u 0 n−1
TheobjectivestatedinEquation4isthereforeequivalenttoassigningagentstoclusterssuchthatthevariance
inclustercentroids(centroidscomputedasthemeanoftheunobservedindividualcontributionsv ofallagents
i
assignedtoagivencluster)ismaximized.
Table3: DatasetstatisticsinOvercooked.
Imitationmethod CrampedRoomDλ CoordinationRingDλ CrampedRoomDadv CoordinationRingDadv
Minimum 0 0 0 0
Mean 20.6 33.58 12 19.39 16.91 40.64 3 11.15
Maximum 1±50 ±80 1±60 ±80
B OVERCOOKED EXPERIMENTS
WegeneratethesimulateddatasetsusingtheplanningalgorithmgiveninCarrolletal.(2019)3. Tobeableto
simulateagentswithdifferentbehaviors(fromadversarialtooptimal),wefirstintroducealatenttraitparameter,
3https://github.com/HumanCompatibleAI/overcooked_ai
15PublishedasaconferencepaperatICLR2024
Relation between [%] of data observed and within cluster variance
Relation between latent behaviour and EVs
0.1 Cramped Room 6 C Cr oa om rdp ine ad t iR oo no Rm in - g V -a Vri aa rn iace n cC el r C. lr. Coordination Ring
Cramped Room - Behavior Clr.
5 Coordination Ring - Behavior Clr.
4
0.0 3
2
1
0.1 0
-1 Latent behavio0 r parameter 1 50% 30% 10% single-obs
Figure 5: Relation between an agent’s
Figure6:Within-clustervarianceinrelationtofraction
traitλanditsEVinOvercooked.
of observations for simulated data in Cramped Room
and Coordination Ring (Overcooked). Two cluster-
ingmethodsshown(BehaviorclusteringandVariance
Clustering).Inthecaseofrandomclusterassignments,
thewithin-clustervarianceis5.11 0.11,whileunder
±
optimalclusterassignments,thevarianceis0.156. See
sectionB.1fordiscussion.
λ,whichdeterminesthelevelofadversarialoroptimalactionsforagivenagent. Avalueofλ=1represents
apolicythatalwayschoosesthebestactionwithcertainty. Asλdecreases, agentsaremorelikelytoselect
non-optimalactions.Forλ<0,weinvertthecostfunctiontocreateagentswithadversarialbehavior.Notably,
weassignahighcost(orlowcostwheninverted)tooccupyingthecellnexttothecounterintheOvercooked
environment. Occupying the cell next to the counter enables adversarial agents to block other agents in the
executionoftheirtasks.
Forhumangameplaydatasets,weutilizedtherawversionsoftheOvercookeddatasets.4 Thesedatasetswere
usedas-is,withoutmanualpre-filtering.
EVs. Toestimateagents’EVsaccordingtoSection4.2,weusedeitherthefullsetofallpossiblegroupsora
fractionofit(seeFigure3fortherelationshipbetweendatasetsizeandEVestimationerror).Foreachobserved
grouping,weconducted10rolloutsintheenvironmentandcalculatedtheaveragescoreacrosstheserollouts
toaccountforstochasticityintheenvironment.
Imitationlearning. ForEV2BC,BC,andgroup-BC,weusedtheimplementationofBehaviorCloningin
Overcookedasgivenbytheauthorsof(Carrolletal.,2019)5. Weimplementtheofflinemulti-agentreinforce-
mentlearningmethodOMAR(Panetal.,2022)usingtheauthor’simplementation.6 FortheOMARbaseline,
wesettherewardatthelasttimesteptotheDVF’sscoreforagiventrajectory,asourworkassumesthatnoper-
steprewardsignalisgiven,incontrasttothestandardoffline-RLframework. Weconductedahyperparameter
sweepforthefollowingparameters: learningratewithoptions{0.01,0.001,0.0001},Omar-coewithoptions
{0.1,1,10},Omar-iterswithoptions{1,3,10},andOmar-sigmawithoptions{1,2,3}. Thebest-performing
parameterswereselectedbasedontheevaluationresults.
B.1 CLUSTERINGOFAGENTSINOVERCOOKED
Behaviorclustering. ThebehaviorclusteringprocessintheOvercookedenvironmentinvolvesthefollow-
ing steps. Initially, we identify the 200 states that are most frequently visited by all agents in the given set
ofobservations. AstheactionspaceinOvercookedisrelativelysmall(≤7actions),wecalculatetheempir-
icalactiondistributionforeachstateforeveryagent. These200actiondistributionsarethenconcatenatedto
formabehaviorembeddingforeachagent.Toreducethedimensionalityoftheembedding,weapplyPrincipal
ComponentAnalysis(PCA),transformingtheinitialembeddingspaceintothreedimensions.Subsequently,we
employthek-meansclusteringalgorithmtoassignagentstobehaviorclusters. Thenumberofclusters(3for
Overcooked)isdeterminedusingtheELBOWmethod(Thorndike,1953),whilelinearkernelsareutilizedfor
bothPCAandk-means.Itisnoteworthythattheresultsarefoundtoberelativelyinsensitivetotheparameters
usedinthedimensionalityreductionandclusteringsteps,thusstandardimplementationsareemployedforboth
4https://github.com/HumanCompatibleAI/human_aware_rl/tree/master/human_
aware_rl/data/human/anonymized
5https://github.com/HumanCompatibleAI/overcooked_ai/tree/master/src/
human_aware_rl/imitation
6https://github.com/ling-pan/OMAR
16
)desilamron(
eulav
egnahcxE
ecnairav
retsulc
nihtiw
-
dekoocrevOPublishedasaconferencepaperatICLR2024
1 3 10
1x-dpl. 3x-dpl. 10x-dpl.
1% 3% 10%
avg.3
−
avg.
avg.+3
ke ke ke ke ke ke ke ke ke ke ke ke
Ta Ta Ta Ta Ta Ta Ta Ta Ta Ta Ta Ta
vfinal
vtotal
vmin
Figure7: Colour-codedorderingofEVsforagentswithvaryingbehaviorsinTragedyoftheCom-
mons. Thebrighter,thehigheranagent’scontributiontoagivenvaluefunction.
methods(Pedregosaetal.,2011). Importantly,thisclusteringprocedurefocusesexclusivelyontheobserved
behaviorofagents,specificallytheactionstakeninspecificstates,andisindependentofthescoresassignedto
trajectoriesbytheDVF.
EV-Clustering. Incontrasttobehaviorclustering,EV-Clustering(seeSection4.2.1)focusessolelyonthe
scoresassignedtotrajectoriesbytheDVFanddisregardsagentbehavior. Theobjectiveofvarianceclustering
istomaximizethevarianceinassignedEVs,asstatedinEquation4.Tooptimizethisobjective,weutilizethe
SLSQPnon-linearconstrainedoptimizationintroducedbyKraft(1988).
Weusesoftclusterassignmentsandenforceconstraintstoensurethatthetotalprobabilityisequaltoonefor
eachagent.Thesolverisinitializedwithauniformdistributionandrunsuntilconvergenceorforamaximumof
100steps.Giventhattheoptimizationproblemmayhavelocalminima,weperform500randominitializations
andoptimizations,selectingthesolutionwiththelowestloss(i.e.thehighestvarianceinassignedEVs).
CombiningBehaviorClusteringandEVClustering. AsdescribedinSections4.2.2and5.1,behavior
clustering (which utilizes behavior information but disregards DVF scores) and variance clustering (which
utilizesDVFscoresbutdisregardsbehaviorinformation)arecombinedtoestimateEVsfordegeneratedatasets.
WeinitializetheSLSQPsolverwiththeclusterassignmentsobtainedfrombehaviorclusteringandintroducea
smalllosstermintheobjectivefunctionofEquation4. Thisadditionallossterm,weightedby0.1(selectedin
asmallsensitivityanalysis),penalizesdeviationsfromthebehaviorclusters.Similartobefore,weperform500
iterationswhileintroducingasmallamountofnoisetotheinitialclusterassignmentsateachstep.Thesolution
withthehighestvarianceinassignedEVsisthenselected.
Ablationstudy. Wepresentanablationstudytoexaminetheimpactofdifferentcomponentsinthecluster-
ingapproachdiscussedinSection5.1.Weproposedtwosequentialclusteringmethods:behaviorclusteringand
varianceclustering. Thisablationstudyinvestigatestheperformanceofbothclusteringstepswhenperformed
independently,alsoundertheconsiderationofthefractionofthedatathatisobserved.Weassessperformance
asthewithin-clustervarianceintheunobservedagent-specificlatenttraitvariableλ,wherelowerwithin-cluster
varianceindicateshigherperformance.Itisimportanttonotethatλissolelyusedforevaluatingtheclustering
stepsandnotutilizedduringtheclusteringprocess.TheresultsoftheablationstudyaredepictedinFigure6.
WefirstdiscussEV-Clustering. EV-ClusteringasintroducedinSeciton4generallyleadstoasignificantde-
creaseinwithin-clustervarianceintheunobservedvariableλ. Morespecifically,theproposedvarianceclus-
teringapproach(when50%ofdataisobserved),resultsina∼ 89%reductionofthewithin-clustervariance
inλ,whichvalidatestheapproachofclusteringagentsbytheirunobservedindividualcontributionsbymaxi-
mizingthevarianceinestimatedEVs. However,weobserveinFigure6thatasthefractionofobserveddata
decreases,thewithin-clustervarianceincreases,indicatingadecreaseinthequalityofclustering. Thehighest
within-clustervarianceisobservedwhenusingonlyasingleobservation(’single-obs’),whichcorrespondsto
afully-anonymizeddataset. Thisfindingisconsistentwiththefactthatafully-anonymizeddatasetpresentsa
degeneratecreditassignmentproblem,asdiscussedinSection4.2.2.
Wenowdiscussbehaviorclustering. Figure6showsthatbehaviorclusteringgenerallyresultsinaverylow
within-clustervariance. However, itisimportanttonotethattheseresultsmaynotdirectlytranslatetoreal-
worlddata,astheablationstudyusessimulatedtrajectories. Notethatsuchanablationstudycannotbecon-
ductedforthegivenreal-worldhumandatasets,asthesearefullyanonymized.
17PublishedasaconferencepaperatICLR2024
C TRAGEDY OF THE COMMONS EXPERIMENTS
Clustering. WemodelToCasamulti-agentenvironmentwhereagentsconsumefromacommonpoolof
(cid:0) (cid:80) (cid:1)
resourcesx ,whichgrowsatafixedrateg=25%ateachtimestept:x =max (1+g)·x − c , 0 ,
t t+1 t i ti
withc astheconsumptionoftheithagentattimetandx =200asthestartingpool.Hence,ifallresources
ti 0
areconsumed, nonecanregrowandnoagentscanconsumemoreresources. TheTragedyoftheCommons
(ToC)environmentfeatures4differentbehaviorpatterns:Take-XconsumesXunitsateverytimestep,Take-X-
x-dplconsumesXunitsifthisdoesnotdepletethepoolofresources,TakeX%consumesX%oftheavailable
resources,andTakeAvgconsumestheaverageoftheresourcesconsumedbytheotheragentsattheprevious
timestep(0inthefirsttimestep). Forthesmall-scaleexperimentof12agents, weconsiderthreeagentsfor
eachpattern, withXvaluesselectedfromtheset1,3,10. Forthelarge-scaleexperimentof120agents, we
simplyreplicateeachagentconfiguration10times.Wesimulatebothexperimentsforgroupsofsize3and10,
respectively.Wegenerateasimulateddatasetusingagentswithfourdifferentbehaviorpatterns.Wefirstcollect
adatasetofobservationsforasmall-scaleexperimentof12agentsandsimulateToCforgroupsofthreeagents
for50timesteps(welaterconsideragroupof120agents).
DuetothecontinuousnatureofthestateandactionspacesinToC,wefirstdiscretizebothandthenapplythe
sameclusteringmethodsusedintheOvercookedscenario. WeproceedbycomputingEVsforallagentsas
doneinOvercooked(seeFigure3forresults). Weimplementimitationpoliciesbyreplicatingtheaveraged
actiondistributionsinthediscretizedstates.
C.1 COMPUTATIONALDEMAND.
We used an Intel(R) Xeon(R) Silver 4116 CPU and an NVIDIA GeForce GTX 1080 Ti (only for training
BC,EV2BC,group-BC,andOMARpolicies). InOvercooked,generatingadatasettookamaximumofthree
hours,andestimatingEVsfromagivendatasettakesafewseconds. Behaviorclusteringconsumesacouple
ofminutes, whileVarianceclusteringtakesuptotwohoursperconfiguration(notethatitisrun500times).
TrainingoftheBC,group-BC,andEV2BCpoliciestooknomorethan30minutes(usingaGPU),whilethe
OMARbaselinewastrainedforupto2hours.InTragedyofCommons,eachrolloutonlyconsumesacoupleof
seconds. ClusteringtimeswerecomparabletothoseinOvercooked. Computingimitationpoliciesissimilarly
onlyamatterofafewminutes.
D STARCRAFT EXPERIMENTS
Datasetgeneration. Foreachenvironmentconfiguration,wefirsttrainagentswiththegroundtruthreward
functionfordifferentseeds. Wethenextractagentsatthreedifferenttimesteps(0%,50%,and100%)ofthe
training. Werandomlysamplegroupsoftheextractedagentstogeneratealargesetoftrajectoryrolloutsand
recordtheaveragescoreachievedbyagroup, whichservesastheobservedDVFscore. Weanonymizethe
datasetbyassigningone-time-useIDstoallagents.
ClusteringofagentsandEVcomputation. Wefirstextracthigh-dimensionalfeaturesfromagenttra-
jectories using Term Frequency-Inverse Document Frequency (TF-IDF (Spa¨rck Jones, 1972)) and compute
agentclusters.Wecomputealargenumberofpossibleclusterassignmentsusingdifferenthyperparametersfor
TF-IDFanddifferenthyperparametersforspectralclustering. InaccordancewiththeobjectiveofEVcluster-
ing,wethenchoosetheclusterassignmentthatresultsinthelargestvarianceinEVsacrossallagents.
Foreachindividualagent,weextracttheactionsequence(uptoacutoffofeither1000or50000environment
steps).WethenconverttheactionsequenceintoastringtoapplyTD-IDF.Thiscanbethoughtofasrepresenting
eachactionasanindividualword.Thisresultsinacorpusthatcontainsonedocumentperagent.
Forthetransformationofthecorpusintoafeaturespace,weapplytheTF-IDFvectorization,parameterizedby:
• Min df:Minimumdocumentfrequencysetat0.05,ensuringinclusionoftermspresentinatleast5%ofthe
documents.
• Max df: Maximumdocumentfrequenciestestedwere0.3and0.9,tofilterouttermsexcessivelycommon
acrossdocuments.
• Max features:Theupperlimitonthenumberoffeaturesconsideredweresetto100,000or1,000,000.
• Ngram range: Weexploredn-gramrangesof(1,3),(1,5),and(1,10),tocapturevaryinglengthsofterm
dependencies.
PostTF-IDFvectorization, weusespectralclusteringtoachieveclusterassignments. Wecomputeclustered
EVsforeachpossibleclusterassignmentandchoosetheclusterassignmentwiththehighestvarianceinEVs
acrossallagents.ThecomputedEVsforthehighest-varianceclusterassignmentarethenusedastheestimated
EVsforallagents.
18PublishedasaconferencepaperatICLR2024
ImitationwithEV2BC. WeapplyEV2BcbyfilteringforagentswithhighestimatedEVs.Wefoundthat
pre-filteringfortrajectorieswithhighcollectivescoressignificantlyimprovesperformance.Inotherwords,we
filterforagentswithhighestimatedEVsintrajectorieswithhighcollectivescores. Weassumethatthisisthe
caseassomeagentsmight‘block’anyprogressintrajectorieswithlowcollectivescores.
19