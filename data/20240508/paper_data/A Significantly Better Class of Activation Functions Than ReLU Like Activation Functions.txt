A Significantly Better Class of Activation Functions Than
ReLU Like Activation Functions
Mathew Mithra Noela, Yug Oswalb
aSchool of Electrical Engineering, Vellore Institute of Technology, Email:
mathew.m@vit.ac.in, India
bSchool of Computer Science and Engineering, Vellore Institute of Technology, Email:
yoswal071@gmail.com, India
Abstract
This paper introduces a significantly better class of activation functions than
the almost universally used ReLU like and Sigmoidal class of activation functions.
Two new activation functions referred to as the Cone and Parabolic-Cone that differ
drastically from popular activation functions and significantly outperform these on
the CIFAR-10 and Imagenette benchmmarks are proposed. The cone activation
functions are positive only on a finite interval and are strictly negative except at
the end-points of the interval, where they become zero. Thus the set of inputs that
produce a positive output for a neuron with cone activation functions is a hyperstrip
andnotahalf-spaceasistheusualcase. Sinceahyperstripistheregionbetweentwo
parallel hyper-planes, it allows neurons to more finely divide the input feature space
into positive and negative classes than with infinitely wide half-spaces. In particular
the XOR function can be learn by a single neuron with cone-like activation functions.
Both the cone and parabolic-cone activation functions are shown to achieve higher
accuracies with significantly fewer neurons on benchmarks. The results presented
in this paper indicate that many nonlinear real-world datasets may be separated
with fewer hyperstrips than half-spaces. The Cone and Parabolic-Cone activation
functions have larger derivatives than ReLU and are shown to significantly speedup
training.
Keywords:
Activation Function, Image Classification, Artificial Neural Network, Deep
Learning, XOR Problem
Preprint submitted to journal May 8, 2024
4202
yaM
7
]IA.sc[
1v95440.5042:viXra1. Introduction
Since the discovery of ReLU like activation functions [1], the question of the
existence of an even better class of activation functions that differ significantly from
both sigmoidal and ReLU like activation functions has remained unanswered [2].
This paper answers the above fundamental question in the affirmative by proposing
a new class of activation functions. Despite the complexity of deep Artificial Neural
Networks (ANNs), each individual neuron in an ANN essentially makes a linear
decision by separating its inputs with a single hyperplane. In particular the set of
inputs that elicit a positive output from a single neuron is a halfspace.
1.1. Nature of neuronal decision boundaries
The output (activation) of a single neuron is given by a = g(wTx+b), where g
is the activation function. The hyperplane boundary associated with a neuron is the
set of points:
H = {x ∈ Rn : wTx+b = 0}
The set of points for which a neuron produces positive and negative outputs are
half-spaces for most populat activation functions. The positive and negative half
spaces are defined to be:
H = {x ∈ Rn : wTx+b < 0}
−
H = {x ∈ Rn : wTx+b > 0}
+
Any hyperplane divides it’s input space Rn into 3 connected regions: the positive
half-space H , the negative half-space H and an affine-space H. The weight vector
+ −
w points into the positive half-space H . Fig. X shows illustrates the separation of
+
the input space by a single neuron.
The cone and the parabolic-cone activation function is defined to be g(z) =
1 − |z − 1| and g(z) = z(2 − z) respectively. A wider class of cone-like activation
functions can also be defined: g(z) = β − |z − γ|α, where α,β and γ are learnable
parameters that affects the shape of the activation function. In contrast to Fig.
X, Fig. Y shows the separation of the input space by single neuron with cone-like
activation function.
The set of inputs to a neuron that produce a strictly positive output is denoted
by C and the set of inputs that produce a strictly negative output is denoted by
+
C .
−
The set of inputs that exactly produce an output of zero constitutes the decision
boundary of the neuron. In particular the decision boundary of a single neuron
2that produces an output a = g(z) = g(wTx + b) is the set B(g) = {x ∈ Rn :
g(wTx+b) = 0}. Based on the above, the decision boundary of popular activation
function like Leaky ReLU, Swish and GELU that zero have a single zero at the origin
is a hyperplane.
g(z) = 0 ⇐⇒ z = 0
z = wTx+b = 0
In other words the decision boundary is a single hyperplane (B = H = {x ∈ Rn :
wTx+b = 0}).
On the other hand, the decision boundary of Cone-like activation functions that
are zero at the endpoints of a closed interval [0,δ] consists of two hyperplanes.
g(z) = 0 ⇐⇒ z = 0 OR z = δ
=⇒ wTx+b = 0 OR wTx+b = δ
The set of inputs to a Cone-like neuron that produce a strictly positive output is
C = {x ∈ Rn : 0 < wTx+b < δ}. Thus C for Cone-like neurons is a hyper-strip
+ +
and not a half-space as is usual for popular activation functions 1.
Table 1: List of activation functions
Activation Equation Range
Function
Cone 1−|z−1| [-∞,1]
Parabolic-Cone z(2−z) [-∞,1]
Parameterized-Cone 1−|z−1|β [-∞,1]
Sigmoid/Softmax 1 [0,1]
1+e−z
Tanh tanh(z) [−1,1]
LiSHT ztanh(z) (−∞,∞)
Softplus ln(1+ez) [0,∞)
ReLU max(0,z) [0,∞)
(cid:26) 0.01z z<0;
LeakyReLU (−∞,∞)
z z≥0;
GELU
0.5z(1+tanh((cid:113)2z+0.044715z3))
[-0.5,∞)
π
SELU   λ λz α(ez−1) z z<≥ 00; . [−λα,∞)
 α≈1.6733 λ≈1.0507
Mish ztanh(ln(1+ez)) [-0.31,∞)
Swish z [-0.5,∞)
1+e−z
(cid:26) z z≥0;
ELU (ez−1) z<0. [-1,∞)
3Figure 1: Comparison of ReLU with Cone and Parabolic-Cone activation functions. The set of
inputs that provide a strictly positive output for Cone and Parabolic-Cone activation functions is
a finite interval (0,2) as apposed to (0,∞) for ReLU.
Figure2: AComparisonofthefirstderivativesofdifferentactivationfunctions. Cone-likeactivation
functions never saturate and have larger derivative values for most inputs.
Fig. 1 compares the Cone and Parabolic-Cone activation functions with ReLU.
Fig. compares the derivatives of different activation functions. Cone-like activation
functions never have small or zero derivative for any input. Cone-like activation
functions also have larger derivative values than ReLU for most inputs facilitating
faster learning. A parameterized version of the Cone activation is shown in Fig. 3.
4Figure 3: Variation in the shape of the Parameterized-Cone activation with parameter β.
1.2. Halfspaces versus Hyper-strips
Since hyper-strips are narrower compared to infinitely wide halfspaces, fewer
hyper-strips are needed to accurately partition the inputs space into different classes.
Fig. 4 below illustrated how a simple two layer ANN with just 2 hidden layer Cone
neurons and a single sigmoidal neuron can learn a complex linearly non-separable
dataset.
Figure 4: Only two hyper-strips are needed to accurately partition this dataset. Two neurons with
Cone or Parabolic-Cone can be used to learn the 2 hyper-strips. However 4 ReLU or sigmoidal
neurons will be needed to learn 4 hyperplane boundaries.
5Figure 5: The classic XOR problem can be solved with a single neuron with Cone activation, since
C is a hyper-strip for Cone-like neurons.
+
Fig. 5 illustrates how the classic XOR problem can be solved with a single neuron
with Cone-like activation function. The main contributions of this paper are:
• A new class of activation functions significantly better than ReLU like activa-
tion functions is proposed
• The proposed activation functions learn a hyper-strip instead of a halfspace to
separate the class of positive inputs.
• Using hyper-strips instead of halfspaces to separate the class of positive inputs
is shown to result in smaller ANNs
2. Results: Performance comparison on benchmark datasets
In the following, Cone-like activation functions are compared with most popular
activation functions on the CIFAR-10 [3] and Imagenette [4] benchmarks. Appendix-
I and Appendix-II show the CNN architectures used with different benchmarks.
A standard CNN architecture consisting of convolutional layers followed by fully
connected dense layers was used. The features computed by the convolutional filters
are input to a single dense layer with the activation function being tested. The
output in all cases consisted of a Softmax layer. Tables below show the average
results obtained over 5 independent trials to account for variation in performance
due to random initialization. Adam optimizer with a learning rate of 10-4 and
standard momentum with both β and β was used to train the models. All models
1 2
were trained for 30 epochs with Categorical Cross-Entropy loss. Tables 2, 3 and 4
clearly show that the Cone and Parabolic-Cone activation functions achieve higher
accuracies on the CIFAR-10 benchmark with significantly fewer neurons.
6Table 2: Performance comparison of different activation functions on the CIFAR-10 dataset
with a single fully-connected layer composed of 64 neurons.
Activation Mean Test Median Test Std. Dev. Test Best Test Worst Test
Function Accuracy Accuracy Accuracy Accuracy Accuracy
ReLU 0.7196 0.7208 0.0044 0.7234 0.7120
LeakyReLU 0.7247 0.7234 0.0035 0.7303 0.7209
Cone 0.742728 0.7427 0.0046 0.7495 0.7377
Parabolic-Cone 0.7510 0.7499 0.006 0.7586 0.7446
Table 3: Performance comparison of different activation functions on the CIFAR-10 dataset
with a single fully-connected layer composed of 32 neurons.
Activation Mean Test Median Test Std. Dev. Test Best Test Worst Test
Function Accuracy Accuracy Accuracy Accuracy Accuracy
ReLU 0.7196 0.6893 0.0061 0.6952 0.6807
LeakyReLU 0.7052 0.7040 0.0113 0.7238 0.6953
Cone 0.7291 0.7260 0.0062 0.7388 0.7236
Parabolic-Cone 0.7439 0.7428 0.0065 0.7538 0.7378
Table 4: Performance comparison of different activation functions on the CIFAR-10 dataset
with a single fully-connected layer composed of 10 neurons.
Activation Mean Test Median Test Std. Dev. Test Best Test Worst Test
Function Accuracy Accuracy Accuracy Accuracy Accuracy
ReLU 0.6157 0.6110 0.0140 0.6317 0.5993
LeakyReLU 0.6292 0.6329 0.0110 0.6405 0.6174
Cone 0.6844 0.6783 0.0122 0.6999 0.6714
Parabolic-Cone 0.6998 0.6997 0.0064 0.7088 0.6920
Tables 5, 6 and 7 show that the Cone and Parabolic-Cone activation functions
achieve overall higher accuracies on the Imagenette benchmark when the number of
neurons is reduced.
Table 5: Performance comparison of different activation functions on the Imagenette benchmark
with a single fully-connected layer composed of 64 neurons.
Activation Mean Test Median Test Std. Dev. Test Best Test Worst Test
Function Accuracy Accuracy Accuracy Accuracy Accuracy
ReLU 0.852229 0.850701 0.004028 0.859363 0.849682
LeakyReLU 0.852688 0.851465 0.006892 0.863185 0.844076
Cone 0.852994 0.852739 0.007127 0.863185 0.843057
Parabolic-Cone 0.847949 0.85172 0.007872 0.855541 0.838471
7Table 6: Performance comparison of different activation functions on the Imagenette benchmark
with a single fully-connected layer composed of 32 neurons.
Activation Mean Test Median Test Std. Dev. Test Best Test Worst Test
Function Accuracy Accuracy Accuracy Accuracy Accuracy
ReLU 0.844178 0.841783 0.00504 0.851465 0.838981
LeakyReLU 0.851006 0.846879 0.005905 0.859108 0.846624
Cone 0.85228 0.851465 0.003751 0.857834 0.848408
Parabolic-Cone 0.849121 0.849682 0.004422 0.855796 0.844076
Table 7: Performance comparison of different activation functions on the Imagenette benchmark
with a single fully-connected layer composed of 10 neurons.
Activation Mean Test Median Test Std. Dev. Test Best Test Worst Test
Function Accuracy Accuracy Accuracy Accuracy Accuracy
ReLU 0.774573 0.773758 0.006244 0.78344 0.767898
LeakyReLU 0.822981 0.824713 0.006718 0.82879 0.811465
Cone 0.830981 0.829809 0.002532 0.834904 0.82879
Parabolic-Cone 0.842089 0.839745 0.007008 0.852229 0.835159
Figure 6: Training curves with different activation functions on CIFAR-10 with a single layer of 10
dense neurons.
TheConeandtheParabolic-Coneactivationfunctionsproposedinthispaperalso
significantly speedup training (Fig. 6). The faster training can be attributed to the
largerderivativevaluesforCone-likeactivationfunctionsformostinputscomparedto
otheractivationfunctions. Thefundamentalquestionofwhetheractivationfunctions
8withevenlargerderivativeswilltrainfasterorleadtotheexplodinggradientproblem
remains unanswered.
3. Conclusion
ReLU like activation function differ drastically from sigmoids and significantly
outperform sigmoidal activation functions and allows deep ANNs to be trained by
alleviating the ”Vanishing Gradient Problem.” Thus a fundamental question in the
field of neural networks is the question of whether an even better class of activation
functions that differ substantially from ReLU like and sigmoidal activation functions
exist. Inspired by the fact that hyper-strips allow smaller cuts to be made in the
input space than halfspaces, this paper proposed a new class of cone-like activation
functions. Cone-like activation functions use a hyper-strip to separate C (the set of
+
inputs that elicit a positive output) from other inputs. Since C is a hyper-strip the
+
XOR function can be learned by a single neuron with Cone-like activation functions.
The paper showed that many nonlinearly separable datasets can be separated with
fewer hyper-strips than half-spaces resulting in smaller ANNs. ANNs with Cone-like
activation functions are shown to achieve higher accuracies with significantly fewer
neurons on CIFAR-10 and Imagenette benchmarks. Results indicate that Cone-
like activation functions with larger derivatives than ReLU-like activation functions
speedup training and achieve higher accuracies.
9Appendix I: CNN architecture for CIFAR-10
1011Appendix II: CNN architecture for Imagenette
1213References
[1] X. Glorot, A. Bordes, Y. Bengio, Deep sparse rectifier neural networks, in: Pro-
ceedings of the fourteenth international conference on artificial intelligence and
statistics, JMLR Workshop and Conference Proceedings, 2011, pp. 315–323.
[2] V. Kunc, J. Kl´ema, Three decades of activations: A comprehensive survey of 400
activation functions for neural networks, arXiv preprint arXiv:2402.09092 (2024).
[3] K. Alex, Learning multiple layers of features from tiny images, https://www. cs.
toronto. edu/kriz/learning-features-2009-TR. pdf (2009).
[4] https://www.tensorflow.org/datasets/catalog/imagenette.
14