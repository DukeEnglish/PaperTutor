Edit-Your-Motion: Space-Time Diffusion Decoupling Learning for
Video Motion Editing
YiZuo LinglingLi LichengJiao
XidianUniversity XidianUniversity XidianUniversity
Xi’an,China Xi’an,China Xi’an,China
yiiizuo@163.com llli@xidian.edu.cn lchjiao@mail.xidian.edu.cn
FangLiu XuLiu WenpingMa
XidianUniversity XidianUniversity XidianUniversity
Xi’an,China Xi’an,China Xi’an,China
f63liu@163.com xuliu361@163.com wpma@mail.xidian.edu.cn
ShuyuanYang YuweiGuo
XidianUniversity XidianUniversity
Xi’an,China Xi’an,China
syyang@xidian.edu.cn ywguo@xidian.edu.cn
Source video Source Prompt: “A girl with a striped top and a black short skirt is dancing.”
Reference video Reference Prompt: “A girl with a pink top and a white dress is waving.”
Edited video Target Prompt: “A girl with a striped top and a black short skirt is waving.”
Figure1:Edit-Your-Motioneditsthemotionofthesourcevideotoalignthereferencevideowithoutchangingtheobject’s
contentorbackground.
Source videos
Reference videos
Edited videos
4202
yaM
7
]VC.sc[
1v69440.5042:viXraABSTRACT usingavideomotioneditingmodel,eliminatingthenecessityfor
Existingdiffusion-basedvideoeditingmethodshaveachievedim- complexsoftware.
pressiveresultsinmotionediting.Mostoftheexistingmethods Inpriorstudies,researchersprimarilyutilizedgenerativemeth-
focusonthemotionalignmentbetweentheeditedvideoandthe ods to create videos featuring specific actions, with few efforts
referencevideo.However,thesemethodsdonotconstraintheback- focusing on editing motions within a specific video. For exam-
groundandobjectcontentofthevideotoremainunchanged,which ple,severalpriorstudies[26,64,65]havefocusedonpose-guided
makesitpossibleforuserstogenerateunexpectedvideos.Inthis videogeneration,whichinvolvescreatingvideosthatalignwith
paper,weproposeaone-shotvideomotioneditingmethodcalled specifiedhumanposes.Otherstudies[9,17,25,35,57,66]togen-
Edit-Your-Motionthatrequiresonlyasingletext-videopairfor eratevideoswiththesamemotionbylearningthemotionfeatures
training.Specifically,wedesigntheDetailedPrompt-GuidedLearn- inthesourcevideo.Thesestudiesoperatewithinthetext-driven
ingStrategy(DPL)todecouplespatio-temporalfeaturesinspace- space-timediffusionmodelframework,engineeredtolearnthelink
timediffusionmodels.DPLseparateslearningobjectcontentand betweentextualpromptinputsandcorrespondingvideooutputs.
motion into two training stages. In the first training stage, we However,thespatialandtemporalfeaturesofthevideoarenot
focusonlearningthespatialfeatures(thefeaturesofobjectcon- separatedduringthetraining,whichmakesthementangled.The
tent)andbreakingdownthetemporalrelationshipsinthevideo spatialfeaturesareusuallyrepresentedastheobject’scontent,and
framesbyshufflingthem.WefurtherproposeRecurrent-Causal thetemporalfeaturesareusuallyrepresentedasthebackground
Attention (RC-Attn) to learn the consistent content features of andmotion.Thisentangledstateleadstooverlappingobjectcon-
theobjectfromunorderedvideoframes.Inthesecondtraining tent,backgroundandmotioninthespace-timediffusionmodel.As
stage,werestorethetemporalrelationshipinvideoframestolearn aresult,itischallengingtogeneratehighlyalignedvideoswith
thetemporalfeature(thefeaturesofthebackgroundandobject’s thefine-grainedforegroundandbackgroundofthesourcevideo,
motion).WealsoadopttheNoiseConstraintLosstosmoothout evenwhendetailedtextdescriptionsareused.Intuitively,thekey
inter-framedifferences.Finally,intheinferencestage,weinject tovideomotioneditingliesindecoupling[8,54,60]thetemporal
thecontentfeaturesofthesourceobjectintotheeditingbranch andspatialfeaturesofthespace-timediffusionmodel.
throughatwo-branchstructure(editingbranchandreconstruc- MotionEditor[45]firstexploredthisproblembyutilizingatwo-
tionbranch).WithEdit-Your-Motion,userscaneditthemotionof branchstructureintheinferencestagetodecoupletheobject’s
objectsinthesourcevideotogeneratemoreexcitinganddiverse contentandbackgroundinthefeaturelayerbytheobject’ssegmen-
videos.Comprehensivequalitativeexperiments,quantitativeex- tationmask.However,sincetheMotionEditor’smodellearnsthe
perimentsanduserpreferencestudiesdemonstratethatEdit-Your- relationshipbetweenthepromptandtheentirevideoduringthe
Motionperformsbetterthanothermethods.Codesareavailableat: trainingstage,thefeaturesofobjectsandthebackgroundoverlap
https://github.com/yiiizuo/Edit-Your-Motion. inthefeaturelayer.Thisoverlapmakesitchallengingtodistinguish
betweenthebackgroundandtheobjectsusingonlythesegmenta-
CCSCONCEPTS tionmask[23,39,50].
Inthispaper,weexploremethodstoseparatethelearningof
•Appliedcomputing→Mediaarts.
temporalandspatialfeaturesinspace-timediffusionmodels.To
this end, we propose a one-shot video motion editing method
KEYWORDS
namedEdit-Your-Motionthatrequiresonlyasingletext-videopair
VideoMotionEditing,Space-TimeDiffusionDecouplingLearning,
fortraining.Specifically,weproposetheDetailedPrompt-Guided
VideoUnderstand.
LearningStrategy(DPL),atwo-stagelearningstrategydesignedto
separatespatio-temporalfeatureswithinspace-timediffusionmod-
1 INTRODUCTION
els.Furthermore,weproposeRecurrent-CausalAttention(RC-Attn)
Diffusion-based[22,41,44,49,53]videomotioneditingaimsto asanenhancementoverSparse-CausalAttention.TheRecurrent-
control the motion (e.g., standing, dancing, running) of objects CausalAttentionallowsearlyframesinavideotoreceiveinfor-
in the source video based on text prompts or other conditions mationfromsubsequentframes,ensuringconsistentcontentof
(e.g.,depthmap,visibleedges,humanposes,etc),whilepreserving objectsthroughoutthevideowithoutaddingcomputationalbur-
theintegrityofthesourcebackgroundandobject’scontent.This den.Additionally,weconstructtheNoiseConstraintLoss[31]to
techniqueisespeciallyvaluableinmultimedia[6,10,21,33,52,56, minimizeinter-framedifferencesoftheeditedvideoduringthe
58,63],includingadvertising,artisticcreation,andfilmproduction. secondtrainingstage.
Itallowsuserstoeffortlesslymodifythemotionofobjectsinvideos DuringDPL,weusethespace-timediffusionmodel(inflatedU-
Net[37])asthebackboneandintegrateControlNet[61]tocontrol
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
thegenerationofmotion.Inthefirsttrainingstage,weactivate
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation Recurrent-CausalAttentionandfreezetheotherparameters.Then,
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe werandomlydisrupttheorderofframesinthesourcevideoand
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
maskthebackgroundtoguideRecurrent-CausalAttentiontofocus
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. onlearningthecontentfeaturesofobjects.Inthesecondtraining
ACMMM,2024,Melbourne,Australia stage,weactivateTemporalAttention[48]andfreezeotherparam-
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
eterstolearnmotionandbackgroundfeaturesfromorderedvideo
ACMISBN978-x-xxxx-xxxx-x/YY/MM
https://doi.org/10.1145/nnnnnnn.nnnnnnnEdit-Your-Motion:Space-TimeDiffusionDecouplingLearningforVideoMotionEditing ACMMM,2024,Melbourne,Australia
frames.Concurrently,NoiseConstraintLossisusedtominimize However,thesemethodslacktemporalmodeling,anditisdif-
thedifferencebetweenframes. ficulttomaintainconsistencybetweenframeswhengenerating
Intheinferencestage,wefirstperformaDDIM[42]inversionfor video.
thesourcevideotointroducelatentnoiseandfacilitatethesmooth-
nessoftheeditedvideo.Then,theposeinformationofthereference 2.2 Pose-guidedandMotion-Customization
videoisintroducedviaControlNet.Next,toensurethatthecontent VideoGeneration
oftheobjectsintheeditedvideoremainsconsistentwiththatof
Pose-guidedimageandvideogenerationisamethodtocontrol
thesourcevideo,weutilizeatwo-branchstructure(editbranchand
imageandvideogenerationbyaddingadditionalhumanposes.Con-
reconstructionbranch)similarto[45].However,unlikeMotionEd-
trolNet[61]referencesadditionalconditionsviaauxiliarybranches
itor,DPLdistinctlydecoupledspatialandtemporalfeaturesinto
toproduceimagesconsistentwiththeconditionmap.Follow-Your-
Recurrent-CausalAttentionandTemporalAttention,respectively.
Pose[26]controlsvideogenerationgivenhumanskeletons.Ituses
Therefore,weonlyinjectthekeyandvalueofRecurrent-Causal
atwo-stagetrainingtolearntoposeandcontroltemporalcon-
Attentionfromthereconstructionbranchintotheeditingbranch,
sistency.ControlVideo[64]isadaptedfromControlNetanduses
eliminatingtheneedforthesegmentationmask.
cross-frameinteractiontoconstrainappearancecoherencebetween
Inconclusion,ourcontributionsareasfollows:
frames.Control-A-Video[65]enhancesfaithfulnessandtemporal
• Wefurtherexploredhowtodecouplespatio-temporalfea- consistencybyfine-tuningtheattentionmodulesinboththediffu-
turesinvideomotioneditingexplicitlyandproposedaone- sionmodelsandControlNet.
shotvideomotioneditingmethodnamedEdit-Your-Motion. Unlike the pose-guided video generation model, the motion-
• WedesignedtheDetailedPrompt-GuidedLearningStrat- customizationvideogenerationmodelgeneratesvideoswiththe
egy(DPL),atwo-stagetrainingmethod.Itcandecouplethe samemotionbylearningthemotionfeaturesinthesourcevideo.
space-timediffusionmodel’soverlappingspatialandtempo- Customize-A-Video[35]designedanAppearanceAbsorbermodule
ralfeatures,therebyavoidinginterferencefrombackground todecomposethespatialinformationofmotion,thusdirectingthe
featuresduringtheeditingobject’smotion. TemporalLoRA[16]tolearnthemotioninformation.MotionCrafter
• WedesignedRecurrent-CausalAttentiontoassistDPLin [66]customizesthecontentandmotionofthevideobyinjectingmo-
learningthemorecomprehensivecontentofobjectsinthe tioninformationintoU-Net’stemporalattentionmodulethrough
firsttrainingstage.Inaddition,WeconstructedtheNoise aparallelspatial-temporalarchitecture.VMC[17]fine-tunesonly
ConstraintLosstosmoothoutinter-framedifferencesinthe thetemporalattentionlayerinthevideodiffusionmodeltoachieve
secondtrainingstage. successfulmotioncustomization.
• Weconductexperimentsonin-the-wildvideos,wherethe Unlikethesemethods,videomotioneditingrequirescontrolling
resultsshowthesuperiorityofourmethodcomparedwith themotionofthesourcevideoobjectwhilemaintainingitscontent
thestate-of-the-art. andbackground.
2.3 VideoEditing
2 RELATEDWORK
Thecurrentvideoeditingmodelscanbedividedintotwocategories:
Inthissection,weprovideabriefoverviewofthefieldsrelatedto
video content editing models [1, 5, 20, 24, 32, 51, 67] and video
videomotioneditingandpointouttheconnectionsanddifferences
motioneditingmodels[45].Thevideocontenteditingmodelis
betweenthemandvideomotionediting.
designedtomodifythebackgroundandobject’scontent(e.g.,the
sceneinthebackground,theclothescolour,thevehicle’sshape,
2.1 ImageEditing
etc.)inthesourcevideo.
Recently,alargeamountofworkhasbeendoneonimageediting Invideocontentediting,Tune-A-Video[51]introducestheOne-
usingdiffusionmodels[7,30,36].SDEdit[28]isthefirstmethod ShotVideoTuningtaskforthefirsttime,whichtrainsthespace-
forimagesynthesisandeditingbasedondiffusionmodels.Prompt- timediffusionmodelbyasingletext-videopair.FateZero[32]uses
to-Prompt[13]editsimagesbyreferencingcross-attentioninthe cross-attention maps to edit the content of videos without any
diffusionprocess.Plug-and-play[46]providesfine-grainedcontrol training.Mix-of-show[12]fine-tunethemodelthroughlow-rank
overthegenerativestructurebymanipulatingspatialfeaturesdur- adaptions[16](LoRA)topreventthecrashofknowledgelearned
inggeneration.UniTune[47]completestext-conditionedimage bythepre-trainedmodel.Someotherapproaches[2,5,20]use
editingtasksbyfine-tuning.Fornon-rigidlytransformedimage NLA[18]mappingtomapthevideotoa2Datlastodecouplethe
editing, Imagic [19] preserves the overall structure and compo- objectcontentfromthebackgroundtoeditthecontentoftheobject
sitionoftheimagebylinearlyinterpolatingbetweentexts,thus effectively.
accomplishingnon-rigideditingwhile.Masactrl[4]convertsself- In video motion editing, MotionEditor [45] uses the object’s
attentiontomutualself-attentionfornon-rigidimageediting.On segmentationmasktodecouplethecontentandbackgroundinthe
theotherhand,InstructPix2Pix[3]hasdevisedamethodofedit- featurelayer.Contentfeaturesaretheninjectedintotheediting
ingimagesbywritteninstructionsratherthantextualdescriptions branchtomaintaincontentconsistency.Sincetheobjectandthe
ofimagecontent.Unliketext-drivenimageediting,DreamBooth backgroundoverlapinthefeaturelayer,itisdifficulttoaccurately
[38]generatesnewimageswiththemeattributesbyusingseveral separatetheobject’scontentfromthebackgroundfeatureswith
differentimagesofagiventheme. thesegmentationmask.ACMMM,2024,Melbourne,Australia YiZuo,LinglingLi,LichengJiao,FangLiu,XuLiu,WenpingMa,ShuyuanYang,andYuweiGuo
Ourapproachdecouplestheobjectfromthebackgroundduring LatentDiffusionModels.LatentDiffusionmodels(LDM)[29,
thetrainingstageanddirectsRC-AttnandTemporalAttentionto 36,59]isanewlyintroducedvariantofDDPMthatoperatesin
learnspatialandtemporalfeatures,respectively.Thisensuresthat the latent space of the autoencoder. Specifically, the encoder E
thesourcevideocontentisaccuratelyinjected. compressestheimagetolatentfeatures𝒛=E(𝒙).Thenperforms
adiffusionprocessover𝑧,andfinallyreconstructslatentfeatures
3 METHOD back into pixel space using the decoder D. The corresponding
objectivecanberepresentedas:
In video motion editing, the focus is on decoupling the spatio-
tem Tp oo tr ha il sf ee na dtu ,r we eso pf roth pe osd eiff Eu ds iti -o Yn oum ro -Mde ol t.
ion,aone-shotvideomo-
𝐿 𝐿𝐷𝑀 =E E(𝑥),𝜖∼N(0,1),𝑡(cid:104) ∥𝜖−𝜖 𝜃(𝑧 𝑡,𝑡)∥2 2(cid:105) . (4)
tioneditingmethodtrainedonlyonapairofsourceandreference Text-to-VideoDiffusionModels.Text-to-VideoDiffusionModels
videos.Specifically,wedesigntheDetailedPrompt-GuidedLearn- [43]traina3DUNet𝜖3𝐷 withtextprompts𝑐 asaconditionto
𝜃
ingstrategy(DPL),atwo-stagelearningstrategycapableofdecou- generatevideosusingtheT2Vmodel.Giventhe𝐹 frames𝒙1...𝐹 of
plingspatio-temporalfeaturesinthespace-timediffusionmodel.In
avideo,the3DUNetistrainedby
thefirsttrainingstage,weshufflethevideoframestodisruptthe
t lee am rp no ir na tl er ne tl la yti so pn as th iaip lfo ef at th ue rev sid (oe bo j. eT ch te cn o, nm tea ns tk )ft rh oe mba tc hk eg uro nu on rdd ea rn edd 𝐿 𝑇2𝑉 =E E(𝑥1...𝐹),𝜖∼N(0,1),𝑡,𝑐 (cid:20)(cid:13) (cid:13) (cid:13)𝜖−𝜖 𝜃3𝐷 (𝑧 𝑡1...𝐹,𝑡,𝑐)(cid:13) (cid:13) (cid:13)2 2(cid:21) , (5)
f inra sm tee as d. oW fe Spfu ar rsth e-e Cr ap ur so ap lo Ase ttR ene tc iu or nre tont c- oC na su ts ra ul cA tct ote nn st ii so ten n( tR fC ea-A tut rt en s) where𝑧 𝑡1...𝐹 isthelatentfeaturesof𝒙1...𝐹 ,𝑧 𝑡1...𝐹 =E(𝒙1...𝐹).
ofobjectsoverthewholesequence.Inthesecondtrainingstage,we
3.2 Recurrent-CausalAttention
recoverthetemporalrelationshipsinthevideoframestolearnthe
temporalfeatures(thebackgroundandobjectmotion).Tosmooth LikeTune-A-Video[51],weusetheinflatedU-Netnetwork(space-
outtheinter-framedifferences,wealsoconstructNoiseConstraint timediffusionmodel)asthebackboneofEdit-Your-Motion,con-
Loss.Finally,intheinferencestage,weusethedeconstructionwith sistingofstacked3Dconvolutionalresidualblocksandtransform
a two-branch structure [66] (reconstruction branch and editing blocks.EachtransformerblockconsistsofSparse-CausalAtten-
branch).Sincethespatialandtemporalfeatureshavebeendecou- tion,CrossAttention,TemporalAttention,andaFeed-Forward
pledinthetrainingstage,weobtainthebackgroundandmotion Network(FFN).Tosavecomputationaloverhead,Tune-A-Video
featuresintheeditingbranchandinjectthecontentfeaturesofthe usesthecurrentframelatent𝑧 𝑣𝑖 ∈ (cid:8)𝑧 𝑣 0,...,𝑧 𝑣𝑖𝑚𝑎𝑥(cid:9) asthequery
objectsinthereconstructionbranchintotheeditingbranch.Fig.2 forSparse-CausalAttention.Meanwhile,thepreviousframelatent
illustratesthepipelineofEdit-Your-Motion. 𝑧 𝑣𝑖−1 iscombinedwiththefirstframelatent𝑧 𝑣
1
toobtainthekey
TointroduceourproposedEdit-Your-Motion,wefirstintroduce andvalue.Thespecificformulaisasfollows:
thebasicsofthetext-videodiffusionmodelinSec.3.1.Then,Sec.3.2
Ain ft tr eo rd tu hc ae ts ,io nu Sr ep cr .o 3p .3o ,s oe ud rR pe rc ou pr ore sn edt-C Da eu tas ia ll edAt Pt re on mtio pn t-G(R uC id-A edtt Len eati ro n) -. 𝑄 =𝑊𝑄𝑧 𝑣𝑖,𝐾 =𝑊𝐾 (cid:2)𝑧 𝑣 1,𝑧 𝑣𝑖−1(cid:3),𝑉 =𝑊𝑉 (cid:2)𝑧 𝑣 1,𝑧 𝑣𝑖−1(cid:3), (6)
ingstrategyandNoiseConstraintLossaredescribed.Finally,we where [·] denotesconcatenationoperation.where𝑊𝑄 ,𝑊𝐾 and
willintroducetheinferencestageinSec.3.4. 𝑊𝑉 areprojectionmatrices.However,becausethereislessinfor-
mationintheearlyframesofavideo,Sparse-CausalAttentiondoes
3.1 Preliminaries notconsidertheconnectionwiththesubsequentframes.Asaresult,
DenoisingDiffusionProbabilisticModels.Thedenoisingdif- itmayleadtoinconsistenciesbetweenthecontentatthebeginning
fusionprobabilisticmodels[11,14,27,55](DDPMs)consistsofa andtheendofthevideo.
forwarddiffusionprocessandareversedenoisingprocess.During Tosolvethisproblem,weproposeasimpleRecurrent-CausalAt-
theforwarddiffusionprocess,itgraduallyaddsnoise𝜖toaclean tentionwithnoincreaseincomputationalcomplexity.InRecurrent-
image𝒙0 ∼𝑞(𝒙0) withtimestep𝑡,obtaininganoisysample𝑥 𝑡. CausalAttention,keyandvalueareobtainedbycombiningthe
Theprocessofaddingnoisecanberepresentedas:
previousframelatent𝑧 𝑣𝑖−1withthecurrentframelatent𝑧 𝑣𝑖,not𝑧
𝑣 1
𝑞(𝒙𝑡|𝒙𝑡−1)=N(𝒙𝑡|√︁ 1−𝛽 𝑡𝒙𝑡−1,𝛽 𝑡I), (1) w ari eth ob𝑧 t𝑣 a𝑖− in1. eN do frt oab mly t, ht ehe lak stey fraa mnd ev laa tl eu ne to 𝑧f 𝑣𝑖t 𝑚h 𝑎e 𝑥fi wrs it thfr ta hm ee fil ra st te fn rt am𝑧 𝑣 e1
where𝛽 𝑡 ∈ (0,1)isavarianceschedule.Theentireforwardprocess latent𝑧 𝑣 1.Thisallowstheobject’scontenttopropagatethroughout
thevideosequencewithoutaddinganycomputationalcomplexity.
ofthediffusionmodelcanberepresentedasaMarkovchainfrom
TheformulaforRecurrent-CausalAttentionisasfollows:
time𝑡 totime𝑇,
𝑇 𝑄 =𝑊𝑄𝑧 𝑣𝑖, (7)
(cid:214)
𝑞(𝒙1:𝑇)=𝑞(𝒙0) 𝑞(𝒙𝑡|𝒙𝑡−1). (2)
Then,inreverseprocessing,no𝑡 i= s1
eisremovedthroughadenois- 𝐾
=(cid:40) 𝑊𝑊 𝐾𝐾 (cid:2)(cid:2) 𝑧𝑧 𝑣𝑖−
,1
𝑧,𝑧 𝑣 (cid:3)𝑖(cid:3) 𝑒if 𝑙𝑠𝑖 𝑒<𝑖 𝑚𝑎𝑥
, (8)
ingautoencoders𝜖 𝜃(𝑥 𝑡,𝑡) togenerateacleanimage.Thecorre-
𝑣
0
𝑣𝑖
spondingobjectivecanbesimplifiedto:
𝑉
=(cid:40) 𝑊𝑉 (cid:2)𝑧 𝑣𝑖−1,𝑧 𝑣𝑖(cid:3) if𝑖 <𝑖
𝑚𝑎𝑥
. (9)
𝐿 𝐷𝑀 =E 𝑥,𝜖∼N(0,1),𝑡 (cid:2) ∥𝜖−𝜖 𝜃(𝑥 𝑡,𝑡)∥2 2(cid:3). (3) 𝑊𝑉 (cid:2)𝑧 𝑣 0,𝑧 𝑣𝑖(cid:3) 𝑒𝑙𝑠𝑒Edit-Your-Motion:Space-TimeDiffusionDecouplingLearningforVideoMotionEditing ACMMM,2024,Melbourne,Australia
The First Training Stage: Learning Spatial Features from Inference Stage: A Two-Branch
Shuffled Images Structure that Injects Spatial Features
Source Reference
video video
Unordered P
a
Frames
nn nn
ControlNet
nn CCttttAA
-- ss ss
oottttAA
--
pptttt mmAA
-- P a P t
RR rr CC ee TT
P
a “A boy wearing black clothes and gray pants.”
C
o
The Second Trainin Og rS dt ea rg ee d: VL ie da er on i Fn rg
a
T me em sporal Feature from h c
n
a Br h c
n
eNlortn
n o
itc k
ga Br t
u r ts
n
o
V n iti
Ed
c
e
R P
t
P
Ordered s
Frames
nn nn “A boy
ControlNet
nn CCttttAA
-- ss ss
oottttAA
--
pptttt mmAA
--
w ce la or ti hn eg
s
b al na dc k
RR rr CC ee TT gray pants is
dancing.”
S
sr
P s“A boy wearing black clothes and gray Pants is playing basketball.” Edited video
Figure 2: The overall pipline of Edit-Your-Motion. Edit-Your-Motion decouples spatial features (object appearance) from
temporalfeatures(backgroundandmotioninformation)ofthesourcevideousingtheDetailedPrompt-GuidedLearning
Strategy(DPL).Inthefirsttrainingstage,Recurrent-Causalattention(RC-Attn)isguidedtolearnspatialfeatures.Inthesecond
trainingstage,TemporalAttention(Temp-Attn)isguidedtolearntemporalfeatures.Duringinference,thespatialfeaturesof
thesourcevideoareinjectedintotheeditingbranchthroughthekeyandvalueofRecurrent-CausalAttention,thuskeeping
thesourcecontentandbackgroundunchanged.
Overall, Recurrent-Causal Attention enables early frames to In order to be able to decouple overlapping spatio-temporal
acquire more comprehensive content information compared to features,wedesigntheDetailedPrompt-GuidedLearningStrategy
Sparse-CausalAttention,byestablishingalinktothelastframein (DPL).
thefirstframe. DPLisdividedintotwotrainingstages:(1)TheFirstTraining
Stage:LearningSpatialFeaturesfromShuffledImages,and(2)The
SecondTrainingStage:LearningTemporalFeaturesfromOrdered
videoframes.Next,wewilldescribethetwostagesindetail.
3.3 TheDetailedPrompt-GuidedLearning
TheFirstTrainingStage:LearningSpatialFeaturesfromShuf-
Strategy fledImages.Inthisstage,thespace-timediffusionmodelfocuses
Thepurposeofdiffusion-basedvideomotioneditingistocontrol onlearningthespatialfeaturesofthesourceobject.First,wedisrupt
themotionofobjectsinthesourcevideobasedonareferencevideo theorderofvideoframestodestroytheirtemporalinformation
withapromptandtoensurethatthecontentandbackgroundofthe
andgenerateunorderedvideoframesU ={𝑢 𝑖|𝑖 ∈ [1,𝑛]},where𝑛
objectsremainunchanged.Thekeyliesindecouplingthediffusion isthelengthofthevideo.
model’soverlappingtemporalandspatialfeatures.MotionEditor Ifwetrainthemodeldirectlyusingunorderedframes,thefea-
usestheobject’ssegmentationmasktodecoupletheobjectcontent turesoftheobjectandthebackgroundwilloverlap.Suchoverlap-
andthebackgroundinthefeaturelayer.However,thedecoupled pingspatio-temporalfeaturesarechallengingtodecouplelaterand
featuresalsooverlapsincethespatio-temporalfeatureshavebeen willleadtointerferencefrombackgroundfeatureswhencontrol-
obfuscatedinthemodel. lingobjectmotion.Therefore,weuseanexistingsegmentation
S
sr
C
C
rf
sr
S
C
rf
rfACMMM,2024,Melbourne,Australia YiZuo,LinglingLi,LichengJiao,FangLiu,XuLiu,WenpingMa,ShuyuanYang,andYuweiGuo
networktoextractthesegmentationmask𝑀 fortheunordered
videoframes.Therefore,weuseanexistingsegmentationnetwork 𝐶 𝑟𝑓 =𝐶𝑜𝑛𝑡𝑟𝑜𝑙𝑁𝑒𝑡(𝑆 𝑟𝑓,𝑃 𝑡), (16)
toextractthesegmentationmaskMforthevideoframesandmask where𝐶 𝑟𝑓 istheposefeatureofthereferencevideotobeusedto
outthebackgroundas:
guidethegenerationofmotionintheeditingbranch.Next,wewill
injectthespatialfeaturesfromthereconstructionbranchintothe
UM =U·M, (10) editingbranch.Duetodisruptingthetimerelationshipandmask
thebackgroundinthefirsttrainingstageofDPL.Therefore,we
Z 𝑡M =E(UM), (11) directlyinjectthekeysandvaluesoftheRC-Attninthereconstruc-
whereZ 𝑡MisthelatentfeaturesofUM,andE(·)isencoder.Then, tionbranchintotheeditingbranchwithoutneedingsegmentation
masks.Thespecificformulacanbewrittenas:
weutilizeanexistingskeletonextractionnetworktoobtainthe
h alu om ngan ws ik thele thto en p𝑆 r𝑠 o𝑟 mi pn tt 𝑃h 𝑎e .sourcevideoandfeeditintoControlNet 𝐾𝑟 =𝑊𝐾𝑧𝑠 𝑣𝑖,𝑉𝑟 =𝑊𝑉𝑧𝑠 𝑣𝑖, (17)
𝐶 𝑠𝑟 =𝐶𝑜𝑛𝑡𝑟𝑜𝑙𝑁𝑒𝑡(𝑆 𝑠𝑟,𝑃 𝑎), (12)
𝐾𝑒 = (cid:104) 𝑊𝐾𝑧𝑒 𝑣𝑖−1,𝑊𝐾𝑧𝑒 𝑣𝑖,𝐾𝑟(cid:105) ,𝑉𝑒 = (cid:104) 𝑊𝑉𝑧𝑒 𝑣𝑖−1,𝑊𝑉𝑧𝑒 𝑣𝑖,𝑉𝑟(cid:105) , (18)
𝑉𝑟 where𝑒representstheeditingbranch.𝑟 representstherecon-
where𝐶 𝑠𝑟 istheposefeatureofsourcevideo.Next,wewillfreeze
structionbranch.Intheend,weobtainedtheeditedvideo.
otherparametersandonlyactivateRecurrent-CausalAttention.
Finally,wewill𝑃 𝑎and𝐶 𝑠𝑟 intothespace-timediffusionmodelfor
4 EXPERIMENTAL
training.Thereconstructionlosscanbewrittenasfollows:
4.1 ImplementationDetails
𝐿 𝑟𝑒𝑐 =E 𝑧𝑚
𝑡
,𝜖∼N(0,1),𝑡,𝑃𝑎,𝐶𝑠𝑟 (cid:20)(cid:13) (cid:13) (cid:13)𝜖−𝜖 𝜃3𝐷 (𝑧𝑚 𝑡 ,𝑡,𝑃 𝑎,𝐶 𝑠𝑟)(cid:13) (cid:13) (cid:13)2 2(cid:21) . (13) O Mu or dep lro [3p 6o ]s (e Sd taE bd ei lt- DYo iffu ur s-M iono )t .io Tn heis db aa ts ae id no tn hit sh ae rtL ia clt een ct omDi eff su fs ri oo mn
TaichiHD[40]andYouTubevideodatasets,inwhicheachvideohas
TheSecondTrainingStage:LearningTemporalFeaturesfrom
aminimumof70frames.Duringtraining,wefinetune300steps
OrderedVideoFrames.Unlikethefirsttrainingstage,werestored
foreachofthetwotrainingstagesatalearningrateof3×10−5.
thetemporalrelationshipofvideoframes.Then,guidethespace-
Forinference,weusedtheDDIMsampler[42]withnoclassifier
timediffusionmodeltolearnthetemporalfeaturesofmotionand
backgroundfromorderedvideoframesV ={𝑣 𝑖|𝑖 ∈ [1,𝑛]}. guidance[15]inourexperiments.Foreachvideo,thefine-tuning
Specifically,Weconstructanewprompt𝑃 𝑠,whichaddsadescrip- takesabout15minuteswithasingleNVIDIAA100GPU.
tionofthemotionto𝑃 𝑎.Then,TemporalAttentionisactivated
4.2 ComparisonsMethod
to learn motion features while other parameters are frozen. To
smooththevideo,weaddedNoiseConstraintLoss[31].Thenoise TodemonstratethesuperiorityofourEdit-Your-Motion,wehave
constraintlosscanbewrittenasfollows: selectedmethodsfrommotioncustomization,pose-guidedvideo
generation, video content editing, and video motion editing as
𝐿 𝑛𝑜𝑖𝑠𝑒 =
𝑛−1 1𝑛 ∑︁ 𝑖=− 11(cid:13)
(cid:13) (cid:13)𝜖
𝒛𝑓𝑖
𝑡 −𝜖
𝒛𝑓𝑖 𝑡+1(cid:13)
(cid:13)
(cid:13)2
2, (14)
wc mo
o
om
r
dkp ea lor tfi ooso
n
3n De-m
s th
oe ot hh
t
avo nid
d
ds
e
l. eo(1
te
h)
d
eT itu
vin
in dge e.- oIA
t
t-
i
aV
n sfl
kid
a
.te
(e
2o
s
)[
a
M5 p1 or] te: i-T oth
r
nae Einfi der ids tt oTp
r2
1r Ie [ds 4e
i
5ffn ]ut :s
s
Tit
o
hh
n
ee
where𝑓 𝑖 denotethe𝑖-thframeofthevideo.𝜖
𝒛𝑓𝑖
𝑡
isthenoisepredic- firstexaminestheworkofvideomotioneditingwhilemaintaining
tionattimestep𝑡.Thetotallossforthesecondtrainingstageis theobjectcontentandbackgroundunchanged.(3)Follow-Your-
constructedasfollows: Pose[26]:Generatingpose-controllablevideosusingtwo-stage
training.(4)MotionDirector[66]:Generatemotion-alignedvideos
𝐿 𝑇𝑜𝑡𝑎𝑙 =(1−𝜆)𝐿 𝑛𝑜𝑖𝑠𝑒 +𝜆𝐿 𝑟𝑒𝑐, (15) bydecouplingappearanceandmotioninreferencevideosforvideo-
where𝐿 𝑟𝑒𝑐 isconstructedfromorderedvideoframesV without motion-customization.
segmentationmask𝑀.𝜆issetto0.9.
4.3 Evaluation
3.4 InferencePipelines Ourmethodcaneditthemotionofobjectsinthesourcevideobyus-
Intheinferencestage,wefirstextractthehumanskeleton𝑆 𝑟𝑓 from ingthereferencevideoandpromptingwithoutchangingtheobject
contentandthebackground.Fig.4showssomeofourexamples.
thereferencevideotoguidemotiongeneration.Then,toensure
Ascanbeseen,ourproposedEdit-Your-Motionaccuratelycontrols
thattheobject’scontentandbackgroundareunchanged,weusea
themotionandpreservestheobject’scontentandbackgroundwell.
two-brancharchitecture(reconstructionbranchandeditingbranch)
Themorecasesareintheappendix.
similarto[45]toinjecttheobject’scontentandbackgroundfeatures
QualitativeResults.Fig.3showstheresultsofthevisualcom-
intotheeditingbranch.
Specifically,wefirstinputthelatentnoise𝑧𝑠
fromthesource
parisonofEdit-Your-Motionwithothercomparisonmethodson25
videoDDIMinversionand𝑃 𝑎 intothereconstructionbranch.Si- in-the-wildcases.AlthoughFollow-Your-PoseandMotionDirector
multaneouslyinput𝑧𝑠 and𝑃 𝑡 intotheeditingbranch.Then,we canalignwellwiththemotionofthereferencevideo,itisdifficultto
willinputthehumanskeleton𝑆
𝑟𝑓
fromthereferencevideoand𝑃
𝑡 1Sincethearticle’scodeisnotprovided,theexperimentalresultsinthispaperare
intoControlNettoobtainfeature𝐶 𝑟𝑓 as: obtainedbyreplication.Edit-Your-Motion:Space-TimeDiffusionDecouplingLearningforVideoMotionEditing ACMMM,2024,Melbourne,Australia
0 2 22 4 8 12 16
6
Source video
Reference video
Follow-Your-Pose
MotionDirector
Tune-A-Video
MotionEditor
Ours
A girl in a plaid top and black skirt is dancing A boy with a black top and gray pants is playing basketball
practicing wugong. dancing.
0 2 22
Figure3:Qualitativecomparisonwithstate-of-the-artme6thods.Comparedtootherbaselines,Edit-Your-Motionsuccessfully
achievesmotionalignmentwiththereferencevideoandmaintainsthecontentconsistencyofthebackgroundandobjects.
Source video
maintainconsistencybetweentheobjectcontentandbackground Table1:QuantitativeevaluationusingCLIPandLPIPS.TA,
inboththesourceandreferencevideos.Itdemonstratesthatgen- TC, L-N, L-S represent Text Alignment, Temporal Consis-
eratingspecificbackgroundandcontentusingonlytextpromptsis tency,LPIPS-NandLPIPS-S,respectively.
Reference video
difficult.Tune-A-VideoandMotionEditorshownoticeablecontent
changes.Inaddition,MotionEditorshowsmotionoverlap(arms) Method TA↑ TC↑ L-N↓ L-S↓
causedbyusingofthesegmentationmasktodecoupleoverlapping
Follow-Your-Pose[26] 0.236 0.913 0.213 0.614
features.Incontrasttotheabove,ourproposedEdit-Your-Motion
Follow Your Pose MotionDirector[66] 0.239 0.872 0.141 0.430
alignsthemotionoftheeditedvideoandthereferencevideowell
Tune-A-Video[51] 0.278 0.934 0.137 0.359
andpreservesthecontentandbackgroundoftheobjectsinthe
MotionEditor[45] 0.286 0.948 0.102 0.300
sourcevideointact.Thisalsodemonstratestheeffectivenessofour
Ours 0.289 0.950 0.109 0.276
methodinvideomotionediting.
MotionDirector
Quantitativeresults.Weevaluatethemethodswithautomatic
evaluationsandhumanevaluationson25in-the-wildcases.
AutomaticEvaluations.Toquantitativelyassessthedifferences
betweenourproposed TuE nd e-i At -- VY io du eor-Motionandothercomparative Similaritybetweeneditedframesandsourceframes.Table1shows
methods,weusethefollowingmetricstomeasuretheresults:(1) thequantitativeresultsofEdit-Your-Motionwithothercomparative
TextAlignment(TA).WeuseCLIP[34]tocomputetheaverage methods.TheresultsshowthatEdit-Your-Motionoutperformsthe
cosinesimilaritybetweenthepromptandtheeditedframes.(2) othermethodsonallmetrics.
TemporalConsistency(MToCti)o.nWEdeitourseCLIPtoobtainimagefeatures UserStudy.Weinvited70participantstoparticipateintheuser
andcomputetheaveragecosinesimilaritybetweenneighbouring study.Eachparticipantcouldseethesourcevideo,thereference
videoframes.(3)LPIPS-N(L-N):WecalculateLearnedPerceptual video,andtheresultsofourandothercomparisonmethods.For
ImagePatchSimilarity[62]betweeneditedneighbouringframes. eachcase,wecombinedtheresultsofEdit-Your-Motionwiththe
(4)LPIPS-S(L-S):WecalcOuularsteLearnedPerceptualImagePatch resultsofeachofthefourcomparisonmethods.Then,wesetthreeACMMM,2024,Melbourne,Australia YiZuo,LinglingLi,LichengJiao,FangLiu,XuLiu,WenpingMa,ShuyuanYang,andYuweiGuo
A woman in a blue top and white skirt is waving her hand dancing. A girl with a black top and black skirt is dancing practicing Tai Chi.
A boy wearing black clothes and gray pants is playing basketball dancing. A man with a dark green top and black pants is standing practicing Tai Chi.
Figure4:SomeexamplesofmotioneditingresultsforEdit-Your-Motion. 22
6 12 18
0
Table2:UserStudy.Higherindicatestheusersprefermoreto thatRC-Attncanbetterestablishcontentconsistencyovertheen-
ourMotionEditor.TA,CA,andMArepresentTextAlignment, tiresequencethanwithSparseAttention.Incolumn4,w/oNoise
ContentAlignment,andMotionAlignment,respectively. ConstraintLoss(NCL)affectsthesmoothnessbetweenframes,caus-
ingthebackgroundtobeinconsistentbetweenframes.Incolumn5,
Method TA CA MA wetrainRC-AttnandTemporalAttentioninatrainingstage.How-
ever,thelackofspatio-temporaldecouplingresultsinbackground
Follow-Your-Pose[26] 87.142% 96.663% 90.953%
andobjectcontentinterfering,generatingundesirableeditedvideos.
MotionDirector[66] 94.522% 96.190% 86.188%
Atthesametime,italsodemonstratestheeffectivenessofDPLin
Tune-A-Video[51] 78.810% 82.145% 84.047%
decouplingtimeandspace.
MotionEditor[45] 76.428% 82.380% 80.950%
questionstoevaluateTextAlignment,ContentAlignmentandMo-
tionAlignment.Thethreequestionsare"Whichismorealigned
tothetextprompt?","Whichismorecontentalignedtothesource
video?"and"Whichismoremotionalignedtothereferencevideo?".
Table2showsthatourmethodoutperformstheothercompared
methodsinallthreeaspects.
4.4 AblationStudy
Toverifytheeffectivenessoftheproposedmodule,weshowthe
resultsoftheablationexperimentsinFig.5.Incolumn3,wereplace
RC-AttnwithSparseAttention,whichmakesthefirstframeincon-
sistentwiththeobjectcontentinthesubsequentframes.Thisshows4488
4400 4444 5588
Edit-Your-Motion:Space-TimeDiffusionDecouplingLearningforVideoMotionEditing ACMMM,2024,Melbourne,Australia
Source video
Source video Reference video w/o RCA w/o NCL w/o DPT Edit-Your-Motion
[5] WenhaoChai,XunGuo,GaoangWang,andYanLu.2023. Stablevideo:Text-
drivenconsistency-awarediffusionvideoediting.InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision.23040–23050.
Reference video 40 [6] YangChen,YingweiPan,YehaoLi,TingYao,andTaoMei.2023. Control3d:
Towardscontrollabletext-to-3dgeneration.InProceedingsofthe31stACMInter-
nationalConferenceonMultimedia.1148–1156.
[7] Florinel-AlinCroitoru,VladHondru,RaduTudorIonescu,andMubarakShah.
2023.Diffusionmodelsinvision:Asurvey.IEEETransactionsonPatternAnalysis
w/o RCA 44 andMachineIntelligence(2023).
[8] HanFang,KejiangChen,YupengQiu,JiayangLiu,KeXu,ChengfangFang,
WeimingZhang,andEe-ChienChang.2023.DeNoL:AFew-Shot-Sample-Based
DecouplingNoiseLayerforCross-channelWatermarkingRobustness.InPro-
ceedingsofthe31stACMInternationalConferenceonMultimedia.7345–7353.
w/o loss2 48 [9] RinonGal,YuvalAlaluf,YuvalAtzmon,OrPatashnik,AmitHBermano,Gal
Chechik,andDanielCohen-Or.2022.Animageisworthoneword:Personalizing
text-to-imagegenerationusingtextualinversion.arXivpreprintarXiv:2208.01618
(2022).
[10] YifanGao,JinpengLin,MinZhou,ChuanbinLiu,HongtaoXie,TiezhengGe,
w/o DPT 58 andYuningJiang.2023.TextPainter:MultimodalTextImageGenerationwith
Visual-harmonyandText-comprehensionforPosterDesign.InProceedingsof
the31stACMInternationalConferenceonMultimedia.7236–7246.
A girl with a black top and black shorts is waving her hand dancing. [11] DeepanwayGhosal,NavonilMajumder,AmbujMehrish,andSoujanyaPoria.
2023.Text-to-AudioGenerationusingInstructionGuidedLatentDiffusionModel.
ACE InProceedingsofthe31stACMInternationalConferenceonMultimedia.3590–3598.
Figure5:Someexamplesofvideomotioneditingresultsfor [12] YuchaoGu,XintaoWang,JayZhangjieWu,YujunShi,YunpengChen,Zihan
Edit-Your-Motion. Fan,WuyouXiao,RuiZhao,ShuningChang,WeijiaWu,etal.2024.Mix-of-show:
Decentralizedlow-rankadaptationformulti-conceptcustomizationofdiffusion
models.AdvancesinNeuralInformationProcessingSystems36(2024).
[13] AmirHertz,RonMokady,JayTenenbaum,KfirAberman,YaelPritch,andDaniel
5 CONCLUSION Cohen-Or.2022.Prompt-to-promptimageeditingwithcrossattentioncontrol.
arXivpreprintarXiv:2208.01626(2022).
Inthispaper,weexploremethodstoseparatethelearningoftempo-
[14] JonathanHo,AjayJain,andPieterAbbeel.2020.Denoisingdiffusionprobabilistic
ralandspatialfeaturesinspace-timediffusionmodels.Tothisend, models.Advancesinneuralinformationprocessingsystems33(2020),6840–6851.
weproposeaone-shotvideomotioneditingmethodcalledEdit- [15] JonathanHoandTimSalimans.2022.Classifier-freediffusionguidance.arXiv
preprintarXiv:2207.12598(2022).
Your-Motionthatrequiresonlyasingletext-videopairfortraining. [16] EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,Shean
Specifically,wedesigntheDetailedPrompt-GuidedLearningStrat- Wang,LuWang,andWeizhuChen.2021.Lora:Low-rankadaptationoflarge
languagemodels.arXivpreprintarXiv:2106.09685(2021).
egy(DPL)todecouplethediffusionmodel’sspace-timefeaturesin
[17] HyeonhoJeong,GeonYeongPark,andJongChulYe.2023.VMC:VideoMotion
twotrainingstages.Furthermore,weproposeRecurrent-CausalAt- CustomizationusingTemporalAttentionAdaptionforText-to-VideoDiffusion
tention(RC-Attn)asanenhancementoverSparse-CausalAttention. Models.arXivpreprintarXiv:2312.00845(2023).
[18] YoniKasten,DolevOfri,OliverWang,andTaliDekel.2021. Layeredneural
Inthefirsttrainingstage,RC-Attnfocusesonlearningthespatial
atlasesforconsistentvideoediting.ACMTransactionsonGraphics(TOG)40,6
featurebyshufflingthetemporalrelations.Inthesecondtraining (2021),1–12.
stage,weguidetheTemporalAttentiontolearntemporalfeatures. [19] BahjatKawar,ShiranZada,OranLang,OmerTov,HuiwenChang,TaliDekel,
InbarMosseri,andMichalIrani.2023.Imagic:Text-basedrealimageeditingwith
Inaddition,NoiseConstraintLossisconstructedtosmooththe diffusionmodels.InProceedingsoftheIEEE/CVFConferenceonComputerVision
video. In the inference stage, we utilize a two-branch structure andPatternRecognition.6007–6017.
[20] Yao-ChihLee,Ji-ZeGenevieveJang,Yi-TingChen,ElizabethQiu,andJia-Bin
toinjectspatialfeaturesintotheeditingbranchtogenerateedit
Huang.2023.Shape-awaretext-drivenlayeredvideoediting.InProceedingsofthe
videos.Extensiveexperimentsdemonstratetheeffectivenessofour IEEE/CVFConferenceonComputerVisionandPatternRecognition.14317–14326.
proposedEdit-Your-Action. [21] JinpengLin,MinZhou,YeMa,YifanGao,ChenxiFei,YangjianChen,ZhangYu,
andTiezhengGe.2023. AutoPoster:AHighlyAutomaticandContent-aware
LimitationsandFutureWork.AlthoughourproposedEdit-Your-
DesignSystemforAdvertisingPosterGeneration.InProceedingsofthe31stACM
Motionachievescompellingresultsinvideomotionediting,two- InternationalConferenceonMultimedia.1250–1260.
stagetrainingconsumesmorecomputationalresources.Therefore, [22] JinLiu,XiWang,XiaomengFu,YeshengChai,CaiYu,JiaoDai,andJizhongHan.
2023.MFR-Net:Multi-facetedResponsiveListeningHeadGenerationviaDenois-
howtoperformvideomotioneditingwithlimitedcomputational ingDiffusionModel.InProceedingsofthe31stACMInternationalConferenceon
resourcesstilldeservesfurtherexplorationinfutureresearch.We Multimedia.6734–6743.
[23] JiamingLiu,YueWu,MaoguoGong,QiguangMiao,WenpingMa,andCaiXu.
alsoexpectvideomotioneditingtoreceivemoreattentionfrom
2023. ExploringDualRepresentationsinLarge-ScalePointClouds:ASimple
researchers. WeaklySupervisedSemanticSegmentationFramework.InProceedingsofthe31st
ACMInternationalConferenceonMultimedia.2371–2380.
REFERENCES [24] ShaotengLiu,YuechenZhang,WenboLi,ZheLin,andJiayaJia.2023.Video-p2p:
Videoeditingwithcross-attentioncontrol.arXivpreprintarXiv:2303.04761(2023).
[1] JianhongBai,TianyuHe,YuchiWang,JunliangGuo,HaojiHu,ZuozhuLiu,and [25] JianMa,JunhaoLiang,ChenChen,andHaonanLu.2023.Subject-diffusion:Open
JiangBian.2024.UniEdit:AUnifiedTuning-FreeFrameworkforVideoMotion domainpersonalizedtext-to-imagegenerationwithouttest-timefine-tuning.
andAppearanceEditing.arXivpreprintarXiv:2402.13185(2024). arXivpreprintarXiv:2307.11410(2023).
[2] OmerBar-Tal,DolevOfri-Amar,RafailFridman,YoniKasten,andTaliDekel.2022. [26] YueMa,YingqingHe,XiaodongCun,XintaoWang,SiranChen,XiuLi,and
Text2live:Text-drivenlayeredimageandvideoediting.InEuropeanconference QifengChen.2024.Followyourpose:Pose-guidedtext-to-videogenerationusing
oncomputervision.Springer,707–723. pose-freevideos.InProceedingsoftheAAAIConferenceonArtificialIntelligence,
[3] TimBrooks,AleksanderHolynski,andAlexeiAEfros.2023. Instructpix2pix: Vol.38.4117–4125.
Learningtofollowimageeditinginstructions.InProceedingsoftheIEEE/CVF [27] JiafengMao,XuetingWang,andKiyoharuAizawa.2023.Guidedimagesynthe-
ConferenceonComputerVisionandPatternRecognition.18392–18402. sisviainitialimageeditingindiffusionmodel.InProceedingsofthe31stACM
[4] MingdengCao,XintaoWang,ZhongangQi,YingShan,XiaohuQie,andYinqiang InternationalConferenceonMultimedia.5321–5329.
Zheng.2023.Masactrl:Tuning-freemutualself-attentioncontrolforconsistent [28] ChenlinMeng,YangSong,JiamingSong,JiajunWu,Jun-YanZhu,andStefano
imagesynthesisandediting.InProceedingsoftheIEEE/CVFInternationalConfer- Ermon.2021. Sdedit:Imagesynthesisandeditingwithstochasticdifferential
enceonComputerVision.22560–22570. equations.arXivpreprintarXiv:2108.01073(2021).ACMMM,2024,Melbourne,Australia YiZuo,LinglingLi,LichengJiao,FangLiu,XuLiu,WenpingMa,ShuyuanYang,andYuweiGuo
[29] DavideMorelli,AlbertoBaldrati,GiuseppeCartella,MarcellaCornia,Marco InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.7623–
Bertini,andRitaCucchiara.2023.LaDI-VTON:latentdiffusiontextual-inversion 7633.
enhancedvirtualtry-on.InProceedingsofthe31stACMInternationalConference [52] RuiXu,LeHui,YuehuiHan,JianjunQian,andJinXie.2023.SceneGraphMasked
onMultimedia.8580–8589. VariationalAutoencodersfor3DSceneGeneration.InProceedingsofthe31st
[30] AlexanderQuinnNicholandPrafullaDhariwal.2021.Improveddenoisingdiffu- ACMInternationalConferenceonMultimedia.5725–5733.
sionprobabilisticmodels.InInternationalconferenceonmachinelearning.PMLR, [53] HaiboYang,YangChen,YingweiPan,TingYao,ZhinengChen,andTaoMei.
8162–8171. 2023. 3dstyle-diffusion:Pursuingfine-grainedtext-driven3dstylizationwith
[31] LiangPeng,HaoranCheng,ZhengYang,RuisiZhao,LinxuanXia,ChaotianSong, 2ddiffusionmodels.InProceedingsofthe31stACMInternationalConferenceon
QinglinLu,WeiLiu,andBoxiWu.2023. SmoothVideoSynthesiswithNoise Multimedia.6860–6868.
ConstraintsonDiffusionModelsforOne-shotVideoTuning. arXivpreprint [54] KunYang,DingkangYang,JingyuZhang,HanqiWang,PengSun,andLiangSong.
arXiv:2311.17536(2023). 2023.What2comm:Towardscommunication-efficientcollaborativeperception
[32] ChenyangQi,XiaodongCun,YongZhang,ChenyangLei,XintaoWang,Ying viafeaturedecoupling.InProceedingsofthe31stACMInternationalConference
Shan,andQifengChen.2023.Fatezero:Fusingattentionsforzero-shottext-based onMultimedia.7686–7695.
videoediting.InProceedingsoftheIEEE/CVFInternationalConferenceonComputer [55] ZhaoYang,BingSu,andJi-RongWen.2023.SynthesizingLong-TermHuman
Vision.15932–15942. MotionswithDiffusionModelsviaCoherentSampling.InProceedingsofthe31st
[33] LeigangQu,ShengqiongWu,HaoFei,LiqiangNie,andTat-SengChua.2023. ACMInternationalConferenceonMultimedia.3954–3964.
Layoutllm-t2i:Elicitinglayoutguidancefromllmfortext-to-imagegeneration. [56] JietengYao,JunjieChen,LiNiu,andBinSheng.2023. Scene-awarehuman
InProceedingsofthe31stACMInternationalConferenceonMultimedia.643–654. posegenerationusingtransformer.InProceedingsofthe31stACMInternational
[34] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh, ConferenceonMultimedia.2847–2855.
SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark, [57] DanahYatim,RafailFridman,OmerBarTal,YoniKasten,andTaliDekel.2023.
etal.2021.Learningtransferablevisualmodelsfromnaturallanguagesupervision. Space-TimeDiffusionFeaturesforZero-ShotText-DrivenMotionTransfer.arXiv
InInternationalconferenceonmachinelearning.PMLR,8748–8763. preprintarXiv:2311.17009(2023).
[35] YixuanRen,YangZhou,JimeiYang,JingShi,DifanLiu,FengLiu,MingiKwon,and [58] ChaohuiYu,QiangZhou,JingliangLi,ZheZhang,ZhibinWang,andFanWang.
AbhinavShrivastava.2024.Customize-A-Video:One-ShotMotionCustomization 2023.Points-to-3d:Bridgingthegapbetweensparsepointsandshape-controllable
ofText-to-VideoDiffusionModels.arXivpreprintarXiv:2402.14780(2024). text-to-3dgeneration.InProceedingsofthe31stACMInternationalConferenceon
[36] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörn Multimedia.6841–6850.
Ommer.2022.High-resolutionimagesynthesiswithlatentdiffusionmodels.In [59] JunYu,JiZhao,GuochenXie,FengxinChen,YeYu,LiangPeng,MingleiLi,
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition. andZonghongDai.2023. Leveragingthelatentdiffusionmodelsforoffline
10684–10695. facialmultipleappropriatereactionsgeneration.InProceedingsofthe31stACM
[37] OlafRonneberger,PhilippFischer,andThomasBrox.2015. U-net:Convolu- InternationalConferenceonMultimedia.9561–9565.
tionalnetworksforbiomedicalimagesegmentation.InMedicalimagecomputing [60] HuiminZeng,WeinongWang,XinTao,ZhiweiXiong,Yu-WingTai,andWenjie
andcomputer-assistedintervention–MICCAI2015:18thinternationalconference, Pei.2023.FeatureDecoupling-RecyclingNetworkforFastInteractiveSegmen-
Munich,Germany,October5-9,2015,proceedings,partIII18.Springer,234–241. tation.InProceedingsofthe31stACMInternationalConferenceonMultimedia.
[38] NatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,and 6665–6675.
KfirAberman.2023.Dreambooth:Finetuningtext-to-imagediffusionmodelsfor [61] LvminZhang,AnyiRao,andManeeshAgrawala.2023.Addingconditionalcon-
subject-drivengeneration.InProceedingsoftheIEEE/CVFConferenceonComputer troltotext-to-imagediffusionmodels.InProceedingsoftheIEEE/CVFInternational
VisionandPatternRecognition.22500–22510. ConferenceonComputerVision.3836–3847.
[39] LeoShan,WenzhangZhou,andGraceZhao.2023.IncrementalFewShotSemantic [62] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang.
SegmentationviaClass-agnosticMaskProposalandLanguage-drivenClassifier. 2018. Theunreasonableeffectivenessofdeepfeaturesasaperceptualmetric.
InProceedingsofthe31stACMInternationalConferenceonMultimedia.8561–8570. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.
[40] AliaksandrSiarohin,StéphaneLathuilière,SergeyTulyakov,ElisaRicci,andNicu 586–595.
Sebe.2019.Firstordermotionmodelforimageanimation.Advancesinneural [63] Shao-KuiZhang,Jia-HongLiu,YikeLi,TianyiXiong,Ke-XinRen,HongboFu,
informationprocessingsystems32(2019). andSong-HaiZhang.2023. AutomaticGenerationofCommercialScenes.In
[41] JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli. Proceedingsofthe31stACMInternationalConferenceonMultimedia.1137–1147.
2015. Deepunsupervisedlearningusingnonequilibriumthermodynamics.In [64] YaboZhang,YuxiangWei,DongshengJiang,XiaopengZhang,WangmengZuo,
Internationalconferenceonmachinelearning.PMLR,2256–2265. andQiTian.2023.Controlvideo:Training-freecontrollabletext-to-videogenera-
[42] JiamingSong,ChenlinMeng,andStefanoErmon.2020. Denoisingdiffusion tion.arXivpreprintarXiv:2305.13077(2023).
implicitmodels.arXivpreprintarXiv:2010.02502(2020). [65] MinZhao,RongzhenWang,FanBao,ChongxuanLi,andJunZhu.2023.Con-
[43] XueSong,JingjingChen,andYu-GangJiang.2023.RelationTripletConstruc- trolvideo:Addingconditionalcontrolforoneshottext-to-videoediting.arXiv
tionforCross-modalText-to-VideoRetrieval.InProceedingsofthe31stACM preprintarXiv:2305.17098(2023).
InternationalConferenceonMultimedia.4759–4767. [66] RuiZhao,YuchaoGu,JayZhangjieWu,DavidJunhaoZhang,JiaweiLiu,Weijia
[44] YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,Stefano Wu,JussiKeppo,andMikeZhengShou.2023. Motiondirector:Motioncus-
Ermon,andBenPoole.2020.Score-basedgenerativemodelingthroughstochastic tomizationoftext-to-videodiffusionmodels. arXivpreprintarXiv:2310.08465
differentialequations.arXivpreprintarXiv:2011.13456(2020). (2023).
[45] ShuyuanTu,QiDai,Zhi-QiCheng,HanHu,XintongHan,ZuxuanWu,and [67] ZhichaoZuo,ZhaoZhang,YanLuo,YangZhao,HaijunZhang,YiYang,and
Yu-GangJiang.2023.MotionEditor:EditingVideoMotionviaContent-Aware MengWang.2023.Cut-and-Paste:Subject-DrivenVideoEditingwithAttention
Diffusion.arXivpreprintarXiv:2311.18830(2023). Control.arXivpreprintarXiv:2311.11697(2023).
[46] NarekTumanyan,MichalGeyer,ShaiBagon,andTaliDekel.2023.Plug-and-play
diffusionfeaturesfortext-drivenimage-to-imagetranslation.InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition.1921–1930.
[47] DaniValevski,MatanKalman,YossiMatias,andYanivLeviathan.2022.Unitune:
Text-drivenimageeditingbyfinetuninganimagegenerationmodelonasingle
image.arXivpreprintarXiv:2210.094772,3(2022),5.
[48] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,
AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.2017. Attentionisall
youneed.Advancesinneuralinformationprocessingsystems30(2017).
[49] YaohuiWang,XinyuanChen,XinMa,ShangchenZhou,ZiqiHuang,YiWang,
CeyuanYang,YinanHe,JiashuoYu,PeiqingYang,etal.2023. Lavie:High-
qualityvideogenerationwithcascadedlatentdiffusionmodels.arXivpreprint
arXiv:2309.15103(2023).
[50] ZixinWang,YadanLuo,ZhiChen,SenWang,andZiHuang.2023.Cal-SFDA:
Source-FreeDomain-adaptiveSemanticSegmentationwithDifferentiableEx-
pectedCalibrationError.InProceedingsofthe31stACMInternationalConference
onMultimedia.1167–1178.
[51] JayZhangjieWu,YixiaoGe,XintaoWang,StanWeixianLei,YuchaoGu,Yufei
Shi,WynneHsu,YingShan,XiaohuQie,andMikeZhengShou.2023.Tune-a-
video:One-shottuningofimagediffusionmodelsfortext-to-videogeneration.