A General Model for Detecting Learner Engagement:
Implementation and Evaluation
SomayehMalekshahi* JavadM.Kheyridoost OmidFatemi*
s.malekshahi@ut.ac.ir javadkheyridoost@gmail.com ofatemi@ut.ac.ir
ABSTRACT
Consideringlearnerengagementhasamutualbenefitforbothlearnersandinstructors. Instructors
canhelplearnersincreasetheirattention,involvement,motivation,andinterest. Ontheotherhand,
instructorscanimprovetheirinstructionalperformancebyevaluatingthecumulativeresultsofall
learnersandupgradingtheirtrainingprograms. Thispaperproposesageneral,lightweightmodel
for selecting and processing features to detect learners’ engagement levels while preserving the
sequentialtemporalrelationshipovertime. Duringtrainingandtesting,weanalyzedthevideosfrom
thepubliclyavailableDAiSEEdatasettocapturethedynamicessenceoflearnerengagement. We
have also proposed an adaptation policy to find new labels that utilize the affective states of this
datasetrelatedtoeducation,therebyimprovingthemodels’judgment. Thesuggestedmodelachieves
an accuracy of 68.57% in a specific implementation and outperforms the studied state-of-the-art
modelsdetectinglearners’engagementlevels.
Keywords EngagementDetection,EmotionsDetection,E-learning,Educational,DeepLearning
1 Introduction
Onlinelearningfacilitatesaccesstoasynchronousmaterialsatanytimeandprovidesaccesstosynchronousresources
fromanywhere. Consequently,theappropriateonlinelearningmaterialstoprovidehighinteractivityandreliabilityare
crucial[1]. Thequalityofthedevelopingmaterialinonlinelearningdirectlyimpactstheeffectivenessofthelearning
process[2]. Additionally,thelearner’sengagementwithcontentandtheirinteractionwiththeinstructorandother
learnersduringlearninghelptoconstructpersonalmeaning[3]forthelearnerwhoisatadistancefromtheinstructor
to promote learning and attain the desired learning outcomes. Analysis engagement of learners in an e-learning
environmentisthemainconcerninthiswork. Weconcentrateonautomaticallydetectinglearners’engagementlevels
toimprovethequalityofe-learningeducation. Awarenessoflearners’engagementduringonlineeducationprovidesa
goodinsightintolearningprogramperformanceandisagoodcriterionforinstructorstolevelupthelearningprogram
basedonthecapabilitiesoflearners.
Inautomaticlearnerengagementdetection,modelsusevideo-basedorframe-basedevaluations. [4]usedframe-by-
framelabelingforrecognitionofstudentengagementbyaveragingdedicatedlabelstoeachframeasafinallabelofthe
10-secondvideoclip. In[5],thelevelofstudentengagementwasassessedbasedontheimage,astheauthorssupposed
thattheengagementvaryacross10-secondvideoclips,makingitdifficultandsometimesinaccuratetoassignalabelto
eachvideoclip. In[6]althoughthevideosoftheclasseshavebeenusedingroups,thedatasethasbeencreatedbased
onphotos. Theyalsousedemotionalandbehavioralaspectsasengagementstates. InDAiSEEdataset[7,8],theuser’s
affectivestateswererecognized,andbenchmarkresultswerepresentedusingStatic(Frameclassification/prediction)
and Dynamic (temporal video classification) models, both of which use images as inputs. In video classification,
spatiotemporalrelationshipsandactionsbetweenadjacentframesarerecognized,andalabelisassignedtotheentire
video.
Inthispaper,weusevideo-basedevaluations,andourcontributionsareasfollows: 1)Weproposeageneralmodel
forengagementrecognitionbasedonemotionalexpressionsbypreservingthesequentialtemporalrelationshipover
∗DepartmentofElectricalandComputerEngineering,UniversityofTehran,Tehran,Iran
4202
yaM
7
]VC.sc[
1v15240.5042:viXraAGeneralModelforDetectingLearnerEngagement: ImplementationandEvaluation
time. 2)WeapplydifferentmachinelearningclassifiersandfindappropriateimplementationsettingsontheDAiSEE
datasetforcompetitiveperformanceoverstate-of-the-artresultsonthesamedataset3)WeanalyzetheDAiSEEdataset
consideringeducationalissues,proposeanadaptivelabelingforlearnerengagement,andtrainourmodelbasedonthis
adaptivelabelingforbetterresults. Ourpaperisstructuredasfollows: First,wesummarizethelearner’sengagement
meaningandthepreviousworksofautomaticdetectionoflearners’engagementlevels. Subsequently,wepresentthe
maincontributionofthispaper. Next,wedelveintothedetailsofourexperiments. Finally,weconcludeandoutline
potentialavenuesforfutureresearch.
2 LearnerEngagement
EducationalpsychologistRalphTyler’sbasicideaofthepositiveeffectsoftimeontaskforlearningwasextendedby
manyresearcherslikePace(1980)as"qualityofeffort"forstudentsuccessorAlexanderAstin’stheoryofinvolvement
(1984)[9]. GeorgeKuhdescribedengagementasaremarkablefactorinstudentsuccessasasetofelementsthatmeasure
theamountoftimeandenergyastudentallocatestomeaningfuleducationallearningactivities[10]. Grocciaconsidered
itasanindicatorofstudentandinstitutionalsuccessandquality[11]. Additionally,thereareotherrepresentationsfor
studentengagementrelatedtotheiroutcomeswithinthefieldofeducation.
Students’engagementisbeyondjustattendanceandinvolvementwithlearningactivities. Interest,confidence,enjoy-
ment, andasenseofbelongingaresignsoflearningengagement[12,13,14]. Self-perceivedisanotablepointof
engagementinlearningdistinguishedbyaninstructorasabehavioral,cognitive,oremotionalengagement[15,16,17].
Feelingandactingarebothsidesofmeasuringstudents’engagement. Althoughusingarangeofmeasurableoutcomes
[18]isacriterionforengagementevaluation,learningarobustengagementmeasureisstillchallenging[19]. Behavioral,
emotional,andcognitivelevelsofstudentengagementaredescribedindifferentdimensions,andconsideringthesethree
categoriesinstudents’engagementprocessiscomplicated. Thebehavioralengagementregardinglearnerparticipation
oreffortneedstobecontinuouslymonitored. Detectingtheaffectiveoremotionalleveloflearners’engagementis
complex. Nevertheless, evaluatingthelevelofinterest, motivationimprovement, andenjoymentorconfusionand
frustrationprovidesinsightsintothedegreeofattentionoflearnersduringtheinstructor’spresentation. Usingsuch
information,theinstructorscanencouragethelearnersnotinvolvedinthelessonattherighttime. Inthiswork,weuse
emotionalfacialexpressionstodetectemotionalengagement. Engagementonacognitivelevelisrelatedtomental
activityandneedsevaluationmetricstoassess[20].
3 RELATEDWORK
In[21],aDeepMulti-InstanceLearning(DMIL)frameworkwasusedtolocalize(intime)engaging/non-engagingparts
intheonlinevideolectures,whichisusefulfordesigningMassiveOpenOnlineCourses(MOOCs)videomaterial. By
assigningbaselinescoresbasedontheDMILandcomputinglocalizationasaMILproblem,theypredictedstudents’
engagement. Additionally,theauthorscreatedanew’inthewild’databaseforengagement,consistingof725,000
framescapturedfrom75subjectsof102videos,manuallyannotatedbylabelersforfourlevelsofengagement. These
areengagementintensity0ascompletelydisengaged,engagementintensity1asbarelyengaged,engagementintensity2
asengaged,andengagementintensity3ashighlyengaged. Thedurationofusers’concentrationwasusedasacriterion
todeterminetheencouragementoftheusertodifferentpartsofthevideoandtheengagementlevel. [22]proposeda
DeepFacialSpatiotemporalNetwork(DFSTN)topredictengagement,consistingoftwoparts. Thefirstpartextracted
facialspatialfeaturesusingthepre-trainedSE-ResNet-50. Inthesecondpart,amorecomplicatedarchitecturewas
used,employingLongShortTermMemory(LSTM)andattentionmechanismtogenerateanattentionalhiddenstate.
TheauthorsevaluatedtheperformanceoftheproposedmodelusingtheDAiSEEdatasetinfour-classclassificationand
obtainedanaccuracyof58.84%. TheyalsotestedtherobustnessofthemodelusingtheEmotiW-EPdataset.
[23]presentedanend-to-endarchitectureusingResNet+TCNtodetectstudents’engagementlevels.Thehybridnetwork
wastrainedinsequencesofframesinvideosoftheDAiSEEdatasetwithaconcentrationonstudentengagement,with
fourlevelsofengagement: verylow,low,high,andveryhigh. Theyusedanalteredversionofthenetworkof[24]
toextractthespatiotemporalfeatures. Theintroducedmodelbenefitsfrommaintaininglong-termhistorybyTCN
comparedwithLSTMsandGRUs. TheimbalanceprobleminDAiSEEwasresolvedusingacustomsamplingstrategy
and a weighted cross-entropy loss function. In [25], a CNN was introduced as an optimized lightweight model to
detectstudents’learningengagementusingfacialexpressions. ThismodelwasdesignedbasedontheShuffleNetv2
architecturesothattheauthorsutilizedanattentionmechanismtoachievethebestperformance. TheShuffleNetv2
0.5xstructurewasusedtominimizecomputationalcomplexity,differentactivationfunctionsliketheLeakyReLUwere
examinedtoimprovetheaccuracyofthemodel,andtheattentionmodulewasembeddedinthenetworktoprovide
anoptimizedmodel. Finally,theyobtainedanaccuracyof63.9%ontheDAiSEEdatasetwithafocusonthestateof
engagement,withfourlevelsofengagementasverylow,low,high,andveryhighusingtheproposedmodel,Optimized
ShuffleNetv2.
2AGeneralModelforDetectingLearnerEngagement: ImplementationandEvaluation
AnewapproachbasedonNeuralTuringMachineforautomaticengagementrecognitionwasproposedin[19]. The
approachusedmulti-modalfeaturecombinationsforlearningtheweightsoffeatures,whichincludedeyegaze,facial
actionunit,headpose,andbodyposefeaturesextractedfromshortvideosusingOpenFace2.0[26]. Experimental
resultsdemonstratedtheaccuracyof61.3%ontheDAiSEEdatasetinfourlevelsof0,1,2,and3fromlowtohighon
videoengagementtagsbasedonstudentparticipation. In[27],authorsproposedthreemodelsbasedondeeplearning
andusedlong-termmemoryincombinationwithEfficientNetB7todetectstudents’engagementlevelsinanonline
e-learningvideo. Theyevaluatedtheirmodel’saccuracyusingtheself-developeddatasetVRESEEandthepublic
datasetDAiSEEwithaconcentrationonstudentengagement,withfourlevelsofengagement: verylow,low,high,and
veryhigh. Thespatialfeaturevectorsrelatedtothesequenceofavideowererepresentedwithk-frames,butbecauseof
thedifficultiesoftraining,theyfoundk=60andk=40asthebestnumbersofframesforDAiSEEandVRESEEdatasets.
Theyusedthreemodelsforextractingtemporalfeatures: EfficientNetB7withtheBidirectionalLSTM,thehybrid
EfficientNetB7alongwiththeTemporalConvolutionNetwork(TCN),andthehybridEfficientNetB7andLSTM.
[28]exploredtheestimationoflearningengagementincollaborativelearningusinggazebehaviorandfacialexpression
while the final student engagement was classified into four levels: high engagement, engagement, a little bit low
engagement,andlowengagement. Collaborativelearninginvolvesteachingactivitiesthatarehandledbyagroupof
participants,andinterpersonalinteractionbetweengroupmembersisasignificantfactorinthesuccessoflearning. To
tacklefacialanalysis,thepaperusedafuzzylogicofjointfacialexpressionandgazeandintroducedamulti-modal
deepneuralnetwork(MDNN)topredictstudents’engagement. However,thelackofappropriateandpubliclyavailable
datasetsforcollaborativelearningenvironmentsposesachallengeinevaluatingtheresultsofsuchstudies. In[29],
authorshaveadifferentviewofmeasuringlearnerengagement. Theydidnotconsidertheimportanceoftheorder
ofthebehavioralandemotionalstatesandjustfocusedontheoccurrenceofthesestates. Afterextractingbehavioral
andinfluentialfeaturesfromvideosegments,theyclusteredfeaturevectorsanddecidedbasedonthefrequencyof
occurrenceoftheclustercenters. TheyachievedbetteraccuracyontheIIITBOnlineSEdataset[30]andtheDAiSEE
studentengagementdatasetusingtheBagofStates(BoS)asanon-sequentialapproach. Amongthefourstatesofthe
annotatedvideosontheDAiSEEdataset,thefocuswasonlyontheengagementlevelclassificationinfourlevelsof0,1,
2,and3asverylow,low,high,andveryhigh.
[31]usedunlabeledclassroomvideostodetecttheengagementlevelofthestudents. Theyutilizedunsuperviseddomain
adaptation for inferring student engagement and transferred features from an online setting to a classroom setting
withunsuperviseddomainadaptation. TheyusedJointAdaptationNetworkandadversarialdomainadaptationusing
WassersteindistanceandbasemodelssuchasResNetandI3Dtoresolveabinaryclassificationproblem. Theyachieved
anaccuracyof68%usingtheJANnetworkwithRGB-I3Dasthebasemodelforthebinarystudentengagementnetwork.
Also,anaccuracyof71%wasobtainedusingtheadversarialdomainadaptationwithResNet50asthefeatureextractor
forpredictingtheengagementofthestudents. TheDAiSEEdatasetwasasourcedataset,andanotherwasthetarget
datasetusedintheexperiments. Thetwo-levelengagementdetectionapproachconsideredonlytheaffectivestateof
engagementwhileignoringotherstates. Thelabel’0’,thedistractedlabel,wasdefinedasthecombinationofverylow
andlowengagementlevels. Thelabel’1’,denotedastheengagedlabel,representedthecombinationofhighandvery
highengagementlevels.
4 PROPOSEDMODEL
Inthissection,wefirstpresentourproposedmodelandaspecificimplementationforit,followedbyadescriptionof
thevariouscomponentsofourlearnerengagementdetectionmodel. Weintroducetheavailablemodelforextracting
emotionalfeatures,andthendescribethedatasetandthedatapreparationprocesstoadaptittoourneeds.
4.1 GeneralModel
Inthissection,weintroduceageneralmodelforpredictinglearnerengagementlevelsduringavideoclip,eveninthe
presenceofmissingvalues. Missingfeaturescanresultinbiasedestimationsandnegativelyimpactstatisticalresults
[32], producingvariable-lengthfeaturevectors. Here, wedonotimplementimputationorpredictionsolutionsfor
missingfeatures. Instead,asshowninFigure1,weproposeageneralmodelthatprocessestheavailablefeatureswhile
preservingthesequentialtemporalrelationshipovertime. Alltrainingandtestingproceduresareperformedusing
thevideosfromtheDAiSEEdataset. Withineach10-secondvideo,wecaptureapproximately30framespersecond.
However,thenumberofframesisnotuniformacrossallsamples,asweonlyevaluateframesthatcontainfaces. Hence,
differenttraininginputvideosmayhavevaryingnumbersofframes. Werepresentallframesofavideoasavector
inEquation1,where‘n’,thenumberofframes,isnotuniformacrossallsamplesofthe10-secondvideoclips. To
addressthisissue,weapplyfiltersormapstoshrinkthisvectortothesamelengthforallsamplesandpreparethemto
useinclassificationmodels,resultinginanewvectorrepresentedinEquation2. Anexampleofsuchafiltercouldbe
selectingafixednumberofframeswithinaspecifictimeintervalforallvideos.
3AGeneralModelforDetectingLearnerEngagement: ImplementationandEvaluation
[frame ,frame ,...,frame ] (1)
1 2 n
[frame′,frame′,...,frame′ ];m≤n (2)
1 2 m
Figure1: Proposedgeneralmodelforlearnerengagementdetection. (Thefaceimagesareadaptedfrom[33,34])
Eachframeinthevectorisanalyzedtorepresentaspecificd-lengthfeaturevectorusinganymodeltoextractthese
features like an emotional detection network, as expressed in Equation 3. Features can be further manipulated by
applying particular filters, maps, or encoding systems multiple times to extract the optimal s-length features for
classificationmodels,asshowninEquation4. Themanipulationoffeaturesdependsontheirtypeandimportanceinthe
model,asdeterminedbythedataset. Duetotheroleoftheextractedfeaturesinthefinaldiagnosis,aspecificselection
canbeapplied. Furthermore,theselectedclassificationmodelinthelearnerengagementdetectionstageandtheamount
ofdataavailablefortrainingthemodelplayaroleinestablishingabalancebetweensamplesandfeatures,thereby
influencingtheutilizationofrawfeatures.
[(x ,x ,...,x ),...,(x ,x ,...,x )] (3)
11 12 1d m1 m2 md
[(x∗ ,x∗ ,...,x∗ ),...,(x∗ ,x∗ ,...,x∗ )];s≤d (4)
11 12 1s m1 m2 ms
Theauthorsrefertofilteringasatypeofselection,mappingasatypeofcorrespondencethatisnotnecessarilyreversible,
andencodingasatypeoftransformationandconversionthatcanbedecoded.
4.2 ProposedImplementation
Inthefollowing,wedescribeaparticularimplementationofthemodel,Figure2. Werepresenteachframeofavideoas
avectorinEquation1,andthenwereducethenumberofframesinthesamplestoensurethatthefeaturevectorsareof
4AGeneralModelforDetectingLearnerEngagement: ImplementationandEvaluation
equalsize. However,duetotheimportanceoftheratiobetweenthenumberoffeaturesandthenumberofsamplesin
machinelearningandtheimpactofthisratioontheperformanceandgeneralizationabilityofmodels,andduetothe
limitedavailabledata,welimitthenumberofframesasfeaturesforsmoothtrainingsamples. Table1displaysthe
numberofframesindifferentvideos,andweobservethatifweconsidermorethan300frames,wecanonlyuse309
videos(each10secondslong),whichisinsufficientfortrainingtheclassificationmodels. Therefore,’m’inEquation2
cannotexceed300.
Table1: ExtractedframescountofvideosoftheDAiSEEdataset
Frames >300 ≥300 ≥200 ≥100 ≥60 ≥40
Videos 309 6063 8079 8494 8647 8721
Figure2: Ourspecificimplementationfortheproposedmodel. (Thefaceimagesareadaptedfrom[7])
Todeterminetheappropriatenumberofframespervideo,denotedas‘m’inEquation2,afilteringprocessisapplied.
Regarding the relatively low variations between neighboring frames, every ‘z’th frame is selected instead of all.
Experiments conducted by [7] and ourselves have demonstrated that this approach does not significantly impact
accuracy. However,weinvestigateddifferentvaluesfor‘z’asahyperparameterthatrequiressearchingtoidentifythe
optimalvaluesduringthetrainingphasetocustomizethemodelforthespecificdataset.
A seven-dimensional emotion vector is extracted from each frame using the emotion detection CNN described in
Section4.3,Subsequently,afeaturevectorisconstructedwithadimensionalityofd=7,Equation3. Wethenapplyan
5AGeneralModelforDetectingLearnerEngagement: ImplementationandEvaluation
encodingprocesstoeachfeaturevectorbymultiplyingeachfeaturewiththeTruePositive(TP)probabilityassociated
with each state recognized by the network, as explained in Section 4.3. This approach gives more weight to the
predictionsofemotionswithhighertruepositiveratesandlessweighttoemotionswithlowertruepositiverates. These
values,denotedas(w ,w ,w ,w ,w ,w ,w ),correspondtothe’happy’,’neutral’,’surprised’,’disgusted’,’angry’,
1 2 3 4 5 6 7
’fearful’,and’sad’states,andare(0.9,0.8,0.77,0.62,0.5,0.37,0.28). Thesecoefficientsprioritizetheimportanceof
eachextractedemotioninthefeaturevector. Whileincorporatingtheknowledgeofeducationscienceandpsychology
tochoosethecoefficientsforprioritizingwouldmakethemmoreinformedandinfluential,suchconsiderationsare
beyondthescopeofthispaper.
[(x∗ ,x∗ ,...,x∗ ),...,(x∗ ,x∗ ,...,x∗ )]
11 12 17 m1 m2 m7
=[(w x ,w x ,...,w x ),...,(w x ,w x ,...,w x )] (5)
1 11 2 12 7 17 1 m1 2 m2 7 m7
=[(0.9x ,0.8x ,...,0.28x ),...,(0.9x ,0.8x ,...,0.28x )]
11 12 17 m1 m2 m7
Accuratedetectionoffacialexpressionsreliesondeterminingtheneutralfacialexpression[35]. Inhumanlabeling,
itisessentialtoconsiderneutralfacialexpressionsofanger,anxiety,orhappinesstoavoidincorrectdecisions. For
instance,invideosoftheDAiSEEdataset,somepersonshaveadefaultfrown,whichisnotasignofanger,confusion,
orfrustration. Ignoringtheneutralfacialexpressionina10-secondvideocanleadtoincorrectrecognitionofemotions.
While anger expression is correctly identified through feature-based learning in automatic expression detection,
disregardingtheperson’sneutralfacialexpressionresultsininaccuratepredictionsintheengagementdetectionproblem.
Hence,weassesstheimpactofemotionsonmodelestimationthroughfeatureimportanceanalysisontheDAiSEE
dataset. Eachemotionissystematicallyremovedasafeature,andtheengagementpredictionmodelistrainedand
evaluatedbothwithandwithouteachfeature. Theintensityofinfluenceofthesefeaturesindicatestheircontributionto
thepredictivecapabilityoftheclassifier. Ourexperimentsdemonstratethatneglectingthedefaultfrownleadstoissues
whileexcludingtheexpressionofangerimprovesprediction.
Usingthisextractedsequentialinformationasfeaturesforeachvideo, wetrytopreservethedependencybetween
frames’sequencesinthelearnerengagementdetectionmodel. Afterfeatureselectioninaspecificcondition,weneeda
modeltobetrainedandthenclassifythelearningengagementlevel. Inthiswork,weusesupervisedlearningclassifiers
todetectthelearners’engagementlevel:LogisticRegression,Supportvectormachines(SVMs),Multi-LayerPerceptron
(MLP), K-nearest neighbors (KNN), and eXtreme Gradient Boosting (XGBoost) are used. Logistic Regression is
a statistical method for binary and multiclass classification. It is a Generalized Linear Model that attempts to use
moredatawithalmostthesameeffectstopredictoutcomes. Itisasimpleandefficientmethodusedforclassification
problems. SVMsaresupervisedlearningmethodseffectiveinhigh-dimensionalspaces. SVMsdependonlocaldatato
findinfluentialpointsandmakemoreaccuratedecisionsandareoneofthemostrobustpredictionmethods. Multi-layer
Perceptron(MLP)isafeed-forwardneuralnetworkthatcanbeusedinclassificationproblemsthatarenotlinearly
separable. K-nearestneighbors(KNN)isanon-parametric, supervisedmachinelearningalgorithmweuseforthe
classificationproblem. Itisasimpleandintuitivealgorithmthatlabelsanewsamplebasedonitsproximitytoitsk
nearestneighborsinthefeaturespaceusingdistancemetrics. eXtremeGradientBoosting(XGBoost)isaneffectiveand
flexiblemachinelearningalgorithmbasedonthegradientboostingconceptwithanoptimizedimplementation.
4.3 EmotionDetectionModel
ACNN,designedin[36]withonlineimplementationof[37],isutilizedtodetectemotions. Thisnetworkistrainedon
theFER-2013dataset[33],andincludessevenemotionalstates: ‘angry’,‘disgusted’,‘fear’,‘happy’,‘sad’,‘surprised’,
and‘neutral’. Thehighesttruepositivepercentagesforeachrespectiveemotionareasfollows: 90%for‘happy’,80%
for‘neutral’,77%for‘surprised’,62%for‘disgusted’,50%for‘angry’,37%for‘fearful’,and28%for‘sad’. The
networkreceivesanimageasinputthatpassesthroughaconvolutionallayerwithakernelsizeof5×5andastrideof
1. Thislayerisfollowedbyamax-poolinglayerofkernelsize3×3withastrideof2ineachdimension. Asecond
convolutionallayerwithakernelsizeof5×5andstride1feedsdirectlyintothethirdconvolutionallayerwithakernel
sizeof4×4andstride1. Then,adropoutwithaprobabilityof0.2isappliedtoafullyconnectedlinearlayer. Finally,
thesoftmaxlayerdeterminesthemostprobablerecognizedemotion. WeusethisCNNforextractingemotionalfeatures
fromeachframebytrainingitontheFER-2013datasetfor100epochsas[37]didtotheaccuracyconvergencetothe
optimum.
4.4 Dataset
DAiSEE(DatasetforAffectiveStatesinE-Environments)[7]isavideo-leveldatasetannotatedwithaffectivestatesof
’Boredom’,’Engagement’,’Confusion’,and’Frustration’,eachwithfourlevelsofseverity: VeryLow,Low,High,and
VeryHigh. Itcontains9068videosnippetswithdifferentilluminationsettingsfrom112studentsaged18-30,theAsian
6AGeneralModelforDetectingLearnerEngagement: ImplementationandEvaluation
race,with32femaleand80malesubjects. Authorsin[7]evaluatedthedatasetwithtwotypesoftemporalandstatic
modelsbasedonCNNsandTop-1accuracywasusedtomeasureandanalyzetheresults. Accordingtothereported
experimentalresults,’Confusion’and’Frustration’aremoreaccuratelydetected,with79.1%accuracyfor’Frustration’
usingC3DFineTuning,72.3%accuracyfor’Confusion’usingLRCN,57.9%accuracyfor’Engagement’usingLRCN,
and53.7%accuracyfor’Frustration’usingLRCN.
Whilethedatasetmaynotdemonstratesatisfactoryaccuracyacrossallaffectivestates,DAiSEEhasbeenutilizedin
variousstudiesrelatedtolearnerengagement,asdiscussedinSection3. Itiswidelyemployedbecauseitisoneof
thefewdatasetspubliclyavailablewithavideo-levelstructure. Theversionofthedatasetusedinourstudycontains
2,455,390 frames from 8925 10-second videos, with the distribution of labels shown in Table 2. The presented
distributionexhibitsslightvariationscomparedtothereportedresultsin[7],stemmingfromusingthedatasetversion
accessedonline[38].
Table2: DistributionoflabelsintheDAiSEEdataset
AffectiveState VeryLow Low High VeryHigh
Boredom 3822 2850 1923 330
Confusion 5951 2133 741 100
Engagement 61 455 4422 3987
Frustration 6887 1613 338 87
DAiSEE is an imbalanced dataset, which means that the target class has an uneven distribution of observations.
Furthermore, the distribution of this dataset is inconsistent and should be adapted to the context of the problem.
Solutionstothisissuecanvaryanddependonthecontext. Forexample,[31]changedtheoriginalfour-levelvideo
engagementannotationstotwoclasses. Wealsoproposedanadaptationforatwo-classclassificationproblem,asshown
inTable3. Tothisend,wewillutilizeatwo-levellabelingscheme,consistingof’Engaged’and’Disengaged’labels.
DAiSEEhasthepotentialtobeappliedtovariousdomains,suchase-learning,e-shopping,healthcare,autonomous
vehicles,andevenmorefields[7]. Therefore,wewilladaptthelabelingschemefromthisdatasettomeetourspecific
requirements for education. Considering the importance of all affective states, namely ’Boredom’, ’Engagement’,
’Confusion’,and’Frustration’ineducation,andbydrawingonourexperiencewithvariouslabelingschemes,wehave
foundthatmeaningfulcombinationsofdifferentlabelsenhancethereliabilityofassigningalabeltoeachvideoclip
basedonthetwo-levelengagementdetection. Theselabelsarealsorelevanttolearningengagement,makingthema
moreappropriatechoiceforassessingourmodel’sperformance. ’Frustration’isacommonlyexperiencedemotion
duringtheprocessoflearning,particularlyinscienceandmiddleschoolenvironments. Thisemotionisastartingstep
towardencouragingstudentstobecomemoreengagedandlearnmoreeffectively. Whenteachersrecognizesignsof
frustration,theycanoffersupporttohelpstudentsmanagethesenegativeemotionsandmovetowardamoreengaged
state[39]. Additionally,confusionhasbeenfoundtobeanotablefactorinthelearningprocessinmanypreviousstudies
[40,41,42,43]. Whilefrustrationandconfusionarebothindicatorsofengagement, theseverityofthesenegative
emotionsiscrucialinanalyzingthelevelofengagement. Ifconfusedstudentsarenotguidedtomanagethesenegative
emotions,theymaybecomefrustratedorbored,leadingtodisengagement[44,45].
In this work, we consider all affective states and their severity in the DAiSEE dataset. Table 3 displays all label
combinationsconsidered. Thefirstrowindicatesthat,fortheaffectivestateof’Engagement’,wedesignateaseverity
levelofVeryLowandLowaslabel0(Disengaged),whileHighandVeryHighareassignedlabel1(Engaged). In
’Engagement;Confusion’,fifthrow,theseveritylevelofVeryLowfor’Engagement’isconsideredasDisengaged.
Also, when ’Engagement’ has a Very High value, it is labeled 1 (Engaged). In other cases, both ’Confusion’ and
’Engagement’areinchargeofdeterminingthefinallabel. TheseveritylevelofVeryLoworVeryHighfor’Confusion’
issetasDisengagediftheseveritylevelof’Engagement’isVeryLow. Subsequentrowsfollowthesameinterpretation.
Ifaconditionisnotapplicable,thelabelisassignedas’None’(NotDetermined).
Applyingourcustomadaptingpolicies,wecalculatedthedistributionoflabelsbasedonthetwo-levelengagement
detectionschema. TheresultsinTable4revealanimbalanceforthedifferentlabelcombinationsoriginatingfromthe
DAiSEEdataset,asshowninTable2. Themostimbalancedcasewas‘Engagement;Confusion’,with8305distinct
labels, whiletheleastimbalancedwas‘Confusion; Frustration’, withadifferenceof1753labels. Consideringthe
significanceofbalanceddatainclassification,takingintoaccountthelabeldistributionresultspresentedinTable4is
crucial. Oversamplingtheminorityclass,under-samplingthemajorityclass,focalloss,andweightedcross-entropy
lossareacceptedsolutionstorectifytheimbalanceproblem. In[23],theauthorsaddressedthisissueusingaweighted
cross-entropylossfunction,andweusedrandomunder-samplingbasedontheminimumnumberofsampleslabeled’0’
and’1’. ThelastcolumnoftheTable4isthetotalnumberofbalancedsamples. Forexample,inthefirstrow,thereare
516samplevideoswiththelabel’0’,soweselected516samplesrandomlyamongvideoswiththelabel’1’,andwe
had1032balancedvideosampleswiththe’Engagement’label.
7AGeneralModelforDetectingLearnerEngagement: ImplementationandEvaluation
Table3: AdaptinglabelsoftheDAiSEEdatasettotwo-levelengagementdetection
AffectiveState Policy
Engagement≤1→Disengaged
Engagement
Engagement≥2→Engaged
Confusion==0→Disengaged
Confusion
Confusion≥1→Engaged
Frustration==0→Disengaged
Frustration
Frustration≥1→Engaged
Engagement==0→Disengaged
Engagement;
Engagement==3→Engaged
Boredom
Boredom≥2→Disengaged
Engagement==0→Disengaged
Engagement==3→Engaged
Engagement;
((Confusion==0orConfusion==3)and
Confusion
Engagement==1)→Disengaged
Otherwise→Engaged
Engagement==0→Disengaged
Engagement==3→Engaged
Engagement;
((Frustration==0orFrustration==3)and
Frustration
Engagement==1)→Disengaged
Otherwise→Engaged
Confusion; Frustration==0andConfusion==0→Disengaged
Frustration Frustration≥1orConfusion≥1→Engaged
Engagement==0→Disengaged
Engagement==3→Engaged
Confusion; ((Frustration==3orConfusion==3)and
Frustration Engagement==1)→Disengaged
Engagement ((Frustration==0orConfusion==0)and
Engagement==1)→Disengaged
Otherwise→Engaged
Severitylevel: VeryLow:0,Low: 1,High: 2,VeryHigh: 3
Ifaconditionisnotapplicable,thelabelisassignedas’None’(NotDetermined)
Table4: LabeldistributionoftheDAiSEEdatasetadaptedtotwo-levelengagementdetectionlabels
Numberof Numberof Numberoflabels
AffectiveState total
labelsas‘1’ labelsas‘0’ as‘Notdetermined’
Engagement 8409 516 0 1032
Confusion 2974 5951 0 5948
Frustration 2038 6887 0 4076
Engagement;
3987 1865 3073 3730
Boredom
Engagement;
8615 310 0 620
Confusion
Engagement;
8613 312 0 624
Frustration
Confusion;
3586 5339 0 7172
Frustration
Confusion;
Frustration; 8535 390 0 780
Engagement
After preprocessing and adapting the labels based on our labeling matching policy, and despite losing part of the
data,weselectbalanceddatabasedonthetwo-levellabelingEngagedandDisengaged. Theexperimentalresultsare
explainedinSection5.2.
8AGeneralModelforDetectingLearnerEngagement: ImplementationandEvaluation
5 EXPERIMENTALRESULT
5.1 PerformanceMetric
Indesigningclassificationmodelssuchaslearners’engagementlevel,evaluatingtheperformanceisacriticalaspect. In
supervisedclassificationmodels,variousevaluationmetricsareavailabletomeasurehowwellthemodelwillperform
onunseendata. Inthiswork,weuseaccuracyasanevaluationmetrictocompareourresultswiththoseobtainedusing
theDAiSEEdataset.
5.2 Results
Inthispaper,weusedsupervisedlearning,whereinamodelistrainedtomakepredictionsorclassificationsbased
on labeled training data. In supervised learning, the training dataset comprises input samples (features) and their
correspondingoutputlabels,whichserveasgroundtruthorsupervisionforthelearningprocess. Thegoalistoacquire
amappingfunctionthatcaneffectivelygeneralizefromthetrainingdata,enablingaccuratepredictionsorclassifications
onunseenorfuturedatainstances.
Weevaluatedourresultsintwoways. First,weusedthedefault’Engagement’affectivestateofDAiSEEtoevaluate
ourmodel,followingtheapproachusedinotherpapers. Second,aswedetailedthestructureandchallengesofthe
DAiSEEdatasetinSection4.4,weassignedalabelforeachvideobasedonouradaptedtwo-levellabelingapproach.
Wethenpreprocessedandpreparedthedatafortrainingtheengagementdetectionmodels. Toreproduceexperimental
conditions, we used the origin partitioning of the dataset. We combined all videos of train and validation sets for
trainingandusedthetestvideosexclusivelyformodelevaluation. Webalancedthembyselectinganequalnumberof
thevideoslabeledasengagedandthoselabeledasdisengaged. Inaddition,tohavemorenormallydistributeddata
andarelativelysimilarscale,weusedstandardization. AsdescribedinSection4.2,weinvestigatedhyperparameter,
thenumberofskippedframes,‘z’. Amongthedifferentvalues,weachievedanaccuracyof68.57%usingevery30th
frames(z=30),with60features. Duringthe10-secondvideos,temporalvariationsaregenerallyminimal. Thissuggests
thatselectingdifferentsegmentsislikelytoyieldsimilaroutcomes,aligningwithourexpectations.
Giventhatpriorworksoncomparisonsofengagementdetectionmodelsusedthe’Engagement’labelfromtheDAiSEE
dataset,inourexperimentsofthe10framesand60featurestested,KNNachievedthehighestaccuracyof67.64%,
followedbyXGBwith63.73%,MLP,SVM,andLRwiththeaccuraciesof62.73%,61.82%and,60.91%,shownin
Table5. Basedonourproposedadaptationmethodsforatwo-classclassificationproblemdescribedinSection4.4and
presentedinTable3,theresultsaredepictedinTable6. Amongthecombinationsofeffectivestatesof’Engagement’,
’Confusion’,and’Frustration’thataremorerelevanttolearningengagement,discussedindetailinSection4.4,the
combinationof’Engagement’and’Confusion’ismoreaccurateandoutperformstheindividualstates. Theclassification
reportofthebestresultofthe’Engagement;Confusion’withtheaccuracyof68.57%usingKNNisshowninTable7.
Table5: TheaccuracyoflearnerengagementdetectionforbalanceddataontheDAiSEEdatasetfor’Engagement’
AffectiveState KNN XGB MLP SVM LR
Engagement 67.64% 63.73% 62.73% 61.82% 60.91%
5.3 Comparison
Inthissection,wecomparetheperformanceofourmethodwiththerelatedworksusingmorecomplicatedmodelsto
classifythelearner’sengagementlevel,Table8. Allresultswereevaluatedbyaccuracyastheperformancemetricusing
theDAiSEEdataset. [27]introducedthreedifferentmodelsbasedonEfficientNetB7andLSTM,Bi-LSTM,andTCN
andobtained67.48%,66.39%,and64.67%foraccuracymetric,respectively. In[25]accuracyof63.9%wasreported
fortheproposedoptimizedlightweightmodelbasedonShuffleNetv2architecture. [23]achieved61.15%and63.9%
forproposedmodelsofcombiningResNetasaspatialfeatureextractorwithLSTMandTCNtounderstandthechanges
indifferentframes,respectively. Theresultofusingthepre-trainedSE-ResNet-50,LSTM,andattentionmechanismas
DeepFacialSpatiotemporalNetwork,isanaccuracyof58.84%,[22]. Theaccuracyof61.3%istheachievementof
[19]utilizedtheNeuralTuringMachineandextractedmulti-modalfeaturesforautomatingengagementrecognition. In
ourmodelstheaccuracyof68.57%iswith’Engagement;Confusion’stateand67.64%iswith‘Engagement’stateboth
withKNNonbalanceddata.
9AGeneralModelforDetectingLearnerEngagement: ImplementationandEvaluation
Table6: TheaccuracyoflearnerengagementdetectionforbalanceddataontheDAiSEEdatasetfordifferentadapting
policiesofselectingaffectivestates
AffectiveState KNN
Engagement 67.64%
Confusion 55.45%
Frustration 56.99%
Engagement;
53.10%
Boredom
Engagement;
68.57%
Confusion
Engagement;
66.13%
Frustration
Confusion;
56.71%
Frustration
Confusion;
Frustration; 65.81%
Engagement
Table7: Theclassificationreportof’Engagement;Confusion’,andKNNoftestvideosofthebalanceddata
Label precision recall f1-score
Disengaged 0.67 0.74 0.70
Engaged 0.71 0.63 0.67
Table8: ResultsofdifferentmodelsonvideosoftheDAiSEEdataset
Methods Accuracy Reference
KNNandCNN 68.57% Proposedmodel*
KNNandCNN 67.64% Proposedmodel**
EfficientNetB7andLSTM 67.48% [27]
Bi-LSTM,andTCN 66.39% [27]
ShuffleNetv2 63.9% [25]
ResNetandTCN 63.9% [23]
ResNetandLSTM 61.15% [23]
NeuralTuringMachine 61.3% [19]
SE-ResNet50+LSTMwith
58.84% [22]
GlobalAttention
*basedontheadaptingpolicyof‘Engagement;Confusion’
**basedonthedefault’Engagement’affectivestate
6 CONCLUSION
This paper proposes a general model for detecting learners’ engagement with a custom feature selection method.
Theprocessingofframestoselectinfluentialfeaturesfromthevariable-lengthfeaturevectors,usingavideo-based
dataset while preserving temporal dependencies, resulted in marginally higher accuracy than existing approaches,
surpassing them by approximatelyonepercent. While theimprovement may seem modest, it is noteworthy given
theincrementalgainsoftenobservedinsuchsystemsanddemonstratestheeffectivenessofourmodelinachieving
competitiveperformancewithamorestreamlinedapproach. Also,incorporatingtheaffectivestatesrelatedtoeducation
andcreatingnewlabelstoadaptvideostoeducationalcontextshashelpedachievemorereliableresults. Moreover,in
thelearnerengagementclassificationpart,westreamlinethetrainingprocesssignificantlybyselectingstatisticaland
efficientmodels,comparedwithneuralnetworksanddeeplearningmodelsthatrequireasignificantamountoflabeled
dataandcomputationalresources. Thesimplicityofthismodelmakesitapplicableforuseinsynchronousclassesasan
onlinelearnerengagementdetector. Forfuturework,wewouldliketoexplorealternativemethodstoaddressmissing
featurestoobtainsmoothfeaturevectorsbyconsideringalllearnerstodevelopamodelthataccuratelyevaluatesthe
leveloflearnerengagementinsynchronousclasses. Additionally,weintendtoexploredatasetscontaininglongerclips
toprovideacloserrepresentationofreal-worldeducationalclassrooms.
10AGeneralModelforDetectingLearnerEngagement: ImplementationandEvaluation
References
[1] G.RingandG.Mathieux,“Thekeycomponentsofqualitylearning,”inASTDTechknowledge2002Conference,
LasVegas,February2002.
[2] A.Rovai,“Buildingsenseofcommunityatadistance,”InternationalReviewofResearchinOpenandDistance
Learning(IRRODL),vol.3,no.1,April2002.[Online].Available: http://www.irrodl.org/content/v3.1/rovai.pdf
[3] B.Khan,“Web-basedinstruction:Whatisitandwhyisit?”inWeb-basedInstruction,B.H.Khan,Ed. Englewood
Cliffs,NJ:EducationalTechnologyPublications,1997,pp.5–18.
[4] J. Whitehill, Z. Serpell, Y.-C. Lin, A. Foster, and J. R. Movellan, “The faces of engagement: Automatic
recognitionofstudentengagementfromfacialexpressions,”IEEETransactionsonAffectiveComputing,vol.5,
no.1,pp.86–98,Jan.-March2014.[Online].Available: https://doi.org/10.1109/TAFFC.2014.2316163
[5] O. Mohamad Nezami, M. Dras, L. Hamey, D. Richards, S. Wan, and C. Paris, “Automatic recognition
of student engagement using deep learning and facial expression,” in Machine Learning and Knowledge
Discovery in Databases, ser. Lecture Notes in Computer Science, U. Brefeld, E. Fromont, A. Hotho,
A. Knobbe, M. Maathuis, and C. Robardet, Eds., vol. 11908. Cham: Springer, 2020. [Online]. Available:
https://doi.org/10.1007/978-3-030-46133-1_17
[6] M. Hu, Y. Wei, M. Li, H. Yao, W. Deng, M. Tong, and Q. Liu, “Bimodal learning engagement
recognition from videos in the classroom,” Sensors, vol. 22, no. 16, p. 5932, 2022. [Online]. Available:
https://doi.org/10.3390/s22165932
[7] A.Gupta,A.D’Cunha,K.Awasthi,andV.Balasubramanian,“DAiSEE:Towardsuserengagementrecognitionin
thewild,”arXivpreprint,2016.[Online].Available: https://doi.org/10.48550/arXiv.1609.01885
[8] A.Kamath,A.Biswas,andV.Balasubramanian,“Acrowdsourcedapproachtostudentengagementrecognitionin
e-learningenvironments,”in2016IEEEWinterConferenceonApplicationsofComputerVision(WACV),2016,
pp.1–9.[Online].Available: https://doi.org/10.1109/WACV.2016.7477618
[9] NSSE, “National survey of student engagement,” 2013. [Online]. Available: https://nsse.indiana.edu/nsse/
about-nsse/conceptual-framework/index.html
[10] G. D. Kuh, “Assessing what really matters to student learning inside the national survey of student
engagement,”Change: TheMagazineofHigherLearning,vol.33,no.3,pp.10–17,2001.[Online].Available:
https://doi.org/10.1080/00091380109601795
[11] J.GrocciaandM.S.Hunter,Thefirst-yearseminar: Designing,implementing,andassessingcoursestosupport
studentlearningandsuccess. SanFrancisco: Jossey-Bass,2012.
[12] B. Byrne and R. Guy, “Evaluation of innovative teaching approaches: The moderating effect of
student prior experience,” Creative Education, vol. 03, pp. 755–760, 01 2012. [Online]. Available:
https://doi.org/10.4236/ce.2012.326113
[13] S.R.HarperandS.J.Quaye,StudentEngagementinHigherEducation: TheoreticalPerspectivesandPractical
ApproachesforDiversePopulations. NewYork,NY,USA:Taylor&FrancisGroup,2008.[Online].Available:
https://doi.org/10.4324/9780203894125
[14] J. A. Fredricks, P. C. Blumenfeld, and A. H. Paris, “School engagement: Potential of the concept, state
of the evidence,” Review of Educational Research, vol. 74, no. 1, pp. 59–109, 2004. [Online]. Available:
https://doi.org/10.3102/00346543074001059
[15] M. Ainley, “Connecting with learning: Motivation, affect and cognition in interest processes,” Educational
PsychologyReview,vol.18,pp.391–405,2006.
[16] M. Yorke and P. Knight, “Self-theories: some implications for teaching and learning in higher
education,” Studies in Higher Education, vol. 29, no. 1, pp. 25–37, 2004. [Online]. Available:
https://doi.org/10.1080/1234567032000164859
[17] D.M.A.FazeyandJ.A.Fazey,“Thepotentialforautonomyinlearning: Perceptionsofcompetence,motivation
and locus of control in first-year undergraduate students,” Studies in Higher Education, vol. 26, no. 3, pp.
345–361,2001.[Online].Available: https://doi.org/10.1080/03075070120076309
[18] G.D.Kuh,J.Kinzie,J.A.Buckley,B.K.Bridges,andJ.C.Hayek,“Piecingtogetherthestudentsuccesspuzzle:
Research,propositions,andrecommendations,”ASHEHigherEducationReport,vol.32,no.5,pp.1–182,2007.
[19] X. Ma, M. Xu, Y. Dong, and Z. Sun, “Automatic student engagement in online learning environment based
onneuralturingmachine,”InternationalJournalofInformationandEducationTechnology,vol.11,no.3,pp.
107–111,2021.
11AGeneralModelforDetectingLearnerEngagement: ImplementationandEvaluation
[20] J.E.Groccia,“Whatisstudentengagement?” NewDirectionsforTeachingandLearning,vol.2018,no.154,pp.
11–20,2018.
[21] L.Mehta,A.Mustafa,andAKAD,“Predictionandlocalizationofstudentengagementinthewild,”inDigital
Image Computing: Techniques and Applications (DICTA), 2018 International Conference on. IEEE, 2018.
[Online].Available: https://doi.org/10.1109/DICTA.2018.8615851
[22] J. Liao, Y. Liang, and J. Pan, “Deep facial spatiotemporal network for engagement prediction in
online learning,” Applied Intelligence, vol. 51, pp. 6609–6621, 2021. [Online]. Available: https:
//doi.org/10.1007/s10489-020-02139-8
[23] A.AbediandS.S.Khan,“Improvingstate-of-the-artindetectingstudentengagementwithresnetandtcnhybrid
network,”in202118thConferenceonRobotsandVision(CRV). Canada: IEEE,2021,pp.151–157.[Online].
Available: https://doi.org/10.1109/CRV52889.2021.00028
[24] P.Ma,Y.Wang,J.Shen,S.Petridis,andM.Pantic,“Lip-readingwithdenselyconnectedtemporalconvolutional
networks,” inProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputerVision. IEEE,
2021,pp.2857–2866.[Online].Available: https://doi.org/10.1109/WACV48630.2021.00290
[25] Y.Hu,Z.Jiang,andK.Zhu,“Anoptimizedcnnmodelforengagementrecognitioninane-learningenvironment,”
AppliedSciences,vol.12,no.16,p.8007,2022.[Online].Available: https://doi.org/10.3390/app12168007
[26] T. Baltrusaitis and et al., “Openface 2.0: Facial behavior analysis toolkit,” in Proceedings of the 13th IEEE
InternationalConferenceonAutomaticFaceandGestureRecognition. Xi’an,China: IEEE,2018,pp.59–66.
[Online].Available: https://doi.org/10.1109/FG.2018.00019
[27] T.Selim,I.Elkabani,andM.A.Abdou,“Studentsengagementleveldetectioninonlinee-learningusinghybrid
efficientnetb7togetherwithtcn, lstm, andbi-lstm,” IEEEAccess, vol.10, pp.99573–99583, 2022.[Online].
Available: https://doi.org/10.1109/ACCESS.2022.3206779
[28] Y.Chen,J.Zhou,Q.Gao,J.Gao,andW.Zhang,“MDNN:Predictingstudentengagementviagazedirectionand
facialexpressionincollaborativelearning,” CMES-ComputerModelinginEngineering&Sciences, vol.136,
no.1,pp.381–401,2023.[Online].Available: https://doi.org/10.32604/cmes.2023.023234
[29] A. Abedi, C. Thomas, D. Jayagopi, and S. Khan, “Bag of states: A non-sequential approach
to video-based engagement measurement,” Research Square, 01 2023. [Online]. Available: https:
//doi.org/10.21203/rs.3.rs-2518897/v1
[30] C.Thomas,K.P.Sarma,S.S.Gajula,andD.B.Jayagopi,“Automaticpredictionofpresentationstyleandstudent
engagementfromvideos,”ComputersandEducation: ArtificialIntelligence,vol.3,p.100079,2022.[Online].
Available: https://doi.org/10.1016/j.caeai.2022.100079
[31] C. Thomas, S. Purvaj, and D. B. Jayagopi, “Student engagement from video using unsupervised domain
adaptation,”inIMPROVE,vol.1. Portugal: ScienceandTechnology,2022,pp.118–125.[Online].Available:
https://doi.org/10.5220/0010979400003209
[32] S. E. Whang, Y. Roh, H. Song, and J.-G. Lee, “Data collection and quality challenges in deep learning: a
data-centricaiperspective,”TheVLDBJournal(TheInternationalJournalonVeryLargeDataBases),vol.32,pp.
791–813,2023.[Online].Available: https://doi.org/10.1007/s00778-022-00775-9
[33] I.J.Goodfellow,D.Erhan,P.L.Carrier,A.Courville,M.Mirza,B.Hamner,W.Cukierski,Y.Tang,D.Thaler,
D.-H.Lee,andetal.,“Challengesinrepresentationlearning: Areportonthreemachinelearningcontests,”in
InternationalConferenceonNeuralInformationProcessing. Springer: Springer,2013,pp.117–124.[Online].
Available: https://doi.org/10.1007/978-3-642-42051-1_16
[34] M. SAMBARE, “Fer-2013, learn facial expressions from an image,” 2018. [Online]. Available:
https://www.kaggle.com/datasets/msambare/fer2013
[35] T.Baltrušaitis,M.Mahmoud,andP.Robinson,“Cross-datasetlearningandperson-specificnormalisationfor
automaticactionunitdetection,”in201511thIEEEInternationalConferenceandWorkshopsonAutomaticFace
andGestureRecognition(FG),vol.6. IEEE,2015,pp.1–6.
[36] A. Gudi, “title = Recognizing Semantic Features in Faces using Deep Learning. Thesis, University
of Amsterdam, Netherlands, 2014„” arXiv preprint arXiv:1512.00743, 2015. [Online]. Available:
https://doi.org/10.48550/arXiv.1512.00743
[37] E. Correa, “Emotion recognition using dnn with tensorflow,” https://github.com/isseu/
emotion-recognition-neural-networks,2019.
[38] A. Gupta, A. D’Cunha, K. Awasthi, and V. Balasubramanian, “DAiSEE: Dataset for affective states in e-
environments,”https://people.iith.ac.in/vineethnb/resources/daisee/index.html,2016.
12AGeneralModelforDetectingLearnerEngagement: ImplementationandEvaluation
[39] L.Tomas,D.King,S.Henderson,D.Rigano,andM.Sandhu,TheResolutionofFrustrationinMiddleSchool
ScienceClasses: TheRoleoftheClassroomTeacher. Leiden, TheNetherlands: Brill, 2018, pp.133–156.
[Online].Available: https://brill.com/view/book/edcoll/9789004377912/BP000013.xml
[40] P.RozinandA.B.Cohen,“Highfrequencyoffacialexpressionscorrespondingtoconfusionconcentration,and
worryinananalysisofnaturallyoccurringfacialexpressionsofamericans,”Emotion,vol.3,pp.68–75,2003.
[Online].Available: https://doi.org/10.1037/1528-3542.3.1.68
[41] A.C.GraesserandB.Olde,“Howdoesoneknowwhetherapersonunderstandsadevice? thequalityofthe
questions the person asks when the device breaks down,” Journal of Educational Psychology, vol. 95, pp.
524–536,2003.[Online].Available: https://doi.org/10.1037/0022-0663.95.3.524
[42] J.Piaget,TheOriginsofIntelligence. NewYork: InternationalUniversityPress,1952.
[43] A.C.Graesser,P.Chipman,B.King,B.McDaniel,andS.D’Mello,“Emotionsandlearningwithautotutor,”in
ArtificialIntelligenceinEducation: BuildingTechnologyRichLearningContextsthatWork(AIED07),R.Luckin,
K.R.Koedinger,andJ.Greer,Eds. Washington,DC:IOSPress,2007,pp.554–556.
[44] S.D’MelloandA.Graesser,“Confusion,”inHandbookofEmotionsandEducation,R.PekrunandL.Linnenbrink-
Garcia,Eds. NewYork,NY:Routledge,2014,pp.289–310.
[45] J. Lodge, G. Kennedy, L. Lockyer, A. Arguel, and M. Pachman, “Understanding difficulties and resulting
confusioninlearning: Anintegrativereview,”FrontiersinEducation,vol.3,pp.1–10,2018.[Online].Available:
https://doi.org/10.3389/feduc.2018.00049
13