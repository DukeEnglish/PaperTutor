Iterative Experience Refinement of Software-Developing Agents
ChenQian†♠ JiahaoLi†⋆ YufanDang♠ WeiLiu♠ YiFeiWang♠ ZihaoXie♠
WeizeChen♠ ChengYang♣(cid:66) YingliZhang♦ ZhiyuanLiu♠(cid:66) MaosongSun♠(cid:66)
♠TsinghuaUniversity ⋆DalianUniversityofTechnology
♣BeijingUniversityofPostsandTelecommunications ♦Siemens
qianc62@gmail.com lijihao2021@mail.dlut.edu.cn
yangcheng@bupt.edu.cn liuzy@tsinghua.edu.cn sms@tsinghua.edu.cn
Abstract autonomousagents,enablingthemtoadapttomore
complicated scenarios (Park et al., 2023; Wang
Autonomousagentspoweredbylargelanguage
etal.,2023f;Huaetal.,2023;Wangetal.,2023a,d)
models(LLMs)showsignificantpotentialfor
andtackleabroaderrangeoftasks(Osika,2023;
achieving high autonomy in various scenar-
Gong et al., 2023; Qian et al., 2023a). Further-
iossuchassoftwaredevelopment. Recentre-
more,theadvancementofLLM-basedautonomous
searchhasshownthatLLMagentscanlever-
age past experiences to reduce errors and en- agentshasbroughtaboutanothersignificantbreak-
hance efficiency. However, the static experi- throughthroughtheintegrationofmulti-agentco-
enceparadigm,reliantonafixedcollectionof operation(Parketal.,2023;Lietal.,2023a;Qian
pastexperiencesacquiredheuristically, lacks etal.,2023a). Throughinvolvementinmulti-turn
iterativerefinementandthushampersagents’
communication,agentsactivelyengageinrespon-
adaptability. Inthispaper,weintroducetheIt-
sive or instructive conversations, collaboratively
erativeExperienceRefinementframework,en-
improving the autonomous attainment of a cohe-
ablingLLMagentstorefineexperiencesitera-
tivelyduringtaskexecution. Weproposetwo sive solution for task completion. This paradigm
fundamental patterns: the successive pattern, fostersgreaterautonomyoftheagent,consequently
refiningbasedonnearestexperienceswithina decreasingrelianceonhumanengagement(Lietal.,
taskbatch,andthecumulativepattern,acquir- 2023a;Qianetal.,2023a;Wuetal.,2023). Inorder
ingexperiencesacrossallprevioustaskbatches.
tostudythecooperativedynamicsofautonomous
Augmentedwithourheuristicexperienceelim-
agentsmorepertinently,wechoosesoftwaredevel-
ination,themethodprioritizeshigh-qualityand
opmentasanrepresentativescenario. Thischoice
frequently-usedexperiences,effectivelyman-
is motivated by its complexity, which requires a
agingtheexperiencespaceandenhancingeffi-
ciency. Extensiveexperimentsshowthatwhile combinationofprogrammingandnaturallanguage
the successive pattern may yield superior re- skills(Mills,1976),theprocessualitythatalways
sults,thecumulativepatternprovidesmoresta- demands an deep coding understanding and con-
bleperformance. Moreover,experienceelimi- tinuous adjustment (Barki et al., 1993), and the
nationfacilitatesachievingbetterperformance
objectivity of code that can provide quantifiable
usingjust11.54%ofahigh-qualitysubset.
feedback(ComptonandHauck,2002). Anexam-
1 Introduction ple is ChatDev (Qian et al., 2023a), where LLM-
basedautonomousagentsplayvariousroles(e.g.,
Recently,largelanguagemodels(LLMs)haveat- aninstructivereviewerandaresponsiveprogram-
tained remarkable success, showcasing substan- mer) in a waterfall-like workflow, cooperatively
tial potential in approximating human-like intel- participatinginthesoftwaredevelopmentprocess
ligence(Vaswanietal.,2017;Brownetal.,2020; throughtheirmulti-turncommunication.
Bubeck et al., 2023; Wang et al., 2023b). Fueled Along this line, recent research has focused
bythevastprogressofLLM,autonomousagents on enabling agents to efficiently learn from past
basedonLLMhaveemerged,endowedwithmem- experiences, aiming to prevent recurring errors
ory(Parketal.,2023),planning(Weietal.,2022b), and enhance efficiency in subsequent task execu-
and tool using (Schick et al., 2023). These en- tion (Qian et al., 2023b). These agents work col-
hancementselevatethecapabilitiesofLLM-based laborativelytoacquireandleverageexperiencesac-
quiredfrompasttaskexecutions,substantiallyen-
†EqualContribution.
(cid:66)CorrespondingAuthor. hancingagents’autonomyandtheirproficiencyin
4202
yaM
7
]LC.sc[
1v91240.5042:viXracollectivelyaddressingunseentasks. However,the facilitatesachievingbetterperformancewithjust
acquisitionofexperienceswasaone-timeprocess 11.54%ofahigh-qualitysubsetofexperiences.
usingheuristicrules(Zhaoetal.,2023;Qianetal.,
2 RelatedWork
2023b). Thisstaticexperienceparadigmrestricts
the agent’s ability to adapt to intricate tasks such RecentprogressinLLMsmakesitplayavitalrole
as software development, as it lacks the iterative innaturallanguageprocessing (Brownetal.,2020;
refinement of experiences necessary for adaptive Bubeck et al., 2023; Vaswani et al., 2017; Rad-
improvement. ford et al., 2019; Touvron et al., 2023; Wei et al.,
Tothisend,weproposeanovelIterativeExperi- 2022a; Shanahan et al., 2023; Chen et al., 2021;
enceRefinement(IER)framework,whereinagents Brantsetal.,2007;Chenetal.,2021;Ouyangetal.,
iterativelyrefinetheirpastexperiencesduringtask 2022; Yang et al., 2024; Qin et al., 2023; Kaplan
execution. Thisiterativeprocessinvolvesacycleof et al., 2020; Shumailov et al., 2023) and further
continualacquisitionandutilizationofexperiences. foster the development of autonomous agents in
Technically,weestablishtwofoundationalpatterns independently solving tasks (Zhou et al., 2023a;
forexperiencerefinementacrossvarioustasks: the Wangetal.,2023a;Parketal.,2023;Wangetal.,
successivepatternandthecumulativepattern. In 2023f; Richards, 2023; Osika, 2023; Wang et al.,
the successive pattern, experiences are derived 2024). And enhanced with tool using (Schick
from the latest task batch, while the cumulative etal.,2023;Caietal.,2024;Qinetal.,2024;Ruan
pattern integrates all historical experiences from etal.,2023;Yangetal.,2023),memory(Parketal.,
previous task batches. Moreover, the process of 2023; Sumers et al., 2023) and planning (Chen
accumulatingexperiencesmayinadvertentlylead etal.,2024;Liuetal.,2023),autonomousagents
toanundesiredexpansionoftheexperiencespace, utilizerobustcapabilityofLLMsincomplement-
inevitablyencompassingnumerouslow-qualityor ingcomplextasks (Zhouetal.,2023a;Maetal.,
rarely-usedones. Correspondingly,weproposea 2023;Zhangetal.,2023;Wangetal.,2023c;Ding
heuristicexperienceeliminationmechanism,which etal.,2023;Weng,2023;Osika,2023;Parketal.,
prioritizesfrequentlyemployedexperiencesintask 2023;Zhouetal.,2023b;Zhuetal.,2023).
executionwhilediscardingidentifiedlow-quality Wheninvolvedinautonomouscommunication
ones,streamliningtheevolutionofexperiencesto- amongmultipleagents,thesemulti-agentsystems
wardgreaterefficiency. Weconductedexperiments exhibitenhancedcapabilitiesforaddressingcom-
fromtheperspectivesofbothsoftwarequalityand plextasks(Lietal.,2023a;Qianetal.,2023a;Wu
experiencerefinement,demonstratingthesuperior etal.,2023;Hongetal.,2024;Lietal.,2023b;Park
effectivenessofiterativerefinementinenhancing etal.,2023;Zhouetal.,2023b;Chenetal.,2024;
agents’experiencesfortaskexecution. Chanetal.,2024;Chenetal.,2023;Cohenetal.,
Insummary,themaincontributionsthatwehave 2023;Huaetal.,2023). Morerecently,agentsen-
madeareoutlinedasfollows: dowedwithinstructiveandresponsiveexperiences
furtherexhibitnotablepromotionintheircoopera-
• Tothebestofourknowledge,wearethefirstto tivetaskexecution(Zhongetal.,2023;Lewisetal.,
introduceanovelexperiencerefinementframe- 2020).
work. This new paradigm, grounded in the dy- For example, ExpeL (Zhao et al., 2023) inno-
namic iteration of past experiences, empowers vativelyaccumulatesexperiencesfromsuccessful
agentstoadaptivelysolvenewtasksthroughcon- historical trajectories and leverages these experi-
tinualacquisition,utilizationandelimination. encesduringinference. ExperientialCo-Learning
(ECL) (Qian et al., 2023b) focuses on collecting
• We propose a heuristic mechanism to experi-
shortcut-oriented experiences from past trajecto-
enceeliminationthatprioritizeshigh-qualityand
ries, enabling agents to more effectively handle
frequently-utilizedexperiences,therebymitigat-
unseentasks.
inginefficiencyissuesarisingfromthepotential
expansionoftheexperiencespace. 3 Methodology
• Through extensive experiments, we found that The previous design of agents’ experiences is
while the successive pattern may yield higher throughheuristicrulesinaone-timeprocess(Qian
results,thecumulativepatternoffersmorestable et al., 2023b); these static experiences lack the
performance. Besides, experience elimination capabilitytobedynamicallyrefinedduringfuturetask executions. To tackle the challenge, we
introduce a iterative experience refinement (IER)
framework,inwhichagentsareequippedwithex-
periencesthatundergodynamicrefinementduring
agents’ task-solving processes. Given the overly
fine-grained propagation of experiences between
individualtasks,ourapproachinvolvespartitioning
all tasks into non-overlapping task batches
⟨T ,T ,··· ,T ⟩. Ineachbatch,agentsiteratively
1 2 n
generateanewexperiencepool,whichisthenprop-
agatedtothesubsequentbatches. Inthefollowing,
similar to inheritance, experiences are generated
and propagatedfrom preceding taskbatches (i.e.,
predecessors) to subsequent task batches (i.e.,
descendants). Technically,wehavedevelopedtwo
primarypatternsforthepropagationofexperience
betweenvarioustasks: thesuccessivepatternand
thecumulativepattern. Inthesuccessivepattern,a Figure 1: The task execution chain constructed for
shortcut-orientedexperienceacquiring. Theexecution
descendant’experiencepoolinheritsfromthelatest
chaincreatesproceduraltrajectoriesforvarioustrain-
predecessor, prioritizing the latest knowledge,
ing tasks, where we acquire "shortcuts" linking non-
whileinthecumulativepattern,adescendantinher-
adjacentnodesasagents’experiences.
itsthehistoricalexperiencesfromallpreviouspre-
decessors. Furthermore,consideringthepractical Acquisition Recognizing that not all progres-
limitationoftheexperiencepoolsize,wepropose sionsinthechain(i.e.,asingleroundofsoftware
empirically powerful mechanism for eliminating optimization)necessarilyleadtobettersolutions,
low-quality experiences during the propagation weopttoacquiremoreefficientexperiencesfrom
process based on the information density and non-existingedgesinthechain. AsdepictedinFig-
utilizationfrequencyoftheagents’experiences. ure 1, we traverse non-adjacent node pairs along
theexecutionchainandacquireall"shortcuts"link-
3.1 ExperienceAcquisitionandUtilization
ingnon-adjacentnodes:
Thismoduleaimstoexecutethetaskswithinevery
(cid:57)(cid:57)(cid:75)
batchtoacquireexperiencesforcontinuousitera- E = {(s ,s s ,s )|s ,s ∈N∧
i i j j i j
(2)
tionoftheexperiencepool. IllustratedinFigure1,
(s ,·,s ) ∈/ E ∧[[s → s ]]}
i j i j
followingQianetal.(2023b)whereaninstructive
agentandaresponsiveagentareinvolved,through- where[[s →s ]]indicatesthats isreachablefrom
i j j
(cid:57)(cid:57)(cid:75)
out their cooperative task execution, we record a s ,s s denotesapseudoinstructionviaastandard
i i j
seriesofinstructions(I={i 1,i 2,··· ,i n})along-
self-instruction mechanism (Wang et al., 2023e).
sideacorrespondingseriesofrespondedsolutions Thismechanismextractsshortcutsinsteadoftheex-
(S ={s 1,s 2,··· ,s n}), with each solution repre- istingedges,whichcaneffectivelymotivateagents
senting a complete software code. The commu- usetheshortcutsastheirexperiencestoengagein
nication process is described as a directed chain shortcutthinkingandhasbeenvalidatedbyprevi-
G = (N,E):
ouswork(Qianetal.,2023b).
Utilization Afterexperienceacquisition,ashort-
N={s j|s j ∈ S}∪{s 0} (cid:57)(cid:57)(cid:75) (cid:57)(cid:57)(cid:75)
(1) cut(s ,s s ,s )canbedividedinto(s ,s s ),indi-
i i j j i i j
E={(s ,i ,s )|s ,s ∈S,i ∈I}
j j+1 j+1 j j+1 j+1 catingsolution-to-instructionknowledgepossessed
(cid:57)(cid:57)(cid:75)
whereN denotesthenodesthatcorrespondtothe bytheinstructiveagent,and(s is j,s j),signifying
solutions (with s represents the initial, usually instruction-to-solution knowledge held by the re-
0
empty solution), and E represents the edges that sponsiveagent. Thesedistinctkey-valueformsof
correspond to the instructions. And each edge knowledge are amalgamated into the agents’ ex-
(s ,i ,s ) is guided by the instruction i , perience pools for utilization in their collective
j j+1 j+1 j+1
illustrating the transition from one solution s to reasoning. When executing a unseen task, for a
j
themodifiedones j+1. current solution s j, the instructive agent initiallyemploysretrievalmechanismtoaccessempirical
instructions closely matching the latent meaning
of the query from the solution-to-instruction ex-
perience pool. These retrieved instructions serve
as few-shot examples to assist in generating an
experience-augmentedinstructioni∗ . Similarly,
j+1
the responsive agent retrieves the responses with
thehighestmatchingdegreetotheinstructionfrom
the instruction-to-solution experience pool, serv-
ing as few-shot examples to assist in responding
withanexperience-augmentedsolutions∗ . Thus,
j+1
thewholereasoningprocessisrepresentedasase-
quenceofpairs{(i∗,s∗),(i∗,s∗),···},whereeach
1 1 2 2
includes an experience-enhanced instruction and
thecorrespondingsolution.
3.2 ExperiencePropagation
Successive Pattern Cumulative Pattern
Nonetheless, these static experiences can limit
Task Batch Experience Propagation
agents’ adaptability to new tasks and hinder con-
tinuouslearning. Addressingtherigidityofexperi-
Figure2: Thesuccessivepattern(left)allowseachtask
encesisessentialforovercomingtheselimitations.
batchtoutilizetheexperiencepoolcollectedfromthe
Asagentsaccumulateexperiencesfrompredeces- precedingbatch. Thecumulativepattern(right)enables
sorsforuseinthecurrentbatch,thecurrentbatch eachbatchoftaskstoleveragetheexperiencepoolac-
can also naturally generate new experiences that quiredfromallpreviousbatches.
arepropagatedfordescendants. Basedonthis,we
Thesetwotypesofexperiencepropagationcanbe
propose two types of fundamental experience re-
likenedtothepassingdownofknowledgethrough
finement patterns, namely the successive pattern
generations. Theformerisakintodescendantin-
andthecumulativepattern.
heriting knowledge from their parents, while the
SuccessivePattern Leveragingrecentlyacquired
latter is akin to descendant inheriting knowledge
experiencesalignsnaturallywithourobjectives. In-
fromtheirparentsandallpreviouspredecessors.
spiredbythisinsight,weintroducethesuccessive
3.3 ExperienceElimination
pattern,depictedontheleftsideofFigure2. When
executingataskbatchT i,agentscangatherexperi- Recognizingthattheprocessofaccumulatingex-
encesE i−1 acquiredfromthenearestpredecessor, periencesmayinadvertentlyleadtoanundesired
i.e.,T i−1,whichconstitutestheirexperiencesinthe expansion of the experience space, inevitably en-
nextdescendant. UsingE torepresenttheacquired compassingnumerouslow-qualityorrarely-used
experiencepool,andµ(E,T)todenoteexperience ones. Correspondingly,weproposeaheuristicex-
utilization on a task batch T, this process can be perienceeliminationmechanismbasedonthein-
expressedasfollows: formationdensityandutilizationfrequencyofex-
periences,whichprioritizesfrequentlyemployed
E 1 = ∅, E i = µ(E i−1,T i) (3) experiencesintaskexecutionwhilediscardingiden-
tifiedlow-qualityones,streamliningtheevolution
Cumulative Pattern Alternatively, we explore ofexperiencestowardgreaterefficiency.
whethercontinuousexperienceaccumulationcan Concretely, we measure the information gain
elevate task-solving abilities. In the cumulative of each shortcut by selectively identifying non-
pattern illustrated on the right side of Figure 2, adjacent nodes whose solution optimization pro-
agentscanemployexperiencesfromallprevious cessexhibitsaninformationgainexceedingapre-
experiencepools{E 1,E 2,...,E i−1}intheexecution definedthresholdϵ:
ofthetaskbatchT :
i (cid:57)(cid:57)(cid:75) (cid:57)(cid:57)(cid:75)
E¯= {(s ,s s ,s )|(s ,s s ,s ) ∈ E
i i j j i i j j
i−1
(cid:0) (cid:91) (cid:1) ∧ω(s )−ω(s ) ≥ ϵ} (5)
E = ∅, E = µ E ,T (4) j i
1 i j i
ω(s )=sim(s ,task)·sim(s ,s )·[[s ]]
j=1 j j j |N| jwheresim(·,·)calculatesthesimilaritybetweena employeeswithinavirtualsoftwarecompany. The
solutionwithanothernodeorataskrequirement, software generation task is divided into different
utilizingcosinesimilarityandexternalembedders, sub-tasks,eachadeptlyundertakenbyanagentbe-
[[·]] indicates a binary signal indicating whether spoke to a predefined role. ChatDev (Qian et al.,
compilationissuccessfulviaanexternalcompiler. 2023a)isapowerfulmulti-agentcollaborativesoft-
Additionally,weobservealong-taildistribution waredevelopmentframeworkachievedviaagent
inthedynamicutilizationoftheexperiencepool, communication. ChatDevnavigatestheintricacies
implying that the tail is in fact rarely used. Uti- ofsub-taskresolutionthroughcommunicationes-
lizingthefrequencydistributionoftheexperience tablishedbetweentwoautonomousagents. Inthe
pool utilized for each batch, we selectively elim- proceduralsequence,aninstructiveagentdescribes
inate certain experiences to obtain a subset with the details of a sub-task, such as design or code
relativelyhighretrievalprobability: review,subsequentlyscrutinizingandsolicitingre-
finementsfromanassistantagent,whichiteratively
rank(E)
(cid:88) f(e) refinestheoutcomeinalignmentwithreceivedin-
Eˆ= {e|e ∈ E ∧ ≤ θ} (6)
(cid:80) structions. ECL(Qianetal.,2023b)introducesthe
f(e)
j=1 e∈E historicalexperiencesintothesoftware-developing
agents. Thisentailstheincorporationofexperien-
where f(e) represents the retrieval frequency of
tial shortcuts derived from the agents’ historical
e, rank(·) denotes the index of retrieval frequen-
trajectory,whichareextractedfrompriorsoftware
ciessortedindescendingorder,andθ representsa
generation endeavors. These shortcuts, encapsu-
fractilethreshold.
latingdistilledinsights,areutilizedforeffectively
By combining these criteria, which consider
boostingfuturetaskexecution.
both static information gain and dynamic usage
Datasets Following ECL (Qian et al., 2023b),
frequency,wetaketheunionofbothtoensurethe
we evaluate the quality of generated software on
preservationofhigh-qualityexperiences:
theSRDDdataset(Qianetal.,2023a),whichcon-
E = ∅, E = µ(E¯ ,T ) tains1,200softwarerequirementdescriptionsfrom
1 2 1 2
(7)
E = µ(E¯ ∪Eˆ ,T ) 40 common categories. We perform hierarchical
i i−1 i−2 i
samplingonthedatasetbysoftwarecategoryand
Notethattheinformation-gain-basedelimination divide the dataset into 6 task batches. It ensures
canbedirectlyprocessedononegenerationoftask that the software description of each batch is in-
batch,buttheretrieval-basedonerequiresthepar- dependentandidenticallydistributed,andhasthe
ticipationofatleasttwogenerationsoftaskbatches. samesoftwarecategorydistribution.
Thisheuristicmechanismprioritizeshigh-quality Metrics Evaluating software presents a signifi-
andfrequently-utilizedexperiences,therebymiti- cantchallenge,particularlywhenaimingforacom-
gatingpotentialinefficienciesarisingfromthepo- prehensiveevaluation. Traditionalmetricssuchas
tentialexpansionoftheexperiencespace. function-level code assessment (e.g., pass@k) do
notseamlesslyextendtoevaluatingentiresoftware
4 Evaluation
systemscomprehensively. Thischallengeprimar-
Baselines Ourselectionencompassesseveralrep-
ily arises from the difficulty in creating manual
resentativeandpowerfulLLM-drivenagentframe-
orautomatedtestcasesformuchofthesoftware,
worksspecially-designedforsoftwaredevelopment.
especiallyinscenariosfrequentcommunications,
GPT-Engineer(Osika,2023)standsasapioneering
involvingcomplexinterfaces,ornon-deterministic
endeavor, harnessing the capabilities of an LLM-
feedback. Tosolvethischallenge,followingQian
powered agent to engage in intricate multi-step
etal.(2023b),weadoptthreequantifiableandob-
reasoning. Thisinnovationtranscendstheconven-
jectivedimensionstoevaluatespecificaspectsof
tionalboundsofcode-levelgenerationwithoutthe
thesoftware,alongwithacomprehensivemetricto
perception of past experiences, ascending to the
conductamoreholisticevaluation:
realmofrepository-levelgenerationandrealizing
automatedsoftwareengineering. MetaGPT(Hong • Completeness (α) assesses the extent of code
et al., 2024) elevates from a single-agent pattern completioninsoftwaredevelopment,calculated
toamulti-agentdesign. Withinthisconceptualiza- astheproportionofsoftwaredevoidof"TODO"
tion,individualagentsassumediverserolesakinto codesnippets. AgreaterscoredenotesahigherCompleteness Executability Consistency Quality Duration
GPTEngineer 0.4824 0.3583 0.7887 0.1363 15.6000
MetaGPT 0.4472 0.4208 0.7649 0.1439 154.0000
ChatDev 0.7337 0.8040 0.7909 0.4665 148.2150
ECL 0.8442 0.8643 0.7915 0.5775 122.7750
IER-Successive 0.8744 0.9146 0.7968 0.6372 179.4437
IER-Cumulative 0.8492 0.9347 0.7983 0.6337 181.5961
Table 1: The average performance across all methods. The highest scores are indicated in bold, while the
second-highestscoresareunderlined.
degreeofsoftwarecompletion. whichalsoincorporatesashortcut-orientedexperi-
encemechanism,bothsuccessiveandcumulative
• Executability(β)evaluatesthesoftware’scapa-
patternsdemonstrateuptoan11%relativeimprove-
bilitytooperatecorrectlyinacompilationenvi-
ment, with the former slightly surpassing the lat-
ronment,measuredastheproportionofsoftware
ter. Furthermore,theaveragedurationofIERsoft-
thatcompilessuccessfullyandexecutesdirectly.
waremanufacturingdoesnotsubstantiallyincrease
Agreaterscoredenotesthatthesoftwarehasa
comparedtothebaselines, indicatingthatitdoes
higherprobabilitytoexecutesuccessfully.
notcauseexcessivetimedelays, partlyattributed
• Consistency (γ) assesses the consistency be- to the efficiency of the vector-based retrieval de-
tweenthedevelopedsoftwareandtheinitialnat- sign. TheadvancementfacilitatedbyIERcanbe
ural language requirements. measured as the attributedtoitskeystrengths: 1)Thismethoden-
cosinedistancebetweentheembeddingsofthe ables the bypassing of certain low-level code er-
textual requirements and the source code. A rors and implementation issues through shortcut
greaterscoredenotesacloseralignmentwiththe thinking. Thisallowsagentstoconcentratemore
originalrequirements. onreviewingandoptimizingintrinsiccode-related
problemsratherthansuperficialimplementations,
• Quality(α×β×γ)servesasacomprehensivemet-
ultimatelyenhancingsoftwarequality. 2)Besides,
riccombiningcompleteness,executability,and
shortcut-oriented experiences offer more precise
consistency to evaluate the quality of software
anddetailedinstructionsandsolutionsateachopti-
comprehensively. A greater quality score indi-
mizationthroughtheircommunication. Thisguid-
catesbetteroverallsoftwarequality,reducingthe
ancedirectssoftware-developingagentstoproduce
necessityforadditionalmanualintervention.
codewithgreatercompleteness,executability,and
Implementation Details We use ChatGPT-3.5 consistency,therebyreducingexcessivedelays. 3)
asthefoundationalmodels. Inexperienceacquisi- Crucially,experiencerefinementthroughiterative
tion,weutilizetext-embedding-ada-002fortext propagationandeliminationensurestheretention
andcodeembeddings. Thethresholdsforexperi- ofhigh-qualityexperienceswhileeliminatinglow-
enceeliminationaresetatϵ =0.95andθ =0.95. In quality ones, which makes the experiences more
theutilizationofexperience,thekey-valueknowl- adaptiveforcontinuoustask-solvingscenarios.
edge is used through vector-based retrieval. To
4.2 PropagationAnalysis
ensure comparability, all other hyperparameters
Inadditiontoassessingeffectivenessofsoftware
andenvironmentalconfigurationsremainidentical
quality, we show that agents equipped with high-
acrossallbaselinesandourapproach.
qualityexperiencesplayacrucialroleinenhancing
4.1 QualityAnalysis
theefficiencyofsoftwareproduction. Pleasenote
Webeginbyassessingthesoftwaregenerationqual- thatourmethodexplicitlydividesthesoftwarede-
ity of our IER and other baselines. As shown velopmentprocessintocoding,reviewing,andtest-
in Table 1, there is a significant improvement ingphases.1 Meanwhile,allthetasksofthedataset
overinexperiencedmethodssuchasGPTEngineer,
1Thecodingphaseinvolvesasingleroundofagents’coop-
MetaGPT,andChatDev,evidentacrossallquality-
erativecommunication,whilethereviewingandtestingphases
related metrics. Additionally, compared to ECL, eachentailmultiplerounds.Completeness Executability Consistency Quality
0.94 0.650
0.900 S Cu uc mc ue ls as ti iv ve e 0.92 S Cu uc mc ue ls as ti iv ve e 0.798 S Cu uc mc ue ls as ti iv ve e 0.625 S Cu uc mc ue ls as ti iv ve e
0.875 0.90 00 .. 77 99 67 0.600
0.850 0.88 0.795 0.575
0.825 0.86 0.794 0.550
0.800 0.84 0.793 0.525
000 ... 777 257 505
1 2 3 4 5 6
00 .. 88 02
1 2 3 4 5 6
00 .. 77 99 12
1 2 3 4 5 6
00 .. 45 70 50
1 2 3 4 5 6
Task Batch ID Task Batch ID Task Batch ID Task Batch ID
Figure3: Theaverageperformanceforeachtaskbatchacrossvariousdimensions.
Review Efficiency Test Efficiency Overall Efficiency
4.90 12.50
7.6 Successive Successive Successive
7.4 Cumulative 4.88 Cumulative 12.25 Cumulative
7.2 4.86 12.00
7.0 4.84 11.75
6.8 4.82 11.50
6.6 4.80 11.25
6.4 11.00
4.78
6.2 10.75
4.76
6.0 10.50
1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6
Task Batch ID Task Batch ID Task Batch ID
Figure4:Thephaseefficiencypertaskbatchacrossvariousdimensions.ReviewEfficiencyiscalculatedbyaveraging
theroundsofcodereview,derivedfromthedifferencebetweentheactualandmaximumreviewroundsconductedby
agents.TestEfficiencymeasuresefficiencyduringtesting,whileOverallEfficiencyaccountsforallinteractiverounds
acrossphases,reflectingagents’whole-processsoftwareoptimization. Higherresultsindicatefasteradherence
tosoftwarestandards,reducingthenecessityforadditionalmanualinvolvementandtherebyenhancingsoftware
generationefficiency.
aresplitinto6disjointbatches. Here,weexamine
Successive
43.83%
cross-batch efficiency in the software generation Cumulative 42.13% 42.15% 40.72%
processoverdifferentphases. 36.49%36.49%
Pattern Comparison Figure 3 and Figure 4 il- 24.79%
20.98%
lustratethequalityandefficiencyresultsundertwo
15.89%
differentrefinementpatterns. Initially,bothcumu- 14.21%
lativeandsuccessivepatternsyieldidenticalresults
inthefirstandsecondbatches. Thisisbecausethe
0.00% 0.00%
firstbatchdoesn’tuseexperience,andthesecond
1 2 3 4 5 6
Task Batch ID
batchsolelyreliesonexperiencepropagatedfrom
thefirst,resultinginnodiscernibledifferencebe- Figure 5: The retrieval hit ratio across different task
batches, calculated by dividing the number of experi-
tweenthetwopatterns. Furthermore,bothpatterns
encesretrievedbythetotalnumberofexperiences.
show a noticeable upward trend over subsequent
batches,whichverifiestheeffectivenessofourpro-
posediterativerefinementparadigm. Interestingly, entireexperiencepoolinthesuccessivepatternin-
as experience is continuously propagated among troduces the risk of instability. Poor experience
different task batches, the quality and efficiency refinementsincertainbatchescanadverselyaffect
of software manufacturing consistently improve. theentireexperiencepool.
Thecumulativepatternexhibitsamorestabletrend UtilizationAnalysis Figure5illustratesthefluc-
comparedtothesuccessivepattern. Thisstability tuation in experience retrieval hit rates across
stems from its experience pool containing expe- batches for the two patterns. Notably, distinct
riences from all previous batches, leading to less trendscanbeobservedbetweenthesuccessiveand
drasticchangeswitheachiteration. Whilethesuc- cumulative patterns. In the successive pattern, a
cessivepatternmayachieveahigherupperbound steadyincreaseinthehitratioisobserved,reach-
inqualityandefficiency,itsexperiencepoolscope ingitspeakinthefifthbatch. Thistrendhighlights
remainslimitedtoexperiencesfromtheprevious theincrementalimprovementinexperiencequality
batch. Onthecontrary,constantrefinementofthe witheachiteration,enablinggreaterutilizationof
egatnecreP0.70 0.68
Successive 0.67
0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.65 C Elu imm iu nl aa tt ii ov ne 0.65
0.620.63
0.65 0.640.640.64 0.640.63
100.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.60 0.580.58 0.58 0.59
0.55
47.10% 52.90% 0.00% 0.00% 0.00% 0.00%
0.50
0.470.470.47
0.45
35.39% 34.55% 30.06% 0.00% 0.00% 0.00%
0.40
1 2 3 4 5 6
Task Batch ID 22.73% 31.82% 25.76% 19.70% 0.00% 0.00%
Figure7: Comparisonofsoftwarequalitybetweenthe
23.05% 21.33% 24.21% 16.14% 15.27% 0.00% fundamental patterns and the variant enhanced with
experienceelimination.
1 2 3 4 5
Predecessor Task Batch ID
4.3 EliminationAnalysis
Figure 6: The distribution of the cumulative pattern
acrossalltaskbatches. Pleasenotethatthedistribution Wehaveshownthatthecumulativepatternprovides
ofthesuccessivepatternisnotdepicted,asitonlyshows amorestableutilizationofexperiences,potentially
asinglediagonallineresemblinganidentitymatrix.
recallingabroaderrangeofhistoricalexperiences
from all predecessors. As the pool size contin-
uously expands, high-quality experiences in the
experiencesbydescendants. However,it’simpor- patterninevitablybecomedilutedacrosstheentire
tant to note that experience quality may stabilize experience pool, resulting in a long-tail distribu-
aftersurpassingacertainthreshold,leadingtosta- tionofexperienceutilization. AsshowninFigure
bilizationratherthancontinuousimprovement. In 7,theeliminationmechanismguaranteesthecon-
contrast,thecumulativepatternexhibitsagradual centrationofhigh-qualityexperiencesinthepool,
declineinthehitratio,indicatingadegradationin resulting in comparable or even superior quality
propagatedexperiencequalityduetotheexponen- metricsacrossallbatches. Empirically,themecha-
tialgrowthoftheexperiencepool,whichinevitably nisminoursettingutilizesonly11.54%oftheex-
includesnumerouslow-qualityorrarely-usedones. periencepoolsize,comparedtothenon-eliminated
Thisphenomenonunderscorestheurgentneedfor one, resulting in a total of 930 experiences after
experienceelimination,especiallyforthecumula- elimination, downfrom8053initially. Thisnatu-
tivepattern,aligningwiththeintuitivemotivation rallystrikesatrade-offbetweenthevolumeandthe
forproposingthiscrucialmechanism. utilizationofexperiences,makingithighlyrecom-
mendedforapplicationinreal-worldsystems.
5 Conclusion
Utilization Distribution Having explored the
We’ve introduced an iterative experience refine-
retrieval-basedutilizationoftheexperiencepoolby
ment framework, enabling LLM agents to refine
descendants,wenowanalyzetheoveralldistribu-
experiencesiterativelyduringcontinualtaskexecu-
tionofexperiencesutilizedbyonebatchfromall
tion. Weproposedboththesuccessiveandcumula-
itspredecessors,resultingintheutilizationdistri-
tivepatternsforexperiencerefinement,alongside
butiondepictedinFigure6. Ourfindingsregarding
a heuristic experience elimination mechanism to
the utilization distribution in the cumulative pat-
effectivelymanagetheexperiencespacewhileen-
ternaresummarizedasfollows: 1)Experiencesob-
hancingperformance. Ourexperimentsshowthat
tainedfromapredecessorareutilizedbyalldescen-
whilethesuccessivepatternmayyieldhigherper-
dants,notonlythenearbyone. 2)Vertically,there
formance, the cumulative pattern provides more
is a decline in each column from top to bottom,
stableperformance. Additionally,experienceelimi-
suggestingareductionintheutilizationfrequency
nationallowsachievingsuperiorperformanceusing
of experiences produced by more distant descen-
only 11.54% of a high-quality subset. We antici-
dants. 3)Horizontally,experiencesacquiredbya
patethatourinsightswillcatalyzeaparadigmshift
descendantarenotmainlyderivedfromitsnearest
inshapingthedesignofLLMagents,drivingthem
predecessorbutaredistributedapproximatelyuni-
towardsgreaterautonomyandfosteringevolution-
formly, highlighting that experiences propagated
arygrowthincollectiveintelligence.
fromdifferentpredecessorsremainrelevant.
DI
hctaB
ksaT
tnadnecseD
1
2
3
4
5
6
ytilauQReferences RoiCohen,MayHamri,MorGeva,andAmirGlober-
son.2023. LMvsLM:DetectingFactualErrorsvia
HenriBarki,SuzanneRivard,andJeanTalbot.1993. To-
CrossExamination. InProceedingsofthe2023Con-
wardanAssessmentofSoftwareDevelopmentRisk.
ferenceonEmpiricalMethodsinNaturalLanguage
InJournalofManagementInformationSystems,vol-
Processing(EMNLP),pages12621–12640.
ume10,pages203–225.
Katherine Compton and Scott Hauck. 2002. Recon-
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
figurableComputing: aSurveyofSystemsandSoft-
Och,andJeffreyDean.2007. LargeLanguageMod-
ware. InACMComputingSurveys(csuR),volume34,
els in Machine Translation. In Proceedings of the
pages171–210.
2007JointConferenceonEmpiricalMethodsinNat-
uralLanguageProcessingandComputationalNat-
ShiyingDing,XinyiChen,YanFang,WenruiLiu,Yiwu
ural Language Learning (EMNLP-CoNLL), pages
Qiu, and Chunlei Chai. 2023. DesignGPT: Multi-
858–867.
Agent Collaboration in Design. In arXiv preprint
Tom Brown, Benjamin Mann, Nick Ryder, Melanie arXiv:2311.11591.
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
Neelakantan,PranavShyam,GirishSastry,Amanda RanGong,QiuyuanHuang,XiaojianMa,HoiVo,Zane
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Durante, Yusuke Noda, Zilong Zheng, Song-Chun
Gretchen Krueger, Tom Henighan, Rewon Child, Zhu,DemetriTerzopoulos,LiFei-Fei,andJianfeng
AdityaRamesh,DanielZiegler,JeffreyWu,Clemens Gao.2023. MindAgent: EmergentGamingInterac-
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma- tion. InarXivpreprintarXiv:2309.09971.
teusz Litwin, Scott Gray, Benjamin Chess, Jack
SiruiHong,MingchenZhuge,JonathanChen,Xiawu
Clark, ChristopherBerner, SamMcCandlish, Alec
Zheng,YuhengCheng,CeyaoZhang,JinlinWang,
Radford, Ilya Sutskever, and Dario Amodei. 2020.
ZiliWang,StevenKaShingYau,ZijuanLin,Liyang
Language Models are Few-Shot Learners. In Ad-
Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu,
vances in Neural Information Processing Systems
andJürgenSchmidhuber.2024. MetaGPT:MetaPro-
(NeurIPS),volume33,pages1877–1901.
grammingforAMulti-AgentCollaborativeFrame-
Sébastien Bubeck, Varun Chandrasekaran, Ronen El- work. In The Twelfth International Conference on
dan,JohannesGehrke,EricHorvitz,EceKamar,Pe- LearningRepresentations(ICLR).
ter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
etal.2023. SparksofArtificialGeneralIntelligence: Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei,
Early Experiments with GPT-4. In arXiv preprint Jianchao Ji, Yingqiang Ge, Libby Hemphill, and
arXiv:2303.12712. Yongfeng Zhang. 2023. War and Peace (WarA-
gent): Large Language Model-based Multi-Agent
TianleCai, XuezhiWang, TengyuMa, XinyunChen, Simulation of World Wars. In arXiv preprint
andDennyZhou.2024. LargeLanguageModelsas arXiv:2311.17227.
ToolMakers. InTheTwelfthInternationalConfer-
enceonLearningRepresentations(ICLR). JaredKaplan,SamMcCandlish,TomHenighan,TomB.
Brown,BenjaminChess,RewonChild,ScottGray,
Chi-MinChan,WeizeChen,YushengSu,JianxuanYu,
AlecRadford,JeffreyWu,andDarioAmodei.2020.
WeiXue,ShanghangZhang,JieFu,andZhiyuanLiu.
ScalingLawsforNeuralLanguageModels. InarXiv
2024. Chateval: Towards Better LLM-based Eval-
preprintarXiv:2001.08361.
uatorsthroughMulti-agentDebate. InTheTwelfth
International Conference on Learning Representa-
PatrickLewis,EthanPerez,AleksandraPiktus,Fabio
tions(ICLR).
Petroni,VladimirKarpukhin,NamanGoyal,Hein-
richKüttler, MikeLewis, Wen-tauYih, TimRock-
Dake Chen, Hanbin Wang, Yunhao Huo, Yuzhao Li,
täschel, Sebastian Riedel, and Douwe Kiela. 2020.
andHaoyangZhang.2023. GameGPT:Multi-agent
Retrieval-Augmented Generation for Knowledge-
CollaborativeFrameworkforGameDevelopment. In
IntensiveNLPTasks. InAdvancesinNeuralInfor-
arXivpreprintarXiv:2310.08067.
mation Processing Systems (NeurIPS), volume 33,
MarkChen,JerryTworek,HeewooJun,QimingYuan, pages9459–9474.
Henrique Ponde de Oliveira Pinto, Jared Kaplan,
HarriEdwards,YuriBurda,NicholasJoseph,Greg Guohao Li, Hasan Abed Al Kader Hammoud, Hani
Brockman, et al. 2021. Evaluating Large Lan- Itani, Dmitrii Khizbullin, and Bernard Ghanem.
guageModelsTrainedonCode. InarXivpreprint 2023a. CAMEL:CommunicativeAgentsfor”Mind”
arXiv:2107.03374. ExplorationofLargeLanguageModelSociety. In
Thirty-seventh Conference on Neural Information
Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, ProcessingSystems(NeurIPS).
ChenfeiYuan,ChenQian,Chi-MinChan,YujiaQin,
YaxiLu,RuobingXie,etal.2024. Agentverse: Fa- YuanLi,YixuanZhang,andLichaoSun.2023b. Metaa-
cilitating Multi-agent Collaboration and Exploring gents: SimulatingInteractionsofHumanBehaviors
Emergent Behaviors in Agents. In The Twelfth In- forLLM-basedTask-orientedCoordinationviaCol-
ternationalConferenceonLearningRepresentations laborative Generative Agents. In arXiv preprint
(ICLR). arXiv:2310.06500.Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, AlecRadford,JeffreyWu,RewonChild,DavidLuan,
Shelby Heinecke, Rithesh Murthy, Yihao Feng, DarioAmodei,IlyaSutskever,etal.2019. Language
ZeyuanChen,JuanCarlosNiebles,DevanshArpit, Models are Unsupervised Multitask Learners. In
RanXu,PhilMui,HuanWang,CaimingXiong,and OpenAIblog,volume1,page9.
SilvioSavarese.2023. BOLAA:Benchmarkingand
OrchestratingLLM-augmentedAutonomousAgents. Toran Bruce Richards. 2023. AutoGPT. In
InarXivpreprintarXiv:2308.05960. https://github.com/Significant-Gravitas/AutoGPT.
KaixinMa, HongmingZhang, HongweiWang, Xiao- JingqingRuan,YiHongChen,BinZhang,ZhiweiXu,
manPan,andDongYu.2023. LASER:LLMagent Tianpeng Bao, du qing, shi shiwei, Hangyu Mao,
withstate-spaceexplorationforwebnavigation. In XingyuZeng,andRuiZhao.2023. TPTU:TaskPlan-
NeurIPS2023FoundationModelsforDecisionMak- ningandToolUsageofLargeLanguageModel-based
ingWorkshop. AIAgents. InNeurIPS2023FoundationModelsfor
DecisionMakingWorkshop.
HarlanDMills.1976. Softwaredevelopment. InIEEE
TransactionsonSoftwareEngineering,4,pages265–
TimoSchick,JaneDwivedi-Yu,RobertoDessì,Roberta
273.
Raileanu,MariaLomeli,LukeZettlemoyer,Nicola
Cancedda,andThomasScialom.2023. Toolformer:
Anton Osika. 2023. GPT-Engineer. In
Language Models Can Teach Themselves to Use
https://github.com/AntonOsika/gpt-engineer.
Tools. InarXivpreprintarXiv:2302.04761.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
MurrayShanahan,KyleMcDonell,andLariaReynolds.
CarrollWainwright,PamelaMishkin,ChongZhang,
2023. RolePlaywithLargeLanguageModels. In
SandhiniAgarwal,KatarinaSlama,AlexRay,John
Nature,volume623,pages493–498.
Schulman,JacobHilton,FraserKelton,LukeMiller,
Maddie Simens, Amanda Askell, Peter Welinder,
IliaShumailov,ZakharShumaylov,YirenZhao,Yarin
PaulFChristiano,JanLeike,andRyanLowe.2022.
Gal, Nicolas Papernot, and Ross Anderson. 2023.
Training Language Models to Follow Instructions
The curse of recursion: Training on generated
with Human Feedback. In Advances in Neural In-
data makes models forget. In arXiv preprint
formationProcessingSystems(NeurIPS),volume35,
arXiv:2305.17493.
pages27730–27744.CurranAssociates,Inc.
TheodoreR.Sumers,ShunyuYao,KarthikNarasimhan,
JoonSungPark,JosephO’Brien,CarrieJunCai,Mered-
and Thomas L. Griffiths. 2023. Cognitive Archi-
ithRingelMorris,PercyLiang,andMichaelSBern-
tectures for Language Agents. In arXiv preprint
stein. 2023. Generative Agents: Interactive Simu-
arXiv:2309.02427.
lacraofHumanBehavior. InProceedingsofthe36th
AnnualACMSymposiumonUserInterfaceSoftware
andTechnology(UIST),pages1–22. HugoTouvron,ThibautLavril,GautierIzacard,Xavier
Martinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,Faisal
Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize
Azhar, et al. 2023. Llama: Open and Efficient
Chen,YushengSu,YufanDang,JiahaoLi,Juyuan
Foundation Language Models. In arXiv preprint
Xu,DahaiLi,ZhiyuanLiu,andMaosongSun.2023a.
arXiv:2302.13971.
CommunicativeAgentsforSoftwareDevelopment.
InarXivpreprintarXiv:2307.07924.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Chen Qian, Yufan Dang, Jiahao Li, Wei Liu, Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Weize Chen, Cheng Yang, Zhiyuan Liu, and Kaiser,andIlliaPolosukhin.2017. AttentionisAll
Maosong Sun. 2023b. Experiential co-learning YouNeed. InAdvancesinNeuralInformationPro-
of software-developing agents. In arXiv preprint cessingSystems(NeurIPS),volume30.
arXiv:2312.17025.
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-
YujiaQin,ShihaoLiang,YiningYe,KunlunZhu,Lan dlekar,ChaoweiXiao,YukeZhu,LinxiFan,andAn-
Yan,YaxiLu,YankaiLin,XinCong,XiangruTang, imaAnandkumar.2023a. Voyager: AnOpen-Ended
Bill Qian, et al. 2024. Toolllm: Facilitating Large EmbodiedAgentwithLargeLanguageModels. In
Language Models to Master 16000+ Real-World Intrinsically-Motivated and Open-Ended Learning
APIs. In The Twelfth International Conference on Workshop@NeurIPS2023.
LearningRepresentations(ICLR).
LeiWang,ChengbangMa,XueyangFeng,ZeyuZhang,
Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Hao ran Yang, Jingsen Zhang, Zhi-Yang Chen, Ji-
JunruWu,JiamingShen,TianqiLiu,JialuLiu,Don- akaiTang,XuChen,YankaiLin,WayneXinZhao,
aldMetzler,XuanhuiWang,andMichaelBendersky. Zhewei Wei, and Ji rong Wen. 2023b. A sur-
2023. Large Language Models are Effective Text vey on large language model based autonomous
RankerswithPairwiseRankingPrompting. InarXiv agents. InarXivpreprintarXiv:2308.11432,volume
preprintarXiv:2306.17563. abs/2308.11432.Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, The Twelfth International Conference on Learning
JiakaiTang,ZeyuZhang,XuChen,YankaiLin,Rui- Representations(ICLR).
huaSong,WayneXinZhao,JunXu,ZhichengDou,
Jun Wang, and Ji-Rong Wen. 2023c. When Large RuiYang,LinSong,YanweiLi,SijieZhao,YixiaoGe,
LanguageModelbasedAgentMeetsUserBehavior Xiu Li, and Ying Shan. 2023. GPT4Tools: Teach-
Analysis: A Novel User Simulation Paradigm. In ing Large Language Model to Use Tools via Self-
arXivpreprintarXiv:2306.02552. instruction. InThirty-seventhConferenceonNeural
InformationProcessingSystems(NeurIPS).
ShenzhiWang, ChangLiu, ZilongZheng, SiyuanQi,
Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei AnZhang,LehengSheng,YuxinChen,HaoLi,Yang
Wang,ShijiSong,andGaoHuang.2023d. Avalon’s Deng,XiangWang,andTat-SengChua.2023. On
Game of Thoughts: Battle Against Deception Generative Agents in Recommendation. In arXiv
throughRecursiveContemplation. InarXivpreprint preprintarXiv:2310.10108.
arXiv:2310.01320.
Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu
XinyuanWang,ChenxiLi,ZhenWang,FanBai,Hao- Lin, Yong-Jin Liu, and Gao Huang. 2023. ExpeL:
tianLuo,JiayouZhang,NebojsaJojic,EricP.Xing, LLM Agents Are Experiential Learners. In arXiv
andZhitingHu.2024. PromptAgent: StrategicPlan- preprintarXiv:2308.10144.
ning with Language Models Enables Expert-level
WanjunZhong,LianghongGuo,QiqiGao,HeYe,and
PromptOptimization. InTheTwelfthInternational
YanlinWang.2023. Memorybank: Enhancinglarge
ConferenceonLearningRepresentations(ICLR).
languagemodelswithlong-termmemory. InarXiv
YizhongWang,YeganehKordi,SwaroopMishra,Alisa preprintarXiv:2305.10250.
Liu,NoahA.Smith,DanielKhashabi,andHannaneh
Hajishirzi.2023e. Self-Instruct: AligningLanguage Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou,
Models with Self-Generated Instructions. In Pro- RobertLo,AbishekSridhar,XianyiCheng,Yonatan
ceedingsofthe61stAnnualMeetingoftheAssoci- Bisk, Daniel Fried, Uri Alon, et al. 2023a. We-
ation for Computational Linguistics (ACL), pages barena: A realistic Web Environment for Build-
13484–13508. ing Autonomous Agents. In arXiv preprint
arXiv:2307.13854.
Zhilin Wang, Yu Ying Chiu, and Yu Cheung Chiu.
2023f. HumanoidAgents: PlatformforSimulating Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li,
Human-likeGenerativeAgents. InProceedingsof JialongWu,TiannanWang,ShiQiu,JintianZhang,
the2023ConferenceonEmpiricalMethodsinNat- JingChen,RuipuWu,ShuaiWang,ShidingZhu,Jiyu
uralLanguageProcessing: SystemDemonstrations Chen,WentaoZhang,NingyuZhang,HuajunChen,
(EMNLP),pages167–176. PengCui,andMrinmayaSachan.2023b. Agents:An
Open-sourceFrameworkforAutonomousLanguage
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Agents. InarXivpreprintarXiv:2309.07870.
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
MaartenBosma,DennyZhou,DonaldMetzler,EdH. Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao,
Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy WeijieSu,ChenyuYang,GaoHuang,BinLi,Lewei
Liang,JeffDean,andWilliamFedus.2022a. Emer- Lu,XiaogangWang,YuQiao,ZhaoxiangZhang,and
gentAbilitiesofLargeLanguageModels. InTrans- JifengDai.2023. GhostintheMinecraft: Generally
actionsonMachineLearningResearch. CapableAgentsforOpen-WorldEnvironmentsvia
LargeLanguageModelswithText-basedKnowledge
JasonWei,XuezhiWang,DaleSchuurmans,Maarten andMemory. InarXivpreprintarXiv:2305.17144.
Bosma,brianichter,FeiXia,EdChi,QuocVLe,and
Denny Zhou. 2022b. Chain-of-thought Prompting
Elicits Reasoning in Large Language Models. In
AdvancesinNeuralInformationProcessingSystems
(NeurIPS),volume35,pages24824–24837.
LilianWeng.2023. LLM-poweredAutonomousAgents.
Inlilianweng.github.io.
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,
Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang,
ShaokunZhang,JialeLiu,AhmedHassanAwadallah,
RyenWWhite,DougBurger,andChiWang.2023.
AutoGen: Enabling Next-Gen LLM Applications
viaMulti-AgentConversationFramework. InarXiv
preprintarXiv:2308.08155.
Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao
Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen.
2024. Large Language Models as Optimizers. In