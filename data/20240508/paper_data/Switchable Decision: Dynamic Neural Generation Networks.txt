Switchable Decision: Dynamic Neural Generation Networks
ShujianZhang1 KorawatTanwisuth1 ChengyueGong1 PengchengHe MingyuanZhou1
Abstract Fanetal.,2019;Gordonetal.,2020),quantizingthenumber
ofbitsneeded(Linetal.,2016;Shenetal.,2020),distilling
Auto-regressivegenerationmodelsachievecom-
fromlargeteachermodelstosmallstudentmodels(Hinton
petitiveperformanceacrossmanydifferentNLP
etal.,2015;Jiaoetal.,2019). Thesemethodsproduceonly
tasks such as summarization, question answer-
onesmallmodelwithapredeterminedtargetsize. Another
ing, andclassifications. However, theyarealso
direction is to switch the model parameters for different
knownforbeingslowininference,whichmakes
datainstances,e.g.,themixtureofexperts(Shazeeretal.,
themchallengingtodeployinreal-timeapplica-
2017),andswitchtransformer(Fedusetal.,2021). Early
tions. We propose a switchable decision to ac-
exiting,whichadaptivelyproducesaseriesofsmallmodels
celerateinferencebydynamicallyassigningcom-
fordifferentdatainstances,isoneofthemostcommonprac-
putation resources for each data instance. Au-
tices. Mostpreviousworkmakesexitdecisionsbasedon
tomatically making decisions on where to skip
eithertheconfidenceofoutputprobabilitydistributionsora
andhowtobalancequalityandcomputationcost
trainedagent. Inthiswork,weproposeacarefullydesigned
withconstrainedoptimization,ourdynamicneu-
candidatespaceforencoder-decoderauto-regressivemodels
ralgenerationnetworksenforcetheefficientinfer-
andenhancetheoptimizationstrategieswhentrainingthe
encepathanddeterminetheoptimizedtrade-off.
agent.
Experimentsacrossquestionanswering,summa-
rization,andclassificationbenchmarksshowthat Inthisspirit,weexploretheproblemofdynamicallyallo-
ourmethodbenefitsfromlesscomputationcost cating computation across a generation model. In partic-
during inference while keeping the same accu- ular,weconsiderastandardencoder-decodertransformer
racy. Extensiveexperimentsandablationstudies auto-regressivegenerationmodel. Itcomprisesastacked
demonstratethatourmethodcanbegeneral,ef- structure with multiple layers, each having a multi-head
fective,andbeneficialformanyNLPtasks. attentionlayerfollowedbyafeed-forwardnetwork(FFN)
layer (Zhang et al., 2021b;a; Dai et al., 2022; Tanwisuth
etal.,2023). Tothisend,weintroduceadynamicneural
1.Introduction networkfortheauto-regressivegenerationmodels,which
includestheattention,feed-forward,andinputsequenceas
Large-scale pre-trained language models such as BART
thecandidatespaceforswitchabledecisions. Ourmethod
(Lewis et al., 2019) have demonstrated a significant per-
generates an input-dependent inference strategy for each
formance gain to the natural language processing (NLP)
data. Foreachinputsequence,thereinforcementlearning
community but generally come with the cost of a heavy
agentoutputsallthedecisionsforskippingorkeepingeach
computationalburden. Besidespre-trainingandfine-tuning,
candidate. Withthefirst-layerhiddenrepresentationsasthe
inference of such a large model also comes with a heavy
input,thepolicynetworkistrainedtomaximizeareward
computational cost. On IoT (Internet of things) devices
thatincentivestheuseofasfewblocksortokensaspossible
andreal-worldapplications,lowercomputationcosttoler-
whilepreservingthepredictionaccuracy.
anceandrestrictedcomputationresourceduringinference
impedethesemodelsfromdeployment. Weproposelearningoptimalswitchablestrategiesthatsi-
multaneously preserve prediction accuracy and minimal
Recenteffortsofefficientinferencemainlyfocusonpruning
computationusagebasedoninput-specificdecisions. The
orcompressingthemodelparameters,e.g.,pruningunim-
constrained optimization is utilized as a more principled
portantpartsoftheneuralmodelweights(Hanetal.,2015b;
approachfortradingoffthesetwotargets(qualityv.s. ef-
1TheUniversityofTexasatAustin.Correspondenceto:Shujian ficiency). We target keeping the predicted quality while
Zhang<szhang19@utexas.edu>. achieving betterefficiencyasfaraspossible. Agradient-
basedconstrainedoptimizationalgorithmisimplemented
Proceedings of the 41st International Conference on Machine
underourframework.
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by
theauthor(s).
1
4202
yaM
7
]LC.sc[
1v31540.5042:viXraSwitchableDecision:DynamicNeuralGenerationNetworks
Werunextensiveexperimentsacrosssummarization,e.g., graph. Dynamicjumping(Yuetal.,2018;Fu&Ma,2018)
XSum(Narayanetal.,2018)andCNN/DM(Hermannetal., strategicallyskipssometokenswithoutreadingthem,and
2015), question answering, e.g., SQuAD 1.1 (Rajpurkar directly jumps to an arbitrary location. Early exiting for
etal.,2016)andSQuAD2.0(Rajpurkaretal.,2018)),and pretrainedmodelshasbeenexploredbypreviousliterature.
GLUE (Wang et al., 2018a) classification tasks. ❶ Our RTJ(Schwartzetal.,2020),DeeBERT(Xinetal.,2020),
method not only shows comparable performance across andFastBERT(Liuetal.,2020a)makeearlyexitingdeci-
differenttasksanddatasetsbutalsoacceleratesmodelinfer- sionsbasedonconfidence(oritsvariants)ofthepredicted
encebyupto40%withnegligiblemodelqualitydegrada- probabilitydistributionandarethereforelimitedtoclassifi-
tion. ❷Furthermore,weprovideextensiveablationstudies cationtasks. PABEE(Zhouetal.,2020)andBERxiT(Xin
ondifferentdesignchoicesfortheproposedmethod,includ- etal.,2021)proposepatience-basedearlyexitingbyexploit-
ingtheencoder-onlyordecoder-onlyswitchableschemes. ing the layer information. Runtime Neural Pruning (Lin
❸Ouranalysisshowstheswitchabledecisioncontributes etal.,2017),SkipNet(Wangetal.,2018b),andBlockDrop
theefficiencyimprovementandaccuracyconsistency,help- (Wuetal.,2018)usereinforcementlearning(RL)todecide
ingthegenerationmodeltochoosetheinferencepathand whether to execute a network module. Inspired by them,
candidatesdynamically. ❹Tothebestofourknowledge, weincorporatelightweightreinforcementlearningtomake
we present the first switchable decision in the language input-dependent decisions and build a diversified switch-
generationmodelsettingbydynamicallymakingtheinfer- able candidate space. With the constrained optimization
encedecisionsinsummarization,questionanswering,and approach, our method saves computational costs without
classification. Ourcontributionsaresummarizedasfollows: lossofaccuracy.
• Present a dynamic network for switchable decisions 3.Method
embracingattention,feed-forward,andinputsequence
asskippingcandidates. Our switchable decision (Figure 1) network focuses on
• Proposeanefficientandeffectivewaytotraintheskip- speeding up the inference time for an autoregressive lan-
ping strategies, which can optimize the trade-off be- guagegenerationmodel. Specifically,wesuggestageneral
tweencomputationandquality. recipefortheswitchabledecision: 1)constructtheversatile
• Verifytheeffectivenessandgeneralapplicabilityofthe decisionspace,2)utilizetheinput-dependentreinforcement
proposedmethodinvariousNLPtasks,e.g.,summa- learning agent, and 3) propose the lexicographic (lexico)
rization,questionanswering,andclassificationbench- optimizationstrategy.
marks,andprovidearichanalysisofourmethodwith
Denoteinputo=(o ,··· ,o ). Withaseriesofntokens,a
0 n
variousdesignchoices.
transformer-basedlanguagegenerationmodel,MwithL
layers,firstembedsthetokenstoformamatrixO ∈Rn×e,
e
2.RelatedWorkandBackground whereeisthedimensionoftheembeddingspace. These
token representations then go through the encoders and
CompactNetworkDesignandModelCompression For
decoders of the language model. To speed up the infer-
modelcompression,pruningremovesunimportantpartsof
encetimewhilemaintainingsimilarhighquality,wedecide
theneuralnetwork(Hanetal.,2015a;Fanetal.,2019;Gor-
whethereachinputdatashouldskiponelayer. Thisdecision
don et al., 2020), quantization targets the number of bits
problem grows exponentially as we increase the number
neededtooperateaneuralnetwork(Shenetal.,2020),and
oflayers. Moreover,becauseofthediscretenatureofthe
distillation transfers knowledge from large teacher mod-
decisionspace,optimizationbecomeschallenging. Inthis
elstosmallstudentmodels(Chenetal.,2017;Jiaoetal.,
section,weoutlineouruniquedesignchoicestoaccomplish
2019). EfficientnetworkarchitecturessuchasMobileBERT
ourgoalandovercomeoptimizationchallenges.
(Sunetal.,2020)andALBERT(Lanetal.,2020)havealso
beenexploredforlightweightneuralnetworkarchitectures.
3.1.ConstructDiscreteDecisionSpace
Comparedtothesepreviousapproaches, wefocusondy-
namic networks with the switchable candidate design to Weproposelearningthebestconfigurationsof(input, in-
bestreducetotalcomputationwithoutdegradingprediction ference paths) pair for each example using a switchable
accuracy. decisionnetworktospeedupinferencetime. Weconsider
threesearchspacecandidates,namely,theattentionlayer,
DynamicNetworks Dynamicnetworksenableadaptive thefeed-forwardlayer,andqueryinputsafterthefirstlayer.
computationforvariousinputinstancesthathavebeencon- Wenowexplainthedetailsofeachsearchspacebelow.
ductedfornaturallanguagetasks. Textskimming(Campos
et al., 2017; Hansen et al., 2019) learns to skip state up- AttentionCandidate. Akeycomponentofatransformer-
datesandshortenstheeffectivesizeofthecomputational based language model is the attention layer. Zhang et al.
2SwitchableDecision:DynamicNeuralGenerationNetworks
Figure1.Overviewofthedynamicnetwork.Somenotationsarelabeledalongwithcorrespondingcomponents.‘Layers’referstolayers
withintheauto-regressivegenerationmodel.‘ATT’referstotheattentioncandidate,‘FFN’referstothefeed-forwardcandidate,‘Text
Input’referstothetokencandidate,and‘Decisions’referstotheskippingdecisionsfromthereinforcementlearningagent.Thegreen
colorrepresentsnotskipping.Theno-fillinthetextinputandthedashedlinewiththeno-fillcolorboxrepresentstheskipping.
(2019)discoverthatsomelayersareredundant. Todecide sameasg(·),andtheoutputofh(·)isadistributionoverall
whethertoskipacertainlayer,wemodelthesedecisionsasa sixcandidatedecisions.
sequenceofi.i.d. Bernoullirandomvariablesparameterized
byapolicynetworkq.Letb ldenotetheswitchabledecision EncoderandDecoderStructure. Ourinterestedarchitec-
ofthelthlayer,definedas
turecontainsencodersanddecoders. Fortheencoders,we
applyattentionskippingandfeed-forwardskippingtogether
(cid:40)
b = 1 withprobability g(x) l , (1) withtokenskipping. Forthedecoders, sinceeverytoken
l 0 withprobability 1−g(x) ismeaningfulforthefinaloutputs,weonlyapplyattention
l
skipping and feed-forward skipping. When making deci-
wherex ∈ Re denotestheinputofthedecisionunit,and sions,wesamplefromtheoutputsofourpolicynetwork,
we apply the first encoder layer output as x. The policy andbroadcastthedecisionstothehiddenrepresentationsof
network, g, learns instance-specific probabilities of keep- eachlayer.
ing the hidden representations of each layer. To perform
skipping, wesamplefromthisdistributionandbroadcast 3.2.ReinforcementLearningAgent
theindicators,batt,totheinputrepresentationsofattention
l Policy Network Architecture. Since we aim to speed
layers.
up the inference process, a simple design for the policy
networkisadopted. Weutilizeaone-layerMLPwithlayer
Feed-ForwardCandidate. Inthesamespirit,thefeed-
normalizationandReLUactivationfunction. Tooutputa
forwardlayersmaycontainredundantinformation. Thus,
Binomialdistributionoverdecisions,weapplythesigmoid
weconsiderskippingtheselayersusingthesameapproach
activation to the outputs of the network for attention and
asthatdoneintheattention. Wedecidewhethertoskipor
feed-forwardcandidates. Weusethesoftmaxfunctionto
not based on the indicator bffn. The design of the policy
l outputthedistributionoverthechoicesfortokencandidates.
networkisthesameasthatoftheattentionlayer.
Parameterization. Duringthetrainingprocess,wesam-
TokenCandidate. Inadditiontoskippingthelayers,skip- plefromthedecisiondistributions,whichareparameterized
pingthetokenscanalsobeanalternativewaytosavecom- bythepolicynetwork. Thedistributionoftheswitchablede-
putationcosts. Wecreatetwotokenskippingstrategies: ➀ cisionsforthelayerscanberepresentedasa2L-dimensional
skippingthelastp%tokensand➁uniformlyskippingp%
Bernoullidistribution,whichcanbewrittenas:
tokens. For the former, we set p to 10, 20, and 30. For
the latter, p is equal to 25, 33, and 50. To decide which 2L
(cid:89)
strategytouse,weoptimizeacategoricalrandomvariable π(s|x)= g l(x)sl(1−g l(x))1−sl, (2)
parameterizedbyafunctionh(·). Theinputofh(·)isthe l=1
3SwitchableDecision:DynamicNeuralGenerationNetworks
wheres = {batt}L (cid:83) {bffn}L . Similarly,thedistribu- Algorithm1SwitchableDecision(SD)
l l=1 l l=1
tionofthetokenskippingdecisionscanberepresentedasa 1: Input: Texto. Auto-regressivegenerationmodelM
categoricaldistribution,whichcanbeformalizedas: parameterwwithlearningrateα ,policynetworkpa-
t
rameterθwithlearningrateγ ,numberofiterationsT.
t
J
(cid:89)
η(a|x)= h (x)1(a=j), (3)
j 2: for t=0toT do
j=1 3: w ←w−α t∇(w),
where a denotes the choice of the skipping strategy, and 4: θisupdatedviaEqn(7),
J indicatesthetotalnumberofstrategies. Weapplyseven 5: endfor
candidatesinpractice.
OurEquation. Tooptimizethetrade-offbetweenquality
Reward. We define the reward function (Yang et al., andcomputationinEqn(4),weproposetouselexicographic
2022b;a; Feng et al., 2023) as a trade-off between qual- optimization,inwhichtheparametersareiterativelyupdated
ityandcomputationalcost. Givenaninferencepathanda as
datainstance,therewardcanbecomputedfromthecompu-
tation(estimatedFLOPs). Intuitivelyskippinglayerswill θ t+1 ←θ t−γ te t, (6)
havehighreward. Wefurtherreferqualityasaccuracyand where γ ≥ 0 is an adaptive step size and e ∈ Rd is an
t t
lossinthefollowingway:
updatedirectiontobechosentobalancetheminimization
off andconstraintsatisfactiononq. Oneoftheobjectives
R(s,a)=quality+λcomputation, (4)
(sayf whichiscomputationinourcase)isofsecondary
importancew.r.t.theotherone(sayqwhichisquality).The
where quality is −loss, computation is the estimated
design criterion for the constrained optimization is when
FLOPs (floating point operations), and λ is a coefficient.
the constraint is not satisfied (i.e., q(θ ) ≥ c), the focus
Theoveralllossfunctionisdefinedastheexpectedvalueof t
becomes decreasing q to satisfy the constraint as soon as
thereward:
possible; inthemeantime,f performsasasecondaryob-
jectiveindicatingthatf shouldbeminimizedtothedegree
thatitdoesnothurtthedescentofq. Therefore,weapply
J =E [R(s,a)], (5)
s∼π,t∼η thefollowingupdateruletoobtainsuchagoal:
whereπandηaredefinedin(2)and(3),respectively. θ
t+1
←θ t−γ t(∇quality
+λ∇computation(θ )), (7)
t
Optimization. Tooptimizeourpolicynetwork,weapply
where ∇computation and ∇quality are estimated
policygradienttocomputethegradientofJ,andupdatethe
by score function, and the λ can be computed as
parametersofthepolicynetwork.Weuseaself-criticalbase- (cid:16) (cid:17)
linetoreducethevarianceofthegradients. Theconstraint- λ = max
ϕ(θt)−∇quality(θt)⊤∇computation(θt),
0 ,
∥∇computation(θt)∥2
optimizationstrategyisfurtherappliedonthequalityand where ϕ(θ ) equals to q(θ )−c and the c represents the
t t
computation. Detailsareinthenextsection. minimalloss.
DuringInference. Unlikethetrainingprocess,wedonot TheProposedAlgorithm. Ourswitchabledecision(SD)
sample the skipping decisions during inference. Instead, withefficientcandidatespaceandconstrainedoptimization
we choose the decisions which maximize the likelihood is shown in Algorithm 1. We iteratively update the auto-
function. regressive model and the policy network in a single-loop
manner. The policy network parameter θ is updated by
3.3.ConstrainedOptimization Eqn(6)inadirectiontobalancetheoptimizationofquality
andconstraintsatisfactiononcomputation.
Trade-offisaProblem. Inthejointtrainingofthemain
networkandthepolicynetwork,atrade-offbetweenqual-
4.ExperimentalSettings
ityandcomputationisimportant. Thelinearcombination
of multiple objectives is the most widely used approach.
Table1showstheexperimentaldataconfiguration.
However,thecoefficientofthecombinationrequiresman-
ualtuning,anditistheoreticallyunsuitablefornon-convex
4.1.TaskandEvaluationMetrics
functions. Inthiswork,weconsiderconstrainedoptimiza-
tionontradingofftwoobjectives,withaspecialemphasis Summarization. WeuseCNN/DailyMail(Hermannetal.,
onlexicographic(lexico)optimization. 2015) and XSum (Narayan et al., 2018) to evaluate our
4SwitchableDecision:DynamicNeuralGenerationNetworks
method. CNN/DailyMail consists of 287,226 documents sourceinputandauto-regressivetargetoutput,whichcon-
fortraining,13,368documentsforvalidation,and11,490 tains 12 layers of transformer encoder and 12 layers of
documents for testing. XSum has 226,711 news articles transformerdecoder. Itsembeddingsizeis1,024andfeed-
accompaniedwithaone-sentencesummary,answeringthe forwardsizeis4,096. Wefollowthehyper-parametersused
question“Whatisthisarticleabout?”. Followingthesplits inLewisetal.(2019). Specifically,insummarization,we
ofNarayanetal.(2018),itcontains204,045train,11,332 set the training steps as 50k and the number of warm-up
dev, and 11,334 test. Following prior work (Lewis et al., stepsas500. Themaxnumberoftokensandtheupdatefre-
2019),weuseROUGE(Lin&Hovy,2003)asourprimary quencyaresettobe2,048and4,respectively. Thelearning
metric. WereporttheunigramROUGE1(R-1)andbigram rateissetto3×10−5. Forthequestionanswering(SQuAD
ROUGE-2(R-2)overlaptoassesstheinformativeness,and 1.1/2.0). Wesetthetotalnumberofupdatesandwarm-up
thelongestcommonsubsequenceROUGE-L(R-L)scoreto updatesas5,430and326,respectively. Themaxnumber
assessthefluency. ofsentencesis3perdevicewithanupdatefrequencyof2.
Thelearningrateis1.5×10−5. WereferthereaderstoAp-
QuestionAnswering. TheStanfordQuestionAnswering pendixAforclassificationhyper-parameterconfigurations,
Datasets(SQuAD)v1.1andv2.0(Rajpurkaretal.,2016; andmoredetailsaboutthesettings.
2018;Fanetal.,2020)arepopularmachinereadingcom-
prehension benchmarks. For the SQuAD v2.0 dataset, it 5.Experiments
containsexampleswheretheanswertothequestioncannot
bederivedfromtheprovidedcontext. Similartoprevious We evaluate the performance of our switchable dynamic
settings (Devlin et al., 2018; Lewis et al., 2019), we use network. Ineachtable,weboldthebestresultwithineach
concatenatedquestionandcontextasinputtotheencoderof column block and the results of our method are obtained
BART,andadditionallypassthemtothedecoder. Wereport withthreetrialstodeterminethevariance. SeeAppendixA
ExactMatch(EM)andF1scoreforevaluation(Lewisetal., forfullresultswitherrorbars.
2019).
5.1.Summarization
Classification. The General Language Understanding
Table2reportsourresultsontwosummarizationdatasets.
Evaluation (GLUE) benchmark is a collection of natural
➀ The top block displays the performance of baselines
languageunderstanding(NLU)tasks. AsshowninTable
on CNN/DailyMail and XSum datasets, and the bottom
1, we include Multi-Genre NLI (MNLI; (Williams et al.,
blockshowstheresultsofincorporatingtheswitchabledy-
2017b;Zhangetal.,2021d)),RecognizingTextualEntail-
namic networks. We report the results upon the BART
ment(RTE;(Daganetal.,2005)),andStanfordSentiment
large setting in Lewis et al. (2019). ➁ Summaries in the
Treebank(SST;(Socheretal.,2013)). Thediversityofthe
CNN/DailyMailtendtoresemblesourcesentencesandsum-
tasksmakesGLUEverysuitableforevaluatingthegeneral-
maries in XSUM are highly abstractive. Baseline mod-
izationandrobustnessofourproposedmethod(Liuetal.,
els such as BART (Lewis et al., 2019), UniLM (Dong
2020b). Accuracyisadoptedasourevaluationmetric.
et al., 2019), and BERTSUM (Liu & Lapata, 2019) do
wellenough,andeventhebaselineofthefirst-threesource
Task Dataset Train Val Test
sentencesishighlycompetitiveforCNN/DailyMail. Our
Summarization CNN/DailyMail 287.2K 13.4K 11.5k methodcanreducethecomputationcostwhilehavinglittle
XSum 204K 11.3K 11.3K
ornodroponROUGE.Forexample,weevenhavea0.2
SQuAD1.1 87.6K 10.5K 9.5k
QuestionAnswering increaseonR1forCNN/DailyMailanda0.1increaseon
SQuAD2.0 130.3K 11.9K 8.9K
RTE 2.5K 276 3k R1forXSum,whilereducing39%and18%computation
Classification MNLI 393K 20K 20K costs, respectively. For the quality of the sentence gener-
SST 67K 872 1.8K
ations, our method has almost outperformed all the base-
lines. Especially,fortheCNN/DailyMail,weachievebetter
Table1.DatasetConfiguration. Thetopblockisforsummariza- ROUGEwithlessthantwo-thirdsFLOPscost,comparedto
tion,themiddleblockisforquestionanswering,andthebottom theoriginalBART-largemodel(e.g.,R1: 44.16 → 44.31,
blockistheclassificationtasks. RL:40.90→41.01onCNN/DailyMail). ➂Theseresults
further confirm that SD can work as an effective module
4.2.ImplementationDetails tobeincorporatedintotheauto-regressivegenerationmod-
els. SDonimprovingtheinferencecanalsobeseenasa
FollowingLewisetal.(2019),wetakethepre-trainedBART
complementary module to works focusing on improving
modelasthebackboneandutilizetheprovidedcheckpoint
pre-trainingcomponents(Houetal.,2022;Geetal.,2022).
forfinetuningonthedownstreamdatasets. BARTisapre-
trainedsequence-to-sequencemodelbasedonthemasked
5SwitchableDecision:DynamicNeuralGenerationNetworks
CNN/DailyMail XSum thebaseline,fine-tunesBERTwithanensembleofmasks,
Model
Lead-3 4R 01 .4↑ 2 1R 72 .6↑ 2 3R 6L .6↑ 7 FLOPs -(%)↓ 1R 61 .3↑ 0 R 1.2 60↑ 1R 1L .9↑ 5 FLOPs -(%)↓ someofwhichallowonlyleftwardcontext. ③RoBERTa,
UniLM 43.33 20.21 40.51 - - - - - followingLiuetal.(2019a),ispretrainedwithdynamically
BERTSUM 42.13 19.60 39.18 - 38.81 16.50 31.27 -
BART 44.16 21.28 40.90 100 45.14 22.27 37.25 100 changingthemask. ④ForBART(Lewisetal.,2019),itisa
Ourslarge 44.31 21.18 41.01 61.1 45.20 22.16 37.30 81.9
bi-directionalencoder-decoderstructure.
Table2.Comparison to models on CNN/DailyMail and XSum. Table 4 first displays that SD yields a better trade-off be-
ROUGE are reported for each model. ‘BART’ represents the tweenaccuracyandcomputationalefficiency. Oursshows
BARTlargemodel. comparable performance over BART and a clear-margin
gainoverotherbaselines,whilesufficientlylowerFLOPs.
For example, SD achieves 87.2% accuracy v.s. BART’s
Comparison with inference reduction methods. We
87.0%accuracywithonly83.6%FLOPs. Forthevarious
adoptseveralmethodsfromtheconventionalearly-exiting
GLUEbenchmarks,ourdynamicnetworkdemonstratesthe
method(CALM;(Schusteretal.,2022)),FastandRobust
strong capability of making skipping decisions for auto-
EarlyExiting(FREE)(Baeetal.,2023),Pegasus(Shleifer
regressive generation models. It further verifies that our
&Rush,2020)(pruninganddistillation)andDQ-Bart(Li
methodcanworkfordifferentdatasetsandcangeneralize
et al., 2022) (quantization and distillation) and compare
todifferentinputtypesandfields.
themwithOurs(SD).❶InShleifer&Rush(2020),ituti-
lizestheshrinkandfinetunemethods: BART-student,Pega-
sus,andBARTonCNN/DailyMail. ❷InLietal.(2022), Model MNLI RTE SST
m/mm↑ FLOPs(%)↓ Acc↑ FLOPs(%)↓ Acc↑ FLOPs(%)↓
it uses quantization and distillation. It reports the BART BERT 86.6/- - 70.4 - 93.2 -
(8-8-86-1). Thenumberathererepresentsthenumberof UniLM 87.0/85.9 - 70.9 - 94.5 -
RoBERTa 90.2/90.2 - 86.6 - 96.4 -
bits for weights, word embedding, activations, the num- BART 89.9/90.1 100 87.0 100 96.6 100
Ours 89.7/90.0 82.4 87.2 83.6 96.6 80.7
ber of encoder layers, and the number of decoder layers.
TheresultsshowninTable3demonstrateourswitchable Table4.PerformanceonGLUE.Wereporttheaccuracy.Alllan-
decisionachievesagoodtrade-offbetweenqualityandcom- guage models here are large size. ‘m/mm’ and ‘Acc’ denotes
putation. Theseresultsverifythatourmethodcontributes accuracy on matched/mismatched version MNLI and accuracy,
to efficiency and accuracy, helping the generation model respectively.
to choose the inference path and candidates dynamically.
❸Further,combiningquantizationordistillationmethod,
oureffectivemethodofimprovingthelanguagegeneration 5.3.QuestionAnswering
model can also be seen as a complementary and plug-in
For both SQuAD v1.1 and v2.0, following Lewis et al.
module. Weleavethisasafuturework.
(2019),wefeedthecompletedocumentsintotheencoder
and decoder, and use the top hidden state of the decoder
Data ROUGE-L FLOPs(%)
as a representation for each word. This representation is
BART-student 41.01 93.1
usedtoclassifythetoken. Table5showsourexperiment
Pegasus 40.34 93.1
results.TheBARTlargeisusedastheprimarybaseline,and
DQ-Bart 40.05 18.2
therecentbaselines(Devlinetal.,2019;Dongetal.,2019;
CALM 40.54 80.5
Liuetal.,2019b)arereported. Weloadtheofficialcheck-
FREE 40.69 76.8
pointfromFairseqwiththeofficialpre-processedSQuAD
OurSwitchableDecision 41.01 61.1
data. Onquestionanswering,bydynamicallyskippingthe
Table3.ComparisonSDwithdifferentinferencecostreduction candidatesfromattentionlayers,feed-forwardlayers,and
methodsonCNN/DailyMail. inputtokens,ourmodelachievesasimilarEMandF1score
asBART.Differentfromtheabovetasks,heretheinputis
concatenatedquestionandcontextandadditionallypassed
5.2.Classification
tothedecoder. Althoughtheinputisorganizedindifferent
WefurthershowtheexperimentalresultsontheGLUEin formats,itisinterestingtoseetheconsistentcomputation
Table 4. The Multi-Genre NLI (MNLI; (Williams et al., costimprovementofourproposedswitchabledecisionin
2017b)), Recognizing Textual Entailment (RTE; (Dagan questionanswering. ItfurtherdemonstratesthatSDcanbe
et al., 2005)), and Stanford Sentiment Treebank (SST; utilizedingeneralNLPtasks.
(Socheretal.,2013))areincluded. Weadoptseveralbase-
linesfromtheexistingliterature. ①ForBERT,following 6.Analysis
Devlinetal.(2019),itintroducesmaskedlanguagemodel-
ing,whichallowspre-trainingtolearninteractionsbetween Can we use the proposed dynamic network with the
leftandrightcontextwords. ②UniLM(Dongetal.,2019), different auto-regressive generation models? As dis-
6SwitchableDecision:DynamicNeuralGenerationNetworks
SQuAD1.1 SQuAD2.0 percentage of each candidate. These results confirm our
Model
EM/F1↑ FLOPs(%)↓ EM/F1↑ FLOPs(%)↓ analysisandmotivationfortheswitchabledecisionthatus-
BERT 84.1/90.9 - 79.0/81.8 - ingacombinationofallthesearchitecturalsearchspaces
UniLM -/- - 80.5/83.4 -
comestothebestefficiencyandaccuracytrade-off.
RoBERTa 88.9/94.6 - 86.5/89.4 -
BART 88.8/94.6 100 86.1/89.2 100
Ours 88.7/94.5 80.5 86.0/89.3 83.3 Architecture ATT FFN Token FLOPs(%) ROUGE
BART 100 44.16/21.28/40.90
Encoder-Only ✓ ✓ 91.9 44.21/21.32/40.95
Table5.ResultsacrossdifferentstrategiesonSQuADv1.1and
Decoder-Only ✓ ✓ 90.3 44.13/21.08/40.86
v2.0. Answersaretextspansextractedfromagivendocument Token-Only ✓ 71.5 44.09/21.26/40.92
context.2 Ours ✓ ✓ ✓ 61.1 44.31/21.18/41.01
cussedinSection3,ourproposedmethodtargetstheauto- Table7.Results of skipping strategies on different architecture
spacesforCNN/DailyMail.BART(Izacard&Grave,2021)large
regressive generation model. Thus, can our method be
modelispresented.
adapted to other auto-regressive generation models? We
selecttheGPT-2(Radfordetal.,2019)baseandT5(Raffel
etal.,2020)basetostudytheperformanceafteradaptingour Ablation studies on the components in SD. We con-
proposedswitchabledecisions. Theresultsarepresentedin ducttheablationstudytoexaminetheroleofconstrained
Table6. Itindicatesourmethodisinsensitivetodifferent optimization. Forablation,insteadofautomaticallysearch-
generation models. This confirms our discussion in Sec- ingthetrade-offbetweenthequalityandcomputation,we
tion3thatSDcanserveasanefficientalternativedynamic manually set the λ in Eqn (7) as 0.2, 0.5, 0.8. We also
networkforversatilegenerationmodels. Wealsoanalyze includetherandomselectionstrategy. Therandomselec-
theimpactofmakingdecisionsbasedondifferenthidden tionstrategyisnotlearningswitchabledecisionsandwould
representations. MoredetailsaboutLLaMA(Touvronetal., notdynamicallyassigncomputationforeachdatainstance.
2023)modelsareincludedinAppendixA. ❶Table8showsthattheconstrainedoptimizationofour
methodbringsclearbenefits. ❷WefindthatwithoutCO,
Data ROUGE FLOPs(%) ‘−CO’withdifferentmanuallytunedλvalueshowsanun-
BART 44.16/21.28/40.90 100 stable trade-off between the ROUGE and FLOPs across
+Ours 44.31/21.18/41.01 61.1 all λ values, indicating that manually tuned λ value can
GPT-2 37.55/15.53/25.81 100 notbringbothoptimizedqualityandcomputationtogether.
+Ours 37.76/15.68/25.93 74.5 ❸Empirically,werandomlyselectapolicyfromourdeci-
T5 42.05/20.34/39.40 100
sionspacecandidatesandusethesameotherparameters.
+Ours 41.98/20.38/39.61 74.5
These result in a degradation in performance and lower
FLOPsreduction. Itdemonstratesthenecessityandeffec-
Table6.Theproposedmethodfordifferentgenerationmodelson
tivenessoftheconstrainedoptimizationfortheswitchable
CNN/DailyMail.
candidatesetinSDstructure.
Whatarethedifferencesbetweenencoder-only,decoder- Data ROUGE FLOPs(%)
only, and token-only architecture search space? We BART 44.16/21.28/40.90 100
Random 41.77/19.02/38.72 75.3
testifourresultsaresensitivetothechoiceofarchitectures:
Ours 44.31/21.18/41.01 61.1
encoder-only,decoder-only,andencoder-decoder.Wecreate -CO,λ=0.2 44.12/21.30/40.88 77.8
thefollowingscenarios:①Forencoder-only,weincorporate -CO,λ=1.0 42.89/21.02/40.57 68.5
theattentionandfeed-forwardastheskippingcandidates.② -CO,λ=1.5 41.35/19.87/38.39 49.4
Fordecoder-only,similarly,theattentionandfeed-forward
areincluded. ③Fortoken-only,thetokencandidateisuti- Table8.Comparisonofdifferentλvaluesforthemanuallytuned
trade-offbetweencomputationandqualityvs.Ours.‘CO’denotes
lized. ThenwecomparethesethreedesignswithSDand
constrainedoptimization.
BARTlargetoseetheimpactofincorporatingourdesigned
decisionspaceintothesedifferentmodelarchitectures. As
shown in Table 7, we observe distinct FLOPs (reducing Efficiencyandtime. Weprovidetheparametersizes,av-
10%)savingbyonlyaddingourskippingattentionandfeed- erageGPUmemoryperdevice,persteptrainingtime,and
forwardstrategiesforencoder-onlyanddecoder-only. By inferencetimecomparisonsbetweenthebaselineandSD
onlyincludingthetokenskippingfortheencoder-decoder during the finetuning. Experiments in this part are per-
structure,weobservethelargerFLOPs(reducing29%)sav- formedoneightTeslaV100GPUs. ❶Table9showsthat
ingwhiledeliveringthecomparableROUGEtoBART.We SDkeepstheparametersizeatthesamelevelastheBART
referthereaderstoAppendix6.1forthedetailedskipping largeduringfinetuning. TheGPUmemoryperdeviceand
7SwitchableDecision:DynamicNeuralGenerationNetworks
trainingtimeofSDareslightlyhigher(2.7%formemory Theimpactofmakingdecisionsbasedondifferenthid-
and1.6%forrunningtime)thanBART.SDgivesthebest denrepresentations. InSection3.1, weconsiderthree
inferenceFLOPs,outperformingBARTwhilekeepingthe skippingcandidates’hiddenrepresentations(attention,feed-
comparable ROUGE score and running time. ❷ For the forward,andquery)afterthefirstlayerastheinputforour
inference time, we evaluate our method and BART large reinforcementlearningagenttomakeswitchabledecisions.
onCNN/DailyMailfollowingthesamesettinganddevice Here, we demonstrate that using hidden representations
withbatchsize1. Foreachiteration,5.1seconds(Ours)vs. fromdifferentlayerscomestothesameresults,andthere-
10.3seconds(BART).Ourdynamicnetworkdemonstrates forewepicktheeasiestone. Wesetupabaselinehere,in
thestrongcapabilityofmakingskippingdecisions. ❸With whichwhethertoskipthefollowinglayerisdependenton
theconstrainedoptimizationandthereinforcementlearn- thenearbypreviouslayeroutputs. WeexperimentonOurs
ingagent,ourswitchabledecisionisstillcomputationally (basedontheoutputfromthefirstlayer)andOursLayer
productiveasthedesignofouroptimizationandagent(e.g., Wise (layer-wise decisions based on the output from the
applying one-layer MLP for policy network) has almost nearbypreviouslayers). Thedifferencebetweenthesetwo
negligiblefinetuningcomputationalcost. casesissmallinTable11. Thelayer-wisedesignrequires
more computation as it needs to make decisions at each
layer. Therefore,itfurtherdemonstratesthatthedesignof
Model ROUGE↑ Params↓ GPUmemory↓ s/step↓ IT↓
oursiscapableofmakingskippingdecisionsandimposing
BART 44.16/21.28/40.90 406M 16.8G 1.20 10.3
Ours 44.31/21.18/41.01 423M 17.6G 1.48 5.1 lesscomputationalcost.
Table9.Resultsofparametersize,GPUmemoryperdevice,and Data ROUGE FLOPs(%)
step time for BART and ours finetuning on CNN/DailyMail. Ours 44.31/21.18/41.01 61.1
‘s/step’ represents training step time (second/per step).‘IT’ rep- OursLayerWise 44.38/21.22/40.97 61.8
resentsinferencetime(second)foreachiterations.
Table11.Comparisonofdifferentlayer-wisedecisionofSDon
CNN/DailyMail.‘Ours’representsthedecisionbasedonthehid-
6.1.ContributionsofSearchSpaceCandidates. denafterthefirstlayer.‘OursLayerWise’representsthedecision
basedonthehiddenrepresentationfromthenearbypreviouslayer.
To further identify the contributions of our search space
candidates for efficiency improvements and inference ac-
celeration, we present the details skipping percentage of 7.Conclusion
eachcandidateforCNN/DailyMail,SQuAD1.1,andSST
Ourworkdemonstratesthebenefitsofintroducingaswitch-
inTable10. ForCNN/DailyMail,weobservearound8%
abledecisionofthedynamicnetwork.Theproposedmethod
attentionskippingoftotalattention,11%feed-forwardskip-
candramaticallyincreasetheinferenceefficiencyandstill
pingoftotalfeed-forward,and29%tokenskippingoftotal
enablethemodelperformance. NoticeableFLOPssaving
tokens. Thesimilarskippingpercentageholdsforquestion
andconsistentperformanceareobservedacrosssummariza-
answering. However,wehaveseenanobviouscontrastin
tion,questionanswering,andclassificationbenchmarks.We
thetokenskippingpercentageinclassificationtasks. The
furtherconductadetailedstudywiththeproposedswitch-
keyobservationisthattheskippingpercentagesfortokens
ablestrategyindifferentsettings,e.g.,comparingwithdif-
arehighforbothCNN/DailyMailandSQuAD1.1. Inad-
ferentarchitecturesearchspaces,providingmoreevidence
dition, our method generally takes around 5K iterations
formakingdecisionsbasedonhiddenrepresentations,and
for the reinforcement learning algorithm to converge on
verifying the impact of components. To summarize, the
CNN/DailyMail. ThisconfirmsourconjectureinSection
proposedSDiseffectiveandgeneral,withthepotentialto
5.1. Forsummarizationandquestionansweringtasks,the
beincorporatedintoexistinggenerationmodelsforvarious
first few parts of inputs are more representative. Thus, it
NLPtasks.
perfectlyservesasthecandidateforourswitchablenetwork
tomaketheskippingdecisions.
Dataset ATT FFN Token
CNN/DailyMail 8.50% 11.13% 28.75%
SST 13.30% 13.54% 7.18%
SQuAD1.1 10.21% 11.93% 9.02%
Table10.Skipping percentage of each candidate. For example,
8.50%indicatesthatthereare8.50%oftotalattentionskipped.
8SwitchableDecision:DynamicNeuralGenerationNetworks
References Fu,T.-J.andMa,W.-Y. Speedreading: Learningtoread
forbackward via shuttle. In Proceedings of the 2018
Bae, S., Ko, J., Song, H., andYun, S.-Y. Fastandrobust
ConferenceonEmpiricalMethodsinNaturalLanguage
early-exitingframeworkforautoregressivelanguagemod-
Processing,pp.4439–4448,2018.
elswithsynchronizedparalleldecoding. arXivpreprint
arXiv:2310.05424,2023.
Ge, T., Xia, H., Sun, X., Chen, S.-Q., and Wei, F. Loss-
lessaccelerationforseq2seqgenerationwithaggressive
Campos,V.,Jou,B.,Giro´-iNieto,X.,Torres,J.,andChang,
decoding. arXivpreprintarXiv:2205.10350,2022.
S.-F. Skiprnn: Learningtoskipstateupdatesinrecurrent
neuralnetworks. arXivpreprintarXiv:1708.06834,2017.
Gordon, M.A., Duh, K., andAndrews, N. Compressing
bert: Studyingtheeffectsofweightpruningontransfer
Chen, G., Choi, W., Yu, X., Han, T., and Chandraker, M.
learning. arXivpreprintarXiv:2002.08307,2020.
Learningefficientobjectdetectionmodelswithknowl-
edgedistillation.Advancesinneuralinformationprocess- Han, S., Mao, H., and Dally, W. J. Deep compres-
ingsystems,30,2017. sion: Compressingdeepneuralnetworkswithpruning,
trainedquantizationandhuffmancoding. arXivpreprint
Dagan,I.,Glickman,O.,andMagnini,B. Thepascalrecog-
arXiv:1510.00149,2015a.
nisingtextualentailmentchallenge. InMachineLearning
ChallengesWorkshop,pp.177–190.Springer,2005. Han, S., Pool, J., Tran, J., and Dally, W. Learning both
weights and connections for efficient neural network.
Dai,Y.,Tang,D.,Liu,L.,Tan,M.,Zhou,C.,Wang,J.,Feng,
Advancesinneuralinformationprocessingsystems,28,
Z.,Zhang,F.,Hu,X.,andShi,S. Onemodel,multiple
2015b.
modalities: Asparselyactivatedapproachfortext,sound,
image,videoandcode. arXivpreprintarXiv:2205.06126, Hansen,C.,Hansen,C.,Alstrup,S.,Simonsen,J.G.,and
2022. Lioma, C. Neural speed reading with structural-jump-
lstm. arXivpreprintarXiv:1904.00761,2019.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
Bert: pre-training of deep bidirectional transformers Hermann, K. M., Kocisky, T., Grefenstette, E., Espeholt,
for language understanding. arxiv. arXiv preprint L.,Kay,W.,Suleyman,M.,andBlunsom,P. Teaching
arXiv:1810.04805,2018. machinestoreadandcomprehend. Advancesinneural
informationprocessingsystems,28,2015.
Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K.BERT:
Pre-training of deep bidirectional transformers for lan- Hinton, G., Vinyals, O., Dean, J., et al. Distilling
guageunderstanding. InNAACL-HLT,2019. the knowledge in a neural network. arXiv preprint
arXiv:1503.02531,2(7),2015.
Dong,L.,Yang,N.,Wang,W.,Wei,F.,Liu,X.,Wang,Y.,
Gao, J., Zhou, M., and Hon, H.-W. Unified language Hou,L.,Pang,R.Y.,Zhou,T.,Wu,Y.,Song,X.,Song,X.,
model pre-training for natural language understanding andZhou,D.Tokendroppingforefficientbertpretraining.
andgeneration. AdvancesinNeuralInformationProcess- arXivpreprintarXiv:2203.13240,2022.
ingSystems,32,2019.
Izacard,G.andGrave,E. Distillingknowledgefromreader
Fan, A., Grave, E., and Joulin, A. Reducing transformer toretrieverforquestionanswering. InICLR2021,9th
depthondemandwithstructureddropout. arXivpreprint InternationalConferenceonLearningRepresentations,
arXiv:1909.11556,2019. 2021.
Fan,X.,Zhang,S.,Chen,B.,andZhou,M. Bayesianatten- Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li,
tionmodules. arXivpreprintarXiv:2010.10604,2020. L., Wang, F., and Liu, Q. Tinybert: Distilling bert
for natural language understanding. arXiv preprint
Fedus, W., Zoph, B., and Shazeer, N. Switch transform- arXiv:1909.10351,2019.
ers: Scalingtotrillionparametermodelswithsimpleand
efficientsparsity,2021. Kingma,D.P.andBa,J. Adam: Amethodforstochastic
optimization. arXivpreprintarXiv:1412.6980,2014.
Feng,Y.,Yang,S.,Zhang,S.,Zhang,J.,Xiong,C.,Zhou,
M., and Wang, H. Fantastic rewards and how to tame Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma,
them: Acasestudyonrewardlearningfortask-oriented P., and Soricut, R. Albert: A lite bert for self-
dialogue systems. arXiv preprint arXiv:2302.10342, supervisedlearningoflanguagerepresentations. ArXiv,
2023. abs/1909.11942,2020.
9SwitchableDecision:DynamicNeuralGenerationNetworks
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo- Narayan,S.,Cohen,S.B.,andLapata,M.Don’tgivemethe
hamed,A.,Levy,O.,Stoyanov,V.,andZettlemoyer,L. details,justthesummary! topic-awareconvolutionalneu-
Bart: Denoisingsequence-to-sequence pre-training for ralnetworksforextremesummarization. arXivpreprint
naturallanguagegeneration,translation,andcomprehen- arXiv:1808.08745,2018.
sion. arXivpreprintarXiv:1910.13461,2019.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
Li,Z.,Wang,Z.,Tan,M.,Nallapati,R.,Bhatia,P.,Arnold, Sutskever,I.,etal. Languagemodelsareunsupervised
A.,Xiang,B.,andRoth,D. Dq-bart: Efficientsequence- multitasklearners. OpenAIblog,1(8):9,2019.
to-sequencemodelviajointdistillationandquantization.
arXivpreprintarXiv:2203.11239,2022. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena,M.,Zhou,Y.,Li,W.,andLiu,P.J. Exploringthe
Lin,C.-Y.andHovy,E. Automaticevaluationofsummaries limitsoftransferlearningwithaunifiedtext-to-texttrans-
usingn-gramco-occurrencestatistics. InProceedingsof former. J.Mach.Learn.Res.,21:140:1–140:67,2020.
the2003humanlanguagetechnologyconferenceofthe
NorthAmericanchapteroftheassociationforcomputa- Rajpurkar,P.,Zhang,J.,Lopyrev,K.,andLiang,P. Squad:
tionallinguistics,pp.150–157,2003. 100,000+questionsformachinecomprehensionoftext.
Empirical Methods in Natural Language Processing
Lin,D.,Talathi,S.,andAnnapureddy,S. Fixedpointquan-
(EMNLP),2016.
tizationofdeepconvolutionalnetworks. InInternational
conferenceonmachinelearning,pp.2849–2858.PMLR, Rajpurkar,P.,Jia,R.,andLiang,P. Knowwhatyoudon’t
2016. know: Unanswerablequestionsforsquad. AnnualMeet-
ings of the Association for Computational Linguistics
Lin,J.,Rao,Y.,Lu,J.,andZhou,J. Runtimeneuralpruning.
(ACL),2018.
Advancesinneuralinformationprocessingsystems,30,
2017. Schuster,T.,Fisch,A.,Gupta,J.,Dehghani,M.,Bahri,D.,
Tran, V., Tay, Y., and Metzler, D. Confident adaptive
Liu, W., Zhou, P., Zhao, Z., Wang, Z., Deng, H., and Ju,
language modeling. Advances in Neural Information
Q. Fastbert: aself-distillingbertwithadaptiveinference
ProcessingSystems,35:17456–17472,2022.
time. arXivpreprintarXiv:2004.02178,2020a.
Schwartz,R.,Stanovsky,G.,Swayamdipta,S.,Dodge,J.,
Liu,X.,Wang,Y.,Ji,J.,Cheng,H.,Zhu,X.,Awa,E.,He,P.,
and Smith, N. A. The right tool for the job: Match-
Chen,W.,Poon,H.,Cao,G.,etal. Themicrosofttoolkit
ing model and instance complexities. arXiv preprint
ofmulti-taskdeepneuralnetworksfornaturallanguage
arXiv:2004.07453,2020.
understanding. arXivpreprintarXiv:2002.07972,2020b.
Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le,
Liu,X.,Gong,C.,Wu,L.,Zhang,S.,Su,H.,andLiu,Q.
Q.,Hinton,G.,andDean,J. Outrageouslylargeneural
Fusedream: Training-freetext-to-imagegenerationwith
networks: Thesparsely-gatedmixture-of-expertslayer.
improvedclip+ganspaceoptimization. arXivpreprint
arXivpreprintarXiv:1701.06538,2017.
arXiv:2112.01573,2021.
Shen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A.,
Liu,Y.andLapata,M. Textsummarizationwithpretrained
Mahoney,M.W.,andKeutzer,K. Q-bert: Hessianbased
encoders. arXivpreprintarXiv:1908.08345,2019.
ultralowprecisionquantizationofbert. InProceedings
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., of the AAAI Conference on Artificial Intelligence, vol-
Levy,O.,Lewis,M.,Zettlemoyer,L.,andStoyanov,V. ume34,pp.8815–8821,2020.
Roberta: Arobustlyoptimizedbertpretrainingapproach.
ArXiv,abs/1907.11692,2019a. Shleifer, S. and Rush, A. M. Pre-trained summarization
distillation. arXivpreprintarXiv:2010.13002,2020.
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
Levy,O.,Lewis,M.,Zettlemoyer,L.,andStoyanov,V. Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,
Roberta: Arobustlyoptimizedbertpretrainingapproach. C.D.,Ng,A.Y.,andPotts,C. Recursivedeepmodelsfor
arXive-prints,pp.arXiv–1907,2019b. semanticcompositionalityoverasentimenttreebank. In
Proceedingsofthe2013conferenceonempiricalmethods
Manning, C. D., Surdeanu, M., Bauer, J., Finkel, J. R., innaturallanguageprocessing,pp.1631–1642,2013.
Bethard, S., and McClosky, D. The stanford corenlp
naturallanguageprocessingtoolkit. InProceedingsof Sun,Z.,Yu,H.,Song,X.,Liu,R.,Yang,Y.,andZhou,D.
52ndannualmeetingoftheassociationforcomputational Mobilebert: a compact task-agnostic bert for resource-
linguistics: systemdemonstrations,pp.55–60,2014. limiteddevices. arXivpreprintarXiv:2004.02984,2020.
10SwitchableDecision:DynamicNeuralGenerationNetworks
Tanwisuth, K., Zhang, S., Zheng, H., He, P., and Zhou, Yu, K., Liu, Y., Schwing, A. G., and Peng, J. Fast and
M. Pouf: Prompt-orientedunsupervisedfine-tuningfor accurate text classification: Skimming, rereading and
largepre-trainedmodels. InInternationalConferenceon earlystopping. 2018.
MachineLearning,pp.33816–33832.PMLR,2023.
Zhang,C.,Bengio,S.,andSinger,Y. Arealllayerscreated
Touvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux, equal? 2019.
M.-A.,Lacroix,T.,Rozie`re,B.,Goyal,N.,Hambro,E.,
Azhar,F.,etal. Llama:Openandefficientfoundationlan- Zhang, S., Fan, X., Chen, B., and Zhou, M. Bayesian
guagemodels. arXivpreprintarXiv:2302.13971,2023. attentionbeliefnetworks. InInternationalConferenceon
MachineLearning,pp.12413–12426.PMLR,2021a.
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and
Bowman,S.R. Glue: Amulti-taskbenchmarkandanal- Zhang, S., Fan, X., Zheng, H., Tanwisuth, K., and Zhou,
ysisplatformfornaturallanguageunderstanding. arXiv M. Alignmentattentionbymatchingkeyandquerydis-
preprintarXiv:1804.07461,2018a. tributions. AdvancesinNeuralInformationProcessing
Systems,34:13444–13457,2021b.
Wang, X., Yu, F., Dou, Z.-Y., Darrell, T., and Gonzalez,
J.E. Skipnet: Learningdynamicroutinginconvolutional Zhang, S., Gong, C., and Choi, E. Knowing more about
networks. InProceedingsoftheEuropeanConferenceon questions can help: Improving calibration in question
ComputerVision(ECCV),pp.409–424,2018b. answering. arXivpreprintarXiv:2106.01494,2021c.
Williams, A., Nangia, N., and Bowman, S. R. A broad- Zhang,S.,Gong,C.,andChoi,E. Learningwithdifferent
coverage challenge corpus for sentence understanding amountsofannotation: Fromzerotomanylabels. arXiv
throughinference. InNAACL-HLT,2017a. preprintarXiv:2109.04408,2021d.
Williams, A., Nangia, N., and Bowman, S. R. A broad- Zhang,S.,Gong,C.,andLiu,X.Passage-mask:Alearnable
coverage challenge corpus for sentence understanding regularizationstrategyforretriever-readermodels. arXiv
through inference. arXiv preprint arXiv:1704.05426, preprintarXiv:2211.00915,2022a.
2017b.
Zhang,S.,Gong,C.,Liu,X.,He,P.,Chen,W.,andZhou,
Wu,Z.,Nagarajan,T.,Kumar,A.,Rennie,S.,Davis,L.S., M. Allsh: Activelearningguidedbylocalsensitivityand
Grauman,K.,andFeris,R. Blockdrop: Dynamicinfer- hardness. arXivpreprintarXiv:2205.04980,2022b.
encepathsinresidualnetworks. InProceedingsofthe
IEEEconferenceoncomputervisionandpatternrecogni- Zhang,S.,Wu,L.,Gong,C.,andLiu,X. Languagerecti-
tion,pp.8817–8826,2018. fiedflow: Advancingdiffusionlanguagegenerationwith
probabilistic flows. arXiv preprint arXiv:2403.16995,
Xin, J., Tang, R., Lee, J., Yu, Y., and Lin, J. Deebert:
2024.
Dynamic early exiting for accelerating bert inference.
arXivpreprintarXiv:2004.12993,2020. Zhou, W., Xu, C., Ge, T., McAuley, J., Xu, K., and Wei,
F. Bert loses patience: Fast and robust inference with
Xin,J.,Tang,R.,Yu,Y.,andLin,J.Berxit:Earlyexitingfor
earlyexit. AdvancesinNeuralInformationProcessing
bertwithbetterfine-tuningandextensiontoregression.
Systems,33:18330–18341,2020.
InProceedingsofthe16thconferenceoftheEuropean
chapteroftheassociationforcomputationallinguistics:
MainVolume,pp.91–104,2021.
Yang,S.,Feng,Y.,Zhang,S.,andZhou,M. Regularizinga
model-basedpolicystationarydistributiontostabilizeof-
flinereinforcementlearning. InInternationalConference
onMachineLearning,pp.24980–25006.PMLR,2022a.
Yang, S., Zhang, S., Feng, Y., and Zhou, M. A unified
frameworkforalternatingofflinemodeltrainingandpol-
icylearning. AdvancesinNeuralInformationProcessing
Systems,35:17216–17232,2022b.
Yang, S., Zhang, S., Xia, C., Feng, Y., Xiong, C., and
Zhou,M. Preference-groundedtoken-levelguidancefor
languagemodelfine-tuning. AdvancesinNeuralInfor-
mationProcessingSystems,36,2024.
11SwitchableDecision:DynamicNeuralGenerationNetworks
A.Experimentaldetails a set of Wikipedia articles. The answers, given the ques-
tions, are text span from the given reading passage. The
A.1.FullResultsWithErrorBar
SQuAD1.1containsaround100,000question-answerpairs
Wereportthefullresultsofourmethodwiththeerrorbarfor on about 500 articles. The SQuAD v2.0 dataset includes
summarizationandquestionansweringinTable12and14, unanswerablequestionsaboutthesameparagraphs.
respectively. Thefullresultofclassificationisdemonstrated
inTable13. Classification. GLUE(Wangetal.,2018a;Zhangetal.,
2022b) comprises a collection of text classification tasks
Model R1↑ R2C ↑NN/Daily RM Lai ↑l FLOPs(%)↓ R1↑ R2↑ XSum RL↑ FLOPs(%)↓ meanttotestgenerallanguageunderstandingabilities. We
Lead-3 40.42 17.62 36.67 - 16.30 1.60 11.95 -
UniLM 43.33 20.21 40.51 - - - - - adopt the three datasets for our experiments: natural lan-
BERTSUM 42.13 19.60 39.18 - 38.81 16.50 31.27 -
BART 44.16 21.28 40.90 100 45.14 22.27 37.25 100 guageinference(MNLI(Williamsetal.,2017a)andRTE
Ourslarge 44.31±0.1 21.18±0.2 41.01±0.2 61.1 45.20±0.1 22.16±0.2 37.30±0.2 81.9
(Daganetal.,2005))andsentimentanalysis(SST-2(Socher
Table12.Full results on CNN/DailyMail and XSum. ROUGE
etal.,2013)).
isreportedforeachmodel. ‘BART’representstheBARTlarge
model.
A.3.ExperimentalSettings
Forsummarization,wefollowthesettingin(Lewisetal.,
MNLI RTE SST
Model
m/mm↑ FLOPs(%)↓ Acc↑ FLOPs(%)↓ Acc↑ FLOPs(%)↓ 2019)andinitializeourmodelswiththepretrainedBART
BERT 86.6/- - 70.4 - 93.2 -
UniLM 87.0/85.9 - 70.9 - 94.5 - largecheckpoint. ThecheckpointisfromtheFairseqlibrary
RoBERTa 90.2/90.2 - 86.6 - 96.4 -
BART 89.9/90.1 100 87.0 100 96.6 100 3.T5(Raffeletal.,2020)isalsousedinSection6.Weadopt
Ours 89.7±0.2/90.0±0.3 82.4 87.2±0.1 83.6 96.6±0.2 80.7
the T5 base from the HuggingFace Transformer library4.
FollowingLewisetal.(2019),theAdamoptimizer(Kingma
Table13.FullperformanceonGLUE.Wereporttheaccuracyof
eachdataset.Alllanguagemodelsherearelargesize.‘m/mm’and &Ba,2014;Liuetal.,2021;Zhangetal.,2024)isutilized
‘Acc’denotesaccuracyonmatched/mismatchedversionMNLIand foroptimizingthemodelparameterwiththelearningrate
accuracy,respectively. 3×10−5. Thetrainingstepis50kandthewarmupstepis
500. Bothdropoutandattentiondropoutaresetas0.1. For
classification,thedetailedtrainingsettingsarepresentedin
SQuAD1.1 SQuAD2.0
Model Table15.
EM/F1↑ FLOPs(%)↓ EM/F1↑ FLOPs(%)↓
BERT 84.1/90.9 - 79.0/81.8 -
UniLM -/- - 80.5/83.4 -
RoBERTa 88.9/94.6 - 86.5/89.4 - Model MNLI RTE SST-2
BART 88.8/94.6 100 86.1/89.2 100
NC 3 2 2
Ours 88.7±0.3/94.5±0.4 80.5 86.0±0.3/89.3±0.3 83.3
LR 5×10−6 1×10−5 5×10−6
BSZ 128 32 128
Table14.FullresultsacrossdifferentstrategiesonSQuADv1.1
TS 30,968 1,018 5,233
andv2.0.Answersaretextspansextractedfromagivendocument
context. WS 1,858 61 314
Table15.Experiment setting for MNLI, RTE, and SST-2 (LR:
A.2.ExperimentalDatasets learningrate,BSZ:batchsize,NC:numberofclasses,TS:total
numberoftrainingsteps,WS:warm-upsteps).
Summarization. CNN/DailyMailcontainsnewsarticles
and associated highlights as summaries. Following the
standard splits from Hermann et al. (2015) for training,
Data ROUGE FLOPs(%)
validation,andtesting,wehave90,266/1,220/1,093CNN
BART 44.16/21.28/40.90 100
documents and 196,961/12,148/10,397 DailyMail docu- +Ours 44.31/21.18/41.01 61.1
ments,respectively. ThesentenceissplitbyusingtheStan- GPT-2 37.55/15.53/25.81 100
+Ours 37.76/15.68/25.93 74.5
fordCoreNLPtoolkit(Manningetal.,2014). ForXSum
T5 42.05/20.34/39.40 100
(Narayanetal.,2018),summariesareprofessionallywrit- +Ours 41.98/20.38/39.61 74.5
tenbytheauthorsofthedocuments. Wealsousethepre- LLaMA -/-/46.68 100
+Ours -/-/46.73 77.6
processinganddatasplitsfrom(Narayanetal.,2018;Yang
etal.,2024).
Table16.Theproposedmethodfordifferentgenerationmodelson
CNN/DailyMail.
Question Answering. Stanford Question Answering
3https://github.com/facebookresearch/
Dataset (SQuAD) (Rajpurkar et al., 2016; 2018; Zhang
fairseq/tree/main/examples/bart
et al., 2021c; 2022a) is an extractive question answering 4https://github.com/huggingface/
task, consisting of questions posed by crowdworkers on transformers
12SwitchableDecision:DynamicNeuralGenerationNetworks
A.4.Morecomparisons
AsdiscussedinSection6,WeselecttheGPT-2(Radford
etal.,2019)baseandT5(Raffeletal.,2020)tostudythe
performanceafteradaptingourproposedswitchabledeci-
sions. WealsoincludedLLaMA(Touvronetal.,2023)as
anadditionalcomparisoninTable16.
13