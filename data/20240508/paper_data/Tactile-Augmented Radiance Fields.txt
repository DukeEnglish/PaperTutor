Tactile-Augmented Radiance Fields
YimingDou1 FengyuYang2 YiLiu1 AntonioLoquercio3 AndrewOwens1
1UniversityofMichigan 2YaleUniversity 3UCBerkeley
Vision Touch Sample Vision Touch Pred.
Figure1. Tactile-augmentedradiancefields. Wecaptureatactile-augmentedradiancefield(TaRF)fromphotosandsparselysampled
touchprobes. Todothis,weregisterthecapturedvisualandtactilesignalsintoashared3Dspace,thentrainadiffusionmodeltoimpute
touchatotherlocationswithinthescene. Here,wevisualizetwotouchprobesandtheir(colorcoded)3Dpositionsinthescene. Wealso
showtwotouchsignalsestimatedbythediffusionmodel. Thetouchsignalswerecollectedusingavision-basedtouchsensor[32]that
representsthetouchsignalsasimages.Pleaseseeourprojectpageforvideoresults.
Abstract 1.Introduction
We present a scene representation, which we call a Ashumans,ourabilitytoperceivetheworldreliescrucially
tactile-augmentedradiancefield(TaRF),thatbringsvision on cross-modal associations between sight and touch [19,
and touch into a shared 3D space. This representation 50]. Tactile sensing provides a detailed understanding of
can be used to estimate the visual and tactile signals for material properties and microgeometry, such as the intri-
a given 3D position within a scene. We capture a scene’s
cate patterns of bumps on rough surfaces and the complex
TaRF from a collection of photos and sparsely sampled
motions that soft objects make when they deform. This
touchprobes. Ourapproachmakesuseoftwoinsights: (i)
type of understanding, which largely eludes today’s com-
common vision-based touch sensors are built on ordinary
puter vision models, is a critical component of applica-
cameras and thus can be registered to images using meth-
tionsthatrequirereasoningaboutphysicalcontact,suchas
ods from multi-view geometry, and (ii) visually and struc-
turallysimilarregionsofascenesharethesametactilefea- robotic locomotion [3, 24, 31, 34, 37, 38] and manipula-
tures. We use these insights to register touch signals to a tion[6,7,11,42,60],andmethodsthatsimulatethebehav-
captured visual scene, and to train a conditional diffusion iorofmaterials[4,13,40,41].
model that, provided with an RGB-D image rendered from
Incomparisontomanyothermodalities, collectingtac-
aneuralradiancefield,generatesitscorrespondingtactile
tile data is an expensive and tedious process, since it re-
signal. To evaluate our approach, we collect a dataset of
quires direct physical interaction with the environment. A
TaRFs. Thisdatasetcontainsmoretouchsamplesthanpre-
recent line of work has addressed this problem by having
viousreal-worlddatasets,anditprovidesspatiallyaligned
visual signals for each captured touch signal. We demon- humansorrobotsprobetheenvironmentwithtouchsensors
strate the accuracy of our cross-modal generative model (seeTable1). Earlyeffortshavebeenfocusedoncapturing
and the utility of the captured visual-tactile data on sev- thepropertiesofonlyafewobjectseitherinsimulation[16,
eral downstream tasks. Project page: https://dou- 17,52]orinlab-controlledsettings[6,7,18,28,35,52,63],
yiming.github.io/TaRF. which may not fully convey the diversity of tactile signals
in natural environments. Other works have gone beyond a
1
4202
yaM
7
]VC.sc[
1v43540.5042:viXraDataset Samples Aligned Scenario Source
MoreThanaFeeling[7] 6.5k ✕ Tabletop Robot
FeelingofSuccess[6] 9.3k ✕ Tabletop Robot
VisGel[35] 12k ✕ Tabletop Robot
SSVTP[28] 4.6k ✓ Tabletop Robot
ObjectFolder1.0[16] – ✓ Object Synthetic
ObjectFolder2.0[17] – ✓ Object Synthetic
ObjectFolderReal[18] 3.7k ✕ Object Robot
OF2.0[17] VisGel[35] OFReal[18] SSVTP[28] TG[56] TaRF(Ours)
Burkaetal.[5] 1.1k ✕ Sub-scene Human
TouchandGo[56] 13.9k ✕ Sub-scene Human Figure2.Visual-tactileexamples.Incontrasttothevisual-tactile
YCB-Slide∗[52] - ✓ Object Human datacapturedinpreviouswork,ourapproachallowsustosample
TouchingaNeRF[63] 1.2k ✓ Object Robot unobstructedimagesthatarespatiallyalignedwiththetouchsig-
TaRF(Ours) 19.3k ✓ Fullscene Human
nal,fromarbitrary3DviewpointsusingaNeRF.
Table 1. Dataset comparison. We present the number of real
visual-tactile pairs and whether such pairs are visually aligned, tion for novel positions in the scene. Analogous to quasi-
i.e.,whetherthevisualimageincludesanocclusion-freeviewof dense stereo methods [15, 33], the diffusion model effec-
thetouchedsurface. ∗YCB-Slidehasreal-worldtouchprobesbut tively propagates sparse touch samples, obtained by prob-
syntheticimagesrenderedwithCADmodelsofYCBobjectsona ing,toothervisuallyandstructurallysimilarregionsofthe
whitebackground[9]. scene.
We evaluate our visual-tactile model’s ability to accu-
labsettingandhavecollectedtouchfromrealscenes[5,56].
rately perform cross-modal translation using a variety of
However,existingdatasetslackalignedvisualandtactilein-
quality metrics. We also apply it to several downstream
formation,sincethetouchsensorandtheperson(orrobot)
tasks, including localizing a touch within a scene and un-
thatholdsitoftenoccludelargeportionsofthevisualscene
derstanding material properties of the touched area. Our
(Fig. 2). These datasets also contain only a sparse set of
experimentssuggest:
touchsignalsforeachscene,anditisnotclearhowthesam-
• Touchsignalscanbelocalizedin3Dspacebyexploiting
pledtouchsignalsrelatetoeachotherin3D.
multi-viewgeometryconstraintsbetweensightandtouch.
Inthiswork,wepresentasimpleandlow-costprocedure
• Estimatedtouchmeasurementsfromnovelviewsarenot
to capture quasi-dense, scene-level, and spatially-aligned
only qualitatively accurate, but also beneficial on down-
visual and touch data (Fig. 1). We call the resulting scene
streamtasks.
representation a tactile-augmented radiance field (TaRF).
• Cross-modal prediction models can accurately estimate
We remove the need for robotic collection by leveraging
touchfromsightfornaturalscenes.
a 3D scene representation (a NeRF [39]) to synthesize a
• Visually-acquired 3D scene geometry improves cross-
viewofthesurfacebeingtouched,whichresultsinspatially
modalprediction.
alignedvisual-tactiledata(Fig.2). Wecollectthisdataby
mountingatouchsensortoacamerawithcommonlyavail- 2.RelatedWork
ablematerials(Fig.3). Tocalibratethepairofsensors,we
take advantage of the fact that popular vision-based touch Visual-tactile datasets. Previous work has either used
sensors[25,26,32,48]arebuiltonordinarycameras. The simulators [16, 17] or robotic arms [6, 8, 18, 35, 63] for
relativeposebetweenthevisionandtactilesensorscanthus datageneration.OurworkiscloselyrelatedtothatofZhong
beestimatedusingtraditionalmethodsfrommulti-viewge- et al. [63], which uses a NeRF and captured touch data to
ometry,suchascameraresectioning[20]. generate a tactile field for several small objects. They use
We use this procedure to collect a large real-world the proprioception of an expensive robot to spatially align
datasetofalignedvisual-tactiledata. Withthisdataset, we visionandtouch. Incontrast,weleveragethepropertiesof
train a diffusion model [45, 51] to estimate touch at loca- thetactilesensorandnovelviewsynthesistousecommonly
tionsnotdirectlyprobedbyasensor. Incontrasttothere- availablematerial(asmartphoneandaselfiestick)toalign
centworkofZhongetal.[63],whichalsoestimatestouch vision and touch. This enables the collection of a larger,
from3DNeRFgeometry,wecreatescene-scalereconstruc- scene-level, and more diverse dataset, on which we train a
tions,wedonotrequireroboticproprioception,andweuse higher-capacity diffusion model (rather than a conditional
diffusionmodels[51]. Thisenablesustoobtaintactiledata GAN).Likeseveralpreviousworks[5,56],wealsocollect
at a much larger scale, and with considerably more diver- scene-leveldata. Incontrasttothem,wespatiallyalignthe
sity. Unlikepreviousvisual-tactilediffusionwork[57],we signals by registering them in a unified 3D representation,
condition the model on spatially aligned visual and depth therebyincreasingthepredictionpowerofthevisual-tactile
information,enhancingthegeneratedsamples’qualityand generativemodel.
theirusefulnessindownstreamapplications. Aftertraining, Capturing multimodal 3D scenes. Our work is related
the diffusion model can be used to predict tactile informa- tomethodsthatcapture3Dvisualreconstructionsofspaces
2usingRGB-Ddata[12,49,55,59]andmultimodaldatasets
of paired 3D vision and language [1, 2, 10]. Our work is
alsorelatedtorecentmethodsthatlocalizeobjectsinNeRFs Visual
Camera
usingjointembeddingsbetweenimagesandlanguage[29] Visual frames
or by semantic segmentation [62]. In contrast to language
Tactile
supervision,touchistiedtoaprecisepositioninascene. Sensor
3D touch sensing. A variety of works have studied
Tactile frames Visual-Tactile
the close relationship between geometry and touch, moti- Correspondences
vating our use of geometry in imputing touch. Johnson
Figure3.Capturingsetup.(a)Werecordpairedvisionandtouch
et al. [25, 26] proposed vision-based touch sensing, and
signalsusingacameraattachedtoatouchsensor.(b)Weestimate
showed that highly accurate depth can be estimated from
therelativeposebetweenthetouchsensorandthecamerausing
thetouchsensorusingphotometricstereo. Otherworkhas
correspondencesbetweensightandtouch.
estimatedobject-scale3Dfromtouch[54]. Bycontrast,we
combinesparseestimatesoftouchwithquasi-densetactile
signalsestimatedusinggenerativemodels. onacamera(Fig.3),obtainingsynchronizedtouchsignals
Cross-modal prediction of touch from sight. Recent {τ i}N
i=1
andvideoframesv. Wethenestimatetheposeof
workhastrainedgenerativemodelsthatpredicttouchfrom the video frames using off-the-shelf structure from motion
images. Li et al. [35] used a GAN to predict touch for methods[47],obtainingposes{pv}N . Finally,weusethe
i i=1
images of a robotic arm, while Gao et al. [18] applied calibrationofthemounttoobtaintheposes{pt}N ofthe
i i=1
them to objects collected on a turntable. Yang et al. [57] tactilemeasurementswithrespecttothescene’sglobalref-
used latent diffusion to predict touch from videos of hu- erenceframe. Asacollectiondevice,wemountaniPhone
mans touching objects. Our goal is different from these 14Protooneendofacamerarod,andaDIGIT[32]touch
works: we want to predict touch signals that are spatially sensor to the other end. Note that the devices can be re-
aligned with a visual signal, to exploit scene-specific in- placed with any RGB-D camera and vision-based tactile
formation, and to use geometry. Thus, we use a different sensor.
architecture and conditioning signal, and fit our model to Capturing setup calibration. To find the relative pose
examples from the same scenes at training and test time. between the camera and the touch sensor (Fig. 3), we ex-
Other work has learned joint embeddings between vision ploit the fact that arbitrary viewpoints can be synthesized
andtouch[28,36,56,58,61]. fromF ,andthatubiquitousvision-basedtouchsensorsare
θ
based on perspective cameras. In these sensors, an elas-
3.Method
tomer gel is placed on the lens of a commodity camera,
which is illuminated by colored lights. When the gel is
Wecollectvisualandtactileexamplesfromasceneandreg-
pressed into an object, it deforms, and the camera records
isterthemtogetherwitha3Dvisualreconstructiontobuilda
animageofthedeformation; thisimageisusedasthetac-
TaRF.Specifically,wecaptureaNeRFF :(x,r)(cid:55)→(c,σ)
θ
tilesignal. Thisdesignallowsustoestimatetheposeofthe
that maps a 3D point x = (x,y,z) and viewing direction
tactile sensor through multi-view constraints from visual-
r to its corresponding RGB color c and density σ [39].
tactilecorrespondences: pixelsinvisualimagesandtactile
We associate to the visual representation a touch model
imagesthatareofthesamephysicalpoint.
F :v (cid:55)→τ thatgeneratesthetactilesignalthatonewould
ϕ t
We start the calibration process by synthesizing novel
obtainbytouchingatthecenteroftheimagev . Inthefol-
t
viewsfromF .Theviewsaregeneratedatthecameraloca-
lowing,weexplainhowtoestimateF andF andputthem θ
θ ϕ tion{pv}N ,butrotated90◦onthex-axis. Thisisbecause
intothesameshared3Dspace. i i=1
the camera is approximately orthogonal to the touch sen-
3.1.Capturingvisionandtouchsignals sor(seeFig.3). Then,wemanuallyannotatecorresponding
pixels between the touch measurements and the generated
Obtainingavisual3Dreconstruction. Webuildthevi- frames (Fig. 3). To simplify and standardize this process,
sualNeRF,F θ,closelyfollowingpreviouswork[12,55]. A weplaceabrailleboardineachsceneandprobeitwiththe
humandatacollectormovesthroughasceneandrecordsa touch sensor. This will generate a distinctive touch signal
video,coveringasmuchofthespaceaspossible. Wethen thatiseasytolocalize[23].
estimatecameraposeusingstructurefrommotion[47]and Weformulatetheproblemofestimatingthesixdegrees
createaNeRFusingoff-the-shelfpackages[53].Additional of freedom relative pose (R,t) between the touch sensor
detailsareprovidedinthesupplement. and the generated frames as a resectioning problem [20].
Capturing and registering touch. We simultaneously We use the estimated 3D structure from the NeRF F to
θ
collecttactileandvisualsignalsbymountingatouchsensor obtain 3D points {x }M for each of the annotated corre-
i i=1
3spondences. Each point has a pixel position u ∈ R2 in (cid:31)(cid:30)(cid:30)(cid:30)(cid:30)(cid:29)(cid:30)(cid:30)(cid:30)(cid:30)(cid:28)
i
thetouchmeasurement. Wefind(R,t)byminimizingthe NeRF
reprojectionerror:
RGB Depth
M 1 (cid:88)
min ∥π(K[R | t],X )−u ∥ , (1)
R,t M i i 1
i=1
whereπprojectsa3Dpointusingagivenprojectionmatrix,
K are the known intrinsics of the tactile sensor’s camera,
Gaussian Noise Latent Diffusion Est. Touch
andthepointX isinthecoordinatesystemofthegenerated
i
visionframes. Weperformtheoptimizationon6-15anno- Figure4. Touchestimation. Weestimatethetactilesignalfora
tated correspondences from the braille board. For robust- giventouchsensorpose(R,t). Todothis,wesynthesizeaview-
ness, we compute correspondences from multiple frames. pointfromtheNeRF,alongwithadepthmap.Weuseconditional
latentdiffusiontopredictthetactilesignalfromtheseinputs.
Werepresenttherotationmatrixusingquaternionsandop-
timizeusingnonlinearleast-squares. Oncewehave(R,t)
4.A3DVisual-TactileDataset
withrespecttothegeneratedframes,wecanderivetherel-
ativeposebetweenthecameraandthetouchsensor. Inthefollowing,weshowthedetailsofthedatacollection
processandstatisticsofourdataset.
3.2.Imputingthemissingtouch
4.1.DataCollectionProcedure
Weuseagenerativemodeltoestimatethetouchsignal(rep-
resentedasanimagefromavision-basedtouchsensor)for The data collection procedure is divided into two stages.
otherlocationswithinthescene.Specifically,wetrainadif- First, we collect multiple views from the scene, capturing
fusionmodelp (τ |v,d,b),wherevanddareimagesand enough frames around the areas we plan to touch. During
ϕ
depthmapsextractedfromF (seeFig.4). Wealsopassas thisstage, wecollectapproximately500frames. Next, we
θ
input to the diffusion model a background image captured collectsynchronizedvisualandtouchdata,maximizingthe
bythetouchsensorwhenitisnotincontactwithanything, geometryandtexturebeingtouched. Wethenestimatethe
denotedasb.Althoughnotessential,wehaveobservedthat cameralocationofthevisionframescollectedintheprevi-
thisadditionalinputempiricallyimprovesthemodel’sper- oustwostagesusingoff-the-shelfmappingtools[47].After
formance(e.g.,Fig.1thebackgroundprovidesthelocation estimatingthecameraposesforthevisionframes,thetouch
ofdefectsinthegel,whichappearasblackdots). Wetrain measurements’ poses can be derived by using the mount
themodelp onourentirevision-touchdataset(Sec.4). calibration matrix. More details about the pose estimation
ϕ
The training of p is divided into two stages. In the procedurecanbefoundinthesupplement.
ϕ
first,wepre-trainacross-modalvisual-tactileencoderwith Finally,weassociateeachtouchsensorwithacolorim-
self-supervised contrastive learning on our dataset. This age by translating the sensor poses upwards by 0.4 meters
stage, initially proposed by [23, 57], is equivalent to the andqueryingtheNeRFwithsuchposes. Thefieldofview
self-supervised encoding pre-training that is common for we use when querying the NeRF is 50◦. This provides us
image generation models [45]. We use a ResNet-50 [21] with approximately 1,500 temporally aligned vision-touch
asthebackboneforthiscontrastivemodel. imagepairsperscene.Notethatthiscollectionprocedureis
In the second stage, we use the contrastive model to scalablesinceitdoesnotrequirespecificexpertiseorequip-
generatetheinputforaconditionallatentdiffusionmodel, mentandgeneratesabundantscene-levelsamples.
which is built upon Stable Diffusion [45]. A frozen pre-
4.2.DatasetStatistics
trainedVQ-GAN[14]isusedtoobtainthelatentrepresen-
tationwithaspatialdimensionof64×64. Westarttrain- Wecollectourdatain13ordinaryscenesincludingtwoof-
ingthediffusionmodelfromscratchandpre-trainitonthe fices, a workroom, a conference room, a corridor, a table-
taskofunconditionaltactileimagegenerationontheYCB- top, a corridor, a lounge, a room with various clothes and
Slidedataset[52]. Afterthisstage,wetraintheconditional four outdoor scenes with interesting materials. Typically,
generative model p on our spatially aligned visual-tactile wecollect1kto2ktactileprobesineachscene,resultingin
ϕ
dataset,furtherfine-tuningthecontrastivemodelend-to-end atotalof19.3kimagepairsinthedataset.
withthegenerationtask. Some representative samples from the collected dataset
Atinferencetime,givenanovellocationinthe3Dscene, are shown in Fig. 5. Our data includes a large variety of
we first render the visual signals vˆ and dˆ from NeRF, and geometry(edges, surfaces, corners, etc.)andtexture(plas-
then estimate the touch signal τˆ of the position using the tic, clothes, snow, wood, etc.) of different materials in the
diffusionmodel. scene. During capturing process, the collector will try to
4
{Figure 5. Representative examples from the captured dataset. Our dataset is obtained from nine everyday scenes, such as offices,
classrooms,andkitchens.Weshowthreesuchscenesinthefigureabove,togetherwithsamplesofspatiallyalignedvisualandtactiledata.
Ineachscene,1kto2ktactileprobeswerecollected,resultinginatotalof19.3kimagepairs. Thedataencompassesdiversegeometries
(edges,surfaces,corners,etc.) andtextures(plastic,clothes,snow,wood,etc.) ofvariousmaterials. Thecollectorsystematicallyprobed
differentobjects,coveringareaswithdistinctgeometryandtextureusingdifferentsensorposes.
thoroughly probe various objects and cover the interesting Visual-tactilegenerativemodel. Ourimplementationof
areaswithmoredistinguishablegeometryandtexturewith the diffusion model closely follows Stable Diffusion [46],
different sensor poses. To the best of our knowledge, our with the difference that we use a ResNet-50 to generate
datasetisthefirstdatasetthatcapturesfull,scene-scalespa- thevisualencodingfromRGB-Dimagesforconditioning.
tially aligned vision-touch image pairs. We provide more Specifically,wealsoaddtheRGB-Dimagesrenderedfrom
detailsaboutthedatasetinthesupplement. the tactile sensors’ poses into the conditioning, which we
refer to in Sec. 5.2 as multiscale conditioning. The model
5.Experiments isoptimizedfor30epochsbyAdam[30]optimizerwitha
base learning rate of 10−5. The learning rate is scaled by
Leveragingthespatiallyalignedimageandtouchpairsfrom
gpunumber × batchsize. We train the model with batch
ourdataset,wefirstconductexperimentsondensetouches-
size of 48 on 4 NVIDIA A40 GPUs. At inference time,
timation.Wethenshowtheeffectivenessofboththealigned
the model conducts 200 steps of denoising process with a
datapairsandthesynthesizedtouchsignalsbyconducting
7.5 guidance scale. Following prior cross-modal synthe-
tactilelocalizationandmaterialclassificationastwodown-
sis work [44], we use reranking to improve the prediction
streamtasks.
quality. Weobtain16samplesfromthediffusionmodelfor
5.1.ImplementationDetails everyinstanceandre-rankthesampleswithourpretrained
contrastivemodel.Thesamplewithhighestsimilarityisthe
NeRF. WeusetheNerfactomethodfromNerfstudio[53]. finalprediction.
For each scene, we utilize approximately 2,000 images as
5.2.DenseTouchEstimation
trainingset,whichthoroughlycoverthescenefromvarious
viewpoints. Wetrainthenetworkwithabaselearningrate Experimental setup. We now evaluate the diffusion
of 1×10−2 using Adam [30] optimizer for 200,000 steps model’sabilitytogeneratetouchimages.Toreduceoverlap
onasingleNVIDIARTX2080TiGPUtoachieveoptimal between the training and test set, we first split the frames
performance. into sequences temporally (following previous work [56]).
Visual-tactile contrastive model. Following prior Wesplitthemintosequencesof50touchsamples,thendi-
works [27, 57], we leverage contrastive learning methods vide these sequences into train/validation/test with a ratio
totrainaResNet-50[21]asvisualencoder. Thevisualand of 8/1/1. We evaluate the generated samples on Frechet
tactileencoderssharethesamearchitecturebuthavediffer- Inception Distance (FID), a standard evaluation metric
ent weights. We encode visual and tactile data into latent for cross-modal generation [56]. We also include Peak
vectors in the resulting shared representation space. We Signal to Noise Ratio (PSNR) and Structural Similarity
set the dimension of the latent vectors to 32. Similar to (SSIM), though we note that these metrics are highly sen-
CLIP [43], the model is trained on InfoNCE loss obtained sitive to spatial position of the generated content, and can
from the pairwise dot products of the latent vectors. We be optimized by models that minimize simple pixelwise
trainthemodelfor20epochsbyAdam[30]optimizerwith losses[22].WealsoincludeCVTPmetricproposedbyprior
alearningrateof10−4 andbatchsizeof256on4NVIDIA work [57], which measures the similarity between visual
RTX2080TiGPUs. andtactileembeddingsofacontrastivemodel,analogousto
5Condition G.T. Ours L1 VisGel Condition G.T. Ours L1 VisGel
Figure6. Qualitativetouchestimationresults. EachmodelisconditionedontheRGBimageanddepthmaprenderedfromtheNeRF
(left). The white box indicates the tactile sensor’s approximate field of view (which is much smaller than the full conditional image).
TheG.T.columnshowsthegroundtruthtouchimagesmeasuredfromaDIGITsensor. L1andVisGeloftengenerateblurrytexturesand
inaccurategeometry. Bycontrast,ourmodelbettercapturesthefeaturesofthetactileimage,e.g.,therock’smicrogeometryandcomplex
texturesandshapesoffurniture.Thelastrowshowstwofailurecasesofourmodel.Inbothexamples,ourmodelgeneratesatouchimage
thatisgeometricallymisalignedwiththegroundtruth.Alloftheexamplesshownhereareatleast10cmawayfromanytrainingsample.
CLIP [43] score. We compare against two baselines: Vis- Model PSNR↑ SSIM↑ FID↓ CVTP↑
Gel,theapproachfromLiet.[35],whichtrainsaGANfor
L1 24.34 0.82 97.05 0.01
touch generation, and L1, a model with the same architec-
VisGel[35] 23.66 0.81 130.22 0.03
ture of VisGel but trained to minimize an L1 loss in pixel
Ours 22.84 0.72 28.97 0.80
space.
Table 2. Quantitative results on touch estimation for novel
Results. As is shown in Table 2, our approach performs
views.Whilecomparableonlow-levelmetricswiththebaselines,
muchbetteronthehigh-levelmetrics, withupto4xlower
our approach captures the characteristics of the real tactile data
FIDand80xhigherCVTP.Thisindicatesthatourproposed
moreeffectively,resultinginalowerFIDscore.
diffusionmodelcapturesthedistributionandcharacteristics
of the real tactile data more effectively. On the low-level graineddetailsofatactileimage.Removingdepthimageor
metrics (PSNR and SSIM), all methods are comparable. contrastivepretraininghassmalleffectonCVTPbutresults
In particular, the L1 model slightly outperforms the other in a drop on FID. Contrastive re-ranking largely improves
methods since the loss it is trained on is highly correlated CVTP, indicating the necessity of obtaining multiple sam-
withlow-level,pixel-wisemetrics.Fig.6qualitativelycom- plesfromthediffusionmodel. Wealsofindthatmultiscale
paressamplesfromthedifferentmodels.Indeed,ourgener- conditioningprovideasmallbenefitonFIDandCVTP.
atedsamplesexhibitenhanceddetailsinmicro-geometryof
5.3.DownstreamTaskI:TactileLocalization
fabrics and richer textures, including snow, wood and car-
peting. However, all methods fail on fine details that are
To help understand the quality of the captured TaRFs, we
barelyvisibleintheimage,suchasthetreebark.
evaluatetheperformanceofthecontrastivemodel(usedfor
Ablationstudy. Weevaluatetheimportanceofthemain conditioningourdiffusionmodel)onthetaskoftactilelo-
components of our proposed touch generation approach calization. Given a tactile signal, our goal is to find the
(Table 3). Removing the conditioning on the RGB image correspondingregionsina2Dimageorina3Dscenethat
results in the most prominent performance drop. This is are associated with it, i.e., we ask the question: what part
expected since RGB image uniquely determines the fine- ofthisimage/scenefeellikethis? Weperformthefollowing
6
egde
teprac
ksed
ecafrus
llaw
kcirb
kcor
riahc
afos
ksedQuery Heatmap Query Heatmap Query Heatmap Query Heatmap
Figure 7. Tactile localization heatmaps. Given a tactile query image, the heatmap shows the image patches with a higher affinity to
thistactilesignal,asmeasuredbyacontrastivemodeltrainedonourdataset. Weuseaslidingwindowandcompareeachextractedpatch
withthetouchsignal. Ineachcase,thecenterpatchisthetrueposition. Ourmodelsuccessfullycapturesthecorrelationbetweenthetwo
signals. Thisenablesittolocalizeavarietyoftouchsignals,includingfine-grainedgeometry,e.g.,acableorakeyboard,varioustypesof
cornersandedges,andlargeuniformregions,suchasaclothing. Thisabilityenablesourdiffusionmodeltoeffectivelypropagatesparse
touchsamplestoothervisuallyandstructurallysimilarregionsofthescene.
Modelvariation PSNR↑ SSIM↑ FID↓ CVTP↑ 3D Localization. In 3D, the association of an image to
tactile measurements becomes less ambiguous. Indeed,
Full 22.84 0.72 28.97 0.80
sincetactile-visualsamplesarerotation-dependent,objects
NoRGBconditioning 22.13 0.70 34.31 0.76
with similar shapes but different orientations will generate
Nodepthconditioning 22.57 0.71 33.16 0.80
Nocontrastivepretraining 22.82 0.71 32.98 0.79 different tactile measurements. Lifting the task to 3D still
Nore-ranking 22.92 0.72 29.46 0.61 doesnotremoveallambiguities(forexample,eachsideof
Nomultiscale 23.19 0.72 30.89 0.77 arectangulartablecannotbepreciselylocalized). Nonethe-
less,webelieveittobeagoodfitforaquantitativeevalua-
Table3. Ablationstudy. Sincethefine-graineddetailsoftouch tionsinceit’srarefortwoambiguouspartsofthesceneto
images can be determined from a RGB image, removing condi- betouchedwithexactlythesameorientation.
tioningonthelatterresultsinthelargestperformancedrops. Re- We use the following experimental setup for 3D local-
rankinghasnotableimpactonCVTP,indicatingthenecessityof
ization. Given a tactile image as a query, we compute its
obtainingmultiplesamplesfromthediffusionmodel.
distanceinembeddingspacetoallvisualtestimagesfrom
evaluationsonthetestsetofourdataset. Notethatwerun thesamescene.Notethatalltestimagesareassociatedwith
notask-specifictraining. a3D location. We defineasground-truth correspondences
all test images at a distance of at most r from the 3D lo-
cation of the test sample. We vary r to account for local
2D Localization. To determine which part of an image ambiguities. Astypicalintheretrievalliterature,webench-
areassociatedwithagiventactilemeasurement,wefollow marktheperformancewithmetricmeanAveragePrecision
thesamesetupofSSVTP[28]. Wefirstsplittheimageinto (mAP).
patches and compute their embedding. Then, we generate We consider three baselines: (1) chance, which ran-
thetactileembeddingoftheinputtouchimage. Finally,we domlyselectscorrespondingsamples; (2)real, whichuses
compute the pairwise similarities between the tactile and the contrastive model trained on our dataset; and (3) real
visual embeddings, which we plot as a heatmap. As we + estimated, which trains the contrastive model on both
canseeinFig.7,ourconstrastiveencodercansuccessfully datasetsamplesandasetofsyntheticsamplesgeneratedvia
capturethecorrelationsbetweenthevisualandtactiledata. thescenes’NeRFandourtouchgenerationmodel. Specif-
Forinstance,thetactileembeddingsofedgesareassociated ically, werenderanewimageandcorrespondingtouchby
toedgesofsimilarshapeinthevisualimage. Notethatthe interpolating the position of two consecutive frames in the
majority of tactile embeddings are highly ambiguous: all training dataset. This results in a training dataset for the
edgeswithasimilargeometryfeelthesame. contrastivemodelthatistwiceaslarge.
7r(m) Hard/ Rough/
Dataset Material
Soft Smooth
Dataset 0.001 0.005 0.01 0.05 0.1
Chance 18.6 66.1 56.3
Chance 3.55 6.82 10.25 18.26 21.33
ObjectFolder2.0[17] 36.2 72.0 69.0
Real 12.10 22.93 32.10 50.30 57.15
VisGel[35] 39.1 69.4 70.4
Real+Est. 14.92 26.69 36.17 53.62 60.61
TouchandGo[56] 54.7 77.3 79.4
Table 4. Quantitative results on 3D tactile localization. We +ObjectFolder2.0[17] 54.6 87.3 84.8
evaluateusingmeanAveragePrecision(mAP)asametric. Train- +VisGel[35] 53.1 86.7 83.6
ing the contrastive model on our dataset of visually aligned real +Ours∗(Real) 57.6 88.4 81.7
samples together with estimated samples from new locations in +Ours∗(Real+Estimated) 59.0 88.7 86.1
thesceneresultsinthehighestperformance.
Table 5. Material classification. We show the downstream
material recognition accuracy of models pre-trained on different
Theresults,presentedinTable4,demonstratetheperfor-
datasets. Thefinalrowsshowtheperformancewhencombining
mance benefit of employing both real and synthetic tactile
different datasets with Touch and Go [56]. ∗ The task-specific
pairs. Combiningsynthetictactileimageswiththeoriginal
trainingandtestingdatasetsforthistaskarecollectedwithaGel-
pairs achieves highest performance on all distance thresh-
Sightsensor. Wenotethatourdatacomesfromadifferentdistri-
olds. Overall,thisindicatesthattouchmeasurementsfrom
bution,sinceitiscollectedwithaDIGITsensor[32].
novelviewsarenotonlyqualitativelyaccurate,butalsoben-
eficialforthisdownstreamtask.
6.Conclusion
5.4.DownstreamTaskII:MaterialClassification
We investigate the efficacy of our visual-tactile dataset for
In this work, we present the TaRF, a scene representation
understanding material properties, focusing on the task of
that brings vision and touch into a shared 3D space. This
materialclassification. WefollowtheformulationbyYang
representation enables the generation of touch probes for
et al. [56], which consists of three subtasks: (i) material
novelscenelocations. Tobuildthisrepresentation,wecol-
classification, requiring the distinction of materials among
lectthelargestdatasetofspatiallyalignedvisionandtouch
20 possible classes; (ii) softness classification, a binary
probes.We study the utility of both the representation and
problem dividing materials as either hard or soft; and (iii)
thedatasetinaseriesofqualitativeandquantitativeexperi-
hardnessclassification, whichrequirestheclassificationof
mentsandontwodownstreamtasks: 3Dtouchlocalization
materialsaseitherroughorsmooth.
andmaterialrecognition. Overall,ourworkmakesthefirst
Wefollowthesameexperimentalprocedureof[56]: we steptowardsgivingcurrentscenerepresentationtechniques
pretrainacontrastivemodelonadatasetandperformlinear anunderstandingofnotonlyhowthingslook,butalsohow
probingonthesub-tasks’trainingset.Ourexperimentsonly theyfeel.Thiscapabilitycouldbecriticalinseveralapplica-
varythepretrainingdataset,leavingallarchitecturalchoices tionsrangingfromroboticstothecreationofvirtualworlds
and hyperparameters the same. We compare against four thatlookandfeelliketherealworld.
baselines. A random classifier (chance); the ObjectFolder
2.0 dataset [17]; the VisGel dataset [35]; and the Touch Limitations. Sincethetouchsensorisbasedonahighly
andGodataset[56]. Notethatthetouchsensorusedinthe zoomed-in camera, small (centimeter-scale) errors in SfM
testdata(GelSight)differsfromtheoneusedinourdataset or visual-tactile registration can lead to misalignments of
(DIGIT). Therefore, we use for pretraining a combination severalpixelsbetweentheviewsoftheNeRFandthetouch
ofourdatasetandTouchandGo. Toensureafaircompar- samples, whichcanbeseeninourTaRFs. Anotherlimita-
ison, we also compare to the combination of each dataset tion of the proposed representation is the assumption that
andTouchandGo. the scene’s coarse-scale structure does not change when it
The findings from this evaluation, as shown in Table 5, is touched, an assumption that may be violated for some
suggestthatourdataimprovestheeffectivenessofthecon- inelasticsurfaces.
trastivepretrainingobjective,eventhoughourdataisfrom
adifferentdistribution. Moreover,wefindthataddingesti- Acknowledgements. We thank Jeongsoo Park, Ayush
mated touch probes for pretraining results in a higher per- Shrivastava, Daniel Geng, Ziyang Chen, Zihao Wei, Zix-
formance on all the three tasks, especially the smoothness uanPan,ChaoFeng,ChrisRockwell,GauravKaulandthe
classification. Thisindicatesthatnotonlydoesourdataset reviewers for the valuable discussion and feedback. This
covers a wide range of materials but also our diffusion workwassupportedbyanNSFCAREERAward#2339071,
model captures the distinguishable and useful patterns of a Sony Research Award, the DARPA Machine Common
differentmaterials. Senseprogram,andONRMURIawardN00014-21-1-2801.
8References [13] AbeDavis,JustinGChen,andFre´doDurand. Image-space
modalbasesforplausiblemanipulationofobjectsinvideo.
[1] PanosAchlioptas,AhmedAbdelreheem,FeiXia,Mohamed
ACMTransactionsonGraphics(TOG),2015. 1
Elhoseiny,andLeonidasGuibas. Referit3d:Neurallisteners
[14] Mucong Ding, Kezhi Kong, Jingling Li, Chen Zhu, John
forfine-grained3dobjectidentificationinreal-worldscenes.
Dickerson,FurongHuang,andTomGoldstein. Vq-gnn: A
In Computer Vision–ECCV 2020: 16th European Confer-
universalframeworktoscaleupgraphneuralnetworksusing
ence,Glasgow,UK,August23–28,2020,Proceedings,Part
vector quantization. Advances in Neural Information Pro-
I16,2020. 3
cessingSystems,34:6733–6746,2021. 4
[2] Peter Anderson, Ayush Shrivastava, Joanne Truong, Arjun [15] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and
Majumdar,DeviParikh,DhruvBatra,andStefanLee. Sim- robust multiview stereopsis. IEEE transactions on pattern
to-realtransferforvision-and-languagenavigation. InCon- analysisandmachineintelligence, 32(8):1362–1376, 2009.
ference on Robot Learning, pages 671–681. PMLR, 2021. 2
3
[16] RuohanGao, Yen-YuChang, ShivaniMall, LiFei-Fei, and
[3] Jakub Bednarek, Michal Bednarek, Lorenz Wellhausen, JiajunWu. Objectfolder: Adatasetofobjectswithimplicit
Marco Hutter, and Krzysztof Walas. What am i touching? visual,auditory,andtactilerepresentations. InCoRL,2021.
learning to classify terrain via haptic sensing. In 2019 In- 1,2
ternationalConferenceonRoboticsandAutomation(ICRA), [17] RuohanGao,ZilinSi,Yen-YuChang,SamuelClarke,Jean-
pages7187–7193.IEEE,2019. 1 netteBohg,LiFei-Fei,WenzhenYuan,andJiajunWu. Ob-
[4] Katherine L Bouman, Bei Xiao, Peter Battaglia, and jectfolder 2.0: A multisensory object dataset for sim2real
William T Freeman. Estimating the material properties of transfer. In Proceedings of the IEEE/CVF conference on
fabricfromvideo. InProceedingsoftheIEEEinternational computer vision and pattern recognition, pages 10598–
conferenceoncomputervision,pages1984–1991,2013. 1 10608,2022. 1,2,8
[5] AlexanderBurka. Instrumentation,data,andalgorithmsfor [18] RuohanGao,YimingDou,HaoLi,TanmayAgarwal,Jean-
visuallyunderstandinghapticsurfaceproperties. 2018. 2 netteBohg,YunzhuLi,LiFei-Fei,andJiajunWu. Theob-
jectfolderbenchmark:Multisensorylearningwithneuraland
[6] RobertoCalandra,AndrewOwens,ManuUpadhyaya,Wen-
real objects. In Proceedings of the IEEE/CVF Conference
zhen Yuan, Justin Lin, Edward H. Adelson, and Sergey
onComputerVisionandPatternRecognition,pages17276–
Levine. The feeling of success: Does touch sensing help
predict grasp outcomes? Conference on Robot Learning 17286,2023. 1,2,3
(CoRL),2017. 1,2 [19] MichaelSAGrazianoandCharlesGGross. Therepresen-
tation of extrapersonal space: A possible role for bimodal,
[7] Roberto Calandra, Andrew Owens, Dinesh Jayaraman,
visual-tactileneurons. 1995. 1
JustinLin,WenzhenYuan,JitendraMalik,EdwardH.Adel-
[20] RichardHartleyandAndrewZisserman. Multipleviewge-
son, and Sergey Levine. More than a feeling: Learning to
ometry in computer vision. Cambridge university press,
graspandregraspusingvisionandtouch. RoboticsandAu-
2003. 2,3
tomationLetters(RA-L),2018. 1,2
[21] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
[8] Roberto Calandra, Andrew Owens, Dinesh Jayaraman,
Deep residual learning for image recognition. In Proceed-
JustinLin,WenzhenYuan,JitendraMalik,EdwardH.Adel-
ingsoftheIEEEconferenceoncomputervisionandpattern
son, and Sergey Levine. More than a feeling: Learning to
recognition,pages770–778,2016. 4,5
grasp and regrasp using vision and touch. IEEE Robotics
[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
andAutomationLetters,3:3300–3307,2018. 2
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
[9] BerkCalli,ArjunSingh,AaronWalsman,SiddharthaSrini-
twotime-scaleupdateruleconvergetoalocalnashequilib-
vasa, Pieter Abbeel, and Aaron M Dollar. The ycb object
rium. Advances in neural information processing systems,
andmodelset:Towardscommonbenchmarksformanipula-
30,2017. 5
tionresearch.In2015internationalconferenceonadvanced
[23] Carolina Higuera, Byron Boots, and Mustafa Mukadam.
robotics(ICAR),pages510–517.IEEE,2015. 2
Learningtoreadbraille:Bridgingthetactilerealitygapwith
[10] DaveZhenyuChen,AngelXChang,andMatthiasNießner. diffusionmodels.arXivpreprintarXiv:2304.01182,2023.3,
Scanrefer:3dobjectlocalizationinrgb-dscansusingnatural 4
language.InEuropeanconferenceoncomputervision,2020.
[24] MarkAHoepflinger,CDavidRemy,MarcoHutter,Luciano
3 Spinello,andRolandSiegwart. Hapticterrainclassification
[11] AlexChurch,JohnLloyd,RaiaHadsell,andNathanFLep- forleggedrobots.In2010IEEEInternationalConferenceon
ora.Deepreinforcementlearningfortactilerobotics:Learn- RoboticsandAutomation,pages2828–2833.IEEE,2010. 1
ing to type on a braille keyboard. IEEE Robotics and Au- [25] Micah K Johnson and Edward H Adelson. Retrographic
tomationLetters,5(4):6145–6152,2020. 1 sensing for the measurement of surface texture and shape.
[12] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal- In2009IEEEConferenceonComputerVisionandPattern
ber, Thomas Funkhouser, and Matthias Nießner. Scannet: Recognition,pages1070–1077.IEEE,2009. 2,3
Richly-annotated 3d reconstructions of indoor scenes. In [26] MicahKJohnson,ForresterCole,AlvinRaj,andEdwardH
ProceedingsoftheIEEEconferenceoncomputervisionand Adelson. Microgeometrycaptureusinganelastomericsen-
patternrecognition,2017. 3 sor. ACMTransactionsonGraphics(TOG),2011. 2,3
9[27] Justin Kerr, Huang Huang, Albert Wilcox, Ryan Hoque, [40] AndrewOwens,PhillipIsola,JoshMcDermott,AntonioTor-
Jeffrey Ichnowski, Roberto Calandra, and Ken Goldberg. ralba,EdwardHAdelson,andWilliamTFreeman. Visually
Learning self-supervised representations from vision and indicatedsounds. InProceedingsoftheIEEEconferenceon
touch for active sliding perception of deformable surfaces. computervisionandpatternrecognition,pages2405–2413,
arXivpreprintarXiv:2209.13042,2022. 5 2016. 1
[28] JustinKerr,HuangHuang,AlbertWilcox,RyanHoque,Jef- [41] Senthil Purushwalkam, Abhinav Gupta, Danny M Kauf-
freyIchnowski,RobertoCalandra,andKenGoldberg. Self- man, and Bryan Russell. Bounce and learn: Modeling
supervisedvisuo-tactilepretrainingtolocateandfollowgar- scene dynamics with real-world bounces. arXiv preprint
mentfeatures.InRobotics:ScienceandSystems,2023.1,2, arXiv:1904.06827,2019. 1
3,7 [42] Haozhi Qi, Brent Yi, Sudharshan Suresh, Mike Lambeta,
[29] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo YiMa, RobertoCalandra, andJitendraMalik. Generalin-
Kanazawa,andMatthewTancik. Lerf:Languageembedded hand object rotation with vision and touch. arXiv preprint
radiance fields. In Proceedings of the IEEE/CVF Interna- arXiv:2309.09979,2023. 1
tionalConferenceonComputerVision,2023. 3 [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
[30] Diederik P Kingma and Jimmy Ba. Adam: A method for Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
stochasticoptimization. ICLR,2015. 5 Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
[31] Hendrik Kolvenbach, Christian Ba¨rtschi, Lorenz Well- Krueger, and Ilya Sutskever. Learning transferable visual
hausen, RubenGrandia, andMarcoHutter. Hapticinspec- modelsfromnaturallanguagesupervision,2021. 5,6
tion of planetary soils with legged robots. IEEE Robotics [44] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
andAutomationLetters,4(2):1626–1632,2019. 1 ChelseaVoss,AlecRadford,MarkChen,andIlyaSutskever.
[32] Mike Lambeta, Po-Wei Chou, Stephen Tian, Brian Yang, Zero-shottext-to-imagegeneration.InInternationalConfer-
Benjamin Maloon, Victoria Rose Most, Dave Stroud, Ray- enceonMachineLearning,pages8821–8831.PMLR,2021.
mond Santos, Ahmad Byagowi, Gregg Kammerer, et al. 5
Digit:Anoveldesignforalow-costcompacthigh-resolution [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
tactilesensorwithapplicationtoin-handmanipulation.IEEE PatrickEsser,andBjo¨rnOmmer.High-resolutionimagesyn-
RoboticsandAutomationLetters,2020. 1,2,3,8 thesiswithlatentdiffusionmodels,2021. 2,4
[33] MaximeLhuillierandLongQuan. Aquasi-denseapproach [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
to surface reconstruction from uncalibrated images. IEEE Patrick Esser, and Bjo¨rn Ommer. High-resolution image
transactions on pattern analysis and machine intelligence, synthesis with latent diffusion models. In Proceedings of
27(3):418–433,2005. 2 the IEEE/CVF conference on computer vision and pattern
[34] Hongyu Li, Snehal Dikhale, Soshi Iba, and Nawid Jamali. recognition,pages10684–10695,2022. 5
Vihope: Visuotactilein-handobject6dposeestimationwith [47] Johannes Lutz Scho¨nberger and Jan-Michael Frahm.
shapecompletion. IEEERoboticsandAutomationLetters,8 Structure-from-motion revisited. In Conference on Com-
(11):6963–6970,2023. 1 puterVisionandPatternRecognition(CVPR),2016. 3,4
[35] Yunzhu Li, Jun-Yan Zhu, Russ Tedrake, and Antonio Tor- [48] CarmeloSferrazzaandRaffaelloD’Andrea.Design,motiva-
ralba. Connectingtouchandvisionviacross-modalpredic- tionandevaluationofafull-resolutionopticaltactilesensor.
tion. InProceedingsoftheIEEE/CVFConferenceonCom- Sensors,19(4):928,2019. 2
puterVisionandPatternRecognition, pages10609–10618, [49] NathanSilberman,DerekHoiem,PushmeetKohli,andRob
2019. 1,2,3,6,8 Fergus. Indoor segmentation and support inference from
[36] JustinLin,RobertoCalandra,andSergeyLevine. Learning rgbdimages. InECCV2012,ECCV2012. 3
toidentifyobjectinstancesbytouch:Tactilerecognitionvia [50] LindaSmithandMichaelGasser. Thedevelopmentofem-
multimodalmatching. In2019InternationalConferenceon bodied cognition: Six lessons from babies. Artificial life,
RoboticsandAutomation(ICRA),pages3644–3650.IEEE, 2005. 1
2019. 3 [51] JaschaSohl-Dickstein, EricWeiss, NiruMaheswaranathan,
[37] Antonio Loquercio, Ashish Kumar, and Jitendra Malik. and Surya Ganguli. Deep unsupervised learning using
Learning visual locomotion with cross-modal supervision. nonequilibrium thermodynamics. In International confer-
In2023IEEEInternationalConferenceonRoboticsandAu- enceonmachinelearning,pages2256–2265.PMLR,2015.
tomation(ICRA),2023. 1 2
[38] Gabriel B Margolis, Xiang Fu, Yandong Ji, and Pulkit [52] Sudharshan Suresh, Zilin Si, Stuart Anderson, Michael
Agrawal. Learning physically grounded robot vision with Kaess, and Mustafa Mukadam. Midastouch: Monte-carlo
activesensingmotorpolicies. In7thAnnualConferenceon inferenceoverdistributionsacrossslidingtouch. InConfer-
RobotLearning,2023. 1 enceonRobotLearning,pages319–331.PMLR,2023.1,2,
[39] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, 4
JonathanTBarron,RaviRamamoorthi,andRenNg. Nerf: [53] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li,
Representingscenesasneuralradiancefieldsforviewsyn- Brent Yi, Terrance Wang, Alexander Kristoffersen, Jake
thesis.CommunicationsoftheACM,65(1):99–106,2021.2, Austin, Kamyar Salahi, Abhik Ahuja, David Mcallister,
3 Justin Kerr, and Angjoo Kanazawa. Nerfstudio: A modu-
10larframeworkforneuralradiancefielddevelopment. arXiv
preprintarXiv:2302.04264,2023. 3,5
[54] ShaoxiongWang,JiajunWu,XingyuanSun,WenzhenYuan,
William T Freeman, Joshua B Tenenbaum, and Edward H
Adelson.3dshapeperceptionfrommonocularvision,touch,
andshapepriors. In2018IEEE/RSJInternationalConfer-
enceonIntelligentRobotsandSystems(IROS),2018. 3
[55] Jianxiong Xiao, Andrew Owens, and Antonio Torralba.
Sun3d: A database of big spaces reconstructed using sfm
andobjectlabels. 2013. 3
[56] Fengyu Yang, Chenyang Ma, Jiacheng Zhang, Jing Zhu,
WenzhenYuan,andAndrewOwens. Touchandgo: Learn-
ing from human-collected vision and touch. Neural Infor-
mationProcessingSystems(NeurIPS)-DatasetsandBench-
marksTrack,2022. 2,3,5,8
[57] Fengyu Yang, Jiacheng Zhang, and Andrew Owens. Gen-
erating visual scenes from touch. In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pages22070–22080,2023. 2,3,4,5
[58] FengyuYang,ChaoFeng,ZiyangChen,HyoungseobPark,
DanielWang,YimingDou,ZiyaoZeng,XienChen,RitGan-
gopadhyay,AndrewOwens,andAlexWong. Bindingtouch
toeverything: Learningunifiedmultimodaltactilerepresen-
tations. In Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition,2024. 3
[59] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner,
andAngelaDai.Scannet++:Ahigh-fidelitydatasetof3din-
doorscenes. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pages12–22,2023. 3
[60] Zhao-HengYin,BinghaoHuang,YuzheQin,QifengChen,
and Xiaolong Wang. Rotating without seeing: To-
wards in-hand dexterity through touch. arXiv preprint
arXiv:2303.10880,2023. 1
[61] Wenzhen Yuan, Shaoxiong Wang, Siyuan Dong, and Ed-
ward Adelson. Connecting look and feel: Associating the
visual and tactile properties of physical materials. In Pro-
ceedings of the IEEE Conference on Computer Vision and
PatternRecognition,pages5580–5588,2017. 3
[62] ShuaifengZhi,TristanLaidlow,StefanLeutenegger,andAn-
drewJDavison. In-placescenelabellingandunderstanding
with implicit scene representation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pages15838–15847,2021. 3
[63] Shaohong Zhong, Alessandro Albini, Oiwi Parker Jones,
PerlaMaiolino,andIngmarPosner. Touchinganerf:Lever-
agingneuralradiancefieldsfortactilesensorydatagenera-
tion. InConferenceonRobotLearning, pages1618–1628.
PMLR,2023. 1,2
11