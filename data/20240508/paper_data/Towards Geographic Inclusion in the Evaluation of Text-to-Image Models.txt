TowardsGeographicInclusionintheEvaluationofText-to-ImageModels
MELISSAHALL,MetaAI,UnitedStates
SAMUELJ.BELL,MetaAI,France
CANDACEROSS,MetaAI,UnitedStates
ADINAWILLIAMS,MetaAI,UnitedStates
MICHALDROZDZAL∗,MetaAI,Canada
ADRIANAROMEROSORIANO∗,MetaAI,Mila,McGillUniversity,CanadaCIFARAIChair,Canada
Rapidprogressintext-to-imagegenerativemodelscoupledwiththeirdeploymentforvisualcontentcreationhasmagnifiedthe
importanceofthoroughlyevaluatingtheirperformanceandidentifyingpotentialbiases.Inpursuitofmodelsthatgenerateimagesthat
arerealistic,diverse,visuallyappealing,andconsistentwiththegivenprompt,researchersandpractitionersoftenturntoautomated
metricstofacilitatescalableandcost-effectiveperformanceprofiling.However,commonly-usedmetricsoftenfailtoaccountforthefull
diversityofhumanpreference;oftenevenin-depthhumanevaluationsfacechallengeswithsubjectivity,especiallyasinterpretations
ofevaluationcriteriavaryacrossregionsandcultures.Inthiswork,weconductalarge,cross-culturalstudytostudyhowmuch
annotatorsinAfrica,Europe,andSoutheastAsiavaryintheirperceptionofgeographicrepresentation,visualappeal,andconsistency
inrealandgeneratedimagesfromstate-of-theartpublicAPIs.Wecollectover65,000imageannotationsand20surveyresponses.
Wecontrasthumanannotationswithcommonautomatedmetrics,findingthathumanpreferencesvarynotablyacrossgeographic
locationandthatcurrentmetricsdonotfullyaccountforthisdiversity.Forexample,annotatorsindifferentlocationsoftendisagree
onwhetherexaggerated,stereotypicaldepictionsofaregionareconsideredgeographicallyrepresentative.Inaddition,theutility
ofautomaticevaluationsisdependentonassumptionsabouttheirset-up,suchasthealignmentoffeatureextractorswithhuman
perceptionofobjectsimilarityorthedefinitionof“appeal”capturedinreferencedatasetsusedtogroundevaluations.Werecommend
stepsforimprovedautomaticandhumanevaluations.Thisincludescollectingannotationsfrompeoplelocatedinsideandoutsidethe
regionofinterest,instructingannotatorsonwhethertheyshouldfollowspecificdefinitionsofevaluationcriteriaorutilizetheirown
interpretation,andreportingassumptionsunderlyingautomaticevaluations.
CCSConcepts:•Socialandprofessionaltopics;•Appliedcomputing;•Computingmethodologies→Computervision;
AdditionalKeyWordsandPhrases:text-to-imagegeneration,geography,evaluation
ACMReferenceFormat:
MelissaHall,SamuelJ.Bell,CandaceRoss,AdinaWilliams,MichalDrozdzal,andAdrianaRomeroSoriano.2024.TowardsGeographic
InclusionintheEvaluationofText-to-ImageModels.InThe2024ACMConferenceonFairness,Accountability,andTransparency(FAccT
’24),June3–6,2024,RiodeJaneiro,Brazil.ACM,NewYork,NY,USA,22pages.https://doi.org/10.1145/3630106.3658927
1 INTRODUCTION
Recentyearsbroughtunprecedentedprogressingenerativemodelsforvisualcontentcreation,withworksachieving
impressivelyphotorealisticimagegenerations[18,35,43,46].However,itisimportanttounderstandwhetherthetrue
∗Equalcontributioninresearchandengineeringleadership.
Permissiontomakedigitalorhardcopiesofpartorallofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenot
madeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirstpage.Copyrightsforthird-party
componentsofthisworkmustbehonored.Forallotheruses,contacttheowner/author(s).
©2024Copyrightheldbytheowner/author(s).
ManuscriptsubmittedtoACM
1
4202
yaM
7
]VC.sc[
1v75440.5042:viXraFAccT’24,June3–6,2024,RiodeJaneiro,Brazil Halletal.
(a)Task1 (b)Task2
Fig.1. Wecollect65kannotationsperformedbypeoplelocatedinAfrica,Europe,andSoutheastAsiacorrespondingtoevaluation
criteriaoftext-to-imagemodelsincludinggeographicrepresentation,similarity,visualappeal,andobjectconsistencyinreal
andgeneratedimages.Wedeveloprecommendationsforimprovedhumanandautomaticevaluationsoftext-to-imagemodels.
diversityoftherealworldisconveyedingeneratedimages.Inparticular,generatedimagesshouldberepresentativeof
theworld,capturingthemyriadvariabilityofpeople,objects,andscenesacrossgeographicregions,whilealsovisually
appealingorinteresting,andconsistent withtheinputtextdescription.Assessingthesepropertiesiscrucialforan
in-depthunderstandingoftheperformanceandpotentialbiasesoftext-to-imagesystems.
Commonlyusedautomatedmetrics,suchastheFréchetInceptionDistance(FID)[25],InceptionScore[49]and
precisionandrecall(PR)-basedmetrics[29,34,44,48,52],alreadyexistforevaluatingtherepresentativenessofgenerated
images.OthermetricssuchasLPIPS[56]andVendiScore[17]targetdiversitywithinacorpusofgeneratedimages
explicitly.MetricssuchasCLIPScore[23]havealsobeenintroducedtoautomaticallyevaluatetheconsistencyof
generatedimageswithrespecttotheinputtextdescription.Recently,severalmetricshavebeenextendedtomeasure
theabilityoftext-to-imagesystemstocreatefaithfuldepictionsofobjectsaroundtheglobe[22].
However,thesemetricshavenotablechallengesandareundergoingdebatesabouttheirrobustness[32,54].For
example,theaforementionedmetricsrelyonpre-trainedfeatureextractors,suchasInceptionv3[55],whichcanleadto
unreliablemodelrankingsandsusceptibilitytoundesirable“fringefeatures”[28].FIDandPR-basedmetricsrelyon
referencedatasetstodefinerepresentativenessthatmaynotcapturethetruediversityoftherealworld.Thesemetrics
operationalizerepresentationsinaconstrainedwayandfailtoaccountforthediversityofhumanpreferences[40].
Giventhecomplexityandcontestednessofautomaticmetrics,humanevaluationsremainthegoldstandardtobench-
marktext-to-imagesystems.Humanevaluationsoftenfocusonside-by-sidecomparisonsofimagestakenfromdifferent
sources[10,35,43,46]ortaskhumanauditorswithidentifyingconceptsorgroupinformationinanimagebasedon
instructionalcriteria[5].However,humanevaluationsalsofacechallenges.Mostnotably,theystruggletoaccountfor
thesubjectivityacrossregionsandcultures.Recentstudiessuggestthathumanevaluationstendtobesomewhatadhoc,
withtaskdesignimpactingresultsandleadingtohighvarianceinestimates[57].Furthermore,annotationsofmodelper-
formanceandbiaspresupposedefinitionsthatmaynotencapsulatethefullrangeofhumanperceptions,maybemutually
incompatiblebetweendifferentgroupsofpeople,andmaynotmaptowhatthemodelcreatorisgenuinelyinterestedin.
2TowardsGeographicInclusionintheEvaluationofText-to-ImageModels FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
EvaluationCriteria Definition AutomaticMetric OurAnnotatorData
Geographicrepresentationofobjects:
Whether,forageographicregion,
Precision/Coverage "Whichregionshavesimilarobjects?"
generatedsamplesresemblethe
Realism/Diversity
realworld/capturethevariability
Region-CLIPScore Geographicrepresentationofbackgrounds:
oftherealworld
"Whichregionshavesimilarbackgrounds?"
Similarity:
Whethertwoimagesclosely
Similarity Distance "Whichimagehasanobjectmore
resembleeachother
similartothereferenceimage?"
Whetherimageshavevisual Visualappeal:
Visualappeal Precision
attractivenessorinterest "Whichimageismoreappealing?"
Whetherimagesshowelements Objectconsistency:
Objectconsistency Object-CLIPScore
ofpromptusedingeneration "Istheobjectshown?"
Table1. Westudygeographicrepresentation(encapsulatingrealism/diversityandsimilarity),visualappeal,andobjectconsis-
tency.Thetablesummarizestheirdefinitions,metricsusedintheirapproximation,andrelevantannotationdatathatwecollected.
Inthispaper,wepresentahumanstudytounderstandhowannotatorsconceptualizedesirablepropertiesoftext-to-
imagesystemsacrossgeographiclocations.Wefocusonimagesofobjects,asnotabledisparitieshavebeenobserved
betweengeographicregionsinbothtext-to-imagegenerativesystems[5,6,22]anddiscriminativemodels[12,21,45,53].
Wecollectover65,000annotationswith20qualitativesurveyresponsestostudyhowpeopleinAfrica,Europe,and
SoutheastAsiavaryintheirperceptionofgeographicrepresentation,visualappeal,andconsistency.Thestudyincludes
bothreal,geographicallydiverseimagesandimagescreatedbytwostate-of-the-artgenerativesystemsthatdepict
sixtypesofcross-culturallycommonobjects.Wecontrasttheresultsofthehumanannotationswithcommonlyused
automatedmetrics.Throughourstudy,wefindthat:
• Perceptionsofgeographicrepresentation,appeal,andconsistencyvaryacrossannotatorlocations.
• Annotatorscanreinforcegeographicstereotypesandcontradicteachotherintheirinterpretationofappeal.
Preferredqualitiesincluderealism,foregrounding,exaggeratedbackgrounds,boldcolors,andluxuriousobjects.
• Annotatorsvaryinhowmuchtheyfollowconcretedefinitionsorapplytheirsubjectiveinterpretationwhen
completingtasks.Somerelyonpersonalknowledgewhileothersutilizeexternalreferencesources.
• CLIPandDINOv2featureextractorscorrelatebetterwithhumanjudgmentofsimilaritythanInceptionv3.
• AlthoughCLIPScoreishighlyreliableinassessingthepresenceofanobjectinanimagewhenusedwitha
threshold,itisgenerallyunreliableforassessinggeographicrepresentation.
Inaddition,werecommendstepstoimproveevaluationsoftext-to-imagegenerativemodels.Tostrengthenhuman
evaluations,wesuggestclearlyinstructingannotatorsaboutwhethertheyshouldattempttocapturetheirsubjective
perspectiveortofollowspecifieddefinitions,andwhethertheyshouldleverageexternalresourcesorrelyonlyontheir
existingknowledge.Inaddition,wesuggestcollectingperspectivesfrombothin-regionandout-of-regionannotatorsto
capturemorebreadth.Tostrengthenautomaticevaluations,wesuggestselectingreferencedatasourceswiththoughtful
considerationoftheinterpretedcriteriaandusingmoremodernfeatureextractorsthatbettercaptureimagequalities
relevantfortheevaluation,suchasshape.Whenreportingevaluations,avoid“majority-vote”typesofaggregations
thatreduceuncertaintytobinaryindicatorsofevaluationcriteria,andcommunicateclearlytheassumptionsofcriteria
definitionsusedinthemeasurementprocess.
Wehopethatthisworkstrengthensevaluationsofgeographicrepresentationintext-to-imagegenerativemodels
andpavesthewaytowardstext-to-imagesystemsthattrulyworkforeveryone.
3FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Halletal.
2 EXPERIMENTALSET-UP
Weoutlineevaluationcriteriafortext-to-imagemodelsanddescribeourannotations,summarizedinTable1.
2.1 EvaluationCriteriaforText-to-ImageGenerativeModels
Wediscussevaluationcriteriafortext-to-imagemodelsandautomaticmetricsusedfortheirapproximation.Inlater
analyses,weencapsulatetherealism/diversityandsimilaritycriteriaintoanoverallfocusofgeographicrepresentation.
Wealsostudyimagevisualappealandobjectconsistency.
2.1.1 Realism/Diversity. Realismisthedegreetowhichgeneratedsamplesresembletherealworld,anddiversityis
thedegreetowhichsamplescapturethefullvariabilityoftherealworld[22,29,48].Priorworks[5,22]haveutilized
realismanddiversityintheassessmentoftheextenttowhichthereal-worldvariabilityandnuanceofagivenregionis
depictedinacorpusofgeneratedimages.Wecallthis“geographicrepresentation.”
Automaticmetrics.Imagerealismanddiversityareoftenmeasuredwithprecisionandcoverage,respectively[22,29,48].
Precisionhighlightswhethergeneratedimageslooksimilartorealimages,whilecoveragequantifieswhethergenerated
imagescapturethebreadthofvariationinrealimages.Specifically,precisionmeasurestheproportionofgenerated
imagesthatliewithinthemanifoldofreal,referenceimages.Coveragemeasurestheproportionofrealimagesthathave
generatedimagesnearby.InSection3,weanalyzethevulnerabilityofthesemetricstopredefinedfeatureextractors
andreferencemanifoldsthroughthecontextofsimilarity,whichweintroducenext.Inaddition,thedistancesi.e.,
cosinesimilarity,betweentheCLIPtextembeddingcorrespondingtogrouptermsandembeddingofageneratedimage
hasbeenusedtoidentifydemographicgroups[9]andgeographicrepresentation[5].WeusetheCLIPScore[23]with
regioninformation(“Region-CLIPScore”)asanindicatorofgeographicrepresentation,analogouslytoLeeetal.[31].
2.1.2 Similarity. Whilesimilarity,i.e.,whethertwoimagescloselyresembleeachother,isnotcommonlyevaluatedin
text-to-imagegenerativemodels,itisthefoundationofcriterialikerealismanddiversity.Foranobjectinanimageto
have“realism,”itneedstoresembleinstancesoftheobjectintherealworld.Foradatasetofimagestodisplayenough
“diversity,”theobjectsintheimagesmustmeaningfullydifferfromeachother.
Automaticmetric.Measurementsofsimilarityarecloselyrelatedtoautomaticevaluationsofrealismanddiversity,as
theyapproximaterelativedistancesbetweenimagesinapredefinedfeaturespace.InSection3,westudyhowtheolder,
moreubiquitousInceptionv3[55]comparestothemorerecentCLIPViT-B/32[41]andDINOViT-L/14[36]feature
extractorstrainedonlargerdatasourcesinapproximatinghumanperceptionsofsimilarity.
2.1.3 VisualAppeal. Visualappealcorrespondstowhetherimageshavevisualattractivenessorinterestandhas
becomeanincreasinglycommonevaluationcriteria[10,38,47].Westudyhowautomaticmetricscapturesubjectivity
inappealacrossgeographicregions.
Automaticmetric.Accordingtosomepreviousworks,appealcanbeautomaticallymeasuredbycomparingtoamanifold
ofrealimageswiththeprecisionmetric[22,29].InSection3,westudywhethergeneratedimagesthatfallinthereal
imagemanifoldalignwithhumanperceptionsofvisualappeal.
2.1.4 ObjectConsistency. Consistencycorrespondstowhetheranimageincludesallcomponentsofitsprompt.This
relatestovisualconcreteness[7]andtherelationshipbetweenaconcept’smeaninganditshuman-perceptibleform[37].
Per[22],wefocusonobjectsincludedintheprompt,e.g.,whetheranimagegeneratedwith“carinAfrica”depictsacar.
4TowardsGeographicInclusionintheEvaluationofText-to-ImageModels FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
(a)Bag (b)Car
(c)Cookingpot (d)Dog
(e)Plateoffood (f)Storefront
Fig.2. RandomexamplesofrealimagesfromGeoDEineachregionandgeneratedimagesfromDMw/CLIPandLDM2.1usingthe
prompt{object} in {region}.ThefirsttwocolumnscorrespondtoAfrica,thenexttoEurope,andthelasttoSoutheastAsia.
Automaticmetric.InSection3weexploretheCLIPScore[23],whichcanbeusedtomeasuretheinput-outputconsistency
ofthegenerativemodel[22,31,47].Following[22],weusethetextembeddingcorrespondingtotheobjectthatshould
bepresentintheimageandrefertothemetricas“Object-CLIPScore.”
2.2 Annotations
Wenowdescribetheannotationswecollectedtobetterunderstandhowhumansindifferentgeographiclocationsvary
intheirperceptionoftheaforementionedevaluationcriteriafortext-to-imagemodels.
2.2.1 Images. OurtasksincluderealimagesfromGeoDE[42],adiverse,geographicallyrepresentativedatasetof
imagestakenacrossmultipleregions.Eachimagecontainsanobjectfillingatleast25%oftheimageandoccursin
everydaysettings.Wealsoincludegeneratedimagesfromamulti-modalimplementationofagenerativepre-trained
transformerleveragingCLIPimageembeddings.Thismodel,whichwecall“DMw/CLIP”,hasapproximately3.5billion
5
EDoeG
PILC
/w
MD
1.2
MDL
EDoeG
PILC
/w
MD
1.2
MDL
EDoeG
PILC
/w
MD
1.2
MDL
EDoeG
PILC
/w
MD
1.2
MDL
EDoeG
PILC
/w
MD
1.2
MDL
EDoeG
PILC
/w
MD
1.2
MDLFAccT’24,June3–6,2024,RiodeJaneiro,Brazil Halletal.
parameters[43].Wealsousealatentdiffusionmodeltrainedonapublicdatasetofapproximately5billionimages,
excludingexplicitmaterial[46],whichwerefertoas“LDM2.1.”Weusepaidversionsofbothmodelsandgenerate
imagesusingtheprompts“{object}”and“{object} in {region}”tocapturerepresentationsacrossregions.
Wefocusonsixobjects(bag,car,cookingpot,dog,plateoffood,andstorefront)thatexistacrossregions[42]and
havebeenusedtoauditgenerativeanddiscriminativemodels[4,12,22,45].WeincludetheregionsAfrica,Europe,
andSoutheastAsia.Generatedimagesoftheseobjectsvaryintheirgeographicrepresentationandconsistency[22],as
showninrandomlyselectedimagesinFigure2.Forexample,generatedimagesofcar,cookingpotandstorefrontoften
depictgeographicstereotypesnotseenintherealdataset,suchasrudimentaryinfrastructure,low-incometools,and
vibrantcolorsforAfricaandSoutheastAsia.Similarly,generatedimageshaveexaggeratedbackgrounds,withruraland
cobblestone/alley-filledscenesforAfricaandEurope,respectively,thatarerarelydepictedinGeoDE.Furthermore,
LDM2.1showsconsistencyissueswithintendedobjectslikebag,car,andcookingpotoftenleftoutofgeneratedimages.
2.2.2 Tasks. Ourstudycomprisestwomainannotationtasksthatwepopulatewithdifferentimagecombinations,
balancedacrossobjectsandregions.Figure1showsexamplesoftheuserinterfaceforTask1andTask2.
Task1:ImageComparison&ObjectConsistency.Thefirsttask,inFigure1a,focusesonunderstandingobjectconsistency
andhumanperceptionsofimagesimilarityandappeal.AswithclassicalABXtesting[33],wedisplaytripletsofa
singlereferenceimageandtwoimagesforcomparison,onerealandonegenerated.Theannotatorisaskedtoindicate
whethertheintendedobjectisshownineachimage,providinginsightintotheirperceptionofobjectconsistency.Then,
theyareaskedtoselectwhichofthetwocomparisonimagesshowsanobjectmoresimilartotheobjectinthereference
imageandtoprovideaconfidencescoretotheiranswer.Thisallowsustomeasurepotentialvariationsinannotator
perceptionofsimilarity.Finally,annotatorsdecidewhichcomparisonimageismorevisuallyappealing.
ForTask1,imagesinagiventriplethavetheregionincommonoraregeneratedwiththeobjectprompt.Generated
imagesinatripletarefromthesamemodel.Acrosstriplets,thecomparisonimagesvaryintheirdistancefromeach
otherandfromthereferenceimagesintheInceptionv3featurespace.WeuseInceptionv3asitiswell-establishedwhen
assessingperformanceofgenerativemodels[8,24,43,46](andwestudyadditionalfeatureextractorsinouranalysis
ofsimilarity).Wedefinethetripletdistanceas||𝑓(𝐼 𝑟𝑒𝑓)−𝑓(𝐼 𝑓𝑎𝑟)||2−||𝑓(𝐼 𝑟𝑒𝑓)−𝑓(𝐼 𝑐𝑙𝑜𝑠𝑒)||2,where𝑓 representsthe
featureextractor,𝐼 isthereferenceimage,𝐼 istheclosercomparisonimagetothereferenceintheembedding
𝑟𝑒𝑓 𝑐𝑙𝑜𝑠𝑒
space,and𝐼 isthefurtherimage.Ahighertripletdistancemeansthatonecomparisonimageismuchfurtherfrom
𝑓𝑎𝑟
thereferenceimagethantheothercomparisonimage,suggestingitismuchmore“similar”tothereferenceimage.
Foreverymodel,wecompile425tripletsforeachobject-regioncombination,yielding15,300triplets.AppendixA.1
containsadditionaldetailsaboutthetriplets.
Task 2: Geographic Representation & Object Consistency. The second task, in Figure 1b, focuses on understanding
consistencyandgeographicrepresentationofobjectsandscenesinimages.Asingleimageisshown,andannotators
areaskedtoindicateiftheintendedobjectispresent.Ifso,theyarethenaskedtoselectanyregionfromapre-specified
listthattheyfeeltheobjectcouldbefrom,i.e.,thattheyfeelhasobjectsfromthesamecategory(e.g.“bag”,“car”)that
looksimilartotheobjectintheimage.Theyarealsoaskedthesamequestionaboutthebackgroundscene/context
depictedintheimage.Thesequestionscorrespondtoannotatorperceptionofgeographicrepresentation.
ForTask2,werandomlyselect100imagesfromGeoDEforeachregion-objectcombination.Inaddition,forLDM2.1
andDMw/CLIP,weused100imageswithgenerated{object} in {region}foreachobject-regioncombinationand
100imagesgeneratedwith{object}foreachobject.Thisyields6,600images.
6TowardsGeographicInclusionintheEvaluationofText-to-ImageModels FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
2.2.3 Annotators. Eachtaskisannotatedbythreepeoplefromourpool:onepersonlocatedinAfrica,Europe,and
SoutheastAsia.Theannotatorpoolconsistsof15-25peopleineachregion.Afterbothtasksarecompleted,wefilterout
approximately13%ofTask1annotationsand2%ofTask2annotationsthatdonotmeetqualitychecksonrealimages.
Theremainingannotationsyieldthecorpuswithwhichweperformouranalyses.AppendixA.1describestheannotator
trainingprocessandcompensation,qualitycriteria,andstatisticsaboutannotatorbackgroundandjobexecution.
2.3 AnnotatorSurvey
Onceallannotationswerecollected,wealsoelicitedvoluntarysurveyresponsestolearnmoreabouthowannotators
interpretedquestionsrelatedtogeographicrepresentation.Thefirstsetofquestionsfocusesonthedepictionofobjects.
Weaskthreeversionsofthequestion“Whenperformingthesetasks,howdidyoudetermineifanobjectlookedlikesimilar
objectsin[region]?Ifpossible,pleaseprovidespecificexamples,” whereweconsiderAfrica,Europe,andSoutheastAsia.
Wealsoask,“Didyouevermarkthatnoneoftheregionshadsimilarobjectsastheonesshownintheimage,orthatyou
couldnotdetermine?Ifso,whatpromptedyoutomakethatdecision?” Forthesecondsetofquestions,weaskedthe
samequestions,butaboutthe“backgroundscene/context” oftheimage.Wereceived23responsestotheannotator
survey,from5annotatorsinAfrica,8inEurope,and10inSoutheastAsia.Weappliedaninductivecodingapproachto
categorizethemesdiscussedbytheannotators,resultingin16descriptivecodes.Afteraninitialroundtovalidatethe
codingscheme,responseswereindividuallycodedbythreeoftheauthors.Webinarizethepresenceofeachcodeper
responseandevaluateinter-coderagreementusingFleiss’Kappa[16].Wediscardcodeswithacoefficientlessthan
0.6toensureconsistentdeploymentbycoders.Finalcodeswereassignedviaamajorityvoteoverthethreeauthors,
andresponsesweregroupedaccordingtowhethertheycorrespondtobackgroundsceneorobject.Allcodesandtheir
Fleiss’KappacoefficientscanbefoundinAppendixTable3.
3 RESULTS
Foreachcriteria,wefirstpresentanalysescorrespondingtoHumanInterpretations,leveragingonlyannotationand
surveydata(whererelevant).ThenwediscusstheHuman&AutomaticMetricInteraction,whereweanalyzethe
extenttowhichautomaticmetricscapturehumaninterpretationofevaluationcriteria.Alongtheway,wesuggest
recommendationsforimprovedhumanandautomaticevaluationsoftext-to-imagemodelsbasedonourfindings.
3.1 GeographicRepresentation
3.1.1 HumanInterpretations:Realism/Diversity. Wefirstanalyzehowperceptionsofgeographicrepresentationin
objectsandbackgroundsdepictedinimagesdifferbasedonwheretheannotatorislocated.InFigure3a,wefocus
onrealimagesthatweretakeninaspecificregionandgeneratedimagescreatedwithpromptsmentioningthat
region.Wecompareperceptionsofgeographicrepresentationbetweenin-regionandout-of-regionannotators.We
findthatannotatorslocatedinthesameregionasarealimagetendtoidentifyboththeobjectandthebackgroundas
representativeofthatregionmoresothanannotatorslocatedinotherregions.However,perceptionsbetweenin-region
andout-of-regionannotatorsforgeneratedimagesaremoremixed.
Tobetterunderstandwhyin-andout-of-regionannotatorsdisagree,weincludeexampleimagesinFigure4.In-region
annotatorsoftenconsiderobjectsandbackgroundswithfewgeographically-distinguishingfeaturesasrepresentative
morethanout-of-regionannotators.Forexample,thisisthecaseforrealimagesdepictingbagswithoutgeographically-
associatedpatterns(Figure4a).Itisalsotrueofgeneratedimagesofcarsanddogswithbackgroundsdominatedby
7FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Halletal.
(a)Prop.ofimgsw/geographicrep. (b)Geographicrepresentationinbackgroundvs.Region-CLIPScore
Fig.3. (a)Proportionofimagesthatin-regionandout-of-regionannotationsconsiderdepictinggeographicrepresentationfor
objectsandbackgrounds.(b)RelationshipbetweenRegion-CLIPScoreandannotatordesignationofgeographicrepresentation.The
x-axisshowstheaverageCLIPScoreforallimageswithinabucketwithsize0.01.They-axisshowstheproportionofimageswhere
annotatorssaidtheobjectwaspresent.Weinclude95%confidenceintervalsgeneratedviabootstrapping.Annotatorperceptions
ofgeographicrepresentationdifferaccordingtowhethertheannotatorsarelocatedintheregionoffocusoroutsideit.
Region-CLIPScoredoesnotalwayscapturevariationsinperceivedgeographicrepresentationacrossannotatorlocation.
Theme Quote Re:Region
Builtenvironment R5:“Thetypesofhousesarereallydistinct” Africa
Culture,art&religion R6:“distinctivereligiousarchitecturesuchasBuddhisttemples SoutheastAsia
ormosques”
Absenceofdetail R11:“simplygroundortilefloorwithinternationaldesign” NA
Externalsearch R16:“SometimesIusedGooglepicturetosearchthephoto” Africa
Personallivedexperience R5:“I’mfamiliarwithEuropeanenvironmentssoIcompareit Europe
tothem”
Nature&naturalworld R6:“Presenceoftropicalvegetationsuchaspalmtreesor SoutheastAsia
bamboo”
People R7:“Blackandbrownpeopleinthebackground” Africa
Stereotypedrepresentations R19:“Ifplaceisclassyorelegant” Europe
Stereotypedrepresentations R7:“houseswerenotdevelopedandtoolssurroundingthe Africa
objectswererudimental”
Stereotypedrepresentations R6:“traditionalhouseslikestilthouses” SoutheastAsia
Table2. Selectedsurveyresponsesaboutgeographicrepresentationforthemesthatsatisfyourinter-annotatoragreementthreshold.
“R[N]”indicatesrespondentID.“Re:Region”indicatestheregionspecifiedinthequestion.Annotatorsdeploypositiveand
negativestereotypesandusepersonalexperienceandexternalresourceswhenconsideringgeographicinformationin
images.
agnosticoutdoorstreets(Figure4b).Meanwhile,whengeneratedimagesshowstereotypicalobjects,suchassimplecook-
ingpotsforSoutheastAsiafromLDM2.1andsaucypastasand“meat-and-potato”mealsforEuropefromDMw/CLIP
(asinFigure4c),out-of-regionannotatorsoftenconsidertheimagesmorerepresentativethanin-regionannotatorsdo.
Furthermore,stereotypicallyruralscenesandrudimentaryinfrastructurearecommoninimagesofAfricaandSoutheast
Asiathatout-of-regionannotatorsconsidergeographicallyrepresentativeandin-regionannotatorsdonot(Figure4d).
Theresultsofthequalitativesurvey(seeTable2forasummary)alsosuggestanover-relianceonstereotypical
imagefeaturesamongout-of-regionannotators,e.g.,theexpectationthatEuropeanbackgroundsbe“classyorelegant”
(respondentR19,locatedinSoutheastAsia),orthatAfrican“houseswerenotdeveloped”(R7,locatedinEurope).Wefind
8TowardsGeographicInclusionintheEvaluationofText-to-ImageModels FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
(a)In:Objisrepresentative (b)In:Backisrepresentative (c)In:Objisnotrepresentative (d)In:Backisnotrepresentative
Out:Objisnotrepresentative Out:Backisnotrepresentative Out:Objisrepresentative Out:Backisrepresentative
Fig.4. Examplesofdisagreementbetweenin-andout-of-regionannotatorsaboutgeographicrepresentationofobjectsandbackgrounds.
Out-of-regionannotatorstendtoconsiderstereotypesrepresentativemorethanin-regionannotators.
thatannotatorsreportusingarangeofimagefeaturesindetermininggeographicrepresentationinbothbackground
sceneandobject,withagreaterproportionofEurope-basedrespondentsrelyingonfeaturesincludingbuildings,nature,
andpeoplethanannotatorsbasedinotherlocations.Inaddition,whenaskedaboutthegeographicrepresentationof
objectsandbackgroundscenes,annotatorsfrequentlyrefertocommonstereotypeswhenexplainingtheirjudgments,
suchas“stilthouses”forSoutheastAsia.Finally,someannotatorsreportedusingexternalreferencessuchas“Google
picturetosearchthephoto,”whichcanintroduceadditionalbiasesfromonlinewebdata.Wequantifythesefindings
accordingtoannotatorlocationintheAppendix(Figure9).
Recommendation:Whenannotatingforgeographicrepresentation,includein-regionandout-of-regionannotators:
In-regionannotatorstendtoindicaterealistic,visualqualitiesrepresentativeofaregionwhileout-of-regionannotators
oftenindicateunique,exaggeratedidentifiersofaregion.Contrastingtheseperspectivescanilluminateregional
stereotypingingeneratedimages.
3.1.2 HumanInterpretations:Similarity. Wealsostudyhowannotatorsvaryintheirassessmentofimagesimilarity.
Figures5aand5bshowexamplesofimagetripletsforwhichtheannotatorsagreeanddisagree,respectively,onwhich
comparisonimageismoresimilartothereferenceimage.Wefindthatfactorsinperceptionsofobjectsimilarityinclude
color,size,type(e.g.,dogbreed),andcameraangle.
Inaddition,inFigure5c,westratifyratesofannotatoragreementinsimilaritybythesourceofthetripletimagesand
theregionwheretheimagewastaken/promptedwith.Wedonotfindthatannotatorsagreeonsimilaritymoreconsis-
tentlyforanyimageregion.However,annotatorsagreetheleastwhendeterminingwhichDMw/CLIPcomparisonim-
agesaremoresimilartoaGeoDEreferenceimage.Butwhenthisisswapped,andtheyareselectingbetweentwoGeoDE
imagescomparedtoasingleDMw/CLIPimage,theyhaveamuchhigherrateofagreement.Wehypothesizethatthis
asymmetrymaybeduetotherelativelylowconditionaldiversitybetweentheDMw/CLIPimagesascomparedtoGeoDE
(observedinFigure2):Selectingbetweenlessdiverseimagescanmakeagreeingonthemoresimilarimagechallenging,
whereastwoverydifferentcomparisonimagesmayallowforeasierconsensusaboutwhichismoresimilar.Suchvaria-
tioninannotatorperceptionrecallspriorworksuggestingalternativestomajorityvoteaggregationsofannotations[11].
Recommendation:Collectmultipleannotationsforimagesimilarity.Aggregationslikemajority-votemaymask
thestrengthofdisagreementbetweenannotatorsorobfuscatefactorsrelevanttotheminorityperspective.
9
EDoeG
PILC
/w MD
1.2
MDL
gab
:jbO
rac
:jbO
tnorferots
:jbO
eporuE
:geR
aisA
tsaehtuoS
:geR
acirfA
:geR
EDoeG
PILC
/w MD
1.2
MDL
top
gnikooc
:jbO
rac
:jbO
god
:jbO
acirfA
:geR
eporuE
:geR
aisA tsaehtuoS
:geR
EDoeG
PILC
/w MD
1.2
MDL
gab
:jbO
doof
fo
etalp
:jbO
top gnikooc
:jbO
acirfA
:geR
eporuE
:geR
aisA tsaehtuoS
:geR
EDoeG
PILC
/w MD
1.2
MDL
gab
:jbO
god
:jbO
rac
:jbO
eporuE
:geR
acirfA
:geR
aisA tsaehtuoS
:geRFAccT’24,June3–6,2024,RiodeJaneiro,Brazil Halletal.
(a)Consistentsimilarity. (b)Inconsistentsimilarity (c)Similarity
Fig.5. (a-b)Qualitativeexamplesofconsistentandinconsistentannotatorperceptionsofsimilarity,where(a)depicts(fromleft):
referenceimagefromGeoDEandcomparisonimagesfromDMw/CLIPdesignatedasmoreandlesssimilarbyallthreeannotators
and(b)depicts(fromleft):referenceimagefromGeoDEandtwocomparisonimagesfromDMw/CLIPwithinconsistentsimilarity
annotations.(c)Ratesofannotatoragreementinsimilarity.Variationsinperceptionofobjectsimilaritycandependonthe
diversityoftheimages.Factorsinperceptionofsimilarityincludecolor,size,type(e.g.,dogbreed),andcameraangle.
3.1.3 Human&AutomaticMetricInteraction:Region-CLIPScore. WenextanalyzehowRegion-CLIPScoreapproximates
perceptionsofgeographicrepresentation.HigherCLIPScorevaluesindicatestrongersimilaritybetweentheimage
embeddingandcorrespondingtextembedding.Foreachregion,wecalculatetheRegion-CLIPScoreforallimagesand
compareittotheproportionofimagesthatannotatorsdesignateasrepresentative.ResultsareshowninFigure3b.
WefindthatRegion-CLIPScoretendstocorrespondtoalargerproportionofrealimagesperceivedasrepresentative
ofthatregionbyin-regionannotatorsthanout-of-regionannotators.Furthermore,Region-CLIPScorehasalarger
disparityinitsapproximationofgeographicrepresentationacrossannotatorlocationswhenusedforidentifying
SoutheastAsiarepresentationthanAfricaandEurope.Inaddition,Region-CLIPScorescanbe2−5pointshigherfor
generatedimagesofAfricaandSoutheastAsia,whichtendtoshowstrongregionalstereotyping,thanforEuropeand
realimages.ThissuggeststhatRegion-CLIPScoreissusceptibletostereotypicalrepresentationsofregions.
Recommendation:AvoidRegion-CLIPScoreoruseitwithcaution:itdoesnotcapturevariationinperceptionof
geographicrepresentationbetweenannotatorlocationsandmayfavorstereotypesoverrealisticimages.
3.1.4 Human&AutomaticMetricInteraction:Distance. Wenextstudythealignmentbetweenannotatorperceptionof
similarityandrelativedistancesbetweenreferenceandcomparisonimagesusingdifferentfeatureextractors,1with
resultsshowninFigure6.Weconsideragoodfeatureextractoronethathasahighrateofalignmentbetweenthe
comparisonimagethatisclosertothereferenceimageinthefeaturespaceandtheimagethatannotatorsconsider
moresimilaracrossalltripletdistances,indicatingthatitcorrelateswellwithhumanjudgment.
Forallmodels,thechancethatannotatorsselecttheclosercomparisonimageasthemoresimilaroneiscloseto
randomforlowtripletdistancevalues.However,theprobabilityofalignmentgrowsasthetripletdistanceincreases,
andannotatorsalmostalwaysidentifytheimagethatisclosertothereferenceimageasthemoresimilaroneatlarge
1WecorrectforthebiasofusingInceptionV3distanceswhenselectingimagesforannotationsbysubsamplingtotripletdistancesforwhichthereexist
samplesforallfeatureextractors.Wenormalizetheplotbythemaximumtripletdistanceandrescalethevaluestocomparedifferentfeatureextractors.
10TowardsGeographicInclusionintheEvaluationofText-to-ImageModels FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
(a)InceptionV3 (b)CLIP (c)DINOv2
Fig.6. Frequencythatannotatorsconsidertheclosercomparisonimagetothereferenceimageashavingthesimilarobjectasa
functionoftripletdistance.CLIPandDINOv2bestreflectannotatorperceptionsofobjectsimilarityacrossalllocations.
(a)Agreementonvisualappeal (b)Relationshipb/wappeal&similarity (c)Appealingimginrealmanifold
Fig.7. (a)Annotatoragreementinvisualappeal.(b)Co-occurrenceofappealandsimilarity.(c)Proportionofgeneratedappealing
imagesthatfallintheGeoDEmanifold.Annotatorsagreeonappeallessthansimilarityandatdifferentratesbetween
depictedregions.Theymayleveragequalitiesforappealbeyondsimilaritymorewhentheyhavegreaterfamiliaritywith
whatisintheimage.Themajorityofappealingannotationsofgeneratedimagesfallinthemanifoldofrealimages.
distances.BothDINOv2andCLIPfeaturescorrelatesignificantlybetterwithhumanjudgementthanInceptionV3,with
DINOv2featuresareslightlymoreassociatedwithhumanjudgmentthanCLIPfeaturesatlowdistancevalues.Thisis
trueforallimageregionsandannotatorlocations,suggestingthereislittleeffectofgeographyinassessingsimilarity.
Recommendation:UsemodernfeatureextractorslikeCLIPandDINOv2forautomaticmeasuresofobjectsimilarity.
3.2 VisualAppeal
3.2.1 HumanInterpretations. Tounderstandannotatorsubjectivityindeterminingvisualappeal,wemeasureannotator
agreementindecidingwhichimagelooksmoreappealingtothem(Figure7a).Weobservethattheagreementisthe
lowestforimagesdepictingEurope.Furthermore,annotatorstendtoagreeonwhichimageismoreappealingmore
oftenforGeoDEandLDM2.1thanDMw/CLIP.IntheAppendix,Figures10aand10bshowexampleswhereannotators
agreeanddisagree,respectively,thatoneimagesismoreappealing.Wefindthatconsistentappealcanoccurwhen
onesamplehassignificantlylowervisualquality.However,agreementislowerwhenannotatorsareshowntwohigh
qualityimages.
11FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Halletal.
Moreover,annotatorsindifferentlocationstendtoagreemoreonsimilaritythanappeal,suggestingperceptions
ofappealaremorevariableacrossregions.Thiscouldbebecausesimilarityisgroundedinareferenceimage,while
visualappealevokeslatentinterpretationsofattractiveness.Inaddition,Figure7bshowsthatannotatorsinSoutheast
Asiamoreoftenselectthesameimageasmoresimilarandappealing,suggestingthattheirinterpretationofappeal
coincideswiththatofsimilaritymoresothanannotatorsinotherregions.Forannotatorsacrossalllocations,appeal
coincideswithsimilaritytheleastforimagesthatcorrespondtotheirlocation,suggestingthatannotatorsmayleverage
qualitiesforappealbeyondsimilaritymorewhentheyhavegreaterfamiliaritywithwhatispresentedintheimage.
Recommendation:Collectannotationsofvisualappealfromannotatorsinmultipleregions.Whilecollectingmore
preferenceswilllikelynotresolvedisagreementsinaestheticpreferences,theymaysuggestdirectionsformore
inclusiveevaluationcriteriaordefinitions.Furthermore,theymaybeusefulinchallengingauniversalnotionof
appealbypinpointingspecificpreferencesthatvarybetweencertaingroups.
3.2.2 Human&AutomaticMetricInteraction:ManifoldPresence. Becausevisualappealisapproximatedwithprecision,
weconstructananaloguetoprecisionbycountingtheproportionofvisuallyappealingannotationsforgenerated
imagesthatfallwithinthemanifoldofalltherealimagesinGeoDE.Inthisset-up,GeoDEisrepresentativeofthereal
world’sgeographicdiversity.Figure7cshowsthatformostobjectsandregions,themajorityofvisuallyappealing
annotationsofgeneratedimagesfallwithinthemanifoldofrealimages.
InAppendixFigure11a,weinspectexamplesandfindthatannotatorsoftenpickimagessimilartothoseinGeoDE
aremoreappealing,i.e.,realistic,withtheobjectinatleast25%oftheimage,andhighquality[42].Thistendstooccur
moreforimagesthatfallwithinthemanifold,althoughsomeappealingimageswiththesecharacteristicsareoutside
themanifold.However,wealsofindthatsomeannotators’interpretationof“visuallyappealing”divergefromGeoDE
andfalloutsideofthemanifold.ExamplesareshowninAppendixFigure11b.Insomecases,annotatorsgivegreater
weighttotheaestheticsofthebackgroundthantheobject.Thisisespeciallytruewhenimageshavegreaterbackground
stereotyping,suchasforimagesofcarsinLDM2.1.Annotatorscanalsopreferrealisticobjectseveniftheyarelower
qualityorsmaller.Forexample,afar-awaydogismarkedasmoreappealingthanaclose-updogwithunrealisticcolors.
The“niceness”ofobjectsalsorelatestovisualappealasannotatorstendtoconsiderdirty,rustycars,dullpots,and
plainbagslessappealingthannewer,morepristine,andcolorfulcounterparts.
Recommendation:Whenusingmanifold-basedmetricstomeasurevisualappeal,selectareferencedatasetthat
alignswiththedefinitionusedbyannotatorsorneededformodeldeployment.Whenreportingonappeal,clarify
whoseandwhatdefinitioniscaptured,includingwhetherappealisassociatedwithexaggerationsorstereotypes.
3.3 ObjectConsistency
3.3.1 HumanInterpretations. Finally,weinvestigatetherateatwhichannotatorsidentifythedesiredobjectinaphoto
andtheextenttowhichthiscoincideswiththegeographicrepresentationoftheimage.InFigure8a,wefindthat
annotatorsacrossalllocationsconsiderLDM2.1ashavingtheleastobjectconsistencywhilealsoindicatingthatLDM
2.1tendstohavethegreatestgeographicrepresentation.Thisalignswithqualitativeobservationsfrom[22]thatLDM
2.1oftendepictsstronggeographicrepresentationatthecostofobjectconsistency.
InFigure8b,wevisualizeexamplesinwhichin-regionannotatorssaythedesiredobjectisnotshownwhileout-
of-regionannotatorssayitis.Forexample,annotatorslocatedinSoutheastAsiadonotconsiderasubsetofimages
fromDMw/CLIPgeneratedwiththeprompt“storefrontinSoutheastAsia”asstorefrontswhileannotatorslocated
outsideofSoutheastAsiado.Thesamepatternistrueofin-regionandout-of-regionannotatorsforgeneratedimages
12TowardsGeographicInclusionintheEvaluationofText-to-ImageModels FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
100
80 100 60
95 40
20
90 0 0.15 0.20 0.25 0.30 0.35
85 100 80 80
60
75 40
20
70 70 75 80 85 90 95 100 0 0.15 0.20 0.25 0.30 0.35 % Imgs w/ Region Rep. 100
Annotator Loc. Img. Corpus 80 A E Su ofr uri oc ta p he east Asia G D LDMe Mo wD 2/E .C 1LIP 246 000 Ann E S Ao u o frt ur ia o ct at p ho e er a L so t c A. sia
0 0.15 0.20 0.25 0.30 0.35
Rounded CLIPScore
(a) Geographic representa- (b)Exampleswherein-regionann.saysobjectis (c)RelationshipbetweenObject-CLIPScore
tionvs.objectappearance notshownandout-of-regionann.disagrees. andpresenceofobjectinimage.
Fig.8. (a)Relationshipbetweenobjectconsistencyandgeographicrepresentation.(b)Disagreementsinconsistencybetween
annotators.(c)ComparisonofObject-CLIPScoreandannotatorperceptionofobjectconsistency.Annotatorsindifferentregions
varyslightlyintheirrecognitionofconcreteobjects.Beyondathreshold,Object-CLIPScorecorrespondstoconsistent
presenceofobjectsforallannotatorlocations.
ofa“cookingpotinAfrica.”Thissuggeststhatannotatorslocatedindifferentregionsvaryinwhattheyconsidertobe
accuratevisualrepresentationsofconcreteobjects.Interestingly,in-regionannotatorscanevendisagreeaboutobjects
inrealimagesthatwerecollectedfrompeoplelivinginthatregion,asintheexampleofcookingpotsfromGeoDEin
theFigure.Thissuggestspossibleintra-regionvariationsinthedefinitionofobjectsaswellasinter-regionvariations.
Recommendation:Whenannotatingforobjectconsistency,includeperspectivesfromin-andout-of-region.Multiple
annotatorsperregionareidealforcapturingwithin-regionvariations.Ifuniformityisdesired,instructannotatorsto
focusonaspecificobjectdefinition/representation,includingpositiveandnegativeexamples.
3.3.2 Human&AutomaticMetricInteraction:Object-CLIPScore. InFigure8c,weplotbucketedCLIPScoresandaverage
per-imageannotationsofwhethertheobjectappearsintheimage.ByexaminingLDM2.1,whichsuffersfromsome
consistencychallenges,wefindthatthelowerrangeofCLIPScoresisausefulindicatorofwhethertheobjectappearin
theimage.Thisisalignedwithfindingsin[22].However,athigherscorestheobjectisconsistentlyintheimage.As
withtheRegion-CLIPScore,weseethattherangeofCLIPScorescanvarybetweentypesofimages.Wefindempirically
thatathresholdof0.21-0.25bestindicatesconsistentappearanceofanobject.Thisrangeaccountsforvariations
betweendatasets,withathreshold0.21correspondingtonearlyconstantobjectpresenceforGeoDEandDMw/CLIP
and0.25forLDM2.1.UnliketheRegion-CLIPScore,theObject-CLIPScoreseemsequallyinformativeregardlessof
whichlocationtheannotatorisin.
Recommendation:UseathresholdforObject-CLIPScoretoincludemultiplerepresentationsofthesameconcept.
4 RELATEDWORK
Beyondtext-to-imagemodels,ourworkissituatedwithinawiderdiscussionofgeographicandculturalperformance
disparitiesincomputervisionsystems.Forexample,DeVriesetal.[12]reportthatobjectdetectionmodelssuffer
markeddropsinperformancewhenevaluatedonimagesfromnon-Westerngeographies,andparticularlysoforthose
fromlower-incomecountries.Goyaletal.[20]findsimilardisparitiesintheperformanceofvisualfeatureextractors,as
13
tcejbO
/w sgmI
%
EDoeG
PILC
/w
MD
1.2
MDL
top gnikooc
:jbO
tnorferots
:jbO
top gnikooc
:jbO
acirfA
:geR
aisA tsaehtuoS
:geR
acirfA
:geR
EDoeG
PILC
/w
MD
1.2 MDL
jbO
/w sgmI
%
jbO /w
sgmI
%
jbO
/w sgmI %FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Halletal.
doesGustafsonetal.[21]forthepopularimage-textfoundationmodelCLIP[41].Thesefailuresarelikelytobe,atleast
inpart,theresultofpoorrepresentationinthetrainingdatasets:ImageNet[13]displaysasamplethatisoverwhelmingly
biasedtowardNorthAmericanandWesternEuropeancontexts[51].Remarkably,Richardsetal.[45]reportthatas
performancehasimprovedonclassicobjectdetectionbenchmarks,theperformancegapbetweenregionshasworsened.
Accountingforgeographiccontextisachallengenotlimitedtovisionsystems:inNLP,agrowingbodyofresearchin-
vestigatessubjectivityamongannotators[2,11,19]and,inparticular,cross-culturalsensitivity.Whileculturaldiversity
isnotexactlyproxiedbygeographicdiversity,weexpectculturaldifferencestooftenco-occurwithgeographicshifts,
andconsiderworkoncross-culturalinclusiondeeplyrelevanttoourwork.Researchersworkingonmachinetranslation
havelongsoughttoimprovetranslationbothtoandfromso-calledlow-resourcelanguages,thoughmodelscontinue
toexhibitfailuresalongculturally-specificaxes[1].Recentworkexplorestheroleofculturalcontextinautomated
detectionofstereotypes[14,27],detectingtoxictextorhatespeech[30],andvaluesespousedbylargelanguagemodels
[3,15].Othershavefocusedonhowevaluationpracticessufferfromanarrowculturalframe.Hutchinsonetal.[26]
explorehowculturalfactorsareoverlookedduringNLPsystemevaluation,highlightingtheobfuscationofannotators’
“socio-demographicstandpoints”[p.1867].Similarly,Prabhakaranetal.[39]callfor“culturallysituated”evaluations
[p.3],whileSambasivanetal.[50]arguethatfairnessevaluationsmustaccountfornon-Westernperspectiveson
fairnessitself.Ourworkrespondstosuchcalls,exploringtheextenttowhichautomatedevaluationmetricscanaccount
forgeographicdifferencesandculturalexpectations.
5 CONCLUSION
Westudyhowannotatorsindifferentregionsvaryintheirperceptionsofevaluationcriteriafortext-to-imagegenerative
modelsandhowwellautomaticmetricscapturethesevariations.Fromouranalyses,werecommendthatannotations
ofgeographicrepresentationincludeperspectivesfrompeoplelocatedinsideandoutsideregionsoffocusandthat
instructionsclarifywhethersubjectivityandreferencetoexternalsourcesareencouraged.Inaddition,modernfeature
extractorslikeCLIPandDINOv2canbettercaptureaspectsofimagesimilaritywhenperformingautomaticevaluations
thanthestandardInceptionv3.Wefindthatvisualappealisofteninterpretedincontradictorypreferencesthatcanrein-
forcestereotypesandrecommendimprovingautomaticmetricslikeprecisionviacarefulselectionofreferencedatasets.
PerceptionsofobjectconsistencycanhavewithinregionvariationsbutaregenerallywellapproximatedbyCLIPScore
acrossallannotatorlocations.Finally,contradictoryresponsesforcriterialikeappeal,similarity,andconsistencycan
revealtrueambiguitythatislostin“majority-vote”typeaggregations.Wehopethatthisworkisameaningfulstep
towardsgreatergeographicinclusioninhumanandautomaticevaluationsoftext-to-imagegenerativemodels.
Limitations.Ourstudyislimitedtothreebroadregions,sixobjects,andthreeimagesources.Annotatorsmay
varyintheirfamiliarityofobjectspresentedinthestudy.Duetothevastvariationsinhumansubjectivitywithin
regions,findingsbasedonannotatorresponsesarelimitedintheirextensibilitytothosenotincludedinthestudy.Task
constructionmayalsohaveaffectedannotatorresponses,andconclusionsarelimitedwithoutadeepfocusgroupto
collectqualitativefeedbackandexplanationfromannotators.
ACKNOWLEDGMENTS
WethankCarleighWood,IdaCheng,EmersonBacud,andAdamHakimifortheirmanycontributionsthroughoutthe
annotationprocess.WealsothankLauraGustafson,MarkIbrahim,andPietroAstolfiforprovidingtechnicaladvice.
14TowardsGeographicInclusionintheEvaluationofText-to-ImageModels FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
Ethicalconsiderations. Thisworkwouldnothavebeenpossiblewithoutdozensofindividualhumanannotators.
Inourdatacollectionprocess,wefollowedrecommendationspertainingtoresponsibledatacollection,including(i)
compensatingannotatorsfairlyfortheirworkatanhourlyrate,(ii)respectingtheprivacyofannotatorsbyusing
anonymous ids in the data collection process, (iii) avoiding questions about personal information, (iv) providing
voluntaryandinformedconsentabouttheannotationtaskanduseofdata,(v)providingmultiplebi-directionalchannels
ofcommunicationwithannotators,and(vi)allowingannotatorstoopt-outofthetaskatanytime.Inaddition,we
attemptedtomitigaterisksofexposuretoharmfuloruncomfortablecontentbyfocusingonimagesofeverydayobjects
ratherthanpeopleordivisivetopics.
Researcherpositionality. Theauthorsofthisworkrecognizetherolethatourpersonalexperienceandinstitutional
resourcesandconstraintsplayintheannotationcollectionprocessandsubsequentanalysesandrecommendations.
Thechoiceofquestions,objects,regions,andimagesincludedintheannotationtaskswereselectedwiththeintention
ofincreasingthevarietyofperspectivesaboutannotatorperceptionsofevaluationcriteriawithinourinfrastructure
andfinancialconstraints.Whileweattemptedtomitigatebiasinourinterpretationofquantitativeannotationsby
includingaqualitativesurvey,notallannotatorscompletedthesurvey.Wethereforeremainsusceptibletooursubjective
interpretationofresponses.Furthermore,itislikelythatnuanceinannotatorinterpretationoftheevaluationcriteriais
lostinourquantificationandsummarizationofanalyses,evenwithourefforttoinspectqualitativeexamples.Inaddition,
theinterpretationandcodingofthequalitativesurveydatawasconductedbythreeauthorsbasedinEuropeorNorth
America.Ourinductivecodingapproachislikelytobebiasedbyourownexperiences.Finally,ourrecommendations
pre-supposeadegreeofresourcingthatisnotaccessibletoall,suchasthepotentiallycostlysuggestionofcollecting
multipleannotationsforasinglesampletocapturesubjectivity.Whilewehopethisworkinformsimprovedevaluations
oftext-to-imagegenerativemodels,thereismuchroomforfurtherinvestigationofthemetricsandannotatorbehaviors.
Adverseimpacts. Findingsaboutvariationsinhumanandautomaticevaluationsstudiedinthisworkmaydiffer
whenappliedtoothertypesofimages,suchasdepictionsofpeople,complexscenes,orcreativeimagery,oracross
differentdemographicgroupssuchasgenderorage.Readersshouldusecautionwhenextendingfindingstobroader
regionaltrendsorwheninformingfutureannotatorpatterns.Afuturestudywouldbestrengthenedbyincluding
multipleannotatorsperregionforthesametask,toaccountforintra-regionvariability.
15FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Halletal.
REFERENCES
[1] IdrisAkinade,JesujobaAlabi,DavidAdelani,ClementOdoje,andDietrichKlakow.2023.Epsilonkúmask:IntegratingYorùbáculturalgreetings
intomachinetranslation.InProceedingsoftheFirstWorkshoponCross-CulturalConsiderationsinNLP(C3NLP).AssociationforComputational
Linguistics,Dubrovnik,Croatia,1–7. https://doi.org/10.18653/v1/2023.c3nlp-1.1
[2] HalaAlKuwatly,MaximilianWich,andGeorgGroh.2020. IdentifyingandMeasuringAnnotatorBiasBasedonAnnotators’Demographic
Characteristics.InProceedingsoftheFourthWorkshoponOnlineAbuseandHarms,SeyiAkiwowo,BertieVidgen,VinodkumarPrabhakaran,and
ZeerakWaseem(Eds.).AssociationforComputationalLinguistics,Online,184–190. https://doi.org/10.18653/v1/2020.alw-1.21
[3] ArnavArora,Lucie-aiméeKaffee,andIsabelleAugenstein.2023.ProbingPre-TrainedLanguageModelsforCross-CulturalDifferencesinValues.In
ProceedingsoftheFirstWorkshoponCross-CulturalConsiderationsinNLP(C3NLP),SunipaDev,VinodkumarPrabhakaran,DavidAdelani,DirkHovy,
andLucianaBenotti(Eds.).AssociationforComputationalLinguistics,Dubrovnik,Croatia,114–130. https://doi.org/10.18653/v1/2023.c3nlp-1.12
[4] HritikBansal,DaYin,MasoudMonajatipoor,andKai-WeiChang.2022.HowwellcanText-to-ImageGenerativeModelsunderstandEthicalNatural
LanguageInterventions?.InEMNLP(Short).
[5] AbhipsaBasu,RVenkateshBabu,andDanishPruthi.2023.InspectingtheGeographicalRepresentativenessofImagesfromText-to-ImageModels.
arXivpreprintarXiv:2305.11080(2023).
[6] FedericoBianchi,PratyushaKalluri,EsinDurmus,FaisalLadhak,MyraCheng,DeboraNozza,TatsunoriHashimoto,DanJurafsky,JamesZou,and
AylinCaliskan.2022.EasilyAccessibleText-to-ImageGenerationAmplifiesDemographicStereotypesatLargeScale. arXiv:2211.03759[cs.CL]
[7] MarcBrysbaert,AmyBethWarriner,andVictorKuperman.2014.Concretenessratingsfor40thousandgenerallyknownEnglishwordlemmas.
Behaviorresearchmethods46,3(2014),904–911.
[8] ArantxaCasanova,MarleneCareil,JakobVerbeek,MichalDrozdzal,andAdrianaRomeroSoriano.2021.Instance-conditionedgan.Advancesin
NeuralInformationProcessingSystems34(2021),27517–27529.
[9] JaeminCho,AbhayZala,andMohitBansal.2022. DALL-Eval:ProbingtheReasoningSkillsandSocialBiasesofText-to-ImageGenerative
Transformers.(2022).arXiv:2202.04053[cs.CV]
[10] XiaoliangDai,JiHou,Chih-YaoMa,SamTsai,JialiangWang,RuiWang,PeizhaoZhang,SimonVandenhende,XiaofangWang,AbhimanyuDubey,
MatthewYu,AbhishekKadian,FilipRadenovic,DhruvMahajan,KunpengLi,YueZhao,VladanPetrovic,MiteshKumarSingh,SimranMotwani,Yi
Wen,YiwenSong,RoshanSumbaly,VigneshRamanathan,ZijianHe,PeterVajda,andDeviParikh.2023.Emu:EnhancingImageGenerationModels
UsingPhotogenicNeedlesinaHaystack. arXiv:2309.15807[cs.CV]
[11] AidaMostafazadehDavani,MarkDíaz,andVinodkumarPrabhakaran.2021.DealingwithDisagreements:LookingBeyondtheMajorityVotein
SubjectiveAnnotations.CoRRabs/2110.05719(2021).arXiv:2110.05719 https://arxiv.org/abs/2110.05719
[12] TerranceDeVries,IshanMisra,ChanghanWang,andLaurensVanderMaaten.2019.Doesobjectrecognitionworkforeveryone?.InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognitionWorkshops.52–59.
[13] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.2009. ImageNet:Alarge-scalehierarchicalimagedatabase.In2009IEEE
ConferenceonComputerVisionandPatternRecognition.248–255. https://doi.org/10.1109/CVPR.2009.5206848
[14] SunipaDev,JayaGoyal,DineshTewari,ShachiDave,andVinodkumarPrabhakaran.2023.BuildingSocio-culturallyInclusiveStereotypeResources
withCommunityEngagement.InThirty-seventhConferenceonNeuralInformationProcessingSystemsDatasetsandBenchmarksTrack. https:
//openreview.net/forum?id=uIj1jDc8k6
[15] EsinDurmus,KarinaNyugen,ThomasI.Liao,NicholasSchiefer,AmandaAskell,AntonBakhtin,CarolChen,ZacHatfield-Dodds,DannyHernandez,
NicholasJoseph,LianeLovitt,SamMcCandlish,OrowaSikder,AlexTamkin,JanelThamkul,JaredKaplan,JackClark,andDeepGanguli.2023.
TowardsMeasuringtheRepresentationofSubjectiveGlobalOpinionsinLanguageModels. arXiv:arXiv:2306.16388
[16] J.L.Fleissetal.1971.Measuringnominalscaleagreementamongmanyraters.PsychologicalBulletin76,5(1971),378–382.
[17] DanFriedmanandAdjiBoussoDieng.2022.TheVendiScore:ADiversityEvaluationMetricforMachineLearning.arXivpreprintarXiv:2210.02410
(2022).
[18] OranGafni,AdamPolyak,OronAshual,ShellySheynin,DeviParikh,andYanivTaigman.2022.Make-a-scene:Scene-basedtext-to-imagegeneration
withhumanpriors.InComputerVision–ECCV2022:17thEuropeanConference,TelAviv,Israel,October23–27,2022,Proceedings,PartXV.Springer,
89–106.
[19] NiteshGoyal,IanKivlichan,RachelRosen,andLucyVasserman.2022.IsYourToxicityMyToxicity?ExploringtheImpactofRaterIdentityon
ToxicityAnnotation. arXiv:2205.00501[cs.HC]
[20] PriyaGoyal,AdrianaRomeroSoriano,CanerHazirbas,LeventSagun,andNicolasUsunier.2022.FairnessIndicatorsforSystematicAssessmentsof
VisualFeatureExtractors.In2022ACMConferenceonFairness,Accountability,andTransparency(Seoul,RepublicofKorea)(FAccT’22).Association
forComputingMachinery,NewYork,NY,USA,70–88.
[21] LauraGustafson,MeganRichards,MelissaHall,CanerHazirbas,DianeBouchacourt,andMarkIbrahim.2023.PinpointingWhyObjectRecognition
PerformanceDegradesAcrossIncomeLevelsandGeographies. arXiv:2304.05391[cs.CV]
[22] MelissaHall,CandaceRoss,AdinaWilliams,NicolasCarion,MichalDrozdzal,andAdrianaRomeroSoriano.2023.DIGIn:EvaluatingDisparitiesin
ImageGenerationswithIndicatorsforGeographicDiversity. arXiv:2308.06198[cs.CV]
[23] JackHessel,AriHoltzman,MaxwellForbes,RonanLeBras,andYejinChoi.2021.Clipscore:Areference-freeevaluationmetricforimagecaptioning.
arXivpreprintarXiv:2104.08718(2021).
16TowardsGeographicInclusionintheEvaluationofText-to-ImageModels FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
[24] MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSeppHochreiter.2018.GANsTrainedbyaTwoTime-ScaleUpdate
RuleConvergetoaLocalNashEquilibrium. arXiv:1706.08500[cs.LG]
[25] MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,GünterKlambauer,andSeppHochreiter.2017.GANsTrainedbyaTwo
Time-ScaleUpdateRuleConvergetoaNashEquilibrium.CoRRabs/1706.08500(2017).arXiv:1706.08500 http://arxiv.org/abs/1706.08500
[26] BenHutchinson,NegarRostamzadeh,ChristinaGreer,KatherineHeller,andVinodkumarPrabhakaran.2022.EvaluationGapsinMachineLearning
Practice.InProceedingsofthe2022ACMConferenceonFairness,Accountability,andTransparency(Seoul,RepublicofKorea)(FAccT’22).Association
forComputingMachinery,NewYork,NY,USA,1859–1876. https://doi.org/10.1145/3531146.3533233
[27] AkshitaJha,AidaDavani,ChandanK.Reddy,ShachiDave,VinodkumarPrabhakaran,andSunipaDev.2023.SeeGULL:AStereotypeBenchmark
withBroadGeo-CulturalCoverageLeveragingGenerativeModels. arXiv:arXiv:2305.11840
[28] TuomasKynkäänniemi,TeroKarras,MiikaAittala,TimoAila,andJaakkoLehtinen.2023. TheRoleofImageNetClassesinFréchetInception
Distance. arXiv:2203.06026[cs.CV]
[29] TuomasKynkäänniemi,TeroKarras,SamuliLaine,JaakkoLehtinen,andTimoAila.2019. ImprovedPrecisionandRecallMetricforAssessing
GenerativeModels. arXiv:1904.06991[stat.ML]
[30] NayeonLee,ChaniJung,andAliceOh.2023.HateSpeechClassifiersareCulturallyInsensitive.InProceedingsoftheFirstWorkshoponCross-Cultural
ConsiderationsinNLP(C3NLP),SunipaDev,VinodkumarPrabhakaran,DavidAdelani,DirkHovy,andLucianaBenotti(Eds.).Associationfor
ComputationalLinguistics,Dubrovnik,Croatia,35–46. https://doi.org/10.18653/v1/2023.c3nlp-1.5
[31] TonyLee,MichihiroYasunaga,ChenlinMeng,YifanMai,JoonSungPark,AgrimGupta,YunzhiZhang,DeepakNarayanan,HannahBenitaTeufel,
MarcoBellagente,MingukKang,TaesungPark,JureLeskovec,Jun-YanZhu,LiFei-Fei,JiajunWu,StefanoErmon,andPercyLiang.2023.Holistic
EvaluationofText-To-ImageModels. arXiv:2311.04287[cs.CV]
[32] StanislavMorozov,AndreyVoynov,andArtemBabenko.2021.OnSelf-SupervisedImageRepresentationsfor{GAN}Evaluation.InInternational
ConferenceonLearningRepresentations. https://openreview.net/forum?id=NeRdBeTionN
[33] WAMunsonandMarkBGardner.1950.Standardizingauditorytests.TheJournaloftheAcousticalSocietyofAmerica22,5_Supplement(1950),
675–675.
[34] MuhammadFerjadNaeem,SeongJoonOh,YoungjungUh,YunjeyChoi,andJaejunYoo.2020.ReliableFidelityandDiversityMetricsforGenerative
Models.CoRRabs/2002.09797(2020).arXiv:2002.09797 https://arxiv.org/abs/2002.09797
[35] AlexNichol,PrafullaDhariwal,AdityaRamesh,PranavShyam,PamelaMishkin,BobMcGrew,IlyaSutskever,andMarkChen.2021. GLIDE:
TowardsPhotorealisticImageGenerationandEditingwithText-GuidedDiffusionModels.CoRRabs/2112.10741(2021).arXiv:2112.10741 https:
//arxiv.org/abs/2112.10741
[36] MaximeOquab,TimothéeDarcet,ThéoMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov,PierreFernandez,DanielHaziza,FranciscoMassa,
AlaaeldinEl-Nouby,MahmoudAssran,NicolasBallas,WojciechGaluba,RussellHowes,Po-YaoHuang,Shang-WenLi,IshanMisra,MichaelRabbat,
VasuSharma,GabrielSynnaeve,HuXu,HervéJegou,JulienMairal,PatrickLabatut,ArmandJoulin,andPiotrBojanowski.2023.DINOv2:Learning
RobustVisualFeatureswithoutSupervision. arXiv:2304.07193[cs.CV]
[37] BenPhillips.2019.TheShiftingBorderBetweenPerceptionandCognition.Noûs53(062019). https://doi.org/10.1111/nous.12218
[38] DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann,TimDockhorn,JonasMüller,JoePenna,andRobinRombach.2023.SDXL:Improving
LatentDiffusionModelsforHigh-ResolutionImageSynthesis. arXiv:2307.01952[cs.CV]
[39] VinodkumarPrabhakaran,RidaQadri,andBenHutchinson.2022.CulturalIncongruenciesinArtificialIntelligence. arXiv:arXiv:2211.13069
[40] RidaQadri,ReneeShelby,CynthiaL.Bennett,andEmilyDenton.2023.AI’sRegimesofRepresentation:ACommunity-centeredStudyofText-to-Image
ModelsinSouthAsia.InProceedingsofthe2023ACMConferenceonFairness,Accountability,andTransparency(<conf-loc>,<city>Chicago</city>,
<state>IL</state>,<country>USA</country>,</conf-loc>)(FAccT’23).AssociationforComputingMachinery,NewYork,NY,USA,506–517.
https://doi.org/10.1145/3593013.3594016
[41] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,Jack
Clark,etal.2021.Learningtransferablevisualmodelsfromnaturallanguagesupervision.InInternationalConferenceonMachineLearning.PMLR,
8748–8763.
[42] VikramV.Ramaswamy,SingYuLin,DoraZhao,AaronB.Adcock,LaurensvanderMaaten,DeeptiGhadiyaram,andOlgaRussakovsky.2023.
Beyondweb-scraping:Crowd-sourcingageodiversedataset.InarXivpreprint.
[43] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen.2022.HierarchicalText-ConditionalImageGenerationwithCLIP
Latents. arXiv:2204.06125[cs.CV]
[44] SumanV.RavuriandOriolVinyals.2019. ClassificationAccuracyScoreforConditionalGenerativeModels. CoRRabs/1905.10887(2019).
arXiv:1905.10887 http://arxiv.org/abs/1905.10887
[45] MeganRichards,PolinaKirichenko,DianeBouchacourt,andMarkIbrahim.2023.DoesProgressOnObjectRecognitionBenchmarksImprove
Real-WorldGeneralization? arXiv:arXiv:2307.13136
[46] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer.2021.High-ResolutionImageSynthesiswithLatentDiffusion
Models.CoRRabs/2112.10752(2021).arXiv:2112.10752 https://arxiv.org/abs/2112.10752
[47] ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyDenton,SeyedKamyarSeyedGhasemipour,BurcuKaragolAyan,
S.SaraMahdavi,RaphaGontijoLopes,TimSalimans,JonathanHo,DavidJFleet,andMohammadNorouzi.2022.PhotorealisticText-to-Image
DiffusionModelswithDeepLanguageUnderstanding. arXiv:2205.11487[cs.CV]
17FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Halletal.
[48] MehdiS.M.Sajjadi,OlivierBachem,MarioLučić,OlivierBousquet,andSylvainGelly.2018.AssessingGenerativeModelsviaPrecisionandRecall.
InAdvancesinNeuralInformationProcessingSystems(NeurIPS).
[49] TimSalimans,IanGoodfellow,WojciechZaremba,VickiCheung,AlecRadford,andXiChen.2016.Improvedtechniquesfortraininggans.Advances
inneuralinformationprocessingsystems29(2016).
[50] NithyaSambasivan,ErinArnesen,BenHutchinson,TulseeDoshi,andVinodkumarPrabhakaran.2021. Re-ImaginingAlgorithmicFairnessin
IndiaandBeyond.InProceedingsofthe2021ACMConferenceonFairness,Accountability,andTransparency(VirtualEvent,Canada)(FAccT’21).
AssociationforComputingMachinery,NewYork,NY,USA,315–328. https://doi.org/10.1145/3442188.3445896
[51] ShreyaShankar,YoniHalpern,EricBreck,JamesAtwood,JimboWilson,andDSculley.2017.Noclassificationwithoutrepresentation:Assessing
geodiversityissuesinopendatasetsforthedevelopingworld.arXivpreprintarXiv:1711.08536(2017).
[52] KonstantinShmelkov,CordeliaSchmid,andKarteekAlahari.2018. HowgoodismyGAN? CoRRabs/1807.09499(2018). arXiv:1807.09499
http://arxiv.org/abs/1807.09499
[53] MannatSingh,LauraGustafson,AaronAdcock,ViniciusdeFreitasReis,BugraGedik,RajPrateekKosaraju,DhruvMahajan,RossGirshick,Piotr
Dollár,andLaurensvanderMaaten.2022.RevisitingWeaklySupervisedPre-TrainingofVisualPerceptionModels.InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR).804–814.
[54] GeorgeStein,JesseC.Cresswell,RasaHosseinzadeh,YiSui,BrendanLeighRoss,ValentinVillecroze,ZhaoyanLiu,AnthonyL.Caterini,J.EricT.
Taylor,andGabrielLoaiza-Ganem.2023.Exposingflawsofgenerativemodelevaluationmetricsandtheirunfairtreatmentofdiffusionmodels.
arXiv:2306.04675[cs.LG]
[55] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna.2015. RethinkingtheInceptionArchitecturefor
ComputerVision.CoRRabs/1512.00567(2015).arXiv:1512.00567 http://arxiv.org/abs/1512.00567
[56] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang.2018.Theunreasonableeffectivenessofdeepfeaturesasaperceptual
metric.InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.586–595.
[57] SharonZhou,MitchellL.Gordon,RanjayKrishna,AustinNarcomey,DurimMorina,andMichaelS.Bernstein.2019.HYPE:HumaneYePerceptual
EvaluationofGenerativeModels.CoRRabs/1904.01121(2019).arXiv:1904.01121 http://arxiv.org/abs/1904.01121
A APPENDIX
Wediscussadditionaldetailsregardingtheset-upandresultsofouranalysisinthefollowingsections.
A.1 AdditionalSet-upDetails
Tasks. ForTask1,weconstructtripletswithrealimagesfromGeoDEandgeneratedimagesfromDMw/CLIPand
LDM2.1.Inordertohaverepresentationacrossdifferenttypesofrealandgeneratedimagesaswellasrelativedistances
intheembeddingspace,wedefinedifferentcombinationsoftripletseachobject-regionasfollows:
A Onerealimageandtwogeneratedimages,for85realimagesofobject𝑂inregion𝑅.
– ReferenceImage:Onerealimageof𝑂from𝑅.
– ComparisonImages:Oneimagegeneratedwithprompt“𝑂in𝑅”whichistheclosesttothereferenceimage
inthefeaturespace.andoneimagegeneratedwithprompt“𝑂 in𝑅”whichistheeighthfurthestfromthe
referenceimage.
B Onegeneratedimageandtworealimages,for85imagesgeneratedwithprompt“𝑂in𝑅”.
– ReferenceImage:Oneimagegeneratedwithprompt“𝑂in𝑅”.
– ComparisonImages:Onerealimageof𝑂 from𝑅whichistheclosesttothereferenceimageinthefeature
space,andonerealimageof𝑂from𝑅whichistheeighthfurthestfromthereferenceimage.
C Onerealimageandtwogeneratedimages,for85realimagesofobject𝑂inregion𝑅.
– ReferenceImage:Onerealimageof𝑂from𝑅.
– ComparisonImages:Oneimagegeneratedwithprompt“𝑂in𝑅”whichistheclosestfeaturetothereference
image,andoneimagegeneratedwiththeprompt“𝑂”thatisthe48thfurthestfromthereferenceimage.
D Onerealimageandtwogeneratedimages,for85realimagesofobject𝑂inregion𝑅.
– ReferenceImage:Onerealimageof𝑂from𝑅.
18TowardsGeographicInclusionintheEvaluationofText-to-ImageModels FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
– ComparisonImages:Oneimagegeneratedwithprompt“𝑂”whichisthesixthfeaturetothereferenceimage,
andoneimagegeneratedwiththeprompt“𝑂in𝑅”thatistheeighthfurthestfromthereferenceimage.
E Onegeneratedimageandtworealimages,for85imagesgeneratedwithprompt“𝑂”.
– ReferenceImage:Oneimagegeneratedwithprompt“𝑂”.
– ComparisonImages:Onerealimageof𝑂 from𝑅whichistheclosesttothereferenceimageinthefeature
space,andonerealimageof𝑂from𝑅whichistheeighthfurthestfromthereferenceimage.
Toselectreferenceimages,wepickthe60imagesforeachobject-regioncombinationthathasanotherimagenearest
toitandthe25imagesthathasitsnearestimagefurthestaway.Thisallowsustocapturestatisticsaboutbothimages
thatarerelativelysimilartotheothers(theformer)andimagesthatarequitedifferent(thelatter).
ForourVisualAppealinganalysis,weuseComboA.Thisallowsacomparisonbetweenrealimagesofagivenobject
takeninaspecificregionandgeneratedimagespromptedwiththesameobjectandregion.Inaddition,fortheVisual
AppealanalysisweselectonlytripletswhereexactlyoneimagefallsintotheGeoDEmanifold.
Annotators. Potentialannotatorswerefirstrecruitedaccordingtotheirgeographiclocation.Theywereprovidedan
initialtasktocompletetodemonstrategeneralcompetencythenwentthroughatrainingprogramconsistingofwritten
instructionsforeachtaskandpracticeexamples.BecauseourTasksrequireanamountofsubjectivitythroughoutthem
yetwestillwanttoensureannotatorshaveaclearunderstandingoftheintendedtask,wecreatedatrialtaskthat
involvedannotatorsidentifyingsimilarimagesofcars.Annotatorswhopassedwitha75%ratewerethengraduatedto
theproductiontasksandpaidastandardrateperhourbasedonacountry-specificlivingwage.
Forproductiontasks,annotatorswithinagivenlocationworkedthroughqueuesforTask1andTask2foreach
objectandimageregion.Theyvariedintheproportionofjobscompleted,withsomeannotatorscompletinglessthan
10jobsandothershundredsjobs.Eachuniquejobwasannotatedbyoneannotatorfromeachregion.
Followingthecompletionoftheproductiontasks,weperformedananalysisofannotationcompletionquality.We
useannotationsofwhethertherealGeoDEimagescontainagivenobjectasour“sanity”check,aswehaveaground
truthanswerfortheseimages.WefoundthatwhiletheaccuracyforrealGeoDEimageswasquitehighformost
annotators,averysmallnumberofannotatorshadlowaccuracyforidentifyingobjectsinGeoDEthatwereincludedas
comparisonimageinthetaskconstruction.Inthosecases,webelievetheannotatorsdidnotfullyunderstandthetask
instructionstoselectallimagesshowingtheobjectorwereoperationalizingthetaskincorrectly.Thus,weintroduce
arequirementthatannotatorshaveatleast90%accuracyinidentifyingobjectsinGeoDEimages,toensuresome
conformitytodefinitionofobjectrepresentationandtaskcompletion.Foreachobject-regioncombination,weremove
annotationsfromannotatorswhodidnotmeetthisbar.
Attheconclusionoffiltering,wehadannotationsfrom20annotatorslocatedinAfrica(splitacrossEgypt,Nigeria,
andSouthAfrica),17annotatorsinEurope(locatedinGreatBritain,Italy,RomaniaandSpain)and23annotatorsin
SoutheastAsia(locatedinIndonesia,thePhilippines,andThailand).AllannotatorswerefluentinEnglish,andtasks
werepresentedinEnglish.
A.2 Additionalannotationdetails
Table3containsdescriptivecodesusedtocategorizeresponsesfromannotators.Figure9showsproportionofsurvey
respondentsexhibitingkeythemesintheirresponsetoquestionsabouttheirinterpretationoftheannotationtask.
A.3 AdditionalResults
19FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Halletal.
(a) (b)
Fig.9. (a)Proportionofsurveyrespondentsexhibitingkeythemesintheirresponsestoquestionsaboutidentifyinglocationsfrom
bothbackgroundcontextandobjectspecifics.(b)Proportionofsurveyrespondentssuggestingtheyutilizedcommonstereotypes
whenannotatinggeographicrepresentativeness.Annotatorsinalllocationsfrequentlyexhibitstereotypesintheirresponses.
(a)Consistentappeal (b)Inconsistentappeal
Fig.10. (a-b)Examplesofgeneratedimagesofdogs:(b)depictsimagesselectedasmoreandlessappealing(respectively)byall
threeannotators;(c)depictsimageswithinconsistentappealannotations.Annotatorsmayleveragequalitiesforappealbeyond
similaritymorewhentheyhavegreaterfamiliaritywithwhatisintheimage.
20TowardsGeographicInclusionintheEvaluationofText-to-ImageModels FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
Code Description Kappa
Externalsearch Usinganexternalreference,e.g.searchengine 0.91
People Mentionspeopleorfaces 0.80
Nature&naturalworld Mentionsnature,plants,animals,landscape,weatherorclimate 0.77
Feature-basedrepresentation Mentionslookingforfeature-basedrepresentations,e.g.lookingatcolor,sizeorshape 0.76
Unclassified Unclassifiedresponse 0.73
Stereotypedrepresentations Utilizescommonstereotypesofappearanceofitemswithinaregion 0.72
Builtenvironment Mentionsbuildings,architecture,orthebuiltenvironment 0.71
Absenceofdetail Mentionsanabsencedetailornoteworthycharacteristics 0.70
Culture,art&religion Mentionsculture,food,art,sculpture,religion,fashionandclothing 0.63
Personallivedexperience Leveragesexperiencelivinginalocation 0.60
Mediaportrayals Relianceonportrayalsinmedia 0.60
Brandidentification Usesaknownreference,suchasrecognisingabrandorstore 0.52
Overgeneralisedrepresentations Considerssubsetsofaregionindicativeofabroaderentireregion 0.48
Objectsandtools Useseverydayobjectssuchastoolsorappliances 0.41
Erroneousrepresentation Mentionserroneousorunrealisticobjectorbackgroundrepresentation 0.41
Uniqueness Expectationthatarepresentationbeuniquetoaregion 0.15
Table3. Descriptivecodesusedtocategorizesurveyresponsesfromannotators.Codesaresortedbyinter-annotatoragreement
Fleiss’Kappa,andonlycodeswithKappagreaterthan0.6(horizontalrule)wereincludedinouranalysis.
(a)Exampleswheremoreappealinggeneratedimagefallsin (b) Examples where less appealing generated image falls in
GeoDEmanifold GeoDEmanifold
Fig.11. (a-b)Examplepairswherethemoreorlessappealingimagefallsinthemanifoldofrealimages. Annotatorsoftenconsider
realisticimagesmoreappealing,aligningwiththerealmanifold,butalsointerpretappealincontradictoryways,e.g.,
prioritizingbackgroundaesthetics,stereotypes,or“niceness”ofobjects.
21FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Halletal.
Fig.12. Whenannotatorspickthefurtherreferenceimageasthemoresimilarimage,allannotatorsdifferentiatemorebetween
similarlyandvisuallyappealingthanwhentheywerepickingthecloserimageasthesimilarimage.
Visualappeal. ThetopofFigure12showsthatwhenannotatorspickthecomparisonimagethatisclosertothe
referenceimageasthemoresimilarimage,allannotatorsdifferentiatebetweenvisualappealandsimilaritylessas
comparisonimagesmoveawayfromeachotherinInceptionV3space(solidline)andslightlymoreasimagesmove
furtherfromthereferenceimage(dashedline).Furthermore,annotatorsinEuropetendtodifferentiatebetweensimilar
andvisuallyappealingimagesmorethanannotatorsinAfrica,andbothmorethanannotatorsinSoutheastAsia.Thisis
consistentevenasdistancesbetweencomparisonandreferenceimagesvary.
Inaddition,thebottomofFigure12showsthatwhenannotatorspickthefurtherimageasthemoresimilarimage,all
annotatorsdifferentiatemorebetweensimilarlyandvisuallyappealingthanwhentheywerepickingthecloserimage
asthesimilarimage.However,thisdifferentiationseemslessimpactedbythedistanceofcomparisonandreference
images,andthereislessbetween-regionvariation.
22