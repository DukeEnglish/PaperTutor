Toward In-Context Teaching:
Adapting Examples to Students’ Misconceptions
AlexisRoss JacobAndreas
MITCSAIL
{alexisro,jda}@mit.edu
Abstract
Target multiplication learner addition learner
Concept over-generalize addition over-generalize mult.
Whenateacherprovidesexamplesforastudent if addition: if addition: if addition:
tostudy,theseexamplesmustbeinformative, e
lseam :da dk e n uc mo smmon denoms
e
lseam :da dk e n uc mo smmon denoms
e
lsea :d d nums & denoms
enablingastudenttoprogressfromtheircur- multiply nums & denoms mm ua lk te i pc l+o ym mpnorunom gsdenoms multipl+y pnruomgs & denoms
rentstatetowardatargetconceptorskill.Good
teachers must therefore simultaneously infer
Teacher Students
what students already know and adapt their
teachingtostudents’changingstateofknowl-
?
edge. Thereisincreasinginterestinusingcom- 1) infer student What is 𝟣×𝟣?
misconceptions 𝟤 𝟨
putationalmodels,particularlylargelanguage 𝟥 over-generalize addition if models,aspedagogicaltools. Asstudents,lan- 𝟨 denominators are equal
guage models in particular have shown a re- 𝟣 𝟤×𝟣 𝟨= 𝟥 𝟨 𝟣𝟣 𝟤 if a md ad ki et i co on m: mon denoms
add nums
markable ability to adapt to new tasks given Incorrect. It’s 𝟣. else:
small numbers of examples. But how effec- 2) choose most 𝟣𝟤 mm ua lk te i pc lo ym m no un m sdenoms
informative example
tively can these models adapt as teachers to i f me uq lu ta il p ld ye n no um ms s:
else:
studentsofdifferenttypes? Tostudythisques- multiply nums & denoms
tion,weintroduceasuiteofmodelsandeval- 𝟣×𝟤= 𝟤 𝟤 What is 𝟣 𝟧×𝟤 𝟧?
𝟧 𝟧 𝟧 𝟤𝟧
uationmethodswecall ADAPT. ADAPT has 𝟣 𝟤+𝟤 𝟦= 𝟦 𝟦 𝟤 𝟨 𝟤 𝟧
twocomponents: (1)acollectionofsimulated Incorrect. It’s 𝟤 . CT oa nr cg ee pt t
𝟤𝟧
Bayesianstudentmodelsthatcanbeusedfor
evaluationofautomatedteachingmethods;(2)
aplatformforevaluationwithhumanstudents,
Figure1: IntheADAPT(AdaptiveTeaching)evalu-
ation framework (§3), a teacher selects examples to
tocharacterizethereal-worldeffectivenessof
teachatargetconcepttoastudent;however,thestudent
thesemethods. Weadditionallyintroduce(3)
haspriormisconceptionsthatareunknowntotheteacher.
ATOM,anewprobabilisticmodelforadaptive
In the fraction arithmetic task (§3.1), some students
teaching that jointly infers students’ past be-
(multiplicationlearner)tendtoover-generalizethead-
liefsandoptimizesforthecorrectnessoffuture
ditionprocedureofmakingcommondenominatorsand
beliefs. In evaluations of simulated students
performingarithmeticonlyonnumerators;others(addi-
across three learning domains (fraction arith-
tionlearner)tendtoover-generalizethemultiplication
metic,Englishmorphology,functionlearning),
procedure of applying arithmetic on both numerators
ATOMsystematicallyoutperformsLLM-based
and denominators. In order to teach effectively, the
and standard Bayesian teaching models. In
teachermustjointly1)infer thestudent’smisconcep-
human experiments, both ATOM and LLMs
tionsonlinebyobservingtheirbehaviorthroughouttheir
outperformnon-adaptiverandomexamplese-
interaction (i.e., the teacher infers that the student is
lection. Ourresultshighlightboththedifficulty
anadditiongeneralizerafterobservingtheprediction
oftheadaptiveteachingtaskandthepotential
1 × 1 → 3), and 2) adapt to such misconceptions
oflearnedadaptivemodelsforsolvingit. 2 6 6
byselectingexamplesthatwillmostefficientlycorrect
1 Introduction these misconceptions (i.e., the teacher anticipates the
student’s new incorrect belief that all fractions with
Goodteachingisadaptivetostudents’specificbe- equaldenominatorsshouldbetreatedasadditionprob-
liefsandpreconceptions(Corbett,2001). Imagine, lemsandselectstheexample 1 5 × 2 5 = 22 5 tocorrectit).
for example, that you have been tasked with tu-
WeproposeATOM,atwo-partprobabilisticapproach
thatachievesadaptiveteachingbymaintainingexplicit
toringstudentsfractionarithmetic. Youmaystart
inferencesaboutstudentpriors(§4.1).
byfirstprobingastudent’sunderstanding,asking
1
4202
yaM
7
]LC.sc[
1v59440.5042:viXrathemwhat 1 × 2 is. Supposethestudentanswers Simulated and human experiments allow us to
5 5
2. Immediately, you might develop a hypothesis characterizethepedagogicalcapabilitiesofteach-
5
about this student’s misconceptions: they seem ers along several dimensions: their inferences
tobeover-generalizingtheruleforaddition,only about student beliefs (§5.3, §6.3), adaptivity of
applyingtheoperationtothenumerator. Nowsup- chosenexamples(§5.4),anddifferencesinteach-
poseanotherstudentcorrectlyanswers 1×2 = 2 , ing mathematical concepts (§5.5). In addition to
5 5 25
but answers 1 + 2 = 2. This student would ap- theseevaluations,weintroduce:
2 4 6
pear to be over-generalizing the rule for multipli-
3. Anewprobabilisticmodel,ATOM(Adaptive
cation. These(discreteandsystematic)categories
TeachingtOwardsMisconceptions),which
ofstudentmisconceptionshavebeenfoundtobe
performs online inference of student priors,
widespreadamongrealstudentslearningfraction
then uses these inferences to select infor-
arithmetic(Braithwaiteetal.,2017).
mative teaching examples. ATOM provides
Asthisexamplehighlights,interactionswithstu-
proof-of-viabilityforadaptiveteachingmeth-
dents reveal insights about their misconceptions,
odsinsimulatedandhumanstudents(§4.1).
and these misconceptions in turn influence the
courseofeffectiveteaching. Agoodteachershould Using ADAPT evaluations, we characterize
provide different problems for these students to ATOM, GPT-4, and a range of other methods.
targettheirspecificmisconceptions: Theaddition- In simulated students, we find that while GPT-
generalizerwouldbenefitfrommultiplicationex- 4 exhibitssomeadaptationtostudentmisconcep-
amples,especiallythosewithcommondenomina- tions, there is room for improvement with adap-
tors, while the multiplication-generalizer would tive approaches; it substantially underperforms
benefitfromadditionexamples. ATOM,suggestingpromiseinusingadaptivemeth-
WhatdoesthismeanforNLP?Computational ods (§5). In human experiments, however, both
models—particularlylanguagemodels(LMs)—are ATOM and GPT-4 outperform random example
increasingly used as pedagogical tools (Kasneci selection,highlightingthepotentialoflearnedmod-
etal.,2023). Butitisunclearhoweffectivelyanyof els(ofvariouskinds)foradaptiveteaching(§6).1
today’smodelscantailorinstructiontoperform“in-
contextteaching”(§2)forstudentswithdiffering 2 Preliminaries
degreesofskillandpriorknowledge. Inthiswork,
Weformulateourproblemsettingasoneinwhich
we draw on a long line of literature on rational
ateacherinteractivelyprovidesfeedbacktoastu-
modelsofpedagogy(Shaftoetal.,2014)tostudy
dent to communicate a new concept (a mapping
thisquestion.
frominputsxtooutputsy). Ateacherobservesa
To do so, we introduce ADAPT (Adaptive
sequenceof(x,yˆ)pairsguessedbyastudent,and
Teaching), an evaluation suite targeted at teach-
mustinferwhatadditionaltrainingexample(x,y)
ingstudentswithvariedpriormisconceptions(§3).
pairwillmostimprovethestudent’sunderstanding
InADAPT,ateacheristaskedwithselectingexam-
of the concept. We term this problem in-context
plestoteachaparticulartargetconcept. Asshown
teaching to draw an analogy to the widely stud-
inFigure1,theteacherselectsexamplesonebyone
iedphenomenonof“in-contextlearning”(Brown
and can observe predictions madeby the student.
etal.,2020;Minetal.,2022;Akyüreketal.,2023).
Importantly,thestudenthaspriormisconceptions
Formally:
that are unknown to the teacher. ADAPT is de-
signed such that correct inferences about student • The teacher begins with a target concept h∗
misconceptionscanenablemoreefficientlearning. drawn from some concept space H. A con-
ADAPT hastwocomponents: ceptparameterizesamappingbetweenanin-
putspaceX andanoutputspaceY. InFig.1,
1. Anoffline,probabilisticframeworkforre-
histhetrueprocedureforaddingandmulti-
produciblyevaluatinghowefficientlyteachers
plyingfractions,andH isthespaceofother
canteachsimulatedstudentswithunknown
possiblefractionmanipulationalgorithms,so
priormisconceptions(§3.2).
X containsarithmeticexpressionsinvolving
2. Anevaluationplatformformeasuringteach- fractions,andY containsfractions.
ers’efficacywithhumanstudentswithprior
1Ourcodeispubliclyavailableathttps://github.com/
misconceptions(§3.3). alexisjihyeross/adaptive_teaching.
2Fractions Add & multiply functions Verbs Categorize English past tense verbs Functions Learn what wug(x) does
Concepts if addition: 0.27 0.6 0.05 0.08
T tee aa cc hh e ar sa pim ecs
i
fito
c
elsea mm :d ua md lk
u
te ln
i
tu pc m lo ps ym
r
m ono gun
m
sd e &n o dm es
noms
frT aa cr p tg r iooe ngt rc aao rm itn h c f moe r ep tt i: c +d +ed 1.0+ie 0d
.6
+ 0c+ocnosnosnoannatn+te+ded gT ea nrg e oe ra ft t vic v eo e rbn m sce op dt e: l i ef l sg er : 1u ae na xdt: +ee 7fr i_ n2 e( dx): funT xca tr aiog nne d t t rhc eao tt un rtc a ne k sep ast n: i w n ou uag tn p( uix n t) p ut
target concept … … y fy zz … prime even odd positive
cope accept gamify hop divisible_3 ...divisible_20
f c cr o oo n nm c c et eh p pe t ts s sie n pt a to h cf e e e i e i lf e i lf s lf s ea s ea :d ea :dd :ddiditititioionon:n:: C 3 o s9n u pc be r - eo pp ag rt co rs ha gp m ra asc me s: o1p f0o C , gs 5o es 2n i nb 8c el e e rp ap as tt ire vas t etp mi n ma egc otse e d : o r esf l i ef l sf e( : au ax n x) d +: e bfined great Ce 4r ,o 1_ n 51 c 8e pp. rt. o . s gp raag mcr ee s:a ter_20
Students multiplication addition +ed +d +ied +consonant+ed f learner b learner
learner learner learner learner learner learner
Human & Simulated
D sti uff der ee nn tt
types
pth rH oai cg t eh o d vp uer ri r eo - gr t oeb ne ml eie uraf lt ili pn iz le ip c r aao tdg iodr na itm , io es gn : H oi vg eh pr r-p ogr ci eo enr d eb ure ra eli l e i tz of e i an m d p dur io l tt ig oipr na li ,m c eas gt :it oh nat … … incH oi fg r orh re p +cr iti o epr da b r cae lamli se e sf tin e rs … inH ci opg rrh ro ep g crr [a t Uio m f nr / isb fc
o
e ow rl r mi .ie rt ehf bc in t
e
lb
ief over
c voH ar li urpg e erh co
s
tp g ofrr fai /o i am nr
]
csb o e w rli rie t ehf c i n t b
if addition: if addition:
am da dk e n uc mo smmon denoms add nums & denoms 0.27 0.6 0.5 0.08 if g r ue na dt: ee fr i_ n4 e( dx): if g r ue na dt: ee fr i_ n2 e( dx):
else: else: else: else:
mm ua lk te i pc l+o ym mpnorunom gsdenoms multipl+y pnruomgs & denoms 0.5 3a x+7 3a x+3
have different
priors over the
concept space … … … C dio sn trt ii bn uu to iou ns … …
which determine
their predictive
distributions
𝗉[𝟣 𝟧×𝟤 𝟧→𝟤 𝟧 ]=𝟢.𝟫 𝗉 [ 𝟣 𝟧+𝟤 𝟧→ 𝟣𝟥 𝟢 ]=𝟢.𝟫 𝗉[𝗀g𝖺a𝗆mi𝗂f𝖿𝗒y→+ed ]=𝟢.𝟪 𝗉[𝗐wu𝗎g𝗀((3𝟥))→𝟫 ]=𝟢.𝟪
Figure2: AnoverviewofthetasksandstudenttypesintheADAPT(AdaptiveTeaching)evaluationframework(§3).
ADAPThasthreetasks:fractions,verbs,andfunctions. Forthefractionandfunctiontasks,astudent’sconceptspace
consistsofprograms;fortheverbstask,astudent’sconceptspaceisthespaceofgenerativemodelscorresponding
toEnglishpasttenseverbclasses.
• Weassumethatteachersinteractwithstudents Given this setup, a teaching strategy is a policy
byprovidingexamples(x ,y ) ∈ X ×Y. For p (x,y | x,y)thatchoosesexamplestomaximize
i i T
convenience, we denote a sequence of such the probability that the student assigns to h∗. In
examples (x,y) = [(x ,y ),...,(x ,y )]. the optimal teaching (OT) framework of Shafto
1 1 n n
In each round of teaching, the teacher first etal.(2014),forexample,p (h | x,y)isassumed
S
presentsthestudentwithaninputx ,thestu- known,andteacherschooseexamples:
i
dentproducesaguessyˆ,andthentheteacher
i
revealsthetruey .2 x ,y = arg max p(h∗ | x,y,x,y) (2)
i i+1 i+1
x,y
• Givenacollectionofexamples(x,y),weas-
Intherunningexample,thiscriterionismorelikely
sumethatstudentscomputeaposteriorover
to select examples of addition for a student who
concepts p (h | x,y). For example, a stu-
S
hasalreadymasteredmultiplication,andvice-versa.
dent who just seen that 1 × 2 = 2 may be
3 3 9 Inadditiontothisgreedyapproach, itispossible
lesslikelytobelievethatfractionadditionand
toplansequencesofinformativeexamplesforstu-
multiplicationfollowthesamerules. Thepro-
dents(Raffertyetal.,2016).
cess by which students infer concepts from
Asdiscussedin§1,however,theassumptionthat
examples can in principle be arbitrary; how-
teachers have exact knowledge of p (h | x,y) is
ever, for some models in this paper we will S
oftenunrealistic—real-worldteachinginvolvesstu-
assumethatstudentsareBayesian,with:
dents of many different types, whose beliefs and
(cid:89) misconceptions may not be known (or easily dis-
p (h | x,y) ∝ p (h) p (y | x ,h) (1)
S S S i i cerned) a priori. Thus, we study teaching when
i
students’ priors are themselves unknown. We as-
undersomepriorbeliefp (h)aboutthecon- sume that students are drawn from a distribution
S
ceptspace. over student types p(α), each associated with a
concept prior p(h | α). In the running example,
2Thisisbothamodelofreal-worldeducationalpractice these priors may correspond to different beliefs
and a standard paradigm for online learning; future work
aboutthealgorithmsforfractionadditionandmul-
mightstudyricherformsofinteractionwithexplanationsand
instructions. tiplication,with“additiongeneralizers”assigning
3highprobabilitytoaspuriousmultiplicationalgo- forspecificprimitives(e.g.,f,a,b);arbitrarypri-
rithmthatmanipulatesonlynumerators. orsoverprogramscanthenbederivedfromthese
Inthissetting,teachersmuststillimplementan preferencesforprimitives.
effectiveexampleselectionpolicyp (x,y | x,y);
T
however, choosing informative examples now re- 3.2 SimulatedStudents
quiresinferringstudentpriorsinordertoestimate Thefirstcomponentof ADAPT evaluatesteachers
the effect of these examples on p S(h∗ | x,y). In withsimulated,Bayesianstudents. Thesestudents
thenextsection,wedescribeourproposedframe- maintainbeliefdistributionsoverthefullconcept
workforevaluatingadaptiveteachingpolicies. In space.3 As shown in Figure 2, different “student
§4,wedescribeasetofcandidateadaptiveteaching types” are implemented by initializing students
policies(includingournewATOMmodel),andin with different priors over the concept space. All
§5 and §6 use ADAPT to evaluate these teaching student types begin with low initial belief in the
policieswithsimulatedandhumanstudents. target concept h∗ and assign high probability to
otherspuriousconcepts.
3 The ADAPT EvaluationFramework
Fractions Forthefractiontask,werepresentun-
ADAPT hastwoparts: anofflineevaluationframe-
derstandingoffractionarithmeticasprograms. Stu-
workwithsimulatedstudents(§3.2),andaplatform
dentsmaintainabeliefdistributionoverthespace
fordoingexperimentswithhumanstudents(§3.3).
of possible programs, as shown in Figure 2. We
We first describe the tasks in ADAPT (§3.1). An
createtwostudenttypes,mult-learner(amodel
overviewof ADAPT isshowninFigure2.
ofastudentwhohasnotyetmasteredmultiplica-
3.1 Tasks tionandincorrectlyappliestheprocedureforaddi-
tiontomultiplication)andadd-learner(astudent
Fractions In the first task, the teacher aims to
whoperformsadditionbyincorrectlyapplyingthe
teachthestudenthowtoaddandmultiplyfractions.
procedure for multiplication). These correspond
Here,studenttypescorrespondtodifferentpriorbe-
to common incorrect strategies that children ex-
liefs(possiblyincorrect)abouttherulesforfraction
hibitwhenlearningfractionarithmetic(Braithwaite
arithmetic.
etal.,2017)byover-generalizingtheprocedurefor
Verbs ThesecondtaskisEnglishpast-tensecon- operationtoanother.
jugation. Inthistask,studentsarepresentedwith
lemmas and must choose an appropriate ending Functions Forthefunctiontask,studentsagain
(e.g., play → +ed, fry → +ied). Here, student maintain a belief distribution over the space of
typescorrespondtodifferentdegreesoffamiliarity possible concepts. We create two types of stu-
withpossibleEnglishpast-tenseendings. dentsforeachtargetconcept: ab-learneranda
f-learner. Thef-learnerknowsthetruevalue
Functions In the third task, reminiscent of ex- of b but has an incorrect, spurious belief about
istingnumberconceptlearningtasks(Tenenbaum, what function f(x) is; the b-learner knows the
1999),theteacheraimstoteachthestudentafunc- truef(x)inthetargetprogramh∗ buthasanincor-
tionthattakesinnumbersandreturnseithernum- rectbeliefaboutthevalueofb. See§Bforhowwe
bers or undefined. These functions can be repre-
selectincorrectbeliefsforstudents.
sentedas programs thattakeaninput xandcom-
pute: Verbs Fortheverbtask,werepresentunderstand-
ing of verb conjugation as generative models of
if f(x): return undefined (3) Englishpast-tenseverbs. StudentsarenaïveBayes
else: return a*x+b models with features for word-final character n-
grams, sop(h | x,y)isadistributionovermodel
where f is a boolean function and a, b are inte- parameters, with Dirichlet/Beta priors over the
gers. The teacher chooses input/output pairs (x, verb class/feature occurrence parameters, respec-
wug(x))toshowthestudenttomaximizethestu-
tively. WefitanaïveBayesmodelontheUnimorph
dent’sbeliefthattheconceptisthecorrectprogram
h∗. Wecreate24targetconcepts,whichcombine3 3Theconceptspaceconsistsof9conceptsforthefraction
task,4,158conceptsforthefunctiontask,andacontinuous
uniquesettingsofa/band8settingsoff. Student
spaceofpossiblevaluesfor10,528parameters(corresponding
typescanbeinstantiatedbyselectingpreferences to329featuresand4verbclasses)fortheverbtask.
4dataset4 (Batsurenetal.,2022)andusethemode choosesexamplestomaximizetheposteriorproba-
oftheresultingposteriorasthetargetconcept.5 bilitythatthestudentassignstothetargetconcept
Wecreatefourstudenttypesbypickingoneof h∗. Becausethestudent’spriorisunknown,how-
theclassesasthe“unknown”class: a+d-learner, ever,thisprocessinvolvestwosteps:
+ed-learner, +consonant+ed-learner, and a
1. Maximumaposterioriestimationofstudent
+ied-learner. Tosimulatestudentswhoarefamil-
priors. Recall that, during interaction, the
iarwithallbutoneclass,weinitializethestudent’s
teacherprovidesinputsx ,thenobservesstu-
priors by using the posterior mode parameters of i
dentguessesyˆ beforeprovidingground-truth
the model fit on the full data, setting the parame- i
tersforthe“unknown”classtoall1s(effectively
labels y i. In ATOM, the teacher selects stu-
dent prior parameters that best explain the
removinganylearnedinformationabouttheclass):
student’ssequenceofguesses:
Figure 2 shows how setting the prior in this way
determinesthemodeofthe+ied-learner’sprior
α = arg
max(cid:88)
logp (yˆ | x,y,x ,α) (4)
i S i i
distributionovergenerativemodels. α
i
3.3 HumanStudents where (x,y) = [(x 1,y 1),...,(x i−1,y i−1)]
Estimatingthisarg maxrequiresatractable
Thesecondcomponentofthe ADAPT evaluation
procedureforcomputingtheposteriorpredic-
framework is a platform for evaluating adaptive
tive distribution, which is available for each
teachingwithhumanstudents,specificallyforthe
simulatedstudentmodelweevaluatein§5.
function learning task. Human participants are
tasked with learning what a “mystery machine” 2. Optimalselectionofinformativeexamples.
called wug does. They are given 10 minutes to As in OT, once α has been estimated, we
interact with a teacher who presents teaching ex- chooseanexample(x ,y )tooptimize:
i+1 i+1
amples through a chat interface. Their task is to
figure out when wug(x) is undefined (i.e., guess arg maxp S(h∗ | x,y,x i,y i,x i+1,y i+1,α) (5)
what f is), and when wug(x) is defined, what it
xi+1,yi+1
computes (i.e., what a and b are in a*x+b). They Wenotethatmanymoresophisticated ATOM-
cansubmitguessesforhowwug(x)operateswhen- typemethodsarepossible—forexample,explicit
everandhowevermanytimestheychoosetoduring marginalization (rather than MAP estimation) of
the10minutesofinteraction. wugguesseshave3 student priors. More basically, the method de-
componentscorrespondingtof,a,andb,andwe scribedabovedoesnotperformanyactiveexperi-
allowpartialguesses. See§6.1formoredetailson mentationtoidentifythestudentprior;alternative
instructions,bonuscompensation,andotherparts ATOM implementationscouldexplicitlytradeoff
ofthehumanstudy. betweenexploration(ofthestudenttype)andex-
Wecreateb-learnersandf-learnersbyprim- ploitation(ofthestudentposterior).
ingthehumanparticipantswithhintsfroma“Dr.
Smith”; b-learners receive a hint with the cor- 4.2 OtherModels
rectfbutincorrectvalueforb,andthereversefor Random TheRANDOMbaselineuniformlysam-
f-learners. AnexamplehintisgiveninTable3. plesaninputtoshowthestudent.
4 Models Ranking Asecondbaselineranksthedatapoints
atthefirststepaccordingtotheobjectiveinEq2,
4.1 TeachingTowardMisconceptions
thenchoosestheminthisorderfortherestofthe
Weintroduceanapproachthatmakesexplicitinfer- teaching interaction. The student type is chosen
ences about the parameters of the student’s prior. uniformly. WerefertothisbaselineasRANKING.
WecallthismethodATOM (AdaptiveTeaching
Non-Adaptive Athirdbaselineselectsexamples
tOwardMisconceptions).
according to the OT objective in Eq 2 but main-
Like the OT method described in (§2), ATOM
tainsafixed guessaboutthestudenttype,chosen
assumesthatstudentsareBayesianreasonersand
uniformly at the start of teaching. This baseline
4https://github.com/unimorph/unimorph
can be thought of as an ablation of the adaptive
5ByDirichlet–Multinomialconjugacy,theposteriordistri-
pieceofATOM.WerefertothisbaselineasNON-
butionoverparametersfactorizes,isalsoaproductofDirichlet
andBetadistributions,andcanbeefficientlycomputed. ADAPTIVE(Shaftoetal.,2014).
5Unknown Known
Random
Rank Rank
GPT4-Gold GPT4-Gold
Greedy Greedy (optimal)
Adaptive
Area Under Learning Curves: Simulated Students’ Likelihood of Target Concept
Fractions Functions
multiplication learner addition learner b-learner f-learner
0.8
Verbs
+ied learner +consonant+ed learner +ed learner +d learner
Unknown Student: Random Rank GPT4 Non-Adaptive ATOM
Known Student: Rank GPT4 Non-Adaptive
Figure3: Areaundersimulatedlearners’learningcurves,wherecurvesplotlearners’posteriorbeliefsinthetarget
conceptbynumberofdatapoints. Wereportresultsbytaskandstudenttypewith3randomseedsperbar. Dashed
barsindicatethatthetruestudenttypeisassumed. Notethatthey-axisforthef-learnerforfunctionsstartsat0.8,
astheselearnersalllearntheconceptearlyon,andsodifferencesinteachingmethodsaresmall. Errorbarsshow
min/maxvaluesacrossseeds. FulllearningcurvesareshowninFigure9.
Critical Example Selection by Teaching Methods
Target: divisible_3 Target: greater_2 Target: greater_2 Target: greater_7 Target: positive
Spurious: divisible_6 Spurious: greater_1 Spurious: greater_4 Spurious: greater_9 Spurious: greater_1
Ex: [-15, -9, -3, 3, 9, 15] Ex: [2] Ex: [3, 4] Ex: [8, 9] Ex: [1]
Random
Rank
[Known] Rank
GPT4
[Known] GPT4
Non-Adaptive
[Known] Non-Adaptive
Adaptive
Teaching Methods:
Unknown vs Known Number of Datapoints Number of Datapoints Number of Datapoints Number of Datapoints Number of Datapoints
Student Type
Figure4: Criticalexampleselectionbydifferentteachingmethodsforthefunctiontask. Resultsareforsimulated
f-learners,whohaveaspuriousbeliefaboutfthatagreeswiththetargetfonallbutafewexamples,aslabeled.
Theopacityofeachsquarecorrespondstothemeanvalueofwhethertheexamplechosenbytheteachingmethodat
thatstepinlearningisa“criticalexample”(averagedacrossexperimentalconditions: seedandconcepts). See§5.4
fordetails. Wereportasubsetofresultshere;see§11forfullresults.
GPT-4 Wepromptthegpt-4-0314modeltose- runthisreferenceforallmethodsexceptRANDOM,
lect teaching examples (and provide no other ex- whichdoesnotmakeuseofastudentmodel. Were-
planations); the prompt describes the target con- fertothesemethodsasRANKING-KNOWN,NON-
cept, the student’s hypothesis space, and the stu- ADAPTIVE-KNOWN,andGPT-4-KNOWN.
denttypes. Themodelisinstructedtotrytoinfer
thestudenttypeinordertoteachmostefficiently.
5 SimulatedExperiments
See Appendix D for actual prompts. To control
forthefactGPT-4sometimesgeneratesincorrect
5.1 ExperimentalSet-Up
outputsforexamples,weusegroundtruthoutputs
forgeneratedinputs.6 WecallthismethodGPT-4. We run 3 random seeds for all experimental con-
ditions. Forallmethodsexceptthe GPT-4-based
4.3 OracleModels
methods, we restrict the teaching methods from
Wealsocompareagainstseveralmethodsthatas- selectingpreviouslyselectedexamples. Teaching
sume access to the true student. These serve as interactionslast40stepsforthefraction/function
comparison points for how well methods could tasksand50stepsfortheverbtask. FornonGPT-4
doiftheyinferredthecorrectstudentmodel. We methods,weenumerateovereitherthefulldataset
(fractions/functions)orasampledsubset(500ex-
6WeparseGPT-4generationstogetinputsandcreatenew
messageswithtargetoutputs.Moredetailscanbefoundin§D. amplesforverbs)tochooseteachingexamples.
6
CUA
ytilibaborP
CUA
FDP
goL5.2 Students’LearningEfficiency fiswrong. Therefore,aneffectiveteachershould
selectsuchexamplesearlyinteaching.
We evaluate teacher effectiveness by measuring
AsshowninFigure4,theprobabilisticmethods
the student’s probability of the target concept,
h∗. Figure 3 shows the area under simulated stu- assuming the student type (RANKING-KNOWN,
dents’ learning curves, where curves reflect stu-
NON-ADAPTIVE-KNOWN)allselectcriticalexam-
dents’beliefsinh∗ (fullcurvesareshowninFig- plesearly. GPT-4-KNOWN alsoshowsaconcen-
trationofcriticalexamplesearly,thoughtheyare
ure 9). We observe that ATOM performs almost
morespreadoutforsomeconcepts(i.e.,fortarget
aswellastheoptimalstrategy, NON-ADAPTIVE-
f divisible_3/positive); GPT-4 thus exhibits
KNOWN,andoutperforms NON-ADAPTIVE,sug-
somepedagogicalreasoning,focusingonexamples
gesting adaptation is both possible (i.e., student
that will target the f-learner’s misconceptions
typeisinferrablefrominteraction)andthatitleads
whenitknowsthatthestudenttypeisaf-learner.
toimprovedteachingefficiency. Wealsoobserve
WhenGPT-4 doesnotknowthestudenttype,we
thatbothGPT-4-KNOWNand GPT-4outperform
observe that critical examples are still more con-
RANDOM butunderperformbothATOMandthe
non-adaptiveprobabilisticapproaches.
centratedatthestartthanforRANDOM,suggesting
some degree of adaptivity. Finally, we observe
5.3 GPT-4’sInferencesaboutStudentType thatATOMselectscriticalexamplesatcomparable
WequeryGPT-4forthestudenttypeattheendof
pointstoNON-ADAPTIVE-KNOWNdespitehaving
toguessaboutstudenttype.
theteachinginteraction(Basedonthisinteraction,
whichkindofstudentdoyouthinkIwasatthestart
5.5 QualitativeDifferencesinTeachingMath
of this teaching session ...). See Table 11 for an
Forfunctionconcepts,recallthatwhenwug(x)is
exampleprompt. Themeanaccuraciesof GPT-4’s
defined,itcomputesa*x+b. Weanalyzehowdiffer-
studenttypeguessesare100%forverbs,66.67%
entmethodsteachwhataandbarewhenwug(x)
forfractions,and53.47%forfunctions.
is defined by plotting the inputs they choose. As
A possible explanation for these discrepancies
Figure 5 shows, GPT-4 tends to select inputs in
isthatforthefractionandfunctiontasks,students
successfully learn the target concept by the end
orderofincreasingmagnitude. Incontrast, ATOM
startswithhigher-magnitudeexamples,thenselects
oftheteachinginteractionandthusmakeaccurate
examplesinincreasingorder. Thesequalitativedif-
predictions;fortheverbtask,however,studentsare
still making errors by the end.7 We analyze how ferences suggest that GPT-4 may have encoded
informationthatinputsclosertotheoriginareeas-
these accuracies change throughout the teaching
iertolearnfromthanthosefurtherfromtheorigin.9
interactions. Forthefunctiontask,thestudenttype
accuraciesare64.2%,60.4%,56.3%,53.47%af- 9Incontrast,ATOMscoresinputsaccordingtohowmany
ter10,20,30,and40steps,respectively: Thisde- incorrectconceptsthey“ruleout,”treatingallelseasequal.
creasesuggeststhatGPT-4exhibitsrecencybias
inmakinginferencesaboutstudenttype.
Examples Selected for Teaching What wug(x) Computes
GPT-4 ATOM
5.4 SelectionofCriticalExamples
Forfunctionconcepts,weevaluatehowearlyteach-
ingmethodsselect“criticalexamples,”orkeyex-
amples that distinguish the target f from the spu-
rious f.8 Consider the case where the target f
is greater_2 but the f-learner believes it is
Minutes of Teaching Interaction
greater_4: Thecriticalexamplesare3and4be- Random GPT-4 ATOM
causetheyaretheonlyexamplesonwhichthetar-
getfandspuriousfwouldreturndifferentoutputs.
Order of Datapoint Order of Datapoint
Observing wug(x) on one of these inputs would
makecleartothef-learnerthattheirbeliefabout Figure5: Examplesselectedbydifferentteachingmeth-
odsforteachingaandbinthefunctionlearningtask
7SeeFigure10forhowthecorrectnessofstudentpredic-
(i.e., what wug(x) computes when it is defined). The
tionsonteachingexampleschangesthroughoutlearning.
x-axisindicatestheorderofthechosenexamplecom-
8Weconsiderasubsetoftargetconcepts/spuriousconcepts
paredtootherexamplestargetingaandb.
wherethenumberofcriticalexamplesislessthan10.
7
tupnI
nesohCFunction Results with bymodelbehaviorforf-learners: aftercontrol-
Human Students ling for student type, improvements over the ran-
domteacheraresignificantforf-learnersbutin-
significantforb-learners. Forresultsbystudent
*
wug partial * type,time,andindividualparticipants,see§E.2.
correctness AUC
6.3 InferencesaboutStudentType
Weevaluatetheaccuracyofpredictionsofstudent
RandomGPT-4 ATOM typemadebyATOMandGPT-4aftereachminute
ofteaching. ATOM makesmoreaccuratepredic-
Figure6:Resultswithhumanstudentsshowinghoweffi-
tionsthanGPT-4,withrespectivemeanaccuracies
cientlystudentsguessedthecorrectwugconcept(§6.2).
of 71.33% and 52.61% across participants and
Stars indicate statistically significant results under a
minutes. Accuraciesovertimeareshownin§E.2
pairedt-test. Errorbarsshow95%confidenceintervals.
(Figure19).
7 Discussion
6 HumanExperiments Both our human and simulated results show that
ATOMandGPT-4exhibitpedagogicalabilityover
6.1 ExperimentalSet-Up
randomexampleselection. ATOM’sperformance
We recruit Prolific users who are fluent English with human students suggests that the Bayesian
speakersandwhoindicatedsomeexperiencewith assumptionsmadebyATOMareaccuratemodels
computerprogramming. Wepayparticipantsabase of some aspects of human learning. Across hu-
payof$4.00perstudy($16/hour)andofferabonus manandsimulatedexperiments,wefindevidence
based on the accuracy of their predictions to the ofsomeadaptivityin GPT-4,thoughlessthanin
teacher and on how early they guess the correct ATOM, both in the examples selected (§5.4) and
valueforwug(x). SeeAppendixEfordetails. inferencesaboutstudenttype(§5.3,§6.3). See§A
We evaluate RANDOM, GPT-4, and ATOM foradditionalanalysesabouthowadaptiveselected
teachersonthefunctiontask. Werun5experiments teachingexamplesaretostudentbeliefs.
perteachingmethodperexperimentalcondition.10 Wealsoobserveotherqualitativelimitationsof
These experiments were classified as an exempt GPT-4 as a teacher: selecting the same teaching
BenignBehavioralInterventionbyourIRB. examples multiple times or terminating teaching
early due to an incorrect belief that all teaching
6.2 Students’LearningEfficiency
exampleshavebeenexhausted. Itisimportantto
Weevaluatetheeffectivenessofteachersbymea- highlightthatbecauseweusegroundtruthoutputs
suringthecorrectnessofwugguessesmadebyhu- (e.g.,groundtruthfunctionevaluations)with GPT-
manparticipants,computinganAUC-likemetric. 4,the GPT-4 resultsrepresentanupperboundon
Weconsideralltimestampswhereatleastonepar- GPT-4’sperformance.
ticipantmadeaguessforwugandcomputeapartial Despitetheselimitations,however,GPT-4per-
correctnessmetricforeachguess: (fiscorrect)+ formscomparablytoATOMwithhumanstudents,
0.5(aiscorrect) + 0.5(biscorrect).11 We report suggesting pedagogical benefits beyond adaptiv-
themeancorrectnessvaluesacrosstimestamps. ity. Inparticular,theanalysisin§5.5suggeststhat
AsshowninFigure6,wefindthatbothGPT-4 language models may encode information about
and ATOM improve significantly over the RAN- human learning that is hard to represent in more
DOM baseline(p < 0.05usingapairedt-test). In- structuredmodelslikeATOM—e.g.,thatitiseas-
terestingly,thesedifferencesareentirelyexplained ierforhumanstolearntheweightsofalinefrom
inputsclosertotheorigin.
Becausehigher-magnitudeinputstendtoresultinoutputsthat
Together, our results point to complementary
canbeexplainedbyfewerfunctions,ATOMselectshigher-
magnitudeinputsearlyinteaching. advantagesofLMteacherslike GPT-4 andmore
10Thereare22uniqueexperimentalconditions(11target
structured,probabilisticmethodslikeATOM.They
concepts,2studenttypesperconcept).
suggest that there is substantial headroom to im-
11Ifnonewguesswasmadebyauser, weusetheirlast
guess. prove real-world teaching by augmenting the in-
8ferencesofstructuredmodelswithricherinforma- also uses tools from a broader literature on com-
tionaboutthepriorsthathumansbringtolearning, putationalmodelsoftheoryofmind. Prominent
whether by combining such structured methods work includes general-purpose bayesian models
withinformationencodedinLMsorbydeveloping of other agents’ beliefs and desires (Baker et al.,
otherrichmodelsofstudentpriors—e.g.,bylearn- 2011)andmodelsofpragmaticinferencegrounded
ingmorecomplex“studenttypes”fromnaturally inrecursivereasoningaboutspeakersandlisteners
occurringdata. Weperformaninitialexperiment (FrankandGoodman,2012). Morerecentworkhas
withhumanstudentsinthisdirectionbycombining studied theory of mind capabilities in LMs; they
GPT-4 and ATOM butdonotfindimprovements findlargelynegativeresultsinunaugmentedLLMs
comparedtoeitherteacheralone;see§E.3forde- (Sap et al., 2023) but positive results from LMs
tails. augmented with structured belief representations
Other directions for future work include (Sclaretal.,2023). Recentworkhasalsoexplored
modeling more complex student phenomena— LLMs’theoryofmindabilitiesinteachingsmaller
accountingforstudentswhoaskquestions,reason LMs(Sahaetal.,2023).
pedagogically about teacher intentions, and pro-
There is also a large body of work on how to
videfeedbacktoteachers(Chenetal.,2022)—and
optimally provide and interpret human supervi-
creatingmethodsforadaptiveteachingwithnatu-
sion for ML models. General frameworks for
ral language explanations in real-world teaching
thisproblemincludeMachineTeaching(Zhuetal.,
domains.
2018)andCooperativeIRL(Hadfield-Menelletal.,
2016); related ideas appear in program synthesis
8 RelatedWork
(Vaduguruetal.,2022),robotlearning(Milliand
Dragan, 2019; Dragan et al., 2013), and natural
Thisworkbuildsonalonglineofworkinrational
languageprocessing(Lietal.,2023)aswell.
modelsofpedagogy,includingBayesianmodels
like those described by Shafto et al. (2014) and TherehasbeenincreasedinterestinusingLLMs
Sumersetal.(2021),aswellasimprovedplanning toassistorsupplementhumanteachers. SeeKas-
andinferenceproceduresliketheonedescribedby neci et al. (2023) for a survey and Wang et al.
Raffertyetal.(2016). Pastworkgenerallyassumes (2023) for a specific application to math teach-
students’initialbeliefstatesareknown. Inparallel, ingproblems. ConcurrentworkbyChandraetal.
Raffertyetal.(2015)useaninverseplanningmodel (2024) uses program synthesis techniques to in-
to infer students’ prior beliefs from their actions, fermisconceptionsandprovideexplanationsabout
andChenetal.(2022)showthathumanteachers Javascript. Ourworkaddstothisliteraturebypro-
adaptexamplestothesepriorbeliefs. vidingaframeworkthatallowsreproducibleevalu-
Thisworkisalsocloselyrelatedtootherbodies ationoftheeffectivenessandpersonalizationskills
ofworkthataimtoinferstudentknowledge. Item ofLLMsasteachers,aswellasanewmodelthat
responsetheory(IRT)infersascalarmeasureof empiricallyimprovesuponLLMbaselinesinteach-
student’sskillbasedontheirresponsestoquestions inghumansanewtask.
(HambletonandSwaminathan,1981;Hambelton
andJodoin,2003). Knowledgetracing(KT)mod-
elsstudents’evolvingknowledgestatesovertime
9 Conclusion
separately for a fixed set of skills (Corbett and
Anderson, 1994); previous work has used both
bayesianmethodsformodelingindividualstudents’ WeintroduceADAPT,anevaluationsuitemeasur-
priorknowledge(Yudelsonetal.,2013)andneural ing how effectively automated teaching methods
models for modeling (Piech et al., 2015) and tar- canteachstudentswithdifferentpriormisconcep-
geting(SrivastavaandGoodman,2021)students’ tions. WealsointroduceATOM,atwo-partprob-
evolving learning states. In contrast to IRT and abilisticapproachtoadaptiveteachingthatmain-
KT,ourworkaimstoinferthelearner’sentireprior tainsexplicitinferencesaboutstudentpriors. Our
and posterior over the concept space; these infer- evaluationsofATOM,LLMs,andotherprobabilis-
ences in turn enable more fine-grained design of tic baselines with both simulated and human stu-
individualteachingexamples. dents highlight the potential of learned adaptive
Inferring student misconceptions from errors modelsforsolvingtheadaptiveteachingtask.
910 Limitations Elena Budianskaya, Charbel El-Khaissi, Tiago Pi-
mentel, Michael Gasser, William Abbott Lane,
Onelimitationofourworkisthatinourmodelof Mohit Raj, Matt Coler, Jaime Rafael Montoya
teaching,teachersarelimitedtoexampleselection Samame, Delio Siticonatzi Camaiteri, Esaú Zu-
maeta Rojas, Didier López Francis, Arturo Once-
andstudentsarelimitedtoobservation. Thesere-
vay, Juan López Bautista, Gema Celeste Silva Vil-
strictionsleaveoutthatteacherscanprovideexpla-
legas, Lucas Torroba Hennigen, Adam Ek, David
nations,andstudentscanaskquestionsandprovide Guriel, Peter Dirix, Jean-Philippe Bernardy, An-
feedbacktoteachers. Aninterestingdirectionfor drey Scherbakov, Aziyana Bayyr-ool, Antonios
Anastasopoulos,RobertoZariquiey,KarinaSheifer,
futureworkwouldbetobothcreateanevaluation
Sofya Ganieva, Hilaria Cruz, Ritván Karahógˇa,
frameworkforsuchphenomenaanddevelopmod-
StellaMarkantonatou,GeorgePavlidis,MatveyPlu-
elsforthesericherformsofteachingandlearning garyov, Elena Klyachko, Ali Salehi, Candy An-
feedback. Relatedly, students can also engage in gulo, Jatayu Baxi, Andrew Krizhanovsky, Natalia
Krizhanovskaya,ElizabethSalesky,ClaraVania,Sar-
pedagogical reasoning about why teachers chose
dana Ivanova, Jennifer White, Rowan Hall Maud-
particular examples, which can in turn influence
slay,JosefValvoda,RanZmigrod,PaulaCzarnowska,
howtheylearnfromtheseexamples;whilethesim- Irene Nikkarinen, Aelita Salchak, Brijesh Bhatt,
ulatedstudentsin ADAPT donotmodelthisphe- Christopher Straughn, Zoey Liu, Jonathan North
Washington, YuvalPinter, DuyguAtaman, Marcin
nomenon,futureworkcouldexplorerichermodels
Wolinski, Totok Suhardijanto, Anna Yablonskaya,
ofstudents.
Niklas Stoehr, Hossep Dolatian, Zahroh Nuriah,
Anotherlimitationisthatwecreatethestudent Shyam Ratan, Francis M. Tyers, Edoardo M.
types by instantiating priors in particular ways Ponti, Grant Aiton, Aryaman Arora, Richard J.
Hatcher, Ritesh Kumar, Jeremiah Young, Daria
ratherthanderivingthepriorsfromreal-worlddata.
Rodionova, AnastasiaYemelina, TarasAndrushko,
Future work could explore how to automatically
Igor Marchenko, Polina Mashkovtseva, Alexandra
learnthetypesofpriorsthathumanstudentsbring Serova, Emily Prud’hommeaux, Maria Nepomni-
todifferentteachingscenarios. ashchaya, Fausto Giunchiglia, Eleanor Chodroff,
Mans Hulden, Miikka Silfverberg, Arya D. Mc-
Carthy,DavidYarowsky,RyanCotterell,ReutTsar-
Acknowledgements
faty,andEkaterinaVylomova.2022. UniMorph4.0:
UniversalMorphology. InProceedingsoftheThir-
WearegratefultoEkinAkyürek,LeshemChoshen,
teenthLanguageResourcesandEvaluationConfer-
Gabriel Grand, Robert Hawkins, Andi Peng,
ence, pages840–855, Marseille, France.European
Megha Srivastava, Lionel Wong, Zhaofeng Wu, LanguageResourcesAssociation.
andmembersoftheMITLanguage&Intelligence
David W Braithwaite, Aryn A Pyke, and Robert S
group for helpful discussions and feedback on
Siegler. 2017. A computational model of fraction
this work. AR was funded by the NSF GRFP
arithmetic. Psychol.Rev.,124(5):603–625.
2023357727;additionalfundingwasprovidedby
theNationalScienceFoundationundergrantsIIS- Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
2238240 and IIS-2212310, and by the Alfred P.
Neelakantan,PranavShyam,GirishSastry,Amanda
SloanFoundation.
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
AdityaRamesh,DanielZiegler,JeffreyWu,Clemens
References Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Clark, ChristopherBerner, SamMcCandlish, Alec
Tengyu Ma, and Denny Zhou. 2023. What learn- Radford, Ilya Sutskever, and Dario Amodei. 2020.
ingalgorithmisin-contextlearning? investigations Language models are few-shot learners. In Ad-
with linear models. In The Eleventh International vances in Neural Information Processing Systems,
ConferenceonLearningRepresentations. volume 33, pages 1877–1901. Curran Associates,
Inc.
Chris Baker, Rebecca Saxe, and Joshua Tenenbaum.
2011. Bayesian theory of mind: Modeling joint Kartik Chandra, Tzu-Mao Li, Rachit Nigam, Joshua
Belief-Desireattribution. ProceedingsoftheAnnual Tenenbaum,andJonathanRagan-Kelley.2024. From
MeetingoftheCognitiveScienceSociety,33(33). ’why?’ to’wat!’: Explainingperplexingprogramsby
debuggingmentalmodels. InPLATEAUWorkshop.
KhuyagbaatarBatsuren,OmerGoldman,SalamKhal-
ifa, Nizar Habash, Witold Kieras´, Gábor Bella, AliciaMChen,AndrewPalacci,NataliaVélez,Robert
BrianLeonard,GarrettNicolai,KyleGorman,Yusti- Hawkins,andSamuelJGershman.2022. Learning
nusGhanggoAte, MariaRyskina, SabrinaMielke, toteach,teachingtolearn.
10AlbertCorbett.2001. Cognitivecomputertutors: Solv- Anna N Rafferty, Michelle M LaMar, and Thomas L
ingtheTwo-Sigmaproblem. InUserModeling2001, Griffiths.2015. Inferringlearners’knowledgefrom
pages137–147.SpringerBerlinHeidelberg. theiractions. Cogn.Sci.,39(3):584–618.
AlbertTCorbettandJohnRAnderson.1994. Knowl- SwarnadeepSaha,PeterHase,andMohitBansal.2023.
edgetracing: Modelingtheacquisitionofprocedu- Canlanguagemodelsteachweakeragents? teacher
ral knowledge. User Model. User-adapt Interact., explanationsimprovestudentsviapersonalization.
4(4):253–278.
MaartenSap, RonanLeBras, DanielFried, andYejin
Anca D. Dragan, Kenton C.T. Lee, and Siddhartha S.
Choi.2023. Neuraltheory-of-mind? onthelimitsof
Srinivasa.2013. Legibilityandpredictabilityofrobot
socialintelligenceinlargelms.
motion. InProceedingsofthe8thACM/IEEEInter-
national Conference on Human-Robot Interaction,
MelanieSclar,SachinKumar,PeterWest,AlaneSuhr,
HRI’13,page301–308.IEEEPress.
YejinChoi,andYuliaTsvetkov.2023. Mindinglan-
guagemodels’(lackof)theoryofmind: Aplug-and-
MichaelC.FrankandNoahD.Goodman.2012. Predict-
ingpragmaticreasoninginlanguagegames. Science, playmulti-characterbelieftracker.
336(6084):998–998.
PatrickShafto,NoahDGoodman,andThomasLGrif-
Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, fiths.2014. Arationalaccountofpedagogicalrea-
andStuartRussell.2016. Cooperativeinverserein- soning: teaching by, and learning from, examples.
forcementlearning. Cogn.Psychol.,71:55–89.
RK Hambelton and M Jodoin. 2003. Item response MeghaSrivastavaandNoahGoodman.2021. Question
theory: modelsandfeatures. generation for adaptive education. In Proceedings
of the 59th Annual Meeting of the Association for
Ronald K. Hambleton and Hariharan Swaminathan.
ComputationalLinguisticsandthe11thInternational
1981. Journal of Educational Measurement,
JointConferenceonNaturalLanguageProcessing
18(3):178–180.
(Volume 2: Short Papers), pages 692–701, Online.
AssociationforComputationalLinguistics.
Enkelejda Kasneci, Kathrin Sessler, Stefan Küche-
mann, Maria Bannert, Daryna Dementieva, Frank
TheodoreRSumers,RobertDHawkins,MarkKHo,
Fischer, Urs Gasser, Georg Groh, Stephan Günne-
and Thomas L Griffiths. 2021. Extending rational
mann, Eyke Hüllermeier, Stephan Krusche, Gitta
modelsofcommunicationfrombeliefstoactions.
Kutyniok, Tilman Michaeli, Claudia Nerdel, Jür-
genPfeffer,OleksandraPoquet,MichaelSailer,Al-
JoshuaTenenbaum.1999. Rulesandsimilarityincon-
brechtSchmidt,TinaSeidel,MatthiasStadler,Jochen
cept learning. In Advances in Neural Information
Weller, Jochen Kuhn, and Gjergji Kasneci. 2023.
ProcessingSystems,volume12.MITPress.
Chatgptforgood? onopportunitiesandchallengesof
largelanguagemodelsforeducation. Learningand
SaujasVaduguru,KevinEllis,andYewenPu.2022. Ef-
IndividualDifferences,103:102274.
ficientpragmaticprogramsynthesiswithinformative
BelindaZLi,AlexTamkin,NoahGoodman,andJacob specifications.
Andreas. 2023. Eliciting human preferences with
languagemodels. RoseE.Wang,QingyangZhang,CarlyRobinson,Su-
sannaLoeb,andDorottyaDemszky.2023. Bridging
Smitha Milli and Anca D Dragan. 2019. Literal or thenovice-expertgapviamodelsofdecision-making:
pedagogichuman? analyzinghumanmodelmisspec- Acasestudyonremediatingmathmistakes.
ificationinobjectivelearning.
MichaelVYudelson,KennethRKoedinger,andGeof-
SewonMin,XinxiLyu,AriHoltzman,MikelArtetxe,
freyJGordon.2013. Individualizedbayesianknowl-
MikeLewis,HannanehHajishirzi,andLukeZettle-
edgetracingmodels. InLectureNotesinComputer
moyer.2022. Rethinkingtheroleofdemonstrations:
Science, Lecture notes in computer science, pages
Whatmakesin-contextlearningwork? InProceed-
171–180.SpringerBerlinHeidelberg,Berlin,Heidel-
ingsofthe2022ConferenceonEmpiricalMethodsin
berg.
NaturalLanguageProcessing,pages11048–11064,
AbuDhabi,UnitedArabEmirates.Associationfor
XiaojinZhu,AdishSingla,SandraZilles,andAnnaN
ComputationalLinguistics.
Rafferty.2018. Anoverviewofmachineteaching.
ChrisPiech,JonathanSpencer,JonathanHuang,Surya
Ganguli,MehranSahami,LeonidasJ.Guibas, and A Doteachingexamplestargetstudent
JaschaSohl-Dickstein.2015. Deepknowledgetrac- misconceptions?
ing. CoRR,abs/1506.05908.
Forthesimulatedevaluations,weanalyzewhether
Anna N Rafferty, Emma Brunskill, Thomas L Grif-
theexamplesselectedbydifferentteachingmeth-
fiths,andPatrickShafto.2016. Fasterteachingvia
POMDPplanning. Cogn.Sci.,40(6):1290–1332. odstargetstudents’specificmisconceptions.
11Adaptation of Teaching Examples to Simulated Students’ Misconceptions: Functions
GPT-4 GPT-4: Known ATOM Non-Adaptive: Known
Data
Targets
f?
Number of Datapoints Number of Datapoints Number of Datapoints Number of Datapoints
f-learner b-learner
Figure 7: Plot showing whether the data chosen by teaching methods target learning f (i.e., when wug(x) is
undefined)ora*x+b(i.e.,whatwug(x)computeswhendefined)forthefunctiontask. Aninputxforwhichwug(x)
isundefinedtargetsf,andaninputforwhichwug(x)isdefinedtargetsa*x+b. Intuitively,f-learnersbenefitmore
fromseeingexamplestargetingfearlyoninteaching,andb-learnersbenefitmorefromseeingexamplesthat
targeta*x+b.
Adaptation of Teaching Examples to Simulated Students’ Misconceptions
Fractions Functions Verbs
Data
Targets
Unknown
Concept?
Number of Datapoints Number of Datapoints Number of Datapoints
Unknown Student: GPT4 Non-Adaptive ATOM
Known Student: GPT4 Non-Adaptive
Figure 8: Plot showing whether examples selected by teaching methods target student misconceptions. For
fractions,multiplicationproblemswithcommondenominatorstargetmult-learners’misconceptions,andaddition
problemswithdifferentdenominatorstargetadd-learners’misconceptions. Forfunctions,inputsxforwhich
f(x)returnsTruetargetf-learners’misconceptions,andinputsforwhichf(x)returnsFalsetargetb-learners’
misconceptions. Forverbs,inputswithclass“unknown”bythestudent(e.g.,a+iedverbfora+ied-learner)
targetthestudent’smisconception.
Functions Figure7showswhethertheselected doesnothaveaccesstothetruestudenttype,itdoes
teachingexamplestargetlearningfora*x+binthe notshowthisadaptation;thedataselectionpatterns
targetwugconcept,splitbystudenttype. Weexpect of GPT-4 are highly similar for the b-learners
anadaptiveteachertoselectexamplestargetingf andf-learners,suggestingthat GPT-4 struggles
for f-learners at the start of learning, and simi- withdoingimplicitadaptationonline.
larlyforexamplestargetinga*x+bforb-learners.
AllTasks Figure8showsadaptationofteaching
In both plots, we see that the optimal teacher
examples across all tasks and student types. We
NON-ADAPTIVE-KNOWN,whichassumesaccess
observesimilartrends: Acrosstasks,ATOMshows
to the ground truth student type, exhibits the ex-
similar levels of adaptivity to NON-ADAPTIVE-
pectedbehavior: Itselectsmoreexamplestargeting
KNOWN, despite not knowing student type, and
forthef-learnerthanfortheb-learneratthe
outperforms both GPT-4 methods. We also ob-
beginningoftheteachinginteraction. ATOM,de-
serve GPT-4-KNOWN selecting more examples
spite needing to maintain guesses for the student
targetingunknownconceptsthan GPT-4.
type, shows similar adaptivity to students’ priors
earlyon,selectingmoreexamplestargetingffor
B CreatingFunctionConceptsand
thef-learnerthanfortheb-learneratthestart
StudentTypes
ofteaching.
GPT-4-KNOWNalsoshowsthisadaptivitywhen b has possible values [1,2,··· ,9] and a has pos-
assumingthetruestudenttype. However,whenit siblevalues[−5,−4,··· ,4,5]. Foreachconcept,
12tocreatetheb-learner,weuniformlysamplethe fraction task. For the mult-learner who over-
incorrectbfromtherangeofpossiblevaluesofb, generalizes the procedure for addition, the “spe-
excludingthetargetb;tocreatethef-learner,we cial primitives” are (1), (4), and (6). For the
uniformlysampletheincorrectffromasetofspu- add-learnerwhoover-generalizestheprocedure
riouslyassociatedfunctions: Thesepossiblevalues formultiplication,the“specialprimitives”are(2),
areshowninTable5. Thefulllistofconceptsand (3),and(5). Weuseavalueofc = 1e5.
incorrectstudentbeliefsareshowninTable4.
Functions For b-learners, the “special primi-
B.1 CreatingtheVerbsDataset
tives”arethetargetfandspuriousb(andsopro-
gramswitheitheroftheseprimitiveswouldhave
Fortheverbstask,wecreateverbclasseswithreg
higherpriorbeliefs;programshavingboththetar-
exp matching on their past tense forms. For the
getfandspuriousbwouldhavethehighestprior
GPT-4method,wecreategroundtruthoutputsby
belief). Similarly,forf-learners,theincorrectf
first checking if a lemma exists in the Unimorph
and target b are the special primitives. We use a
dataset;ifnot,weuseaPythonverbinflectionpack-
valueofc = 1e4.
agepyinflect12 tofirstgetthepasttenseformof
the verb, then categorize it. Table 1 shows verb D GPT-4: Details
classesandcorrespondingcountsintheresulting
dataset. PromptsforGPT-4 areshowninTables6,7,and
8forfractions,Tables9,10,and11forfunctions,
Themodelthatwefitonthefulldataset(tode-
and Tables 12, 13, and 14 for verbs. An exam-
rivetheparametersofthetargetconcept)obtainsa
pleconversationbetween GPT-4 andasimulated
predictiveaccuracyof95.47%,andthemeanprob-
studentisshowninTable15.
ability of the ground truth outputs across the full
datasetis0.945.
Processing/FilteringGPT-4Outputs Inorder
tocontrolforthefactthatGPT-4maygeneratein-
C SimulatedExperiments
correctoutputsforexamples,weusegroundtruth
outputs for generated inputs. We parse GPT-4-
Foralltasks,weobtainpredictionsfromsimulated
generatedmessagestoobtaininputs,thencompute
studentsbysamplingfromtheirpredictivedistribu-
ground truth labels for those inputs and append
tions.
themtothemessagehistory,startingwith“That’s
Program Tasks For the programmatic tasks correct/incorrect.” Ifamessagecannotbeparsed,
(functions, fractions), each simulated student’s weappendacannedresponse,i.e.,“Sorry,Icould
prior belief in a program h is proportional to the notlearnfromthatexample. Icanonlylearnfrom
numberof“specialprimitives”thatappearinthe examplesthatareformattedas...” (ifnooutputcan
program. We derive the prior over programs by be parsed from the message) or “I would like to
multiplying a value c by the number of special keeplearning. CanIhaveanotherexample?” (if
primitives that appear in a program to get values no input can be parsed from the message); these
forallprograms;wethennormalizethesevaluesto messagesdonotcountasan“interaction”incom-
getadistributionoverprograms. paringagainstotherteachingmethods. Forhuman
Thesimulatedstudentsforthefunctionandfrac- experiments, we do not display these canned re-
tion tasks also maintain a noise parameter that sponses to the students and instead only display
governshownoisythelabelsareintheexamples messagesaskingforpredictionsonexamplesand
they observe; this noise parameter governs their providinggroundtruthanswers.
posteriorupdates. Forfractions,thisnoiseparame-
E HumanExperiments
teris0.8,andforfunctions,itis0.05. Weusethe
samenoisevaluesfortheteacher’smodelsofthe
E.1 Set-Up
students.
Post-Processing Wefilterandrerunanyexperi-
Fractions Table2showsthemultiplicationand mentswherethechatmessageswereloggedoutof
addition sub-programs that are used to create orderorsenttwice.
the concept space for simulated students for the
Hyperparameters TheATOMusesanoisepa-
12https://github.com/bjascob/pyInflect rametervalueof0.02formodelingsimulatedstu-
13VerbClass Description Example Counts
+ed add‘ed’tothelemma clasp 6,130
+d add‘d’tothelemma smile 13,463
+ied replacelast‘y’with‘ied’ cry 1,056
+consonant+ed doublelastconsonant,add‘ed’ stop 1,878
Table1: Verbclassesandcorrespondingdatasetcountsfortheverbconjugationtask(§3.1).
AdditionSub-Programs
(1) (2) (3)
if denominators are equal:
make common denominators add numerators
add numerators & denominators
add numerators else:
add numerators & denominators
MultiplicationSub-Programs
(4) (5) (6)
if denominators are equal:
make common denominators multiply numerators
multiply numerators & denoms
multiply numerators else:
multiply numerators & denominators
Table2: Theadditionandmultiplicationsub-programsintheconceptspaceforthefractiontaskinADAPT(§3).
Thesub-programsinthetargetconcept(i.e.,correctsub-programsforadding/multiplyingfractions)arebolded.
Learning Curves: Simulated Students’ Likelihood of Target Concept
Fractions Functions
multiplication learner addition learner b learner f learner
Verbs
+ied learner +consonant+ed learner +ed learner +d learner
Unknown Learner: Random Rank GPT4 Non-Adaptive ATOM
Known Learner: Rank GPT4 Non-Adaptive
Figure9: Learningcurvesforsimulatedstudents. Toprow: resultsforfunctionlearningandfractionarithmetic,
withy-axisshowingtheprobabilityofthetargetconcept. Bottomrow: resultsforverbconjugation,withy-axis
showingthelogPDFofthetargetconcept. Eachsubplotcorrespondstoadifferentstudenttype. Colorindicates
teachingmethod. Linestylesindicatewhetherthetruestudenttypeisassumed(dashed=unknown,solid=known).
dents. they will receive 0.05 for every 10 seconds
oftheteachinginteractionthattheirguessis
Instructions An example of hints given to the
correct,withpartialcompensationifonlyfor
human participants is shown in Table 3. The full
onlya/biscorrect.
set of instructions shown to human participants,
alongwiththeinterface,areshowninFigures12/13 2. Accuracyofpredictionsonteachingexamples:
(instructions),14(chat),and15(end). Participantsaretoldtheywillreceiveuptoan
additional1.00basedontheaccuracyoftheir
BonusCompensation Participantsaretoldthat
predictions.
theirbonusdependsontwothings:
As shown in Figure 13, participants are
1. Accuracyofwugguesses: Participantsaretold promptedtoindicatetheirunderstandingofwhat
14
ytilibaborP
FDP
goLDr. Smith spent a bunch of time studying this machine. She figured out that when wug is defined, it
computes a function of the form a*x+b, where a and b are constant numbers, so you only need to figure
out what a and b are.
She also left a note with some thoughts:
I’m pretty sure, but not totally confident, that:
1) wug is undefined when inputs are greater than 2
2) When wug is defined, b = 3
–Dr. Smith
Dr. Smith is quite familiar with wug, so her note should give you a good place to start! But keep in
mind that it is possible that she is wrong.
Table3: Anexampleofahintgiventoahumanlearner. (1)iscorrect,while(2)isnot,thuscreatinganb-learner.
theirbonusdependson. GPT-4+ATOM outperforms RANDOM (p < 0.05
usingapairedt-test)butdoesnotoutperformGPT-
E.2 AdditionalResults 4 or ATOM.
Figure16showsthelearningefficiencyofhuman
studentsbystudenttype. Figure17showsthecor-
rectness of wug guesses by component as a func-
tion of time. Figure 18 shows the AUCs of wug
correctness by component for individual partici-
pants. Figure 19 shows the accuracy of student
typepredictionsovertime.
E.3 Combining GPT-4 and ATOM
We run an experiment combining GPT-4 and
ATOM in the following way: We use ATOM to
make inferences about student type, then prompt
GPT-4 with ATOM’s inference by updating the
system prompt to GPT-4 after each prediction
made by the student. Before any predictions are
given,GPT-4 ispromptedwithbothstudenttypes
(i.e.,withthepromptgiventotheteacherthatdoes
notknowstudenttype). Wecallthismethod GPT-
4+ATOM.
Results are shown in Figure 20. We find that
Correctness of Simulated Student
Predictions on Teaching Examples
Prediction
Correctness
Fractions Functions Verbs
Number of Datapoints
Figure10: Correctnessofsimulatedstudentpredictions
onteachingexamplesbytask. Errorbarsindicate95%
confidenceintervals.
15Critical Example Selection by Teaching Methods
Target: divisible_3 Target: divisible_4 Target: greater_2 Target: greater_2 Target: greater_2
Spurious: divisible_6 Spurious: divisible_8 Spurious: greater_1 Spurious: greater_3 Spurious: greater_4
Ex: [-15, -9, -3, 3, 9, 15] Ex: [-20, -12, -4, 4, 12, 20] Ex: [2] Ex: [3] Ex: [3, 4]
Random
Rank
[Known] Rank
GPT4
[Known] GPT4
Non-Adaptive
[Known] Non-Adaptive
Adaptive
Target: greater_7 Target: greater_7 Target: greater_7 Target: positive Target: positive
Spurious: greater_5 Spurious: greater_8 Spurious: greater_9 Spurious: greater_1 Spurious: greater_2
Ex: [6, 7] Ex: [8] Ex: [8, 9] Ex: [1] Ex: [1, 2]
Random
Rank
[Known] Rank
GPT4
[Known] GPT4
Non-Adaptive
[Known] Non-Adaptive
Adaptive
Teaching Methods: Number of Datapoints Number of Datapoints Number of Datapoints Number of Datapoints Number of Datapoints
Unknown vs Known
Student Type
Figure11: Fullresultsforcriticalexampleselectionbydifferentteachingmethodsforthefunctiontask. Results
areforsimulatedf-learners,whohaveaspuriousbeliefaboutfthatagreeswiththetargetfonallbutafew
examples,aslabeled. Theopacityofeachsquarecorrespondstothemeanvalueofwhethertheexamplechosenby
theteachingmethodatthatstepinlearningisa“criticalexample”(averagedacrossexperimentalconditions: seed
andconcepts). See§5.4fordetails.
Target Incorrect Usedin
f a b f b HumanExperiments?
even 1 7 divisible_6 5
even -5 5 divisible_6 7 Y
even 3 8 divisible_4 3 Y
greater_2 1 7 greater_4 3
greater_2 -5 5 greater_1 6
greater_2 3 8 greater_3 5 Y
prime 1 7 odd 2 Y
prime -5 5 odd 9 Y
prime 3 8 odd 6
divisible_3 1 7 divisible_6 6
divisible_3 -5 5 divisible_6 9
divisible_3 3 8 divisible_6 5
divisible_4 1 7 divisible_8 1
divisible_4 -5 5 divisible_8 8 Y
divisible_4 3 8 divisible_8 9 Y
positive 1 7 greater_2 4
positive -5 5 greater_2 2
positive 3 8 greater_1 4
odd 1 7 divisible_5 3 Y
odd -5 5 prime 2 Y
odd 3 8 divisible_3 6 Y
greater_7 1 7 greater_9 2
greater_7 -5 5 greater_8 6 Y
greater_7 3 8 greater_5 6
Table4: Thetargetconceptsandincorrectbeliefsusedinthefunctionlearningexperiments. Thetargetf,a,b
definetheconceptsbeingtaught. Theincorrectfisthebeliefthatthef-learnershaveaboutfatthestartof
learning,andtheincorrectbisthevaluethattheb-learnersbelievesbtobe.
165/6/24, 8:46 AM Mystery Machine Study
Mystery Machine Study
Introduction
By entering your Prolific ID below, you are participating in a study being performed by computer scientists in
the MIT Department of Electrical Engineering and Computer Science and are agreeing to the following:
You must be at least 18 years old to participate. Your participation in this research is voluntary.
You may decline to answer any or all of the following questions. You may decline further participation, at
any time, without adverse consequences.
Your anonymity is assured; the researchers who have requested your participation will not receive any
personal information about you.
Once you click submit, you will not be able to come back to this page.
Enter your Prolific ID: Submit
5/6/24, 8:31 AM Mystery Machine Study
Mystery Machine Study
Task
Goal
In this study, we are researching the way that people learn from examples. You will be tasked with learning
what a mystery machine called wug does. This machine takes in numbers and outputs numbers; wug(x)=y
means that when wug takes in number x, it outputs y. However, it only works for some numbers and is
undefined for others. Your task is to learn on what inputs wug is undefined, and when it is defined, what it
does.
For example, suppose that you were instead learning about a machine vug that operated on letters instead of
numbers. Suppose you saw the following sequence of examples: vug(A)=B, vug(B)=undefined, vug(E)=F,
vug(I)=J. This data might suggest that vug is undefined on consonants and, when defined (on vowels),
returns the next letter in the alphabet.
5/6/24, 8:31 AM Next Mystery Machine Study
Mystery Machine Study
Task
128.30.64.44:8089 1/1
Details
You will be interacting with a teacher through a
chat interface, shown on the right. The teacher
will show you examples of what wug does on
different numbers, first asking you what you
think wug outputs on a given input, and then
providing the true output. Your goal is to figure
out what wug does as quickly as possible.
Throughout the teaching interaction, you can
provide your guess of what you think wug does
through the sidebar on the right. Your guess will
have two parts:
1. when wug is undefined
2. when defined, what it does.
We also provide a calculator to help with any
calculations you may need for your guess.
You will have 10 minutes to learn from the
teacher. After that, you will no longer be able to
128.30.64.44:8089/taslek1arn about wug or make guesses. If you get 10 1/1
predictions correct in a row while talking to the teacher or go through all the teacher's examples before the 10 minutes are up, you will have the option
to end the interaction early.
Back Next
Figure12: Screens1-3(instructions)forthestudywithhumanparticipants.
f IncorrectfOptions
prime odd
positive greater_nforn∈[−2,−1,1,2]
even divis_4,divis_6
odd prime,divis_3,divis_5,divis_7
128.30.64.44:8089/task2 1/1
divis_n divis_mwheremisthesmallestmultipleofnorthelargestfactorofn(ifm=2,thisiseven)
greater_n greater_mwhere∥m−n∥<=2(ifm=0,thisispositive)
Table5: Descriptionsofhowtheoptionsforincorrectfbeliefsoff-learnersaregeneratedforeachtargetfinthe
functiontask. Weuniformlysampletheincorrectffromthesetofoptionstodeterminetheactualincorrectfbelief
forthef-learners.
175/6/24, 8:31 AM Mystery Machine Study
Mystery Machine Study
Task
Bonus
You will recive a base pay of $4.00 for this study.
In addition, based on your performance at the
task, you will be compensated with a bonus
dependent on:
Accuracy of guesses for wug: You will
receive a higher bonus the sooner and
longer you provide a correct guess
for wug. You will receive $0.05 for every
10 seconds of the teaching interaction that
your guess is correct. For example, if you
provided the correct guess for wug 1
minute into the teaching interaction and
left it unchanged, you would have a correct
guess for 9 minutes, or equivalently 540
seconds. Therefore, you would get a bonus
of $0.05 ∗ (540/10) ∗ = $2.70. You will
receive partial compensation if only one of
the two parts is correct, or if you have the
correct guess submitted for less than 10
seconds.
Accuracy of outputs for teacher examples: You will receive up to an additional $1.00 based on how many of your responses to the teacher
are correct. For example, in the example teaching interaction in the right image, the participant gave 2/3 answers to the teacher, and so their
bonus would be $1.00 ∗ (2/3) = $0.67.
Therefore, you should aim to provide a guess of what wug does as soon as you think you know, and aim to give the teacher accurate answers. You will
not know if the guesses you submit for wug are correct.
5/6/24, 8:32 AM Back NMexytstery Machine Study
Mystery Machine Study
Task
Hints about wug
Fortunately, you are not starting from a blank slate. Dr. Smith spent a bunch of time studying this machine.
She figured out that when wug is defined, it computes a function of the form a∗x+b, where a and b are
constant numbers, so you only need to figure out what a and b are.
She also left a note with some thoughts:
128.30.64.44:8089/task3 1/1
I'm pretty sure, but not totally confident, that:
1. wug is undefined when inputs are divisible by 3
2. When wug is defined, b = 8
--Dr. Smith
Dr. Smith is quite familiar with wug, so her note should give you a good place to start! But keep in mind that
it is possible that she is wrong.
We will provide the note for you throughout learning so that you can refer to it. You can also always come
back to the task instructions by using the navigation buttons "Back" and "Next" below.
5/6/24, 8:32 AM Back Next Mystery Machine Study
Mystery Machine Study
Task
Instruction check
You now have all the information to start learning wug! Before you start, please answer the following
questions. If you are unsure of the answers, we encourage you to read through the task instructions again.
The bonus depends on:
(1) Accuracy of predictions I give to the teacher
(2) How quickly I guess what wug does
Both (1) and (2)
Correct! The correct answer is Both (1) and (2).
True or False: Multiple guesses for wug are allowed.
128.30.64.44:8089/task4 1/1
True
False
Correct! The correct answer is True. Multiple guesses are allowed.
Submit
Press "Next" to enter the chat interface. There, you will have the opportunity to familiarize yourself with the
layout before learning begins and the timer starts. After you press "I Am Ready" on the next page, you will
interact with the teacher for 10 minutes.
Back Next
Figure13: Screens4-6(instructions)forthestudywithhumanparticipants.
18
128.30.64.44:8089/task5 1/15/6/24, 8:32 AM Chat Interface
Timer: 10:00
Mystery Machine Study I'm pretty sure, but not totally confident, that:
1. wug is undefined when inputs are divisible by 3
2. When wug is defined, b = 8
Chat
--Dr. Smith
Accuracy of Predictions:
Streak:1 2 3 4 5 6 7 8 910
Make a guess about wug
(1) wug(x) is undefined when input x is:
--
P t th hre e es s t t i e" m aI ceA hrm e w rR i ale l n a b dd e y sg u" i bnw m, h a ie tnn d g u y yo eou su s ea wr se .il Ylr oe h ua a d v aey r et 1 o 0 e n s mt ca oir n ut u r l ate gea esr dn t oi tn oig n. pt A le af r yt a e c ar t r ot wh uia ntt dh, ( w2 h) ew reh :e n ad =e -fined, w bu =g -(x) computes a∗x+b R a crle l em o dwe item d fb o,e ra r gn: edM t y tu iol nt u gi p c ol ae nn lg y g u oee nts eps e a ps r a t ra i tare l
with the interface before beginning. of wug correct. The sooner you
have the correct guess for wug,
I Am Ready the higher your bonus.
Calculator for a∗x+b where:
a=- b=- OR Reset to Guess Y ho eu lp c ya on u u fs ie g ut rh eis o c ua tl c wu ula gt ; o ir t t wo ill
x= 0 Result: not affect your bonus.
Back
Type your message here... Send
128.30.64.44:8089/chat 1/1
5/6/24, 8:33 AM Chat Interface
Timer: 09:26
Mystery Machine Study I'm pretty sure, but not totally confident, that:
1. wug is undefined when inputs are divisible by 3
2. When wug is defined, b = 8
Chat
--Dr. Smith
Accuracy of Predictions: 1/3
Streak:1 2 3 4 5 6 7 8 910
Make a guess about wug
Teacher What is wug(2)? (1) wug(x) is undefined when input x is:
--
8 You
Teacher That's incorrect. wug(2)=14. What is wug(4)? (2) when defined, wug(x) computes a∗x+b R ale lm owe em db ,e ar n: dM yu olt ui p cl ae n g gu ee ts pse as r ta iare l
20 You where: a=- b=- credit for getting only one part
of wug correct. The sooner you
Teacher That's correct. What is wug(6)? h tha ev e
h
it gh he
e
c
r
o yr or ue rc t
b
ogu ne us ss
.
for wug,
undefined You
Teacher That's incorrect. wug(6)=26. What is wug(8)? Calculator for a∗x+b where:
a=3 b=8 OR Reset to Guess Y ho eu lp c ya on u u fs ie g ut rh eis o c ua tl c wu ula gt ; o ir t t wo ill
x= 4 Result: 3∗x+8=20 not affect your bonus.
Back
Type your message here... Send
Figure14: Thechatinterfaceforthestudywithhumanparticipants.
128.30.64.44:8089/chat 1/1
195/6/24, 8:34 AM Mystery Machine Study
Mystery Machine Study
Exit Questions
A) Was Dr. Smith's note right?
I'm pretty sure, but not totally confident, that:
1. wug is undefined when inputs are divisible by 3
2. When wug is defined, b = 8
--Dr. Smith
Hint (1) was:
Correct Incorrect I don't know
Hint (2) was:
Correct Incorrect I don't know
B) Did Dr. Smith's note influence what you thought wug did?
Yes No
C) Was there anything you found confusing or did not understand throughout
the study, or any other issues that you encountered?
Submit
5/6/24, 8:34 AM Mystery Machine Study
Mystery Machine Study
You have finished the study. Thank you for participating! Here is your completion code: C166E0FJ
The correct answer was:
1. wug is undefined when inputs are odd
128.30.64.44:8089/check 2. When wug is defined, it computes a∗x+b where a = 3 and b = 8 1/1
If you have questions about this research, please contact us at alexisro@mit.edu. We will be in touch about
your bonus.
Figure15: Post-chatendscreensforthestudywithhumanparticipants.
Function Results by Human Student Type
f-learner b-learner
*
wug partial *
correctness AUC
128.30.64.44:8089/end 1/1
RandomGPT-4 ATOM RandomGPT-4 ATOM
Figure16: Resultsshowinghowefficientlyhumanstudentsguessedthecorrectwugconcept(§6.2), bystudent
type. Starsindicatestatisticallysignificantresultsunderapairedt-test: Forf-learners,p<0.05forRANDOMv.s.
ATOMandforRANDOMv.s. GPT-4. Errorbarsshow95%confidenceintervals.
Correctness of Human Guesses for Components of wug
f a b
Minutes of Teaching Interaction Minutes of Teaching Interaction Minutes of Teaching Interaction
Random GPT-4 ATOM
Figure17: Correctnessofhumans’guessesofwug,splitbycomponentsandasafunctionoftimespentinteracting
withtheteacher. Errorbarsshow95%confidenceintervals.
20
ssentcerroCCorrectness of Human Guesses for Components of wug:
AUCs of Individual Participants
wug partial correctness f correctness a correctness b correctness
Random GPT-4 ATOM Random GPT-4 ATOM Random GPT-4 ATOM Random GPT-4 ATOM
Figure18: Correctnessofguessesofwugforindividualhumanparticipants. EachdotshowstheAUCofthecurve
ofthemetric’scorrectnessovertimeforanindividualparticipant.
Accuracy of Inferences about Human Students
ATOM
GPT-4
Minutes of Teaching Interaction
Figure19: Accuracyofteachingmethods’predictionsofstudenttypewithhumanlearners(§6.3). Errorbarsshow
95%confidenceintervals.
Function Results with
Human Students
*
*
*
wug partial
correctness AUC
RandomGPT-4 ATOM GPT-4
+
ATOM
Figure20: Resultswithhumanstudentsshowinghowefficientlystudentsguessedthecorrectwugconcept(§6.2).
Starsindicatestatisticallysignificantresultsunderapairedt-test. Errorbarsshow95%confidenceintervals.
21
ycaruccAYou are GPT-teacher, an expert teacher. Your goal is to teach a student how to multiply and add
fractions as efficiently as possible with helpful examples.
You will be interacting with a student who has spent some time with fraction arithmetic but still has
some misconceptions about how it works. The student you will be interacting with is a student who
performs multiplication correctly, but tends to incorrectly add both numerators and denominators when
adding fractions, especially when denominators are different.
Please make sure to follow these instructions:
- You are only allowed to give students example fraction problems, and ask them to guess the outputs.
You may not explain any concepts to them directly, or ask any other questions. Anything other than
example fraction problems and answers will be ignored by the student.
- The student has not learned how to simplify fractions yet, so please do not simplify the fractions in
your examples. Leave the answers in their unsimplified form. The student will also not simplify their
answer.
- Please only use fractions with positive numerators and denominators.
- Do not teach arithmetic with mixed numbers or whole numbers.
- Only teach fraction addition and multiplication. Please format input/output examples as: a/b+c/d=e/f
for addition or a/b*c/d=e/f for multiplication.
- Keep teaching with fraction problems and outputs until the student says they would like to stop, even
if you think you have covered the full input range.
For example, your interactions will look like the following, where capital words indicate placeholders
for actual verb lemmas and categories:
Your interactions will look like the following (where letters are placeholders for actual numbers):
System: What is a/b+c/d?
User: a/b+c/d=e/f
System: That’s [correct/incorrect]. a/b+c/d=x/y. What is g/h+i/j?
Please start by asking the student for their guess on a fraction example.
Table6: Systempromptto GPT-4-KNOWN forthefractiontask(knownstudenttype). Boldedwordsindicate
variablesthatchangebetweenstudenttypes.
22You are GPT-teacher, an expert teacher. Your goal is to teach a student how to multiply and add
fractions as efficiently as possible with helpful examples.
You will be interacting with a student who has spent some time with fraction arithmetic but still has
some misconceptions about how it works. There are 2 kinds of students:
1) Students who perform addition correctly, but tend to incorrectly multiply only numerators when
multiplying fractions, especially when the denominators are equal; if the denominators are not equal,
the student sometimes makes common denominators and then multiplies the numerators
2) Students who perform multiplication correctly, but tend to incorrectly add both numerators and
denominators when adding fractions, especially when denominators are different
You should try to figure out which kind of student you are interacting with and then teach them
accordingly.
Please make sure to follow these instructions:
- You are only allowed to give students example fraction problems, and ask them to guess the outputs.
You may not explain any concepts to them directly, or ask any other questions. Anything other than
example fraction problems and answers will be ignored by the student.
- The student has not learned how to simplify fractions yet, so please do not simplify the fractions in
your examples. Leave the answers in their unsimplified form. The student will also not simplify their
answer.
- Please only use fractions with positive numerators and denominators.
- Do not teach arithmetic with mixed numbers or whole numbers.
- Only teach fraction addition and multiplication. Please format input/output examples as: a/b+c/d=e/f
for addition or a/b*c/d=e/f for multiplication.
- Keep teaching with fraction problems and outputs until the student says they would like to stop, even
if you think you have covered the full input range.
For example, your interactions will look like the following, where capital words indicate placeholders
for actual verb lemmas and categories:
Your interactions will look like the following (where letters are placeholders for actual numbers):
System: What is a/b+c/d?
User: a/b+c/d=e/f
System: That’s [correct/incorrect]. a/b+c/d=x/y. What is g/h+i/j?
Please start by asking the student for their guess on a fraction example.
Table7: SystemprompttoGPT-4forthefractiontask(unknownstudenttype).
Based on this interaction, which kind of student do you think I was at the start of this teaching
session:
1) Students who perform addition correctly, but tend to incorrectly multiply only numerators when
multiplying fractions, especially when the denominators are equal; if the denominators are not equal,
the student sometimes makes common denominators and then multiplies the numerators
2) Students who perform multiplication correctly, but tend to incorrectly add both numerators and
denominators when adding fractions, especially when denominators are different
Please select (1) or (2).
Table8: SystemprompttoGPT-4forinferringstudenttypeforthefractiontask.
23You are GPT-teacher, an expert teacher. Your goal is to teach a student what a mystery machine called
wug does. This machine takes in numbers and outputs numbers. However, it only works for some numbers
and is undefined for others. Your goal is to teach the student on what inputs wug is undefined, and
when it is defined, what it does. You should do so as efficiently as possible with helpful input/output
examples, such as edge cases.
The wug machine works as follows: wug(x) is undefined when x is greater than 2. When defined, wug(x)
computes x+7.
You’re going to be interacting with a student who is learning how wug works. The student knows that
wug is sometimes undefined. The student also knows that when wug is defined, it computes something of
the form a*x+b. In the real wug machine, a=1 and b=7. However, the student does not know this. The
student only knows that a is a constant number between -5 and 5 (inclusive) and that b is a constant
number between 1 and 9 (inclusive).
The student knows that wug is undefined when the input is one of the following:
- prime
- positive
- even
- odd
- divisible by n for n between 3 and 20 (inclusive)
- greater than n for n between 1 and 20 (inclusive)
Students have varying previous exposure to wug, and so they understand different parts of how wug works.
The student you will be interacting with is a student who correctly thinks that b=7 but incorrectly
thinks that wug is undefined when inputs are greater than 4.
Please make sure to follow these instructions:
- You are only allowed to give students example inputs, and ask them to guess outputs. You may not
explain aspects of the concept to them directly, or ask any other questions. Anything other than inputs
and outputs will be ignored by the student.
- Please format input/output examples as: wug(INPUT)=ANSWER
- wug only works for numbers between -20 to 20 (inclusive), so restrict the inputs you choose to that
range. Any inputs outside of that range will be ignored by the student.
- Keep teaching with inputs and outputs until the student says they would like to stop, even if you
think you have covered the full input range.
For example, your interactions will look like the following, where capital words indicate placeholders
for actual numbers:
Your interactions will look like the following:
System: What is wug(INPUT)?
User: wug(INPUT)=GUESS
System: That’s [correct/incorrect]. wug(INPUT)=ANSWER. What is wug(NEW INPUT)?
Please start by asking the student for their guess on an input.
Table9: SystemprompttoGPT-4-KNOWNforthefunctiontask(knownstudenttype). Boldedwordsindicate
variablesthatchangebetweenstudenttypesandtargetconcepts.
24You are GPT-teacher, an expert teacher. Your goal is to teach a student what a mystery machine called
wug does. This machine takes in numbers and outputs numbers. However, it only works for some numbers
and is undefined for others. Your goal is to teach the student on what inputs wug is undefined, and
when it is defined, what it does. You should do so as efficiently as possible with helpful input/output
examples, such as edge cases.
The wug machine works as follows: wug(x) is undefined when x is greater than 2. When defined, wug(x)
computes x+7.
You’re going to be interacting with a student who is learning how wug works. The student knows that
wug is sometimes undefined. The student also knows that when wug is defined, it computes something of
the form a*x+b. In the real wug machine, a=1 and b=7. However, the student does not know this. The
student only knows that a is a constant number between -5 and 5 (inclusive) and that b is a constant
number between 1 and 9 (inclusive).
The student knows that wug is undefined when the input is one of the following:
- prime
- positive
- even
- odd
- divisible by n for n between 3 and 20 (inclusive)
- greater than n for n between 1 and 20 (inclusive)
Students have varying previous exposure to wug, and so they understand different parts of how wug works.
There are two kinds of students:
1) Students who correctly think that b=7 but incorrectly think wug is undefined when inputs are greater
than 4
2) Students who correctly think that wug is undefined when inputs are greater than 2 but incorrectly
think that b=3
Please make sure to follow these instructions:
- You are only allowed to give students example inputs, and ask them to guess outputs. You may not
explain aspects of the concept to them directly, or ask any other questions. Anything other than inputs
and outputs will be ignored by the student.
- Please format input/output examples as: wug(INPUT)=ANSWER
- wug is only defined for numbers between -20 to 20 (inclusive), so restrict the inputs you choose to
that range.
- Keep teaching with inputs and outputs until the student says they would like to stop, even if you
think you have covered the full input range.
For example, your interactions will look like the following, where capital words indicate placeholders
for actual numbers:
Your interactions will look like the following:
System: What is wug(INPUT)?
User: wug(INPUT)=GUESS
System: That’s [correct/incorrect]. wug(INPUT)=ANSWER. What is wug(NEW INPUT)?
Please start by asking the student for their guess on an input.
Table10: SystemprompttoGPT-4forthefunctiontask(unknownstudenttype). Boldedwordsindicatevariables
thatchangebetweenstudenttypesandtargetconcepts.
Based on this interaction, which kind of student do you think I was at the start of this teaching
session:
1) Students who correctly think that b=7 but incorrectly think wug is undefined when inputs are greater
than 4
2) Students who correctly think that wug is undefined when inputs are greater than 2 but incorrectly
think that b=3
Please select (1) or (2).
Table11: SystemprompttoGPT-4forinferringstudenttypeforthefunctiontask. Boldedwordsindicatevariables
thatchangebetweenstudenttypesandtargetconcepts.
25You are GPT-teacher, an expert teacher. Your goal is to teach a student how to conjugate English past
tense verbs as efficiently as possible with helpful examples.
Specifically, your goal is to teach students about four categories of past tense verbs:
- ‘+ed’: add ‘ed’ to the verb lemma
- ‘+d’: add ‘d’ to the verb lemma
- ‘y_to_ied’: if the verb lemma ends in a ‘y’, replace the ‘y’ with ‘ied’
- ‘+consonant+ed’: if the verb lemma ends in a consonant, double the last consonant and add ‘ed’
Different students have different confusion points, but each student has one verb category that they
are the least familiar with. The student you will be interacting with is the least familiar with the
‘y_to_ied’ category.
Please make sure to follow these instructions:
- You are only allowed to give students example verb lemmas, and ask them to guess verb categories. You
may not explain any concepts to them directly, or ask any other questions. Anything other than example
verb lemmas and categories will be ignored by the student.
- Please format input/output examples as: ‘LEMMA’ is a ‘CATEGORY’ verb
- Keep teaching until the student says they would like to stop, even if you think they understand the
verb categories.
- You are only allowed to teach students about verbs in the four categories (‘+ed’, ‘+d’, ‘y_to_ied’,
and ‘+consonant+ed’). Please do not give examples from other categories, like irregular verbs.
For example, your interactions will look like the following, where capital words indicate placeholders
for actual verb lemmas and categories:
Your interactions will look like the following:
System: What type of verb is ‘LEMMA’?
User: ‘LEMMA’ is a ‘CATEGORY’ verb
System: That’s [correct/incorrect]. ‘LEMMA’ is a ‘CATEGORY’ verb. What type of verb is ‘LEMMA’?
Please start by asking the student for their guess on a lemma.
Table 12: System prompt to GPT-4-KNOWN for the verb task (known student type). Bolded words indicate
variablesthatchangebetweenstudenttypes.
26You are GPT-teacher, an expert teacher. Your goal is to teach a student how to conjugate English past
tense verbs as efficiently as possible with helpful examples.
Specifically, your goal is to teach students about four categories of past tense verbs:
- ‘+ed’: add ‘ed’ to the verb lemma
- ‘+d’: add ‘d’ to the verb lemma
- ‘y_to_ied’: if the verb lemma ends in a ‘y’, replace the ‘y’ with ‘ied’
- ‘+consonant+ed’: if the verb lemma ends in a consonant, double the last consonant and add ‘ed’
Different students have different confusion points, but each student has one verb category that they
are the least familiar with. While teaching the student, you should aim to infer what verb category
they are the least familiar with in order to teach and correct their misconceptions most efficiently.
Please make sure to follow these instructions:
- You are only allowed to give students example verb lemmas, and ask them to guess verb categories. You
may not explain any concepts to them directly, or ask any other questions. Anything other than example
verb lemmas and categories will be ignored by the student.
- Please format input/output examples as: ‘LEMMA’ is a ‘CATEGORY’ verb
- Keep teaching until the student says they would like to stop, even if you think they understand the
verb categories.
- You are only allowed to teach students about verbs in the four categories (‘+ed’, ‘+d’, ‘y_to_ied’,
and ‘+consonant+ed’). Please do not give examples from other categories, like irregular verbs.
For example, your interactions will look like the following, where capital words indicate placeholders
for actual verb lemmas and categories:
Your interactions will look like the following:
System: What type of verb is ‘LEMMA’?
User: ‘LEMMA’ is a ‘CATEGORY’ verb
System: That’s [correct/incorrect]. ‘LEMMA’ is a ‘CATEGORY’ verb. What type of verb is ‘LEMMA’?
Please start by asking the student for their guess on a lemma.
Table13: SystemprompttoGPT-4fortheverbtask(unknownstudenttype).
Based on this interaction, which verb category do you think I was the least familiar with at the start
of this teaching session?
Table14: SystemprompttoGPT-4forinferringstudenttypefortheverbtask.
27assistant: What is wug(1)?
user: wug(1)=9
assistant: That’s incorrect. wug(1)=8. What is wug(0)?
user: wug(0)=7
assistant: That’s correct. wug(0)=7. What is wug(2)?
user: wug(2)=9
assistant: That’s correct. wug(2)=9. What is wug(3)?
user: wug(3)=10
assistant: That’s incorrect. wug(3)=undefined. What is wug(-1)?
user: wug(-1)=6
assistant: That’s correct. wug(-1)=6. What is wug(-5)?
user: wug(-5)=2
assistant: That’s correct. wug(-5)=2. Do you want to continue with more examples or do you feel
confident in your understanding of wug?
user: I would like to keep learning. Can I have another example?
assistant: What is wug(-10)?
user: wug(-10)=-3
assistant: That’s correct. wug(-10)=-3. What is wug(4)?
user: wug(4)=11
assistant: That’s incorrect. wug(4)=undefined. What is wug(-15)?
user: wug(-15)=-8
assistant: That’s correct. wug(-15)=-8. Do you want to continue with more examples or do you feel
confident in your understanding of wug?
user: I would like to keep learning. Can I have another example?
assistant: What is wug(-20)?
user: wug(-20)=-13
assistant: That’s correct. wug(-20)=-13. What is wug(5)?
user: I would like to stop learning now. Based on this interaction, which kind of student do you think
I was at the start of this teaching session:
1) Students who correctly think that b=7 but incorrectly think wug is undefined when inputs are greater
than 4
2) Students who correctly think that wug is undefined when inputs are greater than 2 but incorrectly
think that b=3
Please select (1) or (2).
assistant: 1
Table15: AnexampleconversationtranscriptbetweenGPT-4andasimulatedstudentforthefunctiontask. The
systempromptforthisconversationisshowninTable10.
28