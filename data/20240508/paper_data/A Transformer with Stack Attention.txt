A Transformer with Stack Attention
JiaodaLi1 JenniferC.White2 MrinmayaSachan1 RyanCotterell1
1ETHZürich 2UniversityofCambridge
{jiaoda.li,mrinmaya.sachan,ryan.cotterell}@inf.ethz.ch jw2088@cam.ac.uk
Abstract goes further and argues that snippets of Swiss
GermanareevenhigherontheChomksyhierarchy.
Naturallanguagesarebelievedtobe(mildly) The scientific question treated in this paper is
context-sensitive. Despite underpinning re-
whetherthereexistsaminimalmodificationtothe
markablycapablelargelanguagemodels,trans-
transformerarchitecturethatdoesallowittolearna
formers are unable to model many context-
largerswatheoftheformallanguagesmostclosely
free language tasks. In an attempt to ad-
associatedwithhuman language. Specifically, in
dressthislimitationinthemodelingpowerof
transformer-based language models, we pro- this paper, we augment the transformer architec-
pose augmenting them with a differentiable, ture with a novel stack attention mechanism that
stack-based attention mechanism. Our stack- enablesittolearncertainCFlanguagesempirically.
basedattentionmechanismcanbeincorporated Ourstackattentionmechanismsimulatesastackby
intoanytransformer-basedlanguagemodeland
maintainingaprobabilitydistributionoverwhich
addsalevelofinterpretabilitytothemodel.We
ofthesubsequentlyobservedtokensisatthetopel-
show that the addition of our stack-based at-
ementofthestack. Inturn,thisprobabilitydistribu-
tentionmechanismenablesthetransformerto
modelsome,butnotall,deterministiccontext- tionservesasanattentionmechanism. Compared
freelanguages. to DuSell and Chiang (2024), which also applies
stack augmentation to the transformer, our stack
https://github.com/rycolab/
attentionismorespaceefficientandallowsforeas-
stack-transformer
ierinterpretationthroughvisualizingtheattention
weights. We incorporate our innovation into the
1 Introduction
transformerbyaddingastackattentionsub-layer
Languagemodels(LMs)basedonthetransformer toeachlayer,ratherthancompletelyreplacingthe
architecture (Vaswani et al., 2017) have shown standardattention. Augmentingmodelsinamodu-
great empirical success at a wide range of NLP larwaylikethisallowsfordirectintegrationwith
tasks (Devlin et al., 2019; Radford et al., 2019; pre-trainedtransformer-basedLMs.
Liu et al., 2020; Brown et al., 2020). However, We evaluate our stack-augmented transformer
recent theoretical (Hahn, 2020; Angluin et al., throughcomparisonwithastandardtransformeron
2023) and empirical (Ebrahimi et al., 2020; fourDCFtaskstakenfromDelétangetal.(2023).
Bhattamishra et al., 2020; Delétang et al., 2023) Wefindthatthestack-augmentedtransformerper-
research suggests that language models based formssubstantiallybetterthanthestandardtrans-
on transformers show difficulty in learning basic formerontwoofthefourDCFtasks. Nevertheless,
algorithmic patterns. A prime example is the incontrasttoDuSellandChiang(2024),whoclaim
Dyck-n language, i.e., the language of balanced theirarchitecturecanrecognizetheentireclassof
parenthesesofdepthď n. Whenn ą 1,ithasbeen CFlanguages,wefindtransformerswithourstack
argued that transformers are theoretically (Hahn, attention still struggle on two DCF tasks that in-
2020) and empirically (Ebrahimi et al., 2020) volvemodulararithmetic.
unabletolearnaDyck-nlanguage. Additionally,
2 Preliminaries
Delétangetal.(2023)reportthattransformer-based
LMs fail to learn four deterministic context-free Inthissection,weprovidethenecessarytechnical
(DCF)tasks. Theauthorsofthisworkcontendthat backgroundforourexposition. Wefirstreviewthe
the resolution of this insufficiency is paramount self-attention mechanism and then introduce the
if human-level language understanding is to transformerarchitecture.
be achieved by computers. Indeed, Chomsky
2.1 TheSelf-AttentionMechanism
(1956)famouslyarguesthathumanlanguagehas
many context-free traits; see also Chomsky and Theattentionmechanism(Bahdanauetal.,2015)
Schützenberger(1963). Moreover,Shieber(1987) is the fundamental building block of the trans-
4202
yaM
7
]LC.sc[
1v51540.5042:viXraformerarchitecture(Vaswanietal.,2017),which One drawback of the permutation invariance,
wediscussinthenextsection. Onecommonform however,isthatAisnotalinguisticallyplausible
of attention is self-attention (Cheng et al., 2016; mechanism as human language is decidedly not
Parikh et al., 2016). Our construction of a stack- permutationinvariant. Thisproblemisaddressed
augmentedattentionmechanismisamodification through theincorporation ofattention masks and
ofthisself-attentionmechanism. positionalencodings(Vaswanietal.,2017,§3.5)in
The premise of self-attention is as follows. A thetransformerarchitecture,aswediscussin§2.2.
sentence representation H “ rh ;...;h s P
1 N MaskedSelf-Attention. AnattentionmaskG P
RDˆN
isahorizontalconcatenationofcolumnvec-
BNˆN, where B “ t0,1u, can be applied to the
tors h in RD, where each column is a represen-
n
self-attentionsusingthefollowinggeneralization
tation of the nth word. Our goal is to construct a
distributionovertheindexsett1,...,Nu,denoted exppe qG
α pjq “ ř ij i,j (4)
asrNs. Wedosointhreesteps,describedbelow. i N exppe qG
n“1 in i,n
1 Thefirststepistoconstructareal-valued,pair- Attention masks allow for hard constraints on
wisecompatibilityscore. Themostcommonway whichindicescan beattendedtoby theattention
todothisisthrougha(scaled)dot-product,i.e., head. Acommonlyusedmaskingschemeisfuture
masking where each position is only allowed to
h h
e “def ?i ‚ j (1) attend to positions up to and including itself, i.e.,
ij
D
wedefinethefollowingmask
#
2 The second step is to take the pair-wise com-
1, n ă i
patibilityscoresandprojectthemontothesimplex G i,n “ (5)
0, n ě i
∆N´1 through the softmax. This results in the
followingdistribution Futuremaskingallowstransformerstobeusedin
autoregressivelanguagemodelsbypreventingthe
exppe q
α pjq “def ř ij (2) model from peeking at words that have yet to be
i N
exppe q
n“1 in generated,whichwedetailin§2.3.
which is termed the self-attention distribution. Queries, Keys, and Values. In the version of
NotethereareN self-attentiondistributionsα i. the attention mechanism introduced by Vaswani
etal.(2017),theattentionmechanismisaugmented
3 The third, and final, step is to construct
withadditionallinearprojections. Specifically,the
a weighted average of the representations
vectorsh arelinearlyprojectedtoconstructthree
H “ rh ;...;h s P RDˆN using the self- n
1 N newvectors,definedbelow
attentiondistributionasfollows
q “def W h (query) (6a)
ÿN n Q n
ApHq :,i “def α ipnqh n (3) k n “def W Kh n (key) (6b)
n“1
v “def W h (value) (6c)
n V n
where ApHq denotes the ith column of ApHq.
:,i
The function A : RDˆN Ñ RDˆN (for any N), whereW V,W Q,W K P RD1ˆD areparameterma-
asdefinedabove,iscalledanattentionhead. trices. Compatibility scores are then computed
betweenthecorrespondingquery–keypair:
Importantly, we see that A is a differentiable
q k
i ‚ j
function. Differentiability allows us to learn the e “ ? (7)
ij
D1
parameters of an attention head with gradient-
based methods. And, more importantly, it has a Usingthosecompatibilityscores,aself-attention
specificdesirableproperty—namely,itisinvariant distribution is constructed using the softmax. Fi-
withrespecttopermutationsofthecolumnsofH. nally, as before, a weighted sum of the values is
Computationally, this implies that ApHq :,i and computedusingtheself-attentiondistribution:
ApHq canbecomputedinparallelfori ‰ j. It
:,j ÿN
isspecificallythisformofparallelismthatgrants
ApHq “ α pnqv (8)
:,i i n
the transformer architecture its ability to scale.
n“1Multi-headSelf-Attention. Weadditionallyde- 2.3 ProbabilityModels
fine the multi-head self-attention mechanism. In
Next,wedescribetwonaturalwaysofconstructing
multi-head attention, we combine M attention
aprobabilitydistributionfromatransformer.
headsAp1q,...,ApMq asfollows
MaskedLanguageModeling. First,weconsider
ÿM
MpHq “def W pmqApmqpHq (9) the case of masked language modeling (MLM).
:,i O :,i
Masked language models perform the cloze task,
m“1
i.e.,theyfillinamissingwordgivenaleftandright
where W Opmq P RDˆD1 is the output projection context. Moreformally,considerastringw P Σ˚
matrixforheadApmq. Usually,wesetD1 “ D{M. oflengthT. Letw denotethetth symbolinw,let
t
w “ w ¨¨¨w , and let w “ w ¨¨¨w .
2.2 TheTransformerArchitecture ăt 1 t´1 ąt t`1 T
Weconstructwr “def w ăt[MASK]w
ąt
byreplacing
We now describe the transformer architecture. A
w
t
inw withamasktoken [MASK]. Thealphabet
transformer over a vocabulary Σ constitutes a is expanded to include [MASK]. We denote Σr “def
function of type1 ΣN Ñ RDˆN where a string ΣYt[MASK]u. The transformer HpLq is now of
w aR“ Dˆw N1¨ r¨ e¨ pw reN senP taΣ tiN ono wfl he en rg eth DN isi ts he en mco od de ed lsin izt eo
.
typeΣr N Ñ RDˆN. Amaskedlanguagegivesthe
followingprobabilitydistributionforpositiont
Thetransformerisdefinedcompositionallyovera
sequenceoflayers. First,wedefine ppwr | w ,w q
t ăt ąt
(12)
Hp0q “def Embedding`PE (10) “ softmaxpW PHpLqpwrq :,t`b Pq wr
t
where Embedding of type ΣN Ñ RDˆN is the
embedding layer and PE of type ΣN Ñ RDˆN where wr t P Σr , W P P R|Σr |ˆD and b P P R|Σr |.
In practice, multiple tokens may be masked and
isthepositionalencodingthatinjectsinformation
predictedsimultaneously.
about the relative or absolute position of tokens,
whichextinguishesthepermutationinvarianceof
AutoregressiveLanguageModeling. Incontrast
thetransformer. Eachtransformerlayerconsistsof
tomaskedlanguagemodeling,thegoalofautore-
two sub-layers: a multi-head self-attention M of
gressivelanguagemodeling(ALM)istoconstruct
typeRDˆN Ñ RDˆN andafullyconnectedfeed- a probability distribution over Σ˚. To do so, the
forwardnetworkFFNoftypeRDˆN Ñ RDˆN. A
followingfactorizationisemployed
residualconnectionisemployedaroundeachsub-
layer,followedbyalayernormalization(Baetal., źT
2016)LNoftypeRDˆN Ñ RDˆN : for0 ă ℓ ď ppwq “ pp[EOS] | wq ppw t | w ătq (13)
L,wehavethefollowingrecursivedefinition t“1
´ ´ ¯ ¯
Hpℓq “def LN M Hpℓ´1q `Hpℓ´1q (11a) Everylocalconditionaldistributionppw t | w ătqis
M ´ ´ ¯ ¯ definedoverthesetΣ “def ΣYt[EOS]uandw
ă1
“def
Hpℓq “def LN FFN Hpℓq `Hpℓq (11b) [BOS],where[BOS],[EOS] R Σ.2 Inatransformer-
FFN M M
based autoregressive language model, each local
Hpℓq “def Hpℓq (11c)
FFN conditionalisparameterizedas
whereHp Mℓq ,Hp Fℓ Fq N andHpℓq arefunctionsoftype ppw t | w ătq
ΣN Ñ RDˆN for any N. They have w as input, (14)
“ softmaxpW FpLqpwq `b q
whichweomitforbrevitywhenthecontextisclear. P :,t´1 P wt
Future-maskedTransformer. Ifthefuturemask wherew P Σ,FpLqisafuture-maskedtransformer,
t
in Eq. (5) is used in every Hpℓq , we call such a W P R|Σ|ˆD andb P R|Σ|.
M P P
transformerfuture-maskedtransformer,denoted
as FpLq. As we will see, future-masked trans- 3 ATransformerwithStackAttention
formersarenecessarytoconstructautoregressive
RecentlyDelétangetal.(2023)showedthattrans-
languagemodels,whichcannotpeekatthefuture.
formers are not able to learn several non-regular
1Type-theoretically,N isaparameterofthetype. Thus,
ourtypesignatureisadependenttype(UnivalentFoundations 2This means that the transformer is a function of type
Program,2013) ΣN`1 ÑRDˆpN`1qwhereΣd“efΣYt[BOS]u.Action Stack Attention StackAttentionα
[BOS]a b c α “ r1,0,0,0sJ
0
PUSHa a [BOS]a b c α 1 “ r0,1,0,0sJ
PUSHb b a [BOS]a b c α 2 “ r0,0,1,0sJ
PUSHc c b a [BOS]a b c α 3 “ r0,0,0,1sJ
ř
POP b a [BOS]a b c α 4 “ 3 j“1α 3pjqα j´1 “ α 2 “ r0,0,1,0sJ
NO-OP b a [BOS]a b c α 5 “ α 4 “ r0,0,1,0sJ
ř
POP a [BOS]a b c α 6 “ 5 j“1α 5pjqα j´1 “ α 1 “ r0,1,0,0sJ
Figure1: Anexampleillustratinghowattentionscanemulatestacks. Thefirstcolumnliststheoperationperformed
ateachtimestep. Thesecondcolumnpresentsthestackcontentsafterperformingtheoperation. Thethirdcolumn
showsahardattentionovertheinputtokens. Thepointeroftheattentionindicatesthecurrentstacktop. Thelast
columnistheproposedstackattention.
DCFlanguages. Inspiredbythefactthatpushdown • TheoperationPOP: rNs˚ Ñ rNs˚ removesthe
automata(Oettinger,1961;Schützenberger,1963), top-most element of the stack and is formally
automata that employ a single stack, can model definedasfollows:
CFlanguages(Evey,1963),weintroduceanovel
stack attention mechanism that emulates the POPpεq “ ε (17a)
functionality of a stack and integrate it into the POPpγ ¨¨¨γ q “ γ ¨¨¨γ (17b)
1 T 1 T´1
transformer architecture, aiming to enable it to
learnsomeCFlanguages.
WewillusethisdefinitioninTheorem3.1toargue
thatourstackattentionmechanismcanbeformally
3.1 StacksovertheIndexSet
viewed as a type of stack. Additionally, we will
Wefirstgiveaformaldefinitionofastack. Inour assume an operator PEEK : rNs˚ Ñ prNsYt0uq
paper, we define a stack as a data structure over
thatdoesnotalterthestateofthestack,butrather
the index set rNs. The state of a stack is a string
returnsthetopelement(or0ifthestackisempty).
γ P rNs˚ of indices. There are three operations
Wedefineitbelow
thatwecanperformthatalterthestateofthestack.
Wedescribeeachoperationbelowintermsofγ. PEEKpεq “ 0 (18a)
• TheoperationPUSH: rNs˚ˆrNs Ñ rNs˚ adds PEEKpγ 1¨¨¨γ Tq “ γ T (18b)
anelementtothetopofthestackandisformally
3.2 StackAttention
definedasfollows:
Wenowformallydefineourstackattentionmecha-
PUSHpγ,γq “ γγ (15) nism. Weintroduceabeginning-of-sequencesym-
bol [BOS] atthezerothposition,designatedtorep-
• TheoperationNO-OP: rNs˚ Ñ rNs˚ leavesthe resentanemptystack. Eachpositioni P t0uYrNs
stackunchangedandisdefinedasfollows: isassignedadistinctstackα i P RN`1. Wewrite
α pjq to denote the pj ` 1qth value in α , for
i i
NO-OPpγq “ γ (16) 0 ď j ď N. The stacks are defined inductively.The initial stack, α , is constructed to attend to 3.3 AStackTransformer
0
[BOS] asfollows
The stack is incorporated into the transformer by
α “def r1,0,0,¨¨¨sJ P RN`1 (19) inserting a third sub-layer in each transformer
0
layerafterthestandardattentionandfeedforward
Subsequentstacksarecomputedinductivelybased
layersdefinedinEq.(11a)andEq.(11b):
on the stack contents and the operations (PUSH,
´ ¯
NO-OP, POP) taken at previous timesteps.3 The Hpℓq “def S Hpℓq `Hpℓq (26a)
three stack operations are defined for i ě 1 as S FFN FFN
follows: Hpℓq “ Hpℓq (26b)
S
• PUSH pushes the hidden state of the current po-
Similar to other sub-layers, we also employ a
sition,sowejustsettheattentionweightatthe
residual connection by summing the output of
currentpositiontobe1andtheresttobe0,i.e.,
#
the stack attention mechanism S with its input,
1 j “ i
αpPUSHq pjq “def (20) allowingthemodeltobypassthestackifneeded.
i
0 otherwise Layernormalizationcanalsobeused,butweomit
due to initial results in preliminary experiments.
• NO-OP leaves the previous stack unchanged, so
Because the rest of the model is left unchanged,
thestackfromthelasttimestepispassedforward
it can be directly integrated into pre-trained
withnomodification,i.e.,wehave
language models to augment their ability to
αp iNO-OPq “def α i´1 (21) processhierarchicalsyntacticstructures.
• POPremovesthetopelement,andbacktracksto 3.4 ComputationalOverhead
thesecondelementinthestack,i.e.,wehave
« ff Time. The computation is bottlenecked by the
iÿ´1
POPoperation,whichsumsoverpreviousthepre-
αpPOPq “def α pjqα `α p0qα
i i´1 j´1 i´1 0 viouspositionsandtherebyhasatimecomp`lexit˘y
j“1 of pNq. The total time complexity is N2 .
(22) O O
In contrast to standard attention, stack attention
The first term on the right-hand side retrieves hastobecomputedsequentially,whichbreaksthe
thesecondelementandiszeroedoutwheni “ parallelizability of the transformer and makes it
1. The second term accounts for the case of an substantially slower in practice. However, a and
empty stack—POP cannot be performed on an theoutputSpHqcanstillbecomputedinparallel.
emptystackandinthiscaseitisequivalenttoa Thus, α is a function of the stack operations but
NO-OP. notofthehiddenstates. Itfollowsthatifstructural
The stack attention α at position i is computed supervision of the stack operations is provided,
i
asasuperpositionofthethreeoperations: e.g., as in Sartran et al. (2022) and Murty et al.
(2023), α for all i P rNs can be pre-computed,
α “def a pPUSHq¨αpPUSHq `a pPOPq¨αpPOPq i
i i i i i andtheentiremodelcanbeparallelized.
(23)
`a
pNO-OPq¨αpNO-OPq
i i Space. The stack attention stores N ` 1 at-
where a
i
P ∆2 is a probability distribution over tentions of size N, s`o th˘e space complexity is
possibleoperations “ tPUSH,POP,NO-OPu. This ppN `1qNq “ ` N2 ˘ . This is an improve-
A O O
distributionisdeterminedby: ment over the DN2 space complexity of
O
a “def softmaxpW h `b q (24) DuSellandChiang’s(2024)method.
i A i A
where W P R3ˆD and b P R3 are learned 3.5 TheDualityofStackAttention
A A
parameters. Stackattentionisbothastackovertheindexset,as
Afterobtainingthestackattentionweights,we definedin§3.1,andanattentionmechanism. Inthe
can compute the top element as a weighted sum followingtheorem,wemakeprecisethemannerin
justlikestandardself-attention: whichourstackattentionisastack.
ÿN
SpHq “def α pnqh (25) Notation. We use the symbol υ i to refer to an
:,i i n
operationfromthesettPUSH p¨q,NO-OPp¨q,POPp¨qu
n“0 i
3Weusethetermstimestepandpositioninterchangeably. ateverytimestepi. NotethatPUSH,asdefinedinRNN TransformerMLM TransformerALM
Task
Vanilla Stack Vanilla Stack Vanilla Stack
RS 81.0˘0.8 100.0˘0.0 54.8˘0.0 100.0˘0.0 55.4˘0.8 100.0˘0.0
SM 73.2˘1.0 100.0˘0.0 50.4˘0.1 93.1˘4.4 50.4˘0.1 92.8˘2.6
MA 75.8˘4.3 91.0˘6.3 30.1˘0.0 34.3˘1.4 30.2˘0.1 29.5˘0.6
SE 56.7˘10.3 89.9˘7.2 20.0˘0.0 29.8˘8.0 20.2˘0.1 20.3˘0.2
Table1: Accuracies(%)ofthetransformerandRNNwithoutandwithstacksonDCFtasks.
§3.1,isafunctionoftwoarguments. However,we only perform counting up to a threshold, but not
definePUSH pγq “def PUSHpγ,iq,i.e.,wepushi,the modulocounting(McNaughtonandPapert,1971).
i
index,tothestack. Weintroduceafunction of Recently,Angluinetal.(2023)showedthatthe
type t0uY rNs Ñ BN`1 that converts an in(cid:74)(cid:75) dex classoflanguagesrecognizablebytransformeren-
into its one-hot encoding, a column vector with coders with hard attention, strict future masking,
zeros everywhere except the given index, where and no positional encodings, are exactly the star-
thevalueissettoone. free languages. Building on this result, we con-
Theorem 3.1. Let υ ,...,υ be a se- jecturethatthereexistnon-star-freelanguagesthat
1 N
ries of stack operations where υ P arebeyondthecapabilityofatransformerencoder
i
tPUSH p¨q,NO-OPp¨q,POPp¨qu for all i P rNs. with(hard)stackattentionandnopositionalencod-
i
Furthermore, supposea pυ q “ 1forall i P rNs. ings. This conjecture is supported by our experi-
i i
Then, PEEKpυ p¨¨¨υ pεqqq “ α for all mentsin§4.3whereweshowthatstack-augmented
i 1 i
i P t0uY(cid:74) rNs. (cid:75) transformersalsofailtolearntwotasksinvolving
modulocounting. Wehopetoconstructaproofof
Proof. AppendixA ■
anexpressivityresultinfuturework.
Our stack-based attention is also an attention
4 DeterministicCFTasks
mechanism in the sense that it maintains a
distributionovert0uYrNs. Wemakethisnotion Wenowdiscussseveraltasksthatareencodableby
preciseaswellinthefollowingtheorem. deterministiccontext-freegrammars.
Theorem 3.2. Consider a sequence of stack
4.1 Tasks
a řttention mechanisms α 0,...,α N. Then,
N α pnq “ 1foralli P t0uYrNs. AllfourtasksweconsiderarederivedfromDelé-
n“0 i
tang et al. (2023) and are language transduction
Proof. AppendixA ■ tasks. Every word from the input language x P
Σ ˚ is mapped to a word in the output language
3.6 Expressivity I
y P Σ ˚ by means of a function f: Σ ˚ Ñ Σ ˚.
O I O
Weleavetheexactcharacterizationoftheexpres- To convert a transduction task to a language ac-
sivity of our stack-augmented transformer for ceptance task, a language is constructed over the
futurework. However,weconjecturethatitcannot alphabetΣ “ Σ YΣ asfollows
I O
model all the context-free languages without ! )
positionalencodings. Sucharesultwouldmirror xfpxq | x P Σ ˚ Ď Σ˚ (27)
I
thatofAngluinetal.(2023).
Tocontextualizethisconjecture,wefirstreview To experiment with this setup, in the case of an
the star-free languages. The star-free languages MLM, the input wr is x appended with |y| mask
areregularlanguagesdefinablebyaregularexpres- tokens [MASK]. We then use the transformer to
sionwithoutKleenestarbutwithcomplement(Mc- predictallthemaskedtokensatonceandevaluate
NaughtonandPapert,1971). Theycanalsobechar- thepredictedstringy1againsty “ fpxq. Likewise,
acterized by finite-state automata with aperiodic inthecaseofanALM,givenaprefixx,wesam-
transformation monoids (Schützenberger, 1965), ple y t1 „ pp¨ | xy ătq autoregressively, where y t1
alsotermedcounter-freeautomataorpermutation- denotesthetth symbolofy1 andy ăt “ y 1¨¨¨y t.4
freeautomata(McNaughtonandPapert,1971). It
4Theabovenotationisnotatypo!FollowingDelétangetal.
hasbeenshownthatacounter-freeautomatoncan (2023), each conditional distribution pp¨ | xy ătq actuallyTask Transformer none sincos relative rotary ALiBi
Vanilla 54.8˘0.0 50.7˘0.2 67.6˘2.2 55.4˘1.2 79.4˘3.5
RS
Stack 100.0˘0.0 99.1˘1.7 100.0˘0.0 86.3˘15.0 100.0˘0.0
Vanilla 50.4˘0.1 49.5˘0.6 67.5˘1.0 52.1˘1.8 70.9˘1.2
SM
Stack 93.1˘4.4 74.7˘8.8 98.5˘1.1 73.1˘4.5 92.9˘2.7
Vanilla 30.1˘0.0 30.1˘0.0 30.1˘0.0 30.1˘0.0 30.1˘0.0
MA
Stack 34.3˘1.4 33.8˘0.8 35.0˘1.1 34.5˘1.3 34.7˘1.1
Vanilla 20.0˘0.0 20.0˘0.0 20.0˘0.0 20.0˘0.0 20.0˘0.0
SE
Stack 29.8˘8.0 23.9˘3.0 25.2˘1.8 30.0˘3.8 27.9˘5.8
(a)MLM
Task Transformer none sincos relative rotary ALiBi
Vanilla 55.4˘0.8 55.2˘0.7 62.0˘6.1 72.9˘3.5 57.1˘0.6
RS
Stack 100.0˘0.0 96.8˘4.5 100.0˘0.0 100.0˘0.0 100.0˘0.0
Vanilla 64.9˘2.0 60.8˘3.1 70.5˘0.9 71.9˘0.9 70.5˘1.6
SM
Stack 92.8˘2.6 49.6˘4.4 93.2˘2.3 83.8˘1.7 93.4˘1.2
Vanilla 30.2˘0.1 25.7˘2.3 30.3˘0.1 26.0˘0.8 30.3˘0.1
MA
Stack 30.0˘0.1 28.0˘2.8 30.3˘0.3 25.6˘0.3 30.3˘0.1
Vanilla 20.2˘0.1 20.2˘0.3 20.7˘0.2 20.3˘0.2 20.5˘0.3
SE
Stack 20.3˘0.2 20.2˘0.1 20.7˘0.1 20.2˘0.3 20.3˘0.1
(b)ALM
Table2: Performancecomparisonofavanillaandstacktransformerwithdifferentpositionalencodings.
AsinthecaseofMLM,weevaluatethepredicted whichareignoredwhenaccuracyiscomputed. We
y1 againstthey “ fpxq. Wefollowthechoicesof have Σ
I
“ ta,b,rPUSHas,rPUSHbs,rPOPsu and
Delétangetal.(2023)forΣ
I
andΣ
O
Σ
O
“ ta,b,[PAD]u. Anexampleisgivenbelow.
ReverseString(RS). Thefirsttaskistocompute Example:
the reverse of an input string, i.e., f pxq “ xR.
RS
Inthistask,wetakeΣ “ Σ “ ta,bu.Wegivean x “ babrPOPsrPUSHasrPUSHbs
I O
examplebelow. y “ baab[PAD][PAD][PAD]
Example:
Modular Arithmetic (MA). In the third task,
x “ abb weconsideratransductiontaskbasedonmodular
arithmetic. Analgebraicexpressionxconsistsof
y “ bba
fivenumericalconstantst0,1,2,3,4u,threeoper-
ationst`,´,¨u,bracketstp,qu,andacongruence
Stack Manipulation (SM). In the second task,
sign t”u. We say two integers are congruent if
the input string x consists of a stack of two
andonlyifapre-setmodulusisadivisoroftheir
symbols ta,bu, printed from bottom to top, and
difference. Inthistask,wesetthemodulusto5,so
a sequence of stack operations drawn from the
thefunctionf evaluatestheexpressionmodulo
set trPUSHas,rPUSHbs,rPOPsu. The function MA
5. WehaveΣ “ t0,1,2,3,4,`,´,¨,p,q,”uand
f pxq outputs the final stack after all the given I
SM
Σ “ t0,1,2,3,4u. Anexampleisgivenbelow.
operationsareexecutedsequentiallyontheinput O
stack,printedfromtoptobottom. Wealwayshave Example:
|y| “ |x| ` 1. If the final stack has fewer ele-
mentsthan|y|,itwillbepaddedwith[PAD]tokens, x “ p1`2q¨3 ”
conditionsonthetruey ăt. y “ 4Solve Equation (SE). In our fourth and fi- last two tasks (MA) and SE require the ability to
nal task, we consider a transduction task that perform modular arithmetic, which makes them
solves equations over a single variable, which non-star-free, as discussed in §3.6. Additionally,
we denote z. The input string x is a modular Feng et al. (2023) also directly prove that the
equation with five constants t0,1,2,3,4u, two transformercannotperformmodulararithmetic.
operations t`,´u, brackets tp,qu, a congruence
4.3.2 PositionalEncodings
sign t”u, and a variable tzu. The modulus
Inthissection,weaddvariouspositionalencodings
is set to 5. The function f solves this equa-
SE
tothetransformerandinvestigatetheireffect. We
tion and returns the value of the variable. We
considerfivedifferentpositionalencodings: none,
have Σ “ t0,1,2,3,4,`,´,p,q,”,zu and
I
sincos, relative, rotary, and ALiBi; see Ap-
Σ “ t0,1,2,3,4u. Anexampleisgivenbelow.
O
pendixCformoredetails. Asourstackattention
Example: is computed inductively, positional information
isalreadypresentinthemodel,reducingtheneed
x “ p1`zq`2 ” 2
forpositionalencodings. ThisisevidentinTab.2,
y “ 4 where including positional encodings generally
has a negative impact on the stack transformer’s
4.2 ExperimentalSetup performance. Most notably, sincos and rotary
heavily degrade stack transformer’s performance
Following Delétang et al. (2023), we experiment
onRSandSM.However,relativeconstitutesan
withatransformerwiththenumberoflayersL “ 5
exception, as it results in improved performance
and the model size D “ 64. Unless otherwise
onSM.Incontrast,withthestandardtransformer
specified, no positional encodings are used. We
architecture, positional encodings do seem to
discusstheeffectofvariouspositionalencodings
help on star-free tasks. The largest improvement
in§4.3.2. Lengthgeneralizationhasbeenthefocus
comesfromALiBiintheMLMsettingandrotary
ofmanypapersinthislineofresearch(Joulinand
in the ALM setting. Nevertheless, none of the
Mikolov,2015;Delétangetal.,2023). Wefollow
investigatedpositionalencodingsareabletoboost
suittotrainoninputstringsxwith1 ď |x| ď 40
theperformanceofvanillatransformertoanywhere
and test on x with 40 ă |x| ď 100. Training
nearthatofourstack-augmentedtransformer.
detailscanbefoundinAppendixB.
5 LanguageModeling
4.3 Results
WereportourresultsofthefourDCFtasksinTab.1 We consider masked language modeling using
andTab.2. RoBERTa (Liu et al., 2020) and autoregressive
language modeling using GPT-2 (Radford et al.,
4.3.1 Transformervs. StackTransformer
2019). Followingtheexperimentalsetupproposed
We report the performance of the standard trans- by previous authors (Joulin and Mikolov, 2015;
former and our stack-augmented transformer on DuSellandChiang,2024),weexperimentonthe
thefourDCFtaskspresentedinTab.1. Forcom- PennTreebank(PTB),licensedthroughtheLDC
parison,wealsoexhibitresultsofvanillarecurrent (Marcus et al., 1994), and WikiText-2 (Merity
neuralnetworks(RNNs)andstack-RNNs(Joulin et al., 2017). We consider models both trained
andMikolov,2015). Asexpected,thevanillatrans- from scratch and fine-tuned from pre-trained
formerexhibitspoorperformanceonalltheDCF weights. Thepre-trainedmodelsanddatasetsare
tasks. After being augmented with a stack, the obtained from HuggingFace (Wolf et al., 2020;
transformerimprovesfromnearlychancetoover Lhoest et al., 2021). See Appendix B for more
90%onRSandSM.Theseresultsdemonstratethat detailsaboutsetupandhyperparameters.
ourstack-augmentedattentionhelpsonsometasks. TheresultsinTab.3aremixed. Ourmajorfind-
However, on MA and SE, the performance after ing is that transformers benefit from the stack at-
addingthestackattentiononlyimprovesslightly; tentionwhentrainingdataisscarce,butthebene-
itstillfallsfarbehindstackRNNsandevenvanilla fitsgraduallydiminishasthesizeoftrainingdata
RNNs. We conjecture that the reason for this grows. More concretely, when the models are
shortcomingisourstacktransformer’sincapability trainedfromscratch,theadditionofourstackatten-
to learn non-counter-free languages—both the tionmechanismdoesresultinanoticeablebenefitPennTreebank WikiText-2
Model Task
Vanilla Stack Vanilla Stack
MLM 95.53˘19.66 34.28˘2.76 73.74˘3.79 64.75˘1.75
Scratch
ALM 73.14˘0.34 69.86˘0.26 191.01˘0.71 206.42˘0.80
MLM 3.99˘0.08 4.46˘0.11 4.41˘0.12 4.65˘0.06
Pretrained
ALM 21.26˘0.03 32.36˘0.16 29.29˘0.02 54.96˘0.19
Table3: MLMandALMPerplexitiesonWikiText-2andPTB.
inmostsettings. IntheMLMsetting,where15% English language datasets suggests that a good
ofthetokensarereplacedwith[MASK],stacksre- inductive bias is not needed for larger data sets.
ducetheperplexityunderthetrainedmodelonthe This suggests that, in contrast to the viewpoint
held-outsplitfrom95.53to34.28onPTBandfrom of traditional linguistic theory (Chomsky, 1957),
73.74to65.22onWikiText-2. IntheALMsetting, modelsthatarehigherontheChomskyhierarchy
thestacktransformerstillslightlyimprovestheper- arenotnecessaryfordevelopingagoodstatistical
formanceonPTB—perplexitydropsfrom73.14to languagemodel. Webelievethisclaimisconsistent
69.86. However,thestacktransformerislesseffec- with the literature, in which few successful large
tive on WikiText-2, whose training set is larger. languagemodelsareendowedwithasyntacticbias.
Moreover, when we fine-tune from pre-trained However, there are many smaller syntax-infused
models, stacks are always detrimental across the languagemodels(Dyeretal.,2016)thatdowork
twodatasetsinbothMLMandALMsettings. wellonsmallerdata,asoursdoes.
6 Discussion 7 Conclusion
Fromtheresultsdescribedin§4.3and§5,weob- Weproposeanovelimplementationofadifferen-
servetwotrends: tiablestackandshowthatatransformeraugmented
with such stacks can solve certain deterministic
• Thepositiveimpactofstackattentionisevident
context-free tasks that are beyond the capability
on Delétang et al.’s (2023) 4 DCF tasks (espe-
ofstandardtransformers. However,unlikeastack
cially on 2 of the 4), but almost nonexistent on
RNN,thestacktransformercannotmodeltheentire
Englishlanguagemodeling;
classofdeterministiccontext-freelanguages.
• On the English language modeling task, stack
attentionismorehelpfulinsettingswithlimited Acknowledgements
trainingdata,butislesshelpfulandcanevenbe
harmful when the model is trained on a larger This publication was made possible by an ETH
amountofdata. AICenterdoctoralfellowshiptoJiaodaLi. Ryan
Cotterell acknowledges support from the Swiss
We interpret these trends as support for the idea
National Science Foundation (SNSF) as part of
thatstackattentionimprovestherepresentational
the“TheForgottenRoleofInductiveBiasinInter-
capacity of a transformer language model and,
pretability”project.
additionally, confers an inductive bias to the
transformer architecture that allows it to better
Limitations
learn certain context-free tasks more efficiently.
Thelargerrepresentationalcapacityexplainswhy Theprimarylimitationoftheproposedstackatten-
theperformanceoncertaintasks,i.e.,RSandSM, tionisitonlyallowsonePOPoperationatatime. It
improves drastically with the addition of stack canbeextendedtohavemultiplePOPsinamanner
attention and the better inductive bias explains similartoYogatamaetal.(2018). Itcanalsoonly
why transformer language models with stack handle deterministic context-free languages. We
attentionperformbetterwithlesstrainingdataon wouldliketoextendittonon-deterministicstacks
theEnglishmodelingtask. However,thefactthat infutureworks. Althoughourmethoddoesnotre-
a vanilla transformer language model performs quirestructuralsupervision,itcaninprincipletake
onparwithstackattentionwhenmodelinglarger advantageofitwhenitisavailable. Insuchcases,themodelcanbefullyparallelized,leadingtogreat Noam Chomsky. 1957. Syntactic Structures. De
improvementintimeefficiency. Itwouldbeinter- GruyterMouton,Berlin,Boston.
estingtoexplorethispossibilityinthefuture. NoamChomskyandMarcelPaulSchützenberger.1963.
Thealgebraictheoryofcontext-freelanguages. Stud-
EthicalConsiderations ies in logic and the foundations of mathematics,
35:118–161.
Weforeseenoethicalconcernswiththiswork. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-
bonell, Quoc Le, and Ruslan Salakhutdinov. 2019.
Transformer-XL:Attentivelanguagemodelsbeyond
References a fixed-length context. In Proceedings of the 57th
AnnualMeetingoftheAssociationforComputational
Dana Angluin, David Chiang, and Andy Yang. 2023. Linguistics,pages2978–2988.
Masked hard-attention transformers and boolean
SreerupaDas,C.LeeGiles,andGuo-ZhengSun.1992.
RASP recognize exactly the star-free languages.
Learning context-free grammars: Capabilities and
ComputingResearchRepository,arXiv:2310.13897.
limitationsofarecurrentneuralnetworkwithanex-
Version2. ternal stack memory. In Proceedings of The Four-
JimmyLeiBa,JamieRyanKiros,andGeoffreyE.Hin- teenthAnnualConferenceofCognitiveScienceSoci-
ton. 2016. Layer normalization. Computing Re- ety,volume14.
searchRepository,arXiv:1607.06450.
GrégoireDelétang,AnianRuoss,JordiGrau-Moya,Tim
DzmitryBahdanau,KyunghyunCho,andYoshuaBen- Genewein, Li Kevin Wenliang, Elliot Catt, Chris
gio. 2015. Neural machine translation by jointly Cundy,MarcusHutter,ShaneLegg,JoelVeness,and
learningtoalignandtranslate. InInternationalCon- Pedro A. Ortega. 2023. Neural networks and the
ferenceonLearningRepresentations. Chomsky hierarchy. In 11th International Confer-
enceonLearningRepresentations.
PabloBarcelo,AlexanderKozachinskiy,AnthonyWid-
jajaLin,andVladimirPodolskii.2024. Logicallan- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
guagesacceptedbytransformerencoderswithhard Kristina Toutanova. 2019. BERT: Pre-training of
attention. InInternationalConferenceonLearning deepbidirectionaltransformersforlanguageunder-
Representations. standing. InProceedingsofthe2019Conferenceof
theNorthAmericanChapteroftheAssociationfor
SatwikBhattamishra, KabirAhuja, andNavinGoyal.
ComputationalLinguistics: HumanLanguageTech-
2020. OntheAbilityandLimitationsofTransform-
nologies,Volume1(LongandShortPapers),pages
erstoRecognizeFormalLanguages. InProceedings
4171–4186.
of the 2020 Conference on Empirical Methods in
NaturalLanguageProcessing(EMNLP),pages7096– Brian DuSell and David Chiang. 2020. Learning
7116. context-freelanguageswithnondeterministicstack
RNNs. In Proceedings of the 24th Conference on
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
ComputationalNaturalLanguageLearning, pages
Subbiah,JaredD.Kaplan,PrafullaDhariwal,Arvind
507–519.
Neelakantan,PranavShyam,GirishSastry,Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, BrianDuSellandDavidChiang.2022. Learninghierar-
Gretchen Krueger, Tom Henighan, Rewon Child, chicalstructureswithdifferentiablenondeterministic
AdityaRamesh,DanielZiegler,JeffreyWu,Clemens stacks. InInternationalConferenceonLearningRep-
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
resentations.
teusz Litwin, Scott Gray, Benjamin Chess, Jack BrianDuSellandDavidChiang.2024. Stackattention:
Clark, ChristopherBerner, SamMcCandlish, Alec Improving the ability of transformers to model hi-
Radford, Ilya Sutskever, and Dario Amodei. 2020. erarchicalpatterns. InInternationalConferenceon
Language models are few-shot learners. In Ad- LearningRepresentations.
vances in Neural Information Processing Systems,
Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
volume33,pages1877–1901.
andNoahA.Smith.2016. Recurrentneuralnetwork
Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016. grammars. InProceedingsofthe2016Conference
Longshort-termmemory-networksformachineread- of the North American Chapter of the Association
ing. InProceedingsofthe2016ConferenceonEm- for Computational Linguistics: Human Language
pirical Methods in Natural Language Processing, Technologies,pages199–209,SanDiego,California.
pages551–561,Austin,Texas.AssociationforCom- AssociationforComputationalLinguistics.
putationalLinguistics.
Javid Ebrahimi, Dhruv Gelda, and Wei Zhang. 2020.
DavidChiang,PeterCholak,andAnandPillay.2023. Howcanself-attentionnetworksrecognizeDyck-n
Tighterboundsontheexpressivityoftransformeren- languages? InFindingsoftheAssociationforCom-
coders. InProceedingsofthe40thInternationalCon- putationalLinguistics: EMNLP2020,pages4301–
ferenceonMachineLearning,ICML’23.JMLR.org. 4306.
NoamChomsky.1956. Threemodelsforthedescrip- R.JamesEvey.1963. Applicationofpushdown-store
tionoflanguage. IRETransactionsonInformation machines. In Proceedings of the November 12-14,
Theory,2(3):113–124. 1963, FallJointComputerConference, AFIPS’63(Fall),page215–227,NewYork,NY,USA.Associa- Workshop held at Plainsboro, New Jersey, March
tionforComputingMachinery. 8-11,1994.
GuhaoFeng,BohangZhang,YuntianGu,HaotianYe, Robert McNaughton and Seymour Papert. 1971.
Di He, and Liwei Wang. 2023. Towards revealing Counter-free automata. Research monograph 65.
themysterybehindchainofthought: Atheoretical MITPress.
perspective. InAdvancesinNeuralInformationPro-
StephenMerity,CaimingXiong,JamesBradbury,and
cessingSystems.
RichardSocher.2017. Pointersentinelmixturemod-
Edward Grefenstette, Karl Moritz Hermann, Mustafa els. InInternationalConferenceonLearningRepre-
Suleyman, and Phil Blunsom. 2015. Learning to sentations.
transducewithunboundedmemory. InAdvancesin
WilliamMerrillandAshishSabharwal.2023. Alogic
NeuralInformationProcessingSystems,volume28.
for expressing log-precision transformers. In Ad-
Michael Hahn. 2020. Theoretical limitations of self- vances in Neural Information Processing Systems,
attentioninneuralsequencemodels. Transactionsof volume36,pages52453–52463.CurranAssociates,
theAssociationforComputationalLinguistics,8:156– Inc.
171.
WilliamMerrillandAshishSabharwal.2024. Theex-
Yiding Hao, Dana Angluin, and Robert Frank. 2022. pressivepoweroftransformerswithchainofthought.
Formal language recognition by hard attention InInternationalConferenceonLearningRepresenta-
transformers: Perspectivesfromcircuitcomplexity. tions.
TransactionsoftheAssociationforComputational
MichaelC.MozerandSreerupaDas.1992. Aconnec-
Linguistics,10:800–810.
tionistsymbolmanipulatorthatdiscoversthestruc-
Yiding Hao, William Merrill, Dana Angluin, Robert tureofcontext-freelanguages. InAdvancesinNeu-
Frank, Noah Amsel, Andrew Benz, and Simon ralInformationProcessingSystems,volume5,pages
Mendelsohn.2018. Context-freetransductionswith 863–870.
neuralstacks. InProceedingsofthe2018EMNLP
ShikharMurty,PratyushaSharma,JacobAndreas,and
WorkshopBlackboxNLP:AnalyzingandInterpreting
ChristopherManning.2023. Pushdownlayers: En-
NeuralNetworksforNLP,pages306–315.
coding recursive structure in transformer language
Armand Joulin and Tomas Mikolov. 2015. Inferring models. InProceedingsofthe2023Conferenceon
algorithmicpatternswithstack-augmentedrecurrent EmpiricalMethodsinNaturalLanguageProcessing,
nets. InAdvancesinNeuralInformationProcessing pages3233–3247,Singapore.AssociationforCom-
Systems,volume28. putationalLinguistics.
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A AnthonyG.Oettinger.1961. Automaticsyntacticanal-
methodforstochasticoptimization. InInternational ysisandthepushdownstore. InProceedingsofSym-
ConferenceonLearningRepresentations. posiainAppliedMathematics,volume12: Structure
of Language and Its Mathematical Aspects, pages
Quentin Lhoest, Albert Villanova del Moral, Yacine
104–129,Providence,RhodeIsland.
Jernite,AbhishekThakur,PatrickvonPlaten,Suraj
Patil,JulienChaumond,MariamaDrame,JulienPlu, Ankur Parikh, Oscar Täckström, Dipanjan Das, and
Lewis Tunstall, Joe Davison, Mario Šaško, Gun- Jakob Uszkoreit. 2016. A decomposable attention
jan Chhablani, Bhavitvya Malik, Simon Brandeis, modelfornaturallanguageinference. InProceedings
Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas ofthe2016ConferenceonEmpiricalMethodsinNat-
Patry, Angelina McMillan-Major, Philipp Schmid, uralLanguageProcessing,pages2249–2255,Austin,
Sylvain Gugger, Clément Delangue, Théo Matus- Texas.AssociationforComputationalLinguistics.
sière, Lysandre Debut, Stas Bekman, Pierric Cis- Jorge Pérez, Pablo Barceló, and Javier Marinkovic.
tac, Thibault Goehringer, Victor Mustar, François 2021. AttentionisTuring-complete. JournalofMa-
Lagunas,AlexanderRush,andThomasWolf.2021. chineLearningResearch,22(75):1–35.
Datasets: Acommunitylibraryfornaturallanguage
JordanB.Pollack.1991. Theinductionofdynamical
processing. InProceedingsofthe2021Conference
recognizers. MachineLearning,7(2–3):227–252.
onEmpiricalMethodsinNaturalLanguageProcess-
ing: SystemDemonstrations,pages175–184,Online OfirPress,NoahSmith,andMikeLewis.2022. Train
andPuntaCana,DominicanRepublic.Association short,testlong: Attentionwithlinearbiasesenables
forComputationalLinguistics. inputlengthextrapolation. InInternationalConfer-
enceonLearningRepresentations.
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, AlecRadford,JeffreyWu,RewonChild,DavidLuan,
Luke Zettlemoyer, and Veselin Stoyanov. 2020. DarioAmodei,andIlyaSutskever.2019. Language
RoBERTa: ArobustlyoptimizedBERTpretraining modelsareunsupervisedmultitasklearners.
approach.
Laurent Sartran, Samuel Barrett, Adhiguna Kuncoro,
MitchellMarcus,GraceKim,MaryAnnMarcinkiewicz, Miloš Stanojevic´, Phil Blunsom, and Chris Dyer.
RobertMacIntyre,AnnBies,MarkFerguson,Karen 2022. Transformer grammars: Augmenting trans-
Katz,andBrittaSchasberger.1994. ThePennTree- formerlanguagemodelswithsyntacticinductivebi-
bank: Annotatingpredicateargumentstructure. In ases at scale. Transactions of the Association for
Human Language Technology: Proceedings of a ComputationalLinguistics,10:1423–1439.Marcel Paul Schützenberger. 1963. On context-free
languages and push-down automata. Information
andControl,6(3):246–264.
M.P. Schützenberger. 1965. On finite monoids hav-
ingonlytrivialsubgroups. InformationandControl,
8(2):190–194.
StuartM.Shieber.1987. EvidenceAgainsttheContext-
Freeness of Natural Language, pages 320–334.
SpringerNetherlands.
JianlinSu, MurtadhaAhmed, YuLu, ShengfengPan,
Wen Bo, and Yunfeng Liu. 2023. RoFormer: En-
hancedtransformerwithrotarypositionembedding.
Neurocomputing,page127063.
MiracSuzgun,SebastianGehrmann,YonatanBelinkov,
and Stuart M. Shieber. 2019. Memory-augmented
recurrent neural networks can learn generalized
Dyck languages. Computing Research Repository,
arXiv:1911.03329.
AnejSveteandRyanCotterell.2024. Transformerscan
representn-gramlanguagemodels. InProceedings
ofthe2024ConferenceoftheNorthAmericanChap-
teroftheAssociationforComputationalLinguistics,
MexicoCity,Mexico.AssociationforComputational
Linguistics.
TheUnivalentFoundationsProgram.2013. Homotopy
TypeTheory: UnivalentFoundationsofMathematics.
InstituteforAdvancedStudy.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz
Kaiser,andIlliaPolosukhin.2017. Attentionisall
youneed. InAdvancesinNeuralInformationPro-
cessingSystems,volume30.
Gail Weiss, Yoav Goldberg, and Eran Yahav. 2021.
Thinking like transformers. In Proceedings of the
38thInternationalConferenceonMachineLearning,
volume 139 of Proceedings of Machine Learning
Research,pages11080–11090.PMLR.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond,ClementDelangue,AnthonyMoi,Pier-
ricCistac,TimRault,RémiLouf,MorganFuntowicz,
JoeDavison,SamShleifer,PatrickvonPlaten,Clara
Ma,YacineJernite,JulienPlu,CanwenXu,TevenLe
Scao, Sylvain Gugger, Mariama Drame, Quentin
Lhoest,andAlexanderM.Rush.2020. Transform-
ers: State-of-the-artnaturallanguageprocessing. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations,pages38–45.
DaniYogatama,YishuMiao,GaborMelis,WangLing,
AdhigunaKuncoro,ChrisDyer,andPhilBlunsom.
2018. Memoryarchitecturesinrecurrentneuralnet-
worklanguagemodels. InInternationalConference
onLearningRepresentations.
ZhengZeng,RodneyM.Goodman,andPadhraicSmyth.
1994. Discreterecurrentneuralnetworksforgram-
maticalinference. IEEETransactionsonNeuralNet-
works,5(2):320–330.A Proof
Theorem3.1. Letυ ,...,υ beaseriesofstackoperationswhereυ P tPUSH p¨q,NO-OPp¨q,POPp¨qufor
1 N i i
alli P rNs. Furthermore,supposea pυ q “ 1foralli P rNs. Then, PEEKpυ p¨¨¨υ pεqqq “ α forall
i i i 1 i
i P t0uYrNs. (cid:74) (cid:75)
Proof.
Basecase(i “ 0). Thestackisinitializedtobeempty,i.e.,γ “ εandPEEKpγ q “ 0. Bydefinition,
0 0
wehave
α “ r1,0,...sJ “ PEEKpγ q (28)
0 0
(cid:74) (cid:75)
InductiveStep. Supposethereexistsani ą 0,suchthat@i1 ă i,α
i1
“ PEEKpγ i1q ,anda ipυ iq “ 1.
(cid:74) (cid:75)
• If υ “ PUSH, γ “ PUSHpγ q “ γ i and PEEKpγ q “ i, so according to Eq. (20) we have
i i i´1 i´1 i
α “ PEEKpγ q .
i i
(cid:74) (cid:75)
• If υ “ NO-OP, γ “ NO-OPpγ q “ γ , and α “ α . Since α “ PEEKpγ q , we have
i i i´1 i´1 i i´1 i´1 i´1
α “ PEEKpγ q . (cid:74) (cid:75)
i i
(cid:74) (cid:75)
• Ifυ “ POP,
i
iÿ´1
α “ α pjqα `α p0qα (29)
i i´1 j´1 i´1 0
j“1
Ifα p0q “ 1,i.e. γ isempty,γ “ POPpεq “ ε,and
i´1 i´1 i
α “ α p0qα (30a)
i i´1 0
“ α (30b)
0
“ 0 (30c)
(cid:74) (cid:75)
“ PEEKpγ q (30d)
i
(cid:74) (cid:75)
Otherwise,
iÿ´1
α “ α pjqα (31a)
i i´1 j´1
j“1
“ α (31b)
PEEKpγ q´1
i´1
“ PEEKpγ q (31c)
PEEKpγ q´1
(cid:74) i´1 (cid:75)
“ PEEKpPOPpγ qq (31d)
i´1
(cid:74) (cid:75)
“ PEEKpγ q (31e)
i
(cid:74) (cid:75)
OnecanunderstandEq.(31d)intuitivelyasfollows: γ isthestackrightbeforethecurrent
PEEKpγ q´1
i´1
stacktopPEEKpγ qispushed,sothestacktopatPEEKpγ q´1isthesecondtop-mostelementat
i´1 i´1
i´1,i.e.,PEEKpγ q “ POPpγ q.
PEEKpγ q´1 i´1
i´1
■
ř
Theorem3.2. Considerasequenceofstackattentionmechanismsα ,...,α . Then, N α pnq “ 1
0 N n“0 i
foralli P t0uYrNs.
Proof.
Basecase Itholdsforα “ r1,0,...sJ.
0ř
Inductionstep Supposethereexistsani ą 0,suchthat@i1 ă i, N n“0α i1pnq “ 1.
• PUSH. Obviously,
ÿN
αpPUSHq
pnq “ 1 (32)
i
n“0
• NO-OP.
SinceαpNO-OPq
“ α ,wealsohave
i i´1
ÿN
αpNO-OPq
pnq “ 1 (33)
i
n“0
• POP.
˜ ¸
ÿN ÿN iÿ´1
αpPOPq
pjq “ α pjqα `α p0qα pnq (34a)
i i´1 j´1 i´1 0
n“0 n“0 ˜j“1
¸
ÿN iÿ´1
“ α pjqα pnq`α p0qα pnq (34b)
i´1 j´1 i´1 0
n“0 j“1
˜ ¸ ˜ ¸
iÿ´1 ÿN ÿN
“ α pjq α pnq `α p0q α pnq (34c)
i´1 j´1 i´1 0
j“1 n“0 n“0
iÿ´1
“ α pjq`α p0q (34d)
i´1 i´1
j“1
iÿ´1
“ α pjq (34e)
i´1
j“0
“ 1 (34f)
Therefore,
ÿN
α pnq (35a)
i
n“0˜ ¸
ÿN ÿ
“ a
paqαpaq
pnq (35b)
i i
n“0 aPA
ÿ ÿN
“ a paq
αpaq
pnq (35c)
i i
aÿPA n“0
“ a paq (35d)
i
aPA
“1 (35e)
■
B ExperimentalSetup
B.1 DCFTasks
The model is trained using the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 1e´4,
whichwefindworkswellforallthetasks. OntheRSandSMtasks,weuseabatchsizeof32andwetrain
themodelfor100,000steps. OntheMAandSEtasks,thebatchsizeandthenumberoftrainingsteps
areincreasedto128and1,000,000,respectively,toensuresufficienttraining. Eachexperimentisrun5
timeswithdifferentrandomseeds. MeansandvariancesofaccuraciesarereportedTab.1andTab.2.B.2 LanguageModeling
The texts in the datasets are grouped into chunks of 128 tokens. Each model is, again, trained using
theAdamoptimizerforamaximumof100epochswithearlystoppingappliedwhenthevalidationloss
hasnotimprovedfor5epochsinarow. Wetunethelearningratefromt1e´5,2e´5,1e´4,2e´4uonthe
validationset,andchoose2e´5 thatleadstothebestvalidationperformance. Resultsonthetestsetover5
runswithdifferentrandomseedsarereportedinTab.3. ExperimentsareconductedonasingleNVIDIA
TeslaV100GPU.
C PositionalEncodings
Weconsiderfivedifferentcommonlyusedpositionalencodings:
• none. Nopositionalencodingsareused.
• sincos. The sinusoidal positional encodings used in vanilla transformer (Vaswani et al., 2017).
Positionalinformationencodedsinusoidallyisaddedtotheembeddings.
• relative. InTransformer-XL(Daietal.,2019),relativeratherthanabsolutesinusoidalpositional
informationisaddedtothekeysandqueriesofeachattentionblock.
• rotary. Introduced by Su et al. (2023) and popularized by GPT-3 (Brown et al., 2020), rotary
positionalencodingsmultiplythekeysandqueriesbysinusoidalencodings.
• ALiBi. Pressetal.(2022)addslinearbiasestotheattentionblocksthatfavorthemorerecenttokens.
D Analysis: AttentionMaps
Anadvantageofourstackattentionmechanismisthatwecanvisualizethestacktopsα ,whichprovides
i
greaterinterpretabilitythanmethodswherestacktopsaremixturesofhiddenstates(JoulinandMikolov,
2015). We run a set of toy experiments with the stack transformer in the MLM setting. We randomly
selectonetestexampleforeachtask.
RS. Atthefirsttwolayers(Fig.2a,Fig.2b),thefirst5tokensattendtothemselveswhilethe [MASK]
tokensattendtothelasttokeninx. Themostprobablesequenceofoperationsthatleadstosuchastack
attentionmapistheinputxispushedonebyoneontothestackandNO-OPisperformedonallthe[MASK]
tokens. At the third layer (Fig. 2c), the stacks for the [MASK] tokens shift one position backward at a
time,whichdemonstratesthestackelementsarePOPedonebyonetogeneratetheoutputs. Atthelast
twolayers,allthetokensattendtothemselves,sothestackscanberegardedasbeingskipped(Fig.2d,
Fig.2e).
SM. Lookingattheattentionmapatthefirstlayer(Fig.3a),wecaninfertheoperationstakenbythe
stackasfollows: thestackfirstPUSHestheinitialstackcontents(ab);oncetherPOPsoperationisread,
itrevertstothefirstelementa;thenitperformstheoperationrPUSHbstwiceasinstructed;afterwards,
it POPs the final stack bba for outputs. The stack attention correctly skips the b at timestep 1 as it has
alreadybeenPOPedattimestep2. Thelastthreepositionsare [PAD] tokensandcanbeignored.
MAandSE. WealsoprovideanattentionmapforMAandSEinFig.4andFig.5. Theirattention
mapsarelessinterpretableasthestacktransformerdoesnotlearnthemwell. Nevertheless,wecanstill
observe that the stacks seem to be able to match the parentheses, which matches our expectations of
the stack’s strengths. For MA, at the first layer (Fig. 4a), the stack successfully matches the last two
closingparentheses(attimestep8and9)withtheircorrespondingopenparentheses(attimestep5and
0respectively). ForSE,thepatternislessobviouspresumablybecausetheparenthesesdonothavean
impactontheorderofarithmeticoperationsandcanbeignored.M M M
M M M
M M M
M M M
M M M
a a a
a a a
b b b
b b b
a a a
a b b a a M M M M M a b b a a M M M M M a b b a a M M M M M
(a)Layer1 (b)Layer2 (c)Layer3
1 1 1
M M
M M
M M
M M
M M
a a
a a
b b
b b
a a
a b b a a M M M M M a b b a a M M M M M
(d)Layer4 (e)Layer5
1 1
Figure2: StackattentionmapsatdifferentlayersforRS.Theinputxisabbaa. Mrepresentsa[MASK]token.
M M M
M M M
M M M
M M M
M M M
M M M
b b b
b b b
P P P
b b b
a a a
a b P b b M M M M M M a b P b b M M M M M M a b P b b M M M M M M
(a)Layer1 (b)Layer2 (c)Layer3
1 1 1
M M
M M
M M
M M
M M
M M
b b
b b
P P
b b
a a
a b P b b M M M M M M a b P b b M M M M M M
(d)Layer4 (e)Layer5
1 1
Figure3: StackattentionmapsatdifferentlayersforSM.TheinputxisabrPOPsrPUSHasrPUSHbs. Inthegraphs,
rPUSHas,rPUSHbs,andrPOPsareabbreviatedasa,b,andPrespectively. Mrepresentsa[MASK]token. Thecorrect
outputshouldbebbafollowedby[PAD]tokens.= = =
) ) )
) ) )
0 0 0
−( −( −(
· · ·
) ) )
4 4 4
( ( (
( ( (
( ( 4 ) ( 0 ) ) = ( ( 4 ) ( 0 ) ) = ( ( 4 ) ( 0 ) ) =
· − · − · −
(a)Layer1 (b)Layer2 (c)Layer3
1 = 1 = 1
) )
) )
0 0
−( −(
· ·
) )
4 4
( (
( (
( ( 4 ) ( 0 ) ) = ( ( 4 ) ( 0 ) ) =
· − · −
(d)Layer4 (e)Layer5
1 1
Figure4: StackattentionmapsatdifferentlayersforMA.Theinputxispp4q¨p´0qq“.
M M M
3 3 3
≡) ≡) ≡)
) ) )
z z z
( ( (
+ + +
2 2 2
−( −( −(
( 2 + ( z ) ) 3 M ( 2 + ( z ) ) 3 M ( 2 + ( z ) ) 3 M
− ≡ − ≡ − ≡
(a)Layer1 (b)Layer2 (c)Layer3
1 M 1 M 1
3 3
≡) ≡)
) )
z z
( (
+ +
2 2
−( −(
( 2 + ( z ) ) 3 M ( 2 + ( z ) ) 3 M
− ≡ − ≡
(d)Layer4 (e)Layer5
1 1
Figure5: StackattentionmapsatdifferentlayersforSE.Theinputxisp1`p´zqq“3. Mrepresentsa[MASK]
token.E RelatedWork
E.1 StackAugmentation
Equipping a neural network with a data structure such as an external stack to enhance its ability to
recognizecontext-freelanguageshasbeenextensivelyinvestigatedinpreviousworks(Pollack,1991;Das
et al., 1992; Mozer and Das, 1992; Zeng et al., 1994). The idea has seen a resurgence in recent years,
withworkfocusingprimarilyonrecurrentnetworks(JoulinandMikolov,2015;Grefenstetteetal.,2015;
Hao et al., 2018; Yogatama et al., 2018; Suzgun et al., 2019; DuSell and Chiang, 2020, 2022). Joulin
andMikolov(2015)proposetosuperposetheresultofapplyingeachstackoperationateachstep,which
directlyinspiresourwork. Weadaptitforapplicationtotransformersbyrenderingthisconceptasan
attentionmechanism. Inthatsense,ourworkisrelatedtoDasetal.(1992)andGrefenstetteetal.(2015),
which also assign weights to stack elements. Our stack attention mechanism is different as the stack
attentionweightsareassignedtopreviouslyseentokensindicatingwherethetopelementislocated
Sartranetal.(2022)andMurtyetal.(2023)incorporateastackmechanismintoatransformerlanguage
modelwithstructuralsupervisionduringtraining. DuSellandChiang’s(2024)contemporaneouswork
alsoaugmentsatransformerlanguagemodelwithastack. Boththeirandourmethodsarenamedstack
attention,buttheirstackattentionisanattentionmechanismoverstackactionswhileoursisanattention
mechanismoverinputtokens.
E.2 ExpressivityofTransformers
The expressivity of transformers under various assumptions has been extensively studied. A stream
of research considers transformer encoders with a classification layer at the end as recognizers. Hahn
(2020)provesthattransformerscannotrecognizeparitylanguage,aperiodiclanguageofbinarystrings
with an even number of 1’s, and Dyck-2 language, a CF language of balanced brackets of two types.
Bhattamishra et al. (2020) find that transformers can recognize certain counter languages but fail to
recognizenon-star-freelanguagessuchaspaaq˚. SveteandCotterell(2024)showthattransformerscan
representn-gramlanguagemodels. Haoetal.(2022),Chiangetal.(2023),MerrillandSabharwal(2023),
Barcelo et al. (2024), and Angluin et al. (2023) relate transformers to circuit complexity and formal
logic. Withvariousextensions,transformers’expressivitycanbeincreased. Weissetal.(2021)proposea
programminglanguagethatsharesthesamebasicoperationswithtransformersbutismoreexpressive
thanstandardtransformers. Pérezetal.(2021)andMerrillandSabharwal(2024)showthattransformer
encoder–decodersanddecodersareTuringcompletewithadditionalscratchspace.