INPROGRESS 1
Unified End-to-End V2X Cooperative Autonomous
Driving
Zhiwei Li1, Bozhen Zhang1*,Lei Yang, Tianyu Shen*, Nuo Xu, Ruosen Hao, Weiting Li, Tao Yan, Huaping Liu
Abstract—V2X cooperation, through the integration of sensor ception [15] [16], prediction [17], planning [18], and control
datafrombothvehiclesandinfrastructure,isconsideredapivotal [19]. These multi-stage approaches necessitate maintenance
approach to advancing autonomous driving technology. Current of inter-module communication, potentially leading to system
research primarily focuses on enhancing perception accuracy,
response delays and information loss [20] [21]. Conversely,
oftenoverlookingthesystematicimprovementofaccidentpredic-
tionaccuracythroughend-to-endlearning,leadingtoinsufficient end-to-end autonomous driving methods [22] [23] [24] offer
attention to the safety issues of autonomous driving. To address a more intuitive and streamlined approach by directly trans-
thischallenge,thispaperintroducestheUniE2EV2Xframework, lating environmental data into vehicle control decisions, thus
a V2X-integrated end-to-end autonomous driving system that
reducing system complexity and minimizing delays through
consolidates key driving modules within a unified network. The
unified data representation. However, the perception range
framework employs a deformable attention-based data fusion
strategy,effectivelyfacilitatingcooperationbetweenvehiclesand of individual vehicle intelligence is limited to its onboard
infrastructure. The main advantages include: 1) significantly sensors, potentially compromising its perception capabilities
enhancingagents’perceptionandmotionpredictioncapabilities, under complex road and adverse weather conditions. Vehicle-
therebyimprovingtheaccuracyofaccidentpredictions;2)ensur-
to-Everything (V2X) cooperation [25] [26] [27] enhances
inghighreliabilityinthedatafusionprocess;3)superiorend-to-
autonomousvehiclesbyintegratinginformationexchangeand
end perception compared to modular approaches. Furthermore,
We implement the UniE2EV2X framework on the challenging collaborative operation between vehicles and road infrastruc-
DeepAccident,asimulationdatasetdesignedforV2Xcooperative ture. This provides comprehensive, accurate road and traffic
driving. signalinformation,improvingsafetyandefficiency.Moreover,
Index Terms—V2X Cooperation, End-to-End, Autonomous V2Xcommunicationenablesvehiclestoperceivebeyondtheir
Driving immediate vicinity, facilitating cooperative driving among ve-
hicles. Despite the focus on improving metrics like detection
accuracy and trajectory prediction precision in current V2X
I. INTRODUCTION
research, these improvements do not necessarily equate to
OVER the past few decades, the transportation [1] and
effective planning outcomes due to the introduction of irrele-
automotive sectors [2] [3] have seen increasing automa-
vant information by multi-stage autonomous driving methods.
tionandintelligence,drivenbyadvancementsindeeplearning
This paper proposes an end-to-end V2X-based autonomous
[4] [5] [6], control theory [7] [8], and technologies like
driving framework aimed at collision prediction outcomes,
sensors [9] [10] and network communications [11] [12] [13]
integratingtargetdetection,tracking,trajectoryprediction,and
[14]. Autonomous driving research is burgeoning, typically
collision warning into a unified end-to-end V2X autonomous
segmentingthecomplexintelligencesystemrequiredintomul-
driving method. The remainder of the paper is organized as
tiplesubtasksbasedondifferentstagesofdriving,suchasper-
follows: Section 2 reviews related work, Section 3 introduces
theend-to-endV2Xneural networkmodel,Section4presents
1 ZhiweiLiandBozhenZhangcontributedequallytothiswork.
Correspondingauthors:BozhenZhangandTianyuShen. experiments using public datasets and compares them with
Zhiwei Li is with the College of Information Science and Technology, other models, validating the effectiveness of the proposed
Beijing,100029,China(e-mail:lizw@buct.edu.cn).
method. Section 5 concludes the paper.
Bozhen Zhang is with the School of Information Science and Engi-
neering, Shenyang University of Technology, Shenyang, 110870,China(e-
mail:siriuszhang025@gmail.com).
LeiYangiswiththeSchoolofVehicleandMobility,TsinghuaUniversity,
II. RELATEDWORK
Beijing,100084,China(e-mail:yanglei20@mails.tsinghua.edu.cn).
A. End-to-End Autonomous Driving
Tianyu Shen is with College of Information Science and Technol-
ogy, Beijing University of Chemical Technology, Beijing, 100029,China(e- End-to-end autonomous driving methods are architecturally
mail:tianyu.shen@buct.edu.cn).
simpler than modular approaches, directly producing driving
Nuo Xu is with School of Software, Beihang University, Beijing,
100191,China. commands from perception data, thus avoiding the generation
Ruosen Hao is with the College of Information Science and Technology, of redundant intermediate stage information. These methods
BeijingUniversityofChemicalTechnology,Beijing,100029,China.
can be implemented through two approaches: imitation learn-
Weiting Li is with the School of Information and Engineering, China
UniversityofGeosciencesBeijing,Beijing,100083,China. ing and reinforcement learning [28]. Imitation learning, a
TaoYaniswiththeSchoolofVehicleandMobility,TsinghuaUniversity, form of supervised learning, involves learning strategies and
Beijing,100084,China.
updating models by imitating human driving behavior. Its ad-
HuapingLiuiswiththeDepartmentofComputerScienceandTechnology,
TsinghuaUniversity,Beijing,100084,China. vantageliesinhightrainingefficiency,butitrequiresextensive
4202
yaM
7
]VC.sc[
1v17930.5042:viXraINPROGRESS 2
annotated training data and cannot cover all potential traffic OPV2V [48], and DAIR-V2X [49]. However, these main-
and driving scenarios. Reinforcement learning, in contrast, stream vehicle-road cooperation datasets primarily focus on
learnsdirectlyfromtheinteractionbetweenthemodelandthe perception accuracy as the evaluation metric, suitable for
environment to maximize cumulative rewards. Its advantage testing the performance of autonomous driving perception al-
is that it does not require manually collected and annotated gorithms but not for evaluating end-to-end related algorithms.
data, but the model convergence is slow, and the results DeepAccident[50]isalarge-scaleautonomousdrivingdataset
are significantly affected by the definition of rewards and generated using the CARLA simulator, supporting testing for
other factors. Early end-to-end autonomous driving methods end-to-endmotionandaccidentpredictiontasks.Inthiswork,
primarily focused on imitation learning, typically outputting we propose an end-to-end autonomous driving framework
simple driving tasks. Using CNNs to infer steering angles based on vehicle-road cooperation and utilize the DeepAc-
from images captured by three cameras, [29] achieved lane cident dataset to test the performance of related algorithms.
keeping on roads without lane markings. Considering vehicle
speed, [30] introduced temporal information on top of CNNs
III. METHODOLOGY
using LSTM, an approach effective for simple tasks like
lane keeping but limited in complex traffic scenarios and Thispaperintroducesavehicle-roadcooperativeend-to-end
driving tasks [31]. Several studies have implemented end-to- autonomousdrivingframeworkcomprisingtwomajorcompo-
end autonomous driving through reinforcement learning, han- nents: the V2X Cooperative Feature Encoder and the End-to-
dling more complex scenarios compared to imitation learning End Perception, Motion, and Accident Prediction Module.
[32], [33], [34]. Integrating multimodal data into end-to-end
autonomousdrivingmodelshasresultedinbetterperformance
A. V2X Cooperative Feature Encoding Based on Temporal
than single-modal approaches [35], [36], [37]. However, the
Bird’s Eye View
challenge with end-to-end methods lies in poor model inter-
pretability, making it difficult to diagnose and address issues a) OverallStructure: OurV2Xframeworkincludesboth
when problems arise. UniAD unifies multiple shared BEV the vehicle itself and road infrastructure. During the cooper-
feature-based Transformer networks, containing modules for ative phase, each agent first extracts and converts multi-view
tracking, mapping, and trajectory prediction. This enhances image features into BEV features. These are then encoded to
the model’s interpretability, aids in training and troubleshoot- align the temporal sequence of V2X agent BEV perception
ing, and employs the final planning outcomes to design the information. Finally, by merging the BEV features of the
loss function, constructing an end-to-end autonomous driving vehiclewiththoseoftheroadsideinfrastructure,weobtainthe
model [38]. cooperativeperceptionfeatures.TheprocessofextractingV2X
cooperative features based on temporal BEV, as illustrated in
Figure 1, consists of two main components: the multi-view
B. Vehicle-to-Everything Cooperation
imagetoBEVfeaturemodulebasedonspatialBEVencoding,
Autonomous vehicles based on single-vehicle intelligence and the temporal cascading BEV feature fusion module based
perceive the environment centered around the vehicle itself on temporal BEV encoding. After spatial transformation and
using onboard sensors. However, the real-world traffic scene temporal fusion, the infrastructure BEV features are aligned
is complex and variable, with single vehicles, especially in and integrated with the vehicle’s coordinate system using
terms of perceiving vulnerable road users [39]. Thanks to a deformable attention mechanism to fuse the two aligned
the advancement of communication technologies, Cooperative BEV features.enhancing the vehicle’s perception capabilities
Automated Driving Vehicles (CAVs) have been proposed, to achieve the final V2X cooperative BEV features.
enhancing the vehicle’s perception capabilities by aggregating b) Multi-View Image to BEV Feature Module Based on
perception data from other autonomous vehicles in the traffic Spatial BEV Encoding: The original perception information
environment. Autonomous driving cooperative perception can obtainedfromboththevehicleandroadinfrastructureconsists
be categorized into three types based on the data transmitted of multi-view perspective images. To eliminate spatial seman-
via V2X. The first type involves direct transmission of raw tic differences and merge multi-source perception data, the
point cloud data for cooperative perception, demanding high multi-viewimagesfromboththevehicleandinfrastructureare
transmission bandwidth [40]. The second approach processes processed through two parallel channels of feature extraction
the raw perception data into unified feature information, and transformation to yield unified BEV features. Following
such as BEV spatial features, before transmission to save the approach in [51], we map multi-view perspective images
bandwidth.Thismethodbalancesbandwidthrequirementsand into the BEV space. In the module for converting multi-view
detection accuracy and is the mainstream V2X transmission imagestoBEVfeaturesbasedonspatialBEVencoding,multi-
method [41], [42], [43], [44], [45]. The third type gener- view images are first processed separately. Two-dimensional
ates prediction results for each autonomous vehicle before convolution is used to extract multi-view feature maps, which
transmitting this outcome information via V2X, requiring low are then inputted into the spatial BEV encoder module. The
bandwidth but demanding high accuracy in individual vehicle spatial BEV encoder ultimately generates high-level semantic
prediction results [46]. High-quality datasets in autonomous BEV features of the images. This process can be described
driving cooperative perception have propelled research in by Equation (1), where ResNet refers to the ResNet-101
the field, with mainstream datasets including V2X-Sim [47], backbonenetwork,I0 ,I1 ,...,I5 representcameraimages
ego ego egoINPROGRESS 3
Fig.1. TheoverallarchitectureofV2Xcollaborativefeatureextractionprocessbasedontime-seriesBEV
fromsixviewpointsofthevehicle,andF e0 go,F e1 go,...,F e5 gorep- B =softmax(Q( √Ki)T )Vi (3)
resent the feature maps from these six viewpoints. Similarly, d
F0 ,F1 ,...,F5 are the feature maps from six viewpoints However, the query in traditional Transformer architecture
inf inf inf
of the road infrastructure. encoders conducts attention operations with all keys, which is
neither efficient nor necessary given the vast scale and mixed
F0 ,F1 ,...,F5 =ResNet(I0 ,I1 ,...,I5 ) (1)
ego ego ego ego ego ego signals of multi-view feature maps serving as keys. Hence, in
actualBEVfeatureencoding,encodersbasedonadeformable
Next, the multi-view feature maps are inputted into a spatial
attention mechanism are used to conserve computational re-
BEV encoder based on a deformable spatial cross-attention
sources and enhance efficiency significantly.
mechanism to transform two-dimensional image features into
c) Temporal Cascading BEV Feature Fusion Module
BEV spatial features. Initially, a BEV target query Q ∈
RH×W×C,alearnableparametertensor,iscreatedtogradually Based on Temporal BEV Encoding: The BEV features B t
obtained in the previous section are considered carriers of
learn the BEV information of the multi-view images under
sequential information. Each moment’s BEV feature B is
the action of the spatial BEV encoder. Q serves as the query t
based on the BEV feature from the previous moment B
for the spatial BEV encoder, with multi-view feature maps (t−1)
Fi or Fi as the keys and values for the encoder. After six to capture temporal information. This approach allows for the
ego inf
dynamic acquisition of necessary temporal features, enabling
rounds of BEV feature encoding interactions, the parameters
the BEV features to more quickly and effectively respond
of Q are continually updated to yield a complete and accurate
to changes in the dynamic environment. In the temporal
BEV feature value B. The specific BEV encoding process
can be represented by Equations (2) and (3), where Q,Ki,Vi cascadingBEVfeaturefusionmodulebasedontemporalBEV
encoding, the BEV feature from the preceding frame B
respectivelydenotetheBEVtargetquery,imageBEVkey,and (t−1)
image BEV value. Wq,Wk,Vi represent the weight matrices servesaspriorinformationtoenhancethecurrentframe’sBEV
for Q,Ki,Vi, and B,Fi denote the BEV features and image featureB t.SinceB t andB (t−1) areintheirrespectivevehicle
coordinate systems, the B feature must first be trans-
features, respectively. (t−1)
formed to the current frame B ’s vehicle coordinate system
t
Q=WqB, Ki =WkFi, Vi =WvFi (2) using the vehicle’s position transformation matrix. Then, B
tINPROGRESS 4
andB ,astwoframesofBEVfeatures,areinputted,anda detection queries to identify newly appeared targets, then
(t−1)
temporal BEV encoder based on a deformable cross-attention interacts current frame tracking queries with detection queries
mechanism is used to transform two-dimensional image fea- from preceding frames to aggregate temporal information,
tures into cooperative perception BEV features. First, static and updates the tracking queries for target tracking in sub-
scene alignment is achieved. Knowing the world coordinates sequentframes.Thismulti-objecttrackingquerycontainsfea-
ofthevehicleatmomentst−1andt,andusingthecontinuous tures representing target information over consecutive frames.
frame vehicle motion transformation matrix, B features Additionally, an ego-vehicle query module is introduced to
(t−1)
arealignedtoB .ThisalignmentoperationensuresthatB aggregate the trajectory of the self-driving car, which is later
t (t−1)
and B in the same search position grids correspond to the usedtopredictthevehicle’sfuturetrajectory.Themulti-object
t
samelocationintherealworld,withthealignedBEVfeatures tracking module consists of N Transformer layers, and the
denoted as B′ . Subsequently, dynamic target alignment is outputfeatures,Q ,containrichproxytargetinformationthat
(t−1) A
executed. The BEV feature B at time t serves as the target will be further utilized in the motion prediction module.
t
queryQ∈RH×W×C,progressivelylearningtheBEVfeatures b) Motion Prediction: The motion prediction module
oftimet−1undertheactionofthetemporalBEVencoder.Q takes the multi-object tracking queries, Q , and collaborative
A
is used as the query for the temporal BEV encoder, with the BEV features from the perception module as inputs. Using
previous moment’s BEV features serving as keys and values. a scene-centric approach, it outputs motion queries, Q , to
X
Through BEV feature encoding interactions, Q’s parameters predictthefuturetrajectoriesofeachproxyandtheego-vehicle
are continuously updated, ultimately yielding a complete and over T frames with K possible paths. This method allows
accurate cooperative perception BEV feature value B . The simultaneous prediction of multiple proxies’ trajectories and
t
specific BEV encoding process is represented by Equations fullyconsidersinteractionsbetweenproxiesandbetweenprox-
(4) and (5), where Q, Ki, and Vi respectively represent the ies and target locations. The motion queries between proxies,
targetqueryforBEVfeaturesattimet,thekeyforimageBEV Q , are derived from multi-head cross-attention mechanisms
a
features at time t−1, and the value for image BEV features betweenmotionandtrackingqueries,whilethemotionqueries
attimet−1.Wq,Wk,andVi aretheweightmatricesforQ, relatedtotargetlocations,Q ,aregeneratedthroughavariable
g
Ki, and Vi, with B and B representing BEV features attention mechanism using motion queries, target positions,
t (t−1)
at time t and image BEV features at time t−1, respectively. and collaborative BEV features. Q and Q are combined
a g
and passed through a multilayer perceptron (MLP) to produce
Q=WqB , Ki =WkB , Vi =WvB (4)
t (t−1) (t−1) the query context, Q tx. The motion query positions, Q os,
c p
(cid:18) Q(Ki)T(cid:19) incorporate four types of positional knowledge: scene-level
B =softmax √ Vi (5)
t anchors, proxy-level anchors, the proxies’ current positions,
d
andpredictedtargetpoints.Q txandQ osaremergedtoform
At time t−1, assuming a target is present at some point in c p
the motion query, Q , which directly predicts each proxy’s
B , it is likely that the target will appear near the corre- X
(t−1) motion trajectory.
sponding point in B at time t. By employing the deformable
t c) Accident Prediction: After inputting collaborative
cross-attentionmechanismfocusingonthispointandsampling
BEV features into the end-to-end autonomous driving frame-
features around it, high-precision temporal feature extraction
work, the movement predictions for all agents and the ego-
with low overhead can be achieved in dynamic and complex
vehicle are obtained. These predictions are post-processed
environments.
frame-by-frame to check for potential accidents. For each
timestamp,thepredictedmotiontrajectoriesofeachproxycan
B. End-to-End Autonomous Driving beapproximatedaspolygons,andthenearestothertargetsare
We propose a unified end-to-end V2X cooperative au- identified. By checking if the minimum distance between ob-
tonomousdrivingmodelnamedUniE2EV2X,orientedtowards jectsisbelowasafetythreshold,itcanbedeterminedwhether
accident prediction. The primary tasks of this model include an accident has occurred, providing labels for the colliding
object detection and tracking, motion prediction, and post- objects’ IDs, positions, and the timestamp of the collision.
processing for accident prediction, as illustrated in Figure 2. To assess the accuracy of accident predictions compared to
real accident data, the same post-processing steps are applied
a) Detection and Track: The perception module is the to actual accident movements to ascertain future accidents’
initialcomponentoftheend-to-endautonomousdrivingframe- occurrences.Thebasisforcollisionincludescaseswhereboth
work presented in this paper. It consists of detection and predictions and ground truth indicate an accident and the
tracking sub-modules, taking collaborative BEV features as distance between colliding objects is below the threshold.
input and producing tracked proxy features for use in the
downstream motion prediction module. The detection sub- IV. EXPERIMENTS
module is responsible for predicting target information under
V. CONCLUSION
collaborativeBEVfeaturesineachtimeframe,includingtarget
REFERENCES
locationsanddimensions.Thetrackingsub-moduleassociates
the same targets across frames by assigning consistent IDs. [1] Haiyang Yu, Rui Jiang, Zhengbing He, Zuduo Zheng, Li Li, Runkun
Liu,andXiqunChen. Automatedvehicle-involvedtrafficflowstudies:
In this study, detection and tracking tasks are integrated into
Asurveyofassumptions,models,speculations,andperspectives.Trans-
a unified multi-object tracking module which first conducts portationresearchpartC:emergingtechnologies,127:103101,2021.INPROGRESS 5
Fig.2. ThetasksofUniE2EV2X
[2] Long Chen, Yuchen Li, Chao Huang, Bai Li, Yang Xing, Daxin Tian, [16] YushanHan,HuiZhang,HuifangLi,YiJin,CongyanLang,andYidong
Li Li, Zhongxu Hu, Xiaoxiang Na, Zixuan Li, et al. Milestones in Li. Collaborativeperceptioninautonomousdriving:Methods,datasets,
autonomous driving and intelligent vehicles: Survey of surveys. IEEE and challenges. IEEE Intelligent Transportation Systems Magazine,
TransactionsonIntelligentVehicles,8(2):1046–1056,2022. 2023.
[3] Ekim Yurtsever, Jacob Lambert, Alexander Carballo, and Kazuya [17] Sajjad Mozaffari, Omar Y Al-Jarrah, Mehrdad Dianati, Paul Jennings,
Takeda. A survey of autonomous driving: Common practices and and Alexandros Mouzakitis. Deep learning-based vehicle behavior
emergingtechnologies. IEEEaccess,8:58443–58469,2020. predictionforautonomousdrivingapplications:Areview. IEEETrans-
[4] JingRen,HossamGaber,andSkSamiAlJabar.Applyingdeeplearning actionsonIntelligentTransportationSystems,23(1):33–47,2020.
toautonomousvehicles:Asurvey.In20214thInternationalConference [18] Peng Hang, Chen Lv, Chao Huang, Jiacheng Cai, Zhongxu Hu, and
onArtificialIntelligenceandBigData(ICAIBD),pages247–252,2021. Yang Xing. An integrated framework of decision making and motion
[5] Steffen Hagedorn, Marcel Hallgarten, Martin Stoll, and Alexandru planning for autonomous vehicles considering social behaviors. IEEE
Condurache. Rethinkingintegrationofpredictionandplanningindeep transactionsonvehiculartechnology,69(12):14458–14469,2020.
learning-based automated driving systems: a review. arXiv preprint [19] Sampo Kuutti, Richard Bowden, Yaochu Jin, Phil Barber, and Saber
arXiv:2308.05731,2023. Fallah. A survey of deep learning applications to autonomous vehicle
control. IEEE Transactions on Intelligent Transportation Systems,
[6] BRaviKiran,IbrahimSobh,VictorTalpaert,PatrickMannion,AhmadA
22(2):712–733,2020.
Al Sallab, Senthil Yogamani, and Patrick Pe´rez. Deep reinforcement
learning for autonomous driving: A survey. IEEE Transactions on [20] JingyuanZhao,WenyiZhao,BoDeng,ZhenghongWang,FengZhang,
IntelligentTransportationSystems,23(6):4909–4926,2021. Wenxiang Zheng, Wanke Cao, Jinrui Nan, Yubo Lian, and Andrew F
Burke. Autonomous driving system: A comprehensive survey. Expert
[7] WeiLiu,MinHua,ZhiyunDeng,ZonglinMeng,YanjunHuang,Chuan
SystemswithApplications,page122836,2023.
Hu, Shunhui Song, Letian Gao, Changsheng Liu, Bin Shuai, et al. A
[21] Oskar Natan and Jun Miura. Fully end-to-end autonomous driving
systematic survey of control techniques and applications in connected
with semantic depth cloud mapping and multi-agent. arXiv preprint
andautomatedvehicles. IEEEInternetofThingsJournal,2023.
arXiv:2204.05513,2022.
[8] ChangzhuZhang,JinfeiHu,JianbinQiu,WeilinYang,HongSun,and
[22] Pranav Singh Chib and Pravendra Singh. Recent advancements in
Qijun Chen. A novel fuzzy observer-based steering control approach
end-to-end autonomous driving using deep learning: A survey. IEEE
forpathtrackinginautonomousvehicles. IEEETransactionsonFuzzy
TransactionsonIntelligentVehicles,2023.
Systems,27(2):278–290,2018.
[23] Siyu Teng, Xuemin Hu, Peng Deng, Bai Li, Yuchen Li, Yunfeng
[9] Zhangjing Wang, Yu Wu, and Qingqing Niu. Multi-sensor fusion in
Ai, Dongsheng Yang, Lingxi Li, Zhe Xuanyuan, Fenghua Zhu, et al.
automateddriving:Asurvey. IeeeAccess,8:2847–2868,2019.
Motionplanningforautonomousdriving:Thestateoftheartandfuture
[10] KeliHuang,BotianShi,XiangLi,XinLi,SiyuanHuang,andYikang perspectives. IEEETransactionsonIntelligentVehicles,2023.
Li. Multi-modal sensor fusion for auto driving perception: A survey.
[24] DanielCoelhoandMiguelOliveira.Areviewofend-to-endautonomous
arXivpreprintarXiv:2202.02703,2022.
drivinginurbanenvironments. IEEEAccess,10:75296–75311,2022.
[11] Zhong-guiMA,ZhuoLI,andYan-pengLIANG.Overviewandprospect [25] Shanzhi Chen, Jinling Hu, Yan Shi, Li Zhao, and Wen Li. A vision
of communication-sensing-computing integration for autonomous driv- of c-v2x: Technologies, field testing, and challenges with chinese
ing in the internet of vehicles. Chinese Journal of Engineering, development. IEEEInternetofThingsJournal,7(5):3872–3881,2020.
45(1):137–149,2023. [26] SohanGyawali,ShengjieXu,YiQian,andRoseQingyangHu. Chal-
[12] Salvador V Balkus, Honggang Wang, Brian D Cornet, Chinmay Ma- lenges and solutions for cellular based v2x communications. IEEE
habal, Hieu Ngo, and Hua Fang. A survey of collaborative machine CommunicationsSurveys&Tutorials,23(1):222–255,2020.
learning using 5g vehicular communications. IEEE Communications [27] Mario H Castan˜eda Garcia, Alejandro Molina-Galan, Mate Boban,
Surveys&amp;Tutorials,24(2):1280–1303,2022. JavierGozalvez,BaldomeroColl-Perales,TaylanS¸ahin,andApostolos
[13] Tejasvi Alladi, Vinay Chamola, Nishad Sahu, Vishnu Venkatesh, Adit Kousaridas. Atutorialon5gnrv2xcommunications. IEEECommuni-
Goyal, and Mohsen Guizani. A comprehensive survey on the applica- cationsSurveys&Tutorials,23(3):1972–2026,2021.
tionsofblockchainforsecuringvehicularnetworks. IEEECommunica- [28] Ardi Tampuu, Tambet Matiisen, Maksym Semikin, Dmytro Fishman,
tionsSurveys&amp;Tutorials,24(2):1212–1239,2022. andNaveedMuhammad. Asurveyofend-to-enddriving:Architectures
[14] FaisalHawlader,Franc¸oisRobinet,andRaphae¨lFrank. Leveragingthe and training methods. IEEE Transactions on Neural Networks and
edgeandcloudforv2x-basedreal-timeobjectdetectioninautonomous LearningSystems,33(4):1364–1384,2020.
driving. ComputerCommunications,213:372–381,2024. [29] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard
[15] Hongyang Li, Chonghao Sima, Jifeng Dai, Wenhai Wang, Lewei Lu, Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Mon-
Huijie Wang, Jia Zeng, Zhiqi Li, Jiazhi Yang, Hanming Deng, et al. fort,UrsMuller,JiakaiZhang,etal.Endtoendlearningforself-driving
Delvingintothedevilsofbird’s-eye-viewperception:Areview,evalu- cars. arXivpreprintarXiv:1604.07316,2016.
ationandrecipe. IEEETransactionsonPatternAnalysisandMachine [30] Zhengyuan Yang, Yixuan Zhang, Jerry Yu, Junjie Cai, and Jiebo Luo.
Intelligence,2023. End-to-endmulti-modalmulti-taskvehiclecontrolforself-drivingcarsINPROGRESS 6
with visual perceptions. In 2018 24th international conference on [49] HaibaoYu,YingjuanTang,EnzeXie,JileiMao,JiruiYuan,PingLuo,
patternrecognition(ICPR),pages2289–2294.IEEE,2018. andZaiqingNie. Vehicle-infrastructurecooperative3dobjectdetection
[31] FelipeCodevilla,EderSantana,AntonioMLo´pez,andAdrienGaidon. viafeatureflowprediction. arXivpreprintarXiv:2303.10552,2023.
Exploring the limitations of behavior cloning for autonomous driving. [50] Tianqi Wang, Sukmin Kim, Ji Wenxuan, Enze Xie, Chongjian Ge,
InProceedingsoftheIEEE/CVFinternationalconferenceoncomputer JunsongChen,ZhenguoLi,andPingLuo.Deepaccident:Amotionand
vision,pages9329–9338,2019. accidentpredictionbenchmarkforv2xautonomousdriving.InProceed-
[32] Tanmay Agarwal, Hitesh Arora, and Jeff Schneider. Learning urban ingsoftheAAAIConferenceonArtificialIntelligence,volume38,pages
driving policies using deep reinforcement learning. In 2021 IEEE 5599–5606,2024.
International Intelligent Transportation Systems Conference (ITSC), [51] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima,
pages607–614.IEEE,2021. Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer: Learning bird’s-
[33] MarwaAhmed,AhmedAbobakr,CheePengLim,andSaeidNahavandi. eye-view representation from multi-camera images via spatiotemporal
Policy-based reinforcement learning for training autonomous driving transformers. InEuropeanconferenceoncomputervision,pages1–18.
agents in urban areas with affordance learning. IEEE Transactions on Springer,2022.
IntelligentTransportationSystems,23(8):12562–12571,2021.
[34] ZhenboHuang,ShiliangSun,JingZhao,andLiangMao. Multi-modal
policy fusion for end-to-end autonomous driving. Information Fusion,
98:101834,2023.
[35] Yi Xiao, Felipe Codevilla, Akhil Gurram, Onay Urfalioglu, and Anto-
nioMLo´pez.Multimodalend-to-endautonomousdriving.IEEETrans-
actionsonIntelligentTransportationSystems,23(1):537–547,2020.
[36] Tengju Ye, Wei Jing, Chunyong Hu, Shikun Huang, Lingping Gao,
FangzhenLi,JingkeWang,KeGuo,WencongXiao,WeiboMao,etal.
Fusionad: Multi-modality fusion for prediction and planning tasks of
autonomousdriving. arXivpreprintarXiv:2308.01006,2023.
[37] JianyuChen,ShengboEbenLi,andMasayoshiTomizuka.Interpretable
end-to-end urban autonomous driving with latent deep reinforcement
learning. IEEE Transactions on Intelligent Transportation Systems,
23(6):5068–5078,2021.
[38] YihanHu,JiazhiYang,LiChen,KeyuLi,ChonghaoSima,XizhouZhu,
Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-
orientedautonomousdriving. InProceedingsoftheIEEE/CVFConfer-
enceonComputerVisionandPatternRecognition,pages17853–17862,
2023.
[39] Syed Adnan Yusuf, Arshad Khan, and Riad Souissi. Vehicle-
to-everything (v2x) in the autonomous vehicles domain–a techni-
cal review of communication, sensor, and ai technologies for road
user safety. Transportation Research Interdisciplinary Perspectives,
23:100980,2024.
[40] Qi Chen, Sihai Tang, Qing Yang, and Song Fu. Cooper: Cooperative
perceptionforconnectedautonomousvehiclesbasedon3dpointclouds.
In2019IEEE39thInternationalConferenceonDistributedComputing
Systems(ICDCS),pages514–524.IEEE,2019.
[41] Yen-ChengLiu,JunjiaoTian,Chih-YaoMa,NathanGlaser,Chia-Wen
Kuo,andZsoltKira. Who2com:Collaborativeperceptionvialearnable
handshakecommunication. In2020IEEEInternationalConferenceon
RoboticsandAutomation(ICRA),pages6876–6883.IEEE,2020.
[42] Yen-Cheng Liu, Junjiao Tian, Nathaniel Glaser, and Zsolt Kira.
When2com:Multi-agentperceptionviacommunicationgraphgrouping.
In Proceedings of the IEEE/CVF Conference on computer vision and
patternrecognition,pages4106–4115,2020.
[43] Yue Hu, Shaoheng Fang, Zixing Lei, Yiqi Zhong, and Siheng Chen.
Where2comm: Communication-efficient collaborative perception via
spatial confidence maps. Advances in neural information processing
systems,35:4874–4886,2022.
[44] Tsun-Hsuan Wang, Sivabalan Manivasagam, Ming Liang, Bin Yang,
Wenyuan Zeng, and Raquel Urtasun. V2vnet: Vehicle-to-vehicle com-
munication for joint perception and prediction. In Computer Vision–
ECCV2020:16thEuropeanConference,Glasgow,UK,August23–28,
2020,Proceedings,PartII16,pages605–621.Springer,2020.
[45] HongboYin,DaxinTian,ChunmianLin,XutingDuan,JianshanZhou,
DezongZhao,andDongpuCao. V2vformer++:Multi-modalvehicle-
to-vehicle cooperative perception via global-local transformer. IEEE
TransactionsonIntelligentTransportationSystems,2023.
[46] BradenHurl,RobinCohen,KrzysztofCzarnecki,andStevenWaslander.
Trupercept:Trustmodellingforautonomousvehiclecooperativepercep-
tionfromsyntheticdata. In2020IEEEIntelligentVehiclesSymposium
(IV),pages341–347.IEEE,2020.
[47] Yiming Li, Dekun Ma, Ziyan An, Zixun Wang, Yiqi Zhong, Siheng
Chen, and Chen Feng. V2x-sim: Multi-agent collaborative perception
dataset and benchmark for autonomous driving. IEEE Robotics and
AutomationLetters,7(4):10914–10921,2022.
[48] Runsheng Xu, Hao Xiang, Xin Xia, Xu Han, Jinlong Li, and Jiaqi
Ma. Opv2v: An open benchmark dataset and fusion pipeline for
perceptionwithvehicle-to-vehiclecommunication.In2022International
Conference on Robotics and Automation (ICRA), pages 2583–2589.
IEEE,2022.