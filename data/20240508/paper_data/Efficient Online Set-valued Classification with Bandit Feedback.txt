Efficient Online Set-valued Classification with Bandit Feedback
ZhouWang1 XingyeQiao1
Abstract systems,misidentifyingrealobstaclesasharmlessshadows
ontheroadpotentiallycausesabruptbrakingorevendan-
Conformal prediction is a distribution-free
gerous maneuvers. In medical diagnostics, the challenge
method that wraps a given machine learning
of differentiating between benign and malignant tumors
model and returns a set of plausible labels that
inambiguouscasescanresultincriticalmisdiagnoses,in-
containthetruelabelwithaprescribedcoverage
fluencingtreatmentdecisions. Suchscenariosunderscore
rate. Inpractice,theempiricalcoverageachieved
theneedformodelscapableofcautiouslyhandlingthose
highlyreliesonfullyobservedlabelinformation
observationswithhighuncertainty.
from data both in the training phase for model
fittingandthecalibrationphaseforquantileesti- Quantifyingtheuncertaintyassociatedwitheachobserva-
mation. Thisdependencyposesachallengeinthe tioncanbeaddressedbyreportingapredictionset,which
contextofonlinelearningwithbanditfeedback, canberealizedbysomeset-valuedclassificationparadigms
wherealearneronlyhasaccesstothecorrectness such as Classification with the Reject Option (Herbei &
of actions (i.e., pulled an arm) but not the full Wegkamp, 2006; Bartlett & Wegkamp, 2008; Charoen-
informationofthetruelabel. Inparticular,when phakdee et al., 2021; Zhang et al., 2018) and Conformal
thepulledarmisincorrect,thelearneronlyknows Prediction(Vovketal.,2005;Shafer&Vovk,2008;Balasub-
thatthepulledoneisnotthetrueclasslabel,but ramanianetal.,2014). Intuitivelyspeaking,anobservation
doesnotknowwhichlabelistrue. Additionally, withalargepredictionsetindicatesitsintrinsicdifficulty
banditfeedbackfurtherresultsinasmallerlabeled anditishardtobecorrectlyclassified.UnlikeClassification
datasetforcalibration,limitedtoinstanceswith withtheRejectOption,theConformalPredictionmethod
correctactions,therebyaffectingtheaccuracyof particularlyyieldsasetwithvalidpredictioncoverage,i.e.,
quantileestimation. Toaddresstheselimitations, apredictionsetincludesthetruelabelwithauser-prescribed
weproposeBanditClass-specificConformalPre- coveragerate1−α,α∈[0,1].
diction(BCCP),offeringcoverageguaranteeson
TheliteratureonConformalPrediction(andotherset-valued
a class-specific granularity. Using an unbiased
classificationmethods)coversvariousaspects. Forinstance,
estimationofanestimandinvolvingthetruelabel,
Leietal.(2013);Lei(2014);Leietal.(2015;2018);Sadinle
BCCPtrainsthemodelandmakesset-valuedin-
etal.(2019);Wang&Qiao(2018;2022)considerthecov-
ferencesthroughstochasticgradientdescent. Our
erage guarantees conditional on each class instead of the
approach overcomes the challenges of sparsely
standardmarginalcoverage(Vovketal.,2005). Romano
labeleddataineachiterationandgeneralizesthe
etal.(2020);Angelopoulosetal.(2021)exploredifferent
reliabilityandapplicabilityofconformalpredic-
(un)conformityscorestooutputinformativeconformalpre-
tiontoonlinedecision-makingenvironments.
diction sets. Tibshirani et al. (2019) introducesweighted
conformalpredictioninthesituationofcovariatedistribu-
1.Introduction tionshift,whileHechtlingeretal.(2018);Guan&Tibshirani
(2022);Wang&Qiao(2023)generalizeset-valuedpredic-
Machinelearningmodels,whilehighlyeffective,canfail tionstotherealmofout-of-distributiondetectionduetothe
incomplicatedscenariosduetoinherentuncertaintiesand semantic distribution shift by admitting an empty predic-
henceleadtoirreversibleconsequences,particularlyinhigh- tion set. However, these studies predominantly focus on
stake applications. For instance, in autonomous vehicle thesettingwithaccesstofull-labelinformationandoffline
training,limitingtheirapplicabilityinreal-worldscenarios.
1Department of Mathematics and Statistics, Binghamton
University, NewYork, USA.Correspondenceto: XingyeQiao RecentextensionsofConformalPredictiontoonlinelearn-
<xqiao@binghamton.edu>. ingsettings,(1)addressarbitrarydistributionshifts(Gibbs
& Candes, 2021; Gibbs & Cande`s, 2022; Zaffran et al.,
Proceedings of the 41st International Conference on Machine
2022;Bhatnagaretal.,2023),and(2)applytheprinciples
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by
theauthor(s). on the off-policy evaluation problem (Taufiq et al., 2022;
1
4202
yaM
7
]LM.tats[
1v39340.5042:viXraEfficientOnlineSet-valuedClassificationwithBanditFeedback
Zhangetal.,2023;Stantonetal.,2023)inreinforcement 2.Preliminary
learning. Yet,theseworksrequiresignificantlymorelabel
Inthissection,wereviewsomekeyconceptsofConformal
informationthanwhatbanditfeedbackaffords: inthedis-
PredictionandtheMulti-armedBanditProblem.
tributionshiftproblemwithfullfeedback,alearnerknows
thetruelabelregardlessofitsdecision’scorrectness;inthe
2.1.ConformalPrediction
policyevaluationproblem,thelearnerreceivesarewardthat
reflectstheoptimalityofthepulledarm. Incontrast,inthe
Conformalprediction(Vovketal.,2005;Leietal.,2015)isa
banditfeedbacksetting(Langford&Zhang,2007;Kakade
distribution-freemethodologythatcancomplementvarious
etal.,2008;Wangetal.,2010)alearneronlyreceivesfeed-
machinelearningmodels,suchasneuralnetworks,support
back about the correctness of predictions rather than the
vectormachines(SVMs),andrandomforests. Itisutilized
groundtruthoflabelinformation. Forinstance,alearnerin
toproduceset-valuedpredictionswithatheoreticallyguar-
TikTokcancorrectlycaptureapositiveattitudetowardthe
anteedcoveragerateprescribedbyusers.
videorecommendationthroughauser’sclick,whereasthe
user’spreferencesremainuncertainifthepresentedrecom- ConsideralabeledtrainingdatasetD ={(X i,Y i)} i∈I (I
mendationisdislikedbytheuser(itdoesnotknowwhatthe denotestheindexset)andatestinstanceX withunknown
userlikes). Similarly,inpersonalizedmedicine,amedical label Y, where both are assumed to be i.i.d. from an un-
system adjusts chemotherapy treatments based on partial knowndistributionoverthedomainX×Y.Intheclassifica-
feedback,suchastumorresponse,withoutfullknowledge tionproblem,theStandardConformalPredictionemploys
ofhowothertreatmentsmighthaveworkedforthatpatient. amapping(dependingonthedatasetD)C(cid:98):X (cid:55)→2Y and
returnsapredictionsetC(cid:98)(X)forthetestpointX,ensuring
MotivatedbythelimitedliteratureonConformalPrediction
themarginalcoveragerate
withinthecontextofonlinebanditfeedback,weintroduce
the Bandit Class-specific Conformal Prediction (BCCP) P(Y ∈C(cid:98)(X))≥1−α, (1)
framework for the multi-class classification problem. To
thebestofourknowledge, thisisthefirsteffortinapply- whereα∈[0,1]representsthepre-specifiednominalnon-
ing conformal prediction to this particular context. Our coverageratebypractitioners. Noticethattheprobabilityis
key contributions are as follows: (1) BCCP leverages an takenoverthetrainingdatasetDandthetestpoint(X,Y).
unbiasedestimatorforaccurategroundtruthinferenceof
ConsideringthatthemarginalcoverageguaranteeinStan-
labelinformation,allowingtheuseofthosedatainstances
dardConformalPredictionmaynotbeadequateforcertain
forwhichthewrongarmwaspulledinbothmodelfitting
specificclasses,Leietal.(2013;2015;2018);Sadinleetal.
andquantileestimation;(2)Ourmethodcapitalizesonthe
(2019) explored Class-conditional Conformal Prediction,
efficiency of stochastic gradient descent for dynamically
whichoffersclass-specificcoverage
updatingthequantileestimation,whichdifferentiatesitself
fromthetraditionalsplitconformalmethodinwhichsample
P(Y ∈C(cid:98)(X)|Y =k)≥1−α, ∀k ∈Y. (2)
quantilesbasedonasufficientlylargecalibrationdatasetare
used;(3)Wetheoreticallyprovethatboththeclass-specific
The same paradigm is also considered in Wang & Qiao
coverageandtheexcessriskwithrespecttothecheckloss
(2018;2022;2023). Itiscrucialtounderstandthatwhile
convergeatarateofO(T−1/2)undercertainconditions;(4)
(2)implies(1),theconverseisnotnecessarilytrue. Onthe
Recognizingthepracticalchallengeofselectinganoptimal
otherhand,comparedtothemarginalcoverage,theclass-
learningrateforupdatingthequantileestimation,weusean
specific coverage may yield larger prediction sets when
ensembleapproachtoupdatetheestimationwitharangeof
practitioners have limited data for each class. Motivated
learningrates;(5)TheeffectivenessofBCCPisempirically
by this limitation, Ding et al. (2024) proposed Clustered
validatedusingthreedifferentscorefunctionsandtwopoli-
Conformal Prediction to navigate this trade-off between
cies(forpullingarm)acrossthreedatasets,demonstrating
marginalandclass-specificcoverageinthelow-dataregime,
theversatilityandefficacyofourproposedframework.
while Romano et al. (2020); Angelopoulos et al. (2021)
Therestofthepaperisorganizedasfollows. InSection2, proposeddifferentscorefunctionstoimprovetheprediction
webeginwithareviewoftherelatedwork. Thisisfollowed setsizeespeciallywhentherearemanyclasses.
bySection3,whereweintroduceourmethodologycomple-
Ingeneral,ConformalPredictionstartswitha(conformity)
mentedbyaseriesofassociatedtheorems. InSection4,we scorefunction s : X ×Y (cid:55)→ R. Itis employedto gauge
presentexperimentstodemonstratetheeffectivenessofour
theproximityofanobservationX toanyclassk ∈Y. In-
method. TheconclusionstoourworkaregiveninSection5
tuitivelyspeaking,thelargertheconformityscores(X,k),
andproofsareattachedinAppendixA.
the higher the likelihood that the observation X belongs
totheclassk. Thisscorefunctioncanmanifestinvarious
forms,suchasthesoftmaxprobabilityinneuralnetworks,
2EfficientOnlineSet-valuedClassificationwithBanditFeedback
the functional margin in SVMs, or the average predicted Severalstudieshaveexploredthedomainofcontextualban-
classprobabilitiesoftreesinrandomforests. dits,wherethehypothesisspacecompriseslinearpredictors
(Kakadeetal.,2008;Wangetal.,2010;Crammer&Gentile,
Inthesplitconformalmethod(Papadopoulosetal.,2002;
2013;Abbasi-Yadkorietal.,2011;Gollapudietal.,2021;
Leietal.,2013),theindexsetI associatedwiththeoriginal
vanderHoevenetal.,2021). Theseworksfocusontheeffi-
datasetDispartitionedintotwodisjointsubsets: thetrain-
cacyoflinearmodelsincapturingtherelationshipbetween
ingpartI andthecalibrationpartI . Theformerisused
tr cal contextandactionrewards. However,thelinearrepresenta-
tofitamodelf inthetrainingphase,suchastraininganeu-
tionhasitslimitationsincapturingcomplexrelationships.
ralnetworktominimizecross-entropyloss(Romanoetal.,
2020;Angelopoulosetal.,2021),traininganSVMtomin- Inresponsetotheselimitations,recentstudieshavedelved
imizehingeloss(Wang&Qiao,2018;2022),orgrowing intoneuralcontextualbandits(Zhouetal.,2020;Jinetal.,
a random forest based on Gini-impurity (Guan & Tibshi- 2021;Zhangetal.,2021;Xuetal.,2022).Theseapproaches
rani,2022). Thismodelf isthenutilizedtocustomizethe leverage the expressive power of deep neural networks
aforementionedconformityscorefunctions. Forexample, to model the context-action relationship more effectively.
scouldbedirectlytakenasf,oramonotonicfunctionof Therearevariouspoliciesproposed,includingThompson
f, e.g., softmax score. With the conformity score estab- samplingandUpperConfidenceBoundalgorithms,tonav-
lished,thenextstepinvolvesidentifyingscorethresholds igatethebanditprobleminmorecomplexandnon-linear
τ ,k ∈Y withinthecalibrationpartI ,therebyenabling environments.
k cal
decision-makingfortheupcomingtestpoints. Insummary,
Despitetheseadvancementsinreinforcementlearning,the
apredictionsetforaqueryX withtheclass-specificcover-
existingliteratureprimarilyfocusesonpointpredictionand
ageguarantee(2)isdefinedas
lacksmechanismsforset-valuedpredictionandcoverage
control. Thisgapisparticularlyconcerningincriticaldo-
C(cid:98)(X):={k ∈Y :s(X,k)≥τ k},
mains, as discussed in Section 1. The issue is partially
wherethethresholdτ isdeterminedasthe100×α%sample addressedbyrecentworks(Taufiqetal.,2022;Zhangetal.,
k
quantileoftheconformityscoresforthecalibrationset,i.e., 2023;Stantonetal.,2023),whichapplyConformalPredic-
the (⌊|I |α⌋+1)-th smallest value in {s(X ,k)} tion to off-policy evaluation problems, thereby returning
cal i i∈Ical
(Romanoetal.,2019). Throughoutthisarticle,|·|being prediction sets. However, these researches diverge from
appliedonasetdenotesthesizeorcardinalityoftheset. ourwork,whichspecificallyaddressesthebanditproblem
setting. Ourfocusliesinintegratingset-valuedpredictions
withthebanditfeedbackframework, anareathathasnot
2.2.Multi-armedBanditandMulti-classClassification
beenextensivelyexplored,presentingbothnovelchallenges
The Multi-armed Bandit Problem (Lai & Robbins, 1985; andopportunitiesforadvancingthefield.
Aueretal.,2002)isafundamentalconceptinreinforcement
learning. Itpresentsascenariowherealearneraimstoop- 2.3.Set-valuedClassificationwithBanditFeedback
timizerewardsorminimizeregrets(cumulativelyassessed
fromfeedback)bypullingan“arm”(ortakinganaction),A, TheproposedBCCPmethod(summarizedinAlgorithm1)
fromasetofavailablearmsdenotedas{1,··· ,K},where aimstomakeset-valueddecisionswithacoverageguarantee
K represents the total number of arms. The selection of forinstancesfromthesamedistributionasthetrainingdata
an arm is guided by a policy π, tailored to maximize ex- inthebanditfeedbacksetting. Particularly,givenaquery
pectedgainsovertime. Thepolicyπcouldbeaprobability X t,thelearnerpullsanarmA t andreceivesthefeedback
distributiontogenerateanarmtopull,ordeterministic. 1{A t = Y t}. Withthisfeedback, thelearnerupdatesthe
modelandthresholdsinconformalprediction(lines4–5in
Whenextendedtomulti-classclassificationwithbanditfeed-
Algorithm1). Duringthetestphase,thelearnerreturnsthe
back,thisconceptincorporatescontextualinformationor
prediction set based on the trained model and thresholds
features, X, effectively transforming it into a contextual
(line3inAlgorithm1).
banditproblem. Particularlyinonlinelearningsettings,at
timepointt,thelearnerselectsanarmA ∼πforagiven Takehealthcareasanexample. Duetocostandsafetycon-
t
querycontextX ,andsubsequentlyreceivesbinaryfeed- cerns,insurancecompaniesmayonlyallowthehealthcare
t
back1{A = Y }. Thisfeedback,indicatingwhetherthe provider to prescribe one diagnostic test (e.g., X-ray, fol-
t t
pulledarm(class)matchesthetruelabelY ,introducesun- lowedbyCT,followedbycancerbiomarkerbloodtest,etc.)
t
certaintyregardingthetruelabel,complicatingthelearner’s at a time to the patient (this may be viewed as pulling a
updatingprocess. Forexample,differentfromthefullfeed- singlearm). Whenadiagnostictestturnsoutnegativefor
backsetting(Gibbs&Candes,2021;Gibbs&Cande`s,2022; asuspectcause,itisstillunknownwhatthecausereallyis
Bhatnagaretal.,2023),thelearnerherehasnoideaupon (thisisconsistentwithoursettinginwhichthelearneronly
thetruelabelforthequeryX ifthevalueoffeedbackis0. receivesabanditfeedbackthatconfirmsthecorrectnessof
t
3EfficientOnlineSet-valuedClassificationwithBanditFeedback
thepulledarmbutdoesnotnecessarilyrevealthetruelabel). Policydesigncanbeaflexibleprocess,influencedbyspe-
After a series of training over a large number of patients cific preferences such as the pursuit of simplicity or the
hasbeenconducted,wehaveadiagnosticsystemthatcan goal of minimizing estimation variance. In our research,
makepredictionsforanewpatientbasedonthepatient’s wetheoreticallyanalyzetheperformanceofcertainpolicies
profile. Unless for clear-cut cases, often it is much safer characterized by the associated properties, as detailed in
fortheprovidertoconsiderasetofmostplausiblecauses Corollaries3.3and3.5. Additionally,weconductempirical
anddesign thetreatment planthat considers allplausible evaluationsandcomparetheperformancesoftwodistinct
diseases, as opposed to treating the patient based on one policies: the softmax policy (softmax probability output
singlepredicteddisease. froma neuralnetwork asdefined in(4))and theuniform
policy(uniformdistribution). SeeSection4.
3.TowardstheBanditConformal
3.2.TheCross-entropyLosswithBanditFeedback
In this section, we introduce our method: Bandit Class-
Throughout this article, we train a neural network model
specific Conformal Prediction, specifically designed for
set-valuedmulti-classclassificationproblemsinanonline f W(X)=(f W1 (X),··· ,f W|Y|(X))⊤ ∈R|Y|,whichispa-
banditfeedbacksetting: let{(X ,Y )}T beasequenceof rameterizedbyasetofmatricescollectivelyrepresentedby
t t t=1
i.i.d. pointsfromthedomainX ×Y,wherealeanercannot W. Ourprimaryobjectiveinthetrainingphase,particularly
observethelabelY andreceivesthenon-zerofeedbackonly withinthebanditfeedbackcontext,istominimizeamodi-
t
whenanarmiscorrectlypulled. Weaimtoreportapredic- fiedversionofthecross-entropylossforeachinputquery
tionsetC(cid:98)t−1(X t)(thelearneronlyusestheinformationup X t,formulatedasfollows:
totimet−1)withaclass-specificcoverageguarantee.
(cid:88)
L(X ;W)=− ∆ ·log(pˆ(k |X )). (3)
t t,k t
Ourmethodologyentailsthreepivotalsteps:(1)estimatinga
k∈Y
groundtruthbasedonapolicyandfeedback,(2)trainingthe
modelwiththisestimation,and(3)estimatingthe100×α% By substituting ∆ t,k for the ground-truth label indicator
quantileτ k foreachclassk ∈Y. 1{Y t = k}, the loss function becomes an unbiased esti-
mator of the traditional cross-entropy loss with full feed-
3.1.Estimating1{Y t =k} back−log(pˆ(Y t |X t))byfollowingasimilarderivation
inProposition3.1. Thisallowsusinginformationinthose
In the bandit feedback context, for each query instance instanceswherethetruelabelY isnotexplicitlyavailable.
t
X , the learner pulls an arm A ∈ Y based on a given
t t
Theestimatedprobabilitymassfunctionpˆ(k |X )foreach
policy π := π (· | X ), effectively making an educated t
t t t
classk isderivedfromtheoutputsoftheneuralnetwork.
guessaboutthepotentialtruelabel. Theenvironmentthen
Specifically,itismodeledbyapplyingthesoftmaxfunction
providesbinaryfeedbackindicatingthecorrectnessofthe
chosenarm,i.e.,1{A
t
= Y t}. Asadirectobservationof tothelogitsf Wk (X t)producedbytheneuralnetwork:
Y isnotavailable,werelyonthefollowingestimationto
t exp(fk (X ))
1{Y t =k},i.e., pˆ(k |X t):=
(cid:80)
expW (fk˜t
(X
)), k ∈Y. (4)
1{A =k} k˜∈Y W t
∆ := t 1{A =Y }.
t,k π t(k |X t) t t Byintegratingtheestimator∆ t,k withthesoftmaxoutput,
ourmodelcanupdateefficientlybyoptimizingthetailored
Proposition3.1. ∆ servesasanunbiasedestimatorof
t,k loss function (3) with stochastic gradient descent. It is
1{Y =k}. Thisissubstantiatedbytheequation
t importanttonotethatonemayemployotherlossfunctions,
E (cid:2) ∆ (cid:3) =E (cid:2) ∆ |A =k(cid:3) ·π (k |X ) suchasthehingelossinSVMs(Kakadeetal.,2008).
πt t,k πt t,k t t t
+E (cid:2) ∆ |A ̸=k(cid:3) ·[1−π (k |X )] Figure1presentsaclearvisualizationofthecross-entropy
πt t,k t t t
lossacrossthreerealdatasetsinthebanditfeedbacksetting.
1{k =Y }
= t ·π (k |X )+0=1{Y =k}, Itshowsthemodelfittingperformancewiththesoftmaxand
π (k |X ) t t t
t t uniformpolicies. Theplotsillustratethatduringthemodel
wheretheexpectationistakenwithrespecttopolicyπ ,con- trainingphase,thesoftmaxpolicyconsistentlyachievesa
t
ditioningonallpreviousinformationandthepoint(X ,Y ). morerapidreductioninlosscomparedtotheuniformpolicy.
t t
Thissuperiorperformancecanbeattributedtothecontext-
Thisestimationframeworklaysthegroundworkforsubse- awarenatureofthesoftmaxpolicy,whichstrategicallypulls
quenttasksinourstudy.Itallowsustoeffectivelyutilizethe arms based on the specific context of each query. This
policy’scapabilitytolearntherealdata-generatingprocess approachnotonlyleadstoahigherfrequencyofaccurate
withoutexplicitknowledgeaboutthetruelabelY . predictionsbutalsoensuresbetterutilizationofdatapoints,
t
4EfficientOnlineSet-valuedClassificationwithBanditFeedback
CIFAR10 CIFAR100 SVHN
30
160
50
25 140
120 40
20
100
15 80 30
10 60 20
40
5 10
20
0 0 0
0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000
t t t
softmax uniform
Figure1. Accumulativecross-entropylossundersoftmaxpolicyanduniformpolicy.
therebyenhancingtheoverallefficiencyandeffectiveness p(x,y) is unknown, we instead employ a data-driven ap-
ofthemodeltrainingprocess. proachforquantileestimation: foreachdatapointconsider
theloss
3.3.TheQuantileoftheConformityScore
∆ ·ρ (s(X ,k),τ), (6)
t,k α t
Tocontroltheclass-specificcoverage,ourapproachlever-
agesthresholds/quantilesassociatedwithagivenconformity which is an empirical counterpart of the population loss
score function s(X,k), such as softmax, APS (Romano (5). Consequently,τ k canbedynamicallyupdatedthrough
et al., 2020), or RAPS (Angelopoulos et al., 2021) score stochastic gradient descent by computing the gradient,
(seethedefinitionsinAppendixA).Particularly,theprimary −∆ t,k·(cid:0) α−1{s(X t,k)<τ}(cid:1) ,oftheweightedloss(6).
goalinthisphaseistodeterminea100×α%quantileτ
k
of The updated quantiles τ k,k ∈ Y are then applied as the
thedistributionofs(X,k). Tothisend,thetraditionalsplit thresholdsfortheupcomingdatainthenextiterationonly.
conformal method (Papadopoulos et al., 2002; Lei et al., The complete process, including the model training and
2013)involvespartitioningavailablelabeleddataintotrain- quantileestimationinanonlinelearningcontext,isoutlined
ingandcalibrationsets.However,intheonlinesetting,since inAlgorithm1andFigure2.
weonlyhaveaccesstoalimiteddatasetateachiteration,
Algorithm1BanditConformal
splitconformalmayleadtotwoprimaryissues: (1)reduced
dataformodeltraining,and(2)largepredictionsetsdueto Require: InitializeweightmatricesW0andclass-specific
limitedlabeledcalibrationdata(Dingetal.,2024). These quantiles τ0 = 0,k ∈ Y. Provide a score function
k
twoissuesarefurtheraggravatedinthebanditfeedbackset- st(·,·)1,apolicyπ andlearningratesη ,η .
t 1 2
tingbecauseonlythosedatawhosecorrectarmsarepulled 1: fort=1,2,3,··· ,T do
areconsideredlabeled. 2: LearnerreceivesaqueryX t
3: Generatesapredictionsetforthequery:
To overcome these challenges, we adaptively update a
quantile estimate τ k by utilizing the check loss function C(cid:98)t−1(X t):=(cid:8) k ∈Y :st−1(X t,k)≥τ kt−1(cid:9)
(Takeuchietal.,2006;Koenker&BassettJr,1978;Romano
etal.,2019;Gibbs&Candes,2021)forquantileestimation: 4: LearnerpullsanarmA t ∼π t,receivesthefeedback
ρ (s,τ)=(s−τ)·(cid:0) α−1{s<τ}(cid:1) . 1{A t =Y t},andcomputes∆ t,k
α
5: Updatethenetworkweightmatricesandquantiles:
Moreconcretely,aclass-specific100×α%quantileτ ,k ∈
k 
Y isobtainedbysolvingthebelowoptimizationproblem: Wt=Wt−1−η 1∇ WL(X t;Wt−1)
argminE(cid:2) ρ (s(X,k),τ)|Y =k(cid:3) τt =τt−1+η ∆ (cid:0) α−1{st−1(X ,k)<τt−1}(cid:1)
α k k 2 t,k t k
τ
E(cid:2)1{Y
=k}·ρ
α(s(X,k),τ)(cid:3)
6: endfor
=argmin
τ
E(cid:2)1{Y =k}(cid:3)
=argminE(cid:2)1{Y
=k}·ρ
(s(X,k),τ)(cid:3)
, (5)
When comparing with Gibbs & Candes (2021); Gibbs &
α
τ Cande`s(2022);Zaffranetal.(2022);Bhatnagaretal.(2023),
where the second equality holds due to the fact that
E(cid:2)1{Y = k}(cid:3) = P(Y = k) does not rely on the quan- imp1 rW ese sa thd ad tt ih te des pu ep ne drs sc ori np tt ht eo nn euth rae ls nc eo tr we of ru kn uc pti do an teto duex pp tl oic ti -t tl hy
tile estimation. Given that the true joint density function iteration.Thesameargumentisappliedtoothernotations.
5
ssol
yportne-ssorCEfficientOnlineSet-valuedClassificationwithBanditFeedback
assumptiononE[1{Yt=k}
|F ]furthersuggeststhatthe
πt(k|Xt) t−1
policyshouldnotoverlyunderestimatetheproportionofa
U m qp o ud d aa net l te ia let nh sde tAP hu et;ll fR ea een dc bea air vm ce
k
f τW t−t− 11 S p C(cid:98)re tet −- dv 1ica (Xtl iu oe tnd ): Tcl oas ss o; mot ehe er xw teis ne t,th Te he em orp ei mric 3a .l 2co ev ne sr ua rg ee sg tha ap tm tha ey ain lgc ore ra its he m.
yieldspredictionsetswithsmallsizes. Thisisbecausean
algorithmwithalargepredictionsetsizeoftencomeswith
Xt inflatedcoverage,yetthetheoremstatesthattheempirical
non-coveragemustnotdeviatemuchawayfromthedesired
Figure2.Flowchartoftheonlinelearningwithbanditfeedback. non-coverageofα. Inparticular,Theorem3.2precludesthe
Hereτt−1 =(τ 1t−1,··· ,τ |t Y− |1)⊤. trivialcaseC(cid:98)t−1(X t)=Y forallt∈[T].
Thebelowcorollaryhighlightstheimpactofdifferentpoli-
acriticalaspectdifferentiatingourmethodliesinthequan- ciesontheconvergencerate.
tile updating process in addition to the model training in Corollary 3.3. Assume the learning rate has the order
thebanditfeedbackcontextaselucidatedinSection3.2. In η =O(T−1/2). (1)Ifthepolicyπ alignswiththeBayes
2 t
particular,theaforementionedstudiespredominantlywork posteriorprobability,i.e.,π (k | X ) = P(Y = k | X ),
t t t t
withtheunweightedquantileestimationandrequireveri- then we have E[1{Yt=k} | F ] = bt ≤ 1, and hence
ficationofwhetherthetruelabelY
t
fallsinitsprediction √πt(k|Xt) t−1 k
set C(cid:98)t−1(X t) in their updating rules. This verification is CvgGap k =O( TkT).(2)Ifthepolicyistheuniformdistri-
typicallyachievedeitherbydirectlyutilizingexplicitlabel bution, i.e., π t(k | X t) = |Y1 |, then bt k ≤ |Y|p k (here
informationorthroughmultiplearmpullsuntilthetruelabel p denotes the prior probability of class k), and hence
k √
isascertainedwithabsolutecertainty. Suchmethodologies
CvgGap =O(
T|Y|pk).
arenotfeasibleinoursettingfortwoprimaryreasons: (1) k Tk
welackdirectaccesstothetruelabelinformation,and(2) Corollary 3.3 implies a convergence rate of CvgGap =
k
our framework does not permit multiple arm pulls for a O(T−1/2) when the learning rate η = O(T−1/2) and
2
singledecisioninstance. Incontrast,ourapproach(seethe sample size T = O(T), k ∈ Y under both Bayes pos-
k
updatingruleinAlgorithm1)involvescomputingthegra- terior probability and uniform probability policies. In
dientoftheweightedcheckloss(6)inthebanditfeedback our experiments, due to the lack of access to the precise
setting, whichisanunbiasedestimatorofthegradientof data distribution, we instead use the softmax policy, i.e.,
unweightedchecklossinthefullfeedback. Thisprocessis π (k | X ) = pˆ(k | X )asdefinedin(4),toestimatethe
t t t
tailoredtobanditfeedbackenvironmentswhereeachquery Bayesposteriorprobability. Asnotedby Tibshirani etal.
allowsonlyasinglearmpull. (2019),therearealternativemethodsforprobabilityestima-
tion,suchasmomentmatchingandKullback-LeiblerDiver-
The below theorem implies the empirical coverage con-
genceminimization. Werefertorelatedwork(Sugiyama
vergestotheprescribedcoverage.
etal.,2012)foracomprehensivereview.
Theorem 3.2. Define the filtration F := (σ(X ,Y )×
t t t
Theorem 3.4. Let p be the prior probability of
σ(π )) ∪ F . Assume π (k | X ) ≥ c > 0 for all k
t ∈t [T] andt− E1 [1{Yt=k} | Ft ] =t bt. Wik th probability class k ∈ Y, and τ k∗ = argmin τ T1 (cid:80)T t=11{Y t =
πt(k|Xt) t−1 k k}ρ α(st−1(X t),τ) be the quantile estimate using all
atleast1−δ takenoveralltherandomness,forallclass
the data instances. Define the empirical regret associ-
k ∈Y,Algorithm1yieldstheempiricalcoveragegap
ated with the check loss in the bandit feedback setting
as Reg (T) := 1 (cid:80)T ∆ ρ (st−1(X ),τt−1) −
CvgGap k :=(cid:12) (cid:12) (cid:12) (cid:12)α− T1 (cid:88)T 1{Y t =k}·1{Y t ̸∈C(cid:98)t−1(X t)}(cid:12) (cid:12) (cid:12) (cid:12) T1 (cid:80)T t=k 1, 1ρα {Y t = k}ρ αT (st−t 1= (1 X tt ), ,k τ k∗α ). Bychot osink gη 2 =
k t=1 τ∗p1/2(cid:0)(cid:80)T E(cid:2)1{Yt=k}(cid:3)(cid:1)−1/2 ,Algorithm1yieldsanex-
τT ζ (T,δ/|Y|) k k t=1 π t2(k|Xt)
≤ k + k , pectedregret
η T T
2 k k (cid:118)
w The =re (cid:80)ζ
k
T(T, 1δ {) Y=
=3
k2
c }k
.log2
δ
+ (cid:113) 2log2
δ
·(cid:80)T t=1bt k, and E[Reg k,ρα(T)]≤ τ Tk∗(cid:117) (cid:117) (cid:116)p k(cid:88) t=T 1E(cid:20) π1 t2{ (Y kt |= Xk t} )(cid:21) .
k t=1 t
Theaboveexpectationistakenoveroveralltherandomness,
Theorem 3.2 implies the convergence rate of the class-
includingthedataandalgorithm. Notethatτ∗isbounded,
specificcoverageguaranteemainlydependsonthelearning k
andhencetheupperboundconvergesto0.
rateη andthesamplesizeT ofclassk. Besidesthepol-
2 k
icyshouldbeboundedstrictlybelowby0, theadditional Corollary3.5. Fortheuniformpolicyandanappropriately
6EfficientOnlineSet-valuedClassificationwithBanditFeedback
chosenη asspecifiedinTheorem3.4,theexpectedregret basedonpastperformance. Theguidingprincipleisthatas
2
E[Reg (T)]≤ τ∗ √|Y|pk. theaccumulatedchecklossdecreases,theattentionplaced
k,ρα T
onthecorrespondingestimatedquantilegrows.
Corollary3.5indicatesthattheexpectedregretadheresto
Theorem 3.6 below shows that the aggregated quantile
atheoreticalconvergencerateofO(T−1/2),underthecon-
through the experts converges to the optimal quantile es-
dition that the learning rate η = O(T−1/2) (it can be
2 timateamongtheexperts. Specifically,anincreaseinthe
achievedwhenthepolicyisboundedstrictlybelowby0).
numberofexperts,whilemaintainingtheorderJ =O(1),
ThisconditionalignswiththefindingsinCorollary3.3.
canenhancethechanceofachievinganimprovedlearning
Both Theorem 3.4 and Corollary 3.5 provide theoretical rate,alongwithmoreaccuratequantileestimations. This
guaranteesfortheconvergencebehaviorofAlgorithm1in findingunderscorestheimportanceofexpertintegrationin
aparametricrate,indicatingitspotentialeffectiveness. This improvingalgorithmicperformanceifonehasnoprioridea
resultshowsthatthereexistssuchalearningrateη leading oftheoptimallearningrate.
2
toanoptimalconvergencerate. Howtopracticallyobtain
Theorem3.6. Considerτ¯t−1 astheaggregatedquantile
such a precise learning rate is a challenging problem. In k
acrossJ(≥ 2)expertsasdefinedinAlgorithm2, andthe
practice,asdiscussedintheworkofGibbs&Candes(2021),
samec definedinTheorem3.2. Then,Algorithm2yields
the chosen value of η leads to two distinct scenarios. A k
2
largervalueofη mayleadtounstablequantileestimations,
2
causingoscillationsinpredictionsetsizes. Overtime,this 1 (cid:88)T
could result in increasingly larger prediction sets in the
T
∆ t,kρ α(st−1(X t),τ¯ kt−1)
onlinelearningprocess. Conversely,asmallervalueofη t=1
2
slowstheconvergencerateofthecoverage, necessitating 1 (cid:88)T
− min ∆ ρ (st−1(X ),τt−1)
moreiterationstoachievedesiredcoveragelevels. j∈[J]T t,k α t j,k
t=1
Algorithm2BanditConformalwithExperts 1 2lnJ
≤ √ + √ .
Require: Initialize weight matrices W0, class-specific 4c2 k T T
quantilesτ0 =0,andexpertsweightsω0 =1, j ∈
j,k j,k
[J],k ∈ Y. A score function st(·,·), a policy π and
t Heretheassumptionforc isreasonablyflexibleasitcan
learningratesη ,η ,j ∈[J]. k
1 2,j beachievedthroughthepolicydesign.
1: fort=1,2,3,··· ,T do
2: LearnerreceivesaqueryX t Noticethat,theoretically,theoptimalchoiceoflearningrate
3: Generatesapredictionsetforthequery: should vary depending on the class as indicated in Theo-
rem3.4. However,fortheeaseofpracticalimplementation,
C(cid:98)t−1(X t):=(cid:8) k ∈Y :st−1(X t,k)≥τ¯ kt−1(cid:9) , thesamevalueofη 2(orη 2,j)isappliedacrossallclasses.
whereτ¯t−1 =(cid:80) ωt−1τt−1/(cid:80) ωt−1
k j j,k j,k i i,k 4.Experiments
4: LearnerpullsanarmA t ∼π t,receivesthefeedback
1{A t =Y t},andcomputes∆ t,k Set-up: To assess the effectiveness of our proposed ap-
5: Updateallweightsandquantiles: proach, we employ the ResNet50 architecture (He et al.,
2016)formodelfitting. Ourexperimentalsetupincludes

 τW tt= =W τt−t− 11 +− ηη 1∇ ∆WL (cid:0)( αX −t; 1W {st t− −1 1)
(X,k)<τt−1}(cid:1)
t Sh Ve HC NIF dA aR ta1 s0 e, tsC
,
eIF aA chR u1 n0 d0 er( gw oi it nh g2 50 rc eo pa lir cs ae tr iol nab s.el Cs) o, na sn isd
-
j,k j,k 2,j t,k t j,k
ω jt ,k=exp(−√ t1
+1
t(cid:80) ′≤t∆ t′,k·ρ α(st′−1(X t′,k),τ jt ,′ k−1)) t re an tetly αth =ro 0u .g 0h 5o .ut Foth re cs ot mud py u, taw tie onm aa li en ft fiai cn iea ncn yo ,n t- hc eov me ora dg ee
l
trainingisperformedondatabatchesofsize256,utilizing
6: endfor
theADAMoptimizerwithalearningrateofη =10−4in
1
themodeltrainingphase. Theentireonlinelearningprocess
To mitigate the above limitation due to the choice of η ,
2 spansT =6000iterationsaround. Weevaluateonlineclas-
wedrawinspirationfromtheadaptivecontrolmethodinits
sificationperformanceusingthreescorefunctions: softmax,
fullfeedbacksetting(Zaffranetal.,2022). Weintroduce
APS, and RAPS (see their definition in Appendix A) for
an alternative algorithm, Bandit Conformal with Experts
boththesoftmaxpolicyandtheuniformpolicy.
(outlined in Algorithm 2), which eliminates the need for
manualtuningofη . Specifically,givenagridoflearning
2
ratevaluesη ,j ∈[J],itemploysanensemblemethodol- Metrics: Toexaminetheperformanceduringonlinepre-
2,j
ogytoaggregateestimatedquantilesassociatedwithη ’s dictionfort∈[T],wereportboththeminimumandmaxi-
2,j
7EfficientOnlineSet-valuedClassificationwithBanditFeedback
mumaccumulativecoverage,definedas: numberofiterationsincreases,thetoppanelsinFigures3
and 4 reveal that Algorithm 1 effectively approaches the
Acum cvg min(t)=minAcum cvg(t,k), prescribed class-specific coverage of 95%. Additionally,
k∈Y
thebottompanelsinthesefiguresindicateatrendtowards
Acum cvg max(t)=maxAcum cvg(t,k),
k∈Y smallerpredictionsets.
whereAcum cvg(t,k)isdefinedas The choice of learning rate η 1 indeed affects the perfor-
mance of the model training phase and hence the subse-
(cid:80)t s=1(cid:80)
(cid:80)Xi∈ tBs
(cid:80)1{Y
i
=k 1& {YY
i
=∈ kC(cid:98) }t−1(X i)}
,
q fou ce un stq ou nan thti ele roes letim ofat ηio 2n i. nH sto ew adev oe fr, pi an rto iu cr uls atu rld yy, ow pte imm ia zi in nl gy
s=1 Xi∈Bs i forη 1. Forexample,theCIFAR100experimentsutilizing
the softmax policy and softmax score are presented with
withB representingthebatchofthedatasetattimepoints.
s
a fine-tuned η = 5×10−4 (see the tuning strategy and
Weincludetheaccumulativepredictionsetsize, 2
sensitivitystudiesaboutη inAppendixB).Asdiscussed
2
Acum size(t)=
(cid:80)t s=1(cid:80)
(cid:80)X ti∈Bs
|B|C(cid:98)t |−1(X i)|
,
b pe al ro aw meC to erro ηll 2ar cy an3. r5 e, sa un ltin inap ep nr lo ap rgri ea dte ps re el de ic ct tio ion no sf et th se izh ey spe or r-
s=1 s prolongedconvergencetimes,whichhindersthepractical
applicabilityofAlgorithm1inmoredynamicsettings.
toassesstheinformativenessoftheset-valuedclassification.
CIFAR10 CIFAR100 SVHN CIFAR10 CIFAR100 SVHN
1.00 1.00 1.00 1.00 1.00 1.00
0.95 0.95 0.95 0.95 0.95 0.95
0.90 0.90 0.90 0.90 0.90 0.90
0.85 0.85 0.85 0.85 0.85 0.85
0.80 0.80 0.80 0.80 0.80 0.80
0.75 0 2000 4000 60000.75 0 2000 4000 60000.75 0 2000 4000 6000 0.75 0 2000 4000 60000.75 0 2000 4000 60000.75 0 2000 4000 6000
10 20 10 10 20 10
8 15 8 8 15 8
6 10 6 6 10 6
4 4 4 4
5 5
2 2 2 2
0 2000 t 4000 6000 0 2000 t 4000 6000 0 2000 t 4000 6000 0 2000 t 4000 6000 0 2000 t 4000 6000 0 2000 t 4000 6000
score softmax APS RAPS Acum_cvg min max score softmax APS RAPS Acum_cvg min max
Figure3.PerformancesunderAlgorithm1withsoftmaxpolicy. Figure5.PerformancesunderAlgorithm2withsoftmaxpolicy.
Theblackdottedlinesinthebottompaneldenotetheoracleper-
formanceofthemodelwithaccesstothefulllabels.
CIFAR10 CIFAR100 SVHN
1.00 1.00 1.00
CIFAR10 CIFAR100 SVHN 0.95 0.95 0.95
1.00 1.00 1.00
0.90 0.90 0.90 0.95 0.95 0.95
0.85 0.85 0.85
0.90 0.90 0.90
0.80 0.80 0.80
0.85 0.85 0.85
0.80 0.80 0.80 0.7 15 0 0 2000 4000 60000.7 25 0 0 2000 4000 60000.7 15 0 0 2000 4000 6000 8000
0.7 15 0 0 2000 4000 60000.7 25 0 0 2000 4000 60000.7 15 0 0 2000 4000 6000 8 15 8
6 6 8 15 8 10
4 4
6 10 6 5
2 2
4 4
2 5 2 0 2000 t 4000 6000 0 2000 t 4000 6000 0 2000 400 t0 6000 8000
score softmax APS RAPS Acum_cvg min max
0 2000 4000 6000 0 2000 4000 6000 0 2000 4000 6000
t t t
score softmax APS RAPS Acum_cvg min max Figure6.PerformancesunderAlgorithm2withuniformpolicy.
Figure4.PerformancesunderAlgorithm1withuniformpolicy.
Theblackdottedlinesinthebottompaneldenotetheoracleper- To address this limitation, in this study, we employed a
formanceofthemodelwithaccesstothefulllabels. rangeoflearningratevalues,i.e.,[0.1,0.01,0.001,0.0001],
throughanexpert-basedapproachinAlgorithm2. There-
Results: Figures3and4presenttheset-valuedclassifica- sultsareshowninFigures5and6. Notably,whileusingthe
tionwithBCCPinthebanditfeedbacksettingundersoftmax softmaxpolicy,theresultsfromFigure5indicatethatthe
anduniformpolicies,respectively. Theblackdottedlinesin predictionsetsizesfromAlgorithm2areonlymarginally
thebottompanelforeachfiguredenotethefinalresultofa largercomparedtothosefromAlgorithm1withcarefully
networkaftersufficientlymanyiterationswithaccesstothe tuned η . With the uniform policy, Algorithm 2 demon-
2
fulllabelsandtheusageoftheRAPSscorefunction. Asthe stratesmoreefficientperformance,yieldingsmallerpredic-
8
etar
egarevoC
ezis
tes
noitciderP
etar
egarevoC
ezis
tes
noitciderP
etar
egarevoC
ezis
tes
noitciderP
etar
egarevoC
ezis tes
noitciderPEfficientOnlineSet-valuedClassificationwithBanditFeedback
tion sets for the CIFAR10 and SVHN datasets. Notably, References
the RAPS score function outperforms the other scores in
Abbasi-Yadkori,Y.,Pa´l,D.,andSzepesva´ri,C. Improved
producingsmallerpredictionsetsonthedatasetwhenthere
algorithms for linear stochastic bandits. Advances in
aremanyclasses,i.e.,CIFAR100.
neuralinformationprocessingsystems,24,2011.
5.Conclusion Angelopoulos,A.N.,Bates,S.,Jordan,M.,andMalik,J.
Uncertainty sets for image classifiers using conformal
Inthisarticle,weextendConformalPredictiontotheframe- prediction. In International Conference on Learning
workofonlinebanditfeedback,wherealearnerisonlytold Representations,2021. URLhttps://openreview.
whetherornotapulledarmiscorrectinadynamicmulti- net/forum?id=eNdiU_DbM9.
classclassificationproblem. Wemakeuseofanunbiased
indicator function estimation of the ground truth to over- Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time
cometheincompleteinformationinthefeedback,allowing analysis of the multiarmed bandit problem. Machine
theproposedBanditClass-specificConformalPrediction learning,47:235–256,2002.
(BCCP)toeffectivelymakeset-valuedinferencesandadap-
Balasubramanian,V.,Ho,S.-S.,andVovk,V. Conformal
tivelyfitthemodelaccordingly. Particularly,theindicator
predictionforreliablemachinelearning: theory,adapta-
function estimation allows us to utilize stochastic gradi-
tionsandapplications. Newnes,2014.
ent descent to efficiently achieve the quantile estimation
instead of the traditional split conformal, which requires Bartlett, P. L. and Wegkamp, M. H. Classification with
sufficientlabeledcalibrationdataandmightnotberealistic arejectoptionusingahingeloss. JournalofMachine
inthesettingofbanditfeedback.Theoretically,weshowthe LearningResearch,9(Aug):1823–1840,2008.
O(T−1/2)convergencerateforboththecoverageguarantee
and the regret of the check loss under certain conditions. Bhatnagar,A.,Wang,H.,Xiong,C.,andBai,Y. Improved
Empirically, theexperimentsconductedonthreedatasets onlineconformalpredictionviastronglyadaptiveonline
withthreescorefunctionsandtwopoliciesdemonstrated learning. InInternationalConferenceonMachineLearn-
theeffectivenessofBCCP. ing,pp.2337–2363.PMLR,2023.
Our research opens several promising avenues for future Cesa-Bianchi,N.andLugosi,G. Prediction,learning,and
exploration. Onepotentialdirectionistheinvestigationof games. Cambridgeuniversitypress,2006.
alternativeindicatorfunctionestimationsorpolicydesigns
Charoenphakdee, N., Cui, Z., Zhang, Y., and Sugiyama,
that could offer improved theoretical or empirical perfor-
M. Classificationwithrejectionbasedoncost-sensitive
mance. Additionally,refiningthecoverageguaranteewithin
classification. InInternationalConferenceonMachine
specific fixed-size time windows (Bhatnagar et al., 2023)
Learning,pp.1507–1517.PMLR,2021.
insteadofthefull-timehorizoninourworkcouldfurther
bolster the reliability of BCCP over different time scales. Crammer,K.andGentile,C. Multiclassclassificationwith
Moreover,expandingthescopeofBCCPtoaddresschal- banditfeedbackusingadaptiveregularization. Machine
lengessuchascovariateshift(Tibshiranietal.,2019)and learning,90(3):347–383,2013.
semantic shift (Wang & Qiao, 2023) could significantly
broadenitsapplicability. Ding,T.,Angelopoulos,A.,Bates,S.,Jordan,M.,andTib-
shirani,R.J. Class-conditionalconformalpredictionwith
In conclusion, ourwork notonly contributes anoveland
manyclasses. AdvancesinNeuralInformationProcess-
provablesolutiontotheproblemofonlinemulti-classclas-
ingSystems,36,2024.
sification with bandit feedback but also sets another new
directioninconformalprediction. Itopensuppossibilities Gibbs, I. and Candes, E. Adaptive conformal inference
forreal-worldapplicationsandlaysafoundationforfurther underdistributionshift. AdvancesinNeuralInformation
researchdomains. ProcessingSystems,34:1660–1672,2021.
Gibbs,I.andCande`s,E.Conformalinferenceforonlinepre-
ImpactStatements
dictionwitharbitrarydistributionshifts. arXivpreprint
arXiv:2208.08401,2022.
This paper presents work that aims to advance the field
of Machine Learning. There are many potential societal
Gollapudi,S.,Guruganesh,G.,Kollias,K.,Manurangsi,P.,
consequencesofourwork,noneofwhichwefeelmustbe
Leme, R., and Schneider, J. Contextual recommenda-
specificallyhighlightedhere.
tionsandlow-regretcutting-planealgorithms. Advances
in Neural Information Processing Systems, 34:22498–
22508,2021.
9EfficientOnlineSet-valuedClassificationwithBanditFeedback
Guan,L.andTibshirani,R. Predictionandoutlierdetection Romano,Y.,Patterson,E.,andCandes,E. Conformalized
inclassificationproblems. JournaloftheRoyalStatisti- quantileregression. Advancesinneuralinformationpro-
calSociety.SeriesB,StatisticalMethodology,84(2):524, cessingsystems,32,2019.
2022.
Romano,Y.,Sesia,M.,andCandes,E. Classificationwith
He,K.,Zhang,X.,Ren,S.,andSun,J. Deepresiduallearn- validandadaptivecoverage. AdvancesinNeuralInfor-
ingforimagerecognition. InProceedingsoftheIEEE mationProcessingSystems,33:3581–3591,2020.
conferenceoncomputervisionandpatternrecognition,
Sadinle,M.,Lei,J.,andWasserman,L. Leastambiguous
pp.770–778,2016.
set-valuedclassifierswithboundederrorlevels. Journal
Hechtlinger,Y.,Po´czos,B.,andWasserman,L. Cautious oftheAmericanStatisticalAssociation, 114(525):223–
deeplearning. arXivpreprintarXiv:1805.09460,2018. 234,2019.
Herbei,R.andWegkamp,M.H. Classificationwithreject Shafer,G.andVovk,V. Atutorialonconformalprediction.
option. The Canadian Journal of Statistics/La Revue JournalofMachineLearningResearch,9(Mar):371–421,
CanadiennedeStatistique,pp.709–721,2006. 2008.
Jin,T.,Xu,P.,Shi,J.,Xiao,X.,andGu,Q. Mots: Minimax Stanton,S.,Maddox,W.,andWilson,A.G. Bayesianopti-
optimalthompsonsampling. InInternationalConference mizationwithconformalpredictionsets. InInternational
onMachineLearning,pp.5074–5083.PMLR,2021. Conference on Artificial Intelligence and Statistics, pp.
959–986.PMLR,2023.
Kakade, S. M., Shalev-Shwartz, S., and Tewari, A. Effi-
cientbanditalgorithmsforonlinemulticlassprediction. Sugiyama,M.,Suzuki,T.,andKanamori,T. Densityratio
InProceedingsofthe25thinternationalconferenceon estimationinmachinelearning. CambridgeUniversity
Machinelearning,pp.440–447,2008.
Press,2012.
Koenker,R.andBassettJr,G. Regressionquantiles. Econo-
Takeuchi, I., Le, Q. V., Sears, T. D., and Smola, A. J.
metrica: journaloftheEconometricSociety,pp.33–50,
Nonparametricquantileestimation. JournalofMachine
1978. LearningResearch,7(45):1231–1264,2006.URLhttp:
//jmlr.org/papers/v7/takeuchi06a.html.
Lai,T.L.andRobbins,H. Asymptoticallyefficientadaptive
allocationrules. Advancesinappliedmathematics,6(1):
Taufiq,M.F.,Ton,J.-F.,Cornish,R.,Teh,Y.W.,andDoucet,
4–22,1985.
A. Conformaloff-policypredictionincontextualbandits.
Langford, J. and Zhang, T. The epoch-greedy algorithm
AdvancesinNeuralInformationProcessingSystems,35:
forcontextualmulti-armedbandits. Advancesinneural 31512–31524,2022.
informationprocessingsystems,20(1):96–1,2007.
Tibshirani,R.J.,FoygelBarber,R.,Candes,E.,andRamdas,
Lei,J. Classificationwithconfidence. Biometrika,101(4): A. Conformalpredictionundercovariateshift. Advances
755–769,2014. inneuralinformationprocessingsystems,32,2019.
Lei, J., Robins, J., and Wasserman, L. Distribution-free vanderHoeven, D., Fusco, F., andCesa-Bianchi, N. Be-
predictionsets. JournaloftheAmericanStatisticalAsso- yondbanditfeedbackinonlinemulticlassclassification.
ciation,108(501):278–287,2013. Advancesinneuralinformationprocessingsystems,34:
13280–13291,2021.
Lei,J.,Rinaldo,A.,andWasserman,L. Aconformalpre-
diction approach to explore functional data. Annals of Vovk, V., Gammerman, A., and Shafer, G. Algorithmic
MathematicsandArtificialIntelligence,74(1-2):29–43, learninginarandomworld.SpringerScience&Business
2015. Media,2005.
Lei, J., G’Sell, M., Rinaldo, A., Tibshirani, R. J., and Wang,S.,Jin,R.,andValizadegan,H. Apotential-based
Wasserman,L. Distribution-freepredictiveinferencefor framework for online multi-class learning with partial
regression. JournaloftheAmericanStatisticalAssocia- feedback. InProceedingsoftheThirteenthInternational
tion,113(523):1094–1111,2018. Conference on Artificial Intelligence and Statistics, pp.
900–907.JMLRWorkshopandConferenceProceedings,
Papadopoulos,H.,Proedrou,K.,Vovk,V.,andGammerman,
2010.
A. Inductiveconfidencemachinesforregression. InMa-
chineLearning:ECML2002:13thEuropeanConference Wang,W.andQiao,X. Learningconfidencesetsusingsup-
onMachineLearningHelsinki,Finland,August19–23, portvectormachines. InAdvancesinNeuralInformation
2002Proceedings13,pp.345–356.Springer,2002. ProcessingSystems,pp.4929–4938,2018.
10EfficientOnlineSet-valuedClassificationwithBanditFeedback
Wang,W.andQiao,X. Set-valuedsupportvectormachine
withboundederrorrates. JournaloftheAmericanStatis-
ticalAssociation,pp.1–13,2022.
Wang, Z. and Qiao, X. Set-valued classification with
out-of-distribution detection for many classes. Jour-
nal of Machine Learning Research, 24(375):1–39,
2023. URL http://jmlr.org/papers/v24/
23-0712.html.
Xu, P., Wen, Z., Zhao, H., andGu, Q. Neuralcontextual
banditswithdeeprepresentationandshallowexploration.
InInternationalConferenceonLearningRepresentations,
2022. URLhttps://openreview.net/forum?
id=xnYACQquaGV.
Zaffran,M.,Fe´ron,O.,Goude,Y.,Josse,J.,andDieuleveut,
A. Adaptiveconformalpredictionsfortimeseries. InIn-
ternationalConferenceonMachineLearning,pp.25834–
25866.PMLR,2022.
Zhang,C.,Wang,W.,andQiao,X. Onrejectandrefineop-
tionsinmulticategoryclassification. JournaloftheAmer-
icanStatisticalAssociation,113(522):730–745,2018.
Zhang, W., Zhou, D., Li, L., and Gu, Q. Neural thomp-
sonsampling. InInternationalConferenceonLearning
Representations,2021. URLhttps://openreview.
net/forum?id=tkAtoZkcUnm.
Zhang,Y.,Shi,C.,andLuo,S. Conformaloff-policypre-
diction. InInternationalConferenceonArtificialIntelli-
genceandStatistics,pp.2751–2768.PMLR,2023.
Zhou,D.,Li,L.,andGu,Q. Neuralcontextualbanditswith
ucb-basedexploration. InInternationalConferenceon
MachineLearning,pp.11492–11502.PMLR,2020.
11EfficientOnlineSet-valuedClassificationwithBanditFeedback
A.ConformityScores
Letpˆ(k |X),k ∈Y asdefinedin(4)betheestimatedposteriorprobabilitybasedontheneuralnetworkf (X). Thus,for
W
thetestpoint(X,Y),thesoftmaxscoreisdefinedas
s(X,k)=pˆ(k |X).
Sorttheestimatedposteriorprobabilitiespˆ(k |X),k ∈Y withtheascendingordersuchthatpˆ(k |X)≤pˆ(k |X)≤
1 2
···≤pˆ(k |X).Additionally,denoterastheindexsuchthatk =Y. Thus,theAPSscoreisdefinedas
|Y| r
r−1
(cid:88)
s(X,k)=1− pˆ(k |X)−U ·pˆ(k |X),
l r
l=1
whereU isarandomvariablesampledfromtheuniformdistributionontheinterval[0,1].
Letk bethenumberabovewhichthepredictionsetsizewillbepenalizedwiththepenaltyλ. Thus,theRAPSisdefined
reg
as
r−1
(cid:88)
s(X,k)=1− pˆ(k |X)−U ·pˆ(k |X)−λ·[r−k ] ,
l r reg +
l=1
where[·] =max(0,·). ByfollowingthesimilarroutineinDingetal.(2024),inourexperiments,wepickλ=0.01and
+
k =5forCIFAR100whilek =1fortheremainingtwolessdifficultdatasets.
reg reg
B.ExtraStudiesonη
2
Inourstudy,weadoptthelearningratetuningapproachasdescribedbyGibbs&Candes(2021),selectingavaluethat
ensuresastablelearningtrajectorycharacterizedbyabalancebetweensmallerpredictionsetsizesandsatisfactorycoverage
convergence. However,thistuningstrategypresentschallengesinpracticalapplications. Specifically,differentdatasets
requiredistinctoptimallearningratevalues,andidentifyingthesevaluesthroughmanualtuningisbothtime-consumingand
lessadaptive. Toillustratethesechallenges,weconductedsensitivityanalysesontheimpactofvaryingη inAlgorithm1.
2
Thesestudiesunderscorethelimitationsofmanuallytuningasingleη value.
2
CIFAR10 CIFAR100 SVHN
1.000 1.000 1.000
0.975 0.975 0.975
0.950 0.950 0.950
0.925 0.925 0.925
0.900 0.900 0.900
0.875 0.875 0.875
2 2
0.850 0 0. .0 00 00 01 5 0.850 5 0e .0- 00 05 5 0.850 0.2 0001
0.003 0.001 0.0005
0.825 Acum_cvg 0.825 Acum_cvg 0.825 Acum_cvg
min min min
max max max
0.800 0.800 0.800
0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000
10 20.0 10
2 2 2
0.0001 5e-05 0.0001
0.0005 17.5 0.0005 0.0005
0.003 0.001
8 8
15.0
12.5
6 6
10.0
4 7.5 4
5.0
2 2
2.5
0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000
t t t
Figure7. PerformancesunderAlgorithm1withsoftmaxpolicyandsoftmaxscore.
12
etar
egarevoC
ezis
tes
noitciderPEfficientOnlineSet-valuedClassificationwithBanditFeedback
Ourfindings,presentedinFigures7to10,explorethesensitivityofthelearningrateη . Weobservedthatahigherη value
2 2
acceleratescoveragecontrol,asindicatedbythedarkerlinesinthetoppanelsofeachfigure. However,thisgenerallycomes
atthecostofenlargedpredictionsetsizes,evidentfromthedarkerlinesinthebottompanelsofthefigures. Moreover,the
predictionsetsizeshowsconsiderablesensitivitytovariationsinη . ThishighlightsthepracticallimitationsofAlgorithm1
2
andunderscoresthenecessityofimplementingAlgorithm2,whichutilizesanexpert-basedmethodtoaggregateresults
acrossmultiplelearningratesη ,j ∈ [J]. Thisapproachnotonlyaddressesthechallengesofmanualtuningbutalso
2,j
enhancesthealgorithm’sadaptabilityandeffectivenessacrossdiversedatasets.
CIFAR10 CIFAR100 SVHN
1.000 1.000 1.000
0.975 0.975 0.975
0.950 0.950 0.950
0.925 0.925 0.925
0.900 0.900 0.900
2 2
0.001 2 0.0005
0.875 0.01 0.875 0.0005 0.875 0.005
0.015 0.001 0.008
0.02 0.002 0.01
0.850 0.03 0.850 0.005 0.850 0.015
0.05 0.01 0.03
0.825 Acum_cvg 0.825 Acum_cvg 0.825 Acum_cvg
min min min
max max max
0.800 0.800 0.800
0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000
10 20.0 10
2 2 2
0.001 0.0005 0.0005
0.01 17.5 0.001 0.005
0.015 0.002 0.008
8 0.02 0.005 8 0.01
0.03 15.0 0.01 0.015
0.05 0.03
12.5
6 6
10.0
4 7.5 4
5.0
2 2
2.5
0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000
t t t
Figure8. PerformancesunderAlgorithm1withuniformpolicyandsoftmaxscore.
CIFAR10 CIFAR100 SVHN
1.000 1.000 1.000
0.975 0.975 0.975
0.950 0.950 0.950
0.925 0.925 0.925
0.900 0.900 0.900
2
1e-05
0.875 0.875 0.875 5e-05
2 0.0001
0.850 0 0. .0 00 00 01 3 0.850 0.2 0001 0.850 0 0. .0 00 00 05 8
0.0006 0.0005 0.001
0.825 Acum_cvg 0.825 Acum_cvg 0.825 Acum_cvg
min min min
max max max
0.800 0.800 0.800
0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000
10 20.0 10
2 2 2
0.0001 0.0001 1e-05
0.0003 17.5 0.0005 5e-05
0.0006 0.0001
8 8 0.0005
15.0 0.0008
0.001
12.5
6 6
10.0
4 7.5 4
5.0
2 2
2.5
0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000
t t t
Figure9. PerformancesunderAlgorithm1withsoftmaxpolicyandRAPSscore.
13
etar
egarevoC
etar
egarevoC
ezis
tes
noitciderP
ezis
tes
noitciderPEfficientOnlineSet-valuedClassificationwithBanditFeedback
CIFAR10 CIFAR100 SVHN
1.000 1.000 1.000
0.975 0.975 0.975
0.950 0.950 0.950
0.925 0.925 0.925
0.900 0.900 0.900
0.875 0.875 0.875
0.850 0.2 001 0.850 0.2 0001 0.850 0.2 005
0.01 0.0005 0.01
0.825 Acum_cvg 0.825 Acum_cvg 0.825 Acum_cvg
min min min
max max max
0.800 0.800 0.800
0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000
10 20.0 10
2 2 2
0.001 0.0001 0.005
0.01 17.5 0.0005 0.01
8 8
15.0
12.5
6 6
10.0
4 7.5 4
5.0
2 2
2.5
0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000
t t t
Figure10. PerformancesunderAlgorithm1withuniformpolicyandRAPSscore.
C.DiscussionofPolicyπ
t
In this section, for each class, we show the effectiveness of different policies on the correctness of arm pulling, i.e.,
P(A =Y |Y =k), k ∈Y. InFigure11,underthesoftmaxpolicy(toppanel)andtheuniformpolicy(bottompanel),we
t t t
CIFAR10 CIFAR100 SVHN
0.8 0.7 a f fi lq s oh wua et ri sc mammals 0.8 D D Di i ig g gi i it t t 0 1 2
0.7 0.6 f f ho r ouo uid t s a ec n ho d on l t dva ei en g le eer cts a trb icle as l devices 0.7 D D Di i ig g gi i it t t 3 4 5
household furniture Digit 6
0.6 i ln ars ge ec t cs arnivores D Di ig gi it t 7 8
0.5 large man-made outdoor things 0.6 Digit 9
large natural outdoor scenes
0.5 0.4 l m na or e ng d -e i iu n o m sm e- cn s tii zv ie no d vre ems rt aa emn bd rm a h ta ee l ssrbivores 0.5
people
0.4 0.3 r s t ve rm eep heat isi l cl le lems sa 1mmals 0.4
0.3 vehicles 2 0.3
A Ai ur tp ola mn oe bile 0.2
0.2 B Ci ar td 0.2
D De oger 0.1
0.1 F Hr oo rg se 0.1
Ship
Truck 0.0
0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000
t t t
0.104 0.0520 0.104
Airplane aquatic mammals Digit 0
Automobile fish Digit 1
Bird flowers Digit 2
0.103 Cat 0.0515 food containers 0.103 Digit 3
Deer fruit and vegetables Digit 4
Dog household electrical devices Digit 5
Frog household furniture Digit 6
0.102 Horse 0.0510 insects 0.102 Digit 7
Ship large carnivores Digit 8
Truck large man-made outdoor things Digit 9
large natural outdoor scenes
0.101 0.0505 large omnivores and herbivores 0.101
medium-sized mammals
non-insect invertebrates
people
0.100 0.0500 reptiles 0.100
small mammals
trees
vehicles 1
0.099 0.0495 vehicles 2 0.099
0.098 0.0490 0.098
0.097 0.0485 0.097
0.096 0.0480 0.096
0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000
t t t
Figure11. ProportionofcorrectlypulledarmwithRAPSscoreundersoftmax(top)anduniform(bottom)policy.
14
etar
egarevoC
ezis
tes
noitciderPEfficientOnlineSet-valuedClassificationwithBanditFeedback
reporttheaccumulativeperformanceofarmpullingforeachclass,i.e.,
(cid:80)t (cid:80) 1{A =k}
s=1 Xi∈Bs i , k ∈Y.
(cid:80)t (cid:80) 1{Y =k}
s=1 Xi∈Bs i
DuetotheusageofcontextX ineachbatchB ,s≤t,softmaxpolicyleadstohigheraccuracyforarmpulling. Incontrast,
i s
theuniformpolicy’scorrectnessiscloseto 1 . Thesebehaviorsalignwiththeoneincross-entropylossminimizationin
|Y|
Figure1,wherethesoftmaxpolicyquicklydecreasesthelosscomparedtotheuniformpolicy. Ontheotherhand,whenit
comestotheperformanceofset-valuedclassificationinFigures3and5,theuniformpolicybothconvergesfastertothe
desiredcoveragerateandgetsslightlysmallerpredictionsetsonaveragethanthesoftmaxpolicy.
Theaboveinterestingphenomenonmaymirrortheexploration-exploitationdilemmainreinforcementlearning. Specifically,
thesoftmaxpolicycapitalizesonmoreknowninformationcharacterizedbypˆ(k |X )asdefinedin(4)andhence“guesses”
t
labelswithhigherfrequentsuccess. Suchapolicycangreedilyandquicklydecreasethecross-entropylossbutsacrificesthe
performanceoftheset-valuedprediction. Incontrast,theuniformpolicyhasahighercapabilityofexploration,possibly
leadingtothefastempiricalconvergenceofcoveragerateandsmallerpredictionsets,eventhoughithasaninferiorcapability
toreducethecross-entropylossineachiteration.
15EfficientOnlineSet-valuedClassificationwithBanditFeedback
D.Proofs
ProofofTheorem3.2. DefineM :=[∆ −1{Y =k}]·[α−1{st−1(X ,k)<τt−1}]and
t,k t,k t t k
(cid:20) (cid:20)1{A =k}·1{A =Y } (cid:21)(cid:21)
V[M |F ]:=E E t t t [α−1{st−1(X ,k)<τt−1}]2 |F ,(X ,Y )
t,k t−1 (Xt,Yt) π2(k |X ) t k t−1 t t
t t
(cid:20)1{Y
=k}
(cid:21)
=E t [α−1{st−1(X ,k)<τt−1}]2 |F
(Xt,Yt) π (k |X ) t k t−1
t t
(cid:20)1{Y
=k}
(cid:21) (cid:20)1{Y
=k}
(cid:21)
≤E t |F =E t |F .
(Xt,Yt) π (k |X ) t−1 π (k |X ) t−1
t t t t
Additionally,wehave
1
|M |≤ and E[M |F ]=E [E[M |F ,(X ,Y )]]=0. (7)
t,k c t,k t−1 (Xt,Yt) t,k t−1 t t
k
Then,byutilizingtheChernoffbound,foranyξ >0,wehave
(cid:20) T (cid:21) (cid:20) T (cid:21)
(cid:88) (cid:88)
P M ≥ε ≤exp(−ξε)·E exp(ξ M )
t,k t,k
t=1 t=1
(cid:20) (cid:20) T−1 (cid:21)(cid:21)
(cid:88)
=exp(−ξε)·E E exp(ξ M +ξM )|F
t,k T,k T−1
t=1
(cid:20) T−1 (cid:21)
(cid:88)
=exp(−ξε)·E exp(ξ M )·E[exp(ξM )|F ]
t,k T,k T−1
t=1
(cid:20) T−1 (cid:21)
≤exp(−ξε)·E exp(ξ(cid:88) M )·exp(cid:0)V[M |F ]c2(exp(ξ/c )−c2 −c ξ)(cid:1) (8)
t,k T,k T−1 k k k k
t=1
(cid:20) T−1 (cid:21)
≤exp(−ξε)·exp(cid:0) bT ·c2(exp(ξ/c )−c2 −c ξ)(cid:1) ·E exp(ξ(cid:88) M )
k k k k k t,k
t=1
(cid:32) T (cid:33)
(cid:88)
≤exp c2(exp(ξ/c )−c2 −c ξ) bt −ξε
k k k k k
t=1
(cid:32) T (cid:34) (cid:35)(cid:33)
=exp −c2(cid:88) bt · − ε/c k +( ε/c k +1)·log( ε/c k +1) , (9)
k k (cid:80)T bt (cid:80)T bt (cid:80)T bt
t=1 t=1 k t=1 k t=1 k
where(8)holdsdueto
E[exp(ξM )|F ]=1+E[ξM |F
]+E(cid:34) (cid:88)∞ ξnM tn
,k |F
(cid:35)
t,k t−1 t,k t−1 n! t−1
n=2
≤1+E(cid:34)
M2
(cid:88)∞ ξn|M t,k|n−2
|F
(cid:35)
t,k n! t−1
n=2
(cid:88)∞ ξn
≤1+V[M |F ]
t,k t−1 cn−2n!
n=2 k
=1+V[M |F ]c2(exp(ξ/c )−c2 −c ξ)
t,k t−1 k k k k
≤exp(cid:0)V[M |F ]c2(exp(ξ/c )−c2 −c ξ)(cid:1) ,
t,k t−1 k k k k
and(9)holdssincewesetξ =c log( ε/ck +1).
k (cid:80)T bt
t=1 k
16EfficientOnlineSet-valuedClassificationwithBanditFeedback
(cid:20) (cid:21)
By applying the fact of (1 + u)log(1 + u) − u ≥ u2 , u ≥ 0 on (9), we have P (cid:80)T M ≥ ε ≤
2+2u/3 t=1 t,k
exp(− ε2 ),andhence
2(cid:80)T t=1bt k+2ε/(3ck)
(cid:12) (cid:12)
(cid:20)(cid:12)(cid:88)T (cid:12) (cid:21) ε2
P (cid:12) M (cid:12)≥ε ≤2exp(− ).
(cid:12) (cid:12)
t=1
t,k(cid:12) (cid:12) 2(cid:80)T t=1bt k+2ε/(3c k)
Thus,withtheprobabilityatleast1−δ,wehave
(cid:12) T (cid:12)
(cid:12) (cid:12) (cid:12)(cid:88) ∆ t,k·[α−1{st−1(X t,k)<τ kt−1}]−1{Y t =k}·[α−1{st−1(X t,k)<τ kt−1}](cid:12) (cid:12)
(cid:12)
t=1
(cid:12) (cid:12)
(cid:12)(cid:88)T (cid:12)
=(cid:12) M (cid:12)
(cid:12) t,k(cid:12)
(cid:12) (cid:12)
t=1
(cid:118)
(cid:117) T
1 2 (cid:117) 1 2 (cid:88) 2
≤ log +(cid:116)( log )2+2 bt log
3c δ 3c δ k δ
k k
t=1
(cid:118)
(cid:117) T
2 2 (cid:117) 2(cid:88)
≤ log +(cid:116)2log bt :=ζ (T,δ) (10)
3c δ δ k k
k
t=1
DerivingfromtheupdatingruleforthequantileestimationinAlgorithm1,wehave
T
τT =τ0+η (cid:88) ∆ ·(cid:2) α−1{st−1(X ,k)<τt−1}(cid:3)
k k 2 t,k t k
t=1
(cid:88)T τT
=⇒ ∆ ·[α−1{st−1(X ,k)<τt−1}]= k . (11)
t,k t k η
2
t=1
Therefore,combing(10)with(11),withprobabilityatleast1−δ,wehave
τT (cid:88)T τT
k −ζ (T,δ)≤ 1{Y =k}·[α−1{st−1(X ,k)<τt−1}]≤ k +ζ (T,δ)
η k t t k η k
2 2
t=1
⇒
ητ
k
TT
−
ζ k( TT,δ) ≤α−(cid:88)T 1{Y
Tt
=k}
·1{Y
t
̸∈C(cid:98)t−1(X t)}≤
ητ
k
TT
+
ζ k( TT,δ)
2 k k k 2 k k
t=1
T
ProofofTheorem3.4. Recallthedefinitionτ∗ =min 1 (cid:80)1{Y =k}ρ (st−1(X ),τ). Thus,
k τ T t α t
t=1
T T
(cid:88) (cid:88)
T ·Reg (T)= ∆ ρ (st−1(X ),τt−1)− 1{Y =k}ρ (st−1(X ),τ∗)
k,ρα t,k α t k t α t k
t=1 t=1
T T
(cid:88) (cid:88)
= ∆ ρ (st−1(X ),τt−1)− ∆ ρ (st−1(X ),τ∗)
t,k α t k t,k α t k
t=1 t=1
(cid:124) (cid:123)(cid:122) (cid:125)
Diff1
T T
(cid:88) (cid:88)
+ ∆ ρ (st−1(X ),τ∗)− 1{Y =k}ρ (st−1(X ),τ∗),
t,k α t k t α t k
t=1 t=1
(cid:124) (cid:123)(cid:122) (cid:125)
Diff2
17EfficientOnlineSet-valuedClassificationwithBanditFeedback
whereE[Diff ]=0since∆ isanunbiasedestimatorof1{Y =k}conditionalonF ∪(X ,Y ). Additionally,we
2 t,k t t−1 t t
have
T
(cid:88)
Diff ≤ ∆ ·g ·(τt−1−τ∗), here(sub)gradientg :=−∆ [α−1{st−1(X )<τt−1}]
1 t,k t−1,k k k t−1,k t,k t k
t=1
T
=(cid:88)∆
t,k ·(τt−1−τt)·(τt−1−τ∗)
η k k k k
2
t=1
T
=(cid:88)∆
t,k ·[(τt−1−τt)2+(τt−1−τ∗)2−(τt−τ∗)]
2η k k k k k k
2
t=1
=(cid:88)T ∆3 t,kη 2 ·[α−1{st−1(X )<τt−1}]2+(cid:88)T ∆ t,k ·[(τt−1−τ∗)2−(τt−τ∗)2],
2 t k 2η k k k k
2
t=1 t=1
whichfurtherimplies
E[Diff
]≤(cid:88)T
η
2E(cid:20)1{Y
t
=k}(cid:21) +(cid:88)T
p
k ·E[(τt−1−τ∗)2−(τt−τ∗)2]
1 2 π2(k |X ) 2η k k k k
t=1 t t t=1 2
=
η
2
(cid:88)T E(cid:20)1{Y
t
=k}(cid:21)
+
p
k E[(τ0−τ∗)2−(τT −τ∗)2]
2 π2(k |X ) 2η k k k k
t=1 t t 2
≤
η
2
(cid:88)T E(cid:20)1{Y
t
=k}(cid:21)
+
p k(τ k∗)2
2 π2(k |X ) 2η
t=1 t t 2
(cid:118)
=τ∗(cid:117)
(cid:117) (cid:116)p
(cid:88)T E(cid:20)1{Y
t
=k}(cid:21)
bychoosingη
=τ∗(cid:118)
(cid:117) (cid:117) p k
k k π2(k |X ) 2 k(cid:117) T
t=1 t t (cid:116)(cid:80)E(cid:2)1{Yt=k}(cid:3)
t=1
π t2(k|Xt)
To prove Theorem 3.6, we follow a similar argument in Cesa-Bianchi & Lugosi (2006) with two introduced lemmas.
Additionally,ourproofreliesontheassumptionthatthechecklossfunctionρ isbounded. Itholdsoncethescorefunction
α
isbounded,e.g.,thesoftmax,APS,andRAPSscoresutilizedinourstudy. Therefore,withoutlossofgenerality,weassume
|ρ (·,·)|≤1.
α
LemmaD.1. LetX bearandomvariablewitha≤X ≤b. Thenforanys∈R,
s2(b−a)2
lnE[exp(sX)]≤sE[X]+ .
8
(cid:80)
LemmaD.2. ForallJ ≥2,forallβ ≥β ≥0,andforalld ≥0,j ∈[J]suchthat exp(−β d )≥1,
2 1 j j∈[J] 1 j
(cid:80)
ln
j∈[J]exp(−β 1d j)
≤
β 2−β
1 lnJ.
(cid:80)
exp(−β d ) β
j∈[J] 2 j 1
ProoftoTheorem3.6. Forthenotationsimplicity, letLt = (cid:80)t ∆ ρ (st′−1(X ,k),τt′−1)betheaccumulative
j,k t′=1 t′,k α t′
weightedcheckloss(uptotimet)withj-thexpertforclassk,andjt ∈argmin Lt denoteanexpertwiththesmallest
k j∈[J] j,k
accumulativelossuptotimetforclassk. Afterdefiningtheweights
ωt =exp(−√ 1 Lt ), ω′t =exp(−√1 Lt ), and ω¯t =ωt / (cid:88) ωt ,
j,k t+1 j,k j,k t j,k j,k j,k i,k
i∈[J]
18EfficientOnlineSet-valuedClassificationwithBanditFeedback
wehavethebelowequation
√ tlnω¯t−1 −√ t+1lnω¯t =(√ t+1−√ t)ln 1 +√ tlnω¯ j′t kt,k +√ tlnω¯ jt k− t−1 1,k , (12)
j kt−1,k j kt,k ω¯ jt
t,k
ω¯ jt
t,k
ω¯ j′t
t,k
k k k
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
1 2 3
where
√ √
≤( t+1− t)lnJ (13)
1
sincejt ∈argmin Lt andhenceω¯t ≥ 1,
k j∈[J] j,k jt,k J
k
√ (cid:80) exp[−√1 (Lt −Lt )] √ √1 − √1
= tln j∈[J] t+1 j,k j kt,k ≤ t t t+1 lnJ (duetoLemmaD.2)
2 (cid:80) exp[−√1 (Lt −Lt )] √1
j∈[J] t j,k jt,k t+1
k √ √
=( t+1− t)lnJ, (14)
and
√ ωt−1 √ (cid:80) ω′t
jt−1,k j∈[J] j,k
= tln k + tln
3 ω′t (cid:80) ωt−1
jt,k j∈[J] j,k
k
√ exp(−√1 Lt−1 ) √ (cid:80) ω′t √ (cid:80) ω′t
= tln t j kt−1,k + tln j∈[J] j,k =Lt −Lt−1 + tln j∈[J] j,k . (15)
exp(−√1 tLt jt,k) (cid:80) j∈[J]ω jt ,− k1 j kt,k j kt−1,k (cid:80) j∈[J]ω jt ,− k1
k
(cid:124) (cid:123)(cid:122) (cid:125)
4
Additionally,
=√ tln(cid:80) j∈[J]exp[−√1 t(Lt j− ,k1+∆ t,kρ α(st−1(X t,k),τ jt ,− k1))]
4 (cid:80) exp(−√1 Lt−1)
j∈[J] t j,k
=√ tln(cid:80) j∈[J]ω jt ,− k1·exp(−√1 t∆ t,kρ α(st−1(X t,k),τ jt ,− k1))
(cid:80) ωt−1
j∈[J] j,k
√ (cid:88) 1
= tln ω¯t−1·exp(−√ ∆ ρ (st−1(X ,k),τt−1))
j,k t t,k α t j,k
j∈[J]
√ (cid:20) 1 (cid:88) 1 (cid:21)
≤ t −√ ω¯t−1∆ ρ (st−1(X ,k),τt−1))+ (duetoLemmaD.1)
t j,k t,k α t j,k 8c2t
j∈[J] k
(cid:88) 1
≤−∆ ρ (st−1(X ,k), ω¯t−1τt−1))+ √
t,k α t j,k j,k 8c2 t
j∈[J] k
1
=−∆ ρ (st−1(X ,k),τ¯t−1))+ √ . (16)
t,k α t k 8c2 t
k
Thus,bycombing(12)to(16),wehave
∆ ρ (st−1(X ,k),τ¯t−1)))−(Lt −Lt−1 )
t,k α t k j kt,k j kt−1,k
√ √ 1 √ √
≤ t+1lnω¯t − tlnω¯t−1 + √ +2( t+1− t)lnJ (17)
j kt,k j kt−1,k 8c2
k
t
19EfficientOnlineSet-valuedClassificationwithBanditFeedback
Bytakingthesumovert∈[T]forbothsidesof(17),wehave
(cid:88)T 1 (cid:88)T 1 √
∆ ρ (st−1(X ,k),τ¯t−1))−LT ≤lnJ + √ +2( T +1−1)lnJ
t=1
t,k α t k j kT,k 8c2
k t=1
t
(cid:88)T (cid:88)T 1 √ √
=⇒ ∆ ρ (st−1(X ,k),τ¯t−1))−min ∆ ·ρ (s(X ,k),τt−1)≤ T +2 T lnJ
t=1
t,k α t k
j∈[J] t=1
t,k α t j,k 4c2
k
20