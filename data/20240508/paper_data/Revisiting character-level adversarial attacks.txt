Revisiting character-level adversarial attacks
EliasAbadRocamora1 YongtaoWu1 FanghuiLiu2 GrigoriosG.Chrysos3 VolkanCevher1
Abstract
S == T Thheeyy d doonn’t’ tc caarere a abboouut tu us
s

y = =n engeagtaitvieve
AdversarialattacksinNaturalLanguageProcess-
LM Alphabet
ingapplyperturbationsinthecharacterortoken
levels. Token-levelattacks,gainingprominence
fortheiruseofgradient-basedmethods,aresus- Algorithm 2
ceptible to altering sentence semantics, leading Algorithm 1 _T_h_e_y_a
 _d_o_n_’_t_ _ca
 a_r_e_ _a_a
 _o_u_t_ _u_s_
toinvalidadversarialexamples. Whilecharacter- get_top_positions( , ,3) b
 b
 b
 c

 c

 c


levelattackseasilymaintainsemantics,theyhave
9
 9
 9

receivedlessattentionastheycannoteasilyadopt .
 .
 .

!
 !
 !

populargradient-basedmethods,andarethought ? ? ? Sentences
True
tobeeasytodefend. Challengingthesebeliefs,
weintroduceCharmer,anefficientquery-based = They don’t cJare about us
 LM = positive
adversarial attack capable of achieving high at-
Sentence with
tacksuccessrate(ASR)whilegeneratinghighly False highest loss
similar adversarial examples. Our method suc-
cessfully targets both small (BERT) and large = = T poh se iy t id vo en’t cJare about us
 Success
(Llama 2) models. Specifically, on BERT with
SST-2, Charmer improves the ASR in 4.84% Figure1.Schematic of the proposed method, Charmer: Ex-
pointsandtheUSEsimilarityin8%pointswith ampleofourattackinthesentimentclassificationtaskwiththe
positionssubsetsizen=3.Ateachiteration,ourattackcomputes
respecttothepreviousart. Ourimplementationis
themostimportantpositionsinthesentenceviaAlgorithm1.Then,
availableingithub.com/LIONS-EPFL/Charmer.
wegenerateallpossiblesentencesreplacingacharacterinthetop
positions,togettheonewiththehighestloss. Ifthissentenceis
misclassified,theprocessisfinished.
1.Introduction
Language Models (LMs) have rapidly become the go-to TheapplicationofadversarialattacksinLMsisnotstraight-
tools for Natural Language Processing (NLP) tasks like forward due to algorithmic (Guo et al., 2021) and imper-
languagetranslation(Sutskeveretal.,2014),codedevelop- ceptibility constraints (Morris et al., 2020a). Unlike the
ment(Chenetal.,2021)andevengeneralcounselingvia computer vision tasks, where inputs consist of tensors of
chatinterfaces(OpenAI,2023). However,severalfailures real numbers, in NLP tasks, we work with sequences of
concerningrobustnesstonaturalandadversarialnoisehave discretenon-numericalinputs. Thisresultsinadversarialat-
been discovered (Belinkov & Bisk, 2018; Alzantot et al., tacksbeinganNP-hardproblemevenforconvexclassifiers
2018). Adversarial attacks have been widely adopted in (Leietal.,2019). Thisfactalsohardenstheuseofpopular
thecomputervisioncommunitytodiscovertheworst-case gradient-basedmethodsforobtainingadversarialexamples
performanceofMachineLearningmodels(Szegedyetal., (Guoetal.,2021). Totacklethisproblem,attackersadopt
2014;Goodfellowetal.,2015)orbeusedtodefendagainst gradientbasedstrategiesintheembeddingspace,restricting
suchfailurecases(Madryetal.,2018;Zhangetal.,2019). theattacktothetokenvocabulary(Ebrahimietal.,2018;
Liuetal.,2022;Houetal.,2023)ortheblack-boxsetting,
1LIONS,ÉcolePolytechniqueFédéraledeLausanne,Switzer- where only input-output access to the model is assumed
land2DepartmentofComputerScience,UniversityofWarwick,
(Alzantotetal.,2018;Gaoetal.,2018;Jinetal.,2020;Li
UnitedKingdom3DepartmentofElectricalandComputerEngi-
et al., 2020; Garg & Ramakrishnan, 2020; Wallace et al.,
neering,UniversityofWisconsin-Madison,USA.Correspondence
to:YongtaoWu<yongtao.wu@epfl.ch>. 2020).
Another difficult analogy to make with the computer vi-
Proceedings of the 41st International Conference on Machine
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by sionworldisimperceptibility. Adversarialexamplesshould
theauthor(s). bebydefinitionimperceptible,inthesensethattheattack
1
4202
yaM
7
]GL.sc[
1v64340.5042:viXra
... ... ...Charmer
shouldnotmodifythehumanpredictionorallowtothink
Table1.Attackdesiderata&comparisonwithstate-of-the-art.
thatanattackhasbeendone(Szegedyetal.,2014). Given
ASRstandsforAttackSuccessRate.
aninputx∈Rd,inthenumerical-inputsetting,impercep-
Token Char
tibilityiscontrolledbylookingforanadversarialexample
xˆ ∈Rdinanℓ ballcenteredatxwithperturbationradius Attacklevel Charmer
p
Previous (Thiswork)
ϵ,i.e.,||x−xˆ|| ≤ϵ,whereϵcanbesetarbitrarilysmall.
p
ASR(%) 95.16 0.96∗ 100.00∗
InNLP,Morrisetal.(2020a)suggestdifferentstrategiesfor
controllingimperceptibilityaccordingtotheattacklevel: Highefficiency ✓ ✓ ✓
Character: ConstraintheattacktohavealowLevenshtein Semanticspreserving ✗∗∗ ✓ ✓
(edit)distance. However,character-levelattackshavelost
∗Defendedwith(Jonesetal.,2020).
relevance due to the strength of robust word recognition ∗∗Accordingto(Houetal.,2023;Dyrmishietal.,2023).
defenses(Pruthietal.,2019;Jonesetal.,2020).
Token: Constrain the embedding similarity1 of replaced n. Weuseboldlowercaselettersforvectorsx∈Rd,with
wordsandoftheoverallsentencetobehigh. Nevertheless, theithpositionbeingx i ∈R.
Dyrmishietal.(2023)concludethatstate-of-the-artattacks
do not produce imperceptible attacks in practice. To be 2.Relatedwork
specific, Hou et al. (2023) report 56.5% of their attacks
changethesemanticsofthesentence. WeprovideanoverviewofadversarialattacksinNLP.Ad-
versarialattackshavebeendevisedforproducingmissclas-
TheoverallattackdesiderataissummarizedinTable1. Ex-
sifications(Alzantotetal.,2018),generatingunfaithfulma-
istingdefensesagainstcharacter-levelattacksrelyonrobust
chine translations (Cheng et al., 2020; Sadrizadeh et al.,
wordrecognitionmodules,whichassumetheattackeradopts
2023a;b;Wallaceetal.,2020),providingmaliciousoutputs
unrealisticconstraints,notallowingsimplemodifications
(jailbreaking) (Zou et al., 2023; Zhu et al., 2023; Carlini
such as insertion or deletion of blank spaces, which are
et al., 2023) or even extracting training data (Nasr et al.,
adoptedinpractice(Lietal.,2019).
2023). Wedistinguishthesemethodsintwomainbranches:
Inthiswork,werevisitcharacter-leveladversarialattacks token-levelandcharacter-levelattacks.
as a practical solution to imperceptibility. Our attack,
Tokenbased: Earlytoken-basedattacksrelyinblack-box
Charmer,isbasedonagreedyapproachcombinedwith
tokenreplacement/insertionstrategiesbasedonheuristics
a position subset selection to further speed-up the attack,
for estimating the importance of each position, and the
whileminimallyaffectingperformance. Ourattackisable
candidate tokens for the operation (Ren et al., 2019; Jin
toachieve>95%ASRineverystudiedTextAttackbench-
etal.,2020;Lietal.,2020;Garg&Ramakrishnan,2020;
mark and LLMs Llama-2 and Vicuna, obtaining up to a
Leeetal.,2022). Ebrahimietal.(2018)andLietal.(2019)
23%-pointASRimprovementwithrespecttotherunner-up
consider the token gradient information to select which
method.Weshowthatexistingadversarialtrainingbasedde-
tokentoreplace. Guoetal.(2021)proposeGBDA,thefirst,
fenses(Houetal.,2023)degradecharacter-levelrobustness,
but inefficient, full gradient based text adversarial attack.
i.e., increasing the ASR in 3.32% points when compared
TextGrad (Hou et al., 2023) is a more efficient variante
tostandardtraining. Ourfindingsindicatetypo-corrector
proposedtobeintegratedwithinAdversarialTraining.
defenses (Pruthi et al., 2019; Jones et al., 2020) are only
successfulwhenasetofstrictattackconstraintsisassumed, Characterbased: Belinkov&Bisk(2018)showcasethat
ifjustoneoftheseconstraintsisrelaxed,ASRcanincrease character-levelMachineTranslationmodelsaresensibleto
from0.96%to98.09%. Overall,webelievecharacter-level naturalcharacterlevelperturbations(typos)andadversari-
robustnessamoreconsistentmeasurethantoken-levelro- allychosenones. (Pruthietal.,2019)proposetoiteratively
bustness. changethebestpossiblecharacteruntilsuccess. However,
thisstrategycanbeinefficientforlengthysentences. Ad-
Notation: Sentencesaresequencesofcharactersintheset
dressingthisissue,othermethodsproposepre-filteringthe
Γ. SentencesaredenotedwithuppercaselettersS. Forsets
most important words/tokens in the sentence, to then in-
of sentences we use caligraphic uppercase letters S. We
troducearandomtypo(Gaoetal.,2018),orthebesttypo
denote the concatenation operator of two sentences as ⊕.
among a random sample (Li et al., 2019). In (Liu et al.,
Theemptycharacter∅isdefinedsothatsentencesremain
2022),atoken-basedattackwithcharacter-levelLevenshtein
invarianttoconcatenationswithit,i.e.,S⊕∅=S. Weuse
distance constraints is considered. However, considering
theshorthand[n]for{1,2,...,n}foranypositiveinteger
token-levelinformationforassesingcharacter-levelimpor-
1Similarityiscommonlymeasuredviathecosinesimilarityof tance can be suboptimal. Ebrahimi et al. (2018) propose
theUSEembeddings(Ceretal.,2018). involving embedding gradient information for evaluating
theimportanceofcharacters,makingthestrategyonlyvalid
2Charmer
forcharacter-levelmodels. (Yangetal.,2020)evaluatethe A learning model (e.g., neural networks) f : S(Γ) → Y,
relevanceofeachcharacterbymaskingandevaluatingthe whereY isthelabelspace,iscalledk-robustatSiff(S)=
lossineachposition,tothenmodifythetoppositions. This f(S′), ∀S′ ∈ S (S,Γ). Iff(S) ̸= f(S′)forsomeS′ ∈
k
strategy does not consider character insertions, and does S (S,Γ),wesayS′isanadversarialexample.
k
nottakeintoacounttheeffectofindivudualchangesinthe
importance of positions. Similarly to(Yang et al., 2020), Without loss of generality, we focus on the classification
ourmethodmeasurestheimportanceofeverypositionplus task. Intheadversarialrobustnesscommunity,adversarial
insertions. After a perturbation is done, importances are examplesareusuallysoughtbysolvingsomeoptimization
updatedtoconsidertheinteractionbetweenperturbations. problem (Carlini & Wagner, 2017). Given a data sample
(S,y)∈S(Γ)×[o]andaclassificationmodelf :S(Γ)→
Ro,withoclasses,wesolve:
3.Problemsetting
max L(f(S′),y) , (1)
Inthissection,wesummarizethesetting,Levenshteindis-
S′∈Sk(S,Γ)
tanceandtheoperatorsusedinourattack.
whereLissomelossfunction,e.g.,thecrossentropyloss.
Inthefollowing,weelaborateonhowEq.(1)issolved.
3.1.Thesentencespace
Let Γ be the alphabet set. A sentence S with l char- 3.3.Characterizingtheperturbations
acters (i.e., the length |S| = l) in Γ is defined with
ToexploreadversarialexamplesinS (S,Γ),wemakeuse
S = c c ···c ∈ Γl. For notational simplicity, we k
1 2 l ofthecontraction,expansion(Definition3.3)andreplace-
denote S = c as the character in the ith position and
i i mentoperators(Definition3.5)tocharacterizethisset.
S = c c ···c (S = c c ···c ) as the sequence ob-
i: i i+1 l :i 1 2 i
tainedbytakingthecharactersafter(before)theithposition Definition3.3(Expansionandcontractionoperators). Let
included. We denote S(Γ) as the set of (all possible) se- S(Γ) be the space of sentences with alphabet Γ and the
quenceswithcharactersinΓwithlengthlessthanL. Let specialcharacterξ ∈/ Γ,thepairofexpansion-contraction
d : S(Γ) × S(Γ) → R+ be the Levenshtein distance functionsϕ : S(Γ) → S(Γ∪{ξ})andψ : S(Γ∪{ξ}) →
lev
(Levenshteinetal.,1966),alsoknownastheeditdistance. S(Γ)isdefinedas:
To be specific, for any two sentences S,S′ ∈ S(Γ), the
(cid:26)
ξ if|S|=0
Levenshteindistanceisdefinedas: ϕ(S):=
ξ,S ⊕ϕ(S ) otherwise.
 1 2:
|S| if|S′|=0 

d
(S|S′|
,S′ )
ii ff S|S| == S0
′
ψ(S):=
ψ(S∅
2:)
i if f| SS 1| == ξ0
d lev(S,S′):= le v d2: (S2:
,S′
) 1 1 S 1⊕ψ(S 2:) otherwise.
1+min
d
dl le ev
v( (S
S2
2
,:
:
S,S
′2
′
): )

otherwise.
Clearly, ϕ(S) aims to insert ξ into S in all possible posi-
lev 2:
tionsbetweencharactersandatthebeginningandendof
Example3.1(d levformS =Hellotoseveralmodifications.). thesentence,andthuswehave|ϕ(S)|=2·|S|+1. Simi-
larly,ψ(S)aimstoremoveallξoccurredinS. The(ϕ,ψ)
d (Hello,Helo)=1 d (Hello,Hallo)=1
lev lev pairsatisfiesthepropertythatψ(ϕ(S)) = S. Wegivethe
d (Hello,Helloo)=1 d (Hello,Haloo)=2.
lev lev followingexampleforabetterunderstanding.
Example3.4. Letξ :=⊥forvisibility:
Notethatd representsthecostinnumberofcharacterin-
lev
sertions,deletionsandreplacementsneededforStobecome
ϕ(Hello)=⊥H⊥e⊥l⊥l⊥o⊥ ψ(⊥H⊥eel⊥l⊥o⊥)=Heello
equaltoS′orvice-versa.
ψ(⊥H⊥e⊥l⊥⊥⊥o⊥)=Helo ψ(⊥H⊥el⊥lo⊥)=Hello.
3.2.Adversarialrobustness Definition3.5(Replacementoperator). LetS ∈S(Γ∪{ξ}),
the integer i ∈ [|S|] and the character c, the replacement
Inthiswork,wetacklerobustnessrestrictedbytheLeven-
operator←i coftheithpositionofS withcisdefinedas:
shteindistance. Thisenablesthesearchofhighlysimilar,
hardtodetectandsemantics-preservingadversarialexam- S ←i c:=S ⊕c⊕S
:i−1 i+1:
ples(Morrisetal.,2020a).
Thanks to Definitions 3.3 and 3.5, it is easy to check the
Definition3.2(k-robustnessatS). Denotethesetofsen-
followingproposition.
tencesatdistanceuptokas
Proposition3.6(Characterizationofd -1operations). Let
lev
S (S,Γ)={S′ ∈S(Γ):d (S,S′)≤k} . S ∈ S(Γ) be a non-empty sentence, and S′ be another
k lev
3Charmer
sentencesatisfyingd (S,S′) = 1. Thenwecanfindi ∈ Algorithm1HeuristicforTop-npositionselection.
lev
[2|S|+1]andacharacterc∈Γ∪{ξ}suchthat
1: Inputs: modelf,sentenceS,testcharactert,special
(cid:16) (cid:17)
S′ =ψ ϕ(S)←i c . characterξ,numberofpositionsn,lossLandlabely.
2: l=0 ▷Initializevectortostorelosseswithzeros
3: fori=1,...,2|S|+1do
Remark3.7(Non-uniqueness). Theparametrizationofthe
transformationfromStoS′mightnotbeunique. Forexam- 4: P =ϕ(S) ▷Expandsentence
ple,forS =HelloandS′ =Helo,bothpairs(i=6,c=ξ) 5: ifP i =tthenP i ←ξ
and(i=8,c=ξ)arevalidparametrizations.
6: elseP i ←t
Remark 3.8 (Intuition). Replacing a character in Γ for ξ,
andapplyingψresultsinadeletion. Similarly,replacinga 7: l i =L(f(ψ(P)),y) ▷Contractandeval. loss
ξbyacharacterinΓandapplyingξresultsinaninsertion. 8: returnTop-n(l) ▷Indexesofthetopnvaluesinl
Corollary3.9(GeneratingS ). LetS beanon-emptysen-
k
tenceinthevolcabulary,with|Γ| > 1,foranyk ≥ 1,the
set S k(S,Γ) (see Definition 3.2) can be obtained by the by Corollary 3.9, which can be relatively big for lengthy
followingrecursion: sentences(e.g.,|S|canbeupto844forAG-News). Wepro-
S
k(S,Γ)=  (cid:26) ψ (cid:16)(cid:16) ϕ (cid:16)(S (cid:17))←i c (cid:17)(cid:17) ∀ ∀∀i ic∈ ∈∈[ [2
Γ
2| |S
S∪
ˆ| |{+ +ξ}1 1] ](cid:27)


,ifk=1, p
t th
oo
e
pse
d
nec
p
lo oen cns adid
te
ie onr ncin
y
s,g
o
wa
n
es thu peb rs ole
e
pt
n
oo
g
sf
t
ehn
to
el
f
so ttc
ih
na et gi so ten hns etei rn
n ec
lo eer v.d aTe nr
o
ct eso
e
or lee fcm
et
ao
t
chv hee
ψ ϕ Sˆ ←i c ∀Sˆ∀c ∈∈ SΓ∪ ({ Sξ ,}
Γ)
,ifk>1.
positionbyreplacingeachcharacterwitha“test"character
k−1 andlookingatthechangeintheloss. InAlgorithm1we
Thesizeofthesesetsisboundedas: formalizeourproposedstrategy. Notethatifthetestcharac-
terisgoingtobereplacedbyitselfinacertainposition,we
|Γ|k+1−1
≤|S
(S,Γ)|≤(|Γ|+1)k·(cid:89)k
(2(|S|+k)−1). replacebythespecialcharacterξ(Line5inAlgorithm1).
|Γ|−1 k In practice we use the white space (U+0020) as the test
j=1
character. Overall,Algorithm1performsO(|S|)forward
Remark3.10. Inthecase|Γ|=1,foranyS ∈S(Γ):|S|≥
passestroughthelanguagemodel.
k,wetriviallyhavethat|S (S,Γ)|=2k+1.
k
Proof. RefertoAppendixC. 4.2.Attackclassifier
Note that exactly computing |S (S,Γ)| is non-trivial and In the case of using a classifier f : S(Γ) → Ro, where
k
complexdynamicprogrammingalgorithmshavebeenpro- the predicted class is given by yˆ = argmax y∈[o]f(S) y
posedforthistask(Mihov&Schulz,2004;Mitankin,2005; with o classes, we follow Hou et al. (2023) and use the
Touzet,2016). Theexponentialdependenceof|S (S,Γ)|
Carlini-WagnerLoss2(Carlini&Wagner,2017):
k
onkmakesitunfeasibletoevaluateeverysentenceinthe
L(f(S),y)=maxf(S) −f(S) . (2)
yˆ y
set,therefore,smarterstrategiesareneeded. yˆ̸=y
Inthiscase,asentenceS′isanadversarialexamplewhen
4.Method L(f(S′),y) ≥ 0. To search the closest sentence in Lev-
enshteindistancethatproducesamisclassification, weit-
Letusnowintroduceourmethod(Charmer). InSecs.4.2 eratively solve Eq. (BP) with k = 1 until the adversarial
and4.3wepresentourattackforbothstandardclassifiers sentence S′ is misclassified. Our attack pseudo-code is
andLLM-basedclassifiers. presentedinAlgorithm2.
To circumvent the exponential dependence of |S (S,Σ)|
k
on k as indicated by Corollary 3.9, we propose to greed- 4.3.AttackLLM
ilyselectthesingle-characterperturbationwiththehighest
Now we illustrate how to apply our method on attacking
loss. Furthermore,wereducethesearchspaceforsingle-
LLM-basedclassifiers. Givenadatasample(S,y)∈S×
characterperturbationsbyconsideringasubsetoflocations
[C],theinputtoLLMsisformulatedbyconcatenatingthe
wherecharacterscanbereplaced.
originalsentenceswithsomeinstructivepromptsS ,S
P1 P2
intheformatofS ⊕S⊕S . Aschematicforillustration
4.1.Pre-selectionofreplacementlocations
P1 P2
2Intheoriginalpaper,Carlini&Wagner(2017)clipthevalue
InProposition3.6,weconsiderallpossiblelocations(i∈ ofthelosstobe0atmaximum.Wedonotclipinordertodealwith
[2·|S|+1]),leadingto|S 1(S,Γ)|≤(|Γ|+1)·(2·|S|+1) caseswherethelossispositivefordifferentadversarialexamples.
4Charmer
Table2.AttackevaluationintheTextAttackBERTandRoBERTamodels:Token-levelandcharacter-levelattacksarehighlighted
with●and●respectively.Foreachmetric,thebestmethodishighlightedinboldandtherunner-upisunderlined.Charmerconsistently
achieveshighestAttackSuccessRate(ASR)Additionally,thesimilaritybetweentheoriginalandattackedsentencesisthehighestor
runner-upin8/10cases.
BERT RoBERTa
Method ASR(%)↑ d (S,S′) ↓ Sim(S,S′) ↑ Time(s) ↓ ASR(%)↑ d (S,S′) ↓ Sim(S,S′) ↑ Time(s) ↓
lev lev
GBDA● 42.09 17.76 0.93 13.86 - - - -
±(9.33) ±(0.05) ±(3.14)
BAE-R● 17.09 15.07 0.97 1.61 18.27 15.29 0.97 2.14
±(10.59) ±(0.02) ±(1.36) ±(10.34) ±(0.02) ±(1.81)
BERT-attack● 29.90 20.66 0.93 5.58 27.55 16.96 0.94 1.44
±(16.91) ±(0.05) ±(12.92) ±(12.95) ±(0.04) ±(1.76)
DeepWordBug● 60.51 11.75 0.78 0.81 56.81 11.81 0.79 0.69
±(8.00) ±(0.18) ±(0.52) ±(7.69) ±(0.16) ±(0.35)
TextBugger● 50.85 19.79 0.90 1.53 51.21 21.42 0.90 2.30
±(17.93) ±(0.06) ±(1.13) ±(19.28) ±(0.06) ±(1.61)
TextFooler● 78.98 53.18 0.84 3.75 84.48 52.45 0.84 3.84
±(39.30) ±(0.11) ±(2.76) ±(36.97) ±(0.11) ±(2.77)
TextGrad● 85.85 55.38 0.77 7.98 78.75 31.94 0.86 9.86
±(30.33) ±(0.11) ±(9.24) ±(15.57) ±(0.07) ±(9.74)
CWBA● 86.72 15.71 0.65 174.15 81.39 13.73 0.86 55.33
±(7.17) ±(0.19) ±(130.91) ±(11.24) ±(0.11) ±(43.19)
(Pruthietal.,2019)● 90.02 6.25 0.86 49.47 88.91 6.55 0.86 29.75
±(4.69) ±(0.14) ±(48.26) ±(5.13) ±(0.14) ±(24.53)
Charmer-Fast(Ours)● 95.86 4.85 0.92 3.12 91.87 4.87 0.91 3.15
±(3.96) ±(0.08) ±(3.88) ±(4.07) ±(0.09) ±(3.83)
Charmer(Ours)● 98.51 3.68 0.95 8.74 96.88 3.73 0.95 9.45
±(3.08) ±(0.06) ±(11.10) ±(3.07) ±(0.05) ±(11.20)
GBDA● 97.97 11.45 0.73 11.68 - - - -
±(6.52) ±(0.16) ±(3.23)
BAE-R● 70.00 6.46 0.83 0.53 67.39 6.47 0.84 0.54
±(3.32) ±(0.15) ±(0.44) ±(3.31) ±(0.14) ±(0.37)
BERT-attack● 92.41 6.95 0.83 26.53 97.62 6.56 0.83 2.60
±(6.57) ±(0.13) ±(204.23) ±(4.16) ±(0.14) ±(15.58)
DeepWordBug● 84.88 2.30 0.75 0.23 78.41 2.79 0.71 0.27
±(1.68) ±(0.18) ±(0.12) ±(2.02) ±(0.21) ±(0.15)
TextBugger● 85.36 4.17 0.83 0.44 86.36 5.41 0.80 0.51
±(4.33) ±(0.13) ±(0.32) ±(5.40) ±(0.15) ±(0.38)
TextFooler● 92.26 9.83 0.82 0.52 90.23 10.50 0.81 0.54
±(6.87) ±(0.14) ±(0.41) ±(7.79) ±(0.14) ±(0.42)
TextGrad● 93.69 9.98 0.75 2.50 95.44 9.10 0.79 3.56
±(5.66) ±(0.13) ±(1.97) ±(5.30) ±(0.12) ±(2.83)
(Pruthietal.,2019)● 57.62 1.32 0.83 4.48 52.84 1.36 0.82 7.48
±(0.64) ±(0.12) ±(3.73) ±(0.63) ±(0.13) ±(6.49)
Charmer-Fast(Ours)● 100.00 1.23 0.85 0.21 100.00 1.36 0.82 0.23
±(0.58) ±(0.14) ±(0.17) ±(0.78) ±(0.15) ±(0.19)
Charmer(Ours)● 100.00 1.14 0.85 1.45 100.00 1.17 0.84 1.49
±(0.42) ±(0.13) ±(0.81) ±(0.46) ±(0.13) ±(0.82)
GBDA● 47.16 11.88 0.93 13.85 - - - -
±(7.02) ±(0.06) ±(2.94)
BAE-R● 40.04 11.44 0.95 2.31 41.66 10.37 0.96 2.18
±(8.30) ±(0.07) ±(2.36) ±(9.05) ±(0.04) ±(2.77)
BERT-attack● 70.21 16.21 0.90 239.86 70.65 17.78 0.89 2.70
±(12.44) ±(0.08) ±(1395.15) ±(13.28) ±(0.12) ±(12.58)
DeepWordBug● 71.57 4.52 0.86 0.50 64.34 5.07 0.85 0.59
±(4.04) ±(0.15) ±(0.34) ±(4.67) ±(0.17) ±(0.41)
TextBugger● 75.77 8.16 0.89 0.99 67.39 9.08 0.90 0.90
±(9.93) ±(0.10) ±(0.78) ±(10.31) ±(0.10) ±(0.72)
TextFooler● 80.64 23.42 0.87 1.90 76.01 25.74 0.87 2.00
±(21.56) ±(0.12) ±(1.70) ±(28.53) ±(0.12) ±(2.09)
TextGrad● 77.35 30.03 0.82 4.54 76.80 21.56 0.87 5.80
±(20.41) ±(0.10) ±(3.82) ±(15.74) ±(0.08) ±(4.58)
(Pruthietal.,2019)● 17.70 1.57 0.93 7.22 17.45 1.54 0.93 7.46
±(0.81) ±(0.07) ±(4.91) ±(0.88) ±(0.08) ±(5.33)
Charmer-Fast(Ours)● 94.69 2.21 0.93 1.33 96.95 2.73 0.90 1.72
±(1.69) ±(0.09) ±(1.55) ±(2.15) ±(0.12) ±(2.27)
Charmer(Ours)● 97.68 1.94 0.94 9.19 97.86 2.20 0.92 10.55
±(1.48) ±(0.07) ±(9.60) ±(1.69) ±(0.08) ±(9.69)
GBDA● 76.62 8.99 0.78 16.71 - - - -
±(4.74) ±(0.13) ±(7.29)
BAE-R● 64.68 6.98 0.87 0.84 64.06 6.11 0.89 0.76
±(3.39) ±(0.09) ±(0.65) ±(3.84) ±(0.08) ±(0.69)
BERT-attack● 68.00 10.06 0.78 23.67 31.34 6.00 0.86 5.36
±(9.75) ±(0.19) ±(51.78) ±(3.96) ±(0.08) ±(20.47)
DeepWordBug● 65.67 1.64 0.85 0.12 62.67 1.83 0.82 0.13 ±(0.82) ±(0.10) ±(0.03) ±(1.09) ±(0.14) ±(0.04)
TextBugger● 74.13 3.38 0.88 0.35 71.43 3.91 0.89 0.38
±(3.48) ±(0.09) ±(0.18) ±(3.73) ±(0.08) ±(0.40)
TextFooler● 79.60 7.42 0.88 0.47 74.19 7.68 0.88 0.47
±(6.10) ±(0.09) ±(0.59) ±(5.94) ±(0.09) ±(0.59)
TextGrad● 81.77 10.02 0.76 2.44 73.97 6.40 0.84 3.57
±(5.69) ±(0.13) ±(1.06) ±(3.30) ±(0.08) ±(3.61)
(Pruthietal.,2019)● 62.19 1.18 0.86 8.45 49.31 1.21 0.87 12.23
±(0.41) ±(0.08) ±(6.25) ±(0.54) ±(0.08) ±(8.84)
Charmer-Fast(Ours)● 89.55 1.36 0.87 0.29 91.71 1.78 0.82 0.41
±(0.93) ±(0.12) ±(0.27) ±(1.71) ±(0.15) ±(0.54)
Charmer(Ours)● 97.01 1.55 0.86 2.50 97.24 1.61 0.85 2.74
±(1.42) ±(0.13) ±(2.33) ±(1.39) ±(0.13) ±(2.87)
GBDA● 83.37 12.20 0.85 9.32 - - - -
±(6.94) ±(0.11) ±(1.78)
BAE-R● 66.38 10.10 0.83 1.24 63.16 10.22 0.85 0.73
±(7.00) ±(0.18) ±(0.86) ±(6.33) ±(0.16) ±(0.62)
BERT-attack● 69.57 12.19 0.87 239.80 64.21 11.26 0.86 18.12
±(9.55) ±(0.09) ±(1763.30) ±(7.18) ±(0.10) ±(32.34)
DeepWordBug● 81.39 3.74 0.80 0.22 84.27 4.61 0.75 0.28
±(2.95) ±(0.17) ±(0.12) ±(3.47) ±(0.20) ±(0.16)
TextBugger● 68.49 5.97 0.91 1.75 61.10 6.85 0.90 1.82
±(5.87) ±(0.06) ±(0.91) ±(6.54) ±(0.05) ±(0.97)
TextFooler● 95.16 17.17 0.82 0.90 95.00 17.76 0.82 1.16
±(12.51) ±(0.15) ±(0.57) ±(12.45) ±(0.15) ±(0.76)
TextGrad● 94.04 21.61 0.75 19.94 95.49 17.07 0.81 3.75
±(11.30) ±(0.13) ±(22.32) ±(9.57) ±(0.10) ±(2.83)
CWBA● 72.92 8.55 0.53 33.81 49.84 8.88 0.65 56.35
±(3.78) ±(0.26) ±(33.86) ±(3.94) ±(0.17) ±(46.42)
(Pruthietal.,2019)● 90.94 2.22 0.85 4.86 92.93 2.52 0.84 5.29
±(1.35) ±(0.14) ±(4.02) ±(1.57) ±(0.14) ±(4.68)
Charmer-Fast(Ours)● 100.00 1.74 0.89 0.34 99.39 2.29 0.84 0.47
±(1.02) ±(0.13) ±(0.31) ±(1.53) ±(0.15) ±(0.49)
Charmer(Ours)● 100.00 1.47 0.90 1.27 99.51 1.76 0.89 1.52
±(0.74) ±(0.11) ±(0.84) ±(1.12) ±(0.12) ±(1.25)
5
sweN-GA
m-ILNM
ILNQ
ETR
2-TSSCharmer
Algorithm2CharmerAdversarialAttack
BERTSST-2
1: Inputs: modelf,sentenceS,alphabetofcharactersΣ,
70 3.0
maxLevenshteindistancek,candidatepositionsn,loss
60 2.5
functionLandlabely.
50 2.0
2: S′ =S ▷Initializeattack
40 1.5
3: fori=1,...,kdo
30 1.0
4: Z =get_top_locations(f,S′,y,n) ▷Algorithm1 ASRUpperBound
20 Algorithm1 0.5
5: S′ ={ψ(ϕ(S′)←j c), ∀j ∈Z,∀c∈Σ∪{ξ}} ▷ Random
10 0.0
AllsentenceswithmodificationsinZ 0 20 40 60
n
6: l=L(f(S′),y)▷Batchofn·(|Σ|+1)sentences
Figure2.Selectionofthenumberofcandidatepositions:Attack
7: j∗ =argmaxl j SuccessRate(ASR)atk =1(●leftaxis)andruntime(●right
j∈[|S′|]
axis)forourcandidatepositionselectionstrategy(Algorithm1,
8: S′ =S′ ▷Sentencewithhighestlossinthebatch
j∗ boldlines)andarandomselection(Random,dottedlines). Our
9: ifargmaxf(S′)̸=ythenreturnS′ ▷Successful strategyimprovestherandombaselineatasmallcost(≈0.25s).
yˆ∈[o]
10: returnS′ ▷Unsuccessful
in the dataset. Charmer-Fast simply takes n = 1 to
speed-uptheattack. ForthealphabetΣ,inordertonotin-
and additional details on the prompting strategy can be troduceout-of-distributioncharacters,wetakethecharacters
foundinAppendixB.6. SimilartoEq.(1),weaimtosolve: presentineachevaluationdataset. Allofourexperiments
wereconductedinamachinewithasingleNVIDIAA100
S′∈m Ska (x S,Σ)L(f(S P1 ⊕S′⊕S P2),y) , S chX aM ra4 ctG erP -lU ev. eF lor atb tae ctt ke sr ,il wlu est mrat ai ro kn tb he et mwe wen itt hok ●en a-l ne dve ●lan red
-
wherethemodeloutputf(S ⊕S′⊕S ) :=P(i|S ⊕ spectively. We note that alternative design decisions can
P1 P2 i P1
S′⊕S )istheconditionalprobabilityofthenexttokeni. enabletheusageofprojectedgradientascent(PGA)attacks.
P2
WecanstillusetheCarlini-WagnerLossdefinedinEq.(2) However,weobservedaworseperformanceincomparison
byconsideringthenexttokenprobabilityfortheclasses. withourstrategy,seeAppendixD.
5.Experiments 5.1.Selectingthenumberofpositions
To select the appropriate number of candidate positions
Ourexperimentsareconductedinthepubliclyavailable3
n for Algorithm 1, we evaluate the ASR and runtime of
TextAttackmodels(Morrisetal.,2020b)andopen-source
the attack with n ∈ {1,5,10,20,30,40,50,60,70}. We
largelanguagemodelsincludingLlama2-Chat7B(Touvron
conducttheexperimentwiththefine-tunedBERTonSST-2
et al., 2023) and Vicuna 7B (Chiang et al., 2023). We
from TextAttack at k = 1. For the SST-2 test sentences,
evaluate our attack in the text (or text pair) classification
the maximum number of positions across the dataset is
datasets SST-2 (Socher et al., 2013), MNLI-m (Williams
489 and the average is 213.72. We would like a value of
etal.,2018),RTE(Daganetal.,2006;Wangetal.,2019),
n much smaller than these values. As a comparison, we
QNLI(Rajpurkaretal.,2016)andAG-News(Gulli,2005;
reporttheASRcomputedbyexploringallpossiblepositions
Zhangetal.,2015).
(ASR Upper Bound). Additionally, to test the effect of
In the text pair classification tasks (MNLI-m, RTE, and ourheuristic,weevaluatetheperformancewhenrandomly
QNLI), we perturb only the hypothesis sentence. If the selectingnpositions(Random).
lengthofthetestdatasetismorethan1,000,werestrictto
InFig.2,wecanobservethattheASRconsistentlygrows
thefirst1,000samples. Ifatestdatasetisnotavailablefor
whenincreasingthenumberofcandidatepositions. How-
abenchmark,weevaluateinthevalidationdataset,thisis
ever,theincreaseislessnoticeableforn > 20,therefore,
astandardpractice(Morrisetal.,2020b). Forcomparison
theincreaseinruntimedoesnotpayofftheincreaseinASR.
withotherattacks,weusethedefaulthyperparametersof
Thisleadsustochoosen = 20fortherestofourexperi-
thosemethods.ForCharmerweusen=20positions(see
ments. Whencomparedwiththerandompositionselection,
Algorithm 1) and k = 10 except for AG-news where we
ourmethodgreatlyimprovestheASRforallthestudiedn,
usek = 20becauseofthemuchlongersentencespresent
whileintroducingaminortimeincreaseof0.25secondson
3https://huggingface.co/textattack average.
6
)%(RSA )s(emiTCharmer
5.2.Comparisonagainststate-of-the-artattacks
Table3.Attack evaluation in Llama 2-Chat 7B. Charmer
Firstly,wecompareagainstthefollowingstate-of-the-artat- -Fastoutperformsbaselinesintermsofattacksuccessrate,Lev-
tacks(i)tokenlevel:BAE-R(Garg&Ramakrishnan,2020), enshteindistance,andachievescomparablesimilarityandspeed.
TextFooler(Jinetal.,2020),BERT-attack(Lietal.,2020), Method ASR(%) d (S,S′) Sim(S,S′) Time
lev
GBDA(Guoetal.,2021)andTextGrad(Houetal.,2023),
BAE-R● 60.13 10.55 0.82 2.31
(ii) character level: DeepWordBug (Gao et al., 2018), BERT-attack● 57.86 12.05 0.86 1.61
TextBugger(Lietal.,2019)andCWBA(Liuetal.,2022). DeepWordBug● 50.82 5.24 0.73 1.01
Foreachattackmethod,weevaluatetheattachsuccessrate TextBugger● 41.89 8.99 0.89 1.63
(ASR),theaverageLevenshteindistancemeasuredatcharac- TextFooler● 85.79 20.91 0.79 3.54
terlevel(d (S,S′))andthecosinesimilarity(Sim(S,S′)) Charmer-Fast● 95.47 2.55 0.83 1.47
lev
measuredasin(Guoetal.,2021),i.e.,computingthecosine BAE-R● 47.16 10.36 0.95 2.46
similarityoftheUSEencodings(Ceretal.,2018). Weeval- BERT-attack● 60.30 14.07 0.91 2.72
uatetheperformanceoftheattacksinthefinetunedBERT DeepWordBug● 49.93 3.86 0.88 1.24
(Devlinetal.,2019)andRoBERTa(Liuetal.,2019)from TextBugger● 58.92 10.59 0.91 2.44
TextAttack. Additional experiments with ALBERT (Lan TextFooler● 64.04 18.03 0.91 4.05
etal.,2020)canbefoundinAppendixB. Charmer-Fast● 93.51 2.40 0.93 5.66
BAE-R● 66.02 6.96 0.88 1.39
Secondly,wetesttheperformanceoftheproposedmethod
BERT-attack● 90.78 8.77 0.82 1.41
in Llama 2-Chat 7B (Touvron et al., 2023). Additional DeepWordBug● 50.97 2.67 0.76 0.61
resultsonVicuna7B(Chiangetal.,2023)aredeferredto TextBugger● 79.61 7.76 0.80 1.19
AppendixB.6. NotethatinthecaseofLLMs,theinference TextFooler● 86.41 8.92 0.84 1.73
processisextremelycostly. Asaresult,weonlyusethefast Charmer-Fast● 97.10 1.68 0.82 2.06
versionofCharmer,i.e.,n=1. Moreover,weperforman
additionalpositionselectionframeworktofurtheraccelerate.
Specifically,wefirsttokenizetheinputsentenceandmask
Table4.AdversarialTrainingdefenses: Charmerisaneffec-
eachtokentodeterminethemostimportantonebasedon
tivedefenseagainstcharacter-levelattacks,minimallyaffectsclean
theloss. Next,weperformAlgorithm1forthepositionin accuracyanddoesnotimprovetoken-levelrobustness. Onthe
these important tokens. An ablation study of such token contrary,TextGradhinderscharacter-levelrobustnessandclean
selectionprocedurecanbefoundinAppendixB.6. accuracytoimprovetoken-levelrobustness.
InTables2and3,wecanobserveCharmerconsistently Method Acc.(%)↑ ASR-Char(%)↓ ASR-Token(%)↓
achievesthehighestASRwith>95%ineverybenchmark. Standard 92.43 64.02 95.16
At the same time, our method obtains the lowest Leven- Charmer● 87.20 20.34 95.17
±(1.34) ±(1.17) ±(1.15)
shteindistance(d ). Regardingthesimilarity(Sim),our TextGrad● 80.94 67.34 71.36
lev ±(0.60) ±(4.87) ±(3.63)
Charmerattainsthehighestorrunnerupsimilarityin8/10
cases,provingitsabilitytogeneratehighlysimilaradversar-
ialexamples. Withrespecttotime,Charmerisnotasfast
levelattack,Charmer,forsolvingtheinnermaximization
asthesimpleDeepWordBug,however,theruntimeiscom-
problem. WeuseTextGradwiththerecommendedhyper-
parabletopreviousstate-of-the-arttoken-levelTextGrad. If
parameters for training and Charmer with the standard
speedispreferredtoadversarialexamplequality,wecanset
hyperparametersandk = 1. Every100trainingsteps,we
n = 1(Charmer-Fast),whichattainsaruntimecloser
measuretheclean,TextFoolerandCharmer(k = 1)ad-
toDeepWordBugatthecostofahigherd 4andlowerASR.
lev versarialaccuracies. Wetrainon5randominitializationsof
ThisphenomenonisalignedwiththeresultsofSec.5.1,as
BERT-base(Devlinetal.,2019)for1epochinSST-2.
theASRatk =1islowerwhennislower.
InTable4wecanfirstlyobservethatboththeTextGradand
Charmerdefensesimprovethetoken-levelandcharacter-
5.3.AdversarialTraining
levelrobustnessrespectivelywhencomparedwiththestan-
In this section, we analyze the performance of models dardtrainingbaseline. Thiswasexpectedasthisistheob-
trained with adversarial training defenses (Madry et al., jectiveeachmethodistargetting. Interestingly,Charmer
2018). FollowingtheinsightsofHouetal.(2023),weuse doesnotimprovethetoken-levelrobustnessandTextGrad
theTRADESobjective(Zhangetal.,2019). Wecompare hindersthecharacter-levelrobustness. Thisobservationis
theuseofatoken-levelattack,TextGrad,v.s. acharacter- confirmedwhenlookingatthetrainingevolutioninFig.3.It
remainsopentoknowifweshouldaimatcharacterortoken
4ExceptforRTEwheretheaveraged issmallerduetofailure
lev
levelrobustness,neverthelessourresultsindicatecharacter-
inhardexamples,wherehigherd isneeded.
lev
levelrobustnessislessconflictedwithcleanaccuracy.
7
2-TSS
ILNQ
ETRCharmer
Table5.Robustwordrecognitiondefenses:Charmerisableto
Clean Charmer(k=1) TextFooler breakthestudieddefenseswith100%ASR.Robustwordrecogni-
tiondefensesareeffectiveonlywhenconsideringPCJconstraints.
TextGrad Charmer
1.00 1.00
Defense Acc.(%)PJC?ASR(%) d (S,S′) Sim(S,S′)
lev
0.75 0.75
✗ 100.00 1.47 0.90 0.50 0.50 None 92.43 ✓ 96.65 1.86±(0.74) 0.87±(0.11)
±(1.14) ±(0.14)
0.25 0.25 ✗ 100.00 1.28 0.90
(Pruthietal.,2019) 88.53 ±(0.51) ±(0.11)
0.00 0.00 ✓ 70.34 2.08 0.85
0 1000 2000 3000 0 1000 2000 3000 ±(1.49) ±(0.14)
Trainingstep Trainingstep ✗ 100.00 1.43 0.88
(Jonesetal.,2020) 83.94 ±(0.71) ±(0.11)
Figure3.Adversarial Training Evolution: When employing ✓ 0.96 1.14 ±(0.38) 0.92 ±(0.06)
Charmerasadefense,cleanandcharacter-levelaccuraciesgrow
consistentlythroughtrainingsteps,whiletoken-level(TextFooler) Table6.EffectofeachPJCconstraint: CharmerASRwhen
accuracyisunimproved. TheTextGraddefenseconsistentlyim- individuallyremovingeachconstraintwhilekeepingtherest.The
provesthetoken-levelaccuracyatthecostofhinderingcleanand ASRdrasticallyincreaseswhenremovingtheLowEng,Endor
character-levelaccuracy,whichgrowinthefirst≈ 400stepsto Startconstraints,provingthefragilityofexistingrobustword
thenstartdecreasing. recognitiondefenses.
Defense Acc. (%) Attackconstraint ASR(%)
5.4.Bypassingtypocorrectors PJC 70.34
-LowEng 99.22
We analyze the performance of our attack when attaking
-Length 74.61
modelsdefendedbyatypo-corrector(Pruthietal.,2019),or 88.53
-End 93.91
arobustencodingmodule(Jonesetal.,2020).Wenoticethe
-Start 98.58
successofthesedefensescanbeattributedbytheproperties
-Repeat 74.09
oftheconsideredattacks. InPruthietal.(2019);Jonesetal.
(2020),thestudiedattacksareconstrainedto5: PJC 0.96
-LowEng 98.09
• Repeat: Notperturbthesamewordtwice.
-Length 0.96
• First: Notperturbthefirstcharacterofaword. 83.94 -End 71.72
• Last: Notperturbthelastcharacterofaword. -Start 88.93
-Repeat 5.46
• Length: Notperturbwordswithlessthan4chars.
• LowEng: OnlyperturblowercaseEnglishletters.
bothdefenses,e.g.,from0.96%to98.09%ASRwhenre-
Awordisanythingbetweenblankspaces. Wedenotethese laxing LowEng in the robust encoding case. This result
as the Pruthi-Jones Constraints (PJC). While these con- indicatesthatrobustwordrecognitiondefensesprovidea
straintsaimatpreservingthemeaningofeveryindividual false sempsation of robustness. Together with the obser-
word(Rawlinson,1976;Davis,2003),insentenceclassifi- vations in Sec. 5.3, we believe adversarial training based
cation,wemightsacrificethemeaningofawordduringthe methodssupposeamorepromisingavenuetowardsachiev-
attack,iftheglobalmeaningofthesentenceispreserved. ingcharacter-levelrobustness.
Weanalyzetheperformanceofourattackwithandwithout
thePJCconstraints. Wetrainthestrongesttypo-corrector
6.Conclusion
(Pruthi et al., 2019) and use it in front of the BERT-base
modelfromTextAttack. Fortherobustencodingdefense Wehaveproposedanefficientcharacter-levelattackbased
wetrainaBERT-basemodelovertheagglomerativeclusters on a novel strategy to select the best positions to perturb
(Jonesetal.,2020). ateachiteration. Ourattack(Charmer)isabletoobtain
closeto100%ASRbothinBERT-likemodelsandLLMs
In Table 5, we can observe that Charmer attains 100%
like Llama-2. Charmer defeats both token-based adver-
ASRwhennotconsideringthePJCconstraints. Itisonly
sarialtrainingdefenses(Houetal.,2023)androbustword
when considering PJC that robust word recognition de-
recognitiondefenses(Pruthietal.,2019;Jonesetal.,2020).
fenses are effective. In Table 6 we analyze the effect of
Whenintegratedwithinadversarialtraining, ourattackis
relaxingeachofthePJCconstraintswhilekeepingtherest.
able to improve the robustness against character-level at-
We observe that by relaxing any of the LowEng, End or
tacks. Webelievedefendingagainscharacter-levelattacks
Start constraints, performance grows considerably for
is an interesting open problem, with adversarial training
5Pruthietal.(2019),furtherconstraintheattackbyonlyconsid- posingasapromisingavenuefordefenses.
eringreplacementsofnearbycharactersintheEnglishkeyboard.
8
%.ccA %.ccA
)9102,.lateihturP(
)0202,.latesenoJ(Charmer
Acknowledgements models are few-shot learners. In Advances in neural
informationprocessingsystems(NeurIPS),2020.
Authorsacknowledgetheconstructivefeedbackofreview-
ersandtheworkofICML’24programandareachairs. We Carlini,N.andWagner,D. Towardsevaluatingtherobust-
thankZulip6fortheirprojectorganizationtool. ARO-Re- ness of neural networks. In 2017 IEEE symposium on
searchwassponsoredbytheArmyResearchOfficeandwas securityandprivacy(sp),2017.
accomplished under Grant Number W911NF-24-1-0048.
Carlini,N.,Nasr,M.,Choquette-Choo,C.A.,Jagielski,M.,
HaslerAI-ThisworkwassupportedbyHaslerFoundation
Gao,I.,Koh,P.W.,Ippolito,D.,Tramèr,F.,andSchmidt,
Program: HaslerResponsibleAI(projectnumber21043).
L. Are aligned neural networks adversarially aligned?
SNFproject–DeepOptimisation-Thisworkwassupported
In Advances in neural information processing systems
bytheSwissNationalScienceFoundation(SNSF)under
(NeurIPS),2023.
grantnumber200021_205011. FanghuiLiuissupportedby
theAlanTuringInstituteundertheUK-ItalyTrustworthy Cer, D., Yang, Y., Kong, S.-y., Hua, N., Limtiaco, N.,
AIVisitingResearcherProgramme. St.John,R.,Constant,N.,Guajardo-Cespedes,M.,Yuan,
S.,Tar,C.,Strope,B.,andKurzweil,R. Universalsen-
Broaderimpact tenceencoderforEnglish. InProceedingsofthe2018
ConferenceonEmpiricalMethodsinNaturalLanguage
Inthiswork,werevisitcharacter-leveladversarialattacks Processing: SystemDemonstrations,2018.
and improve upon the prior art performance. We believe
Chen,M.,Tworek,J.,Jun,H.,Yuan,Q.,deOliveiraPinto,
thatshowingthatcharacter-levelattackscannoteasilybe
H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N.,
defended,isimportanttowarnabouttheneedofdefenses.
Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov,
Otherwise,maliciousindividualsororganizationscouldtake
M.,Khlaaf,H.,Sastry,G.,Mishkin,P.,Chan,B.,Gray,
advantageofthisunawareness. However,wenotethatour
S.,Ryder,N.,Pavlov,M.,Power,A.,Kaiser,L.,Bavar-
algorithmcouldempowerindividualstoachievemalicious
ian,M.,Winter,C.,Tillet,P.,Such,F.P.,Cummings,D.,
purposes. Wewillreleaseourcodetoallowdefendersto
Plappert,M.,Chantzis,F.,Barnes,E.,Herbert-Voss,A.,
assesstheirperformanceagainstourattack.
Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang,
J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W.,
References
Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra,
V., Morikawa, E., Radford, A., Knight, M., Brundage,
Alzantot,M.,Sharma,Y.,Elgohary,A.,Ho,B.-J.,Srivas-
M., Murati, M., Mayer, K., Welinder, P., McGrew, B.,
tava,M.,andChang,K.-W. Generatingnaturallanguage
Amodei,D.,McCandlish,S.,Sutskever,I.,andZaremba,
adversarialexamples. InConferenceonEmpiricalMeth-
W. Evaluating large language models trained on code.
odsinNaturalLanguageProcessing(EMNLP),2018.
arXivpreprintarXiv:2107.03374,2021.
Belinkov,Y.andBisk,Y. Syntheticandnaturalnoiseboth
Cheng, M., Yi, J., Chen, P.-Y., Zhang, H., andHsieh, C.-
breakneuralmachinetranslation. InInternationalCon-
J. Seq2sick: Evaluatingtherobustnessofsequence-to-
ferenceonLearningRepresentations,2018.
sequencemodelswithadversarialexamples. AAAICon-
ferenceonArtificialIntelligence,34,2020.
Bengio,Y.,Ducharme,R.,andVincent,P. Aneuralproba-
bilisticlanguagemodel. Advancesinneuralinformation Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,
processingsystems(NeurIPS),2000. H.,Zheng,L.,Zhuang,S.,Zhuang,Y.,Gonzalez,J.E.,
etal. Vicuna: Anopen-sourcechatbotimpressinggpt-4
Bojanowski,P.,Grave,E.,Joulin,A.,andMikolov,T. En-
with90%*chatgptquality. Seehttps://vicuna.lmsys.org
richingwordvectorswithsubwordinformation. Trans-
(accessed14April2023),2023.
actionsoftheAssociationforComputationalLinguistics,
2017. Dagan, I., Glickman, O., and Magnini, B. The pascal
recognisingtextualentailmentchallenge. InQuiñonero-
Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D., Candela,J.,Dagan,I.,Magnini,B.,andd’AlchéBuc,F.
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., (eds.),MachineLearningChallenges.EvaluatingPredic-
Askell,A.,Agarwal,S.,Herbert-Voss,A.,Krueger,G., tiveUncertainty,VisualObjectClassification,andRecog-
Henighan,T.,Child,R.,Ramesh,A.,Ziegler,D.,Wu,J., nising Tectual Entailment. Springer Berlin Heidelberg,
Winter,C.,Hesse,C.,Chen,M.,Sigler,E.,Litwin,M., 2006.
Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,
Davis,M. Psycholinguisticevidenceonscrambledlettersin
S.,Radford,A.,Sutskever,I.,andAmodei,D. Language
reading,2003. URLhttps://www.mrc-cbu.cam.
6https://zulip.com ac.uk/people/matt.davis/cmabridge/.
9Charmer
Demontis,A.,Melis,M.,Pintor,M.,Jagielski,M.,Biggio, Jin,D.,Jin,Z.,Zhou,J.T.,andSzolovits,P. Isbertreally
B., Oprea, A., Nita-Rotaru, C., and Roli, F. Why do robust? astrongbaselinefornaturallanguageattackon
adversarialattackstransfer? explainingtransferabilityof textclassificationandentailment. AAAIConferenceon
evasionandpoisoningattacks. In28thUSENIXsecurity ArtificialIntelligence,2020.
symposium(USENIXsecurity19),pp.321–338,2019.
Jones,E.,Jia,R.,Raghunathan,A.,andLiang,P. Robust
Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K.BERT: encodings: A framework for combating adversarial ty-
Pre-training of deep bidirectional transformers for lan- pos. InProceedingsofthe58thAnnualMeetingofthe
guageunderstanding. InProceedingsofthe2019Confer- AssociationforComputationalLinguistics,2020.
enceoftheNorthAmericanChapteroftheAssociation
Kudo, T. and Richardson, J. SentencePiece: A simple
forComputationalLinguistics: HumanLanguageTech-
andlanguageindependentsubwordtokenizeranddetok-
nologies,Volume1(LongandShortPapers),2019.
enizerforneuraltextprocessing. InProceedingsofthe
Dyrmishi,S.,Ghamizi,S.,andCordy,M. Howdohumans 2018ConferenceonEmpiricalMethodsinNaturalLan-
perceiveadversarialtext? arealitycheckonthevalidity guageProcessing: SystemDemonstrations.Association
andnaturalnessofword-basedadversarialattacks.InPro- forComputationalLinguistics,2018.
ceedingsofthe61stAnnualMeetingoftheAssociation
Lan,Z.,Chen,M.,Goodman,S.,Gimpel,K.,Sharma,P.,
forComputationalLinguistics(Volume1: LongPapers),
and Soricut, R. Albert: A lite bert for self-supervised
2023.
learning of language representations. In International
Ebrahimi, J., Rao, A., Lowd, D., and Dou, D. HotFlip: ConferenceonLearningRepresentations,2020.
White-boxadversarialexamplesfortextclassification. In
Lee,D.,Moon,S.,Lee,J.,andSong,H.O. Query-efficient
Proceedings of the 56th Annual Meeting of the Associ-
andscalableblack-boxadversarialattacksondiscretese-
ation for Computational Linguistics (Volume 2: Short
quentialdataviabayesianoptimization. InInternational
Papers),2018.
Conference on Machine Learning, pp. 12478–12497.
Gao, J., Lanchantin, J., Soffa, M. L., and Qi, Y. Black- PMLR,2022.
box generation of adversarial text sequences to evade
Lei, Q., Wu, L., Chen, P.-Y., Dimakis, A., Dhillon, I. S.,
deeplearningclassifiers. InIEEESecurityandPrivacy
and Witbrock, M. J. Discrete adversarial attacks and
Workshops(SPW),2018.
submodularoptimizationwithapplicationstotextclassi-
Garg,S.andRamakrishnan,G. BAE:BERT-basedadver- fication. ProceedingsofMachineLearningandSystems,
sarialexamplesfortextclassification. InProceedingsof 1:146–165,2019.
the2020ConferenceonEmpiricalMethodsinNatural
Levenshtein,V.I.etal. Binarycodescapableofcorrecting
LanguageProcessing(EMNLP),2020.
deletions, insertions, and reversals. In Soviet physics
Goodfellow,I.J.,Shlens,J.,andSzegedy,C. Explaining doklady,volume10,pp.707–710.SovietUnion,1966.
andharnessingadversarialexamples. InBengio,Y.and
Li, J., Ji, S., Du, T., Li, B., and Wang, T. Textbugger:
LeCun,Y.(eds.),InternationalConferenceonLearning
Generating adversarial text against real-world applica-
Representations(ICLR),2015.
tions. NetworkandDistributedSystemsSecurity(NDSS)
Gulli, A. Ag’s corpus of news articles, 2005. Symposium,2019.
URL http://groups.di.unipi.it/~gulli/
Li, L., Ma, R., Guo, Q., Xue, X., and Qiu, X. BERT-
AG_corpus_of_news_articles.html.
ATTACK:AdversarialattackagainstBERTusingBERT.
Guo,C.,Sablayrolles,A.,Jégou,H.,andKiela,D.Gradient- In Proceedings of the 2020 Conference on Empirical
based adversarial attacks against text transformers. In Methods in Natural Language Processing (EMNLP),
Proceedingsofthe2021ConferenceonEmpiricalMeth- 2020.
odsinNaturalLanguageProcessing,2021.
Liu,A.,Yu,H.,Hu,X.,Li,S.,Lin,L.,Ma,F.,Yang,Y.,and
Held, M., Wolfe, P., and Crowder, H. P. Validation of Wen, L. Character-level white-box adversarial attacks
subgradientoptimization. Mathematicalprogramming,6: againsttransformersviaattachablesubwordssubstitution.
62–88,1974. In Proceedings of the 2022 Conference on Empirical
MethodsinNaturalLanguageProcessing,2022.
Hou,B.,Jia,J.,Zhang,Y.,Zhang,G.,Zhang,Y.,Liu,S.,and
Chang,S. Textgrad: Advancingrobustnessevaluationin Liu,X.,Xu,N.,Chen,M.,andXiao,C. Autodan: Generat-
NLPbygradient-drivenoptimization. InInternational ingstealthyjailbreakpromptsonalignedlargelanguage
ConferenceonLearningRepresentations(ICLR),2023. models. arXivpreprintarXiv:2310.04451,2023.
10Charmer
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Pruthi,D.,Dhingra,B.,andLipton,Z.C. Combatingad-
Levy,O.,Lewis,M.,Zettlemoyer,L.,andStoyanov,V. versarialmisspellingswithrobustwordrecognition. In
Roberta: Arobustlyoptimizedbertpretrainingapproach. Proceedingsofthe57thAnnualMeetingoftheAssocia-
arXivpreprintarXiv:1907.11692,2019. tionforComputationalLinguistics,2019.
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
Vladu, A. Towards deep learning models resistant to Sutskever,I.,etal. Languagemodelsareunsupervised
adversarialattacks.InInternationalConferenceonLearn- multitasklearners. OpenAIblog,2019.
ingRepresentations(ICLR),2018.
Rajpurkar,P.,Zhang,J.,Lopyrev,K.,andLiang,P. SQuAD:
Mihov, S. and Schulz, K. U. Fast approximate search in 100,000+questionsformachinecomprehensionoftext.
largedictionaries. ComputationalLinguistics,30(4):451– In Su, J., Duh, K., and Carreras, X. (eds.), Proceed-
477, 2004. doi: 10.1162/0891201042544938. URL ings of the 2016 Conference on Empirical Methods in
https://aclanthology.org/J04-4003. Natural Language Processing, pp. 2383–2392, Austin,
Texas,November2016.AssociationforComputational
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Linguistics. doi: 10.18653/v1/D16-1264. URLhttps:
Dean,J. Distributedrepresentationsofwordsandphrases //aclanthology.org/D16-1264.
andtheircompositionality. InAdvancesinneuralinfor-
mationprocessingsystems(NeurIPS),2013. Rawlinson,G. TheSignificanceofLetterPositioninWord
Recognition. Phdthesis,NottinghamUniversity,1976.
Mitankin,P.N. Universallevenshteinautomata.building
and properties. Sofia University St. Kliment Ohridski, Ren, S., Deng, Y., He, K., and Che, W. Generating nat-
2005. urallanguageadversarialexamplesthroughprobability
weightedwordsaliency. InProceedingsofthe57thAn-
Morris, J., Lifland, E., Lanchantin, J., Ji, Y., and Qi, Y. nualMeetingoftheAssociationforComputationalLin-
Reevaluatingadversarialexamplesinnaturallanguage. guistics,2019.
InFindingsoftheAssociationforComputationalLinguis-
tics: EMNLP2020,2020a. Sadrizadeh,S.,Aghdam,A.D.,Dolamic,L.,andFrossard,
P. Targeted adversarial attacks against neural machine
Morris,J.,Lifland,E.,Yoo,J.Y.,Grigsby,J.,Jin,D.,andQi, translation. In ICASSP 2023-2023 IEEE International
Y. Textattack: Aframeworkforadversarialattacks,data ConferenceonAcoustics,SpeechandSignalProcessing
augmentation,andadversarialtraininginnlp. InProceed- (ICASSP),pp.1–5.IEEE,2023a.
ings of the 2020 Conference on Empirical Methods in
NaturalLanguageProcessing: SystemDemonstrations, Sadrizadeh, S., Dolamic, L., and Frossard, P. Transfool:
2020b. An adversarial attack against neural machine transla-
tion models. Transactions on Machine Learning Re-
Nasr, M., Carlini, N., Hayase, J., Jagielski, M., Cooper, search, 2023b. ISSN 2835-8856. URL https://
A.F.,Ippolito,D.,Choquette-Choo,C.A.,Wallace,E., openreview.net/forum?id=sFk3aBNb81.
Tramèr,F.,andLee,K. Scalableextractionoftraining
datafrom(production)languagemodels,2023. Sennrich,R.,Haddow,B.,andBirch,A. Neuralmachine
translation of rare words with subword units. arXiv
OpenAI. Gpt-4 technical report. arXiv preprint preprintarXiv:1508.07909,2015.
arXiv:2303.08774,2023.
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,
Palmer, D. D. Tokenisation and sentence segmentation. C.D.,Ng,A.,andPotts,C. Recursivedeepmodelsfor
Handbookofnaturallanguageprocessing,2000. semanticcompositionalityoverasentimenttreebank. In
Proceedingsofthe2013ConferenceonEmpiricalMeth-
Pennington, J., Socher, R., and Manning, C. D. Glove:
odsinNaturalLanguageProcessing(EMNLP),2013.
Globalvectorsforwordrepresentation. InProceedings
ofthe2014conferenceonempiricalmethodsinnatural Song,X.,Salcianu,A.,Song,Y.,Dopson,D.,andZhou,D.
languageprocessing(EMNLP),pp.1532–1543,2014. FastWordPiecetokenization. InProceedingsofthe2021
ConferenceonEmpiricalMethodsinNaturalLanguage
Peters,M.E.,Neumann,M.,Iyyer,M.,Gardner,M.,Clark,
Processing.AssociationforComputationalLinguistics,
C., Lee, K., and Zettlemoyer, L. Deep contextualized
2021.
wordrepresentations. InProceedingsofthe2018Confer-
enceoftheNorthAmericanChapteroftheAssociation Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to se-
forComputationalLinguistics: HumanLanguageTech- quencelearningwithneuralnetworks.Advancesinneural
nologies,Volume1(LongPapers),2018. informationprocessingsystems(NeurIPS),27,2014.
11Charmer
Szegedy,C.,Zaremba,W.,Sutskever,I.,Bruna,J.,Erhan, cc/paper_files/paper/2015/file/
D., Goodfellow, I., and Fergus, R. Intriguing proper- 250cf8b51c773f3f8dc8b4be867a9a02-Paper.
tiesofneuralnetworks. InInternationalConferenceon pdf.
LearningRepresentations(ICLR),2014.
Zhu,S.,Zhang,R.,An,B.,Wu,G.,Barrow,J.,Wang,Z.,
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, Huang,F.,Nenkova,A.,andSun,T. Autodan:Automatic
A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., and interpretable adversarial attacks on large language
Bhosale,S.,etal. Llama2: Openfoundationandfine- models. arXivpreprintarXiv:2310.15140,2023.
tuned chat models. arXiv preprint arXiv:2307.09288,
Zou,A.,Wang,Z.,Kolter,J.Z.,andFredrikson,M. Uni-
2023.
versalandtransferableadversarialattacksonalignedlan-
Touzet,H. Onthelevenshteinautomatonandthesizeofthe guagemodels. arXivpreprintarXiv:2307.15043,2023.
neighbourhoodofaword. InLanguageandAutomata
TheoryandApplications,pp.207–218.Springer,2016.
Wallace,E.,Stern,M.,andSong,D. Imitationattacksand
defensesforblack-boxmachinetranslationsystems. In
Webber,B.,Cohn,T.,He,Y.,andLiu,Y.(eds.),Proceed-
ings of the 2020 Conference on Empirical Methods in
NaturalLanguageProcessing(EMNLP),Online,2020.
AssociationforComputationalLinguistics.
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and
Bowman, S. R. GLUE: A multi-task benchmark and
analysisplatformfornaturallanguageunderstanding. In
InternationalConferenceonLearningRepresentations
(ICLR),2019.
Webster,J.J.andKit,C. Tokenizationastheinitialphasein
NLP. InCOLING1992Volume4:The14thInternational
ConferenceonComputationalLinguistics,1992.
Williams, A., Nangia, N., and Bowman, S. A broad-
coverage challenge corpus for sentence understanding
throughinference. InProceedingsofthe2018Confer-
enceoftheNorthAmericanChapteroftheAssociation
forComputationalLinguistics: HumanLanguageTech-
nologies,Volume1(LongPapers).AssociationforCom-
putationalLinguistics,2018.
Yang, P., Chen, J., Hsieh, C.-J., Wang, J.-L., and Jordan,
M.I. Greedyattackandgumbelattack: Generatingad-
versarialexamplesfordiscretedata. JournalofMachine
Learning Research, 21(43):1–36, 2020. URL http:
//jmlr.org/papers/v21/19-569.html.
Zhang, H., Yu, Y., Jiao, J., Xing, E., Ghaoui, L. E., and
Jordan, M. Theoretically principled trade-off between
robustnessandaccuracy. InInternationalConferenceon
MachineLearning(ICML),2019.
Zhang, X., Zhao, J., and LeCun, Y. Character-level
convolutional networks for text classification. In
Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and
Garnett, R. (eds.), Advances in Neural Information
ProcessingSystems,volume28.CurranAssociates,Inc.,
2015. URL https://proceedings.neurips.
12Charmer
TableS7. Qualitativecomparisonwithothercharacter-levelmethods:
Method Two-phase Any architecture Insertions and deletions Importance re-evaluation PJC
Hotflip ✓ ✗ ✓ ✓ ✗
DeepWordBug ✓ ✓ ✓ ✗ ✗
TextBugger ✓ ✓ ✓ ✗ ✗
(Yangetal.,2020) ✓ ✓ ✗ ✗ ✗
(Pruthietal.,2019) ✗ ✓ ✓ - ✓
CWBA ✗ ✗ ✓ - ✗
Charmer ✓ ✓ ✓ ✓ ✗
ContentsoftheAppendix
InAppendixAweprovideadditionalbackgroundinNLP,character-leveladversarialattacksandtheemployeddatasets.
In Appendix B we provide additional experimental validation of Charmer. In Appendix C, we provide the proof of
Corollary3.9. Lastly,AppendixDcontainspossiblegradient-basedmethodsforsolvingEq.(1).
A.Additionalbackground
Weintroduceadditionalinformationaboutothercharacter-levelattacksinAppendixA.1,theemployeddatasetsinAp-
pendixA.2andaboutNeuralNetworks(NNs)forNLPinAppendixA.3.
A.1.Comparisonwithothercharacter-levelattacks
Inthissectionweanalyzeindetailtherelationshipbetweenexistingcharacter-levelattacksandCharmer. Asshownin
Fig.2, thekeyimprovementinperformance, withoutsacrificingmuchASR,comesfromourpositionsubsetselection
strategy(Algorithm1). Inadditiontothistechnique,weanalyzethemaindifferencesbetweenmethodswiththefollowing
characteristics:
• Two-phase: Doesthemethodadoptthetwo-phaseparadigm? I.e.,evaluatetheimportanceofchars/wordstothen
greedilyselectthebestchange.
• Any architecture: Canthemethodhandleanymodelarchitecture? E.g.,canithandlechar-levelandtoken-level
models?
• Insertions and deletions: Cantheattackperforminsertionsanddeletionsofcharacters?
• Importance re-evaluation: Doesthemethodre-evaluatetheimportanceofaword/characterafteraperturba-
tionisintroduced?
• PJC:DoesthemethodadoptthePJCconstraints?
InTableS7,wecoverthecharacteristicsofthecharacter-levelattacksstudiedinthiswork. Notethatthistabledoesnot
captureaspectslikethespecificstrategyemployedforeverymethodtoselectthesubsetofchangesandthefinalchangein
thetwo-stageparadigm.
A.2.Datasets
InTableS9weprovidethesize,classes,alphabetandexamplesforallthestudieddatasets. Allofourdatasetsarepublicly
availableinhttps://huggingface.co/datasets.
A.3.NNarchitecturesforNLP
UnlikeComputerVisionapplications,whereimagescanbedirectlyfedintotheNN,somepre-processingisneededinorder
tofeedtextintoourmodels. Acommonpracticeisgroupingcharactersintotokens(Webster&Kit,1992;Palmer,2000;
Sennrichetal.,2015;Kudo&Richardson,2018;Songetal.,2021)andassigningavectorrepresentation(embedding)to
eachtokeninthetext(Bengioetal.,2000;Mikolovetal.,2013;Penningtonetal.,2014;Bojanowskietal.,2017). Aftera
sequenceofvectorrepresentationsisobtained,anappropriateNNarchitecturecanbeused,e.g.,RNNsorTransformers
13Charmer
inanencoderand/ordecoderfashion(Sutskeveretal.,2014;Petersetal.,2018;Devlinetal.,2019;Brownetal.,2020).
Overall,thearchitecturewillbe:
f(S)=fˆ(G(S)T) ,
whereE =G(S)T istheembeddingrepresentationofthesequence,G:S(Γ)→S(V )isthetokenizerwithV asthe
tok tok
tokenvocabularyandT ∈R|Vtok|×disthematrixcontainingtheembeddingsofeachtokenrow-wise.
TableS8.Attacktransferability:AdversarialexamplesaregeneratedintheSourceModeltobeevaluatedintheTargetModel.Forboth
CharmerandTextFooler,theASRisconsiderabylowerwhenthetargetmodelisdifferentfromthesourcemodel.Weobservenoclear
differenceintransferattackperformancebetweenTextFoolerandCharmer.MNLI-mandAG-Newsaretheeasiestandhardestdatasets
forgeneratingtransferattacksrespectively.
Attack AG-News MNLI-m QNLI RTE SST-2
78.98 9.77 8.13 92.26 34.89 29.12 80.64 14.49 16.36 79.60 29.52 29.17 95.16 36.51 28.29
6.05 76.22 6.44 30.24 94.98 27.08 13.72 80.72 15.05 23.38 68.25 23.96 28.54 95.79 19.15
8.07 8.92 84.48 41.90 38.35 90.23 17.48 17.21 76.01 29.85 30.33 74.19 40.12 40.64 95.00
BERT ALBERTRoBERTa BERT ALBERTRoBERTa BERT ALBERTRoBERTa BERT ALBERTRoBERTa BERT ALBERTRoBERTa
98.51 9.13 5.60 100.00 43.25 28.67 97.68 14.38 11.23 97.01 33.33 20.37 100.00 33.66 26.71
6.37 97.13 7.07 43.33 100.00 28.90 17.03 96.23 9.79 30.35 100.00 23.50 25.43 100.00 20.24
4.88 8.17 97.25 55.48 49.58 100.00 21.02 19.17 98.47 37.31 34.60 97.24 42.48 40.27 99.51
BERT ALBERTRoBERTa BERT ALBERTRoBERTa BERT ALBERTRoBERTa BERT ALBERTRoBERTa BERT ALBERTRoBERTa
SourceModel SourceModel SourceModel SourceModel SourceModel
0 100
14
relooFtxeT
remrahC
ledoMtegraT
ledoMtegraT
TREB
TREBLA
aTREBoR
TREB
TREBLAaTREBoR
TREB
TREBLA
aTREBoR
TREB
TREBLAaTREBoR
TREB
TREBLA
aTREBoR
TREB
TREBLAaTREBoR
TREB
TREBLA
aTREBoR
TREB
TREBLAaTREBoR
TREB
TREBLA
aTREBoR
TREB
TREBLAaTREBoRCharmer
TableS9.Descriptionoftheemployeddatasets:WhenacharacterisnotprintableinLATEX,wedefaulttoitsUnicodeencoding.
AG-News
TestSize 1,000
Classes 4(World,Sports,Business,Sci/Tech)
Alphabet Γ={’’,’!’,’"’,’#’,’$’,’&’,”’,’(’,’)’,’*’,’,’,’-’,’.’,’/’,’0’,’1’,’2’,’3’,’4’,’5’,’6’,’7’,’8’,’9’,’:’,’;’,’=’,
|Γ|=82 ’?’,’A’,’B’,’C’,’D’,’E’,’F’,’G’,’H’,’I’,’J’,’K’,’L’,’M’,’N’,’O’,’P’,’Q’,’R’,’S’,’T’,’U’,’V’,’W’,’X’,
’Y’,’Z’,’\’,’_’,’a’,’b’,’c’,’d’,’e’,’f’,’g’,’h’,’i’,’j’,’k’,’l’,’m’,’n’,’o’,’p’,’q’,’r’,’s’,’t’,’u’,’v’,’w’,’x’,
’y’,’z’}
Example S =’FearsforTNpensionaftertalksUnionsrepresentingworkersatTurnerNewallsaytheyare
’disappointed’aftertalkswithstrickenparentfirmFederalMogul.’,y =3(Business)
MNLI-m
TestSize 1,000
Classes 3(Entailment,Neutral,Contradiction)
Alphabet Γ={’’,’!’,’"’,’$’,’%’,’&’,”’,’(’,’)’,’,’,’-’,’.’,’/’,’0’,’1’,’2’,’3’,’4’,’5’,’6’,’7’,’8’,’9’,’:’,’;’,’?’,’A’,
|Γ|=81 ’B’,’C’,’D’,’E’,’F’,’G’,’H’,’I’,’J’,’K’,’L’,’M’,’N’,’O’,’P’,’Q’,’R’,’S’,’T’,’U’,’V’,’W’,’X’,’Y’,’Z’,
’a’,’b’,’c’,’d’,’e’,’f’,’g’,’h’,’i’,’j’,’k’,’l’,’m’,’n’,’o’,’p’,’q’,’r’,’s’,’t’,’u’,’v’,’w’,’x’,’y’,’z’,’£’,’é’,
’ô’}
Example S =’Thenewrightsareniceenough’,S =’Everyonereallylikesthenewestbenefits’,
premise hypothesis
y =2(Neutral)
QNLI
TestSize 1,000
Classes 2(Entailment,Notentailment)
Alphabet Γ={[’21513’,’21068’,’25104’,’8722’,’8211’,’8212’,’8216’,’8217’,’8220’,’8221’,’24605’,’27166’,’’,
|Γ|=233 ’!’,’"’,’#’,’$’,’%’,’&’,”’,’(’,’)’,’8230’,’+’,’,’,’-’,’.’,’/’,’0’,’1’,’2’,’3’,’4’,’5’,’6’,’7’,’8’,’9’,’:’,’;’,
’8243’,’=’,’>’,’?’,’<’,’A’,’B’,’C’,’D’,’E’,’F’,’G’,’H’,’I’,’J’,’K’,’L’,’M’,’N’,’O’,’P’,’Q’,’R’,’S’,
’T’,’U’,’V’,’W’,’X’,’Y’,’Z’,’[’,’8260’,’]’,’601’,’_’,’‘’,’a’,’b’,’c’,’d’,’e’,’f’,’g’,’h’,’i’,’j’,’k’,’l’,
’m’,’n’,’o’,’p’,’q’,’r’,’s’,’t’,’u’,’v’,’w’,’x’,’y’,’z’,’’,’|’,’’,’’,’20094’,’642’,’8838’,’21129’,’650’,
’38498’,’7841’,’£’,’7845’,’7847’,’8364’,’20140’,’8366’,’°’,’7857’,’±’,’·’,’½’,’38081’,’Å’,’Ç’,’712’,
’É’,’7879’,’Î’,’720’,’40657’,’7889’,’Ö’,’×’,’Ü’,’ß’,’à’,’á’,’ä’,’1063’,’æ’,’ç’,’è’,’é’,’ê’,’í’,’ï’,’ð’,’ñ’,
’ó’,’38515’,’õ’,’ö’,’ø’,’ù’,’8801’,’û’,’ü’,’65279’,’7940’,’263’,’268’,’269’,’272’,’1072’,’275’,’27735’,
’281’,’283’,’30494’,’30495’,’8478’,’1075’,’287’,’22823’,’26408’,’299’,’26413’,’815’,’305’,’1079’,
’1080’,’321’,’322’,’20803’,’324’,’333’,’1085’,’27491’,’1089’,’34157’,’7547’,’1093’,’379’,’626’,’928’,
’592’,’37941’,’39340’,’941’,’942’,’943’,’594’,’945’,’946’,’947’,’948’,’949’,’432’,’951’,’952’,’953’,
’954’,’955’,’956’,’596’,’950’,’959’,’957’,’961’,’962’,’964’,’966’,’23494’,’8134’,’969’,’973’,’603’,
’8172’,’8242’]}
Example S =’Whatcameintoforceafterthenewconstitutionwasherald?’,S =’Asofthatday,
premise hypothesis
thenewconstitutionheraldingtheSecondRepubliccameintoforce.’,y =1(Entailment)
RTE
TestSize 277
Classes 2(Entailment,Notentailment)
Alphabet Γ={’’,’"’,’$’,’%’,’&’,”’,’(’,’)’,’,’,’-’,’.’,’0’,’1’,’2’,’3’,’4’,’5’,’6’,’7’,’8’,’9’,’A’,’B’,’C’,’D’,’E’,
|Γ|=72 ’F’,’G’,’H’,’I’,’J’,’K’,’L’,’M’,’N’,’O’,’P’,’Q’,’R’,’S’,’T’,’U’,’V’,’W’,’Y’,’Z’,’a’,’b’,’c’,’d’,’e’,
’f’,’g’,’h’,’i’,’j’,’k’,’l’,’m’,’n’,’o’,’p’,’q’,’r’,’s’,’t’,’u’,’v’,’w’,’x’,’y’,’z’}
Example S =’DanaReeve,thewidowoftheactorChristopherReeve,hasdiedoflungcanceratage44,
premise
accordingtotheChristopherReeveFoundation.’,S =’ChristopherReevehadanaccident.’,
hypothesis
y =2(Notentailment)
SST-2
TestSize 872
Classes 2(Negative,Positive)
Alphabet Γ={’æ’,’à’,’é’,’’,’!’,’$’,’%’,”’,’(’,’)’,’,’,’-’,’.’,’/’,’0’,’1’,’2’,’3’,’4’,’5’,’6’,’7’,’8’,’9’,’:’,’;’,’?’,
|Γ|=55 ’‘’,’a’,’b’,’c’,’d’,’e’,’f’,’g’,’h’,’i’,’j’,’k’,’l’,’m’,’n’,’o’,’p’,’q’,’r’,’s’,’t’,’u’,’v’,’w’,’x’,’y’,’z’,’ö’}
Example S =’it’sacharmingandoftenaffectingjourney.’,y =2(Positive)
15Charmer
B.Additionalexperimentalvalidationanddetails
B.1.Perplexitycomparison
Tocompletetheanalysis,wecomparetheperplexityoftheattackedsentencesforallthestudiedmethods. Providedsome
methodsdirectlyoptimizetheperplexityoftheattackedsentence(Guoetal.,2021;Houetal.,2023),wereporttheGPT-2
perplexity(PPL)(Radfordetal.,2019). Nevertheless,wedonotconsiderPPLagoodmetricforassessingimperceptibility
ofadversarialexamples. Considerthefollowingsentencesandtheircorrespondingperplexity:
a) Thisfilmisgood,136.60
b) Thisfilmisbad,237.38
c) Thisfilmisgoodd,1044.97
a)andb)aregrammaticallycorrectand,asexpected,havealowPPL.However,theyhaveacompletelydifferentmeaning.
Alternatively,c)isatypoofa)andsharesitsmeaning,buthasamuchhigherPPL.Forthisreason,wewarnagainsttheuse
ofPPLforassessingimperceptibility.
TableS10. GPT-2perplexity(PPL)andattacksuccessrate(ASR)intheBERT-SST-2TextAttackbenchmark:
SST-2 QNLI MNLI-m RTE AG-News
Attack ASR(%) PPL ASR(%) PPL ASR(%) PPL ASR(%) PPL ASR(%) PPL
GBDA● 83.37 296.32 48.12 124.17 97.50 715.44 76.62 747.21 46.71 192.85
BAE-R● 66.38 351.12 40.04 60.48 70.00 359.58 64.68 258.66 17.09 78.30
BertAttack● 69.57 346.92 70.21 102.12 92.41 289.52 68.00 442.74 29.90 118.09
TextFooler● 95.16 539.19 80.64 210.61 92.26 576.84 79.60 971.69 78.98 334.05
TextGrad● 94.04 334.76 77.35 182.21 93.69 360.48 81.77 532.13 85.85 297.98
DeepWordBug● 81.39 699.91 71.57 257.02 84.88 988.90 65.67 555.76 60.51 482.79
TextBugger● 68.49 396.88 75.77 200.58 85.36 665.93 74.13 439.07 50.85 224.74
CWBA● 72.92 1206.91 - - - - - - 86.72 758.29
(Pruthietal.,2019)● 90.94 610.01 17.70 120.36 57.62 741.15 62.19 487.09 90.02 277.59
Charmer● 100 569.17 97.68 153.73 100 998.59 97.01 987.32 98.51 161.07
Charmer-Fast● 100 644.66 94.69 191.42 100 1157.81 89.55 971.88 95.86 220.13
InTableS10weobservethatCharmerandCharmer-FastobtainsimilarPPLstoothercharacter-levelattacks. We
noticetoken-levelattackspresentalowerPPLthancharacter-levelattacks,whichwasexpectedfromthepreviousexample.
B.2.QualitativeanalysisofCharmer
InthissectionweanalyzethecharacteristicsoftheperturbationsintroducedbyCharmer.
Commonperturbations: Wedisplaythemostcommonoperationsforeachdatasetwhenattackingthecorresponding
TextAttack BERT model. In Fig. S4 we can observe that across datasets, the most common operations correspond to
insertionsofpunctuationmarkssuchasparanthesis,dots,commas,questionmarks,percentagesordollarsymbols.
Locationdistribution: Weanalyzethedistributionofthelocationofperturbationsacrossthesentences. FromFig.S5
ontheonehand, wecanconcludethatthereisnoclearregionwhereattacksaremorecommonfortheMNLI-m, RTE
andSST-2datasets. Ontheotherhand,forAG-NewsandQNLI,perturbationsappeartobemorecommonclosertothe
beginningofthesentence. ThisinclinationtowardsperturbationsinAG-News,couldbeexplainedbythefactthatmost
sentencesinAG-Newsstartwiththenewsheader,therefore,themodelmightbebiasedtowardsclassifyingbasedonthe
header.
Attackexamples: Forcompleteness,weprovideinTablesS14toS18theBERTadversarialexamplesprovidedbyevery
attackinTable2inthefirst3correctlyclassifiedsentencesforeachdataset. Additionally, weincludeexamplesofthe
successful BERT-SST-2 attacks from Table 5 in Table S13. We notice that these defenses specially struggle when a
white-spaceisintroducedinthemiddleofaword. Forexample,correcting“chrming”and“astonding”as“coldrunning”
and“alsonothing”,whichgreatlychangethemeaningofthesentence.
16Charmer
TableS11. PercentageofattackedsentenceswithatleastoneOOVtokenintheBERTTextAttackmodels:
Attack SST-2 QNLI MNLI-m RTE AG-News
GBDA● 0 0 0 0 0
BAE-R● 0 0 0 0 0
BertAttack● 0 0 0 0 0
TextFooler● 0 0 0 0 0
TextGrad● 0 0 0 0 0
CWBA● 0 - - - 0
(Pruthietal.,2019)● 0 0 0 0 0
DeepWordBug● 0 0 0 0 0
TextBugger● 28.99 49.93 31.66 16.78 47.60
Charmer● 0 10.76 0 0 0
Charmer-Fast● 0 9.46 0 0 0
IntroductionofOutofVocabulary(OOV)tokens: WeanalyzetheappearanceofOOVtokensafterattackingwithall
thestudiedmethods,wecomputethepercentageofsentencewithatleastoneOOVtokenaftertheattackinalltheBERT
modelsfromTextAttack.
InTableS11,wecanobserveCharmeronlyintroducestheOOVtoken[UNK]in11%oftheattackedsentencesofthe
QNLIdataset. Inordertoavoidintroducingweirdcharacters,weonlyconsiderthecharacterspresentinthedatasetweare
attacking. Nevertheless,characterstokenizedas[UNK]likethetheU-8366characterarepresentintheQNLIdataset. In
comparisonwithothermethods,wenoticeallattacksexceptTextBuggerproduce0%OOVtokens. Thiswasexpectedasthe
strategyinTextBuggeristointroducethe[UNK]toforceamisclassification.
Tocompletetheanalysis,wehaveremovedthecharactersleadingtothe[UNK]tokenfromthealphabetconsideredduring
theattackintheBERT-QNLImodel. InTableS12wecanobservetheperformanceofCharmerandCharmer-Fastis
minimallyaffected.
TableS12.PerformancecomparisonofCharmer(-Fast)whenallowingandnotallowingtheintroductionofOOVtokens:
[UNK]? ASR(%) d
lev
✓ 97.68 1.94
Charmer
✗ 97.68 1.95
✓ 94.69 2.21
Charmer-Fast
✗ 94.47 2.19
B.3.TextAttackbaseline
InthissectionwecomplementtheanalysisinSec.5.2byreportingresultsfortheTextAttackALBERTmodels(Lanetal.,
2020). InTableS19,wecanobserveCharmerconsistentlyattainsthehighestASRamongallthestudiedmethods,while
obtainingthelowestLevenshteindistancein4/5casesandhighestsimilarityin3/5cases.
TableS13.AttackandrecognitionexamplesintheBERT-SST-2modelsdefendedwith(Pruthietal.,2019;Jonesetal.,2020):
(Pruthietal.,2019) (Jonesetal.,2020)
Original Attacked Recognition Attacked Recognition
it’sacharmingandoftenaffecting it’sachrmingandoftenaffecting it’sacoldrunningandoftenaffecting it’saæharmingandoftenaffecting it’sa[MASK]andoftenadolfeating
journey. journey. journey. journey. [MASK]early.
unflinchinglybleakanddesperate unflinchinglybleakanddesperate unflinchingbleakanddoespleasure unflinchinglybleakanddesprate unflinchinglybreakanddeeprole
allowsustohopethatnolanispol-
allowsustohopethatnolanispoised allowsustohopethatnolanispoised allowsustohopethatnoanispoised angelesustohethatnoanisplayed
ishedbadembracesamajorcareeras
toembarkamajorcareerasacom- tdembarkamajorcareerasacom- toembarkamajorcareerasacom- toembarkamothercenterasacom-
acommercialn’tinventivefilmmaker
mercialyetinventivefilmmaker. mercialye’tinventivefilmmaker. mercialyetinventivefilmmaker. mercialyetintensefilmmaker.
.
theacting,costumes,music,cin- theacting,costumes,music,cin- theacting,costumes,music,cin- theamong,costumes,music,cine-
theacting,costumes,music,cine-
ematographyandsoundareallas- ematographyandsoundareallan- ematographyandsoundareallasto matographyandsaidareallalsonoth-
matographyandsoundareallannoy-
toundinggiventheproduction’saus- toundinggiventheproduction’saus- ndinggiventheproduction’saustere inggiventheproduction’saustere
inggiventheproduction’salocales.
terelocales. terelocales. locales. less.
17Charmer
AG-News MNLI-m QNLI RTE SST-2
(ξ,5) 23 (n,G)6 (ξ,;)10 3(r,%) (ξ,u) 8
(ξ,!) 23 (o,) 7 („.)10 3(.,S) (s,‘) 8
(ξ,=) 23 (ξ,O) 7 (ξ,5)10 4(ξ,G) (ξ,w) 8
(-,") 24 (ξ,W)7 (w,.)10 4(ξ,C) (ξ,o) 9
(,ξ) 24 (n,S) 7 (ξ,)10 4(ξ,") (ξ,h) 10
(s,’) 25 (ξ,U) 7 (ξ,ı)10 4(ξ,U) (o,) 11
(ξ,-) 28 (ξ,£) 7 (ξ,))11 4(ξ,9) (ξ,)) 11
(ξ,) 29 (t,N) 8 11(ξ,&) 4(,2) (,n) 12
(ξ,F) 31 (n,L) 8 (ξ,()11 (ξ,A) 5 (ξ,:) 13
(ξ,J) 32 (ξ,4) 8 (,.) 12 (ξ,O)5 (ξ,‘) 13
(ξ,B) 34 (n,%) 9 (ξ,%)14 (ξ,&)5 (s,n) 14
(ξ,Q) 35 (ξ,S) 9 (ξ,}) 14 (ξ,) 6 (ξ,%) 14
(ξ,_) 36 (ξ,C) 10 14(ξ,22823) (e,$) 6 (ξ,y) 15
(ξ,:) 43 (ξ,!) 10 (ξ,$) 15 (ξ,)) 7 (ξ,l) 15
(ξ,$) 45 (ξ,’) 11 (ξ,?) 18 (ξ,() 7 (t,!) 15
(ξ,\) 48 (ξ,,) 13 (t,.) 20 (ξ,.) 7 (ξ,.) 16
(ξ,") 52 (e,$) 16 (ξ,,) 22 (.,) 7 (ξ,,) 17
(ξ,’) 57 (,N) 18 (ξ,21513) 25 (ξ,S) 14 (ξ,() 18
(ξ,.) 58 (ξ,$) 26 (ξ,.) 52 (ξ,$) 15 (ξ,!) 19
(ξ,)) 60 (ξ,%) 29 (,8366) 70 (ξ,%) 20 (ξ,’) 34
0 20 40 60 0 10 20 30 0 20 40 60 0 10 20 0 10 20 30 40
Frequency Frequency Frequency Frequency Frequency
FigureS4.Top20mostcommonreplacementswithCharmer:Thepairofcharacters(c ,c )indicatesthatc isreplacedbyc inthe
1 2 1 2
sentence.Ifc =ξ,thereplacementrepresentsaninsertionandifc =ξtheoperationrepresentsadeletion.Thespecialcharacteris
1 2
denotedasξastheGreekcharacterξdidnotappearinthemostcommonoperations. Themostcommonoperationsareinsertionsof
punctuationandspecialcharacters.
AG-News MNLI-m QNLI RTE SST-2
100 10
100 20 20
50 10 50 5 10
0 0 0 0 0
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
Relativelocation Relativelocation Relativelocation Relativelocation Relativelocation
FigureS5.DistributionoftherelativelocationofperturbationsinthesentencewithCharmer:0and1representaninsertionbefore
thefirstcharacterandafterthelastcharacterinthesentencerespectively.WedonotobserveanytendencyintheQNLI,RTEandSST-2
datasets.ForAG-NewsandQNLI,theperturbationsinlocationscloserto0appeartobemorecommon.
18
.qerF .qerF .qerF .qerF .qerFCharmer
TableS14. Attackexamplesinthefirst3sentencesofAG-News.
Method Sentence Prediction
Original Fears for T N pension after talks Unions representing workers at Turner Newall say they are ’disappointed’ after talks 2
with stricken parent firm Federal Mogul.
TextBugger● Fears for T percent pension after conversationAssociations representing workers at Turner Newall say they are 3
’dsappointed’ after talks with stricken parent firm Federal Mogul.
TextGrad● Fears for t n pension after chatcustomers representing hours at turner newall complain they are’disappointed’after 3
chat with stricken parent provider federal mogul.
TextFooler● Fears for T percent pension after debateSyndicatesportrayal worker at Turner Newall say they are ’disappointed’ after 0
chatter with bereaved parenting corporationsCanada Mogul.
DeepWordBug● Fears for T N pension after alks Unions representing workers at Turner Newall say they are ’disppointed’ after taclks 2
with stricken parent firm GFederal Mogul.
BAE-R● Fears for T pl pension after talks Unions representing workers at Turner controls say they are ’disappointed’ after 3
talks with stricken parent firm Federal Mogul.
BERT-attack● Fears for T e pension after talks Unions representing workers at Turner Newall say they are ’disappointed, after 3
discussion with stricken parent firm global Mogul.
GBDA● fears for t n pension after talks unions representing workers at turner newall say they are’disappointed’after talks 0
with stricken parent knesset federal mogul’
CWBA● fe±rs for t n pen8314ion af1408er ta322ks un1603ons representing wo248yers at tu650ner newall say they 3
are’disaxacalanted’after ta38525ks with stfucken par12459nt firm federal mogul.
(Pruthietal.,2019)● Fears for T N pension after talks Unions representing workers at Tuurner Neqall say they are ’disappointed’ after 0
talks with stricken parent firm Federal Mogul.
Charmer● Fears for T E pension :fter talks Unions representing workers at Turner Newall say they are ’disappointed’ after talks 3
with stricken parent firm Federal Mogul.
Original The Race is On: Second Private Team Sets Launch Date for Human Spaceflight (SPACE.com) SPACE.com - TORONTO, Canada 3
- A second \team of rocketeers competing for the #36;10 million Ansari X Prize, a contest for\privately funded
suborbital space flight, has officially announced the first\launch date for its manned rocket.
TextBugger● The Race is On: SegundoOwn Team Sets InitiateStardate for Humanitarian Spaceflight (SPACE.com) SAPCE.com - VANCOUVER, 3
Canadian - per secs\team of rocketeers compete for the #36;10 m1110llion Asari X Prize, a contest for\secretly funded
suborbital space flight, has solemn announced the first\lau1400ch date for its manned rocket.
TextGrad● The election is on: second private party sets eva date for human spacecapsule (moon.com) space.com - paris, 0
canada -- a fourth\trio of rocketeers seeking for the #36;10 million ansari x prize, a contest for\privately dollar
suborbitapollo space jump, has openly announced the first\launch date for its young child.
TextFooler● The Race is Around: Second Privy Remit Set Lanza Timeline for Humanitarian Spaceflight (SEPARATION.com) 0
SEPARATION.com - CANADIENS, Countries - para second\squad of rocketeers suitors for the #36;10 billion Ansari X Nobel,
a contestant for\covertly championed suborbital spaceship plane, had solemnly proclaim the first\begantimeline for its
desolatebomb.
DeepWordBug● The Race is On: Second Private Tam Sets ZLaunch Date for HumJan Spaceflight (SPACE.com) SPACE.col - TORONTO, Canada 3
- A second\team of rocketeers competing for the #3;1 million Asari X Priz, a contest for\privately ufnded suborbitIal
space flight, has officially announced the first\launch Bate for its mannwed rocket.
BAE-R● The Race is On: current Private Team Sets launches Date for Human Spaceflight (SPACE.com) SPACE.com - TORONTO, 3
sa--canada second\jury of rocketeers competing for the #36;10 million Ansari X s, a contest for\privately financed
suborbital space flight, has just announced the first\launch date for its proposed rocket.
BERT-attack● The Race is On: Second Private Team Sets landing Date for earth Spaceflight (mars.com) SPACE.com - TORONTO, Canada 3
- is current\team of rocketeers competing for the #36;10 million Ansari X quest, a contest for\privately financed
suborbital space launch, has officially announced the first\landingnumber for its apollo rocket.
GBDA● the race is on : second private team sets launch date for human spaceflight ( barack. com ) continents. com - 0
newsweek, cuba - - a second \team of rocketeers competing for the # 36 ; 10 million ansari x prize, a contest for
\privately funded suborbital space flight, has officially announced the first \launch date for its manned rocket.
CWBA● t20234e rauee is on : sec601nd private te4536m sets lau1410ch da1746e for human space969light ( sp*ce. c2m ) 3
sp64257ce. c699m - tor8482nto, can8482da - - a second \team of rockiaeers com0sting for the # 36 ; 10 million ansari
x pr3936ze, a contest for \privately fun24179ed suborbital space flight, has offeldally announced the first \launch
date for its man2488ed roc20986et.
(Pruthietal.,2019)● The Race is On: Second Privqte Team Sets Launch Dajte for Himan Spacefight (SPADE.com) SPADE.com - TODONTO, Cahada - 3
A sdcond\tam of rocketees copmeting for the #36;10 million Anqari X Pfize, a cntest for\privately funedd sublorbital
space fight, has offically announced the first\launch date for its mabned rocket.
Charmer● The Race is On: SeZond Private Team Sets LaunchDate for ?uQan Spaceflight (SPACE/com) SPDACE.0om - TORONTO, Canada U- 0
A second"team f rocketeers competing for the #36;10 million Ansari X Prize, a contest forDrivatelB funded suborbital
space flight, has officially announced ’he first$launch date for its manned rocket.
Original Ky. Company Wins Grant to Study Peptides (AP) AP - A company founded by a chemistry researcher at the University of 3
Louisville won a grant to develop a method of producing better peptides, which are short chains of amino acids, the
building blocks of proteins.
TextBugger● Ky. Compnay Wins Subsidies to Examine Ppetides (AP) APS - A company based by a chemist research1077r at the Academia 3
of Indianapolis won a grant to develop a methodology of production best peptides, which are brief string of amino aids,
the building block of prote1110ns.
TextGrad● Ky. company wins awarded to treat peptides (ab) ao - a company founded by a chemistry researcher at the time of 2
louisville won a lead to develop a treatment of producing greater peptides, which are short chains of fatty acids, the
basis blocks of muscle.
TextFooler● Ky. Businesses Wins Grant to Study Peptides (HAS) HAS - A company founded by a chemistry researcher at the University 2
of Louisville won a grant to develop a method of producing better peptides, which are short chains of amino acids, the
building blocks of proteins.
DeepWordBug● Ky. Comapny Wins Grant to SWtudy Peptieds (A) AP - A company foRnded by a cFhemistry researchfer at the Univrsity 2
of Louisvlle won a grant to devIelop a Qethod of proucing bette peptides, which are short Shains of amino acids, the
bunlding blocks of profteins.
BAE-R● Ky. mit Wins Grant to Study Peptides (AP) AP - biotechnology company founded by a chemistry researcher at the 3
University of Louisville won a grant to study a method of producing better peptides, which are short chain of amino
acids, the building blocks of genes.
BERT-attack● Ky. university Wins commission to research Peptides (AP) AP - A company owned by a chemistry lecturer at the 3
University of ky won a grant to research a method of research better peptides, which are short chains of amino acids,
the building blocks of protein.
GBDA● ky. company wins grant to study peptides ( ap ) ap - a company founded by a chemistry researcher at the university of 3
louisville won a grant to develop a method of producing better peptides, which are short chains of amino acids, the
building blocks of proteins.
CWBA● ky. com1405any wi2327s gr9824nt to stjdy pep65293ides ( ap ) ap - a company fzonded by a chehritry ressefcher at 1
the unfeitsity of louisdille wan a gr12449nt to devefop a met12369od of producing better pep1110ides, which are short
chains of am1074no ac1495ds, the building blocks of profcins.
(Pruthietal.,2019)● Ky. Compahy Wins Grant to Stuwy Peptdies (AP) AP - A company founded by a chemistry researcher at the UQniversity of 1
Louisville won a grant to develop a method of producing better pepgides, which are short chains of amino aids, the
building blocks of proheins.
Charmer● Ky. Company Wins Grant to StuJdy Peptides (AP) AFP - A company founded by a chemistry researcher at the University of 0
Louisville won a grant to develop a method of producing better peptides, which are short chains of amino acids, the
building blocks of proteins.
19Charmer
TableS15. Attackexamplesinthefirst3sentencesofMNLI-m.
Method Sentence Prediction
Original Everyone really likes the newest benefits 2
TextBugger● Somebody really lies the newest benefits 0
TextGrad● Everyone really hates the newest benefits 0
TextFooler● Nobody really likes the newest benefits 0
DeepWordBug● Everyone really yikes the newest benefits 0
BAE-R● nobody really likes the newest benefits 0
BERT-attack● Everyone really hates the newest benefits 0
GBDA● everyone really likes the newest misery 0
(Pruthietal.,2019)● Everyone really lies the newest benefits 0
Charmer● Everyone really Yikes the newest benefits 0
Original The Government Executive articles housed on the website are not able to 0
be searched.
TextBugger● The Government Executive articles housed on the websites are not able 2
to be searched.
TextGrad● The government executive articles housed on the website are not able to 2
be destroyed.
TextFooler● The Government Executive articles housed on the website are not 2
incapable to be searched.
DeepWordBug● The Government Executive articles housed on the website are not able to 2
be sarched.
BAE-R● The Government cabinet articles housed on the website are not likely to 2
be searched.
BERT-attack● The Government Executive articles housed on the website are not to to 2
be visited.
GBDA● the government executive articles housed on the website are not sure to 2
be raided.
(Pruthietal.,2019)● The Government Executive articles housed on the website are not able to 2
be sarched.
Charmer● The Government Executive articles housed on the website are Got able to 1
be searched.
Original I like him for the most part, but would still enjoy seeing someone beat 1
him.
TextBugger● I like him for the most part, but would still enjoy seeing someone beat 1
him.
TextGrad● I like him for the most cent, but would never enjoy seeing someone beat 0
him.
TextFooler● I like him for the most portion, but would still cherishes seeing 2
someone conquering him.
DeepWordBug● I like him for the most art, but would still enjoy seeing someone beat 2
him.
BAE-R● I like him for the most people, but would still enjoy seeing someone 2
beat him.
BERT-attack● I like him for the most all, but would still enjoy seeing someone beat 2
him.
GBDA● i like him for the most part, but howard always regrets seeing someone 2
beat him.
(Pruthietal.,2019)● I like him for the most part, but would still enjoy sewing someone beat 2
him.
Charmer● I like him for the most p4art, but would still enjoy seeing someone 2
beat him.
20Charmer
TableS16. Attackexamplesinthefirst3sentencesofQNLI.
Method Sentence Prediction
Original As of that day, the new constitution heralding the Second Republic came 0
into force.
TextBugger● As of that day, the new constitution heralding the Second Republics 1
came into for1010e.
TextGrad● As of that day, the new constitution heralding the second republic 1
registered into real.
TextFooler● As of that day, the new constitution heralding the Second Republics 1
went into troupes.
DeepWordBug● As of that day, the new constitution heralding the Second Republic came 1
into ofrce.
BAE-R● As of that document, the new constitution heralding the Second republic 0
came into existence.
BERT-attack● As of that day, the new constitution heraldof the Second Republic came 1
into real.
GBDA● as of that day, the new constitution heralding the second republic came 0
into force.
(Pruthietal.,2019)● As of that day, the new constitution heralding the Second Republic came 1
into forde.
Charmer● As of that day, the new constitution heralding the Second Republic came 1
into for$ce.
Original The most important tributaries in this area are the Ill below of 1
Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
TextBugger● The most important tributaries in this areas are the Ill below of 0
Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
TextGrad● The most important tributaries in this area are the ill below of 0
strasbourg, the neckar in mannheim and the main across from cincinnati.
TextFooler● The most important tributaries in this areas are the Ill below of 0
Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
DeepWordBug● The most important tributaries in this area are the IAl below of 0
Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
BAE-R● The most important tributaries in this sector are the Ill below of 0
Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
BERT-attack● The most important tributaries in this area are the far below of 0
Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
GBDA● the most important tributaries in this area are the ill below of 0
strasbourg, the neckar in mannheim and the jedi across from mainz.
(Pruthietal.,2019)● The most important tributaries in this area are the Ill below of 1
Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
Charmer● The most iðportant tributaries in this area are the Ill below of 0
Strasbourg, the Neckar in Mannheim and the Main across from Mainz.
Original In most provinces a second Bachelor’s Degree such as a Bachelor of 1
Education is required to become a qualified teacher.
TextBugger● In most provinces a s1077cond Bac1392elor’s Degrees such as a 0
Bachelo11397 of Education is required to become a qualified teacher.
TextGrad● In most provinces a minimum bachelor’s degree such as a bachelor of 0
education is required to become a qualified teacher.
TextFooler● ing most provinces a second Bachelor’s Grades such as a Diplomas of 0
Tuition is required to become a qualified teacher.
DeepWordBug● I most provinces a qecond Bachelor’s Wegree such as a BFchelor of 0
ducation is required to beome a qualified teachr.
BAE-R● In most provinces a basic Bachelor’s Degree such as a bachelor of 1
studies is needed to become a qualified teacher.
BERT-attack● In most canadian a diploma bachelorthethe Degree such as a major of 0
Education is required to become a qualified teacher.
GBDA● in most provinces a second bachelor’s degree such as a bachelor of 0
education is minimum to become a qualified teacher.
(Pruthietal.,2019)● In most provinces a second Bachelor’s Degree such as a Bachelor of 1
Education is required to become a qualified teacher.
Charmer● In most provinces a2second Bachelor’s Degree such as a Bachelor of 0
Education is required to become a qualified teacher.
21Charmer
TableS17. Attackexamplesinthefirst3sentencesofRTE.
Method Sentence Prediction
Original Christopher Reeve had an accident. 1
TextBugger● Christopher Reeve had an accident. 1
TextGrad● Christopher reeve had an career. 0
TextFooler● Valeria Reeve was an collisions. 0
DeepWordBug● Christopher Reeve had an accidnt. 1
BAE-R● karen Reeve had an accident. 1
BERT-attack● david Reeve had an stroke. 0
GBDA● christopher reeve had an stroke. 0
(Pruthietal.,2019)● Christopher Reeve had an acciSdent. 1
Charmer● Christopher Reeve had an accidentS 0
Original Pennsylvania has the biggest Amish community in the U.S. 1
TextBugger● Penn has the largest Amish community in the U.S. 0
TextFooler● Pennsylvania has the wide Amish community in the U.S. 0
DeepWordBug● Pennsylvania has the bigges Amish community in the U.S. 1
BAE-R● Pennsylvania has the large Amish community in the U.S. 0
BERT-attack● Pennsylvania has the huge Amish community in the U.S. 1
GBDA● pennsylvania has the strongest amish community in the state. nara 0
geographical
(Pruthietal.,2019)● Pennsylvania has the biggeat Amish community in the U.S. 0
Charmer● Pennsylvania has the biggeAt Amish community in the U.S. 0
Original Security forces were on high alert after a campaign marred by violence. 0
TextBugger● Security forces were on high alert after a countryside marred by 1
violence.
TextGrad● Security families were on high alert after a month marred by violence. 1
TextFooler● Security forces were on high alert after a countryside marred by 1
violence.
DeepWordBug● Security forces were on high alert after a cmpaign marred by violence. 1
BAE-R● ransport force were on high alert after a campaigning marred by 1
violence.
GBDA● security forces were on high alert after a campaign marred by marches. 1
(Pruthietal.,2019)● Security forces were on high alert after a campairn marred by violence. 1
Charmer● Security forces were on high alert after a2campaign marred by violence. 1
22Charmer
TableS18. Attackexamplesinthefirst3sentencesofSST-2.
Method Sentence Prediction
Original it ’s a charming and often affecting journey . 1
TextBugger● it ’s a ch593rming and often affecting voyage . 1
TextGrad● it’s a dangerous and often affecting travelling. 0
TextFooler● it ’s a cutie and often afflicts journey . 0
DeepWordBug● it ’s a Wcharming and otfen affceting journey . 0
BAE-R● it ’s a dark and often winding journey . 0
BERT-attack● it ’s a one and often another journey . 1
GBDA● it’s a colourful and not affecting journey. 0
CWBA● it’s a char640ing a→d ofwen affe37070ting jo1657ney. 0
(Pruthietal.,2019)● it ’s a chrming and often acfecting journey . 0
Charmer● it ’s a %harming and often affecting journey . 0
Original unflinchingly bleak and desperate 0
TextBugger● unflinchingly somber and desperate 1
TextGrad● unflinchingly dark and desperate 1
TextFooler● unflinchingly eerie and desperate 1
DeepWordBug● unflinchingly blak and despertae 1
BAE-R● unflinchingly happy and desperate 1
BERT-attack● unflinchingly dark and desperate 1
GBDA● unflinchingly picturesque and desperate 1
CWBA● unfl30340aringly byaak ayod deshilarate 1
(Pruthietal.,2019)● unflinchingly beak and deseprate 1
Charmer● unflinchingly àbleak and desperate 1
Original allows us to hope that nolan is poised to embark a major career as a 1
commercial yet inventive filmmaker .
TextBugger● allows nous to hope that nolan is poised to embark a major career as a 0
commercial however invntive cinematographers .
TextGrad● allows us to argue that nolan is ineligible to embark a major career as 0
a commercial fails inventive filmmaker.
TextFooler● allows ourselves to hope that nolan is poised to embarked a severe 0
career as a commercial yet noveltysuperintendent .
DeepWordBug● allows Gs to hope that nolan is Loised to embark a major career as a 0
commercial yet invewntive filmmaker .
BAE-R● allows it to hope that nolan is poised to assume a major career as a 1
commercial yet amateur filmmaker .
BERT-attack● allows to to hope that nolan is eligible to embark a major career as a 0
commercial yet inventexperienced filmmaking .
GBDA● allows us to doubt that nolan is poised to embark a major career as a 0
commercial lower inventive writer.
CWBA● all21335ws us to ho26954e that nolan is po2313sed to embark a major 0
career as a commbecial y1705t inventive filmmaker.
(Pruthietal.,2019)● allows us to hope that nolan is poised to ebark a major career as a 0
commercial yet infentive filmmaker .
Charmer● allows us to hope that no$an is poised to embark a major career as a 0
commercial yet inventive filmmaker .
23Charmer
TableS19.AttackevaluationintheTextAttackALBERTmodels:Token-levelandcharacter-levelattacksarehighlightedwith●and
●respectively.foreachmetric,thebestmethodishighlightedinboldandtherunner-upinunderlined.Charmerconsistentlyachieves
highestAttackSuccessRate(ASR).
ALBERT
Method ASR(%)↑ d (S,S′) ↓ Sim(S,S′) ↑ Time(s) ↓
lev
CWBA● 57.96 22.69 0.64 205.69
±(21.07) ±(0.22) ±(162.00)
BAE-R● 18.26 15.15 0.97 1.84
±(11.28) ±(0.02) ±(1.70)
BERT-attack● 37.23 21.34 0.93 2.41
±(15.29) ±(0.05) ±(2.14)
DeepWordBug● 56.90 9.77 0.83 0.73
±(6.77) ±(0.14) ±(0.40)
TextBugger● 71.76 17.48 0.91 1.38
±(16.82) ±(0.06) ±(0.98)
TextFooler● 76.22 46.51 0.87 3.89
±(35.21) ±(0.10) ±(3.11)
TextGrad● 75.37 42.43 0.85 8.23
±(19.05) ±(0.07) ±(9.15)
(Pruthietal.,2019)● 88.00 5.50 0.89 29.17
±(4.74) ±(0.13) ±(25.58)
Charmer-Fast● 95.44 3.25 0.95 2.38
±(2.88) ±(0.06) ±(3.27)
Charmer● 97.13 2.45 0.97 6.94
±(2.31) ±(0.04) ±(12.33)
BAE-R● 71.57 6.28 0.84 0.54
±(3.27) ±(0.14) ±(0.34)
BERT-attack● 97.50 7.14 0.84 2.96
±(5.17) ±(0.12) ±(16.16)
DeepWordBug● 85.90 2.31 0.77 0.27
±(1.63) ±(0.17) ±(0.13)
TextBugger● 88.05 4.86 0.82 0.55
±(4.64) ±(0.13) ±(0.38)
TextFooler● 94.98 9.49 0.82 0.55
±(6.45) ±(0.13) ±(0.40)
TextGrad● 94.15 8.96 0.79 2.33
±(4.90) ±(0.12) ±(1.63)
(Pruthietal.,2019)● 58.18 1.26 0.84 5.14
±(0.57) ±(0.11) ±(5.25)
Charmer-Fast● 100.00 1.17 0.85 0.22
±(0.42) ±(0.13) ±(0.15)
Charmer● 100.00 1.08 0.86 1.53
±(0.28) ±(0.11) ±(0.70)
BAE-R● 45.97 10.94 0.95 2.52
±(7.57) ±(0.06) ±(2.46)
BERT-attack● 73.40 16.74 0.90 760.09
±(16.94) ±(0.10) ±(5904.67)
DeepWordBug● 74.07 5.00 0.85 0.57
±(4.36) ±(0.16) ±(0.43)
TextBugger● 76.03 9.59 0.90 1.15
±(11.48) ±(0.10) ±(0.99)
TextFooler● 80.72 22.56 0.88 2.13
±(20.96) ±(0.11) ±(2.00)
TextGrad● 74.78 28.47 0.84 5.78
±(17.98) ±(0.09) ±(6.02)
(Pruthietal.,2019)● 26.47 1.85 0.93 10.12
±(1.18) ±(0.09) ±(8.49)
Charmer-Fast● 96.19 2.26 0.93 1.58
±(1.74) ±(0.08) ±(2.36)
Charmer● 96.23 1.78 0.94 9.60
±(1.11) ±(0.07) ±(8.10)
BAE-R● 61.14 6.69 0.88 0.82
±(3.43) ±(0.09) ±(0.53)
BERT-attack● 9.80 5.20 0.86 21.39
±(2.95) ±(0.16) ±(34.86)
DeepWordBug● 59.24 1.54 0.84 0.13
±(0.84) ±(0.13) ±(0.05)
TextBugger● 70.62 4.45 0.87 0.44
±(5.24) ±(0.11) ±(0.33)
TextFooler● 68.25 7.60 0.89 0.52
±(5.61) ±(0.09) ±(0.65)
TextGrad● 70.70 7.07 0.83 2.56
±(3.25) ±(0.10) ±(2.28)
(Pruthietal.,2019)● 48.34 1.22 0.86 11.56
±(0.41) ±(0.09) ±(7.69)
Charmer-Fast● 97.16 1.68 0.83 0.42
±(1.32) ±(0.14) ±(0.44)
Charmer● 100.00 1.29 0.87 2.49
±(0.65) ±(0.10) ±(2.13)
CWBA● 77.88 11.18 0.55 58.28
±(4.58) ±(0.25) ±(50.83)
BAE-R● 62.77 10.25 0.85 0.78
±(7.24) ±(0.16) ±(0.77)
BERT-attack● 72.34 11.57 0.85 148.11
±(6.89) ±(0.10) ±(1077.71)
DeepWordBug● 84.78 3.37 0.82 0.23
±(2.47) ±(0.16) ±(0.12)
TextBugger● 72.52 5.61 0.91 1.85
±(5.51) ±(0.06) ±(0.90)
TextFooler● 95.79 15.79 0.83 1.11
±(11.12) ±(0.14) ±(0.74)
TextGrad● 96.28 18.67 0.80 2.95
±(9.73) ±(0.11) ±(1.65)
(Pruthietal.,2019)● 95.05 1.98 0.87 4.66
±(1.28) ±(0.13) ±(3.98)
Charmer-Fast● 99.88 1.62 0.89 0.38
±(0.88) ±(0.12) ±(0.34)
Charmer● 100.00 1.38 0.91 1.38
±(0.67) ±(0.10) ±(0.93)
24
sweN-GA
m-ILNM
ILNQ
ETR
2-TSSCharmer
TableS20.EffectofeachPJCconstraint: CharmerASRwhenindividuallyremovingeachconstraintwhilekeepingtherest. Per-
formancewithnoconstraints(None)putasreference. TheASRdrasticallyincreaseswhenremovingtheLowEng,EndorStart
constraints,provingthefragilityofexistingrobustwordrecognitiondefenses.
RTE MNLI-m QNLI
Defense Attackconstraint Acc. (%) ASR(%) Acc. (%) ASR(%) Acc. (%) ASR(%)
None 92.17 100.00 86.38
PJC 42.17 87.39 43.05
-LowEng 70.48 97.90 65.53
-Length 60.36 45.78 76.33 92.51 73.55 46.19
-End 63.86 96.19 57.63
-Start 69.88 97.11 57.08
-NoRepeat 42.17 89.36 42.51
None 65.93 100.00 98.56
PJC 2.96 2.93 1.05
-LowEng 57.04 96.63 94.62
-Length 48.74 2.96 68.44 2.93 76.20 0.79
-End 44.44 75.26 54.99
-Start 47.41 82.87 67.32
-NoRepeat 6.67 9.81 3.81
B.4.Attacktransferability
InthissectionwestudythetransferabilityofCharmerattacks. Thisisawidelystudiedsetupinthecomputervision
community(Demontisetal.,2019). Foreachdataset,attackandmodel,wegeneratetheattackedsentencesandevaluatethe
ASRwhenusingthemforattackingothermodels. Asareferencewetakethebesttoken-levelmethodfromTable2,i.e.,
TextFooler.
InTableS8wecanobservebothTextFoolerandCharmerfailtoproducehighASRsinthetransferattacksetup. Asa
reference,thehighesttransferASRwas55.48%andwasattainedbyCharmerintheMNLI-mdataset,withBERTasa
SourceModelandRoBERTaasthetargetmodel. WenoticeintheAG-Newsdatasetitisconsiderablyhardertoproduce
transferattacks,withthehighesttransferASRbeing9.77%amongallsetups. WebelieveimprovingtheASRinthetransfer
attacksetupisaninterestingavenue.
B.5.Robustwordrecognitiondefenses
Tocompletetheanalysis,werepeattheexperimentsinSec.5.4intheRTE,MNLI-mandQNLIdatasets7.
InTableS20wecanobserveasimilarphenomenonasinTable6,i.e.,robustwordencodingdefensesonlyworkwhen
assumingtheattackeradoptsthePJCconstraints. WheneithertheLowEng,StartorEndconstrantsarerelaxed,the
ASRconsiderablygrowscloseto100%. ItisworthmentioningthatintheRTEdataset,defendingwithJonesetal.(2020)
resultsinCharmerwithoutanyconstraintsachievingonly65.93%ASR.Nevertheless,thisdefensedegradestheclean
accuracytolessthan50%. Ifthedatasetisbalanced,asRTEapproximatelyis8,aconstantclassifiercanachieve50%clean
accuracyand0%ASR.ThisfactshowsthelittlevalueoftheRTEdefendedmodelinJonesetal.(2020).
B.6.AttackofLLMclassifier
ThepromptdesigninattackingdifferentLLMsissummarizedinTableS23. AschematicofattackingLLMsispresentin
Fig.S6. Inthisexperiment,weusetoken-basedpositionselectiontofurtheracceleratetheprocessofattack. Specifically,
wemaskeachtokenintheinputsandselectthetoptentokenswiththehighestloss. Sincesometokensconsistofmany
characters, we only use 40 positions of these tokens to perform Algorithm 1. The remaining step is the same as in
Algorithm2. Notably,inTableS21,weseethatsuchaprocesscansignificantlyacceleratetheattackwhilemaintainingthe
7TheAG-NewsdatasetisnotstudiedinPruthietal.(2019);Jonesetal.(2020).
8https://huggingface.co/datasets/glue/viewer/rte/validation
25
)9102,.lateihturP(
)0202,.latesenoJ(Charmer
performanceofASRandothermetrics. TheresultonVicuna7BispresentinTableS22,wherewecanseetheproposed
CharmerachievesmuchhigherASRthanotherbaselineswithlesseditdistance.
Is the given review positive or negative?
it 's a càharming and often affecting journey .
Charmer
The answer is
Success
negative LLM
FigureS6.SchematicoftheproposedCharmerinattackingLLM-classifiers.CharmermodifiestheinputtoS′withasmallperturbation
(annotatedingreencolor)totheoriginalinputsothatthemodelproducesthedesiredoutputy.S andS areauxiliarypromptsthat
P1 P2
remainunchangedduringtheattack.
TableS21.AblationstudyonthetimeefficiencyofdifferentmethodsofpositionselectioninLlama2-Chat7B.Wechoosethefastversion
ofCharmerwithn=1andk=10.TheresultshowsthatcombingAlgorithm1withatoken-basedpre-selectionprocedurecannotably
improvetheefficiencyoftheproposedCharmer.
Method ASR(%) d (S,S′) Sim(S,S′) Time
lev
Charmer-Fast(Token-basedAlgorithm1) 95.47 2.55 0.83 1.47
Charmer-Fast(Algorithm1) 95.60 2.56 0.85 3.32
Charmer-Fast(Token-basedAlgorithm1) 93.51 2.40 0.93 5.66
Charmer-Fast(Algorithm1) 96.82 2.34 0.94 10.30
Charmer-Fast(Token-basedAlgorithm1) 97.10 1.68 0.82 2.06
Charmer-Fast(Algorithm1) 98.07 1.64 0.84 2.63
B.7.JailbreakingLLM
Disclaimer: thisattackcanleadtoharmfulcontent
Inthissection,weshowcasethattheproposedmethodcanalsobeappliedtojailbreakingLLMs,whichreferstodesigning
promptstoallowLLMstooutputharmfulcontent. WecompareagainsttherecentjailbreakingattacksGCG(Zouetal.,
2023)andAutoDAN(Liuetal.,2023)intheAdvBenchbanchmark(Zouetal.,2023). First,weoverviewtheformulationof
jailbreakingLLM.GivenaharmfulqueryS,thegoalofjailbreakingistomakethemodelanswercertainqueryS even
whenthesafetysystempromptS isprovided,seeFig.1foranillustrationofS . Formally,ourattackeraimsto
system system
findanadversarialexampleS′basedonS suchthat:
max L(f(S ⊕S′),y):= max P(y|S ⊕S′)
system system
S′∈Sk(S,Σ) S′∈Sk(S,Σ)
whereLisdefinedastheprobabilityofgeneratingthetargety :=“Sure,hereis”⊕S.Forexample,ifS is“howtomakea
bomb”,thenyis“Sure,hereishowtomakeabomb”.
In this experiment, we use the Advbench dataset proposed by Zou et al. (2023), which consists of harmful and toxic
sentences across different topics. Due to the costly computational budget of attacking LLMs, we only use the first 50
sentencesinthedataset. WecomparetheproposedCharmeragainstthewhite-boxattackGCG(Zouetal.,2023)and
black-boxattackAutoDAN(Liuetal.,2023). Weadoptthesamehyper-parametersofLLMandcriterionofsuccessasin
(Zouetal.,2023). TheresultinTableS24showsthatCharmercanpassthesafetyalignmentprocessofLLMswithmuch
lesschangeintermsofLevenshteindistance.
26
2-TSS
ILNQ
ETRCharmer
TableS22.AttackevaluationinVicuna7B:WechoosethefastversionofCharmerwithn=1andk=10.Charmeroutperforms
baselinesintermsofattacksuccessrate,Levenshteindistance,andachievescomparativesimilarityandspeed.
Method ASR(%) d (S,S′) Sim(S,S′) Time
lev
BAE-R● 40.66 12.36 0.96 3.11
BERT-attack● 56.67 16.61 0.91 4.03
DeepWordBug● 43.77 3.63 0.91 1.32
TextBugger● 53.11 9.08 0.93 2.56
TextFooler● 51.28 20.70 0.92 4.76
Charmer-Fast● 98.35 2.04 0.94 4.89
BAE-R● 64.11 5.96 0.89 1.23
BERT-attack● 82.78 8.92 0.82 1.60
DeepWordBug● 50.97 2.67 0.76 0.61
TextBugger● 71.77 6.63 0.84 1.12
TextFooler● 78.47 7.94 0.86 1.64
Charmer-Fast● 89.05 1.56 0.85 1.98
BAE-R● 43.22 14.29 0.75 3.28
BERT-attack● 32.31 13.21 0.85 2.42
DeepWordBug● 30.72 4.22 0.76 0.88
TextBugger● 23.01 9.97 0.88 1.73
TextFooler● 64.04 18.03 0.91 4.05
Charmer-Fast● 91.89 2.47 0.85 1.66
TableS23.PromptingindifferentLLMsanddatasets:Thesentencesoutside“[Input]”areconsideredasauxiliarypromptsS and
P1
S ,asdemonstratedinFig.S6.
P2
Model Dataset Promptdesign
Llama2-Chat7B SST-2 Isthegivenreviewpositiveornegative? [Input]Theansweris
RTE [Input premise] Based on the paragraph above can we conclude the
following sentence, answer with yes or no. [Input hypothesis] The
answeris
QNLI Doesthesentenceanswerthequestion?Answerwithyesorno.Question:
[Inputpremise]Sentence: [Inputhypothesis]Theansweris
Vicuna7B SST-2 Analyzethetoneofthisstatementandrespondwitheitherpositiveor
negative: [Input]Theansweris:
RTE [Input premise] Based on the paragraph above can we conclude the
following sentence, answer with yes or no. [Input hypothesis] The
answeris
QNLI [Inputpremise]Basedonthequestionabove,doesthefollowingsentence
answerthequestion? [Inputhypothesis]Answerwithyesorno. The
answeris
C.ProofofCorollary3.9
Inthissection,weprovidethetechnicalproofofCorollary3.9.
Proof. Startingwiththeupperbound,inthebasecase,wehave|S | = |{S}| = 1. Then,wewillprovetherelationship
0
9InVicuna7BandGuanaco7B,AutoDANusestheinitializedhandcraftedprefixinjailbreakssuccessfullysothatthetimeis0.Our
methodcanalsoworkontopofthesehandcraftedprefixes.
27
ILNQ
ETR
2-TSSCharmer
TableS24. Attackevaluationinjailbreakingdifferentlargelanguagemodels.
Model Method ASR(%) d (S,S′) Time
lev
Vicuna7B GCG 100.00 35.26 56.32
AutoDAN 100.00 3677 -9
Charmer 100.00 3.44 17.41
Guanaco7B GCG 100.00 55.34 30.06
AutoDAN 100.00 3677.00 -9
Charmer 100.00 4.98 24.99
Llama2-Chat7B GCG 86.00 76.74 948.44
AutoDAN 18.00 3209.33 29.01
Charmer 94.00 28.02 341.66
between|S |and|S |. ForacertainS′ ∈S ,wehave:
k k−1 k−1
(cid:12) (cid:12)
(cid:12) (cid:12)
|S k(S,Γ)|=(cid:12) (cid:12) (cid:91) {S′′ :d lev(S′,S′′)≤1}(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)S′∈Sk−1 (cid:12)
(cid:88)
≤ |{S′′ :d (S′,S′′)≤1}|
lev
S′∈Sk−1
[Proposition3.6]= (cid:88) (cid:12) (cid:12)(cid:110) ψ(cid:16) ϕ(S′)←i c(cid:17) ∀i∈[2|S′|+1],∀c∈Γ∪{ξ}(cid:111)(cid:12) (cid:12)
(cid:12) (cid:12)
S′∈Sk−1
(cid:88)
[#combinationsofi’sandc’s]≤ (2|S′|+1)·(|Γ|+1)
S′∈Sk−1
(cid:88)
[|S′|≤|S|+k ∀S′ ∈S ]≤ (2(|S|+k)−1)·(|Γ|+1)
k
S′∈Sk−1
=(2(|S|+k)−1)·(|Γ|+1)·|S | .
k−1
Finally,byinductionwehave
k
(cid:89)
|S (S,Γ)|≤(|Γ|+1)k· (2(|S|+k)−1).
k
j=1
Forthelowerbound,itisenoughtocomputethesizeofthesetofstringsobtainedbyaddingjustprefixes:
|S (S,Γ)|≥|{ψ(P ⊕ϕ(S)),∀P ∈{P′ ∈S(Γ∪{ξ}), |P′|≤k}}|
k
=|{ψ(P),∀P ∈{P′ ∈S(Γ∪{ξ}), |P|≤k}}|
(cid:12) (cid:12)
(cid:12)(cid:91)k (cid:12)
=(cid:12) {P′ ∈S(Γ), |P′|=i}(cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
i=0
k
(cid:88)
[Disjointsets]= |{P′ ∈S(Γ), |P′|=i}|
i=0
k
(cid:88)
= |Γ|i
i=0
(cid:40)
1−|Γ|k+1
if|Γ|>1
[Geometricseries]= 1−|Γ|
k+1 if|Γ|=1
28Charmer
D.Alternativeattackdesigns
Inthissection,wecoveralternativealgorithmicdesignscalledPGA-CharmerforsolvingEq.(1)Specifically,westudy
relaxingthebinaryconstraintsinEq.(BP)inordertoperformaProjectedGradientAscent(PGA)procedure.
D.1.PGA-Charmer
LetE(i) =Token(S(i))T ∈Rli×d betheembeddingsoftokensforanysentenceS(i) ∈S′ ⊆S(Γ)withi∈[|S′|]. The
zero-paddedembeddingsforthesentencesinthesetS′become:
Eˆ(i) =E(i)⊕0 ∈Rl×d, ∀i∈[|S′|] ,
(l−li)×d
wherel=max{l :i∈[|S′|]}and⊕istheconcatenationoperatoralongthefirstdimension.
i
RemarkS1(Modeloutputafterzeropadding). Givenafunctionf,theoutputbeforeandafterzeropaddingisunchanged,
i.e.,f(Eˆ(i))=f(E(i)) ∀E(i) ∈S′.
WecanreformulatetheprobleminEq.(1)as:
(cid:16) (cid:16) (cid:17) (cid:17)
max L f (cid:80)|Sk(S,Γ)|u ·Eˆ(i) ,y
u∈R|Sk(S,Γ)| i=1 i , (BP)
s.t. u ∈{0,1} ∀i∈[|S (S,Γ)|], ||u|| =1
i k 1
which is a constrained binary optimization problem. Note that given uBP a maximizer of Eq. (BP) with iBP :=
argmax uBP,wehavethatthesentenceS(iBP) ∈S (S,Γ)isamaximizerofEq.(1).
i∈[|Sk(S,Γ)|] i k
However, solvingEq.(BP)isashardassolvingEq.(1)becauseoftheexponentialsizeofS (S,Γ), seeCorollary3.9.
k
Alternatively,wecanrelaxthebinaryconstraintsfromtheuvectorandsolve:
(cid:16) (cid:16) (cid:17) (cid:17)
max L f (cid:80)|Sk|u ·Eˆ(i) ,y
u∈R|Sk(S,Γ)| i=1 i (SP)
s.t. u ∈[0,1] ∀i∈[|S (S,Γ)|], ||u|| =1.
i k 1
Inthiscase,givenuSPamaximizerofEq.(SP),weknow:
L(cid:16) f(cid:16)(cid:80)|Sk(S,Γ)|uSP·Eˆ(i)(cid:17) ,y(cid:17) ≥L(cid:16) f(cid:16)(cid:80)|Sk(S,Γ)|uBP·Eˆ(i)(cid:17) ,y(cid:17)
.
i=1 i i=1 i
Notethattheembeddings(cid:80)|Sk(S,Γ)|uSP·Eˆ(i)
havenocorrespondencetoanysentenceinS (S,Γ). However,wecan
i=1 i k
stilltakeiSP =argmax uSPandhopefullyS(iSP) ∈S (S,Γ)isanadversarialexample. TosolveEq.(SP),we
i∈[|Sk(S,Γ)|] i k
employprojectiongradientascentwithstep-sizeηasfollows:
ut+1 =Π (ut+η∇L (ut)),
∆ u
where Π (·) is the projection function. Let us denote by uˆ := ut +η∇L (ut) for notation simplification, then the
∆ u
projectionstepessentiallyaimstosolvethefollowingquadraticprogrammingproblem:
1
ut+1 =argmin ||u−uˆ||2,
2 2
u (QP)
(cid:88)
subjectto: u =1, u ≥0.
i i
TheLagrangianassociatedwithEq.(QP)isasfollows:
1
L(u,λ,v):= ||u−uˆ||2+λ(u⊤1−1)−v⊤uˆ,
2 2
whereλ∈R,v ∈R|Sall|aretheLagrangemultipliers. TheKarush-Kuhn-Tuckeroptimalityconditionsarenecessaryand
29Charmer
TableS25.ComparisonbetweenourPGA-Charmerandquery-basedCharmer(proposedinthemainbody)inBERT:Thebest
methodishighlightedinbold.WhilethePGA-Charmerstrategycannoticeablyimprovetheruntime,theASR,Levenshtinedistance
andsimilarityareconsiderablydegraded.
Method ASR(%)↑ d (S,S′) ↓ Sim(S,S′) ↑ Time(s) ↓
lev
PGA-Charmer 86.94 7.33 0.87 8.15
AG-News ±(5.01) ±(0.11) ±(7.04)
Charmer 98.51 3.68 0.95 8.74
±(3.08) ±(0.06) ±(11.10)
PGA-Charmer 99.05 2.11 0.79 0.85
MNLI-m ±(1.53) ±(0.19) ±(0.72)
Charmer 100.00 1.14 0.85 1.45
±(0.42) ±(0.13) ±(0.81)
PGA-Charmer 81.19 3.46 0.89 5.15
QNLI ±(2.32) ±(0.12) ±(4.60)
Charmer 97.68 1.94 0.94 9.19
±(1.48) ±(0.07) ±(9.60)
PGA-Charmer 72.64 1.53 0.86 0.65
RTE ±(1.45) ±(0.11) ±(0.71)
Charmer 97.01 1.55 0.86 2.50
±(1.42) ±(0.13) ±(2.33)
PGA-Charmer 97.52 2.68 0.84 1.09
SST-2 ±(1.82) ±(0.16) ±(0.89)
Charmer 100.00 1.47 0.90 1.27
±(0.74) ±(0.11) ±(0.84)
sufficientforsolvingEq.(QP),thatis:
∇ L(u,λ,v)=u−uˆ+λ1−v =0, (3)
u
u ≥0, (4)
i
(cid:88)
u −1=0, (5)
i
v ≥0, (6)
i
v u =0. (7)
i i
Clearly,givenanyλ,ifwesetu =max(uˆ −λ,0), v =max(λ−uˆ ,0),thenEqs.(3),(4),(6)and(7)canbesatisfied.
i i i i
Therefore,theremainingproblemreducestofindaλthatsatisfiesEq.(5),i.e.,
(cid:88) (cid:88)
u −1= max(uˆ −λ,0)−1=0.
i i
WeemploythealgorithmproposedinHeldetal.(1974)tosolveit, aspresentedinAlgorithm3. Lastly, weselectthe
argmax (u⋆)elementinS astheattacksentenceS′.
j j all
Algorithm3Projectionintosimplex(Heldetal.,1974)
Input: uˆ:=ut+η∇L u(ut)∈R|Sall|.
Sortuˆsuchthatuˆ ≤uˆ ≤···≤uˆ .
1 2 |Sall|
SetJ :=max(J :
−1+(cid:80)| iS =a Jll| +1uˆi
>uˆ ).
0 |Sall|−J J
Calculateλ=
−1+(cid:80) i|S =a Jll 0| +1uˆi.
|Sall|−J
Setut+1 =max(uˆ −λ,0).
i i
Output: ut+1
D.2.ComparisonbetweenPGA-Charmerandquery-basedCharmer
Inthissection,weexperimentallyvalidatetheefficiencyofPGA-Charmerandcompareitagainstquery-basedCharmer,
whichisproposedinthemainbody. TheresultinTableS25showsthatPGA-Charmercanefficientlyreducetheruntime
asitdoesnotrequiretheforwardpassoveramini-batchofsentencesafterpositionselection. However,PGA-Charmer
isworsethanCharmeronothermetrics,e.g.,ASR,Levenshtinedistanceandsimilarityaredegraded. Webelievethat
combiningtheefficiencyofPGA-CharmerandhighASRin-Charmerholdspromiseforfutureresearchendeavors.
30