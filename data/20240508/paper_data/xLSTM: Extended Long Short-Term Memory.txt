xLSTM: Extended Long Short-Term Memory
MaximilianBeck∗1,2 KorbinianPöppel∗1,2 MarkusSpanring1
AndreasAuer1,2 OleksandraPrudnikova1 MichaelKopp
GünterKlambauer1,2 JohannesBrandstetter1,2,3 SeppHochreiter1,2,3
∗Equalcontribution
1ELLISUnit,LITAILab,InstituteforMachineLearning,JKULinz,Austria
2NXAILab,Linz,Austria, 3NXAIGmbH,Linz,Austria
Abstract
Inthe1990s,theconstanterrorcarouselandgatingwereintroducedasthecentral
ideasoftheLongShort-TermMemory(LSTM).Sincethen,LSTMshavestood
the test of time and contributed to numerous deep learning success stories, in
particular they constituted the first Large Language Models (LLMs). However,
theadventoftheTransformertechnologywithparallelizableself-attentionatits
coremarkedthedawnofanewera,outpacingLSTMsatscale. Wenowraisea
simplequestion: HowfardowegetinlanguagemodelingwhenscalingLSTMsto
billionsofparameters,leveragingthelatesttechniquesfrommodernLLMs,but
mitigatingknownlimitationsofLSTMs? Firstly,weintroduceexponentialgating
withappropriatenormalizationandstabilizationtechniques. Secondly,wemodify
theLSTMmemorystructure,obtaining: (i)sLSTMwithascalarmemory,ascalar
update,andnewmemorymixing,(ii)mLSTMthatisfullyparallelizablewitha
matrixmemoryandacovarianceupdaterule. IntegratingtheseLSTMextensions
intoresidualblockbackbonesyieldsxLSTMblocksthatarethenresiduallystacked
intoxLSTMarchitectures. Exponentialgatingandmodifiedmemorystructures
boostxLSTMcapabilitiestoperformfavorablywhencomparedtostate-of-the-art
TransformersandStateSpaceModels,bothinperformanceandscaling.
LSTM Memory Cells xLSTM Blocks xLSTM
Memory Cells sLSTM
🠆 Constant Error Carousel + Exponential Gating
🠆 Sigmoid Gating + New Memory Mixing
🠆 Recurrent Inference
🠆 Recurrent Training
mLSTM
+ Exponential Gating
+ Matrix Memory
+ Parallel Training
+ Covariance Update Rule
Figure1: TheextendedLSTM(xLSTM)family. Fromlefttoright: 1. TheoriginalLSTMmemory
cellwithconstanterrorcarouselandgating.2.NewsLSTMandmLSTMmemorycellsthatintroduce
exponentialgating. sLSTMoffersanewmemorymixingtechnique. mLSTMisfullyparallelizable
withanovelmatrixmemorycellstateandnewcovarianceupdaterule. 3. mLSTMandsLSTMin
residualblocksyieldxLSTMblocks. 4. StackedxLSTMblocksgiveanxLSTMarchitecture.
4202
yaM
7
]GL.sc[
1v71540.5042:viXra1 Introduction
The Long Short-Term Memory (LSTM) ideas (Hochreiter, 1991; Hochreiter & Schmidhuber,
1997b,a), i.e., theconstanterrorcarouselandgating, wereintroducedtoovercomethevanishing
gradientproblemofrecurrentneuralnetworks(Hochreiter,1991;Hochreiteretal.,2000):
c = f c + i z , h = o ψ(c ). (1)
t t t−1 t t t t t
Theconstanterrorcarouselistheadditiveupdateofthecellstatec (green)bycellinputsz and
t−1 t
moderatedbysigmoidgates(blue). Theinputgatei andtheforgetgatef controlthisupdate,while
t t
theoutputgateo controlstheoutputofthememorycell,i.e.thehiddenstateh . Thecellstateis
t t
normalizedorsquashedbyψandthenoutputgatinggivesthehiddenstate.
LSTMshavebeensuccessfullyappliedtovariousdomains(Hochreiteretal.,2001,2007;Schmid-
huber,2015),andprevailedovertextgenerationuntilthedawnofTransformersin2017(Vaswani
etal.,2017). TheeffectivenessofLSTMshasbeendemonstratedatnumeroussequence-relatedtasks
suchasgeneratingtext(Graves,2013;Karpathy,2015),generatinghandwritings(Graves,2013),
sequence-to-sequencetranslation(Sutskeveretal.,2014),evaluatingcomputerprograms(Zaremba
&Sutskever,2014),generatingimagecaptions(Karpathy&Fei-Fei,2015;Hossainetal.,2019),
generating source code (Karpathy, 2015), rainfall-runoff modeling (Kratzert et al., 2018, 2019),
or hydrological models for flooding warnings (Nearing et al., 2024). In reinforcement learning,
LSTMsarethebestperformingsequencemodels,e.g.,theAlphaStarmodelforStarCraftII(Vinyals
et al., 2017), the OpenAI Five model for Dota 2 (Karpathy, 2019), and models of the magnetic
controllerfornuclearfusion(Degraveetal.,2022). LSTMsexcelatlearningabstractions,i.e.,adeptly
extractingsemanticinformationandstoringitintheirmemorycells(Karpathy,2015),whichfor
examplebecameevidentbynumberandsyntaxneurons(Lakretzetal.,2019),linguisticneurons(Bau
etal.,2019),andsentimentneurons(Radfordetal.,2017). LSTMsarestillusedinhighlyrelevant
applications(Degraveetal.,2022;Nearingetal.,2024)andhavestoodthetestoftime.
Despite their tremendous successes,
LSTMs have three main limitations:
(i) Inability to revise storage deci-
sions. We exemplify this limitation
viatheNearestNeighborSearchprob-
lem(seealsoAppendixB):Witharef-
erencevectorgiven,asequencemust
bescannedsequentiallyforthemost
similarvectorinordertoprovideits
attachedvalueatsequenceend. The
leftpanelofFigure2showsthemean
squarederroratthistask.LSTMstrug- Figure2: LSTMlimitations. Left: NearestNeighborSearch
gles to revise a stored value when a problem in terms of mean squared error (MSE). Given a
more similar vector is found, while referencevector,asequenceisscannedsequentiallyforthe
ournewxLSTMremediatesthislimi- mostsimilarvectorwiththeobjectivetoreturnitsattached
tationbyexponentialgating. (ii)Lim- value at sequence end. LSTM struggles to revise a stored
ited storage capacities, i.e., informa- valuewhenamoresimilarvectorisfound. OurnewxLSTM
tion must be compressed into scalar overcomesthislimitationbyexponentialgating. Right:Rare
cellstates. Weexemplifythislimita- Token Prediction. The perplexity (PPL) of token predic-
tionviaRareTokenPrediction. Inthe tiononWikitext-103,inbucketsoftokenfrequency. LSTM
right panel of Figure 2, the perplex- performsworseonpredictingraretokensbecauseofitslim-
ity of token prediction on Wikitext- itedstoragecapacities,whereasournewxLSTMsolvesthis
103(Merityetal.,2017)isgivenfor problemviaamatrixmemory.
bucketsofdifferenttokenfrequency.
LSTMperformsworseonraretokensbecauseofitslimitedstoragecapacities. OurnewxLSTM
solvesthisproblembyamatrixmemory. (iii)Lackofparallelizabilityduetomemorymixing,i.e.,
thehidden-hiddenconnectionsbetweenhiddenstatesfromonetimesteptothenext,whichenforce
sequentialprocessing.
TheselimitationsofLSTMhavepavedthewayfortheemergenceofTransformers(Vaswanietal.,
2017) in language modeling. What performances can we achieve in language modeling when
overcomingtheselimitationsandscalingLSTMstothesizeofcurrentLargeLanguageModels?
22 ExtendedLongShort-TermMemory
ToovercometheLSTMlimitations,ExtendedLongShort-TermMemory(xLSTM)introducestwo
mainmodificationstotheLSTMideaofEquation(1). Thosemodifications–exponentialgating
andnovelmemorystructures–enrichtheLSTMfamilybytwomembers: (i)thenewsLSTM(see
Section2.2)withascalarmemory,ascalarupdate,andmemorymixing,and(ii)thenewmLSTM
(seeSection2.3)withamatrixmemoryandacovariance(outerproduct)updaterule,whichisfully
parallelizable. BothsLSTMandmLSTMenhancetheLSTMthroughexponentialgating. Toenable
parallelization,themLSTMabandonsmemorymixing,i.e.,thehidden-hiddenrecurrentconnections.
BothmLSTMandsLSTMcanbeextendedtomultiplememorycells,wheresLSTMfeaturesmemory
mixingacrosscells. Further,thesLSTMcanhavemultipleheadswithoutmemorymixingacrossthe
heads,butonlymemorymixingacrosscellswithineachhead. ThisintroductionofheadsforsLSTM
togetherwithexponentialgatingestablishesanewwayofmemorymixing. FormLSTMmultiple
headsandmultiplecellsareequivalent.
Integrating these new LSTM variants into residual block modules results in xLSTM blocks (see
Section2.4). ResiduallystackingthosexLSTMblocksinarchitecturesprovidesxLSTMarchitectures
(seeSection2.4). SeeFigure1forthexLSTMarchitecturewithitscomponents.
2.1 ReviewoftheLongShort-TermMemory
TheoriginalLSTMidea(Hochreiter,1991;Hochreiter&Schmidhuber,1997b,a)introducedthe
scalarmemorycellasacentralprocessingandstorageunitthatavoidsvanishinggradients(Hochreiter,
1991;Hochreiteretal.,2000)throughtheconstanterrorcarousel(thecellstateupdate). Thememory
cellcontainsthreegates: input,output,andforgetgate. TheforgetgatehasbeenintroducedbyGers
etal.(2000). TheLSTMmemorycellupdaterulesattimesteptare:
c = f c + i z cellstate (2)
t t t−1 t t
(cid:16) (cid:17)
h = o h˜ , h˜ = ψ c hiddenstate (3)
t t t t t
z = φ(z˜) , z˜ = w⊤x + r h + b cellinput (4)
t t t z t z t−1 z
i = σ(cid:0)˜i (cid:1) , ˜i = w⊤x + r h + b inputgate (5)
t t t i t i t−1 i
(cid:16) (cid:17)
f = σ ˜f , ˜f = w⊤x + r h + b forgetgate (6)
t t t f t f t−1 f
o = σ(˜o ) , ˜o = w⊤x + r h + b outputgate (7)
t t t o t o t−1 o
Theweightvectorsw ,w,w ,andw correspondtotheinputweightvectorsbetweeninputsx
z i f o t
andcellinput,inputgate,forgetgate,andoutputgate,respectively. Theweightsr ,r,r ,andr
z i f o
correspondtotherecurrentweightsbetweenhiddenstateh andcellinput,inputgate,forgetgate,
t−1
andoutputgate,respectively. b ,b,b ,andb arethecorrespondingbiasterms. φandψ arethe
z i f o
cellinputandhiddenstateactivationfunctions(typicallytanh). ψisusedtonormalizeorsquash
thecellstate,whichwouldbeunboundedotherwise. Allgateactivationfunctionsaresigmoid,i.e.,
σ(x)=1/(1+exp( x)). Inlaterformulations,multiplememorycellswerecombinedinavector,
−
whichallowstheusageofrecurrentweightmatricestomixthecelloutputsofmemorycells(Greff
etal.,2015),formoredetailsseeAppendixA.1. Ablationstudiesshowedthatallcomponentsofthe
memorycellarecrucial(Greffetal.,2015).
2.2 sLSTM
To empower LSTMs with the ability to revise storage decisions, we introduce exponential gates
(red)togetherwithnormalizationandstabilization. Inparticular,inputandforgetgatescanhave
exponentialactivationfunctions. Fornormalization,weintroduceanormalizerstatethatsumsupthe
productofinputgatetimesallfutureforgetgates.
3ThesLSTMforwardpassis:
c = f c + i z cellstate (8)
t t t−1 t t
n = f n + i normalizerstate (9)
t t t−1 t
h = o h˜ , h˜ = c /n hiddenstate (10)
t t t t t t
z = φ(z˜) , z˜ = w⊤x + r h + b cellinput (11)
t t t z t z t−1 z
i = exp (cid:0)˜i (cid:1) , ˜i = w⊤x + r h + b inputgate (12)
t t t i t i t−1 i
(cid:16) (cid:17) (cid:16) (cid:17)
f = σ ˜f OR exp ˜f , ˜f = w⊤x + r h + b forgetgate (13)
t t t t f t f t−1 f
o = σ(˜o ) , ˜o = w⊤x + r h + b outputgate (14)
t t t o t o t−1 o
WebroadcasttheoriginalLSTMgatingtechniques,i.e.,input-and/orhidden-dependentgatingplus
biasterm,tothenewarchitectures.Exponentialactivationfunctionscanleadtolargevaluesthatcause
overflows. Therefore,westabilizegateswithanadditionalstate m (Milakov&Gimelshein,2018):
t
(cid:16) (cid:17)
m = max log(f )+ m ,log(i ) stabilizerstate (15)
t t t−1 t
(cid:16) (cid:17) (cid:16) (cid:17)
i′ = exp log(i ) m = exp ˜i m stabil. inputgate (16)
t t − t t − t
(cid:16) (cid:17)
f′ = exp log(f )+ m m stabil. forgetgate (17)
t t t−1 − t
WeshowinAppendixA.2,thatreplacingf byf′andi byi′ intheforwardpassdoesneitherchange
t t t t
theoutputofthewholenetworknorthederivativesofthelosswithrespecttotheparameters.
New Memory Mixing. sLSTM can have multiple memory cells like the original LSTM (see
AppendixA.2). MultiplememorycellsenablememorymixingviarecurrentconnectionsR ,R ,
z i
R ,R fromhiddenstatevectorhtomemorycellinputzandthegatesi,f,o,respectively. Anew
f o
aspectinmemorymixingistheeffectofexponentialgating. ThenewsLSTMcanhavemultiple
headswithmemorymixingwithineachheadbutnotacrossheads. Theintroductionofheadsfor
sLSTMtogetherwithexponentialgatingestablishesanewwayofmemorymixing.
2.3 mLSTM
ToenhancestoragecapacitiesofLSTMs,weincreasetheLSTMmemorycellfromascalarc Rto
amatrixC Rd×d. Hence,retrievalisperformedviaamatrixmultiplication. Attimet,wew∈ antto
storeapairo∈ fvectors,thekeyk Rdandthevaluev Rd(weusetheTransformerterminology).
t t
Laterattimet+τ,thevaluev sh∈ ouldberetrievedbya∈ queryvectorq Rd. Thisisthesetting
t t+τ
∈
ofBidirectionalAssociativeMemories(BAMs)(Kohonen,1972;Anderson,1972;Nakano,1972;
Andersonetal.,1977). Thecovarianceupdaterule(Sejnowski,1977;Dayan&Willshaw,1991)for
storingakey-valuepairis
C = C + v k⊤. (18)
t t−1 t t
Weassumealayer-normbeforeprojectinginputstokeysandvalues,thereforetheyhavezeromean.
The covariance update rule is optimal (Dayan & Willshaw, 1991) for a maximal separability of
retrievedbinaryvectors,whichisequivalenttoamaximalsignal/noiseratio. Higherseparabilityis
possiblewhenlimitingretrievaltopairwiseinteractionsandconcedingquadraticcomplexitylike
attention(Krotov&Hopfield,2016,2017;Ramsaueretal.,2021). Thecovarianceupdateruleis
equivalenttoFastWeightProgrammers(Schmidhuber,1992;Schlagetal.,2021),whichhavelater
beenequippedwithaconstantdecayratemultipliedtoC andaconstantlearningratemultiplied
t−1
to v k⊤ (Baetal., 2016a). Inthisspirit, weintegratethe covariance updateruleintotheLSTM
t t
framework,wheretheforgetgatecorrespondstodecayrateandtheinputgatetothelearningrate,
whiletheoutputgatescalestheretrievedvector.
Forthismatrixmemory,thenormalizerstateistheweightedsumofkeyvectors,whereeachkey
vectorisweightedbytheinputgateandallfutureforgetgates. Again,thenormalizerstatekeeps
4recordofthestrengthofthegates. Sincethedotproductbetweenqueryandnormalizerstatecan
beclosetozero, weusetheabsolutevalueofthisdotproductandlowerbounditbyathreshold
(typically1.0)asdonepreviously(Sunetal.,2023). ThemLSTMforwardpassis:
C = f C + i v k⊤ cellstate (19)
t t t−1 t t t
n = f n + i k normalizerstate (20)
t t t−1 t t
(cid:110)(cid:12) (cid:12) (cid:111)
h = o h˜ , h˜ = C q / max (cid:12)n⊤ q (cid:12),1 hiddenstate (21)
t t ⊙ t t t t (cid:12) t t (cid:12)
q = W x + b queryinput (22)
t q t q
1
k = W x + b keyinput (23)
t k t k
√d
v = W x + b valueinput (24)
t v t v
i = exp(cid:0)˜i (cid:1) , ˜i = w⊤x + b inputgate (25)
t t t i t i
(cid:16) (cid:17) (cid:16) (cid:17)
f = σ ˜f OR exp ˜f , ˜f = w⊤x + b forgetgate (26)
t t t t f t f
o = σ(o˜ ) , o˜ = W x + b outputgate (27)
t t t o t o
mLSTMcanhavemultiplememorycellsliketheoriginalLSTM.FormLSTM,multipleheadsand
multiplecellsareequivalentasthereisnomemorymixing. Inordertostabilizetheexponentialgates
ofmLSTM,weusethesamestabilizationtechniquesasforsLSTM,seeEquation(15). Sincethe
mLSTMhasnomemorymixing,thisrecurrencecanbereformulatedinaparallelversion. Formore
detailswerefertoAppendixA.3.
2.4 xLSTMArchitecture
xLSTMBlocks. AnxLSTMblock
should non-linearly summarize the
past in a high-dimensional space to
better separate different histories or
contexts. Separating histories is the
prerequisite to correctly predict the
next sequence element such as the
next token. We resort to Cover’s
Theorem(Cover,1965),whichstates
that in a higher dimensional space
non-linearly embedded patterns can
more likely be linearly separated
than in the original space. We con-
Figure 3: xLSTM blocks. Left: A residual sLSTM block
sidertworesidualblockarchitectures:
withpostup-projection(likeTransformers): Theinputisfed
(i) A residual block with post up-
intoansLSTM–withanoptionalconvolution–followed
projection(likeTransformers),which
by a gated MLP. Right: A residual mLSTM block with
non-linearly summarizes the past in
pre up-projection (like State Space models): mLSTM is
theoriginalspace,thenlinearlymaps
wrapped inside two MLPs, via a convolution, a learnable
intoahigh-dimensionalspace,applies
skip connection, and an output gate that acts component-
anon-linearactivationfunction,and
wise. SeeFigure9andFigure10intheappendixfordetails.
linearly maps back to the original
space; see left panel of Figure 3 and third column in Figure 1. A more detailed version is de-
pictedinFigure9intheappendix. (ii)Aresidualblockwithpreup-projection(likeStateSpace
Models),whichlinearlymapstoahigh-dimensionalspace,non-linearlysummarizesthepastinthe
high-dimensionalspaceandthenlinearlymapsbacktotheoriginalspace. ForanxLSTMblock
containingansLSTM,wemostlyusethepostup-projectionblock. ForanxLSTMblockcontaining
anmLSTM,weusethepreup-projectionblocksincethememorycapacitybecomeslargerinthe
high-dimensionalspace. SeeleftpanelofFigure3andthirdcolumninFigure1,orFigure9inthe
appendixformoredetails.
5xLSTM Architecture. An xLSTM architecture is constructed by residually stacking build-
ing blocks (Srivastava et al., 2015; He et al., 2016). We rely on the most commonly used pre-
LayerNorm(Baetal.,2016b)residualbackbonesasusedincontemporaryLargeLanguageModels.
SeelastcolumninFigure1.
2.5 MemoryandSpeedConsiderations
Contrary to Transformers, xLSTM networks have a linear computation and a constant memory
complexitywithrespecttothesequencelength. SincethexLSTMmemoryiscompressive,itiswell
suitedforindustrialapplicationsandimplementationsontheedge.
ThememoryofmLSTMdoesnotrequireparametersbutiscomputationallyexpensivethroughitsd d
×
matrixmemoryandd dupdate. Wetradeoffmemorycapacityagainstcomputationalcomplexity.
×
Nevertheless,thecomputationscanbedoneinparallelonGPUs,thereforethesecomputationshave
onlyaminoreffectonthewallclocktime.
WhilemLSTMisparallelizableanalogtoFlashAttention(Daoetal.,2022;Dao,2024)orGLA(Yang
etal.,2023),sLSTMisnotparallelizableduetothememorymixing(hidden-hiddenconnections).
However,wedevelopedafastCUDAimplementationwithGPUmemoryoptimizationstotheregister
levelwhichistypicallylessthantwotimesslowerthanmLSTM.
3 RelatedWork
LinearAttention. Severalmethodshavebeensuggestedtoovercomethequadraticcomplexity
intermsofcontextlengthoftheTransformerandmakeattentionlinearinthecontextlength. The
Synthesizerlearnssyntheticattentionweightswithouttoken-tokeninteractions(Tayetal.,2020).
Linformer realizes self-attention by a low-rank matrix and even linearly approximates it (Wang
etal.,2020). LinearTransformerlinearizestheattentionmechanism(Katharopoulosetal.,2020).
Performer linearly approximates the attention softmax by positive orthogonal random features
approach(Choromanskietal.,2021). Attentionhasbeenreplacedbyfastlongconvolutionsinthe
StructuredGlobalConvolution(SGConv)(Lietal.,2022)andtheHyenaHierarchy(Polietal.,2023).
StateSpaceModels. Recently,StateSpaceModels(SSMs)becameverypopularsincetheyare
linearinthecontextlengthandshowpromisingperformancecomparedtoTransformers. Oneofthe
firstproposedmodelswasStructuredStateSpacesequencemodel(S4)(Guetal.,2021),followedby
DiagonalStateSpace(DSS)model(Guptaetal.,2022),GatedStateSpace(GSS)models(Mehta
etal.,2022),S5model(Smithetal.,2022),BidirectionalGatedSSM(BiGS)(Wangetal.,2022),H3
model(Fuetal.,2023),andMamba(Gu&Dao,2023).
RecurrentNeuralNetworks. RecurrentNeuralNetworks(RNNs)havebeensuggestedtoreplace
Transformer and attention due to their linearity in the context length. RNNs with Deep Linear
RecurrentUnits(LRUs)showedpromisingresultsforlanguagemodeling(Orvietoetal.,2023;De
etal.,2024),asdidHierarchicallyGatedLinearRNN(HGRN)(Qinetal.,2023)andHGRN2(Qin
etal.,2024). Awell-knownRNNapproachtolargelanguagemodelingisRWKV(Pengetal.,2023,
2024),showcasingcompetitiveperformancetoTransformers.
Gating. OneofthekeyideasofLSTMisgating,whichwasrediscoveredandreinterpretedinmany
recentapproaches. GatingwasusedinHGRN(Qinetal.,2023),HGRN2(Qinetal.,2024),Gated
LinearAttention(GLA)(Yangetal.,2023),GatedStateSpace(GSS)models(Mehtaetal.,2022),
BidirectionalGatedSSM(BiGS)(Wangetal.,2022),MovingAverageEquippedGatedAttention
(MEGA)(Maetal.,2022),RWKV(Pengetal.,2023),andMamba(Gu&Dao,2023).
Covariance Update Rule. To enhance storage capacities, we equipped the mLSTM cell with
a matrix memory with a covariance update rule. Other methods which build on such an update
mechanismareFastWeightProgrammers(Schmidhuber,1992;Schlagetal.,2021),RWKV-5and
RWKV-6(Pengetal.,2024),Retention(Sunetal.,2023),LinearTransformer(Katharopoulosetal.,
2020),andHGRN2(Qinetal.,2024).
6Most Related. Conceptually the closest models to xLSTM are Retention (Sun et al., 2023),
RWKV (Peng et al., 2023, 2024), and HGRN2 (Qin et al., 2024). These models share the con-
ceptsmatrixmemoryand/orgating. However,incontrasttothenewsLSTM,theseapproachesdo
notallowmemorymixing. Memorymixingenablestosolvestatetrackingproblems,andtherefore
LSTMsaremoreexpressivethanStateSpaceModels(SSMs)andTransformers(Merrilletal.,2024;
Delétangetal.,2023).Statetrackingisrequiredtoevaluatecodeortotrackentitiesinalongnarrative.
ResiduallyStackingArchitectures. Likealmostallcontemporarylargedeeplearningmodels,
xLSTMarchitecturesareconstructedbyresiduallystackingbuildingblocks(Srivastavaetal.,2015;
He et al., 2016). This construction enabled deep convolutional networks (He et al., 2016) and
Transformers(Vaswanietal.,2017). TransformersaretheultimateforcebehindLargeLanguage
Models(LLMs)likeGPT-3(Brownetal.,2020),ChatGPT(Schulmanetal.,2022),GPT-4(Achiam
etal.,2023),Megatron-LM(Shoeybietal.,2019),Gopher(Raeetal.,2021),ERNIE3.0Titan(Wang
et al., 2021), GLaM (Du et al., 2021), Chinese M6 (Lin et al., 2021), mutilingual AlexaTM
20B(Soltanetal.,2022),OPT(Zhangetal.,2022),Chinchilla(Hoffmannetal.,2022),BLOOM(Scao
etal.,2022),GLM-130B(Zengetal.,2022),LaMDA(Thoppilanetal.,2022),PaLM(Chowdhery
etal.,2022),Llama(Touvronetal.,2023),Gemini(Google,2023;Reidetal.,2024).
4 Experiments
Inthissection,weexperimentallyevaluatexLSTMandcompareittoexistingmethodswithafocuson
languagemodeling. WeinvestigatexLSTM’sspecificcapabilitiesonsynthetictasksinSection4.1. In
Section4.2,wecomparethevalidationsetperplexityofvariouscurrentlanguagemodelingmethods
thatweretrainedon15BtokensfromSlimPajama(Sobolevaetal.,2023). Onthesamedataset,we
performablationstudiesforxLSTM.Then,weassessthescalingbehaviorofthedifferentmethods
analogous to Kaplan et al. (2020) and Brown et al. (2020). In Section 4.3, we conduct a more
thoroughlanguagemodelingexperiment.WecomparexLSTMandthebestperformingmethodsfrom
Section4.2afterbeingtrainedon300BtokensfromSlimPajama(Sobolevaetal.,2023). First,we
assesshowwellthemethodsperforminextrapolatingtolongercontexts,secondlywetestthemethods
viavalidationperplexityandperformanceondownstreamtasks(Sutawikaetal.,2024),thirdlywe
evaluatethemethodson571textdomainsofthePALOMAlanguagebenchmarkdataset(Magnusson
etal.,2023),fourthlyweagainassessthescalingbehaviorofthedifferentmethods,butnowwith20
timesmoretrainingdata.
For all experiments, we use the notation xLSTM[a:b] for the ratio a/b of mLSTM-based versus
sLSTM-basedxLSTMblocks. Forexample,xLSTM[7:1]meansthatoutofeightblocks,sevenare
mLSTM-basedblocksandoneisansLSTM-basedblock. Foracommontotalblocknumberof48,
thistranslatesto6sLSTM-basedblocksand42mLSTM-basedblocks. Further,forallexperiments,
weusepreandpostup-projectionblocksformLSTMandsLSTM,respectively.
4.1 SyntheticTasksandLongRangeArena
First,wetesttheeffectivenessofxLSTM’snewexponentialgatingwithmemorymixingonformal
languages(Delétangetal.,2023). Then,weassesstheeffectivenessofxLSTM’snewmatrixmemory
ontheMulti-QueryAssociativeRecalltask(Aroraetal.,2023). Finally,xLSTM’sperformanceat
processinglongsequencesintheLongRangeArenaisevaluated(Tayetal.,2021).
TestofxLSTM’sExponentialGatingwithMemoryMixing. WetestxLSTM’snewexponential
gating with memory mixing, which should enable it to solve state tracking problems (Merrill
et al., 2024; Merrill & Sabharwal, 2023). We implement and extend the formal language tasks
fromDelétangetal.(2023)toenablemulti-lengthtrainingforlengthextrapolation. Foradetailed
description of all tasks and extended results see Appendix B.1.1. We compare xLSTM to other
methodsincludingTransformers,StateSpaceModels,andRecurrentNeuralNetworks. Theaccuracy
ofthetestedmethodsisevaluatedonthosetokensrelevanttothetask. Theaccuracyisscaledbetween
0(random)and1(perfect). Wecompare2-blockarchitecturesofthefollowingmethodsonthese
tasks: xLSTM[0:1] (i.e., only sLSTM), xLSTM[1:0] (i.e., only mLSTM), xLSTM[1:1], Llama,
Mamba,RWKV,Retention,Hyena,LSTM,andLSTMinTransformerblocks(LSTM(Block)). The
resultsofthisexperimentareshowninFigure4. ModelssuchasTransformersorStateSpaceModels
withoutmemorymixing(nostatetracking)cannotsolvee.g.regulargrammarsliketheparitytask.
7Deterministic
ContextSentsitive ContextFree Regular
Mod Mod
Missing Arithmetic Solve Arithmetic Majority
BucketSort Duplicate (wBrackets) Equation CycleNav EvenPairs (w/oBrackets) Parity Majority Count
Llama 0.92 0.08 0.02 0.02 0.04 1.0 0.03 0.03 0.37 0.13
±0.02 ±0.0 ±0.0 ±0.0 ±0.01 ±0.0 ±0.0 ±0.01 ±0.01 ±0.0
Mamba 0.69 0.15 0.04 0.05 0.86 1.0 0.05 0.13 0.69 0.45
±0.0 ±0.0 ±0.01 ±0.02 ±0.04 ±0.0 ±0.02 ±0.02 ±0.01 ±0.03
Retention 0.13 0.03 0.03 0.03 0.05 0.51 0.04 0.05 0.36 0.12
±0.01 ±0.0 ±0.0 ±0.0 ±0.01 ±0.07 ±0.0 ±0.01 ±0.0 ±0.01
Hyena 0.3 0.06 0.05 0.02 0.06 0.93 0.04 0.04 0.36 0.18
±0.02 ±0.02 ±0.0 ±0.0 ±0.01 ±0.07 ±0.0 ±0.0 ±0.01 ±0.02
RWKV-4 0.54 0.21 0.06 0.07 0.13 1.0 0.07 0.06 0.63 0.13
±0.0 ±0.01 ±0.0 ±0.0 ±0.0 ±0.0 ±0.0 ±0.0 ±0.0 ±0.0
RWKV-5 0.49 0.15 0.08 0.08 0.26 1.0 0.15 0.06 0.73 0.34
±0.04 ±0.01 ±0.0 ±0.0 ±0.05 ±0.0 ±0.02 ±0.03 ±0.01 ±0.03
RWKV-6 0.96 0.23 0.09 0.09 0.31 1.0 0.16 0.22 0.76 0.24
±0.0 ±0.06 ±0.01 ±0.02 ±0.14 ±0.0 ±0.0 ±0.12 ±0.01 ±0.01
LSTM 0.99 0.15 0.76 0.5 0.97 1.0 0.91 1.0 0.58 0.27
(Block) ±0.0 ±0.0 ±0.0 ±0.05 ±0.03 ±0.0 ±0.09 ±0.0 ±0.02 ±0.0
LSTM 0.94 0.2 0.72 0.38 0.93 1.0 1.0 1.0 0.82 0.33
±0.01 ±0.0 ±0.04 ±0.05 ±0.07 ±0.0 ±0.0 ±0.0 ±0.02 ±0.0
xLSTM[0:1] 0.84 0.23 0.57 0.55 1.0 1.0 1.0 1.0 0.75 0.22
±0.08 ±0.01 ±0.09 ±0.09 ±0.0 ±0.0 ±0.0 ±0.0 ±0.02 ±0.0
xLSTM[1:0] 0.97 0.33 0.03 0.03 0.86 1.0 0.04 0.04 0.74 0.46
±0.0 ±0.22 ±0.0 ±0.01 ±0.01 ±0.0 ±0.0 ±0.01 ±0.01 ±0.0
xLSTM[1:1] 0.7 0.2 0.15 0.24 0.8 1.0 0.6 1.0 0.64 0.5
±0.21 ±0.01 ±0.06 ±0.04 ±0.03 ±0.0 ±0.4 ±0.0 ±0.04 ±0.0
Figure4: TestofxLSTM’sexponentialgatingwithmemorymixing. Resultsaregivenbythescaled
accuracyofdifferentmodelsatsolvingformallanguagetasks,ofwhichsomerequirestatetracking.
ThedifferenttasksaregroupedbytheChomskyhierarchy.
ThisresultisinagreementwithfindingsthatTransformersandStateSpacemodelsarefundamentally
lesspowerfulthanRNNs(Merrilletal.,2024;Merrill&Sabharwal,2023;Delétangetal.,2023).
TestofxLSTM’sMemoryCapacitiesonAssociativeRecallTasks. Inthisexperiment,wetest
xLSTM’s new matrix memory in terms of the memory capacity on the Multi-Query Associative
Recalltask(Aroraetal.,2023): Foreachsequence, key-valuepairsarerandomlychosenfroma
large vocabulary, which must be memorized for later retrieval. To enhance the difficulty of the
originaltask,weincreasethenumberofkey-valuepairsupto256andextendthecontextlengthup
to2048. Thus,wehavebroadertestsforthememorycapacitiesofdifferentmodels. Wecompare
2-blockarchitecturesofLlama,Mamba,RWKV-5,RWKV-6,xLSTM[1:1]andxLSTM[1:0]. The
modelsareevaluatedbytheaccuracyatrecallingthepairs. SinceTransformers(e.g.Llama)have
amemorythatisexponentialinthecodingdimension(Ramsaueretal.,2021),theyconstitutethe
gold standard at this task. Results are shown in Figure 5. xLSTM[1:1] performs best among all
non-Transformermodels,alsoforsmallmodels. Interestingly,thesLSTMblockdoesnotdiminish
thememorycapacitybutratherleveragesit,whichbecomesevidentatthemostdifficulttaskwith256
key-valuepairs. AdditionalresultsarepresentedinAppendixB.1.2,whereextrapolationanalyses
indicatethatxLSTM’senhancedmemorycapacitiesalsopertainwhenextrapolatingtocontextsthat
arelongerthanthoseseenduringtraining.
Llama Mamba RWKV-5 RWKV-6 xLSTM[1:0] xLSTM[1:1]
KVPairs=48 KVPairs=96 KVPairs=256
1.00
0.75
0.50
0.25
0.00
32 64 128 256 512 32 64 128 256 512 32 64 128 256 512
ModelDim ModelDim ModelDim
Figure5: TestofmemorycapacitiesofdifferentmodelsattheMulti-QueryAssociativeRecalltask
withcontextlength2048. Eachpanelisdedicatedtoadifferentnumberofkey-valuepairs. The
x-axisdisplaysthemodelsizeandthey-axisthevalidationaccuracy.
8
ycaruccATestofxLSTM’sLongContextCapabilitiesonLongRangeArena. ToassessxLSTM’sper-
formanceonlongsequencesandlargecontexts,wecomparedifferentmethodsontheLongRange
Arena(Tayetal.,2021). xLSTMdemonstratesconsistentstrongperformanceonallofthetasks,
suggestingthatthexLSTMarchitectureisremarkablyefficientinhandlingdifferentaspectsoflong
contextproblems. Formoredetails,seeAppendixB.1.3.
4.2 MethodComparisonandAblationStudy
Themainquestionofthispaperis,whatcanweachieveinlanguagemodelingwhenscalingupthe
newLSTMvariants. Therefore,wetrainxLSTMs,Transformers,StateSpaceModels,andother
methods on 15B tokens from SlimPajama in an auto-regressive language modeling setting. We
comparethetrainedmodelsonthevalidationset. Finally,weperformablationstudiesforxLSTM.
Comparing xLSTM to Other Methods. For com-
parison,wetrainmodelson15BtokensfromSlimPa- Model #Params SlimPajama
M (15B)ppl↓
jama (Soboleva et al., 2023). The trained models
are evaluated by their perplexity on the validation
GPT-3 356 14.26
set. We compare the following methods: xLSTM
Llama 407 14.25
(ournewmethod),GPT-3(Transformer)(Brownetal.,
2020),Llama(Transformer)(Touvronetal.,2023),H3 H3 420 18.23
(SSM) (Fu et al., 2023), Mamba (SSM) (Gu & Dao, Mamba 423 13.70
2023),RWKV-4(RNN)(Pengetal.,2023),RWKV-5
Hyena 435 17.59
(RNN)(Pengetal.,2024),RWKV-6(RNN)(Pengetal.,
RWKV-4 430 15.62
2024), GLA (linear Transformer) (Yang et al., 2023),
RWKV-5 456 16.53
HGRN(RNN)(Qinetal.,2023),HGRN2(RNN)(Qin
RWKV-6 442 17.40
et al., 2024). RetNet (linear Transformer) (Sun et al.,
RetNet 431 16.23
2023), Hyena (linear Transformer) (Poli et al., 2023),
HGRN 411 21.83
xLSTM[1:0], and xLSTM[7:1] (see Section 4). The
GLA 412 19.56
models were trained with mixed precision, except
HGRN2 411 16.77
RWKV-5, RWKV-6, GLA, HGRN, HGRN2, where
mixed-precisiontrainingwasnotsupportedbytheref- xLSTM[1:0] 409 13.43
erenceimplementation. Wecategorizethemethodsinto xLSTM[7:1] 408 13.48
(a)Transformers,(b)StateSpaceModels(SSMs),and
Table 1: Method comparison on next
(c)RecurrentNeuralNetworks(RNNs)togetherwithlin-
token prediction when trained on 15B
earTransformers. LinearTransformersarelinearmeth-
tokens from SlimPajama. Best valida-
odsthatsubstitutetheTransformerattentionmechanism.
tionperplexitieswithinmodelclasses,i.e.,
ThemodelsmatchaGPT-3modelwith350Mparam-
Transformers,LSTMs,SSMs,RNNs,and
etersinsize,i.e.embeddingdim1024and24residual
linear Transformers are underlined and
blocks. OnlyGPT-3usessharedweightsfortokenand
overall best is in bold. For each model
outputembeddings,thereforehasfewerparameters. The
class, the best performing methods are
resultsinTable1showthatxLSTMoutperformsallex-
laterusedinSection4.3forLLMtraining.
istingmethodsinvalidationperplexity. Fordetailssee
xLSTMswithnewmemory(xLSTM[1:0]
Appendix B.2. Figure 6 shows the scaling behaviour
andxLSTM[7:1])performbest.
for this experiment, indicating that xLSTM will also
performfavorablyforlargermodels.
Ablation Studies. Table 1 and Figure 6 demonstrate that xLSTM achieves excellent results at
languagemodelingwhenbeingtrainedon15BtokensfromSlimPajama. Thus,itisonlynatural
toaskwhichoftheelementsofxLSTMisresponsiblefortheimprovementsovervanillaLSTM
performances, evoking an ablation study of the individual new xLSTM components. For doing
so, we morph a vanilla LSTM architecture step-by-step into an xLSTM architecture. First, we
integrate LSTM layers into pre-LayerNorm residual backbones, second we extend this to a post
up-projectionblock,thenweaddexponentialgating,andfinallythematrixmemory. Theresultsare
showninTable2(top). Theablationstudiesattributethestrongperformanceimprovementtoboth
theexponentialgatingandthematrixmemory. Additionally,sincegatingisanever-occuringtopic
inRNNsandStateSpaceModels,weablatedifferentgatingmechanisms. InTable2(bottom),we
concludethathavingeachgatelearnableandinfluencedbytheinputhasanincrementalpositive
effect. AdditionalstudiesontheindividualbackbonecomponentsarediscussedinAppendixB.2.
919 Figure6: Method comparison
Llama
18 onnexttokenpredictionwhen
Mamba
17 trained on 15B tokens from
RWKV-4
16 SlimPajama. Performancemea-
xLSTM[7:1] sureinvalidationperplexityfor
15
xLSTM[1:0] thebestmethodsofeachmodel
14
class(seeTable1)arereported.
13 Theperformancedegradationof
12 xLSTM[7:1] at 2.7B is due to
initiallyslowertrainingconver-
11 15B Tokens gencethatleadstoanespecially
undertrainedmodel. xLSTMis
10
0.2 0.4 1.0 1.4 2.7 thebestmethodatallsizes.
NumberofParameters 109
×
AblationstudiesonthenewxLSTMcomponents.
Exponential Matrix #Params SlimPajama
Model Modification
Gating Memory M (15B)ppl↓
VanillaMulti-LayerLSTM ✗ ✗ 607.8 2417.86
LSTM AddingResnetBackbone ✗ ✗ 506.1 35.46
AddingUp-ProjectionBackbone ✗ ✗ 505.9 26.01
xLSTM[0:1] AddingExponentialGating ✓ ✗ 427.3 17.70
xLSTM[7:1] AddingMatrixMemory ✓ ✓ 408.4 13.48
Ablationstudiesondifferentgatingtechniques.
ForgetGate InputGate
SlimPajama
LearnableGates Input Learnable Bias Input Learnable Bias (15B)ppl↓
Dependent Bias Init Dependent Bias Init
NoGates ✗ ✗ + ✗ ✗ 0 NaN
NoGates ✗ ✗ [3,∞ 6] ✗ ✗ 0 13.95
ForgetGate ✓ ✓ [3,6] ✗ ✗ 0 13.58
InputGate ✗ ✗ [3,6] ✓ ✓ (0,0.1) 13.69
ForgetGateBias ✗ ✓ [3,6] ✗ ✗ N 0 13.76
Forget+InputGateBias ✗ ✓ [3,6] ✗ ✓ (0,0.1) 13.73
ForgetGate+InputGateBias ✓ ✓ [3,6] ✗ ✓ N (0,0.1) 13.55
N
ForgetGate+InputGate ✓ ✓ [3,6] ✓ ✓ (0,0.1) 13.43
N
Table 2: Ablation studies. Top: Ablation studies on the new xLSTM components, contributing
thestrongperformanceimprovementofxLSTMovervanillaLSTMtoboththeexponentialgating
andthematrixmemory. Bottom: Ablationstudiesondifferentgatingtechniques. Weconsideran
xLSTM[1:0]withsigmoidforgetgateandexponentialinputgate.Biasinitialization meansthatthe
∞
forgetgateissettoone,[3,6]indicatesthatvaluesaretakenequidistantintherespectiveinterval,and
(0,0.1)thatvaluesarerandomlychosenfromaGaussianwithmean0andstd0.1. PPLdenotes
N
validationperplexity. Thefirsttwolinescorrespondtomodelssimilartolinearizedattention,line
fourtoRetention,linefivetoRWKV-5,andlinesixtoRWKV-6. Dependenciesofthegatesonthe
inputleadtobetterperformance.
10
ytixelprePnoitadilaV4.3 xLSTMasLargeLanguageModel
We culminate this study in large-scale language modeling experiments, testing the potential of
xLSTMasanLLM.Wethereforeincreasetheamountoftrainingdataandtrainon300Btokensfrom
SlimPajama. Thesamenumberoftokensisusedine.g.,Mamba(Gu&Dao,2023)andGriffin(De
et al., 2024). We compare xLSTM, RWKV-4, Llama, and Mamba, which were selected as the
best-performingmethodsintheirrespectivemethodclassesinthemodelcomparisoninSection4.2.
Wetraindifferentmodelsizes(125M,350M,760M,1.3B),testallmodelsforlengthextrapolation
capabilitiesandevaluatetheirperformanceonthevalidationset. Weassesstheirperformanceon
downstreamtasks,testtheirperformanceinlanguagemodelingon471textdomainsofthePALOMA
benchmark,and,finally,investigatetheirscalinglawbehavior.
SequenceLengthExtrapolation. First,wetestthesequencelengthextrapolationfor1.3B-sized,
largemodelsofxLSTM,RWKV-4,Llama,andMamba. Allmodelsaretrainedoncontextlength
2048,andthentestedforcontextlengthsupto16384. SeeFigure7fortheresults. Incontrastto
othermethods,xLSTMmodelsmaintainlowperplexitiesforlongercontexts.
SlimPajama
Model (300B)ppl↓
at16k
Llama 337.83
Mamba 14.00
RWKV-4 13.75
xLSTM[7:1] 8.92
xLSTM[1:0] 9.01
Figure7: Sequenceextrapolationinlanguagemodeling. Thisisacomparisonof1.3B-sized,large
models of xLSTM, RWKV-4, Llama, and Mamba at next token prediction on the SlimPajama
validationsetaftertrainingon300BtokensfromSlimPajama. Modelsaretrainedwithcontextlength
2048andthentestedforcontextlengthsupto16384. Left: Tokenperplexitiesevaluatedatdifferent
contextlengths. Incontrasttoothermethods,xLSTMmodelsremainatlowperplexitiesforlonger
contexts. Right: Predictionqualitywhenextrapolatingtolongcontextsizesintermsofvalidation
perplexity(PPL).xLSTMyieldsthebestPPLvalues(bestinbold,secondbestunderlined).
ValidationPerplexityandDownstreamTasks. Secondly,forallmodelsizes,weevaluatethe
performanceofxLSTM,RWKV-4,Llama,andMambamodelsontheSlimPajamavalidationsetfor
nexttokenpredictionandondownstreamtasksthatmeasurecommonsensereasoning. Thethird
columnofTable3liststhevalidationsetperplexitiesofdifferentmethods. BothxLSTM[1:0]and
xLSTM[7:1]arethebestmodelsforallmodelsizeswithrespecttothevalidationsetperplexity. The
othercolumnsofTable3providetheperformanceondownstreamtasks. Inthevastmajorityoftasks
andacrossallmodelsizesxLSTMisthebestmethod—onlyontheARCtaskMambaisinsome
casesthebestmethod. FordetailsseeAppendixB.3.
PerformanceonPALOMALanguageTasks. Thirdly,forallmodelsizes,wetestthenexttoken
predictionperformanceofxLSTM,RWKV-4,Llama,andMambamodelsonPALOMAlanguage
tasks (Magnusson et al., 2023). We measure the performance by the perplexity for next token
predictionon571textdomains,whichrangefromnytimes.comtor/depressiononReddit. Table4
showstokenpredictionperplexitygroupedintolanguagemodeling(firstsevencolumns)andfine-
graineddomainbenchmarks(last5columns). xLSTM[1:0]performsbetterthanxLSTM[7:1]on
theselanguagetasks. xLSTM[1:0]hasin568outof571(99.5%)textdomainsalowerperplexity
thanMamba,in486outof571(85.1%)alowerperplexitythanLlama,in570outof571(99.8%)a
lowerperplexitythanRWKV-4. FordetailsseeAppendixB.3.
11#Params SlimPajama LAMBADA LAMBADA HellaSwag PIQA ARC-E ARC-C WinoGrande Average
Model
M (300B)ppl↓ ppl↓ acc↑ acc↑ acc↑ acc↑ acc↑ acc↑ acc↑
RWKV-4 169.4 16.66 54.72 23.77 34.03 66.00 47.94 24.06 50.91 41.12
Llama 162.2 15.89 39.21 31.54 34.09 65.45 45.33 23.63 50.67 41.78
Mamba 167.8 15.08 27.76 34.14 36.47 66.76 48.86 24.40 51.14 43.63
xLSTM[1:0] 163.8 14.63 25.98 36.52 36.74 65.61 47.81 24.83 51.85 43.89
xLSTM[7:1] 163.7 14.60 26.59 36.08 36.75 66.87 48.32 25.26 51.70 44.16
RWKV-4 430.5 12.62 21.57 36.62 42.47 69.42 54.46 25.43 51.22 46.60
Llama 406.6 12.19 15.73 44.19 44.45 69.15 52.23 26.28 53.59 48.32
Mamba 423.1 11.64 12.83 46.24 47.55 69.70 55.47 27.56 54.30 50.14
xLSTM[1:0] 409.3 11.31 11.49 49.33 48.06 69.59 55.72 26.62 54.38 50.62
xLSTM[7:1] 408.4 11.37 12.11 47.74 47.89 71.16 56.61 27.82 53.28 50.75
RWKV-4 891.0 10.55 10.98 47.43 52.29 72.69 58.84 28.84 55.41 52.58
Llama 834.1 10.60 9.90 51.41 52.16 70.95 56.48 28.75 56.67 52.74
Mamba 870.5 10.24 9.24 50.84 53.97 71.16 60.44 29.78 56.99 53.86
xLSTM[1:0] 840.4 9.86 8.09 54.78 55.72 72.69 62.75 32.59 58.17 56.12
xLSTM[7:1] 839.7 9.91 8.07 55.27 56.12 72.74 61.36 29.61 56.43 55.26
RWKV-4 1515.2 9.83 9.84 49.78 56.20 74.70 61.83 30.63 55.56 54.78
Llama 1420.4 9.44 7.23 57.44 57.81 73.12 62.79 31.74 59.04 56.99
Mamba 1475.3 9.14 7.41 55.64 60.45 74.43 66.12 33.70 60.14 58.41
xLSTM[1:0] 1422.6 8.89 6.86 57.83 60.91 74.59 64.31 32.59 60.62 58.48
xLSTM[7:1] 1420.1 9.00 7.04 56.69 60.26 74.92 65.11 32.34 59.27 58.10
Table3: Validationsetperplexityanddownstreamtasks. ComparisonofxLSTM,RWKV-4,Llama,
andMambaonthevalidationsetatnexttokenpredictionandondownstreamtasksaftertrainingon
300BtokensfromSlimPajama. Modelsizesare125M,250M,760M,and1.3B.Thefirstcolumn
shows the methods and the second the actual number of parameters. The third column lists the
validationsetperplexities,whiletheremainingcolumnsshowtheperformanceondownstreamtasks.
Bestmodelpermodelsizeisdepictedboldandthesecondbestisunderlined. Inthevastmajorityof
tasksandacrossallmodelsizesxLSTMisthebestmethod—onlyontheARCtaskMambaisin
somecasesthebestmethod. xLSTM[1:0]andxLSTM[7:1]arethetwobestmodelswithrespectto
validationsetperplexity.
Model #Pa Mrams C4 M EC N4 Wi 1k 0it 3ext TrP eee bn an nk PaR jae md a R Wefi en bed Dolma SM 2O2D R2 C WM ik2 ipD e2 dia DomC4 ains SuD bo relm dda its CD oo dlm ina g Average
RWKV-4 169.4 26.25 22.33 29.18 38.45 8.99 32.47 17.04 23.86 21.42 22.68 37.08 5.12 23.74
Llama 162.2 24.64 17.23 23.16 31.56 8.26 29.15 15.10 19.71 20.41 21.45 36.73 3.61 20.92
Mamba 167.8 23.12 17.04 22.49 30.63 7.96 27.73 14.60 19.38 19.36 20.14 34.32 3.77 20.05
xLSTM[1:0] 163.8 22.54 16.32 21.98 30.47 7.80 27.21 14.35 19.02 19.04 19.65 34.15 3.64 19.68
xLSTM[7:1] 163.7 22.39 16.13 21.47 30.01 7.75 26.91 14.13 18.6 18.84 19.52 33.9 3.59 19.44
RWKV-4 430.5 19.55 15.82 19.64 27.58 6.97 24.28 12.94 17.59 15.96 16.98 29.40 3.90 17.55
Llama 406.6 18.38 13.28 16.41 21.82 6.56 22.09 11.76 15.05 15.25 15.99 28.30 3.12 15.67
Mamba 423.1 17.33 13.05 16.11 22.24 6.34 21.04 11.42 14.83 14.53 15.16 27.02 3.20 15.19
xLSTM[1:0] 409.3 17.01 12.55 15.17 22.51 6.20 20.66 11.16 14.44 14.27 14.85 26.70 3.08 14.88
xLSTM[7:1] 408.4 16.98 12.68 15.43 21.86 6.23 20.70 11.22 14.62 14.30 14.85 26.61 3.11 14.88
RWKV-4 891.0 15.51 12.76 14.84 21.39 5.91 19.28 10.70 14.27 13.04 13.68 24.22 3.32 14.08
Llama 834.1 15.75 11.59 13.47 18.33 5.82 19.04 10.33 13.00 13.05 13.76 24.80 2.90 13.49
Mamba 870.5 15.08 11.54 13.47 19.34 5.69 18.43 10.15 13.05 12.62 13.25 23.94 2.99 13.30
xLSTM[1:0] 840.4 14.60 11.03 12.61 17.74 5.52 17.87 9.85 12.50 12.20 12.81 23.46 2.87 12.76
xLSTM[7:1] 839.7 14.72 11.11 12.68 17.61 5.55 18.01 9.87 12.59 12.25 12.89 23.43 2.88 12.80
RWKV-4 1515.2 14.51 12.04 13.73 19.37 5.62 18.25 10.11 13.46 12.10 12.87 22.85 3.25 13.18
Llama 1420.4 13.93 10.44 11.74 15.92 5.29 17.03 9.35 11.61 11.53 12.24 22.63 2.74 12.04
Mamba 1475.3 13.35 10.40 11.76 16.65 5.21 16.50 9.17 11.73 11.18 11.83 21.43 2.83 11.84
xLSTM[1:0] 1422.6 13.13 10.09 11.41 15.92 5.10 16.25 9.01 11.43 10.95 11.60 21.29 2.73 11.58
xLSTM[7:1] 1420.1 13.31 10.21 11.32 16.00 5.16 16.48 9.11 11.61 11.10 11.76 21.50 2.75 11.69
Table4: PerformanceonPALOMALanguageModelingTasks. ComparisonofxLSTM,RWKV-4,
Llama,andMambabytheperplexityofnexttokenpredictiononthePALOMAlanguagebenchmark
aftertrainingon300BtokensfromSlimPajama. Modelsizesare125M,250M,760M,and1.3B.
Thesecondcolumnshowstheactualnumberofparameters. The571textdomainsaregroupedinto
languagemodeling(nextsevencolumns)andfine-graineddomainbenchmarks(further5columns).
Thelastcolumnshowstheaverageperplexityacrossallofthesetasks. Bestmodelpermodelsizeis
giveninboldandthesecondbestisunderlined. xLSTMyieldsthebestperformance.
12
M521
M053
M067
B3.1
M521
M053
M067
B3.1ScalingLaws. Fourthly,weassessthepower-lawscalingbehavior,whichallowstoextrapolatethe
performancetolargermodelsizes(Kaplanetal.,2020;Brownetal.,2020). Figure8presentsthe
scalingbehavior. Allmodelsshareasimilarscalingbehaviorbutwithdifferentoffsets. RWKV-4
performsworst,followedbyLlamaandMamba. xLSTMisbetterthanMambawithasimilarmargin
toMambaasMambahastoLlama. ThescalingbehaviorindicatesthatforlargermodelsxLSTM
willcontinuetoperformfavourablecomparedtoTransformersandState-Spacemodels.
17
Llama
16
Mamba
15 RWKV-4
xLSTM[7:1]
14
xLSTM[1:0]
13
12
11
10
300B Tokens
9
0.2 0.4 1.0 1.4
Number of Parameters 109
×
Figure8: Scalinglaws. NexttokenpredictionperplexityofxLSTM,RWKV-4,Llama,andMamba
ontheSlimPajamavalidationsetwhentrainedon300BtokensfromSlimPajama. Modelsizesare
125M,350M,760M,and1.3B.Bestmodelsforeachmodelclass,seeTable1,wereselected. The
scalinglawsindicatethatforlargermodelsxLSTMwillperformwell,too.
5 Limitations
(i)IncontrasttomLSTM,memorymixingofthesLSTMprohibitsparallelizableoperations,and
thereforedoesnotallowafastparallelimplementation. Nevertheless,wedevelopedafastCUDAker-
nelforsLSTM,whichiscurrentlyaround1.5timesslowerthanourparallelmLSTMimplementation.
(ii)TheCUDAkernelsformLSTMarenotoptimized,andthereforethecurrentimplementationis
about4timesslowerthanFlashAttentionorthescanusedinMamba. FasterCUDAkernelscouldbe
obtainedintheveinofFlashAttention. (iii)ThematrixmemoryofmLSTMhashighcomputation
complexitysinced dmatricesmustbeprocessed. Still,thememoryupdateandretrievaldoesnot
×
useparametersandcanbeparallelizedusingstandardmatrixoperations,thereforethewallclock
timeoverheadduetothecomplexmemoryisminor. (iv)Theinitializationoftheforgetgatesmustbe
chosencarefully. (v)Sincethematrixmemoryisindependentofthesequencelength,increasingthe
sequencelengthmightoverloadthememoryforlongercontextsizes. Still,thisdoesnotappeartobe
alimitationforcontextsupto16k,seeSection4.3. (vi)Duetotheexpensivecomputationalloadfor
largelanguageexperiments,wedidneitherfullyoptimizethearchitecturenorthehyperparameters,
especiallyforlargerxLSTMarchitectures. Weanticipatethatanextensiveoptimizationprocessis
neededforxLSTMtoreachitsfullpotential.
13
ytixelpreP
noitadilaV6 Conclusion
Wehavepartlyansweredoursimplequestion: Howfardowegetinlanguagemodelingwhenscaling
LSTMtobillionsofparameters? Sofar, wecananswer: “Atleastasfarascurrenttechnologies
like Transformers or State Space Models”. We have enhanced LSTM to xLSTM by exponential
gatingwithmemorymixingandanewmemorystructure. xLSTMmodelsperformfavorablyon
languagemodelingwhencomparedtostate-of-the-artmethodslikeTransformersandStateSpace
Models. ThescalinglawsindicatethatlargerxLSTMmodelswillbeseriouscompetitorstocurrent
LargeLanguageModelsthatarebuiltwiththeTransformertechnology. xLSTMhasthepotentialto
considerablyimpactotherdeeplearningfieldslikeReinforcementLearning,TimeSeriesPrediction,
orthemodelingofphysicalsystems.
Acknowledgements
WethankSebastianLehner,DanielKlotz,ThomasAdler,MatthiasDellago,GeraldGutenbrunner,
FabianPaischer,VihangPatil,NiklasSchmidinger,BenediktAlkin,KajetanSchweighofer,Anna
Zimmel, Lukas Aichberger, Lukas Hauzenberger, Bernhard Schäfl, Johannes Lehner for helpful
discussionsandfeedback.
References
J.Achiam,S.Adler,S.Agarwal,etal. GPT-4technicalreport. ArXiv,2303.08774,2023.
J.Anderson,J.Silverstein,S.Ritz,andR.Jones. Distinctivefeatures,categoricalperception,and
probabilitylearning: Someapplicationsofaneuralmodel. PsychologicalReview,84:413–451,
1977. doi: 10.1037/0033-295X.84.5.413.
J. A. Anderson. A simple neural network generating an interactive memory. Mathematical Bio-
sciences,14,1972. doi: 10.1016/0025-5564(72)90075-2.
S.Arora,S.Eyuboglu,A.Timalsina,I.Johnson,M.Poli,J.Zou,A.Rudra,andC.Ré. Zoology:
Measuringandimprovingrecallinefficientlanguagemodels. ArXiv,2312.04927,2023.
J.Ba,G.E.Hinton,V.Mnih,J.Z.Leibo,andC.Ionescu. Usingfastweightstoattendtotherecent
past. InD.D.Lee,M.Sugiyama,U.V.Luxburg,I.Guyon,andR.Garnett(eds.),Advancesin
NeuralInformationProcessingSystems29,pp.4331–4339.CurranAssociates,Inc.,2016a.
J.Ba,J.R.Kiros,andG.Hinton. Layernormalization. ArXiv,1607.06450,2016b.
A. Bau, Y. Belinkov, H. Sajjad, N. Durrani, F. Dalvi, and J. Glass. Identifying and controlling
importantneuronsinneuralmachinetranslation. InInternationalConferenceonLearningRepre-
sentations(ICLR),2019. URLhttps://openreview.net/forum?id=H1z-PsR5KX.
Y.Bisk,R.Zellers,R.LeBras,J.Gao,andY.Choi. Piqa: Reasoningaboutphysicalcommonsensein
naturallanguage. InAAAIConferenceonArtificialIntelligence,volume34,pp.7432–7439,2020.
S.L.Blodgett,L.Green,andB.O’Connor. Demographicdialectalvariationinsocialmedia: Acase
studyofAfrican-AmericanEnglish. InConferenceonEmpiricalMethodsinNaturalLanguage
Processing,pp.1119–1130,2016. doi: 10.18653/v1/D16-1120.
T. Brown, B. Mann, N. Ryder, et al. Language models are few-shot learners. In H. Larochelle,
M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin(eds.),AdvancesinNeuralInformationProcessing
Systems,volume33,pp.1877–1901.CurranAssociates,Inc.,2020.
K.M.Choromanski,V.Likhosherstov,D.Dohan,X.Song,A.Gane,T.Sarlós,P.Hawkins,J.Q.
Davis, A. Mohiuddin, L. Kaiser, D. B. Belanger, L. J. Colwell, and A. Weller. Rethinking
attentionwithperformers. In9thInternationalConferenceonLearningRepresentations(ICLR).
OpenReview.net,2021. URLhttps://openreview.net/forum?id=Ua6zuk0WRH.
A.Chowdhery,S.Narang,J.Devlin,etal. PaLM:scalinglanguagemodelingwithpathways. ArXiv,
2204.02311,2022.
14A.Chronopoulou,M.Peters,andJ.Dodge.Efficienthierarchicaldomainadaptationforpretrainedlan-
guagemodels. InConferenceoftheNorthAmericanChapteroftheAssociationforComputational
Linguistics,pp.1336–1351,2022. doi: 10.18653/v1/2022.naacl-main.96.
P.Clark,I.Cowhey,O.Etzioni,T.Khot,A.Sabharwal,C.Schoenick,andO.Tafjord. Thinkyouhave
solvedquestionanswering? TryARC,theAI2reasoningchallenge. ArXiv,1803.05457,2018.
T.M.Cover. Geometricalandstatisticalpropertiesofsystemsoflinearinequalitieswithapplications
inpatternrecognition. ElectronicComputers,IEEETransactionson,EC-14(3):326–334,1965.
T. Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In In-
ternational Conference on Learning Representations (ICLR), volume 12, 2024. URL https:
//openreview.net/forum?id=mZn2Xyh9Ec.
T.Dao,D.Y.Fu,S.Ermon,A.Rudra,andC.Ré. Flashattention: Fastandmemory-efficientexact
attentionwithIO-awareness. InA.H.Oh,A.Agarwal,D.Belgrave,andK.Cho(eds.),Advances
inNeuralInformationProcessingSystems(NeurIPS),2022. URLhttps://openreview.net/
forum?id=H4DqfPSibmx.
P. Dayan and D. J. Willshaw. Optimising synaptic learning rules in linear associative memories.
BiologicalCybernetics,65,1991. doi: 10.1007/bf00206223.
S. De, S. L. Smith, A. Fernando, A. Botev, G. Cristian-Muraru, A. Gu, R. Haroun, L. Berrada,
Y.Chen,S.Srinivasan,G.Desjardins,A.Doucet,D.Budden,Y.W.Teh,R.Pascanu,N.DeFreitas,
andC.Gulcehre.Griffin:Mixinggatedlinearrecurrenceswithlocalattentionforefficientlanguage
models. ArXiv,2402.19427,2024.
J.Degrave,F.Felici,J.Buchli,etal.Magneticcontroloftokamakplasmasthroughdeepreinforcement
learning. Nature,602:414–419,2022. doi: 10.1038/s41586-021-04301-9.
G.Delétang,A.Ruoss,J.Grau-Moya,T.Genewein,L.K.Wenliang,E.Catt,C.Cundy,M.Hutter,
S.Legg,J.Veness,andP.A.Ortega. NeuralnetworksandtheChomskyhierarchy. InInternational
ConferenceonLearningRepresentations(ICLR),volume11,2023. URLhttps://openreview.
net/forum?id=WbxHAzkeQcn.
N.Du,Y.Huang,A.M.Dai,etal. GLaM:efficientscalingoflanguagemodelswithmixture-of-
experts. ArXiv,2112.06905,2021.
D.Y.Fu,T.Dao,K.K.Saab,A.W.Thomas,A.Rudra,andC.Re. Hungryhungryhippos: Towards
languagemodelingwithstatespacemodels. InTheEleventhInternationalConferenceonLearning
Representations,2023. URLhttps://openreview.net/forum?id=COZDy0WYGg.
L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite,
N. Nabeshima, S. Presser, and C. Leahy. The Pile: An 800gb dataset of diverse text for lan-
guagemodeling. ArXiv,2101.00027,2021.
F.A.Gers,J.Schmidhuber,andF.Cummins. Learningtoforget: ContinualpredictionwithLSTM.
NeuralCompututation,12(10):2451–2471,2000.
GeminiTeamGoogle. Gemini: Afamilyofhighlycapablemultimodalmodels. ArXiv,2312.11805,
2023.
A.Graves. Generatingsequenceswithrecurrentneuralnetworks. ArXiv,1308.0850,2013.
S.GreenbaumandG.Nelson. TheinternationalcorpusofEnglish(ICE)project. WorldEnglishes,
15(1):3–15,1996.
K.Greff,R.K.Srivastava,J.Koutník,B.R.Steunebrink,andJ.Schmidhuber. LSTM:Asearch
spaceodyssey. ArXiv,1503.04069,2015.
A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. ArXiv,
2312.00752,2023.
A.Gu,K.Goel,andC.Ré. Efficientlymodelinglongsequenceswithstructuredstatespaces. ArXiv,
2111.00396,2021.
15A.Gupta, A.Gu, andJ.Berant. Diagonalstatespacesareaseffectiveasstructuredstatespaces.
ArXiv,2203.14343,2022.
K.He,X.Zhang,S.Ren,andJ.Sun. Deepresiduallearningforimagerecognition. InProceedings
oftheIEEEConferenceonComputerVisionandPatternRecognition(CVPR),pp.770–778,2016.
S.Hochreiter. UntersuchungenzudynamischenneuronalenNetzen. Master’sthesis,Technische
UniversitätMünchen,1991.
S.HochreiterandJ.Schmidhuber. Longshort-termmemory. NeuralComputation,9(8):1735–1780,
1997a.
S.HochreiterandJ.Schmidhuber. LSTMcansolvehardlongtimelagproblems. InM.C.Mozer,
M.I.Jordan,andT.Petsche(eds.),AdvancesinNeuralInformationProcessingSystems(NeurIPS),
volume9,pp.473–479.MITPress,CambridgeMA,1997b.
S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient flow in recurrent nets: the
difficultyoflearninglong-termdependencies. InJ.KolenandS.Kremer(eds.),AFieldGuideto
DynamicalRecurrentNetworks.IEEE,2000.
S.Hochreiter,A.StevenYounger,andPeterR.Conwell. Learningtolearnusinggradientdescent.
InG.Dorffner,H.Bischof,andK.Hornik(eds.),Proc.Int.Conf.onArtificialNeuralNetworks
(ICANN2001),pp.87–94.Springer,2001.
S.Hochreiter,M.Heusel,andK.Obermayer. Fastmodel-basedproteinhomologydetectionwithout
alignment. Bioinformatics,23(14):1728–1736,2007.
J.Hoffmann,S.Borgeaud,A.Mensch,etal. Trainingcompute-optimallargelanguagemodels. ArXiv,
2203.15556,2022.
M.D.Hossain,F.Sohel,M.F.Shiratuddin,andH.Laga. Acomprehensivesurveyofdeeplearning
forimagecaptioning. ACMComputingSurveys(CSUR),51(6):118,2019.
J.Kaplan,S.McCandlish,T.Henighan,T.B.Brown,B.Chess,R.Child,S.Gray,A.Radford,J.Wu,
andD.Amodei. Scalinglawsforneurallanguagemodels. ArXiv,2001.08361,2020.
A. Karpathy. The unreasonable effectiveness of recurrent neural networks.
http://karpathy.github.io/2015/05/21/rnn-effectiveness/,2015.
A.Karpathy. OpenAIFivedefeatsDota2worldchampions. https://openai.com/research/openai-five-
defeats-dota-2-world-champions,2019.
A.KarpathyandL.Fei-Fei. Deepvisual-semanticalignmentsforgeneratingimagedescriptions. In
ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition(CVPR),pp.
3128–3137,2015.
A.Katharopoulos,A.Vyas,N.Pappas,andF.Fleuret. TransformersareRNNs: Fastautoregressive
transformerswithlinearattention.InE.H.DauméIIIandA.Singh(eds.),InternationalConference
onMachineLearning(ICML),volume119ofProceedingsofMachineLearningResearch,pp.
5156–5165.PMLR,2020.
T. Katsch. GateLoop: Fully data-controlled linear recurrence for sequence modeling. ArXiv,
2311.01927,2023.
D. Kocetkov, R. Li, L. BenAllal, J. Li, C. Mou, C. Mu nozFerrandis, Y. Jernite, M. Mitchell,
S.Hughes,T.Wolf,D.Bahdanau,L.vonWerra,andH.deVries. TheStack: 3TBofpermissively
licensedsourcecode. ArXiv,2211.15533,2022.
T.Kohonen. Correlationmatrixmemories. IEEETransactionsonComputers,C-21(4),1972. doi:
10.1109/tc.1972.5008975.
F.Kratzert,D.Klotz,C.Brenner,K.Schulz,andM.Herrnegger.Rainfall-runoffmodellingusinglong
short-termmemory(LSTM)networks. HydrologyandEarthSystemSciences,22(11):6005–6022,
2018.
16F.Kratzert, D.Klotz, G.Shalev, G.Klambauer, S.Hochreiter, andG.Nearing. Benchmarkinga
catchment-awarelongshort-termmemorynetwork(LSTM)forlarge-scalehydrologicalmodeling.
ArXiv,1907.08456,2019.
A.Krizhevsky. Learningmultiplelayersoffeaturesfromtinyimages. Master’sthesis,Deptartment
ofComputerScience,UniversityofToronto,2009.
D. Krotov and J. J. Hopfield. Dense associative memory for pattern recognition. In D. D. Lee,
M.Sugiyama,U.V.Luxburg,I.Guyon,andR.Garnett(eds.),AdvancesinNeuralInformation
ProcessingSystems,pp.1172–1180.CurranAssociates,Inc.,2016.
D. Krotov and J. J. Hopfield. Dense associative memory is robust to adversarial inputs. ArXiv,
1701.00939,2017.
Y.Lakretz,G.Kruszewski,T.Desbordes,D.Hupkes,S.Dehaene,andM.Baroni. Theemergenceof
numberandsyntaxunitsinLSTMlanguagemodels. InJ.Burstein,C.Doran,andT.Solorio(eds.),
ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics,pp.
11–20.AssociationforComputationalLinguistics,2019. doi: 10.18653/v1/N19-1002.
Y. Li, T. Cai, Y. Zhang, D. Chen, and D. Dey. What makes convolutional models great on long
sequencemodeling? ArXiv,2210.09298,2022.
P.Liang,R.Bommasani,T.Lee,etal. Holisticevaluationoflanguagemodels. AnnalsoftheNew
YorkAcademyofSciences,1525:140–146,2023.
J.Lin,R.Men,A.Yang,C.Zhou,M.Ding,Y.Zhang,P.Wang,A.Wang,L.Jiang,X.Jia,J.Zhang,
J.Zhang,X.Zou,Z.Li,X.Deng,J.Liu,J.Xue,H.Zhou,J.Ma,j.Yu,Y.Li,W.Lin,J.Zhou,
J.Tang,andH.Yang. M6: AChinesemultimodalpretrainer. ArXiv,2103.00823,2021.
D.Linsley,J.Kim,V.Veerabadran,C.Windolf,andT.Serre. Learninglong-rangespatialdependen-
cieswithhorizontalgatedrecurrentunits. AdvancesinNeuralInformationProcessingSystems
(NeurIPS),31,2018.
I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Confer-
enceonLearningRepresentations(ICLR),2019. URLhttps://openreview.net/forum?id=
Bkg6RiCqY7.
X.Ma,C.Zhou,X.Kong,J.He,L.Gui,G.Neubig,J.May,andL.Zettlemoyer. Mega: Moving
averageequippedgatedattention. ArXiv,2209.10655,2022.
A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning word vectors
for sentiment analysis. In Annual Meeting of the Association for Computational Linguistics,
volume49,pp.142–150,2011.
I.Magnusson,A.Bhagia,V.Hofmann,etal. Paloma: Abenchmarkforevaluatinglanguagemodel
fit. ArXiv,2312.10523,2023.
H.Mehta,A.Gupta,A.Cutkosky,andB.Neyshabur. Longrangelanguagemodelingviagatedstate
spaces. ArXiv,2206.13947,2022.
S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. In Interna-
tionalConferenceonLearningRepresentations(ICRL),2017. URLhttps://openreview.net/
forum?id=Byj72udxe.
W.MerrillandA.Sabharwal. Theparallelismtradeoff: Limitationsoflog-precisiontransformers.
TransactionsoftheAssociationforComputationalLinguistics,11:531–545,2023. doi: 10.1162/
tacl_a_00562.
W.Merrill,J.Petty,andA.Sabharwal. Theillusionofstateinstate-spacemodels. ArXiv,2404.08819,
2024.
M.MilakovandN.Gimelshein. Onlinenormalizercalculationforsoftmax. ArXiv, 1805.02867,
2018.
17K.Nakano. Associatron–amodelofassociativememory. IEEETransactionsonSystems,Man,and
Cybernetics,SMC-2(3):380–388,1972. doi: 10.1109/TSMC.1972.4309133.
G.Nearing,D.Cohen,V.Dube,M.Gauch,O.Gilon,S.Harrigan,A.Hassidim,D.Klotz,F.Kratzert,
A. Metzger, S. Nevo, F. Pappenberger, C. Prudhomme, G. Shalev, S. Shenzis, T. Y. Tekalign,
D.Weitzner,andY.M.B.Kosko. Globalpredictionofextremefloodsinungaugedwatersheds.
Nature,627:559–563,2024. doi: 10.1038/s41586-024-07145-1.
C.Olsson,N.Elhage,N.Nanda,etal. In-contextlearningandinductionheads. ArXiv,2209.11895,
2022.
A.Orvieto, S.L.Smith, A.Gu, A.Fernando, C.Gulcehre, R.Pascanu, andS.De. Resurrecting
recurrentneuralnetworksforlongsequences. InProceedingsofthe40thInternationalConference
onMachineLearning(ICML).JMLR.org,2023. doi: 10.5555/3618408.3619518.
A.Papasavva,S.Zannettou,E.DeCristofaro,G.Stringhini,andJ.Blackburn. Raidersofthelost
KeK:3.5yearsofaugmented4chanpostsfromthepoliticallyincorrectboard. InInternational
AAAIConferenceonWebandSocialMedia(ICWSM),volume14,pp.885–894,2020.
D.Paperno,G.Kruszewski,A.Lazaridou,N.-Q.Pham,R.Bernardi,S.Pezzelle,M.Baroni,Gemma
G. Boleda, and R. Fernández. The LAMBADA dataset: Word prediction requiring a broad
discoursecontext. InAnnualMeetingoftheAssociationforComputationalLinguistics,volume1,
pp.1525–1534,2016.
G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier, E. Al-
mazrouei,andJ.Launay.TheRefinedWebdatasetforFalconLLM:Outperformingcuratedcorpora
withwebdata,andwebdataonly. ArXiv,2306.01116,2023.
B.Peng,E.Alcaide,Q.Anthony,etal. RWKV:ReinventingRNNsforthetransformerera. ArXiv,
2305.13048,2023.
B.Peng,D.Goldstein,Q.Anthony,etal. EagleandFinch: RWKVwithmatrix-valuedstatesand
dynamicrecurrence. ArXiv,2404.05892,2024.
M.Poli,S.Massaroli,E.Nguyen,D.Y.Fu,T.Dao,S.Baccus,Y.Bengio,S.Ermon,andC.Ré.Hyena
hierarchy:Towardslargerconvolutionallanguagemodels. InProceedingsofthe40thInternational
ConferenceonMachineLearning(ICML).JMLR.org,2023. doi: 10.5555/3618408.3619572.
M.Poli,A.W.Thomas,E.Nguyen,P.Ponnusamy,B.Deiseroth,K.Kersting,T.Suzuki,B.Hie,S.Er-
mon,C.Ré,C.Zhang,andS.Massaroli. Mechanisticdesignandscalingofhybridarchitectures.
ArXiv,2403.17844,2024.
Z.Qin,S.Yang,andY.Zhong. Hierarchicallygatedrecurrentneuralnetworkforsequencemodeling.
In Advances in Neural Information Processing Systems (NeurIPS), volume 37, 2023. URL
https://openreview.net/forum?id=P1TCHxJwLB.
Z.Qin,S.Yang,W.Sun,X.Shen,D.Li,W.Sun,andY.Zhong. HGRN2: GatedlinearRNNswith
stateexpansion. ArXiv,2404.07904,2024.
D.R.Radev,P.Muthukrishnan,andV.Qazvinian. TheACLanthologynetworkcorpus. InWorkshop
onTextandCitationAnalysisforScholarlyDigitalLibraries(NLPIR4DL),pp.54–61.Association
forComputationalLinguistics,2009.
A.Radford,R.Jozefowicz,andI.Sutskever. Learningtogeneratereviewsanddiscoveringsentiment.
ArXiv,1704.01444,2017.
A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,andI.Sutskever.Languagemodelsareunsupervised
multitasklearners. https://openai.com/index/better-language-models,2019.
J.W.Rae,S.Borgeaud,T.Cai,etal. Scalinglanguagemodels: Methods,analysis&insightsfrom
trainingGopher. ArXiv,2112.11446,2021.
C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu.
Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer. ArXiv,1910.10683,
2019.
18H.Ramsauer,B.Schäfl,J.Lehner,P.Seidl,M.Widrich,L.Gruber,M.Holzleitner,M.Pavlovic´,
G. K. Sandve, V. Greiff, D. Kreil, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter.
Hopfield networks is all you need. In International Conference on Learning Representations
(ICLR).OpenReview,2021.
M.Reid,V.Zhong,S.Gururangan,andL.Zettlemoyer. M2D2: Amassivelymulti-domainlanguage
modeling dataset. In Conference on Empirical Methods in Natural Language Processing, pp.
964–975,2022.
M.Reid,N.Savinov,D.Teplyashin,etal. Gemini1.5: Unlockingmultimodalunderstandingacross
millionsoftokensofcontext. ArXiv,2403.05530,2024.
M.H.Ribeiro,J.Blackburn,B.Bradlyn,E.DeCristofaro,G.Stringhini,S.Long,S.Greenberg,and
S.Zannettou. Theevolutionofthemanosphereacrosstheweb. InProceedingsoftheinternational
AAAIconferenceonwebandsocialmedia,volume15,pp.196–207,2021.
K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd
schemachallengeatscale. CommunicationsoftheACM,64(9):99–106,2021.
T.L.Scao,A.Fan,C.Akiki,etal. BLOOM:A176B-parameteropen-accessmultilinguallanguage
model. ArXiv,2211.05100,2022.
I.Schlag,K.Irie,andJ.Schmidhuber. Lineartransformersaresecretlyfastweightprogrammers.
InM.MeilaandT.Zhang(eds.),Proceedingsofthe38thInternationalConferenceonMachine
Learning(ICML),volume139ofProceedingsofMachineLearningResearch,pp.9355–9366.
PMLR,2021.
J.Schmidhuber. Learningtocontrolfast-weightmemories: Analternativetorecurrentnets. Neural
Computation,4(1):131–139,1992.
J.Schmidhuber. Deeplearninginneuralnetworks: Anoverview. NeuralNetworks,61:85–117,2015.
doi: 10.1016/j.neunet.2014.09.003.
J.Schulman,B.Zoph,C.Kim,J.Hilton,etal. ChatGPT:Optimizinglanguagemodelsfordialogue.
https://openai.com/blog/chatgpt/,2022. OpenAIResearch.
T.J.Sejnowski. Storingcovariancewithnonlinearlyinteractingneurons. JournalofMathematical
Biology,4,1977. doi: 10.1007/BF00275079.
M.Shoeybi,M.Patwary,R.Puri,P.LeGresley,J.Casper,andB.Catanzaro. Megatron-LM:Training
multi-billionparameterlanguagemodelsusingmodelparallelism. ArXiv,1909.08053,2019.
J. T. H. Smith, A. Warrington, and S. W. Linderman. Simplified state space layers for sequence
modeling. ArXiv,2208.04933,2022.
D.Soboleva,F.Al-Khateeb,R.Myers,J.R.Steeves,J.Hestness,andN.Dey. SlimPajama: A627B
tokencleanedanddeduplicatedversionofRedPajama. https://www.cerebras.net/blog/
slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama,
2023. URLhttps://huggingface.co/datasets/cerebras/SlimPajama-627B.
L.Soldaini,R.Kinney,A.Bhagia,etal. Dolma: anopencorpusofthreetrilliontokensforlanguage
modelpretrainingresearch. ArXiv,2306.01116,2023.
S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza, H. Khan, C. Peris, S. Rawls,
A.Rosenbaum,A.Rumshisky,C.S.Prakash,M.Sridhar,F.Triefenbach,A.Verma,G.Tur,and
P.Natarajan. AlexaTM20B:Few-shotlearningusingalarge-scalemultilingualSeq2Seqmodel.
ArXiv,2208.01448,2022.
R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. In C. Cortes,
N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information
ProcessingSystems(NeurIPS),volume28.CurranAssociates,Inc.,2015.
Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei. Retentive network: A
successortotransformerforlargelanguagemodels. ArXiv,2307.08621,2023.
19L.Sutawika,L.Gao,H.Schoelkopf,etal. EleutherAI/lm-evaluation-harness: Majorrefactor,2023.
L. Sutawika, H. Schoelkopf, L. Gao, B. Abbasi, S. Biderman, J. Tow, B. fattori, C. Lovering,
farzanehnakhaee70,J.Phang,A.Thite,Fazz,T.Wang,N.Muennighoff,Aflah,sdtblck,nopperl,
gakada,tttyuntian,researcher2,Chris,J.Etxaniz,H.A.Lee,Z.Kasner,Khalid,J.Hsu,A.Kanekar,
P.S.Ammanamanchi,V.Boykis,andAndyZwei. EleutherAI/lm-evaluation-harness,2024.
I.Sutskever,O.Vinyals,andQ.V.V.Le. Sequencetosequencelearningwithneuralnetworks. In
Z.Ghahramani,M.Welling,C.Cortes,N.D.Lawrence,andK.Q.Weinberger(eds.),Advancesin
NeuralInformationProcessingSystems27(NIPS’13),pp.3104–3112.CurranAssociates,Inc.,
2014.
Y. Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao, and C. Zheng. Synthesizer: Rethinking self-
attentionintransformermodels. ArXiv,2005.00743,2020.
Y.Tay,M.Dehghani,S.Abnar,Y.Shen,D.Bahri,P.Pham,J.Rao,L.Yang,S.Ruder,andD.Metzler.
Longrangearena:Abenchmarkforefficienttransformers.InInternationalConferenceonLearning
Representations(ICRL),2021. URLhttps://openreview.net/forum?id=qVyeW-grC2k.
R.Thoppilan,D.deFreitas,J.Hall,etal. LaMDA:Languagemodelsfordialogapplications. ArXiv,
2201.08239,2022.
TogetherComputer. Redpajama: anopendatasetfortraininglargelanguagemodels,2023. URL
https://github.com/togethercomputer/RedPajama-Data.
H.Touvron,T.Lavril,G.Izacard,X.Martinet,M.-A.Lachaux,T.Lacroix,B.Rozière,N.Goyal,
E.Hambro,F.Azhar,A.Rodriguez,A.Joulin,E.Grave,andG.Lample.Llama:Openandefficient
foundationlanguagemodels. ArXiv,2302.1397,2023.
D.VadasandJ.R.Curran. ParsingnounphrasesinthePennTreebank. ComputationalLinguistics,
37(4):753–809,2011.
A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,L.Kaiser,andI.Polosukhin.
Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS),
volume30,pp.5998–6008.CurranAssociates,Inc.,2017.
O.Vinyals,T.Ewalds,S.Bartunov,etal. StarcraftII:Anewchallengeforreinforcementlearning.
ArXiv,1708.04782,2017.
J.Wang,J.N.Yan,A.Gu,andA.M.Rush. Pretrainingwithoutattention. ArXiv,2212.10544,2022.
S.Wang,B.Z.Li,M.Khabsa,H.Fang,andH.Ma. Linformer: Self-attentionwithlinearcomplexity.
ArXiv,2006.04768,2020.
S.Wang, Y.Sun, Y.Xiang, etal. ERNIE3.0Titan: Exploringlarger-scaleknowledgeenhanced
pre-trainingforlanguageunderstandingandgeneration. ArXiv,2112.12731,2021.
Y.WuandK.He. Groupnormalization. InProceedingsoftheEuropeanconferenceoncomputer
vision(ECCV),pp.3–19,2018.
L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel.
mT5: Amassivelymultilingualpre-trainedtext-to-texttransformer. InConferenceoftheNorth
American Chapter of the Association for Computational Linguistics, pp. 483–498, 2021. doi:
10.18653/v1/2021.naacl-main.41.
S. Yang and Y. Zhang. FLA: A Triton-based library for hardware-efficient implementa-
tions of linear attention mechanism, 2024. URL https://github.com/sustcsonglin/
flash-linear-attention.
S.Yang,B.Wang,Y.Shen,R.Panda,andY.Kim. Gatedlinearattentiontransformerswithhardware-
efficienttraining. ArXiv,2312.06635,2023.
S.Zannettou,B.Bradlyn,E.DeCristofaro,H.Kwak,M.Sirivianos,G.Stringini,andJ.Blackburn.
WhatisGab: Abastionoffreespeechoranalt-rightechochamber. InTheWebConference,pp.
1007–1014,2018. doi: 10.1145/3184558.3191531.
20W.ZarembaandI.Sutskever. Learningtoexecute. ArXiv,1410.4615,2014.
R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really
finishyoursentence? InAnnualMeetingoftheAssociationforComputationalLinguistics,pp.
4791–4800,2019.
A.Zeng,X.Liu,Z.Du,etal. GLM-130B:Anopenbilingualpre-trainedmodel. ArXiv,2210.02414,
2022.
S.Zhang,S.Roller,N.Goyal,M.Artetxe,M.Chen,S.Chen,C.Dewan,M.Diab,X.Li,X.V.Lin,
T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and
L.Zettlemoyer. OPT:Openpre-trainedtransformerlanguagemodels. ArXiv,2205.01068,2022.
21Contents
A ExtendedLongShort-TermMemory 23
A.1 VanillaLongShort-TermMemoryFormulation: VectorNotation . . . . . . . . . . 23
A.2 sLSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
A.3 mLSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
A.4 DetailedBlockStructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
B Experiments 31
B.1 SyntheticTasksandLongRangeArena . . . . . . . . . . . . . . . . . . . . . . . . 31
B.1.1 TestofxLSTM’sExponentialGatingwithMemoryMixing. . . . . . . . . . 31
B.1.2 TestofxLSTM’sMemoryCapacitiesonAssociativeRecallTasks. . . . . . 34
B.1.3 TestofxLSTM’sLongRangeCapabilitiesontheLongRangeArena. . . . 36
B.2 MethodComparisonandAblationStudyonSlimPajama(15B) . . . . . . . . . . . 40
B.3 xLSTMLargeLanguageModels–SlimPajama300B . . . . . . . . . . . . . . . . 42
C DetailedResultsonPALOMALanguageModelEvaluation 44
22A ExtendedLongShort-TermMemory
A.1 VanillaLongShort-TermMemoryFormulation: VectorNotation
ThevanillaLSTMmemorycellupdaterules(Greffetal.,2015)attimesteptextendthescalarcell
stateformulationtoavectorofcellstates:
c = f c + i z cellstate (28)
t t t−1 t t
⊙ ⊙
(cid:16) (cid:17)
h = o h˜ , h˜ = ψ c hiddenstate (29)
t t t t t
⊙
z = φ(z˜) , z˜ = W x + R h + b cellinput (30)
t t t z t z t−1 z
(cid:16) (cid:17)
i = σ ˜i , ˜i = W x + R h + b inputgate (31)
t t t i t i t−1 i
(cid:16) (cid:17)
f = σ ˜f , ˜f = W x + R h + b forgetgate (32)
t t t f t f t−1 f
o = σ(o˜ ) , o˜ = W x + R h + b outputgate (33)
t t t o t o t−1 o
ThematricesW , W , W , andW correspondtotheinputweightsbetweeninputsx andcell
z i f o t
input, input gate, forget gate, and output gate, respectively. The matrices R , R , R , and R
z i f o
correspondtotherecurrentweightsbetweenhiddenstateh andcellinput,inputgate,forgetgate,
t−1
andoutputgate,respectively. b ,b ,b ,andb arethecorrespondingbiasvectors. φandψarethe
z i f o
cellinputandhiddenstateactivationfunctions(typicallytanh). ψisusedtonormalizeorsquashthe
cellstate,whichwouldbeunboundedotherwise.
A.2 sLSTM
SimilartotheLSTMinSectionA.1,alsothesLSTMcanbevectorizedtomultiplecells:
c = f c + i z cellstate (34)
t t t−1 t t
⊙ ⊙
n = f n + i normalizerstate (35)
t t t−1 t
⊙
h = o h˜ , h˜ = c n−1 hiddenstate (36)
t t ⊙ t t t ⊙ t
z = φ(z˜) , z˜ = W x + R h + b cellinput (37)
t t t z t z t−1 z
(cid:16) (cid:17)
i = exp ˜i , ˜i = W x + R h + b inputgate (38)
t t t i t i t−1 i
(cid:16) (cid:17) (cid:16) (cid:17)
f = exp ˜f ORσ ˜f , ˜f = W x + R h + b forgetgate (39)
t t t t f t f t−1 f
o = σ(o˜ ) , o˜ = W x + R h + b outputgate (40)
t t t o t o t−1 o
Here,thecellinputactivationfunctionφistanh,thehiddenstateactivationfunctionistheidentity.
φhelpsstabilizingtherecurrence.
Consideringexternalgradientcontributionδextfromsubsequentlayersandrecurrentgradientcontri-
ht
butionδR fromgradientsfromfuturestatesflowingoverthecellinteractionmatrixR,weobtainthe
ht
recursivebackwardpassofsLSTM,whereδ indicatesgradientswithrespecttoparameter/internal
a
variablea:
23δ = δext + δR (41)
ht ht ht
δ = f δ + o n −1 δ (42)
ct−1 t
⊙
ct t−1
⊙
t−1
⊙
ht−1
δ = f δ o c n−2 δ (43)
nt−1 t ⊙ nt − t ⊙ t−1 ⊙ t−1⊙ ht−1
δ = f′ c δ + f′ n δ (44)
˜ft t ⊙ t−1 ⊙ ct t ⊙ t−1 ⊙ nt
δ = i′ z δ + i′ δ (45)
˜it t⊙ t ⊙ ct t⊙ nt
δ = i φ′(z˜ ) δ (46)
z˜t t
⊙
t
⊙
ct
δ = o′ c n−1 δ (47)
o˜t t⊙ t ⊙ t ⊙ ht
(cid:88)
δ = W⊤δ (48)
xt g g˜t
g∈{f,i,z,o}
(cid:88)
δR = R⊤δ (49)
ht−1 g g˜t
g∈{f,i,z,o}
(cid:88)
δ⊤ = h δ⊤ , g i,f,z,o (50)
Rg t−1 g˜t ∈{ }
t
(cid:88)
δ⊤ = x δ⊤ , g i,f,z,o (51)
Wg t g˜t ∈{ }
t
withthederivativesoftherespectivegateactivationfunctioni′ = exp′(˜i ) = exp(˜i ) = i ,o′ =
t t t t t
σ′(o˜ ),andf′ =σ′(˜f )orf′ =f dependingontheforgetgateactivation. φ′(z)isthederivativeof
t t t t t
thecellinputactivationfunctionφ(z).
The matrices R , R , R , R are block-diagonal which is analogous to multiple heads in the
z i f o
mLSTM.Thisway,theparametersreducetod2/(N ),whereN isthenumberofheads,limitingthe
h h
cellinteractionstoindividualheads. Thisparameterefficientformulationofcellinteractionstogether
withtheexponentialgatingiscalledthenewmemorymixing. Finally, tostabilizethebackward
pass,weclipthemagnitudeofδR to10,asameanstoprohibitexplodinggradientsforlongcontext
ht
lengths.
ProofofEquivalenceforsLSTMStabilizedVersion. Thestabilizationstatem,seeEquation(15)
inthemainpaper,hasnogradient,andhencedoesnotinfluencetheothergradients. Wegobackto
thescalarversion(Equation8)hereforsimplicity. Were-definec(s)andn(s)asstabilizedcelland
t t
normalizerstates:
(cid:16) (cid:17)
c = c(s)exp m (52)
t t t
(cid:16) (cid:17)
n = n(s)exp m (53)
t t t
InsertingEquation15intoEquation8yields:
h˜(s) = c(s)/n(s) = (54)
t t t
(cid:16) (cid:17) (cid:16) (cid:17)
exp log(f )+ m m
c(s)
+exp log(i ) m z
= (cid:16) t t−1 − t (cid:17)t−1 (cid:16) t − t (cid:17)t (55)
exp log(f )+ m m
n(s)
+exp log(i ) m
t t−1 − t t−1 t − t
(cid:16) (cid:17)
exp log(f )+ m
c(s)
+exp(log(i ))z
t t−1 t−1 t t
= (cid:16) (cid:17) (56)
exp log(f )+ m
n(s)
+exp(log(i ))
t t−1 t−1 t
exp(log(f ))c +exp(log(i ))z
=
t t−1 t t
(57)
exp(log(f ))n +exp(log(i ))
t t−1 t
f c +i z
= t t−1 t t = c /n = h˜ (58)
t t t
f n +i
t t−1 t
24Therefore, since the loss solely depends on h , there’s no dependency on m , and consequently,
t t
no gradient exists for this stabilization state. Note that m can be chosen arbitrarily. We choose
t
m =max(log(f )+m ,log(i )),whichstabilizestheexponentialfunction. Onecanevenfind
t t t−1 t
m ,suchthatthenormalizerstaten canbeeliminated,butthisversionwasexperimentallyfoundto
t t
benumericallyunstableinthebackwardpass.
A.3 mLSTM
Throughoutthissection,1 RT denotesacolumnvectorofonesand1⊤ R1×T arowvectorof
∈ ∈
ones,whereT isthedimensionofthisvector.
RecurrentmLSTMBackwardPass. TherecurrentformulationofthemLSTMcellinEquation19
yieldsthefollowingbackwardpassrecurrence,whereδ indicatesgradientswithrespecttoparameter
a
orinternalvariableaandδextdenotesgradientsfromsubsequentlayers:
ht
δ⊤ =o δext (59)
h˜ t t ⊙ ht
q δ⊤
δ Ct−1 =f tδ Ct + max(cid:8)(cid:12)
(cid:12)t n−
⊤
t1 −1h˜
qt− t−1 1(cid:12) (cid:12),1(cid:9) (60)
q⊤ C⊤ δ
δ =f δ t−1 t−1 h˜ t−1 Ω(cid:0) n⊤ q (cid:1) q (61)
nt−1 t nt − max(cid:8)(cid:12) (cid:12)n⊤ t−1q t−1(cid:12) (cid:12),1(cid:9)2 t−1 t−1 t−1
δ⊤ =i k⊤δ⊤ (62)
vt t t Ct
δ⊤ =i (cid:0) v⊤δ +δ⊤(cid:1) (63)
kt t t Ct nt
C⊤δ q⊤C⊤δ
δ qt = max(cid:8)(cid:12) (cid:12)t n⊤ th˜ qt t(cid:12) (cid:12),1(cid:9) − max(cid:8)t (cid:12) (cid:12)n⊤ tt q th˜ (cid:12) (cid:12)t ,1(cid:9)2Ω(cid:0) n⊤ t q t(cid:1) n t (64)
(cid:88)
δ = W⊤δ (65)
xt g gt
g∈{q,k,v}
(cid:88)
δ⊤ = x δ⊤ , g q,k,v (66)
Wg t gt ∈{ }
t
(cid:88)
δ = δ , g q,k,v (67)
bg gt
∈{ }
t
δ =(cid:0) 1⊤(cid:0) C δ (cid:1) 1+1⊤(n δ )(cid:1) γ(cid:16) ˜f (cid:17) (68)
˜ft t−1
⊙
Ct−1 t−1
⊙
nt t
δ =(cid:0) 1⊤(cid:0)(cid:0) v k⊤(cid:1) δ (cid:1) 1+1⊤(n δ )(cid:1) exp(cid:16) ˜f (cid:17) (69)
˜it t t ⊙ Ct−1 t−1 ⊙ nt t
δ =h˜ σ′(˜o ) δ (70)
o˜t t
⊙
t
⊙
ht
andΩ(z)=Θ(z 1) Θ( z 1),Θ(z)beingtheHeavisidestepfunction. γ(z)iseitherσ′(z)
− − − −
orexp(z),dependingontheforgetgateactivation.
ParallelmLSTMForwardPass. ThemLSTMrecurrenceinEquations(19-27)canbereformulated
inaparallelform,whichisusedtospeeduptraining. Aftertrainingwecanstillusetherecurrent
formulationforfasttextgeneration.
Insteadofprocessingeachinputx Rdattimesteptsequentially,theparallelversionprocesses
t
alltimestepsofafullsequenceX
∈RT×d
atonce, whereT isthesequencelengthanddisthe
∈
headdimension. WepresenttheforwardpassofthemLSTMforasingleheadanddropthehead
dimensionforsimplicity.
25Let˜f RT betheforgetgatepre-activationsand˜i RT betheinputgatepre-activationsforafull
sequen∈ ce. Weconstructtheforgetgateactivationm∈ atrixF RT×T by
∈

0 forj >i

F = 1 forj =i , (71)
ij (cid:16) (cid:17)
(cid:81)i σ ˜f forj <i
k=j+1 k
andtheinputgatepre-activationmatrix˜I RT×T by
∈
(cid:26)
0 forj >i
˜I = . (72)
ij i fori⩽j
j
By applying the elementwise exponential input gate activation function naively, we obtain the
unstabilizedgateactivationmatrixD RT×T as
∈
D=F exp(˜I). (73)
⊙
Inordertoavoidoverflowduetotheexponentialfunctionweapplythesamestabilizationasinthe
recurrentsLSTM,seeEquation15. IntheparallelformulationofthemLSTMwegetanumerically
stablegateactivationmatrixD′ RT×T bytakingthelogarithmofDelement-wiseandsubtracting
∈
therow-wisemaximumvalueofDfromeachelement:
(cid:16) (cid:17)
D(cid:101) =logD=log F exp(˜I) =logF+˜I (74)
⊙
D′ =exp(D(cid:101) maxD(cid:101)) (75)
−
Giventhequeries,keysandvaluesQ,K,V RT×d,forafullsequencewecancomputeallhidden
∈
pre-activationstatesH(cid:101) RT×dinparallelfortheun-stabilizedversionby
∈
C(cid:101) QK⊤
H(cid:101) =CV , with C = (cid:16) (cid:17) , and C(cid:101) = D. (76)
max |(cid:80)T j=1C(cid:101)ij |,1 √d ⊙
Notethatweextractthe √1 factorforK explicitlyhereandfurtheron. Forthestabilizedversion
d
thisyields
C(cid:101)′ QK⊤
H(cid:101) =CV , with C = (cid:16) (cid:17) , and C(cid:101)′ = D′, (77)
max (cid:80)T C(cid:101)′ ,exp( maxD(cid:101)) √d ⊙
| j=1 ij| −
whereforbothversionsthehiddenpre-activationstatesH(cid:101) areidentical.
Withtheoutputgatepre-activationsO(cid:101) RT×dwecancomputethehiddenstatesH RT×dforall
∈ ∈
timestepsbyapplyingtheoutputgateinparallelforeachtimestepelement-wise:
H=σ(O(cid:101)) H(cid:101) . (78)
⊙
ThisgivestheparallelforwardpassofthemLSTMforafullinputsequenceX RT×d.
∈
ParallelmLSTMBackwardPass. WepresentthebackwardpassofthemLSTMforthestabilized
versiononly. Forcompletenesswesummarizetheforwardpassinthestabilizedversionbeforewe
presentthebackwardpass.
GiventheforgetgatematrixF RT×T,thelogarithmoftheforgetgatematrixF=logF RT×T,
andtheinputgatematrixI
R∈T×T asintroducedabove,togetherwiththequeries,keysan∈
dvalues
∈
26Q,K,V RT×d,wecanwritetheforwardpassofthemLSTMinthestabilizedversionas:
∈
D(cid:101) =F+˜I (79)
m=maxD(cid:101)ij , row-wisemaximum (80)
j
D′ =exp(D(cid:101) m1⊤) (81)
−
QK⊤
C(cid:101)′ = D′ (82)
√d ⊙
T
(cid:88)
b= C(cid:101)′ =C(cid:101)′1, row-wisesum (83)
ij
j=1
n=max(b,exp( m)) (84)
| | −
C =C(cid:101)′ (cid:0) n−11⊤(cid:1) (85)
⊙
H(cid:101) =C V (86)
Withthisforwardpasswecancomputethegradientsδ forallintermediateandinputvariablestothe
a
mLSTMforwardpassinthebackwardpass. Wedenotethegradientwithrespecttovariableaasδ .
a
Given the output gradient δ RT×d we can compute the backward pass for the intermediate
H(cid:101) ∈
gradientsas:
δ⊤ =Vδ⊤ (87)
C H(cid:101)
δ
n
= (cid:16) C(cid:101)′ (cid:0) n−21⊤(cid:1) δ C(cid:17) 1 (88)
− ⊙ ⊙
(cid:16)(cid:16) (cid:17) (cid:17)
= C(cid:101)′ δ
C
1 n−2 (89)
− ⊙ ⊙
(cid:26)
1 if b >exp( m)
δ b =sign(n) ⊙δ n ⊙ 0 oth| e| rwise − (90)
δ
=(cid:0) n−11⊤(cid:1)
δ , column-wisebroadcast (91)
C(cid:101)′,C ⊙ C
δ⊤ =1δ⊤, column-wisebroadcast (92)
C(cid:101)′,b b
δ =δ +δ (93)
C(cid:101)′ C(cid:101)′,C C(cid:101)′,B
QK⊤
δ D′ = √d ⊙δ C(cid:101)′ (94)
δ D(cid:101) =exp(D(cid:101) −m) ⊙δ D′ =D′ ⊙δ D′ (95)
Wedonotcomputethegradientsformastheycancelout(seetheproofintherecurrentsLSTM).
Withtheseintermediategradientsthegradientsforthelogarithmicforgetgatematrixδ RT×T,
theinputgatematrixδ RT×T,andthequeries,keysandvaluesδ ,δ ,δ RT×daF re∈ givenby
I Q K V
∈ ∈
δ =δ (96)
F D(cid:101)
δ =δ (97)
I D(cid:101)
δ =(cid:0) D′ δ (cid:1) K (98)
Q ⊙ C(cid:101)′ √d
δ =(cid:0) D′ δ (cid:1)⊤ Q (99)
K ⊙ C(cid:101)′ √d
δ =C⊤δ (100)
V H(cid:101)
Having computed the gradients for the logarithmic forget gate matrix δ , we can compute the
F
gradientsfortheforgetgatepre-activationsδ =(cid:2) δ ,δ ,...,δ (cid:3)⊤ RT.
˜f ˜f1 ˜f2 ˜fT
∈
27RecallthelogarithmicforgetgatematrixF=logFiscomputedby

forj >i
−
0∞
forj =i
F ij =logF ij = (cid:80)i logσ(cid:16) ˜f (cid:17) =(cid:80)i f forj <i . (101)
 k=j+1
(cid:124) (cid:123)(cid:122)
k
(cid:125)
k=j+1 k
=:fk
Withthesubstitutionf =logσ(˜f)wecomputethegradientsforthelogarithmicforgetgateactivations
δ =(cid:2) δ ,δ ,...,δ (cid:3)⊤ RT as
f f1 f2 fT ∈
k−1 T
(cid:88) (cid:88) (cid:0) (cid:1)
δ = δ , (102)
fk F ij
j=1i=k+1
δ =σ( ˜f ) δ , (103)
˜fk − k · fk
wherethelastequationmakesuseofthefollowing:
d
(logσ(x))= (1+exp(
x))−1
exp( x) ( 1)
dx − − · − · −
exp( x) 1 (104)
= − =
1+exp( x) 1+exp(x)
−
=σ( x)
−
Finally,wecomputetheinputgatepre-activations’gradientsδ = (cid:2) δ ,δ ,...,δ (cid:3)⊤ RT asthe
˜i ˜i1 ˜i2 ˜iS
∈
column-wisesumovertherowsoftheinputgatematrixδ :
I
T
(cid:88)
δ = (δ ) (105)
˜ik I ik
i=1
ThiscompletesthebackwardpassoftheparallelmLSTMforafullinputsequenceX RT×d.
∈
28A.4 DetailedBlockStructure
PF=¾
GeLU
PF=43 PF=43
GN
sLSTM
NH=4 NH=4 NH=4 NH=4
i f z o
NH=4 NH=4 NH=4 NH=4
Swish
Conv4
LN
Figure9: SchematicrepresentationofansLSTMBlock–postup-projection: Embeddedinapre-
LayerNormresidualstructure,theinputisoptionallypassedthroughacausalconvolutionofwindow
size4thatincludesaSwishactivationforinputandforgetgates. Then,forallinput,forgetandoutput
gatesi,f,o,andthecellupdateztheinputisfedthroughablock-diagonallinearlayerwithfour
diagonalblocksor“heads”. Thesediagonalblockscoincidewiththerecurrentgatepre-activations
fromthelasthiddenstate,whichcorrespondstoansLSTMwithfourheadsdepictedwiththecircular
arrows. TheresultinghiddenstategoesthroughaGroupNormlayer(Wu&He,2018)–ahead-wise
LayerNormforeachofthefourheads. Finally,theoutputisup-anddown-projectedusingagated
MLP,withGeLUactivationfunctionandprojectionfactor4/3tomatchparameters.
29PF=½
GN Swish
mLSTM
NH=4
f i
LSkip
q k v
BS=4 BS=4 BS=4
Swish
Conv4
PF=2 PF=2
LN
Figure 10: Schematic representation of an mLSTM block – pre up-projection: Embedded in a
pre-LayerNormresidualstructure,theinputisup-projectedfirstwithprojectionfactor2,oncefor
an externalized output gate and once as input for the mLSTM cells. The mLSTM cell input is
dimension-wisecausallyconvolved(kernelsize4),beforeenteringalearnableskipconnection. We
obtaininputq andk viablock-diagonalprojectionmatricesofblocksize4. Thevaluesv arefed
directly,skippingtheconvolutionpart. AfterthemLSTMsequencemixing,outputsarenormalized
viaGroupNorm(Wu&He,2018)–ahead-wiselayernormforeachofthefourheads. Finally,the
learnableskipinputisaddedandtheresultisgatedcomponent-wisewiththeexternaloutputgate.
Theoutputisdown-projected.
30B Experiments
TrainingSetup. Forallexperiments,weusePython13.11withPyTorch2.2.02,andCUDA12.13
onNVIDIAA100GPUs.
NearestNeighborSearchTask. Forthisauxiliarytask,weuserandomlysampledfeaturevectors
ofdimension2andunitnorm. Theattachedvalueisauniformlydistributedrandomnumberfrom
[0,1],leadingtoinputsvectorsofdimension3. Thefirstfeaturevectorservesassearchkey,withthe
firstvaluebeingignored. Thenthemodelhastopredictthevalueofthenearestneighborsofarinthe
sequence. Wetrainon8192sequencesofcontextlengthupto64(uniformlysampled)andvalidate
on8192differentsamples. Allmodelshavetwoblocksandembeddingdimension128. Weusea
dropoutof0.1,10%linearwarm-upstepsandcosinedecayto1e-7for100ktotaltrainingsteps. We
sweepoverlearningrates1e-4,1e-3,1e-2,1e-1and5seedseach. ThereportedvaluesinFigure2are
meanvaluesforthebestlearningrateand99%confidenceintervals. NotethatLSTMrequiresvery
highlearningrates,whereasTransformers(Llama)performbestatthesmallestlearningrate. The
xLSTM[0:1]reachessimilarperformanceacrossalllearningrates.
Wikitext-103RareTokenPrediction. Forthisexemplaryexperimentonraretokenprediction,we
trained125M-sizedmodelsonWikitext-103(Merityetal.,2017). Allmodelshaveanembedding
dimensionof768inapostup-projectionstructureof12residualblocks. TheTransformermodel
(Llama)usesMulti-HeadAttention,forwhatiscalledLSTMtheMulti-HeadAttentionisreplacedby
anLSTMandthexLSTM[1:0]containsmLSTMlayerswithmatrixmemory. Modelsweretrained
withmaximumlearningrate1e-3,4kstepslinearwarm-upandcosinedecayforintotal50ksteps,
usingabatchsizeof256andcontextlengthof512. Weusethevalidationperplexityasastopping
criterionandevaluateonthetestset.
B.1 SyntheticTasksandLongRangeArena
B.1.1 TestofxLSTM’sExponentialGatingwithMemoryMixing.
WeevaluatexLSTMonasuiteofformallanguagetaskstotestitsexponentialgatingandmemory
mixingmechanism.
Formallanguagesprovideaframeworktoprobethegeneralizationcapabilitiesofmodels. Theyallow
tospecificallytestdifferentexpressivitylevels,e.g.alongtheChomskyhierarchy. Typicallanguage
modelarchitecturesdonotnecessarilyfitperfectlyinthesehierarchies(Delétangetal.,2023)—
neverthelesstheselanguagesallowtoillustratedifferencesingeneralizationexpressivitybetween
differentarchitectures. OurevaluationtasksareheavilybasedontheworkofDelétangetal.(2023).
ExperimentSetup. Thedifferentformallanguagetasksintheexperiment(seeindividualtasks
descriptionbelow)encompassdifferentlevelsoftheChomskyhierarchyaswellasadditionalcounting
andmemory-focusedtasks. Weusedifferentlengthspersample,whichallowsustovalidateina
lengthextrapolationsetting. Wetrainonavaryingtasklengthupto40. Theevaluationisdonefor
tasklengthsbetween40and256asweareonlyinterestedinthe“taskgeneralizationcapabilities“of
themodels.
Inallexperiments,weusetwoblocks(orlayersforthepureLSTM)forallmodels. Wecompare
Llama,Mamba,Retention,Hyena,RWKV-4,RWKV-5,RWKV-6,LSTM,xLSTM[0:1],xLSTM[1:0]
andxLSTM[1:1]. ThesLSTMblockisusedwithoutaconvolutionandwithnormalweightinitializa-
tion. LSTM(Block)referstoanarchitecturewhereavanillaLSTMisusedinsteadofself-attention
insideaTransformerblock.
All models are trained with 3 different learning rates (1e-2, 1e-3, 1e-4), each with two seeds.
Batchsizeis256—cosineannealing(minlr: 1e-5)with10%warm-upstepsisapplied. Weuse
AdamW(Loshchilov&Hutter,2019)andaweightdecayof0.1fortraining. Ineachexperimentwe
trainfor100ksteps—thesamplesaregeneratedrandomly,however,allexperimentsaretrainedand
evaluatedonthesamesamples.
1https://python.org
2https://pytorch.org
3https://docs.nvidia.com/cuda/archive/12.1.0/
31Context Deterministic
Sentsitive ContextFree
Reverse Stack
OddsFirst String Manipulation Repetition Set
Llama 0.07 0.06 0.11 0.08 0.04
±0.0 ±0.0 ±0.01 ±0.0 ±0.0
Retention 0.03 0.11 0.03 0.02 0.02
±0.0 ±0.0 ±0.0 ±0.0 ±0.0
RWKV-4 0.08 0.12 0.2 0.1 0.1
±0.0 ±0.01 ±0.0 ±0.0 ±0.02
Hyena 0.04 0.15 0.07 0.07 0.03
±0.0 ±0.0 ±0.0 ±0.0 ±0.0
RWKV-5 0.08 0.09 0.16 0.16 0.13
±0.01 ±0.01 ±0.0 ±0.0 ±0.01
RWKV-6 0.13 0.11 0.23 0.15 0.19
±0.01 ±0.0 ±0.01 ±0.01 ±0.01
xLSTM[0:1] 0.09 0.14 0.13 0.09 0.17
±0.01 ±0.03 ±0.01 ±0.01 ±0.01
Mamba 0.08 0.13 0.21 0.15 0.12
±0.01 ±0.02 ±0.0 ±0.01 ±0.0
LSTM 0.08 0.17 0.25 0.15 0.18
(Block) ±0.01 ±0.02 ±0.02 ±0.01 ±0.01
xLSTM[0:1] 0.09 0.14 0.13 0.09 0.17
±0.01 ±0.03 ±0.01 ±0.01 ±0.01
xLSTM[1:0] 0.15 0.22 0.25 0.28 0.17
±0.03 ±0.02 ±0.03 ±0.0 ±0.01
xLSTM[1:1] 0.08 0.2 0.17 0.09 0.15
±0.0 ±0.01 ±0.0 ±0.0 ±0.03
Figure11: Supplementaryresultsgivenbyscaledaccuracyofdifferentmodelsatsolvingformal
languagetasks. TasksaregroupedbytheChomskyhierarchy.
Additional Formal Language Results. Figure 11 showcases supplementary results on formal
languagetask,detailingtaskswherenomodelattainedaminimumscaledaccuracyof0.3. Although
nomodelachievesproperextrapolationofthetasktoalargercontextlength,xLSTMperformsbest
amongtheevaluatedmodels.
Individual Task Description. The majority of tasks are based on Delétang et al. (2023). We
providethevocabularysize V andtherandomaccuracys (foraccuracyscaling),usedinthe
rand
| |
evaluation. Asweevaluatedifferenttasklengthseachtaskhasapaddingtokenwhichisusedtopad
thesequencetothegivencontextlength. InListing1thereisanexampleforeachtask.
• BucketSortGivenastringoftokensofasortedalphabet,computethesortedstring.
V =11 s = 1
| | rand |V|−1
• CycleNavGivenastringof“movementtokens”(+1, 1,STAY)computetheendposition
−
oftheagentwithstartposition0. Thepositionmustbecomputedmodulothemaximum
position.
V =9 s = 1
| | rand |V|−4
• EvenPairsGivenabinarystringofaandbtokens,computewhetherthenumberofaband
baiseven. Thistaskcanbesolvedbycheckingifthefirstandlasttokenofthestringare
equal.
V =3 s =0.5
rand
| |
• Majority Given a string of tokens, compute the token that occurred most often in the
sequence.
V =64 s = 1
| | rand |V|−1
• MajorityCountGivenastringoftokensofanorderedalphabet. Computethecountof
thetokenthatoccurredmostofteninthesequence. Ifthecountexceedsthevocabsize,the
highestvocabtokenshouldbeoutputted.
V =64 s = 1
| | rand |V|−1
• MissingDuplicateGivenastringoftokens. Thestringisrepeatedbutoneofthetokensis
maskedintherepetition. Outputthetokenthatismasked.
V =11 s = 1
| | rand |V|−2
32• ModArithmetic(w/oBrackets)Calculatetheresult—modulothemaxnumber—ofthe
arithmeticoperationsinthecontext. Themaximumnumberisthevocabularysizeminusthe
numberofspecialtokens(+,-,*,=,[PAD]).
V =10 s = 1
| | rand |V|−5
• ModArithmetic(wBrackets)Calculatetheresult—modulothemaximumnumber—of
thearithmeticoperationsinthecontext. Themaximumnumberisvocabularysizeminusthe
numberofspecialtokens(+,-,*,=,(,),[PAD]).
V =12 s = 1
| | rand |V|−7
• OddsFirstAnstringoftokenst ,t ,t ,...t isgiven. Outputalltokenswithandoddindex
1 2 3 n
(t ,t ,...)thenthetokenwithanevenindex(t ,t ,..) . Apartfromthatkeeptheorderingof
1 3 2 4
theinitialstring.
V =12 s = 1
| | rand |V|−2
• ParityGivenabinarystringofaandbtokens,computeifthenumberofb‘siseven. If
thenumberisevenoutputaotherwiseb. Thisisequivalenttosequentiallycalculatingthe
half-addersum.
V =3 s =0.5
rand
| |
• RepetitionGivenastringoftokens—repeatit.
V =12 s = 1
| | rand |V|−2
• ReverseStringGivenastringoftokens—repeatitinreverseorder.
V =12 s = 1
| | rand |V|−2
• StackManipulationAninitialstackcontentisgiven,followedbyasequenceofpushand
popoperations. Computethestackcontentaftertheoperations
V =11 s = 1
| | rand ⌊|V|−3⌋
2
• SetGivenastringoftokens,computetheorderedsetofthetokens. Keeptheorderingso
thattokensthatoccurredfirstarealsooutputtedfirst.
V =128 s = 1
| | rand |V|−2
• Solve Equation Given is an equation with the operators {+,-,*,=,(,)}, number, and an
unknown variable x. Compute the value of the variable modulo the max number. The
maximumnumberisvocabularysizeminusthenumberofspecialtokens(+,-,*,=,(,),[PAD],
[ACT]).
V =14 s = 1
| | rand |V|−9
33Bucket Sort
Sequence: 1 4 8 6 1 1 1 4 6 8
Cycle Nav
Sequence: STAY +1 -1 +1 STAY +1 +1 +1 -1 P3
Even Pairs
Sequence: a b b a a b a b a a
Majority
Sequence: 1 7 6 4 3 8 1 7 2 1
Majority Count
Sequence: 1 7 6 4 4 8 1 7 2 2
Missing Duplicate
Sequence: 4 8 6 2 5 4 8 6 2 [MIS] 5
Mod Arithmetic (w/o Braces)
Sequence: 0 - 4 + 0 - 2 = 4 [PAD]
Mod Arithmetic (w Braces)
Sequence: ( ( ( 2 ) * - 2 ) - ( - 4 - 2 ) ) = 2
Odds First
Sequence: 2 7 3 2 6 9 [ACT] 2 3 6 7 2 9
Parity:
Sequence: a b b a a b a b
Repetition
Sequence: 2 4 8 6 2 [ACT] 2 4 8 6 2
Reverse String
Sequence: 2 4 8 6 2 [ACT] 2 6 8 4 2
Stack Manipulation
Sequence: ST1 ST1 ST3 POP POP PS3 PS3 [ACT] ST1 ST3 ST3
Set
Sequence: 8 6 6 3 5 4 5 3 [ACT] 8 6 3 5 4
Solve Equation:
Sequence: ( ( ( 2 + 0 ) + - x ) - ( 1 ) ) = 2 [ACT] 2
Listing1: Examplesoftheformallanguagetasks. Redtokensareevaluatedforlossandaccuracy
metrics,butarepaddedfortheinput. Thetokensareillustratedinawaythatallowseasysemantic
interpretationforthegiventask—hence,sometokensarerepresentedbymultiplecharacters.
B.1.2 TestofxLSTM’sMemoryCapacitiesonAssociativeRecallTasks.
WetestthememorycapacityofxLSTMwiththeMulti-QueryAssociativeRecalltaskproposedby
Aroraetal.(2023). Figure12illustratesthebasictasksetup.
WhyMulti-QueryAssociativeRecallforMemoryTestsofLLMArchitectures. Associative
Recall(AR),theabilitytoretrieveaspecificvalue(information)associatedwithagivenkey(infor-
mation),constitutesakeycapabilityforLLMtoperformwell(Polietal.,2024;Aroraetal.,2023;
Olssonetal.,2022). Especiallyitsqualityofin-contextlearningseemstobestronglyconnectedto
thiscapability(Olssonetal.,2022). Aroraetal.(2023)attributeperformancegapsbetweenearly
non-TransformerandTransformerlanguagemodelsspecificallytoperformancegapsinassociative
recall. TheyarguethatpriorARevaluationsfallshortofcapturingthesedifferencesandpropose
MQAR,whichcanshowtheARperformancedifferencesthattranslatetoperformancedifferences
inlanguagemodelingperformance. Hence,MQARisespeciallysuitabletoanalyzethememory
capacityofLLM.Transformer(e.g.Llama)modelscanbeseenasthegoldstandardforthistaskas
theirmemoryisexponentialinthecodingdimension(Ramsaueretal.,2021).
ExperimentSetup. Therearetworelevantvariablesthatdeterminedifferentexperimentalsetups.
(1)ContextLength(CL):Lengthofthesequenceofonesample—thisinfluencesthedistances
betweenthekey-valuedefinitionandtherecall. (2)NumberKey-ValuePairs(KV):Influenceshow
manykey-valuepairsthemodelneedstokeeptrackof. Thevocabularysizeisalways8192.
34Inallexperiments,weusetwoblocks(orlayersforthepureLSTM)forallmodels. LSTM(Block)
model refers to an architecture where a vanilla LSTM is used instead of self-attention inside a
Transformerblock.
For each task setup, we train each model with 4 different learning rates (batch size > 24: {1e-2,
2.15e-3, 4.6e-4, 1e-4}, batch size 24: {1e-3, 2.2e-4, 5e-5, 1e-5}). The batch size (BS) changes
dependingonthecontextlength(CL)(CL=64/128: BS=512;CL=256: BS=256;CL=756: BS=128;
CL=1024: BS=96;CL=2048: BS=24). Wevarytheembeddingdimension(ModelDim)between
differentexperiments–differentnumbersofheadsareusedaccordingly. Foreachexperiment,we
generate100,000trainingsamples(validation: 3,000samples)andtrainfor64epochs. Weapply
cosineannealing(minlr: 1e-4and1e-5)with10%warm-upsteps. WeuseAdamW(Loshchilov&
Hutter,2019)andaweightdecayof0.1fortraining.
Weconductthreedifferentexperiments:
• MQAR-Experiment 1 evaluates, in the same fashion as Arora et al. (2023), a vari-
ety of models (Llama, Mamba, Mamba (noWT) - i.e. without weight tying, Reten-
tion, Hyena, H3, RWKV-4, RWKV-5, RWKV-6, LSTM, LSTM (Block), xLSTM[0:1],
xLSTM[1:0]andxLSTM[1:1])onincreasingtaskdifficultybyincreasingthecontextlength
andnumberofkey-valuepairssimultaneously. Webenchmarkthreeparametersettings:
CL,KV={(64,4),(128,8),(256,16)}.
• MQAR-Experiment 2 increases the task difficulty notably and goes beyond previous
evaluationsonthistask. Weindividuallyscalethecontextlength(CL={756,1024,2048})
andthekey-valuepairs(KV={48,96,256})andevaluateallcombinations. Thisexperiment
especially probes the memory capacity because the number of key-value pairs is high.
Toreducethecomputationalburdenweonlyevaluatemodelsthatperformflawlesslyin
Experiment1—additionallyweevaluateTransformeronlyinthehardestsetting(CL=2048)
assanitycheck,becausenoperformancedecreaseisexpected.
• MQAR-Experiment3analyzeswhethertheARcapabilitylearnedonacertaincontext
lengthextrapolatestobiggercontextlengths. ForeachKVsettingofExperiment2,weuse
themodels(weselectthe3biggestmodeldimensions)trainedonCL=2048andevaluate
biggercontextlengths(CL={4096,6144,8192}).
ExtendedResults. TheresultofExperiment1canbefoundinFigure13. Inaccordancetothe
results of Arora et al. (2023) H3, Hyena, RWKV-4 fail to solve the task with a smaller model
dimension. Incontrast,xLSTM[1:1],xLSTM[1:0],Mamba,RWKV-5andRWKV-6areabletosolve
thesesettingsforallmodeldimensions. ThecomparisonofxLSTM[0:1]withbothoriginalLSTM
variantsindicatesthattheexponentialgatingmechanismimprovestheARcapabilitiesofthemodel.
However, bothfallshortbecauseofthereducedmemorycapacitycomparedtoxLSTM[1:1]and
xLSTM[1:0].
TheresultsofExperiment2arepresentedinFigure14. Scalingthecontextlengthhasalowimpact
ontheperformanceofthemodels. However,whilexLSTM[1:1]andxLSTM[1:0]shownoclear
decay, both RWKV variants slightly, but consistently lose performance with increasing context
lengths. Thevaryingnumberofkey-valuepairs,whichmainlyprobesthememorycapacityofthe
non-Transformermodels,hasamorenotableimpactacrossallmodels.RWKV-5seemstooutperform
RWKV-6. ThelatterfailstolearnthetaskatallinsomeKV=256settings. OverallxLSTM[1:1]isthe
best-performingnon-Transformermodel—suggestingthatitprovidesenhancedmemorycapacity,
alsoinlongcontexts.
Figure15showstheextrapolationresultsfromExperiment3. ForxLSTM[1:1],xLSTM[1:0],and
Mambathemodelperformancedoesnotchangeintheextrapolationsetting. TheRWKVmodels
(especiallyRWKV5)degradeslightlywithincreasingcontextlength. xLSTM[1:1]performsbest,as
itmaintainsitssuperiorperformanceofExperiment2.
4Thekeysaredistributedonthe“evaluationpart”ofthesequencegivenapower-lawdistribution.Thisis
motivatedbysimilarstructuresinnaturallanguagetext.
35Target
Input
KV = 4 / CL = 18
Figure12: IllustrationoftheMQARtask. Colorpairsrepresentkey-valuepairs(keyshavedarker
shade). Thefirstpartofthesequencedefinesthekey-valuepairsfortherespectivesample. Afterthat,
thekeysappearrandomlyaccordingtoapowerlawdistribution4. Greytokensintheinputsequence
representazerotoken. The“target”sequencecontainsthevalueaftertherespectivekeyappearance
—therestofthetokensareignoredfortheaccuracyandlosscalculation. Themodelmustpredictthe
valuetokensgiventherespectivekey.
B.1.3 TestofxLSTM’sLongRangeCapabilitiesontheLongRangeArena.
WeassesstheperformanceofxLSTMacrosstasksintheLongRangeArenabenchmark(Tayetal.,
2021),examiningitsabilitytoeffectivelyhandlelongercontextlengthsanddiversedatatypes.
OurexperimentsonLongRangeArenabenchmarkarecomposedoffivetasks:
• Retrieval: Thetaskistopredictiftwodocumentshaveacitationlink. Thedatasetoftext
documentsisderivedfromtheACLAnthologyNetwork(Radevetal.,2009).
• ListOps: Thisisasetofmodulararithmetictasksincludingbracketsandlistsofnumbers,
usingtheoperationsMIN,MAX,MEDIANandSUMMOD(modularsum). Aparticularexample
is: [MAX 4 3 [MIN 2 3 ] 1 0 [MEDIAN 1 5 8 9, 2]] 5
→−
• Image: ThistaskisbasedonaversionoftheCIFARdataset(Krizhevsky,2009),where
imagesaretransformedtoasequenceofpixelsandthissequencehastobeclassifiedintothe
usualCIFARclasses. Wetestbothagray-scale(G-Image)andRGB(RGB-Image)version
ofthisdataset,asOrvietoetal.(2023)usescoloredimagescontrarytothestandardsetup.
• Pathfinder: Theinputforthistaskisa32x32gray-scaleimage,givenaspixelsequence,
with two dots and several curved lines on it. The task is to predict if the two dots are
connectedbyanyofthelines(Linsleyetal.,2018).
WeomittheTextclassificationtask(Maasetal.,2011),asthelanguagemodelingexperimentsalready
testthiskindofdata,andthePathfinder-XversionofPathfinder.
ExperimentSetup. ThearchitecturesthataretestedinthisexperimentcompriseLLama,Mamba,
LSTM,RWKV-4,andxLSTM.LSTM(Block)referstoanarchitecturewhereavanillaLSTMisused
insideapostup-projectionblock(likeTransformerwithattentionreplacedbyLSTM).ForxLSTM
wechoosethebestperformingofxLSTM[0:1]orxLSTM[1:0]onthevalidationset,specificallythe
formerfortheImagetasksandthelatterforallotherones.
WeusethehyperparametersettingsoftheS5model(Smithetal.,2022)andLinearRecurrentUnit
model(Orvietoetal.,2023),withadditionalhyperparamtersearchonlearningratesandschedulers
for all models. We use two different schedulers: Linear Warm-up Cosine Annealing and Linear
Warm-upCosineAnnealingwithRestarts. Bothlearningrateschedulerswereevaluatedwithlearning
ratesof1e-3,6e-4and1e-4. Forthesecondscheduler,thenumberofrestarts(R)issetto3. The
modelhyperparametersforeachdatasetaredisplayedinTable5.
Results. Table6showstheresultofexperimentsontheLongRangeArenabenchmark. xLSTM
demonstratesconsistentstrongperformanceonallofthetasks,suggestingthattheproposedarchitec-
tureisremarkablyefficientinhandlingdifferentaspectsoflongcontextproblems.
36Llama RWKV-4 Retention xLSTM[0:1] LSTM(Block)
Mamba RWKV-5 Hyena xLSTM[1:0] LSTM
Mamaba(noWT) RWKV-6 H3 xLSTM[1:1]
ContextLength=64 ContextLength=128 ContextLength=256
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.8
0.6
0.4
0.2
0.0
32 64 128 256 512 32 64 128 256 512 32 64 128 256 512
ModelDim ModelDim ModelDim
Figure13: ResultofMQAR-Experiment1. Thecolumnsshowdifferenttasksettings(contextlength
andkey-valuepairs). Therowsgrouprelatedmodelsforbetterclarity. Thex-axisgivesthemodel
sizeandthey-axisthevalidationaccuracy.
37
remrofsnarT
ylimaF-MTSL)x(
abmaM
VKWR
srehtO
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccALlama Mamba RWKV-5 RWKV-6 xLSTM[1:0] xLSTM[1:1]
Key-ValuePairs=48 Key-ValuePairs=96 Key-ValuePairs=256
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.8
0.6
0.4
0.2
0.0
32 64 128 256 512 32 64 128 256 512 32 64 128 256 512
ModelDim ModelDim ModelDim
Figure14: ResultofMQAR-Experiment2. Thecolumnsandrowscorrespondtodifferentnumbers
ofkey-valuepairsandthecontextlengthrespectivly. Thex-axisgivesthemodelsizeandthey-axis
thevalidationaccuracy.
Embedding Batch Training
Task #Blocks
Dim Size Steps
Retrieval 6 128 64 100k
ListOps 8 128 32 80k
Pathfinder 6 192 64 500k
G-Image 6 512 64 180k
RGB-Image 6 512 64 180k
Table5: LongRangeArenamodelhyperparameters. Thesearethemodelhyperparametersusedin
eachoftheLongRangeArenatasks. Foreachmodelweusedthebestlearningrateandthebetterof
thetwolearningrateschedulers.
38
657=htgneLtxetnoC
4201=htgneLtxetnoC
8402=htgneLtxetnoC
ycaruccA
ycaruccA
ycaruccAMamba RWKV-5 RWKV-6 xLSTM[1:0] xLSTM[1:1]
Key-ValuePairs=48 Key-ValuePairs=96 Key-ValuePairs=256
1.00
0.75
0.50
0.25
0.00
1.00
0.75
0.50
0.25
0.00
1.00
0.75
0.50
0.25
0.00
128 256 512 128 256 512 128 256 512
ModelDim ModelDim ModelDim
Figure15: ResultofMQAR-Experiment3(Extrapolation). Allevaluatedmodelsweretrainedon
contextlength2048andthenumberofkey-valuepairsgivenbythecolumnsoftheplot. Therows
showthedifferentcontextlengthsusedintheevaluation. Thex-axisgivesthemodelsizeandthe
y-axisthevalidationaccuracy.
Retrieval ListOps Pathfinder G-Image RGB-Image Ranking
acc↑ acc↑ acc↑ acc↑ acc↑ acc↑
RandomBaseline 0.500 0.100 0.500 0.100 0.100
Llama 0.845 0.379 0.887 0.541 0.629 5.2
Mamba 0.902 0.325 0.992 0.689 0.765 2.2
RWKV-4 0.898 0.389 0.914 0.691 0.757 3.0
LSTM X 0.275 X 0.675 0.718 5.4
LSTM(Block) 0.880 0.495 X 0.690 0.756 3.4
xLSTM 0.906 0.411 0.919 0.695 0.761 1.6
Table6: LongRangeArenatestaccuracy. Boldhighlightsthebestperformingmodel,underlined
thesecondbest. Xdenotesmodelsthatfailtooutperformrandombaselines. xLSTMisthebestof
xLSTM[1:0],xLSTM[0:1]basedonvalidationdatasetaccuracy.
39
6904=htgneLtxetnoC
4416=htgneLtxetnoC
2918=htgneLtxetnoC
ycaruccA
ycaruccA
ycaruccAB.2 MethodComparisonandAblationStudyonSlimPajama(15B)
GeneralTrainingProcedure. WetokenizeourdatasetsusingtheHuggingFaceGPT-2tokenizer
(Radfordetal.,2019;Brownetal.,2020)5 andusethistokenizerforallmodelsinthispaper. In
general,wetrytofollowBrownetal.(2020)forthegeneraltrainingsetup,i.e. wechoosecontext
length2048andbatchsizes256or512forourmodels. WeusetheAdamW(Loshchilov&Hutter,
2019) optimizer with beta parameters (β , β )=(0.9, 0.95) and an epsilon parameter of 1e-5. As
1 2
learning rate scheduler we use a linear warm-up with 750 steps and cosine decay to 10% of the
peaklearningrate. Weapplyaweightdecayof0.1toallourmodelsandalwaysexcludethetoken
embedding matrix from weight decay. If not specified otherwise, we do not tie the weights of
thetokenembeddingandthelanguagemodelhead. Forparallelization,weusePyTorchFSDPin
SHARD_GRAD_OPmodewithmixedprecisioninbfloat16,whereapplicable. Forsmallmodelswe
useNO_SHARD.Wekeeptheweightsinfloat32andreducethegradientsacrossGPUsinfloat32.
Weusetorch.compiletospeedupmodels,exceptforTransformermodelsastheirtrainingcurves
didnotmatchthenon-compiledversions. ForxLSTM[7:1],weusepositions[3,5,7,40,42,44]for
sLSTM-basedblocks,exceptforthe125Msize,whereweuse[3,20](thisisactuallya[11:1]ratio).
#Params PeakLR PeakLR
Model EmbeddingDim #Blocks #Heads/HeadDim
M (15B) (300B)
RWKV-4 768 12 - 169.4 3e-3 6e-4
Llama 768 12 12/64 162.2 3e-3 3e-3
Mamba 768 24 - 167.8 3e-3 3e-3
xLSTM 768 24 4/384 163.8 1e-3 1.5e-3
RWKV-4 1024 24 - 430.5 1-e3 4e-4
Llama 1024 24 16/64 406.6 1.5e-3 1.5e-4
Mamba 1024 48 - 423.1 1.5e-3 1.5e-3
xLSTM 1024 48 4/512 409.3 1e-3 7.5e-4
RWKV-4 1536 24 - 891.0 1e-3 2.5e-4
Llama 1536 24 16/96 834.1 1.25e-3 1.25e-4
Mamba 1536 48 - 870.5 1.25e-3 1.25e-3
xLSTM 1536 48 4/768 840.4 9e-4 6.25e-4
RWKV-4 2048 24 - 1515.2 1e-3 2e-4
Llama 2048 24 32/64 1420.4 1e-3 1e-3
Mamba 2048 48 - 1475.3 1e-3 1e-3
xLSTM 2048 48 4/1024 1422.6 9e-4 5e-4
RWKV-4 2560 32 - 2984.8 8e-4 -
Llama 2560 32 32/80 2779.5 8e-4 -
Mamba 2560 64 - 2897.2 8e-4 -
xLSTM 2560 64 4/1280 2788.3 8e-4 -
Table7: Peaklearningratesandmodeldimensionsforscalinglawplots.
DetailsonComparisontoOtherMethods. Forthemodelcomparisonon15Btrainingtokens
of SlimPajama we train all models with context length 2048 and batch size 256. We use a peak
learningrateof1e-3forallmodelsforcomparability. Thelearningratedecaysover30ktraining
steps. Themodelsarecomparedafteroneepochattrainingstep28170. Asmodelimplementations
weusetheoriginalrepositories’codeforMamba(Gu&Dao,2023)6,RWKV-5,RWKV-6(Peng
et al., 2024) 7. For RWKV-4, we use a cleaned and validated re-implementation based on the
original repo and kernels (Peng et al., 2023). For HGRN (Qin et al., 2023), GLA (Yang et al.,
2023), HGRN2 (Qin et al., 2024) we use the a re-implementation by the authors of GLA (Yang
5https://huggingface.co/docs/transformers/en/model_doc/gpt2
6https://github.com/state-spaces/mamba
7https://github.com/BlinkDL/RWKV-LM/
40
M521
M053
M067
B3.1
B7.2et al., 2023; Yang & Zhang, 2024) 8. For GPT-3 and Llama-like Transformers, we use our own
implementationsbasedonPyTorch. NotethatforallxLSTMs,Transformers,MambaandRWKV-4,
weuseMixedPrecisiontrainingwithbfloat16andweightsinfloat32precision,whileresorting
tofullbfloat16precision(weightsandoperations)forallothermodelsduetotheircustomkernels
thatforceoneprecisioninternally. Followingthegeneraltrainingprocedureweusetorch.compile
for all models, except for models using the flash-linear-attention (Yang & Zhang, 2024)
librarybecauseofcompilationproblems.
General Details on Ablation Studies. We follow our general training procedure and train all
modelswithcontextlength2048,batchsize256andpeaklearningrate1e-3. Wereportperplexity
valuesonthevalidationset.
Additional Ablation Study on Matrix Memory. As default block configuration we use the
mLSTMinthepreup-projectionblock(seeFigure10)andthesLSTMinthepostup-projectionblock
(seeFigure9). InthisexperimentwestudycombinationofmLSTMwithdifferentblockvariants
usingthexLSTM[1:0]architecture. WecomparethemLSTMinapostup-projectionblock(see
Figure3and9)withReLU2 activationfunctionandnon-gatedfeed-forwardnetworktomLSTM
inapreup-projectionblockwithandwithoutadimension-wisecausalconvolution. Table8shows
thatthematrixmemorybenefitsfromthepreup-projectionblockstructure,andthattheconvolution
withinthisblockisimportant.
Embedding #Params SlimPajama
Model Details #Blocks
Dim M (15B)ppl↓
PostUp-ProjectionBlock(ReLU2) 24 1024 430.4 13.90
xLSTM[1:0] PreUp-ProjectionBlock,NoConvolution 48 1024 408.8 15.41
PreUp-ProjectionBlock,WithConvolution 48 1024 409.3 13.43
Table8: MatrixMemoryvariants. Westudydifferentconfigurationsforthematrixmemory. Matrix
memoryinthepreup-projectionblockperformsbestandgivesxLSTM[1:0]. Notably,itseemsthat
thedimension-wisecausalconvolutionwithinthepreup-projectionblockisimportant.
DetailsonnewxLSTMComponentsAblationStudy. InTable2(top),weshowourmodifications
to the vanilla LSTM that transform the vanilla LSTM into the xLSTM. We start with a large
default PyTorch LSTM with 24 layers and 1536 hidden size. Due to a lack of skip-connections
andLayerNorms,vanillaLSTMsofthissizearenottrainable. Wethenaddskip-connectionsand
pre-LayerNormsbeforeeachLSTMlayercorrespondingtoaresidualarchitecture. Thisenables
trainingforLSTMsatthisscale. ReplacingeverysecondLSTMlayerbyanon-gatedfeed-forward
networkwithGeLUactivationfunction(similartoVaswanietal.),whichcorrespondstothepost
up-projectionbackbone(seeFigure3)furtherboostsperformance. AddingExponentialGatingtothis
architectureyieldsthesLSTMasdepictedinFigure9,withanotherlargeperformanceimprovement.
Finally,addingthebestMatrixMemoryvariantfoundinTable8byreplacingsomesLSTMblocks
withthemLSTM(seeFigure10)givesxLSTM[7:1]withthebestperformance.
Details on Gating Technique Ablation Study. In Table 2 (bottom), we investigate the effect
of trainable and input-dependent gates for mLSTM. The results show that, in contrast to other
methods(Katharopoulosetal.,2020;Sunetal.,2023;Qinetal.,2023;Katsch,2023;Yangetal.,
2023;Qinetal.,2024;Pengetal.,2024),havingthegatesbothlearnableandinputdependentgives
thebestresults.
DetailsonScalingExperiments. Wefollowourgeneraltrainingprocedure(seeparagraphabove)
andtrainallmodels,includingthe1.3Band2.7Bmodelsizes,withcontextlength2048andbatch
size256. WeusethepeaklearningratesfromTable7.
8https://github.com/sustcsonglin/flash-linear-attention
41B.3 xLSTMLargeLanguageModels–SlimPajama300B
GeneralTrainingProcedure. WeusethesamegeneraltrainingprocedureasinSectionB.2with
peaklearningratesfromTable7. Allmodelsaretrainedwithcontextlength2048. The125M,350M
and760Mmodelsaretrainedwithbatchsize256for600ktrainingsteps,whereasthe1.3Bmodels
aretrainedwithbatchsize512for300ktrainingsteps. Wekeepthesamelearningratescheduler
acrossallmodels.
Details on Downstream Evaluation. We use the LM Evaluation Harness from
EleutherAI (Sutawika et al., 2023) for evaluating the following tasks that measure common
sensereasoning: LAMBADA(OpenAIversioninLMEvaluationHarness)(Papernoetal.,2016),
HellaSwag(Zellersetal.,2019),PIQA(Bisketal.,2020),ARC-challenge,ARC-easy(Clarketal.,
2018),WinoGrande(Sakaguchietal.,2021). Thisselectionofdownstreamtasksisinspiredby(Gu
&Dao,2023).
FollowingGu&Dao(2023),wereportaccuracyforLAMADA,WinoGrande,PIQA,andARC-easy,
andaccuracynormalizedbysequencelengthforHellaSwagandARC-challenge.
Weevaluateallmodelsinfullfloat32,fullbfloat16andbfloat16MixedPrecisionwithweights
infloat32. Foreachmodelweselectthebestvaluerespectively.
DetailsonPALOMA. Weuse16outofthe18datasourcesofthePALOMAdataset(Magnusson
etal.,2023). WeuseC4(Raffeletal.,2019), MC4-EN(Xueetal.,2021), Wikitext-103(Merity
etal.,2017),PennTreebank(Vadas&Curran,2011),RedPajama(TogetherComputer,2023),Fal-
conRefinedweb(RefinedWeb)(Penedoetal.,2023), Dolmav1.5(Soldainietal.,2023), M2D2
S2ORC,M2D2Wikipedia(Reidetal.,2022),C4-100-Domains(C4Domains)(Chronopoulouetal.,
2022),Dolma-100-Subreddits(DolmaSubreddits)(Soldainietal.,2023),Dolma-100-Programming
Languages(DolmaCoding)(Soldainietal.,2023;Kocetkovetal.,2022),TwitterAAE(Blodgett
etal.,2016;Liangetal.,2023),ManosphereCorpus(Ribeiroetal.,2021),GABCorpus(Zannettou
etal.,2018),4CHANCorpus(Papasavvaetal.,2020). WeleaveoutThePile(Gaoetal.,2021)and
ICE(Greenbaum&Nelson,1996)astheyarenotpartofPaloma’sHuggingfacedatasetrepository9.
AdetaileddescriptionofthesedatasetscanbefoundinMagnussonetal.(2023,Table2). Allmodels
areevaluatedinbfloat16MixedPrecision.
ResultsonthedatasourcesTwitterAAE,Manosphere,GABand4CHANarereportedinTable9and
foreachindividualdatasettheresultsaregiveninSectionC.
#Params Twitter
Model Manosphere 4CHAN GAB
M AAE
RWKV-4 169.4 265.80 39.31 18.48 53.89
Llama 162.2 277.93 32.98 14.03 56.45
Mamba 167.8 258.17 32.14 14.01 51.58
xLSTM[1:0] 163.8 244.53 31.45 13.27 51.00
xLSTM[7:1] 163.7 248.51 30.90 13.45 50.25
RWKV-4 430.5 216.17 30.25 13.82 42.25
Llama 406.6 231.09 25.90 11.49 43.04
Mamba 423.1 202.88 25.24 11.60 40.78
xLSTM[1:0] 409.3 200.61 24.58 11.20 39.83
xLSTM[7:1] 408.4 206.25 24.73 11.31 39.86
RWKV-4 891.0 195.27 24.66 12.00 35.73
Llama 834.1 205.50 22.69 10.40 37.68
Mamba 793.2 182.74 22.58 10.47 36.25
xLSTM[1:0] 840.4 179.74 21.66 10.11 35.33
xLSTM[7:1] 839.7 180.19 21.78 10.22 34.89
RWKV-4 1515.2 174.87 23.51 11.34 33.18
Llama 1420.4 192.52 20.67 9.67 34.84
Mamba 1475.3 171.38 20.37 9.80 32.01
xLSTM[1:0] 1422.6 166.16 19.94 9.64 31.90
xLSTM[7:1] 1420.1 171.36 20.28 9.64 32.17
Table9: Perplexityvaluesperdomain.
9https://huggingface.co/datasets/allenai/paloma
42
M521
M053
M067
B3.1In order to evaluate the perplexity values on each data source, we split the text documents into
sequencesoflength2048,whichcorrespondstothepre-trainingcontextlengthofallmodels. For
documentslongerthan2048tokenswespliteachdocumentintonon-overlappinginputsequences. In
thiscaseforthelastinputsequence,wefollowtheLMEvaluationHarnessandfillupthefull2048
tokencontextwindowwithprevioustokens,butcomputetheperplexityonlyontheremainingtokens.
We compute the token perplexities per data source in Table 4 as the exponential of the negative
loglikelihoodsperdomainweightedbythenumberoftokensperdomaininthatdatasourceasitis
definedinMagnussonetal.(2023,Equation1)
43C DetailedResultsonPALOMALanguageModelEvaluation
Wereporttheperplexityvaluesoneachofthe571subdomainsofPALOMAinTable10. Notethat
theaggregatedperplexityvaluesinTable4arenotmacroaveragesofthevaluesshowninTable10.
Dataset Llama Mamba RWKV-4 xLSTM[7:1]xLSTM[1:0]
#Params(M) 1420 1475 1515 1420 1423
4chan_meta_sep_val-00000000 9.58 9.72 11.37 9.53 9.55
4chan_meta_sep_val-00000001 9.95 10.06 11.57 9.91 9.88
4chan_meta_sep_val-00000002 9.42 9.53 11.00 9.40 9.38
4chan_meta_sep_val-00000003 9.78 9.93 11.48 9.77 9.77
c4_100dom_val_100_www.ign.com 16.22 15.75 17.10 15.67 15.43
c4_100dom_val_10_www.eventbrite.com 12.72 12.33 13.33 12.30 12.12
c4_100dom_val_11_link.springer.com 8.66 8.54 9.31 8.42 8.33
c4_100dom_val_12_www.chicagotribune.com 12.09 11.60 12.49 11.55 11.37
c4_100dom_val_13_www.foxnews.com 9.59 9.21 9.83 9.16 9.08
c4_100dom_val_14_www.aljazeera.com 10.97 10.61 11.31 10.50 10.40
c4_100dom_val_15_www.dailymail.co.uk 12.42 11.97 12.87 11.85 11.69
c4_100dom_val_16_www.ncbi.nlm.nih.gov 7.39 7.31 7.98 7.11 7.07
c4_100dom_val_17_www.express.co.uk 11.57 11.04 11.84 10.99 10.79
c4_100dom_val_18_en.m.wikipedia.org 9.28 8.95 9.52 8.89 8.80
c4_100dom_val_19_www.cnet.com 12.61 12.23 13.12 12.09 11.97
c4_100dom_val_1_www.nytimes.com 13.13 12.66 14.04 12.68 12.44
c4_100dom_val_20_www.telegraph.co.uk 13.71 13.10 14.28 13.06 12.88
c4_100dom_val_21_www.theatlantic.com 14.70 14.17 15.54 14.17 13.97
c4_100dom_val_22_forums.macrumors.com 17.77 17.34 19.15 17.22 16.95
c4_100dom_val_23_www.oreilly.com 13.36 12.99 14.31 13.02 12.88
c4_100dom_val_24_www.washingtonpost.com 12.06 11.58 12.98 11.64 11.41
c4_100dom_val_25_www.zdnet.com 13.22 12.86 13.80 12.78 12.61
c4_100dom_val_26_www.foxbusiness.com 9.32 9.03 9.58 8.92 8.81
c4_100dom_val_27_www.reuters.com 10.67 10.13 11.16 10.13 9.97
c4_100dom_val_28_www.ibtimes.co.uk 11.36 11.01 11.71 10.89 10.76
c4_100dom_val_29_www.rt.com 13.59 12.96 14.24 12.98 12.74
c4_100dom_val_2_en.wikipedia.org 10.75 10.45 11.32 10.32 10.19
c4_100dom_val_30_www.prweb.com 11.18 10.88 11.92 10.83 10.65
c4_100dom_val_31_www.deviantart.com 21.78 21.05 22.78 21.00 20.69
c4_100dom_val_32_www.si.com 11.49 11.00 11.92 10.90 10.76
c4_100dom_val_33_www.bbc.com 9.35 8.91 9.41 8.80 8.70
c4_100dom_val_34_github.com 11.57 11.49 12.94 11.40 11.28
c4_100dom_val_35_nypost.com 14.31 13.41 15.29 13.62 13.31
c4_100dom_val_36_itunes.apple.com 16.49 15.88 17.15 15.98 15.69
c4_100dom_val_37_www.instructables.com 16.75 16.33 17.73 16.28 15.97
c4_100dom_val_38_www.youtube.com 8.42 8.24 8.83 8.22 8.07
c4_100dom_val_39_www.booking.com 8.84 8.49 8.83 8.41 8.32
c4_100dom_val_40_www.etsy.com 11.93 11.66 12.66 11.52 11.43
c4_100dom_val_41_www.marketwired.com 7.66 7.47 7.88 7.33 7.27
c4_100dom_val_42_sites.google.com 14.23 13.81 14.91 13.68 13.51
c4_100dom_val_43_www.baltimoresun.com 11.57 11.16 11.96 11.09 10.95
c4_100dom_val_44_www.agreatertown.com 13.56 12.94 13.57 12.77 12.64
c4_100dom_val_45_www.npr.org 10.59 10.30 11.14 10.19 10.12
c4_100dom_val_46_www.fool.com 11.03 10.63 11.35 10.56 10.42
c4_100dom_val_47_www.tripadvisor.com 15.80 15.26 16.26 15.10 14.93
c4_100dom_val_48_www.bbc.co.uk 12.55 12.10 13.02 12.00 11.85
44Dataset Llama Mamba RWKV-4 xLSTM[7:1]xLSTM[1:0]
c4_100dom_val_49_lists.w3.org 18.75 18.24 19.89 18.05 17.84
c4_100dom_val_4_www.latimes.com 11.88 11.46 12.40 11.39 11.24
c4_100dom_val_50_mashable.com 12.44 11.95 12.85 11.90 11.76
c4_100dom_val_51_disneyparksmomspanel.disney.g1o1..c9o9m 11.29 11.98 11.16 11.00
c4_100dom_val_52_www.cnbc.com 10.65 10.32 10.99 10.24 10.10
c4_100dom_val_53_answers.sap.com 23.59 23.09 25.71 22.99 22.55
c4_100dom_val_54_homestars.com 14.13 13.70 14.51 13.65 13.52
c4_100dom_val_55_www.hindustantimes.com 12.13 11.60 12.74 11.60 11.37
c4_100dom_val_56_www.reference.com 11.57 11.04 11.75 10.92 10.79
c4_100dom_val_57_www.city-data.com 18.38 17.94 19.61 17.73 17.62
c4_100dom_val_58_medium.com 15.50 15.09 16.58 15.18 15.01
c4_100dom_val_59_app-wiringdiagram... 9.74 9.10 9.68 8.88 8.75
c4_100dom_val_5_www.theguardian.com 14.78 14.09 15.47 14.08 13.86
c4_100dom_val_60_www.csmonitor.com 15.35 14.85 15.92 14.75 14.57
c4_100dom_val_61_www.adweek.com 14.55 13.95 15.58 14.09 13.81
c4_100dom_val_62_docs.microsoft.com 7.69 7.79 8.86 7.68 7.58
c4_100dom_val_63_www.yahoo.com 9.29 8.88 9.71 8.89 8.77
c4_100dom_val_64_www.thesun.co.uk 12.18 11.66 12.74 11.59 11.39
c4_100dom_val_65_www.nydailynews.com 12.15 11.60 12.61 11.56 11.36
c4_100dom_val_66_www.dailystar.co.uk 10.65 10.17 11.03 10.09 9.92
c4_100dom_val_67_fineartamerica.com 12.06 11.58 12.29 11.46 11.36
c4_100dom_val_68_www.kickstarter.com 13.85 13.58 15.38 13.55 13.38
c4_100dom_val_69_uk.reuters.com 9.54 9.13 9.90 9.07 8.92
c4_100dom_val_6_www.huffpost.com 13.45 13.03 13.96 12.99 12.83
c4_100dom_val_70_www.insiderpages.com 13.24 12.84 13.55 12.77 12.64
c4_100dom_val_71_www.inquisitr.com 12.12 11.58 12.86 11.71 11.38
c4_100dom_val_72_lists.debian.org 18.18 17.81 19.62 17.67 17.30
c4_100dom_val_73_www.straitstimes.com 11.51 11.06 11.91 10.94 10.79
c4_100dom_val_74_www.cbsnews.com 10.29 9.91 10.60 9.82 9.72
c4_100dom_val_75_simple.wikipedia.org 8.25 7.85 8.37 7.78 7.67
c4_100dom_val_76_deadline.com 14.75 13.83 15.48 13.92 13.51
c4_100dom_val_77_www.androidheadlines.com 11.11 10.74 11.43 10.72 10.59
c4_100dom_val_78_www.wired.com 14.42 13.88 15.14 13.87 13.68
c4_100dom_val_79_www.bustle.com 12.79 12.33 13.19 12.25 12.09
c4_100dom_val_7_patents.google.com 7.59 7.84 9.33 7.72 7.59
c4_100dom_val_80_premium.wpmudev.org 16.86 16.63 18.13 16.50 16.29
c4_100dom_val_81_www.librarything.com 14.36 13.98 15.42 13.91 13.75
c4_100dom_val_82_mail-archives.apache.org 5.67 5.61 6.17 5.56 5.49
c4_100dom_val_83_scholars.duke.edu 8.72 8.43 9.03 8.32 8.21
c4_100dom_val_84_www.glassdoor.com 16.64 15.97 16.99 16.00 15.83
c4_100dom_val_85_www.pcworld.com 12.34 11.95 12.95 11.90 11.72
c4_100dom_val_86_www.shutterstock.com 8.70 8.89 10.75 8.62 8.52
c4_100dom_val_87_myemail.constantcontact.com 14.59 14.24 15.32 14.18 13.98
c4_100dom_val_88_www.eventbrite.co.uk 14.47 13.99 14.89 13.98 13.79
c4_100dom_val_89_www.fastcompany.com 14.24 13.75 15.52 13.82 13.56
c4_100dom_val_8_www.businessinsider.com 10.97 10.69 11.35 10.52 10.46
c4_100dom_val_90_www.firstpost.com 11.71 11.24 12.08 11.12 10.96
c4_100dom_val_91_www.entrepreneur.com 13.10 12.68 13.65 12.72 12.54
c4_100dom_val_92_www.breitbart.com 13.47 12.67 14.29 12.84 12.56
c4_100dom_val_93_techcrunch.com 14.20 13.68 15.18 13.82 13.58
c4_100dom_val_94_www.nme.com 14.12 13.28 15.06 13.43 13.12
c4_100dom_val_95_www.ndtv.com 10.66 10.26 10.90 10.10 10.00
45Dataset Llama Mamba RWKV-4 xLSTM[7:1]xLSTM[1:0]
c4_100dom_val_96_finance.yahoo.com 9.96 9.55 10.22 9.43 9.34
c4_100dom_val_97_archives.lib.state.ma.us 6.53 6.12 7.09 6.27 5.85
c4_100dom_val_98_www.gsmarena.com 23.21 22.15 24.52 22.10 21.76
c4_100dom_val_99_www.lonelyplanet.com 11.33 10.92 12.28 10.84 10.69
c4_100dom_val_9_www.forbes.com 13.72 13.31 14.63 13.34 13.13
c4_en_val-00000000 14.34 13.70 14.87 13.67 13.46
c4_en_val-00000001 14.86 14.28 15.51 14.21 14.09
c4_en_val-00000002 15.29 14.71 15.95 14.71 14.51
c4_en_val-00000003 12.95 12.28 13.32 12.23 12.06
c4_en_val-00000004 12.56 12.13 13.27 12.05 11.87
c4_en_val-00000005 12.77 12.35 13.26 12.32 12.18
dolma-v1_5_val_books 13.00 12.44 13.64 12.44 12.27
dolma-v1_5_val_common-crawl 16.86 16.37 18.00 16.35 16.10
dolma-v1_5_val_pes2o 9.42 9.56 11.25 9.41 9.29
dolma-v1_5_val_reddit_uniform 23.04 21.97 23.84 22.05 21.80
dolma-v1_5_val_stack_uniform 2.30 2.33 2.53 2.30 2.29
dolma-v1_5_val_wiki 10.86 10.48 11.25 10.41 10.31
dolma_100_proglang_val_00_text 5.61 6.30 6.94 5.67 5.69
dolma_100_proglang_val_01_markdown 3.16 3.16 3.56 3.15 3.11
dolma_100_proglang_val_02_c 1.84 1.91 2.23 1.86 1.85
dolma_100_proglang_val_03_php 1.75 1.75 1.83 1.73 1.72
dolma_100_proglang_val_04_java 1.96 1.99 2.18 1.95 1.95
dolma_100_proglang_val_05_c++ 2.19 2.25 2.53 2.21 2.19
dolma_100_proglang_val_06_python 2.35 2.39 2.62 2.36 2.34
dolma_100_proglang_val_07_javascript 2.54 2.59 2.83 2.53 2.53
dolma_100_proglang_val_08_html 1.92 1.94 2.13 1.91 1.91
dolma_100_proglang_val_09_c# 2.23 2.28 2.45 2.19 2.24
dolma_100_proglang_val_10_yaml 2.93 3.01 3.71 2.94 2.92
dolma_100_proglang_val_11_go 1.75 1.78 1.97 1.77 1.75
dolma_100_proglang_val_12_typescript 2.17 2.20 2.41 2.18 2.16
dolma_100_proglang_val_13_xml 2.44 2.50 2.78 2.46 2.48
dolma_100_proglang_val_14_css 2.25 2.25 2.34 2.21 2.20
dolma_100_proglang_val_15_jupyter-nb 1.57 1.60 1.75 1.58 1.58
dolma_100_proglang_val_16_rust 1.96 2.01 2.23 1.97 1.96
dolma_100_proglang_val_17_unity3d-asset 4.01 4.17 4.56 4.10 4.05
dolma_100_proglang_val_18_gettext-catalog 2.84 2.87 3.53 2.86 2.83
dolma_100_proglang_val_19_ruby 2.41 2.44 2.70 2.39 2.38
dolma_100_proglang_val_20_vue 1.95 1.95 2.10 1.94 1.93
dolma_100_proglang_val_21_sql 2.18 2.23 2.46 2.17 2.16
dolma_100_proglang_val_22_swift 1.86 1.88 2.04 1.86 1.84
dolma_100_proglang_val_23_kotlin 2.05 2.07 2.29 2.07 2.04
dolma_100_proglang_val_24_scala 2.24 2.28 2.64 2.25 2.23
dolma_100_proglang_val_25_scss 2.26 2.27 2.38 2.24 2.24
dolma_100_proglang_val_26_tex 4.04 4.21 4.97 4.10 4.04
dolma_100_proglang_val_27_dart 1.79 1.82 2.01 1.80 1.78
dolma_100_proglang_val_28_kicad 2.57 2.79 3.86 2.68 2.67
dolma_100_proglang_val_29_shell 3.71 3.74 4.31 3.69 3.63
dolma_100_proglang_val_30_smali 1.38 1.39 1.45 1.38 1.37
dolma_100_proglang_val_31_lua 5.65 6.01 7.18 5.33 5.45
dolma_100_proglang_val_32_restructuredtext 4.01 4.05 4.66 3.97 3.92
dolma_100_proglang_val_33_perl 2.57 2.62 3.01 2.59 2.55
dolma_100_proglang_val_34_diff 2.87 2.95 3.43 2.89 2.86
46Dataset Llama Mamba RWKV-4 xLSTM[7:1]xLSTM[1:0]
dolma_100_proglang_val_35_ini 3.91 4.16 4.90 4.05 3.98
dolma_100_proglang_val_36_jsx 1.83 1.84 1.95 1.83 1.82
dolma_100_proglang_val_37_haskell 2.94 3.07 3.73 3.02 2.95
dolma_100_proglang_val_38_gnuplot 2.65 2.88 3.36 2.81 2.77
dolma_100_proglang_val_39_postscript 19.09 19.52 19.56 18.66 18.64
dolma_100_proglang_val_40_groff 6.13 6.32 7.45 6.22 6.21
dolma_100_proglang_val_41_turtle 2.35 2.45 3.17 2.39 2.35
dolma_100_proglang_val_42_fortran 2.32 2.39 2.83 2.35 2.31
dolma_100_proglang_val_43_makefile 2.93 3.01 3.51 2.86 2.82
dolma_100_proglang_val_44_mathematica 10.34 11.34 13.24 10.49 10.71
dolma_100_proglang_val_45_pascal 4.18 4.81 5.49 4.17 4.27
dolma_100_proglang_val_46_common-lisp 2.56 2.71 3.32 2.62 2.58
dolma_100_proglang_val_47_gas 2.49 2.73 3.59 2.57 2.53
dolma_100_proglang_val_48_vhdl 3.91 4.06 4.69 3.92 3.90
dolma_100_proglang_val_49_julia 3.25 3.36 4.05 3.30 3.26
dolma_100_proglang_val_50_edn 1.99 2.10 2.67 2.04 2.03
dolma_100_proglang_val_51_visual-basic 2.42 2.49 2.72 2.37 2.38
dolma_100_proglang_val_52_powershell 4.08 4.16 4.50 3.86 3.89
dolma_100_proglang_val_53_g-code 2.26 2.66 3.29 2.44 2.37
dolma_100_proglang_val_54_ocaml 3.06 3.29 4.22 3.19 3.13
dolma_100_proglang_val_55_java-server-p 2.10 2.11 2.31 2.06 2.09
dolma_100_proglang_val_56_solidity 4.09 4.41 5.28 4.05 4.10
dolma_100_proglang_val_57_graphviz-dot 2.17 2.48 3.54 2.32 2.29
dolma_100_proglang_val_58_less 2.24 2.26 2.33 2.22 2.22
dolma_100_proglang_val_59_twig 1.81 1.81 1.91 1.80 1.79
dolma_100_proglang_val_60_asciidoc 5.33 5.50 6.84 5.43 5.34
dolma_100_proglang_val_61_groovy 2.12 2.15 2.41 2.13 2.11
dolma_100_proglang_val_62_llvm 2.26 2.40 3.25 2.31 2.23
dolma_100_proglang_val_63_hcl 2.52 2.56 2.96 2.52 2.48
dolma_100_proglang_val_64_html+erb 2.10 2.09 2.23 2.08 2.07
dolma_100_proglang_val_65_erlang 2.84 2.98 3.87 2.88 2.85
dolma_100_proglang_val_66_elixir 2.93 2.99 3.58 2.91 2.90
dolma_100_proglang_val_67_eagle 5.35 6.90 10.75 5.64 5.76
dolma_100_proglang_val_68_arduino 3.37 3.40 3.81 3.28 3.28
dolma_100_proglang_val_69_coffeescript 2.80 2.85 3.27 2.80 2.77
dolma_100_proglang_val_70_toml 7.76 7.62 8.44 7.53 7.58
dolma_100_proglang_val_71_cuda 2.15 2.21 2.56 2.19 2.16
dolma_100_proglang_val_72_nix 7.80 7.84 9.03 7.88 7.83
dolma_100_proglang_val_73_smalltalk 9.32 9.61 12.60 9.47 9.20
dolma_100_proglang_val_74_cmake 1.87 1.86 2.02 1.84 1.81
dolma_100_proglang_val_75_actionscript 2.45 2.54 2.88 2.46 2.46
dolma_100_proglang_val_76_glsl 2.40 2.42 2.72 2.36 2.32
dolma_100_proglang_val_77_systemverilog 2.53 2.66 3.17 2.58 2.55
dolma_100_proglang_val_78_haxe 2.74 2.81 3.20 2.77 2.76
dolma_100_proglang_val_79_f# 2.89 3.02 3.53 2.93 2.88
dolma_100_proglang_val_80_max 1.59 1.62 1.80 1.61 1.61
dolma_100_proglang_val_81_objective-c++ 2.18 2.19 2.40 2.17 2.16
dolma_100_proglang_val_82_standard-ml 3.57 4.05 4.79 3.81 3.77
dolma_100_proglang_val_83_dockerfile 4.08 4.17 4.37 4.01 4.05
dolma_100_proglang_val_84_emacs-lisp 3.83 3.83 4.44 3.80 3.72
dolma_100_proglang_val_85_scheme 2.78 2.86 3.40 2.84 2.77
dolma_100_proglang_val_86_clojure 3.18 3.30 4.00 3.26 3.17
47Dataset Llama Mamba RWKV-4 xLSTM[7:1]xLSTM[1:0]
dolma_100_proglang_val_87_handlebars 1.79 1.79 1.88 1.78 1.78
dolma_100_proglang_val_88_smarty 2.30 2.35 2.58 2.29 2.30
dolma_100_proglang_val_89_logos 2.37 2.58 2.98 2.46 2.44
dolma_100_proglang_val_90_stata 4.67 5.08 6.85 4.85 4.81
dolma_100_proglang_val_91_yacc 2.42 2.48 2.87 2.44 2.43
dolma_100_proglang_val_92_nimrod 2.75 2.87 3.63 2.81 2.77
dolma_100_proglang_val_93_tcl 3.00 3.16 3.95 3.07 3.02
dolma_100_proglang_val_94_viml 5.56 5.76 7.21 5.59 5.55
dolma_100_proglang_val_95_asp 1.79 1.79 1.90 1.77 1.77
dolma_100_proglang_val_96_protocol-buffer 1.32 1.31 1.38 1.31 1.32
dolma_100_proglang_val_97_r 2.80 2.92 3.66 2.86 2.81
dolma_100_proglang_val_98_cython 2.34 2.39 2.69 2.36 2.35
dolma_100_proglang_val_99_mediawiki 2.01 2.10 2.48 2.12 2.04
dolma_100_subreddits_val_00_AskReddit 20.25 19.29 20.38 19.28 19.14
dolma_100_subreddits_val_01_politics 22.08 20.70 22.07 20.83 20.61
dolma_100_subreddits_val_02_AmItheAsshole 22.49 21.30 22.89 21.60 21.27
dolma_100_subreddits_val_03_worldnews 22.57 21.43 22.77 21.50 21.23
dolma_100_subreddits_val_04_relationships 18.64 17.80 18.89 17.86 17.67
dolma_100_subreddits_val_05_relationship_advice19.40 18.53 19.68 18.63 18.46
dolma_100_subreddits_val_06_news 22.49 21.25 22.51 21.49 21.17
dolma_100_subreddits_val_07_leagueoflegends 34.45 32.41 35.13 32.46 32.04
dolma_100_subreddits_val_08_todayilearned 22.53 21.30 22.68 21.28 21.10
dolma_100_subreddits_val_09_TwoXChromosomes20.20 19.16 20.25 19.20 19.02
dolma_100_subreddits_val_10_personalfinance 18.62 17.65 18.82 17.73 17.64
dolma_100_subreddits_val_11_changemyview 20.02 19.10 20.50 19.17 18.99
dolma_100_subreddits_val_12_unpopularopinion 23.39 22.16 23.63 22.32 22.04
dolma_100_subreddits_val_13_movies 21.62 20.52 21.79 20.64 20.35
dolma_100_subreddits_val_14_Games 22.26 21.15 22.52 21.18 20.87
dolma_100_subreddits_val_15_nba 23.28 21.93 23.60 22.10 21.85
dolma_100_subreddits_val_16_pics 21.84 20.56 21.82 20.64 20.47
dolma_100_subreddits_val_17_gaming 24.45 23.13 24.61 23.15 22.86
dolma_100_subreddits_val_18_soccer 23.38 22.12 23.61 22.19 22.03
dolma_100_subreddits_val_19_nfl 19.86 18.76 20.17 18.81 18.62
dolma_100_subreddits_val_20_explainlikeimfive 18.35 17.21 18.59 17.32 17.03
dolma_100_subreddits_val_21_conspiracy 23.86 22.53 24.09 22.67 22.54
dolma_100_subreddits_val_22_atheism 21.23 20.18 21.43 20.23 20.13
dolma_100_subreddits_val_23_AskMen 20.00 19.04 20.11 19.10 18.94
dolma_100_subreddits_val_24_videos 22.26 21.24 22.51 21.29 21.04
dolma_100_subreddits_val_25_sex 21.13 20.13 21.30 20.09 19.98
dolma_100_subreddits_val_26_raisedbynarcissists 22.07 21.08 22.48 21.20 21.02
dolma_100_subreddits_val_27_NoStupidQuestions19.66 18.59 19.87 18.68 18.52
dolma_100_subreddits_val_28_DestinyTheGame 35.27 33.58 36.13 33.78 33.37
dolma_100_subreddits_val_29_anime 23.21 22.04 23.46 22.12 21.77
dolma_100_subreddits_val_30_DnD 28.22 26.71 28.78 26.72 26.39
dolma_100_subreddits_val_31_ukpolitics 22.35 21.19 22.80 21.31 21.10
dolma_100_subreddits_val_32_funny 20.78 19.45 20.70 19.40 19.23
dolma_100_subreddits_val_33_europe 21.76 20.59 22.10 20.72 20.52
dolma_100_subreddits_val_34_canada 22.44 21.21 22.44 21.30 21.09
dolma_100_subreddits_val_35_Christianity 17.88 17.02 18.10 17.04 16.94
dolma_100_subreddits_val_36_SquaredCircle 25.87 24.31 25.83 24.34 24.03
dolma_100_subreddits_val_37_AskWomen 17.72 16.81 17.77 16.85 16.72
dolma_100_subreddits_val_38_legaladvice 18.66 17.75 18.92 17.74 17.64
48Dataset Llama Mamba RWKV-4 xLSTM[7:1]xLSTM[1:0]
dolma_100_subreddits_val_39_JUSTNOMIL 24.25 23.16 24.86 23.32 23.02
dolma_100_subreddits_val_40_technology 23.39 22.09 23.52 22.21 21.95
dolma_100_subreddits_val_41_IAmA 19.83 18.83 19.86 18.71 18.56
dolma_100_subreddits_val_42_wow 31.26 29.25 31.44 29.39 28.82
dolma_100_subreddits_val_43_Parenting 20.15 19.11 20.43 19.30 19.06
dolma_100_subreddits_val_44_exmormon 23.12 21.90 23.44 21.99 21.84
dolma_100_subreddits_val_45_AdviceAnimals 22.14 20.96 22.14 20.98 20.79
dolma_100_subreddits_val_46_childfree 21.87 20.85 22.13 20.89 20.72
dolma_100_subreddits_val_47_unitedkingdom 23.27 22.00 23.40 22.00 21.85
dolma_100_subreddits_val_48_ffxiv 32.53 30.79 33.33 31.01 30.62
dolma_100_subreddits_val_49_dndnext 29.67 28.03 30.53 28.26 27.63
dolma_100_subreddits_val_50_ADHD 20.75 19.83 21.14 19.95 19.78
dolma_100_subreddits_val_51_loseit 19.36 18.39 19.49 18.52 18.33
dolma_100_subreddits_val_52_asoiaf 25.28 23.99 25.63 23.94 23.69
dolma_100_subreddits_val_53_BabyBumps 20.96 19.82 21.11 19.92 19.76
dolma_100_subreddits_val_54_Advice 19.17 18.29 19.35 18.38 18.19
dolma_100_subreddits_val_55_australia 23.97 22.51 24.06 22.61 22.40
dolma_100_subreddits_val_56_CFB 20.45 19.41 20.92 19.49 19.23
dolma_100_subreddits_val_57_offmychest 19.63 18.79 19.77 18.93 18.77
dolma_100_subreddits_val_58_PublicFreakout 25.96 24.49 26.02 24.65 24.39
dolma_100_subreddits_val_59_TrueOffMyChest 21.53 20.63 21.70 20.73 20.54
dolma_100_subreddits_val_60_science 20.44 19.46 20.64 19.51 19.38
dolma_100_subreddits_val_61_magicTCG 28.82 26.79 28.94 26.69 26.38
dolma_100_subreddits_val_62_asktransgender 20.72 19.86 21.07 19.83 19.62
dolma_100_subreddits_val_63_DotA2 34.35 32.38 34.74 32.57 32.16
dolma_100_subreddits_val_64_neoliberal 21.74 20.59 22.26 20.64 20.45
dolma_100_subreddits_val_65_whowouldwin 29.18 27.81 30.08 27.63 27.30
dolma_100_subreddits_val_66_depression 18.28 17.52 18.31 17.50 17.41
dolma_100_subreddits_val_67_WTF 22.30 21.18 22.38 21.17 20.99
dolma_100_subreddits_val_68_pathofexile 40.48 38.59 41.43 38.75 38.43
dolma_100_subreddits_val_69_PoliticalDiscussion 20.01 18.92 20.16 18.97 18.82
dolma_100_subreddits_val_70_Libertarian 22.97 21.77 23.15 21.87 21.75
dolma_100_subreddits_val_71_PurplePillDebate 24.94 23.66 25.44 23.85 23.55
dolma_100_subreddits_val_72_Fitness 21.57 20.35 21.48 20.34 20.11
dolma_100_subreddits_val_73_books 21.12 20.02 21.31 20.09 19.82
dolma_100_subreddits_val_74_dogs 20.13 19.12 20.32 19.20 18.92
dolma_100_subreddits_val_75_pcmasterrace 23.73 22.49 24.02 22.56 22.21
dolma_100_subreddits_val_76_teenagers 18.37 16.35 16.44 15.56 17.02
dolma_100_subreddits_val_77_stopdrinking 21.08 20.02 21.19 20.17 19.98
dolma_100_subreddits_val_78_Overwatch 30.47 28.77 31.13 29.13 28.57
dolma_100_subreddits_val_79_television 23.97 22.63 24.05 22.75 22.49
dolma_100_subreddits_val_80_buildapc 21.55 20.22 21.78 20.29 19.98
dolma_100_subreddits_val_81_askscience 17.25 16.39 17.52 16.34 16.11
dolma_100_subreddits_val_82_programming 23.66 22.61 24.04 22.55 22.24
dolma_100_subreddits_val_83_Guildwars2 32.98 31.17 33.58 31.39 30.91
dolma_100_subreddits_val_84_cars 22.57 21.41 22.73 21.38 21.15
dolma_100_subreddits_val_85_formula1 23.85 22.65 24.09 22.71 22.49
dolma_100_subreddits_val_86_sysadmin 24.23 22.90 24.41 22.96 22.64
dolma_100_subreddits_val_87_hockey 21.46 20.26 21.74 20.37 20.20
dolma_100_subreddits_val_88_india 24.15 22.92 24.42 23.08 22.68
dolma_100_subreddits_val_89_SubredditDrama 19.14 18.26 19.63 18.29 18.12
dolma_100_subreddits_val_90_DMAcademy 27.77 26.31 28.38 26.41 26.00
49Dataset Llama Mamba RWKV-4 xLSTM[7:1]xLSTM[1:0]
dolma_100_subreddits_val_91_dating_advice 20.18 19.27 20.42 19.40 19.21
dolma_100_subreddits_val_92_Catholicism 19.11 18.22 19.41 18.17 18.03
dolma_100_subreddits_val_93_Drugs 24.50 23.29 24.74 23.32 23.12
dolma_100_subreddits_val_94_trees 23.56 22.38 23.83 22.41 22.25
dolma_100_subreddits_val_95_boardgames 22.69 21.48 23.13 21.61 21.38
dolma_100_subreddits_val_96_Conservative 22.79 21.53 22.97 21.68 21.53
dolma_100_subreddits_val_97_Futurology 23.55 22.36 23.77 22.37 22.17
dolma_100_subreddits_val_98_beyondthebump 21.07 19.89 21.22 20.08 19.83
dolma_100_subreddits_val_99_weddingplanning 20.11 19.01 20.33 19.19 18.96
falcon-refinedweb_val-00000000 15.92 15.46 17.14 15.37 15.22
falcon-refinedweb_val-00000001 18.49 17.91 19.89 17.90 17.71
falcon-refinedweb_val-00000002 18.45 17.90 19.69 17.91 17.68
falcon-refinedweb_val-00000003 16.75 16.23 17.92 16.16 15.89
falcon-refinedweb_val-00000004 16.26 15.66 17.32 15.73 15.41
falcon-refinedweb_val-00000005 15.41 14.96 16.56 14.92 14.74
gab_val-00000000 33.19 30.55 31.57 30.73 30.32
gab_val-00000001 35.64 32.76 33.96 32.80 32.63
gab_val-00000002 34.38 31.68 32.75 31.80 31.65
gab_val-00000003 34.86 32.05 33.26 32.20 32.00
gab_val-00000004 36.20 33.35 34.58 33.42 33.23
gab_val-00000005 33.46 30.82 31.88 31.06 30.72
gab_val-00000006 35.76 32.77 34.26 33.04 32.74
gab_val-00000007 35.54 32.60 33.76 32.78 32.41
gab_val-00000008 35.11 32.03 33.23 32.25 31.86
gab_val-00000009 34.13 31.34 32.36 31.50 31.30
m2d2_s2orc_unsplit_val_Art 20.07 19.80 21.88 19.78 19.44
m2d2_s2orc_unsplit_val_Philosophy 14.80 14.82 16.77 14.69 14.47
m2d2_s2orc_unsplit_val_astro-ph 11.70 11.70 13.18 11.52 11.33
m2d2_s2orc_unsplit_val_astro-ph.CO 11.47 11.49 12.90 11.37 11.15
m2d2_s2orc_unsplit_val_astro-ph.EP 12.76 12.73 14.28 12.60 12.45
m2d2_s2orc_unsplit_val_astro-ph.GA 11.70 11.70 13.18 11.52 11.33
m2d2_s2orc_unsplit_val_astro-ph.HE 11.85 11.77 13.29 11.62 11.46
m2d2_s2orc_unsplit_val_astro-ph.IM 15.36 15.33 17.16 15.21 14.92
m2d2_s2orc_unsplit_val_astro-ph.SR 13.08 13.08 14.89 12.86 12.70
m2d2_s2orc_unsplit_val_astro-ph_l1 15.36 15.33 17.16 15.21 14.92
m2d2_s2orc_unsplit_val_atom-ph 12.74 12.84 14.44 12.75 12.53
m2d2_s2orc_unsplit_val_chem-ph 13.20 13.29 15.22 13.14 12.97
m2d2_s2orc_unsplit_val_cond-mat 11.67 11.78 13.37 11.67 11.50
m2d2_s2orc_unsplit_val_cond-mat.dis-nn 12.54 12.67 14.28 12.58 12.38
m2d2_s2orc_unsplit_val_cond-mat.mes-hall 11.24 11.50 13.19 11.30 11.10
m2d2_s2orc_unsplit_val_cond-mat.mtrl-sci 12.19 12.33 14.09 12.18 11.91
m2d2_s2orc_unsplit_val_cond-mat.other 11.87 11.96 13.55 11.83 11.65
m2d2_s2orc_unsplit_val_cond-mat.quant-gas 11.67 11.78 13.37 11.67 11.50
m2d2_s2orc_unsplit_val_cond-mat.soft 12.18 12.23 13.93 12.18 12.02
m2d2_s2orc_unsplit_val_cond-mat.stat-mech 12.03 12.14 13.60 12.08 11.89
m2d2_s2orc_unsplit_val_cond-mat.str-el 10.39 10.50 11.98 10.41 10.22
m2d2_s2orc_unsplit_val_cond-mat.supr-con 11.57 11.66 13.13 11.53 11.30
m2d2_s2orc_unsplit_val_cond-mat_l1 12.54 12.67 14.28 12.58 12.38
m2d2_s2orc_unsplit_val_cs.AI 11.71 12.09 14.20 12.01 11.79
m2d2_s2orc_unsplit_val_cs.AR 13.09 13.36 15.30 13.18 12.99
m2d2_s2orc_unsplit_val_cs.CC 8.45 8.81 10.46 8.70 8.54
m2d2_s2orc_unsplit_val_cs.CE 13.21 13.31 15.01 13.18 13.02
50Dataset Llama Mamba RWKV-4 xLSTM[7:1]xLSTM[1:0]
m2d2_s2orc_unsplit_val_cs.CG 8.39 8.68 10.12 8.59 8.47
m2d2_s2orc_unsplit_val_cs.CL 14.66 14.75 16.96 14.70 14.47
m2d2_s2orc_unsplit_val_cs.CR 14.63 14.86 16.72 14.74 14.56
m2d2_s2orc_unsplit_val_cs.CV 12.68 12.78 14.38 12.66 12.49
m2d2_s2orc_unsplit_val_cs.CY 16.01 15.93 17.52 15.84 15.67
m2d2_s2orc_unsplit_val_cs.DB 11.86 12.35 14.66 12.27 12.03
m2d2_s2orc_unsplit_val_cs.DC 13.60 14.02 16.20 13.79 13.56
m2d2_s2orc_unsplit_val_cs.DL 14.67 14.83 17.05 14.75 14.50
m2d2_s2orc_unsplit_val_cs.DM 8.11 8.38 9.84 8.27 8.14
m2d2_s2orc_unsplit_val_cs.DS 9.63 9.99 11.76 9.88 9.69
m2d2_s2orc_unsplit_val_cs.ET 14.80 14.95 17.00 14.89 14.67
m2d2_s2orc_unsplit_val_cs.FL 9.51 9.84 11.64 9.74 9.57
m2d2_s2orc_unsplit_val_cs.GL 16.51 16.43 18.18 16.38 16.21
m2d2_s2orc_unsplit_val_cs.GR 13.45 13.60 15.53 13.54 13.29
m2d2_s2orc_unsplit_val_cs.GT 9.25 9.59 11.34 9.49 9.29
m2d2_s2orc_unsplit_val_cs.HC 16.76 16.93 19.08 16.84 16.66
m2d2_s2orc_unsplit_val_cs.IR 13.30 13.46 15.26 13.31 13.21
m2d2_s2orc_unsplit_val_cs.LG 10.39 10.52 12.14 10.44 10.27
m2d2_s2orc_unsplit_val_cs.LO 9.75 10.23 12.50 10.03 9.81
m2d2_s2orc_unsplit_val_cs.MA 11.24 11.65 14.10 11.41 11.19
m2d2_s2orc_unsplit_val_cs.MM 13.12 13.40 15.29 13.25 13.03
m2d2_s2orc_unsplit_val_cs.MS 13.98 14.14 16.27 14.11 13.89
m2d2_s2orc_unsplit_val_cs.NA 10.53 10.80 12.52 10.71 10.47
m2d2_s2orc_unsplit_val_cs.NE 13.76 14.00 16.10 13.89 13.64
m2d2_s2orc_unsplit_val_cs.NI 10.00 10.22 11.61 10.04 9.93
m2d2_s2orc_unsplit_val_cs.OH 15.24 15.43 17.62 15.34 15.10
m2d2_s2orc_unsplit_val_cs.OS 14.61 14.93 17.35 14.80 14.53
m2d2_s2orc_unsplit_val_cs.PF 12.60 12.82 14.71 12.70 12.48
m2d2_s2orc_unsplit_val_cs.PL 15.43 15.74 18.58 15.65 15.40
m2d2_s2orc_unsplit_val_cs.RO 13.04 13.19 14.95 13.12 12.87
m2d2_s2orc_unsplit_val_cs.SC 11.10 11.42 13.33 11.30 11.10
m2d2_s2orc_unsplit_val_cs.SD 13.27 13.42 15.26 13.36 13.13
m2d2_s2orc_unsplit_val_cs.SE 17.72 13.47 15.46 13.40 13.21
m2d2_s2orc_unsplit_val_cs.SI 12.03 12.25 14.03 12.19 11.99
m2d2_s2orc_unsplit_val_cs.SY 11.40 11.79 13.51 11.63 11.39
m2d2_s2orc_unsplit_val_cs_l1 8.39 8.68 10.12 8.59 8.47
m2d2_s2orc_unsplit_val_econ.EM 11.62 11.76 13.73 11.68 11.41
m2d2_s2orc_unsplit_val_econ.TH 9.75 10.16 11.99 9.99 9.88
m2d2_s2orc_unsplit_val_econ_l1 9.75 10.16 11.99 9.99 9.88
m2d2_s2orc_unsplit_val_eess.AS 12.05 12.14 13.88 12.09 11.88
m2d2_s2orc_unsplit_val_eess.IV 13.77 13.89 15.71 13.76 13.54
m2d2_s2orc_unsplit_val_eess.SP 11.29 11.45 12.94 11.28 11.13
m2d2_s2orc_unsplit_val_eess_l1 13.77 13.89 15.71 13.76 13.54
m2d2_s2orc_unsplit_val_gr-qc 12.84 12.99 14.68 12.84 12.71
m2d2_s2orc_unsplit_val_hep-ex 10.47 10.37 11.61 10.13 9.96
m2d2_s2orc_unsplit_val_hep-lat 13.13 13.10 14.57 13.02 12.80
m2d2_s2orc_unsplit_val_hep-ph 11.67 11.81 13.38 11.66 11.45
m2d2_s2orc_unsplit_val_hep-th 11.46 11.49 12.71 11.40 11.24
m2d2_s2orc_unsplit_val_math.AC 7.08 7.37 8.71 7.26 7.13
m2d2_s2orc_unsplit_val_math.AG 8.89 9.27 11.05 9.16 8.95
m2d2_s2orc_unsplit_val_math.AP 9.35 9.53 10.90 9.41 9.35
m2d2_s2orc_unsplit_val_math.AT 8.57 8.77 10.16 8.72 8.53
51Dataset Llama Mamba RWKV-4 xLSTM[7:1]xLSTM[1:0]
m2d2_s2orc_unsplit_val_math.CA 9.18 9.49 11.01 9.36 9.30
m2d2_s2orc_unsplit_val_math.CO 6.99 7.33 8.69 7.21 7.08
m2d2_s2orc_unsplit_val_math.CT 9.78 10.20 12.04 10.12 9.91
m2d2_s2orc_unsplit_val_math.CV 7.81 8.07 9.36 7.99 7.87
m2d2_s2orc_unsplit_val_math.DG 7.96 8.18 9.50 8.08 7.98
m2d2_s2orc_unsplit_val_math.DS 7.88 8.12 9.61 8.08 7.96
m2d2_s2orc_unsplit_val_math.FA 7.71 7.96 9.35 7.88 7.81
m2d2_s2orc_unsplit_val_math.GM 7.85 8.15 9.57 8.07 7.93
m2d2_s2orc_unsplit_val_math.GN 6.27 6.56 7.82 6.45 6.38
m2d2_s2orc_unsplit_val_math.GR 7.39 7.66 9.00 7.51 7.41
m2d2_s2orc_unsplit_val_math.GT 7.47 7.71 9.27 7.62 7.47
m2d2_s2orc_unsplit_val_math.HO 14.52 14.70 16.52 14.51 14.31
m2d2_s2orc_unsplit_val_math.KT 7.54 7.80 9.14 7.70 7.58
m2d2_s2orc_unsplit_val_math.LO 9.84 10.41 12.53 10.13 10.03
m2d2_s2orc_unsplit_val_math.MG 8.25 8.53 9.99 8.42 8.26
m2d2_s2orc_unsplit_val_math.NA 9.85 10.05 11.66 9.95 9.83
m2d2_s2orc_unsplit_val_math.NT 8.26 8.51 9.92 8.43 8.31
m2d2_s2orc_unsplit_val_math.OA 7.21 7.55 9.07 7.47 7.32
m2d2_s2orc_unsplit_val_math.OC 9.70 10.01 11.62 9.85 9.69
m2d2_s2orc_unsplit_val_math.PR 8.91 9.20 10.58 9.04 8.99
m2d2_s2orc_unsplit_val_math.QA 8.09 8.40 9.93 8.28 8.16
m2d2_s2orc_unsplit_val_math.RA 7.18 7.44 8.75 7.39 7.27
m2d2_s2orc_unsplit_val_math.RT 8.39 8.71 10.33 8.65 8.49
m2d2_s2orc_unsplit_val_math.SG 8.63 8.88 10.36 8.76 8.59
m2d2_s2orc_unsplit_val_math.SP 9.39 9.65 11.27 9.52 9.37
m2d2_s2orc_unsplit_val_math_l1 7.81 8.07 9.36 7.99 7.87
m2d2_s2orc_unsplit_val_nlin.AO 11.82 12.01 13.77 11.90 11.75
m2d2_s2orc_unsplit_val_nlin.CD 12.73 12.91 14.88 12.87 12.60
m2d2_s2orc_unsplit_val_nlin.CG 12.43 12.75 14.88 12.61 12.44
m2d2_s2orc_unsplit_val_nlin.PS 11.29 11.44 12.86 11.39 11.22
m2d2_s2orc_unsplit_val_nlin.SI 9.44 9.81 11.28 9.64 9.51
m2d2_s2orc_unsplit_val_nlin_l1 12.43 12.75 14.88 12.61 12.44
m2d2_s2orc_unsplit_val_nucl-ex 13.02 12.94 14.61 12.85 12.63
m2d2_s2orc_unsplit_val_nucl-th 11.65 11.78 13.43 11.68 11.48
m2d2_s2orc_unsplit_val_physics.acc-ph 13.75 14.01 16.17 13.74 13.58
m2d2_s2orc_unsplit_val_physics.ao-ph 13.92 14.04 15.91 13.89 13.68
m2d2_s2orc_unsplit_val_physics.app-ph 13.70 13.81 15.54 13.62 13.43
m2d2_s2orc_unsplit_val_physics.atm-clus 13.00 13.13 15.11 13.00 12.74
m2d2_s2orc_unsplit_val_physics.atom-ph 12.74 12.84 14.44 12.75 12.53
m2d2_s2orc_unsplit_val_physics.bio-ph 13.30 13.42 15.26 13.32 13.08
m2d2_s2orc_unsplit_val_physics.chem-ph 13.20 13.29 15.22 13.14 12.97
m2d2_s2orc_unsplit_val_physics.class-ph 11.01 11.27 12.85 11.12 10.94
m2d2_s2orc_unsplit_val_physics.comp-ph 11.23 11.37 12.88 11.26 11.08
m2d2_s2orc_unsplit_val_physics.data-an 13.18 13.33 14.97 13.25 13.00
m2d2_s2orc_unsplit_val_physics.ed-ph 12.21 12.33 13.88 12.18 12.03
m2d2_s2orc_unsplit_val_physics.flu-dyn 11.81 11.99 13.73 11.81 11.64
m2d2_s2orc_unsplit_val_physics.gen-ph 14.15 14.39 16.76 14.18 14.03
m2d2_s2orc_unsplit_val_physics.geo-ph 14.75 14.86 16.81 14.71 14.57
m2d2_s2orc_unsplit_val_physics.hist-ph 15.57 15.43 16.97 15.40 15.18
m2d2_s2orc_unsplit_val_physics.ins-det 14.01 14.16 16.14 14.07 13.79
m2d2_s2orc_unsplit_val_physics.med-ph 14.34 14.46 16.50 14.29 14.09
m2d2_s2orc_unsplit_val_physics.optics 12.74 12.94 14.64 12.80 12.54
52Dataset Llama Mamba RWKV-4 xLSTM[7:1]xLSTM[1:0]
m2d2_s2orc_unsplit_val_physics.plasm-ph 13.65 13.81 15.77 13.69 13.44
m2d2_s2orc_unsplit_val_physics.pop-ph 13.80 13.67 15.17 13.60 13.41
m2d2_s2orc_unsplit_val_physics.soc-ph 12.79 12.97 14.80 12.83 12.66
m2d2_s2orc_unsplit_val_physics.space-ph 13.00 13.09 14.77 12.94 12.76
m2d2_s2orc_unsplit_val_physics_l1 15.57 15.43 16.97 15.40 15.18
m2d2_s2orc_unsplit_val_plasm-ph 13.65 13.81 15.77 13.69 13.44
m2d2_s2orc_unsplit_val_q-bio 13.69 13.87 15.75 13.75 13.50
m2d2_s2orc_unsplit_val_q-bio.BM 13.28 13.52 15.72 13.41 13.19
m2d2_s2orc_unsplit_val_q-bio.CB 12.06 12.34 14.21 12.19 11.97
m2d2_s2orc_unsplit_val_q-bio.GN 13.21 11.40 12.74 11.32 11.16
m2d2_s2orc_unsplit_val_q-bio.MN 11.96 11.95 13.36 11.90 11.70
m2d2_s2orc_unsplit_val_q-bio.NC 13.69 13.87 15.75 13.75 13.50
m2d2_s2orc_unsplit_val_q-bio.OT 14.90 14.94 17.16 14.92 14.73
m2d2_s2orc_unsplit_val_q-bio.PE 12.57 12.71 14.62 12.69 12.41
m2d2_s2orc_unsplit_val_q-bio.QM 12.49 12.69 14.44 12.56 12.40
m2d2_s2orc_unsplit_val_q-bio.SC 13.68 13.85 15.60 13.75 13.53
m2d2_s2orc_unsplit_val_q-bio.TO 13.49 13.53 15.32 13.48 13.33
m2d2_s2orc_unsplit_val_q-bio_l1 13.69 13.87 15.75 13.75 13.50
m2d2_s2orc_unsplit_val_q-fin.CP 11.37 11.61 13.36 11.41 11.28
m2d2_s2orc_unsplit_val_q-fin.EC 11.72 11.89 13.77 11.77 11.63
m2d2_s2orc_unsplit_val_q-fin.GN 13.79 13.91 15.73 13.83 13.61
m2d2_s2orc_unsplit_val_q-fin.MF 9.91 10.21 11.92 10.04 9.90
m2d2_s2orc_unsplit_val_q-fin.PM 11.00 11.31 13.14 11.14 10.94
m2d2_s2orc_unsplit_val_q-fin.PR 15.87 9.25 10.37 9.20 9.03
m2d2_s2orc_unsplit_val_q-fin.RM 11.35 11.49 13.08 11.41 11.22
m2d2_s2orc_unsplit_val_q-fin.ST 12.43 12.46 14.18 12.43 12.26
m2d2_s2orc_unsplit_val_q-fin.TR 12.79 13.14 15.32 12.89 12.74
m2d2_s2orc_unsplit_val_q-fin_l1 13.79 13.91 15.73 13.83 13.61
m2d2_s2orc_unsplit_val_quant-ph 11.18 11.44 13.18 11.32 11.11
m2d2_s2orc_unsplit_val_stat.AP 13.37 13.56 15.52 13.42 13.15
m2d2_s2orc_unsplit_val_stat.CO 13.07 12.56 14.42 12.46 12.24
m2d2_s2orc_unsplit_val_stat.ME 11.09 11.26 12.91 11.11 10.87
m2d2_s2orc_unsplit_val_stat.ML 11.13 11.39 13.29 11.23 11.06
m2d2_s2orc_unsplit_val_stat.OT 11.31 11.55 13.28 11.45 11.24
m2d2_s2orc_unsplit_val_stat_l1 13.07 12.56 14.42 12.46 12.24
m2d2_s2orc_unsplit_val_supr-con 11.57 11.66 13.13 11.53 11.30
m2d2_wikipedia_unsplit_val_Culture_and_the_arts12.30 11.90 12.82 11.78 11.66
m2d2_wikipedia_unsplit_val_Culture_and_the_arts_1_2C.1u3lture_and1_1H.7u4manities12.82 11.63 11.48
m2d2_wikipedia_unsplit_val_Culture_and_the_arts_1_4G.0a6mes_and1_3T.o8y6s 15.17 13.79 13.57
m2d2_wikipedia_unsplit_val_Culture_and_the_arts_1_2M.1a6ss_media11.80 12.74 11.79 11.55
m2d2_wikipedia_unsplit_val_Culture_and_the_arts_1_1P.7e5rforming_1a1r.t2s5 12.03 11.17 11.03
m2d2_wikipedia_unsplit_val_Culture_and_the_arts_1_0S.0p1orts_and_9R.e6c3reation 10.36 9.58 9.54
m2d2_wikipedia_unsplit_val_Culture_and_the_arts_1_2T.1h3e_arts_an1d1_.E85ntertainm1e2n.t83 11.73 11.58
m2d2_wikipedia_unsplit_val_Culture_and_the_arts_1_2V.3is6ual_arts12.09 13.05 11.99 11.87
m2d2_wikipedia_unsplit_val_General_referece 11.80 11.46 12.43 11.46 11.30
m2d2_wikipedia_unsplit_val_General_referece__Fu1rt0h.e5r2_research1_0t.o2o0ls_and_t1o0p.i9cs6 10.12 9.99
m2d2_wikipedia_unsplit_val_General_referece__Re1f1er.8en0ce_work1s1.46 12.43 11.46 11.30
m2d2_wikipedia_unsplit_val_Health_and_fitness 10.75 10.47 11.14 10.37 10.30
m2d2_wikipedia_unsplit_val_Health_and_fitness__E9x.e6r4cise 9.29 9.95 9.27 9.16
m2d2_wikipedia_unsplit_val_Health_and_fitness__H10e.a1lt0h_science9.80 10.43 9.71 9.56
m2d2_wikipedia_unsplit_val_Health_and_fitness__H9u.1m4an_medic8in.8e3 9.59 8.63 8.54
m2d2_wikipedia_unsplit_val_Health_and_fitness__N8u.9tr1ition 8.68 9.40 8.61 8.47
53Dataset Llama Mamba RWKV-4 xLSTM[7:1]xLSTM[1:0]
m2d2_wikipedia_unsplit_val_Health_and_fitness__P1u0b.7li5c_health10.47 11.14 10.37 10.30
m2d2_wikipedia_unsplit_val_Health_and_fitness__S1e2l.f9_1care 12.49 13.61 12.42 12.28
m2d2_wikipedia_unsplit_val_History_and_events 13.65 13.29 14.48 13.20 13.00
m2d2_wikipedia_unsplit_val_History_and_events__1B1y.7_7continent11.44 12.36 11.36 11.26
m2d2_wikipedia_unsplit_val_History_and_events__1B2y.7_8period 12.41 13.46 12.37 12.12
m2d2_wikipedia_unsplit_val_History_and_events__1B2y.3_6region 11.88 12.87 11.79 11.64
m2d2_wikipedia_unsplit_val_Human_activites 12.43 12.03 12.98 11.95 11.81
m2d2_wikipedia_unsplit_val_Human_activites__Hu1m2a.4n3_activitie1s2.03 12.98 11.95 11.81
m2d2_wikipedia_unsplit_val_Human_activites__Im1p2ac.4t_7of_huma1n2_.0ac5tivity 13.12 12.00 11.82
m2d2_wikipedia_unsplit_val_Mathematics_and_log1ic2.90 12.51 13.79 12.48 12.29
m2d2_wikipedia_unsplit_val_Mathematics_and_logi8c._2_4Fields_of_8m.2a6thematics9.37 8.28 8.06
m2d2_wikipedia_unsplit_val_Mathematics_and_log1ic3_.2_1Logic 12.87 13.90 12.85 12.67
m2d2_wikipedia_unsplit_val_Mathematics_and_log1ic2_.9_0Mathema1ti2c.s51 13.79 12.48 12.29
m2d2_wikipedia_unsplit_val_Natural_and_physical_9s.c1i9ences 8.22 8.81 7.97 7.96
m2d2_wikipedia_unsplit_val_Natural_and_physical_1s0c.9ie7nces__Bi1o0l.o7g0y 11.53 10.64 10.51
m2d2_wikipedia_unsplit_val_Natural_and_physical_1s1c.6ie9nces__Ea1r1th.3_6sciences12.28 11.22 11.05
m2d2_wikipedia_unsplit_val_Natural_and_physical_1s0c.4ie3nces__N1at0u.r1e1 10.95 10.00 9.82
m2d2_wikipedia_unsplit_val_Natural_and_physical_1s1c.4ie8nces__Ph1y1s.i0c9al_scienc1e1s.93 10.98 10.90
m2d2_wikipedia_unsplit_val_Philosophy_and_think1in1g.83 11.72 13.04 11.60 11.45
m2d2_wikipedia_unsplit_val_Philosophy_and_think1in2g.0_0_Philosop1h1y.61 12.66 11.57 11.43
m2d2_wikipedia_unsplit_val_Philosophy_and_think1in0g.9_4_Thinkin1g0.61 11.34 10.56 10.42
m2d2_wikipedia_unsplit_val_Religion_and_belief_s1y2s.t8e1ms 12.45 13.44 12.38 12.19
m2d2_wikipedia_unsplit_val_Religion_and_belief_s1y1s.t1e1ms__Alla1h0.80 11.66 10.71 10.58
m2d2_wikipedia_unsplit_val_Religion_and_belief_s1y1s.t4e6ms__Beli1e1f_.0s6ystems 11.86 10.95 10.85
m2d2_wikipedia_unsplit_val_Religion_and_belief_s1y2s.t3e8ms__Maj1o2r_.0b3eliefs_of_1t2h.9e_4world 11.91 11.79
m2d2_wikipedia_unsplit_val_Society_and_social_sc1i0e.n5c3es 10.24 11.03 10.16 10.05
m2d2_wikipedia_unsplit_val_Society_and_social_sc1i0e.n4c7es__Soci1a0l_.1sc6iences 10.95 10.14 10.04
m2d2_wikipedia_unsplit_val_Society_and_social_sc1i2e.n4c8es__Soci1e2ty.13 13.02 12.07 11.93
m2d2_wikipedia_unsplit_val_Technology_and_appli8e.d5_1sciences 8.18 8.66 7.93 7.88
m2d2_wikipedia_unsplit_val_Technology_and_appl1ie2d.4_s5ciences_1_2A.0g7riculture13.00 12.03 11.88
m2d2_wikipedia_unsplit_val_Technology_and_appl1ie3d.6_s2ciences_1_3C.2o3mputing 14.56 13.18 12.97
m2d2_wikipedia_unsplit_val_Technology_and_appl1ie3d.0_s0ciences_1_2E.n7g2ineering13.87 12.64 12.43
m2d2_wikipedia_unsplit_val_Technology_and_appl1ie4d.3_s4ciences_1_3T.r9a0nsport 15.20 13.94 13.73
manosphere_meta_sep_val_avfm 19.42 19.27 21.88 19.64 19.18
manosphere_meta_sep_val_incels 11.26 12.18 21.40 11.51 11.29
manosphere_meta_sep_val_mgtow 24.83 24.27 27.50 24.12 23.80
manosphere_meta_sep_val_pua_forum 24.22 23.85 26.52 23.86 23.52
manosphere_meta_sep_val_red_pill_talk 34.59 33.90 37.26 33.90 33.27
manosphere_meta_sep_val_reddit 20.63 19.78 21.10 19.94 19.58
manosphere_meta_sep_val_rooshv 22.46 22.17 24.78 22.01 21.69
manosphere_meta_sep_val_the_attraction 20.85 20.57 23.17 20.57 20.20
mc4_val-00000000 8.35 8.41 10.02 8.23 8.15
mc4_val-00000001 12.17 11.97 13.58 11.74 11.64
mc4_val-00000002 9.96 10.06 11.96 9.86 9.67
mc4_val-00000003 11.38 11.29 12.77 11.12 11.00
mc4_val-00000004 11.96 11.64 13.03 11.50 11.35
ptb_val 15.92 16.65 19.37 16.00 15.92
redpajama_val_arxiv 5.15 5.28 5.78 5.12 5.09
redpajama_val_books 12.91 12.71 13.60 12.61 12.50
redpajama_val_c4 13.01 12.51 13.55 12.49 12.27
redpajama_val_commoncrawl 10.90 10.56 11.70 10.52 10.35
redpajama_val_github 1.66 1.66 1.75 1.65 1.64
54Dataset Llama Mamba RWKV-4 xLSTM[7:1]xLSTM[1:0]
redpajama_val_stackexchange 3.73 3.72 4.03 3.68 3.63
redpajama_val_wikipedia 4.64 4.38 4.68 4.35 4.29
twitterAAE_HELM_fixed_val_AA 346.98 302.79 310.30 301.65 289.97
twitterAAE_HELM_fixed_val_white 118.62 107.34 109.13 107.65 105.13
wikitext_103_val 11.74 11.76 13.73 11.32 11.41
Table10: PPLEvaluations: Forthe1.3Bsizedmodelstrainedon300BSlimPajamatokens,theseare
thedetailedevaluationresultsontherespectivevalidationdatasets.
55