Scalable Vertical Federated Learning via Data Augmentation
and Amortized Inference
Conor Hassan∗1,2, Matthew Sutton1,2, Antonietta Mira3,4, Kerrie Mengersen1,2
1 Centre for Data Science, Queensland University of Technology
2 School of Mathematical Sciences, Queensland University of Technology
3 Euler Institute, Università della Svizzera italiana (USI)
4 Insubria University
Abstract
Vertical federated learning (VFL) has emerged as a powerful paradigm for collaborative
model estimation across multiple clients, each holding a distinct set of covariates. This paper
introduces the first comprehensive framework for fitting Bayesian models in the VFL setting.
We propose a novel approach that leverages data augmentation techniques to transform VFL
problems into a form compatible with existing Bayesian federated learning algorithms. We
present an innovative model formulation for specific VFL scenarios where the joint likelihood
factorizes into a product of client-specific likelihoods. To mitigate the dimensionality challenge
posed by data augmentation, which scales with the number of observations and clients, we
develop a factorized amortized variational approximation that achieves scalability independent
of the number of observations. We showcase the efficacy of our framework through extensive
numerical experiments on logistic regression, multilevel regression, and a novel hierarchical
BayesiansplitNNmodel. Ourworkpavesthewayforprivacy-preserving,decentralizedBayesian
inference in vertically partitioned data scenarios, opening up new avenues for research and
applications in various domains.
1 Introduction
Federated learning (FL) () has revolutionized the field of privacy-preserving and communication-
efficient learning, enabling collaborative model training across multiple clients under the coordina-
tion of a server, without the need for data sharing. This paradigm has unlocked new possibilities
for leveraging decentralized data while maintaining strict privacy constraints, paving the way for
innovative applications in various domains.
WhilethemajorityofFLalgorithmsfocusonthehorizontal FL(Yangetal.,2020)setting,where
eachclientpossessesacompletesetofcovariatesforauniquesubsetofobservations, thevertical FL
∗conorhassan.ai@gmail.com
1
4202
yaM
7
]OC.tats[
1v34040.5042:viXra(VFL) () setting remains relatively unexplored. In VFL, each client holds only a subset of desired
covariates for all observations. This work considers two settings: (1) the response variable is shared
among clients, and (2) the response variable is private to the server, and the clients send updates to
the server to fit the desired statistical model.
Inthispaper,weaimtoapproximatetheposteriordistributionoftheunknownvariablesgiventhe
observeddata. ThechallengeintheVFLsettingisthatthelikelihoodfunctionisnotseparableacross
clients, unlike in horizontal FL, where conditional independence is often assumed and exploited to
create distributed or private algorithms. This non-separability arises from the fact that each client
onlypossessesasubsetofthecovariates,makingitimpossibletocomputethelikelihoodofanysingle
client independently. Often, each client must contribute a value to a summation term required to
evaluatethelikelihood(). Securemulti-partycomputation (MPC)()isoftenemployedtoaddressthis
challenge, but it can significantly impact the scalability of the proposed methods, leaving room for
further research. Moreover, to our knowledge, there is currently no literature on Bayesian modeling
and inference in the VFL setting.
To overcome this challenge, we reframe the target posterior distribution via asymptotically-exact
data augmentation (AXDA) (). This approach introduces auxiliary variables that create condi-
tional independence between the client-specific parameters, allowing for independent updates on
each client. The augmented-variable model decouples the unknown variables from the likelihood,
enablinglocalupdatesoneachclientindependentlyoftheothers. Thisinterpretationcanbeseenas
the probabilistic analog of an optimization problem that solves subproblems requiring information
unique to each client, similar to the alternating direction method of multipliers (ADMM) (Boyd
et al., 2011).
Undertheaugmented-variablemodel,recentlydevelopedalgorithmicschemesintendedforhierar-
chicalmodelsinthehorizontalFLsetting,suchasstructuredfederatedvariationalinference(SFVI)
(Hassanetal.,2023),canbeadaptedtoupdatetheclient-specificparameters. Forthesettingwhere
the clients share the response variable, we develop a novel model formulation that expresses the
likelihood as a product of client-specific contributions.
The auxiliary variable schemes used here require the addition of variables to the model that are
equalinsizetothenumberofobservationstimesthenumberofclients. Toaddressthis,weproposea
factorized amortized variational approximation, removing the increase in computational cost due to
the dimensionality of the auxiliary variables per client. We provide numerical examples showcasing
performance on logistic regression, multilevel regression, and a novel hierarchical Bayes split NN ().
Contributions: The contributions of this paper are:
1. We provide the first Bayesian vertical FL methods by reformulating the posterior distribution
of a model in the VFL setting using data augmentation. We then use the abovementioned
methodstoprovidethefirstnumericalexamplesoffittingBayesianregressionmodels,including
multilevel models, in the VFL setting.
2. We create a novel model formulation for the VFL setting and show its improved performance
2relative to existing data augmentation methods.
3. Wedevelopafactorizedamortizedvariationalapproximationthatstopstheinferenceproblem
scaling with the number of observations.
4. We develop a novel hierarchical Bayes split learning model.
Thepaperproceedsasfollows. Section2providesthenecessarybackground. Section3introduces
the methods proposed. Section 5 provides numerical examples. Section 6 concludes the paper.
2 Preliminaries
This section provides the necessary background for auxiliary variable methods and structured fed-
erated variational inference.
2.1 Asymptotically-Exact Data Augmentation
In this paper, we are interested in targeting the posterior distribution π(θ|y) ∝ p(y|θ)p(θ) where
θ ∈ Rd is an unknown parameter, with prior distribution p(θ) and p(y|θ) is the likelihood of
the observed data y ∈ Rn. Direct inference of the posterior distribution can often be intractable
or computationally expensive. Data augmentation (DA) (Van Dyk & Meng, 2001) is a common
approach to address this issue by introducing auxiliary variables z ∈ Rm into the statistical model
throughamodel-specificjointdensityπ(θ,z|y). Thisjointdensityischosentobesimplertocompute
and satisfies the marginalization property,
(cid:90)
π(θ|y)= π(θ,z|y)dz. (1)
Asymptotically-exact data augmentation (AXDA) (Vono et al., 2020) is a flexible form of DA that
relaxes the marginalization property in (1) by introducing an augmented distribution p(z|θ;ρ) that
depends on a positive scalar hyperparameter ρ>0. The resulting augmented posterior distribution
is given by
(cid:90)
π(θ|y;ρ)= π(θ,z|y;ρ)dz, (2)
where π(θ,z|y;ρ) ∝ p(y|z,θ)p(z,θ;ρ), with augmented likelihood p(y|z,θ) and prior p(z,θ;ρ) =
p(z|θ;ρ)p(θ). The conditional distribution p(z|θ;ρ) is chosen to be a Gaussian with mean equal
to a function of θ and variance ρ2. The key property of AXDA is that, under the assumption
that the mean function of z|θ;ρ is chosen correctly, the augmented posterior distribution π(θ|y;ρ)
converges to the original posterior distribution π(θ|y) as ρ → 0, i.e. lim π(θ|y;ρ) = π(θ|y).
ρ→0
This property justifies the name “asymptotically-exact” and ensures that the AXDA framework can
provideavalidapproximationtotheoriginalposteriordistribution. AXDAoffersseveraladvantages
over traditional data augmentation methods. It provides a flexible framework that can be applied
to a wide range of models without requiring model-specific choices. Additionally, the introduction
3of the hyperparameter ρ allows for a trade-off between computational tractability and the accuracy
of the approximation.
2.2 Structured Federated Variational Inference
Variational inference (VI) () is a powerful algorithmic framework for approximating intractable
posterior distributions π(θ|y). The core idea behind VI is to introduce a family of tractable distri-
butions q (θ), parameterized by variational parameters ϕ, and find the member of this family that
ϕ
best approximates the true posterior.
The optimal variational approximation is found by minimizing the Kullback-Leibler (KL) di-
vergence between q (θ) and π(θ|y), which is equivalent to maximizing the evidence lower bound
ϕ
(ELBO),
L(ϕ)=E [logp(y,θ)−logq (θ)]. (3)
θ∼qϕ ϕ
The ELBO is a lower bound on the log marginal likelihood logp(y) and can be used as a surrogate
objectivefunctionforoptimization. Ifthevariationalapproximationq (θ)exactlyrecoversthetrue
ϕ
posterior π(θ|y), the ELBO is equal to the log marginal likelihood.
To optimize the ELBO, VI methods typically employ gradient-based optimization algorithms
(Ruder, 2017). A key challenge in VI is to obtain low-variance, unbiased gradient estimates of the
ELBO with respect to the variational parameters ϕ. The reparameterization trick () has emerged
as a powerful technique for addressing this challenge, which allows for an efficient and potentially
low-varianceestimateoftheELBO.Thereparameterizationinvolvesexpressingasampleofθ drawn
from q (θ) as a deterministic function of ϕ by transforming random noise ϵ: θ =f (ϵ). The noise
ϕ ϕ
variable ϵ is drawn from a distribution q , which is independent of ϕ. In this context, f might be,
ϵ ϕ
for example, a location-scale transform. Given this formulation, a resulting single-sample unbiased
Monte Carlo estimator of the desired gradient vector is the sticking-the-landing (STL) estimator
(Roeder et al., 2017),
∂f (ϵ)⊤
∇ˆ L:= ϕ ∇ [logp(θ,y)−logq (θ)]. (4)
ϕ ∂ϕ θ ϕ
StructuredFederatedVariationalInference(SFVI)(Hassanetal., 2023)isaframeworkthatuses
structured approximations () to fit hierarchical Bayesian models in the horizontal FL setting. The
models considered have a joint probability distribution of the form,
J
(cid:89)
p(y,z,θ)=p(z) p(θ |z)p(y|θ ,z),
j j
j=1
where θ = (θ⊤,...,θ⊤)⊤ such that the parameters θ only appear in the likelihood contribution
1 J j
from the j-th client and are conditionally independent across clients, while z are shared global pa-
rametersthatappearinalllikelihoodcontributionsacrossclients. SFVIproposestouseafactorized
4variational approximation,
J
(cid:89)
q (z,θ)=q (z) q (θ |z),
ϕ ϕz ϕj j
j=1
where q (z) is Gaussian distributed, parameterized by ϕ , and each q (θ |z) is Gaussian dis-
ϕz z ϕj j
tributed, parameterized by ϕ respectively, and potentially include dependencies on z.
j
Hassan et al., 2023 derive two algorithms, SFVI and SFVI-Avg, for combinations of models and
algorithmsthatsharethisform. Bothofthesealgorithmsenabletheupdatingofthelocalvariational
parameters ϕ for each client in parallel and privately (i.e. they are not shared with the server),
j
only requiring communicating gradient information regarding ϕ to the server at each iteration.
z
The SFVI framework provides an effective way to apply VI in the horizontal FL setting, allowing
for efficient and privacy-preserving model fitting. In the following sections, we will discuss how the
concepts of AXDA and SFVI can be leveraged to develop methods for the vertical FL setting.
3 Auxiliary Variable Methods for Vertical Federated Learning
Thissectionintroducesasymptotically-exact data augmentation (AXDA),similarinflavortothatof
forverticalFL.Thisschemeincorporatesauxiliary variables basedonthefeatures (e.g., covariates),
facilitating the construction of a VFL algorithm for specific classes of statistical models. We then
present an alternative modeling approach for specific vertical FL settings. As both the augmented-
variable model (presented in Section 3.2) and the power-likelihood model (presented in Section 3.3)
satisfythesameasymptotically-exactpropertyasAXDA,thetheorypresentedforAXDAisequally
applicable for the two following models presented.
3.1 Vertical Federated Learning Motivation and Setting
We aim to perform inference for the unknown variable of interest θ ∈Rp. In the vertical federated
learning (VFL) setting, the joint data matrix x∈Rn×p is distributed across J clients, such that it
can be written as x = (x 1,...,x J), where each x
j
∈ Rn×pj and (cid:80)J j=1p
j
= p. This distribution
of the data matrix occurs because each client holds a subset of the features for all observations,
necessitating collaboration among clients to perform inference on the complete set of features. We
also assume that the parameter vector θ is distributed across clients as θ =(θ⊤,...,θ⊤)⊤. In this
1 J
paper, we aim to approximate models of the form
 
(cid:18) (cid:12) J (cid:19) J
(cid:12)(cid:88) (cid:88)
π(θ|y)∝explogp y(cid:12) g j(x j,θ),γ + logp j(θ j), (5)
(cid:12)
j=1 j=1
in the VFL setting, where each client j has the choice to set their own prior distribution p(θ ) :
j
Rpj → R, for the parameters θ
j
associated with their subset of the data matrix x j, and γ is an
unknown parameter shared across the clients. The challenge arises in evaluating the likelihood
5p(y|(cid:80)J
g (x ,θ),γ) in the VFL setting because each client only has access to a subset of the
j=1 j j
data and parameters, making it impossible to compute the likelihood independently.
Running example: Consider linear regression with unknown mean and variance, where J clients
hold the desired covariates. By setting θ = β for all J clients, γ = σ2, and g (x,θ) = x⊤θ , the
j j j j j
likelihood f can be can be expressed as y|β,σ2 ∼N((cid:80)J x⊤β ,σ2).
j=1 j j
Inference of such a target density is difficult in the VFL setting because each client j would be
required to share the value of their likelihood contribution g (x,θ). Thus, it would not be possible
j
to perform multiple computation steps updating θ on client j without communicating with the
j
server. This issue motivates us to formulate the target density in an alternative manner to enable
theuseofanexistingFLalgorithmthatcanhandletheuniquechallengesposedbytheVFLsetting
while maintaining privacy.
3.2 Augmented-Variable Model
Here,wepresentanAXDAsettingsuitableforVFL.Letz =(z⊤,...,z⊤)⊤,wherez ∈Rn denotes
1 J j
the auxiliary variables introduced with respect to client j. The joint posterior of θ and z in the
augmented-variable model is proportional to
 
J J
(cid:88) (cid:88)
π(θ,z|y;ρ)∝explogp(y|z,γ)+ logp(z j|θ j;ρ)+ logp(θ j), (6)
j=1 j=1
where
p(y|z,γ):=p(y|(cid:80)J
g (x ,θ ),γ) is the likelihood, with each g (x ,θ )=z . This formu-
j=1 j j j j j j j
lation effectively decouples the likelihood from the original parameters θ, making it a function of
the auxiliary variables z.
The conditional distribution z |θ ;ρ is chosen to be a Gaussian distribution with mean equal to
j j
g (x ,θ ),andvarianceρ2. ThischoiceofconditionaldistributionismotivatedbytheAXDAframe-
j j j
work, which allows for a flexible and computationally tractable augmented posterior distribution.
The conditional distribution of θ is proportional to p(θ |θ ,z;ρ) ∝ p(z |θ ;ρ)p(θ ), depending
j j −j j j j
only on the auxiliary variable z and the prior p (θ ).
j j j
Running example: Consider a linear regression with unknown mean and variance, where J clients
hold the desired covariates. Setting γ = σ2 and θ = β , g (x ,θ ) = x⊤θ , for all J clients, the
j j j j j j j
augmented-variable model can be written as
J
(cid:0)(cid:88) (cid:1)
y|z ∼N z ,σ ,
j
j=1
z |θ ∼N(x⊤β ,ρ), j =1,...,J,
j j j j
θ ∼p(θ ), j =1,...,J.
j j
AsinAXDA,theaugmentedposteriordistributionπ(θ|y;ρ)ofthismodelconvergestotheoriginal
posterior distribution π(θ|y) as ρ → 0, i.e. lim π(θ|y;ρ) = π(θ|y). Given the formulation of
ρ→0
6the augmented-variable model, it follows that, for all pairs of clients i,k, we have the conditional
independence property θ ⊥⊥θ |z ,z , meaning that the elements of θ can be updated in parallel.
i k i k
In addition to satisfying the above conditional independence property, the augmented-variable
model can be fit in two FL scenarios:
1. The response variable y is shared across clients; clients can either update their respective z
j
and send it to the server or send the necessary gradient to the server, where the server carries
out the update; the server is responsible for redistributing updated values of z among clients.
2. The response variable y is not shared and is only available to the server; the clients send the
server the necessary gradients to update their z ; the server updates all z and redistributes to
j
the clients.
3.3 Power-Likelihood Model
The likelihood contribution g (θ ,x ) is known locally to client j in the VFL setting. This allows
j j j
for an alternative formulation of the target density, where the likelihood evaluated at client j is a
functionofg (x ,θ )ratherthantheauxiliaryvariablez ,asintheaugmented-variablemodel. This
j j j j
leadstoalog-likelihoodthatisaweightedsummationofJ client-specificlog-likelihoodcontributions.
We write the target density as
 
J J J
1 (cid:88) (cid:88) (cid:88)
π(θ,z|y;ρ)∝exp
J
logp(y|g j(θ j,x j),z −j)+ logp(θ j)+ logp(z j|θ;ρ). (7)
j=1 j=1 j=1
Itisimportanttonotethatthelikelihoodinthepower-likelihoodmodelhasadifferentformcom-
pared to the augmented-variable model. In the power-likelihood model, the likelihood is a product
ofJ client-specificlikelihoodcontributions,eachraisedtothepowerof1/J. Thisformulationdiffers
from the general framework presented in Section 3.2, where the likelihood is a function of the sum
of the g functions.
j
Running example: Consider the linear regression with unknown mean and variance. The power-
likelihood model can be written as
J (cid:18) J (cid:19)1/J
(cid:89) (cid:88)
y|θ,z ∼ N x⊤β + z ,σ ,
j j k
j=1 k̸=j
z |θ ∼N(x⊤β ,ρ), j =1,...,J,
j j j j
θ ∼p(θ ), j =1,...,J.
j j
Despite the difference in the likelihood formulation, the power-likelihood model still maintains
the AXDA property, where the augmented posterior distribution converges to the original posterior
7distribution as ρ→0, i.e. lim π(θ|y;ρ)=π(θ|y).
ρ→0
Unlike the augmented-variable model, the power-likelihood model can only be fit in the setting
where the response variable y is known to the clients, and the clients share the necessary gradients
with the server to update their respective variables z . This limitation arises because the likelihood
j
of the power-likelihood model has an explicit dependence on the data x and the parameters θ.
Figure1graphicallyillustratestheflowofinformationandthedependencystructurerequiredfor
the augmented-variable and the power-likelihood models, respectively.
Augmented-variable model Power-likelihood model
Client 1 Client 2 Client 1 Client 2
Figure 1: Graphical illustration of the dependency structure of the proposed models: augmented-
variable (left), power-likelihood (right)
The power-likelihood model provides an alternative formulation for the VFL setting that priori-
tizes improved modeling by allowing the likelihood evaluated at each client to be a function of their
local likelihood contribution. However, this comes at the cost of reduced flexibility in the settings
where the model can be fit, as it requires the clients to know the response variable. In contrast,
the augmented-variable model offers more flexibility regarding the settings where it can be applied,
albeit with a different likelihood formulation.
4 Algorithm details
This section describes how the augmented-variable model in Section 3.2 and the power-likelihood
modelinSection3.3canbeinferredintheVFLsetting. Typically,onewouldestimatetheposterior
distribution π(θ,z|y;ρ) using Markov chain Monte Carlo (MCMC) methods (Brooks et al., 2011).
However, updating the auxiliary variables specific to each client in the VFL setting requires the
conditionaldistributionz |z ,θ . Assumingthatwewanttoreducecommunicationofdata-related
j −j j
8quantities,suchasgradientsoflogdensities,fromclientstotheserver,weusealternativealgorithms
that can exploit the conditional independence structure among the clients given z.
Two such algorithms are structured federated variational inference (SFVI) (Hassan et al., 2023)
and the stochastic optimization via unadjusted Langevin (SOUL) () algorithm. These algorithms
leverage the conditional independence structure by allowing for local updates on each client and
reducing the need for frequent communication between the clients and the server. In this section,
wewillfocusondescribingSFVIindetail, whilethediscussionofSOULisprovidedinAppendixA.
We will explore two variational approximations suitable for the FL setting:
1. A structured variational approximation that employs ideas from amortized inference. This
approximation leverages the conditional independence structure of the model and enables
efficient local updates on each client.
2. A mean-field variational approximation, which assumes independence among the variables in
the approximation.
4.1 Amortized Structured Variational Approximation
For the structured variational approximation, we choose a similar form to Hassan et al., 2023, such
that
J
(cid:89)
q (θ,z)= q (θ )q (z |θ ),
ϕ,ψ ϕj j ψj j j
j=1
where q (θ ) and q (z |θ ) are the variational distributions for the parameters θ and auxiliary
ϕj j ψj j j j
variables z of client j, parameterized by variational parameters ϕ and ψ respectively.
j j j
For the parameterization of the variational approximation of θ , we choose that q (θ ) =
j ϕj j
N(µ ,Σ ), where ϕ = (µ⊤,vec(Σ )⊤)⊤ with the reparameterization applied such that θ =
θj θj j θj θj j
µ +Σ ϵ . For the parameterization of the variational approximation of z , we propose differ-
θj θj j j
ent parameterizations for the augmented-variable and power-likelihood models. In the augmented-
variable model, we choose q (z |θ ) = N(cid:0) µ (x⊤θ ),σ (x⊤θ )(cid:1) . For the power-likelihood model,
ψj j j j j j j j j
we choose q (z |θ ) = N(cid:0) µ (y,x⊤θ ),σ (y,x⊤θ )(cid:1) . In both cases, µ (·) and σ (·) are output
ψj j j j j j j j j j j
by a NN parameterized by ψ for j = 1,...,J. Using a neural network allows for a flexible and
j
expressive parameterization of the variational approximation for z , enabling it to capture complex
j
dependencies between z and θ .
j j
Asaconsequenceofthechoiceoffactorization,weapplytheappropriatejointreparameterization,
θ =µ +Σ ϵ , j =1,...,J, (8)
j θj θj j
z =µ (x⊤θ )+σ (x⊤θ )·τ , j =1,...,J, (9)
j j j j j j j j
(cid:8) (cid:9)J
where the vectors (τ ,ϵ ) are drawn from a standard Gaussian distribution.
j j j=1
9The amortized structured variational approximation is efficient because the cost of representing
q (z |θ ) scales with the size of the neural network parameterization ψ , rather than the dimen-
ψj j j j
sion of z or the number of observations. The variational approximation’s computational cost and
j
memory footprint remain manageable even when dealing with high-dimensional auxiliary variables
or large datasets, making it suitable for a wide range of practical applications.
4.2 Mean-field Variational Approximation
The mean-field approximation that we parameterize takes the form
J
(cid:89)
q (θ,z)= q (θ )q (z ),
ϕ,ψ ϕj j ψj j
j=1
wheretheonlydifferencewithSection4.1isthatwechooseq (z )suchthatq (z )=N(µ ,σ )
ψj j ψj j zj zj
where ψ = (µ⊤,σ⊤)⊤ such that the variational approximation for z no longer has dependence
j zj zj j
on θ . This form of variational approximation simplifies the reparameterization,
j
z =µ +σ ·τ , j =1,...,J, (10)
j zj zj j
where g is now only parameterized by ψ rather than both ψ and ϕ .
j j j
4.3 Algorithm details for Augmented-variable model
This section describes the algorithm steps required to fit the augmented-variable model using the
structured or mean-field variational approximation. We begin by considering the evidence lower
bound (ELBO) of the augmented-variable model using the structured variational approximation,
(cid:20) J (cid:20) (cid:21)(cid:21)
L=E (cid:81)J j=1q(zj|θj)q(θj) (cid:124)logp (cid:123)( (cid:122)y|z
(cid:125))+(cid:88)
j=1
log q
ψp( jz (zj| jθ |θj)
j) +log q
ϕp( jθ (θj)
j)
dependent (cid:124) (cid:123)(cid:122) (cid:125)
independent
(cid:20) (cid:21) J (cid:20) (cid:21)
=E (cid:81)J j=1q(zj|θj) logp(y|z)
+(cid:88)
j=1E q(zj|θj)q(θj) log q
ψp( jz (zj| jθ |θj)
j) +log q
ϕp( jθ (θj)
j)
J
(cid:88)
=L + L .
0 j
j=1
The ELBO decomposes into a term, L (in red), which involves the likelihood p(y|z) and requires
0
global information depending on all the parameters and auxiliary variables, and an independent
terms, L (in blue), which can be computed locally by each client. Due to the structure in both the
j
model and the variational approximation, the following gradients are equal to
∂L ∂L ∂L ∂L ∂L
= j and = 0 + j.
∂θj ∂θ
j
∂z
j
∂z
j
∂z
j
10Exploiting the sparsity in the Jacobian matrix of the reparameterization, the gradient estimate
of ∇(cid:98)ϕjL can be expressed as
∂f(ϵ )⊤ ∂L ∂g(τ ,ϵ )⊤ ∂L
∇(cid:98)ϕjL= ∂ϕj
∂θ
+ ∂j
ϕ
j
∂z
,
j j j j
∂f(ϵ )⊤∂L ∂g(τ ,ϵ )⊤(cid:20) ∂L ∂L (cid:21)
= j j + j j 0 + j , (11)
∂ϕ ∂θ ∂ψ ∂z ∂z
j j j j j
and the gradient ∇(cid:98)ψjL is given by
∂g(τ ,ϵ )⊤ ∂L
∇(cid:98)ψjL= ∂j
ψ
j
∂z
,
j j
∂g(τ ,ϵ
)⊤(cid:20)
∂L ∂L
(cid:21)
= j j 0 + j , (12)
∂ϕ ∂z ∂z
j j j
where, following the same color coding introduced above, terms in blue can be computed indepen-
dently by each client, and terms in red must be computed on the server.
When using the mean-field variational approximation, the dependency of the auxiliary variables
z
j
on the variational parameters ϕ
j
vanishes, resulting in a simplified gradient for ∇(cid:98)ϕjL,
∇(cid:98)ϕjL=
∂f ∂( ϕϵ j)⊤ ∂∂ θL
+
(cid:24)∂g (cid:24)(τ
∂(cid:24)j
ϕ, (cid:24)ϵ j)(cid:24)⊤(cid:24) ∂∂ z(cid:24) L(cid:24)
,
j j j j
(cid:124) (cid:123)(cid:122) (cid:125)
dependencyremoved
∂f(ϵ )⊤∂L
= j j. (13)
∂ϕ ∂θ
j j
Having derived the necessary gradients for updating the variational parameters ϕ and ψ , we now
j j
present Algorithm 1 for fitting the augmented-variable model using either the amortized structured
or the mean-field variational approximation. The algorithm follows an iterative scheme where, at
each iteration, the server calculates the gradient of the likelihood with respect to the auxiliary
variables and sends the respective gradients to the clients. The clients then compute the required
terms to update their parameters θ and auxiliary variables z locally and return the updated
j j
auxiliary variables to the server. This process is repeated for a specified number of iterations or
until convergence.
11Algorithm 1: SFVI for auxiliary-variable model
Input: Server: number of iterations N. Clients: initial local variational parameters ϕ ,ψ .
j j
Output: Clients: variational parameters ϕ ,ψ .
j j
1 Initialization step: Each client generates their value for z j and sends it to the server
2 for i=1,...,N do
3 Server computation
4 Receive z j from client j =1,...,J
5 Calculate ∂L0 for j =1,...,J
∂zj
6 Send ∂L0 to client j for j =1,...,J
∂zj
7 for each client j in parallel do
8 Receive ∂L0 from the server
∂zj
9 Generate θ j via (8)
10 Calculate ∇(cid:98)ϕjL via (11) for structured or (13) for mean-field
11 Calculate ∇(cid:98)ψjL via (12)
(cid:0) (cid:1)
12 ϕ j ←optimizer.step ∇(cid:98)ϕjL
(cid:0) (cid:1)
13 ψ j ←optimizer.step ∇(cid:98)ψjL
14 Generate z j via (9) for structured or (10) for mean-field
15 Send z j to server
4.4 Algorithm details for Power-likelihood Model
The ELBO of the power-likelihood model with the amortized structured variational approximation
is
(cid:20) J J (cid:20) (cid:21)(cid:21)
L=E (cid:81)J j=1qψj(zj|θj)qϕj(θj)
J1 (cid:88)
j=1(cid:124)logp(y (cid:123)|θ (cid:122)j,z −j
(cid:125))+(cid:88)
j=1
log q
ψp( jz (zj| jθ |θj)
j) +log q
ϕp( jθ (θj)
j)
dependent (cid:124) (cid:123)(cid:122) (cid:125)
independent
J (cid:20) (cid:21)
(cid:88) 1
= JE qϕj(θj)(cid:81)J k̸=jqψk(zj|θj) logp(y|θ j,z −j) (14)
j=1
J (cid:20) (cid:21)
+(cid:88)
E log
p(z j|θ j)
+log
p(θ j)
j=1
qψj(zj|θj)qϕj(θj) q ψj(z j|θ j) q ϕj(θ j)
J J
(cid:88) (cid:88)
= L + L .
0,j 1,j
j=1 j=1
Due to the dependence structure in the ELBO, the following gradients are equal to
∂L ∂L ∂L
= 0,j + 1,j
∂θj ∂θ
j
∂θ
j
and
J (cid:20) (cid:21)
∂L = ∂L 1,j +(cid:88) ∂L 0,k .
∂z ∂z ∂z
j j j
k̸=j
12Due to the sparsity in the Jacobian matrix of the reparameterization used by the amortized struc-
turedvariationalapproximation,thegradientoftheELBOwithrespecttothevariationalparameters
ϕ can be written as
j
∂f(ϵ )⊤ ∂L ∂g(τ ,ϵ )⊤ ∂L
∇(cid:98)ϕjL= ∂ϕj
∂θ
+ ∂j
ϕ
j
∂z
,
j j j j
=
∂f(ϵ
j)⊤(cid:20)
∂L
0,j +
∂L
1,j(cid:21)
+
∂g(τ j,ϵ
j)⊤(cid:20) (cid:88)J (cid:20)
∂L
0,k(cid:21)
+
∂L
1,j(cid:21)
, (15)
∂ϕ ∂θ ∂θ ∂ϕ ∂z ∂z
j j j j j j
k̸=j
where terms in blue can be computed on a particular client with no added information from other
clients required, and terms in red mean that information from other clients is required to calculate
this term. The gradient of the ELBO with respect to the variational parameters ψ are
j
∂g(τ ,ϵ )⊤ ∂L
∇(cid:98)ψjL= ∂j
ψ
j
∂z
,
j j
=
∂g(τ j,ϵ
j)⊤(cid:20) (cid:88)J (cid:20)
∂L
0,k(cid:21)
+
∂L
1,j(cid:21)
. (16)
∂ψ ∂z ∂z
j j j
k̸=j
Similar to the mean-field approximation with the auxiliary-variable model, when using the mean-
field approximation with the power-likelihood model, the gradient of the ELBO with respect to ϕ
j
simplifies to
∇(cid:98)ϕjL=
∂f ∂( ϕϵ j)⊤ ∂∂ θL
+
(cid:24)∂g (cid:24)(τ
∂(cid:24)j
ϕ, (cid:24)ϵ j)(cid:24)⊤(cid:24) ∂∂ z(cid:24) L(cid:24)
,
j j j j
(cid:124) (cid:123)(cid:122) (cid:125)
dependencyremoved
∂f(ϵ
)⊤(cid:20)
∂L ∂L
(cid:21)
= j 0,j + 1,j . (17)
∂ϕ ∂θ ∂θ
j j j
Withthegradientsforthevariationalparametersϕ andψ derived,wenowpresentAlgorithm2for
j j
fittingthepower-likelihoodmodelusingeithertheamortizedstructuredorthemean-fieldvariational
approximation. The algorithm follows an iterative scheme that requires two communication rounds
per iteration. First, the server receives the auxiliary variables z from each client and sends all
j
clients the complete set of auxiliary variables z. The clients then compute the gradients of the
likelihood with respect to the auxiliary variables of the other clients and send these gradients back
to the server. The server aggregates the gradients and sends the respective sums to each client.
Finally, the clients update their variational parameters and auxiliary variables locally and send the
updated auxiliary variables back to the server. This process is repeated for a specified number of
iterations or until convergence.
5 Numerical Examples
Weprovidenumericalresultsforthreeclassesofmodels. InSection5.1,thelogisticregressionmodel
example showcases the models and variational approximations that we describe and the effect that
13Algorithm 2: SFVI for power-likelihood model
Input: Server: number of iterations N. Clients: initial local variational parameters ϕ ,ψ .
j j
Output: Clients: variational parameters ϕ ,ψ .
j j
1 Initialization step: Each client generates their value for z j and sends it to the server
2 for i=1,...,N do
3 Server computation
4 Receive z j from client j =1,...,J
5 Send z to all clients
6 for each client j in parallel do
7 Receive z from the server
8 Generate θ j via (8)
9 Calculate
(cid:8)∂L0,j(cid:9)J
∂zk k̸=j
10 Send
(cid:8)∂L0,j(cid:9)J
to the server
∂zk k̸=j
11 Server computation
12 Receive
(cid:8)∂L0,j(cid:9)J
for k =1,...,J from the clients
∂zk k̸=j
13 Calculate (cid:80)J k̸=j ∂ ∂L z0 j,k for j =1,...,J
14 Send (cid:80)J k̸=j ∂ ∂L z0 j,k to the j-th client for j =1,...,J
15 for each client j in parallel do
16 Receive (cid:80)J k̸=j ∂ ∂L z0 j,k from the server
17 Calculate ∇(cid:98)ϕjL via (15) for structured or (17) for mean-field
18 Calculate ∇(cid:98)ψjL via (16)
(cid:0) (cid:1)
19 ϕ j ←optimizer.step ∇(cid:98)ϕjL
(cid:0) (cid:1)
20 ψ j ←optimizer.step ∇(cid:98)ψjL
21 Generate z j via (9) for structured or (10) for mean-field
22 Send z j to server
varying the value of the noise hyperparameter ρ and the number of clients J has on inference. In
Section 5.2, the multilevel Poisson regression model demonstrates the feasibility of fitting complex
modelswhilepotentiallyenhancingclientprivacythroughamulti-levelstructure. InSection5.3,we
demonstrate a novel hierarchical Bayes split NN model for the task of split learning, and compare
its’ benefits to the originally proposed split NN model.
5.1 Logistic Regression
Here, we fit a logistic regression model and the corresponding augmented-variable and power-
likelihood model to a synthetic data set. The descriptions for all three model variations are defined
in Table 1.
We generate the synthetic data from the underlying logistic regression model, with random stan-
dard Gaussian noise added to the linear predictor for each observation. The number of observations
is n = 500, and the number of covariates is p = 20. We generate all elements of the data matrix x
fromthestandardGaussiandistribution. Inthisexample, wecomparetheperformanceofthethree
models, fit using Markov chain Monte Carlo (MCMC), in particular with the No-U-Turn sampler
14Logistic regression model Augmented-variable model Power-likelihood model
y ∼Bernoulli(p ) y ∼Bernoulli(p ) y ∼(cid:81)J Bernoulli(p )1/J
i i i i i j=1 ij
logit(p )=b+x⊤β logit(p )=b+(cid:80)J z logit(p )=b+x⊤β +(cid:80)J z
i i i j=1 ij ij ij j m̸=j im
z ∼N(x⊤β ,ρ) z ∼N(x⊤β ,ρ)
ij ij j ij ij j
β ∼N(0,1) β ∼N(0,1) β ∼N(0,1)
k jk jk
b∼N(0,1) b∼N(0,1) b∼N(0,1)
Table 1: Comparison of the logistic regression model, augmented-variable model, and power-
likelihoodmodel. Inallmodels,i=1,...,N denotestheobservationindex. Inthelogisticregression
model,k =1,...,pdenotesthecovariateindex. Intheauxiliary-variableandpower-likelihoodmod-
els, j =1,...,J denotes the client index, k =1,...,p denotes the covariate index for client j, and
j
m ̸= j denotes the index for clients other than j. The auxiliary-variable z is introduced for each
ij
observation i and client j in both the auxiliary-variable and power-likelihood models.
True model Augmented-variable Power-likelihood
MCMC ✓ ✓ ✓
Mean-field VI ✓ ✓ ✓
Amortized VI ✗ ✓ ✓
Table2: Theeightcombinationsofmodelandinferencealgorithmswerun. Thecolumnsdenotethe
three models: the true model (i.e. the true data-generating model), the augmented-variable model,
and the power-likelihood model. The rows denote three inference algorithms: MCMC (specifically
the No-U-Turn sampler) and two “federated” variants, represented as two differing variational ap-
proximations, namely mean-field VI
(NUTS) fit in NumPyro (Phan et al., 2019), and two different variational approximations, a mean-
field approximation and the amortized approximation. Note that the posterior approximation for
the parameters β and b are the same throughout the two approximations, and the only difference
comes in the variational approximation for the parameters z. Table 2 shows the combination of
models and algorithms that we consider fitting in this example.
Wemanipulatetwokeyparameters: thenoisehyperparameterρ,testedatvalues0.5,1.0,2.0,and
the number of clients, considering scenarios with two clients (J = 2) holding ten covariates each,
and ten clients (J = 10) with two covariates each. These variations help us explore the models’
performance across different federated environments. Figure 2 shows the results from the two-client
example and Figure 3 shows the results from the ten-client example.
TheELBOvaluesforthetwo-andten-clientexamplesaregivenbyFigures2aand3arespectively.
The ELBO values show that in both the two- and ten-client examples, the power-likelihood model
outperforms the augmented-variable model. There is less of a difference between the two models
in the ten-client example compared to the two-client example; this is expected as the weightings of
each of the client-specific likelihood contributions reduce with the increase in the number of clients,
tending closer towards the augmented-variable model.
Figures 2b and 3b show marginal density plots for the same single parameter from MCMC and
variational approximations, where the thick blue line represents the true parameter value. We plot
15Augmented-variable model ELBO with 2 clients Power-likelihood model ELBO with 2 clients
140
160
180
200
MFVI, true model MFVI, true model
MFVI, =0.5 MFVI, =0.5
220
MFVI, =1.0 MFVI, =1.0
240 MFVI, =2.0 MFVI, =2.0
Amort. VI, =0.5 Amort. VI, =0.5
260 Amort. VI, =1.0 Amort. VI, =1.0
Amort. VI, =2.0 Amort. VI, =2.0
25000 50000 75000 100000 125000 150000 175000 200000 25000 50000 75000 100000 125000 150000 175000 200000
Iteration Iteration
(a)Theplotontheleft-handsideshowstheELBOvaluesofthevariationalapproximationsfitwiththe
augmented-variable model. The plot on the right-hand side shows the performance of the variational
approximations with the power-likelihood model. Both plots compare to the ELBO of the true logistic
regression model (blue).
MCMC and VI density estimates with 2 clients
5
True True True
Augmented Augmented Augmented
4 Power Power Power
3
2
1
0
5
True w/ MFVI True w/ MFVI True w/ MFVI
Augmented w/ MFVI Augmented w/ MFVI Augmented w/ MFVI
4 Augmented w/ amort. Augmented w/ amort. Augmented w/ amort.
Power w/ MFVI Power w/ MFVI Power w/ MFVI
Power w/ amort. Power w/ amort. Power w/ amort.
3
2
1
0
0.5 0.0 0.5 1.0 1.5 2.0 0.5 0.0 0.5 1.0 1.5 2.0 0.5 0.0 0.5 1.0 1.5 2.0
=0.5 =1.0 =2.0
(b)Marginaldensityplotscomparingtheperformanceofthethreemodelsforthesameparameter. Top
row shows MCMC results. Bottom row shows variational approximation results. True parameter value
is denoted by the vertical blue line.
Figure 2: ELBO and marginal density plots for the two-client example. Figure 2a shows the ELBO
values for a combination of models and variational approximations. Figure 2b shows marginal
densities for the various models using both MCMC and variational approximations.
the marginal densities of the same, single parameter, as there is no expected difference in quality of
the marginal densities across different parameters in the model. For lower values of ρ, i.e. ρ = 0.5
or ρ = 1.0, the amortized approximation tends to outperform MFVI. For high values of ρ, i.e.
ρ = 2.0, we see that MFVI outperforms the amortized approximation. This is expected as the
16
OBLE
CMCM
IVamortized approximation is estimating a shared function for the marginal posterior approximations
forz,givingdifferinginputsperobservation. Asρincreases,theparametersz havegreatervariance,
and it is now difficult to estimate a shared function suitable for all parameters.
Augmented-variable model ELBO with 10 clients Power-likelihood model ELBO with 10 clients
200
300
MFVI, true model MFVI, true model
400 MFVI, =0.5 MFVI, =0.5
MFVI, =1.0 MFVI, =1.0
500 MFVI, =2.0 MFVI, =2.0
Amort. VI, =0.5 Amort. VI, =0.5
Amort. VI, =1.0 Amort. VI, =1.0
600 Amort. VI, =2.0 Amort. VI, =2.0
25000 50000 75000 100000 125000 150000 175000 200000 25000 50000 75000 100000 125000 150000 175000 200000
Iteration Iteration
(a)Theplotontheleft-handsideshowstheELBOvaluesofthevariationalapproximationsfitwiththe
augmented-variable model. The plot on the right-hand side shows the performance of the variational
approximations with the power-likelihood model. Both plots compare to the ELBO of the true logistic
regression model (blue).
MCMC and VI density estimates with 10 clients
5
True True True
Augmented Augmented Augmented
4 Power Power Power
3
2
1
0
5
True w/ MFVI True w/ MFVI True w/ MFVI
Augmented w/ MFVI Augmented w/ MFVI Augmented w/ MFVI
4 Augmented w/ amort. Augmented w/ amort. Augmented w/ amort.
Power w/ MFVI Power w/ MFVI Power w/ MFVI
Power w/ amort. Power w/ amort. Power w/ amort.
3
2
1
0
1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0
=0.5 =1.0 =2.0
(b) Marginal density plots comparing the performance of the three models for the same parameter.
The top row shows MCMC results. The bottom row shows variational approximation results. True
parameter value is denoted by the vertical blue line.
Figure 3: ELBO and marginal density plots for the ten-client example. Figure 3a shows the ELBO
values for a combination of models and variational approximations. Figure 3b shows marginal
densities for the various models using both MCMC and variational approximations.
17
OBLE
CMCM
IV5.2 Poisson Multilevel Regression
In this example, we demonstrate the application of a multilevel Poisson regression model to analyze
a synthetic dataset designed to emulate the complexities of real-world health data associated with
Australian geography at the Statistical Area 2 (SA2) level, which includes approximately N =2000
areas. Our analysis focuses on cancer diagnosis counts per area as the response variable y ∈ Rn,
inspired by the data representation in the Australian Cancer Atlas (Duncan et al., 2019). We
consider a scenario where two clients each hold a unique set of SA2-level health-related covariates,
representedasx ∈RN×2 andx ∈RN×2. Thissetupreflectsthereal-worldchallengesofefficiently
1 2
combining disparate health datasets while addressing logistical and privacy concerns. Additionally,
we incorporate two publicly available variables: the population count per SA2 area (pop ) and a
i
remoteness level indicator (r ) for each area i. The log of the population count serves as an offset
i
term to model the rate of cancer diagnoses per area. At the same time, the remoteness indicator
allows us to explore how associations vary across different levels of remoteness.
The motivation for this example is twofold. First, we aim to demonstrate the feasibility of
fittingmultilevelmodelsintheverticalfederatedlearning(VFL)settingusingtheauxiliaryvariable
approaches described in this paper. Second, fitting multilevel models such as these might enable
individual clients to randomly choose the multilevel structure they want to model their data with
to hide information about their underlying data while still obtaining meaningful estimates of the
covariate effects of interest.
Weexaminetwomodelsinthisexample: thetrueunderlyingPoissonmultilevelmodelfromwhich
we simulate the synthetic dataset and the equivalent augmented-variable model. The specification
for the Poisson multilevel regression model is
y ∼Poisson(λ ), i=1,...,N,
i i
J
(cid:88)
log(λ )=b+log(pop )+ x⊤βri, i=1,...,N, j =1,...,J,
i i ij ij
j=1
βri ∼N(µri,σri), j =1,...,J,
ij j j
µri ∼N(0,1), j =1,...,J,
j
σri ∼HN(1), j =1,...,J,
j
b∼N(0,1).
18The specification for the equivalent augmented-variable model is
y ∼Poisson(λ ), i=1,...,N,
i i
J
(cid:88)
log(λ )=b+log(pop )+ z , i=1,...,N, j =1,...,J,
i i ij
j=1
z ∼N(x⊤βri,ρ), i=1,...,N, j =1,...,J,
ij ij ij
βri ∼N(µri,σri), j =1,...,J,
ij j j
µri ∼N(0,1), j =1,...,J,
j
σri ∼HN(1), j =1,...,J,
j
b∼N(0,1).
In both model specifications, i = 1,...,N denotes the index for the SA2 areas, j = 1,...,J
denotes the index for the clients, and r denotes the remoteness level for area i. The term βri
i ij
represents the varying slopes for the covariates held by client j for area i, which depend on the
remoteness level r . In the augmented-variable model, z denotes the auxiliary variable introduced
i ij
for area i and client j.
Thisexampleshowcasestheaugmented-variablemodel’spotentialbenefitsinthecontextofVFL.
By introducing auxiliary variables, the augmented-variable model allows for efficient and privacy-
preserving estimation of the multilevel model parameters without requiring direct sharing of the
client-specific covariates.
We generate the synthetic data using the true data-generating process of the multilevel Poisson
regression model. Each element of the data matrices x and x is drawn from a standard Gaussian
1 2
distribution. The population term is generated from a discrete uniform distribution between the
values of 250 and 350, reflecting a realistic range of population counts at the SA2 level. The
remotenessindicatorrandomlyanduniformlyassignseachobservationtooneoffivelevels,capturing
the variation in remoteness across Australian SA2 regions. After constructing the linear predictor
for each observation, we generate the response from a Poisson distribution.
Figure 4 shows posterior density estimates for the true Poisson multilevel model fit using both
MCMCandMFVI,andtheaugmented-variablemodelwithtwodifferentvaluesofρ,fitusingMCMC
andtheamortizedstructuredfamilyproposedinSection4.1. Posteriordensityestimatesforasingle
µ1 and β1 parameter are provided, but we observe similar relationships and patterns for different
1 1
parameters at the same level. For both ρ = 1 and ρ = 2, the augmented-variable model returns
reasonable estimates for µ using both MCMC and the amortized structured family, comparable to
1
the true multilevel model fit using either MCMC or MFVI. However, we note that during MCMC
samplingoftheaugmented-variablemodel,weobserveamultimodaldensityestimate,whichsignifies
potential issues. For the estimate of β , the MCMC of the augmented-variable model yields a wide
1
and uncertain posterior, whereas the amortized VI captures the shape more accurately. It is worth
noting that the MCMC and MFVI estimates for β in the true model are aligned almost precisely
1
195
True w/ MCMC
True w/ MFVI
4 Augmented w/ MCMC
Augmented w/ Amort. VI
3
2
1
0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
2.00
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00
0.0 0.2 0.4 0.6 0.8 1.0 1.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2
=1.0 =2.0
Figure 4: The top row shows posterior density estimates for the global mean effect µ of the first
1
covariate belonging to the first client. The bottom row shows posterior density estimates for one of
the β levels for the first covariate on the first client. The left column shows augmented-variable
1
models fit using ρ = 1, and the right shows augmented-variable models fit using ρ = 2. The blue
line shows the true parameter value.
with the true value (indicated by the blue line).
Usingthismodelandalgorithmapproach,theabilitytofitarbitrarymultilevelstructuresenables
clients to randomly choose the multilevel structure they want to use for modeling the data. This
increases privacy, as the values of the auxiliary variables that the clients must communicate depend
onamultilevelstructureunknowntotheotherclients. Furtherexplorationofthisaspectcouldhelp
quantify the privacy benefits afforded by this approach.
5.3 Hierarchical Bayes Split Neural Net
SplitNN()areaspecificmodeldevelopedforverticalFLapplicationswherebyclientsmaynothave
access to the response variable; sometimes this task is referred to as split learning. In this section,
weconsideraclassificationtaskwithtwoclasses. ThesplitNNvariantofalogisticregressionmodel
20
1
1is
y ∼Bernoulli(λ ), i=1,...,N,
i i
J
(cid:88)
logit(λ )= z , i=1,...,N
i ij
j=1
z =f (x ), i=1,...,N, j =1,...,J,
ij ϕj ij
where each f is a client-specific NN parameterized by ϕ , that outputs z ∈ RN. In order to
ϕj j j
fit the above model via gradient descent using automatic differentiation, updating the parameters
ϕ requires each client to calculate ∂p(yi|λi)∂zij. If each client communicates z to the server, the
j ∂zij ∂ϕj ij
server can calculate and return ∂p(yi|λi) to each client. Each client can independently calculate ∂hij
∂zij ∂ϕj
and thus have the necessary gradient information to calculate the required update.
Client 1
Client 1
sample
Server Server
Client 2
Client 2
sample
(a) Split NN (b) Hierarchical Bayes Split NN
Figure 5: Figure 5a shows the split NN model. Figure 5b shows our proposed hierarchical Bayes
split NN. In the split NN, each client learns a function f , parameterized as a NN with weights
ϕj
ϕ , that maps the covariates from each client to a vector z ∈RN. Each client communicates their
j j
z to the server, where the set of z ’s is used to create the linear predictor λ, which is then used
j j
to evaluate the objective function, i.e. likelihood, on the server. In the hierarchical Bayes variant,
the final-layer weights of the function f are denoted as w . These weights are treated as random
ϕj j
variables and are assigned a prior distribution. We dot-product the weights with the hidden vector
h to parameterize the mean of the prior distribution of an additional set of random variables z .
j j
Each client sends their respective parameters z to the server, and the server takes the same steps
j
as before.
Using the augmented-variable model and the amortized variational approximation, we develop a
21hierarchical Bayes variant of the split NN as follows,
y ∼Bernoulli(λ ), i=1,...,N,
i i
J
(cid:88)
logit(λ )= z , i=1,...,N,
i ij
j=1
z ∼N(h⊤w ,ρ), i=1,...,N, j =1,...,J,
ij ij j
h =f (x ), i=1,...,N, j =1,...,J,
ij ϕj ij
w ∼N(0,1), j =1,...,J.
j
In this hierarchical Bayes variant, the final-layer weights w of the function f are treated as
j ϕj
random variables and assigned a prior distribution. The dot product the hidden vector h with the
ij
weights w parameterizes the mean of the prior distribution for auxiliary variable z . Each client
j ij
sends their respective parameters z to the server, and the server performs the same steps as in the
j
standard split NN. Figure 5 illustrates the differences between the formulations of the hierarchical
Bayes split NN and the original split NN.
Bayesian models offer advantages such as natural probabilistic interpretations of uncertainty and
more accurately calibrated estimates, especially when trained on limited data. In this FL context,
the Bayesian analog of split NNs provides two specific benefits related to privacy and efficient
computation. First, the random variable z has a Gaussian prior distribution with a standard
ij
deviation equal to the hyperparameter ρ; this allows the client to send a noisy estimate of the
function output of f to the server, enhancing privacy. Second, the hierarchical Bayes variant
ϕj
offers computational advantages. In a split NN, only one gradient step can be taken for the weights
ϕ per communication round with the server, as the value ∂p(yi|λi) is required at each step. The
j ∂zij
hierarchical Bayes variant benefits from z being a stochastic function of x and the conditional
ij ij
independence of the parameters ϕ and local latent variables from the likelihood function given z .
j j
Thisallowsformultipleupdatesofϕ andw forafixedvalueofz withoutrequiringcommunication
j j j
with the server.
WeevaluatetheperformanceofthehierarchicalBayessplitNNusingtheheartdiseaseprediction
datasetfromKaggle(Fedesoriano,2021). Thisdataset,createdbyaggregatingfiveexistingdatasets,
contains 918 unique patients (i.e. observations) with 11 covariates and a binary response indicating
the presence of heart disease. We preprocess the data by normalizing all continuous variables via
a Z-score transform and creating one-hot-encodings for all categorical variables. The dataset is
split between two clients, with client 1 allocated the first five covariates and client 2 allocated the
remaining six covariates.
To assess the model’s performance, we employ a 10-fold cross-validation approach, splitting the
dataset into 10 different 90%/10% training and test splits. The model is trained and evaluated
on each split, ensuring that all observations appear in a test set exactly once. We use a standard
2-layer feedforward NN architecture with 8 dimensions for the weights in the first two layers and 2
dimensions for the weights in the final layer. All models are trained for 50,000 iterations using the
22Adam optimizer (Kingma & Ba, 2014) with a learning rate of 1e−3.
Prediction Average log- Average log-
Model accuracy (%) likelihood likelihood
per batch (all predictions) (incorrect predictions)
Split NN 80.73%±3.65 −2.21±4.41 −10.95±1.99
Hier. Bayes split NN, ρ=1 86.39%±4.20 −0.36±0.84 −1.99±1.42
Hier. Bayes split NN, ρ=5 84.98%±4.92 −0.59±1.71 −3.53±3.01
Hier. Bayes split NN, ρ=10 84.10%±5.23 −0.67±1.99 −3.89±3.53
Table 3: Comparison of the test predictive accuracy and average test log-likelihood for the split NN
versus the hierarchical Bayes split NN.
Table 3 presents the test predictive accuracy and average test log-likelihood for the split NN and
the hierarchical Bayes split NN with different values of the hyperparameter ρ (ρ = 1,5,10). The
results demonstrate that all variants of the hierarchical Bayes split NN outperform the standard
splitNN,withbetterperformanceobservedforsmallervaluesofρ,whichisexpected,becauseusing
higher values of ρ is analogous to reducing the amount of signal that the set of auxiliary variables
contains regarding the mean, which is the value of interest. Notably, higher variance is observed for
larger values of ρ, which is intuitive as it introduces additional variance to the model.
It is worth acknowledging that the split NN exhibits signs of overfitting. Various strategies,
such as sub-sampling, dropout (), or additional regularization terms in the objective function, can
mitigateoverfitting. WhiletheBayesianvariantiscloselyassociatedwithregularization, theabove-
mentioned strategies are equally applicable to both standard and Bayesian NNs. In this study,
we compare the Bayesian and non-Bayesian variants using a standard architecture and objective
function to ensure a fair comparison.
The performance of the hierarchical Bayes split NN supports the growing evidence that Bayesian
NNswithstochasticlastlayerscanprovideimprovedresults(). Thisnumericalsectiondemonstrates
the effectiveness of the proposed hierarchical Bayes split NN in the context of VFL, highlighting
its advantages in terms of privacy and computational efficiency. Future research could explore
the application of this approach to other datasets and investigate the impact of different model
architectures and hyperparameter settings on performance.
6 Discussion
Thispaperrepresents,toourknowledge,thefirstcontributiontotheliteratureonBayesianmethods
intheVFLsetting. WeformulatedauxiliaryvariablemethodsforfittingBayesianmodelsinVFLand
designed a new model type tailored for specific VFL settings, demonstrating improved performance
overexistingaugmented-variablemodels. Theauxiliaryvariablemethodsusedinthispaperincrease
the dimensionality of the model, with the number of additional parameters equaling the product
of the number of observations and clients. To address this, we propose a factorized amortized
approximation that prevents the dimensionality of the inference task from scaling with the number
23of observations while improving performance over standard Gaussian variational approximations,
exhibiting faster convergence and enabling subsampling.
We showcase the proposed methods through three illustrative examples. First, we fit a logistic
regression model and compare the estimates from the actual, augmented-variable, and novel power-
likelihood models using both MCMC and variational approximations, assessing performance with
varying values of the privacy-aiding hyperparameter ρ and a varying number of clients. Second, we
fitacomplexmultilevelregressionmodel,demonstratingtheeaseoffittingmultilevelmodelsandthe
ability of clients to create an additional multilevel structure unknown to other clients or the server,
potentially enhancing data privacy while maintaining accurate parameter estimates. Lastly, we in-
troduceanovelhierarchicalBayessplitNNandcompareittoasplitNN,showcasingthescalabilityof
theproposedBayesianauxiliaryvariablemethodswiththeamortizedapproximation,handlinglarge
numbers of observations and incorporating sub-sampling. The hierarchical Bayes variant exhibits
similar test accuracy but a higher average test log-likelihood than the original, indicating better-
calibratedestimates. AdditionalbenefitsofthehierarchicalBayesvariantincludetheadditionofthe
ρ hyperparameter, aiding privacy and enabling more computationally and communication-efficient
algorithmic schemes by exploiting the conditional independence created.
This work opens up numerous promising avenues for future investigation. While the current
algorithms are not embarrassingly parallel for updating the parameters z, specific schemes like the
SOULalgorithm (DeBortolietal., 2021), for whichwe providederivationsinAppendixA, orother
EM-style algorithms ()canfit thesamemodelswith feweriterationsfor estimatingz. However, the
SOUL algorithm provides only maximum a posteriori (MAP) estimates for z, resulting in narrower
posterior distributions for θ conditional on z rather than marginalizing over z. Moreover, using
algorithms like SOUL would make the inference problem’s scaling dependent on the dimensionality
of z, potentially causing issues as SOUL was initially used in low-dimensional z scenarios.
Another promising research direction involves developing mechanisms to estimate weight param-
etersforeachclient’slikelihoodcontributions, aidingmodelselectionandrankingtheimportanceof
variables and clients. Asynchronous client updates, an open question in most distributed Bayesian
inferencesettings(Winteretal.,2024),representanotheravenuetoelevatetheframework’spractical
utility. Incontrasttotheconventionalsynchronousapproach,whereclientupdatesarecoordinated,
leadingtobottlenecksastheclientcountincreases,asynchronousupdatesallowclientstosubmitup-
dates independently without strict synchronization demands. Asynchronous updates offer reduced
latency, increased resilience, and enhanced scalability, making the proposed methods more suitable
for real-world applications and industry settings.
References
Baldi, P., & Sadowski, P. J. (2013). Understanding dropout. Advances in neural information pro-
cessing systems, 26.
Blei, D. M., Kucukelbir, A., & McAuliffe, J. D. (2017). Variational inference: A review for statisti-
cians. Journal of the American statistical Association, 112(518), 859–877.
24Boyd, S., Parikh, N., Chu, E., Peleato, B., Eckstein, J., et al. (2011). Distributed optimization and
statistical learning via the alternating direction method of multipliers. Foundations and
Trends® in Machine learning, 3(1), 1–122.
Brooks, S., Gelman, A., Jones, G., & Meng, X.-L. (2011). Handbook of Markov chain Monte Carlo.
CRC press.
Byrd, D., & Polychroniadou, A. (2020). Differentially private secure multi-party computation for
federated learning in financial applications. Proceedings of the First ACM International
Conference on AI in Finance, 1–9.
Ceballos, I., Sharma, V., Mugica, E., Singh, A., Roman, A., Vepakomma, P., & Raskar, R. (2020).
Splitnn-driven vertical partitioning. arXiv preprint arXiv:2008.04137.
Chen,W.,Ma,G.,Fan,T.,Kang,Y.,Xu,Q.,&Yang,Q.(2021).Secureboost+:Ahighperformance
gradient boosting tree framework for large scale vertical federated learning. arXiv preprint
arXiv:2110.10927.
Cramer, R., Damgård, I. B., et al. (2015). Secure multiparty computation. Cambridge University
Press.
De Bortoli, V., Durmus, A., Pereyra, M., & Vidal, A. F. (2021). Efficient stochastic optimisation by
unadjustedlangevinmontecarlo:Applicationtomaximummarginallikelihoodandempirical
bayesian estimation. Statistics and Computing, 31, 1–18.
Duncan,E.W.,Cramb,S.M.,Aitken,J.F.,Mengersen,K.L.,&Baade,P.D.(2019).Development
of the australian cancer atlas: Spatial modelling, visualisation, and reporting of estimates.
International journal of health geographics, 18, 1–12.
Fedesoriano. (2021). Heart failure prediction dataset [Retrieved [Date Retrieved]].
Gal, Y., & Ghahramani, Z. (2016). Dropout as a bayesian approximation: Representing model un-
certainty in deep learning. international conference on machine learning, 1050–1059.
Gruffaz,S.,Kim,K.,Durmus,A.O.,&Gardner,J.R.(2024).Stochasticapproximationwithbiased
mcmc for expectation maximization. arXiv preprint arXiv:2402.17870.
Hardy, S., Henecka, W., Ivey-Law, H., Nock, R., Patrini, G., Smith, G., & Thorne, B. (2017).
Privatefederatedlearningonverticallypartitioneddataviaentityresolutionandadditively
homomorphic encryption. arXiv preprint arXiv:1711.10677.
Harrison,J.,Willes,J.,&Snoek,J.(2024).Variationalbayesianlastlayers.arXivpreprintarXiv:2404.11599.
Hassan, C., Salomone, R., & Mengersen, K. (2023). Federated variational inference methods for
structured latent variable models. arXiv preprint arXiv:2302.03314.
Hoffman, M., & Blei, D. (2015). Stochastic structured variational inference. Artificial Intelligence
and Statistics, 361–369.
Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., Bonawitz, K.,
Charles, Z., Cormode, G., Cummings, R., et al. (2021). Advances and open problems in
federated learning. Foundations and Trends® in Machine Learning, 14(1–2), 1–210.
Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980.
Kingma,D.P.,&Welling,M.(2013).Auto-encodingvariationalbayes.arXivpreprintarXiv:1312.6114.
25Kotelevskii, N., Vono, M., Durmus, A., & Moulines, E. (2022). Fedpop: A bayesian approach for
personalised federated learning. Advances in Neural Information Processing Systems, 35,
8687–8701.
Kristiadi, A., Hein, M., & Hennig, P. (2020). Being bayesian, even just a bit, fixes overconfidence in
relu networks. International conference on machine learning, 5436–5446.
Kuntz,J.,Lim,J.N.,&Johansen,A.M.(2023).Particlealgorithmsformaximumlikelihoodtraining
of latent variable models. International Conference on Artificial Intelligence and Statistics,
5134–5180.
Li, Y., Zhou, Y., Jolfaei, A., Yu, D., Xu, G., & Zheng, X. (2020). Privacy-preserving federated
learningframeworkbasedonchainedsecuremultipartycomputing.IEEEInternetofThings
Journal, 8(8), 6178–6186.
Liu, Y., Kang, Y., Zhang, X., Li, L., Cheng, Y., Chen, T., Hong, M., & Yang, Q. (2019). A commu-
nication efficient collaborative learning framework for distributed features. arXiv preprint
arXiv:1912.11187.
McMahan, B., Moore, E., Ramage, D., Hampson, S., & y Arcas, B. A. (2017). Communication-
efficientlearningofdeepnetworksfromdecentralizeddata.Artificial intelligence and statis-
tics, 1273–1282.
Osband, I., Wen, Z., Asghari, S. M., Dwaracherla, V., Ibrahimi, M., Lu, X., & Van Roy, B. (2024).
Epistemic neural networks. Advances in Neural Information Processing Systems, 36.
Phan, D., Pradhan, N., & Jankowiak, M. (2019). Composable effects for flexible and accelerated
probabilistic programming in numpyro. arXiv preprint arXiv:1912.11554.
Poirot, M. G., Vepakomma, P., Chang, K., Kalpathy-Cramer, J., Gupta, R., & Raskar, R. (2019).
Splitlearningforcollaborativedeeplearninginhealthcare.arXivpreprintarXiv:1912.12115.
Ranganath,R.,Tran,D.,&Blei,D.(2016).Hierarchicalvariationalmodels.Internationalconference
on machine learning, 324–333.
Rendell, L. J., Johansen, A. M., Lee, A., & Whiteley, N. (2020). Global consensus monte carlo.
Journal of Computational and Graphical Statistics, 30(2), 249–259.
Rezende, D. J., Mohamed, S., & Wierstra, D. (2014). Stochastic backpropagation and approximate
inference in deep generative models. International Conference on Machine Learning, 1278–
1286.
Roeder,G.,Wu,Y.,&Duvenaud,D.K.(2017).Stickingthelanding:Simple,lower-variancegradient
estimatorsforvariationalinference.AdvancesinNeuralInformationProcessingSystems,30.
Ruder, S. (2017). An overview of gradient descent optimization algorithms.
Sharma, M., Farquhar, S., Nalisnick, E., & Rainforth, T. (2023). Do bayesian neural networks need
to be fully stochastic? International Conference on Artificial Intelligence and Statistics,
7694–7722.
Sharrock, L., Dodd, D., & Nemeth, C. (2023). Coinem: Tuning-free particle-based variational infer-
ence for latent variable models. arXiv preprint arXiv:2305.14916.
Tan, L. S., Bhaskaran, A., & Nott, D. J. (2020). Conditionally structured variational gaussian
approximation with importance weights. Statistics and Computing, 30, 1255–1272.
26Thapa, C., Arachchige, P. C. M., Camtepe, S., & Sun, L. (2022). Splitfed: When federated learning
meetssplitlearning.ProceedingsoftheAAAIConferenceonArtificialIntelligence,36,8485–
8493.
Van Dyk, D. A., & Meng, X.-L. (2001). The art of data augmentation. Journal of Computational
and Graphical Statistics, 10(1), 1–50.
Vono, M., Dobigeon, N., & Chainais, P. (2019). Split-and-augmented gibbs sampler—application to
large-scale inference problems. IEEE Transactions on Signal Processing, 67(6), 1648–1661.
Vono, M., Dobigeon, N., & Chainais, P. (2020). Asymptotically exact data augmentation: Models,
properties, and algorithms. Journal of Computational and Graphical Statistics, 30(2), 335–
348.
Vono,M.,Paulin,D.,&Doucet,A.(2022).Efficientmcmcsamplingwithdimension-freeconvergence
rate using admm-type splitting. The Journal of Machine Learning Research, 23(1), 1100–
1168.
Wei,K.,Li,J.,Ma,C.,Ding,M.,Wei,S.,Wu,F.,Chen,G.,&Ranbaduge,T.(2022).Verticalfeder-
atedlearning:Challenges,methodologiesandexperiments.arXivpreprintarXiv:2202.04309.
Weng,H.,Zhang,J.,Xue,F.,Wei,T.,Ji,S.,&Zong,Z.(2020).Privacyleakageofreal-worldvertical
federated learning. arXiv preprint arXiv:2011.09290.
Winter, S., Campbell, T., Lin, L., Srivastava, S., & Dunson, D. B. (2024). Emerging directions in
bayesian computation. Statistical Science, 39(1), 62–89.
Yang, Q., Liu, Y., Chen, T., & Tong, Y. (2019). Federated machine learning: Concept and applica-
tions. ACM Transactions on Intelligent Systems and Technology (TIST), 10(2), 1–19.
Yang, Q., Liu, Y., Cheng, Y., Kang, Y., Chen, T., & Yu, H. (2020). Horizontal federated learning.
In Federated learning (pp. 49–67). Springer International Publishing. https://doi.org/10.
1007/978-3-031-01585-4_4
Zhang, C., Bütepage, J., Kjellström, H., & Mandt, S. (2018). Advances in variational inference.
IEEE transactions on pattern analysis and machine intelligence, 41(8), 2008–2026.
A Derivations for the SOUL algorithm
HerewedescribehowthemodelsformulatedinSections3.2and3.3canbeinferredintheVFLsetting
usingthestochasticoptimizationviaunadjustedLangevin (SOUL)(DeBortolietal.,2021)algorithm
oritsfederatedvariant(Kotelevskiietal.,2022)byexploitingtheconditionalindependencestructure
that we induce among the clients, given z.
A.1 Stochastic Optimization via Unadjusted Langevin
An alternative to using MCMC or variational approximation algorithms are maximum marginal
likelihoodestimation(MMLE)algorithms. OnemightviewtheuseofMMLEasanempirical Bayes
() procedure, where the auxiliary variables z are hyperparameters, and we find the maximum a
posteriori (MAP) estimates zˆ. The SOUL algorithm results in MAP estimates for zˆ, and draws
from the conditional target density p(θ|zˆ,y;ρ).
27Ifthedimensionislarge,theoptimizationofthehyperparameterscanbeachievedusingaRobbins-
Monro scheme ().
In the VFL setting with J clients, parameters θ = (θ 1⊤,...,θ J⊤)⊤, where θ
j
∈ Rpj, auxiliary
variables z = (z⊤,...,z⊤)⊤, where z ∈ Rn, the gradient with respect to z of the log marginal
1 J j
likelihood conditional on z and the hyperparameter ρ is written as
(cid:90) ∇ p(θ,y|z;ρ)
∇ logp(y|z;ρ)=
zj
p(θ|y,z;ρ)dθ,
zj p(θ,y|z;ρ)
(cid:90)
= ∇ logp(θ,y|z;ρ)p(θ|y,z;ρ)dθ,
zj
allowing us to calculate an estimator ∇(cid:98)zjlogp(y|z,ρ) for the gradient as,
M
∇(cid:98)zjlogp(y|z,ρ)= m1 (cid:88) ∇ zjlogp(θ k(m),y|z;ρ), θ(m) ∼p(θ j|y,z;ρ), m=1,...,M.
m=1
A.1.1 Augmented-variable model details
Due to the formulation of the augmented target density in Section 3.2, we can write the log condi-
tional density of θ , independent of other elements θ , as
j −j
(cid:0) (cid:1)
p(θ |y,z;ρ)∝exp −logp(z |θ ;ρ)−logp(θ ) , (18)
j j j j
and the gradient as
(cid:2) (cid:0) (cid:1)(cid:3)
∇ logp(θ,y|z;ρ)∝∇ exp −logp(y|z)−p(z |θ ;ρ) . (19)
zj zj j j
For each client j, both the above terms are independent of θ , enabling each client to draw new
−j
values of θ and update their estimate for zˆ . Each client communicate their new estimate of zˆ to
j j j
the server, that can then calculate and returns the value
(cid:80)J
zˆ to each client. The algorithm can
j=1 j
iterate in this manner until convergence or until reaching a stopping condition.
A.2 Power-likelihood model details
Wedemonstratethatthepower-likelihoodmodelproposedinSection3.3integrationoftheproposed
power -likelihood model can be fit with the SOUL algorithm. Recall that the log density of the
power-likelihood model, conditional on z is equal to
J J J
1 (cid:88) (cid:88) (cid:88)
logp(y,θ|z;ρ)= logp(y|θ ,z )+ logp(z |θ ;ρ)+ logp(θ ).
J j −j j j j
j=1 j=1 j=1
The gradient terms with respect to θ required for drawing new values of θ via an unadjusted
j j
28Langevin algorithm are
(cid:20) (cid:21)
1
∇ logp(y,θ|z;ρ)=∇ logp(y|θ ,z ;ρ)+logp(z |θ ;ρ)+logp(θ ) .
θj θj J j −j j j j
These gradients are conditionally independent across the J clients given z, allowing the unadjusted
Langevin steps to run in parallel across the J clients. However, added complexity arises when
considering the gradient with respect to z , which is equal to
j
(cid:90)
∇ logp(y|z;ρ)= ∇ logp(y,θ|z;ρ)p(θ|y,z;ρ)dθ, (20)
zj zj
(cid:90) (cid:20) 1 (cid:88)J (cid:21)
= ∇ logp(y|θ ,z ;ρ)+logp(z |θ ;ρ) p(θ|y,z;ρ)dθ, (21)
zj J k −k j j
k̸=j
(cid:20) (cid:20) J (cid:21)(cid:21)
1 (cid:88)
=E ∇ logp(y|θ ,z ;ρ)+logp(z |θ ;ρ) , (22)
p(θ|y,z;ρ) zj J k −k j j
k̸=j
(cid:20) (cid:20) (cid:21)(cid:21)
=E ∇ logp(z |θ ;ρ) (23)
p(θj|y,z;ρ) zj j j
J (cid:20) (cid:20) (cid:21)(cid:21)
(cid:88) 1
+ E ∇ logp(y|θ ,z ;ρ) . (24)
p(θk|y,z;ρ) zj J k −k
k̸=j
Here, the challenge is highlighted by the terms that are red in colour, indicating densities depen-
dent on terms unique to other clients. This dependency implies that updating z on each client,
j
independently and in parallel, is not feasible. Therefore, after running ULA to obtain M draws,
{θ(m)}M , client j computes the following J terms,
j m=1

(cid:80)M ∇ logp(y,θ(m),z ), required for update of z where k ̸=j,
Client j computes = m=1 zk j k k
(cid:80)M ∇ logp(z |θ(m);ρ), required for update of z ,
m=1 zj j j j
enabling the server to calculate the gradient in (20), and update the vector of variables z on the
server, and sending each set of updated variables z to each client.
A.3 Shared parameter details
Assume that we now want the expression ∇ p(y|z,σ), where
σ
(cid:90)
∇ p(y|z,σ)= ∇ logp(y,θ|z,σ;ρ)p(θ|y,z,σ;ρ)dθ, (25)
σ σ
(cid:2) (cid:3)
=∇ logp(y|z,σ)+logp(σ) . (26)
σ
If all of the values of z are communicated on the server, then we can run multiple steps of the
gradient descent using the above expression on the server, and then communicate the values of z
29and σ. For the augmented power likelihood model, the gradient is
(cid:90)
∇ p(y|z,σ)= ∇ logp(y,θ|z,σ;ρ)p(θ|y,z,σ;ρ)dθ, (27)
σ σ
=E (cid:2) ∇ [logp(y|θ,z,σ)+logp(σ)](cid:3) , (28)
p(θ|y,z,σ;ρ) σ
(cid:20) J (cid:21)
=E ∇ (cid:2)1 (cid:88) logp(y|θ ,z ,σ)+logp(σ)(cid:3) , (29)
p(θ|y,z,σ;ρ) σ J j −j
j=1
J
=∇ logp(σ)+ 1 (cid:88) E (cid:2) ∇ [logp(y|θ ,z ,σ)](cid:3) . (30)
σ J p(θj|y,z,σ;ρ) σ j −j
j=1
Ifeachclientj, communicatesE (cid:2) ∇ [logp(y|θ ,z ,σ)]totheserver, thentheservercan
p(θj|y,z,σ;ρ) σ j −j
update the value of σ.
Appendix References
Carlin, B. P., & Louis, T. A. (2000). Empirical bayes: Past, present and future. Journal of the
American Statistical Association, 95(452), 1286–1289.
Casella, G. (1985). An introduction to empirical bayes data analysis. The American Statistician,
39(2), 83–87.
De Bortoli, V., Durmus, A., Pereyra, M., & Vidal, A. F. (2021). Efficient stochastic optimisation by
unadjustedlangevinmontecarlo:Applicationtomaximummarginallikelihoodandempirical
bayesian estimation. Statistics and Computing, 31, 1–18.
Delyon, B., Lavielle, M., & Moulines, E. (1999). Convergence of a stochastic approximation version
of the em algorithm. Annals of statistics, 94–128.
Kotelevskii, N., Vono, M., Durmus, A., & Moulines, E. (2022). Fedpop: A bayesian approach for
personalised federated learning. Advances in Neural Information Processing Systems, 35,
8687–8701.
Robbins, H., & Monro, S. (1951). A stochastic approximation method. Annals of Mathematical
Statistics, 22, 400–407.
30