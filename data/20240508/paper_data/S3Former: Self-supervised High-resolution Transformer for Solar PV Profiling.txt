JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 1
S3Former: Self-supervised High-resolution
Transformer for Solar PV Profiling
Minh Tran Student, IEEE, Adrian De Luis, Haitao Liao Member, IEEE, Ying Huang Member, IEEE,
Roy McCann Member, IEEE, Alan Mantooth Fellow, IEEE, Jack Cothren Member, IEEE, Ngan Le Member, IEEE
Abstract—As the negative impact of climate change escalates, Therefore, accurate energy forecasting of such systems is
the global necessity to transition to sustainable energy sources crucial, especially in those areas with high PV adoptions, to
becomes increasingly evident. Renewable energies have emerged prevent grid congestion and imbalances [4].
as a viable solution for users, with Photovoltaic (PV) technology
being a favored choice for small installations due to its high To gain real-time insights into PV distribution and energy
reliability,competitivemarketandincreasingefficiency.Accurate production,recentresearchhasfocusedonprofilingPVinstal-
mapping of PV installations is crucial for understanding the lations using remote sensing imagery from unmanned aerial
trend of technology adoption and informing energy policy-
vehicles and satellites [6], coupled with Machine Learning
making. To meet this need, S3Former is introduced, which
and Computer Vision. Remote Sensing (RS) images provide
is designed to segment solar panels from aerial imagery and
provide size and location information critical for analyzing the anoverheadviewoftheEarth’ssurfaceanddefinethelevelof
impact of such installations on the grid. Although computer detail as Ground Sample Distance (GSD). RS imagery serves
vision has become a preferred choice for such implementations, multiple practical purposes, including climate change analysis
solar panel identification is challenging due to factors such as
[7], natural disaster monitoring [8], urban planning [9] [10],
time-varying weather conditions, different roof characteristics,
and land cover analysis [11].
Ground Sampling Distance (GSD) variations and lack of ap-
propriate initialization weights for optimized training. To tackle Deep Learning (DL) has shown remarkable image under-
thesecomplexities,S3FormerfeaturesaMaskedAttentionMask standing capabilities, even surpassing human performance. It
Transformer incorporating a self-supervised learning pretrained
has been applied to extract solar PV distribution through
backbone. Specifically, the model leverages low-level and high-
a two-step process: a classifier is trained to identify solar
level features extracted from the backbone and incorporates
an instance query mechanism incorporated on the Transformer panels to then be segmented such as Kasmi, et al., [12],
architecturetoenhancethelocalizationofsolarPVinstallations. HyperionSolarNet [13]. However, existing DL-based methods
Moreover,aself-supervisedlearning(SSL)phase(pretexttask)is have three major limitations:
introducedtofine-tunetheinitializationweightsonthebackbone
ofS3Former,leadingtoanoticeableimprovementontheresults. (i) The existing works employ a two-stage framework, con-
To rigorously evaluate the performance of S3Former, diverse sisting of separate classifier and segmentor networks.
datasetsareutilized,includingGGE(France),IGN(France),and
This approach heavily relies on the performance of
USGS (California, USA), across different GSDs. Our extensive
classifier network, leading to suboptimal learning in the
experiments consistently demonstrate that the proposed model
either matches or surpasses state-of-the-art models (SOTA) and segmentation network. They overlook the unique chal-
validatethebenefitofusingtheSSLmethodtoimprovetheseg- lengesposedbysolarpanels,inwhichsolarcellscanbe
mentationarchitecture.Sourcecodeisavailableuponacceptance. arranged on multitude of non-repeating structures. This
IndexTerms—AerialImages,SolarPVs,Segmentation,Trans- challenge is known as intra-class homogeneity and can
formers, Self-Supervised Learning. hinder network performance as it may lead to missing
elementsfromtheresultingmask.Thisproblemisfurther
exacerbated by a variety of choices for cell materials.
I. INTRODUCTION
(ii) The specific characteristics of aerial imagery are not
Photovoltaic (PV) energy production has grown rapidly in properly captured by mainstream initialization weights,
recent years due to improved cell manufacturing technology withmodelsmissingdetailingpresentonanimage.This
[1], competitive market, and increasing energy costs [2]. In problem is exacerbated at higher GSDs, significantly
2022, the U.S. Energy Information Administration reported a impairing the ability of DL methods to detect small
440%increaseinsmall-scalePVinstallationcapacity,reaching objects and potentially causing the resultant masks to
39.5 GW from 7.3 GW in 2014 [3]. omit portions of solar panels.
Whilesolarpanelsofferasustainablesourceofenergy,they (iii) These existing DL frameworks are originally designed
arefacedwithchallengesforenergyforecasting[4].PVenergy fornaturalimagesanddonotadequatelyaddressthespe-
relies on weather conditions and physical characteristics of cificchallengesencounteredinsolarPVimagery,leading
the PV module to operate. Although this information can to the misidentification of solar cells. This confusion is
be leveraged to calculate the predicted output of solar farms due to solar arrays sharing similar characteristics, such
over a time period, small-scale PV operates in decentralized asshapeorcolor,withcommonlyoccurringobjects(e.g.,
systems, and the real-time energy output of these systems windows, roof tiles). This challenge is known as inter-
is usually invisible to Transmission System Operators [5]. class heterogeneity.
4202
yaM
7
]VC.sc[
1v98440.5042:viXraJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 2
RGB Image Solar PV Groundtruth RGB Image Solar PV Groundtruth RGB Image Solar PV Groundtruth Intra-class homogeneity
Small object identification
Inter-class homogeneity
Fig.1. ExamplesofchallengingcharacteristicsofsolarPVsegmentationon(a)IGNFrance,(b)GGEFrance,(c)USGSCalifornia..Withinaclass,thereis
alargediversityinappearance:intra-classheterogeneity(red),somedifferentclassessharethesimilarappearance:inter-classhomogeneity(green),solarPV
aredenseandsmallsuchthattheyarehardlyidentifiable(blue).
Problems (i), (ii), (iii) are depicted in Fig. 1. To over- merousunannotatedaerialimagestoenhanceourmodel’s
come these limitations, we introduce a S3Former, an end- understandingofaerialimagerybeforetacklingthedown-
to-end transformer-based segmentation network, featuring stream task.
a Masked Attention Mask Transformer incorporating a self- • We conduct extensive experiments on three publicly
supervisedlearningpretrainedbackboneforfeatureextraction. available datasets (i.e. USGS, California [14]; IGN,
Specifically, to address issue (i), we develop S3Former, France [15]; USGS, France [15]), comparing the perfor-
a novel end-to-end transformer-based segmentation network. mance of S3Former to the current SOTA methods.
Unlike previous approaches that require localization prior
to segmentation, S3Former seamlessly segments solar pan- II. RELATEDWORK
els directly from images. This approach eliminates subopti- A. Deep Learning-based Solar PV Analysis
mal learning segmentation networks. Additionally, leveraging
In recent years, high-fidelity solar mapping has gained
transformer-based learning allows us to effectively address
significant importance due to the widespread adoption of
issue(ii).Byleveragingattentionmechanisms,themodelsare
photovoltaic (PV) energy, improved aerial imagery resolution,
able to learn correlations among multi-scale image features
andadvancementsindeeplearning(DL)techniques[16],[17].
from low to high resolutions. Consequently, we achieve im-
Early efforts focused on binary image classification to detect
provedsegmentationaccuracy,particularlyintheidentification
PV installations. DeepSolar [6] introduced a dual-stage Con-
of small objects. Regarding issue (iii), prior research employs
volutional Neural Network (CNN) for classification and seg-
backbone models pretrained on common scenes from large
mentation, marking a significant milestone. Subsequent work
imagedatasets,resultinginvisualfeatureslackingexposureto
integrated state-of-the-art CNN methods into classifier and
representationsfoundinaerialimagery.Thisdeficiencyiscrit-
segmentation architectures, while others concentrated solely
icalforaccuratesolarpanelsegmentation.Therefore,weintro-
on segmentation [18], [19]. These methods also enable PV
duce a pretraining phase to our network using self-supervised
capacityestimation[12],[20]andsocioeconomicanalyses[6].
learning, also known as a pretext task. This approach allows
A major challenge lies in creating suitable datasets, requiring
ustoleveragenumerousunannotatedaerialimagestoenhance
time-consuming annotation and addressing data sensitivity
our model’s understanding of aerial imagery before tackling
concerns. Organizations like IGN and USGS offer freely
the segmentation task (downstream task). The core concept of
accessibleaerialimages,butresearchersoftenfacelimitations
this pretext task is to eliminate non-semantic features from
in annotation and data release [13]–[15].
image representations. In essence, the representation of an
augmentedviewofanimageshouldpredicttherepresentation
B. Transformer
of another augmented view of the same image.
Transformer [21] and Vision Transformer (ViT) [22] have
In summary, the contributions of this work are as follows:
drawn significant interest in recent research. Initially devel-
• We propose S3Former, a novel end-to-end transformer- opedforNaturalLanguageProcessing(NLP),theTransformer
based segmentation framework for solar panels segmen- [21] employs a self-attention mechanism to improve learning
tation. The attention mechanism in our transformer mod- of long-range dependencies within data, facilitating paral-
ules learn correlations among multi-scale image features lelization for faster training on large datasets across multiple
from low to high resolutions, lead to accuracy improve- nodes. These attributes have proved invaluable for various
ment, particularly in the identification of small objects. NLP tasks [23]. ViT divides 2D images into patches, treating
• We introduce a pretraining phase to our network using them akin to words in an NLP Transformer. This approach
self-supervised learning that allows us to leverage nu- has shown efficacy, particularly with large datasets, enablingJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 3
effectiveinformationaggregationfromlowerlevelstoproduce
superior global context compared to CNNs. ViT has been
successfully applied to tasks such as image recognition [24],
videocaptioning[25][26],actionlocalization[27],[28],aerial
imaging [12], object detection [29], [30], image segmentation
[30], and medical imaging [31], showcasing its ability to syn-
thesize global information. Most deep learning (DL) methods
for solar PV analysis are currently adaptations of existing
(a)
DL techniques customized for solar PV data. However, these
approaches often struggle with the unique challenges of solar
PVdatasets,suchaslowresolution,intra-classvariability,and (b)
inter-class overlap. S3Former, on the other hand, is tailored
specifically for extracting solar PV data while effectively Fig. 2. Examples of six augmentations for the pretext task self-supervised
learning.(a):originalimage.(b)topline:colorjitter;bottomline,fromleft
addressing these challenges.
toright:randomcropping,Gausiannoise,horizontalflip.
C. Self-supervised Learning
F(3) ∈RC F(3)× 1H 6×W 16, and F(4) ∈RC F(4)× 3H 2×W 32, where
Self-supervised learning (SSL) is a rapidly growing tech-
C ,C ,C ,C denotethenumberofchannels.
niqueforlearningfeaturerepresentationswithoutlabeleddata, F(1) F(2) F(3) F(4)
• Pixel Decoder This module works as a visual feature
as opposed to supervised pre-training. Much of the research
enhancement with attention mechanism. The goal of the
in SSL focuses on discriminative techniques termed instance
Pixel Decoder is to enhance the relationships between
classification[32]–[35].Inthisapproach,eachimageistreated
the multi-scale feature maps F(1),F(2),F(3),F(4) cre-
as a separate class, and the model learns to differentiate
ated by the backbone and learn their correlations. This
between them using various data augmentations. However,
producesricherencodedfeaturesinthepixeldecoderlay-
employing an explicit classifier for all images encounters
ers D(1),D(2),D(3),D(4) with the same spatial feature
scalability issues as the dataset size increases [33]. To over-
shape as F(1),F(2),F(3),F(4), respectively.
come this, Wu et al. [35] propose using a noise contrastive
• Masked Attention Mask Transformer Given the en-
estimator (NCE) [36] for instance comparison instead of
riched visual features extracted from the preceding two
classification. Nevertheless, this method requires comparing
modules, denoted as D(1),D(2),D(3),D(4), this mod-
featuresacrossalargenumberofimagessimultaneously,often
ule focuses on predicting the segmentation mask M ∈
necessitating large batches [32] or memory banks [34], [35].
RH×W for identifying solar panels within the image.
Various adaptations have emerged, such as automatic instance
Each pixel M is assigned a value of 0 if it corresponds
grouping through clustering techniques [37], [38]. i
to the background; otherwise, it is set to 1, indicating
Recent studies have demonstrated the feasibility of learn-
the presence of a solar panel. This module employs a
ing unsupervised features without explicitly discriminating
transformer-based approach for mask prediction, inte-
betweenimages.Thisistypicallyachievedbyfirsttrainingthe
grating learnable queries to represent the instance mask
network unsupervised with a pretext task in the pre-training
within the image. The process involves decoding the
stage, then finetuning it on a downstream task. Grill et al.
learnable queries from the extracted visual features and
[39] introduce BYOL, a metric-learning framework where
associating them with the visual features to generate the
featuresaretrainedbyaligningthemwithrepresentationsfrom
segmentation mask.
amomentumencoder.WhilemethodslikeBYOLcanfunction
InordertotrainourS3Formertoperformthetask,wedoit
withoutamomentumencoder,theymayexperiencedecreased
in two phases: Pretext Task (Section III-A) and Downstream
performance [39], [40]. This observation is supported by
Task (Section III-B). In the pretext task phase, we pretrain
several other approaches, which demonstrate the alignment of
theBackboneNetworkwithapretexttaskinaself-supervised
representations, training features to match a uniform distribu-
manner.Thisphaseenhancesthemulti-scalefeaturesprovided
tion, or utilizing whitening techniques [41], [42]. Inspired by
by the Backbone Network with stronger representations of
BYOL,DINO[43]adoptsasimilararchitecturebutintroduces
aerial imagery. Thus, it enables us to address the afore-
the similarity matching loss, along with self-distillation [44].
mentioned intra-class homogeneity issues effectively. In the
downstream phase, we begin by initializing the pretrained
III. S3FORMER
weightsoftheBackboneNetworkintoourS3Former.Wethen
We design the architecture of our S3Former with three
proceedtotrainitonthesegmentationtask,supervisedbythe
components: Backbone Network, Pixel Decoder, and Masked
segmentation ground truth.
Attention Mask Transformer. More specifically:
• BackboneNetwork:Thismoduleisforextractingvisual
features from the input image I ∈ RH×W×3. The ex- A. Pretext Task
tracted visual feature is a set of four multi-scale feature A Backbone Network, foundational for feature extraction,
mapsF(i),wherei=1..4.Thesefeaturemapsarerepre-
is pre-trained on various tasks, notably effective in computer
sented as F(1) ∈ RC F(1))×H 4×W 4 , F(2) ∈ RC F(2)×H 8×W 8 , vision, e.g., object detection. AlexNet [45] and VGG [46]JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 4
Pretext Task
Student
View Backbone Softmax
EMA Cross Entropy Loss
Teacher
Image View Backbone
Softmax
stop
gradient
Transfer
Downstream Task Learnable queries
...
Backbone Pixel Decoder Masked Attention
Masked Transformer
Image Output Mask
Fig.3. OverallpipelineoftheproposedS3Former.Thetrainingpipelineincludestwophases:PretexttaskandDownstreamtask.Thegoalofthepretexttask
istolearntheoptimalparametersuchthatthebackbonenetworkcanextractsimilarrepresentationsfrombothVsandVt,regardlessofnon-semanticfactors
introduced by augmentation. In the downstream task, the Pixel Decoder learns the correlations within multi-scale feature maps and utilizes them to decode
enrichedfeaturemaps.Followingthis,theMaskedAttentionMaskTransformerintegratesthesedecodedfeaturemapswithN learnablequeries,employing
amaskedattentionmechanismtoaccuratelypredictthesegmentationmask.
are early deep learning backbones, while ResNets [47], like However, directly minimizing the distribution discrepancy
ResNet-18/34/50, are prominent for object detection. ResNets between the representations output by f could lead to a
θ
create multi-scale features, aiding computational tasks. problematicscenarioknownascollapse.Thisoccurswhenthe
Typically,BackboneNetworksarepretrainedonlarge-scale network learns to output a constant representation, ignoring
generalimagedatasetslikeImageNet[48],whereclassification the input data, in order to minimize the difference between
tasks supervise the learning process. These datasets predomi- representations. To address this issue, we employ the self-
nantlyconsistofimagesdepictingcommonobjectsandscenes. distillation technique used in DINO [43]. This involves intro-
Previous research in solar segmentation [19], [20], [49] often ducing both a student backbone, f θs, and a teacher backbone,
leverages pretrained Backbone Networks (e.g., ResNet [47]) f θt, which share similar architectures with f but are parame-
that are pretrained on these datasets, followed by fine-tuning terizedbytwodifferentsetsofweights,θ sandθ t,respectively.
on the specific downstream task of solar panel segmentation. In traditional distillation methods [51], a student network,
However, one limitation of these pretrained networks is their f θs, is trained to replicate the outputs of a given teacher
lack of exposure to aerial imagery, which is crucial for tasks network, f θt. However, in the context of SSL, this technique
like solar panel detection. As conventional datasets seldom is termed self-distillation, where the main network utilized
contain aerial images, there’s a gap in representation. To for downstream tasks is the teacher network. Moreover, the
address this, we employ SSL techniques to train the network teacher network is not directly trained via gradient descent
without relying on labeled data. This approach enhances the optimization; rather, it is constructed from past iterations of
network’sabilitytocapturefeaturesspecifictoaerialimagery, thestudentnetworkusingexponentialmovingaverage(EMA)
thus bridging the gap in representation. on the student weights. This learning approach helps prevent
Many successful SSL approaches build upon the cross- the networks from encountering the collapse issue previously
view prediction framework introduced in [50]. The idea is to mentioned.
remove non-semantic feature out of the image representation. Inspecific,ateachiteration,asillustratedinFig.3,Givenan
That means the representation of an augmented view of an inputimageI,bothstudentandteachernetworksoutputprob-
image should be predictive of the representation of another ability distributions over K dimensions denoted by p s ∈RK
augmented view of the same image. andp t ∈RK,respectively.Theseprobabilitiesareobtainedby
Consider a backbone network, denoted as f and parameter- normalizingtheoutputofthenetworkwithasoftmaxfunction.
izedbyθ.LetT andT representaugmentationssampledfrom More precisely,
s t
a set including color jitter, random crop, Gaussian noise, and
exp(f (T (I))(i)/τ )
horizontalflip(asillustratedinFig.2).Thesetransformations, p(i) = θs s s , (1)
s (cid:80)K exp(f (T (I))(k)/τ )
when applied to an image I, generate two augmented views, k=1 θs s s
V s and V t. The goal of the pretext task is to learn the exp(f (T (I))(i)/τ )
optimal parameter set θ∗ such that the network can extract p(i) = θt t t , (2)
t (cid:80)K exp(f (T (I))(k)/τ )
similar representations from both V
s
and V t, regardless of k=1 θt t t
non-semantic factors introduced by augmentation. with τ ,τ > 0 a temperature parameter that controls the
s t
In essence, the objective is for f to produce comparable sharpnessoftheoutputdistribution.Givenafixedteachernet-
θ
representations for diverse augmentations of the same image. workf ,welearntomatchthesedistributionsbyminimizing
θtJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 5
the cross-entropy loss with respect to the parameters of the Afterthedecodingprocessisfinished,thelastlayerqueries
student network θ : Q(3) is passed through two multi-layer perceptrons (MLPs)
s
f and f , respectively, to obtain the mask embedding
L =minH(p ,p ), (3) mask class
pretext
θs
t s Q
mask
∈RN×Ce and the classification embedding C∈RN.
Subsequently, the model generates the final proposal seg-
where H(a,b) = −alogb. Regarding teacher network’s
mentation masks Mˆ ∈ RN×H×W as Mˆ = Q ⊗E .
parameter θ , it is updated it from past iterations of the mask pixel
t
studentnetwork.Specifically,weadoptanexponentialmoving
average (EMA) on the student weights, i.e., a momentum Q =f (Q(3)) (4a)
mask mask
encoder [34], to update our teacher network. The update rule
Mˆ =Q ⊗E (4b)
is θ ←λθ +(1−λ)θ , with λ following a cosine schedule mask pixel
t t s
from 0.996 to 1 during training, followed [39], [43]. When C=f class(Q(3)) (4c)
the pretext task is finished training, the optimized teacher
Loss function. Let Mgt ∈ RH×W denote the ground
network’s parameter will be used as initialized weight of the
truth segmentation mask. The training procedure at each
Backbone network for the downstream task.
iteration for the downstream task optimizes the loss function
L as follows: first, it identifies the index ˆi of
downstream
B. Downstream Task
the proposal segmentation mask that minimizes the cross-
In the downstream phase, we begin by initializing the entropy loss with the ground truth mask, as shown in Eq. (5).
pretrained weights of the Backbone Network into our Subsequently, gradient descent optimization is performed on
S3Former. After the image is passed through the pre- thedownstreamlossL ,whichrepresentsthecross-
downstream
trained Backbone Network, we obtained the multi-scale fea- entropy loss between Mˆ and Mgt (refer to Eq. (6)).
ˆi
tures F(1),F(2),F(3),F(4). These features are now captured
(cid:104) (cid:105)
stronger representations of aerial imagery, trained from the ˆi=arg min −log(C )+L (Mgt,Mˆ ) (5)
i CE i
i∈1..N
pretexttask.Subsequently,thePixelDecodermodule,coupled
with the deformable multi-scale transformer encoder [52], en-
L =−log(C )+L (Mgt,Mˆ ) (6)
hances the relationships between the multi-scale feature maps downstream ˆi CE ˆi
createdbythebackboneandlearntheircorrelations.Thispro- Inference At test time, all N proposal masks Mˆ is joined
cess leads to richer encoded features D(1),D(2),D(3),D(4).
together based on classification embedding C into one final
Next, the features D(1),D(2),D(3),D(4) are taken as input
mask M ∈ RH×W by compute the dot product between the
for the Masked Attention Mask Transformer module to
classification embedding C and the predicted proposals mask
predict the segmentation mask of the solar panels. Mˆ as follow:
Firstly, the feature D(1) ∈ RCF(1)×H 4×W 4 is scaled to the M=CT ⊗Mˆ (7)
image’s original spatial shape of H ×W, creating the per-
pixel embeddings E ∈ RCe×H×W by using a sequence
pixel
of two 2×2 transposed convolutional layers with stride 2.
IV. EXPERIMENTS
Each pixel embedding of dimension C in E represents
e pixel
A. Datasets
the segmentation feature of the corresponding pixel on the
original image. OurS3FormerisbenchmarkedonthreeRGBaerialimagery
Secondly, N query embeddings Q ∈ RN×Ce is pro- datasets. We split them into 60/20/20 for train, test, and
posed to represent the N proposal segmentation masks of validation, dividing PV installation images equally among
the solar panels. These N query embeddings Q is further folds (Table I). The datasets, each containing PV installations
passedthroughaMaskedAttentionTransformerDecoder[49]. and background, were gathered from:
This module decodes Q from the encoded feature maps • USGS,California[14]:601TIFimages(5000×5000,30
D(1),D(2),D(3),D(4). The decoding procedure is done re- cm/pixel), cropped to 400×400, totaling 37660 images
currently, where each step is treated as a layer and there will (50.87% with PVs).
be 4 layers. We process each encoded feature from the lowest • IGN, France [15]: 17325 thumbnails (400 × 400, 20
to the highest resolution, i.e we start from D(4) all the way cm/pixel), with 44.34% containing solar installations.
to D(1). At each decoding layer, the query Ql+1 is decoded • USGS, France [15]: 28807 thumbnails (400×400, 10
from the previous layer’s Q(l) and the feature map D4−l. cm/pixel), with 46.11% showing solar installations.
Furthermore,ateachlayerl,anintermediatesetofpredicted
proposal masks M(cid:102)l ∈ RN×H×W is computed by correlating
Q(l) with per-pixel embeddings E via dot product across B. Evaluation Metrics
pixel
the feature space C e. Mathematically, M(cid:102)l = Q(l) ⊗E pixel. Toassessthemodel’sperformance,threecommonsemantic
ThesepredictedmasksM(cid:102)l arethenutilizedasattentionmasks segmentation scores are used: Intersection over Union (IoU),
to highlight salient regions on the feature map D(4−l). This F1 score, and Accuracy. These metrics measure the accuracy
selective focus aids the model in concentrating on salient ofthepredictedsegmentationmaskMagainstthegroundtruth
features, enhancing the efficiency and convergence speed of segmentation mask Mgt. Accuracy denotes pixel-wise accu-
the decoding process. racy, representing the proportion of correctly classified pixelsJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 6
TABLEI
DATASETSCHARACTERISTICSCOMPARISON.”POSITIVESAMPLES”INDICATEIMAGESCONTAININGSOLARPANELS,WHILE”NEGATIVESAMPLES”
LACKSOLARARRAYS.
Solar PV Datasets division
Fold
GGE,France USGS,California IGN,France
Negative Samples Positive Samples Negative Samples Positive Samples Negative Samples Positive Samples
Train 9,312 7,968 11,100 11,496 5,784 4608
Test 3,104 2,656 3,700 3,832 1,928 1,536
Validation 3,104 2,656 3,700 3,832 1,928 1,536
Total 28,807 37,660 17,325
TABLEII
PERFORMANCECOMPARISONBETWEENOURS3FORMERWITHEXISTINGDL-BASEDSOLARPVPROFILINGMETHODS.THEBOLDAND
ITALICUNDERLINEREPRESENTTHEBESTANDSECONDBESTPERFORMANCES.
SolarPVSegmentationPerformanceoneachdataset
Methods GGE,France USGS,California IGN,France
IoU F1-score Accuracy IoU F1-score Accuracy IoU F1-score Accuracy
Zechetal.,[19] 68.59 81.40 77.79 69.80 73.29 82.20 38.60 55.69 45.19
3D-PV-Locator[20] 80.70 89.30 90.70 80.60 89.30 87.40 53.10 69.40 66.40
Zhuang,etal.,[18] 76.60 86.69 84.20 84.39 91.60 90.89 48.50 65.30 59.60
Hyperion-Solar-Net 81.49 88.46 89.80 83.04 90.73 90.74 58.80 74.06 81.67
OurS3Former 79.56 88.61 93.20 89.05 94.21 94.33 59.22 74.39 82.96
TABLEIII
PERFORMANCECOMPARISONBETWEENOURS3FORMERWITHEXISTINGDL-SEGMENTATIONNETWORKSONVARIOUSBACKBONES.THEBOLDAND
ITALICUNDERLINEREPRESENTTHEBESTANDSECONDBESTPERFORMANCESFOREACHBACKBONENETWORK
SolarPVSegmentationPerformanceoneachdataset
Backbones Networks GGE,France USGS,California IGN,France
IoU F1-score Accuracy IoU F1-score Accuracy IoU F1-score Accuracy
FCN[53] 74.10 85.12 86.59 63.02 77.32 75.55 45.23 62.29 59.97
UNet[54] 76.60 86.69 84.20 84.39 91.60 90.89 48.50 65.30 59.60
PSPNet[55] 77.79 87.50 85.50 77.30 87.19 86.10 50.90 67.50 62.90
ResNet-50 UperNet[56] 79.40 88.49 89.80 84.50 91.60 90.49 52.89 69.19 65.49
Mask2Former[57] 74.14 85.15 90.73 85.33 92.08 92.80 54.09 70.21 87.63
OurS3Former 79.56 88.61 93.20 89.05 94.21 94.33 59.22 74.39 82.96
FCN[53] 73.20 84.53 87.14 61.83 76.41 73.55 45.52 62.55 62.43
UNet[54] 68.59 81.40 77.79 69.80 73.29 82.20 38.60 55.69 45.19
PSPNet[55] 78.29 86.29 87.80 76.70 86.79 85.50 48.80 65.60 59.10
ResNet-101 UperNet[56] 79.19 88.40 90.10 83.79 89.30 91.20 52.20 68.59 65.10
Mask2Former[57] 77.03 87.02 92.39 86.98 93.03 94.10 49.34 66.08 66.52
OurS3Former 79.68 88.69 92.91 89.24 94.13 94.56 58.69 73.89 82.87
overthetotal.IoUmeasuresmasksimilaritybycomputingthe C. Baselines
intersection over the union of the two masks:
|M∩Mgt|
IoU = (8)
|M∪Mgt| SolarPVprofilingmethods.TheS3Formerwascompared
with SOTA solar PV profiling methods, namely Zech et al.
Lastly, F1-score is computed on the comprehension of four
[19],3D-PV-Locator [20],Zhuang,etal. [18]andHyperion-
keyvalues:truepositive(TP),truenegative(TN),falsepositive
Solar-Net [13]. The evaluation was conducted across three
(FP), and false negative (FN). Each of them is denoted as:
datasets using ResNet 101 as the backbone architecture.
H W H W Deep Learning (DL) Segmentation methods. To au-
TP= (cid:88) (cid:88) Mg ht ,w∧M h,w;TN = (cid:88) (cid:88) ¬(Mg ht ,w∨M h,w) thors’ knowledge, there are currently no solar PV profiling
h=1w=1 h=1w=1
(9a) methods employing an end-to-end network for predicting
H W H W solar panel segmentation without prior localization. There-
FP= (cid:88) (cid:88) ¬Mg ht ,w∧M h,w;FN = (cid:88) (cid:88) Mg ht ,w∧¬M h,w fore, state-of-the-art DL segmentation networks are employed
h=1w=1 h=1w=1 for comparison with the S3Former model. These methods
(9b)
include FCN [53], UNet [54], PSPNet [55], UperNet [56],
Then, F1-score is computed as follow:
Mask2Former [57]. The evaluation is performed across three
TP datasets, utilizing ResNet-50 and ResNet-101 as backbone
F1-score= (10)
TP+ 1(FP+FN) architectures.
2JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 7
TABLEIV
ABLATIONSTUDYONAUGMENTATIONSUSEINPRETEXTTASKSFOROURS3FORMER,EMPLOYINGRESNET-50BACKBONEACROSSTHREEDATASETS.
Augmentations Segmentation performance on each datasets
GGE,France USGS,California IGN,France
Color Gaussian Horizontal
Jitter Noise Flip IoU F1-score Accuracy IoU F1-score Accuracy IoU F1-score Accuracy
✗ ✗ ✗ 75.99 86.25 92.20 86.28 92.56 92.76 57.35 72.74 77.53
✗ ✗ ✓ 76.43 86.64 92.61 86.72 92.89 93.13 57.77 73.23 77.87
✗ ✓ ✗ 79.36 88.49 90.99 86.38 92.69 93.59 54.40 70.46 83.94
✓ ✗ ✗ 79.34 88.46 92.65 88.00 93.62 92.95 56.93 72.56 80.35
✓ ✓ ✓ 79.56 88.61 93.20 89.05 94.21 94.33 59.22 74.39 82.96
a) IGN, France b) GGE, France c) USGS, California
(i)
Background
class
PV Installation
class
(ii)
RGB Image
cut-out
Ground Truth
(iii) cut-out
DeepLabv3+
cut-out
(iv) UperNet
cut-out
S3Former
cut-out
(v)
Fig. 4. Qualitative comparison on (a) IGN France, (b) GGE France, (c) USGS California. From top to bottom: (i) Original RGB Image, (ii) Groundtruth,
(iii)Upernet([56]),(iv)DeepLabv3+([58])and(v)S3Former.
D. Quantitative Results and Analysis Table III presents performance comparisons between
S3FormerandtheSOTADLsegmentationmethods.Overall,
Table II summarizes the performance of S3Former against S3Former achieves highest performance on most datasets and
current SOTA solar PV profiling methods. This segmen- metrics across different backbone ResNet-50 and ResNet-
tation model outperforms Zech et al. [19], 3D-PV-Locator 101. Specifically, regarding GGE, France., S3Former demon-
[20], Zhuang, et al. [18] and Hyperion-Solar-Net [13] on strate significant improvements over most of SOTA meth-
all three datasets. For instance, when analyzing the USGS ods. On the ResNet-50 backbone, S3Former outperforms the
California dataset, the S3Former model exhibits substantial second best method, UperNet ( [57]), by 0.16% on IoU
improvements,includinganimpressive4.66%increaseinIoU, and 0.12% on F1-score and improves Mask2former( [57])
a notable 2.61% enhancement in F1-score, and a substantial by 2.47% on Accuracy. When benchmarked on the ResNet-
boost of 3.56% in Accuracy compared to the second-best 101 backbone, S3Former gains 0.49% IoU and 0.29% F1-
CNN method Zhuang, et al., [18]. Similarly, on IGN France, score on UperNet( [56]) and improves the Accuracy result
the model showcases remarkable advancements, with gains of Mask2former( [57]) by 0.52%. On USGS, California.,
of 0.42%, 0.33%, and 1.29% in IoU, F1-score, and Accu- S3Former displays notable gains over SOTA methods on both
racy, respectively, when compared to the runner-up method, ResNet backbones. On the ResNet-101 backbone, when com-
Hyperion-Solar-Net [13].JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 8
RGB Image Ground Truth w/o pretext w/ pretext
Background PV Installation Intra-class homogeinity Inter-class homegeinity Small Object Incorrect Annotations
class class cut-out cut-out cut-out cut-out
Fig. 5. Extended qualitative comparison on different datasets (a) IGN France, (b) GGE France, (c) USGS California. From left to right: RGB Image,
Ground-Truth,w/opretextandw/pretext.SpecialcaseshighlightingthestrengthofbothmodelsandimprovementsofS3Formerwithrespecttopretexttask
pretrainingwereselected:inter-classhomogeneity(green),intra-classheterogeneity(red),andsmall-objectidentification(blue).Casesofmissingannotations
onthedatawerehighlighted(purple).
paredtothetopperformingCNN-basedmodelUperNet([56]). 4.18% on IoU and F1-score, respectively.
The difference with Mask2former( [57]) is still remarkable at
1.8%, 1.02% and 0.29%. For ResNet-50, S3Former improves E. Ablation study
3.10% on Accuracy with respect to UperNet( [56]). On IGN, Table IV illustrates the difference between the S3Former
France.,despiteitslowerresolution,S3Formerexhibitsrobust network performance using different augmentations, namely
scores against other models on the ResNet backbones. The ColorJitter,GaussianNoiseandHorizontalFlip.Thebaseline
dataset’srelativelymodestperformancecanbeattributedtoits performancewithoutanyaugmentationsexhibitsdecentresults
lower resolution and limited availability of imagery. Similar across all datasets, with the highest scores observed in GGE,
to the GGE France dataset, the granularity in the imagery France. Introducing flip augmentation marginally improves
could impact the model’s efficiency in detecting installations. performance, while Gaussian augmentation notably enhances
Notably, when benchmarked on the ResNet-101 backbone, it,particularlybenefitingtheGGE,FranceandUSGS,Califor-
S3Former showcases improvements of 4.21% on IoU, 2.54% nia datasets. Moreover, color augmentation leads to improved
onUperNet([56]),and13.14%onAccuracyincomparisonto performance,especiallynoticeableintheIGN,Francedataset,
Mask2Former ( [57]). On the ResNet-50 backbone, S3Former though its impact is less pronounced compared to Gaussian
improves the results of Mask2Former ( [57]) by 5.13% and augmentation. The combination of all augmentations yields
ecnarF
,NGI
)a
ecnarF
,EGG
)b
ainrofilaC
,SGSU
)cJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 9
the best overall performance across all datasets, indicating Overall performance: Fig. 5 exhibits S3Former’s superior
their collective positive contribution to the model’s ability to capability to delineate small installations and enhance the
generalize across diverse datasets. model’s proficiency in distinguishing solar cells from back-
ground objects. This reinforces the robust features extracted
F. Qualitative Results and Analysis by the pretext task pretraining in S3Former.
Comparison between S3Former and DL-Segmentation
Networks Fig. 4 provides a qualitative comparison of our V. CONCLUSION
S3Former against widely-used baseline models: DeepLabV3+ We present S3Former, a novel approach aimed at segment-
( [58]) and UperNet ( [56]). We highlight the advancements ing solar panels from aerial imagery and providing crucial
of our model in addressing challenges previously outlined on: size and location data essential for grid impact analysis of
Small objects: Fig. 4 highlight S3Former’s proficiency in such installations. S3Former features a Masked Attention
identifying small objects, even as diminutive as solar cells Mask Transformer coupled with a self-supervised learning
measuring 10×5 pixels, underscoring its potential in high- pretrained backbone. Our model effectively utilizes both low-
resolutionaerialimagery.Specifically,intheUSGS,California level and high-level features extracted from the backbone
example (Fig. 4c)), S3Former excels in identifying a small and incorporates an instance query mechanism within the
PV module situated at the top-left of the image, a task Transformer architecture to enhance solar PV installation
where the compared methods fail. Fig. 4 a) further reinforces localization. We introduce a self-supervised learning (SSL)
this observation by illustrating the enhancements made by phase(pretexttask)toenhancetheinitializationweightsonthe
S3FormerinaccuratelysegmentingthetargetPVinstallations. backbone of S3Former, resulting in noticeable improvements
Intra-class heterogeneity:Figs.4a),b),c)illustratethediverse in performance. Through rigorous evaluation using diverse
PV installations even within a single rooftop. The variety datasets from GGE (France), IGN (France), and USGS (Cali-
in shapes, colors, and orientations poses challenges for deep fornia, USA) across various GSDs, our extensive experiments
learning.Notably,S3Formeraccuratelymapsvariedcolorcells consistently demonstrate that S3Former either matches or
in Fig. 4 a) and identifies accurate boundary in Fig. 4 b). surpasses state-of-the-art models (SOTA).
Inter-class heterogeneity: Figs. 4 b), c) depict challenges due
to visual similarities between PV installations and other el- REFERENCES
ements. In Fig. 4 b), while both DeepLabV3+ and UperNet
[1] I. M. Peters, C. D. R. Gallegos, S. E. Sofia, and T. Buonassisi, “The
falter due to the rooftop’s similar tone to PV installations, value of efficiency in photovoltaics,” Joule, vol. 3, no. 11, pp. 2732–
S3Former accurately masks it. In Fig. 4 c), DeepLabV3+ 2747,2019.
[2] A. C. Lemay, S. Wagner, and B. P. Rand, “Current status and future
erroneouslymisidentifiesapoolreflectionasasolarcell.Pools
potentialofrooftopsolaradoptionintheunitedstates,”EnergyPolicy,
andwindowsarepronetocreatereflectionsundercertainsolar vol.177,p.113571,2023.
conditions which, due its similar tone and shape with PV [3] “Recordu.s.small-scalesolarcapacitywasaddedin2022,”https://www.
eia.gov/todayinenergy/detail.php?id=603411.
modules, are mistaken by segmentation networks.
[4] M. Pierro, D. Gentili, F. R. Liolli, C. Cornaro, D. Moser, A. Betti,
Overall performance:Fig.4exhibitsS3Former’ssuperiorabil- M.Moschella,E.Collino,D.Ronzio,andD.vanDerMeer,“Progress
ity to accurately depict PV installations, outperforming base- in regional pv power forecasting: A sensitivity analysis on the italian
casestudy,”RenewableEnergy,vol.189,pp.983–996,2022.
line models in clarity and precision. It consistently identifies
[5] H.Shaker,H.Zareipour,andD.Wood,“Adata-drivenapproachforesti-
minute panels throughout the image and consistently offers matingthepowergenerationofinvisiblesolarsites,”IEEETransactions
robust results with minimal background confusion. onSmartGrid,vol.7,no.5,pp.2466–2476,2015.
[6] J.Yu,Z.Wang,A.Majumdar,andR.Rajagopal,“Deepsolar:Amachine
Comparison between S3Former with and without pre-
learningframeworktoefficientlyconstructasolardeploymentdatabase
text task In this section, we conduct a qualitative comparison intheunitedstates,”Joule,vol.2,no.12,pp.2605–2617,2018.
betweenS3Formerwithandwithoutpretexttaskpretrainingto [7] S.J.O’neill,M.Boykoff,S.Niemeyer,andS.A.Day,“Ontheuseof
imageryforclimatechangeengagement,”Globalenvironmentalchange,
validate the enhancements of it. We will revisit the challenges
vol.23,no.2,pp.413–421,2013.
discussed in the previous section, as highlighted in Fig. 5. [8] N. Said, K. Ahmad, M. Riegler, K. Pogorelov, L. Hassan, N. Ahmad,
Small objects: Fig. 5 showcases the improvements of andN.Conci,“Naturaldisastersdetectioninsocialmediaandsatellite
imagery: a survey,” Multimedia Tools and Applications, vol. 78, pp.
S3Former with pretext task pretraining. Fig. 5 a) and c)
31267–31302,2019.
showcasesharperlinesandmoreaccurateboundariesonsolar [9] S. P. B, P. Sheladiya Kaushik, J. Patel, C. R. Patel, and R. M. Tailor,
modulesproducedbyS3Formerw/pretexttask.Thisbehavior “Assessinglandsuitabilityformanagingurbangrowth:Anapplication
ofgisandrs,”inIGARSS2020-2020IEEEInternationalGeoscience
is reaffirmed in Panel b), where S3Former w/ pretext task
andRemoteSensingSymposium,2020,pp.4243–4246.
effectively masks a small indent at the bottom of the solar [10] H.A.EffatandM.N.Hegazy,“Amultidisciplinaryapproachtomapping
installation. potentialurbandevelopmentzonesinsinaipeninsula,egyptusingremote
sensingandgis,”J.ofGeographicInformationSystem,vol.2013,2013.
Inter-class heterogeneity: Figs. 5 b), depicts a case where
[11] J. Wang, Z. Zheng, A. Ma, X. Lu, and Y. Zhong, “Loveda: A remote
CNN models traditionally struggle: an object situated next to sensingland-coverdatasetfordomainadaptivesemanticsegmentation,”
a solar installation with a similar size and color. Both models arXivpreprintarXiv:2110.08733,2021.
[12] G. Kasmi, L. Dubus, P. Blanc, and Y.-M. Saint-Drenan, “Towards
accurately discard this element as part of the solar mask.
unsupervised assessment with open-source data of the accuracy of
However, Figs. 5 a) and c) contain instances of S3Former deeplearning-baseddistributedpvmapping,”inWorkshoponMachine
w/pretexttaskoutperformingwithoutpretrainingwithpretext LearningforEarthObservation(MACLEAN),2022.
[13] P. Parhar, R. Sawasaki, A. Todeschini, H. Vahabi, N. Nusaputra, and
task in identifying erroneous elements and eliminating them
F.Vergara,“Hyperionsolarnet:solarpaneldetectionfromaerialimages,”
from the result mask. arXivpreprintarXiv:2201.02107,2022.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 10
[14] K. Bradbury, R. Saboo, T. L Johnson, J. M. Malof, A. Devarajan, [35] Z.Wu,Y.Xiong,S.X.Yu,andD.Lin,“Unsupervisedfeaturelearning
W. Zhang, L. M Collins, and R. G Newell, “Distributed solar pho- vianon-parametricinstancediscrimination,”2018.
tovoltaic array location and extent dataset for remote sensing object [36] M. Gutmann and A. Hyva¨rinen, “Noise-contrastive estimation: A new
identification,”Scientificdata,vol.3,no.1,pp.1–9,2016. estimation principle for unnormalized statistical models,” in Interna-
[15] G.Kasmi,Y.-M.Saint-Drenan,D.Trebosc,R.Jolivet,J.Leloux,B.Sarr, tionalConferenceonArtificialIntelligenceandStatistics,2010.
andL.Dubus,“Acrowdsourceddatasetofaerialimageswithannotated [37] Y.M.Asano,C.Rupprecht,andA.Vedaldi,“Self-labellingviasimul-
solar photovoltaic arrays and installation metadata,” Scientific Data, taneousclusteringandrepresentationlearning,”2020.
vol.10,no.1,p.59,2023. [38] M.Caron,P.Bojanowski,J.Mairal,andA.Joulin,“Unsupervisedpre-
[16] J.M.Malof,K.Bradbury,L.M.Collins,andR.G.Newell,“Automatic trainingofimagefeaturesonnon-curateddata,”2019.
detectionofsolarphotovoltaicarraysinhighresolutionaerialimagery,” [39] J.-B. Grill, F. Strub, F. Altche´, C. Tallec, P. H. Richemond,
Appliedenergy,vol.183,pp.229–240,2016. E.Buchatskaya,C.Doersch,B.A.Pires,Z.D.Guo,M.G.Azar,B.Piot,
[17] J. M. Malof, L. M. Collins, and K. Bradbury, “A deep convolutional K.Kavukcuoglu,R.Munos,andM.Valko,“Bootstrapyourownlatent:
neuralnetwork,withpre-training,forsolarphotovoltaicarraydetection Anewapproachtoself-supervisedlearning,”2020.
inaerialimagery,”in2017IEEEInternationalGeoscienceandRemote [40] X.ChenandK.He,“Exploringsimplesiameserepresentationlearning,”
SensingSymposium(IGARSS),2017,pp.874–877. preprintarXiv:2011.10566,2020.
[18] L. Zhuang, Z. Zhang, and L. Wang, “The automatic segmentation of [41] S.Gidaris,A.Bursuc,N.Komodakis,P.Pe´rez,andM.Cord,“Learning
residentialsolarpanelsbasedonsatelliteimages:Acrosslearningdriven representationsbypredictingbagsofvisualwords,”2020.
u-netmethod,”AppliedSoftComputing,vol.92,p.106283,2020. [42] J. Zbontar, L. Jing, I. Misra, Y. LeCun, and S. Deny, “Barlow twins:
[19] M. Zech and J. Ranalli, “Predicting pv areas in aerial images with Self-supervised learning via redundancy reduction,” arXiv preprint
deeplearning,”in202047thIEEEPhotovoltaicSpecialistsConference arXiv:2103.03230,2021.
(PVSC),2020,pp.0767–0774. [43] M.Caron,H.Touvron,I.Misra,H.Je´gou,J.Mairal,P.Bojanowski,and
[20] K.Mayer,B.Rausch,M.-L.Arlt,G.Gust,Z.Wang,D.Neumann,and A.Joulin,“Emergingpropertiesinself-supervisedvisiontransformers,”
R.Rajagopal,“3d-pv-locator:Large-scaledetectionofrooftop-mounted inProceedingsoftheIEEE/CVFinternationalconferenceoncomputer
photovoltaicsystemsin3d,”AppliedEnergy,vol.310,p.118469,2022. vision,2021,pp.9650–9660.
[21] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez, [44] A. Tarvainen and H. Valpola, “Mean teachers are better role mod-
L.u.Kaiser,andI.Polosukhin,“Attentionisallyouneed,”inAdvances els:Weight-averagedconsistencytargetsimprovesemi-superviseddeep
in Neural Information Processing Systems, I. Guyon, U. V. Luxburg, learningresults,”preprintarXiv:1703.01780,2017.
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, [45] A.Krizhevsky,I.Sutskever,andG.E.Hinton,“Imagenetclassification
Eds.,vol.30. CurranAssociates,Inc.,2017. withdeepconvolutionalneuralnetworks,”CommunicationsoftheACM,
[22] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, vol.60,no.6,pp.84–90,2017.
T.Unterthiner,M.Dehghani,M.Minderer,G.Heigold,S.Gellyetal., [46] K.SimonyanandA.Zisserman,“Verydeepconvolutionalnetworksfor
“An image is worth 16x16 words: Transformers for image recognition large-scaleimagerecognition,”arXivpreprintarXiv:1409.1556,2014.
atscale,”InternationalConferenceonLearningRepresentations(ICLR), [47] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningforimage
2020. recognition,”inProceedingsoftheIEEEconferenceoncomputervision
[23] A. Gillioz, J. Casas, E. Mugellini, and O. A. Khaled, “Overview of andpatternrecognition,2016,pp.770–778.
the transformer-based models for nlp tasks,” in 15th Conference on [48] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
ComputerScienceandInformationSystems,2020,pp.179–183. Z.Huang,A.Karpathy,A.Khosla,M.Bernsteinetal.,“Imagenetlarge
[24] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and scale visual recognition challenge,” International journal of computer
H. Je´gou, “Training data-efficient image transformers & distillation vision,vol.115,pp.211–252,2015.
through attention,” in International conference on machine learning. [49] A.deLuis,M.Tran,T.Hanyu,A.Tran,L.Haitao,R.McCann,A.Man-
PMLR,2021,pp.10347–10357. tooth, Y. Huang, and N. Le, “Solarformer: Multi-scale transformer for
[25] K. Yamazaki, S. Truong, K. Vo, M. Kidd, C. Rainwater, K. Luu, and solarpvprofiling,”arXivpreprintarXiv:2310.20057,2023.
N. Le, “Vlcap: Vision-language with contrastive learning for coherent [50] S. Becker and G. E. Hinton, “Self-organizing neural network that
videoparagraphcaptioning,”in2022IEEEInternationalConferenceon discovers surfaces in random-dot stereograms,” Nature, vol. 355, no.
ImageProcessing(ICIP). IEEE,2022,pp.3656–3661. 6356,pp.161–163,1992.
[26] K. Yamazaki, K. Vo, Q. S. Truong, B. Raj, and N. Le, “Vltint: [51] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a
visual-linguistictransformer-in-transformerforcoherentvideoparagraph neuralnetwork,”inNIPSDeepLearningandRepresentationLearning
captioning,”inAAAI,vol.37,no.3,2023,pp.3081–3090. Workshop,2015.[Online].Available:http://arxiv.org/abs/1503.02531
[27] K. Vo, K. Yamazaki, P. X. Nguyen, P. Nguyen, K. Luu, and N. Le, [52] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable detr:
“Contextualexplainablevideorepresentation:Humanperception-based Deformabletransformersforend-to-endobjectdetection,”arXivpreprint
understanding,”in202256thAsilomarConferenceonSignals,Systems, arXiv:2010.04159,2020.
andComputers. IEEE,2022,pp.1326–1333. [53] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
[28] K.Vo,S.Truong,K.Yamazaki,B.Raj,M.-T.Tran,andN.Le,“Aoe- forsemanticsegmentation,”inProceedingsoftheIEEEconferenceon
net: Entities interactions modeling with adaptive attention mechanism computervisionandpatternrecognition,2015,pp.3431–3440.
for temporal action proposals generation,” International Journal of [54] O.Ronneberger,P.Fischer,andT.Brox,“U-net:Convolutionalnetworks
ComputerVision,vol.131,no.1,pp.302–323,2023. for biomedical image segmentation,” in International Conference on
[29] P. Sun, R. Zhang, Y. Jiang, T. Kong, C. Xu, W. Zhan, M. Tomizuka, Medicalimagecomputingandcomputer-assistedintervention. Springer,
L. Li, Z. Yuan, C. Wang et al., “Sparse r-cnn: End-to-end object 2015,pp.234–241.
detection with learnable proposals,” in Proceedings of the IEEE/CVF [55] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing
conference on computer vision and pattern recognition, 2021, pp. network,” in Proceedings of the IEEE conference on computer vision
14454–14463. andpatternrecognition,2017,pp.2881–2890.
[30] M. Tran, K. Vo, K. Yamazaki, A. Fernandes, M. Kidd, and N. Le, [56] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun, “Unified perceptual
“Aisformer: Amodal instance segmentation with transformer,” BMVC, parsing for scene understanding,” in Proceedings of the European
2022. conferenceoncomputervision(ECCV),2018,pp.418–434.
[31] T.-P. Nguyen, T.-T. Pham, T. Nguyen, H. Le, D. Nguyen, H. Lam, [57] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar,
P. Nguyen, J. Fowler, M.-T. Tran, and N. Le, “Embryosformer: De- “Masked-attentionmasktransformerforuniversalimagesegmentation,”
formabletransformerandcollaborativeencoding-decodingforembryos in Proceedings of the IEEE/CVF Conference on Computer Vision and
stagedevelopmentclassification,”inTheIEEE/CVFWinterConference PatternRecognition,2022,pp.1290–1299.
onApplicationsofComputerVision,2023,pp.1981–1990. [58] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking
[32] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple atrous convolution for semantic image segmentation,” arXiv preprint
framework for contrastive learning of visual representations,” preprint arXiv:1706.05587,2017.
arXiv:2002.05709,2020.
[33] A. Dosovitskiy, P. Fischer, J. T. Springenberg, M. Riedmiller, and
T. Brox, “Discriminative unsupervised feature learning with exemplar
convolutionalneuralnetworks,”TPAMI,2016.
[34] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast
forunsupervisedvisualrepresentationlearning,”2020.