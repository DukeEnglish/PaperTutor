TorchDriveEnv: A Reinforcement Learning Benchmark for Autonomous
Driving with Reactive, Realistic, and Diverse Non-Playable Characters
Jonathan Wilder Lavington1,2,∗ Ke Zhang1,2,∗ Vasileios Lioutas1,2 Matthew Niedoba1,2
Yunpeng Liu1,2 Dylan Green1,2 Saeid Naderiparizi1,2 Xiaoxuan Liang1,2 Setareh Dabiri1
Adam S´cibior1,2 Berend Zwartsenberg1 Frank Wood1,2,3
Abstract—The training, testing, and deployment, of au-
tonomous vehicles requires realistic and efficient simulators.
Moreover, because of the high variability between different
problems presented in different autonomous systems, these
simulators need to be easy to use, and easy to modify. To
address these problems we introduce TorchDriveSim and its (a) Empty Intersection
benchmark extension TorchDriveEnv. TorchDriveEnv is
a lightweight reinforcement learning benchmark programmed
entirely in Python, which can be modified to test a number
of different factors in learned vehicle behaviour, including the
effect of varying kinematic models, agent types, and traffic
control patterns. Most importantly unlike many replay based
(b) Crowded Highway
simulation approaches, TorchDriveEnv is fully integrated
with a state of the art behavioural simulation API. This allows
users to train and evaluate driving models alongside data
driven Non-Playable Characters (NPC) whose initializations
and driving behaviour are reactive, realistic, and diverse. We
illustrate the efficiency and simplicity of TorchDriveEnv by
evaluating common reinforcement learning baselines in both (c) Crowded Left Turn
training and validation environments. Our experiments show
that TorchDriveEnv is easy to use, but difficult to solve.
I. INTRODUCTION
In the last decade, autonomous vehicles have gone from (d) Controlled Intersection
academic obscurity to one of the biggest industries in the
technology sector. This is in no small part due to the
rapid evolution of deep learning [2], and improvements to
autonomous vehicle (AV) simulation [3]. In fact in almost all
modernsystemsfortrainingortestingofautonomousvehicles,
(e) Roundabout
simulation plays a crucial role [4]. This is because both
trainingandtestingintherealworldisnotonlyexpensiveand Fig. 1: Frames from five TorchDriveEnv stochastic
time-consuming, but represents a legitimate danger to public episodes encountered during SAC [1] RL agent training. In
safety. Therefore academic and industry researchers have all examples, the ego vehicle is red, the next waypoint is
placedprogressivelygrowingeffortintoproducingsimulators a green circle, and non-playable character (NPC) vehicles
which are more realistic and efficient to use. In many cases, are blue. Drivable surfaces are grey, while traffic lights are
these simulators have allowed both academic researchers as denoted by thin coloured rectangles at stop-lines that are red,
well as industry professionals to solve a number of difficult yellow, or green. Lanes are indicated by purple, dark green
problems in autonomous control [5], and even computer and white lines for right, left, or overlapping lane boundaries.
vision [6]. However, even many modern simulators ignore In each example, five uniformly spaced frames were taken
the current technical requirements for both the training and from a single episode and are displayed sequentially left to
testing of complex controllers, which may need to satisfy a right. These examples illustrate the diverse traffic and road
rangeofdifferentengineeringrequirements.Morespecifically, conditions encountered during training. Note the high density
existing AV simulators often lack realistic traffic behaviour and realistic behaviour displayed by the NPCs, particularly
and are not easy to modify as requirements for training or in Figures 1b to 1e.
testing change. Additionally, many of these simulators have
ignored advances in computation graphs and deep-learning
that make learning better policies easier [7].
1InvertedAI,2UniversityofBritishColumbia,3MILA
∗Denotesequalcontribution ToaddresstheseissueswefirstpresentTorchDriveSim,
4202
yaM
7
]IA.sc[
1v19440.5042:viXra(a) Parked-Car (b) Three-Way (c) Chicken (d) Roundabout (e) Traffic-Lights
Fig. 2: Stochastic initializations of validation scenarios used to test learned agents’ out of distribution performance. Each
scenario tests a different set of agent capabilities. The first scenario (a) tests an agent’s ability to drive around a parked
vehicle obstructing the road. The three-way intersection (b) tests an agent’s ability to navigate a three way intersection and
yield as needed to cross traffic. The example in (c) requires the agent to negotiate around a collision with an oncoming
passing car in its lane. The roundabout in (d) requires the agent to merge, yield, change lanes, and finally, exit the roundabout.
The last example in (e) tests if the agent can navigate through controlled intersections in the presence of NPCs that obey
traffic lights realistically.
a lightweight 2D driving simulator built entirely in Py- initial conditions or waypoint information, that can then be
Torch. TorchDriveSim is differentiable and produces a integrated with additional realistic, reactive, and diverse non-
computation graph which includes state transition functions playable characters (NPCs) to create complex AV scenarios
based upon kinematic models and observation information which test specific controller behaviors.
produced via differentiable rendering. Additionally, the kine- In the following sections, we will illustrate how
matic models which define the underlying transitions of TorchDriveSimfunctions,howitcanbeusedormodified,
TorchDriveSim can be easily modified or customized, and some of the advantages it offers over comparable simu-
as can the rendered observations. Furthermore, metrics like lators. We then introduce the TorchDriveEnv benchmark,
collisions, off-road, and wrong way infractions are also fully and provide a set of standard RL baselines where we report
differentiable.Foreaseofuseinmodeltrainingandevaluation, both training and validation performance. We conclude with
TorchDriveSim provides first class support for batch a discussion of future research directions.
processing,andisdefinedbyamodularcollectionofwrappers
which can change simulator behaviour. TorchDriveSim II. RELATEDWORK
also allows for more diverse traffic environments through the
Recent years have seen a major surge in autonomous
useofheterogeneousagenttypessuchasvehicles,pedestrians,
vehicle simulation, and this increased interest has led to
or cyclists which each have their own kinematic model.
progressively more realistic simulation environments [3]. In
Access to control information such as traffic lights is also
many cases these simulators were originally constructed to
included in order to further improve realism.
reduce the so called simulation to reality gap [10], [11],
Next we introduce the TorchDriveEnv benchmark which is characterized by how well controllers learned in
suite, a training and testing environment for developing simulation, function in a real world setting. Eventually, many
autonomous driving algorithms. Unlike many existing sim- of these simulators evolved into, or were adapted to machine
ulation environments, TorchDriveEnv provides access learning and control benchmarks in order to test a number of
to realistic external agents via integration with a state of different policy learning problems [12]. Some simulators and
the art commercial (free and low cost academic licensing benchmarks test performance in multi agent environments
available) API, and provides a differentiable renderer through whereexternal(non-ego)agentsareexecutedusingeitherrule
TorchDriveSim. Additionally, TorchDriveEnv uses based setups [13] or by replaying historical data [14]. Many
standard reinforcement learning (RL) environment construc- of these multi-agent autonomous vehicle benchmarks have
tion through the use of step and reset functions following the alsobeentailoredforimagebaseddeeplearningsystems,and
Open-AI gym environment standard [8], making integration can test a variety of algorithm classes [15]. In some cases,
with RL libraries (e.g. stable baselines [9]) quick and easy. these benchmarks also include egocentric driving log data
In contrast to most RL benchmark suites which train and test which can be used for learning via behavioural cloning [16].
in the same environment, TorchDriveEnv also includes In other cases only local (non-expert) information is
both validation and training environments in order to allow included, like the state-action reward [17] or an infraction
users to verify the generalization of their learned controllers. indicator which shows whether the ego vehicle experienced a
Lastly, TorchDriveEnv comes with access to its own collision,wentoffroad,orisdrivingthewrongdirectionalong
environment labelling tool with which users can define their a roadway. The algorithms which can be tested in this setting
own unique AV scenarios to train and test models. This generally include some version of classical reinforcement
labelling tool allows the user to define information like static learning [18], or decision transformers [19], [20]. In someAccel. SensorSim ExpertData Sim-agents RealData Routes/Goals NPCtype License
SUMMIT[31] ✓ ✓ ✓ - Rulebased MIT
MACAD[32] ✓ ✓ Goalpoint JointlyRLtrained MIT
DeepDrive-Zero[33] ✓ - JointlyRLtrained MIT
SMARTS[34] ✓ Waypoints Rulebased MIT
MADRaS[35] ✓ ✓ Goalpoint Rulebased GNUAffero
Nocturne[14] ✓ ✓ ✓ Goalpoint Replay MIT
MetaDrive[36] ✓ ✓ ✓ ✓ - Rulebased+RL Apache
Intersim[37] ✓ ✓ ✓ Goalpoint Datadriven(deterministic) MIT∗
TorchDriveSim(Ours) ✓ ✓ Waypoints Datadriven(diverse) Apache
tbsim[38] ✓ ✓ ✓ Goalpoint Datadriven(diverse) Nvidia
Waymax[39] ✓ ✓ ✓ ✓ Waypoints Replay+IDM Waymax
TABLE I: Reproduced in modified form with permission from the authors from [39]. Comparison of driving simulators in
chronological order, with each column indicating the features available. Accel.: In-graph compilation for hardware (GPU/TPU)
acceleration. Sensor Sim: Sensors (e.g. camera, lidar & radar) input simulation. Expert Data: Human demonstrations or
rollout trajectories collected with an expert policy. Sim-agents: agent models for simulated objects (e.g. other vehicles). Real
data: Real world driving data. Routes/Goals: "−" means no routes or goals are provided; "Waypoints" means positions
sampled from a trajectory; "Directions" means discrete driving directions including left, straight, and right; "Goal point"
means the goal position. NPC type: Reports the type of NPCs provided with the simulator. “Data driven” refers to modelled
NPCs learned from data. License: License type. MIT∗ indicates the simulator is MIT licensed, but the provided models are
trained on Waymo licensed data. Unlike previous work we only include AV simulators which provide multi-agent simulated
environments, for a more complete list see [39].
cases,whenexplicitwaypointinformationisavailablewithout the TorchDriveEnv environment:
directexpertsupervision(e.g.theactionstakenforeachstate),
# env = gym.make(’tde/v0/validation’)
itiscommontouseeitherinversereinforcementlearning[21],
env = gym.make(’tde/v0/train’)
or to combine some form of closed loop control with a observation, info = env.reset(seed=42)
computer vision system [22]. Such control systems also for _ in range(1000):
action = env.action_space.sample()
extend to trajectory prediction [23], and given infraction obs, reward, terminated, truncated, info = \
information can be easily adapted to problems related to env.step(action)
if terminated or truncated:
learning safe autonomous vehicle controllers [24].
observation, info = env.reset()
Structurally such benchmarks follow the example of env.close()
[8], and include standardized step, reset, and initialization
functions. This structure allows for ease and accessibility, A. Simulator
and has been adopted by the majority of more modern RL The TorchDriveSim package can broken down into a
benchmarks [25]. Unfortunately, due to the fundamental dif- number of different classes and helper functions, but the
ficulty surrounding RL (e.g. non-stationary, policy dependent most important is the simulator class. This class defines a
data distributions [26]), the majority of benchmarks test simulator object created using a static background, which
generalization. One example which does, is MetaWorld [27], by default includes both road and lane markings represented
a multi-agent robot manipulation environment. While Meta- by a rendered triangle mesh. This simulator also includes
World allows the user to split learning into training and control elements (e.g. traffic lights, yield signs) represented
validation scenarios that test how well the learned policy by rectangles that each have a local internal state whose
generalizes to new objects and tasks, it is primarily tailored semantics are not explicitly enforced. Each simulator object
to Meta-RL [28] and few-shot learning [29]. Another popular includes a collection of agents grouped by their type (car,
example is ProcGen [30], a procedurally generated Atari- pedestrian, or bicycle) which are represented with rigid
like gaming environment which generates levels which the rectangles. Each agent has a local kinematic model which
agent must solve using random procedural generation. Both defines its action space, and how actions from that agent
of these benchmarks present excellent test-beds for RL will translate into motion. Lastly, each simulator contains
environments, however neither includes both data-driven a configurable render object, which can display the world
scenario generation and the train-validation scenario splits from a birds eye view using a customizable colour map.
provided by TorchDriveEnv. For other examples of comparable AV simulators related to
TorchDriveSim see Table I.
III. DESIGNANDFEATURES
B. Agents
In the following section we will describe the basic fea-
tures available through TorchDriveSim, then discuss the Each agent within the simulator is defined by a state. This
benchmark defined by TorchDriveEnv. Before getting stateisdescribedbysetofstaticattributes(length,width,and
into the specific details however, we briefly include a small anything else necessary to its kinematic models), a dynamic
code snippet which can be used to generate samples from state (x, y, orientation, speed), and a flag (present mask)also included. Unless otherwise specified, all wrappers are
composable, and thus can be combined to achieve a wide
range of user requirements.
The default kinematic model used by all non-egocentric
vehicles in TorchDriveSim is the bicycle model [40],
in which actions are defined by steering and acceleration.
Other kinematic models are also available, including the
unconstrained model where each action is defined by the
change in state at subsequent time steps. We also include
a teleporting variant where each action directly sets the
next state. Tools and documentation are provided which
(a) Scene 1, Initialization 1 (b) Scene 1, Initialization 2
illustrate how to modify these action models, or create
completely custom, potentially more complex action models.
The teleporting model for instance, is defined simply through
inheritance by the following class:
class TeleportingModel(KinematicModel):
def step(self, action, dt=None):
self.set_state(action)
def fit_action(self, future_state, \
current_state=None, dt=None):
return future_state
In this example, the state is set directly through the action
passed to the kinematic models step function. While this
(c) Scene 2, Initialization 1 (d) Scene 2, Initialization 2
kinematic model is simple, extending this framework more
Fig.3:Twostochasticinitializationsdrawnfromtwoseparate complex kinematic models follows in a similar fashion.
training scenarios. The first scene (Figure 3a and Figure 3b)
includes the initialization over the entire map, while the sec- C. Generative Models of Vehicle Behaviour
ond(Figure3candFigure3d)providesasimilarvisualization Provided with a known kinematic model, avoiding basic
of a specific intersection in a separate scenario. While certain infractions like off-road or wrong-way behaviour can often
distributional characteristics are similar (e.g. cars are stopped be accomplished through methods in classic control [41].
at a stop-lights), positions and even initial velocities of these The more difficult scenarios which most practitioners and
vehicles differ between initializations. In all scenes, green researchersareinterestedinhowever,arethosewhichcontain
dotsindicatethesequenceofwaypointsprovidedtotheagent, non-static state information like changing traffic lights, or
the red rectangle indicates the ego vehicle, blue rectangles include external agents (NPCs). While environment attributes
indicate NPCs, and an orange arrow indicates the direction liketrafficlightscanbecontrolledrealisticallythroughsimple
of the vehicles. Traffic lights are provided by coloured bars, state machines, inclusion of NPCs within an environment
which are red, yellow, or green. Notice that all NPCs in requires realistic, reactive, and diverse models of driving
each of the initializations are going the correct direction and behaviour [42]. Through integration with with an API that
conform to traffic light state. serves agent initializations and behaviour predictions, we
extendTorchDriveSimandintroduceTorchDriveEnv.
which indicates whether a given agent is currently active Through TorchDriveEnv, complex behavioural models
within the simulation. At each time step, all agents perform can be directly queried to provide a service that initializes
an action simultaneously which alongside the current agent and controls NPCs. Through this integration, expensive AV
state, determines their next state. TorchDriveSim allows modelsand(theGPUsrequiredtorunthem)canbeabstracted
the agents to overlap, and defines this overlap as a collision behind a simple API layer, thereby reducing the need for
event. Similarly TorchDriveSim allows for overlap (or increased local computing resources.
lackthereof)withanyotherboundingboxesorlanemarkings The two primary API endpoints which TorchDriveEnv
inside the mesh and identifies these events as other types of uses are INITIALIZE and DRIVE. As is illustrated in
infractions (such as wrong-way or off-road). Figure3,INITIALIZEiscalledduringsimulatorinitialization
TorchDriveSim and its extension TorchDriveEnv to populate the scenario with a randomized but realistic
also include wrappers which can modify the simulator’s collection of NPCs, and is based upon recent work in
behaviour. Such wrappers can define which subset of agents conditional diffusion generative modeling [43], [44]. Notably,
will be controlled through replay or predefined controllers, the user can also choose to manually set the initializations
or may remove agents that exit a designated area on the map. of all, or a subset of NPCs, through modifications of the
Wrappers for monitoring and visualization, which allow the environment’s associated definition. During initialization, any
user to log more common infractions like-off road or record user-defined agents are placed in the scene first, after which,
videos of the scenes generated through agent interaction are additionalagentsaresampledconditionallyusinglocationsofthemanuallyplacedagents.Conditionallysampledagentscan of the egocentric birds-eye view of the scene (see Figure 1),
have their agent type specified, but are otherwise set as basic i.e. the ego agent is always facing the same direction and the
vehicles. We note that this conditional (or unconditional) maptranslatesandrotates.Foreachbenchmark,thisrendered
initialization of external agents is traffic-light state aware. observation is provided using a tensor of size 3×3×64×64,
After all agents have been initialized, DRIVE is then called wherethefirstindexoftensorindicateshistoricalframesfrom
during the environments step function, and provides driving the previous two time-steps plus the current observation.
behaviors for each of the initialized NPCs present within the 3) End-Conditions: Like most popular RL environments
scene using recent advances in models of human realistic TorchDriveEnv defines a set of conditions which if met,
vehicle behaviors [42]. will halt simulation. These end conditions include: if the
ego agent goes off-road, if there is a traffic light violation
D. Environment
(the ego agent passes over a traffic-light bounding box), and
As described above, the TorchDriveEnv package bun- finally if the ego agent collides with another agent within the
dles TorchDriveSim with API based behavioural models scene. We also include a maximum timesteps end condition
for initialization and reactive NPCs. Using this environment to ensure metrics and evaluation can be computed efficiently.
as its basis, TorchDriveEnv then defines scenarios with 4) Reward: In this setting, because we implicitly limit
which various RL and AV algorithms can be trained or the maximum cumulative return through the end conditions
tested. For this initial TorchDriveEnv benchmark release definedabove,weincludeonlysimplefeedbackviathereward
all maps come from the CARLA simulator [3]. Each of which penalizes based upon a notion of smoothness, and
these training and testing environments are set up such that rewards based upon how accurately the agent navigates be-
the user controls only the actions of the ego agent within a tweenwaypoints.WithinTorchDriveEnv,thesewaypoints
given scenario, while DRIVE controls the remaining agents. define the general path which the agent should take, and are
Similarly, while additional NPCs are generated through INI- denoted by a small circle rendered on top of the rendered
TIALIZE, the ego agent is randomly initialized locally along birdview observations. In order to reduce the need for strong
a scenario-specific sequence of ego-initialization waypoints. exploration,wealsoincludeasmallbonusforwhentheagent
TorchDriveEnv is also augmented with a reward based moves some minimum distance between different timesteps.
on infraction metrics and waypoints, comes with pre-defined Within the benchmark suite, the reward is defined using
training and validation scenarios, and has the option to use two bonuses (strictly non-negative values), and a penalty (a
external data and a scenario editor to create hand tailored strictly non-positive value). This gives a simple reward r
new scenarios for whatever the user may wish to test. The defined using three values,
core TorchDriveEnv environment is based on OpenAI
r=α |Movement Bonus|+α |Waypoint Bonus|
Gym [8], and thus can be easily modified through wrappers 1 2
−β |Smoothness Penalty|,
whichfollowstandardfunction.Forexample,onecouldeasily 1
modify the state observations or reward by simply creating where for the benchmark results shown in Figure 4: α =1,
1
their own environment wrapper: α =10,andβ =0.05.CSVfilescontainingthedatausedto
2 1
generate the plots can be found on the accompanying github
class MyTorchDriveEnv(gym.Wrapper):
def __init__(self, env: gym.Env): repository. We also note that a movement bonus is only
super().__init__(env) includediftheagenttravelsatleast0.5meterswithinagiven
def reset(self, **kwargs):
time-step. Similarly, the waypoint bonus is only included
obs, _ = super().reset(**kwargs)
return self.transform_out(obs), _ if the agent transitions to a position which is within two
def step(self, action: Tensor):
meters of the next waypoint in the queue. At which point
obs, reward, terminated, trunc, info \
= super().step(action) the waypoint is popped from the queue and the next one is
return obs, reward, terminated, trunc, info placedintheenvironmentasanewgreendot.Thesmoothness
def render(self, *args, **kwargs):
return self.env.render(*args, **kwargs) penalty is defined by the cosine similarity between headings
def close(self): at the current time-step and the next time-step.
self.env.close()
IV. BENCHMARK
For the set of standard training and validation benchmarks, In order to illustrate the use of TorchDriveEnv as
we break down each of the attributes which define the a reinforcement learning (RL) benchmark, we have fully
reinforcement learning problem below. integratedthestable-baselines[9]libraryinorderevaluatethe
1) Action Space: The action space defines the set of performance a number of different policy learning algorithms.
admissible actions which can be taken by a controller. These algorithms include A2C [45], PPO [46], SAC [1],
Within TorchDriveEnv these actions are continuous, and and TD3 [47]. We report a number of different metrics
defined by a steering angle b, and acceleration c such that availabletotheuserthroughtheTorchDriveEnvinterface
b∈(−0.3,0.3) and c∈(−1.0,1.0). Constraints for b are alongside the base return (cumulative reward), including
chosen to ensure vehicle behaviour remains visually realistic. infraction information, the average time-horizon observed
2) Observation Space: The observation space within the by the agent, and the average number of waypoints achieved.
TorchDriveEnv environment is given by a 2d rendering Figure4arepresentsatrainingenvironmentwheretherearenoSAC PPO TD3 A2C Multi-agent Single-agent
1.0 0.8 10.0
0.75 0.6 7.5
0.5 0.4 5.0
0.25 0.2 2.5
0.0 0.0 0.0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
1.0
0.75 200
0.5 100
100
0.25 50
0.0 0 0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Time-steps (1e6) Time-steps (1e6) Time-steps (1e6)
(a) Multi-Agent-Train
1.0 0.8 5.0
0.75 0.6
0.5 0.4 2.5
0.25 0.2
0.0 0.0 0.0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
1.0 200
0.75
100
0.5 100
0.25 50
0.0 0 0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Time-steps (1e6) Time-steps (1e6) Time-steps (1e6)
(b) Multi-Agent-Validation
1.0 0.8
10.0 0.75 0.6 7.5
0.5 0.4 5.0
0.25 0.2 2.5
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
1.0 300
0.75 200 150
0.5 100
0.25 100 50
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Time-steps (1e6) Time-steps (1e6) Time-steps (1e6)
(c) Single-Agent-Train
1.0 0.8 10.0
0.75 0.6 7.5
0.5 0.4 5.0
0.25 0.2 2.5
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
1.0 300
150 0.75 200
0.5 100
0.25 100 50
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Time-steps (1e6) Time-steps (1e6) Time-steps (1e6)
(d) Single-Agent-Validation
Fig. 4: The TorchDriveEnv benchmark: training curves for multi-agent and single-agent TorchDriveEnv environments
acrosseachoftheirrespectivetrainingandvalidationscenarios.Collisionindicatesthepercentageoftimeinwhichanepisode
ends in a collision, Traffic-Light indicates the percentage of times an episode ends in a traffic light violation, Waypoint #
indicates the average number of waypoints achieved during an episode, Offroad indicates the percentage of episodes which
end in an off road infraction, Return indicates the average cumulative reward per episode, and finally Horizon indicates the
average length of episodes observed. We include two dotted and dashed lines which indicate the average performance of
across 4 randomly seeded RL agents evaluated on 10 episodes each. The dotted line indicates performance of agents trained
in a multi-agent environment, while the dashed line indicates performance of agents trained in an ego-only environment.
The plots above illustrate two important things. The first is that learning agents within a multi-agent environment is more
difficult then in a single agent environment, as illustrated by significantly lower reward and time-horizon metrics. Second the
performance gap between multi-agent trained models in a single agent environment is generally much lower (see the horizon
and return plots) then single-agent trained models being evaluated in a multi-agent setting. Across all benchmarks, vehicles
trained in a multi-agent environment survive for longer (larger horizon) than ego only trained environments.
noisilloC
daorffO
noisilloC
daorffO
noisilloC
daorffO
noisilloC
daorffO
thgiL
ciffarT
nruteR
thgiL
ciffarT
nruteR
thgiL
ciffarT
nruteR
thgiL
ciffarT
nruteR
#
tniopyaW
noziroH
#
tniopyaW
noziroH
#
tniopyaW
noziroH
#
tniopyaW
noziroHexternalagentsduringtraining,whileFigure4crepresentsthe [3] AlexeyDosovitskiy,GermanRos,FelipeCodevilla,AntonioLopez,
training environment which contains reactive external agents. andVladlenKoltun. CARLA:Anopenurbandrivingsimulator. In
Proceedingsofthe1stAnnualConferenceonRobotLearning,pages
As expected the inclusion of additional agents dramatically
1–16,2017.
increases the difficulty of the environment. Unlike many [4] Hans-Peter Schöner. Simulation in development and testing of
standard RL benchmarks, we also include a validation autonomousvehicles. In18.InternationalesStuttgarterSymposium:
Automobil-undMotorentechnik,pages1083–1095.Springer,2018.
environment for both the single (Figure 4b) and multi-
[5] MMDetection3DContributors. MMDetection3D:OpenMMLabnext-
agent (Figure 4d) settings. Notably, the two most performant
generationplatformforgeneral3Dobjectdetection,2020.
algorithms are SAC and PPO across all benchmarks. To [6] AdrienGaidon,QiaoWang,YohannCabon,andEleonoraVig. Virtual
illustrate the importance of including multi-agent simulation worlds as proxy for multi-object tracking analysis. In Proceedings
oftheIEEEconferenceoncomputervisionandpatternrecognition,
inAVtraining,wealsoincludedottedanddashedlineswhich
pages4340–4349,2016.
define the average performance across models trained using [7] J. Krishna Murthy, Miles Macklin, Florian Golemo, Vikram Voleti,
the best performing (in terms of reward) RL algorithm within Linda Petrini, Martin Weiss, Breandan Considine, Jérôme Parent-
Lévesque, Kevin Xie, Kenny Erleben, Liam Paull, Florian Shkurti,
singleagentandmulti-agentenvironmentsacrossbothtraining
Derek Nowrouzezahrai, and Sanja Fidler. gradsim: Differentiable
and testing environments. As expected, agents trained within simulation for system identification and visuomotor control. In
the multi-agent environment tend to function reasonably well InternationalConferenceonLearningRepresentations,2021.
[8] GregBrockman,VickiCheung,LudwigPettersson,JonasSchneider,
in both single agent and multi-agent environments, while
John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym.
agents trained using only single agent simulation do not arXiv:1606.01540[cs],June2016.
generalize to the multi-agent setting. [9] AntoninRaffin,AshleyHill,AdamGleave,AnssiKanervisto,Max-
imilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable
V. DISCUSSION reinforcementlearningimplementations. JournalofMachineLearning
Research,22(268):1–8,2021.
In this paper we introduce TorchDriveSim and its [10] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech
benchmark library TorchDriveEnv. We illustrate ease of Zaremba,andPieterAbbeel. Domainrandomizationfortransferring
deep neural networks from simulation to the real world. In 2017
use through both visual and program examples, and provide
IEEE/RSJinternationalconferenceonintelligentrobotsandsystems
results on the TorchDriveEnv benchmark using [9]. This (IROS),pages23–30.IEEE,2017.
benchmark confirms the importance of reactive and realistic [11] Błaz˙ejOsin´ski,AdamJakubowski,PawełZiecina,PiotrMiłos´,Christo-
pherGalias,SilviuHomoceanu,andHenrykMichalewski. Simulation-
NPCs in the training of AV controllers by considering the
basedreinforcementlearningforreal-worldautonomousdriving. In
effectsofoutofdistributionerrorthroughavalidationdataset, 2020 IEEE International Conference on Robotics and Automation
as well as through the comparison of agents trained using (ICRA),pages6411–6418.IEEE,2020.
only single agent interactions. We hope that through the [12] WeiZhan,LitingSun,DiWang,HaojieShi,AubreyClausse,Maximil-
ianNaumann,JuliusKummerle,HendrikKonigshof,ChristophStiller,
introductionofthisbenchmark,researcherswillworktowards
Arnaud de La Fortelle, et al. Interaction dataset: An international,
developingcontrollersthataremorerobustchangesindistribu- adversarial and cooperative motion dataset in interactive driving
tion. Additionally, the results using standard RL baselines on scenarioswithsemanticmaps. arXivpreprintarXiv:1910.03088,2019.
[13] Wei Xiao, Noushin Mehdipour, Anne Collin, Amitai Y Bin-Nun,
thisbenchmarkillustratetheneedforamoreprecisetreatment
EmilioFrazzoli,RadboudDuintjerTebbens,andCalinBelta. Rule-
ofperformancethanisprovidedbytheaveragereturn.Thisis basedoptimalcontrolforautonomousdriving. InProceedingsofthe
because, regardless of how high the reward in examples from ACM/IEEE12thInternationalConferenceonCyber-PhysicalSystems,
pages143–154,2021.
TorchDriveEnv became for standard baselines, even the
[14] EugeneVinitsky,NathanLichtlé,XiaomengYang,BrandonAmos,and
highestperformingalgorithmsmaintainednon-trivialcollision JakobFoerster. Nocturne:ascalabledrivingbenchmarkforbringing
and off-road infractions. Actively designing algorithms that multi-agentlearningonestepclosertotherealworld. arXivpreprint
arXiv:2206.09889,2022.
target both reward maximization while driving infractions
[15] Khan Muhammad, Amin Ullah, Jaime Lloret, Javier Del Ser, and
to zero represents an interesting area of future research.
VictorHugoCdeAlbuquerque. Deeplearningforsafeautonomous
In the future, we also wish to expand the baselines we driving:Currentchallengesandfuturedirections. IEEETransactions
consider to include: expert trained imitation learning models,
onIntelligentTransportationSystems,22(7):4316–4336,2020.
[16] MayankBansal,AlexKrizhevsky,andAbhijitOgale. Chauffeurnet:
RL algorithms with pretrained feature encoders, and finally
Learningtodrivebyimitatingthebestandsynthesizingtheworst. In
policy learning algorithms which can take advantage of Proceedings of Robotics: Science and Systems, FreiburgimBreisgau,
TorchDriveSim’s differentiable dynamics. Germany,June2019.
[17] RichardS.SuttonandAndrewG.Barto. ReinforcementLearning:An
Introduction. TheMITPress,secondedition,2018.
REFERENCES
[18] YirenLu,JustinFu,GeorgeTucker,XinleiPan,EliBronstein,Rebecca
[1] TuomasHaarnoja,AurickZhou,KristianHartikainen,GeorgeTucker, Roelofs,BenjaminSapp,BrandynWhite,AleksandraFaust,Shimon
Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Whiteson,etal. Imitationisnotenough:Robustifyingimitationwith
Pieter Abbeel, et al. Soft actor-critic algorithms and applications. reinforcement learning for challenging driving scenarios. In 2023
arXivpreprintarXiv:1812.05905,2018. IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems
[2] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBrad- (IROS),pages7553–7560.IEEE,2023.
bury,GregoryChanan,TrevorKilleen,ZemingLin,NataliaGimelshein, [19] Wen Fu, Yanjie Li, Zhaohui Ye, and Qi Liu. Decision making for
LucaAntiga,AlbanDesmaison,AndreasKopf,EdwardYang,Zachary autonomousdrivingviamultimodaltransformeranddeepreinforcement
DeVito,MartinRaison,AlykhanTejani,SasankChilamkurthy,Benoit learning. In 2022 IEEE International Conference on Real-time
Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An ComputingandRobotics(RCAR),pages481–486.IEEE,2022.
imperativestyle,high-performancedeeplearninglibrary.InH.Wallach, [20] JiqianDong,SikaiChen,MohammadMiralinaghi,TiantianChen,and
H.Larochelle,A.Beygelzimer,F.d'Alché-Buc,E.Fox,andR.Garnett, SamuelLabi. Developmentandtestingofanimagetransformerfor
editors,AdvancesinNeuralInformationProcessingSystems,volume32. explainableautonomousdrivingsystems. JournalofIntelligentand
CurranAssociates,Inc.,2019. ConnectedVehicles,5(3):235–249,2022.[21] MaximilianIgl,DaewooKim,AlexKuefler,PaulMougin,PunitShah, Conference on Robotics and Automation (ICRA), pages 2929–2936.
KyriacosShiarlis,DragomirAnguelov,MarkPalatucci,BrandynWhite, IEEE,2023.
andShimonWhiteson.Symphony:Learningrealisticanddiverseagents [39] Cole Gulino, Justin Fu, Wenjie Luo, George Tucker, Eli Bronstein,
forautonomousdrivingsimulation. In2022InternationalConference YirenLu,JeanHarb,XinleiPan,YanWang,XiangyuChen,JohnD.
on Robotics and Automation (ICRA), page 2445–2451. IEEE Press, Co-Reyes,RishabhAgarwal,RebeccaRoelofs,YaoLu,NicoMontali,
2022. PaulMougin,ZoeyYang,BrandynWhite,AleksandraFaust,Rowan
[22] Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp Krähenbühl. McAllister,DragomirAnguelov,andBenjaminSapp. Waymax:An
Learningbycheating. InConferenceonRobotLearning,pages66–75. accelerated,data-drivensimulatorforlarge-scaleautonomousdriving
PMLR,2020. research. In Alice Oh, Tristan Naumann, Amir Globerson, Kate
[23] Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, Kratarth Goel, Saenko,MoritzHardt,andSergeyLevine,editors,AdvancesinNeural
KhaledSRefaat,andBenjaminSapp. Wayformer:Motionforecasting Information Processing Systems 36: Annual Conference on Neural
viasimple&efficientattentionnetworks. In2023IEEEInternational InformationProcessingSystems2023,NeurIPS2023,NewOrleans,
Conference on Robotics and Automation (ICRA), pages 2980–2987. LA,USA,December10-16,2023,2023.
IEEE,2023. [40] PhilipPolack,FlorentAltché,Brigitted’AndréaNovel,andArnaud
[24] Matt Vitelli, Yan Chang, Yawei Ye, Ana Ferreira, Maciej Wołczyk, de La Fortelle. The Kinematic Bicycle Model: a Consistent Model
Błaz˙ej Osin´ski, Moritz Niendorf, Hugo Grimmett, Qiangui Huang, forPlanningFeasibleTrajectoriesforAutonomousVehicles? InIEEE
Ashesh Jain, et al. Safetynet: Safe planning for real-world self- IntelligentVehiclesSymposium(IV),LosAngeles,UnitedStates,June
drivingvehiclesusingmachine-learnedpolicies. In2022International 2017.
ConferenceonRoboticsandAutomation(ICRA),pages897–904.IEEE, [41] StevenMLaValle. Planningalgorithms. Cambridgeuniversitypress,
2022. 2006.
[25] SaranTunyasuvunakool,AlistairMuldal,YotamDoron,SiqiLiu,Steven [42] AdamS´cibior,VasileiosLioutas,DanieleReda,PeymanBateni,and
Bohez,JoshMerel,TomErez,TimothyLillicrap,NicolasHeess,and Frank Wood. Imagining the road ahead: Multi-agent trajectory
YuvalTassa. dm_control:Softwareandtasksforcontinuouscontrol. predictionviadifferentiablesimulation. In2021IEEEInternational
SoftwareImpacts,6:100022,2020. IntelligentTransportationSystemsConference(ITSC),pages720–725,
[26] RichardSSuttonandAndrewGBarto. Reinforcementlearning:An 2021.
introduction. MITpress,2018. [43] Berend Zwartsenberg, Adam Scibior, Matthew Niedoba, Vasileios
[27] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Lioutas,JusticeSefas,YunpengLiu,SetarehDabiri,JonathanWilder
Hausman,ChelseaFinn,andSergeyLevine.Meta-world:Abenchmark Lavington,TrevorCampbell,andFrankWood.Conditionalpermutation
and evaluation for multi-task and meta reinforcement learning. In invariantflows. TransactionsonMachineLearningResearch,2023.
ConferenceonRobotLearning(CoRL),2019. [44] SaeidNaderiparizi,XiaoxuanLiang,BerendZwartsenberg,andFrank
[28] Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa Wood. Don’tbesonegative!score-basedgenerativemodelingwith
Zintgraf, Chelsea Finn, and Shimon Whiteson. A survey of meta- oracle-assistedguidance,2023.
reinforcementlearning. arXivpreprintarXiv:2301.08028,2023. [45] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex
[29] Yisheng Song, Ting Wang, Puyu Cai, Subrota K. Mondal, and Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray
JyotiPrakashSahoo. Acomprehensivesurveyoffew-shotlearning: Kavukcuoglu. Asynchronousmethodsfordeepreinforcementlearning.
Evolution,applications,challenges,andopportunities. ACMComput. InInternationalconferenceonmachinelearning,pages1928–1937.
Surv.,55(13s),jul2023. PMLR,2016.
[46] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,and
[30] KarlCobbe,ChrisHesse,JacobHilton,andJohnSchulman.Leveraging
OlegKlimov. Proximalpolicyoptimizationalgorithms. arXivpreprint
procedural generation to benchmark reinforcement learning. In
International conference on machine learning, pages 2048–2056.
arXiv:1707.06347,2017.
[47] ScottFujimoto,HerkevanHoof,andDavidMeger.Addressingfunction
PMLR,2020.
approximationerrorinactor-criticmethods.InJenniferDyandAndreas
[31] Panpan Cai, Yiyuan Lee, Yuanfu Luo, and David Hsu. Summit: A
Krause,editors,Proceedingsofthe35thInternationalConferenceon
simulatorforurbandrivinginmassivemixedtraffic. In2020IEEE
MachineLearning,volume80ofProceedingsofMachineLearning
InternationalConferenceonRoboticsandAutomation(ICRA),pages
Research,pages1587–1596.PMLR,10–15Jul2018.
4023–4029.IEEE,2020.
[32] PraveenPalanisamy. Multi-agentconnectedautonomousdrivingusing
deepreinforcementlearning. In2020InternationalJointConference
onNeuralNetworks(IJCNN),pages1–7.IEEE,2020.
[33] CraigQuiter. Deepdrivezero,2020.
[34] Ming Zhou, Jun Luo, Julian Villella, Yaodong Yang, David Rusu,
JiayuMiao,WeinanZhang,MontgomeryAlban,IMANFADAKAR,
ZhengChen,ChongxiHuang,YingWen,KimiaHassanzadeh,Daniel
Graves,ZhengbangZhu,YihanNi,NhatNguyen,MohamedElsayed,
HaithamAmmar,AlexanderCowen-Rivers,SanjeevanAhilan,Zheng
Tian,DanielPalenicek,KasraRezaee,PeymanYadmellat,KunShao,
dongchen,BaokuanZhang,HongboZhang,JianyeHao,WulongLiu,
andJunWang. Smarts:Anopen-sourcescalablemulti-agentrltraining
school for autonomous driving. In Jens Kober, Fabio Ramos, and
ClaireTomlin,editors,Proceedingsofthe2020ConferenceonRobot
Learning,volume155ofProceedingsofMachineLearningResearch,
pages264–285.PMLR,16–18Nov2021.
[35] Anirban Santara, Sohan Rudra, Sree Aditya Buridi, Meha Kaushik,
AbhishekNaik,BharatKaul,andBalaramanRavindran.Madras:Multi
agent driving simulator. Journal of Artificial Intelligence Research,
70:1517–1555,2021.
[36] QuanyiLi,ZhenghaoPeng,LanFeng,QihangZhang,ZhenghaiXue,
andBoleiZhou. Metadrive:Composingdiversedrivingscenariosfor
generalizable reinforcement learning. IEEE transactions on pattern
analysisandmachineintelligence,2022.
[37] Qiao Sun, Xin Huang, Brian Williams, and Hang Zhao. InterSim:
Interactivetrafficsimulationviaexplicitrelationmodeling. In2022
IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems
(IROS).IEEE,2022.
[38] Danfei Xu, Yuxiao Chen, Boris Ivanovic, and Marco Pavone. Bits:
Bi-levelimitationfortrafficsimulation. In2023IEEEInternational