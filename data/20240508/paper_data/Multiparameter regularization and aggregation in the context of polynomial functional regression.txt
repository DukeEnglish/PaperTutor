Multiparameter regularization and aggregation in the context of polynomial
functional regression
ElkeR.Gizewskia,b,MarkusHolzleitnerc,∗,LukasMayer-Suessd,SergiyPereverzyevJr.a,b,SergeiV.Pereverzyeve
aDepartmentofRadiology,MedicalUniversityofInnsbruckAnichstrasse356020,Innsbruck,Austria
bNeuroimagingResearchCoreFacility,MedicalUniversityofInnsbruckAnichstrasse356020,Innsbruck,Austria
cMaLGaCenter,DepartmentofMathematics,UniversityofGenoaViaDodecaneso3516146,Genoa,Italy
dDepartmentofNeurology,MedicalUniversityofInnsbruckAnichstrasse356020,Innsbruck,Austria
eJohannRadonInstituteforComputationalandAppliedMathematics,AustrianAcademyofSciencesAltenbergerStraße69A-4040,Linz,Austria
Abstract
Mostoftherecentresultsinpolynomialfunctionalregressionhavebeenfocusedonanin-depthexplorationofsingle-
parameterregularizationschemes.Incontrast,inthisstudywegobeyondthatframeworkbyintroducinganalgorithm
formultipleparameterregularizationandpresentingatheoreticallygroundedmethodfordealingwiththeassociated
parameters. Thismethodfacilitatestheaggregationofmodelswithvaryingregularizationparameters. Theefficacyof
theproposedapproachisassessedthroughevaluationsonbothsyntheticandsomereal-worldmedicaldata,revealing
promisingresults.
Keywords: Statisticallearningtheory,Functionalpolynomialregression,Multiparameterregularization,Aggregation
2020MSC: Primary65K10,Secondary62G20
1. Introduction
Functional data analysis has emerged as a vibrant and dynamic research area and is present in various aspects
of our daily lives, such as climate studies, medicine, economics, and healthcare, just to name a few. Typically,
functionaldataappearintheformsoftimeseries,shapes,images,andanalogousobjects. Whiletheterm”functional
data analysis” was first used in [25, 26], significant advancements have happened since then. For a comprehensive
explorationofmethods,theory,andapplications,werefertoseminalreviewarticleslike[16,27,28,32],andalsoto
theveryrecentlyappearedspecialissue[1].
This work focuses specifically on functional data inputs that are labeled with scalar-valued outputs. One of the
most extensively studied methods in this context assumes a linear relationship between inputs and outputs, so that
theoutputscanberepresentedaslinearfunctionalsofthe(functional)inputs, accompaniedmaybebyanadditional
noiseterm. OnepopularapproachtocapturelinearfunctionalregressionisbasedonreproducingkernelHilbertspace
(RKHS) techniques, so that the known arguments from kernel regression (see e.g. [6, 11, 17, 18]) can be used. A
by no means complete list of works in this direction can be found in [30, 31, 34] and references therein. We may
alsomention[13],wherelinearfunctionalregressionapproachisproposedinamoresophisticatedsettingofdomain
generalization.
Similarly to the case of extending standard linear regression by allowing polynomial interactions, polynomial
functionalregression(PFR),whichincludesthefunctionallinearmodelandfunctionalquadraticmodelastwospecial
cases,wasproposedin[22]. Thenithasbeendiscussedin[28,29]andrecentlyin[12],whereacompletetreatment
oftheinterplaybetweensmoothness,capacityandgeneraloneparameterregularizationschemesisprovided(asdone
∗Correspondingauthor
Emailaddresses:elke.gizewski@i-med.ac.at(ElkeR.Gizewski),holzleitner@dima.unige.it(MarkusHolzleitner),
lukas.mayer@i-med.ac.at(LukasMayer-Suess),sergiy.pereverzyev@i-med.ac.at(SergiyPereverzyevJr.),
sergei.pereverzyev@oeaw.ac.at(SergeiV.Pereverzyev)
PreprintsubmittedtoElsevier
4202
yaM
7
]LM.tats[
1v74140.5042:viXrae.g. for standard kernel regression in [11, 18]). In particular, the study [12] has advocated the use of iterated one-
parameterTikhonovregularizationmethodinthecontextofPFR.
One drawback of using single-parameter (iterated) Tikhonov regularization is, that all norms of the individual
monomialsintheregularizationtermaregivenequalweight,andthereforetheadvantageofusinghigherordermono-
mials is not developed to its fullest potential. Consequently, it is advisable to introduce specific weight parameters
foreachindividualmonomial, andweenvisionitasagoodplacetoadvertisemulti-parameterregularizationinthis
context.
Ingeneral,multi-parameter(MP)regularizationschemeshavearichhistory,bothintermsoftheoryandapplica-
tions,andwereferto[20][Chapter3]andreferencesthereinforacomprehensivesummary.Itisinterestingtonotethat
theusageofmultipleparametershasbeenjudgedvariouslybydifferentauthors. Justtogivetwoexamples: in[33],
the authors found, that it provides only marginal improvements, whereas in [5] it is claimed that MP-regularization
helps significantly, when the one parameter counterparts do not lead to satisfying results. One main finding of our
workis,thatinthecaseofPFR,weareinasimilarsituationasin[5],andonecandemonstratetheadvantageofusing
multi-parameterPFRinnumericalexamplesbasedonsynthetictoydataandonsomereal-worldmedicaldata.
At the same time, there is a common belief that the choice of the regularization parameters is crucial, and we
are only aware of a few works that tackle this serious challenge in the MP case: a heuristic L-curve based strategy
is proposed in [4], in [2, 3, 8] knowledge of the noise structure is required and in [19] an approach based on the
discrepancyprincipleisdiscussed,whichiscostlytocompute.
ThesolutionthatwewillproposeinthespecificsettingofPFR,isbasedontheso-calledaggregationbythelinear
functionalstrategy,whichmaybetracedbackto[7](seealsoSection3.5[24]andreferencestherein). Inthecontext
ofstandardscalarandvectorvaluedregressionsuchtypeofaggregationhasrecentlyleadtosuccessfulperformances
in domain adaptation, a field that in many aspects is very sensitive to parameter selection as well, see e.g. [9, 10].
However,wearenotawareofanyworksthatemploytheaggregationtechniquesinthecontextoffunctionaldatawith
MPregularizationyet,andthusanothermainpartofourstudyistoprovidetheoreticalandnumericalevidence,that
aggregationcanbesuccessfullyappliedinthesesettingsaswell.
Themainfindingsofthisworkcanthereforebesummarizedasfollows:
• Weintroducemulti-parameterregularizationinthecontextofPFRandderivealinearsystemthatallowsusto
computethecorrespondingsolutions.
• Inordertodealwithturningofmultipleregularizationparameters,weproposeanaggregationprocedureinthe
contextofPFR.
• Weprovidenumericalevidence,thatMPregularizationandaggregationcanbeusefulconceptsforPFRalsoin
practice,ontheonehandonsyntheticdata,andontheotherhand,ondatafromamedicalapplication,where
thetaskistodetectstenosisinbrainarteries.
Our work will now be structured as follows. In Section 2, we will recall the setting of regularized PFR, by re-
peatingthedefinitions,assumptionsandestimatesfrom[12]. InSection3,analgorithmhowtocomputethesolution
associatedtoMP-PFRisdiscussed,whereasSection4proposesanaggregationstrategyforPFR,whichcanbeevalu-
atednumericallyandwhichcomeswithadditionaltheoreticalguarantees.Section5isthendevotedtotheexperiments
onsyntheticandreal-worldmedicaldata.
2. Setting
2.1. Overallsettingandassumptions
LetI ⊂ Rd andconsidertheassociatedspace L2(I)consistingofsquareintegrablefunctionswithrespecttothe
Lebesguemeasureµ,sothat
(cid:90)
∥u∥2 = |u(t)|2dµ(t).
L2(I)
I
2Moreover,letL2(Ω,P)beaspaceofrandomvariablesY =Y(ω)definedonaprobabilityspace(Ω,F,P),ω∈Ω,with
boundedsecondmoments,sothat
(cid:90)
∥Y∥2 :=E|Y|2 = |Y(ω)|2dP(ω).
L2(Ω,P)
Ω
Consider also the tensor product L2(Ω,P) ⊗ L2(I), which is nothing but a collection of random variables X(ω,s)
indexedbypointss∈Iandhavingboundedsecondmomentsinthefollowingsense:
∥X∥2 :=E∥X(ω,·)∥2 .
P,µ L2(I)
TheinnerproductsintheconsideredHilbertspacesH willalwaysbedenotedby⟨.,.⟩ ,andthespaceisindicatedby
H
asubscript.
Functionaldataconsistofrandomi.i.d. samplesoffunctions X (s),...,X (s), thatcanbeseenasrealizationsof
1 N
astochastic process X(ω,s) ∈ L2(Ω,P)⊗L2(I). Now letusdiscuss thesettingof polynomialfunctional regression
(PFR):LetY ∈ L2(Ω,P)beascalarresponse,andX ∈ L2(Ω,P)⊗L2(I)bethecorrespondingfunctionalpredictor. We
makethefollowingassumptiononX(asimposedinasimilarway,e.g.,in[31,34]):
Assumption1.
sup∥X(ω,·)∥
L2(I)
≤κ.
ω∈Ω
InPFRoneaimsatminimizingtheexpectedpredictionrisk:
(cid:16) (cid:17)
E(U (X))=E |Y(ω)−U (X(ω,·))|2 →min, (1)
p p
whereU (X(ω,·))isapolynomialregressionoforder p:
p
(cid:88)p (cid:90) (cid:89)l
U (X(ω,·))=u + u(s ,...,s) X(ω,s )dµ(s ).
p 0 l 1 l j j
l=1 Il j=1
Hereu ∈ L2 :=R,andu ∈ L2,where
0 0 l l
L2 = L2(I)⊗···⊗L2(I).
l (cid:124)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125)
l-times
Toproceedandformalizethesettingfurther,considertheoperator
A :R→ L2(Ω,P)
0
assigningtoanyu ∈ Rthecorrespondingconstantrandomvariable. Moreover,consider A : L2 → L2(Ω,P),such
0 l l
that
(cid:90) (cid:89)l
(Au)(ω)= u(s ,...,s) X(ω,s )dµ(s ). (2)
l l 1 l j j
Il j=1
Let, also, L2 =
(cid:76)p
L2 be a direct sum of spaces L2 consisting of finite sequences u = (u ,...,u ), u ∈ L2,
l = 0,1,...,p,equippel= d0 wl iththenorm∥u∥2 = (cid:80)p ∥u∥2l ,andconsidertheboundedlinearoperat0 or(whp ichil salsol a
L2 l=0 l L2
l
Hilbert-Schmidtone,aswillbeseenfromLemma1)A:L2 → L2(Ω,P),givenby
(cid:88)p
Au=(A ,A ,...,A )◦(u ,u ,...,u )= Au. (3)
0 1 p 0 1 p l l
l=0
3Observethatforanyu∈ L2(Ω,P)theoperatorA∗ : L2(Ω,P)→ L2assignstoittheelement
l l
(cid:90) (cid:89)l
(A∗u)(s ,...s)= u(ω) X(ω,s)dP(ω),
l 1 l i
Ω i=1
andtherefore,A∗Aisa(p+1)×(p+1)matrixoftheoperators
(cid:110) (cid:111)
A∗A= A∗A : L2 → L2,k,l=0,1,...,p
k l l k
,whereA∗A u =u and
0 0 0 0
(cid:90) (cid:90) (cid:89)l
A∗Au= u(s ,...,s) X(ω,s)dµ(s)dP(ω),
0 l 1 l i i
Ω Il i=1
(cid:90) (cid:89)k (cid:90) (cid:89)l
A∗Au(s ,...,s )= X(ω,s ) u(s˜ ,...,s˜) X(ω,s˜)dµ(s˜)dP(ω),
k l 1 k j 1 l i i
Ω j=1 Il i=1
k,l=1,...,p.
Equipped with this notation, we can write that U (X(ω,·)) = Au, such that (1) is reduced to the least square
p
solution of the equation Au = Y, because E(U (X)) = ∥Y−Au∥2 . Let us also use the following standard
p L2(Ω,P)
assumption:
Assumption2. TheprojectionPY ofY ontheclosureoftherangeofAissuchthatPY ∈Range(A).
Itiswellknown(see,e.g.,[20][Proposition2.1.]),thatunderAssumption2theminimizeru=u+ =(u+,...,u+)of
0 p
(1)solvesthenormalequationA∗Au=A∗Y.
Wewillalsousethefactthatforanyu∈L2
(cid:13)√ (cid:13)
∥Au∥ L2(Ω,P) =(cid:13) (cid:13) (cid:13) A∗Au(cid:13) (cid:13) (cid:13) L2, (4)
whichfollowsimmediatelyfromthepolardecompositionoftheoperatorA.
Forthefurtheranalysisletusalsoadoptthefollowingresponsenoisemodel:
Assumption3.
Y
=Au++ε,
(5)
whereanoisevariableε : Ω → Risindependentfrom X,E(ε) = 0,andforsomeσ > 0itshouldsatisfyeitherthe
condition
E(|ε(ω)|2)≤σ2, (6)
orobey,foranyintegerm˜ ≥ 2andsome M > 0,aslightlystrongermomentcondition,whichisalsostandardin
theliterature,seee.g. [30],
1
E(|ε(ω)|m˜)≤ σ2m˜!Mm˜−2. (7)
2
However,theinvolvedoperatorsareinaccessible,becausewedonotknowP. Thus,wewanttoapproximatethem
byusingtrainingdata(Y,X(·)),i = 1,...,N,consistingofN independentsamplesoftheresponseandthefunctional
i i
predictor(Y(ω),X(ω,·)),sothat
Y
=Au++ε,
i i i
whereA isdefinedinthesamewayasAbythereplacementofX(ω,·)intheformulas(2)and(3)withX(·),andε
i i i
isasamplefromthenoisevariableintroducedinAssumption3.
4Moreover,u+doesnotdependcontinuouslyontheinitialdatum,suchthatweneedtoemployaregularization.
Thesimplestandarguablymostwellknownregularizationinthiscontextisthesingle-parameterTikhonovregu-
larization,soforλ>0wewanttofindtheminimizeru oftheregularizedPFR
λ
∥Y−Au∥2 +λ∥u∥2 →min, (8)
L2(Ω,P) L2
whichsolvestheequationλu+A∗Au=A∗Y andcanbeapproximatedbythesolutionuN of
λ
λu+[A∗A] u=[A∗Y] . (9)
N N
(cid:110) (cid:111)
Theseapproximationsaregivenby[A∗A] = [A∗A] : L2 → L2,k,l=0,1,...,p sothat:
N k l N l k
[A∗A ] u=u,
0 0 N
1
(cid:88)N (cid:90) (cid:89)l
[A∗A] u= u(s ,...,s) X(s )dµ(s ),
0 l N N i=1 Il 1 l j=1 i j j
1
(cid:88)N (cid:89)k (cid:90) (cid:89)l
[A∗A] u(s ,...,s )= X(s ) u(s˜ ,...,s˜) X(s˜ )dµ(s˜ ),
k l N 1 k N i=1 j=1 i j Il 1 l m=1 i m m
k,l=1,...,p. (10)
and[A∗Y] =([A∗Y] ,...,[A∗Y] )∈L2,sothat
N 0 N p N
1
(cid:88)N
[A∗Y] = Y,
0 N N i
i=1
1
(cid:88)N (cid:89)l
[A∗Y] (s ,...,s)= Y X(s ),
l N 1 l N i i j
i=1 j=1
l=1,...,p. (11)
Athoroughanalysisofone-parameterregularizedPFRhasbeenexecutedin[12]forageneralizedregularization
scheme,seee.g. Theorem1fortheirmainfinding.
Yet,whenconsideringthesingle-parameterregularizationwithintherealmofPFR,itcouldbecontendedthatthis
mightnotbethemostsuitableselection.Thisapproachoverlooksindividualcontributionsassociatedwithmonomials
of varying degrees, treating them all with equal weight. In this context, a more fitting alternative for PFR is the
employmentofMPregularization,achoicethatwewilldiscussthoroughlyinSection3. Butbeforethatletusmove
onbycollectingseveralauxiliaryresults,whichhavemostlybeenderivedalreadyin[12].
2.2. Operatornormsandrelatedauxiliaryestimates
Herewecollectseveralestimatesrelatedtothenormsofthepreviouslydiscussedoperators. Mostoftheseresults
havebeendiscussedin[12].
Lemma1(Lemma1in[12]). LetHS(H ,H )denotetheHilbertspaceofHilbert-SchmidtoperatorsbetweenHilbert
1 2
spacesH andH . ForsimplicityletusalsouseHS(H ,H )=HS(H ).UnderAssumption1wehavethat
1 2 1 1 1
(cid:88)p
∥A∥
HS(L2,L2(Ω,P))
≤κ˜ =: κl
l=0
∥A∗A∥ HS(L2),∥[A∗A] N∥
HS(L2)
≤κ˜2
Lemma2(Lemma2in[12]). Foranyδ∈(0,1),withconfidenceatleast1−δwehavethat
4κ˜2 2
∥A∗A−[A∗A] N∥ L2→L2 ≤∥A∗A−[A∗A] N∥ HS(L2) ≤ √
N
log
δ
(12)
5Lemma3(comparewithLemma4in[12]). Foranyδ∈(0,1),withconfidenceatleast1−δwehavethatincaseof
noiseassumption(6):
(cid:13) (cid:13) σκ˜
(cid:13) (cid:13)[A∗A] Nu+−[A∗Y] N(cid:13) (cid:13)
L2
≤ √ Nδ, (13)
whereasincaseof (7):
(cid:13) (cid:13) (M+σ)κ˜log(2/δ)
(cid:13) (cid:13)[A∗A] Nu+−[A∗Y] N(cid:13) (cid:13)
L2
≤ √
N
. (14)
Lemma 3 can, to some extend, be seen as a special case of Lemma 4 in [12], however, in order to introduce
notation and techniques required for some further technical results as e.g. Lemma 6, we still decided to provide its
proofhere:
Tothisendwealsoneedtorecallthefollowingwell-knownconcentrationbound:
Lemma 4 (see e.g. Theorem 3.3.4. in [35]). Let ξ be a random variable with values in a Hilbert space H. Let
{ξ ,ξ ,...,ξ } be a sample of N independent observations of ξ. Furthermore, assume that the bound E∥ξ∥m˜ ⩽
1 2 N H
vm˜!um˜−2holdsforevery2⩽m˜ ∈N,thenforany0<δ<1withconfidenceatleast1−δwehave
2
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)N1
(cid:88)N
(cid:2) ξ
i−E(ξ)(cid:3)(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13)
(cid:13)
⩽ 2ulo Ng(2/δ) +
(cid:114)
2vlo Ng(2/δ)
(cid:13) i=1 (cid:13) H
ProofofLemma3. Letusfirstfocusonthemoreinvolvedestimate(14). Theestimate(13)canbeprovenbysimilar
reasoning. Considerthematrixofoperators
(cid:110) (cid:111)
A(ω)= A (ω): L2 → L2(Ω,P)⊗L2,k,l=0,...,p ,
k,l l k
whereA u(ω)=u, A u(ω)=(Au)(ω),
0,0 0,l l
(cid:89)k (cid:90) (cid:89)l
A u(ω,s ,...,s )= X(ω,s ) u(s˜ ,...,s˜) X(ω,s˜ )ds˜ (15)
k,l 1 k j 1 l m m
j=1 Il m=1
k,l=1,...,p,ω∈Ω.
Then the operators Ai, i = 1,...,N, defined by using X(·) instead of X(ω,·) in the above formulas, can be seen as
i
independentobservationsofA(ω).
ItisclearthatE(A(ω))=A∗Aandthat∥A(ω)∥
HS(L2)
≤κ˜2,sothatA(ω)isarandomvariableinHS(L2).Moreover
weintroducethevectorsX(ω)∈ L2(Ω,P)⊗L2,
(cid:89)k
X(ω)=(X (ω))p , X (ω)=1, X (ω)= X(ω,s ), k=1,...,p, (16)
k k=0 0 k j
j=1
with∥X(ω)∥
L2
≤κ˜,andtheL2-valuedrandomvariable
ξ(ω)=(Y(ω)−A(ω)u+ )X(ω)=ε(ω)X(ω),
wherethelastequalityisduetoAssumption3. Thenthefunctions
ξ
=YXi−Aiu+
,
i i
where Xi are defined by using X(·) instead of X(ω,·) in (16), can be seen as independent observations of ξ(ω).
i
Moreoverwehave:
1
(cid:88)N
1
(cid:88)N
1
(cid:88)N
ξ = YXi− Aiu+ ,
N i N i N
i=1 i=1 i=1
6sothatfork=0,...,p,recalling(11):
 
N1
(cid:88)N
Y
iXi
(s 1,...,s k)=
N1
(cid:88)N
Y
i(cid:89)k
X i(s j)=[A∗ kY] N(s 1,...,s k), (17)
i=1 k i=1 j=1
andrecalling(10):
 N1 (cid:88) i=N
1
Aiu+ 
k(s 1,...,s k)=
N1 (cid:88) i=N
1
(cid:88) l=p
0
(cid:89) j=k
1
X i(s
j)(cid:90)
Ilu+ l(s˜ 1,...,s˜
l)(cid:89) m=l
1X i(s˜ m)dµ(s˜ m)=([A∗A] Nu+ ) k(s 1,...,s k), (18)
whichallowsustoconclude:
1
(cid:88)N
ξ =[A∗Y] −[A∗A] u+ .
N i N N
i=1
DuetoAssumption3wehave
E(ξ(ω))=E(X(ω))E(ε(ω))=0.
Moreover,theindependenceofεandXleadstotheconclusionthat:
σ2κ˜2
E(∥ξ∥m˜ )≤E(∥X(ω)∥m˜ )·E(|ε(ω)|m˜)≤ (Mκ˜)m˜−2m˜!.
L2 L2 2
NowtheapplicationofLemma4forξ(ω)andξ yieldsthedesiredbound(14).
i
Toobtain(13)weneedtofollowthesamelinesofproof, butonlyconsiderthecasem˜ = 2andafterwardapply
Tschebyshev’sinequalityinstead.
3. Multiparameterregularization
EquippedwiththenecessarybackgroundandnotationonPFR,letuscontinueourdiscussiononMPregularization
inthiscontext. Insteadofdealingwith(8)andusingonlyasingleparameterλ, weconsideravectorλ = (λ ,...λ )
0 p
of the regularization parameters λ ≥ 0, l = 0,...,p, and the corresponding regularization functional with multiple
l
penalties:
(cid:88)p
∥Y−Au∥2 + λ ∥u∥2 →min. (19)
L2(Ω,P) l l L2
l=0 l
Next,letP : L2 → L2,l = 0,...,p,betheprojectionofL2 ontoL2,i.e. P(u ,...,u ) = (0,..,0,u,0,...0),sothat
l l l 0 p l
clearly∥P lu∥ L2 =∥u l∥2 L2. Then(19)canequivalentlybewrittenas
l
(cid:88)p
∥Y−Au∥2 + λ ∥Pu∥2 →min,
L2(Ω,P) l l L2
l=0
andusingargumentssimilartothosegivenin[20,Formula(3.12)]),wecaneasilymakethefollowingobservation:
Lemma5. Theminimizeru of (19)solvestheequation
λ
(cid:88)p
λPu+A∗Au=A∗Y. (20)
l l
l=0
7aP nr doo tf h. atT oh fe FF lr (e uc )h =e´t λd le ∥ri Pv la ut ∥iv
2
Le
2
o bf yF F( l′u () u)= (v∥ )Y =− 2A
λ
lu (cid:68)∥ P2
L2
∗
l( PΩ, lP u)
,i vn (cid:69)d L2ire =ct 2io λn l⟨v P∈ lu,L v2
⟩
Lis 2.g Siv ee tn tinb gy tF h′ e(u d) e( rv i) va= ti⟨ v2 e(A of∗A Fu +− (cid:80)A
lp
=∗ 0Y F), lv t⟩ oL2,
zeroandusingtheconvexityoftheproblemunderconsiderationwearriveat(20).
NextweemployaMonte-Carlotypediscretizationof(20)andapproximatetheminimizerof(19)bythesolution
uN of
λ
(cid:88)p
λPu+[A∗A] u=[A∗Y] (21)
l l N N
l=0
Inviewof(10),(11)theregularizedapproximationuNcanbeconstructedintheformuN =(uN ,uN ,...,uN )∈L2,
λ λ λ,0 λ,1 λ,p
sothat
uN =b ∈R,
λ,0 0
(cid:88)N (cid:89)l
uN (s ,...,s)= b X(s )∈ L2, l=1,...,p.
λ,l 1 l l,i i j l
i=1 j=1
Insertingthisansatzinto(21)andequatingthecorrespondingcoefficientsweobtainthefollowingsystemofpN+1
linearequationsforb andb ,k=1,...,p,i=1,...,N:
0 k,i
1
(cid:88)N (cid:88)p (cid:88)N
1
(cid:88)N
(λ +1)b + b (c )l = Y, (22)
0 0 N l,s i,s N i
i=1 l=1 s=1 i=1
1 1
(cid:88)p (cid:88)N
1
λ b + b + b (c )l = Y, (23)
k k,i N 0 N l,s i,s N i
l=1 s=1
(cid:82)
wherec = X(s˜)X (s˜)dµ(s˜). Notethatforsingle-parameterregularization,i.e. forthecaseofλ = λ = ... = λ ,
i,s I i s 0 p
thesystem(22),(23)allowsforareductiontoalinearsystemofonlyN+1equations. Thiswasdiscussedindetailin
Section3of[12].
Acrucialissue, however, isthechoiceoftheregularizationparametersλ ,...,λ . InSection1wealreadymen-
0 p
tioned several approaches to this issue. But most of them select just one set of parameters. On the other hand, it
seemsmorepracticaltouseallvaluesfromagridofparametersandthenaggregatealltheresultingmodels,suchthat
evenbadlychosenregularizationparameterscanintheendcontributetoanimprovedmodel. Inthenextsectionwe
willtheoreticallyjustifyanaggregationmethodinthecontextofPFRandalsoobserveitsusefulnessintheempirical
evaluationsinSections5.1–5.2.
4. Aggregationofmultipleregularizedpolynomialfunctionalmodels
TocontinuewithadiscussionofanaggregationstrategyinthePFRcontext,letusnowassumethatwearegiven
asequenceofmodelsu ,...,u ∈L2,sothatthefollowingassumptionisvalid:
1 R
Assumption4.
(cid:13) (cid:13)
(cid:13) (cid:13)u+(cid:13) (cid:13) L2,∥u r∥ L2 ≤C R
forr=1,··· ,RandsomeC >0.
R
8Ourgoalistocomputeanaggregation
(cid:88)R
c u (24)
r r
r=1
withcoefficientsc ,...,c ∈R,sothattheexcessofriskE((cid:80)R c u )−E(u+)isassmallaspossible. Wealreadyknow
1 r r=1 r r
from(4)and(5)that
E(u)−E(u+ )=(cid:13) (cid:13) (cid:13)A(cid:0) u−u+(cid:1)(cid:13) (cid:13) (cid:13)2
L2(Ω,P)
=(cid:13) (cid:13) (cid:13) (cid:13)√ A∗A(cid:0) u−u+(cid:1)(cid:13) (cid:13) (cid:13) (cid:13)2 L2, (25)
foranyu∈L2,sothatourmainobjectivecanbewrittenasfollows:
c1,m ...,cin
R∈R(cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13)
(cid:13)√ A∗A (cid:88) r=R
1c
rur−u+ (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13)
(cid:13)2
L2. (26)
Next we observe that the minimizer of (26), i.e. the best approximation u∗ of the target regression function u+ by
linear combinations, corresponds to the vector c∗ = (c∗,...,c∗) of ideal coefficients in (24) that solves the linear
1 R
systemGc∗ =g¯ withtheGrammatrix
G
=(cid:16)(cid:68)√
A∗Au r,
√
A∗Au
r′(cid:69) L2(cid:17)R
r,r′=1
=(cid:16)
⟨Au r,Au r′⟩
L2(Ω,P)(cid:17)R
r,r′=1
andthevector
g¯ =(cid:16)(cid:68)√ A∗Au+ , √ A∗Au (cid:69) (cid:17)R =(cid:16)(cid:10)Au+ ,Au (cid:11) (cid:17)R
r L2 r=1 r L2(Ω,P) r=1
(seee.g. [24,Section3.5.] foraproofofthiswellknownobservation).
NotethatthesuccessfulinversionofGdependsontheassumptionthatourmodelsexhibitsufficientdissimilarity.
Thisrequirementisinherent,aswithoutit,wecouldeffortlesslyeliminateredundantmodels. But,ofcourse,neither
GrammatrixGnorthevectorg¯isaccessible,becausethereisnoaccesstoP,soweswitchtotheempiricalcounterparts
G(cid:101)and(cid:101)g,i.e.
G(cid:101)=
N1
(cid:88)N
(A iu r)(A iu
r′) R
(27)
i=1 r,r′=1
(cid:101)g=
N1
(cid:88)N
Y i(A iu
r) R
. (28)
i=1 r=1
Thenwecomputethesolutionc˜ =(c˜ 1,...,c˜ R)tothesystemG(cid:101)c˜ = (cid:101)g,sothatouraggregatedmodelisgivenby
(cid:88)R
u˜ = c˜ u . (29)
r r
r=1
Ourmainresultisaboutthequalityofthisaggregationcomputedfromdata,andweshowthatE(u˜)−E(u+)approaches
2(E(u∗)−E(u+))whenthesamplesizeincreases:
Theorem1. Underassumptions1–4withprobability1−δitholdsthatforsufficientlylargeN
E(u˜)−E(u+ )≤2(cid:0) E(u∗)−E(u+ )(cid:1)+CN−1log2 1
, (30)
δ
wherethecoefficientC >0doesnotdependonN andδ.
9Accordingtoourtheorem,theexcessofriskoftheproposedalgorithmisasymptoticallynotworsethantwicethe
excessofriskoftheunknownoptimalaggregation,becauseitisclear(see,e.g.,Corollary1in[12])thatthesecond
termintherighthandsideof(30)isnegligiblysmall.
TheproofofthisresultwillcruciallydependonthefollowingLemma,whichrelatestheentriesofGandG˜ andg¯
andg˜,respectively:
Lemma6. Underassumptions1–4withprobability1−δwehavethatforanyr,r′ =1,...,R:
(cid:12) (cid:12)
(cid:12) (cid:12) 1 (cid:88)N (cid:12) (cid:12) Clog2
(cid:12) (cid:12)
(cid:12)
(cid:12)⟨Au r,Au r′⟩ L2(Ω)−
N
i=1
A iu rA iu r′(cid:12) (cid:12)
(cid:12)
(cid:12)≤ √ Nδ, (31)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)(cid:10)Au+ ,Au r(cid:11) L2(Ω)− N1 (cid:88) i=N
1
Y iA iu r(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)≤ C √lo Ng2 δ, (32)
whereCissomegenericconstantC,whichdoesnotdependonN orδ.
Proof. Letusstartbyshowing(31). Observethatinviewof(10)wehave
1
(cid:88)N
1
(cid:88)N
N
A iu rA iu r′ =
N
⟨X i,u r⟩ L2⟨X i,u r′⟩ L2 =⟨[A∗A] Nu r,u r′⟩ L2,
i=1 i=1
Then
⟨A∗Au r,u r′⟩ L2 =⟨[A∗A] Nu r,u r′⟩ L2 +⟨(A∗A−[A∗A] N)u r,u r′⟩ L2,
anditremainstoestimatethelasttermtoarriveat(31):
4κ˜2 2
|⟨(A∗A−[A∗A] N)u r,u r′⟩ L2|≤∥A∗A−[A∗A] N∥ L2→L2∥u r∥ L2∥u r′∥ L2 ≤C R2 √
N
log δ,
whereweusedCauchy-Schwartzinequality,Lemma2andAssumption4.
Nowletusdealwith(32). Itisclearfrom(17)that
1
(cid:88)N
1
(cid:88)N
⟨[A∗Y] N,u r⟩ L2 =
N
Y i⟨X i,u r⟩ L2 =
N
Y iA iu r.
i=1 i=1
Thenwecancontinueasfollows:
(cid:10)A∗Au+ ,u r(cid:11) L2 =⟨[A∗Y] N,u r⟩ L2 +(cid:10) ( (cid:32)(cid:32)A (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)∗ (cid:32)(cid:32)A (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)− (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)[ (cid:32)(cid:32)A (cid:32)(cid:32)(cid:32)(cid:32)∗A] (cid:32)(cid:32)(cid:32)N(cid:32)(cid:32)(cid:32)(cid:32)) (cid:32)(cid:32)u (cid:32)(cid:32)(cid:32)+ (cid:32)(cid:32)(cid:32)(cid:32), (cid:32)(cid:32)(cid:32)u (cid:32)(cid:32)(cid:32)r(cid:32)(cid:32)(cid:11) (cid:32)(cid:32)(cid:32)L(cid:32)2+(cid:10) [ (cid:32)(cid:32)A (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)∗ (cid:32)(cid:32)A (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)] (cid:32)(cid:32)(cid:32)N(cid:32)(cid:32)(cid:32)(cid:32)u (cid:32)(cid:32)(cid:32)+ (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)− (cid:32) [A (cid:32)(cid:32)(cid:32)(cid:32)∗ (cid:32)(cid:32)(cid:32)Y (cid:32)(cid:32)(cid:32)(cid:32)] (cid:32)(cid:32)(cid:32)N(cid:32)(cid:32)(cid:32)(cid:32), (cid:32)(cid:32)u (cid:32)(cid:32)(cid:32)(cid:32)r(cid:32)(cid:32)(cid:11) (cid:32)(cid:32)(cid:32)L(cid:32)2.
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(I) (II)
For(I)weapplyLemma2,Assumption4andCauchy-Schwartzinequalitytoobtainthebound:
4κ˜2 2
(I)≤C2 √ log ,
R N δ
whereasfor(II)weuse(13)or(14)andagainAssumption4andCauchy-Schwartztohave:
C 2
(II)≤C2 √ log ,
R N δ
wheretheconstantCmaybedifferent,dependingonwhethernoiseassumption(6)or(7)isinforce.Now(32)follows
bycombining(I)and(II).
10Nowwecanusesimilarargumentsasused,e.g.,intheproofofTheorem1of[9].Inthesequel,∥.∥
RR
and∥.∥
RR→RR
denotetheusualEuclideanandtheFrobeniusnorm,respectively. FromLemma6wecanarguethatwithprobability
1−δitholds:
1
∥g¯−g˜∥
RR
≤Clog δN−1 2, (33)
(cid:13) (cid:13) 1
(cid:13) (cid:13)G−G˜(cid:13) (cid:13)
RR→RR
≤Clog δN−1 2. (34)
ItisalsostraightforwardtoboundtheentriesofG˜ uniformly:
|G˜ r,r′|≤∥[A∗A] N∥ L2→L2∥u r∥ L2∥u r′∥
L2
≤κ˜2C R2.
Moreoverwecanusethefollowingsimplemanipulation:
G−1 =G˜−1(GG˜−1)−1 =G˜−1(I−(I−GG˜−1))−1 =G˜−1(I−(G˜ −G)G˜−1)−1.
ThenusingtheNeumannseriesfor(I−(G˜ −G)G˜−1)−1weobtainthefollowingbound:
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)G˜−1(cid:13)
(cid:13)
(cid:13) (cid:13)G−1(cid:13) (cid:13) ≤ (cid:13) (cid:13) R (cid:13)R→RR (cid:13) ≤c. (35)
RR→RR 1−(cid:13) (cid:13)G˜−1(cid:13)
(cid:13)
(cid:13) (cid:13)G−G˜(cid:13)
(cid:13)
RR→RR RR→RR
(cid:13) (cid:13)
Tosee(35),wefirstobservethatitisnaturaltoassumethat(cid:13) (cid:13)G˜−1(cid:13) (cid:13)≤cforsomegenericc>0(otherwise,wecan,e.g.,
orthogonalizeourmodelsandcoefficientswithoutchangingtheaggregation,butwithreducingtheconditionnumber
(cid:13) (cid:13)(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)G˜−1(cid:13) (cid:13)(cid:13) (cid:13)G˜(cid:13) (cid:13)). Secondly,by(34)itisalsonaturaltoassumethat(cid:13) (cid:13)G−G˜(cid:13) (cid:13) < 1 bychoosingN sufficientlylarge.
RR→RR (cid:13) 2c (cid:13)
ThereforetheNeumannseriesassociatedto(I−(G˜−G)G˜−1)−1converges,since(cid:13) (cid:13)(G˜ −G)G˜−1(cid:13) (cid:13) < 1.Thisallows
(cid:13) (cid:13) RR→RR 2
todeduce(cid:13) (cid:13)G−1(cid:13) (cid:13) ≤2c. Nowweareinthepositiontoproveourmaingeneralizationbound(30):
RR→RR
ProofofTheorem1. Since:
G−1(g˜−g¯)+G−1(G−G˜)c˜ =G−1g˜−c∗+c˜−G−1g˜ =c˜−c∗,
wecanuse(33)–(35)andHo¨lder’sinequalitytoclaimthatforsufficientlylargeN withprobability1−δitholds
∥c˜−c∗∥2
RR
≤2(cid:13) (cid:13) (cid:13)G−1(cid:13) (cid:13) (cid:13)2 RR→RR(cid:16) ∥g˜−g¯∥2
RR
+∥G−G˜∥2 RR→RR∥c˜∥2 RR(cid:17) ≤CN−1log2 1
δ
(36)
Moreover:
(cid:13)√ (cid:13)
E(u˜)−E(u+ )=(cid:13) (cid:13) A∗A(u˜−u+ )(cid:13) (cid:13)2
(cid:13) (cid:13)
L2
≤(cid:18)(cid:13)
(cid:13)
(cid:13)√
A∗A(u∗−u+
)(cid:13)
(cid:13) (cid:13)
+(cid:13)
(cid:13)
(cid:13)√ A∗A(u˜−u∗)(cid:13)
(cid:13) (cid:13)
(cid:19)2
(cid:13) (cid:13) (cid:13) (cid:13)
L2 L2
(cid:13)√ (cid:13) (cid:13)√ (cid:13)
≤2(cid:13) (cid:13) A∗A(u∗−u+ )(cid:13) (cid:13)2 +2(cid:13) (cid:13) A∗A(u˜−u∗)(cid:13) (cid:13)2
(cid:13) (cid:13) (cid:13) (cid:13)
L2 L2
(cid:13)√ (cid:13)
=2(cid:0) E(u∗)−E(u+ )(cid:1)+2(cid:13) (cid:13) A∗A(u˜−u∗)(cid:13) (cid:13)2
(cid:13) (cid:13)
L2
≤2(cid:0) E(u∗)−E(u+
)(cid:1)+2 (cid:88)R
|c∗ k−c˜
r|(cid:13)
(cid:13) (cid:13)
(cid:13)√
A∗Au
r(cid:13)
(cid:13) (cid:13)
(cid:13)
L2 2
r=1
(cid:13)√ (cid:13)
≤2(cid:0) E(u∗)−E(u+ )(cid:1)+2R∥c∗−c˜∥2 max(cid:13) (cid:13) A∗Au (cid:13) (cid:13)2
RR r (cid:13) r(cid:13) L2
(cid:13)√ (cid:13)
≤2(cid:0) E(u∗)−E(u+ )(cid:1)+2R(cid:13) (cid:13) A∗A(cid:13) (cid:13)2 C2∥c∗−c˜∥2 , (37)
(cid:13) (cid:13) L2→L2 R RR
Thestatementofthetheoremfollowsnowfrom(36)–(37).
115. Experimentalevaluation
5.1. Toyexample
InthissectionweatoyexampletodemonstratetheadvantageofMPregularizationandaggregation. Tothisend,
asanexplanatoryvariable,weconsiderarandomprocess
(cid:88)5
X(ω,t)= ξ (ω)cos(kt),t∈[0,2π],
k
k=0
whereξ (ω)arerandomvariablesuniformlydistributedon[−1,1]. ConsideralsotheresponsevariableY(ω)related
k
totheexplanatoryvariableX(ω,t)asfollows:
(cid:90) 2π (cid:90) 2π(cid:90) 2π
Y(ω)=u++ X(ω,t)u+ (t)dµ(t)+ X(ω,t)X(ω,τ)u+ (t,τ)dµ(t)dµ(τ).
0 1 2
0 0 0
(cid:16) (cid:17)
Inoursimulations,weuseu+ = u+,u+,u+ with
0 1 2
u+ =2,u+ =1+4cost+cos5t,u+ =cos3t+cos2tcos2τ.
0 1 2
WesimulateNindependentsamples(Y,X(·))of(Y(ω),X(ω,t))andusethemtoconstructtheregularizedquadratic
i i (cid:16) (cid:17)
approximation uN = (uN ,uN ,uN ) of u+ = u+,u+,u+ by MP regularization as described in Section 3, for 27
λ λ,0 λ,1 λ,2 0 1 2 (cid:110) (cid:111)
differentvaluesofλ,sothatallpossiblechoicesofλ ,λ ,λ ∈ 10−5,10−7,10−9 areconsidered.
0 1 2
(cid:13) (cid:13)
OnFigure1weplottheerror(cid:13) (cid:13)u+−uN(cid:13) (cid:13) againstthenumberoftheusedsamples N = 1,2,...,40,forthese27
λ L2
choicesofλ. Inseveralcases(e.g. λ = λ = 10−9,λ = 10−7)itisclearlyvisiblethatchoosingdifferentvaluesof
0 1 2
λ ,λ ,λ canbeadvantageouscomparedtothecaseofone-parameterregularizationλ = λ = λ . Wealsoobserve
0 1 2 0 1 2
thattheerrorcurvescorrespondingtoallthecomputedmodelssaturateatlowvalues(roughly∼3.14)forN ≥27.
Next, toseetheadvantageofcombiningallthe27computedmodelsintermsofanaggregationasdiscussedin
Section4,inFigure2weevenobservesaturationat∼3.14alreadyatN ≥21. Letusalsomention,thatweprovided
animplementationinPytorch[23]. ThisallowsthecodetoleverageGPUacceleration,enablingfastcomputationof
theinvolvedintegrals.
Theseresultslookpromising,thereforeinordertounderpintheusefulnessofourmethod,weshowexperiments
onrealworldmedicaldatainthenextsubsection.
5.2. StenosisData
Inthissection,wedemonstrateanapplicationofMP-PFRandtheassociatedaggregationapproachpresentedin
Sections 3, 4 to the problem of automatic stenosis detection from lumen diameters. Stenosis refers to an abnormal
narrowingofabloodvesselduetoalesion,leadingtoareductioninthelumen’sspace. Thispathologyisparticularly
critical in cervical arteries, including internal carotid arteries (ICA) and vertebral arteries (VA), where stenosis can
impede or block blood flow to the brain, significantly elevating the risk of a stroke. Consequently, the automatic
detectionofstenosisbecomesacrucialchallengeinneuroradiology.
Thisdetectionissuetypicallyarisesinthefinalorquantificationstageofcomputerizedtomography(CT)ormag-
netic resonance imaging (MRI) angiography, when the vessel lumen segmentation and centerline extraction have
already been executed. The detection mentioned above is the result of all work in the earlier stages, and therefore
deservesspecialattention. FollowingthesegmentationofCT/MRIscans,theexistingsoftwarefacilitatestheestima-
tionofvesselcross-sectiondiameters,denotedasd (s=1,2,...,approximately500),atvariouspositionst alongthe
s s
vesselcenterlines. Giventhevariabilityinpositionst andtheirtotalnumbersacrossdifferentpatients,itisnaturalto
s
organizethesedataintheformoffunctionsX(t).Forinstance,cubicinterpolationsplineswithknotsatt candescribe
s
thediametervariation,withthevaluesX(t )correspondingtod (s=1,2,...). Thisapproachallowsclinicaldatatobe
s s
representedasasamples(X,Y)consistingoffunctionalinputsX = X(t)(i=1,2,...,N)labeledbyoutputsY which
i i i i i
are assigned the value of 0 for a diagnosis indicating no stenosis, and values of 0.25, 0.5, 0.75, 1.0 for diagnoses
12representing light, medium, moderate, or high stenosis, respectively. With the use of this dataset, a predictor can
beconstructedtoautomaticallydetectthepresenceorabsenceofstenosisbyassigninganappropriatelabelY tothe
correspondingprofileX(t)ofvariationsinvesselcross-sectiondiameters.
We have permission for research-driven secondary use of anonymized clinical data collected at the Department
ofRadiologyandDepartmentofNeurology, MedicalUniversityofInnsbruck, withintheReSect-study[21]. Inour
experimentsbelow,weusethedataaboutN =40ICA.Theavailabledatasamplecontainsonly7arteriesaffectedby
stenosis,andweneedtoensuretheirinclusioninboththetrainingandtestsets. Toachievethis,weoptforarandom
train-testsplit,sothatthetrainingsetwillconsistentlycompriseofdataof16ICAwithoutstenosisand4ICAwith
stenosis,whilethetestsetwillconsistofdataof17non-stenosisarteriesand3stenosis-affectedones.
Recall that in the present context, the variables X(t) are functions of the position t along vessel centerlines, and
thelengthsofthatcenterlinesvaryfrompatienttopatient. Therefore,inordertocomputetheintegralsrequiredinthe
algorithmsofSections3and4,weconfinetheinputstoaspecificintervalI = [0,b],wherebistheminimumlength
observedintheavailableclinicaldata. Inourexperiments,weuseb = 140mm. Moreover,letusalsomentionthat
similartoSection5.1,alsohereweusePytorch[23]forourimplementations,sothatthecodeisGPU-compatibleand
allowsforfastcomputationoftheinvolvedintegrals.
Weconstructthemodelsu asdescribedin(22)–(23),forbothlinearandquadraticfunctionalregression,andfor
λ (cid:110) (cid:111)
allpossiblechoicesofλ ,λ ,λ ∈ 10−2,10−1,1 . Afterward,wecomputeanaggregationu˜ ofallu correspondingto
0 1 2 λ
thedifferentchoicesofλ,againbothforlinearandquadraticcase. HerebyweusetheapproachfromSection4,i.e.
wesolvethesystemassociatedto(27)–(28)andthencombinetheaggregationfunction(29). Foragivenfunctional
data sample X, the predicted value is then computed as f(X) = Au , or f(X) = Au˜, respectively. Note also that
i i i λ i i
whenacontinuous-valuedpredictor f(X )isusedasbinaryclassifier,itsdiagnosticabilitydependsontheso-called
j
discrimination threshold c, such that a particular artery corresponding to an input X is assumed to be affected by
i
stenosisif f(X)>c. Inourexperiments,wechoosec=0.5.
i
Itisknownthatinmedicalstatisticstheaccuracyofpredictionofthepresenceorabsenceofamedicalcondition
ismathematicallydescribedintermsofsensitivity(SE)andspecificity(SP).RecallthatSEiscalculatedas TP ,
TP+FN
whileSP = TN . Here, TPrepresentstheinstanceswhereastenosisintheexaminedarteryisidentifiedbyboth
TN+FP
the reference standard and the algorithm, irrespective of its severity (given the preventive measures for even mild
narrowing of the cervical artery). TN accounts for cases where no stenosis in the considered artery is detected by
boththereferencestandardandthealgorithm. Meanwhile,FNandFPdenotetherespectivecountsofcaseswherethe
algorithmincorrectlyidentifiestheabsenceorpresenceofstenosis.
Thediagnosticefficacyofaspecificclassifiercanalsobeeffectivelyevaluatedusingthereceiver/relativeoperating
characteristic(ROC)curve. Thisgraphicalrepresentationillustratesthediagnosticperformanceof f acrossvarying
discriminationthresholds. TheROCcurveisconstructedbyplottingthesensitivity(SE)againstthecomplementof
specificity(1−SP)fordifferentthresholdsettings.
The outcomes of ROC analysis can be succinctly summarized using a single metric, namely the area under the
ROC curve (AUC). The AUC ranges from approximately 0.5 for randomly assigned diagnoses to 1.0, indicating
perfectdiagnosticclassification. Inthesubsequentanalysis,wepresenttheperformanceoftheconsideredclassifiers
basedontestinputs,consideringalltheaforementionedmetricsandassumingthatallclassifiersusethesamethreshold
c=0.5.
OurresultsforthelinearandquadraticcasearedepictedinTables1and2. Wereporttheperformancemeasure
asanaverageover10runs,bothlinear,quadraticandaggregatedmodelsusethesamedatafortrainingandtestingin
eachrun,sothatafaircomparisonisprovided.
Wecanmakethefollowingimportantobservations:
1. MP regularisation leads to better results than the single parameter counterpart both for linear and quadratic
functionalregression.
2. Aggregation is a reliable strategy to address the issue of dealing with multiple regularisation parameters and,
especiallyinthequadraticcase,significantlyimprovesperformance.
3. Inthecontextofstenosisdetection, SEismoreimportantthanSP,becauseitislessdangeroustomisdetecta
pathologythantomisdetectitsabsence. Fromthisviewpoint,inthepresentstudytheaggregationdemonstrates
an ability to stabilize performance of linear functional regression. Moreover, the results reported in Tables 1
13Parameters SE SP AUC
λ =1,λ =1 0.333333 0.988235 0.915686
0 1
λ =1,λ =0.1 0.4 0.952941 0.833333
0 1
λ =1,λ =0.01 0.333333 0.911765 0.668627
0 1
λ =0.1,λ =1 0.333333 1 0.915686
0 1
λ =0.1,λ =0.1 0.4 0.958824 0.833333
0 1
λ =0.1,λ =0.01 0.333333 0.911765 0.668627
0 1
λ =0.01,λ =1 0.266667 1 0.933333
0 1
λ =0.01,λ =0.1 0.366667 0.964706 0.839216
0 1
λ =0.01,λ =0.01 0.333333 0.923529 0.67451
0 1
Aggregation 0.6 0.488235 0.641176
Table1:PerformancemetricsforlinearMP-FR,averagedover10runs.
and 2 for the aggregation clearly indicate that in terms of SE the quadratic approach outperforms its linear
counterpart,thatshouldnotbealwaysexpectedortakenforgranted(see,e.g.,[14]).
Let us conclude this section by comparing our results with some alternative approaches. The comprehensive
survey[15]offersathoroughexaminationofalgorithmsdesignedfordetectingstenosisbasedonvesselcross-section
diameters,utilizingthesameinputsasourconsideredmethods. Whilethealgorithmsdiscussedin[15]wereinitially
developedforcoronaryarterystenosisdetection,theyhavethepotentialapplicabilitytodiagnosestenosesinvarious
arterytypes,includingICA.
It seems that our algorithms demonstrates superior results compared to those reported in [15] (where the stated
valueswereSE = 0.55andSP = 0.33). Atthesametime,wewouldliketonotethattheapplicationofpolynomial
functional regression to the problem of automatic stenosis detection from lumen diameters has been presented here
forillustrationpurposes,andonemayexpectthatnonlinearandnon-polynomialfunctionalregressionmethodsmay
exhibitevenbetterperformance.
6. Acknowledgements
The research reported in this paper has been supported by the Federal Ministry for Climate Action, Environ-
ment, Energy, Mobility, Innovationand Technology (BMK), the Federal Ministry for Digitaland Economic Affairs
(BMDW),andtheProvinceofUpperAustriaintheframeoftheCOMET–CompetenceCentersforExcellentTech-
nologiesProgrammeandtheCOMETModuleS3AImanagedbytheAustrianResearchPromotionAgencyFFG.
The data used in Section 5.2 was acquired through the ReSect-study performed at the Medical University of
Innsbruck. ThisstudyisfundedbytheOeNBAnniversaryFund(15644).
Thisworkisadditionallyco-fundedbytheEuropeanUnion(ERC,SAMPDE,101041040). Viewsandopinions
expressed are however those of the authors only and do not necessarily reflect those of the European Union or the
EuropeanResearchCouncil. NeithertheEuropeanUnionnorthegrantingauthoritycanbeheldresponsibleforthem.
References
[1] Germa´nAneiros,IvanaHorova´,MarieHusˇkova´,andPhilippeVieu. Onfunctionaldataanalysisandrelatedtopics. JournalofMultivariate
Analysis,189:104861,2022.
[2] FrankBauerandSergeiPereverzev. Anutilizationofaroughapproximationofanoisecovariancewithintheframeworkofmulti-parameter
regularization.Int.J.Tomogr.Stat,4:1–12,2006.
[3] FrankBauer,SergeiPereverzyev,andLorenzoRosasco.Onregularizationalgorithmsinlearningtheory.Journalofcomplexity,23(1):52–72,
2007.
[4] MuratBelge, MishaEKilmer, andEricLMiller. Efficientdeterminationofmultipleregularizationparametersinageneralizedl-curve
framework.Inverseproblems,18(4):1161,2002.
[5] Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for learning from labeled and
unlabeledexamples.Journalofmachinelearningresearch,7(11),2006.
[6] AndreaCaponnettoandErnestoDeVito. Optimalratesfortheregularizedleast-squaresalgorithm. FoundationsofComputationalMathe-
matics,7(3):331–368,2007.
14Parameters SE SP AUC
λ =1,λ =1,λ =1 0.6 0.482353 0.54902
0 1 2
λ =1,λ =1,λ =0.1 0.6 0.488235 0.552941
0 1 2
λ =1,λ =1,λ =0.01 0.6 0.488235 0.552941
0 1 2
λ =1,λ =0.1,λ =1 0.633333 0.417647 0.492157
0 1 2
λ =1,λ =0.1,λ =0.1 0.6 0.482353 0.54902
0 1 2
λ =1,λ =0.1,λ =0.01 0.6 0.488235 0.552941
0 1 2
λ =1,λ =0.01,λ =1 0.766667 0.270588 0.4
0 1 2
λ =1,λ =0.01,λ =0.1 0.633333 0.411765 0.490196
0 1 2
λ =1,λ =0.01,λ =0.01 0.6 0.482353 0.54902
0 1 2
λ =0.1,λ =1,λ =1 0.6 0.482353 0.54902
0 1 2
λ =0.1,λ =1,λ =0.1 0.6 0.488235 0.552941
0 1 2
λ =0.1,λ =1,λ =0.01 0.6 0.488235 0.552941
0 1 2
λ =0.1,λ =0.1,λ =1 0.633333 0.417647 0.492157
0 1 2
λ =0.1,λ =0.1,λ =0.1 0.6 0.482353 0.54902
0 1 2
λ =0.1,λ =0.1,λ =0.01 0.6 0.488235 0.552941
0 1 2
λ =0.1,λ =0.01,λ =1 0.766667 0.270588 0.4
0 1 2
λ =0.1,λ =0.01,λ =0.1 0.633333 0.411765 0.490196
0 1 2
λ =0.1,λ =0.01,λ =0.01 0.6 0.482353 0.54902
0 1 2
λ =0.01,λ =1,λ =1 0.6 0.482353 0.54902
0 1 2
λ =0.01,λ =1,λ =0.1 0.6 0.488235 0.552941
0 1 2
λ =0.01,λ =1,λ =0.01 0.6 0.488235 0.552941
0 1 2
λ =0.01,λ =0.1,λ =1 0.633333 0.417647 0.492157
0 1 2
λ =0.01,λ =0.1,λ =0.1 0.6 0.482353 0.54902
0 1 2
λ =0.01,λ =0.1,λ =0.01 0.6 0.488235 0.552941
0 1 2
λ =0.01,λ =0.01,λ =1 0.766667 0.270588 0.4
0 1 2
λ =0.01,λ =0.01,λ =0.1 0.633333 0.411765 0.490196
0 1 2
λ =0.01,λ =0.01,λ =0.01 0.6 0.482353 0.54902
0 1 2
Aggregation 0.8 0.441176 0.756863
Table2:PerformancemetricsforquadraticMP-FR,averagedover10runs.
[7] JieyangChen,SergiyPereverzyevJr,andYueshengXu. Aggregationofregularizedsolutionsfrommultipleobservationmodels. Inverse
Problems,31(7):075005,2015.
[8] ZhongyingChen,YaoLu,YueshengXu,andHongqiYang. Multi-parametertikhonovregularizationforlinearill-posedoperatorequations.
Journalofcomputationalmathematics,pages37–55,2008.
[9] Marius-ConstantinDinu,MarkusHolzleitner,MaximilianBeck,HoanDucNguyen,AndreaHuber,HamidEghbal-Zadeh,BernhardMoser,
Sergei Pereverzyev, Sepp Hochreiter, and Werner Zellinger. Addressing parameter choice issues in unsupervised domain adaptation by
aggregation.In11thInternationalConferenceonLearningRepresentations,2023.
[10] ElkeRGizewski,LukasMayer,BernhardAMoser,DucHoanNguyen,SergiyPereverzyevJr,SergeiVPereverzyev,NataliaShepeleva,
andWernerZellinger. Onaregularizationofunsuperviseddomainadaptationinrkhs. AppliedandComputationalHarmonicAnalysis,
57:201–227,2022.
[11] Zheng-ChuGuo,Shao-BoLin,andDing-XuanZhou. Learningtheoryofdistributedspectralalgorithms. InverseProblems,33(7):074009,
2017.
[12] MarkusHolzleitnerandSergeiVPereverzyev.Onregularizedpolynomialfunctionalregression.toappearinJournalofComplexity,2024.
[13] MarkusHolzleitner,SergeiVPereverzyev,andWernerZellinger. Domaingeneralizationbyfunctionalregression. toappearinNumerical
FunctionalAnalysisandOptimization,2024.
[14] LajosHorva´thandRonReeder. Atestofsignificanceinfunctionalquadraticregression. BernoulliSocietyforMathematicalStatisticsand
Probability.,19(5A):2120–2151,2013.
[15] HAKiris¸li, MichielSchaap, CTMetz, ASDharampal, WillemBobMeijboom, Stella-LidaPapadopoulou, AdmirDedic, KoenNieman,
MichielAdeGraaf,MFLMeijs,etal. Standardizedevaluationframeworkforevaluatingcoronaryarterystenosisdetection,stenosisquan-
tificationandlumensegmentationalgorithmsincomputedtomographyangiography.Medicalimageanalysis,17(8):859–876,2013.
[16] PiotrKokoszkaandMatthewReimherr.Introductiontofunctionaldataanalysis.CRCpress,2017.
[17] Shao-BoLin,XinGuo,andDing-XuanZhou.Distributedlearningwithregularizedleastsquares.TheJournalofMachineLearningResearch,
18(1):3202–3232,2017.
15[18] ShuaiLu,PeterMathe´,andSergeiVPereverzyev. Balancingprincipleinsupervisedlearningforageneralregularizationscheme. Applied
andComputationalHarmonicAnalysis,48(1):123–148,2020.
[19] ShuaiLuandSergeiVPereverzev.Multi-parameterregularizationanditsnumericalrealization.NumerischeMathematik,118:1–31,2011.
[20] ShuaiLuandSergeiVPereverzyev.Regularizationtheoryforill-posedproblems:selectedtopics,volume58.WalterdeGruyter,2013.
[21] LukasMayer,ChristianBoehme,ThomasToell,BenjaminDejakum,JohannWilleit,ChristophSchmidauer,KlausBerek,ChristianSieden-
topf,ElkeRuthGizewski,GudrunRatzinger,etal.Localsignsandsymptomsinspontaneouscervicalarterydissection:asinglecentrecohort
study.JournalofStroke,21(1):112,2019.
[22] Hans-GeorgMu¨llerandFangYao.Additivemodellingoffunctionalgradients.Biometrika,97(4):791–805,2010.
[23] A.Paszke,S.Gross,F.Massa,A.Lerer,J.Bradbury,G.Chanan,T.Killeen,Z.Lin,N.Gimelshein,L.Antiga,etal. Pytorch:Animperative
style,high-performancedeeplearninglibrary.arXivpreprintarXiv:1912.01703,2019.
[24] SergeiPereverzyev.AnIntroductiontoArtificialIntelligenceBasedonReproducingKernelHilbertSpaces.SpringerNature,2022.
[25] JamesORamsay.Whenthedataarefunctions.Psychometrika,47:379–396,1982.
[26] JamesORamsayandC.J.Dalzell. Sometoolsforfunctionaldataanalysis. JournaloftheRoyalStatisticalSocietySeriesB:Statistical
Methodology,53(3):539–561,1991.
[27] JamesORamsayandBernardWSilverman.Appliedfunctionaldataanalysis:methodsandcasestudies.Springer,2002.
[28] PhilipTReiss,JeffGoldsmith,HanLinShang,andRToddOgden. Methodsforscalar-on-functionregression. InternationalStatistical
Review,85(2):228–249,2017.
[29] ZhangTao,ZhangQingzhao,andWangQihua. Modeldetectionforfunctionalpolynomialregression. ComputationalStatisticsandData
Analysis,70(4):83–197,2014.
[30] HongzhiTong.Distributedleastsquarespredictionforfunctionallinearregression.InverseProblems,38(2):025002,2021.
[31] HongzhiTongandMichaelNg.Analysisofregularizedleastsquaresforfunctionallinearregressionmodel.JournalofComplexity,49:85–94,
2018.
[32] Jane-Ling Wang, Jeng-Min Chiou, and Hans-Georg Mu¨ller. Functional data analysis. Annual Review of Statistics and its application,
3:257–295,2016.
[33] PeiliangXu,YoichiFukuda,andYumeiLiu.Multipleparameterregularization:numericalsolutionsandapplicationstothedeterminationof
geopotentialfromprecisesatelliteorbits.JournalofGeodesy,80:17–27,2006.
[34] MingYuanandTTonyCai. Areproducingkernelhilbertspaceapproachtofunctionallinearregression. AnnalsofStatistics,6(38):3412–
3444,2010.
[35] VadimYurinsky.SumsandGaussianVectors.LectureNotesinMathematics.SpringerBerlin,Heidelberg,1995.
16lam_0=1e-05, lam_1=1e-05, lam_2=1e-05 lam_0=1e-05, lam_1=1e-05, lam_2=1e-07 lam_0=1e-05, lam_1=1e-05, lam_2=1e-09
22.5 40
9 20.0 35
8 17.5 30
7 15.0 25
6 12.5 20
45 1 570 ... 050 11 505
3 2.5
0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 35 40
lam_0=1e-05, lam_1=1e-07, lam_2=1e-05 lam_0=1e-05, lam_1=1e-07, lam_2=1e-07 lam_0=1e-05, lam_1=1e-07, lam_2=1e-09
22.5
12 9 20.0
10 8 17.5
7 15.0
8 6 12.5
10.0
6 5 7.5
4 4 5.0
3 2.5
0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 35 40
lam_0=1e-05, lam_1=1e-09, lam_2=1e-05 lam_0=1e-05, lam_1=1e-09, lam_2=1e-07 lam_0=1e-05, lam_1=1e-09, lam_2=1e-09
70 14
9
60 12
8
45 00 10 7
30 8 6
20 6 5
10 4 4
0 3
0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 35 40
lam_0=1e-07, lam_1=1e-05, lam_2=1e-05 lam_0=1e-07, lam_1=1e-05, lam_2=1e-07 lam_0=1e-07, lam_1=1e-05, lam_2=1e-09
25
12 50
20
10 40
8 15 30
6 10 20
4 5 10
0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 35 40
lam_0=1e-07, lam_1=1e-07, lam_2=1e-05 lam_0=1e-07, lam_1=1e-07, lam_2=1e-07 lam_0=1e-07, lam_1=1e-07, lam_2=1e-09
22.5
10 9 20.0
9 8 17.5
8 7 15.0
67 6 11 02 .. 05
5 5 7.5
4 4 5.0
3 3 2.5
0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 35 40
lam_0=1e-07, lam_1=1e-09, lam_2=1e-05 lam_0=1e-07, lam_1=1e-09, lam_2=1e-07 lam_0=1e-07, lam_1=1e-09, lam_2=1e-09
30 12 9
25 8
10
20 7
15 8 6
10 6 5
5 4 4
3
0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 35 40
lam_0=1e-09, lam_1=1e-05, lam_2=1e-05 lam_0=1e-09, lam_1=1e-05, lam_2=1e-07 lam_0=1e-09, lam_1=1e-05, lam_2=1e-09
18 25 70
11 46 20 60
50
12
15 40
10
8 10 30
6 20
4 5 10
0
0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 35 40
lam_0=1e-09, lam_1=1e-07, lam_2=1e-05 lam_0=1e-09, lam_1=1e-07, lam_2=1e-07 lam_0=1e-09, lam_1=1e-07, lam_2=1e-09
25
16 12
14 20
12 10
10 8 15
8 6 10
6
4 4 5
0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 35 40
lam_0=1e-09, lam_1=1e-09, lam_2=1e-05 lam_0=1e-09, lam_1=1e-09, lam_2=1e-07 lam_0=1e-09, lam_1=1e-09, lam_2=1e-09
18 10 9
11 46 89 8
12 7 7
10 6 6
8 5 5
6 4 4
4
3 3
0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 35 40 0 5 10 15 20 25 30 35 40
Figure1:Errorcurvesforallpossiblechoicesofλ.Redlinedepictserrorrateof3.14
179
8
7
6
5
4
3
0 5 10 15 20 25 30 35 40
Figure2:Errorcurveforaggregation.Redlinedepictserrorrateof3.14
18