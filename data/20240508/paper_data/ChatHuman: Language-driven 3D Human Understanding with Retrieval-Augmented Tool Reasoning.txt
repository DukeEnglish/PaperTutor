ChatHuman:
Language-driven 3D Human Understanding with
Retrieval-Augmented Tool Reasoning
Jing Lin3,4,⋆, Yao Feng1,2,3,⋆,†, Weiyang Liu1,5, and Michael J. Black1
1Max Planck Institute for Intelligent Systems-Tübingen 2ETH Zürich
3Meshcapade 4Tsinghua University 5University of Cambridge
chathuman.github.io
Abstract. Numerous methods have been proposed to detect, estimate,
and analyze properties of people in images, including the estimation of
3D pose, shape, contact, human-object interaction, emotion, and more.
Eachofthesemethodsworksinisolationinsteadofsynergistically.Here
weaddressthisproblemandbuildalanguage-drivenhumanunderstand-
ing system – ChatHuman, which combines and integrates the skills of
manydifferentmethods.Todoso,wefinetuneaLargeLanguageModel
(LLM) to select and use a wide variety of existing tools in response to
user inputs. In doing so, ChatHuman is able to combine information
from multiple tools to solve problems more accurately than the individ-
ualtoolsthemselvesandtoleveragetooloutputtoimproveitsabilityto
reasonabouthumans.ThenovelfeaturesofChatHumanincludeleverag-
ing academic publications to guide the application of 3D human-related
tools,employingaretrieval-augmentedgenerationmodeltogeneratein-
context-learningexamplesforhandlingnewtools,anddiscriminatingand
integratingtoolresultstoenhance3Dhumanunderstanding.Ourexper-
iments showthat ChatHuman outperformsexisting modelsin both tool
selection accuracy and performance across multiple 3D human-related
tasks. ChatHuman is a step towards consolidating diverse methods for
humananalysisintoasingle,powerful,systemfor3Dhumanreasoning.
1 Introduction
Humansaredistinguishedfromotheranimalsbytheirextensivecreationanduse
of tools. We exploit tools to solve specialized tasks and integrate these solutions
into our general understanding of the world. Recent work on Large Language
Models(LLMs)hasshownthatsuchmodelscanalsolearntousetoolstoextend
their skills. Inspired by this, we develop ChatHuman, a multi-modal LLM that
is specialized for understanding humans and their 3D behavior. ChatHuman
learns to exploit a wide range of specialized human-related models for tasks
such as 3D pose estimation, emotion recognition, reasoning about contact, and
more.WeintroducenoveltrainingmethodsandshowthatChatHumanisableto
navigate and exploit multiple tools to achieve state-of-the-art results on various
tasks regarding 3D human understanding. The power of ChatHuman makes a
⋆Equal contribution. †Corresponding author.
4202
yaM
7
]VC.sc[
1v33540.5042:viXra2 J. Lin, Y. Feng, W. Liu and M. Black
Reasoning-based Pose Estimation Tools Body Shape Measurement
Themaninthedarkblueshirt Please estimate the height,
i f 3s i dnp d hr ue h mp ima ar nin o pg u ot st eo a .nsi dtd eo sw tin m. aP telea hs ie s LLaVA HMR 2.0 DECA PoseSciptBC EL DIF LAF M- Sheis1.6w of 3ei tg mhh et tp aa e ln lr ,d so ab n n.o ddy
w
em ige ha ssu 6r 0em .6en kt
g.
… man Herchest,waist,andhipscircumference
are0.91m,0.71m,and0.97m.
DECO GD SAM SD Pleaserecommendaclothessizeforher.
Her clothes size would be Middle or
Large,dependingonherwearinghabits.
Human-Object Interaction Understanding DT oo co sl Too Tl- rC ao nn sfd oit ri moned Human Image Editing
Pleasedescribethehuman- Thereisamanintheimage,
objectinteractionregionsof couldyoureplacehimwitha
thepersonintheimage. ? + supermanfacingtocamera?
Thepersoniscrouchingwithhis QU us ee rr y LLM Response
lefthandtouchingtheballonthe
groundandhisrighthandholding Sure.
atennisracket.Bothfeetarein
contactwiththeground. ChatHuman
Fig.1: ChatHuman is a novel system that uses a multimodal LLM to exploit tools,
discriminatetheirresults,andintegratetheresultstosolvetasksfocusedon3Dhumans.
step closer towards a foundation model for holistic human understanding with
applications in domains like AR/VR [26,67], games [26,60], and fashion [14,68].
Research on 3D humans has progressed rapidly in recent years, resulting in
the creation of many tools that can perform tasks like estimating a human’s
3D pose from a single image [16,22,24,30], predicting face/body shapes [5,
12], capturing emotions [6,12], and identifying regions of touch/contact [42,
63]. Each of these tools, however, focuses on a specific problem; that is, they
are all “specialists”. Moreover, each of these separate tools cannot benefit from
the expertise of the others. Combining these tools to solve more complex tasks
requires significant domain expertise. To address complex, real-world, tasks, we
would like a “generalist” model that can solve a wide range of problems with
equal or better accuracy than the specialists. Such a system should be able to
useawiderangeofspecialisttools,knowhowtoapplythemtoappropriatetasks,
and be able to synthesize the results of several tools to solve new problems. For
example, reasoning-based pose estimation (Figure 1) can be addressed through
the combination of text-guided detection (LLaVA [34]), cropping, and human
pose estimation tools (HMR2 [16]), rather than relying on a single tool alone.
ChatHumanprovidesoneoftheearliestsolutionstothisproblembyfinetun-
ing a LLM to serve as an agent to call the appropriate tools in response to user
inputs,therebycompletingreal-worldtasksandenhancingitsresponsestousers
with the results from the tools. Sharing a similar spirit, some recent approaches
haveutilizedoff-the-shelforfinetunedLLMstosolvebasicvisionproblems(e.g.,
Visual ChatGPT [51]), to combine mobile applications (e.g., AppAgent [72]),
and to understand complex visual inputs (e.g., GPT4Tools [71]). In contrast to
these works, we focus on the challenging task of 3D human understanding. This
is challenging because there are many disparate tools for analyzing different as-
pects of humans in images. Our model has to select the appropriate tool, for a
givenuserquery,usethetoolcorrectly,andcombinetheoutputwithitsbroader
knowledge to respond to the user. The tools themselves produce a variety of
output formats including images, text, and 3D parametric meshes.
We draw inspiration from how humans use tools: 1) we read related papers
to understand the tools, which helps us know when and how to use them; 2)ChatHuman 3
afterusingthetool,weevaluatewhethertheoutcomeisreliable,orcomparethe
results of several methods to determine which is most trustworthy; and 3) we
then incorporate these results to form our response. We apply the same steps to
our LLM-based model, resulting in a novel approach with several contributions:
Paper-based Retrieval-Augmented Tool Usage:Detailsofamodelare
described in a research paper. We give the LLM access to various parts of these
papers and show that “reading the paper” improves tool use performance. Fur-
thermore,weanalyzewhichsectionsofthepaperaremostvaluableforinstruct-
ing tool use. Additionally, when encountering a tool for the first time, people
often turn to the user guide for assistance. We compile documentation for these
toolsandutilizeapaper-basedRetrieval-AugmentedGeneration(RAG)mecha-
nismtoimproveLLM’sunderstandingandmanagementofnewtools.Thismeans
thatalthoughtheLLMhasnotencounteredsuchtoolsduringfine-tuning,itcan
still effectively use the tools with the aid of the paper-based RAG model.
Tool Result Discrimination and Integration: After using the tools, an-
alyzing the outcomes is crucial. In scenarios with various outcomes, the LLM
shoulddiscernthemostreliableresult.InspiredbyCognitiveLoadTheory [59],
which posits that choosing from options is faster and demands less cognitive ef-
fort than creating detailed explanations, we’ve designed a method to streamline
theLLM’sdecision-makingprocess.Wepresenttheresultsfromvarioustoolsas
multiple-choicequestions,makingiteasierfortheLLMtoevaluateandselectthe
best outcome, thus enhancing the tool’s output. These results, combined with
theLLM’sgeneralknowledge,areusedtogenerateresponsesabout3Dhumans.
Specifically, ChatHuman consists of a multi-modal LLM, and 22 human-related
tools.TheLLMisfinetunedfortasksthatinvolveusingthesetoolsandincorpo-
ratingtheirresults.Userrequestscanbeintheformoftextdescriptions,images
or other 3D information (if applicable), and the model produces text descrip-
tions, images, or other 3D outputs after tool reasoning. Extensive evaluations
demonstrate that ChatHuman not only surpasses previous models in tool usage
accuracy but also enhances performance on a variety of human-related tasks.
Tosummarize,ourkeycontributionsinclude:(1)anewframeworkthatlever-
ages LLMs to address tasks related to 3D human understanding with human-
centrictools;(2)ascientific-paper-basedRAGmechanism,whichensuresprecise
tool usage by comprehending tool descriptions from scholarly articles, enhanc-
ing tool application and contextual understanding; and (3) the integration of
tool outcomes with LLMs, where the LLM is finetuned to discriminate between
good and bad tool results to provide more accurate results than any single tool.
ChatHuman achieves superior performance in tool usage and human-related
tasks compared to other LLM-based methods or task-specific work. We will
make available the code, trained models, and datasets for research purposes.
2 Related work
3D Humans: There is an extensive literature on the analysis of 3D humans,
which we only sample here. Reasoning about 3D humans typically leverages
parametric models like SMPL [38], SMPLX [45], or GHUM [69] for the body,4 J. Lin, Y. Feng, W. Liu and M. Black
BFM[20]orFLAME[29]forfaces,andMANO[53]forhands.Thesemodelsen-
able the representation of the human body, face, and hands as low-dimensional
vectors, facilitating subsequent applications in estimation and generation. Es-
timation of human pose and shape either relies on optimization-based meth-
ods [2,21] or regression-based methods [5,11,16,22,24,28,30,31,54,74], which
estimate SMPL shape and pose parameters from a single image. Similarly, face
reconstruction methods [9,12,61] estimate shape and expression parameters of
thefacemodelfromsingleimages.Theanalysisofcontact,vitalforunderstand-
ing human-environment interaction and social properties, has seen innovations
in methods [17,42,63]. In the generation domain, PoseScript [7] and PoseFix [8]
offer methods for synthesizing and correcting 3D human poses from text de-
scriptions. Recent language-to-3D generation methods [3,18] create 3D human
shapes.Forunderstanding,therearestudiesfocusingonclassifyingactionlabels
in video sequences [44,49] or recognizing human emotions [6,65], enhancing our
comprehension of human behavior. These basic methods excel in their respec-
tive scenarios. Recently, ChatPose [13] unifies pose generation, estimation, and
LLM’s general understanding into one model, but is still limited in full-body
pose tasks. In contrast, our model leverages and integrates the performance of
22 3D human-related tasks into a single, LLM-based model. We emphasize the
significance of ChatHuman, as it enables non-experts to solve real-world tasks
by invoking appropriate tools and adding an extra layer of language-driven un-
derstanding that effectively leverages the output of different tools.
Large Language Models and Tool Use:ToexpandthecapabilitiesofLLMs
without expensive retraining, recent work has focused on enabling them to use
specializedoff-the-shelftools.Inthislineofwork,alibraryoftoolsisconstructed
and LLMs act as the planner to navigate the use of the tools. Differnt types of
toolshavebeenadopted,e.g.,visionmodules[51,58,73],mobileapplications[72],
community tools [55] and system tools [71]. However, general-purpose LLMs
often lack a deep understanding of specific tools, especially those containing
domain knowledge. To address this, a few recent works [10,25,66] propose to
fintune general-purpose LLMs (e.g., LLaMA [62], LLaVA [35,36]) with domain-
specific tool use data. Distinct from previous works, ChatHuman focuses on
understanding3Dhumansthroughlanguageinteractionbyleveragingthepower
of off-the-shelf human-related tools.
RetrievalAugmentedGeneration:RAG[15,27,75]isatechniquetoenhance
generative tasks by retrieving relevant information from external databases, al-
lowing for continuous knowledge updates. Based on this feature, we design a
RAG mechanism to facilitate the usage of newly introduced, unseen tools.
3 Language-driven 3D Human Understanding
ThegoalofChatHumanistoleveragethecapabilitiesofLLMsfortasksregard-
ing 3D human understanding. The model is designed to interact with users and
deal with various user requests through input like text, image, or any informa-
tion about 3D humans. This includes details on how a person is posed (using
SMPL [38] pose parameters), their emotions (using FLAME [29] expressions),ChatHuman 5
User Input ChatHuman Response
Text
Text Multimodal LLM-based Agent 𝑓 !
𝑋 $ 𝑌(,𝑌( 𝑌 &
& %
Image Paper-based Tool-Conditioned Image
RAG Transform Ψ
𝑋 % Tool Docs 𝑇! 𝑌 ’ 𝑌 %
ToolParam 𝑌!""# Tools
Thought: Do I need to use a …
Others pose…expr t Ao co tl i? o nY :e s < tool name> Others pose…expr
𝑋 ’ A Oc bt si eo rn v aI tn ip ou nt :: << oa ur tg pu um t e in mt as g> e > 𝑓 #! 𝑓 #" 𝑓 ## 𝑌 ’
Fig.2:MethodOverview.Givenauserquery,themultimodalLLM-basedagentadopts
apaper-basedRAGmechanismtodeterminewhethertoemploytoolsandidentifythe
optimalwaytoutilizethem.Afterapplyingthetools,thetoolresultsaretransformed
into a text or visual format and fed back to the agent to formulate responses.
and how they touch or interact with the environment (contact), etc. Figure 2
gives an overview of the proposed ChatHuman model. We start by elaborating
the architecture design and the detailed training strategy (Section 3.1). This
includes the introduction of the paper-based retrieval-augmented generation for
accurate tool usage (Section 3.2) and a feedback mechanism aimed at enhanc-
ing the LLM’s responses to human-related questions (Section 3.3). Finally, we
provide the details of our training data construction by employing GPT-4V [43]
with a range of datasets associated with different tools.
3.1 Overall Pipeline
We refer to our ChatHuman model as Φ, which consists of a multi-modal LLM
f (·), along with a set of 3D human-related functions f (·),f (·),.... These
ϕ θ1 θ2
functionsserveastoolsforvarioustasks,suchas3Dhumanposeestimation,pose
generation, and 3D face reconstruction. Our model takes input text queries X ,
q
imagesX ,orX representingother3Dhuman-relatedmodalities.Forinstance,
v m
X could represent 3D human poses encoded as SMPL parameters. Then it
m
produces outputs in the form of text Y , images Y , or modalities Y related to
t v m
3D humans, as described by Y ,Y ,Y = Φ(X ,X ,X ). For training, we keep
t v m q v m
allthetoolfunctionsfixed,andonlyfinetunetheLLMf (·).Specifically,weuse
ϕ
LoRA [19] to finetune the LLM, where the trainable parameters are denoted as
ϕ . With the ground truth textual output Yˆ, we optimize the model using
lora t
the following objective function: L = CE(Yˆ,Y ), where CE denotes the cross-
t t
entropy loss. In the following sections, we introduce the details of the model
function Φ and the construction of our training data.
3.2 Paper-based Retrieval-Augmented Tool Reasoning
Teaching LLMs to accurately discern when and how to utilize tools presents
a significant challenge. A straightforward approach [51,71] might involve sum-
marizing the usage scenarios and input arguments of each tool within the input6 J. Lin, Y. Feng, W. Liu and M. Black
promptoftheLLMf ϕ(·),formulatedasYtool =f ϕ(X q,X t),whereX
t
represents
the tool definitions. But this often falls short for specialized tools, particularly
whenthereexistnumerousprofessionaltoolsrelatedto3Dhumantasks.Firstly,
some tools have various usage scenarios and require background knowledge for
proper utilization. For instance, for HMR tool [16], relevant inquiries might in-
clude “Can you estimate the pose of this person?”, “What are the SMPL param-
eters?”, or “I want to get the 3D mesh of this person.” It is difficult to concisely
summarize all potential application scenarios and requisite background knowl-
edge in a few sentences within the prompt. Furthermore, as the number of tools
grows, the descriptions of these tools in the prompt get longer and more com-
plex. This makes it harder for the LLM to use different tools, especially new
ones for which it was not trained.
To solve these problems, we introduce a paper-based Retrieval-augmented
Generation (RAG) mechanism to facilitate tool usage. As illustrated in Figure
3, we first input the academic papers associated with each tool to GPT-4 and
promptittorearticulatethetoolfunctionsandenumeratepotentialuserqueries
for tool activation. Academic papers, with their extensive background, detailed
instructions, and varied applications, enable the generation of user queries cov-
ering a wide range of application scenarios. By combining these queries with
the structured arguments of each tool, we compile a document consisting of
question-answering pairs about tool operation for each tool. Figure 3 presents
oneexamplefromthetooldocument.ThesedocumentsX serveasanauxiliary
d
knowledge base during inference, from which we retrieve a relevant example X
e
inresponsetoauserqueryX .Theretrievalprocessisimplementedbymatching
q
the text embedding of the query and those within the tool documents using a
pretrainedtextembeddingmodel[57].Theretrievedsampleisthenmergedwith
the user query and provided to the agent f as an in-context learning example,
ϕ
X
e
=f r(X q,X d), Ytool =f ϕ(X q,X e,X t), (1)
wheref
r
representstheretrievalfunctionandYtool isatextualdescriptionspec-
ifying whether to use a tool or not, tool names, and input arguments for tool
functioncalling.Thistextualdescription,alongsidethetextquery,vision,orin-
puts from other modalities, enables the tool function f to compute the results,
θ
which is denoted by Y
m
=f θ(Ytool,X q,X v,X m).
3.3 Tool Result Discrimination and Integration
Afterusingthetools,it’simportanttointegratetheoutcomestosolveproblems.
However, outputs from various tools come in different forms (e.g., language,
images, vectors like SMPL poses), some of which might be difficult for current
multimodal LLMs to understand, e.g. LLaVA [34] only accepts text and image
as input. To leverage these results and enhance the LLMs’ understanding of
3D humans, we introduce a tool-conditioned transformation Ψ(·) that converts
tool outcomes Y into textual or visual formats. For example, we transform
m
the vertex-wise contact label predicted by DECO [63] into a body part-levelChatHuman 7
P pole sa ese ofe ts ht eim wat oe mat nhe whh oan id s LLM-based Agent𝑓 ! T toh oo lu ?g Yh et s: Do I need to use a
holdingayellowplate.
images/ismaaguc.png Action: Described Person Hand
Pose Estimation
User Query 𝑋#
Retriever 𝑓 " U ims ae gr e I s/n jp ku st d: j fgh.png, Help me predict the hand pose I wn op mut a: n im hoa lg de ins g/ is am ya ea lg lou wc. p pn lag t, e the
of the boy who is catching a bottle. Textual Response 𝑌$%%&
Response:
Thought: Do I need to use a tool? Yes
Action: Described Person Hand Pose Estimation
Input: images/jksdjfgh.png, boy catching a bottle
Tool Papers GPT-4 Tool Documents 𝑋! Retrieved Example 𝑋"
Fig.3:Paper-basedRetrieval-AugmentedToolUsage.Wefirstfeedtheacademicpapers
of each tool to GPT-4 and build a document for each tool. During inference, given a
user query, a relevant sample is retrieved from the documents and provided to the
LLM-based agent as an in-context example to improve the tool usage accuracy.
descriptionbasedonthevertex-to-partmappingdictionaryofSMPL[38],andwe
render the mesh generated by PoseScipt [7] into an RGB image using rendering
techniques. After that, the transformed results will be merged with the user
query as a clue to help the agent generate a response:
′ ′ ′ ′
Y =f (X ,Y ,Y ), (Y ,Y )=Ψ(Y ). (2)
t ϕ q t v t v m
In scenarios where multiple tools can address a user’s request, leading to
various potential outcomes, we aim for the LLM to identify the most accurate
result. To achieve this, we present the outcomes as multiple-choice questions,
prompting the model to select the most relevant answer to the user query:
(Y′,Y′)=f (X ,Ψ(Y ),Ψ(Y ),...), (3)
t v ϕ d m1 m2
where Y denotes the i-th tool result, X is a prompt specifying the discrimi-
mi d
nation rule. Details of the prompt are available in Appendix.
3.4 Training Data Construction
Tool Usage Instruction-following Data.Givenauserquery,theLLM-based
agentaimstoselectappropriatetoolsandformulatetheargumentsfortheirop-
eration.FollowingGPT4Tools[71],weprovideGPT-4[43]withanimagecontent
X and a tool-related prompt P to compile the instruction-following data (see
I T
Figure4(a)).TheimagecontentX consistsofatextualdescriptionofanimage,
I
detailing captions and object locations. The tool-related prompt includes a sys-
tem message, and a description of tools delineated as "<tool name>: <usage
scenario>, <arguments>". As proposed in Section 3.2, we further incorpo-
rate the paper content X into GPT-4 to generate the tool usage instruction-
P
following data I , formulated as I ∼ f (P |X ,X ), where f (·) denotes the
T T g T I P g
GPT-4 model. I is a question-answering data pair consisting of a user query
T
X
q
and a response Yˆ
t
specifying whether to use a tool Yˆ thought, the tool name
Yˆ act, and the tool input arguments Yˆ args.8 J. Lin, Y. Feng, W. Liu and M. Black
Tool Description Tool Papers Image Content Tool Results Human GT Labels
<tool name>:
<usage scenario>, <arguments>
DECA HMR 2.0 HAMER
c a ea xp tb et a nio s den e: d.A witc hatc hh ie sr gs lq ou va et dtin hg ana dt H W We e aig i ig sh th t 9t 1 977 58 H W We e aig i ig sh th t 5t 1 832 08
HMR 2.0 DECA Pos …eScipt … p be ar ss eo bn a:
l
l[ :0 [. 03 .4 6, 10
,
. 01 .9 5, 00
,
. 07 .6 7, 60
,
. 09 .3 6]
6]
H Ci hp es s 1 t 0 91 0 H Ci hp es s 6 t 5 60
DECO HAMER DECO SMPL Body Shape Human Pose HOI Regions Body Shape Human Pose HOI Regions
GPT-4 GPT-4V
(a) Tool Usage Instructions (b) Tool Feedback Discrimination Instructions (c) Tool Feedback Integration Instructions
System Prompt.
PH H Dr u eu e smm f ci aa x rn in p:P: t r P ioo r nm o :v p i <t d. i me aa gn e im caa pg te io nn >amed <image path> S H < ADy u ss i sm st ie c sam r tn ai mP : n r i t< no :I am n t <p s Rit ot e. r n su pRc ot ui nlo e sn e>> > <Tool result> S H Ay u ss smt ie sam tn a P : n r t<o :Im n <p s Rtt e. r su pc oti no sn e> > <Tool result>
A E N S A Tux heI sfa: s ow f iim uR sx gti pe n P a hlc pe r n te ou: :ti mtv :< D: e pr oe <d t .ut. I r s i e ne r ev ee in dd p tue otx >a um sep le a> tool? Yes Image S H e m w a shy s cu hes t acm s it i pc ue m ha h erm )a an at o tP : oi n en for d leo nP ym tl T oe hrp ro fa eet eo. s s ptle u hlhB il ucetc mas (o t tr am e w eo nd sfp oa fm tT mr ih gee o e e uo s s rhlt h p e)h e. oAe s s iA nes( mp b s a too el hu nrs se e ee ds Image S H o r r Ae ify u sgs g sm ht i ie t o t sam h n tn fe s aoP : n or d tm to D :e ,m a e t ln Hsp e ect ic f. sr t tii n eb rhde igat hnht b tdeh y ,e hri atm igH noa hO o dg tlI se ih s.c aao inr nn deTt .: ca h oc e l net tf ac tr o ce n tg ft oi a wo ocn it tts , h
Action: <tool name> originalimage,takingintoaccount thehandleofacookingtool,whilehisleft
Action Input: <arguments> theiralignmentandresemblance. handisincontactwithabottle.Hisleftand
Observation: <tool output> Assistant: Tool B’s result is better. rightfeetareincontactwiththeground.
Fig.4:Illustrationofourintruction-followingdataconstructionpipeline.Weconstruct
the instruct-following data about tool usage and feedback by providing GPT-4 with
multiple tool-related information, image content, and ground truth labels. Gray text
shows some examples of the instruction.
Tool Feedback Instruction-following Data. After calling the tools, the
LLM-based agent should be able to discriminate and integrate the tools’ out-
comes. To achieve this, we construct instruction-following data based on the
tool results and corresponding ground truth labels. Specifically, we transform
the tool results and ground truth human labels into textual or visual formats
using specialized techniques or rendering tools. Subsequently, we feed GPT-4V
with the image content X , the transformed tool feedback X , and the ground
I T
truth label X to generate the instruction-following data about tool feedback,
L
denoted as I ∼f (P |X ,X ,X ), where P is a system prompt, I consists
F g F I T L F F
of an instruction X and a corresponding answer Yˆ′. We instruct GPT-4V to
q t
curate two types of data. The first as illustrated in Figure 4(b), is about iden-
tifying the most suitable tool response. The second (Figure 4(c)) focuses on the
integration of tool results, featuring a query posed by a user, the tool’s output
serving as a hint, and the assistant’s reply.
Multi-Modal Instruction-following Data. To preserve the LLM’s inher-
ent capability for multi-turn conversations, we also incorporate the multi-modal
instruction-following data I from LLaVA [34] during training.
M
Finally, We consolidate the tool usage instruction-following data I , tool
T
feedback instruction-following data I , and multi-modal instruction-following
F
data I into a unified format consistent with LLaVA [34]:
M
Human:Xinstruct<STOP> Assistant:Y t<STOP>, (4)
where <STOP> is sequence termination token and Xinstruct is input instruction.
We set Xinstruct as X
q
for a language-only instruction, and as X
q
<\n>X
v
for
a multimodal instruction that incorporates both text and visual elements.ChatHuman 9
SeenTools UnseenTools
Method SR SR SR SR IoU SR SR SR SR IoU
t act args t act args
GPT4Tools[70] 0.609 0.547 0.525 0.520 0.566 0.612 0.546 0.542 0.525 0.573
GPT4Tools-FT[70] 0.825 0.710 0.687 0.690 0.741 0.904 0.807 0.690 0.747 0.800
VisualChatGPT-3.5[51] 0.498 0.319 0.237 0.251 0.791 0.507 0.314 0.226 0.293 0.803
VisualChatGPT-4[51] 0.892 0.802 0.715 0.753 0.797 0.998 0.913 0.801 0.872 0.907
ChatHuman 1.000 0.974 0.950 0.9700.975 0.999 0.967 0.893 0.954 0.953
Table 1: Comparison of tool usage accuracy. Successful rate of thought (SRt), action
(SRact), arguments (SRargs), execution (SR), and IoU are reported.
4 Experiments and Results
4.1 Implementation Details
We employ LLaVA-1.5V [34] as the multimodal LLM backbone, with CLIP [48]
for vision encoding and Vicuna [4] for the LLM backbone. For retrieval, we
adopt INSTRUCTOR [57] for text embedding and utilize the vector similarity
searchingalgorithmfromChromatoidentifyarelevantexample.Topreservethe
generalization ability of the pre-trained multi-modal LLM, we leverage LoRA
[19] to perform efficient fine-tuning, with a rank of 128 and an alpha of 256.
Alternatively,orthogonalfinetuning[37,47]canbeusedtoimproveperformance.
For optimization, we use AdamW [39], with a learning rate and weight decay
set to 2e-4 and 0, respectively. All models are fine-tuned over 2 epochs with a
mixture of tool usage, tool feedback, and LLaVA multimodal instruction-tuning
data. During the training phase, we use 8 Nvidia 80G A100 GPUs and use
the DeepSpeed [50] engine to enhance the training efficiency. Unless otherwise
specified, we used LLaVA-1.5V-7B as the base model for the ablation study.
4.2 Datasets
Tool Usage Instruction-Tuning Data. To teach the agent to correctly use
tools, we construct 90K instruction-response pairs about tool usage. Our tool
library consists of 22 human-related tools: 8 for perception, 7 for reasoning, and
7 for generation. We further construct a validation and test set for evaluation.
Thevalidationsethas1000sampleswiththesametoolsasthetrainingset,while
thetestsetincludes689samplesrelatedto3toolsnotpresentedduringtraining.
To ensure the difference between the training and test sets, we use varied image
caption sources for input prompts. More details are provided in the Appendix.
Tool Feedback Instruction-Tuning Data. To help the multimodal LLM
modeldiscriminateand integratethetoolresults,weconstruct88Kinstruction-
following data based on existing 3D human datasets, including 61K tool result
discriminationinstructionsbuiltwithMoYo[64],3DPW[40],andPoseScript[7],
and 27K tool result integration instructions from SHAPY [5] and DECO [63].
4.3 Evaluation on Tool Usage
We compare our method with Visual ChatGPT [51] and GPT4Tools [70] on the
proposedevaluationsetandreport5metricsproposedinGPT4Tools[70].Please10 J. Lin, Y. Feng, W. Liu and M. Black
The man in the dark blue shirt is preparing to The man in the blue shirt is running. He
sit down. He is looking down at the step's appears to be heading to the other side of the
surface, bending his hands and lifting one leg, street, running at a fast pace, with his hair
getting ready to take a seat. flying. Please help me help his SMPL pose.
ChatPose ChatHuman Target HMR 2.0 ChatPose ChatHuman Target HMR 2.0
Fig.5: Qualitative comparison with ChatPose [13], HMR 2.0 [16] for reasoning-based
human pose estimation on SPG [13] benchmark.
refer to Appendix for details of the metrics. For Visual ChatGPT, we experi-
mentwithtwoversionsofGPT:“gpt-3.5-turbo-1106” and“gpt-4-turbo-preview”.
Regarding GPT4Tools, we adopt the official pretrained 13B model. For a fair
comparison, we also finetune GPT4Tools with our training data using the offi-
cial training code and obtain a variant, GPT4Tools-FT. Table 1 shows that the
original GPT4Tools does not perform well in our benchmark due to differences
between the common visual tools in GPT4Tools and our human-centric tools.
VisualChatGPT-4exhibitsimpressivetoolusageaccuracy,showingitspowerful
zero-shot ability to follow a standardized format and use tools accurately. Our
ChatHuman consistently outperforms the competitors, particularly with tools
not seen in training, thanks to our paper-based RAG mechanism.
4.4 Evaluation on 3D Human Understanding
In the following, we evaluate the performance of ChatHuman on four represen-
tative human-related tasks and report the performance on six benchmarks.
Pose Estimation.FollowingChatPose[13],weevaluatetheperformanceofour
method in both classical and reasoning-based pose estimation tasks. MPJPE,
PA-MPJPE, and MPJRE on the 3DPW [40] and RPE [13] benchmarks are re-
ported.Forthe reasoning-basedpose estimationtask,ChatHumanfirstgrounds
a human based on a textual description and feeds it into the pose estimation
tool to get the result. As shown in Table 2, ChatHuman achieves compara-
ble performance to the task-specific models on the classical pose estimation
task.Forreasoning-basedhumanposeestimation,whichinvolvesbothreasoning
ability and advanced human pose estimation ability, ChatHuman outperforms
bothtask-specificandmulti-modalLLMmethodsbyalargemargin(34.6%↓in
MPVPE). As shown in Figure 5, only our method achieves a satisfactory result.
The multimodal LLM competitor ChatPose could find the correct person but
fails to obtain an accurate pose due to the limited perception ability, while the
task-specific tool could not match the correct person due to the lack of reason-
ing ability. This demonstrates the advantages of ChatHuman, which combines
task-specific tool use expertise with the general reasoning ability of an LLM.
PoseGeneration.HereweevaluatetheposegenerationcapabilityofChatHuman
ontheclassicaltext-to-posegenerationtaskandthespeculativeposegeneration
task(SPG)[13].Followingpreviouswork[7,13],wereportthetext-to-poserecallChatHuman 11
3DPW[40] RPEBenchmark[13]
Method MPJPE↓ PA-MPJPE↓ MPJRE↓ MPJPE↓ PA-MPJPE↓ MPJRE↓
SPIN[24] 102.9 62.9 10.1 244.9 107.3 12.4
HMR2.0[16] 91.0 58.4 9.2 225.2 105.1 12.1
LLaVA-S[34] 440.8 205.4 21.8 490.7 207.4 21.1
LLaVA*-S[34] 232.1 101.1 12.8 - - -
GPT4-S[43] 322.0 136.7 16.0 - - -
LLaVA-P[34] 335.2 172.3 16.5 391.5 191.9 17.8
GPT4-P[43] 396.5 203.4 18.6 - - -
ChatPose[13] 163.6 81.9 10.4 253.6 103.8 11.7
ChatHuman 91.3 58.7 9.2 147.2 79.1 10.3
Table 2: Comparison of vanilla human pose estimation and reasoning-based pose esti-
mation on 3DPW [40] and RPE [13] benchmarks. LLaVA* is fine-tuned with human
keypointsdata.“S” usesmultimodalLLMsforkeypointdetectionandSMPLify[2]for
pose optimization. “P” utilizes multimodal LLMs for textual pose descriptions, pro-
cessed by PoseScript [7] to generate poses. MPJPE (in mm), MP-MPJPE (in mm),
and MPJRE (×100) are reported.
PoseScript[7] SPGBenchmark[13]
Method RP2T ↑ RT2P ↑ RP2T ↑ RT2P ↑
PoseScript[7] 40.4 52.3 65.0 41.4 54.1 65.9 1.5 3.5 6.2 1.4 2.3 5.1
ChatPose[13] 17.6 25.3 35.8 28.0 39.0 54.4 3.3 5.5 8.2 3.5 5.8 11.0
LLaVA-P[34] - - - - - - 2.1 4.0 7.1 2.1 3.3 6.1
GPT4-P[43] - - - - - - 2.7 4.7 9.2 2.7 5.3 8.2
ChatHuman 41.8 52.6 65.1 42.1 52.3 66.5 3.2 5.0 9.9 3.5 6.5 10.6
Table 3: Comparison of classical and speculative pose generation on PoseScript [7]
and SPG [13] benchmarks. “P” utilizes LLMs for textual pose descriptions rephrase,
processed by PoseScript [7] to generate poses. Top 5,10,20 recall rates are reported.
rate RT2P and pose-to-text recall rate RP2T of the retrieval models trained on
real poses and evaluated on generated poses. For the SPG task, ChatHuman
first rephrases the indirect pose descriptions into explicit ones and adopts Pos-
eScript(journalversion)[7]togenerateapose.AsshowninTable3,ourmethod
archives comparable performance to the SOTA methods on both benchmarks.
In contrast, the previous LLM-based method, ChatPose, performances poorly
on the classical pose generation benchmark, while the task-specific model Pos-
eScript lags in the SPG benchmark due to limited reasoning ability.
Body Shape Measurement. Here we evaluate the body shape measurement
accuracy of ChatHuman. We randomly sample 100 images from the HBW val-
idation set [5] and compare our method with a multimodal LLM, LLaVA [34],
and a SOTA body shape estimation method, CLIFF-BEDLAM [1]. For LLaVA
and ChatHuman, we ask them the same question to inquire about the height,
weight, chest, waist, and hip circumferences of a person in the image and then
prompt GPT-3.5 to extract the value from the model output. The details of the
question and prompt are available in Appendix. CLIFF-BEDLAM predicts the
body shape parameter, which is then converted to measurements based on the
shape-to-measurementfunctionfromSHAPY[5].Anthropometricmeasurement
errors are reported in Table 4. As shown, ChatHuman achieves superior perfor-12 J. Lin, Y. Feng, W. Liu and M. Black
Method Height↓ Weight↓ Chest↓ Waist↓ Hip↓ Method Precision↑ Recall↑ F1Score↑
LLaVA[34] 6.7 10.1 16.5 22.9 17.6 LLaVA[34] 0.26 0.81 0.39
CLIFF-BEDLAM[1] 7.8 13.9 8.6 13.5 7.0 GPT-4[43] 0.61 0.48 0.49
ChatHuman 6.7 10.4 6.1 13.0 6.4 ChatHuman 0.67 0.67 0.63
Table 4: Comparison of body shape measure- Table 5: Comparison of HOI under-
ment. Measurement errors (in cm and kg) on standing.Precision,RecallRate,and
HBW validation set [5] are reported. F1ScoreonDECO[63]arereported.
SeenTools UnseenTools
Paper RAG SR SR SR SR IoU SR SR SR SR IoU
t act args t act args
× × 0.998 0.967 0.928 0.960 0.964 0.946 0.894 0.775 0.822 0.872
× ✓ 1.000 0.967 0.928 0.961 0.965 0.996 0.945 0.842 0.891 0.927
✓ ✓ 1.000 0.974 0.950 0.970 0.975 0.999 0.967 0.893 0.954 0.953
Table 6: Ablation study of paper-based RAG mechanism. Successful rate of thought
(SRt), action (SRact), arguments (SRargs), execution (SR), and IoU are reported.
mance in most measurements, outperforming the multimodal LLM competitor
LLaVA by 42% and CLIFF-BEDLAM by 15.7% in average metrology accuracy.
Human-Object Interaction. Here we evaluate the human-object interaction
understanding ability of ChatHuman on the DECO [63] test set. The textual
ground truth (GT) labels are obtained by converting the vertex-level contact
labels into body part-level contact labels with the vertex-to-part mapping dic-
tionary of SMPL. Given a human image, we ask the multimodal LLM to detect
the body parts that contact with objects and prompt GPT-3.5 to extract the
bodypartlabelsfromtheanswer.Subsequently,wecomparethepredictedbody
partswiththeGTbodypartlabelandcomputetheaveragedetectionprecision,
recallrate,andF1Score.FromTable5,ChatHumanachievesSOTAperformance
in the precision and F1 score metrics, demonstrating the superior human-object
interaction understanding ability of our method. Notably, although LLaVA has
a high recall rate, its precision and F1 score are rather poor, which means that
it tends to predict all the body parts to be in contact with objects.
4.5 Ablation Study
Paper-based RAG Mechanism. To improve tool use accuracy, we design a
paper-based RAG mechanism. Here we perform a break-down ablation to inves-
tigate the effect of each component and their interactions. The baseline model
is derived by removing the RAG operation and is trained with the instruction-
following data constructed without referring to paper content. The results are
listed in Table 6. The baseline model’s success rate (SR) is 0.96 for seen tools
and 0.82 for unseen tools. Adding RAG increases the SR for unseen tools to
0.89, demonstrating its effectiveness in zero-shot settings. Further incorporating
scholarly articles into training data boosts the performance: the successful rate
of arguments (SRargs) rises from 0.93 to 0.95 for the seen tools and 0.84 to 0.94
fortheunseentools.Thissuggeststheuseofscholarlyarticlescanhelpconstruct
high-quality instruction-following data and tool documents, stemming from the
detailed usage instructions and diverse application scenarios within the paper.ChatHuman 13
Method Precision↑ Recall↑ F1Score↑ Method Height↓ Weight↓ Chest↓ Waist↓ Hip↓
w/oTool 0.26 0.81 0.39 w/oTool 6.7 10.1 16.5 22.9 17.6
w/Tool 0.67 0.67 0.63 w/Tool 6.7 10.4 6.1 13.0 6.4
(a) HOI Contact Detection. (b) Body Shape Measurement.
Table7:Ablationstudyabouthowcantoolsimprovehumanunderstandingonhuman-
object contact detection and body shape measurement tasks.
Method MPJPE↓ PA-MPJPE↓ PA-MPVPE↓ Method Height↓ Weight↓ Chest↓ Waist↓ Hip↓
ToolA 126.2 81.4 101.9 Tool[1] 7.8 13.9 8.6 13.5 7.0
ToolB 124.0 84.6 104.7 ChatHuman 6.7 10.4 6.1 13.0 6.4
ChatHuman 119.6 78.2 98.3 (b)BodyShapeMeasurementError(incm
(a) Mesh Error (in mm) on MixPose. or kg) on HBW [5] Validation Set.
Table8:Studyrevealinghowtooluseimproveshumanunderstandingonposeestima-
tion and body shape measurement tasks.
We further analyze the effects of each component of the paper for instructing
tool usage, please see Appendix for details.
Tool Result Integration and Discrimination. We first conduct an abla-
tion to study how can the tools enhance the human understanding capacity of
multimodal LLM. The model without tools is our multimodal LLM backbone,
LLaVA-1.5-7B [34], and the model with tools is our ChatHuman. The quanti-
tative results are listed in Table 7. When equipped with tools, the HOI contact
detectionF1scoreincreasesfrom0.39to0.63andtheaveragedbodyshapemea-
surement error declines by 38%. These results demonstrate the effectiveness of
tools in enriching the LLM’s comprehension of human models and behaviors.
Additionally, we study whether the multimodal LLM-based agent can uti-
lize its world knowledge to discriminate and improve the tool performance. We
design two discrimination schemes, i.e., selection and modification, and conduct
an ablation study on two human-related tasks by comparing ChatHuman with
the SOTA task-specific tools. For the selection scheme, we experiment with the
pose estimation task and select two SOTA methods, HMR 2.0 [16] and CLIFF-
SMPLify [2,30], as our tools to generate two poses of each person. We then
prompt the LLM-based agent to discriminate the results and choose the better
oneasthefinalresponse.Differenttoolsexcelindifferentscenariosandtocover
more diverse human poses and camera views, we build a new benchmark Mix-
Posebyselecting100imageswithextremecameraviewsfromtheMoYo[64]test
set, 100 full-body samples and 100 severely-truncated samples from 3DPW [40]
test set. Details of the prompt and MixPose benchmark are in Appendix. As
shown in Table 8(a), ChatHuman archives a lower reconstruction error on the
MixPose benchmark, validating the effectiveness of the agent as a discriminator
to improve the tool performance. For the modification scheme, we validate on
thebodyshapemeasurementtask.WeuseCLIFF-BEDLAM[1]asthetooland
prompt the agent to discriminate and modify the tool result. The result is re-
portedinTable8(b)andFigure6.Asshown,theLLM-basedagentcanimprove14 J. Lin, Y. Feng, W. Liu and M. Black
What are the height, chest, hip, What are the height, chest,
waist circumferences (in hip, waist circumferences (in
meter), and weight (in kg) of meter), and weight (in kg)
the person in the image? of the person in the image?
The woman is about 1.68 m tall, with Height: 1.44m H e i g h t : 1 . 6 9 m T h e m a n i s 1 . 8 8 m t a l l , w i t h a H e i g h t : 1 . 7 9 m H e i g h t : 1 . 8 3 m
a chest circumference of 0.90 m, Chest: 0.83 m Chest: 0.91 m chest circumference of 1.02 m, a Chest: 1.02 m Chest: 1.06 m
a hip circumference of 0.93 m, Hip: 0.91 m Hip: 1.0 m waist circumference of 0.97 m, and Hip: 0.97 m Hip: 1.02 m
a waist circumference of 0.78 m, Waist: 0.7 m Waist: 0.75 m hips circumference of 0.88 m. He Waist: 0.88 m Waist: 1.07 m
weighs approximately 69 kg Weight: 45.5kg Weight: 59.9 kg weighs approximately 80.4 kg Weight: 75.6 kg Weight: 91.7 kg
ChatHuman Tool GT Label ChatHuman Tool GT Label
(a) Tool generates unreasonable output (b) Tool generates reasonable output
Fig.6: Illustration about how the multimodal LLM-based agent discriminates and in-
tegrate tool results. The Agent will fix the unreasonable tool result and integrate the
reasonable tool result to generate a final response.
T p al nh ei as as vei as tahm rey l cp him am ra aeg ce t, r ee rc co o fu n ol s rd t mry u eo c .u t W weh iga ht' ts ot fhe thehe mig ah nt ?and W ofh ta ht e's pt eh re sonhe inigh tt hea in md agw ee ?ight
The man is about 1.76 meters tall and The woman appears to be tall, with a height
weighs approximately 71.28 kilograms. of about 1.82 m, and she weighs 63.2 kg.
N fao c, eI b ow oa kn t h et ao d u ps oe r tit r aa is t , m py le ase C p hl io san ys hei ed r ie , gr hcin tog u al nd dh ye wo eu ii gs p hl tea a as geb aia nes ?sk te imtb aa tl el C h hi eo g in h gs h i hd te e ar ei nln , d g w wt hh eaa it gt ' hs ts h ?h ee r is b aw re ea for oin tg a
reconstruct a facial avatar.
G h abeiv i oge uhn tt t 8ih s 0a .at 3 b 8h o e u k t gi s .1 a .8 b 8a msk e at nb da hll i sp l wa ey ie gr h, th ii ss W b we oi ut ah lp do p u brt eo x t ah im re oa uh t neig dlh y 6 h 1 1e . .7 1e 2 4l, m kh ge a .r nh de hig eh rt ww eo iu gl hd t
(a) Ground Truth: 1.96 m tall and weighs 90.7 kg (b) Ground Truth: 1.75 m tall and weighs 60.3 kg
Fig.7: Human interaction can improve performance and tool usage accuracy.
the tool performance by utilizing its general world knowledge to discriminate
and fix the unreasonable tool results, e.g., the height and weight in Figure 6(a).
5 Discussion and Concluding Remarks
Inconclusion,weintroduceChatHuman,anLLM-basedmodeldesignedtolearn
theuseoftoolsrelatedto3Dhumansandassistusersinsolvingtasksassociated
with 3D humans. The model processes requests from users, analyzes the needs,
and applies the necessary tools. It then evaluates and synthesizes the tools’
outputs to respond to the user’s queries and address the problems effectively.
Having computers better understand 3D humans has many potential benefits in
healthcare, fitness, human-robot interaction, entertainment, etc. Possible nega-
tive impacts include unwanted surveillance, body shaming, and deep fakes. The
risk-benefit ratio should be monitored and, like other LLMs, guardrails may be
needed to prevent malicious use cases.
Limitations. ChatHuman may fail in certain calling scenarios, particularly
when the user request is vague, and subsequent LLM internal analysis cannot
rectify an incorrect initial function call. However, further interaction with users
can remedy this if they provide additional information. For example, Figure 7
illustratesaninstanceofusingbodyestimationandfacereconstructiontoolsfor
avatar creation. Even with the application and analysis of the tool, outcomes
likeheightestimationmaynotbeentirelyprecise.Onecontributingfactoristhe
accuracyofthetrainingdata;forinstance,mostheightlabelsindatasetsusethe
official height of models or celebrities, which may not account for variations likeChatHuman 15
shoeheight,suchasa7-inchheel.Incorporatingmorecuesfromusers,combined
with the LLM’s knowledge of the world and reasoning capabilities, can enhance
theaccuracyofresults,asshowninFigure7.ChatHumanmainlyexplorestools
related to text and images, and briefly delves into human motion in Appendix.
Further exploration of video and motion remains for future work.
Future Work. ChatHuman offers several exciting avenues for future develop-
ment: 1) Integrated Learning and Self-Improving: Merging tool uses learning
with user feedback to continuously refine the model’s understanding and ap-
proach to 3D human tasks. 2) User Feedback for Enhanced Training: As shown
in Figure 7, user interaction has a tangible impact on improving outcomes. Fu-
ture work could explore how ongoing dialogues with users might provide valu-
ablefeedbackforrefiningandexpandingthesystem’scapabilitiesin3Dhuman-
related research.
Acknowledgement
We thank Naureen Mahmood, Nicolas Keller and Nicolas Heron for the support
of the data collection. MJB has received research gift funds from Adobe, Intel,
Nvidia, Meta/Facebook, and Amazon. MJB has financial interests in Amazon
andMeshcapadeGmbH.WhileMJBisaco-founderandChiefScientistatMesh-
capade, his research in this project was performed solely at, and funded solely
by, the Max Planck Society.
References
1. Black, M.J., Patel, P., Tesch, J., Yang, J.: Bedlam: A synthetic dataset of bodies
exhibiting detailed lifelike animated motion. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 8726–8737 (2023)
11, 12, 13, 20, 23
2. Bogo,F.,Kanazawa,A.,Lassner,C.,Gehler,P.,Romero,J.,Black,M.J.:Keepit
SMPL: Automatic estimation of 3D human pose and shape from a single image.
In: ECCV (2016) 4, 11, 13, 21
3. Cao, Y., Cao, Y.P., Han, K., Shan, Y., Wong, K.Y.K.: Dreamavatar: Text-and-
shape guided 3d human avatar generation via diffusion models. arXiv preprint
arXiv:2304.00916 (2023) 4
4. Chiang,W.L.,Li,Z.,Lin,Z.,Sheng,Y.,Wu,Z.,Zhang,H.,Zheng,L.,Zhuang,S.,
Zhuang,Y.,Gonzalez,J.E.,Stoica,I.,Xing,E.P.:Vicuna:Anopen-sourcechatbot
impressing gpt-4 with 909
5. Choutas, V., Müller, L., Huang, C.H.P., Tang, S., Tzionas, D., Black, M.J.: Accu-
rate3dbodyshaperegressionusingmetricandsemanticattributes.In:Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
2718–2728 (2022) 2, 4, 9, 11, 12, 13, 23
6. Daněček, R., Black, M.J., Bolkart, T.: Emoca: Emotion driven monocular face
captureandanimation.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition. pp. 20311–20322 (2022) 2, 4
7. Delmas,G.,Weinzaepfel,P.,Lucas,T.,Moreno-Noguer,F.,Rogez,G.:Posescript:
3d human poses from natural language. In: ECCV (2022) 4, 7, 9, 10, 11, 20, 21,
2216 J. Lin, Y. Feng, W. Liu and M. Black
8. Delmas, G., Weinzaepfel, P., Moreno-Noguer, F., Rogez, G.: Posefix: Correcting
3d human poses with natural language. In: Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision. pp. 15018–15028 (2023) 4, 20
9. Deng,Y.,Yang,J.,Xu,S.,Chen,D.,Jia,Y.,Tong,X.:Accurate3dfacereconstruc-
tion with weakly-supervised learning: From single image to image set. In: IEEE
Computer Vision and Pattern Recognition Workshops (2019) 4
10. Du, Y., Wei, F., Zhang, H.: Anytool: Self-reflective, hierarchical agents for large-
scale api calls. arXiv preprint arXiv:2402.04253 (2024) 4
11. Feng,Y.,Choutas,V.,Bolkart,T.,Tzionas,D.,Black,M.J.:Collaborativeregres-
sion of expressive bodies using moderation (2021) 4
12. Feng, Y., Feng, H., Black, M.J., Bolkart, T.: Learning an animatable detailed 3D
face model from in-the-wild images. vol. 40 (2021), https://doi.org/10.1145/
3450626.3459936 2, 4, 20
13. Feng,Y.,Lin,J.,Dwivedi,S.K.,Sun,Y.,Patel,P.,Black,M.J.:ChatPose:Chatting
about 3d human pose. In: CVPR (2024) 4, 10, 11, 21
14. Feng,Y.,Yang,J.,Pollefeys,M.,Black,M.J.,Bolkart,T.:Capturingandanimation
ofbodyandclothingfrommonocularvideo.In:SIGGRAPHAsia2022Conference
Papers. pp. 1–9 (2022) 2
15. Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., Wang,
H.: Retrieval-augmented generation for large language models: A survey. arXiv
preprint arXiv:2312.10997 (2023) 4
16. Goel, S., Pavlakos, G., Rajasegaran, J., Kanazawa, A., Malik, J.: Humans in 4D:
Reconstructing and tracking humans with transformers. In: ICCV (2023) 2, 4, 6,
10, 11, 13, 20, 21
17. Han,S.,Joo,H.:Chorus:Learningcanonicalized3dhuman-objectspatialrelations
from unbounded synthesized images. In: Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision. pp. 15835–15846 (2023) 4
18. Hong,F.,Zhang,M.,Pan,L.,Cai,Z.,Yang,L.,Liu,Z.:Avatarclip:Zero-shottext-
driven generation and animation of 3d avatars. arXiv preprint arXiv:2205.08535
(2022) 4
19. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen,
W.:Lora:Low-rankadaptationoflargelanguagemodels.arXiv:2106.09685(2021)
5, 9
20. IEEE: A 3D Face Model for Pose and Illumination Invariant Face Recognition
(2009) 4
21. Joo,H.,Neverova,N.,Vedaldi,A.:Exemplarfine-tuningfor3Dhumanposefitting
towards in-the-wild 3D human pose estimation (2020) 4
22. Kanazawa,A.,Black,M.J.,Jacobs,D.W.,Malik,J.:End-to-endrecoveryofhuman
shape and pose. In: CVPR (2018) 2, 4
23. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. In: Proceedings
of the IEEE/CVF International Conference on Computer Vision. pp. 4015–4026
(2023) 20
24. Kolotouros, N., Pavlakos, G., Black, M.J., Daniilidis, K.: Learning to reconstruct
3D human pose and shape via model-fitting in the loop. In: ICCV. pp. 2252–2261
(2019) 2, 4, 11
25. Kong, Y., Ruan, J., Chen, Y., Zhang, B., Bao, T., Shi, S., Du, G., Hu, X., Mao,
H.,Li,Z.,etal.:Tptu-v2:Boostingtaskplanningandtoolusageoflargelanguage
model-basedagentsinreal-worldsystems.arXivpreprintarXiv:2311.11315(2023)
4ChatHuman 17
26. LaViolaJr,J.J.:Bringingvrandspatial3dinteractiontothemassesthroughvideo
games. IEEE Computer Graphics and Applications 28(5), 10–15 (2008) 2
27. Lewis,P.,Perez,E.,Piktus,A.,Petroni,F.,Karpukhin,V.,Goyal,N.,Küttler,H.,
Lewis, M., Yih, W.t., Rocktäschel, T., et al.: Retrieval-augmented generation for
knowledge-intensivenlptasks.AdvancesinNeuralInformationProcessingSystems
33, 9459–9474 (2020) 4
28. Li, J., Xu, C., Chen, Z., Bian, S., Yang, L., Lu, C.: HybrIK: A hybrid analytical-
neural inverse kinematics solution for 3D human pose and shape estimation. In:
CVPR (2021) 4
29. Li,T.,Bolkart,T.,Black,M.J.,Li,H.,Romero,J.:Learningamodeloffacialshape
andexpressionfrom4Dscans.ACMTransactionsonGraphics,(Proc.SIGGRAPH
Asia)36(6),194:1–194:17(2017),https://doi.org/10.1145/3130800.3130813 4
30. Li, Z., Liu, J., Zhang, Z., Xu, S., Yan, Y.: CLIFF: Carrying location information
in full frames into human pose and shape estimation. In: ECCV (2022) 2, 4, 13,
21
31. Lin, J., Zeng, A., Wang, H., Zhang, L., Li, Y.: One-stage 3d whole-body mesh
recovery with component aware transformer. CVPR (2023) 4
32. Lin, K., Wang, L., Liu, Z.: End-to-end human pose and mesh reconstruction with
transformers. In: Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition. pp. 1954–1963 (2021) 20
33. Lin, T.Y., Maire, M., Belongie, S.J., Hays, J., Perona, P., Ramanan, D., Dollár,
P.,Zitnick,C.L.:Microsoftcoco:Commonobjectsincontext.In:ECCV(2014) 21
34. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: NeurIPS (2023)
2, 6, 8, 9, 11, 12, 13, 20, 24
35. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. arXiv preprint
arXiv:2304.08485 (2023) 4
36. Liu, S., Cheng, H., Liu, H., Zhang, H., Li, F., Ren, T., Zou, X., Yang, J., Su, H.,
Zhu, J., Zhang, L., Gao, J., Li, C.: Llava-plus: Learning to use tools for creating
multimodal agents (2023) 4
37. Liu,W.,Qiu,Z.,Feng,Y.,Xiu,Y.,Xue,Y.,Yu,L.,Feng,H.,Liu,Z.,Heo,J.,Peng,
S., et al.: Parameter-efficient orthogonal finetuning via butterfly factorization. In:
International Conference on Learning Representations (2024) 9
38. Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: A
skinned multi-person linear model. In: ACM TOG (2015) 3, 4, 7, 22
39. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101 (2017) 9
40. vonMarcard,T.,Henschel,R.,Black,M.J.,Rosenhahn,B.,Pons-Moll,G.:Recov-
ering accurate 3D human pose in the wild using IMUs and a moving camera. In:
ECCV (2018) 9, 10, 11, 13, 21
41. Matl, M.: Pyrender. https://github.com/mmatl/pyrender (2019) 22
42. Muller,L.,Osman,A.A.,Tang,S.,Huang,C.H.P.,Black,M.J.:Onself-contactand
human pose. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 9990–9999 (2021) 2, 4
43. OpenAI: GPT-4 technical report. (2023) 5, 7, 11, 12, 21, 23
44. Pan,J.,Chen,S.,Shou,M.Z.,Liu,Y.,Shao,J.,Li,H.:Actor-context-actorrelation
networkforspatio-temporalactionlocalization.In:ProceedingsoftheIEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 464–474 (2021) 4
45. Pavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A.A.A., Tzionas,
D.,Black,M.J.:Expressivebodycapture:3Dhands,face,andbodyfromasingle
image. In: CVPR (2019) 318 J. Lin, Y. Feng, W. Liu and M. Black
46. Petrovich, M., Black, M.J., Varol, G.: TMR: Text-to-motion retrieval using con-
trastive 3D human motion synthesis. In: International Conference on Computer
Vision (ICCV) (2023) 20, 25
47. Qiu, Z., Liu, W., Feng, H., Xue, Y., Feng, Y., Liu, Z., Zhang, D., Weller, A.,
Schölkopf, B.: Controlling text-to-image diffusion by orthogonal finetuning. Ad-
vances in Neural Information Processing Systems 36, 79320–79362 (2023) 9
48. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from
naturallanguagesupervision.In:Internationalconferenceonmachinelearning.pp.
8748–8763. PMLR (2021) 9
49. Rajasegaran,J.,Pavlakos,G.,Kanazawa,A.,Feichtenhofer,C.,Malik,J.:Onthe
benefits of 3d pose and tracking for human action recognition. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
640–649 (2023) 4
50. Rasley, J., Rajbhandari, S., Ruwase, O., He, Y.: Deepspeed: System optimiza-
tionsenabletrainingdeeplearningmodelswithover100billionparameters.arXiv
preprint arXiv:2002.11681 (2020) 9
51. Rasley, J., Rajbhandari, S., Ruwase, O., He, Y.: Visual chatgpt: Talking, drawing
andeditingwithvisualfoundationmodels.arXivpreprintarXiv:2303.04671(2023)
2, 4, 5, 9, 25, 27
52. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition.pp.10684–10695(2022) 20
53. Romero, J., Tzionas, D., Black, M.J.: Embodied hands: Modeling and capturing
hands and bodies together. ACM Transactions on Graphics, (Proc. SIGGRAPH
Asia) 36(6) (Nov 2017) 4
54. Rong,Y.,Shiratori,T.,Joo,H.:Frankmocap:Amonocular3dwhole-bodyposees-
timationsystemviaregressionandintegration.In:IEEEInternationalConference
on Computer Vision Workshops (2021) 4
55. Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y.: Hugginggpt: Solving ai
taskswithchatgptanditsfriendsinhuggingface.arXivpreprintarXiv:2303.17580
(2023) 4
56. Shin,S.,Kim,J.,Halilaj,E.,Black,M.J.:Wham:Reconstructingworld-grounded
humans with accurate 3d motion. In: CVPR (2024) 20
57. Su, H., Shi, W., Kasai, J., Wang, Y., Hu, Y., Ostendorf, M., Yih, W.t., Smith,
N.A.,Zettlemoyer,L.,Yu,T.:Oneembedder,anytask:Instruction-finetunedtext
embeddings. arXiv preprint arXiv:2212.09741 (2022) 6, 9, 25
58. Surís,D.,Menon,S.,Vondrick,C.:Vipergpt:Visualinferenceviapythonexecution
for reasoning. Proceedings of IEEE International Conference on Computer Vision
(ICCV) (2023) 4
59. Sweller, J., Ayres, P., Kalyuga, S.: Cognitive Load Theory. Springer, New York,
NY (2011) 3
60. Szolin, K., Kuss, D., Nuyens, F., Griffiths, M.: Gaming disorder: A systematic
reviewexploringtheuser-avatarrelationshipinvideogames.ComputersinHuman
Behavior 128, 107124 (2022) 2
61. Tewari,A.,Zollhofer,M.,Kim,H.,Garrido,P.,Bernard,F.,Perez,P.,Theobalt,C.:
Mofa: Model-based deep convolutional face autoencoder for unsupervised monoc-
ularreconstruction.In:ProceedingsoftheIEEEinternationalconferenceoncom-
puter vision workshops. pp. 1274–1283 (2017) 4ChatHuman 19
62. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-
lykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023) 4
63. Tripathi, S., Chatterjee, A., Passy, J.C., Yi, H., Tzionas, D., Black, M.J.: Deco:
Dense estimation of 3d human-scene contact in the wild. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 8001–8013 (2023)
2, 4, 6, 9, 12, 20, 22, 23
64. Tripathi, S., Müller, L., Huang, C.H.P., Taheri, O., Black, M.J., Tzionas, D.: 3d
human pose estimation via intuitive physics. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 4713–4725 (2023)
9, 13, 21, 24
65. Usman,M.,Latif,S.,Qadir,J.:Usingdeepautoencodersforfacialexpressionrecog-
nition. In: 2017 13th International Conference on Emerging Technologies (ICET).
pp. 1–6. IEEE (2017) 4
66. Wang,C.,Luo,W.,Chen,Q.,Mai,H.,Guo,J.,Dong,S.,Xuan,X.M.,Li,Z.,Ma,
L.,Gao,S.:Mllm-tool:Amultimodallargelanguagemodelfortoolagentlearning.
arXiv preprint arXiv:2401.10727 (2024) 4
67. Wei,S.E.,Saragih,J.,Simon,T.,Harley,A.W.,Lombardi,S.,Perdoch,M.,Hypes,
A., Wang, D., Badino, H., Sheikh, Y.: Vr facial animation via multiview image
translation. ACM Transactions on Graphics (TOG) 38(4), 1–16 (2019) 2
68. Xiang, D., Bagautdinov, T., Stuyck, T., Prada, F., Romero, J., Xu, W., Saito, S.,
Guo, J., Smith, B., Shiratori, T., et al.: Dressing avatars: Deep photorealistic ap-
pearanceforphysicallysimulatedclothing.ACMTransactionsonGraphics(TOG)
41(6), 1–15 (2022) 2
69. Xu,H.,Bazavan,E.G.,Zanfir,A.,Freeman,W.T.,Sukthankar,R.,Sminchisescu,
C.:GHUM&GHUML:Generative3Dhumanshapeandarticulatedposemodels.
In: CVPR (2020) 3
70. Yang,R.,Song,L.,Li,Y.,Zhao,S.,Ge,Y.,Li,X.,Shan,Y.:Gpt4tools:Teaching
large language model to use tools via self-instruction (2023) 9, 27
71. Yang,R.,Song,L.,Li,Y.,Zhao,S.,Ge,Y.,Li,X.,Shan,Y.:GPT4Tools:Teaching
llm to use tools via self-instruction. arXiv preprint arXiv:2305.18752 (2023) 2, 4,
5, 7, 20, 21, 25
72. Yang, Z., Liu, J., Han, Y., Chen, X., Huang, Z., Fu, B., Yu, G.: Appagent: Mul-
timodal agents as smartphone users. arXiv preprint arXiv:2312.13771 (2023) 2,
4
73. Yang, Z., Li, L., Wang, J., Lin, K., Azarnasab, E., Ahmed, F., Liu, Z., Liu, C.,
Zeng, M., Wang, L.: Mm-react: Prompting chatgpt for multimodal reasoning and
action. arXiv preprint arXiv:2303.11381 (2023) 4
74. Zhang,H.,Tian,Y.,Zhou,X.,Ouyang,W.,Liu,Y.,Wang,L.,Sun,Z.:Pymaf:3d
human pose and shape regression with pyramidal mesh alignment feedback loop.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision.
pp. 11446–11456 (2021) 4
75. Zhao, R., Chen, H., Wang, W., Jiao, F., Do, X.L., Qin, C., Ding, B., Guo, X., Li,
M.,Li,X.,etal.:Retrievingmultimodalinformationforaugmentedgeneration:A
survey. arXiv preprint arXiv:2303.10868 (2023) 420 J. Lin, Y. Feng, W. Liu and M. Black
Appendix
A Additional Details
Here we describe in more detail how we train a multimodal LLM to use tools,
discriminate, and integrate tool results to improve its ability to reason about
humans. As mentioned in the main paper, here we provide additional informa-
tion about the implementation, training process, evaluation methodology, and
ablation study. Due to space limits in the main paper, we provide more qualita-
tive results here that illustrate the performance of ChatHuman and its limita-
tions. The vector similarity searching algorithm from Chroma can be found in
LangChain/Chroma.
A.1 Tool Details
Inthemainpaper,wesupport22human-relatedtoolsinourtoolpockets,includ-
ing8perceptiontools,7generationtools,and7reasoningtoolsaslistedinTable
9. In section C, we further explore tools related to human motion, specifically
adding Motion Capture and Text-to-Motion Generation tools, as listed.
Perception Reasoning Generation
BodyPoseEstimation[16] SelectivePersonPoseDetection[16,34] Text-to-PoseGeneration[7]
BodyShapeMeasurement[1] SpecificPersonShapeMeasurement[1,34] SpeculativePoseGeneration[7,34]
HandPoseEstimation[32] TargetedHandPoseEstimation[32,34] Text-to-ImageGeneration[52]
FaceReconstruction[12] DescribedPersonFaceReconstruction[12,34] Text-basedPoseEditing[8]
HumanSegmentation[23] DescribedPersonSegmentation[23,34] RemoveSomethingFromThePhoto[23,34,52]
HOIDetection[63] SelectivePersonContactEstimation[34,63] ReplaceSomethingFromThePhoto [23,34,52]
PoseDescription[7] VisualQuestionAnswering[34] InstructImageUsingText[52]
ImageCaption[34] Text-to-MotionGeneration[46]
MotionCapture[56]
Table 9: List of tools used in ChatHuman. ChatHuman supports 24 human-related
tools, including 9 perception tools, 8 generation tools, and 7 reasoning tools.
A.2 Evaluation Metric Details
Tool Usage. We use the evaluation metrics proposed in GPT4Tools [71] to
measure the tool usage accuracy, including:
– SuccessfulRateofThought(SRt),whichmeasuresthedecisionaccuracy,
calculated as SRt = N1 (cid:80)N i=1I(τ i), where N is the number of instructions
and τ is a singular process. When the predicted thought is the same as the
i
ground-truth thought, I(τ ) is equal to 1, and 0 otherwise.
i
– Successful Rate of Action(SRact),whichmeasuresthetoolnamepredic-
tionaccuracy,calculatedasSRact = N1 (cid:80)N i=1I(α i),whereα
i
isthematching
processofthetoolname.Ifthepredictedtoolnameiscorrect,I(α )isequal
i
to 1, and 0 otherwise.
– Successful Rate of Arguments (SRargs), which measures the tool argu-
ments prediction accuracy, calculated as:
N K
1 (cid:88) 1 (cid:88)
SRargs =
N
η i, η
i
=
K
η i,j, (5)
i=1 j=1ChatHuman 21
whereK isthenumberoftoolarguments.Whentheargumentisafilename,
η equals 1 if the predicted file name is the same as the ground-truth file
i,j
name, and 0 otherwise. When the argument is text, η equals the BLEU
i,j
score between the predicted and groud-truth text.
– Interaction over Union (IoU), which quantifies the percent overlap be-
tween the predicted text and ground-truth text.
Human Understanding. We use the following evaluation metrics to measure
the performance of ChatHuman in human-related tasks:
– Pose Estimation. We adopt the same evaluation metrics as ChatPose [13]
toevaluatethe3Dposeestimationaccuracy,includingMeanPer-JointPosi-
tion Error (MPJPE), Mean Per-Joint Position Error after Procrustes align-
ment (PA-MPJPE), and Mean Per-Joint Rotation Error (MPJRE).
– Pose Generation.WeusetheevaluationmetricsestablishedinPoseScript
[7], including the text-to-pose recall rate RP2T and pose-to-text recall rate
RT2P of the retrieval models trained on real poses and evaluated on gen-
erated poses. We use the retrieval model from the jounal-version of Pos-
eScript[7]andChatPose[13]fortheclassicalposegenerationandspeculative
pose generation tasks, respectively.
A.3 Training Data Details
Tool Usage Instruction-following Data. To teach the LLM-based agent to
correctlyusetools,weconstruct90Kinstruction-responsepairsabouttoolusage.
Following GPT4Tools [71], we provide GPT-4 [43] with a textual description of
animagefromtheCOCOtrainingset[33]andatool-relatedpromptcontaining
a description of tools. One of our key observations is that human-related tools
often come with an academic paper containing rich background knowledge and
variedapplications,whichareusefulforthegenerationofuserqueriescoveringa
widerangeofapplicationscenarios.Thus,wealsoincorporatethepapercontent
into GPT-4 to generate the tool usage instruction-following data. To improve
efficiency, we first prompt GPT-4 to summarize the paper content, rearticulate
the tool functions and enumerate 50 potential user queries for tool activation.
ThedetailsofthepromptarerepresentedinTable15.Thesummarizedtoolde-
scriptionanduserquerieswillbefedtoGPT-4alongwiththeimagedescription
to generate the instruction-following data about tool usage. Table 16 illustrates
the prompt for the second step.
Tool Feedback Instruction-following Data. To help the multimodal LLM
model discriminate and integrate the tool results, we construct 88K pairs of
instruction-following data based on existing 3D human datasets.
– Pose Estimation Results Discrimination. To teach the LLM-based
model to discriminate the pose estimation results from different tools, we
build 17K pairs of instruction-following data based on the 3DPW [40] and
MOYO [64] training sets. Specifically, we use HMR 2.0 [16] and CLIFF-
SMPLify [2,30] to predict the human mesh and calculate the reconstruction22 J. Lin, Y. Feng, W. Liu and M. Black
Human: Please compare the pose Human: Please compare the pose
estimation results of Tool A (blue estimation results of Tool A (blue
mesh)andToolB(redmesh).Assess mesh)andToolB(redmesh).Assess
whichoneofthetwomeshesmore whichoneofthetwomeshesmore
accuratelyreplicatestheposeand accuratelyreplicatestheposeand
shape of the human figure in the shape of the human figure in the
original image, taking into account original image, taking into account
theiralignmentandresemblance. theiralignmentandresemblance.
Assistant: Tool B’s result is better. Assistant: Tool A.
Fig.8: Examples of the instruction-following data for discriminating between pose es-
timation results.
Human: Pleaseidentifythemostfittingtextdescriptionfortheposein
Human: Fromthequartetofposesinthe thisimage.
image,selecttheonethatbestaligns (A)Thepersonisstandingontheirleftlegwiththerightlegraisedlevel
with the pose description. Pose withthehipandthekneebentsothatthelowerlegcrossesthe
Description:Heisbalancingonhisright bodyandthefootisinfrontoftheleftleg.Theleftarmisraised.
footwithhiskneeslightlybent.Hisleft (B)Theyarebalancingmainlyontheirrightlegwhichisbentwiththeir
legisextendedoutstraightinfrontof foot flat but pointed outwards also. Their left leg is fully
him with his knee slightly bent. Both outstretchedinfrontofthemwithhalftheirfootofftheground.
armsareelevatedaboutfaceheightwith (C)Thispersonisbentoverandonallfoursasiftheyarecrawling.Both
hisleftelbowslightlybent.Heappears legsarebentattheknees,withtheleftfootslightlyinfrontofthe
tobelookingupwardslightly. rightfoot.Thearmsareextendedslightlyforward.
Assistant: Pose B best matches the (D)Thispersonisstanding,slightlybendingtheirbodytotheright.The
provided pose description. handsareplacedontheabdominalregion.
Assistant: Description A best matches the pose in the image.
(a) Text-to-Pose Selection (b) Pose-to-Text Selection
Fig.9: Examples of the instruction-following data for discriminating pose generation
and pose description results.
errorbetweenthepredictedmeshandgroundtruthmesh.BasedonMPVPE,
we determine which tool is better for each image and construct instruction-
following data as shown in Figure 8. Pose visualization results are rendered
with Pyrender [41].
– Pose Generation Results Discrimination. The human pose generation
tool,PoseScript[7],hasmultipleoutcomesforeachtextinput.Herewecon-
struct44Kpairsofinstruction-followingdatatoteachthemultimodalLLM-
basedmodeltodiscriminatethemultipleposegenerationresults.Specifically,
we use PoseScript training data as the source and construct the data in two
formats. The first one is about text-to-pose selection, as shown in Figure
9(a). Given a textual description, we visualize the corresponding pose and
three other different poses from the training data and ask the agent to dis-
criminate and choose the one that best aligns with the textual description.
The second one is about pose-to-text matching, as shown in Figure 9(b).
Givena3Dpose,wevisualizeitasanimagebyrenderingthe3Dbodymesh
in that pose. Then, we combine it with the corresponding text description
and three other pose descriptions in the format of a multiple choice ques-
tion.Finally,weasktheagenttochoosetheonethatbestdescribesthepose
shown in the image.
– Human Contact Detection Results Integration. The outcome of the
human contact prediction tool, DECO [63], is a vertex-wise contact predic-
tion in a vector representation y ∈R6890×1, which can not be directly used
c
as input for our multimodal LLM baseline, LLaVA. To solve this problem,
we transform the vertex-wise contact label of ground-truth and DECO’s
resultintoatextualdescriptionbasedonthevertex-to-partmappingdictio-
naryoftheSMPLmodel[38].Subsequently,wefeedthetextualdescriptionsChatHuman 23
Human: Pleasedescribetheinteractionbetween Human: Arebothfeetoftheperson
thepersonandanyobjectsinasmuchdetailas touching the ground or any object?
possible.ThecontactregiondetectedbyDECO ThecontactregiondetectedbyDECO
are:righthand,leftfoot,rightfoot.Notably, are:rightupperleg,leftfoot,back,
the provided contact regions might not be rightfoot,lefthand,righthand,left
completeandyoushoulddetectallthecontact upperleg,hips.Notably,theprovided
regionsbyyourself. contact regions might not be
Assistant: Thepersonisholdingatennisracket complete and you should detect all
withhisrighthand,andhisstanceindicatesthat thecontactregionsbyyourself.
hemaybeintheactionofswingingatatennis Assistant: Yes,boththeleftandthe
ball. Both feet are planted on the ground, rightfeetofthepersonaretouching
providingstabilityfortheactionheisperforming. theground.
Fig.10: Instruction-following data about integrating results from human contact de-
tection. The contact labels detected by DECO [63] are combined with the user query.
Human: what would be the appropriate Human: Whatisthebodyheightandmass
clothes size for this person? Clues from of the person in the image? Clues from
tool: height: 1.65m,weight: 65kg,chest tool: height: 1.82 m, weight: 80.2 kg,
circumference:1.03m,waistcircumference: chest circumference: 1.03 m, waist
0.87m,hipscircumference:1.24m.Please circumference:0.89m,hipscircumference:
note that these clues are provided as 1.01m.Pleasenotethatthesecluesare
references and may not be entirely provided as references and may not be
accurate. entirelyaccurate.
Assistant: I would recommend a clothes Assistant: The person is about 1.98 m,
sizeofLfortheperson. 87.3 kg.
Fig.11:Instruction-followingdataforintegratingresultsfromhumanbodyestimation.
Measurements of the estimated body shape from BEDLAM-CLIFF [1] are added to
the user query as clues.
along with the RGB image from the DECO training set [63] into GPT-4V
andpromptGPT4[43]togenerateinstruction-followingdataabouthuman-
objectinteractionasshowninFigure10.Notably,thetransformedtoolresult
ismergedwiththeuserqueryasaclue.Thedetailsofthepromptareshown
in Table 17.
– Body Shape Measurement Integration. Similar to human contact pre-
diction,theoutcomeofthebodyshapemeasurementtoolistheSMPLbody
shape parameter β ∈ R10, which is also in a vector representation and can
not be used by the LLM directly. Thus, we first convert the shape param-
eter into measurements based on the shape-to-measurement module from
SHAPY [5] and represent it in a textual format. Subsequently, we feed the
body measurement description along with attribute labels from the SHAPY
trainingsetintoGPT-4andpromptittogenerateinstruction-followingdata
about human body shape as shown in Figure 11. Similarly, we merge the
body measurement predicted by the tool with the user query as a clue. The
prompt for GPT-4 is detailed in Table 18.
A.4 Benchmark Details
Tool Usage Benchmark. To evaluate the tool usage accuracy of our method,
we construct a validation and test set. The validation set has 1000 samples with
thesametoolsasthetrainingset,whilethetestsetincludes689samplesrelated
to 3 tools unseen during training. Similar to the training data construction,
we feed a textual description of an image from the COCO validation set, a tool
description,andsomeexamplessummarizedfromthetoolpaperintoGPT-4and24 J. Lin, Y. Feng, W. Liu and M. Black
prompt it to generate instruction-following data about tool usage. We use the
imagedescriptioncaptionedbyLLaVA[34]insteadoftheoriginalimagecaptions
toensureadifferencebetweentrainingandtestsets.Finally,wemanuallycheck
the question-answering pairs to ensure the accuracy of the benchmark.
MixPose Benchmark. To validate whether the multimodal LLM-based agent
candiscriminatetheposeestimationresultsfromdifferenttools,webuildanew
benchmark,MixPose.Consideringthatdifferenttoolsexcelindifferentscenarios,
abenchmarkcoveringdiversescenariosandcornercasesisneeded.Toconstruct
this benchmark, we selected 100 images featuring extreme camera angles from
the MoYo [64] test set, 100 full-body images from the 3DPW test set, and 100
images with significant truncation also from the 3DPW test set. This approach
ensures our benchmark includes typical in-the-wild images, shots taken from
extreme views, and images with heavy truncation. This diverse distribution al-
lows us to test whether the agent can accurately choose the right tool based on
sceneries of the image and the tool performance. To get the truncated images
from 3DPW, we resize the human bounding box by 2/3 and crop the human
image based on the rescaled human bounding box.
B Additional Ablation Studies
Paper Components.Toimprovethetoolusageaccuracy,weproposeapaper-
basedRAGmechanism.Hereweconductanablationstudytoanalyzetheeffects
of each component of the paper for instructing tool usage. The baseline model
is derived by removing the RAG operation and is trained with the instruction-
following data constructed based on the manually defined tool descriptions and
examples (T), without referring to paper content. We then add the tool de-
scriptions and examples summarized from different paper components into the
prompt and request GPT-4 to generate new instruction-following data. The pa-
per components include the abstract (A), introduction (I), related work (R),
method (M), and experiment section (E). The results are presented in Table 10.
As shown, incorporating the paper content consistently improves the accuracy
of tool usage. Interestingly, feeding the abstract and introduction of the paper
achievesthebestperformance,andaddingadditionalpapercomponentslikethe
experiments does not result in further improvement. This is expected since the
abstract and introduction have covered the tool function and potential applica-
tionsinmostcases.Wealsofindthatexamplesgeneratedbasedonpapercontent
can be noisy in some cases, and thus we manually check the examples and re-
move the incorrect samples before feeding them to GPT-4. This data cleaning
operation improves the accuracy.
BaseModelforValueExtraction.Duringourbodyshapemeasurementand
human contact detection experiments, the answer of ChatHuman is a sentence
andthuscouldnotbedirectlyusedtocomputetheevaluationmetrics.Thus,we
needtopromptanLLMtoextractthevaluefromthesentenceandcomparethe
extractedvaluewiththegroundtruthlabeltocalculatethemetric.Theprompts
forbodyshapemeasurementandcontactestimationareshowninTable13.HereChatHuman 25
Paper SeenTools UnseenTools
T A I R M E C SR SR SR SR IoU SR SR SR SR IoU
t act args t act args
✓ ✓ 1.0 0.97 0.93 0.96 0.96 0.95 0.89 0.78 0.82 0.87
✓ ✓ 1.0 0.97 0.95 0.97 0.97 0.99 0.94 0.85 0.90 0.93
✓ ✓ ✓ 1.0 0.97 0.95 0.97 0.97 1.0 0.97 0.86 0.91 0.94
✓ ✓ ✓ ✓ 1.0 0.98 0.95 0.97 0.97 1.0 0.97 0.84 0.91 0.93
✓ ✓ ✓ ✓ ✓ 1.0 0.98 0.94 0.97 0.97 0.99 0.95 0.82 0.87 0.92
✓ ✓ ✓ ✓ ✓ ✓ 1.0 0.98 0.95 0.97 0.97 1.0 0.96 0.86 0.91 0.94
✓ ✓ ✓ ✓ 1.0 0.97 0.95 0.97 0.98 1.0 0.97 0.89 0.95 0.95
Table 10: Ablation study on the impact of each paper component in the paper-based
RAGmechanism.Tdenotestooldescription,A,I,R,M,Eareabstract,introduction,
related work, method, experiment section from the paper, and C denotes the data
after manually cleaning. Successful rate of thought (SRt), action (SRact), arguments
(SRargs), execution (SR), and IoU are reported.
Evaluator Precision↑ Recall↑ F1Score↑ Evaluator Height↓ Weight↓ Chest↓ Waist↓ Hip↓
GPT-3.5 0.67 0.67 0.63 GPT-3.5 6.7 10.4 6.1 13.0 6.4
GPT-4 0.69 0.69 0.64 GPT-4 6.7 10.4 6.1 13.0 6.4
(a) HOI Contact Detection. (b) Body Shape Measurement.
Table 11: Ablation study on the base model for value extraction.
we use two different LLM models for value extraction and report the results in
Table 11. As shown, the choice of the LLM model for value extraction does not
introduce a significant difference.
Text Embedding Model in Retrieval. During the paper-based retrieval
mechanism, we retrieve a relevant example by matching the text embedding
of the query and those within the tool documents using a pretrained text em-
bedding model [57]. Here we conduct an ablation study of the text embedding
model to analyze the robustness of our RAG mechanism. We experiment with
two models, i.e., instructor-xl and instructor-base. As shown in Table 12, the
performanceofourmethodisnotgreatlyaffectedbythetextembeddingmodel,
which demonstrates the robustness of ChatHuman.
Tool Use in Multi-turn Dialogue. In many real-world scenarios, the models
needtocorrectlyusethetoolbasedonthecontextwithinamulti-turndialogue.
Here, we build a benchmark to evaluate the tool use accuracy within multi-turn
conversations.Werandomlyselectseveralsingle-turnquestion-answerpairsfrom
the original multi-turn tool use benchmark and ask GPT-4 to merge them into
a multi-turn dialogue. We then evaluate the performance of GPT4Tools [71],
Visual ChatGPT [51], and ChatHuman on the built benchmark, which contains
1000 instruction-response pairs. As shown in Table 14, ChatHuman achieve a
superior performance in the multi-turn setting, demonstrating its capacity to
understand the comprehensive context information within the multi-turn dia-
logue and correctly use the tool to solve the problems.
C More Results
Motion-related Tools. We also integrate motion tools for tasks related to
human motions like text-to-motion generation. Specifically, we utilize TMR [46]26 J. Lin, Y. Feng, W. Liu and M. Black
SeenTools UnseenTools
Method SR SR SR SR IoU SR SR SR SR IoU
t act args t act args
instructor-base 1.000 0.975 0.947 0.972 0.974 0.997 0.950 0.884 0.949 0.949
instructor-xl 1.000 0.974 0.950 0.970 0.975 0.999 0.967 0.893 0.954 0.953
Table12:AblationstudyoftextembeddingmodelforRAG.Successfulrateofthought
(SRt), action (SRact), arguments (SRargs), execution (SR), and IoU are reported.
(a)YouareanAIassistant.Yourinputwillbeadescriptionofbodymeasure-
ments, including height, weight, chest circumference, hip chest circumference,
andwaistcircumference.Yourtaskistoextractthevalueofeachattributeand
return a result like:
“height:1m,weight:1kg,chestcircumference:1m,waistcircumference:1m,
hip circumference: 1 m”
If there is no measurement value, return “There is no measurement value.”
(b) You are an AI assistant. Your input will be a description of the human-
object interaction information of a person. Your task is to extract the body
partsthatcontactwithobjectsandreturnalist.Considerthefollowingpossible
body parts: right hand, right upper leg, left arm, left leg, left foot, back, left
shoulder, right shoulder, right foot, head, right arm, left hand, right leg, left
forearm, right forearm, neck, left upper leg, hips.
Table13:PrompttoextractthetargetvaluesfromasentencegeneratedbyChatHuman
formetriccomputation.Prompt(a)isusedtorequestGPT-3.5forbodymeasurement
values. Prompt (b) instructs GPT-3.5 to extract the body part names.
Whatwouldamanlooklikeif Please help me create a video
hewassittingontheground? of a dog running.
(a) Vague user request. (b) Unsupported tasks.
Fig.12: Failure cases of ChatHuman. When the user inputs a vague query or requests
a task that could not be finished by current tools, ChatHuman might fail.
I’mlookingtoanimateacharacterkickinga Iwanttolosemyweight,couldyouplease
football.Canyouassistmewithcreatingthe recommedanexerciseforme?
motion?
Toloseweight,Iwouldrecommendjumpingjacks.
Certainly,hereistheanimationof
thecharacterkickingafootball. Canyougenerateamotionofthisexercise?
Sure,hereisthemotionofjumpingjacks.
Fig.13: Text-to-motion generation examples.
to generate 3D human motions from text input. Figure 13 shows two examples
about synthesizing human motion from a textual description.ChatHuman 27
SR SR IoU SR SR
args t act
GPT4Tools[70] 0.582 0.551 0.553 0.513 0.612
VisualChatGPT-3.5[51] 0.438 0.203 0.162 0.173 0.691
VisualChatGPT-4[51] 0.860 0.794 0.711 0.744 0.789
ChatHuman 1.000 0.959 0.927 0.955 0.962
Table 14: Comparison of tool usage accuracy within multi-turn dialogue. Successful
rateofthought(SRt),action(SRact),arguments(SRargs),execution(SR),andIoUare
reported.
Failure Cases and Limitations. There are mainly two kinds of failure cases
of ChatHuman. Firstly, ChatHuman may fail in certain calling scenarios, par-
ticularly when the user request is vague, as also shown in Figure 7 in the main
paper.Secondly,whentheuserqueryrequiresatask(e.g.,videogeneration)that
isnotsupportedbythetoolsinthetoollibraries,ChatHumanmightincorrectly
choose a tool to generate a response, instead of informing the user that the task
cannot be completed and recommending the creation or introduction of a new
tool. As shown in Figure 12(b), there is a need for tools for video generation.
Futuredevelopmentsmayincludeexpandingtotoolsrelatedtothesescenarios.28 J. Lin, Y. Feng, W. Liu and M. Black
YouareanAIvisualassistanttaskedwithanalyzingapaperonamethodinthe
fieldof3Dhumanmodeling.Yourgoalistoextractkeyinformationaboutthe
method—itsname,purpose,uses,andpotentialapplicationscenarios.Basedon
this,youneedtosuccinctlydefinethemethodinthefollowingformats“Method
name is a tool to do something. Useful when you want to do something. Like:
user query.”
Subsequently,craft50diverse,realisticuserpromptsthatindirectlypertainto
using this method. These queries should be framed as questions, demands, or
scenarios from consumers who are unaware of the method’s name but whose
needs align with its capabilities. Assume that consumers have an image and
seek assistance in achieving a task related to the image using this method.
Eachpromptshouldintroducethetaskoftheuserinanimperativetone.The
prompt should specify and refer to the image.
Here is one example:
Method definition:
name=“HMR2.”,
description=“HMR2.0isatooltoestimatethe3Dposeandshapeoftheperson
intheimage.Usefulwhenyouwanttodetectposesofthehumansintheimage.
Like: estimate the human poses in the image.”
Possible queries:
1. Can you help me estimate the pose of the person in the photo?
2. Please reconstruct a 3D avatar for the person in the image.
3. Could you please estimate the SMPL parameter of the man in the picture?
4.Someoneintheimageisridingabicycle,couldyoupleasehelpmeestimate
her pose?
Table 15: Prompt to request GPT-4 to summarize paper content, rearticulate tool
functions, and enumerate possible user queries for tool activation.ChatHuman 29
Givenanimagewhoseimagepathis“example.jpg”.Imagecaption:“{caption}”.
Theimagecaptionincludesdetailimagedescriptionandeachobjectpairedwith
theboundingbox[x1,y1,x2,y2].Fortheboundingbox,(x1,y1)referstothe
top left, and (x2, y2) refers to the bottom right. x1 less than x2, and y1 less
than y2.
Below are 22 visual tools. Each tool is defined as “tool name: usage scenario,
and tool arguments”.
Please generate 3 visual instructions for each tool, so you need to generate 66
visual instructions in total.
The generated instructions should follow the format of “instruction content,
[tool name, tool arguments]”. Each instruction must relate to the caption and
can be solved by the tool.
You can not revise the “tool name”, or add any other fake tools that is not
defined. You must keep the correct “tool arguments”.
Tools: {tool description}
Note that your generated visual instructions should be highly related to the
image caption. Directly reply to me with the list, here are some examples:
{examples}
Diversify the instructions to cover a wide range of possible user queries. Feel
freetoadaptandrephrasetheexamplesprovidedtogeneratediverse,complex,
and deceptive instructions as much as possible.
For example, you can also change the subject position or the person and pose
descriptionpositions.Don’tusetoomuchimperativesentence,youshouldalso
use interrogative sentence.
Table 16: Prompt to request GPT-4 to generate instruction-following data about tool
usage based on the image description, tool description, and tool usage examples sum-
marized from tool paper.30 J. Lin, Y. Feng, W. Liu and M. Black
You are an AI visual assistant, and you are seeing a single image and a sen-
tenceaboutthehuman-objectcontactregionsofthepersonintheimage.The
sentence include the human-object contact body parts of the person. Notably,
the provided contact regions might not be complete and you should detect all
the contact regions by yourself.
Design a conversation between you and a person asking about the human-
object contact information of the person. The answers should be in a tone
that a visual AI assistant is seeing the image and answering the question. Ask
diverse questions and give corresponding answers.
Includequestionsaskingabouttheperson’shuman-objectcontactinformation,
etc. Only include questions that have definite answers: (1) one can see the
contentintheimagethatthequestionasksaboutandcananswerconfidently;
(2) one can determine confidently from the image that it is not in the image.
Do not ask any questions that cannot be answered confidently.
Provide detailed answers when answering complex questions. In your answer,
youshouldimitateasifyouseetheimageandthecontactregionsareestimated
byyou.Youshouldonlyaskquestionsaboutthehuman-objectinteraction.The
answer should be as detailed as possible. Don’t mention any other irrelevant
information! Directly reply to me with a list, here are some examples:
1. Please help me detect the contact regions of the person in the image. [The
person’s contact region includes his feet, hands, and back. His feet touch the
ground and his hands are holding a mobile phone.]
2. Does the person’s hand contact any objects? [Yes, her right hand holds an
umbrella.]
3.Describethehuman-objectinteractioninformationoftheperson,asdetailed
as possible. [The person holds a phone with his left hand and stands on a
skateboard with both their left and right feet.]
Notably,atleastonequestionistoaskallthecontactregionsoftheperson.In
your answer, you should distinguish and specify the left and right body parts.
Notably,youshoulddistinguishbasedonthebodyposeandorientation.Ifthe
person is facing the camera, the hand, foot, and ear on the left side of the
image is the person’s right hand, right foot, and right ear, and the one on the
right side of the image is the person’s left hand, left foot, and left ear. If the
person has their back to the camera, the one on the left side of the image is
the person’s left body part, and the one on the right side of the image is the
person’s right body part.
Table 17: Prompt to request GPT-4V to generate instruction-following data about
human-object interaction based on the textual contact description and RGB image.ChatHuman 31
You are an AI visual assistant, and you are seeing a single image. What you
see are provided with a sentence, describing the body shape of the person in
the image. Answer all questions as you are seeing the image.
The sentence includes information about the person’s gender, body mass,
height, chest circumference, waist circumference, and hip circumference. Be-
sides,itincludes15linguisticshapeattributesscalefrom1(stronglydisagree)
to 5 (strongly agree).
Designaconversationbetweenyouandapersonaskingaboutthebodyshape
of the person. The answers should be in a tone that a visual AI assistant is
seeing the image and answering the question.
Ask diverse questions and give corresponding answers.
Include questions asking about the visual content of the image, including the
person’soverallbodyfit,shape,height,mass,etc.Onlyincludequestionsthat
have definite answers:
(1)onecanseethecontentintheimagethatthequestionasksabout andcan
answer confidently;
(2) one can determine confidently from the image that it is not in the image.
Do not ask any questions that cannot be answered confidently.
Provide detailed answers when answering complex questions. When the ques-
tionisaboutthemeasurement,provideanexplicitandconcretemetricnumber
in the answer.
In your answer, you should imitate as if you see the image and the measure-
ments and linguistic attributes are estimated by you. The linguistic attribute
scoreisonlyusedtohelpyouunderstandanddon’tmentionitinyouranswer.
Directly reply to me with a list, here are some examples:
1.Howtallisthepersonintheimage?[Thepersonlooksquitetall.Heisabout
1.85 m.]
2. Please help me estimate the body measurements of the man in the image.
[The man is about 1.74 m and 60 kg. His chest circumference is about 0.9 m.]
3. What’s the waist circumference of the person? [The chest circumference is
about 0.95 m.]
Table18:PrompttorequestGPT-4togenerateinstruction-followingdataabouthuman
body shape based on the textual description about human body.