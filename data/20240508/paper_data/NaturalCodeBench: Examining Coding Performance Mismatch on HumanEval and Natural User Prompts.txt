NaturalCodeBench: Examining Coding Performance
Mismatch on HumanEval and Natural User Prompts
ShudanZhang12†∗,HanlinZhao1∗,XiaoLiu12∗,QinkaiZheng12∗,
ZehanQi12†,XiaotaoGu1,XiaohanZhang1,YuxiaoDong2,JieTang2
1Zhipu.AI 2TsinghuaUniversity
Case of HumanEval Case of NaturalCodeBench
Hello, please write a Python function for me. The function should read a
def has_close_elements(numbers:
markdown file, add numbering like x.y.z... to the titles of each level, and
List[float], threshold: float) -> bool:
then return the modified string. Please note not to write into the original file.
""" Check if in given list of numbers, are any
def add_section_numbering(markdown_file):
two numbers closer to each other than
""" markdown_file is the path to the markdown file. Return modified
given threshold.
markdown file content string
"""
"""
Figure1: ComparisonbetweenHumanEvalandNATURALCODEBENCH. (Upper)Performanceplot
oftestedLLMsonbothbenchmarks. LLMsinredcirclepresentrelativelymismatchedperformances
ontwobenchmarks. (Lower)CasestudyoncodingtasksinHumanEvalandNCB.NCBisgrounded
onnaturalpromptsfromreal-worldusersandevaluatedinanexecutabledockerenvironment.
*SZ, HZ, XL, and QZ contributed equally. Emails: {zsd22@mails.tsinghua.edu.cn,
hanlin.zhao@zhipuai.cn, shawliu9@gmail.com, qinkai.zheng1028@gmail.com}
†WorkdonewhenSZandZQinternedatZhipuAI.
Preprint.Underreview.
4202
yaM
7
]LC.sc[
1v02540.5042:viXraAbstract
Largelanguagemodels(LLMs)havemanifestedstrongabilitytogeneratecodes
forproductiveactivities. However,currentbenchmarksforcodesynthesis,such
asHumanEval,MBPP,andDS-1000,arepredominantlyorientedtowardsintro-
ductorytasksonalgorithmanddatascience,insufficientlysatisfyingchallenging
requirementsprevalentinreal-worldcoding. Tofillthisgap,weproposeNATU-
RALCODEBENCH(NCB),achallengingcodebenchmarkdesignedtomirrorthe
complexity and variety of scenarios in real coding tasks. NCB comprises 402
high-qualityproblemsinPythonandJava,meticulouslyselectedfromnaturaluser
queries from online coding services, covering 6 different domains. Noting the
extraordinarydifficultyincreatingtestingcasesforreal-worldqueries, wealso
introduceasemi-automatedpipelinetoenhancetheefficiencyoftestcaseconstruc-
tion. Comparingwithmanualsolutions,itachievesanefficiencyincreaseofmore
than4times. Oursystematicexperimentson39LLMsfindthatperformancegaps
onNCBbetweenmodelswithcloseHumanEvalscorescouldstillbesignificant,
indicatingalackoffocusonpracticalcodesynthesisscenariosorover-specified
optimizationonHumanEval. Ontheotherhand,eventhebest-performingGPT-4
isstillfarfromsatisfyingonNCB.Theevaluationtoolkitanddevelopmentsetare
availableathttps://github.com/THUDM/NaturalCodeBench.
1 Introduction
Largelanguagemodels(LLMs)pre-trainedonextensiveopencoderepositories[13;45;33;14]have
demonstratedimpressiveperformanceoncodesynthesisandevenachieveperformancecomparable
toaveragehumanlevelincodingcompetitions[35]. Unlikeopentextgeneration,whichoftenunder-
scoreshumanpreferencesasnotedby[47],codesynthesisprioritizesaccuracyandthefulfillmentof
userintent,essentialforpracticalproductionandapplication.
Asaresult,evaluatingcodesynthesispresentsuniquechallengesintheeraofLLMs. Traditional
evaluationmetricsbytokenmatching[48;36;50]showaweakcorrelationwithhumanjudgement
[21]andoverlookfunctionalcorrectnessofthegeneratedcode20;56. Recently,execution-based
evaluationhasgainedincreasingpopularity,wherecodegeneratedbymodelsistestedthroughunit
teststoverifyitsfunctionalcorrectness. Itleadstothedevelopmentofseveralbenchmarks,including
HumanEval[13],MBPP[7],MBXP[6],CodeContests[35],andDS-1000[32].
Notwithstandingtheircommendablereliabilityandaccuracy,thesebenchmarksfallshorttosuffi-
cientlycapturethewiderangeofneedsandcomplexityfoundinreal-worldengineeringapplications.
Theyareprimarilylimitedtowell-definedcodingproblemsinalgorithm,programbasics,ordata
science.Forexample,asshowninFigure1,aproblemfromHumanEval[13]teststheimplementation
ofabasicfunctionhas_close_elementsandtakesfloating-pointargumentsasinputs. However,
inpracticalapplications,userengineeringrequirementscanbemuchmorecomplexandvaried. In
Figure1,weshowcaseanexampleadaptedfromarealuserquery,wheretheuseraskstoreadand
parseXMLfilesgivencertaintags. Difficultandcostlythoughitis,curatingabenchmarkcomposed
ofsuchproblemsismeaningfulforevaluatingtherealuserexperienceofLLMcodesynthesis.
Contributions.Inlightofthechallenge,weintroduceNATURALCODEBENCH(NCB),achallenging
application-drivendatasetforcodesynthesisevaluation. NCB isdedicatedtocreatingareliable
evaluationenvironmentthatismorealignedwithreal-worldapplications. WeleverageanCodeGeeX
[70]onlineservicestocollectrealanddiverseapplication-relateduserqueries. Afterfilteringand
reprocessing,402high-qualityPythonandJavaproblemsarecompiled,covering6domainsincluding
software,front-end,systemadministration,andartificialintelligence,highlightingpracticalscenarios.
Beyond basic data structures like lists and numbers, the test inputs for NCB problems include
versatilefiletypesandothercomplexstructures,makingitmorechallenging.
Thechallengingnatureof NCB necessitatessignificanthumanlaborinitsannotationprocessTo
improveconstructionefficiency,wetailorasemi-automatedannotationpipelinetocuratehigh-quality,
testable,andusefulquerieswithcorrespondingtestcases. Specifically,weemployGPT-4[45]to
generatereferencesolutionsfollowedbymanualcorrection. Subsequently,GPT-4,guidedbythe
problemdescriptionsandreferencesolutions,generatesmultipletestcases,whicharealsorefined
21. Data Collection 2. Semi-Automated Pipeline
Reference Solution
Instruction: ...generate
Real-World
6 high-coverage and 4 Large def groundTruth(file_path,tag_name)
Queries corner test cases … root =
Language ET.parse(file_path).getroot()
Model . . .
Problems in 6 Domains for … in root.findall(tag_name):
33,120 PA rou bt lo e mF sil tering Front-End sG oe lun te ior na t ae n a d r e t ud ra nta d_ ali ts at _.a lip sp tend(…)
• Testable Algorithm 10 test cases
Test Cases
• Useful Data Science
Human
• Deterministic
Artificial Intelligence Annotated
Mannully def testcase1():
Selecting Software Engineering …
Annotators fixes all errors in assert groundTruth …
402 High-Quality Problems System Administration the solution and test cases
Figure2: OverviewofNATURALCODEBENCH. 1)DataCollection: collectingreal-worldqueries
fromcodingonlineservicesandselectinghigh-qualityproblemsfromthequeriesbyGPT-3.5and
humanannotators. 2)Semi-AutomatedPipeline: improvingefficiencyofconstructingevaluation
frameworkbygeneratingasolutionandtestcaseswithLLMsandthenhavingthemcorrectedby
humanannotators.
withmanualcorrection,foreachproblem. Consequently,theannotatorsareonlyrequiredtocorrect
anyerrors,substantiallyreducingthetimeandmanpowerrequired. Comparativeexperimentsreveal
thatoursemi-automatedpipelinecanquadrupletheconstructionspeedoftheevaluationframework,
asevidencedbytestsinvolvingprogrammingexpertswithorwithoutthepipeline.
BasedonNCB,weconductextensiveexperimentsonavarietyrangeofLLMs,encompassing39APIs
oropenmodels.TheresultsindicatethatalthoughcertainLLMsdemonstratecomparableperformance
onestablishedbenchmarkslikeHumanEval,theyexhibitsignificantperformancedisparitieswhen
evaluated using NCB. It suggests that there may be inadequate focus on optimizing LLMs for
practicalcodingapplications,orhaveconductedover-specifiedoptimizationonHumanEval-style
problems. More importantly, even the best-performing GPT-4 only reaches about a pass rate of
53%,demonstratingalargeroomforLLMstoimprovetheircodingskillstofacereal-worldcoding
challenges.
Tofacilitatecommunityresearch, wepackupthewhole NCB testingenvironmentintoadocker
imageandmakeitsdevelopmentsetpubliclyavailable. Tosumupourcontributions:
• We propose NATURALCODEBENCH, a benchmark that aligns with real-world applications,
comprising402problemsinPythonandJavaacross6domains. Weopensource140problems
(70Python,70Java)asthedevelopmentsetof NCB forresearchpurposes,butkeepthe262
problemsofthetestsetclosedtoavoidcontamination.
• Weintroduceasemi-automatedpipelinefortheconstructionofcodesynthesisbenchmarks,which
significantlyreducestimeandmanpowercostswithoutcompromisingthequalityoftestcases.
Comparativeexperimentsrevealthatoursemi-automatedpipelinecanquadrupletheconstruction
speedoftheevaluationframework
• Wesystematicallybenchmarkthecodegenerationcapabilitiesof39LLMsusingNCB.Besides
quantitativeevaluation,wecarryoutadeepinsightintothepresentstageofdevelopmentinLLMs
forcodegeneration,andoutlinepotentialpathwaysforfutureprogress.
2 BenchmarkConstruction
TheoverviewofNCBisshowninFigure2. ThepipelineofconstructingNCBconsistsoffoursteps:
1)collectingandfilteringhigh-qualityproblemsfromonlineservices(Section2.1)2)constructing
a complete evaluation framework through a semi-automated pipeline (Section 2.2) 3) designing
promptstoaligndifferentmodels(Section2.3)4)translatingallproblemsandinstructionstoproduce
bilingualversions(Section2.4).
32.1 ProblemSelection
CollectingReal-WorldProblems. Toestablishameaningfulandpracticalbenchmark,wecentered
oncollectingreal-worldcodeproblemsfrequentlyencounteredbyusers. Toachievethis,theseed
problemsofNCBarecleanedfromthequeriesincodingonlineservices. Apartofusershavegranted
permissionfortheirdatatobeutilizedexclusivelyforresearchpurposes. Wehavestrictlyadheredto
thisdirectivebycollectingonlytherelevantdatafromtheseconsentingusersandhaveimplemented
robustde-identificationmeasurestoeliminateanypossibilityofinformationleakage. Wecollecta
variedcollectionofqueries,spanningmultipleprogramminglanguages,problemtypes,andlevels
ofcomplexity. Thisdiversityensuresthatourbenchmarkaccuratelyreflectsabroadrangeofcode
issuesusersencounteringinpractice. WespecificallyconcentratedonqueriesrelatedtoPythonand
Java,chosenfortheirwidespreaduseindifferentdomains.
FilteringTestableProblems.Whileit’spossibletosourceinexhaustiblequeriesfromonlineservices,
manyofthesequeriesposedbyusersareeitheroflowvalueorchallengingtotestthesolutionof
these queries. For instance, some users may only seek basic clarifications on a built-in function,
whileothersmaynotclearlyarticulatetheirobjectives. Tosieveoutunsuitablequeriesforourtesting,
we’ve implemented a two-step filtering process. Initially, we employ GPT-3.5 to filter out low-
qualityqueries,whichsavesonlabour. Thisisachievedbyaddingspecificcriteriaintheinstruction,
instructingGPT-3.5toabandonthoseproblemsthatcannotmeetallspecifiedrequirements. These
criteria are as follows: 1) Each query must involve at least one task, where the user requests the
model’sassistanceinsolvingoneormoreproblems. 2)Eachqueryshouldbeassociatedwithseveral
input-outputpairs, ensuringthatagiveninputcorrespondtoasingular, definitiveoutput. 3)The
querymustnotcontainanyelementsofrandomnessoruncertainty. Thespecificsoftheinstruction
aredetailedin(AppendixA).Followingthisautomatedpre-screening,weconductamanualreview
tofurtherrefinetheselection,adheringtotheoutlinedcriteria. Thisprocessyieldsafinalsetof201
uniquePythonand201uniqueJavaproblems. Itisnoteworthythatover80%oftheinitialqueries
failedtomeetourstringentrequirements.
2.2 Semi-automatedPipeline
Inthissection,wewillintroduceoursemi-automatedpipeline. Togeneratestructurallycomplexand
accuratetestcasesbyGPT-4,itisfirstnecessarytodeterminetheargumentsandreturnvaluesof
functions,aswellasthenamesofobjects. Therefore,acompletelyaccuratereferencesolutionis
requiredinitially. WegenerateasolutionusingGPT-4,thenmanuallycorrectallerrors. Afterthis,
basedontheproblemdescriptionandthereferencesolution,weinstructGPT-4togeneratemultiple
testcases. Thesearethenreviewedbyprogrammingexpertswhocorrecterrorsandsupplementany
deficienciesinthegeneratedtestcases.
Generating and Rewriting Reference Solution. GPT-4 is instructed to generate a solution for
eachprobleminNCB.ItisimportanttonotethatwhileGPT-4ishighlycapable,itisnotinfallible.
Therefore,eachsolutiongeneratedbyGPT-4ismeticulouslyexaminedbyexpertprogrammersto
ensurecorrectness.Incaseswherethegeneratedcodecontainserrors,theexpertprogrammersrewrite
thecodetorectifytheseissues. Thisprocessensuresthequalityofthereferencesolutions. Even
thoughwedidnotusethereferencesolutioninNCBforevaluation,weprovidedthemtofacilitate
thegenerationoftestcasesandfutureresearch.
BuildHigh-CoverageandCornerEvaluation. WeemployGPT-4togenerateevaluationcodes
for each problem. We construct a prompt using 1) the description of the problem for GPT-4 to
inspect;2)thereferencesolutiontodemonstratethenamesandformatsinthecode;3)aninstruction
toencourageGPT-4tocomeupwitheffectivetestcases. Specifically, eachpromptstartwithan
instruction thatask GPT-4 toproduce ten testcases based on thedescription of problemand the
reference solution. Then, we present both the description of problem and its reference solution.
WefinalizethepromptwithainitialsegmentoftheevaluationcodetoassistGPT-4inaccurately
generatingthedesiredcodeformat. OurobjectiveistoharnessGPT-4’sadvancedcomprehension
andanalyticalabilitiestolearnvalidformatinthecodeandessentialfunctionalitiesofthereference
solutiontoenablethegenerationofsuperiortestcasesthatareadeptatuncoveringlatenterrorsin
code.
4InstructionInformation Evaluation
Benchmark
#Problem Domain #DataType#Word Source #TestCase Method
Humaneval[13] 164 Algorithm 5 23.0 Hand-Written 7.7 Test-Case
MBPP[7] 974 ProgramBasics 5 15.7 Hand-Written 3.0 Test-Case
DS-1000[32] 1,000 DataSci. 6 140.0 StackOverflow 1.6 Test-Case+SFC.
APPS[24] 10,000 Algorithm 5 293.2 Competitions 13.2 Test-Case
Humaneval+[37] 164 Algorithm 5 23.0 Hand-Written 764.1 AugmentedTestCases
NaturalCodeBench 402 Application 6 78.3 OnlineServices 9.3 Test-Case
Table1: ComparisonbetweenNATURALCODEBENCHandotherbenchmarksforcodegeneration.
Acompleteandeffectivetestshouldseektoidentifypotentialbugsatdifferentlocationsinthecode,
while also finding inputs that might trigger errors in the code. High coverage ensures that each
testcaseexaminesmorecodeandbranches,therebyfacilitatingthediscoveryofconcealederrors.
Meanwhile,itisoftenobservedthatcornervaluesinaproblem’sinputaremostpronetotriggercode
errors. Consequently,ourinstructionwillcausesomeofthetestcasesgeneratedbyGPT-4tohave
highercoverage,whiletheotherpartwillbesomecornervaluescontainedintheproblem,soasto
obtainmoreeffectivetestcases.
Subsequently, expertprogrammersreviewandcorrectanytestcaseswithformattingandanswer
errors. Toensurethatthefinalevaluationframeworkiserror-free.
2.3 AlignmentBetweenDifferentModels
Incontrastto theproblemformatin Humaneval, the majorityofproblemsin ourbenchmarkare
composed in natural language by actual users. Consequently, there is no predetermined naming
conventionforfunctionsorclassescreatedbymodels. Thisdivergencecanleadtoinconsistencies
between the names generated by LLMs and those referenced in test cases. To address this issue
ofnamemisalignment,wepresentarepresentativetestcasethatincludesthedesignatedfunction
or class name and its usage within the test. We then instruct the LLMs to adhere to the naming
conventionspecifiedintheprovidedtestcasewhengeneratingsolutions. Itisimportanttonotethat
thetestcasesutilizedforsolutiongenerationarenotemployedinsubsequenttestingphases. The
detailsoftheinstructionisshowedinAppendixA.
2.4 BuildingBilingualBenchmark
ThemajorityofthequestionswecollectedfromonlineservicesareinChinese,whichisnotfairfor
theLLMsthatareprimarilydesignedforEnglish. Therefore,wetranslatealltheproblems,resulting
inbothChineseandEnglishversions.
3 DatasetStatistics
WeprovidemoredetailedstatisticsinTable2. NCBcomprisesatotalof402problemscollectedfrom
onlineservices,with201problemsinPythonand201inJava,spanningacross6domains: Database,
ArtificialIntelligence,DataScience,AlgorithmandDataStructure,Front-End,SoftwareEngineering,
andSystemAdministration. ThisdiversityalsoleadstocomplexinputdatatypesinNCB,which
areclassifiedinto9categories: number(int/float/boolean),string,list(array),dict,tensor(matrix),
dataframe(table),plaintextfile,image,andspecialformatfile. Thefirstfourarethemostcommon
andsimplestdatatypes. Sinceabooleancanberepresentedby1and0, weconsideritasatype
ofnumber. Matrixandlistaretwosimilartypesofdata, buttheyarecategorizedseparatelydue
todifferencesintheirusagescenarios. Duetothecurrentpopularityofdeeplearning,tensorhas
becomeaverycommondataformat. Therefore,wehavedesignatedaseparatecategoryfortensor
andhaveincludedmatrixwithinthiscategory. Thelastthreeareallfiletypes,differentiatedbytheir
processingmethods. Thecontentofaplaintextfileistextandcanbedirectlyread. Figuresrequire
processingofeachpixelvalue. Aspecialformatfilereferstofilesthatrequirespecificmethodsfor
processing,suchasPDFandDOCX.
5#Problems Avg.#TestCases
Dataset Test Dev Total Test Dev Total
Software 88 44 132 9.7 8.6 9.3
DataSci. 68 32 100 9.6 8.6 9.3
Algorithm 73 22 95 9.5 8.8 9.3
Sys.Admin. 22 17 33 9.6 8.5 9.1
AI.System 13 15 28 9.6 9.1 9.3
Front-End 3 11 14 10.0 8.7 9.0
Total/Avg. 262 140 402 9.6 8.7 9.3
Table2: DetailedstatisticsofNATURALCODEBENCH.
Each problem within the dataset has been carefully curated with a set of test cases to assess the
correctnessofsolutions. Onaverage,thereare9.3testcasesassociatedwitheachproblem. These
casesarestrategicallydesigned,withabout60%focusedonenhancingstatementandbranchcoverage,
andtheremaining40%dedicatedtoevaluatingtherobustnessofsolutionsagainstcornervalues. The
averagewordcountforeachproblemintheNCBis78.3.
ComparedwithOtherBenchmark. Table1comparesNCBtootherbenchmarks. Itisnoteworthy
thatourbenchmarkoffersasubstantialsupplementtocurrentbenchmarksintermsofbothproblem
anddatatypes. UnlikeHumanevalandMBPP,whichconsistof96.9%and89.5%algorithmicand
basic programming problems respectively, our benchmark features a more balanced distribution
acrosseachdomain.
Inaddition, NCB includemoredatatypes. Furthermore, NCB focusesonassessingthemodel’s
abilitytohandlemultiplefileformats,atypeofdatathatisbothverycommonlyusedindailylife
andrelativelychallengingtoprocess. Wenotethattheproblemsinvolvingfileshavefewertestcases,
sinceGPT-4stillstrugglestofullygeneratevarioustypesoffile. Thisisalsomorechallengingfor
humanannotatorstodesigncomparedtosimplerdatatypes.
On the other hand, NCB is also limited by its size due to the high costs of problems collection
andtheconstructionoftheevaluationframework. Wearecontinuouslyworkingonexpandingour
benchmark.
4 Experiments
4.1 Setup
We conducted comprehensive evaluations of 33 popular state-of-the-art models. For proprietary
models,ourfocuswasonOpenAI’sGPT-4-Turbo-0125,GPT-4-Turbo-1106,GPT-4,GPT-3.5-Turbo,
Anthropic’sClaude-2,ZhipuAI’sCodeGeeX3. Inthecaseofopen-sourcemodels,weperformed
evaluationsusingthevLLM[31]andFastChat[69]framework. Ourevaluationprimarilyutilizes
pass@k[13]asthemetrictoaccuratelyassessthefunctionalcorrectnessofcodegeneratedbythese
models. Forkequalto1,weemploygreedy-searchdecoding. Forrandomsampling,wedemonstrate
thebestpass@kresultsofthebest-performingmodelswitheachLLMfamilyforeachk ∈{10,50},
wherethesamplingtemperatureissetto0.2andtoppto0.9.
Oursemi-automatedpipelineiscapableofreducingthetimerequiredforbenchmarkconstruction
without compromising the quality of test cases. This paper primarily focuses on evaluating the
efficiency of benchmark construction and the quality of test cases. Specifically, we adopt code
coverage[26], awidelyusedmetricforassessingtheeffectivenessoftesting, asthecriterionfor
evaluatingthequalityoftestcases.Weinvitefiveprogrammingexperts,eachtaskedwithconstructing
thesamefiveproblems. Initially,weaskeachexperttomanuallywriteastandardsolutionand5test
cases. Subsequently,forthesameproblems,theycompletethewritingofstandardsolutionsandtest
casesusingthesemi-automatedpipeline. Asitischallengingtoensureidenticaltestcasecoverage,
werequirethatthetestcaseswrittenunderbothmethodsshouldnothaveacodecoverageofless
than80%. Then,forthesakeofconvenientcomparison,wecalculatethescoresforeachconstruction
6Model NCB(zh) NCB(en) NCBTotal HumanEval
Size ∆Rank
Python Java Total Python Java Total Score Rank Score Rank
APILLMs
GPT-4[45] N/A 53.4 51.1 52.3 55.7 51.1 53.4 52.8 1 80.5 5 4
GPT-4-Turbo-0125[45] N/A 51.4 58.6 55.0 48.6 51.4 50.0 52.5 2 87.2 1 -1
GPT-4-Turbo-1106[45] N/A 47.3 51.9 49.6 51.9 55.0 53.5 51.5 3 81.7 3 0
GPT-3.5-Turbo[46] N/A 39.7 38.9 39.3 42.0 42.0 42.0 40.7 8 65.2 18 10
Claude-3-Opus[5] N/A 45.0 50.4 47.7 48.9 48.9 48.9 48.3 4 84.9 2 -2
Claude-3-Sonnet[5] N/A 44.6 35.5 40.1 40.5 35.1 37.8 38.9 9 73.0 11 2
Claude-3-Haiku[5] N/A 41.3 35.9 38.6 36.9 30.5 33.7 36.2 11 75.9 9 -2
Claude-2.1[4] N/A 33.6 32.8 33.2 34.4 36.6 35.5 34.4 13 71.2 16 3
ChatGLM-4[68;19] N/A 43.5 45.3 44.4 41.5 45.3 43.4 43.9 5 72.6 12 7
Gemini-1.5-Pro[10] N/A 41.5 43.1 42.3 45.0 39.7 42.3 42.3 7 71.9 14 7
CodeGeeX3[70] N/A 29.0 29.0 29.0 36.6 32.8 34.7 31.9 18 69.5 17 -1
OpenLLMs
33B 44.3 38.9 41.6 44.3 44.3 44.3 43.0 6 79.3 6 0
Deepseek-Coder-Instruct[23]
6.7B 38.9 29.8 34.4 35.9 35.9 35.9 35.1 12 78.6 7 -5
1.3B 18.3 24.4 21.4 27.5 25.2 26.4 23.9 22 65.2 19 -3
70B 39.1 34.4 36.7 35.4 39.7 37.5 37.1 10 81.7 4 -6
Llama-3-Instruct[2]
8B 35.9 21.5 28.7 19.7 21.7 20.7 24.7 21 62.2 21 0
67B 35.9 28.2 32.1 35.1 33.6 34.4 33.2 14 78.3 8 -6
Deepseek-Chat[15]
7B 3.8 12.2 8.0 8.4 19.1 13.8 10.9 30 48.2 26 -4
70B 35.1 32.1 33.6 32.8 30.5 31.7 32.6 15 72.0 13 -2
Codellama-Instruct[51]
34B 23.7 17.6 20.7 28.2 17.6 22.9 21.8 24 51.8 25 1
13B 20.6 16.8 18.7 26.7 19.1 22.9 20.8 25 42.7 26 1
7B 16.8 17.6 17.2 21.4 17.6 19.5 18.4 26 34.8 31 5
Phind-Codellama[49] 34B 34.4 29.0 31.7 33.6 32.1 32.9 32.3 16 71.3 15 -1
Qwen-1.5[9] 110B 35.4 28.2 31.8 38.5 26.7 32.6 32.2 17 52.4 24 7
72B 28.2 29.8 29.0 24.4 29.0 26.7 27.9 19 64.6 20 1
Qwen-Chat[8]
7B 11.5 13.0 12.3 16.0 11.5 13.8 13.0 28 37.2 30 2
34B 24.4 22.9 23.7 29.8 22.1 26.0 24.8 20 73.2 10 -10
WizardCoder[41]
15B 29.0 17.6 23.3 25.2 19.1 22.2 22.7 23 59.8 22 -1
StarCoder[33] 15.5B 13.0 13.0 13.0 16.8 9.9 13.4 13.2 27 40.8 29 2
Mistral-Instruct[28] 7B 7.6 9.9 8.8 11.5 19.1 15.3 12.0 29 28.7 34 5
16B 0.8 11.5 6.2 2.3 13.0 7.7 6.9 31 19.5 36 5
CodeGen2[43]
7B 2.3 5.3 3.8 6.9 5.3 6.1 5.0 32 18.3 37 5
3.7B 0.0 0.0 0.0 0.0 3.1 1.6 0.8 38 15.9 38 0
1B 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39 11.0 39 0
2.7B 5.3 3.1 4.2 3.1 5.3 4.2 4.2 33 53.7 23 -10
Phi[34]
1.3B 0.0 0.8 0.4 3.8 0.0 1.9 1.2 37 41.4 28 -9
16B 0.8 5.3 3.1 0.3 9.2 4.8 3.9 34 32.9 32 -2
CodeGen[44]
6B 0.0 0.0 0.0 2.3 3.8 3.1 1.5 35 29.3 33 -2
2B 0.0 0.0 0.0 2.3 3.8 3.1 1.5 36 24.4 35 -1
Table3: EvaluatingLLMsonthetestsetof NATURALCODEBENCH. Allresultsarepass@1on
greedydecoding. DevsetresultsarereportedinTable6. ComparedtoHumanEval[13],someLLMs
presentsignificantvariations
7Hand-Written Semi-Automated
TimeCost Line Branch Score TimeCost Line Branch Score
Expert_1 179.5 97.6 95.9 10.8 36.0 97.0 96.9 53.9
Expert_2 195.0 97.6 95.0 9.9 41.0 88.1 91.7 43.9
Expert_3 145.0 84.5 84.0 11.6 26.0 82.0 85.0 64.2
Expert_4 180.0 90.9 100.0 10.6 41.0 84.4 91.7 42.9
Expert_5 180.0 98.1 83.3 10.1 56.0 100.0 100.0 35.7
Total/Avg. 175.9 93.7 91.6 10.5 40.0 90.3 93.1 48.1
Table4: TestcaseconstructioncomparisonbetweenbySemi-AutomatedPipelineandHand-Written
methodinastraightforwardmanner,whichisoutlinedasfollows:
LineCov.+BranchCov.
Score= ∗10
TimeCost
4.2 ResultsofLLMs
Table 3 and Table 6 shows the pass@1 results on the test set and dev set of NCB, respectively.
Considering the high consistency of results, we primarily analyze the results on the test set. As
expected,OpenAI’sGPT-4achievesthehighestscoreof52.8%. TheperformanceofGPT-4-Turbois
veryclosetothatofGPT-4,differingonlyby1.3%,withGPT-4-TurboperformingbetterinJava
butshowingalargerdifferenceinPython. Amongtheopen-sourcemodels,DeepSeek-Coder-33B-
Instructperformsthebest, reachingascoreof43.0%. However, the9.8%scoregapwithGPT-4
remainssignificant. Ontheotherhand,itsurpassesthe40.7%achievedbyGPT-3.5,exceedingitby
2.3%. Insummary,theperformanceofstate-of-the-artopen-sourcemodelsisnowbetweenGPT-3.5
andGPT-4,yetthemajorityofopen-sourcemodelsstilldonotmatchtheperformanceofGPT-3.5.
When compared to a perfect score of 100%, it is observed that even the best-performing model,
GPT-4,stillfallssignificantlyshort. ThisisincontrasttoitsperformanceinHumanEval,whereithas
approached90%.
ComparingtheperformanceofmodelsinChineseandEnglishversions,itisevidentthatthevast
majorityofmodelsperformbetterinEnglish. Thisholdstrueevenforthetopmodels,GPT-4and
GPT-4-Turbo,whichoutperformtheiraveragescoresinChineseby1.1%and3.9%,respectively.
Furthermore, Table 3 systematically presents the performance of various open-source models at
differentscales. Modelssmallerthan10Bscoredbetween0.0%and23.9%,modelsbetween10B
and30Bscoredbetween3.9%and35.1%,modelsbetween30Band60Bscoredbetween21.8%and
43.0%,andmodelslargerthan60Bscoredbetween27.9%and33.2%. Itisevidentthatthescaleof
themodelstillhasasignificantimpactonperformance. Largermodelsgenerallyoutperformsmaller
models,indicatingthatincreasingscalecanindeedenhanceamodel’scapabilities. However,this
isnottosaythatscaleiseverything;morerefineddataandtrainingstrategiescanalsosignificantly
impactamodel’sperformance. Somesmallermodels,suchasDeepSeek-Coder-6.7B-Instruct,can
outperformthoselargerthan30Bbyapproximately2.8%andthoselargerthan60Bbyapproximately
1.9%.
Table 5 shows the pass@k results of best-performing LLMs with each LLM family on NCB,
wherek ∈{10,50}. Wefoundthatunderrandomsampling,thescoresofsomemodelsincreased
significantly. For instance, Codellama-70B-Instruct, unlike its performance on pass@1, clearly
outperformedGPT-3.5onbothPass@10andPass@50.
WecomparedthePythonscoresonthetestsetof NCB withtheperformancesofmodelsonHu-
maneval,asshownintheFigure1. Mostmodelsarelocatedintheuppertriangularareaofthegraph,
withmanymodelsscoringhighonHumanevalbutexhibitingrelativelylowerperformanceonNCB.
4.3 PerformancemismatchonHumanEvalandNCB
WeshowtherankordersofalltestedLLMsinTable3withregardtoHumanEvalandNCB,aswell
asthedifferenceofrankorders. Wealsoplotthecorrespondingperformancesontwobenchmarksto
scatterdiagraminFigure1. Basedonthetableandfigure,wehavesomeinterestingfindings.
8PerformancesofmostLLMsontwobenchmarksgrowlinearlyproportional,andthedifferencesof
scores’rankorderarearound0. ItdemonstratesthatNCBcanindeedreflectthecodingabilitiesof
LLMsasHumanEvaldoesinmostcases.
However,weobservethatsomemodelseries,notablythePhi,Deepseek-Chat,andWizardCoder,
consistentlyexhibitapropensitytoachievesuperiorrankingsontheHumanevaldatasetasopposed
totheNCBacrossvariousscales,asshownintheTable 3. Additionalmodelfamilies,including
CodeGenandLlama-3-Instruct,similarlydisplaythetrend,thoughtoareduceddegree.
Theremightbeafewpotentialhypothesesfortheobservation. First,asproblemsinNCBaremore
difficultandderivedfromnaturaluserprompts,comparedtothoseinHumanEval,LLMswithpoorer
generalizationandinstruction-followingcapabilitiestendtoperformworse. Wefindinpreliminary
experimentsthatproblemsinNCBcannotbeproperlysolvedbypre-trainedbaseLLMsviamere
in-contextlearningasHumanEvaldoes,whichindicatesthattosolveNCBproblemsrequiresstronger
alignmentandgeneralizabilitythanHumanEvalneeds.
Second,itispossiblethattrainingsetsofsomeLLMsareover-specifiedlyoptimizedforHumanEval-
styleproblems. Ononehand,pre-trainingdataofcertainLLMsmaybecontaminated. AsGPT-4[45]
reported,25%ofHumanEvalhasbeencontaminatedintheirpre-trainingcorpus. Ontheotherhand,
instructionfine-tuningdatasetmayalsobepolluted. Forexample,Phi[34]reportsaconsiderable
amountofsyntheticpromptsresonatingtosometestsamplesinHumanEval. In[64],theauthors
reportleakageunidentifiablebyn-gramoverlapwhenusingpopularrephrasingtechniquestocreate
trainingsets. TheperformancediscrepancybetweenHumanEvalandNCBinourexperimentsisalso
anevidenceofthepotentialcontamination.
4.4 ResultsofSemi-automatedConstruction
InTable 4,wecanobservethatthecoverageofhand-writtentestcasesisalmostidenticaltothat
of test cases constructed through a semi-automatic pipeline, yet the time required for the former
significantlyexceedsthetimeneededforconstructingtestcasesviathesemi-automaticpipeline.
Specifically,testcasescanbeconstructedviathesemi-automatedpipelineinjust40minutes,whereas
manual writing requires 175.9 minutes, a difference of more than 4x. Consequently, the scores
obtainedfortestcasesconstructedusingthesemi-automatedpipelinearefarhigherthanthosefor
manuallywrittentestcases,withanaveragedifferenceof37.6. Insummary,constructingtestcases
throughthesemi-automaticframeworkcanachievesignificantlyhigherefficiencywithoutsubstantial
lossinqualitycomparedtomanualwriting.
5 RelatedWork
LLMsforcode. SignificantadvancementsinLLMs (57, 18, 11)are transforming everydaylife,
particularly in the field of coding, driven by the vast amount of openly available codebases and
thepushtoenhanceproductivityamongdevelopers. Code-specificLLMshaveproventheirability
to perform various tasks such as code generation (13, 27, 35), program repair (29, 58, 60, 61),
automated testing (16, 17, 39, 59, 63), code translation (52, 53) and code summarization (1, 40).
Notably,prominentLLMsincludingCODEX[13],CodeGen[44],INCODER[22],andPolyCoder
[62]havebeendevelopedandrigorouslytested, particularlyincodegeneration. Thisarea, often
referredtoastheultimategoalincomputerscienceresearchsincetheearlydaysofAIinthe1950s,
involves the model producing code snippets from natural language explanations of the required
functionality. ThelandscapeofcodeLLMsiscurrentlyexperiencingasurge,withnewmodelsbeing
introducedregularly. Thisincludesbothproprietaryones(42,45)andopen-sourceones(36,44,55,
33,3,54),markingatrendoffrequentreleasesinthisdomain.
CodeSynthesisBenchmarks. Asthecapabilitiesofmodelsadvance,researchersaredeveloping
morechallengingandversatilebenchmarksforcodegeneration. Initially,theearlierfocuswason
domain-specificlanguages[67],whilethesubsequenteffortlaunchedaText-to-SQLbenchmarkto
evaluatethecapacityforgeneratingcomprehensiveSQLprograms[66]. Ainvestigation[65]assesses
theabilitytocomposebriefyetbroadlyapplicablePythonsnippets. Morerecentstudies(25,35)
havetestedmodels’proficiencyinsolvingcompetitiveprogrammingchallengesusingPython. A
leadingandextensivelyresearchedbenchmarkinthisdomainisHumanEval[13],whichfeatures164
Pythonfunctionsignaturesaccompaniedbydocstringsandcorrespondingtestcasesforvalidating
9correctness. Additionally,eachprobleminHumanEvalincludesareferencesolution. TheMBPP
[7]dataset,anotherPython-centriccollection,wasdevelopedbyhavingparticipantscontribute974
programming challenges. Each challenge encompasses a problem description (i.e., docstring), a
functionsignature,andthreetestcases. Therearealsobenchmarksforotherprogramminglanguages,
suchasHumanEval-X[70]forC++,JavaScript,andGo,CodeContests[35]forC++andJava,and
MultiPL-E[12],whichexpandsHumanEvalandMBPPto18languages.
Morerecenteffortshaveintroducedbenchmarksthatmorecloselymirrorreal-worldcodingscenarios
thatrequireinteractivecoding. Forexample,AgentBench[38]introducesinteractivetasksregarding
unixshellandMySQL.SWE-Bench[30]compilesGitHubissues,theirassociatedcodebases,and
tests,togaugeLLMs’effectivenessinpracticalsoftwareengineeringtasks.
6 Conclusion
We propose NATURALCODEBENCH for evaluating the code generating ability of LLMs. Our
benchmarkcomprisesatotalof402problemsselectedfromcodingonlineservices,anditsupports
automaticevaluationofcodegeneratedbyLLMs. Wehavealsoproposedasemi-automatedpipeline
for efficiently constructing the entire benchmark, achieving an efficiency gain of more than 4x
compared to manual construction. We hope that NCB can provide a fair environment for the
comparisonbetweenmodels,andourpiplinecanalsoprovideinspirationtoothercomplextasksor
domainswhereevaluationcostsarehigh.
Limitations
Here,wediscussseverallimitationsofthiswork.
Tocovermoredomains. Althoughourproblemsarederivedfromreal-worldapplicationscenarios,
duetothedifficultyofconstructingaccurateandefficientevaluationenvironments,weareunableto
testsometypesofproblems,suchasthoseinvolvinginterfacecreation,webservices,etc.,which
arealsocommonproblemtypesinactualapplications. Thisresultsinsomebiasesinourevaluation,
whichmayaffecttheaccuracyoftheevaluationofcertainmodels. Wewillleavetheseissuesfor
futureresearch.
To reduce the cost. The semi-automated pipeline can significantly reduce the time and human
resourcesrequiredtoconstructanevaluationframework,butthecostofaccessingOpenAI’sAPI
remainsexpensive,anditdoesnotcompletelyeliminatetheuseofhumanresources.
References
[1] T.AhmedandP.Devanbu. Few-shottrainingllmsforproject-specificcode-summarization. In
Proceedingsofthe37thIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineer-
ing,ASE’22,NewYork,NY,USA,2023.AssociationforComputingMachinery.
[2] AI@Meta. Llama3modelcard. 2024.
[3] Anonymous. Wizardcoder: Empoweringcodelargelanguagemodelswithevol-instruct. InThe
TwelfthInternationalConferenceonLearningRepresentations,2024.
[4] Anthropic. Claude-2,2023.
[5] Anthropic. Introducing the claude 3 family. https://www.anthropic.com/news/
claude-3-family,2023. Accessed: 2024-04-28.
[6] B.Athiwaratkun,S.K.Gouda,Z.Wang,X.Li,Y.Tian,M.Tan,W.U.Ahmad,S.Wang,Q.Sun,
M.Shang,S.K.Gonugondla,H.Ding,V.Kumar,N.Fulton,A.Farahani,S.Jain,R.Giaquinto,
H.Qian,M.K.Ramanathan,R.Nallapati,B.Ray,P.Bhatia,S.Sengupta,D.Roth,andB.Xiang.
Multi-lingualevaluationofcodegenerationmodels. InTheEleventhInternationalConference
onLearningRepresentations,2023.
10[7] J.Austin,A.Odena,M.Nye,M.Bosma,H.Michalewski,D.Dohan,E.Jiang,C.Cai,M.Terry,
Q.Le,andC.Sutton. Programsynthesiswithlargelanguagemodels,2021.
[8] J.Bai,S.Bai,Y.Chu,Z.Cui,K.Dang,X.Deng,Y.Fan,W.Ge,Y.Han,F.Huang,etal. Qwen
technicalreport. arXivpreprintarXiv:2309.16609,2023.
[9] J.Bai,S.Bai,Y.Chu,Z.Cui,K.Dang,X.Deng,Y.Fan,W.Ge,Y.Han,F.Huang,B.Hui,L.Ji,
M.Li,J.Lin,R.Lin,D.Liu,G.Liu,C.Lu,K.Lu,J.Ma,R.Men,X.Ren,X.Ren,C.Tan,
S.Tan,J.Tu,P.Wang,S.Wang,W.Wang,S.Wu,B.Xu,J.Xu,A.Yang,H.Yang,J.Yang,
S.Yang,Y.Yao,B.Yu,H.Yuan,Z.Yuan,J.Zhang,X.Zhang,Y.Zhang,Z.Zhang,C.Zhou,
J.Zhou,X.Zhou,andT.Zhu. Qwentechnicalreport. arXivpreprintarXiv:2309.16609,2023.
[10] G.A.Blog. Googlegemini: Nextgenerationmodel. https://blog.google/technology/
ai/google-gemini-next-generation-model-february-2024/,Feb.2024.
[11] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
P.Shyam,G.Sastry,A.Askell,S.Agarwal,A.Herbert-Voss,G.Krueger,T.Henighan,R.Child,
A.Ramesh,D.Ziegler,J.Wu,C.Winter,C.Hesse,M.Chen,E.Sigler,M.Litwin,S.Gray,
B.Chess,J.Clark,C.Berner,S.McCandlish,A.Radford,I.Sutskever,andD.Amodei. Lan-
guage models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan,
andH.Lin,editors,AdvancesinNeuralInformationProcessingSystems,volume33,pages
1877–1901.CurranAssociates,Inc.,2020.
[12] F.Cassano,J.Gouwar,D.Nguyen,S.Nguyen,L.Phipps-Costin,D.Pinckney,M.-H.Yee,Y.Zi,
C.J.Anderson,M.Q.Feldman,A.Guha,M.Greenberg,andA.Jangda. Multipl-e: Ascalable
andextensibleapproachtobenchmarkingneuralcodegeneration,2022.
[13] M.Chen,J.Tworek,H.Jun,Q.Yuan,H.P.deOliveiraPinto,J.Kaplan,H.Edwards,Y.Burda,
N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry,
P.Mishkin,B.Chan,S.Gray,N.Ryder,M.Pavlov,A.Power,L.Kaiser,M.Bavarian,C.Winter,
P.Tillet,F.P.Such,D.Cummings,M.Plappert,F.Chantzis,E.Barnes,A.Herbert-Voss,W.H.
Guss,A.Nichol,A.Paino,N.Tezak,J.Tang,I.Babuschkin,S.Balaji,S.Jain,W.Saunders,
C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight,
M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish,
I.Sutskever,andW.Zaremba. Evaluatinglargelanguagemodelstrainedoncode,2021.
[14] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.
Chung,C.Sutton,S.Gehrmann,etal.Palm:Scalinglanguagemodelingwithpathways.Journal
ofMachineLearningResearch,24(240):1–113,2023.
[15] DeepSeek-AI. Deepseekllm: Scalingopen-sourcelanguagemodelswithlongtermism. arXiv
preprintarXiv:2401.02954,2024.
[16] Y.Deng, C.S.Xia, H.Peng, C.Yang, andL.Zhang. Largelanguagemodelsarezero-shot
fuzzers: Fuzzingdeep-learninglibrariesvialargelanguagemodels. InProceedingsofthe32nd
ACMSIGSOFTInternationalSymposiumonSoftwareTestingandAnalysis,ISSTA2023,page
423–435,NewYork,NY,USA,2023.AssociationforComputingMachinery.
[17] Y.Deng,C.S.Xia,C.Yang,S.D.Zhang,S.Yang,andL.Zhang. Largelanguagemodelsare
edge-casefuzzers: Testingdeeplearninglibrariesviafuzzgpt,2023.
[18] J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova. BERT:Pre-trainingofdeepbidirectional
transformers for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors,
Proceedings of the 2019 Conference of the North American Chapter of the Association for
ComputationalLinguistics: HumanLanguageTechnologies, Volume1(LongandShortPa-
pers),pages4171–4186,Minneapolis,Minnesota,June2019.AssociationforComputational
Linguistics.
[19] Z.Du,Y.Qian,X.Liu,M.Ding,J.Qiu,Z.Yang,andJ.Tang. GLM:Generallanguagemodel
pretrainingwithautoregressiveblankinfilling. InS.Muresan,P.Nakov,andA.Villavicencio,
editors, Proceedings of the 60th Annual Meeting of the Association for Computational Lin-
guistics(Volume1: LongPapers),pages320–335,Dublin,Ireland,May2022.Associationfor
ComputationalLinguistics.
11[20] A.EghbaliandM.Pradel. Crystalbleu: Preciselyandefficientlymeasuringthesimilarityof
code. InProceedingsofthe37thIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering,ASE’22,NewYork,NY,USA,2023.AssociationforComputingMachinery.
[21] M.Evtikhiev,E.Bogomolov,Y.Sokolov, andT.Bryksin. Outofthebleu: Howshouldwe
assessqualityofthecodegenerationmodels? JournalofSystemsandSoftware,203:111741,
Sept.2023.
[22] D.Fried,A.Aghajanyan,J.Lin,S.Wang,E.Wallace,F.Shi,R.Zhong,S.Yih,L.Zettlemoyer,
andM.Lewis. Incoder: Agenerativemodelforcodeinfillingandsynthesis. InTheEleventh
InternationalConferenceonLearningRepresentations,2023.
[23] D.Guo,Q.Zhu,D.Yang,Z.Xie,K.Dong,W.Zhang,G.Chen,X.Bi,Y.Wu,Y.K.Li,F.Luo,
Y.Xiong,andW.Liang. Deepseek-coder: Whenthelargelanguagemodelmeetsprogramming
–theriseofcodeintelligence,2024.
[24] D.Hendrycks,S.Basart,S.Kadavath,M.Mazeika,A.Arora,E.Guo,C.Burns,S.Puranik,
H.He,D.Song,andJ.Steinhardt. Measuringcodingchallengecompetencewithapps. NeurIPS,
2021.
[25] D.Hendrycks,S.Basart,S.Kadavath,M.Mazeika,A.Arora,E.Guo,C.Burns,S.Puranik,
H.He,D.Song,andJ.Steinhardt. Measuringcodingchallengecompetencewithapps,2021.
[26] M.Ivankovic´,G.Petrovic´,R.Just,andG.Fraser. Codecoverageatgoogle. InProceedings
of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and
SymposiumontheFoundationsofSoftwareEngineering,ESEC/FSE2019,page955–963,New
York,NY,USA,2019.AssociationforComputingMachinery.
[27] S.Iyer,I.Konstas,A.Cheung,andL.Zettlemoyer. Mappinglanguagetocodeinprogrammatic
context. In E. Riloff, D.Chiang, J. Hockenmaier, and J. Tsujii, editors, Proceedings of the
2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages1643–1652,
Brussels,Belgium,Oct.-Nov.2018.AssociationforComputationalLinguistics.
[28] A.Q.Jiang,A.Sablayrolles,A.Mensch,C.Bamford,D.S.Chaplot,D.d.l.Casas,F.Bressand,
G.Lengyel,G.Lample,L.Saulnier,etal. Mistral7b. arXivpreprintarXiv:2310.06825,2023.
[29] N. Jiang, K. Liu, T. Lutellier, and L. Tan. Impact of code language models on automated
programrepair. InProceedingsofthe45thInternationalConferenceonSoftwareEngineering,
ICSE’23,page1430–1442.IEEEPress,2023.
[30] C.E.Jimenez,J.Yang,A.Wettig,S.Yao,K.Pei,O.Press,andK.Narasimhan. Swe-bench:
Canlanguagemodelsresolvereal-worldgithubissues? arXivpreprintarXiv:2310.06770,2023.
[31] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and
I.Stoica. Efficientmemorymanagementforlargelanguagemodelservingwithpagedattention.
InProceedingsoftheACMSIGOPS29thSymposiumonOperatingSystemsPrinciples,2023.
[32] Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-T. Yih, D. Fried, S. Wang,
andT.Yu. DS-1000: Anaturalandreliablebenchmarkfordatasciencecodegeneration. In
A.Krause,E.Brunskill,K.Cho,B.Engelhardt,S.Sabato,andJ.Scarlett,editors,Proceedings
of the 40th International Conference on Machine Learning, volume 202 of Proceedings of
MachineLearningResearch,pages18319–18345.PMLR,23–29Jul2023.
[33] R.Li,L.B.allal,Y.Zi,N.Muennighoff,D.Kocetkov,C.Mou,M.Marone,C.Akiki,J.LI,
J.Chim,Q.Liu,E.Zheltonozhskii,T.Y.Zhuo,T.Wang,O.Dehaene,J.Lamy-Poirier,J.Mon-
teiro, N. Gontier, M.-H. Yee, L. K. Umapathi, J. Zhu, B. Lipkin, M. Oblokulov, Z. Wang,
R.Murthy,J.T.Stillerman,S.S.Patel,D.Abulkhanov,M.Zocca,M.Dey,Z.Zhang,U.Bhat-
tacharyya, W. Yu, S. Luccioni, P. Villegas, F. Zhdanov, T. Lee, N. Timor, J. Ding, C. S.
Schlesinger, H. Schoelkopf, J. Ebert, T. Dao, M. Mishra, A. Gu, C. J. Anderson, B. Dolan-
Gavitt,D.Contractor,S.Reddy,D.Fried,D.Bahdanau,Y.Jernite,C.M.Ferrandis,S.Hughes,
T. Wolf, A. Guha, L. V. Werra, and H. de Vries. Starcoder: may the source be with you!
TransactionsonMachineLearningResearch,2023. ReproducibilityCertification.
12[34] Y.Li,S.Bubeck,R.Eldan,A.D.Giorno,S.Gunasekar,andY.T.Lee. Textbooksareallyou
needii: phi-1.5technicalreport,2023.
[35] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling,
F.Gimeno,A.DalLago,T.Hubert,P.Choy,C.deMassond’Autume,I.Babuschkin,X.Chen,P.-
S.Huang,J.Welbl,S.Gowal,A.Cherepanov,J.Molloy,D.J.Mankowitz,E.SutherlandRobson,
P.Kohli,N.deFreitas,K.Kavukcuoglu,andO.Vinyals. Competition-levelcodegeneration
withalphacode. Science,378(6624):1092–1097,Dec.2022.
[36] C.-Y.Lin. ROUGE:Apackageforautomaticevaluationofsummaries. InTextSummarization
Branches Out, pages 74–81, Barcelona, Spain, July 2004. Association for Computational
Linguistics.
[37] J.Liu,C.S.Xia,Y.Wang,andL.Zhang. Isyourcodegeneratedbychatgptreallycorrect?
rigorousevaluationoflargelanguagemodelsforcodegeneration,2023.
[38] X. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang, et al.
Agentbench: Evaluatingllmsasagents. arXivpreprintarXiv:2308.03688,2023.
[39] Z.Liu,C.Chen,J.Wang,X.Che,Y.Huang,J.Hu,andQ.Wang. Fillintheblank: Context-
aware automated text input generation for mobile gui testing. In Proceedings of the 45th
InternationalConferenceonSoftwareEngineering,ICSE’23,page1355–1367.IEEEPress,
2023.
[40] S.Lu,D.Guo,S.Ren,J.Huang,A.Svyatkovskiy,A.Blanco,C.Clement,D.Drain,D.Jiang,
D.Tang,G.Li,L.Zhou,L.Shou,L.Zhou,M.Tufano,M.Gong,M.Zhou,N.Duan,N.Sun-
daresan,S.K.Deng,S.Fu,andS.Liu. Codexglue: Amachinelearningbenchmarkdatasetfor
codeunderstandingandgeneration,2021.
[41] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin, and D. Jiang.
Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint
arXiv:2306.08568,2023.
[42] A.MoradiDakhel, V.Majdinasab, A.Nikanjam, F.Khomh, M.C.Desmarais, andZ.M.J.
Jiang. Githubcopilotaipairprogrammer: Assetorliability? J.Syst.Softw.,203(C),sep2023.
[43] E.Nijkamp,H.Hayashi,C.Xiong,S.Savarese,andY.Zhou. Codegen2: Lessonsfortraining
llmsonprogrammingandnaturallanguages. arXivpreprintarXiv:2305.02309,2023.
[44] E.Nijkamp,B.Pang,H.Hayashi,L.Tu,H.Wang,Y.Zhou,S.Savarese,andC.Xiong.Codegen:
Anopenlargelanguagemodelforcodewithmulti-turnprogramsynthesis. InTheEleventh
InternationalConferenceonLearningRepresentations,2023.
[45] OpenAI,:,J.Achiam,S.Adler,S.Agarwal,L.Ahmad,I.Akkaya,F.L.Aleman,D.Almeida,
J.Altenschmidt,S.Altman,S.Anadkat,R.Avila,I.Babuschkin,S.Balaji,V.Balcom,P.Bal-
tescu,H.Bao,M.Bavarian,J.Belgum,I.Bello,J.Berdine,G.Bernadett-Shapiro,C.Berner,
L.Bogdonoff,O.Boiko,M.Boyd,A.-L.Brakman,G.Brockman,T.Brooks,M.Brundage,
K. Button, T. Cai, R. Campbell, A. Cann, B. Carey, C. Carlson, R. Carmichael, B. Chan,
C. Chang, F. Chantzis, D. Chen, S. Chen, R. Chen, J. Chen, M. Chen, B. Chess, C. Cho,
C.Chu,H.W.Chung,D.Cummings,J.Currier,Y.Dai,C.Decareaux,T.Degry,N.Deutsch,
D.Deville, A.Dhar, D.Dohan, S.Dowling, S.Dunning, A.Ecoffet, A.Eleti, T.Eloundou,
D.Farhi,L.Fedus,N.Felix,S.P.Fishman,J.Forte,I.Fulford,L.Gao,E.Georges,C.Gibson,
V.Goel,T.Gogineni,G.Goh,R.Gontijo-Lopes,J.Gordon,M.Grafstein,S.Gray,R.Greene,
J.Gross,S.S.Gu,Y.Guo,C.Hallacy,J.Han,J.Harris,Y.He,M.Heaton,J.Heidecke,C.Hesse,
A.Hickey,W.Hickey,P.Hoeschele,B.Houghton,K.Hsu,S.Hu,X.Hu,J.Huizinga,S.Jain,
S.Jain,J.Jang,A.Jiang,R.Jiang,H.Jin,D.Jin,S.Jomoto,B.Jonn,H.Jun,T.Kaftan,Łukasz
Kaiser,A.Kamali,I.Kanitscheider,N.S.Keskar,T.Khan,L.Kilpatrick,J.W.Kim,C.Kim,
Y. Kim, H. Kirchner, J. Kiros, M. Knight, D. Kokotajlo, Łukasz Kondraciuk, A. Kondrich,
A.Konstantinidis,K.Kosic,G.Krueger,V.Kuo,M.Lampe,I.Lan,T.Lee,J.Leike,J.Leung,
D.Levy,C.M.Li,R.Lim,M.Lin,S.Lin,M.Litwin,T.Lopez,R.Lowe,P.Lue,A.Makanju,
K.Malfacini,S.Manning,T.Markov,Y.Markovski,B.Martin,K.Mayer,A.Mayne,B.Mc-
Grew,S.M.McKinney,C.McLeavey,P.McMillan,J.McNeil,D.Medina,A.Mehta,J.Menick,
13L.Metz,A.Mishchenko,P.Mishkin,V.Monaco,E.Morikawa,D.Mossing,T.Mu,M.Mu-
rati, O. Murk, D. Mély, A. Nair, R. Nakano, R. Nayak, A. Neelakantan, R. Ngo, H. Noh,
L.Ouyang, C.O’Keefe, J.Pachocki, A.Paino, J.Palermo, A.Pantuliano, G.Parascandolo,
J.Parish,E.Parparita,A.Passos,M.Pavlov,A.Peng,A.Perelman,F.deAvilaBelbutePeres,
M.Petrov,H.P.deOliveiraPinto,Michael,Pokorny,M.Pokrass,V.Pong,T.Powell,A.Power,
B.Power,E.Proehl,R.Puri,A.Radford,J.Rae,A.Ramesh,C.Raymond,F.Real,K.Rimbach,
C. Ross, B. Rotsted, H. Roussez, N. Ryder, M. Saltarelli, T. Sanders, S. Santurkar, G. Sas-
try, H. Schmidt, D. Schnurr, J. Schulman, D. Selsam, K. Sheppard, T. Sherbakov, J. Shieh,
S.Shoker,P.Shyam,S.Sidor,E.Sigler,M.Simens,J.Sitkin,K.Slama,I.Sohl,B.Sokolowsky,
Y.Song,N.Staudacher,F.P.Such,N.Summers,I.Sutskever,J.Tang,N.Tezak,M.Thompson,
P.Tillet,A.Tootoonchian,E.Tseng,P.Tuggle,N.Turley,J.Tworek,J.F.C.Uribe,A.Vallone,
A. Vijayvergiya, C. Voss, C. Wainwright, J. J. Wang, A. Wang, B. Wang, J. Ward, J. Wei,
C.Weinmann,A.Welihinda,P.Welinder,J.Weng,L.Weng,M.Wiethoff,D.Willner,C.Winter,
S. Wolrich, H. Wong, L. Workman, S. Wu, J. Wu, M. Wu, K. Xiao, T. Xu, S. Yoo, K. Yu,
Q.Yuan,W.Zaremba,R.Zellers,C.Zhang,M.Zhang,S.Zhao,T.Zheng,J.Zhuang,W.Zhuk,
andB.Zoph. Gpt-4technicalreport,2023.
[46] OpenAI. Introducingchatgpt,2022.
[47] L.Ouyang,J.Wu,X.Jiang,D.Almeida,C.Wainwright,P.Mishkin,C.Zhang,S.Agarwal,
K.Slama,A.Ray,J.Schulman,J.Hilton,F.Kelton,L.Miller,M.Simens,A.Askell,P.Welinder,
P.F.Christiano,J.Leike,andR.Lowe. Traininglanguagemodelstofollowinstructionswith
humanfeedback. InS.Koyejo,S.Mohamed,A.Agarwal,D.Belgrave,K.Cho,andA.Oh,
editors,AdvancesinNeuralInformationProcessingSystems,volume35,pages27730–27744.
CurranAssociates,Inc.,2022.
[48] K.Papineni,S.Roukos,T.Ward,andW.-J.Zhu. Bleu: amethodforautomaticevaluationof
machinetranslation. InP.Isabelle,E.Charniak,andD.Lin,editors,Proceedingsofthe40th
AnnualMeetingoftheAssociationforComputationalLinguistics,pages311–318,Philadelphia,
Pennsylvania,USA,July2002.AssociationforComputationalLinguistics.
[49] Phind. Phind-codellama-34b-v2,2023.
[50] M. Popovic´. chrF: character n-gram F-score for automatic MT evaluation. In O. Bojar,
R.Chatterjee,C.Federmann,B.Haddow,C.Hokamp,M.Huck,V.Logacheva,andP.Pecina,
editors,ProceedingsoftheTenthWorkshoponStatisticalMachineTranslation,pages392–395,
Lisbon,Portugal,Sept.2015.AssociationforComputationalLinguistics.
[51] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez,
J.Rapin,etal. Codellama:Openfoundationmodelsforcode. arXivpreprintarXiv:2308.12950,
2023.
[52] B.Roziere,M.-A.Lachaux,L.Chanussot,andG.Lample. Unsupervisedtranslationofprogram-
minglanguages. InProceedingsofthe34thInternationalConferenceonNeuralInformation
ProcessingSystems,NIPS’20,RedHook,NY,USA,2020.CurranAssociatesInc.
[53] B.Roziere,J.M.Zhang,F.Charton,M.Harman,G.Synnaeve,andG.Lample. Leveraging
automatedunittestsforunsupervisedcodetranslation,2022.
[54] B.Rozière,J.Gehring,F.Gloeckle,S.Sootla,I.Gat,X.E.Tan,Y.Adi,J.Liu,R.Sauvestre,
T.Remez,J.Rapin,A.Kozhevnikov,I.Evtimov,J.Bitton,M.Bhatt,C.C.Ferrer,A.Grattafiori,
W.Xiong,A.Défossez,J.Copet,F.Azhar,H.Touvron,L.Martin,N.Usunier,T.Scialom,and
G.Synnaeve. Codellama: Openfoundationmodelsforcode,2024.
[55] H.Touvron,T.Lavril,G.Izacard,X.Martinet,M.-A.Lachaux,T.Lacroix,B.Rozière,N.Goyal,
E.Hambro,F.Azhar,A.Rodriguez,A.Joulin,E.Grave,andG.Lample. Llama: Openand
efficientfoundationlanguagemodels,2023.
[56] N.Tran,H.Tran,S.Nguyen,H.Nguyen,andT.N.Nguyen. Doesbleuscoreworkforcode
migration? InProceedingsofthe27thInternationalConferenceonProgramComprehension,
ICPC’19,page165–176.IEEEPress,2019.
14[57] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and
I.Polosukhin. Attentionisallyouneed. InProceedingsofthe31stInternationalConference
onNeuralInformationProcessingSystems,NIPS’17,page6000–6010,RedHook,NY,USA,
2017.CurranAssociatesInc.
[58] Y.Wei,C.S.Xia,andL.Zhang. Copilotingthecopilots: Fusinglargelanguagemodelswith
completion engines for automated program repair. In Proceedings of the 31st ACM Joint
EuropeanSoftwareEngineeringConferenceandSymposiumontheFoundationsofSoftware
Engineering, ESEC/FSE 2023, page 172–184, New York, NY, USA, 2023. Association for
ComputingMachinery.
[59] C.S.Xia,M.Paltenghi,J.L.Tian,M.Pradel,andL.Zhang. Fuzz4all: Universalfuzzingwith
largelanguagemodels,2024.
[60] C. S. Xia, Y. Wei, and L. Zhang. Automated program repair in the era of large pre-trained
languagemodels. InProceedingsofthe45thInternationalConferenceonSoftwareEngineering,
ICSE’23,page1482–1494.IEEEPress,2023.
[61] C.S.XiaandL.Zhang. Lesstraining,morerepairingplease: revisitingautomatedprogram
repairviazero-shotlearning. InProceedingsofthe30thACMJointEuropeanSoftwareEngi-
neeringConferenceandSymposiumontheFoundationsofSoftwareEngineering,ESEC/FSE
2022,page959–971,NewYork,NY,USA,2022.AssociationforComputingMachinery.
[62] F.F.Xu,U.Alon,G.Neubig,andV.J.Hellendoorn. Asystematicevaluationoflargelanguage
modelsofcode.InProceedingsofthe6thACMSIGPLANInternationalSymposiumonMachine
Programming,MAPS2022,page1–10,NewYork,NY,USA,2022.AssociationforComputing
Machinery.
[63] C.Yang,Y.Deng,R.Lu,J.Yao,J.Liu,R.Jabbarvand,andL.Zhang. White-boxcompiler
fuzzingempoweredbylargelanguagemodels,2023.
[64] S.Yang,W.-L.Chiang,L.Zheng,J.E.Gonzalez,andI.Stoica. Rethinkingbenchmarkand
contaminationforlanguagemodelswithrephrasedsamples. arXivpreprintarXiv:2311.04850,
2023.
[65] P. Yin, B. Deng, E. Chen, B. Vasilescu, and G. Neubig. Learning to mine aligned code
and natural language pairs from stack overflow. In Proceedings of the 15th International
ConferenceonMiningSoftwareRepositories,MSR’18,page476–486,NewYork,NY,USA,
2018.AssociationforComputingMachinery.
[66] T. Yu, R. Zhang, K. Yang, M. Yasunaga, D. Wang, Z. Li, J. Ma, I. Li, Q. Yao, S. Roman,
Z.Zhang,andD.Radev. Spider: Alarge-scalehuman-labeleddatasetforcomplexandcross-
domain semantic parsing and text-to-SQL task. In E. Riloff, D. Chiang, J. Hockenmaier,
andJ.Tsujii,editors,Proceedingsofthe2018ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,pages3911–3921,Brussels,Belgium,Oct.-Nov.2018.Associationfor
ComputationalLinguistics.
[67] J. M. Zelle and R. J. Mooney. Learning to parse database queries using inductive logic
programming. InProceedingsoftheThirteenthNationalConferenceonArtificialIntelligence-
Volume2,AAAI’96,page1050–1055.AAAIPress,1996.
[68] A.Zeng,X.Liu,Z.Du,Z.Wang,H.Lai,M.Ding,Z.Yang,Y.Xu,W.Zheng,X.Xia,W.L.
Tam,Z.Ma,Y.Xue,J.Zhai,W.Chen,P.Zhang,Y.Dong,andJ.Tang. Glm-130b: Anopen
bilingualpre-trainedmodel,2023.
[69] L.Zheng,W.-L.Chiang,Y.Sheng,S.Zhuang,Z.Wu,Y.Zhuang,Z.Lin,Z.Li,D.Li,E.P.Xing,
H.Zhang,J.E.Gonzalez,andI.Stoica. Judgingllm-as-a-judgewithmt-benchandchatbot
arena,2023.
[70] Q.Zheng,X.Xia,X.Zou,Y.Dong,S.Wang,Y.Xue,L.Shen,Z.Wang,A.Wang,Y.Li,T.Su,
Z.Yang,andJ.Tang. Codegeex: Apre-trainedmodelforcodegenerationwithmultilingual
benchmarking on humaneval-x. In Proceedings of the 29th ACM SIGKDD Conference on
KnowledgeDiscoveryandDataMining, KDD’23, page5673–5684, NewYork, NY,USA,
2023.AssociationforComputingMachinery.
15A Instructionsin NATURALCODEBENCH
Toenhancetheefficiencyofbenchmarkconstructionandreducehumanlaborcosts,weutilizedthe
extensiveknowledgestorageandnatruallanguageunderstandingcapabilitiesofLLMsduringthe
benchmarkconstructionprocess. Belowarethedetailsoftheinstructionsusedintheconstruction
process:
• Figure3showstheinstructionweemployedtoswiftlyfilteroutqueriesunsuitablefortesting.
• Figure13showshowweinstructtheGPT-4togeneratediverseandhigh-qualitytestcases.
• Figure4illustrateshowweaddresstheissueofmisalignmentbetweenclassorfunctionnames
generatedbytheLLMsandthenamesinthetestcases.
I will give you a #Given Prompt# which ask the LLM to generate
code. Please verify whether the #Given Prompt# satisfies the
following requirements:
1. #Given Prompt# should contain a task, that is, the user asks the
model to help solve one or some problems.
2. It is easily to find the type of input and ouput in the #Given
Prompt#
3. There is no randomness or uncertainty in the #Given Prompt#
If the #Given Prompt# satisfies the above requirements, reply
"yes", otherwise reply "no". YOU CAN ONLY GENERATE "yes" or
"no", OTHER TOKENS ARE NOT ALLOWED.
#Given Prompt#:
{{given_prompt}}
#Response#:
Figure3: Theinstructionusedtoquicklyfilteroutlow-qualityqueries
Your task is to generate {{language}} code to solve the
following problem. The generated code must be
placed between the ```{{language}} and ```, and only
one code block is allowed:
{{prompt}}
You need to follow the function names or class names
in the test cases. The generated code should not
contain any test cases:
{{test_demo}}
Figure4: TheinstructionusedtoalignthenamesofclassesorfunctionsgeneratedbytheLLMswith
thenamesinthetestcases.
B Examples
B.1 ExamplesofSemi-AutomatedPipeline
Inthissection,wepresenttwoexamples,oneeachforPythonandJava,ofsemi-automatedpipeline
withonetestcasetoillustratehowweconstructtestcasesandrectifyerrorstherein.
Figure5showsthePythonexample. Followingtheprovisionofproblemdescriptionandreference
solution, GPT-4 writes the majority of the test case, including the execution procedure and test
caseinput. However,GPT-4couldnotguaranteethecorrectnessofeachtestcase,resultinginthe
generationoferroneousexpectedoutputs. Atthispoint,ourprogrammingexpertsonlyneededto
correcttheincorrectexpectedoutputs.
Figure6showstheJavaexmaple. Inthisproblem,wheretheinputtypeinvolvesmorecomplexfile
formats,oursemi-automaticpipelineisunabletodirectlygeneratetheinputfilescorrespondingto
eachtestcase. Therefore,inthisinstance,ourprogrammingexpertsneedtonotonlysupplementthe
missingproceduresinthetestcasesbutalsocreateaninputfileforeachtestcase. However,GPT-4
hasprovidedreferencecontentfortheinputfilesinthecomments,soourprogrammingexpertsdo
notneedtodesigntheinputsthemselves.
16Model NCB(zh) NCB(en)
Dataset
Python Java Python Java
Pass@10 Pass@50 Pass@10 Pass@50 Pass@10 Pass@50 Pass@10 Pass@50
Test 62.4 67.9 64.6 71.8 65.3 70.2 62.7 67.9
GPT-4[45]
Dev 53.3 55.7 69.2 72.9 51.8 54.3 62.0 64.3
Test 46.5 48.9 49.3 56.5 53.5 55.7 51.5 57.3
GPT-3.5-Turbo[46]
Dev 44.0 47.7 45.5 51.4 43.6 47.1 48.4 50.0
Test 55.7 61.8 48.0 51.1 56.6 64.9 52.8 59.5
Deepseek-Coder-33B-Instruct[23]
Dev 48.1 51.4 46.8 51.4 46.5 48.6 46.7 50.0
Test 49.6 56.5 52.7 61.8 51.0 62.6 48.2 58.0
Codellama-70B-Instruct[51]
Dev 47.5 54.3 53.9 62.9 47.6 54.3 50.5 60.0
Test 42.3 46.6 39.4 45.8 40.6 43.5 47.6 56.5
Phind-Codellama-34B[49]
Dev 45.4 50.0 41.7 45.7 44.0 45.7 49.4 51.4
Test 44.3 48.9 40.8 47.8 47.3 51.9 40.9 45.8
Deepseek-67B-Chat[15]
Dev 42.3 47.1 44.5 47.1 37.9 41.4 43.6 50.0
Test 34.9 37.4 36.5 39.7 32.7 35.9 36.5 38.2
Qwen-72B-Chat[8]
Dev 43.4 47.1 31.4 38.6 41.0 44.3 31.5 35.7
Test 23.1 28.2 23.3 29.8 24.1 31.3 26.8 32.1
StarCoder[33]
Dev 29 32.9 27.3 32.9 35.5 41.4 27.0 30.0
Test 15.5 18.3 17.3 20.6 19.6 22.9 22.0 24.4
Mistral-7B-Instruct[28]
Dev 18.2 21.4 16.3 20.0 19.7 24.3 17.8 21.4
Test 8.6 16.8 18.0 22.9 13.0 19.1 21.0 26.0
CodeGen2-16B[43]
Dev 11.6 21.4 12.8 15.7 16.0 24.3 18.5 24.3
Test 4.6 9.2 13.3 18.3 9.9 15.3 17.5 21.4
CodeGen-16B[44]
Dev 10.7 17.1 15.6 18.6 16.1 22.9 17.4 21.4
Test 14.5 21.4 5.5 7.6 11.9 19.8 10.7 14.5
Phi-2[34]
Dev 15.3 27.1 5.1 7.1 10.9 18.6 6.4 7.1
Table5: Pass@kresultsofbest-performingLLMswitheachLLMfamilyonNaturalCodeBench.
B.2 ExampleProblems
Here,wepresentanexampleproblemandtestcasesforeachofthe6domains.
Figure 7 shows a problem of Algorithm and Data Structure, querying the pattern of a sequence
transformationandthetotalnumberofalltransformations.
Figure8illustratesanexampleprobleminsoftwareengineering,requiringtheadditionoftagsto
differenttitlesinamarkdownfileaccordingtotheirlevels.
Figure 9 presents an example problem in data science, asking to select the row with the highest
temperaturefromthetemperatureCSVfilesofeachcityandwritetheserowsintoanewCSVfile.
Figure10depictsanexampleprobleminfront-enddevelopment,requiringthereplacementofgiven
specialtagswithinastringwithspecificHTMLformats.
Figure11showsanexampleprobleminartificialintelligence,requiringthecalculationofthedistance
betweeneachpointoftwotensors,wherethedimensionofeachtensorisbatchsize*n*3,withthe
thirddimensionrepresentingthecoordinatesofthepoints.
Figure12presentsanexampleprobleminsystemadministration,inquiringhowtorenameallthe
fileswithinafolderaccordingtoagivenrule.
C ExtraResults
Table6showsthepass@1resultsonthedevelopmentsetofNCB.Theresultsonthedevelopment
setareessentiallyconsistentwiththoseonthetestset, withsomechangesintherankingamong
severalmodels. Thisisduetodifferencesinthedistributionofproblemsacrossdomainsbetweenthe
developmentsetandthetestset.
Table5showsthepass@kresultsofbest-performingLLMswitheachLLMfamilyonNCB,where
k ∈{10,50}. Wedonotevaluatetheperformanceonpass@kforErnieBot4,CodeGeeX3,Claude-3,
Gemini-1.5-ProandLlama-3-InstructduetolimitationsontheuseofAPIandotherresources.
17Model NCB(zh) NCB(en)
Size Total
Python Java Total Python Java Total
APILLMs
GPT-4[45] N/A 50.0 64.3 57.2 47.1 57.1 52.1 54.6
GPT-4-Turbo-1106[45] N/A 54.3 55.7 55.0 50.0 54.3 52.2 53.6
GPT-4-Turbo-0125[45] N/A 51.5 55.7 53.6 48.6 51.4 50.0 51.8
GPT-3.5-Turbo[46] N/A 38.6 38.6 38.6 37.1 41.4 39.3 38.9
Claude-3-Opus[5] N/A 46.4 44.3 45.3 50.0 47.1 48.6 47.0
Claude-3-Haiku[5] N/A 40.3 32.9 36.6 43.8 32.9 38.4 37.5
Claude-3-Sonnet[5] N/A 37.8 41.4 39.6 38.6 31.4 35.0 37.3
Claude-2.1[4] N/A 41.4 37.1 39.3 35.7 35.7 35.7 37.5
ChatGLM-4[68;19] N/A 42.9 47.1 45.0 44.3 42.9 43.6 44.3
Gemini-1.5-Pro[10] N/A 44.3 35.7 40.0 48.6 34.3 41.4 40.7
CodeGeeX3[70] N/A 40.0 25.7 32.9 35.7 25.7 30.7 31.8
OpenLLMs
33B 41.4 40.0 40.7 35.7 41.4 38.6 39.6
Deepseek-Coder-Instruct[23] 6.7B 34.3 40.0 37.2 34.4 40.0 37.2 37.2
1.3B 22.9 21.4 22.2 20.0 27.1 23.6 22.9
70B 42.9 37.1 40.0 37.1 41.4 39.3 39.6
Llama-3-Instruct[2]
8B 22.9 20.0 21.4 12.9 20.0 16.4 18.9
Phind-Codellama[49] 34B 34.1 31.4 32.8 38.6 40.0 39.3 36.0
Qwen-1.5[9] 110B 35.7 30.0 32.9 37.1 35.7 36.4 34.6
70B 30.0 30.0 30.0 35.7 35.7 35.7 32.9
34B 14.3 25.7 20.0 25.7 25.7 25.7 22.9
Codellama-Instruct[51]
13B 21.4 20.0 20.7 22.9 20.0 21.5 21.1
7B 25.7 14.3 20.0 18.6 17.1 17.9 18.9
67B 28.6 35.7 32.2 28.6 32.9 30.8 31.5
Deepseek-Chat[15]
7B 12.9 11.4 12.2 10.0 14.3 12.2 12.2
34B 31.4 31.4 31.4 30.0 31.4 30.7 31.1
WizardCoder[41]
15B 30.0 24.3 27.2 31.4 24.3 27.9 27.5
72B 35.7 24.3 30.0 34.3 25.7 30.0 30.0
Qwen-Chat[8]
7B 10.0 12.9 11.5 20.0 15.7 17.9 14.7
StarCoder[33] 15.5B 17.1 15.7 16.4 21.4 15.7 18.6 17.5
Mistral-Instruct[28] 7B 11.4 12.9 12.2 15.7 11.4 13.6 12.9
16B 5.7 7.1 6.4 8.6 7.1 7.9 7.1
7B 1.4 5.7 3.6 1.4 5.7 3.6 3.6
CodeGen2[43]
3.7B 0.0 5.7 2.9 2.9 2.9 2.9 2.9
1B 0.0 2.9 1.5 0.0 2.9 1.5 1.5
16B 1.4 5.7 3.6 7.1 8.6 8.6 5.7
CodeGen[44] 6B 2.9 2.9 2.9 4.3 7.1 5.7 4.3
2B 0.0 2.9 1.5 2.9 5.7 4.3 2.9
2.7B 4.3 4.3 4.3 5.7 4.3 5.0 4.7
Phi[34]
1.3B 1.4 2.9 2.2 5.7 4.3 5.0 3.6
Table6: EvaluatingLLMsonthedevsetof NATURALCODEBENCH. Allresultsarepass@1on
greedydecoding.
18Problem Test Case Generated by GPT-4
I have a dataframe that includes the price and date of
def test_fluctuation_periods_2(self):
a symbol, how can I identify the time periods where
df = pd.DataFrame({
the price has consistently fluctuated within an x 'symbol': ['AAPL', 'AAPL', 'AAPL', 'AAPL'],
percent range? 'price': [100, 110, 120, 130],
'date': pd.to_datetime([
For instance, the output of the following statements:
'2021-01-01',
1) From December 10 to December 30 '2021-01-02',
2) From March 10 to March 23 '2021-01-03',
'2021-01-04'])
})
Reference Solution assert find_fluctuation_periods(df, 'AAPL', 10) ==
def find_fluctuation_periods(df, symbol, x): [('2021-01-01', '2021-01-03’)] Wrong Output
symbol_data = df[…==symbol].sort_values(by='date')
…
for ind, row in symbol_data.iterrows():
Human Rewritten Test Case
if start_date is None:
…
def test_fluctuation_periods_2(self):
else:
df = pd.DataFrame({
change = abs((row['price'] - prev_price) / prev_price
'symbol': ['AAPL', 'AAPL', 'AAPL', 'AAPL'],
* 100)
'price': [100, 110, 120, 130],
if change > x:
'date': pd.to_datetime([
if ind - start_ind > 1:
'2021-01-01',
periods.append((start_date.strftime('%Y-%m-
'2021-01-02',
%d'), prev_date.strftime('%Y-%m-%d')))
'2021-01-03',
…
'2021-01-04'])
if start_date != end_date:
})
periods.append((start_date.strftime('%Y-%m-%d'),
assert find_fluctuation_periods(df, 'AAPL', 10) ==
end_date.strftime('%Y-%m-%d')))
[('2021-01-01', '2021-01-04')]
return periods
Figure5: APythonexampleofsemi-automatepipeline.
Problem
Design a method in Java Test Case Generated by GPT-4
Use the following encryption method, encrypt the
@Test
content in the given encodingFile text file, and then
void testEncodeDigits() throws IOException {
save it to the encodedFile file.
File input = new File("testEncode.txt");
Encryption rules:
File output = new File("testEncodeOutput.txt");
1. Numbers: If it is not the number 9, add 1 to the
// numbers.txt contains "123456789"
original basis, If it is the number 9, it becomes 0.
2. Letter characters: If it is a non-z character, move // encodedNumbers.txt should contain
one to the right, If it is z, z->a, Z->A. "234567890"
3. Non-numeric and non-letter characters can remain } Not completely generated
unchanged, such as Chinese characters and
punctuation marks, etc., just need to remain
unchanged.
Reference Solution
void encodeFile(File encodingFile, File encodedFile) { Human Rewritten Test Case
try (FileReader reader = …(encodingFile);
FileWriter writer = …(encodedFile)) { @Test
while ((c = reader.read()) != -1) { void testEncodeDigits() throws IOException {
char character = (char) c; File input = new File("testEncode.txt");
if (Character.isDigit(character)) { File output = new File("testEncodeOutput.txt");
character = character == '9' ? '0' : (char)
FileEncoder.encodeFile(input, output);
(character + 1);
}else if (Character.isLetter(character)) { assertEquals(“234567890",
. . . readFileContent(output));
else if ((character >= 'a' && …) { }
character=(char)(character+1);
. . .
Figure6: AJavaexampleofsemi-automatepipeline.
19Rerference Solution
Problem:
def max_possible_sequences(n):
Given a sequence that only contains two possible
if n <= 0:
characters "O" and "x". There is a magical operation
return 0
that can combine two consecutive "x" characters in
elif n == 0:
the sequence into one "O" character. Suppose there is return 0
a sequence of length n, containing only "x" characters, elif n == 0:
and the magic operation can be used any number of return 2
times. What is the maximum number of possible result else:
sequences? return max_possible_sequences(n-1) \
For example: + max_possible_sequences(n-1)
For a sequence of length 2, the initial state is "xx", you
can choose not to use the magic operation or use it
Test Cases
only once. There are two possible final results: "xX" or
"O". class Testmax_possible_sequences:
For a sequence of length 3, the initial state is "xxX",
def test_max_possible_sequences_1(self):
you can choose not to use the magic operation or use
assert max_possible_sequences(4) == 5
it once. There are three possible final results: "xxx",
"OX!" (combining the first two "x" characters) or def test_max_possible_sequences_2(self):
"XO" (combining the last two "x" characters). assert max_possible_sequences(7) == 21
. . .
Figure7: AnexampleproblemofAlgorithmandDataStructure.
Problem:
Rerference Solution
Hello, please write a Python function for me. The
function should read a markdown file, add def add_section_numbering(markdown_file):
with open(markdown_file, 'r') as file:
numbering like x.y.z... to the titles of each level,
lines = file.readlines()
and then return the modified string. Please note
not to write into the original file. numbering = []
result = ''
for line in lines:
Test Cases if line.startswith('#'):
level = line.count('#')
class Testadd_section_numbering: numbering = numbering[:level]
def test_case1(self): if len(numbering) < level:
numbering.append(0)
with open('test1.md', 'w') as f:
numbering[-1] += 1
f.write('# Title\n## Subtitle\n### Sub-Subtitle\n##
line = '#'*level + ' ' + '.'.join(map(str, numbering))
Another Subtitle\n# Another Title')
+ ' ' + line[level:].strip() + '\n'
assert add_section_numbering(
result += line
'test1.md') == '# 1 Title\n## 1.1 Subtitle\n### 1.1.1
return result[:-1]
Sub-Subtitle\n## 1.2 Another Subtitle\n# 2 Another Title'
. . .
Figure8: AnexampleproblemofSoftwareEngineering.
20Problem:
There are multiple CSV files in the data folder, each file has
two columns, containing the daily temperature records of a
certain city in 2022. The first row is the title, which are Date
Rerference Solution
and Temperature. The temperature value is an integer. I
need to find out the highest temperature value and the
def find_max_temperature(folder_path, output_file):
corresponding date of each city in that year, and save the csv_files = [f for f in os.listdir(folder_path)
results to a new CSV file. The result CSV consists of three if f.endswith('.csv')]
columns, including city, highest temperature, and date. result_df = pd.DataFrame(columns=[
Note that if the highest temperature is the same for multiple 'City',
days, keep all dates that reach the highest temperature. 'Max_Temperature',
How can I use the pandas library's dataframe to complete 'Date'])
this task? for csv_file in csv_files:
file_path = os.path.join(folder_path, csv_file)
df = pd.read_csv(file_path)
Test Cases city_name = csv_file[:-4]
max_temp = df['Temperature'].max()
class Testmax_possible_sequences:
max_temp_dates = df.loc[
def test_single_file_single_max(self, tmpdir): df['Temperature'] == max_temp,
data = 'Date'].tolist()
"Date,Temperature\n2022-01-01,10\n2022-01-02,20\n2022-01- for date in max_temp_dates:
03,30" result_df = result_df._append({
p = tmpdir.mkdir("data").join("city1.csv") 'City': city_name,
p.write(data) 'Max_Temperature': max_temp,
output_file = tmpdir.join("output.csv") 'Date': date}, ignore_index=True)
find_max_temperature(str(tmpdir.join("data")), result_df.to_csv(output_file, index=False)
str(output_file))
assert output_file.read() ==
"City,Max_Temperature,Date\ncity1,30,2022-01-03\n"
. . .
Figure9: AnexampleproblemofDataScience.
Problem:
Rerference Solution
How to replace a string containing content like ```html ```,
```css ```, ```python ```, ```javascript ```, ```golang ``` with strings def replace_code_block(text):
like <pre><code class=\"language-html\">...</code></pre>, languages = {
<pre><code class=\"language-css\">...</code></pre>, "html": "language-html",
<pre><code class=\"language-python\">...</code></pre>, "css": "language-css",
<pre><code class=\"language-javascript\">...</code></pre>, "python": "language-python",
<pre><code class=\"language-golang\">...</code></pre>. "javascript": "language-javascript",
Please use python code. "golang": "language-golang"
}
for lang, html_class in languages.items():
Test Cases pattern = rf"```{lang}\b\s*(.*?)\s*```"
replacement = rf'<pre><code
class Testreplace_code_block: class="{html_class}">\1</code></pre>'
def test_replace_code_block_1(self): text = re.sub(pattern, replacement, text,
assert replace_code_block('```html ```') == '<pre><code flags=re.DOTALL)
class="language-html"></code></pre>' return text
. . .
Figure10: AnexampleproblemofFront-End.
21Test Cases
Problem:
class Testcalculate_distance:
Python code, calculate distance given two Pytorch def test_case_1(self):
tensors with dimension batchsize x n x 3, n is points, 3 tensor_a = torch.tensor([[[1,2,3],[4,5,6]]])
is x,y,z. Compute point wise distance along the last tensor_b = torch.tensor([[[1,2,3],[4,5,6]]])
dimension, for example only compute distance expected_output = torch.tensor([[0.0, 0.0]])
between a[0,1] and b[0,1] not a[0,1] and b[0,2]. assert torch.allclose(calculate_distance(tensor_a,
tensor_b), expected_output)
def test_case_2(self):
tensor_a = torch.tensor([[[1,1,1],[2,2,2]]])
Rerference Solution tensor_b = torch.tensor([[[0,0,0],[0,0,0]]])
expected_output = torch.tensor([[1.7321,
def calculate_distance(tensor_a, tensor_b): 3.4641]])
diff = tensor_a - tensor_b assert torch.allclose(calculate_distance(tensor_a,
dist = torch.sqrt(torch.sum(diff ** 2, dim=-1)) tensor_b), expected_output, atol=1e-4)
return dist
. . .
Figure11: AnexampleproblemofArtificialIntelligence.
Test Cases
Problem: class Testrename_files_in_folder:
def test_rename_files_in_folder_1(self, tmpdir):
I want to write a python program that rename p = tmpdir.mkdir("sub").join("file123abc.txt")
the files of a folder . p.write("content")
please remove all letters and keep the numbers rename_files_in_folder(str(tmpdir) + '/sub/')
assert os.path.isfile(str(tmpdir) + '/sub/123.txt')
def test_rename_files_in_folder_2(self, tmpdir):
p = tmpdir.mkdir("sub").join("file456def.txt")
p.write("content")
Rerference Solution rename_files_in_folder(str(tmpdir) + '/sub/')
assert os.path.isfile(str(tmpdir) + '/sub/
def rename_files_in_folder(folder_path): 456.txt')
for filename in os.listdir(folder_path):
file_type = filename.split('.')[-1] def test_rename_files_in_folder_3(self, tmpdir):
new_filename = re.sub("[A-Za-z]", "", p = tmpdir.mkdir("sub").join("file789ghi.txt")
filename[:-len(file_type)]) + file_type p.write("content")
os.rename(os.path.join(folder_path, filename), rename_files_in_folder(str(tmpdir) + '/sub/')
os.path.join(folder_path, new_filename)) assert os.path.isfile(str(tmpdir) + '/sub/
789.txt')
. . .
Figure12: AnexampleproblemofSystemAdministration.
22I will give you a #Prompt# and a piece of #Code#. I need you to write 10 diverse
test cases to verify whether the function in the #Code# meets the requirements of
the #Prompt#. Among them, 6 test cases should cover as many lines and
branches in the #Code# as possible, and the other 4 test cases should try to
reach the boundaries of the requirements in the #Prompt#. The test cases should
conform to the Pytest/JUnit call format. You should only generate test cases
without any explanation.
#Prompt#:
{{given_prompt}}
#Code#:
```
{{given_code}}
```
#Test cases#:
class Test{{class_name}} :/{
Figure13:TheinsturcitonusedinSemi-automatedPipeline.Generating6testcasesforhigh-coverage
and4cornertestcases.
23