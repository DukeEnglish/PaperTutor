Multimodal Contextualized Semantic Parsing from Speech
JordanVoas and RaymondMooney and DavidHarwath
jvoas@utexas.edu and mooney@utexas.edu and harwath@utexas.edu
TheUniversityofTexasatAustin
Abstract Toward this goal, this work introduces Seman-
tic Parsing in Contextual Environments (SPICE),
WeintroduceSemanticParsinginContextual a task designed to capture the process of itera-
Environments(SPICE),ataskdesignedtoen- tiveknowledgeconstructionthroughgroundedlan-
hance artificial agents’ contextual awareness
guage. Itemphasizesthecontinuousneedtoupdate
by integrating multimodal inputs with prior
contextual states based on prior knowledge and
contexts. SPICE goes beyond traditional se-
newinformation. SPICErequiresagentstomain-
manticparsingbyofferingastructured,inter-
taintheircontextualstatewithinastructured,dense
pretableframeworkfordynamicallyupdating
an agent’s knowledge with new information, information framework that is scalable and inter-
mirroring the complexity of human commu- pretable,facilitatinginspectionbyusersorintegra-
nication. WedeveloptheVG-SPICEdataset, tionwithdownstreamsystemcomponents. SPICE
crafted to challenge agents with visual scene accomplishes this by formulating updates as For-
graphconstructionfromspokenconversational
mal Semantic Parsing, with the formal language
exchanges,highlightingspeechandvisualdata
defining the allowable solution space of the con-
integration. WealsopresenttheAudio-Vision
structedcontext.
DialogueSceneParser(AViD-SP)developed
Because the SPICE task is designed to model
foruseonVG-SPICE.Theseinnovationsaim
toimprovemultimodalinformationprocessing real-world and embodied applications, such as
and integration. Both the VG-SPICE dataset teaching a mobile robot about an environment
andtheAViD-SPmodelarepubliclyavailable. or assisting a doctor with medical image an-
1 2
notations, there are crucial differences between
SPICEandtraditionaltext-basedsemanticparsing.
1 Introduction
First,SPICEconsidersparsinglanguagewithina
grounded, multimodal context. The language in
Imagine you are taking a guided tour of an art
caseslikethesemayhaveambiguitiesthatcanonly
museum. During the tour as you visit each piece
beresolvedbytakingintoaccountmultimodalcon-
ofart,yourguidedescribesnotonlytheartworks
textualinformation,suchasfromvision.
themselvesbutalsothehistoryanduniquefeatures
Furthermore, SPICE supports linguistic input
of the galleries and building itself. Through this
that comes in the form of both speech and text.
dialog,youareabletoconstructamentalmapof
In real-world embodied interactions, language is
themuseum,whoseentitiesandtheirrelationships
predominantlyspoken,notwritten. Whilemodern
withoneanotheraregroundedtotheirreal-world
automaticspeechrecognition(ASR)technologyis
counterpartsinthemuseum. Weengageinthistype
highlyaccurate,itisstillsensitivetoenvironmental
of iterative construction of grounded knowledge
noiseandreverberation,andrepresentingtheinput
throughdialogeveryday,suchaswhenteachinga
language as both a waveform as well as a noisy
friend how to change the oil in their car or going
ASRtranscriptcanimproverobustness. Whilewe
overasetofX-rayswithourdentist. Asintelligent
donotconsiderithere,theSPICEframeworkalso
agents continue to become more ubiquitous and
supportsparalinguisticinputsuchasfacialexpres-
integratedintoourlives,itisincreasinglyimportant
sions,eyegaze,andhandgestures.
todevelopthesesamesortsofcapabilitiesinthem.
Wepresentanoveldataset,VG-SPICE,derived
1https://github.com/jvoas655/VG-SPICE from the Visual Genome (Krishna et al., 2016),
2https://github.com/jvoas655/AViD-SP anexistingdatasetcomprisedofannotatedvisual
4202
nuJ
01
]LC.sc[
1v83460.6042:viXraFigure1: ExampleofVG-SPICEinputsaswellasaplausibleoutputtoproducethecorrectnextstatecontext. New
informationthattheagentisexpectedtoaddtothecontextisshowningreenwhilealreadyknowninformationis
notedinred. Groundingentitiesthathavenewinformationbeingaddedtothemarenotedinblueandorange. The
currentcontextisshownasatextuallypromptedrepresentationoftheactualknowledgegraph(discussedinSection
F).
scenegraphsrepresentingconstituententitiesand • Aninitialbaselinemodel,Audio-VisionDia-
relational prepositions, enhanced with additional logueSceneParser(AViD-SP),forVG-SPICE
processing and synthetic augmentation to form a that integrates Language Models with Au-
foundationalrepresentationforSPICEtasks. VG- dio/Visual feature extractors, establishing a
SPICE simulates the conversational construction researchbenchmarkforSPICE.Asacompo-
ofvisualscenegraphs,whereinaknowledgegraph nent of AViD-SP, we also introduce a novel
representationoftheentitiesandrelationshipscon- pretrainedencoderadaptionandmultimodal
tained within an image must be collected from fusionmethod,theGroupedMultimodalAt-
thevisualinputsandaudiodialogue. Thisdataset, tentionDownSampler(GMADS)tomotivate
alongwithaninitialmodeltrainedforVG-SPICE, theexplorationofadditionalmultimodaladap-
setsthebaselineforfutureefforts. Figure1shows tationmethods.
an example of a typical VG-SPICE sample. The
2 RelatedWork
figureshowshowpotentialsemanticparsescanbe
extracted from the visual scene and spoken utter-
TheSPICEtaskintersectswithresearchindialogue
ance conditioned on what information is already
systemsandsemanticparsing. Whilepreviousef-
knownaboutthescene.
fortsintheseareashaveaddressedsomeelements
Theremainderofthispaperisstructuredasfol-
ofSPICE,nonehavefullyencapsulatedthecom-
lows: It begins with a detailed analysis of the
prehensiverequirementsoftheSPICEtask.
SPICEtask,introducestheVG-SPICEdataset,and
presents our AViD-SP model. It then delves into 2.1 DialogueSystemsandMultimodality
experimentalresults,showcasingthemodel’sabil-
Dialogue systems share similarities with SPICE
itytoprocessandinterpretcontextconsistentwith
tasks, particularly in their aim to emulate human
theSPICEframework. Finallyweoutlinetheim-
conversational skills, including referencing prior
plicationsanddirectionsforfutureresearch. The
conversational context. However, SPICE differ-
maincontributionsinclude:
entiatesitselfbynecessitatingmultimodalinterac-
tions,theutilizationofstructuredandinterpretable
• AdefinitionoftheSemanticParsinginCon-
knowledgerepresentations,andthecapabilityfor
textualEnvironments(SPICE)task,highlight-
dynamicknowledgeupdatesduringconversations,
ingitschallenges,scope,andsignificancein
settingitapartfromconventionaldialoguemodels.
enhancinghuman-AIcommunication.
Recentadvancementsindialoguesystems,par-
• The creation of a large, machine-generated ticularly through large language models (LLMs)
SPICEdataset,VG-SPICE,leveragingexist- (Weietal.,2022;Chowdheryetal.,2022;Ouyang
ingmachinelearningmodelsandtheVisual et al., 2022; Jiang et al., 2023; Touvron et al.,
Genomedataset,tomotivateSPICEresearch. 2023a,b), have enhanced the ability to managecomplex,multi-turnconversations. Thisislargely (Li et al., 2020). However, most CDSP research
thankstotheemploymentofextensivecontextwin- hasbeenaimedatdatabaseapplications,wherethe
dows(Dao,2023),improvinglanguagecomprehen- contextisastaticschema(Yuetal.,2019). While
sionandgenerationformorecoherentandcontex- these tasks leverage context for query execution,
tuallyappropriateexchanges. Nevertheless,LLMs’ they do not involve dynamic schema updates, in-
relianceonbroadtextualcontextscancompromise steadmaintainingastaticcontextbetweeninterac-
efficiencyandinterpretabilityinmanyapplications. tions. Outsidetheseapplications,CDSPismainly
Not only must all prior inputs be reprocessed for appliedinDST(Yeetal.,2021;Chengetal.,2020;
future updates but the uncompressed format pre- Moradshahietal.,2023;Hecketal.,2020),which
ventseasyend-userinspectionoftheinformation wehavepreviouslydifferentiatedfromSPICE.
themodelistrackingforfutureinteractions. Furthermore,semanticparsinghastraditionally
Advances in multimodal dialogue systems, in- been limited to textual inputs and unimodal ap-
corporatingtext,image,andaudioinputs(Liuetal., plications. It has been extended to visual modal-
2023;Zhuetal.,2023;Daietal.,2023;Zhangetal., ities, notably in automated Scene Graph Genera-
2023a;Maazetal.,2023),edgeclosertoSPICE’s tion(SGG)tasks(Zhangetal.,2023b;Abdelsalam
vision of multimodal communication. Yet, these etal.,2022;Zareianetal.,2020). Althoughthere
systems cannot often distill accumulated knowl- has been exploration into using spoken audio for
edgeintoconcise,understandableformats,instead semanticparsing(Tomaselloetal.,2022;Coucke
still relying on raw dialogue histories or opaque etal.,2018;Lugoschetal.,2019;SenandGroves,
embeddingsforpriorcontext. 2021), these efforts have been constrained by fo-
While some systems are beginning to interact cusing on simple intent and slot prediction tasks,
with and update external knowledge bases, these and have not incorporated contextual updates or
interactionstendtobeunidirectional(Chengetal., complexsemanticoutputs.
2022;Wuetal.,2021)orinvolveknowledgestor- As such, we believe SPICE to be considerably
age as extensive, barely processed texts (Zhong distinctfromanyworksthathavecomepreviously.
et al., 2023; Wang et al., 2023). Dialogue State While individual components of SPICE’s frame-
Tracking (DST) (Balaraman et al., 2021) shares work have been studied, such as semantic pars-
similaritieswithSPICEinthatagentsuseandup- ingfromaudio,context,ormultimodalinputs,no
datetheirknowledgebasesduringdialogues. How- workhasutilizedalloftheseatonce. Additionally,
ever, most DST efforts are unimodal, with lim- SPICEgoesbeyondmostsemanticparsinganddia-
itedexplorationofmultimodalinputs(Kotturetal., logueworks,eventhoseoperatingonsomeformof
2021). Moreover,existingdatasetsandmodelsfor knowledgerepresentation,bytaskingtheagentto
DST do not align with the SPICE framework, as producecontinualupdatestosaidknowledgegraph
theyoftenrelyonregeneratingtheknowledgebase andtomaintaintheminaninterpretableformat.
witheachdialoguestepfromallhistoricaldialogue
inputswithoutofferingastructuredrepresentation 3 TaskDefinition
ofthepriorcontext. SPICE,conversely,envisions
sequential updates based on and directly applied Semantic Parsing in Contextual Environments
topriorcontext,afeaturenotyetexploredinDST. (SPICE) is defined as follows. Consider a model
Further,weareunawareofanyDSTworkthathas agent,denotedasa,designedtomaintainandup-
attemptedtoutilizespokenaudio. dateaworldstateacrossinteractiontimesteps. Let
C represent this world state during the ith turn.
i
2.2 SemanticParsing ForinterpretabilityanddownstreamuseC isrep-
i
SemanticParsinginvolvestranslatingnaturallan- resentedasaformalknowledgegraph(Chenetal.,
guageintoastructured,symbolic-meaningrepre- 2020). Thisstaterepresentstheaccumulatedcon-
sentation. Traditional semantic parsing research textfrompriorinteractions. Initially,C i canbeset
focusesonprocessingindividual,short-spaninputs toadefaultoremptystate.
toproducetheirsemanticrepresentations(Kamath Duringeachinteractionturn,theagentencoun-
and Das, 2019). Some studies have explored se- ters a set of new inputs, referred to as informa-
manticparsingindialoguesorwithcontextualin- tioninputsFm,withmindicatingthediversityof
i
puts,knownasSemanticParsinginContext(SPiC) modalities the agent is processing. The agent’s
or Context Dependent Semantic Parsing (CDSP) goalistoconstructaformalsemanticparse,P =
iDataset #Scenes #Nodes #Predicates Avg. Size
VisualGenome(Krishnaetal.,2016) 108077 76,340 - -
VG80K(Zhangetal.,2019) 104832 53304 29086 19.02
VG150(Xuetal.,2017) 105414 150 50 6.98
Ours 22346 2032 282 19.64
Table1: ComparisonofourVisualGenomecurationstatisticstootherworks. FurtherdetailsareinSectionD.
a(Fm,C ). Thisparseisformulatedbyintegrating parseformatisdiscussedinSectionG.
i i
the prior context C i with the new information in- Foreachcontextpair(C i,C i+1),featuresfrom
putsFm. Withtheaidofanexecutionfunctione, C and modified features for C are structured
i i i+1
thisresultsinanupdatedcontextC i+1 =e(P i,C i). into natural language prompts. These prompts
This newly formed context C i+1 should repre- areprocessedbytheLlama270BLLM(Touvron
sentalltaskessentialinformation,bothfrompre- etal.,2023a)togenerateplausiblesentencesthat
vious context C i and the most recent interaction describethedifferencebetweenC i andC i+1. We
round,forfuturerounds. C i+1 isexpectedtoalign thensynthesizespokenversionsofthesesentences
withareferencecontext,denotedasCˆ i+1,which via the Tortoise-TTS-V2 (Betker, 2022) text-to-
representstheidealpost-interactionstate. speech(TTS)synthesissystem. Weconfigurethe
TTS model to randomly sample speaker charac-
4 Dataset teristics from its pretrained latent space, and use
the built-in “high_quality” setup for other gener-
ThissectionintroducesVG-SPICE,anoveldataset
ationsettings. BeforeTTSconversionfilteringis
forSPICEtasks,providingastructuredbenchmark
performedonthetextualutterancestoremovecom-
for model training and evaluation. To our knowl-
monrecurrenttermsindicativeofnewinformation
edge, VG-SPICE is the first of its kind and is de-
(eg.,"therenowisa"versus"thereisa"). Theaudio
rived from the Visual Genome dataset (Krishna
recordings and visual images are the multimodal
etal.,2016)tosimulatea“tourguide”providing
inputsFm ofVG-SPICE,emphasizingspokenau-
sequential descriptions of aspects of the environ- i
dioforpracticalityinreal-worldapplicationsand
ment. Inthesescenarios,thetourguidedescribesa
necessitatingaddressingthechallengesofsemantic
visualscenewithsequentialutterances,eachintro-
parsing from audio such as speaker diversity and
ducingnewelementstothescene. Thesedescrip-
noiserobustness. Thepresenceofbothtextualand
tions,combinedwithapre-establishedworldstate
spokenaudiorepresentationsfortheupdateutter-
ofthescene,mimictheaccumulationofworldstate
ancesallowsVG-SPICEtobeutilizedforsemantic
informationthroughsuccessiveinteractions.
parsingevaluationsineithermodality.
VG-SPICE utilizes the Visual Genome’s 108k
VG-SPICE includes over 131k SPICE update
imageswithhuman-annotatedscenegraphsforen-
samplesfrom20kuniquescenes,with2.5%allo-
tity identification via bounding boxes, originally
cated to each of the validation and test sets, en-
detectedusinganobjectidentificationmodel. The
suring distinct scenes across splits. We perform
graphs include named nodes, optional attributes,
noiseaugmentationontheinputspeechusingthe
anddirectededgesforrelationalpredicates.
CHiME5 dataset (Barker et al., 2018) to simu-
The dataset is constructed by extracting sub-
late realistic noise conditions, with performance
graphsfromscenegraphsastheinitialcontext,C ,
i evaluatedatvariousSignaltoNoiseRatios(SNR).
sampledfromemptytonearlycomplete. Theseare
VG-SPICEsamplesandsummarystatisticsarepre-
then augmented by reintegrating a portion of the
sentedinFigure1andTable2,respectively.
omitted graph to form the updated context, C .
i+1
Beforeextractingoursamples,theVisualGenome
4.1 ChallengeSubset
dataunderwentpreprocessingtoenhancedataset
quality(SectionDandsummaryresultsshownin Inadditiontothestandardtestset,weaugmentVG-
Table1). Thedatasetallowsflexiblemodelimple- SPICEwithanadditionalChallengeSubset, VG-
mentation with semantic parses (P ) and parsing SPICE-C.Althoughthissubsetissmall,spanning
i
functions(e)notpredefined,allowingflexibilityin only50individualvisualscenes,itprovidesdistinct
modelingimplementation. Ourmodel’ssemantic capabilitiesnotpresentintheprimaryVG-SPICEStatistic Value spokenutterancesbyaindividualhumanannotator.
#Samples 131362 This Challenge Subset offers a rigorous evalu-
#UniqueScenes 22346 ation framework for models, promoting advance-
HoursofAudio 10.56 ments in handling diverse visual representations,
Avg. WordsperUtterance 71.83 maintaininghigh-qualityscenegraphs,performing
Avg. NodesAdded 1.27 coherent iterative updates, and managing out-of-
Avg. AttributesAdded 0.93 domainandreal-worldspeechscenarios.
Avg. EdgesAdded 0.60
5 AViD-SPModel
Table2: SummarystatisticsforourVG-SPICEdataset.
To address the challenges of VG-SPICE, our ap-
proachutilizesarangeofpretrainedmodels,specif-
testdataset,asdetailedbelow.
icallyfine-tunedtoenhanceSPICE-focusedseman-
Broad Visual Representation: To sample the
tic parsing capabilities. Figure 2 illustrates our
ChallengeSubset,weusedarepresentation-based
modelarchitecture,termedAudio-VisionDialogue
process to promote diverse image types. We ob-
SceneParser(AViD-SP).Atthecoreofourframe-
tainedtheCLIP3 representationsforeachimagein
work lies the pretrained Llama 2 7B model (Tou-
theoriginalVG-SPICEtestsplit. UsingKMeans
vronetal.,2023b). Despitedeployingitssmallest
clustering,thedatasetwaspartitionedinto50dis-
variant,theextensivepretrainingendowsourmodel
tinct groupings of visual representations, with a
withrobustfunctionalabilities,particularlybene-
singlesampletakenfromeachcluster.
ficial for processing the diverse semantic parses
ManualSceneGraphQualityEnhancements:
inherenttoVG-SPICE.However,Llama2,trained
Despite automated generation processes in VG-
ontextualdata,lacksinherentsupportforthemul-
SPICEaimingtoimprovescenegraphquality,per-
timodalinputstypicalinVG-SPICE.
sistentissuesremain. Toensureacleanandreliable
Toaccommodatediverseinputs,weextendtech-
testingsubset,manualscenegraphimprovements
niquesfrompriorstudies(Rubensteinetal.,2023;
weremadetoensurethefinalscenegraphforeach
Gong et al., 2023; Lin et al., 2023) by projecting
imagewasaccurate. Thisinvolvedremovingincor-
embeddingsfrompretrainedmodality-specificfea-
rect, low-quality, or duplicate scene features and
ture extractors. This approach has been proven
enhancingthescenegraphstoachievefargreater
to enable text-based LLMs to process informa-
densitythanoriginallypresentinVG-SPICEorVi-
tionacrossvariousmodalities. Directlyintegrating
sualGenome,particularlyforEdgesandAttributes.
these projected embeddings into the LLM’s con-
CoherentIterativeUpdates: Toimprovesam-
textwindow,however,introducessignificantcom-
plediversity,VG-SPICEwasgeneratedinaniter-
putationaloverheadduetotheirtypicallyextensive
ativelyincoherentfashion,meaningsamplesfora
contextlengths. Whilepreviousresearchoftenem-
singleupdatecannotbeusedtocoherentlyevaluate
ployedpoolingmethods(Gongetal.,2023)tocon-
end-to-endSPICEevaluations. FortheChallenge
denseembeddingsbymodality,thisstrategyincom-
Subset,wemanuallyannotatedeachofthe50sam-
pletelyaddressesthechallengesofmergingvaried
pled scenes with five individual utterances, each
modalityembeddingsforLLMuse. Forinstance,
addingnovelinformationwhilereferringtoprevi-
audioembeddingsofferfinertemporalgranularity
ously mentioned details. These utterances are of
thantextualembeddings, andthereverseisoften
greaterdiversityandquality(duetomanualannota-
true for vision embeddings, complicating the ad-
tionratherthanLLMproduction)andcanbeused
justmentofdownsamplingfactors. Moreover,even
sequentiallytoevaluatescenegraphgenerationer-
withoptimizeddownsampling,pooledembeddings
rorsovermultipleinteractionrounds.
must preserve their original sequential order and
OOD and Real Speech: To enhance the eval-
arerestrictedtoinformationfromsolelythepooled
uative capabilities of the Challenge Set, we pro-
segments. Many applications could benefit from
vide speech samples for the utterances from two
capabilitiestoestablishdownsampledfeaturesen-
sources: Tortoise-TTS as used for the remainder
compassingbothlocalandglobalcontextsandto
of VG-SPICE (with three random voice samples
rearrangethesefeaturestoanextent.
perutterance)aswellasmanualrecordingsofthe
To surmount these challenges, we introduce a
3openai/clip-vit-base-patch32fromHuggingface novel Grouped Modality Attention Down Sam-pler (GMADS) module. This module initially quences,somelevelofsimilarityisexpected,and
projectsembeddingsfromnon-textualmodalities entirely dissimilar values (cosine similarity less
intoaunified,fixed-dimensionalspace. Weform than zero) are not feasible. Thus, we modify Eq.
a set of modality groupings, one for each input 2toincludeaslightmarginallowingforminimal
modality(audioandvisualwithVG-SPICE),anda similarity. Below,e representsasingleGMADS
i
cross-modalitygroupingderivedfromconcatenat- outputembedding(pre-outputprojection)withina
ingallmodalityembeddings,eachprefixedwitha batchofB sequences,eachoflengthK.
modality-specifictoken. Aseriesofself-attention
layers processes each embedding sequence and
downsamplestheoutputsbyafactorofS through
ℓ =
2(cid:80)B i=∗ 1K−1(cid:80)B j=∗ iK +1max( ∥eie ∥i∗ ∗e ∥j ej∥−h,0))
(2)
meanpooling. Thesevaluesarethenconcatenated Ortho B∗K∗(B∗K−1)
with the mean-pooled pre-self-attention embed- TheGMADSmoduleattemptstoprovideseveral
dings along the embedding dimension, akin to a advantagesoverthedirectuseofrawmodalityem-
skipconnection. Afinalprojectionadjuststheout- beddingswiththeLLMdecoderormeanpooling.
puts to match the dimensionality of the Llama 2 Firstly,GMADSoperatesatreduceddimensional
7Bdecoder,andallembeddingsequencesarecon- scalescomparedtothepretrainedLLM,whichsig-
catenated. Thisprocessyieldsanembeddingout- nificantlylowersmemoryrequirements,requiring
put that is effectively downsampled by a factor themuchlargerdecodertoprocessshorter(reduced
of S/2. All weights in the GMADS module are toonly2/S thesize)inputsequences. Moreover,
shared across the groups, substantially reducing themodalityinputsdonotnecessitateautoregres-
the parameter count. Additionally, we employ a sivegenerationalongsidetheseinputs,furthercon-
self-supervised representation learning objective serving cost. Secondly, GMADS empowers the
on the embeddings from the downsampled cross- model to selectively learn its downsampling pro-
modality group outputs by upsampling them to cess,includingchoicesonwhethertofocuslocally
theiroriginalsizeandthenprocessingthemthrough orintegrateglobalfeatures,allowingsomedegree
asecondarysetofself-attentionlayers. Therecon- ofinformationrestructuring. Theincorporationof
structedcross-modalityembeddingsarethenseg- cross-modalityencodingenablespartsofthedown-
mentedbymodality,withper-modalityprojections sampledembeddingstocaptureessentialinforma-
strivingtorestorethemtotheiroriginalinputsize. tionacrossmodalitieswhilemaintainingindividual
We apply a contrastive reconstruction loss objec- modalitycomponentsintheoutputsensuringthat
tiveasoutlinedinEq. 1,usingthecorresponding some portion of the output embeddings is condi-
groundtruthembeddingasananchorandallother tioned on each modality, requiring the attention
embeddingsinthebatchascontrastivesamples. mechanismstoremainsensitivetoallmodalities.
Forfeatureextraction, weutilizethevisualen-
ℓ n,Contrast =(cid:80) jB =∗ 1Klog (cid:80) kB =∗K 1e [x kp ̸=(s i]im ex( pz (i s, iz mj) (/ zτ i,) zk)/τ) coderfromDINOv2(Oquabetal.,2024)forvisual
inputs and the encoder from Whisper-Large V3
(1)
(Radford et al., 2022) for audio. We retain only
In this equation z i denotes the reconstructed in- thenecessaryencoderportionsofthesepretrained
put embedding, K represents the length of each models. In alignment with successful semantic
sequence,B denotesthebatchsize,andτ isatun- parsingeffortsfromspeech(Aroraetal.,2023),we
abletemperaturehyperparameter. performASRtranscriptionontheaudio,appending
Wealsoobservedthatnon-textualmodalityin- thesetextualembeddingstothepriorcontextem-
putstendedtocollapsewhencombinedwithsim- beddings. ASRtranscriptionsaregeneratedusing
pler textual inputs, such as prior context or ASR theWhisper-medium.enmodel. Toenablescalable
transcripts. To counter this, we include an addi- fine-tuning, we integrate LoRa adaptation layers
tional orthogonality loss, designed to encourage intoLlama27Bandfreezeallfeatureextractors.
maximaldissimilarityamongalignedembeddings
5.1 TrainingRoutine
ineachbatchsequence. Thismethodologyissimi-
lartopreviouseffortstopromotedistinctclassem- WetrainAViD-SPusingcross-entropyloss(Eq. 3)
beddings(Ranasingheetal.,2021),butinourcase, betweenthepredictedandreferenceFormalSeman-
wetreateachembeddingasadistinctclasssample. ticParses,alongsidetheobjectivesinEq. 1and2.
However,giventhenatureoftheseembeddingse- OurcomprehensivelossfunctionisoutlinedbelowWe train AViD-SP by incorporating randomly
sampledCHiME5noisetosimulateaudiocorrup-
tion,addingthisnoiseatvariousSignal-to-Noise
Ratios (SNR) of 0, 2, 5, 10, or 20dB. Further de-
tailsontrainingandinferencehyperparametersare
discussed in Section E. To ensure robustness to
variousinputfeaturecombinations,weimplement
random input dropout with a probability of 30%.
In these instances, we randomly omit one of the
inputmodalities,eitheraudioembeddings,visual
embeddings, or audio transcriptions. We do not
omit the prior context, as we found the task too
difficulttolearnundersuchconditionssinceitre-
quiresboththealreadyknowninformationaswell
as their current assigned labels under our seman-
tic parsing framework. AViD-SP is trained in a
two-stagepipeline,withtheinitialstageactingas
pretrainingwithouttheASRtranscriptionstoallow
theGMADSmoduletoreachasemi-trainedstate
forenhancedefficiency. Subsequently,wecontinue
Figure2: a)ThearchitectureoftheAViD-SPmodelfor fine-tuningthemodelwithASRtranscriptionsuntil
VG-SPICE,integratingpretrainedencodersandlarge convergence. Ourinitialpretraininglastsonefull
languagemodels(LLMs)withLoRaadaptersandfea-
epochs,followedbythefine-tuningstage.
turefusionmodules.Trainedandfrozensegmentsofthe
modelaredenotedbyfireandsnowflakeicons,respec- 5.2 EvaluationMetrics
tively. b)OurnovelGroupedModalityAttentionDown
Weuseseveralmetricstomeasurehowcloselythe
Samplermodule,enablingintegratedcross-modalityfu-
sionanddownsampling. Greenmodulesshareweights. generated semantic parse aligns with the ground
Fordownsampling,weutilizemeanpooling,andforup- truthandhowaccuratelythescenegraphcontext
samplingwelinearlyinterpolatetheembeddings. updatesmatchthereference. Unlikeconventional
semantic parsing assessments (Tomasello et al.,
2022), we omit exact-match metrics due to their
inEq. 4,wherep denotesthesoftmaxprediction
i,k unsuitabilityforourproblem,whichallowsforper-
foreachofthek tokensinP , andt represents
i i,k mutationinvarianceintheformal-languageoutput
thecorrespondingground-truthtokenlabel.
(seeSectionG).Thispermitstheparsertogenerate
scene-graphupdatesinanyorderandassignnode
n
(cid:88)
ℓ = − t log(p ) (3) IDsfreely,aslongastheresultingscenegraphis
CE i,k i,k
isomorphictothereference.
k=1
Foreachbelowmetric,weexaminehard("H")
N
γ (cid:88) and soft ("S") variants. The hard variant penal-
L = αℓ +βℓ + ℓ (4)
CE Ortho N n,Contrast izes missing and unnecessary information, while
n=1
thesoftvariantonlypenalizesomissions. Thisap-
AViD-SP employs a three-layer self-attention proach accounts for the Visual Genome dataset’s
transformer as the primary encoder transformer, sparsity and the possibility of LLMs generating
eachlayerhavinganembeddingdimensionalityof extraneous yet potentially valid content. For ex-
1024and8attentionheads. Thesecondaryencoder ample,anLLMmightenhancea"bluetable"toa
transformer,usedfortheupsampledreconstruction "vibrant blue table," making "vibrant" an accept-
trainingobjective,isofthesameconfiguration. The ableattribute. Ouranalysisshowssuchinclusions
GMADSmoduleemploysadownsamplingfactor, arecommonintheVG-SPICEdataset,leadingus
S,of16. Additionally,weenhancethekey,query, tofocusonthesoftmetricandqualitativelyshow
and value layers of the Llama 2 7B model with inSection6howupdatedutterancesaccommodate
Low-RankAdaptation(LoRa)layers. Nohyperpa- theseextraneousadditions. Weincluderesultsfor
rameteroptimizationwasconducted. GEDinthesupplementTable5.Graph Edit Distance (GED): GED calculates ourevaluations. ToablatetheeffectsourGMADS
thenormalizedcosttotransformthepredictedcon- modulehasonperformancewecompareagainsta
text to the reference one, considering only per- versionofAViD-SPtrainedusingtraditionalmean-
fectly semantically equivalent Nodes, Attributes, pooling after a per modality projection layer to
and Edges. Missing or extra Nodes or Edges in- downsample the audio and visual input embed-
creasetheerrorbyone,whileincorrectAttributes dings,withallhyperparameterandtrainingmeth-
haveasmallerpenaltyof0.25. GEDisnotnormal- ods matched between the two except the mean-
izedandshouldbeinterpretedasthemagnitudeof pooling baseline only utilizing the cross entropy
incorrectfeaturescomparedwiththereferenceso- componentofthefulltrainingobjective.
lutionandnotasarecallorprecisionmetric. GED We also extended our evaluations to the VG-
is particularly reliant on exact matches, so minor SPICE-C Subset. Here, we analyze the subset
discrepancies(like"snowboard"vs. "snowboard") through a single-step evaluation approach, with
canincursignificantpenalties,withmisalignments ground truth prior context provided and metrics
doublypenalizedinthehardvariant. measuredaftereachindividualSPICEupdate.
RepresentationEditDistance(RED): REDad-
6 Results
dresses the limitations of GED by employing a
“softer”semanticsimilaritytoevaluateentitypair- The performance of the AViD-SP model on the
ings. Using a transformer model for sentence se- VG-SPICE test set, as shown in Table 3, demon-
mantic similarity4, RED groups Nodes and their stratesthatthebaselineAViD-SPachievesS-RED
Attributesintodescriptivephrases(forexample,a scoresjustbelow0.4,withthemeanpoolingvari-
"table"Nodewith"vibrant"and"blue"Attributes antslightlylower,approaching0.38. Thisperfor-
becomes"vibrantbluetable")andassessesthedis- mance suggests a substantial effectiveness (over
similaritybetweenpotentialpairings,usinganex- 60%)inassimilatingdesiredinformationintothe
haustic search for optimal pairings of Nodes and scene graph. However, the H-RED metrics indi-
Edges. Unmatched Nodes and Edges are consid- catetheintroductionofmoderatequantitiesofir-
ered entirely dissimilar. Since unmodified graph relevantinformation,particularlyintheGMADS
portions from the prior context are pre-matched version. Given that VG-SPICE scene graphs are
andexcludedfromtheexhaustivesearch,thecom- often overly sparse, the elevated H-RED values
putationofthepairingsremainsmanageable. RED for GMADS may reflect an increased utilization
is normalized by the representation edit distance of visual inputs, possibly learning to incorporate
neededtotransformthepriorcontextintotherefer- non-essentialfeaturesdetectedthroughvisualcues.
encecontext,andsonumericallycanbeinterpreted Whilethisinterpretationisspeculative,somelevel
asthepercentageofmissingand/orextrainforma- of elevated H-RED could be reasonable for VG-
tionrelativetothereferencecontext. SPICEinitscurrentstate(SectionC).
Under varying SNR conditions, both GMADS
5.3 BaselinesandEvaluation
andmeanpoolingconfigurationsofAViD-SPshow
To thoroughly evaluate our AViD-SP model, we minimalperformancedegradationatlowerSNRs,
conducted a series of ablation studies to explore indicating resilience to reasonable background
theimpactofvariousinputmodalitycombinations. noise levels. The use of accurate ASR transcrip-
Given that AViD-SP was trained under diverse tionssubstantiallyboostsparsingaccuracy,empha-
noiseconditions,itsperformancewastestedacross sizingthebenefitsofreliableASR.
noiselevelsof0,2,and20dBusingtheCHiME5
Experimentsomittingvisualinputsorincorpo-
dataset. Weassessedthemodel’scapabilitytore-
rating incorrectly paired visual inputs exhibit mi-
solve ambiguities in audio input by introducing
nor performance declines. For the meanpooling
testswithandwithoutvisualmodality,andbyeval-
basedAViD-SPaslightlylarger,butstillquitemi-
uatingthemodelwithincorrectlymatchedimages
nor,degradationinmetricperformanceisobserved
intheGMADSmodule. Additionally,weexplored
when audio inputs are excluded, with only ASR
potential enhancements in ASR performance by
transcriptions being provided. However, a more
incorporatinggroundtruthASRtranscriptionsin
significantdegradationisobservedfortheGMADS
variant of AViD-SP under these same conditions.
4The “en_stsb_roberta_base” model from
https://github.com/MartinoMensio/spacy-sentence-bert ThisimpliesthattheGMADSmultimodaladapta-ModelType H-RED↓ S-RED↓
0dB 2dB 20dB Gold* 0dB 2dB 20dB Gold*
AViD-SP+GMADS Base 1.618 1.517 1.412 1.272 0.402 0.383 0.3765 0.348
w/oImage 1.611 1.527 1.430 1.33 0.407 0.393 0.384 0.364
w/oAudio 1.660 1.607 1.590 1.540 0.570 0.559 0.538 0.481
wIncorrectImage** - - 1.423 - - - 0.381 -
w/oPriorContext*** - - 3.428 - - - 0.478 -
AViD-SP+Meanpool Base 1.083 1.038 0.940 0.817 0.377 0.368 0.359 0.323
w/oImage 1.051 0.980 0.911 0.826 0.386 0.377 0.362 0.330
w/oAudio 0.946 0.897 0.804 0.759 0.414 0.397 0.385 0.363
Table3: REDresultsontheVG-SPICEtestsetforourAViD-SPmodel. AViD-SPwastrainedwithCHiME5noise
augmentationsampledbetween0dband20dBSNR(allCHiME5noisefollowedtheprovidedtrain/eval/testsplits).
*GiventhegroundtruthutterancetranscriptsinplaceoftheASRtranscriptions. **Evaluatedbyoffsettingvisual
featureswithinbatchsoincorrectimagefeaturesarepairedwiththeotherinputcomponents. ***Evaluatedwith
"EmptyContext"priorstatescenegraphssummariesinsteadofthecorrectones.
Variant TTS Read ancy suggests that GMADS possesses more ro-
H-RED↓ S-RED↓ H-RED↓ S-RED↓
bustmultimodalprocessingcapabilities,especially
GMADS 0.739 0.497 0.731 0.497
Meanpool 0.640 0.460 1.415 0.628 in processing out-of-domain real audio distribu-
tions. SincebothmodelvariantsusethesameASR
Table4: REDresultsontheVG-SPICE-Cchallenge
modelwithoutparametertuning,theobserveddif-
testsetforAViD-SPwithSingleStep(groundtruthprior
ferencesindicatethatGMADScompensatesmore
contextprovidedforeachstep)metricsreported.
effectivelyforpoorerASRperformance.
tionprocesshasresultedinamodelwhichismore 7 Conclusion
sensitivetotherawaudioinputsthanwhenmean-
pooling is used, which seems to dominantly rely In this paper, we introduced Semantic Parsing in
onthenativelytextualASRtranscriptions. Wethe- ContextualEnvironments(SPICE),aninnovative
orize that the enhanced capability of GMADS to taskdesignedtoenhanceartificialagents’contex-
processmultimodalinputsmayleadtoitsoverall tual understanding by integrating multimodal in-
worseresults,asitproducesamorecomplexopti- putswithpriorcontexts. Throughthedevelopment
mizationlandscapecomparedwithsimplycollaps- of the VG-SPICE dataset and the Audio-Vision
ingtoutilizeonlythenativetextualASRtranscripts. Dialogue Scene Parser (AViD-SP) model, we es-
Additionally,theabsenceofpriorcontextmarkedly tablished a framework for agents to dynamically
increaseserrorrates,underscoringtheimportance update their knowledge in response to new infor-
ofhistoricalcontextforaccurateSPICEupdates. mation,closelymirroringhumancommunication
Table 4 presents the performance of AViD-SP processes. TheVG-SPICEdataset,craftedtochal-
on the VG-SPICE-C test set. For TTS audio, the lengeagentswiththetaskofvisualscenegraphcon-
metricsdivergesignificantlyfromthoseofthestan- struction from spoken conversational exchanges,
dardVG-SPICEtestset,featuringhigherS-RED representsasignificantstepforwardinthefieldof
and lower H-RED scores. The higher density of semanticparsingbyincorporatingbothspeechand
VG-SPICE-C’sscenegraphs,whichincludefewer visualdataintegration. Meanwhile,theAViD-SP
visually or auditorily supported features that are model, equipped with the novel Grouped Multi-
untracked in reference scene graphs, likely con- modal Attention Down Sampler (GMADS), pro-
tributes to these lower Hard metric scores. How- vides a strong initial baseline for VG-SPICE as
ever,thisincreaseddensityalsopresentsagreater wellasinsightsintopotentialmethodstoimprove
challengeinachievingimprovedSoftmetricscores, multimodalinformationprocessingandintegration.
asthemodelmustcorrectlyincorporateasubstan- Ourworkhighlightstheimportanceofdevelop-
tialamountofinformationateachupdatestep. ingsystemscapableofunderstandingandinteract-
FortheGMADS-basedAViD-SP,performance ingwithincomplex,multimodalenvironments. By
metricsonthereadaudioportionofVG-SPICE-C focusing on the continuous update of contextual
aligncloselywiththoseobservedintheTTSpor- statesbasedonnew,andmultimodal,information,
tion. Conversely, themeanpoolingvariantshows SPICErepresentsashifttowardsmorenaturaland
asubstantialperformancereduction. Thisdiscrep- effectivehuman-AIcommunication.8 Limitations Moreover, VG-SPICE, while pioneering in
SPICE tasks, is only a start, limited to audio and
While VG-SPICE and AViD-SP are novel ap- images,withabasiclanguageforknowledgegraph
proaches,theyhaveseverallimitationsandshould updates. Futureresearchshouldaddresstheselimi-
betreatedasinitialattemptstowardfurtherSPICE tationsbyincorporatingmorerealisticinputs,like
implementationsandbenchmarks. Themainlim- video, 3D environments, and paralinguistic cues,
itation stems from the extensive use of synthetic and by exploring dynamic tasks beyond simple
data augmentation in VG-SPICE’s creation. The scene graph updates. Environments like Matter-
process involved several steps, including dataset port3D (Chang et al., 2017) or Habitat 3.0 (Puig
preprocessingwithBERT-likePOStaggers,craft- etal.,2023)offerpromisingavenuesforembodied
ingupdateutterancesusingtheLlama270BLLM, SPICEresearch. ExpandingSPICEtoincludesec-
andgeneratingsyntheticTTSaudio. Thesestages ondarytasksthatrelyonanagent’scontextualun-
mayintroduceerrors,hallucinations,oroverlysim- derstandingcanalsoenhanceitsutility,suchasaid-
pledatadistributions,potentiallymisaligningwith inginmedicalimageannotationwithco-dialogue.
real-worldapplications. Forexample,ourmodels’
resiliencetobackgroundnoisemayreflectthespe-
cificTTSaudiodistribution,possiblysimplifying References
the ASR model’s speech discernment. Addition-
Mohamed Ashraf Abdelsalam, Zhan Shi, Federico
ally, the Visual Genome, our work’s foundation, Fancellu,KalliopiBasioti,DhaivatBhatt,Vladimir
suffers from notable quality issues, such as poor Pavlovic,andAfsanehFazly.2022. Visualsemantic
parsing: From images to Abstract Meaning Repre-
annotationsandunreliablesyntheticobjectsegmen-
sentation. InProceedingsofthe26thConferenceon
tation, which, despite efforts to mitigate, remain
ComputationalNaturalLanguageLearning(CoNLL),
challengesinVG-SPICE.WhiletheincludedVG- pages282–300,AbuDhabi,UnitedArabEmirates
SPICE-Ctestsubsetattemptstoimprovetheselim- (Hybrid).AssociationforComputationalLinguistics.
itations,andindeedthehardversionsofaremetrics
Siddhant Arora, Hayato Futami, Yosuke Kashiwagi,
aresignificantlyimprovedonthemanuallycleaned Emiru Tsunoo, Brian Yan, and Shinji Watanabe.
samplesofthissubset,theyarestillcomprisedof 2023. Integratingpretrainedasrandlmtoperform
intentionally crafted utterances with read audio, sequencegenerationforspokenlanguageunderstand-
ing. ArXiv,abs/2307.11005.
whichmaynottransfertoreal-worldapplications
andnaturalspokenaudio. Further,thisworkonly VevakeBalaraman,SeyedmostafaSheikhalishahi,and
includes analysis of the VG-SPICE-C challenge BernardoMagnini.2021. Recentneuralmethodson
dialoguestatetrackingfortask-orienteddialoguesys-
subsetinthesimpleSingleSteptaskanddoesnot
tems: Asurvey. InProceedingsofthe22ndAnnual
evaluateinend-to-endsequence-basedanalysis.
MeetingoftheSpecialInterestGrouponDiscourse
The various version of AViD-SP we introduce andDialogue,pages239–251,SingaporeandOnline.
also provides indications of further development AssociationforComputationalLinguistics.
forefficientmultimodaladaptationmethodologies.
JonBarker,ShinjiWatanabe,EmmanuelVincent,and
While the version utilizing GMADS generally Jan Trmal. 2018. The fifth ’chime’ speech separa-
failed to outperform the results of the traditional tion and recognition challenge: Dataset, task and
baselines.
meanpoolingversiontheGMADSmethodalsopro-
videdastrongerindicationofcross-modalityfea-
JamesBetker.2022. TorToiSetext-to-speech.
ture utilization, whereas integration of simplisti-
AngelChang,AngelaDai,ThomasFunkhouser,Maciej
callydownsampledmultimodalfeaturesalongside
Halber, MatthiasNiessner, ManolisSavva, Shuran
nativetextualfeaturesappearstocausestrongun-
Song, Andy Zeng, and Yinda Zhang. 2017. Mat-
derutilization and feature collapse for the multi- terport3d: Learningfromrgb-ddatainindoorenvi-
modal features. This is further supported by the ronments. International Conference on 3D Vision
poor performance achieved by the meanpooling (3DV).
version of AViD-SP, relative to the GMADS ver-
XiaojunChen,ShengbinJia,andYangXiang.2020. A
sion,onrealhumanrecordedaudio,indicatingthe review: Knowledgereasoningoverknowledgegraph.
meanpooling version adapts much worse to out- ExpertSystemswithApplications,141:112948.
of-domainmultimodalinputs. Wesuggestfuture
Jianpeng Cheng, Devang Agrawal, Héctor
worktocontinueinvestigatingmethodssimilarto
Martínez Alonso, Shruti Bhargava, Joris Driesen,
GMADStofurtherrealizetheirtheoreticalbenefits. Federico Flego, Dain Kaplan, Dimitri Kartsaklis,Lin Li, Dhivya Piraviperumal, Jason D. Williams, for value independent neural dialog state tracking.
Hong Yu, Diarmuid Ó Séaghdha, and Anders In Proceedings of the 21th Annual Meeting of the
Johannsen.2020. Conversationalsemanticparsing SpecialInterestGrouponDiscourseandDialogue,
for dialog state tracking. In Proceedings of the pages 35–44, 1st virtual meeting. Association for
2020ConferenceonEmpiricalMethodsinNatural ComputationalLinguistics.
LanguageProcessing(EMNLP),pages8107–8117,
Online.AssociationforComputationalLinguistics. AlbertQ.Jiang,AlexandreSablayrolles,ArthurMen-
sch,ChrisBamford,DevendraSinghChaplot,Diego
ZhoujunCheng,HaoyuDong,ZhiruoWang,RanJia, delasCasas,FlorianBressand,GiannaLengyel,Guil-
JiaqiGuo,YanGao,ShiHan,Jian-GuangLou,and laumeLample,LucileSaulnier,LélioRenardLavaud,
DongmeiZhang.2022. HiTab: Ahierarchicaltable Marie-AnneLachaux,PierreStock,TevenLeScao,
datasetforquestionansweringandnaturallanguage Thibaut Lavril, Thomas Wang, Timothée Lacroix,
generation. InProceedingsofthe60thAnnualMeet- andWilliamElSayed.2023. Mistral7b.
ingoftheAssociationforComputationalLinguistics
(Volume1: LongPapers),pages1094–1110,Dublin, AishwaryaKamathandRajarshiDas.2019. Asurvey
Ireland.AssociationforComputationalLinguistics. onsemanticparsing.
AakankshaChowdhery,SharanNarang,JacobDevlin, SatwikKottur,SeungwhanMoon,AlborzGeramifard,
Maarten Bosma, Gaurav Mishra, Adam Roberts, andBabakDamavandi.2021. SIMMC2.0: Atask-
Paul Barham, Hyung Won Chung, Charles Sutton, oriented dialog dataset for immersive multimodal
Sebastian Gehrmann, Parker Schuh, Kensen Shi, conversations. In Proceedings ofthe 2021 Confer-
Sasha Tsvyashchenko, Joshua Maynez, Abhishek enceonEmpiricalMethodsinNaturalLanguagePro-
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin- cessing,pages4903–4912,OnlineandPuntaCana,
odkumar Prabhakaran, Emily Reif, Nan Du, Ben DominicanRepublic.AssociationforComputational
Hutchinson, Reiner Pope, James Bradbury, Jacob Linguistics.
Austin,MichaelIsard,GuyGur-Ari,PengchengYin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat, RanjayKrishna,YukeZhu,OliverGroth,JustinJohn-
Sunipa Dev, Henryk Michalewski, Xavier Garcia, son, Kenji Hata, Joshua Kravitz, Stephanie Chen,
VedantMisra,KevinRobinson,LiamFedus,Denny Yannis Kalantidis, Li-Jia Li, David A. Shamma,
Zhou,DaphneIppolito,DavidLuan,HyeontaekLim, Michael S. Bernstein, and Fei-Fei Li. 2016. Vi-
Barret Zoph, Alexander Spiridonov, Ryan Sepassi, sualgenome: Connectinglanguageandvisionusing
DavidDohan,ShivaniAgrawal,MarkOmernick,An- crowdsourceddenseimageannotations.
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai,MariePellat,AitorLewkowycz,EricaMoreira, ZhuangLi,LizhenQu,andGholamrezaHaffari.2020.
Rewon Child, Oleksandr Polozov, Katherine Lee, Contextdependentsemanticparsing: Asurvey. In
ZongweiZhou,XuezhiWang,BrennanSaeta,Mark Proceedings of the 28th International Conference
Diaz,OrhanFirat,MicheleCatasta,JasonWei,Kathy on Computational Linguistics, pages 2509–2521,
Meier-Hellstern,DouglasEck,JeffDean,SlavPetrov, Barcelona,Spain(Online).InternationalCommittee
andNoahFiedel.2022. Palm:Scalinglanguagemod- onComputationalLinguistics.
elingwithpathways.
YuanzhiLiang,YalongBai,WeiZhang,XuemingQian,
Alice Coucke, Alaa Saade, Adrien Ball, Théodore
Li Zhu, and Tao Mei. 2019. Vrr-vg: Refocusing
Bluche, Alexandre Caulier, David Leroy, Clément
visually-relevantrelationships.
Doumouro, Thibault Gisselbrecht, Francesco Calt-
agirone, Thibaut Lavril, Maël Primet, and Joseph
ZiyiLin,ChrisLiu,RenruiZhang,PengGao,Longtian
Dureau.2018. Snipsvoiceplatform: anembedded
Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao,
spokenlanguageunderstandingsystemforprivate-
Keqin Chen, Jiaming Han, Siyuan Huang, Yichi
by-designvoiceinterfaces.
Zhang, Xuming He, Hongsheng Li, and Yu Qiao.
2023. Sphinx: The joint mixing of weights, tasks,
Wenliang Dai, Junnan Li, Dongxu Li, Anthony
and visual embeddings for multi-modal large lan-
Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
guagemodels.
BoyangLi,PascaleFung,andStevenHoi.2023. In-
structblip: Towardsgeneral-purposevision-language
HaotianLiu,ChunyuanLi,QingyangWu,andYongJae
modelswithinstructiontuning.
Lee.2023. Visualinstructiontuning.
TriDao.2023. Flashattention-2: Fasterattentionwith
Loren Lugosch, Mirco Ravanelli, Patrick Ignoto,
betterparallelismandworkpartitioning.
Vikrant Singh Tomar, and Yoshua Bengio. 2019.
YuanGong,HongyinLuo,AlexanderH.Liu,Leonid Speechmodelpre-trainingforend-to-endspokenlan-
Karlinsky,andJamesGlass.2023. Listen,think,and guageunderstanding.
understand.
Muhammad Maaz, Hanoona Rasheed, Salman Khan,
MichaelHeck,CarelvanNiekerk,NurulLubis,Chris- andFahadShahbazKhan.2023. Video-chatgpt: To-
tianGeishauser,Hsien-ChinLin,MarcoMoresi,and wardsdetailedvideounderstandingvialargevision
MilicaGasic.2020. TripPy: Atriplecopystrategy andlanguagemodels.NeauMaëlic,PauloE.Santos,Anne-GwennBosser,and PriyankaSenandIsabelGroves.2021. Semanticpars-
CédricBuche.2023. Fine-grainedistoocoarse: A ingofdisfluentspeech. InProceedingsofthe16th
noveldata-centricapproachforefficientscenegraph ConferenceoftheEuropeanChapteroftheAssoci-
generation. ationforComputationalLinguistics: MainVolume,
pages1748–1753,Online.AssociationforComputa-
Mehrad Moradshahi, Victoria Tsai, Giovanni Cam- tionalLinguistics.
pagna,andMonicaLam.2023. Contextualsemantic
parsingformultilingualtask-orienteddialogues. In PadenTomasello,AkshatShrivastava,DanielLazar,Po-
Proceedingsofthe17thConferenceoftheEuropean ChunHsu,DucLe,AdithyaSagar,AliElkahky,Jade
Chapter of the Association for Computational Lin- Copet, Wei-Ning Hsu, Yossi Adi, Robin Algayres,
guistics,pages902–915,Dubrovnik,Croatia.Associ- Tu Ahn Nguyen, Emmanuel Dupoux, Luke Zettle-
ationforComputationalLinguistics. moyer,andAbdelrahmanMohamed.2022. Stop: A
datasetforspokentaskorientedsemanticparsing.
MaximeOquab, TimothéeDarcet, Théo Moutakanni,
HuyVo,MarcSzafraniec,VasilKhalidov,PierreFer-
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
nandez,DanielHaziza,FranciscoMassa,Alaaeldin
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
El-Nouby, Mahmoud Assran, Nicolas Ballas, Woj-
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
ciechGaluba,RussellHowes,Po-YaoHuang,Shang-
Bhosale,DanBikel,LukasBlecher,CristianCanton
WenLi,IshanMisra,MichaelRabbat,VasuSharma,
Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien
JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
Mairal, Patrick Labatut, Armand Joulin, and Piotr
CynthiaGao,VedanujGoswami,NamanGoyal,An-
Bojanowski.2024. Dinov2: Learningrobustvisual
thonyHartshorn,SagharHosseini,RuiHou,Hakan
featureswithoutsupervision.
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
IsabelKloumann,ArtemKorenev,PunitSinghKoura,
LongOuyang,JeffWu,XuJiang,DiogoAlmeida,Car-
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
rollL.Wainwright,PamelaMishkin,ChongZhang,
anaLiskovich,YinghaiLu,YuningMao,XavierMar-
SandhiniAgarwal,KatarinaSlama,AlexRay,John
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
Schulman,JacobHilton,FraserKelton,LukeMiller,
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
Maddie Simens, Amanda Askell, Peter Welinder,
stein,RashiRungta,KalyanSaladi,AlanSchelten,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
Traininglanguagemodelstofollowinstructionswith
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
humanfeedback.
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
Xavier Puig, Eric Undersander, Andrew Szot,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
MikaelDallaireCote,Tsung-YenYang,RuslanPart-
driguez,RobertStojnic,SergeyEdunov,andThomas
sey,RutaDesai,AlexanderWilliamClegg,Michal
Scialom.2023a. Llama2: Openfoundationandfine-
Hlavac,SoYeonMin,VladimírVondruš,Theophile
tunedchatmodels.
Gervet,Vincent-PierreBerges,JohnM.Turner,Olek-
sandrMaksymets,ZsoltKira,MrinalKalakrishnan,
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
JitendraMalik,DevendraSinghChaplot,UnnatJain,
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
DhruvBatra,AksharaRai,andRoozbehMottaghi.
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
2023. Habitat3.0: Aco-habitatforhumans,avatars
Bhosale,DanBikel,LukasBlecher,CristianCanton
androbots.
Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
AlecRadford,JongWookKim,TaoXu,GregBrock-
CynthiaGao,VedanujGoswami,NamanGoyal,An-
man,ChristineMcLeavey,andIlyaSutskever.2022.
thonyHartshorn,SagharHosseini,RuiHou,Hakan
Robustspeechrecognitionvialarge-scaleweaksu-
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
pervision.
IsabelKloumann,ArtemKorenev,PunitSinghKoura,
Kanchana Ranasinghe, Muzammal Naseer, Munawar Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
Hayat,SalmanKhan,andFahadShahbazKhan.2021. anaLiskovich,YinghaiLu,YuningMao,XavierMar-
Orthogonalprojectionloss. tinet,TodorMihaylov,PushkarMishra,IgorMoly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
Paul K. Rubenstein, Chulayuth Asawaroengchai, stein,RashiRungta,KalyanSaladi,AlanSchelten,
Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Ruan Silva, Eric Michael Smith, Ranjan Subrama-
Félix de Chaumont Quitry, Peter Chen, Dalia El nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
Badawy, Wei Han, Eugene Kharitonov, Hannah lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Muckenhirn,DirkPadfield,JamesQin,DannyRozen- ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
berg,TaraSainath,JohanSchalkwyk,MattSharifi, Melanie Kambadur, Sharan Narang, Aurelien Ro-
MichelleTadmorRamanovich,MarcoTagliasacchi, driguez,RobertStojnic,SergeyEdunov,andThomas
AlexandruTudor,MihajloVelimirovic´,DamienVin- Scialom. 2023b. Llama 2: Open foundation and
cent,JiahuiYu,YongqiangWang,VickyZayats,Neil fine-tunedchatmodels.
Zeghidour,YuZhang,ZhishuaiZhang,LukasZilka,
andChristianFrank.2023. Audiopalm: Alargelan- QingyueWang,LiangDing,YananCao,ZhiliangTian,
guagemodelthatcanspeakandlisten. Shi Wang, Dacheng Tao, and Li Guo. 2023. Re-cursivelysummarizingenableslong-termdialogue DeyaoZhu, JunChen, XiaoqianShen, XiangLi, and
memoryinlargelanguagemodels. MohamedElhoseiny.2023. Minigpt-4: Enhancing
vision-languageunderstandingwithadvancedlarge
JasonWei,MaartenBosma,VincentZhao,KelvinGuu, languagemodels.
Adams Wei Yu, Brian Lester, Nan Du, Andrew M.
Dai,andQuocVLe.2022. Finetunedlanguagemod-
elsarezero-shotlearners. InInternationalConfer- A Acknowledgements
enceonLearningRepresentations.
This material is based upon work supported by
Sixing Wu, Ying Li, Minghui Wang, Dawei Zhang,
theNationalScienceFoundationunderGrantNo.
Yang Zhou, and Zhonghai Wu. 2021. More is bet-
2238605
ter: Enhancingopen-domaindialoguegenerationvia
multi-sourceheterogeneousknowledge. InProceed-
ingsofthe2021ConferenceonEmpiricalMethods B AdditionalAViD-SPResults
inNaturalLanguageProcessing,pages2286–2300,
OnlineandPuntaCana,DominicanRepublic.Asso-
WereporttheGraphEditDistance(GED)results
ciationforComputationalLinguistics.
forAViD-SP,andthetestedbaselines,here.
DanfeiXu,YukeZhu,ChristopherB.Choy,andLiFei-
Fei.2017. Scenegraphgenerationbyiterativemes-
C QualitativeAViD-SPExamples
sagepassing.
Fanghua Ye, Jarana Manotumruksa, Qiang Zhang, WeincludeanexampleofatypicalAViD-SPgener-
Shenghui Li, and Emine Yilmaz. 2021. Slot self- ationinFigure3,withmetricscoresapproximately
attentivedialoguestatetracking.
attheaverageobtainedacrossthefulltestingset. In
thisexampleitisevidentthatallofthegroundtruth
Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,
Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze reference information was successfully added to
Shi,ZihanLi,YouxuanJiang,MichihiroYasunaga, theupdatedscenegraph,leadingtotheSoft-RED
SungrokShim,TaoChen,AlexanderFabbri,Zifan score of 0.0. However, considerable extraneous
Li, LuyaoChen, YuwenZhang, ShreyaDixit, Vin-
information is also observed to have been added.
centZhang,CaimingXiong,RichardSocher,Walter
Lasecki, and Dragomir Radev. 2019. CoSQL: A InFigure3threeadditionalNodesareadded,with
conversationaltext-to-SQLchallengetowardscross- twoofthembeingduplicatesofonesthatalready
domainnaturallanguageinterfacestodatabases. In existinthescenegraph,alongwithoneEdge.
Proceedings of the 2019 Conference on Empirical
However,consideringtheTranscriptionandVi-
Methods in Natural Language Processing and the
9thInternationalJointConferenceonNaturalLan- sual Scene for the illustrated sample reveals that
guageProcessing(EMNLP-IJCNLP),pages1962– thesefeatures,whilenotincludedinthereference,
1979,HongKong,China.AssociationforComputa-
likelyarelogicallyreasonablefortheagenttoin-
tionalLinguistics.
clude. For the additional Node of “runway” the
AlirezaZareian,SveborKaraman,andShih-FuChang. motivationisobvious. Notonlyistherunwayand
2020. Weaklysupervisedvisualsemanticparsing. itscorrespondingedgerelationshipmentionedby
theLLM,butarunwayisevenpresentinthescene
HangZhang,XinLi,andLidongBing.2023a. Video-
visual. Similarconditionsapplytothetwodupli-
llama: An instruction-tuned audio-visual language
catenodesadded. Whilethosenodesalreadyexist,
modelforvideounderstanding.
they are mentioned in the Audio Transcription at
Ji Zhang, Yannis Kalantidis, Marcus Rohrbach, two distinct times. Inspection of the highlighted
ManoharPaluri,AhmedElgammal,andMohamed
andblown-uppartsoftheimagealsorevealsthat
Elhoseiny.2019. Large-scalevisualrelationshipun-
thereareinfactduplicatesoftheseentitiesinthe
derstanding.
scene,makingtheiradditiontotheupdatedcontext
YongHongZhang,YingweiPan,TingYao,RuiHuang, reasonable.
TaoMei,andChangWenChen.2023b. Learningto
Thisisnottosayallextraneousadditionsshould
generatelanguage-supervisedandopen-vocabulary
scenegraphusingpre-trainedvisual-semanticspace. betreatedascorrectsincemanyshouldnot. How-
2023IEEE/CVFConferenceonComputerVisionand ever, it does illustrate a key area to seek further
PatternRecognition(CVPR),pages2915–2924. improvement in the VG-SPICE dataset and why,
for this work, we focus more on the “soft” capa-
WanjunZhong,LianghongGuo,QiqiGao,HeYe,and
bility to add all known good information tot he
YanlinWang.2023. Memorybank: Enhancinglarge
languagemodelswithlong-termmemory. graph.ModelType H-GED↓ S-GED↓
0dB 2dB 20dB Gold* 0dB 2dB 20dB Gold*
AViD-SP+GMADS Base 2.010 1.921 1.811 1.621 0.924 0.889 0.862 0.778
w/oImage 2.044 1.973 1.816 1.642 0.944 0.923 0.878 0.791
w/oAudio 2.168 2.101 2.071 1.863 1.209 1.186 1.158 1.004
wIncorrectImage** - - 1.806 - - - 0.861 -
w/oPriorContext*** - - 4.656 - - - 0.909 -
AViD-SP+Meanpool Base 1.739 1.617 1.514 1.295 0.935 0.889 0.859 0.759
w/oImage 1.732 1.599 1.514 1.285 0.939 0.910 0.872 0.759
w/oAudio 1.622 1.560 1.428 1.244 1.002 0.964 0.909 0.815
wIncorrectImage** - - 1.517 - - - 0.857 -
w/oPriorContext*** - - 4.778 - - - 0.905 -
Table5: GEDresultsontheVG-SPICEtestsetforourAViD-SPmodel. AViD-SPwastrainedwithCHiME5noise
augmentationsampledbetween0dband20dBSNR(allCHiME5noisefollowedtheprovidedtrain/eval/testsplits).
*GiventhegroundtruthutterancetranscriptsinplaceoftheASRtranscriptions. **Evaluatedbyoffsettingvisual
featureswithinbatchsoincorrectimagefeaturesarepairedwiththeotherinputcomponents. ***Evaluatedwith
"EmptyContext"priorstatescenegraphssummariesinsteadofthecorrectones.
Figure 3: Sample generation output with corresponding inputs from AViD-SP. Scored a Soft-RED of 0.0 and
Hard-REDof6.727. Significantfeatureshighlightedincolors. Qualitativeevaluationrevealsthatthemajorityof
extraneousadditionswereeithersupportedbytheAudioTranscription,thesceneimage,orboth.D VisualGenomePreprocessing TermFrequencyAnalysis: Next,wemanually
curatedtermsinthefiltereddatasettoestablisha
TheVisualGenomeservesasastrongbasisforVG-
relevantsetfortheSPICEtask, excludingsingle-
SPICEbuthasqualityissuessuchasinconsistent
occurrencetermsfortheirlowquality,andfiltered
namingforNodes,Attributes,andPredicates,du-
scenegraphsbasedonthislist.
plicateNodes,andunnecessaryNodes(e.g.,<man,
Scene Graph Size Restriction: Finally, we fil-
has,head>). PriorsolutionsforSceneGraphGen-
tered out small graphs to ensure a diverse set
eration (SGG) tasks (Liang et al., 2019; Zhang
forVG-SPICE,excludinggraphswithfewerthan
et al., 2019; Xu et al., 2017; Maëlic et al., 2023)
fourNodesorEdgesandapplyingdynamicallyin-
curated versions by limiting predicates and node
creasedthresholdforgraphswithduplicatenodes.
names, reducing predicates from 27k to 50 and
ThesemethodsenhancedtheVisualGenome’s
node names from 53k to 150. While the Visual
graphs, yielding a dataset with improved quality
Genome contains a substantial portion of single-
andannotationdensity,asillustratedinTable1.
sample terms, typically of lower quality, such re-
strictionscanoversimplifyandyieldsmaller,less
E TrainingandInference
representativescenegraphs.
Hyperparameters
OurapproachrefinestheVisualGenomeby:
The training regimen for AViD-SP spans two
Standardization and Correction: We applied epochsacrossthedataset,usingacombinedbatch
rule-basedsystemswithSentenceTransformerPart size of 72 on six Nvidia L40 GPUs. An initial
ofSpeechtaggers5 tofixinconsistenciesandim- learning rate of 5×10−5 is applied, followed by
provescenegraphdensitybyretainingrareNode exponentialdecay. Weemploycross-entropyloss
names(e.g.,"redtable",identifying"red"asanat- forthepredictionoftargetsemanticparses,intro-
tribute). We removed low-quality attributes and ducinglossmaskingforpaddingandfortheprompt
predicates by limiting them to specific parts of thatcombinespriorcontextwithmultimodalinputs.
speech conditions, such as removing proper and We utilize loss factors of α = 1.0, β = 0.1, and
commonnounsfromattributes/edges. Furthermore, γ = 0.1.
weimposedseveralstraightforwardconstraintsto Inferenceleveragesagreedydecodingstrategy
refinethescenegraphstructure. Theseincludedset- withamaxgenerationlengthof160tokensandoth-
tinglimitsonthewordcountsforindividualscene erwisedefaultgenerationparametersforLLAMA
graphelementsandconsolidatingattributeswhen 27B.
redundancywasdetectedwithinaspecificnode,for
F ContextualStateRepresentation
instance,merging"reddish"and"red"whenboth
attributesdescribedthesameentity.
SPICEformulatesthepriorcontexttobeutilized
bytheagentasastructuredknowledgegraph. How-
DuplicateNodeElimination: Weaddedapost-
ever,top-performingsemanticparsinggeneration
standardization phase to remove duplicate nodes.
models,suchasthosebestontheLlamaarchitec-
Unlikeearliermethods(Maëlicetal.,2023)rely-
tureasusedinthiswork,aredecoder-onlymodels
ingsolelyonahighIntersectionoverUnion(IoU)
that can accept inputs from linear text sequences
threshold for exact node matches, we included
only. This requires utilizing either a compatible
a semantic similarity check from the contextual-
knowledge graph encoder which can embed and
ized embeddings from the same Sentence Trans-
projecttheknowledgegraphrepresentationforuse
formerutilizedintheStandardizationandCorrec-
by the semantic parse generation model, or rep-
tionphase. Thisallowsforthedetectionofdupli-
resenting the knowledge graph in the form of a
cateNodeswithsignificantnamesimilaritiesand
textually formatted prompt. For AViD-SP devel-
IoUs. With a preference for visually supported
opedinthiswork,weutilizedthesecond,withthe
scenegraphsoverthepotentialexclusionofsome
formatofthetextuallypromptedrepresentationof
valid Nodes, we set a lower IoU threshold (0.5,
thepriorcontextshowninFigure1.
comparedtopriorworks’0.9)andasemanticsimi-
Whengeneratingthecontextrepresentationsall
laritythresholdof0.7.
existingNodesareassignedNodeIDs,andseman-
tic parses are expected to operate in reference to
5Using"all-mpnet-base-v2"fromPythonSentenceTrans-
formers these Node IDs (Section G). We provide Nodesand Attributes first, followed by any Edges. The
ordering of all information is sorted by Node ID
in ascending order. In practice, all Node IDs are
randomly assigned for each training iteration to
diversitytraininginputs.
G FormalLanguageDefinition
The formal language we used in the semantic
parses P and the corresponding execution func-
i
tion e contained the following executable func-
tion, which together could deterministically up-
date the scene graph prior context C to the next
i
context state C . Since VG-SPICE only rep-
i+1
resents the conversational construction of scene
graphs,andnotdeletionoralterations,ourformal
languageiscomprisedofthreedistinctoperations:
1)#ADD_NODEacceptinganewNodeID,name,
andoptionallyasetofattributestoaddalongwith
it,2)#ADD_ATTRacceptinganexistingNodeID
aswellasasetofattributestobeaddedtothespec-
ifiednode,and3)#ADD_EDGEacceptingasource
andtargetpairofexistingnodeIDsalongwiththe
predicate to be assigned between them. Our for-
mallanguagealwaysgeneratesreferencesemantic
parses with new attributes added first, followed
bynewNodes(andassignedattributes),andlastly
newedges. However,whenevaluatingourmodel
outputstheexecutionfunctionecanacceptthese
commandsinanyorder,solongasthereferenced
nodeIDsalreadyhavebeenadded.
H Licensing
Our paper utilized the Visual Genome dataset
whichislistedunderaCreativeCommonslicense.
All other tools utilized are available from either
Pythons Spacy or Huggingface and are available
foracademicuse. Tothebestofourknowledge,all
artifactsutilizedarealignedwiththeirintendeduse
cases.
I AIAssistance
A minor portion of code development was done
withtheassistanceofChatGPT.Allresearchideas
and writing are of the author’s original creation.
Grammarlywasutilizedforwritingassistance.