Data Augmentation for Multivariate Time Series
Classification: An Experimental Study
Romain Ilbert∗1,2 Thai V. Hoang3 Zonghua Zhang4
1Huawei Noah’s Ark Lab, Paris, France 2LIPADE, Paris Descartes University, Paris, France
3 TH Consulting, Paris, France 4 CRSC R&D Institute Group Co. Ltd, Beijing, China
Abstract—Our study investigates the impact of data augmen- imbalance is critical, especially in multivariate time series
tation on the performance of multivariate time series models, datasets where the complexity and variability of data ex-
focusing on datasets from the UCR archive. Despite the lim-
acerbate the problem. Our study concentrates on the UCR
ited size of these datasets, we achieved classification accuracy
archive,whichhasrecentlybeenenrichedwithabroadarrayof
improvementsin10outof13datasetsusingtheROCKETandIn-
ceptionTimemodels.Thishighlightstheessentialroleofsufficient multivariatetimeseriesdatasets,offeringanidealenvironment
data in training effective models, paralleling the advancements forinvestigatingtheeffectivenessofdataaugmentationwithin
seen in computer vision. Our work delves into adapting and this domain [11, 12].
applying existing methods in innovative ways to the domain
Our work incorporates both traditional and deep learning
of multivariate time series classification. Our comprehensive
approaches, namely, the ROCKET and InceptionTime
explorationofthesetechniquessetsanewstandardforaddressing
data scarcity in time series analysis, emphasizing that diverse models. These models represent the state-of-the-art in time
augmentation strategies are crucial for unlocking the potential series classification, offering a unique blend of speed, ac-
of both traditional and deep learning models. Moreover, by curacy, and adaptability across a wide range of time series
meticulously analyzing and applying a variety of augmentation
data [13, 14, 15, 16]. The inclusion of these models allows us
techniques, we demonstrate that strategic data enrichment can
to comprehensively evaluate the impact of data augmentation
enhance model accuracy. This not only establishes a benchmark
for future research in time series analysis but also underscores on both traditional and deep learning approaches, ensuring
the importance of adopting varied augmentation approaches our findings are broadly applicable and relevant to current
to improve model performance in the face of limited data classification challenges [17, 18].
availability.
Central to our investigation is a detailed exploration of data
Index Terms—multivariate time series, time series classifica-
augmentation techniques tailored specifically for time series
tion, data augmentation, data scarcity
data. Among these, the Synthetic Minority Over-sampling
I. INTRODUCTION Technique (SMOTE), and a noise injection, stand out for
The progression of machine learning, especially in time se- their ability to generate synthetic data that closely mimics
riesclassification,hasbeenmarkedlyacceleratedbytheadvent the original datasets, thus addressing the dual challenges of
ofdeeplearningtechniques.Thesemodels,however,exhibitan data scarcity and class imbalance [19]. While these methods
inherent dependency on large and diverse training datasets to can be applied to both univariate and multivariate time series,
achieve optimal performance, a requirement often challenging we also explore the potential of Time Generative Adversarial
to meet in practice [1]. The scarcity and imbalance of classes Networks (TimeGANs) for their ability to capture complex
in these datasets, poses a critical bottleneck, affecting not inter-variabledependencies[20].Thismakesthemapromising
only model accuracy but also their ability to generalize across candidateformultivariatetimeseriesanalysis,andweevaluate
diverse scenarios [2, 3]. them in our work. Our methodology encompasses a diverse
In the fields of computer vision and natural language range of augmentation strategies, each carefully selected to
processing(NLP),dataaugmentationhasemergedasafunda- enhancetherepresentativenessandqualityofthetrainingdata,
mental technique, effectively addressing the limitations posed therebyenablingmodelstoachievesuperiorgeneralizationand
by insufficient data [4, 5]. By artificially enhancing dataset performance [6].
size and diversity, data augmentation techniques have proven In this study, we rigorously explored a wide range of
to significantly mitigate overfitting, thereby improving model data augmentation techniques, meticulously selected from the
robustness and performance [3]. This success has sparked diverse branches of our newly developed taxonomy. By com-
interest in applying similar strategies within the domain of bining this approach with an in-depth analysis of two leading
timeseriesclassification,wherethechallengesofdatascarcity time series classification methodologies (i.e., ROCKET and
and class imbalance are equally prevalent [6, 7, 8, 9]. InceptionTime ), not only do we demonstrate improve-
Class imbalance, in particular, is a pervasive issue that ments in model accuracy, but also shed light on the complex
skews the learning process, often resulting in models that interplaybetweendatacharacteristics,augmentationstrategies,
are biased towards the majority class [10]. Addressing this and model performance. Our results indicate that accuracy
enhancements are not the result of any single augmentation
Correspondingauthor:romain.ilbert@hotmail.fr. technique, but rather emerge from a combination of methods,
4202
nuJ
01
]GL.sc[
1v81560.6042:viXrahighlighting the lack of a one-size-fits-all solution in applying structure-preserving,whichfocusesonmaintainingthespatial
specific augmentation strategies. structure and inter-point dependencies within the data.
The results of our comprehensive investigation advocate For an overarching view of the taxonomy and to discern
for an informed use of data augmentation in time series the differences between the approaches under the various
classification. This work contributes to the academic and branches, the reader is directed to Figures 2-6.
practicaldiscourseonovercomingchallengeslikedatascarcity In Figure 2, we demonstrate the technique of noise injec-
and class imbalance, and also paves the way for future tion [26, 27, 28, 7], a foundational data augmentation method
advancements in this area. thatmodifiesdatapointsinthetimedomain.Anotherrepresen-
Our contributions can be summarized as follows. tative of basic augmentation methods, depicted in Figure 3, is
• Weexpandtheunderstandingoftimeseriesclassification the SMOTE algorithm [19], which fabricates new instances
by evaluating both InceptionTime and the state-of- by creating convex combinations of pre-existing examples
the-art ROCKET models, highlighting the importance of within the dataset. Turning to generative strategies, Figure 4
considering both deep learning and non-deep learning portrays the TimeGANs technique [20], a generative neural-
approaches. network-based method. The overarching aim of generative
• We conduct an exhaustive review of data augmentation techniques is to construct a model that approximates the
techniques and introducing a new taxonomy (Figure 1) minorityclassdistribution,whichcansubsequentlybeusedto
that categorizes these methods into distinct branches, producenoveltimeseriesdata.Theprimarydistinctionamong
enriching the framework for their application and eval- these generative approaches is their respective methodologies
uation. for approximating the distribution of the minority class. An
• We utilize a variety of augmentation techniques from exemplification of a label-preserving approach is depicted in
different branches of our taxonomy, including those ne- Figure5,showcasingarangemethodwheretheparamountob-
cessitating external training like TimeGANs, marking a jectiveistoensurethatthenewlygenerateddatapointsdonot
first in the context of time series data augmentation. transgressthedecisionboundary.Thisisapotentialissuewith
• We demonstrate accuracy improvements through empiri- elementary noise-injection techniques, such as the one shown
calevaluationonthe13multivariate,imbalanceddatasets in Figure 2, where a straightforward application of noise can
from the UCR/UEA archive. inadvertently shift data points across the decision boundary.
• Our detailed analysis reveals that a broad spectrum of Range techniques meticulously modulate the extent of noise
augmentation techniques can enhance model accuracy, application to guarantee adherence to the decision boundary.
underscoring the variability in their effectiveness across Lastly,Figure6illustratesanexampleofastructure-preserving
datasets and suggesting the potential for optimization technique, notably OHIT [75]. This approach creates clusters
through strategic combination. and computes their covariance matrices, which are then used
Ourstudyestablishesafoundationforfutureresearchaimed to generate new examples that are likely to fall within the
at refining the application of data augmentation in time series boundaries of these clusters.
classification. Inspired by successful strategies in computer Ourtaxonomysetsitselfapartfromothertaxonomies[7,6]
vision, we believe that exploring the synergistic use of var- by incorporating the preserving class of techniques, which try
ied augmentation techniques can lead to further performance to address the following challenges. First, when performing
improvements. data augmentation by adding noise, how can we determine
the optimal amount of noise to augment a series intelligently?
II. ATAXONOMYOFTIMESERIESAUGMENTATION
Second, if our original time series are interdependent (e.g.,
TECHNIQUES
correlated), how can we generate new series that retain these
Figure 1 presents a comprehensive taxonomy of data aug- dependencies?Additionally,weintroduceinthetaxonomythe
mentation techniques, which we discuss in Section III. probabilisticmodels(underthegenerativeclassoftechniques),
The initial category encompasses basic techniques, which which describe time series as transformations of underlying
include methods such as slicing, cropping, or noise injection, Markovprocessesthatareeasiertomodel.Finally,weinclude
applicable within both the time and frequency domains (see anewneural-networkmodelthatrequireexternaltraining,the
Figure 2). This group also includes techniques based on TimeGANs.
oversampling and decomposition.
Subsequently, the generative class of techniques is com-
III. OVERVIEWOFTIMESERIESAUGMENTATION
prised of methods subdivided into statistical, neural network,
TECHNIQUES
and probabilistic approaches. These strategies aim to emulate A Multivariate Time Series (MTS), denoted as x =
the authentic probability distribution of the time series data to (x ,...,x ,...,x ), is composed of T sequentially ordered
1 t T
generate new instances. elements,whereeachelementx residesinanM-dimensional
t
The final class, preserving, maintains the original classes space, i.e., x ∈ RM. We will use the term data point to
t
found within the dataset. It is further segmented into two refer to an individual observation within the series, which has
sub-categories: label-preserving, which fine-tunes common M dimensions, or to the entire series, which encompasses
techniques such as noise injection to preserve accuracy, and T dimensions, contingent on the specific analytical context.TimeSeriesDataAugmentationTechniques
BasicTechniques GenerativeTechniques PreservingTechniques
Time Frequency Oversampling Decomposition Statistical Neural Probabilistic Label Structure
Domain Domain Techniques Techniques Models Networks Models Preserving Preserving
Slicing Fourier Interpolation STL Posterior Autoencoders Autoregressive Range SPO
[21,7,6] Transform [19,34,35,36] [7,39] Sampling [7,49,50,51] Models Techniques [72]
[7,30] [7,45] [52,53,54,55] [63,64] [69,70,71]
Permutation Density EMD INOS
[7,6] Frequency [37,38] [7,40,41] Gaussian GANs Diffusion [73]
Warping Trees [7,56,20,57,58] Models
[7,31] [7,46] [59,60,61,62] [65]
Warping RobusTAD MDO
[21,22,7,6,23] [7,42] [74]
Frequency LGT Normalizing
Masking [7,47] Flows
Masking [7,25] ICA [66,67,68] OHIT
[24,25] [43,44] [75]
GRATIS
Mixing [7,48]
InjectingNoise [7,32,33]
[26,27,28,7,6]
Rotation
[7,6,29]
Scaling
[7,6]
Fig. 1: Comprehensive taxonomy of data augmentation techniques for time series analysis, integrating a wide array of
methodologies from basic transformations to advanced generative models, including a branch on Preserving Techniques.
It’sworthnotingthattheapproachesandtechniquesdiscussed for anomaly detection [42], RobustSTL for seasonality han-
here, while focused on multivariate time series, could poten- dling [39], EMD for sensor data noise reduction [40, 41], and
tially be adapted and applied to univariate time series as well. ICA with D-FANN for series gap filling [43, 44].
Combining techniques like permutation, rotation, time warp-
A. Basic Techniques ing [70], and SpecAugment’s spectrogram operations—time
warping,frequency,andtimemasking—canoptimizeaugmen-
1) Time Domain: Time domain augmentation involves
tation [25].
modifyingtimeormagnitude.Techniquesincludenoiseinjec-
tionforregularization[26,27,28,7,6],scalingformagnitude B. Generative Techniques Overview
adjustment[7,6],rotationaffectingtemporaldependencies[7,
Time series can be sampled directly from a posterior dis-
6, 29], slicing for segment extraction [21, 7, 6], permutation
tribution, as depicted in Figure 4. This section delves into
of series intervals [7, 6], and regularization methods like
two main types of generative models: statistical and neural
masking, cropping, dropout, and pooling [24, 25]. Window
network-based models.
Warping and guided warping use temporal distortions and 1) Statistical Generative Models: Recent years have seen
Dynamic Time Warping for novel series generation [21, 22, significant interest and advancements in generative models.
7, 6, 23, 76, 77, 78]. Tanner and Wong [45] suggested approximating the true
2) Frequency Domain: Frequency domain augmentation posterior distribution for generating new variables. Research
appliesamplitudeandphaseperturbations[42],withSTFTfor by [46] leverages the strong correlation between close time
spectrograms [30]. EMDA perturbs frequency characteristics points. Bellman [79] utilized sparse graphical models to cap-
for Acoustic Event Detection [32]. VTLP and SFM distort ture statistical dependencies over time. Smyl and Kuber [47]
speech spectra, or convert speech data [31, 33]. showcasedtheeffectivenessofLocalandGlobalTrend(LGT)
3) OversamplingTechniques: Oversamplingtreatstimese- data augmentation, particularly when combined with LSTMs.
ries as spatial points for augmentation. Interpolation mixes a GRATIS [48] investigated time series characteristics and time
series with its nearest neighbor [19]. SMOTE and its vari- dependencytoefficientlyproducenewseries.Vinodetal.[80]
ants—ANSMOT and SMOTEFUNA—along with ADASYN implemented a maximum entropy bootstrap method for gen-
and SWIM, address minority class enhancement through erating instances closely related to the originals. Moreover,
density-basedsyntheticsamplegeneration[34,35,37,38,36]. [81] proved that combining data augmentation with neural
4) Decomposition-Based Techniques: Time series can be architecture exploration yields promising outcomes.
decomposed into trend, seasonality, and residual components 2) NeuralNetworksBasedGenerativeModels: Thissection
for targeted augmentation. Techniques include RobustTAD reviews neural network architectures for augmenting timeFig.2:BasicTechniques,likenoisein- Fig. 3: Oversampling Techniques, like Fig. 4: Generative Techniques, like
jection SMOTE timeGANs
Fig. 5: Label-Preserving Techniques, like Fig.6:Structure-PreservingTechniques
rangetechniques likeOHIT
seriesdata.Auto-encoders(AE)leveragealatentspaceforeffi- flow [68]. Diffusion Models, through a Markov chain of dif-
cienttransformationslikeinterpolation[50,51],outperforming fusion steps, gradually introduce then remove noise, learning
direct raw input use [49]. MODALS [52] automates augmen- to recreate the original data from the noise, focusing on the
tation, while LSTM auto-encoders (LSTM-AE) [53] enhance conditional backward probability.
spatial-temporal data. Variational auto-encoders (VAE) and
conditional VAEs, as shown by Kirchbuchner et al. [55], T−1
(cid:89)
effectively reduce target data variance. Combining LSTM- P θ(x)=P(x T) P θ(x t−1|x t) (2)
based VAE samples with interpolation [50] augments time t=1
series, with Qingsong Wen et al. [6] evaluating DeepAR where P (x |x )∼N(µ (x ,t),Σ (x ,t)).
θ t−1 t θ t θ t
and transformer-based techniques. DTW-based SMOTE with
Siamese Encoders (DTWSSE) [54] and generative adversar- C. Structure- and Label-Preserving Techniques
ial networks (GANs) [56] also contribute to augmentation,
In the case of sensor signals, collecting a large amount
including MLP, RNN [61, 82], 1D CNN [60, 62], and 2D
of data samples under various operating conditions, or from
CNN [57] variants. The WGAN discriminator replaces the
different environments, is a complex task. Data augmentation
VAE decoder for enhanced performance in [83], with se-
isasolutiontoaddressthisproblem:differenttransformations
lective WGAN (sWGAN) and VAE (sVAE) outperforming
are applied on the data, in order to create new data points.
conditional WGANs (cWGAN) [59]. DOPING [84] utilizes
However, the labels of these new points may often times
adversarial autoencoders (AAE) [85] for oversampling, while
be sensitive to even small fluctuations of the points’ values.
TimeGANs [20] aim to preserve temporal series dynamics.
Evidently, we do not want to produce new data points that,
3) Probabilistic Models: Generative models also augment
eventhoughintheneighborhoodofanexistingclass,lieonthe
timeseriesdata.Wavenet[63],adeepprobabilisticautoregres-
other side of the class decision boundary (refer to Figure 5).
sive NN, generates raw audio by factorizing the probability
Moreover, sensor data make it hard for a human analyst to
distribution as:
recognizedifferencesinthelabelsbetweenrawandaugmented
(cid:89)T signals (unlike image classification for example, where visual
P(x)= P(x |x ,...,x ) (1)
t 1 t−1 inspection is an effective solution). To resolve this issue, we
t=1 needtomakesurethatthegenerateddatahavetherightlabel,
GluonTS [86] offers transformer and Wavenet implementa- as well as follow the same data characteristics (as the rest of
tions. DeepAR [64] trains an autoregressive RNN for prob- the points in the same neighborhood in the data space).
abilistic forecasting. Normalizing flows [66], introduced by 1) Label-preserving: Augmentation techniques must pre-
Brubaker et al. [67], map simple distributions to complex serve labels to avoid misclassification, such as false positives
onesviainvertible,differentiablemappings.TheyusedaVAE from noise in Parkinson’s disease analysis, where noise could
to initialize a base distribution for training a normalizing mimic dyskinesia symptoms, degrading performance [70].TABLE I: Task accomplished according to the algorithm used
Croppingriskslosingcriticalinformationlikeshapelets,detri-
as baseline model for classification task
mental in small datasets [87, 70]. Classification can be misled
byscalingindatasetswhereintensitydistinguisheslabels[70].
Algorithm Feature-Extractor Classifier
It’s crucial to understand class-specific regions to determine
ROCKET x
safe perturbation amplitudes (see Figure 5), enhancing test
InceptionTime x x
accuracy by 5% without model adjustments [71].
2) Structure-preserving: ResearchhasexploredSNN-based
TABLE II: Methodology based on the baseline classification
density clustering for high-dimensional data, addressing
algorithm employed. Since ROCKET functions primarily as a
MDO’s shortcomings in estimating the true covariance ma-
feature extractor, it is employed in conjunction with a Ridge
trix [88, 74]. OHIT addresses high-dimensional, imbalanced
Regressor (RR) for the classification task.
time-series classification by using similarity-based clustering
to reflect minority class modality, generating new samples to
Algorithm DL-based Ensemble-based Kernel-based
preserve mode covariance structures (Figure 6) [75]. INOS ROCKET+RR x
introduces structure-preserving oversampling for imbalanced InceptionTime x x
time series by first generating samples via interpolation, then
creating additional synthetic samples based on a regularized
minority class covariance matrix, enhancing SPO [73, 72]. the InceptionTime and ROCKET algorithms to cover two
typesofalgorithmicfamilies.Itisimportanttonotethatthese
IV. EXPERIMENTALEVALUATION
algorithms work in different ways. Some, like ROCKET , only
In this research, Python 3.7.3 served as the primary pro- play the role of feature extractor and must be coupled with
gramminglanguage.TheimplementationofSMOTEreliedon a pure classifier, as ridge regression (RR) I. This choice of
the imbalanced-learn library (version 0.8.0). Modifications to RR as the classifier to complement rocket is motivated by
TimeGANswerecarriedoutusingtheydata-syntheticpackage its robustness to high-dimensional data and its regularization
(version 0.7.1) alongside Tensorflow (version 2.4.4). Noise capabilities. On the other hand, other algorithms, such as
injection was facilitated through numpy (version 1.19.2). InceptionTime , play both roles directly. Moreover, they
Classification tasks utilized sktime (version 0.13.0), sklearn are based on different techniques, as showed in Table II.
(version 1.0.1), fastai (version 2.7.7), and tsai (version 0.3.1),
B. Datasets and Experimental Settings
all of which underwent slight modifications for this study.
ComputationswereperformedontheJeanZaysupercomputer, We evaluate the performance of baseline models and data
equippedwithNVIDIAV100GPUsboasting16GBofRAM. augmentation techniques on the UCR/UEA archive [11, 12,
Wemakeallcodeusedinthispaperavailableonline:https: 15], and use the 13 imbalanced multivariate datasets.
//helios2.mi.parisdescartes.fr/∼themisp/tsda/ . We use 5 different data augmentation techniques: a tra-
ditional noise injection with 3 different levels of noise l ∈
A. Baseline Algorithms
{1,3,5}, the SMOTE algorithm [19] and the TimeGANs [20]
Intimeseriesclassification,datasetimbalancesbetweenmi- generative algorithm. Injecting noise is known to be a reliable
nority(positive)andmajority(negative)samplesarecommon, and fast augmentation technique, especially in computer vi-
necessitating dataset augmentation to improve minority repre- sion.Itsusewith3differentlevelsalsoallowsittobeusedas
sentation. This field focuses on categorizing data sequences arangemethodfromthepreservingtechniquebranch.SMOTE
by temporal patterns, crucial for binary classification (normal is a good representative of the interpolation-based techniques
vs. abnormal sequences) and multi-category scenarios. family, and TimeGANs are, to the best of our knowledge,
Advancements in model performance are notable. A the only generative model to take into account the temporal
study [15] highlights top time series classification techniques aspect of time series. Note that among these three techniques,
using intervals, shapelets, or word dictionaries, while another onlytheTimeGANsaretimeseries-basedandrequireexternal
review [89] examines deep learning approaches in this area. training. These techniques were used to augment the original
It appears from the above two studies that the best classi- or downsampled training set. The classification task is then
fication models are COTE [90] for non deep learning mod- performed by ROCKET or InceptionTime .
els, and models with residual connections for deep learning Table II illustrates the diverse methodologies employed
ones [91]. The COTE algorithm was later improved in HIVE- by the baseline algorithms in addressing the classification
COTE[92,93]andHIVE-COTE2.0(HC2)[18],whileResnet challenge. Among these, algorithms utilizing deep learning
became a basis for InceptionTime [14]. [17] proposed a (DL) principles, particularly those based on residual neural
novel time series classification algorithm, TS-CHIEF, which networks, have found extensive application in tasks such as
rivals HIVE-COTE in accuracy but requires only a fraction of image recognition and classification [94, 95]. The ROCKET
the runtime. Then, a new family appeared: ROCKET [13], algorithm, notable for its innovative use of a vast number
which has the advantage of being very fast, compared to of randomly generated weights, aims to maximize the in-
the HIVE-COTE algorithm. [16] gives an overview of some formational input to traditional classifiers, including logistic
recent algorithmic advances in the domain. We therefore use regression (LR) and ridge regression (RR).TABLE III: Information about the original multivariate imbalanced datasets
Dataset n classes Train size Dim Length Var train Var test Im ratio d train test prop miss
CharacterTrajectories 20 1422 3 182 0.15 0.15 13.06 3.35 0.33
EigenWorms 5 128 6 17984 0.18 0.18 3.26 386.95 0
Epilepsy 4 137 3 206 0.18 0.18 1.05 6.03 0
EthanolConcentration 4 261 3 1751 0.24 0.23 2 101616 0
FingerMovements 2 316 28 50 0.16 0.18 0 588.92 0
Handwriting 26 150 3 152 0.15 0.1 12.23 4.04 0
Heartbeat 2 204 61 405 0.09 0.09 0.3 23.15 0
LSST 14 2459 6 36 0.03 0.02 9.49 2259.42 0
PEMS-SF 7 267 963 144 0.17 0.18 3.07 30.79 0
PenDigits 10 7494 2 8 0.3 0.29 4.02 12.53 0
RacketSports 4 151 6 30 0.14 0.14 1.06 19.56 0
SelfRegulationSCP1 2 268 6 896 0.16 0.15 0 3352.33 0
SpokenArabicDigits 10 6599 13 93 0.14 0.13 0 38.48 0.57
To quantify the effectiveness of data augmentation, we • Class Imbalance (Im ratio): We used the imbalanced
introduce the concept of relative gain, G , defined as: degree (ID) proposed by [96] with Helliger distance, as
r
recommended.
acc(model aug)−acc(model)
G r = , (3) • Train/Test distance(d train test): the Euclidean distance
acc(model)
between the training set and the testing set. It is the
where acc represents the average accuracy over five runs, Euclidean distance between the mean vector of the train
model denotes the model trained on the original dataset, and the test vector, the variance being already taken
and model_aug signifies the same model trained on the into account in another definition. This distance allows
augmented dataset. capturing a possible shift domain between the training
Additionally, while some data characteristics are adopted set and the testing set.
from existing literature [7], we propose extensions and addi- • Missingvaluesproportion(prop miss):Numberofmiss-
tionstothesedefinitionstobetteraccommodatethenuancesof ing time steps divided by the total number of time steps
multivariate datasets. This expansion is critical for a compre- in the dataset.
hensive understanding of dataset attributes. For an exhaustive We did not consider the ”patterns per class” property,
enumeration of these properties and their values across the 13 because [7] showed that ”the correlation of the change in
multivariate imbalanced datasets, refer to Table III. accuracytotheaveragenumberofpatternsperclassissimilar
to training set size” nor the intra-class variance, proportional
• Numberofclasses(n classes):Thenumberoftheclasses
to the variance of the dataset.
present in the dataset.
• Training set size (Train size): The number of time series C. Augmentation Protocol and Parameters
in the original training set. We have studied 13 multivariate datasets from the
• Dimension (Dim): The number of features in the dataset. UCR/UEA archive, with different properties. The same di-
• Time series length (Length): The length of time series in vision into training and testing sets was made as in the
the original training set. UCR/UEA archive. Among the imbalanced datasets, each
• Dataset variance (Var train and Var test): To define a one was previously augmented with one of the following
multivariate variance for our dataset, we consider the techniques:timeGANs,SMOTE,noise 1,noise 3andnoise 5
following equations: where i in noise i refers to the standard deviation (std) mul-
tiplicator of noise, i.e. the level of injected noise l∈{1,3,5}.
N N
1 (cid:88) 1 (cid:88)
σ2 = (x − x )2, (4) Indeed, we add to the dimension j of the original time series
mt N imt N imt a noise as the following:
i=1 i=1
M T
σ2 = 1 (cid:88) (cid:88) σ2 (5) Noise∼N(0,l×std j) (6)
D T ×M mt
m=1t=1 where std refers to the std of the dimension number j of
j
where N is the number of time series in the dataset D, the original time series. The addition of noise in a certain
M is the number of dimensions in each time series, T dimension is therefore proportional to the original std of this
is the length of the time series, and x denotes the same dimension. For each class, we extract a time series ran-
imt
value at time step t of dimension m in time series i. domlyandaddnoiseuntilthedatasetisperfectlybalanced.For
Furthermore, σ2 represents the variance at time step t timeGANs, the number of iterations during training steps are
mt
for dimension m across the dataset, and σ2 encapsulates setto2500,2500and1000respectively.Thedimensionofthe
D
the overall variance of the dataset D, averaged across all latentspaceissetto10,gammaissetto1,thelearningrateto
dimensions. 5.10−4 andthebatchsizeto32.WeprovidetothetimeGANs,foreachtraining,timeseriescomingfromasingleclassofthe 2% improvement, all have a baseline accuracy over 80%, and
original dataset, so that the generated series follow the same out of the 6 datasets with a baseline accuracy of 89% or
distribution,untilthedatasetisperfectlybalanced.Concerning more, 5 out of 6 show improvements. This highlights that
SMOTE, the number of neighbors to be considered is defined data augmentation does not necessarily enhance the relative
as the minimum between 5 and the number of elements in accuracies of datasets with already low performance with
the class minus 1. The baseline models were applied on both ROCKET but also optimizes those with very high baseline
theaugmentedandnon-augmenteddatasets,i.e.6datasetsper accuracies. This observation underscores the complexity of
baseline model, and we compare the performance on each of time series classification tasks, and given the exceptionally
them trying to capture some correlations between G and the highaccuracyofthemajorityofdatasetswithastate-of-the-art
aforementioned properties. modellikeROCKET,substantialimprovementsarenotalways
expected. It is also crucial to highlight that the effectiveness
D. Classification Methodology and Setup
of augmentation techniques can vary across different datasets,
Our analysis employs two baseline models for evalua- suggesting that there is no one-size-fits-all solution for data
tion: InceptionTime and ROCKET coupled with a ridge augmentation in time series classification (see Table VI).
regression classifier. In the case of ROCKET , we adhere The InceptionTime architecture, inspired by the suc-
to the default configuration, utilizing 10,000 kernels. For cess of Inception modules in image recognition, demonstrates
InceptionTime , the dataset is partitioned into training significant efficacy in time series classification [14]. Incorpo-
and validation segments, maintaining a 2:1 ratio. Augmented rating multiple Inception modules allows InceptionTime
data are incorporated exclusively during the training phase, to adeptly capture complex features from time series data at
ensuring the validation set comprises solely original, stratified various scales.
samples. This approach aligns our evaluation with standard When examining the impact of data augmentation on
practices, facilitating direct comparison with other studies InceptionTime ’s performance, an average increase of
utilizing the complete UCR/UEA archive’s test set. 0.56% in accuracy is observed across 10 out of the 13
Consistencyinparametersettingsismaintainedacrossmod- multivariatedatasets,asshowninTableV.Thisimprovement,
els,irrespectiveofaugmentation,toensurecomparability.The though seemingly modest, underscores the potential of data
training process extends over 200 epochs, incorporating an augmentation to enhance the model’s generalization from
early stopping mechanism triggered after 30 epochs without training data. Notably, all 7 datasets with a baseline accu-
improvement, preserving the best model based on validation racy of 87% or higher experienced performance gains post-
accuracy.Priortotraining,acyclicallearningrateanalysis[97] augmentation, highlighting a consistent benefit in scenarios
is conducted for each dataset to identify the optimal learning whereInceptionTimealreadyperformswell—specifically,
rate, which is then adjusted to the identified valley point for above the 85% mark. This reveals an interesting pattern: data
subsequent training. augmentation tends to yield advantages especially when the
initial model performance is substantial. On the flip side,
E. Effect of Augmentation on Model Classification
datasets with a relatively low baseline saw a negative impact
In this section, we show that data augmentation can effec- from augmentation, suggesting that for deep learning models
tivelyincreasetheaccuracyperformanceofbothclassification like InceptionTime, augmentation is more beneficial when
modelsused.Thisistrue,eveninthecaseswheretheoriginal starting performance is strong.
performance is already high. Moreover, Table III indicates that the 3 datasets without
The ROCKET algorithm generates a large quantity of ran- augmentation benefits have between 100 to 300 instances in
domconvolutionalkernels,allindependentofeachother[13]. total, with 30 to 150 time series per class. This points to an
The latent space resulting from the extraction has a very large inherent data scarcity issue, particularly challenging for mod-
dimension some of which can be redundant, the information elsthatrequireextensiveexternaltraining,suchasTimeGANs.
is therefore saturated. Thispattern,akintoobservationswiththeROCKETalgorithm,
First,wenotethaton10outofthe13datasets,theaccuracy highlights that while data augmentation can indeed refine a
of the augmented models are better than those of the non- model’s ability to generalize, its effectiveness varies across
augmented model as shown in Table IV. This table shows datasets.AsdepictedinTableVI,diverseaugmentationstrate-
an average relative improvement of 1.55% across the 13 giescontributetoperformanceimprovements,emphasizing,as
multivariate datasets when applying the best-performing data for ROCKET , that there’s no one-size-fits-all solution.
augmentation technique compared to the baseline ROCKET
F. Future Work
classifier. It’s observed that 6 out of the 13 datasets boast
a baseline accuracy of 89% or higher, making an average Note that the contribution of data augmentation techniques
improvement of 1.55%. Notably, in the 3 datasets where no to time series classification is fairly uniform. For instance,
improvement is seen, the augmented accuracies nearly match SMOTE contributes to improvements in 8 out of 13 cases
the baseline, representing the smallest absolute values among forbothROCKETandInceptionTimemodels.TimeGANs
the 13 datasets, with a mere 0.14% depreciation on these 3 shows effectiveness in 7 out of 13 cases for ROCKET , and
datasets. Furthermore, among the 4 datasets with less than in 4 out of 13 for InceptionTime . Noise augmentationTABLE IV: Accuracy for rocket baseline model, and relative improvement
Dataset ROCKET rocket noise 1.0 rocket noise 3.0 rocket noise 5.0 rocket smote rocket timegan Improvement(%)
CharacterTrajectories 98.52 99.09 99.04 99.12 98.47 99.19 0.68
EigenWorms 89.16 79.54 82.60 83.97 91.15 88.93 2.23
Epilepsy 98.99 98.12 98.41 98.26 98.55 99.28 0.29
EthanolConcentration 41.29 39.16 40.08 40.53 42.43 42.05 2.76
FingerMovements 52.20 54.80 54.00 55.00 53.80 54.80 5.36
Handwriting 58.71 59.13 56.61 56.78 59.91 57.93 2.04
Heartbeat 73.76 73.07 74.63 72.59 75.32 74.34 2.11
LSST 63.84 61.97 62.54 62.64 61.39 63.78 -0.09
PEMS-SF 82.43 83.93 82.66 83.35 83.35 82.31 1.82
PenDigits 97.87 97.77 97.75 97.71 97.72 97.66 -0.10
RacketSports 90.66 90.92 91.05 90.53 91.32 91.58 1.01
SelfRegulationSCP1 85.39 84.85 85.19 85.19 84.51 84.98 -0.23
SpokenArabicDigits 96.20 98.34 98.23 98.26 96.44 98.40 2.29
AverageImprovement - - - - - - 1.55
TABLE V: Accuracy for InceptionTime (InT) baseline model, and relative improvement
Dataset InceptionTime InT noise 1.0 InT noise 3.0 InT noise 5.0 InT smote InT timegan Improvement(%)
CharacterTrajectories 99.51 99.51 99.30 99.20 99.55 99.41 0.04
EigenWorms 92.37 92.62 89.31 89.57 94.66 86.77 2.48
Epilepsy 97.10 97.39 96.81 96.96 97.25 96.96 0.30
EthanolConcentration 23.19 24.33 20.15 22.81 24.52 23.57 5.74
FingerMovements 53.20 50.40 48.60 47.80 51.00 48.40 -4.14
Handwriting 64.33 60.78 58.52 58.19 63.29 57.84 -1.62
Heartbeat 71.22 71.41 73.37 72.78 71.51 70.15 3.02
LSST 69.40 65.25 62.40 62.04 67.60 69.91 0.73
PEMS-SF 81.21 78.61 77.75 78.61 78.61 78.61 -3.20
PenDigits 98.96 98.74 98.77 98.99 98.99 98.79 0.03
RacketSports 87.89 89.80 89.80 87.83 88.03 88.82 2.17
SelfRegulationSCP1 76.18 74.74 76.25 76.25 77.27 77.00 1.43
SpokenArabicDigits 99.14 98.93 98.79 99.41 98.93 98.98 0.27
AverageImprovement - - - - - - 0.56
presents improvements in 7 cases for ROCKET and 8 for TABLEVI:CountofImprovementOccurrencesOverBaseline
InceptionTime . Note that simple techniques, like like
SMOTE and Noise, show performance superior to TimeGAN AugmentationTechnique ROCKET InceptionTime
in enhancing InceptionTime (maybe due to the small SMOTE 8 8
training data sizes in our setting), suggesting that complex TimeGAN 7 4
Noise 7 8
techniques are not always the most effective solution.
OVerall, the above results do not suggest a clear pattern
that one could exploit to assert superiority of any specific
augmentation technique over others. Furthermore, since a multivariate, imbalanced datasets. Our findings underscore
technique can perform well across datasets with different the potential of data augmentation to improve accuracy, but
characteristics, it indicates the potential for combining tech- demonstrate that no single technique consistently dominates
niques from various branches of our taxonomy. Similar to the acrossalldatasets.Thissuggeststhatthestrategiccombination
augmentation pipelines in computer vision, where methods of diverse augmentation strategies, inspired by successful
likeCutMix[98]arecombinedtoenhancemodelperformance, methodologies in computer vision, could lead to further im-
aconjunctiveapplicationofmultipletimeseriesaugmentation provements in model accuracy. We hope our work paves the
methods could lead to further improvements. way for innovative approaches to leveraging these techniques
for more robust and accurate models.
V. CONCLUSIONS
This study marks an advancement in the field of time
REFERENCES
series classification by incorporating a broad spectrum of [1] M. Olson, A. Wyner, and R. Berk, “Modern neural
data augmentation techniques, evaluated across both the networks generalize on small data sets,” NeurIPS, pp.
InceptionTime and ROCKET models. By introducing a 3619–3628, 2018.
noveltaxonomyofaugmentationmethods,weprovideastruc- [2] M. Banko and E. Brill, “Scaling to very very large cor-
tured approach to enhancing model performance in handling porafornaturallanguagedisambiguation,”inAssociationfor Computational Linguistic, 39th Annual Meeting and advances,”.DataMin.Knowl.Discov.,vol.31,no.3,pp.
10th Conference of the European Chapter, Proceedings 606–660, 2017.
of the Conference, July 9-11, 2001, Toulouse, France. [16] A. Ruiz and M. F. et al., “The great multivariate time
Morgan Kaufmann Publishers, 2001, pp. 26–33. series classification bake-off: a review and experimental
[3] C. Shorten and T. M. Khoshgoftaar, “A survey on image evaluation of recent algorithmic advances,” Data Min.
dataaugmentationfordeeplearning,”J.BigData,vol.6, Knowl. Discov., vol. 35, pp. 401–449, 2021.
no. 1, 2019. [17] A.Shifaz,C.Pelletier,F.Petitjean,andG.I.Webb,“TS-
[4] A. Mikołajczyk and M. Grochowski, “Data augmenta- CHIEF:Ascalableandaccurateforestalgorithmfortime
tion for improving deep learning in image classification series classification,” CoRR, vol. abs/1906.10329, 2019.
problem,” IEEE, 2018. [18] M. Middlehurst, J. Large, M. Flynn, J. Lines,
[5] S. Y. Feng, V. Gangal, J. Wei, S. Chandar, S. Vosoughi, A.Bostrom,andA.J.Bagnall,“HIVE-COTE2.0:anew
T.Mitamura,andE.H.Hovy,“Asurveyofdataaugmen- metaensemblefortimeseriesclassification,”CoRR,vol.
tation approaches for NLP,” in Findings of the Associa- abs/2104.07551, 2021.
tion for Computational Linguistics: ACL/IJCNLP 2021, [19] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P.
Online Event, August 1-6, 2021, C. Zong, F. Xia, W. Li, Kegelmeyer,“SMOTE:Syntheticminorityover-sampling
and R. Navigli, Eds., vol. ACL/IJCNLP 2021, 2021, pp. technique,” Journal of Artificial Intelligence Research,
968–988. vol. 16, pp. 321–357, 2002.
[6] Q.Wen,L.Sun,F.Yang,X.Song,J.Gao,X.Wang,and [20] J. Yoon, D. Jarrett, and M. van der Schaar, “Time-
H.Xu,“Timeseriesdataaugmentationfordeeplearning: series generative adversarial networks,” in Advances in
A survey,” in Proceedings of the Thirtieth International Neural Information Processing Systems, H. Wallach,
JointConferenceonArtificialIntelligence. International H. Larochelle, A. Beygelzimer, F. d'Alche´-Buc, E. Fox,
JointConferencesonArtificialIntelligenceOrganization, and R. Garnett, Eds., vol. 32. Curran Associates, Inc.,
aug 2021. 2019.
[7] B.K.IwanaandS.Uchida,“Anempiricalsurveyofdata [21] A. Le Guennec, S. Malinowski, and R. Tavenard, “Data
augmentation for time series classification with neural Augmentation for Time Series Classification using Con-
networks,”PLOSONE,vol.16,no.7,p.e0254841,2021. volutionalNeuralNetworks,”inECML/PKDDWorkshop
[8] P. Lang, K. Peng, J. Cui, J. Yang, and Y. Guo, “Data on Advanced Analytics and Learning on Temporal Data,
augmentation for fault prediction of aircraft engine with 2016.
generative adversarial networks,” in CAA Symposium on [22] B. K. Iwana and S. Uchida, “Time series data aug-
Fault Detection, Supervision, and Safety for Technical mentation for neural networks by time warping with a
Processes, SAFEPROCESS 2021, Chengdu, China, De- discriminative teacher,” 2020.
cember 17-18, 2021, 2021, pp. 1–5. [23] K.M.RashidandJ.Louis,“Time-warping:Atimeseries
[9] K. Babaei, Z. Chen, and T. Maul, “Data augmentation data augmentation of imu data for construction equip-
by autoencoders for unsupervised anomaly detection,” ment activity identification,” in Proceedings of the 36th
CoRR, vol. abs/1912.13384, 2019. International Symposium on Automation and Robotics
[10] R. Blagus and L. Lusa, “Smote for high-dimensional in Construction (ISARC), M. Al-Hussein, Ed., 2019, pp.
class-imbalanced data,” BMC Bioinformatics, vol. 14, 651–657.
no. 1, 2013. [24] T. DeVries and G. W. Taylor, “Improved regularization
[11] H. A. Dau, A. J. Bagnall, K. Kamgar, C. M. Yeh, of convolutional neural networks with cutout,” 2017.
Y. Zhu, S. Gharghabi, C. A. Ratanamahatana, and E. J. [Online]. Available: https://arxiv.org/pdf/1708.04552v2.
Keogh, “The UCR time series archive,” CoRR, vol. pdf
abs/1810.07758, 2018. [25] D. S. Park, W. Chan, and al., “SpecAugment:
[12] A. J. Bagnall, H. A. Dau, J. Lines, M. Flynn, J. Large, A simple data augmentation method for automatic
A. Bostrom, P. Southam, and E. J. Keogh, “The UEA speech recognition,” in Interspeech 2019. ISCA, sep
multivariate time series classification archive, 2018,” 2019. [Online]. Available: https://doi.org/10.21437%
CoRR, vol. abs/1811.00075, 2018. 2Finterspeech.2019-2680
[13] A. Dempster, F. Petitjean, and G. I. Webb, “Rocket: [26] K. Matsuoka, “Noise injection into inputs in back-
exceptionally fast and accurate time series classification propagation learning,” IEEE Transactions on Systems,
using random convolutional kernels,” Data Min. Knowl. Man,andCybernetics,vol.22,no.3,pp.436–440,1992.
Discov., vol. 34, no. 5, pp. 1454–1495, 2020. [27] C.M.Bishop, “Training with noise is
[14] C. Pelletier and D. F. S. et al., “Inceptiontime: Finding equivalent to tikhonov regularization,” Neural
alexnet for time series classification,” Data Min. Knowl. Computation, 1995. [Online]. Available:
Discov., vol. 34, no. 6, pp. 1936–1962, 2020. https://www.microsoft.com/en-us/research/wp-content/
[15] A. J. Bagnall, J. Lines, A. Bostrom, J. Large, and E. J. uploads/2016/02/bishop-tikhonov-nc-95.pdf
Keogh, “The great time series classification bake off: a [28] K. Greff, R. K. Srivastava, J. Koutnik, B. R.
reviewandexperimentalevaluationofrecentalgorithmic Steunebrink, and J. Schmidhuber, “LSTM: A searchspace odyssey,” IEEE Transactions on Neural Networks ary 1, 2019, 2019, pp. 5409–5416.
and Learning Systems, vol. 28, no. 10, pp. 2222–2232, [40] N. E. Huang, Z. Shen, S. R. Long, M. C. Wu, H. H.
oct 2017. [Online]. Available: https://doi.org/10.1109% Shih,Q.Zheng,N.Yen,C.C.Tung,andH.H.Liu,“The
2Ftnnls.2016.2582924 empirical mode decomposition and the hilbert spectrum
[29] C. Huang, “Exploring effective data augmentation with for nonlinear and non-stationary time series analysis,”
TDNN-LSTM neural network embedding for speaker Proceedings of the Royal Society of London. Series A:
recognition,” in IEEE Automatic Speech Recognition Mathematical, Physical and Engineering Sciences, vol.
and Understanding Workshop, ASRU 2019, Singapore, 454, pp. 903 – 995, 1998.
December 14-18, 2019. IEEE, 2019, pp. 291–295. [41] G.-H.Nam,S.-J.Bu,N.-M.Park,J.-Y.Seo,H.-C.Jo,and
[30] O. Steven Eyobu and D. S. Han, “Feature representation W.-T. Jeong, “Data augmentation using empirical mode
and data augmentation for human activity classification decomposition on neural networks to classify impact
based on wearable imu sensor data using a deep lstm noise in vehicle,” IEEE ICASSP, 2020.
neural network,” Sensors, vol. 18, no. 9, 2018. [42] J. Gao, X. Song, Q. Wen, P. Wang, L. Sun, and H. Xu,
[31] N. Jaitly and E. Hinton, “Vocal tract length perturbation “Robusttad: Robust time series anomaly detection via
(vtlp) improves speech recognition,” 2013. decompositionandconvolutionalneuralnetworks,”2020.
[32] N. Takahashi, M. Gygli, B. Pfister, and L. Van Gool, [Online]. Available: https://arxiv.org/abs/2002.09545
“Deep convolutional neural networks and data [43] T. Eltoft, “Data augmentation using a combination of
augmentation for acoustic event detection,” 2016. independent component analysis and non-linear time-
[Online]. Available: https://arxiv.org/abs/1604.07160 series prediction,” IJCNN, p. 448–453, 2002.
[33] X. Cui, V. Goel, and B. Kingsbury, “Data augmentation [44] P. Comon, “Independent component analysis, a new
for deep neural network acoustic modeling,” 2014 IEEE concept?”Sig.Process,vol.36,no.3,pp.287–314,1994.
International Conference on Acoustics, Speech and Sig- [45] M. A. Tanner and W. H. Wong, “The calculation of pos-
nal Processing (ICASSP), pp. 5582–5586, 2014. terior distributions by data augmentation,” J. American
[34] K. Sinapiromsaran, “Adaptive neighbor synthetic minor- Stat. Assoc, vol. 82, no. 398, pp. 528–540, 1987.
ity oversampling technique under 1nn outcast handling,” [46] H. Cao, V. Y. Tan, and J. Z. Pang, “A parsimonious
2016. mixture of gaussian trees model for oversampling in
[35] A. S. Tarawneh, A. B. A. Hassanat, K. Almohammadi, imbalanced and multimodal time-series classification,”
D. Chetverikov, and C. Bellinger, “Smotefuna: Syn- IEEE TNNLS, p. 2226–2239, 2014.
theticminorityover-samplingtechniquebasedonfurthest [47] S. Smyl and K. Kuber, “Data preprocessing and aug-
neighbour algorithm,” IEEE Access, vol. 8, pp. 59069– mentation for multiple short time series forecasting with
59082, 2020. recurrent neural networks,” ISF, 2016.
[36] H. Han, W. Wang, and B. Mao, “Borderline-smote: A [48] Y. Kang, R. J. Hyndman, and F. Li, “GRATIS:
newover-samplingmethodinimbalanceddatasetslearn- GeneRAting TIme series with diverse and controllable
ing,”inAdvancesinIntelligentComputing,International characteristics,” Statistical Analysis and Data Mining:
Conference on Intelligent Computing, ICIC 2005, Hefei, The ASA Data Science Journal, vol. 13, no. 4,
China, August 23-26, 2005, Proceedings, Part I, ser. pp. 354–376, may 2020. [Online]. Available: https:
Lecture Notes in Computer Science, D. Huang, X. S. //doi.org/10.1002%2Fsam.11461
Zhang, and G. Huang, Eds., vol. 3644. Springer, 2005, [49] Y. Bengio, G. Mesnil, Y. Dauphin, and S. Rifai,
pp. 878–887. “Better mixing via deep representations,” ICML, pp.
[37] H.He,Y.Bai,E.A.Garcia,andS.Li,“Adasyn:Adaptive 552–560,2012.[Online].Available:https://arxiv.org/pdf/
synthetic sampling approach for imbalanced learning,” 1207.4404.pdf
in 2008 IEEE International Joint Conference on Neu- [50] T. DeVries and G. W. Taylor, “Dataset augmentation in
ral Networks (IEEE World Congress on Computational feature space,” ICLR 2017, pp. 1–12, 2017. [Online].
Intelligence), 2008, pp. 1322–1328. Available: https://arxiv.org/abs/1702.05538
[38] C. Bellinger, S. Sharma, N. Japkowicz, and O. R. Za- [51] V. Verma, A. Lamb, C. Beckham, A. Najafi,
iane, “Framework for extreme imbalance classification: I. Mitliagkas, D. Lopez-Paz, and Y. Bengio, “Manifold
Swim—sampling with the majority class,” Knowledge mixup: Better representations by interpolating hidden
and Information Systems, vol. 62, pp. 841–866, 2019. states,” Proceedings of Machine Learning Research,
[39] Q. Wen, J. Gao, X. Song, L. Sun, H. Xu, and S. Zhu, vol. 97, pp. 6438–6447, 2019. [Online]. Available:
“RobustSTL: A robust seasonal-trend decomposition al- https://arxiv.org/abs/1806.05236
gorithm for long time series,” in The Thirty-Third AAAI [52] T.-H. Cheung and D.-Y. Yeung, “Modality-agnostic au-
Conference on Artificial Intelligence, AAAI 2019, The tomated data augmentation in the latent space,” Interna-
Thirty-First Innovative Applications of Artificial Intelli- tional Conference on Learning Representations (ICLR),
gence Conference, IAAI 2019, The Ninth AAAI Sympo- 2021.
sium on Educational Advances in Artificial Intelligence, [53] J. Tu, H. Liu, F. Meng, M. Liu, and R. Ding, “Spatial-
EAAI2019,Honolulu,Hawaii,USA,January27-Febru- temporal data augmentation based on lstm autoencodernetwork for skeleton-based human action recognition,” [67] R. Deng, B. Chang, M. A. Brubaker, G. Mori, and
in 2018 25th IEEE International Conference on Image A.Lehrmann,“Modelingcontinuousstochasticprocesses
Processing (ICIP), 2018, pp. 3478–3482. with dynamic normalizing flows,” NeurIPS 2020, 2020.
[54] X. Yang, X. Zhang, Z. Zhang, Y. Zhao, [68] R. Morrow and W. Chiu, “Variational autoencoders with
and R. Cui, “DTWSSE: Data augmentation normalizing flow decoders,” CoRR, vol. abs/2004.05617,
with a siamese encoder for time series,” 2020.
in Web and Big Data. Springer International [69] L. Holmstrom and P. Koistinen, “Using additive noise
Publishing, 2021, pp. 435–449. [Online]. Available: in back-propagation training,” Trans. Neur. Netw.,
https://doi.org/10.1007%2F978-3-030-85896-4 34 vol. 3, no. 1, p. 24–38, jan 1992. [Online]. Available:
[55] B. Fu, F. Kirchbuchner, and A. Kuijper, “Data augmen- https://doi.org/10.1109/72.105415
tation for time series: traditional vs generative models [70] T. T. Um, Pfister, and al., “Data augmentation of wear-
on capacitive proximity time series,” Proceedings of able sensor data for parkinson’s disease monitoring us-
the 13th ACM International Conference on PErvasive ing convolutional neural networks,” in Proceedings of
Technologies Related to Assistive Environments, 2020. the 19th ACM International Conference on Multimodal
[56] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, Interaction, 2017, p. 216–220.
D. Warde-Farley, S. Ozair, A. Courville, and Y. Ben- [71] M. Kim and C. Y. Jeong, “Label-preserving data
gio, “Generative adversarial networks,” CoRR, vol. augmentation for mobile sensor data,” Multidimensional
abs/1406.2661, 2014. Syst. Signal Process., vol. 32, no. 1, p. 115–129,
[57] A.MadhuandS.K.Kumaraswamy,“Dataaugmentation jan 2021. [Online]. Available: https://doi.org/10.1007/
using generative adversarial network for environmental s11045-020-00731-2
sound classification,” 2019 27th European Signal Pro- [72] Y.-K. W. H. Cao, X.-L. Li and S.-K. Ng, “Spo: Struc-
cessing Conference (EUSIPCO), pp. 1–5, 2019. ture preserving oversampling for imbalanced time series
[58] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein classification,” ICDM, p. 1008–1013, 2011.
GAN,” CoRR, vol. abs/1701.07875, 2017. [73] D. Y.-K. W. Cao, X.-L. Li and S.-K. Ng, “Integrated
[59] Y. Luo, L.-Z. Zhu, Z.-Y. Wan, and B.-L. Lu, “Data oversampling for imbalanced time series classification,”
augmentation for eeg-based emotion recognition with IEEETransactionsonKnowledgeandDataEngineering,
deep convolutional neural networks,” ICMM, pp. 82–93, vol. 25, no. 12, p. 2809–2822, 2013.
2018. [74] L. Abdi and S. Hashemi, “To combat multi-class imbal-
[60] G. Ramponi, P. Protopapas, M. Brambilla, and anced problems by means of over-sampling techniques,”
R.Janssen,“T-CGAN:conditionalgenerativeadversarial IEEETransactionsonKnowledgeandDataEngineering,
network for data augmentation in noisy time series with vol. 28, no. 1, pp. 238–251, 2016.
irregular sampling,” CoRR, vol. abs/1811.08295, 2018. [75] T.Zhu,Y.Lin,andY.Liu,“Oversamplingforimbalanced
[61] S. Harada, H. Hayashi, and S. Uchida, “Biosignal data time series data,” CoRR, vol. abs/2004.06373, 2020.
augmentation based on generative adversarial networks,” [76] H. Sakoe and S. Chiba, “Dynamic programming algo-
2018 40th Annual International Conference of the IEEE rithm optimization for spoken word recognition,” IEEE
Engineering in Medicine and Biology Society (EMBC), Transactions on Acoustics, Speech, and Signal Process-
pp. 368–371, 2018. ing, vol. 26, pp. 159–165, 1978.
[62] G. Chen,Y. Zhu, Z.Hong, and Z.Yang, “Emotionalgan: [77] J. Zhao and L. Itti, “shapeDTW: Shape dynamic time
Generating ecg to enhance emotion state classification,” warping,” Pattern Recognit., vol. 74, pp. 171–184, 2018.
Proceedings of the 2019 International Conference on [78] F. Petitjean, A. Ketterlin, and P. Ganc¸arski, “A global
Artificial Intelligence and Computer Science, 2019. averagingmethod fordynamic timewarping, withappli-
[63] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, cations to clustering,” Pattern Recogn., vol. 44, no. 3, p.
O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and 678–693, 2011.
K. Kavukcuoglu, “Wavenet: A generative model for raw [79] Bellman, Dynamic programming, 1957.
audio,” 2016. [80] Vinod, H. D, and e. a. Javier Lopez-de Lacalle, “Max-
[64] D. Salinas, V. Flunkert, and J. Gasthaus, “Deepar: Prob- imum entropy bootstrap for time series: the meboot r
abilistic forecasting with autoregressive recurrent net- package,” Journal of Statistical Software, vol. 29, no. 5,
works,” 2017. pp. 1–19, 2009.
[65] J. Benton, Y. Shi, V. D. Bortoli, G. Deligiannidis, and [81] I. Y. Javeri, M. Toutiaee, I. B. Arpinar, T. W. Miller, and
A. Doucet, “From denoising diffusions to denoising J. A. Miller, “Improving neural networks for time series
markov models,” CoRR, vol. abs/2211.03595, 2022. forecasting using data augmentation and automl,” 2021.
[66] I. Kobyzev, S. J. Prince, and M. A. Brubaker, “Nor- [82] R. Hasibi, M. Shokri, and M. D. T. Fooladi, “Aug-
malizing flows: An introduction and review of current mentation scheme for dealing with imbalanced network
methods,” IEEE Transactions on Pattern Analysis and traffic classification using deep learning,” CoRR, vol.
Machine Intelligence, vol. 43, no. 11, pp. 3964–3979, abs/1901.00204, 2019.
nov 2021. [83] H.Lou,Z.Qi,andJ.Li,“One-dimensionaldataaugmen-tationusingawassersteingenerativeadversarialnetwork suring the class-imbalance extent of multi-class prob-
with supervised signal,” in 2018 Chinese Control And lems,” Pattern Recognit. Lett., vol. 98, pp. 32–38, 2017.
Decision Conference (CCDC), 2018, pp. 1896–1901. [97] L. N. Smith, “Cyclical learning rates for training neural
[84] S. K. Lim, Y. Loo, N. Tran, N. Cheung, G. Roig, networks,” in 2017 IEEE Winter Conference on Applica-
and Y. Elovici, “DOPING: generative data augmentation tions of Computer Vision, WACV 2017, Santa Rosa, CA,
for unsupervised anomaly detection with GAN,” CoRR, USA, March 24-31, 2017, 2017, pp. 464–472.
vol. abs/1808.07632, 2018. [Online]. Available: http: [98] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and
//arxiv.org/abs/1808.07632 Y. Yoo, “Cutmix: Regularization strategy to train strong
[85] A. Makhzani, J. Shlens, N. Jaitly, and I. J. classifiers with localizable features,” in Proceedings of
Goodfellow, “Adversarial autoencoders,” CoRR, the IEEE/CVF International Conference on Computer
vol. abs/1511.05644, 2015. [Online]. Available: Vision (ICCV), 2019, pp. 6023–6032.
http://arxiv.org/abs/1511.05644
[86] A. Alexandrov, K. Benidis, M. Bohlke-Schneider,
V.Flunkert,J.Gasthaus,T.Januschowski,D.C.Maddix,
S. S. Rangapuram, D. Salinas, J. Schulz, L. Stella, A. C.
Tu¨rkmen, and Y. Wang, “Gluonts: Probabilistic time
series models in python,” CoRR, vol. abs/1906.05264,
2019.
[87] L. Ye and E. J. Keogh, “Time series shapelets: a new
primitive for data mining,” in Proceedings of the 15th
ACM SIGKDD International Conference on Knowledge
DiscoveryandDataMining,Paris,France,June28-July
1,2009,J.F.E.IV,F.Fogelman-Soulie´,P.A.Flach,and
M. J. Zaki, Eds., 2009, pp. 947–956.
[88] R. A. Jarvis and E. A. Patrick, “Clustering using a
similarity measure based on shared near neighbors,”
IEEE Transactions on Computers, vol. C-22, pp. 1025–
1034, 1973.
[89] H. I. Fawaz and G. F. et al., “Deep learning for time
seriesclassification:areview,”DataMin.Knowl.Discov.,
vol. 33, no. 4, pp. 917–963, 2019.
[90] A. Bagnall and J. L. et al., “Time-series classifica-
tion with cote: the collective of transformation-based
ensembles,” IEEE Trans Knowl Data Eng, vol. 27, p.
2522–2535, 2015.
[91] O. T. Wang Z, Yan W, “Time series classification from
scratch with deep neural networks: A strong baseline,”
International Joint Conference on Neural Networks, p.
1578–1585, 2017b.
[92] B.A.LinesJ,TaylorS,“Hive-cote:Thehierarchicalvote
collective of transformation-based ensembles for time
series classification,” IEEE International Conference on
Data Mining, p. 1041–1046, 2016.
[93] ——, “Time series classification with hive-cote: The
hierarchical vote collective of transformation-based en-
sembles,” ACM Transactions on Knowledge Discovery
from Data, vol. 12, no. 5, 2018.
[94] I. S. A. Krizhevsky and G. E. Hinton, “Imagenet classi-
fication with deep convolutional neural networks,” NiPS,
p. 1097–1105, 2012.
[95] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual
learning for image recognition,” in 2016 IEEE Confer-
enceonComputerVisionandPatternRecognition,CVPR
2016, Las Vegas, NV, USA, June 27-30, 2016, 2016, pp.
770–778.
[96] J. Ortigosa-Herna´ndez, I. Inza, and J. A. Lozano, “Mea-