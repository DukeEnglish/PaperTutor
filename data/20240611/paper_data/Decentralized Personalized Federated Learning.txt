Decentralized Personalized Federated Learning
SalmaKharrat
Computer,Electrical,andMathematicalSciencesandEngineeringDivision
KingAbdullahUniversityofScienceandTechnology(KAUST)
salma.kharrat@kaust.edu.sa
MarcoCanini
Computer,Electrical,andMathematicalSciencesandEngineeringDivision
KingAbdullahUniversityofScienceandTechnology(KAUST)
SamuelHorvath
MachineLearningDepartment
MohamedbinZayedUniversityofArtificialIntelligence(MBZUAI)
Abstract
Thisworktacklesthechallengesofdataheterogeneityandcommunicationlimi-
tationsindecentralizedfederatedlearning. Wefocusoncreatingacollaboration
graphthatguideseachclientinselectingsuitablecollaboratorsfortrainingpersonal-
izedmodelsthatleveragetheirlocaldataeffectively. Ourapproachaddressesthese
issuesthroughanovel,communication-efficientstrategythatenhancesresource
efficiency. Unliketraditionalmethods,ourformulationidentifiescollaboratorsat
agranularlevelbyconsideringcombinatorialrelationsofclients,enhancingper-
sonalizationwhileminimizingcommunicationoverhead. Weachievethisthrough
abi-leveloptimizationframeworkthatemploysaconstrainedgreedyalgorithm,
resultinginaresource-efficientcollaborationgraphforpersonalizedlearning. Ex-
tensiveevaluationagainstvariousbaselinesacrossdiversedatasetsdemonstrates
thesuperiorityofourmethod,namedDPFL.DPFLconsistentlyoutperformsother
approaches,showcasingitseffectivenessinhandlingreal-worlddataheterogeneity,
minimizingcommunicationoverhead,enhancingresourceefficiency,andbuilding
personalizedmodelsindecentralizedfederatedlearningscenarios.
1 Introduction
Theongoingunprecedentedgrowthindatacapturedandstoredonedgedeviceshasledtoaflurry
ofresearchproposingcollaborativelearningparadigmsthatdonotnecessitatetransferringdatato
acentralizedlocation,therebypreservingtheprivacyofthedata. Apopularapproachisfederated
learning(FL)[37],whereclientssharemodelparametersinsteadofdatasamples. Inthisapproach,
trainingoccursoveraseriesofrounds,alternatingbetweenaphaseoftrainingonclientdevicesusing
theirlocally-helddatafollowedbyanaggregationphaseoftheclients’modelupdates.
AkeychallengeinFListhepresenceofstatisticalheterogeneity,wherebymodelsaretrainedwith
clients’datathatarenotindependentandidenticallydistributed(non-IID)[24]. Thisheterogeneity
may lead to local models drifting apart, making it challenging to derive a globally satisfactory
model[54,18]. Moreover,real-worldscenariosfrequentlylackpriorknowledgeofdatadistributions,
makingitchallengingtoquantifydataheterogeneitybeforehand.
Cateringtotheinherentheterogeneity,personalizedFLapproaches(e.g.,[11,30,43,21,8])aimto
trainforeachclient,amodeltailoredtoitsspecificdatapatterns.Inthiswork,weadvocatefortargeted
Preprint.
4202
nuJ
01
]GL.sc[
1v02560.6042:viXracollaboration, identifyingbeneficialcollaboratorsforeachclient. Inthisscenario, decentralized
learningbecomesparticularlyrelevantsincecommunicationcanberestrictedtocollaboratingclients,
therebyreducingcommunicationoverhead.
Indeed,decentralizedlearning–whereclientsdirectlycollaboratewithinapeer-to-peernetwork–
hasrecentlywitnessedasurgeininterestbyresearchers[12,49,7,32,33,23,44,46,2]. Inthis
paradigm,devicesformasparsecollaborationgraphoverwhichmodelupdatesareshared,andthere
isn’tacentralservertoorchestratetheprocess,enhancingresilienceagainstsinglepointsoffailure
while also offering enhanced privacy protection [9], allowing for faster model training [32], and
providingrobustnesstowardsslowclientdevices[39].
However,acrucialstepindecentralizedlearningistheneedtoidentifythecollaborationgraph,i.e.,
identifyforeachclientitssuitablecollaborators. Thequalityofthecollaborationgraphsignificantly
impactstheperformanceoftheresultingpersonalizedmodels,whichdeterminestheeffectivenessof
collaborationamongclients. Wenotethatthereisastronginterdependencebetweendataheterogene-
ityandcollaboration. Forinstance,ifdatadistributionsamongclientsarerelativelyhomogeneous,
collaborative learning will likely benefit every client involved. However, as data heterogeneity
increases,“blind”collaborationmayhavenegativeconsequencesfortheperformanceofsomeor
allclients. Itischallengingtoconstructanoptimalcollaborationgraph(e.g.,onethatmaximizes
clients’modelperformance)becauseitmustaccountfortwokeyfactors: dataheterogeneityand
resource constraints (network bandwidth, memory, and compute resources). This paper seeks to
deviseasolutiontotheabovechallenges.
Several methods have been proposed to personalize models for better adaptation to client data,
utilizingmulti-tasklearning[42],meta-learning[22,14,1],transferlearning[48,35],andknowledge
distillation[28,6]. However,thesemethodsrelyonaglobalmodel. Theyhandlepersonalizationby
eitherincludingaregularizationtermtoencouragethelocalmodeltostayclosetotheglobalmodel,
consideringaninterpolationbetweenlocalandglobalmodels,orfine-tuningglobalmodelstoachieve
personalizedsolutions. Theseapproachesrestrictthescalabilityofthesemethodsindecentralized
settings. Moreover,theydisregardtheimportanceofidentifying,inafine-grainedmannerandfrom
eachclient’sperspective,thesetofcollaboratingdevicesthatcanbestenhanceitsmodel.
In this work, we propose a novel bi-level optimization problem aiming at jointly optimizing the
clients’modelsandthecollaborationgraphwhileensuringcommunicationandresourceefficiency.
Thedrivingprincipleisthateachclientwillcollaborateonlywithclientsthatyieldpositivereturnsfor
theircollaboration.Wedefinethispositivereturnthrougharewardfunction,assessingthecooperation
ofasetofclients. Thealgorithmweproposeforgraphconstructionleveragesthecombinatorial
effectofclientswhilealsorestrictingthenumberofnodesaclientcancommunicatewithandthe
numberofmodelsitcanstoreduetomemoryconstraints. Thisfurtheroptimizescommunicationand
resourceefficiency. Accordingtoourformulation,theaggregatedmodelforeachclientisdirectly
determinedbytheinwardedgesconnectingthatclienttoothers. Ourmethodcaninferbeneficial
collaboratorsforeachclient,effectivelyaddressingdataheterogeneitywithoutrequiringanyprior
knowledgeaboutthedatadistribution.
Itiscrucialtonotethatunlikemanypriorstate-of-the-artsolutions(e.g.,[50,29,31]),ourcollabora-
tiongraphdoesn’tnecessarilyhavetobesymmetric;thatis,clientAcanbenefitfromcollaborating
with client B, while client B may receive a negative reward if it collaborates with client A. This
situationcouldarise,forexample,inascenariowhereclientBhasalargenumberofdatasamples,
andtheoptimalstrategyforitmightbetocollaboratewithnoone. Conversely,otherclientswith
similardatadistributionsbutsmallerdatasetsmightfindcollaborationwithclientBhighlyvaluable.
Beyondourmainobjectiveoffindingmorefine-grainedcollaborators,whichcanbeachievedthrough
potentiallydirectededges,allowingfordirectedgraphsoffersseveraladvantagesoverundirected
graphs. First,adirectedgraphisamoregeneralandrelaxedassumptionthananundirectedgraph,
asthelattercanbeviewedasaspecialcase. Moreover,ithasbeendemonstratedthatundirected
graphsaremoresusceptibletodeadlockinpracticalscenarios[45,33]. Finally,manyapplications
necessitatedirectedcommunicationnetworks[53,38].
2Contributions.
• WeformulateanoveldecentralizedpersonalizedFLproblem,whichintroducesaconstrained
discretecombinatorialobjectivewithinabi-leveloptimizationframework. Thisframework
jointlyaccountsforgraphgeneration,modelpersonalization,andresourceconstraints.
• WeproposeaDPFLalgorithm,whichalternatesbetweenoptimizingpersonalizedmodelsand
inferringsetsofcollaboratorsthroughanefficientapproximationalgorithm. Graphoptimiza-
tionreliesonthemarginalgainsofaddingorremovingcollaborators,therebyconsideringthe
combinatorialeffectsofclientsinsteadofrelyingonpairwisecollaboratorselection.
• Wedeviseasolutionforasymmetricgraphconstruction,offeringenhancedgranularityfor
collaborators’selectionandimprovedcommunicationandresourceefficiency.
• Weconductextensiveexperimentsonvariousdatasetsunderdiverse,realisticscenariosand
evaluateourapproachagainstelevenmethodsbetweenpersonalizedFLmethodsandother
baselines;ourempiricalresultsdemonstratethatDPFLofferssuperiorperformance.
2 Methodology
WeintroduceanovelformulationfortheoptimizationprobleminpersonalizedFL.First,weprovide
theunderlyingintuitionbehindtheproblemandpresentthecomprehensiveproblemformulationin
§2.1. Next,weproposedecomposingtheproblemandemployinganalternatingminimization§2.2.
2.1 Optimizationproblem
GeneralizationinFLaimstofitasingleglobalmodelwithparameterswtothenon-IIDdataheldby
individualclients[37]. Thisdistributedoptimizationproblemcanberepresentedasfollows:
(cid:40) N (cid:41)
(cid:88)
min FD(w)≜ p F (w) , (1)
k k
w∈Rd
k=1
where N is the number of devices, D is a global distribution, p ≥ 0 is the weight of the k-th
k
devices.t.
(cid:80)N
p =1. Itiscommontoselectp proportionaltodevicek’sdatasetsize. Suppose
k=1 k k
thek-thdeviceholdsdatadrawnfromthedistributionD . ThelocalobjectiveF (·)isdefinedas:
k k
F (w)≜E ℓ(w;x )whereℓ(·;·)issomelossfunction.
k xk∼Dk k
InpersonalizedFL,theprimaryobjectiveshiftsfromseekingasingleglobalmodelthatcangeneralize
acrossallclientstothepursuitoflocalmodelstailoredtoeachclient.Theselocalmodelsaredesigned
toperformwellonunseendatasamplesdrawnfromthesamedistributionastheclient’slocaltraining
data. Consequently,theoriginalobjectivefunctioninEq.(1)becomesthefollowing:
(cid:40) N (cid:41)
(cid:88)
min F(w)≜ p F (w ) . (2)
k k k
w={wi∈Rd}N
i=1 k=1
Consideringsignificantheterogeneityinthedatadistributionsamongtheclients,anintuitivesolution
totheproblemoutlinedinEq.(2)isthateachclientindependentlytrainsitslocalmodelusingits
localdata,obviatingthenecessityforcollaborationwithotherclients. However,thisholdsonlyif
eachclienthasaccesstoaninfinitenumberofIIDsamplesfromitslocaldistributionD orthelocal
k
distributionsaresignificantlydifferent.
Wefocusoncollaborativelearninginreal-worldsettings,wherelimiteddataaccessandheterogeneity
arecriticalchallenges. Clientsinsuchscenariosoperatewithafinitenumberoflocalsamples. While
weadvocateforinter-clientcollaboration,thiscollaborationmustalignwith,notcompromise,our
coreobjectiveofpersonalizedmodeleffectiveness. Collaborationneedstobedesignedtoenhance
personalizedmodelperformancewithinthiscomplexdatalandscapewhilerespectingcommunication
andresourceconstraints,particularlyindecentralizedsettingswherecoupledall-to-allcommunication
isprohibitive. Therefore,accountingfortheapplication-imposedresourcebudgetbecomescrucial.
3Wereformulateourobjectivefunctiontoaddressthesechallengesandoptimizemodelperformance
withinthisdynamicenvironment. Thus,weconsiderthisproblemformulation:
(cid:40) N (cid:41)
(cid:88)
min F(w,C)≜ p F (w ,C ) . (3)
k k k k
w={wi∈Rd}N
i=1 k=1
C={Ci∈2[N]\{i};|Ci|≤Bc}N i=1,
Inthisoptimizationproblem(Eq.(3)), ourobjectiveistominimizethefunctionF(w,C), where
w = {w ,...,w } represents a set of parameters and C = {C ,...,C } denotes collaboration
1 N 1 N
assignments. C ∪{i}containsj,ifclientireceivesupdatesfromclientj. Moreover,inouralgorithm
i
design, we enforce ∀k ∈ [N] : |C | ≤ B , where B represents the resource budget to improve
i c c
communicationandresourceefficiency. ThefunctionF (w ,C )isdefinedastheaveragelossover
k k k
adatasetD forclientk,expressedas:
k
F (w ,C )≜E ℓ(w ,C ;x ).
k k k xk∼Dk k k k
Here,ℓ(w ,C ,x )representsthelossfunctionappliedtotheparametersw ,collaborationassign-
k k k k
mentsC ,anddatapointx . Wecanrewriteitasfollows: ℓ(w ,C ;x ) = ℓ(wˆ ;x )wherethe
k k k k k k k
crucialelementinthisformulationiswˆ ,computedastheaverageofindividualparametersw within
k i
thecollaborationsetC forclientk:
k
wˆ ≜ 1 (cid:88) p w ; whereC˜ ≜C ∪{k}. (4)
k (cid:80) p i i k k
i∈C˜
k
i
i∈C˜
k
2.2 Alternatingminimization
Solving Eq. (3) involves concurrently optimizing for both w and C. To simplify this complex
problem, weproposeanalternatingminimizationapproach. Inthefirstsub-problem, weusethe
givencollaborationgraphC⋆toupdatelocalparametersw ,whileinthesecondsub-problem,we
k
focusonidentifyingthebestsetofcollaboratorsC givenw. Thefirstsub-problemcanbewrittenas:
k
(cid:40) N (cid:41)
(cid:88)
min F(w,C⋆)≜ p F (w ,C⋆) . (5)
k k k k
w={wi∈Rd}N
i=1 k=1
WeproposetosolveEq.(5)inadecentralizedsetting,whereeachclientfirstperformsτ localupdates
oftheirlocalparametersw basedontheirlocaldata. Thisisfollowedbytheaggregationstep,in
k
whicheachclientupdatestheirlocalsolutionw asdefinedinEq.(4).
k
Forfindingthecollaborationgraph,givenfixedw⋆,thecollaborationassignmentC foreachclient
k
k ∈[N]isfoundasasolutiontothefollowingminimizationproblem:
 
1 (cid:88)
Cm k∈i Ωn kF kV (cid:80)
i∈Ck∪{k}p
i
i∈Ck∪{k}p iw i⋆ , (6)
whereFV(·)representsthevalidationlossofclientkandΩ ⊆2[N]\{k} suchthat|Ω |≤B isa
k k k c
setofclientsthatclientkcanpotentiallycollaboratewithotherthanhimself. Furtherdetailsabout
theproposedmethodsareprovidedin§3.
3 ProposedMethod
WeintroduceDPFL,whichincorporatestheidentificationofbeneficialcollaboratorsalongsideFL
training. To achieve this, we define a combinatorial objective function on the discrete and large
spaceofclientcombinationsinEq.(6),andintroduceaconstrainedgreedyalgorithmdenotedas
GGC. Thisalgorithmapproximatesasolutionforthisobjectivebyoptimizingclientcombinationsto
enhancedecentralizedpersonalizedFLunderstrictcommunicationandresourceconstraints.
3.1 DecentralizedPersonalizedFL(DPFL)
Theprimarygoalforeachclientistoidentifypotentialcollaboratorswhocanassistinachievingbetter
personalizationwhileadheringtoaresourcebudgetconstraint,denotedasB . Algorithm1,DPFL,
c
4detailsthepseudocodeofouremployedapproach. Asapreprocessingstep,weestablishtheinitial
collaborationgraph,Ω ,ensuringadherencetothebudgetconstraint. Thisprocesscommencesby
k
initializingeachlocalmodelasw,followedbyconductingτ localtrainingepochs. Subsequently,
init
aftereachclientacquiresitsinitialsolutionwinit,itproceedstodetermineitscollaborationset,Ω ,
k k
asdescribedin§3.2andAlgorithm3inAppendixC.Withtheinitialgraphinplace,themaintraining
loopbegins,spanningT communicationrounds. Duringeachround,clientsperformlocaltraining
forτ epochsanddownloadthelocalmodelsw fromallclientsintheirrespectivecollaboration
train i
setsΩ whereΩ containsatmostB clients. Uponreceivingthesemodels,eachclientdetermines
k k c
itsbestcollaboratorsforthatspecificround
Algorithm1DecentralizedPersonalizedFL(DPFL)
using Algorithm 2. Specifically, each
client k calculates the weighted average Require: T,w,N,m,B c,τ init,τ train,LocalOpt
of the models received from Ω that of- 1: fordevicek∈[N]inparalleldo{Preprocess}
k
fersthebestbenefitsbyminimizingitslo- 2: w kinit ←LocalOpt(w,τ init)
calvalidationloss. Theresultingsetfrom 3: Ω k ←BGGC(k,[N],m,B c) {Explainedin§3.3}
this step for each client k is C k, where 4: w k ← (cid:80) i∈Ω1 k∪kpi (cid:80) i∈Ωk∪kp iw iinit
|C | ≤ |Ω | and C ⊆ Ω . In essence, 5: endfor
k k k k
the offline-constructed graph during pre- 6: fort=1,··· ,T −1do{TrainingLoop}
processing using BGGC remains static, 7: fordevicek∈[N]inparalleldo
and the training method serves as an av- 8: w k ←LocalOpt(w k,τ train)
9: sendw ,∀js.t.k∈Ω andreceivew ,∀j ∈Ω ,
eragingmechanismwheretheoptimalset k j j k
10: C ←GGC(k,Ω ,m,B ) {Explainedin§3.2}
ofcollaboratorsisdeterminedusingAlgo- 11: wk ← 1 k (cid:80) c p w
rithm2. Toreducethecomplexityofthe k (cid:80) i∈Ck∪kpi i∈Ck∪k i i
graphconstructionstep,weavoidupdating 12: endfor
13: endfor
thegraphstructurebyremovingedges
whenGGCdoesnotselectaclientforaggregationinline10. Infact,aclientmightbeskippedinthe
currentroundduetofactorslikenoisebutstillmayofferbenefitsinfuturerounds. Inourexperiments,
localSGDisusedasthelocaloptimizer,thoughDPFLisadaptabletoanylocaloptimizer.
3.2 GreedyGraphConstruction(GGC)
OurprimarygoalistosolveEq.(6). Inthiscombinatorialobjective,clientkseeksasetC wherein
k
collaboration with its elements results in a positive return. In our context, this positive return is
quantifiedbyarewardfunctionrepresentingthevalidationlossofeachclient. Thisconceptualization
ispivotalasourapproachdepartsfrompreviousworks,whichtypicallyfocusedonpairsofclients,
e.g.,[50],overlookingthesignificanceandsynergyoftheentiregroup. Forinstance,clientsAandB
collaboratingalonemightnotbeideal,butaddingclientC tothecollaborationsetcouldsignificantly
altertheoutcome. Insuchascenario, thecollaborativeeffortsofclientsA, B, andC contribute
toadecreaseintheoveralllossexperiencedbyclientA. Werefertothemotivationalexamplein
AppendixAexplainingtheimportanceofthecombinatorialeffectinboostingclientperformance,in
whichwedemonstratehowrelyingonpairwisecomparisonsmayfailcomparedtogroupsynergy.
FindingtheoptimalsetC foreachclientkrequiresextensivecomputationalexploration.Furthermore,
k
noticethatourobjectivefunctioninEq.(6)isnotmonotonic,1i.e.,addingclientstothesetC does
k
not always result in a better reward. To address this, we adapt the non-monotone combinatorial
banditalgorithmof[17], proposingagreedygraphconstruction(GGC),detailedinAlgorithm2
inAppendixB,whichefficientlyselectsthesetC . Thecomplexityoftheemployedalgorithmis
k
O(|Ω |),whichisatmostO(B ).
k c
WhileourobjectiveistominimizethelocalvalidationlossofclientsasdefinedinEq.(6),following
theliteratureoncombinatorialmaximization, itismorecommontomaximizearewardfunction.
Thus,wedefineourrewardfunctionasfollows:
 
1 (cid:88)
R(S)≜−F kV (cid:80)
p
p iw i, (7)
i∈S∪{k} i i∈S∪{k}
Giventhedefinedreward,GGCoperatesthroughaseriesofiterations,eachinvolvingadecision-
makingprocessforaddingorremovingclientsfromtwosets,X andY. ThesetX representsthe
setofcollaboratorsandinitiallycontainsclientkwhichistheclientrunningGGC,andY contains
1Asetfunctionf :2Ω →RismonotoneifforanyA⊆B ⊆Ωwehavef(A)≤f(B).
5clientsinΩ ∪{k}. Ineachstep,thealgorithmcomputestwovariables,aandb,representingthe
k
expectedmarginalgainsfromaddingandremovingaspecificclientj ∈S,respectively,anddefined
asfollows: a=max(R(X ∪{j})−R(X),0)andb=max(R(Y \{j})−R(Y),0). Clientk,
runningGGC,addsaclientj tothesetofcollaborators,X,greedilywithaprobabilityp = a ,
a+b
i.e,aclientislikelytobeaddedifthemarginalgainofaddingitishigherthanthemarginalgainof
removingit(a≥b). Ifthemarginalgainofaddingispositiveandthemarginalgainofremovingis
negative,thenaclientisaddedwithp=1. Conversely,ifthemarginalgainofremovingispositive
andthemarginalgainofaddingisnegative,theclientisremovedwithp=1. Incaseswherebotha
andbarezero,thealgorithmdefaultstosettingp=1. Theprocessiteratesuntildecisionsaremade
forallindividualclientsorthecardinalityofthesetofcollaboratorsX reachesthebudgetconstraint
B ,ultimatelyidentifyingtheoptimizedsetofclientssatisfyingtheimposedbudgetconstraint.
c
GGC algorithm complexity: The overall complexity of GGC (Algorithm 2) is O(B ), which
c
reducestoconstantcomplexityO(1)sinceduringtrainingB (budgetconstraint)isconstant. GGC
c
operatesbyiteratingoverthereceivedclientsasinput. Itcomputestwovariablesaandb,requiring
constantnumberforwardpassesoverthenetwork,whichtranslatestoO(1)complexity.Subsequently,
itdecideswhethertoaddorremoveaclient, anoperationalsoachievedinO(1). Therefore, the
algorithm’scomplexityessentiallyreducestoloopingovertheclients,resultinginO(B )duringthe
c
trainingwhichdoesnotincreasewithagrowingnumberofclientsasB isapredefinedconstant.
c
3.3 BatchedGreedyGraphConstruction(BGGC)
Asoutlinedin§2.2,eachclientkcanonlycollaboratewithasetofclientsΩ ,whichisrestrictedin
k
size(i.e.,|Ω |≤B )tomaintaincommunicationandresourceefficiency. Moreover,asdepictedin
k c
Algorithm1,theinitialgraphconstructionisconductedasapreprocessingstepwhereeachclient
trainsamodellocallyforasufficientnumberofepochsusingthesamemodelinitializationparameters,
resulting in the client’s local solution denoted as winit (lines 1-2). Then in line 3, each client k
k
determineitssetofbeneficialcollaboratorsΩ byminimizingthefollowingoptimizationproblem:
k
(cid:32) (cid:33)
1 (cid:88)
min FV p winit s.t.|S|≤B . (8)
S⊆[N]\k k (cid:80) i∈S∪kp i
i∈S∪k
i i c
AlthoughourproposedAlgorithm2in§3.2efficientlyapproximatesthesolutiontoourobjective
Eq.(8)whiledemonstratingresiliencetonoisyrewards(c.f. Corollary2in[17]),thismethodrelies
on reward computations to the objective in Eq. (8), necessitating model updates from potential
collaborators. ThisposesasignificantchallengewhenappliedtotheinitialgraphgenerationofΩ ,
k
wherethenumberofrequiredmodelsmaysurpassourbudgetconstraintB ,dictatedbypractical
c
limitationssuchasnetworkbandwidth,computeresources,andstorage. Toaddressthischallenge,we
proposeanenhancementtothealgorithmGGCcalledBGGC(BatchedGreedyGraphConstruction)
bystreamliningrewardcomputationsintoefficientbatchesofcommunicationandcomputation,each
requiringonlyO(B )resources.DetailsofthisapproachareoutlinedinAlgorithm3;seeAppendixC.
c
NotethatBGGCisnecessaryonlyduringthepreprocessingstep. Duringthetrainingloop(lines
6-12inAlgorithm1),therewardcomputationsofGGCwillrequire,atmost,B models,whichfalls
c
withintheimposedbudgetconstraint.
BGGCalgorithmcomplexity. TheoverallcomputationcomplexityofBGGCisO(N). However,
thecommunicationandresourcecomplexityisonlyO(B )foreachofthe⌈N/B ⌉steps.
c c
Theorem1. Assumingseededrandomness,executingalgorithmsGGCandBGGCwiththesame
seedproducesidenticalresultsforagivenclientk,clientssetS,andbudgetB .
c
TheproofofTheorem1isreportedinAppendixD.
Remark1. DifferencesbetweenGGCandBGGC.ExecutingGGCduringpreprocessingmayviolate
thebudgetconstraint,astherewardcomputationrequiresaccesstoweightsfromallclients. BGGC
addressesthisissuebyadheringtocommunicationandresourceconstraints,thoughitrequiresan
additionalcommunicationphase. Unlikethepreprocessingstep,eachclientconsidersΩ during
k
training,whichdoesnotexceedthebudgetconstraint. Therefore,GGCbecomesmoreefficientduring
trainingbyrequiringlesscommunicationthanBGGCandadheringtothebudgetconstraint.
6CIFAR10 CINIC10 FEMNIST
Dir(0.1) Patho(3) Dir(0.1) Patho(3) NaturalSplit
LocalOnly 80.38±1.62 78.32±0.49 78.59±1.00 77.53±0.47 87.27±0.27
FedAvg[37] 47.22±1.20 50.93±1.97 38.48±0.37 39.27±1.29 87.41±0.72
FedAvg+FT 82.84±0.88 81.89±0.92 80.31±0.33 79.18±0.46 91.56±0.29
FedProx[33] 48.83±1.88 51.14±0.79 38.16±0.82 38.53±0.54 88.14±0.39
FedProx+FT[33] 78.86±0.51 73.61±1.29 76.59±1.19 73.04±1.08 93.58±0.29
APFL[42] 82.56±0.38 80.96±0.71 80.02±1.07 78.21±0.39 90.80±0.37
PerFedAvg[14] 82.38±1.43 81.17±0.52 78.88±1.25 78.90±0.37 91.69±0.40
Ditto[30] 83.10±0.70 81.19±0.63 80.33±1.27 79.52±0.31 90.83±0.46
FedRep[8] 80.81±1.09 78.61±1.67 79.56±1.12 78.46±0.69 90.04±0.28
kNN-Per[36] 82.45±0.69 81.04±0.40 80.42±1.07 79.54±0.32 91.29±0.40
pFedGraph[50] 80.48±0.75 78.34±1.04 78.73±1.04 77.58±0.16 87.70±0.13
DPFL(B =inf) 84.39±0.43 83.04±0.98 81.49±1.32 80.32±0.51 94.25±0.18
c
DPFL(B =0.2N) 84.01±0.44 82.83±0.91 81.24±1.30 80.51±0.39 94.31±0.06
c
DPFL(B =0.1N) 83.86±0.46 82.52±0.78 81.32±1.20 80.24±0.35 94.09±0.14
c
DPFL(B =0.05N) 82.91±0.81 82.17±0.54 80.91±1.31 80.11±0.30 93.82±0.17
c
Table1:Comparisonofaccuracyacrossbenchmarksisillustratedasacc±std ,whereaccrepresents
r
theaveragetestaccuraciesofallclients’models,andstd denotesthestandarddeviationoverthree
r
repetitionswithdifferentrandomseeds.
3.4 Propertiesoftheproposedoptimizationframework
Proposition1. ThecollaborationgraphresultingfromsolvingEq.(3)yieldssuperiororsameresults
compared to any collaboration graph C ∈ {C ∈ 2[N]\i;|C | ≤ B }N that imposes additional
i i c i=1
restrictionsonthecollaborationstructure,whereN isthetotalnumberofclients.
TheproofofProposition1isinAppendixE.
Remark2. ItfollowsfromProposition1thatourtargetedcollaborationgraphoutperformspure
localtraining(noonecollaborateswithanyone),anyrandomgraphwithinC,andanygraphwith
furtherimposedsymmetry.
Remark 3. Similar to [40, 16], we assume that the validation loss is a good approximation of
F (w ). Hence,Eq.(6)isagoodproxyforEq.(3). Additionally,weuseAlgorithm2,whichoffers
k k
approximationguaranteesevenwithnoisyrewardfunctions,asdemonstratedin[17,Corollary2].
Moreover,itreturnsasetnoworsethantheemptysetwithprobability1. Therefore,DPFLusingthis
algorithmforgraphgenerationisexpectedtoyieldagoodsolutiontoEq.(3).
4 Experiments
Wepresentthekeyexperimentalsettingsandresults. WeprovideadditionaldetailsinAppendixF.
4.1 Setup
WeadoptcommonlyuseddatasetsfromexistingliteratureonpersonalizedFL[30,8,36].Weconduct
experimentswithCIFAR10[27],FederatedExtendedMNIST(FEMNIST)[4],andCINIC10[10].
WithCIFAR10andCINIC10weconsidertwoscenariosforpartitioningthedatasetsaccordingto
differentheterogeneousdistributions.Thefirstscenario,denotedasPatho(3),involvesapathological
distribution split [52, 37, 7], wherein each client exclusively receives data from three specified
categories. ThesecondscenarioutilizestheDirichletdistribution[51,47],whereadistributionvector
q isdrawnfromDir (α)foreachcategoryc. Subsequently,theproportionq ofdatasamples
c k c,i
fromcategorycisallocatedtoclienti. ForFEMNIST,weconsiderthenaturalnon-IIDsplitprovided
intheLeafframework[4]whereeachwritercorrespondstoaclientandweaddmoredegreesof
heterogeneitybyhavingeachclientmissingsomeclasses. ForCIFAR10andFEMNIST,weconduct
100roundsoftraining,whileforCINIC10werunfor50rounds. Allexperimentsareexecutedacross
threedifferentrandomseeds;theresultsreporttheaveragelocaltestaccuracyalongwiththestandard
deviation. Wepreserveabestmodelperclientbasedonthevalidationdataset,andthereportedresults
areobtainedbyperforminginferenceonthetestingsetusingsuchbest-validationmodels. Other
experimentaldetails,includinghyperparameterslikethenumbersofinitializationandtrainingepochs
(τ andτ )areintheappendix.
init train
74.2 Qualityofpersonalizationresults
Table1presentstheaveragetestaccuraciesofDPFLunderfourbudgetconstraints,comparedto
otherpersonalizedFLandbaselinemethods. ForDPFL,wevarythebudgetconstraintwith20%
(B =0.2N),10%(B =0.1N),and5%(B =0.05N)ofthetotalnumberofclients,respectively;
c c c
wealsoconsiderthecasewithoutaconstraint(B =inf).
c
The results demonstrate that our method significantly outperforms other personalized methods
regardingaveragetestaccuracy. Weachievebetterresultsthanlocaltrainingby3and5percentage
points (p.p), outperform FedAvg by 37 and 33 p.p, and surpass other personalized methods by
approximately1to6and2to10p.ponCIFAR10usingDir(0.1)andPatho(3),respectively.
Moreover,onFEMNIST,weachievebetterresultsthanlocaltrainingby7p.p,outperformFedAvgby
7p.p,andsurpassotherpersonalizedmethodsbyapproximately2.8to6.5p.p. Finally,onCINIC10,
ourmethoddemonstratessuperiorperformance,surpassinglocaltrainingby3p.pinbothDir(0.1)
andPatho(3). Additionally,DPFLoutperformsFedAvgbyanotablemarginof43and41p.p,while
surpassingotherpersonalizedmethodsbyapproximately1to5and2to7p.pusingDir(0.1)and
Patho(3),respectively.
Variancebetweenlocalmodels. Asourobjectiveisper-
sonalization,apartfromtheoverallimprovementinaverage
115
accuracyamongclients,itiscrucialtoassesswhetherim- pFedGraph
110
provementsaredistributedacrossmostmodelsratherthan Better
confined to just a few clients. To evaluate this, we ana- 105 FedRep APFL
lyzethevariancebetweenlocalmodels,wherelowervari- 100 PerFedAvg
ancesignifiesgreaterparityintheirperformance. Figure1 95 Local Only Ditto FedAvg+FT
PerKNN DPFL (20)
shows results for CIFAR10 with a Patho(3) distribution. 90 DPFL (5)
Thex-axisrepresentsaveragetestaccuracy,andthey-axis 85
representsvariancebetweenclients’models. DPFLiscon- DPFL (10)
80 DPFL (inf)
sistentlyintheright-bottomcorner,showinghigheroverall 79 80 81 82 83
TestAccuracy
accuracyandlowervariancethanothermethods. Addition-
ally,AppendixG.1confirmsthesefindingsforFEMNIST Figure1: Variancebetweenlocalmod-
(Fig.5),CINIC10(Fig.6),andCIFAR10withaDir(0.1) elsusingPatho(3)datasplits.
distribution(Fig.7).
4.3 Visualizationofcollaborationgraph
Weanalyzetheinitialcollaborationgraphanditsevolutionafter50and99rounds. Forclarity,we
illustratetwocaseswithbudgetB =10andB =5(otherbudgetconstraintsareinAppendixG.2).
c c
Figs.2aand2bdepictthecollaborationgraph(plottedastheadjacencymatrix)intwostates: (left)
constructedasapreprocessingstep(line5ofAlgorithm1),and(right)atround99. Thediagonal
indicatesthateveryclientalways"collaborates"withitself. Toillustratethegraphevolution,theright
plotsdisplaycollaborativelinksintwocolors: inredaretheclientsselectedforcollaborationinthat
round;inbluearetheclientsidentifiedduringthepreprocessingstepbutarecurrentlynotchosen.
Theunionofredandblueclientscorrespondstotheinitialcollaborationgraph.
The figures highlight that the initial collaboration graph is denser compared to the actually used
clientsforaggregationinround99. Thisisexpectedsincethegraphisconstructedasapreprocessing
step,andatthisstage,modelweightshavenotyetconverged. Therefore,broadercollaborationcan
bebeneficial. However,astrainingprogresses,itisnaturaltoexpecteachclienttobenefitprimarily
fromcollaboratingwithclientswithsimilardatadistributions,thusleadingtoasparsercollaboration
graph. Anothercontributingfactoristhat,inthepreprocessingstep,thedecisiontoselectaspecific
clientforcollaborationismadefromapoolof100clients,makingitmorechallengingthaninlater
roundswherethedecisionisdrawnfromasmallerpooldenotedasΩ forclientk.
k
Finally,weanalyzethegraphsparsity. WithB =10,initialsparsitystandsat80%anditincreases
c
to 88% at round 99. With B = 5, the initial sparsity is 95% and is 96% at round 99. We also
c
measurethesymmetryofthecollaborationgraphacrossdifferentbudgets(detailsinAppendixG.4);
weobservearound80-88%symmetry,whichdecreasesasthebudgetconstraintlowers.
8
ecnairaVCollaboration Graph Round 99 Collaboration Graph Round 99
0 0 0 0
20 20 20 20
40 40 40 40
60 60 60 60
80 80 80 80
0 20 40 60 80 0 20 40 60 80 0 20 40 60 80 0 20 40 60 80
(a)B =10 (b)B =5
c c
Figure2: CollaborationgraphusingCIFAR10with100clients.
Benign Malicious Benign Connection Malicious Connection Benign Malicious Benign Connection Malicious Connection
Collaboration Graph Round 99 Collaboration Graph Round 99
(a)Maliciousclientsdon’texecuteGGC. (b)MaliciousclientsexecuteGGC.
Figure4: Collaborationgraphwhen40%ofclientshaveflippedlabels(malicious),while60%haveoriginal
labels(benign). Forbothscenarios,weshowtheinitialcollaborationgraph(left)anditsevolutionafter99
rounds(right).Maliciousclientsappearinred;benignonesareinblue.
Round
4.4 Comparisontoarandomly-generatedgraph
84
83
ToillustratetherelativeimportanceofGGC,wecompareDPFL 82
toaversionofourmethodthatreplacesGGCwitharandomly- 81
constructedcollaborationgraph.Fig.3showsthatDPFLoutper- 80
formstherandomgraphby≈4p.p,inthreebudgetconstraints. 79 DPFL (c=20)
ThisexperimentusesCIFAR10with100clientsandDir(0.1). 78 D DP PF FL L ( (c c= =1 50 ))
77 Random graph (c=20)
Random graph (c=10)
76 Random graph (c=5)
0 20 40 60 80 100
4.5 BehaviorofDPFLunderdataflipattack Figure3: DPFLvs.randomgraph.
To analyze how DPFL behaves in the presence of distinct groups of clients, we select 40 clients
(malicious)outof100andfliptheirlabelsusingthesamepermutation; theremaining60clients
(benign) use the true labels (according to CIFAR10). It’s important to note that our objective is
delineatethebehaviorofDPFL,acknowledgingthatwhileitmayexhibitrobustnesscharacteristics,
thestudyofrobustnessfallsoutsidethescopeofthispaper.
We then conduct two experiments: the first in which malicious clients do not execute GGC (i.e.,
theyonlytrainlocally),Fig.4a;thesecondinwhichtheyrunGGC,Fig.4b. Inthefirstcase,the
collaborationgraphinitiallycontainsnumerousedgesinvolvingmaliciousclients. Thisisexpected
andisduetotheinherentrandomnessintheweightsduringthepreprocessingstep,whichmakesit
challengingtoidentifycollaborators. However,astheroundsprogress,weobservethatbenignclients
increasinglyavoidselectingmaliciousonesuntil,ultimately,theyceasechoosingthemaltogether.
Fig.12depictsthisevolutionevery10rounds.
Inthesecondcase,sincemaliciousclientsexecuteGGC,theyinitiallycollaboratewithbenignclients.
Thus,theirmodelsbecomeregularizedtowardsthebenignones. Despitethisbehavior,weobserve
thatastheroundsprogress,clientsbecomealmostsegregatedintotwogroups(redandblack),with
very few links between them serving as a form of regularization (full evolution in Fig. 13). On
average,benignclientshavelessthan10%connectionswithmaliciousones(seeFig.13andFig.14).
9
ycaruccAtseTB
c inf 20 10 5
τ
init
1 81.86±1.00 82.33±0.86 82.53±0.72 82.30±0.24
5 83.03±0.73 82.53±1.36 82.58±0.97 80.90±0.96
10 83.04±0.98 82.83±0.91 82.52±0.78 82.17±0.54
Table2: EffectoflocalepochsperformedinthepreprocessingstepontheconvergenceofDPFL.
4.6 Ablationstudies
Sensitivitytoτ . Table2reportstheresultsofusingCIFAR10with100clientsandPatho(3)split
init
andvaryingthenumberoflocalepochsτ . Theresultsshowthattheaccuraciesacrossdifferent
init
numbersoflocalepochsarecomparable. Performanceonaverageisslightlybetterwithτ =10,
init
buteventhecasewithτ =1yieldsgoodresults.
init
PeriodicityofrefreshingC k. Toimproveefficiency, we B
c inf 20 10 5
considerinvokingGGCatline10inAlgorithm1periodi- P
callyeveryP rounds. Table3reportstheeffectofvaryingP 1 84.20 83.67 84.20 84.33
5 83.83 84.01 83.31 83.73
whiletrainingonCIFAR10using100clientsandDir(0.1).
10 83.24 84.17 83.31 83.03
20 82.34 82.59 82.08 82.73
WeobservethatDPFLmaintainsgoodperformanceacross
variousperiodicities,withaslightdecreaseasP increases. Table 3: Effect of the periodicity of
Thisdemonstratestherobustnessofourmethodandthepo- invokingGGContheconvergenceof
tentialforimprovedefficiencybyperformingitperiodically. DPFL.
5 RelatedWork
PersonalizedFLapproachesaddressdataheterogeneitythroughvarioustechniques. Somemethods
trainseparateglobalandpersonalizedmodels[30,43]andregularizethemusinganl term. Others
2
leveragemulti-tasklearning[21,25,22,35,19,20,36],whicharebasedoninterpolationbetween
localandglobal models. Meta-learning-basedmethods, suchas[15], proposetofinda common
initialmodelthatnewclientscaneasilyadapt. Splittingmodellayers[8,34]orusinghypernetworks
forpersonalizationarealsoexplored[41,5]. Comparedtothesemethods,weeliminatepotential
communicationoverheadbynotrequiringaglobalmodel. Furthermore,weachieveafine-grained
levelofpersonalizationbydynamicallyselectingthemostrelevantcollaboratorsforeachclient.
Furthermore,unlikecosinesimilarity-basedmethods[50,29,31],whichreliesonpairwisecollabo-
ratorselection,asdiscussedin§3.2,overlookingthesignificanceandsynergyoftheentiregroup.
Instead,weproposeanovelutilityfunction(Eq.(6))thatconsidersthecombinedbenefitofcollabo-
rators. Furthermore,unlikeourmethodwhichconsidersabudgetconstraint,in[50],alltheclients
willbeconsideredwithaweightedaverage,whichlimitsitsscalabilityforreal-worldconstrained
scenarios. Finally, personalizeddecentralizedapproacheslike[46,26,13]addressheterogeneity
butassumeagivencollaborationgraphandareoftenlimitedtolinearmodelsorpre-trainedmodel
combinations. Theyalsorelyonparametertuningforgraphsparsity,unlikeourexactbudget.
6 Limitations
OurapproachconsidersuniformresourceandcommunicationbudgetsforallclientsB ,whilein
c
practice, clients may have varying resource capabilities Bi, for client i. Furthermore, this work
c
assumesthepossibilityofchoosinganyavailableclientforcommunication. However,someclients
maynotbewithincommunicabledistanceofothers,preventingestablishingaconnection. These
limitationscanbeaddressedbyconsideringpersonalizedbudgetsforeachclientandrestrictingthe
potentialsetofcollaboratorstoonlythosewithincommunicabledistance.
7 Conclusion
We address the challenge of constructing a collaboration graph in decentralized learning while
consideringdataheterogeneityandadheringtocommunicationandresourceconstraints. Toachieve
this,weproposeabi-leveloptimizationproblemandemployacombinatorialsolutionusingagreedy
algorithmtoefficientlyidentifythecollaborationgraph. Wecomparedoursolutiontostate-of-the-art
methodsandbaselinesondifferentdatasetsanddemonstrateditssuperiorperformance.
10References
[1] MaruanAl-Shedivat,LiamLi,EricXing,andAmeetTalwalkar. Ondataefficiencyofmeta-
learning. InInternationalConferenceonArtificialIntelligenceandStatistics,pages1369–1377.
PMLR,2021.
[2] AurélienBellet,RachidGuerraoui,MahsaTaziki,andMarcTommasi. Personalizedandprivate
peer-to-peer machine learning. In International Conference on Artificial Intelligence and
Statistics,pages473–481.PMLR,2018.
[3] NivBuchbinder,MoranFeldman,JosephSeffi,andRoySchwartz. Atightlineartime(1/2)-
approximation for unconstrained submodular maximization. SIAM Journal on Computing,
44(5):1384–1402,2015.
[4] SebastianCaldas,SaiMeherKarthikDuddu,PeterWu,TianLi,JakubKonecˇny`,HBrendan
McMahan,VirginiaSmith,andAmeetTalwalkar. Leaf: Abenchmarkforfederatedsettings.
arXivpreprintarXiv:1812.01097,2018.
[5] Hong-YouChenandWei-LunChao. Onbridginggenericandpersonalizedfederatedlearning
forimageclassification. arXivpreprintarXiv:2107.00778,2021.
[6] ZihanChen,HowardHaoYang,TonyQuek,andKaiFongErnestChong.Spectralco-distillation
for personalized federated learning. In Thirty-seventh Conference on Neural Information
ProcessingSystems,2023.
[7] IgorColin,AurélienBellet,JosephSalmon,andStéphanClémençon. Gossipdualaveraging
fordecentralizedoptimizationofpairwisefunctions. InInternationalConferenceonMachine
Learning,pages1388–1396.PMLR,2016.
[8] Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared
representationsforpersonalizedfederatedlearning. InInternationalconferenceonmachine
learning,pages2089–2099.PMLR,2021.
[9] EdwigeCyffersandAurélienBellet. Privacyamplificationbydecentralization. InInternational
ConferenceonArtificialIntelligenceandStatistics,pages5334–5353.PMLR,2022.
[10] Luke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not
imagenetorcifar-10. arXivpreprintarXiv:1810.03505,2018.
[11] Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized
federatedlearning. arXivpreprintarXiv:2003.13461,2020.
[12] John C Duchi, Alekh Agarwal, and Martin J Wainwright. Dual averaging for distributed
optimization: Convergenceanalysisandnetworkscaling. IEEETransactionsonAutomatic
control,57(3):592–606,2011.
[13] Mathieu Even, Laurent Massoulié, and Kévin Scaman. Sample optimality and all-for-all
strategiesinpersonalizedfederatedandcollaborativelearning.arXivpreprintarXiv:2201.13097,
2022.
[14] AlirezaFallah,AryanMokhtari,andAsumanOzdaglar. Personalizedfederatedlearning: A
meta-learningapproach. arXivpreprintarXiv:2002.07948,2020.
[15] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning
withtheoreticalguarantees: Amodel-agnosticmeta-learningapproach. AdvancesinNeural
InformationProcessingSystems,33:3557–3568,2020.
[16] VitalyFeldmanandJanVondrak. Highprobabilitygeneralizationboundsforuniformlystable
algorithmswithnearlyoptimalrate. InConferenceonLearningTheory, pages1270–1279.
PMLR,2019.
[17] FaresFourati,VaneetAggarwal,ChristopherQuinn,andMohamed-SlimAlouini. Random-
izedgreedylearningfornon-monotonestochasticsubmodularmaximizationunderfull-bandit
feedback. InProceedingsofThe26thInternationalConferenceonArtificialIntelligenceand
Statistics,volume206ofProceedingsofMachineLearningResearch,pages7455–7471.PMLR,
25–27Apr2023.
11[18] FaresFourati,SalmaKharrat,VaneetAggarwal,Mohamed-SlimAlouini,andMarcoCanini.
FilFL:Clientfilteringforoptimizedclientparticipationinfederatedlearning. arXivpreprint
arXiv:2302.06599,2023.
[19] Elnur Gasanov, Ahmed Khaled, Samuel Horváth, and Peter Richtarik. Flix: A simple and
communication-efficientalternativetolocalmethodsinfederatedlearning. InGustauCamps-
Valls,FranciscoJ.R.Ruiz,andIsabelValera,editors,ProceedingsofThe25thInternational
ConferenceonArtificialIntelligenceandStatistics,volume151ofProceedingsofMachine
LearningResearch,pages11374–11421.PMLR,28–30Mar2022.
[20] FilipHanzely, SlavomírHanzely, SamuelHorváth, andPeterRichtárik. Lowerboundsand
optimal algorithms for personalized federated learning. Advances in Neural Information
ProcessingSystems,33:2304–2315,2020.
[21] FilipHanzelyandPeterRichtárik. Federatedlearningofamixtureofglobalandlocalmodels.
arXivpreprintarXiv:2002.05516,2020.
[22] YihanJiang,JakubKonecˇny`,KeithRush,andSreeramKannan. Improvingfederatedlearning
personalizationviamodelagnosticmetalearning. arXivpreprintarXiv:1909.12488,2019.
[23] ZhanhongJiang,AdityaBalu,ChinmayHegde,andSoumikSarkar. Collaborativedeeplearning
infixedtopologynetworks. AdvancesinNeuralInformationProcessingSystems,30,2017.
[24] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Ar-
junNitinBhagoji,KallistaBonawitz,ZacharyCharles,GrahamCormode,RachelCummings,
etal. Advancesandopenproblemsinfederatedlearning. FoundationsandTrends®inMachine
Learning,14(1–2):1–210,2021.
[25] MikhailKhodak,Maria-FlorinaFBalcan,andAmeetSTalwalkar. Adaptivegradient-based
meta-learningmethods. AdvancesinNeuralInformationProcessingSystems,32,2019.
[26] AlecKoppel,BrianMSadler,andAlejandroRibeiro. Proximitywithoutconsensusinonline
multiagentoptimization. IEEETransactionsonSignalProcessing,65(12):3062–3077,2017.
[27] AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiplelayersoffeaturesfromtinyimages.
CanadianInstituteforAdvancedResearch,2009.URLhttp://www.cs.toronto.edu/kriz/cifar.html,
2009.
[28] DaliangLiandJunpuWang. Fedmd: Heterogenousfederatedlearningviamodeldistillation.
arXivpreprintarXiv:1910.03581,2019.
[29] Shuangtong Li, Tianyi Zhou, Xinmei Tian, and Dacheng Tao. Learning to collaborate in
decentralizedlearningofpersonalizedmodels. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pages9766–9775,2022.
[30] TianLi,ShengyuanHu,AhmadBeirami,andVirginiaSmith. Ditto: Fairandrobustfederated
learningthroughpersonalization. InInternationalConferenceonMachineLearning,pages
6357–6368.PMLR,2021.
[31] Zexi Li, Jiaxun Lu, Shuang Luo, Didi Zhu, Yunfeng Shao, Yinchuan Li, Zhimeng Zhang,
YonghengWang,andChaoWu. Towardseffectiveclusteredfederatedlearning: Apeer-to-peer
frameworkwithadaptiveneighbormatching. IEEETransactionsonBigData,2022.
[32] XiangruLian,CeZhang,HuanZhang,Cho-JuiHsieh,WeiZhang,andJiLiu.Candecentralized
algorithmsoutperformcentralizedalgorithms? acasestudyfordecentralizedparallelstochastic
gradientdescent. Advancesinneuralinformationprocessingsystems,30,2017.
[33] XiangruLian,WeiZhang,CeZhang,andJiLiu. Asynchronousdecentralizedparallelstochastic
gradientdescent. InInternationalConferenceonMachineLearning,pages3043–3052.PMLR,
2018.
[34] PaulPuLiang,TerranceLiu,LiuZiyin,NicholasBAllen,RandyPAuerbach,DavidBrent,
RuslanSalakhutdinov, andLouis-PhilippeMorency. Thinklocally, actglobally: Federated
learningwithlocalandglobalrepresentations. arXivpreprintarXiv:2001.01523,2020.
12[35] YishayMansour,MehryarMohri,JaeRo,andAnandaTheerthaSuresh. Threeapproachesfor
personalizationwithapplicationstofederatedlearning. arXivpreprintarXiv:2002.10619,2020.
[36] OthmaneMarfoq,GiovanniNeglia,RichardVidal,andLaetitiaKameni. Personalizedfederated
learningthroughlocalmemorization. InInternationalConferenceonMachineLearning,pages
15070–15092.PMLR,2022.
[37] BrendanMcMahan,EiderMoore,DanielRamage,SethHampson,andBlaiseAguerayArcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial
intelligenceandstatistics,pages1273–1282.PMLR,2017.
[38] AngeliaNedic´,AlexOlshevsky,andMichaelGRabbat. Networktopologyandcommunication-
computationtradeoffsindecentralizedoptimization. ProceedingsoftheIEEE,106(5):953–976,
2018.
[39] GiovanniNeglia,GianmarcoCalbi,DonTowsley,andGayaneVardoyan. Theroleofnetwork
topology for distributed machine learning. In IEEE INFOCOM 2019-IEEE Conference on
ComputerCommunications,pages2350–2358.IEEE,2019.
[40] ShaiShalev-Shwartz,OhadShamir,NathanSrebro,andKarthikSridharan. Stochasticconvex
optimization. InCOLT,2009.
[41] AvivShamsian,AvivNavon,EthanFetaya,andGalChechik. Personalizedfederatedlearning
usinghypernetworks. InInternationalConferenceonMachineLearning,pages9489–9502.
PMLR,2021.
[42] VirginiaSmith,Chao-KaiChiang,MaziarSanjabi,andAmeetSTalwalkar.Federatedmulti-task
learning. Advancesinneuralinformationprocessingsystems,30,2017.
[43] CanhTDinh,NguyenTran,andJoshNguyen. Personalizedfederatedlearningwithmoreau
envelopes. AdvancesinNeuralInformationProcessingSystems,33:21394–21405,2020.
[44] Hanlin Tang, Xiangru Lian, Ming Yan, Ce Zhang, and Ji Liu. D2: Decentralized training
overdecentralizeddata. InInternationalConferenceonMachineLearning,pages4848–4856.
PMLR,2018.
[45] Konstantinos I Tsianos, Sean Lawlor, and Michael G Rabbat. Consensus-based distributed
optimization: Practicalissuesandapplicationsinlarge-scalemachinelearning. In201250th
annualallertonconferenceoncommunication,control,andcomputing(allerton),pages1543–
1550.IEEE,2012.
[46] PaulVanhaesebrouck,AurélienBellet,andMarcTommasi. Decentralizedcollaborativelearning
ofpersonalizedmodelsovernetworks. InArtificialIntelligenceandStatistics,pages509–517.
PMLR,2017.
[47] HongyiWang,MikhailYurochkin,YuekaiSun,DimitrisPapailiopoulos,andYasamanKhazaeni.
Federatedlearningwithmatchedaveraging. arXivpreprintarXiv:2002.06440,2020.
[48] Kangkang Wang, Rajiv Mathews, Chloé Kiddon, Hubert Eichner, Françoise Beaufays,
and Daniel Ramage. Federated evaluation of on-device personalization. arXiv preprint
arXiv:1910.10252,2019.
[49] ErminWeiandAsumanOzdaglar. Distributedalternatingdirectionmethodofmultipliers. In
2012IEEE51stIEEEConferenceonDecisionandControl(CDC),pages5445–5450.IEEE,
2012.
[50] RuiYe,ZhenyangNi,FangzhaoWu,SihengChen,andYanfengWang. Personalizedfederated
learningwithinferredcollaborationgraphs. InInternationalConferenceonMachineLearning,
pages39801–39817.PMLR,2023.
[51] MikhailYurochkin, MayankAgarwal, SoumyaGhosh, KristjanGreenewald, NghiaHoang,
andYasamanKhazaeni. Bayesiannonparametricfederatedlearningofneuralnetworks. In
InternationalConferenceonMachineLearning,pages7252–7261.PMLR,2019.
13[52] JianqingZhang,YangHua,HaoWang,TaoSong,ZhenguiXue,RuhuiMa,andHaibingGuan.
Fedala: Adaptivelocalaggregationforpersonalizedfederatedlearning. InProceedingsofthe
AAAIConferenceonArtificialIntelligence,2023.
[53] JiaqiZhangandKeyouYou. Fullyasynchronousdistributedoptimizationwithlinearconver-
genceindirectednetworks. arXivpreprintarXiv:1901.08215,2019.
[54] YueZhao,MengLi,LiangzhenLai,NaveenSuda,DamonCivin,andVikasChandra. Federated
learningwithnon-iiddata. arXivpreprintarXiv:1806.00582,2018.
14A MotivationforOurCombinatorialProposition
Inthissection,wewillshowhowourutilityfunctionbasedongroupsynergybyconsideringthe
combinatorialeffectofclienwtsismoreinterestingthanpairwisecomparisons.
WeconductedanexperimentutilizingtheCIFAR10dataset. Weallocatedthreeclientsasfollows:
Client1hasaccesstofourclasses(0,4,6,8)with50,50,50,and50datapointsperclass,respectively.
Client 2 is assigned four classes (0, 6, 1, 3) with 300, 300, 200, and 200 data points per class,
respectively. Lastly,client3hasaccesstofourclasses(4,8,5,7)with300,300,200,and200data
points per class, respectively. In other words, clients 2 and 3 have two classes in common each
withclient1. WethentrainthreemodelsusingFedAvg-fine-tuned-liketraining(Elocalstepsand
aggregation): onewhereclients1and2collaborate,onewhereclients1and3collaborate,andone
whereallthreeclientscollaborate. Remarkably, ourfindingsindicatethatcollaborationbetween
clients1and2,or1and3alone,leadstoadecreaseinperformancebyapproximately11%inboth
caseswhencomparedtoclient1trainingamodelindependentlyonitsownprivatedatasetalone.
However, the collaborative effort involving clients 1, 2, and 3 collectively enhances accuracy by
approximately6%.
TheeffectivenessofGGCinbuildingthecollaborationgraphcouldalsobeshowninthecaseoftwo
clientseachofthemcontributedpositivelytoclient1,howeverthecollaborationof1,2,3leadsto
negativegain.
Moreover,wewouldliketoclarifythatthesetwocasesaretheextremeoneswherethealgorithms
thatcomparepairwiseperformancefailtoaddress,howeversometimesaddingclient1aloneisgood
and adding client 2 alone is good, but the marginal gain of adding them together is significantly
higher,andthiscouldplayanimportantrolewhenwehavecommunicationconstraintswhereweare
restrictedtocollaborateonlywithasubsetofclientsandchoosingthemwiselyisimportant.
We believe that scenarios, where the synergy between a group of clients outweighs the pairwise
comparison of collaboration effectiveness, are common in real-world settings, particularly when
dealingwithhighlyheterogeneousdata. Insuchcases,aspecificclientmightadverselyaffectthe
trainingofanotherclient. However,whencollaboratingwitha"complementaryclient",thecombined
effortcansignificantlyenhanceperformance.
B GreedyGraphConstruction(GGC)
Algorithm2GreedyGraphConstruction(GGC)
Require: clientk,clientsS
1: X ←{k},Y ←S∪{k},
2: forclientj ∈Shuffle(S)do
(cid:16) (cid:17)
3: R(X)←−F kV (cid:80) i∈1 X(cid:16)pi (cid:80) i∈X p iw i
(cid:17)
4: R(X ∪{j})← (cid:16)−F kV (cid:80) i∈X1 ∪{j}pi (cid:80) (cid:17)i∈X∪{j}p iw i
5: R(Y)←−F kV (cid:80) i∈1 Y(cid:16)pi (cid:80) i∈Yp iw i
(cid:17)
6: R(Y \{j})←−F kV (cid:80) i∈Y1 \{k}pi (cid:80) i∈Y\{j}p iw i
7: a←max(R(X ∪{j})−R(X),0)
8: b←max(R(Y \{j})−R(Y),0)
9: withprobabilityp= a do
a+b
10: X ←X ∪{j}andY ←Y
11: else
12: Y ←Y \{j}andX ←X
13: endfor
14: if|X|=B cthen
15: break
16: endif
17: ReturnX
15Additional explanation: We would like to note that as our objective function is not monotone,
meaning that adding won’t always be the best choice, we necessitate the computation of both
marginalgainsofaddingandremovingaclient. Tothisend,fourcaseswillappear,threeofthem
willhappenordonothappenwithprobability1.
• Ifthemarginalgainofaddingispositive(aispositive)andthemarginalgainofremovingis
negative(biszero)thenp=1.
• Ifthemarginalgainofaddingisnegative(aiszero)andthemarginalgainofremovingis
positive(bispositive)thenp=0(doesnothappenwithprobability1).
• Ifthemarginalgainofaddingisnegative(aiszero)andthemarginalgainofremovingis
negative(biszero)thenp=1.
• Ifthemarginalgainofaddingispositive(aispositive)andthemarginalgainofremovingis
positive(bispositive)onlyinthiscaseweflipabiasedcointhatdecidesprobabilitypbased
onthevaluesofaandbforeitheraddingorremovingthatclient. Inourexperiments,we
noticedthatthiscasehappenslessthan1%ofthetime.
Furthermore,ithasbeenshownin[3]thatifyouremovetherandomnessintheagnosticcasei.e.,
whenthemarginalgainsofaddingandremovingarebothpositive(aandbarepositive)thenthe
algorithm guarantees decreases from achieving 1 of the optimal solution to the 1 of the optimal
2 3
solution.
C EfficientRewardComputation
LookingatAlgorithm2,wenoticethatitrequiresrewardcomputationstothefunctionR(S),which
isdefinedinourobjectiveaccordingtoEq.(7). Thisrewardcomputationrequiresaccesstow inS,
i
where|S|mayexceedourbudgetconstraintB (seeEq.(8)). Therefore,toensurethatwedonot
c
violateourconstraintsduringthepreprocessingstepoutlinedinAlgorithm1,weproposeamortizing
thecommunicationcomplexityneededtoevaluatetheobjectivefunction. Thisinvolvesdividing
the necessary downloads into O( n ) steps. Each step entails only O(B ) computation, storage,
Bc c
andcommunicationcomplexity,asopposedtoasinglestepwithO(n)complexity,whichbreaches
thebudgetconstraint. Toelaborate, duringeachcommunicationstep, aclientk receivesatmost
B modelupdatesandmonitorsthenecessaryaveragesforalgorithmicdecision-making,without
c
retainingallindividualmodels.
Toexecutethis,theprocesscommencesbypreparingthecallforvariableb,requiringtheaverage
of all models in Y, beginning with a full set of clients. We envisage ⌈ n ⌉ communication steps
Bc
(line 2), where in each step, B clients transmit their model updates (line 3). We compute the
c
average,asdepictedinlines4-6ofAlgorithm3. Inline5,wY denotesstoringtheweighted-sumof
receivedmodelsinbatches. Uponcompletingthesesteps,wY iscomputed. Thesecondphaseof
communicationsinitiates. ClientkbeginsreceivingbatchesofmodelswithoutreplacementfromB
c
clients,storingtheirindicesinS . SimilartoGGC,clientkcomputesthemarginalgainsofadding
b
andremovingaclientfromS bycomputingaandb,respectively. Adecisionisthenmadebasedon
b
probabilitypinline16. AcrucialstepistokeeptrackofthevaluesofwX andwY andupdatethem
asinlines17and19,forfutureuseevenforthenextreceivedbatchofmodelsB .
c
It’s important to note that this expanded window of communications is solely required for the
preprocessing step; for graph verification at line 10 of Algorithm 1, the algorithm takes as input
|Ω |≤B .
k c
16Algorithm3BatchedGreedyGraphConstruction(BGGC)
Require: clientk,clientsS,budgetB .
c
1: wY ←p kw k
2: forsinrange(⌈ n ⌉)do
Bc
3: ClientkreceivesabatchBofatmostB cmodelswithoutreplacement
4: formodelw s ∈Bdo
5: wY ←wY +p s∗w s
6: endfor
7: endfor
8: X ←{k},Y ←S∪{k},wX ←p kwk,wY ←wY,
9: S ←Shuffle(S)
10: forsinrange(⌈ n ⌉)do
Bc
11: ClientkreceivesabatchBofatmostB cmodelsinorderfromS
withoutreplacementandindicesinS
b
12: forjinS bdo
(cid:16) (cid:17)
1 1 13 4 5: :
:
R R R( ( (X X Y) \∪← {{ jj }− } ))F ←←kV (cid:16)−−(cid:80) FF kViw kV∈X X (cid:16)(cid:16) (p ( −i p (cid:17)pw wj j+X Y +(cid:80) −+ (cid:80)i pp ∈ ijj ∈X ww Yjj p pi i) )(cid:17) (cid:17)
16: R(Y)←−F kV (cid:80) iw ∈Y Ypi
17: a←max(R(X ∪{j})−R(X),0) {marginalgainofaddingaclient}
18: b←max(R(Y \{j})−R(Y),0) {marginalgainofremovingaclient}
19: withprobabilityp= a do
a+b
20: X ←X ∪{j}; Y ←Y ; wX ←wX +p jwj
21: else
22: Y ←Y \{j} ; X ←X ; wY ←wY −p jwj
23: if|X|=B cthen
24: break
25: endif
26: endfor
27: endfor
28: ReturnX
D ProofofTheorem1
Bothalgorithms,GGCandBGGC,foragivenclientk,foragivensetofclientsS,aimtofindasetof
(cid:16) (cid:17)
collaboratorsX ⊆S∪kthatmaximizestherewardfunctionR(X)≜−FV 1 (cid:80) p w .
k (cid:80) i∈Xpi i∈X i i
BothalgorithmsusethemarginalgainsaandbcomputedfromtherewardfunctionR(·)andadda
clientwiththesameprobabilityfunctionp= a . BothBGGCandGGCinitializeX as{k}andY
a+b
asS∪{k}. BothalgorithmsgothroughthepotentialcollaboratorsinS anddecidetoaddorremove
sequentially.
AssumingbothGGCandBGGCapplysomeseededsortingfunctiontosetS andfollowtheshuffled
order,toshowthatbothmethodsyieldexactlythesameoutputwithseededrandomness,itonlyneeds
tobeverifiedthatthecomputedpisthesameforGGCandBGGCforeachclient. Thisrequires
showingthattherewardcomputationyieldsexactlythesameoutcome.
In the following, we show that GGC and BGGC compute the same reward function of the four
consideredsetsineverydecisionround,whichareX,X ∪{k},Y andY \{k},leadingtothesame
valuefortheprobabilityfunctionp,hencethesamedecision.
RewardofY computation
17GGCcomputesR(Y)asfollows:
(cid:32) (cid:33)
1 (cid:88)
R(Y)=−FV p w , (9)
k (cid:80) p i i
i∈Y i i∈Y
whichimplicitlyassumeshavingaccesstoalltheweightsofclientsinY,possiblyrequiringcommu-
nicationwithalltheseclientsaswellasthestorageoftheirmodels.
Incontrast,BGGC,doesnotassumethepossibilityofcommunicatingorstoringmoremodelsthan
theexpectedbudget,henceinitsfirstloop(lines2-6)iteratesthroughbatchesofclients,summing
their weighted models into wY. This effectively pre-computes the sum for the entire client set
Y =S∪{k}:
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
wY =p w + p w =p w + p w = p w = p w , (10)
k k j j k k j j j j j j
s∈⌈ Bn c⌉j∈Bs j∈S j∈S∪{k} j∈Y
whereB denotesabatchofclientsreceivedinthes-thiteration. Thisweightedsummationclearly
s
endsupsummingalltheweightsofalltheclientsinS.
ThissummationisadaptivetothechangeinY,asshowninline23,wherewheneveraclientj is
removed,itsweightedweights(p wj)areremovedfromtheweightedsum(wY −p wj). Therefore,
j j
wY alwaysrepresentstheweightedsumofthemodel’sweightsinthesetY. Hence,ineverydecision
step,itisalwaysthecasethat:
(cid:88)
wY = p w . (11)
j j
j∈Y
BGGCcomputesR(Y),inline17,asfollows:
(cid:18) wY (cid:19)
R(Y)=−FV .
k (cid:80) p
i∈Y i
ReplacingbywY recoversthesamerewardcomputationofGGC.
RewardofY \{k}computation
GGCcomputesR(Y \{j})asfollows:
 
1 (cid:88)
R(Y \{j})=−F kV (cid:80)
p
p iw i,
i∈Y\{j} i i∈Y\{j}
whichimplicitlyassumeshavingaccesstoalltheweightsofclientsinY.
BGGCcomputesR(Y),inline16,asfollows:
(cid:18) wY −p wj (cid:19) (cid:32) wY −p wj(cid:33)
R(Y \{j})=−FV j =−FV j .
k −p +(cid:80) p k (cid:80) p
j i∈Y i i∈Y\{j} i
ReplacingbywY,usingEq.(11)recoversthesamerewardcomputationofGGC.
RewardofX computation
GGCcomputesR(X)asfollows:
(cid:32) (cid:33)
1 (cid:88)
R(X)=−FV p w .
k (cid:80) p i i
i∈X i i∈X
BGGCinitializesX ={k}inthesamewayasGGC.Moreover,initializedwX ←p wk. Therefore,
k
inthefirstiteration,wX representsexactlytheweightedweightofclientk,representingthesumof
thatsingleelement.
18ThissummationisadaptivetothechangeinX,asshowninline21,wherewheneveraclientj is
added,itsweightedweights(p wj)areaddedtotheweightedsum(wX +p wj). Therefore,wX
j j
alwaysrepresentstheweightedsumofthemodel’sweightsinthesetX. Hence,ineverydecision
step,itisalwaysthecasethat:
(cid:88)
wX = p w . (12)
j j
j∈X
BGGCcomputesR(Y),inline14,asfollows:
(cid:18) wX (cid:19)
R(X)=−FV .
k (cid:80) p
i∈X i
ReplacingbywX recoversthesamerewardcomputationofGGC.
RewardofX ∪{k}computation
GGCcomputesR(X ∪{j})asfollows:
 
1 (cid:88)
R(X ∪{j})=−F kV (cid:80)
p
p iw i
i∈X∪{j} i i∈X∪{j}
BGGCcomputesR(Y),inline15,asfollows:
(cid:18) wX +p wj (cid:19) (cid:32) wX +p wj (cid:33)
R(X ∪{j})=−FV j =−FV j
k p +(cid:80) p k (cid:80) p
j i∈X i i∈X∪{j} i
ReplacingbywX usingEq.(12)recoversthesamerewardcomputationofGGC.
E ProofofProposition1
AssumethereexistsacollaborationgraphC ,whichisoptimalwithinarestrictingsubsetP,i.e.,
c
(cid:40) N (cid:41)
(cid:88)
C ∈ argmin F(w,C)≜ p F (w ,C )
c k k k k
w={wi∈Rd}N
i=1 k=1
C={Ci∈2[N]\{i};|Ci|≤Bc}N
i=1
C∈P
SolvingEq.(3)yieldsacollaborationgraphC⋆,where
(cid:40) N (cid:41)
(cid:88)
C⋆ ∈ argmin F(w,C)≜ p F (w ,C )
k k k k
w={wi∈Rd}N
i=1 k=1
C={Ci∈2[N]\{i};|Ci|≤Bc}N i=1,
Therefore,itfollowsthat
(cid:40) N (cid:41)
(cid:88)
min F(w,C⋆)= min F(w,C)≜ p F (w ,C )
k k k k
w={wi∈Rd}N
i=1
w={wi∈Rd}N
i=1 k=1
C={Ci∈2[N]\{i};|Ci|≤Bc}N i=1,
(cid:40) N (cid:41)
(cid:88)
≤ min F(w,C)≜ p F (w ,C )
k k k k
w={wi∈Rd}N
i=1 k=1
C={Ci∈2[N]\{i};|Ci|≤Bc}N
i=1
C∈P
= min F(w,C ),
c
w={wi∈Rd}N
i=1
whichconcludestheproof.
19F ExperimentalDetails
AlltheexperimentsreportedinTables2,and1representtheaverageofthreedifferentrepetitions
acrossthreedifferentseeds. FortheexperimentinTable3,wereporttheresultsforseedequalsto42.
Inallexperiments,wepreservethebestmodelbasedonthevalidationdataset,andthereportedtest
resultsareobtainedbyperforminginferenceonthisbestvalidationmodel.
F.1 Datasets
Inourexperiments,weutilizevariousdatasetsfollowingexistingliteratureonpersonalizedFL,as
highlightedin[30,8,36]. WeconductexperimentswithCIFAR10[27],FederatedExtendedMNIST
(FEMNIST)[4],andtheCINIC10dataset[10].
F.2 Dataheterogeneity
Weexplorevaryingdegreesofdataheterogeneity,particularlyemployingtwodistinctdistribution
strategies for the CIFAR10 and CINIC10 datasets. The first approach involves a pathological
distribution split [52, 37, 7], wherein each client exclusively receives data from three specified
categories. The second approach utilizes the Dirichlet distribution [51, 47], where a distribution
vector q is drawn from Dir (α) for each category c. Subsequently, the proportion q of data
c k c,i
samplesfromcategorycisallocatedtoclienti. Moreover,fortheFEMNISTdataset,weconsiderthe
naturalNon-IIDsplitprovidedintheLeafframework[4]whereeachwritercorrespondstoaclient
andweaddmoredegreesofheterogeneitybyhavingeachclientmissingsomeclasses.
F.3 CIFAR10benchmark
F.3.1 Distribution
CIFAR10isavisiondatasetcomprising50,000trainingimagesand10,000testingimages. Wesplit
thetrainingdatainto20%ofthevalidationdatasetand80%ofthetrainingdataset. Furthermore,
we split the testing data among clients in such a way the local test data follows the distribution
ofthetrainingdata. Duringthetraining, wesavethebestlocalmodelsonthevalidationdataset,
andwemakeinferencesafterwardusingthelocaltestdataandthebestsavedmodel. Tosimulate
real-worldheterogeneity,weconsidertwotypesofdataheterogeneity. Thefirstapproachinvolvesa
pathologicaldistributionsplit[52,37,7],whereineachclientexclusivelyreceivesdatafromthree
specifiedcategories. InthecaseoftheCIFAR10dataset,weusePatho(3)todenotethateachclient
hasaccesstoonlythreeclassesoutoften. ThesecondapproachutilizestheDirichletdistribution
[51,47],whereadistributionvectorq isdrawnfromDir (α)foreachcategoryc. Subsequently,
c k
theproportionq ofdatasamplesfromcategorycisallocatedtoclienti. InCIFAR10datasetwe
c,i
useDir(0.1).
F.3.2 Model
TheemployedmodelisasimpleCNNnetworkcomprisingthreeconvolutionallayersandtwofully
connectedlayers. Thefirstconvolutionallayerhasthreeinputchannels,sixoutputchannels,and
akernelsizeof5. TheReLUactivationfunctionanda2DMaxpoolLayerwithakernelsizeof2
followit. Thesecondconvolutionallayertransformsaninputofsixchannelstosixteenchannelswith
akernelsizeof5,followedbytheReLUactivationfunction. Thefirstfullyconnectedlayertakesan
inputofsize400andproducesanoutputofsize120. Thesecondlayerproducesanoutputofsize84,
andthelastlayerhasasizeequaltothenumberofclasses,whichis10.
F.3.3 Hyperparameters
Forthepreprocessingstepforeachclient,wetrain10localepochs(τ =10). Duringthetraining
init
thenumberoflocalepochsτ =5,thenumberofroundsT =100,howeverasthepreprocessing
train
step corresponds to 2 rounds of training, for fairness with other methods we use only 98 rounds
insteadof100forourmethod. Thenumberofclients|S |=100. Thelearningrateη =0.01. For
t
thetraining,weuseSGDoptimizerwith1e−3decay,0.9momentum,andbatchsizeof16.
20F.4 FEMNISTbenshmark
F.4.1 Distribution
WeutilizetheFEMNISTdatasetwithintheLEAFframework[4]. Thisdatasetconsistsoftraining
andtestingsetsaccompaniedbyaclient-datamappingfilethatpartitionsthedatainanon-IID(non-
identicallydistributed)manneramongtheclients. Thedatasetexhibitsinherentheterogeneitydueto
variationsinthewritingstylesofindividualcontributors. WefirstdownloadedthefullFEMNISTdata
from[4]. Additionally,weemployedafileobtainedfrom[30]tofurtherincreasetheheterogeneityin
thedataset. Thedatafileswillbeprovidedalongwithourcode.
F.4.2 Model
TheemployedmodelisasimpleCNNnetworkcomprisingthreeconvolutionallayersandtwofully
connectedlayers. Thefirstconvolutionallayerhasoneinputchannel,4outputchannels,andakernel
sizeof5. ItisfollowedbytheReLUactivationfunctionanda2DMaxpoolLayerwithakernelsize
of2. Thesecondconvolutionallayertransformsaninputof4channelsto12channelswithakernel
sizeof5,followedbytheReLUactivationfunction. Thefirstfullyconnectedlayertakesaninputof
size192andproducesanoutputofsize120. Thesecondlayerproducesanoutputofsize100,and
thelastlayerhasasizeequaltothenumberofclasses,whichis10.
F.4.3 Hyperparameters
Forthepreprocessingstepforeachclient,wetrain4localepochs(τ =4). Duringthetrainingthe
init
numberoflocalepochsτ =2,thenumberofroundsT =50,howeverasthepreprocessingstep
train
correspondsto2roundsoftraining,forfairnesswithothermethodsweuseonly48roundsinsteadof
50forourmethod. Thenumberofclients|S |=100. Thelearningrateη =0.001. Forthetraining,
t
weuseSGDoptimizerwith1e−3decay,0.9momentum,andbatchsizeof10.
F.5 CINIC10benshmark
F.5.1 Distribution
CINIC10servesasanextensiontoCIFAR10,encompassing90,000trainingimages,90,000validation
images,and90,000testimages. Initially,thetrainingandvalidationsetsarecombined,andthedata
isthendistributedamongclientsinaheterogeneousmanner. Subsequently,20%ofeachpartition
isallocatedforvalidation,withtheremainingportiondesignatedfortraining. SimilartoCIFAR10,
wesplitthetestdataamongclientsinsuchawaythatitfollowsthetraindistributionforeachclient.
ThetrainingdataisdistributedeitherfollowingaDirichletdistributionDir(0.1)orthepathological
distributionPatho(3),whereeachclienthasaccesstoonlythreeclassesamongthetenavailable.
Afterassigningtoeachclientwhichclassesitwillget,wedistributetheclassdatapointsamong
clientsthatsharethesameclass,followingaDir(0.5)distribution. Thisadditionalstepaddsmore
degreesofheterogeneityandsimulatesareal-worldscenariomorerealistically.
F.5.2 Model
ThesamemodelusedinCIFAR10isemployed.
F.5.3 Hyperparameters
Forthepreprocessingstepforeachclient,wetrain10localepochs(τ =10). Duringthetraining
init
thenumberoflocalepochsτ =5,thenumberofroundsT =50,howeverasthepreprocessing
train
step corresponds to 2 rounds of training, for fairness with other methods we use only 48 rounds
insteadof50forourmethod. Thenumberofclients|S |=200. Thelearningrateη =0.01. Forthe
t
training,weuseSGDoptimizerwith1e−3decay,0.9momentum,andbatchsizeof16.
F.6 Baselines
Wecomparedourmethodagainstelevenbaselines:
21• Local: localtrainingforT roundsandwereporttheaveragelocaltestaccuraciesofthe
clientsineveryround.
• FedAvg[37]
• FedAvg+FT:whichisfedavgfine-tuningversion,wherewesavebestmodelsonvalidation
datasetwhentrainingFedAvg,thenstartingfromthatmodelweperform2∗τ localepochs
andreporttheaveragetestaccuracies.
• FedProx [33]: For FedProx we use λ = 0.1 for all datastes (CIFAR10, CINIC10 and
FEMNIST)
• FedProx+FT[33]: ItisFedProxfine-tuningversion,wherewesavethebestmodelsonthe
validationdatasetwhentrainingFedProx,thenstartingfromthatmodelweperform2∗τ
localepochsandreporttheaveragetestaccuracies.
• APFL[42]:Wechosethehyperparameterwhichtheycallτ intheirpapertobeτ =1which
meanswesynchronisethemodelseveryround.
• PerFedAvg[14]: Weusethehyperparameterα = 0.01anditisfixedforallrunsandall
datasets
• Ditto[30]:Wesetthehyperparameterλthatrepresentsthetradeoffbetweenlocalandglobal
modelsto0.75,andkeepitfixedforalldatasets.
• FedRep[8]
• kNN-Per[36]: wesetthehyperparameterk nnto10andtheinterpolationhyperparameter
k
to0.5
• pFedGraph[50]
F.7 Computeresources
WeuseaninternalSLURMclusterforrunningourexperiments. Theexperimentsweredoneonan
ASUSESCN4A-E11server. Thenodehas4A100GPUs,anAMDEPYC7003series64core@
3.5GHzCPUand512GBofRAM.WeusedoneA100,with2cores,andrequiredatmost100GBof
memoryfortheexperiments.
G AdditionalExperiments
G.1 Variancebetweenaccuraciesoflocalmodels
Asourobjectiveispersonalization,webelievethat,apartfromimprovingtheoverallaverageaccuracy
amongclients,itiscrucialtoassesswhetherimprovementsaredistributedacrossmostmodelsrather
thanbeingconfinedtojustafewclients. Toevaluatethis,weexaminethevariancebetweenlocal
models. Lower variance indicates that the accuracies of local models are closer. Therefore, we
considerbothaccuracyandvarianceasimportantmetrics. InFigures1,5and6,thex-axisrepresents
theaveragetestaccuracy,whilethey-axisrepresentsthevariancebetweenclients’models. Forbetter
visualization,inFig.1wechoosetoreportonlymethodsthattheaveragetestaccuracyishigherthan
80%and78%forDir(0.1)andPatho(3)respectively,whileforFig.6weshowthemethodshaving
higherthan78%and77%forDir(0.1)andPatho(3)respectively. Furthermore,inFig.5wewere
abletoshowallmethods. Allfiguresshowthatourmethod(DPFL)issituatedintheright-bottom
corneracrossallvariantsofbudgetconstraints. Thispositioningsignifiesthat,comparedtoother
methods,ourapproachachievessuperioraveragetestaccuracyandlowervariancebetweenlocal
models.
G.2 Visualizationofthecollaborationgraph
WepresentFigures8,9,10and11,showingourinitialcollaborationgraphonthetopleftandthe
clientsconsideredforcollaborationevery10roundfrom0to80forcaseswithoutbudgetconstraint,
withB = 20, B = 10, andwithB = 5respectively. Thediagonalindicatesthateveryclient
c c c
always“collaborates”withitself. Toillustratethegraphevolution,theplotsdisplaycollaborative
linksintwocolors: inpink,aretheclientsthatareselectedforcollaborationinthatround;inyellow,
aretheclientsthatwereidentifiedduringthepreprocessingstepbutarecurrentlynotchosen. The
22Local Only
70
pFedGraph
60
PerKNN
Ditto
50 FedRep APFL Better
FedAvg
FedProx FedAvg+FT
40
30 PerFedAvg
DPFL (20)
20 FedProx+F DT PFD LP (F 5L ) (10)
DPFL (inf)
87 88 89 90 91 92 93 94
AverageTestAccuracy
Figure5: ComparisonofourmethodwithotherpersonalizedmethodsontheFEMNISTdatasetin
termsofvariancebetweenlocalmodels.
230 APFL Local Only
220 95
pFedGraph
210 Better
Local Only 90 FedAvg+FT Better
pFedGraph FedRepPerFedAvg
200 PerFedAvg APFL
190 FedRep FedAvg+FT 85
PerKNN Ditto
PerKNN
180 Ditto DPD FP LF (L 5 ( )10)
DPFL (5)DPFL (10) 80 DPFL (inf)
170 DPFL (20) DPFL (20)
DPFL (inf)
78.5 79.0 79.5 80.0 80.5 81.0 81.5 77.5 78.0 78.5 79.0 79.5 80.0 80.5
AverageTestAccuracy AverageTestAccuracy
Figure6: ComparisonofourmethodwithotherpersonalizedmethodsontheCINIC10datasetin
termsofvariancebetweenlocalmodels.
unionofpinkandyellowclientscorrespondstotheinitialcollaborationgraph. Thefigureshighlight
thattheinitialcollaborationgraphisdensercomparedtotheactuallyusedclientsforaggregation
inlaterrounds. Thisisexpectedsincethegraphisconstructedasapreprocessingstep,andatthis
stage,modelweightshavenotyetconverged. Therefore,broadercollaborationcanbebeneficial.
However,astrainingprogresses,itisnaturaltoexpectthateachclientwillbenefitprimarilyfrom
collaboratingwithclientshavingsimilardatadistributions,thusleadingtoasparsercollaboration
graph. Anothercontributingfactoristhat,inthepreprocessingstep,thedecisiontoselectaspecific
client for collaboration is made from a pool of 100 clients, making it more challenging than in
laterroundswherethedecisionisdrawnfromasmallerpooldenotedasΩ forclientk. Another
k
observationisthatinallcases,fromround60onward,thegraphremainsalmostunchanged. Thisis
attributedtotheweightsnearingconvergence,resultinginalessrandomselectionofcollaborators.
Thediversityofthegraphatearlyroundsprovesthatweshouldn’tremoveanedgebetweentwo
nodesifthatnodehasn’tbeenselected,asthatdecisioncouldchangeinlaterrounds.
23
ecaniraV260 Local Only PerFedAvg 115 pFedGraph
110
Better
240
FedRep Better 105 FedRep APFL
220 pFedGraph 100 PerFedAvg
Local Only Ditto
FedAvg+FT
PerKNN 95
DPFL (5) PerKNN DPFL (20)
200 APFL 90 DPFL (5)
Ditto DPFL (20)
FedAvg+FT
DPFL (inf) 85
180
DPFL (10) DPFL (10)
80 DPFL (inf)
80 81 82 83 84 79 80 81 82 83
TestAccuracy TestAccuracy
Figure 7: Variance between local models using Dir(0.1) (left) and Patho(3) (right) data splits on
CIFAR10.
G.3 Behaviorofthecollaborationgraphundertwogroupsofclients
Tobettervisualizeandhavemoreexplainabilityofthebehaviorofouralgorithm,werananexperiment
usingCIFAR10datasetand100clients,where40amongthemhadtheirlabelflippedusingthesame
permutationand60hadtheirtruelabels. It’simportanttonotethatourobjectivewasn’ttocreatean
attack;rather,weaimedtodelineatethebehaviorofouralgorithm,acknowledgingthatwhileitmay
exhibitrobustnesscharacteristics,thestudyofrobustnessfallsoutsidethescopeofthispaper. Inthis
experiment,weeffectivelycreatedtwodistinctgroupsofclients. Thefirstexperiment,illustratedin
Fig.4a,involvedflippedclientswhodidnotexecutethegreedyalgorithm(Algorithm2).Instead,they
consistentlymaintainedtheirmodels(astrongerattackstrategy).Asexpected,thecollaborationgraph
displayednumerousedgeswiththeseflippedclientsinitially. Thisoutcomealignswiththeinherent
randomnessintheweightsduringthepreprocessingstep,makingitchallengingtoselectbetween
clients. However,astheroundsprogressed,weobservedanotabletrend: blackclientsincreasingly
avoidedselectingtheredonesuntiltheyultimatelyceasedchoosingthemaltogether. Thisevolutionis
depictedinFig.12,wherethecompletegraphevolutionisvisualizedevery10rounds. Inthesecond
experiment(Fig.4b),eventheredclientsexecutedthegreedyalgorithm,resultingintheirselection
fromtheblackclientsinitially. Consequently,theirmodelsbecameregularizedtowardstheblack
ones. Despitethisbehaviorinthecollaborationgraph,weobservedthatastheroundsprogressed,
theclientswerealmostsegregatedintotwosubgroups(redandblack),withveryfewlinksbetween
themservingasaformofregularization(fullgraphisinFig.13). Wevisualizedthepercentageof
connectionwithmaliciousclientsforabenignclient,aspresentedinFig.14,andfoundittobevery
small.
24
ecnairaVCollaboration graph Collaborators at Round 10 Collaborators at Round 20
0 0 0
20 20 20
40 40 40
60 60 60
80 80 80
0 20 40 60 80 0 20 40 60 80 0 20 40 60 80
Collaborators at Round 30 Collaborators at Round 40 Collaborators at Round 50
0 0 0
20 20 20
40 40 40
60 60 60
80 80 80
0 20 40 60 80 0 20 40 60 80 0 20 40 60 80
Collaborators at Round 60 Collaborators at Round 70 Collaborators at Round 80
0 0 0
20 20 20
40 40 40
60 60 60
80 80 80
0 20 40 60 80 0 20 40 60 80 0 20 40 60 80
Figure8: OurcollaborationgraphwithoutconstraintonCIFAR10datasetwith100clients
25Collaboration graph Collaborators at Round 10 Collaborators at Round 20
0 0 0
20 20 20
40 40 40
60 60 60
80 80 80
0 20 40 60 80 0 20 40 60 80 0 20 40 60 80
Collaborators at Round 30 Collaborators at Round 40 Collaborators at Round 50
0 0 0
20 20 20
40 40 40
60 60 60
80 80 80
0 20 40 60 80 0 20 40 60 80 0 20 40 60 80
Collaborators at Round 60 Collaborators at Round 70 Collaborators at Round 80
0 0 0
20 20 20
40 40 40
60 60 60
80 80 80
0 20 40 60 80 0 20 40 60 80 0 20 40 60 80
Figure9: OurcollaborationgraphwithconstraintB =20onCIFAR10datasetwith100clients.
c
26Collaboration graph Collaborators at Round 10 Collaborators at Round 20
0 0 0
20 20 20
40 40 40
60 60 60
80 80 80
0 20 40 60 80 0 20 40 60 80 0 20 40 60 80
Collaborators at Round 30 Collaborators at Round 40 Collaborators at Round 50
0 0 0
20 20 20
40 40 40
60 60 60
80 80 80
0 20 40 60 80 0 20 40 60 80 0 20 40 60 80
Collaborators at Round 60 Collaborators at Round 70 Collaborators at Round 80
0 0 0
20 20 20
40 40 40
60 60 60
80 80 80
0 20 40 60 80 0 20 40 60 80 0 20 40 60 80
Figure10: OurcollaborationgraphwithconstraintB =10onCIFAR10datasetwith100clients.
c
27Collaboration graph Collaborators at Round 10 Collaborators at Round 20
0 0 0
20 20 20
40 40 40
60 60 60
80 80 80
0 20 40 60 80 0 20 40 60 80 0 20 40 60 80
Collaborators at Round 30 Collaborators at Round 40 Collaborators at Round 50
0 0 0
20 20 20
40 40 40
60 60 60
80 80 80
0 20 40 60 80 0 20 40 60 80 0 20 40 60 80
Collaborators at Round 60 Collaborators at Round 70 Collaborators at Round 80
0 0 0
20 20 20
40 40 40
60 60 60
80 80 80
0 20 40 60 80 0 20 40 60 80 0 20 40 60 80
Figure11: OurcollaborationgraphwithconstraintB =5onCIFAR10datasetwith100clients.
c
28Benign Malicious Benign Connection Malicious Connection
Collaboration Graph With Label Flip Collaborators at Round 10 Collaborators at Round 20
Collaborators at Round 30 Collaborators at Round 40 Collaborators at Round 50
Collaborators at Round 60 Collaborators at Round 70 Collaborators at Round 80
Collaborators at Round 90 Collaborators at Round 95 Collaborators at Round 98
Figure12: OurcollaborationgraphwithoutGreedy. 40%ofclientshaveflippedlabels(Malicious),
while60%haveoriginallabels(Benign). Maliciousclientsdon’texecuteAlgorithm2;instead,they
consistentlysendtheirlocalmodeltoBenignclients.
29Benign Malicious Benign Connection Malicious Connection
Collaboration Graph With Label Flip Collaborators at Round 10 Collaborators at Round 20
Collaborators at Round 30 Collaborators at Round 40 Collaborators at Round 50
Collaborators at Round 60 Collaborators at Round 70 Collaborators at Round 80
Collaborators at Round 90 Collaborators at Round 95 Collaborators at Round 98
Figure13: OurcollaborationgraphwithGreedy. 40%ofclientshaveflippedlabels(Malicious),
while60%haveoriginallabels(Benign). MaliciousexecuteAlgorithm2.
301.0
ratio of malicious
ratio of benign
0.8
0.6
0.4
0.2
0.0
0 20 40 60 80 100
Figure14: Visualizationoftheratioofbenignclientscollaboratingwithmaliciousvswithbenign,in
thecasethatmaliciousrunsthegreedyalgorithm
31Collaboration Graph c=inf Collaboration Graph c=20
0 0
20 20
40 40
60 60
80 80
0 20 40 60 80 0 20 40 60 80
Collaboration Graph c=10 Collaboration Graph c=5
0 0
20 20
40 40
60 60
80 80
0 20 40 60 80 0 20 40 60 80
Figure15: OurcollaborationgraphonCIFAR10datasetwith100clients
G.4 MeasuringAsymmetryofthecollaborationgraph
FortheexperimentsconductedusingtheCIFAR10dataset,weanalyzedthepercentageofasymmetry
in the graph across different rounds for three budget settings B = inf, B = 20 and B = 10
c c c
respectively.Weobservedthattheasymmetrypercentageisconsistentlyhigherinround0compared
to subsequent rounds. This aligns with the expectation that the greedy decision-making process
duringpreprocessingexhibitsmorerandomnessduetotwofactors,first,themodelweightsstilldidn’t
captureenoughofthedatastructure,andsecondasthedecisionofchoosingorremovingaclient
ismadefrombiggerpool,itmakesithardertodecide. Additionally,acrossallbudgetsettings,the
asymmetrypercentageappearstostabilizearoundaspecificvalue,exhibitingfluctuations,starting
fromround40onwards.
32