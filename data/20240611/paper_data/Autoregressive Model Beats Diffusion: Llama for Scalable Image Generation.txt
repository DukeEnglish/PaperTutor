Autoregressive Model Beats Diffusion: Llama for
Scalable Image Generation
PeizeSun1 YiJiang2† ShoufaChen1 ShilongZhang1 BingyuePeng2
PingLuo1∗ ZehuanYuan2∗
1TheUniversityofHongKong 2ByteDance
Codesandmodels: https://github.com/FoundationVision/LlamaGen
Figure 1: Image generation with vanilla autoregressive models. We show samples from our
class-conditionalimage(toprow)andtext-conditionalimage(bottomrow)generationmodels.
Abstract
WeintroduceLlamaGen,anewfamilyofimagegenerationmodelsthatapplyorigi-
nal“next-tokenprediction”paradigmoflargelanguagemodelstovisualgeneration
domain. Itisanaffirmativeanswertowhethervanillaautoregressivemodels,e.g.,
Llama,withoutinductivebiasesonvisualsignalscanachievestate-of-the-artimage
generationperformanceifscalingproperly. Wereexaminedesignspacesofimage
tokenizers,scalabilitypropertiesofimagegenerationmodels,andtheirtraining
dataquality. Theoutcomeofthisexplorationconsistsof: (1)Animagetokenizer
withdownsampleratioof16,reconstructionqualityof0.94rFIDandcodebook
usageof97%onImageNetbenchmark. (2)Aseriesofclass-conditionalimage
generationmodelsrangingfrom111Mto3.1Bparameters,achieving2.18FIDon
ImageNet256×256benchmarks,outperformingthepopulardiffusionmodelssuch
asLDM,DiT.(3)Atext-conditionalimagegenerationmodelwith775Mparame-
ters,fromtwo-stagetrainingonLAION-COCOandhighaestheticsqualityimages,
demonstratingcompetitiveperformanceofvisualqualityandtextalignment. (4)
WeverifytheeffectivenessofLLMservingframeworksinoptimizingtheinference
speedofimagegenerationmodelsandachieve326%-414%speedup. Werelease
allmodelsandcodestofacilitateopen-sourcecommunityofvisualgenerationand
multimodalfoundationmodels.
∗:Correspondingauthors,†:projectlead
4202
nuJ
01
]VC.sc[
1v52560.6042:viXra1 Introduction
Builtuponautoregressivemodels,largelanguagemodels(LLMs)[Vaswanietal.2017;Devlinetal.
2018;Radfordetal.2018;Raffeletal.2020;Radfordetal.2019;Brownetal.2020;Zhangetal.
2022] generate the text by predicting the next token in a sequence. This “next-token prediction”
paradigmpresentsunprecedentedcapabilitiesinsolvinglanguagetasksinahuman-likeconversational
manner[Ouyangetal.2022;OpenAI2022,2023b;Google2023;Anthropic2023;Workshopetal.
2022;Touvronetal.2023a,b;Baietal.2023a;Yangetal.2023;Team2023;Bietal.2024]and
incrediblescalability[Kaplanetal.2020;Henighanetal.2020;Hoffmannetal.2022;Weietal.2022;
Alabdulmohsinetal.2022;Chowdheryetal.2023;Aniletal.2023],demonstratingapromisingpath
towardgeneral-purposeartificialintelligencemodels.
Witnessed the scalability of autoregressive models on large language models, pioneering works
attempttoexploreautoregressivemodelsinimagegeneration,forexample,VQVAE[VanDenOord
etal.2017;Razavietal.2019],VQGAN[Esseretal.2021;Leeetal.2022],DALL-E[Rameshetal.
2021],Parti[Yuetal.2021,2022]. Theyintroduceimagetokenizerstoconvertcontinuousimagesto
discretetokens,andapplyautoregressivemodelstogenerateimagetokensinthewayofnext-token
prediction. Theydemonstratestrongperformanceamongtheircontemporaries[Brocketal.2018;
Ho et al. 2020; Dhariwal & Nichol 2021] in the year before 2022. However, their open-source
communitiesarenotwelldeveloped,whichlargelylimitstheirfurtherimprovements.
Atthesameperiod,anotherimagegenerationmethod,diffusionmodels[Song&Ermon2019;Ho
et al. 2020; Song et al. 2020; Dhariwal & Nichol 2021; Nichol et al. 2021; Lu et al. 2022a; Ho
etal.2022a;Ho&Salimans2022;Rombachetal.2022;Rameshetal.2022;Sahariaetal.2022;
Rombachetal.2022]developrapidly. Alongwiththeiropen-sourcecommunities,theydominate
thefieldofvisualgenerationuptotoday. However,diffusionmodelssharedistinctparadigmswith
autoregressivelanguagemodels,whichposesahugechallengetobuildingaunifiedmodelbetween
languageandvision.
Inthiswork,wearecommittedtopushingtheenvelopeofautoregressivemodelsonimagegeneration
further: continuingitsresearchmethodologyandcontributingtoopen-sourcecommunity. Reviewing
theliteratureonimagegenerationintheyearbefore2024,weidentifythreekeystoexistingadvanced
models[Peebles&Xie2023;Podelletal.2023;Xueetal.2023;Chenetal.2023b,c;Betkeretal.
2023; Li et al. 2024; Esser et al. 2024]: 1) well-designed image compressors, 2) scalable image
generationmodelsand3)high-qualitytrainingdata. Motivatedbythis,wereexaminethedesignsof
imagetokenizers(imagecompressorsforautoregressivemodels),thescalabilitypropertiesofimage
generationmodels,andtheeffectsoftrainingdata.
Towardsapotentialunifiedmodelbetweenlanguageandvision,ourdesignisreducingtheinductive
biasesonvisualsignalsandadoptingthesamearchitectureasLLM.Thisbelongstoadifferentre-
searchphilosophywithrecentworks[Changetal.2022;Yuetal.2023b;Tianetal.2024]thatmodify
thearchitecturesundertheguidanceofvision-orienteddesigns. Forexample,MaskGIT[Changetal.
2022],MAGVIT[Yuetal.2023a,b]adoptthemaskedimagemodelingstrategy,VAR[Tianetal.
2024]useshierarchicalmulti-scaleproperty. Althoughtheyhavesucceededinachievingleading
imagegenerationperformance,andevenbetterthandiffusionmodels,itisstillnotclearwhether
theoriginallanguagemodelarchitecturesarecapableofthis. Instead,ourworkrevealsthatvanilla
autoregressivemodelsthatapplytheexactlysame“next-tokenprediction”aslanguagemodelsare
alsoabletoachievestate-of-the-artimagegenerationperformance. Asabonus,wecanleveragethe
techniques[Daoetal.2022;Rasleyetal.2020;Shoeybietal.2019;Zhaoetal.2023;Kwonetal.
2023;Chenetal.2023a;Dettmers2022]developedinLLMcommunitytooptimizethetraining
recipesandinferencespeedsofourmodels.
Insummary,ourcontributionstothecommunityinclude:
1. Imagetokenizer: Animagetokenizerwithdownsampleratioof16,achievesreconstruction
qualityof0.94rFIDandcodebookusageof97%onImageNetbenchmark.Withthedownsample
ratio of 8, our tokenizer is competitive or even better than continuous VAE [Rombach et al.
2022;Podelletal.2023;OpenAI2023a]usedindiffusionmodels. Thisshowsthatdiscrete
representationinimagetokenizersisnolongerthebottleneckoftheimagereconstruction.
2. Scalable image generation model: A series of class-conditional image generation models,
rangingfrom111Mto3.1Bparameters,aredevelopedbasedonLlamaarchitecture[Touvron
2et al. 2023a,b]. The largest model realizes 2.18 FID on ImageNet 256×256 benchmarks,
outperformingthepopulardiffusionmodelssuchasLDM[Rombachetal.2022],DiT[Peebles
&Xie2023]. Thisshowsthatvanillaautoregressivemodelswithoutinductivebiasesonvisual
signalscanserveasthebasisofimagegenerationsystems.
3. Hiqh-qualitytrainingdata: Atext-conditionalimagegenerationmodelwith775Mparameters,
is firstly trained on a 50M subset of LAION-COCO [LAION 2022] and then fine-tuned on
10Minternalhighaestheticsqualityimages. Itdemonstratescompetitiveperformanceofvisual
qualityandtextalignment.
4. Optimizedinferencespeed: WeadoptvLLM[Kwonetal.2023], oneofthemostpopular
LLMservingframeworks,tooptimizetheinferencespeedofourimagegenerationmodels,and
remarkable326%-414%speedupisachieved.
Wereleaseallmodelsandcodestofacilitatetheopen-sourcecommunityofvisualgenerationand
multimodalfoundationmodels. Itisworthnoticingthatourreleasedmodelsarestillbehindstate-of-
the-artvisualgenerationmodelsbasedondiffusionmodels[Alpha-VLLM2024;Esseretal.2024;
Brooksetal.2024]. Whenmoretrainingdataandcomputationresourcesareavailableinthefuture,
large-scaleAR-basedvisualgenerationmodels,e.g.,above7Bparameters,willbeexplored.
2 AutoregressiveModelsforImageGeneration
2.1 Overview
Firstly, image pixels x ∈ RH×W×3 are quantized into q ∈ Qh×w discrete tokens by the image
tokenizer[VanDenOordetal.2017;Esseretal.2021;Yuetal.2021],whereh=H/p,w=W/p,
pisdownsampleratiooftheimagetokenizer,q(i,j) isindicesoftheimagecodebook. Then,these
image tokens are reshaped to a sequence of h·w tokens in raster scan ordering and used to train
Transformer[Vaswanietal.2017]-basedautoregressivemodels.
During image generation, image tokens (q ,q ,...,q ) are generated by autoregressive mod-
1 2 h·w
els[Radfordetal.2018,2019;Brownetal.2020;Touvronetal.2023a]inthewayofnext-token
prediction(cid:81)h·wp(q
|q ,c),wherecisclasslabelembeddingortextembedding. Finally,these
t=1 t <t
imagetokensareconvertedtoimagepixelsbytheimagetokenizerdecoder.
2.2 ImageTokenizer
Quantized-Autoencoder architecture. We use the same architecture as VQGAN [Esser et al.
2021],encoder-quantizer-decoder. TheencoderandthedecoderareConvNetwithdownsampleratio
p. ThequantizercontainsacodebookZ ∈RK×C withK learnablevectors. Theencoderprojects
imagepixelsxtothefeaturemapf. Thequantizationprocessmapseachvectorf(i,j)inthefeature
maptothecodeindexq(i,j)ofitsnearestvectorz(i,j)inthecodebook. Duringdecoding,thecode
indexq(i,j) isremappedtothefeaturevectorz(i,j) andthedecoderconvertsthesefeaturevectors
backtotheimagepixelsxˆ.
Thecodebookhascriticaleffectsonimagetokenizationperformance. Following[Yuetal.2021],we
useℓ -normalizationtocodebookvectors,lowcodebookvectordimensionC,andlargecodebook
2
sizeK. Thesedesignssignificantlyimprovereconstructionqualityandcodebookusage. Moredetails
willbediscussedinexperiments.
Traininglosses. Sincequantizationisanon-differentiableoperation,astraight-throughgradient
estimator [Bengio et al. 2013] is used to preserve the gradient from the decoder to the encoder
z =sg[z−f]+f,sg[·]isstop-gradientoperation. Forcodebooklearning,L =∥sg[f]−z∥2+
VQ 2
β∥f−sg[z]∥2,wherethesecondtermiscommitmentloss[VanDenOordetal.2017]toforcefeature
2
vectorsextractedfromtheencodertobeclosetocodebookvectors,β iscommitmentlossweight.
Forsimplicity,wedon’taddentropyloss[Yuetal.2023a;Changetal.2022]incodebooklearning.
Forimagereconstructiontraining,L =ℓ (x,xˆ)+L (x,xˆ)+λ L (xˆ),whereℓ isareconstruction
AE 2 P G G 2
lossonimagepixels,L (·)isaperceptuallossfromLPIPS[Zhangetal.2018],L (·)isanadversarial
P G
loss from a PatchGAN [Isola et al. 2017] discriminator trained at the same time with the image
tokenizer,andλ isadversariallossweight.
G
3Model Parameters Layers HiddenSize Heads
LlamaGen-B 111M 12 768 12
LlamaGen-L 343M 24 1024 16
LlamaGen-XL 775M 36 1280 20
LlamaGen-XXL 1.4B 48 1536 24
LlamaGen-3B 3.1B 24 3200 32
Table 1: Model sizes and architecture configurations of LlamaGen. The configurations are
followingpreviousworks[Radfordetal.2019;Touvronetal.2023a;OpenLM-Research2023].
2.3 ImageGenerationbyAutoregressiveModels
Llamaarchitecture. OurmodelarchitectureislargelybasedonLlama[Touvronetal.2023a,b],
applyingpre-normalizationusingRMSNorm[Zhang&Sennrich2019],SwiGLUactivationfunc-
tion[Shazeer2020],androtarypositionalembeddings[Suetal.2024]. Specifically,weuse2DRoPE
inateachlayerofourmodel,followingtheimplementationof[Luetal.2023;Fangetal.2023]. We
donotusethetechniqueofAdaLN[Peebles&Xie2023]tokeepourstructurethesameasLLM.
Class-conditional image generation. The class embedding is indexed from a set of learnable
embeddings[Peebles&Xie2023;Esseretal.2021]andisusedastheprefillingtokenembedding.
Startingfromthistokenembedding,themodelgeneratesthesequenceofimagetokensbynext-token
predictionway,andstopsatthelocationofthepre-definedmaximumlength.
Text-conditionalimagegeneration. Tointegratethetextconditionintoautoregressivemodels,we
useFLAN-T5XL[Chungetal.2024]asthetextencoder,theencodedtextfeatureisprojectedbyan
additionalMLP[Chenetal.2023b,c]andisusedasprefillingtokenembeddinginautoregressive
models. Wenotethatthisdesignisnotanultimatedesignformultimodalfoundationmodels,where
aunifiedvocabularyisestablishedbetweenlanguageandvision[Luetal.2023;Teametal.2023].
Weleaveitforfutureresearch.
Classifier-freeguidance. Developedinthediffusionmodelcommunity,classifier-freeguidance[Ho
&Salimans2022]iswell-knownforitsimprovingvisualqualityandtext-imagealignment. Weadopt
it in our models. During training, the conditional is randomly dropped and is replaced by a null
unconditionalembedding[Peebles&Xie2023;Chenetal.2023b]. Ininference,foreachtoken,its
logitℓ isformedbyℓ =ℓ +s(ℓ −ℓ ),whereℓ isconditionallogit,ℓ isunconditionallogit,
g g u c u c u
andsisscaleoftheclassifier-freeguidance.
Itisworthnotingthatalldesignchoicesdiscussedsofararelargelyinspiredbypreviousworks,for
example,imagetokenizerisborrowedfrom[Rombachetal.2022;Yuetal.2021],imagegeneration
isfrom[Peebles&Xie2023;Chenetal.2023b;Esseretal.2021].Alargeportionofthesetechniques
arewellstudiedindiffusionmodelsbutlittleinARmodels. Ourworkadaptstheseadvanceddesigns
collectivelytoAR-basedvisualgenerationmodels.
2.4 ScaleUp
OurmodelarchitectureisalmostthesameasLlama,whichallowsustoseamlesslyadoptoptimization
techniques[Zhang&Sennrich2019;Shazeer2020;Suetal.2024]andtrainingrecipes[Daoetal.
2022;Rasleyetal.2020;Shoeybietal.2019]inLLMcommunity. AsshowninTable1,wescalethe
modelsizeupto3.1Bparametersinthiswork. AllmodelsareimplementedwithPyTorch2[Ansel
etal.2024]andtrainedon80GBA100GPUs. Fortrainingthemodelswithparametersbelow1.4B,
wedirectlyuseDDP,otherwise,weadoptPyTorchFSDP[Zhaoetal.2023]tooptimizeGPUmemory
usage.
2.5 Serving
Autoregressivemodelshavealwayssufferedfromitslowinferencespeed.Withtherapiddevelopment
of large language models, advanced inference techniques [Kwon et al. 2023; Chen et al. 2023a;
Dettmers2022]areproposedintheLLMcommunitytooptimizetheinferencespeed.
4Similartotraining,inferencetechniquesdevelopedintheLLMcommunitycanalsobeadoptedto
optimize our models. We verify the effectiveness of vLLM [Kwon et al. 2023], one of the most
popularLLMservingframeworks,onourimagegenerationmethods. AsshowninTable7,326%-
414%speedupisachievedcomparedtothebaselinesetting.
3 Experiments
3.1 ImageTokenizer
Trainingsetup. ThetrainingisonImageNet[Dengetal.2009]trainset,usingtheresolutionof
256×256andrandomcropdataaugmentation. Theimagetokenizermodelsizeis72Mand70M
whenthedownsampleratiois16and8,respectively. Allmodelsaretrainedwiththesamesettings:
constantlearningrateof10−4,AdamWoptimizerwithβ =0.9,β =0.95,weightdecay=0.05,
1 2
batchsizeof128andtrainingepochsof40. Forthetraininglosses,commitmentlossweightis0.25
andadversariallossweightis0.5. Theadversariallossisenabledafter20ktrainingiterations.
Evaluationmetrics. WeusethepopularImageNetbenchmarkundertheimageresolutionof256
×256. Theimagereconstructionqualityismeasuredbyr-FID,reconstruction-FIDon256×256
ImageNet50kvalidationset. Thecodebookusageiscalculatedasthepercentageofusedcodesinthe
queueofsize65536overthewholecodebooksize. WealsoreportPSNRandSSIMasthemetricsof
reconstructionquality,followingSDXL[Podelletal.2023].
dim rFID↓ PSNR↑ SSIM↑ usage↑ size rFID↓ PSNR↑ SSIM↑ usage↑
256 9.21 18.32 0.575 0.29% 4096 3.02 19.99 0.643 100.0%
32 3.22 19.98 0.646 20.9% 8192 2.91 20.41 0.654 75.0%
8 2.19 20.79 0.675 97.0% 16384 2.19 20.79 0.675 97.0%
4 9.88 19.39 0.593 82.0% 32768 2.26 20.59 0.663 85.0%
(a)Codebookvectordimension. Lowervectordi- (b)Codebooksize.Largercodebooksize(from4096
mension(from256to8)improvesbothreconstruction to16384)benefitstotheoverallperformanceofimage
qualityandcodebookusagesignificantly. tokenizers.
Table 2: Ablation studies on codebook designs in image tokenizers.. The evaluations are on
256×256 ImageNet 50k validation set. The default setting is codebook vector dimension is 8,
codebooksizeis16384,downsampleratiois16.
ratio imgsize tokenssize rFID↓ PSNR↑ SSIM↑ usage↑
256 256(16×16) 2.19 20.79 0.675 97.0%
16 384 576(24×24) 0.94 21.94 0.726 97.0%
512 1024(32×32) 0.70 23.03 0.772 97.0%
256 1024(32×32) 0.59 24.45 0.813 97.6%
8 384 2304(48×48) 0.37 25.63 0.852 97.6%
512 4096(64×64) 0.39 26.98 0.888 97.6%
Table3: Numberoftokenstorepresenttheimage. Thenumberoftokensdependsondownsample
ratioandinputimagesize. Thereconstructedimageisalwaysresizedto256×256whenevaluating
onImageNet50kvalidationset. Thedefaultsettingiscodebookvectordimensionis8,codebook
sizeis16384.
Effectofimagecodebookdesigns. AsshowninTable2,whenthecodebookvectordimensionis
reducedfrom256to32to8,muchbetterreconstructionqualityandcodebookusageareconsistently
achieved. For codebook size, a larger size from 4096 to 16384 benefits the overall performance.
Theseobservationsareconsistentwithpreviousworks[Yuetal.2021,2023b].
Effectofnumberoftokenstorepresenttheimage. Table3studiestheeffectofimagetoken
numberonimagereconstructionquality. Usingthesameimagetokenizer,forexample,downsample
ratioas16,representinganimagewithonly256tokens(16×16)isnotsufficientforgoodreconstruc-
tionquality,andincreasingthenumberoftokensto576(24×24)couldlargelyimprovetheimage
qualityfrom2.43to0.99rFID.
5ImageNet COCO
ratio method dim size
rFID↓ PSNR↑ SSIM↑ rFID↓ PSNR↑ SSIM↑
VQGAN 256 1024 8.30 19.51 0.614 16.95 19.08 0.613
VQGAN 256 16384 4.99 20.00 0.629 12.29 19.57 0.630
16
MaskGIT 256 1024 2.28 - - - - -
Ours 8 16384 2.19 20.79 0.675 8.11 20.42 0.678
VQGANoim. 4 256 1.44 22.63 0.737 6.58 22.289 0.744
VQGANoim. 4 16384 1.19 23.38 0.762 5.89 23.08 0.771
8
ViT-VQGAN 32 8192 1.28 - - - - -
Ours 8 16384 0.59 24.45 0.813 4.19 24.20 0.822
SD-VAEukn. 4 - 0.74 25.68 0.820 4.45 25.41 0.831
8 SDXL-VAEukn. 4 - 0.68 26.04 0.834 4.07 25.76 0.845
OAI-Decoderukn. 4 - 0.81 24.43 0.786 4.59 24.19 0.800
Table4: Comparisonswithotherimagetokenizers. Theevaluationsareon256×256ImageNet
50kvalidationsetandCOCO5kval2017set. AllmodelsaretrainedonImageNetexcept“oim.” is
onOpenImage,“ukn.” isunknowntrainingdata.
Comparisonswithotherimagetokenizers. Wecomparewithotherimagetokenizers,including
VQGAN[Esseretal.2021],MaskGIT[Changetal.2022],ViT-VQGAN[Yuetal.2021]. Asshown
inTable4,ourtokenizeroutperformspreviousimagetokenizers. Wealsoevaluateourtokenizeron
COCOval2017[Linetal.2014]of256×256imageresolutiontoverifytheimagereconstruction
quality,sinceCOCOimagescontainmorecomplexscenes. Thecomparisonresultsareconsistent
withthoseinImageNetvalidationset. Thisshowsourtokenizerisageneralizableimagetokenizer
forbothobject-centricandscene-centricimages.
Importantly, our tokenizer is competitive to continuous latent space representation, such as SD
VAE [Rombach et al. 2022], SDXL VAE [Podell et al. 2023], and Consistency Decoder from
OpenAI [OpenAI 2023a], which are widely used in diffusion models. This shows that discrete
representationintheimagetokenizerisnolongerthebottleneckoftheimagereconstruction.
3.2 Class-conditionalImageGeneration
Trainingsetup. Ourbenchmarkisthepopular256×256ImageNet. Allmodelsaretrainedwith
thesimilarsettings: baselearningrateof10−4per256batchsize,AdamWoptimizerwithβ =0.9,
1
β = 0.95, weightdecay = 0.05, gradient clipping of 1.0. The dropout is always 0.1 for input
2
tokenembedding,attentionmoduleandFFNmodule. Theclassconditionembeddingdropoutfor
classifier-freeguidanceis0.1.
Precomputingimagecodes. Toacceleratethemodeltraining,weusetheimagetokenizertoprecom-
puteimagecodesbeforetraining. Toachievethesimilareffectofrandomcropdataaugmentation,
weextractimagecodesoftencropsoftheoriginalimage. Duringtraining,werandomlyselectone
copycodefromthetenaugmentations.
Evaluationmetrics. WeuseFréchetinceptiondistance(FID)[Heuseletal.2017]asthemain
metric. We also report Inception Score (IS) [Salimans et al. 2016], sFID [Nash et al. 2021] and
Precision/Recall[Kynkäänniemietal.2019]assecondarymetrics. Allevaluationsareimplemented
usingADM’sTensorFlowscripts[Dhariwal&Nichol2021]forfaircomparisons.
Effectofimagetokens. Althoughincreasingtheimagetokensbringsbetterimagereconstruction
quality,itisnotstronglycorrelatedtoimagegenerationquality. AsshowninTable5,whenthemodel
parameterissmallerthan1B,256(16×16)tokensbringbetterimagegenerationperformancethan
576(24×24). Thisshowsthesynergisticeffectofscalingupmodelparametersandtokennumbers.
Nevertheless,fewerimagetokenswouldlimittheimagegenerationperformance,forexample,256
(16×16)tokenslimittheFIDat3.06FID,while576(24×24)couldfurtherimprovetheFIDtoa
lowervalue.
6imagetoken model FID↓ IS↑ Precision↑ Recall↑
B 8.69 124.43 0.78 0.46
imagesize: 256×256 L 4.21 200.00 0.82 0.50
tokens: 256(16×16) XL 3.39 227.08 0.81 0.54
rFID:2.19 XXL 3.09 253.60 0.82 0.52
3B 3.06 279.71 0.84 0.53
B 12.89 92.44 0.73 0.48
imagesize: 384×384 L 5.01 167.31 0.78 0.52
tokens: 576(24×24) XL 3.42 202.93 0.79 0.56
rFID:0.94 XXL 2.89 236.21 0.80 0.56
3B 2.61 251.90 0.80 0.56
Table5: Theeffectofimagetokensonimagegeneration. Thegeneratedimageisalwaysresizedto
256×256whenevaluatingonImageNetbenchmark. Wecompareallmodelsaftertraining50epochs.
Theinferencesettingiscfg=1.75,top-k=0(all),top-p=1.0,temperature=1.0forallexperiments.
(a)withoutclassifier-freeguidance (b)withclassifier-freeguidance
Figure2: Scalingmodelsize. WeshowFIDof256×256ImageNetbenchmarkovertrainingepochs.
ScalingmodelsizebringsconsistentimprovementonFIDduringthewholetrainingprocess. More
detailedevaluationmetricsareinAppendix.
(a)classifier-freeguidance (b)top-ksampling
Figure3: Theeffectofsamplingconfiguration. WeshowFIDandInceptionScoreof256×256
ImageNetbenchmarkoverdifferentsamplingconfigurations. ThemodelisLlamaGen-L,andthe
defaultsettingiscfg=2.0,top-k=0(all),top-p=1.0,temperature=1.0.
Effectofmodelsize. Wetrainourmodelsacrossfivemodelsizes(B,L,XL,XXL,3B)andevaluate
theirperformancewithandwithoutclassifier-freeguidance. Figure2illustrateshowFIDchangesas
boththemodelsizesandthetrainingepochsincrease. NotableimprovementsinFIDareobserved
whenscalingthemodelfromLlamaGen-BtoLlamaGen-XXL.Furtherscalingto3Byieldsonly
marginalimprovements. Aplausibleexplanationforthisphenomenoncouldbethelimitationin
datasetsize: ImageNet[Dengetal.2009]comprisesapproximatelyonly1millionimages,expanding
thedatasetorusingstrongerdataaugmentationcouldpotentiallyleadtofurtherimprovements.
7Type Model #Para. FID↓ IS↑ Precision↑ Recall↑
BigGAN[Brocketal.2018] 112M 6.95 224.5 0.89 0.38
GAN GigaGAN[Kangetal.2023] 569M 3.45 225.5 0.84 0.61
StyleGan-XL[Saueretal.2022] 166M 2.30 265.1 0.78 0.53
ADM[Dhariwal&Nichol2021] 554M 10.94 101.0 0.69 0.63
CDM[Hoetal.2022b] − 4.88 158.7 − −
Diffusion
LDM-4[Rombachetal.2022] 400M 3.60 247.7 − −
DiT-XL/2[Peebles&Xie2023] 675M 2.27 278.2 0.83 0.57
MaskGIT[Changetal.2022] 227M 6.18 182.1 0.80 0.51
Mask.
MaskGIT-re[Changetal.2022] 227M 4.02 355.6 − −
VQGAN[Esseretal.2021] 227M 18.65 80.4 0.78 0.26
VQGAN[Esseretal.2021] 1.4B 15.78 74.3 − −
VQGAN-re[Esseretal.2021] 1.4B 5.20 280.3 − −
AR ViT-VQGAN[Yuetal.2021] 1.7B 4.17 175.1 − −
ViT-VQGAN-re[Yuetal.2021] 1.7B 3.04 227.4 − −
RQTran.[Leeetal.2022] 3.8B 7.55 134.0 − −
RQTran.-re[Leeetal.2022] 3.8B 3.80 323.7 − −
LlamaGen-B(cfg=2.00) 111M 5.46 193.61 0.83 0.45
LlamaGen-L(cfg=2.00) 343M 3.07 256.06 0.83 0.52
LlamaGen-XL(cfg=1.75) 775M 2.62 244.08 0.80 0.57
AR LlamaGen-XXL(cfg=1.75) 1.4B 2.34 253.90 0.80 0.59
LlamaGen-3B(cfg=1.65) 3.1B 2.18 263.33 0.81 0.58
LlamaGen-3B(cfg=1.75) 3.1B 2.32 280.10 0.82 0.56
LlamaGen-3B(cfg=2.00) 3.1B 2.81 311.59 0.84 0.54
Table6:Modelcomparisonsonclass-conditionalImageNet256×256benchmark.Metricsinclude
Fréchetinceptiondistance(FID),inceptionscore(IS),precisionandrecall. “↓”or“↑”indicatelower
orhighervaluesarebetter. “-re”meansusingrejectionsampling. “cfg”meansusingclassifier-free
guidance. MoredetailedresultsareinAppendix.
Effectofclassifier-freeguidance(CFG). First,asshowninFigure2,usingclassifier-freeguidance
cansignificantlyenhancethevisualqualityacrossallmodelsizes. Moreover,Figure3aillustrates
thatthemodelachievesoptimalFIDatCFG=2.0andfurtherincreasingCFGwoulddeteriorateFID,
whichisconsistentwithpreviousfindings[Dhariwal&Nichol2021]. Additionally,theincrementin
CFGresultsinatrade-offbetweendiversityandfidelity,asevidencedbyincreasedprecisionand
decreasedrecall,demonstratedinTable10.
Effectoftop-ksampling. AsshowninFigure3b,asmalltop-kvalueisnotbeneficialforFID
andIS.Increasingtop-kcontinuouslyimprovesFIDbutdecreasesIS,whichtradesofffidelityfor
diversity. We observe a similar trend when changing the parameter of top-p and temperature in
sampling. SinceFIDisourmainmetric,weusemaximumvalueasthedefaulttop-kvalue,whichis
thewholecodebooksize.
Comparisonswithotherimagegenerationmethods. InTable6,wecomparewithpopularimage
generationmodels,includingGAN[Brocketal.2018;Kangetal.2023;Saueretal.2022],Diffusion
models [Dhariwal & Nichol 2021; Ho et al. 2022b; Rombach et al. 2022; Peebles & Xie 2023],
andmasked-predictionmodels[Changetal.2022]. Ourmodelsexhibitcompetitiveperformance
in all metrics of FID, IS, Precision and Recall. Notably, our 3B model outperforms the popular
diffusionmodelsLDM[Rombachetal.2022],DiT[Peebles&Xie2023]. Thisshowsthatvanilla
autoregressivemodelscanserveasthebasisofadvancedimagegenerationsystems.
Whencomparingwithautoregressivemodels[Esseretal.2021;Yuetal.2021;Leeetal.2022],our
modeloutperformsallpreviousmodelsatdifferentlevelsofmodelparameters. Thisbenefitsfrom
betterdesignsofimagetokenizersandbetterscalabilityofimagegenerationmodels. Wehopeour
simpleandeffectiveimplementationwillserveasasolidbaselineandhelpfacilitatefutureresearch
inautoregressivemodelsforimagegenerations.
8Stage I
Stage II
A cutting board topped with bread, meat A kitchen that is in the process of having a big purple bus parked in a parking spot A furry, black bear standing in a rocky,
and vegetables. the floors done weedy, area in the wild.
Figure 4: Visualization of two-stage training of text-conditional image generation models.
ComparisonsofgeneratedimagesbymodelsafterstageItrainingandstageIItraining. Thetext
promptsarefromCOCOPrompts.
3.3 Text-conditionalImageGeneration
Training setup. We adopt a two-stage training strategy. In stage I, the model is trained on a
50M subset of LAION-COCO [LAION 2022] with the image resolution 256×256. In Stage II,
the model is fine-tuned on 10M internal high aesthetic quality images with the image resolution
512×512. ExamplesoftrainingdataareshownintheAppendix. Themaximumlengthoftexttoken
embeddingissetto120, andleftpaddingisusedtoenablebatchprocessing. Thetextcondition
embeddingdropoutforclassifier-freeguidanceis0.1. Allmodelsaretrainedwithsimilarsettings:
modelparametersof775M,baselearningrateof10−4per256batchsize,AdamWoptimizerwith
β =0.9,β =0.95,decay=0.05,gradientclippingof1.0.
1 2
Precomputingimagecodesandtextembeddings. Weusepre-trainedFLAN-T5XL[Chungetal.
2024]toprecomputetextembeddingoftheimagecaptions. Forimagecode,weonlyextractimage
codesoftheoriginalimagecentercropintext-conditionalmodelstraining.
Fine-tuneimagetokenizer. Beforetwo-stagetrainingfortext-conditionalimagegenerationmodels,
wefirstfine-tunetheimagetokenizeronthejointof50MLAION-COCOand10Minternalhigh
aestheticqualitydata.
Visualizations. InFigure4,weselecttextpromptsfromCOCOPrompts[Linetal.2014]togenerate
imagesusingmodelsafterstageItrainingandstageIItraining. AfterstageItraining,themodel
capturesthetext-imagealignment,whileitsabilitytorepresentimagedetailsisnotclear. StageII
trainingimprovesthevisualaestheticqualitybyasignificantmargin. Weexplainthisimprovement
comesfromtwoaspects: highaestheticqualityimagesshiftthedomain,andhighimageresolution
bringsbettervisualdetails. Wenoticethatfurtherincreasingtheimageresolutionto1024×1024
couldbringbettervisualquality,andweleaveitforfutureresearch.
MorevisualizationsonPartiPrompts[Yuetal.2022]areinAppendix. PartiPromptshavemorelonger
captionsthanCOCOPrompts,andourmodeldemonstratescompetitiveperformanceintext-image
alignmentforlongcaptionimagegenerationtasks.
Limitation. Due to the training data and model parameters, our text-conditional models have
several limitations, such as text rendering errors, counting errors, and common misconceptions.
Theseproblemsarepromisingtobemitigatedwhenmoretrainingdataandcomputationresources
areavailableinthefuture.
9model parameters baseline(sec) vllm(sec) speed-upratio
B 111M 7.80 2.39 326%
L 343M 13.72 3.48 380%
XL 775M 19.76 4.84 408%
XXL 1.4B 26.38 6.36 414%
Table 7: Optimized inference speed by vLLM serving framework. The inference time is for
a batch 16 images (generating 8 images with classifier-free guidance). The image resolution is
384×384forallmodels.
3.4 InferenceSpeed
WeverifytheeffectivenessofvLLM[Kwonetal.2023]servingframeworkonourmethods.Sinceour
modelsusethesamearchitectureasLlama,whichisalreadysupportedbyvLLM,wecanseamlessly
adoptitsimplementation. AsshowninTable7,weachieve326%-414%speedupcomparedtothe
baselinesettinginthemodelsfrom111Mto1.4Bparameters. Pleasenotethatthebaselinesetting
hasalreadyintegratedKV-Cachetechnique. Inthe3Bmodel,itsheadsize100isnotsupportedby
PagedAttentioninvLLM.
4 RelatedWork
Visualgeneration. Generativeadversarialnetwork(GAN)[Goodfellowetal.2014;Brocketal.
2018;Karrasetal.2019;Kangetal.2023]isthefirstrepresentativevisualgenerationmethodindeep
learningera. Toimprovethedistributioncoverage,severallikelihood-basedmethodsareproposed.
Diffusionmodels[Hoetal.2020;Song&Ermon2019;Songetal.2020;Dhariwal&Nichol2021]
viewimagegenerationasthereversediffusionprocessfromnoisestoimages. Masked-prediction
models [Chang et al. 2022, 2023; Yu et al. 2023a,b] apply language model BERT-style [Devlin
etal.2018]bylearningtopredictmaskedtokens. Instead,autoregressivemodels[Esseretal.2021;
Ramesh et al. 2021; Yu et al. 2022] leverage GPT-style [Radford et al. 2018] to predict the next
tokeninasequence. Toeasethemodelingandimprovethegenerationquality,thesemethodsalways
introducetheimagetokenizationprocess[Kingma&Welling2013;VanDenOordetal.2017]to
convertpixelspacetosemanticspace.
Multimodal foundation models. Recently, vision-and-language models [Liu et al. 2024; Zhu
et al. 2023; Dai et al. 2024; Peng et al. 2023; Zhang et al. 2023; Ma et al. 2024] have achieved
versatilevisualunderstandingthroughvisualinstructiontuning[Liuetal.2024;Zhuetal.2023].
However,unifyingtheunderstandingandgenerationinmultimodalmodelsisstillinitsearlystages.
Mostexistingmethods[Sunetal.2023b,a;Dongetal.2024;Geetal.2023]trytocollaboratea
pre-traineddiffusionmodelwithexistingmodels,ratherthanutilizingaunifiednext-tokenprediction
paradigm. Thesemethodsneedsophisticateddesignstoconnectthetwopartswithdistincttraining
paradigms,whichmakesscalingupchallenging. Pioneeringmethods[Luetal.2022b,2023;Bai
et al. 2023b; Team et al. 2023; Team 2024] attempt to incorporate image generation into LLM
usinganautoregressiveapproachandachievepromisingresults. Theydonotspecificallyfocuson
demonstratingthataplainautoregressiveapproachcanserveasascalableimagegenerator,whichis
ourmainargumentinthiswork.
5 Conclusion
Inthiswork,wedelveintovanillaautoregressivemodelsforscalableimagegeneration. Byreexamin-
ingtheirimagetokenizers,imagegenerationmodelsandtrainingdata,ourclass-conditionalmodels
outperformthepopulardiffusionmodels,andourtext-conditionalmodelsdemonstratecompetitive
performanceofvisualqualityandtextalignment.
10References
IbrahimMAlabdulmohsin,BehnamNeyshabur,andXiaohuaZhai. Revisitingneuralscalinglawsin
languageandvision. AdvancesinNeuralInformationProcessingSystems,35:22300–22312,2022.
Alpha-VLLM. Large dit. https://github.com/Alpha-VLLM/LLaMA2-Accessory/tree/
main/Large-DiT-ImageNet,2024.
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
SiamakShakeri,EmanuelTaropa,PaigeBailey,ZhifengChen,etal. Palm2technicalreport. arXiv
preprintarXiv:2305.10403,2023.
JasonAnsel,EdwardYang,HoraceHe,NataliaGimelshein,AnimeshJain,MichaelVoznesensky,
BinBao,PeterBell,DavidBerard,EvgeniBurovski,etal. Pytorch2: Fastermachinelearning
throughdynamicpythonbytecodetransformationandgraphcompilation. InProceedingsofthe
29thACMInternationalConferenceonArchitecturalSupportforProgrammingLanguagesand
OperatingSystems,Volume2,pp.929–947,2024.
Anthropic. Claude. https://www.anthropic.com/index/introducing-claude,2023.
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,
YuHan,FeiHuang,etal. Qwentechnicalreport. arXivpreprintarXiv:2309.16609,2023a.
YutongBai,XinyangGeng,KarttikeyaMangalam,AmirBar,AlanYuille,TrevorDarrell,Jitendra
Malik,andAlexeiAEfros. Sequentialmodelingenablesscalablelearningforlargevisionmodels.
arXivpreprintarXiv:2312.00785,2023b.
YoshuaBengio,NicholasLéonard,andAaronCourville. Estimatingorpropagatinggradientsthrough
stochasticneuronsforconditionalcomputation. arXivpreprintarXiv:1308.3432,2013.
JamesBetker,GabrielGoh,LiJing,TimBrooks,JianfengWang,LinjieLi,LongOuyang,Juntang
Zhuang,JoyceLee,YufeiGuo,etal. Improvingimagegenerationwithbettercaptions. Computer
Science.https://cdn.openai.com/papers/dall-e-3.pdf,2(3):8,2023.
XiaoBi,DeliChen,GuantingChen,ShanhuangChen,DamaiDai,ChengqiDeng,HonghuiDing,
KaiDong,QiushiDu,ZheFu,etal. Deepseekllm: Scalingopen-sourcelanguagemodelswith
longtermism. arXivpreprintarXiv:2401.02954,2024.
AndrewBrock,JeffDonahue,andKarenSimonyan. Largescalegantrainingforhighfidelitynatural
imagesynthesis. arXivpreprintarXiv:1809.11096,2018.
Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe
Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video
generationmodelsasworldsimulators. OpenAI,2024. URLhttps://openai.com/research/
video-generation-models-as-world-simulators.
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.
HuiwenChang,HanZhang,LuJiang,CeLiu,andWilliamTFreeman. Maskgit: Maskedgenerative
imagetransformer. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pp.11315–11325,2022.
HuiwenChang,HanZhang,JarredBarber,AJMaschinot,JoseLezama,LuJiang,Ming-HsuanYang,
KevinMurphy,WilliamTFreeman,MichaelRubinstein,etal. Muse: Text-to-imagegeneration
viamaskedgenerativetransformers. arXivpreprintarXiv:2301.00704,2023.
CharlieChen,SebastianBorgeaud,GeoffreyIrving,Jean-BaptisteLespiau,LaurentSifre,andJohn
Jumper. Acceleratinglargelanguagemodeldecodingwithspeculativesampling. arXivpreprint
arXiv:2302.01318,2023a.
JunsongChen,JinchengYu,ChongjianGe,LeweiYao,EnzeXie,YueWu,ZhongdaoWang,James
Kwok,PingLuo,HuchuanLu,etal. Pixart:Fasttrainingofdiffusiontransformerforphotorealistic
text-to-imagesynthesis. arXivpreprintarXiv:2310.00426,2023b.
11ShoufaChen,MengmengXu,JiaweiRen,YurenCong,SenHe,YanpingXie,AnimeshSinha,Ping
Luo,TaoXiang,andJuan-ManuelPerez-Rua. Gentron: Delvingdeepintodiffusiontransformers
forimageandvideogeneration. arXivpreprintarXiv:2312.04557,2023c.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scalinglanguagemodelingwithpathways.JournalofMachineLearningResearch,24(240):1–113,
2023.
HyungWonChung,LeHou,ShayneLongpre,BarretZoph,YiTay,WilliamFedus,YunxuanLi,
XuezhiWang,MostafaDehghani,SiddharthaBrahma,etal.Scalinginstruction-finetunedlanguage
models. JournalofMachineLearningResearch,25(70):1–53,2024.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-
languagemodelswithinstructiontuning. AdvancesinNeuralInformationProcessingSystems,36,
2024.
TriDao,DanFu,StefanoErmon,AtriRudra,andChristopherRé. Flashattention: Fastandmemory-
efficientexactattentionwithio-awareness. AdvancesinNeuralInformationProcessingSystems,
35:16344–16359,2022.
JiaDeng, WeiDong, RichardSocher, Li-JiaLi, KaiLi, andLiFei-Fei. Imagenet: Alarge-scale
hierarchicalimagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition,
pp.248–255.Ieee,2009.
TimDettmers. bitsandbytes. https://github.com/TimDettmers/bitsandbytes,2022.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeep
bidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
PrafullaDhariwalandAlexanderNichol. Diffusionmodelsbeatgansonimagesynthesis. Advances
inneuralinformationprocessingsystems,34:8780–8794,2021.
RunpeiDong,ChunruiHan,YuangPeng,ZekunQi,ZhengGe,JinrongYang,LiangZhao,Jianjian
Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi.
DreamLLM:Synergisticmultimodalcomprehensionandcreation. InTheTwelfthInternational
ConferenceonLearningRepresentations,2024.
PatrickEsser,RobinRombach,andBjornOmmer. Tamingtransformersforhigh-resolutionimage
synthesis. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,
pp.12873–12883,2021.
PatrickEsser,SumithKulal,AndreasBlattmann,RahimEntezari,JonasMüller,HarrySaini,Yam
Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion En-
glish, KyleLacey, AlexGoodwin, YannikMarek, andRobinRombach. Scalingrectifiedflow
transformersforhigh-resolutionimagesynthesis,2024.
YuxinFang,QuanSun,XinggangWang,TiejunHuang,XinlongWang,andYueCao. Eva-02: A
visualrepresentationforneongenesis. arXivpreprintarXiv:2303.11331,2023.
YuyingGe,SijieZhao,ZiyunZeng,YixiaoGe,ChenLi,XintaoWang,andYingShan. Making
llamaseeanddrawwithseedtokenizer. arXivpreprintarXiv:2310.01218,2023.
IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,
AaronCourville,andYoshuaBengio. Generativeadversarialnets. Advancesinneuralinformation
processingsystems,27,2014.
Google. Bard. https://bard.google.com/,2023.
TomHenighan,JaredKaplan,MorKatz,MarkChen,ChristopherHesse,JacobJackson,Heewoo
Jun,TomBBrown,PrafullaDhariwal,ScottGray,etal. Scalinglawsforautoregressivegenerative
modeling. arXivpreprintarXiv:2010.14701,2020.
12MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSeppHochreiter. Gans
trainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium. Advancesinneural
informationprocessingsystems,30,2017.
JonathanHoandTimSalimans. Classifier-freediffusionguidance. arXivpreprintarXiv:2207.12598,
2022.
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin
neuralinformationprocessingsystems,33:6840–6851,2020.
JonathanHo,ChitwanSaharia,WilliamChan,DavidJFleet,MohammadNorouzi,andTimSalimans.
Cascadeddiffusionmodelsforhighfidelityimagegeneration. TheJournalofMachineLearning
Research,23(1):2249–2281,2022a.
JonathanHo,ChitwanSaharia,WilliamChan,DavidJFleet,MohammadNorouzi,andTimSalimans.
Cascadeddiffusionmodelsforhighfidelityimagegeneration. TheJournalofMachineLearning
Research,23(1):2249–2281,2022b.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.
Trainingcompute-optimallargelanguagemodels. arXivpreprintarXiv:2203.15556,2022.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditionaladversarialnetworks. InProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,pp.1125–1134,2017.
MingukKang,Jun-YanZhu,RichardZhang,JaesikPark,EliShechtman,SylvainParis,andTaesung
Park. Scalingupgansfortext-to-imagesynthesis. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pp.10124–10134,2023.
JaredKaplan,SamMcCandlish,TomHenighan,TomBBrown,BenjaminChess,RewonChild,Scott
Gray,AlecRadford,JeffreyWu,andDarioAmodei. Scalinglawsforneurallanguagemodels.
arXivpreprintarXiv:2001.08361,2020.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarialnetworks. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pp.4401–4410,2019.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114,2013.
WoosukKwon,ZhuohanLi,SiyuanZhuang,YingSheng,LianminZheng,CodyHaoYu,JosephE.
Gonzalez,HaoZhang,andIonStoica. Efficientmemorymanagementforlargelanguagemodel
servingwithpagedattention. InProceedingsoftheACMSIGOPS29thSymposiumonOperating
SystemsPrinciples,2023.
Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved
precision and recall metric for assessing generative models. Advances in neural information
processingsystems,32,2019.
LAION. Laion-coco600m. https://laion.ai/blog/laion-coco,2022.
DoyupLee,ChiheonKim,SaehoonKim,MinsuCho,andWook-ShinHan. Autoregressiveimage
generationusingresidualquantization. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pp.11523–11532,2022.
DaiqingLi,AleksKamko,EhsanAkhgari,AliSabet,LinmiaoXu,andSuhailDoshi. Playgroundv2.
5: Threeinsightstowardsenhancingaestheticqualityintext-to-imagegeneration. arXivpreprint
arXiv:2402.17245,2024.
JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip: Bootstrappinglanguage-imagepre-
trainingforunifiedvision-languageunderstandingandgeneration. InInternationalconferenceon
machinelearning,pp.12888–12900.PMLR,2022.
13Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,Piotr
Dollár,andCLawrenceZitnick. Microsoftcoco:Commonobjectsincontext. InComputerVision–
ECCV2014: 13thEuropeanConference,Zurich,Switzerland,September6-12,2014,Proceedings,
PartV13,pp.740–755.Springer,2014.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. Advancesin
neuralinformationprocessingsystems,36,2024.
ChengLu,YuhaoZhou,FanBao,JianfeiChen,ChongxuanLi,andJunZhu. Dpm-solver: Afast
odesolverfordiffusionprobabilisticmodelsamplinginaround10steps. AdvancesinNeural
InformationProcessingSystems,35:5775–5787,2022a.
JiasenLu,ChristopherClark,RowanZellers,RoozbehMottaghi,andAniruddhaKembhavi. Unified-
io: Aunifiedmodelforvision,language,andmulti-modaltasks. arXivpreprintarXiv:2206.08916,
2022b.
JiasenLu,ChristopherClark,SanghoLee,ZichenZhang,SavyaKhosla,RyanMarten,DerekHoiem,
andAniruddhaKembhavi. Unified-io2: Scalingautoregressivemultimodalmodelswithvision,
language,audio,andaction. arXivpreprintarXiv:2312.17172,2023.
Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, and Xiaojuan Qi. Groma: Localized visual
tokenizationforgroundingmultimodallargelanguagemodels. arXivpreprintarXiv:2404.13013,
2024.
Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W Battaglia. Generating images with
sparserepresentations. arXivpreprintarXiv:2103.03841,2021.
AlexNichol, PrafullaDhariwal,AdityaRamesh, PranavShyam, PamelaMishkin,BobMcGrew,
IlyaSutskever,andMarkChen. Glide: Towardsphotorealisticimagegenerationandeditingwith
text-guideddiffusionmodels. arXivpreprintarXiv:2112.10741,2021.
OpenAI. Chatgpt. https://openai.com/blog/chatgpt,2022.
OpenAI. Consistencydecoder. https://github.com/openai/consistencydecoder,2023a.
OpenAI. Gpt-4technicalreport. arXivpreprintarXiv:2303.08774,2023b.
OpenLM-Research. Openllama 3b. https://huggingface.co/openlm-research/open_
llama_3b,2023.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,Chong
Zhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelstofollow
instructions with human feedback. Advances in Neural Information Processing Systems, 35:
27730–27744,2022.
WilliamPeeblesandSainingXie. Scalablediffusionmodelswithtransformers. InProceedingsof
theIEEE/CVFInternationalConferenceonComputerVision,pp.4195–4205,2023.
Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu
Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint
arXiv:2306.14824,2023.
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe
Penna,andRobinRombach. Sdxl: Improvinglatentdiffusionmodelsforhigh-resolutionimage
synthesis. arXivpreprintarXiv:2307.01952,2023.
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language
understandingbygenerativepre-training. article,2018.
AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal. Language
modelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
Zhou,WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-text
transformer. TheJournalofMachineLearningResearch,21(1):5485–5551,2020.
14AdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,MarkChen,
andIlyaSutskever. Zero-shottext-to-imagegeneration. InInternationalConferenceonMachine
Learning,pp.8821–8831.PMLR,2021.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
conditionalimagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,1(2):3,2022.
JeffRasley,SamyamRajbhandari,OlatunjiRuwase,andYuxiongHe. Deepspeed: Systemoptimiza-
tionsenabletrainingdeeplearningmodelswithover100billionparameters. InProceedingsof
the26thACMSIGKDDInternationalConferenceonKnowledgeDiscovery&DataMining,pp.
3505–3506,2020.
AliRazavi,AaronVandenOord,andOriolVinyals. Generatingdiversehigh-fidelityimageswith
vq-vae-2. Advancesinneuralinformationprocessingsystems,32,2019.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconfer-
enceoncomputervisionandpatternrecognition,pp.10684–10695,2022.
ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyLDenton,Kamyar
Ghasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,etal. Photorealistic
text-to-imagediffusionmodelswithdeeplanguageunderstanding.AdvancesinNeuralInformation
ProcessingSystems,35:36479–36494,2022.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improvedtechniquesfortraininggans. Advancesinneuralinformationprocessingsystems,29,
2016.
Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse
datasets. InACMSIGGRAPH2022conferenceproceedings,pp.1–10,2022.
NoamShazeer. Gluvariantsimprovetransformer. arXivpreprintarXiv:2002.05202,2020.
MohammadShoeybi,MostofaPatwary,RaulPuri,PatrickLeGresley,JaredCasper,andBryanCatan-
zaro. Megatron-lm: Trainingmulti-billionparameterlanguagemodelsusingmodelparallelism.
arXivpreprintarXiv:1909.08053,2019.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprintarXiv:2010.02502,2020.
YangSongandStefanoErmon. Generativemodelingbyestimatinggradientsofthedatadistribution.
Advancesinneuralinformationprocessingsystems,32,2019.
JianlinSu,MurtadhaAhmed,YuLu,ShengfengPan,WenBo,andYunfengLiu.Roformer:Enhanced
transformerwithrotarypositionembedding. Neurocomputing,568:127063,2024.
Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,
JingjingLiu,TiejunHuang,andXinlongWang. Generativepretraininginmultimodality. arXiv
preprintarXiv:2307.05222,2023a.
Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,
JingjingLiu,TiejunHuang,andXinlongWang. Generativepretraininginmultimodality. arXiv
preprintarXiv:2307.05222,2023b.
Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint
arXiv:2405.09818,2024.
GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu
Soricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighlycapable
multimodalmodels. arXivpreprintarXiv:2312.11805,2023.
InternLMTeam. Internlm: Amultilinguallanguagemodelwithprogressivelyenhancedcapabilities,
2023.
15KeyuTian,YiJiang,ZehuanYuan,BingyuePeng,andLiweiWang. Visualautoregressivemodeling:
Scalableimagegenerationvianext-scaleprediction. arXivpreprintarXiv:2404.02905,2024.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timothée
Lacroix, BaptisteRozière, NamanGoyal, EricHambro, FaisalAzhar, etal. Llama: Openand
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023a.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundation
andfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023b.
AaronVanDenOord, OriolVinyals, etal. Neuraldiscreterepresentationlearning. Advancesin
neuralinformationprocessingsystems,30,2017.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformationprocessing
systems,30,2017.
JasonWei,YiTay,RishiBommasani,ColinRaffel,BarretZoph,SebastianBorgeaud,DaniYogatama,
MaartenBosma,DennyZhou,DonaldMetzler,etal. Emergentabilitiesoflargelanguagemodels.
arXivpreprintarXiv:2206.07682,2022.
BigScienceWorkshop,TevenLeScao,AngelaFan,ChristopherAkiki,ElliePavlick,SuzanaIlic´,
DanielHesslow,RomanCastagné,AlexandraSashaLuccioni,FrançoisYvon,etal. Bloom: A
176b-parameteropen-accessmultilinguallanguagemodel. arXivpreprintarXiv:2211.05100,2022.
ZeyueXue,GuangluSong,QiushanGuo,BoxiaoLiu,ZhuofanZong,YuLiu,andPingLuo. Raphael:
Text-to-imagegenerationvialargemixtureofdiffusionpaths. arXivpreprintarXiv:2305.18295,
2023.
AiyuanYang,BinXiao,BingningWang,BorongZhang,CeBian,ChaoYin,ChenxuLv,DaPan,
Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint
arXiv:2309.10305,2023.
JiahuiYu,XinLi,JingYuKoh,HanZhang,RuomingPang,JamesQin,AlexanderKu,Yuanzhong
Xu,JasonBaldridge,andYonghuiWu. Vector-quantizedimagemodelingwithimprovedvqgan.
arXivpreprintarXiv:2110.04627,2021.
JiahuiYu,YuanzhongXu,JingYuKoh,ThangLuong,GunjanBaid,ZiruiWang,VijayVasudevan,
AlexanderKu,YinfeiYang,BurcuKaragolAyan,etal. Scalingautoregressivemodelsforcontent-
richtext-to-imagegeneration. arXivpreprintarXiv:2206.10789,2(3):5,2022.
Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander G
Hauptmann,Ming-HsuanYang,YuanHao,IrfanEssa,etal. Magvit: Maskedgenerativevideo
transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pp.10459–10469,2023a.
LijunYu,JoséLezama,NiteshBGundavarapu,LucaVersari,KihyukSohn,DavidMinnen,Yong
Cheng,AgrimGupta,XiuyeGu,AlexanderGHauptmann,etal. Languagemodelbeatsdiffusion–
tokenizeriskeytovisualgeneration. arXivpreprintarXiv:2310.05737,2023b.
Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural
InformationProcessingSystems,32,2019.
RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang. Theunreasonable
effectivenessofdeepfeaturesasaperceptualmetric. InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition,pp.586–595,2018.
ShilongZhang,PeizeSun,ShoufaChen,MinXiao,WenqiShao,WenweiZhang,KaiChen,and
PingLuo. Gpt4roi: Instructiontuninglargelanguagemodelonregion-of-interest. arXivpreprint
arXiv:2307.03601,2023.
SusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,MoyaChen,ShuohuiChen,Christopher
Dewan,MonaDiab,XianLi,XiVictoriaLin,etal. Opt: Openpre-trainedtransformerlanguage
models. arXivpreprintarXiv:2205.01068,2022.
16YanliZhao,AndrewGu,RohanVarma,LiangLuo,Chien-ChinHuang,MinXu,LessWright,Hamid
Shojanazeri,MyleOtt,SamShleifer,etal. Pytorchfsdp: experiencesonscalingfullyshardeddata
parallel. arXivpreprintarXiv:2304.11277,2023.
DeyaoZhu,JunChen,XiaoqianShen,XiangLi,andMohamedElhoseiny. Minigpt-4: Enhancing
vision-languageunderstandingwithadvancedlargelanguagemodels,2023.
17An abstract painting on a pillow with pink,
yellow and red flowers. The dining room is decorated with elegant decor. The new tab in Powerpointis highlighted. The card is attached to an external PCI.
A large building with columns and a clock An assortment of party decorations with owls and other
tower. Two lions are laying under a tree in the wild. items. an image of the coordinate of two circles.
Figure5: ExamplesofstageItrainingdata: 50MsubsetofLAION-COCO.Theshortcaptionis
itsoriginalcaption(generatedfromBLIP[Lietal.2022]).
Acozy bedroom with a large bed situated in the center
MgM ToA lo u hi c v s ela k e e rr e es g cy .e a h Ti, ras ehc rw e aa n cr e opt tao a e gro i ri n e nn at og- i pnl m i pk a g eee r i ta se rp i rd cda s i i s psn toh apti i tln br a tt eg e ya r eo ntn hdf sd e a o o i ms n rs m oh aa vo i i nl w eli d n rfa loig an ll c y,gM u sa sai nc i nok dp e f a t t hy thi hr e e M eo i Mf mo a wu ri ac ts h w gke i eet o. e . y r k . feo s aaf a p tdt n uah rdd rc oe i e e t o a ir so. mo f A n ao u a am r lf ali nb r. w re d l gT a p a e onh l ra fmke w fc ee tb ie rt h n, ie n dca ad goad n wi n ads d , i bn c ba aeo g em l lv as ow be ue wiar taee iir fnnnmd ugc li tw en vh n. i i t at ea Thh t wn e uh a d re or a w oc fb l ooh tle himm gi dt ee hr,f oo t spc or ntro ot mo o m t vo f w ii af l d l yo t lh i t sr n het oe g e r e A s ob cdte eh r wa o ne iu p ter h t l b ei af tu l t hpu sl ep ,eb eg fl f lu ail ovoe r wiw sn f egel to o r r stw s h b . se e eT tr m a h sw n ee a di t t bh if i nr nl u e ga ae s ow h ff uh o lo a ti r t w n e ae d s ge c t ar ve s o ii n n b ra st r are a te r n tn , ha t as ed a tu uo p br rr r p aao n e clu e ka en d dr nd a rw v oe n ii prd ct oh .e b n T.w y mT h aa eeht e nefe r t , w A li ofc i p nea so ' clr sd st e o . s m n o T ib ean h l n y ced e a ir l n a pi io sdw tun i pci r n h era og a sti m nso t hg if a n e a t ep h lns il a oam tt , ny ii af ’tl su i dn i l s d j g oe il n yaxli fgp uo u r gn lte oh, a s is nins tt dsia go n m cn od a, r a i ran m j eeng fa s rd eo tk ii en cit ns a dga m p e ar p mo o esc u eo ak t ah u ri na n ni ons da rc.o e ig Tp n.r h e a T aens h ,s ey
landscape outside. There are several candles placed combination of the blue flowers and the water droplets creates natural environment.
around the room, adding a touch of elegance and a visually appealing and serene atmosphere.
creating a serene atmosphere.
Figure6: ExamplesofstageIItrainingdata: 10Minternalhighaestheticqualityimages. The
longcaptionisgeneratedfromLLaVA.
A ExamplesofImage-TextPairData
TrainingstageI:50MsubsetofLAION-COCO[LAION2022]. Theoriginaldatasethas600M
image-textpair. WefiltertheseimagesbyvalidimageURL,aestheticscore,watermarkscore,CLIP
image-textsimilarityscoreandimagesize. Theremainingimagesareabout50M.Someexamples
areshowninFigure5.
TrainingstageII:10Minternalhighaestheticqualityimages. Eachimageisprovidedalong
captionbyLLaVA[Liuetal.2024]usingthepromptof“Describethisimageinasmuchdetailas
possible”. SomeexamplesareshowninFigure6. Wenoticethatthefirstsentenceofthelongcaption
isalwaysasummarydescriptionofitsimage,soweuseitastheshortcaptiontoaugmentthetraining
oftext-conditionalimagegenerationmodels.
B MoreResultsonImageNetBenchmark
We provide more detailed performance on ImageNet 256×256 benchmark in Table 8 9 10. The
generatedimageisalwaysresizedto256×256whenevaluating.
18Model #Para. epochs cfg FID↓ IS↑ sFID↓ Pre.↑ Rec.↑
B 111M 50 no 31.352 39.576 8.749 0.568 0.614
B 111M 50 1.50 11.984 95.400 7.335 0.738 0.517
B 111M 50 1.75 8.690 124.435 7.165 0.789 0.469
B 111M 50 2.00 7.390 153.974 7.250 0.832 0.417
B 111M 50 2.25 7.220 178.281 7.489 0.861 0.384
B 111M 50 2.50 7.824 197.511 7.857 0.882 0.349
B 111M 300 no 26.262 48.072 9.216 0.593 0.616
B 111M 300 1.50 8.738 120.602 7.668 0.751 0.535
B 111M 300 1.75 6.116 159.123 7.364 0.799 0.492
B 111M 300 2.00 5.464 193.613 7.503 0.839 0.457
B 111M 300 2.25 5.641 220.720 7.668 0.863 0.411
B 111M 300 2.50 6.390 246.565 8.041 0.883 0.382
L 343M 50 no 21.812 59.179 8.772 0.616 0.640
L 343M 50 1.50 5.781 153.792 7.096 0.774 0.555
L 343M 50 1.75 4.218 200.001 7.015 0.824 0.509
L 343M 50 2.00 4.317 242.112 7.077 0.859 0.468
L 343M 300 no 13.452 82.289 8.324 0.656 0.638
L 343M 300 1.50 4.079 198.504 8.157 0.800 0.552
L 343M 300 1.75 3.805 248.280 8.487 0.833 0.515
L 343M 300 2.00 4.407 288.170 8.871 0.858 0.481
XL 775M 50 no 19.417 66.196 8.911 0.610 0.665
XL 775M 50 1.50 4.808 172.170 7.298 0.767 0.585
XL 775M 50 1.75 3.391 227.081 7.022 0.812 0.542
XL 775M 50 2.00 3.642 268.779 7.244 0.846 0.502
XXL 1.4B 50 no 16.822 74.888 9.285 0.628 0.660
XXL 1.4B 50 1.50 3.844 195.527 7.496 0.781 0.577
XXL 1.4B 50 1.75 3.094 253.609 7.305 0.825 0.529
XXL 1.4B 50 2.00 3.644 296.521 7.410 0.857 0.511
3B 3.1B 50 no 13.581 87.902 7.781 0.648 0.666
3B 3.1B 50 1.50 3.050 222.330 6.489 0.801 0.575
3B 3.1B 50 1.75 3.063 279.716 6.686 0.843 0.538
3B 3.1B 50 2.00 4.212 325.150 7.027 0.869 0.492
validationdata 1.684 231.811 3.692 0.752 0.671
Table8: Detailedperformanceonclass-conditionalImageNet256×256benchmark. Thegener-
atedimageis256×256. Allexperimentsusethesamplingconfigurationoftop-k=0(all),top-p=
1.0,temperature=1.0.
...
...
...
19Model #Para. epochs cfg FID↓ sFID↓ IS↑ Pre.↑ Rec.↑
B 111M 50 no 41.025 30.788 9.825 0.523 0.605
B 111M 50 1.50 18.276 69.337 7.557 0.677 0.534
B 111M 50 1.75 12.899 92.447 6.900 0.738 0.487
B 111M 50 2.00 10.029 116.372 6.562 0.787 0.443
B 111M 50 2.25 8.674 136.621 6.428 0.818 0.413
B 111M 50 2.50 8.309 154.719 6.599 0.843 0.376
B 111M 50 2.75 8.391 168.629 6.708 0.860 0.345
B 111M 100 no 33.442 37.528 9.872 0.536 0.609
B 111M 100 1.50 15.629 77.247 7.632 0.698 0.529
B 111M 100 1.75 10.676 104.581 6.960 0.754 0.490
B 111M 100 2.00 8.298 128.941 6.671 0.795 0.452
B 111M 100 2.25 7.256 152.502 6.510 0.827 0.416
B 111M 100 2.50 7.151 172.677 6.517 0.850 0.390
B 111M 200 no 32.105 37.993 10.144 0.559 0.618
B 111M 200 1.50 12.206 90.783 7.531 0.716 0.534
B 111M 200 1.75 8.535 118.399 7.024 0.766 0.503
B 111M 200 2.00 6.951 146.077 6.784 0.808 0.459
B 111M 200 2.25 6.542 167.825 6.695 0.833 0.428
B 111M 200 2.50 6.632 188.157 6.811 0.853 0.393
B 111M 300 no 32.196 39.877 11.838 0.570 0.611
B 111M 300 1.50 12.012 95.553 8.897 0.725 0.528
B 111M 300 1.75 8.012 127.957 8.088 0.778 0.498
B 111M 300 2.00 6.437 157.173 7.487 0.814 0.456
B 111M 300 2.25 6.092 182.538 7.244 0.845 0.416
B 111M 300 2.50 6.249 203.886 6.981 0.861 0.389
B 111M 300 2.75 6.803 220.708 6.928 0.876 0.357
L 343M 50 no 25.889 48.053 9.612 0.570 0.655
L 343M 50 1.50 7.905 123.830 7.381 0.732 0.569
L 343M 50 1.75 5.018 167.310 6.786 0.784 0.524
L 343M 50 2.00 4.240 206.739 6.483 0.825 0.491
L 343M 50 2.25 4.589 238.890 6.325 0.850 0.451
L 343M 100 no 24.654 53.166 10.497 0.594 0.645
L 343M 100 1.50 6.934 138.852 7.910 0.748 0.569
L 343M 100 1.75 4.321 188.536 7.068 0.802 0.528
L 343M 100 2.00 3.705 228.305 6.701 0.839 0.490
L 343M 100 2.25 4.054 263.864 6.407 0.858 0.460
L 343M 200 no 19.742 61.715 7.286 0.601 0.667
L 343M 200 1.50 4.929 158.546 6.066 0.759 0.588
L 343M 200 1.75 3.249 209.372 5.927 0.805 0.544
L 343M 200 2.00 3.220 250.697 5.879 0.841 0.512
L 343M 200 2.25 3.939 288.217 6.076 0.865 0.479
L 343M 300 no 19.070 64.349 8.668 0.607 0.670
L 343M 300 1.50 4.743 165.381 6.740 0.758 0.596
L 343M 300 1.75 3.151 214.152 6.310 0.803 0.552
L 343M 300 2.00 3.075 256.067 6.088 0.832 0.522
L 343M 300 2.25 3.620 291.695 6.122 0.854 0.493
validationdata 1.684 231.811 3.692 0.752 0.671
Table9: Detailedperformanceonclass-conditionalImageNet256×256benchmark. Thegener-
atedimageis384×384andisresizedto256×256whenevaluatingonImageNet. Allexperiments
usethesamplingconfigurationoftop-k=0(all),top-p=1.0,temperature=1.0.
20Model #Para. epochs cfg FID↓ sFID↓ IS↑ Pre.↑ Rec.↑
XL 775M 50 no 19.820 61.363 8.067 0.601 0.669
XL 775M 50 1.50 5.231 154.249 6.284 0.746 0.592
XL 775M 50 1.75 3.420 202.939 6.090 0.796 0.560
XL 775M 50 2.00 3.238 245.680 6.023 0.826 0.529
XL 775M 100 no 18.037 69.879 8.388 0.616 0.665
XL 775M 100 1.50 4.563 173.749 6.591 0.759 0.588
XL 775M 100 1.75 3.089 225.856 6.157 0.804 0.551
XL 775M 100 2.00 3.105 267.608 6.001 0.833 0.531
XL 775M 200 no 14.772 80.826 6.840 0.620 0.681
XL 775M 200 1.50 3.388 193.477 5.753 0.771 0.603
XL 775M 200 1.75 2.617 245.465 5.652 0.811 0.566
XL 775M 200 2.00 2.859 285.900 5.758 0.840 0.527
XL 775M 300 no 15.549 79.157 7.049 0.616 0.689
XL 775M 300 1.50 3.479 194.448 5.816 0.763 0.606
XL 775M 300 1.75 2.629 244.085 5.594 0.807 0.579
XL 775M 300 2.00 2.785 286.875 5.567 0.836 0.542
XXL 1.4B 50 no 17.195 74.123 8.689 0.605 0.681
XXL 1.4B 50 1.50 4.363 178.228 6.818 0.758 0.600
XXL 1.4B 50 1.75 2.893 236.210 6.263 0.805 0.564
XXL 1.4B 50 2.00 3.049 285.390 6.053 0.842 0.522
XXL 1.4B 200 no 13.997 86.776 8.178 0.637 0.684
XXL 1.4B 200 1.50 3.137 207.870 6.060 0.774 0.605
XXL 1.4B 200 1.75 2.331 262.995 5.714 0.816 0.579
XXL 1.4B 200 2.00 2.678 304.631 5.587 0.840 0.545
XXL 1.4B 300 no 14.648 86.328 8.687 0.628 0.681
XXL 1.4B 300 1.50 3.295 202.586 6.476 0.770 0.626
XXL 1.4B 300 1.75 2.340 253.906 5.977 0.809 0.596
XXL 1.4B 300 2.00 2.523 295.374 5.736 0.836 0.559
3B 3.1B 50 no 16.431 72.622 7.217 0.611 0.677
3B 3.1B 50 1.50 3.472 191.979 5.955 0.768 0.600
3B 3.1B 50 1.75 2.611 251.903 6.167 0.807 0.568
3B 3.1B 50 2.00 3.222 300.887 5.764 0.847 0.523
3B 3.1B 200 no 9.949 108.083 7.088 0.667 0.672
3B 3.1B 200 1.50 2.400 237.683 5.548 0.794 0.600
3B 3.1B 200 1.65 2.264 268.180 5.426 0.817 0.581
3B 3.1B 200 1.75 2.381 286.091 5.390 0.828 0.569
3B 3.1B 200 2.00 3.011 321.563 5.514 0.851 0.538
3B 3.1B 300 no 9.380 112.877 8.242 0.685 0.668
3B 3.1B 300 1.50 2.388 233.246 6.145 0.798 0.601
3B 3.1B 300 1.60 2.216 251.338 6.002 0.811 0.584
3B 3.1B 300 1.65 2.189 263.334 5.965 0.819 0.581
3B 3.1B 300 1.75 2.329 280.104 5.818 0.828 0.566
3B 3.1B 300 1.80 2.370 287.452 5.825 0.834 0.570
3B 3.1B 300 2.00 2.816 311.597 5.845 0.848 0.544
validationdata 1.684 231.811 3.692 0.752 0.671
Table10: Detailedperformanceonclass-conditionalImageNet256×256benchmark. Thegener-
atedimageis384×384andisresizedto256×256whenevaluatingonImageNet. Allexperiments
usethesamplingconfigurationoftop-k=0(all),top-p=1.0,temperature=1.0.
21Figure7: 384×384LlamaGen-3Bsamples. Figure8: 384×384LlamaGen-3Bsamples.
Classifier-freeguidancescale=4.0 Classifier-freeguidancescale=4.0
Classlabel="goldenretriever"(207) Classlabel="husky"(250)
22Figure9: 384×384LlamaGen-3Bsamples. Figure10: 384×384LlamaGen-3Bsamples.
Classifier-freeguidancescale=4.0 Classifier-freeguidancescale=4.0
Classlabel="cliffdrop-off"(972) Classlabel="coralreef"(973)
23Figure11: 384×384LlamaGen-3Bsamples. Figure12: 384×384LlamaGen-3Bsamples.
Classifier-freeguidancescale=4.0 Classifier-freeguidancescale=4.0
Classlabel="spaceshuttle"(812) Classlabel="sportcar"(817)
24a tiger an illustration of a teapot.
A plate on a wooden table full of bread. A cat resting on an open laptop computer.
a corgi wearing a red bowtie and a purple Golden Gate bridge on the surface of
A large green truck on a city street.. A few bags laying around in a living room. party hat Mars
a view of the Kremlin with snow falling the Eiffel Tower in winter
Two birds that are sitting in a marsh area. A box of donut vs a o rf ie d tii effe s.rent colors and
A photo of an Athenian vase with a A bare kitchen has wood cabinets and
painting of pandas playing soccer in the white appliances
style of Egyptian hieroglyphics.
A large body of water sitting below a A brown cow laying on top of a lush green
mountain range. field.
A pp hh ao wrt ao hog ithr ea wp t-h e s hao ir rf i tna ag p n so dter lt eara ami tt hp o euf rn a k ja s g ct kla a et su ts .e e o sf , a A O u c n epl d xo e e ps ra r le o a- H du ibop nlu guh s ei yeg en h s li- lgi oc tt h wo in tn g s str k tna ayes r sot x ,fp t arth o o no i dlEt io n ri f ago fe d f e l iS antoy te iwd nrgn ge ye r ,,y
A pickup truck driving through a desert A bathroom with a blue shower curtain swirls of blue
environment. and blue walls..
Figure 13: (Stage I) Text-conditional 256×256 Figure 14: (Stage I) Text-conditional 256×256
imagegenerationonCOCOPrompts. imagegenerationonPartiPrompts.
25a baby penguin a chimpanzee an ostrich a corgi’s head depicted as an explosion of
a nebula
a photograph of a squirrel holding an a racoon detective using a microscope a gorilla climbing up the side of the Great panda mad scientist mixing sparkling
arrow above its head and holding a while riding in a train Pyramid chemicals, high-contrast painting.
longbow in its left hand
A photo of a four-leaf clover made of a sunken ship at the bottom of the ocean a plant at the bottom of a shallow stream red apples on a tree with green leaves
water.
Siberian husky playing the piano. A high resolution photo of a chicken Dogs sitting around a poker table Dogs sitting around a poker table with
working out in a gym. beer bottles and chips. Their hands are
holding cards.
A blue Porsche 356 parked in front of a A photo of an astronaut riding a horse in a photograph of the mona lisadrinking A portrait of a metal statue of a pharaoh
yellow brick wall the forest. There is a river in front of them coffee as she has her breakfast. her plate wearing steampunk glasses and a leather
with water lilies. has an omeletteand croissant jacket over a white t-shirt that has a
drawing of a space shuttle on it.
Figure15: (StageII)Text-conditional512×512imagegenerationonPartiPrompts.
26