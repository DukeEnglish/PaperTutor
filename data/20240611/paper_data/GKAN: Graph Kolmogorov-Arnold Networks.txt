GKAN: Graph Kolmogorov-Arnold Networks
MehrdadKiamari MohammadKiamari
DepartmentofElectricalandComputerEngineering DepartmentofComputerScience
UniversityofSouthernCalifornia RWTHAachenUniversity
LosAngeles,CA90089 Aachen,Germany
kiamari@usc.edu mohammad.kiamari@rwth-aachen.de
BhaskarKrishnamachari
DepartmentofElectricalandComputerEngineering
UniversityofSouthernCalifornia
LosAngeles,CA90089
bkrishna@usc.edu
Abstract
WeintroduceGraphKolmogorov-ArnoldNetworks(GKAN),aninnovativeneu-
ral network architecture that extends the principles of the recently proposed
Kolmogorov-Arnold Networks (KAN) to graph-structured data. By adopting
theuniquecharacteristicsofKANs,notablytheuseoflearnableunivariatefunc-
tionsinsteadoffixedlinearweights,wedevelopapowerfulmodelforgraph-based
learningtasks. UnliketraditionalGraphConvolutionalNetworks(GCNs)thatrely
onafixedconvolutionalarchitecture,GKANsimplementlearnablespline-based
functionsbetweenlayers,transformingthewayinformationisprocessedacross
the graph structure. We present two different ways to incorporate KAN layers
intoGKAN:architecture1—wherethelearnablefunctionsareappliedtoinput
featuresafteraggregationandarchitecture2—wherethelearnablefunctionsare
appliedtoinputfeaturesbeforeaggregation. WeevaluateGKANempiricallyusing
asemi-supervisedgraphlearningtaskonareal-worlddataset(Cora). Wefindthat
architecturegenerallyperformsbetter. WefindthatGKANsachievehigheraccu-
racyinsemi-supervisedlearningtasksongraphscomparedtothetraditionalGCN
model. Forexample,whenconsidering100features,GCNprovidesanaccuracyof
53.5whileaGKANwithacomparablenumberofparametersgivesanaccuracyof
61.76;with200features,GCNprovidesanaccuracyof61.24whileaGKANwith
acomparablenumberofparametersgivesanaccuracyof67.66. Wealsopresent
resultsontheimpactofvariousparameterssuchasthenumberofhiddennodes,
grid-size,andthepolynomial-degreeofthesplineontheperformanceofGKAN.
1 Introduction
Thelandscapeofdeeplearninghaswitnessedtransformativeadvancementsinrecentyears,particu-
larlyinthedevelopmentofmethodologiesthateffectivelyhandlegraph-structureddata—acrucial
elementinapplicationslikerecommendationsystemsthatutilizeintricateuser-to-iteminteraction
andsocialgraphs[Bronsteinetal.(2017),Hamiltonetal.(2018),KipfandWelling(2017),Monti
etal.(2017),vandenBergetal.(2017),Youetal.(2018)]. Amongthenotableinnovations,Graph
ConvolutionalNetworks(GCNs)haveemergedasaparadigm-shiftingarchitecture[Hamiltonetal.
(2018),KipfandWelling(2017),Montietal.(2017),vandenBergetal.(2017),Yingetal.(2018)].
GCNsharnessthepowerofneuralnetworkstoiterativelyaggregateandtransformfeatureinformation
fromlocalgraphneighborhoods[Yingetal.(2018)]. Thismethodenablesarobustintegrationof
Preprint.Underreview.
4202
nuJ
01
]GL.sc[
1v07460.6042:viXrabothcontentandstructuraldatafromgraphs,settingnewbenchmarksacrossvariousrecommender
systemapplications[Hamiltonetal.(2018)]. Furtherimprovementsinaccuracyareessentialacross
manydomains,includinglarge-scalenetworks,toaddressthelimitationsofGCNs.
At their core, GCNs are based on Multi-layer Perceptrons (MLPs), which are foundational to
moderndeeplearningframeworksandessentialfortheirrobustcapabilitytoapproximatenonlinear
functions—atraitanchoredintheuniversalapproximationtheorem[Horniketal.(1989)]. Despite
theirwidespreaduseandcriticalroleincontemporarymodels,MLPsencounternotablelimitations,
suchassignificantconsumptionofnon-embeddingparametersintransformers[Vaswanietal.(2017)]
andlimitedinterpretabilityunlessadditionalpost-analysistoolsareemployed[Hubenetal.(2024)].
Inresponse,Liuetal.(2024)haverecentlyintroducedaninnovativealternative,theKolmogorov-
ArnoldNetworks(KAN),inspirednotbytheuniversalapproximationtheoremlikeMLPsbutbythe
Kolmogorov-Arnoldrepresentationtheorem[Kolmogorov(1956),Kolmogorov(1957),Braunand
Griebel(2009)]. UnlikeMLPs,whichutilizelearnableweightsontheedgesandfixedactivation
functionsonnodes,KANsdeploylearnableunivariatefunctionsontheedgesandsimplesummations
on the nodes. Each weight in a KAN is thus a learnable one-dimensional function, shaped as a
spline,allowingforsignificantlysmallercomputationgraphscomparedtothoserequiredbyMLPs.
Historically, neural network designs based on the Kolmogorov-Arnold theorem have primarily
adhered to a depth-2, width-(2n + 1) model [Lai and Shen (2023)], lacking in modern training
enhancementssuchasbackpropagation. KANs,however,allowthestackingofKAN-layerstocreate
deeplearningnetworksthatcanbetrainedusingbackpropagation.
OurworkextendsthefoundationalconceptsofKANstotherealmofgraph-baseddata,introducing
GraphKolmogorov-ArnoldNetworks(GKANs). Thesenetworksaredesignedtoovercomethescala-
bilityandadaptabilitylimitationsfacedbyconventionalGCNs. SimilartoKANs,byincorporating
learnable functions directly within the graph’s edges, GKANs allow for a more dynamic feature
propagationacrossexpansiveandevolvinggraphstructures. Wearemotivatedtoinvestigatewhether
thisapproachholdsthepromiseofsurpassingthecurrentgraphlearningparadigms.
Throughsoftwareexperiments1groundedinarealdataset,wefindthatGKANssignificantlyoutper-
formstate-of-the-artGCNsintermsofclassificationaccuracyandefficiency. Ourcontributionliesin
harmoniouslyblendingthelearningthegraphstructureofdatawiththeinnovativearchitectureof
KANs,therebysettinganewstandardforgraph-baseddeeplearningmodelstohandlelarge-scale,
dynamic datasets effectively. Though we focus our attention in this paper on GCNs, we believe
thatGKANsopenanewavenueingraphrepresentationlearningandcouldserveasthefoundation
forallkindsofapproachesthatpreviouslyutilizedMLPsattheircore, suchasGCNs[Hamilton
etal.(2018),KipfandWelling(2017),Montietal.(2017),vandenBergetal.(2017),Yingetal.
(2018)],GAT[Velicˇkovic´ etal.(2018),Brodyetal.(2021)],GraphAutoencoders[KipfandWelling
(2016),Salha-Galvanetal.(2020)],GraphTransformers[Lietal.(2019),CaiandLam(2019),Mialon
etal.(2021),Chenetal.(2022)],andmanyothergraphdeeplearningschemes.
Inthefollowingsection,wewillfirstprovideabriefoverviewonKAN.Thenweelaborateuponthe
GKANarchitecture,andpresentempiricalevidenceofitssuperiorityinnodeclassificationtasks.
2 Preliminary
2.1 Kolmogorov–ArnoldNetworks
KANsareinspiredbythemathematicalunderpinningprovidedbythefamousKolmogorov-Arnold
representation theorem [Kolmogorov (1957),Braun and Griebel (2009)]. This theorem offers a
compellingframeworkforconstructingneuralnetworksthatdramaticallydivergefromtraditional
Multi-Layer Perceptrons (MLPs), which are typically grounded in and justified by the universal
approximationtheorem. TheuniqueapproachofKANsutilizesanetworkdesignwheretraditional
weightparametersarereplacedbylearnablefunctions. InSection2.1,wereviewtheoriginsand
implicationsoftheKolmogorov-Arnoldtheorem,settingthestageforadeepdiveintotheinnovative
structureofKANsoutlinedinSection2.2.
1WeplantomakeoursoftwareimplementationofGKANpubliclyavailablesoononGithubatthefollowing
URL:https://github.com/ANRGUSC/GKAN.
22.1.1 Kolmogorov-ArnoldTheorem
Through a series of papers in the late 1950’s [Kolmogorov (1956),Kolmogorov (1957),Arnold
(1957),Arnold (1958),Arnold (1959)], Andrey Kolmogorov and Vladimir Arnold formulated a
significanttheoremwhichpositsthatanycontinuousmultivariatefunctionwithinaboundeddomain
canbeeffectivelybrokendownintoaseriesofcontinuousunivariatefunctionscombinedthroughthe
operationofaddition.
TheKolmogorov-ArnoldRepresentationtheorem[BraunandGriebel(2009)]statesthatasmooth
functionf :[0,1]n →Rcanbeexpressedas:
2n+1 (cid:32) n (cid:33)
(cid:88) (cid:88)
f(x ,...,x )= Φ ϕ (x ) , (1)
1 n q q,p p
q=1 p=1
whereeachϕ isamappingfrom[0,1]toR,andeachΦ isareal-valuedfunction. Thisformu-
q,p q
lationdemonstratesthatmultivariatefunctionscanfundamentallybereducedtoasuitablydefined
compositionofunivariatefunctions,wherethecompositiononlyinvolvessimpleaddition.
EarlyworkGirosiandPoggio(1989)arguedthatdespiteitseleganceandgenerality,thistheorem
is not useful for machine learning because the inner-functions ϕ (.) are not in general smooth.
q,p
Indicatingthattherewerepriorworksonthistopicthatwerenotparticularlysuccessfulinspurring
widerinterestandadoption,Liuetal.(2024)maintainamorepositiveviewregardingtheapplicability
oftheKolmogorov-Arnoldtheoreminmachinelearningcontexts. Firstly,comparedtootherprior
worksthatusedthistheoreminalimitedway,theydonotconstraintheneuralnetworktoadhere
totheoriginalEquation(2.1),characterizedbyitsshallowstructureoftwo-layernonlinearitiesand
limitednumberofterms(2n+1)inthehiddenlayer;instead,theyproposeextendingthenetwork
to arbitrary widths and depths. Secondly, the functions commonly encountered in scientific and
everydaycontextstendtobesmoothandexhibitsparsecompositionalstructures,whichmayenhance
theeffectivenessofKolmogorov-Arnoldrepresentations.
2.1.2 KANArchitecture
Inthedomainofsupervisedlearningtasks,wherewedealwithinput-outputpairs{(x ,y )},their
i i
goalistomodelafunctionf suchthaty ≈ f(x )foralldatapoints. Drawinginspirationfrom
i i
thestructureproposedinEquation(2.1), Liuetal.(2024)designaneuralnetworkthatexplicitly
embodiesthisequation. Thisnetworkisconstructedsuchthatalllearnablefunctionsareunivariate,
witheachbeingparameterizedasaB-spline,enhancingtheflexibilityandlearnabilityofthemodel.
Liuetal.’sinitialmodeloftheKANembodiesEquation(2.1)withinitscomputationgraph,showcas-
ingatwo-layerneuralnetworkwhereactivationfunctionsarelocatedonedgesratherthannodes,and
aggregationisachievedthroughsimplesummationatnodes. Thissetupisdepictedwithaninput
dimensionalityofn=2andamiddlelayerwidthof2n+1,asillustratedintheirfigures.
Giventhesimplisticnatureofthisinitialmodel,itisgenerallyinsufficientforapproximatingcomplex
functionswithhighfidelity. Toovercomethis,Liuetal. proposeanexpansionoftheKANstructure
toincludemultiplelayers,therebyincreasingbothitsdepthandbreadth. TheanalogytoMulti-Layer
Perceptrons(MLPs)becomesapparenthere, asjustlikeinMLPs, onceabasiclayerstructureis
defined—comprisinglineartransformationsandnonlinearities—Liuetal.extendthemodelbyadding
morelayers.
AKANlayer,suitableforadeeperarchitecture,isdefinedbyamatrixofunivariatefunctions:
Φ={ϕ }, p=1,2,...,n , q =1,2,...,n ,
q,p in out
where each function ϕ has trainable parameters. This enables the Kolmogorov-Arnold repre-
q,p
sentations, initially described as a composition of two such layers, to be expanded into deeper
configurations.
ForaconcreteandintuitiveunderstandingoftheKANarchitecture,consideritsrepresentationasan
arrayofintegers[n ,n ,...,n ],wheren denotesthenumberofnodesinthei-thlayer. Activation
0 1 L i
valuesandtheirtransformationsbetweenlayersaredefinedasfollows:
(cid:88)nl
x = ϕ (x ), (2)
l+1,j l,j,i l,i
i=1
3whereϕ connectsnodeioflayerlwithnodej oflayerl+1. ThissetupallowsLiuetal. tostack
l,j,i
deepKANlayers,facilitatingthemodelingofcomplexfunctionsthroughsuccessivetransformations
andsummations.
2.2 GraphConvolutionalNetworks
TheconventionalGCNsassumethatnodelabelsyarefunctionofbothgraphstructure(i.e.,adjacency
matrix A) and node features X, or more formally speaking y = f(X,A). A multi-layer Graph
ConvolutionalNetwork(GCN)ischaracterizedbythelayer-wisepropagationrule:
(cid:16) (cid:17)
H(l+1) =σ D˜−1 2A˜D˜− 21H(l)W(l) , (3)
whereA˜ = A+I istheadjacencymatrixofthegraphaugmentedwithself-connections,I is
N N
theidentitymatrix,andD˜ =(cid:80) A˜ . Here,W(l) representsthetrainableweightmatrixatlayer
ii j ij
l,andσ(·)denotesanactivationfunctionsuchastheReLUfunction. ThematrixH(l) ∈ RN×D
encapsulatestheactivationsinthel-thlayer,withH(0) =X.
InKipfandWelling(2017),theyshowedthattheabovepropagationruleismotivatedbyafirst-order
approximationoflocalizedspectralfiltersongraphs,aconceptpioneeredbyHammondetal.(2011)
and further explored by Defferrard et al. (2016). The application of this rule involves iterative
transformationsacrossthegraphstructure,incorporatingbothnodefeaturesandthetopologyofthe
graphtolearnusefulhiddenlayerrepresentationsinahighlyscalablemanner.
Forthesakeofillustration,weshowaconventionaltwo-layerGCNofagraphnodeclassification
with a symmetric adjacency matrix, whether binary or weighted, in Figure 1. We can perform a
preprocessingstepbycomputingAˆ=D˜−1 2A˜D˜− 21. Then,theforwardmodelisexpressedsuccinctly
as:
(cid:16) (cid:16) (cid:17) (cid:17)
Z =f(X,A)=softmax AˆReLU AˆXW(0) W(1) , (4)
whereW(0) ∈RC×H representstheweightmatrixthatmapsinputstoahiddenlayerwithH feature
maps,whileW(1) ∈RH×F mapshiddenlayerfeaturestotheoutputlayer. Thesoftmaxactivation
function is applied to each row and is defined as softmax(x ) = exp(xi) . For tasks involving
i (cid:80) iexp(xi)
semi-supervisedmulti-classclassification, thecross-entropyerroriscalculatedacrossalllabeled
examples by: L =
−(cid:80) (cid:80)F
Y lnZ , where Y denotesthe setof nodeindicesthat are
l∈YL f=1 lf lf L
labeled. Theweightsoftheneuralnetwork,W(0)andW(1),areoptimizedusingthegradientdescent
method. Thespatial-basedgraphrepresentationoftheembeddingsdescribedaboveispresentedin
Fig. 1(a),asintroducedbyYingetal.(2018).
3 ProposedGKANAchitectures
Inthismodel,theprocessofnodeclassificationleveragesboththegraphstructureandnodefeatures
throughsuccessivelayersoftransformationandnon-linearity,providingapowerfulmechanismfor
learningfrombothlabeledandunlabeleddatainasemi-supervisedsetting. Theeleganceofthis
approachliesinitssimplicityandeffectiveness,enablingthemodeltolearncomplexpatternsofnode
connectivityandfeaturedistributionefficiently.
WeproposetwosimplearchitecturesofGKAN,namedArchitecture1andArchitecture2,whichwe
elaborateuponinthefollowingsubsections.
3.1 GKANArchitecture1
Inthisarchitecture,theembeddingofnodesatlayerℓ+1arebasicallygeneratedbypassingthe
aggregated(e.g.,summation)nodeembeddingatlayerℓthroughKANLayer(ℓ).Moremathematically,
H(ℓ+1) =KANLayer(AˆH(ℓ) ) (5)
Archit.1 Archit.1
withH(0) = X. TheimplementationofKANLayerispresentedinAppendix. ConsideringL
Archit.1
numberoflayersforthisarchitectures,thentheforwardmodelispresentedasZ =softmax(H(L) ).
Archit.1
Thisisaspectral-basedrepresentation,theconstructionofembeddingofnodesindifferentlayersof
spatial-basedrepresentationofGKANArchitecture1ispresentedinFig. 1(b).
4Scheme OverviewofModelArchitecture
CONVOLVE(1)
A
TARGET NODE B
CONVOLVE(2)
C
B
A
h B(1) A
C ℎ!(#) B
A " C
F
h(2)
ℎ%(!)(#)
h(1)
E
D A C F
E
D
INPUT GRAPH h(2)
D A
(a)Overviewofatwo-layerGCNYingetal.(2018)architecture.
∅"=∑!"#$$!%!(")
-' =5
TARGET NODE B KAN Layer(2)
∅" -=
'
%) =# "# 1#0$,* %,*(,)
KAN Layer(1)
CA
B A
A C ℎ!(#$%) ℎ!(#) h( B1) B
F
hA
(2) ℎ%(#$%)
ℎ%(#) hC
(1) E
D
A ℎ'(#) DC
F
E
h(1)
D
INPUT GRAPH A
(b)Overviewofatwo-layerGKANArchitecture1.
KAN Layer(1)
A
TARGET NODE B KAN Layer(2) C
B A
A h(1)
C B B
A
C
D
F h A(2) h C(1) E
F
E D
h(1)
D
INPUT GRAPH A
(c)Overviewofatwo-layerGKANArchitecture2.
Figure1: Comparisonofdifferentmodelarchitectures.
3.2 GKANArchitecture2
Insteadofperformingtheaggregation(accordingtothenormalizedadjacencymatrix)andpassing
theaggregationthroughtheKANLayer,wefirstpasstheembeddingthroughtheKANLayer,thenag-
gregatingtheresultaccordingtothenormalizedadjacencymatrix. Formallyspeaking,theembedding
ofnodesatlayerℓ+1areasfollows,
H(ℓ+1) =AˆKANLayer(H(ℓ) ) (6)
Archit.2 Archit.2
with H(0) = X. Similar to the first architecture, the forward model would be Z =
Archit.2
softmax(H(L) ). Similarly, this is a spectral-based representation of the second architecture.
Archit.2
Theconstructionofembeddingofnodesindifferentlayersofspatial-basedrepresentationofGKAN
Architecture2ispresentedinFig. 1(c).
5
)8102(.lategniYNCG
1erutcetihcrANAKG
2erutcetihcrANAKG4 Experiments
Data: WeconsidertheCoradataset,acitationnetworkdescribedin Senetal.(2008),consistsof
2,708nodesand5,429edgeswherenodesrepresentdocumentsandedgessignifycitationlinks. It
encompasses7differentclassesandincludes1,433featuresperdocument. Thedistributionofthe
datasetisasfollows: thetrainingsetcontains1000samples,thevalidationsetcontains200samples,
andthetestsetincludes300samples. Corafeaturessparsebag-of-wordsvectorsforeachdocument,
andthecitationlinksamongthesedocumentsaretreatedasundirectededgestoconstructasymmetric
adjacencymatrix. Eachdocumenthasaclasslabel,andalthoughonly20labelsperclassareactively
usedfortraining,allfeaturevectorsareincorporated. Thesedetailsunderscorethestructuredand
characteristicnatureoftheCoradataset,whichiscrucialfortrainingandtestingwithinthenetwork.
OverviewofExperiments: Wepresentourexperimentsintwoparts. First,wecomparetheaccuracy
weobtainwiththetwoGKANarchitectureswiththeaccuracyobtainedbytheconventionalGCN,
overbothtrainandtestdata.Onesubtlebutimportantissuethatwetakeintoaccountwhencomparing
GCN with GKAN on a fair-basis is to ensure that the parameter sizes are the same. Second, we
examinetheimpactofvariousparametersforGKANssuchasnumberofhiddennodesandtheorder
andgridparametersfortheB-splinesusedforthelearnableunivariatefunctions.
4.1 ComparisonwithGCN
InordertofairlyevaluatetheperformanceofGKANcomparedtoGCN,weensurethatbothnetworks
haveanidenticalnumberoflayerswiththedimensionsspecifiedas
GCN:[d :h :C]
input GCN
GKANArchitecture1:[d :h:C] (7)
input
GKANArchitecture2:[d :h:C] (8)
input
whered ,h ,h,andC respectivelyrepresentthedimensionofinputfeatures,thedimension
input GCN
of hidden layers of GCN, the dimension of hidden layers of GKAN architectures, and the total
numberofclasses. ToensureafaircomparisonbetweenGKANandGCN,weequippedGCNwitha
higherh comparedtohtoapproximatelyequalizethetotalnumberofparametersinthetwo
GCN
schemes. ThetotalnumberoftrainableparametersofGCNandGKANArchitectures1andGKAN
Architecture2aremeasuredbytheparametercountingmoduleinPyTorch.
TheaccuracyofourproposedGKANfordifferentsettingsofk (thedegreeofthepolynomialin
KANsettings)andg(thenumberofintervalsinKANsettings)againstGCNKipfandWelling(2017)
onCoradatasetusingthefirst100featuresisshowninTable1. Wefurtherpresentthetheaccuracy
ofGKANarchitecturescomparedtoGCNKipfandWelling(2017)onCoradatasetusingthefirst
200featuresinTable2. TheresultsinthesetwotablesshowthatalltheGKANvariantsconsidered
perform better in terms of accuracy compared to a GCN with compable number of parameters.
Further,theysuggestthatarchitecture2generallyperformsbetter. Forthecaseof100features,the
bestGKANmodelshowsmorethan8%higheraccuracythantheconventionalGCNmodel;for200
features,thebestGKANmodelshowamorethan6%higheraccuracythantheconventionalGCN
model.
Table1: Architecturesandtheirperformanceonthefirst100featuresofCoraDataset.
Architecture #Parameters Test
GCN 22,147 53.50
hGCN=205
GKAN(Archit.1) 22,279 59.32
(k=1,g=10,h=16)
GKAN(Archit.2) 22,279 61.48
(k=1,g=10,h=16)
GKAN(Archit.1) 22,279 56.76
(k=2,g=9,h=16)
GKAN(Archit.2) 22,279 61.76
(k=2,g=9,h=16)
Inthefollowingparts,wemeasuretheperformanceofGCNaswellastheproposedGKANarchitec-
tures1and2onCoradatasetconsideringonlythefirst100features.
6Table2: Architecturesandtheirperformanceonthefirst200featuresofCoraDataset.
Architecture #Parameters Test
GCN 21,639 61.24
hGCN=104
GKAN(Archit.1) 21,138 63.58
(k=2,g=2,h=17)
GKAN(Archit.2) 21,138 64.10
(k=2,g=2,h=17)
GKAN(Archit.1) 20,727 67.44
(k=1,g=2,h=20)
GKAN(Archit.2) 20,727 67.66
(k=1,g=2,h=20)
4.1.1 AccuracyandLossValuesvs. Epochs
We set h = 16 and h = 100 to have a fair comparison between GKAN and
GCN (k=1,g=3,h=16)
GCN ensuringalmostthesamenumberoftrainableparameters. Forthissettings,the
(hGCN=100)
totalnumberoftrainableparametersofGCN,GKANArchitectures1andGKANArchitecture2are
10807,10295,and10295,respectively.
Figures2aand2brepresenttheaccuracyoftrainingandtestfortheGCN,andourproposedGKAN
architectureswithk =1andg =3onCoradatasetwiththefirst100features. Asitcanbeobserved,
GKANarchitecturesimprovestestaccuracyofGCNbyalargemarginof6%. Figures3aand3b
illustratetraininglossandtestlossfortheGCNaswellasourproposedGKANarchitecturesforthe
samesettingsofk =1andg =3. Asonecansee,GKANarchitecturesleadtoasharpdecreasein
thelossvalues,implyingthattheGKANarchitecturerequireslessnumberofepochstobetrained,
matchingpreviousobservationsLiuetal.(2024)aboutKANcomparedtoMulti-LayerPerceptron
(MLP).
(a)Trainingaccuracyofdifferentschemes. (b)Testaccuracyofdifferentschemes.
Figure2: AccuracycomparisonofGCNandGKANarchitecturesfork =1andg =3.
4.2 EvaluatingtheInfluenceofParametersonGKAN
Inthissection,weexploretheimpactoftheparametersg (splinegridsize),k (splinepolynomial
degree),andh(numberofhiddennodes)ontheperformanceofGKAN.Toisolatetheinfluenceof
eachparameter,wevaryonewhileholdingtheothersattheirdefaultvalues.Thedefaultsettingsforg,
k,andhare3,1,and16,respectively. Table3outlinestherangeofvaluestestedforeachparameter,
providingastructuredframeworkforourinvestigation. Thisapproachallowsustosystematically
determine how each parameter affects the overall effectiveness of our model. Given our earlier
observations,wefocusonGKANarchitecture2inthissection.
4.2.1 Effectofvaryinggridsizeg
Figures4aand4bshowtheaccuracyofGKANArchitecture2fordifferentvaluesofgridg (i.e.,
g =3,g =7,andg =11)andfixedk =1. Asonecansee,basedonthevalidationperformance,the
bestchoiceofgforthisproblemamongthesethreeisanintermediatevalueofg =7. Itappearsthat
7(a)Traininglossofdifferentschemes. (b)Testlossofdifferentschemes.
Figure3: LossvaluecomparisonofGCNandGKANarchitecturesfork =1andg =3.
Table3: RangeofValuesforParametersforthedegreeofthesplinepolynomialk,gridsizeforspline
g,andnumberofhiddennodesh,withdefaultvaluesinbold
Parameter Values
k {1,2,3}
g {3,7,11}
h {8,12,16}
whilethereissomebenefittoincreasingthegridsizefromg =3tog =7,ahighervalueofg =11
causesadeteriorationandresultsinaperformancethatiscomparabletog =3.
(a)TrainingaccuracyofGKANArchitecture2for (b)TestaccuracyofGKANArchitecture2fordif-
differentparametergandk=1. ferentparametergandk=1.
Figure4: AccuracycomparisonofGKANArchitecture2forg ∈{1,2,3},k =1,andh=16.
ThelossvaluesoftrainingandtestforGKANArchitecture2fordifferentvaluesofgridparameterg
andfixeddegreek =1arepresentedinfigures5aand5b,respectively.
4.2.2 Effectofvaryingthedegreeofpolynomialsk
WepresenttheaccuracyofGKANArchitecture2fordifferentvaluesofdegreek,rangingfromk =1
tok =3whilefixingthegridsizeg =3infigures6aand6b. Weseethatadegreevalueofonehas
thebestperformanceamongthisrangeofk,suggestingthepossibilityofunderlyingground-truth
functiontobepiece-wiselinear. Figures7aand7billustratetraininglossandtestloss,respectively.
8(a)TraininglossvaluesofGKANArchitecture2 (b)TestlossvaluesofGKANArchitecture2for
fordifferentparametergandk=1. differentparametergandk=1.
Figure5: LossvalueofGKANArchitecture2forg ∈{3,7,11}andk =1,andh=16.
(a)TrainingaccuracyofGKANArchitecture2for (b)TestaccuracyofGKANArchitecture2fork∈
k∈{1,2,3}whilefixingg=3. {1,2,3}andfixedg=3.
Figure6: AccuracyofGKANArchitecture2fork ∈{1,2,3},g =3,andh=16.
(a)TraininglossvaluesofGKANArchitecture2 (b)TestlossvaluesofGKANArchitecture2for
fork∈{1,2,3}andfixedg=3. k∈{1,2,3}andg=3.
Figure7: LossvalueofGKANArchitecture2fork ∈{1,2,3},g =3andh=16.
94.2.3 Effectofvaryingthesizeofhiddenlayer
Figures 8a and 8b illustrate the accuracy of GKAN Architecture 2 for the size of hidden layer
h∈{8,12,16}whilefixingk =1andg =3.Moreover,Figures9aand9bdemonstratetheaccuracy
ofGKANArchitecture2forthesamerangeofparametersover600epochs.Theseresultssuggestthat
ahiddenlayersizeofh=12isparticularlyeffectiveintheinitialphasesoftrainingandultimately
achievesalmostthesametestperformanceash=16.
(a)TrainingaccuracyofGKANArchitecture2for (b)TestaccuracyofGKANArchitecture2h ∈
h∈{8,12,16}whilefixingg=3andk=1. {8,12,16}whilefixingg=3andk=1.
Figure8: AccuracyofGKANArchitecture2forh∈{8,12,16},g =3andk =1.
(a)TrainingaccuracyofGKANArchitecture2for (b)TestaccuracyofGKANArchitecture2h ∈
h∈{8,12,16}whilefixingg=3andk=1. {8,12,16}whilefixingg=3andk=1.
Figure9: AccuracyofGKANArchitecture2forh∈{8,12,16},g =3andk =1over600epochs.
WealsopresentthethelossvaluesoftrainingandtestofGKANArchitecture2forh∈{8,12,16}
whilefixingdegreeofpolynomialstok =1andgridsizeg =3infigures10aand10b,respectively.
5 Conclusions
Weconsideredhowtoapplytheideaoflearnablefunctionsfromtherecently-proposedKolmogorov-
ArnoldNeuralNetworks(KANs)tograph-structureddata. Inthiswork,wehavepresented,forthe
firsttime,twodifferentarchitecturesforGraphKolmogorov-ArnoldNetworks(GKANs). Empirical
evaluationsontheCoradatasetshowthatGKANsattainsignificantlybetterparameter-efficiencythan
conventionalGCN,yieldinghigheraccuracyforcomparableparameterssizes. Wealsoexamined
howvariousparameterssuchasnumberofhiddennodes,gridsize,andthesplineorderparameter
impactperformance.
Basedontheevidenceoftheresultspresentedinthispaper,webelievethatGKANsopenanewavenue
ingraphrepresentationlearningandcouldserveasthefoundationforallkindsofapproachesthat
10(a)TraininglossvaluesofGKANArchitecture2 (b)TestlossvaluesofGKANArchitecture2for
forh∈{8,12,16}whilefixingg=3andk=1. h∈{8,12,16}whilefixingg=3andk=1.
Figure10: LossvalueofGKANArchitecture2forh∈{8,12,16},g =3andk =1.
previouslyutilizedMLPsattheircoresuchasGCNs,GAT,GraphAutoencoders,GraphTransformers
andmanyothergraphdeeplearningschemes. Promisingavenuesforfutureworkincludeexploring
and evaluating extensions based on all these approaches using KAN, over more comprehensive
datasets. GKANcurrentlyinheritthepropertyofpresent-generationKANinthatthetrainingprocess
isratherslow,and Liuetal.(2024)leavetofutureworkthetaskofoptimizingtrainingtime;advances
inalternativelearningapproachesandarchitecturesforKANcouldalsobeappliedtoGKANinthe
future.
Acknnowledgements:
This work was supported in part by Army Research Laboratory under Cooperative Agreement
W911NF-17-2-0196. The authors acknowledge the Center for Advanced Research Computing
(CARC) at the University of Southern California for providing computing resources that have
contributedtotheresearchresultsreportedwithinthispublication. URL:https://carc.usc.edu
References
VladimirIArnold.1957. Onfunctionsofthreevariables. Dokl.Akad.NaukSSSR114,679–681
(1957).Englishtranslation: Am.Math.Soc.Transl.(2),28,51–54(1963)(1957).
VladimirIArnold.1958. Ontherepresentationoffunctionsofseveralvariablesasasuperposition
offunctionsofasmallernumberofvariables.
VladimirIgorevichArnold.1959. Ontherepresentationofcontinuousfunctionsofthreevariablesby
superpositionsofcontinuousfunctionsoftwovariables. MatematicheskiiSbornik90,1(1959),
3–74.
JürgenBraunandMichaelGriebel.2009. OnaConstructiveProofofKolmogorov’sSuperposition
Theorem. ConstructiveApproximation30(2009),653–675. https://api.semanticscholar.
org/CorpusID:5164789
Shaked Brody, Uri Alon, and Eran Yahav. 2021. How Attentive are Graph Attention Networks?
ArXivabs/2105.14491(2021). https://api.semanticscholar.org/CorpusID:235254358
MichaelM.Bronstein,JoanBruna,YannLeCun,ArthurSzlam,andPierreVandergheynst.2017.
GeometricDeepLearning: GoingbeyondEuclideandata. IEEESignalProcessingMagazine34,4
(2017),18–42. https://doi.org/10.1109/MSP.2017.2693418
Deng Cai and Wai Lam. 2019. Graph Transformer for Graph-to-Sequence Learning. ArXiv
abs/1911.07470(2019). https://api.semanticscholar.org/CorpusID:208138227
11Dexiong Chen, Leslie O’Bray, and Karsten M. Borgwardt. 2022. Structure-Aware Transformer
forGraphRepresentationLearning.InInternationalConferenceonMachineLearning. https:
//api.semanticscholar.org/CorpusID:246634635
MichaëlDefferrard,XavierBresson,andPierreVandergheynst.2016. Convolutionalneuralnetworks
ongraphswithfastlocalizedspectralfiltering.InProceedingsofthe30thInternationalConference
onNeuralInformationProcessingSystems(Barcelona,Spain)(NIPS’16).CurranAssociatesInc.,
RedHook,NY,USA,3844–3852.
FedericoGirosiandTomasoPoggio.1989. Representationpropertiesofnetworks: Kolmogorov’s
theoremisirrelevant. NeuralComputation1,4(1989),465–469.
William L. Hamilton, Rex Ying, and Jure Leskovec. 2018. Representation Learning on Graphs:
MethodsandApplications. arXiv:1709.05584[cs.SI]
David K. Hammond, Pierre Vandergheynst, and Rémi Gribonval. 2011. Wavelets on graphs via
spectral graph theory. Applied and Computational Harmonic Analysis 30, 2 (2011), 129–150.
https://doi.org/10.1016/j.acha.2010.04.005
Kurt Hornik, Maxwell B. Stinchcombe, and Halbert L. White. 1989. Multilayer feedforward
networks are universal approximators. Neural Networks 2 (1989), 359–366. https://api.
semanticscholar.org/CorpusID:2757547
RobertHuben,HoagyCunningham,LoganRiggsSmith,AidanEwart,andLeeSharkey.2024.Sparse
AutoencodersFindHighlyInterpretableFeaturesinLanguageModels.InTheTwelfthInternational
ConferenceonLearningRepresentations. https://openreview.net/forum?id=F76bwRSLeK
ThomasKipfandMaxWelling.2016. VariationalGraphAuto-Encoders. ArXivabs/1611.07308
(2016). https://api.semanticscholar.org/CorpusID:14249137
ThomasN.KipfandMaxWelling.2017. Semi-SupervisedClassificationwithGraphConvolutional
Networks.InInternationalConferenceonLearningRepresentations. https://openreview.
net/forum?id=SJU4ayYgl
A.N. Kolmogorov. 1956. On the representation of continuous functions of several variables as
superpositionsofcontinuousfunctionsofasmallernumberofvariables.InDokl.Akad.Nauk.
A.N. Kolmogorov. 1957. On the representation of continuous functions of many variables by
superpositionofcontinuousfunctionsofonevariableandaddition.InDokl.Akad.Nauk,Vol.114.
953–956.
Ming-Jun Lai and Zhaiming Shen. 2023. The Kolmogorov Superposition Theorem can
Break the Curse of Dimensionality When Approximating High Dimensional Functions.
arXiv:2112.09963[math.NA]
Yuan Li, Xiaodan Liang, Zhiting Hu, Yinbo Chen, and Eric P. Xing. 2019. Graph Transformer.
https://openreview.net/forum?id=HJei-2RcK7
Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Sol-
jacˇic´, Thomas Y. Hou, and Max Tegmark. 2024. KAN: Kolmogorov-Arnold Networks.
arXiv:2404.19756[cs.LG]
GrégoireMialon,DexiongChen,MargotSelosse,andJulienMairal.2021. GraphiT:EncodingGraph
StructureinTransformers. ArXivabs/2106.05667(2021). https://api.semanticscholar.
org/CorpusID:235390675
FedericoMonti,MichaelM.Bronstein,andXavierBresson.2017. Geometricmatrixcompletion
withrecurrentmulti-graphneuralnetworks.InProceedingsofthe31stInternationalConference
onNeuralInformationProcessingSystems(LongBeach, California, USA)(NIPS’17).Curran
AssociatesInc.,RedHook,NY,USA,3700–3710.
Guillaume Salha-Galvan, Romain Hennequin, and Michalis Vazirgiannis. 2020. Simple and
Effective Graph Autoencoders with One-Hop Linear Models. In ECML/PKDD. https:
//api.semanticscholar.org/CorpusID:210839504
12Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-
Rad. 2008. Collective Classification in Network Data. AI Magazine 29, 3 (Sep. 2008), 93.
https://doi.org/10.1609/aimag.v29i3.2157
Rianne van den Berg, Thomas N. Kipf, and Max Welling. 2017. Graph Convolutional Matrix
Completion. arXiv:1706.02263[stat.ML]
AshishVaswani,NoamM.Shazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,
Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Neural Information
ProcessingSystems. https://api.semanticscholar.org/CorpusID:13756489
PetarVelicˇkovic´,GuillemCucurull,ArantxaCasanova,AdrianaRomero,PietroLiò,andYoshuaBen-
gio.2018. GraphAttentionNetworks.InInternationalConferenceonLearningRepresentations.
https://openreview.net/forum?id=rJXMpikCZ
RexYing,RuiningHe,KaifengChen,PongEksombatchai,WilliamL.Hamilton,andJureLeskovec.
2018. GraphConvolutionalNeuralNetworksforWeb-ScaleRecommenderSystems. Proceedings
ofthe24thACMSIGKDDInternationalConferenceonKnowledgeDiscovery&DataMining
(2018). https://api.semanticscholar.org/CorpusID:46949657
JiaxuanYou,RexYing,XiangRen,WilliamL.Hamilton,andJureLeskovec.2018. GraphRNN:
GeneratingRealisticGraphswithDeepAuto-regressiveModels.InInternationalConferenceon
MachineLearning. https://api.semanticscholar.org/CorpusID:46937309
A KANLayerImplementationDetails
TheKANlayer,althoughappearingsimpleinequation(2),presentschallengesforoptimization. The
strategiesemployedbyLiuetal.(2024)toovercomethesechallengesinclude:
1. ResidualActivationFunctions: Theactivationfunctionϕ(x)combinesabasisfunction
b(x),reminiscentofresidualconnections,andasplinefunction:
ϕ(x)=w b(x)+w spline(x). (9)
b s
Thebasisfunctionistypicallydefinedas:
x
b(x)=silu(x)= . (10)
1+e−x
ThesplinecomponentisexpressedasaweightedsumofB-splines:
(cid:88)
spline(x)= c B (x), (11)
i i
i
wherec arecoefficientsthatcanbeadjustedduringtraining. Notably,w andw couldbe
i b s
absorbedintob(x)andspline(x)butarekeptseparatetofine-tunethefunction’samplitude.
2. Initialization Scales: The activation functions are initialized such that w = 1 and
s
spline(x) ≈ 0. w is set based on the Xavier initialization scheme, traditionally used
b
forinitializinglayersinMLPs.
3. DynamicSplineGridUpdates: Thesplinegridsareupdateddynamicallybasedonthe
inputactivations. Thismodificationcaterstotheinherentboundednatureofsplinefunctions,
accommodatingtheevolutionarynatureofactivationvaluesduringtraining.
13