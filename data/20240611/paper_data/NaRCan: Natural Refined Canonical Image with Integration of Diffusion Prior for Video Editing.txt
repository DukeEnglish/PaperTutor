NaRCan: Natural Refined Canonical Image with
Integration of Diffusion Prior for Video Editing
Ting-HsuanChen JiewenChan Hau-ShiangShiu
Shih-HanYen Chang-HanYeh Yu-LunLiu
NationalYangMingChiaoTungUniversity
Abstract
Weproposeavideoeditingframework,NaRCan,whichintegratesahybriddefor-
mationfieldanddiffusionpriortogeneratehigh-qualitynaturalcanonicalimages
torepresenttheinputvideo. Ourapproachutilizeshomographytomodelglobal
motion and employs multi-layer perceptrons (MLPs) to capture local residual
deformations,enhancingthemodel’sabilitytohandlecomplexvideodynamics.
Byintroducingadiffusionpriorfromtheearlystagesoftraining,ourmodelen-
suresthatthegeneratedimagesretainahigh-qualitynaturalappearance,making
the produced canonical images suitable for various downstream tasks in video
editing,acapabilitynotachievedbycurrentcanonical-basedmethods. Further-
more, we incorporate low-rank adaptation (LoRA) fine-tuning and introduce a
noiseanddiffusionpriorupdateschedulingtechniquethatacceleratesthetraining
processby14times. Extensiveexperimentalresultsshowthatourmethodoutper-
formsexistingapproachesinvariousvideoeditingtasksandproducescoherent
andhigh-qualityeditedvideosequences. Seeourprojectpageforvideoresults:
koi953215.github.io/NaRCan_page.
1 Introduction
Videoeditinghasalwaysbeenafascinatingresearcharea. Forexample,styletransfertransformsthe
originalvideointoacompletelynewstyle,enrichingtheviewingexperience. Othertasksinclude
dynamicsegmentationandhandwriting,whichalldemonstratethebroadapplicationvalueofvideo
editingacrossvariousfields. Currently,diffusionmodeltechnologyisbecomingincreasinglymature
andisknownforitspowerfulgenerativecapabilitiesandfrequentuseinvideoediting. However,in
video-to-videotasks,maintainingtemporalconsistencyiscrucial. Generally,diffusionmodelsthat
arenotspeciallyprocessedcannotgeneratevideosequenceswithsufficienttemporalconsistency.
Thishasledtonumerouspapersaddressingthisissue,attemptingtomakediffusionmodelsproduce
high-qualityvideosequenceswhileensuringsequenceconsistency.
Yet,eveniftheproblemoftemporalconsistencyissolved,diffusion-basedmethods[16,4,19,8]
stillfacechallengesinadaptingtocertaindownstreamtasks,suchashandwriting. Thisiswhere
canonical-basedmethodscomeintoplay. Canonical-basedmethods[3,1,5,44]generateacanonical
imagerepresentingallthevideoinformation,meaningthateditingthissingleimageequatestoediting
theentirevideo. Thus,thesemethodscaneasilyadapttovariousvideoeditingtasks.
Itisimportanttonotethatthisisonlytrueifthecanonicalimageisahigh-quality,naturalimage. If
thecanonicalimageisnotnatural,itlosesanyeditingvalue. Weobservedthatexistingcanonical-
basedmethodsdonotincorporateanyconstraintstoensurethecanonicalimageisofhighquality
andnatural. Toaddressthisissue,weproposeNaRCan,anovelhybriddeformationfieldnetwork
architecturethatintegratesdiffusionpriors(Figure1)intoourtrainingpipeline(Figure2)toensure
thegenerationofhigh-quality,naturalcanonicalimagesacrossvariousscenarios. Throughaseriesof
experiments,wedemonstratethatNaRCanoutperformsexistingvideoeditingmethods.
4202
nuJ
01
]VC.sc[
1v32560.6042:viXraVideo (a)Video
Denoising U-Net
Representation Editing
Video (b)Dynamic
Denoising U-Net
Representation Segmentation
Video (c)Style
Denoising U-Net
Representation Transfer
UnnaturalCanonical DiffusionPrior NaturalCanonical
InputVideoSequence Propagated Edits
Figure1: Videorepresentationwithdiffusionprior. GivenanRGBvideo,wecanrepresentthe
videousingacanonicalimage. However,thecanonicalimageandreconstructiontrainingprocess
focusesonlyonreconstructionqualityandcouldproduceanunnaturalcanonicalimage. Thiscould
causeproblemswithdownstreamtaskssuchasprompt-basedvideoediting. Inthebottomexample,
ifthehandisdistortedinthecanonicalimage,theimageeditor,suchasControlNet[73],maynot
recognizeitandcouldintroduceanirrelevantobjectinstead. Inthispaper,weproposeintroducingthe
diffusionpriorfromaLoRA[18]fine-tuneddiffusionmodeltothetrainingpipelineandconstraining
thecanonicalimagetobenatural. Ourmethodfacilitatesseveraldownstreamtasks,suchas(a)video
editing,(b)dynamicsegmentation,and(c)videostyletransfer.
Ourmaincontributionsarethree-fold:
• Wehavedesignedanoveldeformationfieldstructuretorepresentobjectvariationsthrough-
outanentirescene. Comparedtoothercanonical-basedmodels,ourmodeldemonstrates
superiorexpressivecapabilityandfasterconvergencespeed.
• Wehaveeffectivelyintegratedthediffusionpriorintoourpipeline,enablingourmethodto
generatehigh-qualitynaturalcanonicalimagesinanyscenario. Additionally,wedesigneda
dynamicschedulingmethodthatsignificantlyacceleratestheentiretrainingprocess.
• Wethoroughlyevaluateourmethodtoshowthestate-of-the-artvideoeditingperformance.
2 RelatedWork
Implicitneuralrepresentation. Implicitneuralrepresentation[42]usingcoordinate-basedMLPis
anoutstandingwaytorepresentavideo,capableofobtainingacanonicalimagetorepresenttheentire
video[44]. Recentmethodsemployhashgridencoding[41]orpositionalencoding[39]combined
withMLP.Theapproachin[5]ismoreeffectiveathandlingspatialinformation,buttheresulting
canonicalimageexhibitsseveredistortionandwarpingwithin-the-wildvideos[11]. Therefore,we
proposeahybriddeformationfieldmethodcomposedofhomography[9]andresidualdeformation
MLP.Thismodeldesignfitsthedeformationinformationinvideosbetterthanexistingmethods.
Consistentvideoediting. Therearegenerallythreeapproachestovideoediting: (1)propagation-
based,(2)layeredrepresentation-based,and(3)canonical-based. Thefirstapproach,propagation-
based,focusesonpropagatinginformationacrossframes[34,20,22,23,56,62,66]. Thismethod
caneasilyproduceinaccurateresultsduetoocclusionsanderrorpropagation. Thesecondapproach,
layeredrepresentation-based,separatesavideointoforegroundandbackground,obtainingcanonical
images for both [35, 25, 71, 33, 36, 37]. We can edit the entire video by editing these canonical
imagesandthensynthesizingthevideoafterward. However,thismethodheavilyreliesonmasks. If
themask-RCNNcapturesincorrecttargetsinthepreprocessingstage,thefittedforegroundcanbe
incorrect,especiallyinsceneswithlargecameramovement.
The third approach, canonical-based, typically uses MLP to obtain the deformation information
ofeachpixeltoformacanonicalimage[44]. Transferringthevideotocanonicalspacemaintains
temporalconsistencywhileeditingandsupportsvariousdownstreamtasks,suchassuper-resolution
2and segmentation. CoDeF [44] uses this approach, but canonical images can deform severely in
videoswithsignificantcameraorobjectmovement. CoDeFsuggeststhatusingagroupmodelcould
resolvetheseissues. However,usinggroupCoDeFrequiresmasksfortrainingdataobtainedfrom
SAM-track [12]. Incorrect masks for training data can result in an unnatural canonical image of
thevideo. Evenwithcorrectmasksfortheforegroundobject,thecanonicalimagemightstillbe
corrupted,renderingtheseimagesineffectiveforvideoediting.
Videoprocessingviagenerativemodels. SomeworksutilizeGANinversion[64,76,59,72,47,
26]toeditimagesorvideos. Today,numerousgenerativemodelsexistforeditingimages. Some
methods,suchasGLIDE[43],DALL-E[54,53],stablediffusion[55],andImagen[57],aretrained
onmillionsofimages,resultinginincrediblegenerativeabilities. OthermethodslikeSDEdit[38],
ControlNet[73],andLDM[2]useconditionstoachievebettereditingresults.Instruction-basedvideo
editingmethodslikeInstructPix2Pix[3]andanotherwork[10]oftenyieldsub-optimalresultsfor
differenteditingoperations.TechniqueslikeLoRA[18]canassistinfine-tuningtofindbetterweights
forediting. Additionally,manyzero-shotdiffusion-basedmethods[70,16,74,4,8,19,27]donot
requiremodeltrainingbutstillneedconstraintstomaintaintemporalconsistency.Toaddresstemporal
consistencyconcerns,mostmethodslikeTune-A-Video[68],Text2Video-Zero[27],FateZero[51],
andVid2Vid-Zero[65]incorporatecross-attentionmechanisms. Someworksproposetrainingfor
videoediting,suchasImagenVideo[17]andMake-A-Video[58],buttheserequirelargedatasets
and significant computational resources. Unlike these methods, MeDM [13] uses a flow-coding
algorithmtosolvethisproblem. Canonical-baseddesigndoesnotrequireanothermechanismto
maintaintemporalconsistency,however. Oncethecanonicalimageisedited,thechangescanbe
propagatedtoeveryframeusingadeformationfield.
Liftingthenaturalnessofcanonicalimagebydiffusionmodels. Thediffusionpriorhasbeen
appliedinvariousdomains. Reconfusion[69]isafew-shotnovelviewsynthesisworkthatintroduces
adiffusionmodelbeforeoptimizingsamplednovelviews. Dreamfusion[49], atext-to-3Dwork,
introducesscoredistillationsampling(SDS)loss,referencinga2Ddiffusionmodeltooptimize3D
outputs. Thisapproachinspiresustoutilizethediffusionmodeltoimproveperformance. Other
methodslike[6,7,21,40,32,60,63,67,77]alsoemploydiffusionpriorfortext-to-3Dtasks.
Several diffusion models focus on text-to-image generation [52, 55, 57, 61]. Additionally, some
diffusionmodelscanrefinecorruptedimagestomakethemappearmorenatural. Weproposeadding
a diffusion model to our pipeline (Figure 2) to enhance the naturalness of our canonical images.
Our goal is to improve the restoration of canonical images using the diffusion model. While we
create canonical images using a hybrid deformation field, this method might not always deliver
optimal performance, especially in scenarios with dramatic motion changes or severe non-rigid
transformations. The capabilities of the hybrid deformation field are still limited in such cases.
Therefore, we aim to introduce a diffusion model to make our canonical images more natural,
enhancingtheeffectivenessofvideoediting. Thisresearchhassignificantpracticalimplicationsas
itcanimprovethequalityandrealismofvideoediting,benefitingvariousindustriessuchasfilm
production,advertising,andvirtualreality.
3 Method
In this section, we first introduce our hybrid deformation modeling by combining homography
anddeformationMLPinSection3.1. Next,weelaborateonhowweintegratethediffusionprior
from a LoRA fine-tuned latent diffusion model to ensure the naturalness of our canonical image
representationinSection3.2. Finally,weprovideanadditionalwaytoimprovethequalityofour
videorepresentationbyseparatingmultiplecanonicalimagesanddescribingthenecessarychanges
fordownstreamtasksinSection3.3. Figure2showsourproposedframework.
3.1 HybridDeformationFieldforDeformationModeling
Traditionalmethodsoftenrelyondirectpredictionsof∆uand∆v,whichmeansthedisplacementof
pixelpointsu,vattimet,byusinganMLPg(·,·,·)andquerytheRGBcolorbyanothercanonical
imageMLPf(·,·):
∆u, ∆v =g(u, v, t), [R, G, B]=f(u+∆u, v+∆v), (1)
supplementedwithaTVFlowregularizationtermtopreventoverfittingbylimitingitsexpressive
capacity. However, this regularization term restricts the model’s ability to express itself without
3“[V]” T Ee nx ct oder Denoising U-Net MLP (𝑢′,𝑣′) MLP “[V]” T Ee nx ct oder Denoising U-Net
LoRA LoRA Residual MLP Canonical MLP LoRA LoRA Style Transfer
Multistep
Finetuning T
. Unnatural Canonical Image Noise Scheduling NaturalCanonical Image Dynamic Segmentation
H
(𝑢,𝑣,𝑡)
MSE Loss
W
Input Video Sequence Homography Video Editing
(a) LoRAFinetuning Reconstruction Loss (b) Video Representation (c) Diffusion Prior (d) Applications
Figure2: Ourproposedframework. Givenaninputvideosequence,ourmethodaimstorepresent
thevideowithanaturalcanonicalimage,whichisacrucialrepresentationforversatiledownstream
applications. (a)First, wefine-tunetheLoRAweightsofapre-trainedlatentdiffusionmodelon
the input frames. (b) Second, we represent the video using a canonical MLP and a deformation
field,whichconsistsofhomographyestimationandresidualdeformationMLPfornon-rigidresidual
deformations. By relying entirely on the reconstruction loss, the canonical MLP often fails to
representanaturalcanonicalimage,causingproblemsfordownstreamapplications. E.g.,image-to-
imagetranslationmethodssuchasControlNet[73]maynotbeabletorecognizethatthereisatrain
inthecanonicalimage. (c)Therefore,weleveragethefine-tunedlatentdiffusionmodeltoregularize
andcorrecttheunnaturalcanonicalimageintoanaturalone. Specifically,wesophisticallydesigna
noiseschedulingcorrespondingtotheframereconstructionprocess. (d)Thenaturalandartifacts-free
canonical image can then be facilitated to various downstream tasks such as video style transfer,
dynamicsegmentation,andediting,suchasaddinghandwrittencharactersof“NaRCan”.
providingadditionalinsights.Therefore,weproposeahybriddeformationfieldarchitecturecomposed
ofatrainablehomographymatrixH(u,v,t)andresidualdeformationMLP:
u′, v′ =H(u, v, t)+g(u, v, t), [R, G, B]=f(u′, v′). (2)
UnlikeTVFlow,homographyprovidesglobaldisplacementinformationasheuristicinformationfor
thesubsequentresidualdeformationMLP.ThisenablestheresidualdeformationMLPtolearnand
expressthedeformationfieldmoreaccuratelyandeffectively(Figure9(a)).
3.2 DiffusionPrior
Thecanonicalimageencompassesallinformationwithintheentirevideoandcanbereconstructedfor
eachoriginalvideoframeusingthedeformationfieldoutlinedinSection3.1. Thus,editingsolelythe
canonicalimageyieldsatemporallycoherenteditedvideo. However,editingtaskssuchasdrawing
orwritingonobjectsoreditingbasedonControlNet[73]requireanaturalimageasinputtoproduce
meaningfuleditedimages. Existingcanonicalbasemethodsdonotincorporatemechanismstoensure
thatthegeneratedimageisnatural;instead,theyrelysolelyonthemodel’sabilitytolearnanatural
image. However,whenencounteringscenarioswithcameramovementorsignificantchangesinvideo
content,theseexistingtechniquescannotadapttosuchdrasticvariations. Themodelmaygenerate
acanonicalimagethatisnearlyimpossibletoedit(renderingitdevoidofanysubsequentvalue).
Toaddressthesechallenges,weintroducediffusionpriors,whichsuccessfullymitigatethisissue.
Ourmethodcangeneratehigh-qualitycanonicalimagesthroughdiffusionpriors,providingvaluable
inputsforvariousvideoeditingtasks.
LoRAfine-tuning. Toenhancethecurrentdiffusionmodel’sabilitytorepresentallvideocontent
better,weintroducedaspecialtokenspecifictothisscene. Wethenfine-tunedtheLoRAweightof
thepre-traineddiffusionmodel. Thisensuresthatthediffusionmodelgenerateshigh-qualitynatural
canonicalimagestailoredtothetestingsequenceratherthanrandomlygeneratingnaturalimagesthat
donotbelongtothescene.
Noise and diffusion prior update scheduling. While the hybrid deformation field technique
mentionedinSection3.1alreadyproducesbettercanonicalimagesthanotherexistingmethods,itstill
needstoimprove. AsFigure9(b)depicts,canonicalimagesgeneratedsolelyrelyingonhomography
andresidualdeformationMLPstillexhibitvariousdegreesofdistortionandunnaturalcharacteristics.
Therefore,integratingdiffusionpriorsbecomesimperative. Weextractthecanonicalregioncurrently
4lacin
… …
o
n
aC
1,000≤Step ≤3,000 3,000 <Step ≤5,000 Step >5,000
Early (Updating DM prior target every 10 steps) (Updating DM prior target every 100 steps) (Updating DM prior target every 2000 steps) Late
Adding Adding Adding
severenoise moderatenoise slightnoise
Diffusion Model
te
gr
Ta
… …
Figure3: Noiseanddiffusionpriorupdatescheduling. Initially,ourmodelfitsobjectoutlines
before the fields converge and without the diffusion prior, resulting in unnatural elements in the
canonicalimageduetocomplexnon-rigidobjects.Uponintroducingthediffusionpriorwithincreased
noiseandupdatefrequency,themodellearnstogeneratenatural,high-qualityimages,leadingto
convergence. Thus,thestrengthofnoiseandtheupdatefrequencywillalsodecrease. Moreover,it’s
worthmentioningthatupdateschedulingcutstrainingtimefrom4.8hoursto20minutes.
observedbythemodelandcalculatediffusionlosswiththetargetimagegeneratedbythediffusion
modeltoensurethegenerationofnaturalcanonicalimages. However,generatingatargetimageat
eachstepwouldsignificantlyprolongthetrainingphase. Hence,weproposeahierarchicalupdate
schedulingtoacceleratetheprocess,whichisshowninFigure3. Intheinitialstagesoftraining,
whenthedeformationfieldhasnotyetconverged,moresubstantialnoiseisintroducedtoallowthe
diffusionmodeltodominatethesceneofthecanonicalimage. Simultaneously, thefrequencyof
generatingtargetimagesneedstobedenser.Astrainingprogressesandthedeformationfieldbecomes
morestable,thenoiseintensityandfrequencyofgeneratingtargetimagesdecreaseaccordingly. This
hierarchicalschedulingapproachensuresthatthefinalcanonicalimageapproachesthequalityof
per-stepupdateswhilespeedingupthetrainingprocessby14times(4.8hoursto20minutes.) We
optfordiffusionlossoverSDSbecauseusingSDS,asmentionedintheReconfusion[69]paper,is
morepronetogenerateartifacts.
3.3 SeparatedNaRCan
When encountering overly complex scenes, relying solely on a single natural canonical image
representing the entire scenario is impractical and unrealistic. Hence, we need to segment the
originalvideointomultiplesegments{S ,...,S }andtraindedicatedresidualdeformationMLPs
1 k
{R ,...,R }foreachsegmenttoobtainknaturalcanonicalimages{C ,...,C }. Itisworthnoting
1 k 1 k
thatS andS haveanoverlap,referredtoastheoverlapwindow. Frameswithinthisregionare
i i+1
obtainedusinglinearinterpolationshowninFigure4. Thismethodensuresthatexcellenttemporal
consistencyismaintainedwhenswitchingfromcanonicalimageC toC . Table 1demonstrates
i i+1
thatourtemporalconsistencysurpassesallexistingmethodsevenaftersegmentation. Additionally,
we adopt different processing approaches for various downstream tasks to ensure that Separated
NaRCancanadeptlyadapttothesetasks.
Styletransfer. Withmultiplecanonicalimagesobtained,weutilizethegridtrick[14,24]toensure
sufficientconsistencyinstyleandcontentacrossthek canonicalimages. Specifically,weconcat
thecanonicalimagesintoalargerimage(with2×2canonicalimages)anduseControlNet[73]to
performtext-guidedimageediting.
Video editing. When addressing video editing tasks such as handwriting, we leverage the pre-
trainedopticalflowmodelstocomputetheflowbetweenthekcanonicalimagesandusethisflowto
warptheeditingcontentfromC totheC image.
1 k
5100%
75% ◼︎Canonical1
50%
25% ◼︎Canonical2
0%
Frame 1 Frame 2 Frame 3 Frame 4 Frame 5
Canonical image 1 Frame sequence Canonical image 2
Figure 4: Linear interpolation. After using the grid trick [18] to obtain the highly consistent
canonical images C and C , we interpolate all frames within the overlap window. As time
k k+1
progresses,theweightforreconstructingeachframegraduallyshiftsfromreferencingC tosolely
k
referencingC . Weachieveeditingresultswithremarkabletemporalconsistencythroughthis
k+1
linearinterpolationapproach. Pleaserefertooursupplementarymaterialformorevideoresults.
Table1: QuantitativeresultsontheBalanceCC[15]dataset. Thereare100videosinBalanceCC.
ToensurearepresentativedistributionsimilartoBalanceCC,werandomlyselect50videosfrom
BalanceCC and calculate warping and interpolation errors, which are the metrics for temporal
consistency. Ourmethodoutperformsthesebaselinemethodsintermsoftemporalconsistency.
Method Venue Warpingerror↓ Interpolationerror↓
Hashing-nvd[5] ICCV2023 0.594 9.204
CoDeF[44] CVPR2024 0.486 8.721
MeDM[13] AAAI2024 0.367 9.941
Ours - 0.364 8.365
4 Experiments
4.1 ExperimentalSetup
Weconductexperimentstounderscoretherobustnessandversatilityofourproposedmethod. Our
representationisrobustwithavarietyofdeformations,encompassingrigidandnon-rigidobjects,as
wellascomplexscenariossuchassmogandwaves. Wecommencetheintroductionofthediffusion
model at the 1000th iteration. From iteration 1000 to iteration 3000, the noise intensity is set at
0.4,andthetargetimagegenerationfrequencyisevery10iterations. Subsequently,spanningfrom
iteration3001toiteration5000,thenoiseintensityisadjustedto0.3,withthetargetimagegeneration
frequency occurring every 100 iterations. Beyond the 5000th iteration mark, the noise intensity
decreasesto0.2,andthetargetimageisgeneratedevery2000iterations. Thetotaliterationis12000
iterations,andFigure3isshowntovisualizetheprocessofnoisescheduling. OnasingleNVIDIA
RTX4090GPU,theaveragetrainingdurationisapproximately20minuteswhenutilizing100video
frames. WhenevaluatingthetemporalconsistencyinTable1,wecompareourseparatedNaRCan
withothercomparedmethodsbysettingk =3,i.e.,werepresentthesequenceusingthreecanonical
images. Byadjustingthetrainingparametersaccordingly,theoptimizationdurationcanbevaried
from20minutestoanhour.
4.2 Evaluation
Videoediting. WerunourmethodCoDeF[44],Hashing-nvd[5],CCEdit [15],andMeDM[13]on
theDAVIS[48]andBalanceCCBenchmarkthatCCEditproposedforevaluatingtheresultsofvideo
editing. InFigure5,weshowthecomparisonofvisualresults. BalanceCCBenchmarkprovideda
unifiedtextpromptforeachscene,andweensuredthatallvideosfromtheBalanceCCBenchmark
werekeptwithin100frames. WeutilizedControlNet[73]toeditthevideousingtheunifiedtext
promptprovidedbyBalanceCCBenchmarkforafaircomparison. WeexecutedMeDMandCCEdit
withintheirprovidedenvironmentsettingsandpre-trainedmodels. Moreover,Hashing-nvdisavideo
decompositionworkthatoutputstwoimagesrepresentingforegroundandbackground. Tomaintain
6le
m
a
C
Input Video Ours CoDeF Hashing-nvd MeDM CCEdit
Text Prompt: A camel walking in an enclosure with a wooden fence and greenery in the
background, Minecraft world style.
n
o
o
lla
b
ria
to
H
Input Video Ours CoDeF Hashing-nvd MeDM CCEdit
Text Prompt: Hot air balloons adrift over an ancient desert, ChibiAnimation style.
n
ia
rT
Input Video Ours CoDeF Hashing-nvd MeDM CCEdit
Text Prompt: A model train moving along the track with miniature figures and trees alongside, all
depicted in a claymationstyle.
Figure5:Qualitativecomparisonsontext-guidedvideo-to-videotranslation.Ourmethodachieves
promptalignment,synthesisquality,andtemporalconsistencybest. Zoominforthebestview,and
pleaserefertothesupplementarymaterialsforvideocomparisons. (a)Inthecamelscene,Medm[13]
failstogenerateclear-texturedimagestoensuretemporalconsistency,whileCCEdit[15]failsto
correctlyidentifythesecondcamelinthebackground. (b)CoDeF[44]missescapturingthepresence
ofapersoninthebottomrightcorner,Hashing-nvd[5]exhibitnoticeablecontoursduetomasking,
andbothMeDMandCCEditsufferfromtemporalinconsistencyissues. Forinstance,inMeDM,the
persontransitionsfromwearingblackclothestoblueclothes. (c)MeDMandCCEditstillexhibit
temporalinconsistencyissues,suchassignificantcolor,texture,andstructurechanges.Othermethods
almostentirelylosetheoriginaltraininformationorappearasunnaturalartifacts.
the consistent style of these two images for video editing, We utilize the grid trick proposed by
RAVE[24]totacklethisproblem. Finally,therearealsosomevideo-editingresultsonDAVIS[48],
and the corresponding text prompts originated from the BalanceCC Benchmark. In Table 1, We
choosewarpingerror[29,75]andinterpolationerror[31]asourmetricsoftemporalconsistency.
Metricsforevaluation. Sinceourmainfocusistext-guidedvideoediting,weconductuserstudies
oneditedvideoscomparedwithothermethods,likeCoDeF[44],MeDM[13],andHashing-nvd[5].
Intheuserstudy,36participantswereshowntwoeditedvideos,theoriginalvideooneachpage,and
thetextpromptusedtoeditthevideos. Therearethreecriticalquestionsforuserstoanswer. (1)
Whichvideohasbettertemporalconsistency? (2)Whichvideoalignsbetterwiththetextprompt? (3)
Whichvideohasbetteroverallquality? Figure6summarizestheuserstudyresults,demonstrating
thatourmethodoutperformsinallthreedimensions.
Comparisonofcanonicalimages. InFigure7,werunourmethod,CoDeF[44],andHashing-
nvd[5]onDAVIS[48]andBalanceCC[15]. Notethatthesearethecanonicalimagesoftheinput
video,andweshowtheforegroundatlasforHashing-nvd. Therewillbetwooutputatlasesforthe
backgroundandforegroundofHashing-nvd. Forcomparison,weonlyshowtheforegroundatlasfor
Hashing-nvd. Asaresult,showninFigure7,theoutputcanonicalimagesofourmethodaremore
naturalthanothers,eveninthesceneswiththedramaticmotionoftheobjects,suchasscenesnamed
"train" and "butterfly." We can clearly see that our canonical images preserve the original object
informationwellintheabovetwoscenarios.
7MeDM CoDeF Hashing-nvd Ours
80
60
40
20
0
Temporal Consistency Text Alignment Overall Quality
Figure6: UserStudy. Ourmethodachievesthehighestuserpreferenceratiosacrossallthreeaspects,
comparedwithMeDM[13],CoDeF[44],Hashing-nvd[5].
Camel Train Cow Butterfly Cat
Figure7: Qualitativecomparisonsonthecanonicalimage. Ourmethodgeneratesmorenatural
canonicalimagesthroughafine-tuneddiffusionpriorcomparedwithCoDeF[44],Hashing-nvd[5].
Thecapabilityofthecanonicalimagetorepresentinputframesplaysacrucialroleindownstream
applications. (Hashing-nvdconsistsoftwocanonicalimages. Here,wehaveselectedthecanonical
imagerepresentingtheforeground.)
Downstreamvideoprocessing. Toevaluatetheperformanceofourmethod’shandwrittenvideo
editing,wecomparewithCoDeF[44]andHashing-nvd[5],whichproducecanonicalimagesofthe
videoforuserstowritethecharactersonthecanonicalimagetoaccomplishvideoediting. Wewrite
“NaRCan”totesttheperformanceofthesethreemethodsontwoscenes,“gold-fish”and“train”,in
theBalanceCCBenchmark[15]inFigure8(a). Weextractthesameframeofvideosforcomparison.
For dynamic video segmentation, we segment the mask using the Segment Anything Model
(SAM)[28]basedonthelearnedcanonicalimageofeachmethodandpropagateittothesequence.
Thetargetofthemaskinthesetwoscenesaretheclownfishnamed“coral-reef”andtheflyingbutterfly
inthescenecalled“butterfly.” WeusewhitetomarkthemaskforbettervisibilityinFigure8(b).
4.3 AblationStudy
Homography&ResidualDeformationMLP. Inthissection,weconductedablationexperiments
focusing on both homography and residual deformation MLP. Figure 9(a) clearly illustrates that
usingonlyMLPtofitthedeformationfield[30,45,46,50]resultsinunsuitablecanonicalimages
fordownstreamtasks. Withouttheglobalinformationhomographyprovides,themodelencounters
difficultiesinconvergingthediffusionlossandinsteadfocusessolelyonoptimizingthereconstruction
loss. Conversely,ifwerelysolelyonhomographytoexpressthedeformationfield,homography’s
expressivepowerislimitedincapturingdetailedvariationsinnon-rigidobjects. Asaresult,only
approximateandblurredoutcomescanbeobtained.
Diffusionprior. RelyingonhomographyandtheresidualdeformationMLPachievesrelatively
bettercanonicalimagesthanpreviousmethods. However,thelackofassistancefromthediffusion
prior still prevents the stable generation of high-quality natural canonical images. Figure 9(b)
demonstratesthesignificanceofsupervisingcanonicalimageswiththediffusionprior.
8
sruO
FeDoC
dvn-gnihsaH
)%(fe
d lo
G
laroe r-
C
n
iarT
ylfre
ttu
B
Ours CoDeF Hashing-nvd Ours CoDeF Hashing-nvd
(a)Addinghandwrittencharacters (b)Dynamicvideosegmentation.
Figure8: Qualitativecomparisonson(a)addinghandwrittencharactersand(b)dynamicvideo
segmentation.Ourmethodrepresentsanaturalimageviadiffusionprior,thuscanachievetemporally
consistentvideoeditingandabletopreciselyeditdesiredareas.
Camel Cows Car-roundabout
(a) Deformation modeling (b) Diffusion prior
Figure9:Ablationstudies. (a)Deformationmodeling:(Top)Weshowthatcanonicalimageswithout
homographymodelingfailtogenerateafaithfulimageasthecapacityofresidualdeformationMLP
coulddominatethetrainingprocessandstillachievenear-perfectframereconstruction. (Mid)Onthe
contrary,withoutresidualdeformationMLP,ourmethodcannotmodellocalnon-rigidtransformation,
resultinginblurryforegroundobjects. (Bottom)Combininghomographyandresidualdeformation
MLPhasthebestofbothworldsandachievesthebestcanonicalimagerepresentation. (b)Diffusion
prior: (Top)Withoutdiffusionpriortoregularizingthecanonicalimage,thetrainingprocessrelies
onlyontheframereconstructionandcouldsacrificethefaithfulnessofthecanonicalimage. (Bottom)
Ourfine-tuneddiffusionprioreffectivelycorrectsthecanonicalimagetofaithfullyrepresenttheinput
framesandresultsinnaturalcanonicalimages.
5 Conclusion
In this paper, we introduce NaRCan, a video editing framework, integrating diffusion priors and
LoRA[18]fine-tuningtoproducehigh-qualitynaturalcanonicalimages. Thismethodtacklesthe
challengesofmaintainingthenaturalappearanceofthecanonicalimageandreducestrainingtimes
withnewnoise-schedulingtechniques. TheresultsshowNaRCan’sadvantageinmanagingcomplex
videodynamicsanditspotentialforwideuseinvariousmultimediaapplications.
Limitations. OurmethodreliesonLoRA[18]fine-tuningtoenhancethediffusionmodel’sability
torepresentthecurrentscene. However,LoRAfine-tuningistime-consuming. Additionally,our
training pipeline includes diffusion loss, which increases the training time. In cases of extreme
changesinvideoscenes,diffusionlosssometimesfailstoguidethemodelingeneratinghigh-quality
natural images. These limitations point out the challenge of balancing model adaptability with
computationalefficiencyandeffectivenessinvariedconditions.
9
yhpargomoH
o/w
PLM
laudiseR
o/w
ssrruuOO
roirp
noisuffiD
o/w
roirp
noisuffiD
/wReferences
[1] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live:
Text-drivenlayeredimageandvideoediting. InECCV,2022.
[2] AndreasBlattmann, RobinRombach, HuanLing, TimDockhorn, SeungWookKim, Sanja
Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent
diffusionmodels. InCVPR,2023.
[3] TimBrooks,AleksanderHolynski,andAlexeiAEfros. Instructpix2pix: Learningtofollow
imageeditinginstructions. InCVPR,2023.
[4] DuyguCeylan,Chun-HaoPHuang,andNiloyJMitra. Pix2video: Videoeditingusingimage
diffusion. InICCV,2023.
[5] Cheng-HungChan,Cheng-YangYuan,ChengSun,andHwann-TzongChen. Hashingneural
videodecompositionwithmultiplicativeresidualsinspace-time. InICCV,2023.
[6] Dave Zhenyu Chen, Haoxuan Li, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Nießner.
Scenetex: High-qualitytexturesynthesisforindoorscenesviadiffusionpriors. arXivpreprint
arXiv:2311.17261,2023.
[7] RuiChen,YongweiChen,NingxinJiao,andKuiJia. Fantasia3d: Disentanglinggeometryand
appearanceforhigh-qualitytext-to-3dcontentcreation. InICCV,2023.
[8] WeifengChen,JieWu,PanXie,HefengWu,JiashiLi,XinXia,XuefengXiao,andLiangLin.
Control-a-video: Controllabletext-to-videogenerationwithdiffusionmodels. InICLR,2024.
[9] YueChen,XingyuChen,XuanWang,QiZhang,YuGuo,YingShan,andFeiWang. Local-to-
globalregistrationforbundle-adjustingneuralradiancefields. InCVPR,2023.
[10] JiaxinCheng,TianjunXiao,andTongHe. Consistentvideo-to-videotransferusingsynthetic
dataset. InICLR,2024.
[11] KaLeongCheng,QiuyuWang,ZifanShi,KechengZheng,YinghaoXu,HaoOuyang,Qifeng
Chen,andYujunShen. Learningnaturallyaggregatedappearanceforefficient3dediting. arXiv
preprintarXiv:2312.06657,2023.
[12] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, and
YiYang. Segmentandtrackanything. arXivpreprintarXiv:2305.06558,2023.
[13] ErnieChu,TzuhsuanHuang,Shuo-YenLin,andJun-ChengChen. Medm: Mediatingimage
diffusionmodelsforvideo-to-videotranslationwithtemporalcorrespondenceguidance. In
AAAI,2024.
[14] Jan-NiklasDihlmann, AndreasEngelhardt, andHendrikLensch. Signerf: Sceneintegrated
generationforneuralradiancefields. arXivpreprintarXiv:2401.01647,2024.
[15] RuoyuFeng,WenmingWeng,YanhuiWang,YuhuiYuan,JianminBao,ChongLuo,Zhibo
Chen,andBainingGuo. Ccedit: Creativeandcontrollablevideoeditingviadiffusionmodels.
InCVPR,2024.
[16] MichalGeyer,OmerBar-Tal,ShaiBagon,andTaliDekel. Tokenflow: Consistentdiffusion
featuresforconsistentvideoediting. InICLR,2024.
[17] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,
DiederikPKingma,BenPoole,MohammadNorouzi,DavidJFleet,etal. Imagenvideo: High
definitionvideogenerationwithdiffusionmodels. arXivpreprintarXiv:2210.02303,2022.
[18] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
LuWang,andWeizhuChen. Lora: Low-rankadaptationoflargelanguagemodels. InICLR,
2021.
[19] ZhihaoHuandDongXu. Videocontrolnet: Amotion-guidedvideo-to-videotranslationframe-
workbyusingdiffusionmodelwithcontrolnet. arXivpreprintarXiv:2307.14073,2023.
10[20] AllanJabri,AndrewOwens,andAlexeiEfros. Space-timecorrespondenceasacontrastive
randomwalk. InNeurIPS,2020.
[21] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot
text-guidedobjectgenerationwithdreamfields. InCVPR,2022.
[22] VarunJampani,RaghudeepGadde,andPeterVGehler. Videopropagationnetworks. InCVPR,
2017.
[23] OndˇrejJamriška,ŠárkaSochorová,OndˇrejTexler,MichalLukácˇ,JakubFišer,JingwanLu,Eli
Shechtman,andDanielSy`kora. Stylizingvideobyexample. ACMTOG,2019.
[24] OzgurKara,BariscanKurtkaya,HidirYesiltepe,JamesMRehg,andPinarYanardag. Rave:
Randomized noise shuffling for fast and consistent videoediting with diffusion models. In
CVPR,2024.
[25] YoniKasten,DolevOfri,OliverWang,andTaliDekel. Layeredneuralatlasesforconsistent
videoediting. ACMTOG,2021.
[26] KaiKatsumata,DucMinhVo,BeiLiu,andHidekiNakayama. Revisitinglatentspaceofgan
inversionforrobustrealimageediting. InWACV,2024.
[27] LevonKhachatryan,AndranikMovsisyan,VahramTadevosyan,RobertoHenschel,Zhangyang
Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion
modelsarezero-shotvideogenerators. InICCV,2023.
[28] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,
TeteXiao,SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal. Segmentanything. In
ICCV,2023.
[29] Wei-ShengLai,Jia-BinHuang,OliverWang,EliShechtman,ErsinYumer,andMing-Hsuan
Yang. Learningblindvideotemporalconsistency. InECCV,2018.
[30] TianyeLi,MiraSlavcheva,MichaelZollhoefer,SimonGreen,ChristophLassner,ChangilKim,
TannerSchmidt,StevenLovegrove,MichaelGoesele,RichardNewcombe,etal. Neural3d
videosynthesisfrommulti-viewvideo. InCVPR,2022.
[31] XiruiLi,ChaoMa,XiaokangYang,andMing-HsuanYang. Vidtome: Videotokenmergingfor
zero-shotvideoediting. InCVPR,2024.
[32] Chen-HsuanLin,JunGao,LumingTang,TowakiTakikawa,XiaohuiZeng,XunHuang,Karsten
Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d
contentcreation. InCVPR,2023.
[33] SharonLin,MatthewFisher,AngelaDai,andPatHanrahan.Layerbuilder:Layerdecomposition
forinteractiveimageandvideocolorediting. arXivpreprintarXiv:1701.03754,2017.
[34] Yu-LunLiu,Wei-ShengLai,Ming-HsuanYang,Yung-YuChuang,andJia-BinHuang. Hybrid
neuralfusionforfull-framevideostabilization. InICCV,2021.
[35] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim,
Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In
CVPR,2023.
[36] ErikaLu,ForresterCole,TaliDekel,WeidiXie,AndrewZisserman,DavidSalesin,WilliamT
Freeman, and Michael Rubinstein. Layered neural rendering for retiming people in video.
SIGGRAPHAsia,2020.
[37] ErikaLu, ForresterCole, TaliDekel, AndrewZisserman, WilliamTFreeman, andMichael
Rubinstein. Omnimatte: Associatingobjectsandtheireffectsinvideo. InCVPR,2021.
[38] ChenlinMeng,YutongHe,YangSong,JiamingSong,JiajunWu,Jun-YanZhu,andStefano
Ermon. Sdedit: Guidedimagesynthesisandeditingwithstochasticdifferentialequations. In
ICLR,2022.
11[39] BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRamamoorthi,
andRenNg. Nerf: Representingscenesasneuralradiancefieldsforviewsynthesis. InECCV,
2020.
[40] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Tiberiu Popa. Clip-mesh:
Generatingtexturedmeshesfromtextusingpretrainedimage-textmodels. InSIGGRAPHAsia,
2022.
[41] ThomasMüller,AlexEvans,ChristophSchied,andAlexanderKeller. Instantneuralgraphics
primitiveswithamultiresolutionhashencoding. ACMTOG,2022.
[42] SeonghyeonNam,MarcusABrubaker,andMichaelSBrown. Neuralimagerepresentations
formulti-imagefusionandlayerseparation. InECCV,2022.
[43] AlexNichol,PrafullaDhariwal,AdityaRamesh,PranavShyam,PamelaMishkin,BobMcGrew,
IlyaSutskever,andMarkChen. Glide: Towardsphotorealisticimagegenerationandediting
withtext-guideddiffusionmodels. InICML,2022.
[44] HaoOuyang,QiuyuWang,YuxiXiao,QingyanBai,JuntaoZhang,KechengZheng,Xiaowei
Zhou, Qifeng Chen, and Yujun Shen. Codef: Content deformation fields for temporally
consistentvideoprocessing. InCVPR,2024.
[45] KeunhongPark,UtkarshSinha,JonathanTBarron,SofienBouaziz,DanBGoldman,StevenM
Seitz,andRicardoMartin-Brualla. Nerfies: Deformableneuralradiancefields. InICCV,2021.
[46] KeunhongPark,UtkarshSinha,PeterHedman,JonathanT.Barron,SofienBouaziz,DanB
Goldman, Ricardo Martin-Brualla, and Steven M. Seitz. Hypernerf: A higher-dimensional
representationfortopologicallyvaryingneuralradiancefields. ACMTOG,2021.
[47] GauravParmar,YijunLi,JingwanLu,RichardZhang,Jun-YanZhu,andKrishnaKumarSingh.
Spatially-adaptivemultilayerselectionforganinversionandediting. InCVPR,2022.
[48] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung,
andLucVanGool. The2017davischallengeonvideoobjectsegmentation. arXivpreprint
arXiv:1704.00675,2017.
[49] BenPoole,AjayJain,JonathanTBarron,andBenMildenhall. Dreamfusion: Text-to-3dusing
2ddiffusion. InICLR,2023.
[50] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf:
Neuralradiancefieldsfordynamicscenes. InCVPR,2021.
[51] ChenyangQi,XiaodongCun,YongZhang,ChenyangLei,XintaoWang,YingShan,andQifeng
Chen. Fatezero: Fusingattentionsforzero-shottext-basedvideoediting. InICCV,2023.
[52] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InICML,2021.
[53] AdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,Mark
Chen,andIlyaSutskever. Zero-shottext-to-imagegeneration. InICML,2021.
[54] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen. Hierarchical
text-conditionalimagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,2022.
[55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
High-resolutionimagesynthesiswithlatentdiffusionmodels. InCVPR,2022.
[56] ManuelRuder,AlexeyDosovitskiy,andThomasBrox. Artisticstyletransferforvideos. In
GCPR,2016.
[57] ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyLDenton,Kamyar
Ghasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,etal. Photorealistic
text-to-imagediffusionmodelswithdeeplanguageunderstanding. InNeurIPS,2022.
12[58] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu,
HarryYang,OronAshual,OranGafni,etal. Make-a-video: Text-to-videogenerationwithout
text-videodata. InICLR,2023.
[59] Haorui Song, Yong Du, Tianyi Xiang, Junyu Dong, Jing Qin, and Shengfeng He. Editing
out-of-domainganinversionviadifferentialactivations. InECCV,2022.
[60] JunshuTang,TengfeiWang,BoZhang,TingZhang,RanYi,LizhuangMa,andDongChen.
Make-it-3d: High-fidelity3dcreationfromasingleimagewithdiffusionprior. InICCV,2023.
[61] LumingTang,NatanielRuiz,QinghaoChu,YuanzhenLi,AleksanderHolynski,DavidEJacobs,
BharathHariharan,YaelPritch,NealWadhwa,KfirAberman,etal. Realfill: Reference-driven
generationforauthenticimagecompletion. InCVPR,2024.
[62] OndˇrejTexler,DavidFutschik,MichalKucˇera,OndˇrejJamriška,ŠárkaSochorová,Menclei
Chai,SergeyTulyakov,andDanielSy`kora. Interactivevideostylizationusingfew-shotpatch-
basedtraining. ACMTOG,2020.
[63] HaochenWang, XiaodanDu, JiahaoLi, RaymondAYeh, andGregShakhnarovich. Score
jacobianchaining: Liftingpretrained2ddiffusionmodelsfor3dgeneration. InCVPR,2023.
[64] TengfeiWang,YongZhang,YanboFan,JueWang,andQifengChen.High-fidelityganinversion
forimageattributeediting. InCVPR,2022.
[65] Wen Wang, Yan Jiang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, and
Chunhua Shen. Zero-shot video editing using off-the-shelf image diffusion models. arXiv
preprintarXiv:2303.17599,2023.
[66] XiaolongWang,AllanJabri,andAlexeiAEfros. Learningcorrespondencefromthecycle-
consistencyoftime. InCVPR,2019.
[67] ZhengyiWang,ChengLu,YikaiWang,FanBao,ChongxuanLi,HangSu,andJunZhu. Pro-
lificdreamer: High-fidelityanddiversetext-to-3dgenerationwithvariationalscoredistillation.
InNeurIPS,2023.
[68] JayZhangjieWu,YixiaoGe,XintaoWang,StanWeixianLei,YuchaoGu,YufeiShi,Wynne
Hsu,YingShan,XiaohuQie,andMikeZhengShou. Tune-a-video: One-shottuningofimage
diffusionmodelsfortext-to-videogeneration. InICCV,2023.
[69] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson,
Pratul P Srinivasan, Dor Verbin, Jonathan T Barron, Ben Poole, et al. Reconfusion: 3d
reconstructionwithdiffusionpriors. InCVPR,2024.
[70] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender a video: Zero-shot
text-guidedvideo-to-videotranslation. InSIGGRAPHAsia,2023.
[71] VickieYe,ZhengqiLi,RichardTucker,AngjooKanazawa,andNoahSnavely. Deformable
spritesforunsupervisedvideodecomposition. InCVPR,2022.
[72] AhmetBurakYildirim,HamzaPehlivan,BahriBatuhanBilecen,andAysegulDundar. Diverse
inpaintingandeditingwithganinversion. InICCV,2023.
[73] LvminZhang,AnyiRao,andManeeshAgrawala. Addingconditionalcontroltotext-to-image
diffusionmodels. InICCV,2023.
[74] YaboZhang,YuxiangWei,DongshengJiang,XiaopengZhang,WangmengZuo,andQiTian.
Controlvideo: Training-freecontrollabletext-to-videogeneration. InICLR,2024.
[75] ShangchenZhou,PeiqingYang,JianyiWang,YihangLuo,andChenChangeLoy. Upscale-a-
video: Temporal-consistentdiffusionmodelforreal-worldvideosuper-resolution. InCVPR,
2024.
[76] JiapengZhu,YujunShen,DeliZhao,andBoleiZhou. In-domainganinversionforrealimage
editing. InECCV,2020.
[77] JosephZhuandPeiyeZhuang. Hifa: High-fidelitytext-to-3dwithadvanceddiffusionguidance.
arXivpreprintarXiv:2305.18766,2023.
13A Appendix
A.1 SingleNaRCancomparedwithSeparatedNaRCan
Dissecting canonical-based methods. In this section, we will delve deeper into two different
canonical-basedmethods: CoDeFandHashing-nvd. AsshownclearlyinFigure10,CoDeFgenerates
poor-qualitycanonicalimagesduetothelackofsupervisionandconstraintsfromthediffusionprior.
Incontrast,ourmethodcanconsistentlyproducehigh-qualitycanonicalimagesregardlessofthe
video length, effectively preparing for subsequent downstream editing tasks. The drawbacks of
Hashing-nvdarealsoapparent. ThismethodreliesheavilyontheMask-RCNNtechnique, often
resultingininaccurateorincorrectforegroundandbackgroundsegmentation. Consequently,thefinal
canonicalimagesgeneratedaredifficulttoeditorinapplicabletotechniquessuchasControlNet.
Parameter settings of Separated NaRCan. Subsequently, as shown in Figure 11, From our
experiments, we found that when using Separated NaRCan, it is crucial to limit the number of
segmentationsinthevideo. Thisisbecausetheeditinginformationontheeditedcanonicalimage
ispropagatedthroughwarpingusingflows. Iftheflowisinaccurateorcontainserrors,excessive
warpingcanleadtoseverecumulativeerrors,significantlydamagingtheeditedcontent.
s e m f r u
a r es -
f
2
t
iK
8
s
e m
fe
e r
a
r
la-
f r
7 o
2 C
Ours CoDeF Ours Hashing-nvd Hashing-nvd
foreground background
Figure10: Canonicalanalysis. (a)Comparedtoexistingcanonical-basedmethods,ourapproach
robustly generates high-quality natural canonical images regardless of the video length. (b) Our
methodaccuratelypreservesthecorrectforegroundandbackgroundinformationfromtheoriginal
scenes,avoidingseveredistortionsorwarpingandpreventinggeneratingcontentinconsistentwith
thescene. Forexample,in“kite-surfing”,Hashing-nvderroneouslygeneratesanadditionalperson.
A.2 VideoComparisons
WeprovideaninteractiveHTMLinterfacetobrowsevideoresultsforcomparisons. Specifically,
weprovidevideocomparisonsonthreedifferenttasks: (1)ControlNetstyletransfer,(2)dynamic
segmentation,and(3)addinghandwrittencharacters. Wecompareourproposedmethod,NaRCan,
withstate-of-the-artmethods: Hashing-nvd[5],CoDeF[44],andMeDM[13]. Wealsovisualizethe
optimizedcanonicalimagesifavailableforreference.
A.3 UserStudy
Toconductouruserstudy,weemployGitHubPagesinconjunctionwithaGoogleFormtofacilitate
user evaluations of video quality. Each evaluation session comprises 15 scenes, each of which
contains3questions. Fortheseevaluations,werandomlyselect15scenesfromapoolof100scenes
in BalanceCC Benchmark [15]. Each evaluation page (Figure 12) presents a video edited using
our method alongside a randomly selected video sourced from compared methods: MeDM [13],
CoDeF[44],orHashing-nvd[5].
143
=
n
o
it
a
r
a
p
e
S
e
m
a
r
f
-
r
e
P
t
Figure11: SeparatedNaRCanandper-framewarping. Sincewehavetheflexibilitytoseparate
intomultiplecanonicalimages,weconductanexperimenttodeterminehowmanycanonicalimages
areoptimal. WetestusingSeparatedNaRCanwithsegmentationequalto3andsegmentationequal
toN,whereNequalsthenumberofframesinthevideo. Theresultsshowthattheeditinginformation
isdamagedandhassignificantdisplacementduetoinaccurateopticalflow.
Figure12: UserStudyWebsite. Thevideoaboveistheoriginalvideo. Thetwovideosbelowarethe
onesbeingcompared. Weaskparticipantstodeterminewhichofthegivenvideosbestmatchesthe
descriptionprovidedinthequestions.
A.4 GridTrick
Weadoptthetechniquenamed“gridtrick”fromRAVE[24]. Inthatpaper,amethodcalledgridtrick
isintroduced,wheremultipleimagesaremergedtogetherandthenfedtoControlNet[73]forediting
toobtainstyletransferimageswithsufficientconsistencyincontentandcolortones. Therefore,when
usingSeparatedNaRCan,weonlyneedtoapplythegridtricktoourkcanonicalimagestoachieve
thisdesiredconsistencyamongthecanonicalimageseasily[24].
Finally,coupledwiththelinearinterpolationmethodmentionedinourpaper,wecanensurethatthe
videomaintainssufficienttemporalconsistency,therebysuccessfullycreatingahigh-qualitystyle
transfervideo.
15Yacht Bear Two-swan
Figure 13: Canonical images after using the grid trick. Using Separated NaRCan, we will
obtainmultiplecanonicalimages, andthroughthegridtrick, wecangenerateahigh-qualityand
consistentstyletransfercanonicalimage. Therefore,SeparatedNaRCanstilldemonstratesexcellent
performanceinthistask.
16