ProceedingsofMachineLearningResearchvol196:1–30,2024
Robust Distribution Learning with
Local and Global Adversarial Corruptions
SloanNietert NIETERT@CS.CORNELL.EDU
ZivGoldfeld GOLDFELD@CORNELL.EDU
SorooshShafiee SHAFIEE@CORNELL.EDU
CornellUniversity
Abstract
Weconsiderlearninginanadversarialenvironment,whereanε-fractionofsamplesfromadistribution
P are arbitrarily modified (global corruptions) and the remaining perturbations have average magnitude
bounded by ρ (local corruptions). Given access to n such corrupted samples, we seek a computationally
efficientestimatorPˆ thatminimizestheWassersteindistanceW (Pˆ ,P).Infact,weattackthefine-grained
n 1 n
task of minimizing W (Π Pˆ ,Π P) for all orthogonalprojections Π Rd×d, with performance scaling
1 ♯ n ♯
∈
with rank(Π) = k. This allows us to account simultaneously for mean estimation (k = 1), distribution
estimation(k = d), aswellas thesettingsinterpolatingbetweenthesetwo extremes. We characterizethe
optimalpopulation-limitrisk for this task and then developan efficientfinite-samplealgorithmwith error
boundedby√εk+ρ+dO(1)O˜(n−1/k)whenP hasboundedmomentsoforder2+δ,forconstantδ > 0.
For data distributions with bounded covariance, our finite-sample bounds match the minimax population-
leveloptimumforlargesample sizes. Our efficientprocedurerelies ona noveltrace normapproximation
ofanidealyetintractable2-Wassersteinprojectionestimator. We applythisalgorithmtorobuststochastic
optimization, and, in the process, uncover a new method for overcoming the curse of dimensionality in
Wassersteindistributionallyrobustoptimization.1
Keywords:robuststatistics,optimaltransport,distributionallyrobustoptimization
1. Introduction
In robust statistics and adversarial machine learning, estimation and decision-making are treated as a two-
playergamebetweenthelearnerandabudget-constrained adversary. Throughthislens,researchershavede-
velopedlearningalgorithmswithstrongguaranteesdespiteadversarialcorruptions. Forexample,Huber’sε-
contaminationmodelinclassicalrobuststatistics(Huber,1964)andthetotalvariation(TV)ε-contamination
model (DonohoandLiu, 1988) give the adversary an ε fraction of data to arbitrarily and globally corrupt.
Popularizedrecentlyinthesettingofadversarialtraining(Sinhaetal.,2018),Wassersteincorruptionmodels
permitallofthedata tobelocally perturbed, bounding the average perturbation size bysomeradius ρ 0.
≥
Recallthatthep-Wasserstein distanceisdefinedbetweendistributions P,Qby
W (P,Q) := inf E X Y p p1 ,
p π∈Π(P,Q) (X,Y)∼π k − k2
(cid:2) (cid:3)
where Π(P,Q) is the set of their couplings. This metric naturally lifts the geometry of Rd to the space of
distributions (Rd)withfinitep-thabsolute moments.
P
Ideally, a corruption model should be flexible enough to capture multiple types of data contamination.
Towards this goal, weinvestigate learning under combined TVand Wasserstein adversarial corruptions, re-
cently introduced in the setting of distributionally robust optimization (DRO) (Nietertetal., 2023b). For-
mally, we consider learning where clean samples X ,...,X P are arbitrarily perturbed to obtain
1 n
∼
X˜ n such that X˜ X ρ, where S [n] with S (1 ε)n. Denoting the clean
{ i }i=1 i∈Sk i − i k2 ≤ ⊆ | | ≥ −
and corrupted empirical measures by P and P˜ , respectively, this corruption model ischaracterized byan
P n n
outlier-robust variantoftheWasserstein distance definedin(3)ahead, wherebyWε(P ,P˜ ) ρ. Weask:
1 n n ≤
1.AcceptedforpresentationattheConferenceonLearningTheory(COLT)2024.
©2024S.Nietert,Z.Goldfeld&S.Shafiee.
4202
nuJ
01
]GL.sc[
1v90560.6042:viXraNIETERTGOLDFELDSHAFIEE
Howcanwelearneffectivelyandefficientlywithbothlocalandglobaladversarialcorruptions?
Underthiscombinedmodel,weseekanestimatePˆ thatcanapproximateP inavarietyofdownstream
n
applications. Inparticular, weexplore thedistribution learning taskofrecovering P under W itself. Since
1
the sample complexity and risk bounds associated with standard W suffer a curse of dimensionality, we
1
focusonthefinegrained-goal ofestimatingk-dimensional projections ofP,withperformance scalingwith
k. Quantitatively, weseekPˆ suchthatthek-dimensional max-sliced Wassersteindistance
n
W (Pˆ ,P) := sup W (U Pˆ,U P) = sup E [f(UX)] E [f(UX)]
1,k n
U∈Rk×d
1 # #
U∈Rk×d,f∈Lip (Rk)
Pˆ
n −
P
UU⊤=Ik UU⊤=Ik1
is appropriately small for all k [d]. Since our approach cleanly addresses all slicing dimensions simulta-
∈
neously, we focus on providing bounds which are uniform in k. By doing so, we account not only for the
saiddistribution estimation task(k = d),butalsomeanestimationk = 1.
While this task is relatively straightforward under TV corruption alone (we show in Section 2.3 that
a standard iterative filtering algorithm (Diakonikolas etal., 2016) suffices), and immediate under Wasser-
stein corruption alone, where the corrupted distribution P˜ is itself minimax optimal, the combined model
n
requires a new algorithmic approach and analysis to obtain suitable risk bounds. Eventually, we revisit
the Wasserstein DRO setting that introduced in (Nietertetal., 2023b). Here, the steps we take to employ
our estimate lead to anew perspective on generalization and radius selection when employing Wasserstein
ambiguitysetsfordistributionally robuststochastic optimization.
1.1. OurResults
ConditionedonthecleandatasatisfyingthecovarianceboundΣ
Pn
(cid:22)I d,weproposeanalgorithmW2PROJECT
(Algorithm 1)whichefficientlycomputesanestimatePˆ suchthat
n
W (Pˆ ,P ) . √kε+ρ (1)
1,k n n
forallk [d], andthiserrorbound isminimaxoptimal. Toaccount forsampling error, weassumethatthe
∈
populationmeasureP hasboundedmomentsoforder2+δ,forconstantδ > 0,andemploy(1)plusknown
bounds onE[W (P ,P)]toprovethatE[W (Pˆ ,P)] . √kε+ρ+dO(1)O˜(n−1/(k∨2)). Ouralgorithm
1,k n 1,k n
servesasatractable proxyfortheminimumdistanceestimate
Pˆ = argmin W (Q, ),
MDE Q≤ 1−1 εP˜ n 2 Gcov
where Q ranges over all distributions obtained by deleting an ε-fraction of mass from P˜ and renormaliz-
n
ing, and W (Q, ) = inf W (Q,R). In particular, we approximate W (Q, ) tr(Σ
2 Gcov R:ΣR(cid:22)Id 2 2 Gcov
≈
Q
−
I ) , which lends itself to efficient implementation using spectral decomposition. In the infinite-sample
d +
population-limit, weprovetightriskboundsforbroaderclassofprobability measures(e.g.,thosewhichare
sub-Gaussian orlog-concave) usingafamilyofrelatedminimumdistance estimators.
Given such an algorithm, we then explore applications to robust stochastic optimization. Suppose that
we have an estimate Pˆ of known quality W (Pˆ,P) τ, perhaps from the procedure above. Given a
1,k
≤
family of Lipschitz loss functions which operate on k-dimensional linear features (e.g. k-variate linear
L
regression) weprovethattheWasserstein DROestimate
ℓˆ= argmin sup E [ℓ] (2)
ℓ∈L Q
Q:W 1(Q,Pˆ n).τ
2ROBUST ESTIMATION UNDERLOCAL AND GLOBALADVERSARIAL CORRUPTIONS
achievesriskbounds typicallyassociated withthesliced-Wasserstein DROproblem
min sup E [ℓ],
Q
ℓ∈L Q:W 1,k(Q,Pˆ n).τ
even though P need not belong to the Wasserstein ambiguity set in (2). In particular, we prove that ℓˆsat-
isfies the excess risk bound E [ℓˆ] E [ℓ ] . ℓ τ, where ℓ = argmin E [ℓ]. Plugging in the
P − P ⋆ k ⋆ kLip ⋆ ℓ∈L P
algorithmic results above improves upon existing results for outlier-robust WDRO, exhibiting tight depen-
denceonkandwithsamplingerrorscalingasn−1/k ratherthann−1/d. Thisriskboundwouldbeimmediate
if the W ball above was replaced with aW ball. The fact that this is not necessary is essential for com-
1 1,k
putational tractability, and provides a new framework for avoiding the curse of dimensionality (CoD) in
Wasserstein DRO. We note that this result is new even when ε = 0 and ρ > 0 is taken to model only
stochastic sampling error. Previous results on avoiding the CoD required k = 1 or involved significantly
more complicated analysis. Inparticular, for the rank-one linear structure withk = 1, including univariate
linearregression/classification, thebounds ofO(√n)havebeenestablished in(Shafieezadeh-Abadeh etal.,
2019;ChenandPaschalidis,2018;Oleaetal.,2022;Wuetal.,2022). Ontheotherhand,Gao(2022)relaxes
the rank-one structural assumption and achieves O(√n) bounds as long as the data generating distribution
satisfiescertaintransport inequalities. Nonetheless, therequired assumptions arenoteasilyverifiable.
1.2. RelatedWork
Robuststatistics. LearningfromdataunderTVε-corruptions, astapleofclassical robuststatistics, dates
back to Huber (1964). Various robust and sample-efficient estimators, particularly for mean and scale
parameters, have been developed in the robust statistics community; see RonchettiandHuber (2009) for
a comprehensive survey. Recently, Zhuetal. (2022a), significantly expanding the celebrated results of
DonohoandLiu (1988), developed a unified statistical framework for robust statistics based on minimum
distanceestimation andageneralized resilience quantity, providing sharppopulation-limit andstrongfinite-
sample guarantees for tasks including mean and covariance estimation. Learning under W corruptions is
1
consideredinZhuetal.(2022a)andChaoandDobriban(2023),butourdistributionestimationtaskistrivial
underlocalcorruptions alone,and,assuch,isnotconsidered bytheseworks.
Overthepastdecade, thefocusinthecomputersciencecommunityhasshiftedtothehigh-dimensional
setting, where they have developed computationally efficient estimators achieving optimal estimation rates
formanyproblems(Diakonikolas etal.,2016;Chengetal.,2019;Diakonikolas andKane,2023). Incompu-
tationallearningtheory,olderworkhasexploredprobablyapproximatelycorrect(PAC)learningframework
withadversarially corrupted labels(AngluinandLaird,1988;Bshoutyetal.,2002).
Robustoptimaltransport. Therobustoptimaltransport(OT)literaturehasacloseconnectionwithunbal-
ancedOTtheory,whichdealswithtransportationproblemsbetweenmeasuresofdifferentmass. Unbalanced
OTproblemsinvolvef-divergencesthataccountfordifferencesinmass,whichcanappeareitherinthecon-
straints(Balajietal.,2020)orintheobjectivefunctionasregularizers(PiccoliandRossi,2014;Chizatetal.,
2018a;Lieroetal.,2018;SchmitzerandWirth,2019;Hanin,1992). Theconstraint versionisusually more
difficulttosolve,whereasprimal-dualtypealgorithmshavebeendevelopedtosolvetheregularized version
(Mukherjee etal.,2021;Chizatetal.,2018b;Fatrasetal.,2021;FukunagaandKasai,2022;Leetal.,2021;
Nath,2020). AnalternativeapproachtomodelrobustnessinOTisthroughpartialOTproblems,whereonly
afractionofmassneedstobetransported(CaffarelliandMcCann,2010;Figalli,2010;Nietertetal.,2023a;
Chapeletal.,2020). PartialOThasbeen previously used inthecontext ofDROproblems; however, itwas
introduced toaddressstochastic programswithsideinformation (Esteban-Pe´rez andMorales,2022).
Sliced optimal transport. Max-sliced optimal transport, as used to define W , is also known as k-
1,k
dimensionaloptimaltransport(Niles-WeedandRigollet,2022)andprojection-robustoptimaltransport(Linetal.,
3NIETERTGOLDFELDSHAFIEE
2020, 2021). In general, W defines a metric on the space of d-dimensional distributions by measuring
1,k
discrepancy between k-dimensional projections thereof. Thestructural, statistical, andcomputational prop-
erties of W are well-studied in (Linetal., 2021; Niles-WeedandRigollet, 2022; Nietertetal., 2022b;
1,k
BartlandMendelson, 2022; Nadjahietal., 2020), with the tightest results established for k = 1. Max-
sliced OT has been used in the context of DRO problems; however, it was introduced for rank-one linear
structures (Oleaetal.,2022).
Distributionally robust optimization. Wasserstein distributionally robust optimization has emerged as
a powerful modeling approach for addressing uncertainty in the data generating distribution. In this ap-
proach,theambiguitysetaroundtheempiricaldistribution isconstructedbytheWassersteindistance. Mod-
ernconvexapproach,leveragingdualitytheory(MohajerinEsfahaniandKuhn,2018;BlanchetandMurthy,
2019;GaoandKleywegt,2023),hasledtosignificantcomputational advantages. Despiteitscomputational
success, some studies have raised concerns about the sensitivity of the standard DRO formulations to out-
liers(Hashimotoetal.,2018;Huetal.,2018;Zhuetal.,2022a). Toaddress potential overfittingtooutliers,
Zhaietal. (2021) propose a refined risk function based on a family of f-divergences. Nevertheless, this
approach is not robust to local perturbations, and the risk bounds require a moment condition to hold uni-
formlyoverΘ. Anotherrelatedworkin(Bennouna andVanParys,2022;Bennounaetal.,2023)constructs
the ambiguity set using an f-divergence for statistical errors and the Prokhorov distance for outliers. This
provides computational efficiency and statistical reliability but lacks analysis of minimax optimality and
robustness to Huber contamination. Furthermore, Nietertetal. (2023b) constructs the ambiguity set using
therobustWasserstein distanceintroduced in(Nietertetal.,2021). Werevisitthissetting inSection3.
1.3. NotationandPreliminaries
Let denote theEuclidean norm on Rd. Wewrite (Rd)for the family ofBorel probability measures
2
k·k P
on Rd, equipped with the TVnorm between P,Q (Rd)defined by P Q TV := 1 P Q ( ). We
∈ P k − k 2| − | Z
say that Q is an ε-deletion of P if Q 1 P (where such inequalities are set-wise). We write E [f(X)]
≤ 1−ε P
forexpectation off(X)withX P;whenclearfromthecontext, therandom variable isdropped andwe
∼
writeE [f]. Letµ denotethemeanandΣ thecovariancematrixofP (Rd),andlet (Rd) := P
P P P p
(Rd) : E [ X µ p ] < . Thepush-forward off through P ∈ (RP d)isf P() :=P P(f−1()){ . Th∈ e
P P k − P k2 ∞} ∈ P ♯ · ·
setofintegersupton Nisdenoteby[n];wealsousetheshorthand[x] = max x,0 . Wewrite.,&,
+
∈ { } ≍
for inequalities/equality up to absolute constants, and let a b := max a,b . For a matrix A Rd×d,
∨ { } ∈
we write A := sup Ax for its operator norm. If A is further is diagonalizable, we write
k kop kxk2=1k k2
λ (A) = λ (A) λ (A)foritseigenvalues.
max 1 d
≥ ··· ≥
Classicalandoutlier-robustWassersteindistances. Forp [1, ),thep-Wassersteindistancebetween
∈ ∞
P,Q (Rd) is W (P,Q) := inf E X Y p 1/p , where Π(P,Q) := π ( 2) :
p p π∈Π(P,Q) π
∈ P k − k { ∈ P Z
π( ) = P, π( ) = Q isthe setofalltheir couplings. Somebasic properties ofW are (see, e.g.,
p
·×Z Z ×· } (cid:0) (cid:2) (cid:3)(cid:1)
Villani (2003); Santambrogio (2015)): (i) W is a metric on ( ); (ii) the distance is monotone in the
p p
P Z
order, i.e.,W W forp q;and(iii)W metrizesweakconvergence plusconvergence ofpthmoments:
p q p
≤ ≤
W (P ,P) 0 if and only if P w P and x pdP (x) x pdP(x). For a family of measures
p n n n
→ → k k → k k
(Rd),wewriteW (P, ) := inf W (P,R).
p R∈G p
G ⊆ P G R R
Tohandlecorrupted data,weemploytheε-outlier-robust p-Wassersteindistance2,definedby
Wε(µ,ν) := inf W (µ′,ν) = inf W (µ,ν′). (3)
p p p
µ′∈P(Rd) ν′∈P(Rd)
kµ′−µkTV≤ε kν′−νkTV≤ε
2.Whilenotametric,Wεissymmetricandsatisfiesanapproximatetriangleinequality(Nietertetal.(2023a),Proposition3).
p
4ROBUST ESTIMATION UNDERLOCAL AND GLOBALADVERSARIAL CORRUPTIONS
The second equality is a useful consequence of Lemma 4 in Nietertetal. (2023a) (see Appendix A of
Nietertetal.(2023b)forfurtherdetails).
2. RobustDistributionLearning
We now turn to robust distribution estimation under combined TV-W contamination. Given corrupted
1
samples from anunknown distribution P,weaim toproduce anestimate Pˆ such that W (Pˆ,P)isappro-
1,k
priately small for all k [d]. When k = d, we recover standard W . When k = 1, we shall see that the
1
∈
resulting estimation task is of essentially the same complexity as mean estimation. Omitted proofs appear
inAppendices AandB.
2.1. ThePopulationLimit
Wefirst examine the information-theoretic limits ofthis problem without sampling error, namely, allowing
computationally-intractable estimators and access to population distributions (rather than samples). We
consider learningunderthefollowingenvironment.
Setting A: Fix corruption levels 0 ε 0.493 and ρ 0, along with a clean distribution family
≤ ≤ ≥
(Rd). NatureselectsP ,andthelearnerobserves P˜ suchthatWε(P˜,P) ρ.
G ⊆ P ∈G 1 ≤
Given P˜, we seek an estimator Pˆ such that W (Pˆ,P) is small for all k. To ensure that effective
p,k
learning ispossible, weimposeastabilitycondition onthecleanmeasureP.
Definition1(Stability) Let0 < ε < 1andδ ε. Wesaythat adistribution P (Rd)is(ε,δ)-stable if,
≥ ∈ P
for all Q 1 P, we have µ µ δ and Σ Σ δ2/ε. Write (ε,δ) for the family of
≤ 1−ε k Q − P k2 ≤ k Q − P kop ≤ S
(ε,δ)-stable P suchthatΣ I ,and (ε,δ)forthesubfamilyofthoseforwhichΣ (1 δ2/ε)I .
P d iso P d
(cid:22) S (cid:23) −
A distribution is stable if its first two moments vary minimally under ε-deletions. The near-isotropic
subfamily coincides with a popular definition in algorithmic robust statistics (see, e.g., Chapter 2 of
iso
S
Diakonikolas andKane,2023). Without the bound on Σ Σ , this definition coincides with that of
Q P op
k − k
resilience, a standard sufficient condition for (inefficient) robust mean estimation (Steinhardt etal., 2018).
Stabilityisaflexiblenotion thatconnectstomanystandard tailbounds.
Example1(Concretestabilitybounds) Fix0 < ε 0.994 andP (Rd)withΣ I . Then: [proof]
P d
≤ ∈P (cid:22)
• Boundedcovariance: withnofurtherassumptions, P (ε,O(√ε));
iso
∈ S
• Sub-Gaussian: ifP is1-sub-Gaussian, thenP (ε,O(ε log(1/ε)));
∈ S
• Log-concave: ifP islog-concave, thenP (ε,O(εlog(1p/ε)));
∈ S
• Boundedmomentsoforderq ≥ 2: ifsup v∈Sd−1E P[ |v⊤(Z −µ P) |q]
≤
1,thenP
∈
S(ε,O(ε1−1/q)).
Werefer tothe families ofdistributions satisfying these properties by , , , and ,respectively.
cov subG lc q
G G G G
Notethat = . SimilarboundsarederivedinChapter2ofDiakonikolas andKane(2023).
cov 2
G G
Wenowpresentourprimaryriskboundforthepopulation-limit.
3.As ε approaches the optimal breakdown point of 1/2, it becomes information-theoretically impossible to distinguish inliers
fromoutliers.Thequantity0.49canbereplacedwithanyconstantboundedawayfrom1/2
4.Similarlytotheε≤0.49boundabove,here0.99canbereplacedwithanyconstantboundedawayfrom1.
5NIETERTGOLDFELDSHAFIEE
Theorem2(Population-limitriskbound) Under Setting A, take η = min 2ε,1/4 + ε/2 and assume
{ }
that (2η,δ). Thentheminimumdistance estimate5 Pˆ = argmin W (Q, )satisfies
G ⊆ S MDE Q≤ 1−1 η 2 G
W (Pˆ ,P) . √kδ+ρ, k [d].
1,k MDE
∀ ∈
The minimum distance estimate Pˆ involves an infinite dimensional optimization problem, which is
MDE
computationally intractable. In the subsequent subsection we propose an iterative filtering algorithm that
approximately solvesasurrogateoptimization problemonafinitesampleset.
Proof The constant η is selected so that η ε & ε while keeping 2η 0.99 bounded away from 1. For
− ≤
concreteness, thereadermaywanttofocusonthecasewhereε 1/6andη = 2ε.
≤
To treat combined Wasserstein and TV contamination, we first show that any W perturbation can be
1
decomposed intoaW perturbation followedbyaTVperturbation.
2
[proof] Lemma3(W 1 decomposition) Fix0 < τ < 1andP,Q (Rd)withW 1(P,Q) ρ. Thenthereexists
∈ P ≤
R (Rd)suchthatW 1(P,R) ρ,W 2(P,R) √2ρ/√τ,and R Q TV τ.
∈ P ≤ ≤ k − k ≤
TheproofinAppendixA.2takesZ P andZ +∆ Q,whereE[ ∆ ] ρ. LettingE betheevent
2
∼ ∼ k k ≤
suchthat ∆ islessthanits1 τ quantile, weconclude bysettingRtothelawofZ +∆1 .
2 E
k k −
Next, we prove a version of the theorem when ρ = 0, but the clean measure is close to (ε,δ) under
S
W .
2
[proof] Lemma4(Riskboundwhenρ =0) Fix 0 η < 1/2, λ 0, and (2η,δ). Take R,R˜ (Rd)
≤ ≥ G ⊆ S ∈ P
Wsuch (t Rh ˆa ,t RW )2 .(R, 1G) (≤ √kλ δa +nd λk √R η− ),fR o˜ rkT aV llk≤ η. [dT ].henthe estimate Rˆ = argmin Q≤ 1−1 ηR˜W 2(Q, G) satisfies
1,k 1−2η ∈
The proof in Appendix A.3 observes that R satisfies a generalized stability bound: for all R′ 1 R
≤ 1−ε
andM 0,wehave µ R′ µ R 2 . δ+λ√εand
(cid:23) k − k
δ2
tr M(Σ R′ Σ R) . tr(M)+λ2 M op. (4)
− ε k k
(cid:12) (cid:0) (cid:1)(cid:12)
In words, the latter bounds sh(cid:12)ows that the gap Σ R(cid:12) ′ Σ R lies in the Minkowski sum of an operator norm
−
ballofradiusO(δ2/ε)andatracenormballofradiusO(λ2). Incontrast, themoredirectguarantee Σ R′
k −
Σ . δ2/ε+λ2 wouldonlygiveasuboptimal riskboundof√kδ+√kλ.
R op
k
Giventheabovelemmas,wearereadytoprovethetheorem. FixP andP˜ suchthatWε(P˜,P) ρ.
∈G 1 ≤
Thisrequires theexistence ofQsuchthatW (P,Q) ρand Q P˜ ǫ. ApplyingLemma3toP and
1 TV
≤ | − | ≤
Qwithτ = η εimpliesthatthereexistsRsuchthatW (P,R) ρ,W (P,R) √2ρ/√η ε=:λ,and
1 2
− ≤ ≤ −
R P˜ TV R Q TV + Q P˜ TV η ε+ε = η. ApplyingLemma4withTVcorruption level
k − k ≤ k − k k − k ≤ −
η andW boundλ . ρ/√ε,wefindthatPˆ fromthetheorem statementsatisfies
2 MDE
1 1
W (Pˆ ,P) W (Pˆ ,R)+W (R,P) . √kδ+λ√η . √kδ+ρ ,
1,k MDE 1,k MDE 1,k
≤ 1 2η 1 2η
− −
(cid:0) (cid:1) (cid:0) (cid:1)
asdesired.
Thisboundistightformanydistribution families,including thoseinExample1.
6ROBUST ESTIMATION UNDERLOCAL AND GLOBALADVERSARIAL CORRUPTIONS
[proof] Corollary 5 TheminimumdistanceestimatePˆ fromTheorem2achieveserror
MDE
√kε+ρ, =
cov
G G
W (Pˆ ,P) .
√kε log(1/ε)+ρ,
G
=
GsubG ,
1,k MDE 
   √kε plog(1/ε)+ρ, G = Glc
√kε1−1/q +ρ, =
q
G G



andeachoftheseguarantees isminimaxopti maluptologarithmic factors inε−1.
For the minimax lower bounds, we employ existing constructions for the setting where ρ = 0. To
strengthen these bounds when ρ > 0, we show that the learner cannot distinguish between translations of
magnitude ρ.
Remark6(Comparisontootherminimumdistanceestimators) EstimatorsrelatedtothatinTheorem2
arestandardinrobuststatistics (see,e.g.,DonohoandLiu(1988);Zhuetal.(2022a)formethodsbasedon
(smoothed) TV projection) and robust optimal transport (see, e.g., Nietertetal. (2023a), which employs
projection under Wε). The risk bounds from Lemma 4 match those in the literature for robust mean and
p
distributionestimationwhenρ= 0(recallingthatourresultsextendtomeanestimationsince µ µ
P Q 2
k − k ≤
W (P,Q)). We diverge from these existing estimators by returning Pˆ = T(P˜) which lies not in but
1,1
nearby under W . The fact that Pˆ is an ε-deletion of P˜ is essential in turning this approach iG nto a
2
G
practical algorithm inSection 2.2.
2.2. Finite-SampleAlgorithms
Wenow turn to the finite-sample setting. Here, our rates are only tight when δ & √ε, so we restrict to the
family ofdistributions P (Rd)withΣ I . Indeed, (ε,O(√ε))byExample1.
cov P d cov
G ∈ P (cid:22) G ⊆ S
SettingB:Let0 <ε < ε ,whereε isasufficientlysmallabsoluteconstant6. Fixρ 0andsample
0 0
≥
size n = Ω(dlogd/ε). Nature samples X ,...,X i.i.d. from P , with empirical measure
1 n cov
∈ G
P . Thelearnerobserves X˜ ,...X˜ withempiricalmeasureP˜ suchthatWε(P˜ ,P ) ρ.
n 1 n n p n n ≤
WeaimtomatchtheboundofTheorem2,computinganestimatePˆ suchthatW (Pˆ ,P) . √kε+ρ
n 1,k n
forsufficiently large n. Inorder toturntheW projection procedure intoanefficientalgorithm, wereplace
2
theintractableobjectiveW (Q, )withthetractabletracenormobjectivetr(Σ I ) = [λ (Σ )
2 Gcov Q − d + i i Q −
1] ,whichcanbecomputedviaeigen-decomposition.
+
P
Lemma7(Tracenormcomparison) ForQ (Rd),wehave [proof]
∈ P
1tr(Σ 2I ) W Q, 2 tr(Σ I ) .
2 Q − d + ≤ 2 Gcov ≤ Q − d +
(cid:0) (cid:1)
ThisresultunderliesW2PROJECT (Algorithm1),whichapproximatelysolvestheoptimizationproblem
min tr(Σ σ2I ) usingavariantofiterativefiltering(Diakonikolas etal.,2016). Inthealgo-
Q≤ 1−O1 (ε)P˜ n Q − d +
rithmdescription, weidentifyamultisetT RdwiththeuniformdistributionUnif(T). Weemphasizethat
⊆
5.Hereandthroughout, theexistenceofsuchminimizersisnotconsequential butsimplyassumedforcleaner statements. Ap-
proximateminimizersuptosomeadditiveerrorprovidethesameriskboundsuptosaiderror.
6.Wemakenoefforttooptimizethebreakdownpoint ε . Similarresultsforrobustmeanestimationfirstrequiredε ≪ 1/2,
0 0
but this was later alleviated (Hopkinsetal., 2020; Zhuetal., 2022b; DalalyanandMinasyan, 2022), We expect that similar
improvementsarepossibleunderoursettingbutdefersuchoptimizationfuturework—seeSection4foradditionaldiscussion.
7NIETERTGOLDFELDSHAFIEE
Algorithm1:W2PROJECT
Input: Contamination levelsεandρ,uniformdiscrete measureP˜ supported onT Rd
n
⊆
Output: Uniformdiscrete measurePˆ
n
1 σ 50,C 1010
← ←
2 Computeeigen-decomposition λ 1,...,λ d R,v 1,...,v d Rd ofΣ T σ2I d
∈ ∈ −
3 Π
←
i:λi≥0v iv i⊤
4 iftr(Π P(Σ T −σ2I d)) < Cε+Cρ2/εthenreturnPˆ n = Unif(T)// LHS equals tr(Σ T −σ2I d) +
5 else
6 g(x)
←
kΠ(x −µ T) k2
2
forx
∈
T
7 LetL T besetof6εT pointsforwhichg(x)islargest
⊆ | |
8 f(x) g(x)forx Landf(x) 0otherwise
← ∈ ←
9 Removeeachpointx T fromT withprobability f(x)/max x∈T f(x)
∈
10 ReturntoStep1withnewsetT
11 end
thehigh-levelideaoftrimmingsamplesfromacorruptedobservationtocontroltheempiricalcovariancema-
trixisafamiliarparadigm inalgorithmicrobuststatistics(see,e.g.,Klivansetal.,2009;Diakonikolas etal.,
2016;Steinhardtetal.,2018). Ourmaincontributions areshowingthatthisapproachstillapplieswithlocal
adversarialcorruptionsandidentifyingtr(Σ I ) astheappropriatequantitativemeasureforcovariance
Q d +
−
magnitude.
[proof] Theorem8 UnderSettingB,W2PROJECT(P˜ n,ε,ρ)returnsPˆ
n
intimepoly(n,d)suchthat
W (Pˆ ,P) . √kε+ρ+W (P,P ), k [d].
1,k n 1,k n
∀ ∈
withprobability atleast2/3.
OverP (ε,O(√ε)),thisguaranteeattainstheminimaxoptimalerrorfromCorollary5asthe
cov
∈ G ⊆ S
samplesizenincreases(whencetheempiricalestimationerrorvanishes). Ourproofshowsthattheestimate
Pˆ n satisfies tr(Σ Pˆ n −O(1)I d) + . ρ2/ε, mirroring the martingale-based analysis ofiterative filtering with
thesimplerobjectiveλ (Σ );see,e.g.,Section2.4ofDiakonikolas andKane(2023). ViaLemma7,we
max Q
then convert this trace norm bound into a W bound, and proceed with the analysis of the W projection
2 2
fromTheorem2toarriveattheriskboundabove. AswithTheorem2,thegeneralizedstabilitybound(4)is
essential foravoiding a√kρdependence.
The remaining empirical convergence term, W (P,P ), can always be bounded by W (P,P ), and
1,k n 1 n
the covariance bound implies that E[W (P,P )] = O˜(√dn−1/d) for d 2 (see, e.g., Theorem 3.1 of
1 n
≥
LEI, 2020). Generally, we would hope for a faster n−1/k rate, and this is indeed the case under appro-
priate additional assumptions on the clean distribution P. To name a few instances, Linetal., 2021 de-
rive such rates for general k under a Bernstein tail condition or a Poincare´ inequality assumption, while
Niles-WeedandRigollet,2022providerateswhenP satisfiesatransportinequality(Niles-WeedandRigollet,
2022) (which, in particular, holds for sub-Gaussian distributions). Empirical convergence rates in addi-
tionalsettings havebeenderivedinthek = 1case,e.g.,forlog-concave distributions (Nietertetal.,2022a)
and under certain isotropic and moment boundedness assumptions (BartlandMendelson, 2022). Recently,
Boedihardjo (2024) provided bounds for general k under a milder 2 + δ moment assumption, which we
combineherewiththeboundfromTheorem8.
[proof] Corollary 9(Statistical performance) Fix 0 < δ 1. Under Setting B, W2PROJECT(P˜ n,ε,ρ) returns
≤
8ROBUST ESTIMATION UNDERLOCAL AND GLOBALADVERSARIAL CORRUPTIONS
Pˆ intimepoly(n,d)suchthat
n
W 1,k(Pˆ n,P) .√kε+ρ+kdE P X 2+2δ 2+1 2δO˜ (δn)− k∨1 2 . (5)
k k
(cid:16) (cid:17)
(cid:2) (cid:3)
withprobability atleast2/3.
Here,thefactor ofdinthethirdtermarrivesfromtaking aunionboundoverk [d]. Ifonehasafixed
∈
slicing dimension in mind, a priori, this factor can be eliminated. Similarly, by taking a union bound only
overk whichareapowerof2,onemayreducedtologdatthecostofafactorof2withinthepowerofδn.
Since E P[ X 2]21 = √trΣ
P
√d under Setting B, we expect the expectation in the final term to scale
k k ≤
like √d in most settings of interest. To the best of our knowledge it remains open whether one can obtain
W (Pˆ ,P) . k√dO˜(n−1/(k∨2))assumingonlythatΣ I .
1,k n P d
(cid:22)
Withrespecttooptimality,wenotethatthefirsttwotermsin(5)arenecessary,duetotheminimaxlower
bound for within Corollary 5. Further, evenwhen there areno corruptions and ρ = ε = 0, aminimax
cov
G
lowerbound ofNiles-WeedandRigollet (2022)impliesthatΩ(n−1/(k∨2))errorisunavoidable, evenforP
supported withintheunitcube. Wedeferatightcharacterization offinite-sample riskforfuturework.
Remark10(Recovering standardfilteringviaslicedW 2 projection) WenotethatLemma7canbeadapted
to the sliced W setting. In particular, one can approximate W (Q, ) by the operator norm (Σ
2 2,1 cov Q
G k −
I ) = [ Σ 1] . This is equivalent to the standard objective Σ for iterative filtering
d + op Q op + Q op
k k k − k k
(Diakonikolas etal.,2016) (when Σ > 1, and otherwise the algorithm willhave terminated), provid-
Q op
k k
inganewperspective onthisstandard algorithm.
2.3. OtherCorruptionModelsandRobustMeanEstimation
We now discuss some complementary results. First, we remark that if ε = 0, then P˜ itself satisfies the
n
boundW (P˜ ,P) ρ+W (P ,P),triviallymatchingthepreviousupperbounds. Inthecasethatρ= 0
1,k n 1,k n
≤
and we only suffer TV corruption, standard iterative filtering resolves the question of efficient distribution
learning fornear-isotropic P.
Proposition 11 Under Setting B with ρ = 0 and P (4ε,δ), any estimate Pˆ 1 P˜ such that [proof]
∈ Siso ≤ 1−4ε n
Σ 1+O(δ2/ε)satisfies W (Pˆ,P) . √kδ+W (P,P ),forallk [d].
k
Pˆ kop
≤
1,k 1,k n
∈
Indeed, this λ bound is achieved by all stability-based algorithms for robust mean estimation (see,
max
e.g.,Theorem2.11ofDiakonikolas andKane,2023). Ourproofemploysarefinedversionofthecertificate
lemmaforstabledistributions(seeLemma2.7ofDiakonikolas andKane,2023). Theisotropicrestrictionis
standard in algorithmic robust statistics; hardness results suggest it cannot be eliminated without imposing
furtherassumptions likeGaussianity orlosingcomputational tractability (HopkinsandLi,2019).
Next, we comment on the simpler task of robust mean estimation. Under Setting B, we can simply
return the mean of Pˆ
n
= W2PROJECT(P˜ n,ε,ρ) to obtain error O(√ε + ρ). It is not hard to show that
standard iterative filtering also suffices, employing the W decomposition in Lemma3 to bound the extent
1
to which the Wasserstein corruption can perturb second moments after filtering out its ε-tails. However,
neither approach generalizes to (ε,δ), leaving us with a simple open question: under Setting B with
iso
S
P = (µ,I )andn = poly(d,1/ε),canoneefficientlycomputeµˆfromP˜ suchthat µˆ µ = O˜(ε+ρ)?
d n 2
N k − k
9NIETERTGOLDFELDSHAFIEE
3. RobustStochasticOptimization
Finally,wepresentanapplication torobuststochastic optimization. Weconsiderasettingwherethelearner
seekstomakeadecision θˆ Θthatperformswellonadatadistribution P,givenonlyacorrupted observa-
tionP˜ . Moreprecisely, giv∈ enalossfunction L : Θ Rd R,weseektominimizetheriskE [L(θˆ,X)].
n P
× →
Inthefollowingwesuppressdependence ofLonthemodelparametersθ Θandwriteℓ() = L(θ, )fora
∈ · ·
specificfunction. Wealsointroducetheset = L(θ, ) forthewholeclass,andimposethefollowing.
θ∈Θ
L { · }
Assumption1 Fixp 1. Take tobeafamilyofreal-valued loss functions onRd,suchthateachℓ
≥ L ∈ L
isoftheformℓ = ℓ A,whereA :Rd Rk isaffineandℓ : Rk Rd isl.s.c.withsup ℓ(z) < .
◦ → → z∈Z 1+kzkp ∞
In addition to mild regularity conditions, we assume that the loss functions operate on k-dimensional
linear features of the data. For example, this captures k-variate least-squares regression if Θ Rd×k and
⊆
L maps θ Θ and (x,y) Rd−k Rk to L(θ,(x,y)) = θx y 2. This structural assumption is not
∈ ∈ × k − k2
restrictive aswemayalwayssetk = d.
If it is known that P˜ and P are close under W , a popular decision-making procedure is Wasserstein
n p
distributionally robustoptimization (WDRO),whichselects
ℓˆ :=argmin sup E [ℓ].
WDRO ℓ∈L Q
Q:W p(Q,P˜ n)≤r
Indeed, ifW (P,P˜ ) r,thenitiseasytoprovetheexcessriskbound
p n
≤
E [ℓˆ ] E [ℓ ]. sup E [ℓ ] E [ℓ ]=: (ℓ ;P,2r), (6)
P WDRO P ⋆ Q ⋆ P ⋆ p ⋆
− W p(Q,P)≤2r − R
where ℓ = argmin E [ℓ]. The right-hand side, denoted , is termed the p-Wasserstein regularizer
⋆ ℓ∈L P Rp
andcharacterizes acertainvariationalcomplexityoftheoptimallossfunction(see,e.g.,GaoandKleywegt,
2023). Inparticular, wehave (ℓ;P,r) r ℓ .
1 Lip
R ≤ k k
Alas, the assumption that W (P,P˜ ) is small is quite conservative, especially in the high-dimensional
p n
setting, where Wasserstein empirical convergence rates suffer from the curse of dimensionality. In fact,
given the low-dimensional structure imposed in Assumption 1, it is natural to expect that a much smaller
Wasserstein radius would suffice, e.g., as captured by the k-dimensional sliced distance. The next theorem
indeed shows that the inner WDROmaximization problem automatically adapts to the dimensionality of a
given loss function, which provide a new perspective on beating the curse of dimensionality in WDRO,as
discussed indetailattheendofthissection.
Theorem12 FixP,Pˆ (Rd)withW (P,Pˆ) τ,forsomeτ 0. UnderAssumption1,wehave
p p,k
∈ P ≤ ≥
E [ℓ] sup E [ℓ]
P Q
≤
Q∈P(Rd):W p(Pˆ,Q)≤τ
foreachℓ .
∈ L
Proof Take ℓ = ℓ A to be the decomposition guaranteed by Assumption 1. Assume without loss of
◦
generality that Ais linear. By the QR decomposition, we can rewrite ℓ as ℓ BU for U Rk×d such that
UU⊤ = I . Takeℓ˜= ℓ B. Wenowshowthat ◦ ∈
k
◦
sup E [ℓ]= sup E [ℓ˜]. (7)
Q R
Q∈P(Rd):W p(Pˆ,Q)≤τ R∈P(Rk):W p(U♯Pˆ,R)≤τ
10ROBUST ESTIMATION UNDERLOCAL AND GLOBALADVERSARIAL CORRUPTIONS
Forthe“ ”direction, notethatforanyfeasible Qfortheleftsupremum, R = U Qisfeasible fortheright
♯
≤
handsidewithequalobjectivevalue. Forthe“ ”direction,takeanyRfeasiblefortherightsupremum. Let
≥
(UX,Y) be an optimal coupling for the W (U Pˆ,R) problem, where X Pˆ. Taking Q to be the law of
p ♯
∼
X+U⊤(Y UX),wehavethatW (Pˆ,Q)p E[ U⊤(Y UX) p ]= E[ Y UX p ]= W (U Pˆ,R)p
τ,andE [ℓ− ]= E[ℓ˜(UX +UU⊤(Yp UX))≤ ] = Ek [ℓ˜(Y)] − = E [ℓ˜k ],2 asdesik red.− k2 p ♯ ≤
Q R
−
Atthispoint,wenotethatW (U P,U Pˆ) W (P,Pˆ) τ andbound
p ♯ ♯ p,k
≤ ≤
E [ℓˆ] = E [ℓ˜]
P U♯P
sup E [ℓ˜]
R
≤
R∈P(Rk):W 1(R,U♯Pˆ)≤τ
= sup E [ℓˆ] E [ℓ ], (7)
Q P ⋆
−
Q∈P(Rd):W 1(Q,Pˆ)≤τ
asdesired.
Theorem 12impliesthatwemaycenter theWDROprocedure around anydistribution Pˆ,forwhichwe
havecontroloveritsW 1,kdistancefromthetruepopulationP. Remarkably,theW2PROJECTalgorithmpro-
vides acomputationally efficient waytofindsuch adistribution, and Theorem 8further yields therequired
boundontheW error. Wehavethefollowing.
1,k
Corollary 13 Under Setting B and Assumption 1with p = 1, take Pˆ
n
= W2PROJECT(P˜ n,ε,ρ) and let τ
beanyupperboundontheerrorW (Pˆ ,P) . √kε+ρ+W (P,P ). ThentheWDROestimate
1,k n 1,k n
ℓˆ= argmin sup E [ℓ]
ℓ∈L Q
Q∈P(Rd):W p(Pˆ n,Q)≤τ
satisfies theexcess riskboundE [ℓ] E[ℓ ] 2 ℓ τ,whereℓ = argmin E [ℓ].
P − ⋆ ≤ k ⋆ kLip ⋆ ℓ∈L P
Proof Let ℓˆ = ℓ˜ U denote the decomposition given by Theorem 12. Observe that W (U P,U Pˆ )
1 ♯ ♯ n
◦ ≤
W (P,Pˆ ) τ. Wethusbound
1,k n
≤
E [ℓˆ] E [ℓ ] sup E [ℓˆ] E [ℓ ] (Theorem12)
P P ⋆ Q P ⋆
− ≤ −
Q∈P(Rd):W 1(Q,Pˆ n)≤τ
sup E [ℓ ] E [ℓ ] (ℓˆminimizing)
Q ⋆ P ⋆
≤ −
Q∈P(Rd):W 1(Q,Pˆ n)≤τ
sup E [ℓ ] E [ℓ ]
Q ⋆ P ⋆
≤ Q∈P(Rd):W 1(Q,P)≤2τ −
ℓ 2τ,
⋆ Lip
≤ k k
asdesired.
3.1. BeatingtheCurseofDimensionalityinWDRO
Despite promising applications, the classic Wasserstein DRO approach suffers from the curse of dimen-
sionality. The rate of empirical convergence under the Wasserstein distance scales as n−1/d, which can-
not be generally improved when d 3 (FournierandGuillin, 2015; LEI, 2020). In light of this rate,
≥
MohajerinEsfahaniandKuhn (2018) showed that if the WDRO radius is chosen as ρ = O(n−1/d), then
the worst-case expected loss over all distributions in the Wasserstein ambiguity set of that radius would be
11NIETERTGOLDFELDSHAFIEE
anupperboundfortheexpectedlosswithrespecttothetruedata-generating distribution. Thisprovidedthe
firstnon-asymptotic guarantee fortheWasserstein robust solution, butthebound deteriorates exponentially
fastasdgrows.
Toaddressthecurseofdimensionality,anempiricallikelihoodapproachwasproposedin(BlanchetandKang,
2021;Blanchetetal.,2022,2019)tofindthesmallest radius ρsuch that, withhigh probability, there exists
Q (Rd)withW (Q,Pˆ ) ρandℓ⋆ satisfying
p n
∈ P ≤ ∈ L
ℓ⋆ argmin E [ℓ] argmin E [ℓ].
ℓ∈L Q ℓ∈L P
∈ ∩
This choice leads to a confidence region around the optimal solution, which enables working witha radius
ρ= O(n−1/2). However,thisresultisonlyasymptoticinnature,yetfinite-sampleboundsarecrucialforap-
plications. Forcertain WDROswithlinear structure, such as linear regression/classification andkernelized
versionsthereof,non-asymptoticboundsofO(n−1/2),havebeenestablishedin(Shafieezadeh-Abadeh etal.,
2019;ChenandPaschalidis,2018;Oleaetal.,2022;Wuetal.,2022). Torelaxthesestructuralassumptions,
Gao(2022)demonstratedthatifthedata-generating distributionsatisfiesatransport-entropy inequality,ara-
diusofO(n−1/2)isagainsufficient. However,thetransport-entropy inequality assumptionontheunknown
datadistribution isrestrictive andmaybehardtoverify inpractice. Furthermore, thelossfunction requires
to be α-smooth over the family and admits a sub-root function in order to establish local Rademacher
L
complexitybounds.
Corollary 13 present a clean route to overcome the curse of dimensionality in the classical Wasser-
stein DRO setting, when ε = 0, and obtain finite-sample results without relying on transport inequalities.
Moreover, it provides a simple procedure for achieving the excess risk bounds for outlier-robust WDRO
presented in Nietertetal. (2023b) when p = 1. In fact, the algorithm therein matches our √kε + ρ risk
bound only when k = Θ(1) or k = Θ(d), but not in between. Their approach further requires solving
new optimization problems that are more complicated than standard WDRO. Finally, the analysis in that
workledtofinite-sample excessriskbounds including atermscalingliken−1/d evenwhenk = O(1). The
resultofTheorem13accountsforalltheselimitations, yieldingoptimalratesuniformlyinkviasimpleand
computationally efficientprocedures.
4. Concluding Remarks and Future Work
In this work, wehave provided the first polynomial timealgorithm forrobust distribution estimation under
combined Wasserstein and TV corruptions. In order to apply its guarantees to Wasserstein DRO, we un-
covered apractical and conceptually simpletechnique foralleviating the curse ofdimensionality thatoften
manifests itself in this setting. There are numerous directions for future work; some of particular interest
include:
• For distributions that are (2ε,δ)-stable and isotropic, we have efficient algorithms for robust mean
estimationuptoerrorδunderTVε-corruptions. CanweextendtheseresultstoobtainW estimation
p,k
error δ√k +ρ under our combined model? For an even simpler challenge, as posed in Section 2.3
— can one estimate the mean of a spherical Gaussian up to ℓ error O˜(ε)+ρwith both W and TV
2 1
corruption? Wesuspectthattheremaybesimilarobstaclesasthoseknownforrobustmeanestimation
withstablebutnon-isotropic distributions (HopkinsandLi,2019).
• Relatedly, algorithms for robust mean estimation have been refined and optimized in many ways,
improving their breakdown points (Hopkinsetal., 2020; Zhuetal., 2022b; DalalyanandMinasyan,
2022), running time (Chengetal., 2019), and memory usage (Diakonikolas etal., 2017, 2022). We
expectmanyoftheseimprovementstotranslate toourmodel.
12ROBUST ESTIMATION UNDERLOCAL AND GLOBALADVERSARIAL CORRUPTIONS
• Finally,forWDRO,canwetractablyachievedependenceonthedimensionalityk oftheoptimalloss
⋆
function if k k (recalling that k is a uniform bound over the loss function family)? We suspect
⋆
≪
thiscanbeachievedbyintegratingtheobjectiveofW2PROJECT intotheWassersteinDROambiguity
set.
Acknowledgments
Wethankabunchofpeopleandfundingagency.
References
DanaAngluinandPhilipLaird. Learningfromnoisyexamples. MachineLearning,2:343–370, 1988.
YogeshBalaji, RamaChellappa, andSoheilFeizi. Robustoptimaltransport withapplications ingenerative
modelinganddomainadaptation. InAdvancesinNeuralInformation ProcessingSystems, 2020.
Daniel Bartl and Shahar Mendelson. Structure preservation via the wasserstein distance. arXiv preprint
arXiv:2209.07058, 2022.
Amine Bennouna and Bart Van Parys. Holistic robust data-driven decisions. arXiv preprint
arXiv:2207.09560, 2022.
Amine Bennouna, Ryan Lucas, and Bart Van Parys. Certified robust neural networks: Generalization and
corruption resistance. InInternational Conference onMachineLearning, 2023.
JoseBlanchetandYangKang. Sampleout-of-sample inference basedonWasserstein distance. Operations
Research,69(3):985–1013, 2021.
JoseBlanchetandKarthyekMurthy. Quantifyingdistributionalmodelriskviaoptimaltransport. Mathemat-
icsofOperations Research,44(2):565–600, 2019.
JoseBlanchet, YangKang, andKarthyekMurthy. Robust Wasserstein profileinference andapplications to
machinelearning. JournalofAppliedProbability, 56(3):830–857, 2019.
Jose Blanchet, Karthyek Murthy, and Nian Si. Confidence regions in Wasserstein distributionally robust
estimation. Biometrika, 109(2):295–315, 2022.
March T Boedihardjo. Sharp bounds for the max-sliced wasserstein distance. arXiv preprint
arXiv:2403.00666, 2024.
NaderHBshouty,NadavEiron,andEyalKushilevitz. PAClearningwithnastynoise. TheoreticalComputer
Science, 288(2):255–275, 2002.
LuisA.CaffarelliandRobertJ.McCann. FreeboundariesinoptimaltransportandMonge-Ampe`reobstacle
problems. AnnalsofMathematics. SecondSeries,171(2):673–730, 2010.
PatrickChaoandEdgarDobriban. Statistical estimation under distribution shift: Wasserstein perturbations
andminimaxtheory. arXivpreprint arXiv:2308.01853, 2023.
LaetitiaChapel,MokhtarZ.Alaya,andGillesGasso. Partialoptimaltranportwithapplications onpositive-
unlabeled learning. InAdvancesinNeuralInformation Processing Systems,2020.
13NIETERTGOLDFELDSHAFIEE
Ruidi Chen and Ioannis Ch Paschalidis. A robust learning approach for regression models based on distri-
butionally robustoptimization. JournalofMachineLearningResearch,19(1):517–564, 2018.
YuCheng,IliasDiakonikolas,andRongGe. High-dimensionalrobustmeanestimationinnearly-lineartime.
InSIAMSymposium onDiscreteAlgorithms,2019.
Le´na¨ıc Chizat, Gabriel Peyre´, Bernhard Schmitzer, and Franc¸ois-Xavier Vialard. Unbalanced optimal
transport: dynamic and Kantorovich formulations. Journal of Functional Analysis, 274(11):3090–3123,
2018a.
Le´na¨ıc Chizat, Gabriel Peyre´, Bernhard Schmitzer, and Franc¸ois-Xavier Vialard. Scaling algorithms for
unbalanced optimaltransport problems. MathematicsofComputation, 87(314):2563–2609, 2018b.
Arnak SDalalyan and Arshak Minasyan. All-in-one robust estimator of thegaussian mean. TheAnnals of
Statistics, 50(2):1193–1219, 2022.
Ilias Diakonikolas and Daniel M Kane. Algorithmic High-Dimensional Robust Statistics. Cambridge Uni-
versityPress,2023.
Ilias Diakonikolas, Gautam Kamath, Daniel MKane, Jerry Li, AnkurMoitra, and Alistair Stewart. Robust
estimators in high dimensions without the computational intractability. In IEEE Symposium on Founda-
tionsofComputerScience, 2016.
Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Being
robust(inhighdimensions) canbepractical. InInternational Conference onMachineLearning, 2017.
Ilias Diakonikolas, Daniel M Kane, Ankit Pensia, and Thanasis Pittas. Streaming algorithms for high-
dimensional robuststatistics. InInternational ConferenceonMachineLearning, 2022.
DavidL.DonohoandRichardC.Liu. The”Automatic”RobustnessofMinimumDistanceFunctionals. The
AnnalsofStatistics, 16(2):552 –586,1988.
Adria´n Esteban-Pe´rez and Juan MMorales. Distributionally robust stochastic programs withside informa-
tionbasedontrimmings. MathematicalProgramming, 195(1-2):1069–1105, 2022.
Kilian Fatras, Thibault Sejourne, Re´miFlamary, and Nicolas Courty. Unbalanced minibatch optimal trans-
port;applications todomainadaptation. InInternational ConferenceonMachineLearning, 2021.
Alessio Figalli. The optimal partial transport problem. Archive for Rational Mechanics and Analysis, 195:
533–560, 2010.
Nicolas Fournier and Arnaud Guillin. On the rate of convergence in Wasserstein distance of the empirical
measure. Probability TheoryandRelatedFields,162(3-4):707–738, 2015.
TakumiFukunaga andHiroyuki Kasai. Block-coordinate Frank-Wolfe algorithm andconvergence analysis
forsemi-relaxed optimaltransport problem. InIEEEInternational Conference onAcoustics, Speechand
SignalProcessing, 2022.
RuiGao. Finite-sampleguarantees forWasserstein distributionally robustoptimization: Breakingthecurse
ofdimensionality. Operations Research,2022.
Rui Gao and Anton Kleywegt. Distributionally robust stochastic optimization with Wasserstein distance.
MathematicsofOperationsResearch,48(2):603–655, 2023.
14ROBUST ESTIMATION UNDERLOCAL AND GLOBALADVERSARIAL CORRUPTIONS
Leonid G.Hanin. Kantorovich-Rubinstein norm and its application in the theory ofLipschitz spaces. Pro-
ceedings oftheAmericanMathematical Society, 115(2):345–352, 1992.
Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without demo-
graphics inrepeated lossminimization. InInternational Conference onMachineLearning, 2018.
SamHopkins, Jerry Li,andFredZhang. Robust andheavy-tailed meanestimation madesimple, viaregret
minimization. AdvancesinNeuralInformation ProcessingSystems, 33,2020.
SamuelB.HopkinsandJerryLi. Howhardisrobustmeanestimation? InConference onLearningTheory,
2019.
WeihuaHu,GangNiu,IsseiSato,andMasashiSugiyama. Doesdistributionally robustsupervised learning
giverobustclassifiers? InInternational ConferenceonMachineLearning, 2018.
Peter J. Huber. Robust Estimation of a Location Parameter. The Annals of Mathematical Statistics, 35(1):
73–101, 1964.
AdamRKlivans,PhilipMLong,andRoccoAServedio. Learninghalfspaceswithmaliciousnoise. Journal
ofMachineLearningResearch,10(12), 2009.
Khang Le, Huy Nguyen, Quang M Nguyen, Tung Pham, Hung Bui, and Nhat Ho. On robust optimal
transport: Computational complexity and barycenter computation. In Advances in Neural Information
Processing Systems,2021.
JINGLEI. Convergenceandconcentration ofempiricalmeasuresunderWassersteindistanceinunbounded
functional spaces. Bernoulli, 26(1):767–798, 2020.
Matthias Liero, Alexander Mielke, and Giuseppe Savare´. Optimal entropy-transport problems and a new
Hellinger-Kantorovichdistancebetweenpositivemeasures. InventionesMathematicae,211(3):969–1117,
2018.
Tianyi Lin, Chenyou Fan, Nhat Ho, Marco Cuturi, and Michael Jordan. Projection robust Wasserstein
distance andRiemannianoptimization. InAdvancesinNeuralInformation ProcessingSystems, 2020.
Tianyi Lin, Zeyu Zheng, Elynn Chen, Marco Cuturi, and Michael I Jordan. On projection robust optimal
transport: Sample complexity and model misspecification. In International Conference on Artificial
Intelligence andStatistics, 2021.
Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization using the
Wasserstein metric: Performance guarantees and tractable reformulations. Mathematical Programming,
171(1-2):115–166, 2018.
Debarghya Mukherjee, Aritra Guha, Justin Solomon, Yuekai Sun, and Mikhail Yurochkin. Outlier-robust
optimaltransport. InInternational Conference onMachineLearning,2021.
Kimia Nadjahi, Alain Durmus, Le´na¨ıc Chizat, Soheil Kolouri, Shahin Shahrampour, and Umut Simsekli.
Statisticalandtopologicalpropertiesofslicedprobabilitydivergences. InAdvancesinNeuralInformation
Processing Systems,2020.
J. Saketha Nath. Unbalanced optimal transport using integral probability metric regularization. arXiv
preprintarXiv:2011.05001, 2020.
15NIETERTGOLDFELDSHAFIEE
Sloan Nietert, Rachel Cummings, and Ziv Goldfeld. Outlier-robust optimal transport with applications to
generativemodelinganddataprivacy. InTheoryandPracticeofDifferentialPrivacyWorkshopatICML,
2021.
SloanNietert,ZivGoldfeld,RitwikSadhu,andKengoKato. Statistical,robustness,andcomputationalguar-
anteesforslicedwasserstein distances. InAdvances inNeuralInformation Processing Systems,2022a.
Sloan Nietert, Ritwik Sadhu, Ziv Goldfeld, and Kengo Kato. Statistical, robustness, and computational
guarantees for sliced Wasserstein distances. In Advances in Neural Information Processing Systems,
2022b.
Sloan Nietert, Rachel Cummings, and Ziv Goldfeld. Robust estimation under the Wasserstein distance.
arXivpreprintarXiv:2302.01237, 2023a.
SloanNietert,ZivGoldfeld, andSorooshShafiee. Outlier-robust Wasserstein DRO. InAdvancesinNeural
Information Processing Systems,2023b.
Jonathan Niles-Weed and Philippe Rigollet. Estimation of Wasserstein distances in the spiked transport
model. Bernoulli, 28(4):2663–2688, 2022.
Jose´ LuisMontielOlea,CynthiaRush,AmilcarVelez,andJohannes Wiesel. Onthegeneralization errorof
normpenaltylinearregression models. arXivpreprint arXiv:2211.07608, 2022.
Benedetto Piccoli and Francesco Rossi. Generalized Wasserstein distance and its application to transport
equations withsource. ArchiveforRationalMechanicsandAnalysis,211(1):335–358, 2014.
ElvezioMRonchettiandPeterJHuber. RobustStatistics. JohnWiley&SonsHoboken, 2009.
FilippoSantambrogio. OptimalTransportforAppliedMathematicians. Springer, 2015.
Bernhard Schmitzer and Benedikt Wirth. A framework forWasserstein-1-type metrics. Journal of Convex
Analysis, 26(2):353–396, 2019.
Soroosh Shafieezadeh-Abadeh, Daniel Kuhn, and Peyman Mohajerin Esfahani. Regularization via mass
transportation. Journal ofMachineLearningResearch,20(103):1–68, 2019.
AmanSinha,Hongseok Namkoong,andJohnDuchi. Certifyingsomedistributional robustness withprinci-
pledadversarial training. InInternational ConferenceonLearningRepresentations, 2018.
JacobSteinhardt, MosesCharikar,andGregoryValiant. Resilience: Acriterionforlearning inthepresence
ofarbitrary outliers. InInnovations inTheoretical ComputerScience Conference, volume94,2018.
Ce´dricVillani. TopicsinOptimalTransportation. Graduate Studies inMathematics. AmericanMathemati-
calSociety,2003.
QinyuWu,Jonathan Yu-Meng Li,andTiantian Mao. Ongeneralization andregularization viaWasserstein
distributionally robust optimization. arXivpreprint arXiv:2212.05716, 2022.
Runtian Zhai, Chen Dan, Zico Kolter, and Pradeep Ravikumar. DORO: Distributional and outlier robust
optimization. InInternational ConferenceonMachineLearning,2021.
Banghua Zhu, Jiantao Jiao, and Jacob Steinhardt. Generalized resilience and robust statistics. The Annals
ofStatistics, 50(4):2256 –2283,2022a.
Banghua Zhu,Jiantao Jiao, andJacob Steinhardt. Robustestimation viageneralized quasi-gradients. Infor-
mationandInference: AJournaloftheIMA,11(2):581–636, 2022b.
16ROBUST ESTIMATION UNDERLOCAL AND GLOBALADVERSARIAL CORRUPTIONS
Appendix A. Proofs forSection 2.1
Throughout thissection, weproveresultsunderamoregeneral learningenvironment.
SettingA2: FixTVcorruptionlevel0 ε 0.49andW corruption levelρ 0,wherep 1,2 .
p
≤ ≤ ≥ ∈ { }
Let (Rd). NatureselectsP ,andthelearnerobserves P˜ suchthatWε(P˜,P) ρ.
G ⊆ P ∈ G p ≤
Webeginwithsomeauxiliary definitions andlemmas.
Definition14(Resilience) ForP (Rd)and0 ε< 1,themeanε-resilience ofP isgivenby
∈ P ≤
τ(P,ε) := sup µ µ .
Q P 2
k − k
Q∈P(Rd):Q≤ 1 P
1−ε
For p 1, the pth-order ε-resilience of P (Rd) is defined by τ (P,ε) := τ(f P,ε), where f(z) =
p p ♯
z µ≥ p. Forafamily (Rd),wedefin∈ eP τ( ,ε) := sup τ(P,ε)andτ ( ,ε) := sup τ (P,ε).
k − P k2 G ⊆ P G P∈G p G P∈G p
Itgenerally sufficestoanalyzeresilience forεbounded awayfrom1,duetothefollowingresult.
Lemma15 ForeachP (Rd)and0< ε < 1,wehaveτ(P,1 ε) = 1−ετ(P,ε).
∈P − ε
Proof IfP = (1 ε)Q+εRforQ,R (Rd),wehave
− ∈ P
1 ε 1 ε
µ µ = − µ µ − τ(P,ε).
P R 2 P Q 2
k − k ε k − k ≤ ε
SupremizingoverRgivesonedirection, andsubstituting ε 1 εgivestheother.
← −
Wealsoobserveacertainmonotonicity ofpthmomentresilience termsinp.
Lemma16 Fix1 p < q,0 ε < 1,andP (Rd). Thenτ(P,ε) τ (P,ε)1/p τ (P,ε)1/q.
p q
≤ ≤ ∈ P ≤ ≤
Proof TakeX P andY Q,foranyQ (Rd)suchthatQ 1 P. Wethenbound
∼ ∼ ∈ P ≤ 1−ε
µ µ E[ Y X ] E[ Y µ X µ ] τ (P,ε).
Q P 2 2 P 2 P 2 1
k − k ≤ k − k ≤ k − k −k − k ≤
Moreover, writinga = E[ X µ p ],b = E[ Y(cid:12) µ p ],andr = q/p 1,w(cid:12)ehave
k − P k2 k (cid:12) − P k2 ≥ (cid:12)
E[ Y µ p X µ p ] a b ar br 1/r = E[ Y µ q X µ q ] p/q .
k − P k2−k − P k2 ≤ | − | ≤ | − | k − P k2−k − P k2
Raisin(cid:12)gbothsidestothe(1/p)thpow(cid:12)erandsupremizing overQc(cid:12)ompletes theproof. (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
Stabilityessentially capturesresilience infirstandsecondmoments.
Lemma17 Let0< ε < 1andδ ε. ForP (ε,δ), wehaveτ(P,ε) δ andτ (P,ε) 2dδ2/ε.
2
≥ ∈ S ≤ ≤
Proof Mean resilience follows directly from the definition of (ε,δ)-stability. For second moment re-
silience, wefixanyQ 1 P andbound
≤ 1−ε
δ2 2δ2
tr(Σ (µ )) tr(Σ ) = tr(Σ Σ ) + µ µ 2 +δ2 .
Q P − P | Q − P | k Q − P k2 ≤ ε ≤ ε
(cid:12) (cid:12)
Supremizingo (cid:12)verQgivesthelemma.
(cid:12)
Next,wecompute useful resilience bounds fordistributions thatlienear (ε,δ) under W . Forbrevity,
2
S
wedefine (ε,δ,λ) := P (Rd) :W (p, (ε,δ)) λ .
2
S { ∈ P S ≤ }
17NIETERTGOLDFELDSHAFIEE
Lemma18 Let0< ε < 1,δ ε,andλ 0. ForP (ε,δ,λ), wehave
≥ ≥ ∈ S
2λ√ε
τ(P,ε) δ+
≤ 1 ε
−
4dδ2 16λ2
τ (P,ε) +
2
≤ (1 ε)ε 1 ε
− −
Finally,wehavetr(Σ ) 2d+4λ2 and,foranyQ 1 P,
P ≤ ≤ 1−ε
6dδ2 20λ2
tr(Σ ) + .
Q ≤ (1 ε)ε2 1 ε
− −
Proof Fix P (ε,δ) such that W (P ,P) λ, and let X,Y be optimal coupling for W (P ,P).
0 2 0 2 0
∈ S ≤
Fix any Q 1 P. Augmenting the probability space if necessary, we can realize Q as the law of Y
≤ 1−ε
conditioned on an event E with probability 1 ε. Write P′ for the law of X conditioned on E. We then
− 0
bound
ε
µ µ = E[Y] E[Y Ec]
Q P 2 2
k − k 1 εk − | k
−ε
( E[X] E[X Ec] + E[Y] E[X] + E[Y Ec] E[X Ec] )
2 2 2
≤ 1 ε k − | k k − k k | − | k
−
ε 1
E[X] E[X Ec] +W (P ,P)+ W (P ,P)
2 2 0 2 0
≤ 1 ε k − | k √ε
− (cid:18) (cid:19)
2√ε
≤ kµ P0 −µ P 0′ k2+ 1 ελ
−
2√ε
δ+ λ.
≤ 1 ε
−
SupremizingoverQgivesthemeanresilience bound. Next,weuseMinkowski’sinequality tobound
tr Σ Σ = E[ Y E[Y] 2] tr(Σ )
P − P0 k − k2 − P0
2
(cid:12) (cid:12) (cid:0) (cid:1)(cid:12) (cid:12)
≤
(cid:12) (cid:12) E[ kX −E[Y] k2 2]1 2 +E[ kY(cid:12) (cid:12) −X k2 2]21 + kE[X] −E[Y] k2 −tr(Σ P0)
(cid:12) (cid:12)
(cid:12)(cid:16) 2 (cid:17) (cid:12)
(cid:12) (cid:12)
tr(Σ )+2λ tr(Σ )
≤
(cid:12) P0
−
P0 (cid:12)
(cid:12) (cid:12)
≤
4(cid:12) (cid:12) (cid:12)(cid:16) λp tr(Σ P0)+4λ(cid:17) 2 (cid:12) (cid:12)
(cid:12)
≤
4λp√d+4λ2, (Σ
P0
(cid:22)
I d)
whichimpliesthedesiredboundontr(Σ ). Thesameargument viaMinkowski’sinequality gives
P
4λ√d 4λ2
E[ X E[X] 2 Ec] E[ Y E[Y] 2 Ec] + .
k − k2| − k − k2| ≤ √ε ε
(cid:12) (cid:12)
(cid:12) (cid:12)
18ROBUST ESTIMATION UNDERLOCAL AND GLOBALADVERSARIAL CORRUPTIONS
Wethencompute
tr(Σ (µ ) Σ ) = E Y E[Y] 2 E E Y E[Y] 2
Q P − P k − k2 − k − k2
ε
(cid:12) (cid:12)= (cid:12) (cid:2) E Y E[(cid:12)Y](cid:3)2 (cid:2)E Y E[Y](cid:3)2(cid:12) Ec
(cid:12) (cid:12) (cid:12)1 ε k − (cid:12) k2 − k − k2(cid:12)
−
ε (cid:12) (cid:12) (cid:2) E X E[X](cid:3) (cid:2) E X E[X](cid:12) (cid:12) (cid:3) E(cid:12) (cid:12)c + 8λ√d + 8λ2
2 2
≤ 1 ε k − k − k − k √ε ε
!
−
(cid:12) (cid:2) (cid:3) (cid:2) (cid:12) (cid:3)(cid:12)
(cid:12) (cid:12)8λ√(cid:12)εd 8λ2
= E X E[X] E E X E[X] + +
2 2
k − k − k − k 1 ε 1 ε
− −
(cid:12) (cid:2) 8λ√εd(cid:12) (cid:3) 8λ2(cid:2) (cid:3)(cid:12)
(cid:12) (cid:12) (cid:12)
τ (P ,ε)+ +
2 0
≤ 1 ε 1 ε
− −
2dδ2 8λ√εd 8λ2
+ + (Theorem17)
≤ ε 1 ε 1 ε
− −
2
1 √2dδ
+√8λ (δ ε)
≤ 1 ε √ε ≥
!
−
4dδ2 16λ2
+
≤ (1 ε)ε 1 ε
− −
SupremizingoverQgivesthesecondmomentresilience bound. Finally, webound
tr(Σ ) = tr(Σ )+tr(Σ (µ ) Σ ) µ µ 2
Q P Q P − P −k P − Q k2
tr(Σ )+τ (P,ε)
P 2
≤
4dδ2 16λ2
2d+4λ2+ +
≤ (1 ε)ε 1 ε
− −
6dδ2 20λ2
+ ,
≤ (1 ε)ε2 1 ε
− −
asdesired.
Finally,weproveatechnical lemmausedthroughout.
Lemma19 Fix0 < ε< 1andP (ε,δ,λ). SupposethatQ = (1 ε)P′+εRforsomeP′,R (Rd)
∈ S − ∈ P
suchthatP′ 1 P. Then,for1 q 2,wehave
≤ 1−ε ≤ ≤
W q(P,Q) 7ε1 q−1 δ√d + 12ε1 q−1 2λ +2ε1 q tr(Σ R(µ P)).
≤ √1 ε √1 ε
− −
p
Proof WriteP = (1 ε)P′+εS,forsomeS (Rd). Foranyq [1,2],wehave
− ∈P ∈
W (P,Q)q εW (S,R)q
q q
≤
2qε W (S,δ )q +W (δ ,R)q
≤
q µP q µP
2qε W (P,δ )q +τ (P,1 ε)+W (δ ,R)q .
≤
(cid:0) q µP q
−
(cid:1) q µP
(cid:0) (cid:1)
19NIETERTGOLDFELDSHAFIEE
Takingpthrootsandapplying Lemma18,weobtain
W q(P,Q)
≤
2ε1 qW q(P,δ µP)+2ε1 qτ q(P,1 −ε)1 q +2ε1 qW q(δ µP,R)
≤
2ε1 qW 2(P,δ µP)+2ε1 q τ 2(P,1 −ε)+2ε1 qW 2(δ µP,R)
2ε1
q tr(Σ
P)+2ε1 q−1
2p τ
2(P,ε)+2ε1
q tr(Σ R(µ P))
≤
2ε1
qp 2d+4λ2+2εq1−p
1
2
4dδ2
+
1p6λ2
+2ε1
q tr(Σ R(µ P))
≤ s(1 ε)ε 1 ε
− −
p p
7ε1 q−1 δ√d 12ε1 q−1 2λ 1
+ +2εq tr(Σ R(µ P)),
≤ √1 ε √1 ε
− −
p
asdesired.
As a consequence, we can bound the Wasserstein distance between a stable distribution and any of its
ε-deletions. InNietertetal.(2023a),thisiscalleda“Wasserstein resilience” bound.
Lemma20(Wasserstein resilience fromstability) Fix0 < ε < 1,P (ε,δ,λ), andQ 1 P. Then,
∈ S ≤ 1−ε
for1 q 2,wehave
≤ ≤
12ε1 q−1 δ√d 21ε1 q− 21 λ
W (P,Q) + .
q
≤ √1 ε √1 ε
− −
Proof WritingP = (1 ε)Q+εRforsomeR (Rd),weuseLemma18tobound
− ∈ P
tr(Σ (µ )) tr(Σ )+τ (P,ε)
Q P P 2
≤
4dδ2 16λ2
2d+4λ2+ +
≤ (1 ε)ε 1 ε
− −
6dδ2 20λ2
+ .
≤ (1 ε)ε2 1 ε
− −
NotingthatQ = (1 ε)Q+εQandQ 1 P,weuseLemma19tobound
− ≤ 1−ε
W q(P,Q) 7ε1 q−1 δ√d + 12ε1 q−1 2λ +2ε1 q tr(Σ Q(µ P))
≤ √1 ε √1 ε
− − q
7ε1 q−1 δ√d 12ε1 q−1 2λ 1 6dδ2 20λ2
+ +2εq +
≤ √1 ε √1 ε s(1 ε)ε2 1 ε
− − − −
12ε1 q−1 δ√d 21ε1 q− 21 λ
+ ,
≤ √1 ε √1 ε
− −
asdesired.
A.1. ProofofboundsinExample1
We first observe that a distribution is (ε,δ)-stable if and only if all of its 1-dimensional orthogonal projec-
tionsare(ε,δ)-stable. Wethusassumethatd = 1withoutlossofgenerality.
20ROBUST ESTIMATION UNDERLOCAL AND GLOBALADVERSARIAL CORRUPTIONS
Next we prove a useful stability bound under the more general condition of an Orlicz norm bound.
Recall that an Orlicz function is any convex, non-decreasing function ψ : R R such that ψ(0) = 0
+ +
→
and ψ(x) as x . Fora real random variable X, we define its Orlicz norm with respect to ψ by
→ ∞ → ∞
X := sup σ 0 :E[ψ(X /σ)] 1 .
ψ
k k { ≥ | | ≤ }
Lemma21 Suppose that X E[X] σ, where ψ is an Orlicz function satisfying ψ(x) = φ(x2) for
ψ
k − k ≤
another Orliczfunctionφ. ThenX is ε,O(σεψ−1(1/ε)) -stablefor0 ε 0.99.
≤ ≤
Proof Weassume without loss
ofg(cid:0)
enerality
thatE[X](cid:1)
= 0. Formeanresilience, take E tobe anyevent
withprobability 1 ε′ 1 ε,andbound
− ≥ −
ε′
E[X E] = E[X Ec]
| | | 1 ε′| | |
−
ε′
E X Ec
≤ 1 ε′ | |
−
σε′ (cid:2) (cid:12) (cid:3) X
ψ−1 E(cid:12) ψ | | Ec
≤ 1 ε′ σ
− (cid:18) (cid:20) (cid:18) (cid:19) (cid:12) (cid:21)(cid:19)
σε′ 1 (cid:12)
ψ−1 (cid:12)
(cid:12)
≤ 1 ε′ ε′
− (cid:18) (cid:19)
σε 1
ψ−1 =:δ.
≤ 1 ε ε
− (cid:18) (cid:19)
We note that this approach is well-known; see, e.g., Lemma E.2 of Zhuetal. (2022a). For the second
momentcondition, weapplytheboundabovetoX2 (with X2 σ2),deducing
φ
k k ≤
σ2ε 1
E[X2 E] E[X2] φ−1
| | − | ≤ 1 ε ε
− (cid:18) (cid:19)
σ2ε 1 2
ψ−1
≤ 1 ε ε
− (cid:18) (cid:19)
δ2
.
≤ ε
Combining, webound Var[X E] Var[X] δ2/ε+E[X E]2 2δ2/ε. Thus,X is(ε,√2δ)-stable.
| | − | ≤ | ≤
Wenowturntothespecificexamples.
Bounded covariance: We apply the lemma with ψ(x) = x2 and φ(x) = x, giving (ε,O(√ε))-stability.
Thenear-isotropic restriction isvacuoussinceδ & √ε.
Sub-Gaussian: Weapplythelemmawithψ(x) = exp(x2) 1andφ(x) = exp(x) 1,giving(ε,O(ε log(1/ε)))-
− −
stability.
p
Log-concave: IfX islog-concave withbounded variance, then X = O(1)forψ (x) = exp(x) 1,
k
kψ0 0
−
byBorell’slemma. Thereisaveryslightnon-convexity toexp(√x) 1,whichweremedybytaking
−
exp(√x) 1, x 1
φ(x) := − ≥
(e/2+ex /2 1, 0 x < 1
| | − ≤
andψ(x) = φ(x2). Westillhave X = O(1),giving(ε,O(εlog(1/ε))-stability.
ψ
k k
21NIETERTGOLDFELDSHAFIEE
Boundedqthmoments,q 2: Weapplythelemmawithψ(x) = xq,giving(ε,O(ε1−1/q))-stability.
≥
A.2. ProofofLemma3
Weproveastronger result.
Lemma22 Fix 0 < τ < 1, 1 p < q, and P,Q (Rd) such that W (P,Q) ρ. Then there exists
p
≤ ∈ P ≤
R
∈
P(Rd)suchthatW p(P,R)
≤
ρ,W q(P,R)
≤
q−q
p
1/q ρτ1/q−1/p,and kR −Q kTV
≤
τ.
Proof Let (X,Y) be an optimal coupling for W(cid:0)(P,Q(cid:1) ), and let ∆ = Y X. Writing τ for the 1 τ
p
− −
quantile of ∆ ,letE denotetheeventthat ∆ τ. ByMarkov’sinequality, wehave
2 2
k k k k ≤
ρp
τ =Pr( ∆ > τ)= Pr( ∆ p > τp) ,
k k2 k k2 ≤ τp
andsoτ ρτ−1/p. ConsidertherandomvariableZ whichequalsY if ∆ τ andX otherwise. Taking
2
≤ k k ≤
RtobethelawofZ,wehave R Q TV τ,
k − k ≤
W (P,R)p E[ X Z p ] E[ X Y p ]= ρp,
p ≤ k − k2 ≤ k − k2
and
W (P,R)q E[ X Z q ]
q ≤ k − k2
E ∆ q E
≤ k k2
τq
(cid:2) (cid:12) (cid:3)q
Pr (cid:12)∆ > t E dt
≤ k k2
Z0
τq (cid:2) (cid:12) (cid:3)
= Pr ∆
p
>
tp(cid:12)/q
E dt
k k2
Z0
h τq (cid:12) i
E[ ∆ p E] t−p/q(cid:12) dt
≤ k k2|
Z0
ρp
t1−p/q
τq
≤ 1 p/q 0
−
q (cid:12)
= ρpτq−(cid:12)p
q p
−
q
ρp(ρτ−1/p)q−p
≤ q p
−
q
= ρqτ1−q/p.
q p
−
Takingqthrootsgivesthelemma.
Asaconsequence, wealsohavethefollowing.
Lemma23 Fix0 < ε < 1/2, integers p,q 1, and P,Q (Rd)such that Wε(P,Q) ρ. Then there
≥ ∈ P p ≤
existsR (Rd)suchthatW p(P,R) ρ,W q(P,R) √2ρε−[1/p−1/q]+,and R Q TV 2ε.
∈ P ≤ ≤ k − k ≤
Proof By the W pε bound, there exists P′
∈
P(Rd) such that W p(P,P′)
≤
ρ and kP′ −P˜ kTV
≤
ε. If
q p, then we can simply take R = P′. Otherwise, q p +1, and we can apply Lemma 22 between
≤ ≥
P and P′ to obtain R such that W q(P,R)
≤
q−q
p
1/q ρε1/q−1/p
≤
√2ρε−[1/p−1/q]+ and kR −Q kTV
≤
R P′ TV + P′+P˜ TV 2ε. (cid:0) (cid:1)
k − k k k ≤
22ROBUST ESTIMATION UNDERLOCAL AND GLOBALADVERSARIAL CORRUPTIONS
A.3. ProofofLemma4
Defining (ε,δ,λ) := P (Rd): W (P, (ε,δ)) λ ,weproveastronger statement.
2
S { ∈P S ≤ }
Lemma24 Fix 0 ε < 1/2 and (2ε,δ,λ). Then, for all 1 q < 2 and k [d], we
≤ G ⊆ S ≤ ∈
have W ( , TV ε) . 1 (√kε1/q−1δ + λε1/q−1/2), for all k [d]. This risk is achieved
R q,k G k · k ≤ 1−2ε ∈
by the minimum distance estimator T satisfying T(P˜) argmin W (Q, ), where W (Q, ) :=
∈ Q≤ 1−1 εP˜ 2 G 2 G
inf W (Q,R).
R∈G 2
Proof GivenP andP˜ suchthat P˜ P TV ε,wedecomposeP˜ = (1 ε)P′+εRfordistribution
∈ G k − k ≤ −
P′ andRsuchthatP′ 1 P. ObservethattheestimatePˆ = T(P˜)satisfies
≤ 1−ε
W (Pˆ, ) = min W (Q, )
2 2
G Q≤ 1 p˜ G
1−ε
W (P′, )
2
≤ G
W (P′,P)
2
≤
12ε1 q−1 δ√d 21ε1 q− 21 λ
+ . (Lemma20)
≤ √1 ε √1 ε
− −
Thus,Pˆ (2ε,δ,λ′),where
∈ S
12ε1 q−1 δ√d 21ε1 q−1 2λ 12ε1 q−1 δ√d 22λ
λ′ := + +λ + .
√1 ε √1 ε ≤ √1 ε √1 ε
− − − −
Of course P (2ε,δ,λ′) as well, and Pˆ P TV Pˆ P˜ TV + P˜ P TV 2ε. Defining the
∈ S k − k ≤ k − k k − k ≤
midpointdistribution Q = (1 Pˆ P TV)−1Pˆ P,weagainapplyLemma20tobound
−k − k ∧
W (Pˆ,P) W (Pˆ,Q)+W (Q,P)
q q q
≤
24(2ε)1 q−1 δ√d 42(2ε)1 q−1 2λ′
+
≤ √1 2ε √1 2ε
− −
528ε1 q−1 δ√d 924ε1 q−1 2λ
+ ,
≤ 1 2ε 1 2ε
− −
asdesired. Fork < d,weobservethat,foreachorthogonal projection U Rk×d,UU⊤ = I ,westillhave
k
U ♯P U ♯Pˆ TV 2εandU ♯P,U ♯Pˆ (2ε,δ,λ′),sotheanalysis abov∈ ecanbeapplied inRk togivethe
k − k ≤ ∈ S
desiredboundunderW .
q,k
A.4. ProofofCorollary5
Upper bounds follow by Theorem 2. Matching lower bounds (up to logarithmic factors) when ρ = 0 are
showninTheorem2ofNietertetal.(2023a). Itisstraightforwardtoraisetheselowerboundsbytheneeded
term of ρ. Indeed, suppose the learner observes P˜ = δ but the adversary flipsafair coin to select P = δ
0 0
or P = δ with x = ρ. Then, no estimate Pˆ can incur expected risk less than that of Pˆ = δ , for
x 2 ρ/2
k k
whichW (Pˆ,P) = ρ/2. (SeeproofofTheorem2inNietertetal.(2023b)foramoreformaltreatmentof
1,k
thistwopointmethod).
23NIETERTGOLDFELDSHAFIEE
Appendix B. Proofs forSection 2.2
Throughout thissection, weproveresultsunderamoregeneral learningenvironment.
Setting B2: Fix ε > 0 sufficiently small, ρ 0, and p 1,2 . Nature selects a distribution
≥ ∈ { }
P and produces P′ such that W (P′,P) ρ and W (P′,P) ρε1/2−1/p. The learner
cov p 2
∈ G ≤ ≤
observes P˜ such that P˜ P′ TV ε. All of P, P′, and P˜ are uniform discrete measures over n
k − k ≤
pointsinRd.
The W bound is without loss of generality by Lemma 3 (no such decomposition is necessary when
2
p = 2,wherewemaysimplytakeP′ = P).
B.1. ProofofLemma7
We first prove that W (Q, )2 tr(Σ I ) . Assume without loss of generality that Q has mean 0.
2 cov Q d +
G ≤ −
Write λ λ 0 for the eigenvalues of Σ , with accompanying eigenvectors v ,...,v Rd.
1 d Q 1 d
≥ ··· ≥ ≥ ∈
DefineA = d 1 (1/√λ ) v v⊤,sothatR = A QsatisfiesΣ I . TakingX Q,webound
i=1 ∧ i i i ♯ R (cid:22) d ∼
P (cid:0) (cid:1) W (Q, )2 W (Q,R)2
2 cov 2
G ≤
E[ X AX 2]
≤ k − k2
= E[ (I A)X 2]
k − k2
= tr((I A)2Σ )
Q
−
2
= λ 1
i
−
i: Xλi>1
(cid:0)p (cid:1)
λ 1
i
≤ −
i: Xλi>1
= tr(Σ I ) ,
µ d +
−
asdesired.
Next,weprovethattr(Σ 2I ) 2W Q, 2 . SupposethatW (Q,R) λforsomeR .
Q d + 2 cov 2 cov
− ≤ G ≤ ∈ G
Assumewithoutlossofgenerality thatRhasmean0,andfixanyRd×d with A 1. Taking(AX,AY)
op
(cid:0) (cid:1) k k ≤
tobeanoptimalcouplingfortheW (A Q,A R)problem,sothat(X,Y)isacouplingofQandR,wehave
2 ♯ ♯
tr(A⊤AΣ )= E[ A(X E[X]) 2]
Q k − k2
E[ AX 2]
≤ k k2
= E[ AY +A(X Y) 2]
k − k2
2
E[ AY 2]+ E[ A(X Y) 2] (Minkowski’sinequality)
≤ k k2 k − k2
(cid:18)q q (cid:19)
2
= tr(A⊤AΣ )+W (A Q,A R)
R 2 ♯ ♯
(cid:18)q (cid:19)
2
tr(A⊤AΣ )+λ ( A 1)
R op
≤ k k ≤
(cid:18)q (cid:19)
tr(A⊤A(2Σ ))+2λ2
R
≤
tr(A⊤A2I )+2λ2. (R (σ))
d cov
≤ ∈ G
24ROBUST ESTIMATION UNDERLOCAL AND GLOBALADVERSARIAL CORRUPTIONS
Rearranging andsupremizing overA,wefindthattr(Σ 2I ) 2λ2,asdesired.
Q d +
− ≤
B.2. ProofofTheorem8
WeshallproveastrongerstatementunderSettingB2. Werequireaslightchangetotheterminationcondition
when p = 2. Specifically, we define the modified algorithm W2PROJECT2 by changing the termination
condition atStep4from
tr(Π(Σ σ2I )) < Cε+Cρ2/ε
T d
−
to
tr(Π(Σ σ2I )) < Cε+Cρ2ε1−2/p.
T d
−
Forthisalgorithm (whichmatches W2PROJECT whenp = 1),weprovethefollowing.
Lemma25 Under Setting B2 with ε
0
2−20, W2PROJECT2(P˜,ε,ρ) returns Pˆ in time poly(n,d) such
≤
that,forallq 1,2 andk [d],
∈{ } ∈
W q,k(Pˆ,P) .ε1 q−1 2√k+ε− p1− q1 +ρ
(cid:2) (cid:3)
withprobability atleast2/3.
Proof First, it is easy to show that the algorithm runs in polynomial time. Atleast one point is removed at
eachiteration, sotherecanbeatmostniterations, andeachiteration canbeperformedinpoly(n,d)time.
Next, we show that W2PROJECT approximately minimizes its trace norm objective. We begin with
a technical claim about the function f computed in Steps 6-8. This result mirrors Proposition 2.19 of
Diakonikolas andKane(2023),whichpertains tothesimplerobjective λ (Σ ).
max Q
Lemma26 Under Setting B2, let S Rd denote the support of P′. Suppose that, in some iteration of
⊆
W2PROJECT, the multiset T satisfies T S (1 4ε)S and tr(Σ
T
σ2I d)
+
233ε+219ρ2ε1−2/p.
| ∩ | ≥ − | | − ≥
Thenthefunction f computedinSteps6-8satisfies f(x) 2 f(x).
x∈T ≥ x∈T∩S
P P
ProofUnderSettingB2,wehaveW (S, ) W (S,P) ρε1/2−1/p =:λ. Since (6ε,5√ε),
2 cov 2 cov iso
G ≤ ≤ G ⊆ S
asdescribed inExample1(theconstantcanbeobtained fromLemma21),wehavethatS (6ε,5√ε,λ),
∈ S
usingthenotationfromtheproofofLemma4.
Now,atStep6, W2PROJECT computes thefunction g(x) = kΠ(x −µ T) k2 2,whereΠistheorthogonal
projection onto the non-negative eigenspace of Σ σ2I . At Step 7, we take L to be the set of 6εT
T d
− | |
elements of T for which g(x) is largest. Then, Step 8 takes f(x) = g(x) for x L and f(x) = 0
∈
otherwise. Defineη := tr(Σ σ2I ) = tr(Π(Σ σ2I )), which, by the lemma assumption, satisfies
T d + T d
− −
η 233ε+219λ2. Thislowerboundwillbeusedlater. Wefirstcompute
≥
g(x) = T tr(ΠΣ ) = T (η+σ2tr(Π)).
T
| | | |
x∈T
X
Next, we apply Lemma 18 to Π S, noting that S, and hence Π S, belongs to (6ε,5√ε,λ). For any
♯ ♯
S
S′ S with S′ (1 6ε)S , thisgives
⊆ | | ≥ − | |
2λ√6ε
µ S µ S′ 2 5√ε+ 5√ε+10λ√ε (ε 1/12)
k − k ≤ 1 6ε ≤ ≤
−
25NIETERTGOLDFELDSHAFIEE
and
tr(ΠΣ S′) tr(Π) tr Π(Σ S′ Σ S′(µ S)) + tr Π(Σ S′(µ S) Σ S) + tr Π(Σ S I d)
| − | ≤ − − −
100 16λ2
≤
k(cid:12) (cid:12)µ S(cid:0) −µ S′ k2 2+
1
6(cid:1) ε(cid:12) (cid:12)tr((cid:12) (cid:12)Π)(cid:0) +
1 6ε
+max(cid:1) {(cid:12) (cid:12)tr(Π(cid:12) (cid:12)Σ(cid:0) S),tr(Π)
}
(cid:1)(cid:12) (cid:12)
(cid:18) − − (cid:19)
2
2λ√6ε 100 16λ2
5√ε+ + tr(Π)+ +2tr(Π)+4λ2
≤ 1 6ε 1 6ε 1 6ε
!
− (cid:18) − − (cid:19)
2
2λ√6ε 100 16λ2
5√ε+ + tr(Π)+ +2tr(Π)+4λ2
≤ 1 6ε 1 6ε 1 6ε
!
− (cid:18) − − (cid:19)
152 24λ2
tr(Π)+ (ε 1/12)
≤ 1 6ε 1 6ε ≤
− −
304tr(Π)+48λ2.
≤
Moreover, since T S and T S (1 6ε)S , wehave that T S TV 6ε. Usingthis, that
| | ≤ | | | ∩ | ≥ − | | k − k ≤
tr(Σ σ2I ) = η, and that S (6ε,5√ε,λ), we apply Lemma 27 to T and S with q = k = 1 and
T d +
− ∈ S
ε 6ε E εtoobtain
1
← ≤
µ µ W (T,S)
T S 2 1,1
k − k ≤
21 5√ε+√6εσ 36√ε(λ+√η)
+
≤ 1 6ε (1 6ε)3/2
(cid:0) − (cid:1) −
212√ε+102√ε(λ+√η) (ε 1/12,σ 50)
≤ ≤ ≤
Thus,thetriangle inequality gives
µ T µ S′ 2 µ T µ S 2+ µ S µ S′ 2
k − k ≤ k − k k − k
212√ε+102√ε(λ+√η) + 5√ε+10λ√ε
≤
2 (cid:0)13√ε+112λ√ε+102√ε (cid:1)η
(cid:0) (cid:1)
≤
Combiningtheabove,wehaveforsuchS′ that
g(x) = |S′
|
tr(ΠΣ S′)+ kΠ(µ
T
−µ S′) k2
2
x∈S′
X (cid:0) (cid:1)
S tr(Π)+304tr(Π)+48λ2+ 213√ε+112λ√ε+102√ηε 2
≤ | |
S (cid:16) 305tr(Π)+228ε+214λ2+2(cid:2)15ηε (cid:3) (cid:17)
≤ | |
(cid:0) (cid:1)
and
g(x) = |S′
|
tr(ΠΣ S′)+ kΠ(µ
T
−µ S′) k2
2
x∈S′
X (cid:0) (cid:1)
S′ tr(Π) 304tr(Π) 228ε 214λ2 215ηε
≥ | | − − − −
(1 ε)S 303tr(Π) 228ε 214λ2 215ηε
(cid:0) (cid:1)
≥ − | | − − − −
S 303tr(Π) 228ε 214λ2 215ηε
(cid:0) (cid:1)
≥ | | − − − −
(cid:0) (cid:1)
26ROBUST ESTIMATION UNDERLOCAL AND GLOBALADVERSARIAL CORRUPTIONS
Since T (1 4ε)S 2S /3,combining theabovegives
| |≥ − | | ≥ | |
g(x) g(x) g(x)
≥ −
x∈T\S x∈T x∈S
X X X
= T η+σ2tr(Π) S 305tr(Π)+228ε+214λ2+215ηε
| | −| |
S 2η/3+1300tr(Π) 228ε 214λ2 215ηε (σ 50)
(cid:0) (cid:1) (cid:0) (cid:1)
≥ | | − − − ≥
S (η/4+1300tr(Π)) (ε 2−18,η 230ε+214λ2)
(cid:0) (cid:1)
≥ | | ≤ ≥
Moreover,since L = 6εT 6εS (1 4ε) 4εS T S ,andgtakesitslargestvaluesonpointsof
| | | | ≥ | | − ≥ | | ≥ | \ |
L,wehave
f(x)= g(x) g(x) S (η/4+1300tr(Π)).
≥ ≥ | |
x∈T x∈L x∈T\S
X X X
Finally,plugging inS′ = S andS′ = S Lintotheboundsaboveon g(x),weobtain
\
x∈S′
P
f(x) = g(x)
x∈S∩T x∈S∩L
X X
= g(x) g(x)
−
x∈S x∈S\L
X X
S 609tr(Π)+229ε+215λ2+216ηε
≤ | |
S (609tr(Π)+η/8) (ε 2−20,η 233ε+219λ2)
(cid:0) (cid:1)
≤ | | ≤ ≥
1
f(x),
≤ 2
x∈T
X
asdesired.
Now, by the exact martingale argument used to prove Theorem 2.17 in Diakonikolas andKane (2023),
Theorem26impliesthatW2PROJECT maintainstheinvariant S T (1 4ε)S overalliterationswith
| ∩ |≥ − | |
probability at least 2/3. Since at least one point is removed from T at each iteration, the algorithm must
terminate while satisfying this invariant as well as the (updated) termination condition at Step 4: tr(Σ
T
−
σ2I ) < Cε+Cρ2ε1−2/p. Consequently, thereturnedmeasurePˆ = Unif(T)satisfies
d +
Pˆ P′ TV 4ε
k − k ≤
and
tr(Σ Pˆ −Σ P′ −σ2I d) +
≤
tr(Σ Pˆ −σ2I d) +
≤
Cε+Cρ2ε1−2/p.
Thus,byLemma28andthefactthatP (ε,O(√ε)),wehave
∈ S
W q,k(Q,P) . εq1−1 2√k+ε−[ p1−1 q]+ ρ+εp1−1 2 Cε+2Cρ2ε1− p2
(cid:18) q (cid:19)
.
εq1−1 2√k+ε−[ p1−1 q]+ ρ+εp1
.
ε1 q−1 2√k+ε−[ p1−1 q]+(cid:16)
ρ,
(cid:17)
asdesired.
27NIETERTGOLDFELDSHAFIEE
B.3. ErrorCertificateLemmas
Westate auseful technical lemmaextending the certificate lemma(Lemma2.6) ofDiakonikolas andKane
(2023). Note that here the name is less precise; since P′ is not observed, we cannot certify this condition
fromourobservation unlessweapproximate Σ P′ byI d.
Lemma27 Let λ ,λ ,η 0, ε (0,1), and δ ε. Fix P′ (ε,δ,η) and Q (Rd) such that
1 2
≥ ∈ ≥ ∈ S ∈ P
tr(Σ Q Σ P′ λ 1I d) + λ 2 and Q P′ TV ε. Then
− − ≤ k − k ≤
21ε1 q−1 δ˜√k 36ε1 q− 21 η˜
W (Q,P′) +
q,k ≤ 1 ε (1 ε)3/2
− −
forallk [d],whereδ˜= δ+√λ εandη˜= η+√λ .
1 2
∈
WenowapplythisresultunderSettingB2.
Lemma28 Letλ ,λ 0andC 1. UnderSettingB2withP (Cε,δ),fixanyQ (Rd)suchthat
1 2
≥ ≥ ∈ S ∈ P
tr(Σ Q Σ P′ λ 1I d) + λ 2 and Q P′ TV τ,whereε τ Cε. Then
− − ≤ k − k ≤ ≤ ≤
21τ1 q−1 δ˜√k 37τ−[1/p−1/q]+ρ˜
W (Q,P) +
q,k ≤ 1 −τ (1 −τ)23
forallk [d],whereδ˜= δ+√λ τ andρ˜= ρ+τ1/p−1/2√λ .
1 2
∈
Proof Under Setting B2, we have P′ (Cε,δ,η) (τ,δ,η), where η = ρε1/2−1/p ρτ1/2−1/p.
∈ S ⊆ S ≥
ApplyingLemma27toP′ andQwithTVcorruption levelτ andplugging inourvalueofη gives
21τ1 q−1 δ˜√d 36τ1 q− 21 η 21τq1−1 δ˜√d 36τ1 q− p1 ρ˜
W (Q,P′) + + .
q,k
≤ 1 τ (1 τ)3/2 ≤ 1 τ (1 τ)3/2
− − − −
Moreover, we have W (P,P′) ρε−[1/p−1/q]+ ρ˜τ−[1/p−1/q]+. Noting that τ−[1/p−1/q]+ τ1/q−1/p,
q,k
≤ ≤ ≥
thelemmafollowsbythetriangleinequality.
Wenowreturntotheinitialtechnical lemma.
ProofofLemma27BytheTVbound,wecandecomposeQ = (1 ε)P¯+εRforsomeP¯ 1 P′. Using
− ≤ 1−ε
thisdecomposition, webound
λ 2 tr(Σ Q Σ P′ λ 1I d)
≥ − −
= tr(Σ
P¯
−Σ P′)+εtr(Σ R)+ε(1 −ε) kµ
P¯
−µ
R
k2 2−εtr(Σ P¯) −dλ
1
≥
tr(Σ
P¯
−Σ P′)+εtr(Σ R) −εtr(Σ P¯) −dλ 1.
28ROBUST ESTIMATION UNDERLOCAL AND GLOBALADVERSARIAL CORRUPTIONS
SinceP′ (ε,δ,η), wecanrearrange theaboveandapplyLemma18toobtain
∈ S
λ 1 dλ
2 1
tr(Σ R)
≤ ε
+
ε
tr(Σ P′ −Σ P¯)+tr(Σ P¯)+
ε
λ 1 1 dλ
≤ ε2 + ε tr(Σ P′ −Σ P¯(µ P′))+ εkµ P′ −µ P¯ k2 2 +tr(Σ P¯)+ ε1
λ 1 1 dλ
2 + τ (P′,ε)+ τ(P,ε)2 +tr(Σ )+ 1
≤ ε ε
2
ε
P¯
ε
λ 1 4dδ2 16η2 1 2√εη 2 6dδ2 20η2 dλ
2 1
+ + + δ+ + + +
≤ ε ε (1 ε)ε 1 ε ε 1 ε (1 ε)ε2 1 ε ε
(cid:20) − − (cid:21) (cid:20) − (cid:21) (cid:20) − − (cid:21)
dλ λ 12dδ2 44η2
1 2
+ + +
≤ ε ε (1 ε)ε2 (1 ε)2ε
− −
13dδ˜2 45η˜2
+ .
≤ (1 ε)ε2 (1 ε)2ε
− −
Next, take v to be the unit vector in the direction of µ P′ µ R. Then, applying a similar argument as
−
above,webound
λ 2 v⊤(Σ Q Σ P′ λ 1I d)v
≥ − −
= v⊤(Σ
P¯
−Σ P′)v+2εv⊤Σ Rv+ε(1 −ε) kµ
P¯
−µ
R
k2 2−εv⊤Σ P¯v −λ
1
kv k2
2
≥
v⊤(Σ
P¯
−Σ P′)v+ε(1 −ε) kµ
P¯
−µ
R
k2 2−εv⊤Σ P¯v −λ
1
Rearranging andapplying Lemma18tov⊤P′,weboundε(1 ε) µ µ 2 by
♯ − k P¯ − R k2
λ 2+v⊤(Σ P′ −Σ P¯)v+εv⊤Σ P¯v+λ 1
≤λ 2+v⊤(Σ P′ −Σ P¯(µ P′))v+ kµ P′ −µ P¯ k2 2+εv⊤Σ P¯v+λ 1
4δ2 16η2 2√εη 2 6δ2 20η2
λ + + + δ+ +ε + +λ
≤ 2 (1 ε)ε 1 ε 1 ε (1 ε)ε2 1 ε 1
(cid:18) − − (cid:19) (cid:18) − (cid:19) (cid:18) − − (cid:19)
12δ2 44η2
λ +λ + +
≤ 1 2 (1 ε)ε (1 ε)2
− −
13δ˜2 45η˜2
+
≤ (1 ε)ε (1 ε)2
− −
Wethushave
13δ˜2 45η˜2
µ µ 2 +
k P¯ − R k2 ≤ (1 ε)2ε2 (1 ε)3ε
− −
Combiningwithanapplication ofLemma18toP′,webound
kµ R −µ P′ k2 2 ≤ 2 kµ R −µ P¯ k2 2 +2 kµ P¯ −µ P′ k2 2
26δ˜2 90η˜2 2√εη 2
+ +2 δ+
≤ (1 ε)2ε2 (1 ε)3ε 1 ε
− − (cid:18) − (cid:19)
30δ˜2 98η˜2
+ .
≤ (1 ε)2ε2 (1 ε)3ε
− −
29NIETERTGOLDFELDSHAFIEE
Next,weapplyLemma19toboundW (Q,P′)by
q
7ε1 q−1 δ√d 12ε1 q− 21 η 1
√1 ε + √1 ε +2εq tr(Σ R)+ kµ R −µ P′ k2 2
− − q
7ε1 q−1 δ√d 12ε1 q− 21 η 1 13dδ˜2 45η˜2 30δ˜2 98η˜2
+ +2εq + + +
≤ √1 ε √1 ε s(1 ε)ε2 (1 ε)2ε (1 ε)2ε2 (1 ε)3ε
− − − − − −
7ε1 q−1 δ˜√d 12ε1 q− 21 η˜ 1 43δ˜2 143η˜2
+ +2εq +
≤ √1 ε √1 ε s(1 ε)2ε2 (1 ε)3ε
− − − −
21ε1 q−1 δ˜√d 36ε1 q−1 2η˜
+
≤ 1 ε (1 ε)3/2
− −
as desired. As usual, for k < d, we note that the analysis above applies to all k-dimensional orthogonal
projections oftheinputmeasures,withthesubstitution d k.
←
B.4. ProofofCorollary9
Here, wesimply apply Corollary 2.8ofBoedihardjo (2024). Sincethis empirical convergence guarantee is
in expectation, and we seek a uniform bound over k [d], we must multiply the resulting bound by d to
∈
applyMarkov’sinequality anddeducethedesirederrorboundwithhighconstant probability.
B.5. ProofofProposition11
Wehaveinthiscase thatΣ P′ = Σ P (since ρ = 0). Then, if kΣ Pˆ kop
≤
1+Cδ2/ε, wehavebystability of
P that
tr(Σ Σ (1+(C
1)δ2
)I ) tr(Σ
(1+Cδ2
)I ) 0,
Pˆ − P − − ε d + ≤ Pˆ − ε d + ≤
atwhichpointLemma28withp = q = 1givestheProposition.
30