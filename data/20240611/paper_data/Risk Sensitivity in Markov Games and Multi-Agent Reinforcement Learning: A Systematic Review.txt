PREPRINT 1
Risk Sensitivity in Markov Games and Multi-Agent
Reinforcement Learning: A Systematic Review
Hafez Ghaemi , Shirin Jamshidi §, Mohammad Mashreghi §, Majid Nili Ahmadabadi , and Hamed
Kebriaei , Senior Member, IEEE
Abstract—Markov games (MGs) and multi-agent reinforce- a) Financial markets: The traditional objective of max-
ment learning (MARL) are studied to model decision making imizing the expected return is clearly not sufficient for many
in multi-agent systems. Traditionally, the objective in MG and
investors in financial markets. Most investors do not tolerate
MARLhasbeenrisk-neutral,i.e.,agentsareassumedtooptimize
the possibility of extreme losses and would like to reduce the
a performance metric such as expected return, without taking
into account subjective or cognitive preferences of themselves ”risk” of their investments. Although one can try to model
or of other agents. However, ignoring such preferences leads to financial investment via single-agent risk-sensitive stochastic
inaccurate models of decision making in many real-world sce- optimization and reinforcement learning (RL), the financial
nariosinfinance,operationsresearch,andbehavioraleconomics.
market is inherently an MAS, and a more accurate model
Therefore, when these preferences are present, it is necessary
of decision making for agents in this system would be risk-
to incorporate a suitable measure of risk into the optimization
objective of agents, which opens the door to risk-sensitive MG sensitive stochastic optimization in MGs and learning policies
and MARL. In this paper, we systemically review the literature via risk-sensitive MARL.
on risk sensitivity in MG and MARL that has been growing in
recentyearsalongsideotherareasofreinforcementlearningand
game theory. We define and mathematically describe different b) Autonomous driving: The replacement of traditional
riskmeasuresusedinMGandMARLandindividuallyforeach
cars with autonomous vehicles seems inevitable in the near
measure, discuss articles that incorporate it. Finally, we identify
future. A network of self-driving vehicles can be considered
recent trends in theoretical and applied works in the field and
discuss possible directions of future research. a complex MAS with objectives that can vary from agent to
agent, for example, depending on subjective preferences of
Index Terms—Risk sensitivity, Markov game, multi-agent re-
inforcement learning passengers such as the importance of punctuality, safety, and
choice of itinerary. Again, in this scenario, the existence of
multiple risk-sensitive agents with diverse preferences cannot
I. INTRODUCTION
be modeled without considering interactions between agents,
MARKOV game (MG), also known as stochastic game
and requires a multi-agent risk-sensitive framework.
(SG), is the main theoretical framework for studying
multi-agent systems (MAS). This type of game was initially
Compared to single-agent stochastic optimization and RL,
introduced by Shapley [1] and was later formalized as the
the literature on risk sensitivity in MG and MARL has been
frameworkformulti-agentreinforcementlearning(MARL)by
sparse. However, in recent years, researchers have realized
Littman [2]. Within the classical MG and MARL paradigm,
the importance of incorporating risk into their multi-agent
agents are assumed to pursue a risk-neutral objective, seeking
frameworks in areas such as finance, energy trade, operations
to maximize a notion of expected return and ignore the
research, behavioral economics, and cognitive modeling of
subjective preferences of themselves or other agents within
humandecisionmaking.Althoughthereexistmanyreviewand
the MAS. Risk-neutral analysis of MGs and MARL has
surveyarticlesonMARLandMG[15]–[20],tothebestofour
witnessed substantial progress in the recent years, particularly
knowledge,risksensitivityinmulti-agentdecisionmakinghas
in specific types of MG such as stochastic zero-sum games
not been explored in a review paper before. To fill this gap,
[3]–[9]andstochasticpotentialgames[10]–[14].Nonetheless,
in this work, we present the first review paper on risk
this risk-neutral objective is often inadequate for representing
sensitivity in MG and MARL by identifying the different
agents with distinct subjective preferences and internal cog-
risk measures used in MG and MARL, categorizing them,
nitive biases. To take such preferences into account, agents
and discussing each measure’s related articles in detail. The
incorporateameasureofriskintotheiroptimizationobjective,
remainderofthisreviewisorganizedasfollows;SectionIIfor-
thus transitioning into the domain of risk-sensitive MG. To
mally introduces and lays down the mathematical foundations
further elucidate why incorporating risk in MGs is important,
of risk-sensitive MG and MARL and the different measures
we consider two practical scenarios and highlight the risk-
of risks used in these domains. In Section III, we discuss
sensitive nature of the objective in each.
the methodology of the review. Section IV is dedicated to
Correspondence:ghaemi.hafez@ut.ac.ir. analyzing and discussing the existing literature and selected
HafezGhaemi,ShirinJamshidi,MohammadMashreghi,MajidNiliAhmad- articles. Finally, in Section V we summarize our findings and
abadi, and Hamed Kebriaei are with the School of Electrical and Computer
identify the trends and future directions of research in risk-
Engineering,UniversityofTehran,Iran
§Equalcontribution sensitive MG and MARL.
4202
nuJ
01
]TG.sc[
1v14060.6042:viXraPREPRINT 2
II. PRELIMINARIES from it based on their specific measure of risk. Incorporating
risk into an RL framework can be mathematically interpreted
A Markov game can be formally defined using a multi-
player1 Markov decision process (MDP) 2. as modifying the underlying stochastic optimization. This
modificationisdonebyeitherchangingtheobjectivefunction,
Definition 1 (Markov game). A stochastic (Markov) game3 orbyputtingconstraintsontheoptimizationproblemwiththe
is defined as a multi-player MDP using a tuple of five key same objective function as the risk-neutral setting. Prashanth
elements (N,S,A = {A i|i ∈ N},P,R = {R i|i ∈ N}), and Fu [21] refer the risk measures corresponding to the
where, formerasexplicitrisk,andtheonescorrespondingtothelatter
• N ={1,2,3,...,n} is a set of n players. as implicit risk.
• S is the state space shared by all player. An explicit risk measure alters the objective function in
• A i is the action space of player i. MARL’s stochastic optimization problem. Therefore, agent i
• P :S×A→∆(S) is a transition probability mapping. maximizes a function different from J given in (3), such as
• R i :S×A×S →R is the reward function of player i.
maxG (θ). (4)
The joint policy of agents, probability distributions over θ πi,π−i
actions in each state, is denoted by π = (πi,π−i). The For implicit risk measures, the objective function remains
frameworkforlearningoptimalpoliciesinMGsismulti-agent the same as the risk-neutral one (3), but we will have a risk
reinforcement learning. Learning in MARL can be done in a function G and a risk level κ that would convert the task into
cooperative, competitive, or mixed setting. Denoting the joint a constrained optimization,
action profile at time t by a = (ai,a−i), the optimization
t t t
objective for agent i in the infinite-horizon discounted-reward maxJ (θ) s.t. G(θ)<κ. (5)
πi,π−i
MG is given by θ
Notethatwecanadoptmethodsusedinconstraintoptimiza-
tionsuchasLagrangemultiplierstotransformanimplicitrisk
(cid:26) ∞ (cid:27)
maxJ =maxE (cid:88) γtri(s ,a ) , into an explicit one, which is usually done in practice. In the
πi
πi,π−i
πi
at∼π(.|st),st+1∼p(.|st,at)
t=0
t t
following subsections, we will describe the risk measures in
(1)
both explicit and implicit categories that we have identified in
where γ is a discount factor. In the average-reward setting
the literature of MG and MARL.
the objective is given by
B. Explicit Risk Measures
m πa ixJ πi,π−i =m πa ix Tl →im
∞T1 Eat∼π(.|st),st+1∼p(.|st,at)(cid:26) t(cid:88)∞ =0ri(st,at)(cid:27)
. (2) There are three types of explicit risk measures used in MG
andMARL;exponentialreward,coherentrisk,andcumulative
Assuming that the policy πi is parameterized by variable θ,
prospect theory (CPT).
we can write the objective as 1) Exponential Reward/Cost: Exponential reward (or cost
intheminimizationcase)isthemostfrequentlyusedmeasure
maxJ πi,π−i(θ). (3) in risk-sensitive control, finance, and operations research. The
θ
classical formulation of exponential reward in risk-sensitive
In general, in competitive settings, selfish agents reach a
MDP was given by Howard and Matheson [22]. We can
Markov perfect equilibrium (MPE) policy, also known as a
consider exponential reward in both discounted-reward and
Nash equilibrium (NE) of the MG, from which no agent can
average-reward settings. The exponential reward objective in
unilaterally deviate to increase its expected return without
a discounted-reward MDP can be written as
decreasing another agent’s return. In the cooperative setting,
agents would like to converge to a Pareto optimal policy in maxJ =E [eβiRi], (6)
which there exists no alternative policy where at least one πi
πi,π−i π
player’s expected return is higher. where,
T−1
(cid:88)
R = γ r , (7)
i t t,i
A. Risk Measures in MG and MARL
t=0
Risk-sensitive agents in stochastic optimization and RL, where R is the cumulative reward of agent i over an episode
i
whether in the single-agent or multi-agent settings do not and β is the hyperparameter that controls risk sensitivity; β <
merely maximize an expected return objective, and deviate 0 corresponds to the risk-averse setting, β > 0 to the risk-
seeking settings and the limit β →0 reduces the objective to
1Weusethetermsplayerandagentinterchangeablyinthispaper.Ingeneral,
the risk-neutral case.
playerismorecommoninthegametheoryliteraturewhileagentisoftenused
inRLandMARLliterature. In an average-reward MDP, the risk-sensitive objective can
2Here, we define a discrete-time MDP. Similarly, continuous-time MDP be written as
andcontinuous-timeMGcanbedefined.Someofthepapersanalyzedinthis
reviewworkwithcontinuous-timeMGs.
(cid:26) ∞ (cid:27)
3It is important to distinguish between Markov games where agents are 1 1 (cid:88)
maxG =maxlimsup logE exp(β ri(s ,a )) ,
a ws hs eu rm ee ad geto ntsma tak ke eth the eir ird ae cc ti is oio nn ss ses qim ueu nlt ta ian le lyou as fly teran od bse ex rvte in ns giv te h- efo arm ctiog na sme os f πi πi,π−i πi T→∞ T β t=0 t t
previousplayers. (8)PREPRINT 3
where β is again the risk sensitivity coefficient. where we denote (.) =max(0,.) and (.) =−min(0,.),
+ +
2) Coherent Risk: First introduced by Artzner et al. [23], a and x serves as a reference point that separates gains and
0
coherent risk measure ρ (equivalent to G in (4)) is an explicit losses. Without loss of generality, we can assume x = 0.
0
measureofriskthatpossessesfourmathematicalproperties(X Conventional representations of CPT weighting functions are
and Y are random variables (r.v.s) that can represent returns ω+(p)= pγ and ω−(p)= pδ [26].
(pγ+(1−p)γ)(1/γ) (pδ+(1−p)δ)(1/δ)
of two different policies): Note that by setting δ and γ equal to 1, the definition of
• Monotonicity: If X ≤ Y (almost surely), then ρ(X) ≤ expectedutility EP[u(X)] isrecovered whichshowsthat CPT
ρ(Y). is a generalization of expected utility theory. Furthermore,
• Sub-additivity: ρ(X+Y)≤ρ(X)+ρ(Y). u+ and u− are usually concave functions (−u− is convex)
• Positive homogeneity: ρ(λX)=λρ(X) for any λ≥0. to reflect the higher sensitivity of humans towards losses
• Translationinvariance:Forconstantα≥0,ρ(X+α)= compared to gains, and the principle of diminishing marginal
ρ(X)+α. utility. Therefore, the utility function can have analytical
representationsu+(x)=xα ifx≥0,andu−(x)=λ(−x)β if
In a risk-sensitive MDP, the r.v. X can represent the cumu-
x < 0. The parameters γ,δ,α,β, and λ are subjective model
lative discounted or average reward/cost. The above proper-
parameters that can differ from person to person based on
ties also have investment-related interpretations. For instance,
their level of risk aversion and individual characteristics. The
sub-additivity is necessary in portfolio optimization implying
conventionalrepresentationsofweightingandutilityfunctions
that diversification is intended to mitigate the possibility of
given a set of subjective parameters are plotted in Figure 1.
increased risk (in a cost minimization scenario). Another
It is noteworthy that CPT is a generalization of coherent
important property of this family of risk measures is that
measuresofriskasitonlypossessesmonotonicityandpositive
any coherent risk measure has a dual representation as the
homogeneity but it is neither translation-invariant nor convex.
supremum of an expected value over a risk envelope [24];
Indeed, by choosing appropriate CPT weighting functions,
i.e.,
one can derive different coherent risk measure formulations
[27]. It is also noteworthy that since CPT can be seen as a
ρ(X)= sup E(XQ). (9)
weighted risk measure with a utility function and distorted
Q∈Q
probabilities, its different variants that are similarly defined
It follows that the risk envelope can be explicitly written as using the aforementioned Choquet integral, have been called
distortion risk measure (DRM) [28]–[30]. Due to their similar
mathematical definition, we do not distinguish between CPT
Q=(cid:8) Q∈P :E(XQ)≤ρ(X) for all X ∈L2(cid:9) , (10)
and DRM in this paper.
where L2 denotes X satisfying E(|X|2) < ∞. This ”dual
1.0
representation” is useful when one wants to prove a risk y=p
measure is coherent. w(p) y=x
0.8 u(x) 5
3) Cumulative Prospect Theory: Cumulative prospect the- 4
ory (CPT) [25], [26] is a risk measure that models human 0.6 3
2
attitudestowardriskwhenmakingdecisionsunderuncertainty.
1
It considers weighting functions that are applied to cumu- 0.4
5 4 3 2 1 1 2 3 4 5
lative probabilities of outcomes to account for the fact that 1
0.2 2
humansoverestimate/underestimateoutcomeswithsmall/large
3
probabilities. It also applies a usually convex-concave utility 0.0 4
functiontorewardstomodelhumans’lossaversionanddimin- 0.0 0.2 0.4 0.6 0.8 1.0 5
Probability (p)
ishing marginal utility. When considering CPT as an explicit
risk measure, the expectation operator in (4) is replaced by Fig. 1. (Left) Conventional CPT weighting functions; ω+(p) =
the non-linear CPT operator. We formally define the CPT pγ and ω−(p) = pδ with γ = δ = 0.69.
(pγ+(1−p)γ)(1/γ) (pδ+(1−p)δ)(1/δ)
operator and the corresponding risk-sensitive objective in MG (Right) Conventional CPT utility functions; u+(x) = xα for x ≥ 0, and
−u−(x)=−λ(−x)β)forx<0,withα=β=0.65andλ=2.6.
and MARL below.
Given a real-valued r.v. X with distribution P(X), a ref-
erence point x 0, two monotonically non-decreasing weighting C. Implicit Risk Measures
functions, ω+ : [0,1] → [0,1],ω− : [0,1] → [0,1], utility
There are three types of implicit risk measures used in MG
functions u+ : R+ → R+,u− : R− → R+, and appropriate
and MARL; variance, conditional value-at-risk, and chance
integrability assumptions, we can define the CPT value using
constraint.
Choquet integrals as
1) Variance: Whenusingvarianceasariskmeasure,weare
interested in imposing a constraint on the variance of returns.
(cid:90) ∞ IndiscountedMDPs,varianceasariskwasfirstintroducedby
CPT P[X]:= ω+(P(u+((X−x 0) +)>x))dx−
Sobel [31]. In this setting, we would like to upper-bound the
0 (11)
(cid:90) ∞ overall variance of cumulative discounted return. Therefore,
ω−(P(u−((X−x ) )>x))dx.,
0 − the risk constraint G in (5) will be
0PREPRINT 4
G (s )=Var[D (s )]=U (s )−J (s )2, CVaR α(X)= sup E(XQ) (19)
πi,π−i 0 πi,π−i 0 πi,π−i 0 πi,π−i 0
(12)
Q∈Qα
where, with
(cid:26) (cid:27)
J πi,π−i(s)=E[D πi,π−i(s)] and U πi,π−i(s)=E[D πi,π−i(s)2], Q := Q∈L2 :E(Q)=1,0≤Q≤ 1 . (20)
(13) α 1−α
∞ Consequently, CVaR can also be considered an explicit
(cid:88)
D i(s 0)= γtri(s t,a t), (14) measure of risk as it is coherent.
t=0 3) Chance Constraint: Chance constraints in stochastic
Where D i represents the total discounted reward for a optimizationwasfirstusedbyCharnesetal.[35].Whenusing
trajectory in an infinite-horizon MDP. a chance constraint, we would like the probability that our
On the other hand, in average-reward MDPs, Filar et al. incurred loss is larger than a specific amount to be smaller
[32] introduced the per-period variance defined by deviations than a given threshold, i.e.,
of single-stage reward from the average reward, in contrast to
thevarianceofaveragerewarditself.Therefore,theconstraint P(g(X)≥0)≤α, (21)
in (5) will be
whereg isalossfunctiondefinedwithrelationtoreturn,X
isanr.v,andαisasmallprobabilityquantile.Wecantherefore
(cid:26) T−1 (cid:27)
G = lim 1 E (cid:88) (ri(s ,a )−J )2 . (15) write the constraint in the form of (5) as G≜P(g(X)≥0).
πi,π−i T→∞T t t πi,π−i
t=0
An example can show the rationale behind the choice of III. REVIEWMETHODOLOGY
thisconstraintinsteadofthevarianceofaveragerewarditself. Inthissection,weoutlinethemethodologyusedtoconduct
Consider two sequences of rewards; (20,−20,20,−20,...) this review. Following the PRISMA guidelines for systematic
resulting from policy π 1 and (0,0,0,0,...) resulting from π 2. reviews [36], we conducted a keyword search to first extract
The average reward and its variance are zero for both π 1 and all relevant articles. The final search was performed on May
π 2, however, it is evident that in terms of the constraint (15), 24,2024toretrievetheup-to-datecorpusofarticlesonScopus
policy π 2 is better than π 1. database. We used the following query among title, abstract,
2) Conditional Value-at-Risk: The Value-at-risk (VaR) is a and keywords of documents:
risk measure widely employed in financial applications. For (”Multi-Agent Reinforcement Learning” OR ”Markov
an r.v. X, VaR at a given level α∈(0,1) is defined as game” OR ”Markov game” OR ”Dynamic Games”) AND
(”Risk Sensitiv*” OR ”Risk Measure*” OR ”Cumula-
VaR α(X)≜inf{z|P(X ⩽z)⩾α}, (16) tive Prospect Theory” OR ”Conditional Value-at-Risk” OR
”Chance Constraint” OR ”Exponential Reward” OR ”Expo-
where α∈[0,1) is an α-quantile, and F is the cumulative
nential Cost”).
distribution function (c.d.f) of X. Therefore, the definition of
This query without any additional filters gave 129 doc-
VaR is equivalent to
uments, among which there were 83 journal articles, 41
VaR (X)≜inf{z|F (z)⩾α}=F−1(α). (17) conference papers, 3 book chapters, 1 book, and no review
α X X
article. All documents were in English, except for one that
ItcanbeseenthatVaRquantifiesthelevelofassetsrequired
wasinChinese.WelimitedthearticlestotheEnglishlanguage
to cover a potential loss. The main drawback of VaR is that it
and selected only journal articles and conference papers. The
isnotacoherentriskmeasure(inordertocheckifanimplicit
remaining number of documents was 124 after this stage.
risk measure is coherent, we can use the dual representation
We further performed a manual search on Google Scholar
theorem discussed in Section II-B2). The mathematical prop-
and IEEE Xplore and found two additional recently published
erties of coherent risks make them compatible with standard
papers that have not been indexed by Scopus yet, resulting in
stochasticoptimizationmethods.Inordertotakeadvantageof
a total of 126 articles. It is noteworthy that neither using the
such properties, we can modify VaR to derive another risk
querysearchnortheadditionalmanualsearch,wedidnotfind
measure which is coherent, known as conditional value-at-
a relevant review article, establishing the present work as the
risk(CVaR),firstintroducedbyRockafellarandUryasev[33].
first review on risk-sensitive MG and MARL. Interestingly,
Considering the tail distribution described by VaR, CVaR is
even the area of single-agent risk-sensitive RL was lacking a
the conditional mean over it and is defined as
comprehensive survey until very recently [21].
Wedidnotperformanyadditionalpre-screeningonthe126
CVaR (X)≜E[X |X ≥VaR (X)]. (18)
α α documents and assessed all articles in full. During the full-
Therefore, the corresponding risk constraint in (4) is X ≥ text assessment, we excluded any article that did not meet the
VaR (X). It can be shown that CVaR has the dual rep- following criteria:
α
resentation of coherent risk measures (see [34] for detailed • The underlying framework to be analyzed must be a
derivations), i.e., stochastic MAS involving at least two or more agents.PREPRINT 5
The MAS should be represented either explicitly or demonstrate the presence of a bounded solution to the dy-
implicitly as a multi-agent MDP, where each agent opti- namic programming value function equations by showing that
mizes an objective function in a cooperative, competitive they satisfy an Isaacs equation. Consequently, they prove the
or mixed manner. existence of optimal stationary policies for each player.
• The objective must be risk-sensitive, i.e., at least one of Considering continuous-time MGs with denumerable state
the agents must deviate from the risk-neutral objective spaces and Borel action spaces, Golui and Pal [50] analyze
and incorporate a risk measure, i.e., the ones described zero-sumtwo-playerMGswithafinitetimehorizon.Through
in Section II-A. a non-homogeneous game model, they establish the existence
After the full-text assessment, we excluded 67 documents of the game value and saddle-point equilibrium with history-
thatdidnotmeettheaboveeligibilitycriteriaforrisk-sensitive dependentpolicies.Wei[51]studiesmulti-playernonzero-sum
MG and MARL. Many of the excluded articles studied the MGs with finite horizons and unbounded cost and transition
relationshipbetweenzero-sumMarkovgamesandsingle-agent rates. The paper proves the existence of a randomized MPE
risk-sensitive control [37]–[45]. For this reason, the search with history-dependent policies. Lastly, Ghosh et al. [52]
queryusedabovecouldnotexcludethemautomatically,anda examinetwo-playernonzero-sumMGswithinfinitetimehori-
full-text assessment was necessary to detect these studies and zonsandunboundedcostandtransitionrates.AssumingaLya-
filter them. Finally, 59 remaining documents are included in punov stability assumption and using the principal eigenvalue
ourfinalanalysis;wecategorizethesearticlesbasedontherisk approach, they show that Hamilton–Jacobi–Bellman (HJB)
measure used and discuss them individually in the following equations admit a solution which implies the existence of NE
section. and corresponding stationary Nash policies.
Shifting attention to MGs with continuous state and action
spaces, Caravani and Papavassilopoulos [53] investigate non-
IV. RISKSENSITIVITYINMGANDMARL
cooperative linear-quadratic stochastic dynamic games with
In this section, we discuss the 59 studies selected using conflicting risk aversions and incomplete information and
our selection procedure laid out in Section III. We dedicate a establish conditions for the existence of a risk-sensitive NE.
subsectiontoeachriskmeasureanddiscussthepapersrelated Wang et al. [54] consider stochastic dynamic games and
to each measure by considering the type of MG, the type of proposeaniterativealgorithmthatfindsthefeedbackNEofthe
state and action spaces, the goal and result of the study, and game by a linearized approximation of the system dynamics
the algorithms used. and a quadratic approximation of the cost function. They
experimentally show that the risk-sensitive Nash equilibria
result in safer driving behavior of agents in multiple driving
A. Explicit Risk Measures
scenarios. Lastly, Krajewski [55] delves in competitive linear
1) ExponentialReward/Cost: Theexponentialrewardisthe exponential-quadratic stochastic dynamic games in one-step
most frequently used risk measure in risk-sensitive MG and delay information-sharing patterns. The article shows that a
MARL. We separately consider the average-reward and the unique NE exists in such a setting and can be computed
discounted-reward settings for exponential risk in this section. recursively.
1.1) Average Exponential Reward/Cost: There are four Two studies analyze inter-generational (bequest) MGs in-
studies that work with denumerable state spaces and Borel volving multiple players, in which the utility of each gen-
action spaces in discrete-time MGs and average-reward expo- eration (time step) depends on actions of more than one
nential risk [46]–[49]. Ghosh et al. [46] work in two-player descendant player [56], [57]. Jas´kiewicz and Nowak [56]
zero-sum MGs and consider unbounded payoff functions. consider Borel state and action spaces. They combine risk-
Via Lyapunov stability assumptions, the authors show the sensitive control theory with overlapping generations models
existence of the value of the game and a saddle point equilib- to prove the existence of a stationary MPE in these games
rium and further characterize the corresponding equilibrium’s using the Dvoretzky, Wald, and Wolfowitz theorem. Balbus
stationary Markov policies analytically. Wei and Chen [47] and Jas´kiewicz [57] consider continuous state and action
focus on nonzero-sum two-player MGs. They first establish a spaces, as well as weakly continuous transition probabilities
Feynman–Kac formula for unbounded payoff functions under thatincludebothnon-atomicanddeterministicscenarios.They
randomizedhistory-dependentstrategies,andafterwardsprove prove the existence of a stationary MPE using Fatou’s lemma
the existence of solutions to optimality equations for both and Skorohod’s representation theorem.
players and establish the presence of randomized stationary Klompstra [58], [59] analyzes continous-time linear-
NE under specific conditions. In another work, Wei and Chen quadratic nonzero-sum stochastic dynamic games with two
[48] consider nonzero-sum coordination MGs with multiple players and continuous state and action spaces. These
players.Theyshowtheexistenceofsolutionstocoupledplayer studies analyze both linear-exponential-Gaussian and linear-
optimality equations and confirm the presence of stationary exponential-quadratic-Gaussian control problems to establish
NE under suitable conditions through a discounted approxi- NEsolutionsbothundercompletestateobservationandshared
mation method. They also show that these results hold even partial observation.
with negative risk sensitivity coefficients. Lastly, Herna`ndez- Xu and Wu [60] explore stochastic dynamic games with
Herna`ndez and Marcus [49] consider two-player competitive continuous state and action spaces and a large population of
MGs with infinite horizons and bounded cost functions. They minor agent and a major agent that has a significant influencePREPRINT 6
ontheminoragentinalinear-quadratic-Gaussiansystem.The These works are a generalization of Pal and Pradhan [67],
authors establish a decentralized ϵ-Nash equilibrium using the wherethetransitionandcostratesareassumedtobebounded.
Nash certainty equivalence methodology and limiting control Considering discrete-time zero-sum MGs, Guo et al. [68]
problem. study two-player infinite-horizon case with Borel state and
Naderi Soorki et al. [61] model joint beam forming and actionspaces,unboundedpayoffs,andaninterestingcondition
phase shift control of millimeter wave communications via an where the discount factor varies as a function of the state.
identical-payoffpartiallyobservableMGwithcontinuousstate Considering Borel state and action spaces, they first find new
and action spaces. They design a recurrent neural network for boundariesfortheShapelyequation(SE)undertheunbounded
eachagenttofindrisk-sensitiveoptimalpoliciesviadistributed payoffs condition, and afterwards, prove the existence of the
policy gradient (PG) search in the policy space and prove the solution to the SE and the existence of NE and value of the
convergence of their PG algorithm to the NE of the game. game. They also provide an iterative algorithm to find the NE
Bas¸ar [62] studies different scenarios of risk-sensitive con- of the game.
trol within continuous-time MGs with continuous state and There are also studies that have considered competitive
actionspaces.Afterdiscussingtheequivalencebetweensingle- MGs that are not zero-sum. Asienkiewicz and Balbus [69]
agent risk-sensitive control and filter design with two-player focusontwo-playerresourceextractiongameswithcontinuous
zero-sum stochastic differential games, the paper also dis- state and action spaces, where agents are assumed to have
cusses equivalences between risk-sensitive two-player zero- identicalpreferences.Theyprovetheexistenceofasymmetric
sumstochasticdifferentialgamesandrisk-neutralthree-player stationary MPE given felicity utility functions for agents in
zero-sum stochastic differential games. The paper also dis- the infinite-horizon setting. Monahan and Sobel [70] consider
cusses robustness issues in risk-sensitive multi-player mean- a competitive setting between advertising firms modeled as a
fieldstochasticdifferentialgameswithfiniteorinfinitenumber risk-sensitive stochastic dynamic game with continuous state
of players. and action spaces. The authors introduce the notion of a
Bhabak and Saha [63] consider zero and nonzero-sum two- firm’s ”goodwill”, i.e., the cumulative measure of the firm
player semi-Markov non-homogeneous games with discrete- and its competitors’ advertising expenditure and show that in
finite state and Borel action spaces, allowing for unbounded a symmetric stochastic dynamic game with discounted-cost
transition and cost rates under history-dependent strategies. exponential risk, the equilibrium goodwill levels are inversely
They show the existence of the game value and saddle-point proportional to risk sensitivity, which has the consequence
equilibrium. Via the Feynman-Kac formula, they also prove of lower initial advertising expenditure when risk sensitivity
this equilibrium is unique. is high. Lastly, Wei and Chen [71] consider multi-player
Ghosh et al. [64] analyze two-player continuous-time zero- nonzero-sum MGs with countable state spaces and Borel
sum MGs with countable state and Borel action spaces. They action spaces. They prove the existence of the optimal value
provetheexistenceofthegamevalueandsaddle-pointequilib- functions for each player and a randomized MPE.
rium under the Lyapunov stability conditions by establishing Regardingcoordination-basedMARLinMGs,Nooraniand
the existence of a principal eigenpair for the MG’s Hamilton- Baras[72]modelarepeatedtwo-playercoordinationstag-hunt
Jacobi-Isaacs (HJI) equations. game as an MG with discrete state and action spaces. They
Lastly, Baier et al. [65] study exponential risk for the first used REINFORCE PG algorithm [73] and experimentally
timeinturn-basedminimax-styleMGswiththetotal(average) showed that using a discounted exponential risk criterion
reward objective and continuous state and action spaces. The would encourage coordination between agents and leads them
authors show that these games are determined and admit opti- to a Pareto optimal policy compared to a sub-optimal Nash
mal memoryless deterministic strategies, in contrast to games equilibrium.
with other risk measures (such as variance or CVaR) that 1.3)AverageandDiscountedExponentialReward/Cost:The
require randomization and memory. They provide results on articles in this subsection study both discounted and average
thedecidabilityandcomputationalcomplexityofthethreshold exponential risk.
problemofwhethertheoptimalexponentialriskvalueexceeds Two studies [74], [75] focus on two-player games with
a given threshold, showing that it is decidable subject to countablestateandcontinuousactionspaces.BasuandGhosh
Shanuel’s conjecture in the general case, but more tractable [74] analyze nonzero-sum MGs with infinite horizons and
for rational inputs or small algebraic instances. The paper provetheexistenceofrisk-sensitiveNEpoliciesfordiscounted
also provides an approximation algorithm for the optimal cost under fairly general conditions, and for the average cost
exponential risk value. under a geometric ergodicity and smallness of payoff condi-
1.2) Discounted Exponential Reward/Cost: We start this tions on the MDP. On the other hand, Basu and Ghosh [75]
subsection by studies on competitive zero-sum MGs. Three analyze zero-sum MGs with infinite horizon and countable
studies [50], [66], [67] focus on zero-sum continuous-time state and continuous action spaces. They prove the existence
MGs with two players. Golui et al. [66] and Golui and Pal of game values and saddle-point NE of the game. They also
[50] consider discrete state spaces and Borel action spaces provide upper and lower bounds for game values in both the
andallowforunboundedtransitionandcostrates.Theyprove discounted and average-cost cases. In the average-cost case,
the existence of the game value and saddle-point equilib- a uniform ergodicity condition is required for the results to
rium in the class of history-dependent strategies by imposing hold. Finally, they show the relationship between the game
a Foster-Lyapunov condition and leveraging HJI equations. value and the level of risk sensitivity in the average-costPREPRINT 7
case; they show that the game value is the product of the objectivefunctionconsistsofadeterministicrisk-neutralfirst-
inverse of the risk sensitivity factor and the logarithm of stage component and a second-stage coherent risk-sensitive
the common Perron–Frobenius eigenvalue of the associated component.Theyconsiderlinear-quadraticrecoursefunctions,
controlled nonlinear kernels. and show that when using such functions, the agents’ objec-
Ba¨uerle and Rieder [76] study risk-sensitive zero-sum MGs tives are convex with respect to their decision variables if the
in both discounted and average-reward cases with Borel state first-stage components are convex. They propose smoothing
and action spaces and bounded rewards, with both finite and and regularization techniques to obtain differentiable approxi-
infinite time horizons. For the discounted-reward case, they mations of the total objective function and overcome the non-
prove the existence of the game value, which solves the differentiability of the recourse functions. They also prove
Shapley equation, and the existence of the optimal possibly convergence of their best-response schemes to the NE of the
non-stationary strategies under continuity and compactness MG.
conditions. In the average-reward scenario, they show that Finally, Huang et al. [82] consider a non-cooperative MG
game’s value solves the Poisson’s equation under a local with finite-discrete state and action spaces and coherent risk.
minorization property and a Lyapunov condition. They also They propose a risk-aware centralized Nash Q-learning that
answer an open question posed by Basu and Ghosh [75] and uses stochastic approximation for saddle-point problems to
provetheexistenceofoptimalrisk-sensitivepoliciesforagents approximate the risk function. Under mild conditions, the
in the average-reward case. authors prove the convergence of their algorithm to a risk-
Ghosh et al. [77] examine continuous-time MGs with a sensitive MPE of the MG.
denumerable state and continuous action space, and prove the 3) Cumulative Prospect Theory: Tian et al. [83] use the
existenceofthevalueandsaddle-pointequilibriaundergeneral nested CPT formulation in MDPs [84] and bounded-rational
conditions in the discounted-reward case, and in the average- agents with quantal level-k policies [85] and developed
reward case under certain Lyapunov conditions using the HJI a framework called bounded risk-sensitive Markov game
equations. (BRSMG). Considering only finite-discrete state and action
Cavazos-Cadena and Herna´ndez-Herna´ndez [78] consider spaces and deterministic policies, they proposed a model-
a discrete-time MG with discrete state and compact met- based value iteration algorithm to find optimal CPT-sensitive
ric action spaces. Under standard continuity-compactness as- policies. The authors also provide an inverse reward learning
sumptions and when the state space is communicating under algorithm to find the agents’ reward functions, intelligence
deterministic stationary policies and a minorization property level, and CPT parameters in a two-player model-based sce-
holds at some states, they show that as the discount factor nario.
α increases to 1, an appropriate normalization of the risk- Xiao et al. [86] apply a non-nested version of the dynamic
sensitive discounted value function converges to the risk- CPT risk [27] in an MG with finite-discrete state and action
sensitive average value function, characterized by the solution spaces to study the interactions between a cyber system and
to the Shapley equation. an advanced persistent threat (APT) attacker when they make
Lastly,Tembine[79]introducesamean-fieldMGinvolving subjective decisions to choose their scan and attack intervals,
multiple players, considering both discounted and average respectively (actions in the risk-sensitive MG). The utility in
exponential risk with a finite-discrete state space and a one- this setting is a function of the data protection level and the
dimensional compact action space. They derive backward- scan interval. They first consider a single-state risk-sensitive
forward mean-field equations in the total payoff case. They game where the whole duration of the attack is a single-state
also show that optimal policies for players may not exist in game. Afterwards, they consider an MG where the dynamics
the average-reward case if the underlying MDP does not have of the APT model is not known by the agents and the current
a unique positive recurrent class. stateisconsideredasadiscretizedsafedurationoftheprevious
2) Coherent risk measures: Fre´de´ric Bonnans et al. [80] time interval in the attack. The authors compare normal Q-
investigate a discrete-time mean-field MG with coherent risk learning and a modified policy hill-climbing (PHC) detection
and continuous state and action spaces, where each agent scheme,inwhichtheyutilizeahotbootingtechniquethatuses
controls a linear dynamical system. The model takes the form experiencesfromsimilarscenariostoinitializetheQfunctions
of a coupled system of dynamic programming equations and andacceleratelearningspeedofthePHC-basedalgorithm.The
a Kolmogorov equation. The authors prove the existence of empiricalresultsshowthatthismodifiedPHC-basedalgorithm
a solution to the coupled system obtained using Schauder’s outperformsQ-learningintermsoftheaverageCPTutilityand
fixed point theorem. Finally, using the corresponding optimal data protection level.
feedback control from the mean-field game solution, they Shen et al. [87] consider the non-nested version of the dy-
construct an approximate NE for the N-player MG with a namicCPTrisk(whichtheyrefertoasdistortionriskmeasure
quantified approximation error. also encompassing VaR and CVaR) in a cooperative partially
Pang et al. [81] study a non-cooperative MG with multiple observable MG with finite-discrete state and action spaces.
players,whereeachagentsolvesarival-parameterizedstochas- The authors introduced Risk-sensitive Individual-Global-Max
ticprogramwithquadraticrecourseinatwo-stagesettingwith (RIGM) principle for cooperative risk-sensitive MARL which
continuous state and action spaces. In the first stage, agents ensuresthattheoptimaljointrisk-sensitiveactionisequivalent
take deterministic actions, and after the realized uncertainty, to the collection of each agent’s greedy risk-sensitive actions.
in the second stage they take recourse decisions. Each agent’s They then propose RiskQ, a decentralized algorithm based onPREPRINT 8
implicit qunatile network (IQN) [88] that models the joint photovoltaic prosumers using dynamic pricing. They utilize
return distribution of agents by combining per-agent return a relaxation method to find the sample weighted average
distribution utilities using an attention-based mechanism to approximation CVaR-sensitive NE of the game, where the
satisfytheRIGMprinciple.Theyalsoconductexperimentson weights correspond to different energy scenarios.
multiplecooperativeMARLenvironmentsshowingthatRiskQ Li et al. [99] propose a competitive CVaR-sensitive MG
outperforms existing methods in several scenarios. model with continuous state and action spaces to optimize
Finally, Ghaemi et al. [89] consider network aggregative plug-in electric vehicle (PEV) charging strategies within a
Markovgames(NAMG),aclassofMGswithlocalcommuni- smart grid. The CVaR risk is applied to the cost function to
cationbetweenagentsandrewardfunctionsthataredependent reduce the overload of the electric transformer. They prove
on neighboring agents [90]–[92]. The authors propose an the existence of a sampling average approximation (SAA)
actor-criticstylealgorithmfordistributedrisk-sensitiveMARL generalized NE for this game model and propose an iterative
with a nested CPT risk criterion in NAMGs with finite- algorithm to find this NE using the Nikaido-Isoda relaxation
discrete state and action spaces. Under a set of assumptions, method,inwhichthebestresponseofeachagentiscalculated
they prove the convergence of their algorithm to a risk- using a distributed alternating direction method of multipliers
sensitive and subjective (from the agent’s prespective) MPE. (ADMM) algorithm [100]. They also provide numerical sim-
Experimentally, they show that higher loss aversion in CPT- ulations using real-word data from a PEV taxi network.
sensitive agents leads to a higher tendency for social isolation Lietal.[101]proposeaCVaR-sensitivetwo-stageMGwith
in the MG. continuousstateandactionspacestoquantifytheoverbidding
risk for multi-energy microgrids, considering the risk from
uncertain energy supply and demand. They use the Cournot
B. Implicit Risk Measures
Nash pricing mechanism to characterize the relationship be-
1) Variance: We identified two studies using Variance as a tween price dynamics and energy supply, and employ the
risk, both of which operate in the discounted-reward setting. SAA technique to approximate the risk-sensitive NE of the
Reddy et al. [93] consider variance of return (VOR) as the game. In doing so, they propose a distributed NE-seeking
risk measure in a multi-player mixed cooperative/competitive algorithm based on Nikaido-Isoda function and ADMM. The
MARL scenario with continuous state and action spaces. proposed method is validated through numerical simulations
They incorporate VOR as a constraint into the agent’s policy using real-world data from Australian energy market which
optimization,transformedintoanunconstrainedproblemusing shows that their algorithm can effectively reduce the risk of
Lagrangian relaxation. The authors propose a multi-timescale notmeetingthedemandandimprovetheeconomicbenefitsfor
actor-critic algorithm called RC-MADDPG for learning risk- eachmicrogrid,comparedtostrategiesthatignoreuncertainty.
constrained policies in MGs. The algorithm uses a centralized Also, it is shown that a higher risk aversion weight reduces
training, decentralized execution framework. Experiments on the energy bidding quantity, especially for renewable-based
the Keep Away task [94] demonstrate that the proposed microgrids with high uncertainty.
method learns risk-averse policies that satisfy the VOR con- Lietal.[102]studytheproblemofenergyresourcetrading
straint while achieving rewards comparable to risk-neutral in an Energy Internet (EI) context. Similar to Li et al. [101],
policies. theyconsideraCVaR-sensitivetwo-stageMGwithcontinuous
Parilina and Akimochkin [95] construct a risk-sensitive state and action spaces, which includes a forward market
cooperative discrete-time MG framework with discrete state and a spot market, to quantify the overbidding risk and
andactionspaces.Theframeworkincorporatesmean-variance uncertaintiesinenergysupplyanddemand.TheCournot-based
preferences as risk into the discounted-reward optimization pricing mechanism is used to determine prices based on total
problem.Theauthorsuseamax-minoptimizationapproachto expected/actualenergybids,whileconsideringuncertaintiesin
define the characteristic functions of sub-games. They derive renewablegenerationanddemandresponseinthespotmarket.
the core of the cooperative MG by applying the method The authors use an SAA technique to approximate the NE
proposed by Sujis et al. [96] to a mean-variance risk-sensitive of the game and provide an NE-seeking algorithm based on
setting. the Nikaido-Isoda function and best response dynamics. They
2) Conditional Value-at-Risk: After exponential provetheexistenceoftheapproximatedSAANEundercertain
reward/cost, CVaR is the second most frequently used conditions. Finally, numerical simulations illustrate that the
risk measure in MG and MARL. While the majority of works optimal risk-sensitive converged policies reduce overbidding
with exponential reward were theoretical, in case of CVaR, riskcomparedtodeterministicmodelsthatneglectuncertainty.
they are mostly applied. Also, a higher risk price leads to more conservative bidding
He et al. [97] develop a CVaR-sensitive Cournot MG as for microgrids with high uncertainty, e.g., wind, and more
an energy bidding framework for multiple wind farms with aggressive bidding for microgrids with low uncertainty, e.g.,
continuous state and action spaces. They propose an iterative demand response. Furthermore, with decreasing deviation in
algorithm to find the optimal risk-sensitive bid for each wind thespotmarketcapacities,microgridstendtobidmoreenergy
farm and validate its feasibility by numerical case studies. to gain more benefits, coinciding with results from determin-
Cui et al. [98] introduce a risk-sensitive CVaR energy istic models.
sharing model for community photovoltaic systems as a com- Heidari et al. [103] explore how multi-energy carriers
petitive MG with continuous state and action spaces among function as energy hubs within combined natural gas andPREPRINT 9
electric power in competitive markets. They try to maximize supporting multi-access edge computing (MEC) networks.
profit in a joint-energy market using bi-level programming They introduce a risk-aware energy optimization model for
for strategic bidding and market clearing in both risk-neutral MEC networks as an MG between energy suppliers and
and risk-sensitive (CVaR) settings. The energy market MG consumers in the network with continuous state and action
model has a discrete state space and continuous action space spaces.Theauthorsutilizeasynchronousadvantageactor-critic
and incorporates renewable energy uncertainty. Given the (A3C) [109] to find the CVaR-sensitive NE of the game.
strict concavity of the payoff functions, the authors show Qiu et al. [110] propose RMIX, a cooperative value-based
the existence and uniqueness of NE and find it using a risk-sensitive algorithm for distributional MARL with contin-
bi-level programming method. They show higher economic uous state and action spaces. In distributional RL, instead of
vulnerability of the system when the market operators are learning the expected return, i.e., the mean of the return dis-
CVaR-sensitive compared to the risk-neutral setting. tribution,theaimistolearnthefulldistributionofreturns.By
Bolonhez et al. [104] consider decentralized blockchain learningthereturndistribution,distributionalRLmethodscan
financial networks, particularly Bitcoin. They propose a least- capture the inherent randomness and risk in the environment
core-based quota allocation model for sharing mining pool more effectively. In these methods, instead of defining Q as
rewards,consideringmininguncertaintyandriskaversionina the return of a given state-action pair (s,a), we define it as a
cooperativemultiplayerMGwithadiscretestatespace(coali- distribution Z with respect to three r.v.s (reward, next state-
tions) and a continuous action space (percentage of quotas action pair and its corresponding random return) as
allocated to coalition members). Numerical experiments show
growing cooperative benefits over time but limitations due to Z(s,a)≜R(s,a)+γZ(s′,a′). (22)
dynamic reward changes. The authors also suggest potential
improvements, including varied pool configurations and time- SeeBellemareetal.[111]formoredetailsondistributional
dependent mining probabilities to increase the ”fairness” of RL.TheRMIXalgorithmconsidersCVaRrisksforagentsover
their quota allocation method. thelearneddistributionofZ valuesintheabovedefinitionfor
Li et al. [105] introduce a competitive risk-averse tri-level each agent in a distributional cooperative MARL setting. The
MG with defender and attacker players in supply networks. framework operates under centralized training with decentral-
The model has a discrete state space, binary action space in ized execution. It first learns each agent’s return distribution
levels one (defender’s capacity backups) and two (attacker’s to analytically calculate CVaR for decentralized execution.
facility attacks per locations), and a continous action space Afterwards, the algorithm includes a dynamic agent-specific
in level three (defender’s recovered capacities at different risk level predictor to adjust agents’ risk levels in real-time to
facilities). The model is applied to supply networks to min- handle the temporal nature of the stochastic outcomes during
imize the CvaR cost of the defender and protect against execution.Finally,CVaRpoliciesareoptimizedbyconsidering
worst-case attacks. The article proposes defender recovery CVaR values as auxiliary local rewards and targets in the
strategies while considering uncertainties in facility capacity temporaldifference(TD)errorduringcentralizedtraining,and
and impact of attacks to enhance the resilience of supply updating local return distributions by minimizing a quantile
networks against disruptions. Computational results show the regression loss function. The RMIX algorithm achieves state-
effectofbudgetlimitationsandbackuplevelsonoptimalrisk- of-the-art performance on cooperative cliff navigation scenar-
sensitivepolicies,andthathigherrecoveryabilityreducesrisk ios and the challenging StarCraft II benchmark. An algorithm
sensitivity of the defender. that is related to RMIX yet does not consider a traditional
Zhu et al. [106] examine a combined peer-to-peer (P2P) risk measure was proposed by Son et al. [112]. The authors
electricity market and carbon emission auction market mod- propose disentangled risk-sensitive multi-agent reinforcement
eled as an MG to promote localized energy trading and learning (DRIMA), a distributional MARL framework based
emissionreductionformicrogridswhentheagentsincorporate onIQN[88],whichdisentanglestheuncertaintiesarisingfrom
the CVaR risk criterion. They experimentally find the NE of environment transitions and other agents’ policies. They show
theproposedMGusingamulti-agentdeepdeterministicpolicy that in a cooperative MARL settings, if agents act in a risk-
gradient (MADDPG) algorithm in a continuous state-action neutral manner with respect to environmental uncertainties
setting. and ”optimistically” with respect to other agents’ policies,
Lin et al. [107] propose a p2p virtual power plant market they outperform other state-of-the-art MARL methods, both
bidding model for risk-aware energy trading among small- distributionalandnon-distributional,inthechallengingbench-
scalegeneratorsandconsumers.Theproposedmodelisatwo- mark of StarCraft. The risk measure used in this work can be
stageMGwithcontinuousactionandstatespaces.Theauthors called ”cooperative optimism”, i.e., the agents focus on the
find the CVaR-sensitive Cournot NE of the game using a dis- higherquantilesoftheaction-valuedistributionandessentially
tributed ADMM algorithm. The equilibrium balances supply assume that their teammates will take actions that lead to
and demand of risk-sensitive market players. By analyzing better outcomes, even if those actions have not been observed
case studies from the Australian energy market, the authors frequently during training.
showthattheproposedmarketstructurecaneffectivelyreduce Lyu et al. [113] focus on decentralized cooperative MARL
the overbidding risk while maximizing the renewable energy inpartiallyobservableMGswithcontinuousstateanddiscrete
usage. action spaces, and with an optional risk-seeking setup, where
Munir et al. [108] consider energy challenges in IoT- the risk to be sought is equal to 1-CVaR, which they callPREPRINT 10
conditionalvalue-not-at-riskorCVnaR.Theauthorsintroduce measure, and as we saw in Section IV almost exclusively to
a decentralized quantile estimator, based on IQN [88], called theoreticalanalysis.Althoughthetrendoftheoreticalresearch
likelihood quantile network (LQN). In this framework, each for exponential risk has continued with a steady increase in
agent uses a decentralized quantile estimator to distinguish the number of articles, other risk measures with motivations
non-stationary samples based on the likelihood of returns. that are more application-oriented have become popular in
This helps in identifying samples that are influenced by other recent years. This trend can be attributed to first, the need for
agents’ exploration or sub-optimal policies. The experimental specializedmeasuresofriskfordifferentapplicationdomains,
results show that risk-seeking behavior in cooperative MARL and two, to the emergence of deep RL methods [118]–[120]
encourages exploration of high-reward spaces and converges since mid 2010s and availability of high-end GPUs that have
faster compared to vanilla LQN. enabled researchers to develop risk-sensitive multi-agent deep
Finally, Wang et al. [114] study risk-averse online convex RL algorithms with different measures of risk akin to their
competitive MGs with continuous state and action spaces, specific application. With the increasing need for modeling
whereagentsseektominimizeaCVaRoftheircostfunctions. and learning risk-sensitive behavior and policies in multi-
The authors propose an online algorithm that approximates agent systems in real-world scenarios, specifically in finance,
CVaR using zeroth-order gradient estimates and bandit feed- energy trade, and autonomous driving, and with the rapid
back and achieves sub-linear regret with a high probability. advancement of deep RL, we predict that the use of more
They also introduce two enhancements for improving the diverse measures of risk, best fit for specific application
regret bound, i.e., reusing samples from the previous iteration domains continues to rise in the coming years.
and residual feedback.
3) Chance Constraint: Zhong et al. [115] consider multi-
robot planning in a stochastic dynamic game framework with
10
continuousstate-actionspacesandchanceconstraintriskbased
on Schwarting et al.’s game model [116] to capture agents’ 8
interactions and safety concerns. The paper presents the
chance-constrained iterative linear-quadratic Markov games 6
(CCILQGames) algorithm which uses augmented Lagrangian
4
with automatic weight tuning to find the risk-sensitive NE of
the game. They evaluate this algorithm in three autonomous
2
driving scenarios of lane merging, intersection, and round-
about. 0
Yadollahi et al. [117] propose a generalized stochastic
1990 1991 1995 1996 1997 2000 2011 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023
Year
dynamic game with continuous state and action spaces and
chance constraint risk criterion for modeling and analyzing
Fig.2. Numberofarticlesonrisk-sensitiveMGandMARLovertheyears
demand-sidemanagement,inamicrogridwhereagentsutilize
bothgridenergyandasharedbatterychargedbyrenewableen-
ergy sources. The term generalized in the context of demand- VI. CONCLUSION
side management implies that there are local constraints for
In this work, we conducted a systematic review of risk
individualagentsandsharedcouplingconstraints(imposedon
sensitivity in Markov games and multi-agent reinforcement
state of charge of the shared battery in the article) between
learning.Weidentifiedtheriskmeasuresusedintheliterature
agents in the game. The authors show the uniqueness of
and defined and described them mathematically. Afterwards,
the risk-sensitive generalized NE, and provide an iterative
we provided an in-depth analysis of the existing literature
algorithm to approximate this equilibrium. Simulation results
by considering each risk measure individually and discussing
demonstrate that the proposed stochastic model outperforms
their related articles. Finally, we identified trends and future
deterministic approaches with more effective peak shaving
directionsofresearch.Wehopethatthisreviewfacilitatesand
in the power exchange profile compared to the deterministic
promotes additional research efforts in risk-sensitive multi-
model.
agent systems.
V. TRENDSANDFUTUREDIRECTIONS REFERENCES
To observe the current trends and possible future directions [1] L.S.Shapley,“Stochasticgames,”Proceedingsofthenationalacademy
ofsciences,vol.39,no.10,pp.1095–1100,1953.
of research in risk-sensitive MG and MARL, we extract the
[2] M. L. Littman, “Markov games as a framework for multi-agent rein-
main characteristics of the reviewed studies, including risk forcementlearning,”inMachinelearningproceedings1994. Elsevier,
measure, year, type of MG, and type of state and action 1994,pp.157–163.
[3] M. Sayin, K. Zhang, D. Leslie, T. Basar, and A. Ozdaglar, “Decen-
spaces and provide this summary sorted by risk measure
tralized q-learning in zero-sum markov games,” Advances in Neural
and year in Table I. We also plot the number of articles InformationProcessingSystems,vol.34,pp.18320–18334,2021.
per year in Figure V. From these summaries, we observe [4] K.Zhang,S.Kakade,T.Basar,andL.Yang,“Model-basedmulti-agent
rl in zero-sum markov games with near-optimal sample complexity,”
that prior to 2016, the literature on risk-sensitive MG and
AdvancesinNeuralInformationProcessingSystems,vol.33,pp.1166–
MARL has been limited to the exponential reward/cost risk 1178,2020.
srepaPPREPRINT 11
IELBAT
:.POOC,EVITITEPMOC:.PMOC,MUS-OREZ:SZ,REYALP-N:PN,TSOC/DRAWERLAITNENOPXE:.OPXE:DESUSMROFTROHSFONOISNAPXE.DEWEIVERSELCITRAEHTFOSCITSIRETCARAHCNIAM
ETERCSID:.SID,SUOUNITNOC:.TNOC,NOITANIDROOC:.DROOC,EVITAREPOOC
noitcA/etatS
puteS
ydutS
raeY
ksiR
noitcA/etatS
puteS
ydutS
raeY
ksiR
leroB/elbaremuned
.pmoc
p2
]94[
6991
leroB/elbatnuoc
sz
p2
]46[
4202
.tnoc/.tnoc
.pmoc
p2
]95[
5991
.opxE
leroB/elbaremuned
sz
p2
]64[
3202
.tnoc/.tnoc
.pmoc
p2
]55[
1991
)etinfi
htob(
.sid/.sid
.pmoc
p2
]56[
3202
.tnoc/.tnoc
.pmoc
p2
]35[
0991
leroB/etinfi-.sid
sz-non
&
sz
p2
]36[
3202
.sid&.tnoc/.sid
.pmoc
p2
]501[
3202
leroB/leroB
sz
p2
]86[
3202
.tnoc/.sid
.drooc
pn
]401[
2202
.tnoc/.tnoc
.pmoc
pn
]06[
3202
.tnoc/.tnoc
.pmoc
pn
]601[
2202
leroB/elbaremuned
sz
p2
]05[
2202
.tnoc/.sid
.pmoc
pn
]301[
2202
leroB/.sid
sz
p2
]66[
2202
.tnoc/.tnoc
.pmoc
pn
]411[
2202
leroB/elbaremuned
gmsz-non
p2
]25[
2202
.tnoc/.tnoc
.pmoc
pn
]701[
1202
.sid/.sid
.drooc
p2
]27[
2202
.tnoc/.tnoc
.pmoc
pn
]801[
1202
leroB/elbaremuned
sz-non
pn
]17[
2202
.tnoc/.tnoc
.pooc
pn
]011[
1202
RaVC
.tnoc/leroB
sz
p2
]76[
2202
.tnoc/.tnoc
.pmoc
pn
]79[
1202
leroB/elbaremuned
sz-non
p2
]74[
1202
.sid/.tnoc
.drooc
pn
]311[
0202
.tnoc/.tnoc
dlefi-naem
pn
dna
sz
p3
]26[
1202
.tnoc/.tnoc
.pmoc
pn
]89[
0202
.sid/.tnoc
.pooc
pn
]16[
1202
.opxE
.tnoc/.tnoc
.pmoc
pn
]99[
8102
.tnoc/.tnoc
.pmoc
p2
]45[
0202
.tnoc/.tnoc
.pmoc
pn
]101[
7102
.tnoc/.tnoc
.pmoc
p2
]96[
9102
.tnoc/.tnoc
.pmoc
pn
]201[
6102
leroB/elbaremuned
.drooc
pn
]84[
9102
)etinfi
htob(
.sid/.sid
.pmoc
pn
]98[
4202
tcapmoc/.sid
gmsz
p2
]87[
9102
)etinfi
htob(
.sid/.sid
.pooc
.pn
]78[
3202
TPC
.tnoc/elbatnuoc
sz-non
p2
]47[
8102
)etinfi
htob(
.sid/.sid
.pmoc
p2
]38[
1202
leroB/leroB
sz
p2
]67[
7102
)etinfi
htob(
.sid/.sid
.pmoc
p2
]68[
8102
.tnoc/elbaremuned
sz
p2
]77[
6102
.tnoc/.tnoc
dlefi-naem
pn
]08[
1202
.tnoc/.tnoc
tseuqeb
pn
]75[
5102
)etinfi
htob(
.sid/.sid
.pmoc
pn
]28[
0202
tnerehoC
.tnoc/elbatnuoc
sz
p2
]57[
4102
.tnoc/.tnoc
.pmoc
pn
]18[
7102
leroB/leroB
tseuqeb
pn
]65[
4102
.sid/.sid
.pooc
pn
]59[
1202
ecnairaV
tcapmoc/etinfi-.sid
dlefi-naem
pn
]97[
1102
.sid/.tnoc
.pmoc-.pooc
dexim
pn
]39[
9102
.tnoc/.tnoc
sz-non
p2
]85[
0002
.tnoc/.tnoc
.pmoc
pn
]711[
3202
ecnahC
.tnoc/.tnoc
.pmoc
pn
]07[
7991
.tnoc/.tnoc
.pmoc
pn
]511[
3202
.sid/.tnoc
.drooc
pn
]211[
2202
msimitpO
.pooCPREPRINT 12
[5] A.Alacaoglu,L.Viano,N.He,andV.Cevher,“Anaturalactor-critic [29] E.N.Sereda,E.M.Bronshtein,S.T.Rachev,F.J.Fabozzi,W.Sun,and
framework for zero-summarkov games,” in International Conference S. V. Stoyanov, “Distortion risk measures in portfolio optimization,”
onMachineLearning. PMLR,2022,pp.307–366. Handbookofportfolioconstruction,pp.649–673,2010.
[6] J.Perolat,B.Scherrer,B.Piot,andO.Pietquin,“Approximatedynamic [30] N. Vijayan et al., “Policy gradient methods for distortion risk mea-
programmingfortwo-playerzero-summarkovgames,”inInternational sures,”arXivpreprintarXiv:2107.04422,2021.
ConferenceonMachineLearning. PMLR,2015,pp.1321–1329. [31] M.J.Sobel,“Thevarianceofdiscountedmarkovdecisionprocesses,”
[7] S. Qiu, X. Wei, J. Ye, Z. Wang, and Z. Yang, “Provably efficient JournalofAppliedProbability,vol.19,no.4,pp.794–802,1982.
fictitious play policy optimization for zero-sum markov games with [32] J. A. Filar, L. C. Kallenberg, and H.-M. Lee, “Variance-penalized
structuredtransitions,”inInternationalConferenceonMachineLearn- markov decision processes,” Mathematics of Operations Research,
ing. PMLR,2021,pp.8715–8725. vol.14,no.1,pp.147–161,1989.
[8] Z.Chen,K.Zhang,E.Mazumdar,A.Ozdaglar,andA.Wierman,“A [33] R.T.Rockafellar,S.Uryasevetal.,“Optimizationofconditionalvalue-
finite-sample analysis of payoff-based independent learning in zero- at-risk,”Journalofrisk,vol.2,pp.21–42,2000.
sum stochastic games,” Advances in Neural Information Processing [34] M.Ang,J.Sun,andQ.Yao,“Onthedualrepresentationofcoherent
Systems,vol.36,2024. risk measures,” Annals of Operations Research, vol. 262, pp. 29–46,
[9] C. Park, K. Zhang, and A. Ozdaglar, “Multi-player zero-sum markov 2018.
games with networked separable interactions,” Advances in Neural [35] A. Charnes, W. W. Cooper, and G. H. Symonds, “Cost horizons and
InformationProcessingSystems,vol.36,2024.
certaintyequivalents:anapproachtostochasticprogrammingofheating
[10] D.Ding,C.-Y.Wei,K.Zhang,andM.Jovanovic,“Independentpolicy oil,”Managementscience,vol.4,no.3,pp.235–263,1958.
gradient for large-scale markov potential games: Sharper rates, func-
[36] M.J.Page,J.E.McKenzie,P.M.Bossuyt,I.Boutron,T.C.Hoffmann,
tionapproximation,andgame-agnosticconvergence,”inInternational
C.D.Mulrow,L.Shamseer,J.M.Tetzlaff,E.A.Akl,S.E.Brennan
ConferenceonMachineLearning. PMLR,2022,pp.5166–5220.
etal.,“Theprisma2020statement:anupdatedguidelineforreporting
[11] R. Fox, S. M. Mcaleer, W. Overman, and I. Panageas, “Independent systematicreviews,”Bmj,vol.372,2021.
naturalpolicygradientalwaysconvergesinmarkovpotentialgames,”
[37] T.Bas¸ar,“Nashequilibriaofrisk-sensitivenonlinearstochasticdiffer-
in International Conference on Artificial Intelligence and Statistics.
ential games,” Journal of Optimization Theory and Applications, vol.
PMLR,2022,pp.4414–4425.
100,no.3,pp.479–498,1999.
[12] D. H. Mguni, Y. Wu, Y. Du, Y. Yang, Z. Wang, M. Li, Y. Wen,
[38] S. Golui and C. Pal, “Continucontinuous-time zero-sum games for
J.Jennings,andJ.Wang,“Learninginnonzero-sumstochasticgames
markovdecisionprocesseswithdiscountedrisk-sensitivecostcriterion
with potentials,” in International Conference on Machine Learning.
onageneralstatespaceous-timezero-sumgamesformarkovdecision
PMLR,2021,pp.7688–7699.
processeswithdiscountedrisk-sensitivecostcriteriononageneralstate
[13] S. Leonardos, W. Overman, I. Panageas, and G. Piliouras, “Global
space,”StochasticAnalysisandApplications,vol.41,no.2,p.327–
convergenceofmulti-agentpolicygradientinmarkovpotentialgames,”
357,2023.
arXivpreprintarXiv:2106.01969,2021.
[39] Y. Zhang, Z. Yang, and Z. Wang, “Provably efficient actor-critic
[14] C. Maheshwari, M. Wu, D. Pai, and S. Sastry, “Independent and
for risk-sensitive and robust adversarial rl: A linear-quadratic case,”
decentralized learning in markov potential games,” arXiv preprint
in International Conference on Artificial Intelligence and Statistics.
arXiv:2205.14590,2022.
PMLR,2021,pp.2764–2772.
[15] K. Zhang, Z. Yang, and T. Bas¸ar, “Multi-agent reinforcement learn-
[40] C. Tang and T. Bas¸ar, “Minimax nonlinear control under stochastic
ing: A selective overview of theories and algorithms,” Handbook of
uncertaintyconstraints,”vol.1,2003,pp.1065–1070.
reinforcementlearningandcontrol,pp.321–384,2021.
[41] K. Dvijotham and E. Todorov, “A unifying framework for linearly
[16] L.Canese,G.C.Cardarilli,L.DiNunzio,R.Fazzolari,D.Giardino,
solvablecontrol.” AUAIPress,2011,pp.179–186.
M. Re, and S. Spano`, “Multi-agent reinforcement learning: A review
[42] K. Zhang, X. Zhang, B. Hu, and T. Basar, “Derivative-free policy
of challenges and applications,” Applied Sciences, vol. 11, no. 11, p.
optimizationforlinearrisk-sensitiveandrobustcontroldesign:Implicit
4948,2021.
regularizationandsamplecomplexity,”AdvancesinNeuralInformation
[17] Y. Yang and J. Wang, “An overview of multi-agent reinforce-
ProcessingSystems,vol.34,pp.2949–2964,2021.
ment learning from game theoretical perspective,” arXiv preprint
[43] S.DeyandJ.Moore,“Risk-sensitivefilteringandsmoothingviarefer-
arXiv:2011.00583,2020.
ence probability methods,” IEEE Transactions on Automatic Control,
[18] L.Busoniu,R.Babuska,andB.DeSchutter,“Acomprehensivesurvey
vol.42,no.11,pp.1587–1591,1997.
ofmultiagentreinforcementlearning,”IEEETransactionsonSystems,
Man, and Cybernetics, Part C (Applications and Reviews), vol. 38, [44] W. Fleming and D. Herna´ndez-Herna´ndez, “Risk-sensitive control of
no.2,pp.156–172,2008.
finitestatemachinesonaninfinitehorizoni,”SIAMJournalonControl
andOptimization,vol.35,no.5,pp.1790–1810,1997.
[19] T.T.Nguyen,N.D.Nguyen,andS.Nahavandi,“Deepreinforcement
learning for multiagent systems: A review of challenges, solutions, [45] R. Ding and E. Feinberg, “Cvar optimization for mdps: Existence
andapplications,”IEEEtransactionsoncybernetics,vol.50,no.9,pp. andcomputationofoptimalpolicies,”PerformanceEvaluationReview,
3826–3839,2020. vol.50,no.2,pp.39–41,2022.
[20] S.GronauerandK.Diepold,“Multi-agentdeepreinforcementlearning: [46] M.Ghosh,S.Golui,C.Pal,andS.Pradhan,“Discrete-timezero-sum
a survey,” Artificial Intelligence Review, vol. 55, no. 2, pp. 895–943, games for markov chains with risk-sensitive average cost criterion,”
2022. StochasticProcessesandtheirApplications,vol.158,pp.40–74,2023.
[21] L. Prashanth, M. C. Fu et al., “Risk-sensitive reinforcement learning [47] Q. Wei and X. Chen, “Nonzero-sum risk-sensitive average stochastic
via policy gradient search,” Foundations and Trends® in Machine games:Thecaseofunboundedcosts,”DynamicGamesandApplica-
Learning,vol.15,no.5,pp.537–693,2022. tions,vol.11,no.4,pp.835–862,2021.
[22] R. A. Howard and J. E. Matheson, “Risk-sensitive markov decision [48] ——, “Risk-sensitive average equilibria for discrete-time stochastic
processes,”Managementscience,vol.18,no.7,pp.356–369,1972. games,”DynamicGamesandApplications,vol.9,pp.521–549,2019.
[23] P.Artzner,F.Delbaen,J.-M.Eber,andD.Heath,“Coherentmeasures [49] D. Herna´ndez-Herna´ndez and S. Marcus, “Risk sensitive control of
ofrisk,”Mathematicalfinance,vol.9,no.3,pp.203–228,1999. markov processes in countable state space,” Systems and Control
[24] R. T. Rockafellar, “Coherent approaches to risk in optimization un- Letters,vol.29,no.3,pp.147–155,1996.
der uncertainty,” in OR Tools and Applications: Glimpses of Future [50] S. Golui and C. Pal, “Continuous-time zero-sum games for markov
Technologies. Informs,2007,pp.38–61. chainswithrisk-sensitivefinite-horizoncostcriterion,”StochasticAnal-
[25] D. Kahneman and A. Tversky, “Prospect theory: An analysis of ysisandApplications,vol.40,no.1,pp.78–95,2022.
decisionunderrisk,”Econometrica,vol.47,no.2,pp.363–391,1979. [51] Q. Wei, “Nonzero-sum risk-sensitive finite-horizon continuous-time
[26] A. Tversky and D. Kahneman, “Advances in prospect theory: Cumu- stochasticgames,”StatisticsandProbabilityLetters,vol.147,pp.96–
lative representation of uncertainty,” Journal of Risk and uncertainty, 104,2019.
vol.5,pp.297–323,1992. [52] M. K. Ghosh, S. Golui, C. Pal, and S. Pradhan, “Nonzero-sum risk-
[27] C.Jie,L.Prashanth,M.Fu,S.Marcus,andC.Szepesva´ri,“Stochastic sensitivecontinuous-timestochasticgameswithergodiccosts,”Applied
optimizationinacumulativeprospecttheoryframework,”IEEETrans- Mathematics&Optimization,vol.86,no.1,p.6,2022.
actionsonAutomaticControl,vol.63,no.9,pp.2867–2882,2018. [53] P.CaravaniandG.Papavassilopoulos,“Aclassofrisk-sensitivenonco-
[28] D.Denneberg,“Distortedprobabilitiesandinsurancepremiums,”Meth- operativegames,”JournalofEconomicDynamicsandControl,vol.14,
odsofOperationsResearch,vol.63,no.3,pp.3–5,1990. no.1,pp.117–149,1990.PREPRINT 13
[54] M. Wang, N. Mehr, A. Gaidon, and M. Schwager, “Game-theoretic [78] R.Cavazos-CadenaandD.Herna´ndez-Herna´ndez,“Thevanishingdis-
planningforrisk-awareinteractiveagents.” InstituteofElectricaland countapproachinaclassofzero-sumfinitegameswithrisk-sensitive
ElectronicsEngineersInc.,2020,pp.6998–7005. averagecriterion,”SIAMJournalonControlandOptimization,vol.57,
[55] W. Krajewski, “Note on the dynamic risk sensitive nash games,” no.1,pp.219–240,2019.
LectureNotesinControlandInformationSciences,vol.157,pp.260– [79] H. Tembine, “Risk-sensitive mean field stochastic games.” Institute
268,1991. ofElectricalandElectronicsEngineersInc.,2011,pp.4264–4269.
[56] A.Jas´kiewiczandA.Nowak,“Stationarymarkovperfectequilibriain [80] J.Fre´de´ricBonnans,P.Lavigne,andL.Pfeiffer,“Discrete-timemean
risk sensitive stochastic overlapping generations models,” Journal of field games with risk-averse agents,” ESAIM - Control, Optimisation
EconomicTheory,vol.151,no.1,pp.411–447,2014. andCalculusofVariations,vol.27,2021.
[57] T.Balbus,A.Jas´kiewicz,andA.Nowak,“Stochasticbequestgames,” [81] J.-S. Pang, S. Sen, and U. Shanbhag, “Two-stage non-cooperative
GamesandEconomicBehavior,vol.90,pp.247–256,2015. gameswithrisk-averseplayers,”MathematicalProgramming,vol.165,
[58] M. B. Klompstra, “Nash equilibria in risk-sensitive dynamic games,” no.1,pp.235–290,2017.
IEEE Transactions on Automatic Control, vol. 45, no. 7, pp. 1397– [82] W.Huang,P.Hai,andW.Haskell,“Modelandreinforcementlearning
1401,2000. for markov games with risk preferences.” AAAI press, 2020, pp.
[59] M. Klompstra, “Nash equilibria in risk-sensitive dynamic games,” 2022–2029.
vol.3,1995,pp.2458–2462. [83] R. Tian, L. Sun, and M. Tomizuka, “Bounded risk-sensitive markov
[60] R. Xu and T. Wu, “Risk-sensitive large-population linear-quadratic- games:Forwardpolicydesignandinverserewardlearningwithiterative
gaussian games with major and minor agents,” Asian Journal of reasoningandcumulativeprospecttheory,”vol.7. Associationforthe
Control,2023. AdvancementofArtificialIntelligence,2021,pp.6011–6020.
[61] M. Naderi Soorki, W. Saad, M. Bennis, and C. Hong, “Ultra- [84] K. Lin, C. Jie, and S. I. Marcus, “Probabilistically distorted risk-
reliable indoor millimeter wave communications using multiple arti- sensitiveinfinite-horizondynamicprogramming,”Automatica,vol.97,
ficialintelligence-poweredintelligentsurfaces,”IEEETransactionson pp.1–6,2018.
Communications,vol.69,no.11,pp.7444–7457,2021. [85] J.R.WrightandK.Leyton-Brown,“Predictinghumanbehaviorinun-
repeated,simultaneous-movegames,”GamesandEconomicBehavior,
[62] T. Bas¸ar, “Robust designs through risk sensitivity: An overview,”
Journal of Systems Science and Complexity, vol. 34, pp. 1634–1665, vol.106,pp.16–37,2017.
[86] L.Xiao,D.Xu,N.Mandayam,andH.Poor,“Attacker-centricviewofa
2021.
detectiongameagainstadvancedpersistentthreats,”IEEETransactions
[63] A.BhabakandS.Saha,“Zeroandnon-zerosumrisk-sensitivesemi-
onMobileComputing,vol.17,no.11,pp.2512–2523,2018.
markov games,” Stochastic Analysis and Applications, vol. 41, no. 1,
[87] S.Shen,C.Ma,C.Li,W.Liu,Y.Fu,S.Mei,X.Liu,andC.Wang,
pp.134–151,2023.
“Riskq:risk-sensitivemulti-agentreinforcementlearningvaluefactor-
[64] M.K.Ghosh,S.Golui,C.Pal,andS.Pradhan,“Zero-sumstochastic
ization,”AdvancesinNeuralInformationProcessingSystems,vol.36,
gamesincontinuous-timewithrisk-sensitiveaveragecostcritionona
pp.34791–34825,2023.
countablestatespace,”MathematicalControlandRelatedFields,2024.
[88] W.Dabney,G.Ostrovski,D.Silver,andR.Munos,“Implicitquantile
[65] C. Baier, K. Chatterjee, T. Meggendorfer, and J. Piribauer, “Entropic
networks for distributional reinforcement learning,” in International
Risk for Turn-Based Stochastic Games,” in 48th International Sym-
conferenceonmachinelearning. PMLR,2018,pp.1096–1105.
posium on Mathematical Foundations of Computer Science (MFCS
[89] H.Ghaemi,H.Kebriaei,A.RamezaniMoghaddam,andM.NiliAh-
2023), ser. Leibniz International Proceedings in Informatics (LIPIcs),
madabadi, “Risk-sensitive multi-agent reinforcement learning in net-
J. Leroux, S. Lombardy, and D. Peleg, Eds., vol. 272. Dagstuhl,
workaggregativemarkovgames,”inProceedingsofthe23rdInterna-
Germany: Schloss Dagstuhl – Leibniz-Zentrum fu¨r Informatik, 2023,
tionalConferenceonAutonomousAgentsandMultiagentSystems,ser.
pp.15:1–15:16.
AAMAS’24. Richland,SC:InternationalFoundationforAutonomous
[66] S. Golui, C. Pal, and S. Saha, “Continuous-time zero-sum games
AgentsandMultiagentSystems,2024,p.2282–2284.
for markov decision processes with discounted risk-sensitive cost
[90] A. R. Moghaddam and H. Kebriaei, “Expected policy gradient for
criterion,”DynamicGamesandApplications,vol.12,no.2,pp.485–
networkaggregativemarkovgamesincontinuousspace,”IEEETrans-
512,2022.
actionsonNeuralNetworksandLearningSystems,2024.
[67] C. Pal and S. Pradhan, “Zero-sum games for pure jump processes
[91] F. Parise, S. Grammatico, B. Gentile, and J. Lygeros, “Distributed
withrisk-sensitivediscountedcostcriteria,”JournalofDynamicsand
convergence to nash equilibria in network and average aggregative
Games,vol.9,no.1,pp.13–25,2022.
games,”Automatica,vol.117,p.108959,2020.
[68] X.Guo,J.Chen,andZ.Li,“Zero-sumrisk-sensitivestochasticgames
[92] M. Shokri and H. Kebriaei, “Leader–follower network aggregative
withunboundedpayofffunctionsandvaryingdiscountfactors,”Journal
game with stochastic agents’ communication and activeness,” IEEE
ofMathematicalAnalysisandApplications,vol.519,no.2,2023.
Transactions on Automatic Control, vol. 65, no. 12, pp. 5496–5502,
[69] H. Asienkiewicz and L. Balbus, “Existence of nash equilibria in
2020.
stochastic games of resource extraction with risk-sensitive players,”
[93] D.Reddy,A.Saha,S.Tamilselvam,P.Agrawal,andP.Dayama,“Risk
TOP,vol.27,no.3,pp.502–518,2019.
averse reinforcement learning for mixed multi-agent environments,”
[70] G. Monahan and M. Sobel, “Risk-sensitive dynamic market share vol.4. InternationalFoundationforAutonomousAgentsandMulti-
attractiongames,”GamesandEconomicBehavior,vol.20,no.2,pp. agentSystems(IFAAMAS),2019,pp.2171–2173.
149–160,1997. [94] R.Lowe,Y.I.Wu,A.Tamar,J.Harb,O.PieterAbbeel,andI.Mor-
[71] Q. Wei and X. Chen, “Risk-sensitive first passage stochastic games datch,“Multi-agentactor-criticformixedcooperative-competitiveenvi-
withunboundedcosts,”Optimization,pp.1–34,2022. ronments,”Advancesinneuralinformationprocessingsystems,vol.30,
[72] E. Noorani and J. Baras, “Risk-attitudes, trust, and emergence of 2017.
coordination in multi-agent reinforcement learning systems: A study [95] E. Parilina and S. Akimochkin, “Cooperative stochastic games with
of independent risk-sensitive reinforce.” Institute of Electrical and mean-variancepreferences,”Mathematics,vol.9,no.3,pp.1–15,2021.
ElectronicsEngineersInc.,2022,pp.2266–2271. [96] J.Suijs,P.Borm,A.DeWaegenaere,andS.Tijs,“Cooperativegames
[73] R. J. Williams, “Simple statistical gradient-following algorithms for with stochastic payoffs,” European Journal of Operational Research,
connectionist reinforcement learning,” Machine learning, vol. 8, pp. vol.113,no.1,pp.193–205,1999.
229–256,1992. [97] S. He, S. Yu, L. Wang, Y. Liu, X. Lin, and W. Han, “Trade-off
[74] A.BasuandM.Ghosh,“Nonzero-sumrisk-sensitivestochasticgames stochastic game based bidding strategy for multiple wind farms.”
on a countable state space,” Mathematics of Operations Research, InstituteofElectricalandElectronicsEngineersInc.,2021,pp.1398–
vol.43,no.2,pp.516–532,2018. 1403.
[75] A.BasuandM.K.Ghosh,“Zero-sumrisk-sensitivestochasticgames [98] S.Cui,Y.-W.Wang,C.Li,andJ.-W.Xiao,“Prosumercommunity:A
onacountablestatespace,”Stochasticprocessesandtheirapplications, riskaversionenergysharingmodel,”IEEETransactionsonSustainable
vol.124,no.1,pp.961–983,2014. Energy,vol.11,no.2,pp.828–838,2020.
[76] N. Ba¨uerle and A. Glauner, “Distributionally robust markov decision [99] C. Li, C. Liu, K. Deng, X. Yu, and T. Huang, “Data-driven charging
processes and their connection to risk measures,” Mathematics of strategyofpevsundertransformeragingrisk,”IEEETransactionson
OperationsResearch,vol.47,no.3,pp.1757–1780,2022. ControlSystemsTechnology,vol.26,no.4,pp.1386–1399,2018.
[77] M.Ghosh,K.Kumar,andC.Pal,“Zero-sumrisk-sensitivestochastic [100] E.WeiandA.Ozdaglar,“Distributedalternatingdirectionmethodof
games for continuous time markov chains,” Stochastic Analysis and multipliers,” in 2012 IEEE 51st IEEE Conference on Decision and
Applications,vol.34,no.5,pp.835–851,2016. Control(CDC). IEEE,2012,pp.5445–5450.PREPRINT 14
[101] C. Li, Y. Xu, X. Yu, C. Ryan, and T. Huang, “Risk-averse energy
trading in multienergy microgrids: A two-stage stochastic game ap-
proach,” IEEE Transactions on Industrial Informatics, vol. 13, no. 5,
pp.2620–2630,2017.
[102] C.Li,X.Yu,P.Sokolowski,N.Liu,andG.Chen,“Astochasticgame
for energy resource trading in the context of energy internet,” vol.
2016-November. IEEEComputerSociety,2016.
[103] A.Heidari,R.Bansal,J.Hossain,andJ.Zhu,“Strategicriskaversion
ofsmartenergyhubsinthejoinedenergymarketsapplyingastochastic
gameapproach,”JournalofCleanerProduction,vol.349,2022.
[104] E.Bolonhez,T.Silva,andB.Fanzeres,“Acore-basedquotaallocation
model for the bitcoin-refunded blockchain network,” Expert Systems
withApplications,vol.209,2022.
[105] Q. Li, M. Li, Y. Tian, and J. Gan, “A risk-averse tri-level stochastic
model for locating and recovering facilities against attacks in an
uncertainenvironment,”ReliabilityEngineeringandSystemSafety,vol.
229,2023.
[106] Z. Zhu, K. Chan, S. Bu, B. Zhou, and S. Xia, “Nash equilibrium
estimation and analysis in joint peer-to-peer electricity and carbon
emissionauctionmarketwithmicrogridprosumers,”IEEETransactions
onPowerSystems,pp.1–13,2022.
[107] W.-T. Lin, G. Chen, and C. Li, “Risk-averse energy trading among
peer-to-peerbasedvirtualpowerplants:Astochasticgameapproach,”
International Journal of Electrical Power and Energy Systems, vol.
132,2021.
[108] M.Munir,S.Abedin,N.Tran,Z.Han,E.-N.Huh,andC.Hong,“Risk-
awareenergyschedulingforedgecomputingwithmicrogrid:Amulti-
agent deep reinforcement learning approach,” IEEE Transactions on
NetworkandServiceManagement,vol.18,no.3,pp.3476–3497,2021.
[109] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D.Silver,andK.Kavukcuoglu,“Asynchronousmethodsfordeeprein-
forcementlearning,”inInternationalconferenceonmachinelearning.
PMLR,2016,pp.1928–1937.
[110] W.Qiu,X.Wang,R.Yu,R.Wang,X.He,B.An,S.Obraztsova,and
Z.Rabinovich,“Rmix:Learningrisk-sensitivepoliciesforcooperative
reinforcement learning agents,” Advances in Neural Information Pro-
cessingSystems,vol.34,pp.23049–23062,2021.
[111] M.G.Bellemare,W.Dabney,andR.Munos,“Adistributionalperspec-
tiveonreinforcementlearning,”inInternationalconferenceonmachine
learning. PMLR,2017,pp.449–458.
[112] K.Son,J.Kim,S.Ahn,R.D.Reyes,Y.Yi,andJ.Shin,“Disentangling
sources of risk for distributional multi-agent reinforcement learning,”
C.K.,J.S.,S.L.,S.C.,N.G.,andS.S.,Eds.,vol.162. MLResearch
Press,2022,Conferencepaper,p.20347–20368.
[113] X. Lyu and C. Amato, “Likelihood quantile networks for coordi-
nating multi-agent reinforcement learning,” S. G. An B., El Fallah
Seghrouchni A., Ed., vol. 2020-May. International Foundation for
Autonomous Agents and Multiagent Systems (IFAAMAS), 2020, pp.
798–806.
[114] Z.Wang,Y.Shen,andM.M.Zavlanos,“Risk-averseno-regretlearning
in online convex games,” C. K., J. S., S. L., S. C., N. G., and S. S.,
Eds.,vol.162. MLResearchPress,2022,Conferencepaper,p.22999
–23017.
[115] H. Zhong, Y. Shimizu, and J. Chen, “Chance-constrained iterative
linear-quadratic stochastic games,” IEEE Robotics and Automation
Letters,vol.8,no.1,pp.440–447,2023.
[116] W. Schwarting, A. Pierson, S. Karaman, and D. Rus, “Stochastic
dynamic games in belief space,” IEEE Transactions on Robotics,
vol.37,no.6,pp.2157–2172,2021.
[117] S. Yadollahi, H. Kebriaei, and S. Soudjani, “Generalized stochastic
dynamicaggregativegamefordemand-sidemanagementinmicrogrids
withsharedbattery,”IEEEControlSystemsLetters,2023.
[118] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,
D.Wierstra,andM.Riedmiller,“Playingatariwithdeepreinforcement
learning,”arXivpreprintarXiv:1312.5602,2013.
[119] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforce-
mentlearning,”arXivpreprintarXiv:1509.02971,2015.
[120] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,
A.Guez,T.Hubert,L.Baker,M.Lai,A.Boltonetal.,“Masteringthe
gameofgowithouthumanknowledge,”nature,vol.550,no.7676,pp.
354–359,2017.