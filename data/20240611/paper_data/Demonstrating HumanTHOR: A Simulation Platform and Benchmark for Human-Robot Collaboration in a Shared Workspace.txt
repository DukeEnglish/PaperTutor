Demonstrating HumanTHOR: A Simulation
Platform and Benchmark for Human-Robot
Collaboration in a Shared Workspace
Chenxu Wang†, Boyuan Du†, Jiaxin Xu, Peiyan Li, Di Guo, Huaping Liu∗
Abstract—Human-robot collaboration (HRC) in a shared to support real-time interaction with humans. (2) The human
workspace has become a common pattern in real-world robot andtherobotworkinasharedworkspace,wheretheysharethe
applications and has garnered significant research interest.
same level of observation and similar capabilities rather than
However, most existing studies for human-in-the-loop (HITL)
the instructor-follower paradigm in previous works [16, 33].
collaboration with robots in a shared workspace evaluate in
eithersimplifiedgameenvironmentsorphysicalplatforms,falling For example, for the task of putting an apple in the fridge,
short in limited realistic significance or limited scalability. To the human has to first search for the apple and then search
supportfuturestudies,webuildanembodiedframeworknamed for the fridge to put the apple in it. With the human-robot
HumanTHOR, which enables humans to act in the simulation
collaboration, the human and robot can simultaneously search
environmentthroughVRdevicestosupportHITLcollaborations
for the apple and fridge respectively in the shared workspace.
in a shared workspace. To validate our system, we build a
benchmark of everyday tasks and conduct a preliminary user Whentherobotfindsthefridge,itcouldreportthepositionof
study with two baseline algorithms. The results show that the the fridge to the human and the human could take the apple
robotcaneffectivelyassisthumansincollaboration,demonstrat- directly to the fridge, improving working efficiency.
ing the significance of HRC. The comparison among different
To support further study, establishing a system along with
levels of baselines affirms that our system can adequately
corresponding benchmarks is crucial. However, existing sys-
evaluaterobotcapabilitiesandserveasabenchmarkfordifferent
robot algorithms. The experimental results also indicate that temsmaynotbesufficientforstudyingtheHITLcollaboration
there is still much room in the area and our system can with robots in a shared workspace due to various limitations.
provide a preliminary foundation for future HRC research in OnemainstreampracticeistostudyHRConphysicalsystems,
a shared workspace. More information about the simulation
such as performing collaborative assembly of furniture [30],
environment, experiment videos, benchmark descriptions, and
pickandplacetasks[13],andsimplepickingobjects[36].De-
additionalsupplementarymaterialscanbefoundonthewebsite:
https://sites.google.com/view/humanthor/. spite the practical effectiveness, physical systems may suffer
from high economic costs, limited reproducibility, and rela-
I. INTRODUCTION tively small scales. On the other hand, studying cooperation
in 2D games such as the Overcooked [4] and the cooperative
With the development of various techniques in multi-modal
table-carryinggame[32]mayfallshortinbeingtoosimpleto
perception, reasoning, planning, and control, robots have be-
generalize to physical robots, despite their reproducibility.
come increasingly powerful and are gradually deployed in a
Fortunately, recently there emerge many embodied simula-
variety of application scenarios. Human-robot collaboration
tors such as AI2THOR series [21, 9, 12], which appear to be
(HRC) has been gaining more and more interest and become
excellentplatformsforstudyingrobotalgorithms,forbothbe-
a hot-spot problem in various real-world domains, such as
ingabletoscaleupandapproximatetherealworld.However,
industry [45, 22], surgery [19, 29], rescue [31], health caring
although many studies have noticed the importance of human
[3], agriculture [44], and home service [23].
factors and extended the benchmarks to incorporate various
Acommonandrealisticcollaborationscenarioistheloosely
modalitiesofhumancollecteddata,suchas[18,49,16],there
coupled collaboration in a shared workspace, such as doing
have been limited attempts to study the HITL collaboration in
housework, where people act independently in the house
such embodied simulators.
towards a shared goal and may communicate with the robot
To support the study of HITL collaboration with robots
to exchange information. Studying HRC in such a scenario
in a shared workspace, we build an embodied human-robot
necessitatestwokeyfeaturesofthesimulationsystem:(1)The
collaboration environment based on the AI2THOR frame-
task is human-in-the-loop (HITL), requiring the environment
work, namely HumanTHOR. As shown in Figure 1, humans
Chenxu Wang, Jiaxin Xu, Peiyan Li and Huaping Liu are with Depart- can immersively control their avatars to cooperate with the
ment of Computer Science and Technology, Tsinghua University, Beijing, robot in the environment through VR devices. Compared to
China. Boyuan Du is with Fuzhou University, Fujian, China. Di Guo is
previous works that use VR to collect human demonstration
with the School of Artificial Intelligence, Beijing University of Posts and
Telecommunications, Beijing, China. This work was jointly supported by [24, 14, 25], our environment is dedicated to HRC tasks with
the National Natural Science Fund for Distinguished Young Scholars under variousfunctionalsupports,suchasenablingsynchronouscol-
Grant62025304andNationalNaturalScienceFoundationProjectunderGrant
laboration between humans and robots and providing image-
62273054.† denotesequalcontribution.∗ denotesthecorrespondingauthor:
hpliu@tsinghua.edu.cn text communication interfaces.
4202
nuJ
01
]OR.sc[
1v89460.6042:viXraFig. 1: An overview of the HumanTHOR system, where the human can act in the simulator through the VR device with the
first-person view akin to the robot. The system also supports the top-down view with instant displaying of the positions and
orientations of the human and the robot.
To demonstrate our system, we further implement a pre- II. RELATEDWORK
liminary HRC benchmark of everyday tasks, including two
A. Human-Robot Collaboration
representative tasks: object goal navigation and mobile ma-
nipulation. To generate realistic and diversified episodes, the The pursuit of creating collaborative and user-friendly
initial states of object arrangement are sampled from a set of robots has attracted substantial attention in the research com-
scene priors. To validate our system, we conduct a user study munity. To provide an immersive user experience, Virtual
by employing a rule-based robot and an oracle robot as base- Reality (VR) hasbeen frequently utilized as theuser interface
lines. Experimental results show that robots can significantly in human-robot interaction studies [11]. Beyond serving as a
assist humans in everyday tasks, and our system serves as an display media [34], the matched controllers enable humans
effective testbed for HRC studies. to teleoperate robots or their avatars [8, 20, 42]. Such VR-
We summarize our main contributions as follows: based controls are also extensively used for collecting human
demonstrations for robot learning [24, 15].
• We develop the HumanTHOR system, which enables Since VR provides virtual and digital visions, such tech-
real-timehuman-robotcollaborationinsharedworkspace niques can also be integrated with real-world systems and be-
with multi-modal communication through VR devices. comeAugmentedReality(AR)orMixedReality(MR),which
• Weimplementabenchmarkofeverydaytasks,whichcan significantly benefit collaborations in the physical world. MR
be used for studying HRC in embodied scenarios with a canserveasacommunicationmediumtoconveytheintention
human-robot communication mechanism. ofrobots[36,30],whileARcanfacilitatethestudyofhuman-
• We run a user study with various settings of robots. The robot collaboration in shared workspace [35].
resultssuggesttheeffectivenessofoursystemtoevaluate To provide immersive environment exploration and oper-
the capability of robot algorithms and further serve as a ation and approximate the human-robot collaboration in the
testbed for studying HRC problems. real world, we employ VR as the user interface in our
HumanTHOR system.
Thispaperisorganizedasfollows:inSectionII,wereview
recent works that are related to our study. We introduce the
B. Simulators and Benchmarks for Embodied Intelligence
architecture, main characteristics, and implementation of the
HumanTHOR system in Section III. In Section IV, we elabo- Along with the rising interest in embodied intelligence,
rateonthedesignationanddetailsoftheHRCbenchmark.We a considerable number of simulation environments emerge.
thenpresenttheexperimentresultsandcorrespondinganalysis Environments such as AI2THOR series [21, 12, 9], iGibson
in Section V. In Section VI, we demonstrate the extensibility [39, 24], Habitat [38, 43], Matterport3D [5], and RFUniverse
of our system and extensions regarding multi robots with [15] have present vivid simulation with various physics sys-
more complex HRC tasks and robot algorithms. Finally, we tems and make it possible for building benchmark for tasks
conclude and discuss the future work in Section VII. such as navigation or manipulation.Beyond the platforms, various benchmarks for embodied PPyytthhoonn SSooffttwwaarree
tasks have been proposed, such as question answering [7],
TTaasskk DDaattaasseett RRoobboott AAllggoorriitthhmmss
visual language navigation [1], scene graph generation [26],
andperformingcompositeeverydaytasks[40].Afterachieving PPrroobblleemm DDeeffiinniittiioonn MMaannaaggeemmeenntt UUII
adorable success in these tasks, researchers have turned their BBeenncchhmmaarrkk SSeettttiinngg EExxppeerriimmeennttaall MMoodduulleess
gaze to multi-agent settings [27, 28], considering human- HHuummaann
factors [41, 50], and collaboration with humans. Beyond a PPyytthhoonn AAPPIIss
single paragraph of instructions, GesTHOR [49] augment the
embodied navigation with human gesture indications. Hand- SSoocckkeett--bbaasseedd AAPPIIss
MeThat [46] takes one more step toward HRC by playing a
IImmaaggee PPhhyyssiiccaall Oculus
record of human actions and instructions and then trains the RReennddeerriinngg SSiimmuullaattiioonn Apps
robots to follow such instructions. Recent benchmarks con-
EEnnvviirroonnmmeenntt AAggeenntt HHuummaann--RRoobboott
sider communication between humans and robots. DialFRED CCoonnffiigguurraattiioonn CCoonnttrroolllliinngg CCoommmmuunniiccaattiioonn VVRR ddeevviicceess
[18] and TEACh [33] extend the instructions into dialogue
TTHHOORR ssiimmuullaattoorr
form and encourage the robots to actively raise questions.
Alexa Arena [16] builds an interactive environment where the Fig. 2: The architecture of our HumanTHOR system.
instructor and executor can interactively communicate with
naturallanguage.Whilethebenchmarkisbuiltonofflinedata,
the environment may also support HITL collaborations. • The python software that supports further HRC study,
To comprehensively support our intended HRC study, the including extensible benchmarks which will be intro-
system requires several key features, including: (1) Vivid ducedinSectionIV,acodeframeworkforrunningrobot
3D simulation, for both aligning the simulation to real-life algorithms and a web user interface for controlling and
robot application and better human experience; (2) HITL monitoring the HRC experiments.
interaction, for conducting experiments with real humans • VR devices.WeuseMetaQuest2inoursystem,includ-
instead of with proxy agents or offline data; (3) Shared ing a headset and a pair of controllers. VR devices are
workspace collaboration, which is the domain we intend connected to our simulator through the Oculus platform.
to study; and (4) Instant communication, a vital component
By integrating VR devices into our system, humans and
for human-robot interaction and simulating the collaboration
robots can concurrently work within the environment, thereby
in the real world. For loosely coupled collaboration where
achieving HITL and human-robot collaboration in a shared
the human and robots may not always stay in the vision
workspace.
of each other, the system is expected to support multimedia
message-style communication, including both image and text.
B. Supported APIs
Additionally,thesystemisexpectedtohaveaVRinterfacefor
To support various requirements of HRC benchmarks, our
both immersive human experience and better aligning human
systemprovidesdozensofAPIsintheHumanTHORenviron-
activities to the real life.
ment, which are generally in the following four categories:
However,existingsimulationenvironmentsarenotsufficient
enoughtosimultaneouslymeetalltherequirementsmentioned • Environment configuration: We provide a series of
above. To overcome this shortage, we build the HumanTHOR APIs for setting up the environment, including selecting
system that includes all the abovementioned features. A de- scenes, initialization, and object settings. To support the
tailedcomparisonbetweentheproposedHumanTHORandthe development of benchmarks to set up customized tasks,
existing simulation environment is delineated in Table I. environmental information is also obtainable, such as the
amount, positions, and states of objects.
III. SYSTEMFRAMEWORK • Agent control: Similar to previous works, HumanTHOR
supports agent actions such as move, rotate, teleport,
A. System Overview
pick, and place. For the convenience of customizing
Aiming to support human-in-the-loop collaborations in a benchmarks, we use a flexible format in which the move
shared workspace with flexible and multimodal communica- action and the rotate action take a vector that indicates
tion,wedesignandimplementtheHumanTHORsystembased thevariationasinput,andleavethedesignationofatomic
ontheAI2THORplatform.Asthehierarchicalarchitectureil- actions to the benchmark level.
lustratedinFigure2,oursystemconsistsofthreecomponents: • Perception and monitoring: Following the conventions,
• The THOR simulator implemented with the Unity our environment provides an ego-centric RGB observa-
framework based on the AI2THOR infrastructure, re- tion and a depth map. We also provide a top-view image
sponsible for physical simulation, scene rendering, and and the positions of all agents for monitoring.
solvingtheinteractionbetweenagentsandenvironments. • Communication: A major contribution of our Human-
The simulator exposes a set of socket-based APIs for THOR environment is the multimodal communication
controlling. module for collaboration. We provide communicationCommunication
Benchmark SimulationDomain VRInterface HITLCollaboration SharedWorkspace
Image Text
Overcooked[4] 2D ✗ ✓ ✓ ✗ ✗
Ittakestwo[32] 2D ✗ ✓ ✓ ✗ ✗
HandOverSim[6] 3D ✗ ✗ ✓ ✗ ✗
VRKitchen[17] 3D ✓ ✗ ✗ ✗ ✗
DialFred[18] 3D ✗ ✗ ✗ ✗ ✓
TeaCh[33] 3D ✗ ✗ ✗ ✗ ✓
‘HandMeThat[46] 3D ✗ ✗ ✗ ✗ ✓
GesTHOR[49] 3D ✓ ✗ ✗ ✗ ✗
AlexaArena[16] 3D ✗ ✓ ✗ ✗ ✓
HumanTHOR 3D ✓ ✓ ✓ ✓ ✓
TABLE I: Comparison between HumanTHOR and related HRC benchmarks across various aspects.
APIs including sending image-text messages, responding
to messages, and querying the response status.
C. User Interface and VR Supports
To provide an immersive experience for human players,
we employ VR as the user interface. Humans can perceive
the environment from the first perspective with the headset,
and control their avatars through the buttons and the rocking
bars on the controllers, which enable them to move in the
environment and interact with objects. We further develop
several interfaces for human-robot communication including
an image-text message box and a location map, all can be
presented in the human vision. We present these operations in
Figure 3.
Fig. 3: Interacting with the environment with VR devices. (a)
IV. BENCHMARKINGHRCWITHEVERYDAYTASKS Moving the human avatar by operating the joystick on the
The HumanTHOR platform is eligible for studying various VR controller. (b) With the help of the sensors on the head-
HRCtasksatvariouslevels,beingavailableforbothevaluating mounted display, humans can conveniently rotate the angle
robot algorithms and human-robot collaboration studies. We of view by turning around in reality. (c) When being close
model the benchmark structure in a hierarchical way as enough to an object, humans can pick up a movable object
illustratedinFigure4,wherewestartbysupportingelemental by pressing the side button as shown in the figure, or open a
tasks,basedonwhichwecanperformpracticaleverydaytasks, receptaclesuchasafridge.(d)Afterreceivingamessagefrom
such as visual language navigation [40], room rearrangement the robot, humans can make quick responses with the A/B
[2, 47], tidying up [37]. Ultimately, HumanTHOR supports buttons on the controller. In our benchmark of collaborative
studying high-level scientific problems such as Theory-of- tasks,themessageispresentedinadialogueboxinthehuman
Mind and human trust modeling. view, where button A is for confirmation and button B is for
decline. After confirmation, a map with the relative positions
Topreliminarilyvalidatetheeffectivenessofoursystem,we
build benchmarks for the elementary object goal navigation of the robot and human will be displayed, which can also be
task and a compound HRC task, mobile manipulation, as hidden or redisplayed by the button operation.
defined in the following:
• Object goal navigation, a representative and fundamen-
essential components: initial state that determines the initial
tal task in the embodied intelligence domain. Such tasks
arrangement of objects, goal definition which formalizes the
targetacommonsituationinwhichahumanissearching
criterionofsuccess,andauxiliaryinformationsuchasnatural
for something in the house. The task succeeds when the
language description of the tasks.
human successfully finds the target object.
• Mobile manipulation. In this task, participants need to A. Object-centric Interactive Communication
pick up the target object and place it at the destination We implement an object-centric communication flow with
place.Thisisafundamentaltypeofeverydaytask,which corresponding user interfaces for human-robot collaboration
is also adopted by ALFRED [40], being more complex in everyday tasks. Whenever the robot feels necessary to
than navigation tasks since it involves multiple objects communicate with object-related information, it can send an
and interactions with the environment. image-text message as shown in Figure 6 (a), which consists
Two examples of tasks are presented in Figure 5. Similar of the observation of the robot and a custom message text,
to previous works [40, 16], our tasks are defined with three as in Figure 6 (b). If humans are interested in the message,Future Scientific Problems
Trust Modelling Theory-of-Mind Emergent Behavior
Human-Robot Collaboration Tasks
Mobile Visual Language Room Room
Manipulation Navigation Rearrangement Tidying up
(a) The image-text message sent by the robot
Elementary Skills
Object-goal Navigation PickandPlace
HHuummaann
Fig.4:ThehierarchicalbenchmarkssupportedbytheHuman-
THOR platform.
RRoobboott
TTaarrggeett
(b) The map with relative locations after confirmation
Fig.6:Anillustrationoftheobject-centriccommunicationfor
human-robot collaboration.
Fig. 5: The general processes of navigation tasks and manip-
ulation tasks. The mobile manipulation task is more complex
and difficult since it requires further manipulation after suc-
cessfully finding the target.
Fig. 7: An example of scene priors and the task generation
pipeline.
theycanpressthebuttontoconfirm.Thenamapwithrelative
positions will be shown subsequently to help the human go
to the suggested place. Both user interfaces can be temporally
a possible prior relation (target,relation,reference). We
hidden and displayed whenever the human wants. We also
weight the triplets by combining the similarities between the
provide an API for the robot to query whether its suggestion
textual features of objects adopted from the pretrained BERT
is accepted or declined.
model [10] and human priors.
It is worth noting that robots are free to choose when and
We take three steps to instantiate the task as illustrated in
what to send in the message, and are allowed to cover the
Figure 7. First, we pick a task template that is applicable to
previous message with new ones. Combined with hierarchical
the scene to determine the goal. Then we construct a scene
levels of capabilities in heterogeneous teamwork, there still
graph by sampling scene priors from the knowledge graph.
existsalargeroomforfurtherstudy.Forcomplextasks,robots
Eachsampledrelationcorrespondstoanareawheretheobject
may need to carefully estimate human mental state to choose
should be placed, and the specific position of each object is
the appropriate time for sending messages, and conveying
subsequentlydetermined.Followingthisprogram,wegenerate
important information while not bothering humans too much.
565 navigation tasks and 1583 mobile manipulation tasks. All
B. Generating Everyday Tasks with Scene Priors initial relations of the target object are sampled once for each
task template in each scene, while the irrelevant objects are
Our benchmark incorporates 10 scenes and 14 objects, in-
not limited.
cluding8receptacleobjectsand6targetobjects.Weobtain19
mobile manipulation task templates by artificial annotations,
V. EXPERIMENTS
which are in the form of Picking a/an {target object} and
place it in/on a/an {receptacle objects}. TovalidateourproposedHumanTHORsystem,weconduct
We generate the initial state with the guide of scene priors a preliminary user study on a subset of our built benchmark,
like exhibited in Figure 7. We first build a knowledge graph focusing on the mobile manipulation tasks, which are more
with 65 triplets by human annotations, each triplet represents complex and encompass the navigation task. We aim toRobotsetting SR(%) TWSR(%) AdoptionRate(%)
human participants, which are equally divided into 3 groups.
NoRobot 63.8 46.7 /
Frontier 72.9 54.3 56.0 Each group plays within a robot setting for 10 random tasks,
Oracle 91.4 66.7 90.0 one task in each scene. For a fair comparison, the sequence
of tasks is random for each participant. For each task, the
TABLE II: Average success rate, time-weighted success rate,
participant has 90 seconds to finish the task. After finishing
and adoption rate in all three settings.
each task, the participant is asked to give a trust score for the
robot (if applicable) to study human trust. The trust score is
an integer from 1 to 7, where 1 denotes not trust at all and 7
denotesthehighesttrust.Thoughtheparticipantsdonotknow
the robots, they are told that they will be cooperating with
the same robot throughout the experiment. After eliminating
broken cases, we get 175 unit trials for analysis.
We assess the capability of robots through the success rate
andexecutiontime.Thetasksuccessscoresisdefinedas1if
thetargetobjectisplacedinthecorrectplace,and0otherwise.
Fig. 8: Average time spent to Fig. 9: The variation of aver-
The success rate (SR) is calculated by the ratio of success
complete the tasks age human trust scores over
tasks. As a human-centric experiment, we use the time-
trials.
weighted success rate (TWSR) instead of path path-weighted
successrate(PWSR).TheTWSRscores iscalculatedbythe
t
answer two questions in the experiment: (1) the necessity following formula:
of HRC: can the robot help humans perform the tasks? (2) T∗
The effectiveness of our system as an evaluation benchmark: s t =s∗ max(T∗,T), (1)
can our experiment distinguish different levels of robots and
evaluate their capabilities? where T denotes the time spent on the task, and T∗ is the
minimum required time that is estimated by the distance of
A. Baselines
the shortest path. Besides, we record the adoption rate for
We incorporate two simple robot baselines in the exper- robots to measure their help. A robot is considered adopted if
iments. For both robots, the enabled actions include move, itsendsamessagethatisconfirmedbythehuman.Participants
rotate, and send message. The atomic movement distance canmodifytheiradoptionorallywhenmistouchthebutton,or
can not exceed 0.5 meters, the angle of pitch is limited to exclude it from statistics for boundary cases.
[−30◦,30◦], and sending a message requires a custom text
C. Results
and an estimated position. The details of the baselines are as
follows: 1) Quantitative Analysis: We present the quantitative
• Frontier agent. It is a rule-based agent that explores scores in Table II. Compared to the control group where
the environment with the frontier algorithm and sends no robot assistant exists, both the frontier robot and the
messages whenever it detects the target. To simplify the oraclebringimprovementinallquantitativemetrics,including
task, we endow the frontier agent with an ideal object the success rate and the time-weighted success rate. As an
detector that can get ground truth object detection within approximation of the upper limit, the oracle improves the
1.5 meters. successratesignificantlyto91.4%,muchmoresignificantthan
• Oracle. The oracle robot knows the positions of all the frontier agent which serves as a medium-level baseline.
objects and will navigate to the target object along the The oracle also has a much higher adoption rate, suggesting
shortestpath.However,itstillfollowsthecommunication it is considered more helpful in collaboration. The significant
framework and only sends messages when it is close discrepanciesinquantitativemetricsshowthatoursystemcan
enough to detect the target. successfully distinguish the capability of robots and thus be
an eligible evaluation benchmark.
Both baselines take a default setting to search for the target
object instead of the receptacle or dynamic task allocation. Another representative indicator, the average time spent
Though the baselines may not be practical or optimal, we across tasks is shown by the box plots in Figure 8. Although
clarifythattheyareusedfortheverificationandcalibrationof the robot successfully improves the overall performance, we
oursystem,wheretheoracleservesasanapproximationofthe find the improvement mainly appears in the hard tasks. As
upper bound and the frontier serves as a rule-based baseline illustrated, the lower bound of time spent by the oracle and
withouttrainingorintelligentalgorithms.Besides,wealsoset the frontier are almost the same, and the lower quartiles of
up a control group that has no robot. all three settings do not have significant differences. This fits
our insights that in household tasks, the robot plays its role
B. Experimental setup
when the task is at a specified level of difficulty. Too easy
We sample 3 mobile manipulation tasks in each scene for tasks in which the targets are very close to the robot or the
theexperiments,intotal30tasksinall10scenes.Weinvite18 humancannotexaminethecapabilityofrobots.AssuggestedFig. 10: A representative task for demonstrating the effect of robot assistants. We take one case in each robot setting and
present the top-down maps and human observations at several keyframes. The colored lines in the top-down maps denote the
trajectories of the robot and the human.
by the results, our benchmark covers tasks of various levels the remote and put it on the bed. The global maps show
of difficulty and thus can be used as a general evaluation the layout of the house and the positions of task-related
platform. entities, where the human starts from the corridor and the
2) Human trust: We present the variation and average of robotisinthelivingroom.Asshownintrajectories,theoracle
human trust over trial times in Figure 9, where the shade robot directly navigates to the target and is the first to report
denotes standard errors. Notably, the oracle always receives its position to the human. With such guidance, humans can
higherhumantrustthanthefrontieragent,whichconfirmsthat quickly succeed with almost no extra exploration. Without
humans are aware of the distinction in robot capabilities and prior knowledge, the frontier agent has to spend some time
are willing to give higher trust to better agents. An interesting exploring and may even fail sometimes. Fortunately, it can
phenomenonisthatsincehumansdonothavethetopviewand still successfully find the target in many cases, resulting in
are not aware of the robot’s trajectory, they tend to measure saving some exploration time for humans. In contrast, when
the capability of robots from the time spent for searching. thereisnorobotassistant,thehumanmayhavetoexplorethe
This results in the vibration of the trust scores, e.g., when the full house and spend a lot of time. The human does not even
task is hard and the initial position of the target object is far find the target when the oracle-guided participant succeeds,
fromtherobot,humanstendtoreducetheirtrustscoresdueto while the frontier-assisted participant gets back to the bed
the less help offered by the robot. Nevertheless, humans also with the remote. The comparison among the three trajectories
consider the accumulated performance of the robot, reflected demonstrates the assistance effect of different levels of robots
in the gradually converging trust scores. Besides, we find the on humans in collaboration tasks. Since the picked case is
trust scores for the oracle show a rising trend along with the of medium difficulty in a small house, all three shown trials
progressoftheexperiment.Wethinkthereasonisthathumans finally succeed in nearly 75 seconds. However, the assistance
havenoticedthattheoraclecanalwaysfindthetargetandraise of robots might be vital in hard tasks. For more cases and
the score, whereas the frontier agent still has a probability of details, please refer to our accompanying video.
failing.
The clear gap between the frontier agent and the oracle
demonstratesthatourexperimentcansuccessfullydiscriminate
VI. SYSTEMEXTENSIONS
the capability of agents and serve as an effective benchmark
for evaluating the power of agents and studying human trust
in robots. Based on the introduced functionalities, our platform is
extensible for customized complex tasks and studies. In this
D. Case study
section, we discuss the extensibility of HumanTHOR, includ-
We present a case study to show the general collaboration ing further support on the multi-robot setting, more complex
process in Figure 10, in which the human is asked to find tasks, and more robot algorithms.Robot 1
(navigation)
Robot 2
(+manipulation)
Task: Find three apples and place 11.. HHuummaann ffiinnddss tthhee ffiirrsstt 22.. HHuummaann rreecceeiivveess aa 33.. WWhhiillee hhuummaann iiss ooppeenniinngg 44.. HHuummaann ttaakkeess bbaacckk tthhee 55.. RRoobboott 22 bbrriinnggss bbaacckk tthhee
them on the side table. aappppllee aanndd ttaakkeess iitt bbaacckk.. mmeessssaaggee ffrroomm rroobboott 11 aanndd tthhee ffrriiddggee,, rroobboott 22 ffiinnddss sseeccoonndd aappppllee ffrroomm tthhee llaasstt aappppllee.. TTaasskk ssuucccceeeeddss..
ddeecciiddeess ttoo cchheecckk tthhee ffrriiddggee.. aannootthheerr aappppllee.. ffrriiddggee..
Fig.11:AdemonstrationofHRCinamulti-robotmulti-targetmobilemanipulationtask,wherethehuman-robotteamisasked
to collect and place 3 apples. The complete task process is illustrated in the human’s first perspective. Since the small robot
can not interact with the fridge, it reports to the human and lets the human check the fridge. In contrast, the tall robot with
manipulation capability can directly take the apple back.
Fig.14:WepresentanexampleofemployingtheSAVNmodel
to search for an apple in our HumanTHOR system. Success
in simple cases demonstrates the practicability of leveraging
deep learning models in our system, while failure in more
Fig. 12: The HumanTHOR system supports various types of cases suggests room for further study in robot algorithms.
robots and is extensible for further customization. Current
robots are from the ProcTHOR dataset.
whereas the small robot can only navigate. Both of them can
communicate with the human and contribute to the task in
different ways. The HumanTHOR system is also extensible
for more types of robots, where the robots can have differ-
ent appearance, size, performance, and capabilities. Currently
available robots are illustrated in Figure 12.
B. More complex tasks
As introduced in Section III-B, the HumanTHOR simulator
exposes APIs for configuring scenes and objects on the fly,
enabling multi-stage tasks such as room rearrangement [2].
In this task, the human and robots can first tour the clean
environment and record the object arrangement, as shown in
Figure 13 (a). Once they are ready, some object arrangement
Fig. 13: Comparison between the original scene and the shuf- will be shuffled as in Figure 13 (b), requiring rearrangement.
fledonewithinthreeanglesofviewintheroomrearrangement Similarly, our system supports tidying up [37], another
task. The moved objects including apple, lettuce, and tomato, rearrangement task where no reference environment is pro-
are marked with gold squares in the original scene and with vided and the agents need to rearrange objects according to
red squares in the shuffled scene. commonsense,makingthecommunicationbetweenthehuman
and robots more important. For room rearrangement and
tidying up tasks, we provide more details and corresponding
A. Mutli-robot setting demo videos with an oracle agent on our website.
HumanTHOR supports incorporating multiple robots with
C. More robot algorithms
the same or different types simultaneously in one scene. To
furtherexhibitthisfeature,wedemonstrateamulti-robotmulti- As introduced in Section III-B, the HumanTHOR simu-
target mobile manipulation task as illustrated in Figure 11, in lator provides ego-centric RGB observation and depth map.
which the human has two heterogeneous robots as assistants. Therefore, vision-based navigation algorithms are also well
In this task, the taller robot has the manipulation capability, supported. For demonstration, we reproduce a representativelearning-based visual navigation model, SAVN [48]. As illus- tion. Advancesinneuralinformationprocessingsystems,
trated in Figure 14, the existing visual navigation models are 32, 2019.
applicable in the HumanTHOR system. However, searching [5] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej
for objects in the whole house is still challenging and there Halber, Matthias Niebner, Manolis Savva, Shuran Song,
exists room for further study. Andy Zeng, and Yinda Zhang. Matterport3d: Learning
fromrgb-ddatainindoorenvironments. In2017Interna-
VII. CONCLUSIONS tional Conference on 3D Vision (3DV), pages 667–676.
IEEE, 2017.
In this paper, we introduce HumanTHOR, an extended
[6] Yu-Wei Chao, Chris Paxton, Yu Xiang, Wei Yang,
embodied simulator with an everyday task benchmark for
Balakumar Sundaralingam, Tao Chen, Adithyavairavan
studying human-robot collaboration in a shared workspace.
Murali, Maya Cakmak, and Dieter Fox. Handoversim:
Compared to existing environments and benchmarks, the Hu-
A simulation framework and benchmark for human-to-
manTHORnotonlyprovidesrealisticsimulationandahuman-
robotobjecthandovers.In2022InternationalConference
in-the-loop collaboration platform but is also scalable and
on Robotics and Automation (ICRA), pages 6941–6947.
flexible to support various collaborative robot studies such
IEEE, 2022.
as human trust, emergent behavior, etc. Our preliminary user
[7] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan
studyresultssubstantiatethehelpfulnessandimportanceofthe
Lee, Devi Parikh, and Dhruv Batra. Embodied question
robot assistant in human-robot collaboration by showing the
answering. In Proceedings of the IEEE conference on
improvement in overall quantitative performance and the gap
computer vision and pattern recognition, pages 1–10,
indicates the room for further study on algorithms. Besides,
2018.
thesubjectiveresultsvalidatetheavailabilityofHumanTHOR
[8] Francesco De Pace, Gal Gorjup, Huidong Bai, Andrea
in conducting human-related studies such as human trust.
Sanna, Minas Liarokapis, and Mark Billinghurst. Lever-
The quantitative and qualitative results have exhibited the
agingenhancedvirtualrealitymethodsandenvironments
effectiveness of our system in serving as a benchmark and
for efficient, intuitive, and immersive teleoperation of
experimental platform for HRC.
robots. In 2021 IEEE International Conference on
Beyond the conducted experiments, the HumanTHOR sys-
Robotics and Automation (ICRA), pages 12967–12973.
tem also supports advanced features such as multi-robot
IEEE, 2021.
settings and more complicated tasks such as collaborative
[9] Matt Deitke, Winson Han, Alvaro Herrasti, Anirud-
rearrangement.Customizedrobotalgorithmssuchaslearning-
dha Kembhavi, Eric Kolve, Roozbeh Mottaghi, Jordi
based visual navigation models are also well supported. In
Salvador, Dustin Schwenk, Eli VanderBilt, Matthew
general, our HumanTHOR establishes a foundation and em-
Wallingford, et al. Robothor: An open simulation-to-real
bodied test field for various future work in the human-robot
embodied ai platform. In Proceedings of the IEEE/CVF
collaboration domain.
conference on computer vision and pattern recognition,
pages 3164–3174, 2020.
REFERENCES
[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
[1] PeterAnderson,QiWu,DamienTeney,JakeBruce,Mark Kristina Toutanova. Bert: Pre-training of deep bidirec-
Johnson,NikoSu¨nderhauf,IanReid,StephenGould,and tional transformers for language understanding. arXiv
AntonVanDenHengel. Vision-and-languagenavigation: preprint arXiv:1810.04805, 2018.
Interpreting visually-grounded navigation instructions in [11] Morteza Dianatfar, Jyrki Latokartano, and Minna Lanz.
real environments. In Proceedings of the IEEE confer- Review on existing vr/ar solutions in human–robot col-
ence on computer vision and pattern recognition, pages laboration. Procedia CIRP, 97:407–411, 2021.
3674–3683, 2018. [12] Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli Van-
[2] DhruvBatra,AngelXChang,SoniaChernova,AndrewJ derBilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi,
Davison, Jia Deng, Vladlen Koltun, Sergey Levine, Ji- and Roozbeh Mottaghi. Manipulathor: A framework
tendra Malik, Igor Mordatch, Roozbeh Mottaghi, et al. for visual object manipulation. In Proceedings of the
Rearrangement: A challenge for embodied ai. arXiv IEEE/CVF conference on computer vision and pattern
preprint arXiv:2011.01975, 2020. recognition, pages 4497–4506, 2021.
[3] EBroadbent,CJayawardena,NKerse,RQStafford,and [13] Davide Ferrari, Federico Benzi, and Cristian Secchi.
BA MacDonald. Human-robot interaction research to Bidirectional communication control for human-robot
improve quality of life in elder care: an approach and collaboration. In 2022 International Conference on
issues. In Proceedings of the 12th AAAI Conference on Robotics and Automation (ICRA), pages 7430–7436.
Human-Robot Interaction in Elder Care, pages 13–19, IEEE, 2022.
2011. [14] Haoyuan Fu, Wenqiang Xu, Ruolin Ye, Han Xue, Zhen-
[4] Micah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, jun Yu, Tutian Tang, Yutong Li, Wenxin Du, Jieyi
Sanjit Seshia, Pieter Abbeel, and Anca Dragan. On the Zhang, and Cewu Lu. Demonstrating RFUniverse: A
utility of learning about humans for human-ai coordina- Multiphysics Simulation Platform for Embodied AI. InProceedings of Robotics: Science and Systems, Daegu, [25] Xiaohui Li, Dongfang Fan, Yi Deng, Yu Lei, and Owen
Republic of Korea, July 2023. doi: 10.15607/RSS.2023. Omalley.Sensorfusion-basedvirtualrealityforenhanced
XIX.087. physical training. Robotic Intelligence and Automation,
[15] Haoyuan Fu, Wenqiang Xu, Ruolin Ye, Han Xue, Zhen- 44(1):48–67, 2024.
junYu,TutianTang,YutongLi,WenxinDu,JieyiZhang, [26] Xinghang Li, Di Guo, Huaping Liu, and Fuchun Sun.
andCewuLu. Demonstratingrfuniverse:Amultiphysics Embodied semantic scene graph generation. In Confer-
simulation platform for embodied ai. In Robotics: Sci- enceonRobotLearning,pages1585–1594.PMLR,2022.
ence and Systems, 2023. [27] Xinzhu Liu, Di Guo, Huaping Liu, and Fuchun Sun.
[16] Qiaozi Gao, Govind Thattai, Xiaofeng Gao, Suhaila Multi-agent embodied visual semantic navigation with
Shakiah, Shreyas Pansare, Vasu Sharma, Gaurav scene prior knowledge. IEEE Robotics and Automation
Sukhatme, Hangjie Shi, Bofei Yang, Desheng Zheng, Letters, 7(2):3154–3161, 2022.
etal. Alexaarena:Auser-centricinteractiveplatformfor [28] Xinzhu Liu, Xinghang Li, Di Guo, Sinan Tan, Huap-
embodied ai. arXiv preprint arXiv:2303.01586, 2023. ing Liu, and Fuchun Sun. Embodied multi-agent task
[17] Xiaofeng Gao, Ran Gong, Tianmin Shu, Xu Xie, Shu planning from ambiguous instruction. Proceedings of
Wang, and Song-Chun Zhu. Vrkitchen: an interactive robotics: science and systems, New York City, NY, USA,
3d virtual environment for task-oriented learning. arXiv pages 1–14, 2022.
preprint arXiv:1903.05757, 2019. [29] Yonghao Long, Wang Wei, Tao Huang, Yuehao Wang,
[18] Xiaofeng Gao, Qiaozi Gao, Ran Gong, Kaixiang Lin, and Qi Dou. Human-in-the-loop embodied intelligence
Govind Thattai, and Gaurav S Sukhatme. Dialfred: withinteractivesimulationenvironmentforsurgicalrobot
Dialogue-enabled agents for embodied instruction fol- learning. IEEE Robotics and Automation Letters, 2023.
lowing. IEEE Robotics and Automation Letters, 7(4): [30] Simone Maccio`, Alessandro Carf`ı, and Fulvio Mastro-
10049–10056, 2022. giovanni. Mixed reality as communication medium
[19] Kirsten E Kaplan, Kirk A Nichols, and Allison M Oka- for human-robot collaboration. In 2022 International
mura. Towardhuman-robotcollaborationinsurgery:Per- Conference on Robotics and Automation (ICRA), pages
formance assessment of human and robotic agents in an 2796–2802. IEEE, 2022.
inclusion segmentation task. In 2016 IEEE International [31] Robin R Murphy. Human-robot interaction in rescue
Conference on Robotics and Automation (ICRA), pages robotics. IEEE Transactions on Systems, Man, and
723–729. IEEE, 2016. Cybernetics, Part C (Applications and Reviews), 34(2):
[20] Sebastian Kohn, Andreas Blank, David Puljiz, Lothar 138–153, 2004.
Zenkel, Oswald Bieber, Bjorn Hein, and Jorg Franke. [32] Eley Ng, Ziang Liu, and Monroe Kennedy. It takes two:
Towards a real-time environment reconstruction for vr- Learning to plan for human-robot cooperative carrying.
basedteleoperationthroughmodelsegmentation.In2018 In 2023 IEEE International Conference on Robotics and
IEEE/RSJInternationalConferenceonIntelligentRobots Automation (ICRA), pages 7526–7532. IEEE, 2023.
and Systems (IROS), pages 1–9. IEEE, 2018. [33] Aishwarya Padmakumar, Jesse Thomason, Ayush Shri-
[21] EricKolve,RoozbehMottaghi,WinsonHan,EliVander- vastava, Patrick Lange, Anjali Narayan-Chen, Spandana
Bilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Gella, Robinson Piramuthu, Gokhan Tur, and Dilek
Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An Hakkani-Tur. Teach: Task-driven embodied agents that
interactive 3d environment for visual ai. arXiv preprint chat.InProceedingsoftheAAAIConferenceonArtificial
arXiv:1712.05474, 2017. Intelligence, volume 36, pages 2017–2025, 2022.
[22] Shitij Kumar, Celal Savur, and Ferat Sahin. Survey of [34] Kengkij Promsutipong, Jose V Salazar Luces, Ankit A
human–robot collaboration in industrial settings: Aware- Ravankar, Seyed Amir Tafrishi, and Yasuhisa Hirata.
ness,intelligence,andcompliance.IEEETransactionson Immersive virtual walking system using an avatar robot.
Systems,Man,andCybernetics:Systems,51(1):280–297, In 2022 International Conference on Robotics and Au-
2020. tomation (ICRA), pages 9325–9331. IEEE, 2022.
[23] Kang Woo Lee, Hyoung-Rock Kim, Wan Chul Yoon, [35] Shuwen Qiu, Hangxin Liu, Zeyu Zhang, Yixin Zhu, and
Young-Sik Yoon, and Dong-Soo Kwon. Designing a Song-Chun Zhu. Human-robot interaction in a shared
human-robot interaction framework for home service augmented reality workspace. In 2020 IEEE/RSJ Inter-
robot. In ROMAN 2005. IEEE International Workshop national Conference on Intelligent Robots and Systems
onRobotandHumanInteractiveCommunication,2005., (IROS), pages 11413–11418. IEEE, 2020.
pages 286–293. IEEE, 2005. [36] Eric Rosen, David Whitney, Michael Fishman, Daniel
[24] Chengshu Li, Fei Xia, Roberto Mart´ın-Mart´ın, Michael Ullman, and Stefanie Tellex. Mixed reality as a bidi-
Lingelbach,SanjanaSrivastava,BokuiShen,KentElliott rectional communication interface for human-robot in-
Vainio, Cem Gokmen, Gokul Dharan, Tanish Jain, et al. teraction. In 2020 IEEE/RSJ International Conference
igibson 2.0: Object-centric simulation for robot learning on Intelligent Robots and Systems (IROS), pages 11431–
of everyday household tasks. In 5th Annual Conference 11438. IEEE, 2020.
on Robot Learning, 2022. [37] Gabriel Sarch, Zhaoyuan Fang, Adam W Harley, PaulSchydlo, Michael J Tarr, Saurabh Gupta, and Katerina Roozbeh Mottaghi. Visual room rearrangement. In
Fragkiadaki. Tidee:Tidyingupnovelroomsusingvisuo- Proceedings of the IEEE/CVF conference on computer
semantic commonsense priors. In European conference vision and pattern recognition, pages 5922–5931, 2021.
on computer vision, pages 480–496. Springer, 2022. [48] Mitchell Wortsman, Kiana Ehsani, Mohammad Raste-
[38] ManolisSavva,AbhishekKadian,OleksandrMaksymets, gari, Ali Farhadi, and Roozbeh Mottaghi. Learning
Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, to learn how to learn: Self-adaptive visual navigation
Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: using meta-learning. In Proceedings of the IEEE/CVF
A platform for embodied ai research. In Proceedings conference on computer vision and pattern recognition,
of the IEEE/CVF international conference on computer pages 6750–6759, 2019.
vision, pages 9339–9347, 2019. [49] Qi Wu, Cheng-Ju Wu, Yixin Zhu, and Jungseock Joo.
[39] Bokui Shen, Fei Xia, Chengshu Li, Roberto Mart´ın- Communicative learning with natural gestures for em-
Mart´ın, Linxi Fan, Guanzhi Wang, Claudia Pe´rez- bodied navigation agents with human-in-the-scene. In
D’Arpino, Shyamal Buch, Sanjana Srivastava, Lyne 2021 IEEE/RSJ International Conference on Intelligent
Tchapmi, et al. igibson 1.0: A simulation environment Robots and Systems (IROS), pages 4095–4102. IEEE,
for interactive tasks in large realistic scenes. In 2021 2021.
IEEE/RSJInternationalConferenceonIntelligentRobots [50] Qianyi Zhang, Zhengxi Hu, Yinuo Song, Jiayi Pei, and
and Systems (IROS), pages 7520–7527. IEEE, 2021. Jingtai Liu. The human gaze helps robots run bravely
[40] Mohit Shridhar, Jesse Thomason, Daniel Gordon, and efficiently in crowds. In 2023 IEEE International
Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Conference on Robotics and Automation (ICRA), pages
Zettlemoyer, and Dieter Fox. Alfred: A benchmark for 7540–7546. IEEE, 2023.
interpreting grounded instructions for everyday tasks. In
Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pages 10740–10749,
2020.
[41] Steven Silva, Nervo Verdezoto, Dennys Paillacho,
SamuelMillan-Norman,andJuanDavidHerna´ndez. On-
line social robot navigation in indoor, large and crowded
environments. In 2023 IEEE International Conference
on Robotics and Automation (ICRA), pages 9749–9756.
IEEE, 2023.
[42] Patrick Stotko, Stefan Krumpen, Max Schwarz, Chris-
tian Lenz, Sven Behnke, Reinhard Klein, and Michael
Weinmann. A vr system for immersive teleoperation
and live exploration with a mobile robot. In 2019
IEEE/RSJInternationalConferenceonIntelligentRobots
and Systems (IROS), pages 3630–3637. IEEE, 2019.
[43] Andrew Szot, Alexander Clegg, Eric Undersander,
Erik Wijmans, Yili Zhao, John Turner, Noah Maestre,
Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr
Maksymets, et al. Habitat 2.0: Training home assistants
to rearrange their habitat. Advances in Neural Informa-
tion Processing Systems, 34:251–266, 2021.
[44] JuanPVasconez,GeorgeAKantor,andFernandoAAuat
Cheein. Human–robot interaction in agriculture: A sur-
veyandcurrentchallenges. Biosystemsengineering,179:
35–48, 2019.
[45] Valeria Villani, Fabio Pini, Francesco Leali, and Cristian
Secchi. Survey on human–robot collaboration in indus-
trialsettings:Safety,intuitiveinterfacesandapplications.
Mechatronics, 55:248–266, 2018.
[46] Yanming Wan, Jiayuan Mao, and Joshua B Tenenbaum.
Handmethat: Human-robot communication in physical
and social environments. In Thirty-sixth Conference
on Neural Information Processing Systems Datasets and
Benchmarks Track, 2022.
[47] Luca Weihs, Matt Deitke, Aniruddha Kembhavi, and