Merlin: A Vision Language Foundation Model for 3D
Computed Tomography
LouisBlankemeier1,2,3† JosephPaulCohen2,‡ AshwinKumar2,3 DaveVanVeen1,2,3
SyedJamalSafdarGardezi4 MagdaliniPaschali2,3 ZhihongChen2,3 Jean-BenoitDelbrouck2,3
EduardoReis2,3 CesarTruyts5 ChristianBluethgen2,6 MalteEngmannKjeldskovJensen2,3
SophieOstmeier2,3 MayaVarma2,3,7 JeyaMariaJoseValanarasu2,3,7 ZhongnanFang3 ZepengHuo8
ZaidNabulsi‡ DiegoArdila‡ Wei-HungWeng‡ EdsonAmaroJunior5 NeeraAhuja9
JasonFries7,8 NigamH.Shah3,8 AndrewJohnston3 RobertD.Boutin3 AndrewWentland4
CurtisP.Langlotz2,3 JasonHom9 SergiosGatidis5 AkshayS.Chaudhari2,3,8
1DepartmentofElectricalEngineering,StanfordUniversity
2StanfordCenterforArtificialIntelligenceinMedicineandImaging,StanfordUniversity
3DepartmentofRadiology,StanfordUniversity
4DepartmentofRadiology,UniversityofWisconsin-Madison
5DepartmentofRadiology,HospitalIsraelitaAlbertEinstein
6DepartmentofRadiology,UniversityHospitalZurich
7DepartmentofComputerScience,StanfordUniversity
8DepartmentofBiomedicalDataScience,StanfordUniversity
9DepartmentofMedicine,StanfordUniversity
†Correspondingauthor: louis.blankemeier@stanford.edu
Abstract
Over85millioncomputedtomography(CT)scansareperformedannuallyintheUS,ofwhich
approximatelyonequarterfocusontheabdomen. Giventhecurrentshortageofbothgeneral
andspecializedradiologists,thereisalargeimpetustouseartificialintelligencetoalleviatethe
burden of interpreting these complex imaging studies while simultaneously using the images
toextractnovelphysiologicalinsights. Priorstate-of-the-artapproachesforautomatedmedical
imageinterpretationleveragevisionlanguagemodels(VLMs). However,currentmedicalVLMs
aregenerallylimitedto2Dimagesandshortreports,anddonotleverageelectronichealthrecord
(EHR)dataforsupervision. ToovercometheseshortcomingsforabdominalCTinterpretation,
weintroduceMerlin-a3DVLMthatleveragesbothstructuredEHRandunstructuredradiology
reportsforsupervisionwithoutrequiringadditionalmanualannotations. WetrainMerlinusinga
high-qualityclinicaldatasetofpairedCTscans(6+millionimagesfrom15,331CTs),EHRdiag-
nosiscodes(1.8+millioncodes),andradiologyreports(6+milliontokens). Wecomprehensively
evaluateMerlinon6tasktypesand752individualtasks. Thenon-adapted(off-the-shelf)tasks
includezero-shotfindingsclassification(31findings),phenotypeclassification(692phenotypes),
andzero-shotcross-modalretrieval(imagetofindingsandimagetoimpressions),whilemodel
adaptedtasksinclude5-yearchronicdiseaseprediction(6diseases),radiologyreportgeneration,
and 3D semantic segmentation (20 organs). We perform internal validation on a test set of
5,137CTs,andexternalvalidationon7,000clinicalCTsandontwopublicCTdatasets(VerSe,
TotalSegmentator). Beyondtheseclinically-relevantevaluations,weassesstheefficacyofvarious
networkarchitecturesandtrainingstrategiestodepictthatMerlinhasfavorableperformanceto
existingtask-specificbaselines. Wederivedatascalinglawstoempiricallyassesstrainingdata
needsforrequisitedownstreamtaskperformance. Furthermore,unlikeconventionalVLMsthat
requirehundredsofGPUsfortraining,weperformalltrainingonasingleGPU.Thiscomputa-
tionallyefficientdesigncanhelpdemocratizefoundationmodeltraining,especiallyforhealth
systems with compute constraints. We plan to release our trained models, code, and dataset,
pendingmanualremovalofallprotectedhealthinformation.
1
4202
nuJ
01
]VC.sc[
1v21560.6042:viXra1 Main
Over85millioncomputedtomography(CT)scansareperformedperyearintheUS[1,2,3],withapproximately
300millionCTsperformedglobally[4]. Amongstthesestudies, CTsoftheabdomenrepresentapproximately
onefourthofallexaminationsperformed[5]. TheseabdominalCTscanscanconsistofupwardsof300slicesper
serieswithnumerousanatomicalstructuresthatneedtobeexamined,leadingtotime-consuminginterpretation,
oftenrequiring20minutesperexam[6]. Moreover,recentliteraturesuggeststhatabdominalCTscanscontain
biomarkersofearlydiseasesthatroutinelygounreported[7,8,9,10,11,12,13,14]. Withthecurrentvolume
ofmedicalimaginganda6%annual[15]increaseinmedicalimagingutilization,theburdenonradiologistsis
significant. Nonetheless,overthepastdecades,thenumberofradiologyresidencypositionsintheUShasremained
relativelyconstantat1011in2006and1113in2020[16]. Thisisdespite1,800+openradiologistpositionson
theAmericanCollegeofRadiologyjobboard[17]fromapoolofroughly32,000totalactiveradiologistsinthe
US[18]*. Withthesupplyofnewradiologistsremainingrelativelyconstantandaneverincreasingutilizationof
medicalimaging,theradiologistshortageisprojectedtoexpandtoover19,000by2036[19,20,21,22].
Machinelearning(ML)hasshownpromiseinvariousmedicalimagingtasks[23,24,25,26],inspiringoptimism
about its potential to offset the increasing burden faced by radiologists [19, 27]. As of May 2024, of the 882
FDA-clearedML-enableddevices,671(76%)relatetoradiology[28]. Despitethislargeprevalence,thecurrent
status-quooftrainingMLalgorithmsentailsusingunimodal(imaging-only)algorithmsandretrainingfornew
tasksfromscratchusingsupervisedMLwithmanuallycuratedlabels,evenifitmaybeforthesamemodalityor
anatomy. Inthemedicalimagingscenario,generatingsuchlabelsrequiresexpensiveclinicalexperttime,limiting
thedevelopmentofcapableAImodelsforawidearrayoftasks.
Recentyearshavewitnessedremarkableadvancementsinvision-languagemodels(VLMs),adata-efficientalterna-
tivetosupervisedtraining. Thecontrastivelanguage-imagepretraining(CLIP)technique [29]demonstratedthe
efficacyofaligningtextandvisualrepresentationsinasharedembeddingspaceasameansofsupervisingvision
modelswithnaturallanguage. Thisparadigmenablesleveraginginternet-scaleimagesandcaptions,demonstrating
impressiveimageunderstandingcapabilitiesinoff-the-shelf(zeroshot)settingsorinsettingsthatusesubsequent
adaptation(fewshotlearning)[30]foralargenumberofdownstreamtasks. Suchmodels,trainedonlarge-scale
multi-modalpretrainingdatasetsandenablingadaptationformultipledownstreamtasks,arecommonlyreferredto
asfoundationmodels.
CLIP-basedmethodscouldbereadilyappliedintheclinicalsettingbytrainingwithmedicalimagesandcorre-
spondingradiologyreportsthataregeneratedroutinelyduringclinicalcare, addingnoadditionaldatalabeling
cost. Manyinstitutions,includingours,de-identifythisdatatomaintainpatientprivacywhileprovidingaunique
settingforresearchonthishigh-qualityhuman-annotateddata. Recentethicalviewpointsdescribehowlarge-scale
already-acquiredclinicaldatacouldberesponsiblyusedforsecondarypurposessuchastrainingMLalgorithms, to
ensurethatthedataareusedforthebenefitoffuturepatientsandsociety[31]. Followingsuchframeworks,VLMs
likeMedCLIP[32],BiomedCLIP[33],LLaVA-Med[34],Med-Flamingo[35],Med-PaLMM[36],RadFM[37],
XrayGPT[38],ELIXR[39],RoentGen[40,41],CheXagent[42],andMAIRA-1[43]demonstratethepotentialof
VLMsappliedwithintheradiologydomain.
DespitetheburgeoningpopularityofVLMsforradiology,mostexistingmodelsfocuson2Dmodalitiessuchas
radiographs,notwithstandingthatmostmedicalimagingstudiesare3Dinnature†. Manyapproachesextend2D
modelsto3Dbyaggregatingpredictionsslice-by-sliceorinchunksofslicesacrossthe2Dimagestacksthatmake
upthe3Dvolume[44,45]-aninefficienttechniquetoparsethefull3Dimagingvolume. Unlikeanalysisofvideo,
wheresuccessiveframeshavehighcorrelation,thereislimited3Dcorrelationinvolumetricanatomicalstructures
thatrapidlychangeinalldimensions. Thisscenarioiswell-suitedfor3Dmodelingwherefeaturesspanningall3
dimensionsaresynthesizedtogenerateinsights. Moreover,existingmethodsdonotleveragesupervisionfromthe
multipledatatypes,includingEHRdiagnosiscodesandradiologyreports,availableinclinicalsettings.
Thepotentialclinicalimpactof3Dmedicalimagingmodelsmaybesignificant. 3DmedicalVLMscouldassist
radiologists by flagging missed findings [21], accelerating image interpretation workflows, and serving as AI
assistantsthatdraftradiologyreports[46,27]. TheopportunityisparticularlysubstantialforabdominalCTexams,
whicharethemostcommonlyutilized3Dexamination[5]andrequiresignificanttimeforinterpretationduetothe
numberofanatomicalstructuresthatneedtobeexamined[6].
‡WorknotrelatedtopositionatAmazonorGoogle.
*NumberofjobopeningsontheACRjobboardbasedonaccessingthesiteonMay26,2024.
†Wenotethatintheclinicalcontext,3Dmedicalimagingisconsideredasasubsetofcross-sectionalimagingwherethepixelsareisotropic.
Inthispaper,weusetheterm3Dtoalignwiththeterminologyusedbythemachinelearningcommunitydescribingthesimultaneousprocessing
ofpixelsoriginatingfrom3spatialdimensions
2Nonetheless, adapting existing methods for training VLMs to 3D medical imaging presents a challenge as a
single volumetric image can comprise upwards of 300 individual 2D images, and the corresponding radiology
reports can exceed 300 words [6]. Training models on these large data samples can also require significant
computationalresources,whichareoftennotavailableinacademicinstitutionsorwithinhospitalsystemswherethe
dataresides[47]. Furthermore,clinically-relevantevaluationsforbenchmarking3DVLMsacrossasuiteoftasksis
stilllacking. Evenintheclinicallargelanguagemodel(LLM)space,whereprogressisrapid,popularbenchmarks
basedonmedicallicensingexamsgarnercriticismfornotreflectingreal-worldclinicalusecases[48,49]. Dueto
thesetrainingandevaluationchallenges,thereexistsadearthofmethodsfortrainingandevaluating3Dmedical
imagingVLMsthatcanbeadaptedforawiderangeofdownstreamtasks.
Inthispaper,wefocusondevelopingandevaluatingfoundationVLMsfor3DabdominalCTscanstoenhance
imageunderstandingacrossavarietyoftasks. Ourworkprovidesthefollowingcontributions:
1. We develop a training strategy and foundation model called Merlin‡ that leverages the structured and
unstructureddatawithinhospitalstotrainanabdominalCTvisualmodel. Usingthistrainingstrategy,we
trainanextremelycompute-efficientvisionmodelonasingleGPUusingpairedCTs(6,387,231images
from15,331CTs), EHRdiagnosiscodes(1,839,559codes), andradiologyreports(6,036,645tokens).
Merlinallowsprocessingtheentire3DvoxeldatainaCTimageatonce.
2. WeevaluateMerlinonacomprehensivesetof6tasktypesand752individualtasks. Thenon-adapted
(off-the-shelftasks)includezero-shotfindingsclassification(31findings),phenotypeclassification(692
phenotypes), and zero-shot cross-modal retrieval (image to findings and image to impressions), while
modeladaptedtasksinclude5-yeardiseaseprediction(6diseases),radiologyreportgeneration,and3D
semanticsegmentation(20organs).
3. Weperforminternalvalidationon5,137CTsaswellasexternalvalidationon7,000CTsfromanother
institution,andontwopubliclyavailabledatasetsfocusedonabdominalCT(VerSe[50]andTotalSeg-
mentator[51]). WedemonstratethatoursingularMerlinmodeloutperformscarefullychosentask-specific
baselinesonoursuiteofbenchmarkingtasks.
4. WederivedatascalinglawsforMerlin, providingguidanceaboutthedatarequirementsforachieving
specificlevelsofperformance. Wealsopresentcomprehensiveablationstudiesdemonstratingtheimpact
ofvarioustrainingstrategiesandtheroleofthedifferentclinicaldatatypesonmodelperformance.
5. We plan to release our trained models, code, and dataset, pending manual review and removal of all
personalhealthinformation.
2 Results
Wepresentresultsacross6evaluationtasktypescomprising752individualtasks,thatcanbeperformedof-the-shelf
withoutadaptation(Figure1b-Figure1d),orwithadditionaladaptation(Figure1e-Figure1g). Thenon-adapted
tasksthatweevaluateMerlinonincludezero-shotclassification(31classificationtasks;Figure1b),phenotype
classification(692classificationtasks;Figure1c),andzero-shotcross-modalretrieval(imagetofindingsandimage
to impressions; Figure 1d). The adapted tasks that we evaluate Merlin on include 5-year disease prediction (6
classificationtasks;Figure1e),radiologyreportgeneration(Figure1f),and3Dsegmentation(20organs;Figure1g).
2.1 Zero-shotFindingsClassification
Zero-shotfindingsclassificationassessesMerlin’sabilitytoclassifythepresenceofcommonimagingfindings
based on text prompts that a user can develop, which are likely distinct from prompts seen during training. In
Figure2bweevaluateMerlinzero-shotclassificationacross30abdominalCTfindingsonourinternalandexternal
clinicaldatasets. MerlinachievesanaverageF1scoreof0.741(95%CI[0.727-0.755])ontheinternalvalidation
datasetandanaverageF1scoreof0.647(95%CI[0.607-0.678])ontheexternalvalidationdataset,significantly
outperformingOpenCLIP[53]andBioMedCLIP[54](p<0.001)inbothsettings(Figure2b). Qualitatively,we
observeinFigure2cthatMerlin’sexternalperformanceremainshighondiseaseswithcoarse-grainedorsalient
features,e.g. pleuraleffusion,splenomegaly,ascites,surgicallyabsentgallbladder,prostatomegaly,anasarca,and
abdominalaorticaneurysm. Performanceexpectedlydecreasesonmorechallengingfindingsthatrequiremore
subtleandfine-grainedfeatures,e.g. appendicitis,metastaticdisease,lymphadenopathy,andfreeair. Wealsoplot
Merlinperformancewithoutradiologyreportsplitting(W/OsplittinginFigure2c). Radiologyreportsplittingrefers
‡Namedafterthelegendaryfiguresaidtobeabletoperceiveeverythinginthepresent,ourmodelMerlinsimultaneouslyprocessesthe
entiretyof3DinformationinaCTvolume.
3Merlin Training Strategy Non-Adapted Tasks Adapted Tasks
a b e
Diagnostic Code Supervision Zero-shot Findings Classification 5-Year Disease Prediction
Renal cyst ✓ present
Image 317.11
Encoder Alcoholic No renal X
fatty liver cyst
CT Scan Phenotypes
c f
Report Supervision Phenotype Classification Radiology Report Generation
Lower thorax:
L ni ov re mr a:
l
a nt o❆e rl me ac lt .a s Gi as l.
l
bL li av de dr e:
r :
Text normal.
Encoder Stomach Injury
Hepatic
steatosis d g Text
CT Reports Embeddings Zero-shot Cross-Modal Retrieval 3D Semantic segmentation
Lower thorax: left ❆ ❆
Image pleural effusion.
Encoder Liver: normal.
Visual Gallbladder:
CT Scan Embeddings normal.
Figure1: OverviewofMerlintrainingandevaluation. (a)Merlintrainingstrategy. DiagnosiscodesfromtheEHRareused
aslabelsforMerlintraining,withabinarycrossentropyloss.Radiologyreportsarealsousedfortraining,withanInfoNCE
loss[52].Trainingwithdiagnosiscodesandradiologyreportsiseitherstagedorperformedinamulti-taskmanner.Merlinis
thenevaluatedonnon-adaptedtasksthatcanbeperformedwithoutanyarchitecturalorweightmodifications.Theseinclude(b)
zero-shotfindingsclassification,(c)phenotypeclassification,and(d)zero-shotcross-modalretrieval.AdaptingMerlinenables
ustoperform(e)5-yeardiseaseprediction,(f)radiologyreportgeneration,and(g)3Dsemanticsegmentation.Allerrorbarsare
95%confidenceintervals.
tosplittingtheradiologyreportsintosectionsdescribingdifferentanatomicalstructuresforsubsequentcontrastive
learning(e.g. "liver: normal"and"vasculature: patent"). Withoutradiologyreportsplitting,Merlinachievesan
averageF1scoreof0.656(95%CI[0.640,0.671]),similartotheMerlinexternalevaluationperformance. Ona
separateexternaldataset(VerSe[55]vertebralfracturedetection),Merlinachievesazero-shotF1of0.767(95%CI
[0.623-0.867]). InFigure2d,weestablishquantitativerelationshipstoassesshowexpandingthepretrainingdataset
wouldimprovezero-shotclassification. Thisanalysishelpsdeterminetheextentofpretrainingdatanecessaryfor
obtainingaspecifiedzero-shotclassificationperformance. TheresultingpowerlawisF1=0.458D0.0524,withD
representingthenumberofpairedpretrainingCTimagingandreportdata.
Comparedtotheablations,Merlin(I3Dinitialization[56]where2DImageNetpretrainedweightsarereusedwithin
the3Dmodel,multi-tasklearningwithEHRandradiologyreportsversustraininginstages,andradiologyreport
splitting)performsthebestwithanF1scoreof0.741(95%CI[0.727-0.755])(Figure2e). Reportsplittingand
stagingtrainingacrosstheEHRandradiologyreportsresultsinsecondbestperformance. Learningdirectlyfrom
theradiologyreportswithoutanyEHRsupervision,alongwithradiologyreportsplitting,resultsinthethirdbest
performance. ThesesettingsachieveF1scoresof0.735(95%CI[0.719-0.748])and0.730(95%CI[0.714,0.744]),
respectively. Thelargestimpactonperformanceisdrivenbythechoiceofwhetherornottosplittheradiology
reports. RelativetoMerlin,withoutreportsplitting,theperformancedropsbyanaverageof7.9F1points(p«0.01).
2.2 PhenotypeClassification
ThegoalofthistaskistouseCTscanstodirectlypredictphenotypes[57](basedongroupingsofICDcodes)that
wereassignedtopatientsduringtheirhospitaladmissionthatincludedtheCTscan. Weevaluatetheperformanceof
Merlininpredicting692phenotypesdefinedinPheWAS[57]. WefindthatMerlinreachesamacro-averagearea
underthereceiveroperatingcharacteristiccurve(AUROC)acrossphenotypesof0.812(95%CI,0.808-0.816),and
achievesAUROCsover0.85for258phenotypes(37%ofallphenotypes),andAUROCsover0.9for102phenotypes
(15%ofallphenotypes).
InFigure3a,wegroupthe692phenotypesintogroupsofsimilarphenotypesusingestablishedmethods(exclusion
rangesdefinedinthePheWASdatabase[57])andreportprevalenceandaverageperformancewithinthetop20
mostprevalentgroupsintheinternaltestset. Wefindthatabdominalpainisthemostprevalentphenotypegroup
(68%prevalence),followedbynoninfectivegastrointestinaldisorders(51%prevalence). Measuringaveragemodel
performanceacrossthephenotypeswithineachgroup, wefindthatMerlinperformswellindetectingdiseases
acrossarangeoforgansystems,includingtheliver,kidneys,ureters,andgastrointestinaltract.
4
ssoL
ECB
ssoL
ECNofnIa Disease Presence Disease Absence b
Prompts Prompts
Stable thrombosis No occlusive thrombosis
Partial thrombosis No venous thrombosis
There is thrombosis No thrombosis
Image ❆ ❆ Text ❆ ❆ Text ❆ ❆
Encoder Encoder Encoder
Disease Presence Mean < Disease Absence Mean Negative
Cosine Similarity Cosine Similarity Prediction d
c
Gallbladder
P
AaP
n
tra
c
on
r
pec har yte ica s / SS plp enle oe mn
egB ail li
D
yar ily
a
tD iou
n
Hct
eL
a
pli v ae tor man egd aB lyiliaryTree
Surgically Absent Hepatic
Gallbladder Steatosis
Aortic Valve
Gallstones Calcification
Coronary
ys Renal Calcification
ne Cyst L o
d w
Ki Renal Cardio- er
Hypodensity megaly T h
o
Pleural ra x
Hydro- Effusion
nephrosis
Hiatal Atelec-
Hernia tasis
Submucosal
Edema Metastatic
Disease se
G
a
str oi
ObB sto rw uce tl
i on Anasarca
Diffu
e
nt
e sti Appendicitis Osteopenia
n
al
PeA rs itc oi nte es
u
mF Are ire
P rostatom Ae tg ha el ry osclerT oh sr isombosis dL ey nm opp ah ta h-
y
A Ab
nAdF
eoo
ura
rm
rtc
yici
st nu mare
l
Musculoskeletal
Pelvic Vasculature
Merlin (Internal) W/O Splitting (Internal)
Merlin (External) BioMedCLIP (Internal)
Figure2:Zero-shotfindingsclassification.(a)Depictshowzero-shotclassificationisperformedwheretextembeddingsfrom
diseasepresencepromptsanddiseaseabsencepromptsarecomparetotheimageembedding.(b)Wecomparetheperformance
ofOpenCLIP[53],BioMedCLIP[54],Merlinonaninternaldataset,andMerlinwithoutradiologyreportsplitting.Wefurther
evaluate Merlin on an external clinical dataset and the VerSe [55] external fracture detection dataset. (c) Performance of
BioMedCLIP,Merlin,andMerlinwithoutreportsplittingontheinternaldataset,aswellasMerlinontheexternaldataset,
across30findingsassessedonabdominalCTscans.(d)Merlinzero-shotclassificationperformanceimproveswithincreasing
pretrainingdatasetsize.(e)AnablationstudyacrossvariousaspectsofMerlin’spretrainingstrategy."Rpt."isshorthandfor
"report"andindicatedtrainingwithradiologyreportsonly.Staged(Stg.)referstoperformingweaklysupervisedtrainingwith
EHRinafirsttrainingstageandthentrainingwithradiologyreportsinasecondstage.Thisisincontrasttomulti-tasklearning
(MTL)whereEHRandradiologyreportsareusedfortrainingsimultaneously.
5In Figure 3b we compute data scaling law curves to assess how Merlin performance improves with respect to
training data. Similar to zero-shot classification, we find that increasing training data improves performance.
Ouranalysisalsorevealspowerlawswiththefollowingslopesandintercepts: (AUROC = 0.479D0.0568 and
AUPRC =0.0157x0.239).
InFigure3candFigure3d,wecompareperformanceacross7modelvariations,withdifferentmodelbackbones,
architectures,andtrainingparadigms. Wemakethefollowingobservations: i)modelperformanceconsistently
improvesacrossmodelsize,ii)smallerconvolutionalreceptivefieldsimproveperformance,withsmallerz-dimension
kernelsizesandstridesinthemodelstemperformingbest(Figure3d),iii)ResNetbackbones,within-planeand
out-of-planekernelsizeof3,outperformConvNeXt[58]backbones(in-planekernelsizeof7)thathaveout-of-
planekernelsizesof3(ConvNeXt-B*)or7(ConvNeXt-B)atalllayersofthenetwork(Figure3c). TheSwin
Transformer[59]backboneperformstheworst,withawindowsizeof7andpatchsizeof4.Duetothecomputational
complexityoftheattentionmechanism,thenumberofparametersoftheSwinTransformerislowest(3million).
ThetrendthatweobserveacrossmodelvariationsasmeasuredbyaverageAUROCalignswiththetrenddescribed
byaverageAUPRC.
InFigure3e,weapplythelatentshiftcounterfactualmethod[60]toqualitativelyinvestigatetheimagefeaturesthat
Merlinusestoperformimageclassification. Weexamineaninstanceof“pleuraleffusion”(left)wheretheeffusion
islocalizedtotheleftlungandisreducedinthecounterfactual,indicatingthatMerlinisleveragingthefeatureswe
expect. Wealsoexamine“splenomegaly”classification(right). Weobservethatthesizeofthespleenisreducedin
thecounterfactualrelativetotheoriginalimage,addingcredencetothevalidityofimagingfeaturesthatMerlinuses
forimageclassification.
2.3 Zero-shotCross-ModalRetrieval
Zero-shotcross-modalretrievalevaluatesthemodel’sabilitytomatchaCTimagewiththecorrespondingradiology
reportfindingsorimpressionssectionandviceversa. InFigure4b,weplotthedistributionofradiologyreport
findings and impressions lengths. We find that 21% of findings sections have lengths that exceed 512 tokens,
motivatingourchoiceoftheclinicalLongformer[62]textencoder(OpenCLIP[53]andBioMedCLIP[54]only
allow maximum token lengths of 77 and 256, respectively). We find that Merlin significantly (p << 0.01)
outperformsOpenCLIPandBioMedCLIPonthetaskof3Dretrievalforretrievingthecorrectfindingsoutof64
cases(Image→FindingsinFigure4c). WefindsimilarperformanceforretrievingthecorrectCTscanoutof64,
givenafindingssection(Findings→ImageinFigure4c).
Retrieving the impressions section from radiology reports provides evidence that Merlin generalizes to out-of-
distributiontext,giventhatMerlintrainingusesthefindingssections.Figure4cdemonstratesthatMerlingeneralizes
tothetaskofretrievingthecorrectimpressionssectiongivenascan(Image→Impressions)andretrievingthe
correctscangivenanimpressionssection(Impressions→Image). Ontheexternaltestset,retrievalperformances
decreases (Figure 4c). However, it is important to note that the structure and language used in reports varies
significantlyacrossinstitutions. Nonetheless,theexternalMerlinperformanceremains5-7xbetterthantheexternal
baselines.
Weconductamodelablationstudy(Figure4d)toinvestigatei)theimpactofImageNetinitializationwithI3D
weights,ii)theimpactofeitherstagedtrainingormulti-tasklearningusingtheEHRandradiologyreports,and
iii) the impact of training with full reports or reports that are split across anatomical regions. We find that the
top-performingmodelvariationusesI3Dinitialization,multi-tasklearningwiththeEHRandradiologyreports,and
noreportsplitting. Wehypothesizethatreportsplittingdoesnotimproveperformanceontheretrievaltasksince
retrievalpromptingisperformedwiththefullreportsections. Splittingthereportintosubpartsandadditionally
learning directly from the radiology reports without any EHR supervision results in the second and third best
performance,respectively.
Finally,wefindthatmodelperformanceimproveswithpretrainingdatasetsizebothforimageandfindingsretrieval
andimageandimpressionsretrievalviathedatascalingcurveinFigure4e.
2.4 Multi-Disease5-YearPrediction
Multi-disease5-yearpredictionmeasuresthemodel’sabilitytopredictwhetherapatientwilldevelopachronic
diseasewithin5years,giventhattheyarenotdiagnosedwiththediseasewhentheirCTscanwasacquired. We
fine-tuneMerlintoopportunisticallypredictwhichpatients,whoarehealthyatbaseline,willbediagnosedwith
anyof6chronicdiseases(chronickidneydisease,osteoporosis,cardiovasculardisease,ischemicheartdisease,
hypertension,anddiabetes)intheensuing5yearsbasedontheirCTscan. Onaverageacrossthese6diseases,
6a b
c
Total Phecodes = 692
Upper Quart. Prev. = 8.7%
Lower Quart. Prev. = 0.6%
d e
Pleural Effusion Splenomegaly
Input Counterfactual Difference Input Counterfactual Difference
Smaller receptive field
Figure 3: Phenotype classification. (a) Average AUROC performance for the top 20 phenotype groups listed in order of
prevalence(blackline).(b)DatascalinglawexperimentsthatmeasurehowaverageAUROC(top)andaverageAUPRC(bottom)
acrossthe692phenotypesscaleastheamountofpretrainingdatavaries.(c)AverageAUROC(leftchart)andAUPRC(right
chart)acrossall692phenotypes,thetopquartileof173phenotypes,andthebottonquartileof173phenotypesacrossseveral
baselinemodels.Allbaselinemodelsaretrainedusingthephenotypesinthepretrainingdataset.Thedashedlinesdenoterandom
chanceperformance.NotethatMerlin,whichusesthebestperformingbackboneofResNet152,isfurthertrainedusingradiology
reports.(d)AverageAUROCasafunctionofmodelstemhyper-parameters.Wefindthatasmallerreceptivefieldyieldsbetter
performance. (e)Counterfactualanalysesofpleuraleffusionclassification(left;imagefromTCIA[61])andsplenomegaly
classification(imagefromourinternaltestset).Weannotatethezoomedinimagesbyoutliningthepathologies.Theredlines
borderpathologiesintheoriginalimages.Thebluelinesborderpathologiesinthecounterfactualimages.Counterfactualoutlines
aredrawnovertheoriginalimageswithdottedlinesandtheoriginalimageoutlinesarealsodrawnoverthecounterfactual
imageswithdottedlines. Thisallowscomparingthesizeandshapeofthepathologiesbetweentheoriginalimagesandthe
counterfactuals,indicatingthatMerlinisindeedusingappropriatefeaturesforimageclassification.
7a b
Report Findings Pool
Lower thorax: left Lower thorax:
pleural effusion. normal. Liver:
Liver: normal… hepatis steatosis…
Image ❆ ❆ Text ❆ ❆ Text ❆ ❆❆
Encoder Encoder Encoder
21% findings > 512
Cosine Similarity
c In-Distribution Text Out-of-Distribution Text
Internal test set:
5,137 CTs
External test set:
7,000 CTs
d e
Image ↔ Findings Retrieval Image ↔ Impressions Retrieval
Figure4: Zero-shotcross-modalretrieval. (a)Schematicdemonstratinghowweperformretrieval. Wecomputethecosine
similaritybetweenMerlinreportembeddingsandCTembeddings,enablingustorankCTandreportpairsinorderofsimilarity.
(b)Adistributionofthefindingssectionandimpressionssectionlengthsshowsthat21%offindingshavesequencelengths
greaterthan512tokens. (c)Top-1recalloutofpoolsof64findingssections(left), whichisconsideredanin-distribution
evaluationasMerlinistrainedusingfindingssections.Wealsoreporttop-1recallonout-of-distributionimpressionssections
(right).(d)AnablationstudythatexaminestheimpactofusingI3DImageNetinitialization,multi-tasklearning(MTL)versus
stagedtraining(Stg.) withEHRandreportsversustrainingwithreportsonly(Rpt.),andsplittingtheradiologyreporttext
intoanatomicalsections. (e)Datascalinglawexperimentsthatexaminetheimpactofpretrainingdatasetsizeonretrieval
performance.Thedashedlinesindicaterandomchanceperformance.
Merlinpredictsdiseaseonsetwithin5yearswithanAUROCof0.757(95%CI[0.743,0.772])using100%ofthe
downstreamlabels,andoutperformstheImageNetpretrained(I3D)image-onlymodelby7%(Figure5b). Even
using10%oflabels,Merlinpredictsdiseaseonsetwithin5yearswithanAUROCof0.708(95%CI[0.692,0.723]),
andoutperformstheImageNetpretrainedmodelby4.4%(Figure5b). UsingMerlinfordiseaseriskstratification
canproducesimilaraccuracyasusinganImageNetpretrainedmodel,whileutilizing10xreducedlabeledtraining
data. Theseresultsdepictthatevenfewerthan25positivecases(10%oftrainingdata)canbeusedtobuildfuture
diseaseriskstratificationmodelsusingMerlin. Ourmodelablationstudydemonstratesthat3configurations,all
8
49
:naeM
493
:naeMa b
Num Positives/Total
Image 🔥 100% 10%
Encoder
CKD 90/513 14/46
DM 80/474 9/46
5-Year Disease Predictions:
HTN 111/404 11/34
IHD 69/518 9/49
ASCVD 136/504 20/52
OST 68/527 10/47
c d
Random Init ImageNet Init EHR Pretrained Merlin
Figure5: Multi-disease5-yearprediction. (a)Wefine-tuneMerlinforpredictingchronicdiseaseonsetinotherwisehealthy
patientswithin5-years.(b)WecompareMerlintootherbaselinemodelvariationsfine-tunedforthesametask.Wefindthatwith
both100%and10%ofdownstreamtrainingdata,Merlinoutperformstheothermodelvariations.(c)ComparisonofMerlin
chronicdiseasepredictionperformancetoamodeltrainedusingonlyphenotypes(EHRPretraining),anImageNetI3Dinitialized
model,andarandomlyinitializedmodel.(d)AnablationstudythatmeasurestheimpactofvariousaspectsofMerlin’straining
strategy.WefindthattrainingwithEHRandradiologyreports,usingstagedtraining(Stg.)ormulti-tasklearning(MTL),and
trainingwithradiologyreportsonly(Rpt.),alloutperformtrainingwithEHRonly.
ofwhichuseI3Dinitialization,providesimilarperformance. Reporttrainingonlywithreportsplitting(AUROC
of0.758(95%CI[0.743,0.773])),multi-taskEHRandreporttrainingwithoutreportsplitting(AUROCof0.757
(95%CI[0.743,0.772])),andourMerlinconfigurationwithmulti-taskEHRandreporttraining,alongwithreport
splitting,(AUROCof0.757(95%CI[0.743,0.772]))allproducecomparableresults.
2.5 RadiologyReportGeneration
RadiologyreportgenerationevaluatesMerlin’scapacityforgeneratingreportsbasedontheCTimages. Weselect
RadFM[37]asabaseline,asitisamulti-modaltextgenerationmodelwherethetrainingdatasetincludesabdominal
CTscans. BasedonthequantitativemetricsofRadGraph-F1[63],BERTScore[64],ROUGE-2[65],andBLEU
score[66],MerlinconsistentlyoutperformsRadFMacrossanatomicalsectionsandthefullreportfindings(Figure
6b).
Qualitatively,weobservethatMerlingeneratesreportswithcorrectstructure,wherefindingsareplacedwithin
thecorrectanatomicalsection. WhenMerlinpredictsthepresenceofafinding,thefindingorarelatedfinding
9a c
Annotation Color Key
Generated Report Section:
Liver normal. Green: correct, exists in both human, model reports
Purple: correct, exists in only human report
Blue: correct, exists in only model report
Orange: mischaracterized positive finding for that anatomy
Pink: false positive
RadLlama-7B 🔥
Red: false negative
Human Report
lower thorax: normal. liver and biliary tree: normal. gallbladder:
cholelithiasis. spleen: normal. pancreas: normal. adrenal
Image 🔥 Generate a radiology g rigla hn t d hs e: m n io pr em lva isl. . k ni od n ree ny as l a on r d u ru er te et rae lr s s: t oli nk ee sly . p nohl e hb yo dl ri oth ns e pa ht rt oh se i s.
Encoder Projection report for liver### bowel: appendix is normal. peritoneal cavity: normal. abdominal
Liver: normal.###</s>
wall: normal. bladder: normal.uterus and ovaries: normal.
vasculature: patent. lymph nodes: normal. musculoskeletal:
normal.
Merlin Report
b
RadFM Merlin-1 lower thorax: normal. liver and biliary tree: normal. gallbladder:
normal. spleen: normal. pancreas: normal. adrenal glands:
normal. kidneys and ureters: normal. gastrointestinal tract:
normal. peritoneal cavity: no free fluid. abdominal wall: normal.
bladder: normal.uterus and ovaries: the endometrial stripe is
thickened measuring 10 mm(3/297),there are multiple small
follicles in both adnexa, right greater than left. vasculature:
patent. lymph nodes: normal. musculoskeletal: normal.
Takeaways
• Model misses cholelithiasis finding in gallbladder (red)
• Human does not mention endometrial finding in uterus (red)
• Both model and human miss focal thickening of the anterior
myometrium, possibly indicating a small submucosal fibroid
• Model invents a series/slice location (orange)
Post Hoc Radiologist
Observations
Right: focal thickening of the
anterior myometrium,
possibly indicating a small
submucosal fibroid
Bottom: cholelithiasis
finding in gallbladder
Figure6:Radiologyreportgeneration.(a)Toenablereportgeneration,weextractthelasthiddenlayerembeddingsfromMerlin
andmodifythedimensionoftheseembeddingsusingaprojectionlayer.Wegeneratethereportsectionbysectionandtherefore
alsoembedareportsectionprompt.Theresultingvisionandlanguagetokensareusedasinputtoalanguagemodeltogeneratea
reportsection.(b)WecomparetheperformanceofourmodelagainstRadFM,using4metrics,acrosseachreportsectionandthe
fullreport. (c)WeprovideadenselyannotatedexampleofhumanandMerlingeneratedreports. Weboldthereportsection
headersinthehumanandMerlingeneratedreports.Weinclude"uterusandovaries"ingreen,asMerlinneedstodeducethe
correctpelvicanatomy.
usuallyexistsintheimage. Forexample,intheMerlingeneratedreportinFigure6c,Merlincorrectlyidentifiesthat
"theendometrialstripisthickened". Nonetheless,weobservethatMerlintendstounder-reportpositivefindings.
Forinstance,intheexampleinFigure6c,Merlindoesnotreportthecholelithiasisfinding,whichispresentinthe
humangeneratedreportandintheCTimage. Asthisisanearlydemonstrationofradiologyreportgenerationbased
onCTscans,weanticipateroomforimprovementinthegeneratedreports.
10
❆2.6 3DSemanticSegmentation
3D semantic segmentation evaluates whether Merlin learns geometric information about various anatomical
structures. We find that with 10% of training cases, Merlin performs 11% better than the second best model
variation,whichistheMerlinarchitecturewithImageNetI3Dinitialization(Figure7b). Thisdemonstratesthat
Merlinpretrainingisparticularlybeneficialinlabelscarcescenarios. Althoughtheadvantageisreducedwith100%
oftraininglabels,Merlinstilloutperformstheothermodelvariations(Figure7b). Figure7cshowsthat,where
performanceofbaselinemodelstendstosaturateforlargerorgans(e.g. liver,stomach,andspleen),Merlinperforms
bestrelativetobaselinesonsmallerorgansororganswithcomplicatedshapes,liketheduodenum,smallbowel,
T12vertebrae,L1vertebrae,andgallbladder. Figure7dpresentsqualitativesegmentationresults,wherethered
arrowsindicatedifferencesinthepredictedsegmentationsrelativetothegroundtruth. Weobservethattherandomly
initializedmodelmakesminormistakesinallthreescans,whileMerlinmakesamistakeinoneofthethreescans.
Toprow,centercolumn: therandomlyinitializedmodelmissesasmallpartofthecaudateliverlobe. Middlerow,
centerandrightcolumns: bothMerlinandrandomlyinitializedmodelsmisssmallportionsoftheproximaljejunum.
Bottomrow,centercolumn: therandomlyinitializedmodelincludesthebottomcalycesoftherightkidneyinthe
segmentationmask.
3 Discussion
Inthisstudy,wecurateahigh-qualityclinicaldatasetcomprisingofpairedCTscans(10,628,509total2Dimages
from25,528CTscans),EHRdata(2,995,293ICDcodes),andradiologyreports(10,051,571totaltokensinthe
findings sections). With this dataset, we train Merlin, a 3D vision-language foundation model for interpreting
abdominalCTscansthatleveragesstructuredEHRdataandunstructuredradiologyreportsupervision. Merlin
istrainedonasingleGPU,demonstratinghowhospitalsandresearchinstitutionscanpragmaticallybuildtheir
own models, as well as highlighting the opportunity to efficiently train larger models with additional compute
and data. Despite the minimal resources used to train Merlin, we demonstrate that Merlin can generalize to 6
typesofdownstreamtaskswith752individualtasks. Non-adaptedtasksincludezero-shotfindingsclassification
(31classes),phenotypeclassification(692classes),andzero-shotcross-modalretrieval(includingbothimageto
findingsandimagetoimpressionsretrieval). Modeladaptedtasksinclude5-yeardiseaseprediction(6classes),
radiologyreportgeneration,and3Dsemanticsegmentation(20organs). Weoutperformcarefullychosensingle-task
baselinemodelsacrossalltasks. Wefurthercontextualizeourmethodswithcomprehensiveablationstudiesthat
demonstrateMerlin’sperformanceasafunctionoftheunderlyingmodelarchitectureandthedesignconsiderations
fortheuseoftheunderlyingdata. FortrainingourMerlinfoundationmodel,wedemonstratethebenefitofI3D
weightinitialization,multi-tasklearningwithEHRandreports,andsplittingtheradiologyreportsintoanatomical
sections. Overall,wedepicthowMerlinmayassistininterpretationofabdominalCTscansandmitigatetheburden
onradiologists,whilesimultaneouslyaddingvaluefornewbiomarkerdiscoveryandfuturediseaseriskstratification
usingpre-existingdatasets.
Priorapproachesfor3DCTpretrainingfocusonimage-onlypretraining[67,68,69]. Thesemethodsleverage
maskedautoencoders(MAEs)[70],whichtrainamodeltoreconstructtheinputimage,giventhatsomepartofthe
inputismaskedout. Thisapproachhasbeendemonstratedtobebeneficialfor3Dsegmentationtasks,wherethe
decodernetworkisincludedinpretraining. However,thequalityofthelatentspacerepresentationsforavariety
ofdownstreamtasksonCTimageshasnotbeendemonstrated. ItispossiblethatwhileMAEsrequirethelatent
spacetoencodethegeometricinformationnecessarytoreconstructtheinput,thisgeometricinformationreducesthe
effectivenessofthelatentrepresentationsforimageclassificationtasks.Forimageclassification,itmaybebeneficial
todiscardthisgeometricinformation. IncontrasttoMAEs,Merlintrainsthelatentrepresentationsdirectlyusing
supervisionfromEHRdiagnosiscodesandradiologyreports. Wedemonstratethattheserepresentationsareuseful
foravarietyofdownstreamtasktypes,includingtasksinvolvingtextandimages,whichcannotbeaccomplished
usingimage-onlypretrainingmethods.
Recentconcurrentresearchhasstartedtoinvestigate3Dvisionlanguagemodelsforradiology,particularlyfocusing
onchestandheadCTscans[71,72]. However,theextentofexperimentsand3Devaluationsarelimited. These
studieslacksystematicablationstudiesanddonotutilizeallavailableclinicaldata,includingEHR.Inaddition
toexploringtheuseofmultipleclinicaldatatypesforsupervision,wecarryoutawidearrayofexperimentsand
evaluationsthatcontextualizethemethodologicalchoiceswemakeinthedevelopmentofMerlin.
Throughtheablationstudies,wefindthatI3Dweightinitialization[56]ishelpfulforalltasksthatweexaminewith
ablationstudies. Furthermore,trainingwithbothEHRdiagnosesandradiologyreportsisbeneficialovertraining
witheitheralone. Themannerofcombiningthesesupervisionsourcesisimportant,withmulti-tasktrainingusing
EHRandradiologyreportsoutperformingstagedtraining. Additionally,wefindthatreportsplitting,whileessential
11a b
Image 🔥
Encoder Number of train
examples:
Image 🔥
Decoder 10%: 29
100% 293
c d Ground Truth Random Init Merlin
Figure7:3Dsemanticsegmentation.(a)ToadaptMerlinforsegmentation,weaddadecoderandskipconnectionsbetween
theMerlinencoderanddecoder. (b)WecomparemodelvariationsusingaverageDicescoreacross20organsthatappearin
abdominalCT.Wecompareperformanceofmodelstrainedusing100%oftrainingcasesandalsosimulatethedatascarceregime
with10%oftrainingcases.(c)WereportDicescoresfor20organsacross4modelvariationsusing100%oftrainingcases.(d)
Wequalitativelycomparesegmentationsbetweenthegroundtruthlabels,amodelwiththeMerlinarchitecturethathasbeen
randomlyinitialized,andMerlin.Theredarrowsindicatemistakesmadebythemodelrelativetothegroundtruth.Eachrowisa
differentpatientsampledfromtheTotalSegmentator[51]testset.
for zero-shot classification, slightly degrades zero-shot retrieval performance. This reinforces that pretraining
ismosteffectivewhenthepretrainingdatadistributionmatchesthedownstreamtaskdistribution. Splittingthe
radiologyreportsresultsintextthatbetteralignswithpromptsforzero-shotclassification,butlesscloselyresembles
theradiologyreporttextusedforretrievaltasks.
Inlightofourfindings,weidentifykeyareasthatcouldsignificantlyenhancetheperformanceofMerlinandother
future3Dmedicalimagingmodels:
1. Increasingdatasetsize: Expectedly,ourinitialdatascalingresultsshowthatusingalargerpretraining
datasetimprovesMerlin’sperformance(Figure3b,Figure4e,Figure2d). Ourdatascalingcurvesacross
taskscanserveasausefulprospectivereferencemeasureinevaluatingadequatesizesoftrainingdatasets
forarequisitetaskperformance. Wenotethatasweaddmoredata,wemayneedtoupdateourmodel
architecture,whichwasoptimizedatourcurrentdatascale.
122. Improvingimageresolution: Utilizinghigherresolutionimagesisexpectedtoenhancemodelperformance,
as suggested by prior studies on vision-language models [73, 74, 75]. This may only hold until the
resolution used for training is equal to the resolution of the original CT scan, and we are no longer
downsamplingtheCTscan. Itisimportanttonotethatincreasingthephysicalresolutionduringimage
acquisition may not always yield better results. Radiologists often do not use the highest resolution
availableduringacquisitioninabdominalCTscans,particularlyregardingslicethickness,sincethismay
comeattheexpenseoflowersignal-to-noiseratio(SNR).
3. Optimizingbatchsize: Improvingbatchsizeisachallengewith3Dmedicalimagingwherethesizeofthe
imagesissignificant. However,priorworkdemonstratesthatlargebatchsizesarebeneficialforcontrastive
pretraining[75,76].
4. Extendingtoadditionalanatomiesandmodalities: OurMerlinmodellaysthegroundworkfortraining
anatomy-andmodality-specificradiologyfoundationmodels. Futureworkcanexploretherelativebenefits
ofpretrainingonmultipleanatomiesforthesamemodality,multiplemodalitiesforthesameanatomy,
orboth. Forsuchexperiments,itisimperativetobenchmarkthebenefitthateachmodalityoranatomy
provides,andwehopethatourrigorousevaluationstrategycanhelphighlightoptimaldatamixturesfor
trainingnext-generationradiologyfoundationmodels.
Beyondthestrengthsofourstudy,therearealsolimitationsofourmodelandourwork. First,towardsourgoal
ofdemocratizationoffoundationmodels,wetrainandevaluateMerlinusingasingleGPU.Thismaylimitthe
generalizabilityofsomeofourfindingsastaskperformancemayincreasebysimplyscalingupcomputeresources.
Second,thelimitednumberofpublicly-availablebaselinesforabdominalCT,especiallyVLMbaselines,makesit
challengingtounderstandtherelativeefficacyofourmethods. Ontheotherhand,webelievethatMerlincould
establish a strong and task-agnostic initial baseline for abdominal CT. Third, further exploration is required to
optimizeMerlinforthetasksofradiologyreportgenerationand3Dsegmentation. Forradiologyreportgeneration,
therearenumerousparametersthatcanbetuned,primarilyregardingtheadapterandLLM.Whilewedevisean
initialperformantstrategyfordirectreportgeneration,weleavefurtheroptimizationoftheadapterandLLMto
futurework. Likewise, adaptingMerlinforsegmentationrequiresaddingadecodernetwork. Futureworkcan
exploreadditionalarchitecturalparametersandoptimisationstrategiesfordecodertraining.
4 Methods
4.1 Datasets: PairedCTScan,UnstructuredRadiologyReport,andStructuredEHR
Medicalimagingpresentstheopportunitytocapturesupervisionsignalfromboththestructuredinformationinthe
EHR,aswellastheunstructuredinformationinradiologyreports. WefocusonabdominalCTimages,asabdominal
CTsarethemostcommon3Dimagingexamination[5].
All datasets used in this study were under IRB approval with a waiver of informed consent due to the use of
retrospectivedata. Tocollectthedataset,weidentifiedpatientsfromouracademicmedicalcenterwhounderwent
consecutiveabdominalCTexaminationsfromDecember2012toOctober2018. Thisresultedinahighquality
clinicaldatasetcomprising18,321patients,frominpatient(37%),emergencyservices(35%),outpatient(16%),and
observation(8%). Wecollectedthefollowingdataforeachpatient:
CT Studies: We obtain full abdominal CT studies, each comprising multiple CT series. From each study, we
selecttheserieswiththemostaxialslices,maximizingtheamountofinformationintheCTvolume. Thisresults
in10,628,5092Dimagesfrom25,528CTs. Thissamplingmayincludethenon-contrastseriesandputtheimage
embeddingsatadisadvantagerelativetotheradiologyreportssincecontrastenhancementpatternsmaynotbe
apparentonthenon-contrastseries. Toremedythis,werunanopensourceabdominalCTcontrast-phasedetection
algorithmthathasbeenvalidatedpreviously[77]ontheselectedCTseries. Weusethisalgorithmastheimage
meta-datarelatedtocontrastphaseisoftenmissingorinaccurate. Wefindthat97%oftheselectedseriesareportal
venousphase,while2.4%,0.45%,and0.26%scansaredelayed,arterial,andnon-contrastphases,respectively.
Radiology Reports: Wecompile theassociated radiology reportsfor each CTstudy. These reportsconsist of
multiplesections,mostpredominantlythefindingsandimpressionssections. Thefindingssectionincludesdetailed
observationsfromeachorgansystem.Theimpressionssectioncontainsadescriptionofthemostclinicallyimportant
findings. Weuseonlythefindingssectionsfortraining,giventhegranularityofinformationprovidedandprevious
workdemonstratingtheefficacyofthisapproach[78]. Wefindthatthereare10,051,571tokensfromtheradiology
reportfindingssectionsinourdataset.
EHR:Beyondtheimagesandreports,weacquireEHRdataforeachpatient. Inthiswork,weleveragethediagnosis
information,intheformofICDcodes,formodeltraining. TolinkICDcodeswithCTscans,wecollecttheICD
13codesthatwereassignedduringthepatientencounterthatgeneratedtheCT.Thereare954,013assignedICD9
codeswith5,686uniqueICD9codevaluesinourdataset. Thereare2,041,280ICD10codeswith10,867unique
ICD10codevalues. Intotal,thereare2,995,293assignedcodesinthedatasetwith16,553uniquevalues.
4.2 Vision-Language-EHRModel
WeaimtoleveragebothstructuredEHRandunstructuredradiologyreportinformationassupervisionsignaltotrain
aCTvisualmodel.
CTScanPreprocessing: WereformatallCTscanssothatthefirstaxispointsfromlefttoright,thesecondfrom
posteriortoanterior,andthethirdfrominferiortosuperior. Wethenresamplethein-planeaxialimagesto1.5mm
resolutionandtheout-of-planeslicethicknessto3mmspacingusingbilinearinterpolation. WemaptheHounsfield
unitrange-1000:1000totherange0:1,clippingvaluesthatfalloutsideofthisrange. Finally,wepadandcenter
cropto224by224pixelsin-planeand160pixelsout-of-plane.
EHRPreprocessing: FromtheEHR,weextractallICD-9andICD-10codesthatareassignedduringthehospital
visitswheretheabdominalCTstudiesarecarriedout. Furthermore,weobservethatoftencodescancomprise
disparatecharactersbuthavesimilarunderlyingphenotypicmeaning. Forexample,theICD-9codeV12.55denotes
apersonalhistoryofpulmonaryembolism,whereICD-9code415representsacutepulmonaryheartdisease. While
oneofthesecodesdescribesahistoryandonedescribesanacuteindication,theirimagingphenotypesmayappear
similar. Thus,weleveragethePheWASPhecodeMapping[57]tomap16,553ICD-9andICD-10codesto1,692
hierarchicalphenotypes. Furthermore, weapplyphenotypeexpansionwhereifasubjectispositiveforamore
specificphenotypeduringaparticularhospitalvisit,wepropagatethispositivelabelthroughoutthehierarchical
phenotypetreesotheyarealsopositiveforalllessspecificphenotypes. GroupingtheICDcodesinourdataset
resultsin1,692totalphenotypes. ForeachCTimage,wethushaveanassociatedbinaryvectorwitha0indicating
theabsenceofaphenotypeduringthecorrespondinghospitalvisitanda1indicatingthepresenceofthephenotype.
Thissupervisionsignaliscoarsegrainedinthatthephenotypesmaynotbedirectlyassociatedwithpixelvalues
in the abdominal CT scan; they are associated with the patient’s health status more generally. Thus, the EHR
phenotypecodesserveasweaksupervisionforourmodeltraining.
RadiologyReportPreprocessing: Usingregularexpressions,weextractthefindingssectionfromeachradiology
report. Duetothelongradiologyreports,wehypothesizethatcontrastivetrainingmayoverfittoshortandsalient
partsofthereportstosolvethetask. Furthermore,shortpromptsusedforsubsequentzero-shotclassificationwould
presentasignificantdomainshiftcomparedtothefullreports. Thus,wesplitthereportsintoanatomicalsections
andduringeachtrainingstep,alternatebetweenpresentingthefullreportsandasingleanatomicalsection. We
rotatethroughtheanatomicalsectionseveryotherstep,presentingthemodelasingleanatomyperbatchtoallow
themodeltocompareacrossdifferentdescriptionsofthesameanatomy. Togeneratethesesections,weuseregular
expressions. Iftheregularexpressionsfailforaparticularreportandanatomy,weusethefullreport. Thesections
thatweconsiderarelowerthorax,liverandbiliarytree,gallbladder,spleen,pancreas,adrenalglands,kidneysand
ureters,gastrointestinaltract,peritonealcavity,pelvicorgans,vasculatureandlymphnodes,andmusculoskeletal.
Model Architecture: Merlin uses an inflated 3D (I3D) ResNet152 for the image encoder. Inflation refers to
reusing2Dpretrainedmodelweightsandcopyingthoseweightsacrossthe3rddimensionofthe3Dconvolutional
kernels[56]. Giventhelongtokenlengthsofthereports(Figure4b),weuseclinicalLongformer[62]asthetext
encoderduetoitslongercontextlength(4,096)thanotherbiomedicalpretrainedmaskedlanguagemodelsand
generaldomainCLIPtextencoders. Previousworkfoundthatpretrainedtextencoderswithlongercontextlength
performbetter, givenlongercaptions[33]. Wealsoperformarchitectureablationstudieswhereweinvestigate
3DSwinTransformer[59]andConvNeXt[58]architectures(Figure3c). Inadditiontoarchitectureablations,we
investigatehowtheout-of-planestrideandkernelsizeinthemodelstemoftheResNet152impactperformance
(Figure3d). ThemodelstemintheResNet152isaconvolutionallayerattheinputofthenetworkwithanin-plane
kernelsizeof7andstrideof2, followedbyamaxpoolinglayerwithakernelsizeof3andastrideof2. The
modelstemthusreducessizeoftheinputbyafactorof4ineachdimension. Themodelstemablationonlyadjusts
parametersintheconvolutionallayerofthestem. Wemaintainthein-planesettingsastheyareinthe2Dversion
ofthemodelstoleveragethe2Dpretraining. Furthermore,weassesshowinflatingtheConvNeXtmodelwithan
out-of-planekernelsizeof7,matchingthe2Dkernelsizeoftheoriginal2Dweights,performscomparedtoinflating
theConvNeXtmodelwithanout-of-planekernelsizeof3. Werefertothemodelwithanout-of-planekernelsize
of3asConvNeXt-B*inFigure3c. Additionally,wecomparevariousmodelsizesinFigure3c. Weperformthese
architectureexperimentsusingthephenotypeclassificationtaskduetothesimplicityofthissupervisedtaskandthe
strongevaluationsignalthatresultsfromaveragingperformanceacrossalargenumberofphenotypes.
14ModelTraining: WeusebinarycrossentropyforthephenotypeclassificationlossandInfoNCE[52,29]lossfor
contrastivelearningwiththeradiologyreports. WeuseanAdamW[79]optimizerwithaninitiallearningrateof
1e-5,betas=(0.9,0.999),andacosinelearningrateschedulerwithnumberofepochsfordecayto0setto300. We
usegradientcheckpointingforboththevisualandtextencodersandtrainwithFP16mixedprecision. Thisallows
ustomaximizeabatchsizeof18onasingle48GBA6000GPU.
Multi-TaskLearningVersusStagedTraining: InadditiontotrainingusingtheEHRphenotypesandradiology
reportsjointlyinamulti-taskmanner,weconsiderstagingthetraining. Inthisformulation,wefirsttraintheMerlin
imageencoderusingtheEHRphenotypesinstage1. Instage2,weperformcontrastivetrainingwiththeradiology
reports. Topreventcatastrophicforgetting[80]oftheEHRinformationlearnedinstage1,weincludethephenotype
lossfunctionduringstage2training,withalowrelativeweight. Weusethesamehyper-parametersforstage2as
wedoformulti-tasktraining. Forstage1,weuseanAdamWoptimizerwithaninitiallearningrateof1e-4and
betas=(0.9,0.999),aswellasanexponentiallearningrateschedulerwithgamma=0.99. Weuseabatchsizeof22
onasingleA6000GPU.
DataSplits: Wedividethepretrainingdatasetintosplitsofsize60%(15,331CTs)fortraining,20%(5,060CTs)
forvalidation,and20%(5,137CTs)fortesting. WeensurethatmultipleCTsfromasinglepatientexistinasingle
split. Theexternaldataset,fromanotheruniversitymedicalcenter,consistsof7,000CTswhichareusedfortesting.
4.3 Evaluations
We select evaluation tasks consisting of non-adapted tasks, which Merlin can perform out of the box without
additionaladaptation. Thesetasksincludezero-shotfindingsclassification,phenotypeclassification,andzero-shot
cross-modalretrieval. WealsoselecttasksthatrequireadaptingMerlin,whichinclude5-yeardiseaseprediction,
radiologyreportgeneration,and3Dsemanticsegmentation. Weselectbothnon-adaptedtasksandadaptedtasks
todemonstrateMerlin’seffectivenesswithoutfine-tuning,aswellasitsadaptabilitywhenfine-tunedforspecific
applications.
4.4 Non-AdaptedTasks
Zero-ShotFindingsClassification: Weconsultthreeradiologiststodevelopalistof30findingsthatexhibita
diversityofsizeandlevelofdifficultyforhumandiagnosis. Foreachfinding,wegeneratelistsofdiseasepresence
phrasesthatdescribepossiblesub-types,locations,andseveritiesofthefinding. Wesimilarlygeneratealistof
diseaseabsencephrases,whicharewaysofdescribinganegativefinding. Forexample,adiseasepresencephrase
forascitescouldbe“largevolumeascitespresent"andadiseaseabsencepromptcouldbe“noascites". Weuse
thesephrasestominetheradiologyreportsfornegativeandpositiveexamples. Tostandardizethechancemetric
valueacrossfindings,webalancethenumberofpositiveandnegativeexamplesforeachfinding. Afterextracting
theseexamples,wemanuallyreviewthemtoensurethatthelabelsareaccurate.
Toperformzero-shotclassification,wefollowtheprocedureinFigure2awhereweembedtheCTscanusingthe
imageencoder. Weusethediseasepresenceandabsencephrasesaspromptsforzero-shotclassification. Weembed
eachofthepromptsusingthetextencoder. WethencomputethecosinesimilaritybetweentheCTscanembedding
andeachoftheprompts. Computingthemeancosinesimilarityacrossthediseasepresencepromptsandmean
similarityacrossthediseaseabsencepromptsallowsustoclassifytheCTscanaspositiveornegativeforagiven
finding. Weadaptexisting2Dbaselines(OpenCLIP[53]andBioMedCLIP[54])for3Dzero-shotclassificationby
embeddingeveryaxialsliceofagivenCTimage. TocomputeasimilarityscorebetweenaCTscanandagiven
prompt,wetakethemeanofthecosinesimilaritiesbetweeneachaxialsliceandthepromptembedding.
Wealsoperformzero-shotspinefracturedetectionusingtheVerSedataset[55]. AllCTsintheVerSedatasetwere
evaluatedforfractureseverityateachvertebrallevelinthethoracolumbarspine. WereorientandresampletheVerSe
CTstomatchMerlinpretraining. Forvolumeswithfractures,wecreateaprobabilitymapfromthefracturegrading
annotationsandsamplehigh-probabilitylocationstoobtainrepresentativesub-volumes. WemaptheHounsfield
unitrange-1000:1000to0:1andspatiallypadandcroptheimagesto224×224×160. Forvolumeswithout
fractures,wecentercroptheCTs.
WeperformanablationstudywherewemeasuretheimpactofI3Dinitialization,stagedversusmulti-tasktraining
with EHR and radiology reports, and splitting the report text with every other batch for finer grain contrastive
learning(Figure2e). Wealsoexaminehowzero-shotperformancevariesacrossthepretrainingdatasetsizesof1%,
10%,20%,40%,and100%ofthetotaltrainingset(Figure2d).
PhenotypeClassification: Togeneratethelabelsforthistask,wegroupICD-9andICD-10codesintophenotypes
using the PheWAS [57] phecode mapping. We compare Swin Transformer, ConvNeXt, and ResNet backbone
15architectures (Figure 3(c)), as well as various model stem hyper-parameters (Figure 3(d)). See section 4.2 for
additionaldetails. WereportaverageAUROCandAUPRCusingallphenotypesthathavemorethan20positive
examplesin thetestset, inorder toensureameaningfulmeasureofperformance. Weadditionallyassesshow
phenotypeclassificationperformancevariesacrossthepretrainingdatasetsizesof1%,10%,20%,40%,and100%
of15,331totalpretrainingsamples.
Counterfactualanalysis(Figure3(e))seekstoevaluatewhetheramodelisusingexpectedfeaturesduringphenotype
classification. Modelscaninsteadlearnshortcutsfromspuriouscorrelationsinthetrainingdata[81,82,83],which
areeasiertolearncomparedtomoresubtlephysiologicalfeaturesthatthemodelshoulduse. WeemploytheLatent
Shift approach [60] for counterfactual generation. This method uses a latent variable model, which represents
theinputimageinalowdimensionalspaceandreconstructstheimageinitsoutput. ThegradientoftheMerlin
predictionispassedbacktothelow-dimensionalspace,wherethelatentrepresentationismodifiedsuchthatMerlin’s
predictiondecreases. Thisgeneratesamodifiedimageattheoutputofthelatentvariablemodel,whichcanbe
observed. Thelatentvariablemodelregularizesthechangeinpixelssothattheimageremainsclosetothedata
manifold. TheresultoftheMerlincounterfactualgenerationprocessisanewCTvolume,whichisevaluatedby
observinghowtheclassifierpredictionchangesandhowtheimagefeaturesdrivingtheclassificationoutputare
exaggeratedorcurtailed. Weleveragethismethodtoqualitativelyverifythatthefeaturesusedduringclassification
areconsistentwithwhatweexpectthemodeltohavelearned. Forthisevaluation,weidentifypleuraleffusionand
splenomegalyasphenotypeswithfeaturesthatarewellunderstoodandcanbeobservedin2Dslices. Weselectthe
presentedvolumessuchthatthephenotypepredictionisabovethedecisionboundarydeterminedbyAUROCcurve
analysis,toensurethatweareexplainingapositiveprediction.
Zero-ShotCross-ModalRetrieval: InFigure4(a)weillustratetheprocedureforperformingzero-shotretrieval.
First,wesampleapoolofreportfindingssections. Then,weembedeachofthefindingssectionsusingthetext
encoderandthecorrespondingCTscansusingtheimageencoder. Wethencomputethecosinesimilaritiesbetween
theimagesandeachofthereportfindingssections. Basedonthesecosinesimilarities,werankthereportfindings
inorderoftheirsimilaritytoeachoftheimages.
To evaluate retrieval performance, we divide the test dataset into non-overlapping pools and within each pool
evaluatehowoftenthecosinesimilaritybetweencorrespondingCTimagesandreportsisthehighest(Recall@1).
Tocomputetheoverallscore,weaverageperformanceacrossallpools. Wecomputeretrievalperformancefor
surfacingthemostsimilarfindingssectiongivenaCTimage(Image→FindingsinFigure4c)andthemostsimilar
CTimagegivenafindingssection(Findings→ImageinFigure4c). Wecomputeretrievalperformancebetween
imagesandfindingssectionsonbothourinternaltestdataset(5,137pairs)andourexternaldataset(7,000pairs).
Tovalidatethatthemodelisnotlearningashortcuttomatchimageswithfindingssections,whichseemsplausible
giventhelengthofthefindingssections,wealsoevaluateretrievalbetweentheimagesandimpressionssections.
Giventhatduringtraining,Merlinisonlyexposedtofindingssectionsfromthereports,evaluatingonsemantically
similar,yetdistinctimpressionssectionscanprovideameasureofgeneralizability. Wealsoperformthisanalysison
ourinternaltestdatasetandtheexternaldataset(Figure4c).
Similartozero-shotclassification,weadaptexisting2Dbaselines(OpenCLIPandBioMedCLIP)for3Dcross-
modalityretrievalbyembeddingeachaxialsliceoftheCTvolumes.
Aswithzero-shotclassification,weperformablationstudieswhereweexaminetheimpactofI3Dinitialization,
stagedversusmulti-tasktrainingwithEHRandradiologyreports,andradiologyreportsplitting(Figure4d). We
alsoperformdatascalingexperimentsthatassesshowretrievalperformancevariesacrosspretrainingdatasetsizes
of1%,10%,20%,40%and100%ofthefullpretrainingdataset(Figure4e).
4.5 AdaptedTasks
Multi-Disease5-YearPrediction: Followingpreviouswork[13],wechoose6chronicdiseasesbasedontheir
prevalenceandthepotentialforbeneficialinterventionsfollowingearlydiagnoses: chronickidneydisease(abbr.
CKD; prevalence = 36 million in US [84]), diabetes mellitus (abbr. DM; prevalence = 35 million in US [85]),
hypertension(abbr. HTN;prevalence=120millioninUS[86]),ischemicheartdisease(abbr. IHD;prevalence=21
millioninUS[87]),atheroscleroticcardiovasculardisease(abbr. CVD;prevalence=24millioninUS[88]),and
osteoporosis(abbr. OST;prevalence=10millioninUS[89]).
Tocreatethelabelsforthistask,weextractdiseaseICDcodesofinterestfromtheEHR.Weassignpatientstoone
of4categoriesforeachdiseasebasedontheirICDcodes. Formally,wedefinet tobethetimepointmarkingthe
d
onsetofthediseasebasedonthepresenceofICDcodes,andt tobethetimepointmarkingthedateoftheCTscan.
s
t isthetimepointafteraCTscanthatdefinesawindowstartingwiththescandate(t tot ),wherethepresence
a s a
16ofanewdiagnosisindicatesapositivecase. Wesett to5yearstoensureanadequatetimedurationforpatient
a
followupfollowingtheCTimagingaswellasprovidinganadequatedurationforinterventioninfutureprospective
studies. Wedefinet tobethetimepointmarkingthelastdateofrecordinapatient’shistory. Withthesetime
h
points,weclassifyimagesintothefollowingcategories:
• Class0(Healthy): ThepatientisnotdiagnosedwiththespecifieddiseasesbeforetheirCTscandate(t )
s
orduringthewindowfromt tot . Ifapatientisdiagnosedwiththedisease,ithappensaftert . For
s a a
patientstobeassignedtoclass0,theymusthave5yearsoffollowupintheirEHRhistory.
• Class1(Progressors): ThepatientisnotdiagnosedwiththediseasebeforetheirCTdate,t ,butreceives
s
apositivediagnosisbetweent andt .
s a
• Class2(AlreadyProgressed): ThepatientisalreadydiagnosedwiththediseasebeforetheCTscandate,
t .
s
• Class3(Censored): Thepatienthasnotbeenmonitoredlongenoughtoruleoutthedisease. Inthiscase,
t isbeforet andthepatientdoesnotreceiveapositivediagnosisbeforetheendoftheirEHRhistory,t .
h a h
Fortraining,weuseclass1examplesaspositiveexamplesandclass0examplesasnegativeexamples. Wefine-tune
Merlin on examples that are held out from Merlin pretraining and validation. Furthermore, we do multi-task
multi-diseasepredictionwherethefine-tunedMerlinhasoneheadperdiseaseasshowninFigure10a. Sincewe
trainjointlyonallthediseases,wemasklabelsinabatchthatarefromclassesotherthan0and1foraspecific
disease. Weusebinarycrossentropyloss,anAdamWoptimizerwithalearningrateof1e-5,anexponentiallearning
ratedecaywithγ =0.8,andabatchsizeof8onasingleA6000GPU.Weuse60%,20%,20%train,validation,
andtestsplits,andmeasureperformanceinboththelow-labelregime(10%oftraininglabelswith9-20positive
examplesperdisease),aswellasthemedium-labelregime(100%ofavailabletraininglabelswith69-136positive
examplesperdisease). Thenumberofper-diseasepositiveandnegativeexamplesaregiveninFigure5.
RadiologyReportGeneration: Figure6ademonstratestheprocedureforadaptingMerlinforreportgeneration.
WefirstextractimagefeaturesfromthelasthiddenlayerofMerlin,whichhassize7x7in-plane,10out-of-plane,
andafeaturedimensionof2048. Weuseasinglelinearadapterlayertomapthefeaturestosize4096. Fortext
generation,weuseRadLlama-7B[42],aversionofLlama2-7Bthatisfine-tunedonMIMIC[90]radiologyreports
and clinical notes. We fine-tune the linear adapter, as well as 5% of RadLlama-7B parameters using low-rank
adaptation (LoRA) [91]. We generate the full reports section by section, where we use the following prompt
template:
<visual tokens>Generate a radiology report for <organ system>###
<report section>###</s>
Fortraining,weuse8gradientaccumulationstepswithalocalbatchsizeof6,givingusatotalbatchsizeof48. We
useanAdamWoptimizerwithbetas=(0.9,0.999),alearningrateof1e-4,andacosinelearningrateschedulerwith
500warmupstepsanddecayto0over500epochs.
WecompareourmodelperformanceonreportfindingsectiongenerationversusRadFM[37]usingfourmetrics:
BLEUscore[66]andROUGE-2[65],whichprimarilyassesssyntacticsimilarity;RadGraph-F1[63],whichassesses
findingsandmodifiersoffindings(e.g. locationandseverity);andBERTscore[64],amodel-basedscorewhich
assessessemanticsimilarity. Thecomparisonisconductedbothsection-by-sectionandforthefullfindingssections.
Subsequently,weapplyqualitativeframeworks[92,93]tocompareourmodelgeneratedfindingstoradiologist
findingsinFigure6c. Thisconsistsofdenselyannotatingindividualphrasestobecorrect,mischaracterized,false
positive,orfalsenegative.
3DSemanticSegmentationToadaptMerlinforsegmentation,weaddaUNet[94]decoderandskipconnections
betweentheMerlinencoderandthedecoder. Eachblockofthedecoderconsistsofa3Dtransposeconvolution
operation,withakernelsizeof2andastrideof2ineachdimension,followedbytwo3Dconvolutions,withkernel
sizes of 3 and strides of 1 in each dimension. Each of the two 3D convolution operations is followed by a 3D
batchnormandReLUactivation. WealsoaddskipconnectionswhereoutputsfromtheMerlinResNetblocksare
concatenatedwithoutputsfromthe3Dtransposeconvolutioninthedecoderbeforebeingpassedtothesubsequent
3Dconvolutionallayers. Formodeltraining,weusebothacross-entropylossandaDiceloss. WeuseanAdamW
optimizerwithbetas=(0.9,0.999)andalearningrateof5e-5. Tomirrorpretraining,wesegmentfullvolumesof
size224x224x160. Weachievethiswith4gradientaccumulationstepsandalocalbatchsizeof1,providingatotal
batchsizeof4.
Formodeltrainingandevaluation,wefilterbodyCTscansfromtheTotalSegmentatordatasetusingthefollowing
studytypes: ctpelvis,ctabdomen-pelvis,ctabdomen,ctthorax,ctthorax-abdomen,andctthorax-abdomen-pelvis.
17Thisgivesus401scanstotal. WealsoselectthefollowingorgansthatappearinabdominalCT:stomach,liver,
gallbladder,leftkidney,rightkidney,spleen,prostate,T12vertebrae,L1vertebrae,L2vertebrae,L3vertebrae,L4
vertebrae,L5vertebrae,S1vertebrae,sacrum,urinarybladder,colon,duodenum,smallbowel,andpancreas. We
usetheofficialTotalSegmentatortestsplitfortesting,whichhas34scansafterfiltering. Wesplittheremaining367
scansinto80%trainingexamples(293scans)and20%validationexamples(74scans). Inadditiontotrainingwith
allofthetrainingscans,wesimulatethelabelscarcesettingbysampling10%oftrainingscansrandomly.
WecompareperformanceacrossseveralmodelvariationsusingDicescore. WecompareMerlintomodelswiththe
samearchitecturebutinitializedrandomlyorusingImageNetI3Dinitialization. Wealsocompareagainst,Swin
UNETR[68],anarchitecturespecificallydesignedfor3Dmedicalimagesegmentation.
4.6 StatisticalAnalysis
Wecompute95%confidenceintervalsusingbootstrappingwith1000sampleswithreplacementforallexperiments
exceptfor cross-modalretrieval. For cross-modalretrieval, we dividethetest datasetintopoolsofsize N.For
eachpool,wecomputetheaverageretrievalperformanceforallexamplesinthepool. Wethencomputethe95%
confidenceintervalsusingthedistributionofperformancesacrosspools. Allp-valuesthatwereportareone-sided
p-values.
Acknowledgements
ACreceivesresearchsupportfromtheNationalInstitutesofHealth(grants-R01HL167974,R01AR077604,R01
EB002524,R01AR079431,P41EB027060,andcontracts75N92020C00008,75N92020C00021);andfromGE
Healthcare,Philips,Amazon,Microsoft/OpenAI,andStability.ai.
Contributions
Contributions L.B. collected data, developed code, trained models, ran experiments, analyzed results, created
figures,andwrotethemanuscript. Allauthorsreviewedthemanuscriptandprovidedmeaningfulrevisionsand
feedback. J.P.C, A.K., D.V.V, M.P., Z.C., J.B.D, E.R., C.B., M.E.K.J, S.O., M.V., J.M.J.V., Z.N., D.A., W.W.,
S.G.,andA.S.C.providedtechnicaladvice. J.P.CdevelopedthecounterfactualgenerationmethodforCTandran
counterfactualexperiments. A.K.carriedoutmodelevaluationsonexternaldatasets. D.V.V.assistedincollecting
zero-shotevaluationlabelsandfacilitatedradiologistannotationsofgeneratedreports. S.J.S.G.ranmodelinference
ontheexternalclinicaldataset. M.P.,Z.C.,J.B.D.,E.R.,C.T.,E.A.J.assistedwithmodelevaluations. Z.H.andJ.F.
assistedwithdatasetanonymization. C.B.,E.A.J,A.J.,R.D.B.,A.W.,C.P.L.,J.H.,andS.G.,providedclinicalinput
andfeedback. C.B.providedcounterfactualannotations. S.G.andC.B.providedannotationsofgeneratedreports.
A.S.C.guidedtheproject,servingasprincipalinvestigatorandadvisingontechnicaldetailsandoveralldirection.
Nofundersorthirdpartieswereinvolvedinstudydesign,analysisorwriting.
References
[1] OrganisationforEconomicCo-operationandDevelopment. Computedtomography(ct)exams,2022. URL
https://data.oecd.org/healthcare/computed-tomography-ct-exams.htm.
[2] HarvardHealth. Radiationriskfrommedicalimaging,Sep2021. URLhttps://www.health.harvard.
edu/cancer/radiation-risk-from-medical-imaging.
[3] MateuszWinder,AleksanderJerzyOwczarek,JerzyChudek,JoannaPilch-Kowalczyk,andJanBaron. Are
weoverdoingit? changesindiagnosticimagingworkloadduringtheyears2010–2020includingtheimpactof
thesars-cov-2pandemic. InHealthcare,volume9,page1557.MDPI,2021.
[4] LauraSchöckel,GregorJost,PeterSeidensticker,PhilippLengsfeld,PetraPalkowitsch,andHubertusPietsch.
Developments in x-ray contrast media and the potential impact on computed tomography. Investigative
radiology,55(9):592–597,2020.
[5] KalpanaM.Kanal,PriscillaF.Butler,DebapriyaSengupta,MythreyiBhargavan-Chatfield,LauraP.Coombs,
andRichardL.Morin. U.s.diagnosticreferencelevelsandachievabledosesfor10adultctexaminations.
Radiology,284(1):120–133,2017. doi: 10.1148/radiol.2017161911. URLhttps://doi.org/10.1148/
radiol.2017161911. PMID:28221093.
[6] Amar Udare, Minu Agarwal, Kiret Dhindsa, Amer Alaref, Michael Patlas, Abdullah Alabousi, Yoan K
Kagoma,andChristianBvanderPol. Radiologistproductivityanalytics: factorsimpactingabdominalpelvic
ctexamreportingtimes. JournalofDigitalImaging,pages1–11,2022.
18[7] MFernandaBellolio,HerbertCHeien,LindseyRSangaralingham,MollyMJeffery,RonnaLCampbell,
DanielCabrera,NilayDShah,andErikPHess. Increasedcomputedtomographyutilizationintheemergency
departmentanditsassociationwithhospitaladmission. WesternJournalofEmergencyMedicine,18(5):835,
2017.
[8] Daniel Liu, John W Garrett, Matt H Lee, Ryan Zea, Ronald M Summers, and Perry J Pickhardt. Fully
automated ct-based adiposity assessment: comparison of the l1 and l3 vertebral levels for opportunistic
prediction. AbdominalRadiology,pages1–9,2022.
[9] Matthew H Lee, Ryan Zea, John W Garrett, Peter M Graffy, Ronald M Summers, and Perry J Pickhardt.
Abdominalctbodycompositionthresholdsusingautomatedaitoolsforpredicting10-yearadverseoutcomes.
Radiology,page220574,2022.
[10] RonanThibault,LaurenceGenton,andClaudePichard. Bodycomposition: why,whenandforwho? Clinical
nutrition,31(4):435–447,2012.
[11] David DB Bates and Perry J Pickhardt. Ct-derived body composition assessment as a prognostic tool
inoncologicpatients: Fromopportunisticresearchtoartificialintelligence-basedclinicalimplementation.
AmericanJournalofRoentgenology,2022.
[12] RebeccaKuriyan. Bodycompositiontechniques. TheIndianjournalofmedicalresearch,148(5):648,2018.
[13] LouisBlankemeier,IsabelGallegos,JuanManuelZambranoChaves,DavidMaron,AlexanderSandhu,Fatima
Rodriguez,DanielRubin,BhavikPatel,MarcWillis,RobertBoutin,etal. Opportunisticincidenceprediction
ofmultiplechronicdiseasesfromabdominalctimagingusingmulti-tasklearning. InInternationalConference
onMedicalImageComputingandComputer-AssistedIntervention,pages309–318.Springer,2022.
[14] Juan M Zambrano Chaves, Andrew L Wentland, Arjun D Desai, Imon Banerjee, Gurkiran Kaur, Ramon
Correa, Robert D Boutin, David J Maron, Fatima Rodriguez, Alexander T Sandhu, et al. Opportunistic
assessmentofischemicheartdiseaseriskusingabdominopelviccomputedtomographyandmedicalrecord
data: amultimodalexplainableartificialintelligenceapproach. ScientificReports,13(1):21034,2023.
[15] ImagingTechnologyNews. Theevolvingcomputedtomographymarket,Mar2024. URLhttps://www.
itnonline.com/article/evolving-computed-tomography-market.
[16] VibhorWadhwa,GeorgeKoshyVilanilam,AvneeshChhabra,PuneetBhargava,BhavyaRehani,AtifZaheer,
Kedar Jambhekar, and Roopa Ram. A 15-year analysis of international medical graduates matching into
diagnosticradiologyresidencyprogramsintheunitedstates. Academicradiology,29(1):137–143,2022.
[17] AmericanCollegeofRadiology. Acrjobboard,May2024. URLhttps://jobs.acr.org/jobseeker/
search/results/.
[18] U.S. Bureau of Labor Statistics. Occupational employment and wages, may 2023, April 2024. URL
https://www.bls.gov/oes/current/oes291224.htm.
[19] Chad Hudnall. Maximum capacity: Overloaded radiologists are grappling with solutions to a booming
volume crisis., 2024. URL https://www.acr.org/Practice-Management-Quality-Informatics/
ACR-Bulletin/Articles/April-2024/Maximum-Capacity.
[20] James Milburn M. How will we solve our radiology workforce shortage?, 2024. URL
https://www.acr.org/Practice-Management-Quality-Informatics/ACR-Bulletin/Articles/
March-2024/How-Will-We-Solve-Our-Radiology-Workforce-Shortage.
[21] AnikaGPatel,VictorJPizzitola,CDanielJohnson,NanZhang,andMaitrayDPatel. Radiologistsmake
moreerrorsinterpretingoff-hoursbodyctstudiesduringovernightassignmentsascomparedwithdaytime
assignments. Radiology,297(2):374–379,2020.
[22] AbiRimmer. Radiologistshortageleavespatientcareatrisk, warnsroyalcollege. BMJ:BritishMedical
Journal(Online),359,2017.
[23] DiegoArdila,AtillaPKiraly,SujeethBharadwaj,BokyungChoi,JoshuaJReicher,LilyPeng,DanielTse,
MozziyarEtemadi,WenxingYe,GregCorrado,etal.End-to-endlungcancerscreeningwiththree-dimensional
deeplearningonlow-dosechestcomputedtomography. Naturemedicine,25(6):954–961,2019.
[24] DEng,CChute,NKhandwala,PRajpurkar,JLong,SShleifer,etal. Automatedcoronarycalciumscoring
usingdeeplearningwithmulticenterexternalvalidation. npjDigitalMedicine,2021.
[25] KaiCao,YingdaXia,JiawenYao,XuHan,LukasLambert,TingtingZhang,WeiTang,GangJin,HuiJiang,
XuFang,etal. Large-scalepancreaticcancerdetectionvianon-contrastctanddeeplearning. Naturemedicine,
29(12):3033–3043,2023.
19[26] Yan-Ran Wang, Kai Yang, Yi Wen, Pengcheng Wang, Yuepeng Hu, YongfanLai, Yufeng Wang, Kankan
Zhao, Siyi Tang, Angela Zhang, et al. Screening and diagnosis of cardiovascular disease using artificial
intelligence-enabledcardiacmagneticresonanceimaging. NatureMedicine,pages1–10,2024.
[27] CurtisPLanglotz. Thefutureofaiandinformaticsinradiology: 10predictions. Radiology,309(1):e231114,
2023.
[28] U.S.FoodandDrugAdministration. Artificialintelligenceandmachinelearning(ai/ml)-enabledmedical
devices, 2024. URL https://www.fda.gov/medical-devices/software-medical-device-samd/
artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices.
[29] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,
AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfromnaturallanguage
supervision. InInternationalConferenceonMachineLearning,pages8748–8763.PMLR,2021.
[30] ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,RossWightman,MehdiCherti,
Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale
datasetfortrainingnextgenerationimage-textmodels. AdvancesinNeuralInformationProcessingSystems,
35:25278–25294,2022.
[31] DavidBLarson,DavidCMagnus,MatthewPLungren,NigamHShah,andCurtisPLanglotz. Ethicsof
usingandsharingclinicalimagingdataforartificialintelligence: aproposedframework. Radiology,295(3):
675–682,2020.
[32] ZifengWang,ZhenbangWu,DineshAgarwal,andJimengSun. Medclip: Contrastivelearningfromunpaired
medicalimagesandtext. arXivpreprintarXiv:2210.10163,2022.
[33] ShengZhang,YanboXu,NaotoUsuyama,JaspreetBagga,RobertTinn,SamPreston,RajeshRao,MuWei,
NaveenValluri,CliffWong,etal. Large-scaledomain-specificpretrainingforbiomedicalvision-language
processing. arXivpreprintarXiv:2303.00915,2023.
[34] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann,
HoifungPoon,andJianfengGao. Llava-med: Trainingalargelanguage-and-visionassistantforbiomedicine
inoneday. arXivpreprintarXiv:2306.00890,2023.
[35] MichaelMoor,QianHuang,ShirleyWu,MichihiroYasunaga,CyrilZakka,YashDalmia,EduardoPontes
Reis,PranavRajpurkar,andJureLeskovec. Med-flamingo: amultimodalmedicalfew-shotlearner. arXiv
preprintarXiv:2307.15189,2023.
[36] TaoTu,ShekoofehAzizi,DannyDriess,MikeSchaekermann,MohamedAmin,Pi-ChuanChang,Andrew
Carroll, Chuck Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical ai. arXiv preprint
arXiv:2307.14334,2023.
[37] ChaoyiWu,XiaomanZhang,YaZhang,YanfengWang,andWeidiXie. Towardsgeneralistfoundationmodel
forradiologybyleveragingweb-scale2dand3dmedicaldata,2023.
[38] OmkarThawkar,AbdelrahmanShaker,SahalShajiMullappilly,HishamCholakkal,RaoMuhammadAnwer,
SalmanKhan,JormaLaaksonen,andFahadShahbazKhan. Xraygpt: Chestradiographssummarizationusing
medicalvision-languagemodels. arXivpreprintarXiv:2306.07971,2023.
[39] ShawnXu,LinYang,ChristopherKelly,MarcinSieniek,TimoKohlberger,MartinMa,Wei-HungWeng,
AttilaKiraly,SaharKazemzadeh,ZakkaiMelamed,etal. Elixr: Towardsageneralpurposex-rayartificial
intelligencesystemthroughalignmentoflargelanguagemodelsandradiologyvisionencoders. arXivpreprint
arXiv:2308.01317,2023.
[40] Pierre Chambon, Christian Bluethgen, Curtis P. Langlotz, and Akshay Chaudhari. Adapting pretrained
vision-languagefoundationalmodelstomedicalimagingdomains,2022.
[41] PierreChambon,ChristianBluethgen,Jean-BenoitDelbrouck,RogierVanderSluijs,MałgorzataPołacin,
Juan Manuel Zambrano Chaves, Tanishq Mathew Abraham, Shivanshu Purohit, Curtis P. Langlotz, and
AkshayChaudhari. Roentgen: Vision-languagefoundationmodelforchestx-raygeneration,2022. URL
https://arxiv.org/abs/2211.12737.
[42] ZhihongChen,MayaVarma,Jean-BenoitDelbrouck,MagdaliniPaschali,LouisBlankemeier,DaveVanVeen,
JeyaMariaJoseValanarasu,AlaaYoussef,JosephPaulCohen,EduardoPontesReis,etal.Chexagent:Towards
afoundationmodelforchestx-rayinterpretation. arXivpreprintarXiv:2401.12208,2024.
[43] StephanieLHyland,ShruthiBannur,KenzaBouzid,DanielCCastro,MercyRanjit,AntonSchwaighofer,
FernandoPérez-García,ValentinaSalvatelli,ShaurySrivastav,AnjaThieme,etal. Maira-1: Aspecialised
largemultimodalmodelforradiologyreportgeneration. arXivpreprintarXiv:2311.13668,2023.
20[44] Shih-ChengHuang,TanayKothari,ImonBanerjee,ChrisChute,RobynLBall,NorahBorus,AndrewHuang,
BhavikNPatel,PranavRajpurkar,JeremyIrvin,etal. Penet—ascalabledeep-learningmodelforautomated
diagnosisofpulmonaryembolismusingvolumetricctimaging. NPJdigitalmedicine,3(1):61,2020.
[45] MatthewChristensen,MilosVukadinovic,NealYuan,andDavidOuyang. Vision–languagefoundationmodel
forechocardiograminterpretation. NatureMedicine,pages1–8,2024.
[46] David B. Larson, Alex J. Towbin, Rebecca M. Pryor, and Lane F. Donnelly. Improving consistency in
radiologyreportingthroughtheuseofdepartment-widestandardizedstructuredreporting. Radiology,267
(1):240–250,2013. doi: 10.1148/radiol.12121502. URLhttps://doi.org/10.1148/radiol.12121502.
PMID:23329657.
[47] Nur Ahmed, Muntasir Wahed, and Neil C Thompson. The growing influence of industry in ai research.
Science,379(6635):884–886,2023.
[48] Sergei Polevikov. Med-gemini by google: A boon for researchers, a bane for doctors.
https://sergeiai.substack.com/p/googles-med-gemini-im-excited-and, 2024. URL https://sergeiai.
substack.com/p/googles-med-gemini-im-excited-and.
[49] ScottL.Fleming,AlejandroLozano,WilliamJ.Haberkorn,JenelleA.Jindal,EduardoP.Reis,RahulThapa,
LouisBlankemeier,JulianZ.Genkins,EthanSteinberg,AshwinNayak,BirjuS.Patel,Chia-ChunChiang,
AlisonCallahan,ZepengHuo,SergiosGatidis,ScottJ.Adams,OluseyiFayanju,ShreyaJ.Shah,Thomas
Savage,EthanGoh,AkshayS.Chaudhari,NimaAghaeepour,ChristopherSharp,MichaelA.Pfeffer,Percy
Liang,JonathanH.Chen,KeithE.Morse,EmmaP.Brunskill,JasonA.Fries,andNigamH.Shah. Medalign:
Aclinician-generateddatasetforinstructionfollowingwithelectronicmedicalrecords,2023.
[50] HansLiebl,DavidSchinz,AnjanySekuboyina,LucaMalagutti,MaximilianTLöffler,AmirhosseinBayat,
MalekElHusseini,GilesTetteh,KatharinaGrau,EvaNiederreiter,etal. Acomputedtomographyvertebral
segmentationdatasetwithanatomicalvariationsandmulti-vendorscannerdata. Scientificdata, 8(1):284,
2021.
[51] JakobWasserthal,Hanns-ChristianBreit,ManfredTMeyer,MauricePradella,DanielHinck,AlexanderW
Sauter,TobiasHeye,DanielTBoll,JoshyCyriac,ShanYang,etal. Totalsegmentator: Robustsegmentationof
104anatomicstructuresinctimages. Radiology: ArtificialIntelligence,5(5),2023.
[52] AaronvandenOord,YazheLi,andOriolVinyals. Representationlearningwithcontrastivepredictivecoding.
arXivpreprintarXiv:1807.03748,2018.
[53] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon,
ChristophSchuhmann,LudwigSchmidt,andJeniaJitsev. Reproduciblescalinglawsforcontrastivelanguage-
imagelearning,2022.
[54] ShengZhang,YanboXu,NaotoUsuyama,HanwenXu,JaspreetBagga,RobertTinn,SamPreston,Rajesh
Rao,MuWei,NaveenValluri,etal. Biomedclip: amultimodalbiomedicalfoundationmodelpretrainedfrom
fifteenmillionscientificimage-textpairs. arXivpreprintarXiv:2303.00915,2023.
[55] MaximilianTLöffler,AnjanySekuboyina,AlinaJacob,Anna-LenaGrau,AndreasScharr,MalekElHusseini,
MareikeKallweit,ClausZimmer,ThomasBaum,andJanSKirschke. Avertebralsegmentationdatasetwith
fracturegrading. Radiology: ArtificialIntelligence,2(4):e190138,2020.
[56] JoaoCarreiraandAndrewZisserman. Quovadis,actionrecognition? anewmodelandthekineticsdataset. In
proceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pages6299–6308,2017.
[57] JoshuaCDenny,LisaBastarache,MarylynDRitchie,RobertJCarroll,RaquelZink,JonathanDMosley,
JulieRField,JillMPulley,AndreaHRamirez,EricaBowton,etal. Systematiccomparisonofphenome-
wideassociationstudyofelectronicmedicalrecorddataandgenome-wideassociationstudydata. Nature
biotechnology,31(12):1102–1111,2013.
[58] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeichtenhofer,TrevorDarrell,andSainingXie. Aconvnet
forthe2020s,2022.
[59] ZeLiu, YutongLin, YueCao, HanHu, YixuanWei, ZhengZhang, StephenLin, andBainingGuo. Swin
transformer: Hierarchicalvisiontransformerusingshiftedwindows,2021.
[60] JosephPaulCohen,RupertBrooks,SovannEn,EvanZucker,AnujPareek,MatthewP.Lungren,andAkshay
Chaudhari. GifsplanationviaLatentShift: ASimpleAutoencoderApproachtoCounterfactualGenerationfor
ChestX-rays. MedicalImagingwithDeepLearning,2021. URLhttps://openreview.net/forum?id=
rnunjvgxAMt.
21[61] KennethClark,BruceVendt,KirkSmith,JohnFreymann,JustinKirby,PaulKoppel,StephenMoore,Stanley
Phillips,DavidMaffitt,MichaelPringle,etal. Thecancerimagingarchive(tcia): maintainingandoperatinga
publicinformationrepository. Journalofdigitalimaging,26:1045–1057,2013.
[62] Yikuan Li, Ramsey M Wehbe, Faraz S Ahmad, Hanyin Wang, and Yuan Luo. Clinical-longformer and
clinical-bigbird: Transformersforlongclinicalsequences. arXivpreprintarXiv:2201.11838,2022.
[63] Jean-Benoit Delbrouck, Pierre Chambon, Christian Bluethgen, Emily Tsai, Omar Almusa, and Curtis P
Langlotz. Improvingthefactualcorrectnessofradiologyreportgenerationwithsemanticrewards. arXiv
preprintarXiv:2210.12186,2022.
[64] TianyiZhang,VarshaKishore,FelixWu,KilianQ.Weinberger,andYoavArtzi. Bertscore: Evaluatingtext
generationwithbert,2020.
[65] Chin-YewLin. Rouge: Apackageforautomaticevaluationofsummaries. InTextsummarizationbranches
out,pages74–81,2004.
[66] KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu. Bleu: amethodforautomaticevaluation
of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational
Linguistics,pages311–318,2002.
[67] Shih-ChengHuang,AnujPareek,MalteJensen,MatthewPLungren,SerenaYeung,andAkshaySChaudhari.
Self-supervisedlearningformedicalimageclassification: asystematicreviewandimplementationguidelines.
NPJDigitalMedicine,6(1):74,2023.
[68] YuchengTang,DongYang,WenqiLi,HolgerRRoth,BennettLandman,DaguangXu,VishweshNath,andAli
Hatamizadeh. Self-supervisedpre-trainingofswintransformersfor3dmedicalimageanalysis. InProceedings
oftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages20730–20740,2022.
[69] JeyaMariaJoseValanarasu,YuchengTang,DongYang,ZiyueXu,CanZhao,WenqiLi,VishalM.Patel,
BennettLandman,DaguangXu,YufanHe,andVishweshNath.Disruptiveautoencoders:Leveraginglow-level
featuresfor3dmedicalimagepre-training,2023.
[70] KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollár,andRossGirshick. Maskedautoencoders
arescalablevisionlearners,2021.
[71] IbrahimEthemHamamci,SezginEr,FurkanAlmas,AyseGulnihanSimsek,SevvalNilEsirgun,IremDogan,
Muhammed Furkan Dasdelen, Bastian Wittmann, Enis Simsar, Mehmet Simsar, Emine Bensu Erdemir,
AbdullahAlanbay,AnjanySekuboyina,BerkanLafci,MehmetK.Ozdemir,andBjoernMenze. Afoundation
modelutilizingchestctvolumesandradiologyreportsforsupervised-levelzero-shotdetectionofabnormalities,
2024.
[72] LinYang,ShawnXu,AndrewSellergren,TimoKohlberger,YuchenZhou,IraKtena,AtillaKiraly,Faruk
Ahmed,FarhadHormozdiari,TiamJaroensri,EricWang,ElleryWulczyn,FayazJamil,TheoGuidroz,Chuck
Lau,SiyuanQiao,YunLiu,AkshayGoel,KendallPark,ArnavAgharwal,NickGeorge,YangWang,Ryutaro
Tanno, David G. T. Barrett, Wei-Hung Weng, S. Sara Mahdavi, Khaled Saab, Tao Tu, Sreenivasa Raju
Kalidindi,MozziyarEtemadi,JorgeCuadros,GregorySorensen,YossiMatias,KatherineChou,GregCorrado,
JoelleBarral,ShravyaShetty,DavidFleet,S.M.AliEslami,DanielTse,ShruthiPrabhakara,CoryMcLean,
DaveSteiner,RoryPilgrim,ChristopherKelly,ShekoofehAzizi,andDanielGolden. Advancingmultimodal
medicalcapabilitiesofgemini,2024.
[73] HugoLaurençon,LéoTronchon,MatthieuCord,andVictorSanh.Whatmatterswhenbuildingvision-language
models? arXivpreprintarXiv:2405.02246,2024.
[74] ZhangLi,BiaoYang,QiangLiu,ZhiyinMa,ShuoZhang,JingxuYang,YaboSun,YuliangLiu,andXiangBai.
Monkey: Imageresolutionandtextlabelareimportantthingsforlargemulti-modalmodels. arXivpreprint
arXiv:2311.06607,2023.
[75] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,
AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfromnaturallanguage
supervision. InInternationalconferenceonmachinelearning,pages8748–8763.PMLR,2021.
[76] TingChen,SimonKornblith,MohammadNorouzi,andGeoffreyHinton. Asimpleframeworkforcontrastive
learningofvisualrepresentations. InInternationalconferenceonmachinelearning,pages1597–1607.PMLR,
2020.
[77] EduardoPontesReis,LouisBlankemeier,JuanManuelZambranoChaves,MalteEngmannKjeldskovJensen,
SallyYao,CesarAugustoMadidTruyts,MarcHWillis,ScottAdams,EdsonAmaroJr,RobertDBoutin,etal.
Automatedabdominalctcontrastphasedetectionusinganinterpretableandopen-sourceartificialintelligence
algorithm. EuropeanRadiology,pages1–8,2024.
22[78] CaraVanUden,ChristianBluethgen,MaayaneAttias,MalgorzataPolacin,HaiweiHenryGuo,NehaSimha,
Rishi Raj, and Curtis Langlotz. Exploring the versatility of zero-shot clip for interstitial lung disease
classification. arXivpreprintarXiv:2306.01111,2023.
[79] IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. arXivpreprintarXiv:1711.05101,
2017.
[80] JamesKirkpatrick,RazvanPascanu,NeilRabinowitz,JoelVeness,GuillaumeDesjardins,AndreiARusu,
KieranMilan,JohnQuan,TiagoRamalho,AgnieszkaGrabska-Barwinska,etal. Overcomingcatastrophic
forgettinginneuralnetworks. Proceedingsofthenationalacademyofsciences,114(13):3521–3526,2017.
[81] AndrewRoss,MichaelCHughes,andFinaleDoshi-Velez.RightfortheRightReasons:TrainingDifferentiable
ModelsbyConstrainingtheirExplanations. InInternationalJointConferenceonArtificialIntelligence,2017.
URLhttps://github.com/dtak/rrr.
[82] RobertGeirhos,CarlosR.MedinaTemme,JonasRauber,HeikoH.Schütt,MatthiasBethge,andFelixA.Wich-
mann. Shortcutlearningindeepneuralnetworks. NatureMachineIntelligence,nov2020. ISSN2522-5839.
doi: 10.1038/s42256-020-00257-z. URLhttp://www.nature.com/articles/s42256-020-00257-z.
[83] JosephPaulCohen,TianshiCao,JosephD.Viviano,Chin-WeiHuang,MichaelFralick,MarzyehGhassemi,
MuhammadMamdani,RussellGreiner,andYoshuaBengio. Problemsinthedeploymentofmachine-learned
modelsinhealthcare. CanadianMedicalAssociationJournal,2021. doi: 10.1503/CMAJ.202066. URL
https://www.cmaj.ca/content/193/35/E1391.
[84] CentersforDiseaseControlandPrevention.Chronickidneydiseaseintheunitedstates,2023,May2023.URL
https://www.cdc.gov/kidneydisease/publications-resources/ckd-national-facts.html.
[85] CentersforDiseaseControlandPrevention. Bythenumbers: Diabetesinamerica,Oct2022. URLhttps:
//www.cdc.gov/diabetes/health-equity/diabetes-by-the-numbers.html.
[86] CentersforDiseaseControlandPrevention. Factsabouthypertension,Jul2023. URLhttps://www.cdc.
gov/bloodpressure/facts.htm.
[87] U.S.DepartmentofHealthandHumanServices. Whatiscoronaryheartdisease?,Dec2023. URLhttps:
//www.nhlbi.nih.gov/health/coronary-heart-disease.
[88] Jing Gu, Robert Sanchez, Ankita Chauhan, Sergio Fazio, and Nathan Wong. Lipid treatment status and
goal attainment among patients with atherosclerotic cardiovascular disease in the united states: A 2019
update. Americanjournalofpreventivecardiology,Mar2022. URLhttps://www.ncbi.nlm.nih.gov/
pmc/articles/PMC8968014/.
[89] NicoleCWright,AnneCLooker,KennethGSaag,JeffreyRCurtis,ElizabethSDelzell,SusanRandall,and
BessDawson-Hughes. Therecentprevalenceofosteoporosisandlowbonemassintheunitedstatesbasedon
bonemineraldensityatthefemoralneckorlumbarspine,2014.
[90] AlistairEWJohnson,LucasBulgarelli,LuShen,AlvinGayles,AyadShammout,StevenHorng,TomJPollard,
SichengHao,BenjaminMoody,BrianGow,etal. Mimic-iv,afreelyaccessibleelectronichealthrecorddataset.
Scientificdata,10(1):1,2023.
[91] EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhu
Chen. Lora: Low-rankadaptationoflargelanguagemodels. arXivpreprintarXiv:2106.09685,2021.
[92] DaveVanVeen, CaraVanUden, MaayaneAttias, AnujPareek, ChristianBluethgen, MalgorzataPolacin,
WahChiu,Jean-BenoitDelbrouck,JuanZambranoChaves,CurtisLanglotz,AkshayChaudhari,andJohn
Pauly. RadAdapt: Radiology report summarization via lightweight domain adaptation of large language
models. InDinaDemner-fushman,SophiaAnaniadou,andKevinCohen,editors,The22ndWorkshopon
Biomedical Natural Language Processing and BioNLP Shared Tasks, pages 449–460, Toronto, Canada,
July 2023.Association forComputational Linguistics. doi: 10.18653/v1/2023.bionlp-1.42. URL https:
//aclanthology.org/2023.bionlp-1.42.
[93] DaveVanVeen,CaraVanUden,LouisBlankemeier,Jean-BenoitDelbrouck,AsadAali,ChristianBluethgen,
Anuj Pareek, Malgorzata Polacin, William Collins, Neera Ahuja, Curtis P. Langlotz, Jason Hom, Sergios
Gatidis, John Pauly, and Akshay S. Chaudhari. Adapted large language models can outperform medical
expertsinclinicaltextsummarization. NatureMedicine,2024. doi: 10.1038/s41591-024-02855-5. URL
https://doi.org/10.1038/s41591-024-02855-5.
[94] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical
imagesegmentation. InMedicalimagecomputingandcomputer-assistedintervention–MICCAI2015: 18th
internationalconference, Munich, Germany, October5-9, 2015, proceedings, partIII18, pages234–241.
Springer,2015.
23Supplementary
Split 2DSlices CTs FindingsTokens ICD9Codes ICD10Codes Patients
Train 6,387,231 15,331 6,036,645 577,998 1,261,561 11,010
Validation 2,099,217 5,060 1,985,925 178,194 379,733 3,644
Test 2,142,061 5,137 2,029,001 197,821 399,986 3,667
Total 10,628,509 25,528 10,051,571 954,013 2,041,280 18,321
Table1:Summaryofpretrainingdatasetsplits.
Demographic Patients(n=18,321) Value
Age - 53.8±19.5
Gender
Female 10,254 55.97%
Male 8,065 44.02%
Self-ReportedRace/Ethnicity
Non-HispanicWhite 8,660 47.27%
Asian 2,673 14.59%
Black 952 5.20%
HispanicWhite 515 2.81%
PacificIslander 294 1.60%
NativeAmerican 65 0.35%
Unknown 5,127 27.98%
PatientClass
Inpatient 6,834 37.31%
EmergencyServices 6,388 34.87%
Outpatient 2,959 16.16%
Observation 1,452 7.93%
Table2:Internaldatasetcharacteristics.Theagevalueisprovidedasmean±standarddeviation.Allothervaluesareprovided
aspercentagesofthetotalpatients(n=18,321).
Demographic Patients(n=5,804) Value
Age - 61.4±16.5
Gender
Female 2,982 51.38%
Male 2,822 48.62%
Self-ReportedRace/Ethnicity
Non-HispanicWhite 4,576 78.84%
Black 270 4.65%
HispanicWhite 198 2.76%
Asian 87 1.50%
NativeAmerican 31 0.53%
PacificIslander 4 0.07%
Unknown 602 10.37%
Table3:Externaldatasetcharacteristics.Theagevalueisprovidedasmean±standarddeviation.Allothervaluesareprovided
aspercentagesofthetotalpatients(n=5,804).
24Encoder Init Labels Stem AllPhecodes UpperQuartile LowerQuartile
KSz/ N=691,Prev=3.1% byPrevalence byPrevalence
Stridez Phecodesw/AUC N=173;Prev=8.7% N=173,Prev=0.6%
AUROC AUPRC >0.85 >0.9 AUROC AUPRC AUROC AUPRC
SwinUNETR MAE EHR 4/4 .736 .088 52 8 .734 .207 .738 .029
ConvNeXt-T I3D EHR 7/2 .768 .106 115 20 .765 .239 .764 .035
ConvNeXt-S I3D EHR 7/2 .761 .102 105 14 .760 .234 .750 .031
ConvNeXt-B I3D EHR 7/2 .773 .110 132 28 .768 .244 .766 .036
ConvNeXt-B* I3D EHR 3/2 .789 .131 180 46 .784 .270 .781 .052
ResNet50 I3D EHR 3/1 .798 .137 226 72 .787 .280 .797 .049
ResNet152 I3D EHR 7/2 .798 .135 221 65 .785 .272 .796 .050
↓ I3D EHR 3/2 .798 .136 221 74 .788 .275 .792 .049
I3D EHR 3/1 .801 .140 235 79 .789 .279 .801 .054
(Merlin) I3D MTL 3/1 .812 .142 259 93 .804 .290 .808 .050
Table4:Phenotypeclassification.Weonlyincludephenotypesthathavemorethan20positiveexamplesinthetestsetinorder
toensureameaningfulmeasureofperformance.*inConvNext-B*indicatesthatinsteadofinflatingthezdimensiontoasize
equaltothe2Dkernelheightandwidthof7,thekernelisinflatedtoadepthof3.
Init Labels Split Stem AllPhecodes UpperQuartile LowerQuartile
Text KSz/ N=691,Prev=3.1% byPrevalence byPrevalence
Stridez Phecodesw/AUC N=173;Prev=8.7% N=173,Prev=0.6%
AUROC AUPRC >0.85 >0.9 AUROC AUPRC AUROC AUPRC
I3D Staged ✗ 3/1 .804 .145 240 84 .792 .287 .798 .056
I3D Staged ✓ 3/1 .807 .146 249 87 .795 .291 .801 .056
I3D MTL ✗ 3/1 .814 .153 267 103 .806 .302 .807 .058
Rand MTL ✓ 3/1 .786 .124 117 46 .778 .261 .779 .042
I3D MTL ✓ 3/1 .812 .142 259 93 .804 .290 .808 .050
Table5:Phenotypeclassificationablationstudy.WeperformablationstudieswhereweexaminetheimpactofI3Dinitialization,
stagedtrainingversusmulti-tasklearning(MTL)withEHRandradiologyreports,andsplittingthereporttextwitheveryother
batchforfinergraincontrastivelearning.
Method AverageF1Score
OpenCLIP(Internal) .276[.262,.288]
BioMedCLIP(Internal) .285[.274,.295]
Merlin(Internal) .741[.727,.755]
Merlin(External) .647[.607,.678]
Merlin(VerSe) .767[.630,.867]
Table6:Zero-shotclassification.Theinternalandexternalnumbers(first4rows)representaveragesoverthe30findingsfor
theinternalclinicaldatasetandtheexternalclinicaldatasetrespectively. ThebottomrowrepresentsF1scoreforzero-shot
classificationofvertebralfracturesontheVerSedataset.
Init Labels Split AverageF1Score
Text
I3D Report ✓ .730[.714,.744]
I3D Staged ✗ .669[.653,.683]
I3D Staged ✓ .735[.719,.748]
I3D MTL ✗ .656[.640,.671]
Rand MTL ✓ .698[.681,.711]
I3D MTL ✓ .741[.727,.755]
Table7:Zero-shotclassificationablationstudy.Wemeasurezero-shotperformanceaswevaryparametersofI3Dversusrandom
initialization,stagedtrainingversusmulti-tasklearning(MTL)withEHRandradiologyreports,andtrainingwiththefull
findingssectionsversususingradiologyreportsplitting.
25Task Method Recall@1 Recall@8
N=32 N=64 N=128 N=32 N=64 N=128
Img→F OpenCLIP .030 .016 .009 .243 .125 .062
BioMedCLIP .040 .021 .010 .298 .156 .083
Merlin .780 .696 .608 .989 .968 .927
F→Img OpenCLIP .033 .017 .009 .250 .125 .061
BioMedCLIP .044 .021 .012 .306 .156 .079
Merlin .776 .687 .594 .988 .965 .920
Img→I OpenCLIP .030 .016 .010 .256 .128 .061
BioMedCLIP .036 .017 .009 .273 .141 .073
Merlin .352 .253 .174 .796 .663 .532
I→Img OpenCLIP .032 .017 .008 .252 .126 .064
BioMedCLIP .046 .024 .012 .322 .169 .081
Merlin .384 .277 .194 .854 .706 .564
Table8:Cross-modalityretrieval.WecompareperformanceofOpenCLIP,BioMedCLIP,andMerlinacrossseveralsettings:
retrievingthecorrectfindingssectiongivenanimage(Img→F),retrievingthecorrectimagegivenafindingssection(F→
Img),retrievingthecorrectimpressionssectiongivenanimage(Img→I),andretrievingthecorrectimagegivenanimpressions
section(I→Img).WeperformretrievalwithinpoolsofsizesN=32,N=64,andN=128.
Task Init Labels Split Recall@1 Recall@8
Text N=32 N=64 N=128 N=32 N=64 N=128
Img→F I3D Report ✓ .778 .692 .598 .989 .967 .921
I3D Staged ✗ .654 .547 .449 .967 .920 .844
I3D Staged ✓ .672 .561 .457 .972 .925 .848
I3D MTL ✗ .812 .726 .639 .988 .969 .937
Rand MTL ✓ .690 .583 .468 .978 .937 .867
I3D MTL ✓ .780 .696 .608 .989 .968 .927
F→Img I3D Report ✓ .775 .686 .584 .991 .969 .921
I3D Staged ✗ .646 .539 .434 .965 .913 .841
I3D Staged ✓ .664 .555 .445 .970 .923 .841
I3D MTL ✗ .801 .718 .626 .988 .968 .933
Rand MTL ✓ .683 .571 .455 .980 .940 .869
I3D MTL ✓ .776 .687 .594 .988 .965 .920
Img→I I3D Report ✓ .364 .265 .187 .812 .681 .549
I3D Staged ✗ .307 .220 .159 .737 .590 .449
I3D Staged ✓ .328 .228 .163 .780 .634 .500
I3D MTL ✗ .372 .275 .196 .799 .667 .543
Rand MTL ✓ .288 .202 .131 .740 .592 .453
I3D MTL ✓ .352 .253 .174 .796 .663 .532
I→Img I3D Report ✓ .382 .274 .192 .850 .709 .574
I3D Staged ✗ .324 .234 .161 .770 .616 .490
I3D Staged ✓ .348 .246 .168 .811 .672 .523
I3D MTL ✗ .400 .294 .216 .817 .698 .568
Rand MTL ✓ .289 .200 .127 .779 .613 .460
I3D MTL ✓ .384 .277 .194 .854 .706 .564
Table9:Cross-modalityretrievalablationstudy.Wecompareretrievalperformanceacrossthreeaxesofweightinitializations,
methodsforincorporatingEHRandradiologyreportsintotraining,andsplittingorusingthefullfindingsduringtraining.
26Encoder Init Labels %Tr CKD DM HTN IHD CVD OST Average
10% 14/46 9/46 11/34 9/49 20/52 10/47
100% 90/513 80/474 111/404 69/518 136/504 68/527
SwinTransformer - - 10% .46[.40,.50] .70[.66,.75] .43[.39,.48] .53[.49,.57] .53[.50,.57] .56[.51,.61] .54[.52,.55]
ResNet152 - - 10% .53[.48,.58] .66[.61,.71] .60[.55,.64] .49[.45,.54] .58[.54,.62] .50[.44,.56] .56[.54,.58]
↓ I3D - 10% .72[.67,.76] .72[.67,.76] .67[.63,.71] .67[.63,.71] .67[.64,.71] .66[.61,.71] .68[.67,.70]
I3D EHR 10% .58[.53,.63] .64[.59,.69] .47[.42,.51] .75[.71,.78] .69[.65,.72] .62[.57,.67] .62[.60,.64]
(Merlin) I3D MTL 10% .74[.70,.78] .70[.66,.75] .69[.65,.73] .70[.67,.74] .73[.69,.76] .69[.65,.73] .71[.69,.72]
SwinTransformer - - 100% .55[.50,.59] .73[.68,.77] .61[.57,.65] .52[.48,.56] .54[.49,.57] .60[.55,.66] .59[.57,.61]
ResNet152 - - 100% .63[.58,.67] .74[.69,.78] .65[.61,.69] .65[.61,.69] .57[.54,.61] .60[.55,.66] .64[.62,.66]
↓ I3D - 100% .74[.70,.77] .74[.70,.78] .71[.67,.75] .68[.64,.72] .68[.64,.71] .74[.69,.78] .71[.70,.73]
I3D EHR 100% .76[.73,.80] .72[.68,.76] .74[.70,.77] .74[.70,.78] .73[.69,.76] .68[.64,.73] .73[.71,.74]
(Merlin) I3D MTL 100% .77[.74,.81] .72[.68,.76] .75[.72,.79] .76[.72,.79] .74[.71,.77] .80[.76,.84] .76[.74,.77]
Table10:Multi-disease5-yearprediction.Wefine-tuneMerlinfor5-yeardiseaseprediction.Alldatausedinthisevaluation,
includingtrain,val,andtestsplits,areheldoutfrompretraining.
BLEU↑ ROUGE-2↑ BERT↑ RadGraph-F1↑
Section RadFM Merlin RadFM Merlin RadFM Merlin RadFM Merlin
Lowerthorax .001 .019 .070 .332 .406 .615 .020 .319
Liverandbiliarytree .001 .269 .025 .389 .328 .641 .080 .380
Gallbladder .000 .006 .006 .632 .534 .851 .152 .721
Spleen .000 .002 .004 .710 .382 .853 .283 .805
Pancreas .000 .001 .010 .700 .447 .849 .091 .748
Adrenalglands .006 .030 .067 .882 .490 .942 .106 .879
Kidneysandureters .005 .269 .040 .385 .368 .654 .091 .387
Gastrointestinaltract .001 .013 .037 .152 .398 .531 .092 .167
Peritonealcavity .000 .206 .005 .390 .387 .702 .050 .335
Pelvicorgans .000 .233 .009 .358 .328 .656 .036 .432
Vasculature .000 .026 .004 .485 .232 .748 .006 .548
Lymphnodes .003 .023 .119 .601 .502 .775 .031 .542
Musculoskeletal .001 .046 .018 .303 .449 .689 .008 .293
Fullreport .000 .102 .011 .262 .224 .588 .008 .293
Table11:Radiologyreportgeneration.WecompareMerlinandRadFMforgeneratingradiologyreportsectionscorresponding
tovariousanatomies,aswellasthefullfindings.
27a c
Annotation Color Key Human Report
Green: correct, exists in both human, model reports lower thorax: normal. liver and biliary tree: normal morphology, contour
Purple: correct, exists in only human report and size of liver.stable appearance of focal, subcentimeterhypodensity in
Blue: correct, exists in only model report the posterior right hepatic lobe ( series 3 / 50 ), too small to characterize
Orange: mischaracterized positive finding for that anatomy but statistically favored to represent a cyst. gallbladder: normal. spleen:
Pink: false positive normal. pancreas: normal. adrenal glands: normal. kidneys and ureters:
normal morphology with bilateral kidneys. normal parenchymal
Red: false negative
enhancement and excretion. focal dilatation of the mid right ureter,
without obstructing lesion or calculus seen. no ureteral enhancement.
b gastrointestinal tract: normal caliber of the small and large bowel . no
Human Report bowel wall thickening . normal appearance of the appendix ( series 2 / 203
; 301 / 61 ). peritoneal cavity: no free fluid. abdominal wall: normal.
lower thorax: smallbilateralpleural effusions with associated atelectasis/ bladder: mild, asymmetric thickening of the bladder wall anteriorly.
consolidation. liver and biliary tree: stable subcentimeterhypodensity in prostate and seminal vesicles: normal. vasculature: patent. lymph nodes:
segment 8 of liver , too small to characterize. mild periportal edema likely normal. musculoskeletal: stable appearance of sclerotic focus in the left
postsurgical. new linear hypoattenuating area within lateral segment of left femoral head and right femoral neck ( series 3/286 ; 3/322 ), likely
lobe , most likely related to retraction. gallbladder: surgically absent. representing a bone island.
spleen: normal. pancreas: normal. adrenal glands: normal. kidneys and
ureters: normal. bowel: status post recent whippleprocedure. no
intrahepatic ductal dilatation or pancreatic ductal dilatation. stomach does Merlin Report
not appear significantly distended.mild wall thickening of
gastrojejunostomy due to lack of distention or postoperative edema. lower thorax: no significant pulmonary, pleural, or mediastinal
postoperative stranding in the resection bed, tracking along smaand smv. abnormality is seen in the visualized portions of the thorax. liver: normal.
peritoneal cavity: 8.5 x 1.6 x 4.9 cm fluid collection in right anterior gallbladder/bile ducts: the common bile duct is mildly prominent
pararenal space, with attenuation compatible with hematoma.a small measuring up to 8 mm, likely related to patient's age and
amount of nonspecific free fluid is also present in the pelvis. abdominal postcholecystectomystate.no evidence of choledocholithiasis or
wall: mild diffuse subcutaneous stranding along bilateral flanks. bladder: intrahepatic biliary dilatation. gallbladder: appears normal. spleen:
small amount of air in the bladder likely due to recent catheterization. uterus normal. pancreas: no evidence of masses or calcifications in the
and ovaries: redemonstration of a fibroid originating from the fundus of the pancreas. no pancreatic ductal dilatation. genitourinary: there is a 1 cm
uterus. vasculature: mild narrowing of portosplenicconfluence and proximal cystic lesion within the left kidney, likely representing a simple renal cyst
smv, likely secondary to postop edema.hepatic artery is patent. lymph (series 3 image 207).otherwise, the kidneys are unremarkable without
nodes: a few of left paraaortic subcentimeterlymph nodes are mildly evidence of enhancing renal masses, nephrolithiasis, or hydronephrosis.
increased in size, most likely reactive. musculoskeletal: normal. the bladder appears normal. gastrointestinal: the stomach and small
bowel appear normal.the colon shows mild diverticulitis. adrenal glands:
Merlin Report normal. kidneys and ureters: the kidneys enhance homogeneously.no
focal renal lesions. no hydronephrosis or nephroureterolithiasis. bowel:
lower thorax: there is a smallrightpleural effusion with associated no evidence of obstruction. the appendix is normal in appearance.
compressive atelectasis. liver and biliary tree: normal. gallbladder: surgically peritoneal cavity: no ascites, pneumoperitoneum, or focal intra-
absent. spleen: normal. pancreas: postsurgical changes of whipple abdominal collections. abdominal wall: unremarkable. pelvis: the liver is
procedure. the pancreaticojejunostomyis patent with no evidence of normal without evidence of enhancing masses or biliary dilatation.a 1 cm
obstruction or leakage at this level. there are multiple fluid collections in the low attenuating lesion is noted in segment ivb(image 97, series 4), which
surgical bed, including a 3 x 2 cm collection adjacent to the hepaticojejunal is too small to characterize but likely represents a simple cyst.the
anastomosis (4/107), and a 5 x 6 mm collection anterior to the sma(4/98). hepatic and portal veins are patent. evaluation of the gallbladder
adrenal glands: normal. kidneys and ureters: normal. gastrointestinal tract: demonstrates no evidence of stones, wall thickening, or pericholecystic
postsurgical changes of whippleprocedure. no evidence for bowel fluid. the spleen appears normal in size. no evidence of splenic masses.
obstruction or inflammation. peritoneal cavity: there is a small amount of evaluation the pancreas demonstrates. vasculature: the abdominal aorta
free fluid in the pelvis. abdominal wall: postsurgical changes are seen along is normal in caliber. no atherosclerotic calcifications. lymph nodes: no
the anterior midline abdomen with mild subcutaneous stranding and foci of enlarged retroperitoneal, mesenteric, pelvic or inguinal lymph nodes.
gas, likely related to recent surgery. bladder: normal. uterus and ovaries: musculoskeletal: no aggressive osseous lesion is identified in the
surgically absent. vasculature: patent. lymph nodes: normal. abdomen or pelvis.
musculoskeletal: normal.
Takeaways
Takeaways
• The model report (274 words) is considerably longer than the human
• Human incorrectly labels the pancreas as normal (red). Later in the report (154 words).
report, this was described correctly, indicating the human overlooked a • Model correctly describes the gallbladder appearance as normal
default field in the template, (green), but this logically contradicts the earlier, false statement of
• Model identifies only one side of bilateral pleural effusions (orange) “postcholecystectomy” (pink).
• Model describes diffuse fluid below the anastomosis site as a • Model contains two sections for kidneys which logically contradict: the
“collection,” which is a slight mischaracterization as the fluid is not genitourinary section falsely describes a cyst (pink); the kidneys and
encapsulated (orange) ureters section describes no finding (green, orange).
Figure8:Radiologyreportgeneration.(a)Asshownintheannotationcolorkey,weannotateindividualphrasestobecorrect,
mischaracterized, falsepositive, orfalsenegative. (b-c)WeprovidedenseannotationsoftwosetsofhumanandMerlin
generatedreports.
28