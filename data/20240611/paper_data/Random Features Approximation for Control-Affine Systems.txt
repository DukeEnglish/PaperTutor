ExtendedversionofL4DC2024paper:1–25
Random Features Approximation for Control-Affine Systems
KimiaKazemian KK983@CORNELL.EDU
YahyaSattar YSATTAR@CORNELL.EDU
SarahDean SDEAN@CORNELL.EDU
DepartmentofComputerScience,CornellUniversity,Ithaca,NY,USA.
Abstract
Modern data-driven control applications call for flexible nonlinear models that are amenable to
principled controller synthesis and realtime feedback. Many nonlinear dynamical systems of in-
terestarecontrolaffine. Weproposetwonovelclassesofnonlinearfeaturerepresentationswhich
capturecontrolaffinestructurewhileallowingforarbitrarycomplexityinthestatedependence.Our
methodsmakeuseofrandomfeatures(RF)approximations,inheritingtheexpressivenessofkernel
methodsatalowercomputationalcost. Weformalizetherepresentationalcapabilitiesofourmeth-
odsbyshowingtheirrelationshiptotheAffineDotProduct(ADP)kernelproposedbyCastan˜eda
et al. (2021) and a novel Affine Dense (AD) kernel that we introduce. We further illustrate the
utility by presenting a case study of data-driven optimization-based control using control certifi-
catefunctions(CCF).Simulationexperimentsonadoublependulumempiricallydemonstratethe
advantagesofourmethods.
Keywords: RandomFeatures,Control-AffineSystems,ControlCertificateFunctions.
1. Introduction
Modern control applications require modelling systems with complex and nonlinear dynamics.
Modern machine learning techniques offer a data-driven solution. From deep learning to kernel
methods,learning-basedapproachesfitmodelstodata. Highlyexpressivemodelscanapproximate
arbitraryfunctions, andthereforemodelarbitrarilycomplexphenomena. However, thiscomesata
cost—theycanbecomputationallyexpensivetotrainanddifficulttouseforthepurposeofsynthe-
sizingacontroller. Thisposesachallengeinreal-timefeedbacksystems.
Linear regression is a straightforward approach for learning dynamical models from data, so
long as a suitable nonlinear feature representation, i.e., set of basis functions, is known (Mania
et al., 2020). However, selecting proper basis functions is often challenging and requires mod-
elling detailed properties of the unknown dynamics. One solution is to choose a set of random
basis functions to generate feature vectors of fixed dimension. This approach, called random fea-
tures(RF),canachievehighexpressivenessaslongasthedimensionofthefeaturevectorsislarge
enough (Rahimi and Recht, 2008). Random features have proven useful for dynamical systems
forecasting (Giannakis et al., 2023), receding horizon control (Lale et al., 2021), and policy learn-
ing(Laleetal.,2022).
We propose two novel classes of random feature representations suitable for principled data-
driven control (Section 3). Our key insight is to leverage the control-affine structure of many non-
lineardynamicsofinterest, which enablesprincipledoptimization-basedapproachesforcontroller
synthesis. Weproposetwodistinctmethodsforincorporatingthisstructureintorandombasisfunc-
tionsandformalizetheirrepresentationguaranteesbyshowingthattheyapproximatefunctionsina
© K.Kazemian,Y.Sattar&S.Dean.
4202
nuJ
01
]GL.sc[
1v41560.6042:viXraRFAPPROXIMATIONFORCONTROL
ReproducingKernelHilbertSpace(RKHS).OneofourmethodsapproximatestheAffineDotProd-
uct(ADP)kernelproposedbyCastan˜edaetal.(2021),whiletheothercorrespondstoanovelAffine
Dense (AD) kernel that we propose. The RF methods significantly reduce the computational time
andmemorycomplexitycomparedtotheirkernelcounterparts.
To showcase the utility of an explicit control-affine structure, we present a case study for non-
linear control in Section 4. Our data-driven approach is based on Control Certificate Functions
(CCFs),whichareutilizedtosynthesizecontrollersthatprovablyachievepropertiessuchassafety
andstability(Tayloretal.,2021). CCFshavebeenusedinarangeofapplicationsfromroboticsto
multi-agent systems (Artstein, 1983; Ames et al., 2014; Nguyen et al., 2016; Pickem et al., 2017),
includinginadata-drivenmanner(Castan˜edaetal.,2021;Castan˜edaetal.,2021;Tayloretal.,2021;
Choietal.,2023). Simulationsonadoubleinvertedpendulumillustratethebenefitsofourmodels
when used in a certainty-equivalent manner. In Appendix B.2, we additionally derive uncertainty
estimatesanalogoustothoseofGaussianprocess(GP)regression,andusethemtoproposearobust
data-drivencontrollerinAppendixD.Wehighlightthattheapproximationmethodsthatwepropose
maybebroadlyofinterestforanycontrolapplicationwhichmakesuseofGPs(Kolleretal.,2018;
CaldwellandMarshall,2021;Bradfordetal.,2019;Hewingetal.,2020;Lietal.,2021).
2. ProblemSettingandPreliminaries
In this work, we consider an affine modelling and prediction problem inspired by applications in
data-drivencontrol. Wefirstdefinethegeneralproblemofinterest,andthengiveseveralexamples
thatariseinthecontextoflearningfordynamicsandcontrol.
Definition1(Control-affinemodellingproblem) For data of the form {(x ,u ,z )}N , find a
i i i i=1
function hˆ : X ×U → R which i) is affine in its second argument and ii) accurately models the
relationshipbetween(x,u)andz,i.e. hˆ(x ,u )isnotfarfromz .
i i i
Such a modelling problem naturally arises in applications involving nonlinear control-affine
systems. Thedynamicsaredescribedineithercontinuousordiscretetime:
x˙ = f(x)+g(x)u or x = f(x )+g(x )u . (1)
t+1 t t t
Here,x∈X⊆Rnisthesystemstate,andu∈U⊆Rmisthecontrolinput. Thenonlinearfunctionf
determinestheevolutionofthestateintheabsenceofcontrolinputs,whilegmodelsstate-dependent
actuation. Control-affinedynamicsarisenaturallyfrommanipulatorequations(MurrayandHauser,
1991; Tedrake, 2023), and are thus prevalent in applications like robotics. While many systems
are known to follow dynamics of the form (1), the precise form of f and/or g may be unknown.
Data-drivenapproachesenablethecontrolofsystemswithentirelyorpartiallyunknowndynamics.
Therearemanyexamplesofmodellingtasksthatariseinsuchdata-drivencontrolsettings.
Example1 Consider a model predictive control setting in which the evolution of the state itself
must be predicted (Lale et al., 2021). For a discrete-time control-affine system (1) with unknown
dynamics and direct state observation, a sequence of states and inputs {(x ,u )}N defines n
i i i=0
modellingproblemsoftheformpresentedinDefinition1: oneforeachstatedimension.
Example2 Consider again model predictive control, now for continuous time control-affine dy-
namics(1). Asequenceofsampledstates{x }N canbeusedtoapproximate{x˙ }N withforward
i i=0 i i=1
finitedifferencinganddefinenmodellingproblemsoftheformpresentedinDefinition1.
2RFAPPROXIMATIONFORCONTROL
Example3 Consider Certificate Function Control (Taylor et al., 2021), which enforces safety or
stability using a known certificate function C:X →R. For continuous time control-affine dynam-
ics (1), such controllers require computing C˙ :X×U →R, which cannot be done directly when
thedynamicsareunknown. However,asequence{C(x )}N canbecomputedfromasequenceof
i i=0
sampledstatesandtheknownfunctionC. Then,finitedifferencingapproximatesthetimederivative,
resultinginaproblemoftheformpresentedinDefinition1.
Example4 Foranyofthepreviousexamples,supposethatanapproximatemodelofthedynamics
f˜and g˜ is known. Then learning residual error dynamics also results in a problem of the form
presentedinDefinition1(seee.g.,Tayloretal.(2021);Castan˜edaetal.(2021)).
The examples above serve to motivate the relevance of the modelling problem in Definition 1.
We now turn to background and preliminaries on solving it. Our focus is on nonparametric tech-
niqueswhichcanmodelphenomenaofarbitrarycomplexity. Wereviewkernelregression,whichis
bothnonparametricandamenabletouncertaintyquantification,andrandomfeaturesapproximation,
whichallowsforcomputationalefficiency.
2.1. FromLineartoKernelRegression
Webeginbyreviewingregressionapproachesforgeneraldatacontaininginputvectors{s }N ⊂Rd
i i=1
andatargetoutputvariable{z }N ⊂R. Ourstartingpointisparametriclinearregression,inwhich
i i=1
predictions depend linearly on a known nonlinear feature function of the inputs. Let ϕ:Rd→RD
map an input vector s ∈ Rd to a feature vector ϕ(s) ∈ RD. The feature function, also known
as a basis function, maps the input vectors to a higher-dimensional feature space, where a linear
relationshipcanbeestablishedmoreeasily.
Linear least-squares regression (Watson, 1967) models the relationship as hˆ(s) = wˆ⊤ϕ(s),
wheretheparameterwˆ∈RD islearnedfromdatabysolving
N
(cid:88)
min (ϕ(s )⊤w−z )2+λ∥w∥2, (2)
i i 2
w∈RD
i=1
whereλ ≥ 0isaregularizationparameter. LetthematrixΦ∈RN×D andthevectorz∈RN bethe
aggregationofrows{ϕ(s )⊤}N and{z }N ,respectively. Thenthepredictionis
i i=1 i i=1
hˆ(s) = ϕ(s)⊤(Φ⊤Φ+λI )−1Φ⊤z = ϕ(s)⊤Φ⊤(ΦΦ⊤+λI )−1z. (3)
D N
The first equality is the closed-form solution to the least squares objective, and the second
leveragesthekerneltrick(ScholkopfandSmola,2018;Mu¨lleretal.,2018). Thesignificanceofthis
reformulationisthatthevectorΦϕ(s) =: k(s)andmatrixΦΦ⊤ =: K canbecomputedusingonly
inner products of basis functions evaluated on training data. This is attractive because the class of
basisfunctionsdeterminethecomplexityandrichnessofthemodelledrelationshipbetweeninputss
andoutputsz. Whilelow-dimensionalbasesmaysufficeforhighlystructuredprocesses,generally,
asuitablecompactbasismaynotbeknownapriori.
Kernelmethodsallowforexpressivebasisfunctionsofarbitrarilyhighorinfinitedimension. A
kernel function k :Rd×Rd →R generalizes inner products between basis functions, and is used
as a nonparametric approach for representing complex functions. Appropriately defined, kernel
3RFAPPROXIMATIONFORCONTROL
ridge regression corresponds to regression in Reproducing Kernel Hilbert Spaces (RKHS) (Wend-
land,2004). ManyRKHSaredenseinthesetofcontinuousfunctions,enablingarbitrarilyaccurate
representationofcontinuousfunctionsviakernelregression. Thefollowinglemmapresentsasuffi-
cientconditionforcheckingthatakernelfunctiondefinesaRKHS.Itisadirectimplicationofthe
Moore–AronszajntheoremandLemma1inBerlinetandThomas-Agnan(2011).
Lemma2 Let H be some Hilbert space with inner product ⟨·,·⟩. A function k:Rd×Rd→R is a
reproducingkernelifthereexistsamappingφ:Rd→Hsuchthatk(s,s′)=⟨φ(s),φ(s′)⟩.
Inadditiontotheirexpressivity,kernelmethodsareamenabletotheoreticalguaranteesandun-
certainty characterization. Popularized from the Bayesian perspective as Gaussian Process (GP)
regression (Williams and Rasmussen, 2006), confidence intervals on kernel predictions can be de-
rived even in frequentist settings (Srinivas et al., 2009). We discuss this perspective further in
appendix B.1 as it is useful for robust control. The drawback of kernel methods is computation.
Algorithmshavesuperlinearcomplexityinthenumberofdatapoints. Inparticular, computingthe
kernel weights can be prohibitively expensive for large datasets. Solving (3) generally requires
O(N3)timeandO(N2)memory.
2.2. RandomFeatureApproximation
Ratherthanusingkernelmethodsdirectly,weproposebasisfunctionswhichareexpressive,general
purpose,andyetfinite-dimensional. Consideraparametricfamilyofbasisfunctionsb:Rd×Rp→R.
Then for parameters {ϑ }D ⊂Rp sampled i.i.d. from a fixed probability distribution p(ϑ), the
j j=1
(cid:2) (cid:3)⊤
randombasisisdefinedasϕ(s) = b(s;ϑ ) b(s;ϑ ) ... b(s;ϑ ) . Randombasisfunctions
1 2 D
of this form approximate rich class of functions in the sense that ϕ(s)⊤ϕ(s′) is a Monte-Carlo
estimator which converges uniformly to a kernel k(s,s′) (Rahimi and Recht, 2008). The rate of
convergence is controlled by the feature dimension D and the particular kernel depends on the
definitionofb(·;ϑ)andp(ϑ).
TheunderlyingobservationbehindrandomfeaturesisasimpleconsequenceofBochner’sThe-
orem (Avron et al., 2017): For every normalized shift-invariant kernel (i.e., k(0) = 1), there is a
probabilitydensityfunctionp(·)onRd suchthat
(cid:90)
k(s,s′) = e−i2πϑ⊤(s−s′)p(ϑ)dϑ =: F(p(ϑ)). (4)
ϑ∈Rd
In other words, the inverse Fourier transform F−1 of the kernel k(·) is the probability density
function p(·). This implies a one-to-one correspondence between any shift invariant kernel and a
randomfeaturesbasis.
Example5(RandomFourierbasis) The random Fourier basis consists of sinusoidal nonlinear-
ities of the form b(s;ϑ)=(cid:2) cos(ϑ⊤s) sin(ϑ⊤s)(cid:3) . When ϑ is sampled from a Gaussian distribu-
tion, i.e., p(ϑ)∼N(0,2γI ), then the random Fourier basis approximates the radial basis func-
d
tion(RBF)kernelk(s,s′)= e− γ1∥s−s′∥2 2.
Therandomizednonlinearexpansionsprovideacompactandcomputationallyefficientalterna-
tive to the RKHS representations. This is particularly attractive when the number of data points is
large. Recall from the prior section the feature matrix Φ∈RN×D appearing in the prediction (3).
4RFAPPROXIMATIONFORCONTROL
Since ϕ(s)∈RD and Φ⊤Φ is a D×D matrix, the computation only depends on the dimension of
our feature space. Hence, we can compute a random feature approximation in O(ND2) time and
O(ND)memory,whichiscomputationallyattractivewhenD<N.
3. RandomFeaturesforControl-AffineModelling
Inthissection,weuseideasfromkernelregressionandrandomfeatureapproximationstopropose
representations which capture the control-affine structure from Definition 1. We first present two
general approaches for defining basis functions that are affine in the control variable. Then, we
presentRKHSrepresentationguaranteesbyshowingthattherandombasisapproximatesparticular
kernels. Finally,wepresentexperimentswhichillustratethepredictivemodellingcapabilitiesofthe
proposedmethods.
3.1. Control-AffineBasisFunctions
The control-affine modelling problem (Definition 1) allows for complex dependence on the state
variable, but imposes a restriction on the control variable. Given any arbitrary state-dependent
basesψ :X→RD fori = 1,...,m+1,weproposethefollowingtwobasisfunctionsthatareaffine
i
inthecontrolvariableu.
Definition3(Affinedotproduct(ADP)bases) Thebasisϕ :X×U→RD(m+1),givenby
c
ϕ (x,u) = (cid:2) u ψ (x)⊤ ... u ψ (x)⊤ ψ (x)⊤(cid:3)⊤ ,
c 1 1 m m m+1
istheADPbasisofm+1individualbasisfunctionsψ :X→RD,i = 1,...,m+1.
i
Asweshowinthefollowingsection(seeTheorem6),theADPbasesapproximatetheaffinedot
product(ADP)kernel,whichwasfirstproposedbyCastan˜edaetal.(2021). NotethattheADPbases
canalsobewrittenastheproductofblkdiag(ψ (x),...,ψ (x))withthevector[u⊤ 1]⊤. This
1 m+1
basisexpandsthefeaturedimensionforeverydimensionofthecontrolinput,resultingindimension
whichscalesbym+1. Thisobservationmotivatesasecondproposedrepresentation.
Definition4(Affinedense(AD)bases) Thebasisϕ :X×U→RD,givenby
d
(cid:20) (cid:21)
(cid:2) (cid:3) u
ϕ (x,u) = ψ (x)...ψ (x)
d 1 m+1 1
istheADbasisofm+1individualbasisfunctionsψ :X→RD,i = 1,...m+1.
i
ComparedwiththeADPbasis,theADbasisismorecompact. Forindividualbasisfunctionsof
dimension D, the AD basis will be of dimension D, whereas the ADP basis will be of dimension
D(m+1). Considering the linear regression use case, this means that AD has O(ND2) time and
O(ND)memorycomplexity,whereasforADPitisO(N(m+1)2D2)andO(N(m+1)D).
LeveragingideasfromrandomFourierfeatures,weproposecontrol-affinebasisfunctionscon-
structedwithstate-dependentrandomFourierbasisfunctionsψ fori = 1,...,m+1:
i
ψ (x) := (cid:112) 2/D(cid:104) sin(ϑ⊤ x) cos(ϑ⊤ x) ... sin(ϑ⊤ x) cos(ϑ⊤ x)(cid:105)⊤ , (5)
i i,1 i,1 i,D/2 i,D/2
5RFAPPROXIMATIONFORCONTROL
D/2
where weights {ϑ } are drawn i.i.d. from the distribution p (ϑ) for i = 1,...,m+1. As
i,j j=1 i
described in Example 5, each of these individual basis functions approximates a shift invariant
kernelcorrespondingtotheFouriertransformofthedensityp (ϑ)(RahimiandRecht,2008,2007).
i
Inotherwords,E [ψ (x)⊤ψ (x′)] = k (x−x′)wherek (v) = F(p (ϑ))[v].
ϑ∼pi(·) i i i i i
3.2. RepresentationGuarantees
Wenowdeveloprepresentationguaranteesforthecompoundbasesbyshowingwhichkernelsthey
approximate. Theaffinedotproduct(ADP)kernelwasfirstproposedbyCastan˜edaetal.(2021)for
systemswithcontrol-affinedynamics.
Definition5(Affinedotproduct(ADP)kernel) Definek :X×U×X×U→R,givenby
c
k ((x,u),(x′,u′)) := (cid:2) u⊤ 1(cid:3) diag(k (x,x′),··· ,k (x,x′))(cid:2) u′⊤ 1(cid:3)⊤ ,
c 1 m+1
astheAffineDotProduct(ADP)compoundkernelofm+1individualkernelsk :X×X→R.
i
ThefollowingtheoremshowsthattheADPbasisapproximatestheADPkernel.
Theorem6(ADPApproximation) Fori = 1,...,m+1,supposethebasisfunctionsψ aredefined
i
according to (5) with p the inverse Fourier transform of a shift invariant kernel k . Let ϕ be the
i i c
ADPcompoundbasisofψ andletk thecompoundADPkernelofk . Then
i c i
E[ϕ (x,u)⊤ϕ (x′,u′)] = k ((x,u),(x′,u′)).
c c c
TheresultfollowsbyrelatingthedotproductoffeaturestothediagonalmatrixintheADPkernel.
WedeferallformalproofstoAppendixA.
An alternative way to understand the ADP random feature approximation is to interpret the
(m+1)×(m+1) diagonal matrix of kernels as an operator valued kernel. This operator valued
kernel is the sum of m+1 decomposable kernels, as defined by Brault et al. (2016) (Definition 3).
TheADPblockdiagonalmatrixofbasisfunctionscanbeinterpretedthroughtheirframeworkasa
randomfeatureapproximationforthisoperatorvaluedkernel.
Wenowturntotheaffinedensebasis. First,wedefineanovelAffineDensecompoundkernel.
Definition7(Affinedense(AD)kernel) Form+1individualkernelsk :X×X→R,letD(x,x′)
i
bethediagonalmatrixwithithentryask (x−x′),andA(x,x′)amatrixwithzeroonthediagonal
i
and[A(x,x′)] = k (x)k (x′)fori ̸= j ∈ [m+1]. Then,definetheAffineDense(AD)compound
ij i j
kernelask : X×U×X×U→R,givenby
d
k ((x,u),(x′,u′)) := (cid:2) u⊤ 1(cid:3)(cid:0) D(x,x′)+A(x,x′)(cid:1)(cid:2) u′⊤ 1(cid:3)⊤ .
d
NoticethatthediagonalmatrixD(x,x′)intheADkernelissimilartotheADPkernel. How-
ever,theADkerneladditionallyincludesthedensematrixA(x,x′). Duetothisseconddenseterm,
theADcompoundkernelisnotshiftinvariantinx. Asaresult,itisnotpossibletoviewA+D as
ashiftinvariantoperator-valuedkernel,andthustheresultsofBraultetal.(2016)cannotbeusedto
derive a random features approximation. Furthermore, it is not immediately clear whether the AD
kernelisindeedavalidreproducingkernel. Wethereforebeginbyshowingthatitis.
6RFAPPROXIMATIONFORCONTROL
Theorem8(ADkernel) Let k ((x,u),(x′,u′)) be as in Definition 7. Suppose each k (x,x′) is
d i
anormalizedshiftinvariantreproducingkernel. Then,k ((x,u),(x′,u′))isareproducingkernel.
d
Toprovethisresult,weusethecrucial(butnon-obvious)claimthatifk(x,x′)isanormalized
shiftinvariantreproducingkernel,thenk(x,x′)−k(x)k(x′)isalsoareproducingkernel. Toprove
that the claim is true, we construct an explicit feature mapping of the form required in Lemma 2.
With the claim in hand, the proof follows by algebraic manipulations and the fact that the set of
reproducing kernels is closed under addition. Therefore, the AD kernel k is a reproducing kernel
d
andthusdefinesaRKHS.WenextshowthattheADbasisfunctionsapproximatethisRKHS.
Theorem9(Affine-densekernelapproximation) Supposethatfori = 1,...,m+1thebasisfunc-
tionsψ aredefinedaccordingto(5)withp theinverseFouriertransformofashiftinvariantkernel
i i
k . Let ϕ be the AD compound basis of ψ and let k be the compound AD kernel of k . Then
i d i d i
E[ϕ (x,u)⊤ϕ (x′,u′)] = k ((x,u),(x′,u′)).
d d d
So far our results show that the basis functions we propose approximate kernel regression in
expectation. When the dimension D is large enough, the approximation error can be bounded
with high probability (Rahimi and Recht, 2007; Sutherland and Schneider, 2015). In particular,
Sutherland and Schneider (2015) show conditions under which the pointwise approximation error
isnomorethanϵwithprobabilitydependingonDandϵ. Wethereforeconcludewitharesultwhich
shows that when the individual kernels have bounded approximation errors, so do the compound
kernels. InAppendixB.3,wefurtherderiveboundsonthepredictionerrorsandconfidenceintervals
foruseinrobustcontrol.
Proposition10(KernelApproximationErrors) Consider the ADP kernel k (s,s′) and the AD
c
kernelk (s,s′)fromDefinitions5and7,respectively. Consider{k (x)}m+1 whicharetheindivid-
d i i=1
ual kernels used to construct the ADP and the AD kernels. Recall ψ from 5. Suppose |k (x)|≤1
i
and|k (x)−ψ (x)⊤ψ (x)|≤ϵforallx ∈ X andi∈[m+1]. Then,wehave
i i i
max{|k (s,s′)−ϕ (s)⊤ϕ (s′)|, |k (s,s′)−ϕ (s)⊤ϕ (s′)|} ≤ ϵ(u⊤u′+1) (6)
c c c d d d
whereϕ andϕ aredefinedinTheorems6and9respectively.
c d
3.3. Numericaldemonstration
In this section, we empirically1 study the performance of the the two random features methods
(ADP-RFandAD-RF)aswellasthecorrespondingkernelmethods(ADP-KandAD-K).Wefocus
onperformanceintermsofpredictionaccuracy. Inthenextsection,wealsodemonstratetheutility
ofthesemodelsfordata-drivencontrol.
We consider a prediction task relating to a double pendulum with actuation at both joints. The
state of this system x∈R4 consists of two angle variables and two angular velocities, while the
control input u ∈ R2 consists of the two joint actuation torques. In appendix E.1, we present a
full derivation of the dynamics equation, which is affine in the control inputs. We simulate the
system under closed-loop control and sample at 10 Hz. The controller is imperfectly designed to
bringthesystemtoanuprightandbalancedconfiguration;Furthercontrollerdetailsaredeferredto
1.Codeisavailableathttps:github.com/kimzemian/swift_affine_mastery
7RFAPPROXIMATIONFORCONTROL
Figure1: Evaluationofmodels,comparingpredictionaccuracyontestdata(left)andtrainingtime
on8859points(right)forapredictionproblemonadoublependulumsystem. Horizontal
linesandmarkerscorrespondtokernelmethods. Randomfeaturesaresampled10times
at varying dimensions; the left panel displays median and quartiles over the trials while
therightpanelshowsthemean. Increasingthefeaturedimensionofeachstate-dependent
basisψ (x)resultsinlowerRMSEbutlongertrainingtime,especiallyforADP-RF.
i
the following section. We collect a dataset containing 226 trajectories, each starting at a different
initialpointandlasting5seconds. Thedatasetifoftheform{{(xe,ue),ze}L }E wherez isthe
i i i i=1 e=1 i
time derivative of a scalar function of the state (Example 3); details are described in the following
section. We split the data into train and evaluation subsets with an 80/20 split, so the train size is
8859andtestsizeis2215,formulatingapredictiontaskoftheforminDefinition1.
We compare the performance of five models: three kernel methods (Vanilla-K, ADP-K, AD-
K)andtworandomfeaturesmethods(ADP-RF,AD-RF).Vanilla-KisanRBFkernel(Example5)
that operates on the concatenated state and input without any affine structure. ADP-K and AD-K
(Definitions 5 and 7) use RBF kernel on the state variable, whereas, ADP-RF and AD-RF are as
defined in Theorems 6 and 9 with the corresponding random Fourier bases (5) as in Example 5.
For all models, γ = 1 and λ = 1. Figure 1 plots the performance in terms of test accuracy and
trainingtime. TheleftpanelshowsmedianandquartileRMSEontheevaluationsplitandtheright
panelshowstheaveragetrainingtime. Thekernelsarerepresentedbyhorizontallinesandmarkers.
Vanilla-Kperformsworsethantheaffinekernelssinceitdoesnotcapturetheaffinestructure,while
AD-K and ADP-K have similar performance. For the RF models, we examine the effect of the
random features approximation of state-dependent samples ψ (x) of dimension D. ADP-RF has
i
lowererrorforsmallerfeaturedimension, butbothRFmethodsquicklyapproachtheperformance
of the kernel methods. For small D, the RF methods are both much faster. Train time increases
withD morequicklyforADP-RFthanAD-RF,astrainingADP-RFscalesquadraticallywithm+
1. Comparing the RF models, Figure 1 suggests that, although training with AD-RF is faster as
compared to ADP-RF, the later has smaller RMSE. We attribute this to the higher dimensionality
of the ADP compound basis, which allows for greater expressivity. In Appendix C, we present
extensiveexperimentswithsyntheticdatademonstratingtherelationshipbetweentrainingtimeand
RMSE.Theseshowthatforfixedtrainingtime,AD-RFoutperformsADP-RFonaccuracywhenD
issufficientlylarge,andthisperformanceadvantagegrowsasinputdimensionmincreases.
8RFAPPROXIMATIONFORCONTROL
4. CaseStudy: CertificateFunctionControl
A key motivation for our work is that the affine structure of our data-driven models is amenable
for use in control tasks. We therefore describe how to incorporate these models into a particular
approachtononlinearcontrol. Wethenevaluateclosedloopperformanceofourmodels.
Background Asacasestudy,wedemonstrateanonlinearcontroltechniquebasedoncontrolcer-
tificate functions as proposed by Taylor et al. (2021), which we refer to for a more rigorous and
precise introduction. This approach generalizes and unifies the use of control Lyapunov functions
(CLFs) to guarantee stability (Galloway et al., 2015) and control barrier functions (CBFs) to guar-
anteesafety(Amesetal.,2016). Certificatefunctioncontrolrequiresacontinuouslydifferentiable
certificate function C : Rn → R satisfying certain properties (Taylor et al., 2021), along with a
comparison function α : R → R . Then, an optimization-based state feedback controller can be
+
defined which will guarantee desired properties such as stability or safety by construction (Ames
etal.,2019). Givensome“desired”controlinputu (x),theCCFquadraticprogram(QP)is:
d
u∗(x) = argmin∥u∥2+c ∥u−u (x)∥2 (CCF-QP)
2 1 d 2
u∈Rm
s.t. ∇C(x)⊤(f(x)+g(x)u)+α(C(x)) ≤ 0.
(cid:124) (cid:123)(cid:122) (cid:125)
C˙(x,u)
Data-driven Control We now suppose that the dynamics f and g are unknown, so the CCF-QP
controllercannotbedirectlyimplemented. WeassumethatavalidCCFC andcomparisonfunction
αfortheunknowntruesystemisgiven2. Givenasampledtrajectory{(x ,u )}N ,weconstructa
i i i=1
control affine modelling problem for C˙ :X×U →R as described in Example 3. We therefore use
methodsdiscussedinSection3tocreateacontrol-affinemodelhˆ(x,u)whichcanbeusedinplace
of the unknown C˙(x,u) function in (CCF-QP) in a certainty equivalent (CE) manner. Because
the model hˆ is affine in u, the resulting optimization problem is still a QP. In Appendix E.2, we
provideadditionaldetailsonconstructingthisQPforbothkernelandRFmethods. Wealsodiscuss
methods for robust, rather than CE, data-driven control. The robust approach requires estimates
of uncertainty (e.g. as in Gaussian process regression) as well as the pointwise RF approximation
errors,andresultsinasecondorderconeprogram(SOCP),seeAppendixD.
Simulation experiments We simulate data-driven CCF control of the double pendulum intro-
duced in Section 3.3, where the goal is to swing up and balance in the upright position x=0 with
onlyanincorrectmodelofthedynamicsf˜,g˜.
Knowingonlyitsdegreeofactuation, wemaycon-
cludethatthedynamicsarefeedbacklinearizableandthereforewecandefineaControlLyapunov
Function (CLF) without the exact dynamics model (Taylor et al., 2019). Specifically, we define
C(x) = x⊤Px,α(x) = .725x,u (x)afeedbacklinearizingcontrollerfortheincorrectf˜,g˜,and
d
c = 25. FulldetailsareprovidedinAppendicesE.3,E.4.
1
Wefirstdefinea“nominal”QPcontrollerwhichselectsinputsaccordingto(CCF-QP)withthe
nominal model
f˜,g˜.
We use this controller to gather trajectories and define a dataset as described
inSection3.3. Wesubsamplethedataby1/5andderivedata-drivenmodelsofC˙(x,u)asoutlined
in the paragraph above. We consider four data-driven QP controllers using the four affine models:
AD-K,ADP-K,AD-RF,andADP-RF.Forthedata-drivencontrollers,weaugmenttheinitialdataset
2.Thisassumptionismetforfeedbacklinearizablesystemsaslongasthethedegreeofactuationofthetruedynamics
modelisknown(Tayloretal.,2019).Forexample,manyroboticsystemssatisfythisassumption.
9RFAPPROXIMATIONFORCONTROL
Figure2: Left: ThevalueoftheLyapunovfunctionC(x)overtimefornominal, oracle, anddata-
drivencontrollerswithinitialstate[2,0,0,0]. Right: Illustrationofthependulumconfig-
urationsovertime. Nominalfailstobalancethependulum;data-drivenmethodssucceed.
withepisodicdatacollection: werunthecontrollerfor10secondsat10Hz,retrain,andrepeatfor
ten episodes. The RF dimension is D = N/5 for N the size of the training data. Finally, we
compare the performance of the the nominal and data-driven methods with an “oracle” controller
that solves (CCF-QP) with the true dynamics. Figure 2 plots the system trajectory in terms of the
Lyapunov function C(x) and the pendulum configuration. While the nominal controller fails to
balancethependulum,thedata-drivencontrollerssucceedandaresimilartoeachother.
5. Conclusion
This work considers a control affine modelling problem and proposes two classes of random basis
functionsasasolution: ADPandAD.Therepresentationguaranteesofthesemethodsaremadefor-
malbyconnectiontokernelregressionincorrespondingRKHSs. Acasestudyinnonlinearcontrol
with CCF illustrates the utility of control affine models. Numerical experiments demonstrate the
performanceoftheRFandkernelmethodsintermsofaccuracy,computationtime,andclosedloop
controlperformance. InAppendix B.3, we additionallypresent uncertaintyestimates analogousto
Gaussianprocess(GP)regression,aswellasacorrespondingrobustdata-drivencontrol. Wehigh-
light that the approximation methods that we propose may be broadly of interest for any control
applicationwhichmakesuseofGPs.
Our work opens the door to many future questions of interest. It would be interesting to de-
velopkernelsandrandomfeaturestailoredtoparticularcontrolapplications. Onecouldexplorethe
application of our methods to additional control techniques, like feedback linearization or model
predictive control. It would be interesting to develop principled techniques for acquiring data, ex-
pandingfromthesimplewarmstartepisodicapproachthatweused. Furthermore,additionalmeth-
ods for approximating kernels would provide alternatives to speeding up kernel and GP regression
fordata-drivencontrol.
Acknowledgments
We thank Jason Choi for helping us understand their code base for ADP kernel. This work was
partly funded by NSF CCF 2312774 and NSF OAC-2311521, a LinkedIn Research Award, and a
giftfromWayfair.
10RFAPPROXIMATIONFORCONTROL
References
Aaron D Ames, Jessy W Grizzle, and Paulo Tabuada. Control barrier function based quadratic
programswithapplicationtoadaptivecruisecontrol. In53rdIEEEConferenceonDecisionand
Control,pages6271–6278.IEEE,2014.
Aaron D Ames, Xiangru Xu, Jessy W Grizzle, and Paulo Tabuada. Control barrier function based
quadraticprogramsforsafetycriticalsystems. IEEETransactionsonAutomaticControl, 62(8):
3861–3876,2016.
Aaron D Ames, Samuel Coogan, Magnus Egerstedt, Gennaro Notomista, Koushil Sreenath, and
Paulo Tabuada. Control barrier functions: Theory and applications. In 2019 18th European
controlconference(ECC),pages3420–3431.IEEE,2019.
ZviArtstein. Stabilizationwithrelaxedcontrols. NonlinearAnalysis: Theory,Methods&Applica-
tions,7(11):1163–1173,1983.
HaimAvron,MichaelKapralov,CameronMusco,ChristopherMusco,AmeyaVelingker,andAmir
Zandieh. Random fourier features for kernel ridge regression: Approximation bounds and sta-
tistical guarantees. In International conference on machine learning, pages 253–262. PMLR,
2017.
AlainBerlinetandChristineThomas-Agnan. ReproducingkernelHilbertspacesinprobabilityand
statistics. SpringerScience&BusinessMedia,2011.
Eric Bradford, Lars Imsland, and Ehecatl Antonio del Rio-Chanona. Nonlinear model predictive
controlwithexplicitback-offsforgaussianprocessstatespacemodels. In2019IEEE58thCon-
ferenceonDecisionandControl(CDC),pages4747–4754,2019.doi: 10.1109/CDC40024.2019.
9029443.
RomainBrault,MarkusHeinonen,andFlorenceBuc. Randomfourierfeaturesforoperator-valued
kernels. InAsianConferenceonMachineLearning,pages110–125.PMLR,2016.
Jack Caldwell and Joshua A. Marshall. Towards efficient learning-based model predictive control
via feedback linearization and gaussian process regression. In 2021 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), pages 4306–4311, 2021. doi: 10.1109/
IROS51168.2021.9636755.
Fernando Castan˜eda, Jason J Choi, Bike Zhang, Claire J Tomlin, and Koushil Sreenath. Gaussian
process-basedmin-normstabilizingcontrollerforcontrol-affinesystemswithuncertaininputef-
fects and dynamics. In 2021 American Control Conference (ACC), pages 3683–3690. IEEE,
2021.
FernandoCastan˜eda,JasonJ.Choi,BikeZhang,ClaireJ.Tomlin,andKoushilSreenath. Pointwise
feasibility of gaussian process-based safety-critical control under model uncertainty. In 2021
60th IEEE Conference on Decision and Control (CDC), pages 6762–6769, 2021. doi: 10.1109/
CDC45484.2021.9683743.
11RFAPPROXIMATIONFORCONTROL
Jason J Choi, Fernando Castan˜eda, Wonsuhk Jung, Bike Zhang, Claire J Tomlin, and Koushil
Sreenath. Constraint-guided online data selection for scalable data-driven safety filters in un-
certainroboticsystems. arXivpreprintarXiv:2311.13824,2023.
Kevin Galloway, Koushil Sreenath, Aaron D Ames, and Jessy W Grizzle. Torque saturation in
bipedalroboticwalkingthroughcontrollyapunovfunction-basedquadraticprograms. IEEEAc-
cess,3:323–332,2015.
Dimitrios Giannakis, Amelia Henriksen, Joel A Tropp, and Rachel Ward. Learning to forecast
dynamical systems from streaming data. SIAM Journal on Applied Dynamical Systems, 22(2):
527–558,2023.
Lukas Hewing, Juraj Kabzan, and Melanie N. Zeilinger. Cautious model predictive control using
gaussian process regression. IEEE Transactions on Control Systems Technology, 28(6):2736–
2743,2020. doi: 10.1109/TCST.2019.2949757.
Torsten Koller, Felix Berkenkamp, Matteo Turchetta, and Andreas Krause. Learning-based model
predictivecontrolforsafeexploration.In2018IEEEConferenceonDecisionandControl(CDC),
pages6059–6066,2018. doi: 10.1109/CDC.2018.8619572.
Sahin Lale, Kamyar Azizzadenesheli, Babak Hassibi, and Anima Anandkumar. Model learning
predictive control in nonlinear dynamical systems. In 2021 60th IEEE Conference on Decision
andControl(CDC),pages757–762.IEEE,2021.
Sahin Lale, Yuanyuan Shi, Guannan Qu, Kamyar Azizzadenesheli, Adam Wierman, and Anima
Anandkumar. Kcrl: Krasovskii-constrained reinforcement learning with guaranteed stability in
nonlineardynamicalsystems. arXivpreprintarXiv:2206.01704,2022.
Fei Li, Huiping Li, and Yuyao He. Adaptive stochastic model predictive control of linear systems
usinggaussianprocessregression. IETControlTheory&Applications,15(5):683–693,2021.
HoriaMania,MichaelIJordan,andBenjaminRecht. Activelearningfornonlinearsystemidentifi-
cationwithguarantees. arXivpreprintarXiv:2006.10277,2020.
Klaus-RobertMu¨ller,SebastianMika,KojiTsuda, andKojiScho¨lkopf. Anintroductiontokernel-
based learning algorithms. In Handbook of neural network signal processing, pages 4–1. CRC
Press,2018.
Richard M Murray and John Edmond Hauser. A case study in approximate linearization: The
acrobat example. Electronics Research Laboratory, College of Engineering, University of ...,
1991.
QuanNguyen,AyongaHereid,JessyWGrizzle,AaronDAmes,andKoushilSreenath. 3ddynamic
walking on stepping stones with control barrier functions. In 2016 IEEE 55th Conference on
DecisionandControl(CDC),pages827–834.IEEE,2016.
DanielPickem,PaulGlotfelter,LiWang,MarkMote,AaronAmes,EricFeron,andMagnusEgerst-
edt. The robotarium: A remotely accessible swarm robotics research testbed. In 2017 IEEE
InternationalConferenceonRoboticsandAutomation(ICRA),pages1699–1706.IEEE,2017.
12RFAPPROXIMATIONFORCONTROL
AliRahimiandBenjaminRecht. Randomfeaturesforlarge-scalekernelmachines. InProceedings
ofthe20thInternationalConferenceonNeuralInformationProcessingSystems, NIPS’07,page
1177–1184,RedHook,NY,USA,2007.CurranAssociatesInc. ISBN9781605603520.
AliRahimiandBenjaminRecht. Uniformapproximationoffunctionswithrandombases. In2008
46th annual allerton conference on communication, control, and computing, pages 555–561.
IEEE,2008.
Martin Schechter. Principles of functional analysis. Number 36. American Mathematical Soc.,
2001.
Bernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines,
regularization,optimization,andbeyond. MITpress,2018.
Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian pro-
cess optimization in the bandit setting: No regret and experimental design. arXiv preprint
arXiv:0912.3995,2009.
Dougal Sutherland and Jeff Schneider. On the error of random fourier features. Uncertainty in
ArtificialIntelligence-Proceedingsofthe31stConference,UAI2015,062015.
Andrew J Taylor, Victor D Dorobantu, Hoang M Le, Yisong Yue, and Aaron D Ames. Episodic
learningwithcontrollyapunovfunctionsforuncertainroboticsystems. In2019IEEE/RSJInter-
nationalConferenceonIntelligentRobotsandSystems(IROS),pages6878–6884.IEEE,2019.
Andrew J. Taylor, Victor D. Dorobantu, Sarah Dean, Benjamin Recht, Yisong Yue, and Aaron D.
Ames. Towardsrobustdata-drivencontrolsynthesisfornonlinearsystemswithactuationuncer-
tainty. In202160thIEEEConferenceonDecisionandControl(CDC),pages6469–6476,2021.
doi: 10.1109/CDC45484.2021.9683511.
Russ Tedrake. Underactuated Robotics. 2023. URL https://underactuated.csail.
mit.edu.
Geoffrey S Watson. Linear least squares regression. The Annals of Mathematical Statistics, pages
1679–1699,1967.
HolgerWendland. Scattereddataapproximation,volume17. Cambridgeuniversitypress,2004.
Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for machine learning,
volume2. MITpressCambridge,MA,2006.
13RFAPPROXIMATIONFORCONTROL
AppendixA. OmittedProofs
A.1. ProofofTheorem6
FirstnotethatfortheADPbasis,
ϕ (x,u)⊤ϕ (x′,u′) = (cid:2) u⊤ 1(cid:3) diag(ψ (x)⊤ψ (x′),··· ,ψ (x)⊤ψ (x′))(cid:2) u′⊤ 1(cid:3)⊤ .
c c 1 1 m+1 m+1
Theresultholdsbecausebyassumption,theexpectationofψ (x)⊤ψ (x′)equalsk (x−x′).
i i i
A.2. ProofofTheorem8
Consider the following claim: if k(x,x′) is a normalized shift invariant reproducing kernel, then
k(x,x′)−k(x)k(x′)isalsoareproducingkernel.
Fornowwetaketheclaimastrue. ThennoticethattheADkernelcanalsobewrittenas
k ((x,u),(x′,u′)) = (cid:2) u⊤ 1(cid:3) D˜(x,x′)(cid:2) u⊤ 1(cid:3)⊤ +(cid:2) u⊤ 1(cid:3) A˜(x,x′)(cid:2) u⊤ 1(cid:3)⊤ ,
d
forD˜(x,x′)adiagonalmatrixwithith entryask (x−x′)−k (x)k (x′),andA˜(x,x′)amatrix
i i i
whose entry at i,j is k (x)k (x′) for i,j ∈ [m + 1]. Since sums of kernels are also kernels, it
i j
sufficestoshowthateachtermisakernel. Iftheclaimaboveistrue,thenthefirsttermisaspecial
case of the ADP kernel, and is therefore a reproducing kernel by Lemma 3 of Castan˜eda et al.
(2021). Thesecondtermcanbedirectlywrittenasaninnerproduct(cid:2) u⊤ 1(cid:3) A˜(x,x′)(cid:2) u⊤ 1(cid:3)⊤ =
(cid:2) u⊤ 1(cid:3) ϕ(x)ϕ(x′)⊤(cid:2) u⊤ 1(cid:3)⊤ whereϕ(x) = [k (x) ... k (x)]⊤. Therefore,itisakernel.
1 m+1
It now remains to prove the claim. We proceed by showing that k(x,x′)−k(x)k(x′) can be
writtenastheinnerproductofsomeexplicitfeaturerepresentation. Letk(x,x′) = ⟨φ(x),φ(x′)⟩
for some feature function mapping to an arbitrary real Hilbert space H, φ : X → H. This must
existsincek isareproducingkernel(BerlinetandThomas-Agnan,2011). Then
k(x,x′)−k(x)k(x′) = ⟨φ(x),φ(x′)⟩−⟨φ(x),φ(0)⟩⟨φ(x′),φ(0)⟩ = ⟨φ(x),(I −P)φ(x′)⟩
whereI isidentityoperatorandP : H → HisdefinedasthelinearoperatorPv = φ(0)⟨φ(0),v⟩
forv ∈ H. WenowarguethatI −P isaboundedself-adjointpositiveoperator;itisbounded,i.e.
maps bounded subsets to bounded subsets, since I and P(by Cauchy Schwarz) are bounded; it is
positive,i.e.,thequadraticformv (cid:55)→ ⟨v,(I −P)v⟩ispositivesemi-definite:
⟨v,(I −P)v⟩ = ∥v∥2 −⟨φ(0),v⟩2 ≥ ∥v∥2 −(∥φ(0)∥ ∥v∥ )2 = 0.
H H H H
The first inequality holds by Cauchy Schwarz, and the final equality holds because the kernel is
normalized,i.e.,k(0) = ∥φ(0)∥2 = 1. Itisself-adjoint:
H
⟨v,(I −P)w⟩ = ⟨v,w⟩−⟨v,φ(0)⟩⟨φ(0),w⟩,
= ⟨Iv,w⟩−⟨φ(0)⟨φ(0),v⟩,w⟩ = ⟨(I −P)v,w⟩.
Therefore by theorem 13.15 in Schechter (2001), there exists a unique positive linear operator L
suchthatI −P = L2,andtherefore
k(x,x′)−k(x)k(x′) = ⟨φ(x),(I −P)φ(x′)⟩ = ⟨Lφ(x),Lφ(x′)⟩.
Therefore,wehaveconstructedanexplicitfeaturerepresentationwhichprovestheclaim.
14RFAPPROXIMATIONFORCONTROL
A.3. ProofofTheorem9
Firstnotethatbyassumption,theexpectationofψ (x)⊤ψ (x′)isequalk (x−x′). Thusitremains
i i i
to show that ψ (x)⊤ψ (x′) estimates k (x)k (x′) when i ̸= j. Recall that by assumption, ψ is
i j i j i
constructed using i.i.d. samples from the inverse Fourier transform of the kernel k , i.e., p (ϑ).
i i
Defineζ (x) = eiϑ⊤x. Thentheexpectationisgivenby,
ϑ
(cid:90)
E [ζ (x)ζ (x′)] = p (ϑ)p (ϑ′)eiϑ⊤xe−iϑ′⊤x′ dϑdϑ′,
ϑ,ϑ′ ϑ ϑ′ i j
ϑ,ϑ′∈Rn
(cid:90) (cid:90)
= p (ϑ)eiϑ⊤xdϑ p (ϑ′)e−iϑ′⊤x′ dϑ′,
i j
ϑ∈Rn ϑ′∈Rn
= E [ζ (x)]E [ζ (x′)] = k (x)k (x′).
ϑ ϑ ϑ′ ϑ′ i j
A.4. ProofofProposition10
TheproofofthispropositionisgiveninthefirsttwostepsoftheproofofProposition12.
AppendixB. GaussianProcessRegression
An important advantage of kernel methods is that they are amenable to theoretical guarantees and
uncertainty characterization. Gaussian Process (GP) regression (Williams and Rasmussen, 2006)
takes a Bayesian perspective, and provides posterior mean and variance estimates on function val-
ues. Confidenceintervalsofthisformcanbederivedforkernelpredictionseveninfrequentistset-
tings(Srinivasetal.,2009). PriorworkshavedevelopedrobustCCF-basedcontrollersforunknown
modelsbyincorporatingGaussianprocess(GP)regression(Castan˜edaetal.,2021;Castan˜edaetal.,
2021). We thus describe how to use our approximation methods for GP regression. These results
maybebroadlyofinterestforanycontrollerwhichmakesuseofGPs(Kolleretal.,2018;Caldwell
andMarshall,2021;Bradfordetal.,2019;Hewingetal.,2020;Lietal.,2021).
B.1. Background
This section presents an overview of regression methods which explicitly model the uncertainty.
Bayesianlinearregressionisaprobabilisticapproachtoregressionanalysisthatmodelstherelation-
shipbetweenasetofinputvectors{s }N ∈ S ⊆ Rdandtargetoutputvariables{z }N ∈ Z ⊆ R
i i=1 i i=1
as
z = h(s )+ϵ , h(s ) = s⊤w,
i i i i i
where w ∈ Rd represents the linear model and {ϵ }N i. ∼i.d. N(0,σ2 ) are the noise. In contrast
i i=1 N
to classical linear regression, Bayesian linear regression (BLR) not only estimates the parame-
ters of a linear model, but also provides a probabilistic interpretation of the model’s uncertainty.
More specifically, BLR treats the regression coefficients as random variables with a prior distribu-
tion,andcomputestheposteriordistributionoverthesecoefficientsgivenN input-outputdatapairs
{(s ,z )}N .
i i i=1
TheBayesianlinearmodelsuffersfromlimitedexpressiveness. Asimpleapproachtoovercome
this problem is to map the input vectors {s }N to a higher-dimensional feature space, where a
i i=1
linear relationship can be established more easily. If such a map exists, we call it a basis function.
15RFAPPROXIMATIONFORCONTROL
Specifically, let ϕ : Rd → RD maps an input vector s ∈ Rd to a feature vector ϕ(s) ∈ RD. Now
the model becomes h(s) = ϕ(s)⊤w. Further, let the matrix Φ ∈ RN×D and the vector z ∈ RN
be the aggregation of rows {ϕ(s )⊤}N and {z }N , respectively. Assuming a Gaussian prior on
i i=1 i i=1
the model weights, i.e., w ∼ N(0,Σ ), the posterior distribution of h(s ) at a query point s is
w ∗ ∗
h(s ) ∼ N(µ ,σ2)with,
∗ ∗ ∗
µ = ϕ⊤Σ Φ⊤(ΦΣ Φ⊤+σ2 I )−1z,
∗ ∗ w w N N
(7)
σ2 = ϕ⊤Σ ϕ −ϕ⊤Σ Φ⊤(ΦΣ Φ⊤+σ2 I )−1ΦΣ ϕ ,
∗ ∗ w ∗ ∗ w w N N w ∗
whereweusedtheshorthandϕ = ϕ(s ),µ = µ(s )andσ = σ(s )(WilliamsandRasmussen,
∗ ∗ ∗ ∗ ∗ ∗
2006). This distributional perspective characterizes the uncertainty in the model prediction. One
simple way to express the uncertainty is through a confidence interval. For Gaussian distributions,
confidenceintervalstaketheform,
|µ(s )−h(s )| ≤ βσ(s ),
∗ ∗ ∗
whereβ ≥ 0dependsonthelevelofconfidence. BelowinTheorem11,wepresentaformalversion
ofthisconfidenceboundthatholdseveninthefrequentistsetting,meaningthatitdoesnotdependon
the distributional assumptions or Bayesian priors. However, the validity of the confidence interval
doesdependonthechoiceofbasisfunctions,asthisdeterminesthecomplexityandrichnessofthe
modelled relationship between inputs s and outputs z. For data coming from a highly structured
process,itmaybereasonabletospecifyabasisofsmalldimension. However,ingeneralasuitable
compactbasismaynotbeknownapriori.
Kernel methods are used to allow for expressive basis functions of arbitrarily high or infinite
dimension. Using the kernel trick (Scholkopf and Smola, 2018; Mu¨ller et al., 2018), the posterior
(7)canbecomputedusingonlyinnerproductsofbasisfunctions. Akernelfunctionk : Rd×Rd →
R generalizes the idea of inner products between basis functions. Using kernel functions in this
context leads to the familiar Gaussian Process (GP) regression (Williams and Rasmussen, 2006).
GPs are commonly used as a nonparametric approach for representing complex functions. GP
regression corresponds to Bayesian regression in Reproducing Kernel Hilbert Spaces (RKHS), a
specificclassoffunctionspacedenotedasH (S)(Wendland,2004). TheRKHSisequippedwith
k
(cid:112)
the norm∥h(s)∥ := ⟨h(s),h(s)⟩and is dense in theset of continuous functions, meaning that
k
anycontinuousfunctioncanberepresentedarbitrarilywellbykernelregression.
In the RKHS setting, Srinivas et al. (2009) establishes the following frequentist confidence
interval.
Theorem11(Srinivasetal.(2009)) Assume that the noise sequence {ϵ }∞ is zero mean and
i i=1
uniformly bounded by σ . Let the target function h : S → R be a member of H (S) associated
N k
withaboundedkernelk,withitsRKHSnormboundedbyB. Then,withprobabilityatleast1−δ,
thefollowingholdsforalls ∈ S andN ≥ 1:
(cid:114)
N +1
|µ(s )−h(s )| ≤ 2B2+300γ +ln3( )σ(s ),
∗ ∗ N+1 ∗
δ
whereγ isthemaximuminformationgainaftergettingN +1datapoints.
N+1
The drawback of kernel methods is computation. Algorithms for fitting functions in an RKHS
to data have superlinear complexity in the number of data points. In particular, computing the
kernelapproximatorcanbeprohibitivelyexpensiveforlargedatasets. Solving(7)generallyrequires
O(N3)timeandO(N2)memory.
16RFAPPROXIMATIONFORCONTROL
B.2. Affineposterior
In this section, we explicitly show the affineness of predicted µ and σ in u as products of input-
dependentandindependentparts. WewillusethesecalculationinthecasestudyD.1tocontrolan
acrobatusingCCFfunctions. Throughout,wedefiney := [u⊤ 1]⊤.
Kernel methods Under BLR assumptions, given a set of finite measurements of features and
labelsoftheform{(s ,z )}N ,wherez = h(s )+ϵ ,andϵ ∼ N(0,λ2 ),aposteriordistribution
i i i=1 i i i i N
ofh(s)ataquerypoints := (x,u)canbederivedasfollows: h(x,u) ∼ N(µ (u),σ (u)2)with
x x
µ (u) := µ(x,u) = z⊤(K +λ2 I )−1k , (8)
x N N (x,u)
σ (u)2 := σ(x,u)2 = k((x,u),(x,u))−k⊤ (K +λ2 I )−1k , (9)
x (x,u) N N (x,u)
whereK ∈ RN×N istheGrammatrixwhoseentryati,jisgivenby[K] = k((x ,u ),(x ,u ))
i,j i i j j
for i,j ∈ [N]. Further, k = [k((x,u),(x ,u )) ··· k((x,u),(x ,u ))]⊤ ∈ RN, and
(x,u) 1 1 N N
z ∈ RN isthevectorcontainingtheoutputmeasurementsz = h(s )+ϵ fori ∈ [N].
i i i
In the following, we will show the affineness of µ (u) and σ (u)2, when k((x,u),(x′,u′))
x x
is an AD kernel (Definition 7). We refer to Castan˜eda et al. (2021) section 5 for the case of ADP
kernel (Definition 5). To proceed let X = {x ,...,x } and Y = {y ,...,y } be the two sets
1 N 1 N
containingthetrainingdata,wherey = [u⊤ 1]⊤. LetA(x,x′)andD(x,x′)beasinDefinition7,
i
andsetM(x,x′) := D(x,x′)+A(x,x′). Further,define
k = blkdiag(y⊤,...,y⊤)[M(x ,x)⊤ ... M(x ,x)⊤]⊤.
train 1 N 1 N
It’simmediatethatk = k y. Therefore,
(x,u) train
µ (u) = z⊤(K +λ2 I )−1k y,
x N N train
(cid:124) (cid:123)(cid:122) (cid:125)
=:ΞADK(X,Y)
(cid:20) (cid:21)
u
= Ξ (X,Y) ,
ADK 1
σ (u)2 = y⊤M(x,x)y−y⊤k⊤ (K +λ2 I )−1k y,
x train N N train
= y⊤(cid:2) M(x,x)−k⊤ (K +λ2 I )−1k (cid:3) y,
train N N train
(cid:124) (cid:123)(cid:122) (cid:125)
=:GADK(X,Y)
(cid:20) (cid:21)
u
= [u⊤ 1]G (X,Y) .
ADK 1
Sinceeverynonzerorealvectorcanbescaledtohave1asthelastentry,andfromaboveσ (u)2 =
x
(cid:20) (cid:21)
u
[u⊤ 1]G (X,Y) foreveryu,G (X,Y)ispositivesemi-definite.
ADK 1 ADK
(cid:13) (cid:20) (cid:21)(cid:13)
(cid:13) u (cid:13)
=⇒ σ x(u) = (cid:13) (cid:13)Ω ADK(X,Y)
1
(cid:13)
(cid:13)
.
2
Random basis methods Recall that using random features, the posterior mean and covariance
canbeapproximatedby
µˆ (u) = φ(x,u)⊤(Φ⊤Φ+λ I )−1Φ⊤z, (10)
x N D
σˆ (u)2 = λ φ(x,u)⊤(Φ⊤Φ+λ I )−1φ(x,u). (11)
x N N D
17RFAPPROXIMATIONFORCONTROL
ADP random features: From Definition 3, we know φ(x,u) = blkdiag(ψ (x),...,ψ (x))y.
1 m+1
DefineΨ (x) := blkdiag(ψ (x),...,ψ (x)). Asaresult
ADP 1 m+1
µˆ (u) = y⊤Ψ(x)⊤(Φ⊤Φ+λ I )−1Φ⊤z,
x N D
(cid:124) (cid:123)(cid:122) (cid:125)
ΞADRF(X,Y)
= [u⊤ 1]Ξ (X,Y),
ADRF
σˆ (u)2 = y⊤λ Ψ(x)⊤(Φ⊤Φ+λ I )−1Ψ(x)y,
x N N D
(cid:124) (cid:123)(cid:122) (cid:125)
GADPRF(X,Y)
(cid:20) (cid:21)
u
= [u⊤ 1]λ G (X,Y) ,
N ADPRF 1
(cid:13) (cid:20) (cid:21)(cid:13)
(cid:13) u (cid:13)
=⇒ σˆ x(u) = (cid:13) (cid:13)Ω ADPRF(X,Y)
1
(cid:13)
(cid:13)
.
2
ADrandomfeatures: FromDefinition4,weknowthatφ(x,y) = [ψ (x) ... ψ (x)]y. Define
1 m+1
Ψ (x) := [ψ (x) ... ψ (x)]. Then,similartoADPrandomfeatures,wehave,
AD 1 m+1
µˆ (u) = [u⊤ 1]Ξ (X,Y),
x ADRF
(cid:20) (cid:21)
u
σˆ (u)2 = [u⊤ 1]λ G (X,Y) ,
x N ADRF 1
(cid:13) (cid:20) (cid:21)(cid:13)
(cid:13) u (cid:13)
=⇒ σˆ x(u) = (cid:13) (cid:13)Ω ADRF(X,Y)
1
(cid:13)
(cid:13)
.
2
B.3. Errorbounds
Forthepurposesofrobustcontrol,itisnecessarytotrackhowtheapproximationerroraccumulates
inourcomputationoftheposterior.
SutherlandandSchneider(2015)showsthatwithprobability1−δ, sup|ψ(x)⊤ψ(x)−k(x)| ≤
x∈X
(cid:104) (cid:105)
ϵforD ≥ 8(d+2αϵ) 2 log σpl +logβ d ,wherelisthediameterofX,σ2 = E ∥ω∥2 andβ ,α
ϵ2 1+2 ϵ δ p p d e
d
aredefinedinProposition1ofSutherlandandSchneider(2015).
Definek ∈ RN tobeavectorcontainingthekernelk(s ,s)fori = 1,...,N. LetU ∈ RN×m
s i
beamatrixwithrows{u⊤}N . Wegetthefollowingerrorbound.
i i=1
Proposition12 Assume each i-th element of φ (14) is a member of H with bounded RKHS
C ki
norm,fori = 1,...,m+1. AssumeeithertheADorADPkernelwithboundedkernelsk andthat
√ i
wehaveaccesstomeasurementsz. Assume∥k ∥ ≤ Nκandthatλ = nλ. Letσ bethemax
s n max
singularvalueofU. Thenwithaprobabilityof1−(δ +δ )wehave:
1 2
|C˙(x,u)−µˆ (u)| ≤ βσˆ (u)+ϵ(ν∥u ∥+ιv|u ∥2+∆)
x x x x
whereν := σ √m Na λx(σ
n
√+ √2β Nκ +2βϵ),ι = βϵ Nσ m2 λax,
∆ = β∆
σ
+(βκ+ Nσ n)∆ µ,∆
µ
= √1 [1+ κσ √max + √κ ],∆
σ
= 1+ϵ+ √κ .
λ N N Nλ Nλ Nλ
Proof wesplittheprooftoseveralsteps:
18RFAPPROXIMATIONFORCONTROL
1. ApproximatingADkernelpointwise:
|k (x)k (x′)−kˆ (x)kˆ (x′)| = |k (x)(k (x′)−kˆ (x′))+(k (x)−kˆ (x))kˆ (x)|,
i i i i i i i i i i
≤ ϵmax(|k (x)|,|kˆ (x′)|),
i i
≤ ϵ, (12)
where we used the assumption that |k (x)| ≤ 1 for all x ∈ X and all i ∈ [m + 1]. Let
i
E := A + D = [e ] , that is, e is the element in ith row and jth column of E,
ij {i,j} ij
where A,D are as in Definition 7. Using (12), we conclude that e − eˆ ≤ ϵ for all
ij ij
1 ≤ i,j ≤ m+1. Thisfurtherimplies,
(cid:88)
|k ((x,u),(x′,u′))−kˆ ((x,u),(x′,u′))| = | y (e −eˆ )y′|
d d i ij ij j
1≤i,j≤m+1
≤ ϵ(u⊤u′+1),
where k ((x,u),(x′,u′)) is in Definition 7 and y ,y′ denote the ith and jth elements of
d i j
y := [u⊤ 1]⊤ andy′ := [u′⊤ 1]⊤,respectively.
2. ApproximatingADPKernelpointwise:
(cid:88)
|k ((x,u),(x′,u′))−kˆ ((x,u),(x′,u′))| = | y (e −eˆ )y′|
c c i ii ii i
1≤i≤m+1
≤ ϵ(u⊤u′+1),
3. ApproximatingADP,ADkernelmatrices:
since we have the same bound on the estimation error of ADP kernel and AD kernel, the
followingproofsholdforbothkernels:
∥K −Kˆ∥ ≤ ϵ∥[u⊤u +1] ∥ ≤ ϵσ2 +ϵ(m+1)
2 i j i,j 2 max
√ √
∥k −kˆ ∥ ≤ ϵ∥[u⊤u +1] ∥ ≤ ϵ∥u ∥.∥[u ,...,u ]∥+ϵ N ≤ ϵσ ∥u ∥+ϵ N
s s x i i x 1 N max x
4. Approximatingthemean:
|µ (u)−µˆ (u)| = ∥z⊤∥∥(Kˆ +λ I)−1(k −kˆ )+((K +λ I)−1−(Kˆ +λ I)−1)k ∥
x x n s s n n s
∥z∥ ∥z∥.∥Kˆ −K∥
≤ ∥kˆ −k ∥+ ∥k ∥
λ s s λ2 s
√n √ n
Nσ Nσ κ
≤ n ∥kˆ −k ∥+ n ∥Kˆ −K∥
Nλ s s n2λ2
√ √
Nσ √ Nσ κ
≤ n (ϵσ ∥u ∥+ϵ N)+ n (ϵσ2 +ϵN)
Nλ max x N2λ2 max
σ σ √
max n
≤ ϵ∥u ∥ √ + Nσ ∆
x n µ
Nλ
Whereweused(K+λI)−1−(Kˆ +λI)−1 = (Kˆ +λI)−1(Kˆ −K)(K+λI)−1,andthat
thesmallesteigenvalueofKˆ +λI andK +λI isatleastλ.
19RFAPPROXIMATIONFORCONTROL
5. Approximatingvariance: Assumeσ (u)+σˆ (u) ≤ 1
x x
|σ (u)−σˆ (u)| ≤ |σ (u)2−σˆ (u)2|
x x x x
≤ ϵ+|k [(K +λ)−1k⊤−(Kˆ +λ)−1kˆ⊤]+[k −kˆ ](Kˆ +λ)−1kˆ⊤|
s s s s s s
√
|µ (u)−µˆ (u)| ϵσ ∥u ∥+ϵ N
≤ ϵ+∥k ∥ x x + max x ∥kˆ ∥
s s
∥z∥ Nλ
σ2
≤ ϵ+κ(ϵ∥u ∥ max +∆ )
x µ
Nλ
√
ϵσ ∥u ∥+ϵ N √
+ max x (κ+ϵσ2 ∥u ∥+ϵ N)
Nλ max x
2ϵσ2 κ ϵ2σ2
≤ ∥u ∥ √ max(√ +ϵ)+∥u ∥2 max +∆ +κ∆
x x σ µ
Nλ N Nλ
6. Boundsontotalerror: withaprobabilityof1−(δ +δ ):
1 2
|C˙ (u)−µˆ (u)| ≤ |µ (u)−C˙ (u)|+|µ (u)−µˆ (u)|
x x x x x x
≤ βσˆ (u)+β|σ (u)−σˆ (u)|+|µ (u)−µˆ (u)|
x x x x x
≤ βσˆ (u)+∥u ∥ν +∥u ∥2ι+∆
x x x s
AppendixC. EmpiricalCompoundRandomBasisComparison
In this Appendix, we perform experiments with synthetic data to demonstrate the relationship be-
tweentrainingtimeandRMSEforADP-RFandAD-RFmodels. Specifically,weusethefollowing
control-affinefunctiontogeneratethedata,
m
(cid:88)
h (x,u) = 3sin(2πx⊤w )−2sin(4πx⊤w )+ (γ sin(2πx⊤w ))u +ϵ, (13)
m 1 2 j j+2 j
j=1
wherew ,w ∈ Rn parameterizef(x) := 3sin(2πx⊤w )−2sin(4πx⊤w ), and{w }m ∈
1 2 1 2 j+2 j=1
Rn parameterizeg(x) := [γ sin(2πx⊤w )γ sin(2πx⊤w ) ··· γ sin(2πx⊤w )],suchthat
1 3 2 4 m m+2
h (x,u) = f(x)+g(x)u+ϵ is affine in u. All the (entries of) weights w and γ are sampled
m
uniformlyatrandomfrom[0,1).
We use the function (13) to generate data for varied input dimension m = 1,...,20. For each
m = k, to generate h (x,u), we use the previous weights w ,...,w ,γ ,...,γ and only
m 1 k+1 1 k−1
generatenewweightsforj = k,thatisw andγ .
k+2 k
For each input dimension, we sample 1000 values of x ∈ R6 and u ∈ Rm uniformly at
i i
randomfrom[0,1). Foreachi,wesetthelabely = h (x ,u )+ϵ whereϵ ∼ N(0,0.01). Using
i m i i i i
this dataset, we train and evaluate AD-RF, ADP-RF on a 90/10 train/test split. For all methods,
we use λ = 1 and rbf γ = 1. To choose feature dimensions, we vary the state-dependent basis
dimension starting from 22, and is incremented by 22 for 10 steps; this is roughly chosen based
√
on the widely considered best random features dimension of nlogn. The ADP compound basis
20RFAPPROXIMATIONFORCONTROL
Figure3: Evaluationofmodels,comparingpredictionaccuracyon100testdatapointsagainstav-
erage training time on 900 data points for m = 1,10,20 respectively. Random features
aresampled10timesatmatchingcompounddimensions;thepanelsdisplaymedianand
quartileRMSEoverthetrials. IncreasingmdemonstratestheadvantageofAD-RFbasis
overADP-RFinRMSEforfixedtrain-time.
dimensionwouldbethemultiplicationofthesedimensionsbym+1. Forfaircomparison,wemake
surethatboththecompoundbasisdimensionsmatch. Ateachdimension,weresampletherandom
features10times,andrecordthetrainingtimeandtestRMSE.
Figure 3 plots the median and quartile RMSE against the average training time. For lower
dimensionalinputsu,atagiventraintime,ADP-RFandAD-RFperformsimilarly. Howeverwith
increasing m, the performance gap between AD-RF and ADP-RF becomes larger. Specifically
at larger m’s, AD-RF converges faster at a lower RMSE; This means AD-RF reaches its optimal
RMSE at a lower train-time, which implies that at lower feature dimensions, AD-RF captures the
complexity of the model, whereas to ensure the same for ADP-RF, we need more complex and
higherdimensionalfeatures.
AppendixD. RobustCCFControl
In Section 4, we present a certainty-equivalent approach to data-driven control using CCF control
asacasestudy. Here, weadditionallypresentarobust approach. Itallowsforsynthesizingadata-
drivencontrollawwhichrobustlyenforcestheconstraintinCCF-QP.
D.1. RobustBayesianData-drivenController
In this section, we show how to construct a robust data driven control law given the control-affine
basis functions (introduced in Section 3.1). Similar to Section 4, we suppose that the dynamics f
andg areunknown,sotherobustCCFcontrollercannotbedirectlyimplemented. Weassumethat
avalidCCFC andcomparisonfunctionα fortheunknowntruesystemisgiven. Givenasampled
trajectory {(x ,u )}N , we construct a control affine modelling problem for C˙ : X×U → R as
i i i=1
described in Example 3. Specifically, we use GP regression to model the uncertainty and compute
the mean µ (u) := µ(s) and variance σ (u) := σ(s) according to (7). As first proposed in the
x x
GP context by Castan˜eda et al. (2021), we make use of the 1−δ confidence interval presented in
Theorem 11 to construct an optimization-based controller. The data-driven min-norm stabilizing
21RFAPPROXIMATIONFORCONTROL
feedbackcontrollawu∗: X → U isdefinedas
u∗(x) = argmin∥u∥2 (BLR-CCF-SOCP)
2
u∈U
s.t. µ (u)+βσ (u)+α(C(x)) ≤ 0
x x
This optimization problem will be a Second-Order Cone Program (SOCP) as long as the con-
straint is a conic in u. This follows from the form of the mean and variance function, and can be
guaranteed as long as the basis function ϕ is affine in u. This is a natural requirement due to the
affinestructureofthedynamics.
C˙(x,u) = ∇C(x)⊤f(x)+(∇C(x)⊤g(x))u,
(cid:20) u(cid:21) (14)
= φ
C 1
whereφ ∈ R1×(m+1).
C
When a random features approximation is used, there is an additional error term that much be
accounted for in proper robust control. Leveraging the error analysis presented in Section B.3, we
present(RF-CCF-SOCP)whichisbothcomputationallyefficientandrobust.
u∗(x) = argmin∥u∥2 (RF-CCF-SOCP)
2
u∈Rm
s.t. µˆ (u)+βσˆ (u)+ϵ(ν∥u ∥+ι∥u ∥2+∆)+α(C(x)) ≤ 0
x x x x
AppendixE. ExperimentalDetails
E.1. Doublependulumdynamicsderivation
Weconsiderafrictionlesstwo-linkpendulumwithtorqueτ appliedatafixedbase,wherethefirst
1
link is attached, and torque τ applied at the opposite end, where the second link is attached. The
2
linksaremodeledaspointmassesm andm atlengthsl andl fromthejoints. Wedefineθ as
1 2 1 2 1
theangleofthefirstlink,measuredfromtheuprightpositive,andθ astheangleofthesecondlink,
2
measuredfromthefirstlink.
Thecorrespondingangularratesareθ˙ andθ˙
.
1 2
Lettingq = [θ ,θ ],thetotalkineticenergyofthesystemisgivenby
1 2
1
T(q,q˙) = ((m +m )l2+2m l l cosθ +m l2)θ˙2+(m l l cosθ +m l2)θ˙ θ˙
2 1 2 1 2 1 2 2 2 2 1 2 1 2 2 2 2 1 2
1
+ m l2θ˙2, (15)
2 2 2 2
andthepotentialenergyofthesystemisgivenby
U(q) = (m +m )gl cosθ +m gl cos(θ +θ ). (16)
1 2 1 1 2 2 1 2
Asaresult,theLagrangianofthesystemtakesthefollowingform,
L(q,q˙) = T(q,q˙)−U(q),
1
= ((m +m )l2+2m l l cosθ +m l2)θ˙2+(m l l cosθ +m l2)θ˙ θ˙ ,
2 1 2 1 2 1 2 2 2 2 1 2 1 2 2 2 2 1 2
1
+ m l2θ˙2−(m +m )gl cosθ −m gl cos(θ +θ ). (17)
2 2 2 2 1 2 1 1 2 2 1 2
22RFAPPROXIMATIONFORCONTROL
NowwecanwritetheLagrangeequationsasfollows
d ∂L(q,q˙) ∂L(q,q˙)
− = τ. (18)
dt ∂q˙ ∂q
Thepartialderivativesaregivenby
∂L(q,q˙)
= ((m +m )l2+2m l l cosθ +m l2)θ˙ +(m l l cosθ +m l2)θ˙ ,
∂θ˙ 1 2 1 2 1 2 2 2 2 1 2 1 2 2 2 2 2
1
∂L(q,q˙)
= (m +m )gl sinθ +m gl sin(θ +θ ),
1 2 1 1 2 2 1 2
∂θ
1
∂L(q,q˙)
= (m l l cosθ +m l2)θ˙ +m l2θ˙ ,
∂θ˙ 2 1 2 2 2 2 1 2 2 2
2
∂L(q,q˙)
= −m l l sinθ θ˙2−m l l sinθ θ˙ θ˙ +m gl sin(θ +θ ).
∂θ 2 1 2 2 1 2 1 2 2 1 2 2 2 1 2
2
Thisfurtherimplies,
d ∂L(q,q˙)
= ((m +m )l2+2m l l cosθ +m l2)θ¨ +(m l l cosθ +m l2)θ¨
dt ∂θ˙ 1 2 1 2 1 2 2 2 2 1 2 1 2 2 2 2 2
1
−2m l l sinθ θ˙ θ˙ −m l l sinθ θ˙2,
2 1 2 2 1 2 2 1 2 2 2
d ∂L(q,q˙)
= (m l l cosθ +m l2)θ¨ +m l2θ¨ −m l l sinθ θ˙ θ˙ .
dt ∂θ˙ 2 1 2 2 2 2 1 2 2 2 2 1 2 2 1 2
2
Withtheseresults,wecanwritethefollowingLagrangeequations
d ∂L(q,q˙) ∂L(q,q˙)
− = τ ,
dt ∂θ˙
1
∂θ
1
1
=⇒ ((m +m )l2+2m l l cosθ +m l2)θ¨ +(m l l cosθ +m l2)θ¨
1 2 1 2 1 2 2 2 2 1 2 1 2 2 2 2 2
−2m l l sinθ θ˙ θ˙ −m l l sinθ θ˙2−(m +m )gl sinθ −m gl sin(θ +θ ) = τ .
2 1 2 2 1 2 2 1 2 2 2 1 2 1 1 2 2 1 2 1
Similarly,
d ∂L(q,q˙) ∂L(q,q˙)
− = τ ,
dt ∂θ˙
2
∂θ
2
2
=⇒ (m l l cosθ +m l2)θ¨ +m l2θ¨ −m l l sinθ θ˙ θ˙ +m l l sinθ θ˙2
2 1 2 2 2 2 1 2 2 2 2 1 2 2 1 2 2 1 2 2 1
+m l l sinθ θ˙ θ˙ −m gl sin(θ +θ ) = τ ,
2 1 2 2 1 2 2 2 1 2 2
=⇒ (m l l cosθ +m l2)θ¨ +m l2θ¨ +m l l sinθ θ˙2−m gl sin(θ +θ ) = τ .
2 1 2 2 2 2 1 2 2 2 2 1 2 2 1 2 2 1 2 2
FromtheaboveLagrangeequations,wewritethefollowingmanipulatorequation,
M(q)q¨+C(q,q˙)q˙ = τ (q)+Bu. (19)
g
23RFAPPROXIMATIONFORCONTROL
where
(cid:20) (m +m )l2+2m l l cosθ +m l2 m l l cosθ +m l2(cid:21)
M(q) := 1 2 1 2 1 2 2 2 2 2 1 2 2 2 2 , (20)
m l l cosθ +m l2 m l2
2 1 2 2 2 2 2 2
(cid:20) −2m l l sinθ θ˙ −m l l sinθ θ˙ (cid:21)
C(q,q˙) := 2 1 2 2 2 2 1 2 2 2 , (21)
m l l sinθ θ˙ 0
2 1 2 2 1
(cid:20) (cid:21)
(m +m )gl sinθ +m gl sin(θ +θ )
τ (q) := 1 2 1 1 2 2 1 2 , (22)
g m gl sin(θ +θ )
2 2 1 2
(cid:20) 1(cid:21) (cid:20) τ (cid:21) (cid:20) θ (cid:21) (cid:20) θ˙ (cid:21) (cid:20) θ¨ (cid:21)
B := , u := 1 , q := 1 , q˙ = 1 , q¨= 1 . (23)
1 τ θ θ˙ θ¨
2 2 2 2
Therefore, the state of the acrobat is x = (q,q˙) = (θ ,θ ,θ˙ ,θ˙ ), where, θ ,θ ∈ [0,π] and
1 2 1 2 1 2
θ˙ ,θ˙ ∈ R. Theinputu = [τ ,τ ]isof2dimensions. Themanipulatorequationcanbewrittenas
1 2 1 2
thefollowingcontrolaffinedynamics.
(cid:20) (cid:21) (cid:20) (cid:21)
q˙ 0
x˙ = + u (24)
M(q)−1(−C(q,q˙)q˙ +τ (q)) M(q)−1B
g
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
f(x) g(x)
E.2. CCFModellingProblem
We consider a control affine modelling problem in the setting of learning the residual errors from
a nominal dynamics model (Example 4) for a CCF (Example 3). The true dynamics model f,g is
definedaccordingtotheequationsabovewithm = m = l = l = 1whilethenominaldynamics
1 2 1 2
model f˜,g˜ is defined with incorrect values of mass and length, m˜ = m˜ = ˜l = ˜l = 0.6. The
1 2 1 2
CCFisaCLFandisdefinedtoensurestabilitytotheorigin:
 
12 0 3.16 0
C(x) = x⊤Px, P =   0 12 0 3.16 .
3.16 0 4.04 0 
0 3.16 0 4.04
Usingthenominalmodel,C˜˙ (x,u) = ∇C(x)⊤(f˜(x)+g˜(x)u). Thegoalofthemodellingproblem
is to learn the residual
C˙(x,u)−C˜˙
(x,u). Given a sampled trajectory {x ,u }L+1, we compute
i i i=1
{C(x )}L+1 and use forward finite differencing to approximate
{Cˆ˙
}L . Then the regression tar-
i i=1 i i=1
getsaredefinedasz =
Cˆ˙ −C˜˙
(x ,u ).
i i i i
E.3. NominalControlDataCollection
We collect data using a controller designed with the nominal dynamics models. The control law
is given by CCF-QP with c = 25, u (x) a feedback linearizing controller designed for nominal
1 d
dynamics f˜,g˜, C(x) = x⊤Px defined above, f˜,g˜ used in place of f,g, and α(c) = 0.725c.
Additionally,thehardconstraintisreplacedwithaslackvariablewithpenaltycoefficient1e6.
Thenominalcontrollawissimulatedinclosed-loopwiththetruedynamicsusingRunge-Kutta
4(5)at10Hz. WecollectE = 226trajectoriesstartingfromdifferentinitialconditions. Theinitial
24RFAPPROXIMATIONFORCONTROL
conditionscompriseofameshgridofcoordinatesofthedifferentinitialstates. Eachtrajectoryis5
secondslong,resultinginL+1 = 50sampledpoints,thefinaldatasizeisofsize11074.
Forthepredictionexperiments,thedataissplitintoatestandtrainsetshuffledatrandom.
E.4. Closed-LoopExperiments
We evaluate six controllers starting from initial state x = [2,0,0,0]. All simulations during data
0
collectionandevaluationuseRunge-Kutta4(5)at10Hz.
The nominal controller is described above. The oracle controller is given by CCF-QP with
the same parameters as the nominal, except that the constraint uses the true dynamics f,g. Each
of the four data-driven controllers is defined using an affine model of the residual
hˆ
. The control
law is defined by CCF-QP where C˙(x,u) is replaced with C˜˙ (x,u)+hˆ(x,u), the slack penalty
is 1e6 · (t + 1) where t is the time in seconds, and otherwise the parameters are the same as for
thenominal/oraclecontrollers. SectionB.2presentsthepreciseaffineformintermsofthetraining
data.
Each data-driven model
hˆ
is trained in an episodic manner. The process is warm started with
subsamplingthenominalgriddataatarateof1/5resultingin2215datapoints. Thisinitialtraining
datasetdefinesanaffinemodel(AD-K,ADP-K,AD-RF,orADP-RF)whichinturndefinesadata-
driven controller. We simulate the data-driven controller in closed loop starting from x for 10
0
seconds, addtheresultingdatatothetrainingset, andretrain. Werepeatfor10episodes, resulting
inatrainingsetsizeof3215,andreporttheperformanceofthefinalcontroller.
25