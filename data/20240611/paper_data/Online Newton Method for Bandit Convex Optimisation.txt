ONLINE NEWTON METHOD FOR BANDIT CONVEX
OPTIMISATION
HiddeFokkema DirkvanderHoeven
Korteweg-deVriesInstituteforMathematics MathematicalInstitute
UniversityofAmsterdam LeidenUniversity
h.j.fokkema@uva.nl dirk@dirkvanderhoeven.com
TorLattimore JackJ.Mayo
GoogleDeepMind Korteweg-deVriesInstituteforMathematics
lattimore@google.com UniversityofAmsterdam
jackjamesmayo@gmail.com
ABSTRACT
Weintroduceacomputationallyefficientalgorithmforzeroth-orderbanditconvexoptimisationand
provethatintheadversarialsettingitsregretisatmostd3.5√npolylog(n,d)withhighprobability
wheredisthedimensionandnisthetimehorizon. Inthestochasticsettingtheboundimprovesto
Md2√npolylog(n,d)whereM [d 1/2,d 1/4]isaconstantthatdependsonthegeometryofthe − −
∈
constraintsetandthedesiredcomputationalproperties.
1 Introduction
Banditconvexoptimisationis thebanditversionofthe classicalzeroth-orderoptimisationproblem,whichisbotha
fundamentalproblemin optimisation and has many applicationsin operationsresearch and beyond. Bandit convex
optimisationisframedasagamebetweenalearnerandanadversarywherethelearnerplaysactionsinasetK Rd,
⊂
whichisassumedto be convex,compactandhavea nonemptyinterior. Atthe beginningofthegametheadversary
secretlychoosesasequenceofconvexfunctionsℓ ,...,ℓ :K [0,1]. Thelearnerandadversarytheninteractover
1 n
→
nrounds.InroundtthelearnerchoosesanactionA K,sufferslossℓ (A )andobservesℓ (A )+ε withthenoise
t t t t t t
∈
ε assumedtobeconditionallysubgaussian(definedbelow). Thegoalinbanditconvexoptimisationistocontrolthe
t
regret:
n
Reg = sup (ℓ (A ) ℓ (x)) .
n t t − t
x K
∈ Xt=1
Wepresentacomputationallyefficientalgorithmforbanditconvexoptimisation.Theprinciplechallengeisthatalmost
allofthemachineryforoptimisationisbasedonaccesstogradients,butinoursettingthelearneronlyobservesthe
valueofthelossatasinglepoint. OuralgorithmisbasedonthebanditversionofonlineNewtonstepforstochastic
unconstrainedbanditconvexoptimisationbyLattimoreandGyo¨rgy[2023]withsomecrucialnewingredients:(1)we
modifytheMinkowskiprojectionproposedbyMhammedi[2022]toshowthataboundedconvexfunctiononK can
beapproximatelyextendedtoallofRd inasuchawaythatthefunctioncanstillbequeriedviathebanditmodeland
(2)intheadversarialsettingweemployanimprovedversionoftherestartingconditionusedbySuggalaetal.[2021].
Ourmainresultsarethefollowingregretbounds.
Theorem1. Thereexistsanalgorithmsuchthatwithprobabilityatleast1 δ,
−
Reg d3.5√npolylog(n,d,1/δ).
n ≤
Furthermore,thealgorithmiscomputationallyefficientgivenamembershiporacleforK.
AcceptedforpresentationattheConferenceonLearningTheory(COLT)2024
4202
nuJ
01
]CO.htam[
1v60560.6042:viXraOnlineNewtonMethodfor BanditConvexOptimisation
Theregretboundcanbeimprovedinthestochasticsetting,whereℓ =ℓisthesameineveryround:
t
Theorem2. Thereexistsanalgorithmsuchthatinthestochasticsettingwithprobabilityatleast1 δ,
−
Reg Md2√npolylog(n,d,1/δ),
n ≤
whereM [d 1/2,d 1/4]isaconstantthatdependsonthegeometryofK andthedesiredcomputationalproperties
− −
∈
ofthealgorithm.
The computationalpropertiesdependonthe desiredvalue of M and the propertiesof K. A samplingoracle for K
whenqueriedreturnsasamplefromtheuniformdistributiononK andamembershiporacleforK whenqueriedwith
inputx Rd returns1 (x). Wehavethefollowing:
K
∈
• Without any assumptions on K you can take M = d 1/2 but the algorithm may not be computationally
−
efficient.
• Given access to sampling and membership oracles for K you can take M = d 1/4 and the algorithm is
−
efficient.
• Given access to sampling and membership oracles for a symmetric K you can take M = d 1/2 and the
−
algorithmisefficient.
Besides convexity and boundedness, no regularity assumptions on the losses are needed. By efficiency we mean
polynomialtime, thoughthe (usually heavy)sampling oracle is only neededin the initialisation of the algorithmto
placeK in(approximately)isotropicposition.
Notation Given a function f : Rd R we write f (x) for its gradient at x and f (x) for the Hessian. The
′ ′′
→
Lipschitzconstantforf isLip(f) = sup (f(x) f(y))/ x y . Thedirectionalderivativeatxindirectionν
is ∂f (x). The ball of radius r is B(r)
=x 6=y
x
R−
d : x
k r− ank
d the sphere is S(r) = x Rd : x = r .
ν
Thevectorofallzerosis0andtheidentitym{ atr∈ ixis1,wk hk ich≤ wil} lalwaysbeofdimensionda{ nd∈
d
d,rek spk ectivel}
y.
The standard euclidean norm is . Given a square matrix A we use the notation x 2 = x × Ax. Be warned,
k·k k kA ⊤
this is onlya normwhen A is positivedefinite butoccasionallywe use the notationforrandommatricesAthat are
only positive definite in expectation. The operator norm of a matrix A is A = max x=0 Ax / x . We use P
to refer to the probabilitymeasure on some measurablespace carryingall
tk hek randomva6riabk lesak ssok ciak
ted with the
learner/environmentinteraction, including actions, losses, noise and any exogenous randomness introduced by the
learner.TheassociatedexpectationoperatorisE. WeuseLtodenoteauniversallogarithmicfactor. Moreconcretely,
L=C[1+logmax(n,d,1/δ)],
whereC isasuitablylargeuniversalconstant. We alsoassumethatδ = O(polylog(1/n,1/d)). TheGaussianwith
meanµandcovarianceΣis (µ,Σ). AlldensitiesarewithrespecttotheLebesguemeasure.
N
Relatedwork For simplicity in this paragraphwe ignorelogarithmicfactors. The reader is referredto the recent
notesbyLattimore[2024]foranextensiveliteraturereviewandsummaryofthecurrentParetofrontier.Banditconvex
optimisation was initiated by Flaxmanetal. [2005] and Kleinberg [2005]. For a long time there was speculation
about how the minimax regret should depend on the horizon. Agarwaletal. [2013] showed that poly(d)√n regret
is possible in the stochastic setting where ℓ = ℓ for some fixed (unknown) loss function ℓ and the responses are
t
noisy. Meanwhileintheadversarialsetting√nregretispossibleusingakindofonlineNewtonstepwhenthelosses
are assumed to be both smooth and strongly convex[HazanandLevy, 2014]. When only boundednessis assumed,
then Bubecketal. [2015] showed that √n regret is possible in d = 1, which was extended to poly(d)√n for the
generalcasesbyBubeckandEldan[2018]. Theseapproachesarenon-constructive. Theboundsareestablishedina
non-constructivefashionviaBayesianminimaxduality.Thecurrentstateoftheartisasfollows:(1)Intheadversarial
settingthebestknownboundisd2.5√nbyLattimore[2020],whoalsousedminimaxdualityandhencedonothave
an algorithm. (2)For a polynomialtime algorithmforthe adversarialsetting thereis the kernel-basedalgorithmby
Bubecketal. [2017] for which the regretis d10.5√n. (3) in the stochastic setting a versionof the ellipsoid method
obtainsd4√nregret[LattimoreandGyo¨rgy,2021;Lattimore,2024]. (4)intheunconstrainedstochasticsettingunder
aLipschitzassumptionthereisaversionofonlineNewtonstepforwhichtheregretisd1.5√n[LattimoreandGyo¨rgy,
2023].
Noise Mostoftheexistingworkonadversarialbanditconvexoptimisationdoesnotconsiderthepresenceofnoise
and simply assumes that ℓ (A ) is directly observed. As far as we know all existing algorithms can handle noise
t t
with more-or-less no modifications. We include it for two reasons: (1) to ensure that the adversarial problem is a
2OnlineNewtonMethodfor BanditConvexOptimisation
strictgeneralisationofthestochasticone; (2)sothatouralgorithmcanbeusedforbanditsubmodularminimisation
via the Lova´sz extension. Let Y˜ = ℓ (A )+ε be the realised loss in round t. Our assumption is that the noise
t t t t
isconditionallysubgaussian: E[ε A ,Y˜ ,...,A ,Y˜ ,A ] = 0andE[exp(ε2)A ,Y˜ ,...,A ,Y˜ ,A ] 2.
Note that this is the Orlicz
normt d| efi1 nit1
ion of
sut b−g1 aust s−ia1
n,
wt
hich besides
constt an| ts1
is
e1 quivalent t−t1
o
tht e−1 defint it≤
ions
basedonmoment-generatingfunctionsorrawmoments[Vershynin,2018].
Regularity of constraint set Remember that a convex body is a compact, convex subset of Rd with nonempty
interior. The polar body of K is K = u : max u,x 1 . Since the only assumptions on the losses are
◦ x K
convexityand boundedness, we can
assum{
e that K
∈hash beeni su≤ itab}
ly positioned, which simply means a change of
coordinatesvia an affine transformation. There are two questions: (1) how K should be positionedand (2)can the
correspondingaffinetransformationbefoundefficiently.The(half)meanwidthofthepolarbodyis
M(K◦)= π(x)dρ(x),
ZS(1)
whereπ(x) =inf t>0: x tK istheMinkowskifunctionalassociatedwithK andρistherotationallyinvariant
uniformprobabilit{ ymeasureo∈ nS(1}
). WeletM =max(d 1/2,M(K )). WeneedKtobepositionedsothatM(K )
− ◦ ◦
isassmallaspossibleand
B(1) K 2B(d+1). (1)
⊂ ⊂
Therearevariouscomputationaltradeoffsavailable.Atpresentthisisthecurrentsituation:
(a) WhenKisinisotropicposition,thenEq.(1)holdswithtighterconstants[Kannanetal.,1995]andM(K )
◦
≤
1 holds trivially. A convex body can be placed in approximateisotropic position with high probability by
estimating the covariance of the uniform distribution on K, which can be done in polynomial time in a
varietyofways[Lova´szandVempala,2006]. ThisisatechniquealreadyemployedbyFlaxmanetal.[2005]
andwewillnotdiveintodetailsofthenecessaryapproximationaccuracy.
(b) When dK is in isotropic position, then Eq. (1) holds because polarity reverses inclusion. The positive
◦
resolution to the slicing conjecture and Theorem 9.1.1 by Brazitikosetal. [2014] shows that M(K ) =
◦
O˜(d 1/4). If K is also symmetric, then Milman [2015] improvedthis boundto M(K ) = O˜(d 1/2). K
− ◦ −
canbe positionedsothatdK isapproximatelyin isotropicpositionusinguniformsamplesfromthe polar
◦
body. Sinceamembership/separationoracleforK givesaseparation/membershiporacleforthepolarbody,
standardsamplingalgorithmscan be used to find an affine transformationso thatdK is in approximately
◦
isotropicposition.
(c) Lastly, when K is positioned so that dK is in John’s position, then Eq. (1) holds by John’s theorem
◦
[Artstein-Avidanetal., 2015, Remark 2.1.17]. Furthermore, Barthe [1998] proved that the mean width of
K in thispositionisupperboundedbythe meanwidth ofthe standardsimplexandFinch [2011]showed
◦
this is at most O˜(d 1/2). Even with a separation/membershiporacle there is no known algorithm for effi-
−
cientlypositioningaconvexbodyintoJohn’sposition.
Summarising,ifweignorecomputationcomplexitywecanassumeM = O˜(d 1/2). Ifweneedtofindtheposition
−
inacomputationallyefficientmanner,thenforsymmetricK wecantakeM = O˜(d 1/2)andforgeneralK thebest
−
knownboundisM =O˜(d 1/4).
−
Distribution theory Let h : Rd R be convex with Lip(h) < and W have law (0,1) and X have law
(µ,Σ) forµ Rd andpositived→ efinitecovarianceandletp bethe∞ densitywith respectN to theLebesguemeasure
oN f (µ,Σ). Iti∈ sconvenienttohaveaninterpretationofE[h (X)]evenwhenhisnottwicedifferentiable. Thereare
′′
atlN easttwonaturalwaystodefinethisquantity. ThedistributionalviewistodefineE[h (X)] = h(x)p (x)dx,
′′ Rd ′′
which is an equality using integrationby parts when h is twice differentiable. Alternatively – and equivalently– a
smoothing approach can be employed by defining E[h (X)] = lim E[h (X)] where h (x) =R E[h(x+̺W)].
′′ ̺ 0 ′̺′ ̺
Convexity ensures that E[h (X)] 0. For convex h,g we write h→ g if E[h (X)] E[g (X)] for all non-
′′ ′′ ′′ ′′ ′′
(cid:23) (cid:23) (cid:23)
degenerateGaussianrandomelementsX.
2 Overview oftheanalysis
Here we sketch the analysis and the main ideas involvedin proving Theorems1 and 2. Simply put, the main idea
of this work is to design a surrogateloss that allows us to use an algorithmthat is designed for Rd on any domain
3OnlineNewtonMethodfor BanditConvexOptimisation
K. TheunconstrainedalgorithmweuseisamodifiedversionofthealgorithmbyLattimoreandGyo¨rgy[2023]. Like
LattimoreandGyo¨rgy [2023], we sample an X from a Gaussian distribution with mean µ K and covariance
t t
∈
matrix Σ and update µ and Σ using the online Newton step algorithm [Hazanetal., 2007] or, equivalently, the
t t t
exponentialweightsalgorithmwithaGaussianprior[vanderHoevenetal.,2018]. Asisstandard,weassumethatK
isreasonablywellrounded(eitherinJohn’spositionorisotropicposition)andouralgorithmplaysonK =(1 ε)K
ε
−
where ε = Θ(1/√n). The first step in our analysis is to introduce a bandit version of the extension proposed by
Mhammedi[2022], which is a convexfunction f : Rd R such that f = ℓ on K . Importantly, f (x) can be
t t t ε t
→
evaluatedbycomputingℓ (Π(x))whereΠ(x)=x/max(1,π (x))withπ theMinkowskifunctionalassociatedwith
t ε ε
K . Letx =argmin n ℓ (x). Weshowthatforanyx Rd,
ε ⋆ x ∈Kε t=1 t ∈
P ℓ t(Π(x)) ℓ t(x ⋆) f t(x) f t(x ⋆).
− ≤ −
AtleastinthestochasticsettingonemighttrytoapplytheanalysisofLattimoreandGyo¨rgy[2023]totheextended
loss functions, who study unconstrained stochastic convex bandits. However, the reader may have already noticed
thatwehaveintroducedanewchallenge.LattimoreandGyo¨rgy[2023]assumedLipschitzlossesandprovedabound
that depends polynomiallyon the norm of the minimiser and Lipschitz constant. But the Lipschitz constant of the
extensionf isΘ(√n),whichrendersexistinganalysisvacuous.Atthesametime,LattimoreandGyo¨rgy[2023]only
t
studiedthestochasticsetting,sosomenewideaisneededtohandletheadversarialsetting.
OnlineNewtonstep RememberthatonlineNewtonstepappliedtoasequenceofquadraticlossfunctions(qˆ)n
t t=1
plays
t 1
µ =argmin 1 x 2+η − qˆ(x) .
t x ∈Kε "2σ2 k k
Xs=1
s #
Givenx K let
ε
∈
n
Regqˆ(x)= (qˆ(µ ) qˆ(x))
n t t − t
t=1
X
betheregretassociatedwiththequadraticlosses(qˆ)n . Thisisboundedby
t t=1
diam(K)2 η2 τ
x x 2 ηRegqˆ(x)+ + g 2 , (2)
k τ+1 − ⋆ kΣτ+1 ≤− τ 2σ2 2 k t kΣt+1
t=1
X
whereg t =qˆ t′(µ t)andH t =qˆ t′′(µ t)andΣ−t 1 = σ1 21+η t s− =1 1H t. Whenappliedtoconvexbanditsitisusualtolet
qˆ beanunbiasedestimatorofaquadraticapproximationoff andthisisourapproachaswell. Inordertoestimateg
t t t
andH thealgorithmneedstosampleitsmetaactionX frPomaGaussian (µ ,Σ ).
t t t t
N
Propertiesofquadraticsurrogate Letq =E [qˆ]whereE conditionsonthehistoryuntilthestartofround
t t 1 t t 1
t. Thisfunctionisclosetof tinthesensethatprov−ided kx ⋆ −µ t k− Σ− t1 . λ1 foruser-definedconstantλ>0,then
1
E
t
−1[f t(X t)] −f t(x ⋆).q t(µ t) −q t(x ⋆)+ λtr(q t′′(µ t)Σ t) (3)
Log-determinantargumentsandthedefinitionofΣ meansthat
t
n
1 d
tr(q (µ )Σ )=O˜ .
λ
t′′ t t
λ
t=1 (cid:18) (cid:19)
X
Theparameterλmustbebalancedcarefully. Ontheonehand,wewouldlikeittobelargetocontroltheexpression
above.Ontheotherhand,itmustbesmallenoughthat kx ⋆ −µ t kΣ− t1 . λ1 holdsforalltwithhighprobability.
Challengeoflargelosses Themainproblemisthatg andH dependonthemagnitudeoftheextendedlossfunction
t t
and when the meta algorithm plays outside of K these may not be bounded in [0,1]. More concretely, once the
ε
definitionsofg havebeengiven,itwillbestraightforwardtoshowthat
t
g 2 =O˜(dY2),
k t kΣt+1 t
whereY isthelossoftheextensionthatthelearnerobservesin roundt. Innormalcircumstancesforboundedloss
t
functionsY [0,1]butbecausetheextensiongrowsveryrapidlyoutsideofK ,thisneednotbetrue. Nevertheless,
t ε
∈
4OnlineNewtonMethodfor BanditConvexOptimisation
wearestillabletoshowthat n Y2 = O˜(n)withhighprobability. Thekeyideaistousethefactthatf ismin-
t=1 t t
imisedonK andgrowsrapidlyoutsideK .HenceanysensiblealgorithmshouldnotplayoftenfaroutsideofK .The
ε ε ε
technicaltoolsweusetoprovePthisarequiteinteresting.Sincef isboundedonK andgrowsrapidlyoutside,iteffec-
t ε
tivelyhasvarylargecurvatureneartheboundaryofK ThismeansthatifthedistributionofX hasahighprobability
ε t
ofplayingoutsideK ,thentheestimatedHessianH willbelargeinexpectationandconsequentiallyΣ willrapidly
ε t t
decrease. FormallyweuseaPoincare´-likeinequalityforconvexfunctions(Proposition19). Havingestablishedthat
n t=1Y t2 =O˜(n)withhighprobability,byEq.(2)andEq.(3)itfollowsthatwhenever kµ t −x ⋆ kΣ− t1 .λ −1 forall
t τ,then
P≤
diam(K)2 d
kµ τ+1 −x ⋆ k2 Σ− t+1 1 . −ηRegq τˆ(x ⋆)+ 2σ2 +O˜ λ +ndη2 =(⋆). (4)
(cid:18) (cid:19)
For complicated reasons, n Y2 = O˜(n) can only be established if σM√d = O˜(1) with M =
t=1 t
max(d 1/2,M(K )). Notice we are now well placed to initiate an induction. Since the regretwith respect to the
− ◦
P
quadratics cannot be too negative, the above bound can be used to simultaneously bound the regret and prove by
inductionthat kµ t −x ⋆ kΣ− t1 .λ −1forallt. Allthatisneededistochooseη,σandλsothat
1
(⋆)=O˜ .
λ2
(cid:18) (cid:19)
Thisisachievedbynotingthatunderourregularityassumptionsdiam(K)2 =O(d2)andchoosing
1 1 1 1
σ =Θ˜ λ=Θ˜ η =Θ˜ .
(cid:18)M√d
(cid:19)
(cid:18)Md3/2
(cid:19)
λ rnd
!
Adversarialsetting Intheadversarialsettingtheregretcanbenegative,whichmeansthatEq.(4) cannotbeused
anymoreto drivean induction. Atthe same time, whenthe regretis negativewe couldsimplyrestartthe algorithm.
Thechallengeistodetectwhentorestartthealgorithm. ThisideahasbeenexploredbeforebyHazanandLi[2016];
Bubecketal. [2017]; Suggalaetal. [2021]. We develop a refined restarting mechanism, which is computationally
moreefficientandhasasomewhatsimplifiedanalysisrelativetopriorwork.Weaddnegativebonusestothepotential
ofthealgorithm,whichhasthesameeffectasincreasingthelearningrateusedbyBubecketal.[2017];Suggalaetal.
[2021]. The main reason for the degradationof our regretboundin the adversarialsetting is that we need uniform
concentrationboundsforoursurrogatelossestimates.
Summary Ataveryhighlevel,ourmaintechnicalcontributionsareasfollows:
(a) Weshowhowtoreduceconstrainedbanditconvexoptimisationtounconstrainedbanditconvexoptimisation
usingamodificationoftheextensionproposedbyMhammedi[2022]forfullinformationonlinelearning.
(b) The resulting unconstrainedproblemloses many of the nice propertiesthat exist in the constrainedsetting.
Notably,thelossfunctionisnotboundedandonlybarelyLipschitz. Wedeveloptechniquesforhandlingthis
issueinadversarialbanditproblems.
(c) WerefinetherestartingmechanismbySuggalaetal.[2021]tobemorecomputationallyefficientandsome-
whatsimplifyitsanalysis.
The remainderofthe paperisorganisedasfollows. We continuebyfirstdiscussingthe relatedwork. Afterthatwe
introducetheconvexextensioninSection3andthesurrogatelossinSection4. InSection5wedescribeouralgorithm
andinSection6weprovetheregretboundforthealgorithm.
3 Convex extensions
LikethatofLattimoreandGyo¨rgy[2023],ouralgorithmisreallydesignedforunconstrainedlossfunctionswherethe
learner can play anywhere in Rd. In order to run this algorithm in the constrained setting we employ an extension
of the constrained losses that can be evaluated in the bandit model. The construction is a natural generalisation of
extensionproposedbyMhammedi[2022]forthefullinformationsetting.Themaindifferenceisthattherethelearner
is assumed to have gradientaccess and the gradient is used to extend the function, while in the bandit setting only
(noisy)functionvaluesareavailableandthesemustbeusedinstead. Thereisaminorproblemthatforlossfunctions
thatarenotLipschitzthereneednotbeany(finite)convexextensionatall. Becauseofthisweextendthelossfrom
a subset of K to all of Rd. Let π(x) = inf t > 0 : x tK be the Minkowski functional of K, which is a
{ ∈ }
5OnlineNewtonMethodfor BanditConvexOptimisation
convex function such that x K if and only if π(x) 1. Given ε (0,1), let π (x) = max(1,π (x)) where
∈ ≤ ∈ 2 + ε
π (x) = π(x)/(1 ε)istheMinkowskifunctionalofK = (1 ε)K. Thefunctionπ isconvexbecauseitisthe
ε − ε − +
maximumoftwoconvexfunctions.Moreover,π is2-Lipschitz:
+
Lemma3. sup ∂ π (x) 2forallx Rd.
ν ∈S(1) ν + ≤ ∈
Proof. The Minkowski functional π is the support function of the polar body K and hence the subgradients of
ε ε◦
π (x) are in K , see, for example, Example 3.1.5 in [Nesterov, 2018]. Polarity reverses inclusion and by Eq. (1),
ε ε◦
B(1 ε) K andthereforeK B(2), usingthatε (0,1). Hence∂ π (x) 2foranyν S(1). Theresult
− ⊂ ε ε◦ ⊂ ∈ 2 ν ε ≤ ∈
followsfromthedefinitionofπ (x)=max(1,π (x)).
+ ε
Letℓ:K [0,1]beaconvexfunction.WearenowinthepositiontodefineanextensionofℓonK . Givenx Rd
ε
→ ∈
define
x π (x) 1
e(x)=π (x)ℓ + + − .
+ π (x) ε
(cid:18) + (cid:19)
Lemma4. Thefunctionesatisfiesthefollowing:
(a) e(x)=ℓ(x)forallx K . (c) ∂ e(x) 0forallx / K .
ε x ε
∈ ≥ ∈
(b) eisconvexonRd. (d) π+(x ε) −1 ≤e(x) ≤1+ 1+ 1
ε
[π +(x) −1].
(cid:0) (cid:1)
Remark 5. By part (c), for x ∂K the function t e(tx) is non-decreasing for t 1, which implies that
ε
∈ 7→ ≥
a minimiser of e is always in K . Moreover, since differentiation is linear, this property carries over to sums of
ε
extensions.
Lemma4. Parts (a), (c), and (d) are immediate from the definitionssince π (x) = 1 wheneverx K and using
the fact that the losses are bounded in [0,1]. For part (b), let J = (x,λ)+ : x Rd,λ π(x)∈ . Tε he function
g(x,λ) = λℓ(x/λ) is jointly convexon J [Rockafellar, 2015, page
3{
5]. Let z
∈Rd. We≥
claim
th}
at λ g(z,λ)
is 1-Lipschitzon[π (z), ). Sincegradientsaremonotoneforconvexfunctions∈ itsufficesto checktheg7→ radientat
ε + ∞
λ = π (z)andasλ . Forthelattercase, lim dg(z,λ) = lim [ℓ(z/λ) ∂ ℓ(x/λ)/λ] = ℓ(0) 1.
Forthe+ former, → ∞ λ →∞ dλ λ →∞ − x ≤
π(z)ℓ z π (z)ℓ z
dg(z,λ) (1) g(z,π(z)) −g(z,π +(z))
=
π(z) − + π+(z)
dλ ≥ π (z) π(z) (cid:16) π ((cid:17)z) π(z) (cid:16) (cid:17)
(cid:12)λ=π+(z) + − + −
(cid:12)
(cid:12)
(cid:12)
(2) π +(z) (3) 1
,
≥ −π (z) π(z) ≥ −ε
+ −
where (1) follows from convexityof λ g(z,λ); (2) from the fact that ℓ(x) [0,1] for all x K and (3) since
π (z)=π(z)/(1 ε)andusingthedefin7→ itionofπ (z)=max(1,π (z)).Letx,y∈ Rdandz = x∈ +y.Byconvexity,
ε − + ε ∈ 2 2
1π (x)+ 1π (y) π (z)andhence
2 + 2 + ≥ +
g(z,1π (x)+ 1π (y)) g(z,π (z)) 1 1π (x)+ 1π (y) π (z) . (5)
2 + 2 + ≥ + − ε 2 + 2 + − +
CombiningEq.(5)withjointconvexityofgshowsthat (cid:2) (cid:3)
1
e(z)=g(z,π (z))+ [π (z) 1] g(x + y,1π (x)+ 1π (y))+ 1 1π (x)+ 1π (y) 1
+ ε + − ≤ 2 2 2 + 2 + ε 2 + 2 + −
1g(x,π (x))+ 1g(y,π (y))+ 1 1π (x)+ 1π (y) 1 = 1e((cid:2)x)+ 1e(y). (cid:3)
≤ 2 + 2 + ε 2 + 2 + − 2 2
Sinceeiscontinuous,midpointconvexityimpliescon(cid:2)vexityandhenceeisco(cid:3)nvex[Simon,2011,Proposition1.3].
Extensionandtheregret Letf betheextensionofℓ definedalmostanalogouslytoeaboveby
t t
x 2(π (x) 1)
f (x)=π (x)ℓ + + − .
t + t π (x) ε
(cid:18) + (cid:19)
Note thatf t(x) has an additionalv(x) = π+(x ε) −1 term comparedto e(x), which is added to give a little nudgefor
themetaalgorithmtoplayinsideK. Sincevisconvex,allpropertiesofelistedinLemma4carryovertof ,except
t
6OnlineNewtonMethodfor BanditConvexOptimisation
fortheupperboundoneinLemma4(d), whichcarriesoverwithafactor2. Ourlearnerwillproposeameta-action
X RdsampledfromaGaussiandistribution (µ ,Σ )withµ K . Theactualactionpassedtotheenvironment
t t t t ε
∈ N ∈
is
A =X /π (X ) K Y =π (X )[ℓ (A )+ε ]+2v(X ).
t t + t ∈ ε t + t t t t t
Let F = σ(X ,Y ,...,X ,Y ) be the σ-algebra generated by the data observed at the end of round t. Define
t 1 1 t t
P = P( F )andE [] = E[ F ]. Thenextlemma(proofinAppendixD)showsthatwithhighprobabilitythetrue
t t t t
·| · ·|
regretisboundedintermsofthesurrogateregret.
Lemma 6. With probability at least 1 δ, Reg √nL + nε + max Regf(x), where Regf(x) =
n (E [f (X )] f (x)) − n ≤ x ∈Kε n n
t=1 t −1 t t − t
TP herestofthearticleisfocussedonboundingRegf withhighprobability.Thereadermayalreadynoticetheprinciple
n
challenge.InorderforLemma6tobeusefulweneedε=O(1/√n). ButtheLipschitzconstantoff ismore-or-less
t
O(1/ε), which is then too large to use the analysis by LattimoreandGyo¨rgy[2023]. As we mentioned, the key is
toexploitthefactthatf isminimisedonK andalsoboundedin[0,1]onK ,whichmeansanysensiblealgorithm
t ε ε
shouldnotspendtoomuchtimeplayingoutsideofK wherethelossisnotthatwellbehaved.
ε
4 Surrogate loss
Thealgorithmkeepstrackofaniterateµ K andcovariancematrixΣ . Letp bethedensity (µ ,Σ )anddefine
t ε t t t t
thesurrogatelossfunctions :Rd Rby∈ N
t
→
1 1
s (z)= 1 f (x)+ f ((1 λ)x+λz) p (dx).
t t t t
ZRd(cid:20)(cid:18) − λ
(cid:19)
λ −
(cid:21)
Thesurrogates anditsquadraticapproximationq definedbelowhavebeenusedextensivelyinbanditconvexopti-
t t
misation[Bubecketal.,2017;LattimoreandGyo¨rgy,2021;LattimoreandGyo¨rgy,2023]. Theirpropertiesaresum-
marised in detail in a recent monograph [Lattimore, 2024]. For completeness we include the essential properties
withoutproofinAppendixG.Thesurrogatelossfunctionisnotdirectlyobserved,butcanbeestimatedby
1 R t(z)
p
t
X 1t−λλz
sˆ(z)=Y 1 + R (z)= − .
t t − λ λ t (1 (cid:16)λ)dp (X(cid:17) )
(cid:20) (cid:21) − t t
Everythingisnice enoughthatlimits andintegralsexchangeso thatE [sˆ(z)] = s (z)andE [sˆ (z)] = s (z).
t 1 ′t ′t t 1 ′t′ ′t′
Explicitexpressionsforsˆ(z)andsˆ (z)aregiveninAppendixG.Wele−tg =sˆ(µ )andH =sˆ−(µ )and
′t ′t′ t ′t t t ′t′ t
1 1
qˆ(x)= g ,x µ + x µ 2 q (x)= s (µ ),x µ + x µ 2 .
t h t − t i 4k − t kHt t h ′t t − t i 4k − t ks′ t′(µt)
5 Algorithm
ThealgorithmcanbeviewedasonlineNewtonsteporfollowtheregularisedleaderwithquadraticlossestimatorsand
afewbolt-ongadgets. Thesegadgetsareneededbecausethelossestimatorsareonlywell-behavedonasmallregion,
whichin the adversarialsetting necessitates a restartingprocedure. Thereare threeinterconnectedpartsthathandle
thebehaviourofthelossestimatorsandrestarting:
(a) ThealgorithmcomputesitsiteratesinadecreasingsequenceofsetsK =K K K . Theseare
ε 0 1 n
⊃ ⊃···⊃
thesetsonwhichthesurrogatelossfunctionanditsquadraticapproximationarewell-behaved.
(b) Intheadversarialsetting,asthefocusregionshifts,thealgorithmsubtractsquadraticsfromtheaccumulation
of losses. Thisprocedureis inspiredby ZimmertandLattimore [2022] and mimicsthe increasinglearning
ratesusedbyBubecketal.[2017]andSuggalaetal.[2021]. Wethinktheargumentsarealittlesimplerbut
the outcome is the same. Previous algorithms used volume arguments for deciding when to increase the
learningrate,whileourmethodisbasedonrelativelystandardalgebraicconsiderationsandismorepractical
toimplement.
(c) Lastly,intheadversarialsettingthealgorithmrestartsiftheregretwithrespecttoestimatedsurrogatelosses
issufficientlynegative.
7OnlineNewtonMethodfor BanditConvexOptimisation
1 input n, η, λ, σ, γ and K =K
0 ε
2 for t=1 to n
43 cl oe mt pΦ ut t− e1( µx t) == a2 rσ1 g2 mkx ink x2 ∈+ KtP−1t u Φ− =1 t1 −♭ 1u (( xx ))+ anη
d
PΣt u− = −t1 1 1qˆ =u( Φx)
′t′
−1(µ t)
5 sample X (µ ,Σ )
t t t
∼N
6 play A = Xt and observe Y =π (X )[ℓ (A )+ε ]+2v(X )
t π+(Xt) t + t t t t t
7 K t =K t −1 ∩{x: kx −µ t k2 Σ− t1 ≤F max
}
8 if in the adversarial setting :
9 compute z t =argmin z ∈Rd t s− =1 11(♭ s 6=0) kz −µ s k2 Σ− s1
0 i Pf t s− =1 11(♭ s 6=0) kz t −µ s k2 Σ− s1
≥
Fm 24ax
10 ♭ t(x)= −− γγ kk xx −− µµ tt kk
2
Σ2 Σ
−
t− t 11 ii ff kkP µ·k t2 Σ −t−1 z6(cid:22)
t
k2
ΣP−
t1t s− = ≥1 11 Fm( 3♭ as x6=0) k·k2 Σ− s1
0 otherwise.
1 11
2
ei nf dm ia fxy ∈Ktη t u=1(sˆ u(µ u) −sˆ u(y)) ≤−γF 3m 2ax then restart algorithm
P
13 end for
Algorithm1
Computation Thealgorithmisreasonablypractical,butsomepartsmaybenon-trivialtoimplement. Wedescribe
thekeypartsbelow:
(a) Findingtheiterateµ requiressolvingaconstrainedconvexquadraticoptimisationproblem,whichshouldbe
t
somewhatpracticalwithNewton’smethod. Notetheconstraintsaredefinedbyanintersectionofellipsoids,
ofwhichtherecouldbeuptoO(n)many. Thiscanlikelybereduceddramaticallybysomesortofdoubling
trick. InthestochasticsettingonecouldonlyprojectonK andthesameregretboundwouldhold.
ε
(b) SamplingfromaGaussianrequiresaccesstorandomnoiseandasingularvaluedecomposition.
(c) Findingz isanotherconvexquadraticprogrambutthistimeunconstrainedandhencestraightforward.
t
(d) Testingtheconditionsforaddingabonusrequiresaneigenvaluedecompositionofasymmetricmatrix.
(e) As written in Algorithm 1, testing the condition for a restart involves minimising a (possibly) non-convex
functionoverthefocusregion.Fortunately,thisoptimisationdoesnotneedtobedoneexactlyandthemargin
forerrordominatestheamountofnon-convexity.InAppendixIweshowthatasuitableapproximationcanbe
foundusingconvexprogrammingbyaddingaquadratictotheobjectiveandoptimisingtheresultingconvex
program.Thisproceduredoesnotimpacttheregret.
Constants Thealgorithmistunedandanalysedusinganumberofinterconnectedconstants. Thecorrecttuningin
theadversarialsettingis
1 1 d 1 d3.5L8.5
λ= γ = η = σ2 = ε= F =d5L8.
d3L5 4dL nL3 d2 √n max
r
Inthestochasticsettingwechoose
Md 5 1 Md2L5
γ =0 η = λ= σ2 = ε= F =25M2d3L5.
√n Md3/2L3 16M2dL3 √n max
6 ProofofTheorems 1and 2
We proveTheorems1 and 2 simultaneously, as both argumentsuse the same tools. We begin by introducingsome
notationandoutliningourstrategy.Bythedefinitionofthealgorithmthefunctions♭ arequadraticandmostlyvanish.
t
Letγ
t
= γ1(♭
t
6= 0)andm
t
= t u=11(♭
u
6= 0). Letw
t
= (1 −2γ)mt, whichinthestochasticsettingisalways
P
8OnlineNewtonMethodfor BanditConvexOptimisation
1andintheadversarialsettingwewillproveitisalwaysin[1/2,1]. Later(inSectionC)wearegoingtoprovethat
Σ−t 1satisfies
Σ−t 1 =w t −1
σ1
2
+ηt −1 H
w uu andwedefine H¯ t =E t −1[H t] Σ¯ −t 1 =w t −1
σ1
2
+ηt −1 H w¯
uu .
h u X=1 i h u X=1 i
ByLemma32,H¯ =s (µ )istheHessianofaconvexfunctionandthereforepositivedefinite.SinceE [H ]=H¯
t ′t′ t t 1 t t
youmightexpectthatΣ−t 1 andΣ¯
−t
1 arecloseandindeedthisisthecasewithhighprobability. Weneed− notationfor
theoptimalactionforthevarioussurrogatelosses:
t t t
x =argmin f (x) xs =argmin s (x) xsˆ =argmin sˆ (x).
⋆,t u ⋆,t u ⋆,t u
x ∈Kε
u X=1
x ∈Kε
u X=1
x ∈Kt
u X=1
Note the particularchoices of domain. In the stochastic setting, we let x = x = = x because all f are
⋆,1 ⋆,2 ⋆ t
···
equal.Theregretrelativetothevarioussurrogatelossestimatesare
τ τ
Regf(x)= (E [f (X )] f (x)) Regs(x)= (s (µ ) s (x))
τ t −1 t t − t τ t t − t
t=1 t=1
X X
τ τ
Regq(x)= (q (µ ) q (x)) Regqˆ(x)= (qˆ(µ ) qˆ(x)).
τ t t − t τ t t − t
t=1 t=1
X X
Definition7. Wedefineastoppingtimeτ tobethefirsttimethatoneofthefollowingdoesnothold:
(a) Intheadversarialsetting:xs K . Inthestochasticsetting:x K .
⋆,τ ∈ τ+1 ⋆ ∈ τ+1
(b) 1 2Σ¯ −τ+1
1
(cid:22)Σ−τ+1
1 (cid:22)
3 2Σ¯ −τ+1 1.
(c) Thealgorithmhasnotrestartedattheendofroundτ.
Incasenoneofthesehold,thenτ isdefinedtoben.
Note that Σ and the constraints defining K are F -measurable, which means that τ is a stopping time with
t+1 t+1 t
respect to the filtration (F )n . Also note that in the stochastic setting definition 7(c) holds by definition of the
t t=1
algorithm.Theplanisasfollows:(1)Useconcentrationofmeasuretocontrolalltherandomeventsthatdeterminethe
trajectoryofthealgorithm. Therestoftheanalysisconsistsofprovingthattheregretissmallonthehigh-probability
eventthatthealgorithmiswell-behaved.(2)Derivesomebasicconsequencesoftheconcentrationanalysis. (3)Prove
that the regret is well-controlled until round τ. In the stochastic setting our proof that the regret is well-controlled
simultaneouslyimpliesthatx K , atwhichpointtheproofofTheorem2concludes. (4)Provethatifτ = n,
⋆ τ+1
∈ 6
thenthealgorithmrestarts. (5)Provethatifthealgorithmrestarts, thentheregretisnegative. By theendofallthis
andbyLemma6wewillhaveestablishedthatwithprobabilityatleast1 15n2δ,
−
Fmax inthestochasticsetting
Reg nε+ maxRegf(x) nε+ η
n ≤ x ∈Kε n ≤ (γF ηmax intheadversarialsetting.
Step1: Concentration Because the surrogateestimates arenotgloballywell-behavedwe needto ensurethatour
algorithm behaves in a regular fashion. This necessitates a moderately tedious concentration analysis, which we
summarisehere,deferringessentialproofsforlater. Themainpointisthatwedefineacollectionofeventsandprove
theyall holdwith highprobability. The remainderof the argumentis carriedforwardwithoutprobabilistictoolsby
restricting to sample paths on the intersection of the events defined here. By a union bound and Lemma 15, with
probabilityatleast1 nδ,
−
∀t ≤τ, kX t −µ t k2 Σ− t1 ≤dL. (E1)
ConsideralsotheeventE2that
1 3
2Σ¯ −τ+1
1
(cid:22)Σ−τ+1
1 (cid:22)
2Σ¯ −τ+1 1. (E2)
Notethatbythedefinitionofτ on E2 italsoholdsthat 1 2Σ¯ −t 1
(cid:22)
Σ−t 1
(cid:22)
23Σ¯ −t 1 forallt
≤
τ. Thefollowinglemma
showsthatE2occurswithhighprobability:
9OnlineNewtonMethodfor BanditConvexOptimisation
Lemma8. P(E2) 1 6nδ.
≥ −
Wealsoneed tsˆ tand tqˆ ttobesuitablywell-concentratedonK t. LetE3betheeventthat
Pτ P τ τ
max (sˆ t(x) s t(x)) C τ,max (qˆ t(x) q t(x)) C τ, (qˆ t(x ⋆) q t(x ⋆)) C˜ τ, (E3)
x ∈Kτ(cid:12)
(cid:12)Xt=1
− (cid:12) (cid:12)≤ x ∈Kτ(cid:12)
(cid:12)Xt=1
− (cid:12) (cid:12)≤ (cid:12)
(cid:12)Xt=1
− (cid:12) (cid:12)≤
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
whereC τ = L λ(cid:12) (cid:12) d τ t=1E t −1[Y t(cid:12) (cid:12)2]+10√n ,a(cid:12) (cid:12)ndC˜ τ = L λ τ t=(cid:12) (cid:12) 1E t −1[Y(cid:12) (cid:12)t2]+10√n . (cid:12) (cid:12)
Lemma9. P(Eh3p) P1 4nδ. i hpP i
≥ −
Proof. ByaunionboundandLemma23,withprobabilityatleast1 nδforallt τ,Y 10L. Theresultfollows
− ≤ t ≤ ε
fromLemmas34,36,and35withaunionbound.
Aswe mentioned,a seriouschallengein ouranalysisis thatthe extendedlossfunctionsareboundedin [0,1]in K
ε
butareunboundedonRd. Ofcourse,wehopethealgorithmwillspendmostofitstimeplayingactionsX K butit
t
∈
canhappenintheinitialroundsthatX tisnotinK andthenf t(X t)couldbequitelarge.LetE4betheeventthat
τ τ
M
τ
= Y t2 ≤100nL2 and M¯
τ
= E
t
−1[Y t2] ≤100nL2. (E4)
t=1 t=1
X X
Lemma10. P(E4) 1 4nδ.
≥ −
CombiningallthelemmaswithaunionboundshowsthatE0 = E1 E2 E3 E4holdswithprobabilityatatleast
∩ ∩ ∩
1 14nδ.FortheremainderweboundtheregretonE0.
−
Step2:Basicbounds Wecannowlaythegroundworkforthedeterministicanalysis.Herewecollectsomerelatively
straightforward properties of the random quantities under event E0. On t τ, Σ
t
2Σ¯
t
4Σ¯
1
= 4σ2 we
≤ (cid:22) (cid:22)
additionallyhave
kX t −µ t k= kX t −µ t kΣtΣ− t1
≤
dL kΣ t k≤2σ√dL.
Next,foranyt τ, p
≤
g 2 2 g 2 4 g 2 6 g 2
k t kΣt+1 ≤ k t kΣ¯ t+1 ≤ k t kΣ¯ t ≤ k t kΣt
6R (µ )2Y2 54Y2
= (1t t λ)2t kX t −µ t k2 Σ− t1 ≤ (1 λt )2 kX t −µ t k2 Σ− t1 ≤100Y t2dL,
− −
where in the first line we used the definition of τ and Lemma 21. In the second line we used Lemma 31 to bound
R t(µ t) 3andE1. CombiningtheabovewitheventE4yields
≤
τ
1
g 2 dLM ndL4. (6)
k t kΣt+1 ≤ τ ≤ 2
t=1
X
ThewidthoftheconfidenceintervalsC
τ
isalsowell-controlledunderE0. Recallthat
L L 11L2
C = dM¯ +10√n 10L√nd+10√n √nd
τ τ
λ ≤ λ ≤ λ
hp i h i
wherethefirstinequalityfollowsfromeventE4. Similarly,wehavethatC˜
τ
≤
11 λL2 √n. ThematrixΣ−t 1accumulates
curvature estimates. The following lemma follows from standard trace/log-determinantarguments and its proof is
giveninAppendixE.
Lemma11. τ tr(Σ H¯ ) dL.
t=1 t t ≤ η
ThefollowingPlemmaaffordscontroloverthebonusesweaddintheadversarialsetting.
Lemma 12. The number of rounds where the bonus is non-zero is at most m dL. Furthermore, suppose that
τ
x ∈Rdand kx −µ t k2 Σ− t1 ≥F maxforsomet ≤τ. Then, t s=1♭ s(x) ≤−γF 2m 4ax. ≤
Sinceγ = 1 ,thelemmaalsoshowsthatw [1/2,1].P
4dL t ∈
10OnlineNewtonMethodfor BanditConvexOptimisation
Step3: Regret Wearefinallyinapositiontoboundtheregretuntilroundτ. Westartbyboundingtheregretinthe
adversarialsetting.
RegretintheAdversarialSetting Supposethatxs K andthatthealgorithmdidnotrestartattheendofround
⋆,τ ∈ τ
τ. ByTheorem39,
1 diam(K)2 τ τ
xs µ 2 +2η2 g 2 + ♭ (xs ) ηRegqˆ(xs )
2 ⋆,τ − τ+1 Σ− τ+1 1 ≤ 2σ2 k t kΣt+1 t ⋆,τ − τ ⋆,τ
t=1 t=1
(cid:13) (cid:13) X X
(cid:13) (cid:13) diam(K)2 τ
≤ 2σ2
+2η2 kg
t
k2
Σt+1
−ηRegq τ(xs ⋆,τ)+ηC
τ
(EventE3)
t=1
X
diam(K)2
+η2ndL4 ηRegq(xs )+ηC (Eq.(6))
≤ 2σ2 − τ ⋆,τ τ
4(d+1)2
+η2ndL4+ηC ηRegs(xs ) (Eq.(1)andLemma27)
≤ σ2 τ − τ ⋆,τ
γ
F ηRegs(xs ).
≤ 4 max − τ ⋆,τ
Reordering the above and using that ηRegf(x ) ηRegq(x ) + dL by Lemmas 24, 26, and 11 shows that
τ ⋆ ≤ τ ⋆ λ
max Regf(x) γFmax. Furthermore, by definition of xs we have Regs(xs ) Regs(xsˆ ) and conse-
x ∈K τ ≤ η ⋆,τ − τ ⋆,τ ≤ − τ ⋆,τ
quently
1 xs µ 2 1 F ηRegs(xsˆ )(a) 1 F ηRegsˆ(xsˆ )+ηC (b) 1 F ,
2 ⋆,τ − τ+1 Σ− τ+1 1 ≤ 4 max − τ ⋆,τ ≤ 4 max − τ ⋆,τ τ ≤ 2 max
(cid:13) (cid:13)
(cid:13) (cid:13)
where the (a) is due to event E3 and (b) is due the assumptionthat the algorithmdid notrestart, which meansthat
−ηRegs τˆ(xs ⋆ˆ ,τ)
≤
γF 3m 2ax bydefinition. Thisimpliesthatxs
⋆,τ ∈
K τ+1. OneventE0,theonlywaythatτ 6= nisthat
thealgorithmhasrestartedorxs / K . Henceweonlyneedtoshowthatifthelatterhappens,thenthealgorithm
⋆,τ ∈ τ
restartsandifthealgorithmrestarts,thentheregretuntiltimeτ isnegative.
RegretintheStochasticSetting Observethattheregreton(f )canbeboundedby:
t
0 ≤ηRegf τ(x ⋆) ≤ηRegq τ(x ⋆)+dLλ−1, (7)
Inthestochasticsetting,themostimportantdifferenceisthatf isconstantovertimeandthereforex = x isalso
t ⋆,τ ⋆
constant.Thismeansthatwedonothavetorestarttomakesurethatx staysinK ,andinturnthismeansthatwedo
⋆ τ
nothavetoresorttouniformconcentrationbounds.Aftertheapplyingthesamestepsasintheadversarialsettingand
replacingxs byx andC byC˜ ,leadsto
⋆,τ ⋆ τ τ
1 4(d+1)2 dL 1
2kx ⋆ −µ τ+1 k2 Σ− τ+1 1 ≤ σ2 +η2ndL4+ λ +ηC˜ τ −ηRegf τ(x ⋆) ≤ 2F max −ηRegf τ(x ⋆).
RearrangingandusingthatRegf(x ) 0showsthatwithprobabilityatleast1 3nδwehavethatRegf(x ) Fmax
τ ⋆ ≥ − τ ⋆ ≤ 2η
andthatx K . Thus,aunionboundoverT completestheproofofTheorem2.
⋆ τ+1
∈
Step4: Restartcondition Supposethatxs / K . Wewillshowthatthealgorithmrestarts. Sincethesurrogate
losses(s )areconvexthereexistsy argm⋆, iτ n∈ τ τ s (x)suchthaty ∂K . BythedefinitionofK there
u ∈ x ∈Kτ t=1 t ∈ τ τ
P
11OnlineNewtonMethodfor BanditConvexOptimisation
existsat ≤τ suchthat ky −µ t k2 Σ− t1 =F max. Therefore,
τ τ τ
η sˆ(µ ) sˆ(xsˆ ) =η sˆ(y) sˆ(xsˆ ) +η (sˆ(µ ) sˆ(y))
t t − t ⋆,τ t − t ⋆,τ t t − t
t=1 t=1 t=1
X(cid:0) (cid:1) X(cid:0) (cid:1) X
τ
ηC
τ
+η (sˆ t(µ t) sˆ t(y)) (EventE3,def.y)
≤ −
t=1
X
τ
3ηC
τ
+η (qˆ t(µ t) qˆ t(y)) (EventE3,Lemma27)
≤ −
t=1
X
diam(K)2 τ τ
3ηC + +2η2 g 2 + ♭ (y) (Theorem39)
≤ τ σ2 k t kΣt t
t=1 t=1
X X
diam(K)2 τ γF
3ηC + +2η2 g 2 max (Lemma12)
≤ τ σ2 k t kΣt − 16
t=1
X
4(d+1)2 γF
3ηC + +η2dnL4 max (Eq.(1)andEq.(6))
≤ τ σ2 − 16
γF
max
,
≤− 32
whichmeansthatarestartistriggeredasrequired.
Step5: Regretonrestart The lastjobisto provetheregretisnegativewhenevera restartis triggered. Fromthe
proofofLemma23wecanrecoverthatf (x ) f (x ) 8L.Supposethattherestartdoeshappenattheend
τ ⋆,τ 1 τ ⋆,τ
ofroundτ. Then, − − ≤
τ
ηRegf(x )=ηRegf(x )+η (f (x ) f (x ))
τ ⋆,τ τ ⋆,τ −1 t ⋆,τ −1 − t ⋆,τ
t=1
X
ηRegf(x )+η(f (x ) f (x )) (def.ofx )
≤ τ ⋆,τ −1 τ ⋆,τ −1 − τ ⋆,τ ⋆,τ −1
ηRegf(x )+8ηL
≤ τ ⋆,τ −1
τ
dL
η (s (µ ) s (x ))+ +8ηL (Lemma26)
t t t ⋆,τ 1
≤ − − λ
t=1
X
τ
dL
η (sˆ t(µ t) sˆ t(x
⋆,τ
1))+ +2ηC
τ
+8ηL (EventE3)
≤ − − λ
t=1
X
τ
dL
η sˆ(µ ) sˆ(xsˆ ) + +2ηC +8ηL (x K anddef.ofxsˆ )
≤ t t − t ⋆,τ λ τ ⋆,τ −1 ∈ τ ⋆,τ
t=1
X(cid:0) (cid:1)
dL γF
+ηC +8ηL max (Restartcondition)
τ
≤ λ − 32
<0.
Inotherwords,theregretisnegativeifarestartoccurs.
Proofsummaryadversarialsetting Instep1weshowedthatwithhighprobabilityeventE0holdsandthealgorithm
behavesinapredictablefashion.Instep3weshowedthattheregretuntiltimeτ isatmost γFmax.Instep4weshowed
η
thatthe onlyway τ = nunder E0 isif thealgorithmrestartsafterroundτ andinstep 5we showedthatif a restart
6
occurstheregretisnegative.Thetheoremfollowsbysummingtheregretovertheeachrestart.
7 Discussion
Submodularminimisation OuralgorithmcanbeappliedtobanditsubmodularminimisationviatheLova´szexten-
sion, as explained by HazanandKale [2012], who prove a bound of O(n2/3). All the conditionsof our algorithm
are satisfied using this reductionand since K = [0,1]d can be efficiently positionedto a scaling of John’sposition
12OnlineNewtonMethodfor BanditConvexOptimisation
our algorithms manages d3.5√npolylog(n,d,1/δ) in the adversarial setting and d1.5√npolylog(n,d,1/δ) in the
stochasticsetting.
Constants Wehavechosennottogiveexplicitconstantsforthetuningparameters.Thesituationisabitunfortunate.
Thetheoreticallyjustifiedconstantswillyieldarathersmalllearningrate,alargeregretboundandanalgorithmthat
convergesratherslowly. Ontheotherhand,ifyouchoosethelearningrateη orλtoolarge,thenthealgorithmmay
failcatastrophically.
Self-concordant barriers Another natural approach would be to use self-concordantbarriers as regularizers. In-
deed,thisyieldssublinearregretbounds. WemodifiedthealgorithmofLattimoreandGyo¨rgy[2023]touseaν-self
concordant barrier rather than the Euclidean norm as a regularizer and added some other tricks. In the stochastic
setting,thebestregretboundwecouldprovewasν1.5d1.5√npolylog(n,d,1/δ). Intheadversarialsetting,thebest
boundwecouldprovewasν4d3.5√npolylog(n,d,1/δ). Themajortechnicalchallengecamefromprovingthatthe
hessian ofthe potentialwas positivedefinite: with a self-concordantbarrieras the regularizerthe hessian atx is no
longerindependentofx. Thisforcedustoprovethatthehessianofthepotentialwasproportionalforallxinthefocus
regionofthealgorithm,whichiswheretheν-factorscomefromintheaforementionedregretbounds.Sinceν =O(d)
forarbitraryK theseboundsareconsiderablyworsethantheresultspresentedinthispaper.
Acknowledgements This workwas partiallydonewhile DvdH was atthe Universityof Amsterdamsupportedby
Netherlands Organization for Scientific Research (NWO), grant number VI.Vidi.192.095. Thank you also to our
colleaguesTimvanErven,WouterKoolenandAndra´sGyo¨rgyformanyusefuldiscussions.
References
A.Agarwal,D.P.Foster,D.Hsu,S.M.Kakade,andA.Rakhlin.Stochasticconvexoptimizationwithbanditfeedback.
SIAMJournalonOptimization,23(1):213–240,2013.
S.Artstein-Avidan,A.Giannopoulos,andV.D.Milman.Asymptoticgeometricanalysis,PartI,volume202.American
MathematicalSoc.,2015.
F.Barthe. Anextremalpropertyofthemeanwidthofthesimplex. MathematischeAnnalen,310:685–693,1998.
S. Brazitikos, A. Giannopoulos, P. Valettas, and B.-H. Vritsiou. Geometry of isotropic convex bodies, volume 196.
AmericanMathematicalSoc.,2014.
S.BubeckandR.Eldan. Exploratorydistributionsforconvexfunctions. MathematicalStatisticsandLearning,1(1):
73–100,2018.
S.Bubeck,O.Dekel,T.Koren,andY.Peres. Banditconvexoptimization:√T regretinonedimension.InConference
onLearningTheory,pages266–278,2015.
S.Bubeck,Y.Lee,andR.Eldan. Kernel-basedmethodsforbanditconvexoptimization. InProceedingsofthe49th
AnnualACMSIGACTSymposiumonTheoryofComputing,STOC2017,pages72–85,NewYork,NY,USA,2017.
ACM. ISBN978-1-4503-4528-6.
S.Finch. Meanwidthofaregularsimplex. arXivpreprintarXiv:1111.4976,2011.
A.Flaxman,A.Kalai,andH.McMahan. Onlineconvexoptimizationinthebanditsetting: Gradientdescentwithout
agradient.InSODA’05:ProceedingsofthesixteenthannualACM-SIAMsymposiumonDiscretealgorithms,pages
385–394,2005.
E.HazanandS.Kale. Onlinesubmodularminimization. JournalofMachineLearningResearch,13(10),2012.
E. Hazan and K. Levy. Bandit convex optimization: Towards tight bounds. In Advances in Neural Information
ProcessingSystems,pages784–792,2014.
E.HazanandY.Li. Anoptimalalgorithmforbanditconvexoptimization. arXivpreprintarXiv:1603.04350,2016.
E.Hazan,A.Agarwal,andS.Kale. Logarithmicregretalgorithmsforonlineconvexoptimization. MachineLearning,
69:169–192,2007.
R. Kannan, L. Lova´sz, and M. Simonovits. Isoperimetric problems for convex bodies and a localization lemma.
Discrete&ComputationalGeometry,13:541–559,1995.
R. Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In Advances in Neural Information
ProcessingSystems,pages697–704.MITPress,2005.
T. Lattimore. Improvedregretforzeroth-orderadversarialbanditconvexoptimisation. MathematicalStatisticsand
Learning,2(3/4):311–334,2020.
13OnlineNewtonMethodfor BanditConvexOptimisation
T.Lattimore. Banditconvexoptimisation. arXiv/2402.06535,v2,2024.
T.LattimoreandA.Gyo¨rgy. Improvedregretforzeroth-orderstochasticconvexbandits. InConferenceonLearning
Theory,pages2938–2964,2021.
T. Lattimore and A. Gyo¨rgy. A second-ordermethod for stochastic bandit convex optimisation. In Conference on
LearningTheory,2023.
T.LattimoreandC.Szepesva´ri. Banditalgorithms. CambridgeUniversityPress,2020.
L. Lova´sz and S. Vempala. Simulated annealing in convex bodies and an O (n4) volume algorithm. Journal of
∗
ComputerandSystemSciences,72(2):392–417,2006.
Z.Mhammedi.Efficientprojection-freeonlineconvexoptimizationwithmembershiporacle. InConferenceonLearn-
ingTheory,pages5314–5390,2022.
E. Milman. On the mean-width of isotropic convex bodies and their associated l p-centroid bodies. International
MathematicsResearchNotices,2015(11):3408–3423,2015.
Y.Nesterov. Lecturesonconvexoptimization,volume137. Springer,2018.
R.T.Rockafellar. Convexanalysis. Princetonuniversitypress,2015.
B.Simon. Convexity: ananalyticviewpoint,volume187. CambridgeUniversityPress,2011.
A.Suggala,P.Ravikumar,andP.Netrapalli.Efficientbanditconvexoptimization:Beyondlinearlosses.InConference
onLearningTheory,pages4008–4067,2021.
D. vanderHoeven,T.vanErven,andW. Kotłowski. Themanyfacesofexponentialweightsin onlinelearning. In
Proceedingsofthe31stConferenceonLearningTheory,pages2067–2092,2018.
R.Vershynin.High-dimensionalprobability:Anintroductionwithapplicationsindatascience,volume47.Cambridge
universitypress,2018.
J. Zimmertand T. Lattimore. Return of the bias: Almost minimax optimal high probability boundsfor adversarial
linearbandits. InConferenceonLearningTheory,pages3285–3312,2022.
14OnlineNewtonMethodfor BanditConvexOptimisation
A ProofofLemma 12
LetE = s t:♭ =0 and
t s
{ ≤ 6 }
c t = kµ s −z t k2 Σ− s1 .
s ∈XEt−1
Weproceedintwosteps. Firstweboundthenumberofnon-zerobonustermsandthenweshowthatthebonusesare
sufficientlynegative.
Step 1: Number of non-zero bonuses For the second part of the lemma we need to upperbound the number of
timesthat♭ =0. Bystandardlog-determinantarguments,
t
6
τ
dL
1 ♭ t 6=0and k·k2 Σ− t1
6≤
k·k2 Σ− s1
≤ 2
.
Xt=1 s ∈XEt−1
 
Inmoredetail,letU t−1 = s ∈EtΣ −s1andsupposethattisaroundwhere♭ t 6=0andΣ−t 1 6(cid:22)U t− −1 1. Then
P
U t− −1 1 (cid:22)U t−1 =U t− −1 1+Σ−t 1 6(cid:22)2U t− −1 1,
whereinthefirstinequalityweusedthefactthatΣ−t 1 (cid:23)0. Rearrangingshowsthat
1 (cid:22)U t− −1 1/2U tU t− −1 1/2 6(cid:22)21. (8)
Therefore,usingthefactsthatlogdetA1/2BA1/2 = logdetAB andforanypositivedefiniteAwith1 A 21,
(cid:22) 6(cid:22)
logdetA log(2),
≥
(a)
log(2) 1 Σ−t 1 6(cid:22)U t− −1 1 ≤ log(2)+ logdet U t−1U t −1
t X∈Eτ
(cid:0) (cid:1)
t ∈E Xτ,t>1
(cid:0) (cid:1)
( =b) log(2)+logdet U τ−1U
1
( =c) log(2)+logdet(cid:0) σ2 (cid:1) Σ 1
−s
!
s X∈Eτ
(d)
≤
log(2)+logdet 2σ2 Σ¯ −s1
!
s X∈Eτ
(e) 2nσ2
log(2)+logdet 1
≤ δ
(cid:18) (cid:19)
(f) dLlog(2)
.
≤ 2
where(a)followsfromEq.(8)andthefactthatfor1 A 21,logdetA log(2). (b)followsbytelescoping
thesum,(c)bythedefinitionofU t,(d)bythedefini(cid:22) tiono6(cid:22) fthestoppingtim≥ eτ, (e)sinceΣ¯
−t
1
(cid:22)
1 δ,(f)from
thedefinitionoftheconstants. Rearrangingshowsthat
dL
1 k·k2 Σ− t1
6≤
k·k2 Σ− s1
≤ 2
.
t X∈Eτ s ∈XEt−1
 
15OnlineNewtonMethodfor BanditConvexOptimisation
O
c
tn ≤th Fe mo at xh /e 2r 4ha an nd d, ks µup tp −os ze
t
kt
2
Σha
−
tt 1♭ ≥t 6=
Fm
30 axan and dk h·k e2 Σ n− t ce1 ≤ Ps ∈Et−1k·k2 Σ− s1. Bythedefinitionofthealgorithmwehave
c t+1 = ymi Rn dky −µ t k2 Σ− t1 + ky −µ s k2 Σ− s1

∈ s ∈XEt−1
 
( ≥a) ymi Rn dky −µ t k2 Σ t−1 + 1 2 ky −z t k2 Σ− s1 − kµ s −z t k2 Σ− s1
∈ s ∈XEt−1 s ∈XEt−1
( ≥b) ymi Rn d ky −µ t k2 Σ− t1 + 1 2ky −z t k2 Σ− t1
−
 kµ s −z t k2 Σ− s1
∈ (cid:18) (cid:19) s ∈XEt−1
( ≥c) 1 4kz t −µ t k2 Σ− t1
−
kµ s −z t k2 Σ− s1
s ∈XEt−1
1
( =d) 4kz t −µ t k2 Σ− t1 −c t
(e) F max
.
≥ 24
where(a)followsfromthetriangleinequalityandthefactthat(a+b)2 2a2+2b2. (b)usestheconditionthat
≤
ak s·k su2 Σ m− t1 pt≤ ionths
a∈
tE ct−1 <k· Fk2 Σ− s1 /. 8( .c T) heis rea fn oo reth te hr et tr oia tan lg nle umin beq erua ol fit ry ouan nd ds( wd h) ei ns ♭the =d 0efi in si ati to mn oo sf tc 1t da Lnd +( 1e)fo dl Llo .wsbythe
P t max t 6 2 ≤
Step2: Magnitudeofbonuses Lett
≤
τ andxsatisfy kx −µ t k2 Σ− t1
≥
F max. Supposethatc t
≥
F max/24. Then,
bythedefinitionofz ,
t
F
kx −µ s k2 Σ− s1
≥
kµ s −z t k2 Σ− s1
≥
2m 4ax .
s ∈XEt−1 s ∈XEt−1
Andtheclaimfollows. Ontheotherhand,ifc <F /24and♭ =0,then
t max t
kx −µ s k2 Σ− s1 ( ≥a) 21 kx −z t k2 Σ− s1
−
kz t −µ s k2 Σ− s1
s ∈XEt−1 s ∈XEt−1 s ∈XEt−1
( ≥b) 1
2
kx −z t k2 Σ− s1
−
F 2m 4ax
s ∈XEt−1
( ≥c) 1 2kx −z t k2 Σ− t1
−
F 2m 4ax
( ≥d) 41 kx −µ t k2 Σ− t1
−
1 2kz t −µ t k2 Σ− t1
−
F 2m 4ax
(e) F max
,
≥ 24
where(a)followsfromthe triangleinequalityandbecause(a+b)2 2a2+2b2. (b)followsfromthe assumption
≤
t ah na dt (c et )< folF lom wax s/ b2 e4 c. a( uc s) ef ♭o tll =ow 0s sb oec tha au tse kz♭ tt −= µ0 t kso 2 Σ− tth 1a ≤t PF ms ∈ aE x/t− 31 .k·k2 Σ− s1 > k·k2 Σ− t1. (d)usesthesameargumentas(a)
B ProofofLemma 10
Bythedefinitionofτ,fort τ,
≤
Σ 2Σ¯ 4Σ¯ =4σ21. (9)
t t 1
(cid:22) (cid:22)
ByaunionboundandLemma23,withprobabilityatleast1 nδ,forallt τ,
− ≤
Y 4L+2v(X ).
t t
≤
16OnlineNewtonMethodfor BanditConvexOptimisation
Onthisevent,usingthefactthat(a+b)2 2a2+2b2,
≤
τ τ
M = Y2 32nL2+8 v(X )2. (10)
τ t ≤ t
t=1 t=1
X X
ByCorollary20,
τ τ
E
t
1[v(X t)2](a) n+ 2M d kΣ t ktr(Σ tE
t
1[v′′(X t)])
t=1
− ≤
t=1
pε −
X X
(b) τ 4σM√d
n+ tr(Σ E [v (X )])
t t 1 ′′ t
≤ ε −
t=1
X
(c) τ 4σM√d
n+ tr(Σ E [f (X )])
t t 1 ′′ t
≤ ε −
t=1
X
(d) 8σM√d τ τ 4δσM√d
n+ tr(Σ s (µ ))+ tr(Σ +1)
≤ ελ
t ′t′ t
ε
t
t=1 t=1
X X
(e) 3n 8σM√d τ
+ tr Σ H¯
t t
≤ 2 ελ
t=1
X (cid:0) (cid:1)
(f) 3n 8σd3/2ML
+
≤ 2 ελη
(g)
2n, (11)
≤
where(a)followsfromCorollary20,(b)fromEq.(9),(c)sincef v ,(d)fromProposition30,(e)since
s (µ ) = H¯ andnaivesimplificationusingδ,
(f)followsfromLemt′ m′
a(cid:23)
11′ .′
Movingon,since µ K andusing
′t′ t t t
∈
ε
Lemma3andthedefinitionv(x)=(π (x) 1)/εwehave
+ −
2 2
v(X ) v(µ )+ X µ = X µ .
t t t t t t
≤ ε k − k ε k − k
Defineanevent
dσ2L
E = v(X )2 .
t t ≤ ε2
(cid:26) (cid:27)
ByEq.(9),Σ 4σ21fort τ. Hence,byLemma15,
t
(cid:22) ≤
τ
P Ec δ.
t !≤
t=1
[
Therefore,withprobabilityatleast1 δ,
−
τ τ
v(X )2 = 1 v(X )2.
t Et t
t=1 t=1
X X
Hence,byCorollary18,withprobabilityatleast1 δandwithν =
ε2
,
− dσ2L
τ τ 1 1 dσ2L2
1 v(X )2 2 E [v(X )2]+ log 4n+ 5n.
Et t ≤ t −1 t ν δ ≤ ε2 ≤
t=1 t=1 (cid:18) (cid:19)
X X
whereinthelasttwostepsweusedEq.(11)andthedefinitionoftheconstants. Combiningthiswithaunionbound
andEq.(10)showsthatwithprobabilityatleast1 (2+n)δ,
−
τ
M 32nL2+8 v(X )2 32nL2+40n 100nL2.
τ t
≤ ≤ ≤
t=1
X
Onthesameevent,byLemma23andEq.(11)itfollowsalsothat
τ τ
M¯ = E [Y ]2 24 E [v(X )2]+12n 100nL2.
τ t 1 t t 1 t
− ≤ − ≤
t=1 t=1
X X
17OnlineNewtonMethodfor BanditConvexOptimisation
C ProofofLemma 8
Bydefinition,
x 2 t t
Φ (x)= k k +η qˆ(x)+ ♭ (x)
t 2σ2 s s
s=1 s=1
X X
=Φ (x)+ηqˆ(x)+♭ (x)
t 1 t t
−
=Φ t −1(x)+ηqˆ t(x) −γ t kx −x t k2 Σ− t1 .
NotethatΦ tisquadratic,sowewilluseΦ
′t′
foritsHessianatanypoint.Bydefinition,Σ−t+1
1
=Φ ′t′andtherefore
ηH ηH
Σ−t+1
1
=Φ
′t′
=Φ
′t′
−1+ 2t −2γ tΣ−t 1 =(1 −2γ t)Σ−t 1+ 2t .
Byinductionitfollowsthat
Σ−t 1 =
w
t σ−
211
+w t
−1η
2
t −1 H
w
uu .
u=1
X
LetEbetheevent {Y
t
≤
10 εL }∩E4. ByLemma23andLemma10andaunionbound,P(E) ≥1 −5nδ.Combining
thiswithLemma38andaunionboundshowsthatwithprobabilityatleast1 −6nδforallΣ −1suchthatΣ−t 1 (cid:22)Σ −1
forallt τ,
≤
τ H H¯ d2 1
η s − s ηλL dM¯
τ
+d2Y
max
Σ −1 10ηλL2 √dn+ Σ −1 Σ −1, (12)
(cid:12) (cid:12)Xs=1 w s (cid:12) (cid:12)(cid:22) (cid:16)p (cid:17) (cid:22) (cid:18) ε (cid:19) (cid:22) 8
(cid:12) (cid:12)
(cid:12) (cid:12)
whereinthefi(cid:12)nalinequality(cid:12)weusedthedefinitionoftheconstants. Weassumefortheremainderthattheaboveholds
andprovethatonthiseventtherequiredboundsonthecovariancematriceshold,whichestablishesthelemma.Bythe
definitionofτ,foranys τ,
≤
Σ −s1 (cid:22)2Σ¯ −s1 (cid:22)4Σ¯ −τ+1 1,
whereinthefirstinequalityweusedthedefinitionofτ andinthesecondweusedthatH¯ 0andw [1/2,1]for
t t
allt ≤τ. Therefore,byEq.(12)withΣ −1 =8Σ¯ −τ+1 1, (cid:23) ∈
w 1 η τ H w 1 η τ H¯ 1 3
Σ−τ+1
1
= στ
2
+ 2w
τ
ws
(cid:22)
στ
2
+ 2w
τ
ws + 2Σ¯ −τ+1
1
= 2Σ¯ −τ+1 1.
s s
s=1 s=1
X X
Similarly,
w 1 η τ H w 1 η τ H¯ 1 1
Σ−τ+1
1
= στ
2
+ 2w
τ
ws
(cid:23)
στ
2
+ 2w
τ
ws
−
2Σ¯ −τ+1
1
= 2Σ¯ −τ+1 1.
s s
s=1 s=1
X X
D ProofofLemma 6
We start with a standard concentration argument. Since ℓ (A ) [0,1], by Hoeffding–Azuma’s inequality, with
t t
∈
probabilityatleast1 δ,
−
n
(ℓ (A ) E [ℓ (A )]) √nL.
t t t 1 t t
(cid:12) − − (cid:12)≤
(cid:12)Xt=1 (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
18OnlineNewtonMethodfor BanditConvexOptimisation
Onthisevent,
n
Reg =max (ℓ (A ) ℓ (x))
n x K t t − t
∈ t=1
X
n
√nL+max (E [ℓ (A )] ℓ (x))
t 1 t t t
≤ x K − −
∈ t=1
X
n
=√nL+max (E [ℓ (X /π (X ))] ℓ (x))
x K t −1 t t + t − t
∈ t=1
X
n
√nL+εn+ max (E [ℓ (X /π (X ))] ℓ (x))
≤ x ∈Kεt=1 t −1 t t + t − t
X
n
√nL+εn+ max (E [f (X )] f (x))
t 1 t t t
≤ x ∈Kεt=1 − −
X
=√nL+nε+ maxRegf(x), (13)
n
x ∈Kε
wherethesecondinequalityfollowsbecausethelossesareboundedin[0,1]onK andusing[Lattimore,2024,Propo-
sition3.7].ThelastinequalityfollowsfromLemma4andRemark5.
E ProofofLemma 11
This result is more-or-lessa consequenceof the standard trace/log-determinantinequality. There is a little delicacy
neededinarrangingasuitabletelescopingsum,however.Let
Σ˜ −t 1 =
1
4
σ1
2 +
t −1
H¯ u .
" #
u=1
X
Recallthat
Σ¯ −t 1 =w t −1
"σ1
2 +
t −1 H w¯
uu
#
,
u=1
X
wheretheweights(w )arein[1/2,1]almostsurely.Hence,foranyt τ,
u
≤
1 1 1
Σ˜ −t 1
(cid:22)
2Σ¯ −t 1 (cid:22)Σ−t 1, Σ˜ −t 1
(cid:23)
8Σ¯ −t 1
(cid:23)
16Σ−t 1. (14)
with bothof the secondinequalitiesfollowingfromthe definitionof the stoppingtime τ. Therefore,by Lemma13,
Lemma25
16ηλLip(f ) 32ηλLip(f )
η Σ˜1/2H¯ Σ˜1/2 16η Σ1/2H¯ Σ1/2 t d Σ t √dσ2 1, (15)
t t t ≤ t t t ≤ 1 λ k t k≤ 1 λ ≤
(cid:13) (cid:13) (cid:13) (cid:13) − p −
whereinthe(cid:13)firstinequalit(cid:13)yweuse(cid:13)dLemma13,(cid:13)thesecondfollowsfromLemma25, thethirdfromLemma21and
(cid:13) (cid:13) (cid:13) (cid:13)
thelastfromLemma22andthedefinitionoftheconstants.Wearenowreadytoboundthequantityofinterest:
τ τ τ
tr(H¯ Σ )(a) 1 tr(ηΣ˜1/2H¯ Σ˜1/2)(b) 1 logdet 1+ηΣ˜1/2H¯ Σ˜1/2
t t ≤ η t t t ≤ η t t t
Xt=1 Xt=1 Xt=1 (cid:16) (cid:17)
τ τ
1 1
=
η
logdet Σ˜ t Σ˜ −t 1+ηH¯ t =
η
logdet Σ˜ tΣ˜ t+1
Xt=1 (cid:16) (cid:16) (cid:17)(cid:17) Xt=1 (cid:16) (cid:17)
t 1
= η1 logdet Σ˜ 1Σ˜ −τ+1
1
= η1 logdet 1+4σ2 − H¯
u
!
(cid:16) (cid:17) u X=1
(c) d
log 1+
4σ2 t −1
tr(H¯ )
(d) dL
,
u
≤ η d ! ≤ η
u=1
X
19OnlineNewtonMethodfor BanditConvexOptimisation
where in (a) we used Eq. (14) and the fact that for positive definite A,B,M with A B, tr(AM) tr(BM) =
tr(B1/2MB1/2). In(b)weusedEq.(15)andthefactthatforpositivedefiniteAwith (cid:22) A 1,tr(A)≤ logdet(1+
k k≤ ≤
A), which in turn follows from the inequality x log(1+x) for x [0,1]. (c) uses the inequality logdetA
≤ ∈ ≤
dlog(tr(A)/d), which is a consequence of the arithmetic/geometric mean inequality. Finally, (d) follows because
H¯ =s (µ ) 11byLemma21.
t ′t′ t (cid:22) δ
F Technical lemmas
Lemma13. SupposethatA B andM ispositivedefinite.Then A1/2MA1/2 B1/2MB1/2 .
(cid:22) ≤
(cid:13) (cid:13) (cid:13) (cid:13)
Proof. NotethatbothA1/2MA1/2 andB1/2MB1/2 arepositived(cid:13)efinite. Since(cid:13)A (cid:13)B,itfollowst(cid:13)hatA1/2B(1)
B1/2B(1). Therefore (cid:22) ⊂
B1/2MB1/2 = max x B1/2MB1/2x
⊤
x B(1)
(cid:13) (cid:13) ∈
(cid:13) (cid:13)= max y My
(cid:13) (cid:13) ⊤
y B1/2B(1)
∈
max y⊤My
≥y A1/2B(1)
∈
= max x A1/2MA1/2x
⊤
x B(1)
∈
= A1/2MA1/2 .
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)
Lemma14(Theorem5.2.2,Vershynin2018). Leth : Rd Randδ (0,1)andX havelaw (µ,Σ). Then,with
→ ∈ N
probabilityatleast1 δ,
−
E[h(X)] h(X) CLip(h) Σ log(1/δ)
| − |≤ k k
Lemma15. LetW havelaw (0,1). Thenforanyδ (0,1),P(pW 2 2d/3log(2/δ)) δ.
N ∈ k k2 ≥ ≤
p
Proof. TheprooffollowsbypluggingFact11a)fromLattimoreandGyo¨rgy[2023]intoLemma12aofthesamework
(i.e. Proposition2.7.1ofVershynin[2018]).
Lemma16(Proposition2.7.1,Vershynin[2018]). LetXbearandomvariablesuchthatE[X]=0andE[exp(X2)]
≤
2. Then
P(X log(2/δ)) δ.
| |≥ ≤
p
ThefollowingisastandardBernstein-likeconcentrationinequality:
Lemma17. [Exercise5.15,LattimoreandSzepesva´ri2020]LetX ,...,X beasequenceofnon-negativerandom
1 n
variablesadaptedtoafiltration(F )n andτ astoppingtime. Then,foranyν suchthatνX 1almostsurelyand
t t=1 t ≤
δ (0,1),withprobabilityatleast1 δ,
∈ −
τ τ τ
log(1/δ)
X E[X F ]+ν E[X2 F ]+ .
t ≤ t | t −1 t| t −1 ν
t=1 t=1 t=1
X X X
Corollary18. UnderthesameconditionsasLemma17,
τ τ
log(1/δ)
X 2 E[X F ]+ .
t t t 1
≤ | − ν
t=1 t=1
X X
Proof. Usethefactthat0 νX 1almostsurelytoboundνE[X2 F ] E[X F ]andapplyLemma17.
≤ t ≤ t| t −1 ≤ t | t
The next proposition has the flavour of a Poincare´ inequality but it exploits convexity in a way that the standard
inequalitydoesnot.
20OnlineNewtonMethodfor BanditConvexOptimisation
Proposition19. Supposethat h : Rd R is convex, non-negativeand Lip(h) 1 and X has law (µ,Σ) and
→ ≤ N
h(µ)=0and Σ 1. Then,
k k≤
E[h(X)2] 1+ E[h(X)]+ Σ L tr(ΣE[h (X)]).
′′
≤ k k
(cid:16) p (cid:17)
Proof. Assumewithoutlossofgeneralitythatµ = 0andletE = h(X) E[h(X)]+ Σ L ,Then,usingthe
factthathisLipschitzandh(0)=0, { ≥ k k }
p
E[h(X)21 ] E[ X 21 ] E[ X 4P(Ec)] 1, (16)
Ec Ec
≤ k k ≤ k k ≤
q
whereinthelastinequalityweusedLemma14andalargeenoughchoiceofLandtheassumptionthat Σ 1to
k k ≤
boundE[ X 4]. Ontheotherhand,
k k
E[h(X)21 ] E[h(X)]+ Σ L E[h(X)]
E
≤ k k
(cid:16) E[h(X)]+p Σ L(cid:17) tr(ΣE[h′′(X)]), (17)
≤ k k
wherethefinalequalityfollowssince (cid:16) p (cid:17)
E[h(X)] E[ h(X),X ] hconvex,h(0)=0
′
≤ h i
=E[ Σh′(X),Σ−1X ]
=tr(ΣE[h (X)]) . integratingbyparts
(cid:10) ′′ (cid:11)
TheclaimfollowsbycombiningEq.(16)andEq.(17).
Corollary20. Supposethatµ K andX haslaw (µ,Σ)withΣ 0. Then
∈ N (cid:23)
(a) E[v(X)] M√d kΣ k.
≤ 2ε
(b) E[v(X)2] ≤1+ 2M√ εd kΣ ktr(ΣE[v ′′(X)]).
Proof. LetW havelaw (0, Σ 1)andN havelaw (0, Σ 1 Σ),whichisalegitimateGaussiansince Σ 1
Σ 0. LetX,N andWN beink dek pendent,whichmeanN sthatk Xk +N− hasthesamelawasW. Byconvexity, k k −
(cid:23)
1
E[v(X)] E[v(X +N)]=E[v(µ+W)]= E[π (µ+W) 1]
≤ ε + −
1 1 1
= [max(π(µ+W),1) 1] [max(π(µ)+π(W),1) 1] E[π(W)],
ε − ≤ ε − ≤ ε
whereinthesecondlastinequalityweusedthefactthattheMinkowskifunctionalπissubadditiveandinthelastthat
µ K sothatπ(µ) 1. NotethatW/ W and W areindependentandtheformerusuniformlydistributedon
S(1∈
)sothat
≤ k k k k
M M d Σ
E[π(W)]=E[π(W/ W ) W ]=E[π(W/ W )]E[ W ] E[ W ] k k,
k k k k k k k k ≤ 2 k k ≤ 2
p
where the first inequality follows from the definition of M and the second by Cauchy-Schwarz and because
E[ W 2]=d Σ . Thiscompletestheproofof(a). For(b),byProposition19,
k k k k
√L
E[v(X)2] 1+ E[v(X)]+ tr(ΣE[v (X)])
′′
≤ ε !
1 M d Σ
1+ k k +√L tr(ΣE[v′′(X)]).
≤ ε p2 !
Theresultfollowsbynaivesimplification.
Lemma21. Supposethats t τ. Thefollowinghold:
≤ ≤
(a) Σ¯ 2Σ¯ .
t s
(cid:22)
21OnlineNewtonMethodfor BanditConvexOptimisation
(b) Σ 4σ21 1.
t
(cid:22) (cid:22)
(c) Σ¯
−t
1
(cid:22)
δ11.
(d) H¯ t−1
(cid:22)
δ11.
Proof. Bydefinition,
Σ¯ −t 1 =w t −1
"σ1
2
+ηt −1 H w¯
uu #(cid:23)w t −1
"σ1
2
+ηs −1 H w¯
uu #=
ww
st −1 1Σ¯ −s1 (cid:23)
1
2Σ¯ −s1.
u X=1 u X=1 −
Thisestablishes(a). Part(b)followsform(a)andthedefinitionofτ,whichyieldsΣ 2Σ¯ . Theclaimfollowssince
t t
Σ¯ =σ21. Forpart(c)weproceedbyinduction. Supposethatforallu<tthatΣ 1(cid:22) 11,whichisplainlytruefor
1 −u (cid:22) δ
u=1. Then
Σ¯ −t 1 = (cid:13)w t −1
"σ1
21+
t −1 H w¯
uu
#(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)1 t −1 u X=1 (cid:13) (cid:13) (cid:13)
(cid:13) +2 s (µ ) (cid:13)
≤ σ2 k ′u′ u k
u=1
X
t 1
≤
σ1
2
+
2λ 1Lip( λf t) −
d Σ−u1 Lemma25
− u X=1q
(cid:13) (cid:13)
1 2nλLip(f ) d/δ (cid:13) (cid:13)
t
+
≤ σ2 1 λ
− p
1
,
≤ δ
whereinthefirstinequalityweusedthefactthattheweights(w ) [1/2,1]andthedefinitionofH¯ = s (µ )and
t
∈
u ′u′ u
thetriangleinequality.Part(d)followsnaivelyfromLemma25andpart(c).
Lemma22. TheLipschitzconstantoftheextensionisboundedby
15d
Lip(f ) .
t
≤ ε
Proof. Bydefinition,
x 2(π (x) 1)
f (x)=π (x)ℓ + + − .
t + t π (x) ε
(cid:18) + (cid:19)
NotethatLip(π (x)) 2andℓ (x) [0,1]forallx K byassumption.Furthermore,sinceB(1) K,x+B(ε)
Kforallx K+ andb≤ y[Lattimt ore,2∈ 024,Proposition∈ 3.6],Lip (ℓ ) 1.Sincef isdefinedevery⊂ where,itsuffice⊂ s
∈ ε Kε t ≤ ε t
toboundthemagnitudeofitsgradientsoutsideK. Letx / K whereπ (x)=π (x). Sinceπ isthesupportfunction
ofK itssubgradientsareinK . Leth
S(1)bearbitrar∈
yandθ
K+ besuchε
thatDπ
(x)[ε
h]= θ,h . Then,
ε ε◦
∈ ∈
ε◦ ε
h i
2 x x x θ,h
Df (x)[h]= θ,h +ℓ +Dℓ h h i
t t t
h i ε π (x) π (x) − π (x)
(cid:20) (cid:18) + (cid:19)(cid:21) (cid:18) + (cid:19)(cid:20) + (cid:21)
2 1 x θ,h
θ +1 + h h i
≤k k ε ε − π (x)
(cid:20) (cid:21) (cid:13) + (cid:13)
2 1(cid:13) x θ (cid:13)
(cid:13) (cid:13)
θ +1 + (cid:13)+ k kk k (cid:13)
≤k k ε ε επ (x)
(cid:20) (cid:21) +
2 1 4(d+1)
2 +1 + +
≤ ε ε ε
(cid:20) (cid:21)
15d
,
≤ ε
whereweusedtheassumptionthatK B(2(d+1))andε (0,1/2)sothatθ K B(1/2).
⊂ ∈ ∈
ε◦
⊂
22OnlineNewtonMethodfor BanditConvexOptimisation
Lemma23. Supposethatt τ. ThefollowingholdwithP -probabilityatleast1 δ:
t 1
≤ − −
(a) Y 4L+2v(X ).
t t
| |≤
(b) Y 8L.
| t |≤ ε
(c) E [Y2] 24E [v(X )2]+12.
t −1 t ≤ t −1 t
Proof. Sincet τ,byLemma21,Σ 1. Aunionboundcombinedwiththefactthatπ is2-Lipschitz(Lemma3)
≤ t (cid:22) +
andtheassumptionthatthenoiseissubgaussianandLemma14showsthatwithprobabilityatleast1 δ
−
π (X ) E [π (X )]+ Σ L E [π (X )]+√L and ε √L. (18)
+ t ≤ t −1 + t k t k ≤ t −1 + t | t |≤
Fortheremainderoftheproofweassumetpheaboveholds.ByCorollary20(a),
M d Σ
E [π (X )] 1+ k t k 1+σM√d 2,
t −1 + t ≤ p2 ≤ ≤
whichwhencombinedwithEq.(18)impliesthatπ (X ) 2√L. Bydefinition,
+ t ≤
Y =π (X )ℓ (X /π (X ))+π (X )ε +2v(X )
t + t t t + t + t t t
2√L+2L+2v(X )
t
≤
4L+2v(X )
t
≤
2π (X ) 2
=4L+ + t −
ε
4√L 2
4L+ −
≤ ε
8L
.
≤ ε
Thisestablishesboth(a)and(b).Forthelastpart,usingthefactthat(a+b+c)2 3a2+3b2+3c2,
≤
E [Y2]=E (π (X )ℓ (X /π (X ))+π (X )ε +2v(X ))2
t 1 t t 1 + t t t + t + t t t
− −
12E h [v(X )2]+3E [π (X )2]+3E [π (X )2ε2i ]
≤ t −1 t t −1 + t t −1 + t t
12E [v(X )2]+6E [π (X )2]
≤ t −1 t t −1 + t
12E [v(X )2]+12E [(π (X ) 1)2]+12
≤ t −1 t t −1 + t −
24E [v(X )2]+12,
t 1 t
≤ −
where in the second inequality we used the fact that E [ε2 X ] 1. The remaining steps follow from naive
simplification.
t −1 t| t ≤
G Properties ofthesurrogate
Inthissection we collectthe essentialpropertiesof thesurrogatelossandits quadraticapproximation. In oneform
oranother,mostoftheseresultsappearedintheworkbyLattimoreandGyo¨rgy[2021];LattimoreandGyo¨rgy[2023]
or(lessobviously)Bubecketal.[2017]. YoucanfindanextensiveexplanationintherecentmonographbyLattimore
[2024].
Basicproperties Westartwiththeelementarypropertiesofthesurrogate. Tosimplifynotation,letf :Rd Rbe
convex,letX havelaw (µ,Σ)withΣ 1 11andset →
N − (cid:22) δ
1 1
s(z)=E 1 f(X)+ f((1 λ)X +λz)
− λ λ −
(cid:20)(cid:18) (cid:19) (cid:21)
1
q(z)= s(µ),z µ + z µ 2 .
h
′
− i 4k −
ks′′(µ)
Lemma24(Lemma11.2,Lattimore2024). Thefollowinghold:
23OnlineNewtonMethodfor BanditConvexOptimisation
(a) sisoptimistic: s(z) f(z)forallz Rd;and
≤ ∈
(b) sisconvexandinfinitelydifferentiable.
Lemma25(Proposition11.4,Lattimore2024). Foranyz Rd:
∈
(a) s (z) λLip(f) d Σ 1 .
k ′′ k≤ 1 λ k − k
−
(b) Σ1/2s (z)Σ1/2 pλLip(f) d Σ .
′′ ≤ 1 λ k k
−
Lemma2
(cid:13)
(cid:13)6(Proposition1
(cid:13)
(cid:13)1.7,Lattimopre2024). Ifλ
≤
dL1 2,then
2 2δd
E[f(X)] s(µ)+ tr(s (µ)Σ)+ .
′′
≤ λ λ
Thequadraticsurrogateisclosetosonaregionaroundxandconsequently:
Lemma27(Proposition11.6,Lattimore2024). Forallz Rdsuchthatλ z µ 1/L,
∈ k −
kΣ−1
≤
δ
s(µ) s(z) q(µ) q(z)+ .
− ≤ − λ2
CombiningLemmas24,26and27withthedefinitionofδandλyieldsthefollowing.
Lemma28. Forallz Rdsuchthatλ z µ 1/L,
∈ k −
kΣ−1
≤
2 1
E[f(X)] f(z) q(µ) q(z)+ tr(s (µ)Σ)+ .
′′
− ≤ − λ n
Thereasonwhythequadraticsurrogateisareasonablealternativetothesurrogatesisbecausethesurrogateisnearly
quadratic:
Proposition29(Proposition11.3,Lattimore[2024]). Ifλ x y L 1/2,then
k −
kΣ−1
≤
−
s (x) 2s (y)+δΣ 1.
′′ ′′ −
(cid:22)
Basedonthisweproveanewresult:
Proposition30. Ifλ 1 ,thenE[f (X)] 2s′′(µ) +δ(Σ 1+1).
≤ dL2 ′′ (cid:22) λ −
Proof. LetX (µ,Σ),andξ2 = 2 1,whichischosensothatifZ haslaw (µ,ξ2Σ2)andisindependentof
∼ N λ − N
X,then(1 λ)X +λZ hasthesamelawasX. Thus,bydefinition
−
E[s (Z)]=λE[f ((1 λ)X +λZ)]=λE[f (X)].
′′ ′′ ′′
−
DefineaneventE = λ Z µ L 1/2 .Proposition29showsthatontheeventE,s (Z) 2s (µ)+δΣ 1,
{ k −
kΣ−1
≤
−
}
′′
(cid:22)
′′ −
whichmeansthat
E[s′′(Z)]=E[1 Es′′(Z)]+E[1 Ecs′′(Z)]
δΣ 1+2s (µ)+E[1 s (Z)]
− ′′ Ec ′′
(cid:22)
λLip(f)
(cid:22)δΣ−1+2s′′(µ)+
1 λ
d kΣ −1 k1P(Ec). byLemma25
WenowshowthatP(Ec)issmall. LetW havelaw (0,1). B− ythedpefinitionofZ,
N
1
P(Ec)=P Z µ 2 >
k − kΣ−1 Lλ2
(cid:18) (cid:19)
1
=P W 2 >
k k Lξ2λ2
(cid:18) (cid:19)
1
=P W 2 >
k k (2 λ)λL
(cid:18) − (cid:19)
exp( L). ByLemma15
≤ −
Therefore,
λE[f′′(X)]=E[s′′(Z)]
λLip(f)
(cid:22)2s′′(µ)+δΣ−1+
1 λ
d kΣ −1 kexp( −L)1
−
2s (µ)+δ(Σ 1+1), p
′′ −
(cid:22)
whereinthefinalinequalityweusedthedefinitionsoftheconstants.
24OnlineNewtonMethodfor BanditConvexOptimisation
Estimation Thesurrogatesisnotdirectlyobserved,butcanbeestimatedfromdatacollectedbyabanditalgorithm.
LetY =f(X)+εwhereE[ε]=0andE[exp(ε2)] 2andlet
≤
r(X,z) 1 1
sˆ(z)= 1+
λ
− Y qˆ(z)= hsˆ ′(x),z −x i+ 4kz −x k2
sˆ′′(x)
. (19)
(cid:18) (cid:19)
whererisachange-of-measurefunctiondefinedby
p X λz
1−λ
r(X,z)= − .
(1 (cid:16)λ)dp((cid:17)X)
−
ThegradientandHessianofsˆ aregivenby
t
Y R (z) X λz
sˆ ′t(z)= 1t t
λ
Σ−t 1 1t −
λ
−µ
t
− (cid:20) − (cid:21)
sˆ′t′(z)= λ (1Y t −R t λ( )z 2) Σ−t 1 (cid:20)X 1t −− λλz −µ
t
(cid:21)(cid:20)X 1t −− λλz −µ
t
(cid:21)⊤ Σ−t 1 −Σ−t 1
!
.
Lemma31(Lemma11.10,Lattimore2024). R (µ ) 3forallt.
t t
≤
Lemma32(Propositions11.6and11.7,Lattimore2024). Thefollowinghold:
(a) E[sˆ(z)]=s(z). (b) E[sˆ(z)]=s(z). (c) E[sˆ (z)]=s (z).
′ ′ ′′ ′′
Sequentialconcentration We also needconcentrationforsumsof surrogateestimates andquadraticsurrogatees-
timates. LetX ,Y ,...,X ,Y be a sequenceof randomelementsadaptedto a filtration(F )n andassumethat
1 1 n n t t=1
Y =f(X )+ε andassumethatP (X = )haslaw (µ ,Σ )andε isconditionallysubgaussian:
t t t t 1 t t t t
− · N
E[ε F ,X ]=0 E[exp(ε2)F ,X ] 2.
t | t −1 t t | t −1 t ≤
Letτ beastoppingtimewithrespectto(F )n andlet
t t=1
1 1
s (z)=E 1 f (X )+ f ((1 λ)X +λz)
t t t t t
− λ λ −
(cid:20)(cid:18) (cid:19) (cid:21)
1
q (z)= s (µ ),z µ + z µ 2 .
t
h
′t t
−
t
i 4k −
t ks′ t′(z)
Then let sˆ and qˆ be defined as in Eq. (19) but with X instead of X and similarly Y for Y and so on. We let
t t t t
Y =max Y . Lastly,givenanr >0,letK (r)=K and
max 1 t τ t 0
≤ ≤ | |
K t(r)= {x ∈K t −1(r):λ kx −µ t kΣ− t1 ≤r }.
Lemma33(Proposition11.19,Lattimore2024). Letz Rd andassumethatz K (1/√2L)almostsurely. Then,
τ
∈ ∈
withprobabilityatleast1 δ,
−
τ
1
(s (z) sˆ(z)) 1+ M¯ L+LY .
t t τ max
(cid:12) − (cid:12)≤ λ
(cid:12)Xt=1 (cid:12) hp i
(cid:12) (cid:12)
Lemma34(Proposition11.19,L(cid:12)attimore2024). Wi(cid:12)thprobabilityatleast1 δ,
(cid:12) (cid:12) −
τ
1
max (s (z) sˆ(z)) 2+ dM¯ L+dLY .
t t τ max
z ∈Kτ(1/√2dL)(cid:12)
(cid:12)Xt=1
− (cid:12) (cid:12)≤ λ
hp i
(cid:12) (cid:12)
(cid:12) (cid:12)
Similarresultsholdforthequadratics(cid:12)urrogateapproximat(cid:12)ions:
Lemma35(Proposition11.20,Lattimore2024). Supposethatz K (1/√2L)almostsurely. Then,withprobability
τ
∈
atleast1 δ,
−
τ
1
(q (z) qˆ(z)) M¯ L+LY .
t t τ max
(cid:12) − (cid:12)≤ λ
(cid:12)Xt=1 (cid:12) hp i
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) 25OnlineNewtonMethodfor BanditConvexOptimisation
Andfortheuniformbound:
Lemma36(Proposition11.20,Lattimore2024). Withprobabilityatleast1 δ,
−
τ L2
max (q (z) qˆ(z)) dM¯ +dY .
t t τ max
z ∈Kτ(1/√2L)(cid:12)
(cid:12)Xt=1
− (cid:12) (cid:12)≤ λ
hp i
(cid:12) (cid:12)
(cid:12) (cid:12)
Finally,weneedcontroloftheHessianes(cid:12)timates: (cid:12)
Lemma37(Proposition11.21,Lattimore2024). Givenx Rd,let
∈
τ τ
S τ(x)= sˆ′t′(x) S¯ τ(x)= s′t′(x)
t=1 t=1
X X
let P be the (possibly random)set of positive definitematrices Σ such thatΣ−t 1
(cid:22)
Σ −1 forall t
≤
τ. Then, with
probabilityatleast1 δ,forallΣ 1 P andx K (1/√2dL),
− τ
− ∈ ∈
S¯ (x) λL2 dM¯ +d2Y Σ 1 S (x) S¯ (x)+λL2 dM¯ +d2Y Σ 1.
τ τ max − τ τ τ max −
− (cid:22) (cid:22)
Lemma38(Proposition11hp .22,Lattimore202i
4). LetP
bethesamesetasinLhp
emma37and
i
τ τ
S
τ
= sˆ′t′(µ t) S¯
τ
= s′t′(µ t).
t=1 t=1
X X
Then,withprobabilityatleast1 δ,forallΣ 1 P,
−
− ∈
S¯ λL2 dM¯ +d2Y Σ 1 S S¯ +λL2 dM¯ +d2Y Σ 1.
τ τ max − τ τ τ max −
− (cid:22) (cid:22)
hp i hp i
H Followthe regularised leader
Thefollowingisstandard:
Theorem39. Supposethat(fˆ)n isasequenceofquadraticfunctionsfromK toRand(K )n K isdecreasing
t t=1 t t=0 ⊂
and
x t =a xr ∈g Km t−i 1nk 2x σk 22 +η
u
Xt − =1 1fˆ u(x) and k·kt⋆ = k·kΦ′ t′−1 .
Φt−1(x)
Supposethat(Φ )n areconvex.Then,foranyx K ,
t t=1 ∈ n
n x 2 n
η fˆ t(x t) −fˆ t(x)
≤
k 2σk
2
+2η2 kfˆ t′(x t) k2 t⋆,
Xt=1(cid:16) (cid:17) Xt=1
Proof. LetR(x)= 1 x 2. BythedefinitionofΦ ,
2σ2 k k t
n n
1 Φ (x) R(x)
fˆ(x ) fˆ(x) = (Φ (x ) Φ (x )) n +
t t t t t t 1 t
− η − − − η η
Xt=1(cid:16) (cid:17) Xt=1
n
1 Φ (x ) Φ (x) R(x) R(x )
n n+1 n 1
= (Φ (x ) Φ (x ))+ + −
t t t t+1
η − η − η η
t=1
X
n
1 R(x) R(x )
1
(Φ (x ) Φ (x ))+ − Φ (x ) Φ (x)
t t t t+1 n n+1 n
≤ η − η ≤
t=1
X
n x 2
≤2η kfˆ t′(x t) k2 t⋆+ 2k ησk
2
.
t=1
X
26OnlineNewtonMethodfor BanditConvexOptimisation
wherethefinalinequalityneedstobejustified. ThedifferenceinobjectivescanbeboundedintwowaysusingthatΦ
t
isconvexandonlyconsistsofquadratics. SinceΦ isquadratic,itssecondderivativeisconstant,whichwedenoteby
t
Φ andlet = . First,
′t′ k·kt k·kΦ′ t′
1
Φ (x ) Φ (x )= Φ (x ),x x + x x 2
t t − t t+1 h ′t t+1 t − t+1 i 2k t − t+1 kt
1
x x 2 . Firstorderoptimality
≥ 2k t − t+1 kt
Ontheotherhand,
Φ (x ) Φ (x ) Φ (x ),x x Convexity
t t
−
t t+1
≤h
′t t t
−
t+1
i
= ηfˆ(x )+Φ (x ),x x
h t′ t ′t −1 t t − t+1 i
ηfˆ(x ),x x Firstorderoptimality
≤h
t′ t t
−
t+1
i
η fˆ(x ) x x . Cauchy-Schwarz
≤ k t′ t kt⋆ k t+1 − t kt
Combiningbothgives
(Φ (x ) Φ (x ))2 η2 fˆ(x ) 2 x x 2 2η2 fˆ(x ) 2 (Φ (x ) Φ (x ))
t t − t t+1 ≤ k t′ t kt⋆k t − t+1 kt ≤ k t′ t kt⋆ t t − t t+1
DividingbothsidesbyΦ (x ) Φ (x )givestheresult.
t t t t+1
−
I Optimisingthe surrogate estimate
Intherestartconditionoftheadversarialversionofthealgorithmweneedtooptimiseapossiblynon-convexfunction.
Thisproblemcanbecircumventedbyaddinga quadratictermthatensuresthattheobjectivebecomesconvexagain.
Here, we show that that is possible and that the overhead of this new objective poses no problem for the restart
condition.
Fors t τ wecanboundeachcovariancematrixasintheproofofLemma8
≤ ≤
Σ−s1 (cid:22)2Σ¯ −s1 (cid:22)4Σ¯ −t 1 (cid:22)8Σ−t 1.
Lemma37andt τ thengivesusforallx K that
t
≤ ∈
t
sˆ ′′(x)+λL2 dM¯
τ
+d2Y
max
8Σ−t 1 (cid:23)0
Xi=1 hp i
Wecansimplifythisabitbynoticing
d210L
λL2 dM¯ +d2Y λL2 10L√nd+ 20λL3√nd.
τ max
≤ ε ≤
hp i (cid:20) (cid:21)
Inparticular,thefunction
t t
sˆ i(x)+160λL3√nd kx −µ t k2 Σ− t1 =: sˆ i(x)+Q t(x)
i=1 i=1
X X
isconvexonK . Weimmediatelyget
t
t t t t
sˆ(µ ) min sˆ(x)+Q (x) sˆ(µ ) min sˆ(x), (20)
i i i t i i i
i=1
−x ∈Kt"
i=1
#≤
i=1
−x ∈Kti=1
X X X X
BecauseQ (x)ispositive.Wecanalsoboundtheoriginaloptimumofthenon-convexobjectiveasfollows.
t
t t t t
sˆ(µ ) sˆ(xsˆ )= sˆ(µ ) sˆ(xsˆ ) Q (xsˆ )+Q (xsˆ )
i i − i ⋆,t i i − i ⋆,t − t ⋆,t t ⋆,t
i=1 i=1 i=1 i=1
X X X X
t t
sˆ(µ ) sˆ(xsˆ )+Q (xsˆ ) +160λL3√ndF
≤ i i −" i ⋆,t t ⋆,t # max
i=1 i=1
X X
t t
sˆ(µ ) min sˆ(x)+Q (x) +160λL3√ndF
i i i t max
≤
i=1
−x ∈Kt"
i=1
#
X X
27OnlineNewtonMethodfor BanditConvexOptimisation
Wherehaveusedthat x µ 2 F forallx K . Withtheparametersettingswehave
k − t kt ≤ max ∈ t
F
ηλL3√ndF = max γF
max d2L2.5 ≤ max
Thenewrestartconditionbecomes
t
160F γF
max max
maxη (sˆ(µ ) sˆ(y) Q (y)) +
y ∈Kt
i=1
i i − i − t ≤−
(cid:18)
d2L2.5 32
(cid:19)
X
A restartshouldstillbe triggeredwhenxs K . Thiscanbeachievedbyadjustingthe originalrestartcondition
⋆,τ 6∈ τ
to −γ 1F 6max,retuningtheparametersandconstantssuchthat d λL +2ηC
τ
+8ηL
≤
γF 1m 6ax andLemma12insteadgives
τ ♭ (x) γFmax. ByrepeatingthederivationinStep4and(20)weget
i=1 i ≤− 8
P τ τ
maxη (sˆ(µ ) sˆ(x) Q (x)) maxη (sˆ(µ ) sˆ(x))
t t t τ t t t
x ∈Kt
t=1
− − ≤x ∈Kt
t=1
−
X X
γF 160F γF
max max max
+
≤− 16 ≤− d2L2.5 32
(cid:18) (cid:19)
Whenarestartistriggered,theregretisstillnegative
τ
dL
ηRegf(x ) η sˆ(µ ) sˆ(xsˆ ) + +2ηC +8ηL RepeatofStep5
τ ⋆,τ ≤ t t − t ⋆,τ λ τ
t=1
X(cid:0) (cid:1)
τ τ
dL
η sˆ(µ ) min sˆ(x)+Q (x) +η8λ√ndF + +2ηC
t t t τ max τ
≤
t=1
−x ∈Kt"
t=1
#! λ
X X
160F γF dL
max + max +η160λ√ndF + +2ηC +8ηL
≤− d2L2.5 32 max λ τ
(cid:18) (cid:19)
ApproximateRestartcondition
dL γF
max
+2ηC +8ηL
τ
≤ λ − 16
0.
≤
28