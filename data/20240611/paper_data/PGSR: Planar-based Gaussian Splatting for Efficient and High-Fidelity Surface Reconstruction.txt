1
PGSR: Planar-based Gaussian Splatting for Efficient
and High-Fidelity Surface Reconstruction
Danpeng Chen, Hai Li, Weicai Ye, Yifan Wang, Weijian Xie, Shangjin Zhai,
Nan Wang, Haomin Liu, Hujun Bao, Guofeng Zhang†
Fig.1: PGSRrepresentation.WepresentaPlanar-basedGaussianSplattingReconstructionrepresentationforefficientandhigh-fidelitysurfacereconstruction
frommulti-viewRGBimageswithoutanygeometricprior(depthornormalfrompre-trainedmodel).Thecourthousereconstructedbyourmethoddemonstrates
thatPGSRcanrecovergeometricdetails,suchastextualdetailsonthebuilding.Fromlefttoright:inputSfMpoints,planar-basedGaussianellipsoid,rendered
view,texturedmesh,surface,andnormal.
Abstract—Recently, 3D Gaussian Splatting (3DGS) has at- tion(PGSR)toachievehigh-fidelitysurfacereconstructionwhile
tracted widespread attention due to its high-quality rendering, ensuring high-quality rendering. Specifically, we first introduce
andultra-fasttrainingandrenderingspeed.However,duetothe an unbiased depth rendering method, which directly renders
unstructured and irregular nature of Gaussian point clouds, it the distance from the camera origin to the Gaussian plane
is difficult to guarantee geometric reconstruction accuracy and and the corresponding normal map based on the Gaussian
multi-viewconsistencysimplybyrelyingonimagereconstruction distribution of the point cloud, and divides the two to obtain
loss. Although many studies on surface reconstruction based the unbiased depth. We then introduce single-view geometric,
on 3DGS have emerged recently, the quality of their meshes is multi-viewphotometric,andgeometricregularizationtopreserve
generally unsatisfactory. To address this problem, we propose a global geometric accuracy. We also propose a camera exposure
fast planar-based Gaussian splatting reconstruction representa- compensation model to cope with scenes with large illumination
variations. Experiments on indoor and outdoor scenes show
that our method achieves fast training and rendering while
H. Bao, G. Zhang, W. Ye and H. Li are with the State Key
Lab of CAD&CG, Zhejiang University. E-mails: {baohujun, zhang- maintaininghigh-fidelityrenderingandgeometricreconstruction,
guofeng}@zju.edu.cn. outperforming3DGS-basedandNeRF-basedmethods.Ourcode
D. Chen and W. Xie are with the State Key Lab of CAD&CG, Zhejiang will be made publicly available, and more information can be
UniversityandSenseTimeResearch.D.ChenisalsoaffiliatedwithTetras.AI. found on our project page (https://zju3dv.github.io/pgsr/).
E-mails:chendanpeng@tetras.ai,xieweijian@sensetime.com.
Y.WangiswithShanghaiAILaboratory. Index Terms—Planar-Based Gaussian Splatting, Surface Re-
S.Zhai,N.WangandH.LiuarewithSenseTimeResearch.
† Correspondingauthor construction, Neural Rendering, Neural Radiance Fields.
4202
nuJ
01
]VC.sc[
1v12560.6042:viXra2
Fig.2:Unbiased depth rendering.(a)Illustrationoftherendereddepth:WetakeasingleGaussian,flattenitintoaplane,andfititontothesurfaceasan
example. Our rendered depth is the intersection point of rays and surfaces, matching the actual surface. In contrast, the depth from previous methods [11],
[24] corresponds to a curved surface and may deviate from the actual surface. (b) We use true depth to supervise two different depth rendering methods.
Afteroptimization,wemapthepositionsofallGaussianpoints.Gaussiansofourmethodfitwellontotheactualsurface,whilethepreviousmethodresults
innoiseandpooradherencetothesurface.
I. INTRODUCTION In this paper, we propose a novel unbiased depth render-
ing method based on 3DGS, facilitating the integration of
NOVEL view synthesis and geometry reconstruction are various geometric constraints to achieve precise geometric
challenging and crucial tasks in computer vision, widely estimation. Previous methods [24] render depth by blending
used in AR/VR [13], [65], [71], 3D content generation [10], the accumulations of each Gaussian at the z-position of the
[18], [48], [53], [63], and autonomous driving. To achieve camera, resulting in two main issues as shown in Fig. 2.
a realistic and immersive experience in AR/VR, novel view The depth corresponds to a curved surface and may deviate
synthesis needs to be sufficiently convincing, and 3D re- from the actual surface. To address these issues, we compress
construction [32], [36], [62], [64], [66] needs to be finely 3D Gaussians into flat planes and blend their accumulations
detailed. Recently, neural radiance fields [22], [41], [42], [61] to obtain normal and camera-to-plane distance maps. These
have been widely used to tackle this task, achieving high- maps are then transformed into depth maps. This method
fidelity novel view synthesis [2], [3], [44] and 3D geometry involves blending Gaussian plane accumulations to determine
reconstruction[33],[56].However,duetothecomputationally a pixel’s plane parameters. The intersection of the ray and
intensive volume rendering methods, neural radiance fields plane defines the depth, depending on the Gaussian’s position
often require training times of several hours to even hundreds and rotation. By dividing the distance map by the normal
of hours, and rendering speeds are difficult to achieve in real- map, we cancel out the ray accumulation weights, ensuring
time. Recently, 3D Gaussian Splatting (3DGS) [27] has made the depth estimation is unbiased and falls on the estimated
groundbreaking advancements in this field. By optimizing the plane. In our experiment shown in Fig. 2, we used true depth
positions,rotations,scales,andappearancesoftheexplicit3D to guide two depth rendering methods. After optimization, we
Gaussians and combining alpha-blend rendering, 3DGS has mapped the positions of all Gaussian points. Results show
achieved training times in the order of minutes and rendering that our method produces Gaussians that closely align with
speeds in the millisecond range. the actual surface, while the previous method generates noisy
Although 3DGS achieves high-fidelity novel view render- Gaussians that fail to adhere precisely to the surface.
ing and fast training and rendering speeds. As discussed in After rendering the plane parameters for each pixel, we
previous methods [19], [24], Gaussians often do not conform apply single-view and multi-view regularization to optimize
well to actual surfaces, resulting in poor geometric accuracy. these parameters. Empirically, adjacent pixels often belong to
Fig. 3 also shows this conclusion. Extracting accurate meshes thesameplane.Usingthislocalplaneassumption,wecompute
from millions of discrete Gaussian points is an extremely a normal map from neighboring pixel depth estimations and
challenging task. The fundamental reason for this lies in the ensure consistency between this normal map and the rendered
disorderly and irregular nature of Gaussians, which makes normal map. At geometric edges, the local plane assumption
them unable to accurately model the surfaces of real scenes. fails, so we detect these edges using image edges and reduce
Moreover, optimizing solely based on image reconstruction the weight in these areas, achieving smooth geometry and
loss can easily lead to local optima, ultimately resulting in consistentdepthandnormals.However,duetothediscreteand
Gaussians failing to conform to actual surfaces and exhibiting unordered nature of Gaussians, geometry may be inconsistent
poor geometric accuracy. In many practical tasks, geometric across multiple views. To address this, we apply multi-view
reconstruction accuracy is a crucial metric. Therefore, to regularization ensuring global geometric consistency. Similar
address these issues, we propose a novel framework based totheEikonalloss[56],weincorporateamulti-viewgeometric
on 3DGS that achieves high-fidelity geometric reconstruction consistency loss to ensures smooth and consistent geometric
whilemaintainingthehigh-qualityrenderingquality,fasttrain- reconstruction,eveninareaswithnoise,blur,orweaktextures.
ing, and rendering speeds characteristic of 3DGS. We use two photometric coefficients to compensate for3
overall changes in image brightness, further improving re- eitherthroughtriangulation[6]orimplicitsurfacefitting[25],
construction quality. Finally, we validate the rendering and [26].Despitebeingwell-establishedandextensivelyutilizedin
reconstructionqualityontheMipNeRF360,theDTU[23]and academia and industry, these traditional methods are suscep-
theTanksandTemples(TnT)[28]dataset.Experimentalresults tible to artifacts stemming from erroneous matching or noise
demonstrate that, while maintaining the original Gaussian introducedduringthepipeline.Inresponse,severalapproaches
rendering quality and rendering speed, our method achieves aim to enhance reconstruction completeness and accuracy by
state-of-the-artreconstructionaccuracy.Moreover,ourtraining integrating deep neural networks into the matching process
speedonlyrequiresonehouronasingleGPU,whilethestate- [50], [54].
of-the-art method based on NeRF [33] requires eight GPUs
over two days. In summary, our method makes the following B. Neural Surface Reconstruction
contributions:
Numerous pioneering efforts have leveraged pure deep
• Wepropose anovelunbiaseddepthrenderingmethod. neuralnetworkstopredictsurfacemodelsdirectlyfromsingle
Based on this rendering method, we can render the
or multiple image conditions using point clouds [14], [34],
reliable plane parameters for each pixel, facilitating the
voxels [12], [58], and triangular meshes [32], [55] or implicit
incorporation of various geometric constraints.
fields[40],[47]inend-to-endmanner.However,thesemethods
• We introduce single-view and multi-view regulariza- oftenincursignificantcomputationaloverheadduringnetwork
tions to optimize the plane parameters of each pixel,
inferenceanddemandextensivelylabeledtraining3Dmodels,
achieving high-precision global geometric consistency.
hindering their real-time and real-world applicability.
• The exposure compensation simply and effectively en- Withtherapidadvancementinneuralsurfacereconstruction
hances reconstruction accuracy.
tasks, a meticulously designed scene recovery method named
• Our method, while maintaining the high rendering ac- NeRF [41] emerged. NeRF-based methods take 5D ray in-
curacy and speed of the original GS, achieves state-of-
formation as input and predict density and color sampled in
the-art reconstruction accuracy, and our training time
continuous space, yielding notably more realistic rendering
is near 100 times faster compared to state-of-the-art
results. However, this representation falls short in capturing
reconstruction methods based on NeRF [33].
high-fidelity surfaces.
Consequently, several approaches have transformed NeRF-
basednetworkarchitecturesintosurfacereconstructionframe-
works by incorporating intermediate representations such as
occupancy[46]orsigneddistancefields[56],[60].Despitethe
potent surface reconstruction capabilities exhibited by NeRF-
based frameworks, the stacked multi-layer-perceptron (MLP)
layersimposeconstraintsoninferencetimeandrepresentation
ability.Toaddressthischallenge,variousfollowingstudiesaim
Fig. 3: Rendered Depth. The original depth in 3DGS exhibits significant
to reduce dependency on MLP layers by decomposing scene
noise,whileourdepthissmootherandmoreaccurate.
information into separable structures, such as points [59] and
voxels [31], [33], [35].
II. RELATEDWORK
C. Gaussian Splatting based Surface Reconstruction
Surface reconstruction is a cornerstone field in computer
SuGaR [19] proposed a method to extract Mesh from
graphics and computer vision, aimed at generating intricate
3DGS. They introduced regularization terms to encourage
and accurate surface representations from sparse or noisy
Gaussian fitting to the scene surface. By sampling 3D point
inputdata.Obtaininghigh-fidelity3Dmodelsfromreal-world
clouds from the Gaussian using the density field, they utilized
environments is pivotal for enabling immersive experiences in
Poisson reconstruction to extract a mesh from these sampled
augmented reality (AR) and virtual reality (VR). This paper
point clouds. While encouraging Gaussian fitting to the sur-
focuses exclusively on surface reconstruction under given
faceenhancesgeometricreconstructionaccuracy,irregular3D
poses, which can be readily computed using SLAM [5], [7],
Gaussian shapes make modeling smooth geometric surfaces
[8] or SFM [43], [51], [57] methods.
challenging. Moreover, due to the discreteness and disorder
of the Gaussian, relying solely on image reconstruction loss
A. Traditional Surface Reconstruction
can lead to overfitting, resulting in incomplete geometric
Traditional methods adhere to the universal multi-view information and surface mismatch. 2DGS [21] achieves view-
stereo pipeline, which can be roughly categorized based on consistent geometry by collapsing the 3D volume into a set
the intermediate representation they rely on, such as point of 2D oriented planar Gaussian disks. GOF [69] establishes
cloud [16], [30], volume [29], depth map [4], [17], [52], a Gaussian opacity field, enabling geometry extraction by
etc. The commonly used method separates the overall MVS directly identifying its level-set. However, these 3DGS-based
problem into several parts, by initially extracting dense point methods still produce biased depth and multi-view geometric
clouds from multi-view images through block-based match- consistency is not guaranteed. To address these issues, we
ing [1], followed by the construction of surface structures flattened the Gaussian into a planar shape, which is more4
Fig.4:PGSROverview.WecompressGaussiansintoflatplanesandrenderdistanceandnormalmaps,whicharethentransformedintounbiaseddepthmaps.
Single-view and multi-view geometric regularization ensure high precision in global geometry. Exposure compensation RGB loss enhances reconstruction
accuracy.
suitable for modeling actual surfaces and facilitates rendering (SH) from the Gaussian G . T is the cumulative opacity. N
i i
parameters such as normals and distances from the plane to is the number of Gaussians that the ray passes through.
the origin. Based on these plane parameters, we proposed The center µ of a Gaussian G . can be projected into the
i i
unbiased depth estimation, allowing us to extract geometric camera coordinate system as:
parametersfromtheGaussian.Then,weintroducedgeometric
(cid:2) x ,y ,z ,1(cid:3)⊤ =W[µ ,1]⊤,
regularization terms from single-view and multi-view to opti- i i i i
mizethesegeometricparameters,achievinggloballyconsistent Previous Methods [11], [24] render depth under the current
high-precision geometric reconstruction. viewpoint:
(cid:88)
D = T α z .
III. PRELIMINARYOF3DGAUSSIANSPLATTING i i i
i∈N
3DGS [27] explicitly represents 3D scenes with a set of
3D Gaussians {G }. Each Gaussian is defined by a Gaussian
i
IV. METHOD
function:
Given multi-view RGB images of static scenes, our goal
G i(x|µ i,Σ i)=e−1 2(x−µi)⊤Σ− i1(x−µi),
is to achieve efficient and high-fidelity scene geometry re-
where µ ∈ R3 and Σ ∈ R3×3 are the center of a point construction and rendering quality. Compared to 3DGS, we
i i
p ∈P andcorresponding3Dcovariancematrix,respectively. achieve global consistency in geometry reconstruction while
i
The covariance matrix Σ is factorized into a scaling matrix maintaining similar rendering quality. Initially, we improve
i
S ∈R3×3 and a rotation matrix R ∈R3×3: the modeling of scene geometry attributes by compressing
i i
3D Gaussians into a 2D flat plane representation, which
Σ =R S S⊤R⊤.
i i i i i is used to generate plane distance and normal maps, and
3DGS allows fast α-blending for rendering. Given a trans- subsequently converted into unbiased depth maps. We then
formation matrix W and an intrinsic matrix K, µ and Σ introduce single-view geometric, multi-view photometric, and
i i
canbetransformedtocameracoordinatecorrespondingtoW geometric consistency loss to ensure global geometry consis-
and then projected to 2D coordinate: tency. Additionally, the exposure compensation model further
µ′ =KW[µ ,1]⊤, Σ′ =JWΣ W⊤J⊤, improves reconstruction accuracy.
i i i i
where J is the Jacobian of the affine approximation for the
A. Planar-based Gaussian Splatting Representation
projective transformation. Rendering color C ∈R3 of a pixel
In this section, we will discuss how to transform 3D
u can be obtained in a manner of α-blending:
Gaussians into a 2D flat plane representation. Based on this
i−1
(cid:88) (cid:89) plane representation, we introduce an unbiased depth render-
C = T α c , T = (1−α ),
i i i i i ing method, which will render plane-to-camera distance and
i∈N j=1
normal maps, and can then be converted into depth maps.
where α is calculated by evaluating G (u|µ′,Σ′) multiplied With geometric depth, distance, and normal maps available,
i i i i
with a learnable opacity corresponding to G , and the view- it becomes easier to introduce single-view regularization and
i
dependentcolorc ∈R3isrepresentedbysphericalharmonics multi-view regularization in the following sections.
i5
Fig. 5: The rendering and mesh reconstruction results in various indoor and outdoor scenes that we have achieved. PGSR achieves high-precision
geometricreconstructionfromaseriesofRGBimageswithoutrequiringanypriorknowledge.
to the ambiguity of the normal direction when there are two
directions for the shortest axis, we resolve this issue by using
the viewing direction to determine the normal direction. This
implies that the angle between the viewing direction and the
normal direction should be greater than 90 degrees. The final
normal map under the current viewpoint is achieved through
α-blending:
i−1
(cid:88) (cid:89)
N = RTn α (1−α ), (2)
c i i j
i∈N j=1
Fig.6:UnbiasedDepth.
where R is the rotation from the camera to the global world.
c
The distance from the plane to the camera center can be
expressed as d = (RT(µ − T ))RTnT, where T is the
Duetothedifficultyinmodelingreal-worldscenegeometry i c i c c i c
camera center in the world. µ is the center of gaussian G .
attributes such as depth and normals using 3D Gaussian i i
Thefinaldistancemapunderthecurrentviewpointisachieved
shapes, it’s necessary to flatten the 3D Gaussians into 2D
through α-blending:
flat Gaussians in order to accurately represent the geometry
surface of the actual scene. Achieving precise geometry re- i−1
(cid:88) (cid:89)
construction and high-quality rendering requires the 2D flat D = d α (1−α ), (3)
i i j
Gaussians to accurately conform to the scene surface. Since i∈N j=1
the 2D flat Gaussians approximate a local plane, we can
Referencing Fig. 6, after obtaining the distance and normal
conveniently render the depth and normals of the scene.
of the plane through rendering, we can determine the corre-
(cid:80)(cid:80)(cid:80)
Flattening 3D Gaussian: The covariance matrix =
i sponding depth map by intersecting rays with the plane:
R S STRT ofa3DGaussianexpressestheellipsoidalshape.
i i i i
Here, R represents the orthonormal basis of the ellipsoid’s D
i D(p)= . (4)
three axes, and the scale factor S defines the size along each N(p)K−1p˜
i
direction.Bycompressingthescalefactoralongspecificaxes,
where p = [u,v]T is the 2D position on the image plane.
the Gaussian ellipsoid can be flattened into planes aligned
p˜ denotes the homogeneous coordinate of p, and K is the
withthoseaxes.WecompresstheGaussianellipsoidalongthe
intrinsic of camera.
directionoftheminimumscalefactor,effectivelyflatteningthe
As shown in Fig. 2, our method of rendering depth has
ellipsoid into a plane closest to its original shape. According
twomajoradvantagescomparedtootherdepthrenderingtech-
to the method [9], we directly minimize the minimum scale
niques. First, Our depth shapes are consistent with flattened
factor S =diag(s ,s ,s ) for each Gaussian:
i 1 2 3 Gaussianshapes,whichcantrulyreflectactualsurfaces.Previ-
L =∥min(s ,s ,s )∥ . (1) ousmethodstypicallyinvolvedirectlyrenderingthedepthmap
s 1 2 3 1
based on α-blending of the depth Z of Gaussians. Their depth
UnbiasedDepthRendering:Thedirectionoftheminimum is curved, inconsistent with the flat Gaussian shape, causing
scalefactorcorrespondstothenormaln oftheGaussian.Due geometric conflicts. In contrast, we render the normal and
i6
distancemapsoftheplanefirstandthenconvertthemintothe Finally, we add the single-view normal loss is:
depth map. Our depth lies on the Gaussian fast plane. When
1 (cid:88) (cid:12) (cid:12)5
the3DGaussianflatplanesfittheactualsurface,therendered L svgeo = W (cid:12)∇I(cid:12) ∥N d(p)−N(p)∥ 1, (6)
depthcanensurecompleteconsistencywiththeactualsurface. p∈W
Second, since the accumulation weight for each ray may
Where ∇I is the image gradient normalized to the range of
be less than 1, previous rendering methods are affected by
0 to 1, N(p) is from Equation 2, and W is the set of image
the weight accumulation, potentially resulting in depths that
pixels.
are closer to the camera side and overall underestimated. In
2) Multi-View Regularization: Single-view geometry regu-
contrast, our depth is obtained by dividing the distance from
larizationcanmaintainconsistencybetweendepthandnormal
the rendering origin to the plane by the normal, effectively
geometry, providing fairly accurate initial geometric informa-
eliminating the influence of weight accumulation coefficients.
tion. However, due to the irregular discretization of Gaussian
pointcloudoptimization,wefoundthatthegeometrystructure
across multiple views is not entirely consistent. Therefore, it
is necessary to introduce multi-view geometry regularization
to ensure global consistency of the geometry structure.
Multi-View Geometric Consistency:Theimagelossoften
suffers from influences such as image noise, blur, and weak
textures. In these cases, the geometric solution for photomet-
ric consistency is unreliable. Due to the discrete nature of
Gaussian properties, we cannot establish a spatially dense or
semi-dense SDF field as in SDF methods based on NeRF. We
are unable to use spatial smoothness constraints, such as the
Fig. 7: Qualitative comparison on DTU dataset. PGSR produces smooth Eikonalloss[56],toavoidtheinfluenceofunreliablesolutions.
anddetailedsurfaces. To mitigate the impact of unreliable geometric solutions and
ensure multi-view geometric consistency, we introduce this
consistency prior constraint, which helps converge to the
correct solution position, enhancing geometric smoothness.
B. Geometric Regularization
We render the normals N and the plane distances D to
1) Single-View Regularization: The original 3DGS relying the camera for both the reference frame and the neighboring
solely on image reconstruction loss can easily fall into local frame. As shown in Fig. 9, for a specific pixel p in the
r
overfitting optimization, leading to Gaussian shapes incon- reference frame, the corresponding normal is n and the
r
sistent with the actual surface. Based on this, we introduce distance is d . The pixel p in the reference frame can be
r r
geometric constraints to ensure that the 3D Gaussian fits the mapped to a pixel p in the neighboring frame through the
n
actual surface as closely as possible. homography matrix H :
rn
Local Plane Assumption: Encouraged by these meth-
p˜ =H p˜, (7)
ods[24],[37],[49],weadopttheassumptionoflocalplanarity n rn r
to constrain the local consistency of depth and normals,
H =K (R −
T rnnT
r )K−1, (8)
meaning a pixel and its neighboring pixels can be considered rn n rn d r
r
as an approximate plane. After rendering the depth map,
where p˜ is the homogeneous coordinate of p, R and T
rn rn
we sample four neighboring points using a fixed template.
aretherelativetransformationfromthereferenceframetothe
With these known depths, we compute the plane’s normal.
neighboringframe.Similarly,forthepixelp intheneighbor-
n
This process is repeated for the entire image, generating
ingframe,wecanobtainthenormaln andthedistanced to
n n
normals from the rendered depth map. We then minimize the
compute the homography matrix H . The pixel p undergo
nr r
difference between this normal map and the rendered normal
forwardandbackwardprojectionsbetweenthereferenceframe
map, ensuring geometric consistency between local depth and
andtheneighboringframethroughH andH .Minimizing
rn nr
normals.
the forward and backward projection error constitutes the
Image Edge-Aware Single-View Loss:Neighboringpixels multi-view geometric consistency regularization:
may not necessarily fully adhere to the local planarity as-
1 (cid:88)
sumption, especially in edge regions. To address this issue, L = ϕ(p ) (9)
mvgeom V r
We use image edges to approximate geometric edges. For a
pr∈V
pixel point p, we sample four points from the neighboring
where ϕ(p ) =∥ p − H H p ∥ is the forward and
pixels, such as up, down, left, and right. We project the four r r nr rn r
backwardprojectionerrorofp .Whenϕ(p )exceedsacertain
sampled depth points into 3D points {P |j = 1,...,4} in the r r
j threshold, it can be considered that the pixel is occluded or
camera coordinate system, then calculate the normal of the
that there is a significant geometric error. To prevent errors
local plane for the pixel point p is:
caused by occlusion, these pixels will not be included in the
(P −P )×(P −P ) multi-view regularization term. If these pixels are mistakenly
N (p)= 1 0 3 2 , (5)
d |(P −P )×(P −P )| identified as occluded due to geometric errors, it does not
1 0 3 27
Fig.8:Qualitative comparison on Tanks and Temples dataset.Wevisualizesurfacequalityusinganormalmapgeneratedfromthereconstructedmesh.
PGSRoutperformsotherbaselineapproachesincapturingscenedetails,whereasbaselinemethodsexhibitmissingornoisysurfaces.
affect our final convergence. This is because the single-view p to the neighboring frame patch P using the homography
r n
regularization term and the use of sparse 3D Gaussians to matrix H . Focusing on geometric details, we convert color
rn
representdensesceneswillgraduallypropagatehigh-precision images into grayscale. Multi-view photometric regularization
geometry, eventually leading all Gaussians to converge to the requires that P and P should be as consistent as possible.
r n
correct positions. V is the set of all pixels in the image Weusethenormalizedcrosscorrelation(NCC)[68]ofpatches
excluding those with high forward and backward projection in the reference frame and the neighboring frame to measure
error. the photometric consistency:
Multi-View Photometric Consistency: Drawing inspira- 1 (cid:88)
L = (1−NCC(I (p ),I (H p ))), (10)
tionfrommulti-viewStereo(MVSmethods)[4],[15],[51],we mvrgb V r r n rn r
employ photometric multi-view consistency constraints based
pr∈V
on plane patches. We map a 11x11 pixel patch P centered at where V is the set of all pixels in the image, excluding
r8
TABLEI:Quantitativeresults ofrenderingqualityfor novelviewsynthesis onMip-NeRF360dataset.”Red”,”Orange”and”Yellow”denotethebest,
second-best,andthird-bestresults.PGSRachievesresultscloseto3DGSandoutperformssimilarreconstructionmethodSuGaR.
Indoorscenes Outdoorscenes Averageonallscenes
PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓
NeRF[41] 26.84 0.790 0.370 21.46 0.458 0.515 24.15 0.624 0.443
DeepBlending[20] 26.40 0.844 0.261 21.54 0.524 0.364 23.97 0.684 0.313
INGP[44] 29.15 0.880 0.216 22.90 0.566 0.371 26.03 0.723 0.294
M-NeRF360[2] 31.72 0.917 0.180 24.47 0.691 0.283 28.10 0.804 0.232
Neus[56] 25.10 0.789 0.319 21.93 0.629 0.600 23.74 0.720 0.439
3DGS[27] 30.99 0.926 0.199 24.24 0.705 0.283 27.24 0.803 0.246
SuGaR[19] 29.44 0.911 0.216 22.76 0.631 0.349 26.10 0.771 0.283
2DGS[21] 30.39 0.923 0.183 24.33 0.709 0.284 27.03 0.804 0.239
GOF[69] 30.80 0.928 0.167 24.76 0.742 0.225 27.78 0.835 0.196
PGSR 30.41 0.930 0.161 24.45 0.730 0.224 27.43 0.830 0.193
groundtruthimage,whiletheSSIMlossrequirestherendered
imagetohave similarstructurestotheground truthimage.To
enhance the robustness of exposure coefficient estimation, we
need to ensure that the rendered image and the ground truth
image have sufficient structural similarity before performing
the estimation. After training, Ir is required to be globally
i
consistent and maintain structural similarity with the ground
truth image, while Ia can adjust the brightness of images to
i
match the ground truth image perfectly.
D. Training
In summary, our final training loss L consists of the image
Fig.9:Multi-viewphotometricandgeometricloss. reconstruction loss L , the flattening 3D Gaussian loss L ,
rgb s
the geometric loss L :
geo
those with high forward and backward projection errors. L=L rgb+λ 1L s+L geo. (15)
3) Geometric Regularization Loss: Finally, the geomet-
We set λ = 100. For the image reconstruction loss, we set
1
ric regularization loss includes single-view geometric, multi-
λ=0.2. For the geometric loss, we set λ =0.01, λ =0.2,
2 3
view geometric, and multi-view photometric consistency con-
and λ =0.05.
4
straints:
V. EXPERIMENTS
L geo =λ 2L svgeo+λ 3L mvrgb+λ 4L mvgeom. (11) Datasets: To validate the effectiveness of our method, we
conducted experiments on various real-world datasets, includ-
ing objects, and indoor and outdoor environments. We chose
C. Exposure Compensation Image Loss
thewidelyusedMiP-NeRF360dataset[2]forevaluatingnovel
Duetochangesinexternallightingconditions,camerasmay view synthesis performance. The large and complex scenes
have different exposure times during different shooting mo- of the TnT [28] and 15 object-centric scenes of the DTU
ments, leading to overall brightness variations in images. The dataset [23] were selected to assess reconstruction quality.
original 3DGS does not consider brightness changes, which Evaluation Criterion: We chose three widely used image
can result in floating artifacts in practical scenes. To model evaluation metrics to validate novel view synthesis: peak
the overall brightness variations at different times, we assign signal-to-noise ratio (PSNR), structural similarity index mea-
two exposure coefficients, a and b, to each image. Ultimately, sure(SSIM),andthelearnedperceptualimagepatchsimilarity
imageswithexposurecompensationcanbeobtainedbysimply (LPIPS) [70]. For assessing surface quality, we employed the
computing with exposure coefficients: F1 score and chamfer distance.
Implementation Details: Our training strategy and hy-
Ia =exp(a )Ir+b , (12)
i i i i perparameters are generally consistent with 3DGS [27]. The
where Ir is the rendered image and Ia is the exposure- training iterations for all scenes are set to 30,000. We adopt
i i
adjusted image. We employ the following image loss: the densification strategy of AbsGS [67]. The learning rate
for the exposure coefficient is 0.001. We begin by rendering
L =(1−λ)L (I˜−I )+λL (Ir−I ). (13)
rgb 1 i SSIM i i the depth for each training view, followed by utilizing the
(cid:40)
Ia, if L (Ir−I )<0.5 TSDF Fusion algorithm [45] to generate the corresponding
I˜= i SSIM i i (14) TSDF field. Subsequently, we extract the mesh [38] from the
Ir, if L (Ir−I )>=0.5
i SSIM i i TSDFfield.Weonlyutilizetheexposurecompensationonthe
where I is the ground truth image. The L1 loss constraint Tanks and Temples dataset. All experiments in this paper are
i
ensuresthattheexposure-adjustedimageisconsistentwiththe conducted on Nvidia RTX 4090 GPU.
desab-FReN
desab-SG9
TABLEII:Quantitativeresultsofchamferdistance(mm)↓onDTUdataset[23].PGSRachievesthehighestreconstructionaccuracyandisover100times
fasterthantheSDFmethodbasedonNeRF.
24 37 40 55 63 65 69 83 97 105 106 110 114 118 122 Mean Time
VolSDF[60] 1.14 1.26 0.81 0.49 1.25 0.70 0.72 1.29 1.18 0.70 0.66 1.08 0.42 0.61 0.55 0.86 >12h
NeuS[56] 1.00 1.37 0.93 0.43 1.10 0.65 0.57 1.48 1.09 0.83 0.52 1.20 0.35 0.49 0.54 0.84 >12h
Neuralangelo[33] 0.37 0.72 0.35 0.35 0.87 0.54 0.53 1.29 0.97 0.73 0.47 0.74 0.32 0.41 0.43 0.61 >128h
SuGaR[19] 1.47 1.33 1.13 0.61 2.25 1.71 1.15 1.63 1.62 1.07 0.79 2.45 0.98 0.88 0.79 1.33 1h
2DGS[21] 0.48 0.91 0.39 0.39 1.01 0.83 0.81 1.36 1.27 0.76 0.70 1.40 0.40 0.76 0.52 0.80 0.32h
GOF[69] 0.50 0.82 0.37 0.37 1.12 0.74 0.73 1.18 1.29 0.68 0.77 0.90 0.42 0.66 0.49 0.74 2h
PGSR(DS) 0.34 0.58 0.29 0.29 0.78 0.58 0.54 1.01 0.73 0.51 0.49 0.69 0.31 0.37 0.38 0.53 0.6h
PGSR 0.31 0.52 0.27 0.27 0.76 0.54 0.49 0.98 0.69 0.49 0.46 0.56 0.28 0.35 0.36 0.49 1.0h
TABLEIII:QuantitativeresultsofF1Score↑forreconstructiononTanks
and Temples dataset. PGSR achieves similar reconstruction accuracy to
Neuralgangelo,butourtrainingspeedisoverahundredtimesfaster.
NeuS Geo-Neus Neurlangelo SuGaR 2DGS GOF PGSR
Barn 0.29 0.33 0.70 0.14 0.36 0.51 0.66
Caterpillar 0.29 0.26 0.36 0.16 0.23 0.41 0.41
Courthouse 0.17 0.12 0.28 0.08 0.13 0.28 0.21
Ignatius 0.83 0.72 0.89 0.33 0.44 0.68 0.80
Meetingroom 0.24 0.20 0.32 0.15 0.16 0.28 0.29
Truck 0.45 0.45 0.48 0.26 0.26 0.58 0.60
Mean 0.38 0.35 0.50 0.19 0.30 0.46 0.50
Time >24h >24h >128h 2h 34.2 m 2h 1.2h Fig.10:Thequalitativecomparisonof ourunbiaseddepthmethodwith
thepreviousdepthmethod[11],[24]isdepictedinthenormalmap.Our
overallgeometricstructureappearssmootherandmoreprecise.
A. Real-time Rendering
TABLEIV:AblationstudyontheMeetingroomofTnTdataset.
For the validation of rendering quality, we follow the
3DGS method and conduct validation on the Mip-NeRF360 Modelsetting F1-Score↑ PSNR↑
dataset [2]. We compare with current state-of-the-art meth- w/oSingle-view 0.26 27.46
ods for pure novel view synthesis as well as similar re- w/oMulti-view 0.15 28.14
w/oOurunbiaseddepth 0.20 26.80
construction methods to ours, including NeRF [41], Deep
Blending [20], INGP [44], Mip-NeRF360 [2], NeuS [56], Fullmodel 0.29 27.30
3DGS[27],SuGaR[19],2DGS[21],andGOF[69].Asshown
in Table I and Fig. 5, compared to the current state-of-the- precise, especially in flat regions. Table IV also demonstrates
art methods, our approach not only provides excellent surface thatourdepthrenderingmethodachieveshigherreconstruction
reconstructionqualitybutalsoachievesoutstandingnovelview and rendering accuracy.
synthesis results. Single-View and Multi-view Regularization: The single-
view regularization term can provide a good initial geometric
B. Reconstruction accuracy without relying on multi-view information. When
single-view regularization is removed, the reconstruction ac-
We compared our method, PGSR, with current state-of-the-
curacy decreases. Multi-view regularization effectively con-
artneuralsurfacereconstructionmethodsincludingNeuS[56],
strains the consistency of geometry between multiple views,
Geo-NeuS [15], and NeuralAngelo [33]. We also compared it
improving overall reconstruction accuracy. From Table IV,
withrecentlyemergedreconstructionmethodsbasedon3DGS,
it is evident that multi-view regularization is crucial for
such as SuGaR [19], 2DGS [21], and GOF [69]. All results
reconstruction accuracy.
aresummarizedinFig.5,Fig.7,Fig.8,TableIIandTableIII.
Exposure Compensation: We validated the exposure com-
The DTU dataset: Our method achieves the highest
pensation on the Ignatius series of the TnT dataset. As shown
reconstruction accuracy with relatively fast training speed.
in Table V, exposure compensation enhances reconstruction
PGSR(DS) denotes downsampling to half the original image
and rendering quality.
size for training. Our method significantly outperforms other
3DGS-based reconstruction methods. As shown in Fig. 7, our
D. Virtual Reality Application
surfaces are smoother and contain more details.
The TnT dataset: The F1 score of PGSR is similar to As shown in Fig. 11, we used our method to separately
NeuralAngeloandbettercomparedtoothercurrentreconstruc- reconstruct the original materials. We then extracted the exca-
tion methods. Our training time is over 100 times faster than vator and Ignatius using masks and placed them in the garden
NeuralAngelo. Moreover, compared to NeuralAngelo, we can scene.Byrenderingthesceneandobjectsseparatelyandusing
reconstruct more surface details.
TABLEV:AblationstudyonexposureCompensation.
C. Ablations
Modelsetting F1-Score↑ PSNR↑
OurUnbiasedDepth:FromFig10,itcanbeobservedthat
w/oexposuremodeling 0.76 21.71
our overall geometric structure appears smoother and more wexposuremodeling 0.80 25.7710
[2] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman,
RicardoMartin-Brualla,andPratulPSrinivasan.Mip-nerf:Amultiscale
representation for anti-aliasing neural radiance fields. In Proceedings
of the IEEE/CVF International Conference on Computer Vision, pages
5855–5864,2021.
[3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan,
and Peter Hedman. Zip-nerf: Anti-aliased grid-based neural radiance
fields. In Proceedings of the IEEE/CVF International Conference on
ComputerVision,pages19697–19705,2023.
[4] Neill DF Campbell, George Vogiatzis, Carlos Herna´ndez, and Roberto
Cipolla. Using multiple hypotheses to improve depth-maps for multi-
view stereo. In Computer Vision–ECCV 2008: 10th European Con-
ference on Computer Vision, Marseille, France, October 12-18, 2008,
Proceedings,PartI10,pages766–779.Springer,2008.
[5] Carlos Campos, Richard Elvira, Juan J Go´mez Rodr´ıguez, Jose´ MM
Montiel,andJuanDTardo´s.Orb-slam3:Anaccurateopen-sourcelibrary
for visual, visual–inertial, and multimap slam. IEEE Transactions on
Robotics,37(6):1874–1890,2021.
[6] Fre´de´ric Cazals and Joachim Giesen. Delaunay triangulation based
Fig. 11: Virtual Reality Application. (a) Original materials, including
surfacereconstruction. InEffectivecomputationalgeometryforcurves
gardenscene,excavator,andIgnatius.(b)AVirtualRealityeffectshowcase
andsurfaces,pages231–276.Springer,2006.
synthesizedfromtheseoriginalmaterials.
[7] Danpeng Chen, Nan Wang, Runsen Xu, Weijian Xie, Hujun Bao, and
Guofeng Zhang. Rnin-vio: Robust neural inertial navigation aided
our rendered depth to determine occlusion relationships, we visual-inertial odometry in challenging scenes. In 2021 IEEE Inter-
nationalSymposiumonMixedandAugmentedReality(ISMAR),pages
achieved immersive, high-fidelity virtual reality effects with
275–283.IEEE,2021.
high-precision depth estimation. [8] Danpeng Chen, Shuai Wang, Weijian Xie, Shangjin Zhai, Nan Wang,
HujunBao,andGuofengZhang. Vip-slam:Anefficienttightly-coupled
rgb-dvisualinertialplanarslam. In2022InternationalConferenceon
VI. LIMITATIONSANDFUTUREWORK RoboticsandAutomation(ICRA),pages5615–5621.IEEE,2022.
[9] HanlinChen,ChenLi,andGimHeeLee.Neusg:Neuralimplicitsurface
AlthoughourPGSRefficientlyandfaithfullyperformsgeo- reconstruction with 3d gaussian splatting guidance. arXiv preprint
arXiv:2312.00846,2023.
metric reconstruction, it also faces several challenges. Firstly,
[10] Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang
we cannot perform geometric reconstruction in regions with Tang, Zhongang Cai, Lei Yang, Gang Yu, Guosheng Lin, and Chi
missingorlimitedviewpoints,leadingtoincompleteorlessac- Zhang.Artist-CreatedMeshGenerationwithAutoregressiveTransform-
ers. arXiv,2024.
curategeometry.Exploringmethodstoimprovereconstruction
[11] KaiCheng,XiaoxiaoLong,KaizhiYang,YaoYao,WeiYin,YuexinMa,
quality under insufficient constraints using priors is another Wenping Wang, and Xuejin Chen. Gaussianpro: 3d gaussian splatting
avenue for further investigation. Secondly, our method does withprogressivepropagation. arXivpreprintarXiv:2402.14650,2024.
[12] Christopher B. Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and
notconsiderscenariosinvolvingreflectivesurfacesormirrors,
Silvio Savarese. 3D-R2N2: A unified approach for single and multi-
so reconstruction in these environments will pose challenges. view 3D object reconstruction. In European Conference on Computer
Integrating with existing 3DGS work that accounts for reflec- Vision,volume9912,pages628–644,2016.
[13] Nianchen Deng, Zhenyi He, Jiannan Ye, Budmonde Duinkharjav, Pra-
tive surfaces would enhance reconstruction accuracy in such
neethChakravarthula,XuboYang,andQiSun.Fov-nerf:Foveatedneu-
scenarios.Finally,wefoundthattherearesomefloatingpoints ralradiancefieldsforvirtualreality.IEEETransactionsonVisualization
in the scene, which affect the rendering and reconstruction andComputerGraphics,28(11):3854–3864,2022.
[14] HaoqiangFan,HaoSu,andLeonidasJ.Guibas. Apointsetgeneration
quality.Integratingmoreadvanced3DGSbaselines[39]would
network for 3D object reconstruction from a single image. In IEEE
help further enhance overall quality. ConferenceonComputerVisionandPatternRecognition,pages2463–
2471,2017.
[15] QianchengFu,QingshanXu,YewSoonOng,andWenbingTao. Geo-
VII. CONCLUSION neus: Geometry-consistent neural implicit surfaces learning for multi-
view reconstruction. Advances in Neural Information Processing Sys-
In this paper, we propose a novel unbiased depth rendering tems,35:3403–3416,2022.
[16] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and robust
method based on 3DGS. With this method, we render the
multiview stereopsis. IEEE Transactions on Pattern Analysis and
plane geometry parameters for each pixel, including normal, MachineIntelligence,32(8):1362–1376,2010.
distance,anddepthmaps.Wethenincorporatesingle-viewand [17] Silvano Galliani, Katrin Lasinger, and Konrad Schindler. Massively
parallelmultiviewstereopsisbysurfacenormaldiffusion.InProceedings
multi-view geometric regularization, and exposure compensa-
of the IEEE international conference on computer vision, pages 873–
tion model to achieve precise global consistency in geometry. 881,2015.
We validate our rendering and reconstruction quality on the [18] Peng Gao, Le Zhuo, Dongyang Liu, Ruoyi Du, Xu Luo, Longtian
Qiu, Yuhang Zhang, Chen Lin, Rongjie Huang, Shijie Geng, Renrui
MipNeRF360, DTU, and TnT datasets. The experimental
Zhang,JunlinXi,WenqiShao,ZhengkaiJiang,TianshuoYang,Weicai
resultsindicatethatourmethodachievesthehighestgeometric Ye, He Tong, Jingwen He, Yu Qiao, and Hongsheng Li. Lumina-t2x:
reconstructionaccuracyandrenderingqualitycomparedtothe Transformingtextintoanymodality,resolution,anddurationviaflow-
based large diffusion transformers. arXiv preprint arxiv:2405.05945,
current state-of-the-art methods.
2024.
[19] Antoine Gue´don and Vincent Lepetit. Sugar: Surface-aligned gaussian
splatting for efficient 3d mesh reconstruction and high-quality mesh
REFERENCES rendering. arXivpreprintarXiv:2311.12775,2023.
[20] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George
[1] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan B Gold- Drettakis,andGabrielBrostow.Deepblendingforfree-viewpointimage-
man.Patchmatch:Arandomizedcorrespondencealgorithmforstructural based rendering. ACM Transactions on Graphics (ToG), 37(6):1–15,
imageediting. ACMTrans.Graph.,28(3):24,2009. 2018.11
[21] BinbinHuang,ZehaoYu,AnpeiChen,AndreasGeiger,andShenghua the Asian Computer Vision Conference (ACCV 2012), pages 257–270.
Gao. 2d gaussian splatting for geometrically accurate radiance fields. SpringerBerlinHeidelberg,2012.
arXivpreprintarXiv:2403.17888,2024. [44] Thomas Mu¨ller, Alex Evans, Christoph Schied, and Alexander Keller.
[22] Chenxi Huang, Yuenan Hou, Weicai Ye, Di Huang, Xiaoshui Huang, Instantneuralgraphicsprimitiveswithamultiresolutionhashencoding.
Binbin Lin, Deng Cai, and Wanli Ouyang. Nerf-det++: Incorporating ACMtransactionsongraphics(TOG),41(4):1–15,2022.
semanticcuesandperspective-awaredepthsupervisionforindoormulti- [45] Richard A Newcombe, Shahram Izadi, Otmar Hilliges, David
view3ddetection. arXivpreprintarXiv:2402.14464,2024. Molyneaux, David Kim, Andrew J Davison, Pushmeet Kohi, Jamie
[23] RasmusJensen,AndersDahl,GeorgeVogiatzis,EnginTola,andHenrik Shotton,SteveHodges,andAndrewFitzgibbon.Kinectfusion:Real-time
Aanæs. Largescalemulti-viewstereopsisevaluation. InProceedingsof dense surface mapping and tracking. In 2011 10th IEEE international
theIEEEconferenceoncomputervisionandpatternrecognition,pages symposiumonmixedandaugmentedreality,pages127–136.Ieee,2011.
406–413,2014. [46] MichaelNiemeyer,LarsM.Mescheder,MichaelOechsle,andAndreas
[24] Yingwenqi Jiang, Jiadong Tu, Yuan Liu, Xifeng Gao, Xiaoxiao Long, Geiger. Differentiablevolumetricrendering:Learningimplicit3Drep-
Wenping Wang, and Yuexin Ma. Gaussianshader: 3d gaussian splat- resentationswithout3Dsupervision. InIEEEConferenceonComputer
ting with shading functions for reflective surfaces. arXiv preprint VisionandPatternRecognition,pages3501–3512,2020.
arXiv:2311.17977,2023. [47] JeongJoonPark,PeterFlorence,JulianStraub,RichardA.Newcombe,
[25] MichaelKazhdan,MatthewBolitho,andHuguesHoppe.Poissonsurface andStevenLovegrove. DeepSDF:Learningcontinuoussigneddistance
reconstruction. In Proceedings of the fourth Eurographics symposium functions for shape representation. In IEEE Conference on Computer
onGeometryprocessing,volume7,2006. VisionandPatternRecognition,pages165–174,2019.
[26] MichaelKazhdanandHuguesHoppe. Screenedpoissonsurfacerecon- [48] BenPoole,AjayJain,JonathanTBarron,andBenMildenhall. Dream-
struction. ACMTransactionsonGraphics(ToG),32(3):1–13,2013. fusion:Text-to-3dusing2ddiffusion. arXivpreprintarXiv:2209.14988,
[27] Bernhard Kerbl, Georgios Kopanas, Thomas Leimku¨hler, and George 2022.
Drettakis. 3d gaussian splatting for real-time radiance field rendering. [49] XiaojuanQi,RenjieLiao,ZhengzheLiu,RaquelUrtasun,andJiayaJia.
ACMTransactionsonGraphics,42(4):1–14,2023. Geonet: Geometric neural network for joint depth and surface normal
[28] ArnoKnapitsch,JaesikPark,Qian-YiZhou,andVladlenKoltun. Tanks estimation. InProceedingsoftheIEEEConferenceonComputerVision
and temples: Benchmarking large-scale scene reconstruction. ACM andPatternRecognition,pages283–291,2018.
TransactionsonGraphics(ToG),36(4):1–13,2017.
[50] Paul-EdouardSarlin,CesarCadena,RolandSiegwart,andMarcinDym-
[29] KiriakosNKutulakosandStevenMSeitz. Atheoryofshapebyspace
czyk.Fromcoarsetofine:Robusthierarchicallocalizationatlargescale.
carving. Internationaljournalofcomputervision,38:199–218,2000. InCVPR,2019.
[30] Maxime Lhuillier and Long Quan. A quasi-dense approach to surface
[51] JohannesLSchonbergerandJan-MichaelFrahm.Structure-from-motion
reconstructionfromuncalibratedimages. IEEEtransactionsonpattern
revisited. In Proceedings of the IEEE conference on computer vision
analysisandmachineintelligence,27(3):418–433,2005.
andpatternrecognition,pages4104–4113,2016.
[31] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and
[52] Johannes Lutz Scho¨nberger, Enliang Zheng, Marc Pollefeys, and Jan-
GuofengZhang. Vox-surf:Voxel-basedimplicitsurfacerepresentation.
Michael Frahm. Pixelwise view selection for unstructured multi-view
IEEETransactionsonVisualizationandComputerGraphics,2022.
stereo. InEuropeanConferenceonComputerVision(ECCV),2016.
[32] Hai Li, Weicai Ye, Guofeng Zhang, Sanyuan Zhang, and Hujun Bao.
[53] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng.
Saliency guided subdivision for single-view mesh reconstruction. In
Dreamgaussian: Generative gaussian splatting for efficient 3d content
2020InternationalConferenceon3DVision(3DV),pages1098–1107.
creation. arXivpreprintarXiv:2309.16653,2023.
IEEE,2020.
[54] Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Pablo Speciale,
[33] Zhaoshuo Li, Thomas Mu¨ller, Alex Evans, Russell H Taylor, Mathias
and Marc Pollefeys. Patchmatchnet: Learned multi-view patchmatch
Unberath, Ming-Yu Liu, and Chen-Hsuan Lin. Neuralangelo: High-
stereo. InProceedingsoftheIEEE/CVFconferenceoncomputervision
fidelityneuralsurfacereconstruction. InProceedingsoftheIEEE/CVF
andpatternrecognition,pages14194–14203,2021.
ConferenceonComputerVisionandPatternRecognition,pages8456–
[55] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and
8465,2023.
Yu-Gang Jiang. Pixel2mesh: Generating 3D mesh models from single
[34] Chen-HsuanLin,ChenKong,andSimonLucey.Learningefficientpoint
RGB images. In European Conference on Computer Vision, volume
cloudgenerationfordense3Dobjectreconstruction. InConferenceon
11215,pages55–71,2018.
ArtificialIntelligence,pages7114–7121,2018.
[56] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Ko-
[35] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian
Theobalt.Neuralsparsevoxelfields.InAdvancesinNeuralInformation mura, and Wenping Wang. Neus: Learning neural implicit surfaces
ProcessingSystems,pages15651–15663,2020. by volume rendering for multi-view reconstruction. arXiv preprint
arXiv:2106.10689,2021.
[36] Xiangyu Liu, Weicai Ye, Chaoran Tian, Zhaopeng Cui, Hujun Bao,
and Guofeng Zhang. Coxgraph: multi-robot collaborative, globally [57] Changchang Wu. Towards linear-time incremental structure from mo-
consistent, online dense reconstruction system. In 2021 IEEE/RSJ tion. In2013InternationalConferenceon3DVision-3DV2013,pages
International Conference on Intelligent Robots and Systems (IROS), 127–134.IEEE,2013.
pages8722–8728.IEEE,2021. [58] Haozhe Xie, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou, and
[37] XiaoxiaoLong,YuhangZheng,YupengZheng,BeiwenTian,ChengLin, Shengping Zhang. Pix2Vox: Context-aware 3D reconstruction from
Lingjie Liu, Hao Zhao, Guyue Zhou, and Wenping Wang. Adaptive single and multi-view images. In IEEE/CVF International Conference
surface normal constraint for geometric estimation from monocular onComputerVision,pages2690–2698,2019.
images. arXivpreprintarXiv:2402.05869,2024. [59] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan
[38] William E Lorensen and Harvey E Cline. Marching cubes: A high Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radi-
resolution 3d surface construction algorithm. In Seminal graphics: ance fields. In Proceedings of the IEEE/CVF conference on computer
pioneeringeffortsthatshapedthefield,pages347–353.1998. visionandpatternrecognition,pages5438–5448,2022.
[39] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua [60] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume
Lin,andBoDai.Scaffold-gs:Structured3dgaussiansforview-adaptive renderingofneuralimplicitsurfaces.InAdvancesinNeuralInformation
rendering. arXivpreprintarXiv:2312.00109,2023. ProcessingSystems,pages4805–4815,2021.
[40] Lars M. Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian [61] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys,
Nowozin, and Andreas Geiger. Occupancy networks: Learning 3D Zhaopeng Cui, and Guofeng Zhang. IntrinsicNeRF: Learning Intrinsic
reconstruction in function space. In IEEE Conference on Computer NeuralRadianceFieldsforEditableNovelViewSynthesis. InProceed-
VisionandPatternRecognition,pages4460–4470,2019. ings of the IEEE/CVF International Conference on Computer Vision,
[41] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T 2023.
Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes [62] Weicai Ye, Xinyu Chen, Ruohao Zhan, Di Huang, Xiaoshui Huang,
as neural radiance fields for view synthesis. Communications of the HaoyiZhu,HujunBao,WanliOuyang,TongHe,andGuofengZhang.
ACM,65(1):99–106,2021. Dynamic-Aware Tracking Any Point for Structure from Motion in the
[42] Yuhang Ming, Weicai Ye, and Andrew Calway. idf-slam: End-to-end Wild. arXivpreprint,2024.
rgb-dslamwithneuralimplicitmappinganddeepfeaturetracking.arXiv [63] Weicai Ye, Chenhao Ji, Zheng Chen, Junyao Gao, Xiaoshui Huang,
preprintarXiv:2209.07919,2022. Song-HaiZhang,WanliOuyang,TongHe,CairongZhao,andGuofeng
[43] PierreMoulon,PascalMonasse,andRenaudMarlet. Adaptivestructure Zhang. DiffPano:ScalableandConsistentTexttoPanoramaGeneration
from motion with a contrario model estimation. In Proceedings of withSphericalEpipolar-AwareDiffusion. arXivpreprint,2024.12
[64] WeicaiYe,XinyueLan,ShuoChen,YuhangMing,XingyuanYu,Hujun
Bao,ZhaopengCui,andGuofengZhang.Pvo:Panopticvisualodometry.
In Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition(CVPR),pages9579–9589,June2023.
[65] Weicai Ye, Hai Li, Tianxiang Zhang, Xiaowei Zhou, Hujun Bao, and
Guofeng Zhang. SuperPlane: 3D plane detection and description from
a single image. In 2021 IEEE Virtual Reality and 3D User Interfaces
(VR),pages207–215.IEEE,2021.
[66] Weicai Ye, Xingyuan Yu, Xinyue Lan, Yuhang Ming, Jinyu Li, Hujun
Bao,ZhaopengCui,andGuofengZhang. Deflowslam:Self-supervised
scene motion decomposition for dynamic dense slam. arXiv preprint
arXiv:2207.08794,2022.
[67] ZongxinYe,WenyuLi,SidunLiu,PengQiao,andYongDou. Absgs:
Recovering fine details for 3d gaussian splatting. arXiv preprint
arXiv:2404.10484,2024.
[68] Jae-Chern Yoo and Tae Hee Han. Fast normalized cross-correlation.
Circuits,systemsandsignalprocessing,28:819–843,2009.
[69] ZehaoYu,TorstenSattler,andAndreasGeiger. Gaussianopacityfields:
Efficientandcompactsurfacereconstructioninunboundedscenes.arXiv
preprintarXiv:2404.10772,2024.
[70] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliver
Wang. Theunreasonableeffectivenessofdeepfeaturesasaperceptual
metric. InProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,pages586–595,2018.
[71] Tianxiang Zhang, Chong Bao, Hongjia Zhai, Jiazhen Xia, Weicai Ye,
and Guofeng Zhang. Arcargo: Multi-device integrated cargo load-
ing management system with augmented reality. In 2021 IEEE Intl
Conf on Dependable, Autonomic and Secure Computing, Intl Conf on
Pervasive Intelligence and Computing, Intl Conf on Cloud and Big
DataComputing,IntlConfonCyberScienceandTechnologyCongress
(DASC/PiCom/CBDCom/CyberSciTech),pages341–348.IEEE,2021.13
scan24 scan37 scan40
scan55 scan63 scan65
Fig.12:QualitativecomparisonsinsurfacereconstructionbetweenPGSR,2DGS,andGOFontheDTUdataset.
RSGP
SGD2
FOG
RSGP
SGD2
FOG14
scan69 scan83 scan97
scan105 scan106 scan110
Fig.13:QualitativecomparisonsinsurfacereconstructionbetweenPGSR,2DGS,andGOFontheDTUdataset.
RSGP
SGD2
FOG
RSGP
SGD2
FOG15
scan114 scan118 scan122
Fig.14:QualitativecomparisonsinsurfacereconstructionbetweenPGSR,2DGS,andGOFontheDTUdataset.
RSGP
SGD2
FOG16
Fig.15:QualitativecomparisonsinsurfacereconstructionbetweenPGSR,2DGS,andGOF.
tupnI
RSGP
SGD2
FOG17
(a) Rendered RGB (b) Mesh (c) Mesh Normal
Fig. 16: PGSR achieves high-precision geometric reconstruction in various indoor and outdoor scenes from a series of RGB images without requiring any
priorknowledge.