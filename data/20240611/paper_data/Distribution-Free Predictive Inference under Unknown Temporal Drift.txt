Distribution-Free Predictive Inference under Unknown Temporal Drift
Elise Han∗ Chengpiao Huang† Kaizheng Wang‡
This version: June 2024
Abstract
Distribution-free prediction sets play a pivotal role in uncertainty quantification for complex
statistical models. Their validity hinges on reliable calibration data, which may not be readily
available as real-world environments often undergo unknown changes over time. In this paper,
we propose a strategy for choosing an adaptive window and use the data therein to construct
prediction sets. The window is selected by optimizing an estimated bias-variance trade-off. We
provide sharp coverage guarantees for our method, showing its adaptivity to the underlying
temporal drift. We also illustrate its efficacy through numerical experiments on synthetic and
real data.
Keywords: Prediction set, distribution-free, conformal inference, temporal distribution shift,
adaptivity.
1 Introduction
Practitioners increasingly deploy sophisticated prediction models such as deep neural networks into
real-world applications. Due to their complex structures, these models are generally accessed as
black boxes. To assess their reliability and safeguard against potential errors, it is important to
quantify the uncertainty in their outputs. Predictive inference is a popular methodology for this
purpose. It takes as input a prediction algorithm and calibration data, and outputs a prediction
set that contains the true outcome with a prescribed probability. The validity of the prediction set
hinges on the assumption that the calibration data truthfully represents the underlying environment.
However, this assumption is frequently violated in practice, where the data distribution may drift
over time. Integrating data from both current and historical periods to construct faithful prediction
sets remains a significant challenge. Despite a large body of literature on learning under distribution
drift over the past two decades (Hazan and Seshadhri, 2009; Mohri and Muñoz Medina, 2012; Besbes
et al., 2015; Hanneke et al., 2015; Mazzetto and Upfal, 2023; Huang and Wang, 2023), statistical
inference within this context is much less explored.
Main contributions. In this paper, we study predictive inference under temporal distribution
drift via quantile estimation. We develop an adaptive rolling window strategy to estimate the
population quantile of a drifting distribution. This strategy in turn leads to an algorithm for building
distribution-free prediction sets that wraps around any black-box point prediction algorithm. We
further provide theoretical guarantees and numerical experiments to show that our algorithm adapts
to unknown temporal drifts.
Author names are sorted alphabetically.
∗Department of Computer Science, Columbia University. Email: lh3117@columbia.edu.
†Department of IEOR, Columbia University. Email: chengpiao.huang@columbia.edu.
‡Department of IEOR and Data Science Institute, Columbia University. Email: kaizheng.wang@columbia.edu.
1
4202
nuJ
01
]EM.tats[
1v61560.6042:viXraRelated works. One of the most powerful tools for distribution-free predictive inference is
conformal prediction, also called conformal inference (Papadopoulos et al., 2002; Vovk et al., 2005;
Shafer and Vovk, 2008; Lei et al., 2018; Angelopoulos and Bates, 2023). Under the assumption of
data exchangeability, it takes a black-box point prediction model and uses the empirical quantile
of conformity scores to construct distribution-free prediction sets. Our method shares the same
distribution-free and model-free features. When the data distribution undergoes temporal drift,
the exchangeability assumption no longer holds. A number of works have considered extending
the methodology to handle distribution shifts. Most of them assume that the calibration data are
independent samples from a fixed source distribution (Tibshirani et al., 2019; Lei and Candès, 2021;
Podkopaev and Ramdas, 2021; Fannjiang et al., 2022; Prinster et al., 2022; Park et al., 2022; Qiu
et al., 2023; Yang et al., 2024a; Si et al., 2024; Pournaderi and Xiang, 2024) or from a fixed number of
source distributions (Bhattacharyya and Barber, 2024; Ying et al., 2024), and consider only covariate
shift or label shift. Their methods do not apply when the distribution keeps drifting over time.
Several recent works have studied conformal prediction under temporal drift. Barber et al. (2023)
proposes a general data weighting scheme but does not provide a principled approach for choosing
the weights. Lin et al. (2022); Xu and Xie (2023); Lee and Matni (2024) consider time series data
with temporal dependence, but typically impose certain stationarity assumptions on the data. Gibbs
and Candès (2021); Gibbs and Candès (2024); Zaffran et al. (2022); Angelopoulos et al. (2023, 2024);
Yang et al. (2024b) perform online updates on some hyperparameters such as the miscoverage level
and the quantile tracker, and provide long-term coverage guarantees. In contrast, our work focuses
on achieving good coverage at a specific point in time, by utilizing offline calibration data collected
from the past. It can be used as a sub-routine for online predictive inference, and its coverage
guarantees at individual time points translate to long-term coverage guarantees.
A widely used approach for adapting to temporal drift is rolling window (Bifet and Gavaldà,
2007; Hanneke et al., 2015; Mohri and Muñoz Medina, 2012; Mazzetto and Upfal, 2023; Huang
and Wang, 2023). Our method adaptively selects a look-back window by optimizing an estimated
bias-variance trade-off. It is inspired by the Goldenshluger-Lepski method (Goldenshluger and Lepski,
2008) for bandwidth selection in non-parametric estimation. The latter has been adapted for mean
estimationundertemporaldriftinHanetal.(2024). Comparedwiththatwork, weconsiderthemore
challenging problem of quantile estimation where the estimation error is measured by a nonconvex
loss. Additionally, our approach is distribution-free, and does not make any boundedness assumption.
The methodology of our work shares similarities with but is different from conformal prediction.
In particular, our work is based on estimating the population quantile, while conformal prediction
utilizes properties of the empirical quantile under exchangeability. Our approach allows us to derive
a stronger coverage guarantee called probably approximately correct (PAC) or training-conditional
coverage (Vovk, 2012; Kivaranovic et al., 2020; Bates et al., 2021; Park et al., 2021; Bian and Barber,
2023; Yang and Kuchibhotla, 2024; Lee and Matni, 2024; Pournaderi and Xiang, 2024). It requires
that conditioned on the training and calibration data, the constructed prediction set has good
coverage with high probability. We note that Park et al. (2022); Qiu et al. (2023); Yang et al. (2024a);
Si et al. (2024) develop methods for such guarantees under covariate shift or label shift, but they
assume that the calibration data comes from the same source distribution, and thus do not apply to
drifting data.
Notation. Let Z = {1,2,...} be the set of positive integers. For n ∈ Z , let [n] = {1,2,...,n}.
+ +
For x ∈ R, define x = max{x,0}. For non-negative sequences {a }∞ and {b }∞ , we write
+ n n=1 n n=1
a = O(b ) if there exists C > 0 such that for all n ∈ Z , a ≤ Cb . Unless otherwise stated,
n n + n n
a ≲ b also represents a = O(b ). We write a ≍ b if both a ≲ b and b ≲ a hold. For x ∈ R,
n n n n n n n n n n
2we use δ to denote the Dirac measure at x. The total variation distance between two probability
x
distributions P and Q is denoted by TV(P,Q). We use U(a,b) to denote the uniform distribution
over the interval [a,b]. For an event A, we write 1(A) as its binary indicator.
Outline. The rest of the paper is organized as follows. Section 2 formally describes the problem
setup. Section 3 introduces our rolling window strategy for quantile estimation and predictive infer-
ence. Section 4 presents the theoretical guarantees. Section 5 demonstrates the strong performance
of our algorithms on synthetic and real data. Section 6 concludes the paper and discusses future
directions.
2 Problem setup
In this section, we formally state the problem of predictive inference under temporal drift and then
reduce it to quantile estimation.
2.1 Predictive inference
Let X and Y be covariate and response spaces, respectively. At each time t ∈ Z , we are given a
+
predictive model µ : X → Y designed for the current probability distribution P over X ×Y. As
(cid:98)t t
the environment evolves over time, {P }t can be different. For a new test point (x ,y ) drawn
j j=1 t t
from P , the model µ outputs a point prediction µ (x ) of the true label y . However, the model
t (cid:98)t (cid:98)t t t
could make errors and the new data is random. To quantify the uncertainty of the prediction, we
collect a batch of B ≥ 1 i.i.d. calibration data B = {(x ,y )}Bt from P . Based on the model µ
t t t,i t,i i=1 t (cid:98)t
and all the historical calibration data {B }t , we wish to construct a prediction set that covers y
j j=1 t
with a prescribed probability. This motivates the following problem:
Problem 2.1 (Predictive inference). Given a constant α ∈ (0,1), a model µ and calibration data
(cid:98)t
{B j}t j=1, construct a set-valued mapping C(cid:98)t : X → 2Y such that for a new data point (x t,y t) ∼ P t,
(cid:16) (cid:17)
P y
t
∈ C(cid:98)t(x t) ≈ 1−α. (2.1)
The probability in (2.1) is with respect to the randomness of (x ,y ), {B }t , and µ , which is
t t j j=1 (cid:98)t
common in the conformal prediction literature (Angelopoulos and Bates, 2023). In practice, however,
the latter two are often given to us and cannot be regenerated. Thus, ideally we would like the
coverage probability of C(cid:98)t(·) to be approximately 1−α for typical realizations of {B j}t
j=1
and µ (cid:98)t.
That is, we want
(cid:16) (cid:12) (cid:17)
P y
t
∈ C(cid:98)t(x t) (cid:12)
(cid:12)
{B j}t j=1, µˆ
t
≈ 1−α (2.2)
to hold with high probability. Such guarantee, known as training-conditional coverage or probably
approximately correct (PAC) coverage (Vovk, 2012), is stronger than (2.1). We aim to establish (2.2)
under the following standard assumption.
Assumption 2.1 (Independence). The calibration datasets {B }t are independent, and the model
j j=1
µ is trained independently of them.
(cid:98)t
For simplicity, we will treat µ as deterministic. This can be interpreted as conditioning on the
(cid:98)t
training data and the training algorithm.
32.2 Reduction to quantile estimation under temporal drift
We now present a reduction of Problem 2.1 to quantile estimation using ideas from split conformal
prediction(AngelopoulosandBates,2023). Supposewehaveaconformity score functions : X×Y →
R so that s(x ,y ) gauges the deviation of y from our point prediction µ (x ). Denote by F and q
t t t (cid:98)t t t t
the cumulative distribution function (CDF) and a (1−α)-quantile of s(x ,y ), respectively. Then,
t t
s(x ,y ) ≤ q holds with probability at least 1−α. If q is an estimate of q computed from the
t t t (cid:98)t t
sample scores {s(x ,y )} , then
j,i j,i j∈[t],i∈[Bj]
C(cid:98)t(x t) = {y ∈ Y : s(x t,y) ≤ q (cid:98)t} (2.3)
is a prediction set with coverage probability
P(cid:0) y t ∈ C(cid:98)t(x t) (cid:12) (cid:12) {B j}t j=1(cid:1) = P(cid:0) s(x t,y t) ≤ q (cid:98)t (cid:12) (cid:12) {B j}t j=1(cid:1) = F t(q (cid:98)t), (2.4)
conditioned on the calibration data. In light of the requirement (2.2), we want |F (q )−(1−α)| to
t (cid:98)t
be small with high probability over the randomness of q . In contrast, the goal (2.1) in conformal
(cid:98)t
prediction translates to E[F (q )] ≈ 1−α.
t (cid:98)t
ForregressionproblemswithY = R, anaturalchoiceofs(x,y)istheabsoluteresidual|y−µ (x)|,
(cid:98)t
whose associated prediction set (2.3) has the form
[µ (x )−q , µ (x )+q ]. (2.5)
(cid:98)t t (cid:98)t (cid:98)t t (cid:98)t
Suppose that in addition to µ , we also have an estimate σ : X → [0,+∞) of y ’s uncertainty given
(cid:98)t (cid:98)t t
x (e.g., conditional standard deviation or inter-quartile range). Then, we may use a studentized
t
score s(x,y) = |y−µ (x)|/σ (x) and get a prediction interval
(cid:98)t (cid:98)t
[µ (x )−q σ (x ), µ (x )+q σ (x )].
(cid:98)t t (cid:98)t(cid:98)t t (cid:98)t t (cid:98)t(cid:98)t t
Compared to the constant-width interval (2.5), the new one can better capture heteroscedasticity.
The above discussion shows that our general goal can be reduced to the following problem.
Problem 2.2 (Quantileestimation). Let {Q }t be probability distributions over R, and let {D }t
j j=1 j j=1
be independent datasets, where D = {u }Bj consists of B i.i.d. samples from Q . Given data
j j,i i=1 j j
{D }t and a constant α ∈ (0,1), how to estimate the (1−α)-quantile of Q ?
j j=1 t
Given a conformity score function s, we only need to solve Problem 2.2 with Q = Law(s(x ,y ))
j t t
and u = s(x ,y ). This amounts to quantile estimation under unknown temporal distribution
j,i j,i j,i
shift. Thus, our methodology is fundamentally different from conformal prediction: the former
targets the population quantile, while the latter utilizes the empirical quantile and crucially relies on
the exchangeability of the samples.
We make the following minimal regularity assumption, which is satisfied when Y = Rd, y has a
t
continuous conditional distribution given x , and y (cid:55)→ s(x,y) is continuous for every x ∈ X. No
t
other condition, e.g., moment bound, is needed.
Assumption 2.2 (Continuity). The distributions {Q }t have continuous CDFs.
j j=1
3 Methodology
In this section, we first present a bias-variance decomposition for the sample quantile under temporal
drift. Based on that, we develop a solution to Problem 2.2 and its extension to Problem 2.1.
43.1 A bias-variance decomposition for the sample quantile
To set the stage, we introduce some notations.
Definition 3.1 (Left and right quantiles). Let F : R → [0,1] be a CDF. For γ ∈ (0,1), the left
γ-quantile and right γ-quantile of F are defined by, respectively,
Q−(F) = inf{x ∈ R : F(x) ≥ γ} and Q+(F) = inf{x ∈ R : F(x) > γ}.
γ γ
If P is the probability distribution associated with F, we also write Q−(P) and Q+(P) in place of
γ γ
Q−(F) and Q+(F), respectively.
γ γ
The left quantile is the usual notion of quantile, while the right quantile plays an important role
in our analysis. The relation Q− ≤ Q+ always holds. Problem 2.2 amounts to estimating the left
γ γ
(1−α)-quantile of Q , namely, Q− (F ). A natural candidate is the left (1−α)-quantile of the
t 1−α t
data {D }t in some appropriately chosen look-back window k. This is a plug-in approach:
j j=t−k+1
the data defines an empirical CDF
1
(cid:88)t (cid:88)Bj (cid:88)t
F(cid:98)t,k(x) = 1{x ≥ u j,i} where B
t,k
= B j,
B
t,k
j=t−k+1i=1 j=t−k+1
and the estimator is
q
(cid:98)t,k
= Q− 1−α(F(cid:98)t,k). (3.1)
For any fixed k, the following result characterizes the approximation quality of q through a
(cid:98)t,k
bias-variance decomposition. Its proof is deferred to Appendix A.1.
Theorem 3.1 (Bias-variance decomposition). Let Assumption 2.2 hold. Fix k ∈ [t]. Choose
δ ∈ (0,1). Define
ϕ(t,k) = max ∥F −F ∥ ,
j t ∞
t−k+1≤j≤t
(cid:115)
5 2α(1−α)log(2/δ) 4log(2/δ)
ψ(t,k,δ) = + . (3.2)
4 B B
t,k t,k
With probability at least 1−δ,
|F (q )−(1−α)| ≤ ϕ(t,k)+ψ(t,k,δ). (3.3)
t (cid:98)t,k
Here ϕ(t,k) is the maximum Kolmogorov-Smirnov distance between Q and Q ,...,Q ,
t t−1 t−k+1
measuring the bias incurred by the distribution shift over the last k periods. The term ψ(t,k,δ) is a
Bernstein-type concentration bound (Boucheron et al., 2013), representing the stochastic error in
q . As k increases, the bias grows and the stochastic error shrinks. Ideally, we would like to select
(cid:98)t,k
k that minimizes the bias-variance decomposition in (3.3). However, this cannot be directly achieved
since the bias term ϕ(t,k) involves the unknown distribution shift. Instead, we will construct an
estimate of ϕ(t,k) for window selection.
53.2 Adaptive rolling window for quantile estimation and predictive inference
For two windows i ≤ k, if the environment is relatively stationary between time t−i+1 and time t
but there is significant cumulative shift between time t−k+1 and time t−i, then F(cid:98)t,i(q (cid:98)t,k) would
be far from F(cid:98)t,i(q (cid:98)t,i) ≈ 1−α. Thus, we can estimate the bias of window k using the empirical CDF’s
{F(cid:98)t,i(q (cid:98)t,k)}k
i=1
over shorter windows. To start with, we show that |F(cid:98)t,i(q (cid:98)t,k)−(1−α)| also admits a
bias-variance decomposition similar to that of |F (q )−(1−α)| in Theorem 3.1. The proof is in
t (cid:98)t,k
Appendix A.2.
Theorem 3.2 (Empirical bias-variance decomposition). Let Assumption 2.2 hold. Suppose 1 ≤ i ≤
k ≤ t, and choose δ ∈ (0,1). With probability at least 1−δ,
12 6 4
|F(cid:98)t,i(q (cid:98)t,k)−(1−α)| ≤ ϕ(t,k)+ ψ(t,k,δ/2)+ ψ(t,i,δ/2). (3.4)
5 5 5
We see that (3.4) differs from (3.3) only by multiplicative constants and an additional variance
term associated with the smaller window i. In light of (3.4) and inspired by the Goldenshluger-Lepski
method (Goldenshluger and Lepski, 2008) for adaptive nonparametric estimation, we define a bias
proxy
(cid:18) (cid:20) (cid:21)(cid:19)
ϕ(cid:98)(t,k,δ) = 5 max (cid:12) (cid:12)F(cid:98)t,i(q (cid:98)t,k)−(1−α)(cid:12) (cid:12)− 6 ψ(t,k,δ/2)+ 4 ψ(t,i,δ/2) . (3.5)
12 i∈[k] 5 5
+
Essentially, ϕ(cid:98)(t,k,δ) teases out the bias term ϕ(t,k) in (3.4) by subtracting the stochastic errors
6 5ψ(t,k,δ/2)+4 5ψ(t,i,δ/2) associated with q
(cid:98)t,k
and F(cid:98)t,i. We take maximum over all smaller windows
i ∈ [k] so that we do not miss any distribution shifts over the last k periods. As a direct consequence
of (3.4), 0 ≤ ϕ(cid:98)(t,k,δ) ≤ ϕ(t,k) holds with high probability. It is then natural to choose a window
that minimizes the approximate bias-variance decomposition ϕ(cid:98)(t,k,δ)+ψ(t,k,δ). This leads to
Algorithm 1.
Algorithm 1 Adaptive rolling window for quantile estimation
Input: Datasets {D }t with D = {u }Bj , miscoverage level α and hyperparameter δ′.
j j=1 j j,i i=1
for k = 1,··· ,t do
Compute q (cid:98)t,k, ψ(t,k,δ′), ψ(t,k,δ′/2) and ϕ(cid:98)(t,k,δ′) according to (3.1), (3.2) and (3.5).
end for
Choose any (cid:98)k ∈ argmin {ϕ(cid:98)(t,k,δ′)+ψ(t,k,δ′)}.
k∈[t]
return q .
(cid:98)t,(cid:98)k
As we have described in Section 2.2, Algorithm 1 for quantile estimation immediately yields a
procedure for constructing predictive sets. See Algorithm 2 for a detailed description.
Algorithm 2 Adaptive rolling window for predictive inference
Input: Calibration datasets {B }t with B = {(x ,y )}Bj , conformity score s : X ×Y → R,
j j=1 j j,i j,i i=1
miscoverage level α and hyperparameter δ′.
Let u = s(x ,y ) and set D = {u }Bj .
j,i j,i j,i j j,i i=1
Run Algorithm 1 with input {D }t , α and δ′ to obtain q .
j j=1 (cid:98)t,(cid:98)k
return C(cid:98)t(·) = {y ∈ Y : s(·,y) ≤ q (cid:98)t,(cid:98)k}.
64 Theoretical analysis
We now present theoretical guarantees for Algorithms 1 and 2, showing their adaptivity to the
unknown distribution drift over time.
Theorem 4.1 (Oracle inequality for Algorithm 1). Choose δ ∈ (0,1) and take δ′ = δ/(4t2) in
Algorithm 1. With probability at least 1−δ, Algorithm 1 outputs q satisfying
(cid:98)t,(cid:98)k
(cid:40) (cid:115) (cid:41)
(cid:12) (cid:12)F t(q (cid:98)t,(cid:98)k)−(1−α)(cid:12) (cid:12) ≲ m k∈i [n t]{ϕ(t,k)+ψ(t,k)} ≲ m k∈i [n
t]
ϕ(t,k)+ α( B1− t,kα) + B1
t,k
. (4.1)
Here ≲ only hides a logarithmic factor of t and δ−1.
According to Theorem 4.1, Algorithm 1 chooses a window (cid:98)k that is near-optimal for the bias-
variance trade-off in (3.3), without any knowledge of the underlying distribution drift. We illustrate
this oracle property using the following examples.
Example 4.1 (Change point). Suppose that the environment is stationary in the last K periods
for some K ∈ [t−1] but has been very different before, i.e. Q ̸= Q = ··· = Q . If K were
t−K t−K+1 t
known, then a natural estimate of the (1−α)-quantile of Q would be q . Theorem 4.1 shows that
t (cid:98)t,K
the estimation quality of q is comparable to q which uses B i.i.d. samples: up to logarithmic
(cid:98)t,(cid:98)k (cid:98)t,K t,K
factors,
(cid:115)
(cid:12) (cid:12)F t(q (cid:98)t,(cid:98)k)−(1−α)(cid:12) (cid:12) ≲ ϕ(t,K)+ψ(t,K) ≲ α( B1−α) + B1 .
t,K t,K
In this case, Algorithm 1 adapts to the local stationarity.
Example 4.2 (Bounded drift). Suppose that the distribution drift between consecutive periods is
bounded, in the sense that sup ∥F −F ∥ ≤ ∆ holds for some 0 < ∆ ≤ 1. For simplicity, we
j≥1 j+1 j ∞
further assume B = 1 for all j. In this case, ϕ(t,k) ≤ (k−1)∆, so the bias-variance decomposition
j
in Theorem 3.1 becomes
(cid:114)
α(1−α) 1
|F (q )−(1−α)| ≲ (k−1)∆+ +
t (cid:98)t,k
k k
up to a logarithmic factor. If ∆ were known, then one could pick the optimal window size k∗ ≍ ∆−2/3,
which gives an error of O(∆1/3). Theorem 4.1 shows that without knowing ∆, Algorithm 1 picks (cid:98)k
whose quality is comparable to k∗.
Below we provide a sketch of proof for Theorem 4.1. The full proof is given in Appendix A.3.
Proof sketch for Theorem 4.1. To keep the notations simple and focus on the main ideas, we
suppress the parameters t and δ in the expressions of ϕ, ϕ(cid:98) and ψ. We will prove that with high
probability,
(cid:12) (cid:12)F t(q (cid:98)t,(cid:98)k)−(1−α)(cid:12) (cid:12) ≲ ϕ(k)+ψ(k), ∀k ∈ [t].
By Theorem 3.2 we know that 0 ≤ ϕ(cid:98)≤ ϕ with high probability. For every k ∈ {(cid:98)k+1,··· ,t}, with
high probability,
|F t(q (cid:98)t,(cid:98)k)−(1−α)| ≤ ϕ((cid:98)k)+ψ((cid:98)k) (by Theorem 3.1)
≤ ϕ(k)+ϕ(cid:98)((cid:98)k)+ψ((cid:98)k) (by the motonicity of ϕ and the fact ϕ(cid:98)≥ 0)
7≤ ϕ(k)+ϕ(cid:98)(k)+ψ(k) (by the definition of (cid:98)k)
≲ ϕ(k)+ψ(k). (by ϕ(cid:98)≤ ϕ)
It remains to consider k ∈ [(cid:98)k]. Choose any k in this range. With high probability,
|F(cid:98)t,k(q (cid:98)t,(cid:98)k)−(1−α)| ≲ ϕ(cid:98)((cid:98)k)+ψ((cid:98)k)+ψ(k) (by the definition of ϕ(cid:98))
≤ ϕ(cid:98)(k)+ψ(k)+ψ(k) (by the definition of (cid:98)k)
≲ ϕ(k)+ψ(k). (by ϕ(cid:98)≤ ϕ)
Let r
k
≍ ϕ(k)+ψ(k) denote the right-hand side of the inequality above. The bound |F(cid:98)t,k(q (cid:98)t,(cid:98)k)−
(1−α)| ≤ r implies that the final estimate q is sandwiched between the left (1−α−r )-quantile
k (cid:98)t,(cid:98)k k
and the right (1−α+r k)-quantile of F(cid:98)t,k. Hence,
F t(cid:0) Q−
1−α−r
k(F(cid:98)t,k)(cid:1) ≤ F t(q (cid:98)t,(cid:98)k) ≤ F t(cid:0) Q+
1−α+r
k(F(cid:98)t,k)(cid:1) .
We use a concentration bound for empirical quantile under distribution shift (Lemma B.4 in
Appendix B) to derive that with high probability,
F t(cid:0) Q+
1−α+r
(F(cid:98)t,k)(cid:1) −(1−α+r k) ≲ ϕ(k)+ψ(k),
k
(1−α−r k)−F t(cid:0) Q−
1−α−r
(F(cid:98)t,k)(cid:1) ≲ ϕ(k)+ψ(k).
k
Combining the above estimates, we get
|F t(q (cid:98)t,(cid:98)k)−(1−α)| ≲ ϕ(k)+ψ(k), ∀k ∈ [(cid:98)k].
This finishes the proof.
Theorem 4.1 also translates to a theoretical guarantee for Algorithm 2. In particular, Q and F
j j
are, respectively, the distribution and the CDF of the conformity score s(x ,y ), where (x ,y ) ∼ P .
j j j j j
To facilitate interpretation, we may further upper bound the bias term by
ϕ(t,k) = max ∥F −F ∥ ≤ max TV(P ,P ),
j t ∞ j t
t−k+1≤j≤t t−k+1≤j≤t
which is the maximum total variation distance between P and P ,...,P . This establishes
t t−1 t−k+1
the following result. Its proof is omitted.
Corollary 4.1. Let Assumption 2.1 hold. Choose δ ∈ (0,1) and take δ′ = δ/(4t2) in Algorithm 2.
With probability at least 1−δ, Algorithm 2 outputs C(cid:98)t(·) satisfying
(cid:40) (cid:115) (cid:41)
(cid:12) (cid:12) (cid:12)P(cid:0) y
t
∈ C(cid:98)t(x t) | {B j}t j=1(cid:1) −(1−α)(cid:12) (cid:12)
(cid:12)
≲ m k∈i [n
t]
t−km +1a ≤x j≤tTV(P j,P t)+ α( B1− t,kα) + B1
t,k
,
where ≲ only hides a logarithmic factor of t and δ−1. Consequently,
(cid:12) (cid:12)
(cid:12) (cid:12)P(cid:0) y
t
∈ C(cid:98)t(x t)(cid:1) −(1−α)(cid:12)
(cid:12)
(cid:12) (cid:12)
≤ E(cid:12) (cid:12)P(cid:0) y t ∈ C(cid:98)t(x t) (cid:12) (cid:12) {B j}t j=1(cid:1) −(1−α)(cid:12) (cid:12)
(cid:40) (cid:115) (cid:41)
α(1−α) 1
≲ (1−δ)min max TV(P ,P )+ + +δmax{α,1−α}.
j t
k∈[t] t−k+1≤j≤t B t,k B t,k
85 Numerical experiments
We test our proposed method on synthetic and real data, focusing on regression problems with
Y = R and prediction intervals of the form (2.5). As a common phenomenon in learning theory,
the non-asymptotic analysis provides sharp rates but not necessarily optimal constants. We make
minor changes to the functions ψ and ϕ(cid:98)in Algorithm 1 to correct for overly large constant factors.
To improve computational efficiency, we only consider candidate window sizes that are powers
of 2; see Algorithm 4 in Appendix C.1 for a detailed description. The theoretical guarantees in
Section 4 continue to hold for this modification up to a constant factor. Our prediction intervals
are constructed by Algorithm 2, with Algorithm 4 (instead of Algorithm 1) as the sub-routine for
quantile estimation.
Throughout the experiments, we take α = 0.1 to aim for prediction sets with 90% coverage, and
set the hyperparameter δ′ to be 0.1. For notational convenience, we denote our aforementioned
method by ARW (adaptive rolling window). We will compare it with the following two families of
benchmark approaches:
• Algorithm 3, denoted by V . This is a non-adaptive, fixed-window version of Algorithm 2. We
k
consider window sizes k ∈ {1,4,16,64,256,1024}.
• The nonexchangeable split conformal prediction method in Section 3.1 of Barber et al. (2023),
with weights w = ρt−i, i ∈ [t] at time t. We refer to this method as W , and consider decay rates
i ρ
ρ ∈ {0.99,0.9,0.25}.
Algorithm 3 Predictive inference via fixed-window quantile estimation
Input: Calibration datasets {B }t with B = {(x ,y )}Bj , conformity score s : X ×Y → R,
j j=1 j j,i j,i i=1
miscoverage level α and window size k.
Let u = s(x ,y ). Compute q according to (3.1).
j,i j,i j,i (cid:98)t,k∧t
return C(cid:98)t(·) = {y ∈ Y : s(·,y) ≤ q (cid:98)t,k∧t}.
All our results are averaged over 100 independent runs. The standard errors are less than
10% of those mean values and thus omitted for space considerations. Our code is available at
https://github.com/eliselyhan/predictive-inference.
5.1 Synthetic data
In the synthetic data experiment, we perform predictive inference for two problem instances, namely,
Gaussian mean estimation and linear regression. In both instances, there are T = 1000 periods in
total. In each period, we split the data into a training set for fitting predictors, and a calibration set
for forming prediction sets.
Gaussianmeanestimation. ThemeanestimationproblemhasX = ∅,Y = R,andP = N(µ ,1).
t t
We consider two patterns of the sequence {µ }T , given in Figure 1. The left panel represents the
t t=1
stationary case where µ = ··· = µ . The right panel corresponds to a highly non-stationary pattern
1 T
generated using a randomized mechanism, which we describe in Appendix C.2.
In each period j ∈ [T], we generate i.i.d. training samples Btr and i.i.d. calibration samples Bca
j j
from P . The size |Btr| of the training set Btr is drawn uniformly over {1,··· ,9}, and |Btr| = |Bca|.
j j j j j
At time t ∈ [T], we consider point estimates µ ∈ R that compute moving averages of the data
(cid:98)t,k
92.0
1.04
1.5
1.02 1.0
1.00 0.5
0.0
0.98
0.5
0.96
1.0
0 200 400 600 800 1000 0 200 400 600 800 1000
Figure 1: True means µ .
t
{B }t from the last k ∈ {1,16,256,1024} periods. For each model, we then compute the
j j=t−k∧t+1
conformity scores using the calibration data.
Each of the candidate methods (ARW and the benchmark algorithms V and W ) produces a
k ρ
prediction interval C(cid:98)t of the form (2.5). We calculate the mean absolute error (MAE) between the
true coverage and target level 1−α, given by
T
1 (cid:88)
MAE = |P(z
t
∈ C(cid:98)t | C(cid:98)t)−(1−α)|,
T −100
t=T−99
where z
t
∼ P t, and the conditional probability P(z
t
∈ C(cid:98)t | C(cid:98)t) can be computed using the CDF
of the normal distribution. Following Barber et al. (2023), we average data only after the initial
burn-in period with 100 steps.
Tables 1 and 2 list the MAEs of different algorithms for the two patterns of {µ }T respectively.
t t=1
The results are averaged over 100 independent runs. In each setting, the best performance of
benchmark algorithm is shown in boldface. Our algorithm ARW achieves comparable results while
being agnostic to the underlying temporal drift. Figure 2 provides bar plots to visualize the MAEs
of all algorithms for the prediction model µ , which corresponds to the first rows of Tables 1 and 2.
(cid:98)t,1
Training
Window ARW W W W W V V V V V V
0.99 0.9 0.5 0.99 1 4 16 64 256 1024
1 0.50 0.48 0.48 0.62 0.91 15.32 5.63 2.71 1.33 0.69 0.48
64 0.47 0.46 0.46 0.61 0.90 15.31 5.67 2.72 1.35 0.67 0.46
256 0.47 0.46 0.46 0.62 0.92 15.30 5.66 2.71 1.33 0.68 0.46
1024 0.47 0.45 0.45 0.61 0.91 15.30 5.66 2.71 1.33 0.68 0.45
Table 1: Mean absolute error (in %) of coverage for stationary mean estimation.
Linear regression. We set X = R5. For each t ∈ [T], the data distribution is P = P ×P ,
t t,x t,y|x
where P = N(0,I ) is the marginal distribution of the covariate x, and P = N(x⊤β ,1) is the
t,x 5 t,y|x t
conditional distribution of y given x. As in the mean estimation problem, we consider two patterns
of {β }T . Let 1 = (1,1,1,1,1)⊤ ∈ R5. In the stationary pattern, β = 11 for all t ∈ [T]. In the
t t=1 5 t 5 5
non-stationary pattern, β = 2β 1 , where {β }T follows the non-stationary pattern in the right
t t 5 t t=1
panel of Figure 1; see Appendix C.2 for a detailed description.
10Training
Window ARW W W W W V V V V V V
0.99 0.9 0.5 0.99 1 4 16 64 256 1024
1 3.28 7.24 7.26 7.50 7.71 15.32 5.65 2.99 2.81 4.41 7.24
64 2.53 7.34 7.37 7.65 7.88 15.28 5.70 3.06 2.23 3.68 7.34
256 3.04 7.73 7.76 7.97 8.17 15.34 5.72 3.18 3.10 4.54 7.73
1024 3.50 5.45 5.45 5.47 5.54 15.42 5.79 3.36 3.45 5.23 5.45
Table 2: Mean absolute error (in %) of coverage for non-stationary mean estimation.
0.16 0.16
0.14 0.14
0.12 0.12
0.10 0.10
0.08 0.08
0.06 0.06
0.04 0.04
0.02 0.02
0.00 0.00
ARW W0.99 W0.9 W0.5 W0.25 V1 V4 V16 V64 V256 V1024 ARW W0.99 W0.9 W0.5 W0.25 V1 V4 V16 V64 V256 V1024
Figure 2: Mean absolute errors of coverage for mean estimation. Left: stationary case. Right:
non-stationary case.
In each period j ∈ [T], we generate i.i.d. training samples Btr and i.i.d. calibration samples
j
Bca from P . The size |Bca| of the calibration set Bca is drawn uniformly over {1,··· ,9}, and
j j j j
|Btr| = 3|Bca|. At time t ∈ [T], we run ordinary least squares (OLS) to train a linear prediction
j j
model µ , where k ∈ {1,16,256,1024}, and µ uses the training data from the last k periods
(cid:98)t,k (cid:98)t,k
{Btr}t . We carry out the same procedures as in the mean estimation problem to compute
j j=t−t∧k+1
conformity scores and form prediction intervals using ARW, V and W . The true coverage level
k ρ
c
t
= P(y
t
∈ C(cid:98)t(x t) | C(cid:98)t), where (x t,y t) ∼ P t, is approximated by drawing another 1000 independent
samples from P .
t
InTables3and4,wereporttheMAEsofdifferentalgorithmsover100independentruns. Figure3
provides a bar plot visualizing the MAEs of different algorithms using the OLS point predictor µ .
(cid:98)t,1
Training
Window ARW W W W W V V V V V V
0.99 0.9 0.5 0.99 1 4 16 64 256 1024
1 0.90 0.90 0.90 0.96 1.15 15.44 5.69 2.80 1.53 1.00 0.90
64 0.91 0.90 0.90 0.96 1.16 15.43 5.68 2.80 1.53 1.00 0.90
256 0.90 0.90 0.89 0.96 1.15 15.44 5.68 2.79 1.52 1.00 0.90
1024 0.91 0.90 0.90 0.96 1.15 15.44 5.67 2.79 1.52 1.00 0.90
Table 3: Mean absolute error (in %) of coverage for stationary linear regression.
11Training
Window ARW W W W W V V V V V V
0.99 0.9 0.5 0.99 1 4 16 64 256 1024
1 3.60 8.69 8.67 8.49 8.33 15.46 5.77 3.29 3.61 5.84 8.69
64 3.63 8.63 8.61 8.44 8.26 15.44 5.77 3.29 3.62 5.79 8.63
256 3.69 8.36 8.34 8.15 7.97 15.45 5.77 3.28 3.64 5.58 8.36
1024 3.75 8.07 8.06 7.88 7.71 15.45 5.76 3.30 3.67 5.50 8.07
Table 4: Mean absolute error (in %) of coverage for nonstationary linear regression.
0.16 0.16
0.14 0.14
0.12 0.12
0.10 0.10
0.08 0.08
0.06 0.06
0.04 0.04
0.02 0.02
0.00 0.00
ARW W0.99 W0.9 W0.5 W0.25 V1 V4 V16 V64 V256 V1024 ARW W0.99 W0.9 W0.5 W0.25 V1 V4 V16 V64 V256 V1024
Figure 3: Mean absolute errors of coverage for linear regression. Left: stationary case. Right:
non-stationary case.
Summary. The results from mean estimation and linear regression show that ARW consistently
achieves superior performances in both stationary and non-stationary settings.
5.2 Real data
Finally, we test our method using a real estate dataset maintained by the Dubai Land Department.1
We study the sales of apartments during the past 16 years (from January 1st, 2008 to December
31st, 2023). There are 211,432 samples (sales) in total, and we want to predict the final price given
characteristics of an apartment (e.g., number of rooms, size, location). Each week is treated as
a time period, and there are 826 of them in total. Figure 4 in Appendix C.3 shows the weekly
average of logarithmic prices, illustrating the distribution shift over time. We split week t’s data
into a training set Btr, a calibration set Bca, and a test set Bte with proportions 30%, 10%, and
t t t
60%, respectively. Most samples are allocated to the test set so that the coverage probability can be
estimated accurately. We follow the standard practice to apply a logarithmic transform to the price
and target that in our prediction. See Appendix C.3 for other details of preprocessing.
For each t ∈ {10,20,30,··· ,820}, we run XGBoost (Chen and Guestrin, 2016) on 4 different
windows of historical training data {Btr}t with k ∈ {1,16,256,1024}, to get 4 point
j j=t−k∧t+1
prediction models. For each of them, we compute its associated conformity scores on the calibration
data{B jca}t j=1,runAlgorithm2toobtainapredictionintervalC(cid:98)t,andcomputeitscoveragefrequency
on the test set Bte. We also compare our adaptive rolling window approach to V and W . We run
t k ρ
the experiment independently over 100 random seeds and report the average results in Tables 5
and 6. For all of the 4 training strategies, our approach consistently achieves the nominal coverage
1https://www.dubaipulse.gov.ae/data/dld-transactions/dld_transactions-open
12level with small average interval width. It outperforms weighting methods and is comparable to the
best fixed-window method.
Training
Window ARW W W W W V V V V V V
0.99 0.9 0.5 0.25 1 4 16 64 256 1024
1 3.44 4.47 4.47 4.52 4.58 6.26 3.67 3.00 3.18 4.12 4.46
16 3.13 6.18 6.19 6.26 6.35 6.33 3.80 2.97 3.00 5.24 6.18
256 3.28 4.43 4.43 4.48 4.53 6.25 3.72 3.00 3.76 5.54 4.43
1024 3.10 3.63 3.63 3.64 3.68 6.34 3.73 3.06 3.29 3.84 3.63
Table 5: Mean absolute error (in %) of coverage on the housing data.
Training
Window ARW W W W W V V V V V V
0.99 0.9 0.5 0.25 1 4 16 64 256 1024
1 1.58 1.70 1.70 1.70 1.71 1.37 1.47 1.49 1.55 1.65 1.70
16 1.10 1.41 1.41 1.42 1.43 0.98 0.98 0.97 1.09 1.30 1.41
256 0.98 1.15 1.15 1.16 1.17 0.97 0.99 0.97 0.96 0.98 1.15
1024 1.03 1.09 1.09 1.10 1.10 1.01 1.03 1.03 1.03 1.05 1.09
Table 6: Mean interval width on the housing data.
6 Discussions
We develop rolling window strategies for quantile estimation and predictive inference under temporal
drift. Theoretical analyses and numerical experiments demonstrate their adaptivity to the unknown
distribution shift. This work opens up a number of future directions. Our approach assumes
decoupled training and validation procedures. It is worth investigating how our methods can be
extended to procedures that make more efficient use of data, such as full conformal prediction (Vovk
et al., 2005), jackknife+ and CV+ (Barber et al., 2021). Another feature of our work is that no
structural assumption on the temporal drift is needed. In practice, certain drift patterns can be
learned from data or domain knowledge, such as seasonalities and trends. How to utilize those
structures for better use of data while safeguarding against the risk of misspecification is of great
importance.
Acknowledgement
Elise Han, Chengpiao Huang, and Kaizheng Wang’s research is supported by an NSF grant DMS-
2210907 and a startup grant at Columbia University.
13A Proofs
A.1 Proof of Theorem 3.1
Applying Lemma B.4 to the data {D }t yields
j j=t−k+1
(cid:18) (cid:26) (cid:27)(cid:19)
R(B ,ε,1−α,δ/2)
P |F (q )−(1−α)| ≤ ϕ(t,k)+min t,k ≥ 1−δ,
t (cid:98)t,k
ε>0 1−ε
where the function R is defined in Definition B.1. The proof is finished by taking ε = 1/5.
A.2 Proof of Theorem 3.2
Choose any δ ∈ (0,1), ε > 0 and η > 0. Let R(n,ε,γ,δ) be defined as in Definition B.1 with
γ = 1−α. We will show that with probability at least 1−δ,
1+η
|F(cid:98)t,i(q (cid:98)t,k)−(1−α)| ≤ 2(1+η)ϕ(t,k)+ R(B t,k,ε,1−α,δ/4)+R(B t,i,η,1−α,δ/4). (A.1)
1−ε
If this is true, we can take η = ε = 1/5 and get
12 3
|F(cid:98)t,i(q (cid:98)t,k)−(1−α)| ≤ ϕ(t,k)+ R(B t,k,1/5,1−α,δ/4)+R(B t,i,1/5,1−α,δ/4)
5 2
12 6 4
≤ ϕ(t,k)+ ψ(t,k,δ/2)+ ψ(t,i,δ/2).
5 5 5
We now prove (A.1). Let
R(B ,ε,1−α,δ/4)
t,k
ψ = .
1
1−ε
Denote by A the event that
|F (q )−(1−α)| ≤ ϕ(t,k)+ψ .
t (cid:98)t,k 1
According to Theorem 3.1, P(A) ≥ 1−δ/2. Let r = ϕ(t,k)+ψ . When A happens, we have
1
Q− (F ) ≤ q ≤ Q+ (F )
1−α−r t (cid:98)t,k 1−α+r t
and hence,
(cid:18) (cid:19)
P F(cid:98)t,i(cid:0) Q− 1−α−r(F t)(cid:1) ≤ F(cid:98)t,i(q (cid:98)t,k) ≤ F(cid:98)t,i(cid:0) Q+ 1−α+r(F t)(cid:1) ≥ 1−δ/2.
Let q+ = Q+ (F ), which is deterministic. Then
1−α+r t
F(cid:98)t,i(q+) = 1
(cid:88)t (cid:88)Bj
1(u
j,ℓ
≤ q+) and E(cid:2) F(cid:98)t,i(q+)(cid:3) = F t,i(q+).
B
t,i
j=t−i+1ℓ=1
NotethatF(cid:98)t,i(q+)isanaverageofindependentBernoullirandomvariables. WewillapplyLemmaB.2
to obtain concentration of F(cid:98)t,i(q+) around F t,i(q+). By Assumption 2.2, F
t
is absolutely continuous,
so F (q+) = min{1−α+r,1} = (1−α)+min{r,α}. Hence,
t
|P(u ≤ q+)−(1−α)| ≤ |F (q+)−F (q+)|+min{r,α} ≤ ϕ(t,i)+r, t−i+1 ≤ j ≤ t.
j,ℓ j t
Choose any δ ∈ (0,1) and η > 0. By Lemma B.2,
(cid:16) (cid:17) δ
P F(cid:98)t,i(q+)−F t,i(q+) ≤ η[ϕ(t,i)+r]+R(B t,i,η,1−α,δ/4) ≥ 1− . (A.2)
4
14Since
F (q+) ≤ F (q+)+ϕ(t,i) ≤ 1−α+r+ϕ(t,i),
t,i t
then (A.2) implies
(cid:16) (cid:17) δ
P F(cid:98)t,i(Q+ q+r(F t))−(1−α) ≤ (1+η)[r+ϕ(t,i)]+R(B t,i,η,1−α,δ/4) ≥ 1− 4.
Similarly, we can prove the same tail bound for −[F(cid:98)t,i(Q− q−r(F t))−(1−α)]. By union bounds,
the following inequalities hold simultaneously with probability at least 1−δ/2:
F(cid:98)t,i(Q+ q+r(F t))−(1−α) ≤ (1+η)[r+ϕ(t,i)]+R(B t,i,η,1−α,δ/4),
F(cid:98)t,i(Q− q−r(F t))−(1−α) ≥ −{(1+η)[r+ϕ(t,i)]+R(B t,i,η,1−α,δ/4)}.
Combining this with (A.2), we see that with probability at least 1−δ,
|F(cid:98)t,i(q (cid:98)t,k)−(1−α)| ≤ (1+η)[ϕ(t,k)+ψ 1+ϕ(t,i)]+R(B t,i,η,1−α,δ/4)
(cid:18) (cid:19)
R(B ,ε,1−α,δ/4)
t,k
≤ (1+η) 2ϕ(t,k)+ +R(B ,η,1−α,δ/4).
t,i
1−ε
The last inequality is due to i ≤ k and thus ϕ(t,i) ≤ ϕ(t,k). We have obtained (A.1).
A.3 Proof of Theorem 4.1
Let δ′ = δ/(4t2). We will prove that with probability at least 1−δ,
|F (q )−(1−α)| ≤
6(cid:2) ϕ(t,k)+ψ(t,k,δ′)(cid:3)
, ∀k ∈ [t].
t (cid:98)t,(cid:98)k
By Theorem 3.1 and the union bound,
(cid:18) (cid:19)
P |F (q )−(1−α)| ≤ ϕ(t,k)+ψ(t,k,δ′), ∀k ∈ [t] ≥ 1−tδ′. (A.3)
t (cid:98)t,k
By Theorem 3.2 and union bounds,
(cid:16) (cid:17)
P 0 ≤ ϕ(cid:98)(t,k,δ′) ≤ ϕ(t,k), ∀k ∈ [t] ≥ 1−t2δ′. (A.4)
For all k ∈ [t], deterministically,
ψ(t,k,δ′/2) ≤ 2ψ(t,k,δ′). (A.5)
From now on, assume that both the events in (A.3) and (A.4) happen.
For all k ∈ {(cid:98)k+1,...,t},
|F t(q (cid:98)t,(cid:98)k)−(1−α)| ≤ ϕ(t,(cid:98)k)+ψ(t,(cid:98)k,δ′) (by (A.3))
≤ ϕ(t,k)+ϕ(cid:98)(t,(cid:98)k,δ′)+ψ(t,(cid:98)k,δ′) (by (A.4))
≤ ϕ(t,k)+ϕ(cid:98)(t,k,δ′)+ψ(t,k,δ′) (by the definition of (cid:98)k)
≤ ϕ(t,k)+ϕ(t,k)+ψ(t,k,δ′) (by (A.4))
= 2ϕ(t,k)+ψ(t,k,δ′). (A.6)
15Next, take arbitrary k ∈ [(cid:98)k]. Then
12 6 4
|F(cid:98)t,k(q (cid:98)t,(cid:98)k)−(1−α)| ≤
5
ϕ(cid:98)(t,(cid:98)k,δ′)+ 5ψ(t,(cid:98)k,δ′/2)+ 5ψ(t,k,δ′/2) (by the definition of ϕ(cid:98))
12 12 8
≤ ϕ(cid:98)(t,(cid:98)k,δ′)+ ψ(t,(cid:98)k,δ′)+ ψ(t,k,δ′) (by (A.5))
5 5 5
12 12 8
≤ ϕ(cid:98)(t,k,δ′)+ ψ(t,k,δ′)+ ψ(t,k,δ′) (by the definition of (cid:98)k)
5 5 5
12
≤ ϕ(t,k)+4ψ(t,k,δ′). (by (A.4))
5
Let r
k
= 1 52ϕ(t,k)+4ψ(t,k,δ′). Then |F(cid:98)t,k(q (cid:98)t,(cid:98)k)−(1−α)| ≤ r
k
implies
Q−
1−α−r
k(cid:0) F(cid:98)t,k(cid:1) ≤ q
(cid:98)t,(cid:98)k
≤ Q+
1−α+r
k(cid:0) F(cid:98)t,k(cid:1) .
Therefore,
F t(q (cid:98)t,(cid:98)k) ≤ F t(cid:16) Q+
1−α+r
k(cid:0) F(cid:98)t,k(cid:1)(cid:17) = F
t(cid:32)
Q+
1−α+r
k(cid:18)
B1
(cid:88)t−1 (cid:88)Bj
δ
uj,i(cid:19)(cid:33)
t,k
j=t−k i=1
(cid:18)
1
(cid:88)t−1 (cid:88)Bj (cid:19)
= Q+ δ
1−α+r k B Ft(uj,i)
t,k
j=t−k i=1
(cid:18)
1
(cid:88)t−1 (cid:88)Bj (cid:19)
≤ Q+ δ +ϕ(t,k). (by Lemma B.1)
1−α+r k B Fj(uj,i)
t,k
j=t−k i=1
By Lemma B.3 and the union bound,
 
P Q+
1−α+r
k(cid:18)
B1
(cid:88)t−1 (cid:88)Bj
δ
Fj(uj,i)(cid:19)
≤ (1−α+r k)+
r
4k +
45
R(B t,k,1/5,1−α,δ′), ∀k ∈ [t]
t,k
j=t−k i=1
≥ 1−tδ′, (A.7)
where R(n,ε,γ,δ) is defined in Definition B.1. Similarly,
F t(q (cid:98)t,(cid:98)k) ≥ F t(cid:16) Q−
1−α−r
k(cid:0) F(cid:98)t,k(cid:1)(cid:17) = F
t(cid:32)
Q−
1−α−r
k(cid:18)
B1
(cid:88)t−1 (cid:88)Bj
δ
uj,i(cid:19)(cid:33)
t,k
j=t−k i=1
(cid:18)
1
(cid:88)t−1 (cid:88)Bj (cid:19)
= Q− δ
1−α−r k B Ft(uj,i)
t,k
j=t−k i=1
(cid:18)
1
(cid:88)t−1 (cid:88)Bj (cid:19)
≥ Q− δ −ϕ(t,k). (by Lemma B.1)
1−α−r k B Fj(uj,i)
t,k
j=t−k i=1
By Lemma B.3 and the union bound,
16 
P Q−
1−α−r
k(cid:18)
B1
(cid:88)t−1 (cid:88)Bj
δ
Fj(uj,i)(cid:19)
≥ (1−α−r k)−
r
4k −
45
R(B t,k,1/5,1−α,δ′), ∀k ∈ [t]
t,k
j=t−k i=1
≥ 1−tδ′. (A.8)
When the events in (A.7) and (A.8) happen,
5 5
|F (q )−(1−α)| ≤ r + R(B ,1/5,1−α,δ′)+ϕ(t,k) (by (A.7) and (A.8))
t (cid:98)t,(cid:98)k
4
k
4
t,k
≤ (cid:0) 3ϕ(t,k)+5ψ(t,k,δ′)(cid:1) +ψ(t,k,δ′)+ϕ(t,k)
= 4ϕ(t,k)+6ψ(t,k,δ′). (A.9)
Therefore, when the events in (A.3), (A.4), (A.7) and (A.8) happen, which has probability at
least 1−(3t+t2)δ′ ≥ 1−δ, we have (A.6) and (A.9), and hence
|F (q )−(1−α)| ≤
6min(cid:8) ϕ(t,k)+ψ(t,k,δ′)(cid:9)
.
t (cid:98)t,(cid:98)k
k∈[t]
B Technical lemmas
Lemma B.1 (Perturbation bound for empirical quantile). Let {x }n and {y }n be real numbers.
i i=1 i i=1
We have
(cid:12) (cid:18) n (cid:19) (cid:18) n (cid:19)(cid:12)
γm ∈(a 0,x 1)(cid:12) (cid:12) (cid:12)Q− γ n1 (cid:88) δ xi −Q− γ n1 (cid:88) δ yi (cid:12) (cid:12)
(cid:12)
≤ m i∈a [nx ]|x i−y i|.
i=1 i=1
The same bound holds if we replace Q− by Q+.
Proof of Lemma B.1. Take permutations (k ,...,k ) and (ℓ ,...,ℓ ) of [n] such that x ≤ ··· ≤
1 n 1 n k1
x and y ≤ ··· ≤ y . Fix γ ∈ (0,1). There exists m ∈ [n] such that
kn ℓ1 ℓn
(cid:18) n (cid:19) (cid:18) n (cid:19)
1 (cid:88) 1 (cid:88)
x = Q− δ and y = Q− δ .
km γ n xi ℓm γ n yi
i=1 i=1
Without loss of generality, assume x ≥ y . By the pigeonhole principle, the set S = {k : m ≤
km ℓm i
i ≤ n}∩{ℓ : 1 ≤ i ≤ m} is nonempty. Take j ∈ S, then x ≤ x and y ≥ y . Therefore,
i km j ℓm j
|x −y | = x −y ≤ x −y ≤ max|x −y |.
km ℓm km ℓm j j i i
i∈[n]
Taking maximum over all γ ∈ (0,1) finishes the proof.
Lemma B.2 (Bernstein’s bound with perturbation). Let {x }n be independent Bernoulli random
i i=1
variables, p = Ex and p¯= 1 (cid:80)n p . For any r > 0, ε > 0, δ ∈ (0,1) and q ∈ [0,1] that satisfies
i i n i=1 i
|q−p¯| ≤ r, the following holds with probability at least 1−δ:
n (cid:114) (cid:18) (cid:19)
1 (cid:88) 2q(1−q)log(1/δ) 2 1 log(1/δ)
x −p¯< εr+ + + .
i
n n 3 2ε n
i=1
The same upper bound also holds for p¯− 1 (cid:80)n x with probability at least 1−δ.
n i=1 i
17Proof of Lemma B.2. Below we only prove the upper bound on 1 (cid:80)n x −p¯. The other bound
n i=1 i
follows from a similar argument. Let x¯ = 1 (cid:80)n x and σ2 = 1 (cid:80)n var(x ) = 1 (cid:80)n p (1−p ).
n i=1 i n i=1 i n i=1 i i
By Lemma 3.1 in Han et al. (2024) (a version of Bernstein’s inequality),
(cid:32) n (cid:114) (cid:33)
1 (cid:88) 2log(1/δ) 2log(1/δ)
P x −p¯≤ σ + ≥ 1−δ.
i
n n 3n
i=1
Denote by A the event on the left-hand side. By the concavity of the function x (cid:55)→ x(1−x) and
Jensen’s inequality,
n
1 (cid:88)
σ2 = p (1−p ) ≤ p¯(1−p¯).
i i
n
i=1
When A happens,
(cid:114)
2p¯(1−p¯)log(1/δ) 2log(1/δ)
x¯−p¯≤ + .
n 3n
Let h(x) = x(1−x). We have |h′(x)| = |1−2x| < 1 for x ∈ (0,1). From r > 0 and |q−p¯| ≤ r, we
get p¯(1−p¯) < q(1−q)+r and
(cid:112) (cid:112) (cid:112) √
p¯(1−p¯) < q(1−q)+r ≤ q(1−q)+ r.
We have
(cid:114) (cid:114) (cid:114)
2p¯(1−p¯)log(1/δ) 2q(1−q)log(1/δ) 2rlog(1/δ)
< +
n n n
(cid:114) (cid:18) (cid:19)
2q(1−q)log(1/δ) 1 1 log(1/δ)
≤ + ε·2r+ · , ∀ε > 0.
n 2 ε n
Therefore, on the event A,
(cid:114) (cid:18) (cid:19)
2q(1−q)log(1/δ) 2 1 log(1/δ)
x¯−p¯< εr+ + + .
n 3 2ε n
Consequently, the above inequality holds with probability at least 1−δ.
Lemma B.3 (Concentration of empirical quantile with perturbation). Let {x }n be i.i.d. U(0,1)
i i=1
random variables. For any γ,γ′,δ ∈ (0,1), with probability at least 1−δ we have
(cid:18) 1 (cid:88)n (cid:19) (cid:26) (cid:20) (cid:114) 2γ′(1−γ′)log(1/δ) (cid:18) 2 1 (cid:19) log(1/δ)(cid:21)(cid:27)
Q+ δ −γ ≤ min (1−ε)−1 ε|γ−γ′|+ + + .
γ n xi ε∈(0,1) n 3 2ε n
i=1
The same upper bound also holds for γ −Q−(cid:0)1 (cid:80)n δ (cid:1) with probability at least 1−δ.
γ n i=1 xi
Proof of Lemma B.3. Define q− = Q−(1 (cid:80)n δ ) and q+ = Q+(1 (cid:80)n δ ). Let
γ n i=1 xi γ n i=1 xi
(cid:20) (cid:114) 2γ′(1−γ′)log(2/δ) (cid:18) 2 1 (cid:19) log(2/δ)(cid:21)
T(ε) = (1−ε)−1 ε|γ −γ′|+ + + , ∀ε ∈ (0,1)
n 3 2ε n
be the function inside the min operator. For any t > 0, we have
n n
1 (cid:88) 1 (cid:88)
q+−γ > t ⇒ 1(x ≤ γ +t) ≤ γ ⇔ (γ +t)− 1(x ≤ γ +t) ≥ t,
i i
n n
i=1 i=1
18n n
1 (cid:88) 1 (cid:88)
q−−γ < −t ⇒ 1(x ≤ γ −t) ≥ γ ⇔ 1(x ≤ γ −t)−(γ −t) ≥ t.
i i
n n
i=1 i=1
Based on the above,
(cid:18) n (cid:19)
1 (cid:88)
P(q+−γ ≤ t) ≥ P (γ +t)− 1(x ≤ γ +t) < t ,
i
n
i=1
(cid:18) n (cid:19)
1 (cid:88)
P(γ −q− ≤ t) ≥ P 1(x ≤ γ −t)−(γ −t) < t .
i
n
i=1
Choose any δ ∈ (0,1) and ε > 0. Applying Lemma B.2 to {1(x ≤ γ ±t)}n , p = γ ±t and
i i=1 i
q = γ′ yields that for every ε > 0, each of the following inequalities holds with probability at least
1−δ:
1 (cid:88)n (cid:114) 2γ′(1−γ′)log(1/δ) (cid:18) 2 1 (cid:19) log(1/δ)
(γ +t)− 1(x ≤ γ +t) < εt+ε|γ −γ′|+ + + ,
i
n n 3 2ε n
i=1
1 (cid:88)n (cid:114) 2γ′(1−γ′)log(1/δ) (cid:18) 2 1 (cid:19) log(1/δ)
1(x ≤ γ −t)−(γ −t) < εt+ε|γ −γ′|+ + + .
i
n n 3 2ε n
i=1
Therefore, we would have P(q+−γ ≤ t) ≥ 1−δ and P(γ −q− ≤ t) ≥ 1−δ provided that
(cid:114) (cid:18) (cid:19)
2γ(1−γ)log(1/δ) 2 1 log(1/δ)
t ≥ εt+ε|γ −γ′|+ + + ,
n 3 2ε n
which is equivalent to t ≥ T(ε). Since the function T is continuous on (0,1) and lim T(ε) =
ε→0+
lim T(ε) = +∞, then T attains its minimum value at some ε ∈ (0,1). The proof is finished by
ε→1− 0
taking t = T(ε ).
0
Based on Lemma B.3, we can study the concentration of empirical quantile under distribution
shift. To state the result, we define a useful function that appears repeatedly in our proofs as
probabilistic upper bounds.
Definition B.1. For any n ∈ Z , ε > 0 and γ,δ ∈ (0,1), let
+
(cid:114) (cid:18) (cid:19)
2γ(1−γ)log(1/δ) 2 1 log(1/δ)
R(n,ε,γ,δ) = + + .
n 3 2ε n
Lemma B.4 (Concentration of empirical quantile under distribution shift). Let {x }n be indepen-
i i=1
dent random variables with absolutely continuous CDF’s {F i}n i=1, and denote by F(cid:98) their empirical
CDF. Define ϕ = max ∥F −F ∥ . For any γ,δ ∈ (0,1), we have
i∈[n] i n ∞
(cid:18) (cid:26) (cid:27)(cid:19)
P F n(cid:0) Q+ γ(F(cid:98))(cid:1) −γ ≤ ϕ+m ε>in
0
R(n 1, −ε,γ ε,δ) ≥ 1−δ.
The same bound also holds for γ −F n(cid:0) Q− γ(F(cid:98))(cid:1).
19Proof of Lemma B.4. We only prove the bound on F n(cid:0) Q+ γ(F(cid:98))(cid:1) −γ, as the other bound follows
from the same analysis. By definition,
(cid:32) n (cid:33)
F n(cid:0) Q+ γ(F(cid:98))(cid:1) = Q+
γ
n1 (cid:88) δ
Fn(xi)
.
i=1
By absolute continuity, {F (x )}n are i.i.d. U(0,1) random variables. Hence, the distribution
i i i=1
of Q+(1 (cid:80)n δ ) does not depend on {F }n . It can be viewed as a homogenized version of
γ n i=1 Fi(xi) i i=1
F n(cid:0) Q+ γ(F(cid:98))(cid:1). For any δ ∈ (0,1), Lemma B.3 implies that with probability at least 1−δ,
(cid:32) (cid:18) n (cid:19) (cid:26) (cid:27)(cid:33)
1 (cid:88) R(n,ε,γ,δ)
P Q+ δ −γ ≤ min ≥ 1−δ. (B.1)
γ n Fi(xi) ε>0 1−ε
i=1
Since |F (x )−F (x )| ≤ ∥F −F ∥ ≤ ϕ a.s., then applying Lemma B.1 to {F (x )}n and
n i i i n i ∞ n i i=1
{F (x )}n gives
i i i=1
(cid:12) (cid:18) n (cid:19)(cid:12)
(cid:12) (cid:12) (cid:12)F n(cid:0) Q+ γ(F(cid:98))(cid:1) −Q+ γ n1 (cid:88) δ Fi(xi) (cid:12) (cid:12)
(cid:12)
≤ ϕ, a.s.
i=1
Combining this with (B.1) completes the proof.
C Numerical experiments: additional details
C.1 A simplified algorithm for quantile estimation
Algorithm 4 Adaptive rolling window for quantile estimation (experiment version)
Input: Datasets {D }t with D = {u }Bj , miscoverage level α and hyperparameter δ′.
j j=1 j j,i i=1
Let m = ⌈log t⌉+1. Let k = 2s−1 for s ∈ [m−1] and k = t.
2 s m
for s = 1,...,m do
Compute q according to (3.1), and
(cid:98)t,ks
(cid:115)
α(1−α)log(1/δ) 1
ψ(t,k ,δ) = + ,
s
B B
t,ks t,ks
(cid:18) (cid:19)
5 (cid:12) (cid:12)
ϕ(cid:98)(t,k s,δ) =
12
m i∈a [kx
]
(cid:12)F(cid:98)t,ki(q (cid:98)t,ks)−(1−α)(cid:12)−[ψ(t,k s,δ)+ψ(t,k i,δ)] .
+
end for
Choose any s (cid:98)∈ argmin s∈[m]{ϕ(cid:98)(t,k s,δ)+ψ(t,k s,δ)}.
return q .
(cid:98)t,k
s(cid:98)
C.2 Details of the synthetic data experiment
We give an outline of how the true mean sequence {µ }T in the right panel of Figure 1 and the
t t=1
sequence {β }T for non-stationary linear regression are generated.
t t=1
We will construct a base sequence {u }T , and set µ = 5u and β = 2u . The base sequence
t t=1 t t t t
{u }T consists of 4 parts, each representing a distribution drift pattern. In the first part, the
t t=1
20sequence experiences large shifts. Then, it switches to a sinusoidal pattern. Following that, the
environment stays stationary for some time. Finally, the sequence drifts randomly at every period,
where the drift sizes are independently sampled from {1,−1} with equal probability and scaled with
a constant.
The function for generating the base sequence takes in 3 parameters N, n and seed, where N is the
total number of periods, n is the parameter determining the splitting points of the 4 parts, and seed
is the random seed used for code reproducibility. In our experiment, we set N = 100, n = 2 and
seed = 2024. The exact function can be found in our code at https://github.com/eliselyhan/
predictive-inference.
C.3 Details of the real data experiment
Figure 4 shows the weekly average of logarithmic prices, illustrating the distribution shift over time.
14.25
14.00
13.75
13.50
13.25
13.00
0 200 400 600 800
Time
Figure 4: Weekly average of logarithmic prices.
We focus on transactions of studios and apartments with 1 to 4 bedrooms, between Jan-
uary 1st, 2008 and December 31st, 2023. We import variables instance_date (transaction date),
area_name_en (English name of the area where the apartment is located in), rooms_en (number
of bedrooms), has_parking (whether or not the apartment has a parking spot), procedure_area
(area in the apartment), actual_worth (final price) from the data.
We use instance_date (transaction date) to construct weekly datasets. The target for prediction
is the logarithmic of actual_worth. The predictors are area_name_en, rooms_en, has_parking and
procedure_area. area_name_en has 58 possible values and encoded as an integer variable.
We remove a sample if its actual_worth or procedure_area is among the largest or smallest
2.5% of the population, whichever is true. After the procedure, 91.6% of the data remain.
We run XGBoost regression using the function XGBRegressor in the Python library xgboost.
We set random_state to be our random seed and do not change any other default parameters.
References
Angelopoulos, A., Candès, E. and Tibshirani, R. J. (2023). Conformal PID control for time
series prediction. In Advances in Neural Information Processing Systems (A. Oh, T. Naumann,
A. Globerson, K. Saenko, M. Hardt and S. Levine, eds.), vol. 36. Curran Associates, Inc.
URL https://proceedings.neurips.cc/paper_files/paper/2023/file/
47f2fad8c1111d07f83c91be7870f8db-Paper-Conference.pdf
21
ksir
ssecxEAngelopoulos, A. N., Barber, R. F. and Bates, S. (2024). Online conformal prediction with
decaying step sizes. In Forty-first International Conference on Machine Learning.
Angelopoulos, A. N. and Bates, S. (2023). Conformal prediction: A gentle introduction.
Foundations and Trends® in Machine Learning 16 494–591.
Barber, R. F., Candès, E. J., Ramdas, A. and Tibshirani, R. J. (2021). Predictive inference
with the jackknife+. The Annals of Statistics 49 486 – 507.
URL https://doi.org/10.1214/20-AOS1965
Barber, R. F., Candès, E. J., Ramdas, A. and Tibshirani, R. J. (2023). Conformal prediction
beyond exchangeability. The Annals of Statistics 51 816 – 845.
URL https://doi.org/10.1214/23-AOS2276
Bates, S., Angelopoulos, A., Lei, L., Malik, J. and Jordan, M. (2021). Distribution-free,
risk-controlling prediction sets. J. ACM 68.
URL https://doi.org/10.1145/3478535
Besbes, O., Gur, Y. and Zeevi, A. (2015). Non-stationary stochastic optimization. Operations
Research 63 1227–1244.
URL https://doi.org/10.1287/opre.2015.1408
Bhattacharyya, A. and Barber, R. F. (2024). Group-weighted conformal prediction. arXiv
preprint arXiv:2401.17452 .
Bian, M. and Barber, R. F. (2023). Training-conditional coverage for distribution-free predictive
inference. Electronic Journal of Statistics 17 2044 – 2066.
URL https://doi.org/10.1214/23-EJS2145
Bifet, A. and Gavaldà, R. (2007). Learning from time-changing data with adaptive windowing.
In Proceedings of the 2007 SIAM International Conference on Data Mining. SIAM.
URL https://epubs.siam.org/doi/abs/10.1137/1.9781611972771.42
Boucheron, S.,Lugosi, G.andMassart, P.(2013).ConcentrationInequalities: ANonasymptotic
Theory of Independence. Oxford University Press.
URL https://doi.org/10.1093/acprof:oso/9780199535255.001.0001
Chen, T. and Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In Proceedings of
the 22nd ACM SIGKDD international conference on knowledge discovery and data mining.
Fannjiang, C., Bates, S., Angelopoulos, A. N., Listgarten, J. and Jordan, M. I. (2022).
Conformal prediction under feedback covariate shift for biomolecular design. Proceedings of the
National Academy of Sciences 119 e2204569119.
URL https://www.pnas.org/doi/abs/10.1073/pnas.2204569119
Gibbs, I. and Candès, E. (2021). Adaptive conformal inference under distribution shift. In
Advances in Neural Information Processing Systems (M. Ranzato, A. Beygelzimer, Y. Dauphin,
P. Liang and J. W. Vaughan, eds.), vol. 34. Curran Associates, Inc.
URL https://proceedings.neurips.cc/paper_files/paper/2021/file/
0d441de75945e5acbc865406fc9a2559-Paper.pdf
22Gibbs, I. and Candès, E. J. (2024). Conformal inference for online prediction with arbitrary
distribution shifts. Journal of Machine Learning Research 25 1–36.
URL http://jmlr.org/papers/v25/22-1218.html
Goldenshluger, A. and Lepski, O. (2008). Universal pointwise selection rule in multivariate
function estimation. Bernoulli 14 1150 – 1190.
URL https://doi.org/10.3150/08-BEJ144
Han, E., Huang, C. and Wang, K. (2024). Model assessment and selection under temporal
distribution shift. In Forty-first International Conference on Machine Learning.
Hanneke, S., Kanade, V. and Yang, L. (2015). Learning with a drifting target concept. In
Algorithmic Learning Theory: 26th International Conference, ALT 2015, Banff, AB, Canada,
October 4-6, 2015, Proceedings 26. Springer.
Hazan, E. and Seshadhri, C. (2009). Efficient learning algorithms for changing environments.
In Proceedings of the 26th Annual International Conference on Machine Learning. ICML ’09,
Association for Computing Machinery, New York, NY, USA.
URL https://doi.org/10.1145/1553374.1553425
Huang, C. and Wang, K. (2023). A stability principle for learning under non-stationarity. arXiv
preprint arXiv:2310.18304 .
Kivaranovic, D., Johnson, K. D. and Leeb, H. (2020). Adaptive, distribution-free prediction
intervals for deep networks. In Proceedings of the Twenty Third International Conference on
Artificial Intelligence and Statistics (S. Chiappa and R. Calandra, eds.), vol. 108 of Proceedings of
Machine Learning Research. PMLR.
URL https://proceedings.mlr.press/v108/kivaranovic20a.html
Lee, B. and Matni, N. (2024). Single trajectory conformal prediction. arXiv preprint
arXiv:2406.01570 .
Lei, J., G’Snell, M., Rinaldo, A., Tibshirani, R. J. and Wasserman, L. (2018). Distribution-
free predictive inference for regression. Journal of the American Statistical Association 113
1094–1111.
URL https://doi.org/10.1080/01621459.2017.1307116
Lei, L.andCandès, E. J.(2021). ConformalInferenceofCounterfactualsandIndividualTreatment
Effects. Journal of the Royal Statistical Society Series B: Statistical Methodology 83 911–938.
URL https://doi.org/10.1111/rssb.12445
Lin, Z., Trivedi, S. and Sun, J. (2022). Conformal prediction with temporal quantile adjustments.
In Advances in Neural Information Processing Systems (S. Koyejo, S. Mohamed, A. Agarwal,
D. Belgrave, K. Cho and A. Oh, eds.), vol. 35. Curran Associates, Inc.
URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
c8d2860e1b51a1ffadc7ed0a06f8d8f5-Paper-Conference.pdf
Mazzetto, A. and Upfal, E. (2023). An adaptive algorithm for learning with unknown
distribution drift. In Advances in Neural Information Processing Systems (A. Oh, T. Naumann,
A. Globerson, K. Saenko, M. Hardt and S. Levine, eds.), vol. 36. Curran Associates, Inc.
URL https://proceedings.neurips.cc/paper_files/paper/2023/file/
1fe6f635fe265292aba3987b5123ae3d-Paper-Conference.pdf
23Mohri, M. and Muñoz Medina, A. (2012). New analysis and algorithm for learning with
drifting distributions. In Algorithmic Learning Theory (N. H. Bshouty, G. Stoltz, N. Vayatis and
T. Zeugmann, eds.). Springer Berlin Heidelberg, Berlin, Heidelberg.
Papadopoulos, H., Proedrou, K., Vovk, V. and Gammerman, A. (2002). Inductive confidence
machines for regression. In Machine Learning: ECML 2002 (T. Elomaa, H. Mannila and
H. Toivonen, eds.). Springer Berlin Heidelberg, Berlin, Heidelberg.
Park, S., Dobriban, E., Lee, I. and Bastani, O. (2022). PAC prediction sets under covariate
shift. In International Conference on Learning Representations.
URL https://openreview.net/forum?id=DhP9L8vIyLc
Park, S., Li, S., Lee, I. and Bastani, O. (2021). PAC confidence predictions for deep neural
network classifiers. In International Conference on Learning Representations.
URL https://openreview.net/forum?id=Qk-Wq5AIjpq
Podkopaev, A. and Ramdas, A. (2021). Distribution-free uncertainty quantification for classifica-
tion under label shift. In Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial
Intelligence (C. de Campos and M. H. Maathuis, eds.), vol. 161 of Proceedings of Machine Learning
Research. PMLR.
URL https://proceedings.mlr.press/v161/podkopaev21a.html
Pournaderi, M. and Xiang, Y. (2024). Training-conditional coverage bounds under covariate
shift. arXiv preprint arXiv:2405.16594 .
Prinster, D., Liu, A. and Saria, S. (2022). JAWS: Auditing predictive uncertainty under
covariate shift. In Advances in Neural Information Processing Systems (S. Koyejo, S. Mohamed,
A. Agarwal, D. Belgrave, K. Cho and A. Oh, eds.), vol. 35. Curran Associates, Inc.
URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
e944bacecce6b06374ac39b260348db0-Paper-Conference.pdf
Qiu, H., Dobriban, E. and Tchetgen Tchetgen, E. (2023). Prediction sets adaptive to
unknown covariate shift. Journal of the Royal Statistical Society Series B: Statistical Methodology
85 1680–1705.
URL https://doi.org/10.1093/jrsssb/qkad069
Shafer, G. and Vovk, V. (2008). A tutorial on conformal prediction. Journal of Machine Learning
Research 9 371–421.
URL http://jmlr.org/papers/v9/shafer08a.html
Si, W., Park, S., Lee, I., Dobriban, E. and Bastani, O. (2024). PAC prediction sets under
label shift. In The Twelfth International Conference on Learning Representations.
URL https://openreview.net/forum?id=4vPVBh3fhz
Tibshirani, R. J., Foygel Barber, R., Candes, E. and Ramdas, A. (2019). Conformal
prediction under covariate shift. Advances in neural information processing systems 32.
Vovk, V. (2012). Conditional validity of inductive conformal predictors. In Proceedings of the Asian
Conference on Machine Learning (S. C. H. Hoi and W. Buntine, eds.), vol. 25 of Proceedings of
Machine Learning Research. PMLR, Singapore Management University, Singapore.
URL https://proceedings.mlr.press/v25/vovk12.html
24Vovk, V., Gammerman, A. and Shafer, G. (2005). Algorithmic learning in a random world,
vol. 29. Springer.
Xu, C. and Xie, Y. (2023). Conformal prediction for time series. IEEE Transactions on Pattern
Analysis & Machine Intelligence 45 11575–11587.
Yang, Y. and Kuchibhotla, A. K. (2024). Selection and aggregation of conformal prediction
sets. Journal of the American Statistical Association 0 1–25.
URL https://doi.org/10.1080/01621459.2024.2344700
Yang, Y.,Kuchibhotla, A. K.andTchetgen Tchetgen, E.(2024a). Doublyrobustcalibration
of prediction sets under covariate shift. Journal of the Royal Statistical Society Series B: Statistical
Methodology qkae009.
URL https://doi.org/10.1093/jrsssb/qkae009
Yang, Z., Candès, E. and Lei, L. (2024b). Bellman conformal inference: Calibrating prediction
intervals for time series. arXiv preprint arXiv:2402.05203 .
Ying, M., Guo, W., Khamaru, K. and Hung, Y. (2024). Informativeness of weighted conformal
prediction. arXiv preprint arXiv:2405.06479 .
Zaffran, M., Feron, O., Goude, Y., Josse, J. and Dieuleveut, A. (2022). Adaptive
conformal predictions for time series. In Proceedings of the 39th International Conference on
Machine Learning (K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu and S. Sabato, eds.),
vol. 162 of Proceedings of Machine Learning Research. PMLR.
URL https://proceedings.mlr.press/v162/zaffran22a.html
25