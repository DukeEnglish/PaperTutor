Adaptive Opponent Policy Detection in
Multi-Agent MDPs: Real-Time Strategy Switch
Identification Using Running Error Estimation
Mohidul Haque Mridul1, Mohammad Foysal Khan1,
Redwan Ahmed Rizvee1*, Md Mosaddek Khan1*
1Department of Computer Science and Engineering, University of
Dhaka, Dhaka, Bangladesh.
*Corresponding author(s). E-mail(s): rizvee@cse.du.ac.bd;
mosaddek@du.ac.bd;
Contributing authors: mohidulhaque-2018125308@cs.du.ac.bd;
mohammadfoysal-2018225299@cs.du.ac.bd;
Abstract
In Multi-agent Reinforcement Learning (MARL), accurately perceiving oppo-
nents’ strategies is essential for both cooperative and adversarial contexts,
particularly within dynamic environments. While Proximal Policy Optimiza-
tion (PPO) and related algorithms such as Actor-Critic with Experience Replay
(ACER), Trust Region Policy Optimization (TRPO), and Deep Deterministic
Policy Gradient (DDPG) perform well in single-agent, stationary environments,
theysufferfromhighvarianceinMARLduetonon-stationaryandhiddenpolicies
of opponents, leading to diminished reward performance. Additionally, existing
methods in MARL face significant challenges, including the need for inter-agent
communication, reliance on explicit reward information, high computational
demands, and sampling inefficiencies. These issues render them less effective in
continuous environments where opponents may abruptly change their policies
without prior notice. Against this background, we present OPS-DeMo (Online
PolicySwitch-DetectionModel),anonlinealgorithmthatemploysdynamicerror
decaytodetectchangesinopponents’policies. OPS-DeMocontinuouslyupdates
its beliefs using an Assumed Opponent Policy (AOP) Bank and selects corre-
sponding responses from a pre-trained Response Policy Bank. Each response
policy is trained against consistently strategizing opponents, reducing training
uncertaintyand enablingtheeffectiveuseofalgorithms likePPO in multi-agent
environments. Comparative assessments show that our approach outperforms
1
4202
nuJ
01
]IA.sc[
1v00560.6042:viXraPPO-trained models in dynamic scenarios like the Predator-Prey setting, pro-
viding greater robustness to sudden policy shifts and enabling more informed
decision-making through precise opponent policy insights.
Keywords:OnlineAlgorithm,DynamicEnvironment,Collaborative-Competitive
Scenario,DynamicDecay
1 Introduction
In real-worldscenarios,artificialagents face significant challengesin navigatinginter-
actions among multiple entities, a task that humans manage with apparent ease [1].
Examples include soccer players who must predict the movements of teammates,
each with varying roles and skill sets, as well as opponents, and autonomous cars
that need to anticipate the diverse behaviors of other vehicles on the road [2]. These
encounters involve distinct behaviors, necessitating different policies for optimal out-
comes. Termed opponents, these entities create a non-stationary environment from a
decentralized agent’s perspective.
In normal situations, it is common for the strategies of opponents or interacting
entities to be hidden. Despite the potential benefits of exchanging policies, frequent
communication is not always feasible, and opponents may be reluctant to share their
strategies,hinderingdirectlearningopportunities[3].Consequently,thereisaneedto
update our beliefs about opponents’ policies by observing their actions in real-time.
However, relying solely on past observations can be problematic, as these may not
accurately reflect the current policy owing to random fluctuations or adaptations in
theopponent’slearningprocess.Byplacinggreateremphasisonrecentactions,wecan
gradually converge toward understanding their true current policy. However, abrupt
shiftsinopponentpoliciescanoccur,leadingtochallengesinaccuratelytrackingtheir
strategies. Failing to detect these sudden changes could slow down learning progress
andhindertheagent’sabilitytoadapteffectively.Anexamplecanbeobservedinana-
lyzing the financial market, where, the policies of the players are often not expressed
explicitly, and based on various factors from small to large, the environment of the
market can abruptly shift.
Interacting effectively with other agents requires understanding their actions and
decisions, a process aided by opponent modeling - constructing and using models of
opponents’ behavior. Ideally, opponent models help extract opponent policies, aiding
in devising strategiesexploiting opponents’weaknesses.However,due to the dynamic
nature of agents’ behaviors, such modeling is challenging. Non-stationary behaviors
require continuous updating of learned opponent models. For example, in soccer, a
defender can become an attacker during a counter-attack, altering their strategy sig-
nificantly. Similarly, autonomous agents may adjust their policies based on changing
beliefs about the environment.
To address the aforementioned concerns, Reinforcement learning, a paradigm for
policylearninghasbeenusedthatfocusesonagents’maximizinglong-termcumulative
2reward through trial-and-error with the environment [4]. While effective in single-
agent environments, it struggles in multi-agent settings due to the non-stationarity
of agents. Common reinforcement learning algorithms such as DQN, DDPG, AAC,
andPPOareoptimized to excelin achievinghighrewardswithin static environments
known as Markov Decision Processes (MDP). However,when applied to Multi-Agent
MDPs, where multiple agents interact, these algorithms encounter challenges due to
increased variance caused by gradual or sudden changes in other agents’ policies.
Several algorithms tailored for multi-agent scenarios, such as BPR+[5], DPN-
BPR+[6], LOLA[7], and meta-MAPG[8], address the challenges of non-stationary
environments through various strategies. LOLA aims to influence opponents’ behav-
ior,whileMeta-MAPGfocusesonmeta-learningenvironmentaldynamics.BPR+and
DPN-BPR+ leverage previously learned response policies, assuming infrequent non-
stationarity in opponents, akin to occasional shifts between stationary opponents.
However,LOLAandMeta-MAPGareinadequateforsuddenshiftsinopponentpolicy.
In contrast, BPR+ and DPN-BPR+ rely on reward signals to detect opponent pol-
icy shifts, limiting their effectiveness to episodic environments with consistent reward
data, posing challenges in continuous environments for accurately detecting sudden
policy switches.
Theneedforaneffectivemethodtodetectpolicyswitchesbasedsolelyonobserved
actions is critical, particularly when traditional statistical measures fall short in sce-
narioswith brief opponenttrajectories.SAM (Switching Agent Model) addressesthis
by estimating the running error of the assumed policy, preventing an indefinite error
increase when the agent follows the assumed policy. However, SAM is tailored for
deterministicactions,oftenpairedwithDDPG[9],andlacksdetailedformulationsfor
running error decay.
BuildingupontheSAMframework,ourapproachextendsitsapplicabilitytoProx-
imal Policy Optimization (PPO) and similar reinforcement-learning algorithms. We
rectifythegapsinSAMbydetailingthedecaycalculationsandenhancingthemethod
forselectingresponsepolicies.Specifically,weproposeamethodtodetectpolicyshifts
in real-time by reusing response policies on a fixed set of opponent policies, assum-
ing occasional switches between them. This method relies solely on observed actions,
employing running error estimation and dynamic error decay adaptable to stochastic
policies. Upon detecting a switch, our algorithm swiftly adjusts the response policy
to align with the most probable opponent policy. To accommodate the resource con-
straints typical in edge devices, we optimize our policy switch detection mechanism
for efficiency, allowing it to operate within strict resource limitations and process
observations on the fly, without the need for storing them.
Tosummarizethemaincontributionofthiswork,wehighlightthefollowingpoints,
1. We introduce a running error estimation metric to assess an agent’s compli-
ance with a stochastic policy, utilizing only observed state-action pairs from the
trajectory. This metric operates online and can be continually updated as new
observations are processed, without the need for storage.
2. Additionally, we propose an online algorithm that utilizes the running error esti-
mation metric to detect the policy switch of an opponent and adapt the response
policy accordingly.
33. Through conducting a rigorous comparative analysis between our proposed algo-
rithm and the current state-of-the-art algorithms, we evaluate the merit of the
proposals and present them.
In Section 2, we present a summary of our related works.In Section 3, we present
our proposals and an experimental evaluation regarding the merit of the work is
presented in Section 4. The article ends with some concluding marks in Section 5.
2 Related Works
In this section, we explore topics closely relatedto our work,beginning with a funda-
mental concept: the Markov Decision Process (MDP), which models decision-making
under uncertainty by defining states, actions, transitionprobabilities, and rewards.It
assumes the Markov property, where future states depend only on the current state
and action. The goal is to find a policy that maximizes expected cumulative rewards.
In the reinforcementlearning paradigm,MDPs are often modeled under single agent-
basedframework.MarkovGames are anextensionof MDP whichalso sharea similar
set of terminologies and objectives, such as states, actions, transition probabilities,
and rewards in conjunction with maximizing the cumulative reward [10].
In MarkovGames, the concept of multiple agents operating within the same envi-
ronment is introduced, each agent focused on maximizing its own reward objective.
These kind of games are effectively dealt with Multi-Agent Reinforcement Learning
(MARL) approaches [11]. MARL extends single-agent reinforcement learning tech-
niques to handle interactions among multiple agents. Agents learn individual policies
based on observations and joint rewards. Common MARL algorithms include Inde-
pendent Q-Learning [12], Q-Learning with Experience Replay [13], and Multi-Agent
Deep Deterministic Policy Gradient (MADDPG) [14]. MARL enables agents to learn
effective strategies in dynamic environments where their actions affect the rewards
and states of other agents. It facilitates the exploration of cooperative, competitive
behaviors, and emergent phenomena in multi-agent systems.
In this context, Multi-Agent Markov Decision Processes (MMDPs) serve as a
natural extension of both Markov Games and Multi-Agent Reinforcement Learning
(MARL).Byincorporatingtheconceptsofstates,actions,transitionprobabilities,and
rewards, MMDPs provide a comprehensive framework for modeling decision-making
in multi-agent environments. Q-Learning [12, 13], (MADDPG) [14], etc. falls under
thewideumbrellaofsolutionsforMMDPs.ModelingthroughMMDPsprovideinvalu-
able insights into the dynamics of interactions among autonomous agents, enabling
the developmentof intelligent and adaptive systems capable of functioning effectively
in real-worldscenarios.
Inreinforcementlearning,algorithmslikeREINFORCE[15],DQN[16],AAC[17],
DDPG [9],SAC [18],TRPO[19],PPO[20],etc.,arewidely studiedfortasks utilizing
techniques such as value iteration, policy iteration, Q-learning, and policy gradient
methods. However, these algorithms are mainly tailored for stationary environments
with single agents, often represented by MDPs. Real-worldscenarios (MMDPs) often
involvemultipleinteractingagents,leadingtonon-stationarityandincreasedvariance,
posing challenges for model convergence and achieving optimal results.
4PPO [20], a policy gradient method, uses stochastic gradient ascent to optimize a
surrogateobjectivefunctionwhile iterativelycollectingsampledatafromtheenviron-
ment. Empirically,PPO performs comparably to or better than other policy gradient
algorithms such as AAC and TRPO in continuous control environments. However,
PPO does not address variance introduced by actions of other agents within the
same environment, as it is designed solely for single-agent scenarios. Numerous stud-
ies havefocused on explicitly modeling the behaviors,goals,and beliefs ofinteracting
agents(referredtoasopponents)toaccommodatetheirdynamicseffectively[21].This
approachdeviates from treating agents solely as elements of the environment, aiming
to achieve a more adaptable and robust policy by incorporating explicit modeling of
agents.
It is crucial to recognize that opponent behavior may be non-stationary due
to factors such as opponents actively learning or modeling our agents’ behavior.
Many studies overlook this concept, failing to adequately address the potential non-
stationarity of opponent behavior. Neglecting this aspect can limit the effectiveness
of approaches in capturing the true complexity of interactive dynamics. Several algo-
rithms, such as RL-CD [22] and QCD [23], are designed to handle non-stationary
opponentsbypositingthattheyperiodicallyswitchbetweenmultiplestationarystrate-
gies.However,RL-CDandQCDprimarilyfocusonaccuratelydetectingswitchpoints
rather than explicitly identifying opponent strategies. Nonetheless, identifying the
opponent’s strategy is crucial in addition to detecting switch points.
The algorithms mentioned treat the non-stationary opponent as part of the envi-
ronment and do not explicitly learn an opponent model. Although this simplifies
the algorithm, it can lead to reduced performance. To address this limitation, other
algorithms such as MDP-CL [24], DriftER [25], and BPR+ [5] incorporate explicit
opponent modeling. MDP-CL dynamically explores opponents during runtime with-
out prior models, switching strategies by comparing modeled policies with observed
behavior. However, it does not store the modeled policy after the switch. DriftER
incorporates drift for switch detection and uses the R-max algorithm instead of ran-
domexploration.BPR+avoidstheneedforanMDPandemployscumulativerewards
andBayes’ruletodetectswitchpointsandopponentstrategies.Itstoreslearnedpoli-
cies in memory. These methods assume opponents will maintain a stationary policy
forseveralepisodes,as frequentswitchingwouldhinder opponentmodeling andmake
the opponent’s strategy appear random.
DPN-BPR+ [6] introduces a methodology for detecting policy shifts using both
rewardsignalsandopponentbehaviors,utilizingapolicybanktogenerateappropriate
responses.Its applicabilityislimited toepisodicenvironments,lacksguidanceoncon-
structing the policy bank using unsupervisedmethods anddoes not addressreal-time
improvementof responses againstknownpolicies. LOLA [7] assumes opponents to be
naively learning and aims to shape their learning to the agent’s advantage, although
opponents may resist such shaping. Meta-PG [26] and meta-MAPG [8] approach
continuous adapt through meta-learning, with meta-MAPG combining LOLA with
meta-PGformultiple agents.M-FOS[27]learnsmeta-policiesforlong-termopponent
shaping and exploitation of LOLA, while MBOM [28] employs recursive imagination
5andBayesianmixingtogenerateresponses.However,theseapproachesdonotaccount
for abrupt policy shifts by opponents, which can be challenging to predict.
RecentadvancementsinDeepReinforcementLearning(DRL)haveaddressednon-
stationarity in multi-agent systems. Algorithms like DRON [29], MADDPG [30], and
DPIQN[31]handlethisissuebylearninggeneralizedpoliciesfrompredefinedfeatures
or observed opponent behaviors. DPIQN learns distinct features for opponent poli-
cies but trains a generalizedQ-network for execution instead of reusing advantageous
response policies.SAM [32] models opponentpolicies to adaptresponse policies effec-
tively, integrating DDPG with opponent modeling. It assesses opponent compliance
using a running error metric and infers policy switches when the error surpasses a
threshold. However, SAM lacks explicit definitions for determining the next response
strategy and the decay mechanism for the running error.
In summary, to address the aforementioned limitations observed in different liter-
ature, in this work, we propose an online continuous running error estimation metric
utilizing only observedstate-actionpairsand anonline algorithmto detect the policy
switch of the opponents to adapt responses.
3 The Online Policy Switch Detection Model
(OPS-DeMo)
In this section, we present our proposals to address the issues discussed in Section 2.
First,weintroduceanewmetrictomeasurehowwellanagentcomplieswithpolicies,
based on its recent actions. This metric can be used on the fly and is detailed in
Section 3.1. Next, we describe the architecture of our proposed model in Section 3.2.
Finally, we introduce an algorithm in Section 3.3 that is specifically tailored to adapt
to changes in the opponent’s behavior.
3.1 Metric to Measure Policy Compliance
Detecting compliance in policies with nearly uniform action probability distribu-
tions across Markov states poses significant challenges, particularly in environments
characterizedby shorttrajectories.Traditionalmethods, which rely on frequency dis-
tribution, often fall short as they require frequent revisits to states—a condition that
isimpracticalwithlimiteddataavailability.Amoreviableapproachentailscomparing
observed actions against their expected probabilities and calculating an error met-
ric in real-time. Should this error exceed a predefined threshold, it suggests potential
deviationfromthepolicy.Nonetheless,tomitigateerroraccumulationstemmingfrom
inherent randomness, implementing a decay mechanism is essential.
This decay mechanism should consider both the expected error when the agent
adheres to the policy and when it deviates from it. By incorporating this decay, the
method aims to prevent error escalation indefinitely, especially in scenarios where
the agent genuinely follows a stochastic policy with inherent sampling errors.Let the
assumedpolicyforanMDPwithadiscreteactionspacebeπ.InagivenMarkovstate
s, the policy π canbe written asper the Equation1.Here p denotes the probability
ai
of choosing action a from state s.
i
6π(s)= p ,p ,p ,...p ,...,p (1)
a1 a2 a3 ai an
Similarly, the observed frequency of actions in a given Markov state s can be
(cid:2) (cid:3)
written as per Equation 2. Here, in Equation 2, f is set to 1 when the action a is
ai i
chosen. Otherwise, it will be set to 0.
f (s)= f ,f ,f ...f ,...,f (2)
o a1 a2 a3 ai an
Now,basedonEquations1and2,the observederrorin state s fromassumingthe
(cid:2) (cid:3)
agent follows policy π can be written as,
n
1
e o(π,s)=
2
|π(s)−f o(s)| ak (3)
k=1
X
Now, we state some lemmas to discuss some features associated with Equation 3.
Lemma 1. Consider a timestep t within the context of a MDP with a discrete action
space of n actions, wherein an agent follows a policy π and selects an action a from
i
a Markovian state s. Within this framework, the observed error at t can be formulated
as (1−p ), where p signifies the probability of opting for action a by the stochastic
ai ai i
policy π.
Proof. Let us examine the observed frequency of each action, noting that all actions
except a havea frequency of 0.Consequently,the observederrorcanbe expressedas
i
follows:
1
e (π,s)= [|0−p |+|0−p |+|0−p |+...|1−p |+|0−p |+...+|0−p |]
o
2
a1 a2 a3 ai ai+1 an
(From Equation 3)
1
= [p +p +p +...+(1−p )+p +...p ]
2
a1 a2 a3 ai ai+1 an
(Because, any 0≤p ≤1)
ai
1
= [(1−p )+(1−p )]
2
ai ai
n n
(Because, any p =1⇒p =1− p )
aj ai ak
j=1 k=1,k6=i
X X
=(1−p )
ai
(4)
Since observed error e (π,s) has occurred due to the selected action a, in this
o
discussion,wealsousee (π,s,a)todenotethesimilarmeaningofobservederrorfrom
o
state s due to an action a under policy π.
Lemma 2. In a MDP, with a discrete action space of n actions, consider a time step
t where an agent follows a policy π, and the system is in a Markovian state s. Within
this framework, the expected naturally occurring error from policy π at t step can be
7expressed as n p (1−p ). Here, p represents the probability of selecting action
j=1 aj aj aj
a following the stochastic policy π.
j
P
Proof. The probability of selecting action a is p when the agent is following policy
i ai
π, which induces observed error (1−p ) (Lemma 1). Therefore the expected error
ai
when following policy π is:
n
e (π,s)
o
E
π
= p aj(1−p aj) (5)
! j=1
X
Lemma 3. In a MDP context, with a discrete action space of n actions, consider
a time step t where an agent follows any policy φ except a certain policy π, and
the system is in a Markovian state s. Within this framework, the expected naturally
occurring error from policy π at t can be expressed as n−1.
n
Proof. Consider a scenario where the agent deviates from policy π and instead com-
plies with analternativebut unknownpolicy φ. Inthis context,the probabilityofthe
agentselecting any actionwhile not following policy π is uniformly distributed across
all actions, although the specific distribution under policy φ remains unknown. Con-
sequently,theexpectedobservederrorfrompolicyπ,whennotfollowingpolicyπ,can
be articulated as Equation 6. Here πc denotes the set of all possible policies for the
particular problem.
n
e (π,s) 1 n−1
o
E
φ∈πc
= n(1−p aj)=
n
(6)
(cid:18) (cid:19) j=1
X
3.2 Architecture of the Model
In the givenenvironmentmodeled by an MDP with a discrete actionspace, we adopt
a strategy of training response policies π against each of the potential opponent
i
policies Φ ∈ Π , where Π denotes the policy bank containing the various probable
i o o
opponentpolicies.Thetrainingofresponsepoliciesinvolvesemployingstate-of-the-art
learningalgorithmssuchasPPO.Followingthetrainingprocess,ouragentisprepared
for deployment within the environment. Incorporating all the ideas, we have given a
high-level overview of our proposed OPS-DeMo architecture in Fig. 1.
At any given instance when the opponent is presumed to follow a specific policy
φ and is observed selecting action a , we leverage the policy bank Π to ascertain
i j o
the probabilityofeachprobableopponentpolicychoosingtheobservedaction.Subse-
quently,wecalculatethecorrespondingobservederrorandupdateourbeliefregarding
the currentopponentpolicyutilizing a designatedalgorithm.After this beliefupdate,
weselectanappropriateresponsepolicyanddeterminetheagent’sactionbasedonthe
chosen response policy. This iterative process allows our agent to adapt dynamically
to varying opponent strategies encountered during its deployment.
8Fig. 1 ArchitectureofOPS-DeMo
3.3 Algorithm Description
We propose an algorithm (Algorithm 1) for running error estimation (lines 8-17),
policy switch detection (lines 18-19), and adapting the response policy (lines 20-21).
This algorithm utilizes the already trained opponent’s policy bank Π and the PPO-
o
trainedresponse policy bank ρ accordingto the updated beliefs about the opponent’s
current policy to maximize accumulated rewards. The algorithm uses the provided
running error estimation method to see which policy from Π more aligns with the
o
opponent’srecentactionsandthenpicksthepolicywiththelowestrunningerror,and
the appropriateresponse policy is taken to maximize rewardsin the currentscenario.
3.4 Detection of Policy Switch
Weusethe observederrorstoaccumulatearunningerrorforeachofthe policiesfrom
theopponent’spolicybankΠ (refertoAlgorithm1,lines8-24).Iftherunningerrorof
o
the currently presumed opponent policy Φ exceeds a threshold value, we assume that
the opponent has switched its policy in the meantime. However, naturally occurring
errorsmay makethe runningerrorgoindefinitely large.Therefore,adecaymethodof
running error is essential.
3.5 Error Decay
For a given Markovian state s, where the expected error when following policy Φ is
denotedas e (refer to Equation5),andthe expected errorwhen notfollowing policy
f
Φisdenotedase (refertoEquation6),thedecaybetweenthesevaluesisdefinedby
nf
Equation7. Here φc denotes the set ofall possible policies for the particularproblem.
9Algorithm 1: OPS-DeMo
Input: Observed state-action pairs, Opponent’s Probable Policy Bank Π ,
o
PPO-Trained Response Policy Bank ρ
1 Initialization:
Initialize Opponent’s Probable Policy Bank Π ;
o
2 Initialize PPO-Trained Response Policy Bank ρ;
3 Randomly choose an assumed opponent policy φassumed from Π o;
4 Initialize running errors for all opponent policies: running error(φ) ←0 for
all φ∈Π ;
o
5 Initialize threshold value for policy switch detection;
6 foreach observed state-action pair (s,a) of the opponent do
7 foreach opponent policy φ∈Π o do
8 Calculate Observed Error e o(φ,s,a) (Lemma 1), expected error e f
when following the assumed policy φ (Equation 5), and e while not
nf
following the assumed policy (Equation 6) using the provided
equations;
9 decay(s,φ)←α×e f +(1−α)×e nf;
10 running error(φ) ←running error(φ)+e o(φ,s,a)−decay(s,φ);
11 if running error(φ) <0 then
12 running error(φ) ←0;
13 end
14 if running error(φ) >threshold then
15 running error(φ) ←threshold;
16 end
17 end
18 if running error(φassumed)=threshold then
19 Detect policy switch;
20 Choose a new assumed opponent policy φassumed with minimum
running error;
21 running error(φassumed)←running error(φassumed)/2;
22 end
23 Choose an action according to response policy ρ(φassumed);
24 end
d=αe +(1−α)e
f nf
e (Φ,s) (7)
o
=αE(e (Φ,s))+(1−α)E
o φ′ ∈Φc
(cid:18) (cid:19)
In this equation, the parameter α ∈ [0,1] represents the strictness factor of the
decay. A higher value of α implies a more stringent detection model that disallows
policies similar to, but not significantly differing from, the assumed one. Conversely,
lower values of α allow for a more lenient approach. The careful choice of α is crucial
in tailoring the detection model to specific requirements.
10Thisdecaypreventstherunningerrorfromgrowingindefinitelyandgetscalculated
dynamically (refer to Algorithm 1, line 10).
3.6 Identification of the Post-Switch Policy
In order to reuse the trained response policies effectively, the identification of the
opponent’s policy after a switch becomes a critical task. We suggest maintaining a
record of running errors for all potential opponent policies. When the running error
associatedwiththepresentlyassumedpolicysurpassesapredeterminedthreshold,the
policy exhibiting the minimum current running error is designated as the switched
policy. Subsequently, the running error is halved to mitigate the occurrence of exces-
sivelyfrequentswitches(refertoAlgorithm1,line22).Thisapproachaimstoenhance
the robustness and stability of policy detection in dynamic environments.
4 Empirical Evaluation
Inthissection,weassesstheperformanceofOPS-DeMowithinaMarkovgame,Preda-
tor Prey, by comparing it against current state-of-the-art learning algorithms using
various metrics. The key metrics for this analysis include the accumulated rewards
andtheaccuracyofassumptionsabouttheopponent’spolicy.Thisassessmentaimsto
specificallyevaluatetheefficacyoftherunningerrorestimationmethodinthecontext
of frequent changes in the opponent’s policy and varying strictness levels.
Distinct from conventional learning algorithms, OPS-DeMo utilizes models that
havebeen trainedafter the initial learning phase.For the purposes of this evaluation,
we exclude active learning components, operating under the assumption of a set of
probable opponent policies that change infrequently. The response policies employed
are pre-trained using techniques such as PPO. Notably, models such as BPR+ and
DPN-BPR+ are excluded from the comparison due to their inapplicability in con-
tinuous environments. Additionally, SAM is also omitted due to its undefined decay
parameters and ambiguity in policy definition.
4.1 Implementation
The experimental setup involves a 2-predator,2-prey configurationwith fully observ-
able environmental states and actions. No direct communication between agents is
allowed. This setup accommodates diverse policies for each agent, enabling distinct
optimalresponsepoliciesforvaryingopponentpolicies.Rewardsareintentionallykept
sparse throughout the episode to minimize the information available about the oppo-
nent’spolicy.Ratherthanassumingtheoptimalityoftheopponent’sactions,theagent
focuses on identifying optimal actions based on its understanding of the opponent’s
likelybehavior.TheexperimentationisconductedonamachinewithanAppleSilicon
M2 processor and 8GB of primary memory.
4.2 Environment Setup
The setup involves a Predator Prey grid-world environment with two predators and
twoprey.Theobjectiveofthegameisforeachpredatortosimultaneouslycaptureone
11prey,withtheaimofcapturingbothpreyintheshortesttimepossible.Thisapproach
seeks to maximize the rewards earned within a single episode. Negative rewards are
incurred for each timestep where a predator fails to catch a prey or experiences colli-
sionswithotherpredators.Thissetupaddressesdualobjectives:optimizingsuccessful
captures and minimizing undesirable events.
Y
B
X
A
Fig. 2 Predator-PreyEnvironment
The game setup includes two prey, designated as ’Prey X’ and ’Prey Y’, which
move randomly throughout the environment, relying solely on observations. The first
predatorisnamed“PredatorA”,andthe secondis“PredatorB”.Similarly,the preys
are “PreyX” and“Prey Y.” PredatorB has two probable policies:chasing Prey X or
PreyY, periodically switching.PredatorAadapts to these changes,choosingoptimal
actions based on its belief.
In the training environment for both predators, the reward structure is defined:
+100 for catching both prey, -1 for each timestep without adjacent prey, and -1 for
colliding with agents. The primary objectives are to maximize Predator A’s rewards
and accurately update belief regarding Predator B’s current policy. In Fig. 2, we
present a visual representation to illustrate the described Predator Prey scenario.
124.3 Training Setup
Our training environment is a 10 × 10 predator-prey grid-world created using the
OpenAIGymlibrary[33].Inthisgrid-world,eachtrainingepisodelastsforamaximum
of 40 timesteps. To facilitate the training process, we employ the Stable-Baselines3
library[34].Specifically, we traintwopotentialpolicies for Predator B, with afocus
oneither chasingPrey A orPrey B.This training utilizes PPOalgorithmandranfor
up to 1,000,000iterations.To addressthe sparserewardissue,we introduce a penalty
based on the Manhattan distance between Predator B and its target Prey into the
environment-provided reward. Subsequently, we proceed to train a response policy
for Predator A for each of the potential policies of Predator B using the PPO
algorithm, again reaching 1,000,000iterations.
4.4 Simulation of Policy Switch
Upon deploying the trained models in the environment, we implement a periodic
policy switch for Predator B between chasing Prey X and Prey Y. Crucially, this
informationaboutPredator B’s currentpolicy is keptconcealedfromPredator A.
Predator A only has access to information about its own rewards and the chosen
actions of Predator B at each timestep. Predator B utilizes the online data to
calculatetheobservederrorandthecorrespondingdecayoftheMarkovstate,updating
the running error.The response policy of Predator B is then chosen from its policy
bank based on this information to determine its action for the next timestep.
4.5 Hyperparameters related to the Experiments
Among others, we want to focus on the following hyperparameters that we have
experimented with in our work,
1. Experiment with Different Strictness Factors:We conductexperiments with differ-
entstrictnessfactors,α∈{0.8,0.9,0.95,0.99},toassesstheirimpactonthemodel’s
performance. Data is collected on timesteps where the assumed policy aligns with
the opponent’s concealed policy.
2. Experiment with a Standalone PPO-trained Model: To compare the performance
of OPS–DeMo to PPO, we train a Predator A model with PPO. In this setup,
Predator B switches its policy every 100 timestamps, with training extending
up to 1,000,000 iterations. These trained models are then evaluated, and their
performanceiscomparedusingaccumulatedrewards.ThestandalonePPO-trained
modeldoesnothaveabelief mechanismforpredictinganopponent’sbehaviorand
only uses the environment state to determine its next action.
Now,wepresentsomeempiricalresultstoanalyzethenoveltyandefficiencyofour
solution.
4.6 Performance of Running Error Estimation
We assessed the effectiveness of OPS-DeMo’s running error estimation method when
Predator B switched its policy every n timesteps.
13Fig. 3 RunningerrorsoftwoprobablepoliciesofPredatorB,basedonobservationsfromPredator
A,withPredatorBswitchingitspolicyevery100timesteps.
In both Fig. 3 and Fig. 4, it is evident that the running error remains low when
the assumed policy is correct and stays at the threshold when the assumed policy is
incorrect. Notably, there is a rapid increase in the running error for the incorrectly
assumed policy at the points of policy switch and a relatively slower decline for the
correctpolicy. This is because the errordecayis closerto the expected observederror
when the opponent is following the Assumed Opponent Policy (AOP), the observed
error is much greater than the decay when the opponent is not following AOP. The
halving of the running error for the newly assumed policy after detecting the switch
contributestotherapidconvergenceofthecorrectassumptiontoalowrunningerror.
4.7 Impact of the Strictness Factor
Our experiment in the Predator-Prey environment involved varying the strictness
factors(α)while predatorBswitchesitspolicyevery100timestep.We examinedhow
the running errors behaved under different strictness conditions.
Fig. 5illustrates thatanincreasedstrictness factorresultsin aquickerriseinrun-
ning errors following a policy switch by the opponent. However, it also indicates that
thereductioninrunningerroraftertransitioningtothatspecificAOPismoregradual
under higher strictness conditions. This phenomenon occurs because the component
added to the running error is typically negative when the opponent adheres to AOP
but positive when the opponent deviates from AOP. The closer the decay aligns with
the expected observed error while following AOP (Equation 5), the smaller the mag-
nitude of the negative value becomes, and the larger the magnitude of the positive
value grows.
14Fig. 4 RunningerrorsoftwoprobablepoliciesofPredatorB,basedonobservationsfromPredator
A,withPredatorBswitchingitspolicyevery200timesteps.
4.8 Accuracy of Assumed Opponent Policy
In our experiment, we varied the strictness factors (α) while Predator B switched its
policy every 100 timestep. We assessed the accuracy of the AOP by calculating the
ratio of timesteps where the assumed policy matched the actual policy to the total
number of timesteps.
Fig. 6 illustrates that as we increase the strictness factor, the accuracy improves.
This suggests that the error estimation method becomes less lenient, becoming more
discerning in distinguishing between somewhat similar yet different policies. This is
because a higher strictness factor makes the running error rise faster and detects the
policy switch earlier. But in the trade-off, natural noises from the environment may
get some false positives in that case.
4.9 Comparison through Episodic Accumulated Rewards
Based on the experimental data comprising 25 runs, each lasting 1000 episodes,
discernible enhancements in accumulated rewards per episode are evident. These
improvementsstemfromenhancedcollaborativedynamicsbetweenthetwopredators.
Predator A, exhibiting swift adaptation to policy switches by Predator B, formulates
responses based on its inferred beliefs about Predator B’s current policy.
InFig. 7,weobservethatwhile the standalonePPO-trainedmodelperformscom-
mendably in most episodes, there are instances where it fails to capture both preys
withinthe definedmaximumof40timesteps.Consequently,itmissesoutonthe+100
15Fig. 5 Running errors of a probable policy of Predator B, based on observations from Predator
A. Predator B switches its policy every 100 timestep, illustrating the impact of different strictness
factorsonrunningerrors.
reward due to a lack of collaborative efforts. Conversely, OPS-DeMo, which dynam-
ically detects Predator B’s policy during runtime and adjusts its response policy
accordingly, demonstrates fewer occurrences of such failures.
Fig. 8 illustrates that OPS-DeMo achieves a substantial 49.6% improvement over
the standalone PPO-trained model in terms of Predator A’s mean episodic rewards.
This improvement is attributed to OPS-DeMo’s robustness in handling increased
variance resulting from high uncertainty about Predator B’s current policy.
Table 1 StatisticalSummaryofEpisodicAccumulated
Rewards
Algorithm Mean Standard Deviation
OPS-DeMo+PPO 89.9662 18.7922
PPO 60.1371 53.0235
The consistency observed in OPS-DeMo’s rewards, as evidenced by the lower
standard deviation in Table 1, results from increased certainty about Predator B’s
behavior. OPS-DeMo makes informed decisions based on this certainty, contrasting
with the standalone PPO-trained model, which tends to overlook recent action data
from Predator B.
5 Conclusions and Future Work
Detectingpolicyswitchesinanonstationarymultiagentenvironmentischallengingbut
advantageous.Itisdifficulttocheckcompliancewhenactiondistributionsareuniform
16Fig. 6 Accuracy of Predator B’s Assumed Opponent Policy (AOP) based on observations from
Predator A. Predator B switches its policy every 100 timestep, showing the impact of different
strictnessfactorsonaccuracy.
Fig. 7 Histogram depicting accumulated rewards per episode by Predator A while Predator B
switchesitspolicyevery100timesteps.
or data is limited. Using an error metric comparing observed and expected actions
helpsaddressthis,withadecaymechanismpreventingerrorescalation.Runningerror
calculations for probable policies helps infer switches, enabling appropriate response
policyselection.TheproposedOPS-DeMoalgorithmusesthesemethodsfordetection
and response, outperforming standalone PPO models with more consistent rewards
per episode and lower standard deviation. In the future, we are planning to work
on incorporating continuous learning for more precise opponent policy estimation,
developing a robust method for detecting opponent policies with uniform frequency
distribution of actions alongside detecting and learning unforeseen opponent policies.
17Fig. 8 Comparison of mean accumulated rewards per episode by Predator A, with Predator B
switchingitspolicyevery100timesteps,between OPS-DeMoandPPO.
References
[1] Hafez,W.:Humandigitaltwin:Enablinghuman-multismartmachinescollabora-
tion.In:Bi,Y.,Bhatia,R.,Kapoor,S.(eds.)IntelligentSystemsandApplications,
pp. 981–993.Springer, Cham (2020)
[2] Schwarting, W., Pierson, A., Alonso-Mora, J., Karaman, S., Rus, D.: Social
behavior for autonomous vehicles. Proceedings of the National Academy of
Sciences 116(50), 24972–24978(2019) https://doi.org/10.1073/pnas.1820676116
https://www.pnas.org/doi/pdf/10.1073/pnas.1820676116
[3] Chen, H., Huang, J., Liu, Q., Wang, C., Deng, H.: Detecting and tracing multi-
strategic agents with opponent modelling and bayesian policy reuse, pp. 1098–
1103 (2020)
[4] Andrew, A.M.: Reinforcement learning: An introduction by richard s. sutton
and andrew g. barto, adaptive computation and machine learning series, mit
press (bradfordbook), cambridge,mass., 1998,xviii + 322pp, isbn 0-262-19398-
1, (hardback, £31.95). Robotica 17(2), 229–235 (1999) https://doi.org/10.1017/
S0263574799211174
[5] Hernandez-Leal, P., Rosman, B., Taylor, M.E., Sucar, L.E., Cote, E.: A
bayesianapproachforlearningandtrackingswitching,non-stationaryopponents:
(extended abstract). In: Proceedings of the 2016 International Conference on
Autonomous Agents & Multiagent Systems. AAMAS ’16, pp. 1315–1316. Inter-
nationalFoundation for Autonomous Agents and Multiagent Systems, Richland,
SC (2016)
[6] ZHENG, Y., Meng, Z., Hao, J., Zhang, Z., Yang, T., Fan, C.: A deep bayesian
policy reuse approach against non-stationary agents. In: Bengio, S., Wal-
lach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R. (eds.)
18Advances in Neural Information Processing Systems, vol. 31. Curran Asso-
ciates, Inc. (2018). https://proceedings.neurips.cc/paper files/paper/2018/file/
85422afb467e9456013a2a51d4dff702-Paper.pdf
[7] Foerster,J.N.,Chen,R.Y.,Al-Shedivat,M.,Whiteson,S.,Abbeel,P.,Mordatch,
I.: Learning with Opponent-Learning Awareness (2018)
[8] Kim, D.-K., Liu, M., Riemer, M., Sun, C., Abdulhai, M., Habibi, G., Lopez-Cot,
S., Tesauro, G., How, J.P.: A Policy Gradient Algorithm for Learning to Learn
in Multiagent Reinforcement Learning (2021)
[9] Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D.,
Wierstra, D.: Continuous control with deep reinforcement learning (2019)
[10] Shapley, L.S.: Stochastic Games. RAND Corporation, (1953)
[11] Zhang,K.,Yang,Z.,Ba¸sar,T.:Multi-AgentReinforcementLearning:ASelective
Overview of Theories and Algorithms (2021)
[12] Watkins,C.J.C.H.,Dayan,P.:Q-learning.MachineLearning8(3),279–292(1992)
https://doi.org/10.1007/BF00992698
[13] Szlak, L., Shamir, O.: Convergence Results For Q-Learning With Experience
Replay (2021)
[14] Yang,Y.,Wang,J.:Anoverviewofmulti-agentreinforcementlearningfromgame
theoretical perspective. arXiv preprint arXiv:2011.00583(2020)
[15] Williams, R.J.: Simple statistical gradient-following algorithms for connectionist
reinforcement learning. Machine Learning 8(3), 229–256 (1992) https://doi.org/
10.1007/BF00992696
[16] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D.,
Riedmiller, M.: Playing Atari with Deep Reinforcement Learning (2013)
[17] Iqbal,S.,Sha,F.:Actor-Attention-CriticforMulti-AgentReinforcementLearning
(2019)
[18] Haarnoja, T., Zhou, A., Abbeel, P., Levine, S.: Soft Actor-Critic: Off-Policy
MaximumEntropyDeepReinforcementLearningwithaStochasticActor(2018)
[19] Schulman,J.,Levine,S.,Moritz,P.,Jordan,M.I.,Abbeel,P.:TrustRegionPolicy
Optimization (2017)
[20] Schulman,J.,Wolski,F.,Dhariwal,P.,Radford,A.,Klimov,O.:ProximalPolicy
Optimization Algorithms (2017)
19[21] Yu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen, A., Wu, Y.: The sur-
prisingeffectivenessofppoincooperativemulti-agentgames.AdvancesinNeural
Information Processing Systems 35, 24611–24624(2022)
[22] Laskin,M.,Wang,L.,Oh,J.,Parisotto,E.,Spencer,S.,Steigerwald,R.,Strouse,
D., Hansen, S., Filos, A., Brooks, E., Gazeau, M., Sahni, H., Singh, S., Mnih,
V.: In-context reinforcement learning with algorithm distillation. arXiv preprint
arXiv:2210.14215(2022)
[23] Skands, P.: Introduction to qcd. arXiv preprint arXiv:1207.2389(2012)
[24] Hernandez-Leal, P., Cote, E., Sucar, L.: A framework for learning and planning
against switching strategies in repeated games. Connection Science 26 (2014)
https://doi.org/10.1080/09540091.2014.885294
[25] Hinton, G., Vinyals, O., Dean, J.: Distilling the Knowledge in a Neural Network
(2015)
[26] Al-Shedivat, M., Bansal, T., Burda, Y., Sutskever, I., Mordatch, I., Abbeel, P.:
Continuous Adaptation via Meta-Learning in Nonstationary and Competitive
Environments (2018)
[27] Lu, C., Willi, T., Witt, C.S., Foerster,J.: Model-Free Opponent Shaping (2022)
[28] Yu,X.,Jiang,J.,Zhang,W.,Jiang,H.,Lu,Z.:Model-BasedOpponentModeling
(2022)
[29] He, H., Boyd-Graber, J., Kwok, K., au2, H.D.I.: Opponent Modeling in Deep
Reinforcement Learning (2016)
[30] Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., Mordatch, I.: Multi-Agent
Actor-Critic for Mixed Cooperative-Competitive Environments (2020)
[31] Hong, Z.-W., Su, S.-Y., Shann, T.-Y., Chang, Y.-H., Lee, C.-Y.: A Deep Policy
Inference Q-Network for Multi-Agent Systems (2018)
[32] Everett,R.,Roberts,S.J.:Learningagainstnon-stationaryagentswithopponent
modelling and deep reinforcement learning. In: 2018 AAAI Spring Symposia,
StanfordUniversity,PaloAlto,California,USA,March26-28,2018.AAAIPress,
(2018). https://aaai.org/ocs/index.php/SSS/SSS18/paper/view/17518
[33] Brockman,G., Cheung, V., Pettersson,L., Schneider,J., Schulman,J., Tang, J.,
Zaremba, W.: Openai gym. CoRR abs/1606.01540 (2016) 1606.01540
[34] Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., Dormann, N.:
Stable-baselines3: Reliable reinforcement learning implementations. Journal of
Machine Learning Research 22(268), 1–8 (2021)
20