GaussianCity: Generative Gaussian Splatting for
Unbounded 3D City Generation
HaozheXie ZhaoxiChen FangzhouHong ZiweiLiu(cid:0)
S-Lab,NanyangTechnologicalUniversity
{haozhe.xie, zhaoxi001, fangzhou001, ziwei.liu}@ntu.edu.sg
https://haozhexie.com/project/gaussian-city
Abstract
3DcitygenerationwithNeRF-basedmethodsshowspromisinggenerationresults
butiscomputationallyinefficient. Recently3DGaussiansplatting(3D-GS)has
emergedasahighlyefficientalternativeforobject-level3Dgeneration. However,
adapting3D-GSfromfinite-scale3Dobjectsandhumanstoinfinite-scale3Dcities
isnon-trivial. Unbounded3Dcitygenerationentailssignificantstorageoverhead
(out-of-memoryissues),arisingfromtheneedtoexpandpointstobillions,often
demandinghundredsofGigabytesofVRAMforacityscenespanning10km2. In
thispaper,weproposeGaussianCity,agenerativeGaussiansplattingframework
dedicatedtoefficientlysynthesizingunbounded3Dcitieswithasinglefeed-forward
pass. Ourkeyinsightsaretwo-fold: 1)Compact3DSceneRepresentation: We
introduceBEV-Pointasahighlycompactintermediaterepresentation,ensuringthat
thegrowthinVRAMusageforunboundedscenesremainsconstant,thusenabling
unboundedcity generation. 2) Spatial-awareGaussianAttributeDecoder: We
presentspatial-awareBEV-Pointdecodertoproduce3DGaussianattributes,which
leveragesPointSerializertointegratethestructuralandcontextualcharacteristicsof
BEVpoints. ExtensiveexperimentsdemonstratethatGaussianCityachievesstate-
of-the-artresultsinbothdrone-viewandstreet-view3Dcitygeneration. Notably,
compared to CityDreamer, GaussianCity exhibits superior performance with a
speedupof60times(10.72FPSv.s. 0.18FPS).
1 Introduction
Thegenerationof3Dassetshasattractedconsiderableattentionfrombothacademiaandindustry
duetoitssignificantpotentialapplications. Withtherapidadvancementsingenerativemodeling,
therehavebeennotableachievementsin3Dcontentgeneration,encompassingtheproductionof
objects[42,52,55],avatars[8,19,23],andscenes[10,30,54]. Citygeneration,recognizedasone
ofthemostdemandingtasksin3Dcontentcreation,holdswide-rangingapplicationsacrossdomains
suchasgaming,animation,film,andvirtualreality.
Inrecentyears,significantprogresshasbeenmadeinthefieldof3Dcitygeneration. BothInfiniC-
ity[30]andCityDreamer[54]employNeRF[36]togenerateunboundedphotorealistic3Dcities,
achievingpromisingresults.However,thesemethodssamplepointsatthesamedensityandaggregate
colorvaluesforallpointsalongrays,resultingininefficienciesduringinferenceandlossofdetails.
Overthepastyear,3DGaussiansplatting(3D-GS)[22]hasgainedwidespreadusein3Dgeneration,
offeringasignificantlyfasterrenderingtechniquebyleveragingGPU-basedrasterization. Moreover,
3D-GSprovidesamoreflexiblewaytorepresentdetails,leveragingmore3DGaussianstocapture
finerintricacies. However,existing3D-GS-basedgenerators[52,33,10]onlyproducefinite-scale
objectsorscenescontainingalimitednumberof3DGaussians. AsillustratedinFigure1(b-c),when
the scene scales up, the demands for GPU memory (VRAM) and file storage grow dramatically,
makingitinfeasibleforunboundedscenegeneration.
4202
nuJ
01
]VC.sc[
1v62560.6042:viXra#Points: 20M BEV-Point Decoder Gaussian Attributes 25 3D-GS 2.5 3D-GS
Area: ~1.5 km2 20 Gaussian City 2.0 Gaussian City
Gaussian Rasterizer 15 1.5 10 1.0
5 0.5
0 0.0
BEV-Point # Points 0.3M0.6M1.3M2.5M5.0M 10M 20M # Points 0.3M0.6M1.3M2.5M5.0M 10M 20M
(a) Overview of the Proposed Method: GaussianCity (b) Used VRAM as #Points Increases (c) File Size as #Points Increases
Pers.Nature (5.99 FPS) SceneDreamer (1.61 FPS) InfiniCity (Unknown) CityDreamer (0.18 FPS) GaussianCity (10.72 FPS)
(d) Comparisons of Visuals and Runtimes on the GoogleEarthDataset
Figure1: (a)BenefitingfromthecompactBEV-Pointrepresentation, GaussianCitycangenerate
unbounded3Dcitiesusing3DGaussiansplatting(3D-GS).(b)Asthenumberofpointsincreases,
theVRAMusagefor3D-GSgrowsdramatically,whiletheBEV-Point,servingasacompactrep-
resentation, keepstheVRAMusageconstant. (c)Asthenumberofpointsincreases, BEV-Point
exhibitssignificantlylowergrowthinfilestoragecomparedto3D-GS.(d)TheproposedGaussianCity
achievesnotonlysuperiorgenerationqualitybutalsothebestefficiencyin3Dcitygeneration.
Toaddresstheseissues,weproposeGaussianCity,thefirstgenerativeGaussiansplattingforun-
bounded3Dcitygeneration.AsshowninFigure1(a),GaussianCityleveragesahighlycompactscene
representation,namelyBEV-Point,todecoupleGaussianattributesintotwomajorparts: position-
relatedattributesandstyle-relatedattributes. Theposition-relatedattributescanbefurthercondensed
intobird’s-eye-view(BEV)maps,whilethestyle-relatedattributescanbecompressedintoastyle
lookuptable. OnlyvisibleBEVpointsareconsideredduringtherenderingandoptimizationprocess,
ensuring that VRAM usage is kept at a constant level. To generate 3D Gaussian attributes from
BEVpoints,wepresentBEV-PointDecoder,whichleveragesPointSerializertocapturestructural
andcontextualcharacteristicsofunstructuredBEVpoints. Notably,thecontextualcharacteristics
ofpointsareinherentlypreservedduringpointsamplingwithinalocal3DpatchinNeRF,whereas
thesearedisruptedduetotheunstructurednatureofBEVpoints. TheGaussianRasterizerisfinally
employedtorendertheimagefromthegeneratedGaussianattributes.
Extensiveexperiments,encompassingbothqualitativeandquantitativeanalysesonGoogleEarth[54]
andKITTI-360[28],showcasethesuperiorityofGaussianCityoverstate-of-the-artmethodsfor3D
citygenerationintermsofbothgenerationqualityandefficiency. Thecontributionsaresummarized
as follows: 1) To the best of our knowledge, we propose the first 3D-GS generative model for
unbounded3Dcitygenerationwithbothhighrealismandefficiency. 2)WeintroduceBEV-Pointasa
highlycompactrepresentation,whichensuresthattheVRAMusageremainsconstantasthescene
sizescalesup. 3)WepresentBEV-PointDecoder,whichleveragesPointSerializertocaptureboth
structuralandcontextualcharacteristicsofunstructuredBEVpoints.
2 RelatedWork
3D Gaussian Splatting. 3D Gaussian splatting (3D GS) [22] has gained considerable attention
inrecentmonths,demonstratingpromisingrenderingresultsandfasterperformancecomparedto
NeRF[36]. Numerousmethodshavebeenproposedtointegrate3DGSintothe3Dreconstructionof
objects[50,56,62],avatars[41,46,61],andscenes[31,34,35].Furthermore,severalapproaches[24,
37] have aimed to optimize the high memory consumption of 3D GS during reconstruction. 3D
GS is also extensively used in the generation of 3D objects [51, 52], 3D avatars [1, 33, 60], and
3Dscenes[10]. However,thesemethodsonlygeneratesmall-scale3Dassetsandarenotmemory-
efficientforrepresentinglarge-scale3Dassets. Benefitingfrom3D-GS,efficientlyapplyingitto4D
object[15,44]andhuman[25,27,57]generationwithdeformationbecomesfeasible.
SceneGeneration. Unlikeimpressivemodelsforgeneratingobjectsandavatars,generatingscenes
presentsgreaterchallengesduetotheirextremelyhighdiversity. Earlierapproaches[26,32]generate
scenesbysynthesizingvideos,whichlack3Dawarenessandcannotensure3Dconsistency. Semantic
imagesynthesismethods[16,39,48]exhibitpromisingresultsingeneratingscene-levelcontentby
conditioningonpixel-wisedensecorrespondence,suchassemanticsegmentationmapsordepthmaps.
2
spaM
VEB
…
…
… …
…
P. Attr.S.
LUT.
)BG(
MARV
)BG(
eziS
eliFBEV Points
Ray Intersection
𝐂𝐂R 𝐅𝐅S 𝐈𝐈 𝐙𝐙P 𝐂𝐂A
Point Vis. Map
𝒱𝒱 Mod.
Style LUT. BEV-Point Attr.
C Positional Encoder MLP 𝒯𝒯
𝒫𝒫 MLPs
Point Serializer MLPℳ
ℒ
Ins. Style Code Ins. Abs.Cord. Rel.Cord.
Point Transformer Gaussian Attr.
𝐋𝐋 𝐙𝐙T 𝐈𝐈 𝐂𝐂A 𝐂𝐂R ℱ 𝐀𝐀
Scene Feat.
Rendered Image
𝐅𝐅S
BEV-Point Initialization (§ 3.2) BEV-Point Feature Generation (§ 3.3) BEV-Point Decoding (§𝐑𝐑 3.4)
Figure2: TheframeworkofGaussianCity. Tocreateanunbounded3Dcity,theBEVpointsare
firstlygeneratedfromalocalpatchoftheBEVmaps,whichincludestheheightfieldH,semantic
mapS,andbinarydensitymapD. Then,theBEV-Pointattributes{I,C ,C ,F }aregenerated
A R S
foreachpointandtheStyleLookupTable: T(L) → Z isgeneratedforeachinstance. Next,the
T
BEV-Point decoder generates the Gaussian attributes A from BEV-Point attributes. Finally, the
GaussianRasterizerRproducestherenderedimageR.
Severalmethods[5,10,59]generatenatural3Dscenesbyperforminginpaintingandoutpaintingon
RGBimagesorfeaturemaps. However,mostmethodscanonlyinterpolateorextrapolatealimited
distancefromtheinputviewsandlackgenerativecapabilities. Recentapproaches[9,30,54]have
achieved3Dconsistentscenesataninfinitescalethroughunboundedlayoutextrapolation. Another
setofworks[2,12,40]focusesonindoorscenesynthesisusingcostly3Ddatasets[11,49]orCAD
objectretrieval[13,14].
3 OurApproach
3.1 Background: 3DGaussianSplatting
As introduced in [22], 3D Gaussian splatting (3D-GS) is an explicit point-based 3D scene repre-
sentation,usingasetof3DGaussianswithvariousattributestomodelthescene. EachGaussian
ischaracterizedbyacenterx ∈ R3,ascalingfactorofs ∈ R3,andarotationquaternionq ∈ R4.
Additionally, itmaintainsopacityvalueα ∈ Randacolorfeaturec ∈ RC forrendering, where
sphericalharmonicscanmodelview-dependenteffects. Theseparameterscancollectivelybedenoted
byA={x ,s ,q ,α ,c }N ,whereN isthenumberof3DGaussians. Rendering3DGaussians
i i i i i i=1
involvesprojectingthemontotheimageplaneas2DGaussiansandperformingalphacomposition
foreachpixelinfront-to-backdepthorder,thusdeterminingthefinalcolorandalpha.
3.2 BEV-PointInitialization
Within3D-GS,all3DGaussiansundergoinitializationwithapredefinedsetofparametersduring
optimization. However,asthescenescalesup,theVRAMusageincreasesdramatically,making
itimpracticaltogeneratelarge-scalescenes. Addressingthisissue,weproposeahighlycompact
representationnamelyBEV-Point. IntheBEV-Pointrepresentation, onlyvisibleBEVpointsare
retainedsinceonlytheyimpacttheappearanceofthecurrentframe. ItensuresthatVRAMusage
remainsconstantbecausethenumberofvisibleBEVpointsdoesnotincreasewiththescenescale,
givenfixedcameraparameters.
Inalocalpatchofabird’s-eye-view(BEV)mapcomprisingasemanticmapMandaheightfieldH,
acollectionofBEVpointswithinthispatchcanbeproducedbyextrudingthepixelsinthesemantic
mapSaccordingtothecorrespondingvaluesintheheightfieldH. Wefurtherintroducethebinary
3
ytisneD
.meS
thgieH
𝐃𝐃
𝐒𝐒
𝐇𝐇
…
redocnE
enecS
ℰ
… … … …
… ekaF
laeR
…
rotanimircsiD
…
𝒟𝒟
… …
…
…
reziretsaR
naissuaG
ℛdensitymapDtoadjustthesamplingdensityfordifferentsemanticcategories. Thisisdrivenby
theobservationthatcertaincategoriesexhibitsimplertextures(e.g.,roads,waterareas),allowing
forreduceddensitytomanagecomputationalcosts,whileothercategories(e.g.,buildingfaçades)
possessintricatetextures,necessitatingagreaternumberofpointsforrepresentation. Thecoordinates
ofthegeneratedBEVpoints,denotedasC
F
∈RNpt×3,canbegeneratedas
(cid:8) (cid:9)
C = (x,y,z)|H ≤zandD =1 (1)
F (x,y) (x,y)
whereN isthenumberofBEVpoints.
pt
BenefitingfromthebinarydensitymapD,asignificantnumberofredundantBEVpointshavebeen
omitted. However, the remaining number, typically 20 million BEV points, is still too large for
optimization. Toaddressthisissue, weadditionallyconductrayintersectiontoobtainthebinary
visibilitymapV :(x,y,z)→v,wherev ∈{0,1},forfilteringoutthevisibleBEVpoints.Therefore,
thecoordinatesofthevisibleBEVpointsC
A
∈RNpt×3canbegeneratedas
C ={(x,y,z)|(x,y,z)∈C andV(x,y,z)=1} (2)
A F
3.3 BEV-PointFeatureGeneration
ThefeaturesintheBEV-Pointrepresentationcanbedividedintothreecategories: instanceattributes,
BEV-Pointattributes,andstylelook-uptable. Theinstanceattributesencompassfundamentaldetails
such as Instance label, size, and center coordinates for each instance. The BEV-Point attributes
determinetheappearancewithintheinstance,whilethestylelook-uptablecontrolsthestylevariation
acrossinstances.
Instance Attributes. The semantic map S provides semantic labels for BEV points. Following
CityDreamer[54],theinstancemapQisintroducedtohandlethediversityofbuildingsandcarsin
urbanenvironments. Specifically,
Q=Inst(S) (3)
whereInst(·)denotesinstantiationonsemanticmapsbydetectingconnectedcomponents. Therefore,
theinstancelabelI∈RNpt×1forBEVpointscanbecomputedas
(cid:8) (cid:9)
I= Q |(x,y,z)∈C (4)
(x,y) A
Obviously,Q ∈L,whereL={1,2,...,N }andN isthenumberofinstances. Thesize
(x,y) ins ins
S(l) ∈ R3 representsthesizeofthe3Dboundingboxoftheinstancel,wherel ∈ L. Thecenter
C(l)∈R3denotesthecoordinatesofthecenteroftheboundingboxoftheinstancel.
BEV-PointAttributes. InBEV-Pointinitialization,theabsolutecoordinateC isgenerated,with
A
the origin set at the center of the world coordinate system. Besides the absolute coordinate, the
relativecoordinateC
R
∈RNpt×3isintroduced,withitsoriginsetatthecenterofeachinstance,to
specifythenormalizedpointcoordinatesrelativetotheinstance. Specifically,
C =(cid:8) Ci |Ci ∈R3(cid:9)Npt (5)
R R R i=1
whereCi canbederivedas
R
2(cid:0) Ci −C(Ii)(cid:1)
Ci = A (6)
R S(Ii)
whereCi andIidenotethei-thvaluesinC andI,respectively.
A A
During generation, integrating contextual information for BEV points becomes essential. It is
achievedbyintroducingscenefeaturesF
S
∈RNpt×CFS derivedfromBEVmapsandindexedusing
absolutecoordinatesC . Specifically,
A
G=E(H,S) (7)
(cid:8) (cid:9)
F = G |(x,y,z)∈C (8)
S (x,y) A
whereE denotestheSceneEncoder. GiswiththesamesizeofHandS.
StyleLook-upTable. In3D-GS,theappearanceof3DGaussiansisdefinedbytheattributesofeach
Gaussian. Asthenumberof3DGaussiansincreases,thedemandsonVRAMandfilestoragegrow
4significantly,makingunboundedscenegenerationinfeasible. Tofurtherreducecomputationalcosts,
theappearancesofinstancesareencodedintoasetoflatentvectorsZ
T
∈RNins×CZ,i.e.,
Z =(cid:8) zi |zi ∈RCZ(cid:9)Nins (9)
T T T i=1
Thestylelook-uptableT queriesthestylecodezi fortheinstancel. Specifically,
T
zi =T(l) (10)
T
3.4 BEV-PointDecoding
The BEV-Point decoder is designed to generate the Gaussian Attributes A using the BEV-Point
features. It comprises five key modules: positional encoder, point serializer, point transformer,
modulatedMLP,andGaussianrasterizer.
PositionalEncoder.Ratherthanfeedcoordinatesdirectlyintothesubsequentnetworks,thepositional
encoder transforms each point coordinate and corresponding features into a higher-dimensional
positionalembeddingF
P
∈RNpt×CFP asfollows
F =P(Concat(C ,F )) (11)
P R S
whereP(·)isthepositionalencodingfunctionthatisappliedindividuallytoeachvalueinthegiven
featurex.
P(x)=(cid:8) sin(2iπx),cos(2iπx)(cid:9)NPE−1
(12)
i=0
PointSerializer. UnlikeNeRF[36],whichmaintainsspatialcorrelationamongsampledpointsalong
rays,BEVpointsand3DGaussiansareunstructuredandunorderedduetotheirirregularnatureof
pointclouds. Therefore, directlyapplyingMulti-layerPerceptrons(MLPs)togenerateGaussian
attributesfromF maynotyieldoptimalresults,asMLPsdonotfullyconsiderthestructuraland
P
contextualcharacteristicsofpointclouds.
To transform unstructured BEV points into a structured format, we present the point serializer
L:(x,y,z)→o,whereo∈Z,toconvertthepointcoordinatesintoanintegerreflectingitsorder
withinthegivenBEVpoints.
(cid:22) (cid:23)
x y
L(x,y,z,g)= + +z (13)
g2 g
wheregisthegridsizethatcontrolsthescaleofthediscretespaceforserialization. Thisordering
ensures that points are rearranged based on the spatial structure defined by the point serializer,
resultinginneighboringpointsinthedatastructurebeingclosetogetherinspace. Thus,theserialized
featureFS
P
∈RNpt×CFP canbederivedas
FS
=(cid:110) FOi(cid:111)Npt
(14)
P P
i=1
whereFOi denotesthefeaturesoftheO -thBEVpointinF . O denotestheordergeneratedbythe
P i P i
pointserializer.
PointTransformer. Afterserialization,thefeaturesofBEVpointscanbeprocessedbyamodern
TransformerF [53],producingthefeatureF
T
∈RNpt×CFT.
F =F(FS) (15)
T P
ModulatedMLPs. Theattributesof3DGaussiansA∈RNpt×CA aregeneratedbyapplyingMLPs
M[21],whichmodulatethestylecodeZ andinstancelabelsIwiththefeaturesofBEVpoints.
P
A=M(Concat(F ,F ),Z ,I) (16)
P T P
whereZ
=(cid:8) T(Ii)(cid:9)Npt.
P i=1
GaussianRasterizer. GiventhecameraintrinsicandextrinsicparametersK∈R3×3andT∈R4×4,
theimagecanberenderedwiththeGaussianrasterizerR.
Rˆ =R(C ,A,K,T) (17)
A
Duringrasterization,defaultvaluesareemployediftherequiredattributesarenotgeneratedinA.
Thesedefaultsincludescalefactors=1,rotationquaternionq=[1,0,0,0],andopacityα=1.
5Table1:QuantitativecomparisononGoogleEarthandKITTI-360.Thebestresultsarehighlighted
inbold. Notethat“RT.”denotes“Runtime”,measuredinmillisecondsonanNVIDIATeslaA100.
GoogleEarth KITTI-360
Method FID↓ KID↓ DE↓ CE↓ RT.↓ Method FID↓ KID↓
SGAM[47] 277.64 0.358 0.575 239.291 193 StyleGAN[21] 31.9 0.021
Pers.Nature[5] 123.83 0.109 0.326 86.371 167 GSN[12] 160.0 0.114
SceneDreamer[9] 213.56 0.216 0.152 0.186 620 GIRAFFE[38] 112.1 0.117
CityDreamer[54] 97.38 0.096 0.147 0.060 5580 UrbanGIR.[58] 39.6 0.036
GaussianCity 86.94 0.090 0.136 0.057 93 GaussianCity 29.5 0.017
3.5 LossFunctions
Thegeneratoristrainedusingahybridobjectivethatcombinesreconstructionlosswithadversarial
learning loss. Specifically, we leverage the L1 loss, VGG loss [20], and GAN loss [29] in this
combination.
ℓ=λ ||Rˆ −R||+λ VGG(Rˆ,R)+λ GAN(Rˆ,S ) (18)
L1 VGG GAN G
whereRdenotesthegroundtruthimage. S isthesemanticmapinperspectiveview.
G
4 Experiments
4.1 Datasets
GoogleEarth[54]. TheGoogleEarthdataset,sourcedfromGoogleEarthStudio, comprises400
orbittrajectoriescapturedoverManhattanandBrooklyn. Eachtrajectorycomprises60images,with
orbitradiusesspanningfrom125to813metersandaltitudesrangingfrom112to884meters. In
additiontotheimages,GoogleEarthprovidescameraintrinsicandextrinsicparameters,alongwith
3Dsemanticandbuildinginstancesegmentation.
KITTI-360[28]. KITTI-360isanextensiveoutdoorsub-urbandatasetknownforitsintricatescene
geometry. Within this dataset, scenes are rich in highlights and shadows, resulting in significant
variationsintheappearanceofobjectsacrossdifferentscenes. Capturedinurbanscenariosspanning
approximately73.7kmofdrivingdistance,KITTI-360providescomprehensive3Dboundingbox
annotationsforvariousclassessuchasbuildings,cars,roads,vegetation,andmore.
4.2 EvaluationProtocols
FIDandKID.FréchetInceptionDistance(FID)[17]andKernelInceptionDistance(KID)[3]are
metricsforthequalityofgeneratedimages. FollowingCityDreamer[54],FIDandKIDarecalculated
basedon15KgeneratedframesontheGoogleEarthdataset. AsfortheKITTI-360dataset,FIDand
KIDarecomputedusing5Kgeneratedframes,consistentwiththesettingsofUrbanGIRAFFE[58].
DepthError. Weemploydeptherror(DE)toassessthe3DgeometryfollowingEG3D[6]. The
pseudogroundtruthdepthmapsforthegeneratedframesareobtainedfromapretrainedmodel[43].
ForNeRF-basedmethods,thedepthmapsaregeneratedbytheaccumulationofdensityσ.Conversely,
GaussianCityallowsforthedirectacquisitionofdepthmapsbyrasterization. DEiscalculatedasthe
L2distancebetweenthetwonormalizeddepthmaps. WeevaluateDEusing100framesforeach
assessedmethodfollowingCityDreamer[54].
CameraError. FollowingCityDreamer[54],thecameraerror(CE)isintroducedtoevaluatethe
multi-viewconsistency. CEquantifiesthedisparitybetweentheinferencecameratrajectoryandthe
estimatedcameratrajectoryfromCOLMAP[45],computedasthescale-invariantnormalizedL2
distancebetweenthereconstructedandgeneratedcameraposes.
Runtime. Runtimeisusedtogaugeefficiency. Itmeasuresthetimetakentogeneratea960x540
imageontheGoogleEarthdataset. AlltimingsareconductedonanNVIDIATeslaA100,excluding
IOtime,andaveragedover100iterations.
6Figure3: QualitativecomparisononGoogleEarth. Notethat“Pers.Nature”isshortforPersistent-
Nature[5]. ThevisualresultsofInfiniCity[30]areprovidedbytheauthorssincethesourcecodeis
notaccessible.
4.3 ImplementationDetails
Hyperparameters.Thegridsizegissetto0.01andN issetto64.ThefeaturechannelsC ,C ,
PE Z FS
C ,andC aresetto256,61,1280,and256,respectively. TheGaussianattributesAconsist
FP FT
ofRGBvaluesonly(i.e., C = 3). Thelossweightsλ , λ , andλ are10, 10, and0.5,
A L1 VGG GAN
respectively.
Training Details. The generator is trained using an Adam optimizer with β = (0,0.999) and a
learningrateof10−4. ThediscriminatorisoptimizedusinganAdamoptimizerwithβ =(0,0.999)
andalearningrateof5×10−7. Trainingcontinuesfor200,000iterationswithabatchsizeof8
oneightNVIDIATeslaV100GPUs. Theimagesarerandomlycroppedtosizesof448×448for
GoogleEarthand448×224forKITTI-360.
4.4 MainResults
ComparisonMethods.OntheGoogleEarthdataset,wecomparetheproposedmethodtoSGAM[47],
PersistentNature[5],SceneDreamer[9],InfiniCity[30],andCityDreamer[54],followingtheprotocol
established by CityDreamer. Except for InfiniCity, whose code is not accessible, we retrain the
remainingmethodsusingthereleasedcodeontheGoogleEarthdatasettoensurefaircomparisons.
OntheKITTI-360dataset,wecompareourmethodtoStyleGAN2[21],GSN[12],GIRAFFE[38],
andUrbanGIRAFFE[58],followingtheprotocoloutlinedbyUrbanGIRAFFE.Asthetrainingcode
andpretrainedmodelforUrbanGIRAFFEareunavailable,theresultsofUrbanGIRAFFEaredirectly
obtainedfrom[58].
ComparisononGoogleEarth. Figure3showsqualitativecomparisonsagainstbaselinesonthe
GoogleEarth dataset. PersistentNature utilizes a tri-plane representation, which faces difficulties
ingeneratingrealisticrenderings. BothSceneDreamerandInfiniCityemployvoxelgridsastheir
representation,yettheystillencountersignificantstructuraldistortionsinbuildingsduetoallbuildings
beingassignedthesamesemanticlabel. CityDreamerandGaussianCitybothintroduceinstance
labels,achievingimprovedgenerationresults. However,CityDreamercompositestwoneuralfields,
7
erutaN.sreP
remaerDenecS
ytiCinifnI
remaerDytiC
ytiCnaissuaGGSN UrbanGIRAFFE GaussianCity
Figure4: QualitativecomparisononKITTI-360. ThevisualresultsofUrbanGIRAFFE[58]are
providedbytheauthorssincethetrainingcodeandpretrainedmodelareunavailable.
Perceptual Quality Degree of 3D Realism View Consistency
5.0 5.0
4.0 4.0
3.0 3.0
2.0 2.0
1.0 1.0
0.0 0.0
Pers.Nature SceneDreamer InfiniCity CityDreamer GaussianCity GSN UrbanGIR. GaussianCity
(a) GoogleEarth (b) KITTI-360
Figure 5: User study on GoogleEarth and KITTI-360. All scores are in the range of 5, with
5indicatingthebest. Notethat“Pers.Nature”and“UrbanGIR.”denotesPersistentNature[5]and
UrbanGIRAFFE[58],respectively.
resultinginartifactsattheseamsofimages. Incontrast,GaussianCityachievesbettervisualresults
andsignificantlylowerruntime,asindicatedinTable1.
ComparisononKITTI-360. Figure4illustratesqualitativecomparisonsagainstbaselinemethods
ontheKITTI-360dataset. GSNusesanimplicitrepresentationnamedlatentgrid,whichcaneasily
introducestructuraldistortioninstreet-viewgeneration. UrbanGIRAFFEemploysavoxelgridas
its3Drepresentation,butduetotheresolutionlimitationsofthevoxelgrid,thegeneratedscenes
may exhibit jagged artifacts. In contrast, GaussianCity adopts a more flexible point cloud as its
representation,thusyieldingsuperiorvisualresults. ThelowerFIDandKIDscoresinTable1also
attesttotheeffectivenessoftheproposedmethod.
UserStudy. Tomoreaccuratelyevaluatethe3Dconsistencyandqualityoftheunbounded3Dcity
generation,weconductanoutputevaluation[4],followingtheuserstudyconductedinCityDreamer.
Inthisstudy,werequestedfeedbackfrom20volunteerstoassesseachgeneratedcameratrajectory
basedonthreecriteria: 1)theperceptualqualityoftheimagery,2)thelevelof3Drealism,and3)the
3Dviewconsistency. Ratingsareprovidedonascaleof1to5,with5indicatingthehighestrating.
TheresultsareshowninFigure5,demonstratingthattheproposedmethodsignificantlysurpasses
thebaselinesbyaconsiderablemargin.
4.5 AblationStudies
EffectivenessofBEV-PointRepresentation.TheproposedBEV-Points,servingasahighlycompact
scenerepresentation,playanimportantroleinenabling3D-GStogenerateunboundedscenes. As
showninFigure1(b),theVRAMusageof3D-GSdramaticallyincreasesasthenumberofpoints
increases. Typically,forascenecovering1.5km2 with20millionpoints,optimizationcannotbe
8
serocS
.gvATable2: EffectivenessofBEV-PointDecoder. Table3: Effectivenessofdifferentserialization
Notethat“Ser.” and“Trans”denote“PointSe- methods. Notethat“Eq.13”denotetheserializa-
rializer”and“PointTransformer”,respectively. tionusedinPointSerializer.
Ser. Trans. FID↓ KID↓ DE↓ CE↓ Eq.13 Hilbert FID↓ KID↓ DE↓ CE↓
✗ ✗ 151.27 0.179 0.185 0.135 ✓ ✗ 86.94 0.090 0.136 0.057
✗ ✓ 119.40 0.138 0.159 0.118 ✗ ✓ 86.72 0.088 0.136 0.056
✓ ✓ 86.94 0.090 0.136 0.057 ✓ ✓ 86.28 0.083 0.135 0.055
completedonaGPUwith32GBofVRAM.Incontrast,BEV-Pointensuresthatthenumberofpoints
requiringoptimizationbecomesaconstantindependentofthescenesizethroughRayIntersection,
leadingtosignificantlyreducedVRAMusage. BesidesRayIntersection,wealsoevaluatetheregion-
basedandinstance-basedselectionstrategiestoidentifypointsrequiringoptimization.Thesemethods
canallmaintainthenumberofpointsataconstantlevel,with“Region”indicatingselectionbased
onthecameracoverageregionand“Instance”indicatingselectionbasedonvisibleinstances. After
applying“Region”,“Instance”,andRayIntersectiontoascenespanning1.5km2with20Mpoints,
thenumberofpointsbecomes1.4M,680K,and31.7K,respectively. “Region”and“Instance”areless
efficientcomparedtotherayintersectionadoptedbyBEV-Point,astherayintersectionmaximally
removesinvisiblepoints.
EffectivenessofBEV-PointDecoder. BEV-PointDecoderconsistsoftwokeycomponents: Point
Serializer and Point Transformer. As shown in Table 2, removing both components leads to a
significant degeneration due to the absence of spatial correlations. Introducing only the Point
Transformerpartiallyestablishesspatialcorrelation,butitdoesnotachievethesameeffectivenessas
usingbothcomponentstogether.PointserializeremploysEquation13tostructuretheunorderedBEV
points. Toinvestigatetheimpactofdifferentserializationmethodsinpointserializer,wecomparethe
effectsofusingEquation13,Hilbertcurve[18]adoptedin[7,53],andbothsimultaneouslyonthe
generationresults. AsshowninTable3,differentserializationmethodshaveminimalimpactonthe
finalgenerationresults. Forefficiencyreasons,weuseEquation13inPointSerializerduetoitslower
computationalcomplexity. RefertoFigures6and7intheappendixformorequalitativecomparisons.
4.6 Limitations
Whileourmethoddemonstratespromisingresultsforunconditional3Dcitygeneration,itstillhas
several limitations. Firstly, the BEV-Point Initialization elevates points to a height based on the
correspondingvalueintheheightfield,whichassumesbuildingsadheretotheManhattanassumption
andcannotrepresenthollowstructures. Secondly,theBEV-Pointdecoderdoesnotfullyexploitthe
expressivecapacityof3D-GS,asitonlypredictsRGBintheGaussianattributes. However,predicting
anexcessivenumberofattributessimultaneouslymaydestabilizethetrainingprocess.
5 Conclusion
Inthispaper,weintroduceGaussianCityforgeneratingunbounded3Dcities. Itisthefirstmethodto
employ3DGaussiansplatting(3D-GS)forunboundedscenegeneration. WeintroduceBEV-Point,a
highlycompactscenerepresentationtomitigatethesubstantialincreaseinVRAMusageassociated
with representing large-scale scenes using 3D-GS. Additionally, we present BEV-Point Decoder,
whichleveragesPointSerializertocapturethestructuralandcontextualcharacteristicsofunordered
BEVpoints. ComparedtopreviousNeRF-basedcitygenerators,ourapproachachievesbettervisual
resultsanda60-foldincreaseinruntimespeedcomparedtoCityDreamer. Extensiveexperimental
resultsonGoogleEarthandKITTI-360demonstratetheeffectivenessoftheproposedGaussianCity
ingeneratingbothdrone-viewandstreet-viewincityscenes.
Acknowledgements. ThisstudyissupportedbytheMinistryofEducation, Singapore, underits
MOEAcRFTier2(MOET2EP20221-0012),NTUNAP,andundertheRIE2020IndustryAlignment
Fund–IndustryCollaborationProjects(IAF-ICP)FundingInitiative,aswellascashandin-kind
contributionfromtheindustrypartner(s).
9References
[1] RameenAbdal,WangYifan,ZifanShi,YinghaoXu,RyanPo,ZhengfeiKuang,QifengChen,
Dit-YanYeung,andGordonWetzstein. Gaussianshellmapsforefficient3Dhumangeneration.
InCVPR,2024. 2
[2] Miguel Ángel Bautista, Pengsheng Guo, Samira Abnar, Walter Talbott, Alexander Toshev,
ZhuoyuanChen,LaurentDinh,ShuangfeiZhai,HanlinGoh,DanielUlbricht,AfshinDehghan,
andJoshuaM.Susskind. GAUDI:Aneuralarchitectforimmersive3Dscenegeneration. In
NeurIPS,2022. 3
[3] MikolajBinkowski,DanicaJ.Sutherland,MichaelArbel,andArthurGretton. Demystifying
MMDGANs. InICLR,2018. 6
[4] ZoyaBylinskii, LauraMariahHerman, AaronHertzmann, StefanieHutka, andYileZhang.
Towards better user studies in computer graphics and vision. Foundations and Trends in
ComputerGraphicsandVision,15(3):201–252,2023. 8
[5] LucyChai,RichardTucker,ZhengqiLi,PhillipIsola,andNoahSnavely. PersistentNature: A
generativemodelofunbounded3Dworlds. InCVPR,2023. 3,6,7,8,16
[6] EricR.Chan,ConnorZ.Lin,MatthewA.Chan,KokiNagano,BoxiaoPan,ShaliniDeMello,
OrazioGallo,LeonidasJ.Guibas,JonathanTremblay,SamehKhamis,TeroKarras,andGordon
Wetzstein. Efficientgeometry-aware3Dgenerativeadversarialnetworks. InCVPR,2022. 6
[7] WanliChen,XingeZhu,GuojinChen,andBeiYu. Efficientpointcloudanalysisusinghilbert
curve. InECCV,2022. 9
[8] ZhaoxiChen,FangzhouHong,HaiyiMei,GuangcongWang,LeiYang,andZiweiLiu. PrimD-
iffusion: Volumetricprimitivesdiffusionfor3Dhumangeneration. InNeurIPS,2023. 1
[9] ZhaoxiChen,GuangcongWang,andZiweiLiu. SceneDreamer: Unbounded3Dscenegenera-
tionfrom2Dimagecollections. TPAMI,45(12):15562–15576,2023. 3,6,7
[10] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Lucid-
Dreamer: Domain-free generation of 3D Gaussian splatting scenes. In CVPR, 2024. 1, 2,
3
[11] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas A. Funkhouser, and
MatthiasNießner. ScanNet: Richly-annotated3Dreconstructionsofindoorscenes. InCVPR,
2017. 3
[12] TerranceDeVries,MiguelÁngelBautista,NitishSrivastava,GrahamW.Taylor,andJoshuaM.
Susskind. Unconstrainedscenegenerationwithlocallyconditionedradiancefields. InICCV,
2021. 3,6,7
[13] HuanFu,BowenCai,LinGao,LingxiaoZhang,JiamingWang,CaoLi,QixunZeng,Chengyue
Sun, Rongfei Jia, Binqiang Zhao, and Hao Zhang. 3D-FRONT: 3D furnished rooms with
layoutsandsemantics. InICCV,2021. 3
[14] HuanFu,RongfeiJia,LinGao,MingmingGong,BinqiangZhao,StephenJ.Maybank,and
DachengTao. 3D-FUTURE:3Dfurnitureshapewithtexture. IJCV,129(12):3313–3337,2021.
3
[15] QuankaiGao,QiangengXu,ZheCao,BenMildenhall,WenchaoMa,LeChen,DanhangTang,
andUlrichNeumann. GaussianFlow: SplattingGaussiandynamicsfor4Dcontentcreation.
arXiv,2403.12365,2024. 2
[16] ZekunHao,ArunMallya,SergeJ.Belongie,andMing-YuLiu. GANCraft: unsupervised3D
neuralrenderingofminecraftworlds. InICCV,2021. 2
[17] MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSeppHochreiter.
GANstrainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium. InNIPS,
2017. 6
[18] David Hilbert. Über die stetige abbildung einer linie auf ein flächenstück. Mathematische
Annalen,38:459–460,1891. 9
[19] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu.
AvatarCLIP:zero-shottext-drivengenerationandanimationof3Davatars. TOG,41(4):161:1–
161:19,2022. 1
[20] JustinJohnson,AlexandreAlahi,andLiFei-Fei. Perceptuallossesforreal-timestyletransfer
andsuper-resolution. InECCV,2016. 6
[21] TeroKarras, SamuliLaine, MiikaAittala, JanneHellsten, JaakkoLehtinen, andTimoAila.
AnalyzingandimprovingtheimagequalityofstyleGAN. InCVPR,2020. 5,6,7
[22] BernhardKerbl,GeorgiosKopanas,ThomasLeimkühler,andGeorgeDrettakis. 3DGaussian
splattingforreal-timeradiancefieldrendering. ACMTOG,42(4):139:1–139:14,2023. 1,2,3,
14
10[23] NikosKolotouros,ThiemoAlldieck,AndreiZanfir,EduardGabrielBazavan,MihaiFieraru,
andCristianSminchisescu. DreamHuman: Animatable3Davatarsfromtext. InNeurIPS,2023.
1
[24] JooChanLee,DanielRho,XiangyuSun,JongHwanKo,andEunbyungPark. Compact3D
Gaussianrepresentationforradiancefield. InCVPR,2024. 2
[25] JiahuiLei,YufuWang,GeorgiosPavlakos,LingjieLiu,andKostasDaniilidis. GART:Gaussian
articulatedtemplatemodels. InCVPR,2024. 2
[26] Zhengqi Li, Qianqian Wang, Noah Snavely, and Angjoo Kanazawa. InfiniteNature-Zero:
Learningperpetualviewgenerationofnaturalscenesfromsingleimages. InECCV,2022. 2
[27] ZheLi,ZerongZheng,LizhenWang,andYebinLiu. AnimatableGaussians: Learningpose-
dependentGaussianmapsforhigh-fidelityhumanavatarmodeling. InCVPR,2024. 2
[28] YiyiLiao,JunXie,andAndreasGeiger. KITTI-360: Anoveldatasetandbenchmarksforurban
sceneunderstandingin2Dand3D. TPAMI,45(3):3292–3310,2023. 2,6
[29] JaeHyunLimandJongChulYe. GeometricGAN. arXiv,1705.02894,2017. 6
[30] ChiehHubertLin,Hsin-YingLee,WilliMenapace,MengleiChai,AliaksandrSiarohin,Ming-
HsuanYang,andSergeyTulyakov. InfiniCity: Infinite-scalecitysynthesis. InICCV,2023. 1,3,
7,16
[31] JiaqiLin,ZhihaoLi,XiaoTang,JianzhuangLiu,ShiyongLiu,JiayueLiu,YangdiLu,Xiaofei
Wu,SongcenXu,YouliangYan,andWenmingYang. VastGaussian: Vast3DGaussiansfor
largescenereconstruction. InCVPR,2024. 2
[32] AndrewLiu,AmeeshMakadia,RichardTucker,NoahSnavely,VarunJampani,andAngjoo
Kanazawa. InfiniteNature: Perpetualviewgenerationofnaturalscenesfromasingleimage. In
ICCV,2021. 2
[33] XianLiu,XiaohangZhan,JiaxiangTang,YingShan,GangZeng,DahuaLin,XihuiLiu,and
ZiweiLiu. HumanGaussian: Text-driven3DhumangenerationwithGaussiansplatting. In
CVPR,2024. 1,2
[34] YangLiu,HeGuan,ChuanchenLuo,LueFan,JunranPeng,andZhaoxiangZhang. CityGaus-
sian: Real-timehigh-qualitylarge-scalescenerenderingwithGaussians. arXiv,2404.01133,
2024. 2
[35] TaoLu,MulinYu,LinningXu,YuanboXiangli,LiminWang,DahuaLin,andBoDai. Scaffold-
GS:Structured3DGaussiansforview-adaptiverendering. InCVPR,2024. 2
[36] BenMildenhall,PratulP.Srinivasan,MatthewTancik,JonathanT.Barron,RaviRamamoorthi,
andRenNg. NeRF:Representingscenesasneuralradiancefieldsforviewsynthesis. InECCV,
2020. 1,2,5
[37] SimonNiedermayr,JosefStumpfegger,andRüdigerWestermann. Compressed3DGaussian
splattingforacceleratednovelviewsynthesis. InCVPR,2024. 2
[38] Michael Niemeyer and Andreas Geiger. GIRAFFE: representing scenes as compositional
generativeneuralfeaturefields. InCVPR,2021. 6,7
[39] TaesungPark,Ming-YuLiu,Ting-ChunWang,andJun-YanZhu. Semanticimagesynthesis
withspatially-adaptivenormalization. InCVPR,2019. 2
[40] DespoinaPaschalidou,AmlanKar,MariaShugrina,KarstenKreis,AndreasGeiger,andSanja
Fidler. ATISS:autoregressivetransformersforindoorscenesynthesis. InNeurIPS,2021. 3
[41] ShenhanQian,TobiasKirschstein,LiamSchoneveld,DavideDavoli,SimonGiebenhain,and
MatthiasNießner. GaussianAvatars: Photorealisticheadavatarswithrigged3DGaussians. In
CVPR,2024. 2
[42] AmitRaj,SrinivasKaza,BenPoole,MichaelNiemeyer,NatanielRuiz,BenMildenhall,Shiran
Zada,KfirAberman,MichaelRubinstein,JonathanT.Barron,YuanzhenLi,andVarunJampani.
DreamBooth3D:subject-driventext-to-3Dgeneration. InICCV,2023. 1
[43] RenéRanftl,KatrinLasinger,DavidHafner,KonradSchindler,andVladlenKoltun. Towards
robustmonoculardepthestimation: Mixingdatasetsforzero-shotcross-datasettransfer. TPAMI,
44(3):1623–1637,2022. 6
[44] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu.
DreamGaussian4D:Generative4DGaussiansplatting. arXiv,2312.17142,2023. 2
[45] JohannesL.SchönbergerandJan-MichaelFrahm. Structure-from-motionrevisited. InCVPR,
2016. 6
[46] ZhijingShao,ZhaolongWang,ZhuangLi,DuotunWang,XiangruLin,YuZhang,Mingming
Fan,andZeyuWang. SplattingAvatar: Realisticreal-timehumanavatarswithmesh-embedded
Gaussiansplatting. InCVPR,2024. 2
[47] YuanShen,Wei-ChiuMa,andShenlongWang. SGAM:buildingavirtual3Dworldthrough
simultaneousgenerationandmapping. InNeurIPS,2022. 6,7
11[48] ZifanShi,YujunShen,JiapengZhu,Dit-YanYeung,andQifengChen. 3D-awareindoorscene
synthesiswithdepthpriors. InECCV,2022. 2
[49] JulianStraub,ThomasWhelan,LingniMa,YufanChen,ErikWijmans,SimonGreen,JakobJ.
Engel, Raul Mur-Artal, Carl Yuheng Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan,
BrianBudge,YajieYan,XiaqingPan,JuneYon,YuyangZou,KimberlyLeon,NigelCarter,
JesusBriales,TylerGillingham,EliasMueggler,LuisPesqueira,ManolisSavva,DhruvBatra,
Hauke M. Strasdat, Renzo De Nardi, Michael Goesele, Steven Lovegrove, and Richard A.
Newcombe. TheReplicaDataset: Adigitalreplicaofindoorspaces. arXiv,1906.05797,2019.
3
[50] StanislawSzymanowicz,ChristianRupprecht,andAndreaVedaldi. Splatterimage: Ultra-fast
single-view3Dreconstruction. InCVPR,2024. 2
[51] JiaxiangTang,ZhaoxiChen,XiaokangChen,TengfeiWang,GangZeng,andZiweiLiu. LGM:
largemulti-viewGaussianmodelforhigh-resolution3Dcontentcreation. arXiv,2402.05054,
2024. 2
[52] JiaxiangTang,JiaweiRen,HangZhou,ZiweiLiu,andGangZeng. DreamGaussian:Generative
Gaussiansplattingforefficient3Dcontentcreation. InICLR,2024. 1,2
[53] XiaoyangWu,LiJiang,Peng-ShuaiWang,ZhijianLiu,XihuiLiu,YuQiao,WanliOuyang,
TongHe,andHengshuangZhao. PointtransformerV3: simpler,faster,stronger. InCVPR,
2024. 5,9
[54] Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, and Ziwei Liu. CityDreamer: Compositional
generativemodelofunbounded3Dcities. InCVPR,2024. 1,2,3,4,6,7
[55] HaozheXie,HongxunYao,ShengpingZhang,ShangchenZhou,andWenxiuSun. Pix2Vox++:
Multi-scalecontext-aware3Dobjectreconstructionfromsingleandmultipleimages. IJCV,
128(12):2919–2935,2020. 1
[56] YinghaoXu,ZifanShi,WangYifan,HanshengChen,CeyuanYang,SidaPeng,YujunShen,and
GordonWetzstein. GRM:LargeGaussianreconstructionmodelforefficient3Dreconstruction
andgeneration. arXiv,2403.14621,2024. 2
[57] ZhenXu,SidaPeng,HaotongLin,GuangzhaoHe,JiamingSun,YujunShen,HujunBao,and
XiaoweiZhou. 4K4D:Real-time4Dviewsynthesisat4Kresolution. InCVPR,2024. 2
[58] YuanboYang,YifeiYang,HanleiGuo,RongXiong,YueWang,andYiyiLiao. Urbangiraffe:
Representingurbanscenesascompositionalgenerativeneuralfeaturefields. InICCV,2023. 6,
7,8,17
[59] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William T.
Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, and Charles Herrmann.
WonderJourney: Goingfromanywheretoeverywhere. InCVPR,2024. 3
[60] YeYuan,XuetingLi,YangyiHuang,ShaliniDeMello,KokiNagano,JanKautz,andUmar
Iqbal. Gavatar: Animatable3DGaussianavatarswithimplicitmeshlearning. InCVPR,2024. 2
[61] ShunyuanZheng,BoyaoZhou,RuizhiShao,BoningLiu,ShengpingZhang,LiqiangNie,and
YebinLiu. Gps-Gaussian: Generalizablepixel-wise3DGaussiansplattingforreal-timehuman
novelviewsynthesis. InCVPR,2024. 2
[62] Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, and
Song-HaiZhang. TriplanemeetsGaussiansplatting: Fastandgeneralizablesingle-view3D
reconstructionwithtransformers. InCVPR,2024. 2
12Inthisappendix,weofferextradetailsandadditionalresultstocomplementthemainpaper. Firstly,
weoffermoreextensiveinformationandresultsregardingtheablationstudiesinSec.A.Secondly,
wepresentadditionalexperimentalresultsinSec.B.
A AdditionalAblationStudyResults
A.1 QualitativeResultsforAblationStudies
EffectivenessofBEV-PointDecoder. Figure6givesaqualitativecomparisonasasupplementto
Table2, demonstratingtheeffectivenessofPointSerializerandPointTransformerinBEV-Point
Decoder. Removingeitherofthemsignificantlydegradesthequalityofthegeneratedimages.
Figure6: QualitativecomparisonofdifferentBEV-PointDecodervariants. Notethat“Ser.” and
“Trans.” represent“PointSerializer”and“PointTransformer”,respectively.
EffectivenessofDifferentSerializationMethods. Figure7providesaqualitativecomparisonasa
supplementtoTable3. AsshowninFigure7,usingeitherEquation13,theHilbertcurve,orbothfor
serializationisunlikelytoimpactthequalityofthegeneratedresults. Moreover,Equation13exhibits
significantlylowercomputationalcomplexitycomparedtotheHilbertcurve. Therefore,Equation13
isemployedinthePointSerializerduetoitslowercomputationaldemands.
Figure7: Qualitativecomparisonofdifferentserializationmethods. Notethat“Eq.13”denote
theserializationusedinPointSerializer.
13
✗.snarT
✗.reS
✓.snarT
✗.reS
✓.snarT
✓.reS
✗trebliH
✓31.qE
✓trebliH
✗31.qE
✓trebliH
✓31.qEA.2 DiscussiononGeneratedGaussianAttributes
In3D-GS[22],each3DGaussianpossessesmultipleattributes,includingXYZcoordinates,spherical
harmonics (SHs), opacity, rotation, and scale. These attributes are optimized using supervision
frommulti-viewimagestorepresentthescene. Inreconstruction,addingextraattributeslikeXYZ
offsetsandopacityenhancesrepresentationcapability. However,ascenecanhavemultiplevalid
reconstructionresultsbecausedifferentcombinationsof3DGaussianattributescanyieldthesame
renderingresult. Forinstance,changingthecolorofone3DGaussiancanbeequivalenttooverlaying
multipleGaussianswithvaryingopacitiesoradjustingthescaleofnearbyGaussianswithsimilar
colors. Thisambiguitycausesinstabilitywhenoptimizingalltheseattributessimultaneouslyincity
generation.
DuetothecarefullydesignedBEV-Pointinitialization,allpointsareevenlydistributedonthesurface
ofobjects. Therefore, GaussianattributesotherthanRGBcanbeleftunestimatedanddefaulted.
Table4andFigure8provideaquantitativeandqualitativecomparisonofvariousattributesgenerated
for 3D Gaussians, demonstrating that optimizing 3D Gaussian attributes beyond RGB not only
complicatesnetworkconvergencebutalsosignificantlyimpactsthequalityofthegeneratedresults.
Table4: Quantitativecomparisonofdifferentattributesgeneratedfor3DGaussians. Thebest
resultsarehighlightedinbold. Notethat“-”inCEindicatesthatCOLMAPcannotestimatecamera
posesfromthegeneratedimages.
GeneratedAttributes GoogleEarth KITTI-360
RGB ∆XYZ Opacity Scale FID↓ KID↓ DE↓ CE↓ FID↓ KID↓
✓ ✗ ✗ ✗ 86.94 0.090 0.136 0.057 29.5 0.017
✓ ✓ ✗ ✗ 371.15 0.468 0.367 - 281.5 0.342
✓ ✓ ✓ ✗ 384.81 0.485 0.401 - 256.9 0.297
✓ ✓ ✓ ✓ 535.49 0.709 0.470 - 486.3 0.605
GoogleEarth KITTI-360
Figure8: Qualitativecomparisonofdifferentattributesgeneratedfor3DGaussians. Notethatc,
∆x,α,andsdenotetheRGBcolor,XYZoffsets,opacity,andscaleinthegenerated3DGaussian
attributes,respectively.
14
𝒄𝒄
𝒙𝒙Δ𝒄𝒄
𝛼𝛼𝒙𝒙Δ𝒄𝒄
𝒔𝒔𝛼𝛼𝒙𝒙Δ𝒄𝒄B AdditionalExperimentalResults
B.1 BuildingInterpolation
AsshowninFigure9,GaussianCityshowcasesthecapabilitytointerpolatebuildingstylescontrolled
bythevariablez.
Figure 9: Linear interpolation along the building style. The style of each building gradually
changesfromlefttoright. Inthefirstrow,onlythestylesofthehighlightedbuildingsarealtered,
whileinthesecondrow,thestylesofallbuildingsarechanged.
B.2 Relighting
From the explicit representation of 3D Gaussians, relighting is much simpler than NeRF-based
methods. UsingLumaAI’s3DGaussiansPlugin1,thegeneratedcitycanbeimportedintoUnreal
Engine,enablinghighlyrealisticlightingeffects,asillustratedinFigure10. However,theplugin
isstillunderdevelopment,soshadoweffectsarenotwellgenerated. Webelievethisissuewillbe
resolvedsoon.
Figure10: RelightingeffectsinUnrealEngine5. Fromlefttoright,therelightingeffectisshown
withincreasinglightintensity.
1https://www.unrealengine.com/marketplace/product/66dd775fa3104ecfb3ae800b8963c8b9
15B.3 AdditionalQualitativeComparison
In Figures 11 and 12, we provide more visual comparisons with state-of-the-art methods on
GoogleEarthandKITTI-360,respectively.
Figure11: QualitativecomparisononGoogleEarth. Notethat“Pers.Nature”isshortfor“Persis-
tentNature”[5]. ThevisualresultsofInfiniCity[30]areprovidedbytheauthorsandzoomedfor
optimalviewing.
16
erutaN.sreP
remaerDenecS
ytiCinifnI
remaerDytiC
ytiCnaissuaG
erutaN.sreP
remaerDenecS
ytiCinifnI
remaerDytiC
ytiCnaissuaGGSN UrbanGIRAFFE GaussianCity
Figure12: QualitativecomparisononKITTI-360. ThevisualresultsofUrbanGIRAFFE[58]are
providedbytheauthorssincethetrainingcodeandpretrainedmodelareunavailable.
17