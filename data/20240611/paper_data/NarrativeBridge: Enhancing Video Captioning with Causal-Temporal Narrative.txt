NarrativeBridge: Enhancing Video Captioning with
Causal-Temporal Narrative
AsmarNadeem∗1,FaeghehSardari1,RobertDawes2,SyedSameedHusain1
AdrianHilton1,ArminMustafa1
1CVSSP,UniversityofSurrey,Guildford,UnitedKingdom
2BBCResearchandDevelopment,UnitedKingdom
Abstract
Existingvideocaptioningbenchmarksandmodelslackcoherentrepresentationsof
causal-temporalnarrative,whichissequencesofeventslinkedthroughcauseand
effect,unfoldingovertimeanddrivenbycharactersoragents.Thislackofnarrative
restrictsmodels’abilitytogeneratetextdescriptionsthatcapturethecausaland
temporal dynamics inherent in video content. To address this gap, we propose
NarrativeBridge,anapproachcomprisingof:(1)anovelCausal-TemporalNarrative
(CTN) captions benchmark generated using a large language model and few-
shotprompting,explicitlyencodingcause-effecttemporalrelationshipsinvideo
descriptions,evaluatedautomaticallytoensurecaptionqualityandrelevance;and
(2)adedicatedCause-EffectNetwork(CEN)architecturewithseparateencoders
forcapturingcauseandeffectdynamicsindependently,enablingeffectivelearning
andgenerationofcaptionswithcausal-temporalnarrative. Extensiveexperiments
demonstrate that CEN is more accurate in articulating the causal and temporal
aspectsofvideocontentthanthesecondbestmodel(GIT):17.88and17.44CIDEr
on the MSVD and MSR-VTT datasets, respectively. The proposed framework
understandsandgeneratesnuancedtextdescriptionswithintricatecausal-temporal
narrative structures present in videos, addressing a critical limitation in video
captioning. Forprojectdetails,visithttps://narrativebridge.github.io/.
1 Introduction
Videocaptioningaimstogeneratetextualdescriptionsthatcapturethevisualinformationandtemporal
dynamicsinvideos[48]. Researchhasprimarilyfocusedondevelopingnewmethodstoimprovethe
accuracyofvideocaptioningmodelsonwell-establishedbenchmarkdatasets,MSR-VTT[55]and
MSVD[7]. State-of-the-art(SOTA)approaches[19,44,56,32,51,9]proposednewarchitecturesto
betteralignthegeneratedcaptionswiththeprovidedground-truth. However,theseeffortsenhance
models’accuracyonexistingevaluationcriteria,withoutmodifyingthebenchmarkdatasetsortheir
groundtruthannotationstoaddressthelackofcoherentrepresentationsofcausal-temporalnarrative-
constructionandinterpretationofasequenceofeventslinkedthroughcauseandeffect,unfolding
overtimeandspace,andoftendrivenbyentities(charactersoragents)actingwithintention[52].
AsdiscussedbyWilkensetal. [52],causalrelationships[16]arescenarioswhereoneevent(cause)
directlyinfluencestheoccurrenceofanotherevent(effect),whiletemporalunderstanding[4]involves
recognizingthechronologicalorderofevents,acrucialcomponentinestablishingcausality. Existing
ground-truthcaptionsinpopularbenchmarkslackcausal,temporalandnarrativeinformation. Figure
1illustratestheimportanceofcausal-temporalnarrativeontheMSR-VTTdataset[55]. Intheinput
videosequence,acarisfirstshowndrivingrecklesslythroughanopenfieldandflippingover,which
representsthecauseevent. Thisisthenfollowedbyasceneshowingtheseverelydamagedcar,and
subsequently, a group of guys starting to play beer pong, which represents the effect event. The
causal-temporalnarrativewouldconnecttheseeventsinatemporalorder,linkingtherecklessdriving
∗CorrespondingAuthor(asmar.nadeem@surrey.ac.uk)
Preprint.Underreview.
4202
nuJ
01
]VC.sc[
1v99460.6042:viXraInput Video:
Original 'a car crashes and guys play beer pong', 'a car driving through an open field kicking up dirt', a car flipping over', 'a
Captions car get wracked', 'a car is being flipped over', 'a dirt vehicle riding and rolling', 'a dune buggy flipping over', 'a four
wheeler wrecking', 'a monster truck flips on its side then several young men shout while playing beer pong', 'a
person drives an offroad car around a field', 'a person flipping a go kart while a crowd cheers', 'a race truck is
crashing', 'a truck rolls over itself and boys cheer on a friend', 'a truck tumbles over on itself', 'a tumbler crashes on
a dirt road and then a group of guys play beer pong', 'a vehicle flips over', 'a type of monster truck crashes and
Prompt men are shown celebrating', 'an off road vehicle crashing', 'crashing of a car while driving', 'footage from a
EvLaL lLuL aMMtion monster truck style event followed by a frat party'
CTN Cause: 'a car drove recklessly through an open field flipping over'
Effect: 'the car was severely damaged and a group of guys started playing beer pong'
Caption
Figure 1: Comparison of Original captions vs. Causal-Temporal Narrative (CTN) caption to illustrate the
inclusionofcausal-temporalnarrative.
and resulting car crash to the group’s subsequent behavior of playing beer pong. This example
alignswiththedefinitionofnarrativeprovidedbyWilkensetal. [52],whichemphasizestherole
of cause-effect relationships, the perception of narrativity in videos, and highlights the temporal
sequenceofeventsthatiscrucialforunderstandingthecausal-temporalnarrative. Figure1highlights
thelimitationsoftheoriginalcaptionsinexistingbenchmarkdatasets,suchasMSR-VTT[55]. The
original captions focus on isolated events or actions, such as "a car flipping over" or "a monster
truckflipsonitsside,"lackingcontextualnarrativeandcausal-temporalrelationshipsbetweenevents.
Consequently,modelstrainedonthesecaptionssufferfromthesamelimitations.
Tobridgethisgap,weintroduceNarrativeBridge,anovelframeworkencompassinganewbenchmark
and architecture tailored for causal-temporal narrative learning in video captioning. Our Causal-
TemporalNarrative(CTN)captionsbenchmarkleveragesalargelanguagemodel(LLM)andfew-shot
promptingtogenerateenhancedvideodescriptionsthatexplicitlyencodecause-effectrelationships
andtemporalsequences,asexemplifiedinFigure1. OurCTNcaptionsbenchmark,generatedusing
theapproachdetailedinSection3,establishesaclearconnectionbetweenthecause(recklessdriving)
andtheeffect(damagedcarandsubsequentbehaviorofthegroup). Bygeneratingasingle,coherent
captionthatencapsulatesthecausal-temporalnarrativeofthevideo,ourCTNcaptionsbenchmark
enablesmodelstobetterunderstandandarticulatethecausality,sequence,andsignificanceofevents
withinthebroadervideocontext[52]. Thisapproachaddressesthelimitationsofexistingbenchmark
datasetsandemphasizestheimportanceofincorporatingcausal-temporalnarrativeunderstanding
intovideocaptioningmodelstogenerateaccurate,informative,andcontextuallyrelevantdescriptions.
Toensurethequalityandrelevanceofthegeneratedcaptions,weemployanautomaticevaluation
framework that compares the CTN captions with the video content, keeping or discarding them
basedonascorethreshold. Thisapproachaddressesthelimitationsofexistingbenchmarkdatasets
andemphasizestheimportanceofincorporatingcausal-temporalnarrativeunderstandingintovideo
captioningmodelstogenerateaccurate,informative,andcontextuallyrelevantdescriptions.
ExistingSOTAvideocaptioningmethodsthatusedifferentarchitecturessuchasLSTM[32],GNN
[17], andTransformer[51], struggletoeffectivelylearncausal-temporalnarrativefromtheCTN
captions. These architectures are designed to capture the overall semantics in videos but lack
dedicatedmechanismstoexplicitlymodelthecause-effectrelationshipsandtemporalsequences.
Asaresult,thecaptionsgeneratedbythesemethodsfailtoarticulatethecomplexcausal-temporal
dynamics in the videos, as demonstrated in results (Section 4.4). To address this challenge, we
proposetheCause-EffectNetwork(CEN),anarchitecturespecificallydesignedtocapturecauseand
effectdynamicsusingdedicatedencoders. Byseparatelyencodingthecauseandeffectcomponents
ofthevideoandthencombiningthemtogeneratethefinalcaption,CENisabletobetterunderstand
andarticulatethecausal-temporalnarrative. Theprimarycontributionsofourworkare:
• IntroducingforthefirsttimeCTNcaptionsbenchmarkwithhigh-quality,coherentcausal-
temporalnarrativecaptions,automaticallyevaluated,toaddressthelackofcoherentrepre-
sentationsinvideocaptioningbenchmarks.
• AnovelCENnetworkforlearningcausal-temporalnarrativefromvideostoaddresslimita-
tionofSOTAvideocaptioningmethodstolearnnarrativeinformationonCTN.
2• ExtensiveexperimentsdemonstratingCEN’ssuperiorperformanceonCTN,settinganew
SOTAincausal-temporalnarrativelearning.
Our work marks a significant step forward in video captioning research by explicitly addressing
thechallengesofcausal-temporalnarrativeunderstanding. TheCTNcaptionsbenchmarkandthe
CEN architecture provide a comprehensive framework for learning and generating contextually
relevantandtemporallycoherentvideodescriptions. Thisframeworkhasthepotentialtodrivethe
developmentofmoresophisticatedvideounderstandingsystemsandenhancevariousapplications,
suchasvideosummarization,contentretrieval,andhuman-robotinteraction.
2 RelatedWork
2.1 Benchmarks
MSVD[7]isabenchmarkfocusedonhumanactivities,providingaplatformforevaluatingvideo
captioningmodels. ThecaptionsinMSVDoftendescribetheobservableactionswithoutdelving
intotheunderlyingmotivationsorthecause-effectrelationshipsbetweentheevents. MSR-VTT[55]
isalarge-scalebenchmarkwithdiversevideocontent, encompassingawiderangeoftopicsand
genres. Thecaptionsoftenfocusondescribingtheobservablecontentwithoutcapturingthecausal
linksbetweentheeventsorthetemporalprogressionofthenarrative. Asaresult,modelstrainedon
MSVDandMSR-VTTmaystruggletogeneratedescriptionsthataccuratelyreflectthecausaland
temporaldynamicsinthevideos.
WhilerecentbenchmarkslikeNExT-QA[54]andEgoSchema[30]havemadestridesinincorporating
causal and temporal reasoning, they focus on specific aspects of video understanding, such as
question-answeringandlong-formvideocomprehension. NExT-QA[54]introducesmulti-choice
andopen-endedquestion-answeringtasksthattargetcausalandtemporalactionreasoningbutdo
not address the generation of holistic video descriptions. EgoSchema [30], on the other hand,
emphasizeslong-formvideounderstandingandtemporalreasoningbutdoesnotexplicitlyfocuson
causal-temporalnarrativeforvideocaptioning.
Similarly,effortslikeVCR[60],V2C[12],andMotivation[49]integratecausalityintotheiranalysis
ofvisualdescriptionorquestion-answering,relyingheavilyoncommonsensereasoningforgenerating
predictions. VCR [60] focuses on visual commonsense reasoning, V2C [12] aims to generate
commonsense descriptions for video captioning, and Motivation [49] explores the prediction of
motivationsbehindactionsinvideos.However,theseworksprimarilyrelyoncommonsensereasoning
anddonotdelveintothecausalandtemporalstructuresunderpinningvideonarratives. Weintroduce
anewbenchmarkCTNspecificallydesignedtocaptureandevaluatecausal-temporalnarrativein
videocaptioning.
2.2 VideoCaptioning
VideocaptioningtechniqueshaveevolvedfromLSTM-based[13,42,32]frameworkstothelatest
designsusingSOTAGNNs[17,62,34]andTransformers[51,24,58],withafocusonenhancingthe
complexityofcaptionsthroughtheinjectionofmultimodaldata. Despitetheseadvancements,current
architecturesoftenstruggletocapturetheintricatetemporalsequencesandcausalrelationshipsin
videostorytelling. Tobridgethisgap,videocaptioningcanbenefitfromcross-fertilizationwithideas
andstrategiesdevelopedinrelatedfields,suchasactionrecognition[43,50,21,53,8,14,35,38,
2,36,61],eventlocalization[45,25,11,26],andquestion-answering[1,18,39,15,59,22,40,33].
Theintegrationofcausalreasoning[27,57]hasshownpromiseinenhancingtheabilityofneural
networkstodiscerncausalrelationships,leadingtoimprovedperformanceinimagecaptioning[27]
and visual question answering [57]. However, current SOTA models still struggle to effectively
handlethenarrativecomplexityinvideos,particularlyintermsofcausalandtemporalprogression.
Toaddressthislimitation,thereisaneedtodevelopnewvideocaptioningmodelsthatcanovercome
theshortcomingsofcurrentframeworks.
Inlightofthesechallenges,ourworkaimstobridgethegapinexistingvideocaptioningbenchmarks
andmodelsbyintroducingNarrativeBridge,acomprehensiveframeworkthatencompassestheCTN
captionsbenchmarkandtheCENarchitecture. Byexplicitlyaddressingthelimitationsofcurrent
approachesandprovidingaplatformforcausal-temporalnarrativelearning,ourworkpavestheway
3Input Video:
Original LLM Caption Score > Final CTN
Prompt (CTN Caption
Captions Evaluation Caption
Generation)
Score <
Figure2:CTNcaptiongenerationpipeline.θindicatesathreshold.
forthedevelopmentofmoresophisticatedvideocaptioningmodelscapableofgeneratingcontextually
relevantandtemporallycoherentdescriptions.
3 Method
NarrativeBridge is a comprehensive approach to video captioning that addresses the limitations
ofexistingbenchmarksandmodelsincapturingcausal-temporalnarrative. Itconsistsoftwokey
interconnectedcomponents: (i)theCTNcaptionsbenchmark,whichprovidesarichrepresentation
ofcause-and-effectrelationshipsandeventsequencesinvideocontent,and(ii)theCENarchitecture,
whichisspecificallydesignedtolearnandarticulatethesecausal-temporalnarrativeelements.
Problem Statement: Existing video captioning benchmarks and models struggle to coherently
capture the intricate causal-temporal narrative inherent in video content. As shown in Figure 1,
originalcaptionsfrombenchmarkdatasetsoftenfocusonisolatedevents,failingtocapturethecausal
andtemporalrelationshipsthatdrivethevideo’snarrative. SOTAvideocaptioningmethodsalsofall
shortinarticulatingthecause-and-effectrelationshipsandtemporalsequencesthatdrivetheeventsin
thevideo(seeSection4.4).
You are an advanced language model tasked with generating causal-temporal narrative captions for a
video. However, you cannot directly access the video itself. Instead, you will be provided with a
series of captions that outline the key events and scenes in the video. Your task is to generate a
concise Cause and Effect scenario, based on the information provided in the descriptive captions.
Be careful, your generated Cause and Effect statements should fulfill the following requirements:
1. Your narrative should be grounded in the information provided by the descriptive captions.
2. Cause and Effect scenario is relevant.
3. It should not introduce any new events or details not mentioned.
4. Avoid implying conclusions.
5. Maintain temporal consistency with the provided captions.
6. Use plain English and direct sentences.
7. Cause and Effect statements each limited to a maximum of 15 words.
8. Do not include any additional text before or after the JSON object.
Here are the examples of Cause and Effect:
[Examples]:
[{’Cause’: ’the student overslept due to a malfunctioning alarm clock’, ’Effect’: ’missed catching the
bus to school’}, {’Cause’: ’she absentmindedly skipped applying moisturizer after taking a long hot
shower’, ’Effect’: ’her skin became dry and flaky’}, {’Cause’: ’he carelessly neglected taking his
prescribed allergy medication’, ’Effect’: ’suffered a severe sneezing fit’}, {’Cause’: ’the exhausted
soccer player recklessly fouled an opponent in the penalty area’, ’Effect’: ’the opposing team was
awarded a crucial penalty kick’}, {’Cause’: ’due to unforeseen road closures they found themselves
stuck in heavy traffic’’, ’Effect’: ’missed out on experiencing the opening act of the concert’}]
Now please generate only one Cause and Effect presented in a JSON format based on the following
descriptive captions.
[Descriptive Captions]:
<descriptive_captions>
[Causal Temporal Narrative]:
Prompt1:LLMPromptusedinourCTNcaptionsgeneration.
43.1 CTNCaptionsBenchmark
Toaddressthechallengesofexistingbenchmarksinrepresentingcausalandtemporalrelationships
within video content, we propose an approach that harnesses the potential of LLMs through the
few-shot prompting technique [5]. Our method generates CTN captions without the need for
modelfine-tuning,leveragingtheinherentgenerativecapabilitiesofLLMstoproducecaptionsthat
encapsulatecausal-temporalnarrativestructures.Figure2showstheCTNcaptiongenerationpipeline,
whichconsistsoftwokeysteps: promptdesign,LLM-basedcaptiongenerationandevaluation.
PromptDesign: ThepromptdesignstepiscrucialinguidingtheLLMtogeneratecausal-temporal
narrativecaptions. TheinputtotheCTNcaptiongenerationaretheoriginalvideocaptionsfrom
existing benchmarks. We design a prompt that include a small set of carefully selected example
captions,illustratingthedesiredoutputstructureandhighlightingthekeyaspectsofcausal-temporal
narrative. WedesignPrompt1whichguidesLLMtogenerateaccurateandconcisecausal-temporal
narrative. Thepromptsetsclearrequirementsforgroundingthecaptionsintheprovideddescrip-
tivecontext,maintainstemporalconsistency,andavoidsunsupporteddetailsorconclusions. The
illustrativeexamplesdemonstratethedesiredformat,facilitatingthegenerationofplainlywritten,
length-constrainedCauseandEffectstatementsthateffectivelycapturethevideo’scausal-temporal
narrative. Byexplicitlyoutlininginstructionsandconstraints,thecarefullydesignedpromptsteers
theLLM’sgenerativecapabilitiestowardsproducingcontextuallyrelevantandnarrativelycoherent
CTNcaptions. FurtherdetailsareprovidedinAppendixBandC.
CTNcaptionGenerationandEvaluation: WesendPrompt1intotheMixtralofExpertsLLM
[20],whichgeneratesCTNcaptionsthataccuratelyreflectthecausal-temporalnarrativeinthevideo.
TheLLM’sadvancednaturallanguageunderstandingandgenerationcapabilities,combinedwith
thefew-shotpromptingapproach,enabletheproductionofcaptionsthatencapsulatecomplexcausal
and temporal relationships. To ensure the quality and relevance of the generated CTN captions,
we employ the EMScore [41] metric for evaluation. EMScore directly measures the consistency
betweenacaptionandthevideocontent,andhasbeenshowntobemoreeffectivethanothermetrics
[10] in evaluating video relevance without referenced captions. We set a threshold (θ = 0.2) for
theEMScorevalue,indicatingadequaterelevancetothevideo. CaptionswithanEMScoreabove
theθarekept,whilethosebelowtheθarediscarded,andtheLLMgeneratesanewcaption. This
iterativerefinementprocesscontinuesuntilacaptionmeetstheEMScorethreshold,ensuringthat
thefinalcaptionsefficientlydescribetherelevanteventsinthevideo. Furtherdetailsareprovidedin
AppendixD.
output: a car drove recklessly through an open field
Stage 1 flipping over the car was severely damaged and a group of Stage 1
Similarity Stage 2guys started playing beer pong Similarity
Video Decoder Video
XEmbedding Text Mean Pooling Encoder + Encoder Mean Pooling EmbeddingX
Text Text
Embedding Embedding
Cause Video features Effect Video features
CauseText Encoder - Tcause Effect Text Encoder - Teffect
(Transformer) (Transformer)
Token : Trainable Token
0 1 2 3 4 9 Pos+ ition Caus (Pe r V etid rae io n eE dn Cco Ld IPe -r V - i TE )cause : Frozen Effe (c Pt r V ei td rae io n eE dn c Co Ld IPe -r V - i TE )effect Pos+ ition0 1 2 3 4 13
a car droverecklessly through overEmbedding Embedding the car was severelydamaged pong
Input (Cause): 'a car drove recklessly through an open Input (Effect): 'the car was severely damaged and a
field flipping over' group of guys started playing beer pong'
Input Video
Figure3:Thetwo-stageCause-EffectNetwork(CEN)architecture.Stage1:SeparateCause(E )andEffect
cause
(E )videoencoders,pretrainedusingCLIP-ViT,learnspecializedvideorepresentations.Corresponding
effect
textencoders(T andT )encodethecauseandeffectportionsoftheCTNcaption.Contrastivelosses
cause effect
areappliedtoalignthevideoandtextembeddings. Stage2: Thelearnedcauseandeffectvideofeaturesare
encodedseparately(Enc andEnc )andconcatenatedbeforebeinginputtothedecoder, which
cause effect
generatesthefinalCTNcaption.
3.2 ProposedCause-EffectNetwork(CEN)
ExistingSOTAvideocaptioningmethodsbasedonLSTMs[32],GNNs[17],andTransformers[51]
lackdedicatedmechanismstoexplicitlymodeltheintricatecausal-temporalnarrativesequences. Our
proposedCENemploysatwo-stageapproachtoaddressthislimitation. InStage1(Section3.2.1),
CENlearnsspecializedcauseandeffectvideorepresentationsusingseparateencoderstrainedonthe
correspondingportionsoftheCTNcaptions. Theinputisrawvideoframes,andtheoutputisthe
learnedcauseandeffectfeaturesextractedfromtheencoders. ThelearnedvideofeaturesfromStage
51arethenpassedtoStage2(Section3.2.2),whichutilizesasequence-to-sequencetransformer-based
networkwithtwonewencoderstogeneratethefinalcaptions. Theparametersofstage1encodersare
frozenatthisstage. Thecauseandeffectvideofeaturesareseparatelyencodedandthenconcatenated
beforebeinginputtothedecoder. Thisallowsthemodeltosynthesizethespecializedrepresentations
learnedinStage1andgeneratecaptionsthataccuratelycapturethecausalandtemporalrelationships
inthevideos.
3.2.1 Stage1: CausalVisualEncoding
FortheCENnetworktolearncausal-temporalnarrative,wedesigntwoseparatevisualencoding
modelstrainedontwopartsofthecaptions(seeFigure3).
(1)CauseVideoEncoderE : Trainedonthecausecaptionspartdescribingtheinitiatingevents;
cause
(2)EffectVideoEncoderE : Trainedontheeffectpartofthecaptionscapturingtheconse-
effect
quentialoutcomes.
We instantiate E and E as two separate instances of the CLIP-ViT (ViT-B/32) model
cause effect
[37],adaptedfromCLIP4Clip[29]. ForbothCauseandEffecttextencoders(T andT
cause effect
respectively), weemploya12-layertransformerwithahiddensizeof512and8attentionheads,
usingtheweightsfromthepre-trainedCLIP[37]textencoder. AsinCLIP[37],the[EOS]token’s
representationfromthelastlayerofthetextencodersisusedasthefeaturerepresentationforthe
inputtext. SimilartoCLIP4Clip,weadoptcosinesimilaritytomeasurethesimilaritybetweenthe
videoembedding(meanpooled)andthetextembedding. Givenavideoembeddingrˆ andatext
i
embeddingt ,thesimilarityfunctionisdefinedass(rˆ,t )=
t⊤ jrˆi
. Duringtraining,eachmodel
j i j |tj||rˆi|
isoptimizedusingacontrastiveloss,asinCLIP[37],overitsrespectivecause/effectportionfrom
CTNcaptions. ForabatchofN video-causetextpairs(v ,c )andvideo-effecttextpairs(v ,e ),the
i i i i
contrastivelossesforthecauseandeffectvideoencodersaredefinedas:
L =L (E (v ),T (c ))+L (E (v ),T (c )), (1)
cause v2t cause i cause i t2v cause i cause i
L =L (E (v ),T (e ))+L (E (v ),T (e )), (2)
effect v2t effect i effect i t2v effect i effect i
whereL andL arethevideo-to-textandtext-to-videocontrastivelosses,respectively,definedas:
v2t t2v
L =−
1 (cid:88)N
log
exp(s(rˆ i,t i))
, (3) L =−
1 (cid:88)N
log
exp(s(rˆ i,t i))
(4)
v2t N
i=1
(cid:80)N j=1exp(s(rˆ i,t j)) t2v N
i=1
(cid:80)N j=1exp(s(rˆ j,t i))
Aftertraining,wefreezetheweightsofE andE forthesecondstage,whereweextract
cause effect
thecauseandeffectvideofeatures,respectively.
3.2.2 Stage2: CTNCaptionGeneration
Inthesecondstage(seeFigure3),weemployasequence-to-sequencetransformer-basednetwork,
consistingoftwoseparateencoders(Enc andEnc )forprocessingcauseandeffectvideo
cause effect
features,respectively,andadecoderDecforgeneratingthefinalCTNcaptions. Theencodedcause
andeffectvideorepresentationsareconcatenatedbeforebeingsenttothedecoder. Bothencodersand
thedecoderareinitializedwiththeweightsfromthepre-trainedUni-VLmodel[28]. Givenavideov,
wefirstextractthecauseandeffectvideofeaturesusingthepre-trainedE andE encoders:
cause effect
F =E (v), (5) F =E (v). (6)
cause cause effect effect
TheextractedfeaturesF andF areseparatelyencodedusingEnc andEnc ,
cause effect cause effect
eachconsistingoftwotransformerlayers,toobtaintherespectiveencodedrepresentations: h =
cause
Enc (F )andh =Enc (F ).
cause cause effect effect effect
The encoded cause and effect representations are concatenated to form a single representation
h =[h ;h ],whichistheninputtothedecoderDec,consistingoftwotransformer
concat cause effect
layers,togeneratetheCTNcaptionyˆbyattendingtoh ateachtimestept:
concat
yˆ =Dec(h ],yˆ ), (7)
t concat <t
whereyˆ denotesthepreviouslygeneratedwords. Fortrainingandevaluation, wecombinethe
<t
causeandeffectpartsoftheground-truthCTNcaptionwithaspaceinbetweentoformasingle,
6coherentcaption. Thiscombinedcaptionservesasthetargetforthedecoderduringtrainingand
thereferenceforevaluatingthegeneratedcaptionsusingdiffferentevaluationmetrics. Theentire
networkistrainedend-to-endusingthecross-entropylossbetweenthegeneratedcaptionyˆandthe
ground-truthCTNcaptiony:
T |V|
1 (cid:88)(cid:88)
L =− y log(yˆ ), (8)
caption T t,c t,c
t=1c=1
where: T isthelengthofthecaption. |V|isthesizeofthevocabulary. y istheground-truthlabel
t,c
forthec-thwordattimestept(1ifthewordispresent,0otherwise). yˆ isthepredictedprobability
t,c
ofthec-thwordattimestept. Finally,ourtotallossL is:
total
L =L +L +L (9)
total cause effect caption
Thistwo-stageapproachfirstencodesthecausalaspectsintothevideorepresentations,facilitatingthe
generationofcoherentcausal-temporalnarrativedescriptionsinthesecondstage(seeExperiments
section4)togenerateCTNcaptionsasfinaloutput.
4 Experiments
We conduct extensive experiments to validate our proposed CEN architecture and the new CTN
videocaptioningbenchmark. Thissectiondetailsthedatasetsused,theexperimentalsetup,andthe
evaluationmetrics. Wealsopresentcomprehensiveresultsandevaluationagainstthestate-of-the-art
videocaptioningmethodsdemonstratingtheperformanceofournetworkontheCTNbenchmark.
4.1 Datasets
WeevaluateourCTNcaptionsandCENontwowidely-usedvideocaptioningdatasets: 1)MSR-VTT
[55],consistsof10,000videoswith200,000human-annotatedcaptions,andMSVD[7],with1,970
videosfocusedonhumanactivitieswith78,800captions. Thecaptionsinthesedatasetslackexplicit
representations of the causal relationships, temporal dynamics, motivations, intentions, and the
narrativeofeventsandhumanactionsinthevideos. Toaddresstheselimitations,wegenerateCTN
captionsusingtheMixtralofExpertsLLM[20]asdetailedinSection3.1.
4.2 EvaluationMetrics
ForquantifyingtheperformanceofourmethodCENagainsttheSOTAmethods,weusethreemetrics
asper[6];CIDEr(C)[47]:Measuresalignmentofgeneratedcaptionswithreferencesbyhighlighting
frequently occurring terms, capturing the ability to reproduce salient causal-temporal narrative
elements. ROUGE-L(R-L)[23]: Evaluatesthelongestcommonsubsequencebetweengenerated
and reference captions, considering both precision and recall for assessing semantic similarity,
includingtemporaldynamicsandcausalrelationships. SPICE(S)[3]: Evaluatessemanticquality
byconsideringtheoverlapofscenegraphsbetweengeneratedandreferencecaptions,effectively
assessingtheabilitytocaptureandexpressunderlyingcausalrelationshipsandeventsequences.
4.3 ImplementationDetails
We generate the CTN captions using the Mixtral of Experts LLM [20], running on A100-80GB
GPUs. Allmodels,includingourproposedCENarchitectureandtheSOTAmodels(SEM-POS[32],
AKGNN[17],andGIT[51]),areimplementedusingPyTorch. FortheSOTAmodels,wefollow
thehyperparametersettingsspecifiedintheirrespectivemethodsfortrainingontheMSR-VTTand
MSVDdatasets. OurCENmodelistrainedusingtheAdamoptimizerwithlearningratesof1×10−4
(stage1)and1×10−6(stage2)andabatchsizeof64for10epochs(stage1)and50epochs(stage
2). FurtherdetailsareprovidedintheAppendixA.
4.4 ResultsandDiscussion
WecomprehensivelyevaluatetheperformanceofourproposedCENarchitectureagainstseveral
SOTA methods, including SEM-POS [32], AKGNN [17], and GIT [51], on our CTN captions
generatedforMSR-VTT[55]andMSVD[7]benchmarkdatasets.
7Table2:AblationstudyresultsontheMSVDandMSR-
Table1: ComparisonofourCENarchitectureagainst VTTdatasets. E ,OnlyE andOnlyE are
JointCE cause effect
SOTAmethodsontheMSVDandMSR-VTTdatasets.variantsofourCENarchitecture,whileZeroShotv.v.
Thebestresultsineachcategoryareinbold. R-L,C, and Fine-tune v.v. represent cross-dataset evaluation
and S denote ROUGE-L, CIDEr, and SPICE scores, settings.Thebestresultsineachcategoryareinbold.
respectively.
MSVD MSRVTT
Method
MSVD MSRVTT R-L(↑) C(↑) S(↑) R-L(↑) C(↑) S(↑)
Method
R-L(↑) C(↑) S(↑) R-L(↑) C(↑) S(↑) EJointCE 30.93 55.72 17.04 27.34 45.97 15.07
SEM-POS[32] 25.39 37.16 14.46 20.11 26.01 12.09 OnlyEcause 30.72 56.42 18.43 27.19 47.10 15.21
AKGNN[17] 25.11 35.08 14.55 21.42 25.90 11.99 OnlyEeffect 30.70 57.14 17.89 27.24 45.58 15.19
GIT[51] 27.51 45.63 15.58 24.51 32.43 13.70 ZeroShotv.v. 27.16 39.65 14.45 24.73 29.76 12.26
CEN(Ours) 31.46 63.51 19.25 27.90 49.87 15.76 Fine-tunev.v. 31.78 65.60 19.39 27.47 47.74 15.65
CEN(Ours) 31.46 63.51 19.25 27.90 49.87 15.76
CTN vs Original captions - MSRVTT CTN captions Cause-Effect Mapping - MSRVTT
CTN Cause
Original Effect
(a) (b)
Figure4: (a)UMAPvisualizationofvideofeatureslearnedfromCTN(red)andoriginal(blue)captionson
MSR-VTT,showingnon-overlappingfeaturespaces.(b)UMAPvisualizationofvideofeatureslearnedfrom
cause(black)andeffect(orange)partsofCTNcaptionsonMSR-VTT,showingnear-completeoverlap.
QuantitativeResults:CENoutperformedallSOTAmethodsacrossallmetricsanddatasetsasshown
inTable1. OnMSVD,CENsurpassedthenextbest(GIT)by3.95ROUGE-L,17.88CIDEr,and
3.67SPICEpoints. OnMSR-VTT,CENledGITby3.39ROUGE-L,17.44CIDEr,and2.06SPICE
points. These significant gains highlight CEN’s effectiveness in capturing causal narratives and
temporaldynamics. TogainfurtherinsightsintotheeffectivenessoftheCTNcaptionsincapturing
causal-temporal narratives, we visualize the video feature representations learned by CLIP-ViT
encoderstrainedonCTNandOriginalcaptionsusingUMAP[31]dimensionalityreductionfrom
highdimensionontoa2Dplane. InFigure4(a),wecomparetherepresentationslearnedfromCTN
captions(red)andOriginalcaptions(blue)ontheMSR-VTTdataset. Thenon-overlappingfeature
spacesindicatethatCTNcaptionscapturethecausalandtemporalrelationshipsnotpresentinthe
originalcaptions. InFigure4(b),wevisualizetherepresentationslearnedfromthecause(black)and
effect(orange)partsoftheCTNcaptions. Thenear-completeoverlapsuggestsastrongcorrelation
betweenthecauseandeffectcomponents,aligningwiththeinherentstructureofcausal-temporal
narrativesandsupportingthedesignoftheCENarchitecture.
QualitativeAnalysis: Qualitativeexamples,inFigure5,demonstrateCEN’sstrengthinaccurately
articulatingcausalrelationshipsandeventsequences,whileSOTAmethodsstruggle. Forinstance,
in(a),CENcapturesthefatalitymove’scausaleffect,unlikebaselines. In(c),CENlinksthekick’s
precisiontoscoring,whichothersmiss. Overall,quantitativeandqualitativeresultsshowcaseCEN’s
superior performance in understanding and generating causal-temporal narrative video captions
comparedtoexistingSOTAmethods. MoreresultsareprovidedinAppendixF.
AblationResults: ToexaminetheeffectivenessofCEN,wecompareitwith4baselinesinTable2:
a)E : Insteadofusingtwoseparatevideoencodersforcauseandeffect,wetrainoneCLIP-ViT
JointCE
usingthecombinedcauseandeffectcaptionsseparatedbyspaceforthetextencoder.Theperformance
833
C isT kN ill ec da p int i to hn e G gaT m: ' ea ' player performs a fatality move in mortal kombat another character CTN caption GT: 'a man is folding a piece of paper a paper airplane is being created'
C g i fs ra eE pm dN l dae yy( O k in kiU l rgl uin R eagS g va e) i: dn r ' eo aa ot n hv dgeid ar fe r' m eo S de g E d aa yM nm kd-e P r uf O rc eeh S gda e: dr r' ya a i c n kpt re al uar e y vp ge ide er er rf i o so in r p gm al aas my v a i i en d 'f g e a o Gt aa g Il g Ti at :ay m m ' am e e b o ' f oev yae A t iui K sn r G pim n lN ago y Nr f irt n:a e g'l df rk d aeo y d vm d k idb ry u ea k e ot r g v u gei e ad rg m e aeo enr 'd C ' pa eE m rN sa o ( n nO i isU s R c mrS e a) a k t: ini n' ga g am a n pa an a p ii rs e p rf o l aal id n rpi en l' ag n Aa eK 'p G i Ge Nc ITe N : :o ' a'f a p m pa aep nre s r io s a n m p is aa kfp o ie nldr g ia n ai gr p p al aa ppn eie e r c i ps e l ac o nr fe epa 'ate pd e' r S anE dM t- hP eOS:
(a) (b)
CTN caption GT: 'a soccer player kicked the ball with precision the ball successfully CTN caption GT: 'a boy decided to perform on stage the audience watched and listened
went into the goal' to his singing'
CEN (OURS) : 'a soccer player kicked the ball with precision the ball went into the goal CEN (OURS) : 'a boy decided to sing a song on stage the audience watched and
scoring a point' SEM-POS: 'a player is kicking a ball' AKGNN: 'the soccer player scored listened to him' SEM-POS: 'a man is singing a song' AKGNN: 'a man is singing on
a goal and the team scored a goal' GIT: 'soccer players are playing soccer' stage' GIT: 'a boy is singing a song'
(c) (d)
Figure5: Qualitativeexamplesacrossscenarioslikevideogames,paperfolding,soccer,andsinging. CEN
(Ours)captionsaccuratelycapturecausalnarrativesandtemporalsequencesfromgroundtruth,outperforming
SOTAvideocaptioningmethods.
dropsacrossallmetricsanddatasetscomparedtotheCENarchitecture,whichunderscoresthebenefits
ofdedicatedencodersforcapturingcauseandeffectdynamics.
b) Only Ecause and Only Eeffect: These ablations employ only the cause encoder or only the
effectencoder,respectively. Whileretainingreasonableperformance,bothvariantsfallshortofthe
performanceofthefullmodelacrossallmetricsanddatasets.
c)ZeroShotv.v.: Inthiszero-shotsetting,weevaluatethemodelstrainedononedataset(MSVDor
MSR-VTT)againsttheotherdataset,withoutanyfine-tuningontheotherdataset. Thesubstantial
performance drop highlights the importance of dataset-specific fine-tuning for effective causal-
temporalnarrativelearning.
d)Fine-tunev.v.: Buildinguponthezero-shotcase, thisvariantinvolvesfine-tuningthemodels
trainedononedataset(MSVDorMSR-VTT)againsttheotherdataset. Notably,thisfine-tuning
processleadstoimprovedperformanceontheMSVDdataset,evensurpassingtheCENmodel. This
observationdemonstratesthepotentialfortransferlearningfromlargerdatasetsandtheabilityto
leveragetheirknowledgeeffectivelyonsmallerdatasetsthroughfine-tuning.
Overall,theablationstudyvalidatestheefficacyofourproposedCENarchitecture,emphasizingthe
significanceofdedicatedencodersforcapturingcauseandeffectrelationshipsindependently. The
performancedropsobservedinE ,OnlyE ,andOnlyE results,comparedtotheCEN
JointCE cause effect
model,clearlyindicatethenecessityofexplicitlyembeddingcausal-temporalnarrativeunderstanding
withinthearchitecturethroughdedicatedcauseandeffectencoders.
5 Limitations
Whileourworkrepresentsasignificantstepforwardincausal-temporalnarrativevideocaptioning,
ourmethodassumesacertainlevelofcausalandtemporalcoherencewithinthevideocontent. Highly
complexorconvolutedcausalrelationshipsmaychallengeourapproachlikeexistingapproaches,
necessitating further architectural enhancements for improved generalization. Also, the current
workprimarilyfocusesonunderstandingandgeneratingcaptionsthatcapturecausalnarrativesand
temporaldynamics. Explicitintegrationofspatialreasoning,multi-agentinteractions,andlong-term
dependenciescouldfurtherenhancetherobustnessandapplicabilityofourapproach. Despitethese
limitations, our work opens up new avenues for innovative applications and research directions,
furthersolidifyingtheimportanceofcausal-temporalnarrativeunderstandinginvideoanalysistasks.
6 Conclusion
Inthispaper,weintroducedforthefirsttimeaCausal-TemporalNarrative(CTN)captionsbenchmark
and proposed a novel Cause-Effect Network (CEN) tailored for causal-temporal narrative video
captioning.Thisworkfindsvastapplicationsinautomatedvideosummarization,question-answering,
assistivetechnologies,andsurveillance. TheCTNcaptionsbenchmarkprovidesacomprehensive
testbedforevaluatingvideounderstandingmodels’abilitytograspcomplextemporalandcausal
dynamics,whichwillbereleasedforresearchpurposeonhttps://narrativebridge.github.io/. AndCEN
9explicitlymodelscause-effectrelationshipsandtemporaldynamicstogeneraterich,contextually
relevantdescriptionscapturingnuancedcausal-temporalnarrativeinvideos,demonstratingsignificant
performanceimprovementoverSOTAmethods(Section4). NarrativeBridgelaysthefoundationfor
aparadigmshift,wheremodelscomprehendunderlyingcausal-temporalnarrativedrivingevents,
unlockingnewfrontiersincontextuallyawarehuman-machineinteractionswithvideo.
Forfuturework,weaimtointegrateCTNcaptiongenerationwithexistingimagecaptioningtech-
niques,toannotateunlabeledvideoswithcausal-temporalnarrativelabels. Afewframesofthevideo
will be labelled using off-the-shelf image captioning methods, and CTN caption generation will
exploitthelabelledframestogenerateonecoherentcaptionfortheunlabelledvideo(seeAppendixE).
Thissynergisticapproachopensnewavenuesforcomprehensivevideounderstandingandannotation,
enablingmorerobustandaccuratevideoanalysispipelines.
7 Acknowledgment
This research was partly supported by the British Broadcasting Corporation Research and De-
velopment (BBC R&D), Engineering and Physical Sciences Research Council (EPSRC) Grant
EP/V038087/1“BBCProsperityPartnership: FuturePersonalisedObject-BasedMediaExperiences
DeliveredatScaleAnywhere”.
References
[1] Huda Alamri, Vincent Cartillier, Abhishek Das, Jue Wang, Anoop Cherian, Irfan Essa, Dhruv Batra,
TimKMarks,ChioriHori,PeterAnderson,etal. Audiovisualscene-awaredialog. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pages7558–7567,2019.
[2] SaghirAlfasly,JianLu,ChenXu,andYuruZou. Learnableirrelevantmodalitydropoutformultimodal
actionrecognitiononmodality-specificannotatedvideos. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pages20208–20217,2022.
[3] PeterAnderson,BasuraFernando,MarkJohnson,andStephenGould. Spice: Semanticpropositional
imagecaptionevaluation. InComputerVision–ECCV2016:14thEuropeanConference,Amsterdam,The
Netherlands,October11-14,2016,Proceedings,PartV14,pages382–398.Springer,2016.
[4] JeroenBerrevoets,KrzysztofKacprzyk,ZhaozhiQian,andMihaelavanderSchaar. Causaldeeplearning.
arXivpreprintarXiv:2303.02186,2023.
[5] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind
Neelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsarefew-shotlearners.
Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.
[6] AsliCelikyilmaz,ElizabethClark,andJianfengGao. Evaluationoftextgeneration: Asurvey. arXiv
preprintarXiv:2006.14799,2020.
[7] DavidChenandWilliamBDolan.Collectinghighlyparalleldataforparaphraseevaluation.InProceedings
ofthe49thannualmeetingoftheassociationforcomputationallinguistics:humanlanguagetechnologies,
pages190–200,2011.
[8] JiaweiChenandChiuManHo. Mm-vit: Multi-modalvideotransformerforcompressedvideoaction
recognition. InProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputerVision,
pages1910–1921,2022.
[9] SihanChen,XingjianHe,LongtengGuo,XinxinZhu,WeiningWang,JinhuiTang,andJingLiu. Valor:
Vision-audio-languageomni-perceptionpretrainingmodelanddataset. arXivpreprintarXiv:2304.08345,
2023.
[10] AndreideSouzaInácioandHeitorSilvérioLopes. Evaluationmetricsforvideocaptioning: Asurvey.
MachineLearningwithApplications,13:100488,2023.
[11] BinDuan,HaoTang,WeiWang,ZiliangZong,GuoweiYang,andYanYan. Audio-visualeventlocal-
izationviarecursivefusionbyjointco-attention. InProceedingsoftheIEEE/CVFWinterConferenceon
ApplicationsofComputerVision,pages4013–4022,2021.
[12] ZhiyuanFang,TejasGokhale,PratyayBanerjee,ChittaBaral,andYezhouYang. Video2commonsense:
Generatingcommonsensedescriptionstoenrichvideocaptioning. arXivpreprintarXiv:2003.05162,2020.
[13] LianliGao,ZhaoGuo,HanwangZhang,XingXu,andHengTaoShen. Videocaptioningwithattention-
basedlstmandsemanticconsistency. IEEETransactionsonMultimedia,19(9):2045–2055,2017.
[14] RuohanGao,Tae-HyunOh,KristenGrauman,andLorenzoTorresani. Listentolook:Actionrecognition
by previewing audio. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pages10457–10467,2020.
[15] ShijieGeng,PengGao,MoitreyaChatterjee,ChioriHori,JonathanLeRoux,YongfengZhang,Hongsheng
Li,andAnoopCherian. Dynamicgraphrepresentationlearningforvideodialogviamulti-modalshuffled
transformers. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume35,pages1415–
1423,2021.
[16] CliveWJGranger. Investigatingcausalrelationsbyeconometricmodelsandcross-spectralmethods.
Econometrica:journaloftheEconometricSociety,pages424–438,1969.
10[17] WillyFitraHendria,VaniaVelda,BahyHelmiHartoyoPutra,FikriansyahAdzaka,andCheolJeong.Action
knowledgeforvideocaptioningwithgraphneuralnetworks. JournalofKingSaudUniversity-Computer
andInformationSciences,35(4):50–62,2023.
[18] ChioriHori,AnoopCherian,TimKMarks,andTakaakiHori. Jointstudent-teacherlearningforaudio-
visualscene-awaredialog. InINTERSPEECH,pages1886–1890,2019.
[19] VladimirIashinandEsaRahtu. Multi-modaldensevideocaptioning. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognitionWorkshops,pages958–959,2020.
[20] AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,ChrisBamford,
DevendraSinghChaplot,DiegodelasCasas,EmmaBouHanna,FlorianBressand,etal. Mixtralof
experts. arXivpreprintarXiv:2401.04088,2024.
[21] EvangelosKazakos,ArshaNagrani,AndrewZisserman,andDimaDamen. Epic-fusion: Audio-visual
temporal binding for egocentric action recognition. In Proceedings of the IEEE/CVF International
ConferenceonComputerVision,pages5492–5501,2019.
[22] GuangyaoLi, YakeWei, YapengTian, ChenliangXu, Ji-RongWen, andDiHu. Learningtoanswer
questionsindynamicaudio-visualscenarios. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages19108–19118,2022.
[23] Chin-YewLin. Rouge:Apackageforautomaticevaluationofsummaries. InTextsummarizationbranches
out,pages74–81,2004.
[24] KevinLin,LinjieLi,Chung-ChingLin,FaisalAhmed,ZheGan,ZichengLiu,YumaoLu,andLijuan
Wang. Swinbert:End-to-endtransformerswithsparseattentionforvideocaptioning. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages17949–17958,2022.
[25] Yan-BoLin,Yu-JheLi,andYu-ChiangFrankWang. Dual-modalityseq2seqnetworkforaudio-visual
eventlocalization. InICASSP2019-2019IEEEInternationalConferenceonAcoustics,SpeechandSignal
Processing(ICASSP),pages2002–2006.IEEE,2019.
[26] Yan-Bo Lin, Hung-Yu Tseng, Hsin-Ying Lee, Yen-Yu Lin, and Ming-Hsuan Yang. Exploring cross-
videoandcross-modalitysignalsforweakly-supervisedaudio-visualvideoparsing. AdvancesinNeural
InformationProcessingSystems,34:11449–11461,2021.
[27] BingLiu,DongWang,XuYang,YongZhou,RuiYao,ZhiwenShao,andJiaqiZhao. Show,deconfound
andtell:Imagecaptioningwithcausalinference.InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages18041–18050,2022.
[28] HuaishaoLuo,LeiJi,BotianShi,HaoyangHuang,NanDuan,TianruiLi,JasonLi,TaroonBharti,and
MingZhou. Univl:Aunifiedvideoandlanguagepre-trainingmodelformultimodalunderstandingand
generation. arXivpreprintarXiv:2002.06353,2020.
[29] HuaishaoLuo, LeiJi, MingZhong, YangChen, WenLei, NanDuan, andTianruiLi. Clip4clip: An
empiricalstudyofclipforendtoendvideoclipretrievalandcaptioning. Neurocomputing,508:293–304,
2022.
[30] KarttikeyaMangalam,RaiymbekAkshulakov,andJitendraMalik. Egoschema:Adiagnosticbenchmark
forverylong-formvideolanguageunderstanding. AdvancesinNeuralInformationProcessingSystems,
36,2024.
[31] LelandMcInnes,JohnHealy,andJamesMelville. Umap:Uniformmanifoldapproximationandprojection
fordimensionreduction. arXivpreprintarXiv:1802.03426,2018.
[32] AsmarNadeem,AdrianHilton,RobertDawes,GrahamThomas,andArminMustafa. Sem-pos:Grammat-
icallyandsemanticallycorrectvideocaptioning. arXivpreprintarXiv:2303.14829,2023.
[33] AsmarNadeem,AdrianHilton,RobertDawes,GrahamThomas,andArminMustafa.Cad-contextualmulti-
modalalignmentfordynamicavqa. InProceedingsoftheIEEE/CVFWinterConferenceonApplications
ofComputerVision,pages7251–7263,2024.
[34] BoxiaoPan,HaoyeCai,De-AnHuang,Kuan-HuiLee,AdrienGaidon,EhsanAdeli,andJuanCarlos
Niebles. Spatio-temporalgraphforvideocaptioningwithknowledgedistillation. InProceedingsofthe
IEEE/CVFconferenceoncomputervisionandpatternrecognition,pages10870–10879,2020.
[35] RameswarPanda, Chun-FuRichardChen, QuanfuFan, XimengSun, KateSaenko, AudeOliva, and
RogerioFeris. Adamml:Adaptivemulti-modallearningforefficientvideorecognition. InProceedingsof
theIEEE/CVFInternationalConferenceonComputerVision,pages7576–7585,2021.
[36] MircoPlanamente,ChiaraPlizzari,EmanueleAlberti,andBarbaraCaputo. Cross-domainfirstperson
audio-visualactionrecognitionthroughrelativenormalignment. arXivpreprintarXiv:2106.01689,2021.
[37] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InInternationalconferenceonmachinelearning,pages8748–8763.PMLR,
2021.
[38] FaeghehSardari,ArminMustafa,PhilipJBJackson,andAdrianHilton. Pat:Position-awaretransformer
fordensemulti-labelactiondetection. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages2988–2997,2023.
[39] IdanSchwartz,AlexanderGSchwing,andTamirHazan. Asimplebaselineforaudio-visualscene-aware
dialog. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
12548–12558,2019.
[40] AnkitShah,ShijieGeng,PengGao,AnoopCherian,TakaakiHori,TimKMarks,JonathanLeRoux,and
ChioriHori. Audio-visualscene-awaredialogandreasoningusingaudio-visualtransformerswithjoint
student-teacherlearning. InICASSP2022-2022IEEEInternationalConferenceonAcoustics,Speechand
11SignalProcessing(ICASSP),pages7732–7736.IEEE,2022.
[41] YayaShi,XuYang,HaiyangXu,ChunfengYuan,BingLi,WeimingHu,andZheng-JunZha. Emscore:
Evaluatingvideocaptioningviacoarse-grainedandfine-grainedembeddingmatching. InProceedingsof
theIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages17929–17938,2022.
[42] JingkuanSong,ZhaoGuo,LianliGao,WuLiu,DongxiangZhang,andHengTaoShen. Hierarchicallstm
withadjustedtemporalattentionforvideocaptioning. arXivpreprintarXiv:1706.01231,2017.
[43] ZehuaSun,QiuhongKe,HosseinRahmani,MohammedBennamoun,GangWang,andJunLiu. Human
actionrecognitionfromvariousdatamodalities: Areview. IEEEtransactionsonpatternanalysisand
machineintelligence,2022.
[44] YapengTian,ChenxiaoGuan,JustinGoodman,MarcMoore,andChenliangXu.Audio-visualinterpretable
andcontrollablevideocaptioning. InIEEEComputerSocietyConferenceonComputerVisionandPattern
Recognitionworkshops,2019.
[45] YapengTian,JingShi,BochenLi,ZhiyaoDuan,andChenliangXu. Audio-visualeventlocalizationin
unconstrainedvideos. InProceedingsoftheEuropeanConferenceonComputerVision(ECCV),pages
247–263,2018.
[46] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov, SoumyaBatra, PrajjwalBhargava, ShrutiBhosale, etal. Llama2: Openfoundationand
fine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[47] RamakrishnaVedantam,CLawrenceZitnick,andDeviParikh. Cider:Consensus-basedimagedescription
evaluation. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages
4566–4575,2015.
[48] SubhashiniVenugopalan,MarcusRohrbach,JeffreyDonahue,RaymondMooney,TrevorDarrell,andKate
Saenko. Sequencetosequence-videototext. InProceedingsoftheIEEEinternationalconferenceon
computervision,pages4534–4542,2015.
[49] CarlVondrick,DenizOktay,HamedPirsiavash,andAntonioTorralba. Predictingmotivationsofactions
byleveragingtext. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages2997–3005,2016.
[50] ChengWang,HaojinYang,andChristophMeinel. Exploringmultimodalvideorepresentationforaction
recognition. In2016InternationalJointConferenceonNeuralNetworks(IJCNN),pages1924–1931.
IEEE,2016.
[51] JianfengWang, ZhengyuanYang, XiaoweiHu, LinjieLi, KevinLin, ZheGan, ZichengLiu, CeLiu,
andLijuanWang. Git: Agenerativeimage-to-texttransformerforvisionandlanguage. arXivpreprint
arXiv:2205.14100,2022.
[52] ToddWilkens,AnthonyHughes,BarbaraMWildemuth,andGaryMarchionini. Theroleofnarrativein
understandingdigitalvideo:Anexploratoryanalysis. ProceedingsoftheAmericanSocietyforInformation
ScienceandTechnology,40(1):323–329,2003.
[53] FanyiXiao,YongJaeLee,KristenGrauman,JitendraMalik,andChristophFeichtenhofer. Audiovisual
slowfastnetworksforvideorecognition. arXivpreprintarXiv:2001.08740,2020.
[54] JunbinXiao,XindiShang,AngelaYao,andTat-SengChua. Next-qa:Nextphaseofquestion-answeringto
explainingtemporalactions. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages9777–9786,2021.
[55] JunXu,TaoMei,TingYao,andYongRui. Msr-vtt:Alargevideodescriptiondatasetforbridgingvideo
andlanguage. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages
5288–5296,2016.
[56] JunXu,TingYao,YongdongZhang,andTaoMei. Learningmultimodalattentionlstmnetworksforvideo
captioning. InProceedingsofthe25thACMinternationalconferenceonMultimedia,pages537–545,
2017.
[57] DizhanXue,ShengshengQian,andChangshengXu. Variationalcausalinferencenetworkforexplanatory
visualquestionanswering. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pages2515–2525,2023.
[58] AntoineYang,ArshaNagrani,PaulHongsuckSeo,AntoineMiech,JordiPont-Tuset,IvanLaptev,Josef
Sivic,andCordeliaSchmid. Vid2seq:Large-scalepretrainingofavisuallanguagemodelfordensevideo
captioning. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages10714–10726,2023.
[59] HeeseungYun,YoungjaeYu,WonsukYang,KangilLee,andGunheeKim. Pano-avqa:Groundedaudio-
visualquestionansweringon360degvideos. InProceedingsoftheIEEE/CVFInternationalConference
onComputerVision,pages2031–2041,2021.
[60] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual
commonsensereasoning. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages6720–6731,2019.
[61] YunhuaZhang,HazelDoughty,LingShao,andCeesGMSnoek. Audio-adaptiveactivityrecognition
acrossvideodomains. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages13791–13800,2022.
[62] ZiqiZhang,YayaShi,ChunfengYuan,BingLi,PeijinWang,WeimingHu,andZheng-JunZha. Object
relationalgraphwithteacher-recommendedlearningforvideocaptioning.InProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages13278–13288,2020.
12Appendix
A ImplementationDetails
WefirstextractframesfromthevideoclipandthenencodethemusingtheCauseandEffectVideo
Encoders. Weadoptasamplingstrategywherewesampleoneframepersecond,withamaximum
of20frames. Thisapproachensuresthatthesampledframescovertheentiredurationofthevideo,
providing a comprehensive representation of the video content while maintaining computational
efficiency. Forthefine-tuningexperimentinTable2,allthehyperparametersremainthesameexcept
thelearningratei.e. 0.0000005. ForourexperimentationoftheCauseandEffectNetwork(CEN),
weusestandardsplitsoftheMSVD[7]andMSRVTT[55]datasets. ForMSVD,weuse1200videos
fortraining,100forvalidation,and670fortesting. ForMSRVTT,weuse6513videosfortraining,
497forvalidation,and2990fortesting. Thesesplitsarecommonlyusedinvideocaptioningresearch
toensurefaircomparisonwithothermethods. AlltheexperimentsinStage1andStage2ofourCEN
arerunusingA100-80GBandRTX3090-24GBGPUsrespectively. WeimplementSEM-POS[32],
AKGNN[17]andGIT[51]usingRTX3090-24GB,A100-80GBandA100-80GBGPUsrespectively.
B PromptDesignProcess
InSection3.1,weintroduceourapproachforgeneratingCausal-TemporalNarrative(CTN)captions
usingalargelanguagemodel(LLM)andfew-shotprompting. Thepromptdesignplayedacrucial
roleinguidingtheLLMtogeneratecaptionsthataccuratelycapturethecause-effectrelationshipsand
temporaldynamicsinthevideocontent. Figure1showcasestheeffectivenessofourfew-shotbased
prompt,whichresultedinacoherentandcontextuallyrelevantCTNcaptionforthegivenvideo. To
furtherillustratetheimportanceofpromptdesignandthebenefitsoffew-shotlearning,weconduct
additionalexperimentswithzero-shotprompting. Intheseexperiments,weevaluatetheperformance
oftheLLMingeneratingCTNcaptionswithoutprovidinganyexamplecaptionsintheprompt.
Zero-ShotPrompting:Zero-shotpromptingreferstotheprocessofprovidinglargelanguagemodels
(LLMs)withataskpromptwithoutanyaccompanyingexamplesordemonstrations,requiringthe
modeltogeneraterelevantresponsesbasedsolelyonitspre-trainedknowledgeandunderstanding
oftheprompt. First,weaimtoevaluatetheperformanceofLLMsingeneratingcausal-temporal
narrative(CTN)captionsforvideoswithoutprovidinganyfew-shotexamples.
Toachievethis,weconstruct4distinctprompts,eachwithvaryingdegreesofquality,andexperi-
mentedwithallofthemtoassesstheireffectivenessinazero-shotsetting,startingwiththemost
challengingpromptandprogressivelyimprovingthequality.Prompt2presentsanextremelyterseand
poorlystructuredsetofinstructions. Itcontainsminimalcontextandinformation,merelyinstructing
themodelto"MakeCauseEffectthingfromcaptions"andlistingasetofnumberedrequirements
withoutprovidingclearexplanations. Asexpected,theresultingcaptions(Figure6)areoflowquality
andoftenlackcoherenceandrelevancetothevideocontent. Moreover,theLLMgeneratedmultiple
captionsinsteadofasinglecoherentcaption,andtheoutputformatdoesnotadheretothespecified
structure(asingleJSONobjectcontainingtheCauseandEffectstatements).
Make Cause Effect thing from captions. Follow: 1. Use only caption info 2. Be relevant 3. No new stuff
4. No conclusions 5. Same time 6. Plain English short sentences 7. 15 words max 8. Just JSON.
[Descriptive Captions]:
<descriptive_captions>
[Causal Temporal Narrative]:
Prompt2:Zero-shotLLMPromptwithminimalcontextandunexplainedrequirementsforgeneratingcause-
effectfromcaptions.
13{ "1": {"Cause": "A car crashes", "Effect": "Guys play beer pong"},
"2": {"Cause": "A car drives through an open field", "Effect": "Kicks up
dirt"},
"3": {"Cause": "Unclear", "Effect": "A car flips over"},
"4": {"Cause": "Unclear", "Effect": "A car gets wracked"},
"5": {"Cause": "Unclear", "Effect": "A car is being flipped over"},
"6": {"Cause": "A dirt vehicle rides", "Effect": "It rolls"},
"7": {"Cause": "Unclear", "Effect": "A dune buggy flips over"},
"8": {"Cause": "Unclear", "Effect": "A four wheeler wrecks"},
"9": {"Cause": "A monster truck flips on its side", "Effect": "Several young
men shout while playing beer pong"},
"10": {"Cause": "A person drives an offroad car around a field", "Effect":
"Unclear"},
"11": {"Cause": "A person flips a go kart", "Effect": "A crowd cheers"},
"12": {"Cause": "Unclear", "Effect": "A race truck is crashing"},
"13": {"Cause": "A truck rolls over itself", "Effect": "Boys cheer on a
friend"},
"14": {"Cause": "Unclear", "Effect": "A truck tumbles over on itself"},
"15": {"Cause": "A tumbler crashes on a dirt road", "Effect": "Then a group of
guys play beer pong"},
"16": {"Cause": "A type of monster truck crashes", "Effect": "Men are shown
celebrating"},
"17": {"Cause": "Unclear", "Effect": "A vehicle flips over"},
"18": {"Cause": "Unclear", "Effect": "An off road vehicle crashes"},
"19": {"Cause": "Unclear", "Effect": "A car crashes while driving"},
"20": {"Cause": "Footage from a monster truck style event", "Effect":
"Followed by a frat party"} }
Figure6:CaptiongeneratedusingLLMfortheexampleinFigure1usingPrompt2
Wethenprogressivelyimprovethepromptquality,movingtoamoreconcisesetofinstructionsin
Prompt 3 and a relatively well-structured prompt in Prompt 4. While the captions generated by
theseimprovedprompts(Figures7and8)showsomeenhancements,theystillfallshortintermsof
capturingthecausal-temporalnarrativeaccuratelyandconsistently. Additionally,theLLMcontinues
togeneratemultiplecaptionsanddeviatefromthedesiredoutputformat.
You need to create a concise Cause and Effect scenario in JSON format based on some descriptive
captions about a video. The scenario must follow these rules: stick to the given information, be
relevant, don’t add new events/details or draw conclusions, maintain time consistency, use
simple language in short sentences, and limit each statement to 15 words. Don’t put any extra text
outside the JSON.
[Descriptive Captions]:
<descriptive_captions>
[Causal Temporal Narrative]:
Prompt3:Zero-shotLLMPromptwithreasonablestructureconveyingcoretaskofcreatingcause-effectscenario
fromcaptions.
{{"Event": "Car crashes", "Effect": "Guys play beer pong"},
{"Event": "Car drives in open field", "Effect": "Dirt is kicked up"},
{"Event": "Car flips over", "Effect": "None specified"},
{"Event": "Car gets wracked", "Effect": "None specified"},
{"Event": "Car is flipped over", "Effect": "None specified"},
{"Event": "Dirt vehicle rides and rolls", "Effect": "None specified"},
{"Event": "Dune buggy flips over", "Effect": "None specified"},
{"Event": "Four wheeler wrecks", "Effect": "None specified"},
{"Event": "Monster truck flips", "Effect": "Young men shout and play beer
pong"},
{"Event": "Person drives offroad car", "Effect": "None specified"},
{"Event": "Person flips go kart", "Effect": "Crowd cheers"},
{"Event": "Race truck crashes", "Effect": "None specified"},
{"Event": "Truck rolls over", "Effect": "Boys cheer on a friend"},
{"Event": "Tumbler crashes on dirt road", "Effect": "Guys play beer pong"},
{"Event": "Monster truck crashes", "Effect": "Men are shown celebrating"},
{"Event": "Vehicle flips over", "Effect": "None specified"},
{"Event": "Off road vehicle crashes", "Effect": "None specified"},
{"Event": "Car crashes while driving", "Effect": "None specified"},
{"Event": "Monster truck event footage", "Effect": "Frat party is shown"}}
Figure7:CaptiongeneratedusingLLMfortheexampleinFigure1usingPrompt3
14You are tasked with generating a causal narrative caption for a video based on a series of provided
descriptive captions. Your generated Cause and Effect scenario should be grounded in the given
information, relevant, avoid introducing new details or implying conclusions, maintain temporal
consistency, use plain English indirect sentences, and limit each Cause and Effect statement to
15 words maximum. Do not include any additional text outside the JSON object.
[Descriptive Captions]:
<descriptive_captions>
[Causal Temporal Narrative]:
Prompt4:Zero-shotLLMPromptclearlydescribingcaption-basedcause-effectgenerationtaskwithspecified
requirements.
{{"Event": "Car crashes", "Effect": "Guys play beer pong"},
{"Event": "Car drives in open field", "Effect": "Dirt is kicked up"},
{"Event": "Car flips over", "Effect": "None specified"},
{"Event": "Car gets wracked", "Effect": "None specified"},
{"Event": "Car is flipped over", "Effect": "None specified"},
{"Event": "Dirt vehicle rides and rolls", "Effect": "None specified"},
{"Event": "Dune buggy flips over", "Effect": "None specified"},
{"Event": "Four wheeler wrecks", "Effect": "None specified"},
{"Event": "Monster truck flips", "Effect": "Young men shout and play beer
pong"},
{"Event": "Person drives offroad car", "Effect": "None specified"},
{"Event": "Person flips go kart", "Effect": "Crowd cheers"},
{"Event": "Race truck crashes", "Effect": "None specified"},
{"Event": "Truck rolls over", "Effect": "Boys cheer on a friend"},
{"Event": "Tumbler crashes on dirt road", "Effect": "Guys play beer pong"},
{"Event": "Monster truck crashes", "Effect": "Men are shown celebrating"},
{"Event": "Vehicle flips over", "Effect": "None specified"},
{"Event": "Off road vehicle crashes", "Effect": "None specified"},
{"Event": "Car crashes while driving", "Effect": "None specified"},
{"Event": "Monster truck event footage", "Effect": "Frat party is shown"}}
Figure8:CaptiongeneratedusingLLMfortheexampleinFigure1usingPrompt4
Finally, we experiment with a zero-shot prompt (Prompt 5) that closely resembles our few-shot
based Prompt 1 from Section 3.1, but without the example captions. The captions generated by
thisprompt(Figure9)demonstrateimprovedcoherenceandrelevancecomparedtotheprevious
zero-shotprompts. However,theystilllackthelevelofdetail,accuracy,andcontextualunderstanding
exhibitedbythecaptionsgeneratedusingourfew-shotbasedprompt(Prompt1). Furthermore,the
LLMpersistsingeneratingmultiplecaptions.
You are an advanced language model tasked with generating causal temporal narrative captions for a
video. However, you cannot directly access the video series of itself. Instead, you will be provided
with a captions that outline the key events and scenes in the video. Your task is to generate a
concise Cause and Effect scenario, based on the information provided in the descriptive captions.
Be careful, your generated Cause and Effect statements should fulfill the following requirements:
1. Your narrative should be grounded in the information provided by the descriptive captions.
2. Cause and Effect scenario is relevant.
3. It should not introduce any new events or details not mentioned.
4. Avoid implying conclusions.
5. Maintain temporal consistency with the provided captions.
6. Use plain English and direct sentences.
7. Cause and Effect statements each limited to a maximum of 15 words.
8. Do not include any additional text before or after the JSON object.
[Descriptive Captions]:
<descriptive_captions>
[Causal Temporal Narrative]:
Prompt5:Zero-ShotPromptingforCausalNarrativeCaptionGeneration
15{ "Cause and Effect Statements": [ "Car crashes in field, causing it to flip
over.", "Dune buggy flips, resulting in a wreck.", "Go kart flipped by person,
leading to crowd cheers.", "Race truck crashes, boys cheer in response.", "Truck
tumbles, friends shout while playing beer pong.", "Monster truck crashes, men
celebrate afterwards.", "Off-road vehicle crashes, causing a tumble.", "Car crash
occurs, followed by a game of beer pong." ] }
Figure9:CaptiongeneratedusingLLMfortheexampleinFigure1usingPrompt5
Theseexperimentshighlightthesignificanceofeffectivepromptdesignandtheadvantagesoffew-
shot learning in generating high-quality CTN captions. By providing carefully crafted example
captionsintheprompt,weenabletheLLMtobetterunderstandthedesiredformat,style,andlevelof
detailforthegeneratedcaptions.Thefew-shotbasedpromptallowstheLLMtolearnfromandmimic
theprovidedexamples,resultingincaptionsthataccuratelycapturethecausal-temporalnarrativeof
thevideocontent.
Crucially,thefew-shotbasedpromptalsoguidestheLLMtogenerateasinglecoherentcaptionin
thespecifiedJSONformat,ensuringconsistencyandusabilityofthegeneratedcaptions. Incontrast,
thezero-shotprompts,evenwhenwell-structured,relysolelyontheLLM’spre-existingknowledge
andunderstandingofthetaskdescription. WhiletheLLMcangeneraterelevantcaptionstosome
extent, the lack of explicit examples limits its ability to produce captions with the same level of
coherence, accuracy, and contextual understanding as those generated using the few-shot based
prompt. Moreover,thezero-shotpromptsfailtoguidetheLLMtowardgeneratingasinglecaptionin
thedesiredJSONformat,resultinginmultiplecaptionsandinconsistentoutputstructures.
Thesefindingsunderscoretheimportanceofourfew-shotbasedpromptdesignintheCTNcaption
generationprocessanddemonstrateitssuperiorityoverzero-shotpromptingapproaches. Bylever-
agingthepoweroffew-shotlearning,weenabletheLLMtogeneratehigh-qualityCTNcaptions
thateffectivelycapturethecause-effectrelationshipsandtemporaldynamicsinvideocontent,as
evidencedbytheresultspresentedinFigure1. Additionally,ourfew-shotbasedpromptensuresthat
thegeneratedcaptionsadheretothespecifiedJSONformat,facilitatingseamlessintegrationand
usabilityindownstreamapplications.
C ComparisonofLargeLanguageModelsforCTNCaptionGeneration
Toevaluatetheeffectivenessoflargelanguagemodels(LLMs)ingeneratinghigh-qualityCausal-
TemporalNarrative(CTN)captions,wecomparetheperformanceoftwoopensourcestate-of-the-art
LLMsatthetimeofexperimentation: MixtralofExperts[20],whichweutilizedinourCTNcaption
generationpipeline,andLlama2-70b[46].
Weprovidetheoriginalvideocaptionsasinputtobothmodelsandassessthequalityoftheirgenerated
CTNcaptions. Inthevideogameexample(Figure10),MixtralofExpertsaccuratelycapturesthe
causalrelationshipbetweenthefatalitymoveperformedbyonecharacterandtheconsequentdeath
ofanothercharacterinthegame. Incontrast,Llama2-70bfocusesonaspecificeffect(bloodspurting
fromthecharacter’sneck)withoutexplicitlylinkingittothecause.
For the paper folding tutorial (Figure 11), both models correctly identify the causal relationship
betweenfoldingthepaperandcreatingapapercraft. However,MixtralofExpertsprovidesamore
precisedescription,specifyingthatthepapercraftbeingcreatedisapaperairplane.
Inthesoccerhighlightscenario(Figure12),MixtralofExpertssuccessfullycapturesthecausallink
betweenthesoccerplayer’sprecisekickandtheballsuccessfullygoingintothegoal. Llama2-70b,
ontheotherhand,mentionstheeffect(peoplecheering)withoutexplicitlyconnectingittothecause
(theplayerscoringagoal).
Lastly,forthesingingperformance(Figure13),bothmodelsaccuratelydescribethecause-effect
relationshipbetweentheboy’sdecisiontoperformonstageandtheaudience’sreactionofwatching
andlistening. However,MixtralofExpertsprovidesamoredetaileddescriptionoftheaudience’s
response.
Overall, theCTNcaptionsgeneratedbyMixtralofExpertsconsistentlydemonstrateabetterun-
derstandingofthecausal-temporalnarrativeinthevideoscomparedtoLlama2-70b. Ourapproach
usingMixtralofExpertseffectivelycapturesthecause-effectrelationshipsandtemporaldynamics,
resultinginmoreaccurateandcontextuallyrelevantcaptions.
16Input Video:
1. 'an animated cartoon cuts another's throat'
2. 'a character in a video game gets his throat cut open'
3. 'a fatality from Mortal Kombat is shown'
Original 4. 'a Mortal Kombat fatality sequence'
Captions 5. 'a person is playing a video game'
6. 'a person playing Mortal Kombat'
7. 'a scene from the video game of fatality'
8. 'a video game character kills another character'
9. 'a video game scene is going on of fighting'
10. 'a violent scene from a video game'
11. 'a woman stabs a man through the head then slit his throat'
Prompt 12. 'gameplay with lots of blood'
EvLaL lLuL aMMtion 13. 'in a video game, a woman attacks a man with a knife'
14. 'in a video game, a woman is killing a man'
15. 'one character wins by perishing the other'
16. 'Skarlet fatality in Mortal Kombat'
17. 'someone is playing a game'
CTN 18. 'someone is playing a video game'
Caption 19. 'there is a woman killing someone in a game'
20. 'Skarlet fatality in Mortal Kombat'
Mixtral of Experts (Our CTN) : {'Cause': 'a player performs a fatality move in mortal kombat', 'Effect': 'another
character is killed in the game'}
Llama2-70b : {'Cause': 'a character in a video game gets his throat cut open', 'Effect': ' blood is shown spurting out
of the characters neck'}
Figure10:ComparisonofMixtralofExpertsandLlama2-70bonavideogamesequenceinvolvingafatality
move.
Input Video:
1. 'a man folding paper'
2. 'a man is folding paper'
3. 'a man is showing how to fold paper'
Original 4. 'a man showing some crafts'
Captions 5. 'a person folds paper to relaxing music'
6. 'a person is folding a white paper'
7. 'a person is folding a white sheet of paper'
8. 'a person is folding paper at the table'
9. 'a person showing people how to fold origami'
10. 'a young man is folding a piece of paper'
11. 'craft work in a piece of paper'
Prompt 12. 'paper plane folding tutorial video clip'
EvLaL lLuL aMMtion 13. 'somebody folds a piece of paper'
14. 'someone explain how to fold a paper plane'
15. 'someone is folding origami'
16. 'someone is folding paper'
17. 'someone is making a paper craft'
CTN 18. 'someone is folding paper'
Caption 19. 'paper plane folding tutorial video clip'
20. 'a person is folding a white paper'
Mixtral of Experts (Our CTN) : {'Cause': 'a man is folding a piece of paper', 'Effect': 'a paper airplane is being
created'}
Llama2-70b : {'Cause': 'a person folds paper', 'Effect': ' the person is making a paper craft'}
Figure11: ComparisonofMixtralofExpertsandLlama2-70bonapaperfoldingtutorialtocreateapaper
airplane.
17Input Video:
1. 'a professional soccer game that is set to music'
2. 'a soccer highlight video'
3. 'a soccer match is happening and one of the players just kicked the ball in the goal'
Original 4. 'a soccer player hops over a wall'
Captions 5. 'a soccer player is scoring the goal for his team'
6. 'a soccer player is scoring a goal'
7. 'a soccer player makes a goal'
8. 'a soccer player scores a goal'
9. 'compilation of popular soccer clips'
10. 'highlights of a soccer game'
11. 'people are playing sports in the ground'
Prompt 12. 'soccer players are running around on a field'
EvLaL lLuL aMMtion 13. 'soccer players playing soccer'
14. 'sports highlights are displayed'
15. 'there are people playing soccer and scores a goal'
16. 'there is some footage of a soccer game'
17. 'this is a video from a soccer game'
CTN 18. 'video of athletes performing amazing plays'
Caption 19. 'a soccer player hops over a wall'
20. 'a soccer player makes a goal'
Mixtral of Experts (Our CTN) : {'Cause': 'a soccer player kicked the ball with precision', 'Effect': 'the ball
successfully went into the goal'}
Llama2-70b : {'Cause': 'a soccer player scores a goal', 'Effect': ' people are cheering'}
Figure12:ComparisonofMixtralofExpertsandLlama2-70bonhighlightsfromasoccermatch.
Input Video:
1. 'a boy giving a stage performance'
2. 'a boy is singing'
3. 'a boy is singing a song'
Original 4. 'a boy is singing in the stage'
Captions 5. 'a boy is singing on a scene in front of the crowd'
6. 'a boy is singing on stage in front of an audience'
7. 'a boy singing on stage in front of crowd'
8. 'a boy sings in front of judges and an audience'
9. 'a guy is singing on stage'
10. 'a kid is singing'
11. 'a man is singing a song on a stage'
Prompt 12. 'a man plays piano while a young boy sings on stage'
EvLaL lLuL aMMtion 13. 'a person is singing on stage'
14. 'a young boy is signing on tv in front of an audience'
15. 'a young man is singing to a crowd on a stage'
16. 'guy singing on stage'
17. 'man is singing in live show'
CTN 18. 'young boy singing a song'
Caption 19. 'a man is singing a song on a stage'
20. 'guy singing on stage'
Mixtral of Experts (Our CTN) : {'Cause': 'a boy decided to perform on stage', 'Effect': 'the audience watched and
listened to his singing'}
Llama2-70b : {'Cause': 'the boy is singing', 'Effect': ' the crowd enjoys the performance'}
Figure13:ComparisonofMixtralofExpertsandLlama2-70bonaboysingingonstageinfrontofanaudience.
18These results highlight the importance of selecting an appropriate LLM and designing effective
promptsforgeneratinghigh-qualityCTNcaptions. ThesuperiorperformanceofMixtralofExperts
canbeattributedtoitsarchitectureandtraining,whichenableittobetterunderstandandarticulate
thecomplexcausal-temporalnarrativespresentinvideocontent.
D ImpactofAutomaticEvaluationonCTNCaptionGeneration
InSection3.1,wediscusstheimportanceofusingautomaticevaluationtoensurethequalityand
relevanceofthegeneratedCausalTemporalNarrative(CTN)captions. WeemploytheEMScore
metrictomeasuretheconsistencybetweenthegeneratedcaptionsandthevideocontent,discarding
captionsthatfellbelowapredefinedthreshold. Thisappendixvisuallydemonstratestheimpactof
theautomaticevaluationsteponthequalityofthegeneratedCTNcaptions.
33
CTN caption GT - Ours: {'Cause': 'a female sportscaster interviewed a male athlete',
'Effect': 'she asked him questions'}
CTN caption (w/o automatic evaluation): {'Cause': 'a man carelessly neglected taking
his prescribed allergy medication', 'Effect': 'he suffered a severe sneezing fit'}
Figure14:CTNcaptioncomparisonforavideoofafemalesportscasterinterviewingamaleathlete.
33
CTN caption GT - Ours: {'Cause': 'bus ticket prices increased in Brazil', 'Effect': 'a
woman was interviewed about her participation in the protests'}
CTN caption (w/o automatic evaluation): {'Cause': 'protest caused by a rise in bus
tickets', 'Effect': 'mass gathering of people in rio de janeiro and sao paulo'}
Figure15:CTNcaptioncomparisonforavideoofprotestsinBrazilduetoincreasedbusticketprices.
33
CTN caption GT - Ours: {"Cause": "a terror attack occurred in Texas", "Effect": "news
anchors discussed freedom of speech issues"}
CTN caption (w/o automatic evaluation): {'Cause': 'a band performs a song on stage',
'Effect': 'the audience cheers and enjoys the music'}
Figure16:CTNcaptioncomparisonforavideodiscussingaterrorattackinTexas.
Figure14showsanexamplewheretheCTNcaptiongeneratedwithautomaticevaluationaccurately
capturesthecausalrelationshipbetweenthefemalesportscasterinterviewingthemaleathleteandher
askinghimquestions. Incontrast,thecaptiongeneratedwithoutautomaticevaluationisirrelevantto
thevideocontent,discussingamanneglectinghisallergymedicationandsufferingasneezingfit.
Similarly,inFigure15,theCTNcaptiongeneratedwithautomaticevaluationcorrectlyidentifies
thecauseoftheprotestsinBrazilastheincreaseinbusticketpricesandlinksittotheeffectofa
1933
CTN caption GT - Ours: {"Cause": "the Air Force used a surveillance system in Iraq",
"Effect": "a woman talked about the man who created it"}
CTN caption (w/o automatic evaluation): {'Cause': 'a group starts dancing at an
event', 'Effect': 'the atmosphere becomes more lively'}
Figure17:CTNcaptioncomparisonforavideoabouttheAirForceusingasurveillancesysteminIraq.
woman being interviewed about her participation in the protests. The caption generated without
automaticevaluation,whilementioningtheprotestanditscause,failstocapturethespecificeffectof
thewoman’sinterview.
Figure16demonstrateshowtheCTNcaptiongeneratedwithautomaticevaluationaccuratelyde-
scribes the cause of a terror attack in Texas and its effect on news anchors discussing freedom
of speech issues. The caption generated without automatic evaluation is completely unrelated,
mentioningaband’sperformanceandtheaudience’sreaction.
Lastly,inFigure17,theCTNcaptiongeneratedwithautomaticevaluationcorrectlylinksthecause
oftheAirForceusingasurveillancesysteminIraqtotheeffectofawomantalkingabouttheman
whocreatedit. Thecaptiongeneratedwithoutautomaticevaluationisagainirrelevant,discussinga
groupstartingtodanceataneventandtheatmospherebecomingmorelively. Theseexamplesclearly
illustratetheimportanceofincorporatingautomaticevaluationintheCTNcaptiongenerationprocess.
Byensuringthatthegeneratedcaptionsareconsistentwiththevideocontent,wecansignificantly
improve the quality and relevance of the CTN captions, enabling the CEN model to learn more
meaningfulcausalandtemporalrelationshipsfromthevideos.
E ApplicationofCTNCaptionGenerationforLabelingUnlabeledVideos
OurCTNcaptiongenerationapproach, asdescribedinSection3.1, canbeeffectivelyappliedto
thetaskoflabelingunlabeledvideos. Thisapplicationleveragesthepowerofourfew-shotbased
promptandtheLLM’sabilitytogeneratecoherentandcontextuallyrelevantcaptionsthatcapturethe
causal-temporalnarrativeinvideocontent.
Figure18illustratesthepipelineforlabelinganunlabeledvideousingourCTNcaptiongeneration
approach. First,weextractframesfromtheunlabeledvideo. Inthisexample,weextractfiveframes
whichareequallyspacedinthevideo. Next,wegenerateimagecaptionsforeachoftheextracted
framesusingastate-of-the-artimagecaptioningmodel. Forthisdemonstration,weemploytheGIT
[51],whichhasshownimpressiveperformanceingeneratingaccuratecaptionsforindividualimages.
TheGITmodelgeneratescaptionssuchas"atractorisdriving","therearefourtrucks","atractoris
upsidedown","agameisbeingplayed",and"twopeoplearecelebrating"forthefiveframes. These
imagecaptionsserveastheinputtoourLLM-basedCTNcaptiongenerationpipeline(seeFigure2).
WeusePrompt1inSection3.1,replacingtheoriginaldescriptivecaptionswiththeimagecaptions
generatedbytheGITmodel. TheLLMthengeneratesaCTNcaptionbasedontheseimagecaptions,
followingthespecifiedrequirementsandformat. Inthisexample, thegeneratedCTNcaptionis:
"Cause: ’atractordrivingrecklesslyflipsoverandendsupupsidedown’Effect: ’causinganaccident,
whileotherscelebratedunaware’". Thiscaptioneffectivelycapturesthekeyeventsandtheircausal-
temporal relationships in the video, providing a concise and informative summary of the video
content.
ToensurethequalityandrelevanceofthegeneratedCTNcaption,weemploythesameevaluation
frameworkdescribedinSection3.1. Thecaptioniscomparedagainstthevideocontentusingthe
EMScore[41]metric,andonlycaptionsthatmeetaspecifiedthresholdareretained. Thisapplication
demonstratestheversatilityandeffectivenessofourCTNcaptiongenerationapproachinlabeling
unlabeled videos. By leveraging the power of state-of-the-art image captioning models and our
20Input Video:
Sample Frames
Frame 1 Frame 2 Frame 3 Frame 4 Frame 5
Input Images from
unlabelled video
Zero-shot 'a tractor is driving' 'there are four trucks' 'a tractor is upside 'a game is being played' 'two people are celebrating'
Image Captions down'
CTN Caption Prompt
Generation EvLaL lLuL aMMtion
Cause: 'a tractor driving recklessly flips over and ends up upside down'
CTN Caption
Effect: 'causing an accident, while others celebrated unaware'
Figure 18: Application of our CTN caption generation approach for labeling unlabeled videos. Given an
unlabeledvideo,weextractframesandgenerateimagecaptionsusingastate-of-the-artimagecaptioningmodel
(GIT[51]).ThesecaptionsarethenusedasinputtoourLLM-basedCTNcaptiongenerationpipeline,which
producesaCTNcaptionfortheentirevideo.ThegeneratedCTNcaptioncapturesthecause-effectrelationships
andtemporaldynamicsinthevideo,enablingeffectivelabelingofunlabeledvideocontent.
few-shot based prompt, we can generate high-quality CTN captions that accurately capture the
causal-temporalnarrativeinvideocontent,evenintheabsenceofhuman-annotatedcaptions. This
approachhasthepotentialtosignificantlystreamlinetheprocessoflabelinglarge-scalevideodatasets
andenablemoreeffectivevideounderstandingandretrievaltasks.
F AdditionalQualitativeResultsforCENArchitecture
Inthissection,wepresentadditionalqualitativeresultscomparingthegroundtruthCausal-Temporal
Narrative(CTN)captionswiththecaptionsgeneratedbyourCENarchitecture. Theseexamples
furtherdemonstratetheeffectivenessofourapproachincapturingthecausal-temporalrelationships
andgeneratingaccurateandcontextuallyrelevantcaptions.
33
CTN caption GT: 'a person is working on a rubiks cube the rubiks cube gets solved
piece by piece'
CEN (OURS): 'a person is playing with a rubiks cube and the person successfully
solved the cube'
Figure19:ComparisonofCTNandCENcaptionsforavideoofapersonsolvingaRubik’scube.
InFigure19,boththeCTNandCENcaptionsaccuratelycapturethecausalrelationshipbetweenthe
personworkingontheRubik’scubeandthecubebeingsolvedpiecebypiece. Similarly,inFigure
20,bothcaptionscorrectlydescribethecause-effectrelationshipbetweenthemanplayingthepiano
andthemusicfillingtheroom.
2133
CTN caption GT: 'a man is playing the piano music fills the room'
CEN (OURS): 'the man is playing the piano and he is demonstrating how to play the
piano'
Figure20:ComparisonofCTNandCENcaptionsforavideoofamanplayingthepiano.
33
CTN caption GT: 'man shared a sad story on stage audience became emotionally
engaged'
CEN (OURS): 'the man gave a lecture on stage and the audience listened intently'
Figure21:ComparisonofCTNandCENcaptionsforavideoofamangivingalectureonstage.
33
CTN caption GT: 'a band performs a song in a video people watch and listen to the
music'
CEN (OURS): 'a band is performing a song and the crowd is enjoying the music'
Figure22:ComparisonofCTNandCENcaptionsforavideoofabandperformingasong.
Figure 21 demonstrates the ability of our CEN architecture to generate captions that capture the
audience’sengagementinresponsetotheman’slectureonstage. WhiletheCTNcaptionspecifically
mentions the emotional engagement of the audience due to the sad story, the CEN caption more
generallydescribestheaudiencelisteningintentlytothelecture.
Lastly, in Figure 22, both the CTN and CEN captions accurately depict the causal relationship
between the band performing a song and the crowd enjoying the music. The CEN caption, in
particular,directlystatesthecrowd’senjoymentasaresultoftheband’sperformance.
TheseadditionalqualitativeexamplesfurthervalidatetheeffectivenessofourCENarchitecturein
understandingandarticulatingthecausal-temporalnarrativespresentinvideos,generatingcaptions
thatarecoherent,accurate,andcontextuallyrelevant.
22