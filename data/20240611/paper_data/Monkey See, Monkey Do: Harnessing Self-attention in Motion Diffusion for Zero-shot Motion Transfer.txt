Monkey See, Monkey Do: Harnessing Self-attention in Motion Diffusion
for Zero-shot Motion Transfer
SigalRaab1, InbarGat1, NathanSala1, GuyTevet1, RotemShalev-Arkushin1,
OhadFried2, AmitH.Bermano1, DanielCohen-Or1
TelAvivUniversity,Israel1 ReichmanUniversity,Israel2
sigal.raab@gmail.com
Giventheremarkableresultsofmotionsynthesiswithdiffusionmodels,
anaturalquestionarises:howcanweeffectivelyleveragethesemodels
formotionediting?Existingdiffusion-basedmotioneditingmethodsover-
Leader
looktheprofoundpotentialofthepriorembeddedwithintheweightsof
pre-trainedmodels,whichenablesmanipulatingthelatentfeaturespace; Followers
hence,theyprimarilycenteronhandlingthemotionspace.Inthiswork,we
exploretheattentionmechanismofpre-trainedmotiondiffusionmodels.
Weuncovertherolesandinteractionsofattentionelementsincapturingand
representingintricatehumanmotionpatterns,andcarefullyintegratethese
elementstotransferaleadermotiontoafolloweronewhilemaintaining
thenuancedcharacteristicsofthefollower,resultinginzero-shotmotion
transfer.Manipulatingfeaturesassociatedwithselectedmotionsallowsusto
confrontachallengeobservedinpriormotiondiffusionapproaches,which
usegeneraldirectives(e.g.,text,music)forediting,ultimatelyfailingtocon-
veysubtlenuanceseffectively.Ourworkisinspiredbyhowamonkeyclosely
imitateswhatitseeswhilemaintainingitsuniquemotionpatterns;hence
wecallitMonkeySee,MonkeyDo,anddubitMoMo.Employingourtech-
niqueenablesaccomplishingtaskssuchassynthesizingout-of-distribution
motions,styletransfer,andspatialediting.Furthermore,diffusioninversion
isseldomemployedformotions;asaresult,editingeffortsfocusongener-
atedmotions,limitingtheeditabilityofrealones.MoMoharnessesmotion
inversion,extendingitsapplicationtobothrealandgeneratedmotions.
Experimentalresultsshowtheadvantageofourapproachoverthecurrent
art.Inparticular,unlikemethodstailoredforspecificapplicationsthrough time
training,ourapproachisappliedatinferencetime,requiringnotraining. Fig.1. Motiontransfer.Thetoprowdisplaysaleaderperformingawalk-
Ourwebpage,whichincludeslinkstovideosandcode,canbefoundat ingmotion.Theleftcolumnshowcasessampleframesoffourfollowers,
https://monkeyseedocg.github.io. eachengagedinadifferentmotion.Thecentralblockpresentstheoutput
motion,wheretheoutlineoftheleader(e.g.,leadingleg)istransferredto
1 INTRODUCTION
thefollowersandintegratedwiththeirdistinctmotifs.Notethealignment
Humanmotionsynthesisisafundamentaltask,usefulforvarious ofthestepsfortheleaderandoutputmotions.Ourmotiontransferiscon-
ductedbymanipulatingself-attentionlatentfeaturesinazero-shotfashion.
fieldsincludingrobotics,autonomousdriving,healthcare,gaming
andanimation.Diffusionmodels[Hoetal.2020;Sohl-Dicksteinetal.
2015]standoutastheprevailingsynthesisparadigmacrossdifferent significantlyfewerDoF.Therefore,insightsregardingpre-trained
modalities,suchasimaging[Sahariaetal.2022b],video[Hoetal. imagingmodelsdonotdirectlyapplytomotion.
2022],3Dpointclouds[LuoandHu2021],andalsomotion[Dabral Inthefollowing,thetermmotionoutlinedenotesastructured
etal.2023;Tevetetal.2023].Withtheemergenceoffoundationmod- planthatorganizesthekeyblocksinvolvedinaspecificmotion
els[Bommasanietal.2021],ithasbeennaturalforsomemodalities, (e.g.,locomotionrhythm),andthetermmotionmotifsdenotesges-
likeimagingandvideo,toevolvetowardszero-shotediting[Geyer turesorpatterns(e.g.,characteristicpose).Thesetermsarefurther
etal.2024;Hertzetal.2023].Suchworkstypicallydependonthe elaboratedinSec.3.
priorinformationencodedintheweightsofpre-trainedmodels, Humansubjectsexhibithighlyexpressivemotions,containinga
whichfacilitatelatentfeaturespacemanipulation.Thiscapitalizes wealthofinformation.Forexample,ajumpingmotioncanbecom-
onadeepunderstandingoftheirintricacies,withattentionlayers binedwiththemessuchasraisingthearmsorclappingthehands.
playingadominantroleinthesemethods. Asubtlechangeineachmotionpatternwouldleadtheviewerto
However,thepriorinformationencodedinpre-trainedmotion acompletelydifferentimpression.Similarly,discerninganindivid-
diffusionmodelsremainslargelyunexplored.Furthermore,ano- ual‚Äôsmoodoragebecomespossiblebyobservingashortdurationof
tabledisparityexistsbetweentheimagingandmotiondomains. theirwalkingpattern.However,accuratelyconveyingthesesubtle
Imagespossessaregularized2Dspatialstructurewithaconsider- nuancesthrougheitherhigh-levelcontrolssuchastextorlow-level
ablenumberofdegreesoffreedom(DoF),whereasourmotionis controlssuchasjointtrajectoryposesachallenge.Moreover,mo-
definedovera3Dhumanskeletonwitha1Dtemporalaxis,and tiondatasetsarelimited,andacquiringreal-worldhumanmotions
4202
nuJ
01
]VC.sc[
1v80560.6042:viXra2 ‚Ä¢ Raab,S.etal.
representingvariousmotifssolelythroughmotioncapture(MoCap) Weintroduceacomprehensivebenchmarktoevaluateourwork.
isexpensive,slow,andunscalable. Ourbenchmark,namedMTB,willbemadepubliclyavailable.It
Diffusionworksthatfacilitatemotionediting[Goeletal.2023; comprisesselectedmotionpairsfromtheHumanML3D[Guoetal.
Tevet et al. 2023] modify motion features (e.g., rotation angles), 2022]testsetandisdescribedinSec.5.1.UsingMTB,wecompare
hencearelimitedtofixedsetsofjointsoroperations.Furthermore, our model to current state-of-the-art methods and demonstrate
theyrelyontextualcontrolandthuscannotconveysubtlemotifs. thatnoneofferthesamebreadthoffunctionalityasMoMo,which
Worksthatdomanipulatelatentfeatures[Raabetal.2023;Tevet consistentlyoutperformsthem.
etal.2022]precedethediffusionmodelsera,lackingaccesstotheir
2 RELATEDWORK
richpriors.
Inthiswork,weexaminetheself-attentionmechanismwithinthe 2.1 MotionSynthesis
motiondomain(Sec.4)andsuggestanunpairededitingframework
Multimodalsynthesis. Petrovichetal.[2021,2022]incorporate
thattransfersaleadermotiontoafolloweronewhilepreservingthe
aTransformer[Vaswanietal.2017]architectureforthetasksof
subtlemotifsofthefollower,therebyreducingrelianceoncostly
action-to-motionandtext-to-motion.T2M[Guoetal.2022],T2M-
MoCapsystemsoroverlygeneraltextualdescriptions.Wenameour
GPT[Zhangetal.2023b]andMotionGPT[Jiangetal.2024]use
work‚ÄúMonkeySee,MonkeyDo‚Äù(dubbedMoMo),asitencapsulates
VQ-VAE[VanDenOordetal.2017]toquantizemotion,thense-
theconceptofdirectlytransferringamotionoutlinefromaleader
quentiallysynthesizeitinthequantizedspaceconditionedontext.
toafollowerwhileretainingthefollower‚Äôsuniquemotifs,muchlike
Morerecently,MDM[Tevetetal.2023]andMofusion[Dabraletal.
amonkeymimickingobservedbehaviorinamonkeyishmanner.
2023]adaptedthedenoisingdiffusionframework[Hoetal.2020]for
MoMooffersaversatilemotiontransfertechnique,facilitating
motionsynthesisandshoweditsmeritsformultimodaltaskssuchas
tasksunifiedbythecoreconceptoftransferringmotifsfromonemo-
action-to-motion,text-to-motion,andmusic-to-motion[Tsengetal.
tiontoanother.Specialcasesincludestyletransfer(e.g.,transferring
2023].TheMAS[Kaponetal.2023]algorithmextendeddiffusionto
awalkingmotiontozombiewalking),spatialediting(e.g.,transform-
out-of-domainmotionsbyleveragingvideodata.MoMoexcelsby
ingajumpingmotioninto‚Äújumpingwithhandsup‚Äù),actiontransfer
usingafollowerreferenceinsteadofanoverlygeneralizedtext.
(e.g.,transitioningfromwalkingtorunning),andout-of-distribution
synthesis(e.g.,generatingadancinggorilla).SeeFigs.1and7for Spatialcontrolandediting. MotionGraphs[ArikanandForsyth
moreexamples.Ourmodeloperatesatinferencetimewithoutre- 2002;Kovaretal.2002;Leeetal.2002]areapopulardatastructure
quiringoptimizationortrainingandcanfunctionseamlesslywith fortraversingtheposespaceofagivenmotiondatasettosynthesize
anyunderlyingmotionsynthesisbackbone(i.e.,foundationmodel) newmotionvariations,oftenaccordingtoaninputtrajectory.GAN-
thatutilizesself-attention,regardlessofitsspecificarchitecture. imator[Lietal.2022]andSinMDM[Raabetal.2024]showthat,
Ourstudyhypothesizesthatself-attentionelementscancapture analogouslytoimages[Nikankinetal.2023;Shahametal.2019],
complexmotionpatterns,delvesintothedistinctfunctionalities overfittingasinglemotionallowslearningitsinternaldistribution
of attention elements and examines their interplay. Inspired by andsynthesizingnewvariationsofitwithspatialandtemporal
researchintheimagingdomain[Alalufetal.2023;Caoetal.2023; variations.Usingdiffusion,MDMenablesbothjointsandtemporal
Huetal.2023],wedeviseamethodwherethecapabilityofaqueryùëÑ editingbyadaptingimagediffusioninpainting[Sahariaetal.2022a;
inaleadermotion,isutilizedtodetectthemostrelevantkeysùêæfrom Songetal.2020b].Followupworksdemonstratethemeritsofprop-
afollowermotion.Specifically,wecalculateanattentionscoreusing agatingcontrolsignalsthroughthegradualdenoisingprocessfor
theùëÑfromtheleadermotionandtheùêæ fromthefollowermotion. variousapplicationssuchaslongmotionsynthesis[Petrovichetal.
Thisscoreisutilizedtoextractaweightedcombinationofvalues 2024;Zhangetal.2023a],motionin-betweening[Cohanetal.2024;
ùëâ fromthesamefollowermotion.Consequently,anewmotionis Xieetal.2023],andsinglejointcontrol[Karunratanakuletal.2023;
synthesized,incorporatingtheoutlinefromtheleadermotion,and Shafiretal.2024].ComMDM[Shafiretal.2024]usesdiffusionto
themotifsfromthefollowermotion,maintainingfidelitytoboth.In synthesizetwoactors.InterGen[Liangetal.2024]followsitand
essence,theleadermotiondeterminesthe‚Äúwhat‚Äù and‚Äúwhen‚Äù,and showstheeffectivenessofcross-attentioninthetrainingprocess.
thefollowermotiondeterminesthe‚Äúhow‚Äù.Forexample,aninstance SINC[Athanasiouetal.2023]andFineMoGen[Zhangetal.2024]
ofthe‚Äúwhat‚Äùcouldbe‚Äúastepforwardwiththerightleg‚Äù,whilean useLargeLanguageModels(LLMs)tobreakthetextpromptto
instanceofthe‚Äúhow‚Äùcanbe‚Äúrunning‚Äù,‚Äútiredly‚Äù,or‚Äúwithhands instructeachbodypartseparately.Goeletal.[2023]usethecoding
raised‚Äù.Figure5illustratestheimplicitsemanticcorrespondence skillsofLLMswithpredefinedposemodifiersforframeediting,then
betweentheleaderandfollowermotions,whichdoesnotrequire blendthemusingdiffusion.Ourapproachenablesspatialeditingof
additionalsupervision. theleadermotionasaspecialcaseofmotiontransfer.
Ourworkstandsasthesoleapproachcapableofutilizingmotion Styletransfer. OneofthespecialcasesenabledbyMoMoisstyle
DDIMinversionwithindiffusionmodels[DhariwalandNichol2021; transfer.Holdenetal.[2017a,2016]havesuggestedlearningthe
Songetal.2020a].Inthefieldofimaging,theintegrationofinver- motionmanifoldusinganauto-encoderneuralnetwork.Thelatent
sionwithdiffusioniswidelyused,facilitatingthemanipulationof spaceoftheauto-encoderexposessemanticfeaturesofthemotion,
realimages[Garibietal.2024;Huberman-Spiegelglasetal.2023; whichenablesmotionstylizationusingtheGrammatrixheuristic
Mokadyetal.2023].However,inthemotiondomain,diffusionmod- assuggestedbyGatysetal.[2015].Abermanetal.[2020]usethe
elstypicallyavoidemployinginversion.MoMoutilizesinversion, AdaINheuristictodisentanglecontentandstyleaspresentedin
therebyenablingeditingofbothrealandgeneratedmotions. StyleGAN[Karrasetal.2019],followedbyGuoetal.[2024]andKimMonkeySee,MonkeyDo ‚Ä¢ 3
etal.[2024].UnlikeMoMo,thesemodelsaretrainedonpredefined Xout
stylesandstruggletogeneralize.
Xl 0dr
0
Xf 0lw
2.2 AttentionControlintheImagingDomain Denoising Net Denoising Net Denoising Net
Thelatentinformationencapsulatedintheattentionlayersofthe
popularUNet[Ronnebergeretal.2015]architectureisextensively Self Attention Mixed Attention Self Attention
usedintheimagedomaintoguideandcontrolthedenoisingdif-
Qldr Kldr Vldr Qldr Kflw Vflw Qflw Kflw Vflw
fusionprocess.PnP[Tumanyanetal.2023],MasaCtrl[Caoetal. t t t t t t t t t
2023]andCIA[Alalufetal.2023]showthattheself-attentionlayers
encodestructuralinformationthatcanbeusedtoeditanimage
Xldr Xout Xflw
withoutlosingitsoriginalcomposition.Prompt-to-Prompt[Hertz t t t
etal.2023]andAttend-and-Excite[Cheferetal.2023]showthat
certainaspectsoftheimagecanbeeditedbymanipulatingthecross- Xout cout
T
attentionwiththeinputtext,withoutaffectingtherest.Patashnik
etal.[2023]andDaharyetal.[2024]aremanipulatingtheself-and cldr Xldr copy copy cflw Xflw
T T
cross-attentionlayerstocontrolthelayoutoftheimageandavoid
semanticleakagebetweenitsdifferentparts.Tune-A-Video[Wu
etal.2023],TokenFlow[Geyeretal.2024]andQ-NeRF[Patashnik Inversion Inversion
Optional
etal.2024]observethattheattentionquery,ùëÑ,encodesthestructure
whilethekeyandvalue,ùêæ andùëâ,encodetheappearance,anduse
Xldr Xflw
itformutualeditingofimagespreservingtemporalandstructural
consistencies.Ourworkfollowsthelatter,leveragingself-attention
layersformotionediting.Unlikeimagingworks,ourworkuses
Fig.2. TheMoMoPipeline.Theinputtoourmodelistwonoisytensors,
layersfromatransformerandnotfromaUNet. ùëãldr andùëãflw,producedbyeitherinvertingrealmotionsorsamplinga
ùëá ùëá
Gaussiannoise.Thetwotensorsrepresentleaderandfollowermotions,and
3 MODEL aregivenalongwiththeirassociatedtextprompts.Weinitializeouroutput
motion,ùëãout,usingtheinitialnoisefromtheleadermotionandpairitwith
Thissectionsuggestsaneditingframeworkthattransferstheoutline ùëá
thetextpromptfromthefollowermotion.Thethreenoisedmotionsùëãldr,
ofaleadermotiontoafolloweronewhilepreservingthemotion ùëãflwandùëãout,arepassedtothefrozendenoisingnetworkateachtimestùë°
ep
motifsofthefollower.Ourunpairedframeworkoperatesatzero- ùë°,ùë° alongwitùë° htheirpromptsandwithùë°.Withinthedenoisingnetwork,ùëãout
ùë°
shot,withoutrequiringoptimizationormodeltraining. undergoesmixed-attentionbycombiningthequeryfromtheleadermotion
A motion outline denotes a structured plan that arranges the withthekeyandvaluefromthefollowermotion.Meanwhile,ùëãldrandùëãflw
ùë° ùë°
essentialmovementsnecessaryforaspecificmotion.Itprovides followastandarddiffusionprocess.
avisualblueprintforcomprehendingthesequenceofactionsand
transitionsneededtoexecutethemotioneffectively.Anexampleof
asingleframe,alsoknownaspose.Finally,letùêΩ denotethenum-
amotionoutlinewouldbe‚Äústandstillonframes10-20,stepwith
berofskeletaljoints.Weadheretotherepresentationusedinthe
rightlegonframes21-25‚Äù,etc.Notethattheoutlineisasgeneralas
HumanML3Ddataset[Guoetal.2022],wherethefeaturesfromall
possible;forexample,thetypeofstep(walk,run,hop)belongstothe
thejointsareconcatenatedintoasinglelargefeature,resultingin
motifs.Motionmotifsincludesubtlenuances,gestures,orpatterns
amotionrepresentationùëã ‚ààRùëÅ√óùêπ .Detailsregardingtheinternal
thatconveymeaningandemotion.Thesemotifsmayrepeatand
representationofthefeaturescanbefoundintheAppendix.
vary,formingexpressivemotionsequences,andaidinginestablish-
ingvisualthemesandnarratives.Considervariousrunningmotions, SelfAttention. Werecapself-attention[Vaswanietal.2017],as
eachwithdistinctmotifs.Thesemotionsconveypersonalstyles itplaysakeyroleinourframework.Letùêºùêª (‚Äúinputhidden‚Äù)bea
expressedbybodyangle,footpositioning,handgestures,airborne latenttensoroffeaturesfedasinputtoaself-attentionlayer,and
duration,etc.Evenwithextensivepromptengineering,capturing letùêªÀÜ betheoutputofthislayer.Theelementsquery,key,andvalue,
everysubtlemotifremainsunattainable.Conversely,incorporating arecalculatedrespectivelyby
motifsfromagivenmotionensurescompletefidelitytothatmotion. ùëÑ =ùêºùêª ¬∑ùëä ùëûùëá +ùëè ùëû, ùêæ =ùêºùêª ¬∑ùëä ùëòùëá +ùëè ùëò, ùëâ =ùêºùêª ¬∑ùëä ùë£ùëá +ùëè ùë£, (1)
Ourframeworkenablestransitionsacrossmotionsofdifferent
temporallengths.Theoutlinesofaleadermotioncanbetransferred
where (ùëä ùëû,ùëè ùëû), (ùëä ùëò,ùëè ùëò), and (ùëä ùë£,ùëè ùë£) are learned linear projec-
tions.
tomultiplefollowers,asshowninFig.1,andmultipleoutlinescan
beseparatelyappliedtoasinglefollower.
Foreachqueryvectorùëû
ùëõ
‚ààùëÑattemporallocation(i.e.,frame)ùëõ,
anattentionscoreiscomputedbasedonallkeysinùêæ.Thisscore
indicatestherelevanceofeachkeytothequeryùëû ùëõ,assessingtheir
3.1 Preliminaries
similarity.Theattentionscoresarenormalizedthroughasoftmax
MotionRepresentation. LetùëÅ denotethenumberofframesina operation,whichdeterminestheweightingofeachvalueinùëâ,to
motionsequence,andùêπ denotethelengthofthefeaturesdescribing beusedforupdatingthefeaturesatlocationùëõ.Theweightedvalues4 ‚Ä¢ Raab,S.etal.
Q K
Walk
Walk then turn
Turn while
crouching
Locomotion phases Motion start/end Bend down Stand Walk Crouch Turn
Fig.3. DominantfeaturesinQvs.K.Eachrowdepictstwocopiesofthesamemotion,showcasingtheK-MeansclusteringofitsùëÑandùêæfeaturesinthe
leftandrightcolumns,respectively.NotehowthefeaturesinùëÑaredominatedbytheoutline,whilethoseinùêæaredominatedbythemotionmotifs.IntheùëÑ
column,periodicstepsshareclusters,ignoringuniquepatterns.Intheùêæcolumn,clustersarerelatedtomotionmotifs;thus,walking,turningwhilewalking,
andcrouchingwhilewalkinghavedistinctclusters.TemporalinformationisevidentintheclustersofùëÑbutnotinthoseofùêæ.IntheùëÑcolumn,thebeginnings
ofthefirsttwomotionsandtheendofallthreearehighlightedbythecolorsoflowandhighframenumbers,respectively.
arethenaggregatedtoproducetheoutputateachquerylocation, standardDDIMdenoising,whileùëã ùë°outisdenoisedusingourmixed-
via attentionblock,describednext.Finally,MoMoproducestheoutput
ùê¥ ùëõ =softmax(cid:32) ùëû ‚àöÔ∏Åùëõ |ùêº¬∑ ùêªùêæ ùëõùëá |(cid:33) , ‚ÑéÀÜ ùëõ =ùê¥ ùëõ¬∑ùëâ, (2) m moo T dt hi eo e ln d thùëã e an0o tou uit s. ti in lig zen se st ew lfo -r ak tti en no tiu or np li ap ye eli rn se .Ic nan oub re ea xn py em rio mti eo nn ts-d ,e wn eo eis min -g
ploy a variant of MDM as a backbone. Note that, while related
whereùê¥ ùëõisthenormalizedattentionscoreatframeùëõ,‚ÑéÀÜ ùëõ ‚ààùêªÀÜ isthe worksintheimagingdomainutilizetheself-attentionlayersofa
self-attentionresultatframeùëõ,and|ùêºùêª ùëõ|isthefeaturesnumberin
UNet[Ronnebergeretal.2015],weleveragetheself-attentionlayers
frameùëõ.Finally,ùêªÀÜ isusedasaresidualandaddedtotheinputùêºùêª,
ofaTransformer[Vaswanietal.2017]duetotheusageofMDM.
ùëÇùêª =ùêºùêª+ùêªÀÜ, (3) 3.3 LeveragingSelf-attention
whereùëÇùêª (‚Äúoutputhidden‚Äù)isthetensorpassedtothenextlayer. Ourproposedframeworkintegratesself-attentioncomponentsfrom
Weusemulti-headattentionandomititsnotationsforbrevity. boththeleaderandfollowermotionsintoasingleoutputmotion.In
theimagingdomain,Caoetal.[2023]havestudiedtheself-attention
MDM,DDPMandDDIM. MotionDiffusionModel(MDM)[Tevet
layersoftext-to-imagedenoisingnetworks.Theydemonstratethat
etal.2023]isawidespreadmodelforhumanmotionsynthesisand
keepingthekeysandvaluesoftheselayershelpspreservethevisual
editing.Inourworkweuseavariationofit,hencewerecapithere.
characteristicsofobjectswhenperformingnon-rigidmanipulations
MDMusesDenoisingDiffusionProbabilisticModels(DDPM)[Ho
on a given image. Alaluf et al. [2023] made further progress by
etal.2020],whicharetrainedtotransformunstructurednoiseinto
combiningstructureandappearancefromtwoimages.
samplesfromaspecifieddistribution.Thisisachievedthroughan
Inspiredbytheseinsights,thisworkillustratesthecrucialfunc-
iterativeprocessinvolvingthegradualremovalofsmallamountsof
tionsofqueries,keys,andvaluesinencodingsemanticmotioninfor-
Gaussiannoise.UnlikeMDM,inthiswork,weemployDDIM[Song
mation.Wefindthatleveragingthequeries,keys,andvaluesfrom
et al. 2021] for inversion and deterministic inference, aiming to
self-attentionlayersenablesthetransferofsemanticinformation
reconstruct inverted motions precisely to their original form. A
acrossdifferentmotions.
recapregardingDDPMandDDIMcanbefoundintheAppendix.
InSec.4weshowthatthisapproachenablestheimplicittransfer
ofmotionpatternsbetweensemanticallysimilarframes.Morepre-
3.2 Pipeline
cisely,ateachdenoisingstepùë°,weuseourmixed-attentionblock
MoMoemploysapre-trainedandfixedmotiondiffusionmodelto toinjectthequeriesfromtheleadermotionùëãldr,andthekeysand
synthesizetheoutputmotionùëãoutbyapplyingthemotionoutlineof valuesfromthefollowermotionùëãflwtotheself-attentionblockof
ùëãldrontoùëãflw,whereùëãldrandùëãflwareeithergiven(real)motions theoutputmotionùëãout,via
orgeneratedones(SeeFig.2).
Theinputtoourframeworkistwoinputnoises,ùëã ùëáldrandùëã ùëáflwand
ùëÇùêªout=ùêºùêªout+softmax(cid:32) ùëÑldr¬∑ùêæflwùëá (cid:33)
ùëâflw. (4)
theircorrespondingtextpromptsùëêldrandùëêflw,respectively.Thein- ‚àöÔ∏Å |ùêºùêª ùëõ|
putnoisesareeitherinvertedfromrealmotionsusingDDIM[Song
4 UNDERSTANDINGSELF-ATTENTIONFEATURES
etal.2021]orsampledfromaGaussiandistribution,inwhichcase
MoMorunsongeneratedmotions.Thetextpromptsassistincon- Inthissection,weexploresomeofthepriorinformationencodedin
trollingsynthesisfromnoiseandinvertingthemotionsifinversion pre-trainedmotiondiffusionmodelsandidentifyusefulattributes
isused.Ateachtimestepùë°,wepassthethreenoisedmotionsùëã ùë°ldr, withinit.Inthefollowing,wedemonstratethat(i)thequeriesùëÑ
ùëã ùë°flwandùëã ùë°outtothedenoisingnetwork.ùëã ùë°ldrandùëã ùë°flwgothrougha establishafocalpointforcontextualdetermination,(ii)thekeysùêæMonkeySee,MonkeyDo ‚Ä¢ 5
er <Jumping forward= followermotionthatelicitsthegreatestactivationintheattention
ollow mapùëû ùëõldr¬∑ùêæflwùëá .Thesepairingsportraythetransitionfromtheleader
F tothefollowermotion,highlightingthemostsignificantactivations.
Asdepicted,thesepairingsexemplifyalignmentintheoutline,such
as the synchronization of body movement (‚Äúup‚Äù/‚Äúdown‚Äù) across
frames.Theattentionmapsinthissectionarecomputedforlayer
<Jumping jacks= 10anddiffusionstep50.
4.3 AttentionMaps
After computing attention maps in the previous section, which
Each leader frame is replaced by its nearest neighbor from the follower
revealedthehighestactivationforeachframe,wenowdelveinto
Fig.4. Correspondenceviaattention.Followerframesarecolor-coded
thecompleteattentionmapsfortwoselectedframes.Withineach
accordingtoconsecutiveindices(toprow).Nearestneighborfollowerframes
(bottom)aretheonesthatachievethehighestmixed-attention(ùëÑldr¬∑ùêæflwùëá) rowinFig.5,wefocusonatemporalregionidentifiedbythenumber
ofitscenterframe.Ineachcolumn,wepresenttheattentionmaps
activation,shownrespectivelytoleader‚Äôsframes(middlerow).Thesecor-
respondencesaresemanticallyaligned,e.g.,moving‚Äúup‚Äùand‚Äúdown‚Äùsub- foreachquerylocationusingvariouscombinationsofkeysand
motionsareconsistentlyassignedwithfollowermoving‚Äúup‚Äùand‚Äúdown‚Äù queries.Naturally,whenkeysandqueriesfromthesamemotion
frames.Someofthenearestneighborsarehighlightedwitharrows.
aremultiplied(ùëÑldr¬∑ùêældrùëá
),theyproducehighsimilarityscoresfor
regionsresemblingthequeriedpose,especiallythequeriedframe
serveasalearnedframedescriptor,enablingthemodeltoassessthe itself.Incontrast,inthefollowermotion,theposeatthesameframe
importanceofdifferentframesinthemotionrelativetoaspecific numberasthequerymaynotbesimilar,makingitunhelpfulfor
query,and(iii),thevaluesùëâ denotethecontextualrepresentation retargetingtheleadermotion.Intherightcolumn,weuseMoMoto
weseektogenerate,guidingthemodelinshapingthefeaturesat
computeùëÑldr¬∑ùêæflwùëá
.Asillustrated,thequeriesfocusonsemantically
eachquery‚Äôstemporallocation.Inparticular,weshowthatwhile similarregionsinthefollowermotion.
informationregardingtheoutlineandthemotifsofthemotionis Notethatthesecorrespondencesaremadedespitethetwomo-
containedinbothùëÑandùêæ,theoutlineinformationismoredominant tionshavingdifferentsequencesofposes.Asaresult,multiplying
inùëÑ andthemotifsinformationismoredominantinùêæ.Thiskey theattentionmapswiththefollowervaluesùëâflw,enablesthetrans-
insightispresentedinSec.4.1andservesasthefoundationonwhich feroftheoutlineoftheleadermotionontothefollower‚Äôsmotifs.
ourmixed-attentionblockwasbuilt;tothebestofourknowledge, Finally,Fig.6depictsattentionmapsforthefullmotionlength,
thisinsighthasnotbeenexploredintheimagingormotiondomains. providingvisualinsightintoleader-followercorrespondence.
4.1 DistinguishingBetweenùëÑ andùêæ 5 EXPERIMENTS
Tounderstandthedifferentrolesofqueriesandkeys,weextract Theresultsinthisworkarecomputedusingthetransformerdecoder
theirfeaturesfromachosenself-attentionlayer‚Ñìatdiffusionstep versionofMDM.Theexacthyperparametervaluesaredetailedin
ùë°,reducethedimensiontoùëëoutputchannelsbyapplyingPCA,and theAppendix.Inpractice,eq.(4)isappliedindiffusionsteps90to
grouptheframesintoùëöclustersusingK-Means.WhileùëÑ andùêæ 10(outof100)andinlayers2to12(outof12).
containbothoutlineandmotifsinformation,applyingK-Means
5.1 Benchmark
emphasizesthemoredominantfeatures.Figure3visualizeseach
clusterusingadifferentcolor.Clusteringùêæ showsthatitisdomi- ToevaluateMoMo,weintroduceMotionTransferBenchmark,dubbed
natedbythemotionmotifsandthatsimilarmotifsaregroupedinto MTB,ofleaderandfollowermotionpairs,whichwillbemadeavail-
thesameclusters.ClusteringùëÑdepictsthatitisdominatedbythe
able.MTBisasubsetoftheHumanML3D[Guoetal.2022]testset
motionoutline.Inparticular,(i)periodicsub-motions,likesteps,are andisstraightforwardlyfilteredaccordingtothetextualprompts
groupedintothesamecluster,dominatingovermotionmotifs,and attachedtothemotions.Fortheleadermotions,weincludemotions
(ii)thetemporalsignalisdominant.From(i)weconcludethatthe thatcontainlocomotionverbssuchas‚Äúrun‚Äùor‚Äúwalk‚Äù.Forthefol-
ùëÑùë†indifferentperiodicmotionssharesimilarfeatures,thustheycan
lowermotions,weincludemotionswithtextthatcontainswords
be‚Äúunderstood‚Äùbyùêæùë†fromothermotions.Thisfindingexplains
suggestingmotionmotifs,suchas‚Äúchicken‚Äùor‚Äúclap‚Äù.Theword
whyourmotiontransferworkswell.Interestingly,ùëÑencodesthelo-
choices,sharedinourAppendix,aremadestraightforwardlyand
comotionphaseswithoutexplicitlylearningitasinPFNNs[Holden donotinvolvelargelanguagemodels.
etal.2017b;Starkeetal.2022].OurPCAisappliedon256generated Wecreatepairsofleaderandfollowermotionsbycombining
motionswith128textprompts.Weuse‚Ñì=11,ùë°=30,ùëë=10,andùëö=10.
sentencesfromtherespectivegroupsinaCartesianproduct.How-
ever,suchacombinationinducesapproximately46ùêæ pairs,which
4.2 Correspondenceviaattention
ismorethanweneed.Todecreasethenumberofpairs,insteadof
Given two entirely different motions, we show that the nearest usingallthecrosscombinationsofleaderandfollower,weallow
neighborfromafollowermotion,foraqueryùëÑfromaleadermotion, upto20repetitionsofeachfollowersampleandnorepetitionsfor
wouldbetheframethatresemblesitmostinitsoutline.InFig.4,for theleadersamples.Inpractice,weuse4leadersearchwords,re-
everyframeùëõwithintheleadermotion,wediscerntheframeinthe sultingin842motions,and17followersearchwords,resultingin
tseraeN
redaeL
srobhgieN6 ‚Ä¢ Raab,S.etal.
Leader Follower
A person does jumping jacks A person raises his hands then lowers them down
24 24
60 60
Similarity
low high
Fig.5. Attentionmapperquery.Intheleftcolumn,wedisplaythreecopiesoftheleader;intherightcolumn,weshowcopiesofthefollower.Thetopcopies
depictthemotionsastheyare,whiletheonesbelowhighlightattentionscores.Wedefinetwoqueriescorrespondingtodifferentsemantictemporalregionsin
theleadermotion.Eachquerycorrespondstoadifferentpose,withvariedarmdirectionorbodystretch.Eachmotioncolumndisplaysattentionmapsfroma
singlelayer,computedindifferentways.Intheleftcolumn,wepresentself-attentionmapsderivedfromqueriesandkeysfromtheleadermotion,causingeach
querytoconcentrateonsemanticallysimilarregionswithinthatmotion.Theframenumberrelatedtoeachqueryisindicatedwithanarrow.Forexample,the
queryinframe24focusesonaposeof‚ÄústandinglowinanApose‚Äù,intheleadermotion.However,framenumber24correspondstoanentirelydifferentpose
inthefollowermotionintherightcolumn.Intherightcolumn,weapplyMoMo,aligningleaderqueriesùëÑldrwithfollowerkeysùêæflw.Thiswayweensurethat
eachqueryfromtheleadermotionalignswithsemanticallysimilarregionsofthefollowermotion.Forinstance,inframe60,thequeryhighlightstheregion
wherethecharacterraisestheirarms.Theframeswithhighercorrespondence(red)intherightcolumnalsobelongtocharactersraisingtheirarms.
55motions(somesearchtermsresultinlessthanthreesentences). secondcomputesasoftmaxontopofthesimilarityscores,extract-
Altogether,MTBincludes842(leader,follower)motionpairs. ingfollower‚Äôsframesaccordingtothesoftmaxweights.Thethird
computesthesimilaritybetweentheleader‚ÄôsùëÑandthefollower‚Äôs
5.2 Metrics ùêæ latentfeatures,andextractsthefollower‚Äôsvalueùëâ accordingto
themostsimilar(i.e.,nearestneighbor)frame.
Weaimtoassesstheresultsbasedonthreecriteria:(a)thequalityof
StyleTransfer. StyleTransferisaspecialcaseofmotiontransfer.
theoutputmotions,(b)howcloselytheoutputmotionsfollowthe
Itreferstodoingagivenactioninadifferentwaythatrepresents
outlineoftheleadermotion,and(c)howwelltheoutputmotions
anemotionoraphysicalstate,suchas‚Äúhappily‚Äùor‚Äúlikeamon-
alignwiththemotifsoftheirassociatedfollowermotions.Toevalu-
key‚Äù.Typically,itretainstheoriginalaction(e.g.,walk),whilemo-
atetheseaspects,wehavechosenspecificmetrics.Criterion(a)is
tiontransferencompassesanychangeinmotionmotifs,including
evaluatedusingFID,(b)isassessedbyfootcontactsimilarity,and
changingtheaction(actiontransfer)ormodifyingasubsetofjoints
criterion(c)isevaluatedbyR-precisionandsimilaritytofollower
withoutalteringthestyle(spatialediting).Whilemostmotionstyle-
locationsandrotations.Detailsaboutthesemetricsareprovidedin
transferapproaches[Abermanetal.2020;Guoetal.2024;Jangetal.
theAppendix.
2022]excelwithpredefinedstyleclassesbutstrugglewithunseen
5.3 Baselines styles,ourmethodseamlesslyhandlesanygivenstylemotion.Exist-
ingstyletransfermethodsrequiretrainingandnoneofthemutilizes
Noexistingbaselineencompassesallaspectsofmotiontransfer
diffusionmodels.Wecomparewithstate-of-the-artMoST[Kimetal.
addressedbyMoMo.Hence,wecompareMoMowithseveralworks,
2024],afterwetrainitontheHumanML3Ddataset.
each representing a different special case. We demonstrate that
Spatial Editing. Spatial editing adjusts specific joints, like the
MoMoofferssuperiorgeneralizationcapabilitiescomparedtothese
arms,whilepreservingtheoverallmotion.Amongspatialediting
works.Additionally,mostpriorartistailoredforspecificapplica-
motiondiffusionmethods,MDMinpainting[Tevetetal.2023],as
tionsthroughtraining,andoftenrequirespaireddata.Incontrast,
wellasMEO[Goeletal.2023],usetextualcontrolandmodifyend
MoMoisunpairedandaccomplishesitsfunctionalitiesduringinfer-
featuresofamotion(e.g.,rotationangles)withoututilizinglatent
encewithoutnecessitatingspecifictrainingoroptimization.
space.MDMinpaintingexcelsateditingbroadsetsofjoints,but
MotiontransferUsingNa√ØveNearestNeighbor. Toillustratethe needsrefinementforindividualjointedits[Shafiretal.2024].It
necessityoflatentspaceediting,wecompareMoMowithitsna√Øve reliesonhard-codedjointmasks,limitingeditstopredefinedbody
nearestneighbor(NN)equivalent,andexaminethreeapproaches. parts.MEO[Goeletal.2023]usesafinitehard-codedsetofediting
Thefirstapproachselectseachframe‚Äôsnearestcounterpartinthe operations.Incontrast,ourapproachisnotlimitedtoeditingalarge
followermotionandincorporatesitintotheoutputmotion.The setofjoints,hard-codedmasks,orafinitesetofeditingoperations.MonkeySee,MonkeyDo ‚Ä¢ 7
„óÑ     „óÉ     „óÑ     „óÉ     „óÑ     „óÉ     results.MoMo‚Äôsversionoperatingongeneratedmotionsoutper-
high
formstheoneoninvertedmotions.
5.5 QualitativeResults
Oursupplementaryvideoreflectsthequalityofourresults.Itpresents
leader multipletransferredmotionsandcomparisonstootherbaselines.
low frame 37 Itshowsthatthefirstnearestneighborapproachresultsinjittery
Key Key Key
outcomes,thesecondproducesunnaturaloutcomeswithfootslid-
ing,MDMInpaintingcannotgeneralizebeyonditsspatialediting
expertise,andMoSTstrugglestogeneralizeonunseenstyles.MoMo
output follower offersaversatilemotiontransfertechniqueexpressedinFigs.1and7
frame 63 frame 63
whereweseehowitfacilitatestasksunifiedbythecoreconcept
Fig.6. Attentionmapsforfullmotions.Theleftandmiddleself-attention
mapsrelatetoaleaderandafollower,performing‚Äúwalking‚Äùand‚Äúwalking oftransferringamotionoutlinefromaleadertoafollowerwhile
likeagorilla‚Äùmotions,respectively.Totherightistheirmixed-attention retainingthefollower‚Äôsuniquemotifs.Arrangedfromtoptobottom,
map.Forself-attention,themaindiagonalindicatesself-correlationand Fig.1demonstratesspecialcasesofspatialediting,styletransfer,and
thesecondarydiagonalsindicatetheperiodicityinthewalks.Formixed- actiontransfer.Fig.7illustratesactiontransfer,out-of-distribution
attention,thehighattentiondiagonalsrelatetothecorrelationbetween synthesis,andanotheractiontransfer.
theleaderandthefollower.Intheframesattributedtothecircledhigh
attentionscorewithinthemixed-attentionmap,bothleaderandfollower 5.6 Ablation
depictasimilarsteppingpose,justifyingthehighactivation.Theresulting
Ourinitialstudyfocusesonidentifyingtheappropriatelayersand
outputframeissimilar,butnotidentical,tothefollower‚Äôs,attributedtothe
diffusionstepsforwhichmixed-attentionshouldbeapplied.We
weightedsumthatcombinesmultipleinputs.Allmapsrelatetolayer6and
diffusionstep70inourbackbone. denotetherangeoflayersanddiffusionstepswheremixed-attention
isappliedas[ùë†-ùëôùëéùë¶ùëíùëü,ùëí-ùëôùëéùë¶ùëíùëü]and[ùë†-ùë†ùë°ùëíùëù,ùëí-ùë†ùë°ùëíùëù],respectively.We
havetestedapproximately200configurations,withvaryingvalues
ofùë†-ùëôùëéùë¶ùëíùëü,ùëí-ùëôùëéùë¶ùëíùëü,ùë†-ùë†ùë°ùëíùëù,andùëí-ùë†ùë°ùëíùëù.Table2displaysrepresentative
MDMInpaintingistheclosestdiffusionspatialeditingworkthat resultsofthevariationsweexperimentedwith.
providescode.Itexpectsaninputmotionandatext,soforeachpair Anotherablationtestexaminesthetextualpromptusedduring
ofbenchmarkmotions,theleaderisusedasis,andthefolloweris thesynthesisoftheoutputmotion.ResultsinTab.3confirmthat
givenbyitstextprompt. ourchoicetoreplicatethefollower‚Äôspromptisoptimal.
OODSynthesis. Out-of-distribution(OOD)synthesisentailsgener-
6 CONCLUSION,LIMITATIONSANDFUTUREWORK
atingmotionsthatwerenotencounteredduringthetrainingphase,
posingachallengetothenetwork‚Äôsgeneralizationcapabilities.For Wehaveexploredandleveragedthepowerfulmechanismofself-
instance,whenattemptingtogenerateadancinggorilla,thenetwork attention,withinpre-trainedmotiondiffusionmodels.Ourstudy
strugglestogeneralizetothiscombinationdespitebeingtrainedon hasresultedinanovelapproachformotioneditingindiffusion
individualinstancesofdancinghumansandofnon-dancinggorillas. models,whichwehavedemonstratedthroughmotiontransfer.
Thisdifficultyarisesfromthesparserepresentationofsuchcom-
binationsinthelatentspace.Byapplyingourproposedtechnique,
Table1. ComparisonwithbaselinesontheMTBbenchmark.MoMo
wegeneratelatentfeatureslocatedsparselyawayfromallother exhibitsthebestFIDandR-precisionresultsandcomparablesimilarity
features,andyet,theyaredenoisedintoanaturalmotion.Existing scores.MoMo‚Äôsversionthatrunsongeneratedmotions(abbreviated‚ÄúGen.‚Äù)
motiondiffusionmethodsdonottransferagivenmotionintoan hasslightlybetterresultsthantheonethatrunsoninvertedones(‚ÄúInv.‚Äù).
OODone.Hence,weevaluatetheOODsamplesinourbenchmark Baselinesthatexhibitthehighestsimilaritymetricsscores,showpoorFID
usingstyletransferandspatialeditingbaselines. andR-precisionresults.Na√Øvenearestneighborvariationsattaingoodsimi-
laritytothefollower,astheyarebuiltbyfeaturescopiedfromit.However,
ActionTransfer. Actiontransferistheapplicationoftransferring
theyexhibitpoorFIDduetoajitteryoutput.MoSTresultsaretoosimilar
theoutlineofoneactionintoanother,say,‚Äúrunningtowalking‚Äù totheleader,hencefailinsimilaritytothefollower,andMDMInpainting
or ‚Äúwalking to crawling‚Äù, where the output would be doing the workswellonlywhentheupperbodypartneedstobeedited,duetoits
follower‚Äôsactioninthesamerhythmandlegorderastheleader. fixedmask.Boldandunderlineindicatebestandsecond-bestresults,re-
AsforOODsynthesis,nocurrentmethoddiffusesmotionintoa spectively.
differentaction.Weassessactiontransfersamplesinourbenchmark
Metric RPrecision LeaderFoot Follower Follower
usingstyletransferandspatialeditingbaselines. FID‚Üì ‚Üë ‚Üë ‚Üë ‚Üë
Model (top3) ContactSim. Rot.Sim. Loc.Sim.
NNmotionspace 6.17 0.385 0.830 1.00 1.00
5.4 QuantitativeResults
+softmax 11.9 0.312 0.756 0.994 0.986
NNlatentspace 3.63 0.384 0.798 0.981 0.966
InTab.1wecompareMoMowiththebaselinesmentionedabove,
MoST[2024] 15.2 0.240 0.824 0.207 0.227
usingtheMTBbenchmark.OurworkexcelsinFIDandR-precision MDMInp.[2023] 3.51 0.213 1.00 0.244 0.329
whileachievingcomparablesimilarityscores.However,baselines MoMoGen.(Ours) 2.33 0.439 0.816 0.993 0.972
MoMoInv.(Ours) 2.50 0.490 0.793 0.933 0.856
withthehighestsimilarityscoresshowinferiorFIDandR-precision
yreuQ8 ‚Ä¢ Raab,S.etal.
Follower Leader Out
A person dancing A person walks in a circle
A person is dancing like a gorilla A person is dancing
A person is crouching A person is walking forward
time time time
Fig.7. QualitativeResults.Theresultsheredemonstratethetransferoftheleadermotion‚Äôsoutline,suchasthesequenceandpaceofsteps,tothefollower
whilepreservingthesubtlenuancesofthefollower‚Äôsmotion.Thefirstandthirdrowsrelatetothespecialcaseofanactiontransfer.Inthesecondrow,we
illustrateanout-of-distributionsynthesisscenario;initially,thefollowerlacksadancingmotion,despitethetextprompt.However,afterthetransferprocess,
thesynthesizedmotionimitatesagorilladancing,asspecified.
Ourzero-shotandunpairedmethodology,namedMoMo(Monkey transfer,actiontransfer,andspatialediting.Byharnessingmotion
See, Monkey Do), demonstrates promising capabilities in trans- inversion,MoMoextendsitsapplicabilitytobothrealandgenerated
ferringtheoutlineofaleadermotionontoafolloweronewhile motions.
preservingthesubtlemotifsofthefollower.Thisenablesavari- Experimental results demonstrate the merits of our approach
etyofsub-taskssuchasout-of-distributionmotionsynthesis,style comparedtothebaselines.Notably,itexcelsinapplyingfunction-
alitiesatinferencetimewithoutadditionaltraining.Ourfindings
shedlightontheimportanceofattentioninhumanmotionandlay
Table2. Layersandstepsablation.Thetabledisplaysrepresentative
thegroundworkforfutureadvancementsinthefield.
resultsofthevariationsweexperimentedwithdiffusionstepsandlayers
Theprimarylimitationofourworkisthedifficultyintransferring
inwhichtheself-attentionfeaturesaremanipulated.Thetoprowdisplays
amotionwhenbasicoutlineelementsoftheleadermotionarelack-
theconfigurationofourselectedmodel,whereweapplymixed-attention
forself-attentionlayers2-12andfordiffusionsteps90to10(outof100). inginthefollower.Forexample,iftheleadermotiondepictswalking
Inthemiddleblock,wemaintainourbestlayerconfigurationandtest whilethefollowerremainsstationary,thentheoutputmotionwill
variousdiffusionstepranges.Inthebottomblock,wemaintainourbest alsobestationary,receivingnoinputfromtheleader.However,
diffusionstepsconfigurationandexperimentwiththerangeoflayers.To thislimitationmaybeinherenttothetaskdescriptionitself,asour
selectthebestconfiguration,weprioritizedFIDandR-Precisionoverthe objectiveisprimarilytotransfermotionsthatsharesomedegree
othermetrics. ofcommonalityintheiroutlines.Anothercurrentlimitationisthat
oncealeaderandafollowermotionsaredetermined,thereisno
Diffusion RPrecision LeaderFoot Follower Follower
Layers Steps FID‚Üì (top3) ‚Üë ContactSim.‚Üë Rot.Sim.‚Üë Loc.Sim.‚Üë diversityintheoutputmotionsobtainedfromthem.Thislimitation
2-12 10-90 2.334 0.439 0.816 0.993 0.972 arisesfromthedeterministicnatureoftheDDIM[Songetal.2020a]
diffusionmodelweareusing.Inthefuture,outputdiversitycould
2-12 20-80 3.028 0.412 0.820 0.993 0.973
2-12 15-90 2.833 0.406 0.817 0.992 0.975 beachievedusingnon-deterministicmodels(e.g.,DDPM).
2-12 20-70 2.867 0.416 0.821 0.991 0.973 A possible future direction we offer here is to explore latent
4-9 10-90 4.063 0.373 0.795 0.978 0.971 featurelayersotherthanself-attention,e.g.,cross-attentionandfeed-
5-11 10-90 2.971 0.393 0.837 0.989 0.967
forward.Additionally,ourfindingsonmotionmotifspreservation
4-10 10-90 3.098 0.404 0.821 0.991 0.974
couldbeappliedtopersonalizationapplications,adomainpopular
inimaging[Galetal.2023]butcurrentlylackinginthemotion
Table3. Textpromptablation.Ineachrow,adifferenttextualpromptis
domain.
utilizedfortheoutputmotion.Resultsindicatethatusingthesametextas
thefolloweryieldsthebestoutcomes. 7 ACKNOWLEDGMENTS
WethankRinonGalandOrPatashnikforfruitfuldiscussions.This
Metric RPrecision LeaderFoot Follower Follower
FID‚Üì ‚Üë ‚Üë ‚Üë ‚Üë
Prompt (top3) ContactSim. Rot.Sim. Loc.Sim. researchwassupportedinpartbytheIsraelScienceFoundation
Sameasfollower 2.572 0.434 0.814 0.994 0.975 (grantsno.2492/20and3441/21),LenBlavatnikandtheBlavatnik
None 3.182 0.410 0.817 0.986 0.948 familyfoundation,andtheTelAvivUniversityInnovationLabora-
‚ÄúAperson‚Äù 3.282 0.391 0.824 0.986 0.947
tories(TILabs).MonkeySee,MonkeyDo ‚Ä¢ 9
REFERENCES
LiHu,XinGao,PengZhang,KeSun,BangZhang,andLiefengBo.2023. Animate
KfirAberman,YijiaWeng,DaniLischinski,DanielCohen-Or,andBaoquanChen.2020. Anyone:ConsistentandControllableImage-to-VideoSynthesisforCharacterAni-
Unpairedmotionstyletransferfromvideotoanimation. ACMTransactionson mation.
Graphics(TOG)39,4(2020),64‚Äì1. InbarHuberman-Spiegelglas,VladimirKulikov,andTomerMichaeli.2023. AnEdit
Yuval Alaluf, Daniel Garibi, Or Patashnik, Hadar Averbuch-Elor, and Daniel FriendlyDDPMNoiseSpace:InversionandManipulations.
Cohen-Or. 2023. Cross-Image Attention for Zero-Shot Appearance Transfer. Deok-KyeongJang,SoominPark,andSung-HeeLee.2022.Motionpuzzle:Arbitrary
arXiv:2311.03335[cs.CV] motionstyletransferbybodypart.ACMTransactionsonGraphics(TOG)41,3(2022),
OkanArikanandDavidAForsyth.2002.Interactivemotiongenerationfromexamples. 1‚Äì16.
ACMTransactionsonGraphics(TOG)21,3(2002),483‚Äì490. BiaoJiang,XinChen,WenLiu,JingyiYu,GangYu,andTaoChen.2024.MotionGPT:
NikosAthanasiou,MathisPetrovich,MichaelJBlack,andG√ºlVarol.2023. SINC: HumanMotionasaForeignLanguage.AdvancesinNeuralInformationProcessing
Spatialcompositionof3Dhumanmotionsforsimultaneousactiongeneration. Systems36(2024).
InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.IEEE Roy Kapon, Guy Tevet, Daniel Cohen-Or, and Amit H. Bermano. 2023. MAS:
ComputerSociety,Washington,DC,USA,9984‚Äì9995. Multi-viewAncestralSamplingfor3Dmotiongenerationusing2Ddiffusion.
RishiBommasani,DrewAHudson,EhsanAdeli,RussAltman,SimranArora,Sydney arXiv:2310.14729[cs.CV]
vonArx,MichaelSBernstein,JeannetteBohg,AntoineBosselut,EmmaBrunskill, TeroKarras,SamuliLaine,andTimoAila.2019.Astyle-basedgeneratorarchitecture
etal.2021.Ontheopportunitiesandrisksoffoundationmodels. forgenerativeadversarialnetworks.InProceedingsoftheIEEE/CVFconferenceon
MingdengCao,XintaoWang,ZhongangQi,YingShan,XiaohuQie,andYinqiang computervisionandpatternrecognition(CVPR).IEEEComputerSociety,Washington,
Zheng.2023.MasaCtrl:Tuning-FreeMutualSelf-AttentionControlforConsistent DC,USA,4401‚Äì4410.
ImageSynthesisandEditing. KorraweKarunratanakul,KonpatPreechakul,SupasornSuwajanakorn,andSiyuTang.
HilaChefer,YuvalAlaluf,YaelVinker,LiorWolf,andDanielCohen-Or.2023.Attend- 2023. GuidedMotionDiffusionforControllableHumanMotionSynthesis.In
and-excite:Attention-basedsemanticguidancefortext-to-imagediffusionmodels. ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.IEEE
ACMTransactionsonGraphics(TOG)42,4(2023),1‚Äì10. ComputerSociety,Washington,DC,USA,2151‚Äì2162.
SetarehCohan,GuyTevet,DanieleReda,XueBinPeng,andMichielvandePanne. BoeunKim,JunghoKim,HyungJinChang,andJinYoungChoi.2024.MoST:Motion
2024.FlexibleMotionIn-betweeningwithDiffusionModels. StyleTransformerbetweenDiverseActionContents.
Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, and Christian LucasKovar,MichaelGleicher,andFr√©d√©ricPighin.2002.Motiongraphs.InProceed-
Theobalt.2023.MoFusion:AFrameworkforDenoising-Diffusion-basedMotionSyn- ingsofthe29thannualconferenceonComputergraphicsandinteractivetechniques
thesis.InComputerVisionandPatternRecognition(CVPR).IEEEComputerSociety, (SIGGRAPH2002).ACM,NewYork,NY,USA,473‚Äì‚Äì482.
Washington,DC,USA. JeheeLee,JinxiangChai,PaulSAReitsma,JessicaKHodgins,andNancySPollard.2002.
OmerDahary,OrPatashnik,KfirAberman,andDanielCohen-Or.2024.BeYourself: Interactivecontrolofavatarsanimatedwithhumanmotiondata.InProceedingsof
BoundedAttentionforMulti-SubjectText-to-ImageGeneration. the29thannualconferenceonComputergraphicsandinteractivetechniques.ACM
PrafullaDhariwalandAlexanderNichol.2021.Diffusionmodelsbeatgansonimage NewYork,NY,USA,NewYork,NY,USA,491‚Äì500.
synthesis.AdvancesinNeuralInformationProcessingSystems34(2021),8780‚Äì8794. PeizhuoLi,KfirAberman,ZihanZhang,RanaHanocka,andOlgaSorkine-Hornung.
RinonGal,YuvalAlaluf,YuvalAtzmon,OrPatashnik,AmitHaimBermano,GalChechik, 2022.GANimator:NeuralMotionSynthesisfromaSingleSequence.ACMTransac-
andDanielCohen-or.2023.AnImageisWorthOneWord:PersonalizingText-to- tionsonGraphics(TOG)41,4(2022),138.
ImageGenerationusingTextualInversion.InTheEleventhInternationalConference HanLiang,WenqianZhang,WenxuanLi,JingyiYu,andLanXu.2024. Intergen:
onLearningRepresentations.OpenReview.net,OpenReview.net. https://openreview. Diffusion-basedmulti-humanmotiongenerationundercomplexinteractions.In
net/forum?id=NAQvF08TcyG InternationalJournalofComputerVision.Springer,Berlin/Heidelberg,Germany,
DanielGaribi,OrPatashnik,AndreyVoynov,HadarAverbuch-Elor,andDanielCohen- 1‚Äì21.
Or.2024.ReNoise:RealImageInversionThroughIterativeNoising. ShitongLuoandWeiHu.2021. Diffusionprobabilisticmodelsfor3dpointcloud
LeonA.Gatys,AlexanderS.Ecker,andMatthiasBethge.2015.ANeuralAlgorithmof generation.InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
ArtisticStyle. arXiv:1508.06576[cs.CV] PatternRecognition.IEEEComputerSociety,Washington,DC,USA,2837‚Äì2845.
MichalGeyer,OmerBar-Tal,ShaiBagon,andTaliDekel.2024. TokenFlow:Con- RonMokady,AmirHertz,KfirAberman,YaelPritch,andDanielCohen-Or.2023.Null-
sistentDiffusionFeaturesforConsistentVideoEditing.InTheTwelfthInterna- textinversionforeditingrealimagesusingguideddiffusionmodels.InProceedingsof
tionalConferenceonLearningRepresentations.OpenReview.net,OpenReview.net. theIEEE/CVFConferenceonComputerVisionandPatternRecognition.IEEEComputer
https://openreview.net/forum?id=daEqXJ0yZo Society,Washington,DC,USA,6038‚Äì6047.
PurviGoel,Kuan-ChiehWang,CKarenLiu,andKayvonFatahalian.2023.Iterative YanivNikankin,NivHaim,andMichalIrani.2023. SinFusion:TrainingDiffusion
MotionEditingwithNaturalLanguage. ModelsonaSingleImageorVideo.InInternationalConferenceonMachineLearning.
ChuanGuo,YuxuanMu,XinxinZuo,PengDai,YouliangYan,JuweiLu,andLiCheng. PMLR,PMLR,PMLR,26199‚Äì26214.
2024.GenerativeHumanMotionStylizationinLatentSpace.InTheTwelfthInterna- OrPatashnik,RinonGal,DanielCohen-Or,Jun-YanZhu,andFernandoDelaTorre.
tionalConferenceonLearningRepresentations.OpenReview.net,OpenReview.net. 2024.ConsolidatingAttentionFeaturesforMulti-viewImageEditing.
https://openreview.net/forum?id=daEqXJ0yZo OrPatashnik,DanielGaribi,IdanAzuri,HadarAverbuch-Elor,andDanielCohen-Or.
ChuanGuo,ShihaoZou,XinxinZuo,SenWang,WeiJi,XingyuLi,andLiCheng.2022. 2023.LocalizingObject-LevelShapeVariationswithText-to-ImageDiffusionModels.
GeneratingDiverseandNatural3DHumanMotionsFromText.InProceedingsofthe InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision(ICCV).
IEEE/CVFConferenceonComputerVisionandPatternRecognition.IEEEComputer SpringerInternationalPublishing,Berlin/Heidelberg,Germany,23051‚Äì23061.
Society,Washington,DC,USA,5152‚Äì5161. MathisPetrovich,MichaelJ.Black,andG√ºlVarol.2021.Action-Conditioned3DHuman
AmirHertz,RonMokady,JayTenenbaum,KfirAberman,YaelPritch,andDaniel MotionSynthesiswithTransformerVAE.InInternationalConferenceonComputer
Cohen-Or.2023.Prompt-to-promptimageeditingwithcrossattentioncontrol.In Vision(ICCV).IEEEComputerSociety,Washington,DC,USA,10985‚Äì10995.
TheEleventhInternationalConferenceonLearningRepresentations(ICLR).OpenRe- MathisPetrovich,MichaelJ.Black,andG√ºlVarol.2022.TEMOS:Generatingdiverse
view.net,OpenReview.net. humanmotionsfromtextualdescriptions.InEuropeanConferenceonComputer
MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSepp Vision(ECCV).SpringerInternationalPublishing,Berlin/Heidelberg,Germany.
Hochreiter.2017.Ganstrainedbyatwotime-scaleupdateruleconvergetoalocal MathisPetrovich,OrLitany,UmarIqbal,MichaelJ.Black,G√ºlVarol,XueBinPeng,
nashequilibrium.Advancesinneuralinformationprocessingsystems30(2017). andDavisRempe.2024.STMC:Multi-TrackTimelineControlforText-Driven3D
JonathanHo,AjayJain,andPieterAbbeel.2020. Denoisingdiffusionprobabilistic HumanMotionGeneration.
models.AdvancesinNeuralInformationProcessingSystems33(2020),6840‚Äì6851. SigalRaab,InbalLeibovitch,PeizhuoLi,KfirAberman,OlgaSorkine-Hornung,and
JonathanHo,TimSalimans,AlexeyGritsenko,WilliamChan,MohammadNorouzi, DanielCohen-Or.2023.MoDi:UnconditionalMotionSynthesisfromDiverseData.
andDavidJFleet.2022.Videodiffusionmodels. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition
DanielHolden,IkhsanulHabibie,IkuoKusajima,andTakuKomura.2017a.Fastneural (CVPR).IEEEComputerSociety,Washington,DC,USA,13873‚Äì13883.
styletransferformotiondata.IEEEcomputergraphicsandapplications37,4(2017), SigalRaab,InbalLeibovitch,GuyTevet,MoabArar,AmitHBermano,andDaniel
42‚Äì49. Cohen-Or.2024.SingleMotionDiffusion.InTheTwelfthInternationalConference
DanielHolden,TakuKomura,andJunSaito.2017b.Phase-functionedneuralnetworks onLearningRepresentations(ICLR).OpenReview.net,OpenReview.net.
forcharactercontrol.ACMTransactionsonGraphics(TOG)36,4(2017),1‚Äì13. OlafRonneberger,PhilippFischer,andThomasBrox.2015. U-net:Convolutional
DanielHolden,JunSaito,andTakuKomura.2016. Adeeplearningframeworkfor networksforbiomedicalimagesegmentation.InInternationalConferenceonMedical
charactermotionsynthesisandediting.ACMTransactionsonGraphics(TOG)35,4 imagecomputingandcomputer-assistedintervention.Springer,SpringerInternational
(2016),1‚Äì11. Publishing,Berlin/Heidelberg,Germany,234‚Äì241.
ChitwanSaharia,WilliamChan,HuiwenChang,ChrisLee,JonathanHo,TimSalimans,
DavidFleet,andMohammadNorouzi.2022a. Palette:Image-to-imagediffusion10 ‚Ä¢ Raab,S.etal.
models.InACMSIGGRAPH2022ConferenceProceedings.ACM,NewYork,NY,USA, APPENDIX
1‚Äì10.
ThisAppendixaddsdetailsontopoftheonesgiveninthemain
ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyDen-
ton,SeyedKamyarSeyedGhasemipour,RaphaelGontijo-Lopes,BurcuKaragol paper.Whilethemainpaperstandsonitsown,thedetailsgiven
Ayan,TimSalimans,JonathanHo,DavidJ.Fleet,andMohammadNorouzi.2022b. heremayshedmorelight.
PhotorealisticText-to-ImageDiffusionModelswithDeepLanguageUnderstand-
ing. AdvancesinNeuralInformationProcessingSystems35(2022),36479‚Äì36494. InappendixAweprovidemoredetailsregardingtheprelimi-
https://openreview.net/forum?id=08Yk-n5l2Al nariesofourwork:motionrepresentation,andthemodelsMDM,
YoniShafir,GuyTevet,RoyKapon,andAmitHaimBermano.2024.HumanMotion
DDPM,andDDIM.AppendixBelaboratesonourexperiments,in
DiffusionasaGenerativePrior.InTheTwelfthInternationalConferenceonLearning
Representations.OpenReview.net,OpenReview.net. particularthebenchmark,metrics,hypermparameters,andaddi-
TamarRottShaham,TaliDekel,andTomerMichaeli.2019.SinGAN:Learningagenera- tionalimplementationdetails.
tivemodelfromasinglenaturalimage.InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision.IEEEComputerSociety,Washington,DC,USA,
4570‚Äì4580. A PRELIMINARIES-MOREDETAILS
JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli.2015.
Deepunsupervisedlearningusingnonequilibriumthermodynamics.InInternational Motion Representation. Recall that ùëÅ denotes the number of
ConferenceonMachineLearning.PMLR,PMLR,PMLR,2256‚Äì2265. framesinamotionsequence,ùêπ denotesthelengthofthefeatures
JiamingSong,ChenlinMeng,andStefanoErmon.2020a.DenoisingDiffusionImplicit
Models.InInternationalConferenceonLearningRepresentations.OpenReview.net, describingasingleframe,ùêΩ denotesthenumberofskeletaljoints,
OpenReview.net. and ùëã ‚àà RùëÅ√óùêπ denotes a motion. Each feature is redundantly
JiamingSong,ChenlinMeng,andStefanoErmon.2021.DenoisingDiffusionImplicit
represented with the joint angles, positions, velocities, and foot
Models.InInternationalConferenceonLearningRepresentations.OpenReview.net,
OpenReview.net. https://openreview.net/forum?id=St1giarCHLP contact[Guoetal.2022].Eachsingleposeisdefinedby
YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoEr-
mon,andBenPoole.2020b.Score-BasedGenerativeModelingthroughStochastic (ùëü(cid:164)ùëé,ùëü(cid:164)ùë•,ùëü(cid:164)ùëß,ùëüùë¶,ùëóùëù,ùëóùëü,ùëóùë£,ùëêùëì
)
‚ààRùêπ,
DifferentialEquations.InInternationalConferenceonLearningRepresentations.Open-
Review.net,OpenReview.net.
SebastianStarke,IanMason,andTakuKomura.2022.Deepphase:Periodicautoencoders whereùëü(cid:164)ùëé ‚ààRistherootangularvelocityalongtheY-axis.ùëü(cid:164)ùë•,ùëü(cid:164)ùëß ‚ààR
forlearningmotionphasemanifolds.ACMTransactionsonGraphics(TOG)41,4
(2022),1‚Äì13. arerootlinearvelocitiesontheXZ-plane,andùëüùë¶ ‚àà Ristheroot
GuyTevet,BrianGordon,AmirHertz,AmitHBermano,andDanielCohen-Or.2022. height.ùëóùëù ‚ààR3(ùêΩ‚àí1),ùëóùëü ‚ààR6(ùêΩ‚àí1) andùëóùë£ ‚ààR3ùêΩ arethelocaljoint
M ECo Cti Von 20c 2li 2p :: 1E 7x thp Eo usi rn og peh au nm Ca on nfm ero enti co en ,Tg ee ln Ae vr ia vt ,i Io srn aeto l,Ocl cip tobsp erac 2e 3. ‚ÄìI 2n 7,C 2o 0m 22p ,u Pt re or ceV ei ds ii non gs‚Äì
,
positions,velocities,androtationsrelativetotheroot,andùëêùëì ‚ààR4
PartXXII.Springer,SpringerInternationalPublishing,Berlin/Heidelberg,Germany, arebinaryfeaturesdenotingthefootcontactlabelsforfourfoot
358‚Äì374.
joints(twoforeachleg).
GuyTevet,SigalRaab,BrianGordon,YoniShafir,DanielCohen-or,andAmitHaim
Bermano.2023. HumanMotionDiffusionModel.InTheEleventhInternational
MDM,DDPMandDDIM. RecallthatMDM[Tevetetal.2023]does
ConferenceonLearningRepresentations(ICLR).OpenReview.net,OpenReview.net.
https://openreview.net/forum?id=SJ1kSyO2jwu humanmotionsynthesisandeditingandthatitusesDDPMs[Ho
JonathanTseng,RodrigoCastellon,andKarenLiu.2023.Edge:Editabledancegeneration etal.2020].Inthefollowing,webrieflydescribethemechanismof
frommusic.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
DDPM.
Recognition(CVPR).IEEEComputerSociety,Washington,DC,USA,448‚Äì458.
NarekTumanyan,MichalGeyer,ShaiBagon,andTaliDekel.2023. Plug-and-Play Aninputmotionùë• 0,issubjectedtoaMarkovnoiseprocesscon-
D thi eff Iu Es Eio En /CF Ve Fat Cu or ne fs ef ro enr cT ee ox nt- CD or miv pe un tI em rVag ise i- ot no- aI nm da Pg ae ttT er ra nn Rsl ea ct oi go nn i. tiI on nP (r Co Vce Pe Rd )i .n Ig Es Eo Ef sistingofTsteps,resultinginthesequence{ùë• ùë°}ùëá ùë°=0,suchthat
ComputerSociety,Washington,DC,USA,1921‚Äì1930. ‚àö
AaronVanDenOord,OriolVinyals,etal.2017.Neuraldiscreterepresentationlearning. ùëû(ùë• ùë°|ùë• ùë°‚àí1)=N( ùõº ùë°ùë• ùë°‚àí1,(1‚àíùõº ùë°)ùêº), (5)
Advancesinneuralinformationprocessingsystems30(2017).
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN
Gomez,≈ÅukaszKaiser,andIlliaPolosukhin.2017.Attentionisallyouneed.Advances whereùõº ùë° ‚àà (0,1)areconstanthyper-parameters.Whenùõº ùë° issmall
inneuralinformationprocessingsystems30(2017). enough,wecanapproximateùë•
ùëá
‚àºN(0,ùêº).
JayZhangjieWu,YixiaoGe,XintaoWang,StanWeixianLei,YuchaoGu,YufeiShi,
WynneHsu,YingShan,XiaohuQie,andMikeZhengShou.2023.Tune-a-video:One- ùë• 0canbemodeledviathereverseddiffusionprocessbygradually
shottuningofimagediffusionmodelsfortext-to-videogeneration.InProceedingsof cleaning ùë• ùëá, using a generative network ùëù ùúÉ. MDM [Tevet et al.
Wthe asI hE iE nE g/ tC oV nF ,DIn Cte ,r Un Sa Atio ,n 7a 6l 23C ‚Äìo 7n 6fe 3r 3e .nceonComputerVision.IEEEComputerSociety, 2023]predictstheinputmotion,denotedùë•ÀÜ0,ratherthanùúñ ùë°,such
YimingXie,VarunJampani,LeiZhong,DeqingSun,andHuaizuJiang.2023.Omni-
thatùë•ÀÜ0=ùëù ùúÉ(ùë• ùë°,ùë°).Then,thewidespreaddiffusionlossisapplied:
Control:ControlAnyJointatAnyTimeforHumanMotionGeneration.InThe
TwelfthInternationalConferenceonLearningRepresentations.OpenReview.net,Open-
JianR re ov ni gew Zh.n ae nt g.
,YangsongZhang,XiaodongCun,ShaoliHuang,YongZhang,Hongwei
L simple=E ùë°‚àº[1,ùëá]‚à•ùë• 0‚àíùëù ùúÉ(ùë• ùë°,ùë°)‚à•2 2. (6)
Zhao,HongtaoLu,andXiShen.2023b.T2M-GPT:GeneratingHumanMotionfrom
TextualDescriptionswithDiscreteRepresentations.InProceedingsoftheIEEE/CVF Duringinference,synthesisiteratesfrompurenoiseùë• ùëá.Ineach
C Soo cn if ee tr ye ,n Wce aso hn inC go tm onp ,u Dte Cr ,V Ui Ssi Aon . andPatternRecognition(CVPR).IEEEComputer iteration,thedenoisingnetworkùëù ùúÉ predictsacleanversionofthe
MingyuanZhang,HuirongLi,ZhongangCai,JiaweiRen,LeiYang,andZiweiLiu.2024.
currentsampleùë• ùë°.Thepredictedcleansampleùë•ÀÜ0isthen‚Äúre-noised"
Finemogen:Fine-grainedspatio-temporalmotiongenerationandediting.Advances tocreatethenextsampleùë• ùë°‚àí1,repeatedlyuntilùë° =0.
inNeuralInformationProcessingSystems36(2024).
DenoisingDiffusionImplicitModels(DDIM)[Songetal.2021]
QinshengZhang,JiamingSong,XunHuang,YongxinChen,andMingyuLiu.2023a.
DiffCollage:ParallelGenerationofLargeContentwithDiffusionModels.In2023 enableanonMarkovian,deterministic,versionofDDPMs.DDIM
IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR).IEEE,IEEE canbeappliedonanetworkpre-trainedwithDDPM,foreither
ComputerSociety,Washington,DC,USA,10188‚Äì10198.
acceleratedinference,orinversion.Inthiswork,weemployDDIM
forinversionanddeterministicinference,toreconstructinverted
motionspreciselytotheiroriginalform.MonkeySee,MonkeyDo ‚Ä¢ 11
B EXPERIMENTS-MOREDETAILS
countedasmisses.Themetricscoreisdeterminedbytherateof
B.1 Benchmark hits.
SimilaritytoFollowerRotations. Thismetricevaluatesthefi-
ThecompletelistoffiltertermsusedtocreatetheMTBbenchmark
delityoftheoutputmotiontothesubtlemotifsfoundinthefol-
isasfollows.Foreachpairinthebenchmark,theleadercontains
lower.Specifically,itassessestheresemblanceofrotationswithin
oneofthefollowinglocomotionkeywords:‚Äúrun‚Äù,‚Äúwalk‚Äù,‚Äújump‚Äù,
theoutputmotiontoeithertheleaderorfollowermotions.Given
‚Äúdanc‚Äù.Thefollowercontainsoneoftheaforementionedlocomotion
thatsubtlenuancesaretypicallyexpressedinmostjoints‚Äôrotations,
keywords,plusonestyleoractionkeyword,fromthelist‚Äúgorilla‚Äù,
weanticipateobservingahigherdegreeofsimilaritytothefollower
‚Äúdrunk‚Äù,‚Äúrobot‚Äù,‚Äúchicken‚Äù,‚Äúfrog‚Äù,‚Äúmonkey‚Äù,‚Äústyle‚Äù,‚Äúlike‚Äù,‚Äúold‚Äù,
motion.
‚Äúchild‚Äù,‚Äúraise‚Äù,‚Äúclap‚Äù,‚Äúwav‚Äù,‚Äúkick‚Äù,‚Äúpunch‚Äù,‚Äúpush‚Äù,‚Äúpull‚Äù.
Tocomputethismetric,theinitialstepistoidentify,foreach
FollowingareseveralexamplesofpairsfromtheMTBbenchmark.
frame,thenearestneighborinboththeleaderandfollowermotions.
Eachexampleisidentifiedbyitstextpromptanditsindexnumber
Thisentailsidentifyingtheclosestmatchforeachframe.
intheHumanML3Ddataset.
Oncethenearestneighborsareidentified,themetriccalculates
‚Ä¢ Leader:‚Äúapersonwalkstowardstheleftmakingawide‚Äôs‚Äô therateofframeswherethesimilaritytothefollower‚Äôsnearest
shape‚Äù(#009488).Follower:‚Äúapersonwalksinaclockwise neighbor surpasses that of the leader‚Äôs. Essentially, it evaluates
circleandraisestheirhandtotheirfacetoyawn‚Äù(#004222). theproportionofrotationframeswheretheoutboundmotionex-
‚Ä¢ Leader:‚Äúapersonrunsandthenjumps‚Äù(#007291).Follower: hibitsgreateralignmentwiththefollower‚Äôsmotioncomparedto
‚Äúthedrunkguystrugglestowalkdownthestreet‚Äù(#005037). thetrackedone.
‚Ä¢ Leader:‚Äúitisapersonwalkingbackwards‚Äù(#007199).Fol- SimilaritytoFollowerLocations. Thismetricalsoevaluatesthe
lower:‚Äúpersoniswalkingwithhisarmsoutlikeheisbal- fidelityoftheoutputmotiontofollower‚Äôsmotifs.Similartothe
ancing‚Äù(#010823). previousmetric,itcalculatesthenearestneighborsforeachframe,
but this time for the locations relative to the root. As with the
B.2 Metrics
previous metric, we expect a greater degree of similarity to the
FID[Guoetal.2022;Heuseletal.2017]. Weusethismetricto followermotion.
assess the quality of the output motions. The Fr√©chet Inception
B.3 ImplementationDetails
Distance (FID) evaluates the similarity between the distribution
ofrealmotionsandsynthesizedonesWeconsiderthebenchmark
datasetasthegroundtruthdistributionandextractfeaturesfrom
Table4. Hyperparametersusedforourbackbone.Ourbackboneisa
variationofMDM,withthehyperparameterslistedhere.
bothrealmotionsinthebenchmarkandgeneratedoutputmotions.
Motionsaredeemedofhighqualityiftheyexistwithinthemanifold Name Value
ofthegroundtruth. Model
R-precision[Guoetal.2022]. Weusethismetrictoassessthe Architecture TransformerDecoder
alignmentbetweeneachoutputmotionandthemotionmotifsof layers 12
theirassociatedfollowermotions.R-precisionusesalatentspace latentdim 512
sharedbetweentextandmotiontomeasurethedistancebetween Diffusion
motionandtextembeddings.Hence,itmeasureswhetheramotion diffusionsteps 100
reflectssometextprompt.Thetextpromptofeachoutputmotion noiseschedule cosine
iscopiedfromthefollowerusedwhengeneratingit,soasuccessful guidancescale 2.5
matchmeanstheoutputmotionadherestothemotionpatternof Training
thefollower. batchsize 32
Foreachgeneratedmotion,wecreateadescriptionpoolconsist- lr 0.0001
ingofitsground-truthtextdescriptionand31randomlychosen, dropout 0.1
unrelateddescriptionsfromthetestset.Wethencalculateandrank numsteps 600000
theEuclideandistancesbetweenthemotionfeatureandthetext warmupsteps 0
featureofeachdescriptioninthepool.Wemeasuresuccessbythe weightdecay 0
averageaccuracyatthetop-1,top-2,andtop-3positions.Asuccess- seed 10
fulretrievaloccurswhenthegroundtruthentryranksinthetop-k
candidates.
Backboneandhyperparameters. Thebackboneweuseforre-
FootContactSimilarity. Thismetricassesseshowwelltheoutput portingexperimentalresultsisthetransformerdecodervariation
motionretainsthelocomotionrhythmoftheleadermotion.Suc- ofMDM[Tevetetal.2023].WeretrainitontheHumanML3D[Guo
cessfulmotiontransfershouldpreservetherhythmfromleaderto etal.2022]dataset,withthehyperparametersshowninTab.4.In
outputmotion,sothefootcontactlabelsofthetwomotionsshould particular,weslightlychangethearchitecturesuchthatthetext
closelyalign.Wemeasurethissimilarityscorebycomparingthe embeddingisgiventwice:onceasanextra-temporaltokenlike
footcontactdatawithintheHumanML3Dmotionfeatures.Match- inthetransformerencodervariation,andonceasseparateword
ingfootcontactlabelsarecountedashitswhiledifferinglabelsare tokensusingcross-attention.12 ‚Ä¢ Raab,S.etal.
Table 5. Hyperparameters used during inference when applying butwouldproceedinadifferentdirection.Fortunately,thesolu-
MoMo.
tionisstraightforward.Forgeneratedmotions,wecreatemultiple
Name Value followers,eachgeneratedwithadifferentseed,forthegiventext
Applyingmixed-attention prompt.MoMoisthenappliedtotheconcatenationofallfollowers
together.Ourexperimentsshowthatgeneratingjustafewfollowers
Layers range(1,12)
ensuresthattheoutputmotioncloselyfollowsthedirectionofthe
Diffusionsteps range(10,91)
follower.Forinvertedmotions,usingdifferentseedswouldnothelp
Textprompt
astheinversionbyDDIMisdeterministic.Oursolutionistorotate
Outputmotion Sameasfollower
thegivenmotionaroundtheverticalaxisinseveralrotationan-
gles.Similartothegenerativecase,weapplyourframeworktothe
Additionally,Tab.5provideshyperparametersusedduringinfer- concatenationofallrotatedfollowers.Consequently,eachleader‚Äôs
encewhenapplyingMoMo. frameattendstotheframesofallthefollowersandachievesthe
highestattentionscoreswiththosethathavedirectionssimilarto
Controllingdirectionsduringmotiontransfer. Anaturalchal-
itsown.Thisfacilitatesnear-accuratedirectiontransfer.Inpractice,
lengeariseswhenthelocomotiondirectionoftheleadermotion
wehavefoundthatthissolutionisnearlyequivalenttocopying
differsfromthatofthefollower‚Äôs.Insuchcases,theoutputmotion
therootrotationvaluefromtheleadertotheoutputmotion.For
wouldretaintheoutlineoftheleader,suchasthetimingofsteps,
evaluation,weusedthelatter,toacceleratecomputationtime.