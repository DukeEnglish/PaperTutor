[
    {
        "title": "NarrativeBridge: Enhancing Video Captioning with Causal-Temporal Narrative",
        "authors": "Asmar NadeemFaegheh SardariRobert DawesSyed Sameed HusainAdrian HiltonArmin Mustafa",
        "links": "http://arxiv.org/abs/2406.06499v1",
        "entry_id": "http://arxiv.org/abs/2406.06499v1",
        "pdf_url": "http://arxiv.org/pdf/2406.06499v1",
        "summary": "Existing video captioning benchmarks and models lack coherent representations\nof causal-temporal narrative, which is sequences of events linked through cause\nand effect, unfolding over time and driven by characters or agents. This lack\nof narrative restricts models' ability to generate text descriptions that\ncapture the causal and temporal dynamics inherent in video content. To address\nthis gap, we propose NarrativeBridge, an approach comprising of: (1) a novel\nCausal-Temporal Narrative (CTN) captions benchmark generated using a large\nlanguage model and few-shot prompting, explicitly encoding cause-effect\ntemporal relationships in video descriptions, evaluated automatically to ensure\ncaption quality and relevance; and (2) a dedicated Cause-Effect Network (CEN)\narchitecture with separate encoders for capturing cause and effect dynamics\nindependently, enabling effective learning and generation of captions with\ncausal-temporal narrative. Extensive experiments demonstrate that CEN is more\naccurate in articulating the causal and temporal aspects of video content than\nthe second best model (GIT): 17.88 and 17.44 CIDEr on the MSVD and MSR-VTT\ndatasets, respectively. The proposed framework understands and generates\nnuanced text descriptions with intricate causal-temporal narrative structures\npresent in videos, addressing a critical limitation in video captioning. For\nproject details, visit https://narrativebridge.github.io/.",
        "updated": "2024-06-10 17:34:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.06499v1"
    },
    {
        "title": "Demonstrating HumanTHOR: A Simulation Platform and Benchmark for Human-Robot Collaboration in a Shared Workspace",
        "authors": "Chenxu WangBoyuan DuJiaxin XuPeiyan LiDi GuoHuaping Liu",
        "links": "http://arxiv.org/abs/2406.06498v1",
        "entry_id": "http://arxiv.org/abs/2406.06498v1",
        "pdf_url": "http://arxiv.org/pdf/2406.06498v1",
        "summary": "Human-robot collaboration (HRC) in a shared workspace has become a common\npattern in real-world robot applications and has garnered significant research\ninterest. However, most existing studies for human-in-the-loop (HITL)\ncollaboration with robots in a shared workspace evaluate in either simplified\ngame environments or physical platforms, falling short in limited realistic\nsignificance or limited scalability. To support future studies, we build an\nembodied framework named HumanTHOR, which enables humans to act in the\nsimulation environment through VR devices to support HITL collaborations in a\nshared workspace. To validate our system, we build a benchmark of everyday\ntasks and conduct a preliminary user study with two baseline algorithms. The\nresults show that the robot can effectively assist humans in collaboration,\ndemonstrating the significance of HRC. The comparison among different levels of\nbaselines affirms that our system can adequately evaluate robot capabilities\nand serve as a benchmark for different robot algorithms. The experimental\nresults also indicate that there is still much room in the area and our system\ncan provide a preliminary foundation for future HRC research in a shared\nworkspace. More information about the simulation environment, experiment\nvideos, benchmark descriptions, and additional supplementary materials can be\nfound on the website: https://sites.google.com/view/humanthor/.",
        "updated": "2024-06-10 17:33:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.06498v1"
    },
    {
        "title": "Insights from Social Shaping Theory: The Appropriation of Large Language Models in an Undergraduate Programming Course",
        "authors": "Aadarsh PadiyathXinying HouAmy PangDiego Viramontes VargasXingjian GuTamara Nelson-FrommZihan WuMark GuzdialBarbara Ericson",
        "links": "http://dx.doi.org/10.1145/3632620.3671098",
        "entry_id": "http://arxiv.org/abs/2406.06451v1",
        "pdf_url": "http://arxiv.org/pdf/2406.06451v1",
        "summary": "The capability of large language models (LLMs) to generate, debug, and\nexplain code has sparked the interest of researchers and educators in\nundergraduate programming, with many anticipating their transformative\npotential in programming education. However, decisions about why and how to use\nLLMs in programming education may involve more than just the assessment of an\nLLM's technical capabilities. Using the social shaping of technology theory as\na guiding framework, our study explores how students' social perceptions\ninfluence their own LLM usage. We then examine the correlation of self-reported\nLLM usage with students' self-efficacy and midterm performances in an\nundergraduate programming course. Triangulating data from an anonymous\nend-of-course student survey (n = 158), a mid-course self-efficacy survey\n(n=158), student interviews (n = 10), self-reported LLM usage on homework, and\nmidterm performances, we discovered that students' use of LLMs was associated\nwith their expectations for their future careers and their perceptions of peer\nusage. Additionally, early self-reported LLM usage in our context correlated\nwith lower self-efficacy and lower midterm scores, while students' perceived\nover-reliance on LLMs, rather than their usage itself, correlated with\ndecreased self-efficacy later in the course.",
        "updated": "2024-06-10 16:40:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.06451v1"
    },
    {
        "title": "How is the Pilot Doing: VTOL Pilot Workload Estimation by Multimodal Machine Learning on Psycho-physiological Signals",
        "authors": "Jong Hoon ParkLawrence ChenIan HigginsZhaobo ZhengShashank MehrotraKevin SalubreMohammadreza MousaeiSteven WillitsBlain LevedahlTimothy BukerEliot XingTeruhisa MisuSebastian SchererJean Oh",
        "links": "http://arxiv.org/abs/2406.06448v1",
        "entry_id": "http://arxiv.org/abs/2406.06448v1",
        "pdf_url": "http://arxiv.org/pdf/2406.06448v1",
        "summary": "Vertical take-off and landing (VTOL) aircraft do not require a prolonged\nrunway, thus allowing them to land almost anywhere. In recent years, their\nflexibility has made them popular in development, research, and operation. When\ncompared to traditional fixed-wing aircraft and rotorcraft, VTOLs bring unique\nchallenges as they combine many maneuvers from both types of aircraft. Pilot\nworkload is a critical factor for safe and efficient operation of VTOLs. In\nthis work, we conduct a user study to collect multimodal data from 28 pilots\nwhile they perform a variety of VTOL flight tasks. We analyze and interpolate\nbehavioral patterns related to their performance and perceived workload.\nFinally, we build machine learning models to estimate their workload from the\ncollected data. Our results are promising, suggesting that quantitative and\naccurate VTOL pilot workload monitoring is viable. Such assistive tools would\nhelp the research field understand VTOL operations and serve as a stepping\nstone for the industry to ensure VTOL safe operations and further remote\noperations.",
        "updated": "2024-06-10 16:39:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.06448v1"
    },
    {
        "title": "Multimodal Contextualized Semantic Parsing from Speech",
        "authors": "Jordan VoasRaymond MooneyDavid Harwath",
        "links": "http://arxiv.org/abs/2406.06438v1",
        "entry_id": "http://arxiv.org/abs/2406.06438v1",
        "pdf_url": "http://arxiv.org/pdf/2406.06438v1",
        "summary": "We introduce Semantic Parsing in Contextual Environments (SPICE), a task\ndesigned to enhance artificial agents' contextual awareness by integrating\nmultimodal inputs with prior contexts. SPICE goes beyond traditional semantic\nparsing by offering a structured, interpretable framework for dynamically\nupdating an agent's knowledge with new information, mirroring the complexity of\nhuman communication. We develop the VG-SPICE dataset, crafted to challenge\nagents with visual scene graph construction from spoken conversational\nexchanges, highlighting speech and visual data integration. We also present the\nAudio-Vision Dialogue Scene Parser (AViD-SP) developed for use on VG-SPICE.\nThese innovations aim to improve multimodal information processing and\nintegration. Both the VG-SPICE dataset and the AViD-SP model are publicly\navailable.",
        "updated": "2024-06-10 16:31:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.06438v1"
    }
]