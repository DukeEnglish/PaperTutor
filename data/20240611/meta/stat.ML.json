[
    {
        "title": "Distribution-Free Predictive Inference under Unknown Temporal Drift",
        "authors": "Elise HanChengpiao HuangKaizheng Wang",
        "links": "http://arxiv.org/abs/2406.06516v1",
        "entry_id": "http://arxiv.org/abs/2406.06516v1",
        "pdf_url": "http://arxiv.org/pdf/2406.06516v1",
        "summary": "Distribution-free prediction sets play a pivotal role in uncertainty\nquantification for complex statistical models. Their validity hinges on\nreliable calibration data, which may not be readily available as real-world\nenvironments often undergo unknown changes over time. In this paper, we propose\na strategy for choosing an adaptive window and use the data therein to\nconstruct prediction sets. The window is selected by optimizing an estimated\nbias-variance tradeoff. We provide sharp coverage guarantees for our method,\nshowing its adaptivity to the underlying temporal drift. We also illustrate its\nefficacy through numerical experiments on synthetic and real data.",
        "updated": "2024-06-10 17:55:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.06516v1"
    },
    {
        "title": "Random Features Approximation for Control-Affine Systems",
        "authors": "Kimia KazemianYahya SattarSarah Dean",
        "links": "http://arxiv.org/abs/2406.06514v1",
        "entry_id": "http://arxiv.org/abs/2406.06514v1",
        "pdf_url": "http://arxiv.org/pdf/2406.06514v1",
        "summary": "Modern data-driven control applications call for flexible nonlinear models\nthat are amenable to principled controller synthesis and realtime feedback.\nMany nonlinear dynamical systems of interest are control affine. We propose two\nnovel classes of nonlinear feature representations which capture control affine\nstructure while allowing for arbitrary complexity in the state dependence. Our\nmethods make use of random features (RF) approximations, inheriting the\nexpressiveness of kernel methods at a lower computational cost. We formalize\nthe representational capabilities of our methods by showing their relationship\nto the Affine Dot Product (ADP) kernel proposed by Casta\\~neda et al. (2021)\nand a novel Affine Dense (AD) kernel that we introduce. We further illustrate\nthe utility by presenting a case study of data-driven optimization-based\ncontrol using control certificate functions (CCF). Simulation experiments on a\ndouble pendulum empirically demonstrate the advantages of our methods.",
        "updated": "2024-06-10 17:54:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.06514v1"
    },
    {
        "title": "Robust Distribution Learning with Local and Global Adversarial Corruptions",
        "authors": "Sloan NietertZiv GoldfeldSoroosh Shafiee",
        "links": "http://arxiv.org/abs/2406.06509v1",
        "entry_id": "http://arxiv.org/abs/2406.06509v1",
        "pdf_url": "http://arxiv.org/pdf/2406.06509v1",
        "summary": "We consider learning in an adversarial environment, where an\n$\\varepsilon$-fraction of samples from a distribution $P$ are arbitrarily\nmodified (*global* corruptions) and the remaining perturbations have average\nmagnitude bounded by $\\rho$ (*local* corruptions). Given access to $n$ such\ncorrupted samples, we seek a computationally efficient estimator $\\hat{P}_n$\nthat minimizes the Wasserstein distance $\\mathsf{W}_1(\\hat{P}_n,P)$. In fact,\nwe attack the fine-grained task of minimizing $\\mathsf{W}_1(\\Pi_\\# \\hat{P}_n,\n\\Pi_\\# P)$ for all orthogonal projections $\\Pi \\in \\mathbb{R}^{d \\times d}$,\nwith performance scaling with $\\mathrm{rank}(\\Pi) = k$. This allows us to\naccount simultaneously for mean estimation ($k=1$), distribution estimation\n($k=d$), as well as the settings interpolating between these two extremes. We\ncharacterize the optimal population-limit risk for this task and then develop\nan efficient finite-sample algorithm with error bounded by $\\sqrt{\\varepsilon\nk} + \\rho + d^{O(1)}\\tilde{O}(n^{-1/k})$ when $P$ has bounded moments of order\n$2+\\delta$, for constant $\\delta > 0$. For data distributions with bounded\ncovariance, our finite-sample bounds match the minimax population-level optimum\nfor large sample sizes. Our efficient procedure relies on a novel trace norm\napproximation of an ideal yet intractable 2-Wasserstein projection estimator.\nWe apply this algorithm to robust stochastic optimization, and, in the process,\nuncover a new method for overcoming the curse of dimensionality in Wasserstein\ndistributionally robust optimization.",
        "updated": "2024-06-10 17:48:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.06509v1"
    },
    {
        "title": "Online Newton Method for Bandit Convex Optimisation",
        "authors": "Hidde FokkemaDirk van der HoevenTor LattimoreJack J. Mayo",
        "links": "http://arxiv.org/abs/2406.06506v1",
        "entry_id": "http://arxiv.org/abs/2406.06506v1",
        "pdf_url": "http://arxiv.org/pdf/2406.06506v1",
        "summary": "We introduce a computationally efficient algorithm for zeroth-order bandit\nconvex optimisation and prove that in the adversarial setting its regret is at\nmost $d^{3.5} \\sqrt{n} \\mathrm{polylog}(n, d)$ with high probability where $d$\nis the dimension and $n$ is the time horizon. In the stochastic setting the\nbound improves to $M d^{2} \\sqrt{n} \\mathrm{polylog}(n, d)$ where $M \\in\n[d^{-1/2}, d^{-1 / 4}]$ is a constant that depends on the geometry of the\nconstraint set and the desired computational properties.",
        "updated": "2024-06-10 17:44:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.06506v1"
    },
    {
        "title": "GKAN: Graph Kolmogorov-Arnold Networks",
        "authors": "Mehrdad KiamariMohammad KiamariBhaskar Krishnamachari",
        "links": "http://arxiv.org/abs/2406.06470v1",
        "entry_id": "http://arxiv.org/abs/2406.06470v1",
        "pdf_url": "http://arxiv.org/pdf/2406.06470v1",
        "summary": "We introduce Graph Kolmogorov-Arnold Networks (GKAN), an innovative neural\nnetwork architecture that extends the principles of the recently proposed\nKolmogorov-Arnold Networks (KAN) to graph-structured data. By adopting the\nunique characteristics of KANs, notably the use of learnable univariate\nfunctions instead of fixed linear weights, we develop a powerful model for\ngraph-based learning tasks. Unlike traditional Graph Convolutional Networks\n(GCNs) that rely on a fixed convolutional architecture, GKANs implement\nlearnable spline-based functions between layers, transforming the way\ninformation is processed across the graph structure. We present two different\nways to incorporate KAN layers into GKAN: architecture 1 -- where the learnable\nfunctions are applied to input features after aggregation and architecture 2 --\nwhere the learnable functions are applied to input features before aggregation.\nWe evaluate GKAN empirically using a semi-supervised graph learning task on a\nreal-world dataset (Cora). We find that architecture generally performs better.\nWe find that GKANs achieve higher accuracy in semi-supervised learning tasks on\ngraphs compared to the traditional GCN model. For example, when considering 100\nfeatures, GCN provides an accuracy of 53.5 while a GKAN with a comparable\nnumber of parameters gives an accuracy of 61.76; with 200 features, GCN\nprovides an accuracy of 61.24 while a GKAN with a comparable number of\nparameters gives an accuracy of 67.66. We also present results on the impact of\nvarious parameters such as the number of hidden nodes, grid-size, and the\npolynomial-degree of the spline on the performance of GKAN.",
        "updated": "2024-06-10 17:09:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.06470v1"
    }
]