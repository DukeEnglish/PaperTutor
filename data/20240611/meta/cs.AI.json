[
    {
        "title": "IllumiNeRF: 3D Relighting without Inverse Rendering",
        "authors": "Xiaoming ZhaoPratul P. SrinivasanDor VerbinKeunhong ParkRicardo Martin BruallaPhilipp Henzler",
        "links": "http://arxiv.org/abs/2406.06527v1",
        "entry_id": "http://arxiv.org/abs/2406.06527v1",
        "pdf_url": "http://arxiv.org/pdf/2406.06527v1",
        "summary": "Existing methods for relightable view synthesis -- using a set of images of\nan object under unknown lighting to recover a 3D representation that can be\nrendered from novel viewpoints under a target illumination -- are based on\ninverse rendering, and attempt to disentangle the object geometry, materials,\nand lighting that explain the input images. Furthermore, this typically\ninvolves optimization through differentiable Monte Carlo rendering, which is\nbrittle and computationally-expensive. In this work, we propose a simpler\napproach: we first relight each input image using an image diffusion model\nconditioned on lighting and then reconstruct a Neural Radiance Field (NeRF)\nwith these relit images, from which we render novel views under the target\nlighting. We demonstrate that this strategy is surprisingly competitive and\nachieves state-of-the-art results on multiple relighting benchmarks. Please see\nour project page at https://illuminerf.github.io/.",
        "updated": "2024-06-10 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.06527v1"
    },
    {
        "title": "Decentralized Personalized Federated Learning",
        "authors": "Salma KharratMarco CaniniSamuel Horvath",
        "links": "http://arxiv.org/abs/2406.06520v1",
        "entry_id": "http://arxiv.org/abs/2406.06520v1",
        "pdf_url": "http://arxiv.org/pdf/2406.06520v1",
        "summary": "This work tackles the challenges of data heterogeneity and communication\nlimitations in decentralized federated learning. We focus on creating a\ncollaboration graph that guides each client in selecting suitable collaborators\nfor training personalized models that leverage their local data effectively.\nOur approach addresses these issues through a novel, communication-efficient\nstrategy that enhances resource efficiency. Unlike traditional methods, our\nformulation identifies collaborators at a granular level by considering\ncombinatorial relations of clients, enhancing personalization while minimizing\ncommunication overhead. We achieve this through a bi-level optimization\nframework that employs a constrained greedy algorithm, resulting in a\nresource-efficient collaboration graph for personalized learning. Extensive\nevaluation against various baselines across diverse datasets demonstrates the\nsuperiority of our method, named DPFL. DPFL consistently outperforms other\napproaches, showcasing its effectiveness in handling real-world data\nheterogeneity, minimizing communication overhead, enhancing resource\nefficiency, and building personalized models in decentralized federated\nlearning scenarios.",
        "updated": "2024-06-10 17:58:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.06520v1"
    },
    {
        "title": "Merlin: A Vision Language Foundation Model for 3D Computed Tomography",
        "authors": "Louis BlankemeierJoseph Paul CohenAshwin KumarDave Van VeenSyed Jamal Safdar GardeziMagdalini PaschaliZhihong ChenJean-Benoit DelbrouckEduardo ReisCesar TruytsChristian BluethgenMalte Engmann Kjeldskov JensenSophie OstmeierMaya VarmaJeya Maria Jose ValanarasuZhongnan FangZepeng HuoZaid NabulsiDiego ArdilaWei-Hung WengEdson Amaro JuniorNeera AhujaJason FriesNigam H. ShahAndrew JohnstonRobert D. BoutinAndrew WentlandCurtis P. LanglotzJason HomSergios GatidisAkshay S. Chaudhari",
        "links": "http://arxiv.org/abs/2406.06512v1",
        "entry_id": "http://arxiv.org/abs/2406.06512v1",
        "pdf_url": "http://arxiv.org/pdf/2406.06512v1",
        "summary": "Over 85 million computed tomography (CT) scans are performed annually in the\nUS, of which approximately one quarter focus on the abdomen. Given the current\nradiologist shortage, there is a large impetus to use artificial intelligence\nto alleviate the burden of interpreting these complex imaging studies. Prior\nstate-of-the-art approaches for automated medical image interpretation leverage\nvision language models (VLMs). However, current medical VLMs are generally\nlimited to 2D images and short reports, and do not leverage electronic health\nrecord (EHR) data for supervision. We introduce Merlin - a 3D VLM that we train\nusing paired CT scans (6+ million images from 15,331 CTs), EHR diagnosis codes\n(1.8+ million codes), and radiology reports (6+ million tokens). We evaluate\nMerlin on 6 task types and 752 individual tasks. The non-adapted\n(off-the-shelf) tasks include zero-shot findings classification (31 findings),\nphenotype classification (692 phenotypes), and zero-shot cross-modal retrieval\n(image to findings and image to impressions), while model adapted tasks include\n5-year disease prediction (6 diseases), radiology report generation, and 3D\nsemantic segmentation (20 organs). We perform internal validation on a test set\nof 5,137 CTs, and external validation on 7,000 clinical CTs and on two public\nCT datasets (VerSe, TotalSegmentator). Beyond these clinically-relevant\nevaluations, we assess the efficacy of various network architectures and\ntraining strategies to depict that Merlin has favorable performance to existing\ntask-specific baselines. We derive data scaling laws to empirically assess\ntraining data needs for requisite downstream task performance. Furthermore,\nunlike conventional VLMs that require hundreds of GPUs for training, we perform\nall training on a single GPU.",
        "updated": "2024-06-10 17:53:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.06512v1"
    },
    {
        "title": "Monkey See, Monkey Do: Harnessing Self-attention in Motion Diffusion for Zero-shot Motion Transfer",
        "authors": "Sigal RaabInbar GatNathan SalaGuy TevetRotem Shalev-ArkushinOhad FriedAmit H. BermanoDaniel Cohen-Or",
        "links": "http://arxiv.org/abs/2406.06508v1",
        "entry_id": "http://arxiv.org/abs/2406.06508v1",
        "pdf_url": "http://arxiv.org/pdf/2406.06508v1",
        "summary": "Given the remarkable results of motion synthesis with diffusion models, a\nnatural question arises: how can we effectively leverage these models for\nmotion editing? Existing diffusion-based motion editing methods overlook the\nprofound potential of the prior embedded within the weights of pre-trained\nmodels, which enables manipulating the latent feature space; hence, they\nprimarily center on handling the motion space. In this work, we explore the\nattention mechanism of pre-trained motion diffusion models. We uncover the\nroles and interactions of attention elements in capturing and representing\nintricate human motion patterns, and carefully integrate these elements to\ntransfer a leader motion to a follower one while maintaining the nuanced\ncharacteristics of the follower, resulting in zero-shot motion transfer.\nEditing features associated with selected motions allows us to confront a\nchallenge observed in prior motion diffusion approaches, which use general\ndirectives (e.g., text, music) for editing, ultimately failing to convey subtle\nnuances effectively. Our work is inspired by how a monkey closely imitates what\nit sees while maintaining its unique motion patterns; hence we call it Monkey\nSee, Monkey Do, and dub it MoMo. Employing our technique enables accomplishing\ntasks such as synthesizing out-of-distribution motions, style transfer, and\nspatial editing. Furthermore, diffusion inversion is seldom employed for\nmotions; as a result, editing efforts focus on generated motions, limiting the\neditability of real ones. MoMo harnesses motion inversion, extending its\napplication to both real and generated motions. Experimental results show the\nadvantage of our approach over the current art. In particular, unlike methods\ntailored for specific applications through training, our approach is applied at\ninference time, requiring no training. Our webpage is at\nhttps://monkeyseedocg.github.io.",
        "updated": "2024-06-10 17:47:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.06508v1"
    },
    {
        "title": "Adaptive Opponent Policy Detection in Multi-Agent MDPs: Real-Time Strategy Switch Identification Using Running Error Estimation",
        "authors": "Mohidul Haque MridulMohammad Foysal KhanRedwan Ahmed RizveeMd Mosaddek Khan",
        "links": "http://arxiv.org/abs/2406.06500v1",
        "entry_id": "http://arxiv.org/abs/2406.06500v1",
        "pdf_url": "http://arxiv.org/pdf/2406.06500v1",
        "summary": "In Multi-agent Reinforcement Learning (MARL), accurately perceiving\nopponents' strategies is essential for both cooperative and adversarial\ncontexts, particularly within dynamic environments. While Proximal Policy\nOptimization (PPO) and related algorithms such as Actor-Critic with Experience\nReplay (ACER), Trust Region Policy Optimization (TRPO), and Deep Deterministic\nPolicy Gradient (DDPG) perform well in single-agent, stationary environments,\nthey suffer from high variance in MARL due to non-stationary and hidden\npolicies of opponents, leading to diminished reward performance. Additionally,\nexisting methods in MARL face significant challenges, including the need for\ninter-agent communication, reliance on explicit reward information, high\ncomputational demands, and sampling inefficiencies. These issues render them\nless effective in continuous environments where opponents may abruptly change\ntheir policies without prior notice. Against this background, we present\nOPS-DeMo (Online Policy Switch-Detection Model), an online algorithm that\nemploys dynamic error decay to detect changes in opponents' policies. OPS-DeMo\ncontinuously updates its beliefs using an Assumed Opponent Policy (AOP) Bank\nand selects corresponding responses from a pre-trained Response Policy Bank.\nEach response policy is trained against consistently strategizing opponents,\nreducing training uncertainty and enabling the effective use of algorithms like\nPPO in multi-agent environments. Comparative assessments show that our approach\noutperforms PPO-trained models in dynamic scenarios like the Predator-Prey\nsetting, providing greater robustness to sudden policy shifts and enabling more\ninformed decision-making through precise opponent policy insights.",
        "updated": "2024-06-10 17:34:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.06500v1"
    }
]