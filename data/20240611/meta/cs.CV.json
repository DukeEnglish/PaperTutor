[
    {
        "title": "IllumiNeRF: 3D Relighting without Inverse Rendering",
        "authors": "Xiaoming ZhaoPratul P. SrinivasanDor VerbinKeunhong ParkRicardo Martin BruallaPhilipp Henzler",
        "links": "http://arxiv.org/abs/2406.06527v1",
        "entry_id": "http://arxiv.org/abs/2406.06527v1",
        "pdf_url": "http://arxiv.org/pdf/2406.06527v1",
        "summary": "Existing methods for relightable view synthesis -- using a set of images of\nan object under unknown lighting to recover a 3D representation that can be\nrendered from novel viewpoints under a target illumination -- are based on\ninverse rendering, and attempt to disentangle the object geometry, materials,\nand lighting that explain the input images. Furthermore, this typically\ninvolves optimization through differentiable Monte Carlo rendering, which is\nbrittle and computationally-expensive. In this work, we propose a simpler\napproach: we first relight each input image using an image diffusion model\nconditioned on lighting and then reconstruct a Neural Radiance Field (NeRF)\nwith these relit images, from which we render novel views under the target\nlighting. We demonstrate that this strategy is surprisingly competitive and\nachieves state-of-the-art results on multiple relighting benchmarks. Please see\nour project page at https://illuminerf.github.io/.",
        "updated": "2024-06-10 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.06527v1"
    },
    {
        "title": "GaussianCity: Generative Gaussian Splatting for Unbounded 3D City Generation",
        "authors": "Haozhe XieZhaoxi ChenFangzhou HongZiwei Liu",
        "links": "http://arxiv.org/abs/2406.06526v1",
        "entry_id": "http://arxiv.org/abs/2406.06526v1",
        "pdf_url": "http://arxiv.org/pdf/2406.06526v1",
        "summary": "3D city generation with NeRF-based methods shows promising generation results\nbut is computationally inefficient. Recently 3D Gaussian Splatting (3D-GS) has\nemerged as a highly efficient alternative for object-level 3D generation.\nHowever, adapting 3D-GS from finite-scale 3D objects and humans to\ninfinite-scale 3D cities is non-trivial. Unbounded 3D city generation entails\nsignificant storage overhead (out-of-memory issues), arising from the need to\nexpand points to billions, often demanding hundreds of Gigabytes of VRAM for a\ncity scene spanning 10km^2. In this paper, we propose GaussianCity, a\ngenerative Gaussian Splatting framework dedicated to efficiently synthesizing\nunbounded 3D cities with a single feed-forward pass. Our key insights are\ntwo-fold: 1) Compact 3D Scene Representation: We introduce BEV-Point as a\nhighly compact intermediate representation, ensuring that the growth in VRAM\nusage for unbounded scenes remains constant, thus enabling unbounded city\ngeneration. 2) Spatial-aware Gaussian Attribute Decoder: We present\nspatial-aware BEV-Point decoder to produce 3D Gaussian attributes, which\nleverages Point Serializer to integrate the structural and contextual\ncharacteristics of BEV points. Extensive experiments demonstrate that\nGaussianCity achieves state-of-the-art results in both drone-view and\nstreet-view 3D city generation. Notably, compared to CityDreamer, GaussianCity\nexhibits superior performance with a speedup of 60 times (10.72 FPS v.s. 0.18\nFPS).",
        "updated": "2024-06-10 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.06526v1"
    },
    {
        "title": "Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation",
        "authors": "Peize SunYi JiangShoufa ChenShilong ZhangBingyue PengPing LuoZehuan Yuan",
        "links": "http://arxiv.org/abs/2406.06525v1",
        "entry_id": "http://arxiv.org/abs/2406.06525v1",
        "pdf_url": "http://arxiv.org/pdf/2406.06525v1",
        "summary": "We introduce LlamaGen, a new family of image generation models that apply\noriginal ``next-token prediction'' paradigm of large language models to visual\ngeneration domain. It is an affirmative answer to whether vanilla\nautoregressive models, e.g., Llama, without inductive biases on visual signals\ncan achieve state-of-the-art image generation performance if scaling properly.\nWe reexamine design spaces of image tokenizers, scalability properties of image\ngeneration models, and their training data quality. The outcome of this\nexploration consists of: (1) An image tokenizer with downsample ratio of 16,\nreconstruction quality of 0.94 rFID and codebook usage of 97% on ImageNet\nbenchmark. (2) A series of class-conditional image generation models ranging\nfrom 111M to 3.1B parameters, achieving 2.18 FID on ImageNet 256x256\nbenchmarks, outperforming the popular diffusion models such as LDM, DiT. (3) A\ntext-conditional image generation model with 775M parameters, from two-stage\ntraining on LAION-COCO and high aesthetics quality images, demonstrating\ncompetitive performance of visual quality and text alignment. (4) We verify the\neffectiveness of LLM serving frameworks in optimizing the inference speed of\nimage generation models and achieve 326% - 414% speedup. We release all models\nand codes to facilitate open-source community of visual generation and\nmultimodal foundation models.",
        "updated": "2024-06-10 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.06525v1"
    },
    {
        "title": "NaRCan: Natural Refined Canonical Image with Integration of Diffusion Prior for Video Editing",
        "authors": "Ting-Hsuan ChenJiewen ChanHau-Shiang ShiuShih-Han YenChang-Han YehYu-Lun Liu",
        "links": "http://arxiv.org/abs/2406.06523v1",
        "entry_id": "http://arxiv.org/abs/2406.06523v1",
        "pdf_url": "http://arxiv.org/pdf/2406.06523v1",
        "summary": "We propose a video editing framework, NaRCan, which integrates a hybrid\ndeformation field and diffusion prior to generate high-quality natural\ncanonical images to represent the input video. Our approach utilizes homography\nto model global motion and employs multi-layer perceptrons (MLPs) to capture\nlocal residual deformations, enhancing the model's ability to handle complex\nvideo dynamics. By introducing a diffusion prior from the early stages of\ntraining, our model ensures that the generated images retain a high-quality\nnatural appearance, making the produced canonical images suitable for various\ndownstream tasks in video editing, a capability not achieved by current\ncanonical-based methods. Furthermore, we incorporate low-rank adaptation (LoRA)\nfine-tuning and introduce a noise and diffusion prior update scheduling\ntechnique that accelerates the training process by 14 times. Extensive\nexperimental results show that our method outperforms existing approaches in\nvarious video editing tasks and produces coherent and high-quality edited video\nsequences. See our project page for video results at\nhttps://koi953215.github.io/NaRCan_page/.",
        "updated": "2024-06-10 17:59:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.06523v1"
    },
    {
        "title": "PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity Surface Reconstruction",
        "authors": "Danpeng ChenHai LiWeicai YeYifan WangWeijian XieShangjin ZhaiNan WangHaomin LiuHujun BaoGuofeng Zhang",
        "links": "http://arxiv.org/abs/2406.06521v1",
        "entry_id": "http://arxiv.org/abs/2406.06521v1",
        "pdf_url": "http://arxiv.org/pdf/2406.06521v1",
        "summary": "Recently, 3D Gaussian Splatting (3DGS) has attracted widespread attention due\nto its high-quality rendering, and ultra-fast training and rendering speed.\nHowever, due to the unstructured and irregular nature of Gaussian point clouds,\nit is difficult to guarantee geometric reconstruction accuracy and multi-view\nconsistency simply by relying on image reconstruction loss. Although many\nstudies on surface reconstruction based on 3DGS have emerged recently, the\nquality of their meshes is generally unsatisfactory. To address this problem,\nwe propose a fast planar-based Gaussian splatting reconstruction representation\n(PGSR) to achieve high-fidelity surface reconstruction while ensuring\nhigh-quality rendering. Specifically, we first introduce an unbiased depth\nrendering method, which directly renders the distance from the camera origin to\nthe Gaussian plane and the corresponding normal map based on the Gaussian\ndistribution of the point cloud, and divides the two to obtain the unbiased\ndepth. We then introduce single-view geometric, multi-view photometric, and\ngeometric regularization to preserve global geometric accuracy. We also propose\na camera exposure compensation model to cope with scenes with large\nillumination variations. Experiments on indoor and outdoor scenes show that our\nmethod achieves fast training and rendering while maintaining high-fidelity\nrendering and geometric reconstruction, outperforming 3DGS-based and NeRF-based\nmethods.",
        "updated": "2024-06-10 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.06521v1"
    }
]