[
    {
        "title": "IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation",
        "authors": "Luke Melas-KyriaziIro LainaChristian RupprechtNatalia NeverovaAndrea VedaldiOran GafniFilippos Kokkinos",
        "links": "http://arxiv.org/abs/2402.08682v1",
        "entry_id": "http://arxiv.org/abs/2402.08682v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08682v1",
        "summary": "Most text-to-3D generators build upon off-the-shelf text-to-image models\ntrained on billions of images. They use variants of Score Distillation Sampling\n(SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation\nis to fine-tune the 2D generator to be multi-view aware, which can help\ndistillation or can be combined with reconstruction networks to output 3D\nobjects directly. In this paper, we further explore the design space of\ntext-to-3D models. We significantly improve multi-view generation by\nconsidering video instead of image generators. Combined with a 3D\nreconstruction algorithm which, by using Gaussian splatting, can optimize a\nrobust image-based loss, we directly produce high-quality 3D outputs from the\ngenerated views. Our new method, IM-3D, reduces the number of evaluations of\nthe 2D generator network 10-100x, resulting in a much more efficient pipeline,\nbetter quality, fewer geometric inconsistencies, and higher yield of usable 3D\nassets.",
        "updated": "2024-02-13 18:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08682v1"
    },
    {
        "title": "Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance",
        "authors": "Linxi ZhaoYihe DengWeitong ZhangQuanquan Gu",
        "links": "http://arxiv.org/abs/2402.08680v1",
        "entry_id": "http://arxiv.org/abs/2402.08680v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08680v1",
        "summary": "The advancement of Large Vision-Language Models (LVLMs) has increasingly\nhighlighted the critical issue of their tendency to hallucinate non-existing\nobjects in the images. To address this issue, previous works focused on using\nspecially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the\noutputs of LVLMs. However, these approaches require either expensive\ntraining/fine-tuning or API access to advanced LLMs to correct the model's\noutput post-generation. In this paper, we tackle this challenge by introducing\na framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE\n(MARINE), which is both training-free and API-free, and can effectively and\nefficiently reduce object hallucinations during the generation process.\nSpecifically, MARINE enriches the visual context of LVLMs by integrating\nexisting open-source vision models, and employs classifier-free guidance to\nincorporate the additional object grounding features to improve the precision\nof LVLMs' generations. Through comprehensive evaluations across $6$ popular\nLVLMs with diverse evaluation metrics, we demonstrate the effectiveness of\nMARINE, which even outperforms existing fine-tuning-based methods. Remarkably,\nit not only reduces hallucinations but also improves the detailedness of LVLMs'\ngenerations, as assessed by GPT-4V.",
        "updated": "2024-02-13 18:59:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08680v1"
    },
    {
        "title": "COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability",
        "authors": "Xingang GuoFangxu YuHuan ZhangLianhui QinBin Hu",
        "links": "http://arxiv.org/abs/2402.08679v1",
        "entry_id": "http://arxiv.org/abs/2402.08679v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08679v1",
        "summary": "Jailbreaks on Large language models (LLMs) have recently received increasing\nattention. For a comprehensive assessment of LLM safety, it is essential to\nconsider jailbreaks with diverse attributes, such as contextual coherence and\nsentiment/stylistic variations, and hence it is beneficial to study\ncontrollable jailbreaking, i.e. how to enforce control on LLM attacks. In this\npaper, we formally formulate the controllable attack generation problem, and\nbuild a novel connection between this problem and controllable text generation,\na well-explored topic of natural language processing. Based on this connection,\nwe adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a\nstate-of-the-art, highly efficient algorithm in controllable text generation,\nand introduce the COLD-Attack framework which unifies and automates the search\nof adversarial LLM attacks under a variety of control requirements such as\nfluency, stealthiness, sentiment, and left-right-coherence. The controllability\nenabled by COLD-Attack leads to diverse new jailbreak scenarios which not only\ncover the standard setting of generating fluent suffix attacks, but also allow\nus to address new controllable attack settings such as revising a user query\nadversarially with minimal paraphrasing, and inserting stealthy attacks in\ncontext with left-right-coherence. Our extensive experiments on various LLMs\n(Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5) show COLD-Attack's broad\napplicability, strong controllability, high success rate, and attack\ntransferability. Our code is available at\nhttps://github.com/Yu-Fangxu/COLD-Attack.",
        "updated": "2024-02-13 18:58:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08679v1"
    },
    {
        "title": "Graph Mamba: Towards Learning on Graphs with State Space Models",
        "authors": "Ali BehrouzFarnoosh Hashemi",
        "links": "http://arxiv.org/abs/2402.08678v1",
        "entry_id": "http://arxiv.org/abs/2402.08678v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08678v1",
        "summary": "Graph Neural Networks (GNNs) have shown promising potential in graph\nrepresentation learning. The majority of GNNs define a local message-passing\nmechanism, propagating information over the graph by stacking multiple layers.\nThese methods, however, are known to suffer from two major limitations:\nover-squashing and poor capturing of long-range dependencies. Recently, Graph\nTransformers (GTs) emerged as a powerful alternative to Message-Passing Neural\nNetworks (MPNNs). GTs, however, have quadratic computational cost, lack\ninductive biases on graph structures, and rely on complex Positional/Structural\nEncodings (SE/PE). In this paper, we show that while Transformers, complex\nmessage-passing, and SE/PE are sufficient for good performance in practice,\nneither is necessary. Motivated by the recent success of State Space Models\n(SSMs), such as Mamba, we present Graph Mamba Networks (GMNs), a general\nframework for a new class of GNNs based on selective SSMs. We discuss and\ncategorize the new challenges when adopting SSMs to graph-structured data, and\npresent four required and one optional steps to design GMNs, where we choose\n(1) Neighborhood Tokenization, (2) Token Ordering, (3) Architecture of\nBidirectional Selective SSM Encoder, (4) Local Encoding, and dispensable (5) PE\nand SE. We further provide theoretical justification for the power of GMNs.\nExperiments demonstrate that despite much less computational cost, GMNs attain\nan outstanding performance in long-range, small-scale, large-scale, and\nheterophilic benchmark datasets.",
        "updated": "2024-02-13 18:58:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08678v1"
    },
    {
        "title": "A Convergence Analysis of Approximate Message Passing with Non-Separable Functions and Applications to Multi-Class Classification",
        "authors": "Burak ÇakmakYue M. LuManfred Opper",
        "links": "http://arxiv.org/abs/2402.08676v1",
        "entry_id": "http://arxiv.org/abs/2402.08676v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08676v1",
        "summary": "Motivated by the recent application of approximate message passing (AMP) to\nthe analysis of convex optimizations in multi-class classifications [Loureiro,\net. al., 2021], we present a convergence analysis of AMP dynamics with\nnon-separable multivariate nonlinearities. As an application, we present a\ncomplete (and independent) analysis of the motivated convex optimization\nproblem.",
        "updated": "2024-02-13 18:56:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08676v1"
    }
]