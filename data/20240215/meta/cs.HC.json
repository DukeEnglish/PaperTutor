[
    {
        "title": "The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting",
        "authors": "David HaagDevender KumarSebastian GruberMahdi SarebanGunnar TreffJosef NiebauerChristopher BullJan David Smeddinck",
        "links": "http://arxiv.org/abs/2402.08658v1",
        "entry_id": "http://arxiv.org/abs/2402.08658v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08658v1",
        "summary": "We explored the viability of Large Language Models (LLMs) for triggering and\npersonalizing content for Just-in-Time Adaptive Interventions (JITAIs) in\ndigital health. JITAIs are being explored as a key mechanism for sustainable\nbehavior change, adapting interventions to an individual's current context and\nneeds. However, traditional rule-based and machine learning models for JITAI\nimplementation face scalability and reliability limitations, such as lack of\npersonalization, difficulty in managing multi-parametric systems, and issues\nwith data sparsity. To investigate JITAI implementation via LLMs, we tested the\ncontemporary overall performance-leading model 'GPT-4' with examples grounded\nin the use case of fostering heart-healthy physical activity in outpatient\ncardiac rehabilitation. Three personas and five sets of context information per\npersona were used as a basis of triggering and personalizing JITAIs.\nSubsequently, we generated a total of 450 proposed JITAI decisions and message\ncontent, divided equally into JITAIs generated by 10 iterations with GPT-4, a\nbaseline provided by 10 laypersons (LayPs), and a gold standard set by 10\nhealthcare professionals (HCPs). Ratings from 27 LayPs indicated that JITAIs\ngenerated by GPT-4 were superior to those by HCPs and LayPs over all assessed\nscales: i.e., appropriateness, engagement, effectiveness, and professionality.\nThis study indicates that LLMs have significant potential for implementing\nJITAIs as a building block of personalized or \"precision\" health, offering\nscalability, effective personalization based on opportunistically sampled\ninformation, and good acceptability.",
        "updated": "2024-02-13 18:39:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08658v1"
    },
    {
        "title": "Assessing the Privacy Risk of Cross-Platform Identity Linkage using Eye Movement Biometrics",
        "authors": "Samantha AzizOleg Komogortsev",
        "links": "http://arxiv.org/abs/2402.08655v1",
        "entry_id": "http://arxiv.org/abs/2402.08655v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08655v1",
        "summary": "The recent emergence of ubiquitous, multi-platform eye tracking has raised\nuser privacy concerns over re-identification across platforms, where a person\nis re-identified across multiple eye tracking-enabled platforms using\npersonally identifying information that is implicitly expressed through their\neye movement. We present an empirical investigation quantifying a modern eye\nmovement biometric model's ability to link subject identities across three\ndifferent eye tracking devices using eye movement signals from each device. We\nshow that a state-of-the art eye movement biometrics model demonstrates\nabove-chance levels of biometric performance (34.99% equal error rate, 15%\nrank-1 identification rate) when linking user identities across one pair of\ndevices, but not for the other. Considering these findings, we also discuss the\nimpact that eye tracking signal quality has on the model's ability to\nmeaningfully associate a subject's identity between two substantially different\neye tracking devices. Our investigation advances a fundamental understanding of\nthe privacy risks for identity linkage across platforms by employing both\nquantitative and qualitative measures of biometric performance, including a\nvisualization of the model's ability to distinguish genuine and imposter\nauthentication attempts across platforms.",
        "updated": "2024-02-13 18:37:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08655v1"
    },
    {
        "title": "Artificial Intelligence for Literature Reviews: Opportunities and Challenges",
        "authors": "Francisco BolanosAngelo SalatinoFrancesco OsborneEnrico Motta",
        "links": "http://arxiv.org/abs/2402.08565v1",
        "entry_id": "http://arxiv.org/abs/2402.08565v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08565v1",
        "summary": "This manuscript presents a comprehensive review of the use of Artificial\nIntelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous\nand organised methodology that assesses and integrates previous research on a\ngiven topic. Numerous tools have been developed to assist and partially\nautomate the SLR process. The increasing role of AI in this field shows great\npotential in providing more effective support for researchers, moving towards\nthe semi-automatic creation of literature reviews. Our study focuses on how AI\ntechniques are applied in the semi-automation of SLRs, specifically in the\nscreening and extraction phases. We examine 21 leading SLR tools using a\nframework that combines 23 traditional features with 11 AI features. We also\nanalyse 11 recent tools that leverage large language models for searching the\nliterature and assisting academic writing. Finally, the paper discusses current\ntrends in the field, outlines key research challenges, and suggests directions\nfor future research.",
        "updated": "2024-02-13 16:05:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08565v1"
    },
    {
        "title": "Exploring diversity perceptions in a community through a Q&A chatbot",
        "authors": "Peter KunAmalia De GötzenMiriam BidogliaNiels Jørgen GommesenGeorge Gaskell",
        "links": "http://dx.doi.org/10.21606/drs.2022.807",
        "entry_id": "http://arxiv.org/abs/2402.08558v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08558v1",
        "summary": "While diversity has become a debated issue in design, very little research\nexists on positive use-cases for diversity beyond scholarly criticism. The\ncurrent work addresses this gap through the case of a diversity-aware chatbot,\nexploring what benefits a diversity-aware chatbot could bring to people and how\ndo people interpret diversity when being presented with it. In this paper, we\nmotivate a Q&A chatbot as a technology probe and deploy it in two student\ncommunities within a study. During the study, we collected contextual data on\npeople's expectations and perceptions when presented with diversity during the\nstudy. Our key findings show that people seek out others with shared niche\ninterests, or their search is driven by exploration and inspiration when\npresented with diversity. Although interacting with chatbots is limited,\nparticipants found the engagement novel and interesting to motivate future\nresearch.",
        "updated": "2024-02-13 15:59:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08558v1"
    },
    {
        "title": "Moonwalk: Advancing Gait-Based User Recognition on Wearable Devices with Metric Learning",
        "authors": "Asaf LibermanOron LevySoroush ShahiCori Tymoszek ParkMike RalphRichard KangAbdelkareem BedriGierad Laput",
        "links": "http://arxiv.org/abs/2402.08451v1",
        "entry_id": "http://arxiv.org/abs/2402.08451v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08451v1",
        "summary": "Personal devices have adopted diverse authentication methods, including\nbiometric recognition and passcodes. In contrast, headphones have limited input\nmechanisms, depending solely on the authentication of connected devices. We\npresent Moonwalk, a novel method for passive user recognition utilizing the\nbuilt-in headphone accelerometer. Our approach centers on gait recognition;\nenabling users to establish their identity simply by walking for a brief\ninterval, despite the sensor's placement away from the feet. We employ\nself-supervised metric learning to train a model that yields a highly\ndiscriminative representation of a user's 3D acceleration, with no retraining\nrequired. We tested our method in a study involving 50 participants, achieving\nan average F1 score of 92.9% and equal error rate of 2.3%. We extend our\nevaluation by assessing performance under various conditions (e.g. shoe types\nand surfaces). We discuss the opportunities and challenges these variations\nintroduce and propose new directions for advancing passive authentication for\nwearable devices.",
        "updated": "2024-02-13 13:38:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08451v1"
    }
]