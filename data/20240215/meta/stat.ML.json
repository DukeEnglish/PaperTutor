[
    {
        "title": "Target Score Matching",
        "authors": "Valentin De BortoliMichael HutchinsonPeter WirnsbergerArnaud Doucet",
        "links": "http://arxiv.org/abs/2402.08667v1",
        "entry_id": "http://arxiv.org/abs/2402.08667v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08667v1",
        "summary": "Denoising Score Matching estimates the score of a noised version of a target\ndistribution by minimizing a regression loss and is widely used to train the\npopular class of Denoising Diffusion Models. A well known limitation of\nDenoising Score Matching, however, is that it yields poor estimates of the\nscore at low noise levels. This issue is particularly unfavourable for problems\nin the physical sciences and for Monte Carlo sampling tasks for which the score\nof the clean original target is known. Intuitively, estimating the score of a\nslightly noised version of the target should be a simple task in such cases. In\nthis paper, we address this shortcoming and show that it is indeed possible to\nleverage knowledge of the target score. We present a Target Score Identity and\ncorresponding Target Score Matching regression loss which allows us to obtain\nscore estimates admitting favourable properties at low noise levels.",
        "updated": "2024-02-13 18:48:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08667v1"
    },
    {
        "title": "A Generalized Approach to Online Convex Optimization",
        "authors": "Mohammad PedramfarVaneet Aggarwal",
        "links": "http://arxiv.org/abs/2402.08621v1",
        "entry_id": "http://arxiv.org/abs/2402.08621v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08621v1",
        "summary": "In this paper, we analyze the problem of online convex optimization in\ndifferent settings. We show that any algorithm for online linear optimization\nwith fully adaptive adversaries is an algorithm for online convex optimization.\nWe also show that any such algorithm that requires full-information feedback\nmay be transformed to an algorithm with semi-bandit feedback with comparable\nregret bound. We further show that algorithms that are designed for fully\nadaptive adversaries using deterministic semi-bandit feedback can obtain\nsimilar bounds using only stochastic semi-bandit feedback when facing oblivious\nadversaries. We use this to describe general meta-algorithms to convert first\norder algorithms to zeroth order algorithms with comparable regret bounds. Our\nframework allows us to analyze online optimization in various settings, such\nfull-information feedback, bandit feedback, stochastic regret, adversarial\nregret and various forms of non-stationary regret. Using our analysis, we\nprovide the first efficient projection-free online convex optimization\nalgorithm using linear optimization oracles.",
        "updated": "2024-02-13 17:42:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08621v1"
    },
    {
        "title": "Adjustment Identification Distance: A gadjid for Causal Structure Learning",
        "authors": "Leonard HenckelTheo WürtzenSebastian Weichwald",
        "links": "http://arxiv.org/abs/2402.08616v1",
        "entry_id": "http://arxiv.org/abs/2402.08616v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08616v1",
        "summary": "Evaluating graphs learned by causal discovery algorithms is difficult: The\nnumber of edges that differ between two graphs does not reflect how the graphs\ndiffer with respect to the identifying formulas they suggest for causal\neffects. We introduce a framework for developing causal distances between\ngraphs which includes the structural intervention distance for directed acyclic\ngraphs as a special case. We use this framework to develop improved\nadjustment-based distances as well as extensions to completed partially\ndirected acyclic graphs and causal orders. We develop polynomial-time\nreachability algorithms to compute the distances efficiently. In our package\ngadjid (open source at https://github.com/CausalDisco/gadjid), we provide\nimplementations of our distances; they are orders of magnitude faster than the\nstructural intervention distance and thereby provide a success metric for\ncausal discovery that scales to graph sizes that were previously prohibitive.",
        "updated": "2024-02-13 17:32:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08616v1"
    },
    {
        "title": "Globally-Optimal Greedy Experiment Selection for Active Sequential Estimation",
        "authors": "Xiaoou LiHongru Zhao",
        "links": "http://arxiv.org/abs/2402.08602v1",
        "entry_id": "http://arxiv.org/abs/2402.08602v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08602v1",
        "summary": "Motivated by modern applications such as computerized adaptive testing,\nsequential rank aggregation, and heterogeneous data source selection, we study\nthe problem of active sequential estimation, which involves adaptively\nselecting experiments for sequentially collected data. The goal is to design\nexperiment selection rules for more accurate model estimation. Greedy\ninformation-based experiment selection methods, optimizing the information gain\nfor one-step ahead, have been employed in practice thanks to their\ncomputational convenience, flexibility to context or task changes, and broad\napplicability. However, statistical analysis is restricted to one-dimensional\ncases due to the problem's combinatorial nature and the seemingly limited\ncapacity of greedy algorithms, leaving the multidimensional problem open.\n  In this study, we close the gap for multidimensional problems. In particular,\nwe propose adopting a class of greedy experiment selection methods and provide\nstatistical analysis for the maximum likelihood estimator following these\nselection rules. This class encompasses both existing methods and introduces\nnew methods with improved numerical efficiency. We prove that these methods\nproduce consistent and asymptotically normal estimators. Additionally, within a\ndecision theory framework, we establish that the proposed methods achieve\nasymptotic optimality when the risk measure aligns with the selection rule. We\nalso conduct extensive numerical studies on both simulated and real data to\nillustrate the efficacy of the proposed methods.\n  From a technical perspective, we devise new analytical tools to address\ntheoretical challenges. These analytical tools are of independent theoretical\ninterest and may be reused in related problems involving stochastic\napproximation and sequential designs.",
        "updated": "2024-02-13 17:09:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08602v1"
    },
    {
        "title": "Theoretical Analysis of Leave-one-out Cross Validation for Non-differentiable Penalties under High-dimensional Settings",
        "authors": "Haolin ZouArnab AuddyKamiar Rahnama RadArian Maleki",
        "links": "http://arxiv.org/abs/2402.08543v2",
        "entry_id": "http://arxiv.org/abs/2402.08543v2",
        "pdf_url": "http://arxiv.org/pdf/2402.08543v2",
        "summary": "Despite a large and significant body of recent work focused on estimating the\nout-of-sample risk of regularized models in the high dimensional regime, a\ntheoretical understanding of this problem for non-differentiable penalties such\nas generalized LASSO and nuclear norm is missing. In this paper we resolve this\nchallenge. We study this problem in the proportional high dimensional regime\nwhere both the sample size n and number of features p are large, and n/p and\nthe signal-to-noise ratio (per observation) remain finite. We provide finite\nsample upper bounds on the expected squared error of leave-one-out\ncross-validation (LO) in estimating the out-of-sample risk. The theoretical\nframework presented here provides a solid foundation for elucidating empirical\nfindings that show the accuracy of LO.",
        "updated": "2024-02-14 16:28:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08543v2"
    }
]