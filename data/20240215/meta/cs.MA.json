[
    {
        "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast",
        "authors": "Xiangming GuXiaosen ZhengTianyu PangChao DuQian LiuYe WangJing JiangMin Lin",
        "links": "http://arxiv.org/abs/2402.08567v1",
        "entry_id": "http://arxiv.org/abs/2402.08567v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08567v1",
        "summary": "A multimodal large language model (MLLM) agent can receive instructions,\ncapture images, retrieve histories from memory, and decide which tools to use.\nNonetheless, red-teaming efforts have revealed that adversarial images/prompts\ncan jailbreak an MLLM and cause unaligned behaviors. In this work, we report an\neven more severe safety issue in multi-agent environments, referred to as\ninfectious jailbreak. It entails the adversary simply jailbreaking a single\nagent, and without any further intervention from the adversary, (almost) all\nagents will become infected exponentially fast and exhibit harmful behaviors.\nTo validate the feasibility of infectious jailbreak, we simulate multi-agent\nenvironments containing up to one million LLaVA-1.5 agents, and employ\nrandomized pair-wise chat as a proof-of-concept instantiation for multi-agent\ninteraction. Our results show that feeding an (infectious) adversarial image\ninto the memory of any randomly chosen agent is sufficient to achieve\ninfectious jailbreak. Finally, we derive a simple principle for determining\nwhether a defense mechanism can provably restrain the spread of infectious\njailbreak, but how to design a practical defense that meets this principle\nremains an open question to investigate. Our project page is available at\nhttps://sail-sg.github.io/Agent-Smith/.",
        "updated": "2024-02-13 16:06:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08567v1"
    },
    {
        "title": "Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning for Digital Twins",
        "authors": "Eslam EldeebHoussem SifaouOsvaldo SimeoneMohammad ShehabHirley Alves",
        "links": "http://arxiv.org/abs/2402.08421v1",
        "entry_id": "http://arxiv.org/abs/2402.08421v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08421v1",
        "summary": "Digital twin (DT) platforms are increasingly regarded as a promising\ntechnology for controlling, optimizing, and monitoring complex engineering\nsystems such as next-generation wireless networks. An important challenge in\nadopting DT solutions is their reliance on data collected offline, lacking\ndirect access to the physical environment. This limitation is particularly\nsevere in multi-agent systems, for which conventional multi-agent reinforcement\n(MARL) requires online interactions with the environment. A direct application\nof online MARL schemes to an offline setting would generally fail due to the\nepistemic uncertainty entailed by the limited availability of data. In this\nwork, we propose an offline MARL scheme for DT-based wireless networks that\nintegrates distributional RL and conservative Q-learning to address the\nenvironment's inherent aleatoric uncertainty and the epistemic uncertainty\narising from limited data. To further exploit the offline data, we adapt the\nproposed scheme to the centralized training decentralized execution framework,\nallowing joint training of the agents' policies. The proposed MARL scheme,\nreferred to as multi-agent conservative quantile regression (MA-CQR) addresses\ngeneral risk-sensitive design criteria and is applied to the trajectory\nplanning problem in drone networks, showcasing its advantages.",
        "updated": "2024-02-13 12:49:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08421v1"
    },
    {
        "title": "Logic of Awareness for Nested Knowledge",
        "authors": "Yudai Kubono",
        "links": "http://arxiv.org/abs/2402.08282v1",
        "entry_id": "http://arxiv.org/abs/2402.08282v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08282v1",
        "summary": "Reasoning abilities of human beings are limited. Logics that treat logical\ninference for human knowledge should reflect these limited abilities. Logic of\nawareness is one of those logics. In the logic, what an agent with a limited\nreasoning ability actually knows at a given moment (explicit knowledge) is\ndistinguished from the ideal knowledge that an agent obtains by performing all\npossible inferences with what she already knows (implicit knowledge). This\npaper proposes a logic for explicit knowledge. In particular, we focus more on\nnested explicit knowledge, which means another agent's knowledge that an agent\nactually knows at a given moment. We develope a new formalization of two ideas\nand propose Kripke-style semantics. The first idea is the effect on an agent's\nreasoning ability by a state of an agent's awareness. We incorporate a relation\non possible worlds called an indistinguishable relation to represent ignorance\ndue to lack of awareness. The second idea is a state of each agent's awareness\nin the other agent's mind. We incorporate a non-empty finite sequence of agents\ncalled \\textit{a chain of belief for awareness}. Our logic is called Awareness\nLogic with Partitions and Chains (ALPC). Employing an example, we show how\nnested explicit knowledge is formalized with our logic. Thereafter, we propose\nthe proof system and prove the completeness. Finally, we discuss directions for\nextending and applying our logic and conclude. Our logic offers a foundation\nfor a formal representation of human knowledge. We expect that the logic can be\napplied to computer science and game theory by describing and analyzing\nstrategic behavior in a game and practical agent communication.",
        "updated": "2024-02-13 08:18:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08282v1"
    },
    {
        "title": "Group Decision-Making among Privacy-Aware Agents",
        "authors": "Marios PapachristouM. Amin Rahimian",
        "links": "http://arxiv.org/abs/2402.08156v1",
        "entry_id": "http://arxiv.org/abs/2402.08156v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08156v1",
        "summary": "How can individuals exchange information to learn from each other despite\ntheir privacy needs and security concerns? For example, consider individuals\ndeliberating a contentious topic and being concerned about divulging their\nprivate experiences. Preserving individual privacy and enabling efficient\nsocial learning are both important desiderata but seem fundamentally at odds\nwith each other and very hard to reconcile. We do so by controlling information\nleakage using rigorous statistical guarantees that are based on differential\nprivacy (DP). Our agents use log-linear rules to update their beliefs after\ncommunicating with their neighbors. Adding DP randomization noise to beliefs\nprovides communicating agents with plausible deniability with regard to their\nprivate information and their network neighborhoods. We consider two learning\nenvironments one for distributed maximum-likelihood estimation given a finite\nnumber of private signals and another for online learning from an infinite,\nintermittent signal stream. Noisy information aggregation in the finite case\nleads to interesting tradeoffs between rejecting low-quality states and making\nsure all high-quality states are accepted in the algorithm output. Our results\nflesh out the nature of the trade-offs in both cases between the quality of the\ngroup decision outcomes, learning accuracy, communication cost, and the level\nof privacy protections that the agents are afforded.",
        "updated": "2024-02-13 01:38:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08156v1"
    },
    {
        "title": "Mixed Q-Functionals: Advancing Value-Based Methods in Cooperative MARL with Continuous Action Domains",
        "authors": "Yasin FindikS. Reza Ahmadzadeh",
        "links": "http://arxiv.org/abs/2402.07752v1",
        "entry_id": "http://arxiv.org/abs/2402.07752v1",
        "pdf_url": "http://arxiv.org/pdf/2402.07752v1",
        "summary": "Tackling multi-agent learning problems efficiently is a challenging task in\ncontinuous action domains. While value-based algorithms excel in sample\nefficiency when applied to discrete action domains, they are usually\ninefficient when dealing with continuous actions. Policy-based algorithms, on\nthe other hand, attempt to address this challenge by leveraging critic networks\nfor guiding the learning process and stabilizing the gradient estimation. The\nlimitations in the estimation of true return and falling into local optima in\nthese methods result in inefficient and often sub-optimal policies. In this\npaper, we diverge from the trend of further enhancing critic networks, and\nfocus on improving the effectiveness of value-based methods in multi-agent\ncontinuous domains by concurrently evaluating numerous actions. We propose a\nnovel multi-agent value-based algorithm, Mixed Q-Functionals (MQF), inspired\nfrom the idea of Q-Functionals, that enables agents to transform their states\ninto basis functions. Our algorithm fosters collaboration among agents by\nmixing their action-values. We evaluate the efficacy of our algorithm in six\ncooperative multi-agent scenarios. Our empirical findings reveal that MQF\noutperforms four variants of Deep Deterministic Policy Gradient through rapid\naction evaluation and increased sample efficiency.",
        "updated": "2024-02-12 16:21:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.07752v1"
    }
]