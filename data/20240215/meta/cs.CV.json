[
    {
        "title": "IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation",
        "authors": "Luke Melas-KyriaziIro LainaChristian RupprechtNatalia NeverovaAndrea VedaldiOran GafniFilippos Kokkinos",
        "links": "http://arxiv.org/abs/2402.08682v1",
        "entry_id": "http://arxiv.org/abs/2402.08682v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08682v1",
        "summary": "Most text-to-3D generators build upon off-the-shelf text-to-image models\ntrained on billions of images. They use variants of Score Distillation Sampling\n(SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation\nis to fine-tune the 2D generator to be multi-view aware, which can help\ndistillation or can be combined with reconstruction networks to output 3D\nobjects directly. In this paper, we further explore the design space of\ntext-to-3D models. We significantly improve multi-view generation by\nconsidering video instead of image generators. Combined with a 3D\nreconstruction algorithm which, by using Gaussian splatting, can optimize a\nrobust image-based loss, we directly produce high-quality 3D outputs from the\ngenerated views. Our new method, IM-3D, reduces the number of evaluations of\nthe 2D generator network 10-100x, resulting in a much more efficient pipeline,\nbetter quality, fewer geometric inconsistencies, and higher yield of usable 3D\nassets.",
        "updated": "2024-02-13 18:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08682v1"
    },
    {
        "title": "Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance",
        "authors": "Linxi ZhaoYihe DengWeitong ZhangQuanquan Gu",
        "links": "http://arxiv.org/abs/2402.08680v1",
        "entry_id": "http://arxiv.org/abs/2402.08680v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08680v1",
        "summary": "The advancement of Large Vision-Language Models (LVLMs) has increasingly\nhighlighted the critical issue of their tendency to hallucinate non-existing\nobjects in the images. To address this issue, previous works focused on using\nspecially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the\noutputs of LVLMs. However, these approaches require either expensive\ntraining/fine-tuning or API access to advanced LLMs to correct the model's\noutput post-generation. In this paper, we tackle this challenge by introducing\na framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE\n(MARINE), which is both training-free and API-free, and can effectively and\nefficiently reduce object hallucinations during the generation process.\nSpecifically, MARINE enriches the visual context of LVLMs by integrating\nexisting open-source vision models, and employs classifier-free guidance to\nincorporate the additional object grounding features to improve the precision\nof LVLMs' generations. Through comprehensive evaluations across $6$ popular\nLVLMs with diverse evaluation metrics, we demonstrate the effectiveness of\nMARINE, which even outperforms existing fine-tuning-based methods. Remarkably,\nit not only reduces hallucinations but also improves the detailedness of LVLMs'\ngenerations, as assessed by GPT-4V.",
        "updated": "2024-02-13 18:59:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08680v1"
    },
    {
        "title": "Are Semi-Dense Detector-Free Methods Good at Matching Local Features?",
        "authors": "Matthieu VilainRémi GiraudHugo GermainGuillaume Bourmaud",
        "links": "http://arxiv.org/abs/2402.08671v1",
        "entry_id": "http://arxiv.org/abs/2402.08671v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08671v1",
        "summary": "Semi-dense detector-free approaches (SDF), such as LoFTR, are currently among\nthe most popular image matching methods. While SDF methods are trained to\nestablish correspondences between two images, their performances are almost\nexclusively evaluated using relative pose estimation metrics. Thus, the link\nbetween their ability to establish correspondences and the quality of the\nresulting estimated pose has thus far received little attention. This paper is\na first attempt to study this link. We start with proposing a novel structured\nattention-based image matching architecture (SAM). It allows us to show a\ncounter-intuitive result on two datasets (MegaDepth and HPatches): on the one\nhand SAM either outperforms or is on par with SDF methods in terms of\npose/homography estimation metrics, but on the other hand SDF approaches are\nsignificantly better than SAM in terms of matching accuracy. We then propose to\nlimit the computation of the matching accuracy to textured regions, and show\nthat in this case SAM often surpasses SDF methods. Our findings highlight a\nstrong correlation between the ability to establish accurate correspondences in\ntextured regions and the accuracy of the resulting estimated pose/homography.\nOur code will be made available.",
        "updated": "2024-02-13 18:53:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08671v1"
    },
    {
        "title": "PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs",
        "authors": "Michael DorkenwaldNimrod BarazaniCees G. M. SnoekYuki M. Asano",
        "links": "http://arxiv.org/abs/2402.08657v1",
        "entry_id": "http://arxiv.org/abs/2402.08657v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08657v1",
        "summary": "Vision-Language Models (VLMs), such as Flamingo and GPT-4V, have shown\nimmense potential by integrating large language models with vision systems.\nNevertheless, these models face challenges in the fundamental computer vision\ntask of object localisation, due to their training on multimodal data\ncontaining mostly captions without explicit spatial grounding. While it is\npossible to construct custom, supervised training pipelines with bounding box\nannotations that integrate with VLMs, these result in specialized and\nhard-to-scale models. In this paper, we aim to explore the limits of\ncaption-based VLMs and instead propose to tackle the challenge in a simpler\nmanner by i) keeping the weights of a caption-based VLM frozen and ii) not\nusing any supervised detection data. To this end, we introduce an\ninput-agnostic Positional Insert (PIN), a learnable spatial prompt, containing\na minimal set of parameters that are slid inside the frozen VLM, unlocking\nobject localisation capabilities. Our PIN module is trained with a simple\nnext-token prediction task on synthetic data without requiring the introduction\nof new output heads. Our experiments demonstrate strong zero-shot localisation\nperformances on a variety of images, including Pascal VOC, COCO, LVIS, and\ndiverse images like paintings or cartoons.",
        "updated": "2024-02-13 18:39:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08657v1"
    },
    {
        "title": "Learning Continuous 3D Words for Text-to-Image Generation",
        "authors": "Ta-Ying ChengMatheus GadelhaThibault GroueixMatthew FisherRadomir MechAndrew MarkhamNiki Trigoni",
        "links": "http://arxiv.org/abs/2402.08654v1",
        "entry_id": "http://arxiv.org/abs/2402.08654v1",
        "pdf_url": "http://arxiv.org/pdf/2402.08654v1",
        "summary": "Current controls over diffusion models (e.g., through text or ControlNet) for\nimage generation fall short in recognizing abstract, continuous attributes like\nillumination direction or non-rigid shape change. In this paper, we present an\napproach for allowing users of text-to-image models to have fine-grained\ncontrol of several attributes in an image. We do this by engineering special\nsets of input tokens that can be transformed in a continuous manner -- we call\nthem Continuous 3D Words. These attributes can, for example, be represented as\nsliders and applied jointly with text prompts for fine-grained control over\nimage generation. Given only a single mesh and a rendering engine, we show that\nour approach can be adopted to provide continuous user control over several\n3D-aware attributes, including time-of-day illumination, bird wing orientation,\ndollyzoom effect, and object poses. Our method is capable of conditioning image\ncreation with multiple Continuous 3D Words and text descriptions simultaneously\nwhile adding no overhead to the generative process. Project Page:\nhttps://ttchengab.github.io/continuous_3d_words",
        "updated": "2024-02-13 18:34:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.08654v1"
    }
]