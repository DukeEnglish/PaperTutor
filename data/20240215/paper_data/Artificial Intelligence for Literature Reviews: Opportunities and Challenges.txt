Artificial Intelligence for Literature Reviews:
Opportunities and Challenges
Francisco Bolan˜os1*, Angelo Salatino1, Francesco Osborne1,2,
Enrico Motta1
1Knowledge Media Institute, The Open University, Walton Hall, Milton
Keynes, MK7 6AA, UK.
2Department of Business and Law, University of Milano Bicocca, Piazza
dell’Ateneo Nuovo, 1, Milan, 20126, IT.
*Corresponding author(s). E-mail(s):
francisco.bolanos-burgos@open.ac.uk;
Contributing authors: angelo.salatino@open.ac.uk;
francesco.osborne@open.ac.uk; enrico.motta@open.ac.uk;
Abstract
This manuscript presents a comprehensive review of the use of Artificial Intel-
ligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and
organisedmethodologythatassessesandintegratespreviousresearchonagiven
topic. Numerous tools have been developed to assist and partially automate
the SLR process. The increasing role of AI in this field shows great potential
in providing more effective support for researchers, moving towards the semi-
automaticcreationofliteraturereviews.OurstudyfocusesonhowAItechniques
are applied in the semi-automation of SLRs, specifically in the screening and
extractionphases.Weexamine21leadingSLRtoolsusingaframeworkthatcom-
bines23traditionalfeatureswith11AIfeatures.Wealsoanalyse11recenttools
thatleveragelargelanguagemodelsforsearchingtheliteratureandassistingaca-
demic writing. Finally, the paper discusses current trends in the field, outlines
key research challenges, and suggests directions for future research.
Keywords:SystematicLiteratureReviews,LiteratureReview,ArtificialIntelligence,
LargeLanguageModels,NaturalLanguageProcessing,Usability
1
4202
beF
31
]IA.sc[
1v56580.2042:viXra1 Introduction
A Systematic Literature Review (SLR) is a rigorous and organised methodology that
assesses and integrates previous research on a specific topic. Its main goal is to metic-
ulously identify and appraise all the relevant literature related to a specific research
question, ensuring a reduction in bias by adhering to stringent protocols [1, 2]. This
methodology originally emerged within the realm of Evidence-Based Medicine [3],
and it was subsequently adapted and employed in diverse research disciplines includ-
ing social sciences [4], engineering and technology [5], education [6], environmental
sciences [7], and business and management [8].
SLRs are recognised for being time-consuming and resource-intensive. This is due
toseveralfactors,suchasthedurationofthewholeprocess,whichcanextendbeyond
a year [9]; the necessity for a multidisciplinary team of experts, often accompanied by
specific training [10]; significant financial implications arising from database subscrip-
tions, specialised software utilisation, and personnel remunerations [11]; the growing
rate of the number of publication [12], and the periodic need for updates to ensure
continued relevance [13].
Over the past decades, numerous tools have been developed to support and even
partially automate SLRs, aiming at addressing several of the highlighted challenges.
Several of these tools adopted Artificial Intelligence (AI) solutions [14, 15] in order
to support SLRs, in particular for the screening and data extraction phases. The
incorporationofAIintoSLRtoolshasbeenfurtherpropelledbytheemergenceofmore
sophisticated AI techniques in Natural Language Processing (NLP), such as Large
LanguageModels(LLMs),whichhavethepotentialtorevolutionisethesesystems[16].
While it exists an extensive body of literature examining earlier iterations of SLR
tools, it appears that these studies have largely overlooked the role of AI embedded
within such tools [17–23].
This survey aims to address the existing gap by rigorously examining the appli-
cation of AI techniques in the semi-automation of SLRs, within the two main stages
of application, namely screening and extraction. For this purpose, we first conducted
an analysis of four prior surveys and identified the most prominent features examined
in the literature. Next, we defined a framework of analysis that integrates 23 gen-
eral features and 11 features pertinent to AI-based functionalities. We then selected
21 prominent SLR tools and subjected them to rigorous analysis using the result-
ing framework. We extensively discuss the current status, the research challenges and
opportunities associated with AI for SLR. We also perform additional analysis on 11
very recent tools that utilise the capabilities of LLMs (predominantly ChatGPT via
the OpenAI API) for searching the literature and aiding academic writing. Although
these tools do not cater directly to SLRs, there is potential for their features to be
integrated into future SLR tools. In conclusion, this survey seeks to offer scholars a
thorough insight into the application of Artificial Intelligence in this field, while also
highlighting potential avenues for future research.
The remainder of this paper is structured as follows. Section 2 includes a descrip-
tion of the SLR stages and their relationship with AI (Section 2.1) and a detailed
description of the methodology we followed to identify the tools covered in our sur-
vey (Section 2.2). In Section 3, we provide a meta-review of previous surveys about
2SLRs. In Section 4, we provide an in-depth examination of the 21 tools. Section 5
discussesthekeyresearchchallenges.Section6analysesthelatestgenerationofLLM-
based systems designed to assist researchers. Finally, Section 7 concludes the paper
by summarising the contributions and the main findings.
2 Background
In this section, we first provide a comprehensive explanation of the SLR stages and
the AI involvement (Section 2.1). We then describe the PRISMA methodology [24]
employed to identify, filter, and analyse the SLR tools (Section 2.2).
2.1 SLR Stages and AI involvement
In this section, we examine the various stages of a SLR and the extent of support
they receive from AI in contemporary tools. Here, the term ‘AI’ specifically denotes
weak or narrow AI, which includes systems designed and trained for specific tasks
like classification, clustering, or named-entity recognition [25]. In the context of SLR,
these methodologies are predominantly utilised to semi-automate tasks like screening
and data extraction [17, 21].
The SLR methodology consists of six distinct stages [1, 5]: i) Planning, ii) Search,
iii) Screening, iv) Data Extraction and Synthesis, v) Quality Assessment, and
vi) Reporting. Each stage plays a pivotal role in ensuring the comprehensiveness and
rigour of the review process.
The planning phase is foundational to the entire review process, as it involves
formulating a set of precise and specific research questions that the SLR seeks
to address [26]. A detailed protocol is also developed during this stage, outlining
the appropriate methodologies that will be adopted to carry out the review [27].
This protocol ensures consistency, reduces bias, and enhances the transparency and
reproducibility of the review.
The search phase aims to identify relevant papers using search strategies, snow-
balling,orahybridapproach.Searchstrategiesaretypicallyimplementedbycreating
a query based on a combination of terms using boolean operators [28, 29]. This query
is then executed on designated search engines. In snowballing, the researcher exam-
ines the references and citations of an initial group of papers (also known as seed
papers)toidentifyadditionalarticles.Thisprocessisiterativelyrepeateduntilnonew
relevant scholarly documents are found [30, 31]. The hybrid approach is the combina-
tion of search strategy and snowballing [32, 33]. Traditionally, the search phase had
notbeensignificantlysupportedbyartificialintelligencetechniques[34].Nevertheless,
there are some emerging tools, which we will examine in Section 6, that have begun
to incorporate LLMs in academic search engines, often within a Retrieval-Augmented
Generation (RAG) framework [35]. This innovative approach allows for the formu-
lation of precise questions and complex queries in natural language, surpassing the
capabilities of traditional keyword-based searches.
The screening phase uses a set of inclusion and exclusion criteria to further filter
the paper obtained from the search stage. It typically consists of two stages: i) title
andabstractscreeningandii)full-textscreening.Inthefirststep,thereviewersscreen
3therelevantpapersaccordingonlytothetitleandabstract[2].Thesecondstepentails
a detailed evaluation of the content of each paper, a task that demands significantly
more effort but leads to a thorough assessment.
It is customary to document the rationale for excluding any given paper during
this process. The predominant application of AI in SLR regards this phase. It usu-
ally involves employing machine learning classifiers, which are trained on an initial
set of user-selected papers and then used to identify additional relevant articles [36].
This process frequently involves iteration, where the user refines the automatic classi-
fications or selects new papers, followed by retraining the classifier to better identify
further pertinent literature.
In the data extraction and synthesis phase, all the pertinent information is sys-
tematically extracted from the selected studies. The techniques for data extraction
vary greatly depending on the research field and the objective of the researcher. For
example, in Medicine there are protocols for identifying key elements from clinical
studies, such as PECODR [37] that identifies Patient-Population-Problem, Exposure-
Intervention, Comparison, Outcome, Duration, and Results. Others are PIBOSO [38]
(Population, Intervention, Background, Outcome, Study Design, and Other) and
STARD [39], which is a checklist to support readers in assessing the risk of bias in
the study and evaluate the relevance of the results. Following the extraction, the
data is aggregated and summarised [40, 41]. Depending on the nature and hetero-
geneity of the data, the resulting synthesis might be qualitative or quantitative. This
phase is also occasionally supported by AI solutions. Commonly, the relevant tools
employ classifiers to identify articles possessing specific characteristics [42] or imple-
ment named-entity recognition for extracting specific entities or concepts [43] (e.g.,
RCT entities [44], entities pertaining environmental health studies [45]).
The quality assessment phase evaluates the rigour and validity of the selected
studies [46–49]. This analysis provides evidence of the overall strength and the level
of trustworthiness presented in the review [50, 51].
Finally, the reporting phase involves presenting the findings in a structured and
coherent manner within a research paper. This presentation typically follows an
established format comprising sections like introduction, methods, results, and dis-
cussion, but this may differ depending on the journal in which the manuscript will
be published [24, 52]. Historically, this stage did not benefit from the use of artifi-
cial intelligence techniques [53, 54]. However, as we will discuss in Section 6, recent
advancementshaveledtothedevelopmentoftoolsbasedonLLMsdesignedtosupport
academicwriting,whichcanbeparticularlyusefulinthisphase.Thesetoolstypically
enableuserstodraftaninitialoutlineofthedesireddocumentanditerativelyrefineit.
2.2 Methodology for the retrieval of the relevant SLR tools
We adopt the standard PRISMA [24] (Preferred Reporting Items for Systematic
ReviewsandMeta-Analyses)methodologyforconductingandreportingthesystematic
review and the meta-analysis.
The primary objective of our analysis was to examine the application of artificial
intelligence in the current generation of SLR tools to identify trends and emerging
research directions. In order to identify the set of AI-enhanced SLR tools, we first
4formulatedthreeinclusioncriteriaandtwoexclusioncriteria.Specifically,theinclusion
criteria are the following:
IC 1. The SLR tool must incorporate AI techniques to semi-automate the
screening or extraction process, while still maintaining the user’s capacity
to make the final decision. [55];
IC 2. The tool must possess a user interface that facilitates paper screening or
information extraction by the user;
IC 3. The tool should not require advanced technical expertise for installation
and execution.
The exclusion criteria are:
EC 1. The tool is under maintenance;
EC 2. The tool has not been updated in the last 10 years.
The PRISMA diagram, depicted in Figure 1 illustrates the main phases of the
process. We utilised three main strategies for identifying the tools. First, we used the
systematicliteraturereviewtoolbox[56],awebsiteinwhichSLRtoolsarepostedand
updated.Specifically,weutilisedtheadvancedsearchfunctionalityretrievingalltools
under the the “software” category. The query returned 236 tools that were manually
analysed. After applying the inclusion and exclusion criteria, we identified a total of
21 tools. Out of these, 16 were associated with academic papers, while the remaining
5 lacked such documentation.
We then performed a customised query in the CRAN repository1. Specifically, we
utilised the packagefinder [57] library and conducted searches using the terms “sys-
tematicliteraturereview”,“systematicreview”and “literature review”.Thisresulted
in 329 tools. However, these tools either did not implement AI solutions or they did
not offer a visual interface. Therefore, we discarded all of them.
Finally, we searched for relevant survey papers in Scopus2. Specifically, we used
the query: (“Literature Reviews” OR “Systematic Review”) AND (“Tools” OR
“Automation” OR “Semi-Automation” OR “Semiautomation” OR “Software”)3.
Wefilteredtheresultsbydocumenttypefocusingonreviews.Thenweselectedthe
surveys that analysed the implementation of AI in SLRs. Additionally, we applied a
snowballing search [30], which consisted of examining the references and the citations
of the selected surveys aiming to add additional surveys. This resulted in 11 survey
papersaboutSLRtools.AnanalysisoftheseSLRsledtotheidentificationof23tools.
Of them, 17 were associated with academic papers, whereas 6 were not.
From the 33 tools identified in the systematic literature review toolbox and the
previous surveys, we removed 16 duplicates. This led to an initial collection of 17
unique seed papers.
Toensurecompletenessinourtoolset,weconductedanothersnowballingsearchfor
tools linked with scholarly papers. This search was carried out on Semantic Scholar4,
selected for its extensive coverage in Computer Science, particularly for snowballing
searches [58]. For this, we developed a custom script to interface with the Semantic
Scholar API, allowing for the efficient retrieval of references from initial (seed) papers
1CRANrepository-https://cran.r-project.org/
2Scopus-https://www.scopus.com/
3Thereadermaynoticethatthisqueryretrievesalsodocumentsabout“SystematicLiteratureReviews”.
4SemanticScholar-https://www.semanticscholar.org/
5andthepaperscitingthem.Thisprocessledtotheidentificationof584referencesfrom
the seed papers and their 8,009 references. Upon further examination of these papers,
we found no additional relevant papers. The lack of new papers identified through
the snowballing process suggests that our collection of tools is both exhaustive and
comprehensive.
Fig. 1: PRISMA Diagram of our SLR about AI-enhanced SLR Tools.
In conclusion, this process led to a collection of 17 tools with associated papers
and 8 without (Covidence, DistillerSR, Nested Knowledge, pitts.ai, Iris.ai, LaserAI,
DRAGON/litstream, Giotto Compliance). We then excluded Giotto Compliance,
DRAGON/litstream, and LaserAI from our study due to the lack of available
information5. We also consolidated RobotReviewer and RobotSearch into a single
entry, recognising their shared algorithmic basis. Consequently, the final set of tools
considered in our study comprised 21 distinct tools.
Table 1 lists the 21 selected tools. The majority of them (17) were primarily
designed for the purpose of screening. Two tools, namely Dextr and ExaCT, focused
exclusivelyonextraction.Theremainingtwotools,Iris.aiandRobotReviewer/Robot-
Search,hadadualfocusonbothscreeningandextraction.Insummary,19toolscanbe
5Specifically,therewerenoassociatedresearchpapersorcomprehensivedocumentationforthesetools,
andattemptstocontactthedevelopersforfurtherdetailswereunsuccessful.
6Table 1: The 21 SLR tools analysed in this survey.
ID Tool Stage SLR Mode Reference
1 Abstrackr Screening Web [59]
2 Colandr Screening Web [60,61]
3 Covidence Screening Web -
4 EPPI-Reviewer Screening Web [62,63]
5 Rayyan Screening Web [64]
6 SWIFT-ActiveScreener Screening Web [65]
7 DistillerSR Screening Web -
8 SWIFT-Review Screening Desktop [66]
9 SysRev Screening Web [67]
10 NestedKnowledge Screening Web -
11 RobotAnalyst Screening Web [68]
12 LitSuggest Screening Web [69]
13 Pitts.ai Screening Web -
14 ResearchScreener Screening Web [70]
15 ASReview Screening Desktop [71]
16 PICOPortal Screening Web [72,73]
17 FAST2 Screening Web [74]
18 Dextr Extraction Web [45]
19 ExaCT Extraction Web [43]
20 Iris.ai Both Web -
21 RobotReviewer/RobotSearch Both Web [42,75]
used for the screening phase and 4 tools for the extraction phase. The majority of the
tools (19) are web applications, while only 2, namely SWIFT-Review and ASReview,
need to be installed locally.
3 Meta-review of previous surveys
This section provides a brief meta-analysis of how the previous systematic literature
reviewshavedescribedthetoolsinrelationtoArtificialIntelligence.Wefocusonfour
surveys that analysed AI features [17, 20–22].
The previous survey papers characterised AI according to five main features:
1. Approach - identifies the implemented method/technique for performing
a certain task. This is the most examined feature, receiving attention from
four studies [17, 20–22].
2. Text representation - describes the processes employed to convert text
into suitable input for the algorithm (e.g., BoW [76], LDA topics [77], word
embeddings[78]).Thisfeaturewasanalysedbytwoprevioussurveys[20,21].
3. Human interaction - specifies how users engage with a tool, detailing the
operations and options available to them, as well as the characteristics of
the user interface. This is among the least explored features with just one
previous study [20].
4. Input-specifiesthetypeofcontent(full-textorjusttitleandabstract)the
tool will need to train its model. Alongside Human interaction, this is the
least explored feature, with just one previous study [20].
5. Output - represents the outcome generated by the trained algorithm, and
it has been analysed in three studies [20–22].
Table 2 summarises the analysis of the four systematic literature reviews and
shows how 17 tools have been reviewed according to the five AI features. Only three
7Table 2: Analysis of SLR tools based on AI features, as con-
ducted by previous surveys [17, 20–22]. The tools are listed in
alphabeticalorder,withthereviewsconductingtheanalysiscited
in the final column.
ID Tool Papers
1 Abstrackr Y Y Y Y Y [17,20,21]
2 ASReview Y Y N N Y [21,22]
3 Colandr Y Y N N Y [17,21]
4 Covidence Y Y N N Y [17,21]
5 DistillerSR Y Y N N Y [17,21]
6 EPPI-Reviewer Y Y Y Y Y [17,20,21]
7 FASTREAD Y Y Y Y Y [20,21]
8 GiottoCompliance Y N N N N [17]
9 NestedKnowledge Y N N N N [17]
10 PICOPortal Y N N N N [17]
11 Rayyan Y Y N N Y [17,21,22]
12 ResearchScreener Y Y N N Y [21]
13 RobotAnalyst Y Y N N Y [17,21]
14 RobotReviewer/RobotSearch Y Y N N Y [17,21]
15 SWIFT-ActiveScreener Y Y N N Y [17,21]
16 SWIFT-Review Y Y N N Y [21,22]
17 SysRev Y N N N N [17]
Note:Y=Yes,N=No.
tools (FASTRED, EPPI-Reviewer, and Abstractr) were actually assessed according
to all five AI features. For ten tools (ASReview, Colandr, Covidence, DistillerSR,
Rayyan, Research Screener, RobotAnalyst, RobotReviewer/RobotSearch, SWIFT-
Active Screener, and SWIFT-Review) only three features named approach, text
representation, and output have been assessed. The remaining tools were assessed by
using only one feature (approach).
Upon examining the four survey papers, it is apparent that there is a limited
exploration of AI features. De la Torre-L´opez et al. [20] provide the most comprehen-
sive analysis, utilising all five specified features to examine seven tools. In contrast,
Burgard et al. [21] employed only three features: text representation, approach, and
output. Robledo et al. [22] focused on just two features: approach and output. Lastly,
Cowies et al. [17] conducted the most restricted analysis, considering only one feature
(approach).Furthermore,thefivereportedfeaturesonlyofferanarrowperspectiveon
how AI can support systematic literature reviews.
In summary, the previous systematic reviews offer a rather limited analysis of the
expanding ecosystem of AI-based SLR tools and their characteristics. In the next
section, we will address this gap by introducing an updated array of 11 AI features.
8
hcaorppA
noitatneserpeR
txeT
noitcaretnI
namuH
tupnI
tuptuO4 Survey of SLR Tools
Weanalysedthe21SLRtoolsaccordingto34features(11AI-specificand23general)
byexaminingtherelevantliterature(seeTable1),theirofficialwebsites,andtheonline
tutorials. When necessary, we sought additional information by reaching out to the
developers through email or online interviews.
Section4.1describesthefullsetoffeatures,payingparticularattentiontothenew
AI features that we first introduced for this survey. Section 4.2 reports the results
of the review. Section 4.3 discusses the most suitable systems for specific use cases.
Finally, Section 4.4 outlines the limitations of our analysis.
4.1 Features overview
4.1.1 AI features
To analyse the extent of AI usage within SLR tools, we considered a total of eleven
features. This evaluation included the five features previously described in Section 3
(approach, text representation, human interaction, input, and output) along with six
new features unique to this study. These additional features were identified through
a review of the relevant literature [17, 20–22] and a preliminary analysis of the tools.
The six novel features are as follows:
• SLR Task - which categorises the tasks for which the AI approach is used
(e.g., paper classification, paper clustering, named-entity recognition);
• Minimum requirement-whichreferstotheminimumnumberofrelevant
and irrelevant papers required to effectively train a classifier tasked with
selecting pertinent papers;
• Model execution - which evaluates whether the models operate in real
time (synchronously) or later, typically overnight (asynchronously);
• Research field - which identifies the research domains in which the tools
can be effectively employed;
• Pre-screening support - which specifies the application of AI techniques
to assist users in manually selecting relevant papers, typically by highlight-
ing key terms or grouping similar papers (e.g., topic maps [65] based on
LDA [77], clustering approaches [68]);
• Post-screening support-whichreferstotheapplicationofAItechniques
to conduct a final review of the screened papers (e.g., summarisation [79]).
Five of the eleven features (minimum requirement, model execution, human interac-
tion,pre-screeningsupport,andpost-screeningsupport)areexclusivetothescreening
phase and will not be considered when analysing the extraction phase.
4.1.2 General features
We analysed the non-AI characteristics of SLR tool based on 23 features. We derived
thesefeaturesfrompreviousstudies[17,80–83]afteraprocessofsynthesisandintegra-
tion. Table 3 shows the description of each feature with its category. To facilitate the
9Table 3: Description of the SLR Features.
# SLR Feature Description
1 Multiplatform(F) Abilityofthetooltoberunondifferentplatforms(e.g.,web,desktop).
2 Multipleuserroles(F) Abilityofthetooltoallowtheusertohavedifferentroles(e.g.,reviewer,
admin)withinandbetweenprojects.
3 Multiple user support Abilityofthetooltoallowmultipleuserstoworkonthesameproject.
(F)
4 Projectauditing(F) Abilityofthetooltotrackallthechangesdoneintheproject.
5 Projectprogress(F) Ability of the tool to determine the overall progress of the annotation
withrespecttothetotalnumberofpaperstoannotate.
6 Authentication(F) Abilityofthetooltoauthenticatetheusersinvolvedintheproject.
7 Status of the software Itexaminestheextenttowhichthetoolisactivelymaintainedandhas
(F) astablerelease.
8 Automated full-text Ability of the tool to support full-text retrieval from bibliographic
retrieval(R) databases.
9 Automatedsearch(R) Abilityofthetooltosupportliteraturesearchthroughtheintegration
ofAPIs.
10 Snowballing(R) Ability of the tool to support the automated retrieval of the citations
frombibliographicdatabases(snowballing).
11 Manual reference Abilityofthetooltoallowtheusertoenterpapersmanually,typically
importing(R) viaaform.
12 Manually inserting full- Abilityofthetooltoallowtheusertomanuallyaddfull-textpapers.
text(R)
13 Referenceimporting(R) Abilityofthetooltoimportpapersusingavarietyofformats(BibTeX,
RIS,CSV).
14 Deduplication(Di) Abilityofthetooltosupporttheautomaticdeduplicationoftherefer-
ences.
15 Discrepancy resolving Ability of the tool to handle differences of opinion between screeners,
(Di) e.g., by allowing comments or assigning the problematic papers to a
seniorscreener.
16 In-/excluding references Abilityofthetooltoallowtheusertocommentonreferenceinclusion
(Di) andexclusion.
17 Reference labelling & Ability of the tool to allow the user to write additional comments on
comments(Di) thereferences(e.g.,’todoublecheck’).
18 Screeningphases(Di) Ability of the tool to allow the user to perform the different stages
screeningphase.
19 Exportingresults(Do) Abilityofthetooltoallowexportingthescreenedreferences.
20 Flow diagram creation AbilityofthetooltoprovidethePRISMAdiagramoftheSLRprocess.
(Do)
21 Protocol(Do) Ability of the tool to provide the user with pre-defined protocol tem-
plates(e.g.,Cochraneguidelines).
22 Living/updatable(L) Ability of the tool to update the screened references by automatically
includingrecentandrelevantarticles.
23 Freetouse(E) Itdetermineswhetherthetoolisavailableforfreeorrequirespayment.
systematic analysis, we grouped them into six categories: Functionality (F), Retrieval
(R), Discovery (Di), Documentation (Do), Living Review (L), and Economic (E).
Thefunctionality categoryincludesfeaturesforauditingandevaluatingthetechni-
calaspectsofthetools.Theretrieval categorycoversfeaturesrelatedtotheacquisition
andinclusionofscholarlydocuments.Thediscovery categoryconsistsoffeaturesthat
facilitate the inclusion, exclusion, and management of references during the screening
phase. The documentation category encompasses features that support the reporting
of the findings. The living review category captures the ability of tools to incorpo-
rate new relevant documents based on AI techniques. Lastly, the economic category
reflects the financial considerations associated with the tools.
104.2 Results
In this section, we present the results of our analysis. Section 4.2.1 describes the tools
for the screening phase through the AI features. Section 4.2.2 presents the tools for
the extraction phase also through the AI features. Finally, Section 4.2.3 describes the
full set of 21 SLR tools according to the general features.
4.2.1 The Role of AI in the Screening Phase
As reported in Table 1, 19 tools use AI for the screening phase. In the following, we
analyse them according to the 11 AI features. To eliminate repetition, this discussion
combines the features input and text representation into the category Input Data and
Text Representation. Moreover, the output feature is discussed within the context of
the SLR Task, as it is contingent upon the specific task requirements. The Table in
Appendix A.1 reports detailed information on how each of the 19 tools addresses the
11 AI features.
Research field. Twelve tools can be employed across a diverse range of research
fields.EPPI-Reviewer,PICOPortal,andCovidencesupportbothageneralmodeanda
settingfocusedonbiomedicalstudies.Pitts.ai,RobotReviewer/RobotSearch,SWIFT-
Review, and LitSuggest exclusively focus on the biomedical domain.
SLR Task.Fifteentoolsutiliseartificialintelligenceforonlyonetask,mostoften
to classify papers as relevant/irrelevant. The other four tools (Covidence, PICOPor-
tal, and EPPI-Reviewer, Colandr) undertake two AI-related tasks. They all classify
papers as relevant/irrelevant, but also execute an additional task, such as identify-
ing a specific type of paper (e.g., economic evaluation, randomised controlled trials,
etc.) or categorising papers according to a set of entities defined by the user. For the
sake of clarity, in our discussion of the subsequent features, we will systematically
address the first group (one task) followed by the second group (two tasks). In the
first group, twelve tools focus on selecting relevant papers given a set of seed papers.
Typically,eachpaperisassignedaninclusionprobabilityscore,usuallyrangingfrom0
to 1. Of the remaining three, two of them (Pitts.ai and RobotReviewer/RobotSearch)
identify RCTs based on a pre-built classification model, while the third (Iris.ai) clus-
ters similar papers to build topic maps that assist users in selecting the relevant
papers. In the second group, all four systems classify pertinent papers using a set of
seed papers as a reference. However, they vary in their secondary AI-driven tasks.
Specifically,CovidenceandPICOPortalidentifyRCTsusingapredefinedclassification
model. EPPI-Reviewer can identify various types of studies, including RCTs, system-
aticreviews,economicevaluations,andCOVID-19relatedstudies.Finally,Colandr,in
additiontothestandardidentificationofrelevantpapers,enablesuserstodefinetheir
own set of categories (e.g., “water management”) and subsequently performs a multi-
label classification of articles based on them [60]. It also maps individual sentences to
the user-defined categories and provides a confidence score for each classification.
AI Approach. In the group of tools performing one task, twelve tools focus
exclusively on categorising relevant papers based on seed papers employed various
types of machine learning classifiers. The most adopted approach is Support Vector
Machine (SVM), which aligns with the findings of prior studies [84]. Four of the tools
11(Abstractr, FAST2, Rayyan, RobotAnalyst) exclusively rely on SVM. Distiller sup-
ports both SVM and Naive Bayes. ASReview allows the user to select a vast range
of methods, including Logistic Regression, Random Forest, Naive Bayes, SVM, and
a Neural Networks classifier. Litsuggest use logistic regression, while SWIFT-Review
and SWIFT-Active Screener use a method based on log-lineal regression. Pitts.ai and
RobotReviewer/RobotSearchusealsoaSVMforidentifyingRCTs[42].Finally,Iris.ai
identifiesandgroupssimilarpapersbasedonthesimilarityoftheir‘fingerprint’,avec-
tor representation of the most meaningful words and their synonyms extracted from
the abstract [85].
With regards to the four tools that perform two AI tasks, Covidence, EPPI-
Reviewer, and PICOPortal also identify relevant papers by using a SVM classifier. In
contrast, Colandr employs a method where it identifies papers by searching for key-
words that are related to a set of user-defined search terms [60]. For instance, it can
recognise terms commonly associated with ‘artificial intelligence’ and select papers
containingtheseterms.Covidencealsoimplementsamachinelearningclassifierbased
on SVM with a fixed threshold for the identification of RCTs following the Cochrane
guidelines[86].EPPI-Reviewerutilisesarangeofproprietaryclassifierstrainedonvar-
ious databases to identify papers with distinct characteristics6. It uses the Cochrane
Randomised Controlled Trial classifier [86] to categorise records into two groups:
those unlikely to be randomised trials and those that potentially are. It employs a
classifier trained with the NHS Economic Evaluation Database (NHS EED) [87] for
identifying economic evaluations and another trained on the Database of Abstracts of
Reviews of Effects [88] to identify systematic reviews in the biomedical field. Finally,
it uses a classifier trained on the ‘Surveillance and disease data on COVID-19’7 for
identifying research related to COVID. PICOPortal employs instead an ensemble
of machine learning classifiers, which combines both decision trees and neural net-
works[89].Finally,fortheidentificationofthecategoryattributedtothepaperbythe
user,ColandrusedacombinationofNamedEntityRecognitionforextractingentities
relevant to the categories and a classifier based on logistic regression [61].
Input Data and Text representation. The AI techniques employed by these
tools take as input the title, abstract, or full text of papers. All the tools analysed
need only titles and abstracts as input, with the exception of Colandr, which requires
the full text of papers. The tools generate different representations of the papers to
input into the AI models. Specifically, of the 15 tools dedicated to classifying relevant
papers, the majority (8 out of 15) use a Bag of Words (BoW) approach [76], while
theremainderemployvariouswordembeddingtechniques[78].Pitts.aiandRobotRe-
viewer/RobotSearch use SciBERT embeddings [90]. Research Screener employs the
doc2vec embeddings [91]. ASReview offers multiple embedding options, including
Sentence-BERT [92] and doc2vec [91]. Iris utilises a unique representation called fin-
gerprint [85], which is a vector characterising the most meaningful words and their
synonyms extracted from the abstract. In the second group, Covidence and EPPI-
Reviewer adopt a BoW representation, while PICOPortal uses both BoW and the
BioBERT embeddings [93]. Finally, Colandr uses both word2vec [94] and GloVe [95]
6EPPI-ReviewerDocumentation-https://eppi.ioe.ac.uk/cms/Default.aspx?tabid=3772
7COVID surveillance - https://eppi.ioe.ac.uk/cms/Projects/DepartmentofHealthandSocialCare/
Publishedreviews(bydate)/COVID-19Livingsystematicmapoftheevidence/tabid/3765/Default.aspx
12(a) ASReview (b) RobotAnalyst
Fig. 2: Examples of interfaces for paper classification.
embeddings. Overall, much like other NLP applications, these tools are evolving from
traditionaltextrepresentationslikeBoWtoarangeofmoremodernwordandsentence
embeddings.
Human Interaction. We identified three main types of interfaces. The first and
most typical one, implemented by 16 tools, regards the classification of paper as rel-
evant. These graphical interfaces typically feature similar templates that allow users
to upload and examine papers. Some tools (Rayyan, SWIFT-Active Screener, Sys-
Rev,Covidence,andPICOPortal)alsoofferamenuwithadditionalfunctionalitieslike
ranking or filtering papers based on specific criteria. A few tools (Rayyan, SWIFT-
Active Screener, Research Screener, pitts.ai, SysRev, Covidence, PICOPortal) also
enable multiple users to collaboratively perform this task, allowing them to add com-
ments for discussion about problematic papers or to delegate challenging papers to a
senior reviewer. For illustration, Figure 2 (a) and Figure 2 (b) depict the interfaces
used by ASReview and RobotAnalyst, respectively, for selecting relevant papers. The
ASReview interface enables users to classify papers as either relevant or irrelevant. In
contrast, the RobotAnalyst interface provides options for users to categorise papers
as included, excluded, or undecided.
The second interface type, offered by Colandr, enables users to define specific cat-
egories to assign to the papers. This approach offers greater flexibility compared to
thetraditionalbinaryclassificationofrelevantornotrelevantpapers.Forinstance,in
Figure 3 Colandr suggests that for the given paper, the shown sentences are classified
with a confidence level of high, medium or low in the category “land/water manage-
ment”previouslydefinedbytheuser.Theusercanaccept,skiporrejectthesuggested
classification.
The third interface type, offered by Iris.ai, is based on a topic map, a visualization
technique that clusters papers based on thematic similarities. The user initiates the
searchprocesswithabriefdescriptionoftheuser’ssearchintent(typically300to500
words),atitle,oranabstractofapaper.Thesystemthenclustersthepapersaccording
to their topics and generates a topic map, such as the ones depicted in Figure 4 (c).
These interactive visualisations enable users to effectively navigate and select papers
13Fig. 3: Examples of the tagging process of Colandr. This figure is courtesy of [96].
relevant to their research. The user can iteratively repeat the clustering process until
they are satisfied that all pertinent papers have been incorporated into the analysis.
Iris.ai also enables users to further filter the papers according to a variety of facets.
Minimum requirements. Generally, the accuracy of a classifier improves with
an increasing number of annotated papers, but this also increases the time and effort
required from researchers. Most methods need between 1-15 relevant papers and typ-
ically the same number of irrelevant ones. This is a relatively low number that should
allow researchers to quickly annotate the initial set of seed papers. However, the
necessary quantity varies a lot among tools. For instance, ASReview, SWIFT-Active
Screener, and SWIFT-Review require just one relevant and one irrelevant paper to
begin classification. Covidence and Rayyan require two and five papers, respectively.
Other tools have stricter minimum requirements. For example, Colandr requires 10
papers, while SysRev demands 30.
Model execution. Thirteen tools employ a real-time model execution strat-
egy, wherein the training and classification of the model occur immediately after the
user selects the relevant and irrelevant paper. Conversely, SysRev and SWIFT-Active
Screener adopt a delayed-model-execution approach in which the training and classi-
fication steps are conducted at predetermined intervals. Specifically, SysRev executes
these operations overnight, whereas SWIFT-Active Screener updates its model after
every thirty papers, maintaining a minimum two-minute interval between the most
recent and the currently used model.
Pre-screening support. Among the 19 tools evaluated, eight implement only
simple techniques for pre-screening support, such as keyword search, boolean search,
and tag search. ASReview, Covidence, DistillerSR, and SWIFT-Active Screener only
enable the user to filter the paper by keyword. Rayyan and EPPI-Reviewer enhance
this functionality by highlighting keywords in their visual interface. Additionally,
Colandr and Abstrackr offer the feature of colour-coding keywords based on their rel-
evance level. Rayyan incorporates abooleansearchfeature, allowingusers tocombine
keywordswithoperatorslikeAND,OR,andNOT.Forexample,abooleansearchsuch
as “literature review” AND “tools” will retrieve scholarly documents containing both
14keywordsintheirtitlesorabstracts.Rayyanalsoprovidesoptionstosearchbyauthor
or publication year. EPPI-Reviewer, on the other hand, offers a tag search function,
whereuserscantagpaperswithspecifickeywordsandthensearchbasedonthesetags.
Five tools adopt more advanced techniques for pre-screening support. Specifically,
RobotAnalyst, SWIFT-Review, and Iris.ai use topic modelling. The first two use
LDA [77], which probabilistically assigns a topic to a paper based on the most recur-
rent terms shared by other papers. RobotAnalyst presents the topics in a network, as
showninFigure4(a)inwhicheachnode(circle)representsatopic,anditssizeispro-
portionaltothefrequencyofthetermsthatbelongtoit.SWIFT-Reviewusesasimpler
approach displaying the topics and their terms in a bar chart, as shown in Figure 4
(b). Iris.ai clusters the papers according to a two-level taxonomy of global topics and
specifictopics.Forinstance,inFigure4(c)wecanobserveasetofglobaltopicsinthe
background,whichinclude‘companion’,‘labour’,‘provider’,and‘woman’.Whereasin
the cyan section there are the second-level specific topics, in this case concerning the
‘labor’globaltopic,suchas‘woman’,‘companion’,‘market’,‘management’,and‘care’.
Ontheotherhand,NestedKnowledge,PICOPortal,andRobotReviewer/RobotSearch
providePICOidentification,whichusesdistinctcolourstohighlightthepatient/popu-
lation, intervention, comparison, and outcome, based on a pre-trained model. Finally,
RobotAnalyst offers a cluster-based search feature. This feature employs a spectral
clustering algorithm [97] to group papers. It also incorporates a statistical selection
process for identifying the key terms characterising each cluster [98]. The resulting
clusters are presented to the user, emphasising the most representative terms.
Post-screening support. Only two tools offer support for post-screening: Iris.ai
and Nested Knowledge. Specifically, Iris.ai generates summaries from either a single
document, multiple abstracts, or multiple documents. It employs an abstractive sum-
marisation technique [79], where the summary is formed by generating new sentences
that encapsulate the core information of the original text. The system also provides
users with the flexibility to adjust the length of the summary, ranging from a brief
two-sentence overview to a more comprehensive one-page summary. Nested Knowl-
edge allows users to create a hierarchy of user-defined tags that can be associated
with the documents. For instance, in Figure 5 (a), Mean Diastolic blood pressure was
definedasasub-tagofPatientCharacteristics.Theusercanalsovisualisetheresulting
taxonomy as a radial tree chart, as shown in Figure 5 (b).
4.2.2 The Role of AI in the Extraction Phase
In this section, we describe the four tools that support the extraction phase (Dextr,
ExaCT, Iris.ai, and RobotReviewer/RobotSearch) with a focus on the six AI features
relevanttotheextractionphase.WeapplythesamefeaturegroupingofSection4.2.1.
The Table in Appendix A.2 reports detailed information on how each of the 4 tools
addresses the relevant features.
Research Field. RobotReviewer/RobotSearch and ExaCT focus on the medical
field, whereas Dextr covers environmental health science. In contrast, Iris.ai can be
employed across various research domains.
15(a) RobotAnalyst: Topic Modelling.
(b) SWIFT-Review: Topic Modelling.
(c) Iris.ai: Specific Topics.
Fig. 4: Examples of interactive interfaces for pre-screening.
16(a) Nested Knowledge: Hierarchical Ontol-
ogy (b)NestedKnowledge:QualitativeSynthesis
Fig. 5: Examples of interactive interface for the post-screening.
SLR Task. ExaCT, Dextr, and Iris.ai perform Named Entity Recognition
(NER) [99] to extract various types of information from the relevant articles. Specifi-
cally,ExaCTidentifiesRCTentitiesbasedontheCONSORTstatement[44].Itreturns
the top five supporting sentences for each extracted RCT entity, ranked according to
relevance. Dextr detects data entities used in environmental health experimental ani-
mal studies (e.g., species, strain) [45]. Finally, Iris.ai allows users to customise entity
extraction by defining their own set of categories and associating them with a set of
exemplarypapers.ThisisdonebyfillinginaformcalledOutputDataLayout(ODL),
which is essentially a spreadsheet detailing all the entities that need to be extracted.
Finally,RobotReviewer/RobotSearchcategorisesbiomedicalarticlesaccordingtotheir
assessed risk of bias and provides sentences that support these evaluations.
AI Approach. The tools perform the NER tasks with a variety of algorithms.
ExaCTappliesatwo-stepapproach[43].First,itidentifiessentencesthatarepredicted
tobesimilartothoseinthepre-trainedmodel,usingaSVMclassifier.Next,itextracts
from these sentences a set of entities via a rule-based approach, relying on the 21
CONSORT categories [44]. Dextr employs a Bidirectional Long Short-Term Memory
- Conditional Random Field (BI-LSTM-CRF) neural network architecture [45, 100].
Iris.ai does not share specific information about the method used for NER. Finally,
RobotReviewer/RobotSearchemploysanensembleclassifier,combiningmultipleCNN
models [101] and soft-margin Support Vector Machines [102] in order to categorise
articlesbasedontheirriskofbiasassessment(eitherloworhigh/unclear)andconcur-
rently extract sentences that substantiate these judgements. The final score for each
predicted sentence is the average of the scores obtained from each model.
Input Data and Text representation. The majority of the models accept the
full-text document as input, except for Dextr, which utilises only titles and abstracts.
The format requirements vary across these tools. Dextr, Iris.ai, and RobotReview-
er/RobotSearch, Iris.ai process papers in PDF format. Dextr also supports input in
RIS or EndNote format. ExaCT encodes papers as HTML. The methods for text
17representation also differ among tools. Dextr encodes text using two pre-trained
embeddings: GloVe [95] (Global Vectors for Words Representations) and ELMo [103]
(Embeddings from Language Models). Iris.ai utilises the same fingerprint representa-
tion[85]discussedinSection4.2.1.ExaCTusesasimpleBoWrepresentation.Finally,
RobotReviewer/RobotSearch uses BoW for the linear model and an embedding layer
for the CNN model.
4.2.3 General features
Table4providesanoverviewoftheproportionoftoolscoveringeachofthe23features.
These features are categorised across the six categories outlined in Section 4.1.2. The
Table in Appendix A.3 provides a more general analysis, detailing how the 21 tools
address the 23 features.
The functionality category exhibits the highest degree of implementation, with 5
outof7featuresbeingeffectivelyexecutedbyallthetools.Theremainingtwofeatures,
namely authentication and project auditing, respectively demonstrate an intermedi-
ate (18/21) and lower (9/21) level of implementation. The other categories present a
more heterogeneous scenario. Within the retrieval category, only reference importing
is implemented by all tools. Interestingly, no tools provide the ability to automati-
cally retrieve the reference of a paper from bibliographic databases. The tools also
offer limited support for the feature within the discovery category. Notably, approx-
imately 50% of the tools lack basic functionalities such as reference deduplication,
options for manual annotation and exclusion of references, and features for labelling
and commenting on the references. Regarding the documentation category, only 4
tools (DistillerSR, Nested Knowledge, Rayyan, and Covidence) provide the PRISMA
diagramof theentireSLR process orthe protocoltemplates. Significantly,LitSuggest
stands out as the sole tool providing a living review, that is it enables users to easily
updatetheirearlieranalysesbyautomaticallyaddingrecentpapersthatexhibitahigh
degreeofsimilaritytothepreviouslyselectedones.Intermsofeconomic)aspects,the
majority of the tools (13 out of 21) are accessible for free.
In summary, only eight of the evaluated tools implement at least 70% of the des-
ignated features. Specifically, DistillerSR, Nested Knowledge, Dextr, and ExaCT lead
with the highest feature coverage at 82%. They are followed by PICOPortal and
Rayyan, each with 78%, EPPI-Reviewer with 74%, and SWIFT-Active Screener with
70%.Amongtheremaining13tools,eightcoverbetween50%and70%ofthefeatures,
while the last five cover between 35% and 50%.
4.3 Outstanding SLR tools
Comparing our results with the previous studies in the literature [17, 80, 82, 83], we
observed that many SLR tools have undergone significant development and advance-
mentsinthelastfewyears.Particularly,thefeaturesinthefunctionality categoryhave
received more attention and are now considered standard functions. These include
capabilitiesfortrackingandauditingprojects,multipleusersupport,andmultipleuser
roles.Additionally,themanagementofreferenceshasseenconsiderableenhancement.
18Table 4: Proportion of the 21 tools implementing the 23 generic features.
Category Feature Yes No
Multiplatform 21(100%) 0(0%)
Multipleuserroles 21(100%) 0(0%)
Multipleusersupport 21(100%) 0(0%)
Functionality Projectauditing 9(43%) 12(57%)
Projectprogress 21(100%) 0(0%)
Authentication 18(86%) 3(14%)
Statusofsoftware 21(100%) 0(0%)
Automatedfull-textretrieval 3(16%) 16(84%)
Automatedsearch 8(42%) 11(58%)
Snowballing 0(0%) 19(100%)
Retrieval
Manualreferenceimporting 5(26%) 14(74%)
Manuallyinsertingfull-text 8(42%) 11(58%)
Referenceimporting 21(100%) 0(0%)
Deduplication 8(42%) 11(58%)
Discrepancyresolving 12(57%) 9(43%)
Discovery In-/excludingreferences 13(68%) 6(32%)
Referencelabelling&comments 10(53%) 9(47%)
Screeningphases 19(100%) 0(0%)
Exportingresults 21(100%) 0(0%)
Documentation Flowdiagramcreation 4(21%) 15(79%)
Protocol 4(21%) 15(79%)
LivingSystematicReview Living/updatable 1(5%) 18(95%)
Economic Freetouse 13(62%) 8(38%)
Asdiscussedintheprevioussection,themorecompletetoolsintermsoffeaturecover-
ageincludeDistillerSR,NestedKnowledge,Dextr,ExaCT,PICOPortal,andRayyan.
However, in practical scenarios, the selection of these tools should be guided by the
user’s specific needs and use cases.
In non-biomedical fields, ASReviewer stands out for its comprehensive range of
methods for selecting relevant articles, including Logistic Regression, Random For-
est, Naive Bayes, and Neural Networks classifiers. This makes it a potentially optimal
choice for this phase of research. Iris.ai and Colandr are also strong contenders that
may enable the greatest flexibility since they allow users to respectively cluster doc-
uments based on their semantic similarity, and create specific categories for paper
classification. Moreover, they offer user-friendly interfaces for analysing the resulting
data. These features may offer significant advantages for certain applications.
In the biomedical field, Covidence, PICOPortal, EPPI-Reviewer, and RobotRe-
viewer/RobotSearch are all reliable tool options. Covidence, PICOPortal, and EPPI-
Reviewer have also the capability to identify Randomised Controlled Trials (RCTs)
using a predefined classification model. Among these, EPPI-Reviewer offers the most
flexibility, since it can be customised to identify a broader range of studies, including
systematic reviews, economic evaluations, and research related to COVID-19. Finally,
RobotReviewer/RobotSearch stands out as the only tool that offers automated bias
19analysis.Thisfeaturemakesitanidealchoiceforresearcherswhorequirethisspecific
functionality.
4.4 Limitations
In this section, we discuss some inherent limitations of our analysis. First, the assess-
ment of the features was limited by the extent of available information. To mitigate
this constraint, a thorough review of relevant research papers, the tools’ official web-
sites,andinstructionalmaterialswasconducted.Incaseswhereinformationwasscarce
or unclear, particularly regarding the detailed technical aspects, we reached out to
the creators of these tools. This outreach was met with a positive response from sev-
eral creators, leading to interviews that enriched our study with additional and more
precise information. Second, our analysis excluded tools that lack a graphical user
interface. This decision was based on the premise that tools with user interfaces are
more likely to be widely adopted and used within the broader scientific community.
Finally, the tools that incorporate AI are often updated and improved. The growing
fieldofgenerativeAIandlargelanguagemodelsareatestamenttothisrapiddevelop-
ment,anditisanticipatedthatnumeroustoolswillintegratethesetechnologiesinthe
near future. Therefore, while our findings offer a snapshot of the current landscape,
they may not fully capture the ongoing advancements in these tools. For this reason,
in Section 6 we will explore various tools based on LLMs that, although they do not
directly correspond to a specific stage of a SLR, can offer substantial assistance to
researchers in conducting literature reviews.
5 Research Challenges
The current generation of SLR tools can demonstrate significant effectiveness when
utilised properly. Nonetheless, these tools still lack crucial abilities, which hampers
their widespread adoption among researchers. This section will discuss some of the
keyresearchchallengesidentifiedfromouranalysisthattheacademiccommunitywill
need to address in future work.
5.1 AI for SLR
Aspreviouslydiscussed,severalSLRtoolsnowincorporateAItechniquesforsupport-
inginparticularthescreeningandextractionphases.However,currentapproachesstill
suffer from several limitations. Consistent with prior research [21, 81, 84], our study
revealsthatthemajorityofSLRtoolsstilldependonpossiblyoutdatedmethodologies.
Thisincludestheuseofbasicclassifiers,whicharenolongerconsideredstate-of-the-art
for text and document classification. Likewise, several tools continue to employ BoW
methods for text representation, although some of the most recent ones [42, 45, 71]
have shifted towards adopting word and sentence embedding techniques, such as
GloVe [95], ELMo [103], SciBERT [90], and Sentence-BERT [92]. Therefore, the first
interesting research direction regards incorporating advanced NLP technologies, par-
ticularly the rapidly evolving Large Language Models (LLMs) [104]. LLMs represent
the state of the art for many NLP tasks and demonstrated remarkable proficiency in
20classifying and extracting information from documents [105, 106]. However, integrat-
ingthesemodelspresentsseveralchallenges[107].Firstly,LLMsaretrainedongeneral
data, resulting in less effective performance in specialised fields and languages with
fewer resources. Secondly, LLMs may generate inaccurate or fabricated information,
known as “hallucinations”. Additionally, understanding the decision-making process
ofLLMsiscomplex,andtheiroutputscanbeinconsistent.Apossiblesolutiontothese
issuesistheintegrationofLLMswithdifferenttypesofknowledgebasesthatcanpro-
vide verifiable factual information [108]. For example, the recent CORE-GPT [109]
utilises a vast database of research articles to assist GPT3 [110] and GPT4 [111] in
generating accurate answers. In addition, the extraction phase in particular could be
enhancedbyalsoincorporatingmoderninformationextractionmethodssuchasevent
extraction [112], open information extraction [113], and relation prediction [114].
A second interesting research direction regards interpretability. Indeed, current
classification methods for the screening phase typically operate as ‘black boxes’, not
giving much additional information on why a certain paper was deemed as relevant.
One important research challenge here is to improve this step by including inter-
pretabilitymechanismssuchasfact-checking[115]orargumentmining[116]toprovide
furtherinsights.Suchtechniqueswouldprovidedeeperinsightsintothescreeningpro-
cess, enhancing the reliability and credibility of the tools. In the field of explainable
AI [117], significant research has been conducted to improve our understanding of the
processesmodelsusetogeneratespecificoutputs.Specifically,inthecontextofLLMs,
various prompting techniques have been developed to enhance the models’ ability to
explain their reasoning and justify their decisions. These techniques include Chain-
of-Thought (CoT) [118], Tree of Thoughts (ToT) [119, 120] and Graph of Thoughts
(GoT) [121].
Athirdpromisingresearchdirectioninvolvestheuseofsemantictechnologies[122],
particularly knowledge graphs, to enhance the characterisation and classification of
research papers [123]. Knowledge graphs consist of large networks of entities and
relationships that provide machine-readable and understandable information about a
specific domain following formal semantics [124]. They typically organise information
accordingtoadomainontology,whichprovidesaformaliseddescriptionofentitytypes
and their relationships [125]. In recent years, we saw the emergence of several knowl-
edge graphs that offer machine-readable, semantically rich, interlinked descriptions
of the content of research publications [126–128]. For instance, the latest iteration of
the Computer Science Knowledge Graph (CS-KG)8 details an impressive array of 24
million methods, tasks, materials, and metrics automatically extracted from approx-
imately 14.5 million scientific articles [129]. Similarly, the Open Research Knowledge
Graph (ORKG)9 provides a structured framework for describing research articles,
facilitating easier discovery and comparison [126]. This knowledge graph currently
encompassesaround10,000articles,4,500researchproblems,and3,300datasets.Ina
similar vein, Nanopublications10 allow the representation of scientific facts as knowl-
edgegraphs[130].Thismethodhasbeenrecentlyappliedtosupport“livingliterature
reviews”, which can be dynamically updated with new findings [128]. The integration
8ComputerScienceKnowledgeGraph-http://w3id.org/cskg/
9https://www.orkg.org/
10https://nanopub.org/
21of these knowledge bases offers significant possibilities. It allows for a more detailed
and multifaceted analysis of document similarity, and aids in identifying documents
related to specific concepts. For instance, it would enable the retrieval of articles that
mention particular technologies or that utilise specific materials.
Other SLR phases, such as appraisal and synthesis, received relatively little atten-
tion. This gap offers a substantial research opportunity for the application of AI
techniques in these areas. In the appraisal phase, incorporating AI-driven scientific
fact-checking tools to evaluate the accuracy of research claims could provide signifi-
cantbenefits[115].Forthesynthesisphase,theuseofsummarisationtechniques[131]
and text simplification methods [132] has the potential to enhance both the efficiency
of the analysis and the clarity of the final output.
Finally, we recommend that the research community participates to scientific
events and initiatives in this field, such as ICASR11 [133–136], ALTAR12 [137], and
the MSLR Shared Task13 [138]. These initiatives are focused on discovering the most
effective ways in which AI can improve the SLR stages.
5.2 Evaluation and Benchmarks
The assessment of SLR tools presents a significant challenge due to the absence of
established frameworks and benchmarks for comprehensive quality evaluation. Exist-
ingliteratureincludesvariousevaluationsofSLRtoolsthatfocusonindividualphases
oftheSLRprocess[21,139,140].However,theseevaluationsarenotcomparabledueto
the absence of standard datasets and evaluation frameworks. Furthermore, most SLR
tools are evaluated using small, custom datasets [21], which, although informative,
may not accurately reflect real-world usage scenarios.
Another concern is related to the performance metrics. Indeed, canonical metrics
like precision, recall, and F1-score may not suffice to assess these tools. For instance,
forthescreeningphase,itiscriticaltominimisethecostsofscreeningwhilepreserving
a high recall. For this reason, it was suggested to use F2 score [141] over the F1 score.
The F2 score is computed as the weighted harmonic mean of precision and recall. In
contrastwiththeF1score,whichassignsequalimportancetoprecisionandrecall,the
F2scoreplacesgreateremphasisonrecallcomparedtoprecision.TheWorkSavedover
Sampling(WSS)[71],isanothermetricthatprovedtobequiteeffectiveinassessingthe
screening phase [21]. However, Kusa et al. [142] point out that this measure depends
on the number of documents and the proportion of relevant documents in a dataset,
making it difficult to compare the performance of different screening tasks performed
over different systematic reviews. To address this, they introduced the Normalised
Work Saved over Sampling (nWSS) metric [143], which facilitates the comparison of
paper screening performance across various datasets.
In summary, while advancements have been made in this field, there remains a
significant need for further research, especially in the creation of robust benchmarks
and evaluation frameworks that would allow the scientific community to objectively
compare different tools and methodologies.
11InternationalCollaborationfortheAutomationofSystematicReviews(https://icasr.github.io/)
12AugmentedIntelligenceforTechnology-AssistedReviewsSystems(https://altars2022.dei.unipd.it/)
13MultidocumentSummarisationforLiteratureReview(https://github.com/allenai/mslr-shared-task)
225.3 Usability
The current generation of SLR tools remains underutilised [144]. Most researchers
continue to depend on manual methods, often supported by software like Microsoft
Excel, or reference management tools [145] such as Zotero14 and Mendeley15. Recent
studies [146], suggest that this limited usage primarily stems from usability issues, in
additiontoseveralotherrelevantfactors:i)steeplearningcurve,asresearchersmaybe
unfamiliarwiththetools’functionalities[147],ii)misalignmentwithuserrequirements,
as many of these software deviate from the guidelines set forth by SLR protocols and
exhibit limited compatibility with other software systems [148, 149], iii) distrust, as
thereisuncertaintyaboutthereliabilityandthemechanismsofthesetools[150,151],
and iv) financial obstacles, predominantly arising from licensing expenses, along with
feature restrictions in trial versions [152]. This suggests that usability and accessibil-
ity should be prioritised in the design process to encourage wider adoption of these
tools [153–155].
TheliteraturehasgivenlimitedattentiontotheusabilityofSLRtools.Tothebest
of our knowledge, only a few studies focused on this aspect. For instance, Harrison
et al. [83] conducted an experiment where six researchers were tasked with using six
different tools in trial projects. Findings indicated that two tools also presented in
this study, Rayyan and Covidence, were perceived as easier to use. Van Altena et al.
[146] conducted a survey involving 81 researchers about the usage of SLR tools and
foundthattheprimaryreasonscitedbyparticipantsfordiscontinuingtheuseofatool
included poor usability (43%), insufficient functionality (37%), and incompatibility
withtheirworkflow(37%).Inthesamestudy,asetofSLRtoolswasassessedusingthe
SystemUsabilityScale(SUS)questionnaire[156].Thetoolsachievedsimilarusability,
withusabilityscoresrangingfrom66to77.Thesescoresareequivalenttoa‘B’or‘C’
grade, indicating satisfactory but not excellent performance.
Therefore, a critical challenge in this field lies in the need for more comprehensive
researchfocusedonusability.Thisinvolvesconductingin-depthstudiestounderstand
the various aspects of usability, such as effectiveness, efficiency, engagement, error
tolerance, and ease of learning [157]. The goal is to gather empirical data and user
feedback that can provide insights into how users interact with tools, identify com-
mon usability issues, and understand the specific needs and preferences of different
user groups. Based on these findings, it is essential to develop robust, evidence-
based usability guidelines [158]. These guidelines should offer clear and actionable
recommendations for designing user-friendly interfaces and functionalities in future
tools.
6 Emerging AI Tools for Literature Review
Over the past year, there has been an emergence of a new generation of AI tools
aimed atassisting researchers.This development islargely influencedbythe advance-
ments in Large Language Models [159]. Several leading bibliographic search engines
14Zotero-https://www.zotero.org/
15Mendeley-https://www.mendeley.com/
23are currently introducing LLM technology. For instance, Scopus and Dimensions16
are working on their own chatbot engine and are planning to release them through-
out the 2024 [160, 161]. Similarly, CORE17, a search engine providing access to 280
million papers, has recently presented the prototype CORE-GPT, an enhanced ver-
sion that can answer natural language queries by extracting information from these
documents [109].
These LLM-based tools do not directly support specific SLR phases as the appli-
cations that we reviewed in Section 4. Nevertheless, their functionalities can aid
researchers in conducting literature reviews and are expected to be integrated into
future SLR tools. Therefore, it is advantageous to outline these tools and their
distinctive features.
For the analysis of this emerging generation of tools, we focused on those that
are accessible as online services, excluding those that are only referenced in academic
papersandnotavailableassuch.Toidentifythetools,weutilisedTopAI18,arenowned
searchengineforAIsystems,andsearchedforrelevanttermssuchas“literaturereview
tools”, “search engine”, and “writing assistant”. This process yielded eleven tools in
this domain. Table 5 reports an overview of these tools.
The eleven systems that we identified typically employ LLMs (mostly via the
OpenAI API19) often enhanced with a Retriever-Augmented Generation (RAG)
framework [35] to integrate knowledge from scientific and technical documents. The
RAG framework enhances LLMs by enabling them to interact with and query a col-
lection of documents, incorporating this knowledge into the model’s context. This
allows the LLMs to rely on verifiable information, thereby reducing inaccuracies or
hallucinations in their outputs [107].
Weclassifiedthe11toolsintotwocategories:searchenginesandwritingassistants.
Search engines enable users to enter a query using natural language and provide a
list of related research papers and their summaries. Their main contribution is the
ability to use natural language rather than keywords for searching research papers.
On the other hand, writing assistants accept a description of a document, such as
“Surveypaperaboutknowledgegraphs”,andgeneratepertinenttextthatcanbethen
iteratively refined by a researcher. Seven tools were categorised as search engines and
three as writing assistants. Textero.ai was the only identified tool fitting into both
categories.
6.1 Search Engine Tools
The tools in this category allow users to formulate a natural language query and
generate a list of relevant research papers sourced from online repositories. Generally,
these tools also provide concise summaries of the most prominent papers. Beyond
the natural language query functionality, some tools incorporate additional search
features. For instance, EvidenceHunt allows users to locate papers using keywords,
medical specialisations, or filters specific to PubMed searches. Similarly, Scite offers
the capability to conduct keyword searches in titles and abstracts, and uniquely, to
16Dimensions-https://www.dimensions.ai/
17CORE-https://core.ac.uk/
18TopAI-https://topai.tools/
19OpenAIAPI-https://openai.com/blog/openai-api
24Table 5: Literature Review Tools based on LLMs.
ID Tool Mode Type Website
1 Scite[162–164] Web SearchEngine https://scite.ai/
2 Elicit[165] Web SearchEngine https://elicit.com/
3 Consensus Web SearchEngine https://consensus.app/
4 EvidenceHunt Web SearchEngine https://evidencehunt.com/
5 MirrorThink Web SearchEngine https://mirrorthink.ai/
6 Perplexity Web/App SearchEngine https://www.perplexity.ai/
7 Scispace Web SearchEngine https://typeset.io/
8 Jenni.ai Web/App WritingAssistant https://jenni.ai/
9 ResearchBuddies Web WritingAssistant https://researchbuddy.app/
10 Silatus Web WritingAssistant https://silatus.com/
11 Textero.ai Web Both https://textero.ai/
search for specific terms within ‘citation statements’ [162], i.e., segments of text that
include a citation [166]. Additionally, Scispace and Elicit allow users to automatically
extract information from papers based on predefined categories. For instance, a user
can request the extraction of all references to ‘technologies’ within a text. However,
the quality of the extracted results can vary significantly.
The bibliographic databases employed by these tools differ. Elicit, Consensus,
and Perplexity utilise Semantic Scholar20. EvidenceHunt relies on PubMed21. Scite
sources its content from Semantic Scholar and a broader array of publishers, such
as Wiley, Sage, Europe PMC, Thieme, and Cambridge University Press. The biblio-
graphicdatabasesusedbyScispace,Textero.ai,andMirrorThinkarenotdocumented.
Scite and Consensus process full-text papers, while Elicit and EvidenceHunt only use
titles and abstracts.
The majority of the tools (6 out of 8) are versatile and applicable across different
researchfields.EvidenceHuntisspecificallytailoredforuseinbiomedicine,whileElicit
is designed to cater to both biomedicine and social sciences.
The specific details of the implementation for many of these tools remain undis-
closed, as they are proprietary commercial products. However, it appears that a
majority of them employ the OpenAI API, utilising various prompting strategies and
often integrating a RAG framework [35] to incorporate text from pertinent articles.
Notably, two of the tools explicitly state their models: Elicit and Perplexity; both of
which leverage OpenAI’s GPT technology.
6.2 Writing Assistant Tools
These tools enable the user to describe the document they want to generate and then
iterativelyrefineit.Jenni.aiisahighlyinteractivetoolthatenablescollaborativeedit-
ingbetweentheuserandtheAI.Initially,theuserprovidesastep-by-stepdescription
of the desired text. Subsequently, the system generates a template for the document
and progressively incorporates new sections. These sections can be edited by the user
in real-time, facilitating a dynamic and iterative writing process. Textero.ai operates
20SemanticScholar-https://www.semanticscholar.org/
21PubMed-https://pubmed.ncbi.nlm.nih.gov/
25similarly. Users are required to input the title and description of the text they wish
to create. They can then request the tool to gather pertinent references for integra-
tion and select a citation style, such as MLA or APA. The generated text can be
further refined by the user either manually or through various AI functions designed
to enhance or summarise sections of the text. Additionally, a panel on the right side
provides convenient access to the list of cited references, with each paper accompa-
nied by a brief summary. For this reason, we categorised this tool also as a search
engine. Silatus can operate in four distinct modes: question answering, which gener-
ates a specific answer; research report, producing a comprehensive explanation of a
research topic; blog post, creating content suitable for blogs; and social media post,
tailored for social media platforms. In each mode, the user is prompted to provide
a concise initial prompt to initiate text generation. Optionally, the user can instruct
Silatus to retrieve and integrate pertinent references into the generated text.
As before, most systems do not disclose their technologies, yet they appear to
incorporatedifferentversions of theOpenAI API,augmentedwith specificprompting
techniques.SilatusemploysGPT-4,whileJenni.aiusesacombinationofGPT-3.5and
itsproprietaryAItechnologies.Itremainsunclearwhetheranyofthemhavefine-tuned
their models for writing-related tasks.
The quality of the text produced by these systems varies significantly, even when
using very informative prompts. Presently, these tools may be more beneficial for
master’s students who are required to write brief essays rather than for researchers.
Nonetheless, as the technology continues to evolve, it is anticipated that a new
generation of tools will emerge, offering substantial assistance in academic writing.
These advanced systems could potentially automate even complex tasks, such as the
generation of comprehensive literature reviews.
7 Conclusion
In this survey, we performed an extensive analysis of SLR tools, with a particular
focusontheintegrationofAItechnologiesinthescreeningandextractionphases.Our
study includes a detailed evaluation of 21 tools, examining them across 11 AI-specific
features and 23 general features. The analysis extended to 11 additional applications
thatleverageLLMstoaidresearchersinretrievingresearchpapersandsupportingthe
writingprocess.Throughoutthesurvey,wecriticallydiscussedthestrengthsandweak-
nesses of existing solutions, identifying which tools are most suitable for specific use
cases. We also explored the main research challenges and the emerging opportunities
that AI technologies present in this field.
Our findings paint an exciting picture of the current state of SLR tools. We
observed that the existing generation of tools, when used effectively, can be highly
powerful. However, they often fall short in terms of usability and user-friendliness,
limiting their adoption within the broader research community. Concurrently, a new
generationoftoolsbasedonLLMsisrapidlydeveloping.Whilepromising,thesetools
are still in their infancy and face challenges, such as the well-documented issue of
hallucinations in LLMs [107]. This highlights the need for the research community to
focus on knowledge injection [167] and RAG [35] strategies to ensure the generation
of robust and verifiable information.
26The challenges identified in our survey represent a vibrant and evolving area of
interest for researchers. It is anticipated that in the next five years, we may see the
emergence of a novel generation of AI-enabled research assistants based on LLMs.
TheseAI-enabledresearchassistantscouldsupportresearchersbyperformingavariety
of crucial tasks such as generating comprehensive literature reviews [168], identifying
new scientific hypotheses [169], and fostering crucial innovation in research prac-
tices[170].TheresearchcommunitybearsthecrucialtaskofsteeringthegrowthofAI,
minimising bias, and upholding strict ethical standards [171, 172]. With the AI revo-
lution impacting many fields, it is essential to remember that human critical thinking
and creativity [173] are still vital and remain a core responsibility of the researchers.
Supplementary information. The full tables describing the 21 tools for SLRs
according to the 34 features are available in digital format at: https://angelosalatino.
github.io/ai-slr/.
Acknowledgments. We would like to express our gratitude to the developers
of the following tools for providing additional information via email or in per-
sonal interviews: Covidence, DistillerSR, SWIFT-Reviewer, SWIFT-Active Screener,
SysRev.com, Nested Knowledge, Pitts.ai, and PICOPortal.
Appendix A Systematic Literature Review Tools
analysed through AI and Generic
Features
In this appendix, we report three tables that describe the 21 systematic litera-
ture review tools examined according to both generic and AI-based features. In
Appendix A.1 and Appendix A.2, we present the analysis of the AI features for the
screening and the extraction phases, respectively. In Appendix A.3, we report the
analysisofthetoolsaccordingtothegenericfeatures.Duetospaceconstraints,onlya
summarisedversionofthesetablesisincludedhere.Thefullversionisavailableonline
at https://angelosalatino.github.io/ai-slr/.A.1 Screening Phase of Systematic Literature Review Tools analysed through AI Features
Research
Tool SLR Task Text Representation Input Minimum Requirement
Field
Classification of relevant
Abstrackr Any Bag of words. Title & Abstract -
papers.
Classification of relevant Bag of words. Embeddings: Relevant papers: 1.
ASReview Any Title & Abstract
papers. SentenceBERT, doc2vec. Irrelevant papers: 1.
Task 1: Classification of
Task 1: 10 relevant papers
relevant papers. Task 2: Task 1: Embeddings:
Task 1: Title & Abstract and 10 irrelevant papers.
Colandr Any Identification of the Word2vec. Task 2:
Task 2: Full content Task 2: Minimum 50
category attributed to the Embeddings: Glove
papers.
paper by the user.
Task 1: Classification of
Task 1: 2 relevant papers
relevant papers. Task 2: Bag of words for both Task 1: Title & Abstract
Covidence Any and 2 irrelevant papers.
Identification of biomedical tasks: ngrams. Task 2: Title & Abstract
Task 2: Not Applicable.
studies (RCTs).
Classification of relevant Relevant papers: 10.
DistillerSR Any Bag of words. Title & Abstract
papers. Irrelevant papers: 40.
Task 1: Classification of
Task 1: Bag of words
relevant papers. Task 2:
(ngrams). Task 2: The Task 1: 5 relevant papers.
Identification of biomedical
Cochrane RCT classifer Task 1: Title & Abstract Number of irrelevant papers
EPPI-Reviewer Any studies (RCTs, Systematic
uses bag of words. For the Task 2: Title & Abstract not available. Task 2: Not
Reviews, Economic
other approaches the Applicable
Evaluations, COVID-19
information is not available.
categories, long COVID).
Classification of relevant
FAST2 Any Bag of words. Title & Abstract -
papers.
Iris.ai Any Clustering of Abstracts Embeddings. Title & Abstract Not Applicable
Classification of relevant
LitSuggest Biomedicine Bag of words. Title & Abstract -
papers.
Classification of relevant
Nested Knowledge Any - Title & Abstract -
papers.
Task 1: Classification of
Embeddings for Task 2:
relevant papers. Task 2: Task 1: Title & Abstract
PICOPortal Any BioBERT. No information -
Identification of biomedical Task 2: Title & Abstract
regardin Task 1.
studies (RCTs).
Identification of biomedical
pitts.ai Biomedicine Embeddings: SciBERT Title & Abstract Not Applicable
studies (RCTs).
Classification of relevant Relevant papers: 5.
Rayyan Any Bag of words: ngrams Title & Abstract
papers. Irrelevant papers: 5.
Relevant papers: 1.
Classification of relevant Embeddings: paragraph
Research Screener Any Title & Abstract Irrelevant papers:
papers. embedding
Information not available.
Classification of relevant
RobotAnalyst Any Bag of words. Title & Abstract -
papers.
RobotReview- Identification of biomedical Relevant papers: NA.
Biomedicine Embeddings: SciBERT Title & Abstract
er/RobotSearch studies (RCTs). Irrelevant papers: NA.
SWIFT-Active Classification of relevant Relevant papers: 1.
Any Bag of words. Title & Abstract
Screener papers. Irrelevant papers:1.
Classification of relevant Relevant papers: 1.
SWIFT-Review Biomedicine Bag of words. Title & Abstract
papers. Irrelevant papers:1.
Classification of relevant Relevant papers: 30.
SysRev.com Any - Title & Abstract
papers. Irrelevant papers: 30.A.2 Extraction Phase of Systematic Literature Review Tools analysed through AI Features
Research Text
Tool SLR Task Approach Input Output
Field Representation
ML classifier, combining a lineal model
and a Convolutional Neural Network Bagofword:ngrams.
RobotRe- Identifies risks of
(CNN) model. These models are trained Embeddings: Full-text Risk of bias classification
viewer / Biomedical bias:howreliableare
on a dataset containing manually embedding layer paper. (as Low, High, Unclear)
RobotSearch the results?
annotated sentences stating the level of from CNN Model.
bias.
Task 1: ML classifier based on SVM to
NER of Randomised identify sentences regarding a control Bag of words: Full-text
ExaCT Biomedical Possible RCT entities
Controlled Trials trial. Task 2: Rule base detection to ngrams. paper.
identify the 21 CONSORT categories.
Task 1: ML Classifier implementing a Task 1: Possible animal
Task 1: NER of neural network model based on Task 1: entities. Task 2:
Environmen-
animal studies. Task bidirectional LSTM with a Conditional Embeddings: GloVe, Title and Relationships of animal
Dextr tal Health
2: Entity linking of Random Field (BI-LSTM-CRF) ELMo. Task 2: Not Abstracts models and exposures vs
Science
animal studies. architecture. Task 2: Linking according Applicable. experimentas ot endpoints
to a customised ontology vs experiments.
Task 1: ML classifier. Algorithim is
Task 1: NER of Task 1: Possible entities
unknown. Task 2: Uses a knowledge Task 1:
entities selected by based on a confidence
graph to represent the relations of within Embeddings: word Full-text
Iris.ai Any the user. Task 2: interval. Task 2:
the entities on the paper or between the embedding. Task 2: paper.
Entity linking of the Additional semantics on the
entities of the table. The technical Not Applicable.
identified entities. extracted entities.
implementation is unknown.A.3 Systematic Literature Review Tools analysed based on General Features
nts
Tool
M
ultiple us Mer ulr tiol pe ls e Pru os je er ctsu a Ap u up d tio otr it mn ag t Ae ud tf oul ml- at te ex dt sr ee at rr ci heval
Snow
balli Mn ag nualr Mef ae nr ue an llce y Di i en dm s up e pro lt ir i cti n Da ign t sig cf o ru nl el p-t a Ie n nx c -t /y er xe cs lo ul Rd ev ii fn en rgg er ne cf Fe ler ole a wn bc e dl ie ls ai Pgn rrg oa t& om cc c oro le Lim a vt iim noe gn /u Fp red eat ta obl ue se
Abstrackr Sing. 2 Yes No None No Yes No No Yes No Yes No No No Yes
Colandr Sing. 2 No No None No No No No Yes Yes Yes No Yes No Yes
DistillerSR Mult. >1 Yes Yes PubMed No Yes Yes Yes Yes Yes Yes Yes No No No
EPPI-Reviewer Mult. >1 Yes No PubMed No Yes Yes Yes Yes Yes Yes No No No No
LitSuggest Sing. No No No PubMed No No No No No No No No No Yes Yes
Nested
Mult. >1 Yes Yes PubMed; Europe PMC; DOAJ; ClinicalTrials.gov No No Yes Yes Yes Yes Yes Yes Yes No No
Knowledge
Rayyan Mult. >1 Yes No None No Yes Yes Yes Yes Yes Yes Yes No No Yes
RobotAnalyst Sing. No No No PubMed No Yes No No No Yes No No No No Yes
SWIFT-Active
Mult. >1 Yes No None No No Yes Yes Yes Yes Yes No Yes No No
Screener
SWIFT-Review Sing. No No No None No No No No No No No No No No Yes
FAST2 Sing. No No No None No No No No No No No No No No Yes
ASReview Sing. >1 No No None No No No No No Yes No No No No Yes
Research
Mult. >1 No No None No No No Yes Yes Yes No No No No Yes
Screener
pitts.ai Mult. >1 No No PubMed No No No No Yes Yes No No No No No
SysRev.com Mult. >1 Yes No PubMed No No Yes No Yes Yes Yes No No No No
Covidence Mult. >1 No No None No No Yes Yes Yes Yes Yes Yes No No No
RobotReviewer
Sing. No No No None No No No No No No No No No No Yes
/RobotSearch
Iris.ai Sing. No Yes No CORE; PubMed; US Patent Office; CORDIS No No No No No No No No No No No
PICO Portal Mult. >1 Yes Yes None No No Yes Yes Yes Yes Yes No Yes No Yes
Dextr Sing. No No NA None NA NA NA NA No NA NA NA NA NA Yes
ExaCT Sing. No No NA None NA NA NA NA No NA NA NA NA NA YesReferences
[1] Higgins, J.: Cochrane handbook for systematic reviews of interventions. version
5.1. 0 [updated march 2011]. the cochrane collaboration. www. cochrane-
handbook. org (2011)
[2] Moher,D.,Liberati,A.,Tetzlaff,J.,Altman,D.G.,Group*,P.:Preferredreport-
ing items for systematic reviews and meta-analyses: the prisma statement.
Annals of internal medicine 151(4), 264–269 (2009)
[3] Sackett, D.L., Rosenberg, W.M., Gray, J.M., Haynes, R.B., Richardson, W.S.:
Evidence based medicine: what it is and what it isn’t. British Medical Journal
Publishing Group (1996)
[4] Petticrew, M., Roberts, H.: Systematic Reviews in the Social Sciences: A
Practical Guide. John Wiley & Sons, Glasgow (2008)
[5] Keele, S., et al.: Guidelines for performing systematic literature reviews in
softwareengineering.Technicalreport,ver.2.3ebsetechnicalreport.ebse(2007)
[6] Gough, D., Thomas, J., Oliver, S.: An Introduction to Systematic Reviews.
SAGE Publications Ltd, London (2017)
[7] Pullin,A.S.,Stewart,G.B.:Guidelinesforsystematicreviewinconservationand
environmental management. Conservation biology 20(6), 1647–1656 (2006)
[8] Tranfield, D., Denyer, D., Smart, P.: Towards a methodology for develop-
ing evidence-informed management knowledge by means of systematic review.
British journal of management 14(3), 207–222 (2003)
[9] Borah, R., Brown, A.W., Capers, P.L., Kaiser, K.A.: Analysis of the time and
workers needed to conduct systematic reviews of medical interventions using
data from the prospero registry. BMJ open 7(2), 012545 (2017)
[10] Shojania, K.G., Sampson, M., Ansari, M.T., Ji, J., Doucette, S., Moher, D.:
How quickly do systematic reviews go out of date? a survival analysis. Annals
of internal medicine 147(4), 224–233 (2007)
[11] Shemilt, I., Khan, N., Park, S., Thomas, J.: Use of cost-effectiveness analysis
to compare the efficiency of study identification methods in systematic reviews.
Systematic reviews 5, 1–13 (2016)
[12] Bornmann, L., Mutz, R.: Growth rates of modern science: A bibliometric anal-
ysis based on the number of publications and cited references. Journal of the
Association for Information Science and Technology 66(11), 2215–2222 (2015)
[13] Moher, D., Tsertsvadze, A., Tricco, A.C., Eccles, M., Grimshaw, J., Sampson,
M., Barrowman, N.: A systematic review identified few methods and strate-
gies describing when and how to update systematic reviews. Journal of clinical
epidemiology 60(11), 1095–1 (2007)
[14] van den Bulk, L.M., Bouzembrak, Y., Gavai, A., Liu, N., van den Heuvel, L.J.,
Marvin,H.J.:Automaticclassificationofliteratureinsystematicreviewsonfood
safetyusingmachinelearning.CurrentResearchinFoodScience5,84–95(2022)
[15] Kebede, M.M., Le Cornet, C., Fortner, R.T.: In-depth evaluation of machine
learning methods for semi-automating article screening in a systematic review
of mechanistic literature. Research Synthesis Methods 14(2), 156–172 (2023)
[16] Robinson,A.,Thorne,W.,Wu,B.P.,Pandor,A.,Essat,M.,Stevenson,M.,Song,
X.:Bio-sieve:Exploringinstructiontuninglargelanguagemodelsforsystematic
review automation. arXiv preprint arXiv:2308.06610 (2023)
[17] Cowie, K., Rahmatullah, A., Hardy, N., Holub, K., Kallmes, K., et al.: Web-
based software tools for systematic literature review in medicine: Systematic
search and feature analysis. JMIR Medical Informatics 10(5), 33219 (2022)
31[18] Khalil, H., Ameen, D., Zarnegar, A.: Tools to support the automation of sys-
tematic reviews: a scoping review. Journal of Clinical Epidemiology 144, 22–42
(2022)
[19] Cierco Jimenez, R., Lee, T., Rosillo, N., Cordova, R., Cree, I.A., Gonzalez,
A., Indave Ruiz, B.I.: Machine learning computational tools to assist the per-
formance of systematic reviews: A mapping review. BMC Medical Research
Methodology 22(1), 1–14 (2022)
[20] de la Torre-L´opez, J., Ram´ırez, A., Romero, J.R.: Artificial intelligence to
automatethesystematicreviewofscientificliterature.Computing,1–24(2023)
[21] Burgard, T., Bittermann, A.: Reducing literature screening workload with
machine learning. Zeitschrift fu¨r Psychologie (2023)
[22] Robledo, S., Grisales Aguirre, A.M., Hughes, M., Eggers, F.: “hasta la
vista, baby”–will machine learning terminate human literature reviews in
entrepreneurship? Journal of Small Business Management 61(3), 1314–1343
(2023)
[23] Napole˜ao, B.M., Petrillo, F., Hall´e, S.: Automated support for searching and
selecting evidence in software engineering: A cross-domain systematic mapping.
In: 2021 47th Euromicro Conference on Software Engineering and Advanced
Applications (SEAA), pp. 45–53 (2021). IEEE
[24] Page, M.J., McKenzie, J.E., Bossuyt, P.M., Boutron, I., Hoffmann, T.C., Mul-
row, C.D., Shamseer, L., Tetzlaff, J.M., Akl, E.A., Brennan, S.E., et al.: The
prisma 2020 statement: an updated guideline for reporting systematic reviews.
International journal of surgery 88, 105906 (2021)
[25] Iansiti,M.,Lakhani,K.R.:CompetingintheAgeofAI:StrategyandLeadership
WhenAlgorithmsandNetworksRuntheWorld.HarvardBusinessPress,Boston
(2020)
[26] O’Connor, D., Green, S., Higgins, J.P.: Defining the review question and devel-
oping criteria for including studies. Cochrane handbook for systematic reviews
of interventions: Cochrane book series, 81–94 (2008)
[27] Fontaine, G., Maheu-Cadotte, M.-A., Lavallee, A., Mailhot, T., Lavoie, P.,
Rouleau, G., Vinette, B., Garc´ıa, M.-P.R., Bourbonnais, A.: Designing, plan-
ning,andconductingsystematicreviewsandotherknowledgesyntheses:Sixkey
practical recommendations to improve feasibility and efficiency. Worldviews on
Evidence-Based Nursing 19(6), 434–441 (2022)
[28] Glanville,J.,Dooley,G.,Wisniewski,S.,Foxlee,R.,Noel-Storr,A.:Development
ofasearchfiltertoidentifyreportsofcontrolledclinicaltrialswithincinahlplus.
Health Information & Libraries Journal 36(1), 73–90 (2019)
32[29] Team, E.: Information resources group (irg) workshop: Pushing the frontiers of
htainformationmanagement.EvidenceBasedLibraryandInformationPractice
(2007)
[30] Webster,J.,Watson,R.T.:Analyzingthepasttoprepareforthefuture:Writing
a literature review. MIS quarterly, (2002)
[31] Wohlin, C.: Guidelines for snowballing in systematic literature studies and a
replication in software engineering. In: Proceedings of the 18th International
Conference on Evaluation and Assessment in Software Engineering, pp. 1–10
(2014)
[32] Mour˜ao, E., Kalinowski, M., Murta, L., Mendes, E., Wohlin, C.: Investigating
the use of a hybrid search strategy for systematic reviews. In: 2017 ACM/IEEE
International Symposium on Empirical Software Engineering and Measurement
(ESEM), pp. 193–198 (2017). IEEE
[33] Wohlin, C., Kalinowski, M., Felizardo, K.R., Mendes, E.: Successful combina-
tion of database search and snowballing for identification of primary studies in
systematicliteraturestudies.InformationandSoftwareTechnology147,106908
(2022)
[34] Adam, G.P., Wallace, B.C., Trikalinos, T.A.: Semi-automated tools for system-
atic searches. Meta-Research: Methods and Protocols, 17–40 (2022)
[35] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Ku¨ttler,
H., Lewis, M., Yih, W.-t., Rockt¨aschel, T., et al.: Retrieval-augmented gen-
eration for knowledge-intensive nlp tasks. Advances in Neural Information
Processing Systems 33, 9459–9474 (2020)
[36] Miwa, M., Thomas, J., O’Mara-Eves, A., Ananiadou, S.: Reducing system-
atic review workload through certainty-based screening. Journal of biomedical
informatics 51, 242–253 (2014)
[37] Dawes,M.,Pluye,P.,Shea,L.,Grad,R.,Greenberg,A.,Nie,J.-Y.:Theidentifi-
cationofclinicallyimportantelementswithinmedicaljournalabstracts:Patient–
population–problem,exposure–intervention,comparison,outcome,durationand
results (pecodr). Informatics in Primary care 15(1) (2007)
[38] Kim, S.N., Martinez, D., Cavedon, L., Yencken, L.: Automatic classification of
sentences to support evidence based medicine. In: BMC Bioinformatics, vol. 12,
pp. 1–10 (2011). BioMed Central
[39] Bossuyt,P.M.,Reitsma,J.B.,Bruns,D.E.,Gatsonis,C.A.,Glasziou,P.P.,Irwig,
L.M.,Lijmer,J.G.,Moher,D.,Rennie,D.,etal.:Towardscompleteandaccurate
reporting of studies of diagnostic accuracy: the stard initiative. (2003)
33[40] Garousi, V., Felderer, M.: Experience-based guidelines for effective and efficient
data extraction in systematic reviews in software engineering. In: Proceedings
of the 21st International Conference on Evaluation and Assessment in Software
Engineering, pp. 170–179 (2017)
[41] Munn, Z., Tufanaru, C., Aromataris, E.: Jbi’s systematic reviews: data extrac-
tion and synthesis. AJN The American Journal of Nursing 114(7), 49–54
(2014)
[42] Marshall, I.J., Noel-Storr, A., Kuiper, J., Thomas, J., Wallace, B.C.: Machine
learning for identifying randomized controlled trials: an evaluation and practi-
tioner’s guide. Research synthesis methods 9(4), 602–614 (2018)
[43] Kiritchenko, S., De Bruijn, B., Carini, S., Martin, J., Sim, I.: Exact: auto-
matic extraction of clinical trial characteristics from journal publications. BMC
medical informatics and decision making 10, 1–17 (2010)
[44] Moher, D., Schulz, K.F., Altman, D.G.: The consort statement: revised recom-
mendations for improving the quality of reports of parallel-group randomised
trials. The Lancet 357(9263), 1191–1194 (2001)
[45] Walker, V.R., Schmitt, C.P., Wolfe, M.S., Nowak, A.J., Kulesza, K., Williams,
A.R., Shin, R., Cohen, J., Burch, D., Stout, M.D., et al.: Evaluation of a semi-
automateddataextractiontoolforpublichealthliterature-basedreviews:Dextr.
Environment international 159, 107025 (2022)
[46] Higgins,J.P.,Altman,D.G.:Assessingriskofbiasinincludedstudies.Cochrane
handbookforsystematicreviewsofinterventions:Cochranebookseries,187–241
(2008)
[47] Wells, G.A., Shea, B., O’Connell, D., Peterson, J., Welch, V., Losos, M., Tug-
well, P., et al.: The newcastle-ottawa scale (nos) for assessing the quality of
nonrandomised studies in meta-analyses (2000)
[48] Von Elm, E., Altman, D.G., Egger, M., Pocock, S.J., Gøtzsche, P.C., Van-
denbroucke, J.P.: The strengthening the reporting of observational studies in
epidemiology (strobe) statement: guidelines for reporting observational studies.
The Lancet 370(9596), 1453–1457 (2007)
[49] Project, E.P.H.P.: Quality assessment tool for quantitative studies. National
Collaborating Centre for Methods and Tools, McMaster University, Hamilton,
Ontario (1998)
[50] Zhou, Y., Zhang, H., Huang, X., Yang, S., Babar, M.A., Tang, H.: Quality
assessment of systematic reviews in software engineering: A tertiary study. In:
Proceedingsofthe19thInternationalConferenceonEvaluationandAssessment
in Software Engineering, pp. 1–14 (2015)
34[51] Chen, Z., He, L., Liu, S., Liu, H., Yu, J., Li, Y.: The grading of recommen-
dations, assessment, development, and evaluation approach was rarely followed
andinconsistentlyappliedinpressureinjurypreventionguidelinesdevelopment:
A cross-sectional survey. Journal of Tissue Viability 31(3), 438–443 (2022)
[52] Stroup, D.F., Berlin, J.A., Morton, S.C., Olkin, I., Williamson, G.D., Rennie,
D., Moher, D., Becker, B.J., Sipe, T.A., Thacker, S.B., et al.: Meta-analysis of
observational studies in epidemiology: a proposal for reporting. Jama 283(15),
2008–2012 (2000)
[53] Li, X., Ouyang, J.: Automatic related work generation: A meta study. arXiv
preprint arXiv:2201.01880 (2022)
[54] Justitia, A., Wang, H.-C.: Automatic related work section in scientific arti-
cle: Research trends and future directions. In: 2022 International Seminar on
IntelligentTechnologyandItsApplications(ISITIA),pp.108–114(2022).IEEE
[55] Tsafnat, G., Glasziou, P., Choong, M.K., Dunn, A., Galgani, F., Coiera, E.:
Systematic review automation technologies. Systematic reviews 3, 1–15 (2014)
[56] Marshall, C., Brereton, P.: Systematic review toolbox: a catalogue of tools to
supportsystematicreviews.In:Proceedingsofthe19thInternationalConference
on Evaluation and Assessment in Software Engineering, pp. 1–6 (2015)
[57] Zuckarelli, J.: packagefinder: Comfortable search for r packages on cran directly
from the r console. CRAN. Last access 29 (2023)
[58] Hannousse, A.: Searching relevant papers for software engineering secondary
studies: Semantic scholar coverage and identification role. IET Software 15(1),
126–146 (2021)
[59] Wallace, B.C., Small, K., Brodley, C.E., Lau, J., Trikalinos, T.A.: Deploying
an interactive machine learning system in an evidence-based practice cen-
ter: abstrackr. In: Proceedings of the 2nd ACM SIGHIT International Health
Informatics Symposium, pp. 819–824 (2012)
[60] Cheng, S., Augustin, C., Bethel, A., Gill, D.A., Anzaroot, S., Brun, J.L.,
Dewilde, B., Minnich, R., Garside, R., Masuda, Y.J., Miller, D.C., Wilkie, D.S.,
Wongbusarakum, S., McKinnon, M.C.: Using machine learning to advance syn-
thesisanduseofconservationandenvironmentalevidence.ConservationBiology
32 (2018)
[61] Cheng,S.,Augustin,C.:Keepahumaninthemachineandotherlessonslearned
from deploying and maintaining colandr. CHANCE 34(3), 56–60 (2021)
[62] Thomas, J., Brunton, J., Graziosi, S.: Eppi-reviewer 4.0: software for research
35synthesis. EPPI-Centre Software. London: Social Science Research Unit, Insti-
tute of Education (2010)
[63] Machine Learning Functionality in EPPI-Reviewer.
https://eppi.ioe.ac.uk/CMS/Portals/35/machine learning in eppi-
reviewer v 7 web version.pdf
[64] Ouzzani, M., Hammady, H., Fedorowicz, Z., Elmagarmid, A.: Rayyan—a web
and mobile app for systematic reviews. Systematic reviews 5, 1–10 (2016)
[65] Howard, B.E., Phillips, J., Tandon, A., Maharana, A., Elmore, R., Mav, D.,
Sedykh, A., Thayer, K., Merrick, B.A., Walker, V., et al.: Swift-active screener:
Accelerated document screening through active learning and integrated recall
estimation. Environment International 138, 105623 (2020)
[66] Howard, B.E., Phillips, J., Miller, K., Tandon, A., Mav, D., Shah, M.R.,
Holmgren, S., Pelch, K.E., Walker, V., Rooney, A.A., et al.: Swift-review: a
text-miningworkbenchforsystematicreview.Systematicreviews5,1–16(2016)
[67] Bozada Jr, T., Borden, J., Workman, J., Del Cid, M., Malinowski, J., Luechte-
feld,T.:Sysrev:afairplatformfordatacurationandsystematicevidencereview.
Frontiers in Artificial Intelligence 4, 685298 (2021)
[68] Przybyl(cid:32)a,P.,Brockmeier,A.J.,Kontonatsios,G.,LePogam,M.-A.,McNaught,
J., von Elm, E., Nolan, K., Ananiadou, S.: Prioritising references for systematic
reviews with robotanalyst: a user study. Research synthesis methods 9(3), 470–
488 (2018)
[69] Allot,A.,Lee,K.,Chen,Q.,Luo,L.,Lu,Z.:Litsuggest:aweb-basedsystemfor
literature recommendation and curation using machine learning. Nucleic acids
research 49(W1), 352–358 (2021)
[70] Chai, K.E., Lines, R.L., Gucciardi, D.F., Ng, L.: Research screener: a machine
learning tool to semi-automate abstract screening for systematic reviews.
Systematic reviews 10, 1–13 (2021)
[71] Van De Schoot, R., De Bruin, J., Schram, R., Zahedi, P., De Boer, J., Wei-
jdema, F., Kramer, B., Huijts, M., Hoogerwerf, M., Ferdinands, G., et al.: An
opensourcemachinelearningframeworkforefficientandtransparentsystematic
reviews. Nature machine intelligence 3(2), 125–133 (2021)
[72] Agai, E.: A new machine-learning powered tool to aid citation screening for
evidence synthesis: Picoportal. Advances in Evidence Synthesis: Special Issue.
Cochrane Database Syst Rev 9(suppl 1), 172 (2020)
[73] Minion, J.T., Egunsola, O., Mastikhina, L., Farkas, B., Hofmeister, M., Flana-
gan, J., Salmon, C., Clement, F.: Pico portal. The Journal of the Canadian
36Health Libraries Association 42(3), 181 (2021)
[74] Yu, Z., Menzies, T.: Fast2: An intelligent assistant for finding relevant papers.
Expert Systems with Applications 120, 57–71 (2019)
[75] Marshall, I.J., Kuiper, J., Banner, E., Wallace, B.C.: Automating biomedical
evidence synthesis: Robotreviewer. In: Proceedings of the Conference. Associa-
tion for Computational Linguistics. Meeting, vol. 2017, p. 7 (2017). NIH Public
Access
[76] Zhang,Y.,Jin,R.,Zhou,Z.-H.:Understandingbag-of-wordsmodel:astatistical
framework. International journal of machine learning and cybernetics 1, 43–52
(2010)
[77] Blei, D.M., Ng, A.Y., Jordan, M.I.: Latent dirichlet allocation. Journal of
machine Learning research 3(Jan), 993–1022 (2003)
[78] Wang, S., Zhou, W., Jiang, C.: A survey of word embeddings based on deep
learning. Computing 102, 717–740 (2020)
[79] Shah,C.A.,Phadnis,P.N.:Textsummarizationusingextractiveandabstractive
techniques. International Journal of Scientific Research in Computer Science,
Engineering and Information Technology (2022)
[80] Marshall,C.,Brereton,P.,Kitchenham,B.:Toolstosupportsystematicreviews
in software engineering: a feature analysis. In: Proceedings of the 18th Interna-
tional Conference on Evaluation and Assessment in Software Engineering, pp.
1–10 (2014)
[81] Kohl, C., McIntosh, E.J., Unger, S., Haddaway, N.R., Kecke, S., Schiemann, J.,
Wilhelm, R.: Online tools supporting the conduct and reporting of systematic
reviews and systematic maps: a case study on cadima and review of existing
tools. Environmental Evidence 7(1), 1–17 (2018)
[82] Van der Mierden, S., Tsaioun, K., Bleich, A., Leenaars, C.H., et al.: Software
toolsforliteraturescreeninginsystematicreviewsinbiomedicalresearch.Altex
36(3), 508–517 (2019)
[83] Harrison,H.,Griffin,S.J.,Kuhn,I.,Usher-Smith,J.A.:Softwaretoolstosupport
title and abstract screening for systematic reviews in healthcare: an evaluation.
BMC medical research methodology 20, 1–12 (2020)
[84] Schmidt,L.,Olorisade,B.K.,McGuinness,L.A.,Thomas,J.,Higgins,J.P.:Data
extractionmethodsforsystematicreview(semi)automation:Alivingsystematic
review. F1000Research 10 (2021)
[85] Wu, R., Stauber, V., Botev, V., Elosua, J., Brede, A., Ritola, M., Marinov, K.:
37Scithon™-anevaluationframeworkforassessingresearchproductivitytools.In:
Proceedings of the Eleventh International Conference on Language Resources
and Evaluation (LREC 2018). European Language Resources Association
(ELRA), Paris, France (2018)
[86] Thomas, J., McDonald, S., Noel-Storr, A., Shemilt, I., Elliott, J., Mavergames,
C.,Marshall,I.J.:Machinelearningreducedworkloadwithminimalriskofmiss-
ingstudies:developmentandevaluationofarandomizedcontrolledtrialclassifier
for cochrane reviews. Journal of Clinical Epidemiology 133, 140–151 (2021)
[87] Craig, D., Rice, S.: NHS Economic Evaluation Database Handbook. Centre for
Reviews and Dissemination, York (2007)
[88] La Toile, Q.: Database of abstracts of reviews of effects (dare). Douleurs 5(2)
(2004)
[89] Onan, A., Koruko˘glu, S., Bulut, H.: Ensemble of keyword extraction methods
and classifiers in text classification. Expert Systems with Applications 57, 232–
247 (2016)
[90] Beltagy,I.,Lo,K.,Cohan,A.:Scibert:Apretrainedlanguagemodelforscientific
text. arXiv preprint arXiv:1903.10676 (2019)
[91] Le,Q.,Mikolov,T.:Distributedrepresentationsofsentencesanddocuments.In:
International Conference on Machine Learning, pp. 1188–1196 (2014). PMLR
[92] Reimers, N., Gurevych, I.: Sentence-bert: Sentence embeddings using siamese
bert-networks. arXiv preprint arXiv:1908.10084 (2019)
[93] Lee,J.,Yoon,W.,Kim,S.,Kim,D.,Kim,S.,So,C.H.,Kang,J.:Biobert:apre-
trained biomedical language representation model for biomedical text mining.
Bioinformatics 36(4), 1234–1240 (2020)
[94] Mikolov,T.,Sutskever,I.,Chen,K.,Corrado,G.S.,Dean,J.:Distributedrepre-
sentations of words and phrases and their compositionality. Advances in neural
information processing systems 26 (2013)
[95] Pennington, J., Socher, R., Manning, C.D.: Glove: Global vectors for word rep-
resentation. In: Proceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pp. 1532–1543 (2014)
[96] Google.https://drive.google.com/file/d/1gsg8s8WGrTETJxL3dL2eqzPowxbwcr37/
view
[97] Ng,A.,Jordan,M.,Weiss,Y.:Onspectralclustering:Analysisandanalgorithm.
Advances in neural information processing systems 14 (2001)
38[98] Brockmeier, A.J., Mu, T., Ananiadou, S., Goulermas, J.Y.: Self-tuned descrip-
tive document clustering using a predictive network. IEEE Transactions on
Knowledge and Data Engineering 30(10), 1929–1942 (2018)
[99] Nasar, Z., Jaffry, S.W., Malik, M.K.: Named entity recognition and relation
extraction: State-of-the-art. ACM Computing Surveys (CSUR) 54(1), 1–39
(2021)
[100] Nowak, A., Kunstman, P.: Team ep at tac 2018: Automating data extraction
in systematic reviews of environmental agents. arXiv preprint arXiv:1901.02081
(2019)
[101] Krichen, M.: Convolutional neural networks: A survey. Computers 12(8), 151
(2023)
[102] Boser,B.E.,Guyon,I.M.,Vapnik,V.N.:Atrainingalgorithmforoptimalmargin
classifiers. In: Proceedings of the Fifth Annual Workshop on Computational
Learning Theory, pp. 144–152 (1992)
[103] Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettle-
moyer, L.: Deep contextualized word representations. ArXiv abs/1802.05365
(2018)
[104] Min, B., Ross, H., Sulem, E., Veyseh, A.P.B., Nguyen, T.H., Sainz, O., Agirre,
E.,Heintz,I.,Roth,D.:Recentadvancesinnaturallanguageprocessingvialarge
pre-trained language models: A survey. ACM Computing Surveys 56(2), 1–40
(2023)
[105] Dunn, A., Dagdelen, J., Walker, N., Lee, S., Rosen, A.S., Ceder, G., Persson,
K.,Jain,A.:Structuredinformationextractionfromcomplexscientifictextwith
fine-tuned large language models. arXiv preprint arXiv:2212.05238 (2022)
[106] Xu, D., Chen, W., Peng, W., Zhang, C., Xu, T., Zhao, X., Wu, X., Zheng,
Y., Chen, E.: Large language models for generative information extraction: A
survey. arXiv preprint arXiv:2312.17617 (2023)
[107] Ji,Z.,Lee,N.,Frieske,R.,Yu,T.,Su,D.,Xu,Y.,Ishii,E.,Bang,Y.J.,Madotto,
A., Fung, P.: Survey of hallucination in natural language generation. ACM
Computing Surveys 55(12), 1–38 (2023)
[108] Meloni, A., Angioni, S., Salatino, A., Osborne, F., Recupero, D.R., Motta, E.:
Integrating conversational agents and knowledge graphs within the scholarly
domain. IEEE Access 11, 22468–22489 (2023)
[109] Pride, D., Cancellieri, M.,Knoth, P.:Core-gpt: Combining open access research
and large language models for credible, trustworthy question answering. In:
International Conference on Theory and Practice of Digital Libraries, pp.
39146–159 (2023). Springer
[110] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P.,
Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models
are few-shot learners. Advances in neural information processing systems 33,
1877–1901 (2020)
[111] OpenAI: GPT-4 Technical Report (2023)
[112] Li, Q., Li, J., Sheng, J., Cui, S., Wu, J., Hei, Y., Peng, H., Guo, S., Wang, L.,
Beheshti, A., et al.: A survey on deep learning event extraction: Approaches
andapplications.IEEETransactionsonNeuralNetworksandLearningSystems
(2022)
[113] Liu,P.,Gao,W.,Dong,W.,Huang,S.,Zhang,Y.:Openinformationextraction
from 2007 to 2022–a survey. arXiv preprint arXiv:2208.08690 (2022)
[114] Tagawa, Y., Taniguchi, M., Miura, Y., Taniguchi, T., Ohkuma, T., Yamamoto,
T.,Nemoto,K.:Relationpredictionforunseen-entitiesusingentity-wordgraphs.
In: Proceedings of the Thirteenth Workshop on Graph-Based Methods for
Natural Language Processing (TextGraphs-13), pp. 11–16 (2019)
[115] Vladika, J., Matthes, F.: Scientific fact-checking: A survey of resources and
approaches. arXiv preprint arXiv:2305.16859 (2023)
[116] Lawrence, J., Reed, C.: Argument mining: A survey. Computational Linguistics
45(4), 765–818 (2020)
[117] Linardatos, P., Papastefanopoulos, V., Kotsiantis, S.: Explainable ai: A review
of machine learning interpretability methods. Entropy 23(1), 18 (2020)
[118] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le,
Q.,Zhou,D.:Chain-of-ThoughtPromptingElicitsReasoninginLargeLanguage
Models (2023)
[119] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.L., Cao, Y., Narasimhan,
K.: Tree of Thoughts: Deliberate Problem Solving with Large Language Models
(2023)
[120] Long, J.: Large Language Model Guided Tree-of-Thought (2023)
[121] Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gianinazzi, L., Gajda, J.,
Lehmann,T.,Podstawski,M.,Niewiadomski,H.,Nyczyk,P.,Hoefler,T.:Graph
of Thoughts: Solving Elaborate Problems with Large Language Models (2023)
[122] Patel, A., Jain, S.: Present and future of semantic web technologies: a research
statement.InternationalJournalofComputersandApplications43(5),413–422
(2021)
40[123] Salatino, A., Osborne, F., Motta, E.: Cso classifier 3.0: a scalable unsuper-
visedmethodforclassifyingdocumentsintermsofresearchtopics.International
Journal on Digital Libraries, 1–20 (2022)
[124] Peng, C., Xia, F., Naseriparsa, M., Osborne, F.: Knowledge graphs: Opportuni-
ties and challenges. Artificial Intelligence Review, 1–32 (2023)
[125] Hitzler, P.: A review of the semantic web field. Communications of the ACM
64(2), 76–83 (2021)
[126] Jaradeh, M.Y., Oelen, A., Farfar, K.E., et al.: Open research knowledge graph:
Nextgenerationinfrastructureforsemanticscholarlyknowledge.In:Proceedings
ofthe10thInternationalConferenceonKnowledgeCapture,pp.243–246(2019)
[127] Salatino, A.A., Osborne, F., Birukou, A., Motta, E.: Improving editorial work-
flow and metadata quality at springer nature. In: The Semantic Web – ISWC
2019, pp. 507–525. Springer, Cham (2019)
[128] Wijkstra, M., Lek, T., Kuhn, T., Welbers, K., Steijaert, M.: Living literature
reviews. arXiv preprint arXiv:2111.00824 (2021)
[129] Dess´ı, D., Osborne, F., Reforgiato Recupero, D., Buscaldi, D., Motta, E.: Cs-
kg: A large-scale knowledge graph of research entities and claims in computer
science. In: Sattler, U., Hogan, A., Keet, M., Presutti, V., Almeida, J.P.A.,
Takeda, H., Monnin, P., Pirr`o, G., d’Amato, C. (eds.) The Semantic Web –
ISWC 2022, pp. 678–696. Springer, Cham (2022)
[130] Groth, P., Gibson, A., Velterop, J.: The anatomy of a nanopublication. Infor-
mation Services & Use 30(1-2), 51–56 (2010)
[131] Altmami, N.I., Menai, M.E.B.: Automatic summarization of scientific articles:
A survey. Journal of King Saud University-Computer and Information Sciences
34(4), 1011–1028 (2022)
[132] Sikka, P., Mago, V.: A survey on text simplification. arXiv preprint
arXiv:2008.08612 (2020)
[133] Beller, E., Clark, J., Tsafnat, G., Adams, C., Diehl, H., Lund, H., Ouzzani, M.,
Thayer, K., Thomas, J., Turner, T., et al.: Making progress with the automa-
tion of systematic reviews: principles of the international collaboration for the
automation of systematic reviews (icasr). Systematic reviews 7, 1–7 (2018)
[134] O’Connor, A.M., Tsafnat, G., Gilbert, S.B., Thayer, K.A., Wolfe, M.S.: Moving
toward the automation of the systematic review process: a summary of discus-
sions at the second meeting of international collaboration for the automation of
systematic reviews (icasr). Systematic reviews 7, 1–5 (2018)
41[135] O’Connor, A.M., Tsafnat, G., Gilbert, S.B., Thayer, K.A., Shemilt, I., Thomas,
J., Glasziou, P., Wolfe, M.S.: Still moving toward automation of the systematic
review process: a summary of discussions at the third meeting of the interna-
tional collaboration for automation of systematic reviews (icasr). Systematic
reviews 8, 1–5 (2019)
[136] O’Connor, A.M., Glasziou, P., Taylor, M., Thomas, J., Spijker, R., Wolfe, M.S.:
Afocusoncross-purposetools,automatedrecognitionofstudydesigninmultiple
disciplines,andevaluationofautomationtools:asummaryofsignificantdiscus-
sions at the fourth meeting of the international collaboration for automation of
systematic reviews (icasr). Systematic reviews 9(1), 1–6 (2020)
[137] Di Nunzio, G.M., Kanoulas, E., Majumder, P.: Augmented intelligence in
technology-assisted review systems (altars 2022): Evaluation metrics and proto-
cols for ediscovery and systematic review systems. In: European Conference on
Information Retrieval, pp. 557–560 (2022). Springer
[138] Wang, L.L., DeYoung, J., Wallace, B.: Overview of mslr2022: A shared task
on multi-document summarization for literature reviews. In: Proceedings of the
Third Workshop on Scholarly Document Processing (2022)
[139] Liu, J., Timsina, P., El-Gayar, O.: A comparative analysis of semi-supervised
learning:thecaseofarticleselectionformedicalsystematicreviews.Information
Systems Frontiers 20, 195–207 (2018)
[140] Yu, Z., Kraft, N.A., Menzies, T.: Finding better active learners for faster
literature reviews. Empirical Software Engineering 23, 3161–3186 (2018)
[141] Sumbul, G., de Wall, A., Kreuziger, T., Marcelino, F., Costa, H., Benevides,
P., Caetano, M., Demir, B., Markl, V.: Bigearthnet-mm: A large-scale, mul-
timodal, multilabel benchmark archive for remote sensing image classification
and retrieval [software and data sets]. IEEE Geoscience and Remote Sensing
Magazine 9(3), 174–180 (2021). https://doi.org/10.1109/MGRS.2021.3089174
[142] Kusa, W., Knoth, P., Hanbury, A.: Evaluation of automated citation screening
insystematicliteraturereviewswithworksavedoversampling:ananalysis.In::
1st Workshop on Augmented Ingelligence for Technology-Assisted Reviews Sys-
tems: Evaluation Metrics and Protocols for eDiscovery and Systematic Review
Systems, pp. 1–7 (2022)
[143] Kusa, W., Lipani, A., Knoth, P., Hanbury, A.: An analysis of work saved
over sampling in the evaluation of automated citation screening in systematic
literature reviews. Intelligent Systems with Applications 18, 200193 (2023)
[144] Marshall,C.,Kitchenham,B.,Brereton,P.:Toolfeaturestosupportsystematic
reviews in software engineering-a cross domain study. e-Informatica Software
Engineering Journal 12(1), 79–115 (2018)
42[145] Marshall,C.,Brereton,P.,Kitchenham,B.:Toolstosupportsystematicreviews
in software engineering: a cross-domain survey using semi-structured inter-
views. In: Proceedings of the 19th International Conference on Evaluation and
Assessment in Software Engineering, pp. 1–6 (2015)
[146] Van Altena, A., Spijker, R., Olabarriaga, S.: Usage of automation tools in
systematic reviews. Research synthesis methods 10(1), 72–82 (2019)
[147] Scott,A.M.,Forbes,C.,Clark,J.,Carter,M.,Glasziou,P.,Munn,Z.:Systematic
review automation tools improve efficiency but lack of knowledge impedes their
adoption: a survey. Journal of clinical epidemiology 138, 80–94 (2021)
[148] Thomas, J.: Diffusion of innovation in systematic review methodology: why is
study selection not yet assisted by automation. OA Evidence-Based Medicine
1(2), 1–6 (2013)
[149] Arno, A., Elliott, J., Wallace, B., Turner, T., Thomas, J.: The views of health
guideline developers on the use of automation in health evidence synthesis.
Systematic Reviews 10, 1–10 (2021)
[150] O’Connor, A.M., Tsafnat, G., Thomas, J., Glasziou, P., Gilbert, S.B., Hutton,
B.:Aquestionoftrust:canwebuildanevidencebasetogaintrustinsystematic
review automation technologies? Systematic reviews 8(1), 1–8 (2019)
[151] Haddaway, N.R., Callaghan, M.W., Collins, A.M., Lamb, W.F., Minx, J.C.,
Thomas,J.,John,D.:Ontheuseofcomputer-assistancetofacilitatesystematic
mapping. Campbell Systematic Reviews 16(4), 1129 (2020)
[152] Dell,N.A.,Maynard,B.R.,Murphy,A.M.,Stewart,M.:Technologyforresearch
synthesis:Anapplicationofsociotechnicalsystemstheory.JournaloftheSociety
for Social Work and Research 12(1), 201–222 (2021)
[153] Hassler,E.,Carver,J.C.,Kraft,N.A.,Hale,D.:Outcomesofacommunitywork-
shoptoidentifyandrankbarrierstothesystematicliteraturereviewprocess.In:
Proceedingsofthe18thInternationalConferenceonEvaluationandAssessment
in Software Engineering, pp. 1–10 (2014)
[154] Hassler,E.,Carver,J.C.,Hale,D.,Al-Zubidy,A.:Identificationofslrtoolneeds–
results of a community workshop. Information and Software Technology 70,
122–129 (2016)
[155] Al-Zubidy, A., Carver, J.C., Hale, D.P., Hassler, E.E.: Vision for slr tooling
infrastructure: prioritizing value-added requirements. Information and Software
Technology 91, 72–81 (2017)
[156] Lewis, J.R.: The system usability scale: past, present, and future. International
Journal of Human–Computer Interaction 34(7), 577–590 (2018)
43[157] Quesenbery, W.: The five dimensions of usability. In: Content and Complexity,
pp. 93–114. Routledge, New York (2014)
[158] Schall Jr, M.C., Cullen, L., Pennathur, P., Chen, H., Burrell, K., Matthews,
G.:Usability evaluation andimplementationof ahealth information technology
dashboard of evidence-based quality indicators. CIN: Computers, Informatics,
Nursing 35(6), 281–288 (2017)
[159] Sanderson,K.:Aisciencesearchenginesareexplodinginnumber—aretheyany
good? Nature 616(7958), 639–640 (2023)
[160] Van Noorden, R.: Chatgpt-like ais are coming to major science search engines.
Nature 620(7973), 258–258 (2023)
[161] Aguilera Cora, E., Lopezosa, C., Codina, L.: Scopus ai beta: functional analysis
and cases (2024)
[162] Nicholson, J.M., Mordaunt, M., Lopez, P., Uppala, A., Rosati, D., Rodrigues,
N.P., Grabitz, P., Rife, S.C.: Scite: A smart citation index that displays the
context of citations and classifies their intent using deep learning. Quantitative
Science Studies 2(3), 882–898 (2021)
[163] Rife, S.C., Rosati, D., Nicholson, J.M.: scite: The next generation of citations.
Learned Publishing 34 (2021)
[164] Brody,S.:Scite.JournaloftheMedicalLibraryAssociation:JMLA109(4),707
(2021)
[165] Kung, J.: Elicit (product review). Journal of the Canadian Health Libraries
Association/Journal de l’Association des biblioth`eques de la sant´e du Canada
44(1) (2023)
[166] Ding, Y., Zhang, G., Chambers, T., Song, M., Wang, X., Zhai, C.: Content-
based citation analysis: The next generation of citation analysis. Journal of the
association for information science and technology 65(9), 1820–1833 (2014)
[167] Cadeddu, A., Chessa, A., De Leo, V., Fenu, G., Motta, E., Osborne, F., Recu-
pero, D.R., Salatino, A., Secchi, L.: Enhancing scholarly understanding: A
comparison of knowledge injection strategies in large language models. In: Pro-
ceedings Workshop on Deep Learning for Knowledge Graphs at ISWC 2023
(2023)
[168] Hope, T., Downey, D., Weld, D.S., Etzioni, O., Horvitz, E.: A computational
inflection for scientific discovery. Communications of the ACM 66(8), 62–73
(2023)
[169] Borrego, A., Dessi, D., Hern´andez, I., Osborne, F., Recupero, D.R., Ruiz, D.,
44Buscaldi, D., Motta, E.: Completing scientific facts in knowledge graphs of
research concepts. IEEE Access 10, 125867–125880 (2022)
[170] Wang, H., Fu, T., Du, Y., Gao, W., Huang, K., Liu, Z., Chandak, P., Liu, S.,
Van Katwyk, P., Deac, A., et al.: Scientific discovery in the age of artificial
intelligence. Nature 620(7972), 47–60 (2023)
[171] Mittelstadt, B.: Principles alone cannot guarantee ethical ai. Nature machine
intelligence 1(11), 501–507 (2019)
[172] Ntoutsi,E.,Fafalios,P.,Gadiraju,U.,Iosifidis,V.,Nejdl,W.,Vidal,M.-E.,Rug-
gieri,S.,Turini,F.,Papadopoulos,S.,Krasanakis,E.,et al.:Biasindata-driven
artificial intelligence systems—an introductory survey. Wiley Interdisciplinary
Reviews: Data Mining and Knowledge Discovery 10(3), 1356 (2020)
[173] Spector, J.M., Ma, S.: Inquiry and critical thinking skills for the next gener-
ation: from artificial intelligence back to human intelligence. Smart Learning
Environments 6(1), 1–11 (2019)
45