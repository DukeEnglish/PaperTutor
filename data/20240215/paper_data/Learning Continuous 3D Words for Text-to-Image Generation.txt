Learning Continuous 3D Words for Text-to-Image Generation
Ta-YingCheng1* MatheusGadelha2 ThibaultGroueix2 MatthewFisher2
Radom´ırMe˘ch2 AndrewMarkham1 NikiTrigoni1
1UniversityofOxford 2AdobeResearch
(a) “[ ] photo of a race car on the road” (b) “A [ ] photo of a wooden chair by a lake”
Illumination Dolly
Zoom
(c) “[ ] photo of a [ ] lion by the beach” (d) “[ ] photo of an [ ] owl”
Illumination Wing
Orientation Orientation
Figure1.WeintroduceContinuous3DWords–specialtokensintext-to-imagemodelsthatallowuserstohavefine-grainedcontrolover
severalattributeslikeillumination [ ](aandc),non-rigidshapechange [ ](d),orientation [ ](candd),andcameraparameters [ ]
(b).Ourapproachcanbetrainedusingasingle3Dmeshandarenderingenginewhileincurringintonegligibleruntimeandmemorycosts.
Abstract generation. Givenonlyasinglemeshandarenderingen-
gine,weshowthatourapproachcanbeadoptedtoprovide
Current controls over diffusion models (e.g., through continuoususercontroloverseveral3D-awareattributes,
text or ControlNet) for image generation fall short in rec- including time-of-day illumination, bird wing orientation,
ognizing abstract, continuous attributes like illumination dollyzoom effect, and object poses. Our method is capa-
direction or non-rigid shape change. In this paper, we ble of conditioning image creation with multiple Continu-
present an approach for allowing users of text-to-image ous 3D Words and text descriptions simultaneously while
models to have fine-grained control of several attributes addingnooverheadtothegenerativeprocess. ProjectPage:
inanimage. Wedothisbyengineeringspecialsetsofin- https://ttchengab.github.io/continuous 3d words
puttokensthatcanbetransformedinacontinuousmanner
– we call them Continuous 3D Words. These attributes
can, for example, be represented as sliders and applied
jointlywithtextpromptsforfine-grainedcontroloverimage *WorkwasdoneduringinternshipatAdobeResearch.
1
4202
beF
31
]VC.sc[
1v45680.2042:viXra1.Introduction each value of an attribute as a new object, which would
prevent us from generalizing the attribute to new objects.
Photographyisfascinatingbecauseitenablesverydetailed
Second,weapplyControlNet[30]withvariousconditioned
controloverthecompositionandaestheticsofthefinalim-
imagestogenerateasetofadditionalimageswithvarying
age. Ontheonehand,thisissimplytheresultofapplication
backgroundsandobjecttextures. Thispreventsthemodel
ofphysicallawstoachieveimageacquisition. Ontheother,
fromoverfittingtotheartificialbackgroundsofrenderedim-
theslightestchangesinthemomentcaptured,illumination,
ages. Theentiretrainingwasdoneinalight-weightLower
objectorientation,orcameraparametersbringacompletely
RankAdaptation(LoRA)[14]manner,makingitfastand
differentfeelingtotheviewer. Whilethegiantleapofmod-
accessiblewithsingleGPUs.
erntext-to-imagediffusioncanbringgenerated2-Dimages
We implement our continuous vocabulary and training
tocloseproximitywithrealphotos,textpromptsareinher-
method, across various sets of single (e.g., dollyzoom ex-
entlylimitedtohigh-leveldescriptions,farremovedfromthe
tractedfromchairs)andmultiple(e.g.,objectposeandillu-
detailedcontrolsonehasoveractualphotography. Thisis
minationextractedfromadogmesh)attributes,andshow
mainlyduetothescarcityofsuchdescriptionsinthetraining
throughquantitativeuserstudiesandqualitativecomparisons
dataset—veryfewwoulddescribeaphotobasedonexact
thatourmethodcanproperlyreflectvariousattributeswhile
objectmovementsandcameraparameterslikethewingpose
maintainingtheaestheticsoftheimage—significantlyout-
ofabirdortherotationofaperson’sheadindegrees. Onthe
performingcompetitivebaselines.
otherhand,3Drenderingenginesallowustomimicmany
Insummary,wepresent1)Continuous3DWords,anew
ofthese3Dcontrolsthatphotographersenjoy. Wecanren-
methodofgaining3D-aware,continuousattributecontrols
derimagesofobjectswithpredefinedcamera,illumination
overtext-to-imagegenerationthatcanbeeasilytailoredtoa
and pose changes at a very fine-grained scale. However,
plethoraofnewconditions,2)aseriesoftrainingstrategies
creatingdetailed3Dworldsisincrediblylaborious,which
todisentangletheattributesfromobjectidentitytoenhance
limitsthediversityofthescenesthatcanbegeneratedby
theimprovementsinimagegenerationand3)extensivequal-
non-specializedpractitioners. Inthatregard,usingtext-to-
itativeandquantitativestudiestoshowcaseourapproachin
imagediffusiontocreateimagesisamuchmoreaccessible
variousinterestingapplications.
technology,whereasprecise3Dscenecontrolremainsfirmly
inthedomainofexperts.
2.RelatedWork
In this work, we aim to bring together the best of two
worldsbyexpandingthevocabularyoftext-to-imagediffu- ConditionalDiffusionModels. Eversincediffusionmod-
sionmodelswithveryfewsamplesgeneratedfromrender- els[13,27]pushedthequalityandgeneralizabilityofimage
ing engines. Specifically, we render meshes based on the generation beyond GANs [10], the vision community has
attributeweaimtocontrol,creatingimageswithcolorand introduced a diverse range of modalities that can be used
otherusefulinformationtogenerateasmallsetofdatasam- asconditionstocontroltheimagegenerationprocess. The
ples. Thegoalistodisentangletheseabstractattributesfrom most common condition is currently text. Works such as
theoriginalobjectandencodethemintothetextualspacein DALLE[2,22,23]andImagen[26]usedlargescaletext-
acontrollablemanner–wetermtheseattributesContinuous image datasets and strong language understandings from
3D Words. They allow users to create custom sliders that pretrainedLLMs[4,8,21]toguidethegenerationprocess.
enablefine-grainedcontrolduringimagegenerationandcan StableDiffusionfurtherpopularizedthisclassofmethods
beseamlesslyusedalongtextprompts. byemployingmemoryefficientmodelsthroughlatent-space
At the heart of our approach is an algorithm to learn a diffusion[24]. Otherworksbuiltontopofthesemodelsby
continuousvocabulary. Thebenefitsofcontinuityaretwo- addingotherformsofconditioning[19,30]. Highlyrelevant
fold: i)theassociationbetweendifferentvaluesofthesame toourworkisControlNet[30], whichproposesageneral
attributemakesitmucheasiertolearn,ratherthanhavingto pipelinewithzero-convolutionsforconditioningontextand
learnhundredsofdiscretetokensasanapproximationandii) imagedata(e.g., depthmaps, cannymaps, sketches). De-
welearnanMLPthatwouldallowinterpolationduringinfer- spitetheirimpressiveimagequality, itisnotclearhowto
encetogenerateanactualcontinuouscontrol. Ontopofthis, usethesemodelstocontrolotherattributesofimageslike
wealsoproposetwotrainingstrategiestopreventdegenerate illuminationorobjectorientation.
solutionsandenablegeneralizationonnewobjectsbeyond Othersetofworksexploredhowtoperformimageedits
thetrainingcategory. First,weapplyatwo-stagetraining usingtextualinstructions.Givenatext-generatedimage,they
strategy: we first apply the Dreambooth [25] approach to demonstratehowtheusercanedittheimagebyamending
learn the object identity of the underlying mesh used for theprompt, yetstillpreservesomeaspectsoftheoriginal
rendering,thensequentiallylearnthevariousattributeval- image[3,12,20]. Whileconvenient,theseapproachesdo
uesdisentangledfromtheobjectidentity. Thispreventsthe not allow fine-grained control over image elements since
modelfromfallingintoadegeneratesolutionofencoding theyareultimatelyrestrictedbytheuser’sabilitytodescribe
2(cid:41)(cid:76)(cid:81)(cid:72)(cid:87)(cid:88)(cid:81)(cid:76)(cid:81)(cid:74) (cid:44)(cid:81)(cid:73)(cid:72)(cid:85)(cid:72)(cid:81)(cid:70)(cid:72)
(cid:54)(cid:87)(cid:68)(cid:74)(cid:72)(cid:3)(cid:20)
(cid:53)(cid:72)(cid:70)(cid:82)(cid:81)(cid:86)(cid:87)(cid:85)(cid:88)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:47)(cid:82)(cid:86)(cid:86)
(cid:36)(cid:87)(cid:87)(cid:85)(cid:17)(cid:3)(cid:20) (cid:36)(cid:87)(cid:87)(cid:85)(cid:17)(cid:3)(cid:21)
(cid:53)(cid:72)(cid:81)(cid:71)(cid:72)(cid:85)(cid:76)(cid:81)(cid:74)(cid:3)(cid:40)(cid:81)(cid:74)(cid:76)(cid:81)(cid:72)
(cid:48)(cid:72)(cid:86)(cid:75)(cid:3)(cid:62)(cid:82)(cid:69)(cid:77)(cid:64) (cid:36)(cid:87)(cid:87)(cid:85)(cid:17)(cid:3)(cid:20) (cid:36)(cid:87)(cid:87)(cid:85)(cid:17)(cid:3)(cid:21)
(cid:5)(cid:36)(cid:3)(cid:62)(cid:3)(cid:3)(cid:64)(cid:3)(cid:62)(cid:3)(cid:3)(cid:64)(cid:3)(cid:83)(cid:75)(cid:82)(cid:87)(cid:82)
(cid:5)(cid:36)(cid:3)(cid:83)(cid:75)(cid:82)(cid:87)(cid:82)(cid:3)(cid:82)(cid:73)(cid:3)(cid:68)(cid:3)(cid:62)(cid:82)(cid:69)(cid:77)(cid:64)(cid:3)(cid:71)(cid:82)(cid:74)(cid:5)
(cid:82)(cid:73)(cid:3)(cid:68)(cid:3)(cid:75)(cid:82)(cid:85)(cid:86)(cid:72)(cid:5)
(cid:54)(cid:87)(cid:68)(cid:74)(cid:72)(cid:3)(cid:21)
(cid:53)(cid:72)(cid:70)(cid:82)(cid:81)(cid:86)(cid:87)(cid:85)(cid:88)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:47)(cid:82)(cid:86)(cid:86)
(cid:53)(cid:72)(cid:81)(cid:71)(cid:72)(cid:85)(cid:76)(cid:81)(cid:74)(cid:3)(cid:40)(cid:81)(cid:74)(cid:76)(cid:81)(cid:72)
(cid:48)(cid:72)(cid:86)(cid:75)(cid:3)(cid:62)(cid:82)(cid:69)(cid:77)(cid:64) (cid:36)(cid:87)(cid:87)(cid:85)(cid:17)(cid:3)(cid:20) (cid:36)(cid:87)(cid:87)(cid:85)(cid:17)(cid:3)(cid:21)
(cid:51)(cid:85)(cid:82)(cid:80)(cid:83)(cid:87)(cid:3)(cid:40)(cid:80)(cid:69)(cid:72)(cid:71)(cid:71)(cid:76)(cid:81)(cid:74)
(cid:5)(cid:36)(cid:3)(cid:62)(cid:3)(cid:3)(cid:64)(cid:3)(cid:62)(cid:3)(cid:3)(cid:64)(cid:83)(cid:75)(cid:82)(cid:87)(cid:82)(cid:3)(cid:82)(cid:73)(cid:3)(cid:68)
(cid:62)(cid:82)(cid:69)(cid:77)(cid:64)(cid:3)(cid:71)(cid:82)(cid:74)(cid:5)
Figure2. MethodOverview.Finetuning:Ourfinetuningisdividedintotwostages.Inthefirststage,werenderaseriesofimagesusing
differentattributevalues(e.g.,illuminationandpose).Wefeedthemintothetext-to-imagediffusionmodeltolearntokenembedding[Obj]
representingthesinglemeshusedfortraining. Inthesecondstage,weaddthetokensrepresentingindividualattributesintotheprompt
embedding. Thetwostagetrainingallowsustobetterdisentangletheindividualattributesagainst[Obj]. Inference: Attributescanbe
appliedtodifferentobjectsfortext-to-imagegeneration.
visualcontentthroughtext–e.g.,itwouldbeverydifficult tended the word embedding to a time-space conditioned
tochangetheilluminationdirectionbyapreciseanglesuch neuralmapperforbettergenerationwhilepreservingqual-
as11◦. ity. Similarly, Dreambooth[25]aimstoachievethesame
Recently,astheamountof3Ddataavailablesignificantly goal, but by using a repurposed token rarely used in text
increased, Liu et al. [16] introduced Zero-1-to-3, a diffu- andfinetuningtheentirediffusionmodelwithanadditional
sionmodeltrainedonvariousviewpointsof3Drendering constrainttopreventgenerativeloss. Therearenumerous
that enables viewpoint editing given an image of a single subsequentworksshowingimprovementsonfinetuningdif-
object. Similarly,workslikeDreamSparse[29]alsoemploy ferentlayers/weightsandbyimprovingthetrainingstrategy
diffusionmodelstosynthesizenovelviewsonopen-setcate- [11,15,17].
gories. Differentlyfromourapproach,thesetechniquesare
focusedonlyonobjectorientationandrelyonvast3Dshape
datasets. On the other hand, we investigate how to learn
severalcontinuousconcepts(e.g.illumination [ ],wing
Despitetheadvancesinaddingnewpersonalizedentities
pose [ ],dollyzoom [ ])thatcanbedirectlyusedin
toexistingmodels,fewworksfocusonlearninggeneralcon-
text-to-imagescenarios;i.e.wedon’tgenerateanimageto
ceptsthatcanbeappliedtoavarietyofscenarios. Aconcur-
thenchangetheorientationorilluminationlater,butinstead
rentwork,ViewNETI[5],isthefirsttolearnviewpointsasa
weuseContinuousWordsdirectlyontextprompts.
concept,butwehypothesisethatthe3Dawarenessoflarge
text-to-imagediffusionmodelsgoesfarbeyondmerelyview-
Learningnewconceptsondiffusionmodels. Withdiffu- points,allowingustoassociateandevencreateinteractions
sionmodelsbeingtrainedonunforeseenquantitiesinimages withmultiple3D-awareconceptslikeillumination,poseand
andtexts,astreamofworkfocusedonaddingspecificcon- cameraparametersatthesametime. Ourmethod,despite
cepts with very few data samples. For example, given a beingtrainedonlyusingasinglemesh,showssuperiorgener-
small set of images representing one particular object in- alizationproperties–whiletrainedtolearnilluminationand
stance, textual inversion learns a new word embedding to orientationfromrenderingsofasingledog,wearecapable
describetheobject,suchthatthewordcanbeappliedwith ofemployingthelearnedconceptstogeneratecars,horses
new text prompts for image generation [9]. NETI [1] ex- (Figure4),polarbears(Figure8),lions(Figure1)andsoon.
3extendedtoallowcontrolofmultipleattributesatthesame
time.
(cid:42)(cid:55)(cid:3)(cid:39)(cid:72)(cid:83)(cid:87)(cid:75) (cid:38)(cid:82)(cid:81)(cid:87)(cid:85)(cid:82)(cid:79)(cid:49)(cid:72)(cid:87) 3.2.ContinuousControl
Anaivewaytocontrolanattributeaistousesomerealistic
renderingenginetogenerateimagesoftheavailableobjects
that have the same value a = x, and then apply similar
approaches to previous works by assigning a token T to
(cid:47)(cid:76)(cid:81)(cid:72)(cid:68)(cid:85)(cid:87) (cid:38)(cid:82)(cid:81)(cid:87)(cid:85)(cid:82)(cid:79)(cid:49)(cid:72)(cid:87) x
(cid:40)(cid:91)(cid:87)(cid:85)(cid:68)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81) identifyimageswiththisparticularvalue. Thisisnotideal,
however,sinceaisoftencontinuousandhaveinfinitelymany
values–wewouldrequireanunfeasiblenumberoftokens
Figure3. ControlNetAugmentations.DepthControlNetisused
togainfine-grainedcontrolovertheseattributes.
forattributescreatingdirectshapechanges.LineartControlNetis
Therefore,weproposetoinsteadlearnacontinuousfunc-
appliedformoresubtlechangesthatcannotbereflectedbydepths
tiong (a):D →T thatmapsasetofattributesfromsome
(e.g.,illumination). ϕ
continuous domain D to the token embedding domain T.
Weusepositionalencodingtofirstcasteachattributea∈a
3.Method toahigherfrequencyspacebeforefeedingintothefunction,
whichisrepresentedbyaverysimple2-layerMLP.Theout-
3.1.Preliminaries
putofthisnetworkisnamedContinuous3DWordandwill
WedefineanimageIthatcapturesobjectOfromcategoryC allowuserstoeasilycontrolcontinuousattributesfromtext
asafunctionofseveralattributesI =f(a ,a ,a ,...,a ), promptsaugmentedbythesetokens. Finally, ourtraining
1 2 3 n
where a belongs to a vast set of image attributes A: objectivecanthenbeformulatedas:
i
s trh ia np sie c, /m exa trte inri sa icl sr ,efl she ac pti evi dty e, foro rmtat ai to ion n/t sr ,an es tcla .ti So on m,c eam ofe tr ha ei sn e-
argminE
(cid:20)(cid:13)
(cid:13)S (cid:0) Iˆ ,P(g (a))(cid:1) −I
(cid:13) (cid:13)2(cid:21)
. (2)
componentscanbetranslatedtoothercategorieswhileoth- θ,ϕ
Iˆ ϵ,a,a (cid:13) θ ϵ,a ϕ a(cid:13)
2
erscannot,soforsimplicityweassumetheyonlyworkfor 3.3.DisentanglingObjectIdentityandAttributes
asinglecategory. Intheexperimentalsectionofthisstudy,
In practice, when we only utilize a single object to learn
wewilldemonstratethatthedefinitionofcategoryforsome
attributes,adegeneratesolutionoccurswhendirectlyopti-
attributesisratherlooseandtheuseriscapableofgenerating
mizing(2),whereS treatsthesameobjectwithdifferent
imageswithcontinuouswordsdepictingobjectsverydiffer- θ
valuesforaasdifferentobjects. Thishindersthegeneraliza-
entfromtheonesseenduringtraining. Noticethatimages
tioncapabilityofchangingtheattributewhengivenanimage
annotatedwiththeattributesweareinterestedareveryrare,
ofanewobject. Tothisend,weproposetwostrategies,one
sothemodelsdonothaveaverypreciseknowledgeofthem,
duringtrainingandoneduringinference,todisentanglethe
exceptforwhatisalreadydescribedintext-imagepairs.
GivenanimagesetIO withimagescapturinganobject objectidentityandindividualattributes.
O, previous methods like Dreambooth [25], Custom Dif- Training: Learningidentityandattributes. Weprovidea
fusion [15], or Textual-Inversion [9] aim to minimize the simpleregularizationbyexplicitlyforcingthemodeltouse
followingobjective: thesameidentifierforimagesrepresentingthesameobject.
Theobjectivecanthusbeformulatedas:
(cid:20)(cid:13) (cid:13)2(cid:21)
E Iˆ ϵ,i,TO,Ii∈IO (cid:13) (cid:13)S θ(Iˆ ϵ,i,P(T O))−I i(cid:13) (cid:13) 2 , (1) argminE (cid:20)(cid:13) (cid:13)S (cid:0) Iˆ ,P(T ,g (a))(cid:1) −I (cid:13) (cid:13)2(cid:21) . (3)
θ,ϕ
Iˆ ϵ,a,a (cid:13) θ ϵ,a O ϕ a(cid:13)
2
whereS isaText-to-Imagediffusionmodel[24],Iˆ isa
θ ϵ,i
Optimizing both T and g (a) concurrently proved to be
noisedimageα I +σ ϵwithnoiseϵandnoiseschedulers O ϕ
t i t
difficultfromourexperiments(seeSection5). Wepropose
α ,σ . P(T ) is the prompt condition which contains a
t t O
a two-stage training strategy as depicted in Figure 2. For
tokenembeddingT usedasanidentifierofobjectO. In
O every available image in IO with varying a, we first use
practiseP(·)isthetextencoderfromCLIP.Thefine-tuned
thesamepromptconditionP(T )toassociatethemtothe
network S can then generate new images containing O O
θ
whengivenanewpromptconditionP′(T )andsomeGaus- same object, then learn the diffusion model parameters θ
O
andourContinuous3DWordsMLPg byusingtheprompt
siannoise. Unlikepreviousmethods,ourgoalisnottoadd ϕ
conditionP(T ,g (a)).
conceptsrepresentingspecificobjects,butratherhavethem O ϕ
describesomeattributesa ∈A,bylearningtodisentangle Inference: Negatively prompting object identifier. To
i
themusingasfewobjectsaspossiblewithinC –mostofthe furtherthedisentanglementofattributesagainstobjectiden-
timejustoneobjectsuffices. Ourmodelcanalsobeeasily tities. Weproposeasimpletrickduringinferencebyadding
4
(cid:71)(cid:72)(cid:86)(cid:68)(cid:69)(cid:16)(cid:75)(cid:87)(cid:83)(cid:72)(cid:39)
(cid:71)(cid:72)(cid:86)(cid:68)(cid:69)(cid:16)(cid:87)(cid:85)(cid:68)(cid:72)(cid:81)(cid:76)(cid:47)(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:53)(cid:72)(cid:81)(cid:71)(cid:72)(cid:85)(cid:76)(cid:81)(cid:74)(cid:86) (cid:53)(cid:72)(cid:86)(cid:88)(cid:79)(cid:87)(cid:86)
(cid:38)(cid:82)(cid:81)(cid:87)(cid:85)(cid:82)(cid:79)(cid:49)(cid:72)(cid:87)(cid:3)(cid:11)(cid:19)(cid:17)(cid:24)(cid:12) (cid:38)(cid:82)(cid:81)(cid:87)(cid:85)(cid:82)(cid:79)(cid:49)(cid:72)(cid:87)(cid:3)(cid:11)(cid:20)(cid:17)(cid:19)(cid:12) (cid:50)(cid:88)(cid:85)(cid:86)
(cid:68)(cid:12)
(cid:5)(cid:36)(cid:3)(cid:62)(cid:3)(cid:3)(cid:3)(cid:3)(cid:64)(cid:3)(cid:62)(cid:3)(cid:3)(cid:3)(cid:3)(cid:64)
(cid:83)(cid:75)(cid:82)(cid:87)(cid:82)(cid:3)(cid:82)(cid:73)(cid:3)(cid:68)(cid:81)
(cid:72)(cid:68)(cid:74)(cid:79)(cid:72)(cid:3)(cid:76)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)
(cid:73)(cid:82)(cid:85)(cid:72)(cid:86)(cid:87)(cid:5)
(cid:5)(cid:36)(cid:3)(cid:62)(cid:3)(cid:3)(cid:3)(cid:3)(cid:64)(cid:3)(cid:62)(cid:3)(cid:3)(cid:3)(cid:3)(cid:64)
(cid:83)(cid:75)(cid:82)(cid:87)(cid:82)(cid:3)(cid:82)(cid:73)(cid:3)(cid:68)
(cid:83)(cid:68)(cid:85)(cid:85)(cid:82)(cid:87)(cid:3)(cid:76)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)
(cid:73)(cid:82)(cid:85)(cid:72)(cid:86)(cid:87)(cid:5)
(cid:69)(cid:12)
(cid:5)(cid:36)(cid:3)(cid:62)(cid:3)(cid:3)(cid:3)(cid:3)(cid:64)(cid:3)(cid:62)(cid:3)(cid:3)(cid:3)(cid:3)(cid:64)
(cid:83)(cid:75)(cid:82)(cid:87)(cid:82)(cid:3)(cid:82)(cid:73)(cid:3)(cid:68)(cid:3)(cid:75)(cid:82)(cid:85)(cid:86)(cid:72)(cid:5)
(cid:5)(cid:36)(cid:3)(cid:62)(cid:3)(cid:3)(cid:3)(cid:3)(cid:64)(cid:3)(cid:62)(cid:3)(cid:3)(cid:3)(cid:3)(cid:64)
(cid:83)(cid:75)(cid:82)(cid:87)(cid:82)(cid:3)(cid:82)(cid:73)(cid:3)(cid:68)(cid:3)(cid:87)(cid:68)(cid:91)(cid:76)(cid:5)
(cid:70)(cid:12)
(cid:5)(cid:36)(cid:3)(cid:62)(cid:3)(cid:3)(cid:3)(cid:3)(cid:64)(cid:3)(cid:83)(cid:75)(cid:82)(cid:87)(cid:82)
(cid:82)(cid:73)(cid:3)(cid:68)(cid:3)(cid:70)(cid:82)(cid:80)(cid:73)(cid:82)(cid:85)(cid:87)(cid:68)(cid:69)(cid:79)(cid:72)
(cid:69)(cid:79)(cid:88)(cid:72)(cid:3)(cid:70)(cid:75)(cid:68)(cid:76)(cid:85)(cid:3)(cid:76)(cid:81)(cid:3)(cid:68)(cid:81)
(cid:82)(cid:73)(cid:73)(cid:76)(cid:70)(cid:72)(cid:5)
Figure4. QualitativeComparisons.WecompareourContinuous3DWordstrainedunderthreesettingsagainstControlNetofvarious
strengths.Notethatthedollyzoomsetupwastrainedwithtrainedwithmultiplechairmeshes,sowegiveadditionalControlNetbymanually
pickingthechairrenderingthatbestfollowstheprompt(i.e.,“comfortable”andin“theoffice”).
the object identity as a negative prompt. Specifically, for reflected on shape changes (e.g., wing pose), we directly
eachsamplingstep,weswapthenull-textembeddingused rendertheground-truthdepthmapstouseasthecondition
by classifier-free guidance with T . Intuitively, since our forControlNet. Ontheotherhand,forattributesthatcannot
O
traininghappenedmostlyusingO,wewanttodisincentivize be reflected directly from depths (e.g., illumination), we
themodeltogenerateimagescontainingsuchobject. firstrendertheimageswithouttextures,thenusealineart
extractortoobtaina“sketch”oftheimage. Thiscaptures
3.4.ControlNetAugmentation subtle changes such as shades and shadows in the pixel
space,whichcanthenbeusedastheconditionforLineart
Topreventthefine-tuningprocessfromoverfittingtoasim-
ControlNet.
plewhitebackgroundsandpre-definedobjecttextures,we
augmentthebackgroundsandtexturesintherenderingpro- Weaddadditionalpromptsdescribingtheobjectappear-
cess. However,directlydoingthisinsimulationenginesis anceandbackgroundduringtheControlNetgeneration. Itis
timeconsumingspeciallyifoneistargettingrealisticscenes. importanttonotethatpromptsdeviatingawaytoomuchfrom
Thus, we propose an automated solution by utilizing pre- theoriginalmeshcategorywouldleadtodegenerateimages,
trainedControlNets. soourpromptsareingeneralverysimpleandstraightfor-
Figure3showstwotypesofControlNetaugmentations ward (details in supplemental material). We include the
weusedinourframework. Forattributesthatcanbedirectly ControlNetgeneratedimagesasasmallsetofdataaugmen-
5tation,andweusethesamepromptwhichweusetoguide
ControlNetgenerationtoguideourStage2fine-tuning.
4.Experiments
Weuseoff-the-shelfStableDiffusionv2.1[24]astheback-
bone of our method. We resort to the recent Low-Rank
Adaptation(LoRA)[14]forthefine-tuningofthedenoising
U-Netandtextencoder,allowingustotrainonasingleA10
GPUoccupyingroughly16GBofmemory. Thankstothe
low-rankoptimization, ourmodelshaveaverysmallsize
(approximately6MB).Trainingtimevariesbythecomplex-
ityofthesingle/multipleattributestolearn,butfallswithin
15k to 20k steps, which generally takes around 3-4 hours
inasingleGPU.ForControlNetaugmentation,weusethe
officialimplementationofControlNetv1.1[30].
WeimplementourContinuous3Dwordsunderfivedif-
ferentattributesettings. Forsingleattributesweimplement
1) illumination [ ] using a single dog mesh, 2) wing
pose [ ]usingasingleanimateddovemesh,and3)Dolly
zoom [ ]withfivePix3Dchairs[28]. Formulti-concepts,
wetrain4)illuminationandobjectorientation [ ]+[ ]
usingasingledogmeshand5)wingposeandorientation
[ ]+[ ]usingasingleanimateddovemesh. Settings1)
and4)uselineartimages[6]whiletheothersusedepthmap
tocomputetheControlNetbackgroundaugmentation(see
Section3.4).
4.1.ComparisonwithBaselines
BaselineDesign. Wedesignaverycompetivebaselinethat
enables fine-grained attribute control in image generation
bycombiningthemeshtrainingdataweusedinourexperi-
ments,arenderingengineandControlNet[30]. Specifically,
wetakeanoveltextpromptforatrainingobject,andgrab
acorrespondingconditionmapinthetrainingsetrendered
with the intended attributes. For example, if the prompt
is a [ ] eagle flying in a forest, we select
the frame of the dove mesh that corresponds to the user-
prescribedwingpose [ ],renderitsdepthmapusinga
rendering engine and pass it through ControlNet with the
same prompt but removing the continuous 3D word. The
strengthoftheControlNetguidanceisacriticalhyperparam-
eter—increaseinstrengthcouldincreasetheaccuracyof
reflectingtheattributebutdecreasetherobustnesstogeneral-
izetothetext-promptintendedobject. Therefore,wepresent
theControlNetbaselinewithbothfullandhalfstrengthin
terms of guidance. We also explored an interpolation of
null-textembedding[18]baseline,buttheresultsfailedto
Figure 5. Disentangling Multiple Attributes. We show four captureeventhesimplestattributes, sotheywereomitted
examplesofcontrollingmultipleContinuous3Dwordsinaddition fromouranalysis.
totextdescriptions. Thefirst6rowsweretrainedwithasingle
QuantitativeResults. Duetothecomplexityandabstract
goldenretrievermesh,whilethebottom6weretrainedwithasingle
natureoftheattributesweareanalyzing,automaticallymea-
animateddove.
suring whether a generated image reflects a set of values
6
(cid:78)(cid:85)(cid:68)(cid:83)(cid:3)(cid:87)(cid:86)(cid:72)(cid:85)(cid:82)(cid:73)(cid:3)(cid:68)(cid:3)(cid:81)(cid:76)(cid:3)(cid:15)(cid:70)(cid:76)(cid:87)(cid:86)(cid:76)(cid:79)(cid:68)(cid:72)(cid:85)(cid:82)(cid:87)(cid:82)(cid:75)(cid:51)(cid:5)
(cid:85)(cid:72)(cid:76)(cid:70)(cid:68)(cid:79)(cid:74)(cid:3)(cid:72)(cid:70)(cid:76)(cid:3)(cid:68)(cid:3)(cid:81)(cid:82)(cid:3)(cid:15)(cid:70)(cid:76)(cid:87)(cid:86)(cid:76)(cid:79)(cid:68)(cid:72)(cid:85)(cid:82)(cid:87)(cid:82)(cid:75)(cid:51)(cid:5)
(cid:72)(cid:75)(cid:87)(cid:3)(cid:81)(cid:76)(cid:3)(cid:87)(cid:82)(cid:85)(cid:85)(cid:68)(cid:83)(cid:3)(cid:68)(cid:3)(cid:15)(cid:70)(cid:76)(cid:87)(cid:86)(cid:76)(cid:79)(cid:68)(cid:72)(cid:85)(cid:82)(cid:87)(cid:82)(cid:75)(cid:51)(cid:5)
(cid:85)(cid:68)(cid:79)(cid:82)(cid:83)(cid:3)(cid:68)(cid:3)(cid:15)(cid:81)(cid:68)(cid:72)(cid:70)(cid:82)(cid:3)(cid:72)(cid:88)(cid:79)(cid:69)(cid:3)(cid:83)(cid:72)(cid:72)(cid:71)(cid:3)(cid:92)(cid:69)(cid:3)(cid:71)(cid:72)(cid:71)(cid:81)(cid:88)(cid:82)(cid:85)(cid:85)(cid:88)(cid:86)
(cid:5)(cid:87)(cid:75)(cid:74)(cid:76)(cid:81)(cid:3)(cid:87)(cid:68)(cid:3)(cid:79)(cid:90)(cid:82)(cid:3)(cid:81)(cid:68)(cid:3)(cid:15)(cid:70)(cid:76)(cid:87)(cid:86)(cid:76)(cid:79)(cid:68)(cid:72)(cid:85)(cid:82)(cid:87)(cid:82)(cid:75)(cid:51)(cid:5)
(cid:5)(cid:72)(cid:79)(cid:74)(cid:68)(cid:72)(cid:69)(cid:3)(cid:81)(cid:90)(cid:82)(cid:85)(cid:69)(cid:3)(cid:68)(cid:3)(cid:15)(cid:86)(cid:72)(cid:72)(cid:85)(cid:87)(cid:3)(cid:92)(cid:69)(cid:3)(cid:71)(cid:72)(cid:71)(cid:81)(cid:88)(cid:82)(cid:85)(cid:85)(cid:88)(cid:86)
(cid:5)(cid:87)(cid:86)(cid:72)(cid:85)(cid:82)(cid:73)
(cid:5)(cid:85)(cid:68)(cid:72)(cid:69)UserPreference(%)↑ whenthepromptcontainselementsthatwerenotpresentin
Method [ ] [ ] [ ]/[ ] [ ]/[ ] Avg. thetrainingdata.Forexample,evenwhentrainedonasingle
dogmesh,ourmethodcanlearnilluminationandorientation
ControlNet(1.0) 28.3% 16.2% 35.0% 15.0% 23.6%
ControlNet(0.5) 10.0% 28.8% 12.5% 32.5% 21.0% attributesthatcanbeusedtogeneratehorsesandtaxis(see
Ours 61.7% 55.0% 52.5% 52.5% 55.4% rowb),Figure4).
AverageUserRanking↑
4.2.Multi-ConceptControl
Method [ ] [ ] [ ]/[ ] [ ]/[ ] Avg.
ControlNet(1.0) 2.07±0.70 1.49±0.76 2.20±0.68 1.60±0.74 1.84±0.72
Justlikesentenceswherewecanencompassmultiplewords,
ControlNet(0.5) 1.38±0.66 2.11±0.67 1.38±0.70 2.06±0.76 1.73±0.70
but each disentangled from one another when controlling
Ours 2.55±0.62 2.40±0.73 2.43±0.67 2.33±0.77 2.43±0.70
theimagegeneration,ourContinuous3DWordscandothe
Table1.Userstudyresults.Weaskeduserstorankthreeimages same. WeshowfourexamplesinFigure5,twofrom [ ]
accordingtotheirpreferenceandhowwelltheyfollowedthegiven and [ ],andtwo [ ]and [ ],wherewecankeep
conditions–textpromptandoneortwocontinuouscontrols.The oneattributefixedbutchangetheotherwithoutsabotaging
controlsweredescribedbyrepresentativeimages(i.e.arrowsfor
thequalityofimagegenerations. Theycanbejointlyused
orientation,shadedsphereforillumination,etc.).Seesupplemen-
withcomplexpromptsdescribingthebackgroundandobject
talmaterialformoredetails. Weevaluatefourdifferentcontrol
texture. Moreover,whileallthesewordsarelearnedfroma
types: wingpose [ ],illumination [ ],wingpose [ ]+
singlemesh,wecaneasilytransfertheattributetoobjects
orientation [ ],andillumination [ ]+orientation [ ].
withfairlyclosesemantics(e.g.,alabradormeshtoapolar
Cellscoloredasredandyellowrepresentthebestandsecondbest
method,respectively.Ourmethodwasselectedasthefavoritefor bear,oradovemeshtoaparrot).
themajorityofusersinallevaluatedsetups.
4.3.RealWorldImageEditing
is a considerable challenge. Thus, following previous pa-
Our Continuous 3D Words can be directly applied to real
pers [12, 25, 30], we create a user study to evaluate the
worldimagestoperformediting. Todoso,wesimplyhave
qualityofthegeneratedimageswhilecontrollingseveralat-
toencodearealworldimagetoararetokenviaDreambooth
tributes. Foreachsetting1,2,4,and5,werandomlysample
[25]. Then,weonlyhavetousethattokeninconjunction
asetof3Dconditionsandprompts,generatingover60ques-
withourContinuous3DWordstogeneratetheeditedimage.
tionspersetting. Similarlytotheuserstudiesabove,wethen
Weshow8examplesinFigure6, changing [ ], [ ]
invite20participants,showingthemallthegivenconstraints
, [ ]. Aswecansee, theDreamboothtokenpreserves
wewanttheimagetopertain(promptsandattributes),and
mostoftheimageappearance,whileourContinuousWords
askthemtorankeachimagefrombesttoworst.
understandsandbringseditstothemainsubject.
Table1showstheresultsinboththepercentageofuser
ComparingwithZero-1-to-3Whileourapproachisisfo-
preference(imagechoseasthebestone)andtheoverallrank-
cusedonenhancingtext-to-image,giventhecapabilitiesof
ing. Bestresultsarehighlightedasred. OurContinuous3D
real-worldimageediting,wecanusethesamesetuptopro-
Wordswonthemajorityofvotes(over50%)inallanalyzed
videacomparisonwithonlywithobjectorientationchanges
scenarios. Interestingly,thesecondbest(ashighlightedin
(ResultsinFigure6). NoticethatZero1-to-3[16]operates
yellow) alternates between the two guidance strengths of
in foreground-only images so we also had to segment the
ControlNet. Forwing-posebasedcontrols,havingaweaker
object[7],inpaintthebackground[24]andplacethenovel
controldiminishesthestrongpriorofferedbythedepthmap,
orientationbacktointotheimage. Eachoneofthesesteps
resultingininaccurateposes. Conversely,strongguidance
containerrorsthat,whencompounded,hurtthequalityofthe
forilluminationforcesControlNettogenerateobjectsaround
finalresult. Moreimportantly,ourmethodallowscontrols
theshadowoutline(furthershowninFigure4). Differently
beyondorientationchangeswithoutrelyingonmassive3D
fromthebaselines,ourtrainingstrategyisone-size-fits-all
datasets.
withoutanyhyperparameterrequired.
QualitativeAnalysesWepresentadetailedcomparisonof
5.DiscussionandLimitations
Continious3DWordsagainstControlNetofdifferentguid-
ancestrengthsinFigure4. Weshowtheresultsunderthree WhyNotDiscreteTokens? Thebenefitsoffittingasingle
trainingsettings: a)wingposeandorientation, b)illumi- MLPinsteadofmultipletokensonanattributewithdifferent
nationandorientation,andc)dollyzoom. Weobservethat valuesaretwo-fold. First,theMLPlearnsacontinuousfunc-
thedollyzoomsetupwasaharderconcepttotrainandhad tion, allowing us to interpolate between two training data
tobedoneusingfivechairs. Wealsomanually“helped”the sampleswhereasfittingmultipletokensleadstotwodata-
ControlNetbaselinebymanuallypickingthechairthatbest pointsastwodiscretemappings. Second,finetuningamodel
followedourtextpromptastheconditionimage. Moreim- tolearnmultiplecustomtokenssimultaneouslyisveryhard.
portantly,theControlNetbaselinessignificantlydeteriorate ThesebenefitsareillustratedinFigure7. Weshowacom-
7RealWorldImage Continuous3DWords RealWorldImage Continuous3DWords
Zero123* Zero123*
Comparison Comparison
Figure6. RealWorldresults&Comparisonw/Zero1-to-3.WelearntokensrepresentingsingleimagesandusealongwithContinuous
3DWordsforimageediting.Thelearnedimagetokenencodesmostoftheimageappearance,whiletheContinuous3DWordmodifiesits
relevantaspects.Zero123,ontheotherhand,tendstoyieldimageswherethemodifiedobjectisdeformedornotproperlyharmonized.
(cid:90)(cid:18)(cid:82)(cid:3)(cid:55)(cid:90)(cid:82)(cid:16)(cid:54)(cid:87)(cid:68)(cid:74)(cid:72) (cid:90)(cid:18)(cid:82)(cid:3)(cid:38)(cid:82)(cid:81)(cid:87)(cid:85)(cid:82)(cid:79)(cid:49)(cid:72)(cid:87) (cid:90)(cid:18)(cid:82)(cid:3)(cid:62)(cid:82)(cid:69)(cid:77)(cid:64)(cid:3)(cid:49)(cid:72)(cid:74)(cid:17)(cid:3)(cid:51)(cid:85)(cid:82)(cid:80)(cid:83)(cid:87) (cid:41)(cid:88)(cid:79)(cid:79)(cid:3)(cid:51)(cid:76)(cid:83)(cid:72)(cid:79)(cid:76)(cid:81)(cid:72)
Interpolation
(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)
(cid:48)(cid:72)(cid:86)(cid:75)
(cid:5)(cid:76)(cid:81)(cid:3)(cid:68)(cid:3)(cid:73)(cid:82)(cid:85)(cid:72)(cid:86)(cid:87)(cid:3)(cid:86)(cid:88)(cid:85)(cid:85)(cid:82)(cid:88)(cid:81)(cid:71)(cid:72)(cid:71)(cid:3)(cid:69)(cid:92)(cid:3)(cid:87)(cid:85)(cid:72)(cid:72)(cid:86)(cid:15)(cid:3)(cid:68)(cid:3)(cid:62)(cid:3)(cid:3)(cid:3)(cid:3)(cid:64)(cid:3)(cid:83)(cid:75)(cid:82)(cid:87)(cid:82)(cid:3)(cid:82)(cid:73)(cid:3)(cid:68)(cid:3)(cid:69)(cid:85)(cid:82)(cid:90)(cid:81)(cid:3)(cid:69)(cid:72)(cid:68)(cid:74)(cid:79)(cid:72)(cid:5)
Figure7. Interpolatingcontinuous3Dwords.Wepresenttwo
(cid:5)(cid:68)(cid:3)(cid:62)(cid:3)(cid:3)(cid:3)(cid:3)(cid:64)(cid:3)(cid:83)(cid:75)(cid:82)(cid:87)(cid:82)(cid:3)(cid:82)(cid:73)(cid:3)(cid:68)(cid:3)(cid:87)(cid:85)(cid:88)(cid:70)(cid:78)(cid:5)
resultsofinterpolation,onewithdiscretetokensandonewithour
Continuous3DWords.Ourmethodpreservestheattributesbetter
Figure8. AblationStudy.Wepresentourablationsfor:w/otwo-
andenablesinterpolationbetweentwovalues. stagetraining,w/oControlNetaugmentation,w/o[obj]asnegative
prompt,andourfullpipeline.Noticethat,whenthepromptdeviates
alotfromthetrainingdata(truckinprompt,meshofadogfor
training),theablatedversionfailstofollowtheprompt. Without
parisonoflearningasingleContinuous3DWord [ ]and
two-stagetraining,itignoresthepromptandcreatesadog;without
18discretetokensfordifferentvaluesofawingpose. Both
theotherpartsityieldsdeformedshadowsanddog-shapedtrucks.
werelearnedwiththesametrainingmethod(2-stagewith
ControlNetaugmentation). Duringinference,wepresentthe
generatedresultsofthreeattributevaluesthatarepresentin
ure7donotfollowthetrainingdataandsometimesgenerate
thetrainingset,aswellastheresultswhenyouinterpolate
acompletelydifferentpose(thirdcolumn,secondrow). On
the value in between. Interpolation is straightforward for
theotherhand,ourimagescloselyfollowtheposeprescribed
ourcasewherewejustinputtheintermediatevalueintoour
bytheuser(toprow)whileyieldingappealingimageseven
Continuous3DWordsMLP.Fordiscretetoken,wetakethe
whenthevalueswerenotseenduringtraining(columns2,4
interpolation of the two nearest discrete bins. Notice that
and6;secondrow).
thediscretetokenshavedifficultynotonlyininterpolating
resultsbutalsoinlearningalltheconceptssimultaneously. Ablationstudy. Figure8showsanablationofourtrain-
NoticehowthewingsoftheeagleinthesecondrowofFig- ing strategy. We remove each component of our training
8
gniniarT
etercsiD
suounitnoC
ataD
snekoT
sdroW
D3
noitanimullI
esoPgniW
tpecnoC-itluM
noitatneirOa) b)
“Monetpainting”styleimposedbyourprompt. Second,the
generatedobjectmaysometimesstilloverfittothetraining
set. Inb),theT-Rexhadfourfeetonthegroundinsteadof
standingwithtwoclawsinair–anattributethatissimilarto
thetrainingdogmeshusedtolearnillumination.
"a [ ] photo of a "a [ ] photo of a
white bird on sand" colorful bird" 6.Conclusion
Figure9. Conditionv.s.AccuracyinUserStudy.a)showstwo WepresentedContinuous3DWords,aframeworkthatallows
imagesgeneratedbytheprompt“a [ ]photoofawhitebird us to learn 3D-aware attributes reflected in renderings of
onsand”.b)showstwoimagesgeneratedbytheprompt“a [ ]
meshesasspecialwords,whichcanthenbeinjectedintothe
photoofacolorfulbird”. Leftisours,rightisControlNet(1.0).
textpromptsforfine-grainedtext-to-imagegeneration. We
Userspreferredrightoverleftforboth.
madeanextensivestudyonlearningbothsingleandmultiple
(cid:68)(cid:12) (cid:69)(cid:12) continuouswordsandshowthatwecancontrolchallenging
attributes.Withthelightweightdesignandpromisingresults,
wehopethatthisworkopensupinterestingapplicationsin
thevisioncommunitytocreatetheirown3Dwordswitha
singlemeshandanaccessiblerenderingengine.
Futurework. Identifyingwhichdataneedstobeusedfor
(cid:5)(cid:68)(cid:3)(cid:62)(cid:3)(cid:3)(cid:3)(cid:3)(cid:64)(cid:3)(cid:62)(cid:3)(cid:3)(cid:3)(cid:3)(cid:64)(cid:3)(cid:48)(cid:82)(cid:81)(cid:72)(cid:87) (cid:5)(cid:68)(cid:3)(cid:62)(cid:3)(cid:3)(cid:3)(cid:3)(cid:64)(cid:3)(cid:62)(cid:3)(cid:3)(cid:3)(cid:3)(cid:64)(cid:3)(cid:83)(cid:75)(cid:82)(cid:87)(cid:82)
specificattributesandtrainingmultiplemodelsforeachof
(cid:51)(cid:68)(cid:76)(cid:81)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)(cid:82)(cid:73)(cid:3)(cid:17)(cid:17)(cid:17)(cid:17)(cid:5) (cid:82)(cid:73)(cid:3)(cid:68)(cid:3)(cid:55)(cid:16)(cid:53)(cid:72)(cid:91)(cid:3)(cid:17)(cid:17)(cid:17)(cid:17)(cid:5)
themisacumbersometask.Ontheotherhand,astheamount
Figure10. Failurecases.a)showstwoimagesgeneratedbythe
of3Ddataavailablesignificantlyincreases,webelievethat
prompt“a [ ] [ ]MonetPaintingof.....”.b)showstwoimages
aninterestingdirectionistotraingeneralmodelsthathandle
generatedbytheprompt“a [ ] [ ]photoofaT-Rex”. Both
multipleattributesontheirown, withouttheneedtotrain
yieldedsuboptimalresults.
attribute-specificnetworks.
strategy(w/otwo-stagetraining,w/oControlNetaugmenta-
References
tion,w/o[obj]asnegativeprompt)andcompareitwithour
fullpipeline. Twoexamplesarepresented: onewherethe [1] Yuval Alaluf, Elad Richardson, Gal Metzer, and Daniel
promptissimilartothetrainingandanotheronewhereitis Cohen-Or. A neural space-time representation for text-to-
significantlydifferent. Withoutthetwo-stagetraining,the image personalization. arXiv preprint arXiv:2305.15391,
modelfailstodisentangleobjectidentitywithourattributes, 2023. 3
hinderingthegeneralizationcapabilitytonewobjects. This [2] James Betker, Gabriel Goh, Li Jing, † TimBrooks, Jian-
isparticularlynoticeableforthebottomrowwhenthetext fengWang,LinjieLi,†LongOuyang,†JuntangZhuang,†
promptisatruckbutadogsimilartothetrainingmesh JoyceLee,†YufeiGuo,†WesamManassra,†PrafullaDhari-
wal,†CaseyChu,†YunxinJiao,andAdityaRamesh. Improv-
isgeneratedwhenthetwo-stagetrainingisremoved. With-
ingimagegenerationwithbettercaptions,2023. 2
outControlNet,thefinetuningprocessoftenoverfitstothe
[3] TimBrooks,AleksanderHolynski,andAlexeiA.Efros. In-
backgroundtrainingrenderings,resultinginaninabilityto
structpix2pix:Learningtofollowimageeditinginstructions.
generaterealisticbackgrounds. Finally,adding[obj]asthe
InCVPR,2023. 2
negativepromptservesasaminorimprovementinfurther
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
disentanglingboththebackgroundsandobjectshapeseen
biah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakantan,
duringtraining,resultinginamoreaestheticimage. PranavShyam,GirishSastry,AmandaAskell,etal.Language
modelsarefew-shotlearners.Advancesinneuralinformation
Conditionv.s.GeneratedAccuracyinUserStudy.During
processingsystems,33:1877–1901,2020. 2
ouruserstudy,werealizethatoccasionallyusersselectthe
[5] JamesBurgess,Kuan-ChiehWang,andSerenaYeung. View-
imagewhichmorestrictlyfollowstheexactconditionsover
point textual inversion: Unleashing novel view synthe-
the image that is more physically probable. For example,
sis with pretrained 2d diffusion models. arXiv preprint
inbothcasesa)andb)shownbyFigure9,theheadofthe arXiv:2309.07986,2023. 3
birdisgeneratedpoorly,butuserspreferredthemastheyare [6] CarolineChan,FredoDurand,andPhillipIsola. Learningto
“whiter”and“morecolorful”. generatelinedrawingsthatconveygeometryandsemantics.
InCVPR,2022. 6
FailureCases. WepresentinFigure10twoexamplesof [7] Danielgatis. Danielgatis/rembg:Rembgisatooltoremove
typicalfailurecasesinourresults. First,ourmodelcurrently imagesbackground. 7
failsonmoredifficultscenarioswherethestyleisgivenby [8] JacobDevlin,Ming-WeiChang,KentonLee,andKristina
the text prompt. In a), the image cannot fully reflect the Toutanova. Bert: Pre-training of deep bidirectional
9transformers for language understanding. arXiv preprint Zero-shottext-to-imagegeneration. InInternationalConfer-
arXiv:1810.04805,2018. 2 enceonMachineLearning,pages8821–8831.PMLR,2021.
[9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, 2
Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. [23] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,
An image is worth one word: Personalizing text-to-image andMarkChen. Hierarchicaltext-conditionalimagegener-
generation using textual inversion. In The Eleventh Inter- ationwithcliplatents. InarXivpreprintarXiv:2204.06125,
nationalConferenceonLearningRepresentations,2022. 3, page3,2022. 2
4 [24] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Patrick Esser, and Bjo¨rn Ommer. High-resolution image
Xu,DavidWarde-Farley,SherjilOzair,AaronCourville,and synthesiswithlatentdiffusionmodels,2021. 2,4,6,7
YoshuaBengio. Generativeadversarialnets. Advancesin [25] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
neuralinformationprocessingsystems,27,2014. 2 MichaelRubinstein,andKfirAberman. Dreambooth:Fine
[11] LigongHan,YinxiaoLi,HanZhang,PeymanMilanfar,Dim- tuningtext-to-imagediffusionmodelsforsubject-drivengen-
itrisMetaxas,andFengYang. Svdiff: Compactparameter eration.InProceedingsoftheIEEE/CVFConferenceonCom-
spacefordiffusionfine-tuning. ICCV,2023. 3 puterVisionandPatternRecognition,pages22500–22510,
[12] AmirHertz,RonMokady,JayTenenbaum,KfirAberman, 2023. 2,3,4,7
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im- [26] ChitwanSaharia,WilliamChan,SaurabhSaxena, LalaLi,
ageeditingwithcrossattentioncontrol. InarXivpreprint JayWhang,EmilyLDenton,KamyarGhasemipour,Raphael
arXiv:2208.01626,2022. 2,7 GontijoLopes,BurcuKaragolAyan,TimSalimans,etal.Pho-
[13] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu- torealistictext-to-imagediffusionmodelswithdeeplanguage
sionprobabilisticmodels. Advancesinneuralinformation understanding. AdvancesinNeuralInformationProcessing
processingsystems,33:6840–6851,2020. 2 Systems,35:36479–36494,2022. 2
[14] EdwardJHu,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi, [27] JiamingSong,ChenlinMeng,andStefanoErmon. Denoising
Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low- diffusionimplicitmodels. InInternationalConferenceon
rankadaptationoflargelanguagemodels. InInternational LearningRepresentations,2020. 2
ConferenceonLearningRepresentations,2021. 2,6 [28] XingyuanSun,JiajunWu,XiumingZhang,ZhoutongZhang,
[15] NupurKumari,BingliangZhang,RichardZhang,EliShecht- ChengkaiZhang, TianfanXue, JoshuaBTenenbaum, and
man,andJun-YanZhu. Multi-conceptcustomizationoftext- WilliamTFreeman. Pix3d:Datasetandmethodsforsingle-
to-imagediffusion. InProceedingsoftheIEEE/CVFConfer- image3dshapemodeling. InProceedingsoftheIEEEcon-
enceonComputerVisionandPatternRecognition(CVPR), ferenceoncomputervisionandpatternrecognition,pages
2023. 3,4 2974–2983,2018. 6
[16] RuoshiLiu,RundiWu,BasileVanHoorick,PavelTokmakov, [29] PaulYoo,JiaxianGuo,YutakaMatsuo,andShixiangShane
SergeyZakharov,andCarlVondrick. Zero-1-to-3:Zero-shot Gu. Dreamsparse: Escaping from plato’s cave with
oneimageto3dobject,2023. 3,7 2d diffusion model given sparse views. arXiv preprint
[17] ZhihengLiu,YifeiZhang,YujunShen,KechengZheng,Kai arXiv:2306.03414,2023. 3
Zhu,RuiliFeng,YuLiu,DeliZhao,JingrenZhou,andYang [30] LvminZhang,AnyiRao,andManeeshAgrawala. Adding
Cao. Cones2:Customizableimagesynthesiswithmultiple conditional control to text-to-image diffusion models. In
subjects. arXivpreprintarXiv:2305.19327,2023. 3 IEEEInternationalConferenceonComputerVision(ICCV),
[18] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, 2023. 2,6,7
and Daniel Cohen-Or. Null-text inversion for editing real
images using guided diffusion models. arXiv preprint
arXiv:2211.09794,2022. 6
[19] ChongMou,XintaoWang,LiangbinXie,JianZhang,Zhon-
gangQi,YingShan,andXiaohuQie. T2i-adapter:Learning
adapterstodigoutmorecontrollableabilityfortext-to-image
diffusionmodels. arXivpreprintarXiv:2302.08453,2023. 2
[20] GauravParmar,KrishnaKumarSingh,RichardZhang,Yijun
Li,JingwanLu,andJun-YanZhu. Zero-shotimage-to-image
translation. InACMSIGGRAPH2023ConferenceProceed-
ings,pages1–11,2023. 2
[21] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,
SharanNarang,MichaelMatena,YanqiZhou,WeiLi,and
PeterJLiu. Exploringthelimitsoftransferlearningwith
aunifiedtext-to-texttransformer. TheJournalofMachine
LearningResearch,21(1):5485–5551,2020. 2
[22] AdityaRamesh, MikhailPavlov, GabrielGoh, ScottGray,
ChelseaVoss,AlecRadford,MarkChen,andIlyaSutskever.
10Learning Continuous 3D Words for Text-to-Image Generation
Supplementary Material
7.ControlNetAugmentationPrompts
Condition 1: Illumination under the same setting as the sphere:
Both our depth and lineart ControlNet augmentations for
traininghadtobecarefullydesignedastoolargeofadevia-
tionmayleadtowronglysynthesizedimages. Wedescribe
thepromptsusedfortheControlNetaugmentationsforour Condition 2: The object pointing in the same direction as this arrow.
settings1)wingpose,and2)dollyzoom,and3)illumination.
Wingpose [ ]. Sincethecorrectpositionofthewings
is particularly important for this case, we engineered the
promptsthathelpedthemostinreflectingbothwingsduring
Condition 3: A text prompt "in a forest park surrounded by trees, a brown beagle"
generation, which are {with two wings, flying}.
After seeing the 3 conditions I want you to look at three images:
Wealsoaddedtwotime-of-daypromptstoincreasetheva-
riety of backgrounds. Therefore, the overall prompts are (a) (b) (c)
generatedrandomlywith:a bird {with two wings,
flying} on a {rainy, sunny} day.
Dollyzoom [ ] . As there already comprises 5 types
of chair inside the training dataset for dollyzoom we
focus on augmenting the background with ControlNet.
Our overall prompts are generated by: a chair {in
the Acropolis, in a forest, under the
Which image most closely follows ALL the above conditions? Which one is the worst?
snow, on a beach, in Times Square, in a
department store}. Figure11. Orientation/IlluminationSurvey.
Illumination [ ]. WerealizethatLineartControlNetfor
shadowgenerationworksbestwithControlNetguidance0.6.
Condition 1: Wing orientation in the same angle as this silhouette. IMPORTANT: only look
However,thisweakerstrengthlimitstheabilityforControl- at the wing orientation, not the other features of this silhouette.
Nettokeeptheshadow/illuminationconsistencyifadditional
backgroundsaredescribed(e.g.,addingbackgrounddescrip-
tions like in a forest during Augmentation). Hence,
ourControlNetaugmentationonlyfocusesonavarietyof
Condition 2: The object pointing in the same direction as this arrow.
different dogs. Our overall prompts are generated by: a
{white, gray, brown} dog.
Multi-ConceptControl. Whentrainingmulti-conceptcon-
trolsforbyaddingorientationtowingpose/illumination,we
stillusethesamepromptsasdescribedabove. Condition 3: A text prompt "a parrot in the forest"
After seeing the 3 conditions I want you to look at three images:
8.UserStudyExamples
(a) (b) (c)
Weprovidetwoexamples,fromobject [ ]and [ ]and
[ ]and [ ],respectivelyasshowninFigure11and12.
8.1.PromptSelection.
Ourpromptsusedforuserstudycomparisonaredesigned
suchthatobjectsfromveryclosetoveryfarproximityfrom
Which image most closely follows ALL the above conditions? Which one is the worst?
the trainingmesh aretested. For illumination [ ] , our
comparisonsinvolvebeagles(highproximitytothedog Figure12. Orientation/WingPoseSurvey.
mesh),horse (medium proximity to the dog mesh), and
taxi, rockets(lowproximitytothedogmesh).
Similarly,forwingpose [ ],ourcomparisonsinvolve bird mesh) to eagle/parrot in the forest (dif-
bird with black head(similarcolortothetraining ferentidentitywithadditionalbackgrounddescriptions).
11Fine-tuned Non-finetuned Fine-tuned Non-finetuned Fine-tuned Non-finetuned Fine-tuned Non-finetuned
Figure13. Non-finetunedStableDiffusionComparison. We
comparethegeneratedimagesusingthesamepromptswith/without
finetuning the diffusion model to ensure there is no significant
texturedifference.
ComparisonAgainstNon-finetunedStableDiffusion. We
provide4examplecomparisonsusingthesamepromptsfor
text-to-imagegenerationagainstunfinetunedstablediffusion
(Figure13)toevaluatethetexturedifferencesbefore/after
fine-tuning. Whiletheremaybeaslighttexturedifference
(potentiallyduetothelimitedsingle-meshtrainingset),we
believeournewlygeneratedimagearestillofhighquality.
12