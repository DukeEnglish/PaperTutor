PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs
MichaelDorkenwald NimrodBarazani CeesG.M.Snoek* YukiM.Asano*
UniversityofAmsterdam
https://quva-lab.github.io/PIN/
Abstract
Vision-LanguageModels(VLMs),suchasFlamingoand
GPT-4V,haveshownimmensepotentialbyintegratinglarge
language models with vision systems. Nevertheless, these
modelsfacechallengesinthefundamentalcomputervision
task of object localisation, due to their training on multi-
modaldatacontainingmostlycaptionswithoutexplicitspa-
tial grounding. While it is possible to construct custom,
supervised training pipelines with bounding box annota-
tions that integrate with VLMs, these result in specialized
andhard-to-scalemodels. Inthispaper,weaimtoexplore
the limits of caption-based VLMs and instead propose to
tackle the challenge in a simpler manner by i) keeping the Figure1. WelearnasinglePositionalInsert(PIN)forunlocking
weights of a caption-based VLM frozen and ii) not using zero-shotobjectlocalisationabilitiesinafrozenVisionLanguage
Model (VLM) without adding any additional heads or requiring any supervised detection data. To this end, we introduce
superviseddatasets.FurtheroutputexamplesshowninFig.5&6.
aninput-agnosticPositionalInsert(PIN),alearnablespa-
tial prompt, containing a minimal set of parameters that
Equipping VLMs with precise object localisation abili-
are slid inside the frozen VLM, unlocking object localisa-
ties is important for tasks like autonomous driving [1,61,
tioncapabilities. OurPINmoduleistrainedwithasimple
62],assistivetechnology[64],androbotics[8,16,18]. De-
next-tokenpredictiontaskonsyntheticdatawithoutrequir-
spitetheirproficiencyinintegratingvisual-textualdata,cur-
ingtheintroductionofnewoutputheads. Ourexperiments
rent image-caption training hinders accurate spatial under-
demonstratestrongzero-shotlocalisationperformanceson
standing. Therefore, enhancing spatial comprehension in
a variety of images, including Pascal VOC, COCO, LVIS,
VLMsiskeytoenablingmorenuancedandcontext-aware
anddiverseimageslikepaintingsorcartoons.
interactions.
Onerecentstreamofresearch[9,40,56–58,63,65,69]fo-
1.Introduction
cusesondevelopingunifiedexpert VisionLanguageMod-
els (VLMs) capable of performing a variety of tasks, in-
Vision-Language Models (VLMs) have shown remark-
cludinglocalisation,withauniversalarchitecture.Although
ableresultsacrossdiversetasks,propelledbytheadvance-
thesemodelsshowimpressiveresultsacrossdifferenttasks,
ments in Large Language Models (LLMs) [12, 17, 53].
their success largely depends on the availability of exten-
Early works [23, 34, 44, 52, 66] used extensive image-
sive task-specific, supervised data [9,36,57,58]. Further-
caption data for end-to-end training, a trend later evolved
more,[9,40,43,56–58]requirealargeamountofcompute
by works like [4,11,28,32,33,68], which efficiently inte-
for training. The setting we tackle in this paper is differ-
grated pretrained vision and language models through fu-
ent. Our goal is to efficiently enable the localisation capa-
sion networks to further enhance cross-modal understand-
bilitiesofVLMswhilekeepingtheirparametersuntouched
ing. Flamingo[4]demonstratesimpressivemultimodalin-
andwithouttheneedforlocalisationsuperviseddatasets.
context learning abilities. However, like many caption-
based VLMs, it faces challenges in object localisation, a Our work aims to unlock the localisation abilities of
consequenceofitstrainingonwebdata. caption-based VLMs by integrating spatial understanding
into their existing zero-shot capabilities. We introduce a
*Equallastauthor. PositionalInsert(PIN),alearnablespatialpromptdesigned
1
4202
beF
31
]VC.sc[
1v75680.2042:viXraFigure2. Examplesfromouranalysisonlocalisationabilitiesofexistingcaption-basedVLMs. GPT-4V[42]istheonlymodeltoreturn
bounding boxes and by that roughly localised the object. All other VLMs struggle to easily localise the objects in the image. Further
examplesanddifferentkindsofpromptsareprovidedinthesupplementalSec.A.
toinfusespatialawarenessintoVLMswithoutalteringtheir transformative for the field of natural language process-
pretrainedweights. OurlearnedPINissimplyaddedtothe ing but have also significantly propelled the development
vision encoder embedding and follows the VLMs forward of multimodal models. Initial works for Vision Language
pass from there, thereby not imposing any computational Models(VLMs)[2,4,11,32,33,55,59]concentratedonex-
overhead. TotrainourPINmoduleeffectivelyandwithout tensiveimage-textpretraining. Thesemodelstypicallyun-
superviseddata,wecreateasyntheticdatasetcomposedof dergopretrainingwithvastcollectionsofinterleavedimage-
synthesizedobjectrenderingssuperimposedonbackground text data [48,76]. Flamingo was a pioneer in merging
images, providing precise ground truth locations. We as- a pretrained CLIP [44] image encoder with a pretrained
sess our approach on COCO [38], PVOC [14], LVIS [21], LLMthroughaperceiverandgatedcross-attentionblocks,
and RefCOCO [67]. Our findings reveal a significant en- demonstratingstrongmultimodalin-contextlearningabili-
hancementinVLMs’objectlocalisationabilities. Ourcon- ties. Given the image-text pretraining data containing de-
tributionscanbesummarizedasfollows: scriptive captions for images, we categorize these VLMs
as caption-based. This kind of pretraining naturally limits
• Weprovideananalysisoftheabilitiesofcaption-based
thespatialcomprehensionandexpressionabilitiesofthose
VLMsforobjectlocalisation.
VLMs. In this paper, we present a new, simple, and effi-
cientwaydesignedtoenableobjectlocalisationcapabilities
• WeproposePIN,aspatialprompt,tounlockthelocal-
withinthesemodels.
isationabilitiesincaption-basedVLMs.
• We demonstrate on the OpenFlamingo [5] and BLIP- Expert-based Vision-Language Models. Universal
2[32]VLMstheabilitytosuccessfullylocaliseobjects frameworks[10,36,40,46,63]havebeenintroducedtounify
onCOCO,PVOC,LVIS,andotherdata. architecturesandtrainingtasksbytreatingitasalanguage
modeling problem conditioned on e.g. observed pixel in-
2.RelatedWork puts. Recentworks[28,31,58,70,75]appliedthistomul-
timodal instruction-tuned data, promoting more intuitive
Caption-based Vision-Language Models. Large Lan- human-modelinteractionsforVLMs. Theresultingunified
guage Models (LLMs) [12,17,42,53] have not only been expert VLMs are capable of handling diverse tasks. Many
2Figure3. Schematicoverviewofourmethod. Wegeneratesynthetictrainingdatabyoverlayingobjectsonbackgroundimagesusingour
compositionfunctionC. Theseimagesarethenencoded,andourlightweightlearnablespatialpromptvectorπfromthePINmoduleis
addedtotheirvisionencodingsx .UsingtheVLM’sstandardforwardpass,alocationtextresponseisgeneratedbasedontheinputobject
v
nameandtheenhancedvisualfeaturex⋆. ThePINmoduleisoptimizedwithcross-entropybycomparingthisgeneratedtextagainstthe
v
knownobjectlocationsfromthecompositionfunctionC.
others[9,40,43,56–58,63,65,69]additionallytargetvisual byanalysingtheirtextualresponsesgivenvariousprompts.
groundingtaskslikelocalisation. Yet,thoseVLMsrelyon We examine models such as GPT-4V [42], BLIP-2 [32],
largeannotatedlocalisationdatasets[29,38,49,67]. Inad- Flamingo[4,5],andFromage[28].Forthat,weuseprompts
dition,manyofthoseworks[40,43,57,63,69]requiresub- aimed at generating a bounding box response from these
stantial amounts of compute to leverage this data. While VLMs. Note that due to the undisclosed training data for
these models exhibit impressive performance across vari- GPT-4V[42],wecannotruleoutitsexposuretosupervised
ous tasks, hence the name experts, their success hinges on object localisation training. We compare this against the
large quantities of task-specific, supervised data and com- publiclyavailable9BversionofOpenFlamingo[5]andthe
putational resources. Our work diverges from this path, 7BversionofBLIP-2[32]. Anoverviewoftheresultsand
seeking to unlock the object localisation capabilities of prompts can be found in Fig. 2. We find that among the
caption-based VLMs without relying on manually anno- evaluated VLMs, only GPT-4V [42] successfully returns
tated datasets. We propose a more flexible and efficient bounding boxes that roughly localise the intended object.
strategy, exploring how far we can go without supervised Other VLMs [5,28,32] are unable to provide any loca-
data. tioninformationevenintextformandinsteadare“chatty”
VisualPrompt Learning. PromptLearning isa method (FROMAGe,OpenFlamingo)orreturntheinputorprovide
originatedfromNLP[30,37,39]wherepromptsareviewed nooutput(BLIP-2). InSec.5.1,wequantitativelyevaluate
as continuous, task-specific vectors optimized during fine- thein-contextlearningabilitiesforlocalisationoftheOpen-
tuning.Thistechniquematchestheperformanceoffullfine- Flamingomodel. InthesupplementarymaterialSec.A,we
tuningbutrequires1000timesfewerparameters,enhancing broadenourstudybyexaminingawidervarietyofprompts,
efficiency and reducing resource usage. Beginning works specificallyincludingthosethatdonotrequiregeneratinga
focused on adapting those methods to VLMs by adding boundingbox,andbyanalyzingalargernumberofsamples.
learnable tokens to the language model [19,72–74]. Sub- Yet,theconclusionremainsthesameaswiththeexemplary
sequent works [7,24,25,41,60] extended them to the vi- resultsinFig.2thatcaption-basedVLMsareunabletolo-
sion model and recently to both the vision and language caliseobjectsinagivenimageviatextualresponses.
branch [26]. However, these works have been applied to
encoder-onlymodels,suchasCLIP[44],leavingtheiradap- 4.Method
tion to VLMs with a decoder unexplored. Motivated by
We tackle the shortcomings of caption-based Vision-
thesemethods,weintroduceapositionalpromptforspecif-
LanguageModels(VLMs)intheirabilitytolocaliseobjects
icallytargetinglocalisationingenerativeVLMs.
withinimages.Tothisend,weintroduceasimpleyeteffec-
tivePositionalInsert(PIN),designedtoenhancetheVLMs’
3.LocalisationbyCaption-basedVLMs
object localisation capabilities without altering their exist-
Before discussing our proposed method, we first assess ingparameters. Anoverviewofourapproachcanbefound
the object localisation capabilities of caption-based VLMs inFig.3.
3Preliminary. Vision-Language Models (VLMs) accept
inputscomposedofvisualdatasuchasimagesI alongside
a textual input T. The visual component I is processed
by a vision encoder ϕ producing a feature vector x ∈
V v
RNp×Dv,whereN
p
denotesthenumberofpatchesandD
v
the channel dimension. Similarly, the textual information
T istokenized,yieldingtextualembeddingsx
t
∈ RM×Dt,
withM representingtheamountoftextualtokensandD
V
the vocabulary size. The visual features x go through a
v
fusion network F before being processed with the textual
featuresx toproducearesponsetextt =LLM(F(x ),x )
t r v t
bytheLargeLanguageModel.
4.1.PIN:PositionalInsert
Figure4.Sampleimagesfromoursyntheticdatageneration.
ThePositionalInsertisalearnableinput-agnosticspatial
predictedtokens
feature vector and is inserted directly after the vision en-
coderϕ V.ToinstillspatialawarenessintoourPIN,westart
L
(θ)=−(cid:88)T
logp (y |y ,x⋆), (4)
withfixedpositionalembeddingsofdimensiondemploying CE θ t <t v
sinusoidalfunctions[54] t=1
wherey correspondstothetargettokenatpositiontinthe
t
(cid:18) position (cid:19) text, T is the total number of tokens to be predicted and
S[i,2k]=sin 100002k/dmodel , (1) x⋆ v isthepositionalenhancedfeaturevector. Herep θ isthe
(cid:18) (cid:19) probabilityassignedbythemodeltothecorrecttokenatpo-
position
S[i,2k+1]=cos , (2) sitiont,conditionedontheprevioustokensy ,thevisual
100002k/dmodel <t
features,andthetextualprompt.Thislearningobjectiveen-
ablestheeasyadaptionofpretrainedVLMsforlocalisation
where i denotes the index of the position and k represents
withoutthedependencyonspecializedcomponentslikere-
the index within the dimension of the embedding, with
gionproposalnetworks.
d as the dimensionality of the embedding space. The
model
range for k extends from 1 to d . Each of the spatial 4.2.SyntheticDataGeneration
model
sinusoidalvectorsisfurtherrefinedbyalearnable,shallow
We do not rely on manually labeled data to unlock the
feed-forwardneuralnetworkψparametrizedbyθ,resulting
positional information in the VLM. Instead, we generate
in our PIN π=ψ(S) with the output dimension matching
ourownsyntheticdatafollowing[20,71]byutilizingStable
theonesfromthevisionencoderπ ∈RM×Dt.Thislearned
Diffusion[47]tosynthesizeobjectsfromtheLVIS[21]cat-
embedding is then added to the output from the vision en-
egorylist.TheCLIP[45]moduleisusedtosortoutimplau-
coderx , resultingintheenrichedvisualfeaturerepresen-
v sibleimagesbyremovingthosewithalowCLIP[45]score,
tation
a matching score between the input image I and the tex-
tualinformationT.Note,sincethevisionencoder’sweights
x⋆ =x +π. (3)
v v remain unchanged, it is unlikely to overfit to any pasting
artifacts. The composition function C overlays objects on
Training Objective. The PIN module’s parameters θ of randomlypickedlocationswhileconsideringthefollowing
ψ are optimized via the text output produced by the large constraints: theaspectratior ofobjects, minimals and
min
languagemodel. Thisprocessrequiresnoadditionalheads maximal s pasting sizes, the number of objects a ,
max max
orprojectionlayers, thusmaintainingthemodel’ssimplic- andthemaximaloverlapo w.r.t.alreadyinsertedobjects.
max
ityandnativenaturallanguageoutput. Themodelistrained GivenabackgroundimageI ∈I,thecompositionfunction
b
withaninputsequenceconsistingofatextualpromptt ∈ yields
p
T suchas‘Intheimageisa<obj>locatedat’andistasked
(t ,I )=C(I ,r,a ,s ,s ,o ), (5)
to complete the sequence with the bounding box coordi- p p b max min max max
nates. For a given object name <obj>, present within the withageneratedimageI ∈I andthetextt ∈T contain-
p p
image,themodelpredictsasequenceofboundingboxcoor- ing the object location for a randomly selected object by
dinatesinthetemplateoft ∈T like[x ,y ,x ,y ] C. Thisprocesscreatesaself-generatedsupervisionsignal
r min min max max
conditioned on the image features and the initial textual thatissubsequentlyexploitedinthetrainingofPIN.Typical
prompt. We employ a negative log-likelihood loss for the sampleimagescanbefoundinFig. 4.
4PVOC COCO LVIS
Method ≤3Objects ≤3Objects ≤3Objects
mIoU mIoU mIoU mIoU mIoU mIoU mIoU mIoU mIoU
M L M L M L
Baselines
raw 0 0 0 0 0 0 0 0 0
random 0.22±0.04 0.10±0.02 0.33±0.06 0.12±0.04 0.07±0.02 0.22±0.08 0.07±0.03 0.06±0.02 0.18±0.09
2context 0.19±0.11 0.08±0.05 0.30±0.18 0.10±0.08 0.06±0.04 0.18±0.16 0.04±0.06 0.03±0.04 0.10±0.15
5context 0.19±0.09 0.07±0.04 0.31±0.15 0.10±0.08 0.06±0.04 0.20±0.16 0.06±0.05 0.04±0.03 0.17±0.13
10context 0.20±0.11 0.06±0.03 0.32±0.18 0.09±0.07 0.05±0.04 0.17±0.14 0.05±0.05 0.03±0.03 0.15±0.14
PEFT
CoOponLLM 0.28 0.11 0.43 0.22 0.10 0.39 0.13 0.07 0.40
VPTonF 0.34 0.16 0.51 0.26 0.15 0.47 0.19 0.14 0.48
VPTonϕ 0.42 0.21 0.61 0.33 0.22 0.57 0.23 0.19 0.56
V
LoRAonϕ 0.44 0.26 0.62 0.33 0.23 0.58 0.23 0.19 0.55
V
PIN(ours) 0.45 0.27 0.62 0.35 0.26 0.59 0.26 0.24 0.61
PEFT
VPTonF 0.33 0.12 0.51 0.27 0.12 0.50 0.18 0.11 0.47
VPTonϕ 0.32 0.12 0.50 0.26 0.11 0.48 0.17 0.10 0.46
V
PIN(ours) 0.44 0.24 0.63 0.34 0.22 0.60 0.26 0.23 0.60
Table 1. Comparison on object localisation on a subset of PVOC [14], COCO [38] and LVIS [21] with up to 3 objects per image,
yielding3,582,2,062and6,016testimagesrespectively. PINimprovesontheOpenFlamingoin-contextandPEFTbaselinesforboththe
OpenFlamingoandBLIP-2VLM.
5.Experiments i.e.around0.04%oftheVLM’ssizeof3B.
Syntheticdatasetdetails. WefollowX-Paste[71]tocre-
We apply our approach to the Flamingo [4] and BLIP-
ateoursyntheticdatasetusingStableDiffusion[47]version
2 [32] VLM. More specifically we use the open-source
1generating60samplesforeachcategoryinLVIS[21]re-
version OpenFlamingo [5] for Flamingo. We evaluate
sulting in around 70k object images. We exclude all cat-
the localisation abilities of our approach on a subset of
egories overlapping with COCO [38] and PVOC [14] for
COCO [38], PVOC [14], and LVIS [21] with up to 3 ob-
training. For the background, we use images from the
jectsperimageresultingin3,582,2,062and6,016testim-
BG20-k [35] dataset on which we paste the objects. Fol-
ages respectively. We use ground truth object names and
lowingX-Paste’sfilteringprocedure,weexcludeallclasses
localisethoseinagivenimage. Wereportnumbersonthe
with less than ≤ 20 images remaining per class, as these
PVOC 2007, COCO, and LVIS evaluation set. The mean
classes might not be well-generated. For our composition
Intersection over Union (IoU) is reported quantifying the
function,wesetthemaximumallowedoverlaptoo =0.5,
overlap between the true and predicted bounding box. We max
thenumberofimagesa =3,r=r ,s =[0.3,0.2,0.1]
report this metric for all bounding boxes and additionally max orig min
ands =[1.0,1.0,1.0],foruptothreeobjectsrespectively.
for medium and large bounding box sizes only. A bound- max
ingboxisconsideredlargeifitisover96×96pixels,and
5.1.QuantitativeResults
medium if between 32×32 and 96×96 pixels. We keep
OpenFlamingo and BLIP-2 in its native form, which uses
Baselines. For comparison, we use OpenFlamingo’s in-
imageresolutionsof224,makingitparticularlydifficultto
contextlearningversion,configuredwithvariablenumbers
localise small objects. For all experiments, we use the 3B
of context images. To account for performance variation
parameterversionwiththeinstruction-tunedLLMofOpen-
duetocontextimageselectionbeingsampledrandomly,we
FlamingoandtheOPT2.7BparameterversionofBLIP-2.
execute each setup ten times and report the average and
Implementationdetails. ThePINmodulestartsoffrom standard deviation. BLIP-2 is not able to do in-context
a1Dsinusoidalembedding[54]with64dimensions. From learning due to the lack of interleaved image-text training
there a two-layer Multi-Layer-Perceptron is applied, each data[32].Weselectboundingboxesrandomlyfromcontext
consistingofafullyconnected(FC)layer,LayerNorm[6] images as a baseline to assess the in-context learning abil-
and SwiGLU [50]. Lastly, a final FC layer is added to ities. Additionally, we compare against other Parameter-
match the target vision encoder embedding dimension of EfficientFineTuning(PEFT)methodssuchasCoOp[73],
1024.TheparametersofthePINmoduleareoptimizedwith using the strongest version with adding 16 learnable to-
Adam[27]withalearningrateof10−3. WetrainourPIN kens to the input to the LLM. In addition, we append 100
moduleon2×A6000GPUforaroundtwodays. Overall, learnable tokens in the spirit of Visual Prompt Learning
our PIN module consists of only around 1.2M parameters, (VPT)[24]toeitherthevisionencoderϕortheFusionnet-
5
]5[ognimalFnepO
]23[2-PILBFigure5.Localisationonawiderangeofimagetypesrangingfrompaintings,andcomicstouniquescenarios.Despitethevaryingimage
content,enhancingtheOpenFlamingocaption-basedVLMwithourPINshowsstronglocalisationabilities.
work F (the same location where PIN is added). We also OpenFlamingo[5] P@0.3
evaluate our method against finetuning the ViT vision en-
+Raw 0
coderϕ usingLoRA[22]withα=16andr=16.
V
+In-contextlearning 3.7
LocalisationonPVOC,COCO,andLVIS. Fromthere-
+PINw/opositionalreferral 14.1
sults in Tab. 1, we first observe that our introduced PIN,
+PINw/positionalreferral 26.4
when combined with OpenFlamingo, surpasses both the
Table 2. Evaluation on RefCOCO [67] Test-A. PIN shows de-
rawandthein-contextlearningversionsofOpenFlamingo
centgroundingabilitieswithoutusinganyannotatedtrainingdata,
across all evaluated metrics, considerably. In particular,
outperformingthein-contextlearningFlamingobaseline.Extend-
compared to the best OpenFlamingo in-context learning
ingoursyntheticdatasetwithpositionalreferralsimprovesperfor-
version, we improve in mIoU by a factor of 2× on PVOC
manceconsiderably.
and a factor of 3× on COCO. Notably, the PIN module
achieves this without any exposure to COCO or PVOC 100learnabletokenstoeitherthevisionencoderϕ V orfu-
classes during training, in contrast to the few-shot nature sion network F. PIN outperforms the VPT baseline ap-
of in-context learning. The raw zero-shot OpenFlamingo plied to the fusion network considerably and also the one
variant fails to generate any meaningful bounding boxes, applied to the vision encoder ϕ V, especially for medium-
asvisualizedinFig.2. Weobservethattherandombound- sizedboundingboxes(IoU M). Thesefindingsdemonstrate
ingboxselectorconsistentlyperformsbetterthantheOpen- thatPINbetterincorporatespositionalinformationintothe
Flamingo in-context learning version. This demonstrates pretrained VLM. In addition, we also show PIN’s neces-
thatOpenFlamingocannotleveragethepositionalinforma- sity by comparing it against finetuning the vision encoder
tiongivenbyin-contextboundingboxestogenerateplausi- ϕ V with LoRA [22]. PIN slightly outperforms the strong
bleboundingboxesforthequerysamples. LoRA baseline while having 5× fewer parameters. We
observe that the LoRA-adapted VLM can nearly perfectly
Furthermore, we also compare the adapted VLM with
solve our synthetic training examples, overfitting poten-
PIN for OpenFlamingo against other Parameter-Efficient
tially to synthetic data artifacts. In contrast, PIN utilizes
Fine-Tuning (PEFT) methods. First, we observe low per-
the strong concepts learned in the ViT without changing
formanceofCoOp. Thisisprimarilybecauseofthelackof
its weights, thus excluding the possibility of overfitting to
spatialpositionalinformationinCoOp’sadaptation. Open-
syntheticdataartifacts. Wecanalsoconfirmtheeffective-
Flamingo employs a perceiver resampler as a fusion net-
nessofPINonBLIP-2,outperformingagaintheotherPEFT
work, which removes most positional information during
baselines. These findings demonstrate that PIN can effec-
caption-basedpretraining. Thus,theCoOpadaptionstrug-
tivelyunlocklocalisationabilitiesinvariousVLMsbeyond
glestosolvethelocalisationtask. Incontrast,ourPINout-
OpenFlamingo.
performsthisbaselineconsiderably,asitcanaddpositional
information directly to the vision embedding during adap- Grounding on RefCOCO. We also evaluate PIN on Re-
tion. We also compare against a different PEFT baseline fCOCO [67] Test-A split in a zero-shot manner, paving a
which follows Visual Prompt Tuning (VPT) [24], adding new way for reporting model performance without using
6Figure6. ObjectlocalisationresultsonPVOC[14]andCOCO[38]. ThePINmoduleunlocksspatiallocalisationinthecaption-based
OpenFlamingo[5]VLM.
any of its annotated training data. To this end, we ex- comicsandpaintings, asillustratedinFig.5. Notably, our
tend our synthetic dataset with positional expressions like methoddemonstratesrobustperformanceinlocalisingdis-
‘leftapple’,‘monkeyontheright’etc. Withthissimplistic tinctcharactersandobjects,evenamidstsignificantdomain
setup,weachieve26.4P@0.3,indicatingdecentgrounding variations.Forinstance,itsuccessfullyidentifiesthecatand
abilities, compared to only 3.7 for the in-context learning mouseinacomicimage(Fig.5E)andaccuratelylocatesthe
Flamingobaseline. Extendingoursyntheticdatawithrefer- person in a painting (Fig. 5D), as well as the owl and ap-
ralexpressionimprovesresultsconsiderably,byafactorof pleinanother(Fig.5B).Additionally,ourVLMshowcases
nearly2. InFig.7wevisualizedourgroundingpredictions its ability to differentiate between closely related objects.
for RefCOCO. A limiting factor is the rather small 1B pa- This is evident in its distinguishing between a donkey and
rameterLLMinOpenFlamingo,havingtroubleunderstand- ahorse(Fig.5F),aswellasbetweenaglassofwineanda
ingmorecomplexandlongerreferrals. glass of beer (Fig. 5I). These observations lead us to con-
clude that our adapted VLM not only excels in localising
5.2.QualitativeResults
objectsacrossvariedimagetypesbutalsoretainsthestrong
zero-shotcapabilitiestypicalofcaption-basedVLMs.
Localisation on diverse images. We also explore the
objectlocalisationabilitiesofouradaptedVLMonawide Localisation on PVOC and COCO. The adapted VLM
range of images, encompassing various domains such as accurately localises objects of different sizes, as demon-
7Method
PVOC≤3Objects COCO≤3Objects #pastedobjects mIoU
≤3
mIoU
≤4
mIoU
≤5
mIoU mIoUM mIoUL mIoU mIoUM mIoUL
Generalization ≤2 0.24 0.21 0.19
PIN(COCO) 0.45 0.27 0.63 0.39 0.31 0.62 ≤3 0.35 0.31 0.29
PIN(Synth.) 0.45 0.27 0.62 0.35 0.26 0.59 ≤4 0.35 0.30 0.28
HigherResolution ≤5 0.34 0.30 0.27
PIN(224) 0.45 0.27 0.62 0.35 0.26 0.59
PIN(448) 0.47 0.30 0.65 0.37 0.29 0.59 Table4. Ablationonthenumber ofobjectsbeingpastedduring
PIN(224) 0.44 0.24 0.63 0.34 0.22 0.60 training on our synthetic data evaluated on COCO. Pasting with
PIN(364) 0.47 0.27 0.66 0.37 0.26 0.62 1-3objectsworksbestacrossallmIoUscores.
Table3.Ablatingtheimageresolutionandthechoiceofsynthetic
trainingdataforPIN. the ViT, allowing our PIN to be trained at a resolution of
448×448.Asexpected,thisleadstoanimprovementacross
strated in Fig. 6. Variety in object sizes: It identifies both all IoU metrics, particularly for medium-sized bounding
boxes(mIoU ).Wescaledthesizeoftheboundingboxfor
large(personinFig.6Q)andsmallobjects(birdinFig.6I; M
personinFig.6O).Varietyinobjectlocations:Wealsofind mediumM,andlargeLaccordingtotheincreaseinscaleof
the image resolution. Most VLMs of BLIP-2 are trained
that the enhanced VLM localises objects at various loca-
tions in an image, e.g. boxes near the bottom (Fig. 6C,E),
onanimageresolutionof224×224,yet,captionfinetuned
VLMsareavailableon364×364imageresolution. Wevi-
top (Fig. 6B,M), left (Fig. 6F,R) and right (Fig. 6E,N).
Crowded and overlapping: Additionally, our model ef- suallycomparethedifferenceinFig.8andobservetighter
bounding boxes with the higher resolution VLM. Training
fectively manages more complex situations such as more
PIN on a higher BLIP-2 resolution results in similar IoU
crowded scenes (train in Fig. 6C), partial occlusions (per-
son riding a horse in Fig. 6D). Multi-object: Our method improvementsasforOpenFlamingo.
iscapableoflocalisingmultipleobjectswithinasingleim- ImpactofPINonVLM’sgeneralabilities. Weanalyze
age, demonstrating its ability to recognize more than just the impact of applying PIN on the general abilities of the
the most salient object. This can be seen e.g. in Fig. 6Q VLMusingtheVQAv2[3]dataset. Thebaseperformance
for the person and toilet and in Fig. 6B for the person and ofOpenFlamingois44.1%wheninsertingPIN,theperfor-
themotorbike. Yet,theadaptedmodelstruggleswithmore mance reduces to 34.3%, yet it does not compromise the
confusingscenesyieldingmorelooseboundingboxpredic- VLM.Moreover,wecomparethistotheVLMadaptedwith
tionslikethetrailoftheaeroplaneinFig.6J.Similarly,for the finetuned vision encoder. We observe a bigger reduc-
small bounding boxes, our approach cannot locate objects tion in performance with 33.4%. In addition, our PIN can
veryprecisely, e.g.thesofaandchairinFig.6Horsinkin be easily deactivated, thereby retaining the general VLM
Fig.6Q.Overall,weconcludethatthemodelcanextendits abilities,aflexibilitynotpossiblewhenfinetuningtheViT.
zero-shotabilitiestotheobjectlocalisationtask. InFig.8, Amountofobjectstopaste. Lastly,weevaluatethemax-
wevisualizeresultswiththeBLIP-2VLM. imum number of objects, denoted as a , that are pasted
max
onto the background for each image. Separate models are
5.3.Ablations
trainedfor1,2,3,4,and5allowedobjectsperimage. The
results are shown in Tab. 4 and the mIoU on the COCO
Generalization of synthetic data. In Tab. 3 (General-
dataset is reported for a maximum of 3, 4, and 5 objects
ization),wedelvedeeperintothechoiceoftrainingdataon
perimage. Weobserveadecreaseinperformancewhentoo
thezero-shotabilitiesofourPINmodule.Forthat,wecom-
fewobjectsarepastedduringtraining(mIoU of0.24vs.
paretrainingPINoneithertheCOCOdatasetsorusingthe ≤3
0.35) as the VLM only focuses on the most salient object.
syntheticdatainwhichallCOCOandPVOCcategoriesare
Alternatively, pastingtoomanyobjectsalsodecreasesper-
excluded. Asexpected, weobservebetterperformancefor
formance,especiallyformIoU . Witha =3westrikea
the PIN trained on COCO and evaluated on COCO. How- ≤5 max
goodbalancebetweenthesetwoextremes,yieldingthebest
ever, we observe equivalent performance when analyzing
accuraciesacrossallmIoUvalues.
their generalization abilities to PVOC. From that, we con-
cludethatsyntheticdataservesasaviablesolutiontoadapt
6.Conclusion
pretrained VLMs for object localisation while preserving
theirgeneralizationcapabilities. In this work, we introduced PIN, a lightweight mod-
Higherimageresolution. InTab.3(HigherResolution), ule that enables object localisation capabilities in a frozen
we analyze the impact of using higher image resolutions VLM.Wefirstshowedthelimitedobjectlocalisationabil-
ontheperformanceofPIN.AllOpenFlamingomodelsare ities of caption-based VLMs. Subsequently, we verified
pretrained on a resolution of 224 × 224. To circumvent that these capabilities were enabled with our PIN module
that, we extrapolate the frozen positional embeddings of onOpenFlamingoandBLIP-2.Ourzero-shotresultsacross
8
ognimalFnepO
2PILBFigure7.Zero-shotvisualgroundingresultsonRefCOCO[67]ofPINwiththeOpenFlamingo[5]VLM.TheadaptedVLMstruggleswith
morecomplexscenarios(BandC),yet,iteffectivelyhandlessimplercases(F,G,H,J).
Figure 8. Object localisation results with BLIP-2 [32] on 224×224, BLIP-2 (224), image resolution and 364×364, BLIP-2 (364), on
PVOC[14].ThePINtrainedwiththehigherimageresolutionBLIP-2versionisabletopredictmoreaccurateboundingboxes.
PVOCandCOCO,variousimagetypes,andobjectsdemon- challengestofuturework.
strate that the strong performance of caption-based VLMs
canbetransferredtolocalisation. Acknowledgement
Limitations. Owing to our simplistic training procedure This work is financially supported by Qualcomm Tech-
andthecaption-basedpretrainingfocusingonbigobjectsin nologies Inc., the University of Amsterdam, and the al-
relatively low-resolution images, our model struggles with lowance Top consortia for Knowledge and Innovation
generatingtightboundingboxes,especiallyaroundsmaller (TKIs)fromtheNetherlandsMinistryofEconomicAffairs
objects. As a no-bells-and-whistles paper, we leave these andClimatePolicy.
9References Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander
Kolesnikov, JoanPuigcerver, NanDing, KeranRong, Has-
[1] Wayve lingo-1. https://wayve.ai/thinking/
san Akbari, Gaurav Mishra, Linting Xue, Ashish V. Thap-
lingo - natural - language - autonomous -
liyal,JamesBradbury,andWeichengKuo. Pali: Ajointly-
driving/. 1
scaledmultilinguallanguage-imagemodel. InICLR,2023.
[2] ArmenAghajanyan,BernieHuang,CandaceRoss,Vladimir
1,2
Karpukhin,HuXu,NamanGoyal,DmytroOkhonko,Man-
[12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
darJoshi,GargiGhosh,MikeLewis,andLukeZettlemoyer.
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
CM3: A causal masked multimodal model of the internet.
Barham, Hyung Won Chung, Charles Sutton, Sebastian
CoRR,2022. 2
Gehrmann, et al. Palm: Scaling language modeling with
[3] Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret
pathways. JournalofMachineLearningResearch,2023. 1,
Mitchell,C.LawrenceZitnick,DeviParikh,andDhruvBa-
2
tra. VQA:visualquestionanswering. Int.J.Comput.Vis.,
[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
2017. 8
Toutanova. BERT:pre-trainingofdeepbidirectionaltrans-
[4] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, Antoine
formersforlanguageunderstanding. InNAACL-HLT,2019.
Miech,IainBarr,YanaHasson,KarelLenc,ArthurMensch,
14
KatherineMillican, MalcolmReynolds, etal. Flamingo: a
[14] ZhengDong,KeXu,YinYang,HujunBao,WeiweiXu,and
visuallanguagemodelforfew-shotlearning.NeurIPS,2022.
RynsonW.H.Lau. Location-awaresingleimagereflection
1,2,3,5,13
removal. InICCV,2021. 2,5,7,9
[5] AnasAwadalla,IrenaGao,JoshGardner,JackHessel,Yusuf
[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Samir Yitzhak Gadre, Shiori Sagawa, Jenia Jitsev, Simon
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
Kornblith, PangWeiKoh, GabrielIlharco, MitchellWorts-
vainGelly,JakobUszkoreit,andNeilHoulsby. Animageis
man,andLudwigSchmidt. Openflamingo: Anopen-source
worth16x16words: Transformersforimagerecognitionat
frameworkfortraininglargeautoregressivevision-language
scale. InICLR,2021. 14
models. CoRR,2023. 2,3,5,6,7,9,13,15
[16] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey
[6] LeiJimmyBa,JamieRyanKiros, andGeoffreyE.Hinton.
Lynch,AakankshaChowdhery,BrianIchter,AyzaanWahid,
Layernormalization. CoRR,2016. 5
Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong
[7] HyojinBahng,AliJahanian,SwamiSankaranarayanan,and
Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-
Phillip Isola. Visual prompting: Modifying pixel space to
worth,SergeyLevine,VincentVanhoucke,KarolHausman,
adaptpre-trainedmodels. CoRR,2022. 3
Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,
[8] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen
and PeteFlorence. Palm-e: An embodiedmultimodal lan-
Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,
guagemodel. InICML,2023. 1
DannyDriess,AvinavaDubey,ChelseaFinn,PeteFlorence,
[17] TomB.Brownetal.Languagemodelsarefew-shotlearners.
ChuyuanFu,MontseGonzalezArenas,KeerthanaGopalakr-
InNeurIPS,2020. 1,2
ishnan, Kehang Han, Karol Hausman, Alexander Herzog,
[18] Jensen Gao, BidiptaSarkar, Fei Xia, Ted Xiao, JiajunWu,
JasmineHsu,BrianIchter,AlexIrpan,NikhilJ.Joshi,Ryan
BrianIchter,AnirudhaMajumdar,andDorsaSadigh. Phys-
Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal,
icallygroundedvision-languagemodelsforroboticmanipu-
Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu,
lation. CoRR,2023. 1
HenrykMichalewski,IgorMordatch,KarlPertsch,Kanishka
Rao,KristaReymann,MichaelS.Ryoo,GreciaSalazar,Pan- [19] Chunjiang Ge, Rui Huang, Mixue Xie, Zihang Lai, Shiji
nagSanketi,PierreSermanet,JaspiarSingh,AnikaitSingh, Song, Shuang Li, and Gao Huang. Domain adaptation via
Radu Soricut, Huong T. Tran, Vincent Vanhoucke, Quan promptlearning. CoRR,2022. 3
Vuong,AyzaanWahid,StefanWelker,PaulWohlhart,Jialin [20] GolnazGhiasi,YinCui,AravindSrinivas,RuiQian,Tsung-
Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, YiLin,EkinD.Cubuk,QuocV.Le,andBarretZoph.Simple
andBriannaZitkovich. RT-2: vision-language-actionmod- copy-pasteisastrongdataaugmentationmethodforinstance
elstransferwebknowledgetoroboticcontrol. CoRR,2023. segmentation. InCVPR,2021. 4
1 [21] AgrimGupta,PiotrDolla´r,andRossB.Girshick. LVIS:A
[9] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun datasetforlargevocabularyinstancesegmentation.InCVPR,
Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, 2019. 2,4,5,15,19
Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. [22] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Minigpt-v2: largelanguagemodelasaunifiedinterfacefor Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen.
vision-languagemulti-tasklearning. CoRR,2023. 1,3 Lora: Low-rank adaptation of large language models. In
[10] TingChen,SaurabhSaxena,LalaLi,DavidJ.Fleet,andGe- ICLR,2022. 6
offreyE.Hinton. Pix2seq:Alanguagemodelingframework [23] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,
forobjectdetection. InICLR,2022. 2 HieuPham,QuocV.Le,Yun-HsuanSung,ZhenLi,andTom
[11] Xi Chen, Xiao Wang, Soravit Changpinyo, A. J. Piergio- Duerig.Scalingupvisualandvision-languagerepresentation
vanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, learningwithnoisytextsupervision. InICML,2021. 1
10[24] MenglinJia,LumingTang,Bor-ChunChen,ClaireCardie, [40] JiasenLu,ChristopherClark,RowanZellers,RoozbehMot-
SergeJ.Belongie,BharathHariharan,andSer-NamLim.Vi- taghi, and Aniruddha Kembhavi. UNIFIED-IO: A unified
sualprompttuning. InECCV(33),2022. 3,5,6 modelforvision,language,andmulti-modaltasks. InICLR,
[25] ChenJu,TengdaHan,KunhaoZheng,YaZhang,andWeidi 2023. 1,2,3
Xie. Prompting visual-language models for efficient video [41] Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu,
understanding. InECCV(35),2022. 3 and Xinmei Tian. Prompt distribution learning. In CVPR,
[26] Muhammad Uzair Khattak, Hanoona Abdul Rasheed, 2022. 3
Muhammad Maaz, Salman H. Khan, and Fahad Shahbaz [42] OpenAI. GPT-4technicalreport. CoRR,2023. 2,3,13
Khan.Maple:Multi-modalpromptlearning.InCVPR,2023.
[43] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,Shaohan
3 Huang,ShumingMa,andFuruWei. Kosmos-2: Grounding
[27] Diederik P Kingma and Jimmy Ba. Adam: A method for multimodallargelanguagemodelstotheworld.CoRR,2023.
stochasticoptimization. ICLR,2016. 5 1,3
[28] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.
[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Groundinglanguagemodelstoimagesformultimodalinputs
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
andoutputs. InICML,2023. 1,2,3,13
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
[29] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Krueger, and Ilya Sutskever. Learning transferable visual
KenjiHata,JoshuaKravitz,StephanieChen,YannisKalan-
modelsfromnaturallanguagesupervision. InInternational
tidis,Li-JiaLi,DavidA.Shamma,MichaelS.Bernstein,and
ConferenceonMachineLearning,2021. 1,2,3
LiFei-Fei. Visualgenome:Connectinglanguageandvision
[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
usingcrowdsourceddenseimageannotations. Int.J.Com-
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
put.Vis.,(1),2017. 3
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
[30] BrianLester,RamiAl-Rfou,andNoahConstant.Thepower
Krueger, and Ilya Sutskever. Learning transferable visual
of scale for parameter-efficient prompt tuning. In EMNLP
modelsfromnaturallanguagesupervision. InICML,2021.
(1),2021. 3
4
[31] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
[46] Scott E. Reed, Konrad Zolna, Emilio Parisotto, Ser-
JingkangYang,andZiweiLiu. Otter:Amulti-modalmodel
gioGo´mezColmenarejo,AlexanderNovikov,GabrielBarth-
within-contextinstructiontuning. CoRR,2023. 2
Maron,MaiGimenez,YurySulsky,JackieKay,JostTobias
[32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.
Springenberg, TomEccles, JakeBruce, AliRazavi, Ashley
Hoi. BLIP-2: bootstrapping language-image pre-training
Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol
withfrozenimageencodersandlargelanguagemodels. In
Vinyals,MahyarBordbar,andNandodeFreitas. Ageneral-
ICML,2023. 1,2,3,5,9,13
istagent. Trans.Mach.Learn.Res.,2022. 2
[33] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.
[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Hoi. BLIP: bootstrapping language-image pre-training for
PatrickEsser,andBjo¨rnOmmer.High-resolutionimagesyn-
unified vision-language understanding and generation. In
thesiswithlatentdiffusionmodels. InCVPR,2022. 4,5
ICML,2022. 1,2
[48] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
[34] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Gotmare,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
ShafiqR.Joty,CaimingXiong,andStevenChu-HongHoi.
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
Alignbeforefuse:Visionandlanguagerepresentationlearn-
man,etal.Laion-5b:Anopenlarge-scaledatasetfortraining
ingwithmomentumdistillation. InNeurIPS,2021. 1
nextgenerationimage-textmodels. InNeurIPS,2022. 2
[35] Jizhizi Li, Jing Zhang, Stephen J. Maybank, and Dacheng
[49] ShuaiShao,ZemingLi,TianyuanZhang,ChaoPeng,Gang
Tao. Bridgingcompositeandreal:Towardsend-to-enddeep
Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365:
imagematting. Int.J.Comput.Vis.,(2),2022. 5,15
A large-scale, high-quality dataset for object detection. In
[36] LiunianHaroldLi,PengchuanZhang,HaotianZhang,Jian-
ICCV,2019. 3
wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu
Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and [50] NoamShazeer. GLUvariantsimprovetransformer. CoRR,
Jianfeng Gao. Grounded language-image pre-training. In 2020. 5
CVPR,2022. 1,2 [51] Aleksandar Shtedritski, Christian Rupprecht, and Andrea
[37] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimiz- Vedaldi. WhatdoesCLIPknowaboutaredcircle? visual
ingcontinuouspromptsforgeneration. InACL/IJCNLP(1), promptengineeringforvlms. CoRR,2023. 13
2021. 3 [52] Hao Tan and Mohit Bansal. LXMERT: learning cross-
[38] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James modality encoder representations from transformers. In
Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and EMNLP/IJCNLP(1),2019. 1
C.LawrenceZitnick. MicrosoftCOCO:commonobjectsin [53] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
context. InECCV(5),2014. 2,3,5,7 Martinet,Marie-AnneLachaux,Timothe´eLacroix,Baptiste
[39] Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Rozie`re,NamanGoyal,EricHambro,FaisalAzhar,Aure´lien
Yang,andJieTang.P-tuningv2:Prompttuningcanbecom- Rodriguez, ArmandJoulin, EdouardGrave, andGuillaume
parable to fine-tuning universally across scales and tasks. Lample. Llama: Open and efficient foundation language
CoRR,2021. 3 models. CoRR,2023. 1,2
11[54] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko- [67] LichengYu,PatrickPoirson,ShanYang,AlexanderC.Berg,
reit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia andTamaraL.Berg. Modelingcontextinreferringexpres-
Polosukhin. Attentionisallyouneed. NeurIPS,2017. 4,5 sions. InECCV(2),2016. 2,3,6,9
[55] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, [68] XiaohuaZhai,XiaoWang,BasilMustafa,AndreasSteiner,
KevinLin,ZheGan,ZichengLiu,CeLiu,andLijuanWang. Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer.
GIT:Agenerativeimage-to-texttransformerforvisionand Lit: Zero-shot transfer with locked-image text tuning. In
language. Trans.Mach.Learn.Res.,2022. 2 CVPR,2022. 1
[56] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, [69] HaotianZhang, PengchuanZhang, XiaoweiHu, Yen-Chun
Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu
Hongxia Yang. OFA: unifying architectures, tasks, and Yuan, Jenq-NengHwang, andJianfengGao. Glipv2: Uni-
modalities through a simple sequence-to-sequence learning fying localization and vision-language understanding. In
framework. InICML,2022. 1,3 NeurIPS,2022. 1,3
[57] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, [70] ShengyuZhang,LinfengDong,XiaoyaLi,SenZhang,Xi-
XizhouZhu,GangZeng,PingLuo,TongLu,JieZhou,Yu aofeiSun,ShuheWang,JiweiLi,RunyiHu,TianweiZhang,
Qiao, andJifengDai. Visionllm: Largelanguagemodelis FeiWu,andGuoyinWang. Instructiontuningforlargelan-
alsoanopen-endeddecoderforvision-centrictasks. CoRR, guagemodels:Asurvey. CoRR,2023. 2
2023. 1,3 [71] Hanqing Zhao, Dianmo Sheng, Jianmin Bao, Dongdong
[58] WeihanWang,QingsongLv,WenmengYu,WenyiHong,Ji Chen, Dong Chen, Fang Wen, Lu Yuan, Ce Liu, Wenbo
Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Zhou,QiChu,WeimingZhang,andNenghaiYu. X-paste:
Song,JiazhengXu,BinXu,JuanziLi,YuxiaoDong,Ming Revisitingscalablecopy-pasteforinstancesegmentationus-
Ding, and Jie Tang. Cogvlm: Visual expert for pretrained ingCLIPandstablediffusion. InICML,2023. 4,5
languagemodels. CoRR,2023. 1,2,3
[72] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Zi-
[59] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia
wei Liu. Conditional prompt learning for vision-language
Tsvetkov,andYuanCao. Simvlm: Simplevisuallanguage models. InCVPR,2022. 3
modelpretrainingwithweaksupervision. InICLR,2022. 2
[73] KaiyangZhou,JingkangYang,ChenChangeLoy,andZiwei
[60] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,
Liu. Learningtopromptforvision-languagemodels. IJCV,
Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jen-
2022. 3,5
niferG.Dy,andTomasPfister. Learningtopromptforcon-
[74] BeierZhu,YuleiNiu,YuchengHan,YueWu,andHanwang
tinuallearning. InCVPR,2022. 3
Zhang. Prompt-alignedgradientforprompttuning. CoRR,
[61] Licheng Wen, Xuemeng Yang, Daocheng Fu, Xiaofeng
2022. 3
Wang, Pinlong Cai, Xin Li, Tao Ma, Yingxuan Li, Linran
[75] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
Xu, DengkeShang, etal. Ontheroadwithgpt-4v(ision):
hamed Elhoseiny. Minigpt-4: Enhancing vision-language
Earlyexplorationsofvisual-languagemodelonautonomous
understandingwithadvancedlargelanguagemodels. CoRR,
driving. CoRR,2023. 1
2023. 2
[62] Licheng Wen, Xuemeng Yang, Daocheng Fu, Xiaofeng
[76] WanrongZhu, JackHessel, AnasAwadalla, SamirYitzhak
Wang, Pinlong Cai, Xin Li, Tao Ma, Yingxuan Li, Linran
Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig
Xu, Dengke Shang, Zheng Zhu, Shaoyan Sun, Yeqi Bai,
Schmidt,WilliamYangWang,andYejinChoi. Multimodal
Xinyu Cai, Min Dou, Shuanglu Hu, and Botian Shi. On
C4:anopen,billion-scalecorpusofimagesinterleavedwith
the road with gpt-4v(ision): Early explorations of visual-
text. CoRR,2023. 2
languagemodelonautonomousdriving. CoRR,2023. 1
[63] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu,
Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang.
Unitab: Unifyingtextandboxoutputsforgroundedvision-
languagemodeling. InECCV(36),2022. 1,2,3
[64] ZongmingYang,LiangYang,LirenKong,AilinWei,Jesse
Leaman, Johnell Brooks, and Bing Li. Seeway: Vision-
language assistive navigation for the visually impaired. In
SMC,2022. 1
[65] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,
Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,
YayaShi,ChenliangLi,YuanhongXu,HehongChen,Jun-
fengTian, QianQi, JiZhang, andFeiHuang. mplug-owl:
Modularizationempowerslargelanguagemodelswithmul-
timodality. CoRR,2023. 1,3
[66] JiahuiYu,ZiruiWang,VijayVasudevan,LeggYeung,Mo-
jtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive
captionersareimage-textfoundationmodels. Trans.Mach.
Learn.Res.,2022. 1
12Supplemental
response, yet, for the numbered grid its response does not
matchtheobjects(e.g. thedoginFig.11B),andforitonly
roughly matches the object (e.g. the cat in Fig. 11H). The
A.Extendedanalysisofcaption-basedVLMs
other models generally fail to provide accurate or relevant
Thissectionbroadensthescopeofouranalysisofthelo- coordinates in response to the grid-based prompts. Their
calisationabilitiesofcaption-basedVision-LanguageMod- responses are often off-task, with Flamingo providing un-
els (VLMs) from the main paper. Our goal is to assess a related continuations (such as ’cells [...] of the brain’),
widerrangeofpromptsonmoresampleimages. Thestudy Fromagerepeatingtheprompt, andBLIP-2sometimesnot
employsthesamecollectionofVLMsasbefore,namely: respondingatall.Thisindicatesagapinthesemodels’abil-
itytounderstandandexecutespatialtasks.
• GPT-4V[42]
Relative position Here, we evaluate the VLMs’ relative
• 7BversionofBLIP-2[32]
positionabilities.Forthat,wetaskthemodelstoidentifyan
• 9BversionofFlamingo[4,5] object relative to a center object (Fig. 12A-D). Therefore,
we designed an artificial image with a pizza at the center,
• Fromage[28] surroundedbyalemontotheleft, asharktothebottom, a
cowtotheright,andadogabove. WeobservethatBLIP-2
Note that due to the undisclosed training data for
listedthreerandomobjects,regardlessoftheprompt. Fro-
GPT-4V[42],wecannotruleoutitsexposuretosupervised
magedetectstheobjectstotheleftcorrectlyFig.12A,yet,
objectlocalisationtraining.Ourexpandedanalysisincludes
allotherdirectionsarewrong.OpenFlamingoresponsesare
three prompt types, designed to test the VLMs’ abilities
onlyaboutthepizzaignoringthesurroundingobjects.GPT-
in various aspects of spatial understanding and object lo-
4V does answers correctly for all directions except for the
calisation. The prompts cover a spectrum of challenges,
one above the pizza Fig. 12B. We extend our study to ask
from generating bounding boxes around a specified object
VLMshowaspecificobjectisplacedrelativetoaredcircle
(showninFig.10)toperforminggrid-basedlocalisation(il-
thatisoverlaidontheimage(Fig.12E-H).Thisisinspired
lustratedinFig.11)anddeterminingrelativepositions(de-
by[51]whichshowedthatredcirclescanbeusedforVLMs
pictedinFig.12).
todirecttheirattentiontoaspecificregion. Weobservethat
Generateboundingbox Similartothestudyinthemain
FromageandBLIP-2arenotabletoprovideanymeaning-
paper, we evaluate caption-based VLMs in their ability to
fulresponses. Instead,oftentheseVLMstrytodescribethe
generate a bounding box for the specified object. For this
absolutepositionoftheobjecte.g.forFromageFig.12Cand
purpose, we applied the prompt from the main paper to
BLIP-2Fig.12B.OpenFlamingoanswersgiveindeedrela-
moresampleimages, whicharedepictedatthetopofFig-
tivepositionalinformation,yet,mostoftenwrongandin3
ure 10. Our observations indicate that only GPT-4V is ca-
of4cases‘ontheleftside’. Again,onlyGPT4-Visableto
pable of generating a bounding box that is approximately
giveroughlycorrectresponsese.g. Fig.12D,yet,Fig.12A
located near the object of interest, yet not with high pre-
andCarepartiallyandFig.12Biscompletelywrong.From
cision; for example, the cat in Figure 10D. In contrast,
that, we conclude that caption-based VLMs struggle with
all other VLMs, such as OpenFlamingo as shown in Fig-
solvingrelativepositionaltasksindicatingalackofspatial
ure10B,completethesentencewith’intheimage,’without
understandingontherelativeplacementofobjects.
providinganyboundingboxinformatiom. Tofurtherevalu-
atetheseVLMs,weaddedmoredetailedinstructionstothe Summary Theextendedanalysisofcaption-basedVLMs
prompt, such as ‘in the format of [x min,y min,x max,y max]’, revealslimitationsintheirspatialunderstandingandobject
whichcanbefoundinFigure10E-H.Weobservethateven localisation abilities. Among all evaluated models, only
withmoreinstructions,theseVLMsarenotabletoprovide GPT-4V managed to generate responses that partially met
any bounding box or positional information about the in- the task criteria. Yet, due to the undisclosed training data
quiredobject. forGPT-4V[42],wecannotruleoutitsexposuretosuper-
Grid-based localisation In this part, we evaluate the vised object localisation training. Despite varying prompt
VLMswithagrid-basedlocalisationtaskusingtwodiffer- complexities and image scenarios, all other VLMs consis-
entgridstyles.Thefirststyleusesastandardnumberedgrid tently underperform in tasks requiring precise localisation
(Fig.11A-D),whilethesecondusesachessboard-stylegrid and relative positioning. The study’s findings underscore
(Fig. 11E-H). In both cases, an 8x8 grid is overlaid on the a gap in the current capabilities of caption-based VLMs,
images.Thesizeofeachgridcellvariestomatchtheaspect highlightingtheirstruggleswithaccuratelyinterpretingand
ratiooftheimage. ThegoalistoevaluatetheVLMs’abil- respondingtospatially-orientedtasks. Thismotivatedusto
itytopinpointtheobjectlocationusingthedesignatedgrid. designthePINmoduletounlocklocalisationabilitiesinthe
WeobservethatonlyGPT4-Visabletolistgridcellsinits caption-basedVLMFlamingo.
13Figure9. Visualizationofpair-wisesimilaritiesoftherawsinusoidalembedding,theCLIPencoder’sspatialembeddingsandourlearned
PIN.Ourembeddingcaptureslocalpositionalinformation,makingiteffectiveforlocalisation.
#layersinψ Sembedding mIoU mIoU mIoU
M L
(a) 1 sinusoidal 0.34 0.25 0.57
(b) 2 sinusoidal 0.35 0.26 0.59
(c) 3 sinusoidal 0.33 0.24 0.56
(d) 2 sinusoidal 0.35 0.26 0.59
(e) 2 learned 0.35 0.27 0.59
Table 5. Ablation on the number of layers in ψ and the type of positional embedding S used in PIN evaluated on COCO. The best
performanceisobtainedwithonly2layersinψandsinusoidalvslearnedpositionalembeddingsforSleadstothesameresults.
B.Additionalablations successfully captures local positional information, making
itparticularlyeffectivefortaskslikelocalisation.
Visualizingπ InFig.9, wepresentavisualizationofour
B.1.Depthofψ
learned input-independent feature vector π from the PIN
module. Following ViT [15], we compute the cosine sim- In this ablation, we analyse the impact of varying the
ilarity for all pairings of the 16×16 patches. This results numberoflayersinthefeed-forwardneuralnetworkψ in-
in a 16×16 grid visualization, where each cell shows the sidethePINmodule. Table 5(a)-(c)displaystheresults.
similarity between a specific patch with all other patches. Increasingthelayerquantityresultsinariseinparameters,
Forreadability, weomittedeverysecondpatch, thusbeing advancingfrom0.6Mforonelayerto1.2Mfortwolayers,
a8×8plot. Wealsovisualizethe1Dsinusoidalembedding and reaching 2.3M for three layers. We find that the opti-
asitisthestartingpointforourPINmodule. FromFig.9, malnumberoflayersinψis2,asevidencedbythehighest
wefindthatthisembeddingonlyobtainsthehighestsimilar- mIoUscoresacrossallcategories.Thefindingsindicatethat
itieswithitselfandamongpatchesinthesamerow,achar- afewlearnableparametersaresufficient,aligningwiththe
acteristicfeatureofthesinusoidalembeddings. Conversely, input-agnosticcharacteristicsofthePINmodule.
ourlearnedembeddingπ demonstrateshighsimilaritypri-
B.2.Sinusoidalvslearned
marilywithinitselfanditsimmediateneighboringpatches,
an attribute advantageous for localisation tasks, highlight- We investigate the effectiveness of the sinusoidal em-
ing the similarity among the spatial locations. We also vi- bedding [13] and compare it against a learned variant. As
sualizethesimilarityfortherawCLIPvisionencoderem- showninTable 5(d)-(e),bothtypesofembeddingsyield
beddingsbyaveragingthesimilaritiesover50images. We similarperformance,withnosignificantdifferenceinmIoU
observethattheembeddingofthevisionencoderdoesnot scores. Our goal is to incorporate spatial information into
containanypositionalinformationasonlyonebrightspot, the VLM, for which the sinusoidal embedding is ideally
thesimilaritywithitself,canbefoundineachcell. Insum- suited. Its performance matches that of the learned ver-
mary,ourvisualizationsshowthatourlearnedembeddingπ sion, which in theory provides greater adaptability and ca-
14Background o mIoU mIoU mIoU in scenarios where pixel boundaries are not distinctly de-
max M L
fined.
(a) White 0.5 0.24 0.12 0.48
(b) BG-20k[35] 0.5 0.35 0.26 0.59 C.3.Visualizationsoffailurecases
(c) BG-20k[35] 0.0 0.33 0.26 0.56
WevisualizetypicalfailurecasesofourmodelinFig.15.
(d) BG-20k[35] 0.5 0.35 0.26 0.59
Asdiscussedinthelimitationsection,ourmodelcannotef-
Table6.Ablationonchoiceofbackgroundimageandoverlapbe- fectively localise multiple instances from the same object
tweenobjects(o max)onCOCO.Realisticbackgroundimagesand duetooursimplistictrainingprocedure. Wefoundthatthe
allowingforoverlapbetweenthepastedobjectsimproveslocali-
model typically handles those cases by drawing a bound-
sationperformance.
ingboxaroundallinstancesfromthesameclasswhichcan
be seen in Fig. 15A-E. As we keep the original input res-
pacityforthemodel. Thus,thesinusoidalembeddingwith
olution of the OpenFlamingo [5] VLM of 224, our model
no learnable parameters is the optimal choice for our PIN
strugglestolocalisetheseobjectswithatightboundingbox
module due to its efficiency and effectiveness in this con-
(Fig.15F-I)sincetheobjectspansonlyacrossafewpixels.
text.
D.Additionalimplementationdetails
B.3.Choiceofbackground
Oursynthetictrainingandvalidationdatasetsarecreated
Weablatethechoiceofbackgroundimagesforoursyn-
from 1,116 object categories, based on LVIS, with overall
thetic data generation. To this end, we compare the BG-
56,064 images generated by Stable Diffusion. These cate-
20k[35]byusingplainwhitebackgroundimagesonCOCO
gories exclude those of COCO and PVOC to enable mea-
inTab.6rows(a-b). Weobserveastrongperformancede-
suringtrulyzero-shotlocalisationperformance. Adifferent
crease in terms of IoU with white backgrounds, especially
setof81categories(whichincludestheCOCOandPVOC
for medium-sized bounding boxes. We conjecture that the
classes), amounting to 4,296 images, is reserved for zero-
morerealisticimagesinBG-20kcontributetoamorerobust
shot evaluation. The dataset averages 50.43 ± 12.11 im-
spatialembeddingπ,enhancinglocalisationperformance.
agesperobjectcategory. Forpastingobjectsontotheback-
ground images, we find dividing the images into grids of
B.4.Overlapbetweenobjects
16×16 worked best for OpenFlamingo, 14×14 for BLIP-
Lastly, we evaluate the effect of allowing for overlap 2,aligningwiththeshapesofthevisionembedding. Thus,
o betweenpastedobjectsduringtrainingonoursynthetic the network only needs to predict numbers between 0 and
max
generateddataonCOCO.Wecomparetwosettingsofno- 224 in steps of the grid size, simplifying the task at hand.
overlap o =0.5 in Tab. 6, rows (c-d). We find that by This also leads to bounding boxes not being perfectly pre-
max
creatingmorerealisticgenerationsbyallowingforoverlap- cise around the inquired object, though, it has better per-
ping pasted objects, we obtain slightly better localisation formance than the model trained on a grid size matching
performance,indicatingabetterlearnedPINmodule. imagesize. ForRefCOCO,weextendoursyntheticdataset
with positional referral expressions. For that, we increase
C.Additionalqualitativeresults thelikelihoodofsamplingthesameobjecttypeto0.7. We
stillrandomlyselectoneofthepastedobjectsfortraining,
C.1.VisualizationsonLVIS yet, whensamplinganobjectforwhichitsobjecttypeoc-
cursmultipletimesintheimage,weaddapositionalrefer-
Our adapted VLM demonstrates effective object locali-
ral to it. These are computed by measuring the axes with
sation also on LVIS [21] as demonstrated in Fig. 13. Our
the highest difference between the center points of the ob-
model can localise multiple objects within a single image,
jects. Then,weextendthepromptwithe.g. ’leftperson’,or
as illustrated in Fig. 13A, D, E, and I. It also effectively
’persononleft’forleft,right,top,andbottom.
identifiesobjectsinunusualsettings,suchasateddybearin
atree(Fig.13J)andaremoteunderacat(Fig.13H).These
examplessupporttheconclusionthatourmodelextendsits
zero-shotcapabilitiestothetaskofobjectlocalisation.
C.2.Zero-shotvisualizationsonsyntheticdata
InFig.14,wedemonstratethezero-shotlocalisationca-
pabilitiesofourVLMonoursyntheticgenerateddata. This
visualization showcases the model’s ability to accurately
identifyandlocalisemultipleobjectswithinanimage,even
15Figure10. Analysisoflocalisationabilitiesofcaption-basedVLMstoprovideaboundingbox. A-Dshowsresultswiththesameprompt
ondifferentsampleimagesandE-Hillustratespromptswithmoreinstructioninformationonthesamecatanddogimage(D).
16Figure11. Analysisofgrid-basedlocalisationofcaption-basedVLMs. A-Dshowsresultswithanumberedgridoverlaidontheimage.
whileE-Hshowsfindingswithacheckerboard-stylegrid.
17Figure12. Analysisofrelativepositionabilitiesofcaption-basedVLMs. InA-D,VLMshavetoidentifytheobjectrelativetothecenter
one.InE-H,VLMsaretaskedtoprovidethelocationrelativetoaredcircle.
18Figure13.ObjectlocalisationresultsonLVIS[21]withtheOpenFlamingoVLM.
Figure14.Zero-shotobjectlocalisationresultsonoursyntheticdatawiththeOpenFlamingoVLM.
19Figure15.Typicalfailurecases:Duetotheminimalisticdesignofourmethod,thePINenhancedVLMcannotlocalisemultipleinstances
ofthesameclass(A-E).OftentheVLMdrawsaboundingboxaroundallobjectsofthesameinstance. Additionally,keepingtheoriginal
inputresolutionof224fromtheVLMlimitsourabilitytoeffectivelymanageverysmallobjects(D-I).
20