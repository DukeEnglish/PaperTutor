IM-3D: Iterative Multiview Diffusion and Reconstruction for
High-Quality 3D Generation
LukeMelas-Kyriazi*12 IroLaina2 ChristianRupprecht2 NataliaNeverova1 AndreaVedaldi1 OranGafni1
FilipposKokkinos*1
SyncDreamer (~ 13 minutes) Ours: IM-3D (~ 3 minutes)
A girl riding a wolf A bulldog wearing a pirate hat A beautiful, intricate butterfly Samurai koala bear
A frog wearing a sweater
Zero123-XL (~ 10 minutes) Ours: IM-3D (~ 3 minutes)
A squirrel playing guitar Mecha vampire girl, chibi An astronaut riding a horse A ghost eating a hamburger
A squirrel playing guitar
ImageDream (~ 2 hours) Ours: IM-3D (~ 3 minutes)
A monkey on a kick drum A katana A toy tank Military mech, future, scifi
A girl riding a wolf
A Darth Vader helmet Dragon armor A frog wearing a sweater A pig wearing a backpack Army jacket Figure1: IM-3Dgenerateshigh-qualityandfaithful3Dassetsfromtext/imagepair. IteschewsScoreDistillationSampling
(SDS)forrobust3Dreconstructionoftheoutputofavideodiffusionmodel,tunedtogeneratea360°videooftheobject.
Abstract networks to output 3D objects directly. In this paper, we
Most text-to-3D generators build upon off-the-shelf text- furtherexplorethedesignspaceoftext-to-3Dmodels. We
to-imagemodelstrainedonbillionsofimages. Theyuse significantlyimprovemulti-viewgenerationbyconsidering
variantsofScoreDistillationSampling(SDS),whichisslow, video instead of image generators. Combined with a 3D
somewhatunstable,andpronetoartifacts. Amitigationis reconstruction algorithm which, by using Gaussian splat-
tofine-tunethe2Dgeneratortobemulti-viewaware,which ting, canoptimizearobustimage-basedloss, wedirectly
canhelpdistillationorcanbecombinedwithreconstruction producehigh-quality3Doutputsfromthegeneratedviews.
Our new method, IM-3D, reduces the number of evalua-
*Equalcontribution 1Meta2UniversityofOxford,Oxford,UK.
tionsofthe2Dgeneratornetwork10-100×,resultingina
Correspondenceto:FilipposKokkinos<fkokkinos@meta.com>,
muchmoreefficientpipeline,betterquality,fewergeometric
LukeMelas-Kyriazi<lukemk@robots.ox.ac.uk>.
inconsistencies,andahighyieldofusable3Dassets.
1
4202
beF
31
]VC.sc[
1v28680.2042:viXraIM-3D:IterativeMultiviewDiffusionandReconstructionforHigh-Quality3DGeneration
1.Introduction Our second contribution is to show that we can extract a
high-quality 3D object by directly fitting a 3D model to
All state-of-the-art open-world text-to-3D generators are
theresultingviews—withoutdistillationorreconstruction
builtontopofoff-the-shelf2Dgeneratorstrainedonbillions
networks—quickly and robustly. To do so, we rely on a
ofimages. Thisisnecessarybecausethereisn’tenough3D
3D reconstruction algorithm based on Gaussian splatting
trainingdatatodirectlylearngeneratorsthatcanunderstand
(GS)(Kerbletal.,2023). TheimportanceofGSisthatit
languageandoperateinanopen-endedmanner. However,
affordsfastdifferentiablerenderingofthe3Dobject,which
thebestwayofbuildingsuchmodelsisstilldebated.
allowstheuseofimage-basedlosseslikeLPIPS. Thelatter
One approach is to perform 3D distillation by adopting iskeytobridgingthesmallinconsistenciesleftbythe2D
ScoreDistillationSampling(SDS)(Pooleetal.,2023)or generatorwithoutrequiringad-hocreconstructionmodels.
oneofitsvariants. Thesemodelscanworkontopofnearly
Third,wenoticethat,whilethisprocessresultsinmostly
anymodern2Dgenerator,buttheyrequiretensofthousands
verygood3Dmodels,someinconsistenciesmaystillremain.
ofevaluationsofthe2Dgenerator, andcantakehoursto
Wethusproposetoclosetheloopandfeedthe3Drecon-
generate a single asset. They are also prone to artifacts
structionbackto the2Dgenerator. Inordertodo so, we
and may fail to converge. Mitigating these shortcomings
simplyrendernoisedimagesofthe3Dobjectandrestartthe
inspiredasignificantbodyofresearch(Wangetal.,2023b).
videodiffusionprocessfromthose. Thisapproachiscloser
Thefundamentalreasonfortheselimitationsisthattheun- inspirittoSDSasitbuildsconsensusprogressively,butthe
derlying2Dgeneratorisnot3Daware. SDSslowlymakes feedback loop is closed two or three times per generated
thedifferentviewsofthe3Dobjectagreewiththe2Dmodel, asset,insteadoftensofthousandsoftimes.
whichcharacterizesthemindependentlyofeachother. Sev-
There are many advantages to our approach. Compared
eralauthors(Shietal.,2023b;Wang&Shi,2023;Shietal.,
toSDS,itreducesdramaticallythenumberofevaluations
2023a) have shown that fine-tuning the 2D generator to
of the 2D generator network. Using a fast sampler, gen-
understandthecorrelationbetweendifferentviewsofthe
eratingthefirstversionofthemulti-viewimagesrequires
objectsignificantlyfacilitatesdistillation. Morerecently,ap-
onlyaround40evaluations. Iteratedgenerationsaremuch
proachessuchas(Lietal.,2023)avoiddistillationentirely
shorter (as they start from a partially denoised result), at
andinsteadjustreconstructthe3Dobjectfromthegener-
mostdoublingthetotalnumberofevaluations. Thisisa10-
atedviews. However,inordertocompensatefordefectsin
100×reductioncomparedtoSDS.The3Dreconstruction
multi-viewgeneration,theymustincorporateverylarge3D
isalsoveryfast,takingonlyaminuteforthefirstversion
reconstructionnetworks. Ultimately,theseapproachesare
oftheasset,andafewsecondsforthesecondorthird. It
manytimesfasterthandistillation,butqualityislimited.
also sidesteps typical issues of the SDS such as artifacts
Inthispaper,weexplorethebenefitsoffurtherincreasing (e.g.,saturatedcolors,Janusproblem),lackofdiversity(by
the quality of multi-view generation and how this might avoidingmodeseeking),andlowyield(failuretoconverge).
affect the design space of future text-to-3D models. We Comparedtomethodslike(Lietal.,2023),IM-3Disslower,
are inspired by the fact that, in the limit, a 2D generator butachievesmuchhigherquality,anddoesnotrequireto
couldoutputenoughconsistentviewsoftheobjecttoafford learnlargereconstructionnetworks,offloadingmostofthe
simplemulti-viewreconstruction,sidesteppingdistillation workto2Dgenerationinstead.
andreconstructionnetworksentirely.
Inanutshell,ourcontributionistoshowhowvideogenera-
Tothisend,weintroduceIM-3D,atext-to-3Dgeneration tornetworkscanimproveconsistentmulti-viewgenerator
approachthatleveragesIterativeMultiviewdiffusionand toapointwhereitispossibletoobtainstate-of-the-artand
reconstruction (Figures 1 and 2). IM-3D is based on sig- efficient text/image-to-3D results without distillation and
nificantlyboostingthequalityofthemulti-viewgeneration withouttrainingreconstructionnetworks.
networkbyswitchingfromatext-to-imagetoatext-to-video
generatornetwork. Specifically,wepickEmuVideo(Gird- 2.Relatedwork
haretal.,2023),avideogeneratorconditionedbothonaref-
erenceimageandatextualprompt. Ourfirstcontributionis 3DDistillation. 3Ddistillationistheprocessofextracting
toshowthatEmuVideocanbefine-tuned,usingarelatively a3Dobjectfroma2Dneuralnetworktrainedtogenerate
smallnumberofsynthetic3Dassets, togeneratedirectly images from text, or otherwise match them to text. For
up to 16 high-resolution consistent views (512×512) of example,methodslikeDreamFields(Jainetal.,2022)do
theobject. WhileEmuVideoisinitselfaniterativemodel so starting from the CLIP image similarity score. How-
basedondiffusion,byadoptingafastsamplingalgorithm, ever,mostrecentmethodsbuildondiffusion-basedimage
theviewscanbegeneratedinafewsecondsandinasmall generatorsthatutilizevariantsoftheScoreDistillationSam-
numberofiterations. pling(SDS)lossintroducedwithDreamFusion(Pooleetal.,
2IM-3D:IterativeMultiviewDiffusionandReconstructionforHigh-Quality3DGeneration
2023). Fantasia3D(Chenetal.,2023a)disentanglesillu- fusion(Tewarietal.,2023),SyncDreamer(Liuetal.,2023c)
minationfrommaterials. Magic3D(Linetal.,2022)recon- andDMV3D(Xuetal.,2023)denoisemultipleviewsofthe
structshigh-resolutiontexturemeshes. RealFusion(Melas- 3Dobjectsimultaneouslytoimproveconsistency.
Kyriazietal.,2023)startsfromareferenceimageandfine-
3DGen(Guptaetal.,2023)learnsalatentspacetoencode
tunesthepromptofa2Dgeneratortomatchit, distilling
3DobjectsusingaVAE-liketechnique. Thelatentspace
a 3D object afterwards. Make-it-3D (Tang et al., 2023b)
isthenusedbyadiffusionmodelthatdrawssamplesfrom
alsostartsfroma2Dimage,combiningSDSwithaCLIP
it. However, this approach is not very scalable as it re-
losswithrespecttothereferenceimageandadepthprior.
quirestrainingthemodelfromscratchusing3Ddata. Hexa-
HiFi-123(Yuetal.,2023)usesDDIMinversiontoobtain
Gen3D(Mercieretal.,2024)extends3DGentousefeatures
thecodeforthereferenceimage. ATT3D(Lorraineetal.,
fromanSDmodelinstead,thusincreasingthescalability
2023) develops an amortized version of SDS, where sev-
oftheapproach. AconcurrentworkisViVid-1-to-3(Kwak
eral variants of the same object are distilled in parallel.
etal.,2023),whichalsousesavideogeneratorformulti-
HiFA (Zhu & Zhuang, 2023) reformulates the SDS loss
viewgeneration,butdoesnotproduceany3Dassets(only
andannealsthediffusionnoise. DreamTime(Huangetal.,
novelviews).
2023) also proposes to optimize the noise schedule. Pro-
lificDreamer (Wang et al., 2023b), SteinDreamer (Wang
Non-SDSmethods. Sometext-to-3Dmethodsperform“di-
et al., 2023a), Collaborative SDS (Kim et al., 2023) and
rect”3Dreconstructionontopofgeneratedviewswithout
Noise-freeSDS(Katziretal.,2023)improvethevariance
usingSDS. One–2–3–45(Liuetal.,2023a)compensates
oftheSDSgradientestimate. DreamGaussian(Tangetal.,
fortheshortcomingsofthemulti-viewgeneratorbytraining
2023a),GaussianDreamer(Yietal.,2023)and(Chenetal.,
areconstructionnetwork. Instant3D(Lietal.,2023)issimi-
2023c)applyGaussiansplattingtotheSDSloss.
lar,butbasedonamuchlargerreconstructionmodel(Hong
etal.,2023). Wonder3D(Longetal.,2023)furtherlearns
Methods using multi-view generation. Many methods
togeneratemultipleviewsofagiveninputimagetogether
haveproposedtousemulti-viewgenerationtoimprove3D
withthecorrespondingnormalmaps,whicharethenused
generation. Formulti-viewgeneration,themostcommon
toreconstructthe3Dobject. AGG(Xuetal.,2024)builds
approachisZero-1-to-3(Liuetal.,2023b),whichfine-tunes
asingle-imagereconstructionnetworkontopofGaussian
the Stable Diffusion (SD) model to generate novel views
splatting. CAD (Wan et al., 2023) learns a 3D generator
of an object. Zero123++ (Shi et al., 2023a) further im-
networkfromimagesamplesusinga2Ddiffusionmodel,
provesonthisbasemodelinvariousways,includinggener-
replacingtheSDSlosswithadversarialtraining.
atingdirectlyagridofseveralmulti-viewimages. Cascade-
Zero123(Chenetal.,2023b)proposestoapplytwosuch OurapproachalsoeschewstheSDSloss,butshowsthatitis
modelsinsequence:thefirsttoobtainapproximatemultiple possibletooffloadmostofthemodellingburdentothe2D
viewsoftheobject,andthesecondtoachievebetterquality generatornetwork,utilizingastraightforwardandefficient
viewsconditionedontheapproximateones. 3Dreconstructionalgorithm.
Magic123(Qianetal.,2023)andDreamCraft3D(Sunetal.,
2023) combine Zero-1-to-3 and SD. They start from a 3.Method
generated2Dimage,extractdepthandnormals,andapply
Wefirstdescribeourvideo-basedmulti-viewgeneratornet-
theRealFusion/DreamBoothtechniquetofine-tunethe2D
work in Section 3.1 and its training data in Section 3.2,
diffusionmodeltogeneratedifferentviewsoftheobject.
followedbyadescriptionoftherobust3Dreconstruction
MVDream(Shietal.,2023b)directlygeneratesfourfixed module in Section 3.3 and of iterative refinement in Sec-
viewpoints of an object from a text prompt. Consis- tion3.4. AnoverviewofourmethodisshowninFigure2.
tent123(Wengetal.,2023)usesadifferentformofcross-
view attention and generates several views sufficient for 3.1.Multi-viewasvideogeneration
direct reconstruction. ConsistNet (Yang et al., 2023) in-
Our multi-view generation model is based on fine-
troduces an explicit 3D pooling mechanism to exchange
tuninganexistingtext-to-video(T2V)generatornetwork
information between views. ImageDream (Wang & Shi,
EmuVideo(Girdharetal.,2023). First,itutilizesatext-to-
2023)extendsMVDreamtostartfromagiveninputimage,
image(T2I)model(Emu(Daietal.,2023))togeneratean
and proposes a new variant of image conditioning com-
initialimageIcorrespondingtothegiventextualpromptp.
paredtothatofZero-1-to-3. RichDreamer(Qiuetal.,2023)
Second,theimageI∈R3×H×W andthetextpromptpare
furtherlearnstogeneratenormalsandseparationbetween
fedintoasecondgenerator,whichproducesuptoK =16
materialandlighting.
framesofvideoJ∈RK×3×H×W,utilizingIasguidance
ViewsetDiffusion(Szymanowiczetal.,2023),ForwardDif- forthefirstframe. Noticethat,whilethemodelistrained
3IM-3D:IterativeMultiviewDiffusionandReconstructionforHigh-Quality3DGeneration
Figure2: OverviewofIM-3D.Ourmodelstartsfromaninputimage(e.g., generatedfromaT2Imodel). Itfeedsthe
latterintoanimage-to-videodiffusionmodeltogenerateaturn-tablelikevideo. Thelatterispluggedinto3DGaussian
Splattingtodirectlyreconstructthe3Dobjectusingimage-basedlossesforrobustness. Optionally,rendersoftheobjectsare
generatedandfedbacktothevideodiffusionmodel,repeatingtheprocessforrefinement.
suchthatI≈J ,thisisnotanexactequality. Instead,the thenoiseϵfromit. Thetrainingusesthestandarddiffusion
1
modeldrawsasampleJfromalearnedconditionaldistribu- lossL (ϵˆ|J,I,p,t,ϵ)=wdiff·∥ϵˆ(J ,t,I,p)−ϵ∥2where
diff t t
tionp(J|I,p),whichallowsittoslightlydeviatefromthe (J,I,p) ∈ J is a training video, ϵ is a Gaussian sample,
inputimagetobetterfitinthegeneratedvideo. Anadvan- t is a time step, also randomly sampled, and w is a cor-
t
tageofEmuVideocomparedtoothervideogeneratorsis responding weighing factor. To finetune Emu Video, we
thatthevideoframesJarealreadyhigh-qualityandhigh- useL ,butfreezeallparametersexceptforthetemporal
diff
resolution, without requiring sophisticated coarse-to-fine convolutionalandattentionlayers.
samplingschemes.Itisarchitecturedasafine-tunedversion
oftheoriginalT2IEmunetworkwithsomemodifications 3.2.Data
toaccountforthetemporaldependenciesbetweenframes.
ThedatasetJ usedtotrainourmodelconsistsofturn-table-
Starting from the pre-trained Emu Video model, we then likevideosofsynthetic3Dobjects. Severalrelatedpapers
fine-tune it to generate a particular kind of video, where inmulti-viewgenerationalsousesyntheticdata,takingOb-
the camera moves around a given 3D object, effectively javerse(Deitkeetal.,2022)orObjaverse-XL(Deitkeetal.,
generating simultaneously several views of it, in a turn- 2023)asasource. Here,weutilizeanin-housecollection
table-like fashion. In order to do so, we consider an in- of3Dassetsofcomparablequality,forwhichwegenerate
ternaldatasetofsynthetic3Dobjects,furtherdescribedin textualdescriptionsusinganimagecaptioningnetwork.
Section3.2. Thisdatasetprovidesuswithtrainingvideos
J ={(J ,I ,p )}N ,eachcontainingK =16viewsof Similar to prior works (Li et al., 2023), we use a subset
n n n n=1
of 100k assets selected for quality, as determined by the
theobjecttakenatfixedangularintervalandarandombut
CLIP (Radford et al., 2021) alignment between rendered
fixedelevation,theinitialimageI =[J ] ,andthetextual
n n 1
images and textual descriptions. Each video J ∈ J is
promptp . Thecameradistanceisfixedacrossallrenders.
n
obtained by sampling one of the 100k assets, choosing a
Differently from many prior multi-view generation net- randomelevationin[0,π/4],andthenplacingthecamera
works,wedonotpassthecameraparameterstothemodel; aroundtheobjectatuniform(2π/K degree)intervals.
instead,weuseafixedcameradistanceandorientation,ran-
domizingonlytheelevation. Themodelsimplylearnsto
3.3.Fastandrobustreconstruction
produceasetofviewsthatfollowthisdistribution.
Togeneratea3Dassetfromapromptp,wefirstsamplean
Like most image and video generators, Emu Video is
imageI∼p(I|p)fromtheEmuimagemodel,followedby
based on diffusion and implements a denoising neural
samplingamulti-viewvideoJ ∼ p(J|I,p)fromthefine-
network ϵˆ(J ,t,I,p) that takes as input a noised video
t tunedEmuVideomodel.GiventhevideoJ,wethendirectly
(cid:112)
J t = 1−σ t2J+σ tϵ, where ϵ ∼ N(0,I) is Gaussian fita3DmodelG. Whiletherearemanypossiblechoices
noiseandσ
t
∈[0,1]isthenoiselevel,andtriestoestimate
forthismodel,hereweuseGaussiansplatting(Kerbletal.,
4IM-3D:IterativeMultiviewDiffusionandReconstructionforHigh-Quality3DGeneration
2023),aradiancefieldthatusesalargenumberof3DGaus- invocationstothemodel.BecausethevideoJisalreadysuf-
sianstoapproximatethe3Dopacityandcolorfunctions. ficientlyview-consistent,the3Dreconstructionconverges
quicklytoagoodsolution. Furthermore,wecanadoptfast
Given the 3D model G and a camera viewpoint Π, the
stochasticODEsolverssuchasDPM++(Luetal.,2022)to
differentiableGaussiansplattingrendererproducesanim-
furtherreducethenumberofmodelevaluationstoobtain
age ˆI = R(G,Π). Compared to other methods such as
thevideointhefirstplace. Overall,comparedtousingthe
NeRF(Mildenhalletal.,2020),orevenfasterversionssuch
SDSloss,thenumberofmodelevaluationsisreducedbya
asDVGO(Sunetal.,2022)orTensoRF(Chenetal.,2022),
factor10-100×(seetheAppendixforadditionalanalysis).
thekeyadvantageofGaussiansplattingistheefficiencyof
thedifferentiablerenderer,bothintimeandspace,which DespitetheoverallconsistencyofgeneratedvideosJ,they
allowsrenderingafullhigh-resolutionimageIateachtrain- arestillnotperfect. Wethusadditionallycompensatefor
ingiterationinsteadofjustselectedpixelsasinmostprior suchinconsistenciesduringmodelfitting,butstillwithout
works.Becauseofthisfact,wecanutilizeimage-levellosses resortingtotheSDSloss. Instead,wealternate3Drecon-
such as LPIPS (Zhang et al., 2018), i.e., L (ˆI,I) = structionandvideogeneration.Todoso,oncethefirstvideo
LPIPS
(cid:80)Q ∥w ⊙(Φ (ˆI)−Φ (I))∥2whereΦ :R3×H×W → Jandcorresponding3DmodelG∗areobtained,weusethe
q=1 q q q q
RC is a family of Q patch-wise feature extractors imple- lattertogenerateavideoJ∗ =R(G∗,Π)usingthe3Dren-
derer,sampleanintermediatednoisedvideoJ∗byadding
mentedbytheVGG-VDneuralnetwork(Simonyan&Zis- t
noise to it as shown above, and then invoking the video
serman,2015). Wealsoutilizeasecondimage-basedloss
L ,themulti-scalestructuralsimilarityindexmeasure
generatoragaintoobtainadenoisedandupdatedvideoJ′.
MS-SSIM
(MS-SSIM) (Wang et al., 2003). Finally, we use a mask Weiteratethisprocesstwotimes. Thisisvastlyfasterthan
loss L Mask with masks obtained using the method intro- usingtheSDSlosswhilestillbeinghighlyrobust.
ducedin(Qinetal.,2022). Inourablationstudies,weshow
thesignificantbenefitsofusingtheseimage-basedlosses
4.Experiments
ratherthanthestandardpixel-wiseRGBlossL (ˆI,I)=
RGB
∥ˆI−I∥2. Ourfinallossistheweightedlosscombination Our method generates 3D objects from a textual descrip-
L = w LPIPSL LPIPS +w SSIML SSIM +w MaskL Mask. The ob- tionpandareferenceimageI. Inordertocomparetoprior
ject G is thus reconstructed via direct optimization, i.e., work,weconsiderinparticularthesetoftextualprompts
G∗ = argmin G(cid:80)K k=1L(R(G,Π k),[J] k) where [J] k de- from(Shietal.,2023b),whichareoftenusedforevaluation.
notesthek-thimageinthevideo.
Givenaninputimageandprompt(I,p),previousmethods
3.4.Fastsamplinganditerativegeneration eithersynthesizeamulti-viewimagesequenceJ(usually
bymeansofageneratornetwork), oroutputa3Dmodel,
The SDS loss can be seen as a way to bridge the gap be-
orboth. Wecomparethequalityoftheproducedartifacts
tweenimagegeneratorsthatareunawareof3Dobjectsand
visually,utilizingtheimagesequenceJdirectly,orcorre-
their3Dreconstructions,absorbingmulti-viewconsistency sponding renders Jˆ of the 3D model. In general, we can
defectsinthegeneration. Becauseourmodelisratherview-
expectthequalityandfaithfulnessofJtobebetterthanthat
consistentfromtheoutset,andbecausewecanuserobust of Jˆ because the generated image sequence needs not be
reconstructionlosses,theSDSlossisunnecessary. Instead, perfectlyview-consistent. Ontheotherhand,therendersJˆ
givenapromptp,wesimplygenerateanimageI,followed
fromthe3Dmodelareconsistentbyconstruction,butmay
byvideoJ,andthenfita3DobjectGtothelatter.
beblurrierthanJ,orcontainotherdefects.
Onemainadvantageisthatthisdramaticallyreducesthe
4.1.Comparisontothestate-of-the-art
numberofmodelevaluationscomparedtousingtheSDS
loss. OptimizingtheSDSlossis(approximately)thesame Inthissection,wecompareIM-3Dtorelevantstate-of-the-
asascendingthescore,i.e.,thegradient∇logp(J t|I,p)of artmethodsintheliterature,includingMVDream(Shietal.,
thelogdistributionovernoisedvideos(orimages),sothe 2023b),Zero123XL(Deitkeetal.,2023),Magic123(Qian
optimizationoftheassetGcanbeseenasaformofmulti- et al., 2023), SyncDreamer (Liu et al., 2023c), Image-
viewmodeseeking. Thescoreisobtainedfromthesame Dream (Wang & Shi, 2023), LRM (Hong et al., 2023)
networkϵˆ. However,despitetheconditioningonaspecific and One2345++ (Liu et al., 2023a). For LRM, since no
textualpromptpandinputviewI,thesampleddistribution public models are available, we utilize the open-source
isratherwide,requiringaverylargenumber(thousands)of OpenLRM(He&Wang,2023). ForOne2345++,whichis
iterationstoconvergetoamode;furthermore,regressingto onlyavailableviaawebinterface,wemanuallyuploadeach
amodereducesthediversityandqualityoftheoutput. imageintheevaluationset. Wecarryoutbothquantitative
andqualitativecomparisonsusingthesetofpromptsand
In our case, the network ϵˆis used to generate directly a
imagesfrom(Shietal.,2023b;Wang&Shi,2023).
singlevideoJ,whichisthenreconstructedwithoutfurther
5IM-3D:IterativeMultiviewDiffusionandReconstructionforHigh-Quality3DGeneration
Dragon armor A pig wearing a backpack A beautiful, intricate butterfly
A girl riding a wolf A bulldog wearing a pirate hat A beautiful, intricate butterfly Samurai koala bear
Zero-123-XL
A squirrel playing guitar Mecha vampire girl, chibi An astronaut riding a horse A ghost eating a hamburger
A monkey on a kick drum A katana A toy tank Military mech, future, scifi
Dragon armor A frog wearing a sweater A pig wearing a backpack Army jacket
A Darth Vader helmet
Figure3: QualitativeComparisons. OurmethodIM-3D(lastrow)andothersforthesametext/imagepromptpairs. For
IM-3D,weshowthefinalGSreconstruction(whichguaranteesmulti-viewconsistency).Wematchtheinputimagefaithfully
andobtainhigh-quality,detailedreconstructionsinjust3minutes. FastermethodssuchasOpenLRMarealsomuchworse.
Iteration 1 (Initial Reconstruction - Above) vs Iteration 2 (After Generation - Below) Iteration 1 (Initial Reconstruction - Above) vs Iteration 2 (After Generation - Below)
Input Image
A girl riding a wolf A bulldog wearing a pirate hat A beautiful, intricate butterfly Samurai koala bear An astronaut riding a horse A ghost eating a hamburger
A squirrel playing guitar Mecha vampire girl, chibi An astronaut riding a horse A ghost eating a hamburger
A bulldog Mecha vampire girl, chibi
Figure4: Avisualizationofreconstructionqualityovermultipleiterationsofmultiviewdiffusionandreconstruction.
Wecomparetheinitialreconstructionsobtainedbyourmodel(i.e. theresultoftrainingonourinitialgeneratedvideos)tothe
resultafteroneiterationofreconstructionandrefinement. Weseethatalthoughtheinitialreconstructionshavereasonable
A monkey on a kick drum A katana A toy tank Military mech, future, scifi shapes,theylackfine-graineddetailsduetosmallinconsistenciesinthegeneratedmultiviewimages. Afteroneiterationof
noising,denoising,andreconstruction,ourmethodresolvestheseinconsistenciesandproduces3Dassetswithsignificantly
higherlevelsofdetail(ashighlightedbytheredcirclesabove).
6
An astronaut riding a horse
Dragon armor A frog wearing a sweater A pig wearing a backpack Army jacket
Iteration 1 (Initial Reconstruction)
Input Image
Iteration 2 (After Generation)
A Darth Vader helmet
A girl riding a wolf
An astronaut riding a horse
Input ImageIM-3D:IterativeMultiviewDiffusionandReconstructionforHigh-Quality3DGeneration
Table 1: Faithfulness to the textual and visual prompts of image sequences synthesised or rendered by various
methods. Assessedonthepromptlistfrom(Wang&Shi,2023;Shietal.,2023b).
synthesizedview re-renderedview
model Time(min) CLIP(Text) CLIP(Image) CLIP(Text) CLIP(Image)
SDXL(Podelletal.,2023)[upperbound] 0.03 33.33 100 — —
MVDream(Shietal.,2023b) 72 31.26±2.9 76.44±6.5 30.63±2.7 76.94±5.2
Zero123XL(Deitkeetal.,2023) 10 19.58±1.3 60.29±5.8 29.06±3.3 81.33±6.9
Magic123(Qianetal.,2023) 15 — — 29.51±4.7 84.14±10.2
SyncDreamer(Liuetal.,2023c) 13 27.76±3.0 77.26±7.2 26.22±3.4 74.95±6.6
ImageDream(Wang&Shi,2023) 120 31.08±3.4 85.39±5.8 30.73±2.3 83.77±5.2
OpenLRM(Hongetal.,2023) 0.17 — — 29.75±3.2 83.08±9.5
One2345++(Liuetal.,2023a) 0.75 — — 29.71±2.3 83.78±6.4
IM-3D(ours) 3 31.92±1.6 92.38±5.1 31.66±1.7 91.40±5.5
Generation Quality Image Faithfulness Table2: Ablationontheimportanceoflosstermsand
100.0 3Drepresentationduringthefittingstage.
OpenLRM
97.4
100.0 Loss/Representation CLIP(Text) CLIP(Image)
Zero123-XL
92.3 7.7 IM-3D(ours) 31.66±1.7 91.40±5.5
92.3 7.7 -L 29.38±2.1 84.71±6.4
IM-3D Magic123 LPIPS
82.1 17.9 -L insteadofL 29.67±2.0 84.99±5.9
RGB LPIPS
56.4 43.6 -L SSIM 31.53±1.8 90.64±5.7
ImageDream
100.0 -L Mask 31.43±1.9 90.14±6.0
w/NeRFinsteadofGS 30.42±2.1 87.37±5.4
87.2 12.8 One2345++* IF
94.95.1
0 20 40 60 80 100
thanmost(3minutesvshoursforsomemodels).
Win Rate (Us)
Figure 5: Human evaluation. We perform human eval-
Humanevaluation. Automatedmetricsfortheevaluation
uation of IM-3D vs state-of-the-art in Image-to-3D and
ofgenerativemodelsarenotfullyrepresentativeofvalueof
Text-to-3D.HumanraterspreferredIM-3Dtoallcompeti-
theoutputinapplications. Thus,wealsoconductahuman
torswithregardtobothgenerationqualityandfaithfulness,
study. Weaskannotatorstoevaluateourmodelagainsta
oftenbyalargemargin.
competitorbasedon(1)ImageAlignmentand(2)3Dquality.
Quantitativecomparison. Table1providesaquantitative Wepresenttoannotatorswiththeoutputsoftwodifferent
comparison of our method to others. We adopt the same methods,renderedas360°videos,andaskthemtoindicate
metricsas(Shietal.,2023b;Wang&Shi,2023),whichare a preference based on these two criteria. Table 1 shows
basedontheCLIP(Radfordetal.,2021)similarityscores. thewinratewhencomparingourmethodsagainstothers.
Specifically, we utilize the ability of CLIP to embed text Ourmethodsurpassesinperformanceallotherbaselinesin
andimagesinthesamespace. Wethenusetheembeddings bothstudiesindicatingthattheproposedmethodproduces
tocomparethetextualpromptpandtheimagepromptIto high-quality 3D results that closely align with the image
theimagesJoftheobject(eithersynthesizedorrendered). prompt. FurtherdetailsareprovidedintheAppendix.
A high CLIP similarity score means high faithfulness to
theprompt. Asanupperbound, wealsoreporttheCLIP 4.2.Ablations
scoresofthepromptimagesIwhichweregeneratedusing
Effect of iterative refinements. In Figure 4, we demon-
theSDXL(Podelletal.,2023)model.
stratetheefficacyofourproposediterativerefinementpro-
ThekeytakeawayfromTable1isthatIM-3Doutperforms cess.Ourmodel’sinitialreconstructions(derivedfromtrain-
all others in terms of both textual and visual faithfulness. ingonourinitiallygeneratedvideos)arecomparedtothe
ThisistrueforboththeimagesequencesJoutputbythe outcomefollowingasingleiterationofmultiviewdiffusion
videogeneratoraswellastherendersJˆfromthefitted3D andreconstruction. Whiletheinitialreconstructionsexhibit
GSmodelsG. IM-3Disparticularlystrongwhenitcomes satisfactoryshapes, theymissoutonintricatedetailsdue
tovisualfaithfulness,whichalsomeansthattheimageswe tominorinconsistenciesintheinitialmultiviewimages. In
generateareofaqualitycomparabletotheinputimageI. afewinstances,somepartsoftheseinitialreconstructions
Additionally, our method requires significantly less time lookasiftwocopiesofashapehavebeensuperimposed
7
2.6IM-3D:IterativeMultiviewDiffusionandReconstructionforHigh-Quality3DGeneration
Pixel-Level Loss (L2 - Above) vs Image-Level Loss (LPIPS - Below) Pixel-Level Loss (L2 - Above) vs Image-Level Loss (LPIPS - Below)
Input Image
A girl riding a wolf A bulldog wearing a pirate hat A beautiful, intricate butterfly Samurai koala bear Military mech, future Flying dragon
A squirrel playing guitar Mecha vampire girl, chibi An astronaut riding a horse A ghost eating a hamburger
A katana A crab, low poly
Figure6: ReconstructionQualitywithPixel-LevelandImage-LevelLosses. Wefindthatimage-levellossesarecrucial
tothesuccessofourmethod. Withpixel-levellossessuchastheL2loss,smallinconsistenciesinthegeneratedimagesare
effectivelyaveragedtogether,resultinginunnaturalandblurry-lookingreconstructions.
A monkey on a kick drum A katana A toy tank Military mech, future, scifi
upon one another, as the reconstruction process tries to Table3:AblationonUsingFewerFrames.Weshowquan-
satisfytwoinconsistentviews. However,ourtechniquerec- titative performance when performing our reconstruction
tifiesthesediscrepancieswithoneiterationofdenoisingand andgenerationusingfewerframes.
reconstruction;significantlyenhancingthelevelofdetail.
#Frames CLIP(Text) CLIP(Image)
Image-LevelLosses. InTable2andFigure6,wecompare
An astronaut riding a horse 16 31.66±1.7 91.40±5.5
Dragon armor A frog wearing a sweater A pig wearing a backpack Army jacket resultsofoptimizationwithpixel-levelandimage-levelloss 8 31.38±1.8 90.06±6.3
functions. Wefindthatimage-leItvereatliolno 1 s (Is nie tias l Rea cor ne struc cte ionn )traltoour 4 30.06±2.6 86.96±8.6
method’sabilitytogeneratehigh-quality3Dassets. Theuse
Input Image 4.3.Limitations
ofpixel-levellossessuchasL2lossisdetrimental,asminor
inconsistenciesinthemultiviewimagesareemphasizedby The fine-tuned video generator is generally very view-
theoptimizationprocessandeffeItecrtaitivone 2l y(Aftaerv Geenreraatgione)dtogether. consistent,butitstillhaslimitations. Oneinterestingfailure
ThisaveragingresultsinalowCLIPscore(29.67vs31.66 caseisthatforhighlydynamicsubjects(e.g.,horses,which
A Darth Vader helmet
A girl riding a wolf
forLPIPS)aswellasblurryandunnaturalgenerations. areoftencapturedrunning),themodelsometimesrenders
An astronaut riding a horse
spurious animations (e.g., walking or galloping) despite
Comparing3DRepresentations ThelastlineofTable2 ourfine-tuning,whichisproblematicfor3Dreconstruction.
providesacomparisonof3Drepresentations,showingthe This occurs more often when the prompt contains verbs
effect of using NeRF as an underlying 3D representation describingmotion;seetheAppendixforanexample.
ratherthanGaussiansplatting(GS).Wefindthatthevisual
qualityofmodelsgeneratedusingNeRFisslightlyworse 5.Conclusions
than GS. The true benefit of GS is that it is much faster
Inthiswork,wehaveshownthatstartingfromavideogen-
andmuchmorememory-efficient;trainingwithGStakes
eratornetworkinsteadofanimagegeneratorcanresultin
3 minutes whereas training with NeRF takes 40 minutes.
bettermulti-viewgeneration,toapointwhereitcanimpact
Additionally,thememory-efficientnatureofGaussiansplat-
the design of future text-to-3D models. In fact, we have
tingmakesiteasytorenderatourdiffusionmodel’snative
shown that the quality is sufficient to eschew distillation
resolutionof512px,whereasforNeRFonehastouseray
losses like SDS as well as large reconstruction networks.
microbatchingoroptimizeatalowerresolution.
Instead,onecansimplyfitthe3Dobjecttothegenerated
UsingFewerFrames Differentlyfromthevastmajority viewsusingarobustimage-basedloss. Reconstructioncan
of other diffusion-based text-to-3D and image-to-3D ap- befurtheralternatedwithrefiningthetargetvideo,quickly
proaches,whichgenerateonly1-4frames,IM-3Dgenerates convergingtoabetter3Dobjectwithminimalimpacton
16framessimultaneously. Wedemonstratethesignificance efficiency. Compared to works that rely on SDS, our ap-
ofthisinTable3,findingthatourquantitativeperformance proachsignificantlyreducesthenumberofevaluationsof
improvesasweincreasethenumberofgeneratedframes. the 2D generator network, resulting in a faster and more
memory-efficientpipelinewithoutcompromisingonquality.
8
Input ImageIM-3D:IterativeMultiviewDiffusionandReconstructionforHigh-Quality3DGeneration
6.ImpactStatement Girdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S.,
Rambhatla, S. S., Shah, A., Yin, X., Parikh, D., and
OurworkusesGenerativeAI,whosepotentialimpactsare
Misra,I.Emuvideo:Factorizingtext-to-videogeneration
andhavebeenextensivelydiscussedintheacademic,busi-
byexplicitimageconditioning. CoRR,abs/2311.10709,
nessandpublicspheres. Ourworkdoesnotchangethese
2023.
issuesqualitatively.TheEmumodels(Daietal.,2023)were
explicitly designed with fairness and safety in mind, and Gupta, A., Xiong, W., Nie, Y., Jones, I., and Oguz, B.
fine-tuningthemoncurated3Dmodelsislikelytofurther 3DGen: Triplanelatentdiffusionfortexturedmeshgen-
reducethepotentialforharm. eration. corr,abs/2303.05371,2023.
He, Z. and Wang, T. Openlrm: Open-source large
References
reconstruction models. https://github.com/
Caron, M., Touvron, H., Misra, I., Je´gou, H., Mairal, J., 3DTopia/OpenLRM,2023.
Bojanowski, P., andJoulin, A. Emergingpropertiesin
self-supervisedvisiontransformers. InProc.ICCV,2021. Hong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D.,
Liu,F.,Sunkavalli,K.,Bui,T.,andTan,H. LRM:Large
Chen,A.,Xu,Z.,Geiger,A.,Yu,J.,andSu,H. TensoRF: reconstructionmodelforsingleimageto3D. arXiv,2023.
Tensorialradiancefields. InarXiv,2022.
Huang,Y.,Wang,J.,Shi,Y.,Qi,X.,Zha,Z.,andZhang,L.
Chen,R.,Chen,Y.,Jiao,N.,andJia,K. Fantasia3d: Dis- Dreamtime: Animprovedoptimizationstrategyfortext-
entangling geometry and appearance for high-quality to-3Dcontentcreation. CoRR,abs/2306.12422,2023.
text-to-3dcontentcreation: Disentanglinggeometryand
appearanceforhigh-qualitytext-to-3dcontentcreation. Jain,A.,Mildenhall,B.,Barron,J.T.,Abbeel,P.,andPoole,
arXiv.cs,abs/2303.13873,2023a. B. Zero-shottext-guidedobjectgenerationwithdream
fields. InProc.CVPR,2022.
Chen, Y., Fang, J., Huang, Y., Yi, T., Zhang, X., Xie, L.,
Wang, X., Dai, W., Xiong, H., and Tian, Q. Cascade- Katzir,O.,Patashnik,O.,Cohen-Or,D.,andLischinski,D.
Zero123: One image to highly consistent 3D with Noise-freescoredistillation. arXiv.cs,abs/2310.17590,
self-promptednearbyviews. arXiv.cs,abs/2312.04424, 2023.
2023b.
Kerbl,B.,Kopanas,G.,Leimku¨hler,T.,andDrettakis,G.
Chen,Z.,Wang,F.,andLiu,H. Text-to-3DusingGaussian 3DGaussianSplattingforReal-TimeRadianceFieldRen-
splatting. arXiv,(2309.16585),2023c. dering. Proc.SIGGRAPH,42(4),2023.
Cline,D.B.H. Admissibilekernelestimatorsofamulti- Kim,S.,Lee,K.,Choi,J.S.,Jeong,J.,Sohn,K.,andShin,
variatedensity. TheAnnalsofStatistics,16(4),1988. J. Collaborative score distillation for consistent visual
synthesis. arXiv.cs,abs/2307.04787,2023.
Dai, X., Hou, J., Ma, C., Tsai, S.S., Wang, J., Wang, R.,
Zhang, P., Vandenhende, S., Wang, X., Dubey, A., Yu, Kwak, J., Dong, E., Jin, Y., Ko, H., Mahajan, S., and Yi,
M.,Kadian,A.,Radenovic,F.,Mahajan,D.,Li,K.,Zhao, K. M. ViVid-1-to-3: Novel view synthesis with video
Y., Petrovic, V., Singh, M. K., Motwani, S., Wen, Y., diffusionmodels. arXiv.cs,abs/2312.01305,2023.
Song, Y., Sumbaly, R., Ramanathan, V., He, Z., Vajda,
P., and Parikh, D. Emu: Enhancing image generation Li,J.,Tan,H.,Zhang,K.,Xu,Z.,Luan,F.,Xu,Y.,Hong,Y.,
modelsusingphotogenicneedlesinahaystack. CoRR, Sunkavalli,K.,Shakhnarovich,G.,andBi,S. Instant3D:
abs/2309.15807,2023. Fast text-to-3D with sparse-view generation and large
reconstruction model. arXiv, 2023. URL https://
Deitke,M.,Schwenk,D.,Salvador,J.,Weihs,L.,Michel, instant-3d.github.io.
O.,VanderBilt,E.,Schmidt,L.,Ehsani,K.,Kembhavi,
A.,andFarhadi,A. Objaverse: Auniverseofannotated Lin,C.,Gao,J.,Tang,L.,Takikawa,T.,Zeng,X.,Huang,
3dobjects. arXiv.cs,abs/2212.08051,2022. X.,Kreis,K.,Fidler,S.,Liu,M.,andLin,T. Magic3D:
High-resolution text-to-3d content creation. arXiv.cs,
Deitke,M.,Liu,R.,Wallingford,M.,Ngo,H.,Michel,O., abs/2211.10440,2022.
Kusupati,A.,Fan,A.,Laforte,C.,Voleti,V.,Gadre,S.Y.,
VanderBilt,E.,Kembhavi,A.,Vondrick,C.,Gkioxari,G., Liu, M., Xu, C., Jin, H., Chen, L., T, M. V., Xu, Z., and
Ehsani,K.,Schmidt,L.,andFarhadi,A. Objaverse-XL: Su, H. One-2-3-45: Any single image to 3D mesh in
Auniverseof10M+3Dobjects. CoRR,abs/2307.05663, 45 seconds without per-shape optimization. arXiv.cs,
2023. abs/2306.16928,2023a.
9IM-3D:IterativeMultiviewDiffusionandReconstructionforHigh-Quality3DGeneration
Liu,R.,Wu,R.,Hoorick,B.V.,Tokmakov,P.,Zakharov,S., Qiu,L.,Chen,G.,Gu,X.,Zuo,Q.,Xu,M.,Wu,Y.,Yuan,
andVondrick,C. Zero-1-to-3: Zero-shotoneimageto3d W., Dong, Z., Bo, L., and Han, X. Richdreamer: A
object. CoRR,abs/2303.11328,2023b. generalizable normal-depth diffusion model for detail
richnessintext-to-3d. arXiv.cs,abs/2311.16918,2023.
Liu, Y., Lin, C., Zeng, Z., Long, X., Liu, L., Komura, T.,
and Wang, W. SyncDreamer: Generating multiview- Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,
consistent images from a single-view image. arXiv, Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,
(2309.03453),2023c. J.,Krueger,G.,andSutskever,I. Learningtransferable
visualmodelsfromnaturallanguagesupervision. InProc.
Long,X.,Guo, Y.,Lin, C.,Liu,Y.,Dou,Z.,Liu, L.,Ma, ICML,volume139,pp.8748–8763,2021.
Y.,Zhang,S.,Habermann,M.,Theobalt,C.,andWang,
W. Wonder3D:Singleimageto3Dusingcross-domain Shen,T.,Gao,J.,Yin,K.,Liu,M.-Y.,andFidler,S. Deep
diffusion. arXiv.cs,abs/2310.15008,2023. marching tetrahedra: a hybrid representation for high-
resolution3dshapesynthesis. AdvancesinNeuralInfor-
Lorraine, J., Xie, K., Zeng, X., Lin, C., Takikawa, T., mationProcessingSystems,34:6087–6101,2021.
Sharp, N., Lin, T., Liu, M., Fidler, S., and Lucas, J.
Shi,R.,Chen,H.,Zhang,Z.,Liu,M.,Xu,C.,Wei,X.,Chen,
ATT3D:amortizedtext-to-3Dobjectsynthesis. arXiv.cs,
L.,Zeng,C.,andSu,H. Zero123++: asingleimageto
abs/2306.07349,2023.
consistent multi-view diffusion base model. arXiv.cs,
Lu,C.,Zhou,Y.,Bao,F.,Chen,J.,Li,C.,andZhu,J. DPM- abs/2310.15110,2023a.
Solver: A fast ODE solver for diffusion probabilistic
Shi,Y.,Wang,P.,Ye,J.,Long,M.,Li,K.,andYang,X.MV-
modelsamplinginaround10steps. InProc.NeurIPS,
Dream: Multi-viewdiffusionfor3Dgeneration. arXiv.cs,
2022.
abs/2308.16512,2023b.
Melas-Kyriazi, L., Rupprecht, C., Laina, I., and Vedaldi,
Simonyan,K.andZisserman,A. Verydeepconvolutional
A. RealFusion: 360 reconstruction of any object from
networks for large-scale image recognition. In Proc.
asingleimage. InProceedingsoftheIEEEConference
ICLR,2015.
on Computer Vision and Pattern Recognition (CVPR),
2023. URL https://lukemelas.github.io/ Sun,C.,Sun,M.,andChen,H. Directvoxelgridoptimiza-
realfusion/. tion: Super-fast convergence for radiance fields recon-
struction. InProc.CVPR,2022.
Mercier,A.,Nakhli,R.,Reddy,M.,andYasarla,R. Hexa-
Gen3D:Stablediffusionisjustonestepawayfromfast Sun,J.,Zhang,B.,Shao,R.,Wang,L.,Liu,W.,Xie,Z.,and
anddiversetext-to-3Dgeneration. arXiv,2024. Liu,Y. DreamCraft3D:Hierarchical3Dgenerationwith
bootstrappeddiffusionprior. arXiv.cs,abs/2310.16818,
Mildenhall,B.,Srinivasan,P.P.,Tancik,M.,Barron,J.T.,
2023.
Ramamoorthi,R.,andNg,R.NeRF:Representingscenes
as neural radiance fields for view synthesis. In Proc. Szymanowicz, S., Rupprecht, C., and Vedaldi, A.
ECCV,2020. Viewset diffusion: (0-)image-conditioned 3D genera-
tive models from 2D data. In Proceedings of the In-
Podell,D.,English,Z.,Lacey,K.,Blattmann,A.,Dockhorn,
ternational Conference on Computer Vision (ICCV),
T., Mu¨ller, J., Penna, J., and Rombach, R. SDXL: im- 2023. URL https://szymanowiczs.github.
provinglatentdiffusionmodelsforhigh-resolutionimage io/viewset-diffusion.
synthesis. arXiv.cs,abs/2307.01952,2023.
Tang,J.,Ren,J.,Zhou,H.,Liu,Z.,andZeng,G. Dream-
Poole,B.,Jain,A.,Barron,J.T.,andMildenhall,B. Dream- Gaussian: Generativegaussiansplattingforefficient3D
Fusion: Text-to-3Dusing2Ddiffusion. InProc.ICLR, contentcreation. arXiv,(2309.16653),2023a.
2023.
Tang, J., Wang, T., Zhang, B., Zhang, T., Yi, R., Ma,
Qian,G.,Mai,J.,Hamdi,A.,Ren,J.,Siarohin,A.,Li,B., L., and Chen, D. Make-it-3d: High-fidelity 3d cre-
Lee,H.,Skorokhodov,I.,Wonka,P.,Tulyakov,S.,and ationfromAsingleimagewithdiffusionprior. arXiv.cs,
Ghanem,B. Magic123: Oneimagetohigh-quality3D abs/2303.14184,2023b.
objectgenerationusingboth2Dand3Ddiffusionpriors.
Tewari,A.,Yin,T.,Cazenavette,G.,Rezchikov,S.,Tenen-
arXiv.cs,abs/2306.17843,2023.
baum,J.B.,Durand,F.,Freeman,W.T.,andSitzmann,
Qin, X., Dai, H., Hu, X., Fan, D.-P., Shao, L., and Gool, V. Diffusion with forward models: Solving stochastic
L.V. Highlyaccuratedichotomousimagesegmentation. inverse problems without direct supervision. arXiv.cs,
InECCV,2022. abs/2306.11719,2023.
10IM-3D:IterativeMultiviewDiffusionandReconstructionforHigh-Quality3DGeneration
Wan, Z., Paschalidou, D., Huang, I., Liu, H., Shen, B.,
Xiang, X., Liao, J., and Guibas, L. Cad: Photoreal-
istic 3d generation via adversarial distillation. arXiv,
(2312.06663),2023.
Wang, P. and Shi, Y. ImageDream: Image-prompt
multi-view diffusion for 3D generation. arXiv.cs,
abs/2312.02201,2023.
Wang,P.,Fan,Z.,Xu,D.,Wang,D.,Mohan,S.,Iandola,
F., Ranjan, R., Li, Y., Liu, Q., Wang, Z., and Chandra,
V. SteinDreamer: Variancereductionfortext-to-3dscore
distillationviasteinidentity. arXiv,2023a.
Wang,Z.,Simoncelli,E.,andBovik,A. Multiscalestruc-
tural similarity for image quality assessment. In The
Thrity-SeventhAsilomarConferenceonSignals,Systems
& Computers, 2003, volume 2, 2003. doi: 10.1109/
ACSSC.2003.1292216.
Wang,Z.,Lu,C.,Wang,Y.,Bao,F.,Li,C.,Su,H.,andZhu,
J. ProlificDreamer: High-fidelityanddiversetext-to-3D
generation with variational score distillation. arXiv.cs,
abs/2305.16213,2023b.
Weng, H., Yang, T., Wang, J., Li, Y., Zhang, T., Chen, C.
L.P.,andZhang,L. Consistent123: Improveconsistency
foroneimageto3dobjectsynthesis. arXiv,2023.
Xu,D.,Yuan,Y.,Mardani,M.,Liu,S.,Song,J.,Wang,Z.,
andVahda,A. AGG:Amortizedgenerative3dgaussians
forsingleimageto3d. arXiv.cs,2024.
Xu, Y., Tan, H., Luan, F., Bi, S., Wang, P., Li, J., Shi,
Z.,Sunkavalli,K.,Wetzstein,G.,Xu,Z.,andZhang,K.
Dmv3d: Denoisingmulti-viewdiffusionusing3dlarge
reconstructionmodel,2023.
Yang,J.,Cheng,Z.,Duan,Y.,Ji,P.,andLi,H. Consistnet:
Enforcing3dconsistencyformulti-viewimagesdiffusion.
arXiv.cs,abs/2310.10343,2023.
Yi,T.,Fang,J.,Wu,G.,Xie,L.,Zhang,X.,Liu,W.,Tian,
Q., and Wang, X. Gaussiandreamer: Fast generation
fromtextto3dgaussiansplattingwithpointcloudpriors.
arXiv.cs,abs/2310.08529,2023.
Yu,W.,Yuan,L.,Cao,Y.,Gao,X.,Li,X.,Quan,L.,Shan,
Y., and Tian, Y. HiFi-123: Towards high-fidelity one
imageto3dcontentgeneration.arXiv.cs,abs/2310.06744,
2023.
Zhang,R.,Isola,P.,Efros,A.A.,Shechtman,E.,andWang,
O. Theunreasonableeffectivenessofdeepfeaturesasa
perceptualmetric. InProc.CVPR,pp.586–595,2018.
Zhu,J.andZhuang,P. HiFA:High-fidelitytext-to-3Dwith
advanced diffusion guidance. CoRR, abs/2305.18766,
2023.
11IM-3D:IterativeMultiviewDiffusionandReconstructionforHigh-Quality3DGeneration
A.Appendix
A.1.Trainingdetails
Inlinewith(Girdharetal.,2023),wemaintainthespatialconvolutionalandattentionlayersofEmuVideo,fine-tuning
onlythetemporallayers. Weminimizethestandarddiffusionlossoveraspanof5days,employing80A100GPUswitha
totalbatchsizeof240andalearningrateof1e-5. Ourfindingsindicatethatprolongedtrainingeffectivelycountersthe
network’sinclinationtogenerate360videosofdeformingobjects,giventhattheinitializationisavideogenerationmodel.
ContrarytoMVDream(Shietal.,2023b)andInstant3D(Lietal.,2023),weobservenodegradationintexturequalitywith
extendedtraining. Thiscanbeascribedtothefactthatthespatiallayersremainstaticandthenetworkisimage-conditioned,
necessitatingthatthegenerated360videoretainthehigh-frequencytextureelementsoftheinput.
ForGaussianfitting,weinitialize5000pointsatthecenterofthe3Dspace,anddensifyandprunetheGaussiansevery
50iterations. Weconductoptimizationfor1200iterationsandexecuteEmuVideotwicefor10iterationseachusingthe
DPMsolver(Luetal.,2022)duringthisprocess,repeatingthisevery500iterations. Empirically,wefoundthatsettingthe
weightstow =10,w =0.2andw =1yieldsthebestresultsduringthefittingstage.
LPIPS SSIM Mask
Priortofitting,weneedtoestimatetheelevationoftheveryfirstgeneratedvideo. Tothatend,wetrainedanelevation
estimatorontopofDINO(Caronetal.,2021)featuresusingthe100k3Drenderings. Thenetworkaveragesthefeaturesof
4uniformlydistributedframesandusesa2-layerMLPtoregresstheelevationinradians.
A.2.Humanevaluation
In our study, we employed the prompt set delineated in (Shi et al., 2023b) to conduct a human annotation evaluation
via Amazon Mechanical Turk (AMT). The task assigned to the annotators involved choosing between two 3D assets,
bothrenderedas360°videos, withoneoftheassetsbeingtheoutputofourproposedmethod. Toensurearobustand
unbiasedevaluation,werandomizedthepresentationorderofthemethodsforeachquestion. Eachquestionwasassessed
byfiveannotators, andwereportedtheconsensusopinion. Sinceeachquestioncorrespondstoatripletof(competing
method,3Dreconstruction,andquality/faithfulness),thisisatotalof5annotators×(5methods×39reconstructions×
2questiontypes)=1950annotations. Weinstructedtheannotatorstooverlookanydisparitiesinbackgroundcolors,as
normalizingallmethodstoyieldthesamescaleisanon-trivialtask.
A.3.Furtherinformationonnetworkefficiency
ProlificDreamer MVDream ImageDream Zero123XL SyncDreamer IM-3D
Numberof
320000 20000 25000 1200 200 80
networkcalls
Table4: Numberofdiffusionnetworkcallstogenerateone3Dasset. Theproposedmethod,IM-3D,requiresonlya
fractionofthemodelevaluationstocomputea3Dasset.
InTable4,wepresentthenumberofdiffusionmodelforwardpassesusedtoreconstructasingle3Dobjectforvarious3D
generationmethods. Whereassomeothermethodsrequirethousandsortensofthousandsofiterations,ourmethodrequires
lessthanonehundred.
A.4.FailureCases
Asdescribedinthelimitationssectionofthemainpaper,thevideogeneratordoesnotproduceperfectresultsinallcases. A
notableinstanceoffailureisobservedwithsubjectsthatexhibithighdynamism,suchashorsesoftendepictedinmotion.
Insuchcases,themodeloccasionallyproducesunwarrantedanimationslikewalkingorgalloping,whichdisruptsthe3D
reconstructionprocess. WeshowanexampleinFigure7.
A.5.ConversiontoMeshes
AlthoughGaussianSplattingisbeingrapidlyadoptedbythecomputervisionandgraphicscommunities,someproduction
applications require that objects be converted to meshes. We show that in these cases, it is straightforward to extract
12IM-3D:IterativeMultiviewDiffusionandReconstructionforHigh-Quality3DGeneration
Failure Case
A girl riding a wolf A bulldog wearing a pirate hat A beautiful, intricate butterfly Samurai koala bear A horse walking
Figure7: Visualisationofafailurecase. Inthiscase,ourfinetunedvideonetworkgeneratedananimatedvideoofahorse
walkingratherthanastaticvideo. Asaresult,the3Dreconstructionprocessproduceserroneousgeometry(e.g. theheadof
thehorseisbarelyvisiblefromsomeviews).
Gaussian Splatting (Above) & Mesh (Below) Gaussian Splatting (Above) & Mesh (Below)
A squirrel playing guitar Mecha vampire girl, chibi An astronaut riding a horse A ghost eating a hamburger
Input Image
A girl riding a wolf A bulldog wearing a pirate hat A beautiful, intricate butterfly Samurai koala bear Corgi riding a rocket Viking axe, fantasy
A monkey on a kick drum A katana A toy tank Military mech, future, scifi
A squirrel playing guitar Mecha vampire girl, chibi An astronaut riding a horse A ghost eating a hamburger
A frog wearing a sweater Mecha vampire girl, chibi
Figure8: VisualisationofMAn easstrhoneasut. riWdineg ac hoornsevertourGaussianSplattingrepresentationtoDMTet(Shenetal.,2021)using
Dragon armor A frog wearing a sweater A pig wearing a backpack Army jacket
marchingcubesandoptimizetheresultingmeshesusingouriterativemultiviewdiffusionandreconstructionprocess.
Iteration 1 (Initial Reconstruction)
A monkey on a kick drum A katana A toy tank Military mech, future, scifi
high-quality meshes from our GInpauuts IsmiaagneSplatting representation: one can run marching cubes (Cline, 1988) and then
optionallycontinuetooptimizetheresultingmeshusingDMTet(Shenetal.,2021). Weshowvisualresultsofconverting
ourmodelstomeshesinFigure8.
Iteration 2 (After Generation)
A Darth Vader helmet
A girl riding a wolf An astronaut riding a horse
Dragon armor A frog wearing a sweater A pig wearing a backpack Army jacket
An astronaut riding a horse
Iteration 1 (Initial Reconstruction)
Input Image
Iteration 2 (After Generation)
A Darth Vader helmet
A girl riding a wolf
An astronaut riding a horse
13
Input Image
Input Image