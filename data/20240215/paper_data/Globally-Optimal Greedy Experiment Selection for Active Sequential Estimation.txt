Globally-Optimal Greedy Experiment Selection for
Active Sequential Estimation
Xiaoou Li
University of Minnesota
Hongru Zhao
University of Minnesota
Abstract
Motivated by modern applications such as computerized adaptive testing, sequen-
tial rank aggregation, and heterogeneous data source selection, we study the problem
of active sequential estimation, which involves adaptively selecting experiments for se-
quentially collected data. The goal is to design experiment selection rules for more
accurate model estimation. Greedy information-based experiment selection methods,
optimizing the information gain for one-step ahead, have been employed in practice
thanks to their computational convenience, flexibility to context or task changes, and
broad applicability. However, statistical analysis is restricted to one-dimensional cases
duetotheproblem’scombinatorialnatureandtheseeminglylimitedcapacityofgreedy
algorithms, leaving the multidimensional problem open.
In this study, we close the gap for multidimensional problems. In particular, we
propose adopting a class of greedy experiment selection methods and provide statisti-
cal analysis for the maximum likelihood estimator following these selection rules. This
class encompasses both existing methods and introduces new methods with improved
numerical efficiency. We prove that these methods produce consistent and asymptoti-
callynormalestimators. Additionally,withinadecisiontheoryframework,weestablish
thattheproposedmethodsachieveasymptoticoptimalitywhentheriskmeasurealigns
with the selection rule. We also conduct extensive numerical studies on both simulated
and real data to illustrate the efficacy of the proposed methods.
From a technical perspective, we devise new analytical tools to address theoretical
challenges. For instance, we demonstrate that functions of inverted Fisher information
have a regularization effect when used in selection rules, thereby automatically explor-
ing necessary experiments. Additionally, we show that a class of greedy and stochastic
optimization methods converges to the minimum of a convex function over a simplex
1
4202
beF
31
]TS.htam[
1v20680.2042:viXraalmostsurely. Theseanalyticaltoolsareofindependenttheoreticalinterestandmaybe
reused in related problems involving stochastic approximation and sequential designs.
Keywords: Active sequential estimation, optimality theory, sequential analysis, computer-
ized adaptive testing
1 Introduction
In many modern applications, data are collected sequentially and adaptively through varied
experiments, with the distribution being influenced by both unknown model parameters and
the experiments. Active sequential estimation, which involves the adaptive selection of the
experiments, enables more efficient model estimation. It has received considerable attention
across various disciplines recently. A few examples are provided below.
Computerized Adaptive Testing (CAT) CAT refers to a form of educational assess-
ment where test items are administered adaptively and sequentially based on the test taker’s
responses to previous items. For instance, if a test taker answers questions correctly, they
may receive a more challenging item subsequently. Over the past decades, CAT has gained
popularity due to its ability to achieve a more accurate assessment with fewer test items
compared to traditional non-adaptive tests. To implement CAT, Item Response Theory
(IRT) models are typically employed (Chen et al., 2024; Reckase, 2006). IRT models assume
that a test-taker’s responses, whether correct or incorrect, are influenced by both their latent
trait parameter and the selected item. A crucial aspect of CAT design involves developing
effective item selection rules to estimate the latent trait parameter as accurately as possible.
For a comprehensive review on this topic, see Bartroff et al. (2008); Chang and Ying (2009);
Wang et al. (2017), and the references therein.
Sequential rank aggregation The rank aggregation problem involves inferring a global
rank for a set of items by aggregating noisy pairwise comparison results. This problem
finds applications across various domains such as social choice (Saaty and Vargas (2012)),
sports (Elo (1978)), and search rankings (Page et al. (1999)). Statistical models such as the
Bradley-Terry model (Bradley and Terry, 1952), which assigns a latent score parameter to
each object, are often utilized to model the noisy pairwise comparison results. Subsequently,
the global rank can be inferred from the estimated latent score parameters. Recently, the
sequentialrank aggregation problemhas attracted increasedinterest. This approach involves
sequentially and adaptively selecting the next pair to compare based on the comparison
results of previously selected pairs (see, e.g.,Chen et al. (2013, 2022, 2016)). A key question
2ofinterestisthedesignofpairselectionrulestoenhancetheefficiencyoftherankaggregation
process.
Besides the aforementioned applications, additional areas of application include active
sampling in signal processing (Mukherjee et al., 2022), active contextual search (Chen et al.,
2023), and dynamic pricing (Chen and Wang, 2023), among others.
In all of the above applications, the problem can be formulated as a sequential design-
and-estimation problem where data X ,··· ,X ,··· are collected sequentially. Each X has
1 n n
a density function f (·) relative to a baseline measure, with θ ∈ Rp representing the
θ,an
underlying model parameter, a ∈ A denoting the experiment selected at time n, and A
n
being a finite set encompassing all possible experiment choices. For example, in the context
of CAT, θ corresponds to the latent proficiency level of a test-taker on p subjects or skills,
a indicates the n-th test item, A indicates the item bank which collects all the potential
n
test items, and X ∈ {0,1} indicates that whether the test-taker answers the n-th question
n
correctly or not. At each time step n, a decision maker needs to select an experiment a
n
based on the past observations X ,a ,X ,a ,··· ,X ,a , sample X accordingly, and
1 1 2 2 n−1 n−1 n
construct an estimator θ(cid:98) for estimating θ. The goal is to find a good adaptive experiment
n
selection rule and an estimator θ(cid:98) so that θ(cid:98) is as accurate as possible, where N could be
N N
a fixed sample size or a random stopping time depending on the application.
Greedy information-based experiment selection rules that maximize one-step-ahead in-
formation gain have been commonly adopted for item selection in CAT (see, e.g., Chang and
Ying (1996); Cheng (2009); Van Der Linden (1999); Wang and Chang (2011)). For example,
Wang and Chang (2011) and Tu et al. (2018) describe the following experiment selection
rule:
(cid:104) (cid:105)
a = argmintr
(cid:8)
I(θ(cid:98)ML;a
,a)(cid:9)−1
, (1)
n+1 n n
a∈A
where a = (a ,··· ,a ) denotes the experiments selected up to time n,
n 1 n
n
(cid:88)
θ(cid:98)ML = argmax logf (X )
n
θ
θ,ai i
i=1
denotes the maximum likelihood estimator (MLE) with n observations,
n
1 (cid:110)(cid:88) (cid:111)
I(θ(cid:98)ML;a ,a) = I (θ(cid:98)ML)+I (θ(cid:98)ML)
n n n+1 ai n a n
i=1
represents the rescaled Fisher information matrix associated with the first n experiments
and one extra experiment a, while I (θ) = E [∇logf (X){∇logf (X)}T] denotes
a X∼f θ,a θ,a
θ,a
the Fisher information matrix associated with the experiment a at the parameter θ. Other
3experiment selection rules in a similar form (e.g., substituting the trace function with other
functions like logdet(·)) are also explored in Wang et al. (2011).
These information-based experiment selection rules offer several benefits. First, the se-
lection processes only require the calculation of the Fisher information and are easy to
implement. Moreover, they are inherently parallelizable, offering scalability when |A| is
large. Second, they quantify the information gain associated with each experiment, thereby
providing priority scores for them. This feature enables extension of these rules to various
contexts and tasks (e.g., A varies over time). Additionally, given a parametric model, these
rules can readily address problems in other applications.
Despite the computational advantages and wide applicability, the statistical analysis of
greedy information-based experiment selection methods is limited to the one-dimensional
case (p = 1) in existing research. In this context, Chang and Ying (2009) established the
consistency, asymptotic normality and optimality results for the MLE, and discussed the
application in CAT. However, the multidimensional (p > 1) case remains an open problem,
partly due to the challenges regarding the combinatorial nature of the multidimensional
problem and the seemingly limited capacity of greedy methods. The following example,
which mimics the settings of an educational test measuring two latent traits, illustrates that
one has to combine experiments carefully in order to obtain a consistent and/or risk-optimal
estimator.
Example 1. Let θ = (θ ,θ )T and A = {1,2,3}. Let f be the probability mass func-
1 2 θ,a
tion for Bernoulli variables with the probability parameter (1 + exp(−θ + 0.1))−1, (1 +
1
exp(−θ ))−1, and (1+exp(−θ /2−θ ))−1, for a = 1,2,3, respectively. Let n be the number
2 1 2 k
of times that experiment k is selected and π = n /n be its frequency (k = 1,2,3) with
k k
(cid:80)
n = n . Then, a necessary condition for the existence of a consistent estimator θ(cid:98) is
k k n
max{min(n ,n ),min(n ,n ),min(n ,n )} → ∞. Moreover, in order to minimize the mean
1 2 1 3 2 3
squared error E∥θ(cid:98) − θ∥2 asymptotically, a necessary condition is (π ,π ,π ) → π∗(θ) as
n 1 2 3
the total sample size grows, where π∗(θ) is a vector-valued optimal proportion function de-
pending on θ. See Figure 1 for an illustration of the function π∗(θ) and additional details
in Section 5.2.
Inthisexample, achievingconsistentorasymptoticallyoptimalestimatorsrequiresexper-
imentstobecombinedcarefullywithaparameter-dependentfrequency. However,information-
based selection methods, being one-step-ahead greedy, do not consider the benefits of com-
bining experiments or multi-step planning. Thus, it remains an open question whether these
selection methods lead to consistent, asymptotically normal, or risk-optimal estimators.
In this study, we provide a definitive answer to the above question for a class of greedy-
information-based experiment selection rules. In particular, we introduce two experiment
4selection rules based on a pre-specified criterion function G : Rp×p → R,
θ
(cid:104) (cid:105)
GI0 : a = argminG (cid:8) I(θ(cid:98)ML;a ,a)(cid:9)−1 , and (2)
n+1
a∈A
θ(cid:98)nML n n
(cid:104) (cid:105)
GI1 : a = argmaxtr ∇G (cid:0) Σ(cid:98) (cid:1) Σ(cid:98) I (θ(cid:98)ML)Σ(cid:98) , (3)
n+1
a∈A
θ(cid:98)nML n n a n n
(cid:16) (cid:17)
where Σ(cid:98) = {I(θ(cid:98)ML;a )}−1, ∇G (Σ) = ∂G θ(Σ) denotes the gradient of G
n n n θ ∂Σij
1≤i,j≤p
θ
with respect to its matrix input and recall I(θ(cid:98)ML;a ) = 1 (cid:80)n I (θ(cid:98)ML). We refer to the
n n n i=1 ai n
selection rule in (2) as the zero-order greedy information-based selection rule (GI0), and
that in (3) as the first-order greedy information-based selection rule (GI1), because GI0 is
minimizing a certain function of the Fisher information at the next time point, while GI1 is
derived based on a first-order Taylor expansion of GI0; see Section 3 for more details. GI0
generalizes the selection rule in (1), accommodating more diverse settings. New methods can
be obtained by specifying an appropriate function G . GI1 offers a class of new experiment
θ
selection rules which share similar asymptotic properties as GI0 but are computationally
more efficient when both p and |A| are large.
Our main theoretical contributions are as follows. First, we show that MLE is strongly
consistent and asymptotically normal when using GI0 or GI1 as the experiment selection
rule, under mild conditions. Second, we derive the asymptotic covariance matrix of the MLE
as a function involving G (·) and the Fisher information. Third, we prove that the empirical
θ
frequency of selected experiments converges to a limiting frequency. Fourth, we show that
theexperimentselectionruleGI0(orGI1)combinedwiththeMLEisasymptoticallyoptimal
in minimizing certain risk measures related to G (·). In particular, if G (·) = tr(·), then the
θ θ
MLE has the smallest asymptotic mean squared error (MSE), when compared with other
experiment selection rules and estimators. Moreover, these results are valid not only for
fixed sample sizes, but also for random stopping times, which is beneficial for applications
that use early stopping criteria.
Beyond the methodological and theoretical contributions, we have developed new ana-
lytical tools for addressing technical challenges. For example, we show that the inverted
Fisher information, through its directional derivatives in experiment selection rules, acts as
a regularizer. This facilitates automatic exploration of necessary experiments, removing the
need for additional exploration steps traditionally employed in stochastic control methods
for related problems (e.g., two-stage design in sequential design for hypothesis tests (Cher-
noff, 1959; Naghshvar and Javidi, 2013)). Furthermore, we show that a class of greedy
and stochastic optimization methods converges to the minimum of a convex function over a
simplex almost surely. In addition, we refine and extend several classic results in stochastic
5analysis, suchasAnscombe’stheorem(Anscombe,1952)andtheRobbins-Siegmundtheorem
(Robbins and Siegmund, 1971). These theoretical results and technical tools are important
in their own right and may be reused in other related problems. See Section 6 for more
details of the technical challenges and our new analytical tools.
The rest of the paper is organized as follows. Section 2 formalizes the active sequential
estimation problem. Section 3 introduces the greedy information-based experiment selection
rules GI0 and GI1, elaborating on their implementation. Section 4 offers the main theo-
retical results regarding the MLE and the experiment selection rules. Section 5 details the
methods and theory in applications including the item selection in CAT and sequential rank
aggregation. Section 6 gives new analytical tools and a proof sketch. Section 7 presents two
simulation studies, which illustrate the finite sample performance and the computational
efficiency of the proposed methods. Section 8 showcases the performance of the proposed
method on a real-data example. Section 9 summarizes the main results and provides discus-
sions on future directions. All the technical proofs for the theoretical results and additional
simulation results are given in the supplementary material.
1.1 Notations
In this paper, we use the following notations and mathematical conventions. Let C and C
represent generic constants that are bounded from above and below, respectively. These
generic constants are independent of θ and a ∈ A, and their values may vary from place to
place. Let |A| denote the cardinality of a set A. Let I(·) denote the indicator function. Let
I denote the p×p identity matrix. The inner product between real matrices (or vectors)
p
A and B of the same size is defined by ⟨A,B⟩ = tr(ATB). For a real matrix A, define
the operator norm ∥A∥ as the maximum singular value of A. For a vector x, denote
op
(cid:112)
its Euclidean norm by ∥x∥ = ⟨x,x⟩. For a symmetric matrix A, λ (A), λ (A),
max min
and κ(A) denote its maximum eigenvalue, minimum eigenvalue, and condition number,
respectively. If A is a positive definite matrix, then κ(A) = λmax(A). For a differentiable
λmin(A)
matrix function G(Σ), its gradient is denoted by ∇G(Σ), and is defined as the matrix such
that G(Σ + ∆Σ) − G(Σ) = ⟨∇G(Σ),∆Σ⟩ + o(∥∆Σ∥). For symmetric matrices A and
B, define the partial order A ⪯ B if and only if B −A is a positive semidefinite matrix.
Throughout the paper, all the vectors are column vectors, unless otherwise specified.
62 Problem Statement
Let X ,··· ,X ,··· be data collected sequentially, A be a finite set with cardinality k,
1 n
and a ,··· ,a ,··· ∈ A be the experiments selected at different time points. Denote by
1 n
F = σ(a ,X ,··· ,a ,X ), the sigma field that contains information of the observations
n 1 1 n n
and the selected experiments up to time n. At each time n, a decision maker needs to select
the experiment a adaptively based on past information. That is, a is measurable with
n+1 n+1
respect to F . Throughout the study, we assume that the distribution of X satisfies
n n+1
X |F ∼ f (·) for θ ∈ Θ ⊂ Rp
n+1 n θ,an+1
where θ is a p-dimensional model parameter, Θ is a compact parameter space and f (·)
θ,an+1
denotes the probability density of X with respect to a baseline measure. That is, X
n+1 n+1
is assumed to follow a parametric model, and its distribution is determined by both the
underlying model parameter θ and the selected experiment a .
n+1
In an active sequential estimation problem, the goal is to design an experiment selection
rule for {a } and find an estimator θ(cid:98) that is measurable with respect to F , so that θ(cid:98)
n n≥1 n n n
is close to the true underlying parameter θ∗ with high probability. In some applications, the
data collection process may be stopped early to save for the sampling cost. In these cases,
we are also interested in θ(cid:98) , where N is a random stopping time.
N
3 Methods
For the estimation method, we focus on the MLE, although some of the methods and the-
oretical results may be extended to other estimators. The definition of MLE is given as
follows. Let the selected experiments up to time n be a = (a ,··· ,a ). Then, the rescaled
n 1 n
log-likelihood and the corresponding MLE are
n
1 (cid:88)
l (θ) = l (θ;a ) = logf (X ), and (4)
n n n
n
θ,ai i
i=1
θ(cid:98)ML ∈ argmaxl (θ;a ). (5)
n n n
θ∈Θ
Weproposeadoptingtwoexperimentalselectionrules,includingthezero-ordergreedyinformation-
based selection rule GI0 and the first-order greedy information-based selection rule GI1. The
precise description of these methods are given in Algorithms 1 and 2.
We explain steps in Algorithms 1 and 2. First, we note that both algorithms require a
pre-specified criterion function G : Rp×p → R. Motivated by Kiefer (1974) on the design of
θ
7Algorithm 1 GI0 Algorithm
1: Input: θ(cid:98) , a0,··· ,a0 .
0 1 n0
2: Require: θ(cid:98) 0 ∈ Θ, a0 1,··· ,a0 n0 ∈ A such that (cid:80)n i=0 1I a0 i(θ(cid:98) 0) is nonsingular.
3: Initialization: a = a0,··· ,a = a0 ,collectingresponsesX ,X ,··· ,X correspond-
1 1 n0 n0 1 2 n0
ingly.
4: for n = n to N do
0
5: calculating the MLE θ(cid:98)ML according to equation (5)
n
6: selecting experiment a according to equation (2)
n+1
7: collecting response X corresponding to the selected experiment a
n+1 n+1
8: end for
9: Output: θ(cid:98)ML
N
Algorithm 2 GI1 Algorithm
1: Input: θ(cid:98) , a0,··· ,a0 .
0 1 n0
2: Require: θ(cid:98) 0 ∈ Θ, a0 1,··· ,a0 n0 ∈ A such that (cid:80)n i=0 1I a0 i(θ(cid:98) 0) is nonsingular.
3: Initialization: a = a0,··· ,a = a0 ,collectingresponsesX ,X ,··· ,X correspond-
1 1 n0 n0 1 2 n0
ingly.
4: for n = n to N do
0
5: calculating the MLE θ(cid:98)ML according to equation (5)
n
6: selecting experiment a according to equation (3)
n+1
7: collecting response X corresponding to the selected experiment a
n+1 n+1
8: end for
9: Output:θ(cid:98)ML
N
experiments, a reasonable choice is

logdet(Σ), if q = 0;


G (Σ) = Φ (Σ) = tr(Σq), if 0 < q < 1; (6)
θ q

 (tr(Σq))1/q, if q ≥ 1,
for a prespecified q ≥ 0. In the context of adaptive item selection in CAT, GI0 with
G (Σ) = Φ (Σ)hasbeenadoptedinVanDerLinden(1999)andWangandChang(2011). In
θ q
particular, theselectionrulein(1)correspondstoGI0withG (·) = Φ (·). BothGI0andGI1
θ 1
arerelativelynewinotherapplicationsdescribedinSection1. NotethatforG (Σ) = Φ (Σ),
θ q
it is a function independent with the input θ. Another option is G (Σ) = tr(H Σ), where
θ θ
H is a positive definite matrix depending on θ. This criterion function is useful in the cases
θ
where we would like to assign different weights to different values of θ. For more details,
please refer to Theorem 4.8.
Second, both algorithms require an initialization step where n experiments are selected
0
so that the Fisher information matrix I(θ(cid:98) ;a ) is nonsingular. This initialization step
0 n0
8ensures that I(θ(cid:98) ;a ) is nonsingular and the experiment selection rules in (2) and (3) are
n n
well-defined for all n ≥ n . In practice, it is usually straightforward to find such a0,··· ,a0 .
0 1 n0
For instance, in Example 1, we could choose θ(cid:98) = (0,0), (a0,a0) = (1,2), and n = 2. Then,
0 1 2 0
ateachtimepoint, thealgorithmfirstcalculatestheMLEbasedontheavailableinformation,
selects a new experiment according to (2) for GI0 (or (3) for GI1), and then samples a new
observation according to the selected experiment.
We refer to the selection rule in Algorithm 1 as GI0 and that in Algorithm 2 as GI1,
(cid:104) (cid:105)
because GI0 tries to minimize the criterion function G (cid:8) I(θ(cid:98)ML;a ,a)}−1 for one-step
θ(cid:98)n n n
ahead, while GI1 tries to minimize its first-order approximation, i.e.,
(cid:104) (cid:105) (cid:104) (cid:105)
G (cid:8) I(θ(cid:98)ML;a ,a)}−1 −G (cid:8) I(θ(cid:98)ML;a )}−1
θ(cid:98)nML n n θ(cid:98)nML n n
≈(cid:10) πa −π , ∂ G (cid:2)(cid:8)(cid:88) π(a)I (θ(cid:98)ML)(cid:9)−1(cid:3)(cid:12) (cid:12) (cid:11)
n+1 n ∂π θ(cid:98)nML a n (cid:12)
π=πn
a∈A (7)
1 (cid:104) (cid:16) (cid:17) (cid:105)
=− tr ∇G {I(θ(cid:98)ML;a )}−1 {I(θ(cid:98)ML;a )}−1I (θ(cid:98)ML){I(θ(cid:98)ML;a )}−1
n+1 θ(cid:98)nML n n n n a n n n
1 (cid:104) (cid:16) (cid:17) (cid:105)
+ tr ∇G {I(θ(cid:98)ML;a )}−1 {I(θ(cid:98)ML;a )}−1 ,
n+1 θ(cid:98)nML n n n n
where the empirical frequency vector π is defined as
n
(cid:16)1 (cid:17)
π = π [a ] = |{i;a = a,1 ≤ i ≤ n}| , (8)
n n n i
n
a∈A
witha = (a ,··· ,a )collectsexperimentsselecteduptotimen,andπa (a′) = n π (a′)+
n 1 n n+1 n+1 n
1 I(a = a′) is the empirical frequency at the time n+1 if a′ is selected at that time. Note
n+1
that GI0 minimizes the first line of (7), GI1 minimizes the first term on the last equation
of (7), and the second term on the last equation of (7) does not depend on the choice of
experiment a. This suggests that GI0 and GI1 are asymptotically equivalent, although the
rigorous theoretical justification is much more involved.
3.1 Improving Computational Efficiency
Ifk,parelarge, andI (θ)hassomelow-dimensionalrepresentation, GI1canbeimplemented
a
with improved numerical efficiency. In particular, we consider two specific cases which are
commonly seen in applications, including (1) low-rank information: I (θ) = L (θ)LT(θ)
a a a
where L (θ) ∈ Rp×s for all a and θ and s < p; (2) sparse and low-rank information: L (θ)
a a
has no more than s non-zero rows. For these cases, Algorithm 2 can be implemented using
the following accelerated version.
9Algorithm 3 Accelerated GI1 Algorithm
We modify line 6 in Algorithms 2, while keeping the other lines of the algorithms
unchanged.
6: selecting experiment a according to
n+1
M = {I(θ(cid:98)ML;a )}−1∇G (cid:0) {I(θ(cid:98)ML;a )}−1(cid:1) {I(θ(cid:98)ML;a )}−1,
n n θ(cid:98)nML n n n n
(cid:104) (cid:105)
a = argmaxtr LT(θ(cid:98)ML)ML (θ(cid:98)ML) .
n+1 a n a n
a∈A
Lemma 3.1. Assume the computational complexity of evaluating G (Σ) and ∇G (Σ) is no
θ θ
more than O(p3). Given the MLE θ(cid:98)ML and I(θ(cid:98)ML;a ), we have
n n n
1. the computational complexity for each iteration in GI0 is of the order O(kp3);
2. the computational complexity for each iteration in the accelerated GI1 Algorithm 3 is
O(ksp2+p3), assuming that the information matrices are low-rank matrices with given
L (θ) ∈ Rp×s. Moreover, the computational cost for the accelerated GI1 Algorithm 3
a
becomes O(ks2p+p3) if L (θ) has no more than s non-zero rows.
a
According to the above lemma, the accelerated GI1 algorithm is computationally much
more efficient than GI0, when k and p are large and s is small. Numerical results supporting
these findings can be found in Section 7.2.
3.2 Early Stopping
Inmanyapplications,thedatacollectionprocessisstoppedearlywhensufficientobservations
have been gathered to make accurate statistical inference. For instance, in the context of
CAT, educational tests often have variable lengths determined by specific early stopping
rules. These rules generally lead to less fatigue and a better experience for examinees. In
this section, we introduce two early stopping rules suitable for active sequential estimation.
The first stopping rule τ(1) is concerned with the estimation of a differentiable function
c
of the parameter h(θ) ∈ R, and it is defined as
(cid:8) (cid:9)
τ(1) = min m ≥ n ;S(cid:99)E(h(θ(cid:98)ML)) ≤ c , (9)
c 0 m
2
where S(cid:99)E (h(θ(cid:98)ML)) = 1{∇h(θ(cid:98)ML)}T{I(θ(cid:98)ML;a )}−1∇h(θ(cid:98)ML). The second stopping rule
m m m m m m
τ(2) is concerned with the estimation of the vector θ, and is defined as
c
(cid:110) (cid:111) 1 (cid:16) (cid:17)
(cid:91) (cid:91)
τ(2) = min m ≥ n ;MSE(θ(cid:98)ML) ≤ c , where MSE(θ(cid:98)ML) = tr {I(θ(cid:98)ML;a )}−1 . (10)
c 0 m m m m m
10(cid:91)
Here, S(cid:99)E serves as an approximation of sd(h(θ(cid:98)ML)) and MSE(θ(cid:98)ML) serves as an approxima-
m m
tion of MSE(θ(cid:98) mML) = E θ∗∥θ(cid:98) mML−θ∗∥2. Both rules terminate the data collection process once
a certain error estimator falls below a predetermined threshold c.
4 Theoretical Results
In this section, we first introduce the regularity conditions, and then present the main the-
oretical results regarding the consistency, asymptotic normality, and the optimality of the
proposed method.
4.1 Regularity Conditions
Throughout Section 4, we make the following Assumptions 1–5, along with Assumptions 6A
and 7A, and we will refer to this set of assumptions as the ‘regularity conditions’. All the
theoreticalresultsstillholdwhen6Aand7AarereplacedwiththemorerelaxedAssumptions
6B and 7B.
Assumption 1. The parameter space Θ is a non-empty compact and convex subset of Rp.
The true parameter θ∗ is an interior point of Θ.
Assumption 2. The support of the probability density f , denoted as supp(f ), depends
θ,a θ,a
only on a and does not depend on θ, where the support of a function is defined as
supp(f ) = cl{xa;f (xa) > 0},
θ,a θ,a
and cl(S) denotes the closure of a set S. Moreover, for all a ∈ A and Xa ∈ supp(f ),
θ,a
the gradient ∇ logf (Xa) = (∂logf θ,a(Xa) ) and the Hessian matrix ∇2 logf (Xa) =
θ θ,a ∂θi 1≤i≤p θ θ,a
(∂2logf θ,a(Xa)
) exist, where θ = (θ ,··· ,θ )T. Assume that there exist functions Ψa
∂θi∂θj 1≤i,j≤p 1 p 1
and Ψa satisfying sup E {Ψa(Xa)}2 < ∞, sup E Ψa(Xa) < ∞,
2 θ∈Θ Xa∼f θ,a 1 θ∈Θ Xa∼f θ,a 2
∥∇ logf (Xa)−∇ logf (Xa)∥ ≤ Ψa(Xa)∥θ −θ ∥, and (11)
θ θ1,a θ θ2,a 1 1 2
(cid:13) (cid:13)
(cid:13)∇2 logf (Xa)−∇2 logf (Xa)(cid:13) ≤ Ψa(Xa)∥θ −θ ∥, (12)
θ θ1,a θ θ2,a op 2 1 2
for all θ ,θ ∈ Θ and a ∈ A. Furthermore, for all a ∈ A,
1 2
supE {∥∇ logf (X)∥2} < ∞ and supE {(cid:13) (cid:13)∇2 logf (X)(cid:13) (cid:13) } < ∞.
X∼f θ∗,a θ θ,a X∼f θ∗,a θ θ,a op
θ∈Θ θ∈Θ
11Assumption 3. The Fisher information matrices satisfy the following conditions:
I (θ) = E (cid:2) ∇ logf (X){∇ logf (X)}T(cid:3) = −E (cid:8) ∇2 logf (X)(cid:9) ,
a X∼f θ,a θ θ,a θ θ,a X∼f θ,a θ θ,a
and those Fisher information matrices are continuously differentiable with respect to θ for
(cid:80)
all a ∈ A. Furthermore, I (θ) is positive definite for every θ ∈ Θ.
a∈A a
Assumption 4. Let M(θ;π) = (cid:80) π(a)E {logf (X)} for π = (π(a)) . As-
a∈A X∼f θ∗,a θ,a a∈A
sume the following uniform law of large numbers holds for all sequence a = (a ,··· ,a )
n 1 n
such that a is measurable with respect to F , for all 1 ≤ i ≤ n:
i i−1
(cid:26) (cid:27)
P lim sup|l (θ;a )−M(θ;π [a ])| = 0 = 1, (13)
n n n n
n→∞θ∈Θ
where π [a ] = (π (a;a )) , and π (a;a ) = 1|{i;a = a,1 ≤ i ≤ n}| denotes the
n n n n a∈A n n n i
empirical frequency that the experiment a is selected up to time n.
Assumption 5. The criterion function G takes one of the following forms:
θ
1. G (·) = Φ (·) for some q ≥ 0 where Φ (·) is defined in (6), or
θ q q
2. the function G (·) : Rp×p (cid:55)→ R is convex, and it satisfies: for all positive defi-
θ
nite matrix Σ, ∇ ∇G (Σ) and ∇2G (Σ) are continuous in (θ,Σ); and for all pos-
θ θ θ
itive definite matrices satisfying A ⪰ B, we have G (A) ≥ G (B). Additionally,
θ θ
sup κ(∇G (A)) < ∞ and lim inf G (A) = ∞.
A θ λmax(A)→∞ θ∈Θ θ
Assumption 6A (Reparametrization). There exist matrices {Z } and probability den-
a a∈A
sity functions {h (·)} satisfying the following requirements
Zaθ,a a∈A
1. Z is a matrix of dimension p ×p with rank p and f (·) = h (·) for all a ∈ A.
a a a θ,a Zaθ,a
2. Let ξ = Z θ be a reparametrization of θ. Assume that the Fisher information matrix
a a
of each experiment a is nonsingular with respect to ξ . That is, the compressed Fisher
a
information matrix
I (ξ ) = E (cid:2) ∇ logh (X){∇ logh (X)}T(cid:3)
ξa,a a X∼h
ξa,a
ξa ξa,a ξa ξa,a
= −E (cid:8) ∇2 logh (X)(cid:9)
X∼h ξa,a ξa ξa,a
is nonsingular for all θ ∈ Θ.
Assumption 7A (Identifiability). There exists a constant C > 0 such that for all θ ∈ Θ,
D (h ∥h ) ≥ C∥ξ∗ −ξ ∥2, (14)
KL ξ a∗,a ξa,a a a
12where ξ∗ = Z θ∗ is the compressed parameter after reparametrization, and D (h ∥h )
a a KL ξ a∗,a ξa,a
denotes the Kullback–Leibler divergence between the density functions h and h , and
(cid:16) (cid:17)
ξ a∗,a ξa,a
is defined as D (h ∥h ) = E log h ξa∗,a(X) .
KL ξ a∗,a ξa,a X∼h ξa∗,a h ξa,a(X)
Wecommentontheaboveregularityconditions. Assumptions1, 2, 3and4areextensions
of standard regularity conditions for the consistency of the MLE based on independent and
identically distributed (i.i.d.) observations (see, e.g., Chapter 5 of Van der Vaart (2000)).
In particular, Assumption 1 ensures the existence of MLE. Assumption 2 requires that the
gradient of log-density function associated with each experiment is stochastic Lipschitz and
has a bounded second moment. Condition (12) can be replaced by a more relaxed condition:
(cid:13) (cid:13)
(cid:13)∇2 logf (Xa)−∇2 logf (Xa)(cid:13) ≤ Ψa(Xa)ψ(∥θ −θ ∥), (15)
θ θ1,a θ θ2,a op 2 1 2
where ψ : [0,∞) → [0,∞) is a strictly increasing continuous function such that ψ(0) = 0.
Assumption 3 requires that the Fisher information matrices are well-behaved. Under this
assumption, each Fisher information matrix I (θ) may be singular, but their sum is nonsin-
a
gular. In other words, if we combine all the experiments together, the Fisher information
matrix is nonsingular. Assumption 4 requires that the log-likelihood follows the uniform law
of large numbers. This assumption can be verified by uniform martingale laws of large num-
bers(seeRakhlinetal.(2015))inmostapplications. Assumption5describestherequirement
on the criterion function G (·). Assumptions 6A and 7A require that for each experiment
θ
a, we can reparameterize the model with a new parameter ξ with possibly lower dimension
a
p ≤ p such that ξ is locally identifiable around the true model parameter, and the Fisher
a a
information matrix with respect to ξ is nonsingular. Note that Fisher information with
a
respect to θ may be singular in this case.
All the regularity assumptions are easily satisfied in practical problems, including the
item selection in CAT and the sequential rank aggregation problem described in Section 1.
See Section 5 for detailed justifications of the assumptions in these applications. Note that
6A and 7A can be relaxed to a more general condition, allowing for non-linear model repa-
rameterization. These relaxed conditions are provided below.
(cid:80)
Assumption 6B. For Q ⊂ A, define a vector space V = V (θ) = R(I (θ)), where
Q Q a∈Q a
R(A) represents the column space of a matrix A. Assume that the dimension dim(V (θ))
Q
does not depend on θ, and there exist constants 0 < c ≤ c < ∞, which do not depend on Q
and θ, such that for all Q ⊂ A and θ ∈ Θ
(cid:88)
c·P ⪯ I (θ) ⪯ c·P , (16)
VQ(θ) a VQ(θ)
a∈Q
13where P denotes the orthogonal projection matrix onto vector space V (θ).
VQ(θ) Q
Assumption 7B. Let SA = {π = (π(a)) : (cid:80) π(a) = 1 and π(a) ≥ 0 for all a ∈ A}
a∈A a∈A
denote the simplex in RA. Assume that there exists a positive constant C such that for all
π ∈ SA and θ ∈ Θ,
(cid:88) (cid:88)
π(a)D (f ∥f ) ≥ C π(a)(θ −θ∗)TI (θ∗)(θ −θ∗), (17)
KL θ∗,a θ,a a
a∈A a∈A
where D (f ∥f ) is the Kullback–Leibler divergence between the density functions f
KL θ∗,a θ,a θ∗,a
and f .
θ,a
4.2 Main Theoretical Results
In this section, we present the main theoretical results, including the consistency, asymptotic
normality and the optimality of the proposed method. Recall that the regularity conditions
(Assumptions 1 – 5, along with Assumptions 6A – 7A or 6B – 7B) are assumed throughout
the section.
4.2.1 Strong Consistency
We start with the strong consistency of the MLE following GI0 or GI1.
Theorem 4.1 (Strong consistency). Let θ(cid:98)ML be the MLE following the experiment selection
n
rule GI0 or GI1, as described in Algorithm 1 and Algorithm 2. Then,
lim θ(cid:98)ML = θ∗ a.s. P ,
n ∗
n→∞
where P denotes the data-generating probability distribution under the true model parameter
∗
θ∗.
Theorem 4.1 suggests that the MLE will be close to the true model parameter with a
large sample size following GI0 or GI1.
4.2.2 Limiting Selection Frequency and Asymptotic Normality of MLE
Let
(cid:88)
Iπ(θ) = π(a)I (θ),
a
a∈A
be the weighted Fisher information associated with a proportion vector π ∈ SA. The
distribution of MLE depends on the empirical frequency vector π , which is defined by (8).
n
14We first present an auxiliary asymptotic normality result for the MLE following a general
active experiment selection rule that is not necessarily GI0 or GI1.
Theorem 4.2 (Asymptoticnormalityfollowinggeneralexperimentselectionrules). Let θ(cid:98)ML
n
be the MLE calculated according to (5) following an active experiment selection rule that is
not necessarily GI0 or GI1. Let π be the corresponding empirical frequency vector.
n
Assume that there exists π ∈ SA such that π converges to π in probability P as n → ∞,
n ∗
and Iπ(θ∗) is nonsingular. Then,
√ (cid:16) (cid:17)
n(θ(cid:98)ML −θ∗) →d N 0 ,(cid:8) Iπ(θ∗)(cid:9)−1 as n → ∞, (18)
n p p
d
where ‘→’ denotes the convergence in distribution.
The above Theorem 4.2 extends the classic asymptotic normality results for MLE to the
sequential setting with active experiment selection. It roughly states that if the frequency
of the selected experiment approximates a limiting proportion as the sample size grows, and
the Fisher information weighted by the limiting proportion is nonsingular, then the MLE is
asymptotically normal and the asymptotic covariance matrix is the inverted weighted Fisher
information. Next, we will show that if we follow the experiment selection rule GI0 or GI1,
then the frequency for the selected experiments is approaching a limiting proportion that
is determined by the criterion function G . For this purpose, we first define a function
θ
F : SA → R,
θ
(cid:104)(cid:110)(cid:88) (cid:111)−1(cid:105)
F (π) = G π(a)I (θ) . (19)
θ θ a
a∈A
Theorem 4.3 (Limiting experiment selection frequency following GI0 or GI1). Assume that
F (π) has a unique minimizer, denoted by π∗. That is, π∗ = argmin F (π). Then,
θ∗ π∈SA θ∗
GI0 and GI1 both satisfy
lim π = π∗ a.s. P , (20)
n ∗
n→∞
where π is the corresponding empirical frequency vector. Moreover, for a general function
n
F (·) whose minimizer is not necessarily unique, we have
θ∗
lim nβ{F (π )− min F (π)} = 0 a.s. P . (21)
θ∗ n θ∗ ∗
n→∞ π∈SA
for all 0 ≤ β < 1/2, given that GI0 or GI1 is used as the experiment selection rule.
The asymptotic normality of the MLE following GI0 or GI1 is proved by combining the
above two theorems. We summarize this result in the next theorem.
15Theorem 4.4 (Asymptotic normality following GI0 or GI1). Let θ(cid:98)ML be the MLE following
n
the experiment selection rule GI0 or GI1, as described in Algorithm 1 and Algorithm 2.
Assume F (π) has a unique minimizer π∗. Then,
θ∗
√ (cid:16) (cid:17)
n(θ(cid:98)ML −θ∗) →d N 0 ,(cid:8) Iπ∗(θ∗)(cid:9)−1 . (22)
n p p
ThecovarianceoftheMLEcanbeapproximatedbytheplug-inestimatorn−1{Iπn(θ(cid:98)ML)}−1.
n
This is justified by the next theorem.
Theorem 4.5 (Asymptotic covariance matrix of the MLE). Under the settings of Theo-
rem 4.4,
√
n(cid:8) Iπn(θ(cid:98)ML)(cid:9)1/2 (θ(cid:98)ML −θ∗) →d N (0 ,I ). (23)
n n p p p
In addition, for any continuously differentiable function g : Θ → R such that ∇g(θ∗) ̸= 0 ,
p
√
n(g(θ(cid:98) nML)−g(θ∗))
→d N(cid:0) 0,1(cid:1)
. (24)
(cid:13) (cid:13)
(cid:13)(cid:8) Iπn(θ(cid:98)ML)(cid:9)−1/2
∇g(θ(cid:98)ML)(cid:13)
(cid:13) n n (cid:13)
Thefirstpartoftheabovetheoremjustifiestheuseoftheplug-inestimatorforthecovari-
ance matrix of the MLE. The second part of the theorem suggests that the approximate 1−α
(cid:13) (cid:13)
confidence interval for g(θ) can be constructed as g(θ(cid:98)ML)±z
(cid:13)(cid:8) Iπn(θ(cid:98)ML)(cid:9)−1/2
∇g(θ(cid:98)ML)(cid:13)
n α/2(cid:13) n n (cid:13)
where z is the 1−α/2 quantile of the standard normal distribution.
α/2
4.2.3 Asymptotic Optimality
In this section, we present results regarding the optimality of the proposed methods. We
consider two notions of optimality, including the optimal design and asymptotic efficiency of
the estimators under a decision theory framework. The former extends a similar concept in
the literature on the design of experiments, and the latter builds upon the classic asymptotic
efficiency results for MLE with i.i.d. observations. We start with the notion of optimality in
terms of the optimal design.
Definition 4.6 (G - optimality). A selection rule is said to be G a.s. optimal design if
θ∗ θ∗
its corresponding selection frequency {π } satisfies
n n∈Z +
lim G ({Iπn(θ∗)}−1) = min G ({Iπ(θ∗)}−1) a.s. P . (25)
θ∗ θ∗ ∗
n→∞ π∈SA
The above notion of G - optimal selection rules extends the classic concept of optimal
θ∗
designs adopted in the literature on the design of experiments (see, e.g., Kiefer (1974); Yang
16et al. (2013)). It allows for general criteria functions and adaptive experiment selection
rules. If an adaptive experiment selection rule is G - optimal, it approximately minimizes
θ∗
the criterion function when the sample size is large. Theorem 4.3 implies the following result.
Theorem 4.7 (G - optimal selection). Both GI0 and GI1 are G a.s. optimal.
θ∗ θ∗
The above theorem indicates that the proposed experiment selection rules have the best
performance in some sense when compared with other experiment selection rules. Next, we
consider the optimality property of the MLE when combined with GI0 or GI1 under the lens
of a sequential decision theory framework for the design-and-estimation problem.
Consider a loss function L(θ∗,θ(cid:98)) for an estimator θ(cid:98) following an active experiment se-
lection rule, and the corresponding risk E θ∗L(θ∗,θ(cid:98)). The next theorem first establishes a
lower bound for the asymptotic risk for unbiased estimators and then shows that the MLE
combined with the selection rule GI0 (or GI1) achieves this lower bound when the criterion
function matches the loss function.
Theorem 4.8 (Minimum risk for unbiased estimators). Let L(θ,θ(cid:98)) be a loss function twice
continuously differentiable in θ(cid:98)satisfying that L(θ,θ(cid:98)) ≥ 0, L(θ,θ(cid:98)) = 0 if and only if θ(cid:98)= θ,
and ηI ⪯ 1∇2L(θ∗,θ(cid:98)) ⪯ η′I for some positive constants η and η′, and all θ(cid:98) ∈ Θ. Let
p 2 θ(cid:98) (cid:12) p
H = 1∇2L(θ,θ(cid:98))(cid:12) . Then, the following results hold.
θ 2 θ(cid:98) (cid:12) θ(cid:98)=θ
1. Assume regularity conditions (but without Assumption 5) hold. Consider an unbiased
estimator T of θ following an arbitrary adaptive experiment selection rule. If the loss
n
function does not satisfy L(θ∗,θ(cid:98)) ≡ ⟨H θ∗(θ∗−θ(cid:98)),θ∗−θ(cid:98)⟩, we further assume for any
ε > 0, limsup E n∥T −θ∗∥2I(∥T −θ∗∥ > ε) = 0. Then,
n→∞ θ∗ n n
(cid:104) (cid:105)
liminfE n·L(θ∗,T ) ≥ inf tr(H {Iπ(θ∗)}−1). (26)
θ∗ n θ∗
n→∞ π∈SA
In particular, if the squared error loss L(θ,θ(cid:98)) = ∥θ−θ(cid:98)∥2 is used, then for any unbiased
(cid:104) (cid:105)
estimator T , liminf n·MSE(T ) ≥ inf tr({Iπ(θ∗)}−1).
n n→∞ n π∈SA
2. Under Assumptions 1-4, 6A and 7A, and further assume that there exists α > 0, such
that for any ξ = Z θ,θ ∈ Θ and xa ∈ supp(f ),
a a θ,a
λ (−∇2 logh (xa)) ≥ α > 0. (27)
min ξa ξa,a
Assume there exists δ > 0 such that E ∥∇ logf (Xa)∥2+δ < ∞. Assume that
Xa∼f θ∗,a θ θ,a
tr(H {Iπ(θ∗)}−1) has a unique minimizer, denoted by π∗. If we choose G (Σ) =
θ∗ θ
17tr(H Σ) and use the experiment selection rule GI0 (or GI1) described in Algorithm 1
θ
(or Algorithm 2), then the MLE achieves the lower bound in (26). That is,
lim E θ∗{n·L(θ∗,θ(cid:98) nML)} = min tr(H θ∗{Iπ(θ∗)}−1). (28)
n→∞ π∈SA
In particular, if L(θ,θ(cid:98)) = ∥θ − θ(cid:98)∥2, the corresponding criterion function is G (·) =
θ
Φ (·) = tr(·). MLE combined with GI0 (or GI1) achieves the asymptotic lower bound
1
for n·MSE(T ) for unbiased estimator T .
n n
The first part of the above theorem provides a lower bound for the risk of any unbiased
estimator combined with an arbitrary experiment selection rule. In particular, when p =
|A| = 1, it aligns with the classic Cram´er - Rao lower bound for the variance of unbiased
estimators with independent observations. The second part of the theorem suggests that
the asymptotic risk of the MLE combined with the proposed GI0 (or GI1) matches the
lower bound, if the criterion function aligns with the loss function. When p = |A| = 1, this
matching risk gives an extension of the classic asymptotic efficiency result for MLE with
i.i.d. data.
We note that Theorem 4.8 does not directly imply that the proposed method minimizes
risk within a class of decision rules, since the MLE is not necessarily unbiased. This scenario
is analogous to the classic asymptotic efficiency result for MLE with i.i.d. observations,
where the MLE is shown to have the asymptotic variance matching the Cram´er - Rao bound
for unbiased estimators but the MLE itself is not unbiased. On the other hand, the asymp-
totic optimality of the MLE within a decision theory framework can be formalized using
concepts such as local asymptotically normal (LAN) estimators and asymptotic concentra-
tion (see Chapter 8 of Van der Vaart (2000)) in classic asymptotic statistics. The next
theorem suggests that MLE combined with the proposed experiment selection method is
also asymptotically optimal in a similar sense. Here, we omit the definitions of notations
and terminology such as “⇝”, “⋆”, and “bowl-shaped functions”, and refer readers to The-
orem 8.8 and 8.11 in Chapter 8 of Van der Vaart (2000), as the formal definitions of these
notations are lengthy.
Theorem 4.9 (Local asymptotic minimax risk). Assume a ,··· ,a ,··· are experiments
1 n
selected following an active experiment selection rule such that a is measurable with respect
n+1
to F for all n. Assume that the sequence (T (a ,X ,··· ,a ,X ),π ) is regular at (θ,π) ∈
n n 1 1 n n n
Θ×SA for estimating parameter θ, which means that for every h ∈ Rp,
√ (cid:16) h (cid:17) θ+√h θ+√h
n T −(θ + √ ) ⇝n Lπ and π ⇝n π, (29)
n n θ n
18for some distribution Lπ, and Iπ(θ) = (cid:80) π(a)I (θ) is nonsingular. Then, the following
θ a∈A a
statements hold.
1. (Convolution theorem) There exists a probability measure Mπ such that
θ
Lπ = N (0 ,{Iπ(θ)}−1)∗Mπ. (30)
θ p p θ
In particular, if Lπ has the covariance matrix Σπ, then Σπ ⪰ {Iπ(θ)}−1.
θ θ θ
2. (Local asymptotic minimax theorem) For any bowl-shaped loss function ℓ,
(cid:16)√ h (cid:17)
sup liminfsupE ℓ n(cid:0) T −(θ + √ )(cid:1) ≥ Eℓ(Vπ) ≥ minEℓ(Vπ), (31)
|F|<∞,F⊂Rp n→∞ h∈F
θ+√h
n
n
n π
where the first supremum is taken over all finite subsets F of Rp, and Vπ ∼
N (0 ,{Iπ(θ)}−1).
p p
In the case where ℓ(θ) = ∥θ∥2 and G (·) = Φ (·) = tr(·), the second part of Theorem 4.8
θ 1
together with Theorem 4.9 imply that the MLE combined with both GI0 and GI1 selection
achieves the local asymptotic minimax lower bound on the MSE of estimators.
4.3 Theoretical Results Regarding Early Stopping Rules
As discussed in Section 3.2, early stopping rules are adopted in many applications to reduce
the expected sample size. In this section, we provide consistency and asymptotic normality
results for the MLE obtained at a large random stopping time.
Theorem 4.10 (Strongconsistencyatarandomstoppingtime). Let θ(cid:98)ML be the MLE follow-
n
ing the experiment selection rule GI0 or GI1, as described in Algorithm 1 and Algorithm 2,
and let τ ∈ N be a sequence of stopping time with respect to the filtration {F } such
n n n∈Z +
that lim τ = ∞ a.s. and τ < ∞ a.s. for each n. Then,
n→∞ n n
lim θ(cid:98)ML = θ∗ a.s. P .
n→∞
τn ∗
The above theorem extends Theorem 4.1 to allow for random stopping times. It suggests
that the MLE is close to the true model parameter at a large random sample size. Next,
we present the result on asymptotic normality, which enables statistical inference at large
stopping times.
Theorem 4.11 (Asymptotic normality following GI0 or GI1 with an early stopping rule).
Let θ(cid:98)ML be the MLE following the experiment selection rule GI0 or GI1, as described in
n
19Algorithm 1 and Algorithm 2. Assume F (π) has a unique minimizer π∗. Let {c }
θ∗ n n≥0
be a positive and decreasing sequence such that c → 0 as n → ∞. Let h : Θ → R
n
be a continuously differentiable function such that ∇h(θ) ̸= 0 for all θ ∈ Θ. Consider
p
stopping times τ(1) and τ(2) defined in (9) and (10), respectively. Then, for both stopping
cn cn
time τ = τ(1) and τ = τ(2), we have
n cn n cn
√
τ (cid:8) Iπτn(θ(cid:98)ML)(cid:9)1/2 (θ(cid:98)ML −θ∗) →d N (cid:0) 0 ,I (cid:1) . (32)
n τn τn p p p
Furthermore, for any continuously differentiable function g : Θ → R such that ∇g(θ∗) ̸= 0 ,
p
√
τ (g(θ(cid:98)ML)−g(θ∗))
n τn →d N(0,1). (33)
(cid:13) (cid:13)
(cid:13)
(cid:13)(cid:8)
Iπτn(θ(cid:98) τM
nL)(cid:9)−1/2
∇g(θ(cid:98) τM nL)(cid:13)
(cid:13)
5 Applications
In this section, we provide details on the methods and theoretical results to applications
discussed in Section 1, including item selection in CAT and adaptive pairs selection in se-
quential rank aggregation problems. We also provide results regarding active estimation for
generalized linear models (GLM), which encompass many useful models as its special cases.
5.1 Active Estimation for GLM
Consider the case where the distribution of the observations falls into an exponential family
(see, e.g., McCullagh (2019)). Following the setting in Section 5 of Chaudhuri et al. (2015),
we consider the density functions
(cid:8) (cid:9)
f (x ) = ζa(x )exp x zTθ −B (zTθ) , (34)
θ,a a a a a a a
where x ∈ R, z ∈ Rp, and B (·),a ∈ A. Assume that the support of B is R. Under
a a a a
this model, θ serves as the unknown linear coefficient in a GLM and we are interested in
estimating it using the proposed Algorithm 1 and Algorithm 2. The Fisher information is
given by
n
(cid:88)
I (θ) = B′′(zTθ)z zT and I(θ;a ) = B′′(zTθ)z zT. (35)
a a a a a n ai ai ai ai
i=1
Based on the above equations, Algorithms 1 and 2 are simplified as follows.
20Algorithm 4 Simplified GI0/GI1 Algorithm for GLM
We modify the following lines in Algorithms 1 and 2, while keeping the other lines of
the algorithms unchanged.
2: Require: choose a0,··· ,a0 such that dim(span{z ;n = 1,2,··· ,n }) = p.
1 n0 a0 n 0
6: The Fisher information matrices used in line 6 of Algorithms 1 and 2 are calculated
using the formula (35).
Corollary 5.1. Assume the function B has the support R, dim(span{z ;a ∈ A}) = p, and
a a
Assumptions 1 and 5 hold. If the above Algorithm 4 for GI0 or GI1 is used, then all the
theorems presented in Section 4.2 hold.
Note that in the above corollary, the assumptions are greatly simplified compared to the
regularity conditions described in Section 4.1, thanks to the nice form of GLMs. It only
requires that the parameter space is compact, the true parameter is an interior point of the
parameter space, and the parameter is identifiable when using all the experiments together.
In practice, the parameter space Θ may not be given in advance. In these cases, we may
specify Θ as a box (i.e., Θ = [−r,r]p) or ball (i.e., Θ = {θ : ∥θ∥ ≤ r}) for some large r.
The theoretical results still apply, if the true parameter is an interior point of the parameter
space.
5.2 Computerized Adaptive Testing (CAT)
CAT has gained prominence in recent decades as an innovative approach to educational
assessment (Bartroff et al., 2008; Chang and Ying, 2009; Wainer et al., 2000). In CAT,
test items are sequentially and adaptively chosen from an item pool based on the test-
taker’s previous responses. This approach enhances test precision and shortens test length
by selecting items tailored to the test-taker’s individual latent traits. Item Response Theory
(IRT) and Multidimensional Item Response Theory (MIRT) models are commonly used to
modelatest-taker’sresponses(See, e.g., Chenetal.(2024), EmbretsonandReise(2013), and
Reckase (2006) for reviews on IRT and MIRT models). In a binary MIRT model, a response
to an item is coded as 0 or 1, where 1 indicates that the item was answered correctly and 0
indicates the it was answered incorrectly.
Let k be the total number of items in the item pool for an educational test, and let
A = {1,··· ,k} represent the indices of these items. Under a MIRT model, each item
j ∈ A is associated with a multidimensional item parameter (z ,b ), which quantifies item
j j
properties such as the item’s difficulty and the skills it measures. The test taker is associated
with a latent trait parameter θ ∈ Rp, typically interpreted as proficiency in p different skills.
21Given the selected items and the test-taker’s latent trait parameter, responses are assumed
to be conditionally independent. The correct response probability P(θ;z ,b ), also known
j j
as the item response function (IRF) of item j, is a function of θ and depends on (z ,b ).
j j
Forexample, thecommonlyadoptedmultidimensionaltwo-parameterlogisticmodel(M2PL)
assumes that the IRF takes the form
P(θ;z ,b ) = {1+exp(−zTθ −b )}−1, (36)
j j j j
where z is the discrimination parameter, indicating the strength of each latent trait’s influ-
j
ence on the response, and −b is the difficulty parameter of item j.
j
Item selection is critical for efficient CAT design. The objective is to accurately estimate
the latent trait parameter θ ∈ Rp by selecting the next item a ∈ A based on previously
n+1
selected items and responses a ,X ,··· ,a ,X . Note that item parameters are typically
1 1 n n
pre-calibrated based on historical data and are assumed to be known in CAT. In the rest of
the section, we provide details on applying the item selection rules GI0 and GI1 under the
M2PL model. First, the density function is f (x) = P(θ;z ,b )x(1−P(θ;z ,b ))1−x, and
θ,a a a a a
the corresponding Fisher information is
(cid:88)
I (θ) = P(θ;z ,b )(1−P(θ;z ,b ))z zT and I(θ;a ) = π (a)I (θ). (37)
a a a a a a a n n a
a∈A
If the criterion function G (·) = Φ (·), then GI1 can be simplified as:
θ q
(cid:16) (cid:17)−q−1
a = argmaxP(θ(cid:98)ML;z ,b )(1−P(θ(cid:98)ML;z ,b ))·zT I(θ(cid:98)ML;a ) z . (38)
n+1 n a a n a a a n n a
a∈A
Algorithm 5 Simplified GI0/GI1 Algorithm for M2PL model
We modify the following lines in Algorithms 1 and 2, while keeping the other lines of
the algorithms unchanged.
2: Require: dim(span{z ;a ∈ A}) = p and choose a0,··· ,a0 such that
a 1 n0
dim(span{z ;n = 1,2,··· ,n }) = p.
a0 0
n
6: The Fisher information matrices used in line 6 of Algorithms 1 for GI0 are calculated
using the formula (37). Selection in line 6 of Algorithms 2 for GI1 is replaced by (38).
Corollary 5.2. Assume Assumption 1 holds, dim(span{z ;a ∈ A}) = p, and criterion
a
function G (·) = Φ (·). If we consider simplified Algorithm 5 for GI0 and GI1 with M2PL
θ q
model, then the conclusions for GI0 and GI1 from all the theorems presented in Section 4.2
hold.
225.3 Sequential Rank Aggregation from Noisy Pairwise Compari-
son
Consider the problem of determining the global rank over p+1 objects. Let A ⊂ {(j,l);j,l ∈
{0,1,2,...,p}} be a subset of all possible pairs for comparison. At each time n, a pair
a = (a ,a ) ∈ A is chosen for comparison, yielding a random pairwise comparison
n n,1 n,2
outcome X ∈ {0,1}. Here, X = 1 indicates that the object a is preferred over a in
n n n,1 n,2
the comparison, and X = 0 indicates the opposite. To infer the global rank of objects,
n
ranking models (e.g., Bradley-Terry-Luce (BTL) model (Bradley and Terry, 1952; Duncan,
1959)andtheThurstonemodel(Thurstone,1927))areusuallyassumedforthenoisypairwise
comparison results. These models assume that each object i is associated with a latent score
parameter θ , the pairwise comparison result between object i and object j is depending on
i
θ and θ , and the true global rank is the rank of the latent score parameters. For example,
i j
the BTL model assumes
(cid:18) eθi (cid:19)x(cid:18) eθj (cid:19)1−x
f (x) = (39)
θ,a eθi +eθj eθi +eθj
for the pair a = (i,j). For sequential rank aggregation, the goal is to design an active pair
selection rule that determines the next pair a for comparison based on the prior pair
n+1
comparison results (a ,X ,··· ,a ,X ), so that the global rank can be inferred accurately.
1 1 n n
This problem boils down to the active sequential estimation of the latent score parameters.
In the rest of the section, we elaborate on the implementation and theoretical results for
GI0 and GI1 for the sequential rank aggregation problem under a BTL model. Note that the
distribution of the comparison results only depends on the differences θ −θ for 0 ≤ i,j ≤ p.
i j
Thus, we fix θ = 0 to ensure the identifiability of θ = (θ ,...,θ )T.
0 1 p
When a = (i,j), we set z = e −e , where {e ,··· ,e } is the standard basis of Rp and
a j i 1 p
e = 0 .
0 p
For a = (i,j), the Fisher information and the weighted Fisher information are given by
eθi−θj (cid:88) eθi−θj
I (θ) = z zT,and I(θ;a ) = π (a) z zT. (40)
a (1+eθi−θj)2 a a n n (1+eθi−θj)2 a a
a=(i,j)∈A
If we take the criterion function G (·) = Φ (·), GI1 can be simplified as
θ q
ez aTθ(cid:98)nML (cid:16) (cid:17)−q−1
a = argmax zT I(θ(cid:98)ML;a ) z . (41)
n+1
a∈A
(1+ez aTθ(cid:98)nML)2 a n n a
We treat V = {0,1,··· ,p} as vertices and A as the set of edges. Then, G = (V,A) is an
23undirected graph. Assume that G is a connected graph. This condition ensures that θ is
identifiable when all the pairs in A are compared. Under this condition, it is possible to
select A = {a0,a0,··· ,a0 } ⊂ A so that (V,A ) is a connected subgraph of G.
0 1 2 n0 0
Algorithm 6 Simplified GI0/GI1 Algorithm for BTL model
We modify the following lines in Algorithms 1 and 2, while keeping the other lines of
the algorithms unchanged.
2: Require: The subgraph (V,{a0,··· ,a0 }) is a connected graph.
1 n0
6: The Fisher information matrices used in line 6 of Algorithms 1 for GI0 are calculated
using the formula (40). Selection in line 6 of Algorithms 2 for GI1 is replaced by (41).
Corollary 5.3. Assume that Assumption 1 holds, G is a connected graph, and the criterion
function G (·) = Φ (·). If the above Algorithm 6 for GI0 or GI1 is used, then the conclusions
θ q
for GI0 and GI1 from all the theorems presented in Section 4.2 hold.
6 Technical Challenges, New Analytical Tools and a
Proof Sketch for Theorem 4.3
In this section, we highlight the key technical challenges in proving Theorem 4.3 and in-
troduce new analytical tools to address these challenges. The primary challenge lies in
demonstrating that GI0/GI1 effectively balances the trade-off between exploration and ex-
ploitation, a well-known concept in the literature on sequential decision making involving
unknown parameters. Exploration means sufficient sampling of all relevant experiments
to ensure consistent parameter estimation. Exploitation means optimally sampling exper-
iments once the parameter has been accurately estimated. Below, we discuss these two
facets—exploration and exploitation—in the context of active sequential estimation.
6.1 Exploration
In order to have a consistent estimator, the selection rule needs to sample relevant experi-
ments sufficiently often. This is formalized by the following condition,
n := max minn → ∞ as n → ∞, (42)
I a
S⊂A:S isrelevant a∈S
where n = |{i;a = a,1 ≤ i ≤ n}| for a ∈ A. Here, we say that a set of experiments S
a i
(cid:80)
is relevant if I (θ) is nonsingular for any θ ∈ Θ. If S is relevant, then the model
a∈S a
24parameter is identifiable when all the experiments in S are sampled. Equation (42) says
that at least one of the relevant sets of experiments needs to be sampled infinitely often, in
order to have a consistent estimator.
Challenge 6.1. Show that n → ∞ as n → ∞ following GI0/GI1.
I
We note that in related sequential design problems, exploration is usually achieved by
incorporating an extra exploration step in the experiment selection rule. For example, in
active sequential hypothesis testing problems (see, e.g., Chernoff (1959); Naghshvar and Ja-
vidi (2013)), a two-stage algorithm is often utilized, where the first stage is designed for
exploration and the second stage is designed for exploitation. Another prevalent method for
ensuring sufficient exploration is the use of the epsilon-greedy algorithm in reinforcement
learning and multi-armed bandit (MAB) problems, where all available experiments are sam-
pled with a minimum probability of ε. For methods that incorporate an explicit exploration
component, verifying (42) is usually straightforward. However, for algorithms like GI0/GI1,
which are greedy and lack an additional exploration component, proving (or disproving)
Equation (42) is much more challenging.
Nevertheless, we tackle Challenge 6.1 and establish the following proposition concerning
sufficient exploration for GI0 and GI1.
Proposition 6.2. Under regularity conditions described in Section 4.1, both GI0 and GI1
satisfy that liminf nI > 0.
n→∞ n
Below, we discuss the heuristic ideas for justifying the above proposition, while clar-
ifying the rigorous proof is much more involved. Let A = argmax n be the set
max a∈A a
of experiments that are most frequently selected. A key observation is that the inverted
Fisher information, through its directional derivatives in experiment selection rules, acts as
a regularizer, which means that if n /n is large enough, then we can show that
max I
∂ F (π ) > ∂ F (π ) (43)
l am θ(cid:98)n n l a′ θ(cid:98)n n
for all am ∈ A and some a′ ∈/ A , where
max max
∂ F = (cid:10) πa −π , ∂ G (cid:2)(cid:8)(cid:88) π(a)I (θ(cid:98) )(cid:9)−1(cid:3)(cid:12) (cid:12) (cid:11)
la θ(cid:98)n n+1 n ∂π θ(cid:98)n a n π=πn
a∈A
denotes the directional derivative along the direction l = πa −π . This implies that no
a n+1 n
action from A will be selected and the ratio n /n is bounded from below, if we follow
max I
the experiment selection rule a ∈ argmin ∂ F (π ). According to Equation (7) and
n+1 a∈A la θ(cid:98)n n
25additionalasymptoticanalysis, thisexperimentselectionrulebasedondirectionalderivatives
is asymptotically equivalent to GI0 and GI1, and, consequently, Proposition 6.2 holds.
Note that (43) itself is challenging to prove, for which we first prove that the following
decomposition of the information holds: Σ(cid:98) = I(θ(cid:98) ;a ) = A+E, and A,E satisfy:
n n n
1. A = (cid:80) naI (θ(cid:98) ) and E = (cid:80) naI (θ(cid:98) ), for some U > 0.
a∈A,na≥U n a n a∈A,na<U n a n
n n
2. A is a singular and positive semidefinite matrix.
3. E is a positive semidefinite matrix and the maximum eigenvalue of E is much smaller
than the smallest non-zero eigenvalue of A.
4. There exists a′ ∈ A such that n
a′
≤ n
I
and I a′(θ(cid:98) n) ∈/ R(A), where R(A) denotes the
column space of A. This implies
(cid:104) (cid:105)
l ∥i Em ∥→in 0f tr ∇G θ(cid:98)n(cid:0) Σ(cid:98) n(cid:1) (A+E)−1I a′(θ(cid:98) n)(A+E)−1 = ∞.
5. For all am ∈ A max, I am(θ(cid:98) n) ∈ R(A). This implies
(cid:104) (cid:105)
limsup tr ∇G θ(cid:98)n(cid:0) Σ(cid:98) n(cid:1) (A+E)−1I am(θ(cid:98) n)(A+E)−1 < ∞.
∥E∥→0
We treat A as the dominating term and E as a small perturbation when using the above
matrix decomposition. With additional matrix perturbation analysis of Σ(cid:98)−1 = (A+E)−1
n
arounditsnon-continuouspointA, acarefuluseoftheDavis-KahansinΘtheorem(Yuetal.,
2015), and additional iterative analysis, we can show that tr(cid:2) ∇G θ(cid:98)n(Σ(cid:98) n)Σ(cid:98)− n1I a′(θ(cid:98) n)Σ(cid:98)− n1(cid:3) >
tr(cid:2) ∇G θ(cid:98)n(Σ(cid:98) n)Σ(cid:98)− n1I am(θ(cid:98) n)Σ(cid:98)− n1(cid:3) for all am ∈ A max. This, along with Equation (7) implies
(43).
6.2 Exploitation
In active sequential estimation, optimal exploitation requires frequency of the selected ex-
periments to approximate the optimal proportion π∗ = argmin G ({Iπ(θ∗)}−1), when
π∈SA θ∗
the estimator is accurate enough. In related sequential decision problems, such as active
sequential hypothesis testing, optimal exploitation is commonly attained through a ‘plug-in’
method. This method assumes the estimator is accurate and replaces θ∗ with the estimator
for calculating the proportion for the subsequent sampling. The ‘plug-in’ method’s the-
oretical analysis usually combines the consistency result with the optimization problem’s
continuity. However, this approach does not work for GI0/GI1 algorithms, which optimize
26one-step-ahead information gain over the discrete set A, rather than the probability simplex
SA. Consequently, it is challenging to determine whether GI0/GI1 approximately solve the
long-term optimization problem argminF (π) over the probability simplex. This issue is
θ∗
divided into two specific challenges:
Challenge 6.3 (Noiseless case). For GI0 selection (2) and GI1 selection (3) with θ(cid:98) = θ(cid:98) =
1 2
··· = θ∗, do we have the convergence lim F (π ) = F (π∗)?
n→∞ θ∗ n θ∗
Challenge 6.4 (Noisy case). How does the difference between θ(cid:98) and θ∗ affect the conver-
n
gence of the algorithms?
Challenge 6.3 is roughly addressed using the following arguments. First, we can show
that F (·) is convex. By Jensen’s inequality, we obtain that
θ∗
n−1 1 1
F (cid:0) π + π∗(cid:1) −F (π∗) ≤ (1− )(cid:8)F (π )−F (π∗)(cid:9) .
θ∗ n−1 θ∗ θ∗ n−1 θ∗
n n n
Notice that by Taylor expansion, for any π ∈ SA,
n−1 1 1 1
F (cid:0) π + π(cid:1) −F (π ) = (cid:10) ∇F (π ), π − π (cid:11) +O(1/n2).
θ∗ n−1 θ∗ n−1 θ∗ n−1 n−1
n n n n
The first term on the right-hand side of the above equation is linear in π over the sim-
plex SA. Thus, its minimum is achieved at a point mass at a′ for some a′ ∈ A, i.e.,
π = δ := (I(a = a′)) . It can be shown that the solution to the optimization
a′ a∈A
argmin (cid:10) ∇F (π ), 1δ − 1π (cid:11) coincides with the selection rule GI1 if we replace
a′∈A θ∗ n−1 n a′ n n−1
the MLE with θ∗ (see Equation (3)). Let a0 and a1 be the experiments selected by GI0
n n
and GI1 (with the MLE replaced by the true parameter), respectively. Combining the above
analysis with the definition of GI0, we obtain
1 1
F θ∗(πa n0 n)−F θ∗(π∗) ≤ F θ∗(πa n1 n)−F θ∗(π∗) = am ′∈i An(cid:10) ∇F θ∗(π n−1), nδ
a′
− nπ n−1(cid:11) +O(1/n2)
n−1 1 1
≤F ( π + π∗)−F (π∗)+O(1/n2) ≤ (1− )(F (π )−F (π∗))+O(1/n2).
θ∗ n−1 θ∗ θ∗ n−1 θ∗
n n n
The above display suggests that the distance F (π )−F (π∗) is reduced at the factor
θ∗ n−1 θ∗
1 − 1/n for GI0 and GI1 under the noiseless case. With additional iterative analysis, we
can further show that F (π )−F (π∗) ≤ O(logn/n). Consequently, the frequency of the
θ∗ n θ∗
selected experiment converges to the optimal proportion.
On the other hand, the above heuristic analysis does not justify the convergence of the
algorithm in the noisy case (Challenge 6.4), nor does it provide the convergence rate. We
address these challenges by establishing and using a modified Robbins-Siegmund theorem,
27which extends the classic result by Robbins and Siegmund (1971), to the stochastic process
Z = F (π )−F (π∗).
n θ∗ n θ∗
7 Simulation
In this section, we present two simulation studies. The first assesses the finite sample per-
formance of the proposed methods under the setting of Example 1. The second is concerned
with situations where p or |A| is large. Throughout the section, we choose the criterion
function G (Σ) = Φ (Σ) = tr(Σ) for GI0 and GI1. Due to the page limit, we leave some
θ 1
detailed specifications and additional simulation results in the supplementary material.
7.1 Simulation Study 1
We first evaluate the performance of the proposed methods under the settings of Example 1.
Specifically, let p = 2, A = {1,2,3}, and f (1) = 1/(1+e−(−0.1+θ1)), f (1) = 1/(1+e−θ2)
θ,1 θ,2
and f (1) = 1/(1+e−(θ1/2+θ2)). Also, let Θ = [−3,3]2 in this section.
θ,3
We start with illustrating the optimal proportion π∗. According to Theorem 4.3, the
optimal proportion for experiment selection is
π∗ = (π∗(1),π∗(2),π∗(3)) = arg min F (π) = arg min tr(cid:8) Iπ(θ∗)−1(cid:9) . (44)
θ∗
π∈SA π∈SA
Note that π∗ is dependent on the true model parameter θ∗. Figure 1 illustrates the de-
pendency between π∗ and θ while fixing θ∗ = 1. From the figure, we see that the optimal
2 1
proportion varies as θ changes. Additionally, for relatively small θ , all three experiments
2 2
have non-zero optimal proportions. However, for large θ , π∗(3) stays at zero, meaning that
2
experiment a = 3 is unnecessary in this case.
Next, we investigate the empirical proportion of selected experiments following the pro-
(cid:12)
posed methods. Recall that the empirical proportion is defined as π (a) = 1 (cid:12){i;a = a,1 ≤
n n i
(cid:12)
i ≤ n}(cid:12), for a ∈ {1,2,3}. We generate data from the model with the true parameter
θ∗ = (1,0)T and plot the sample path of π (a) against different sample size n following
n
GI0 and GI1 in Figure 3. We clarify that the values of the empirical proportion in the
figure are obtained without averaging. That is, they are based on a Monte Carlo simulation
with only one replication. From Figure 3, we can see that the empirical proportions are
approximating their respective optimal values as n increases, for both GI0 and GI1. This is
consistent with Theorem 4.3, which states that the empirical proportion almost surely con-
verges to the optimal proportion. We also observe that the selections made by GI0 and GI1
28are almost identical. This may be due to the fact that they are asymptotically equivalent
(see Equation (7)), and they are initialized with the same random seed.
Now, we evaluate the estimation accuracy of MLE following the proposed GI0 and GI1,
and compare it with other experiment selection methods. The estimation accuracy is quan-
tified using the estimated MSE, defined as M(cid:91) SE = 1 (cid:80)N ∥θ(cid:98)ML−θ∗∥2, where N = 20000
n N j=1 n,j
is the number of Monte Carlo replications and θ(cid:98)ML is the MLE from the j-th Monte Carlo
n,j
experiment with the sample size n. We compare GI0 and GI1 with the following experiment
selection rules:
1. Uniform selection (Unif): a is uniformly sampled from A.
n+1
2. Random optimal proportion selection (Opt random): a is sampled randomly from
n+1
A according to the optimal proportion π∗. That is, P(a = a|F ) = π∗(a) for a ∈ A.
n+1 n
3. Deterministic optimal proportion selection (Opt deterministic):
a = argmin {π (a)−π∗(a)}.
n+1 a∈A n
We clarify that both Opt random and Opt deterministic methods require knowledge of the
unknown parameter θ∗, so they are not implementable in practice. These methods serve
as ‘oracle’ benchmarks allowing comparison with the proposed methods. Figure 2 depicts
the estimated MSE as a function of the sample size n for different experiment selection
rules. According to the figure, GI0, GI1, and Opt deterministic perform very similarly and
outperform both Unif and Opt random. These findings are consistent with Theorem 4.8.
Finally, we check the finite sample validity of the normal approximation of the MLE.
According to Theorem 4.5 and Theorem 4.11, for large n and small c,
(cid:13) (cid:13)
1 (cid:13)(cid:110) (cid:111)−1/2 (cid:13)
dTθ(cid:98)ML ± √ Z (cid:13) Iπn(θ(cid:98)ML) d(cid:13) and dTθ(cid:98)ML ±Z ·c (45)
n n α/2 (cid:13) n (cid:13) τc α/2
give approximate 1−α confidence intervals (CIs) for dTθ where d ∈ R2 is nonzero and the
stopping time τ is defined in (9) with h(θ) = dTθ. Table 1 shows the coverage probability
c
of the above CIs at different sample sizes, following GI0 or GI1, where we set α = 0.05,
d = (−0.5454216,−0.8381619)T and θ∗ = (1,0)T, based on a Monte Carlo simulation. From
the table, we see that the coverage probability is close to the confidence level 1 − α for
reasonably large n and the random stopping time τ .
c
We have also performed additional simulation studies and produced histograms of the
estimators. These additional simulation results are given in the supplementary material.
29n 25 50 100 τ
0.1
GI0 0.981 0.954 0.951 0.955
GI1 0.977 0.958 0.959 0.938
Table 1: Coverage probability for CIs based on (45), where the number of Monte Carlo
replications is 1000. The Monte Carlo standard error for the values presented in the table is
upper bounded by 0.007626.
p *(1)
p *(2) GI0 p *(3) GI1
Unif
Opt_deterministic
Opt_random
−1.5 −0.5 0.0 0.5 1.0 1.5
q 10 20 30 40 50
2
Sample Size
Figure 1: Optimal proportion π∗ as a
Figure 2: MSE of the MLE as sample size
function of θ , where the true parameter
2 n varies.
satisfies θ∗ = (1,θ )T.
2
Empirical Proportion for GI0 Empirical Proportion for GI1
E E Em m mp p pe e er r ri i ic c ca a al l l p p p n n n( ( (1 2 3) ) ) p p p * * *( ( (1 2 3) ) ) E E Em m mp p pe e er r ri i ic c ca a al l l p p p n n n( ( (1 2 3) ) ) p p p * * *( ( (1 2 3) ) )
10 20 30 40 50 10 20 30 40 50
Sample Size Sample Size
Figure 3: Empirical proportion π (a) and the optimal proportion π(a) for a = 1,2,3.
n
7.2 Simulation Study 2
In our theoretical results, we assumed that |A| and p are fixed and n grows to infinity. In this
simulation study, we investigate the impact of large |A| and p on the computational time and
theperformanceoftheproposedmethods. Considerthesequentialrankaggregationproblem
described in Section 5.3. We simulate the pairwise comparison results from a BTL model
(see Equation (39)). Each coordinate of the true value of θ ∈ Rp are sampled independently
from a uniform distribution U(−2,2). We vary the value of p and |A|, with p and |A| ranging
from 25 to 500 and from p to p(p+1), respectively. The computation time is given by Table 2.
2
30
noitroporP
lamitpO
6.0
4.0
2.0
0.0
6.0
4.0
2.0
0.0
ESM
0.3
5.2
0.2
5.1
0.1
5.0
0.0
6.0
4.0
2.0
0.0p = 25 p = 50
k = 25 k = 52 k = 325 k = 50 k = 102 k = 1275
GI1 non-parallel 0.676 sec 0.430 sec 0.416 sec 0.963 sec 0.489 sec 1.046 secs
GI0 parallel 1.328 secs 1.070 secs 1.784 secs 1.639 secs 1.963 secs 10.296 secs
GI0 non-parallel 1.079 secs 1.325secs 4.790 secs 3.202 secs 5.030 secs 33.430 secs
p = 100 p = 500
k = 100 k = 202 k = 5050 k = 500 k = 1002 k = 125250
GI1 non-parallel 2.028 secs 1.358 secs 10.758 secs 1.387 mins 57.900 secs 2.03 hours
GI0 parallel 6.736 secs 9.594 secs 3.240 mins 34.322 mins 1.168 hours 6 days
GI0 non-parallel 19.45 secs 35.065 secs 12.992 mins 1.925 hours 3.843 hours about 20 days
Table 2: The computation time for solving the MLE and selecting a new experiment at a
single time point, based on 100 Monte Carlo replications, is recorded for the non-paralleled
GI1 algorithm as well as for the non-paralleled and paralleled versions of the GI0 algorithm.
For each value of p, k = |A| takes values in p, 2(p + 1), and p(p+1). All computations are
2
carried out on a MacBook Pro (13-inch, 2019) equipped with a 1.4 GHz Quad-Core Intel
Core i5 processor.
Based on Table 2, the non-paralleled GI1 is much faster than both the non-paralleled and
paralleled GI0 when both p and |A| are large. This is consistent with Lemma 3.1.
We also perform additional Monte Carlo simulations to assess the estimation accuracy of
the proposed methods, and to study how the choice of r in Θ = [−r,r]p affects the accuracy.
Due to the page limit, details of these additional simulation studies are postponed to the
supplementary material.
8 Real Data Example
We apply the proposed methods to a sushi preference dataset (Maystre and Grossglauser,
2017). This dataset contains feedback from 5,000 participants who ranked 10 different types
of sushi, selected from a total of 100 types of sushi. Similar to the data pre-processing steps
in Maystre and Grossglauser (2017), we first transform each 10-item ranking into pairwise
comparison results, yielding
(cid:0)10(cid:1)
× 5000 = 225000 pairwise comparison results. We fit the
2
BTL model described in Equation (39) with Θ = [−3,3]p using all pairwise comparison data
and treat the MLE of θ as the ground truth. Under this setting, p = 99, and |A| = 4809.
We note that |A| <
(cid:0)100(cid:1)
due to the absence of comparisons for some pairs in the dataset.
2
We vary the sample size n and compare the performance of the proposed GI0 and GI1
with two other experiment selection methods: uniform sampling and uncertainty sampling.
Uncertainty sampling is a popular approach for active learning. In the context of sequential
rank aggregation (see Maystre and Grossglauser (2017)), uncertainty sampling refers to
31sampling the pair that is most difficult to distinguish. That is,
(cid:2) (cid:3)
a = argmax min{1−f (1),f (0)} = arg min {|θ(cid:98) −θ(cid:98) |}. (46)
n+1
a∈A
θ(cid:98)n,a θ(cid:98)n,a
a=(i,j)∈A
n,i n,j
The performance of the experiment selection rules is measured through the Kendall’s τ
correlation, which is often used to measure the accuracy of ranking algorithms. Specifically,
define Kendall’s τ correlation as
(cid:18) 100(cid:19)−1
(cid:88)
τ(θ(cid:98) ,θ∗) = sign(θ(cid:98) −θ(cid:98) )·sign(θ∗ −θ∗),
n 2 n,i n,j i j
1≤i<j≤100
where sign denotes the sign function, θ(cid:98) denotes the MLE based on n observations, and θ∗
n
is the ground truth obtained using the MLE based on all 225000 comparisons.
Figure 4 illustrates the Kendall’s τ coefficient for GI0, GI1, uniform selection and uncer-
tainty sampling for different number of comparisons n, based on a Monte Carlo simulation
with 100 replications. From Figure 4, GI0 and GI1 behave similarly, and both outperform
uniform selection and uncertainty sampling. Additional details of the Monte Carlo simula-
tion are provided in the supplementary material.
Uncertainty Sampling
GI1
GI0
Unif
0 1000 2000 3000 4000
Number of comparisons
Figure 4: Comparison of different selection methods through Kendall’s τ coefficient. The
averaged Kendall’s τ correlation between θ∗ and θ(cid:98)ML versus the number of comparisons
n
is plotted, along with the first and third quartiles, following different active experiment
selection methods.
32
noitalerroc
knar
lladneK
8.0
7.0
6.0
5.0
4.0
3.0
2.09 Conclusion and Further Discussion
In this study, we consider the problem of efficient sequential design for active sequential
estimation. This problem has widespread applications across different fields; however, a
systematic statistical analysis is lacking for the multidimensional case. We introduce a
class of experiment selection rules that not only covers existing methods but also presents
new approaches with improved numerical efficiency. Furthermore, we provide theoretical
analysis including the consistency, asymptotic normality, and asymptotic optimality of the
MLE following the proposed selection rule. These findings are also extended to scenarios
involving early stopping rules, which are commonly used in practice. The theoretical results
arehighlynon-trivial, andstandardtechniquesintheliteratureofsequentialdecisionmaking
and stochastic control are not applicable. We have developed new analytical tools to tackle
the theoretical challenges, which are important on their own and may be reused for other
related problems.
The current study can be extended in several directions. First, in some applications,
different experiments are associated with varying sampling cost. The current method may
be extended to incorporate the sampling cost in the experiment selection rules. We expect
similaranalyticaltoolscanbeusedinthetheoreticalanalysis. Second, theoreticalresultscan
be extended to the case where p and k slowly grow to infinity as n grows. On the other hand,
the consistency results do not hold under the high-dimensional setting where p ≥ n. Some
modifications to the estimation and experiment selection methods are necessary to ensure
valid statistical inference in this case. Third, nuisance parameters may be present in some
applications, where we are only interested in estimating part of the parameter efficiently. In
this case, the proposed GI0 and GI1 still lead to a consistent and asymptotically normal
MLE. However, the estimator may be asymptotically inefficient when there are redundant
experiments measuring nuisance parameters. Of interest is how to design an experiment
selection rule and an estimator to achieve asymptotic optimality. This is worth further
investigation.
33Supplement to “Globally-Optimal Greedy Experiment
Selection for Active Sequential Estimation”
This supplement contains additional simulation results, specifications for simulation and real
data analysis, and technical proof for all the theoretical results.
10 Detailed Specifications for Simulation Studies
In this section, we provide detailed specifications for the simulation studies in Section 7.
Recall that G (Σ) = tr(Σ) throughout the simulation study.
θ
10.1 Detailed Specifications for Section 7.1
10.1.1 Algorithm for Solving the Optimal Optimal Selection Proportion
To solve the optimal selection proportion π∗ numerically, we apply the projected gradient
descent algorithm over the simplex SA (see, e.g., Chen and Ye (2011)). Let P denote the
SA
projection operator onto the simplex SA. Initializing π = (1, 1, 1), the iterative algorithm
0 3 3 3
is given by
π = P (π −η∇F (π )),
n+1 SA n θ∗ n
where the learning rate η is set to 0.001 and the maximum number of iterations is set to
10000.
10.1.2 GI0 and GI1 Implementation
When implementing GI0 and GI1, the first two lines of the algorithm requires the input θ(cid:98)
0
and a0,··· ,a0 . Here we specify θ(cid:98) = (0,0)T, n = 9, and a = a = a = 1, a = a = a =
1 n0 0 0 1 4 7 2 5 8
2, and a = a = a = 3.
3 6 9
Additionally, the MLE is solved using the R function glmnet function from the R package
glmnet with the constraint Θ = [−3,3]2.
10.1.3 Coverage Probability for CIs
The coverage probability of confidence intervals described (45) is estimated as follows:
N (cid:13) (cid:13)
1 (cid:88) I(cid:16)
|dTθ(cid:98)j −dTθ∗| ≤
Z √α/2(cid:13) (cid:13)(cid:110) Iπn(θ(cid:98)ML)(cid:111)−1/2 d(cid:13) (cid:13)(cid:17)
, for n ∈ {25,50,100},
N n n (cid:13) n (cid:13)
j=1
34and
N
1 (cid:88) (cid:16) (cid:17)
I |dTθ(cid:98)j −dTθ∗| ≤ Z ·c ,
N τc α/2
j=1
where N = 1000 is the number of Monte Carlo simulation, θ(cid:98)j and θ(cid:98)j are the MLE obtained
τc
in the j-th Monte Carlo replication with the sample size n and τ , respectively.
c
10.2 Detailed Specifications for Section 7.2
Let Θ = [−3,3]p. To solve for the MLE (constrained in Θ), we use the glmnet function
from the R package glmnet. The computation time shown in Table 2 is determined using
RStudio.
10.2.1 Sampling of A
For a sequential rank aggregation problem under a BTL model, the graph G =
({0,··· ,p},A) needs to be a connected graph for the identifiability of the model param-
eter (see Corollary 5.3). This implies that A needs to satisfy some condition rather than
being an arbitrary set of pairs to ensure the identifiability of the problem. Below we describe
the random sampling scheme of A used in the Monte Carlo simulation which ensures the
connectivity of G. Note that |A| ∈ {p,2(p+1), p(p+1)} in the simulation study.
2
• If|A| = p(p+1),Gisafullyconnectedgraph,meaningthatAcollectsallthepairsamong
2
the p+1 objects. In this case, A is fixed throughout the Monte Carlo simulation.
• If |A| = p, a connected G is equivalent to that G is a minimal spanning tree for a
fully connected graph. In this case, we sample G uniformly from all minimal span-
ning trees in the Monte Carlo simulation. This is implemented using the function
sample spanning tree from the R package igraph.
• If |A| = 2(p + 1), we restrict G to be 4-regular, which means that each node from
{0,1,··· ,p} has exactly 4 neighbors. In this case, we sample G uniformly from all
4-regular graphs. This is implemented using the function sample k regular from the
R package igraph.
10.2.2 Initial estimator θ(cid:98) and experiments a0,··· ,a0
0 1 n0
For implementing GI0 and GI1, we set θ(cid:98) = 0 in Algorithms 6.
0
According to Corollary 5.3 in Section 5.3, the initial experiments needs to be selected
so that G = ({0,··· ,p},{a0,··· ,a0 }) is a connected subgrpah of G. Here, elements of
0 1 n0
35{a0,··· ,a0 } are not necessary to be distinct. Throughout the Monte Carlo simulation, we
1 n0
set n = 4p, and sample G randomly using the following steps and collect the edges in G
0 0 0
to form the set of initial experiments {a0,··· ,a0 }.
1 n0
Step 1: Sample uniformly from all the minimal spanning trees from G, which is implemented
using the R function sample spanning tree. Let ({0,··· ,p},{atree,··· ,atree}) denote
1 p
the sampled tree.
Step 2: Randomly sample 3p pairs from A without replacement. Let {a′ ,··· ,a′ } denote
p+1 4p
all sets of pairs (possibly repeated) sampled from this step.
Step 3: {a0,··· ,a0 } = {atree,··· ,atree,a′ ,··· ,a′ } collects all the edges generated in the
1 n0 1 p p+1 4p
first and second steps.
Among the steps mentioned above, the first step yields a connected subgraph of G with p
edges, and the second step expands this subgraph into another connected subgraph to have
at most p+3p = 4p edges.
10.2.3 Algorithm Acceleration
The accelerated GI1 algorithm (as described in Algorithm 3) is employed for GI1 selection,
because in sequential rank aggregation problem the information matrix can be decomposed
intoastructurethatisbothsparseandoflowrankwiths = 2(seeLemma3.1). Toaccelerate
GI0 Algorithm 6, we parallel the calculation of (2) when |A| is large.
11 Detailed Specifications for the Real Data Analysis
in Section 8
11.1 Data Structure
The transformed dataset contains 225000 pairwise comparison results. We list these compar-
ison results as the dataset D = {(a(i),X(i))}T , where T = 225000, a(i) indicates the pairs
i=1
to compare and X(i) is binary, indicating the corresponding pairwise comparison result. We
note that D is a multiset, meaning that it may have repeated elements.
11.2 Sequential Sampling of the Pairwise Comparison Data
We note that, for the real data analysis, each element in the data set D is sampled at most
once, to prevent the redundancy of using the same data points multiple times. As a result,
36when we implementing an active sampling scheme for the real data analysis, we will always
sample elements from D without replacement.
Specifically, for all the experiment selection methods compared in this section, we set
n = p = 99, and generate the initial experiments {a0,··· ,a0 } randomly following the Step
0 1 n0
1 procedure in Section 10.2.2. For each j ∈ {1,2,··· ,n }, we sample the initial pairwise
0
comparison results as follows: we sample s uniformly from {i : a(i) = a0,1 ≤ i ≤ T}. Then,
j j
the initial pairs and comparison results are given by (a(s1),X(s1)),···(a(sn0),X(sn0)). This
gives the initial data (a0,X ),··· ,(a0 ,X ).
1 1 n0 n0
Let S = {s ,··· ,s }, [T] = {1,2,··· ,T}. For each S ⊂ [T], define
n0 1 n0
A = {a(i) ∈ A : i ∈ [T]\S}.
S
Next, we provide details of the implementation of different adaptive pair selection rules for
n > n .
0
Uniform sampling: For n = n ,··· ,T −1, sample s uniformly from [T]\S . Let S = S ∪{s }.
0 n+1 n n+1 n n+1
The (n+1)−th pair and comparison result (a ,X ) is given by (a(sn+1),X(sn+1)).
n+1 n+1
GI0 and GI1: For n = n ,··· ,T − 1, calculate a according to (2) and (3) with A replaced by
0 n+1
A for GI0 and GI1, respectively. Next, we uniformly sample the index s from
Sn n+1
{i ∈ [T]\S : a(i) = a }. Let S = S ∪ {s }. The (n + 1)−th pair and
n n+1 n+1 n n+1
comparison result (a ,X ) is given by (a(sn+1),X(sn+1)).
n+1 n+1
Uncertainty sampling: For n = n ,··· ,T − 1, calculate a according to (46) with A replaced by A .
0 n+1 Sn
Next, we uniformly sample the index s from {i ∈ [T]\S : a(i) = a }. Let
n+1 n n+1
S = S ∪{s }. The (n+1)−th pair and comparison result (a ,X ) is given
n+1 n n+1 n+1 n+1
by (a(sn+1),X(sn+1)).
11.3 Specifications for the Monte Carlo Experiments
For Figure 4, we perform a Monte Carlo simulation with 100 replications. For each repli-
cation, we sample 99 pairs of comparisons at random for initialization, and then perform
sequential sampling for following different methods using the initialization and sampling
method described in Section 11.2. We specify G (Σ) = tr(Σ) for implementing GI0 and GI1
θ
and Θ = [−3,3]p to solve the MLE.
12 Additional Simulation Results
In this section, we present additional simulation results.
3712.1 Additional Simulation Results for Simulation Study 1
Let the true value θ∗ = (1,0)T. The initial estimator θ(cid:98) , n , and experiments {a0,···a0 }
0 0 1 n0
are selected according to Section 10.1.2. Let the sample size n = 50. Define
√ √
N(θ(cid:98)j −θ∗) N(θ(cid:98)j −θ∗)
Zj = 1 1 and Zj = 2 2 ,
1 (cid:0) eT 1{Iπn(θ(cid:98)nj)}−1e 1(cid:1)1/2 2 (cid:0) eT 2{Iπn(θ(cid:98)nj)}−1e 2(cid:1)1/2
where θ(cid:98)j = (θ(cid:98)j,θ(cid:98)j)T represents the MLE of θ based on j−th Monte Carlo replication, and
n 1 2
N = 1000 is the number of Monte Carlo replications. That is, Zj and Zj are i.i.d. copies of
√ √ 1 2
Z = N(θ(cid:98)1−θ 1∗) and Z = N(θ(cid:98)2−θ 2∗) . In Figure 5, we plot the histogram
1 (cid:0) (cid:1)1/2 2 (cid:0) (cid:1)1/2
eT 1{Iπn(θ(cid:98)n)}−1e1 eT 2{Iπn(θ(cid:98)n)}−1e2
for {Zj}N and {Zj}N following GI0 and GI1.
1 j=1 2 j=1
Histogram for GI0 Histogram for GI0
−2 0 2 4 −3 −1 0 1 2 3
Histogram for GI1 Histogram for GI1
−2 0 2 4 −2 0 2 4
Figure 5: Histograms for {Zj}N and {Zj}N following GI0 and GI1, and the density
1 j=1 2 j=1
curve for the standard normal distribution. The upper left and bottom left panels show the
histogram of {Zj}N following GI0 and GI1, respectively. The upper right and bottom right
1 j=1
panels show the histogram of {Zj}N following GI0 and GI1, respectively.
2 j=1
38
ytisneD
ytisneD
4.0
3.0
2.0
1.0
0.0
4.0
3.0
2.0
1.0
0.0
ytisneD
ytisneD
4.0
3.0
2.0
1.0
0.0
4.0
3.0
2.0
1.0
0.0In Figure 5, the histogram closely approximates the standard normal density curve. This
is consistent with Theorem 4.4.
12.2 Additional Simulation Results for Simulation Study 2
The theoretical results in the manuscript assume that p and |A| are fixed while the sample
size n grows large. In this section, we investigate the performance of the proposed method
when p and |A| are comparable with n, and this condition is violated. We investigate the
performance of the proposed methods under a sequential rank aggregation problem assuming
a BTL model.
We consider the following simulation settings. Set p = 10 or 50. Entries of θ∗ are i.i.d.
and generated from U(−2,2). |A| = 2(p+1), and A is sampled uniformly from all 4-regular
graphs (see Section 10.2.1). The initial estimator and experiments are selected in the same
way as those in Section 10.2.2 except that n is set as 2p instead of 4p.
0
12.2.1 Empirical and Optimal Frequency
We plot the expected value of F = F (π ) − F (π∗) for different methods in Figure 6
n θ∗ n θ∗
based on N = 1000 Monte Carlo replications. Here, the optimal proportion π∗ is computed
according to Section 10.1.1. From the Figure 6, we see that F is approaching zero when n is
n
large. This is consistent with Theorem 4.3. However, it is far from zero when p is comparable
withn(e.g., p = 50andn = 150). Thisisexpectedasitbecomesahigh-dimensionalproblem
under this setting.
GI0 GI0
GI1 GI1
Unif Unif
100 200 300 400 100 150 200 250 300 350 400
Sample Size Sample Size
Figure 6: Comparison of F for different selection methods (GI0, GI1, Unif selections) and
n
different sample size n. The left panel and the right panel show F with p = 10 and p = 50,
n
respectively.
39
F
n
005
003
001
0
F
n
00052
00001
012.2.2 Estimation Accuracy
GI0 GI0
GI1 GI1
Unif Unif
20 40 60 80 100 100 120 140 160 180 200
Sample Size Sample Size
Figure 7: Comparison of performance of MSE for different selection methods (GI0, GI1,
uniform selections) for the rank aggregation problem. The left panel and the right panel
show MSE with p = 10 and p = 50, respectively.
In Figure 7, we plot the MSE for the MLE at different sample size n following different
experiment selection methods based on N = 10000 Monte Carlo replications. The MSE is
not close to zero when p is relatively large compared to n, which is expected. However, GI0
and GI1 still perform much better when compared with Unif.
12.2.3 Impact of the Choice of Θ
In our theoretical results, we assume the true parameter θ∗ is an inner point of Θ. In this
section, we study the impact of the choice of Θ on the estimation accuracy. We consider the
following simulation setting: p = 50, each element of θ∗ is sampled i.i.d. from U(−2,2), A
is randomly sampled from all 4−regular graphs with N = 100 Monte Carlo simulations. As
a result, |A| = 102. We consider 4 choices of Θ when solving for the MLE: Θ = [−1,1]p,
Θ = [−2,2]p, Θ = [−3,3]p and Θ = [−5,5]p. The initial sample size is set to n = p.
0
In Figure 8, we compare the Kendall’s correlation of the MLE following GI1 for different
choices of Θ, and obtain the following findings.
1. For cube 2 (Θ = [−2,2]p), it coincides with the data generation distribution U(−2,2).
The Kendall’s τ correlation is the largest among all cubes and sample sizes.
2. For cube 1 (Θ = [−1,1]p), it does not satisfy the condition θ∗ ∈ Θ for the theoretical
results. For small sample size (n < 500), it performs similarly as cube 2. However, for
larger n, it’s performance becomes worse.
40
ESM
05
03
01
0
ESM
052
051
05
03. For cube 3 and cube 5 (Θ = [−3,3]p and Θ = [−5,5]p), they cover the true model
parameter, but are larger than the support of sampling distribution of θ∗. For small
sample size, the larger the cube, the poorer the performance is. However, as the sample
size increases, the performance becomes better than cube 1.
4. Overall, the choice of r in Θ = [−r,r]p does not seem affect the overall trend between
Kendall’s correlation and sample size.
cube1
cube2
cube3
cube5
0 500 1000 1500 2000
Sample Size
Figure 8: Comparison of Performance of Different Compact Cubes with GI1 Selection for the
Rank Aggregation Problem. The curves for Cube1, Cube2, Cube3, and Cube5 represent
the plot of Kendall’s τ correlation versus sample size over compact cubes Θ = [−1,1]p,
Θ = [−2,2]p, Θ = [−3,3]p, and Θ = [−5,5]p, respectively.
13 Preliminary Theoretical Results and Supporting
Lemmas
In this section, we present preliminary theoretical results and supporting lemmas which are
usefulfortherestofthetheoreticalanalysis. Moreover,theymaybeusefulforotherproblems
involving the analysis of stochastic processes, functions of matrices, and linear algebra for
spaces indexed by a parameter.
41
noitalerroC
s'lladneK
8.0
7.0
6.0
5.0
4.0
3.013.1 Useful Results for the Convergence of Stochastic Processes
ThenextlemmaextendstheclassicKolmogorov’sthree-seriestheoremwithrelaxedmoments
and independence conditions. It is useful for proving almost sure convergence results for
dependent stochastic processes.
Lemma 13.1 (ModifiedKolmogorov’sthree-seriestheorem). Consider nested σ−fields F ⊂
n
F ,n ≥ 0. Let {X }∞ and {ε }∞ be two sequences of random variables, adaptive to
n+1 n n=1 n n=1
{F }∞ , respectively. Consider a sequence of events E such that
n n=1 n
(cid:32) (cid:33)
∞ ∞
(cid:91) (cid:92)
P(liminfE ) = P E = 1.
n m
n
n=1m=n
If there exists 0 < γ ≤ 1 such that,
∞
(cid:88)
E[|X |γI | F ] ≤ ε a.s., and, Eε < ∞,
n En n−1 n−1 n
n=0
then
(cid:80)∞
X converges almost surely.
n=1 n
Proof of Lemma 13.1. Let S =
(cid:80)N
X . It is sufficient to show that with probability 1,
N n=1 n
lim sup |S −S | = 0.
n l
m→∞n,l≥m
Applying C inequality (see 9.1.a in Lin (2010)), for any 0 < γ ≤ 1, k ≥ 1, we have
r
(cid:12) (cid:12)γ
(cid:12)(cid:88)k (cid:12) (cid:88)k
(cid:12) X (cid:12) ≤ |X |γ.
(cid:12) m+i(cid:12) m+i
(cid:12) (cid:12)
i=1 i=1
For any m ∈ N, and ε > 0, applying C inequality (see 9.1.a in Lin (2010)) and Markov
r
42inequality, we have
(cid:18) (cid:19)
P sup |S −S | ≥ 2ε
n l
n,l≥m
(cid:32) (cid:12) (cid:12) (cid:33)
(cid:12)(cid:88)k (cid:12)
≤P 2sup(cid:12) X (cid:12) ≥ 2ε
(cid:12) m+i(cid:12)
k∈N(cid:12) (cid:12)
i=1
(cid:32) (cid:12) (cid:12)γ (cid:33)
(cid:12)(cid:88)k (cid:12)
=P sup(cid:12) X (cid:12) ≥ εγ
(cid:12) m+i(cid:12)
k∈N(cid:12) (cid:12)
i=1
(cid:32) (cid:33)
k
(cid:88)
≤P sup |X |γ ≥ εγ
m+i
k∈N
i=1
(cid:32)(cid:40) (cid:41) (cid:32) (cid:33)(cid:33) (cid:32) (cid:33)
k ∞ ∞
(cid:88) (cid:92) (cid:92) (cid:92)
≤P sup |X |γ ≥ εγ E +P E
m+i n n
k∈N
i=1 n=m+1 n=m+1
(cid:32) (cid:33) (cid:32) (cid:33)
k ∞ ∞
≤limsupP (cid:88) |X |γI(cid:0) (cid:92) E (cid:1) ≥ εγ +P (cid:92) E
m+i n n
k→∞
i=1 n=m+1 n=m+1
(cid:32) (cid:33) (cid:32) (cid:33)
k ∞
≤limsupP (cid:88) |X |γI(cid:0) E (cid:1) ≥ εγ +P (cid:92) E
m+i m+i n
k→∞
i=1 n=m+1
(cid:32) (cid:33)
k ∞
≤limsup 1 (cid:88) E[E(cid:8) |X |γI | F (cid:9) ]+P (cid:92) E
εγ m+i Em+i m+i−1 n
k→∞
i=1 n=m+1
(cid:32) (cid:33)
∞ ∞
1 (cid:88) (cid:92)
≤ Eε +P E ,
εγ m+i−1 n
i=1 n=m+1
where we used the assumption E[|X |γI | F ] ≤ ε for all n for obtaining the last
n En n−1 n−1
inequality. Notice that
(cid:32) (cid:33) (cid:32) (cid:33) (cid:32) (cid:33)
∞ ∞ ∞ ∞
(cid:92) (cid:92) (cid:91) (cid:92)
lim P E = 1− lim P E = 1−P E = 0.
n n n
m→∞ m→∞
n=m+1 n=m+1 m=1n=m+1
43Let m → ∞, we obtain that for all ε > 0,
(cid:32) (cid:33)
(cid:26) (cid:27)
(cid:92)
P sup |S −S | ≥ 2ε
n l
n,l≥m
m≥1
(cid:18)(cid:26) (cid:27)(cid:19)
= lim P sup |S −S | ≥ 2ε
n l
m→∞ n,l≥m
(cid:32) (cid:33)
∞ ∞
1 (cid:88) (cid:92)
≤ lim Eε + lim P E
εγ m→∞ m+i−1 m→∞ n
i=1 n=m+1
=0
This implies
P(cid:0)(cid:84) (cid:8)
sup |S −S | ≥
2ε(cid:9)(cid:1)
= 0 and completes the proof.
m≥1 n,l≥m n l
Next, we extends Theorem 2.19 in Hall and Heyde (1980) obtain a law of large number
result for martingale differences which allows for adaptive experiment selection.
Lemma 13.2 (ModifiedTheorem2.19inHallandHeyde(1980)). Let {X }∞ be a sequence
n n=1
of random variables and {F }∞ be an increasing sequence of σ−fields with X measurable
n n=1 n
with respect to F for all n. Let {a }∞ denote a sequence of discrete random variables,
n n n=1
where each variable takes values from the set {1,2,...,k}. Let X1,··· ,Xk be a sequence of
random variables such that max E|Xa| < ∞. If the conditional distribution function of
1≤a≤k
X |F ,a = a is the same as the distribution function Xa with probability 1, then
n n−1 n
n
n−1(cid:88) {X −E(X | F )} −a. →s. 0. (47)
i i i−1
i=1
Proof of Lemma 13.2. Let Y = X I , n ≥ 1.
n n {|Xn|≤n}
Note that E|Xa| < ∞ for any 1 ≤ a ≤ k, and for any x > 0,
k
(cid:88)
P(|X | > x) = E P(|X | > x|F ) = E P(|X | > x|F ,a = a)P(a = a|F )
n n n−1 n n−1 n n n−1
a=1
k k
(cid:88) (cid:88)
=E P(|Xa| > x)P(a = a|F ) ≤ P(|Xa| > x) < ∞.
n n−1
a=1 a=1
Similar to the proof of Theorem 2.19 in Hall and Heyde (1980), we obtain that
∞ ∞ (cid:90)
(cid:88) 1 (cid:88) 1
E[{Y −E(Y |F )}2] ≤ 2 xP(|X | > x)dx
n2 n n n−1 n2 n
n=1 n=1 0<x≤n
k ∞ (cid:90) k ∞
(cid:88)(cid:88) 1 (cid:88)(cid:88)
≤2 xP(|Xa| > x)dx ≤ 4 P(|Xa| > i−1) < ∞,
n2
a=1 n=1 0<x≤n a=1 i=1
44n
n−1(cid:88) {Y −E(Y | F )} −a. →s. 0,
i i i−1
i=1
∞ ∞ k ∞
(cid:88) (cid:88) (cid:88)(cid:88)
P(X ̸= Y ) = P(|X | > n) ≤ P(|Xa| > n) < ∞
n n n
n=1 n=1 a=1 n=1
and
n
n−1(cid:88) {X −E(Y | F )} −a. →s. 0. (48)
i i i−1
i=1
Notice that with probability 1, as n → ∞,
E(|X |I(|X | > n)|F )
n n n−1
(cid:90) ∞
= P(|X | > x | F )dx
n n−1
n
(cid:90) ∞ k
(cid:88)
= P(|X | > x | F ,a = a)P(a = a|F )dx
n n−1 n n n−1
n a=1
(cid:90) ∞ k
(cid:88)
≤ P(|Xa| > x)dx
n a=1
k
(cid:88)
= E(|Xa|I(|Xa| > n))
a=1
→0.
Thus, with probability 1,
n
(cid:88)
n−1 |E(X −Y | F )|
i i i−1
i=1
n
(cid:88)
≤n−1 E(|X |I(|X | > i)|F )
i i i−1
i=1
(49)
n k
(cid:88)(cid:88)
≤n−1 E[{|X |I(|X | > i)|F ,a = a}P(a = a|F )]
i i i−1 i i i−1
i=1 a=1
k n
(cid:88) 1 (cid:88)
≤ E(|Xa|I(|Xa| > i)).
n
a=1 i=1
Because E|Xa| < ∞, we know that lim E(|Xa|I(|Xa| > n)) = 0. Because the arithmetic
n→∞
mean of a sequence converges to the same limit as the sequence itself, we obtain that for all
45a ∈ {1,··· ,k}
n
1 (cid:88)
lim E(|Xa|I(|Xa| > i)) = 0.
n→∞ n
i=1
In conclusion, we obtain, with probability 1, that
(cid:12) (cid:88)n (cid:12) (cid:88)k 1 (cid:88)n
(cid:12)n−1 E(X −Y | F )(cid:12) ≤ E(|Xa|I(|Xa| > i)),
(cid:12) i i i−1 (cid:12) n
i=1 a=1 i=1
and the expression on the right-hand side is a deterministic sequence converging to 0, which
implies that
n
n−1(cid:88) E(X −Y | F ) −a. →s. 0. (50)
i i i−1
i=1
Combining (48) and (50), we obtain (47).
Anscombe’s theorem (Anscombe, 1952) is a classic limit theorem for randomly indexed
processes. We prove a multivariate version of Anscombe’s theorem as follows, which gen-
eralizes the univariate Anscombe’s theorem with Gaussian limit (see Mukhopadhyay and
Chattopadhyay (2012)).
Theorem 13.3 (Multivariate Anscombe’s theorem). Let {T } be a sequence of column
n n≥1
random vectors and {W } be a sequence of positive definite matrices satisfying multivari-
n n≥1
ate Anscombe’s condition, namely, for every ε > 0, 0 < γ < 1 there exists some δ > 0 such
that
(cid:18) (cid:19)
limsupP max ∥T −T ∥ ≥ ελ (W ) < γ
n′ n min n
n→∞ |n′−n|≤δn
hold. Moreover, we assume that
λ (W )
max n
sup < ∞,
λ (W )
n≥1 min n
and there exists positive sequence ρ → ∞ such that
n
P
ρ W →∗ W, (51)
n n
where W is a real positive definite matrix.
Assume that there exists a real column vector θ ∈ Rp and as n → ∞
W−1(T −θ) →d N (0 ,I ).
n n p p p
Consider {N } , a sequence of positive integer-valued stopping times defined on the same
n n≥1
46probability space where {T } is defined. Let {r } be an increasing sequence of positive
n n≥1 n n≥1
integers such that lim r = ∞. If N /r → 1 in probability as n → ∞, then as n → ∞
n→∞ n n n
W−1(T −θ) →d N (0 ,I ).
rn Nn p p p
Proof of Theorem 13.3. For any b ∈ Rp such that ∥b∥ = 1, we have
bT(T −θ) (W b)T
n = n W−1(T −θ).
∥W b∥ ∥W b∥ n n
n n
Let h = Wnb = ρnWnb and h = Wb . By the continuous mapping theorem, we know that
n ∥Wnb∥ ∥ρnWnb∥ ∥Wb∥
h → h in probability.
n
Recall that W−1(T −θ) →d N (0 ,I ). Hence, by Slutsky theorem, as n → ∞
n n p p p
(W b)T (cid:16) (cid:17)T
n W−1(T −θ) = hTW−1(T −θ)+ h −h W−1(T −θ) →d N(0,1).
∥W b∥ n n n n n n n
n
In conclusion, we know that
bTT −bTθ
n d
→ N(0,1).
∥W b∥
n
Note that N /r → 1 in probability and
n n
(cid:16) (cid:17) (cid:16) (cid:17)
P max |bTT −bTθ| ≥ ε∥W b∥ ≤ P max ∥T −θ∥ ≥ ε·λ (W ) ,
n n n min n
|n′−n|≤δn |n′−n|≤δn
and
(cid:16) (cid:17)
limsupP max ∥T −θ∥ ≥ ε·λ (W ) ≤ γ.
n min n
n→∞ |n′−n|≤δn
Applying Theorem 3.1 in Mukhopadhyay and Chattopadhyay (2012), we obtain that for any
b ̸= 0 and as n → ∞
bTT −bTθ
Nn →d
N(0,1). (52)
∥W b∥
rn
Furthermore, by (51), we know that
ρ ∥W b∥ → ∥Wb∥. (53)
rn rn
Thus, we know that ρ bT(T −θ) = O (1) for any b ∈ Rp, which implies
rn Nn p
ρ (T −θ) = O (1). (54)
rn Nn p
47Note that
(cid:13) (cid:13)
(cid:13)W−1(T −θ)(cid:13)
rn Nn
∥T −θ∥
≤
Nn
λ (W )
min rn
p
(cid:88) |eT(T −θ)|
≤ i Nn supκ(W )
n
∥W e ∥
i=1
rn i n≥1
=O (1),
p
where
λ (W )
max n
κ(W ) = .
n
λ (W )
min n
By Cram´er–Wold theorem (see Billingsley (1999) p383), it is sufficient to show that for all
h ∈ Rp such that ∥h∥ = 1, we have
hTW−1(T −θ) →d N(0,1).
rn Nn
Set b = Wr− n1h , and b = W−1h . We have h = Wrnbn . By the continuous mapping
n ∥Wr− n1h∥ ∥W−1h∥ ∥Wrnbn∥
theorem, we know that b → b in probability. Notice that
n
(cid:12) (cid:12)
(cid:12) (cid:12)∥W rnb n∥ −1(cid:12)
(cid:12) ≤ κ(W )∥b −b∥ → 0,
(cid:12) ∥W b∥ (cid:12)
rn n
rn
which implies ∥Wrnbn∥ → 1 in probability. Combine this with (52), we obtain that as n → ∞
∥Wrnb∥
hTW−1(T −θ)
rn Nn
bT(T −θ) ∥W b∥
= n Nn rn
∥W b∥ ∥W b ∥
rn rn n
bT(T −θ)
= n Nn (1+o (1))
p
∥W b∥
rn
(cid:110)bT(T −θ) (b −b)Tρ (T −θ)(cid:111)
=
Nn
+
n rn Nn
(1+o (1)).
p
∥W b∥ ρ ∥W b∥
rn rn rn
Combining (53), (54) and b → b in probability, we know that
n
(b −b)Tρ (T −θ)
n rn Nn
= o (1),
p
ρ ∥W b∥
rn rn
48which implies that
(cid:110)bT(T −θ) (cid:111)
hTW−1(T −θ) = Nn +o (1) (1+o (1)) →d N(0,1).
rn Nn ∥W b∥ p p
rn
Thus, we know that for any h ̸= 0, as n → ∞
hTW−1(T −θ) →d N(0,∥h∥2).
rn Nn
By Cram´er–Wold theorem (see Billingsley (1999) p383), we complete the proof of Theorem
13.3.
13.2 Results regarding Functions of Matrices
In this section, we provide results on derivatives of functions of matrices, and properties on
functions of a convex combination of matrices.
Lemma 13.4. Let I ,a ∈ A be a sequence of positive semidefinite matrix. Assume π (a) ≥
a 0
0,a ∈ A (not necessary that π ∈ SA) such that (cid:80) π (a′)I is a real positive definite
0 a′∈A 0 a′
matrix. Then, for all a ∈ A we have
∂((cid:80) a′∈Aπ(a′)I a′)−1(cid:12) (cid:12)
(cid:12) =
−(cid:16) (cid:88)
π 0(a′)I
a′(cid:17)−1
I
a(cid:16) (cid:88)
π 0(a′)I
a′(cid:17)−1
. (55)
∂π(a) (cid:12)
π=π0 a′∈A a′∈A
Proof of Lemma 13.4. By definition,
∂((cid:80) π(a′)I )−1(cid:12) ((cid:80) π (a′)I +εI )−1 −((cid:80) π (a′)I )−1
a′∈A a′ (cid:12) (cid:12) = lim a′∈A 0 a′ a a′∈A 0 a′ .
∂π(a) (cid:12) ε→0 ε
π=π0
Because (cid:80) π (a′)I is a positive definite matrix, then for small enough ε, the inverse of
a′∈A 0 a′
(cid:80) π (a′)I +εI exists. Furthermore,
a′∈A 0 a′ a
(cid:88) (cid:88) (cid:88) (cid:88)
( π (a′)I +εI )−1 −( π (a′)I )−1 = −ε( π (a′)I +εI )−1I ( π (a′)I )−1,
0 a′ a 0 a′ 0 a′ a a 0 a′
a′∈A a′∈A a′∈A a′∈A
and
(cid:88) (cid:88)
lim( π (a′)I +εI )−1 = ( π (a′)I )−1,
0 a′ a 0 a′
ε→0
a′∈A a′∈A
which implies (55).
Next, we will define and derive the Gateaux derivative of the criteria function Φ . For a
q
49real positive definite matrix Σ, recall that
Φ (Σ) = log|Σ|, Φ (Σ) = trΣq,0 < q < 1,
0 q
and
Φ (Σ) = (tr(Σq))1/q,q ≥ 1.
q
The Gateaux derivative ∇ Φ (Σ) of Φ at Σ in direction H, which is a symmetric matrix,
H q q
is defined as
(cid:12)
Φ q(Σ+εH)−Φ q(Σ) d (cid:12)
∇ Φ (Σ) = lim = Φ (Σ+εH)(cid:12) . (56)
H q q
ε→0 ε dε (cid:12)
ε=0
If the limit specified in (56) exists for all symmetric matrices H, we says that Φ is Gateaux
q
differentiable at Σ.
The next lemma provides the Gateaux derivative of Φ . This lemma allows non-integer
q
values for q, and is thus more general than a similar result in Yang et al. (2013).
Lemma 13.5. Φ is Gateaux differentiable at any real positive definite matrix Σ for any
q
q ≥ 0. Moreover, we have

tr(HΣ−1), if q = 0,



(cid:0) (cid:1)
∇ Φ (Σ) = q ·tr Σq−1H , if 0 < q < 1, (57)
H q


 (tr Σq)1/q−1 ·tr(Σq−1H), if q ≥ 1,
and

 −tr(I−πI a), if q = 0,

∂Φ (I−π)  (cid:16) (cid:17)
q = −q ·tr (I−π)q+1I , if 0 < q < 1, (58)
a
∂π(a)
   −(cid:104) tr(cid:16) (cid:0) I−π(cid:1)q(cid:17)(cid:105)1/q−1 ·tr(cid:16) (I−π)q+1I (cid:17) , if q ≥ 1,
a
where Iπ = (cid:80) π(a)I and I−π = (cid:8)(cid:80) π(a)I (cid:9)−1 .
a∈A a a∈A a
Remark 13.6. Based on (57), and the Riesz representation theorem over the Hilbert space
of symmetric matrix, for any positive definite Σ, there exists unique symmetric matrix
∇Φ (Σ) = { ∂ Φ (Σ)} , such that for any symmetric matrix H of comparable size,
q ∂Σij q 1≤i,j≤n
∇ Φ (Σ) = ⟨∇Φ (Σ),H⟩.
H q q
Proof of Lemma 13.5. Let q = 0. By the definition of the Gateaux derivative, for any
50symmetric H and positive definite matrix Σ,
Φ (Σ+εH)−Φ (Σ)
0 0
∇ Φ (Σ) = lim
H 0
ε→0 ε
1 log(1+ελ )+log(1+ελ )+···+log(1+ελ )
=lim log|I +εHΣ−1| = lim 1 2 n
ε→0 ε ε→0 ε
=(λ +λ +···+λ ) = tr(HΣ−1).
1 2 n
where λ ,λ ,··· ,λ denote all eigenvalues of HΣ−1 counting multiplicity.
1 2 n
When q is a positive integer, by expanding (Σ+εH)q, we have
tr((Σ+εH)q)−tr(Σq) = ε·q ·tr(Σq−1H)+o(ε).
Note that when q is a positive integer
Φ (Σ+εH)−Φ (Σ)
q q
ε
(cid:20) (cid:21)
1 (cid:16)
(cid:0) (cid:1)
(cid:17)1/q
= (tr Σq)1/q 1+ tr(Σ+εH)q −tr(Σq) /tr(Σq) −1
ε
(59)
1 (cid:104) (cid:105)
= (tr(Σq))1/q−1 · tr((Σ+εH)q)−tr(Σq) +o(1)
ε·q
=(tr(Σq))1/q−1 ·tr(Σq−1H)+o(1).
Thus, when q is a positive integer, (57) holds.
Now, consider the case when q > 0 and q is not an integer. Because we can not expand
(Σ + εH)q and due to the lack of commutative between Σ and H, we need some more
complicated techniques. Assume Σ is of size n×n. Let λ (ε) ≥ λ (ε) ≥ ··· ≥ λ (ε) be all
1 2 n
the eigenvalues of Σ+εH. Denote the corresponding eigenvectors by u (ε),··· ,u (ε).
1 n
Let λ = λ (0), and u = u (0) for 1 ≤ i ≤ n. Set λ = −∞,λ = ∞.
i i i i 0 n+1
Let λ be an eigenvalue of Σ+εH, there exists 0 ≤ r < s ≤ n+1 such that
λ > λ = λ = ··· = λ > λ .
r−1 r s s+1
Let d = s−r+1, U = [u ,r ,··· ,u ] and U (ε) = [u (ε),u (ε),··· ,u (ε)].
λ r r+1 s λ r r+1 s
By Wely’s inequality, |λ (ε)−λ| ≤ |ε|∥H∥ .
i op
51Notice that when |ε| is small enough, we have
tr(UT(ε)(Σ+εH)qU (ε))−tr(UTΣqU )
λ λ λ λ
=(λq(ε)−λq)+···+(λq(ε)−λq)
r s
(cid:20)(cid:18) (cid:16) λ (ε)−λ(cid:17)q (cid:19) (cid:18) (cid:16) λ (ε)−λ(cid:17)q (cid:19)(cid:21) (60)
=λq 1+ r −1 +···+ 1+ s −1
λ λ
=qλq−1((λ (ε)−λ)+···+(λ (ε)−λ))+o(ε),
r s
and
((λ (ε)−λ)+···+(λ (ε)−λ))
r s
=tr(UT(ε)(Σ+εH)U (ε))−tr(UTΣU ) (61)
λ λ λ λ
=ε·tr(UT(ε)HU (ε))+tr(UT(ε)ΣU (ε))−tr(UTΣU ).
λ λ λ λ λ λ
To proceed, we first prove the following equation,
tr((Σ−λI)(U (ε)−U )(U (ε)−U )T) = tr(UT(ε)ΣU (ε))−tr(UTΣU ). (62)
λ λ λ λ λ λ λ λ
(62) is justified by the following matrix calculation,
(cid:0) (cid:1) (cid:2) (cid:3)
tr (Σ−λI)(U (ε)−U )(U (ε)−U )T − tr(UT(ε)ΣU (ε))−tr(UTΣU )
λ λ λ λ λ λ λ λ
(cid:0) (cid:1) (cid:2) (cid:3)
=tr (U (ε)−U )T(Σ−λI)(U (ε)−U ) − tr(UT(ε)(Σ−λI)U (ε))−tr(UT(Σ−λI)U )
λ λ λ λ λ λ λ λ
(cid:0) (cid:1) (cid:0) (cid:1)
=−2tr UT(ε)(Σ−λI)U −2tr UT(Σ−λI)U = 0, because (Σ−λI)U = 0.
λ λ λ λ λ
Note that Σ − λI = (I − U UT)(Σ − λI)(I − U UT), −∥Σ−λI∥ I ⪯ Σ − λI ⪯
λ λ λ λ op
∥Σ−λI∥ I, and (U (ε)−U )(U (ε)−U )T is positive semidefinite, we obtain
op λ λ λ λ
(cid:0) (cid:1)
|tr (Σ−λI)(U (ε)−U )(U (ε)−U )T |
λ λ λ λ
(cid:0) (cid:1)
≤∥Σ−λI∥ ·tr (I −U UT)(U (ε)−U )(U (ε)−U )T(I −U UT)
op λ λ λ λ λ λ λ λ
(cid:0) (cid:1)
=∥Σ−λI∥ ·tr (I −U UT)U (ε)UT(ε)
op λ λ λ λ
(63)
=∥Σ−λI∥ ·(d−(cid:13) (cid:13)UTU (ε)(cid:13) (cid:13)2 )
op λ λ F
=∥Σ−λI∥ ·(d−∥cosΘ(U ,U (ε))∥2)
op λ λ F
=∥Σ−λI∥ ·∥sinΘ(U ,U (ε))∥2 ,
op λ λ F
(cid:13) (cid:13)
where ∥cosΘ(U ,U (ε))∥ = (cid:13)UTU (ε)(cid:13) is due to the definition of principal angles be-
λ λ F λ λ F
tween column spaces of U and U (ε) (see Yu et al. (2015)).
λ λ
52By Davis-Kahan theorem (see Yu et al. (2015)), we obtain
2∥(Σ+εH)−Σ∥
F
∥sinΘ(U ,U (ε))∥ ≤ = O(ε). (64)
λ λ F
min(λ −λ ,λ −λ )
r−1 r s s+1
Combining (60),(61),(62),(63) and (64), we obtain
((λ (ε)−λ)+···+(λ (ε)−λ)) = ε·tr(UT(ε)HU (ε))+O(ε2), and
r s λ λ
tr(UT(ε)(Σ+εH)qU (ε))−tr(UTΣqU ) = ε·qλq−1tr(UT(ε)HU (ε))+o(ε).
λ λ λ λ λ λ
Let P = U (ε)UT(ε) and P = U UT, we know that
ε λ λ λ λ
∥P −P∥2 = tr(P −2P P +P) = 2(d−tr(P P)) = 2(d−(cid:13) (cid:13)UTU (ε)(cid:13) (cid:13)2 )
ε F ε ε ε λ λ F (65)
=2(d−∥cosΘ(U ,U (ε))∥2) = 2∥sinΘ(U ,U (ε))∥2 ,
λ λ F λ λ F
√
and ∥P −P∥ = 2∥sinΘ(U ,U (ε))∥ = O(ε). Due to ΣU = λU , we know that
ε F λ λ F λ λ
Σq−1U = λq−1U , which means that
λ λ
((λ (ε)−λ)+···+(λ (ε)−λ)) = ε·qλq−1tr(HU (ε)UT(ε))+o(ε)
r s λ λ
=ε·qλq−1tr(HU UT)+o(ε) = ε·qλq−1tr(UTHU )+o(ε) = ε·q ·tr(Σq−1HU UT)+o(ε).
λ λ λ λ λ λ
This leads to
(cid:88)
tr((Σ+εH)q)−tr(Σq) = tr(UT(ε)(Σ+εH)qU (ε))−tr(UTΣqU )
λ λ λ λ
λ (66)
(cid:88)
= ε·q ·tr(Σq−1HU UT)+o(ε) = ε·q ·tr(Σq−1H)+o(ε),
λ λ
λ
where the last equation holds due to (cid:80) U UT = I.
λ λ λ
Thus, when 0 < q < 1, we know that
(cid:0) (cid:1)
∇ Φ (Σ) = q ·tr Σq−1H .
H q
Combining the first two equations in (59) with the equation (66), if q ≥ 1, we know that
∇ Φ (Σ) = (tr(Σq))1/q−1 ·tr(Σq−1H).
H q
Applying (59) again, we complete the proof of (57) in Lemma 13.5.
53By applying the chain rule and combining (55) in Lemma 13.4 with (57), we obtain that
∂Φ (I−π) (cid:28) ∂I−π (cid:29)
q = ∇Φ (I−π), = (cid:10) ∇Φ (I−π),−I−πI I−π(cid:11) = ∇ Φ (I−π),
∂π(a)
q
∂π(a)
q a −I−πIaI−π q
which completes the proof of (58) in Lemma 13.5.
Lemma 13.7. Assume I ,a ∈ SA are positive semidefinite matrices. Consider a convex
a
matrix function, g, such that for any pair of positive semidefinite matrices, A and B, if
A−B is a positive semidefinite matrix (denoted as A ⪰ B), then g(A) ≥ g(B). For any
π ∈ SA, define
(cid:32) (cid:33)
(cid:110)(cid:88) (cid:111)−1
F(π) = g π(a)I . (67)
a
a∈A
Then, F(π) is a convex function over the set {π ∈
SA;(cid:80)
π(a)I is nonsingular}.
a∈A a
In particular, under Assumption 5, the function F (π) is a convex function over the set
θ
{π ∈
SA;(cid:80)
π(a)I (θ) is nonsingular}.
a∈A a
Proof of Lemma 13.7. Let C = {π ∈
SA;(cid:80)
π(a)I is nonsingular}. Assume that
0 a∈A a
π,π′ ∈ C . Then, A = (cid:80) π(a)I and B = (cid:80) π′(a)I are positive definite.
0 a∈A a a∈A a
For any 0 < t < 1, tA+(1−t)B is positive definite. Note that for any vector v, applying
Schur complement condition (see Theorem 1.12 (b) in Zhang (2006)), we obtain that
(cid:34) (cid:35) (cid:34) (cid:35)
vTA−1v vT vTB−1v vT
and
v A v B
are positive semi-definite matrices. Notice that the following matrix is positive semi-definite
(cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35)
vTA−1v vT vTB−1v vT tvTA−1v +(1−t)vTB−1v vT
t +(1−t) = ,
v A v B v tA+(1−t)B
by Theorem 1.12 (b) in Zhang (2006), we obtain that
tvTA−1v +(1−t)vTB−1v ≥ vT(tA+(1−t)B)−1v.
Since v is arbitrary, we have
tA−1 +(1−t)B−1 ⪰ (tA+(1−t)B)−1. (68)
54Now, we have
(cid:0) (cid:1) (cid:0) (cid:1)
tF(π)+(1−t)F(π′) = tg A−1 +(1−t)g B−1
≥g(tA−1 +(1−t)B−1) ≥
g(cid:0)(cid:8) tA+(1−t)B(cid:9)−1(cid:1)
= F(tπ +(1−t)π′).
This shows that F is convex.
We proceed to the proof of the ‘In particular’ part of the lemma. Note that under
Assumption 5, there are two cases: Case 1: G (Σ) = Φ (Σ) for q ≥ 0, and Case 2: G (·) is a
θ q θ
convex function satisfying G (A) ≥ G (B) whenever A ⪰ B. For Case 2, we can apply our
θ θ
previous analysis directly for g(·) = G (·), and obtain that F (π) is convex. Thus, we focus
θ θ
our analysis on Case 1 in the rest of the proof. By Courant-Fischer-Wely minimax principle
(see Corollary III.1.2 in Bhatia (1997)), the i−th largest eigenvalue satisfies λ (A) ≥ λ (B)
i i
for any 1 ≤ i ≤ n. Thus, Φ (A) ≥ Φ (B) for any q ≥ 0.
q q
If G (Σ) = Φ (Σ) = (cid:0) tr(Σq)(cid:1)1/q with q ≥ 1, then Φ (Σ) is the Schatten q−norm (see
θ q q
equation (IV.31) in Bhatia (1997)), which implies that G (Σ) is convex. More generally, if
θ
G (Σ) is convex in Σ, then by (67), we obtain that
θ
F (π) = G ({Iπ(θ)}−1)
θ θ
is convex in π over
(cid:88)
{π ∈ SA; π(a)I (θ) is nonsingular}.
a
a∈A
If G (Σ) = Φ (Σ) = logdetΣ, we know that
θ 0
(cid:88)
Φ ({Iπ(θ)}−1) = −logdet( π(a)I (θ)).
0 a
a∈A
We aim to show that −logdet(A) is convex over positive definite matrices.
Notice that for any p×p positive definite matrix A,
(cid:90)
1
e−1/2⟨Ax,x⟩dx = .
(2π)p/2det(A)1/2
Rp
By H¨older’s inequality, for any positive definite p×p matrices A and B, we have
(cid:90) (cid:90) (cid:90)
(cid:16) (cid:17)t(cid:16) (cid:17)1−t
e−1/2⟨(tA+(1−t)B)x,x⟩dx ≤ e−1/2⟨Ax,x⟩dx e−1/2⟨Bx,x⟩dx , (69)
Rp Rp Rp
55which implies that
−logdet(tA+(1−t)B) ≤ −tlogdet(A)−(1−t)logdet(B).
This shows that −logdet(A) is convex over positive definite matrices.
Thus, F (π) = −logdet((cid:80) π(a)I (θ)) is convex in π over
θ a∈A a
(cid:88)
{π ∈ SA; π(a)I (θ) is nonsingular}.
a
a∈A
If G (Σ) = Φ (Σ) = tr(cid:0) Σq(cid:1) with 0 < q < 1, we know that
θ q
(cid:16)(cid:88) (cid:17)−q
Φ ({Iπ(θ)}−1) = tr π(a)I (θ) .
q a
a∈A
(cid:0) (cid:1)
By Lo¨wner-Heinz Theorem (see Theorem 2.6 in Carlen (2010)), we know that tr A−q is
operator convex, which means that for all positive definite matrices A and B,
(cid:16) (cid:17)−q
tA+(1−t)B ⪯ tA−q +(1−t)B−q,
(cid:0) (cid:1)
whichimpliesthattr A−q isaconvexfunctionoverpositivedefinitematrices. Inconclusion,
we obtain that F (π) = Φ ({Iπ(θ)}−1) is convex in π over
θ q
(cid:88)
{π ∈ SA; π(a)I (θ) is nonsingular}.
a
a∈A
13.3 Decoupling Active Sequential Sampling
The next lemma provides a decoupling result which makes it easier to analyze the likelihood
for problems with adaptive experiment selection.
Lemma 13.8. Consider deterministic sequential selection functions h (·),
m
h (a ,Y ,a ,Y ,··· ,a ,Y ) ∈ A, for m ≥ 2,
m 1 1 2 2 m−1 m−1
and h ∈ A. Consider two random vectors generated from the following procedures.
1
1. (Decoupled Sampling) Independently generate {Xa} , where Xa ∼ f (·). Let
m m≥1,a∈A m θ∗,a
a = h ,a = h (a ,Xa1),··· ,a = h (a ,Xa1,a ,Xa2,··· ,a ,Xam),··· .
1 1 2 2 1 1 m+1 m+1 1 1 2 2 m m
562. (Iterative Sampling) Let a′ = h . Generate X ∼ f (·). For m ≥ 1, obtain a′
1 1 1 θ∗,a′ m+1
1
and X iteratively as
m+1
a′ = h (a′,X ,··· ,a′ ,X ),
m+1 m+1 1 1 m m
then generate X |F ∼ f , where the σ-algebra F =
m+1 m θ,a′ m
m+1
σ(a′,X ,a′,X ,··· ,a′ ,X ).
1 1 2 2 m m
Then, the random vectors (Xa1,··· ,Xam,a ,··· ,a ) and (X ,··· ,X ,a′,··· ,a′ ) have the
1 m 1 m 1 m 1 m
same distribution for all m ≥ 1.
Proof of Lemma 13.8. We prove the lemma by induction. When m = 1, we know that
a = h = a′, and X |a′ and Xa1|a have the same distribution. Thus, (Xa1,a ) and
1 1 1 1 1 1 1 1 1
(X ,a′) have the same distribution.
1 1
By induction, assume that when m = n, random vectors (Xa1,Xa2,··· ,Xan,a ,··· ,a )
1 2 n 1 n
and (X ,X ,··· ,X ,a′,··· ,a′ ) have the same distribution.
1 2 n 1 n
Let m = n+1. Define XA = {Xa} , and an = (a1,··· ,an) ∈ An. The density
n i 1≤i≤n,a∈A
of XA is given by
n
n
(cid:89)(cid:89)
f (XA) = f (Xa).
θ n θ,a i
i=1a∈A
Let a = (a ,··· ,a ), where a ,··· ,a are obtained from the decoupled sampling. Given
m 1 m 1 m
XA, the conditional probability mass function of a and a are
n n n+1
f (an|XA) = I(a1 = a ,··· ,an = a ) and f (an+1|XA) = I(a1 = a ,··· ,an+1 = a ),
θ n 1 n θ n 1 n+1
where we used the fact that the {a } is measurable with respect to σ(XA). As a
m 1≤m≤n+1 n
result, the joint density functions for (XA,a ) and (XA ,a ) are
n n n+1 n+1
n
(cid:89)(cid:89)
f (XA,an) = f (Xa)I(a1 = a ,··· ,an = a )
θ n θ,a i 1 n
i=1a∈A
and
(cid:89)
f (XA ,an+1) = f (XA,an) f (Xa )I(an+1 = a ).
θ n+1 θ n θ,a n+1 n+1
a∈A
Thus, given XA and a , the condition density for {Xa } ,a is
n n n+1 a∈A n+1
(cid:89)
f ({Xa } ,an+1|XA,an) = f (Xa )I(an+1 = a ).
θ n+1 a∈A n θ,a n+1 n+1
a∈A
Note that a = h (a ,Xa1,a ,Xa2,··· ,a ,Xan). So f ({Xa } ,an+1|XA,an) de-
n+1 n+1 1 1 2 2 n n θ n+1 a∈A n
57pends on XA,an only through a ,Xa1,··· ,a ,Xan.
n 1 1 n n
Define σ-algebra F′ = σ(a ,Xa1,··· ,a ,Xan). Because f ({Xa } ,an+1|XA,an) is
n 1 1 n n θ n+1 a∈A n
measurable in F′, we have
n
(cid:89)
f ({Xa } ,an+1|F′) = f ({Xa } ,an+1|XA,a ) = f (Xa )I(an+1 = a )
θ n+1 a∈A n θ n+1 a∈A n n θ,a n+1 n+1
a∈A
(70)
We have Xan+1|{a ,Xa1,··· ,a ,Xan} ∼ f (·), and a = h (a ,Xa1,··· ,a ,Xan).
n+1 1 1 n n θ,an+1 n+1 n 1 1 n n
Recall that X |{a′,X ,··· ,a′ ,X } ∼ f (·), a′ = h (a′,X ,··· ,a′ ,X ), as
n+1 1 1 n n θ,a′ n+1 n 1 1 n n
n+1
well as the induction assumption that random vectors (Xa1,Xa2,··· ,Xan,a ,··· ,a ) and
1 2 n 1 n
(X ,X ,··· ,X ,a′,··· ,a′ ) have the same distribution. Consequently, random vectors
1 2 n 1 n
(Xa1,Xa2,··· ,Xan+1,a ,··· ,a ) and (X ,X ,··· ,X ,a′,··· ,a′ ) also have the same
1 2 n+1 1 n+1 1 2 n+1 1 n+1
distribution. We complete the proof of Lemma 13.8 by induction.
13.4 Results on Linear Spaces Indexed by a Parameter
Linear spaces spanned by the Fisher information play a crucial role in the proof of the
theorems. Note that the Fisher information matrices are depending on the parameter θ. In
this section, we present useful linear algebra results where the linear spaces are indexed by
a parameter.
(cid:80)
RecallV (θ) = R(I (θ)), andI (θ)istheFisherinformationmatrixattheparam-
Q a∈Q a a
eter θ with the experiment a. Throughout the section, we only used the property that I (θ)
a
is a positive semidefinite matrix and is continuous in θ, for all a ∈ A, which is guaranteed
under the regularity assumptions in Section 4.1. The results in this section still hold even
when I (θ) is not the Fisher information matrix, as long as it is still positive semidefinite
a
and continuous in θ, for all a ∈ A. We do not require any further assumptions.
Lemma 13.9. For all Q ⊂ A, and x > 0,a ∈ Q, we have
a
(cid:16)(cid:88) (cid:17)
dim(V (θ)) = rank x I (θ) .
Q a a
a∈Q
Proof of Lemma 13.9. It suffices to show that V (θ)⊥ = ker((cid:80) x I (θ)). This equation
Q a∈Q a a
holds because u ∈ V (θ)⊥ if and only if ⟨I (θ)y ,u⟩ = 0 for all a ∈ Q and y ∈ Rp, if and
Q a a a
only if I (θ)u = 0 for all a ∈ Q, if and only if uTI (θ)u = 0 for all a ∈ Q, if and only if
a a
uT((cid:80)
x I (θ))u = 0, if and only if u ∈
ker((cid:80)
x I (θ)).
a∈Q a a a∈Q a a
Lemma 13.10. Assume Θ is a path connected and compact set. The following statements
are equivalent:
581. for all Q ⊂ A, dim(V (θ)) does not depend on θ,
Q
(cid:80)
2. for all Q ⊂ A, rank( I (θ)) does not depend on θ,
a∈Q a
3. there exists 0 < c < c < ∞, which does not depend on Q, such that
(cid:88)
c·P ⪯ I (θ) ⪯ c·P ,∀Q ⊂ A,
VQ(θ) a VQ(θ)
a∈Q
where P denotes the orthogonal projection matrix onto vector space V.
V
Proof of Lemma 13.10.
(cid:80)
1 ⇐⇒ 2 This equivalency holds because dim(V (θ)) = rank( I (θ)), according to
Q a∈Q a
Lemma 13.9.
(cid:80)
3 =⇒ 2 For Q ⊂ A, let r(θ) = rank( I (θ)). Also, let
a∈Q a
(cid:88)
r = suprank( I (θ)). (71)
a
θ∈Θ
a∈Q
By the definition of supremum, there exists θ ∈ Θ such that
0
(cid:88)
r−1/2 ≤ rank( I (θ )) ≤ r.
a 0
a∈Q
(cid:80)
Becausetherankofamatrixcanonlytakeintegervalues,weknowthatrank( I (θ )) =
a∈Q a 0
r. Let µ (A) ≥ µ (A) ≥ ··· ≥ µ (A) be the eigenvalues of a positive semidefinite
1 2 p
matrix A. Applying Courant–Fischer–Weyl min-max principle (see Chapter I of Hilbert
(cid:80)
and Courant (1953) or Corollary III.1.2 in Bhatia (1997)) to c·P ⪯ I (θ), and
VQ(θ) a∈Q a
r(θ) = dim(V (θ)) (see Lemma 13.9), we obtain
Q
(cid:88)
µ ( I (θ)) ≥ µ (c·P ) = c > 0,∀θ ∈ Θ. (72)
r(θ) a r(θ) VQ(θ)
a∈Q
(cid:80)
Applying Courant–Fischer–Weyl min-max principle to I (θ) ⪯ c·P , and r(θ) =
a∈Q a VQ(θ)
dim(V (θ)), we obtain
Q
(cid:88)
µ ( I (θ)) ≤ µ (c·P ) = 0,∀θ ∈ Θ.
r(θ)+1 a r(θ)+1 VQ(θ)
a∈Q
59We will prove r(θ) = r for all θ ∈ Θ by contradiction. Assume, in contrast, that there
exists r = r(θ ) < r(θ ) = r. Then, there exists a continuous path h : [0,1] → Θ such that
1 1 0
h(0) = θ , and h(1) = θ . Set
0 1
(cid:88)
u(t) = µ ( I (h(t))).
r a
a∈Q
(cid:80) (cid:80)
Note that u(0) = µ ( I (θ )) ≥ c and u(1) = µ ( I (θ )) = 0. Because u(t) is a
r a∈Q a 0 r a∈Q a 1
continuous function in t ∈ [0,1], by the intermediate value theorem, there exists t′ ∈ (0,1)
such that u(t′) = c/2.
Let θ = h(t′). Because µ ((cid:80) I (θ )) = c/2 > 0, we know that rank((cid:80) I (θ )) ≥
2 r a∈Q a 2 a∈Q a 2
(cid:80)
r. By definition (71), we know that rank( I (θ )) ≤ r. Thus, r(θ ) = r. However,
a∈Q a 2 2
(cid:80)
µ ( I (θ )) = c/2 contradicts inequality (72). This completes the proof that r(θ) =
r(θ2) a∈Q a 2
r for all θ ∈ Θ.
2 =⇒ 3 For Q ⊂ A, define
(cid:16)(cid:88) (cid:17) (cid:16)(cid:88) (cid:17)
c (Q) = minΛ I (θ) , and c (Q) = maxΛ I (θ) ,
min min a max max a
θ∈Θ θ∈Θ
a∈Q a∈Q
where Λ and Λ represent the smallest and largest non-zero eigenvalue of a positive
min max
semidefinite matrix, respectively.
(cid:80) (cid:80)
Because rank( I (θ)) does not depend on θ, let r = rank( I (θ)). Let
a∈Q a a∈Q a
(cid:80) (cid:80)
λ ( I (θ)) denote the s−th largest eigenvalue of I (θ), s = 1,2,··· ,p. Note
(s) a∈Q a a∈Q a
(cid:80) (cid:80) (cid:80) (cid:80)
that Λ ( I (θ)) = λ ( I (θ)) and Λ ( I (θ)) = λ ( I (θ)).
min a∈Q a (r) a∈Q a max a∈Q a (1) a∈Q a
(cid:80)
Now, we know that Λ and Λ are continuous functions provided rank( I (θ))
min max a∈Q a
does not depend on θ, and I (θ) is continuous over compact set Θ. Thus, 0 < c (Q) ≤
a min
c (Q) < ∞.
max
Recall that V (θ)⊥ = ker((cid:80) I (θ)) from Lemma 13.9. Because for the positive
Q a∈Q a
semidefinite matrix (cid:80) I (θ), ker((cid:80) I (θ))⊥ = R((cid:80) I (θ)), we obtain V (θ) =
a∈Q a a∈Q a a∈Q a Q
(cid:80)
R( I (θ)).
a∈Q a
(cid:80)
Applying eigendecomposition of I (θ), we have
a∈Q a
(cid:88)
c (Q)·P ⪯ I (θ) ⪯ c (Q)·P .
min VQ(θ) a max VQ(θ)
a∈Q
Set c = min c (Q) and c = max c (Q). Since A is a finite set, we know that
Q⊂A min Q⊂A max
c > 0 and c < ∞.
6013.5 Other Supporting Lemmas
Lemma 13.11. Assume that positive semidefinite matrices A,B and C have the same size.
If A ⪰ C, then
tr(AB) ≥ tr(CB). (73)
Proof of Lemma 13.11. Because B1/2(A−C)B1/2 is positive semi-definite,
tr(AB)−tr(CB) = tr(B1/2(A−C)B1/2) ≥ 0. (74)
This completes the proof.
Lemma 13.12 (Multivariate Cauchy-Schwartz Inequality). For any random variable z and
random vector y, if cov(y) = Σ is positive definite matrix, then
y
var(z) ≥ cov(z,y)cov(y)−1cov(y,z). (75)
Proof of Lemma 13.12. Because
(cid:0) (cid:1)
0 ≤var z −cov(z,y)Σ−1y
y
(cid:16) (cid:17) (cid:16) (cid:17)
=var(z)−2cov cov(z,y)Σ−1y,z +var cov(z,y)Σ−1y
y y
=var(z)−2cov(z,y)Σ−1cov(y,z)+cov(z,y)Σ−1Σ Σ−1cov(y,z)
y y y y
=var(z)−cov(z,y){cov(y)}−1cov(y,z),
we complete the proof of Lemma 13.12.
Lemma 13.13. Assumptions 6A and 7A imply Assumptions 6B and 7B.
Proof of Lemma 13.13. Under Assumption 6A, we have
I (θ) = ZTI (Z θ)Z .
a a ξa,a a a
Let Z† be the Moore-Penrose inverse of Z . Because Z has full row rank, we know that
a a a
Z Z† = I , and
a a pa
I (Z θ) = {Z†}TI (θ)Z†,
ξa,a a a a a
which implies that I (Z θ) is continuous in θ. Thus, there exists 0 < c < c < ∞ such
ξa,a a 1 2
that
c I ⪯ I (Z θ) ⪯ c I ,
1 pa ξa,a a 2 pa
61and for any Q ⊂ A,
(cid:88) (cid:88)
V (θ) = R(I (θ)) = R(ZT).
Q a a
a∈Q a∈Q
By Lemma 13.10, we know that
(cid:16)(cid:88) (cid:17)
dim(V (θ)) = dim R(ZT)
Q a
a∈Q
does not depend on θ.
By Lemma 13.10, we obtain inequality (16). This proves that Assumption 6A implies
Assumption 6B.
Next, we consider Assumption 7B. Under Assumptions 6A and 7A, we obtain
D (f ∥f )
KL θ∗,a θ,a
=D (h ∥h )
KL ξ a∗,a ξa,a
≥C∥ξ∗ −ξ ∥2
a a
=C(θ −θ∗)TZTZ (θ −θ∗)
a a
C
≥ (θ −θ∗)TZTI (Z θ∗)Z (θ −θ∗)
c a ξa,a a a
2
C
= (θ −θ∗)TI (θ∗)(θ −θ∗).
a
c
2
Thus, for any π ∈ SA and θ ∈ Θ, we have
(cid:88) C (cid:88)
π(a)D (f ∥f ) ≥ π(a)(θ −θ∗)TI (θ∗)(θ −θ∗).
KL θ∗,a θ,a a
c
2
a∈A a∈A
Replacing the constant C by C, we obtain inequality (17) in Assumption 7B. Thus, we
c2
obtain Assumptions 6B-7B.
14 Proof of Theoretical Results
14.1 Proof of Lemma 3.1
Proof of Lemma 3.1. Thestandardcomputationalcomplexityforbothmatrixmultiplication
and matrix inversion of a matrix of size p × p is O(p3). Consequently, the computational
(cid:104) (cid:105)
complexity of evaluating G (cid:8) I(θ(cid:98)ML;a ,a)(cid:9)−1 for each a ∈ A is O(p3). Therefore, the
θ(cid:98)nML n n
computational complexity for the GI0 selection is O(kp3).
62The GI1 selection rule (3) can be reformulated as
a
n+1
(cid:104) (cid:105)
=argmaxtr ∇G (cid:0) {I(θ(cid:98)ML;a )}−1(cid:1) {I(θ(cid:98)ML;a )}−1I (θ(cid:98)ML){I(θ(cid:98)ML;a )}−1
a∈A
θ(cid:98)nML n n n n a n n n
(cid:104) (cid:105)
=argmaxtr LT(θ(cid:98)ML){I(θ(cid:98)ML;a )}−1∇G (cid:0) {I(θ(cid:98)ML;a )}−1(cid:1) {I(θ(cid:98)ML;a )}−1L (θ(cid:98)ML) .
a∈A
a n n n θ(cid:98)nML n n n n a n
Thus, Algorithm 3 produce the same outcome as Algorithm 2.
Note that the matrix
M = {I(θ(cid:98)ML;a )}−1∇G (cid:0) {I(θ(cid:98)ML;a )}−1(cid:1) {I(θ(cid:98)ML;a )}−1
n n θ(cid:98)nML n n n n
only needs to be computed once. Matrix multiplication involving matrices of sizes p×p and
p × s is of order O(p2s), given s ≤ s. Under the assumption that L (θ) has size p × s ,
a a a a
the computational complexity of the GI1 is bounded by O(p3 +ksp2).
Furthermore, if the matrices {L (θ)} are primarily supported on an s×s submatrix, then
a
the computational cost of the multiplication ΣL (θ(cid:98)ML) is at most equivalent to multiplying
a n
matrices of sizes p × s and s × s for any matrix Σ, which has a complexity of O(s2p).
Therefore, the overall computational complexity of the GI1 selection is O(p3 +ks2p).
14.2 Proof of Proposition 6.2
WeprovethefollowingTheorem14.1instead,whichisageneralizedversionofProposition6.2
allowing for an arbitrary sequence of θ that is not necessarily the MLE.
n
Theorem 14.1. Under the regularity conditions described in Section 4.1, and also assume
that the initial experiments a ,··· ,a ∈ A are such that I(θ;a ) is nonsingular. For any
1 n0 n0
sequence of (random or non-random) vectors θ ,θ ,··· in Θ, if we consider the following
1 2
generalized GI0 or GI1 selection rules: for any n ≥ n
0
(cid:104) (cid:105)
GI0 : a = argminG (cid:8) I(θ ;a ,a)(cid:9)−1 , and (76)
n+1 θn n n
a∈A
(cid:104) (cid:105)
GI1 : a = argmaxtr ∇G (cid:0) Σ (cid:1) Σ I (θ )Σ , where we define Σ = {I(θ ;a )}−1,
n+1 θn n n a n n n n n
a∈A
(77)
then there exists C > 0 such that
n
I
inf ≥ C,
n≥n0 n
63and the lower bound is independent of the choice of θ . Moreover, under the same settings,
n
Iπn(θ) ⪰ c·C ·I . (78)
p
for all θ ∈ Θ.
Proof of Proposition 6.2. Applying Theorem 14.1 with θ = θ(cid:98)ML, we complete the proof of
n n
Proposition 6.2.
TheproofofTheorem14.1isinvolved. Webreakitdowntothefollowingseriesoflemmas
and steps.
Step 1: Define the order statistics of experiments counts, permutations, and find
their connections with n and n Let mn = |{i;a = a,1 ≤ i ≤ n}| be the number of
max I a i
times that the experiment a has been selected up to time n. Without loss of generality, let
A = [k] = {1,2,··· ,k}. Let P be the set of all permutations over [k].
k
For each mn = (mn) , define Pmn ⊂ P , which is described by the following state-
a a∈A k k
ments: permutation τ ∈ Pmn if and only if τ ∈ P and
k k
mn ≥ mn ≥ ··· ≥ mn .
τ(1) τ(2) τ(k)
The set Pmn is not empty, because order statistic exists.
k
For any permutation τ ∈ Pmn, define the set Q (τ) = {τ(1),τ(2),··· ,τ(s)} for s ∈ [k].
k s
Q (τ) collects the indices of the top-s most frequently selected experiments. Here, τ is
s
introduced to handle the case where there may be ties among mn for a ∈ A.
a
Define
t = t (mn) = min {s ∈ [k];dim(V (θ)) = p}. (79)
n n
τ∈Pmn
Qs(τ)
k
Note that dim(V (θ)) = rank((cid:80) I (θ)) = p for all τ ∈ Pmn, according to
Q k(τ) a∈A a k
Lemma 13.9 and Assumption 3. Also note that according to Lemma 13.10 and Assump-
tion 6B, dim(V (θ)) does not depend on θ. Thus, the above t is well defined and does
Qs(τ) n
not depend on θ. For the same reason, we will drop ‘θ’ and write dim(V ) for dim(V (θ))
A A
for A ⊂ A in the rest of the proof when the context is clear.
The next lemma specifies the permutations that we would like to focus on when there
may be ties among mn.
a
Lemma 14.2. There exists τ ∈ P such that
n k
mn ≥ mn ≥ ··· ≥ mn , (80)
τn(1) τn(2) τn(k)
64dim(V ) = p, and for all τ′ ∈ Pmn and all s ≤ t −1, dim(V ) < p.
Qtn(τn) k n Qs(τ′)
Proof of Lemma 14.2. First, according to the definition of t in (79) and Pmn ̸= ∅, we know
n k
that
S′ = arg min {s ∈ [k];dim(V (θ)) = p}
τ∈Pmn
Qs(τ)
k
is not empty. Let τ ∈ S′. We know that τ satisfies (80), and dim(V ) = p.
n n Qtn(τn)
Assume there exist τ′ ∈ Pmn and s ≤ t −1, such that dim(V ) = p. This leads to
k n Qs(τ′)
the following contradiction
t = min {s ∈ [k];dim(V (θ)) = p} ≤ s ≤ t −1.
n
τ∈Pmn
Qs(τ) n
k
This completes the proof of Lemma 14.2.
Recall that n = max n = max mn is defined in Section 6. We obtain that
max a∈A a a∈A a
n = mn . The following Lemma shows that n = mn .
max τn(1) I τn(tn)
Lemma 14.3. Let τ be a permutation satisfying the properties described in Lemma 14.2.
n
Then, n = mn , where n is defined in (42).
I τn(tn) I
Proof of Lemma 14.3. Because dim(V (θ)) = p, we know that Q = Q (τ ) is relevant,
Qtn(τn) tn n
(cid:80)
which means that I (θ) is non-singular for all θ ∈ Θ. By the definition of n in (42),
a∈Q a I
n ≥ minmn = mn .
I
a∈Q
a τn(tn)
It suffices to prove n ≤ mn . Assume, on the contrary, that n > mn . In the rest
I τn(tn) I τn(tn)
of the proof, we aim to find a contradiction.
For any S ⊂ A such that S is relevant, define Q(S) = {a ∈ A;mn ≥ min mn}. Since
a a∈S a
S ⊂ Q(S), Q(S) is also relevant, and
minmn = min mn.
a a
a∈S a∈Q(S)
By the definition of n in (42), there exists a relevant S′ ⊂ A such that min mn = n .
I a∈S′ a I
Thus, n = min mn = min mn, Q(S′) is relevant and min mn > mn .This
I a∈S′ a a∈Q(S′) a a∈Q(S′) a τn(tn)
implies that
min mn ≥ mn .
a∈Q(S′)
a τn(tn−1)
Thus, Q(S′) ⊂ Q (τ ), which implies that dim(V ) ≤ dim(Q (τ )) < p. By As-
tn−1 n Q(S′) tn−1 n
sumption 6B and Lemma 13.10, we know that Q(S′) is not relevant, which contradicts the
previous assumption that Q(S′) is relevant.
65Thenextlemmacomparestheratiobetweenn = mn andn = mn withtheratio
max τn(1) I τn(tn)
between the maximum and minimum counts of experiments for a set of relevant experiments.
Lemma 14.4. For any Q ⊂ A such that dim(V ) = p,
Q
mn max mn
τn(1) ≤ a∈A a.
mn min mn
τn(tn) a∈Q a
Proof of Lemma 14.4. By the definition of τ , we know that
n
mn ≥ ··· ≥ mn .
τn(1) τn(k)
Define mn = ∞ and mn = −∞.
τn(0) τn(k+1)
Because (mn ,··· ,mn ) forms the order statistic of (mn) (with possibly ties),
τn(1) τn(k) a a∈[k]
there exists s ∈ [k] such that Q ⊂ {τ (1), ··· ,τ (s)} = Q (τ ), and
n n s n
mn = minmn and mn < minmn.
τn(s)
a∈Q
a τn(s+1)
a∈Q
a
Because V (θ) ⊂ V (θ), we have p = dim(V ) ≤ dim(V ). By the definition of t in
Q Qs(τn) Q Qs(τn) n
(79), we obtain that t ≤ s. This implies mn ≥ mn = min mn. We complete the
n τn(tn) τn(s) a∈Q a
proof by noting that mn = max mn.
τn(1) a∈A a
Step 2: Unify the proof for generalized GI0 and GI1 To simplify the analysis, we
use the next lemma to extract a key property shared by generalized GI0 and GI1.
Lemma 14.5. Assume that
(cid:80)n0
I (θ) is non-singular.
i=1 ai
For a fixed (or random) sequence θ ∈ Θ and for any n ≥ n , we consider the following
n 0
generalized GI0 selection rule
(cid:32) (cid:33)
n 1
(cid:26)
1 1
(cid:27)−1
a = argminF ( π + δ ) = argminG A+ I (θ ) ,
n+1
a∈A
θn
n+1
n
n+1
a
a∈A
θn
n+1 n+1
a n
and GI1 selection rule
∂F (π )
a = argmin θn n = argmax(cid:10) ∇G ({A/n}−1),A−1I (θ )A−1(cid:11) , (81)
n+1
a∈A ∂π(a) a∈A
θn a n
where A = (cid:80) mnI (θ ),δ = (δ (a′)) , and δ (a′) = I(a = a′).
a∈A a a n a a a′∈A a
(cid:80)
Let A (t ,t ) = I (θ ) + t I (θ ) + t I (θ ), and S = S (t ,t ) =
2 1 2 a∈A a n 1 a′ n 2 a′′ n n n 1 2
66∇G ({A (t ,t )/(n + 1)}−1). Then, both generalized GI0 and GI1 satisfy the following
θn 2 1 2
property for all n ≥ n :
0
If a′,a′′ ∈ A are such that
(cid:10) (cid:11) (cid:10) (cid:11)
S ,{A (t ,t )}−1I (θ ){A (t ,t )}−1 > S ,{A (t ,t )}−1I (θ ){A (t ,t )}−1 ,
n 2 1 2 a′ n 2 1 2 n 2 1 2 a′′ n 2 1 2
(82)
for all t ,t ∈ [0,1] , then a ̸= a′′.
1 2 n+1
Remark 14.6. The generalized GI0 and GI1 defined in (76) and (77) are the same as GI0
and GI1 selections described in Lemma 14.5, respectively.
Proof of Lemma 14.5. Let a′,a′′ satisfy (82). Assume, in the contrast, that a = a′′. We
n+1
will find contradictions for both GI0 and GI1 in the rest of the proof.
We start with GI0, which selects a = argmin F ( n π + 1 δ ). Thus, a′′ =
n+1 a∈A θn n+1 n n+1 a
a satisfies
n+1
n 1 n 1
F ( π + δ ) ≤ F ( π + δ ). (83)
θn
n+1
n
n+1
a′′ θn
n+1
n
n+1
a′
Define h(t) = F ( n π + 1 {(1−t)δ +tδ }). Then, (83) is equivalent to that h(1)−
θn n+1 n n+1 a′ a′′
h(0) ≤ 0.
(cid:80)
Let A (t) = I (θ )+(1−t)I (θ )+tI (θ ). By Lemma 13.4, we know that
1 a∈A a n a′ n a′′ n
h′(t) = (cid:10) ∇G ({A (t)/(n+1)}−1),−{A (t)/(n+1)}−1{I (θ )−I (θ )}{A (t)/(n+1)}−1(cid:11) .
θn 1 1 a′′ n a′ n 1
Note that A (t) = A (1−t,t). Thus, (82) holds for all t ,t ∈ [0,1] implies that it holds for
1 2 1 2
(t ,t ) = (1−t,t), which further implies h′(t) > 0 for any t ∈ (0,1). This contradicts with
1 2
h(1)−h(0) ≤ 0. Thus, a ̸= a′′.
n+1
We proceed to the analysis of GI1. By the definition of the generalized GI1, a′′ = a
n+1
satisfies
(cid:10) ∇G ({A/n}−1),A−1I (θ )A−1(cid:11) ≤ (cid:10) ∇G ({A/n}−1),A−1I (θ )A−1(cid:11) ,
θn a′ n θn a′′ n
Note that A = A (0,0). Thus, the above inequality contradicts with (82) with (t ,t ) =
2 1 2
(0,0).
Step 3: Regularization effect of GI0 and GI1 In this step, we show that both
GI0 and GI1 regularize the experiment selection process through the property established
in Lemma 14.5. This is proved through the following Lemma 14.7, Lemma 14.8, and
Lemma 14.10.
67Lemma 14.7. Assume that
(cid:80)n0
I (θ) is non-singular for some n .
i=1 ai (cid:16) (cid:16) (cid:17)(cid:17) 0
Assume the condition number max κ ∇G Σ ≤ K for some 0 < K < ∞. Let
θ∈Θ,Σ⪰0 θ
(a(1),a(2),··· ,a(k)) be a permutation of A such that mn ≥ ··· ≥ mn and dim(Q ) <
a(1) a(k) tn−1
dim(Q ) = p, where Q = {a(1),a(2),··· ,a(s)}, and t = t (mn).
tn s n n
If for some n ≥ n and 1 ≤ s ≤ t −1
0 n
(cid:18) mn (cid:19)2 8c3pK(cid:16) c2(cid:16)mn (cid:17)2(cid:17)
a(s) > 1+16p a(s+1) , (84)
mn c3 c2 mn
a(s+1) a(tn)
then
a ∈ G(Q ) = {a ∈ A : dim(V ) > dim(V )}
n+1 s Qs∪{a} Qs
for GI0 and GI1, where
(cid:88)
V = V (θ ) = R(I (θ )).
Q Q n a n
a∈Q
Proof of Lemma 14.7. Let t = t . (84) implies that t ≥ 2. By Lemma 14.2, for any s ≤ t−1,
n
we have dim(V ) = p. Because dim(V ) < p, we know that |G(Q )| > 0.
Qt Qs s
For any Q ⊂ A, let P be the orthogonal projection matrix on V (θ ). We will simplify
VQ Q n
the notation and write it as P for the ease of exposition when the context is clear. Then
Q
P denote the orthogonal projection matrix on V .
Qs Qs
According to Lemma 14.5, it is sufficient to show that, if (84) holds, then for all a′′ ̸∈
G(Q ), a′ ∈ G(Q ) and all t ,t ∈ [0,1].
s s 1 2
(cid:10) (cid:11) (cid:10) (cid:11)
S ,{A (t ,t )}−1I (θ ){A (t ,t )}−1 > S ,{A (t ,t )}−1I (θ ){A (t ,t )}−1 , (85)
n 2 1 2 a′ n 2 1 2 n 2 1 2 a′′ n 2 1 2
where we recall that S = ∇G ({A (t ,t )/(n+1)}−1). In the rest of the proof, we abuse
n θn 2 1 2
the notation a little and write A = A (t ,t ) for the ease of exposition. Then, it is sufficient
2 1 2
to show that for all a′′ ̸∈ G(Q ), a′ ∈ G(Q ) and all t ,t ∈ [0,1]
s s 1 2
(cid:10) (cid:11) (cid:10) (cid:11)
S ,A−1I (θ )A−1 > S ,A−1I (θ )A−1 . (86)
n a′ n n a′′ n
The rest proof of the consists of the following three steps:
Step A: Connect (86) with tr(P A−2) and tr(P A−2) Let
Qs∪{a′} Qs

mn +t if a = a′
  a′ 1

mn = mn +t if a = a′′ . (87)
a a′ 2


 mn otherwise
a
68Then, for 0 ≤ t ,t ≤ 1, mn ≤ mn +1 for all a ∈ A, and A = (cid:80) mnI (θ ).
1 2 a a a∈A a a n
Note that λ (S )I ⪰ S ⪰ λ (S )I . we have
max n p n min n p
(cid:10) (cid:11)
S ,A−1I (θ )A−1
n a′ n
=tr(S A−1I (θ )A−1)
n a′ n
(cid:88) (cid:88) (88)
=tr(S A−1 I (θ )A−1)−tr(S A−1 I (θ )A−1)
n a n n a n
a∈Qs∪{a′} a∈Qs
≥c·λ (S )tr(P A−2)−c·λ (S )tr(A−1P A−1),
min n Qs∪{a′} max n Qs
where the last inequality is due to Assumption 6B and Lemma 13.11.
Notice that a′′ ̸∈ G(Q ) implies
s
dim(V ) = dim(V ).
Qs∪{a′′} Qs
Combined with V ⊂ V , we know that V = V . This implies
Qs Qs∪{a′′} Qs Qs∪{a′′}
R(I (θ )) ⊂ V . (89)
a′′ n Qs
By Assumption 6B, we obtain
I (θ ) ⪯ c·P ⪯ c·P .
a′′ n R(I a′′(θn)) Qs
Hence,
(cid:10) (cid:11)
S ,A−1I (θ )A−1 = tr(S A−1I (θ )A−1) ≤ c·λ (S )tr(A−1P A−1). (90)
n a′′ n n a′′ n max n Qs
Thus, to show (86), it is sufficient to show that (84) implies
c·λ (S )tr(A−1P A−1) > 2c·λ (S )tr(A−1P A−1), (91)
min n Qs∪{a′} max n Qs
which is equivalent to
tr(P A−2) 2c·λ (S ) 2c
Qs∪{a′}
>
max n
= κ(S ). (92)
tr(P A−2) c·λ (S ) c n
Qs min n
We focus on proving the above inequality in the rest of the proof.
Step B: Establish a lower bound for tr(P A−2) Because V ⊂ V and
Qs∪{a′} Qs Qs∪{a′}
dim(V ) < dim(V ), we know that (I − P )P = P − P ̸= 0. Thus,
Qs Qs∪{a′} Qs Qs∪{a′} Qs∪{a′} Qs
69there exists a unit vector u ∈ Rp such that (cid:13) (cid:13)(I −P Q)P Qs∪{a′}u(cid:13) (cid:13) = 1.
Applying the Rayleigh–Ritz quotient for the largest eigenvalue, we know that
tr(P A−2) = tr(P A−2P )
Qs∪{a′} Qs∪{a′} Qs∪{a′}
(93)
≥λ max(P Qs∪{a′}A−2P Qs∪{a′}) ≥ uTP Qs∪{a′}A−2P Qs∪{a′}u = (cid:13) (cid:13)A−1P Qs∪{a′}u(cid:13) (cid:13)2 .
Set v = A−1P u. Then,
Qs∪{a′}
(cid:13) (cid:13)
∥(I −P Qs)A∥ op∥v∥ ≥ ∥(I −P Qs)Av∥ = (cid:13)(I −P Qs)P Qs∪{a′}u(cid:13) = 1. (94)
Notice that mn ≥ mn ≥ 1 for any s ≤ t − 1. Thus, mn + 1 ≤ 2mn . Also
a(s+1) a(tn) a(s+1) a(s+1)
note that by the definition of G(Q ), a ∈/ G(Q ) for all s′ ∈ [s]. Thus, for all a ∈ G(Q ),
s s′ s s
m ≤ m(s+1). The above analysis, together with Assumption 6B, implies
a a
(cid:88)
mnI (θ ) ⪯ c·(m(s+1) +1)·P I (θ ) ⪯ 2c·m(s+1) ·P . (95)
a a n a G(Qs) a n a G(Qs)
a∈G(Qs)
Note that A = (cid:80) mnI (θ )+(cid:80) mnI (θ ). Also note that if a ∈/ G(Q ), then
a∈G(Qs) a a n a∈/G(Qs) a a n s
I (θ ) ∈ V , which implies (I −P )I (θ ) = 0. Thus, (95) further implies
a n Qs Qs a n
∥(I −P )A∥
Qs op
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:88) (cid:13)
=(cid:13) mn(I −P )I (θ )(cid:13)
(cid:13) a Qs a n (cid:13)
(cid:13)a∈G(Qs) (cid:13)
op
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:88) (cid:13)
≤∥(I −P )∥ (cid:13) mnI (θ )(cid:13)
Qs op(cid:13) a a n (cid:13)
(cid:13)a∈G(Qs) (cid:13)
op
≤2c·m(s+1).
a
The above inequality and (94) implies ∥v∥ ≥ 1 . This, along with (93), implies
2c·mn
a(s+1)
1
tr(P A−2) ≥ ∥v∥2 ≥ . (96)
Qs∪{a′} 4c2 ·(mn )2
a(s+1)
Step C: Establish an upper bound for tr(P A−2)
Qs
tr(A−1P A−1) ≤ p·λ (A−1P A−1). (97)
Qs max Qs
(cid:80)
Set A = mnI (θ ). Let r = dim(V ). We first show that rank(A ) = r and
s a̸∈G(Qs) a a n Qs s
70R(A ) = V . By Lemma 13.9,
s Qs
rank(A ) = dim(V ). (98)
s A\G(Qs)∩{a∈A;mn a≥1}
Because Q ⊂ A\G(Q )∩{a ∈ A;mn ≥ 1}, we know that
s s a
dim(V ) ≤ dim(V ) ≤ dim(V ). (99)
Qs A\G(Qs)∩{a∈A;mn a≥1} A\G(Qs)
By (89) and V ⊂ V , we know that
Qs A\G(Qs)
(cid:88)
V = R(I (θ )) ⊂ V ⊂ V (100)
A\G(Qs) a n Qs A\G(Qs).
a̸∈G(Qs)
Hence, V = V , and dim(V ) = dim(V ). Combined with (98) and (99), we
A\G(Qs) Qs A\G(Qs) Qs
know that
rank(A ) = dim(V ) = dim(V ) = dim(V ) = r.
s Qs A\G(Qs)∩{a∈A;mn a≥1} A\G(Qs)
The above analysis and
(cid:88)
R(A ) ⊂ R(I (θ )) = V = V
s a n A\G(Qs) Qs
a̸∈G(Qs)
together imply that R(A ) = V .
s Qs
Assume the eigendecomposition A su
(cid:98)i
= λ(cid:98) iu (cid:98)i, and Au
i
= λ iu i, 1 ≤ i ≤ p, where
λ(cid:98)
1
≥ ··· ≥ λ(cid:98)
r
> λ(cid:98)
r+1
= ··· = λ(cid:98)
p
= 0, λ
1
≥ ··· ≥ λ
p
with U(cid:98)
s
= [u (cid:98)1,u (cid:98)2,··· ,u (cid:98)r], U(cid:98)
−s
=
[u (cid:98)r+1,u (cid:98)r+2,··· ,u (cid:98)p], U(cid:98) = [U(cid:98) s,U(cid:98) −s], U
s
= [u 1,u 2,··· ,u r], U
−s
= [u r+1,u r+2,··· ,u p],
and U = [U ,U ]. Based on the previous notation, we know that P is the orthogonal
s −s Qs
projection on R(A ), and thus, it equals U(cid:98) U(cid:98)T.
s s s
Let Θ(U(cid:98) ,U ) denote the r ×r diagonal matrix whose j−th diagonal entry is the j−th
s s
principal angle cos−1(σ ), where σ ≥ σ ≥ ··· ≥ σ are the singular values of U(cid:98)TU .
j 1 2 r s s
Applying a variant of the Davis–Kahan theorem (Theorem 2 in Yu et al. (2015)), we have
(cid:13) (cid:13) 2∥A−A ∥ 2∥A−A ∥
(cid:13)sinΘ(U(cid:98) ,U )(cid:13) ≤ s F = s F. (101)
(cid:13) s s (cid:13)
F λ(cid:98) −λ(cid:98) λ(cid:98)
r r+1 r
Note that
(cid:88)
A−A = mnI (θ ) ⪯ 2cmn P , (102)
s a a n a(s+1) G(Qs)
a∈G(Qs)
71and
∥A−A ∥2 ≤ 4p·c2(mn )2. (103)
s F a(s+1)
Note that Q ⊂ A\G(Q ). Because A ⪰ A ⪰ (cid:80) mnI (θ ) ⪰ cmn · P and
s s s a∈Qs a a n a(s) Qs
A ⪰ (cid:80) mnI (θ ) ⪰ cmn I , by Courant–Fischer–Weyl min-max principle (see Chapter
a∈Qt a a n a(t) p
I of Hilbert and Courant (1953) or Corollary III.1.2 in Bhatia (1997)), we have
λ ≥ λ(cid:98) ≥ cmn and λ ≥ cmn . (104)
r r a(s) p a(t)
Combining (101), (103), and (104), we obtain
(cid:13) (cid:13)2 16p·c2(mn )2
(cid:13)sinΘ(U(cid:98) ,U )(cid:13) ≤ a(s+1) . (105)
(cid:13) s s (cid:13) c2(mn )2
F
a(s)
By definition, we obtain
(cid:13) (cid:13)2 (cid:13) (cid:13)2
(cid:13)sinΘ(U(cid:98) ,U )(cid:13) = r−(cos2(σ )+cos2(σ )+···+cos2(σ )) = r−(cid:13)U(cid:98)TU (cid:13) ,
(cid:13) s s (cid:13) 1 2 r (cid:13) s s(cid:13)
F F
and
(cid:13) (cid:13)2 (cid:13) (cid:13)2 (cid:13) (cid:13)2
r = (cid:13)U(cid:98)T[U ,U ](cid:13) = (cid:13)U(cid:98)TU (cid:13) +(cid:13)U(cid:98)TU (cid:13) .
(cid:13) s s −s (cid:13) (cid:13) s s(cid:13) (cid:13) s −s(cid:13)
F F F
Thus,
(cid:13) (cid:13)2 (cid:13) (cid:13)2
(cid:13)sinΘ(U(cid:98) ,U )(cid:13) = (cid:13)U(cid:98)TU (cid:13) . (106)
(cid:13) s s (cid:13) (cid:13) s −s(cid:13)
F F
Combining (105) and (106), we have
λ (A−1P A−1)
max Qs
=λ (U(cid:98)TA−2U(cid:98) )
max s s
=λ (U(cid:98)T[U ,U ]diag(λ−2,λ−2,··· ,λ−2)[U ,U ]TU(cid:98) )
max s s −s 1 2 p s −s s
=λ (U(cid:98)TU diag(λ−2,··· ,λ−2)UTU(cid:98) +U(cid:98)TU diag(λ−2 ,··· ,λ−2)UT U(cid:98) )
max s s 1 r s s s −s r+1 p −s s
≤λ max(U(cid:98) sTU sdiag(λ− 12,··· ,λ− r2)U sTU(cid:98) s)+λ max(U(cid:98) sTU −sdiag(λ− r+2 1,··· ,λ− p2)U −T sU(cid:98) s)
(107)
(cid:13) (cid:13)2 (cid:13) (cid:13)2
≤λ−2(cid:13)U(cid:98)TU (cid:13) +λ−2(cid:13)UT U(cid:98) (cid:13)
r (cid:13) s s(cid:13) p (cid:13) −s s(cid:13)
op op
(cid:13) (cid:13)2
≤λ−2 +λ−2(cid:13)sinΘ(U(cid:98) ,U )(cid:13)
r p (cid:13) s s (cid:13)
F
1 (cid:18) 16p(cmn )2(cid:19)
≤ 1+ a(s+1) .
(cmn )2 (cmn )2
a(s) a(t)
72The above display and (97) implies
1 (cid:18) 16p(cmn )2(cid:19)
tr(A−1P A−1) ≤ p 1+ a(s+1) . (108)
Qs (cmn )2 (cmn )2
a(s) a(t)
Combining (96), (97) and (108), we have
tr(P A−2) (cid:110)4c2p (cid:18) 16p(cmn )2(cid:19) (cid:111)−1(cid:18) mn (cid:19)2
Qs∪{a′} ≥ 1+ a(s+1) a(s) (109)
tr(A−1P A−1) c2 (cmn )2 mn
Qs a(t) a(s+1)
Thus, (84) implies (92).
Lemma 14.8. Assume that
(cid:80)n0
I (θ) is nonsingular for some n .
i=1 ai 0
Consider the pre-specified criteria function G (Σ) = Φ (Σ). Let (a(1),a(2),··· ,a(k)) be
θ q
a permutation of A such that mn ≥ ··· ≥ mn and dim(Q ) < dim(Q ) = p, where
a(1) a(k) tn−1 tn
Q = {a(1),a(2),··· ,a(s)}, and t = t (mn).
s n n
For a fixed (or random) sequence θ ∈ Θ and for any n ≥ n , we consider the generalized
n 0
GI0 selection rule
(cid:32) (cid:33)
(cid:26)
1 1
(cid:27)−1
a = argminΦ A+ I (θ ) ,
n+1 q a n
a∈A n+1 n+1
and GI1 selection rule
(cid:16) (cid:17)
a = argmaxtr A−(q+1)I (θ ) , (110)
n+1 a n
a∈A
where A = (cid:80) mnI (θ ).The generalized GI1 selection based on (110) coincides with the
a∈A a a n
selection (81).
If for some 1 ≤ s ≤ t −1
n
(cid:18) mn
a(s)
(cid:19)q+1(cid:16)
1−
16pc2 (cid:0)mn a(s+1)(cid:1)2(cid:17)
>
(cid:16)2c(cid:17)q+2 p(cid:18)
1+
16pc2(cid:16)mn a(s+1)(cid:17)q+1(cid:16)mn a(s+1)(cid:17)1−q(cid:19)
,
mn c2 mn c c2 mn mn
a(s+1) a(s) a(tn) a(s)
(111)
then
a ∈ G(Q ) = {a ∈ A : dim(V ) > dim(V )}
n+1 s Qs∪{a} Qs
for generalized GI0 and GI1, where
(cid:88)
V = R(I (θ )).
Q a n
a∈Q
Proof of Lemma 14.8 . To prove the lemma, we follow similar steps as those in the proof of
73Lemma 14.7. We will omit the repetitive details and only state the main differences.
By assumption G (Σ) = Φ (Σ), Lemma 13.4 and 13.5, we know that for any a,a′ ∈ A
θ q
and positive definte matrix A ∈ Rp×p,
(cid:10) (cid:11) (cid:10) (cid:11)
∇Φ ({A/n}−1),A−1I (θ )A−1 > ∇Φ ({A/n}−1),A−1I (θ )A−1
q a n q a′ n
(112)
if and only if tr(A−(q+1)I (θ )) > tr(A−(q+1)I (θ )).
a n a′ n
Thus, the generalized GI1 selection based on (110) coincides with the selection (81).
Similar to the arguments for (86), to prove the lemma, it is sufficient to show that (111)
implies that for all a′′ ̸∈ G(Q ), a′ ∈ G(Q ) and all t ,t ∈ [0,1],
s s 1 2
(cid:10) (cid:11) (cid:10) (cid:11)
S ,A−1I (θ )A−1 > S ,A−1I (θ )A−1 , (113)
n a′ n n a′′ n
where A is redefined as A (t ,t ) and S = ∇G ({A (t ,t )/(n+1)}−1) = ∇Φ ({A/(n+
2 1 2 n θn 2 1 2 q
1)}−1). Applying (112), we know that (113) is equivalent to
(cid:0) (cid:1) (cid:0) (cid:1)
tr A−(q+1)I (θ ) > tr A−(q+1)I (θ ) . (114)
a′ n a′′ n
It is sufficient to show that (111) implies (114) for all a′′ ̸∈ G(Q ), a′ ∈ G(Q ) and all
s s
t ,t ∈ [0,1]. Similar to the proof of Lemma 14.7, this is proved using the following 3 Steps.
1 2
Step A: Connect (114) with tr(A−(q+1)P ) and tr(A−(q+1)P ) Similar to the
Qs∪{a′} Qs
derivation leading to (88), we have
tr(A−(q+1)I (θ )) ≥ c·tr(A−(q+1)P )−c·tr(A−(q+1)P ).
a′ n Qs∪{a′} Qs
Similar to (90), we have
tr(A−(q+1)I (θ )) ≤ c·tr(A−(q+1)P ). (115)
a′′ n Qs
Thus, to prove (114), it is sufficient to show (111) implies that
c·tr(A−(q+1)P ) > 2c·tr(A−(q+1)P ), (116)
Qs∪{a′} Qs
which is equivalent to
tr(A−(q+1)P ) 2c
Qs∪{a′}
> . (117)
tr(A−(q+1)P ) c
Qs
74Step B: Establish a lower bound for tr(A−(q+1)P ) Similar to the proof of
Qs∪{a′}
(cid:80)
Lemma 14.7, we define A = mnI (θ ), then r = dim(V ) = rank(A )
s a∈G(Qs) a a n Qs s
and R(A s) = V Qs. Assume the eigendecomposition A su
(cid:98)i
= λ(cid:98) iu (cid:98)i, and Au
i
= λ iu i,
1 ≤ i ≤ p, where λ(cid:98) ≥ ··· ≥ λ(cid:98) > λ(cid:98) = ··· = λ(cid:98) = 0, λ ≥ ··· ≥ λ with
1 r r+1 p 1 p
U(cid:98)
s
= [u (cid:98)1,u (cid:98)2,··· ,u (cid:98)r], U(cid:98)
−s
= [u (cid:98)r+1,u (cid:98)r+2,··· ,u (cid:98)p], U(cid:98) = [U(cid:98) s,U(cid:98) −s], U
s
= [u 1,u 2,··· ,u r],
U = [u ,u ,··· ,u ], and U = [U ,U ].
−s r+1 r+2 p s −s
Because a′ ∈/ V , there exists a unit vector u ∈ Rp such that P u = 0, and P u =
Qs Qs Qs∪{a′}
u. Then,
λ (A−(q+1)P ) = λ (P A−(q+1)P ) ≥ uTA−(q+1)u.
max Qs∪{a′} max Qs∪{a′} Qs∪{a′}
Assume u = (cid:80)p i=1b iu
i
= (cid:80)p i=1(cid:98)b iu (cid:98)i. Because P
Qs
= (cid:80)r i=1u (cid:98)T
i
u
(cid:98)i
and
p r
(cid:88) (cid:88)
0 = P Qsu = (cid:98)b iP Qsu
(cid:98)i
= (cid:98)b iu (cid:98)i,
i=1 i=1
(cid:13) (cid:13)
we obtain that (cid:98)b =(cid:98)b = ··· =(cid:98)b = 0. Thus, we can rewrite u as U(cid:98) β(cid:98) with (cid:13)β(cid:98) (cid:13) = 1.
1 2 r −s 1 (cid:13) 1(cid:13)
Note that
p p
uTA−(q+1)u = (cid:88) λ−(q+1)b2 ≥ λ−(q+1) (cid:88) b2 = λ−(q+1)(cid:13) (cid:13)UT u(cid:13) (cid:13)2 .
i i r+1 i r+1 −s
i=1 i=r+1
(cid:13) (cid:13)
Because β(cid:98) ∈ Rp−r and (cid:13)β(cid:98) (cid:13) = 1, we know that there exist unit vectors β(cid:98) ,β(cid:98) ,··· ,β(cid:98)
1 (cid:13) 1(cid:13) 2 3 p−r
such that β(cid:98) = [β(cid:98) ,β(cid:98) ,··· ,β(cid:98) ] is an orthogonal matrix. Thus, we know that
1 2 p−r
p−r
(cid:13) (cid:13)UT u(cid:13) (cid:13)2 = (cid:13) (cid:13)UT U(cid:98) β(cid:98) (cid:13) (cid:13)2 = (cid:13) (cid:13)UT U(cid:98) β(cid:98)(cid:13) (cid:13)2 −(cid:88)(cid:13) (cid:13)UT U(cid:98) β(cid:98)(cid:13) (cid:13)2
−s (cid:13) −s −s 1(cid:13) (cid:13) −s −s (cid:13) (cid:13) −s −s i(cid:13)
i=2
(cid:13) (cid:13)2 (cid:13) (cid:13)2
≥(cid:13)UT U(cid:98) (cid:13) −(p−r−1) = 1−(cid:13)sinΘ(U(cid:98) ,U )(cid:13) ,
(cid:13) −s −s(cid:13) (cid:13) −s −s (cid:13)
F F
where the last equation holds because of the definition of sinΘ(U(cid:98) ,U ).
−s −s
Combining the above inequalities, we obtain that
(cid:16) (cid:13) (cid:13)2 (cid:17)
λ max(A−(q+1)P Qs∪{a′}) ≥ λ− r+(q 1+1) · 1−(cid:13) (cid:13)sinΘ(U(cid:98) −s,U −s)(cid:13)
(cid:13)
. (118)
F
By Weyl’s inequality and (102), we know that
λ = |λ −λ(cid:98) | ≤ ∥A−A ∥ ≤ 2cmn .
r+1 r+1 r+1 s op a(s+1)
75Similar to how we show (105), we also have that
(cid:13) (cid:13)2 16p·c2(mn )2
(cid:13)sinΘ(U(cid:98) ,U )(cid:13) ≤ a(s+1) . (119)
(cid:13) −s −s (cid:13) c2(mn )2
F
a(s)
Thus, we obtain
(cid:110) 16p·c2(mn )2(cid:111)
tr(A−(q+1)P ) ≥ λ (A−(q+1)P ) ≥ (2cmn )−q−1 1− a(s+1) .
Qs∪{a′} max Qs∪{a′} a(s+1) c2(mn )2
a(s)
(120)
Step C: Establish an upper bound for tr(A−(q+1)P ) Similar to (107) and according
Qs
to (105), we have
λ (A−(q+1)P )
max Qs
=λ (U(cid:98)TA−(q+1)U(cid:98) )
max s s
=λ (U(cid:98)T[U ,U ]diag(λ−(q+1),λ−(q+1),··· ,λ−(q+1))[U ,U ]TU(cid:98) )
max s s −s 1 2 p s −s s
=λ (U(cid:98)TU diag(λ−(q+1),··· ,λ−(q+1))UTU(cid:98) +U(cid:98)TU diag(λ−(q+1),··· ,λ−(q+1))UT U(cid:98) )
max s s 1 r s s s −s r+1 p −s s
≤λ (U(cid:98)TU diag(λ−(q+1),··· ,λ−(q+1))UTU(cid:98) )+λ (U(cid:98)TU diag(λ−(q+1),··· ,λ−(q+1))UT U(cid:98) )
max s s 1 r s s max s −s r+1 p −s s
(cid:13) (cid:13)2 (cid:13) (cid:13)2
≤λ−(q+1)(cid:13)U(cid:98)TU (cid:13) +λ−(q+1)(cid:13)UT U(cid:98) (cid:13)
r (cid:13) s s(cid:13) p (cid:13) −s s(cid:13)
op op
(cid:13) (cid:13)2
≤λ−(q+1) +λ−(q+1)(cid:13)sinΘ(U(cid:98) ,U )(cid:13)
r p (cid:13) s s (cid:13)
F
1 (cid:18) 16p(c)2(mn )2 (cid:19)
≤ 1+ a(s+1) ,
(cmn )q+1 (c)2(mn )q+1(mn )1−q
a(s) a(tn) a(s)
where we used λ ≥ cmn in the last inequality. Thus,
p a(t)
p (cid:18) 16p(c)2(mn )2 (cid:19)
tr(A−(q+1)P ) ≤ pλ (A−(q+1)P ) ≤ 1+ a(s+1) .
Qs max Qs (cmn )q+1 (c)2(mn )q+1(mn )1−q
a(s) a(tn) a(s)
(121)
Combining (120) and (121), we obtain
(cid:16) 16p·c2(mn )2(cid:17)
tr(A−(q+1)P Qs∪{a′})
≥
(cid:16) c (cid:17)q+1 (mn a(s))q+1 1− c2(mn aa (s(s )+ )21)
. (122)
(cid:18) (cid:19)
tr(A−(q+1)P Qs) 2c p(mn a(s+1))q+1
1+
16p(c)2(mn a(s+1))2
(c)2(mn )q+1(mn )1−q
a(tn) a(s)
76Hence, if
(cid:18) mn
a(s)
(cid:19)q+1(cid:16)
1−
16pc2 (cid:0)mn a(s+1)(cid:1)2(cid:17)
>
(cid:16)2c(cid:17)q+2 p(cid:18)
1+
16pc2(cid:16)mn a(s+1)(cid:17)q+1(cid:16)mn a(s+1)(cid:17)1−q(cid:19)
,
mn c2 mn c c2 mn mn
a(s+1) a(s) a(tn) a(s)
then (117) holds.
The next lemma is useful for controlling a sequence based on some iterative inequality.
Lemma 14.9. Given M > 0,A′ > 0, B′,C′ ≥ 0, and q ≥ 0, there exists M ,M ,··· such
0 1 2
that
xq+1 ≤ A′xq−1 +B′(1+C′Mq+1xq−1),
j
then x ≤ M /M . Here, each M depends only on M ,··· ,M , q, and A′,B′,C′.
j+1 j j 0 j−1
Proof of Lemma 14.9. Let
M := M ·sup{x;xq+1 ≤ A′xq−1 +B′(1+C′Mq+1xq−1)}. (123)
j+1 j j
By induction, assume M < ∞. Due to
j
1 (cid:16) (cid:17)
lim A′xq−1 +B′(1+C′Mq+1xq−1) = 0,
x→∞ xq+1 j
we know that the set {x;xq+1 ≤ A′xq−1 +B′(1+C′Mq+1xq−1)} is bounded from above.
j
Thus, M defined by (123) is bounded from above. By induction, we complete the
j+1
proof.
Lemma 14.10. Assume that
(cid:80)n0
I (θ) is nonsingular.
i=1 ai
Let (a(1),a(2),··· ,a(k)) be a permutation of A such that mn ≥ ··· ≥ mn and
n n n a( n1) a( nk)
dim(Q ) < dim(Q ) = p, where Q = {a(1),a(2),··· ,a(s)}, and t = t (mn). To
tn−1 tn s,n n n n n n
simplify the notation, let (a(1),a(2),··· ,a(k)) = (a(1),a(2),··· ,a(k)) and Q = Q .
n n n s s,n
Also assume that the experiment selection rule satisfy the following property:
There exists constants A′ ≥ 0,B′ > 0,C′ > 0 such that for all n ≥ n , if for some
0
1 ≤ s ≤ t −1,
n
(cid:18) mn
a(s)
(cid:19)q+1(cid:16) 1−A′(cid:0)mn a(s+1)(cid:1)2(cid:17)
>
B′(cid:18) 1+C′(cid:16)mn a(s+1)(cid:17)q+1(cid:16)mn a(s+1)(cid:17)1−q(cid:19)
,
mn mn mn mn
a(s+1) a(s) a(tn) a(s)
then a ∈ G(Q ).
n+1 s
77mn
Then, this experiment selection rule also satisfies that sup a(1) < ∞ and inf nI ≥
n≥n0 mn
a(tn)
n≥n0 nmax
mn0
C > 0, where C depending only on A′,B′,C′,k and a(1) .
mn0
a(tn0)
Proof of Lemma 14.10. Recall the definition of t = t (mn), there exists a permutation
n n
τ ∈ P over A such that
n
mn ≥ mn ≥ ··· ≥ mn ,
τn(1) τn(2) τn(k)
a(1) = τ (1),··· ,a(k) = τ (k),
n n
and dim(V ) < dim(V ) = p. Define
Qtn−1 Qtn
mn
Ind(n) = a(1) .
mn
a(tn)
mn
To show sup a(1) < ∞, it is sufficient to show that if n ≥ n ,
n≥n0 mn 0
a(tn)
sup Ind(n) < ∞.
n≥n0
mn0
Let M
0
= mna 0(1) . According to the definition of a(1) and a(tn0), we know that M
0
≥ 1. Next,
a(tn0)
we use induction to prove that for all n ≥ n
0
Ind(n) ≤ 2 max M , (124)
i
0≤i≤k−1
where the sequence {M }k−1 is defined in Lemma 14.9.
i i=1
For the base case, when n = n , we know that
(cid:80)n0
I (θ) is nonsingular, and thus
0 i=1 ai
mn0 ≥ 1. This implies,
a(tn0)
Ind(n ) ≤ M ≤ mn0 < ∞.
0 0 a(1)
For the induction step n > n , assume that Ind(n) ≤ 2max M . We discuss two
0 0≤i≤k−1 i
cases
Case 1: if a ∈ argmax mn, then we will show that Ind(n) ≤ max M and Ind(n+
n+1 a a 0≤i≤k−1 i
1) ≤ 2max M ,
0≤i≤k−1 i
Case 2: if a ̸∈ argmax mn, then we will show that Ind(n+1) ≤ Ind(n).
n+1 a a
Below are the detailed analysis for these two cases.
Case 1: a ∈ argmax mn Without loss of generality, we assume that τ (1) = a(1) =
n+1 a a n
a . If t = 1, then Ind(n) = 1 ≤ max M . Now, we focus on the case where t ≥ 2.
n+1 n 0≤i≤k−1 i n
78Note that τ (1) has been selected as a , and τ (1) ∈/ G(Q ). According to the lemma’s
n n+1 n 1
assumption, we know that for all 1 ≤ s ≤ t −1,
n
(cid:18) mn
a(s)
(cid:19)q+1(cid:16) 1−A′(cid:0)mn a(s+1)(cid:1)2(cid:17)
≤
B′(cid:18) 1+C′(cid:16)mn a(s+1)(cid:17)q+1(cid:16)mn a(s+1)(cid:17)1−q(cid:19)
. (125)
mn mn mn mn
a(s+1) a(s) a(tn) a(s)
Next, we use this inequality iteratively for s = t −1,t −2,··· ,1 to show that Ind(n) ≤
n n
max M . We start with setting s = t −1 in (125), we obtain that
0≤i≤k−1 i n
xq+1 ≤ A′xq−1 +B′(1+C′Mq+1xq−1),
0
mn mn
where x = a(tn−1). According to Lemma 14.9, this implies x = a(tn−1) ≤ M .
mn mn 1
a(tn) a(tn)
mn
Set s = t −2 in (125), and combine it with a(tn−1) ≤ M , we have
n mn 1
a(tn)
xq+1 ≤ A′xq−1 +B′(1+C′Mq+1xq−1),
1
mn mn
where x = a(tn−2). Apply Lemma 14.9 again, we obtain that a(tn−2) ≤ M /M , which
mn mn 2 1
a(tn−1) a(tn−1)
mn
further implies a(tn−2) ≤ M . By similar arguments, set s = t −3,t −4,··· ,1, we obtain
mn 2 n n
a(tn)
that
mn
a(1) ≤ M ≤ max M .
mn tn−1
0≤i≤k−1
i
a(tn)
That is, Ind(n) ≤ max M .
0≤i≤k−1 i
Note that in this case mn+1 = mn + 1, and mn+1 = mn , for any s ≥ 2. Set
τn(1) τn(1) τn(s) τn(s)
Q = {τ (1),τ (2),··· ,τ (t )}. We know that dim(V ) = p. Hence,
n n n n Q
max mn+1 mn+1 mn +1 mn
a a = τn(1) = τn(1) ≤ 2 τn(1) ≤ 2 max M .
min a∈Qmn a+1 mn τn+ (1
tn)
mn
τn(tn)
mn
τn(tn)
0≤i≤k−1 i
By Lemma 14.4, we know that
mn+1 max mn+1
Ind(n+1) = τn+1(1) ≤ a a ≤ 2 max M .
mn τn+ +1
1(tn+1)
min a∈Qmn a+1 0≤i≤k−1 i
Case 2: a ̸∈ argmax mn In this case, max mn+1 = mn and mn+1 ≥ mn =
n+1 a a a a τn(1) τn(tn) τn(tn)
min mn, where we let Q = {τ (1),τ (2),··· ,τ (t )}.
a∈Q a n n n n
Applying Lemma 14.4, we have
max mn+1 mn
Ind(n+1) ≤ a a ≤ τn(1) = Ind(n) ≤ 2 max M ,
min a∈Qmn a+1 mn
τn(tn)
0≤i≤k−1 i
79where the last inequality in the above display is due to the induction assumption.
Combine the results from both cases. By induction, we have
mn
a(1) ≤ 2 max M ,
mn
0≤i≤k−1
i
a(tn)
for all n ≥ n . Combined with Lemma 14.3, we know that
0
n mn mn 1
I = a(tn) ≥ a(tn) ≥ > 0,
n n k ·mn 2k ·max M
a(1) 0≤i≤k−1 i
mn0
where k ·max M only depend on A′,B′,C′,k and a(1) .
0≤i≤k−1 k−1 mn0
a(tn0)
Proof of Theorem 14.1. Combining Lemmas 14.7, 14.8 and 14.10, we compete the proof of
n
I
inf ≥ C > 0.
n≥n0 n
By Assumption 6B, we know that
(cid:88) mn n (cid:88)
Iπn(θ) ⪰ aI (θ) ⪰ I I (θ) ⪰ c·C ·I . (126)
a a p
n n
a;mn a≥nI a;mn a≥nI
for all θ ∈ Θ.
14.3 Proof of Theorem 4.1
To show Theorem 4.1, we prove the following more general Theorem 14.11 instead, which
applies to general experiment selection rules that are not necessarily GI0 and GI1.
Theorem 14.11. Let U ∈ (0,1). Assume the experiment selection rule satisfies that
π (a ) ∈ K for large enough n, where
n n U
(cid:26) (cid:27)
K = π ∈ SA : max minπ(a) ≥ U . (127)
U
S⊂A:S is relevant a∈S
(cid:80)
Here, we say that a set of experiments S is relevant if I (θ) is nonsingular for any
a∈A a
θ ∈ Θ. Given Assumptions 1-4 along with either Assumptions 6A-7A or 6B-7B, the θ(cid:98)ML
n
converges to θ∗ almost surely.
Proof of Theorem 4.1. According to Proposition 6.2, there exists U > 0 such that (127)
holds for n large enough, following GI0 or GI1. Theorem 4.1 then follows by applying
80Theorem 14.11.
Proof of Theorem 14.11. Let θ(cid:98) = θ(cid:98)ML for the ease of exposition. According to (5), we know
n n
that l (θ(cid:98) ;a ) ≥ l (θ∗;a ). According to (13) in Assumption 4, with probability 1, for any
n n n n n
η > 0, there exists N such that for n > N, π ∈ K
n U
η η
|l (θ∗;a )−M(θ∗;π )| ≤ and |l (θ(cid:98) ;a )−M(θ(cid:98) ;π )| ≤ .
n n n n n n n n
3 3
It follows that
η
l (θ(cid:98) ;a ) ≥ M(θ∗;π )− .
n n n n
3
Also, we have
η 2η
M(θ∗;π )−M(θ(cid:98) ;π ) ≤ l (θ(cid:98) ;a )−M(θ(cid:98) ;π )+ ≤ .
n n n n n n n n
3 3
That is, for η > 0,
(cid:40) (cid:41)
∞ ∞
(cid:91) (cid:92) 2
P {M(θ(cid:98) ;π )−M(θ∗;π ) ≥ − η} = 1.
n n n
3
m=1n=m
It follows that
(cid:40) (cid:41)
∞ ∞
(cid:92) (cid:91)
P {M(θ(cid:98) ;π )−M(θ∗;π ) ≤ −η} = 0.
n n n
m=1n=m
Notice that
(cid:88)
M(θ∗;π)−M(θ;π) = π(a)D (f ∥f )
KL θ∗,a θ,a
a∈A
By Assumption 7B, we can show that for any π ∈ K , and any ε > 0, there exists a finite
U
positive number η = η(U,ε), such that
sup M(θ;π) ≤ M (θ∗,π)−η.
θ:∥θ−θ∗∥≥ε
This means that for large enough n,
(cid:110) (cid:111) (cid:110) (cid:111)
||θ(cid:98) −θ∗|| ≥ ε ⊂ M(θ(cid:98) ;π ) ≤ M(θ∗;π )−η .
n n n n
81It follows that for any ε > 0,
(cid:32) (cid:33)
(cid:92)∞ (cid:91)∞ (cid:110)(cid:13) (cid:13) (cid:111)
P (cid:13)θ(cid:98) −θ∗(cid:13) ≥ ε = 0.
(cid:13) n (cid:13)
m=1n=m
Thus,
(cid:32) (cid:33)
(cid:16) (cid:17) (cid:92)∞ (cid:91)∞ (cid:92)∞ (cid:26) (cid:13) (cid:13) 1(cid:27)
P lim θ(cid:98) = θ∗ = P (cid:13)θ(cid:98) −θ∗(cid:13) <
n→∞ n (cid:13) n (cid:13) l
l=1m=1n=m
(cid:32) (cid:33)
(cid:91)∞ (cid:92)∞ (cid:26) (cid:13) (cid:13) 1(cid:27)
= lim P (cid:13)θ(cid:98) −θ∗(cid:13) < = 1.
l→∞ (cid:13) n (cid:13) l
m=1n=m
14.4 Proof of Theorem 4.2
The proof of Theorem 4.2 follows the similar strategy as the proof of the classic asymptotic
normality result for MLE with i.i.d. observations, which involves the asymptotic analysis of
the Taylor expansion of the score equation. However, the proof for Theorem 4.2 requires the
analysis of dependent stochastic processes and is more delicate.
In the following series of lemmas, we first justify the use of the score equation in
Lemma 14.12. Then, we provide (almost surely) asymptotic bounds for the Hessian of
the log-likelihood and the score statistic in Lemma 14.13. Lemma 14.14 provides a Taylor
expansion for the score function around the true parameter and the MLE, and gives an upper
bound for the remaining terms. Finally, these lemmas are combined together to obtain the
proof of Theorem 4.2.
Lemma 14.12. Under the setting of Theorem 14.11, if θ∗ ∈ int(Θ), we have
(cid:32) (cid:33)
∞ ∞
(cid:91) (cid:92)
P {∇ l (θ(cid:98) ;a ) = 0} = 1.
θ n n n
m=1n=m
Proof of Lemma 14.12. LetB(θ∗,δ)denotetheopenballwiththecenterθ∗ andradiusδ > 0
such that B(θ∗,δ) ⊂ int(Θ).
Because l (θ;a ) is differentiable in θ, we know that
n n
(cid:110)(cid:13) (cid:13) (cid:111) (cid:110) (cid:111)
(cid:13)θ(cid:98) −θ∗(cid:13) < δ ⊂ ∇ l (θ(cid:98) ;a ) = 0 .
(cid:13) n (cid:13) θ n n n
82Thus,
(cid:32) (cid:33) (cid:32) (cid:33)
(cid:91)∞ (cid:92)∞ (cid:91)∞ (cid:92)∞ (cid:110)(cid:13) (cid:13) (cid:111)
1 ≥ P {∇ l (θ(cid:98) ;a ) = 0} ≥ P (cid:13)θ(cid:98) −θ∗(cid:13) < δ = 1,
θ n n n (cid:13) n (cid:13)
m=1n=m m=1n=m
where the last equation is due to the almost sure convergence of θ(cid:98) obtained from Theo-
n
rem 14.11.
Lemma 14.13. Under Assumptions 1-4, if π ∈ K for large enough n, i.e.,
n U
(cid:32) (cid:33)
∞ ∞
(cid:91) (cid:92)
P {π ∈ K } = 1,
n U
m=1n=m
Also assume that the estimator θ(cid:98) → θ∗ a.s. P . Then, with probability 1,
n ∗
n−1 k
1 (cid:88) (cid:88)
limsup Ψaj(X ) ≤ E Ψa(X) =: µ < ∞,
n−1 2 j X∼f θ∗,a 2 Y
n→∞
j=1 a=1
(cid:13) (cid:13)
lim (cid:13)−∇2l (θ∗;a )−Iπn−1(θ∗)(cid:13) = 0,
n→∞ θ n−1 n−1 op
n−1
(cid:13) (cid:13) 1 (cid:88)
(cid:13)−∇2l (θ;a )+∇2l (θ∗;a )(cid:13) ≤ Ψai(X )∥θ −θ∗∥,
θ n−1 n−1 θ n−1 n−1 op n−1 2 i
i=1
(cid:13) (cid:13) 1
limsup(cid:13)(−∇2l (θ(cid:98) ;a ))−1(cid:13) ≤ < ∞,
(cid:13) θ n−1 n−2 n−1 (cid:13) min λ (Iπ(θ∗))
n→∞ op π∈KU min
(cid:13) (cid:13) 1
limsup(cid:13)(−∇2l (θ(cid:98) ;a ))−1(cid:13) ≤ < ∞,
(cid:13) θ n−2 n−2 n−2 (cid:13) min λ (Iπ(θ∗))
n→∞ op π∈KU min
(cid:13) (cid:13) 1
limsup(cid:13)(−∇2l (θ∗;a ))−1 (cid:13) ≤ < ∞.
θ n n op min λ (Iπ(θ∗))
n→∞ π∈KU min
Proof of Lemma 14.13. Let the information filtration be
F = σ({a ,X ,··· ,a ,X }).
n 1 1 n n
In the rest of the proof, we restrict the analysis to the event
(cid:83)∞ (cid:84)∞
{π ∈ K },
m=1 n=m n U
which has probability 1 by the assumption. Applying Lemma 13.2 on each entry of
−∇2 logf (X ), and note that E(∇2 logf (X )|F ) = −I (θ∗), we obtain
θ θ∗,ai i θ θ∗,ai i i−1 ai
n−1
−∇2l (θ∗;a )−Iπn−1(θ∗) = 1 (cid:88)(cid:0) −∇2 logf (X )−I (θ∗)(cid:1) −a. →s. 0. (128)
θ n−1 n−1 n−1 θ θ∗,ai i ai
i=1
83By Assumption 2 and the relaxed condition (15), we know that
n−1
(cid:13) (cid:13) 1 (cid:88)
(cid:13)−∇2l (θ;a )+∇2l (θ∗;a )(cid:13) ≤ Ψai(X )ψ(∥θ −θ∗∥).
θ n−1 n−1 θ n−1 n−1 op n−1 2 i
i=1
Let{Xa} beasequenceofindependentrandomvariablessuchthatXa ∼ f forall
j 1≤j≤n,a∈A j θ∗,a
j ≥ 1anda ∈ A. ByLemma13.8,wecanreplace(X ,X ,··· ,X )with(Xa1,Xa2,··· ,Xan)
1 2 n 1 2 n
without changing the joint distribution for all n.
Let Y = (cid:80) Ψa(Xa). We know that {Y }∞ are i.i.d. and by Assumption 2, µ :=
i a∈A 2 i i i=1 Y
E Y < ∞.
θ∗ 1
The strong law of large numbers (see Theorem 2.1 in Ross (2014)) implies that with
probability 1,
n
1 (cid:88)
Y → µ .
j Y
n
j=1
Thus, with probability 1
n−1 n−1 k
1 (cid:88) 1 (cid:88)(cid:88) (cid:88)
limsup Ψaj(Xaj) ≤ limsup Ψa(Xa) = E Ψa(X) = µ < ∞.
n−1 2 j n−1 2 j X∼f θ∗,a 2 Y
n→∞ n→∞
j=1 j=1 a∈A a=1
Set A = Iπn−1(θ∗), and ∆A = −∇2l (θ(cid:98) ;a )−A for all n ≥ 1. Notice that
n n n−1 n−2 n−1 n
A−1 −(A +∆A )−1∆A A−1 = (A +∆A )−1(A +∆A −∆A )A−1 = (A +∆A )−1,
n n n n n n n n n n n n n
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)(A +∆A )−1 −A−1 (cid:13) = (cid:13)−(A +∆A )−1∆A A−1 (cid:13) ≤ (cid:13)(A +∆A )−1 (cid:13) ∥∆A ∥ (cid:13)A−1 (cid:13) ,
n n n op n n n n op n n op n op n op
as well as
(cid:13) (cid:13) 1
(cid:13)A−1 (cid:13) ≤ < ∞,
n op min λ (Iπ(θ∗))
π∈KU min
for any n. Furthermore,
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)(A +∆A )−1 (cid:13) ≤ (cid:13)A−1 (cid:13) +(cid:13)(A +∆A )−1 (cid:13) (cid:13)A−1 (cid:13) ∥∆A ∥ ,
n n op n op n n op n op n op
which implies
∥A−1∥
(cid:13)
(cid:13)(A +∆A
)−1(cid:13)
(cid:13) ≤
n op
n n op 1−∥A−1∥ ∥∆A ∥
n op n op
84given that ∥A−1∥ ∥∆A ∥ < 1. Note that
n op n op
∥∆A ∥ ≤ 1 (cid:88)n−1 Ψaj(X )(cid:13) (cid:13)θ(cid:98) −θ∗(cid:13) (cid:13)+(cid:13) (cid:13)−∇2l (θ∗)−Iπn−1(θ∗)(cid:13) (cid:13) .
n op n−1 2 j (cid:13) n−2 (cid:13) n−1 op
j=1
The first term on the right-hand side of the above inequality converges to 0 a.s., because
ˆ
of the almost sure convergence assumption on θ , and the second term converges to 0 a.s.
n
a.s.
because of (128). Consequently, ∥∆A ∥ −→ 0. This further implies that, for n large
n op
enough, ∥∆A ∥ ≤ 1 min λ (Iπ(θ∗)). For such n, we have
n op 2 π∈KU min
∥∆A ∥
∥A−1∥2
(cid:13) (cid:13)(A +∆A )−1 −A−1(cid:13) (cid:13) ≤ n op n op −a. →s. 0.
n n n op 1−∥∆A ∥ ∥A−1∥
n op n op
Note that A +∆A = −∇2l (θ(cid:98) ;a ). Thus, with probability 1,
n n n−1 n−2 n−1
(cid:13) (cid:13) (cid:13) (cid:13) 1
limsup(cid:13)(−∇2l (θ(cid:98) ;a ))−1(cid:13) ≤ limsup(cid:13)A−1 (cid:13) ≤ < ∞.
(cid:13) θ n−1 n−2 n−1 (cid:13) n op min λ (Iπ(θ∗))
n→∞ op n→∞ π∈KU min
Similarly, we can also show that with probability 1,
(cid:13) (cid:13) 1
limsup(cid:13)(−∇2l (θ(cid:98) ;a ))−1(cid:13) ≤ < ∞,
(cid:13) θ n−2 n−2 n−2 (cid:13) min λ (Iπ(θ∗))
n→∞ op π∈KU min
as well as
(cid:13) (cid:13) 1
limsup(cid:13)(−∇2l (θ∗;a ))−1 (cid:13) ≤ < ∞.
θ n n op min λ (Iπ(θ∗))
n→∞ π∈KU min
Lemma 14.14. Under Assumptions 1-4, the Taylor expansion for the score function
∇ l (θ;a ) is given by
θ n n
√ √
∇ l (θ∗ +W / n;a ) = ∇ l (θ∗;a )+∇2l (θ∗;a )W / n+R(θ∗,W ),
θ n n n θ n n θ n n n n
(129)
∇ θl n(θ(cid:98) n′;a n) = ∇ θl n(θ(cid:98) n;a n)+∇2 θl n(θ(cid:98) n;a n)(θ(cid:98)
n′
−θ(cid:98) n)+R′(θ(cid:98) n′,θ(cid:98) n),
√
where θ(cid:98)
n
and θ(cid:98)
n′
are the MLE based on l n(θ;a n) and l n′(θ;a n′),respectively, W
n
= n(θ(cid:98) n−
θ∗),
1 (cid:88)n (cid:13) (cid:13) (cid:16)(cid:13) (cid:13)(cid:17)
∥R(θ∗,W )∥ ≤ Ψaj(X )(cid:13)θ(cid:98) −θ∗(cid:13)ψ (cid:13)θ(cid:98) −θ∗(cid:13) , and
n n 2 j (cid:13) n (cid:13) (cid:13) n (cid:13)
j=1
(cid:13) (cid:13) 1 (cid:88)n (cid:13) (cid:13) (cid:16)(cid:13) (cid:13)(cid:17)
(cid:13) (cid:13)R′(θ(cid:98) n′,θ(cid:98) n)(cid:13)
(cid:13)
≤
n
Ψa 2j(X j)(cid:13) (cid:13)θ(cid:98)
n′
−θ(cid:98) n(cid:13) (cid:13)ψ (cid:13) (cid:13)θ(cid:98)
n′
−θ(cid:98) n(cid:13)
(cid:13)
.
j=1
85(cid:68) (cid:69)
Proof of Lemma 14.14. Set g(t) = b,∇ l (θ∗ +t(θ(cid:98) −θ∗);a ) . By the Lagrange mean
θ n n n
value theorem, there exists 0 < t∗ < 1 such that
g(1)−g(0) = g′(t∗),
i.e.,
(cid:68) (cid:69) (cid:68) (cid:69)
b,∇ l (θ(cid:98) ;a )−∇ l (θ∗;a ) = b,∇2l (θ∗ +t∗(θ(cid:98) −θ∗);a )(θ(cid:98) −θ∗)) .
θ n n n θ n n θ n n n n
Then,
(cid:68) (cid:110) (cid:111) (cid:69)
⟨b,R(θ∗,W )⟩ = b, ∇2l (θ∗ +t∗(θ(cid:98) −θ∗);a )−∇2l (θ∗;a ) (θ(cid:98) −θ∗) . (130)
n θ n n n θ n n n
Under Assumption 2, we have
∥R(θ∗,W )∥ = sup ⟨b,R(θ∗,W )⟩
n n
∥b∥≤1
(cid:13) (cid:13) (cid:13) (cid:13)
≤ max (cid:13)∇2l (θ∗ +t∗(θ(cid:98) −θ∗);a )−∇2l (θ∗;a )(cid:13) (cid:13)θ(cid:98) −θ∗(cid:13)
(cid:13) θ n n n θ n n (cid:13) (cid:13) n (cid:13)
0≤t∗≤1 op
1 (cid:88)n (cid:13) (cid:13) (cid:16)(cid:13) (cid:13)(cid:17)
≤ Ψaj(X )(cid:13)θ(cid:98) −θ∗(cid:13)ψ (cid:13)θ(cid:98) −θ∗(cid:13) .
n 2 j (cid:13) n (cid:13) (cid:13) n (cid:13)
j=1
Similarly, we can show that
(cid:68) (cid:69) (cid:68) (cid:110) (cid:111) (cid:69)
b,R′(θ(cid:98) n′,θ(cid:98) n) = b, ∇2 θl n(θ(cid:98)
n
+t∗(θ(cid:98)
n′
−θ(cid:98) n);a n)−∇2 θl n(θ(cid:98) n;a n) (θ(cid:98)
n′
−θ(cid:98) n) .
Under Assumption 2, we have
(cid:13) (cid:13)
(cid:13) (cid:13)R′(θ(cid:98) n′,θ(cid:98) n)(cid:13)
(cid:13)
(cid:68) (cid:69)
= sup b,R′(θ(cid:98) n′,θ(cid:98) n)
∥b∥≤1
(cid:13) (cid:13) (cid:13) (cid:13)
≤ max (cid:13) (cid:13)∇2 θl n(θ(cid:98)
n
+t∗(θ(cid:98)
n′
−θ(cid:98) n);a n)−∇2 θl n(θ(cid:98) n;a n)(cid:13)
(cid:13)
(cid:13) (cid:13)θ(cid:98)
n′
−θ(cid:98) n(cid:13)
(cid:13)
0≤t∗≤1 op
1 (cid:88)n (cid:13) (cid:13) (cid:16)(cid:13) (cid:13)(cid:17)
≤
n
Ψa 2j(X j)(cid:13) (cid:13)θ(cid:98)
n′
−θ(cid:98) n(cid:13) (cid:13)ψ (cid:13) (cid:13)θ(cid:98)
n′
−θ(cid:98) n(cid:13)
(cid:13)
.
j=1
Proof of Theorem 4.2. Write θ(cid:98) = θ(cid:98)ML for the ease of exposition. By Theorem 4.1, we know
n n
that θ(cid:98) converges to θ∗ almost surely. By Lemma 14.12, with probability 1, there exists
n
86√
random integer N < ∞ such that for any n ≥ N, ∇ l (θ(cid:98) ;a ) = 0. Let W = n(θ(cid:98) −θ∗).
θ n n n n n
Recall the remainder function defined in Lemma 14.14,
√ √
R(θ∗,W ) := ∇ l (θ∗ +W / n;a )−∇ l (θ∗;a )−∇2l (θ∗;a )W / n.
n θ n n n θ n n θ n n n
√
With ∇ l (θ∗ +W / n;a ) = 0 provided n ≥ N in mind, we can write W
θ n n n n
√ √
W =
−(cid:8)
∇2l (θ∗;a
)(cid:9)−1(cid:8)
n∇ l (θ∗;a )+ nR(θ∗,W
)(cid:9)
. (131)
n θ n n θ n n n
√
The rest of the proof consists of three parts: in Part I, we show that n∇ l (θ∗;a ) →d
θ n n
N(0,(cid:80) π(a)I (θ∗)); in Part II, we show that ∇2l (θ∗;a ) →P ∗ −(cid:80) π(a)I (θ∗); and
a∈A a √ θ n n a∈A a
in Part III, we show that nR(θ∗,W ) = o (1).
n p
√
Part I: Show that n∇ l (θ∗;a ) →d N(0,(cid:80) π(a)I (θ∗)) as n → ∞ Let b be any
θ n n a∈A a
constant vector in Rp with ∥b∥ = 1. For i = 1,...,n, let
1
ξ := √ bT∇ logf (X ).
n,i
n
θ θ∗,ai i
Set F = σ{a ,X ,a ,X ,··· ,a ,X } for any i ≥ 1 and F denote the trivial σ−algebra.
i 1 1 2 2 i i 0
Applying the Dominated Convergence Theorem, coupled with the classical proof of differ-
entiation under the integral sign, we arrive at the conclusion that E(ξ |F ) = 0, which
n,i i−1
implies that E(ξ ) = 0. Denote σ2 := E(ξ2 |F ) = 1bTI (θ∗)b.
n,i n,i n,i i−1 n ai
Let S := (cid:80)n ξ . Note that E(S ) = 0 and E(S2) < ∞, since E(ξ2 ) < ∞ for all i.
n i=1 n,i n n n,i
Then, {S ,F } is a martingale array with mean 0 and finite variance. We will apply the
n n n≥1
martingale central limit theorem to S . We check the conditions first.
n
We first check the conditional variance condition. We write
(cid:40) (cid:41) (cid:40) (cid:41)
n n
(cid:88) 1 (cid:88) (cid:88)
σ2 = bT I (θ∗) b = bT π (a)I (θ∗) b.
n,i n ai n a
i=1 i=1 a∈A
Due to the convergence assumption of π , we have
n
(cid:40) (cid:41) (cid:40) (cid:41)
(cid:88) P (cid:88)
bT π (a)I (θ∗) b →∗ bT π(a)I (θ∗) b.
n a a
a∈A a∈A
Then the conditional variance condition holds.
We then check the conditional Lindeberg’s condition. Assume random variables {Xa}
a∈A
87have densities {f } , respectively. For any ε > 0, with probability 1,
θ∗,a a∈A
(cid:88)n
E(cid:8) ξ2 I(|ξ | > ε)|F (cid:9) ≤
(cid:88)k
E(cid:8) ∥∇ logf (Xa)∥2I(∥∇ logf (Xa)∥ >
√
nε)(cid:9) .
n,i n,i i−1 θ θ∗,a θ θ∗,a
i=1 a=1
By Assumption 2,
√
lim E(cid:8) ∥∇ logf (Xa)∥2I(∥∇ logf (Xa)∥ > nε)(cid:9) = 0.
θ θ∗,a θ θ∗,a
n→∞
Thus, the conditional Lindeberg condition holds.
By the Martingale Central Limit Theorem (Corollary 3.1 in Hall and Heyde (1980)), we
have
(cid:32) (cid:40) (cid:41) (cid:33)
n
(cid:88) ξ →d N 0,bT (cid:88) π(a)I (θ∗) b .
n,i a
i=1 a∈A
It follows by Cram´er–Wold theorem (see Billingsley (1999) p383) that
(cid:32) (cid:33)
√ n∇ l (θ∗;a ) →d N 0,(cid:88) π(a)I (θ∗) . (132)
θ n n a
a∈A
Part II: Show that ∇2l (θ∗;a ) →P ∗ −(cid:80) π(a)I (θ∗) For each i = 1,...,n, by As-
θ n n a∈A a
sumption A3, we have
E(cid:8)
∇2 logf (X )|F
(cid:9)
= −I (θ∗).
θ θ∗,ai i i−1 ai
Also, the conditional expectation has
n
1 (cid:88) P (cid:88)
I (θ∗) = Iπn(θ∗) →∗ π(a)I (θ∗),
n
ai a
i=1 a∈A
due to the convergence assumption of π .
n
For each i,l = 1,...,p, define
(cid:88) (cid:12) (cid:0) (cid:1) (cid:12)
G = (cid:12) ∇2 logf (Xa) (cid:12).
i,l (cid:12) θ θ∗,a i,l(cid:12)
a∈A,Xa∼f θ∗,a(·)
Then, for all x ≥ 0 and i,l ≥ 1,
(cid:110)(cid:12) (cid:12) (cid:111)
P (cid:12)(cid:0) ∇2 logf (Xa)(cid:1) (cid:12) > x ≤ P{G > x}
(cid:12) θ θ∗,a i,l(cid:12) i,l
88(cid:12) (cid:12)
This implies that (cid:80) E((cid:12)(∇2 logf (Xa)) (cid:12)) ≤ (cid:80) E(G ) < ∞ under Assumption 2.
a∈A (cid:12) θ θ∗,a i,l(cid:12) i,l i,l
By Lemma 13.2 and the Slutsky’s theorem, we arrive at
n
1 (cid:88) P (cid:88)
∇2 logf (X ) = ∇2l (θ∗;a ) →∗ − π(a)I (θ∗). (133)
n θ θ∗,ai i θ n n a
i=1 a∈A
√
Part III: Show that nR(θ∗,W ) = o (1) AccordingtoLemma14.13andLemma14.14,
n p
we know that
(cid:13) (cid:13)√ nR(θ∗,W )(cid:13) (cid:13) ≤ 1 (cid:88)n Ψaj(X )∥W ∥ψ(cid:16)(cid:13) (cid:13)θ(cid:98) −θ∗(cid:13) (cid:13)(cid:17) . (134)
n n 2 j n (cid:13) n (cid:13)
j=1
By (131), we have
(cid:13) (cid:13) √ √
∥W ∥ ≤
(cid:13)(cid:8)
∇2l (θ∗;a
)(cid:9)−1(cid:13) (cid:8)(cid:13)
(cid:13) n∇ l (θ∗;a
)(cid:13) (cid:13)+(cid:13)
(cid:13) nR(θ∗,W
)(cid:13) (cid:13)(cid:9)
. (135)
n (cid:13) θ n n (cid:13) θ n n n
op
The above two inequalities together implies
(cid:16)(cid:13) (cid:13)(cid:17)(cid:13) (cid:13) √
1 (cid:80)n Ψaj(X )ψ (cid:13)θ(cid:98) −θ∗(cid:13) (cid:13){∇2l (θ∗)}−1(cid:13) ∥ n∇ l (θ∗)∥
(cid:13) (cid:13)√ nR(θ∗,W n)(cid:13) (cid:13) ≤n j=1 2 j (cid:13) n (cid:13) (cid:13) (cid:13) θ n (cid:13) (cid:16)(cid:13) (cid:13)op (cid:13)θ (cid:17)n (136)
1− 1 (cid:80)n Ψaj(X )(cid:13){∇2l (θ∗)}−1(cid:13) ψ (cid:13)θ(cid:98) −θ∗(cid:13)
n j=1 2 j (cid:13) θ n (cid:13) (cid:13) n (cid:13)
op
(cid:13) (cid:13) (cid:16)(cid:13) (cid:13)(cid:17)
given that 1 (cid:80)n Ψaj(X )(cid:13){∇2l (θ∗)}−1(cid:13) ψ (cid:13)θ(cid:98) −θ∗(cid:13) < 1. We have shown in (133)
n j=1 2 j (cid:13) θ n (cid:13) (cid:13) n (cid:13)
op
in Part II that
(cid:88)
∇2l (θ∗;a ) = − π(a)I (θ∗)+o (1),
θ n n a p
a∈A
and thus
(cid:32) (cid:33)
(cid:13) (cid:13)(cid:8)
∇2l (θ∗;a
)(cid:9)−1(cid:13)
(cid:13) = λ−1
(cid:88)
π(a)I (θ∗) +o (1). (137)
(cid:13) θ n n (cid:13) min a p
op
a∈A
Furthermore, under Assumption 5, by the consistency result in Theorem 4.1 and Lemma
14.13, we have
(cid:13) (cid:13)
(cid:13)θ(cid:98) −θ∗(cid:13) = o (1),
(cid:13) n (cid:13) p
which implies that
1 (cid:88)n
Ψaj(X
)(cid:13)
(cid:13)(cid:8) ∇2l
(θ∗)(cid:9)−1(cid:13)
(cid:13)
ψ(cid:16)(cid:13)
(cid:13)θ(cid:98)
−θ∗(cid:13) (cid:13)(cid:17)
= o (1). (138)
n 2 j (cid:13) θ n (cid:13) (cid:13) n (cid:13) p
op
j=1
89(cid:13) (cid:13) (cid:16)(cid:13) (cid:13)(cid:17)
Let the event D := (cid:8) 1− 1 (cid:80)n Ψaj(X )(cid:13){∇2l (θ∗)}−1(cid:13) ψ (cid:13)θ(cid:98) −θ∗(cid:13) > 1(cid:9) . We have
n n j=1 2 j (cid:13) θ n (cid:13) (cid:13) n (cid:13) 2
op
P(D ) → 1, as n → ∞.
n
On the event D , according to (132) , we have
n
√
(cid:13) (cid:13)
(cid:13) n∇ l (θ∗)(cid:13) = O (1),
θ n p
which together with (136), (138) and Lemma 14.13 yields ∥W ∥ = O (1). It follows from
n p
(134) that
√
(cid:13) (cid:13)
(cid:13) nR(θ∗,W )(cid:13) = o (1).
n p
Therefore, applying Slutsky’s Theorem and the continuous mapping Theorem to (131), we
have
 
(cid:32) (cid:33)−1
W
n
→d N 0, (cid:88) π(a)I a(θ∗) ,
a∈A
which concludes the proof.
14.5 Proof of Theorem 4.3
We first present an extension of the classic convergence theorem by Robbins and Siegmund
(1971), which is frequently employed to prove convergence of stochastic processes within the
fields of stochastic approximation and reinforcement learning. It provides conditions on a
stochastic process {Z } for it to converge almost surely. The following modified version of
n
the Robbins-Siegmund Theorem allows us to obtain a better estimate of the convergence
rate of {Z }. Later in this section, we will apply this result to Z = F (π )−F (π∗) for
n n θ∗ n θ∗
proving Theorem 4.3.
Lemma 14.15 (Modified Robbins-Siegmund Theorem). Let a ,c be integrable random
n n
variables and Z be a non-negative integrable random variable adaptive to filtration F for
n n
all n ≥ 1, and F ⊂ F ···. Assume that
1 2
E[Z | F ] ≤ (1−a )Z +c , for all n ≥ 1. (139)
n+1 n n n n
Set a− = max{0,−a } and a+ = max{0,a }. Assume
n n n n
∞
(cid:88)
a− < ∞.
n
n=1
90Then, the following statements hold.
1. If
∞
(cid:88)
c exists with probability 1, (140)
n
n=1
then there exists non-negative random variable Z such that lim Z = Z with
∞ n→∞ n ∞
probability 1.
2. If we assume (140) holds and further require
∞
(cid:88)
{a+Z }∞ are all intergrable, and a = +∞ with probability 1, (141)
n n n=1 n
n=1
then lim Z = 0 with probability 1.
n→∞ n
3. Assume (141) holds. If there exists 0 < β < c such that a ≥ c and the limit (cid:80)∞ nβc
n n n=1 n
exists with probability 1, then lim nβZ = 0 with probability 1.
n→∞ n
Proof of Lemma 14.15.
Part 1 First of all, (139) implies
E[Z | F ] ≤ (cid:0) 1+a−(cid:1) Z +c .
n+1 n n n n
SetZ′ = Zn , andc′ = cn . Noticethat(cid:80)∞ a− < ∞implies(cid:81)∞ (1+a−) <
n (cid:81)n−1(1+a−) n (cid:81)n (1+a−) n=1 n n=1 n
i=1 i i=1 i
∞. By Abel’s test for series (see Exercise 9.15 in Ghorpade and Limaye (2006)), we know
that
∞
(cid:16)(cid:88) (cid:17)
P c′ exsits = 1, (142)
n
n=1
Because |c′ | ≤ |c |, 0 ≤ Z′ ≤ Z as well as c and Z are integrable, we know that c′ and
n n n n n n n
Z′ are also integrable. Note that
n
E(cid:2)
Z′ | F
(cid:3)
≤ Z′ +c′ . (143)
n+1 n n n
Let Y = Z′ and Y = Z′ −(c′ +···+c′ ), which are integrable for all n ≥ 2. We know
1 1 n n 1 n−1
that Y is integrable for all n ≥ 1. By (143), we obtain
n
E[Y | F ] ≤ Y . (144)
n+1 n n
91Let τ = inf{n;(cid:80)n c′ > T}, for any T ≥ 0. Note that {τ > n} = {(cid:80)n c′ ≤ T} ∈ F .
T k=1 k T i=1 n n
Because
n n
(cid:88) (cid:88)
Y = Y ·I(τ = l)+Y ·I(τ > n), and |Y | ≤ |Y |,
n∧τT l T n T n∧τT i
l=1 i=1
we obtain that Y is integrable for all n ≥ 1.
n∧τT
By the definition of τ , we know that i ≤ τ −1 =⇒ (cid:80)i c′ ≤ T, which implies that
T T k=1 k
for any n ≥ 1
n∧ (cid:88)τT−1
Y ≥ − c′ ≥ −T.
n∧τT k
k=1
By (144), we know that for any n ≥ 1,
E[Y |F ] = Y I(τ ≤ n)+E[Y |F ]I(τ > n)
(n+1)∧τT n τT T n+1 n T
n
(cid:88)
≤ Y I(τ = l)+Y I(τ > n) = Y , and
τT T n T n∧τT
l=1
0 ≤ E[Y +T] ≤ ··· ≤ E[Y +T] = EZ′ +T < ∞.
n∧τT 1∧τT 1
This concludes that Y +T is a non-negative supermartingale (see Section 1.1 in Hall and
n∧τT
Heyde (1980)). Applying Doob’s convergence theorem (see Theorem 2.5 in Hall and Heyde
(1980)) to L1 uniformly bounded submartingale −(Y +T) , we know that lim Y
n∧τT n→∞ n∧τT
exists and is finite for any T ≥ 0.
In conclusion, lim Y exists and is finite almost surely on event
n→∞ n
n
(cid:8)(cid:88) (cid:9)
{τ = ∞} = c′ ≤ T for any n ≥ 1 for any T ≥ 0.
T i
i=1
Combining this with (142), we know that lim Y exists and is finite almost surely. Hence,
n→∞ n
with probability 1, we have
∞ ∞
(cid:89) (cid:16) (cid:88) (cid:17)
Z = lim Z = (1+a−) lim Y − c′ .
∞ n n n k
n→∞ n→∞
n=1 k=1
Part 2 Because a+ = max{0,a }, we have a′ = a+ n ≥ 0. Similar to the arguments in
n n n 1+a−
n
Part 1, we have
E(cid:2)
Z′ | F
(cid:3)
≤ (1−a′ )Z′ +c′ .
n+1 n n n n
92Because we assume that
(cid:80)∞
a = +∞ with probability 1, we have
n=1 n
N N
(cid:88) 1 (cid:88)
a′ ≥ (a −a−) → +∞,
n 1+sup a− n n
n=1 n n n=1
as N → ∞ with probability 1. Let Y′ = Z′ and for any n ≥ 2
1 1
n−1 n−1
(cid:88) (cid:88)
Y′ = Z′ + a′Z′ − c′.
n n k k k
k=1 k=1
Since |a′ | ≤ |a |, |a′ Z′| ≤ a+Z , |a | and a+Z are intergrable, we know that Y′ is inter-
n n n n n n n n n n
grable for any n ≥ 1. Similar to the arguments in Part 1, we obtain that Y′ is intergrable
n∧τT
for any T ≥ 0,
E[Y′ |F ] ≤ Y′ and
n+1 n n
n∧ (cid:88)τT−1
Y′ ≥ − c′ ≥ −T, and
n∧τT k
k=0
E[Y′ |F ] = Y′ I(τ ≤ n)+E[Y′ |F ]I(τ > n)
(n+1)∧τT n τT T n+1 n T
n
(cid:88)
≤ Y′ I(τ = l)+Y′I(τ > n) = Y′ .
τT T n T n∧τT
l=1
In conclusion, we obtain that Y′ is a super-martingale, such that
n∧τT
0 ≤ E[Y′ +T] ≤ ··· ≤ E[Y′ +T] = EZ′ +T < ∞.
n∧τT 1∧τT 1
This concludes that {Y′ +T}∞ is a L1 uniformly bounded supermartingale (see Section
n∧τT n=1
1.1 in Hall and Heyde (1980)). Applying Doob’s convergence theorem (see Theorem 2.5 in
Hall and Heyde (1980)) to L1 uniformly bounded submartingale −(Y′ +T), we know that
n∧τT
lim Y′ exists and is finite for any T ≥ 0. Similar to the arguments in Part 1, we know
n→∞ n∧τT
that with probability 1, lim Y′ exists and is finite.
n→∞ n
Notice that with probability 1,
n−1 n−1 n−1
(cid:88) (cid:88) (cid:88)
0 ≤ a′Z′ = Y′ −Z′ + c′ ≤ Y′ + c′ < ∞.
k k n n k n k
k=1 k=1 k=1
Combined with (142), we obtain that (cid:80)∞ a′Z′ exists with probability 1.
k=1 k k
93Because with probability 1,
∞ ∞
(cid:88) (cid:88)
a′ = +∞, a′Z′ < ∞, and lim Z′ exists,
n k k n
n→∞
n=1 k=1
we obtain that with probability 1
lim Z′ = 0.
n
n→∞
Part 3 We define g(t) = (1 − ct)(1 + t)β,t ≥ 0. Notice that lim g(t)−g(0) = g′(0) =
t→0+ t
−(c − β) < 0. Thus, there exists N > 0 such that g(1) ≤ 1 − c−β, for all n ≥ N. Define
n 2n
C = (n+1)βc and A = 1−A′ , where
n n n n

 g(1),n < N
A′ = n
n
 1− c−β,n ≥ N.
2n
Note that
(n+1)β 1 c 1
(cid:0) (cid:1)
(1−a ) ≤ (1+ )β(1− ) = g ≤ A′ = 1−A ,n ≥ 1.
nβ n n n n n n
This implies that
E(cid:2)
(n+1)βZ | F
(cid:3)
≤ (1−A )nβZ +C .
n+1 n n n n
Becausethelimit(cid:80)∞ nβc existsand{(n+1)β}∞ isamonotoneandboundedsequence
n=1 n n n=1
with probability 1, by Abel’s test for series (see Exercise 9.15 in Ghorpade and Limaye
(2006)), the limit
(cid:80)∞
C exists with probability 1.
n=1 n
It is straightforward to check that
∞ ∞
(cid:88) (cid:88)
A = ∞, and A− < ∞.
n n
n=1 n=1
Applying the second conclusion in Lemma 14.15, we obtain that with probability 1
lim nβZ = 0.
n
n→∞
94In the rest of the section, let Z = F (π ) − F (π∗). Applying Assumption 5 and
n θ∗ n θ∗
Lemma 13.7, we know that F (π) is convex in π. Notice
θ
(cid:18) (cid:19)
n−1 1
E[Z | F ] = F π + δ −F (π∗),
n n−1 θ∗
n
n−1
n
an θ∗
and
Z = F (π )−F (π∗).
n−1 θ∗ n−1 θ∗
Lemma 14.16. K defined in (127) satisfies that
U
• π,π′ ∈ K ,t ∈ (0,1) =⇒ tπ +(1−t)π′ ∈ K , and
U U/2
• π ∈ K =⇒ λ (cid:0) {Iπ(θ)}−1(cid:1) ≤ 1 .
U max c·U
Moreover, there exists U > 0 such that
0
(cid:91)
arg min F (π) ⊂ K . (145)
π∈SA
θ(cid:98) U0
θ(cid:98)∈Θ
and for both generalized GI0 and GI1 defined in (76) and (77), and for all n ≥ n , we have
0
π ∈ K ,∀n ≥ n ,
n U0 0
where n satisfies that
(cid:80)n0
I (θ(cid:98) ) is non-singular for some θ(cid:98) ∈ Θ.
0 i=1 ai 0 0
Proof of Lemma 14.16. For any π,π′ ∈ K , and t ∈ (0,1/2],
U
1 U
max mintπ(a)+(1−t)π′(a) ≥ max minπ′(a) ≥ .
S⊂A:S isrelevant a∈S 2 S⊂A:S isrelevant a∈S 2
When t ∈ [1/2,1), we can obtain the same lower bound, which means that tπ+(1−t)π′ ∈
K for any t ∈ (0,1). For any π ∈ SA, define
U/2
π := max minπ(a). (146)
I
S⊂A:S isrelevant a∈S
By Assumption 6B,
(cid:88) (cid:88)
cπ ·I ⪯ π(a)I (θ) ⪯ I (θ) ⪯ c·P . (147)
I p a a V (θ)
{a;π(a)>0}
a∈A a:π(a)>0
Notice that for any θ,
(cid:16) (cid:17)
π > 0 =⇒ Iπ(θ) ≻ 0 =⇒ dim V (θ) = p =⇒ π ≥ min π(a) > 0.
I {a;π(a)>0} I
a;π(a)>0
95Thus, we know that π > 0, if and only if Iπ(θ∗) is nonsingular, if and only if Iπ(θ) is
I
nonsingular for any θ ∈ Θ.
Let Q = {a ∈ A;π(a) > π }. We will show by contradiction that if Q is not empty,
I
then dim(V ) < p. If dim(V ) = p, then by Assumption 6B, we know that P = I . By
Q Q VQ(θ) p
(16), we know that Iπ(θ∗) is nonsingluar, which means that Q ⊂ A is relevant. However,
min π(a) > π , which contradicts the definition of π in (146).
a∈Q I I
Thus, dim(V ) < p and P ̸= I . By Assumption 6B, we obtain
Q VQ(θ) p
(cid:88) (cid:88) (cid:88)
cπ ·I ⪯ π(a)I (θ) ⪯ π(a)I (θ)+ π I (θ) ⪯ c·P +cπ ·I . (148)
I p a a I a VQ(θ) I p
a∈A a∈Q a̸∈Q
Applying Courant–Fischer–Weyl min-max principle (see Chapter I of Hilbert and Courant
(1953) or Corollary III.1.2 in Bhatia (1997)) for Rayleigh quotient on (148), we obtain that
λ (Iπ(θ)) ∈ [cπ ,cπ ]. (149)
min I I
Applying Theorem 14.1, π ∈ K implies λ (Iπ(θ)) ≥ cU, which further implies
U min
(cid:0) (cid:1)
λ {Iπ(θ)}−1 ≤ 1 .
max c·U
Also, we have
(cid:0) (cid:1) (cid:0) (cid:1)
λ {Iπ(θ∗)}−1 → ∞ ⇐⇒ λ {Iπ(θ)}−1 → ∞,∀θ ∈ Θ ⇐⇒ π → 0. (150)
max max I
We will show (145) by contradiction. Set U = 1. Assume, in contrast to (145), that there
n n
exists θ(cid:98)n ∈ Θ and πn ∈ SA, such that
1
F (πn) = min F (π), and πn ≤ U = .
θ(cid:98)n π∈SA θ(cid:98)n I n n
Then,
limsupF (πn) = limsup min F (π) ≤ max min F (π) < ∞ (151)
n→∞
θ(cid:98)n
n→∞ π∈SA
θ(cid:98)n
θ∈Θ π∈SA
θ
Set A = Iπn(θ). We know that ∥A ∥ ≤ c and by (149) λ (A ) ≤ cπn ≤ c/n. Let
n n op min n I
G (A−1) = Φ (A−1). When q = 0, we know that
θ n q n
lim minΦ (A−1) ≥ lim log(n/c)+(p−1)log(1/c) = ∞. (152)
0 n
n→∞θ∈Θ n→∞
When q > 0, we know that
lim minΦ (A−1) ≥ lim minλ (A−1) ≥ lim n/c = ∞. (153)
q n max n
n→∞θ∈Θ n→∞θ∈Θ n→∞
96Combining (152) and (153) with Assumption 5 and (150), we know that
lim minF (πn) = lim minG ({Iπn(θ)}−1) = ∞. (154)
θ θ
πn→0θ∈Θ πn→0θ∈Θ
I I
By equivalence result (150) and limit result (154), since πn → 0 as n → ∞, we obtain
I
liminfF (πn) ≥ liminfminF (πn) → ∞,
n→∞
θ(cid:98)n
πn→0 θ∈Θ
θ
I
which contradicts (151). Thus, (145) holds.
By Theorem 14.1, we know that there exists U > 0 such that
0
π ∈ K ,n ≥ n .
n U0 0
Combined with (149), we completes the proof.
Lemma 14.17. Under Assumptions 1-5, there exists L < ∞ such that
U
∥∇∇ F (π)∥ ≤ L , and (cid:13) (cid:13)∇2F (π)(cid:13) (cid:13) ≤ L ,
θ θ op U θ op U
for any θ ∈ Θ and π ∈ K .
U
Proof of Lemma 14.17. Define u (A) = G (A−1).
θ θ
For any positive definite matrix A, each element of A−1 is a well-defined composition of
elementary functions of A. Therefore, each element of A−1 is infinitely differentiable.
By Assumptions 5 and Lemma 13.5, ∇ G (Σ) and ∇2G (Σ) are continuous in (θ,Σ)
θ θ θ
for any θ ∈ Θ and positive definite matrix Σ. Set A = Iπ(θ). We have F (π) = u (Iπ(θ)).
θ θ
By the chain rule, we know that
(cid:42) (cid:43)
(cid:12) (cid:12)
∂ F (π) = ∂ u (A)(cid:12) (cid:12) , ∂Iπ(θ) + ∂ G (A−1)(cid:12) (cid:12) . (155)
θ θ θ
∂θ ∂A (cid:12) ∂θ ∂θ (cid:12)
i A=Iπ(θ) i i A=Iπ(θ)
(cid:12)
Notice that each element of ∂ u (A)(cid:12) is continuously differentiable in A. Thus
∂A θ A=Iπ(θ)
∂ ∂
u (Iπ(θ)) (156)
θ
∂π(a)∂A
exists and is continuous in (π,θ) ∈ K ×Θ.
U
Furthermore, we know that
∂ ∂Iπ(θ) ∂I (θ)
a
= , and (157)
∂π(a) ∂θ ∂θ
i i
97(cid:42) (cid:43)
(cid:12) (cid:12)
∂ (cid:16) ∂ G (A−1)(cid:12) (cid:12) (cid:17) = ∂ ∂ G (A−1)(cid:12) (cid:12) ,I (θ) . (158)
θ θ a
∂π(a) ∂θ (cid:12) ∂θ ∂A (cid:12)
i A=Iπ(θ) i A=Iπ(θ)
Combining (155), (156), (157) and (158), we obtain that ∥∇∇ F (π)∥ is continuous over
θ θ
Θ×K for any U > 0. By Lemma 14.16 and the definition of K in (127), we know that
U U
K is a close subset of SA. Thus, Θ×K is compact.
U U
By chain rule, we know that
(cid:42) (cid:43)
∂ F (π) = ∂ u (A)(cid:12) (cid:12) (cid:12) , ∂Iπ(θ) = (cid:28) ∂ u (Iπ(θ)),I (θ)(cid:29) . (159)
θ θ θ a
∂π(a) ∂A (cid:12) ∂π(a) ∂A
A=Iπ(θ)
Because u (Iπ(θ)) is twice continously differentiable in A, we know that ∇2F (π) is con-
θ θ
tinuous over compact set Θ×K for any U.
U
In conclusion, there exists L < ∞ such that
U
∥∇∇ F (π)∥ ≤ L , and (cid:13) (cid:13)∇2F (π)(cid:13) (cid:13) ≤ L ,
θ θ op U θ op U
for any θ ∈ Θ and π ∈ K .
U
Lemma 14.18. Under Assumptions 1-5 as well as 6A-7A (or 6B-7B), the generalized GI0
and (76) and GI1, defined in (77), satisfy that there exists a constant L > 0 such that,
1 L
F (π )−F (π∗) ≤ (1− )(F (π )−F (π∗))+ ,n ≥ n . (160)
θn−1 n θn−1 n n θn−1 n−1 θn−1 n n2 0
Proof of Lemma 14.18. By Theorem 14.1 and Lemma 14.16, there exists 0 < U < ∞ such
that π ,π∗ ∈ K , where we define
n n U
π∗ ∈ arg min F (π).
n θn−1
π∈SA
By Lemma 13.7, F (π) is convex in π. According to Jensen’s inequality,
θn−1
n−1 1 n−1 1
F ( π + π∗) ≤ F (π )+ F (π∗). (161)
θn−1 n n−1 n n n θn−1 n−1 n θn−1 n
Thus,
n−1 1 1
F ( π + π∗)−F (π∗) ≤ (1− )(F (π )−F (π∗)). (162)
θn−1 n n−1 n n θn−1 n n θn−1 n−1 θn−1 n
98Notice that
(cid:28) (cid:29)
n−1 1 1 1
F ( π + π∗)−F (π ) = ∇F (π ), π∗ − π +R(π ,π∗),
θn−1 n n−1 n n θn−1 n−1 θn−1 n−1 n n n n−1 n−1 n
(163)
where
(cid:28) (cid:29)
1 1
R(π ,π∗) = ∇F (π′)−∇F (π ), π∗ − π ,
n−1 n θn−1 θn−1 n−1 n n n n−1
for some π′ between π and n−1π + 1π∗. By Lemma 14.16, we know that π′ ∈ K .
n−1 n n−1 n n U/2
By Assumptions 1-5 and Lemma 14.17, there exists a constant C′ < ∞ such that
|R(π ,π∗)|
n−1 n
≤∥1 π∗ − 1 π ∥2 sup (cid:13) (cid:13)∇2F (π)(cid:13) (cid:13)
n n n n−1 θ op
π∈K ,θ∈Θ
U/2
≤C′
sup (cid:13) (cid:13)∇2F (π)(cid:13) (cid:13)
(164)
n2 θ op
π∈K ,θ∈Θ
U/2
L
= ,
n2
where
L = C′ sup (cid:13) (cid:13)∇2F (π)(cid:13) (cid:13) < ∞.
θ op
π∈K ,θ∈Θ
U/2
By Lemma 13.4, we know that
∂
⟨∇F (π),δ ⟩ = F (π) = −(cid:10) ∇G ({Iπ(θ)}−1),{Iπ(θ)}−1I (θ){Iπ(θ)}−1(cid:11) .
θ a θ θ a
∂π(a)
Let a(1) be the experiment selected following the generalized GI1. Then, according to the
n
definition of GI1, it minimizes the following function over SA with respect to a:
(cid:28) (cid:29) k (cid:28) (cid:29)
1 1 (cid:88) 1 1
∇F (π ), π − π = π(a) ∇F (π ), δ − π .
θn−1 n−1
n n
n−1 θn−1 n−1
n
a
n
n−1
a=1
By similar Taylor expansion arguments as those for (164), we have for all n ≥ n ,
0
(cid:12) (cid:28) (cid:29)(cid:12)
(cid:12) (cid:12)F (n−1 π + 1 δ )−F (π )− ∇F (π ), 1 δ − 1 π (cid:12) (cid:12) ≤ L .
(cid:12) θn−1 n n−1 n a( n1) θn−1 n−1 θn−1 n−1 n a( n1) n n−1 (cid:12) n2
The above inequality implies that for all n ≥ n , GI1 satisfies
0
n−1 1 n−1 1 L
F (π ) = F ( π + δ ) ≤ F ( π + π∗)+ . (165)
θ(cid:98)n−1 n θ(cid:98)n−1 n n−1 n a( n1) θ(cid:98)n−1 n n−1 n n n2
99For GI0, let a(0) be the experiment selected at time n. Then, according to its definition we
n
have
n−1 1 n−1 1
F ( π + δ ) ≤ F ( π + δ ). (166)
θ(cid:98)n−1 n n−1 n a( n0) θ(cid:98)n−1 n n−1 n a( n1)
Therefore, the proof of Lemma 14.18 is concluded by combining inequalities (162) – (166).
Lemma 14.19. Under Assumptions 1-5 as well as 6A-7A (or 6B-7B), the generalized GI0
selection (76) and GI1 selection (77) satisfy that there exists 0 < C < ∞ such that
1
F (π )−F (π∗) ≤ (1− )(F (π )−F (π∗))+c ,n ≥ n , (167)
θ∗ n θ∗ θ∗ n−1 θ∗ n−1 0
n
where c = C + C ∥θ −θ∗∥.
n−1 n2 n n−1
Proof of Lemma 14.19. We can rewrite (160) as
1 L
F (π )−F (π )+ (F (π )−F (π∗)) ≤ . (168)
θn−1 n θn−1 n−1 n θn−1 n−1 θn−1 n n2
We first show that for all π ,π ∈ K , and θ ,θ ∈ Θ,
0 1 U 1 2
|F (π )−F (π )−{F (π )−F (π )}| ≤ C ∥π −π ∥∥θ −θ ∥, (169)
θ1 1 θ1 0 θ2 1 θ2 0 1 0 1 1 2
where C = sup ∥∇ ∇F (π)∥ is a positive constant. To show this, set g(t) =
1 π∈K U/2,θ∈Θ θ θ op
F (π(t))−F (π(t)), where π(t) = tπ +(1−t)π ,t ∈ [0,1] and π ,π ∈ K , where K
θ1 θ2 1 0 0 1 U U
is chosen according to the proof of Lemma 14.18. By Lagrange mean value theorem, there
exists 0 < t < 1 such that
g(1)−g(0) = g′(t) = ⟨∇F (π(t))−∇F (π(t)),π −π ⟩.
θ1 θ2 1 0
By Assumptions 1-5, Lemma 14.16 and Lemma 14.17, we know that
∥∇F (π(t))−∇F (π(t))∥
θ1 θ2 ≤ sup ∥∇ ∇F (π)∥ < ∞.
∥θ −θ ∥ θ θ op
1 2 π∈K ,θ∈Θ
U/2
Set
C = sup ∥∇ ∇F (π)∥ .
1 θ θ op
π∈K ,θ∈Θ
U/2
Then, the above inequality implies (169). Note that
2
∥π −π ∥ ≤ .
n n−1
n
100The above inequality together with (169) implies
2C
|(F (π )−F (π ))−(F (π )−F (π ))| ≤ 1 ∥θ −θ∗∥, and
θ∗ n θ∗ n−1 θn−1 n θn−1 n−1
n
n−1
(170)
|(F (π )−F (π∗))−(F (π )−F (π∗))| ≤ 2C ∥θ −θ∗∥.
θ∗ n−1 θ∗ θn−1 n−1 θn−1 1 n−1
Because F (π∗) ≥ F (π∗), we have
θn−1 θn−1 n
(F (π )−F (π∗))−(F (π )−F (π∗))
θ∗ n−1 θ∗ θn−1 n−1 θn−1 n
≤(F (π )−F (π∗))−(F (π )−F (π∗)) (171)
θ∗ n−1 θ∗ θn−1 n−1 θn−1
≤2C ∥θ −θ∗∥.
1 n−1
By triangular inequality, inequalities (168), (170) and (171), we obtain
1
F (π )−F (π )+ (F (π )−F (π∗))
θ∗ n θ∗ n−1 θ∗ n−1 θ∗
n
≤|(F (π )−F (π ))−(F (π )−F (π ))|
θ∗ n θ∗ n−1 θn−1 n θn−1 n−1
1(cid:16) (cid:17) 2C ∥θ −θ∗∥ (172)
+(F (π )−F (π ))+ F (π )−F (π∗) + 1 n−1
θn−1 n θn−1 n−1 n θn−1 n−1 θn−1 n n
4C L
≤ 1 ∥θ −θ∗∥+ .
n n−1 n2
In conclusion, we know that
1
F (π )−F (π )+ (F (π )−F (π∗)) ≤ c ,
θ∗ n θ∗ n−1 θ∗ n−1 θ∗ n−1
n
where c = C + C ∥θ −θ∗∥, and C = 4C +L.
n−1 n2 n n−1 1
As a corollary of Theorem 4.2, we establish the following:
Corollary 14.20. Under Assumptions 1-5 as well as Assumptions 6A-7A (or Assump-
tions 6B-7B), if there exists U > 0 such that n ≥ n =⇒ π ∈ K , then with probability
0 n U
1,
(cid:88)∞ (cid:13)√ (cid:13)t
n−s(cid:13) n(θ(cid:98)ML −θ∗)(cid:13) < ∞.
(cid:13) n (cid:13)
n=1
provided s > 1,0 < t ≤ 2.
Proof of Corollary 14.20. Let θ(cid:98) = θ(cid:98)ML. We first assume t = 2. Applying Lemma 14.13,
n n
(cid:13) (cid:13) 1
limsup(cid:13){−∇2l (θ∗)}−1 (cid:13) ≤ .
θ n op min λ (Iπ(θ∗))
n→∞ π∈KU min
101(cid:13) (cid:13) (cid:16)(cid:13) (cid:13)(cid:17)
SetD := (cid:8) 1−1 (cid:80)n Ψaj(X )(cid:13){∇2l (θ∗)}−1(cid:13) ψ (cid:13)θ(cid:98) −θ∗(cid:13) > 1,∥{−∇2l (θ∗)}−1∥ ≤
n n j=1 2 j (cid:13) θ n (cid:13) (cid:13) n (cid:13) 2 θ n op
op
(cid:9)
2 . By Lemma 4.1, we know that
minπ∈Kλmin(Iπ(θ∗))
(cid:32) (cid:33)
∞ ∞
(cid:91) (cid:92)
P D = 1. (173)
m
n=1m=n
Note that (134) and (135) yield
(cid:13) (cid:13) √
(cid:13){∇2l (θ∗)}−1(cid:13) ∥ n∇ l (θ∗)∥
∥W n∥I
Dn
≤ I
Dn
1− 1 (cid:80)n
(cid:13) Ψajθ (Xn
)(cid:13)
(cid:13){∇2(cid:13) lop (θ∗)}−1θ
(cid:13)
(cid:13)n
ψ(cid:16)(cid:13)
(cid:13)θ(cid:98)
−θ∗(cid:13) (cid:13)(cid:17)
≤
C′(cid:13) (cid:13)√
n∇ θl
n(θ∗)(cid:13)
(cid:13),
n j=1 2 j (cid:13) θ n (cid:13) (cid:13) n (cid:13)
op
√
where C′ < ∞ and W
n
= n(θ(cid:98) nML −θ∗). Set S
n
= (cid:80)n i=1∇ θlogf θ∗,ai(X i). By Assumption
2, we know that
σ2 := maxE (cid:8) ∥∇ logf (X)∥2(cid:9) < ∞.
a∈A
X∼f θ∗,a θ θ∗,a
By induction, we obtain that
√
E(cid:13)
(cid:13) n∇ l
(θ∗)(cid:13) (cid:13)2
θ n
1
= E∥S ∥2
n
n
=
n1 E(cid:2)E(cid:8)
∥logf θ∗,an(X n)+S
n−1∥2(cid:12)
(cid:12)F
n−1(cid:9)(cid:3)
= n1 E(cid:2) ∥S n−1∥2 +2⟨S n−1,E{logf θ∗,an(X n)|F n−1}⟩+E(cid:8) ∥logf θ∗,an(X n)∥2(cid:12) (cid:12)F n−1(cid:9)(cid:3)
= n1 E(cid:2) ∥S n−1∥2 +E(cid:8) ∥logf θ∗,an(X n)∥2(cid:12) (cid:12)F n−1(cid:9)(cid:3)
1
≤ (cid:0)E∥S ∥2 +σ2(cid:1) ≤ ··· ≤ σ2.
n−1
n
(cid:13)√ (cid:13)2
Apply Lemma 13.1 with X = 1 (cid:13) n(θ(cid:98) −θ∗)(cid:13) , E = D , γ = 1, and ε =
n ns (cid:13) n (cid:13) n n n−1
√
1 (C′)2E[∥ n∇ l (θ∗)∥2 |F ], because
ns θ n n−1
(cid:88)∞ (cid:88)∞ σ2(C′)2
Eε ≤ < ∞,
n ns
n=0 n=1
we obtain that with probability 1,
(cid:88)∞ (cid:26) (cid:13)√ (cid:13)2 (cid:27)
n−s ·E (cid:13) n(θ(cid:98) −θ∗)(cid:13) I < ∞.
(cid:13) n (cid:13) Dn
n=1
102Combined with (173), we obtain that
(cid:16)(cid:88)∞ (cid:13)√ (cid:13)2 (cid:17)
P n−s(cid:13) n(θ(cid:98) −θ∗)(cid:13) = ∞
(cid:13) n (cid:13)
n=1
(cid:16)(cid:88)∞ (cid:13)√ (cid:13)2 (cid:17) (cid:16)(cid:88)∞ (cid:13)√ (cid:13)2 (cid:17)
≤P n−s(cid:13)
(cid:13)
n(θ(cid:98)
n
−θ∗)(cid:13)
(cid:13)
I
Dn
= ∞ +P n−s(cid:13)
(cid:13)
n(θ(cid:98)
n
−θ∗)(cid:13)
(cid:13)
I
D nc
= ∞
n=1 n=1
(cid:32) (cid:33)
∞ ∞ ∞
(cid:16)(cid:88) (cid:17) (cid:92) (cid:91)
≤0+P I = ∞ = P Dc = 0,
Dn m
n=1 n=1m=n
(cid:13)√ (cid:13)2
that is, (cid:80)∞ n−s(cid:13) n(θ(cid:98) −θ∗)(cid:13) < ∞ with probability 1.
n=1 (cid:13) n (cid:13)
If 0 < t < 2, set s = 1 − t, s = t, s = s−1, p = 1 > 1, and q = 1 > 1. We have
1 2 2 2 0 2 s1 s2
1/p+1/q = 1, and s +s +2s = s. Notice that
1 2 0
(cid:32)
∞
(cid:33)1/p (cid:32)
∞
(cid:33)1/p
(cid:88) (cid:88)
n−(s1+s0)p = n−(1+s0p) < ∞,
n=1 n=1
and with probability 1,
(cid:88)∞ (cid:13)√ (cid:13)tq (cid:88)∞ (cid:13)√ (cid:13)2
n−(s2+s0)q(cid:13) n(θ(cid:98) −θ∗)(cid:13) = n−1−s0q(cid:13) n(θ(cid:98) −θ∗)(cid:13) < ∞.
(cid:13) n (cid:13) (cid:13) n (cid:13)
n=1 n=1
By H¨older’s inequality, with probability 1
(cid:88)∞ (cid:13)√ (cid:13)t
(cid:32)
(cid:88)∞
(cid:33)1/p(cid:32)
(cid:88)∞ (cid:13)√
(cid:13)tq(cid:33)1/q
n−s(cid:13) n(θ(cid:98) −θ∗)(cid:13) ≤ n−(s1+s0)p n−(s2+s0)q(cid:13) n(θ(cid:98) −θ∗)(cid:13) < ∞.
(cid:13) n (cid:13) (cid:13) n (cid:13)
n=1 n=1 n=1
Lemma 14.21. Under Assumptions 1-5 as well as Assumptions 6A-7A (or Assumptions 6B-
7B), if the sequence of estimators θ(cid:98) satisfies that for 0 ≤ β < 1,
n 2
(cid:88) (cid:13) (cid:13)
nβ−1(cid:13)θ(cid:98) −θ∗(cid:13) < ∞ a.s. (174)
(cid:13) n−1 (cid:13)
n≥n0
then the generalized GI0 and GI1 (with θ replaced by θ(cid:98) ) satisfy
n n
nβZ −a. →s. 0,
n
where Z = F (π )−F (π∗).
n θ∗ n θ∗
103Proof of Lemma 14.21. Based on Lemma 14.19, the GI0 and GI1 selection rules satisfy that
there exists C < ∞ such that for any n ≥ n ,
0
(cid:18) (cid:19)
1
E[Z | F ] ≤ 1− Z +c ,
n n−1 n−1 n−1
n
where c =
C(cid:16)
1 +
∥θ(cid:98)n−1−θ∗∥(cid:17)
. Notice that with probability 1, we have
n−1 n2 n
(cid:13) (cid:13)
(cid:88)∞ (cid:88)∞
(cid:16) 1
(cid:13) (cid:13)θ(cid:98)
n−1
−θ∗(cid:13)
(cid:13)(cid:17)
(n−1)βc ≤ C ·nβ +
n−1 n2 n
n=n0+1 n=n0
(cid:88)∞ 1 (cid:88)∞ C (cid:13) (cid:13)
= C · + (cid:13)θ(cid:98) −θ∗(cid:13) < ∞.
n2−β n1−β(cid:13) n−1 (cid:13)
n=n0+1 n=n0+1
Applying the third part of Lemma 14.15 to Z with a = 1/(n+1), c = 1, we obtain
n n
nβZ −a. →s. 0.
n
Proof of Theorem 4.3. By Corollary 14.20, we know that with probability 1, if 0 ≤ β < 1,
2
then with probability 1,
(cid:88)∞ (cid:13) (cid:13) (cid:88)∞ (cid:13)√ (cid:13)
nβ−1(cid:13)θ(cid:98)ML −θ∗(cid:13) = nβ−3/2(cid:13) n(θ(cid:98)ML −θ∗)(cid:13) < ∞.
(cid:13) n (cid:13) (cid:13) n (cid:13)
n=1 n=1
By Lemma 14.21, we obtain that nβZ → 0 a.s. P . That is, lim nβ{F (π )−F (π∗)},
n ∗ n→∞ θ∗ n θ∗
a.s.
Next, we prove by contradiction that, when F (·) has a unique minimizer, we also have
θ∗
lim π = π∗ a.s. Assume, on the contrary, that there exists a sub-sequence such that
n→∞ n
π → π ̸= π∗, as l → ∞. Then, by the continuity of F (·), we have F (π ) → F (π ).
n 1 θ∗ θ∗ n θ∗ 1
l l
Set β = 0. We obtain that
F (π ) → F (π∗) a.s. P .
θ∗ n θ∗ ∗
GiventhatF (π)hasauniqueglobalminimizer, itmustbethecasethatF (π ) ̸= F (π∗).
θ∗ θ∗ 1 θ∗
This contradicts with the above display.
10414.6 Proof of Theorem 4.4
Proof of Theorem 4.4. By applying Theorem 4.2 and Theorem 4.3, we conclude the proof of
Theorem 4.4.
14.7 Proof of Theorem 4.5
Proof of Theorem 4.5. Under the assumptions of Theorem 4.4, the conclusions from Theo-
rem 4.1 and Theorem 4.3 still apply. Hence, we have
(cid:88)
lim Iπn(θ(cid:98)ML) = lim π (a)I (θ(cid:98)ML) = Iπ∗(θ∗) a.s. ,
n n a n
n→∞ n→∞
a∈A
and
(cid:13) (cid:13)
lim (cid:13){Iπn(θ(cid:98)ML)}−1/2∇g(θ(cid:98)ML)(cid:13) =
(cid:13) (cid:13){Iπ∗(θ∗)}−1/2∇g(θ∗)(cid:13)
(cid:13) a.s.
(cid:13) n n (cid:13)
n→∞
By Slutsky’s theorem and Theorem 4.4, we derive the limit result as in (23).
Moreover, through the Delta method, we find
√
n(g(θ(cid:98) nML)−g(θ∗))
−→d
N(0,1).
∥{Iπ∗(θ∗)}−1/2∇g(θ∗)∥
Once again, by Slutsky’s theorem, we establish the limit result in (24).
14.8 Proof of Theorem 4.8
We first provide an extension of the Cram´er-Rao lower bound for unbiased estimators based
on sequential observations following an active experiment selection rule.
Lemma 14.22 (Cram´er-Rao lower bound for sequential data). Assume that for some initial
values a0,··· ,a0 ∈ A, we consider initial selections a = a0 for i = 1,...,n , such that the
1 n0 i i 0
sum
(cid:80)n0
I (θ) is nonsingular for all θ ∈ Θ. Given any deterministic selection function
i=1 a0
i
h , we consider the selections a = h (a ,X ,··· ,a ,X ) ∈ A,∀n > n . Let T =
n n n 1 1 n−1 n−1 0 n
T(X ,X ,··· ,X ,a ) be an unbiased estimator of vector h(θ) with a finite second moment,
1 2 n n
for all θ ∈ Θ, that is h(θ) = E [T ] and sup E ∥T ∥2 < ∞. Then, under Assumptions 1-
θ n θ∈Θ θ n
4, we have
1(cid:110) (cid:111)T
cov (T ) ⪰ ∇ h(θ) {IE θπn(θ)}−1∇ h(θ).
θ n θ θ
n
Specifically, if h(θ) = θ, then
G (ncov (T )) ≥ inf G ({Iπ(θ)}−1).
θ θ n θ
π∈SA
105(cid:104) (cid:105)
Proof of Lemma 14.22. Assume h(θ) ∈ Rl. For any b ∈ Rl, define h (θ) = bTE T .
b θ n
Let {Xa} be a sequence of independent random elements, such that Xa ∼ f (·).
i a∈A,i≥1 i θ,a
According to Lemma 13.8, we can assume that the observations and experiments are
a ,Xa1,··· ,a ,Xan in the rest of the proof, where a = h (a ,Xa1,··· ,a ,Xan), for
1 1 n n n+1 n+1 1 1 n n
all n ≥ n .
0
The joint density for XA = {Xa} and a = (a ,··· ,a ) is given by
n i 1≤i≤n,a∈A n 1 n
n
(cid:89)(cid:89)
f (XA,an) = f (Xa)I(a = a1,··· ,a = an0,a = an0+1,··· ,a = an).
θ n θ,a i 1 n0 n0+1 n
i=1a∈A
Notice that
n
(cid:88)(cid:88)
∇ f (XA,an) = f (XA,an) ∇ logf (Xa).
θ θ n θ n θ θ,a i
i=1 a∈A
Assume that probability density f (·) is with respect to baseline measure µ (·). By As-
θ,a a
sumption 2, denote the support of probability density f (·) by Ω = supp(f ), which does
θ,a a θ,a
not depend on θ. Let product measure dµn(XA) = (cid:81) dµ (Xa), and product space
n 1≤i≤n,a∈A a i
Ω1 = × Ω , Ωn = × Ω ×Ωn−1.
a∈A a a∈A a
Set T = T (a ,Xa1,··· ,a ,Xan). Because
n n 1 1 n n
(cid:90)
(cid:88)
E [bTT ] = bTT (a ,Xa1,··· ,a ,Xan)f (XA,an)dµn(XA),
θ n n 1 1 n n θ n n
an∈An
Ωn
we know that
(cid:90)
(cid:88)
∇ h (θ) = ∇ E [bTT ] = ∇ bTT (a ,Xa1,··· ,a ,Xan)f (XA,an)dµn(XA).
θ b θ θ n θ n 1 1 n n θ n n
an∈An
Ωn
By Assumption 2, we know that for any a ∈ A
∥∇ logf (Xa)∥ ≤ ∥∇ logf (Xa)∥+Ψa(Xa) max ∥θ −θ′∥ =: F (Xa),∀Xa ∈ Ω ,
θ θ,a θ θ∗,a 1 a a
θ,θ′∈Θ
where the dominate function F satisfies that
a
supE {F (Xa)}2 < ∞.
Xa∼f a
θ,a
θ∈Θ
106Notice that
(cid:90)
bTT (a ,Xa1,··· ,a ,Xan)∇ f (XA,an)dµn(XA)
n 1 1 n n θ θ n n
Ωn
(cid:90) n
(cid:88)(cid:88)
= bTT (a ,Xa1,··· ,a ,Xan) ∇ logf (Xa)f (XA,an)dµn(XA),
n 1 1 n n θ θ,a i θ n n
Ωn
i=1 a∈A
(cid:13) (cid:13)
(cid:13) (cid:88)n (cid:88) (cid:13) (cid:88)n (cid:88)
(cid:13)bTT ∇ logf (Xa)(cid:13) ≤ |bTT |· F (Xa),
(cid:13) n θ θ,a i (cid:13) n a i
(cid:13) (cid:13)
i=1 a∈A i=1 a∈A
and by H¨older’s inequality,
n n
(cid:88)(cid:88) (cid:88)(cid:88)(cid:16) (cid:17)1/2
E [|bTT |· F (Xa) ≤ E [(bTT )2]·E [{F (Xa)}2] < ∞
θ n a i θ n θ a i
i=1 a∈A i=1 a∈A
Taking into account that Ωn is independent of θ, and by applying the Dominated Con-
vergence Theorem together with the classical proof of differentiation under the integral sign,
we arrive at
(cid:90)
∇ bTT (a ,Xa1,··· ,a ,Xan)f (XA,an)dµn(XA)
θ n 1 1 n n θ n n
Ωn
(cid:90)
= bTT (a ,Xa1,··· ,a ,Xan)∇ f (XA,an)dµn(XA).
n 1 1 n n θ θ n n
Ωn
In conclusion, we know that
(cid:34) (cid:35)
n
(cid:88)(cid:88)
∇ h (θ) = E bTT ∇ logf (Xa) .
θ b θ n θ θ,a i
i=1 a∈A
Next, we show that
(cid:34) (cid:35)
n
(cid:88) (cid:88)
E bTT ∇ logf (Xa) = 0. (175)
θ n θ θ,a i
i=1 a∈A,a̸=ai
First of all, let F = σ{a ,Xa1,a ,Xa2,··· ,a ,Xai}. Note that a is measurable with
i+1 1 1 2 2 i i i+1
respect to F for all i. Notice that {Xa} are independent of F , as well as {Xa}
i n a∈A n−1 n a∈A,a̸=an
107and Xan are independent, given F . Also recall that a is measurable in F . Thus,
n n−1 n n−1
(cid:104) (cid:88) (cid:12) (cid:105)
E bTT ∇ logf (Xa)(cid:12)F ,Xan
θ n θ θ,a n (cid:12) n−1 n
a∈A,a̸=an
(cid:88) (cid:104) (cid:12) (cid:105)
=bTT E ∇ logf (Xa)(cid:12)F ,Xan
n θ θ θ,a n (cid:12) n−1 n
a∈A,a̸=an
(cid:88) (cid:104) (cid:105)
=bTT E ∇ logf (Xa) = 0.
n θ θ θ,a n
a∈A,a̸=an
Note that for fixed 1 ≤ i < n, {Xa} and F are independent. Define another
j j≥i,a∈A i−1
σ-algebra, G = σ(F ,{Xa} ). Note that a ,a ,··· ,a are measurable in
i−1 i−1 j i+1≤j≤n,a∈A i i+1 n
σ(G ,Xai). Furthermore, {Xa} and Xai are independent, given G . Thus, for
i−1 i i a∈A,a̸=ai i i−1
any 1 ≤ i < n
(cid:104) (cid:88) (cid:12) (cid:105)
E bTT ∇ logf (Xa)(cid:12)G ,Xai
θ n θ θ,a i (cid:12) i−1 i
a∈A,a̸=ai
(cid:88) (cid:104) (cid:12) (cid:105)
=bTT E ∇ logf (Xa)(cid:12)G ,Xai
n θ θ θ,a i (cid:12) i−1 i
a∈A,a̸=ai
(cid:88) (cid:104) (cid:105)
=bTT E ∇ logf (Xa) = 0.
n θ θ θ,a i
a∈A,a̸=ai
By the law of iterated expectation, we have proved (175). Hence, we know that
(cid:34) (cid:35)
n
(cid:88)
∇ h (θ) = E bTT ∇ logf (Xai) = 0.
θ b θ n θ θ,ai i
i=1
Set Y = ∇ (cid:80)n logf (Xai), and we have
n θ i=1 θ,ai i
∇ h (θ) = E [Y TTb] = cov (Y ,bTT ).
θ b θ n n θ n n
By multivariate Cauchy-Schwartz inequality (75), for any b ∈ Rl,
bT cov (T )b
θ n
=var (bTT )
θ n
(cid:16) (cid:17) (cid:16) (cid:17)
≥cov bTT ,Y
(cid:8)
cov (Y
)(cid:9)−1
cov Y ,bTT
θ n n θ n θ n n
=bT(cid:8)
∇
h(θ)(cid:9)T(cid:8)
cov (Y
)(cid:9)−1
∇ h(θ)b.
θ θ n θ
108Note that
(cid:110) (cid:111)
E [Y {∇ logf (Xai)}T] = E Y ·E (cid:2) {∇ logf (Xai)}T|F (cid:3) = 0.
θ i−1 θ θ,ai i θ i−1 θ θ θ,ai i i−1
Thus
cov (Y ) = E [Y Y T] = E [Y Y T ]+E [I (θ)] = n·IE θπn(θ).
θ n θ n n θ n−1 n−1 θ ai
In conclusion, for any b ∈ Rl, we obtain that
(cid:20) (cid:21)
1(cid:110) (cid:111)T(cid:110) (cid:111)−1
bT cov (T )b = var (bTT ) ≥ bT ∇ h(θ) IE θπn(θ) ∇ h(θ) b.
θ n θ n θ θ
n
This implies that
1
cov (T ) ⪰ {∇ h(θ)}T{IE θπn(θ)}−1∇ h(θ).
θ n θ θ
n
If h(θ) = θ, we know that
ncov (T ) ⪰ {IE θπn(θ)}−1.
θ n
By assumption 5, we obtain
G (ncov (T )) ≥ G ({IE θ[πn](θ)}−1) ≥ inf G ({Iπ(θ)}−1).
θ θ n θ θ
π∈SA
Proof of Theorem 4.8.
Part 1 Notice that L(θ∗,θ(cid:98)) is a loss function, which means that L(θ∗,θ(cid:98)) ≥ L(θ∗,θ∗) = 0.
Due to L(θ∗,θ(cid:98)) is differentiable in θ(cid:98), we know that ∇ L(θ∗,θ∗) = 0.
θ(cid:98)
Applying first order Taylor expansion to L(θ∗,θ(cid:98)) with respect to θ(cid:98), we obtain that
(cid:28) (cid:29)
1 (cid:12)
L(θ∗,T ) = ∇2L(θ∗,θ(cid:98))(cid:12) (θ∗ −T ),θ∗ −T ≥ η∥θ∗ −T ∥2, (176)
n 2 θ(cid:98) (cid:12)
θ(cid:98)=θ(cid:101)n
n n n
where θ(cid:101) = t θ∗ +(1−t )T for some t ∈ (0,1). Thus,
n n n n n
√
E θ∗n·L(θ∗,T n) ≥ ηE θ∗(cid:13) (cid:13) n(T
n
−θ∗)(cid:13) (cid:13)2 .
To show (26), without loss of generality, we assume that
√
limsupE θ∗(cid:13) (cid:13) n(T
n
−θ∗)(cid:13) (cid:13)2 < ∞.
n→∞
109P
ThisimpliesT →∗ θ∗. ItalsoimpliesthatT hasfinitesecondmoment, and, thus, conditions
n n
of Lemma 14.22 are satisfied. By Lemma 14.22, we have cov (T ) ⪰ 1{IEπn(θ)}−1.
(cid:68) (cid:69) θ n n
If L(θ∗,θ(cid:98)) ≡ H θ∗(θ∗ −θ(cid:98)),θ∗ −θ(cid:98) , we obtain
E [nL(θ∗,T )] = n⟨H ,cov (T )⟩ ≥ tr(H {IEπn(θ∗)}−1).
θ∗ n θ∗ θ∗ n θ∗
(cid:68) (cid:69)
If L(θ∗,θ(cid:98)) ̸≡ H θ∗(θ∗ −θ(cid:98)),θ∗ −θ(cid:98) , under the theorem’s assumption
limsupE n∥T −θ∗∥2I(∥T −θ∗∥ > ε) = 0.
θ∗ n n
n→∞
√
(cid:0) (cid:1)
Define V = n θ∗ −T , and its truncation V M = V I(∥V ∥ ≤ M). Define
n n n n n
1 (cid:12)
H(θ∗,T ) = ∇2L(θ∗,θ(cid:98))(cid:12) ,
n 2 θ(cid:98) (cid:12)
θ(cid:98)=θ(cid:101)n
whereθ(cid:101) = t θ∗+(1−t )T . Accordingto(176),L(θ∗,T ) = ⟨H(θ∗,T )(θ∗ −T ),θ∗ −T ⟩.
n n n n n n n n
Furthermore, for any ε > 0,
(cid:12) (cid:12)
E (cid:12)n·L(θ∗,T )−⟨H V ,V ⟩(cid:12)I(∥T −θ∗∥ ≤ ε)
θ∗(cid:12) n θ∗ n n (cid:12) n
(cid:13) (cid:13)
≤ max (cid:13) (cid:13)H(θ∗,θ(cid:98))−H θ∗(cid:13)
(cid:13)
E θ∗∥V n∥2
∥θ(cid:98)−θ∗∥≤ε op
=o(1).
Now, we obtain that
(cid:104) (cid:105) (cid:104) √ (cid:105)
E n·L(θ∗,T ) ≥ E n·L(θ∗,T )·I(∥V ∥ ≤ ε n)
θ∗ n θ∗ n n
(cid:68) √ √ (cid:69) (cid:13) (cid:13)
≥ E
θ∗
H θ∗V nε n,V nε n − max (cid:13) (cid:13)H(θ∗,θ(cid:98))−H θ∗(cid:13)
(cid:13)
E θ∗∥V n∥2
∥θ(cid:98)−θ∗∥≤ε op
= E ⟨H V ,V ⟩−E n∥T −θ∗∥2I(∥T −θ∗∥ > ε)−o(1)
θ∗ θ∗ n n θ∗ n n
≥ min tr(H {Iπ(θ∗)}−1)−E n∥T −θ∗∥2I(∥T −θ∗∥ > ε)−o(1),
θ∗ θ∗ n n
π∈SA
where the last inequality is due to Lemma 14.22. Taking the inferior limit as n → ∞ and
then taking the inferior limit as ε → 0+, we obtain
(cid:104) (cid:105)
liminfE n·L(θ∗,T ) ≥ min tr(H {Iπ(θ∗)}−1).
θ∗ n θ∗
n→∞ π∈SA
The ‘in particular’ part is proved by noting that H = I in this case.
θ∗ p
110Part 2 Recall the log-likelihood defined in 4. Because f (·) = h (·), we obtain that
θ,a ξa,a
n
1 (cid:88) (cid:88)
−∇2l (θ;a ) = − ZT∇2 logh (X )Z ⪰ α π (a)ZTZ , (177)
θ n n n ai ξai ξai,ai i ai n a a
i=1 a∈A
and I (ξ ) = −E ∇2 logh (Xa) ⪰ αI, where ξ = Z θ.
ξa,a a Xa∼h ξa,a ξa ξa,a a a
Under Assumption 6A, we obtain I (θ) = ZTI (ξ )Z and
a a ξa,a a a
∇2 logf (Xa) = ZT∇2 logh (Xa)Z ,
θ θ,a a ξa ξa,a a
and
(cid:88) (cid:88)
Iπ(θ∗) = π(a)ZTI (ξ∗)Z ⪰ α π(a)ZTZ
a ξa,a a a a a
a∈A a∈A
is a positive definite matrix.
Applying the first order Taylor expansion of L(θ∗,θ(cid:98)) over θ(cid:98), we obtain that
(cid:28) (cid:29)
1 (cid:12) (cid:13) (cid:13)2
L(θ∗,θ(cid:98) ) = ∇2L(θ∗,θ(cid:98))(cid:12) (θ∗ −θ(cid:98) ),θ∗ −θ(cid:98) ≤ η′(cid:13)θ∗ −θ(cid:98) (cid:13) , (178)
n 2 θ(cid:98) (cid:12)
θ(cid:98)=θ(cid:101)n
n n (cid:13) n(cid:13)
where θ(cid:101) = t θ∗+(1−t )θ(cid:98) for some t ∈ (0,1). Recall that G (Σ) = tr(H Σ). Note that
n n n n n θ θ
∇G (Σ) = H and κ(H ) ≤ η′ < ∞, which implies that G (Σ) satisfies Assumption 5.
θ θ θ η θ
By Theorem 4.3, π →P ∗ π∗ = argmin F (π) and Iπ∗(θ) is nonsingular for any
n π∈SA θ∗
θ ∈ Θ, applying Theorem 4.2, we obtain
√ (cid:16) (cid:17)
n(θ(cid:98)ML −θ∗) →d N 0,(cid:8) Iπ∗(θ∗)(cid:9)−1 as n → ∞. (179)
n p
Notice that for any n ≥ n , by Lemma 14.1 and Assumption 6B, there exists C > 0 such
0
that
(cid:88) (cid:16) (cid:17)
−∇2l (θ) ⪰ α π (a)ZTZ ⪰ α inf λ π (a)ZTZ I ⪰ 2CI .
θ n n a a min n a a p p
n≥n0
a∈A
By Taylor expansion, we obtain
(cid:68) (cid:69) (cid:13) (cid:13)2
0 ≤ l (θ(cid:98)ML;a )−l (θ∗;a ) ≤ ∇ l (θ∗;a ),θ(cid:98)ML −θ∗ −C(cid:13)θ(cid:98)ML −θ∗(cid:13) .
n n n n n θ n n n (cid:13) n (cid:13)
Thus,
(cid:13) (cid:13) 1
(cid:13)θ(cid:98)ML −θ∗(cid:13) ≤ ∥∇ l (θ∗;a )∥.
(cid:13) n (cid:13) C θ n n
111By Theorem 6.2 in DasGupta (2008), to show that
E
θ∗(cid:104) n(cid:13)
(cid:13) (cid:13)θ(cid:98) nML
−θ∗(cid:13)
(cid:13)
(cid:13)2(cid:105)
→ tr((cid:8) Iπ(θ∗)(cid:9)−1 ),
combined with (179), it suffices to show that
(cid:16)√ (cid:13) (cid:13)(cid:17)2+δ
limsupE
θ∗
n(cid:13) (cid:13)θ(cid:98) nML −θ∗(cid:13)
(cid:13)
< ∞.
n→∞
Note that
E θ∗(cid:16)√ n(cid:13) (cid:13) (cid:13)θ(cid:98) nML −θ∗(cid:13) (cid:13) (cid:13)(cid:17)2+δ ≤ (C)1 1+δE θ∗(cid:13) (cid:13)√ n∇ θl n(θ∗;a n)(cid:13) (cid:13)2+δ .
By classical c -inequality (see Chapter 9 of Lin (2010)), we have
r
E θ∗(cid:13) (cid:13)√ n∇ θl n(θ∗;a n)(cid:13) (cid:13)2+δ ≤ pδ/2(cid:88)p E θ∗(cid:12) (cid:12) (cid:12)(cid:88)n √1 neT
j
∇ θlogf θ∗,ai(X i)(cid:12) (cid:12) (cid:12)2+δ .
j=1 i=1
Since (cid:80)n eT∇ logf (X ) is a martingale, applying inequality (45) in Lin (2010), we
i=1 j θ θ∗,ai i
obtain
(cid:12)(cid:88)n 1 (cid:12)2+δ (cid:88)n (cid:12) 1 (cid:12)2+δ
E (cid:12) √ eT∇ logf (X )(cid:12) ≤ C ·nδ/2 E (cid:12)√ eT∇ logf (X )(cid:12)
θ∗(cid:12) n j θ θ∗,ai i (cid:12) 2+δ θ∗(cid:12) n j θ θ∗,ai i (cid:12)
i=1 i=1
(cid:88)
≤ C E ∥∇ logf (Xa)∥2+δ
2+δ Xa∼f θ∗,a θ θ∗,a
a∈A
≤ C 2+δ(cid:88) E
Xa∼f
θ∗,a(cid:13) (cid:13)∇ θlogh
ξ
a∗,a(Xa)(cid:13) (cid:13)2+δ ∥Z a∥2 op+δ.
a∈A
In conclusion, we obtain
(cid:16)√ (cid:13) (cid:13)(cid:17)2+δ
sup E
θ∗
n(cid:13) (cid:13)θ(cid:98) nML −θ∗(cid:13)
(cid:13)
< ∞. (180)
n≥n0
P
Notice that as θ(cid:98) →∗ θ∗, we know that
n
1 (cid:12) P
2∇2 θ(cid:98)L(θ∗,θ(cid:98))(cid:12)
(cid:12)
θ(cid:98)=θ(cid:101)n
→∗ H θ∗.
Thus, we obtain that
(cid:68) (cid:69)
nL(θ∗,θ(cid:98) nML) = n H θ∗(θ∗ −θ(cid:98) nML),θ∗ −θ(cid:98) nML +o p(1). (181)
112By (180) and (178), we obtain that
(cid:104) (cid:105)1+δ/2 (cid:16)√ (cid:13) (cid:13)(cid:17)2+δ
sup E
θ∗
nL(θ∗,θ(cid:98) nML) ≤ (η′)1+δ/2 sup E
θ∗
n(cid:13) (cid:13)θ(cid:98) nML −θ∗(cid:13)
(cid:13)
< ∞.
n≥n0 n≥n0
Applying Theorem 6.2 in DasGupta (2008), we obtain that as n → ∞,
(cid:104) (cid:105)
E
θ∗
nL(θ∗,θ(cid:98) nML) → E⟨H θ∗V ,V ⟩ = tr(H θ∗{Iπ(θ∗)}−1),V ∼ N p(0 p,{Iπ(θ∗)}−1).
Applying Theorem 4.7, the proof of the second part of Theorem 4.8 is completed.
14.9 Proof of Theorem 4.9
Proof of Theorem 4.9. The proof of Theorem 4.9 is similar to that of Theorem 8.8 and
Theorem 8.11 in Van der Vaart (2000). Thus, we will only state the main differences and
omit the repetitive details.
For proving the first part of the theorem, we follow the proof of Theorem 8.8 in Van der
Vaart (2000). We need to verify Theorem 8.3, Theorem 7.10, as well as Proposition 8.4, as
presented in Van der Vaart (2000), under our sequential setting.
Forprovingthesecondpartofthetheorem,wefollowtheproofofTheorem8.11inVander
Vaart (2000). It is sufficient to modify and prove Theorem 7.2 and Proposition 8.6, as
presented in Van der Vaart (2000), under our sequential setting.
Below we verify the above mentioned results in our context.
Differentiable in quadratic mean We need to show that densities {f (·)} are dif-
θ,a a∈A
ferentiable in quadratic mean at θ, which means that
(cid:90) (cid:104)(cid:113) (cid:113) 1 (cid:113) (cid:105)2
f (x)− f (x)− hT∇ logf (x) f (x) dµ(x) = o(∥h∥2). (182)
θ+h,a θ,a θ θ,a θ,a
2
By applying the regularity conditions and using Lemma 7.6 from Van der Vaart (2000), we
have completed the proof of (182) for any a ∈ A and θ, where θ is an interior point of Θ.
Modified Theorem 7.2 in Van der Vaart (2000) We modified Theorem 7.2 in
Van der Vaart (2000) in our context as follows. Let P denote the joint distribution of
n,θ
(a ,X ,··· ,a ,X ) following some experiment selection rule with the empirical selection
1 1 n n
113proportion π . Then, given that h = h+o(1),
n n
P √
log
n,θ+hn/ n
(a ,X ,··· ,a ,X )
1 1 n n
P
n,θ
∼log(cid:89)n f θ+hn/√ n,aj(X jaj)
f
(Xaj) (183)
j=1 θ,aj j
n
1 (cid:88) 1
=√ hT∇ logf (Xaj)− hTIπ(θ)h+o (1)
n θ θ,aj j 2 p
j=1
where {Xa} , where Xa ∼ f (·) are independent random variables, ‘∼’ means that
j a∈A,j≥1 j θ,a
random variables on both sides share the same distribution, the second line is due to
Lemma 13.8, and the last line is obtained following a similar proof as that of Theorem
7.2 in Van der Vaart (2000), which is detailed below.
By Assumptions 1-4, the Dominated Convergence Theorem, and the proof of the classical
differentiation under the integral sign, we arrive at
n
(cid:104)(cid:88) (cid:105)
E[∇ logf (Xaj)|F ] = 0 and E ∇ logf (Xaj) = 0.
θ θ,aj j j−1 θ θ,aj j
j=1
The proof of the first Equation (7.3) in Van der Vaart (2000) needs to be modified as follows.
(cid:115)
Let W = 2(cid:16) f θ+hn/√ n,aj(X jaj) −1(cid:17) and V = (cid:80)n W −√1 hT (cid:80)n ∇ logf (Xaj). We
nj f (Xaj) n j=1 nj n j=1 θ θ,aj j
θ,aj j
know that
(cid:16) (cid:17)
var V
n
(cid:16) (cid:17) (cid:16) 1 (cid:17) (cid:16) 1 (cid:17)
=var V +var W − √ hT∇ logf (Xan) +2cov V ,W − √ hT∇ logf (Xan)
n−1 nn n θ θ,an n n−1 nn n θ θ,an n
and
(cid:16) 1 (cid:17)
cov V ,W − √ hT∇ logf (Xan)
n−1 nn n θ θ,an n
(cid:104) 1 1 (cid:105)
=E (cid:0) V −EV (cid:1)(cid:0) W − √ hT∇ logf (Xan)−E(cid:0) W − √ hT∇ logf (Xan)(cid:1)(cid:1)
n−1 n−1 nn n θ θ,an n nn n θ θ,an n
(cid:110) (cid:104) 1 1 (cid:12) (cid:105)(cid:111)
=E (cid:0) V −EV (cid:1)E (cid:0) W − √ hT∇ logf (Xan)−E(cid:0) W − √ hT∇ logf (Xan)(cid:1)(cid:1) (cid:12)F
n−1 n−1 nn n θ θ,an n nn n θ θ,an n (cid:12) n−1
=0.
114By induction and (182), we obtain that as n → ∞,
n
(cid:16) (cid:17) (cid:88) (cid:16) 1 (cid:17)
var V = var W − √ hT∇ logf (Xaj)
n nj n θ θ,aj j
j=1
n
(cid:88) (cid:16) 1 (cid:17)2
≤ E W − √ hT∇ logf (Xaj)
nj n θ θ,aj j
j=1
(cid:115)
≤(cid:88)n (cid:88) E(cid:16) 2(cid:16) f θ+hn/√ n,a(X ja) −1(cid:17)
−
√1
hT∇ logf
(Xa)(cid:17)2
f (Xa) n θ θ,a j
j=1 a∈A θ,a j
(cid:88)(cid:90) (cid:104)(cid:113) (cid:113) 1 (cid:113) (cid:105)2
≤8n f √ (x)− f (x)− √ hT∇ logf (x) f (x) dµ(x)
θ+hn/ n,a θ,a 2 n n θ θ,a θ,a
a∈A
(cid:88)
+2(h−h )T I (θ)(h−h )
n a n
a∈A
(cid:88)
=8o(∥h∥2)+2(h−h )T I (θ)(h−h ) → 0.
n a n
a∈A
Because of (182), we obtain that
(cid:12) (cid:12)
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:12) (cid:13)(cid:113) (cid:113) (cid:13) (cid:13) 1 (cid:113) (cid:13) (cid:12)
(cid:12) (cid:13) f √ (x)− f (x)(cid:13) −(cid:13) √ hT∇ logf (x) f (x)(cid:13) (cid:12)
(cid:12) (cid:13) θ+hn/ n,a θ,a (cid:13) (cid:13)2 n n θ θ,a θ,a (cid:13) (cid:12)
(cid:12) L2(µ) L2(µ)(cid:12)
(cid:16)(cid:90) (cid:104)(cid:113) (cid:113) 1 (cid:113) (cid:105)2 (cid:17)1/2 (cid:16)∥h∥(cid:17)
≤ f √ (x)− f (x)− √ hT∇ logf (x) f (x) dµ(x) = o √ .
θ+hn/ n,a θ,a 2 n n θ θ,a θ,a n
Note that
(cid:13) (cid:13) 1 (cid:113) (cid:13) (cid:13)2 1 (cid:0)∥h∥2 (cid:1)
(cid:13) √ hT∇ logf (x) f (x)(cid:13) = hTI (θ)h = O .
(cid:13)2 n n θ θ,a θ,a (cid:13) 4n n a n n
L2(µ)
(cid:12) (cid:12) (cid:12) (cid:12)
By inequality |x2 −y2| ≤ |x−y|(cid:12)|x|+|y|(cid:12) ≤ |x−y|(cid:12)2|x|+|x−y|(cid:12), we obtain
(cid:12) (cid:12)
(cid:12)(cid:13) (cid:13)(cid:113) (cid:113) (cid:13) (cid:13)2 1 (cid:12)
(cid:12) (cid:13) f √ (x)− f (x)(cid:13) − hTI (θ)h (cid:12)
(cid:12) (cid:13) θ+hn/ n,a θ,a (cid:13) 4n n a n(cid:12)
(cid:12) L2(µ) (cid:12)
(cid:16)∥h∥(cid:17)(cid:12) (cid:12) ∥h∥ (cid:16)∥h∥(cid:17)(cid:12) (cid:12) (cid:16)∥h∥2(cid:17)
≤o √ (cid:12)2·O(√ )+o √ (cid:12) = o .
n (cid:12) n n (cid:12) n
115Thus, the second Equation (7.3) in Van der Vaart (2000) is modified by
(cid:16)(cid:90) (cid:113) (cid:17)
E[W |F ] = 2 f √ (x)f (x)dµ(x)−1
nj j−1 θ+hn/ n,aj θ,aj
(cid:90) (cid:104)(cid:113) (cid:113) (cid:105)2 1 1
=− f √ (x)− f (x) dµ(x) = − hTI (θ)h +o( ) (184)
θ+hn/ n,aj θ,aj 4n n aj n n
1 1
=− hTI (θ)h+ o(1),
4n
aj
n
where the o(1) converges uniformly to 0 as n → ∞. Now, we obtain that
n
(cid:88) 1 1
Eπ = π +o(1), and E W = − hTIEπn(θ)h+o(1) = − hTIπ(θ)h+o(1).
n nj
4 4
j=1
We define
(cid:16) (cid:17)2
A = nW2 − hT∇ logf (Xai)
ni ni θ θ,ai i
and
(cid:115)
(cid:88)(cid:12) (cid:16) f √ (Xa) (cid:17)2 (cid:16) (cid:17)2(cid:12)
A′ = (cid:12)4n θ+hn/ n,a i −1 − hT∇ logf (Xa) (cid:12).
ni (cid:12) f (Xa) θ θ,a i (cid:12)
a∈A θ,a i
Notice that
(cid:115)
(cid:88)(cid:12) (cid:16) f √ (Xa) (cid:17)2 (cid:16) (cid:17)2(cid:12)
A′ = (cid:12)4n θ+hn/ n,a i −1 − hT∇ logf (Xa) (cid:12)
ni (cid:12) f (Xa) θ θ,a i (cid:12)
a∈A θ,a i
(cid:115)
(cid:88)(cid:12) √ (cid:16) f √ (Xa) (cid:17) (cid:16) (cid:17)(cid:12)
≤ (cid:12)2 n θ+hn/ n,a i −1 − hT∇ logf (Xa) (cid:12)·
(cid:12) f (Xa) θ θ,a i (cid:12)
a∈A θ,a i
(cid:115)
(cid:12) √ (cid:16) f √ (Xa) (cid:17) (cid:16) (cid:17)(cid:12)
(cid:12)2 n θ+hn/ n,a i −1 + hT∇ logf (Xa) (cid:12)
(cid:12) f (Xa) θ θ,a i (cid:12)
θ,a i
By H¨older’s inequality and the definition of differentiable in quadratic mean at θ, we obtain
116that
(cid:115)
(cid:88)(cid:16) (cid:12) √ (cid:16) f √ (Xa) (cid:17) (cid:16) (cid:17)(cid:12)2(cid:17)1/2
E|A′ | ≤ E(cid:12)2 n θ+hn/ n,a i −1 − hT∇ logf (Xa) (cid:12) ·
ni (cid:12) f (Xa) θ θ,a i (cid:12)
a∈A θ,a i
(cid:115)
(cid:16) (cid:12) √ (cid:16) f √ (Xa) (cid:17) (cid:16) (cid:17)(cid:12)2(cid:17)1/2
E(cid:12)2 n θ+hn/ n,a i −1 + hT∇ logf (Xai) (cid:12)
(cid:12) f (Xa) θ θ,ai i (cid:12)
θ,a i
(cid:115)
(cid:88)(cid:16) (cid:12) √ (cid:16) f √ (Xa) (cid:17) (cid:16) (cid:17)(cid:12)2(cid:17)1/2
≤ E(cid:12)2 n θ+hn/ n,a i −1 − hT∇ logf (Xa) (cid:12) ·
(cid:12) f (Xa) θ θ,a i (cid:12)
a∈A θ,a i
(cid:115)
(cid:104)(cid:16) (cid:12) √ (cid:16) f √ (Xa) (cid:17) (cid:12)2(cid:17)1/2
E(cid:12)2 n θ+hn/ n,a i −1 −hT∇ logf (Xa)(cid:12)
(cid:12) f (Xa) θ θ,a i (cid:12)
θ,a i
(cid:16) (cid:12)(cid:16) (cid:17)(cid:12)2(cid:17)1/2(cid:105)
+2 E(cid:12) hT∇ logf (Xa) (cid:12) .
(cid:12) θ θ,a i (cid:12)
Due to
(cid:115)
(cid:16) (cid:12) √ (cid:16) f √ (Xa) (cid:17) (cid:16) (cid:17)(cid:12)2(cid:17)1/2
E(cid:12)2 n θ+hn/ n,a i −1 − hT∇ logf (Xa) (cid:12)
(cid:12) f (Xa) θ θ,a i (cid:12)
θ,a i
(cid:115)
(cid:16) (cid:12) √ (cid:16) f √ (Xa) (cid:17) (cid:16) (cid:17)(cid:12)2(cid:17)1/2
≤ E(cid:12)2 n θ+hn/ n,a i −1 − hT∇ logf (Xa) (cid:12)
(cid:12) f (Xa) n θ θ,a i (cid:12)
θ,a i
(cid:16) (cid:12)(cid:16) (cid:17)T (cid:12)2(cid:17)1/2
+ E(cid:12) h −h ∇ logf (Xa)(cid:12)
(cid:12) n θ θ,a i (cid:12)
(cid:16) (cid:17)1/2
=o(∥h ∥)+ (h−h )TI (θ)(h−h ) = o(1),
n n a n
we obtain that
E|A′ | = (cid:88) o(1)(cid:16) o(1)+2(cid:0) hTI (θ)h(cid:1)1/2(cid:17) = o(1).
ni a
a∈A
Because |A | ≤ A′ , we know that E|A | → 0 and E1 (cid:80)n |A | → 0 as n → ∞. By
ni ni ni n i=1 ni
Lemma 13.2 and (29), we know that
n n n
(cid:88) 1 (cid:88)(cid:16) (cid:17)2 1 (cid:88) P
W2 = hT∇ logf (Xai) + A →θ hTIπ(θ)h.
ni n θ θ,ai i n ni
i=1 i=1 i=1
117By triangle inequality and Markov’s inequality, as n → ∞,
√ √
P(max |W | > ε 2) ≤ nP(|W | > ε 2)
ni ni
1≤i≤n
(cid:16) (cid:17) (cid:16) (cid:17)
≤nP (cid:0) hT∇ logf (Xai)(cid:1)2 > nε2 +nP |A | > nε2
θ θ,ai i ni
≤nP(cid:16)(cid:88)(cid:0)
hT∇ logf
(Xa)(cid:1)2
>
nε2(cid:17) +nP(cid:16)
|A′ | >
nε2(cid:17)
θ θ,a i ni
a∈A
≤ 1 E(cid:88)(cid:0) hT∇ logf (Xa)(cid:1)2 I(cid:16)(cid:88)(cid:0) hT∇ logf (Xa)(cid:1)2 > nε2(cid:17) + EA′ ni
ε2 θ θ,a i θ θ,a i ε2
a∈A a∈A
→0.
Based on the rest of the proof of Theorem 7.2 in Van der Vaart (2000), we complete the
proof of modified Theorem 7.2.
Modified Theorem 7.10 in Van der Vaart (2000) . Themodifiedtheoremisasfollows:
if statistics T = T (a ,Xa1,··· ,a ,Xan) satisfies the limit results in (29) under every h,
n n 1 1 n n
then there exists a randomized statistic T in the experiment {N (h,{Iπ(θ)}−1) : h ∈ Rp}
p
such that T
⇝h
T for every h.
n
The proof mostly follows that of Theorem 7.10 in Van der Vaart (2000) with the following
modifications. Without loss of generality, let
n
1 (cid:88)
P = P √ (a ,Xa1,··· ,a ,Xan),J = Iπ(θ),∆ = √ ∇ logf (Xaj).
n,h n,θ+h/ n 1 1 n n n n θ θ,aj j
j=1
There exists random vector (S,∆) such that
(T ,∆ )
⇝0
(S,∆).
n n
Applying the modified Theorem 7.2 and follow similar arguments as those in the proof
of Theorem 7.10 in Van der Vaart (2000), we obtain
(cid:18) (cid:19) (cid:18) (cid:19)
dP 1
T ,log n,h ⇝0 S,hT∆− hTJh .
n
dP 2
n,0
The rest of the proof remains unchanged.
Modified Theorem 8.3 in Van der Vaart (2000) With a similar proof, the conclusion
in Theorem 8.3 in Van der Vaart (2000) is modified as follows. If the limit results in (29)
hold, then there exists a randomized statistic T in {N (h,{Iπ(θ)}−1) : h ∈ Rp} such that
p
118T −h has the distribution Lπ for every h.
θ
Proposition 8.4 in Van der Vaart (2000) Thispropositiondirectlyapplytooursetting
and does not required to be changed.
Withtheabovemodifications,wefollowasimilarproofasthatforTheorem8.8inVander
Vaart (2000), we obtain (30) as well as the first part of the theorem.
Proposition 8.6 in Van der Vaart (2000) This proposition directly applies to our
problem and does not need to be modified.
Following the proof of Theorem 8.11 Van der Vaart (2000) with the above modifications,
we complete the proof of (31) and the second part of the theorem.
14.10 Proof of Theorem 4.10
Proof of Theorem 4.10. By Theorem 4.1, we know that
lim θ(cid:98)ML = θ∗,a.s.
n
n→∞
Because lim τ = ∞ a.s., we obtain that
n→∞ n
lim θ(cid:98)ML = θ∗,a.s.
n→∞
τn
14.11 Proof of Theorem 4.11
We prove the theorem for a class more general stopping rules instead. We first define a
deterministic stopping rule
(cid:110) 1 (cid:111)
τ(Γ ,c,θ,π) = min m ≥ n ; Γ ({Iπ(θ)}−1) ≤ c ,
θ∗ 0 θ∗
m
whereΓ isacontinuousfunctionthatmapsapositivedefinitematrixtoapositivenumber, c
θ
is a positive number, θ ∈ Θ, and π ∈ SA. Note that τ(Γ,c,θ,π) = max{⌈Γ({Iπ(θ)}−1)⌉,n },
c 0
where ⌈·⌉ is the ceiling function.
Consider a class of functions {Γ } , such that for any 0 < u < u < ∞,
θ θ∈Θ 1 2
lim max |Γ (Σ)−Γ (Σ)| = 0, and min min Γ (Σ) > 0. (185)
θ θ∗ θ
θ→θ∗u1I⪯Σ⪯u2I θ∈Θu1I⪯Σ⪯u2I
119Define a random stopping time
(cid:110) 1 (cid:111)
τ = min m ≥ n ; Γ ({Iπm(θ(cid:98) )}−1) ≤ c , (186)
c 0 m θ(cid:98)m m
whereθ(cid:98) isanestimatorofθ basedonmobservations. Later, wewillshowthatthestopping
m
rules considered in Theorem 4.11 are special cases of the general stopping rule defined above.
The following theorem generalizes Theorem 4.11.
Theorem 14.23 (General result for Asymptotic normality with stopping time). Let θ(cid:98)ML
n
be the MLE following the experiment selection rule GI0 or GI1, as described in Algorithm 1
and Algorithm 2. Assume that F (π) has a unique minimizer π∗. Let {c } be a positive
θ∗ n n≥0
decreasing sequence such that c → 0 as n → ∞. Consider the stopping time τ given by
n cn
(186), where Γ satisfies (185). Then,
θ
√ (cid:110) (cid:111)1/2 (cid:16) (cid:17)
τ
n
Iπτcn(θ(cid:98) τM cnL) (θ(cid:98) τM cnL −θ∗) →d N
p
0 p,I
p
. (187)
Furthermore, for any continuously differentiable function g : Θ → R such that ∇g(θ∗) ̸= 0,
√
τ (g(θ(cid:98)ML)−g(θ∗)) (cid:16) (cid:17)
cn τcn →d
N 0,1 . (188)
(cid:13) (cid:13)
(cid:13)(cid:110) (cid:111)−1/2
(cid:13)
(cid:13)
(cid:13)
Iπτcn(θ(cid:98) τM cnL) ∇g(θ(cid:98) τM cnL)(cid:13)
(cid:13)
Given the above generalized theorem, the proof of 4.11 is provided below. The proof of
Theorem 14.23 is provided later in this section.
Proof of Theorem 4.11. Note that τ(1) and τ(2) can be rewritten as
c c
1
τ(1) = min{m ≥ n ;
Γ(1)(cid:0)
{I(θ(cid:98)ML;a
)}−1(cid:1)
≤ c2}
c 0 m θ τn m
1
τ(2) = min{m ≥ n ;
Γ(1)(cid:0)
{I(θ(cid:98)ML;a
)}−1(cid:1)
≤ c},
c 0 m θ τn m
where
(cid:16) (cid:17)
Γ(1)(Σ) = tr {∇h(θ)}TΣh(θ) ,Γ(2)(Σ) = tr(cid:0) Σ(cid:1) ,
θ θ
Both Γ(l)(Σ) (l = 1,2) are continuously differentiable in θ and Σ so the first part of (185)
θ
is satisfied. The second part of (185) is satisfied for Γ(2) is straightforward. For Γ(1)(Σ),
θ
the second part of (185) is satisfied due to the assumption that ∇h(θ) ̸= 0 for all θ.
Thus, conditions of Theorem 14.23 are satisfied, and the proof is completed by applying this
theorem.
120Intherestofthesection, wepresenttheproofofTheorem14.23. Roughly, Theorem14.23
is proved by combining the following lemma, compares the random and deterministic stop-
ping times, with the multivariate Anscombe’s theorem (Lemma 13.3).
Lemma 14.24. Assume a family of function Γ satisfies (185). Assume there exists con-
θ
stants 0 < u < u < ∞ such that for any n ≥ n and θ ∈ Θ,
1 2 0
u I ⪯ Iπn(θ) ⪯ u I. (189)
1 2
If as n → ∞,
c → 0,c ≥ c > 0,
n n n+1
θ(cid:98) → θ∗ a.s. P , and (190)
n ∗
π → π a.s. P ,
n ∗
where Iπ(θ∗) is nonsingular. Then, τ < ∞ a.s. P and as n → ∞,
cn ∗
τ
τ(Γ ,c ,θ∗,π) → ∞, τ → ∞, and cn → 1, a.s. P . (191)
θ∗ n cn τ(Γ ,c ,θ∗,π) ∗
θ∗ n
Proof of Lemma 14.24. By Theorem (14.1) and equation (78), we know that there exists
0 < c < c < ∞ such that
cI ⪯ Iπm(θ(cid:98) ) ⪯ cI ,
p n p
for all m ≥ n . By assumption (185), there exists 0 < v < v < ∞ such that for any n ≥ n ,
0 1 2 0
v ≤ Γ ({Iπm(θ(cid:98) )}−1) ≤ v .
1 θ(cid:98)m m 2
Note that τ(Γ ,c ,θ∗,π) = max{⌈Γ θ∗({Iπ(θ∗)}−1)⌉,n } → ∞, as n → ∞. Also note that
θ∗ n cn 0
(cid:110) v (cid:111) (cid:110) 1 (cid:111) (cid:110) v (cid:111)
m ≥ n ; 2 ≤ c ⊂ m ≥ n ; Γ ({Iπm(θ(cid:98) )}−1) ≤ c ⊂ m ≥ n ; 1 ≤ c .
0 m n 0 m θ(cid:98)m m n 0 m n
Thus, for any fixed n,
(cid:110) v (cid:111) (cid:108)v (cid:109)
τ ≤ min m ≥ n ; 2 ≤ c ≤ 2 +n < ∞,a.s. P ,
cn 0
m
n
c
0 ∗
n
and as n → ∞,
(cid:110) v (cid:111) (cid:108)v (cid:109)
1 1
τ ≥ min m ≥ n ; ≤ c ≥ → ∞.
cn 0
m
n
c
n
121By assumption (190), we know that
lim{Iπn(θ(cid:98) )}−1 = {Iπ(θ∗)}−1 a.s. P .
n ∗
n→∞
Combining the compact convergence assumption (185) and (189), we obtain that
nl →im ∞Γ θ(cid:98)n(cid:0) {Iπn(θ(cid:98) n)}−1(cid:1) = Γ θ∗(cid:0) {Iπ(θ∗)}−1(cid:1) a.s. P ∗.
That is, with probability 1, for any η > 0, there exits N ≥ n such that for any n ≥ N ,
η 0 η
(cid:12) (cid:0) (cid:1) (cid:0) (cid:1)(cid:12)
(cid:12)Γ θ(cid:98)n {Iπn(θ(cid:98) n)}−1 −Γ θ∗ {Iπ(θ∗)}−1 (cid:12) < η.
(cid:110) (cid:111)
Set N = min n ≥ N ;⌈v1⌉ ≥ N < ∞. For any n ≥ N , we obtain that,
2 η cn η 2
(cid:110) 1 (cid:111)
(cid:8) (cid:0) (cid:1) (cid:9)
m ≥ n ; Γ {Iπ(θ∗)}−1 +η ≤ c
0 θ∗ n
m
(cid:110) 1 (cid:111)
⊂ m ≥ n ; Γ ({Iπm(θ(cid:98) )}−1) ≤ c
0 m θ(cid:98)m m n
(cid:110) 1 (cid:111)
(cid:8) (cid:0) (cid:1) (cid:9)
⊂ m ≥ n ; Γ {Iπ(θ∗)}−1 −η ≤ c ,
0 θ∗ n
m
which implies that
(cid:108)Γ ({Iπ(θ∗)}−1)−η(cid:109) (cid:108)Γ ({Iπ(θ∗)}−1)+η(cid:109)
θ∗ θ∗
≤ τ ≤ .
c
cn
c
n n
Set η = ξ ·Γ ({Iπ(θ∗)}−1) and d = Γ ({Iπ(θ∗)}−1)/c → ∞. Note that
θ∗ n θ∗ n
⌈(1−ξ)d ⌉ τ ⌈(1+ξ)d ⌉
n
≤
cn
≤
n
. (192)
⌈d ⌉ τ(Γ ,c ,θ∗,π) ⌈d ⌉
n θ∗ n n
Taking the infimum limit and supremum limit over both sides of inequalities (192), we obtain
that for any ξ > 0,
τ τ
(1−ξ) ≤ liminf
cn
≤ limsup
cn
≤ (1+ξ).
n→∞ τ(Γ θ∗,c n,θ∗,π) n→∞ τ(Γ θ∗,c n,θ∗,π)
In conclusion,
τ
lim cn = 1, a.s. P .
n→∞ τ(Γ θ∗,c n,θ∗,π) ∗
Proof of Theorem 14.23. Let v = τ(Γ ,c ,θ∗,π∗). Accordin to Theorem 4.1 and Theorem
n θ∗ n
1224.3, the conditions in (190) are satisfied. By Lemma 14.24, we obtain (191), which implies
that
√ 1
τ {Iπn(θ(cid:98)ML)}1/2√ {Iπ∗(θ∗)}−1/2 → I , a.s. P . (193)
n n v p ∗
n
For the ease of exposition, we write τ = τ . To show the limit result (187), by (193) and
n cn
Slutsky’s theorem, it suffices to show that
√ (cid:110) (cid:111)1/2 (cid:16) (cid:17)
v Iπ∗(θ∗) (θ(cid:98)ML −θ∗) →d N 0 ,I . (194)
n τn p p p
According to Theorem 13.3 with T = θ(cid:98)ML, θ = θ∗, N = τ , r = v , and W =
n n n n n n n
n−1/2(cid:8) Iπ∗(θ∗)(cid:9)−1/2
, (194) we only need to verify the following conditions for Theorem 13.3:
for all γ > 0, ε > 0, there exists 0 < δ < 1 such that
(cid:16) (cid:13) (cid:13) ε (cid:17)
limsupP max (cid:13)θ(cid:98)ML −θ(cid:98)ML(cid:13) ≥ √ λ ({Iπ∗(θ∗)}−1/2) ≤ γ.
n→∞
|n′−n|≤δn(cid:13) n′ n (cid:13) n min
To show the above inequality, it suffices to show that for any γ > 0, ε > 0, there exists
0 < δ < 1 such that
(cid:16) √ (cid:13) (cid:13) (cid:17)
limsupP max n′′′(cid:13)θ(cid:98)ML −θ(cid:98)ML(cid:13) ≥ ελ ({Iπ∗(θ∗)}−1/2) ≤ γ.
(cid:13) n′ n′′ (cid:13) min
n→∞ n′,n′′,n′′′∈[n,(1+δ)n]
Note that
(cid:16) √ (cid:13) (cid:13) (cid:17)
P max n′′′(cid:13)θ(cid:98)ML −θ(cid:98)ML(cid:13) ≥ ελ ({Iπ∗(θ∗)}−1/2)
(cid:13) n′ n′′ (cid:13) min
n′,n′′,n′′′∈[n,(1+δ)n]
(cid:16) √ (cid:16)(cid:13) (cid:13) (cid:13) (cid:13)(cid:17) ε (cid:17)
≤P max n (cid:13)θ(cid:98)ML −θ(cid:98)ML(cid:13)+(cid:13)θ(cid:98)ML −θ(cid:98)ML(cid:13) ≥ √ λ ({Iπ∗(θ∗)}−1/2)
(cid:13) n′ n (cid:13) (cid:13) n n′′ (cid:13) min
n′,n′′∈[n,(1+δ)n] 1+δ
(cid:16) √ (cid:13) (cid:13) ε (cid:17)
≤2·P max n(cid:13)θ(cid:98)ML −θ(cid:98)ML(cid:13) ≥ √ λ ({Iπ∗(θ∗)}−1/2) .
(cid:13) n′ n (cid:13) min
n′∈[n,(1+δ)n] 2 2
Thus, we only need to show that for all ε > 0,
(cid:16) √ (cid:13) (cid:13) (cid:17)
limlimsupP max n(cid:13)θ(cid:98)ML −θ(cid:98)ML(cid:13) ≥ ελ ({Iπ∗(θ∗)}−1/2) = 0. (195)
(cid:13) n′ n (cid:13) min
δ→0 n→∞ n′∈[n,(1+δ)n]
123Let
(cid:110) (cid:111)
D = ∇ l (θ(cid:98)ML;a ) = 0
n θ n n n
(cid:92)(cid:110)1 (cid:88)n
Ψaj(X )(cid:13) (cid:13){∇2l (θ∗;a )}−1(cid:13) (cid:13)
supψ(cid:16)(cid:13)
(cid:13)θ(cid:98)ML
−θ(cid:98)ML(cid:13) (cid:13)(cid:17)
≤
1(cid:111)
n 2 j θ n n op (cid:13) n n′ (cid:13) 2
n′≥n
j=1
(cid:92)(cid:110)1 (cid:88)n
Ψaj(X )(cid:13) (cid:13){∇2l (θ∗;a )}−1(cid:13) (cid:13)
ψ(cid:16)(cid:13)
(cid:13)θ(cid:98)ML
−θ∗(cid:13) (cid:13)(cid:17)
≤
1(cid:111)
n 2 j θ n n op (cid:13) n (cid:13) 2
j=1
(cid:92)(cid:110)(cid:13) (cid:13) 2 (cid:111)
(cid:13){∇2l (θ(cid:98)ML;a )}−1(cid:13) ≤
(cid:13) θ n n n (cid:13) λ (Iπ∗(θ∗))
op min
(cid:92)(cid:110) (cid:13) (cid:13) 2 (cid:111)
(cid:13){∇2l (θ∗;a )}−1 (cid:13) ≤ .
θ n n op λ (Iπ∗(θ∗))
min
By Theorem 4.1, Theorem 14.1, Corollary 14.12, Lemma 14.13, and with probability 1,
(cid:13) (cid:13) 1
limsup(cid:13){∇2l (θ(cid:98)ML;a )}−1(cid:13) ≤ ,
(cid:13) θ n n n (cid:13) λ (Iπ∗(θ∗))
n→∞ op min
we know that
(cid:32) (cid:33)
∞ ∞
(cid:91) (cid:92)
P D = 1.
n
m=1n=m
By Lemma 14.14, we obtain that
(cid:92) (cid:110) (cid:13) (cid:13) 4 (cid:13) (cid:13)(cid:111)
D ⊂ sup (cid:13)θ(cid:98)ML −θ(cid:98)ML(cid:13) ≤ sup (cid:13)∇ l (θ(cid:98)ML;a )(cid:13) ,
m (cid:13) n′ n (cid:13) λ (Iπ∗(θ∗)) (cid:13) θ n n′ n (cid:13)
n′:n≤n′≤(1+δ)n min n′:n≤n′≤(1+δ)n
m≥n
and
(cid:110)√ (cid:13) (cid:13) 4 √ (cid:111)
D ⊂ n(cid:13)θ(cid:98)ML −θ∗(cid:13) ≤ n∥∇ l (θ∗;a )∥ .
n (cid:13) n (cid:13) λ (Iπ∗(θ∗)) θ n n
min
Thus
(cid:16) (cid:13) (cid:13) ε (cid:17)
P sup (cid:13)θ(cid:98)ML −θ(cid:98)ML(cid:13) ≥ √ λ ({Iπ∗(θ∗)}−1/2)
(cid:13) n′ n (cid:13) n min
n′:n≤n′≤(1+δ)n
(cid:16)(cid:110) (cid:92) (cid:111)c(cid:17)
≤P D
m (196)
m≥n
(cid:16)(cid:110) √ (cid:13) (cid:13) (cid:111)(cid:92) (cid:92) (cid:17)
+P sup n(cid:13)∇ l (θ(cid:98)ML;a )(cid:13) ≥ C(ε) D ,
(cid:13) θ n n′ n (cid:13) m
n′:n≤n′≤(1+δ)n
m≥n
where C(ε) = ε 4λ {m λi mn( in{I (Iπ π∗ ∗(θ (θ∗ ∗)} ))− }1 −/ 12). Let n′ ∈ [n + 1,(1 +δ)n]. With ∇ θl n′(θ(cid:98) nM ′L;a n′) = 0, we
124know that that over ∩ D
m≥n m
(cid:13) (cid:13)
√ (cid:13) (cid:13) 1 (cid:13)
(cid:88)n′
(cid:13)
n(cid:13)∇ l (θ(cid:98)ML;a )(cid:13) = √ (cid:13)∇ logf (X )(cid:13)
(cid:13) θ n n′ n (cid:13) n (cid:13)
(cid:13)
θ θ(cid:98) nM ′L,aj j (cid:13)
(cid:13)
j=n+1
≤√1
n
(cid:13) (cid:13)
(cid:13)
(cid:13)
(cid:88)n′
∇ θlogf θ∗,aj(X
j)(cid:13) (cid:13)
(cid:13)
(cid:13)+δ(cid:80)n j=′
n+1
δnΨa 1j(X j)√ n(cid:13)
(cid:13) (cid:13)θ(cid:98) nM ′L
−θ∗(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:13)
j=n+1
(197)
(cid:13) (cid:13)
1 (cid:13)
(cid:88)n′
(cid:13)
≤√ (cid:13) ∇ logf (X )(cid:13)
n (cid:13) θ θ∗,aj j (cid:13)
(cid:13) (cid:13)
j=n+1
+
4δ (cid:80)( j1 =+ nδ +)n 1Ψa 1j(X j) √1 (cid:13) (cid:13) (cid:13)(cid:88)n′
∇ logf (X
)(cid:13) (cid:13)
(cid:13).
λ (Iπ∗(θ∗)) δn n (cid:13) θ θ∗,aj j (cid:13)
min (cid:13) (cid:13)
j=1
By Markov inequality, we obtain that for any M > 0
P(cid:16)(cid:80) n+1≤j≤(1+δ)n)Ψa 1j(X j)
≥
M(cid:17)
≤
µ
Y
,
δn M
where µ = (cid:80) E Ψa(Xa) < ∞. Thus,
Y a∈A Xa∼f θ∗,a 1
sup
(cid:80)n′ Ψaj(X
)
(cid:80)(1+δ)nΨaj(X
)
n′:n+1≤n′≤(1+δ)n j=n+1 1 j j=n+1 1 j
= = O (1).
p
δn δn
Note that Sn = (cid:80)n+m ∇ logf (X ) is martingale sequence with respect to filtration
m j=n+1 θ θ∗,aj j
Fn = F .
m n+m
By Assumption 2, C := max max E {∥∇ logf (X)∥2} < ∞. Since ∥·∥ is
1 a∈A θ∈Θ X∼f θ∗,a θ θ,a
convex, by Jensen’s inequality, ∥Sn∥ is a submartingale. Applying Doob’s inequality (see
m
Theorem 6.5.d. in Lin (2010)), we obtain that
(cid:16) (cid:17) E∥Sn∥2 l·C
P max ∥Sn∥ ≥ M ≤ l ≤ 1 .
1≤m≤l m M2 M2
Hence, we obtain
(cid:13) (cid:13)
(cid:16) 1 (cid:13)
(cid:88)n′
(cid:13) (cid:17) C
P max √ (cid:13) ∇ logf (X )(cid:13) ≥ M ≤ 1 , (198)
n+1≤n′≤(1+δ)n nδ (cid:13) (cid:13) θ θ∗,aj j (cid:13) (cid:13) M2
j=n+1
and
(cid:13) (cid:13)
(cid:16) 1
(cid:13)(cid:88)n′
(cid:13) (cid:17) (1+δ)n·C 2C
P max √ (cid:13) ∇ logf (X )(cid:13) ≥ M ≤ 1 ≤ 1 . (199)
n+1≤n′≤(1+δ)n n (cid:13) (cid:13) θ θ∗,aj j (cid:13) (cid:13) n·M2 M2
j=1
125Combining (197), (198) and (199), we obtain
√ (cid:13) (cid:13) √
max n(cid:13)∇ l (θ(cid:98)ML;a )(cid:13) = δO (1)+δO (1).
(cid:13) θ n n′ n (cid:13) p P
n′:n+1≤n′≤(1+δ)n
Thus,
(cid:16)(cid:110) √ (cid:13) (cid:13) (cid:111)(cid:92) (cid:92) (cid:17)
limsuplimsupP sup n(cid:13)∇ l (θ(cid:98)ML;a )(cid:13) ≥ C(ε) D
(cid:13) θ n n′ n (cid:13) m
δ→0 n→∞ n′:n≤n′≤(1+δ)n
m≥n
(cid:16)√ (cid:17)
≤limsupP δO (1) ≥ C(ε) = 0.
p
δ→0
This completes the proof of (187).
We proceed to the proof of the ‘Furthermore’ part of the theorem. Note that
g(θ(cid:98)ML)−g(θ∗) = {∇g(θ(cid:101) )}T(θ(cid:98)ML −θ∗),
τn n τn
where θ(cid:101) → θ∗ and ∇g(θ(cid:101) ) → ∇g(θ∗) a.s. P as n → ∞. Then,
n n ∗
√ √
τ (g(θ(cid:98)ML)−g(θ∗)) τ {∇g(θ(cid:101) )}T(θ(cid:98)ML −θ∗)
n τn = n n τn
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)(cid:110) (cid:111)−1/2
(cid:13)
(cid:13)(cid:110) (cid:111)−1/2
(cid:13)
(cid:13)
(cid:13)
Iπτn(θ(cid:98) τM nL) ∇g(θ(cid:98) τM nL)(cid:13)
(cid:13)
(cid:13)
(cid:13)
Iπτn(θ(cid:98) τM nL) ∇g(θ(cid:98) τM nL)(cid:13)
(cid:13)
(cid:104)(cid:110) (cid:111)−1/2 (cid:105)T
=
(cid:13)
(cid:13)(cid:110)Iπτn(θ(cid:98) τM nL) (cid:111)−1/2∇g(θ(cid:101) n)
(cid:13)
(cid:13)√
τ
n(cid:110)
Iπτn(θ(cid:98) τM
nL)(cid:111)1/2
(θ(cid:98) τM nL −θ∗).
(cid:13)
(cid:13)
Iπτn(θ(cid:98) τM nL) ∇g(θ(cid:98) τM nL)(cid:13)
(cid:13)
By Theorem 14.1 and Assumption 6B, we know that there exists U > 0 such that for any
n ≥ n
0
cUI ⪯ Iπn(θ) ⪯ cI , ∀θ ∈ Θ.
p p
(cid:16) (cid:17)
Thus, the condition number of Iπn(θ), κ Iπn(θ) ≤ c < ∞, for any θ ∈ Θ and n ≥ n .
cU 0
Moreover, we know that
(cid:13) (cid:13)
(cid:13)(cid:110) (cid:111)−1/2 (cid:13)
(cid:13)
(cid:13)
Iπτn(θ(cid:98) τM nL) (∇g(θ(cid:101) n)−∇g(θ(cid:98) τM nL))(cid:13)
(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)(cid:110) (cid:111)−1/2 (cid:13) (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Iπτn(θ(cid:98) τM nL) ∇g(θ(cid:98) τM nL)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:13)
≤κ(cid:16)(cid:110)
Iπτn(θ(cid:98) τM
nL)(cid:111)−1/2(cid:17)(cid:13) (cid:13)∇g(
(cid:13)
(cid:13)θ(cid:101) ∇n) g− (θ(cid:98)∇ MLg )(
(cid:13)
(cid:13)θ(cid:98) τM nL)(cid:13)
(cid:13)
= o p(1).
(cid:13) τn (cid:13)
126(cid:110) (cid:111)−1/2
Iπτn(θ(cid:98)τM nL) ∇g(θ(cid:98)τM nL)
Let h = , then ∥h ∥ = 1. By continuous mapping theorem,
n (cid:13) (cid:13)(cid:110) (cid:111)−1/2 (cid:13)
(cid:13)
n
(cid:13) (cid:13)
(cid:13)
Iπτn(θ(cid:98)τM nL) ∇g(θ(cid:98)τM nL)(cid:13) (cid:13)
(cid:13)
(cid:13) (cid:13)
(cid:110) (cid:111)−1/2
Iπ∗(θ∗) ∇g(θ∗)
h → h := a.s. P .
n (cid:13) (cid:13)(cid:110) (cid:111)−1/2 (cid:13)
(cid:13)
∗
(cid:13) Iπ∗(θ∗) ∇g(θ∗)(cid:13)
(cid:13) (cid:13)
As n → ∞,
√
τ (g(θ(cid:98)ML)−g(θ∗)) √ (cid:110) (cid:111)1/2
(cid:13) (cid:13)(cid:110)
n τn
(cid:111)−1/2 (cid:13)
(cid:13)
= hT τ
n
Iπτn(θ(cid:98) τM nL) (θ(cid:98) τM nL −θ∗)+o p(1) →d N(0,1).
(cid:13)
(cid:13)
Iπτn(θ(cid:98) τM nL) ∇g(θ(cid:98) τM nL)(cid:13)
(cid:13)
This completes the proof of (188).
14.12 Proof of Corollaries 5.1, 5.2, and 5.3
In this section, we will verify the regularity conditions for the applications presented in
Sections 5.1, 5.2, and 5.3, thereby proving Corollaries 5.1, 5.2, and 5.3. First, according to
Lemma 13.13, Assumptions 6A and 7A imply Assumptions 6B and 7B. The next lemma is
useful for verifying Assumption 4.
Lemma 14.25. Let Fa = {logf (·) : θ ∈ Θ},a ∈ A be collections of measurable functions
θ,a
with a P integrable envelope functions. That is, F satisfies that for all θ ∈ Θ,
∗ a
|logf (xa)| ≤ F (xa),a.s. P and E F (Xa) < ∞.
θ,a a ∗ Xa∼f θ∗,a a
If Θ is compact and mapping θ (cid:55)→ logf (xa) is continuous for every xa and a ∈ A, then
θ,a
(cid:26) (cid:27)
P lim sup|l (θ;a )−M(θ;π )| = 0 = 1.
∗ n n n
n→∞θ∈Θ
Proof of Lemma 14.25. The proof is similar to the proof of Theorem 2.4.1 in Vaart and
Wellner (1997).First, we show that the bracketing numbers N (ε,Fa,L (P )) < ∞, for
[ ] 1 ∗
every a ∈ A and ε > 0, where the definition of bracketing number N (ε,F,∥·∥) is given by
[ ]
Definition 2.1.6 in Vaart and Wellner (1997).
127Let B(θ,δ) = {θ′ ∈ Θ;∥θ′ −θ∥ < δ}. Define
ua (xa) = sup logf (xa), and, la (xa) = inf logf (xa).
B(θ′,δ) θ,a B(θ′,δ) θ,a
∥θ−θ′∥<δ ∥θ−θ′∥<δ
Because the envelope function F is integrable with respect to P and logf (·) is continuous
a ∗ θ,a
in θ, the Dominated Convergence Theorem ensures that for any θ′ and ε > 0, there exists
δ > 0 such that
E (cid:0) ua (Xa)−la (Xa)(cid:1) < ε.
θ∗ B(θ′,δ) B(θ′,δ)
By compactness of Θ, there exists (θ ,δ ),(θ ,δ ),··· ,(θ ,δ ), such that for any θ ∈ Θ,
1 1 2 2 m m
there exists 1 ≤ j ≤ m,
l (xa) ≤ logf (xa) ≤ u (xa),
j θ,a j
whereu = ua andl = la . Thus,thebracketingnumbersN (ε,Fa,L (P )) < ∞,
j B(θj,δj) j B(θj,δj) [ ] 1 ∗
for all ε > 0 and a ∈ A. That is, we can choose finitely many ε−brackets [la,ua], whose
j j
union contains Fa and such that E (ua(Xa) − la(Xa)) < ε, for every j. Hence, for every
θ∗ j j
θ ∈ Θ, a ∈ A, there exists j such that
a
l (xa) ≤ logf (xa) ≤ u (xa).
ja θ,a ja
The above inequality also implies that
E l (X) ≤ E logf (xa) ≤ E u (X) (200)
X∼f
θ,a
ja X∼f
θ,a
θ,a X∼f
θ,a
ja
Note that, if the functions f (·) are inside the brackets [la,ua] for all a, then
θ,a
l (θ;a )−M(θ;π )
n n n
n
1 (cid:88)
≤ (uai(X )−E[lai(X )|F ])
i i i−1
n
i=1
n
1 (cid:88)
≤ (uai(X )−E[uai(X )|F ])+ε.
i i i−1
n
i=1
Thus,
n
1 (cid:88)
sup(l (θ;a )−M(θ;π )) ≤ max (uai(X )−E[uai(X )|F ])+ε.
n n n i i i−1
θ∈Θ a∈A n
ua∈{ua;1≤j≤m} i=1
j
128By Lemma 13.2,
n
1 (cid:88) (uai(X )−E[uai(X )|F ]) −a. →s. 0.
i i i−1
n
i=1
Consequently,
limsupsup(l (θ;a )−M(θ;π )) ≤ ε,a.s. P .
n n n ∗
n→∞ θ∈Θ
A similar argument yields that
liminf inf(l (θ;a )−M(θ;π )) ≥ −ε,a.s. P .
n n n ∗
n→∞ θ∈Θ
Taking ε → 0, we obtain that
(cid:26) (cid:27)
P lim sup|l (θ;a )−M(θ;π )| = 0 = 1.
n n n
n→∞θ∈Θ
Remark 14.26. Under Assumptions 1 and 2, if we assume ∇2f (xa) is continuous in (θ,xa)
θ θ,a
and
(cid:13) (cid:13)
L := max sup (cid:13)∇2 logf (xa)(cid:13) < ∞,
a∈A θ∈Θ,xa∈supp(f ) θ θ,a op
θ,a
then by the first order Taylor expansion with Lagrange remainder, we can choose the envelop
function F as
a
L
F (xa) = |logf (xa)|+∥∇ logf (xa)∥·diameter(Θ)+ ·diameter(Θ)2.
a θ∗,a θ θ∗,a
2
Proof of Corollary 5.1. Let ξ = zTθ and h (xa) = ζ (xa)exp{xaξ −B (ξ )}. Let Xa ∼
a a ξa,a a a a a
f . Note that
θ∗,a
∇ logh (Xa) = Xa−B′(ξ ) and −∇2 logh (Xa) = B′′(ξ ) ≥ min B′′(ξ ) > 0.
ξa ξa,a a a ξa ξa,a a a
ξa=z aTθ,θ∈Θ
a a
Assumption 1, θ∗ is in the interior of Θ, ξ∗ = zTθ∗ is in the interior of {zTθ;θ ∈ Θ}.
a a a
Applying Theorem 5.8 in Lehmann and Casella (2006), we know that all moments for
∇ logh (Xa) = Xa−B′(ξ∗) exist. Also note that B′′(zTθ) > 0 and I (ξ ) = B′′(ξ )
ξa ξ a∗,a a a ai ai ξa,a a a
is nonsingular. Thus, Assumptions 2 and 6A hold. Note that from the above derivations,
(cid:13) (cid:13)
(cid:13)∇2 logf (xa)(cid:13) does not depend on xa. This, together with Lemma 14.25 and the ac-
ξa θ,a op
companying Remark 14.26, implies that Assumption 4 is satisfied.
129Note that
1
D KL(h
ξ
a∗,a∥h ξa,a) = B a(ξ a)−B a(ξ a∗)+(ξ a∗ −ξ a∗)B a′(ξ a∗) ≥
2
∥ξ a∗ −ξ a∗∥2 min B a′′(ξ(cid:101) a).
ξ(cid:101)a=z aTθ,θ∈Θ
Thus, Assumption 7A is satisfied with C = min B′′(ξ(cid:101) )/2.
ξ(cid:101)a=z aTθ,θ∈Θ a a
Thus, to prove the corollary, it is sufficient to verify Assumption 3, which will be the
focus of the rest of the proof.
Because the Fisher information is I (ξ ) = B′′(ξ ), the first part of conditions of
ξa,a a a
Assumption 3 on the smoothness of the Fisher information in θ holds. We proceed to verify
(cid:80)
that I (θ) is positive definite.
a a
Note that
I (θ) = B′′(zTθ)z zT.
a a a a
Thus,
(cid:88) (cid:88) (cid:88)
c z zT ≤ I (θ) ≤ c z zT,
a a a a a
a∈A a∈A a∈A
where c = inf B′′(zTθ) > 0 and c = sup B′′(zTθ) < ∞.
θ∈Θ,a∈A ai a θ∈Θ,a∈A ai a
So, it is sufficient to show that (cid:80) z zT is non-singular. In the rest of the proof, we
a∈A a a
show that (cid:80) z zT is non-singular by proving the following result in linear algebra:
a∈A a a
(cid:16)(cid:88) (cid:17)
{z ;a ∈ A}⊥ = ker z zT . (201)
a a a
a∈A
Proof of (201) Because (cid:80) z zT is positive semidefinite,
a∈A a a
(cid:16)(cid:88) (cid:17) (cid:16)(cid:88) (cid:17) (cid:88)
z ∈ ker z zT ⇐⇒ zT z zT z = 0 ⇐⇒ |zTz|2 = 0
a a a a a
a∈A a∈A a∈A
⇐⇒ ⟨z,z ⟩ = 0,∀a ∈ A ⇐⇒ z ∈ {z ;a ∈ A}⊥.
a a
Since (cid:80) z zT is symmetric,
a∈A a a
(cid:16)(cid:88) (cid:17) (cid:16)(cid:88) (cid:17)⊥ (cid:16) (cid:17)⊥
R z zT = ker z zT = {z ;a ∈ A}⊥ = span{z ;a ∈ A} = Rp.
a a a a a a
a∈A a∈A
This completes the proof of Corollary 5.1.
130Proof of Corollary 5.2. Because
(cid:8) (cid:9)
f (x ) = exp(b x )exp zTθ −log(1+exp(zTθ +b )) ,x ∈ {0,1},
θ,a a a a a a a a
the M2PL model described in (36) is a special case of the GLM described in (34), with
B (ξ ) = log{1 + exp(ξ + b )}, and ζa(x ) = exp(b x ). Because the support of B (·) is
a a a a a a a a
R, conditions of Corollary 5.1 are satisfied. As a result, Corollary 5.2 follows by directly
applying Corollary 5.2.
Proof of Corollary 5.3. Note that the BTL model described in (39) is a special case of the
M2PL model described in (36) with the following z and b for a = (i,j), and 0 ≤ i < j ≤ p,
a a

(e −e ,0) if i ̸= 0
j i
(z ,b ) = . (202)
a a
(e ,0) if i = 0
j
Thus, Corollary 5.3 is implied by Corollary 5.2 as long as we can verify that a connected
graph G ensures that dim(span{z ;a ∈ A}) = p. In the rest of the proof, we prove a slightly
a
stronger result: for all G = {a1,··· ,as} ⊂ A, if the graph (V,G) is connected, where
V = {0,1,2,··· ,p}, then dim(span{z ;a ∈ A}) = p.
a
First of all, if the graph G = (V,E) is connected, then it implies that s ≥ p. Let Z =
G
[z ,··· ,z ]. It suffices to demonstrate that rank(Z ) = p. To proceed, we construct a
a1 as G
matrix that possesses the same rank as Z , as described below. For a = (i,j), 0 ≤ i < j ≤ p,
G
let

(−1,zT)T if i = 0
z+ = a (203)
a
(0,zT)T if i > 0.
a
Then, define Z+ = [z+,··· ,z+] ∈ R(p+1)×s. Note that z+ = (−zT1 ,zT)T for all a.
G a1 as a a p a
Consequently, rank(Z ) = rank(Z+).
G G
Let e+ = e ,··· ,e+ = e , where e ,...,e is the standard basis for Rp+1. It is easy
0 1 p p+1 1 p+1
to check that z+ = e+ −e+, where a = (a ,a ). Let v = a1, v = a1, S = {v ,v } and
a a1 a2 1 2 1 1 2 2 2 1 2
G = {a ∈ G;a ̸= a1}, where a1 = (a1,a1). Set a = a1. Now we know that rank(z+) = 1
−1 1 2 (cid:98)1 (cid:98)a1
Since (V,G) is a connected graph, there exists a′ ∈ G such that a′ = (a′,a′) a′ ∈ S
−1 1 2 1 2
and a′ ̸∈ S (or a′ ∈ S and a′ ̸∈ S ). Set v = a′ (or v = a′), S = S ∪{v }, a = a′, and
2 2 2 2 1 2 3 2 3 1 3 2 3 (cid:98)2
G = {a ∈ G ;a ̸= a }. By our construction, we know that z+ = e+ −e+ ̸∈ span{z+}.
−2 −1 (cid:98)2 (cid:98)a2 a′
1
a′
2
(cid:98)a1
Thus, rank([z+z+]) > rank(z+).
(cid:98)a1 (cid:98)a2 (cid:98)a1
Because (V,G) is a connected graph, we can always repeat the above process, until
131S = V. By this process, we obtain a sequence a ,··· ,a , such that
p+1 (cid:98)1 (cid:98)p
1 = rank(z+) < ··· < rank([z+,··· ,z+ ]) < rank([z+,··· ,z+]) = p.
(cid:98)a1 (cid:98)a1 (cid:98)ap−1 (cid:98)a1 (cid:98)ap
Notice that p = rank([z+,··· ,z+]) ≤ rank(Z+) = rank(Z ) ≤ p. This implies that
(cid:98)a1 (cid:98)ap G G
rank(Z ) = p.
G
References
Anscombe, F. J. (1952). Large-sample theory of sequential estimation. In Mathematical
Proceedings of the Cambridge Philosophical Society, volume 48, pages 600–607. Cambridge
University Press.
Bartroff, J., Finkelman, M., and Lai, T. L. (2008). Modern sequential analysis and its
applications to computerized adaptive testing. Psychometrika, 73:473–486.
Bhatia, R. (1997). Matrix analysis, volume 169. Springer Science & Business Media.
Billingsley, P. (1999). Convergence of probability measures. John Wiley & Sons.
Bradley, R. A. and Terry, M. E. (1952). Rank analysis of incomplete block designs: I. the
method of paired comparisons. Biometrika, 39(3/4):324–345.
Carlen, E. (2010). Trace inequalities and quantum entropy: an introductory course. Entropy
and the quantum, 529:73–140.
Chang, H.-H. and Ying, Z. (1996). A global information approach to computerized adaptive
testing. Applied Psychological Measurement, 20(3):213–229.
Chang, H.-H. and Ying, Z. (2009). Nonlinear sequential designs for logistic item response
theory models with applications to computerized adaptive tests. The Annals of Statistics,
pages 1466–1488.
Chaudhuri, K., Kakade, S. M., Netrapalli, P., and Sanghavi, S. (2015). Convergence rates
of active learning for maximum likelihood estimation. Advances in Neural Information
Processing Systems, 28.
Chen, X., Bennett, P. N., Collins-Thompson, K., and Horvitz, E. (2013). Pairwise ranking
aggregation in a crowdsourced setting. In Proceedings of the sixth ACM international
conference on Web search and data mining, pages 193–202.
132Chen, X., Chen, Y., and Li, X. (2022). Asymptotically optimal sequential design for rank
aggregation. Mathematics of Operations Research, 47(3):2310–2332.
Chen, X., Jiao, K., and Lin, Q. (2016). Bayesian decision process for cost-efficient dynamic
ranking via crowdsourcing. The Journal of Machine Learning Research, 17(1):7617–7656.
Chen, X., Liu, Q., and Wang, Y. (2023). Active learning for contextual search with binary
feedback. Management Science, 69(4):2165–2181.
Chen,X.andWang,Y.(2023). Robustdynamicpricingwithdemandlearninginthepresence
of outlier customers. Operations Research, 71(4):1362–1386.
Chen, Y., Li, X., Liu, J., and Ying, Z. (2024). Item response theory–a statistical framework
for educational and psychological measurement. Statistical Science.
Chen, Y. and Ye, X. (2011). Projection onto a simplex. arXiv preprint arXiv:1101.6081.
Cheng, Y. (2009). When cognitive diagnosis meets computerized adaptive testing: CD-CAT.
Psychometrika, 74:619–632.
Chernoff, H. (1959). Sequential Design of Experiments. The Annals of Mathematical Statis-
tics, 30(3):755 – 770.
DasGupta, A. (2008). Asymptotic theory of statistics and probability, volume 180. Springer.
Duncan, L. R. (1959). Individual choice behavior: A theoretical analysis. Courier Corpora-
tion.
Elo, A. (1978). The rating of chessplayers past and present. arco pub (1978). Glickman,
ME, Paired comparison models with time-varying parameters, Tech.
Embretson, S. E. and Reise, S. P. (2013). Item response theory. Psychology Press.
Ghorpade, S. R. and Limaye, B. V. (2006). A course in calculus and real analysis. Springer.
Hall, P. and Heyde, C. C. (1980). Martingale limit theory and its application. Academic
press.
Hilbert,D.andCourant,R.(1953). Methods of Mathematical Physics,volume1. Interscience,
New York.
Kiefer, J. (1974). General equivalence theory for optimum designs (approximate theory).
The Annals of Statistics, pages 849–879.
133Lehmann, E. L. and Casella, G. (2006). Theory of point estimation. Springer Science &
Business Media.
Lin, Z. (2010). Probability inequalities. Springer.
Maystre, L. and Grossglauser, M. (2017). Just sort it! a simple and effective approach
to active preference learning. In International Conference on Machine Learning, pages
2344–2353. PMLR.
McCullagh, P. (2019). Generalized linear models. Routledge.
Mukherjee, A., Tajer, A., Chen, P.-Y., and Das, P. (2022). Active Sampling of Multiple
Sources for Sequential Estimation. IEEE Transactions on Signal Processing, 70:4571–
4585.
Mukhopadhyay,N.andChattopadhyay,B.(2012). AtributetoFrankAnscombeandrandom
central limit theorem from 1952. Sequential Analysis, 31(3):265–277.
Naghshvar, M. and Javidi, T. (2013). Active sequential hypothesis testing. The Annals of
Statistics, 41(6):2703 – 2738.
Page, L., Brin, S., Motwani, R., and Winograd, T. (1999). The Pagerank citation ranking:
Bringing order to the web. Technical report, Stanford infolab.
Rakhlin, A., Sridharan, K., and Tewari, A. (2015). Sequential complexities and uniform
martingale laws of large numbers. Probability Theory and Related Fields, 161:111–153.
Reckase, M. D. (2006). Multidimensional item response theory. Handbook of statistics,
26:607–642.
Robbins, H. and Siegmund, D. (1971). A convergence theorem for non negative almost
supermartingales and some applications. In Optimizing Methods in Statistics, pages 233–
257. Elsevier.
Ross, S. M. (2014). Introduction to probability models. Academic press.
Saaty, T. L. and Vargas, L. G. (2012). The possibility of group choice: pairwise comparisons
and merging functions. Social Choice and Welfare, 38(3):481–496.
Thurstone, L. L. (1927). A law of comparative judgment. Psychology Review, 34:273–286.
134Tu, D., Han, Y., Cai, Y., and Gao, X. (2018). Item selection methods in multidimensional
computerized adaptive testing with polytomously scored items. Applied Psychological
Measurement, 42(8):677–694.
Vaart, A. v. d. and Wellner, J. A. (1997). Weak convergence and empirical processes with
applications to statistics. Journal of the Royal Statistical Society-Series A Statistics in
Society, 160(3):596–608.
Van Der Linden, W. J. (1999). Multidimensional adaptive testing with a minimum error-
variance criterion. Journal of Educational and Behavioral Statistics, 24(4):398–412.
Van der Vaart, A. W. (2000). Asymptotic statistics, volume 3. Cambridge university press.
Wainer, H., Dorans, N. J., Flaugher, R., Green, B. F., and Mislevy, R. J. (2000). Comput-
erized adaptive testing: A primer. Routledge.
Wang, C. and Chang, H.-H. (2011). Item selection in multidimensional computerized adap-
tive testing—Gaining information from different angles. Psychometrika, 76(3):363–384.
Wang, C., Chang, H.-H., and Boughton, K. A. (2011). Kullback–Leibler information and its
applications in multi-dimensional adaptive testing. Psychometrika, 76:13–39.
Wang, S., Fellouris, G., and Chang, H.-H. (2017). Computerized adaptive testing that allows
for response revision: Design and asymptotic theory. Statistica Sinica, pages 1987–2010.
Yang, M., Biedermann, S., and Tang, E. (2013). On optimal designs for nonlinear mod-
els: a general and efficient algorithm. Journal of the American Statistical Association,
108(504):1411–1420.
Yu, Y., Wang, T., and Samworth, R. J. (2015). A useful variant of the Davis–Kahan theorem
for statisticians. Biometrika, 102(2):315–323.
Zhang, F. (2006). The Schur complement and its applications, volume 4. Springer Science
& Business Media.
135