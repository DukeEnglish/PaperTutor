Theoretical Analysis of Leave-one-out Cross
Validation for Non-differentiable Penalties under
High-dimensional Settings
Haolin Zou,† Arnab Auddy,∗
Kamiar Rahnama Rad,‡ and Arian Maleki†
∗ University of Pennsylvania
† Columbia University
‡ Baruch College, City University of New York
Abstract: Despite a large and significant body of recent work focused on estimating
theout-of-sampleriskofregularizedmodelsinthehighdimensionalregime,atheoret-
ical understanding of this problem for non-differentiable penalties such as generalized
LASSOandnuclearnormismissing.Inthispaperweresolvethischallenge.Westudy
this problem in the proportional high dimensional regime where both the sample size
nandnumberoffeaturesparelarge, andn/pandthesignal-to-noiseratio(perobser-
vation)remainfinite.Weprovidefinitesampleupperboundsontheexpectedsquared
error of leave-one-out cross-validation (LO) in estimating the out-of-sample risk. The
theoretical framework presented here provides a solid foundation for elucidating em-
pirical findingsthat show theaccuracy of LO.
1. Introduction
1.1. Main goal
Estimating theout-of-sample risk (OO) to assess modelperformanceand to set modelcom-
plexity is a crucial task in statistical learning. A large body of recent work has studied the
theoretical and empirical properties of cross-validation (or its variants) for estimating OO
(Rahnama Rad et al.,2020; Rahnama Rad and Maleki,2020; Patil et al.,2021, 2022, 2023;
Xu et al.,2021;Bellec,2023;Beirami et al.,2017;Auddy et al.,2023;Stephenson and Broderick,
2020; Stephenson et al., 2021; Luo et al., 2023).
In spite of these recent advances, a complete quantitative description of the risk in us-
ing leave-one-out cross-validation (LO) is missing when it comes to problems that have
the following characteristics 1) non-differentiable penalties such as generalized LASSO and
nuclear norm, and 2) finite signal-to-noise ratio when the dimension of the feature space
grows proportionally with the number of observations. In this paper, we focus on the afore-
mentioned characteristics for regularized generalized linear models (including logistic and
Poissonregression).Weuseminorassumptionsaboutthedatageneratingprocesstoprovide
finite sample upper bounds on the expected squared error of leave-one-out cross-validation
in estimating the out-of-sample error. A consequence of our theory is that leave-one-out
cross-validation is a consistent estimator of the out-of-sample risk.
1
4202
beF
41
]TS.htam[
2v34580.2042:viXra/ 2
1.2. Problem statement and related work
Consider the dataset = (y ,x ) n , where x Rp and y R denote the features and
D { i i }i=1 i ∈ i ∈
response of the ith data point, respectively. Observations are independent and identically
distributed (iid) draws from some unknown joint distribution q(y x⊤β∗)p(x ), where β∗
i | i i ∈
Rp represents the true parameter.
The class of estimates, known as regularized empirical risk minimizers (R-ERM), are
based on the following optimization:
n
β := argmin ℓ(y ,x⊤β)+λr(β), (1)
i i
β∈Θ
i=1
X
b
where ℓ(y,z) is a non-negative convex loss function between y and z, e.g., square loss
ℓ(y,z) = 1(y z)2, r(β) is a non-negative convex regularizer, and Θ is a convex set to
2 −
which β∗ belongs. By selecting particular forms for ℓ and r we can cover a wide range of
popular estimators within the high-dimensional setting. For instance, one may use logistic
or Poisson for the loss function, and fused LASSO or nuclear norm for the regularizer.
One widely used criterion for model selection and evaluating the accuracy of β is the
out-of-sample prediction error (OO), defined as
b
OO:= E[φ(y ,x⊤β) ],
0 0 |D
where φ(y,z) is a function measuring the difference between y and z (often, but not nec-
b
essarily the same as ℓ(y,z)), and (y ,x ) is a sample from the same joint distribution
0 0
q(y x⊤β∗)p(x), independent of the training set = (y ,x ) n .
| D { i i }i=1
Several methods are proposed to estimate OO such as K-fold cross-validation, Gener-
alized Cross-Validation, and Bootstrap, among others. In this paper, we employ the term
“risk estimate” as a generic term for referring to these techniques. Despite the abundance
of theoretical and empirical results in the literature, studies specifically examining the ac-
curacy of risk estimates in high-dimensional settings, where the number of features is either
larger than or comparable to the sample size n, are noticeably scarce. As a result many
fundamental questions regarding the performance of risk estimates have remained open.
The main objective of this paper is to prove the accuracy of the leave one out risk
estimate LO for a large class of R-ERMs under the high-dimensional setting. For 1 i n,
≤ ≤
define
β := argmin ℓ(y ,x⊤β)+λr(β). (2)
/i j j
β∈Θ
j6=i
X
b
LO is defined as:
n
1
LO := φ(y ,x⊤β ).
n i i /i
i=1
X
Thesuccess of LOinpractical scenarios has encouragedbnumerousresearchers to investigate
itsaccuracyanddevelopseveralcomputationally-efficientapproximationsforit(Rahnama Rad et al.,
2020;Rahnama Rad and Maleki,2020;Patil et al.,2021,2022,2023;Xu et al.,2021;Bellec,
2023;Beirami et al.,2017;Auddy et al.,2023;Stephenson and Broderick,2020;Stephenson et al.,
2021; Luo et al., 2023; Austern and Zhou, 2020). Notably, key references that are related to
our current work are (Rahnama Rad et al., 2020; Burman, 1989). In their study, (Burman,
1989) studied the accuracy of LO in the low-dimensional setting, p fixed, while n
→ ∞/ 3
p
and established the consistency of LO. In other words, they proved that LO OO. In
→
the more recent paper (Rahnama Rad et al., 2020), the authors studied the accuracy of the
LO under the high-dimensional setting, similar to the setting we consider in this paper.
However, it is worth noting that the conclusions drawn in (Rahnama Rad et al., 2020) are
contingent upon the following two assumptions that restrict the broader applicability of
their outcomes:
1. Restriction 1: The regularizer is twice differentiable.
2. Restriction 2: The set Θ is the same as Rp.
The first assumption excludes a majority of crucial regularizers commonly employed in
practical scenarios, including the ℓ norm, nuclear norm, group-LASSO, etc. Additionally,
1
the second assumption eliminates scenarios where parameters adhere to a specific set. In-
stances of such situations arise when a practitioner seeks to estimate a positive definite
matrix or when the elements of β∗ are required to be positive, increasing, or both.
Our paper aims to eliminate Restriction 1 and Restriction 2, which constrained the
applicability of theresults in(Rahnama Rad et al.,2020)to numerousreal-world problems,
and still obtain a sharp upper bound on the error LO OO.
−
The main technical novelties that have enabled us to achieve these goals are clarified in
Section 2.
1.3. Notations
Inthissection,wesummarizethesymbolsweusethroughoutthearticle.Vectorsaredenoted
with boldfaced lowercase letter, such as x. Matrices are represented by boldfaced capital
letters, such as X. For a matrix X, σ (X), X , tr(X) denote the minimum singular
min
k k
value, the spectral norm (equal to the maximum singular value σ (X)), and the trace of
max
the matrix X respectively.
We denote as the full dataset (y ,x ) n , and as the dataset excluding the ith
D { i i }i=1 D/i
observation, i.e. = (y ,x ) : j = i . We also use [n] := 1,2,...,n for any positive
/i j j
D { 6 } { }
integer n.
For brevity we denote ℓ (β) := ℓ(y ,x⊤β), and similarly φ (β) := φ(y ,x⊤β). The
j j j j j j
following definitions are also used:
∂ℓ(y ,z) ∂φ(y ,z)
ℓ˙(β) := i ,φ˙ (β) := i .
i i
∂z (cid:12)z=x⊤β ∂z (cid:12)z=x⊤β
i i
(cid:12) (cid:12)
(cid:12) (cid:12)
Polynomials of log(n) are denoted(cid:12) by PolyLog(n). When (cid:12)it is specifically defined, a
subscriptis added.For x,y R,we write x y and x y to denote min x,y and max x,y
∈ ∧ ∨ { } { }
respectively.
TheusageofO(1)ando(1)areconventional, anda = Θ(1)iffbotha anda−1 areO(1).
n n n
Similarly, for a sequence of random variables, X = O (1) means X being stochastically
n p n
bounded, i.e. C > 0 s.t. P(X > C) 0. Similarly X = o (1) means X converge to
n n p n
∃ | | →
zero in probability, and X = Θ (1) iff both X and X−1 are O (1).
n p n n p
For a closed subset Θ of Rp, let k(Θ) denote the collection of all functions on Θ with
C
continuous kth Fr´echet derivative on the interior of Θ. In particular, 0(Θ) consists of all
C
continuous functions on Θ./ 4
1.4. Organization of our paper
The rest of the paper is organized as follows. The assumptions are listed in Section 2.1,
followed by discussions on them in Section 2.2. The main theorem is stated in Section 2.3.
In Section 3 we introduce the main challenges and proof techniques (Section 3.1) and a
proof sketch of the main theorem (Section 3.2). The concluding remarks are in Section 4.
Detailed proofs of the lemmas are postponed to the Appendix.
2. Main result
2.1. Assumptions
As we discussed in Section 1.2 our goal is to prove the accuracy of LO under the high-
dimensionalsettings. Before westate our main theorem we explain some of the assumptions
we have made and discuss their validity.
A1 Θ Rp is a closed convex set.
⊂
A2 Both n and p are large, while n/p γ (0, ).
0
≡ ∈ ∞
A3 X = (x , ,x )⊤ where x Rp are i.i.d. N(0,Σ) samples. Moreover, there exist
1 n i
··· ∈
constants 0 <c C such that p−1c σ (Σ) σ (Σ) p−1C .
X X X min max X
≤ ≤ ≤ ≤
A4 r(β) = (1 η)r (β) + ηβ⊤β where η (0,1) and r is non-negative, convex and
0 0
− ∈
Lipschitz continuous.
A5 Assume that ℓ(y,z) and φ(y,z) are non-negative, convex and continuously differen-
tiable with respect to z and that ℓ, ℓ˙, φ and φ˙ (defined in Section 1.3) grow polyno-
mially in y,z, i.e., there exist constants s and C > 0 such that
max ℓ(y,z), ℓ˙(y,z), φ(y,z), φ˙(y,z)
{| | | | | | | |}
C(1+ y s+ z s)
≤ | | | |
for all (y,z). Furthermore, assume that all moments of y are bounded, i.e. m > 0
i
| | ∀
, constant C (m) depending only on m, such that n 1, i [n]:
Y
∃ ∀ ≥ ∀ ∈
E y m C (m).
i Y
| | ≤
2.2. Discussion of Assumptions
The goal of this section is to provide further intuition on the assumptions we introduced in
the previous section.
Assumption A1 is standard in convex optimization. Assumption A2 has become one
of the standard frameworks for studying high-dimensional problems (Obuchi and Sakata,
2019; Mousavi et al., 2018; Xu et al., 2021; Miolane and Montanari, 2021; Donoho et al.,
2009, 2011; Bu et al., 2019; Patil et al., 2021, 2022; Jalali and Maleki, 2016; Bellec, 2023;
Celentano et al.,2023;Li and Wei,2022;Liang and Sur,2022;Fan,2022;Dudeja and Bakhshizadeh,
2022; Dudeja et al., 2023), since it has been able to prove some of the peculiar features esti-
mators exhibit in high-dimensional settings, such as phase transitions (Weng et al., 2018).
The Gaussianity of x in Assumption A3 is prevalent in theoretical papers dealing with
i
high-dimensional problems. While it is straightforward to relax this assumption to sub-
Gaussianity, we focus on Gaussian distribution to keep the notations clear and simple. The/ 5
scaling we have adopted here, i.e.,
p−1c σ (Σ) σ (Σ) p−1C ,
X min max X
≤ ≤ ≤
is based on the following rationale. First note that
c C
X β∗ 2 E(x⊤β∗)2 X β∗ 2.
p k k2 ≤ i ≤ p k k2
When n,p with n/p γ and when the elements of β∗ are O(1), we have β∗ =
0
O(√p),
and→ he∞
nce
E(x⊤β∗)≡
2 = O(1). Therefore, under the settings of the
paperk wek
can
i
var(x⊤β∗)
see that the signal-to-noise ratio (SNR) of each data point, defined as i , remains
var(yi|x⊤
i
β∗)
bounded. The reader may check Appendix B for details. Intuitively speaking, if SNR per
datapoint is very large, the estimation problem will be easy and the problem of risk es-
timation is not of particular interest; maximum likelihood estimators perform very well.
On the other hand, if SNR per data point is very small, the accurate estimation of β will
not be possible unless we impose conditions on β∗, such as stringent sparsity assumption.
However, since we want our results to be generic in terms of β∗, we do not want to imbpose
any stringent constraint on β∗.
Another way we can justify the scaling in Assumption A3 is that under this scaling,
neither theloss ℓ(y ,x⊤β∗)nor theregularizer λr(β∗)dominateeach other whenn,p
j j j →
. In other words, for a large class of losses ℓ(y ,x⊤β∗) = O (n) and for a large class
∞ P j j j p
of regularizers λr(β∗) = O (p). Given that n/p = γ , the two terms have the same order.
p 0
P
Effectively, thismakestheoptimalchoiceofλ(thatgivestheminimumout-of-sampleerror),
O (1). For moreprecisearguments behindthis, werefer the reader to (Mousavi et al.,2018;
p
Wang et al., 2020, 2022).
AssumptionsA4guaranteesthelossfunctiontobe2λη-stronglyconvex.Researchershave
noticedthatinwiderangeofapplications,addingηβ⊤β inadditiontothenon-differentiable
regularizeroftenimprovesthepredictionperformance(Hastie et al.,2017;Mazumder et al.,
2023; Wang et al., 2022; Guo et al., 2023).
AssumptionA4alsopositstheLipschitzcontinuityofr .Nearlyallpopularnon-differentiable
0
regularizerssatisfythiscondition.AppendixC.1presentsseveralexamplesincludingLASSO,
generalized LASSO and Schatten norms with the nuclear norm as a special case.
Assumption A5 also holds for a wide range of models that are used in practice. For
instance Appendix C.2 considers standard data generation mechanisms that are used in
linear regression, logistic regression and Poisson regression, and show that for all those
models Assumption A5 holds.
2.3. Main theorem
The following theorem and its discussion are the main contributions of this paper:
Theorem 2.1. Under Assumption A1-A5, there exists a constant C depending only on
v
γ ,λ,η,s,C ,C () such that
0 X Y
{ · }
C
E(LO OO)2 v .
− ≤ n/ 6
Before we present the sketch of the proof, we would like to discuss this theorem and
provide some intuition.
Remark 2.1. A simple conclusion of this theorem is that under the asymptotic setting in
which n,p and the ratio n/p remains fixed, and p−1c σ (Σ) σ (Σ)
X min max
→ ∞ ≤ ≤ ≤
p−1C , LO offers a consistent estimate of the out-of-sample prediction error in the sense
X
that LO OO in probability. However, note that Theorem 2.1 offers more than the consis-
→
tency, and it captures the convergence rate as well.
Remark 2.2. The rate 1 obtained in Theorem 2.1 is expected to be sharp. Note that the
n
leave-one-outcross-validation takesanaverageofnestimatesoftheOO,namelyφ(y ;x⊤β ).
i i /i
The variance of each of these estimates is O(1). Hence, if all these estimates were indepen-
dent, the variance of LO would be still proportional to O(1/n), which is the same as bthe
bound we have obtained.
Remark 2.3. The rate O(1/n) has also been seen in the previous work on the analysis
of LO under the low-dimensional asymptotic, where p is fixed, while n . For in-
→ ∞
stance, (Burman, 1989) provided a rate estimate of the variance of LO (Theorem 6.2(c)
in (Burman, 1989), notations modified) and showed:
var(LO OO) = O(1/n).
−
Notethatvar(LO OO) E(LO OO)2.Ourresultshowthattherate isstillO(1/n)inhigh-
− ≤ −
dimensional settings. More recently, the paper (Austern and Zhou, 2020) has characterized
the limiting distribution of the k-fold cross-validation under the low-dimensional setting. In
order to obtain a non-degenerate limiting distribution for LO, (Austern and Zhou, 2020)
has to scale LO OO with √n. This scaling translates to the same scaling as the one in
−
(Burman, 1989).
3. Proofs
3.1. Main challenges and novel techniques
As mentioned earlier, (Rahnama Rad et al., 2020) established a result akin to Theorem
2.1 in the case where the regularizer is twice differentiable, and there is no constraint, i.e.
Θ = Rp.
In our proof, there are two new challenges: the non-smoothness of the regularizer r, and
the existence of the convex constraint Θ. We introduce two novel elements that allowed
us to significantly broaden the scope of Theorem 2.1 well beyond what was offered by
(Rahnama Rad et al., 2020): smoothing and projection.
1. Smoothing: We start with approximating the non-smooth regularizer, r (β) with a
0
smooth function:
rα(β) = r (β z)αφ(αz)dz (3)
0 0 −
ZΘ/ 7
where φ(z) = (2π)−p/2e−
21z⊤z
is the density of a standard Gaussian vector.∗ The
following lemma shows one of the properties of this approximation that will be used
throughout the paper.
Lemma 1. Suppose Θ is closed and r 0(Θ).
∈C
(a) Suppose there exists a positive integer K such that r(z) z ke− 21 kβ−zk2 2 is inte-
k k2
grable. Then rα defined in (3) is in k(Θ ) where Θ is the interior of Θ.
0 0
C
(b) Suppose there exist constants L > 0 and k (0,1] such that for all x,y Θ,
∈ ∈
r(x) r(y) L x y k.
| − | ≤ k − k2
Then as α , we have
→ ∞
rα r 0.
k − k∞ →
The proof of this lemma is presented in Appendix D.1. Note that all assumptions
in Lemma 1 are satisfied when r is Lipschitz continuous as in Assumption A4. This
lemma implies that rα(β) is an accurate approximation of r (β) for all values of β.
0 0
Define
n
βα := argmin ℓ (β)+λ(1 η)rα(β)+ληβ⊤β
j − 0
β∈Θ
j=1
X
βbα := argmin ℓ (β)+λ(1 η)rα(β)+ληβ⊤β.
/i j − 0
β∈Θ
j6=i
X
b
Our final goal is to show that by analyzing the LO for the surrogate estimates βα
/i
and βα we can also analyze the LO for the original estimates β and β. This will be
/i
b
clarified in Section 3.2.
2. Projebction operator: The second challenge we have to addresbs in probving Theorem
2.1 is the existence of the constraint set Θ. One can write the optimization problem
β = argmin n ℓ (β)+λr(β) as
i=1 i
β∈Θ
P
b
n
β = argmin ℓ (β)+λr(β)+I (β),
i Θ
β
i=1
X
b
where I (β) is the convex indicator function of the set Θ, and treat the constraint
Θ
as another non-differentiable regularizer and use smoothing again. However, indicator
function I (β) is not a Lipschitz function and cannot be uniformly approximated
Θ
by smooth functions. Hence the smoothing argument discussed before will not work.
Hence, we pick adifferent approach and representβα in a different way. Thefollowing
lemma provides this alternative representation:
b
Lemma 2. Suppose
• R : Rp R , is proper convex with dom(R) being closed.
→ ∪{−∞ ∞}
• L : Rp R is differentiable on the relative interior of dom(R) and proper convex
→
on dom(R).
∗Notethatwhenβisap1byp2matrix,weconcatenateitsrowstoformavectorofdimensionp=p1×p2./ 8
Define the proximal operator of the function R as
1
prox (u) := argmin R(x)+ u x 2 .
R x 2k − k2
(cid:26) (cid:27)
Then a solution of the following equation:
x = prox (x L(x)) (4)
R −∇
is also a minimizer of the problem
min L(x)+R(x) (5)
x { }
and vice versa.
Proof. See Propositon 3.1 (iii)(b) in (Combettes and Wajs, 2005).
Using this lemma we find a new representation for β. Consider the problem with a
convex constraint:
β = argmin ℓ (β)+λrb(β).
i
β∈Θ
i
X
where ℓ and r are smooth,bΘ is a closed convex set. Using Lemma 2 with L(β) =
ℓ (β)+λr(β) and R(β) = I (β) where I (β) is the convex indicator function†,
i i Θ Θ
we have that β should satisfy
P
b β = prox IΘ(β
−
ℓ˙ i(β)x i −λ ∇r(β))
i
X
b= Π (β b ℓ˙(β)xb λ r(β)),b (6)
Θ i i
− − ∇
i
X
b b b
where Π is the metric projection operator
Θ
Π (x) := argmin x z .
Θ 2
k − k
z∈Θ
Note that to obtain the last equality in (6), we have used the fact that the proximal
operator of I is the same as the metric projection onto set Θ.
Θ
Using(6),insteadofrepresentingβα astheminimizerof n ℓ (β)+λrα(β)+I (β),
i=1 i Θ
we represent it as the solution of the following fix point equation:
P
b
βα = Π (βα hα(βα))
Θ
−∇
where Π () denotes the metrbic projectibon onto Θ.bSimilarly, βα satisfies:
Θ · /i
βα = Π (βα hα(βα)), b
/i Θ /i −∇ /i /i
The following lemma establisbhes some obf the imporbtant properties of the projection
operators that are particularly useful in our paper:
†Convex indicator function of set Θ is defined as: I Θ(β)=0 if β∈Θ and I Θ(β)=+∞ otherwise/ 9
Lemma 3. Suppose Θ is a closed convex set in Rp, and x,y Rp. Let Π be the
Θ
∈
metric projection onto Θ. Then there exists a matrix-valued function J(t) with
0 λ (J(t)) λ (J(t)) 1, t [0,1]
min max
≤ ≤ ≤ ∀ ∈
such that
1
Π (y) Π (x) = J(t)dt(y x).
Θ Θ
− −
Z0
For completeness, we include the proof of this well known result in Section D.2.
In the next section, we explain how using the surrogate estimates βα and βα and rep-
/i
resenting them as the solution of, e.g.
b b
βα = Π (βα hα(βα)),
/i Θ /i −∇ /i /i
enable us to obtain an upper bobund on thebdifference LbO OO.
−
3.2. Proof summary of Theorem 2.1
In this subsection, we present a brief sketch of the proof. Similar to the proof of Theorem
1 in (Rahnama Rad et al., 2020) we use the following two definitions:
1
V = LO E[φ (β ) ],
1 i /i /i
− n |D
i∈[n]
X
1 b
V = E[φ (β ) ] OO,
2 i /i /i
n |D −
i∈[n]
X
b
and use the following upper bound on E[LO OO]2:
−
E[LO OO]2 = E(V +V )2 2EV2+2EV2. (7)
− 1 2 ≤ 1 2
Hence, the remaining steps are to obtain upper bounds for EV2 and EV2. To see how the
1 2
ideas weintroducedinSection 3.1enable usto obtain therequiredupperbound,weprovide
the details of proving an upper bound for EV2 (which is shorter) here, and postpone the
2
problem of finding an upper bound for EV2 to Appendix D.6.
1
To bound EV2, first notice that for all j = i we have
2 6
E[φ (β ) ] = E[φ (β ) ]= E[φ (β ) ].
j /i /i 0 /i /i 0 /i
|D |D |D
By the mean-value theoremb, for each i therebexists a random vbariable ξ = t β +(1 t )β
i i /i i
−
with t [0,1] such that
i
∈
b b
φ (β ) φ (β) =φ˙ (ξ )x⊤(β β).
0 /i − 0 0 i 0 /i −
b b b b/ 10
Then we have
n 2
1
E(V2) = E E[φ (β ) ] E[φ (β) ]
2 n i /i |D/i − 0 |D
!
i=1
X
n b b 2
1
= E E[φ (β ) ] E[φ (β) ]
0 /i 0
n |D − |D
!
i=1
X
n b b 2
1
= E E[φ˙ (ξ )x⊤(β β) ]
n2 0 i 0 /i − |D
!
i=1
X
1 n n b b
= E E[φ˙ (ξ )x⊤(β β) ]
n2 0 i 0 /i − |D
Xi=1 Xj=1 (cid:16)
E[φ˙ (ξ )x⊤(β b β)b ]
· 0 j 0 /j − |D
n n (cid:17)
1 2
E E[φ˙ (ξ b)x⊤(βb β) ]
≤ n2 0 i 0 /i − |D
r
Xi=1 Xj=1 (cid:16) (cid:17)
b b
2
E E[φ˙ (ξ )x⊤(β β) ]
· 0 j 0 /j − |D
r
(cid:16) 2 (cid:17)
= E E[φ˙ (ξ )x⊤(β β) ] b, b (8)
0 1 0 /1 − |D
(cid:16) (cid:17)
where we use Cauchy Schwarz inequality in tbhe penbultimate step. Next we have
E[φ˙ (ξ )x⊤(β β) ]
0 i 0 /i − |D
(cid:12) (cid:12)
≤(cid:12)
(cid:12)
E[φ˙2 0(ξ i) |D]bE[(β /ib −β)(cid:12) (cid:12)⊤x 0x⊤ 0(β
/i
−β) |D]
q
C
E[φ˙2(ξ ) ] bX βb β 2 b b (9)
≤ 0 i |D s p k /i − k
q
b b
where the last inequality uses the independence between x and and also the fact that
0
D
σ (Σ) CX by Assumption A3.
max ≤ p
Hence, in order to bound E(V2), we have to obtain bounds on β β and also
2 k − /i k
E[φ˙2(ξ ) ]. That is what the next two lemmas aim to do. The first lemma connects β
0 i |D b b /i
and β:
b
Lemma 4. Under assumptions A1-A4, we have for all α > 0 that
b
ℓ˙(β ) x
i /i i
β β | |k k.
/i
− ≤ 2λη 1
(cid:13) (cid:13) b ∧
(cid:13) (cid:13)
b b
Theproofofthislemmaispre(cid:13)sentedin(cid:13)SectionD.3andusestheideasthatwementioned
in Section 2, i.e. smoothing and projection operator.
The next lemma bounds the moments of φ and φ˙ :
0 0
Lemma 5. Suppose assumptions A1-A5 hold. Then there exists a constant C depending
φ/ 11
only on s (from Assumption A5) and C (from Assumption A3) such that β Rp:
X
∀ ∈
s/2
1
E[φ2(β)] C +C β 2 ,
0 ≤ φ φ pk k
q (cid:18) (cid:19)
s/2
1
E[φ˙2(β)] C +C β 2 ,
0 ≤ φ φ pk k
q (cid:18) (cid:19)
s
1
E[φ˙4(β)] C2+C2 β 2 .
0 ≤ φ φ pk k
q (cid:18) (cid:19)
The proof of this lemma can be found in Section D.4.
Inserting Lemma 4 and Lemma 5 back into (9) we have
E[φ˙ (ξ )x⊤(β β) ]
0 1 0 /1 − |D
(cid:12) (cid:12) (cid:12) C +C 1b ξ 2b2s (cid:12) (cid:12) (cid:12) C X |ℓ˙ 1(β /1) |kx 1 k.
φ φ 1
≤ pk k !s p 2λη 1
(cid:18) (cid:19) b ∧
Hence, if we use (8), then we will obtain
EV2
2
ξ s 2 C
E C2 1+ k 1 k X ℓ˙2(β ) x 2
≤ φ ps/2 p(2λη 1)2 1 /1 k 1 k
!
(cid:18) (cid:19) ∧
(a) C φ2C X E 1+ 1 ξ s 4 b
≤p(2λη 1)2s ps/2k 1 k ×
∧ (cid:18) (cid:19)
E ℓ˙4(β ) x 4
× 1 /1 k 1 k
r
(cid:16) (cid:17)
(b)1 C φ2C Xγ 0
8
1+Eb 1
ξ 2
2s
≤n(2λη 1)2v pk 1 k
∧ u (cid:18) (cid:19) !
u
t1
1
Eℓ˙8(β ) 4 E x 8 4 . (10)
× 1 /1 · k 1 k
(cid:16) (cid:17)
(cid:0) (cid:1)
wheresteps (a) and (b) both usebd Cauchy Schwarz Inequality. Thefollowing lemma bounds
t
E 1 β 2 and Eℓ˙8(β ):
pk k 1 /1
(cid:16) (cid:17)
Lemma6. Underassumptions A1-A5,there existconstantsC (t)depending on γ ,λ,η,C (),s,t
b b β 0 Y
{ · }
and C depending on γ ,λ,η,C (),s,C such that
ℓ 0 Y X
{ · }
(a) For t 1,
≥
E[p−1 β 2]t C (t)
β
k k ≤
E[p−1 β 2]t C (t),
/1 β
k bk ≤
(b) Eℓ˙8(β ) C . b
1 /1 ≤ ℓ
b/ 12
The proof of this lemma can be found in Section D.5. Observe that ξ is a convex
1
combination of β and β , then using part (a) of Lemma 6 we have that the same bound
/i
appliestop−1 ξ 2.Next,byLemma8wehaveE x 8 24C4 .Insertingtheabovebounds
k 1 k k 1 k ≤ X
into (10) we havbe b
EV 22
≤
n1 (2C λφ2 ηC Xγ 10
)2
8(1+C β(2s)) ·C ℓ1 4 ·(24C X4 )41
∧ q
17C φ2C X2C ℓ1/4 γ
0
1+C (2s)
≤ n (2λη 1)2 β
∧ q
C
v2
:= . (11)
n
The second line uses the fact that √8(24)1/4 7.
≤
Obtaining an upper bound for E(V2) uses similar techniques, although it involves more
1
cumbersome calculations. In fact we can prove that
Lemma 7. Under assumptions A1-A5, for large enough n,p we have:
C
EV2 v1
1 ≤ n
for C > 0 depending only on γ ,λ,η,C (),s,C .
v1 0 Y X
{ · }
The proof of Lemma 7 is given in Section D.6.
Combining Lemma 7 and (11) with (7) completes the proof of Theorem 2.1:
2(C +C ) C
E[LO OO]2 2EV2+2EV2 v1 v2 := v .
− ≤ 1 2 ≤ n n
4. Conclusion
In this paper, our focus was on the class of regularized empirical risk minimization (R-
ERM) techniques that incorporate non-differentiable regularizers. We studied the accuracy
of leave-one-out cross-validation techniques within a high-dimensional setting, where both
the number of observations n and the number of features p are large while the ratio n/p
is bounded. We derived a finite-sample upper bound for the difference between the out-of-
sample prediction error and its leave-one-out cross-validation estimate. Our upper bound
shows that if OO and LO represent the out-of-sample prediction error and its leave-one-out
estimate, then E(LO OO)2 =O(1).
− n
Acknowledgments
Arian Maleki would like to thank NSF (National Science Foundation) for their generous
support through grant number DMS-2210506. Kamiar Rahnama Rad would like to thank
NSF(National ScienceFoundation) fortheirgenerous supportthroughgrantnumberDMS-
1810888./ 13
References
Arnab Auddy, Haolin Zou, Kamiar Rahnama Rad, and Arian Maleki. Approximate leave-
one-out cross validation for regression with ℓ regularizers (extended version). arXiv
1
preprint arXiv:2310.17629, 2023.
Morgane Austern and Wenda Zhou. Asymptotics of cross-validation. arXiv preprint
arXiv:2001.11111, 2020.
Ahmad Beirami, Meisam Razaviyayn, Shahin Shahrampour, and Vahid Tarokh. On opti-
mal generalizability in parametric learning. Advances in Neural Information Processing
Systems, 30, 2017.
Pierre C Bellec. Out-of-sample error estimation for m-estimators with convex penalty.
Information and Inference: A Journal of the IMA, 12(4):2782–2817, 2023.
Zhiqi Bu, Jason M. Klusowski, Cynthia Rush, and Weijie J. Su. Algorithmic Analysis and
Statistical Estimation of SLOPE via Approximate Message Passing. IEEE Transactions
on Information Theory, 67:506–537, 2019.
Prabir Burman. A comparative study of ordinary cross-validation, v-fold cross-validation
and the repeated learning-testing methods. Biometrika, 76(3):503–514, 1989.
Michael Celentano, Andrea Montanari, and Yuting Wei. The lasso with general gaussian
designswithapplicationstohypothesistesting. TheAnnalsof Statistics,51(5):2194–2220,
2023.
Patrick L. Combettes and Val´erie R. Wajs. Signal recovery by proximal forward-backward
splitting. Multiscale Modeling & Simulation, 4(4):1168–1200, 2005.
David L Donoho, Arian Maleki, and Andrea Montanari. Message-passing algorithms for
compressed sensing. Proceedings of the National Academy of Sciences, 106(45):18914–
18919, 2009.
David L Donoho, Arian Maleki, and Andrea Montanari. The noise-sensitivity phase transi-
tionincompressedsensing.IEEETransactions onInformation Theory,57(10):6920–6941,
2011.
Rishabh Dudeja and Milad Bakhshizadeh. Universality of linearized message passing for
phase retrieval with structured sensing matrices. IEEE Transactions on Information
Theory, 68(11):7545–7574, 2022.
Rishabh Dudeja, Yue M. Lu, and Subhabrata Sen. Universality of approximate message
passing with semirandom matrices. The Annals of Probability, 51(5):1616–1683, 2023.
Zhou Fan. Approximate message passing algorithms for rotationally invariant matrices.
The Annals of Statistics, 50(1):197–224, 2022.
Yilin Guo, Haolei Weng, and Arian Maleki. Signal-to-noise ratio aware minimaxity and
higher-order asymptotics. IEEE Transactions on Information Theory, 2023.
TrevorHastie,RobertTibshirani,andRyanJTibshirani.Extendedcomparisonsofbestsub-
set selection, forward stepwise selection, and the lasso. arXiv preprint arXiv:1707.08692,
2017.
R. Ibragimov and Sh. Sharakhmetov. The best constant in the rosenthal inequality for
nonnegative random variables. Statistics & Probability Letters, 55(4):367–376, 2001.
Shirin Jalali and Arian Maleki. New approach to bayesian high-dimensional linear regres-
sion. Information and Inference: A Journal of the IMA, 7, 07 2016.
Gen Li and Yuting Wei. A non-asymptotic framework for approximate message passing in
spiked models. arXiv preprint arXiv:2208.03313, 2022.
TengyuanLiangandPragyaSur. Aprecisehigh-dimensionalasymptotictheoryforboosting
andminimum-ℓ -norminterpolated classifiers. The Annals of Statistics, 50(3):1669–1695,
1/ 14
2022.
Yuetian Luo, Zhimei Ren, and Rina Barber. Iterative approximate cross-validation. In
International Conference on Machine Learning, pages 23083–23102. PMLR, 2023.
Rahul Mazumder, Peter Radchenko, and Antoine Dedieu. Subset selection with shrinkage:
Sparse linear modeling when the snr is low. Operations Research, 71(1):129–147, 2023.
L´eo Miolane and Andrea Montanari. The distribution of the Lasso: Uniform control over
sparse balls and adaptive parameter tuning. The Annals of Statistics, 49(4):2313–2335,
2021.
Ali Mousavi, Arian Maleki, and Richard G. Baraniuk. Consistent parameter estimation
for LASSO and approximate message passing. The Annals of Statistics, 46(1):119 – 148,
2018.
Tomoyuki Obuchi and Ayaka Sakata. Cross validation in sparse linear regression with
piecewise continuous nonconvex penalties and its acceleration. Journal of Physics A:
Mathematical and Theoretical, 52(41):414003, 2019.
Pratik Patil, Yuting Wei, Alessandro Rinaldo, and Ryan Tibshirani. Uniform Consistency
of Cross-Validation Estimators for High-Dimensional Ridge Regression. In Proceedings
of The 24th International Conference on Artificial Intelligence and Statistics, volume 130
of Proceedings of Machine Learning Research, pages 3178–3186. PMLR, 2021.
Pratik Patil, AlessandroRinaldo, and Ryan Tibshirani. Estimating Functionals of the Out-
of-Sample Error Distribution in High-Dimensional Ridge Regression. In Proceedings of
The 25th International Conference on Artificial Intelligence and Statistics, volume 151
of Proceedings of Machine Learning Research, pages 6087–6120. PMLR, 2022.
Pratik Patil, Yuchen Wu, and Ryan Tibshirani. Failures and successes of cross-validation
for early-stopped gradient descent in high-dimensional least squares. 2023.
KamiarRahnamaRadandArianMaleki. Ascalableestimateoftheout-of-sampleprediction
error via approximate leave-one-out cross-validation. Journal of the Royal Statistical
Society Series B: Statistical Methodology, 82(4):965–996, 2020.
Kamiar Rahnama Rad, Wenda Zhou, and Arian Maleki. Error bounds in estimating the
out-of-sample prediction error using leave-one-out cross validation in high-dimensions.
In International Conference on Artificial Intelligence and Statistics, pages 4067–4077.
PMLR, 2020.
H.P. Rosenthal. On the subspaces of L (p > 2) spanned by sequences of independent
p
random variables. Israel Journal of Mathematics, 8:273–303, 1970.
Will Stephenson, Zachary Frangella, Madeleine Udell, and Tamara Broderick. Can we
globally optimize cross-validation loss? quasiconvexity in ridge regression. Advances in
Neural Information Processing Systems, 34:24352–24364, 2021.
William Stephenson and Tamara Broderick. Approximate cross-validation in high dimen-
sions with guarantees. In International Conference on Artificial Intelligence and Statis-
tics, pages 2424–2434. PMLR, 2020.
Roman Vershynin. High-dimensional probability: An introduction with applications in data
science, volume 47. Cambridge university press, 2018.
Shuaiwen Wang, Haolei Weng, and Arian Maleki. Which bridge estimator is the best for
variable selection? The Annals of Statistics, 48(5):2791 – 2823, 2020.
Shuaiwen Wang, Haolei Weng, and Arian Maleki. Does SLOPE outperform bridge regres-
sion? Information and Inference: A Journal of the IMA, 11(1):1–54, 2022.
Haolei Weng, Arian Maleki, and Le Zheng. Overcoming the limitations of phase transition
by higher order analysis of regularization techniques. The Annals of Statistics, 46(6A):
3099 – 3129, 2018./ 15
Ji Xu, Arian Maleki, Kamiar Rahnama Rad, and Daniel Hsu. Consistent risk estimation
in moderately high-dimensional linear regression. IEEE Transactions on Information
Theory, 67(9):5997–6030, 2021.
Ming Yuan and Yi Lin. Model Selection and Estimation in Regression with Grouped
Variables. Journal of the Royal Statistical Society Series B: Statistical Methodology, 68
(1):49–67, 12 2005. ISSN 1369-7412.
Appendix A: Technical Lemmas
To improve the readability of the rest of the manuscript, we include several standard tech-
nical results used in our detailed proofs which are presented in the rest of the Appendix.
Lemma 8. Suppose Assumption A2 holds. Then for p 2 we have
≥
E x 8 24C
i X
k k ≤
1
Proof. Let z = Σ− 2x
i
then z N(0,I p) and z 2 χ2(p). By standard results on χ2
∼ k k ∼
distribution we have
E z 8 = p(p+2)(p+4)(p+6)
k k
. Then we have for p 2:
≥
E x 8 = E(z⊤Σz)4
i
k k
4
C
E X z 2
≤ p k k
(cid:18) (cid:19)
C4
XE z 8
≤ p4 k k
2 4 6
C4 1+ 1+ 1+
≤ X p p p
(cid:18) (cid:19)(cid:18) (cid:19)(cid:18) (cid:19)
24C4 .
≤ X
Lemma 9 ((Rosenthal, 1970)). Let X be a sequence of independent non-negative
i i∈[n]
{ }
random variables, and t 1. Then we have
≥
t t
E X A(t)max EXt, EX .
 i  ≤  i  i  
i X∈[n] i X∈[n] i X∈[n] 
   
Proof. See Theorem 3 in (Rosenthal, 1970).For a discussion on sharp choice of A(t), the
readers may refer to (Ibragimov and Sharakhmetov, 2001).
Appendix B: Bounded Signal-to-noise Ratio
First we explain what we mean by ’bounded SNR’ in Section 2.1, using the three examples
of linear, logistic and Poisson regression (with log exponential link):
• Linear: y x N(x⊤β∗,σ2)
i | i ∼ i/ 16
• Logistic: y i x i Binomial((1+e−x⊤ i β∗ )−1)
• Poisson: y i| x i ∼ Poisson(log(1+ex⊤ i β∗ ))
| ∼
Define the signal-to-noise ratio as
var(x⊤β∗)
SNR := i .
var(y x⊤β∗)
i | i
When β∗
2
= O(√p) or each elements of of β∗ is O(1), by Assumption A2 we have
k k
C
var(x⊤β∗)= β∗⊤Σβ∗ X β∗ 2 = O(1).
i ≤ p k k2
On the other hand,
σ−2, Linear
[var(y i x i)]−1 = (e−1 2x⊤ i β∗ +e21x⊤ i β∗ )2, Logistic
|
 [log(1+ex⊤ i β∗ )]−1, Poisson
They are all O p(1) when n,p increas e. To see this, notice that
x⊤β∗ N(0,β∗⊤Σβ∗)
i ∼
and
1 1
4 (e− 2z +e2z)2 2(e|z|+1),
≤ ≤
(log(2)+z )−1 [log(1+ez)]−1 z−1
+ ≤ ≤ +
where z = z when z 0 and 0 otherwise. Suppose C p−1/2 β∗ C for some
+ 1 2
constants C ,C , then
x⊤≥
β∗ = Θ (1) and so is the ratio
var(≤ x⊤
i
β∗)
.
k k ≤
1 2 i p var(yi|x⊤
i
β∗)
Appendix C: Discussion of the assumptions
C.1. On Assumption A4
In this subsection we present several commonly used regularizers in machine learning, and
show that they are all Lipshitz continuous.
Example C.1 (LASSO). The classic LASSO penalty is r (β) = β and is clearly non-
0 1
k k
negative, convex and Lipschitz continuous.
Example C.2 (Group LASSO). The group LASSO was introduced in (Yuan and Lin,
2005) to achieve joint variable selection among different data groups. Assume the fea-
tures are partitioned in G groups J ,J ,...,J , and the penalty takes the form r (β) =
1 2 G 0
G (β⊤K β )1/2 where β Rpj is the coefficient vector for group j, K Rpj×pj is
j=1 Jj j Jj Jj ∈ j ∈
a positive definite matrix, β = (β⊤, ,β⊤ )⊤ Rp is the concatenated coefficient with
P J1 ··· JG ∈
p = G p .
j=1 j
Clearly r is non-negative. Moreover itisalso convexsince itisa sumof convex functions
P 0
r (β) = (β⊤K β )1/2. Finally, r is also J σ (K )-Lipschitz with respect to β.
0j Jj j Jj 0 k=1 max j
q
P/ 17
Example C.3 (Generalized LASSO). The generalized LASSO penalty takes the form of
r (β) = Dβ and encompasses many LASSO type penalty such as LASSO (D = I ) and
0 1 p
k k
fused LASSO (D = (d ) where d = 1 if i = j, d = 1 if j = i+1 and d = 0
ij (p−1)×p ij ij ij
−
otherwise). The nonnegativity and convexity are immediate since r is the ℓ norm of Dβ.
0 1
Moreover, it follows that Dβ is σ (D)-Lipschitz in β.
1 max
k k
Example C.4 (Nuclear normandSchatten norms). When the estimand is a matrix B with
rank d, the nuclear norm is a popular regularizer:
d
r (B) = σ (B)
0 i
i=1
X
where σ (B) is the i-th largest singular value of B. More generally, the Schatten norm of B
i
is defined as
1
d p
r (B) = σp (B)
0 i
!
i=1
X
and takes the nuclear norm as a special case when p = 1. Since r (B) is a norm on the
0
singular values of B, we obtain the nonnegativity and convexity of r . To show the Lipschitz
0
continuity we argue as follows. Since r (B) is the p-norm of the vector (σ (B), ,σ (B))
0 1 d
···
and using the triangular inequality we have
r (x) r (y) r (x y)
0 0 0
| − | ≤ −
1 1
Kp− 2 x y if 1 p 2
k − k2 ≤ ≤
≤ ( x y if p 2
k − k2 ≥
√K x y ,
≤ k − k2
where B denotes the Frobenius norm of B. The second line uses the relationship between
k k2
p-norms: for u Rp,
∈
1
K p
u = up
k kp k
!
k=1
X
1 1
Kp− 2 u if 1 p 2
k k2 ≤ ≤
≤ ( u if p 2.
k k2 ≥
Then if we let u = (σ (B), ,σ (B))⊤ we then have u = r (B) and
1 K p 0
··· k k
1
K 2
u = σp (B) = tr(B⊤B) = B .
k k2 k k k2
!
k=1 q
X
C.2. On Assumption A5
In this section, we show that the moment bound of y and polynomial growth of ℓ,ℓ˙,φ,φ˙ in
i
Assumption A5 are justified for many popular data generating mechanisms. We show this/ 18
for three examples: linear, logistic and Poisson regression (with log exponential link). In
this subsection we assume Assumptions A2 and A3 hold, namely, n/p γ > 0 and x are
0 i
≡
i.i.d. N(0,Σ) with σ (Σ) C /p. In addition we assume p−1/2 β∗ ξ for some ξ > 0.
max X
≤ k k≤
Finally, both ℓ and φ are set to the negative log-likelihood.
Example C.5 (Linear). Suppose y x N(x⊤β∗,σ2). Then y N(0,σ˜2) where σ˜2 =
i | i ∼ i i ∼
σ2+β∗⊤Σβ∗. Using standard results on Gaussian moments, we have
2m/2Γ m+1 2m/2Γ m+1
E y m = σ˜m 2 σ2+C ξ2 m/2 2 for any m 0.
X
| | × √π ≤ √π ≥
(cid:0) (cid:1) (cid:0) (cid:1)
(cid:0) (cid:1)
For Gaussian linear model, the negative log-likelihood is known to be the ℓ loss:
2
1
ℓ(y,z) = (y z)2 y2+z2,
2 − ≤
and ℓ˙(y,z) = z y z + y .
| | | − |≤ | | | |
Example C.6 (Logistic). Suppose y i x i Bernoulli(p) where p = (1+e−x⊤ i β∗ )−1. Then
| ∼
y 0,1 and obviously
i
∈ { }
E y m 1 for all m 0.
| | ≤ ≥
The negative log-likelihood of this model is
ℓ(y,z) = ylog(1 e−z)+(1 y)log(1+ez), y 0,1 ,
− − ∈ { }
and
ℓ(y,z) 2log(2)+2z .
| | ≤ | |
For its derivative,
ez
ℓ˙(y,z) = y 1+ y .
| | 1+ez − ≤ | |
(cid:12) (cid:12)
(cid:12) (cid:12)
Example C.7 (Poisson). Suppose y i x(cid:12) (cid:12)i Poisson(cid:12) (cid:12)(µ) with µ = log(1 + ex⊤ i β∗ ). Then y i
| ∼
are non-negative. Using the equivalence between Poisson and exponential distributions, it
can be shown that y Poisson(µ) implies y Sub exp(µ). It then follows by results for
∼ ∼ −
sub-exponential random variables (see, e.g., Proposition 2.7.1 of (Vershynin, 2018)) that
E(y m x ) (C µ)mmm
i ∗
| | | ≤
whence taking expectation over x it follows that for any m 0 we have
≥
Eym E(log(1+ex⊤β∗ ))m (C m)m (C m)mEemx⊤β∗ (C m)meβ∗⊤Σβ∗m2/2 (C m)mem2CXξ2/2.
∗ ∗ ∗ ∗
≤ × ≤ ≤ ≤
for a numerical constant C > 0. In the last series of inequalities we have used first the
∗
fact that log(1+x) x for x > 0. Next we have used the moment generating function of
≤
the normal distribution x⊤β∗ N(0,β∗⊤Σβ∗) and finally the inequality β∗⊤Σβ C ξ2
X
∼ ≤
in the last step./ 19
The negative log-likelihood is
ℓ(y,z) = log(y!)+log(1+ez) yloglog(1+ez)
| −
ylog(y) +log(2)+ z + ylog(log(2)+ z )
≤ | | | | | | | |
z
y2+log(2)+ z + y(loglog(2)+ )
≤ | | | log(2) |
1
y2+log(2)+ z +loglog(2)y + (y2+z2)
≤ | | | | 2log(2)
C(y2+z2+1),
≤
and the derivative satisfies
1 yez
ℓ˙(y,z) = 1+ y .
| | 1+e−z − (1+ez)log(1+ez) ≤ | |
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
Appendix D: Detailed Proofs
D.1. Proof of Lemma 1
We start with the proof of Part (a). First we extend r to Rp by simply letting r (β) = 0
0 0
for β / Θ. Then we have
∈
rα(β) = r (β z)αφ(αz)dz
0 Rp 0 −
Z
= α r (z)φ(α(β z))dz.
0
Rp −
Z
Forapointβ Θ ,consideritsdirectionalderivativeoveradirectionv Rp with v = 1:
∈ 0 ∈ k k2
rα(β)
∇v 0
1
:= lim [rα(β+hv) rα(β)]
h→0h 0 − 0
2α φ(α(β z))
v
≤ ∇ −
1
= lim α r (z) [φ(α(β z+hv)) φ(α(β z))]dz
0
h→0 Rp h − − −
Z
Since φ ∞(Rp), we have that for small enough h:
∈ C
1
[φ(α(β z+hv)) φ(α(β z))]
h − − −
2αv⊤ φ(α(β z))
≤ ∇ −
2α φ(α(β z))
≤ k∇ − k
2α(2π)−p/2e−1 2α2kβ−zk2
2 β z .
≤ k − k2
Byassumptionr
0(z)e−1 2α2kβ−zk2
2 kβ −z
k2
isintegrable,thenusingDominatedConvergence,
the limit can be taken within theintegral, so that rα(β) exists. Using similar arguments,
∇v 0
in order that krα(β) exists, we only need r (z) kφ(α(β z)) to be integrable. To see
∇v 0 0 ∇v −
this, notice that
∂uk1
∂k
∂ukpφ(u) =
e−1 2kuk2
P k1···kp(u)
1 ··· p/ 20
where P (u) = uk1uk2 ukp + o(uk1uk2 ukp) is a polynomial with its dominating
k1···kp 1 2 ··· p 1 2 ··· p
term being uk1uk2 ukp with order K. Then we have
1 2 ··· p
kφ(u)
v
∇
(cid:12) (cid:12)
(cid:12) (cid:12) ∂k
=(cid:12) (cid:12) φ(u)vk1 vkp
(cid:12) ∂uk1 ∂ukp 1 ··· p (cid:12)
(cid:12) (cid:12)k1+· X··+kp=k 1 ··· p (cid:12) (cid:12)
≤(cid:12) (cid:12)
(cid:12)
v 1k1 ···v pkp e− 21 kuk2 P k1···kp(u(cid:12) (cid:12) (cid:12))
k1+· X··+kp=k(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
≤
P k1···kp(u) e−
21 kuk2
|v
i
|
k
k1+· X··+kp=k (cid:12) (cid:12) (cid:16)X (cid:17)
=O
( u
ke−1 2ku(cid:12) k2
pk/2 v
(cid:12) k
)
k k k k2
=O( u
ke−1 2kuk2
)
k k
Inserting u = α(β z) we have
−
kφ(α(β z)) = O(αk β z ke− 21 α2kβ−zk2 2)
∇v − k − k2
(cid:12)
(cid:12)
(cid:12)
(cid:12) = O( z
ke−1 2α2kβ−zk2
2)
(cid:12) (cid:12) k k2
So r 0(z) ∇k vφ(α(β −z)) is integrable if r 0(z) kz kk 2e−1 2kβ−zk2 2 is integrable. The continuity of
the derivatives follows then from exchanging the integration and differentiation.
We now turn tho the proof of Part (b) of the paper. For all β Θ ,
0
∈
rα(β) r (β) r (β z) r (β)αφ(αz)dz
| 0 − 0 | ≤ Rp| 0 − − 0 |
Z
= r (β α−1u) r (β)dΦ(u)
0 0
Rp| − − |
Z
L
u kdΦ(u)
≤ αk Rpk k2
Z
0 (α )
→ → ∞
The second line uses change of variable u = αz and the third line uses our assumption on
r . The last line is because the integral is finite.
0
Remark D.1. Although Lemma 1 adopts a weaker assumption than Lipschitz continuity,
it can be shown that k = 1 is the only value that allows r to be convex. In fact, for k < 1
0
we have
r (x) L x k.
0
≤ k k
It is obvious that unless r is constant, it would eventually increase at least linearly thus
0
cannot be bounded by L x k for some k < 1.
k k
Example D.1. Generalized LASSO: r (β) = Dβ for a fixed D Rm×p. It is contin-
0 k k1 ∈
uous, and r (x) r (y) = Dx Dy Dx Dy √m x y . Note that
| 0 − 0 | |k k1 −k k1| ≤ k − k1 ≤ k − k2
the classical LASSO is a special case with D = I .
p
Remark D.2. In fact if r is a norm such that Er (β) < where β has i.i.d. N(0,1)
0 0
∞
entries, then the results of the above two lemmas hold./ 21
D.2. Proof of Lemma 3
Fixx,y andconsiderf(t):= Π ((1 t)x+ty).Bythefirmnon-expansivenessofprojection
Θ
−
operators, we have t ,t [0,1],
1 2
∀ ∈
0 (f(t ) f(t ))2
1 2
≤ −
= Π ((1 t )x+t y) Π ((1 t )x+t y) 2
Θ 1 1 Θ 2 2
k − − − k
Π ((1 t )x+t y) Π ((1 t )x+t y),(t t )(y x)
Θ 1 1 Θ 2 2 1 2
≤ h − − − − − i
= f(t ) f(t ),(t t )(y x)
1 2 1 2
h − − − i
(t t )2 y x 2. (12)
1 2
≤ − k − k
By taking a square root, we thus have f(t) is x y Lipschitz on [0,1]. Therefore it is
k − k
absolutely continuous and differentiable almost everywhere. Let t = t+ǫ and t = t, and
1 2
then divide (12) by ǫ2, then if f′(t) exists we have:
0 f′(t) 2 f′(t),y x y x 2
≤ k k ≤ h − i ≤ k − k
For each t, consider solving the following equation for J(t):
f′(t) = J(t)(y x). (13)
−
The claim is that we can find J(t) with its eigenvalues between [0,1] such that (13) holds.
In fact, since the rotation in Rp is a unitary matrix with all eigenvalues being 1, we can
choose J(t) =
kf′(t)kR
where R is a rotation that rotates y x to the direction of f′(t),
ky−xk −
and this choice of J(t) has its all eigenvalues being
kf′(t)k
[0,1].
ky−xk ∈
With a slight abuse of notation we assume f′(t) = 0 wherever the derivative does not
exist. Then by the Newton-Leibniz formula (for Lebesgue integral and absolute continuous
functions):
Π (y) Π (x) = f(1) f(0)
Θ Θ
− −
1
= f′(t)dt
Z0
1
= J(t)(y x)dt
−
Z0
where the last line uses (13).
Remark D.3. In the proof of Lemma 3, the matrix J(t) is not explicitly derived. However
by equation (13) one can easily see that, when Π ((1 t)x+ty) is smooth at t = t , we
Θ 0
−
can use its Jacobian as J(t), i.e.
∂
J(t ) = Π (z) .
0 Θ
∂z
(cid:12)z=(1−t0)x+t0y
(cid:12)
(cid:12)
(cid:12)/ 22
D.3. Proof of Lemma 4
Proof. Begin we start the proof, let us introduce the following notations:
n
h(β) := ℓ (β)+λ(1 η)r (β)+ληβ⊤β,
j 0
−
j=1
X
n
hα(β) := ℓ (β)+λ(1 η)rα(β)+ληβ⊤β,
j − 0
j=1
X
n
h (β) := ℓ (β)+λ(1 η)r (β)+ληβ⊤β,
/i j 0
−
j6=i
X
n
hα(β) := ℓ (β)+λ(1 η)rα(β)+ληβ⊤β.
/i j − 0
j6=i
X
As mentioned before, in order to obtain a bound on β β we use the smoothing trick.
/i 2
k − k
Hence, we first obtain an upper bound for βα βα .
k − /ikb b
Similar to h and hα, let h and hα denote the loss functions for β and βα respectively.
/i /i b b /i /i
By Lemma 2, βα and βα satisfy
/i
b b
b b βα = Π (βα hα(βα))
Θ
−∇
βα = Π (βα hα(βα)).
/bi Θ /bi −∇ /i b/i
By subtracting one from the othber we haveb b
βα βα =Π (βα hα(βα)) Π (βα hα(βα))
− /i Θ −∇ − Θ /i −∇ /i /i
=J¯ βα hα(βα) βα + hα(βα) (14)
b b × b −∇ b − /i b∇ /i /i b
(cid:16) (cid:17)
where the second line comes from Lbemma 3 anbd b b
1
J¯ := J(t)dt.
Z0
It is straightforward to use Lemma 3 to show that
0 λ (J¯) λ (J¯) 1.
min max
≤ ≤ ≤
On the other hand,
βα hα(βα) βα + hα(βα)
−∇ − /i ∇ /i /i
=βα βα ℓ˙ (βα) ℓ˙ (βα) x λ rα(βα) rα(βα) ℓ˙(βα)x
b − /i−b bj − j b/i j − ∇ −∇ /i − i /i i
j X∈[n]h i h i
b b b b b b b
= I X⊤diag[ℓ¨(ξ )] X λ 2rα(Ξ) (βα βα) ℓ˙(βα)x (15)
− j j j∈[n] − ∇ − /i − i /i i
(cid:16) (cid:17)
where the second line uses the definition of hα andbthe thbird line ubses mean-value-theorem
on ℓ˙ and rα:
j
∇/ 23
1
ℓ¨(ξ ) := ℓ¨(tβα+(1 t)βα)dt
j j j − /i
Z0
with b b
ℓ˙ (βα) ℓ˙ (βα)= ℓ¨(ξ )x⊤(βα βα)
j − j /i j j j − /i
and likewise
b b b b
1
2rα(Ξ) := 2rα(tβα+(1 t)βα)dt
∇ ∇ − /i
Z0
with b b
rα(βα) rα(βα) = 2rα(Ξ)(βα βα).
∇ −∇ /i ∇ − /i
Inserting this back to (14) we have
b b b b
βα βα = G−1J¯ ℓ˙(βα)x
− /i − i /i i
(cid:16) (cid:17)
where
b b b
G:= I+J¯ X⊤diag[ℓ¨(ξ )] X+λ 2rα(Ξ) I
j j j∈[n]
∇ −
(cid:16) (cid:17)
= I J¯+J¯ X⊤diag[ℓ¨(ξ )] X +λ 2rα(Ξ) .
− /i j j j6=i /i ∇
(cid:16) (cid:17)
Since I J¯ is positive semidefinite (note that all the eigenvalues of J¯ are in [0,1] ) and
X⊤diag− [ℓ¨(ξ )] X+λ 2rα(Ξ) is positive definite (due to the ridge component), G is
j j j∈[n]
∇
also positive definite. Hence, we have
βα βα [σ (G)]−1σ (J¯) ℓ˙(βα)x . (16)
− /i 2 ≤ min max i /i i 2
(cid:13) (cid:13) (cid:13) (cid:13)
We have already esta(cid:13) (cid:13)bblishedbσ m(cid:13) (cid:13)ax(J¯) 1. Now we bound(cid:13) (cid:13)σ mb in(G):(cid:13) (cid:13)
≤
σ (G) = λ (G)
min min
= 1+λ J¯ X⊤diag[ℓ¨(ξ )] X +λ 2rα(Ξ) I
min /i j j j∈/Ii /i ∇ −
:= 1+λ
(cid:16) (J¯M(cid:16)
)
(cid:17)(cid:17)
min
where M := X⊤diag[ℓ¨(ξ )] X +λ 2rα(Ξ) I. Observe that due to the existence of
/i j j j∈/Ii /i ∇ −
the ridge component in r, we have
λ (M) 2λη 1.
min
≥ −
• If λ (M) 0, then we have
min
≥
λ J¯M λ (J¯)λ (M) 0.
min min min
≥ ≥
• If 2λη 1 λ (M) < 0, the(cid:0) n we(cid:1) have
min
− ≤
λ J¯M λ (J¯)λ (M) 2λη 1.
min max min
≥ ≥ −
(cid:0) (cid:1)/ 24
Therefore we have
σ (G) 1+(2λη 1) 0 = 2λη 1.
min
≥ − ∧ ∧
inserting this back to (16) we have
ℓ˙(βα)x
i /i i
βα βα .
− /i ≤ (cid:13)2λη 1 (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) b ∧ (cid:13)
(cid:13) (cid:13)
Thenext step of theproof is to u(cid:13)sebthisbup(cid:13)perboundon thesmoothed estimates andobtain
an upper bound for β β . Towards this goal, we first prove the following lemma:
/i
−
(cid:13) (cid:13)
Lemma 10. Under(cid:13)assumpti(cid:13)ons A1-A4, we have that
b b
(cid:13) (cid:13)
2(1 η)
βα β − rα r ,
− ≤ s η k 0 − 0 k∞
(cid:13) (cid:13)
(cid:13) (cid:13)
b b
and similarly (cid:13) (cid:13)
2(1 η)
βα β − rα r .
/i − /i ≤ s η k 0 − 0 k∞
(cid:13) (cid:13)
(cid:13) (cid:13)
b b
Proof. We have (cid:13) (cid:13)
hα(β) hα(βα)= hα(β) h(βα)+h(βα) hα(βα)
− − −
hα(β) h(β)+h(βα) hα(βα) 2λ(1 η) rα r (17)
≤ b − b b − b ≤ b − kb0 − 0 k∞
where the first inequalbity usesbthe fabct that β,bβα are the minimizers of h(β) and hα(β)
respectively, and the last inequality uses the definition of h,hα. On the other hand we also
have from the second order mean value theorebmb(or Taylor expansion) that
hα(β) hα(βα)
−
1
= hα(βα)⊤(β βα)+ (β βα)⊤ 2hα(Ξ)(β βα)
b b
∇ − 2 − ∇ −
1
(β bβα)⊤ b 2hbα(Ξ)(β bβα)b b b
≥2 − ∇ −
λη β βα 2. (18)
b b b b
≥ k − k
In the first equality, by ba slibght abuse of the notation, we have written 2hα(Ξ) as the
∇
Hessian in the second order Taylor expansion.‡ To obtain the second line, observe that
∂
h (βα)⊤(β βα) = hα((1 t)βα+tβ) .
α
∇ − ∂t −
(cid:12)t=0
(cid:12)
b b b b b (cid:12)
Sinceβα istheminimizerofhα,t = 0istheminimizerofhα((1 t)β(cid:12) α+tβ)fort [0,1].And
− ∈
sincehα issmoothon(0,1)wemusthave h (βα)⊤(β βα)= ∂ hα((1 t)βα+tβ)
b ∇ α − ∂t b b− t=0 ≥
0. The last line of (18) is because 2hα(Ξ) is positive-definite and σ ( 2hα(Ξ)) (cid:12) 2λη
∇ b b b min ∇b b ≥(cid:12)
due to the existence of the ridge component. (cid:12)
‡Sincethemean-valuetheorem doesn’t exist forvector-valuedfunctions,thematrix ∇2hα(Ξ)isactually
theHessian ofhα,with each rowevaluatedat adifferent convexcombination ofβb andβbα,asthematrix Ξ
indicates./ 25
Combining (17) and (18) we have
2 2(1 η)
β βα − rα r .
− ≤ η k 0 − 0 k∞
(cid:13) (cid:13)
Using a similar argument we(cid:13) bcan abls(cid:13)o prove that
(cid:13) (cid:13)
2 2(1 η)
β βα − rα r .
/i − /i ≤ η k 0 − 0 k∞
(cid:13) (cid:13)
(cid:13) (cid:13)
b b
(cid:13) (cid:13)
From lemma 1 we can see that rα r 0 as α .
k 0 − 0 k∞ → → ∞
Continuing with the proof of Lemma 4, by Lemma 10, βα β and βα β when
→ /i → /i
α . Hence, we can let α and have
→ ∞ → ∞
b b b b
ℓ˙(β )x
i /i i
β β .
− /i ≤ (cid:13) 2λη 1(cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) b ∧ (cid:13)
(cid:13) (cid:13)
b b
(cid:13) (cid:13)
D.4. Proof of Lemma 5
We first obtain an upper bound for the 4th moment. Using Assumption A5, we have
Eφ˙4(β) E 1+ y s+ x⊤β s 4
0 ≤ | 0 | | 0 |
q q
27(cid:0)(1+E y 4s +E x⊤(cid:1)β 4s)
≤ | 0 | | 0 |
q
2s
C
27 1+C (4s)+(4s 1)!! X β 2
≤ v Y − p k k
u (cid:18) (cid:19) !
u
t C s
27(1+C (4s))+ 27(4s 1)!!Cs X β 2 .
≤ Y − X p k k
(cid:18) (cid:19)
p p
where the second line uses the simple equation (a+b+c)4 27a4+27b4+27c4, the third
≤
line uses x⊤β N(0,β⊤Σβ) with β⊤Σβ CX β 2. It also uses the moment formula for
0 ∼ ≤ p k k
standard Gaussian variable. The last line uses the equation √a+b √a+√b for a,b 0.
≤ ≥
Define C2 = max 27(1+C (4s)), 27(4s 1)!!Cs , then
φ { Y − X}
p p C s
Eφ˙4(β) C2 1+ X β 2
0 ≤ φ p k k
q (cid:18) (cid:18) (cid:19) (cid:19)
For the second moment, notice that
Eφ˙2(β) [Eφ˙4(β)]1
4
0 ≤ 0
q
s
C
C 1+ X β 2
φ
≤ s p k k
(cid:18) (cid:19)
s/2
C
C 1+ X β 2 .
φ
≤ p k k
!
(cid:18) (cid:19)/ 26
where the last step uses √a+b √a+√b again. The same arguments lead to the same
≤
bound for Eφ2(β).
0
p
D.5. Proof of Lemma 6
Proof. (a) Without loss of generality we assume C = 1 in Assumption A5. Throughout
this proof we use h(β),h (β),hα(β) and hα(β) for the loss functions of the corre-
/i /i
sponding models, defined as:
n
h(β) := ℓ (β)+λ(1 η)r (β)+ληβ⊤β,
j 0
−
j=1
X
n
hα(β) := ℓ (β)+λ(1 η)rα(β)+ληβ⊤β,
j − 0
j=1
X
n
h (β) := ℓ (β)+λ(1 η)r (β)+ληβ⊤β,
/i j 0
−
j6=i
X
n
hα(β) := ℓ (β)+λ(1 η)rα(β)+ληβ⊤β.
/i j − 0
j6=i
X
It is straightforward to show that
n
λη β 2 ℓ(y ,x⊤β)+λ(1 η)r (β)+λη β 2 ℓ(y ,0),
k k2 ≤ j j − 0 k k2 ≤ j
j∈[n] j=1
X X
b b b b
where the last inequality is due to the fact that h(β) h(0) and that ℓ(y,z) 0.
Similarly we have λη β 2 n ℓ(y ,0). ≤ ≥
k /i k2 ≤ j=1 j
We can then use Assumption A5 to obtain b
P
b
t
1
E β 2 (pλη)−tE[n+ y s + + y s]t.
1 n
pk k ≤ | | ··· | |
(cid:18) (cid:19)
By Rosenthal inequality (bLemma 9), we have
t
1
E β 2 (pλη)−tA(t)max nt+nE y st,(n+nE y s)t
1 1
pk k ≤ | | | |
(cid:18) (cid:19)
t (cid:8) (cid:9)
b γ 0 A(t)max 1+n1−tC (st),(1+C (s))t
Y Y
≤ λη
(cid:18) (cid:19)
t (cid:8) (cid:9)
γ
0 A(t)max 1+C (st),(1+C (s))t := C (t), (19)
Y Y β
≤ λη
(cid:18) (cid:19)
(cid:8) (cid:9)
where C () is the constant in Assumption A5, and A() is the Rosenthal constant in
Y
· ·
Lemma 9.
(b) Our next goal is to obtain an upper bound for Eℓ˙8(β ). By using Assumption A5 we
1 /1
have
b
Eℓ˙8(β ) E[1+ y s+ x⊤β s]8
1 /1 ≤ | 1 | | 1 /1 |
37 1+E y 8s+E x⊤β 8s . (20)
b ≤ | 1 | b| 1 /1 |
(cid:16) (cid:17)
b/ 27
Next we bound E x⊤β 8s:
| 1 /1 |
bE x⊤β 8s = E E[x⊤β 8s β ]
| 1 /1 | | 1 /1 | | /1
E[h (8s 1)!!(β⊤Σβi )4s]
b ≤ − b /1b /1
4s
C
(8s 1)!!E b X βb 2
≤ − p k /1 k
(cid:18) (cid:19)
≤
(8s −1)!!C X4sC β(4sb)
where the second line uses the fact that x⊤β β N(0,β⊤Σβ ) and that the
1 /1 | /1 ∼ /1 /1
tth moment of a standard Gaussian variable is (t 1)!! whenever t is a positive even
b b− b b
number. The last line uses (19). Inserting this back to (20) we have:
Eℓ˙8(β ) 37(1+C (8s)+(8s 1)!!C4sC (4s)) := C .
1 /1 ≤ Y − X β ℓ
b
D.6. Proof of Lemma 7
Recall that
n
1
LO= φ (β ).
j /i
n
i=1
X
b
Then we have
n 2
1
EV2 =E LO E[φ (β ) ]
1 − n i /i |D/i
!
i=1
X
1 b 2
= E φ (β ) E[φ (β ) ]
n 1 /1 − 1 /1 |D/1
n(cid:16) 1 (cid:17)
+ − E b φ (β ) bE[φ (β ) ] φ (β ) E[φ (β ) ] . (21)
1 /1 1 /1 /1 2 /2 2 /2 /2
n − |D · − |D
(cid:16) (cid:17) (cid:16) (cid:17)
Note that b b b b
2
E φ (β ) E(φ (β ) )
1 /1 1 /1 /1
− |D
=
Evh a(cid:16)
r(φ (β ) )
(cid:17)i
1b /1 |D/1 b
E E[φ2(β ) ]
≤ 1 b/1 |D/1
(cid:16) (cid:17) s 2
E C +b C 1 β 2 2
φ φ /1
≤ pk k
!
(cid:18) (cid:19)
s
1 b
2C2+2C2E β 2
≤ φ φ pk /1 k
(cid:18) (cid:19)
≤
2C φ2(1+C β(s)) :=b C
v1,1
(22)
where the fourth line uses Lemma 5, and the last line uses part (a) of Lemma 6. Next, we
study
E φ (β ) E(φ (β ) ) φ (β ) E(φ (β ) ) . (23)
1 /1 1 /1 /1 2 /2 2 /2 /2
− |D · − |D
(cid:16) (cid:17) (cid:16) (cid:17)
b b b b/ 28
Define β := argmin ℓ(y ,x⊤β)+λr(β) . By the mean-value theorem, there exists
/12 j≥3 j j
β∈Θ
n o
a random variable t [0P,1] such that
b ∈
φ (β ) = φ (β )+φ˙ (tβ +(1 t)β )x⊤(β β ).
1 /1 1 /12 1 /1 − /12 1 /1 − /12
Hence, b b b b b b
E[φ (β ) ]
1 /1 /1
|D
= E[φ (β ) ]+E[φ˙ (tβ +(1 t)β )x⊤(β β ) ]
1 b/12 |D/1 1 /1 − /12 1 /1 − /12 |D/1
= E[φ (β ) ]+E[φ˙ (tβ +(1 t)β )x⊤ ](β β ).
0 b/12 |D/12 0
b
/1
− b
/12 0|bD/1 b/1
−
/12
Similarly, we have b b b b b
φ (β ) = φ (β )+φ˙ (tβ +(1 t)β )x⊤(β β ).
2 /2 2 /12 2 /2 − /12 2 /2 − /12
and b b b b b b
E[φ (β ) ]= E[φ (β ) ]+E[φ˙ (tβ +(1 t)β )x⊤ ](β β )
2 /2 |D/2 0 /12 |D/12 0 /2
−
/12 0|D/2 /2
−
/12
Then (23)bcan be decomposebd into four terms: b b b b
E φ (β ) E(φ (β ) ) φ (β ) E(φ (β ) )
1 /1 1 /1 /1 2 /2 2 /2 /2
− |D · − |D
(cid:16) (cid:17) (cid:16) (cid:17)
=A +B +C +D , (24)
1 b1 1 1b b b
where A ,B ,C , and D are defined as
1 1 1 1
A := E φ (β ) E[φ (β ) ] φ (β ) E[φ (β ) ]
1 1 /12 0 /12 /12 2 /12 0 /12 /12
− |D − |D
(cid:16) (cid:17)(cid:16) (cid:17)
B := E φ (β ) E φ (β )
1 1 b/12 0 b/12 /12 b b
− |D
h(cid:16) (cid:17)
φ˙ (β +(1 t(cid:2))β )x⊤(β (cid:3) β ) E[φ˙ (β +(1 t)β )x⊤(β β ) ]
× 2 /b2 − /1b2 2 /2 − /12 − 2 /2 − /12 2 /2 − /12 |D/2
(cid:16) (cid:17)i
C := E φ (β ) E[φ (β ) ]
1 2b /12 0b /12 b/12 b b b b b
− |D
h(cid:16) (cid:17)
φ˙ (β +(1 t)β )x⊤(tβ β ) E φ˙ (tβ +(1 t)β )x⊤(β β )
× 1 /b1 − /1b2 1 /1 − /12 − 1 /1 − /12 1 /1 − /12 |D/1
(cid:16) h i(cid:17)i
D := E φ˙b (tβ +(1 b t)β )x⊤b (β b β ) E φ˙ (b tβ +(1 tb )β )x⊤b (β b β )
1 1 /1 − /12 1 /1 − /12 − 1 /1 − /12 1 /1 − /12 |D/1
(
(cid:18) (cid:19)
(cid:2) (cid:3)
b b b b b b b b
φ˙ (β +(1 t)β )x⊤(β β ) E φ˙ (β +(1 t)β )x⊤(β β ) .
× 2 /2 − /12 2 /2 − /12 − 2 /2 − /12 2 /2 − /12 |D/2
)
(cid:18) (cid:19)
(cid:2) (cid:3)
b b b b b b b b
We bound each of these four terms below. For A we have
1
A := E φ (β ) E[φ (β ) ] φ (β ) E[φ (β ) ]
1 1 /12 0 /12 /12 2 /12 0 /12 /12
− |D − |D
(cid:16) (cid:17)(cid:16) (cid:17)
= E E φ (β ) E[φ (β ) ] φ (β ) E[φ (β ) ]
b1 /12 b0 /12 /12 b 2 /12 b 0 /12 /12 /12
− |D − |D D
h h(cid:16) (cid:17) (cid:16) (cid:17)(cid:12) ii
= 0. (25)
(cid:12)
b b b b
(cid:12)/ 29
where the last equality is correct, because given , φ (β ) and φ (β ) are condition-
/12 1 /12 2 /12
D
ally independent, and hence the inner expectation can be taken to each term separately.
Similarly, b b
B := E φ (β ) E φ (β )
1 1 /12 0 /12 /12
− |D
h(cid:16) (cid:17)
φ˙ (β +(1 t(cid:2))β )x⊤(β (cid:3) β ) E[φ˙ (β +(1 t)β )x⊤(β β ) ]
× 2 /b2 − /1b2 2 /2 − /12 − 2 /2 − /12 2 /2 − /12 |D/2
(cid:16) (cid:17)i
= E E φ (β ) E φ (β )
b1 /12 b 0 /12b /12b b b b b
− |D
n h(cid:16) (cid:17)
φ˙ (β +(1 t)β(cid:2) )x⊤(β (cid:3)β ) E[φ˙ (β +(1 t)β )x⊤(β β ) ]
× 2 /2 b − /12 b2 /2 − /12 − 2 /2 − /12 2 /2 − /12 |D/2 D/2
(cid:16) (cid:17)(cid:12) io
= 0. (26)
(cid:12)
b b b b b b b b
(cid:12)
Again, the last equality is correct, because given , φ (β ) and φ˙ (β +(1 t)β )x
/2 1 /12 2 /2 /12 2
D −
are conditionally independent. Similarly,
b b b
C := E φ (β ) E[φ (β ) ]
1 2 /12 0 /12 /12
− |D
h(cid:16) (cid:17)
φ˙ (β +(1 t)β )x⊤(tβ β ) E φ˙ (tβ +(1 t)β )x⊤(β β )
× 1 /b1 − /1b2 1 /1 − /12 − 1 /1 − /12 1 /1 − /12 |D/1
(cid:16) h i(cid:17)i
= 0. (27)
b b b b b b b b
Finally,
D := E φ˙ (tβ +(1 t)β )x⊤(β β ) E φ˙ (tβ +(1 t)β )x⊤(β β )
1 1 /1 − /12 1 /1 − /12 − 1 /1 − /12 1 /1 − /12 |D/1
(
(cid:18) (cid:19)
(cid:2) (cid:3)
b b b b b b b b
φ˙ (β +(1 t)β )x⊤(β β ) E φ˙ (β +(1 t)β )x⊤(β β )
× 2 /2 − /12 2 /2 − /12 − 2 /2 − /12 2 /2 − /12 |D/2
)
(cid:18) (cid:19)
(cid:2) (cid:3)
(a) b b b b b b b b
Evar φ˙ (tβ +(1 t)β )x⊤(β β )
≤ 1 /1 − /12 1 /1 − /12 |D/1
h i
E E φ˙2(tβ +(1 t)β )[x⊤(β β )]2
≤ 1 b/1 − b/12 1 b/1 − b/12 |D/1
(b) h h ii
E E[φ˙4(bξ ) ] Eb[x⊤(β b β b)]4
≤ 1 1 |D/1 1 /1 − /12 |D/1
(cid:20)q q (cid:21)
(c) 1
s(cid:2)
√3C
(cid:3)
E C2 1+ ξ 2 X β β 2
≤ φ pk k · p k /1 − /12 k
" #
(cid:18) (cid:18) (cid:19) (cid:19)
(d) √3C φ2C X E 1+ p−1 ξ 2 s ℓ˙b (β )b 2 x 2
≤ p(2λη 1)2 k k | 2 /12 | k 2 k
∧ h i
(e) √3C φ2C X E(cid:0) (1+(cid:0) (p−1 ξ 2(cid:1) )s(cid:1) )2 b E ℓ˙ (β )8 41 E x 8 41
≤ p(2λη 1)2 k k · | 2 /12 | k 2 k
∧ q (cid:16) (cid:17)
(f) √3γ 0C φ2C X 2+2C (2s)C1 4(24C4 )41 b (cid:0) (cid:1)
≤ n(2λη 1)2 β ℓ X
∧ q
C
v1,2
:= . (28)
n
where inequality (a) uses Cauchy Schwarz inequality and symmetry between φ˙ and φ˙ ,
1 2
(b) uses Cauchy Schwarz again, (c) uses Lemma 5 and the fact that, conditioned on ,
D/ 30
x⊤(β β ) N(0,β β⊤ Σβ β ). Inequality (d) uses Lemma 4. Inequality (e)
1 /1 − /12 ∼ /1 − /12 /1 − /12
uses Cauchy Schwarz twice. Finally inequality (f) uses Lemma 6 and Lemma 8. Plugging
in thbe resublts of equatiobns (25b)-(28)binto (b24), and then using (22), the bound in (21) boils
down to
C +C C
EV2 v1,1 v1,2 := v1
1 ≤ n n
where, after some simplification:
1 1
C = 2C2(1+C (s))+
√6(24)4C φ2C X2C ℓ4γ
0 1+C (2s). (29)
v1 φ β (2λη 1)2 β
∧ q
This finishes the proof of Lemma 7.