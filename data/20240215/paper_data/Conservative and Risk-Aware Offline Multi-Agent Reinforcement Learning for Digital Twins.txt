1
Conservative and Risk-Aware Offline Multi-Agent
Reinforcement Learning for Digital Twins
Eslam Eldeeb, Houssem Sifaou, Osvaldo Simeone, Mohammad Shehab, and Hirley Alves
Abstract—Digital twin (DT) platforms are increasingly re-
DigitalTwin
garded as a promising technology for controlling, optimizing,
and monitoring complex engineering systems such as next- Policies
𝒟 𝝅
generationwirelessnetworks.Animportantchallengeinadopting
DT solutions is their reliance on data collected offline, lacking
direct access to the physical environment. This limitation is Offline Training
particularlysevereinmulti-agentsystems,forwhichconventional
multi-agent reinforcement (MARL) requires online interactions
with the environment. A direct application of online MARL 𝑠,𝑟 𝑠,𝑟
𝝅𝟏 𝝅𝟏
schemes to an offline setting would generally fail due to the 𝜷
epistemicuncertaintyentailedbythelimitedavailabilityofdata.
In this work, we propose an offline MARL scheme for DT- ⋮ Training data set ⋮
based wireless networks that integrates distributional RL and 𝝅𝑰 𝒟= 𝑠𝑡,𝑎𝑡,𝑟𝑡,𝑠𝑡+1 𝝅𝑰
conservative Q-learning to address the environment’s inherent 𝑎 𝜷 𝑎
aleatoric uncertainty and the epistemic uncertainty arising from Data Collection Physical Twin Deployment
limited data. To further exploit the offline data, we adapt
the proposed scheme to the centralized training decentralized
execution framework, allowing joint training of the agents’
policies.TheproposedMARLscheme,referredtoasmulti-agent Fig. 1: A digital twin of a physical system may only have access to data
conservative quantile regression (MA-CQR) addresses general
collectedofflinefollowingsomefixedandunknownpoliciesπ
β
={π βi}I
i=1
atthephysicaltwinconsistingofI agents.Basedonthisdataset,thedigital
risk-sensitive design criteria and is applied to the trajectory
twinwishestooptimizepoliciesπ={πi}I forthephysicalsystemwhile
planningproblemindronenetworks,showcasingitsadvantages. i=1
ensuringrobustnesstotheuncertaintyarisingfromthestochasticenvironment,
Index Terms—Digital twins, offline multi-agent reinforcement fromthelimiteddata,andfromthelackofinteractionswiththeenvironment.
learning, distributional reinforcement learning, conservative Q-
learning, UAV networks
physical systems: digital twin (DT) platforms [2], [3]. A
I. INTRODUCTION DT system consists of a digital mirror of the physical twin
A. Context and Motivation (PT) counterpart built and maintained using data from the
PT. Originating in manufacturing, DTs have emerged as a
Recent advances in machine learning (ML), high-
promising solution for fields such as healthcare and next-
performance computing, cloudification, and simulation intel-
generation wireless networks, allowing algorithms and ML
ligence [1] have supported the development of a novel data-
modelstobeoptimizedandtestedvirtuallybeforedeployment
driven paradigm for the design, monitoring, and control of
onthePT[4],[5].Inthecontextofwirelessnetworks,DTsare
Eslam Eldeeb and Hirley Alves are with Centre for Wireless Communi- particularly well suited as a component in open radio access
cations (CWC), University of Oulu, Finland. (e-mail: eslam.eldeeb@oulu.fi; network architectures (RANs) [6], [7].
hirley.alves@oulu.fi). Houssem Sifaou and Osvaldo Simeone are with the
The DT generally updates its internal representation of the
King’s Communications, Learning & Information Processing (KCLIP) lab
within the Centre for Intelligent Information Processing Systems (CIIPS), PTandtheMLmodelsbeingtrainedonbehalfofthePTbased
Department of Engineering, King’s College London, WC2R 2LS London, on data received from the PT. The process of synchronizing
U.K. (e-mail: houssem.sifaou@kcl.ac.uk; osvaldo.simeone@kcl.ac.uk). Mo-
the DT is limited by the communication bottleneck between
hammad Shehab is with CEMSE Division, King Abdullah University of
ScienceandTechnology(KAUST),Thuwal23955-6900,SaudiArabia(email: DT and PT [7], [8]. As a result, an important challenge
mohammad.shehab@kaust.edu.sa). in adopting DT solutions is their reliance on data collected
The work of E. Eldeeb and H. Alves was partially supported by the
offline, lacking direct access to the physical system. This
Research Council of Finland (former Academy of Finland) 6G Flagship
Programme (Grant Number: 346208) and by the European Commission limitation is particularly severe in multi-agent systems, for
throughtheHexa-X-II(GAno.101095759).TheworkofH.SifaouandO. which conventional multi-agent reinforcement (MARL) re-
Simeone was partially supported by the European Union’s Horizon Europe
quires online interactions with the environment [9].
project CENTRIC (101096379). O. Simeone was also supported by the
Open Fellowships of the EPSRC (EP/W024101/1) by the EPSRC project A direct application of online MARL schemes to an offline
(EP/X011852/1),andbyProjectREASON,aUKGovernmentfundedproject
setting would generally fail due to the epistemic uncertainty
undertheFutureOpenNetworksResearchChallenge(FONRC)sponsoredby
theDepartmentofScienceInnovationandTechnology(DSIT). entailed by the limited availability of data. In particular, even
The second author has contributed to the problem definitions and the in the case of a single agent, offline reinforcement learning
experiments.Thethirdauthorhashadanactiveroleindefiningtheproblems,
(RL) may over-estimate the quality of given actions that
aswellasinwritingthetext,whilethelasttwoauthorshavehadasupervisory
roleandhaverevisedthetext. happened to perform well during data collection due to the
4202
beF
31
]GL.sc[
1v12480.2042:viXra2
inherent stochasticity and outliers of the environment [10]. A popular risk measure for use in DRL is the conditional
This problem can be addressed in online RL via exploration, value at risk (CVaR) [20]–[23], which evaluates the average
trying actions, and modifying return estimates based on envi- performance by focusing only on the lower tail of the return
ronmental feedback. However, exploration is not feasible in distribution. Furthermore, a state-of-the-art DRL strategy is
offline RL, as policy design is based solely on the offline quantile-regression deep Q-network (QR-DQN), which ap-
dataset. Furthermore, in multi-agent systems, this problem is proximates the return distribution by estimating N uniformly
exacerbated by the inherent uncertainty caused by the non- spaced quantiles [19].
stationarybehaviorofotheragentsduringtraining[11,Chapter
11]. C. Related Work
Inthispaper,weproposeanovelofflineMARLstrategythat Offline MARL: Offline MARL solutions have been proposed
addressestheoveralluncertaintyattheDTabouttheimpactof inseveralpreviousworksbyadaptingtheideaofconservative
actionsonthePT.Theintroducedapproachesaretermedmulti- offline learning to the context of multi-agent systems [24]–
agent conservative independent quantile regression (MA- [28]. Specifically, conservative estimates of the value function
CIQR) via independent learning and multi-agent conservative in a decentralized fashion are obtained in [24] via value
centralized quantile regression (MA-CCQR) via centralized deviation and transition normalization. Several other works
training and decentralized execution. These aproaches inte- proposed centralized learning approaches. The authors in [25]
grate distributional RL [11] and offline RL [12] to support leveraged first-order policy gradients to calculate conserva-
a risk-sensitive multi-agent design that mitigates impairments tive estimates of the agents’ value functions. The work [26]
arising from access to limited data from the PT. We showcase presented a counterfactual conservative approach for offline
the performance of MA-CIQR and MA-CCQR by focusing MARL, while [27] introduced a framework that converts
on the problem of designing trajectories of unmanned aerial global-level value regularization into equivalent implicit local
vehicles (UAVs) used to collect data from sensors in an value regularization. The authors in [28] addressed the over-
Internet-of-Things (IoT) scenario [13]–[15] (see Fig. 1). estimation problem using implicit constraints.
Overall, all of these works focused on risk-neutral objec-
B. Offline RL and Distributional RL
tives,hencenotmakinganyprovisionstoaddressrisk-sensitive
OfflineRLhasgainedincreasinginterestinrecentyearsdue criteriasuchasCVaR.Inthisregard,thepaper[23]combined
to its wide applicability to domains where online interaction distributional RL and conservative Q-learning to develop a
with the environment is impossible or presents high costs and risk-sensitive algorithm, but only for single-agent settings.
risks. Offline RL relies on a static offline transition dataset Regarding applications of offline RL to wireless systems,
collected from the PT using some behavioral policy. The therecentwork[29]investigatedaradioresourcemanagement
behavioralpolicyisgenerallysuboptimalandmaybeunknown problembycomparingtheperformanceofseveralsingle-agent
to the designer [10]. The reliance on a suboptimal policy offline RL algorithms.
for data collection distinguishes offline RL from imitation Applications of MARL to wireless systems: Due to the multi-
learning, in which the goal is reproducing the behavior of objectiveandmulti-agentnatureofmanycontrolandoptimiza-
an expert policy [16]. The discrepancy between the behavior tion problems in wireless networks, MARL has been adopted
and optimized policies creates a distributional shift between asapromisingsolutioninrecentyears.Forinstance,relatedto
trainingdataanddesignobjective.Thisshiftcouldberesolved our contribution, the work in [30] proposed an online MARL
by collecting more data, but this is not possible in an offline algorithmtojointlyminimizetheage-of-information(AoI)and
setting. Therefore, the distributional shift contributes to the the transmission power in IoT networks with traffic arrival
epistemic uncertainty of the agent. prediction, whereas the authors in [31] leveraged MARL for
Several approaches have been proposed to address this AoI minimization in UAV-to-device communications. More-
problem in offline RL. One class of methods constrains the over, MARL was used in [32] for resource allocation in UAV
differencebetweenthelearnedandbehaviorpolicies[17].An- networks. The work [33] developed a MARL-based solution
otherpopularapproachistolearnconservativeestimatesofthe for optimizing power allocation dynamically in wireless sys-
action-value function or Q-function. Specifically, conservative tems.Theauthorsin[34]usedMARLfordistributedresource
Q-learning (CQL), proposed in [12], penalizes the values of managementandinterferencemitigationinwirelessnetworks.
the Q-function for out-of-distribution (OOD) actions. OOD ApplicationsofdistributionalRLtowirelesssystems:Distribu-
actions are those whose impact is not sufficiently covered by tional RL has been recently leveraged in [35] to carry out the
the data set. optimizationforadownlinkmulti-usercommunicationsystem
Apart from offline RL via CQL, the proposed scheme with a base station assisted by a reconfigurable intelligent re-
builds on distributional RL (DRL), which is motivated by the flector(IR).Meanwhile,reference[36]focusedonthecaseof
inherent aleatoric uncertainty caused by the stochasticity of mmWave communications with IRs on a UAV. Distributional
the environment [18]–[20]. Rather than targeting the average RL has also been used in [37] for resource management in
return as in conventional RL, DRL maintains an estimate of network slicing.
the distribution of the return. This supports the design of All in all, to the best of our knowledge, our work in this
risk-sensitive policies that disregard gains attained via risky paper is the first to integrate conservative offline RL and
behavior, favoring policies that ensure satisfactory worst-case distributional MARL, and it is also the first to investigate the
performance levels instead. application of offline MARL to wireless systems.3
TABLEI:Abbreviations
D. Main Contributions
AoI Age-of-information
This work introduces MA-CQR, a novel offline MARL CDF Cumulativedistributionfunction
scheme that supports optimizing risk-sensitive design criteria CQL ConservativeQ-learning
CTDE Centralizedtraininganddecentralizedexecution
such as CVaR. MA-CQR is evaluated on the relevant problem
CVaR Conditionalvalueatrisk
of UAV trajectory design for IoT networks. The contributions DQN DeepQ-networks
of this paper are summarized as follows. DRL Distributionalreinforcementlearning
DT Digitaltwin
• WeproposeMA-CQR,anovelconservativeanddistribu- MA-CCQL Multi-agentconservativecentralizedQ-learning
tional offline MARL solution. MA-CQR leverages quan- MA-CCQR Multi-agentconservativecentralizedquantileregression
MA-CIQL Multi-agentconservativeindependentQ-learning
tile regression (QR) to support the optimization of risk-
MA-CIQR Multi-agentconservativeindependentquantileregression
sensitive design criteria and CQL to ensure robustness MA-CQR Multi-agentconservativequantileregression
to OOD actions. As a result, MA-CQR addresses both MARL Multi-agentreinforcementlearning
OOD Out-of-distribution
the epistemic uncertainty arising from the presence of
PT Physicaltwin
limited data and the aleatoric uncertainty caused by the QR-DQN Quantile-regressionDQN
randomness of the environment. UAV Unmannedaerialvehicles
• We present two versions of MA-CQR with different
TABLEII:Notations
levels of coordination among the agents. In the first
version,referredtoasMA-CIQR,theagents’policiesare I Numberofagents
optimized independently. In the second version, referred
st OverallstateofthePTsystemattimestept
at Jointactionofallagentsattimestept
to as MA-CCQR, we leverage value decomposition tech- ai Actionofagentiattimestept
t
niques that allow centralized training and decentralized rt Immediaterewardattimestept
γ Discountfactor
execution [38].
Q(s,a) Q-function
• To showcase the proposed schemes, we consider atra- Z(s,a) Returnstartingfrom(s,a)
jectory optimization problem in UAV networks [30]. As P(st+1|st,at) Transitionprobability
illustratedinFig.1,thesystemcomprisesmultipleUAVs πi(ai t|st) Policyofagenti
P Distributionofthereturn
collecting information from IoT devices. The multi- Z(s,a)
R(rt|st,at) Stationaryrewarddistribution
objective design tackles the minimization of the AoI for ξ Risktolerancelevel
data collected from the devices and the overall transmit JCVaR CVaRriskmeasure
ξ
power consumption. We specifically exploit MA-CQR to F Z− π1(cid:0) ξ(cid:1) InverseCDFofthereturn
D OfflinedatasetcollectedatthePT
design risk-sensitive policies that avoid excessively risky
trajectories in the pursuit of larger average returns.
θ ji(s,a) QuantileestimateofthedistributionP Zi(s,a)(θi)
ζ (u) QuantileregressionHuberloss
τ
• Numerical results demonstrate that MA-CIQR and MA- ∆i(k) TDerrorsevaluatedwiththequantileestimates
jj′
CCQR versions yield faster convergence and higher re- ofagenti
turns than the baseline algorithms. Furthermore, both α CQLhyperparameter
M Numberofdevicesinthesystem
schemes can avoid risky trajectories and provide the
Am AoIofdevicemattimestept
t
best worst-case performance. Experiments also depict gi,m Channelgainbetweenagentianddevicem
t
that centralized training provides faster convergence and attimestept
requires less offline data. P ti,m Transmissionpowerfordevicemtocommunicatewith
agentiattimestept
The rest of the paper is organized as follows. Section II de- p risk Riskprobability
scribestheMARLsettingandthedesignobjective.SectionIII P risk Riskpenalty
introduces distributional RL and conservative Q-Learning. In
section IV, we present the proposed MA-CQR algorithm. In
setting and formulate the offline learning problem. Tables I
Section VI, we provide numerical experiments on trajectory
and II summarize the list of abbreviations and notations.
optimization in UAV networks. Section VII concludes the
paper.
A. Multi-Agent Setting
II. PROBLEMDEFINITION
Consider an environment characterized by a time-variant
WeconsiderthesettingillustratedinFig.1,whereI agents state s t, where t = 1,2,... is the discrete time index. At
act in a physical environment that evolves in discrete time time step t, each agent i takes action ai ∈ Ai within some
t
as a function of the agents’ actions and random dynamics. discrete action space Ai. We denote by a t = (cid:2) a1 t,··· ,aI t(cid:3)
The design of the agents’ policies π = {πi}I i=1 is carried the vector of actions of all agents at timestep t. The state s t
out at a central unit – the digital twin (DT) in Fig. 1 – that evolves according to a transition probability P(s t+1|s t,a t) as
has only access to a fixed dataset D, while not being able to a function of the current state s t and of the action vector a t.
interact with the physical system. The dataset D is collected The transition probability P(s t+1|s t,a t) is stationary, i.e., it
offline by allowing the agents to act in the environment does not vary with time index t.
according to arbitrary, fixed, and generally unknown policies We focus on a fully observable multi-agent reinforcement
π = {πi}I . In this section, we describe the multi-agent learning setting, in which each agent i has access to the full
β β i=14
system state s and produces action ai by following a policy
t t
πi(ai |s ).
t t
B. Design Goal
As illustrated in Fig. 1, the DT aims at finding the optimal
policiesπ (a|s)={πi(a|s)}I thatmaximizeariskmeasure CVaR[Z]
∗ ∗ i=1
ρ(·) of the return Zπ =(cid:80)∞ γtr , which we write as
t=0 t
J (π)=ρ[Zπ], (1)
ρ
where 0 < γ < 1 is a given discount factor. The distribution
of the return Zπ depends on the policies π through the
distribution of the trajectory T = (s ,a ,r ,s ,a ,r ,...),
0 0 0 1 1 1
which is given by
∞
(cid:89)
P(T)=P(s ) π(a |s )R(r |s ,a )P(s |s ,a ),
0 t t t t t t+1 t t Fig. 2: Illustration of the conditional value-at-risk (CVaR). The quantile
t=0 function F−1(ξ) is plotted as a function of the risk tolerance level ξ. The
(2) shadedareaZ representingthelowertailofthedistributiondepictstheξ-level
with π(a |s )=(cid:81)I πi(ai |s ) being the joint conditional CVaR.
t t i=1 t t
distribution of the agents’ actions; P(s ) being a fixed initial
0
distribution; and R(r | s ,a ) being the stationary reward
t t t
distribution. C. Offline Multi-Agent Reinforcement Learning
The standard choice for the risk measure in (1) is the
Conventional MARL [39] assumes that agents optimize
expectation ρ[·]=E[·], yielding the standard criterion theirpoliciesπ ={πi(a|s)}I viaanonlineinteractionwith
i=1
theenvironment,allowingfortheexplorationofnewactionsa
Javg(π)=E(cid:2) Zπ(cid:3)
. (3)
t
asafunctionthestates .Inthispaper,asillustratedinFig.1,
t
we assume that the design of policies is carried out at the
The average criterion in (3) is considered to be risk neutral,
DT on the basis solely of the availability of an offline dataset
as it does not directly penalize worst-case situations, catering D = {(s,a,r,s′)} of transitions (s,a,r,s′). Each transition
only to the average performance.
follows the stationary marginal distribution from (2), with
In stochastic environments where the level of aleatoric policyπ(a|s)givenbythefixedandunknownbehaviorpolicy
uncertainty caused by the transition probability and/or the π (a|s)=(cid:81)I πi(ai|s).
β i=1 β
reward distribution is high, maximizing the expected return
may not be desirable since the return Zπ has high variance.
In such scenarios, designing risk-sensitive policies may be III. BACKGROUND
preferabletoenhancetheworst-caseoutcomeswhilereducing
the average performance (3). In this section, we present a brief review of distributional
RL, as well as of offline RL via CQL for a single agent
A common risk-sensitive measure is the conditional value-
model [12]. This material will be useful to introduce the
at-risk (CVaR) [21], which is defined as the conditional mean
proposedmulti-agentofflineDRLsolutioninthenextsection.
JCVaR(π)=E(cid:2) Zπ |Zπ ≤F−1(cid:0) ξ(cid:1)(cid:3) , (4)
ξ Zπ
where
F−1(cid:0) ξ(cid:1)
is the inverse cumulative distribution function
A. Distributional Reinforcement Learning
Zπ
(CDF) of the return Zπ for some ξ ∈ [0,1], i.e., the ξ-th
Distributional RL aims at optimizing the agent’s policy,
quantileofthedistributionofthereturn.TheCVaR,illustrated π, while accounting for the inherent aleatoric uncertainty
in Fig. 2, focuses on the lower tail of the return distribution
associated with the stochastic environment. To this end, it
by neglecting values of the return that are larger than the ξ-
tracks the return’s distribution, allowing the minimization of
th quantile
F−1(cid:0) ξ(cid:1)
. Accordingly, the probability ξ represents
Zπ an arbitrary risk measure, such as the CVaR.
the risk tolerance level, with ξ =1 recovering the risk-neutral
Toelaborate,letusdenotetherandomvariablerepresenting
objective (3). The CVaR can also be written as the integral of
the return starting from a given state-action pair (s,a) as
the quantile function F Z− π1(ξ) as Zπ(s,a). Taking the expectation of the return Zπ(s,a) over
distribution (2) yields the state-action value function, also
1(cid:90) ξ
JCVaR(π)= F−1(u)du. (5) known as Q-function, as
ξ ξ Zπ
0
Qπ(s,a)=E[Zπ(s,a)]. (6)
nruteR5
Classical Q-learning algorithms learn the optimal policy π∗ B. Conservative Q-Learning
by finding the optimal Q-function Q(s,a) as the unique fixed Conservative Q-learning (CQL) is a Q-learning variant that
point of the Bellman optimality operator [40]
addresses epistemic uncertainty in offline RL. Specifically, it
(cid:20) (cid:21)
tackles the uncertainty arising from the limited available data,
Q(s,a)=E r+γ maxQ(s′,a′) , (7)
a′∈A which may cause some actions to be OOD due to the lack of
exploration. This way, CQL is complementary to QR-DQN,
with average evaluated with respect to the random variables
(r,s′) ∼ R(r|s,a)P(s′|s,a). The optimal policy π∗ for the which,instead,targetstheinherentaleatoricuncertaintyinthe
stochastic environment.
average criterion (3) is directly obtained from the optimal Q-
To introduce CQL, let us first review conventional offline
function as
(cid:26) (cid:27) DQN [10], which approximates the solution of the Bellman
π∗(a|s)=1 a=arg max Q(s,a) , (8) optimalitycondition(7)byiterativelyminimizingtheBellman
a∈A
loss
with 1{·} being the indicator function. (cid:34)(cid:18) (cid:19)2(cid:35)
Similarly, for any risk measure ρ[·], one can define the L(Q,Qˆ(k))=Eˆ r+γmaxQˆ(k)(s′,a′)−Q(s,a) ,
distributional Bellman optimality operator for the random a′∈A
return Z(s,a) as [18], [19] (13)
(cid:18) (cid:19)
Z(s,a)=D r+γZ s′,arg max ρ[Z(s′,a′)] , (9) where Eˆ[·] is the empirical average over samples (s,a,r,s′)
a′∈A from the offline dataset D; Qˆ(k) is the current estimate of the
where equality holds regarding the distribution of the random optimal Q-function Q at iteration k; and the optimization is
variables on the left- and right-hand sides, and the random over function Q(s,a), which is typically modeled as a neural
variables (r,s′) are distributed as in (7). The optimal policy network.Thetermr+γmax Qˆ(k)(s′,a′)−Q(s,a)isalso
a′∈A
for the general criterion (1) can be expressed directly as a known as the TD-error.
function of the optimal Z(s,a) in (9) as [18], [19] The maximization over the actions in the TD error in
(cid:26) (cid:27) (13) may yield over-optimistic return estimates when the Q-
π(a|s)=1 a=arg maxρ[Z(s,a)] . function is estimated using offline data. In fact, a large value
a∈A
of the estimated maximum return max Q(s,a) may be
Quantile regression DQN (QR-DQN) [19] estimates the
a′∈A
obtained based purely on the randomness in the environment
distribution P of the optimal return Z(s,a) by approx-
Z(s,a) during data collection. This uncertainty could be resolved by
imating it via a uniform mixture of Dirac functions centered
collecting additional data. However, this is not possible in an
at N values {θ (s,a)}N , i.e.,
j j=1 offline setting, and hence one should consider such actions as
Pˆ Z(s,a)(θ)=
N1 (cid:88)N
δ θj(s,a). (10)
O thO eD ep[ is1 t0 e] m, i[ c41 u] n, ca en rd tac ino tu yn ot fth te here Dsu Tl .ting uncertainty as part of
j=1 To account for this issue, the CQL algorithm adds a
Each value θ (s,a) in (10) is an estimate of the quantile regularization term to the objective in (13) that penalizes
j
F−1 (τˆ ) of distribution P corresponding to the quan- excessively large deviations between the maximum estimated
Z(s,a) j Z(s,a)
tile target τˆ j =(τ j−1−τ j)/2, with τ j =j/N for 1≤j ≤N. return max a′ (cid:80)∈AQ(s,a) (cid:0), approxi (cid:1)mated with the differentiable
Note that {θ (s,a)}N are estimated via quantile regression, quantity log exp Q(s,a˜) , and the average value of
j j=1 a˜∈A
which is achieved by modeling the function mapping (s,a) to Q(s,a) in the data set D as
tt ah ke eN
s
av sa tl au te es a{ sθ ij n( ps u, ta ,) a} nN j= d1 oa us tpa utsne tu hr eal esn te imtw ao terk
d
[ θ1 j9 (] s, ,aw )hi fc oh
r
L CQL(Q,Qˆ(k))= 21 L(Q,Qˆ(k)) (14)
all actions a∈A . (cid:20) (cid:21)
The neural network is trained by minimizing the loss +αEˆ log(cid:88) exp(cid:0) Q(s,a˜)(cid:1) − Q(s,a) ,
a˜∈A
N N
1 (cid:88)(cid:88) ζ (cid:0) ∆ (cid:1) , where α>0 is a hyperparameter [12].
N2 τˆj jj′
j=1j′=1 AcombinationofQR-DQNandCQLwasproposedin[23]
for a single-agent setting to address risk-sensitive objectives
where ∆ are the temporal difference (TD) errors corre-
jj′
inofflinelearning.Thisapproachappliesaregularizationterm
sponding to the quantile estimates, i.e.,
as in (14) to the distributional Bellman operator (9). The next
∆ =r+γθ (s′,a′)−θi(s,ai), (11)
jj′ j′ j section will introduce an extension of this approach for the
with a′ = arg max 1 (cid:80)N θ (s′,a), and ζ is the multi-agent scenario under study in this paper.
a∈A N j′=1 j′ τ
quantile regression Huber loss defined as
(cid:40) IV. OFFLINECONSERVATIVEDISTRIBUTIONALMARL
−1u2|τ −1{u<0}|, if |u|≤1
ζ (u)= 2 (12) WITHINDEPENDENTTRAINING
τ (cid:0) |u|− 1(cid:1) |τ −1{u<0}|, otherwise.
2 This section proposes a novel offline conservative distribu-
We refer the reader to [19] for more details about the tional independent Q-learning approach for MARL problems.
theoretical guarantees and practical implementation of QR- The proposed method combines the benefits of distributional
DQN. RL and CQL to address the risk-sensitive objective (1) in6
Algorithm1:ConservativeIndependentQ-learningfor the inherent stochasticity of the environment. This section
Offline MARL (MA-CIQL) introduces a risk-sensitive Q-learning algorithm for offline
Input: Discount factor γ, learning rate η, conservative MARL to address the more general design objective (4) for
penalty constant α, number of agents I, number of some risk tolerance level ξ.
training iterations K, number of gradient steps G, The proposed approach, which we refer to as multi-agent
and offline dataset D conservative independent quantile regression (MA-CIQR),
Output: Optimized Q-functions Qi(s,ai) for maintains an estimate of the lower tail of the distribution of
i=1,...,I the return Zi(s,a), up to the risk tolerance level ξ, for each
Initialize network parameters agent i. This is done in a manner similar to (10) by using N
for iteration k in {1,...,K} do estimated quantiles, i.e.,
for gradient step g in {1,...,G} do N
Sample a batch B from the dataset D Pˆ (θi)= 1 (cid:88) δ . (16)
for agent i in {1,...,I} do
Zi(s,a) N θ ji(s,a)
j=1
Estimate the MA-CIQL loss L
MA-CIQL Generalizing(10),however,thequantityθi(s,a)isanestimate
in (15) j
ofthequantileF−1 (τˆ ),withτˆ = τj−1−τj andτ =ξj/N
Perform a stochastic gradient step based on Zi(s,a) j j 2 j
for 1≤j ≤N. This way, only the quantiles of interest cover
the estimated loss
end the return distribution up to the ξ-th quantile.
end At each iteration k, for each agent, i, the DT updates the
end distribution (16) by minimizing a loss function that combines
Return Qi(s,ai)=Qˆi(K)(s,ai) for i=1,...,I the quantile loss used by QR-DQN and the conservative
penalty introduced by CQL. Specifically, the loss function of
MA-CIQR is given by
m inu Flt ii g-a .g 1e .n Tt hsy est ae pm prs ob aa cs he ed so sn tuo df ifl edine heo rp eti am pi pz la yti ao nna int dth epe eD ndT ea ns
t L MA-CIQR(θi,θˆi(k))=
2N1 2Eˆ(cid:88)N (cid:88)N
ζ
τˆj(cid:16)
∆i j( jk
′)(cid:17)
(17)
Q-learning approach, whereby learning is done separately for j=1j′=1
e ma ec th hoa dg sen bt a. seT dhe onne tx ht es ce ec nti to ran lizw ei dll trs atu ind iy ngm ao nr de dso ecp eh nis trti ac la izte ed
d
+αEˆ(cid:34) N1 (cid:88)N (cid:34)
log
(cid:88) exp(cid:0)
θ
ji(s,a˜i)(cid:1)
−θ
ji(s,ai)(cid:35)(cid:35)
,
execution (CTDE) framework. j=1 a˜i∈Ai
where ζ (u) is the quantile regression Huber loss defined in
τ
A. Multi-Agent Conservative Independent Q-Learning (12) and ∆i(k) are the TD errors evaluated with the quantile
jj′
estimates as
Wefirstpresentamulti-agentversionofCQL,referredtoas
multi-agentconservativeindependentQ-learning(MA-CIQL), ∆i(k) =r+γθˆi(k)(s′,a′i)−θi(s,ai), (18)
jj′ j′ j
for the offline MARL problem. As in its single-agent version
described in the previous section, MA-CIQL addresses the where a′i = arg max ai∈Ai N1 (cid:80)N j′=1θˆ ji( ′k)(s′,ai). Note that
averagecriterion(3),aimingtomitigatetheeffectofepistemic the TD error ∆i(k) is obtained by using the j′-th quantile
jj′
uncertainty caused by OOD actions. of the current k-th iteration? to estimate the return as r +
To this end, for each agent i, the DT maintains a separable γθˆi(k)(s′,a′i), while considering the j-th quantile θi(s,ai) as
j′ j
Q-function Qi(s,ai), which is updated at each iteration k by the quantity to be optimized.
approximately minimizing the loss The corresponding optimized policy is finally obtained as
 
L (Qi,Qˆi(k))= 1 L(Qi,Qˆi(k)) (15)  1 (cid:88)N 
MA-CIQL 2 πi(ai|s)=1 ai =arg max θi(s′,ai) . (19)
+αEˆ(cid:20) log(cid:18) (cid:88) exp(Qi(s,a˜i))(cid:19) −Qi(s,ai)(cid:21)
,
 ai∈Ai N j=1 j 
By (5), the objective in (19) is an estimate of the CVaR at
a˜i∈Ai
the risk tolerance level ξ. The pseudocode of the MA-CIQR
which is the multi-agent version of (14) over the Q-function
algorithmisprovidedinAlgorithm2.AsforMA-CIQL,MA-
Qi, where L(Qi,Qˆi(k)) is the offline DQN loss in (13) and
CIQR applies separately across all agents.
Qˆi(k) is the estimate of the Q-function of agent i at the k-th
iteration. Algorithm 1 summarizes the MA-CIQL algorithm V. OFFLINECONSERVATIVEDISTRIBUTIONALMARL
for offline MARL. Note that the algorithm applies separately WITHCENTRALIZEDTRAINING
toeachagentandisthusanexampleofindependentper-agent
The independent learning strategies studied in the previous
learning.
section may fail to yield coherent policies across different
agents. This section addresses this issue by introducing meth-
B. Multi-Agent Conservative Independent Quantile-
odsbasedonvaluedecomposition[38].Specifically,weadopt
Regression
the CTDE framework, which supports the optimization of
MA-CIQL can only target the average criterion (3), thus the agents’ policies based on a global loss, along with the
not accounting for risk-sensitive objectives that account for decentralized execution of the optimized policies.7
Algorithm 2: Conservative Independent Quantile Re- where the function Q˜i(s,ai) indicates the contribution of the
gression for Offline MARL (MA-CIQR) i-th agent to the overall Q-function. For conventional offline
Input: Discount factor γ, learning rate η, number of DQN, the Bellman loss (13) is minimized over the functions
quantiles N, conservative penalty constant α, number {Q˜i(s,ai)}I i=1.Thisproblemcorrespondstotheminimization
of agents I, number of training iterations K, number of the global loss
of gradient steps G, offline dataset D, and CVaR (cid:34)(cid:32) I
parameter ξ L({Q˜i}I ,{Qˆi(k)}I )=Eˆ r+γ(cid:88) max Qˆi(k)(s′,ai)
i=1 i=1
Output: Optimized quantile estimates {θ ji(s,ai)}N
j=1
i=1a˜i∈Ai
for all i=1,...,I I (cid:33)2(cid:35)
Define τ =ξi/N, i=1,...,N −(cid:88) Q˜i(s,ai) , (21)
i
Initialize network parameters for each agent i=1
for iteration k in {1,...,K} do whereQˆi(k) isthecurrentestimateofthecontributionofagent
for gradient step g in {1,...,G} do i. In practice, every function Q˜i(s,ai) is approximated using
Sample a batch B from the dataset D
a neural network. Furthermore, the policy of each agent is
for agent i in {1,...,I} do
obtained from the optimized function Q˜i(s,ai) as
for j in {1,...,N} do
for j′ in {1,...,N} do (cid:26) (cid:27)
πi(ai|s)=1 ai =arg max Q˜i(s,ai) . (22)
Calculate TD errors ∆ ji( jk ′) using ai∈Ai
(18)
The same approach can be adopted to enhance MA-CIQL
end
by using (20) in the loss (15). This yields the loss
end
Estimate the MA-CIQR loss L MA-CIQR in L ({Q˜i}I ,{Qˆi}I )= 1 L({Q˜i}I ,{Qˆi}I )
(17) MA-CCQL i=1 i=1 2 i=1 i=1
Perform a stochastic gradient step based on +αEˆ(cid:88)I (cid:20) log(cid:18) (cid:88) exp(Q˜i(s,a˜i))(cid:19) −Q˜i(s,ai)(cid:21)
, (23)
the estimated loss
end i=1 a˜i∈Ai
end
with L({Q˜i}I ,{Qˆi}I ) defined in (21). The obtained
end i=1 i=1
scheme, whose steps are detailed in Algorithm 3, is referred
Return {θi(s,ai)}N ={θˆi(K)(s,ai)}N for all
j j=1 j j=1 to as multi-agent conservative centralized Q-learning (MA-
i=1,...,I
CCQL). The optimized policy is given in (22).
Algorithm 3: Conservative Centralized Q-learning for B. Multi-AgentConservativeCentralizedQuantile-Regression
Offline MARL (MA-CCQL) TheCTDEapproachbasedonthevaluedecomposition(20)
Input: Discount factor γ, learning rate η, conservative canalsobeappliedtoMA-CIQRtoobtainacentralizedtrain-
penalty constant α, number of agents I, number of ing version referred to as multi-agent conservative centralized
training iterations K, number of gradient steps G, quantile regression (MA-CCQR).
and offline dataset D To this end, we first recall that the lower tail of the
Output: Optimized Q-functions Qi(s,ai) for distribution of Z(s,a) is approximated by MA-CIQR as in
i=1,...,I (16) using the estimates of the quantiles F−1 (τˆ ), with
Z(s,a) j
Initialize network parameters for each agent τˆ = τj−1−τj and τ = ξj/N for 1 ≤ j ≤ N. To jointly
j 2 j
for iteration k in {1,...,K} do
optimize the agents’ policies, we decompose each quantile
for gradient step g in {1,...,G} do θ (s,a) as
j
Sample a batch B from the dataset D
I
Estimate the MA-CCQL loss L MA-CCQL in (23) θ j(s,a)=(cid:88) θ˜ ji(s,ai), (24)
Perform a stochastic gradient step to update the
i=1
network parameters of each agent
where θ˜i(s,ai) represents the contribution of agent i. The
end j
end functions {{θ˜ ji(s,ai)}I i=1}N j=1 are jointly optimized using a
Return Q˜i(s,ai)=Qˆi(K)(s,ai), for i=1,...,I loss obtained by plugging the decomposition (24) into (17) to
obtain
L ({{θ˜i}I }N ,{{θˆi(k)}I }N )=
MA-CCQR j i=1 j=1 j i=1 j=1
A. Multi-Agent Conservative Centralized Q-Learning
N N
In the CTDE framework, it is assumed that the global Q-
1 Eˆ(cid:88)(cid:88)
ζ
(cid:16) ∆(k)(cid:17)
(25)
2N2 τˆj jj′
function can be decomposed as [38]
j=1j′=1
Q(s,a)=(cid:88)I
Q˜i(s,ai), (20)
+αEˆ(cid:88)I (cid:34) 1 (cid:88)N (cid:34) log(cid:88) exp(cid:16) θ˜i(s,a˜i)(cid:17) −θ˜i(s,ai)(cid:35)(cid:35)
,
N j j
i=1 i=1 j=1 a˜∈A8
Algorithm 4: Conservative Centralized Quantile Re-
gression for Offline MARL (MA-CCQR)
Input: Discount factor γ, learning rate η, number of
quantiles N, conservative penalty constant α, number
of agents I, number of training iterations K, number
of gradient steps G, offline dataset D, and CVaR
parameter ξ
Output: Optimized functions {θ˜i(s,ai)}N for all
j j=1
i=1,...,I
Define τ =ξi/N, i=1,...,N
i
Initialize network parameters for each agent
for iteration k in {1,...,K} do
Fig. 3: Multiple UAVs serve limited-power sensors to minimize power
for gradient step g in {1,...,G} do expenditure while also minimizing the age of information for data retrieval
Sample a batch B from the dataset D from the sensors. The environment is characterized by a riskier region for
for j in {1,...,N} do navigationoftheUAVsinthemiddleofthegridworld(coloredarea).
for j′ in {1,...,N} do
Calculate global TD error ∆(k)
jj′ grid world. The devices report their observations to I fixed-
using (26)
velocity rotary-wing UAVs flying at height h and starting
end
from positions selected randomly on the grid. The grid world
end
containsnormalcells,representedaswhitesquares,andarisk
Estimate the MA-CCQR loss L in (25)
MA-CCQR region of special cells, colored in the figure. Whenever UAVs
Perform a stochastic gradient step to update the
cross the risk region, there is a chance of failure, e.g., due
network parameters of each agent
to a higher collision probability. The current position at each
end
time t of each UAV i is projected on the plane as coordinates
end
Return {θˆi(K)(s,ai)}N , for i=1,...,I (cid:0) xi t,y ti(cid:1) . The DT monitoring of this system aims to determine
j j=1 trajectories for the UAVs on the grid that jointly minimize the
AoI and the transmission powers across all the IoT devices.
TheAoImeasuresthefreshnessoftheinformationcollected
where {{θˆ ji(k)}I i=1}N j=1 represents the current estimate of the by the UAVs from the devices [42]. For each device m, the
contribution of agent i and ∆(k) is given by AoIisdefinedasthetimeelapsedsincethelasttimedatafrom
jj′
the device was collected by a UAV [43], [44]. Accordingly,
I I
∆(k) =r+γ(cid:88) θˆi(k)(s′,a′i)−(cid:88) θ˜i(s,ai), (26) the AoI of device m at time t is updated as follows
jj′ j′ j
(cid:40)
i=1 i=1 1, if Vm =1,
Am = t (27)
with a′i = arg max 1 (cid:80)N θˆi(k)(s′,ai). The individ- t min{A ,Am +1}, otherwise;
ai∈Ai N j′=1 j′ max t−1
ual policies of the agent are finally obtained as
where A is the maximum AoI, and Vm =1 indicates that
  max t
πi(ai|s)=1
ai =arg max
1 (cid:88)N θ˜i(s,ai)
.
device m is served by a UAV at time step t. The maximum
 ai∈Ai N j  value A max determines the maximum penalty assigned to the
j=1 UAVs for not collecting data from a device at any given time.
For each agent, the function that maps (s,ai) to the N values For the sake of demonstrating the idea, we assume line-of-
{θ˜i(s,a)}N ismodeledasaneuralnetworkandthestepsof sight (LoS) communication links and write the channel gain
j j=1
the MA-CCQR scheme are provided in Algorithm 4. between agent i and device m at time step t as
g
gi,m = 0 , (28)
VI. APPLICATION:TRAJECTORYLEARNINGINUAV t h2+(Li,m)2
t
NETWORKS
where g is the channel gain at a reference distance of 1 m
Inthissection,weconsidertheapplicationofofflineMARL 0
andLi,m isthedistancebetweenUAVianddevicemattime
to the trajectory optimization problem in UAV networks. t
t. Using the standard Shannon capacity formula, for device
Following [42], as illustrated in Fig. 3, we consider multiple
m to communicate to UAV i at time step t, the transmission
UAVs acting as BSs to receive uplink updates from limited-
power must be set to [45]
power sensors.
(cid:16) (cid:17)
2BE−1 σ2
A. Problem Definition and Performance Metrics Pi,m = , (29)
t gi,m
Consider a grid world, as shown in Fig. 3, where each t
cell is a square of length L . The system comprises a set where E is the size of the transmitted packet, B is the
c
M of M uplink IoT devices deployed uniformly in the bandwidth, and σ2 is the noise power.9
TABLEIII:Simulationparametersandhyperparameters
Parameter Value Parameter Value
−1
g0 30dB α 1
B 1MHz γ 0.99
h 100m ξ 0.15
−2
E 5Mb σ2 −100dBm
MA-CIQR
γ 0.99 Amax 100
Batchsize 128 λ 500 MA-CIQR-CVaR
IterationsK 150 p risk 0.1 −3 MA-CIQL
Lc 100m Optimizer Adam MA-DIQN
MA-QR-DIQN
−4
If all the UAVs are outside the risk region, the reward
function is given deterministically as a weighted combination
of the sums of AoI and powers across all agents −5
1 (cid:88)M (cid:88)M 0 20 40 60 80 100 120 140
r t =− M Am t −λ P tim,m, (30) Epochs
m=1 m=1
where λ>0 is a parameter that controls the desired trade-off
between AoI and power consumption. In contrast, if any of Fig.4:Averagetestreturnasafunctionofthenumberoftrainingepochsat
the UAVs is within the risk region, with probability p , the
theDTusingP risk=100and16%offlinedatasetforasystemof2UAVs
risk serving10sensors.Thereturnisaveragedover100testepisodesattheend
reward is given by (30) with the addition of a penalty value ofeachtrainingepochandshownupondivisionby1000.
P >0, while it is equal to (30) otherwise.
risk
To complete the setting description, we define state and
all its variants), the learning rate is set to 10−5, while for all
actions as follows. The global state of the system at
other schemes, we use a learning rate of 10−4.
each time step t is the collection of the UAVs’ posi-
The offline dataset D is collected using online independent
tions and the individual AoI of the devices, i.e., s =
(cid:2) x1,y1,··· ,xI,yI,A1,A2,··· ,AM(cid:3) . At each time t,t the DQN agents. In particular, we train the UAVs using an online
t t t t t t t MA-DQN algorithm until convergence and use 6% and 16%
action ai = [wi,di] of each UAV i includes the direction
t t t ofthetotalnumberoftransitionsfromtheobservedexperience
wi ∈ {north,south,east,west,hover}, where “hover” repre-
t as the offline datasets1.
sents the decision of staying in the same cell, while the other
actions move the UAV by one cell in the given direction. It
C. Numerical Results
also includes the identity di ∈M∪{0} of the device served
t First,weshowthesimulationresultsoftheproposedmodel
at time t, with di = 0 indicating that no device is served by
t viaindependentQ-learningcomparedtothebaselineschemes.
UAV i.
Then, we investigate the benefits of the CTDE approach
compared to independent training.
B. Implementation and Dataset Collection 1) Independent Learning: Fig. 4 shows the average test
Weconsidera10×10gridworldwithI =2UAVsserving return, evaluated online using 100 test episodes, for the
M = 10 limited-power sensors and a 5 × 4 risk region in policies obtained after a given number of training epochs at
the DT. The figure thus reports the actual return obtained by
the middle of the grid world as illustrated in Fig. 3. We use
the system as a function of the computational load at the DT,
a fully connected neural network with two hidden layers of
size 256 and ReLU activation functions to represent the Q- which increases with the number of training epochs.
We first observe that both MA-DIQN and MA-QR-DIQN,
function and the quantiles. The experiments are implemented
designed for online learning, fail to converge in the offline
using Pytorch on a single NVIDIA Tesla V100 GPU. Ta-
setting at hand. This well-known problem arises from overes-
ble III shows the UAV network parameters and the proposed
timatingQ-valuescorrespondingtoOODactionsintheoffline
schemes’ hyperparameters. We compare the proposed method
dataset [10]. In contrast, conservative strategies designed for
MA-CQR to baseline offline MARL schemes, namely MA-
offline learning, namely MA-CIQL, MA-CIQR, and MA-
DQN[46],MA-CQL(seeSec.IV-A),andMA-QR-DQN.MA-
CIQR-CVaR,exhibitanincreasingaveragereturnasafunction
DQN corresponds to MA-CQL when no conservative penalty
of the training epochs. In particular, the proposed MA-CIQR
for OOD is applied, i.e., α = 0 in (15), whereas MA-QR-
andMA-CIQR-CVaRprovidethefastestconvergence,needing
DQN corresponds to MA-CQR when α=0 and ξ =1. Both
around 30 training epochs to reach the maximum return. In
the independent and centralized training frameworks apply to
contrast,MA-CIQLshowsslowerconvergence.Thishighlights
MA-DQN and MA-QR-DQN.
thebenefitsofdistributionalRLinhandlingtheinherentuncer-
For the proposed MA-CQR, we consider two settings for
tainties arising in multi-agent systems from the environment
the risk tolerance level ξ, namely ξ = 1 and ξ = 0.15, with
and the actions of other agents [11].
the former corresponding to a risk-neutral design. We refer to
theformerasMA-CQRandthelatterasMA-CQR-CVaR.For
1The code and datasets are available at https://github.com/Eslam211/
alldistributionalRLschemes(MA-QR-DQNandMA-CQRin Conservative-and-Distributional-MARL
nr
ter
tset
egarevA10
120 In the next experiment, we investigate the capacity of
the proposed risk-sensitive scheme MA-CIQR-CVaR to avoid
risky trajectories. As a first illustration of this aspect, Fig. 6
100
shows two examples of trajectories obtained via MA-CIQR
and MA-CIQR-CVaR. It is observed that the risk-neutral
80
policies obtained by MA-CIQR take shortcuts through the
risky area, while the risk-sensitive trajectories obtained via
60 MA-CIQR-CVaR avoid entering the risky area.
2) CentralizedLearning: Here,wecomparethecentralized
40 MA-QR-DIQN training approach with independent learning. Fig. 7 shows the
MA-DIQN
average test return as a function of training epochs for an
MA-CIQL
20 MA-CIQR-CVaR environment of 2 agents and 10 sensors. We use two offline
MA-CIQR datasets with different sizes, equal to 6% and 16% of the
Idle
total transitions from the observed experience of online DQN
0
0.0 0.5 1.0 1.5 2.0 2.5 3.0 agents. We increase the value of the risk region penalty to
Sum-power P =300comparedtotheprevioussubsectionexperiments.
risk
Fig. 7a elucidates that the performance of the independent
learning schemes is affected by increasing the risk penalty
Fig.5:Sum-AoIasafunctionofthesum-powerusingP risk=λ/4and16% P .Specifically,MA-CIQLfailstoreachconvergence,while
offlinedatasetforasystemof2UAVsserving10sensors. risk
MA-CIQR and MA-CIQR-CVaR reach near-optimal perfor-
mance but with a slower and less stable convergence than
x their centralized variants. However, as in Fig. 7b, a significant
x performance gap is observed between the proposed indepen-
x x dent schemes and their centralized counterpart for reduced
x x dataset size. This illustrates the merits of the CTDE approach
in coordinating between agents during training, requiring less
x data to obtain effective policies.
x In a manner similar to Fig. 5, Fig. 8 shows the trade-
off between sum-AoI and sum-power consumption for both
x
independent and centralized training approaches for a system
x
of 3 agents serving 15 sensors. Increasing the parameter λ
in (30) reduces the total power consumption at the expense
Risk-neutral Risk-sensitive
of AoI. Here again, we observe a significant gain in per-
formance for MA-CCQR and MA-CCQR-CVaR compared to
their independent variants. In contrast, the non-distributional
Fig. 6: A comparison between the trajectories of two UAVs using risk-
neutral and risk-sensitive policies obtained via MA-CIQR and MA-CIQR- schemes, MA-CIQL and MA-CCQL, show similar results, as
CVaR,respectively.Crossesrepresentthepositionsofthedevices. both perform poorly in this low data regime.
Finally,togainfurtherinsightsintothecomparisonbetween
MA-CCQRandMA-CCQR-CVaR,weleveragetwometricsas
InFig5,wereporttheoptimalachievabletrade-offbetween in[23],namelythepercentageofviolationsandtheCVaR
0.15
sum-AoI and sum-power consumption across the devices. return. The former is the percentage of timesteps at which
This region is obtained by training the different schemes one of the UAVs enters the risk region with respect to the
while sweeping the hyperparameter values λ. We recall that total number of timesteps. In contrast, the CVaR metric is
0.15
the hyperparameter λ controls the weight of the power as the average return of the 15% worst episodes.
comparedtotheAoIintherewardfunction(30).Inparticular,
In Table IV, we report these two metrics, as well as the
setting λ → 0 minimizes the AoI only, resulting in a round-
averagereturn,withallreturnsnormalizedby1000.Thanksto
robinoptimalpolicy.Attheotherextreme,settingalargevalue
the ability of MA-CCQR-CVaR to learn how to avoid the risk
of λ causes the UAV never to probe the devices, achieving
region, this scheme has the lowest percentage of violations
the minimum power equal to zero, and the maximum AoI
among all the schemes. In addition, it achieves the largest
A =100. This point is denoted as “idle point” the figure.
max CVaR return, with a small gain in terms of average return
0.15
The other curves represent the minimum sum-AoI achievable
ascomparedtoMA-CCQR.Thisdemonstratestheadvantages
as a function of the sum-power.
of the risk-sensitive design of policies.
From Fig. 5, we observe that the proposed MA-CIQR
alwaysachievesthebestage-powertrade-offwiththeleastage
VII. CONCLUSIONS
andsum-powerconsumptionwithinalthecurves.AsinFig.4,
MA-DIQNandMA-QR-DIQNprovidetheworstperformance Inthispaper,wedevelopedadistributionalandconservative
duetotheirfailuretohandletheuncertaintyarisingfromOOD offline MARL scheme for DT-based wireless systems. We
actions. considered optimizing the CVaR of the cumulative return to
IoA-muS11
MA-CIQR-CVaR MA-CIQR MA-CIQL 100
MA-CCQR-CVaR MA-CCQR MA-CCQL
−1 80
−2 60
MA-CIQL
40 −3 MA-CCQL
MA-CIQR-CVaR
MA-CCQR-CVaR
20 MA-CIQR
−4
MA-CCQR
Idle
0
−5 0 1 2 3 4 5
Sum-power
0 20 40 60 80 100 120 140
Epochs
Fig.8:Sum-AoIasafunctionofthesum-powerforpenaltyP risk=λ/2.5
(a)16%datasetsize inasystemof3UAVsand15sensors(datasetsizeequalto6%).
networks. Numerical results illustrate that the learned policies
−1
avoid risky trajectories more effectively and yield the best
performance compared to the baseline MARL schemes. The
proposedapproachcanbeextendedbyconsideringonlinefine-
−2
tuningofthepoliciesinthePTtohandlethepossiblechanges
inthedeploymentenvironmentcomparedtotheonegenerating
the offline dataset.
−3
REFERENCES
−4 [1] A.Lavin,D.Krakauer,H.Zenil,J.Gottschlich,T.Mattson,J.Brehmer,
A.Anandkumar,S.Choudry,K.Rocki,A.G.Baydinetal.,“Simulation
intelligence: Towards a new generation of scientific methods,” arXiv
preprintarXiv:2112.03235,2021.
−5 [2] R. Saracco, “Digital twins: Bridging physical space and cyberspace,”
0 20 40 60 80 100 120 140 Computer,vol.52,no.12,pp.58–64,2019.
[3] M. Raza, P. M. Kumar, D. V. Hung, W. Davis, H. Nguyen, and
Epochs
R. Trestian, “A digital twin framework for industry 4.0 enabling next-
(b)6%datasetsize genmanufacturing,”in20209thinternationalconferenceonindustrial
technologyandmanagement(ICITM). IEEE,2020,pp.73–77.
Fig.7:Averagetestreturnasafunctionofthenumberoftrainingepochsat [4] P. Coveney and R. Highfield, Virtual You: How Building Your Digital
theDTforpenaltyP risk=300inasystemof3UAVsand15sensorswith Twin Will Revolutionize Medicine and Change Your Life. Princeton
datasetsizeequalto(a)16%and(b)6%.Thereturnisaveragedover100 UniversityPress,2023.
testepisodesattheendofeachtrainingepoch,andshownupondivisionby [5] L.U.Khan,W.Saad,D.Niyato,Z.Han,andC.S.Hong,“Digital-twin-
1000. enabled 6g: Vision, architectural trends, and future directions,” IEEE
CommunicationsMagazine,vol.60,no.1,pp.74–80,2022.
[6] J.Mirzaei,I.Abualhaol,andG.Poitau,“Networkdigitaltwinforopen
TABLEIV:Performanceevaluationover100testepisodesafter150training
RAN:Thekeyenablers,standardization,andusecases,”arXivpreprint
iterationsforpenaltyP risk=λ/2.5inasystemof3UAVsand15sensors
arXiv:2308.02644,2023.
(datasetsizeequalto6%).
[7] D. Villa, M. Tehrani-Moayyed, C. P. Robinson, L. Bonati, P. Johari,
M. Polese, S. Basagni, and T. Melodia, “Colosseum as a digital twin:
Algorithm Averagereturn CVaR0.15 return Violations
Bridging real-world experimentation and wireless network emulation,”
arXivpreprintarXiv:2303.17063,2023.
MA-DQN(online) −1.5633 −1.9611 11.83%
[8] J.Zheng,T.H.Luan,Y.Zhang,R.Li,Y.Hui,L.Gao,andM.Dong,
MA-DCQN −4.8993 −5.4930 8.29%
“Datasynchronizationinvehiculardigitaltwinnetwork:Agametheo-
MA-QR-DCQN −4.2987 −4.5743 6.85%
reticapproach,”IEEETransactionsonWirelessCommunications,2023.
MA-CCQL −3.8518 −4.4695 17.7%
[9] S.V.Albrecht,F.Christianos,andL.Scha¨fer,Multi-AgentReinforcement
MA-CCQR −1.4028 −2.1775 8.56%
Learning: Foundations and Modern Approaches. MIT Press, 2024.
MA-CCQR-CVaR −1.3641 −1.8513 5.83%
[Online].Available:https://www.marl-book.com
[10] S. Levine, A. Kumar, G. Tucker, and J. Fu, “Offline reinforcement
learning: Tutorial, review, and perspectives on open problems,” arXiv
preprintarXiv:2005.01643,2020.
obtain risk-sensitive policies. We introduce two variants of [11] M. G. Bellemare, W. Dabney, and M. Rowland, Distributional Rein-
the proposed scheme depending on the level of coordination forcementLearning. MITPress,2023,http://www.distributional-rl.org.
[12] A.Kumar,A.Zhou,G.Tucker,andS.Levine,“ConservativeQ-learning
between the agents during training. The proposed algorithms
for offline reinforcement learning,” Advances in Neural Information
were applied to the trajectory optimization problem in UAV ProcessingSystems,vol.33,pp.1179–1191,2020.
nruter
tset
egarevA
nruter
tset
egarevA
IoA-muS12
[13] M.A.Abd-Elmagid,A.Ferdowsi,H.S.Dhillon,andW.Saad,“Deep [37] Y. Hua, R. Li, Z. Zhao, X. Chen, and H. Zhang, “Gan-powered
reinforcement learning for minimizing age-of-information in UAV- deep distributional reinforcement learning for resource management in
assisted networks,” in 2019 IEEE Global Communications Conference networkslicing,”IEEEJournalonSelectedAreasinCommunications,
(GLOBECOM). IEEE,2019,pp.1–6. vol.38,no.2,pp.334–349,2019.
[14] M. Samir, C. Assi, S. Sharafeddine, D. Ebrahimi, and A. Ghrayeb, [38] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi,
“Age of information aware trajectory planning of UAVs in intelligent M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls et al.,
transportation systems: A deep learning approach,” IEEE Transactions “Value-decomposition networks for cooperative multi-agent learning,”
onVehicularTechnology,vol.69,no.11,pp.12382–12395,2020. arXivpreprintarXiv:1706.05296,2017.
[15] E. Eldeeb, J. M. de Souza Sant’Ana, D. E. Pe´rez, M. Shehab, N. H. [39] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch,
Mahmood,andH.Alves,“Multi-UAVpathlearningforageandpower “Multi-agent actor-critic for mixed cooperative-competitive environ-
optimizationinIoTwithUAVbatteryrecharge,”IEEETransactionson ments,”2020.
VehicularTechnology,vol.72,no.4,pp.5356–5360,2022. [40] R.Bellman,“Dynamicprogramming,”Science,vol.153,no.3731,pp.
[16] K. Ciosek, “Imitation learning by reinforcement learning,” in 34–37,1966.
InternationalConferenceonLearningRepresentations,2022.[Online]. [41] H.Xu,L.Jiang,J.Li,Z.Yang,Z.Wang,V.W.K.Chan,andX.Zhan,
Available:https://openreview.net/forum?id=1zwleytEpYx “OfflineRLwithnoOODactions:In-samplelearningviaimplicitvalue
[17] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust regularization,”2023.
region policy optimization,” in International conference on machine [42] E.Eldeeb,D.E.Pe´rez,J.MicheldeSouzaSant’Ana,M.Shehab,N.H.
learning. PMLR,2015,pp.1889–1897. Mahmood, H. Alves, and M. Latva-Aho, “A learning-based trajectory
[18] M.G.Bellemare,W.Dabney,andR.Munos,“Adistributionalperspec- planning of multiple UAVs for AoI minimization in IoT networks,” in
tiveonreinforcementlearning,”inInternationalconferenceonmachine 2022JointEuropeanConferenceonNetworksandCommunications&
learning. PMLR,2017,pp.449–458. 6GSummit(EuCNC/6GSummit),2022,pp.172–177.
[19] W.Dabney,M.Rowland,M.Bellemare,andR.Munos,“Distributional [43] E.Eldeeb,M.Shehab,A.E.Kalør,P.Popovski,andH.Alves,“Traffic
reinforcementlearningwithquantileregression,”inProceedingsofthe predictionandfastuplinkforhiddenmarkoviotmodels,”IEEEInternet
AAAIConferenceonArtificialIntelligence,vol.32,no.1,2018. ofThingsJournal,vol.9,no.18,pp.17172–17184,2022.
[20] W. Dabney, G. Ostrovski, D. Silver, and R. Munos, “Implicit quantile [44] A. Kosta, N. Pappas, and V. Angelakis, “Age of information: A new
networks for distributional reinforcement learning,” in International concept,metric,andtool,”FoundationsandTrendsinNetworking,Now
conferenceonmachinelearning. PMLR,2018,pp.1096–1105. Publishers,Inc.,2017.
[21] R.T.Rockafellar,S.Uryasevetal.,“Optimizationofconditionalvalue- [45] E. Eldeeb, M. Shehab, and H. Alves, “Age minimization in massive
at-risk,”Journalofrisk,vol.2,pp.21–42,2000. iotviauavswarm:Amulti-agentreinforcementlearningapproach,”in
[22] S.H.LimandI.Malik,“Distributionalreinforcementlearningforrisk- 2023 IEEE 34th Annual International Symposium on Personal, Indoor
sensitivepolicies,”AdvancesinNeuralInformationProcessingSystems, andMobileRadioCommunications(PIMRC),2023,pp.1–6.
vol.35,pp.30977–30989,2022. [46] A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru,
[23] Y. Ma, D. Jayaraman, and O. Bastani, “Conservative offline distribu- J. Aru, and R. Vicente, “Multiagent cooperation and competition with
tional reinforcement learning,” Advances in Neural Information Pro- deep reinforcement learning,” PloS one, vol. 12, no. 4, p. e0172395,
cessingSystems,vol.34,pp.19235–19247,2021. 2017.
[24] J. Jiang and Z. Lu, “Offline decentralized multi-agent reinforcement
learning,”2023.
[25] L. Pan, L. Huang, T. Ma, and H. Xu, “Plan better amid conservatism:
Offline multi-agent reinforcement learning with actor rectification,” in
International Conference on Machine Learning. PMLR, 2022, pp.
17221–17237.
[26] J.Shao,Y.Qu,C.Chen,H.Zhang,andX.Ji,“Counterfactualconser-
vativeQlearningforofflinemulti-agentreinforcementlearning,”2023.
[27] X.Wang,H.Xu,Y.Zheng,andX.Zhan,“Offlinemulti-agentreinforce-
mentlearningwithimplicitglobal-to-localvalueregularization,”arXiv
preprintarXiv:2307.11620,2023.
[28] Y. Yang, X. Ma, C. Li, Z. Zheng, Q. Zhang, G. Huang, J. Yang, and
Q.Zhao,“Believewhatyousee:Implicitconstraintapproachforoffline
multi-agentreinforcementlearning,”2021.
[29] K. Yang, C. Shen, J. Yang, S. ping Yeh, and J. Sydir, “Offline
reinforcement learning for wireless network optimization with mixture
datasets,”2023.
[30] E. Eldeeb, M. Shehab, and H. Alves, “Traffic learning and proactive
UAV trajectory planning for data uplink in markovian IoT models,”
2023.
[31] F. Wu, H. Zhang, J. Wu, L. Song, Z. Han, and H. V. Poor, “AoI
minimization for UAV-to-device underlay communication by multi-
agentdeepreinforcementlearning,”inGLOBECOM2020-2020IEEE
GlobalCommunicationsConference,2020,pp.1–6.
[32] J.Cui,Y.Liu,andA.Nallanathan,“Multi-agentreinforcementlearning-
based resource allocation for UAV networks,” IEEE Transactions on
WirelessCommunications,vol.19,no.2,pp.729–743,2020.
[33] Y. S. Nasir and D. Guo, “Multi-agent deep reinforcement learning
for dynamic power allocation in wireless networks,” IEEE Journal on
Selected Areas in Communications, vol. 37, no. 10, pp. 2239–2250,
2019.
[34] N.Naderializadeh,J.J.Sydir,M.Simsek,andH.Nikopour,“Resource
management in wireless networks via multi-agent deep reinforcement
learning,” IEEE Transactions on Wireless Communications, vol. 20,
no.6,pp.3507–3523,2021.
[35] Q.Zhang,W.Saad,andM.Bennis,“Millimeterwavecommunications
withanintelligentreflector:Performanceoptimizationanddistributional
reinforcement learning,” IEEE Transactions on Wireless Communica-
tions,vol.21,no.3,pp.1836–1850,2021.
[36] ——, “Distributional reinforcement learning for mmwave communica-
tions with intelligent reflectors on a uav,” in GLOBECOM 2020-2020
IEEEGlobalCommunicationsConference. IEEE,2020,pp.1–6.