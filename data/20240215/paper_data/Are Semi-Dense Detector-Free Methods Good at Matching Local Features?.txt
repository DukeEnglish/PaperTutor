Are Semi-Dense Detector-Free Methods Good at Matching Local
Features?
MatthieuVilain1,Re´miGiraud1,HugoGermain,andGuillaumeBourmaud1
1Univ.Bordeaux,CNRS,BordeauxINP,IMS,UMR5218,F-33400Talence,France
{matthieu.vilain,remi.giraud,guillaume.bourmaud}@u-bordeaux.fr
Keywords: Imagematching,Transformer,Poseestimation
Abstract: Semi-dense detector-free approaches (SDF), such as LoFTR, are currently among the most popular image
matchingmethods. WhileSDFmethodsaretrainedtoestablishcorrespondencesbetweentwoimages,their
performancesarealmostexclusivelyevaluatedusingrelativeposeestimationmetrics. Thus,thelinkbetween
theirabilitytoestablishcorrespondencesandthequalityoftheresultingestimatedposehasthusfarreceived
little attention. This paper is a first attempt to study this link. We start with proposing a novel structured
attention-based image matching architecture (SAM). It allows us to show a counter-intuitive result on two
datasets(MegaDepthandHPatches):ontheonehandSAMeitheroutperformsorisonparwithSDFmethods
in terms of pose/homography estimation metrics, but on the other hand SDF approaches are significantly
betterthanSAMintermsofmatchingaccuracy. Wethenproposetolimitthecomputationofthematching
accuracy to textured regions, and show that in this case SAM often surpasses SDF methods. Our findings
highlightastrongcorrelationbetweentheabilitytoestablishaccuratecorrespondencesintexturedregionsand
theaccuracyoftheresultingestimatedpose/homography.Ourcodewillbemadeavailable.
1 Introduction attentionlayersaremainlyresponsibleforthisbreak-
through (Sarlin et al., 2020) as they enable the lo-
Imagematchingisthetaskofestablishingcorrespon- cal features of detected keypoints in both images to
dences between two partially overlapping images. It communicate and adjust with respect to each other.
isconsideredtobeafundamentalproblemof3Dcom- Priorsiamesearchitectures(Yietal.,2016;Onoetal.,
putervision,asestablishingcorrespondencesisapre- 2018; Dusmanu et al., 2019), (Revaud et al., 2019),
conditionforseveraldownstreamtaskssuchasstruc- (Germain et al., 2020; Germain et al., 2021) had so
ture from motion (Heinly et al., 2015; Scho¨nberger farpreventedthistypeofcommunication. Shortlyaf-
and Frahm, 2016), visual localisation (Sva¨rm et al., ter this breakthrough, a second significant milestone
2017; Sattler et al., 2017; Taira et al., 2018) or si- wasreached(Sunetal.,2021)bycombiningtheusage
multaneouslocalizationandmapping(Strasdatetal., of attention layers (Sarlin et al., 2020) with the idea
2011;Sarlinetal.,2022). Despitetheabundantwork ofhavingadetector-freemethod(Roccoetal.,2018;
dedicatedtoimagematchingoverthelastthirtyyears, Rocco et al., 2020a; Rocco et al., 2020b; Li et al.,
thistopicremainsanunsolvedproblemforchalleng- 2020), (Zhou et al., 2021; Truong et al., 2021). In
ingscenarios,suchaswhenimagesarecapturedfrom LoFTR(Sunetal.,2021),low-resolutiondensecross-
strongly differing viewpoints (Arnold et al., 2022) attention layers are employed that allow semi-dense
(Jin et al., 2020), present occlusions (Sarlin et al., low-resolutionfeaturesofthetwoimagestocommu-
2022) or feature day-to-night changes (Zhang et al., nicateandadjusttoeachother. Suchamethodissaid
2021). tobedetector-free,asitmatchessemi-denselocalfea-
Aninherentpartoftheproblemisthedifficultyto tures instead of sparse sets of local features coming
evaluate image matching methods. In practice, it is fromdetectedkeypointlocations(Lowe,1999).
oftentackledwithaproxytasksuchasrelativeorab- Semi-dense Detector-Free (SDF) methods (Chen
solutecameraposeestimation,or3Dreconstruction. et al., 2022; Giang et al., 2023; Wang et al., 2022;
In this context, image matching performances were Sunetal.,2021),(Maoetal.,2022;Tangetal.,2022),
recently significantly improved with the advent of suchasLoFTR,areamongthebestperformingimage
attention layers (Vaswani et al., 2017). Cross- matchingapproachesintermsofposeestimationmet-
4202
beF
31
]VC.sc[
1v17680.2042:viXraMA@2=60.6 MAtext@2=84.0 errR=2.1' errT=5.4' ings highlight a strong correlation between the abil-
ity to establish accurate correspondences in textured
regions and the accuracy of the resulting estimated
pose/homography(seeFigure1).
Organization of the paper - Sec. 2 discusses the
related work. The proposed SAM architecture is in-
MA@2=62.8 MAtext@2=64.0 errR=5.5' errT=22.4' troducedinSec.3andtheexperimentsinSec.4.
2 Related work
Sincethispaperfocusesonthelinkbetweentheabil-
Figure 1: Given query locations within textured regions ityofSDFmethodstoestablishcorrespondencesand
of the source image (left), we show their predicted corre- the quality of the resulting estimated pose, we only
spondentsinthetargetimage(right)for: (Toprow)SAM presentSDFmethodsinthisliteraturereviewandre-
(Proposed) - Structured Attention-based image Matching, fer the reader to (Edstedt et al., 2023; Zhu and Liu,
(Bottomrow)LoFTR(Sunetal., 2021)+QuadTree(Tang
2023;Nietal.,2023)forabroaderliteraturereview.
et al., 2022) - a semi-dense detector-free approach (line
Tothebestofourknowledge,LoFTR(Sunetal.,
colors indicate the distance in pixels with respect to the
ground truth correspondent). We report: (MA@2) - the 2021)wasthefirstmethodtoperformattention-based
matching accuracy at 2 pixels computed on all the semi- detector-free matching. A siamese CNN is first ap-
denselocationsofthesourceimagewithavailableground plied on the source/target image pair to extract fine
truthcorrespondent(whichincludesbothtexturedanduni- densefeaturesofresolution1/2andcoarsedensefea-
form regions), (MAtext@2) - the matching accuracy at 2
tures of resolution 1/8. The source and target coarse
pixelscomputedonallthetexturedsemi-denselocationsof
featuresarefedintoadenseattention-basedmodule,
thesourceimagewithavailablegroundtruthcorrespondent
interleaving self-attention layers with cross-attention
(i.e.,uniformregionsareignored),(errRanderrT)-therel-
ative pose error. SAM has a better pose estimation but a layersasproposedin(Sarlinetal.,2020). Toreduce
lower matching accuracy (MA@2), which seems counter- thecomputationalcomplexityofthesedenseattention
intuitive. However, if we consider only textured regions layers, LinearAttention(Katharopoulosetal., 2020)
(MAtext@2),thenSAMoutperformsLoFTR+QuadTree. isusedinsteadofvanillasoftmaxattention(Vaswani
et al., 2017). The resulting features are matched to
rics. However,tothebestofourknowledge,thelink
obtain coarse correspondences. Each coarse corre-
betweentheirabilitytoestablishcorrespondencesand
spondenceisthenrefinedbycropping5×5windows
thequalityoftheresultingestimatedposehasthusfar
into the fine features and applying another attention-
received little attention. This paper is a first attempt
based module. Thus for each location of the semi-
tostudythislink.
dense (factor of 1/8) source grid, a correspondent is
Contributions - We start with proposing a novel predicted. In practice, a Mutual Nearest Neighbor
StructuredAttention-basedimageMatchingarchitec- (MNN)stepisappliedattheendofthecoarsematch-
ture (SAM). We evaluate SAM and 6 SDF methods ingstagetoremoveoutliers.
on 3 datasets (MegaDepth - relative pose estimation In(Tangetal.,2022),aQuadTreeattentionmod-
andmatching,HPatches-homographyestimationand ule is proposed to reduce the computational com-
matching,ETH3D-matching). plexity of vanilla softmax attention from quadratic
We highlight a counter-intuitive result on two to linear while keeping its power, as opposed to
datasets(MegaDepthandHPatches): ontheonehand Linear Attention (Katharopoulos et al., 2020) which
SAMeitheroutperformsorisonparwithSDFmeth- was shown to underperform on local feature match-
odsintermsofpose/homographyestimationmetrics, ing (Germain et al., 2022). The QuadTree attention
but on the other hand SDF approaches are signifi- module is used as a replacement for Linear Atten-
cantly better than SAM in terms of Matching Accu- tionmoduleinLoFTR.AnarchitecturecalledASpan-
racy(MA).HeretheMAiscomputedonallthesemi- Former is introduced in (Chen et al., 2022) that em-
dense locations (of the source image) with available ploysthesamerefinementstageasLoFTRbutadif-
groundtruthcorrespondent,whichincludesbothtex- ferent coarse stage architecture. Instead of classical
turedanduniformregions. cross-attention layers, the coarse stage uses global-
Weproposetolimitthecomputationofthematch- local cross-attention layers that have the ability to
ingaccuracytotexturedregions,andshowthatinthis focus on regions around current potential correspon-
case SAM often surpasses SDF methods. Our find- dences.MatchFormer(Wangetal.,2022)proposesan
)sruo(MAS
eerTdauQ+RTFoLSource PE
: Dot product
CNN
S Imou ar gc ee Query 2D locations
xS
Co Mr are ps sp.
Structured Structured
Cross- Self-
Attention Attention
Softmax
lateL ne ta r vn ee cd to rs Latent space Attention Maps
Target PE
CNN Str Cu rc ot su sr -e d
Attention
Target
Image Structured Attention
(a)SAMarchitecture (b)StructuredAttention
Figure 2: Overview of the proposed Structured Attention-based image Matching (SAM) method. (a) The matching
architecturefirstextractsfeaturesfrombothsourceandtargetimagesatresolution1/4. Thenitusesasetoflearnedlatent
vectors alongside the descriptors of the query locations, and performs an input structured cross-attention with the dense
featuresofthetarget. Thelatentspaceisthenprocessedthroughasuccessionofstructuredself-attentionlayers. Anoutput
structured cross-attention is applied to update the target features with the information from the latent space. Finally, the
correspondencemapsareobtainedusingadotproduct.(b)Proposedstructuredattentionlayer.Seetextfordetails.
attention-basedbackbone,interleavingselfandcross- 2Dcorrespondentlocations{pˆ } inatargetim-
t,i i=1...L
attentionlayers,toprogressivelytransformatensorof ageI:
t
size2×H×W×3intoatensorofsize2× 1H 6×W 16×C.
{pˆ } =M
(cid:0)
I ,I,{p }
(cid:1)
. (1)
EfficientattentionlayerssuchasSpatialEfficientAt- t,i i=1...L s t s,i i=1...L
tention(Wangetal.,2021;Xieetal.,2021)andLin-
HereM istheimagematchingmethod. Inthecaseof
ear Attention (Katharopoulos et al., 2020) are em-
semi-densedetector-freemethods,thequerylocations
ployed. Afeaturepyramidnetwork-likedecoder(Lin
{p } are defined as the source grid locations
et al., 2017) is used to output a fine tensor of size s,i i=1...L
using a stride of 8. As we will see, SAM is more
2×H×W×C and a coarse tensor of size 2×H×
2 2 f 8 flexible and can process any set of query locations.
W
8
×C c,andthematchingisperformedasinLoFTR. However, for a fair comparison, in the experiments
TopicFM(Giangetal.,2023)followsthesameglobal allthemethods(includingSAM)willusesourcegrid
architecture as LoFTR with a different coarse stage. locationsusingastrideof8asquerylocations.
Here, topic distributions (latent semantic instances)
Vanilla softmax attention - A cross-attention op-
are inferred from coarse CNN features using cross-
eration, with H heads, between a D-dimensional
attention layers with topic embeddings. These topic
query vector x and a set of D-dimensional vectors
distributionsareusedtoaugmentthecoarseCNNfea-
{y } canbewrittenasfollows:
tures with self and cross-attention layers. In (Mao n n=1...N
et al., 2022), a method called 3DG-STFM proposes
H N
1×1D×DHDH×DD×1
totraintheLoFTRarchitecturewithastudent-teacher ∑ ∑(cid:122) s(cid:125)(cid:124)(cid:123)(cid:122) W(cid:125)(cid:124)(cid:123) (cid:122) W(cid:125)(cid:124)(cid:123)(cid:122) y(cid:125)(cid:124)(cid:123) , (2)
method. The teacher is first trained on RGB-D im- h,n o,h v,h n
h=1n=1
agepairs. Theteachermodelthenguidesthestudent
1×DD×DHDH×DD×1
modeltolearnRGB-induceddepthinformation. (cid:122)(cid:125)(cid:124)(cid:123)(cid:122)(cid:125)(cid:124)(cid:123) (cid:122)(cid:125)(cid:124)(cid:123)(cid:122)(cid:125)(cid:124)(cid:123)
exp( x⊤ W⊤ W y⊤ )
q,h k,h n
where s = . (3)
h,n ∑N m=1exp(x⊤W⊤ q,hW k,hy m)
3 Structured attention-based image
Inpractice, allthematricesW arelearned. Usu-
matching architecture ·,·
allyD = D.Theoutputisalinearcombination(with
H H
coefficients s ) of the linearly transformed set of
Inthefollowingsection,wepresentournovelStruc- h,n
vectors {y } . This operation allows to extract
tured Attention-based image Matching architecture n n=1...N
therelevantinformationin{y } fromthepoint
(SAM),whosearchitectureisillustratedinFigure2. n n=1...N
ofviewofthequeryx. Thequeryxisnotanelement
ofthatlinearcombination.Asaconsequence,aresid-
3.1 Backgroundandnotations
ual connection is often added, usually followed by a
layernormalizationanda2-layerMLP(withresidual
Imagematching- Givenasetof2Dquerylocations
connection)(Vaswanietal.,2017).
{p } inasourceimageI ,weseektofindtheir
s,i i=1...L sSourceImage TargetImage Averagequerymap Averagelatentmap
Figure3:VisualizationofthelearnedlatentvectorsofSAM.Theaveragequerymapisobtainedbyaveraging64correspon-
dencemapsof64querylocations(Redcrosses)whiletheaveragelatentmapisobtainedbyaveragingthe128correspondence
mapsofthe128learnedlatentvectors.Weobservethattheaveragequerymapismainlyactivatedaroundthecorrespondents,
whereastheseregionsarelessactivatedintheaveragelatentmap.
In practice, a cross-attention layer between a set input cross-attention layer to extract the relevant in-
of query vectors {x } and {y } is per- formation from H, and finally obtain an updated set
n n=1...Q n n=1...N (cid:110)t (cid:110) (cid:111) (cid:110) (cid:111) (cid:111)
formed in parallel, which is often computationally of latent vectors m(0) , h(0) . On
i s,i
demanding since, for each head, the coefficients are i=1...M i=1...L
(cid:110) (cid:111)
(0)
stored in a Q×N matrix. A self-attention layer is a the one hand, the outputs h contain the
s,i
cross-attentionlayerwhere{y n} n=1...N={x n} n=1...Q. information relevant to find theiri= re1 s.. p.L ective corre-
spondents within H. On the other hand, the outputs
t
3.2 Featureextractionstage (cid:110) (0)(cid:111)
m extractedageneralrepresentationofH
i t
i=1...M
since they are not aware of the query 2D locations.
Our method takes as input a source image I (H ×
s s
Inpractice,wesetM to128. Afterward,aseriesofS
W ×3), a target image I (H ×W×3) and a set of
s t t t
self-attentionlayersareappliedtothelatentvectorsto
2D query locations {p s,i} i=1...L. The first stage of (cid:110)(cid:110) (S)(cid:111) (cid:110) (S)(cid:111) (cid:111)
SAM is a classical feature extraction stage. From get m , h . In these layers,
i s,i
i=1...M i=1...L
the source image I s (H s×W s×3) and the target im- alllatentvectorscancommunicateandadjustwithre-
age I
t
(H t×W t×3), dense visual source features F
s
specttoeachother.Forinstance,theset(cid:110) m(s)(cid:111)
(H 4s×W 4s×128) and target features F t (H 4t×W 4t×128) canbeusedtodisambiguatecertaincorrespi ondei= n1 c.. e.M
s.
are extracted using a siamese CNN backbone. The
Then,H isusedasaqueryinanoutputcross-attention
PositionalEncodings(PE)ofthesourceandtargetare t
layer to extract the relevant information from the la-
computed, using an MLP (Sarlin et al., 2020), and
tentvectors. TheresultingtensoriswrittenHout. For
concatenated with the visual features of the source t
andtargettoobtaintwotensorsH (Hs×Ws×256)and each updated descriptor h(S) , a correspondence map
s 4 4 s,i
H t (H 4t×W 4t×256). For each 2D query point p s,i, a C t,i (ofsize H 4t×W 4t)isobtainedbycomputingthedot
descriptor h s,i of size 256 is extracted from H s. In product between h(S) and Hout. Finally, for each 2D
practice, we use integer query locations thus there is s,i t
query location p , the predicted 2D correspondent
s,i
noneedforinterpolationhere. Technicaldetailscon-
locationpˆ isdefinedastheargmaxofC .
t,i t,i
cerningthisstageareprovidedintheappendix.
In Figure 3, we propose a visualization of the
learnedlatentvectorsofSAM.Forvisualizationpur-
3.3 Latentspaceattention-basedstage poses,weusedL=64.Thustheaveragequerymapis
obtainedbyaveragingthe64correspondencemapsof
In order to allow for the descriptors {h s,i}
i=1...L
(at the64querylocations,whiletheaveragelatentmapis
training-time we use L=1024) to communicate and obtained by averaging the 128 correspondence maps
adjust with respect to H t, we draw inspiration from ofthe128learnedlatentvectors. Weobservethatthe
Perceiver (Jaegle et al., 2021; Jaegle et al., 2022), averagequerymapismainlyactivatedaroundthecor-
and consider a set of N=M+L latent vectors, com- respondents, whereastheseregionsarelessactivated
posed of M learned latent vectors {m i} i=1...M and intheaveragelatentmap.
the L descriptors {h } . These latent vectors
s,i i=1...L
(cid:8) (cid:9)
{m} ,{h } are used as queries in an
i i=1...M s,i i=1...LSourceImage TargetImage Visio-positionalmap Positionalmap
Figure4: Visualization-Structuredattention. Thevisio-positionalandpositionalmapsarecomputedbeforetheoutput
cross-attention. Red crosses represent the ground-truth correspondences. Blue and Green crosses are the maxima of the
visio-positional and positional maps, respectively. One can see that the visio-positional maps are highly multimodal (i.e.,
sensitivetorepetitivestructures)whilethepositionalmapsarealmostunimodal.
3.4 Structuredattention 3.5 Loss
At the end of the feature extraction stage, the upper At training time, we use a cross-entropy (CE) loss
halfofeachvectorisvisualfeatures,whilethelower function (Germain et al., 2020) on each correspon-
half is positional encodings. In each attention layer dence map C in order to maximise its score at the
t,i
(inputcross-attention,self-attentionandoutputcross- groundtruthlocationp :
t,i
attention), wefound it importantto structure the lin- (cid:18) (cid:19)
expC (p )
eartransformationmatricesW o,h andW v,h toconstrain CE(C t,i,p t,i)=−ln t,i t,i . (5)
thelowerhalfofeachoutputvectortoonlycontaina
∑ qexpC t,i(q)
lineartransformationofthepositionalencodings:
3.6 Refinement
D 2×DH
D (cid:122)× (cid:125)(cid:124)D (cid:123)H (cid:20) W(cid:122) o(cid:125) ,h(cid:124) ,u(cid:123)
p
(cid:21) The previously described architecture produces
W = . (4)
o,h 0 | W coarse correspondence maps of resolution 1/4. Thus
o,h,low
(cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125) the predicted correspondents {pˆ } need to be
D 2×D 2H D×DH
refined. To do so, we simply
ust, ei ai= s1 e.. c.L
ond siamese
2 2
CNNthatoutputsdensesourceandtargetfeaturesat
The matrices W and the fully connected layers
v,h
fullresolution. Foreach2Dquerylocationp ,acor-
within the MLP (at the end of each attention mod- s,i
respondence map is computed on a window of size
ule)arestructuredtheexactsameway. Consequently,
11 centered around the coarse prediction. The pre-
throughoutthenetwork,thelowerhalfofeachlatent
dicted2Dcorrespondentlocationpˆ isdefinedasthe
vectoronlycontainstransformationsofpositionalen- t,i
argmaxofthiscorrespondencemap. Thisrefinement
codings,butnovisualfeature.
network is trained separately using the same cross-
InFigure4weproposeavisualizationoftwodif-
entropy loss eq. 5. Technical details are provided in
ferent correspondence maps before the output cross-
theappendix.
attention.
The first map is produced using the first 128 di-
mensions of the feature representation and contains
high-levelvisio-positionalfeatures. Thesecondmap 4 Experiments
isproducedusingthe128lastdimensionsofthefea-
ture representation and contains only positional en- Intheseexperiments,wefocusonevaluatingsixSDF
codings. By building these distinct correspondence networks(LoFTR(Sunetal.,2021),QuadTree(Tang
maps we can observe that while the visio-positional etal.,2022),ASpanFormer(Chenetal.,2022),3DG-
representationissensitivetorepetitivestructures,the STFM(Maoetal.,2022),MatchFormer(Wangetal.,
purelypositionalrepresentationtendstobeonlyacti- 2022), TopicFM (Giang et al., 2023)) and our pro-
vatedintheareasneighbouringthematch. posed architecture SAM. In all the following Tables,
bestandsecondbestresultsarerespectivelyboldand
underlined.Table 1: Evaluation on MegaDepth1500 (Sarlin et al., 2020). We report Matching Accuracy (6) for several thresholds η,
computedonthesetofallthesemi-densequerylocations(sourcegridwithstride8)withavailablegroundtruthcorrespon-
dents(MA),andonasubsetcontainingonlyquerylocationsthatarewithinatexturedregionofthesourceimage(MAtext).
Concerningtheposeestimationmetrics,wereporttheclassicalAUCat5,10and20degrees. TheproposedSAMmethod
outperformsSDFmethodsintermsofposeestimation,whileSDFmethodsaresignificantlybetterintermsofMA.However,
whenuniformregionsareignored(MAtext),SAMoftensurpassesSDFmethods. Theseresultshighlightastrongcorrelation
betweentheabilitytoestablishprecisecorrespondencesintexturedregionsandtheaccuracyoftheresultingestimatedpose.
MatchingAccuracy Poseestimationon
MatchingAccuracy(MA)(6)↑
Method ontexturedregions(MAtext)(6)↑ 1/8grid(AUC)↑
η=1 η=2 η=3 η=5 η=10 η=20 η=1 η=2 η=3 η=5 η=10 η=20 @5o @10o @20o
LoFTR(Sunetal.,2021) 49.7 73.2 81.6 87.4 90.5 91.8 55.3 75.2 81.6 86.9 89.9 91.9 52.8 69.2 82.0
MatchFormer(Wangetal.,2022) 51.1 73.4 81.0 86.9 89.5 90.9 56.5 75.6 81.8 87.1 89.6 90.9 52.9 69.7 82.0
TopicFM(Giangetal.,2023) 51.4 75.4 83.7 89.9 92.9 93.5 59.8 77.6 84.8 90.4 92.9 93.7 54.1 70.1 81.6
3DG-STFM(Maoetal.,2022) 51.6 73.7 80.7 86.4 89.0 90.7 57.0 75.8 81.8 86.8 88.8 90.5 52.6 68.5 80.0
ASpanFormer(Chenetal.,2022) 52.0 76.2 84.5 90.7 93.7 94.8 62.2 80.3 85.9 91.0 93.7 94.7 55.3 71.5 83.1
LoFTR+QuadTree(Tangetal.,2022) 51.6 75.9 84.1 90.2 93.1 94.0 61.7 79.9 85.5 90.5 93.3 94.1 54.6 70.5 82.2
SAM(ours) 48.5 70.4 78.0 83.0 85.4 86.4 67.9 83.8 87.3 90.6 93.6 95.2 55.8 72.8 84.2
Table2:EvaluationonHPatches(Balntasetal.,2017).WereportMatchingAccuracy(6)forseveralthresholdsη,computed
onthesetofallthesemi-densequerylocations(sourcegridwithstride8)withavailablegroundtruthcorrespondents(MA),
andonasubsetcontainingonlyquerylocationsthatarewithinatexturedregionofthesourceimage(MAtext).Concerningthe
homographyestimationmetrics,wereporttheclassicalAUCat3,5and10pixels.TheproposedSAMmethodisonparwith
SDFmethodsintermsofhomographyestimation,whileSDFmethodsaresignificantlybetterintermsofMA.However,when
uniformregionsareignored(MAtext),SAMmatchesSDFperformances.Theseresultshighlightastrongcorrelationbetween
theabilitytoestablishprecisecorrespondencesintexturedregionsandtheaccuracyoftheresultingestimatedhomography.
MatchingAccuracy MatchingAccuracyon Homographyestimation
Method (MA)(6)↑ texturedregions(MAtext)(6)↑ (AUC)↑
η=3 η=5 η=10 η=3 η=5 η=10 @3px @5px @10px
LoFTR(Sunetal.,2021) 66.8 74.3 77.3 67.6 75.3 78.4 65.9 75.6 84.6
MatchFormer(Wangetal.,2022) 66.2 74.9 78.2 67.7 75.8 79.1 65.0 73.1 81.2
TopicFM(Giangetal.,2023) 72.7 85.0 87.5 74.0 86.0 88.5 67.3 77.0 85.7
3DG-STFM(Maoetal.,2022) 64.9 75.1 78.2 66.2 74.3 77.6 64.7 73.1 81.0
ASpanFormer(Chenetal.,2022) 76.2 86.2 88.7 73.9 85.8 88.4 67.4 76.9 85.6
LoFTR+QuadTree(Tangetal.,2022) 70.2 83.1 85.9 73.5 84.3 86.9 67.1 76.1 85.3
SAM(ours) 62.4 70.9 74.2 73.4 86.6 89.3 67.1 76.9 85.9
Table 3: Evaluation on ETH3D (Scho¨ps et al., 2019) for different frame interval sampling rates r. We report Matching
Accuracy (6) for several thresholds η, computed on the set of all the semi-dense query locations (source grid with stride
8)withavailablegroundtruthcorrespondents(MA).ForETH3D,groundtruthcorrespondentsarebasedonstructurefrom
motiontracks. Consequently,theMAalreadyignoresuntexturedregionsofthesourceimageswhichexplainswhySAMis
abletooutperformSDFmethods.
MatchingAccuracy(MA)(6)↑
Method
r=3 r=7 r=15
η=1 η=2 η=3 η=5 η=10 η=1 η=2 η=3 η=5 η=10 η=1 η=2 η=3 η=5 η=10
LoFTR(Sunetal.,2021) 44.8 76.5 88.4 97.0 99.4 39.7 73.1 87.6 95.9 98.5 33.3 66.2 84.8 92.5 96.3
MatchFormer(Wangetal.,2022) 45.5 77.1 89.2 97.2 99.7 40.4 73.8 87.8 96.6 99.0 34.2 66.7 84.9 93.5 97.0
TopicFM(Giangetal.,2023) 45.1 76.9 89.0 97.2 99.6 39.9 73.5 87.9 96.4 99.0 33.8 66.4 85.0 92.8 96.5
3DG-STFM(Maoetal.,2022) 43.9 76.3 88.0 96.9 99.3 39.3 72.7 87.4 95.5 98.3 32.4 65.7 84.7 92.0 96.0
ASpanFormer(Chenetal.,2022) 45.8 77.6 89.6 97.8 99.8 40.6 73.8 88.1 96.8 99.0 34.3 66.8 85.3 93.9 97.3
LoFTR+QuadTree(Tangetal.,2022) 45.9 77.5 89.5 97.8 99.7 40.8 74.0 88.3 97.0 99.2 34.5 66.8 85.4 94.0 97.3
SAM(ours) 53.4 79.9 91.5 98.0 99.8 48.6 78.6 91.7 98.2 99.4 40.1 70.2 87.8 95.4 97.8Implementations - For each network, we employ 4.1 EvaluationonMegaDepth
the code and weights (trained on MegaDepth) made
availablebytheauthors.ConcerningSAM,wetrainit WeconsidertheMegaDepth1500(Sarlinetal.,2020)
similarlytotheSDFnetworksonMegaDepth(Liand benchmark. We use the exact same settings as those
Snavely, 2018), during 100 hours, on four GeForce used by SDF methods, such as an image resolution
GTX1080Ti(11GB)GPUs(seetheappendixforde- of1200. TheresultsareprovidedinTable1. Were-
tails). Ourcodewillbemadeavailable. portMatchingAccuracy(6),forseveralthresholdsη,
computedonthesetofallthesemi-densequeryloca-
Query locations - Concerning SDF methods, the
tions(sourcegridwithstride8)withavailableground
query locations are defined as the source grid loca-
truth correspondents (MA). The ground truth corre-
tions using a stride of 8. Thus, in order to be able
spondentsareobtainedusingtheavailabledepthmaps
to compare the performances of SAM against such
and camera poses. Consequently, many query loca-
methods, we use the exact same query locations. To
tionslocatedinuntexturedregionshaveagroundtruth
do so (recall that SAM was trained with L = 1024
correspondent. Thus, we also report the matching
querylocations),wesimplyshufflethesourcegridlo-
accuracy computed only on query locations that are
cations(withstride8)andfeedthemtoSAMbybatch
withinatexturedregionofthesourceimage(MA ).
ofsize1024(theCNNfeaturesarecachedmakingthe text
Concerningtheposeestimationmetrics,wereportthe
processingofeachminibatchveryefficient).
classicalAUCat5,10and20degrees. Theproposed
Evaluation criteria - Regarding the pose and ho-
SAM method outperforms SDF methods in terms of
mography estimation metrics, we use the classical
poseestimation,whileSDFmethodsaresignificantly
AUCmetricsforeachdataset.ThusineachTable,the
better in terms of MA. However, when uniform re-
pose/homographyresultsweobtainedforSDFmeth-
gionsareignored(MA ),SAMoftensurpassesSDF
text
ods (we re-evaluated each method) are the same as
methods. These results highlight a strong correla-
the results published in the respective papers. Con-
tion between the ability to establish precise corre-
cerningSAM,thecorrespondencesareclassicallyfil-
spondences in textured regions and the accuracy of
tered(similarlytowhatSDFmethodsdo)beforethe
theresultingestimatedpose.
pose/homography estimator using a simple Mutual
InFigure5,wereportqualitativeresultsthatvisu-
NearestNeighborwithathresholdof5pixels.
allyillustratethepreviousfindings.
Inordertoevaluatetheabilityofeachmethodto
establishcorrespondences,weconsidertheMatching
4.2 EvaluationonHPatches
Accuracy (MA) as in (Truong et al., 2020), i.e., the
averageonallimagesoftheratioofcorrectmatches,
We evaluate the different architectures on
fordifferentpixelerrorthresholds(η):
(cid:104)(cid:13) (cid:13) (cid:105) HPatches (Balntas et al., 2017) (see Table 2).
MA(η)=
1 ∑K ∑L i=k 1 (cid:13) (cid:13)pˆ tk,i−pG tk,T i(cid:13) (cid:13) 2<η
, (6)
W SDe Fus me ett hh oe dse .xac Wt esa rm epe ors tet Mtin ag ts cha ins gth Ao cs ce uru as ce yd (b 6y
)
K L
k=1 k for several thresholds η, computed on the set of all
where K is the number of pairs of images, L is the
k the semi-dense query locations (source grid with
numberofgroundtruthcorrespondencesavailablefor
stride 8) with available ground truth correspondents
the image pair #k and [·] is the Iverson bracket. We
(MA). The ground truth correspondents are obtained
refertothismetricasMAandnot”PercentageofCor-
using the available homography matrices. Conse-
rectKeypoints”becausewefoundthistermmislead-
quently, many query locations located in untextured
ing in cases where the underlying ground truth cor-
regions have a ground truth correspondent. Thus,
respondences are not based on keypoints, as it is the
we also report the matching accuracy computed
caseinMegaDepthandHPatches.
only on query locations that are within a textured
We also propose to introduce the Matching Ac-
region of the source image (MA ). Concerning
text
curacy in textured regions (MA ) that consists in
text the homography estimation metrics, we report the
ignoring, in eq. (6), ground truth correspondences
classical AUC at 3, 5 and 10 pixels. The proposed
whosequerylocationiswithinalow-contrastregion
SAM method is on par with SDF methods in terms
ofthesourceimage,i.e.,analmostuniformregionof
of homography estimation, while SDF methods are
thesourceimage.
significantly better in terms of MA. However, when
Note that SAM’s MNN (and the MNN of SDF
uniformregionsareignored(MA ), SAMmatches
text
methods) is only used for the pose/homography es-
SDF performances. These results highlight a strong
timation, i.e., it is not used to compute matching ac-
correlation between the ability to establish precise
curacies,otherwisethesetofcorrespondenceswould
correspondencesintexturedregionsandtheaccuracy
notbethesameforeachmethod.Table 4: Ablation study of proposed SAM method
5 Conclusion
(MegaDepthvalidationset).
WeproposedanovelStructuredAttention-basedim-
Method η=1 η=2 η=M 5Ate ηxt =↑ 10 η=20 η=50 age Matching architecture (SAM). The flexibility of
thisnovelarchitectureallowedustofairlycompareit
SiameseCNN 0.029 0.112 0.436 0.581 0.622 0.687
+InputCAandSA(x16) 0.137 0.321 0.671 0.734 0.767 0.822 against SDF methods, i.e., in all the experiments we
+LearnedLVandoutputCA 0.132 0.462 0.823 0.871 0.898 0.935 usedthesamequerylocations(sourcegridwithstride
+PEconcatenated 0.121 0.419 0.796 0.868 0.902 0.939 8). The experiments highlighted a counter-intuitive
+StructuredAM 0.140 0.487 0.857 0.902 0.922 0.947 result on two datasets (MegaDepth and HPatches):
+Refinement(Fullmodel) 0.673 0.791 0.870 0.902 0.921 0.946
on the one hand SAM either outperforms or is on
parwithSDFmethodsintermsofpose/homography
estimation metrics, but on the other hand SDF ap-
oftheresultingestimatedhomography.
proachesaresignificantlybetterthanSAMintermsof
InFigure6,wereportqualitativeresultsthatvisu-
MatchingAccuracy(MA).HeretheMAiscomputed
allyillustratethepreviousfindings.
on all the semi-dense locations of the source image
withavailablegroundtruthcorrespondent, whichin-
4.3 EvaluationonETH3D
cludes both textured and uniform regions. We pro-
posed to limit the computation of the matching ac-
We evaluate the different networks on several se-
curacy to textured regions, and showed that in this
quences of the ETH3D dataset (Scho¨ps et al., 2019)
caseSAMoftensurpassesSDFmethods. Thesefind-
asproposedin(Truongetal.,2020). Differentframe
ingshighlightedastrongcorrelationbetweentheabil-
interval sampling rates r are considered. As the rate
ity to establish precise correspondences in textured
r increases, the overlap between the image pairs re-
regions and the accuracy of the resulting estimated
duces,hencemakingthematchingproblemmoredif-
pose/homography. We also evaluated the aforemen-
ficult. The results are provided in Table 3. We re-
tioned methods on ETH3D which confirmed, on a
portMatchingAccuracy(6)forseveralthresholdsη,
third dataset, that SAM has a strong ability to estab-
computedonthesetofallthesemi-densequeryloca-
lish correspondences in textures regions. We finally
tions(sourcegridwithstride8)withavailableground
performed an ablation study of SAM to demonstrate
truthcorrespondents(MA).ForETH3D,groundtruth
thateachpartofthearchitectureisimportanttoobtain
correspondents are based on structure from motion
suchastrongmatchingcapacity.
tracks. Consequently,theMAignoresuntexturedre-
gionsofthesourceimageswhichexplainswhySAM
isabletooutperformSDFmethodsintermsofMA.
REFERENCES
InFigure7,wereportqualitativeresultsthatillus-
tratetheaccuracyoftheproposedSAMmethod.
Arnold, E., Wynn, J., Vicente, S., Garcia-Hernando, G.,
Monszpart, A´., Prisacariu, V. A., Turmukhambetov,
4.4 Ablationstudy
D.,andBrachmann,E.(2022). Map-freevisualrelo-
calization: Metricposerelativetoasingleimage. In
In Table 4, we propose an ablation study of SAM to ECCV.
evaluate the impact of each part of our architecture. Balntas, V., Lenc, K., Vedaldi, A., and Mikolajczyk, K.
This study is performed on MegaDepth validation (2017). Hpatches: A benchmark and evaluation of
scenes. Starting from a standard siamese CNN, we handcraftedandlearnedlocaldescriptors. InCVPR.
show that a significant gain in performance can sim- Chen,H.,Luo,Z.,Zhou,L.,Tian,Y.,Zhen,M.,Fang,T.,
ply be obtained with the input cross-attention layer McKinnon,D.,Tsin,Y.,andQuan,L.(2022). Aspan-
(herethePEisaddedandnotconcatenated)andself- former: Detector-free image matching with adaptive
spantransformer. InECCV.
attention layers. We then add learned latent vectors
Dusmanu, M., Rocco, I., Pajdla, T., Pollefeys, M., Sivic,
(LV) in the latent space and use an output cross-
J.,Torii,A.,andSattler,T.(2019). D2-net: Atrain-
attentionwhichagainsignificantlyimprovestheper-
ableCNNforjointdescriptionanddetectionoflocal
formance. Concatenatingthepositionalencodingin- features. InCVPR.
formation instead of adding it to the visual features
Edstedt,J.,Athanasiadis,I.,Wadenba¨ck,M.,andFelsberg,
reduces the matching accuracy at η=2 and η=5. M. (2023). Dkm: Dense kernelized feature match-
However,combiningitwithstructuredattentionleads ing for geometry estimation. In Proceedings of the
toasignificantimprovementintermsofMA.Finally, IEEE/CVFConferenceonComputerVisionandPat-
asexpected,therefinementstepimprovesthematch- ternRecognition,pages17765–17775.
ingaccuracyforsmallpixelerrorthresholds. Germain, H., Bourmaud, G., and Lepetit, V. (2020).SAM(ours) LoFTR+QuadTree(Tangetal.,2022)
MA@2=60.6MAtext@2=84.0errR=2.1’errT=5.4’ MA@2=62.8MAtext@2=64.0errR=5.5’errT=22.4’
MA@2=62.3MAtext@2=81.1errR=1.9’errT=5.5’ MA@2=66.2MAtext@2=67.5errR=2.1’errT=9.4’
MA@2=79.0MAtext@2=83.1errR=1.5’errT=3.9’ MA@2=79.9MAtext@2=78.7errR=2.0’errT=6.4’
MA@5=69.1MAtext@5=78.0errR=3.8’errT=15.4’ MA@5=70.7MAtext@5=72.1errR=4.9’errT=19.6’
Figure5: QualitativeresultsonMegaDepth1500. Foreachimagepair: (toprow)Visualizationofestablishedcorrespon-
dences used to compute the MA, (bottom row) Visualization of established correspondences used to compute the MAtext.
Linecolorsindicatethedistanceinpixelswithrespecttothegroundtruthcorrespondent.SAM(ours) LoFTR+QuadTree(Tangetal.,2022)
MA@5=69.7MAtext@5=88.6AUC@5=77.4 MA@5=72.1MAtext@5=76.6AUC@5=76.1
MA@5=74.4MAtext@5=86.2AUC@5=81.5 MA@5=77.1MAtext@5=42.8AUC@5=75.9
Figure6:QualitativeresultsonHPatches.Linecolorsindicatethedistanceinpixelstothegroundtruthcorrespondent.
S2DNet: learningimagefeaturesforaccuratesparse- Li, X., Han, K., Li, S., and Prisacariu, V. (2020). Dual-
to-densematching. InECCV. resolutioncorrespondencenetworks. NeurIPS.
Germain,H.,Lepetit,V.,andBourmaud,G.(2021). Neural Li,Z.andSnavely,N.(2018).Megadepth:Learningsingle-
reprojectionerror:Mergingfeaturelearningandcam- viewdepthpredictionfrominternetphotos. InCVPR.
eraposeestimation. InCVPR. Lin, T.-Y., Dolla´r, P., Girshick, R., He, K., Hariharan, B.,
Germain,H.,Lepetit,V.,andBourmaud,G.(2022). Visual and Belongie, S. (2017). Feature pyramid networks
correspondencehallucination. InICLR. forobjectdetection. InCVPR.
Giang,K.T.,Song,S.,andJo,S.(2023). TopicFM:Robust Lowe, D.G.(1999). Objectrecognitionfromlocalscale-
andinterpretablefeaturematchingwithtopic-assisted. invariantfeatures. InICCV.
InAAAI. Mao,R.,Bai,C.,An,Y.,Zhu,F.,andLu,C.(2022). 3DG-
Heinly,J.,Scho¨nberger,J.L.,Dunn,E.,andFrahm,J.-M. STFM: 3D geometric guided student-teacher feature
(2015). ReconstructingtheWorld*inSixDays*(as matching. InECCV.
Captured by the Yahoo 100 Million Image Dataset). Ni,J.,Li,Y.,Huang,Z.,Li,H.,Bao,H.,Cui,Z.,andZhang,
InCVPR. G.(2023). PATS:Patchareatransportationwithsub-
Jaegle, A., Borgeaud, S., Alayrac, J.-B., Doersch, C., division for local feature matching. In Proceedings
Ionescu,C.,Ding,D.,Koppula,S.,Zoran,D.,Brock, oftheIEEE/CVFConferenceonComputerVisionand
A.,Shelhamer,E.,Henaff,O.J.,Botvinick,M.,Zis- PatternRecognition,pages17776–17786.
serman,A.,Vinyals,O.,andCarreira,J.(2022). Per- Ono,Y.,Trulls,E.,Fua,P.,andYi,K.M.(2018). LF-Net:
ceiverIO:Ageneralarchitectureforstructuredinputs learninglocalfeaturesfromimages. NeurIPS.
&outputs. InICLR.
Revaud, J., DeSouza, C., Humenberger, M., andWeinza-
Jaegle,A.,Gimeno,F.,Brock,A.,Vinyals,O.,Zisserman, epfel,P.(2019). R2D2:reliableandrepeatabledetec-
A., and Carreira, J. (2021). Perceiver: General per- toranddescriptor. NeurIPS.
ceptionwithiterativeattention. InICML.
Rocco,I.,Arandjelovic´,R.,andSivic,J.(2020a). Efficient
Jin, Y., Mishkin, D., Mishchuk, A., Matas, J., Fua, P., Yi, neighbourhood consensus networks via submanifold
K.M.,andTrulls,E.(2020). ImageMatchingacross sparseconvolutions. InECCV.
WideBaselines:FromPapertoPractice. IJCV.
Rocco, I., Cimpoi, M., Arandjelovic´, R., Torii, A., Pajdla,
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. T., and Sivic, J. (2018). Neighbourhood consensus
(2020). TransformersareRNNs: Fastautoregressive networks. NeurIPS.
transformerswithlinearattention. InICML.SAM(ours) LoFTR+QuadTree(Tangetal.,2022)
MAtext@2=75.0 MAtext@2=68.8
MAtext@2=65.4 MAtext@2=61.5
MAtext@2=80.0 MAtext@2=68.0
MAtext@2=87.2 MAtext@2=69.2
Figure7:QualitativeresultsonETH3D.Linecolorsindicatethedistanceinpixelstothegroundtruthcorrespondent.
Rocco, I., Cimpoi, M., Arandjelovic´, R., Torii, A., Pajdla, Sva¨rm,L.,Enqvist,O.,Kahl,F.,andOskarsson,M.(2017).
T., and Sivic, J. (2020b). NCNet: neighbourhood City-scalelocalizationforcameraswithknownverti-
consensus networks for estimating image correspon- caldirection. PAMI.
dences. PAMI. Taira,H.,Okutomi,M.,Sattler,T.,Cimpoi,M.,Pollefeys,
Sarlin,P.-E.,DeTone,D.,Malisiewicz,T.,andRabinovich, M., Sivic, J., Pajdla, T., and Torii, A. (2018). In-
A.(2020).Superglue:Learningfeaturematchingwith Loc: Indoor visual localization with dense matching
graphneuralnetworks. InCVPR. andviewsynthesis. InCVPR.
Sarlin, P.-E., Dusmanu, M., Scho¨nberger, J. L., Speciale, Tang,S.,Zhang,J.,Zhu,S.,andTan,P.(2022). Quadtree
P.,Gruber,L.,Larsson,V.,Miksik,O.,andPollefeys, attentionforvisiontransformers. InICLR.
M.(2022). LaMAR:BenchmarkingLocalizationand Truong, P., Danelljan, M., and Timofte, R. (2020). Glu-
MappingforAugmentedReality. InECCV. net:Global-localuniversalnetworkfordenseflowand
Sattler,T.,Torii,A.,Sivic,J.,Pollefeys,M.,Taira,H.,Oku- correspondences. InCVPR.
tomi, M., and Pajdla, T. (2017). Are large-scale 3D Truong, P., Danelljan, M., Van Gool, L., and Timofte, R.
models really necessary for accurate visual localiza- (2021).Learningaccuratedensecorrespondencesand
tion? InCVPR. whentotrustthem. InCVPR.
Scho¨nberger, J. L. and Frahm, J.-M. (2016). Structure- Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
from-motionrevisited. InCVPR. L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I.
Scho¨ps, T., Sattler, T., and Pollefeys, M. (2019). BAD (2017). Attentionisallyouneed. NeurIPS.
SLAM: Bundle adjusted direct RGB-D SLAM. In Wang, Q., Zhang, J., Yang, K., Peng, K., and Stiefelha-
CVPR. gen, R.(2022). Matchformer: Interleavingattention
Strasdat,H.,Davison,A.J.,Montiel,J.M.,andKonolige, intransformersforfeaturematching. InACCV.
K.(2011). Doublewindowoptimisationforconstant Wang,W.,Xie,E.,Li,X.,Fan,D.-P.,Song,K.,Liang,D.,
timevisualslam. InICCV. Lu,T.,Luo,P.,andShao,L.(2021). Pyramidvision
Sun, J., Shen, Z., Wang, Y., Bao, H., and Zhou, X. transformer: A versatile backbone for dense predic-
(2021). LoFTR:detector-freelocalfeaturematching tionwithoutconvolutions. InICCV.
withtransformers. InCVPR. Xie,E.,Wang,W.,Yu,Z.,Anandkumar,A.,Alvarez,J.M.,andLuo,P.(2021). Segformer: Simpleandefficient
design for semantic segmentation with transformers.
NeurIPS.
Yi,K.M.,Trulls,E.,Lepetit,V.,andFua,P.(2016). LIFT:
Learnedinvariantfeaturetransform. InECCV.
Zhang, Z., Sattler, T., andScaramuzza, D.(2021). Refer-
enceposegenerationforlong-termvisuallocalization Figure 8: Training (left) and Validation (right) loss for
vialearnedfeaturesandviewsynthesis. IJCV. refinementResNet-18+FPN.x-axisrepresentthenumber
Zhou,Q.,Sattler,T.,andLeal-Taixe,L.(2021). Patch2Pix: of mini-batches seen by the model and y-axis the cross-
Epipolar-guided pixel-level correspondences. In entropyvalue.
CVPR.
Zhu,S.andLiu,X.(2023). Pmatch: Pairedmaskedimage
modelingfordensegeometricmatching. InProceed-
ingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pages21909–21918.
Figure 9: Training (left) and Validation (right) loss for
APPENDIX
SAM.x-axisrepresentthenumberofmini-batchesseenby
themodelandy-axisthecross-entropyvalue.
SAMarchitecture. EachattentionlayerinSAMis
aclassicalseriesofsoftmaxattention,normalization, of 10−3 and a batch size of 1 (Figure 8). We min-
MLPandnormalizationlayerswithresidualconnec- imize the cross-entropy of the full resolution corre-
tions, except for the ”output” cross-attention where spondencemapsproducedusingtheFPN.
we removed the MLP, the last normalization and the TheCNNbackboneofSAMisinitializedwiththe
residualconnection. weights of the previously trained network. We train
We empirically choose a combination of 16 self- SAM for 100 hours. The same setup with 4 Nvidia
attention layers and 128 vectors in the learned latent GTX1080TiGPUs,Adamoptimizerandabatchsize
space. Wefoundthiscombinationtobeagoodtrade- of 1 is used. Concerning the learning rate schedule,
offbetweenperformanceandcomputationalcost. we use a linear warm-up of 5000 steps (from 0 to
10−4) and then an exponential decay (towards 10−5)
(Figure 9). We minimize the cross-entropy of the 1
Backbonedescription. WeuseamodifiedResNet- 4
resolutioncorrespondencemapsproducedbySAM.
18asourfeatureextractionbackbone. Weextractthe
feature map after the last ResBlock in which we re-
moved the last ReLU. The features are reduced to
1th
of the image resolution (3×H×W) by apply-
4
ing a stride of 2 in the first and third layer. The fea-
ture representation built by the backbone is of size
128×1H×1W.
4 4
Refinementdescription. ThepreviousCNNback-
boneisusedforrefinement. WesimplyaddedaFPN
(FeaturePyramidNetwork)moduletoextractthefea-
turemapsafterthefirstconvolutionallayerandeach
ResNetlayer. Itleavesuswithafeaturemapofsize
128×H×W on which the refinement stage is per-
formed.
Structuredattention. Inourimplementation,D=
256,weuse8heads,thusD =32.
H
Training details. The first part of the model to be
trained is the refinement model (CNN+FPN). It is
trained during 50 hours on 4 Nvidia GTX1080 Ti
GPUsusingAdamoptimizer,aconstantlearningrate