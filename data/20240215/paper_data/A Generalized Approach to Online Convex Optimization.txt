Pages:1–22
A Generalized Approach to Online Convex Optimization
Mohammad Pedramfar and Vaneet Aggarwal
Purdue University
{mpedramf,vaneet}@purdue.edu
Abstract
In this paper, we analyze the problem of online convex optimization in different settings.
We show that any algorithm for online linear optimization with fully adaptive adversaries is
an algorithm for online convex optimization. We also show that any such algorithm that
requires full-information feedback may be transformed to an algorithm with semi-bandit
feedback with comparable regret bound. We further show that algorithms that are designed
for fully adaptive adversaries using deterministic semi-bandit feedback can obtain similar
bounds using only stochastic semi-bandit feedback when facing oblivious adversaries. We
usethistodescribegeneralmeta-algorithmstoconvertfirstorderalgorithmstozerothorder
algorithms with comparable regret bounds. Our framework allows us to analyze online
optimization in various settings, such full-information feedback, bandit feedback, stochastic
regret, adversarial regret and various forms of non-stationary regret. Using our analysis, we
provide the first efficient projection-free online convex optimization algorithm using linear
optimization oracles.
1. Introduction
1.1. Overview
Onlineoptimizationproblemsrepresentaclassofproblemswherethedecision-maker,referred
to as an agent, aims to make sequential decisions in the face of an adversary with incomplete
information(Shalev-Shwartz,2012;Hazanetal.,2016). Thissettingmirrorsarepeatedgame,
where the agent and the adversary engage in a strategic interplay over a finite time horizon,
commonly denoted as T. The dynamics of this game involve the agent’s decision-making
process, the adversary’s strategy, the information exchange through a query oracle.
In this paper, we present a comprehensive approach to solving online convex optimization
problems, addressing scenarios where the adversary can choose a function from the class of
µ–strongly convex functions (or convex functions for µ = 0) at each time step. Our approach
encompasses different feedback types, including bandit, semi-bandit and full-information
feedback in each iteration. It also encompasses different types of regret, such as adversarial
regret, stochastic regret, and various forms of non-stationary regret. While the problem has
been studied in many special cases, this comprehensive approach sheds light on relations
between different cases and the powers and limitations of some of the approaches used in
different settings.
One of our key contribution lies in establishing that any semi-bandit feedback online
linear (or quadratic) optimization algorithm for fully adaptive adversary is also an online
convex(orstronglyconvex)optimizationalgorithm. Whiletheaboveresultisforsemi-bandit
feedback, we then show that in online convex optimization for fully adaptive adversary,
4202
beF
31
]GL.sc[
1v12680.2042:viXraFigure 1: Summary of the main results
Convex Linear
Oblivious Fully Adaptive
D, SB; D D, FI; D
Theorem 1
Theorem 4
Linear Linear Convex Convex
Oblivious Theorem 1 Fully Adaptive Theorem 2 Fully Adaptive Theorem 5 Oblivious
D, SB; D D, SB; D D, SB; D S, SB; D
Theorem 8 Convex
Oblivious
Linear Convex Convex
Theorem 8 S, B; S
Fully Adaptive Theorem 2 Fully Adaptive Theorem 5 Oblivious
D, SB; S D, SB; S S, SB; S
Theorem 4
Linear
Fully Adaptive
D, FI; S
Each arrow describes a procedure or meta-algorithm to convert an algorithm from one setting into
another. The first line describes the function class - linear/convex. The second lines specifies the
type of the adversary - fully adaptive or oblivious. The last line starts with D or S, to denote
deterministic or stochastic oracles. After that, SB is used to denotes semi-bandit, B to denote bandit
and FI to denote full information feedback. Finally, the last D or S is used to denotes deterministic
or stochastic algorithm.
semi-bandit feedback is generally enough and more information is not needed. Further, we
show that algorithms that are designed for fully adaptive adversaries using deterministic
semi-bandit feedback can obtain similar bounds using only stochastic semi-bandit feedback
when facing oblivious adversaries. Finally, we introduce a meta algorithm, based on the
variant of classical result of (Flaxman et al., 2005), that converts algorithms for semi-bandit
feedback, such as the ones mentioned above, to algorithms for bandit feedback. The key
results are summarized in Figure 1.
As an application of our approach, we consider the problem of efficient projection-free
online convex optimization which was an open problem for over a decade until the result
of (Mhammedi, 2022; Garber and Kretzu, 2022). We show that the Follow-The-Perturbed-
Leader (FTPL) algorithm which was proposed for online linear optimization in (Kalai and
Vempala, 2005) achieves the optimal regret and time complexity in convex optimization
without the use of projections.
1.2. Related works
The term “online convex optimization” was first defined in (Zinkevich, 2003), where Online
Gradient Descend (OGD) was proposed, while this setting was first introduced in (Gordon,
1999). The Follow-The-Perturbed-Leader (FTPL) for online linear optimization was intro-
duced by (Kalai and Vempala, 2005), inspired by Follow-The-Leader algorithm originally
suggested by (Hannan, 1957). Follow-The-Regularized-Leader (FTRL) in the context of
online convex optimization was coined in (Shalev-Shwartz and Singer, 2007; Shalev-Shwartz,
2007). The equivalence of FTRL and Online Mirror Descent (OMD) was observed by (Hazan
2A Generalized Approach to Online Convex Optimization
and Kale, 2010). We refer the reader to surveys (Shalev-Shwartz, 2012) and (Hazan et al.,
2016) for more details.
Oblivious vs. adaptive adversaries: As we will discuss in Section 4, in deterministic
settings, i.e., deterministic algorithms and deterministic query oracles, the handling of
adaptive adversaries are trivially equivalent with the handling of oblivious adversaries. For
this reason, many previous papers that considered deterministic algorithms, such as OGD,
FTRL and OMD mentioned above simply did not discuss the difference. However, for
algorithms that are not deterministic such as FTPL, the distinction is non-trivial.
Projection-free convex optimization: In the past decade, numerous projection-free
OCO algorithms have emerged to tackle the computational limitations of their projection-
based counterparts (Hazan and Kale, 2012; Chen et al., 2018; Xie et al., 2020; Chen et al.,
2019; Hazan and Minasyan, 2020; Garber and Kretzu, 2020). Most of such projection-free
algorithms perform one or more linear optimizations per round. The best regret bound for
such algorithms, with no extra assumption on the functions or the convex set, is O(T3/4)
achieved in (Hazan and Minasyan, 2020).
There have been some results using membership or separation oracle class, instead of
calls to a linear optimization oracle. In particular, the approach proposed in (Mhammedi,
√
2022) obtains the optimal regret bound of O( T) with a time complexity of O(T log(T))
using a membership oracle. (Garber and Kretzu, 2022) obtain the optimal regret and time
complexity, i.e., O(T), using a separation oracle. Recently, (Gatmiry and Mhammedi, 2023)
proposed a new efficient method using Newton iterations. However, their algorithm requires
access to second order feedback.
Table 1 includes previous results on projection-free online convex optimization with O(T)
time complexity that use at most first order feedback. Only results that hold for general
bounded convex sets are included. Further, we do not include Lipschitz assumption in the
table for the first order case, since all existing results assume every function is Lipschitz and
the regret lower bound also contains the Lipschitz constant.
1.3. Key contributions
The closest work to our analysis in Theorem 2 is (Abernethy et al., 2008), where the authors
analyze online convex optimization with deterministic gradient feedback and fully adaptive
adversary, referred to as online convex game, and show that the min-max regret bounds of
online convex optimization is equal to that of online linear optimization (or online quadratic
optimization is all convex functions are µ-strongly convex) (See Corollary 2). Here we take a
step further and show in a more general setting, any algorithm for online linear optimization
with stochastic subgradient feedback results in a comparable regret with if the function class
is replaced by a class of convex functions. This allows us to show that the FTPL of (Kalai
and Vempala, 2005) is efficient in both time complexity and regret, and obtain first such
bound for an algorithm that uses a linear optimization oracle.
We also show that for any full-information algorithm for online convex optimization with
fully adaptive adversary, there is a corresponding algorithm with regret bound of the same
order which only requires semi-bandit feedback.
Next we show that any online optimization algorithm for fully adaptive adversaries using
deterministic semi-bandit feedback obtains similar regret bounds when facing an oblivious
3Table 1: Projection-free online convex optimization with O(T) time complexity
Algorithm Regularity Feedback Guarantee Regret/Advtype Regret
OSFW(Chenetal.,2018) β conv. sto. grad. exp. sto. /- O˜(T2/3)
ORGFW(Xieetal.,2020) β conv. sto. grad. w.h.p.† sto. /- O˜(T1/2)
RegularizedOFW(Hazanetal.,2016) diff. conv. det. grad. det. adv. /f. ada. O(T3/4)
OSPF(HazanandMinasyan,2020) β conv. det. grad. exp. &w.h.p. adv. /w. ada. O(T2/3)
SO-OGD(GarberandKretzu,2022) conv. det. subgrad. det. adv. /f. ada. O(T1/2)
Theorem3 conv. det. subgrad. exp. &w.h.p. adv. /f. ada. O(T1/2)
Theorem6 conv. sto. subgrad. exp. &w.h.p. adv. /obl. O(T1/2)
(Chenetal.,2019) Ldiff. conv. det. val. exp. adv. /obl. O(T4/5)
(GarberandKretzu,2020) L‡conv. det. val. exp. adv. /obl. O(T3/4)
Theorem9 Lconv. sto. val. exp. adv. /obl. O(T3/4)
Theorem9 conv. sto. val. exp. adv. /obl. O(T5/6)
Regularity: conv. = convex function that is not necessarily differentiable; diff. = differ-
entiable; L = L-Lipschitz; β = β-smooth, i.e. continuously differentiable with β-Lipschitz
gradient;
Feedback: det. = deterministic oracle; sto. = stochastic oracle; grad. = gradient oracle; subgrad. =
subgradient oracle; val. = value oracle;
Guarantee: det. =Deterministicregret(algorithmandtheoraclearedeterministic);exp. =Expected
regret(withrespecttotherandomnessofthealgorithmandtheoracle);w.h.p=withhighprobability;
Regret/adv type: sto. = stochastic regret; adv. = adversarial regret; obl. = oblivious adversary; w.
ada. = weakly adaptive adversary; f. ada. = fully adaptive adversary;
† The notion of w.h.p regret used in ORGFW is slightly different than the one we use here, since
it considers the randomness of the oracle and the algorithm is deterministic. The analysis of this
algorithm also requires extra assumptions on the query oracle.
‡In(GarberandKretzu,2020),itisassumedthatsubgradientsarebounded. AccordingtoLemma2.6
in (Shalev-Shwartz, 2012), this is equivalent to the functions being Lipschitz.
adversary using only stochastic semi-bandit feedback. This allows us to have a result that
generalizes both adversarial regret and stochastic regret, as they are commonly defined in
the literature.
Finally we show that the one point gradient estimator, which was first used in online
convex optimization by (Flaxman et al., 2005) on top of OGD could be directly applied to
any online convex optimization algorithm, which allows us to math the SOTA projection-
free online convex optimization and obtain first results that do not require the Lipschitz
assumption.
Our work sheds light on the relation between different problems, algorithms and tech-
niquesusedinonlineoptimizationandprovidesgeneralmeta-algorithmsthatallowconversion
between different problem settings. In particular, Theorem 2 shows some of the limitations
using smoothness to improve regret bounds. As an example, this shows that smoothness is
not needed in Theorem 3.1 in (Hazan and Minasyan, 2020) or differentiability is not needed
in (Zhang et al., 2018). This allows us to obtain a stronger form of the results in (Hazan
and Minasyan, 2020) without the smoothness assumption or even differentiability. On the
other hand, Theorem 5 shows that in many settings, there is no need for variance reduction
techniques such as momentum to move from deterministic to stochastic feedback, as long as
the algorithm is designed to handle fully adaptive adversaries.
4A Generalized Approach to Online Convex Optimization
2. Background and Notation
For a set D ⊆ Rd, we define its affine hull aff(D) to be the set of αx+(1−α)y for all x,y
in K and α ∈ R. The relative interior of D is defined as
relint(D) := {x ∈ D | ∃r > 0,B (x)∩aff(D) ⊆ aff(D)}.
r
A function class is simply a set of real-valued functions. Given a set D, a function class over
D is simply a subset of all real-valued functions on D. Given a real number µ ≥ 0 and a set
D, we use Q to denote the class of functions of the form q (y) = ⟨o,y−x⟩+ µ∥y−x∥2,
µ x,o 2
where o ∈ Rd and x ∈ D. A set K ⊆ Rd is called a convex set if for all x,y ∈ K and
α ∈ [0,1], we have αx+(1−α)y ∈ K. Given a convex set K, a function f : K → R is called
convex if, for all x,y ∈ K and α ∈ [0,1], we have f(αx+(1−α)y) ≤ αf(x)+(1−α)f(y).
All convex functions are continuous on any point in the relative interior of their domains. In
this work, we will only focus on continuous functions. If x ∈ relint(K) and f is convex and
is differentiable at x, then we have f(y)−f(x) ≥ ⟨∇f(x),y−x⟩, for all y ∈ K. A vector
o ∈ Rd is called a subgradient of f at x if f(y)−f(x) ≥ ⟨o,y−x⟩. for all y ∈ K. Given a
convex set K, a function f : K → R is convex if and only if it has a subgradient at all points
x ∈ K. More generally, given µ ≥ 0, we say a vector o ∈ Rd is a µ-subgradient of f at x
if f(y)−f(x) ≥ ⟨o,y−x⟩+ µ∥y−x∥2. for all y ∈ K. Given a convex set K, a function
2
f : K → R is µ-strongly convex if and only if it has a µ-subgradient at all points x ∈ K.
3. Problem Setup
Online optimization problems can be formalized as a repeated game between an agent and
an adversary. The game lasts for T rounds and T is known to both players. The game
consists of four components. The agent, the adversary, and the query oracle. We start with
a discussion of the agent and the adversary.
In t-th round, the agent chooses an action x from an action set K ⊆ Rd, then the
t
adversary chooses a loss function f ∈ F. Then, for 1 ≤ i ≤ k , the agent chooses a points
t t
y and receives the output of the query oracle for the tuple (t,f ,y ).
t,i t t,i
To be more precise, an agent consists of a tuple (Aaction,Aquery), where Aaction =
(Aaction,··· ,Aaction) is a sequence of functions such that A that maps the history Kt−1×
1 T t
(cid:81)t s− =1 1(K×O)ks to the random variable x
t
taking values in K where we use O to denote
range of the query oracle. The next element in the tuple, Aquery, is the query policy. For
each 1 ≤ t ≤ T and 1 ≤ i ≤ k t, Aq t,u iery : Kt ×(cid:81)t s− =1 1(K×O)ks ×(K×O)i−1 is a function
that, given previous actions and observations, either selects a random variable yi, i.e., query,
t
or signals that the query policy at this time-step is terminated.
We say the agent query function is trivial if k = 1 and y = x for all 1 ≤ t ≤ T. In this
t t,i t
case, we simplify the notation and use the notation A = Aaction = (A ,··· ,A ) to denote
1 T
the agent action functions and assume that the domain of A is (K×O)t−1.
t
The query oracle is a function that provides the feedback to the agent. Formally, a query
oracle is a function Q defined on [T]×F×K such that for each 1 ≤ t ≤ T, f ∈ F and x ∈ K,
the Q(t,f,x) is a random variable taking value in O. The query oracle is called a stochastic
value oracle or stochastic zeroth order oracle if O = R and f(x) = E[Q(t,f,x)]. Similarly,
it is called a stochastic (sub)gradient oracle or stochastic first order oracle if O = Rd and
5E[Q(t,f,x)] is a (sub)gradient of f at x. In all cases, if the random variable takes a single
value with probability one, we refer to it as a deterministic oracle. Note that, given the
domain, there is a unique deterministic gradient oracle, but there may be many deterministic
subgradient oracles. We will use ∇ to denote the deterministic gradient oracle. We say an
oracle is bounded by B if its output is always within the Euclidean ball of radius B centered
at the origin.
We say the agent takes semi-bandit feedback if the oracle is first-order and the agent
query function is trivial. Similarly, it takes bandit feedback if the oracle is zeroth-order and
the agent query function is trivial1. If the agent query function is non-trivial, then we say
the agent requires full-information feedback.
Let H
t
:= Kt−1×(cid:81)t s− =1 1(K×O)ks denote the space of all histories from the perspective of
the agent. In other words, each element of H corresponds to all choices and observations of
t
theagentfromtime-step1tot−1. AnadversaryAdvisasetsuchthateachelementB ∈ Adv,
referred to as a realized adversary, is a sequence (B ,··· ,B ) of functions B : H ×K → F.2
1 T t t
We say an adversary Adv is oblivious if for any realization B = (B ,··· ,B ), all functions
1 T
B are constant. In this case, a realized adversary may be simply represented by a sequence
t
(f ,··· ,f ) ∈ FT. We say an adversary is a weakly adaptive adversary if each function
1 T
B described above does not depend on x and therefore may be represented as a map
t t
H → F. In this work we focus on adversaries that are fully adaptive, i.e., adversaries with
t
no restriction. Clearly any oblivious adversary is an offline adaptive adversary and any
offline adaptive adversary is an online adaptive adversary. Given a function class F, we use
Adv(F) to denote the set of all possible values of B = (B ,··· ,B ). We also use Advo(F)
1 T
to denote the oblivious adversary FT.
In order to handle different notions of regret with the same approach, for an agent A,
query oracle Q, adversary Adv, compact set U ⊆ KT and 1 ≤ a ≤ b ≤ T, we define regret as
(cid:34) b b (cid:35)
(cid:88) (cid:88)
RA (U)[a,b] := sup E f (x )− min f (u ) ,
Adv,Q t t t t
B∈Adv
t=a
u=(u1,···,uT)∈U
t=a
where the expectation in the definition of the regret is over the randomness of the algorithm
andthequeryoracle. WeusethenotationRA (U)[a,b] := RA (U)[a,b]whenAdv = {B}
B,Q Adv,Q
is a singleton.
Static adversarial regret or simply adversarial regret corresponds to a = 1, b = T
and U = U := {(x,··· ,x) | x ∈ K}. When a = 1, b = T and U contains only a
0
single element then it is referred to as the dynamic regret.(Zinkevich, 2003; Zhang et al.,
2018) If instead U = KT, then it is referred to as restricted dynamic regret.(Besbes et al.,
2015) Weakly adaptive regret, sometimes simply referred to as adaptive regret, is defined
as max RA (U )[a,b].(Hazan and Seshadhri, 2009) Similarly, strongly adaptive
1≤a≤b≤T Adv,Q 0
regret, defined as max RA (U )[a,a+τ −1].(Daniely et al., 2015) Note that
1≤a≤a+τ−1≤T Adv,Q 0
weakly adaptive regret is a function of T while strongly adaptive regret is a function of τ.
1. This is a slight generalization of the common use of the term bandit feedback. Usually, bandit feedback
refers to the case where the oracle is a deterministic zeroth-order oracle and the agent query function is
trivial.
2. Note that we do not assign a probability to each realizated adversary since the notion of regret simply
computesthesupremumoverallrealizations. Also,notethatsinceeachrealizedadversaryisdeterministic,
the function B does not require the knowledge of the values of (f ,··· ,f )
t 1 t−1
6A Generalized Approach to Online Convex Optimization
We drop a, b and U when the statement is independent of their value. When discussing fully
adaptive adversaries, we use the notation RA := RA .
F,Q Adv(F),Q
Remark 1 As a special case of static adversarial regret, if Adv is oblivious and every
B ∈ Adv can be represented as (f,··· ,f) where f ∈ F and the oracle is stochastic3, then
the regret is referred to as the stochastic regret.
Another metric for comparing algorithms is high probability regret bounds which we
define for stochastic algorithms. Let ΩA capture the sources of randomness in an online
algorithm A. Therefore, given ω ∈ Ω, we may consider A as a deterministic algorithm. We
ω
say h : [0,1] → [0,∞) is a high probability regret bound for (A,Adv,Q) if for each B ∈ Adv
and δ ∈ [0,1], we have
(cid:16) (cid:17)
P {ω ∈ Ω | RAω ≤ h(δ)} ≥ 1−δ.
B,Q
Note that the infimum of any family of high probability regret bounds is a high probability
A
regret bound. Hence we use R : [0,1] → [0,∞) to denote the smallest high probability
Adv,Q
A
regret bound for (A,Adv,Q). In other words, R is the quantile function for the random
B,Q
variable ω (cid:55)→ RAω .
B,Q
4. Oblivious adversary to fully adaptive adversary
The following theorem states a result that is well-known in the literature. For example,
(Hazan et al., 2016) only defined the notion of adaptive adversary when discussing stochastic
algorithms and mentions that the results of all previous chapters, which were focused on
deterministic algorithms, apply to any type of adversary. Here we explicitly mention it as a
theorem for completion.
Theorem 1 Assume A and Q are deterministic and F is a function class. Then we have
RA = RA .
F,Q Advo(F),Q
See Appendix A for proof.
Corollary 1 The OGD, FTRL, and OMD algorithms are deterministic and therefore have
same regret bound in fully-adaptive setting as the oblivious setting.
5. Linear to convex with fully adaptive adversary
Here we show that any semi-bandit feedback online linear optimization algorithm for fully
adaptive adversary is also an online convex optimization algorithm. We start with a
definition.
3. Usually, the oracle is assumed to have extra restrictions to ensure that any sample of Q(t,f,·) belongs to
function class F.
7Definition 1 Let F be a function class over K and let µ ≥ 0. We define µ-quadratization
of F with respect to a first order oracle Q to be class of functions q such that there exists
t
f ∈ F, x ∈ K, and o ∈ Image(Q(·,f,x)) such that
µ
q := y (cid:55)→ ⟨o,y−x ⟩+ ∥y−x ∥2 ∈ Q .
t t t µ
2
When µ = 0, we may also refer to F [Q] as the linearization of F with respect to Q. We say
µ
F is closed under µ-quadratization if for any deterministic (sub)gradient oracle ∇∗, we have
F ⊇ F [∇∗].
µ
Theorem 2 Let K ⊆ Rd be a convex set, let µ ≥ 0 and let A be algorithm for online
optimization with semi-bandit feedback. If F be a µ-strongly convex function class over K
and ∇∗ is a deterministic subgradient oracle for F, then we have
RA ≤ RA , RA ≤ RA
F,∇∗ Fµ[∇∗],∇ F,∇∗ Fµ[∇∗],∇
Moreover, if F is closed under µ-quadratization, then we have equality.
See Appendix B for proof.
Corollary 2 (Theorem 14 in (Abernethy et al., 2008)) The min-max regret bound
of the problem of online linear optimization with deterministic feedback and online convex
optimization with deterministic feedback are equal.
To see why this is true, note that any algorithm has the same performance in both settings.
So it follows that an optimal algorithm for one setting is optimal for the other and therefore
the min-max regret bounds are equal.
Theorem 3 Given a convex M -Lipschitz function class F over K and a deterministic
1
subgradient oracle ∇∗ over F, we have
RFTPL ≤ 2DM √ dT, RFTPL (δ) ≤ 2DM √ T (cid:16)√ d+(cid:112) 2log(4/δ)(cid:17) ,
F,∇∗ 1 F,∇∗ 1
where FTPL denotes Follow The Perturbed Leader (FTPL) algorithm (Kalai and Vempala,
2005) and D = diam(K).
See Appendix C for proof.
6. Full information feedback to semi-bandit feedback feedback
Any algorithm designed for semi-bandit setting may be trivially applied in the first-order
full-information feedback setting. In the following theorem, we show that in online convex
optimization for fully adaptive adversary, semi-bandit feedback is generally enough. Specifi-
cally, we show that an algorithm that requires full information feedback could be converted
to an algorithm that only requires semi-bandit feedback with the same regret bounds. First
we describe the meta-algorithm that does this conversion.
8A Generalized Approach to Online Convex Optimization
Next we show that FTS(A) always
Algorithm 1: Full information to semi-
performs at least as good as A when
bandit - FTS(A)
the oracle is deterministic.
Input : horizon T, algorithm
Theorem 4 Let K ⊆ Rd be a convex A=(Aaction,Aquery), strong convexity
coefficient µ≥0
set, let µ ≥ 0 and let A be algorithm
for t=1,2,...,T do
for online optimization with full infor-
Play the action x chosen by Aaction
t
mation first order feedback. If F is a
Let f be the function chosen by the adversary
t
µ-strongly convex function class over K
and ∇∗ is a deterministic subgradient Query the oracle at the point x to get o
t t
oracle for F, then we have Let q t :=y(cid:55)→⟨o t,y−x t⟩+ µ 2∥y−x t∥2
for i starting from 1, while Aquery is not
RA′ ≤ RA terminated for this time-step do
F,∇∗ Fµ[∇∗],∇
Let y be the query chosen by Aquery
t,i
where A′ = FTS(A) is Algorithm 1. Return ∇q t(y t,i)=o t+µ⟨y t,i,y t,i−x t⟩
In particular, if F is closed under µ- as the output of the query oracle to A
end
quadratization, then
end
RA′ ≤ RA
F,∇∗ F,∇∗
See Appendix D for proof. Note that when the base algorithm A is semi-bandit, we have
FTS(A) = A and the above theorem becomes trivial.
Remark 2 In (Zhang et al., 2018), Algorithm 3, i.e., “Improved Ader”, is the result of
applying the FTS meta-algorithm to to Algorithm 2 in their paper, i.e. “Ader”.
7. Deterministic feedback to stochastic feedback
In this section, we show that algorithms that are designed for fully adaptive adversaries
using deterministic semi-bandit feedback can obtain similar bounds using only stochastic
semi-bandit feedback when facing oblivious adversaries. In particular, there is no need for
any variance reduction method, such as momentum (Mokhtari et al., 2020; Chen et al., 2018;
Xie et al., 2020), as long as we know that the algorithm is designed for a fully adaptive
adversary.
Theorem 5 Let K ⊆ Rd be a convex set, let µ ≥ 0 and let A be algorithm for online
optimization with semi-bandit feedback. If F be a µ-strongly convex function class over K
and Q is a stochastic subgradient oracle for F, then we have
RA ≤ RA , RA ≤ RA
Advo(F),Q Fµ[Q],∇ Advo(F),Q Fµ[Q],∇
See Appendix E for proof. Theorem 6.5 in (Hazan et al., 2016) may be seen as a special
case of the above theorem when U is a singleton and A is deterministic.
Corollary 3 Let A be an online algorithm with semi-bandit feedback. Assume that for any
linear function class L where each function has a gradient bounded by M , we have
1
RA ≤ g(M ,K,T),
L,∇ 1
9where g is a real valued function. Then, for any convex function class F and any stochastic
subgradient oracle Q for F that is bounded by B , we have
1
RA ≤ g(B ,K,T).
Advo(F),Q 1
Moreover, the same is true if we replace R with R.
To see why this is true, note that any the gradient of functions in F [Q] is bounded by B .
0 1
Using Theorem 3, we immediately see that.
Theorem 6 Given a convex function class F over K and a stochastic subgradient oracle
Q over F that is bounded by B , we have
1
RFTPL ≤ 2DB √ dT, RFTPL (δ) ≤ 2DB √ T (cid:16)√ d+(cid:112) 2log(4/δ)(cid:17) ,
Advo(F),Q 1 Advo(F),Q 1
where D = diam(K).
8. First order feedback to zeroth order feedback
When we do not have access to a gra-
Algorithm 2: First order to zeroth order -
dient oracle, we rely on samples from
FOTZO(A)
a value oracle to estimate the gradient.
Input : Linear space L , smoothing parameter
0
The “smoothing trick” is a classical re-
δ ≤α, horizon T, algorithm A
sult by (Flaxman et al., 2005) which in- k ←dim(L )
0
volves averaging through spherical sam- for t=1,2,...,T do
pling around a given point. Here we use x ← the action chosen by A
t
a variant that was introduced in (Pe- Play x t
Let f be the function chosen by the adversary
dramfar et al., 2023) which does not t
require extra assumptions on the con-
for i starting from 1, while Aquery is not
vex set K.
terminated for this time-step do
We choose a point c ∈ relint(K) Sample v ∈S1∩L uniformly
t,i 0
and a real number r > 0 such that Let y be the query chosen by Aquery
t,i
Baff(K)
(c) ⊆ K. Then, for any shrink- Query the oracle at the point y +δv
r t,i t,i
ing parameter α < r, we define Kˆ α := to get o t,i
(1− α r)K+ α rc. Pass k δo tv t as the oracle output to A
For a function f : K → R defined end
on a convex set K ⊆ Rd and a smooth- end
ing parameter δ ≤ α, its δ-smoothed
version fˆ : Kˆ → R is given as
δ α
fˆ(x) := E [f(z)] = E [f(x+δv)],
δ z∼Baff(K)(x) v∼BL0(0)
δ 1
where L = aff(K)−x, for any x ∈ K is the linear space that is a translation of the affine
0
hull of K and v is sampled uniformly at random from the k = dim(L )-dimensional ball
0
BL0(0). Thus, the function value fˆ(x) is obtained by “averaging” f over a sliced ball of
1 δ
radius δ around x.
10A Generalized Approach to Online Convex Optimization
For a function class F over K, we use Fˆ to denote {fˆ | f ∈ F}. Similarly, for a realized
δ δ
adversary B = (B ,··· ,B ), we define Bˆ as the element-wise smoothing of the output of
1 T δ
each B . Finally, we use Aˆdv to denote the element-wise smoothed version of the realized
t δ
adversaries. We will drop the subscripts α and δ when there is no ambiguity.
We want to show that, given an oblivious adversary, the behavior of A′ = FOTZO(A)
with a value oracle is comparable to the behavior of A with a corresponding gradient oracle.
We start by defining a class of gradient oracles.
Let G be the class of all maps g : Fˆ →
∗ ∗ Algorithm 3: Semi-bandit to bandit -
F such that the smoothed version of g (fˆ)
∗ STB(A)
is fˆfor all fˆ∈ Fˆ. In other words, for any
Input :Linear space L , smoothing
f ∈ Fˆ, we pick f ∈ F such that f = fˆ 0
1 2 1 2 parameter δ ≤α, horizon T,
and we set g ∗(f 1) = f 2. Since the smoothing algorithm A
operator is not one-to-one, there may be k ←dim(L )
0
infinitely many such maps in G . We define for t=1,2,...,T do
∗
G := GT ∗ = {g = (g 1,··· ,g T) | g t ∈ G ∗}. Sample v t ∈S1∩L 0 uniformly
For any g ∈ G, let Qˆ be the stochastic x t ← the action chosen by A
g
gradient oracle for Fˆ defined by Play x t+δv t
Let f be the function chosen by the
t
adversary
k
Qˆ (t,fˆ,x) := Q(t,g (fˆ),x+δv)v, Let o be the output of the value oracle
g t t
δ
Pass ko v as the oracle output to A
δ t t
end
wherev isarandomvector, takingitsvalues
uniformly from S1∩L = S1∩(aff(K)−z), for any z ∈ K and k = dim(L ).
0 0
Theorem 7 Let F be a function class over a convex set K and choose c and r as described
above and let δ ≤ α < r. Let U ⊆ KT be a compact set and let Uˆ = (1− α)U + αc. Assume
r r
A is an algorithm for online optimization with stochastic first order feedback and oblivious
adversary and Q is a stochastic value oracle. Also let Adv ⊆ FT be an oblivious adversary.
Then, if A′ = FOTZO(A) where FOTZO is described by Algorithm 2, we have:
• if F is convex and bounded by M , we have
0
(cid:18) (cid:19)
6δ α
RA′ (U) ≤ supRA (Uˆ)+ + M T.
Adv,Q g∈G Aˆdv,Qˆ g α r 0
• if F is M -Lipschitz (but not necessarily convex), then we have
1
(cid:18) (cid:19)
2Dα
RA′ ≤ supRA (Uˆ)+ 3+ δM T.
Adv,Q g∈G Aˆdv,Qˆ g rδ 1
Theorem 8 Under the assumptions of Theorem 7, if we further assume that A is semi-
bandit, then the same regret bounds hold with A′ = STB(A), where STB is described by
Algorithm 3.
See Appendix F for proof.
11Remark 3 While STB turns a semi-bandit algorithm into a bandit algorithm, FOTZO turns
a general first order algorithm into a zeroth order algorithm. However, we note that even
when A is semi-bandit, FOTZO(A) is not a bandit algorithm. Instead it is a full-information
zeroth order algorithm that uses a single query per iteration, but its query is not the same as
its action. This is because FOTZO(A) plays the same point x as the base algorithm A, but
t
then adds a noise before querying according to Aquery.
Remark 4 We have shown in the previous sections that for online convex optimization
with fully adaptive adversary, semi-bandit feedback is enough. However, we have included
the FOTZO meta-algorithm since the results in this section apply also to non-convex online
optimization problems, but they are limited to oblivious adversaries.
Corollary 4 Let A be an online algorithm with semi-bandit feedback and let F∗ be a
function class that is closed under the smoothing operator. Assume that there is a function
g : R → R such that for any convex set K, function class F ⊆ F∗ over K and stochastic
subgradient oracle Q for F that is bounded by B , we have
1 1
RA ≤ g(B ,K).
Advo(F),Q1 1
Let Q be a stochastic value oracle for F which is bounded by B and let A′ = STB(A).
0 0
Then, if F is convex and bounded by M ≤ B , then we have
0 0
(cid:18) (cid:19) (cid:18) (cid:19)
k 6δ α
RA′ ≤ g B ,Kˆ + + M T.
Advo(F),Q0
δ
0
α r
0
Similarly, if F is M -Lipschitz, then we have
1
(cid:18) (cid:19) (cid:18) (cid:19)
k 2Dα
RA′ ≤ g B ,Kˆ + 3+ δM T.
Advo(F),Q0
δ
0
rδ
1
Let K ⊆ Rd be a convex set such that diam(K) ≤ D. Let F be a class of M -Lipschitz
0 1
convex functions over K. The Online Gradient Descent (OGD) algorithm is a deterministic
algorithm, designed for online convex optimization with a deterministic gradient oracle and
an oblivious adversary with the following regret bound (See (Hazan et al., 2016) for a proof
with this exact bound):
3 √
ROGD ≤ DM T.
F0,∇ 2 1
It follows that a more general version of the results of (Flaxman et al., 2005), where we
allow the value oracle to be stochastic, could be obtained as an application of Corollary 4 to
the above regret bound of OGD.
Theorem 9 Let F be a class of convex functions over K and let Q be a stochastic value
oracle for F that is bounded by B . Then, by choosing α = T−1/6 and δ = T−1/3, we have
0
RSTB(FTPL) = O(T5/6).
Advo(F),Q
On the other hand, if all functions in F are M -Lipschitz, then by choosing α = δ = T−1/4,
1
we see that
RSTB(FTPL) = O(T3/4).
Advo(F),Q
See Appendix G for proof.
12A Generalized Approach to Online Convex Optimization
References
Jacob Abernethy, Peter L Bartlett, Alexander Rakhlin, and Ambuj Tewari. Optimal
strategies and minimax lower bounds for online convex games. Conference on Learning
Theory (COLT), 2008.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization.
Operations Research, 63(5):1227–1244, 2015.
Lin Chen, Christopher Harshaw, Hamed Hassani, and Amin Karbasi. Projection-free online
optimization with stochastic gradient: From convexity to submodularity. In Jennifer Dy
and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine
Learning, volume 80 of Proceedings of Machine Learning Research, pages 814–823. PMLR,
10–15 Jul 2018.
Lin Chen, Mingrui Zhang, and Amin Karbasi. Projection-free bandit convex optimization.
In Proceedings of the Twenty-Second International Conference on Artificial Intelligence
and Statistics, pages 2047–2056. PMLR, 2019.
Amit Daniely, Alon Gonen, and Shai Shalev-Shwartz. Strongly adaptive online learning. In
Proceedings of the 32nd International Conference on Machine Learning, pages 1405–1411.
PMLR, 2015.
Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex
optimization in the bandit setting: gradient descent without a gradient. In Proceedings of
the sixteenth annual ACM-SIAM symposium on Discrete algorithms, pages 385–394, 2005.
Dan Garber and Ben Kretzu. Improved regret bounds for projection-free bandit convex
optimization. In Proceedings of the Twenty Third International Conference on Artificial
Intelligence and Statistics, pages 2196–2206. PMLR, 2020.
Dan Garber and Ben Kretzu. New projection-free algorithms for online convex optimization
with adaptive regret guarantees. In Proceedings of Thirty Fifth Conference on Learning
Theory, pages 2326–2359. PMLR, 2022.
Khashayar Gatmiry and Zakaria Mhammedi. Projection-free online convex optimization via
efficientnewtoniterations. InThirty-seventh Conference on Neural Information Processing
Systems, 2023.
Geoffrey J. Gordon. Regret bounds for prediction problems. In Proceedings of the twelfth
annual conference on Computational learning theory, COLT ’99, pages 29–40. Association
for Computing Machinery, 1999.
James Hannan. Approximation to bayes risk in repeated play. In M. Dresher, A. W. Tucker,
and P. Wolfe, editors, Contributions to the Theory of Games, pages 97–140. Princeton
University Press, 1957.
Elad Hazan and Satyen Kale. Extracting certainty from uncertainty: regret bounded by
variation in costs. Machine Learning, 80(2):165–188, 2010.
13Elad Hazan and Satyen Kale. Projection-free online learning. In Proceedings of the 29th
International Coference on International Conference on Machine Learning, ICML’12,
pages 1843–1850. Omnipress, 2012.
Elad Hazan and Edgar Minasyan. Faster projection-free online learning. In Proceedings of
Thirty Third Conference on Learning Theory, pages 1877–1893. PMLR, 2020.
Elad Hazan and C. Seshadhri. Efficient learning algorithms for changing environments. In
Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09,
pages 393–400. Association for Computing Machinery, 2009.
Elad Hazan et al. Introduction to online convex optimization. Foundations and Trends® in
Optimization, 2(3-4):157–325, 2016.
Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal
of Computer and System Sciences, 71(3):291–307, 2005.
Zakaria Mhammedi. Efficient projection-free online convex optimization with membership
oracle. In Proceedings of Thirty Fifth Conference on Learning Theory, pages 5314–5390.
PMLR, 2022.
Aryan Mokhtari, Hamed Hassani, and Amin Karbasi. Stochastic conditional gradient
methods: From convex minimization to submodular maximization. The Journal of
Machine Learning Research, 21(1):4232–4280, 2020.
Mohammad Pedramfar, Christopher John Quinn, and Vaneet Aggarwal. A unified approach
for maximizing continuous DR-submodular functions. In Thirty-seventh Conference on
Neural Information Processing Systems, 2023.
Shai Shalev-Shwartz. Online learning: theory, algorithms and applications. PhD thesis,
Hebrew University, 2007.
Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and
Trends® in Machine Learning, 4(2):107–194, 2012.
Shai Shalev-Shwartz and Yoram Singer. A primal-dual perspective of online learning
algorithms. Machine Learning, 69(2):115–142, 2007.
Jiahao Xie, Zebang Shen, Chao Zhang, Boyu Wang, and Hui Qian. Efficient projection-free
online methods with stochastic recursive gradient. Proceedings of the AAAI Conference
on Artificial Intelligence, 34(4):6446–6453, 2020.
Lijun Zhang, Shiyin Lu, and Zhi-Hua Zhou. Adaptive online learning in dynamic envi-
ronments. In Advances in Neural Information Processing Systems, volume 31. Curran
Associates, Inc., 2018.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent.
In Proceedings of the 20th international conference on machine learning (icml-03), pages
928–936, 2003.
14A Generalized Approach to Online Convex Optimization
Appendix A. Proof of Theorem 1
Proof Note that Advo(F) ⊆ Adv(F) which directly implies that
RA ≥ RA .
F,Q Advo(F),Q
Toprovetheinequalityintheotherdirection,letB ∈ Adv(F). Thegamebetween(A,B,Q)is
a deterministic game and only a single sequence of functions (f ,··· ,f ) could be generated
1 T
by B, i.e. there is a unique B′ := (f ,··· ,f ) ∈ Advo(F) such that the game played by
1 T
(A,B,Q) is identical to the game played by (A,B′,Q). Therefore, we have
RA = sup RA = sup RA ≤ sup RA = RA .
F,Q B,Q B′,Q B′,Q Advo(F),Q
B∈Adv(F) B∈Adv(F) B′∈Advo(F)
Appendix B. Proof of Theorem 2
Proof
Regret:
Let o := ∇∗(t,f ,x ) denote the output of the subgradient query oracle at time-step t.
t t t
For any realization B = (B ,··· ,B ) ∈ Adv(F), we define
1 T
µ
B′(x ,··· ,x ) := q := y (cid:55)→ ⟨o ,y−x ⟩+ ∥y−x ∥2 ∈ Q ,
t 1 t t t t 2 t µ
and B′ = (B′,··· ,B′ ). Note that each B′ is a deterministic function of x ,··· ,x and
1 T t 1 t
therefore it belongs to Adv(F [∇∗]). Since the algorithm uses semi-bandit feedback, the
µ
sequence of random vectors (x ,··· ,x ) chosen by A is identical between the game with
1 T
(B,∇∗) and (B′,∇). Therefore, for any y ∈ K, we have
µ
q (x )−q (y) = ⟨o ,x −y⟩− ∥x −y∥2 ≥ f (x )−f (y),
t t t t t t t t t
2
where the last inequality follows from fact that o is a subgradient of f at x . Therefore, we
t t t
have
(cid:32) b b (cid:33) (cid:32) b b (cid:33)
(cid:88) (cid:88) (cid:88) (cid:88)
max f (x )− f (u ) ≤ max q (x )− q (u ) , (1)
t t t t t t t t
u∈U u∈U
t=a t=a t=a t=a
which implies that
(cid:34) (cid:32) b b (cid:33)(cid:35)
(cid:88) (cid:88)
RA = sup RA = sup E max f (x )− f (u )
F,∇∗ B,∇∗ t t t t
B∈Adv(F) B∈Adv(F) u∈U t=a t=a
(cid:34) (cid:32) b b (cid:33)(cid:35)
(cid:88) (cid:88)
≤ sup E max q (x )− q (u ) ≤ sup RA = RA
B∈Adv(F) u∈U t=a
t t
t=a
t t
B′∈Adv(Fµ[∇∗])
B′,∇ Fµ[∇∗],∇
15Toprovethesecondclaim,wenotethatF ⊇ F [∇∗]impliesthatAdv(F) ⊇ Adv(F [∇∗]).
µ µ
Therefore
RA = sup RA ≥ sup RA = RA
F,∇∗ B,∇∗ B,∇∗ Fµ[∇∗],∇
B∈Adv(F) B∈Adv(Fµ[∇∗])
High probability regret bound:
Assume that h : [0,1] → R is a high probability regret bound for (A,Adv(F),∇∗). Using
Inequality 1, we see that
(cid:32) b b (cid:33) (cid:32) b b (cid:33)
(cid:88) (cid:88) (cid:88) (cid:88)
RAω = max f (x )− f (u ) ≤ max q (x )− q (u ) = RAω ,
B,∇∗ t t t t t t t t B′,∇
u∈U u∈U
t=a t=a t=a t=a
which immediately implies that
(cid:16) (cid:17) (cid:16) (cid:17)
P {ω ∈ ΩA | RAω ≤ h(δ)} ≥ P {ω ∈ ΩA | RAω ≤ h(δ)} ≥ 1−δ.
B,∇∗ B′,∇
Hence g is a high probability regret bound for (A,Adv(F [∇∗]),∇) as well.
µ
Moreover, if F ⊇ F [∇∗], then Adv(F) ⊇ Adv(F [∇∗]) and therefore, for any function
µ µ
h : [0,1] → [0,∞), we have
(cid:16) (cid:17) (cid:16) (cid:17)
inf P {ω ∈ ΩA | RAω ≤ h(δ)} ≤ inf P {ω ∈ ΩA | RAω ≤ h(δ)} .
B,∇∗ B,∇∗
B∈Adv(F) B∈Adv(Fµ[∇∗])
Hence any high probability regret bound for (A,Adv(F [∇∗]),∇) is one for (A,Adv(F),∇∗)
µ
as well.
Appendix C. Proof of Theorem 3
Proof Let K ⊆ Rd be a convex set such that diam(K) ≤ D. Let L be a class of linear
functions over K such that ∥l∥ ≤ M for all l ∈ L. Follow The Perturbed Leader (FTPL)
1
algorithm (Kalai and Vempala, 2005) is a stochastic algorithm, designed for online linear
optimization with a deterministic gradient oracle and a fully adaptive adversary with the
following regret bound (See Remark 3.2 and Theorem 5.4 in (Hazan and Minasyan, 2020)
for a proof of these exact bounds):
RFTPL ≤ 2DM √ dT, RFTPL (δ) ≤ 2DM √ T (cid:16)√ d+(cid:112) 2log(4/δ)(cid:17) .
L,∇ 1 L,∇ 1
The result now follows from Theorem 2.
Appendix D. Proof of Theorem 4
Proof First we note that for any adversary Adv over Q , under the algorithms A′, we
µ
have q = f . Hence A′ and A return the same sequence of points and we have
t t
RA′ = RA and RA′ = RA
Adv,∇ Adv,∇ Adv,∇ Adv,∇
16A Generalized Approach to Online Convex Optimization
Therefore, using Theorem 2 and the fact that F [∇∗] ⊆ Q , we see that
µ µ
RA′ ≤ RA′ = RA ,
F,∇∗ Fµ[∇∗],∇ Fµ[∇∗],∇
and similarly
A′ A
R ≤ R .
F,∇∗ Fµ[∇∗],∇
As a special case, if F is closed under µ-quadratization, then we may use the same
argument in the proof of Theorem 2 to see that
RA ≤ RA
Fµ[∇∗],∇ F,∇∗
and
A A
R ≤ R
Fµ[∇∗],∇ F,∇∗
which completes the proof.
Appendix E. Proof of Theorem 5
Proof Let ΩQ = ΩQ ×···×ΩQ capture all sources of randomness in the query oracle,
1 T
i.e., for any choice of θ ∈ ΩQ, the function Q(·,·,·) is deterministic. We will use E to
ΩQ
t
denote the expectation with respect to the randomness of query oracle at time-step t and
E to denote the expectation with respect to the randomness of query oracle at all times.
ΩQ
Similarly, let E denote the expectation with respect to the randomness of the agent. Let
ΩA
o := Q(t,f ,x ) be the random variable denoting the output of the (sub)gradient query
t t t
oracle at time-step t and let
o¯ := E[o | f ,x ] = E [o ]
t t t t ΩQ t
t
be a subgradient of f at x .
t t
For any realization B = (f ,··· ,f ) ∈ Advo(F) and any θ ∈ ΩQ, we define
1 T
µ
B′ (x ,··· ,x ) := q := y (cid:55)→ ⟨o ,y−x ⟩+ ∥y−x ∥2 ∈ Q ,
θ,t 1 t t t t 2 t µ
and B′ = (B′ ,··· ,B′ ). Note that a specific choice of θ is necessary to make sure that
θ θ,1 θ,T
B′ is a deterministic function of x ,··· ,x and not a random variable and therefore it
θ,t 1 t
belongs to Adv(F [Q]).
µ
Since the algorithm uses (semi-)bandit feedback, given a specific value of θ, the sequence
of random vectors (x ,··· ,x ) chosen by A is identical between the game with (B,Q ) and
1 T θ
(B′,∇). Therefore, for any u ∈ K, we have
θ t
µ
f (x )−f (u ) ≤ ⟨o¯ ,x −u ⟩− ∥x −u ∥2
t t t t t t t t t
2
µ
= ⟨E[o | f ,x ],x −u ⟩− ∥x −u ∥2
t t t t t t t
2
(cid:104) µ (cid:105)
= E ⟨o ,x −u ⟩− ∥x −u ∥2 | f ,x
t t t t t t t
2
= E[q (x )−q (u ) | f ,x ]
t t t t t t
= E [q (x )−q (u )]
ΩQ t t t t
t
17where the first inequality follows from the fact that f is µ-strongly convex and o¯ is a
t t
subgradient of f at x . Therefore we have
t t
(cid:34) b b (cid:35) (cid:34) b (cid:35) (cid:34) b (cid:35)
(cid:88) (cid:88) (cid:88) (cid:88)
E f (x )− f (u ) ≤ E E [q (x )−q (u )] = E q (x )−q (u ) .
ΩQ t t t t ΩQ ΩQ t t t t ΩQ t t t t
t
t=a t=a t=a t=a
Since B is oblivious, the sequence (f ,··· ,f ) is not affected by the randomness of Q.
1 T
Therefore we have
(cid:34) b b (cid:35)
(cid:88) (cid:88)
RAω = E f (x )−min f (u )
B,Q ΩQ t t t t
u∈U
t=a t=a
(cid:34) b b (cid:35)
(cid:88) (cid:88)
= maxE f (x )− f (u )
ΩQ t t t t
u∈U
t=a t=a
(cid:34) b b (cid:35)
(cid:88) (cid:88)
≤ maxE q (x )− q (u )
ΩQ t t t t
u∈U
t=a t=a
(cid:34) (cid:32) b b (cid:33)(cid:35)
(cid:88) (cid:88) (cid:104) (cid:105)
≤ E max q (x )− q (u ) = E RAω ,
ΩQ t t t t ΩQ B′,∇
u∈U θ
t=a t=a
where the second inequality follows from Jensen’s inequality.
Regret:
By taking the expectation with respect to ω ∈ ΩA, we see that
(cid:104) (cid:105) (cid:104) (cid:104) (cid:105)(cid:105) (cid:104) (cid:105)
RA = E RAω ≤ E E RAω = E RA ≤ sup RA
B,Q ΩA B,Q ΩA ΩQ B′,∇ ΩQ B′,∇ B′,∇
θ θ θ∈ΩQ θ
Hence we have
RA = sup RA ≤ sup RA ≤ sup RA = RA
Advo(F),Q
B∈Advo(F)
B,Q
B∈Advo(F),θ∈ΩQ
B θ′,∇
B′∈Adv(Fµ[Q])
B′,∇ Fµ[Q],∇
High probability regret bound:
By taking the supremum over B ∈ Advo(F), we see that
(cid:104) (cid:105)
RAω = sup RAω ≤ sup E RAω
Advo(F),Q B,Q ΩQ B′,∇
B∈Advo(F) B∈Advo(F) θ
≤ sup RAω ≤ sup RAω = RAω
B∈Advo(F),θ∈ΩQ
B θ′,∇
B′∈Adv(Fµ[Q])
B′,∇ Fµ[Q],∇
Therefore we may use the same argument in the proof of Theorem 2 to see that
A A
R ≤ R
Advo(F),Q Fµ[Q],∇
18A Generalized Approach to Online Convex Optimization
Appendix F. Proof of Theorems 7 and 8
First we prove the following lemma which is a simple generalization of Observation 3
in (Flaxman et al., 2005) to the construction of Kˆ that we use here and is proved similarly.
Lemma 1 For any x ∈ Kˆ and y ∈ K and any convex function f : K → [−M ,M ], we
α 0 0
have
2M
0
|f(x)−f(y)| ≤ ∥x−y∥
α
Proof Let y = x+∆. If ∥∆∥ ≥ α, then we have
2M
0
|f(x)−f(y)| ≤ 2M ≤ ∥x−y∥.
0
α
Otherwise, then let z = x+α ∆ . According to Lemma 7 in (Pedramfar et al., 2023), we
∥∆∥
(cid:16) (cid:17)
∥∆∥ ∥∆∥
have z ∈ K. We have y = z+ 1− x. Therefore, since f is convex and bounded
α α
by M , we have
0
(cid:18) (cid:19)
∥∆∥ ∥∆∥ ∥∆∥ 2M
0
f(y) ≤ f(z)+ 1− f(x) = f(x)+ (f(z)−f(x)) ≤ f(x)+ ∥x−y∥.
α α α α
We start with a proof of Theorem 8 and then adapt the proof to work for Theorem 7.
Proof [Proof of Theorem 8]
Let B = (f ,··· ,f ) ∈ Adv be a realized adversary and consider the game (A′,B| ,Q).
1 T Kˆ
According to the definition of G, there is g ∈ G such that
g(Bˆ) = (g (fˆ),··· ,g (fˆ )) = (f ,··· ,f ) = B.
1 1 T T 1 T
Hence we have
k k
Qˆ (t,fˆ,x) = Q(t,g (fˆ),x+δv)v = Q(t,f,x+δv)v. (2)
g t
δ δ
Therefore, according to Remark 4 in (Pedramfar et al., 2023), Qˆ (t,fˆ,x) is an unbiased
g
estimatorof∇fˆ(x). 4 InotherwordsQˆ isagradientoracleforFˆ. UsingEquation2andthe
g
definition of the Algorithm 3, we see that the responses of the queries are the same between
the game (A,Bˆ,Qˆ ) and (A′,B,Q). It follows that the sequence of actions (x ,··· ,x ) in
g 1 T
(A,Bˆ,Qˆ ) corresponds to the sequence of actions (x +δv ,··· ,x +δv ) in (A′,B,Q).
g 1 1 T T
Let u ∈ argmin (cid:80)b f (u ) and uˆ ∈ argmin (cid:80)b fˆ(u ). We have
u∈U t=a t t u∈Uˆ t=a t t
(cid:34) b b (cid:35) (cid:34) b b (cid:35)
RA′ −RA = E (cid:88) f (x +δv )−(cid:88) f (u ) −E (cid:88) fˆ(x )−(cid:88) fˆ(uˆ )
B,Q Bˆ,Qg t t t t t t t t t
t=a t=a t=a t=a
(cid:34)(cid:32) b b (cid:33) (cid:32) b b (cid:33)(cid:35)
(cid:88) (cid:88) (cid:88) (cid:88)
= E f (x +δv )− fˆ(x ) + fˆ(uˆ )− f (u ) . (3)
t t t t t t t t t
t=a t=a t=a t=a
4. When using a spherical estimator, it was shown in (Flaxman et al., 2005) that fˆis differentiable even
when f is not. When using a sliced spherical estimator as we do here, differentiability of fˆis not proved
in(Pedramfaretal.,2023). However,theirproofisbasedontheproofforthesphericalcaseandtherefore
the results carry forward to show that fˆis differentiable.
19First we consider the convex case. By definition, fˆ(x ) is an average of f at points that
t t t
are within δ distance of x. Therefore, according to Lemma 1 we have |fˆ(x )−f (x )| ≤ 2M0δ.
t t t t α
By using Lemma 1 again for the pair (x ,x +δv ), we see that
t t t
4M δ
|f (x +δv )−fˆ(x )| ≤ |f (x +δv )−f (x )|+|f (x )−fˆ(x )| ≤ 0 . (4)
t t t t t t t t t t t t t t
α
On the other hand, we have
b b
(cid:88) (cid:88)
fˆ(uˆ ) = min fˆ(uˆ )
t t t t
uˆ∈Uˆ
t=a t=a
b
2M 0δ (cid:88)
≤ T +min f (uˆ ) (Lemma 1)
t t
α uˆ∈Uˆ
t=a
b
=
2M 0δ
T
+min(cid:88)
f
(cid:16)(cid:16)
1−
α(cid:17)
u +
α c(cid:17)
(Definition of Uˆ)
t t
α u∈U r r
t=a
b
2M 0δ (cid:88)(cid:16)(cid:16) α(cid:17) α (cid:17)
≤ T +min 1− f (u )+ f (c) (Convexity)
t t t
α u∈U r r
t=a
b
2M 0δ (cid:88)(cid:16)(cid:16) α(cid:17) α (cid:17)
≤ T +min 1− f (u )+ M
t t 0
α u∈U r r
t=a
(cid:18) (cid:19) b
2δ α (cid:16) α(cid:17) (cid:88)
= + M T + 1− min f (u )
0 t t
α r r u∈U
t=a
(cid:18) (cid:19) b
2δ α (cid:88)
≤ + M T + f (u ).
0 t t
α r
t=a
Putting these together, we see that
(cid:18) (cid:19) (cid:18) (cid:19)
4M δ 2δ α 6δ α
RA′ −RA ≤ 0 T + + M T = + M T.
B,Q Bˆ,Qg α α r 0 α r 0
Therefore, we have
RA′
= sup
RA′
Adv,Q B,Q
B∈Adv
(cid:18) (cid:19)
6δ α
≤ sup RA + + M T (Here g depends on B as above)
B∈Adv
Bˆ,Qg α r 0
(cid:18) (cid:19)
6δ α
≤ sup sup RA + + M T
g∈GB∈Adv
Bˆ,Qg α r 0
(cid:18) (cid:19)
6δ α
= supRA + + M T.
g∈G
Aˆdv,Qg α r 0
To prove the result in the Lipschitz case, we note that, according to Lemma 3 in (Pe-
dramfar et al., 2023), we have |fˆ(x )−f (x )| ≤ δM . By using Lipschitz property for the
t t t t 1
20A Generalized Approach to Online Convex Optimization
pair (x ,x +δv ), we see that
t t t
|f (x +δv )−fˆ(x )| ≤ |f (x +δv )−f (x )|+|f (x )−fˆ(x )| ≤ 2δM . (5)
t t t t t t t t t t t t t t 1
On the other hand, we have
b b
(cid:88) (cid:88)
fˆ(uˆ ) = min fˆ(uˆ )
t t t t
uˆ∈Uˆ
t=a t=a
b
(cid:88)
≤ δM T +min f (uˆ ) (Lemma 3 in (Pedramfar et al., 2023))
1 t t
uˆ∈Uˆ
t=a
b
(cid:88) (cid:16)(cid:16) α(cid:17) α (cid:17)
= δM T +min f 1− u + c (Definition of Uˆ)
1 t t
u∈U r r
t=a
b
(cid:88) (cid:16) α (cid:17)
= δM T +min f u + (c−x)
1 t t
u∈K r
t=a
b (cid:18) (cid:19)
(cid:88) 2αM 1D
≤ δM T +min f (u )+ (Lipschitz)
1 t t
u∈U r
t=a
(cid:18) (cid:19) b
2Dα (cid:88)
= 1+ δM T + f (u )
1 t t
rδ
t=a
Therefore, using Equation 3, we see that
(cid:18) (cid:19) (cid:18) (cid:19)
2Dα 2Dα
RA′ −RA ≤ 2δM T + 1+ δM T = 3+ δM T.
B,Q Bˆ,Qg 1 rδ 1 rδ 1
Therefore, we have
RA′
= sup
RA′
Adv,Q B,Q
B∈Adv
(cid:18) (cid:19)
2Dα
≤ sup RA + 3+ δM T
B∈Adv
Bˆ,Qg rδ 1
(cid:18) (cid:19)
2Dα
≤ sup sup RA + 3+ δM T
g∈GB∈Adv
Bˆ,Qg rδ 1
(cid:18) (cid:19)
2Dα
= supRA + 3+ δM T.
g∈G
Aˆdv,Qg rδ 1
Proof [Proof of Theorem 7]
The proof of the bounds for this case is similar to the previous case. As before, we see
that the responses of the queries are the same between the game (A,Bˆ,Qˆ ) and (A′,B,Q).
g
It follows from the description of Algorithm 2 that the sequence of actions (x ,··· ,x ) in
1 T
(A,Bˆ,Qˆ ) corresponds to the same sequence of actions in (A′,B,Q).
g
21Let u ∈ argmin (cid:80)b f (u ) and uˆ ∈ argmin (cid:80)b fˆ(u ). We have
u∈U t=a t t u∈Uˆ t=a t t
(cid:34) b b (cid:35) (cid:34) b b (cid:35)
RA′ −RA = E (cid:88) f (x )−(cid:88) f (u ) −E (cid:88) fˆ(x )−(cid:88) fˆ(uˆ )
B,Q Bˆ,Qg t t t t t t t t
t=a t=a t=a t=a
(cid:34)(cid:32) b b (cid:33) (cid:32) b b (cid:33)(cid:35)
(cid:88) (cid:88) (cid:88) (cid:88)
= E f (x )− fˆ(x ) + fˆ(uˆ )− f (u ) . (6)
t t t t t t t t
t=a t=a t=a t=a
To obtain the same bound as before, we note that in the convex case, instead of Inequality 4,
we have
2M δ 4M δ
|f (x )−fˆ(x )| ≤ 0 < 0 , (7)
t t t t
α α
and instead of Inequality 5, we have
|f (x )−fˆ(x )| ≤ δM < 2δM .
t t t t 1 1
The rest of the proof follows verbatim.
Appendix G. Proof of Theorem 9
Proof According to Theorem 6, given a stochastic gradient oracle Q bounded by B , we
1 1
have
√
RFTPL ≤ 2DB dT.
Advo(F),Q1 1
Therefore we may apply Corollary 4 to see that, by choosing α = T−1/6 and δ = T−1/3, we
have
2kB √ (cid:18) 6δ α(cid:19)
RSTB(FTPL) ≤ 0 D dT + + B T = O(T5/6).
Advo(F),Q δ α r 0
Moreover, if F is M -Lipschitz, then by choosing α = δ = T−1/4, we see that
1
2kB √ (cid:18) 2Dα(cid:19)
RSTB(FTPL) ≤ 0 D dT + 3+ δM T = O(T3/4).
Advo(F),Q δ rδ 1
22