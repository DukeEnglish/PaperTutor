Model Assessment and Selection under Temporal Distribution Shift
Elise Han∗ Chengpiao Huang† Kaizheng Wang‡
This version: February 2024
Abstract
We investigate model assessment and selection in a changing environment, by synthesizing
datasets from both the current time period and historical epochs. To tackle unknown and
potentiallyarbitrarytemporaldistributionshift,wedevelopanadaptiverollingwindowapproach
toestimatethegeneralizationerrorofagivenmodel. Thisstrategyalsofacilitatesthecomparison
between any two candidate models by estimating the difference of their generalization errors.
We further integrate pairwise comparisons into a single-elimination tournament, achieving near-
optimal model selection from a collection of candidates. Theoretical analyses and numerical
experiments demonstrate the adaptivity of our proposed methods to the non-stationarity in data.
Keywords: Model assessment, model selection, temporal distribution shift, adaptivity.
1 Introduction
Statistical learning theory is traditionally founded on the assumption of a static data distribution,
wherestatisticalmodelsaretrainedanddeployedinthesameenvironment. However, thisassumption
is often violated in practice, where the data distribution keeps changing over time. The temporal
distribution shift can lead to serious decline in model performance post-deployment, which underlines
the critical need to monitor models and detect potential degradation.
Moreover, one often needs to choose among multiple candidate models originating from different
learning algorithms (e.g., linear regression, random forests, neural networks) and hyperparameters
(e.g., penalty parameter, step size, time window for training). Temporal distribution shift poses a
major challenge to model selection, as past performance may not reliably predict future outcomes.
Learners usually have to work with limited data from the current time period and abundant historical
data, whose distributions may vary significantly.
Main contributions. In this paper, we develop principled approaches to model assessment and
selection under unknown temporal distribution shift.
• (Modelassessment)Weproposeanadaptivewindowmethodforestimatingamodel’sgeneralization
error by selectively utilizing historical data.
• (Model selection) The above method can be used to compare any pair of models when applied to
the difference between their generalization errors. Based on this, we develop a single-elimination
tournament scheme for selecting the best model from a pool of candidates.
Author names are sorted alphabetically.
∗Department of Computer Science, Columbia University. Email: lh3117@columbia.edu.
†Department of IEOR, Columbia University. Email: chengpiao.huang@columbia.edu.
‡Department of IEOR and Data Science Institute, Columbia University. Email: kaizheng.wang@columbia.edu.
1
4202
beF
31
]GL.sc[
1v27680.2042:viXraFurthermore, we provide theoretical guarantees and numerical results to demonstrate the adaptivity
of our methods to non-stationarity.
Related works. Model assessment and selection are fundamental pillars in statistical learning
(Hastie et al., 2009). Hold-out and cross-validation are arguably the most popular methods in
practice. However, in the presence of distribution shift, the validation data may no longer accurately
represent the test cases. This challenge has attracted considerable attention over the past two
decades (Quinonero-Candela et al., 2022). Existing works mostly focused on the static scenario
where the validation data comprises independent samples from a fixed distribution. These methods
do not apply when the environment is continuously changing over time. Rolling windows offer a
practical solution to this issue and have been widely adopted for learning under temporal changes
(Bifet and Gavalda, 2007; Hanneke et al., 2015; Mohri and Muñoz Medina, 2012; Mazzetto and
Upfal, 2023; Mazzetto et al., 2023; Huang and Wang, 2023). Our method automatically selects a
window tailored to the underlying non-stationarity near a given time. The strategy is inspired by the
Goldenshluger-Lepski method for bandwidth selection in non-parametric estimation (Goldenshluger
and Lepski, 2008).
Adaptation to a non-stationary environment has emerged as a prominent area of study in online
learning, where the goal is to attain a small cumulative error over an extended time horizon (Hazan
and Seshadhri, 2009; Besbes et al., 2015; Daniely et al., 2015; Jadbabaie et al., 2015; Wei and Luo,
2021; Gibbs and Candes, 2021; Bai et al., 2022). A classical problem in this area is prediction from
expert advice (Littlestone and Warmuth, 1994), which aims to track the best expert in the long run.
In contrast, our approach focuses on evaluating a model or selecting from multiple candidates at a
specific point in time, leveraging offline data collected from the past. Its performance is measured
over the current data distribution. Consequently, our problem can be viewed as one of transfer
learning, with source data from historical epochs and target data from the current distribution. We
point out that our algorithms for the offline setting can be used as a sub-routine for online model
selection, where the candidate models may vary across different time points.
Our problem also is different from change-point detection (Niu et al., 2016; Truong et al., 2020).
The latter typically assumes that distribution shifts occur only at a small number of times called
change points. Our setting allows changes to happen at every time period with arbitrary magnitudes,
encompassing a much broader spectrum of shift patterns.
Notation. Let Z = {1,2,...} be the set of positive integers. For n ∈ Z , let [n] = {1,2,...,n}.
+ +
Forx ∈ R, define x = max{x,0}. For a,b ∈ R, define a∧b = min{a,b}. For non-negative sequences
+
{a }∞ and {b }∞ , we write a = O(b ) if there exists C > 0 such that for all n ∈ Z , a ≤ Cb .
n n=1 n n=1 n n + n n
Unless otherwise stated, a ≲ b also represents a = O(b ). We write a = o(b ) if a /b → 0 as
n n n n n n n n
n → ∞.
2 Problem setup
Let Z and F be spaces of samples and models, respectively. Denote by ℓ : F ×Z a loss function.
The quantity ℓ(f,z) measures the loss incurred by a model f ∈ F on a sample z ∈ Z. At time
t ∈ Z , the quality of a model f is measured by its generalization error (also called risk or population
+
loss) L (f) = E ℓ(f,z) over the current data distribution P . When the environment changes
t z∼Pt t
over time, the distributions {P }∞ can be different.
t t=1
Suppose that at each time t, we receive a batch of B ∈ Z i.i.d. samples B = {z }Bt from P ,
t + t t,i i=1 t
independent of the history. We seek to solve the following two questions based on the data {B }t :
j j=1
2Problem 2.1 (Model assessment). Given a model f ∈ F, how to estimate its population loss L (f)?
t
Problem 2.2 (Model selection). Given a collection of candidate models {f }m ⊆ F, how to
r r=1
select one with a small population loss? In other words, we want to choose r ∈ [m] so that
(cid:98)
L (f ) ≈ min L (f ).
t r r∈[m] t r
(cid:98)
3 Model Assessment
In this section we study Problem 2.1, under the assumption that the loss ℓ is bounded.
Assumption 3.1 (Bounded loss). The loss function ℓ takes values in a given interval [a,b].
In fact, we will consider a more general problem.
Problem 3.1 (Mean estimation). Let {Q }t be probability distributions over [a,b] and {D }t
j j=1 j j=1
be independent datasets, where D = {u }Bj consists of B i.i.d. samples from Q . Given {D }t ,
j j,i i=1 j j j j=1
how to estimate the mean µ of Q ?
t t
Problem 2.1 is a special case of Problem 3.1 with u = ℓ(f,z ) and µ = L (f). To tackle
j,i j,i t t
Problem 3.1, a natural idea is to take a look-back window k ∈ [t], and approximate µ by the sample
t
average over the k most recent periods:
(cid:18) t (cid:19)−1 t Bj
(cid:88) (cid:88) (cid:88)
µ = B u . (3.1)
(cid:98)t,k j j,i
j=t−k+1 j=t−k+1i=1
To measure the quality of µ , we invoke the Bernstein inequality (Boucheron et al., 2013). See
(cid:98)t,k
Appendix A.1 for the proof of the version we use.
Lemma 3.1 (Bernstein bound). Let {x }n be independent random variables taking values in [a,b]
i i=1
almost surely. Define the average variance σ2 = 1 (cid:80)n var(x ). For any δ ∈ (0,1), with probability
n i=1 i
at least 1−δ,
(cid:12) n (cid:12) (cid:114)
(cid:12) (cid:12)1 (cid:88) (x i−Ex i)(cid:12) (cid:12) ≤ σ 2log(2/δ) + 2(b−a)log(2/δ) .
(cid:12)n (cid:12) n 3n
i=1
Denote by µ and σ2 the mean and the variance of Q , respectively. Let B = (cid:80)t B ,
j j j t,k j=t−k+1 j
t t
1 (cid:88) 1 (cid:88)
µ = B µ , σ2 = B σ2.
t,k B j j t,k B j j
t,k t,k
j=t−k+1 j=t−k+1
From Lemma 3.1 and the triangle inequality
|µ −µ | ≤ |µ −µ |+|µ −µ |,
(cid:98)t,k t (cid:98)t,k t,k t,k t
we obtain the following bias-variance decomposition of the approximation error of µ .
(cid:98)t,k
Corollary 3.1. For t ∈ Z , k ∈ [t] and δ ∈ (0,1), define M = b−a,
+
ϕ(t,k) = max |µ −µ |,
j t
t−k+1≤j≤t
3
M, if B = 1
 t,k
ψ(t,k,δ) = (cid:113) .
2log(2/δ) 2Mlog(2/δ)
σ + , if B ≥ 2
t,k B 3B t,k
t,k t,k
With probability at least 1−δ,
|µ −µ | ≤ ϕ(t,k)+ψ(t,k,δ).
(cid:98)t,k t
Here ϕ(t,k) upper bounds the bias induced by using data from the k most recent periods, and
ψ(t,k,δ) upper bounds the stochastic error associated with µ . We seek an answer to the following
(cid:98)t,k
question:
How to find (cid:98)k such that |µ
(cid:98)t,(cid:98)k
−µ t| is comparable to min k∈[t]{ϕ(t,k)+ψ(t,k,δ)}?
Both ϕ and ψ involve unknown quantities µ and σ , so direct minimization over k ∈ [t] is infeasible.
j j
In the following, we will construct proxies for ψ and ϕ that are computable from data.
We first work on ψ(t,k,δ). A natural approximation of σ2 is the sample variance over the k
t,k
most recent periods, given by
1
(cid:88)t (cid:88)Bj
v2 = (u −µ )2. (3.2)
(cid:98)t,k
B −1
j,i (cid:98)t,k
t,k
j=t−k+1i=1
It has been used for deriving empirical versions of the Bernstein inequality (Audibert et al., 2007;
Maurer and Pontil, 2009). Based on the estimate v2 , we define our proxy as
(cid:98)t,k

M, if B = 1
 t,k
ψ(cid:98)(t,k,δ) = (cid:113) . (3.3)
v 2log(2/δ) + 8Mlog(2/δ) , if B ≥ 2
(cid:98)t,k B t,k 3(B t,k−1) t,k
As Lemma 3.2 shows, with high probability, ψ(cid:98) is an upper bound on ψ, and is at the same time
not too much larger than ψ. Its proof is deferred to Appendix A.2.
Lemma 3.2. Choose any δ ∈ (0,1). If B ≥ 2, define
t,k
(cid:115)
4log(2/δ) 13(b−a)log(2/δ)
ξ(t,k,δ) = max |µ −µ |+ .
j t
B t,k t−k+1≤j≤t 3(B t,k −1)
If B = 1, define ξ(t,k,δ) = 0. Then
t,k
(cid:16) (cid:17)
P ψ(t,k,δ) ≤ ψ(cid:98)(t,k,δ) ≥ 1−δ
(cid:16) (cid:17)
P ψ(cid:98)(t,k,δ) ≤ ψ(t,k,δ)+ξ(t,k,δ) ≥ 1−δ.
Combining Corollary 3.1 with the first bound in Lemma 3.2 immediately gives the following
useful corollary.
Corollary 3.2. Let δ ∈ (0,1). With probability at least 1−2tδ, it holds
|µ
(cid:98)t,k
−µ t| ≤ ϕ(t,k)+ψ(cid:98)(t,k,δ), ∀k ∈ [t]. (3.4)
4Algorithm 1 Adaptive Rolling Window for Mean Estimation (Problem 3.1)
Input: Data {D }t , hyperparameters δ and M.
j j=1
for k = 1,··· ,t do
Compute µ (cid:98)t,k, v (cid:98)t,k, ψ(cid:98)(t,k,δ) and ϕ(cid:98)(t,k,δ) according to (3.1), (3.2), (3.3) and (3.5).
end for
Choose any (cid:98)k ∈ argmin {ϕ(cid:98)(t,k,δ)+ψ(cid:98)(t,k,δ)}.
k∈[t]
Output: µ .
(cid:98)t,(cid:98)k
To construct a proxy for ϕ(t,k), we borrow ideas from the Goldenshluger-Lepski method for
adaptive non-parametric estimation (Goldenshluger and Lepski, 2008). Define
(cid:18) (cid:19)
ϕ(cid:98)(t,k,δ) = max |µ
(cid:98)t,k
−µ (cid:98)t,i|−[ψ(cid:98)(t,k,δ)+ψ(cid:98)(t,i,δ)] . (3.5)
i∈[k]
+
We now give an interpretation of ϕ(cid:98). In light of the bias-variance decomposition in Corollary 3.1, the
quantity
|µ
(cid:98)t,k
−µ (cid:98)t,i|−[ψ(cid:98)(t,k,δ)+ψ(cid:98)(t,i,δ)] (3.6)
can be viewed as a measure of the bias between the windows k and i, where subtracting ψ(cid:98)(t,k,δ)
and ψ(cid:98)(t,i,δ) eliminates the stochastic error and teases out the bias. Indeed, as the following lemma
shows, ϕ(cid:98)(t,k,δ) ≤ 2ϕ(t,k) holds with high probability. Its proof is given in Appendix A.3.
Lemma 3.3. When the event (3.4) happens,
0 ≤ ϕ(cid:98)(t,k,δ) ≤ 2ϕ(t,k).
We take the positive part in (3.5) so that when the quantity (3.6) is negative, we regard the bias
as dominated by the stochastic error and hence negligible. Taking maximum over all windows i ∈ [k]
makes sure that we detect all possible biases between window k and the smaller windows.
Replacing ψ and ϕ with their proxies ψ(cid:98)and ϕ(cid:98)gives Algorithm 1. We note that the quantities µ
(cid:98)t,k
and v can be conveniently computed from summary statistics of individual datasets. Define empir-
(cid:98)t,k
ical first and second moments in the j-th time period, µ = B−1(cid:80)Bj u and ω = B−1(cid:80)Bj u2 .
(cid:98)j j i=1 j,i (cid:98)j j i=1 j,i
Then,
t
1 (cid:88)
µ = B µ , (3.7)
(cid:98)t,k j(cid:98)j
B
t,k
j=t−k+1
(cid:18) t (cid:19)
v2 = B t,k · 1 (cid:88) B ω −µ2 . (3.8)
(cid:98)t,k
B −1 B
j(cid:98)j (cid:98)t,k
t,k t,k
j=t−k+1
We now present theoretical guarantees for Algorithm 1.
Theorem 3.1. Let Assumption 3.1 hold. Choose δ ∈ (0,1). With probability at least 1−3tδ, the
output of Algorithm 1 satisfies
|µ −µ | ≲ min{ϕ(t,k)+ψ(t,k,δ)}.
(cid:98)t,(cid:98)k t
k∈[t]
Here ≲ only hides a logarithmic factor of δ−1.
5Theorem3.1statesthattheselectedwindow (cid:98)k isnear-optimalfortheerrorboundinCorollary3.1
derived from bias-variance decomposition. Before proving the theorem, let us use a toy example to
illustrate this oracle property. Imagine that the environment remained unchanged over the past K
time periods but had been different before, i.e. P ̸= P = ··· = P . If K were known, one
t−K t−K+1 t
could take the window size K and output µ as an estimate of µ . Theorem 3.1 implies that µ is
(cid:98)t,K t (cid:98)t,(cid:98)k
at least comparable to µ in terms of the estimation error. Therefore, Algorithm 1 automatically
(cid:98)t,K
adapts to the local stationarity and is comparable to using B i.i.d. samples. Up to logarithmic
t,K
factors, we have a Bernstein-type bound
σ 1
|µ −µ | ≲ t + .
(cid:98)t,(cid:98)k t (cid:112)
B B
t,K t,K
We now turn to the proof of Theorem 3.1. A key ingredient is the following lemma, which can
be seen as an empirical version of Theorem 3.1 with ψ replaced by ψ(cid:98).
Lemma 3.4. Let the event (3.4) happen, which has probability at least 1−2tδ. Then the window (cid:98)k
satisfies
|µ
(cid:98)t,(cid:98)k
−µ t| ≤ 3m k∈i [n t]{ϕ(t,k)+ψ(cid:98)(t,k,δ)}.
Proof. For any k ∈ [(cid:98)k],
|µ −µ | ≤ |µ −µ |+|µ −µ |
(cid:98)t,(cid:98)k t (cid:98)t,(cid:98)k (cid:98)t,k (cid:98)t,k t
(cid:104) (cid:105) (cid:104) (cid:105)
≤ ϕ(cid:98)(t,(cid:98)k,δ)+ψ(cid:98)(t,(cid:98)k,δ)+ψ(cid:98)(t,k,δ) + ϕ(t,k)+ψ(cid:98)(t,k,δ) (by (3.5) and (3.4))
(cid:104) (cid:105) (cid:104) (cid:105)
= ϕ(cid:98)(t,(cid:98)k,δ)+ψ(cid:98)(t,(cid:98)k,δ) + ϕ(t,k)+2ψ(cid:98)(t,k,δ)
(cid:104) (cid:105) (cid:104) (cid:105)
≤ ϕ(cid:98)(t,k,δ)+ψ(cid:98)(t,k,δ) + ϕ(t,k)+2ψ(cid:98)(t,k,δ) (by the definition of (cid:98)k)
(cid:104) (cid:105) (cid:104) (cid:105)
≤ 2ϕ(t,k)+ψ(cid:98)(t,k,δ) + ϕ(t,k)+2ψ(cid:98)(t,k,δ) (by Lemma 3.3)
(cid:104) (cid:105)
= 3 ϕ(t,k)+ψ(cid:98)(t,k,δ) .
On the other hand, for any k ∈ {(cid:98)k+1,··· ,t},
|µ
(cid:98)t,(cid:98)k
−µ t| ≤ ϕ(t,(cid:98)k)+ψ(cid:98)(t,(cid:98)k,δ) (by (3.4))
(cid:104) (cid:105)
≤ ϕ(t,k)+ ψ(cid:98)(t,(cid:98)k,δ)+ϕ(cid:98)(t,(cid:98)k,δ) (by Lemma 3.3)
(cid:104) (cid:105)
≤ ϕ(t,k)+ ψ(cid:98)(t,k,δ)+ϕ(cid:98)(t,k,δ) (by the definition of (cid:98)k)
(cid:104) (cid:105)
≤ 3 ϕ(t,k)+ψ(cid:98)(t,k,δ) . (by Lemma 3.3)
The proof is finished by taking minimum over k ∈ [t].
Combining Lemma 3.4 and the second bound in Lemma 3.2 proves Theorem 3.1. We provide a
full proof in Appendix A.4, along with a more precise bound.
6Algorithm 2 Adaptive Rolling Window for Model Comparison
Input: Models f and f , data {B }t , hyperparameters δ and M.
1 2 j j=1
Let u = ℓ(f ,z )−ℓ(f ,z ) and D = {u }Bj .
j,i 1 j,i 2 j,i j j,i i=1
(cid:40)
1, if ∆(cid:98) ≤ 0
Run Algorithm 1 with inputs {D j}t j=1, δ and M to obtain ∆(cid:98) t,(cid:98)k. Let r
(cid:98) (cid:98)k
=
2, if
∆(cid:98)t,(cid:98)k
>
0.
t,(cid:98)k
Output: f(cid:98)= f
r
.
(cid:98)k(cid:98)
4 Model Selection
In this section, we consider Problem 2.2. We will first study the case of selection between two models,
and then extend the approach to the general case of m ∈ Z models.
+
4.1 Warmup: Model Comparison
We first consider the case m = 2, where the goal is to compare two models f and f , and choose
1 2
the better one. As in Section 3, using a look-back window k ∈ [t], we can estimate L (f) by
t
1
(cid:88)t (cid:88)Bj
L(cid:98)t,k(f) = ℓ(f,z j,i).
B
t,k
j=t−k+1i=1
We will choose k ∈ [t] and return
r
(cid:98)k
∈ argminL(cid:98)t,k(f r).
r∈[2]
Our approach is based on the following key observation, proved in Appendix B.1.
Lemma 4.1. For every k ∈ [t], the index r satisfies
(cid:98)k
(cid:12) (cid:12)
L t(f
r
(cid:98)k)− rm ∈i [2n ]L t(f r) ≤ (cid:12) (cid:12)[L(cid:98)t,k(f 1)−L(cid:98)t,k(f 2)]−[L t(f 1)−L t(f 2)](cid:12) (cid:12).
Define
∆ = L (f )−L (f )
j j 1 j 2
and ∆(cid:98)t,k = L(cid:98)t,k(f 1)−L(cid:98)t,k(f 2). Then, ∆
j
is the performance gap between f
1
and f
2
at time j, and
∆(cid:98)t,k is a sample average. By Lemma 4.1, it suffices to choose k such that |∆(cid:98)t,k −∆ t| is small. That
is, an accurate estimate of the performance gap guarantees near-optimal selection.
This reduces the problem to Problem 3.1, with u = ℓ(f ,z )−ℓ(f ,z ) and thus µ = ∆ .
j,i 1 j,i 2 j,i t t
We can then readily apply Algorithm 1. The detailed description is given in Algorithm 2.
Theorem 3.1 and Lemma 4.1 directly yield the following guarantee of Algorithm 2.
Theorem 4.1. Let Assumption 3.1 hold. Choose δ ∈ (0,1). With probability at least 1 − 3tδ,
Algorithm 2 outputs f(cid:98)satisfying
(cid:40) (cid:41)
σ 1
L t(f(cid:98))−minL t(f r) ≲ min max |∆
j
−∆ t|+ (cid:112)(cid:101)t,k + ,
r∈[2] k∈[t] t−k+1≤j≤t B t,k B t,k
where
t
1 (cid:88)
σ2 = B var [ℓ(f ,z)−ℓ(f ,z)]
(cid:101)t,k
B
j z∼Pj 1 2
t,k
j=t−k+1
and ≲ only hides a logarithmic factor of δ−1.
7Consider again the case where P ̸= P = ··· = P for some K. Theorem 4.1 admits a
t−K t−K+1 t
similar interpretation as Theorem 3.1: Algorithm 2 selects f(cid:98)satisfying
σ 1 1
L t(f(cid:98))−minL t(f r) ≲ (cid:112)(cid:101)t + ≲
(cid:112)
, (4.1)
r∈[2] B t,K B t,K B t,K
where σ2 = var (ℓ(f ,z)−ℓ(f ,z)).
(cid:101)t z∼Pt 1 2
In the setting of bounded regression without covariate shift, we may further improve the rate in
(4.1). To state the results, we let X be a feature space and consider the following assumptions.
Assumption 4.1 (Bounded response). For j ∈ Z , a sample z ∼ P takes the form z = (x,y),
+ j
where x ∈ X is the covariate and y ∈ R is the response. There exists M > 0 such that |y| ≤ M
0 0
holds for (x,y) ∼ P and j ∈ Z .
j +
Assumption 4.2 (Bounded models). The loss ℓ is given by ℓ(f,(x,y)) = [f(x)−y]2. For all x ∈ X,
|f (x)| ≤ M and |f (x)| ≤ M .
1 0 2 0
Assumption 4.3 (Nocovariateshift). The distributions {P }∞ have the same marginal distribution
j j=1
of the covariate, denoted by P.
Assumptions 4.1 and 4.2 imply Assumption 3.1 with a = 0 and b = 4M2. In Assumption 4.3,
0
the distribution of the covariate x stays constant, while the conditional distribution of y given
j,1 j,1
x may experience shifts. The latter is commonly known as concept drift (Gama et al., 2014).
j,1
Before we state the result, we introduce a few notations. Define f∗(·) = E(y |x = ·), which
j j,1 j,1
minimizes the mean square error E[f(x)−y]2 over the class of all measurable functions f : X → R.
For h ,h ∈ F, define an inner product
1 2
⟨h ,h ⟩ = E [h (x)h (x)],
1 2 P x∼P 1 2
which induces a norm ∥h∥ = (cid:112) ⟨h,h⟩ . It can be readily checked that L (f)−L (f∗) = ∥f −f∗∥2
P P t t t t P
for all f ∈ F. Thus, we may measure the performance of a model f ∈ F by
(cid:113)
∥f −f∗∥ = L (f)−L (f∗),
t P t t t
which can be interpreted as both the distance between f and f∗ under ∥·∥ , and the square root of
t P
the excess risk L (f)−L (f∗). For j ̸= t, the quantity ∥f∗−f∗∥ serves as a measure of distribution
t t t j t P
shift between time j and time t.
We are now ready to state our result. In Appendix B.2 we provide a more precise bound and its
proof.
Theorem 4.2. Let Assumptions 4.1, 4.2 and 4.3 hold. Let M be a constant. Choose δ ∈ (0,1).
0
With probability at least 1−3tδ, Algorithm 2 outputs f(cid:98)satisfying
(cid:40) (cid:41)
1
∥f(cid:98)−f t∗∥
P
−min∥f
r
−f t∗∥
P
≲ min max ∥f j∗−f t∗∥
P
+
(cid:112)
.
r∈[2] k∈[t] t−k+1≤j≤t B t,k
Here ≲ only hides a logarithmic factor of δ−1.
The oracle inequality in Theorem 4.2 shares the same bias-variance structure as that of Theo-
rem 4.1. By squaring both sides of the bound, we see that in the case of P ̸= P = ··· = P ,
t−K t−K+1 t
1
L t(f(cid:98))−L t(f t∗) ≲ rm ∈i [2n ]L t(f r)−L t(f t∗)+
B
t,K. (4.2)
8Algorithm 3 Single-Elimination Tournament for Model Selection
Input: Models {f }m , data {B }t , hyperparameters δ and M
r r=1 j j=1
Let S = ⌈log m⌉ and F = {f : r ∈ [m]}.
2 1 r
for s = 1,··· ,S do
Split F into disjoint groups:
s
F = G ∪···∪G ∪G ,
s s,1 s,ms s,ms+1
where |G | = 2 for i ∈ [m ], and |G | ∈ {1,2}.
s,i s s,ms+1
for i = 1,··· ,m +1 do
s
Run Algorithm 2 with inputs G , {D }t , δ and M to obtain g ∈ G .
s,i j j=1 (cid:98)s,i s,i
If |G | = 1, simply take g ∈ G .
s,i (cid:98)s,i s,i
end for
Let F = {g : i ∈ [m +1]}.
s+1 (cid:98)s,i s
end for
Output: The only model f(cid:98)∈ F S.
When either f or f has a small excess risk so that min L (f )−L (f∗) = o(1/(cid:112) B ), (4.2)
1 2 r∈[2] t r t t t,K
providesamuchsharperguaranteeonL t(f(cid:98))comparedwith(4.1). AstheproofofTheorem4.2reveals,
such an improvement relies crucially on the structure of var(ℓ(f ,z)−ℓ(f ,z)) in the Bernstein
1 2
bound. In particular, it cannot be achieved by the naïve method of applying Algorithm 1 to f and
1
f separately and choosing the one with a lower estimated generalization error.
2
4.2 Selection from Multiple Candidates
We now consider the general case of selecting over m ∈ Z models {f }m . We will use a
+ r r=1
straightforward single-elimination tournament procedure. In each round, we pair up the remaining
models, and use Algorithm 2 to perform pairwise comparison. Within each pair, the model picked
by Algorithm 2 advances to the next round. When there is only one model left, the procedure
terminates and outputs the model. Algorithm 3 gives the detailed description.
Here F is the set of remaining models after round s. By design, Algorithm 3 eliminates about
s+1
half of the remaining models in each round: |F | = ⌈|F |/2⌉. Thus, only one model remains
s+1 s
after ⌈log m⌉ rounds. Since each call of Algorithm 2 eliminates one model, then Algorithm 3 calls
2
Algorithm 2 exactly m−1 times.
We now give the theoretical guarantee of Algorithm 3 in the setting of bounded regression. We
provide a more precise bound and its proof in Appendix B.3.
Theorem 4.3. Let Assumptions 4.1, 4.2 and 4.3 hold. Let M be a constant. Choose δ ∈ (0,1).
0
With probability at least 1−3tδ, Algorithm 3 outputs f(cid:98)satisfying
(cid:40) (cid:41)
1
∥f(cid:98)−f t∗∥
P
− min ∥f
r
−f t∗∥
P
≲ min max ∥f j∗−f t∗∥
P
+
(cid:112)
.
r∈[m] k∈[t] t−k+1≤j≤t B t,k
Here ≲ hides a polylogarithmic factor of m and δ−1.
We remark that Theorem 4.3 takes the same form as Theorem 4.2 up to a factor of log m. Thus,
2
in the case of P
t−K
̸= P
t−K+1
= ··· = P t, the model f(cid:98)selected by Algorithm 3 also enjoys a fast
rate similar to (4.2):
1
L t(f(cid:98))−L t(f t∗) ≲ rm ∈[i mn ]L t(f r)−L t(f t∗)+
B
t,K.
9Algorithm 4 Fixed Window Model Selection Algorithm
Input: Models {f }m , data {B }t , window size k.
r r=1 j j=1
Compute s = min{t,k}.
for r = 1,··· ,m do
L(cid:98)t,k(f r) = B1
t,s
(cid:80)t j=t−s+1(cid:80)B i=j 1ℓ(f r,z j,i).
end for
Compute r (cid:98)∈ argmin r∈[m]L(cid:98)t,k(f r).
Output: f(cid:98)k = f r.
(cid:98)
(a) Example 5.1 (b) Example 5.2
Figure 1: True means {µ }100 in synthetic data.
t t=0
5 Numerical Experiments
We test our main method, Algorithm 3 for Problem 2.2, on synthetic data and two real-world
datasets. For simplicity, the hyperparameters δ and M are set to be 0.1 and 0 throughout our
numerical study. For notational convenience, we denote Algorithm 3 by V . We compare V
ARW ARW
with fixed-window model selection procedures given by Algorithm 4 and denoted by V , where k is
k
the window size taking values in I = {1,4,16,64,256}. The code and the results for the synthetic
data experiment are available at https://github.com/eliselyhan/ARW.
5.1 Synthetic Data
We will start by training different models on synthetic data for a mean estimation task. Then,
we deploy Algorithm 3 and Algorithm 4 to select top-performing models. Finally, we assess and
compare their qualities. Throughout the simulations, we consider 100 time periods. Let σ > 0 be a
fixed number. At the t-th time period, we generate µ ∈ R and a batch of i.i.d. samples B from
t t
N(µ ,σ2). The goal is to estimate µ .
t t
Forthet-thtimeperiod,wesplitB intoatrainingsetBtr andavalidationsetBva. Thevalidation
t t t
batch size |Btr| is sampled uniformly from {2,3,4}. The training batch size is |Btr| = 3|Bva|. To
t t t
estimate the true parameter µ , we construct 5 estimates {T(t,w)} using {Btr}t , where T(t,w)
t w∈I j j=1
is the sample average of the data {Btr}t in the past w time periods. Then, we select one
j j=t−w∧t+1
of them using the validation data {Bva}t . Its out-of-sample performance is measured by the
j j=1
excess risk (equivalent to squared estimation error). The following examples are used as testbeds for
comparing V with the fixed-window benchmarks {V } .
ARW k k∈I
Example 5.1. Figure 1(a) illustrates the stationary case where the true mean stays constant. We
explore both low- and high-variance regimes with σ2 = 1 and 10. Table 1 records the average excess
10Table 1: Mean excess risks of selection methods for Example 5.1.
V V V V V V
ARW 1 4 16 64 256
0.015 0.043 0.025 0.013 0.010 0.010
1.293 4.117 2.572 1.396 1.015 0.982
Table 2: Mean excess risks of selection methods for Example 5.2.
V V V V V V
ARW 1 4 16 64 256
0.139 0.157 0.171 0.539 1.034 1.067
2.052 4.425 2.934 1.920 1.771 1.784
risks over 100 periods and 20 independent trials for V and {V } . The first and second rows
ARW k k∈I
correspond to small and large σ2. In both regimes, the adaptive V can leverage the underlying
ARW
stationarity, yielding excess risks comparable to using the larger k’s, whereas the smaller windows
perform worse. In Figure 2, we plot the average excess risks over 20 trials at each time t for
V ,V ,V in red, orange and blue.
ARW 1 256
Example 5.2. We carry out the same experiments in a scenario with sufficient non-stationarity. The
true means {µ }100 in Figure 1(b) are generated using a randomized mechanism. See Appendix C.1
t t=1
for a detailed description. Similar to that of Example 1, Table 2 summarizes the mean excess
risks. When σ2 = 1, the non-stationary of the underlying means is largely preserved, and V
ARW
outperforms all other V ’s, demonstrating its adaptive abilities. When σ2 = 10, the large noise makes
w
the non-stationarity less significant, so V with a larger w will be more stable. Nevertheless, V
w ARW
maintains a competitive performance. We also plot the average excess risks over 20 trials at each
time t in Figure 3.
5.2 Real Data: Topic Frequency Estimation
We now analyze a real-world dataset collected from arXiv.org,1 which contains basic information of
the submitted papers, such as title, abstract and categories. We study the topics of papers in the
category cs.LG from March 1st, 2020 to December 31st, 2023. There are 118,883 papers in total,
and we would like to estimate the proportion of deep learning papers in this category. Each week
is a time period, and there are 200 weeks in total. We regard a paper as a deep learning paper if
its abstract contains at least one of the words “deep”, “neural”, “dnn” and “dnns”. The data in each
period is randomly split into training, validation and test sets. The training set Btr has 15 samples,
j
the validation set Bva has 5 samples and the rest of the samples Bte are used for testing. Typically
j j
|Bte| ∈ [300,1200]. In Figure 6(a), we plot the frequencies over 200 weeks estimated from {Bte}200,
j j j=1
which exhibits a slowly drifting pattern.
In each period t, we construct 5 estimates {T(t,w)} , where T(t,w) computes the average
w∈I
frequency from the data {Btr}t . The selection procedure is similar to that in the synthetic
j j=t−w∧t+1
dataexperiment. Tomeasurethequalityoftheselectedmodel, wecomputeitsexcessrisk(equivalent
tosquaredestimationerror)usingBte. WecompareV withthefixed-windowbenchmarks{V } .
t ARW k k∈I
1https://www.kaggle.com/datasets/Cornell-University/arxiv
11Figure 2: Excess risks in Example 1: σ2 = 1 (up) and σ2 = 10 (down). Red: V . Orange: V .
ARW 1
Blue: V .
256
Figure 3: Excess risks in Example 2: σ2 = 1 (up) and σ2 = 10 (down). Red: V . Orange: V .
ARW 1
Blue: V .
256
The average excess risks over 200 weeks and 20 independent runs are listed in Table 3. In Figure 4,
we also plot the excess risks in every period. We observe that the performance of V is comparable
ARW
to that of the large-window benchmark V , while the small-window model selection algorithm V
256 1
performs poorly.
Table 3: Overall average excess risks (×10−3) of selection methods on the arXiv data.
V V V V V V
ARW 1 4 16 64 256
2.4 6.7 4.5 2.4 1.7 1.9
Figure 4: Error curves of different model selection methods on the arXiv data. Red: V . Orange:
ARW
V . Blue: V .
1 256
125.3 Real Data: Housing Price Prediction
Finally, we test our method using a real estate dataset maintained by the Dubai Land Department.2
We study sales of apartments during the past 16 years (from January 1st, 2008 to December 31st,
2023). There are 211,432 samples (sales) in total, and we want to predict the final price given
characteristics of an apartment (e.g., number of rooms, size, location). Each month is treated as a
time period, and there are 192 of them in total. Our goal is to build a prediction model for each
period using historical data. In Figure 6(b), we plot the monthly average prices. Compared with the
arXiv data, the distribution shift in this case is more abrupt.
The data in each period is randomly split into training, validation and test sets with proportions
60%, 20% and 20%, respectively. We follow the standard practice to apply a logarithmic transform
to the price and target that in our prediction. See Appendix C.3 for other details of preprocessing.
Table 4: Overall MSE of different methods on the housing data.
V V V V V V
ARW 1 4 16 64 256
0.072 0.072 0.071 0.072 0.090 0.095
For the t-th period, we use each of the 5 training windows w ∈ I to perform random forest
regression on training data {Btr}t . This results in models {T(t,w)} . One of them is
j j=t−w∧t+1 w∈I
selected using validation data and finally evaluated on the test data Bte by the mean squared error
t
(MSE). For each model selection method, we compute the average MSE over all of the 192 time
periodsand20independentruns. Wecompareourproposedapproachwithfixed-windowbenchmarks
{V } . The mean values are reported in Table 4. In addition, we also plot the test MSEs of our
k k∈I
method, V and V in all individual time periods, see Figure 5. Due to the strong non-stationarity,
1 256
model selection based on large windows does not work well. Our method still nicely adapts to the
changes.
0.3
0.2
0.1
0 25 50 75 100 125 150 175 200
Time
Figure 5: Error curves of different model selection methods on the housing data. Red: V .
ARW
Orange: V . Blue: V .
1 256
6 Discussions
We developed adaptive window approaches to model assessment and selection in a changing environ-
ment. Theoretical analyses and numerical experiments demonstrate their adaptivity to unknown
2https://www.dubaipulse.gov.ae/data/dld-transactions/dld_transactions-open
13
ESMtemporal distribution shift. Several further directions are worth exploring. First, our algorithms and
analyses only apply to bounded losses. It is important both theoretically and practically to extend
them to unbounded loss functions. Second, our model selection algorithm attains a fast rate in the
setting of bounded regression without covariate shift. A natural question is whether a fast rate can
still be achieved in other scenarios such as classification. Finally, another interesting direction is to
extend our model selection algorithm to infinite model classes.
Acknowledgement
Elise Han, Chengpiao Huang and Kaizheng Wang’s research is supported by an NSF grant DMS-
2210907 and a startup grant at Columbia University.
14A Proofs for Section 3
A.1 Proof of Lemma 3.1
Inequality (2.10) in Boucheron et al. (2013) implies that for any t ≥ 0,
(cid:18) 1 (cid:88)n (cid:19) (cid:18) nt2/2 (cid:19)
P (x −Ex ) > t ≤ exp − .
n i i σ2+(b−a)t/3
i=1
Fix δ ∈ (0,1). Then,
(cid:18) nt2/2 (cid:19)
exp − ≤ δ
σ2+(b−a)t/3
nt2 t(b−a)log(1/δ)
⇔ ≥ σ2log(1/δ)+
2 3
n(cid:18) (b−a)log(1/δ)(cid:19)2 n(cid:18) (b−a)log(1/δ)(cid:19)2
⇔ t− ≥ σ2log(1/δ)+
2 3n 2 3n
(cid:18) (b−a)log(1/δ)(cid:19)2 (cid:18) (cid:114)
2log(1/δ)
(b−a)log(1/δ)(cid:19)2
⇐ t− ≥ σ +
3n n 3n
(cid:114)
2log(1/δ) 2(b−a)log(1/δ)
⇐ t ≥ σ + .
n 3n
Hence,
(cid:32) n (cid:114) (cid:33)
1 (cid:88) 2log(1/δ) 2(b−a)log(1/δ)
P (x −Ex ) > σ + ≤ δ.
i i
n n 3n
i=1
Replacing each x by −x gives bounds on the lower tail and the absolute deviation.
i i
A.2 Proof of Lemma 3.2
The result is trivial when B
t,k
= 1 (i.e. k = B
t
= 1), as ψ(cid:98)(t,k,δ) = ψ(t,k,δ). From now on, we
assume that B ≥ 2. We first present a useful lemma.
t,k
Lemma A.1. Let {x }n be independent, [0,1]-valued random variables. Define the sample mean
i i=1
µ = 1 (cid:80)n x and the sample variance v2 = 1 (cid:80)n (x −µ)2. Let µ = 1 (cid:80)n Ex and v2 = Ev2.
(cid:98) n i=1 i (cid:98) n−1 i=1 i (cid:98) n i=1 i (cid:98)
We have
n n
1 (cid:88) 1 (cid:88)
v2 = (Ex −µ)2+ var(x )
i i
n−1 n
i=1 i=1
and for any δ ∈ (0,1),
(cid:18) (cid:114) (cid:19) (cid:18) (cid:114) (cid:19)
2log(1/δ) 2log(1/δ)
P v ≤ v+ ≥ 1−δ and P v ≥ v− ≥ 1−δ.
(cid:98) (cid:98)
n−1 n−1
Proof of Lemma A.1. Define x = (x ,··· ,x )⊤, µ = Ex and Σ = diag(var(x ),··· ,var(x )). Let
1 n 1 n
1 be the n-dimensional all-one vector. We have µ = 1⊤x/n and
n (cid:98) n
n
(cid:88)
(x −µ)2 = ∥(I −1 1⊤/n)x∥2 = x⊤(I −1 1⊤/n)x.
i (cid:98) n n 2 n n
i=1
15Then,
(n−1)Ev2 = ⟨µµ⊤+Σ,I −1 1⊤/n⟩ = µ⊤(I −1 1⊤/n)µ+⟨Σ,I −1 1⊤/n⟩
(cid:98) n n n n n n
n (cid:18) (cid:19) n
(cid:88) 1 (cid:88)
= (Ex −µ)2+ 1− var(x ).
i i
n
i=1 i=1
This verifies the expression of v2. The concentration bounds come from Theorem 10 in Maurer and
Pontil (2009).
We now come back to Lemma 3.2. It suffices to consider the special case b−a = 1. From
Lemma A.1 we immediately get Ev2 = v2 , where
(cid:98)t,k t,k
t
1 (cid:88)
v2 = σ2 + B (µ −µ )2. (A.1)
t,k t,k B −1 j j t,k
t,k
j=t−k+1
In addition, for any δ ∈ (0,1),
(cid:32) (cid:115) (cid:33) (cid:32) (cid:115) (cid:33)
2log(1/δ) 2log(1/δ)
P v ≤ v + ≥ 1−δ and P v ≥ v − ≥ 1−δ.
t,k (cid:98)t,k t,k (cid:98)t,k
B −1 B −1
t,k t,k
(cid:113)
With probability at least 1−δ, we have σ ≤ v ≤ v + 2log(1/δ) and thus
t,k t,k (cid:98)t,k B −1
t,k
(cid:32) (cid:115) (cid:33)(cid:115)
2log(1/δ) 2log(2/δ) 2log(2/δ)
ψ(t,k,δ) ≤ v + +
(cid:98)t,k
B −1 B 3(B −1)
t,k t,k t,k
(cid:115)
2log(2/δ) 8log(2/δ)
≤ v
(cid:98)t,k
+ = ψ(cid:98)(t,k,δ).
B 3(B −1)
t,k t,k
To prove the second bound, note that
(cid:18)
1
(cid:88)t (cid:19)1/2
v = σ2 + B (µ −µ )2
t,k t,k B −1 j j t,k
t,k
j=t−k+1
(cid:18)
1
(cid:88)t (cid:19)1/2
≤ σ + B (µ −µ )2
t,k j j t,k
B −1
t,k
j=t−k+1
(cid:115)
= σ +
B
t,k
·(cid:18)
1
(cid:88)t
B (µ −µ
)2(cid:19)1/2
t,k j j t
B −1 B
t,k t,k
j=t−k+1
(cid:115)
B
t,k
≤ σ + · max |µ −µ |.
t,k j t
B t,k −1 t−k+1≤j≤t
(cid:113)
By Lemma A.1, with probability at least 1−δ, we have v ≥ v − 2log(1/δ) and thus
t,k (cid:98)t,k B −1
t,k
(cid:115) (cid:115) (cid:115)
2log(1/δ) B 2log(1/δ)
t,k
v ≤ v + ≤ σ + · max |µ −µ |+ .
(cid:98)t,k t,k t,k j t
B t,k −1 B t,k −1 t−k+1≤j≤t B t,k −1
16As a result,
(cid:115)
2log(2/δ) log(2/δ)
ψ(t,k,δ) ≥ σ +
t,k
B 3(B −1)
t,k t,k
(cid:115) (cid:115) (cid:115)
(cid:18) (cid:19)
B 2log(1/δ) 2log(2/δ) log(2/δ)
t,k
≥ v − · max |µ −µ |− +
(cid:98)t,k j t
B t,k −1 t−k+1≤j≤t B t,k −1 B t,k 3(B t,k −1)
(cid:115) (cid:115)
2log(2/δ) 2log(2/δ) 5log(2/δ)
≥ v − · max |µ −µ |−
(cid:98)t,k j t
B t,k B t,k −1 t−k+1≤j≤t 3(B t,k −1)
(cid:115) (cid:115)
(cid:18) (cid:19)
2log(2/δ) 8log(2/δ) 2log(2/δ) 13log(2/δ)
= v + − · max |µ −µ |−
(cid:98)t,k j t
B t,k 3(B t,k −1) B t,k −1 t−k+1≤j≤t 3(B t,k −1)
(cid:115)
2log(2/δ) 13log(2/δ)
= ψ(cid:98)(t,k,δ)− · max |µ
j
−µ t|−
B t,k −1 t−k+1≤j≤t 3(B t,k −1)
≥ ψ(cid:98)(t,k,δ)−ξ(t,k,δ).
The first and last inequalities follow from the fact that B ≥ 2.
t,k
A.3 Proof of Lemma 3.3
When the event (3.4) happens, for every i ∈ [k],
(cid:104) (cid:105) (cid:104) (cid:105)
|µ
(cid:98)t,k
−µ (cid:98)t,i| ≤ |µ
(cid:98)t,k
−µ t|+|µ (cid:98)t,i−µ t| ≤ ϕ(t,k)+ψ(cid:98)(t,k,δ) + ϕ(t,i)+ψ(cid:98)(t,i,δ) ,
so
(cid:104) (cid:105)
|µ
(cid:98)t,k
−µ (cid:98)t,i|− ψ(cid:98)(t,k,δ)+ψ(cid:98)(t,i,δ) ≤ ϕ(t,k)+ϕ(t,i) ≤ 2ϕ(t,k),
where we used ϕ(t,i) ≤ ϕ(t,k). Taking maximum over all i ∈ [k] gives ϕ(cid:98)(t,k,δ) ≤ 2ϕ(t,k).
A.4 Proof of Theorem 3.1
We will prove that with probability at least 1−3tδ,
(cid:40) (cid:115) (cid:41)
(cid:112) 2log(2/δ) 10(b−a)log(2/δ)
|µ −µ | ≤ 3min 3 log(2/δ)· max |µ −µ |+σ + .
(cid:98)t,(cid:98)k t
k∈[t] t−k+1≤j≤t
j t t,k
B t,k B t,k
By Lemma 3.4,
(cid:18) (cid:19)
(cid:110) (cid:111)
P |µ
(cid:98)t,(cid:98)k
−µ t| ≤ 3m k∈i [n
t]
ϕ(t,k)+ψ(cid:98)(t,k,δ) ≥ 1−2tδ. (A.2)
By the second bound in Lemma 3.2,
(cid:18) (cid:19)
P ψ(cid:98)(t,k,δ) ≤ ψ(t,k,δ′)+ξ(t,k,δ), ∀k ∈ [t] ≥ 1−tδ. (A.3)
By (A.2), (A.3) and the union bound, we obtain that with probability at least 1−3tδ,
(cid:26) (cid:27)
|µ −µ | ≤ 3min ϕ(t,k)+ψ(t,k,δ)+ξ(t,k,δ) .
(cid:98)t,(cid:98)k t
k∈[t]
17Finally, when B ≥ 2,
t,k
ϕ(t,k)+ψ(t,k,δ)+ξ(t,k,δ)
(cid:115)
2log(2/δ) 2(b−a)log(2/δ)
≤ max |µ −µ |+σ +
j t t,k
t−k+1≤j≤t B t,k 3(B t,k −1)
(cid:115)
4log(2/δ) 13(b−a)log(2/δ)
+ · max |µ −µ |+
j t
B t,k t−k+1≤j≤t 3(B t,k −1)
(cid:115)
(cid:112) 2log(2/δ) 5(b−a)log(2/δ)
≤ 3 log(2/δ)· max |µ −µ |+σ +
j t t,k
t−k+1≤j≤t B t,k B t,k −1
(cid:115)
(cid:112) 2log(2/δ) 10(b−a)log(2/δ)
≤ 3 log(2/δ)· max |µ −µ |+σ + .
j t t,k
t−k+1≤j≤t B t,k B t,k
When B = 1, we have ϕ(t,k)+ψ(t,k,δ) = b−a, which is dominated by the bound above. This
t,k
finishes the proof.
B Proofs for Section 4
B.1 Proof of Lemma 4.1
Fix k ∈ [t]. For every f ∈ {f ,f },
1 2
(cid:2) (cid:3) (cid:2) (cid:3)
L t(f r)−L t(f) = L t(f
r
)−L(cid:98)t,k(f
r
) + L(cid:98)t,k(f
r
)−L t(f)
(cid:98) (cid:98)k (cid:98)k (cid:98)k
(cid:2) (cid:3) (cid:2) (cid:3)
≤ L t(f
r
)−L(cid:98)t,k(f
r
) + L(cid:98)t,k(f)−L t(f)
(cid:98)k (cid:98)k
(cid:2) (cid:3) (cid:2) (cid:3)
= L(cid:98)t,k(f)−L(cid:98)t,k(f
r
) + L t(f
r
)−L t(f)
(cid:98)k (cid:98)k
(cid:12) (cid:12)
(cid:12)(cid:2) (cid:3) (cid:2) (cid:3)(cid:12)
≤ (cid:12) L(cid:98)t,k(f 1)−L(cid:98)t,k(f 2) + L t(f 1)−L t(f 2) (cid:12).
(cid:12) (cid:12)
Taking minimum over f ∈ {f ,f } yields
1 2
(cid:12) (cid:12)
L t(f
r
(cid:98))− rm ∈i [2n ]L t(f r) ≤ (cid:12) (cid:12)(cid:2) L(cid:98)t,k(f 1)−L(cid:98)t,k(f 2)(cid:3) +(cid:2) L t(f 1)−L t(f 2)(cid:3)(cid:12) (cid:12).
B.2 Proof of Theorem 4.2
We will prove that with probability at least 1−3tδ, Algorithm 2 outputs f(cid:98)satisfying
(cid:40) (cid:41)
(cid:112) M
∥f(cid:98)−f t∗∥
P
−min∥f
r
−f t∗∥
P
≤ C′ log(2+δ−1)min max ∥f j∗−f t∗∥
P
+
(cid:112)
,
r∈[2] k∈[t] t−k+1≤j≤t B t,k
where C′ > 0 is a universal constant.
Following the notation in Problem 3.1 with u = ℓ(f ,z )−ℓ(f ,z ), we define
j,i 1 j,i 2 j,i
t
1 (cid:88)
σ2 = var(u ) = var(ℓ(f ,z )−ℓ(f ,z )) and σ2 = B σ2.
j j,1 1 j,1 2 j,1 t,k B j j
t,k
j=t−k+1
18By Lemma 4.1 and Theorem 3.1, with probability at least 1−3tδ,
L t(f(cid:98))− rm ∈i [2n ]L t(f r) ≤ |∆(cid:98)
t,(cid:98)k
−∆ t|
(cid:40) (cid:115) (cid:41)
(cid:112) 2log(2/δ) 40M2log(2/δ)
≤ 3min 3 log(2/δ)· max |∆ −∆ |+σ + 0 . (B.1)
j t t,k
k∈[t] t−k+1≤j≤t B t,k B t,k
From now on suppose that (B.1) happens.
We first derive a bound for |∆ −∆ |. Since
j t
∆ = ∥f −f∗∥2 −∥f −f∗∥2 = ⟨f −f ,f +f −2f∗⟩ ,
j 1 j P 2 j P 1 2 1 2 j P
then for every j ∈ Z ,
+
|∆ −∆ | = 2|⟨f −f ,f∗−f∗⟩ | ≤ 2∥f −f ∥ ∥f∗−f∗∥ .
j t 1 2 j t P 1 2 P j t P
Hence
max |∆ −∆ | ≤ 2∥f −f ∥ max ∥f∗−f∗∥ . (B.2)
j t 1 2 P j t P
t−k+1≤j≤t t−k+1≤j≤t
Next, we bound σ . We have
t,k
σ2 ≤ 2var(cid:0) [f (x )−y ]2−[f∗(x )−y ]2(cid:1) +2var(cid:0) [f (x )−y ]2−[f∗(x )−y ]2(cid:1) .
j 1 j,1 j,1 j j,1 j,1 2 j,1 j,1 j j,1 j,1
For every f ∈ F,
(cid:104) (cid:105)
var(cid:0) [f(x )−y ]2−[f∗(x )−y ]2(cid:1) ≤ E (cid:0) [f(x )−y ]2−[f∗(x )−y ]2(cid:1)2
j,1 j,1 j j,1 j,1 j,1 j,1 j j,1 j,1
(cid:104) (cid:105)
= E (cid:0) f(x )−f∗(x )(cid:1)2(cid:0) f(x +f∗(x )−2y (cid:1)2
j,1 j j,1 j,1 j j,1 j,1
≲ M2∥f −f∗∥2.
0 j P
Thus,
σ ≲ M (cid:0) ∥f −f∗∥ +∥f −f∗∥ (cid:1) ≲ M (cid:0) ∥f −f∗∥ +∥f −f∗∥ +∥f∗−f∗∥ (cid:1) ,
j 0 1 j P 2 j P 0 1 t P 2 t P j t P
which implies
(cid:18) (cid:19)
σ ≲ M ∥f −f∗∥ +∥f −f∗∥ + max ∥f∗−f∗∥ . (B.3)
t,k 0 1 t P 2 t P j t P
t−k+1≤j≤t
Substituting (B.2) and (B.3) into (B.1), there exists a universal constant C > 0 such that for
every k ∈ [t],
(cid:34) (cid:32) (cid:33)
L t(f(cid:98))−minL t(f r) ≤ C (cid:112) log(2+δ−1) ∥f 1−f 2∥
P
+ (cid:112)M 0 max ∥f j∗−f t∗∥
P
r∈[2] B t,k t−k+1≤j≤t
(cid:115) (cid:35)
log(2+δ−1) M2log(2+δ−1)
+M (∥f −f∗∥ +∥f −f∗∥ )+ 0
0 B 1 t P 2 t P B
t,k t,k
(cid:20) (cid:21)
= C 2Φ(t,k)(∥f −f∗∥ +∥f −f∗∥ )+Ψ(t,k) , (B.4)
1 t P 2 t P
19where
(cid:32) (cid:33)
Φ(t,k) =
1(cid:112)
log(2+δ−1) max ∥f∗−f∗∥ +
M
0 ,
2 t−k+1≤j≤t j t P (cid:112) B t,k
(cid:115)
log(2+δ−1) M2log(2+δ−1)
Ψ(t,k) = M max ∥f∗−f∗∥ + 0 ,
0 B t,k t−k+1≤j≤t j t P B t,k
and we used the triangle inequality ∥f −f ∥ ≤ ∥f −f∗∥ +∥f −f∗∥ .
1 2 P 1 t P 2 t P
Without loss of generality, assume L t(f 1) ≥ L t(f 2). When r
(cid:98) (cid:98)k
= 1, we have f(cid:98)= f 1. Then (B.4)
yields
(cid:20) (cid:21)
∥f 1−f t∗∥2
P
−∥f 2−f t∗∥2
P
= L t(f(cid:98))−minL t(f r) ≤ C 2Φ(t,k)(∥f 1−f t∗∥
P
+∥f 2−f t∗∥ P)+Ψ(t,k) .
r∈[2]
Completing the squares gives
(cid:20) (cid:21)2 (cid:20) (cid:21)2
∥f −f∗∥ −CΦ(t,k) ≤ ∥f −f∗∥ +CΦ(t,k) +CΨ(t,k).
1 t P 2 t P
Hence
(cid:112)
∥f −f∗∥ ≤ ∥f −f∗∥ +2CΦ(t,k)+ CΨ(t,k)
1 t P 2 t P
(cid:32) (cid:33)
≤ ∥f −f∗∥ +C′(cid:112) log(2+δ−1) max ∥f∗−f∗∥ + M 0
2 t P j t P (cid:112)
t−k+1≤j≤t B t,k
for some universal constant C′ > 0. Since this holds for all k ∈ [t], we get
∥f(cid:98)−f t∗∥
P
−min∥f
r
−f t∗∥
P
r∈[2]
= ∥f −f∗∥ −∥f −f∗∥
1 t P 2 t P
(cid:40) (cid:41)
≤ C′(cid:112) log(2+δ−1)min max ∥f∗−f∗∥ + M 0 . (B.5)
j t P (cid:112)
k∈[t] t−k+1≤j≤t B t,k
We have proved this bound for the case r
(cid:98) (cid:98)k
= 1. When r
(cid:98) (cid:98)k
= 2, we have f(cid:98) = f
2
and hence
∥f(cid:98)−f t∗∥
P
−min r∈[2]∥f
r
−f t∗∥
P
= 0, so the bound (B.5) continues to hold.
B.3 Proof of Theorem 4.3
We will prove that with probability at least 1−3tδ, Algorithm 3 outputs f(cid:98)satisfying
(cid:40) (cid:41)
∥f(cid:98)−f t∗∥
P
− min ∥f
r
−f t∗∥
P
≤ C′(cid:112) (logm)log(m+δ−1)min max ∥f j∗−f t∗∥
P
+ (cid:112)M 0
r∈[m] k∈[t] t−k+1≤j≤t B t,k
for some universal constant C′ > 0.
Denote Algorithm 2 with data {B }t by A, which takes as input two models f′,f′′ ∈ F and
j j=1
outputs the selected model A({f′,f′′}) ∈ {f′,f′′}. For notational convenience, we set A({f}) = f
20for every f ∈ F. By Theorem 4.2 and the union bound, with probability at least 1−3tδ, the
following holds for all f′,f′′ ∈ {f }m :
r r=1
∥A({f′,f′′})−f∗∥ − min ∥f −f∗∥
t P t P
f∈{f′,f′′}
(cid:40) (cid:41)
≤ C(cid:112) log(m+δ−1)min max ∥f∗−f∗∥ + M 0 , (B.6)
j t P (cid:112)
k∈[t] t−k+1≤j≤t B t,k
where C > 0 is a universal constant. From now on, suppose that this event happens.
For each s ∈ [S +1], let
g ∈ argminL (g) = argmin∥g−f∗∥ .
s t t P
g∈Fs g∈Fs
Since F = F
1
⊇ ··· ⊇ F
S
⊇ F
S+1
= {f(cid:98)}, then
S
∥f(cid:98)−f t∗∥
P
− min ∥f
r
−f t∗∥
P
= (cid:88)(cid:0) ∥g s+1−f t∗∥
P
−∥g s−f t∗∥ P(cid:1) .
r∈[m]
s=1
For each s ∈ [S], there exists i ∈ [m +1] such that g ∈ G . Let g = A(G ) be the model
s s s s,is (cid:98)s s,is
selected from G in Algorithm 3. By (B.6) and the definition of g ,
s,is s
(cid:40) (cid:41)
∥g −f∗∥ −∥g −f∗∥ ≤ C(cid:112) log(m+δ−1)min max ∥f∗−f∗∥ + M 0 .
(cid:98)s t P s t P j t P (cid:112)
k∈[t] t−k+1≤j≤t B t,k
Moreover, since g ∈ F , then ∥g −f∗∥ ≤ ∥g −f∗∥ . Therefore,
(cid:98)s s+1 s+1 t P (cid:98)s t P
S
∥f(cid:98)−f t∗∥
P
− min ∥f
r
−f t∗∥
P
= (cid:88)(cid:0) ∥g s+1−f t∗∥
P
−∥g s−f t∗∥ P(cid:1)
r∈[m]
s=1
S
≤ (cid:88)(cid:0) ∥g −f∗∥ −∥g −f∗∥ (cid:1)
(cid:98)s t P s t P
s=1
S (cid:40) (cid:41)
≤ (cid:88) C(cid:112) log(m+δ−1)min max ∥f∗−f∗∥ + M 0
j t P (cid:112)
s=1 k∈[t] t−k+1≤j≤t B t,k
(cid:40) (cid:41)
≤ C′(cid:112) (logm)log(m+δ−1)min max ∥f∗−f∗∥ + M 0
j t P (cid:112)
k∈[t] t−k+1≤j≤t B t,k
for some universal constant C′ > 0.
C Numerical Experiments: Additional Details
C.1 Example 5.2 of the Synthetic Data
We give an outline of how the true mean sequence {µ } used in Figure 1(b) is generated. The
t
sequence is constructed by combining 4 parts, each representing a distribution shift pattern. In
the first part, we create big shifts in the sequence values. Then, we switch to a sinusoidal pattern.
Following that, we introduce a horizontal line. Finally, we will take a random step every period,
21where the step sizes are independently sampled from {1,−1} with equal probability and scaled with
a constant. The function takes in 3 parameters (N,n,seed) where N is the total number of periods,
n is the parameter determining the splitting points of the 4 parts, and seed is the random seed we
use for code reproducibility. In our experiment, we used N = 100, n = 2, and seed = 2024. The
exact function can be found in our code.
C.2 Non-Stationarity in arXiv and Housing Data
See Figure 6(a) and Figure 6(b).
13.8
13.6
13.4
13.2
13.0
0 25 50 75 100 125 150 175 200
Time
(a) arXiv data (b) Dubai housing data
Figure 6: Two patterns of temporal distribution shift: slow drift and abrupt changes.
C.3 Experiment on the Housing Data
We focus on transactions of studios and apartments with 1 to 4 bedrooms, between January 1st, 2008
and December 31st, 2023. We import variables instance_date (transaction date), area_name_en
(English name of the area where the apartment is located in), rooms_en (number of bedrooms),
has_parking (whether or not the apartment has a parking spot), procedure_area (area in the
apartment), actual_worth (final price) from the data.
We use instance_date (transaction date) to construct monthly datasets. The target for predic-
tion is the logarithmic of actual_worth. The predictors are area_name_en, rooms_en, has_parking
and procedure_area. area_name_en has 58 possible values and encoded as an integer variable.
We remove a sample if its actual_worth or procedure_area is among the largest or smallest
2.5% of the population, whichever is true. After the procedure, 91.6% of the data remain.
We run random forest regression using the function RandomForestRegressor in the Python
library scikit-learn. We set random_state = 0 and do not change any other default parameters.
References
Audibert, J.-Y., Munos, R. and Szepesvári, C. (2007). Tuning bandit algorithms in stochastic
environments. In Algorithmic Learning Theory (M. Hutter, R. A. Servedio and E. Takimoto, eds.).
Springer Berlin Heidelberg, Berlin, Heidelberg.
22
naeM
detamitsEBai, Y., Zhang, Y.-J., Zhao, P., Sugiyama, M. and Zhou, Z.-H. (2022). Adapting to online
label shift with provable guarantees. Advances in Neural Information Processing Systems 35
29960–29974.
Besbes, O., Gur, Y. and Zeevi, A. (2015). Non-stationary stochastic optimization. Operations
Research 63 1227–1244.
URL http://www.jstor.org/stable/24540443
Bifet, A. and Gavalda, R. (2007). Learning from time-changing data with adaptive windowing.
In Proceedings of the 2007 SIAM international conference on data mining. SIAM.
Boucheron, S.,Lugosi, G.andMassart, P.(2013).ConcentrationInequalities: ANonasymptotic
Theory of Independence. Oxford University Press.
URL https://doi.org/10.1093/acprof:oso/9780199535255.001.0001
Daniely, A., Gonen, A. and Shalev-Shwartz, S. (2015). Strongly adaptive online learning.
In Proceedings of the 32nd International Conference on Machine Learning (F. Bach and D. Blei,
eds.), vol. 37 of Proceedings of Machine Learning Research. PMLR, Lille, France.
URL https://proceedings.mlr.press/v37/daniely15.html
Gama, J. a., Žliobaitundefined, I., Bifet, A., Pechenizkiy, M. and Bouchachia, A. (2014).
A survey on concept drift adaptation. ACM Comput. Surv. 46.
URL https://doi.org/10.1145/2523813
Gibbs, I. and Candes, E. (2021). Adaptive conformal inference under distribution shift. Advances
in Neural Information Processing Systems 34 1660–1672.
Goldenshluger, A. and Lepski, O. (2008). Universal pointwise selection rule in multivariate
function estimation .
Hanneke, S., Kanade, V. and Yang, L. (2015). Learning with a drifting target concept.
In Algorithmic Learning Theory (K. Chaudhuri, C. GENTILE and S. Zilles, eds.). Springer
International Publishing, Cham.
Hastie, T., Tibshirani, R., Friedman, J. H. and Friedman, J. H. (2009). The elements of
statistical learning: data mining, inference, and prediction, vol. 2. Springer.
Hazan, E. and Seshadhri, C. (2009). Efficient learning algorithms for changing environments. In
Proceedings of the 26th annual international conference on machine learning.
Huang, C. and Wang, K. (2023). A stability principle for learning under non-stationarity. arXiv
preprint arXiv:2310.18304 .
Jadbabaie, A.,Rakhlin, A.,Shahrampour, S.andSridharan, K.(2015). OnlineOptimization
: CompetingwithDynamicComparators. InProceedingsoftheEighteenthInternationalConference
on Artificial Intelligence and Statistics (G. Lebanon and S. V. N. Vishwanathan, eds.), vol. 38 of
Proceedings of Machine Learning Research. PMLR, San Diego, California, USA.
URL https://proceedings.mlr.press/v38/jadbabaie15.html
Littlestone, N. and Warmuth, M. K. (1994). The weighted majority algorithm. Information
and computation 108 212–261.
23Maurer, A. and Pontil, M. (2009). Empirical Bernstein bounds and sample variance penalization.
In Conference on Learning Theory.
Mazzetto, A., Esfandiarpoor, R., Upfal, E. and Bach, S. H. (2023). An adaptive method
for weak supervision with drifting data. arXiv preprint arXiv:2306.01658 .
Mazzetto, A.andUpfal, E.(2023). Anadaptivealgorithmforlearningwithunknowndistribution
drift. In Thirty-seventh Conference on Neural Information Processing Systems.
URL https://openreview.net/forum?id=exiXmAfuDK
Mohri, M. and Muñoz Medina, A. (2012). New analysis and algorithm for learning with drifting
distributions. In Algorithmic Learning Theory: 23rd International Conference, ALT 2012, Lyon,
France, October 29-31, 2012. Proceedings 23. Springer.
Niu, Y. S., Hao, N. and Zhang, H. (2016). Multiple change-point detection: A selective overview.
Statistical Science 31 611–623.
URL http://www.jstor.org/stable/26408091
Quinonero-Candela, J., Sugiyama, M., Schwaighofer, A. and Lawrence, N. D. (2022).
Dataset Shift in Machine Learning. MIT Press.
Truong, C., Oudre, L. and Vayatis, N. (2020). Selective review of offline change point detection
methods. Signal Processing 167 107299.
URL https://www.sciencedirect.com/science/article/pii/S0165168419303494
Wei, C.-Y. and Luo, H. (2021). Non-stationary reinforcement learning without prior knowledge:
an optimal black-box approach. In Proceedings of Thirty Fourth Conference on Learning Theory
(M. Belkin and S. Kpotufe, eds.), vol. 134 of Proceedings of Machine Learning Research. PMLR.
URL https://proceedings.mlr.press/v134/wei21b.html
24