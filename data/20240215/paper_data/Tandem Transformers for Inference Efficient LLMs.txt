Tandem Transformers for Inference Efficient LLMs
AishwaryaPS1 PranavAjitNair1 YashasSamaga1 TobyBoyd2 SanjivKumar3 PrateekJain*1
PraneethNetrapalli*1
Abstract 2023),thewidespreaddeploymentofverylargelanguage
models(LLMs)remainshinderedbytheirsubstantialcom-
Theautoregressivenatureofconventionallarge
putationalcosts. Akeyfactorcontributingtohighinference
languagemodels(LLMs)inherentlylimitsinfer-
latencyistheautoregressivegenerationprocess,whereto-
encespeed,astokensaregeneratedsequentially.
kensareproducedsequentially. Thisinherentlimitationre-
While speculative (Leviathan et al., 2023) and
strictsthefullutilizationofMLaccelerators(GPUs/TPUs),
parallel(Sternetal.,2018)decodingtechniques
whichareoptimizedformatrix-matrixmultiplicationsrather
attempttomitigatethis,theyfacelimitations: ei-
thanthematrix-vectoroperationsprevalentinLLMs. Con-
therrelyingonlessaccuratesmallermodelsfor
sequently,promptprocessing(wherealltokensarehandled
generation or failing to fully leverage the base
simultaneously)issignificantlymoreefficientthanautore-
LLM’srepresentations.
gressiveresponsegeneration.
Weintroduceanovelarchitecture,Tandemtrans-
On the other hand, it is not well understood how much
formers, to address these issues. This architec-
capacityisrequiredtounderstandtheprompt/query/prefill
ture uniquely combines (1) a small autoregres-
(naturallanguageunderstandingakaNLU)vsthecapacity
sive model and (2) a large model operating in
requiredtogeneratearesponse(naturallanguagegeneration
blockmode(processingmultipletokenssimulta-
akaNLG).Currentdecoder-onlyLLMarchitecturestightly
neously). Thesmallmodel’spredictiveaccuracy
coupleboththesetasks.
issubstantiallyenhancedbygrantingitattention
tothelargemodel’sricherrepresentations.Onthe TandemTransformers. Inthiswork,weinvestigatethis
PaLM2pretrainingdataset,atandemofPaLM2- fundamentalquestionfromanefficiencyperspective. We
Bison and PaLM2-Gecko demonstrates a 3.3% proposeTandemTransformers,anovelarchitecturethatallo-
improvement in next-token prediction accuracy catessignificantlymoremodelcapacitytoprefillprocessing
overastandalonePaLM2-Gecko,offeringa1.16x (NLU)comparedtoresponsegeneration(NLG).Ourgoal
speedupcomparedtoaPaLM2-Ottermodelwith istounderstandwhetherhigh-qualityresponsegeneration
comparabledownstreamperformance. Wefurther canbemaintainedunderthisdesign. Concretely,Tandem
incorporatethetandemmodelwithinthespecu- transformersconsistsoftwomodels–asmallmodelM
S
lative decoding (SPEED) framework where the andalargemodelM ,where:
L
largemodelvalidatestokensfromthesmallmodel.
This ensures that the Tandem of PaLM2-Bison 1. M Lprocessestheprompt/query.
andPaLM2-Geckoachievessubstantialspeedup 2. M S generatesthefirstγ tokens(calledablock)autore-
(around1.14×fasterthanusingvanillaPaLM2- gressively,whileattendingtotheprompt/queryrepresen-
Gecko in SPEED) while maintaining identical tationsgeneratedbyM L.
downstreamtaskaccuracy. 3. M Lprocessestheγ tokensgeneratedbyM S together
(i.e.,inanon-autoregressivefashion)andcomputestheir
representations.
4. M thengeneratesthenextγ tokensautoregressively,
1.Introduction S
whileattendingtorepresentationsofalltokensuntilthe
Despitesignificantadvancementsininferenceoptimization previousprefillblockgeneratedbyM .
L
techniques(Leviathanetal.,2023;Duetal.,2022;Liuetal., 5. Thisprocessisrepeateduntiltheresponsegenerationis
complete.
*Equalcontribution 1GoogleResearch,India2GoogleDeep-
Mind 3Google Research, New York City. Correspondence to:
TandemTransformerTraining. Weintroduceaprojection
AishwaryaPS<aishwaryaps@google.com>,PraneethNetrapalli
layertoalignthepotentiallyhigher-dimensionalrepresen-
<pnetrapalli@google.com>.
tation space of M with that of M . For efficiency, we
L S
initializeM andM asindependentlytrained,standard
L S
1
4202
beF
31
]IA.sc[
1v44680.2042:viXraTandemTransformersforInferenceEfficientLLMs
decoder-onlymodels. (see benchmark numbers in Table 3). Furthermore, Tan-
dem+SPEEDisabletouserepresentationsofM while
ExperimentswithTandem(PaLM2-Bison,PaLM2-Gecko) L
still generating tokens autoregressively, which is able to
(wherePaLM2-Gecko<PaLM2-Otter<PaLM2-Bison,in
provideoverallmuchbettertradeoffintermsoftokenqual-
termsofmodelsize)demonstratethatthecapacityneeded
ityvsmodellatencyforthedrafter. Finally,recentworks
forNLUvsNLGaspectsofLLMscanindeedbedecoupled,
havealsoshowntheefficacyoflogitdistillationfortraining
leadingtoamoreefficientarchitecturewithoutsignificant
better drafter models within SPEED (Zhou et al., 2023).
accuracyloss. Evaluationonbenchmarkdatasetsshowthat
Ourapproachiscomplementary,andcanbecombinedwith
Tandem(PaLM2-Bison,PaLM2-Gecko)withblocklength
distillation.
γ = 3issubstantiallymoreaccuratethanPaLM2-Gecko,
andcomparabletoPaLM2-Otter,whileachievingapprox- Empirical Results for Tandem + SPEED. Finally, we
imately1.16×lowerinferencelatencythanPaLM2-Otter. conductextensivelatencyevaluationonTPUv5eforboth
Forexample,onSuperGLUE(Wangetal.,2019),thetan- standa alone and SPEED versions of Tandem (PaLM2-
dem model is 3% less accurate than PaLM2-Bison, 16% Bison, PaLM2-Gecko) with PaLM2-Bison and PaLM2-
moreaccuratethanPaLM2-Geckoand0.2%lessaccurate GeckobeingtheprimaryM andsecondaryM model,
L S
thanPaLM2-Otter,with1.16×speedupoverPaLM2-Otter. respectively. Inparticular,onmultipledatasets,weobserve
thatTandem+SPEEDwithdistillationcanbeatleast2.19×
Encoder-Decoder. In contrast to an encoder-decoder ar-
fasterthanthebaselinePaLM2-Bisonmodelwhileensuring
chitecturewhichwouldonlyprocessquery/prefixthrough
same output quality. Furthermore, compared to standard
anencoderandthengeneratetheentireresponsethrough
SPEED with M being secondary model, our model is
a decoder, Tandem is able to generate only block-size γ S
1.11×to1.17×faster. AnadaptiveblocklengthinSPEED
(say = 3) tokens through the secondary model M and
S furtherhelpsreduceTandem’slatencyby1.04×to1.09×
thenrefreshtheentireprefillrepresentationsusingprimary
onmultipledatasets. Finally,wedemonstratethatourre-
modelM whichiscriticaltomaintaininghighaccuracy.
L sultsalsoholdforpracticalsettingslikebatch-size>1.
Thatis,bysettingγ =0,Tandemcanmimicdecoder-only
M model while setting γ → ∞ leads to decoder-only Contributions. Insummary,followingarethekeycontribu-
L
M model. tionsofthework:
S
Tandem+SPEED.Forapplicationsrequiringoutputiden- 1. Tandemarchitecture: Anovelarchitecturetodisaggre-
ticaltotheprimarymodel,weproposeTandem+SPEED. gate prompt/prefill processing capacity from response
Thespeculativedecoding(SPEED)framework(Leviathan generation.
etal.,2023)leveragesthesmallmodelM inTandemto 2. Tandem+SPEED:Improvedspeculativedecodinglever-
S
generatedrafttokens,whicharethenverifiedbythelarge agingTandem’ssuperiordraftingforguaranteedoutput
modelM . Crucially,theabilityofM inTandemtoat- equivalencewithlowerlatency.
L S
tendtoM ’srepresentationssignificantlyimprovesdraft 3. AdaptiveBlockLength: EnhancesTandem+SPEEDby
L
quality,reducingverificationoverheadcomparedtostandard dynamicallyadjustingdraftedtokencount.
SPEED. For example, on the Reddit Posts dataset, using 4. TPUv5eevaluation: End-to-endevaluationonTPUv5e
theM inTandemasthedraftermodelinSPEEDleadsto withPaLM2-Bisonbeingtheprimarymodel. Adistilled
S
about11.24%higherper-blockacceptanceratecompared Tandem + SPEED is 2.4x faster compared to vanilla
toavanillasecondarymodel. Finally,weshowthatTandem PaLM2-Bisonmodeland1.11−1.17×fastercompared
transformerscanbefurtherimprovedusinglogitdistillation to distilled M + SPEED (Leviathan et al., 2023) ap-
S
andtheirefficacywithinSPEEDcanbeimprovedusingan pliedinthesamesetting.
adaptiveblocklengthparameter.
Contrast with Parallel Decoding and Distillation. Re-
Outlineofthepaper : Therestofthepaperisorganized
centlymultiplespeculativeorparalleldecodingstyletech-
as follows. We briefly review related work in Section 2.
niqueshavebeenproposedintheliterature(Leviathanetal.,
In Section 3, we present the main ideas and the design
2023;Kimetal.,2023;Sternetal.,2018). Thesetechniques
of Tandem transformers architecture. Section 4 presents
attempttogenerateadraftoftokensusingarelativelyinex-
theexperimentalresultsonTandemtransformers. Wethen
pensivedraftermodel. Paralleldecodingattemptstogener-
concludewithsomefuturedirectionsinSection6.
atemultipledraftertokensinparallelbylearningclassifiers
ontopofoutputofprimarymodelM whilespeculative
L
2.RelatedWork
decodingcouldprovidesignificantlybetterdraftsbyusing
a small, but auto regressive model. In contrast, Tandem
Encoder-Decodermodels :Encoder-decodertransformer
isastandalonemodelonitsownanddoesn’tnativelyre-
architecturesarewidelyusedforspecifictaskssuchasma-
quire verification by M to generate reasonable outputs
L chinetranslation(Vaswanietal.,2017). Giventhecomputa-
2TandemTransformersforInferenceEfficientLLMs
tionalinefficiencyofautoregressivedecoding,severalworks where x(0) = Emb(t ) is the embedding of t , x(j) is
i i i i
haveexploredusingalargeencoderwithasmalldecoder. the representation after the jth layer and Atn(j)(·|·) and
Our work can be seen as extending these ideas to use an FF(j)(·)arethejthattentionandfeedforwardlayersrespec-
encoder-decodermodelforthedecoderitself. tively(Vaswanietal.,2017).Notethattheattentionispurely
causal(i.e.,theith tokenattendsonlytokenst fork ≤ i)
k
Mixture of experts (MoE)/Sparsity based approaches sinceweareconsideringadecoder-onlytransformer.
: Mixture of experts (Du et al., 2022) and sparsity based
approaches(Lietal.,2022)havealsobeenstudiedforopti- Tandem transformer : A Tandem transformer model
mizinginferencecostofLLMs. Howevertheseapproaches comprisesofaprimarymodelM andasecondarymodel
L
arecomplementarytotheapproachesproposedinourpaper. M S. Typically, SIZEOF(M L) ≫ SIZEOF(M S). Given
Forexample,eitherorboththelargemodelM andsmall asequenceoftokenst ,t ,··· ,t asinputs, theprimary
L 1 2 S
modelM canbeanMoEorsparsemodel. modelM processesthesetokensjustlikeastandard(de-
S L
coder)transformer(1).
Distillation :Sincetheseminalpaper(Hintonetal.,2015), Let γ be the block length parameter, and L and L be
S L
distillingtheknowledgeofalargemodeltoasmallermodel thenumberoflayersofthesecondarymodelandprimary
byusingthelogitsoflargemodelasatrainingtargethas model,respectively. Letℓ:[L ]→[L ]bealayerassign-
S L
beenwidelyusedinseveralsettings. Ourworkcanbeseen mentfunctionfromsecondarymodeltoprimarymodel.The
asa moregeneral version ofdistillationfor transformers, secondarymodelattendstotheprimarymodel’srepresenta-
wherethesmallmodelcandirectlyrefertolargemodelrep- tionsforalltokensfromthepreviousblocks. Moreformally,
resentationsfortokensfrompreviousblocks. Furthermore, wehave:
ourexperiments(seeSection4)showthatourtechniquesare
complementarytologitdistillation,andprovideadditional y (cid:98)i(j) =FF( Tj an) dem(x i(ℓ(j)))
gainsontopofvanillalogitdistillation. i
y(j+1) =Atn(j+1)(y(j)|y(j),y(j) )wherek =⌊ ⌋∗γ
(cid:101)i S i (cid:98)≤k [k+1,i] γ
Speculative decoding (SPEED) : Speculative decod- y(j+1) =FF(j+1)(y(j+1)) forj =0,··· ,L −1,
i S (cid:101)i S
ing (Leviathan et al., 2023; Kim et al., 2023) is a frame- (2)
worktoreduceinferencelatencyofLLMswithoutaffecting
theirquality,whichhasshownsubstantialimprovementsin wherex(j) andy(j) denotethejth layerrepresentationof
i i
LLM inference. We demonstrate that Tandem transform- theith tokenunderM andM respectively,FF(j) (·)
erscanbeusedwithintheSPEEDframework,improving L S Tandem
denotesafeedforwardlayerthatconvertstherepresentation
theefficacyofSPEED.Whilemultipledraftershavebeen
x(ℓ(j)) oftheith tokenfromtheℓ(j)th layeroftheprimary
explored in the context of SPEED such as a stand alone i
model, to a representation y(j) of the same ith token for
model (Leviathan et al., 2023), retrieval based (He et al., (cid:98)i
2023),distillationbased(Zhouetal.,2023),asofnowdis- the jth layer of the secondary model, and Atn(j)(·|·) and
S
tillation based drafters seem to perform the best. As we FF(j)(·)denotetheattentionandfeedforwardblocksrespec-
S
demonstrateinSection4,Tandemisabletoprovidesignifi- tivelyinthejthlayerofthesecondarymodelM . Thefinal
S
cantlymorepowerfuldrafterthusprovidingbetterdraftof outputofthetandemmodelisy(LS). Wenotethatthepri-
tokensleadingtolowerlatency. maryandthesecondarymodelcanvaryinalmostallscale
parameterssuchasrepresentationdimensions, expansion
3.TandemTransformers factorsoffeedforwardlayers,numberofattentionheads,etc.
aswellaswhethertheattentionismulti-headormulti-query,
Inthissection,wewilldescribetandemtransformersarchi- etc. In all of our experiments, we take FF(·) (j) to be
Tandem
tecture,it’strainingandinference. linearprojectionlayers.
Standard (decoder) transformer : Given a sequence Training : Given a block length parameter γ, we parti-
t ,t ,··· ,t ofS tokensasinputs,wheret corresponds tionthetrainingsequenceintoblocks,eachconsistingofγ
1 2 S i
totheith tokenid,astandarddecodertransformerwithL consecutivetokens. Considertheautoregressiveprediction
layersexecutesasfollows: ofthejthtoken(forsomej ≤γ)withintheithblock. The
input to the secondary model M is the previous token.
S
x (cid:101)( ij+1) =Atn(j+1)(x( ij)|x( ≤j i)) Crucially,withintheattentionblocksofM S:
x( ij+1) =FF(j+1)(x (cid:101)( ij+1)) forj =0,··· ,L−1, • Key/valuepairsforalltokensuptothejthtokeninthe
(1)
currentblockarecomputedbyM itself.
S
3TandemTransformersforInferenceEfficientLLMs
Figure1.TrainingofTandemtransformerswithablocklengthγ =2.Atn(ℓ(j)+1)andFF(ℓ(j)+1)denotetheattentionandfeedforward
L L
blocksinthe(ℓ(j)+1)thlayerofM ,whileAtn(j+1)andFF(j+1)denotethoseof(j+1)thlayerofM .M processesthetokens
L L L S L
(cid:16) (cid:17)th
asastandarddecodertransformer.M ontheotherhandprocessesthetokensinthe i blockusingitsownrepresentationsy(j)and
S γ i
y(j),butwhileattendingtotherepresentationsofalltokensfromthepreviousblockfromthe(ℓ(j)+1)thlayerofM passedthrougha
i+1 L
feedforwardlayerFF(j) .
Tandem
• Key/valuepairsfortokensinpreviousblocksarecom- adistillationlossonitspredictions,usingthelogitsofthe
putedbytheprimarymodelM . Aprojection/tandem pretrainedM astargetswithCEloss.Thisalignsnaturally
L L
feedforwardlayerthenalignstherepresentationaldi- withtheTandemarchitecture,asM alreadyincorporates
S
mensionsfromM toM ,asdescribedinEquation representationsfromM .
L S L
(2).
TheTandem-Distilmodelfollowsatwostagetrainingsetup,
where initially it is trained to minimize the CE loss with
We explore multiple training configurations for Tandem
respecttothegroundtruthlabels,andinthesecondstagea
transformers:
weighingfactorofλ = 0.5isusedtobalancetheCEloss
• PrimaryModelFrozen: Onlythesecondarymodelpa- withrespecttogroundtruthlabelsandtheCElogitdistil-
rametersM andthetandemfeedforwardlayerFF(j)are lationlosswithrespecttotheoutputsofthePaLM2-Bison
S S
updated. Lossisappliedsolelytothesecondarymodel’s model. We note that Tandem-Distil in general performs
outputy(LS)(Equation(2)). betterthanTandem-CE.
• Both Models Trained, Loss on Secondary Outputs:
Similar to the above, loss is applied to the secondary
Inference. Theinferenceprocessbeginswiththeprimary
model’soutput. However,bothM andM ,alongwith
L S
model (M ) processing the prompt and generating rep-
FF(j)aretrained. L
S resentations for all prompt tokens. The secondary model
• BothModelsTrained,LossonBothOutputs: Thecom-
(M )thenautoregressivelygeneratesthefirstblockofγ
binedlossincorporatesboththeprimarymodel’soutputs S
response tokens. Crucially, M attends to the primary
x(LL)andthesecondarymodel’soutputsy(LS). S
model’srepresentations,alignedviatheprojectionlayer.
For training efficiency, we initialize the primary and sec-
Once the first response block is generated, the primary
ondary models with high quality pretrained checkpoints,
model (M ) processes these tokens and computes their
andthencontinuepretrainingthetandemarchitecturefora L
representations. Weconsidertwoinferenceconfigurations:
smallnumberofadditionalsteps. Inparticular,weusethe
pretrainedPaLM2-BisonandPaLM2-Geckocheckpoints • RepresentationGeneration+TokenPrediction(Figure
toinitializeM andM respectively. Inthissetting,we 2): M additionallypredictsthenexttoken.
L S L
foundthatPrimaryModelFrozenapproachprovidesthe • RepresentationGenerationOnly(AppendixB,Figure
best accuracy. Our Tandem-CE model is obtained by us- 4): M solelygeneratesrepresentationsfortheresponse
L
ingcrossentropy(CE)lossontheoutputofthesecondary block.
modelasdescribedabove.
In both configurations, the representations generated by
Tandem-Distil: TofurtherenhanceM ’squality,weapply M areusedbythesecondarymodel(M )togeneratethe
S L S
4TandemTransformersforInferenceEfficientLLMs
plains of India from the Tibetan
The Himalayas...mountain range separating the The Himalayas...separating the plains of India
Figure2.InferenceofTandemtransformerswithfreetokenfromtheprimarymodelM .(left)Firstblockprediction.(right)Second
L
blockprediction.GiventhequeryTheHimalayasareamountainrangeseparatingthe,M firstprocessesthisqueryandproducesthe
L
firstresponsetokenplains.WhenweusethispredictionfromM ,thisisdirectlyfedasaninputtothesecondarymodelM ,which
L S
autoregressivelyproducesofIndiaforthefirstblockwithγ =2.Inthesecondblock,theentireresponsefromthefirstblockplainsof
IndiaisfedtotheprimarymodelM ,whichagainproducesthenextresponsetokenfrom,andthenthesecondarymodelM produces
L S
thenexttwotokensoftheblocktheTibetanautoregressively.TheeventualoutputofthemodelwillbeplainsofIndiafromtheTibetan....
subsequentblockofγ responsetokens. Alsonotethat,as suitedforthisframework,withoursecondarymodelM
S
intraining, M attendstoits ownrepresentations forall actingasthe“drafter”andprimarymodelM actingasthe
S L
previoustokenswithinthecurrentblock. “verifier”.
To disaggregate query and response generation, we use Given a Tandem model, we use M to process the
L
RepresentationGenerationOnlyforprocessingtheinput query/prefix and generate representations for them. M
S
query/prefix. However,forsubsequentblockswherethepre- uses these and produces a draft for the first γ tokens au-
fill(query+generatedresponsetillthispoint)isprocessed, toregressively. M thenverifiesthisentireblocksimulta-
L
weuseRepresentationGeneration+TokenPrediction neously and identifies the first location i where the draft
fromM . tokenisdeemedincorrectbyM (i=γ+1,ifallthedraft
L L
tokensareverifiedsuccessfully). Wetaketheoutputofthe
Dependingonthetrainingprotocol–specifically,whether
largemodelfortheithtoken,andthesmallmodelM then
primary model outputs are reliable – we may optionally S
continuestogeneratedrafttokensfromthe(i+1)thposition
allowtheprimarymodel(M )togeneratethefirsttokenof
L onwards,whileusingtherepresentationsofalltheprevious
thesubsequentblock(processingγ+1tokens). Crucially,
tokensfromthelargemodelM . Thisprocesscontinues
inthisscenario,wemustensurethefollowing: thekeysand L
untilafullresponseisgenerated.
valuesassociatedwiththenextblock’sfirsttoken,computed
by M , are not overwritten when the secondary model Theaboveprocesscanbegeneralizedtothesetting,where
L
(M )executesitsattentionlayers. wegeneratemultiplefullresponsesforthesamequery,we
S
refertoitasnum-samples,forexampletoeventuallyrank
Inference-TimeBlockLengthFlexibility. Whilewetrain
theseresponsesandselectthe“best”response(Mudgaletal.,
Tandemtransformerswithafixedblocklengthγ,thearchi-
2023). Inthiscase,thelocationoftherejectedtokencan
tecturesupportsarbitraryγ valuesduringinference. Larger
varyacrossthedifferentsamplesbeinggenerated.
γ valuesgenerallyimproveefficiencybymaximizingthe
primarymodel’s(M )utilizationofacceleratorhardware. Similarly, the above approach generalizes to larger batch
L
AlthoughTandemistrainedwithafixedγ,inSPEEDeval- sizesaswell,whenwearesimultaneouslyprocessingmulti-
uationswefindthattheoptimalγ isoftenmuchlarger,indi- plequeriestogether. Practicalsystemspotentiallyuseboth
catingtherobustnessofTandemtochangesinγatinference num-samples and batch-size to be > 1 but latency gains
time. forTandem+SPEEDdependonoverallbatch-sizewhich
isnum-samples×batch size. So,forsimplicitywefocus
3.1.Tandem+SPEED:Tandeminthespeculative onlyonnum-samples>1andfixbatch-sizetobe11.
decodingframework
Adaptive Block Length: While standard SPEED uses a
SPEEDmitigatestheinefficiencyofautoregressivegener-
1Notethatitismorechallengingtoobtainlatencyimprove-
ationusingasmallerdrafter/secondarymodeltogenerate
ments with increasing num-samples, compared to that in batch
tokensandalargerverifier/primarymodeltoconfirmthem. sizesince,evenwithoutanyoftheseoptimizationssuchasSPEED
SPEEDguaranteesoutputqualitymatchingtheverifier,but etc.,largernum-samplesobtainbetterefficiencyonalllayerswhile
itsefficacyhingesonthedrafter’sabilitytogeneratelong, largerbatchsizeobtainsbetterefficiencyonlyonfeedforwardand
softmaxlayers,andnottheattentionlayer.
accuratedraftsequences. Tandemtransformersareuniquely
5TandemTransformersforInferenceEfficientLLMs
PaLM2- Tandem- Tandem- withintheSPEEDframework,weconsideralogitdistilla-
PaLM2-
Gecko- CE Distil tionversionofPaLM2-Gecko,calledPaLM2-Gecko-Distil,
Gecko
Distil (ours) (ours) whichisinitializedwiththePaLM2-Geckomodelandthen
Accuracy trainedusinglogitdistillation,similartothesecondphase
(ground 55.06 56.50 58.35 58.61 of training of the Tandem-Distil model, since distillation
truth) hasbeenshowntohelpimprovethesecondarymodelsin
CE loss SPEED(Zhouetal.,2023).
(ground 2.14 2.12 1.94 1.99
AdaptiveblocklengthinSPEED.Wetrainasmall,2-layer
truth)
MLPmodeltopredictwhetherthecurrentdraftertokenfrom
Relative ac-
74.64 75.30 80.00 81.00 M S islikelytobeacceptedbyprimarymodelM L. Weset
curacy
τ =0.8asthethresholdtodetermineifM cancontinue
S
Relative TV
0.391 0.318 0.178 0.141 generatingmoretokens.
distance
4.1.PerformanceEvaluation
Table1.Accuracyandcrossentropy(CE)lossofTandemtrans-
formerswithrespecttogroundtruthlabelsaswellasthepredic- WecomparetheperformanceofTandem-CEandTandem-
tionsoftheprimarymodelM ,PaLM2-Bison.Asisclearfrom DistilagainstPaLM2-Gecko,PaLM2-Gecko-Distil,PaLM2-
L
theresults,theTandemmodelofPaLM2-GeckoandPaLM2-Bison OtterandPaLM2-Bisononseveraldownstreamtasksaswell
substantiallyoutperformsthestandalonePaLM2-Geckomodel. asintermsoflatency.
For downstream task evaluation, we compare on Super-
GLUE(Wangetal.,2019),TydiQA(Clarketal.,2020),a
fixed block length γ, we introduce an adaptive approach.
largecollectionofgenerationtasks,whichwecallGen-tasks
Wetrainarelativelysmall2-layermulti-layerperceptron
(comprisingofSQuADv2(Rajpurkaretal.,2018),Natural
–routerMLP–modeltopredictwhetherthecurrentdraft
Questions(Kwiatkowskietal.,2019),TriviaQA(Joshietal.,
token from M is likely to be accepted by the primary
S 2017), WebQuestions (Berant et al., 2013) and Lambada
modelM . Ateachtimestep,wecomparetheprediction
L (Paperno et al., 2016)), MBPP (Austin et al., 2021), and
ofthissmallmodeltoathresholdτ,decidingwhetherto: a.
WMT22(Zervaetal.,2022). WMT22resultsareaveraged
VerifywithM ,orb. ContinuedraftingwithM .
L S over x → en translations for different languages x. For
InputfeaturestotherouterMLPare:M ’sentropyoverthe TydiQA,wepassthegoldpassageaspartoftheinput,and
S
currenttoken’svocabularydistribution,top-kprobabilities reporttheaverageF1-scoreoveralllanguages. ForSuper-
forthecurrenttokenforanappropriatek,andM ’smodel GLUEandGen-tasks,wefollowtheexperimentalsettings
S
embeddings corresponding to these top-k most probable as described in (Anil et al., 2023) and report the average
tokens. WetraintherouterMLPtopredicttheprobability results. We report 1-shot evaluations for all performance
ofdisagreementusingcross-entropyloss,withgroundtruth evaluationexperiments.
being: TV(yS,yP), whereTV(yS,yP)isthetotalvaria-
j j j j
tion (TV) distance between the output logits of M and 4.2.LatencyEvaluation
S
M forthejthtoken.
L We perform latency evaluation in two different settings.
Inthefirstsetting,weuseTandem-CEandTandem-Distil
4.Experiments as secondary models within SPEED, with PaLM2-Bison
as the primary model. Note that the SPEED framework
Inthissection,wepresentexperimentalresultsevaluating
guarantees that the outputs will be of the same quality
Tandemtransformermodels. Exceptforthenewarchitec-
as the primary model PaLM2-Bison. For comparison,
tureofTandemtransformers,wegenerallyfollowthesame
we use PaLM2-Bison as a stand alone model, as well as
trainingprotocolsasdescribedin(Aniletal.,2023),includ-
SPEEDwithPaLM2-BisonasprimaryandPaLM2-Gecko-
ingthetrainingdataset,optimizer,etc.
Distilassecondaryasourbaselines. Inthesecondsetting,
Further Training Details. For both Tandem-CE and weevaluatethelatencyofTandem-CEandTandem-Distil
Tandem-Distil, we initialize the secondary model M to asstandalonemodelswithPaLM2-Gecko, PaLM2-Otter
S
be the pretrained PaLM2-Gecko, while freezing primary and PaLM2-Bison. All the evaluations are performed on
modelM tobethepretrainedPaLM2-Bison(Aniletal., TPUv5e(Cloud).
L
2023). Theprojection/Tandemfeedforwardlayersarecho-
WeevaluatelatencyonthetestsetsofCNNDailyMail(Her-
sen to be linear layers and initialized randomly. Both
mannetal.,2015),andRedditPostssummarization(Kim
theTandemmodels–Tandem-CEandTandem-Distil–are
et al., 2018), and 1000 prompts from the 1 Billion Word
trained with a block length of γ = 2. For our evaluation
6TandemTransformersforInferenceEfficientLLMs
PaLM2-Gecko-
Tandem-Distil Tandem-Distil
Dataset Num- Distil
(ours; relative
Samples (baseline) (ours)
gain)
1 × 2.169(γ =7) 2.471× (γ =7) 1.139×
Reddit
4 × 1.919(γ =5) 2.234× (γ =7) 1.164×
1 × 2.219(γ =7) 2.473× (γ =7) 1.115×
CNN/DailyMail
4 × 1.940(γ =5) 2.190× (γ =7) 1.129×
1 × 2.348(γ =7) 2.610× (γ =7) 1.112×
LM1B
4 × 2.011(γ =5) 2.359× (γ =7) 1.173×
Table2.End-to-endlatencygainofvarioussecondarymodels,whenusedwithintheSPEEDframeworkwithPaLM2-Bisonastheprimary
model. Thesecondarymodelsweconsiderare: PaLM2-Gecko-DistilandTandem-Distil. SinceTandem-Distilhasbetteracceptance
ratecomparedtoPaLM2-Gecko-Distil,e.g.,forγ = 5,Tandem-Distilhas,onaverage,11.24%moretokensacceptedcomparedto
PaLM2-Gecko-Distil,foreachsecondarymodel,andoneachdataset,weusetheoptimalblocklengthγparameter.Weconsidertwo
settings,onewherewegenerateasingleresponseandanotherwherewegenerate4responsesforthegivenquery.Thethirdandfourth
columnprovidethespeedupbyusingPaLM2-Gecko-DistilandTandemmodelsrespectively,withrespecttothePaLM2-Bisonmodel.
ThelastcolumnindicatestherelativegainofusingtheTandemmodelasthesecondarymodelinSPEED,insteadofPaLM2-Gecko-Distil.
TheresultsclearlydemonstratetheadditionalimprovementsTandemobtains,ontopoflogitdistillation.
Tandem-Tandem- 4.3.EvaluationResults
PaLM2- PaLM2- PaLM2-
Dataset CE Distil
Gecko Otter Bison Wenowpresentresultsofourevaluationoftandemtrans-
(ours) (ours)
formers.
Generative-
28.8 37.1 44.0 51.1 57.5
tasks
Pretrainingmetrics : Table1presentsacomparisonof
MBPP 4.8 13.8 21.2 20.8 30.4
accuracyandcrossentropy(CE)lossofvariousbaselinesas
WMT22- wellastandemmodels,withrespecttoboththegroundtruth
1shot-to- 35.1 37.4 44.1 48.4 50.5 labelsaswellastheprimarymodelM ’spredictions. As
L
nonenglish wecansee,tandemtransformersperformsbetterthanlogit
TydiQA- distillation,whilecombininglogitdistillationwithtandem
55.0 65.7 69.0 69.7 73.4
GoldP transformers,furtherimprovesitsperformance.
Super-
62.8 78.5 78.8 79.0 81.5
GLUE
LatencywithinSPEED :Table2presentsresultsonthe
Speedup latencyofTandemtransformerswithintheSPEEDframe-
over 6.397× 2.744× 2.744× 2.359× 1× work. Specifically,wecomparethespeedupobtainedover
PaLM2- the PaLM2-Bison model, by using SPEED with PaLM2-
Bison Gecko-DistilasthesecondarymodelvsTandem-Distilas
thesecondarymodel. Theresultsclearlydemonstratethe
Table3.StandaloneevaluationoftheTandemmodel.Thefirstfive improvementsobtainedbytandemontopofdistillation. Ta-
rowspresentdownstreamevaluationsoftheTandemtransformers ble8inAppendixApresentsthespeedupscomputedonly
on a variety of generative and ranking tasks. We see that the overthedecodetime(i.e.,excludingthequeryprocessing
Tandemmodelsubstantiallyimprovesupontheperformanceof time). Note that since the SPEED framework guarantees
standalonePaLM2-Geckomodel,andisonparwiththePaLM2-
thattheoutputsareofsamequalityasthoseoftheprimary
Ottermodel.Ontheotherhand,thelatencyevaluationsinthelast
model,PaLM2-Bison,thelatencyimprovementsgivenby
rowdemonstratethattheTandemmodelisabout1.16xfasterthan
thetandemmodeldonothaveanyqualitytradeoffs.
thePaLM2-Ottermodel.
Evaluation as a standalone model : We evaluate the
Tandemmodelasastandalonemodelinitsownright. Ta-
ble3presentsacomparisonofbothdownstreamevaluations
on standard downstream benchmarks, as well as latency
Benchmark(Chelbaetal.,2014). Wereportlatencyresults evaluations. As can be seen, the Tandem model substan-
forbothnum-samples=1aswellas4. tially improves upon the downstream performance of the
7TandemTransformersforInferenceEfficientLLMs
speedup over Tandem-
speedup over PaLM- Tandem-
Dataset Tandem-Distil Distil +
Bison Distil
+SPEED AG
Reddit 2.582× (γ =17) 1.045× Primarymodelruns 51.53 54.67
max
CNN/ 2.599× (γ max =17) 1.051× Secondary model 360.73 271.63
DailyMail runs
LM1B 2.853× (γ =27) 1.093×
max
Table5.PrimarymodelandsecondarymodelrunsforTandem-
Table4.End-to-endlatencyspeedupobtainedbyTandem-Distil+
DistilandTandem-Distil+AGontheLM1Bbenchmark. Note
SPEED+Adaptiveγondifferentevaluationdatasets.Thesecond
thattheseresultsareobtainedfornum-samples=1. Wecansee
andthirdcolumnsshowthespeedupoverthestandalonePaLM2-
thatthenumberofsecondarymodelrunshavecomedownby90
BisonmodelandTandem-Distil+SPEEDmodelrespectively.The
whereasthenumberoflargemodelrunshasgoneuponlyby3.
latencyisevaluatedforgeneratingasingleresponse. Adaptive
The results clearly showcase that an adaptive block length can
γ enables us to use much larger block lengths without losing
significantlycutdownonthenumberofsecondarymodelrunsand
performance. Forexample,ontheRedditdataset,theoptimalγ
givenon-triviallatencygains.
forthetandemmodelinthestandardSPEEDsetupis7, while
adaptiveγobtainsbetterresultswithγ =17.
max
asketchofthenextblockoftokensinparallel,whileM
S
doestheactualsamplinginanautoregressivemanner. More
baselinemodel,andisalmostonparwiththePaLM2-Otter concretely,wehave:
model. Detailed results presented in Tables 10 and 11 in
i−γ
AppendixAshowthat,insomecases,thetandemmodelis x(j+1) =Atn(j+1)(x(j)|x(j) )wherek =⌈ ⌉
closertothePaLM2-Bisonmodelitself. Atthesametime, (cid:101)i L i ≤k∗γ γ
thetandemmodelisabout1.16xtimesfastercomparedto x(j+1) =FF(j+1)(x(j+1)) forj =0,··· ,L −1,
i L (cid:101)i L
thePaLM2-Ottermodel,makingitacompellingcandidate (3)
forstandalonedeploymentaswell.
andx(0) =Emb (x[i−γ])isgivenbythelargemodel’s
i L
(cid:16) (cid:17)th
Adaptive block length : We now present a way to im- embedding of the ⌈i−γ⌉∗γ token, where the large
prove the performance of SPEED with adaptive block γ
lengths (Adaptive γ or AG), where after every token pre- model,givenalltokensx( 10),··· ,x( s0 ∗) γ,producesadraftof
dictedbythesecondarymodel,weuseasmall,inexpensive thenextγ tokensx(LL) ,··· ,x(LL) . Wethenaddthe
k∗γ+1 (k+1)∗γ
router to determine whether to continue predicting with previoustokenrepresentationstothesesketchesandthen
thesecondarymodel,orverifythetokensgeneratedsofar pass it through the small model, which predicts the next
withtheprimarymodel. Table4presentsthespeedupob- tokenautoregressively:
tainedbyTandem-Distil+SPEED+AGcomparedwiththe
PaLM2-BisonmodelaswellastheTandem-Distil+SPEED y i(0) =Emb S(x[i−1])+FF Tandem(x( iLL))
model. Table9inAppendixApresentsthespeedupasmea- y(j+1) =Atn(j+1)(y(j)|y(j))
suredonlyoverthedecodecomponentofthelatencyi.e., (cid:101)i S i ≤i
excludingqueryprocessingtime. y i(j+1) =FF( Sj+1)(y (cid:101)i(j+1)) forj =0,··· ,L S −1.
(4)
InTable5,wepresentthenumberofprimarymodel,and
secondary model runs for Tandem-Distil + SPEED and Theeventualoutputofthemodelisy(LS)whichisitspredic-
Tandem-Distil + SPEED + Adaptive γ. The results put tionoftheithtokenintheinputsequei
nce. Thisispictorially
forththebenefitsofusinganadaptiveblocklength,since
depictedinFigure3.
itdrasticallyreducesthenumberofsecondarymodelruns
whileslightlyincreasingthenumberofprimarymodelruns.
5.1.Experimentalresultsfordeeptandemtransformers
Inthissection,wepresentpreliminaryexperimentalresults
5.DeepTandemTransformers ondeeptandemtransformerscomparedwiththestandardar-
chitecture.Forthissection,weconsidertheLaMDAmodels
In tandem transformers, we used the large model M to alongwiththetrainingprotocolasdescribedin(Thoppilan
L
processtokensinblocks,sothatthesmallmodelM can etal.,2022). Inparticular,weconsiderthe1Bparameter
S
use large model’s representations for all the tokens from modelfromtheLaMDAfamilyandconstructadeeptandem
previousblocks. Inthissection,wepresentadifferentap- versionofitbysplittingthe16layersequallybetweenM
L
proachtouseM andM intandem,whereM predicts and M (so each of them has 8 layers), and with block
L S L S
8TandemTransformersforInferenceEfficientLLMs
8-Blockpre-
Autoregressive
diction
2.71 5.55
Table7.Pretraininglogperplexityofanautoregressivemodelcom-
paredtoablockpredictionmodelwithblocklengthγ =8.Both
modelsaretakentohavethesamearchitectureasLaMDA-1B,ex-
ceptforthedifferencebetweenblockpredictionandautoregressive
prediction.
6.ConclusionsandDiscussion
In this work, we introduce a novel architecture, Tandem
transformers,whichcombinesasmallautoregressivemodel
withalargemodeloperatinginblockmode. Tandemtrans-
formers substantially boost the small model’s predictive
accuracy by allowing it to attend to representations from
thelargemodel. Inourexperiments,aTandemmodelcom-
prisingofPaLM2-BisonandPaLM2-Geckosubstantially
improvesoverastandalonePaLM2-Gecko,andgivescom-
Figure3.ThearchitectureofDeepTandemtransformerswitha
parableperformancetothePaLM2-Ottermodel,whilebe-
blocklengthofγ.SeetextandEquations(3)and(4)fordescrip-
ing1.16×fasterthanthePaLM2-Ottermodel. Whenused
tion.
withintheSPEEDsetupasasecondarymodel,thedistilled
TandemPaLM2-Geckomodelgivesaround1.14×speedup
overadistilledPaLM2-Geckomodel. Wefurtherimprove
length γ = 2. The results, presented in Table 6 suggest our Tandem model through an adaptiveblock length pro-
that we suffer minimal loss by letting the autoregressive cedureinSPEEDandobtainaround1.22×speedupover
componentbeonlyapartoftheentiremodel. usingPaLM2-Gecko-Distilasthesecondarymodel.
LimitationsandFuturedirections
Topline Tandem
Accuracy 45.1 43.3 • Othervariantsoftandem: Inourcurrentapproach,
weusethelargemodelonlythroughitsrepresentations
CEloss 2.71 2.85
ofthepasttokens. Isitpossibletousethelargemodel
Speedup
1× 1.25× toalsogenerateaplanforthefutureγtokensalongthe
estimate
linesofdeeptandemtransformers?
Table6.AccuracyandCElossoftandemmodelwithrespectto • AlternativetoLoRAforfinetuning: Thecurrentap-
groundtruthlabelsonthepretrainingdata. proachforfinetuningabasemodelformultipledown-
stream applications is through low rank adaptation
(LoRA) (Hu et al., 2021). It will be interesting to
explore whether tandem with block length 0 can be
5.2.Importanceofthesmallautoregressivecomponent aneffectivealternativetoLoRA,whilereducingthe
trainingcostsubstantiallysincebackpropagationneeds
Inthissectionwepresentthelogperplexityachievedbya
tobedoneonlyforthesmallmodel.
blockpredictionmodelsimilarto(Sternetal.,2018),where
wepredictthenextblockofγ =8tokenssimultaneously. • Adaptive γ for larger num-samples/batch-size:
In other words, we directly train the output x(LL) of the While we see promising results with adaptive γ in
i
large model in Equation (3) to predict the ith token x[i]. SPEEDfornumsamples1,extendingittolargernum
TheCElossoftheresultingmodel,anditscomparisonwith samples seems challenging. Identifying an effective
a fully autoregressive model is presented in Table 7. As wayofdeterminingwhentocontinuegeneratingwith
wecansee,thecrossentropylossofsuchamodelismuch smallmodelvsverifyingwithlargemodel,inthelarger
highercomparedtothatoftheoriginalmodel,whichisfully numsamplessetting,isalsoaninterestingdirectionof
autoregressive. futurework.
9TandemTransformersforInferenceEfficientLLMs
• SmallerdraftermodelsinSPEED:Finally,wehope odsinNaturalLanguageProcessing,EMNLP2013,18-
thattandemcanenableusingevensmallerdraftermod- 21 October 2013, Grand Hyatt Seattle, Seattle, Wash-
elsinSPEED,comparedtotheonescurrentlybeing ington, USA, A meeting of SIGDAT, a Special Interest
pursued, leading to both memory as well as latency Group of the ACL, pp. 1533–1544. ACL, 2013. URL
improvements. https://aclanthology.org/D13-1160/.
Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T.,
7.BroaderImpactStatement
Koehn, P., and Robinson, T. One billion word bench-
markformeasuringprogressinstatisticallanguagemod-
Ourworkprovidesamorecomputationallyefficientlarge
eling. In Li, H., Meng, H. M., Ma, B., Chng, E., and
languagemodelinferencesolution,whichwehopecanbring
Xie,L.(eds.),INTERSPEECH2014,15thAnnualCon-
downcarbonemissionsassociatedwithLLMinference. It
ferenceoftheInternationalSpeechCommunicationAsso-
alsohelpswitheasierdeploymentofLLMs,whichcould
ciation, Singapore, September 14-18, 2014, pp. 2635–
havepotentialsocietalconsequences,thatseemdifficultto
2639. ISCA, 2014. doi: 10.21437/INTERSPEECH.
predict.
2014-564. URL https://doi.org/10.21437/
Interspeech.2014-564.
References
Clark,J.,Choi,E.,Collins,M.,Garrette,D.,Kwiatkowski,
Anil,R.,Dai,A.M.,Firat,O.,Johnson,M.,Lepikhin,D.,
T., Nikolaev, V., and Palomaki, J. Tydi qa: A bench-
Passos,A.,Shakeri,S.,Taropa,E.,Bailey,P.,Chen,Z.,
mark for information-seeking question answering in
Chu, E., Clark, J. H., Shafey, L. E., Huang, Y., Meier-
typologically diverse languages. Transactions of the
Hellstern, K., Mishra, G., Moreira, E., Omernick, M.,
Association for Computational Linguistics, 8:454–470,
Robinson,K.,Ruder,S.,Tay,Y.,Xiao,K.,Xu,Y.,Zhang,
2020. URL https://api.semanticscholar.
Y.,Abrego,G.H.,Ahn,J.,Austin,J.,Barham,P.,Botha,
org/CorpusID:212657414.
J., Bradbury, J., Brahma, S., Brooks, K., Catasta, M.,
Cheng,Y.,Cherry,C.,Choquette-Choo,C.A.,Chowd- Cloud, G. Cloud tpu v5e inference. URL
hery, A., Crepy, C., Dave, S., Dehghani, M., Dev, S., https://cloud.google.com/tpu/docs/
Devlin,J.,D´ıaz,M.,Du,N.,Dyer,E.,Feinberg,V.,Feng, v5e-inference. AccessedonFeb1,2024.
F., Fienber, V., Freitag, M., Garcia, X., Gehrmann, S.,
Gonzalez,L.,Gur-Ari,G.,Hand,S.,Hashemi,H.,Hou, Du,N.,Huang,Y.,Dai,A.M.,Tong,S.,Lepikhin,D.,Xu,
L.,Howland,J.,Hu,A.,Hui,J.,Hurwitz,J.,Isard,M.,It- Y.,Krikun,M.,Zhou,Y.,Yu,A.W.,Firat,O.,etal. Glam:
tycheriah,A.,Jagielski,M.,Jia,W.,Kenealy,K.,Krikun, Efficient scaling of language models with mixture-of-
M.,Kudugunta,S.,Lan,C.,Lee,K.,Lee,B.,Li,E.,Li, experts. InInternationalConferenceonMachineLearn-
M.,Li,W.,Li,Y.,Li,J.,Lim,H.,Lin,H.,Liu,Z.,Liu, ing,pp.5547–5569.PMLR,2022.
F.,Maggioni,M.,Mahendru,A.,Maynez,J.,Misra,V.,
He, Z., Zhong, Z., Cai, T., Lee, J. D., and He, D. Rest:
Moussalem,M.,Nado,Z.,Nham,J.,Ni,E.,Nystrom,A.,
Retrieval-based speculative decoding. arXiv preprint
Parrish,A.,Pellat,M.,Polacek,M.,Polozov,A.,Pope,
arXiv:2311.08252,2023.
R.,Qiao,S.,Reif,E.,Richter,B.,Riley,P.,Ros,A.C.,
Roy, A., Saeta, B., Samuel, R., Shelby, R., Slone, A.,
Hermann, K. M., Kocisky, T., Grefenstette, E., Espeholt,
Smilkov,D.,So,D.R.,Sohn,D.,Tokumine,S.,Valter,
L.,Kay,W.,Suleyman,M.,andBlunsom,P. Teaching
D., Vasudevan, V., Vodrahalli, K., Wang, X., Wang, P.,
machinestoreadandcomprehend. InAdvancesinneural
Wang,Z.,Wang,T.,Wieting,J.,Wu,Y.,Xu,K.,Xu,Y.,
informationprocessingsystems,pp.1693–1701,2015.
Xue, L., Yin, P., Yu, J., Zhang, Q., Zheng, S., Zheng,
C.,Zhou,W.,Zhou,D.,Petrov,S.,andWu,Y. Palm2 Hinton, G., Vinyals, O., and Dean, J. Distilling
technicalreport,2023. the knowledge in a neural network. arXiv preprint
arXiv:1503.02531,2015.
Austin,J.,Odena,A.,Nye,M.,Bosma,M.,Michalewski,
H., Dohan, D., Jiang, E., Cai, C. J., Terry, M., Hu,E.J.,Shen,Y.,Wallis,P.,Allen-Zhu,Z.,Li,Y.,Wang,
Le, Q. V., and Sutton, C. Program synthesis S.,Wang,L.,andChen,W. Lora:Low-rankadaptationof
with large language models. ArXiv, abs/2108.07732, largelanguagemodels. arXivpreprintarXiv:2106.09685,
2021. URL https://api.semanticscholar. 2021.
org/CorpusID:237142385.
Joshi,M.,Choi,E.,Weld,D.S.,andZettlemoyer,L. Trivi-
Berant, J., Chou, A., Frostig, R., and Liang, P. Seman- aqa: Alargescaledistantlysupervisedchallengedataset
ticparsingonfreebasefromquestion-answerpairs. In forreadingcomprehension. InBarzilay,R.andKan,M.
Proceedingsofthe2013ConferenceonEmpiricalMeth- (eds.), Proceedings of the 55th Annual Meeting of the
10TandemTransformersforInferenceEfficientLLMs
Association for Computational Linguistics, ACL 2017, 10.18653/V1/P16-1144. URL https://doi.org/
Vancouver,Canada,July30-August4,Volume1: Long 10.18653/v1/p16-1144.
Papers,pp.1601–1611.AssociationforComputational
Rajpurkar,P.,Jia,R.,andLiang,P. Knowwhatyoudon’t
Linguistics, 2017. doi: 10.18653/V1/P17-1147. URL
know: Unanswerablequestionsforsquad. InGurevych,
https://doi.org/10.18653/v1/P17-1147.
I.andMiyao,Y.(eds.),Proceedingsofthe56thAnnual
Kim,B.,Kim,H.,andKim,G. Abstractivesummarization Meeting of the Association for Computational Linguis-
ofredditpostswithmulti-levelmemorynetworks. CoRR, tics,ACL2018,Melbourne,Australia,July15-20,2018,
abs/1811.00783, 2018. URL http://arxiv.org/ Volume 2: Short Papers, pp. 784–789. Association for
abs/1811.00783. Computational Linguistics, 2018. doi: 10.18653/V1/
P18-2124. URL https://aclanthology.org/
Kim, S., Mangalam, K., Moon, S., Malik, J., Mahoney,
P18-2124/.
M.W.,Gholami,A.,andKeutzer,K. Speculativedecod-
ingwithbiglittledecoder. InThirty-seventhConference Stern,M.,Shazeer,N.,andUszkoreit,J. Blockwiseparallel
onNeuralInformationProcessingSystems,2023. decodingfordeepautoregressivemodels. Advancesin
NeuralInformationProcessingSystems,31,2018.
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M.,
Parikh, A. P., Alberti, C., Epstein, D., Polosukhin, I., Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul-
Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, shreshtha,A.,Cheng,H.-T.,Jin,A.,Bos,T.,Baker,L.,
M., Chang, M., Dai, A. M., Uszkoreit, J., Le, Q., and Du,Y.,etal. Lamda: Languagemodelsfordialogappli-
Petrov,S. Naturalquestions: abenchmarkforquestion cations. arXivpreprintarXiv:2201.08239,2022.
answeringresearch. Trans.Assoc.Comput.Linguistics,
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
7:452–466,2019. doi: 10.1162/TACL\ A\ 00276. URL
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. At-
https://doi.org/10.1162/tacl_a_00276.
tentionisallyouneed. Advancesinneuralinformation
Leviathan,Y.,Kalman,M.,andMatias,Y. Fastinference processingsystems,30,2017.
from transformers via speculative decoding. In Inter-
Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A.,
nationalConferenceonMachineLearning,pp.19274–
Michael, J.,Hill, F.,Levy, O., andBowman, S. Super-
19286.PMLR,2023.
glue: Astickierbenchmarkforgeneral-purposelanguage
Li,Z.,You,C.,Bhojanapalli,S.,Li,D.,Rawat,A.S.,Reddi, understandingsystems. Advancesinneuralinformation
S. J., Ye, K., Chern, F., Yu, F., Guo, R., et al. The processingsystems,32,2019.
lazy neuron phenomenon: On emergence of activation
Zerva, C., Blain, F., Rei, R., Lertvittayakumjorn, P.,
sparsityintransformers. InTheEleventhInternational
DeSouza,J.G.,Eger,S.,Kanojia,D.,Alves,D.,Oraˇsan,
ConferenceonLearningRepresentations,2022.
C.,Fomicheva,M.,etal.Findingsofthewmt2022shared
Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z., taskonqualityestimation. InProceedingsoftheSeventh
Shrivastava,A.,Zhang,C.,Tian,Y.,Re,C.,etal. Deja ConferenceonMachineTranslation(WMT),pp.69–99,
vu:Contextualsparsityforefficientllmsatinferencetime. 2022.
In International Conference on Machine Learning, pp.
Zhou, Y., Lyu, K., Rawat, A. S., Menon, A. K., Ros-
22137–22176.PMLR,2023.
tamizadeh,A.,Kumar,S.,Kagy,J.-F.,andAgarwal,R.
Mudgal,S.,Lee,J.,Ganapathy,H.,Li,Y.,Wang,T.,Huang, Distillspec: Improvingspeculativedecodingviaknowl-
Y.,Chen,Z.,Cheng,H.,Collins,M.,Strohman,T.,Chen, edgedistillation. arXivpreprintarXiv:2310.08461,2023.
J.,Beutel,A.,andBeirami,A. Controlleddecodingfrom
language models. CoRR, abs/2310.17022, 2023. doi:
10.48550/ARXIV.2310.17022. URL https://doi.
org/10.48550/arXiv.2310.17022.
Paperno,D.,Kruszewski,G.,Lazaridou,A.,Pham,Q.N.,
Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and
Ferna´ndez, R. The LAMBADA dataset: Word pre-
diction requiring a broad discourse context. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL 2016, August
7-12, 2016, Berlin, Germany, Volume 1: Long Papers.
The Association for Computer Linguistics, 2016. doi:
11TandemTransformersforInferenceEfficientLLMs
A.AdditionalResults
Inthissection,wewillpresentadditionalexperimentalresults.
A.1.DecodeTimeResults
InTables8and9,wecomparethedecodetimeresults(i.e. end-to-endtime−timerequiredtoprocesstheinputprefix)of
ourTandemmodelanditsAdaptiveγ variantwiththebaselines.
PaLM2-Gecko-
Tandem-Distil Tandem-Distil
Dataset Num- Distil
(ours; relative
Samples (baseline) (ours)
gain)
1 2.356× (γ =7) 2.737× (γ =7) 1.162×
Reddit
4 2.042× (γ =5) 2.425× (γ =7) 1.188×
1 2.418× (γ =7) 2.740× (γ =7) 1.133×
CNN/DailyMail
4 2.066× (γ =5) 2.369× (γ =7) 1.146×
1 2.460× (γ =7) 2.756× (γ =7) 1.120×
LM1B
4 2.080× (γ =5) 2.466× (γ =7) 1.186×
Table8.Decode-time-onlylatencygainofvarioussecondarymodels,whenusedwithintheSPEEDframeworkwithPaLM2-Bisonasthe
primarymodel.Thesecondarymodelsweconsiderare:PaLM2-Gecko-DistilandTandem-Distil.Foreachsecondarymodel,andoneach
dataset,weusetheoptimalblocklengthγparameter.Weconsidertwosettings,onewherewegenerateasingleresponseandanother
wherewegenerate4responsesforthegivenquery.ThethirdandfourthcolumnprovidethespeedupbyusingPaLM2-Gecko-Distiland
tandemmodelsrespectively,withrespecttothePaLM2-Bisonmodel.ThelastcolumnindicatestherelativegainofusingtheTandem
modelasthesecondarymodelinSPEED,insteadofPaLM2-Gecko-Distil.Theresultsclearlydemonstratetheadditionalimprovements
Tandemobtains,ontopoflogitdistillation.
speedup over PaLM- speedup over Tandem-
Dataset
Bison Distil+SPEED
Reddit 2.885× (γ =17) 1.054×
max
CNN/ (γ =17)
2.908× max 1.061×
DailyMail
LM1B 3.040× (γ =27) 1.103×
max
Table9.Decode-time-onlylatencyspeedupobtainedbyTandem-Distil+SPEED+Adaptiveγ ondifferentevaluationdatasets. The
secondandthirdcolumnsshowthespeedupoverthestandalonePaLM2-BisonmodelandTandem-Distil+SPEEDmodelrespectively.
The latency is evaluated for generating a single response. Adaptive γ enables us to use much larger block lengths without losing
performance.Forexample,ontheRedditdataset,theoptimalγforthetandemmodelinthestandardSPEEDsetupis7,whileadaptiveγ
obtainsbetterresultswithγ =17.
max
A.2.DetailedPerformanceEvaluationResults
InTable10,wepresentresultsforourTandemmodelandthecomparedbaselinesoneachindividualtaskinGenerative-tasks.
Likewise,inTable11wepresentresultsoneachindividualtaskinSuperGLUE.
B.InferenceofTandemTransformers
Figure4presentstheinferenceforTandemtransformerswithoutthethefreetokenfromtheprimarymodelM .
L
12TandemTransformersforInferenceEfficientLLMs
Tandem- Tandem-
PaLM2- PaLM2- PaLM2-
Dataset CE Distil
Gecko Otter Bison
(ours) (ours)
Lambada(acc=Ac-
45.5 59.2 68.3 78.9 82.9
curacy)
NaturalQuestions
7.7 9.9 14.4 19.9 28.1
(em=ExactMatch)
SQuADv2(em) 45.3 67.8 70.2 70.3 75.4
TriviaQA(em) 36.8 36.9 51.2 68.9 77.3
WebQuestions(em) 9.0 12.0 16.0 17.6 23.8
Table10.EvaluationoftheTandemmodeloneachoftheGenerative-tasks.WeseethattheTandemmodelsubstantiallyimprovesupon
theperformanceofstandalonePaLM2-Geckomodel,andonmostdatasets,isonparwiththePaLM2-Ottermodel.Ontheotherhand,the
latencyevaluationsinthelastrowdemonstratethattheTandemmodelisabout1.16xfasterthanthePaLM2-Ottermodel.
Tandem- Tandem-
PaLM2- PaLM2- PaLM2-
Dataset CE Distil
Gecko Otter Bison
(ours) (ours)
BoolQ(acc) 65.4 87.8 87.6 85.5 88.8
CB(acc) 39.3 82.1 83.9 71.4 87.5
COPA(acc) 80.0 78.0 82.0 88.0 88.0
RTE(acc) 55.2 80.1 78.3 84.1 77.6
ReCoRD(acc) 85.5 87.8 87.2 91.2 92.2
WIC(acc) 47.5 50.0 50.6 49.7 50.9
WSC(acc) 75.8 81.1 80.4 86.3 86.3
MultiRC(F1) 53.9 80.8 80.1 76.1 80.5
Table11.EvaluationoftheTandemmodeloneachoftheSuperGLUEtasks.WeseethattheTandemmodelsubstantiallyimprovesupon
theperformanceofstandalonePaLM2-Geckomodel,andonmostdatasets,isonparwiththePaLM2-Ottermodel.Ontheotherhand,the
latencyevaluationsinthelastrowdemonstratethattheTandemmodelisabout1.16xfasterthanthePaLM2-Ottermodel.
Tibetan plateau from India
The Himalayas...mountain range separating the The Himalayas...separating the Tibetan plateau
Figure4.InferenceofTandemtransformerswithoutfreetokenfromtheprimarymodelM .(left)Firstblockprediction.(right)Second
L
blockprediction.GiventhesamequeryTheHimalayasareamountainrangeseparatingtheasinFigure2,here,M firstprocessesthis
L
queryexceptthelasttokenthe.ThelasttokenispassedasaninputtothesecondarymodelM ,whichattendstoM representationsfor
S L
allpasttokens,andproducesthefirstblockofresponsesTibetanplateauautoregressively.Inthesecondblock,M processestheTibetan
L
inablockmodewhileplateauispassedasaninputtoM ,whichthenautoregressivelygeneratethenextblockofresponsefromIndia.
S
ThiseventuallyleadstoaresponseofTibetanplateaufromIndia....
13