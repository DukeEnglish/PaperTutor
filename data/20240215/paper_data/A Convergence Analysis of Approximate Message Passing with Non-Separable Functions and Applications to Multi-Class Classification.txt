A Convergence Analysis of Approximate Message
Passing with Non-Separable Functions
and Applications to Multi-Class Classification
Burak Çakmak∗, Yue M. Lu†, Manfred Opper∗‡§
∗Technical University of Berlin, Berlin 10587, Germany, {burak.cakmak, manfred.opper}@tu-berlin.de
†Harvard University, Cambridge, MA 02138, USA, yuelu@seas.harvard.edu
‡University of Birmingham, Birmingham B15 2TT, United Kingdom
§University of Potsdam, Potsdam 14469, Germany
Abstract—Motivatedbytherecentapplicationofapproximate properties of a one-dimensional nonlinear mapping which
message passing (AMP) to the analysis of convex optimizations governs the dynamics of order parameters. In such cases,
in multi-class classifications [Loureiro, et. al., 2021], we present
parameter values of the algorithm which separate regions of
a convergence analysis of AMP dynamics with non-separable
(local) convergence from a divergent behavior can usually be
multivariate nonlinearities. As an application, we present a
complete (and independent) analysis of the motivated convex showntoberelatedtotheso-calleddeAlmeida–Thouless(AT)
optimization problem. [12] stability criterion of the static problem [13]–[16].
I. INTRODUCTION We study AMP-style algorithms with non-separable func-
tions [17]–[20] (i.e., the so-called denoiser function is vector-
Theanalysisofthestatisticalpropertiesofhigh-dimensional
valued and has non-separable nonlinearities over indices) and
random convex optimization problems is a very active topic
their application to the analysis of convex optimization prob-
in information theory, statistics, machine learning [1]–[10].
lems. These more complex models have become increasingly
The interest in studying such problems is motivated by the
interesting, because of their practical relevance ranging from
fact that they can serve as models for large-scale parameter
multi-class classification in machine learning [6], [8], [10] to
estimation in statistical machine learning and communication
channel estimation in communication theory [21], [22].
theory, where the data are the source of randomness in the
models. The asymptotic limit considered in such models, The analysis of the asymptotic convergence properties in
where both the number of data instances and the number of the high–dimensional limit is less straightforward. The order
modelparametersgrowlarge(whiletheirratioisfixed),makes parameters in the state evolution are matrices for which
the theoretical analysis non-trivial. contraction properties are not easily obtained. While recent
Research on this topic was dominated for a long time works [8], [10] show promising results in this direction, we
by non–rigorous approaches, such as the replica method, conjecturethatsuchresultsmaynotbecomplete.E.g.,explicit
developed in the field of statistical physics [1], [3], [11]. In (AT style) stability criteria have not been obtained so far.
recent years, a variety of models have been treated rigorously
Ourmaincontributionistoleveragetheconvexityproperties
by methods of high–dimensional probability [2], [4], [9].
of the state evolution mapping and present a complete (and
A promising, but somewhat indirect approach derives prop-
novel) analysis of the convergence properties of the AMP
erties of the static optimization problems by studying the
dynamics involving non-separable functions. It reveals that a
dynamics of a class of algorithms which are constructed
simplified approach relying solely on monotonicity properties
to converge to their solutions. They are usually known as
of the mapping, akin to the one employed by Loureiro et
approximate message passing (AMP) style algorithms. For
al. (2021) [8], is insufficient to establish convergence (see
high-dimensional problems the dynamics of individual nodes
Remark 1). Also, our analysis does not rely on several model
intheAMPalgorithmscanoftenbeasymptoticallydecoupled,
assumptions made in [8], [10], such as the uniqueness of the
leading to an effective equivalent stochastic dynamics for a
fixed-pointsolutioninthestateevolutionandtheboundedness
single node. The high–dimensional dynamical problem is re-
of the observation vectors.
placedbythetimeevolution(usuallytermed“stateevolution”)
of a finite number of deterministic “order parameters” which Organization: In Section II, we present a contraction map-
convergetothecorrespondingorderparametersdescribingthe pinganalysisofAMPdynamicswithnon-separablefunctions.
static properties of the optimization problem. The necessary Section III applies this analysis to convex optimization. Sec-
convergence properties of AMP–style algorithms for cases tion IV provides numerical illustrations of AT-type stability.
wheredynamicalnodesconsistofscalar randomvariablesare Conclusions are in Section V. Proofs of intermediate results
often obtained in a relatively simple way from the contraction are in the appendix.
4202
beF
31
]GL.sc[
1v67680.2042:viXraA. Notations with a convex loss function l(θ;y) (w.r.t. θ) and a regulariza-
We use the index set notation, [N] =∆ {1,...,N}. The tion constant λ 0 >0. The optimization (5) can be interpreted
scaling parameters in the paper are d and n while their ratio asthemaximumaposteriorestimationofω 0 ofaBayesianin-
α=∆ n/d is fixed as d,n→∞. We write m≍d to imply that ferenceapproachwiththeassumedpriorω 0 ∼ i.i.d. N(0,I/λ 0)
and the assumed likelihood p(y|θ)∝exp(−l(θ;y)).
the ratio m/d is fixed as d,m → ∞. Throughout the paper,
K,T ∈ N are fixed (w.r.t. d). For m ≍ d, we use bold-faced Similar to [8], we consider a generic AMP algorithm to
solve the optimization problem. It is defined for t ∈ [T] by
lowercaseletters,e.g.,a,b,todenotem×K matrices,whose
the dynamics
(normalized) inner product is defined as
1 γ(t) =Xω(t)−f(t−1) (6a)
⟨a,b⟩=∆ a⊤b. (1)
m f(t) =f(γ(t);y) (6b)
The kth column and the ith row of a ∈ Rm×K are denoted ω(t+1) =X⊤f(t)−αω(t)Q(t) . (6c)
by a ∈ Rm×1 and a[i] ∈ R1×K, respectively. We use
k
Here, f(γ;y) is an n×K matrix with its ith row denoted by
calligraphicletters,e.g.A,forK×K matricesandI standsfor
f(γ[i];y[i])andf(·;·):(RK×RK)→RK isanappropriately
the K×K identity matrix. A>0 (resp. A≥0) implies that
chosenvector-valuedfunctionsuchthatthefixed-pointofω(t)
A is a positive definite (resp. positive semi-definite) matrix.
coincides with the reconstruction of the unkown, i.e., ω⋆.
The multivariate Gaussian distribution (resp. density func-
OurgoalistoanalyzethedistancesbetweenAMPdynami-
tion of x) with mean µ and covariance matrix Σ is denoted
cal variables with a fixed number of memory-step differences
byN(µ,Σ)(resp.g(x|µ,Σ)).A∼Pindicatesthatarandom √
(e.g., d−1∥ω(t+τ) −ω(t)∥ for τ ≥ 1) and show if these
variable or random vector (rv or RV) A has a distribution P. F
contract as t increases in the high–dimensional limit d→∞.
We use the notation a ∼ A to indicate that the rows of
i.i.d.
Thedynamicsisinitializedfromarandomlyperturbedweight
thevectoraareindependentandidenticallydistributed(i.i.d.)
vector ω . To be specific, we set f(0) =0 and
copies of the RV A. We may also indicate this as a ∼ P 0
i.i.d.
where P denotes the distribution of A. ω(1) =r B(1)+u(0)(cid:112) C(1,1) . (7)
Concentrations with Lp norms: Let κ > 0 be a deter- 0
d
ministic sequence indexed by d, e.g., κ d = d−1 2 or κ d = 1. Here u(0) ∼ i.i.d N(0,I) is an arbitrary (i.e., independent
We write of everything else constructed so far) random matrix and
A=O(κ ) B(1),C(1,1) ∈ RK×K are some deterministic matrices with
d
C(1,1) ≥0 and r is from the QR decomposition
to imply that for each p ∈ N there is a constant C such 0
p
that ∥A∥
Lp
≤C pκ
d
with ∥A∥
Lp
=∆ (E|A|p)p1 . We say aˆ is a w 0 =r 0⟨r 0,ω 0⟩ s.t. ⟨r 0,r 0⟩=I. (8)
high-dimensional equivalent of a, denoted by Note that this choice of initialization contains information
about the true matrix ω (through r ). A fully random
aˆ ≃a if ∥aˆ−a∥ =O(1). (2) 0 0
F
initialization corresponds to setting B(1) =0.
E.g., if aˆ ≃a, then for any small constant c>0 we have the While the AMP dynamics (6) describes a nonlinear system
almost sure convergence as d→∞ ofnodescoupledbytherandommatrixX,onecanshowthat
1 (with an appropriate choice of Q(t), see below) the dynamics
∥aˆ−a∥
=O(d−c)a →.s.
0. (3)
dc F ofthenodesdecoupleasd→∞andcanberewritteninterms
of i.i.d. stochastic processes described by the state-evolution:
Indeed,thedefinitionofO(d−c)andMarkov’sinequalityyield
P(|O(d−c)| ≥ ϵ) ≤ C pp d−cp for some ϵ > 0 and p ∈ N and Definition 1 (State Evolution). Let {Ψ(t) ∈R1×K} t∈[T+1] is
ϵp
choosing p> 1 leads from Borel-Cantelli’s lemma to (3). a zero-mean Gaussian process with the two-time covariances
c C(t,s) =∆ E[(Ψ(t))⊤Ψ(s)] for all t,s ∈ [T + 1] recursively
II. ANALYSISOFTHECONTRACTIONMAPPING
constructed as
We consider the problem of reconstructing of an unknown
matrix ω
0
∈ Rd×K from an observation matrix y ∈ Rn×K B(t+1) =αE[G⊤ 0f(Γ(t);Y)]−αB(t)E[f′(Γ(t);Y)] (9)
which is generated according to a log-likelihood function C(t+1,s+1) =αE[f(Γ(t);Y)⊤f(Γ(s);Y)] (10)
(cid:88)
lnp(y|ω 0,X)= lnp 0(y[i]|x⊤ i ω 0) (4) where C(1,t) =∆ C(1,1)δ t1 for all t ≥ 1 with δ ts denoting
1≤i≤n the Kronecker delta and we have introduced random vectors
where p 0(y|θ) denotes generating likelihood function and x⊤
i
Γ(t) =∆ G 0B(t)+Ψ(t) with Ψ(t) being independent of
denotes the ith row of the weight matrix X ∈ Rn×d. As a (cid:112) ∆
(Y,G )∼p (Y|G C )g(G |0,I) with C =⟨ω ,ω ⟩.
0 0 0 0 0 0 0 0
concrete application, we will later consider the reconstruction
(11)
of ω 0, denoted by ω⋆, by the following convex optimization Finally, we have set Q(t) =∆ E[f′(Γ(t);Y)] where f′(γ;·)
ω⋆ =argmin (cid:88) l(x⊤ω;y[i])+ λ 0∥ω∥2 (5) denotestheK×K Jacobianoff(γ;·)w.r.t.γ,withtheentries
ω 1≤i≤n i 2 F [f′(γ;·)] kk′ = ∂[f ∂(γ γ; k·)] k′ for any k,k ∈[K].Proposition 1 (Decoupling Principle). Let X ∼ A. The Proof of Theorem 1
i.i.d.
N(0,I/d). Let f(γ;y) be differentiable and Lipschitz contin- ForB(1) =B⋆ andC(1,1) =C⋆,fromDefinition1itfollows
uous w.r.t γ and f(0;Y)=O(1) where Y as in (11). Define
inductively (over iterations steps) that
∆
g =Xr . Then, we have for any t∈[T]
0 0 B(t) =B⋆ and C(t,t) =C⋆ ∀t. (19)
γ(t) ≃g 0B(t)+ψ˜(t) (12) Moreover, since C(1,t) = 0 for all t > 1, it is also easy to
ω(t+1) ≃r B(t+1)+ψ(t+1), (13) verify from (19) that
0
C(t,t+τ) =C(t,t+1) =C(t+1,t) ∀τ ≥1. (20)
where the two sequences {ψ˜(t) } and {ψ(t+1)} are
t∈[T] t∈[T]
independentwithψ˜(t) ∼ Ψ(t) andψ(t) ∼ Ψ(t) withthe Then, we have
i.i.d. i.i.d.
stochastic process Ψ(t) as in Definition 1. ∥ω(t+τ)−ω(t)∥2 F ( =a) ∥ψ(t+τ)−ψ(t)∥2 F +O(d−1 2) (21)
d d
Proof. See Appendix F for a sketch of the proof.
( =b) 2(cid:13) (cid:13)∆(t)(cid:13) (cid:13) +O(d− 21), (22)
F
Note that the above result is non-asymptotic and we recall
wherewehavedefinedthesingle-time-stepdeviationoperator
(2)and(3)forthedefinitionanditsasymptoticimplication(as
d→∞) of the high dimensional equivalence notation aˆ ≃a. ∆(t) =∆ C⋆−C(t,t+1). (23)
With the decoupling principle, the contraction mapping
Here, steps (a), and (b) use Proposition 1 and Lemma 4 in
problem essentially boils down to analyzing how the two-time
Appendix A, respectively. Then, from (53) we have
(i.e.,(t,s))state-evolution(see (9)and(10))convergetotheir
fi pux re pd osp eo ,in wt es aa ss suth me en thu am tb se ur chof fixit ee dra pti oo in ns tsi en xc ir se ta .ses. For this ∥ω(t+τ √)− dω(t)∥ F =√ 2(cid:13) (cid:13)∆(t)(cid:13) (cid:13) F21 +O(d− 21). (24)
Assumption 1. There exist the matrices C⋆ and B⋆ satisfying Similarly, we have
B⋆ =αE[G⊤ 0f(Γ⋆;Y)]−αB⋆E[f′(Γ⋆;Y)] (14)
∥γ(t+τ √)−γ(t)∥
F
=√
2(cid:13) (cid:13)∆(t)(cid:13) (cid:13)21 +O(d− 21). (25)
n F
C⋆ =αE[f(Γ⋆;Y)⊤f(Γ⋆;Y)], (15)
Hence, we only need to verify that ∥∆(t)∥ < Cρt for a
√ F AT
where Γ⋆ =∆ G B⋆+G C⋆ with G ∼ N(0,I) and (Y,G ) constant C independent of t. To this end, inspired by the
0 0
(see (11)) being independent. mapping ψ in [13, Lemma 2.2], we introduce the mapping
for 0≤X ≤C⋆
Theorem1. LetX ∼ i.i.d. N(0;I/d).Letf(γ;y)betwo-times √ √
differentiable and Lipschitz continuous w.r.t γ and f(0;Y)= T(X)=∆ αE[f(G B⋆+G X +G′ C⋆−X;Y)⊤
0
√ √
O(1)whereY asin(11).SupposeAssumption1holds.Chose ×f(G B⋆+G X +G′′ C⋆−X;Y)] (26)
0
B(1) =B⋆ and C(1,1) =C⋆. Then, for all τ ≥1
wheretheK.dimrandomvectors{G,G′,G′′}areallmutually
∥γ(t+τ √)−γ(t)∥ F <Cρ2t +O(d−1 2) (16) independentanddistributedasN(0,I),andtheyareindepen-
n AT dent of (Y,G ). E.g. notice that if 0≤C(t−1,t) we have
0
∥ω(t+τ √)−ω(t)∥
F <Cρ2t +O(d−1 2), (17) ∆(t) =T(C⋆)−T(C(t−1,t)). (27)
d AT
Lemma 1. Let f(γ;y) be two-times differentiable and Lips-
where C >0 denotes a fixed constant (independent of t) and chitz continuous w.r.t. γ. Suppose Assumption 1 holds. Then,
we have for 0≤X ≤C⋆
ρ =∆ ρ(α(E[f′(Γ⋆;Y)⊗f′(Γ⋆;Y)]), (18)
AT
(a) (b)
0 ≤ T(X)−T(Y) ≤ αE[f′(Γ⋆;Y)⊤(X −Y)f′(Γ⋆;Y)].
with ρ(·) denoting the spectral radius of the matrix in the
(28)
argument and ⊗ denoting the Kronecker product.
Proof. See Appendix B
Here, we note from the property (3) that e.g. the result (16)
implies almost surely Remark 1. It is not enough to employ the monotonicity,
inequality (a) in (28) to show convergence to C⋆ while (a)
lim
∥ω(t+τ √)−ω(t)∥
F <Cρ2t
.
yields
d→∞ d AT 0≤C(t,t+1) ≤C(t+1,t+2) ≤C⋆ ∀t (29)
Furthermore, we use the notation ρ to associate AT sta- whichimpliesconvergenceC(t,t+1) →Cˆ⋆ =T(Cˆ⋆)withCˆ⋆ ≤
AT
bility. Indeed, the AMP dynamics (6) is not stable in the C⋆. Even assuming that C⋆ has a unique solution of the fixed-
region ρ ≥ 1, specifically, when ρ ≥ 1 we have point equation in (15) does not imply that Cˆ⋆ =C⋆. A similar
AT AT
lim ∥C⋆−C(t,t+1)∥ ̸=0 (see Appendix C). argument was misinterpreted in the proof of [8].
t→∞ FDefinition 2. The vectorization of a K × K matrix X, We set the non-linear function f of the AMP dynamics (6) as
denotedX,isaK2×1columnvectorbystackingthecolumn
f(γ;y)=m(γ;y)−γ. (40)
vectors of X =[x ,x ,··· ,x ] below one another as X =
1 2 K
[x⊤,x⊤,··· ,x⊤]⊤. Moreover, we say Y ≤X ⇐⇒ Y ≤X, Here, it is worth to noting the relations
1 2 K
i.e. (X −Y)⊤(u⊗u)≥0 for all u∈RK×1.
f(γ;y)=−l′(m(γ;y);y)(V⋆)−1, (41)
ForthearbitraryrandomelementsX t ∼f′(Γ⋆;Y)indepen- f′(γ;y)=−l′′(m(γ;y);y)(V⋆+l′′(m(γ;y);y))−1 , (42)
dent for each t we write from Lemma 1 that
where l′(θ;y) and l′′(θ;y) denote the gradient and Hessian
∆(t+1) ≤αE[X t⊤∆(t)X t] (30) (w.r.t. θ) of the loss function, respectively.
≤α2E[(X X )⊤∆(t−1)(X X )] (31)
t−1 t t−1 t Assumption 2. Let λ > 0 and the function f(γ;y) be as
0
. . in (40). There exist the K ×K order matrices C⋆, B⋆, and
.
V⋆ >0 satisfying the system of equations
≤αtE[(X X ···X )⊤C⋆(X X ···X )] (32)
1 2 t 1 2 t
V⋆ =λ I−αE[f′(Γ⋆;Y)]V⋆ (43a)
0
with noting that ∆(1) =C⋆. Hence, in terms of the vectoriza- α
B⋆ = E[G⊤f(Γ⋆;Y)]V⋆ (43b)
tion notation we have λ 0
0
∆(t+1) ≤αtE[X X ···X ⊗X X ···X ]⊤ C⋆ (33) C⋆ =αE[f(Γ⋆;Y)⊤f(Γ⋆;Y)], (43c)
1 2 t 1 2 t
√
=αtE[(X ⊗X )(X ⊗X )···(X ⊗X )]⊤ C⋆ where we define the random vector Γ⋆ =G B⋆+G C⋆ with
1 1 2 2 t t 0
=(cid:0) αE[X ⊗X ]⊤(cid:1)t C⋆ (34) G∼N(0,I) being independent of (Y,G 0) (see (11)).
1 1
Note that Assumption 2 coincides with Assumption 1, i.e.
with noting that X and X are independent for all t̸=t′.
t t′ existence of the fixed point of the state-evolution.
From [23, Theorem 4.3.1] we note that
Remark 2. Let f(γ;y) be as in (40) and V⋆ as in As-
Y ≤X =⇒ ∥Y∥ ≤∥X∥ .
F F sumption 2. The fixed-point of ω(t) in the AMP dynamics (6)
Note also that ∥X∥ =∥X∥. Then, from (34) we have coincides with ω⋆ in (37).
F
∥∆(t+1)∥ F ≤(cid:13) (cid:13)(cid:0) αE[f′(Γ⋆;Y)⊗f′(Γ⋆;Y)]⊤(cid:1)t C⋆(cid:13) (cid:13) (35) Proof. See Equation 48.
(cid:124) (cid:123)(cid:122) (cid:125) Assumption 3. Let the loss function l(θ;y) be three times
UDtU−1
differentiable (w.r.t. to θ) and convex. Let ∥l′′(θ;y)∥ be
≤∥U∥ ∥U−1C⋆∥ρt (36) 2
2 AT bounded for any (θ,y). Furthermore, let l′(0;Y) = O(1)
where the later inequality uses the eigenvalue decomposition where the random variable Y as in Assumption 2.
UDtU−1 with D being diagonal. This completes the proof.
When Y = O(1), the cross entropy loss function (38)
III. APPLICATIONTOANALYZINGCONVEX fulfills the conditions specified in Assumption 3. While in
OPTIMIZATIONS multi-classclassificationY isoftendefinedasaone-hotcoded
vectorsuchthat∥Y∥=1,wenotethatthefamilyofrvsO(1)
As an application of Theorem 1, we analyze the convex
includesawiderangeofdistributionscharacterizedby"heavy"
optimization
exponentialtails,e.g.AD =O(1)forasub-GaussianrvAand
ω⋆ =∆ argmin (cid:88) l(x⊤ω;y[i])+ λ 0∥ω∥2 (37) foranylarge(constant)D.Hence,ouranalysiscanbeapplied
i 2 F toabroaderrangeofempiricalriskminimizationapplications.
ω
1≤i≤n
Proposition 2 (The AT stability). Suppose Assumption 2
foraconvexlossfunctionl(θ;y)(w.r.t.θ)andaregularization
holds. Let the constant ρ be as in (18) with the function
constant λ > 0. Here, the data matrix y is assumed to be AT
0
f(γ;y) as in (40). Let the loss function l(θ;y) be two-times
generated according to the log-likelihood (4). In particular,
differentiable (w.r.t. to θ) and convex. Then, ρ <1.
we may consider the so-called cross-entropy loss–commonly AT
used in multi-class classifications— Proof. See Appendix D.
l(θ;y)=− k(cid:88) ≤Ky kln (cid:80) ke ′θk eθ k′ , (38) T Suh pe po or se em A2 ss. uL me pt tiω on⋆ sb 2e aa ns din 3 ( h3 o7 ld). .L Te ht enX , fo∼ ri.i a.d n. yN fi( x0 ed,I (/ ad n) d.
large) t∈N (independent of d) we have
whereyisoftendefinedasone-hotencodedvector,i.e.y =e k √
for some k ∈[K] with e k denoting a K-dim. unit vector. ∥ω⋆−(r 0B √⋆+u C⋆)∥ F <Cρ2t +O(d−1 2), (44)
Given an appropriate K ×K deterministic matrix V⋆ > 0 d AT
(seeAssumption2below),weintroducetheproximaloperator
where r as in (8) which is independent of the d×K random
0
m(γ;y)=∆ argmin(cid:18)
l(θ;y)+
1 (γ−θ)V⋆(γ−θ)⊤(cid:19)
. (39)
matrix u ∼ i.i.d. N(0,I), ρ AT < 1 is as in Proposition 2 and
C is an irrelevant constant independent of t.
2
θBefore proceeding to the proof of Theorem 2, we present
the following high-dimensional analysis of the reconstruction
error as a consequence of Theorem 2:
Corollary 1. Let ω⋆ be as in (37). Suppose the premises of
Theorem 2 hold. Then, as d→∞ we have
1
∥ω⋆−ω ∥2−tr((B −B⋆)⊤(B −B⋆)+C⋆)a →.s. 0,
d 0 F 0 0
where B⋆ and C⋆ are as in (43) and B =∆ ⟨r ,ω ⟩, see (8).
0 0 0
Proof. See Appendix E.
A. Proof of Theorem 2
Since the optimization (37) is λ -strongly convex [24] (i.e.,
0
thespectralnormoftheHessianoftheoptimizationisbounded Fig.1. TheconvergenceoftheAMPdynamicswithd=105andα=2.The
straight lines on the interval 10≤t≤15 represent ρt . The experiments
ωbel ∈ow Rdb ×y Kλ 0 a) ndw te heca on ptb imou an ld poth ine td ωis ⋆ta an sce [2b 4e ,t Ew qe .en 9.1a 1n ]y point arebasedonsingleinstances(foreachλ0)oftheAMPAT dynamics.
2
∥ω−ω⋆∥
F
≤ ∥G(ω)∥
F
(45) IV. SIMULATIONRESULTS
λ
0
Weconsidertheapplicationoftheconvexoptimization(37)
where G is the gradient matrix of the optimization (37) w.r.t.
with the cross-entropy loss function (38). We generate ω
0
ω, i.e.,
such that ⟨ω ,ω ⟩=I and y according to the log-likelihood
0 0
G(ω)=λ 0ω+X⊤l′(X⊤ω;y). (46) lnp 0(y|θ) = −l(θ;y). The number of classes is K = 3. We
fix α = n/d = 2, which becomes critical as λ → 0. We
Let ω ≡ω(t) for t>1 be constructed by the AMP dynamics 0
simulate the AMP dynamics (6) using the Householder dice
(6) initialized with B(1) =B⋆ and C(1,1) =C⋆. We then write
implementation[26]whichallowstosimulatethedynamicson
a standard personal computer up to d = 106 (instead of 104
G(ω(t))=λ ω(t)+X⊤l′(m(γ(t−1);y)+(γ(t)−γ(t−1));y)
0 withadirectimplementation).Wehavetherateofconvergence
( =a) λ ω(t)+X⊤l′(m(γ(t−1);y);y)+X⊤h(t)
0 ∥ω(t+1)−ω(t)∥2
( =b) λ 0ω(t)+X⊤f(γ(t);y)V⋆+X⊤h(t) tl →im ∞dl →im ∞∥ω(t)−ω(t−1)∥F 2
F
a =.s. ρ AT . (49)
=ω(t)(λ 0I−V⋆)−αω(t−1)Q(t−1)V⋆+X⊤h(t)
We illustrate this result in Figure 1. The numerical results
( =c) (ω(t)−ω(t−1))(λ I−V⋆)+X⊤h(t) . suggest that for α = 2 we have ρ AT → 1 as λ 0 → 0 (and
0 we observe that obtaining the numerical value of ρ is more
AT
difficult the smaller λ is and we were unable to obtain a
In step (a) we have carried out the mean-value theorem such 0
that the rows of h(t) read in the form numerical value of ρ AT for λ 0 = 0). This is similar to the
so-called Gardner instability, see [27, Eq (4)].
(h(t))[i] =∆ ((γ(t))[i]−(γ(t−1)[i])l′′(ξ ;y[i]), ∀i∈[n]. (47)
i
V. CONCLUSION
Note that ∥h(t)∥ F ≤L l′′∥γ(t)−γ(t−1)∥ F where L l′′ denotes We have presented a convergence analysis of the dynam-
an upper bound of the spectral norm of the Hessian l′′(θ;y). ics of an AMP with nonseparable multivariate nonlinearities
In step (b) we use (41) and in step (c) we use the fact that and its application to multi-class classification. The analysis
given B(1) = B⋆ and C(1,1) = C⋆, we have from (19) that reveals a necessary (and sufficient) condition for dynamical
αQ(t)V⋆ =λ 0I−V⋆ for all t. Thus, we have from (45) stability,i.e.,ρ
AT
<1(see(18)).Wehaveshownthatthiscon-
2 ditionalwaysholdsforridge-regularized“softmax”regression
∥ω(t)−ω⋆∥ F ≤
λ
∥λ 0I−V⋆∥ 2∥ω(t)−ω(t−1)∥ F type applications, which are strongly convex problems. On
0
the other hand, ρ < 1 could only hold for some “region”
L AT
+ λl′′ ∥X∥ 2∥γ(t)−γ(t−1)∥ F . (48) of model parameter values of the non-convex (or not strictly
0 convex)problems,weexpectthatthestabilitycriteriacouldbe
Here, ∥ · ∥ stands for the spectral norm of the matrix in an important aspect for the analysis of non-convex problems.
2
the argument and we have e.g. from [25, Theorem 2.7] that Itwouldbeinterestingtoextendtheconvergenceanalysisto
∥X∥ = O(1). Furthermore, it is easy to verify that all the generalized AMP setting. This would allow us to analyze,
2
the premises of Theorem 1 are fulfilled by the premises of for example, the convex optimization (37) with a generalized
Theorem 2. Then, the thesis is evident from Theorem 1. nonlinearregularizationterm.Detailsarediscussedelsewhere.ACKNOWLEDGMENT APPENDIXB
This work was supported by the German Research Founda- PROOFOFLEMMA1
tion, Deutsche Forschungsgemeinschaft (DFG), under Grant
For the sake of notational compactness, let
‘RAMABIM’withNo.OP45/9-1,bytheUSNationalScience
Foundation under Grant CCF-1910410, and by the Harvard f (U)=∆ f(G B⋆+U;Y). (56)
0 0
FAS Dean’s Competitive Fund for Promising Scholarship.
It is also useful to write the mapping (26) as
APPENDIXA
CONCENTRATIONINEQUALITIESWITHLp NORM T(X)=αE[f (Ψ )⊤f (Ψ )] (57)
0 1 0 2
Here we present some elementary results on concentration
inequalities with Lp norm. whereΨ 1andΨ 2arezero-meanGaussianvectorsindependent
of the field (Y,G ) with E[Ψ⊤Ψ ] = C⋆ for m = 1,2
0 m m
Lemma 2. Consider the (scalar) random variables A = and E[Ψ⊤Ψ ]=X. In particular, by using the characteristic-
1 2
O(κ ) and B = O(κ˜ ), where κ and κ˜ are two positive
d d d d function representation of the Gaussian distribution we write
sequences indexed by d. Then the following properties hold:
(cid:90)
A+B =O(max(κ d,κ˜ d)) (50) T(X)=c dP(Y,G 0)dU 1dU 2dΨ 1dΨ 2 f 0(Ψ 1)⊤f 0(Ψ 2)
√AB =O( √κ dκ˜ d) (51) ×e−i(U1Ψ⊤ 1+U2Ψ⊤ 2)− 21U1C⋆U 1⊤− 21U2C⋆U 2⊤ e−U1XU 2⊤
A=O( κ ). (52)
d √
∆
where dP(Y,G ) = dYdG p (Y|G C )g(G |0,I) and
Moreover, let C > 0 be a constant and A = O(κ ) with 0 0 0 0 0 0
d
|A|≤C. Then, we have c =∆ α/(2π)2K. Also, one can verify that the differentiation
√ √ w.r.t. X is interchangeable with the integral above, see e.g.
kk′
C+A− C =O(κ ). (53)
d [29, Lemma 2]. We will perform differentiation with respect
Proof. The results (50) and (51) follow from the Minkowski to the symmetric matrix X and thereby we need to take the
inequalityandHölderinequality,(i.e.,∥A+B∥ ≤∥A∥ + symmetry into account. To that end, we define the so-called
Lp Lp
∥B∥ and ∥AB∥ ≤ ∥A∥ ∥B∥ ), respectively. The elimination and duplication matrices.
Lp Lp L2p L2p
1
result (52) follows from ∥A1/2∥ Lp ≤ ∥A1/2∥ L2p = ∥A∥ L2 p. Definition3. [30]ConsideraK×K matrixX =X⊤.Then,
Finally, the result (53) is evident: X denotesthe 1K(K+1)×1vectorobtainedfromtheK2×1
√ √ |A| |A| vep ctor X (see D2 efinition 2) by eliminating all supra-diagonal
| C+A− C|= √ √ ≤ √ =O(κ ).
d
C+A+ C C elements of X. E.g., when K =3, we have
X =(X ,X ,X ,X ,X ,X ,X ,X X )⊤
11 21 31 12 22 32 13 23 33
Lemma 3. [28, Lemma 7.8] For m≍d, consider a random X =(X ,X ,X ,X ,X ,X )⊤.
vector a∈Rm where a∼ A and A=O(1). Then, p 11 21 31 22 32 33
i.i.d.
1 (cid:88) For each K there is a unique 1K(K+1)×K2 projection
a =E[A]+O(d−1/2). 2
m i matrix P such that
i∈[m]
X =PX. (58)
p
Lemma 4. For m ≍ d, consider the random vectors a,b ∈
R Bm =w Oh (e 1r )e
.
a The∼ ni ,.i.d fo. rA ana ynd
aˆ
b ≃∼ ai. ai.d n. dB bˆ≃wi bth
,
wA e= havO e(1) and M “do ur be lo icv ae tr i, of no ”r mea ac th rixK Dth se ur ce hex this at ts a unique K2×1 2K(K+1)
⟨aˆ,bˆ⟩=E[AB]+O(d−1 2). (54)
X =DX . (59)
p
Proof. Let δ =∆ aˆ − a and δ =∆ bˆ − b with noting that
a b √ For the explicit definitions of the elimination and duplication
e.g. δ = O(1). From Lemma 3 we have a = O( d) and
a√ matrices we refer to [30]. In particular, we will solely need
b=O( d). Hence, from the properties (50) and (51) we get
the following property
⟨aˆ,bˆ⟩−⟨a,b⟩=⟨a,δ b⟩+⟨b,δ a⟩+⟨δ a,δ b⟩=O(d−1 2). (55)
DPX =X, if X =X⊤. (60)
Also, from (51) we have AB =O(1); so that from Lemma 3
we get ⟨a,b⟩=E[AB]+O(d− 21). This completes the proof. Finally, it is useful to note that
Lemma 5. For m ≍ d let the random vectors g,r ∈ Rm×K
∂e−U1XU 2⊤
=
∂e(−U 2⊤U1)⊤X
=
∂X p∂e(−U 2⊤U1)⊤DXp
∂X¯ ∂X¯
∂X ∂X
be independent with g ∼ N(0,I) and ⟨r,r⟩ = I. Then, p
i.i.d.
⟨r,g⟩=O(d−1 2). (61)
√ =P⊤D⊤(−U⊤U ) (62)
Proof. Let G =∆ m⟨r,g⟩. Notice that G ∼ N(0,I). 2 1
i.i.d. =P⊤D⊤(iU )⊤⊗(iU )⊤. (63)
Hence, G =O(1) which completes the proof. 1 2By using this identity we have for all k,k′ ∈[K] B. Proof of the bound (b)
For short we define the mapping for 0≤X ≤C⋆
∂[T(X)]
[T′(X)] kk′ =∆ ∂X kk′ (64) T(cid:101)∆(X)=∆ αE[f 0′(G√ √X +G′√ C √⋆−X)⊤∆
=αP⊤D⊤E[(iU 1)⊤⊗(iU 2)⊤f 0k(Ψ 1)f 0k′(Ψ 2)] ×f′(G X +G′′ C⋆−X)] (75)
0
(65)
for a fixed ∆≥0. Then, for any 0≤Y ≤X ≤C⋆ we study
=αP⊤D⊤E[f′ (Ψ )⊗f′ (Ψ )] (66)
0k 1 0k′ 2 the interpolation for q ∈[0,1]
where for short we adopt the notational setups for k ∈[K] G˜ ∆(q)=∆ T(cid:101)∆(qX +(1−q)Y). (76)
We next show that G˜′ (q)≥0. By following the argument of
f 0k(Ψ)=∆ (f 0(Ψ)) k and f 0′ k(Ψ)=∆ ∂f 0 ∂k Ψ(Ψ) . (67) (70), this will then im∆ ply the bound (b). First, for all k,k′ ∈
[K] we write by the chain rule
A. The proof of the bound (a)
[G˜ ∆′ (q)]
kk′
=([T(cid:101) ∆′(S)] kk′)⊤∆˜ (77)
For any 0 ≤ Y ≤ X ≤ C⋆, we introduce the trivial
where ∆˜ =∆ X −Y and S =qX +(1−q)Y and
interpolation for q ∈[0,1]
G(q)=∆ T(qX +(1−q)Y). (68) [T(cid:101) ∆′(X)] kk′ =∆
∂[T(cid:101)∆ ∂( XX)]
kk′ . (78)
We then obtain
Then, by the general mean-value theorem of [31] we write (cid:16) √ √
[G˜′ (q)] =αtr E[f′′ (G S+G′ C⋆−S)∆
∆ kk′ 0k
T(X)−T(Y)=G(1)−G(0) (69) √ √ (cid:17)
×f′′ (G S+G′′ C⋆−S)⊤]∆˜ (79)
(cid:88) 0k′
= λ iG′(q i) (70) (cid:16)(cid:112) √ √
1≤i≤K2 =αtr ∆˜E[f 0′′ ,k(G S+G′ C⋆−S)∆
√ √ (cid:112) (cid:17)
for some q ∈ (0,1) and (cid:80) λ = 1. In particular, by ×f′′ (G S+G′′ C⋆−S)⊤] ∆˜ (80)
i 1≤i≤K2 i 0k′
the chain rule, we have for all k,k′ ∈[K]
where we have defined f′′ (Ψ) =∆ ∂f0k(Ψ). For further nota-
0k ∂Ψ⊤∂Ψ
[G′(q)] kk′ =([T′(S)] kk′)⊤X −Y (71) tional compactness, we introduce the K×K2 Hessian matrix
(cid:20) (cid:21)
∂f (Ψ) ∂f (Ψ) ∂f (Ψ
where S =qX +(1−q)Y. Then, from (66) we write f′′(Ψ)=∆ 01 , 02 ,..., 0K . (81)
0 ∂Ψ⊤∂Ψ ∂Ψ⊤∂Ψ ∂Ψ⊤∂Ψ
√ √
[G′(q)] =αE[f′ (G S+G′ C⋆−S) Then, we write everything in the compact matrix notation
kk′ ⊗f0 0′k k′(G √√ S+G √′′√ C⋆−S)]⊤DPX−Y G˜ ∆′ (q)=αtr K(cid:18) (I⊗(cid:112) ∆˜)E(cid:20) f 0′′(cid:16) G√ S+G′(cid:112) C⋆−S˜(cid:17)⊤
=αE[f′ (G S+G′ C⋆−S))
0k √ √ ×∆f′′(cid:16) G(cid:112) S˜+G′′(cid:112) C⋆−S˜(cid:17)(cid:105) (I⊗(cid:112) ∆˜)(cid:17)
⊗f′ (G S+G′′ C⋆−S)]⊤X−Y 0
0k′ √ √
=αE[f′ (G S+G′ C⋆−S)(X −Y) where for a matrix X ∈RK2×K2 we write
0k′ √ √    
×f′ (G S+G′′ C⋆−S)⊤] (72) X 11 ... X 1K tr(X 11) ... tr(X 1K)
0k tr K

. .
.
... . .
.
 ≡

. .
.
... . .
.


where the latter equality follows from the property of the
X ... X tr(X ) ... tr(X )
vectorization operator AXB⊤ =(B⊗A)X. We then write K1 KK K1 KK
where X ∈RK×K for all k,k′. Equivalently, we have
everything in the matrix notation as kk′
√ √ tr (X)=E [(I⊗Z)X(I⊗Z⊤)] (82)
K Z
G′(q)=αE[f′(G S+G′ C⋆−S)⊤∆
0 √ √ where Z ∼ N(0;I) is an arbitrary 1 × K dim. Gaussian
×f′(G S+G′′ C⋆−S)] (73)
0 random vector. Thus, if X ≥0 then tr K(X)≥0. We finally
introduce the auxiliary K×K2 random operator
where we have introduced ∆ =∆ X −Y ≥ 0. Moreover, by √ √
F′(G)≡E [f′′(G C⋆−S+G′ S)]
introducing the auxiliary random operator G′ 0
√ √ and then write
F(G)≡E [f′(G S+G′ C⋆−S)] √ √ √ √
G′ 0 E[f′′(G S+G′ C⋆−S)⊤∆f′′(G S+G′′ C⋆−S)]
0 0
we can write G′(q) = E[F(G)⊤∆F(G)] which implies that =E[F′(G)⊤∆F′(G)]. (83)
T is monotonic, i.e. Since ∆ ≥ 0 we have E[F′(G)⊤∆F′(G)] ≥ 0. This
G′(q)≥0. (74) completes the proof.APPENDIXC APPENDIXF
DYNAMICALSTABILITY:NECESSITYOFρ
AT
<1 SKETCHOFTHEPROOFOFPROPOSITION1
Suppose∆(t) =o(1)whereo(1)standsforaK2×1vector The proof is based on the idea of the "Householder dice"
representation of AMP dynamics introduced in [32], which is
with ∥o(1)∥ → 0 as t → ∞. Then, we use the results (73)
a way to represent the AMP dynamics – which are coupled
and obtain the “linearized” updates
by a random matrix (X, in our case) – as equivalent random
∆(t+1) =αE[f′(Γ⋆;Y)⊗f′(Γ⋆;Y)]⊤]∆(t)+o(1) (84) matrix-free dynamics. We have two main steps: In the first
step (detailed in section F-A), we use the Gram-Schmidt
whichimplieswhenρ AT ≥1,∆(t) ̸=o(1).Hence,bycontra- orthogonalization to represent the dynamics of AMP (6) as
diction, the condition ρ AT <1 is necessary for ∆(t) =o(1). an equivalent X-free dynamics, called the "Householder dice
representation". In the second step (detailed in section F-B),
APPENDIXD weusetheCholeskydecomposition,alongwiththeproperties
PROOFOFPROPOSITION2 of the notion of Lp concentration given in Appendix A, to
derive the high-dimensional equivalence of the Householder
We introduce the auxiliary matrix
diceequivalence[andtherebythehigh-dimensionalequivalent
E =∆ (I−αE[Λ⋆(V⋆+Λ⋆)−1⊗Λ⋆(V⋆+Λ⋆)−1])V⋆⊗V⋆ of the original AMP dynamics (6)].
=V⋆⊗V⋆−αE[Λ⋆(V⋆+Λ⋆)−1V⋆⊗Λ⋆(V⋆+Λ⋆)−1V⋆] A. The Householder Dice Representation
(85) We begin with the symmetrization trick [17], [18] to pack
the original AMP dynamics (6), which involves a rectangular
where Λ⋆ =∆ l′′(m(Γ⋆;Y);Y). By the convexity of the loss random coupling matrix X, into a compact form of the
function, we note that Λ⋆ ≥0. On the other hand, we have dynamicsinvolvingonlyasymmetricrandomcouplingmatrix
V⋆ =λ I+αE[Λ⋆(V⋆+Λ⋆)−1V⋆]. (86) A ∈ Rm×m with m =∆ n+d. To this end, we introduce the
0
dynamics for the iteration steps s=1,2,··· ,S =2T
Thus, we get h(s) =Am(s)−m(s−1)Q(s) (93a)
m
V⋆⊗V⋆ =λ I⊗V⋆+αE[Λ⋆(V⋆+Λ⋆)−1V⋆]⊗V⋆ (87) m(s+1) =η (h(s);y) (93b)
0 s
and thereby E reads as with m(0) =∆ 0. Here, we have defined the random matrix as
√
(cid:18) (cid:19)
λ =0I λ⊗ IV ⊗⋆ V+ ⋆α +E[ αΛ E⋆ [( ΛV ⋆⋆ (V+ ⋆Λ +)− Λ1 ⋆V )⋆ −1⊗ VV ⋆⋆ ⊗− VΛ ⋆(⋆ V(V ⋆⋆ ++ ΛΛ ⋆)⋆ −)− 1V1V ⋆]⋆ .] A=∆ √ 11
+α
XαZ ⊤n ZX
d
(94)
0
where Z and Z are arbitrary n×n and d×d Gaussian
n d
Thus E >0 which implies that Orthogonal-Ensemble (GOE) random matrices, respectively,
E(V⋆⊗V⋆)−1 =(I−αE[f′(Γ⋆;Y))⊗f′(Γ⋆;Y)]) (88)
e.g. Z
n
∼ √1 2n(G+G⊤) where G ∼
i.i.d.
N(0,I). So, by
construction A is a GOE random matrix.
TounpacktheoriginalAMPdynamics(6)fromthedynam-
has strictly positive eigenvalues, and thereby ρ AT <1. √ (cid:20) 0 (cid:21)
ics (93) we set m(1) ≡ 1+α and
ω(1)
APPENDIXE
PROOFOFCOROLLARY1  (cid:20) f(h ;y) (cid:21)
For short let ψ =∆ u√ C⋆. Then, we have η s(h;y)=√ 1+α (cid:20) 0 0n (cid:21) s=1,3,5,···
∥ω −(r B⋆+ψ)∥

h
s=2,4,6,···
0 √0 F d
(95)
d
(cid:113) where for convenience we partition h∈R(n+d)×K as
= tr[(B −B⋆)⊤(B −B⋆)+2⟨r ,ψ⟩+⟨ψ,ψ⟩] (89)
0 0 0 (cid:20) (cid:21)
h
(cid:113) h≡ n with h ∈Rn×K .
( =a) tr[(B 0−B⋆)⊤(B 0−B⋆)+⟨ψ,ψ⟩]+O(d− 21) (90) h d n
( =b)(cid:113) tr[(B 0−B⋆)⊤(B 0−B⋆)+C⋆]+O(d−1 2) (91) Also, let Q( ms) = Q( 2s) for s = 2,4,6,··· and Q( ms) = I for
s=3,5,7···. Hence, we have
(cid:113)
( =c) tr[(B 0−B⋆)⊤(B 0−B⋆)+C⋆]+O(d−1 2), (92)
h(2t−1) =γ(t) and h(2t) =ω(t+1) , t∈[T]. (96)
n d
where steps (a), (b), and (c) use Lemma 5, Lemma 4 and
We will adaptively use the following representation of the
Lemma 2, respectively. Then the thesis follows from an ap-
GOE random matrix which has been reported for the case
propriateapplicationofthetriangularinequalityofFrobenious
K =1 in [33].
norm to Theorem 2.Lemma6. LettherandommatricesA˜ ∈Rm×m,v ∈Rm×K, Moreover, we introduce the m×K Gaussian random element
(cid:20) (cid:21)
u∈Rm×K and Z ∈RK×K be mutually independent. Let A˜ u(0) =∆ g 0 ∈Rm×K wherewerecallthatg =Xr and
and √1 Z be both GOE random matrices, ⟨v,v⟩ = I and u 0 0 0
K u ∼ N(0;I) is an arbitrary random element. Finally,
u∼ i.i.d. N(0,I). Let P⊥ v =∆ I− m1vv⊤. Then, P0 ⊥ i.i.d. =∆ I − 1 (cid:80) v(s)(v(s))⊤ which is the projec-
m(0:s) m 0≤s′≤s
1 (cid:18) vZv⊤ (cid:19) tion matrix onto the orthogonal complement of span(m(0:t)).
A=∆ P⊥uv⊤+ √ +vu⊤P⊥ +P⊥A˜P⊥ (97)
m v m v v v
B. The High-Dimensional Equivalent
is also a GOE random matrix and independent of v. As a first step, we begin with the following high-
dimensional equivalence of the field h(s) in (98):
Proof. Since the GOE random matrix is rotational in-
√variant, without loss of generality we can assume v = h(s) ≃ (cid:88) u(s′)⟨v(s′),m(s)⟩
m[e ,e ,··· ,e ] where e ∈ Rm denotes the standard
1 2 K k 0≤s′≤s
basis vector. Then, the proof is evident.
+
(cid:88) v(s)(cid:16) ⟨u(s′),m(s)⟩−⟨v(s′),m(s−1)⟩Q(s)(cid:17)
,
m
For example, consider a matrix m ∈ Rm×K with 0≤s′<s
span(m) = span(v). Then, from Lemma 6 we write (100)
Am = P⊥ mu⟨v,m⟩ + vZ √⟨v m,m⟩ which involves only where we have invoked the results for all 0≤s′ ≤s≤S
the lower-dimensional random elements u and Z. To ex-
tend this idea to the dynamics Am(s), we first introduce ⟨v(s′),m(s)⟩( =a) O(1) (101)
the (block) Gram-Schmidt orthogonalization notation: Let
uˆ(s)−u(s) ( =b) O(1) (102)
v(0:s) = {v(0),v(1),...,v(s−1)} be a collection of matrices
in Rm×K with ⟨v(i),v(j)⟩ = δ ijI for all i,j. Then, for any ϵ(s) ( =c) O(1) (103)
b ∈ Rm×K, by the Gram-Schmidt orthogonalization process √
we can always construct the new orthogonal matrix Here, to verify (a) it is enough to verify that m(s) =O( d)
which can be verified inductively over the iteration steps. The
v(s) =∆ GS(b|v(0:s−1))
step(b)followsfromLemma5.Thestep(c)followsfromthe
such that ⟨v(s),v(j)⟩ = δ I and b = (cid:80) v(s)⟨v(s),b⟩ . fact that Z =O(1).
sj 0≤i≤s We now recall (95), i.e. for all 1≤s≤S we have
We iteratively employ the Gram-Schmidt process to construct
a set of orthogonal matrices v(0:s) ≡ {v(0),v(1),...,v(s)}  (cid:20) 0 (cid:21)
such that span(v(0:s))=span(m(1:s)) and apply Lemma 6 to
√

ω(s+ 21)
s=1,3,5,···
obtain the following A-free equivalent of the dynamics: m(s) = 1+α . (104)
Lemma 7 (The Householder Dice Representation). Let A be
 (cid:20) f( 02s) (cid:21)
s=2,4,6,···
the GOE random matrix. Then, the joint probability distribu-
tionofthesequenceofmatrices{h(1:S),m(1:S+1)}generated We then specialize the Gram-Schmidt process v(s) =
by dynamics (93) is equal to that of the same sequence GS(m(s)|v(0:s−1)). To start we introduce the decompositions
generated by the following dynamics for s=1,2,··· ,S
(cid:34) (cid:35)
√ √1 v˜(s)
v(s) =GS(m(s)|v(0:s−1)). (98a) v(s) ≡ 1+α α v˜(sn ) . s∈[S] (105)
d
uˆ(s) =P⊥ u(s) (98b)
m(0:s) Suppose that we have constructed v(s) by
1
ϵ(s) = √ mv(s)Z(s) (98c)
v˜(s)
=(cid:26) GS(f( 2s)|v˜( n2),v˜( n4),···v˜ n(s−2)) s=2,4,6,···
h(s) = (cid:88) (cid:16) uˆ(s′)+ϵ(s′)(cid:17) ⟨v(s′),m(s)⟩ n 0 s=1,3,5,···
(cid:26) 0 s=2,4,6,···
0≤s′≤s v˜(s) =
+
(cid:88) v(s)(cid:16) ⟨uˆ(s′),m(s)⟩−⟨v(s′),m(s−1)⟩Q(s)(cid:17) d GS(ω(s+ 21)|r 0,v˜( d1),v˜( d3)··· ,v˜ d(s−2)) s=1,3,5,···
m
0≤s′<s Indeed, by construction, we have 0≤s,s′ ≤S
(98d)
⟨v(s),v(s′)⟩=δ
I (106)
m(s+1) =η (h(s),y). (98e) ss′
s
span(m(0:s))=span(v(0:s)). (107)
Here, for each s ≥ 1 we generate arbitrary Gaussian ran-
dom matrices u(s) ∼ u and arbitrary symmetric Gaussian Hence, the only necessary basis elements are r(t) =∆ v˜(2t−1)
d
random matrices Z(s) ∼ Z where u and Z as in Lemma 6. and l(t) =∆ v˜(2t) for t∈[T], i.e.,
n
Furthermore, we recall r =GS(ω ) and we introduce
0 0 r(t) =GS(ω(t)|r ,r(1),r(2),··· ,r(t−1)) (108)
m(0) =∆ (cid:20) 0 (cid:21) and v(0) =∆ √ 1+α(cid:20) 0 (cid:21) . (99) l(t) =GS(f(t)|l(0 1),l(2),··· ,l(t−1)) (109)
ω r
0 0and the only necessary (arbitrary) Gaussian elements are Here, B(t) is as in Definition 1 and the blocks B(t,s) satisfy
the equations of block Cholesky decomposition
z(t) =∆ u(2t−1) (110)
d
 
g(t) =∆ u( n2t) . (111) B(s,s) =cholC(s,s)− (cid:88) B(s,s′)(B(s,s′))⊤

Recall that h(2t) ≡γ(t) and h(2t+1) ≡ω(t+1), we then write 1≤s′<s
n d
from (100) (126a)
γ(t) ≃g Bˆ(t)+ (cid:88) g(s)Bˆ(t,s) B(t,s)(B(s,s))⊤ =C(t,s)− (cid:88) B(t,s′)(B(s,s′))⊤ . (126b)
0
1≤s≤t 1≤s′<s
+
(cid:88) l(s)(cid:16) ⟨z(s),ω(t)⟩−Bˆ(t−1,s)(cid:17)
(112) for each 1≤s≤t≤T +1 where B =chol(A) for A≥0 is
f
1≤s<t a lower-triangular matrix such that A=BB⊤.
(cid:124) (cid:123)(cid:122) (cid:125)
Using the perturbation idea of [18, Section 5.4] one can
=∆˜ϵ(t)
ω(t+1) ≃r (α⟨g ,f(t)⟩−αBˆ(t)Q(t))+√ α (cid:88) z(s)Bˆ(t,s) verify that in proving Theorem 1 we can assume without loss
0 0 f of generality that
1≤s≤t
+α
(cid:88) r(s)(cid:16) ⟨g(s),f(t)⟩−Bˆ(t,s)Q(t)(cid:17)
(113)
C(1:T+1) >0. (127)
1≤s<t Here, C(1:T+1) denote the (T +1)K×(T +1)K matrix with
(cid:124) (cid:123)(cid:122) (cid:125)
its the (t,s) indexed K×K block matrix is C(t,s),i.e.,
=∆ϵ(t)
wherewehavedefinedtheK×K matricesfor1≤s≤t≤T C(t,s) =(e⊤⊗I)C(1:T+1)(e ⊗I) ∀t,s∈[t′].
t s
Bˆ(t) =∆ ⟨r ,ω(t)⟩ (114)
0
where e is the 1×(T+1) dimensional standard basis vector,
Bˆ(t,s) =∆ ⟨r(s),ω(t)⟩ (115) i.e., (e )t = δ . The condition (127) implies the diagonal
t s ts
Bˆ(t,s) =∆ ⟨l(s),f(t)⟩. (116) blocks B(s,s) for s∈[T +1] are all non-singular. Hence, the
f blocks B(t,s) for each 1 ≤ s ≤ t ≤ T +1 can be uniquely
The terms ˜ϵ(t) and ϵ(t) stand for the memory-cancellations in constructed through (126).
which we will outline that they concentrate around zero. To Then, using Lemma 8 below along with the properties of
express the matrices Bˆ(t,s) and Bˆ(t,s) we write for t∈[T] the notion of Lp concentration in Appendix A one can verify
f
that H holds and H implies H for any t′ > 1. This
ψˆ(t) =∆ ω(t)−r 0Bˆ(t) = (cid:88) r(s)Bˆ(t,s) (117) implies1 Proposition 1.t′−1 t′
1≤s≤t
f(t) = (cid:88) l(s)Bˆ(t,s) . (118) Lemma 8. Let f(γ;y) be differentiable and Lipschitz contin-
f uousw.r.tγ andf(0;Y)=O(1)whereY asin(11).Suppose
1≤s≤t (124) holds for each t∈[t′]. Then, for all 1≤s≤t≤t′
Recall that e.g. ⟨r(t),r(s)⟩ = δ I. Thus, from (117) (and
ts
resp. (118)) that Bˆ(t,s) (and resp. Bˆ(t,s)) satisfy the equations ⟨f(γ(t);y),f(γ(s);y)⟩= 1 C(t+1,s+1)+O(d−1 2) (128a)
α
of block Cholesky decomposition for 1≤s≤t≤T:
Bˆ(t,s)(Bˆ(s,s))⊤ =⟨ψˆ(t) ,ψˆ(s) ⟩− (cid:88) Bˆ(t,s′)(Bˆ(s,s′))⊤
⟨g 0,f(γ(t);y)⟩=E[G⊤ 0f(Γ(t);Y)]+O(d− 21 (1)
28b)
1≤s′<s
(119)
⟨g(s),f(γ(t);y)⟩=B(t,s)Q(t)+O(d− 21). (128c)
Bˆ(t,s)(Bˆ(s,s))⊤ =⟨f(t),f(s)⟩− (cid:88) Bˆ(t,s′)(Bˆ(s,s′))⊤ . In(128b)therandomvectors{G ,Γ(t)}areasinDefinition1.
f f f f 0
1≤s′<s
(120) Proof. By the Lipschitz property of f we have: f(γ(t);y)≃
Let H t′ denote the Hypothesis that for 1≤s≤t≤t′ f˜(t) where f˜(t) ∼ i.i.d. f(Γ(t);Y) for all t∈[t′] with
Bˆ(t,s) =B(t,s)+O(d−1 2) (121)
Γ(t) =G B(t)+
(cid:88)
G(s)B(t,s) ,
1 0
Bˆ(t,s) = √ B(t+1,s+1)+O(d−1 2) (122) 1≤s≤t
f α (cid:124) (cid:123)(cid:122) (cid:125)
Bˆ(t+1) =B(t+1)+O(d−1 2) (123) ∼Ψ(t)
γ(t) ≃g B(t)+ (cid:88) g(s)B(t,s) (124) where Ψ(t) as in Definition 1; and the condition f(0;Y) =
0 O(1) implies f(Γ(t);Y) = O(1). Third, as to the lat-
1≤s≤t
ter result (128c) we note that from Stein’s lemma that
ψˆ(t+1) ≃ (cid:88) z(s)B(t+1,s+1) . (125) E[(G(s))⊤f(Γ(t);Y)] = B(t,s)Q(t) . Then, the results (128)
1≤s≤t follow with the appropriate applications of Lemma 4.REFERENCES random access with a massive mimo receiver,” IEEE Transactions on
InformationTheory,vol.67,no.5,pp.2925–2951,2021.
[1] Y.Kabashima,T.Wadayama,andT.Tanaka,“Atypicalreconstruction [22] B. Çakmak, E. Gkiouzepi, M. Opper, and G. Caire, “Joint message
limitforcompressedsensingbasedonlp-normminimization,”Journal detectionandchannelestimationforunsourcedrandomaccessincell-
ofStatisticalMechanics:TheoryandExperiment,vol.2009,no.09,p. free user-centric wireless networks,” arXiv preprint arXiv:2304.12290,
L09003,2009. 2024.
[2] M. Bayati and A. Montanari, “The lasso risk for gaussian matrices,” [23] R.A.HornandC.R.Johnson,Matrixanalysis. Cambridgeuniversity
IEEE Transactions on Information Theory, vol. 58, no. 4, pp. 1997– press,2012.
2017,2012. [24] S. P. Boyd and L. Vandenberghe, Convex optimization. Cambridge
universitypress,2004.
[3] M.Vehkaperä,Y.Kabashima,andS.Chatterjee,“Analysisofregularized
[25] K. R. Davidson and S. J. Szarek, “Local operator theory, random
lsreconstructionandrandommatrixensemblesincompressedsensing,”
IEEE Transactions on Information Theory, vol. 62, no. 4, pp. 2100– matrices and banach spaces,” Handbook of the geometry of Banach
spaces,vol.1,no.317-366,p.131,2001.
2124,2016.
[26] Y. M. Lu, “Householder dice: A matrix-free algorithm for simulating
[4] C.Thrampoulidis,E.Abbasi,andB.Hassibi,“Preciseerroranalysisof
dynamicsongaussianandrandomorthogonalensembles,”IEEETrans-
regularized m -estimators in high dimensions,” IEEE Transactions on
actionsonInformationTheory,vol.67,no.12,pp.8264–8272,2021.
InformationTheory,vol.64,no.8,pp.5592–5628,2018.
[27] E. Gardner and B. Derrida, “Three unfinished works on the optimal
[5] C.Gerbelot,A.Abbara,andF.Krzakala,“Asymptoticerrorsforteacher-
storagecapacityofnetworks,”JournalofPhysicsA:Mathematicaland
studentconvexgeneralizedlinearmodels(or:Howtoprovekabashima’s
General,vol.22,no.12,p.1983,1989.
replica formula),” IEEE Transactions on Information Theory, vol. 69,
[28] L.Erdo˝sandH.-T.Yau,Adynamicalapproachtorandommatrixtheory.
no.3,pp.1824–1852,2023.
AmericanMathematicalSoc.,2017,vol.28.
[6] C. Thrampoulidis, S. Oymak, and M. Soltanolkotabi, “Theoretical
[29] D.P.PalomarandS.Verdú,“Gradientofmutualinformationinlinear
insights into multiclass classification: A high-dimensional asymptotic
vector gaussian channels,” IEEE Transactions on Information Theory,
view,”AdvancesinNeuralInformationProcessingSystems,vol.33,pp.
vol.52,no.1,pp.141–154,2005.
8907–8920,2020.
[30] J.R.MagnusandH.Neudecker,“Theeliminationmatrix:somelemmas
[7] C. Thrampoulidis, S. Oymak, and B. Hassibi, “Regularized linear
andapplications,”SIAMJournalonAlgebraicDiscreteMethods,vol.1,
regression: A precise analysis of the estimation error,” in Conference
no.4,pp.422–449,1980.
onLearningTheory. PMLR,2015,pp.1683–1709.
[31] R. M. McLeod, “Mean value theorems for vector valued functions,”
[8] B. Loureiro, G. Sicuro, C. Gerbelot, A. Pacco, F. Krzakala, and
ProceedingsoftheEdinburghMathematicalSociety,vol.14,no.3,pp.
L. Zdeborová, “Learning gaussian mixtures with generalized linear
197–209,1965.
models: Precise asymptotics in high-dimensions,” Advances in Neural
[32] Y. M. Lu, “Householder dice: A matrix-free algorithm for simulating
InformationProcessingSystems,vol.34,pp.10144–10157,2021.
dynamicsongaussianandrandomorthogonalensembles,”IEEETrans-
[9] M. Celentano and A. Montanari, “Fundamental barriers to high- actionsonInformationTheory,vol.67,no.12,pp.8264–8272,2021.
dimensionalregressionwithconvexpenalties,”TheAnnalsofStatistics,
[33] J. Qiu. Conditioning and Bolthausen’s Lemma. [Online].
vol.50,no.1,pp.170–196,2022. Available: https://www.jiazeqiu.com/uploads/1/3/6/1/136158820/stat_
[10] E. Cornacchia, F. Mignacco, R. Veiga, C. Gerbelot, B. Loureiro, and 217_section_6.pdf
L. Zdeborová, “Learning curves for the multi-class teacher-student
perceptron,”arXivpreprintarXiv:2203.12094,2022.
[11] S. Rangan, A. K. Fletcher, and V. K. Goyal, “Asymptotic analysis of
MAPestimationviathereplicamethodandapplicationstocompressed
sensing,”IEEETransactionsonInformationTheory,vol.58,no.3,pp.
1902–1923,mar2012.
[12] J.R. L.D.Almeida andD.J. Thouless,“Stabilityof theSherrington-
Kirkpatrick solution of a spin glass model,” Journal of Physics A:
MathematicalandGeneral,vol.11,no.5,p.983,1978.
[13] E. Bolthausen, “An iterative construction of solutions of the TAP
equations for the Sherrington–Kirkpatrick model,” Communications in
MathematicalPhysics,vol.325,no.1,pp.333–366.,October2014.
[14] M. Opper, B. Çakmak, and O. Winther, “A theory of solving TAP
equations for Ising models with general invariant random matrices,”
JournalofPhysicsA:MathematicalandTheoretical,vol.49,no.11,p.
114002,2016.
[15] B. Çakmak, Y. M. Lu, and M. Opper, “Analysis of random
sequential message passing algorithms for approximate inference,”
Journal of Statistical Mechanics: Theory and Experiment, vol.
2022, no. 7, p. 073401, jul 2022. [Online]. Available: https:
//dx.doi.org/10.1088/1742-5468/ac764a
[16] T. Takahashi and Y. Kabashima, “Macroscopic analysis of vector
approximate message passing in a model-mismatched setting,” IEEE
TransactionsonInformationTheory,2022.
[17] A. Javanmard and A. Montanari, “State evolution for general approxi-
matemessagepassingalgorithms,withapplicationstospatialcoupling,”
InformationandInference:AJournaloftheIMA,vol.2,no.2,pp.115–
144,2013.
[18] R. Berthier, A. Montanari, and P.-M. Nguyen, “State evolution for ap-
proximatemessagepassingwithnon-separablefunctions,”Information
andInference:AJournaloftheIMA,vol.9,no.1,pp.33–79,012019.
[19] X.Zhong,T.Wang,andZ.Fan,“Approximatemessagepassingforor-
thogonallyinvariantensembles:Multivariatenon-linearitiesandspectral
initialization,”arXivpreprintarXiv:2110.02318,2021.
[20] C.GerbelotandR.Berthier,“Graph-basedapproximatemessagepassing
iterations,”arXivpreprintarXiv:2109.11905,2021.
[21] A.Fengler,S.Haghighatshoar,P.Jung,andG.Caire,“Non-bayesianac-
tivitydetection,large-scalefadingcoefficientestimation,andunsourced