Graph Mamba: Towards Learning on Graphs with State Space Models
AliBehrouz*1 FarnooshHashemi*1
Abstract
GraphNeuralNetworks(GNNs)haveshownpromisingpotentialingraphrepresentationlearning. Themajority
ofGNNsdefinealocalmessage-passingmechanism,propagatinginformationoverthegraphbystackingmultiple
layers. These methods, however, are known to suffer from two major limitations: over-squashing and poor
capturingoflong-rangedependencies. Recently,GraphTransformers(GTs)emergedasapowerfulalternativeto
Message-PassingNeuralNetworks(MPNNs). GTs,however,havequadraticcomputationalcost,lackinductive
biasesongraphstructures,andrelyoncomplexPositional/StructuralEncodings(SE/PE).Inthispaper,weshow
thatwhileTransformers,complexmessage-passing,andSE/PEaresufficientforgoodperformanceinpractice,
neitherisnecessary. MotivatedbytherecentsuccessofStateSpaceModels(SSMs),suchasMamba,wepresent
GraphMambaNetworks(GMNs),ageneralframeworkforanewclassofGNNsbasedonselectiveSSMs. We
discussandcategorizethenewchallengeswhenadoptingSSMstograph-structureddata,andpresentfourrequired
andoneoptionalstepstodesignGMNs,wherewechoose(1)NeighborhoodTokenization,(2)TokenOrdering,
(3)ArchitectureofBidirectionalSelectiveSSMEncoder,(4)LocalEncoding,anddispensable(5)PEandSE.
WefurtherprovidetheoreticaljustificationforthepowerofGMNs. Experimentsdemonstratethatdespitemuch
lesscomputationalcost,GMNsattainanoutstandingperformanceinlong-range,small-scale,large-scale,and
heterophilicbenchmarkdatasets. Thecodeisinthislink.
1.Introduction
Recently,graphlearninghasbecomeanimportantandpopularareaofstudyduetoitsimpressiveresultsinawiderange
of applications, like neuroscience (Behrouz et al., 2023), social networks (Fan et al., 2019), molecular graphs (Wang
etal.,2021),etc. Inrecentyears,Message-PassingNeuralNetworks(MPNNs),whichiterativelyaggregateneighborhood
informationtolearnthenode/edgerepresentations,havebeenthedominantparadigminmachinelearningongraphs(Kipf
&Welling,2016;Velicˇkovic´ etal.,2018;Wuetal.,2020;Gutteridgeetal.,2023). They,however,havesomeinherent
limitations,includingover-squashing(DiGiovannietal.,2023),over-smoothing(Ruschetal.,2023),andpoorcapturingof
long-rangedependencies(Dwivedietal.,2022). WiththeriseofTransformerarchitectures(Vaswanietal.,2017)andtheir
successindiverseapplicationssuchasnaturallanguageprocessing(Wolfetal.,2020)andcomputervision(Liuetal.,2021),
theirgraphadaptations,so-calledGraphTransformers(GTs),havegainedpopularityasthealternativesofMPNNs(Yun
etal.,2019;Kimetal.,2022;Rampa´sˇeketal.,2022).
Graphtransformershaveshownpromisingperformanceinvariousgraphtasks,andtheirvariantshaveachievedtopscores
inseveralgraphlearningbenchmarks(Huetal.,2020;Dwivedietal.,2022). ThesuperiorityofGTsoverMPNNsisoften
explainedbyMPNNs’biastowardsencodinglocalstructures(Mu¨lleretal.,2023),whileakeyunderlyingprincipleof
GTsistoletnodesattendtoallothernodesthroughaglobalattentionmechanism(Kimetal.,2022;Yunetal.,2019),
allowing direct modeling of long-range interactions. Global attention, however, has weak inductive bias and typically
requiresincorporatinginformationaboutnodes’positionstocapturethegraphstructure(Rampa´sˇeketal.,2022;Kimetal.,
2022). Tothisend,variouspositionalandstructuralencodingschemesbasedonspectralandgraphfeatureshavebeen
introduced(Kreuzeretal.,2021;Kimetal.,2022;Limetal.,2023a).
DespitethefactthatGTswithproperpositionalencodings(PE)areuniversalapproximatorsandprovablymorepowerful
*Equalcontribution 1CornellUniversity,Ithaca,USA.Correspondenceto:AliBehrouz<ab2947@cornell.edu>,FarnooshHashemi
<sh2574@cornell.edu>.
Preprint.UnderReview.
1
4202
beF
31
]GL.sc[
1v87680.2042:viXraGraphMamba
thananyWLtest(Kreuzeretal.,2021),theirapplicabilitytolarge-scalegraphsishinderedbytheirpoorscalability. That
is,thestandardglobalattentionmechanismonagraphwithnnodesincursbothtimeandmemorycomplexityofO(n2),
quadraticintheinputsize,makingtheminfeasibleonlargegraphs. Toovercomethehighcomputationalcost,inspired
bylinearattentions(Zaheeretal.,2020),sparseattentionmechanismsongraphsattractsattention(Rampa´sˇeketal.,2022;
Shirzadetal.,2023). Forexample,Exphormer(Shirzadetal.,2023)suggestsusingexpandergraphs,globalconnectors,and
localneighborhoodsasthreepatternsthatcanbeincorporatedinGTs,resultinginasparseandefficientattention. Although
sparseattentionspartiallyovercomethememorycostofglobalattentions,GTsbasedonthesesparseattentions(Rampa´sˇek
etal.,2022;Shirzadetal.,2023)stillmightsufferfromquadratictimecomplexity. Thatis,theyrequirecostlyPE(e.g.,
Laplacianeigen-decomposition)andstructuralencoding(SE)toachievetheirbestperformance,whichcantakeO(n2)to
compute.
AnotherapproachtoimproveGTs’highcomputationalcostistousesubgraphtokenization(Chenetal.,2023;Zhaoetal.,
2021;Kuangetal.,2021;Baeketal.,2021;Heetal.,2023),wheretokens(a.k.apatches)aresmallsubgraphsextracted
withapre-definedstrategy. Typically,thesemethodsobtaintheinitialrepresentationsofthesubgraphtokensbypassing
themthroughanMPNN.Givenkextractedsubgraphs(tokens),thetimecomplexityofthesemethodsisO(k2),whichis
moreefficientthantypicalGTswithnodetokenization. Also,thesemethodsoftendonotrelyoncomplexPE/SE,astheir
tokens(subgraphs)inherentlycarryinductivebias. Thesemethods,however,havetwomajordrawbacks: (1)Toachieve
highexpressivepower,givenanode,usuallyrequireatleastasubgraphpereachremainingnode(Zhangetal.,2023a;
Bar-Shalometal.,2023),meaningthatk ∈O(n)andsothetimecomplexityisO(n2). (2)EncodingsubgraphsviaMPNNs
can transmit all their challenges of over-smoothing and over-squashing, limiting their applicability to heterophilic and
long-rangegraphs.
Recently,SpaceStateModels(SSMs),asanalternativeofattention-basedsequencemodelingarchitectureslikeTransformers
havegainedincreasingpopularityduetotheirefficiency(Zhangetal.,2023b;Nguyenetal.,2023). They,however,donot
achievecompetitiveperformancewithTransformersduetotheirlimitsininput-dependentcontextcompressioninsequence
models,causedbytheirtime-invarianttransitionmechanism. Tothisend,Gu&Dao(2023)present,Mamba,aselective
statespacemodelthatusesrecurrentscansalongwithaselectionmechanismtocontrolwhichpartofthesequencecanflow
intothehiddenstates. Thisselectioncansimplybeinterpretedasusingdata-dependentstatetransitionmechanism(See§2.3
foradetaileddiscussion). Mambaoutstandingperformanceinlanguagemodeling,outperformingTransformersofthesame
sizeandmatchingTransformerstwiceitssize,motivatesseveralrecentstudiestoadaptitsarchitecturefordifferentdata
modalities(Liuetal.,2024b;Yangetal.,2024;Zhuetal.,2024;Ahamed&Cheng,2024).
Mambaarchitectureisspecificallydesignedforsequencedataandthecomplexnon-causalnatureofgraphsmakesdirectly
applyingMambaongraphschallenging. Further,naturalattemptstoreplaceTransformerswithMambainexistingGTs
frameworks (e.g., GPS (Rampa´sˇek et al., 2022), TokenGT (Kim et al., 2022)) results in suboptimal performance in
botheffectivenessandtimeefficiency(See§5forevaluationand§3foradetaileddiscussion). Thereasonis,contraryto
Transformersthatallowseachnodetointeractwithalltheothernodes,Mamaba,duetoitsrecurrentnature,onlyincorporates
informationaboutprevioustokens(nodes)inthesequence. ThisintroducesnewchallengescomparedtoGTs: (1)Thenew
paradigmrequirestokenorderingthatallowsthemodeltakeadvantageoftheprovidedpositionalinformationasmuchas
possible. (2)Thearchitecturedesignneedtobemorerobusttopermutationthanapuresequentialencoder(e.g.,Mamba).
(3)WhilethequadratictimecomplexityofattentionscandominatethecostofPE/SEinGTs,complexPE/SE(withO(n2)
cost)canbeabottleneckforscalingGraphMambaonlargegraphs.
Contributions. Toaddressalltheabovementionedlimitations,wepresentGraphMambaNetworks(GMNs),anewclassof
machinelearningongraphsbasedonstatespacemodels(Figure1showstheschematicoftheGMNs). Insummaryour
contributionsare:
• RecipeforGraphMambaNetworks. WediscussnewchallengesofGMNscomparedtoGTsinarchitecturedesign
and motivate our recipe with four required and one optional steps to design GMNs. In particular, its steps are (1)
Tokenization,(2)TokenOrdering,(3)LocalEncoding,(4)BidirectionalSelectiveSSMEncoderanddispensable(5)PE
andSE.
• AnEfficientTokenizationforBridgingFrameworks. Literaturelacksacommonfoundationaboutwhatconstitutesa
goodtokenization. Accordingly,architecturesarerequiredtochooseeithernode-orsubgraph-leveltokenization,while
eachofwhichhasitsown(dis)advantages,dependingonthedata. Wepresentagraphtokenizationprocessthatnot
onlyisfastandefficient,butitalsobridgesthenode-andsubgraph-leveltokenizationmethodsusingasingleparameter.
2GraphMamba
Tokenization PE/SE Local Encoding Token Ordering Bidirectional Mamba
Random Walk For each node and 𝑚!=1,…,𝑚, PE/SE (1) Sum over the rows of MPNNs To vectorize 𝒎≥𝟏 Tokens have Robust to Permutation We
we sample 𝑀 walks with length 𝑚! and consider non-diagonal elements of the each token one can use implicit order due to scan the sequence of tokens
their induced subgraph as a token. For each 𝑚!, random walk matrix. message-passing to hierarchical structure.in two directions.
we repeat the process 𝑠 times. (2) Eigenvectors of the Laplacian. incorporate local ⊕
(3) Anonymous random walk information.
𝒎 = 𝟎: Each node is an independent token. encoding, i.e., counting the 𝒎=𝟎 Sort nodes
number of times a node appears at RWF Given the walks based on PRR/Degree.
A tol klo ew nis z s aw tii ot nc h ui sn ig n gb e at w sie ne gn le n po ad re a a mn ed t es ru b 𝑚g ,r aph a certain position. t sh ua bt g rc ao prr he ,s op no en d cas nt o u a s e Implicit order +
m hya pk ei rn pg a t rh ae m c eh teo ric de uo rf i nto gk te rn ai iz na it ni gon . a tunable R die ffl ea rt ei nv ce e P ofE P/S EE /S EU s ai sn eg d p ga ei r fe-w ati use r es. l oo fc na ol did ese n tt oi t vy e cr te ol ra it zi eo n it . (𝑚$=𝑚) (𝑚$=1) × ×
… … …
For each Token:
𝜎
s s
𝜎 𝜎
… … … … 𝒎=𝟎
Concatenation
…
s subgraphs s subgraphs s subgraphs small large
(𝑚!=1) (𝑚!=2) (𝑚!=𝑚) PRR/Degree/… MPNN
⊕ PNA
Gated GCN
Long Sequence Mamba shows performance Optional PE/SE When using Features When node Domain Knowledge ⊕ Sum/Concatenation GINE
improvement with longer sequences, and so we subgraph tokens (i.e., 𝑚!≥1), PE/SE or edge features are One can use domain
use parameter s to control the length of the is optional. That is, tokens have available, one can knowledge (when 𝜎 Activation Function
subgraph sequence. Based on the dataset, one their own inductive bias, and do not concatenate them with adapting GMs to Linear Layer
can tune 𝑠 to achieve better results. need additional information about the PE/SE, before the specific domain) or 1-d Convolution
the graph structure. local encoding step. structural properties Selective SSM
like Personalized Required Step
PageRank or degree.
Optional Step
Figure1.SchematicoftheGMNswithfourrequiredandoneoptionalsteps: (1)Tokenization: thegraphismappedintoasequence
oftokens(m ≥ 1: subgraphandm = 0: nodetokenization)(2)(OptionalStep)PE/SE:inductivebiasisaddedtothearchitecture
usinginformationaboutthepositionofnodesandthestrucutreofthegraph. (3)LocalEncoding: localstructuresaroundeachnode
areencodedusingasubgraphvectorizationmechanism.(4)TokenOrdering:thesequenceoftokensareorderedbasedonthecontext.
(Subgraphtokenization(m≥1)hasimplicitorderanddoesnotneedthisstep).(5)(Stackof)BidirectionalMamba:itscansandselects
relevantnodesorsubgraphstoflowintothehiddenstates.†Inthisfigure,thelastlayerofbidirectionalMamba,whichperformsasa
readoutonallnodes,isomittedforsimplicity.
Moreover,thepresentedtokenizationhasimplicitorder,whichisspeciallyimportantforsequentialencoderslikeSSMs.
• NewBidirectionalSSMsforGraphs. InspiredbyMamba,wedesignaSSMarchitecturethatscanstheinputsequence
intwodifferentdirections,makingthemodelmorerobusttopermutation,whichisparticularlyimportantwhenwedo
notuseimplicitlyorderedtokenizationongraphs.
• TheoreticalJustification. WeprovidetheoreticaljustificationforthepowerofGMNsandshowthattheyareuniversal
approximatorofanyfunctionsongraphs. WefurthershowthatGMNsusingproperPE/SEismoreexpressivethanany
WLtest,matchingGTsinthismanner.
• Outstanding Performance and New Insights. Our experimental evaluations demonstrate that GMNs attain an
outstandingperformanceinlong-range,small-scale,large-scale,andheterophilicbenchmarkdatasets,whileconsuming
lessGPUmemory. TheseresultsshowthatwhileTransformers,complexmessage-passing,andSE/PEaresufficientfor
goodperformanceinpractice,neitherisnecessary. Wefurtherperformablationstudyandvalidatethecontributionof
eacharchitecturalchoice.
2.RelatedWorkandBackgrounds
TosituateGMNsinabroadercontext,wediscussfourrelevanttypesofmachinelearningmethods:
2.1.Message-PassingNeuralNetworks
Message-passingneuralnetworksareaclassofGNNsthatiterativelyaggregatelocalneighborhoodinformationtolearnthe
node/edgerepresentations(Kipf&Welling,2016).MPNNshavebeenthedominantparadigminmachinelearningongraphs,
andattractsmuchattention,leadingtovariouspowerfularchitectures,e.g.,GAT(Velicˇkovic´ etal.,2018),GCN(Henaff
3
stnioP
yeKGraphMamba
etal.,2015;Kipf&Welling,2016),GatedGCN(Bresson&Laurent,2017),GIN(Xuetal.,2019),etc. SimpleMPNNs,
however,areknowntosufferfromsomemajorlimitationsincluding:(1)limitingtheirexpressivitytothe1-WLisomorphism
test(Xuetal.,2019),(2)over-smoothing(Ruschetal.,2023),and(3)over-squashing(Alon&Yahav,2021;DiGiovanni
etal.,2023). VariousmethodshavebeendevelopedtoaugmentMPNNsandovercomesuchissues,includinghigher-order
GNNs(Morrisetal.,2019;2020),graphrewiring(Gutteridgeetal.,2023;Arnaiz-Rodr´ıguezetal.,2022),addaptiveand
cooperativeGNNs(Erricaetal.,2023;Finkelshteinetal.,2023),andusingadditionalfeatures(Satoetal.,2021;Murphy
etal.,2019).
2.2.GraphTransformers
WiththeriseofTransformerarchitectures(Vaswanietal.,2017)andtheirsuccessindiverseapplicationssuchasnatural
languageprocessing(Wolfetal.,2020)andcomputervision(Liuetal.,2021),theirgraphadaptationshavegainedpopularity
asthealternativesofMPNNs(Yunetal.,2019;Kimetal.,2022;Rampa´sˇeketal.,2022). Usingafullglobalattention,
GTs consider each pair of nodes connected (Yun et al., 2019) and so are expected to overcome the problems of over-
squashingandover-smoothinginMPNNs(Kreuzeretal.,2021). GTs,however,haveweakinductivebiasandneedsproper
positional/structuralencodingtolearnthestructureofthegraph(Kreuzeretal.,2021;Rampa´sˇeketal.,2022). Tothisend,
variousstudieshavefocusedondesigningpowerfulpositionalandstructuralencodings(Wangetal.,2022;Yingetal.,2021;
Kreuzeretal.,2021;Shiv&Quirk,2019).
SparseAttention. WhileGTshaveshownoutstandingperformanceindifferentgraphtasksonsmall-scaledatasets(up
to10Knodes),theirquadraticcomputationalcost,causedbytheirfullglobalattention,haslimitedtheirapplicabilityto
large-scalegraphs(Rampa´sˇeketal.,2022). Motivatedbylinearattentionmechanisms(e.g.,BigBird(Zaheeretal.,2020)
andPerformer(Choromanskietal.,2021)),whicharedesignedtoovercomethesamescalabilityissueofTransformerson
longsequences,usingsparseTransformersinGTarchitectureshasgainedpopularity(Rampa´sˇeketal.,2022;Shirzadetal.,
2023;Kongetal.,2023;Liuetal.,2023;Wuetal.,2023). ThemainideaofsparseGTsmodelsistorestricttheattention
pattern,i.e.,thepairsofnodesthatcaninteractwitheachother. Asanexample,Shirzadetal.(2023)presentExphormer,
thegraphadaptionofBigBirdthatusesthreesparsepatternsof(1)expandergraphattention,(2)localattentionamong
neighbors,and(3)globalattentionbyconnectingvirtualnodestoallnon-virtualnodes.
SubgraphTokenization. AnothermethodtoovercomeGTs’highcomputationalcostistousesubgraphtokenization(Chen
et al., 2023; Zhao et al., 2021; Baek et al., 2021; He et al., 2023), where tokens are small subgraphs extracted with a
pre-definedstrategy. Thesesubgraphtokenizationstrategiesusuallyarek-hopneighborhood(givenafixedk)(Nguyenetal.,
2022a;Hussainetal.,2022;Parketal.,2022),learnablesampleofneighborhood(Zhangetal.,2022),ego-networks(Zhao
etal.,2021),hierarchicalk-hopneighborhoods(Chenetal.,2023),graphmotifs(Rongetal.,2020),graphpartitions(He
etal.,2023). Tovectorizeeachtoken,subgraph-basedGTmethodstypicallyrelyonMPNNs,makingthemvulnerableto
over-smoothingandover-squashing. Mostofthemalsouseafixedneighborhoodofeachnode,missingthehierarchical
structureofthegraph.TheonlyexceptionisNAGphormer(Chenetal.,2023)thatusesallk =1,...,K-hopneighborhoods
ofeachnodeasitscorrespondingtokens. Althoughthistokenizationletsthemodellearnthehierarchicalstructureofthe
graph,byincreasingthehopoftheneighborhood,itstokensbecomeexponentiallylarger,limitingitsabilitytoscaletolarge
graphs.
2.3.StateSpaceModels
StateSpaceModels(SSMs),atypeofsequencemodels,areusuallyknownaslineartime-invariantsystemsthatmapinput
sequence x(t) ∈ RL to response sequence y(t) ∈ RL (Aoki, 2013). Specifically, SSMs use a latent state h(t) ∈ RN,
evolutionparameterA∈RN×N,andprojectionparametersB∈RN×1,C∈R1×N suchthat:
h′(t)=Ah(t)+Bx(t),
y(t)=Ch(t). (1)
Duetothehardnessofsolvingtheabovedifferentialequationindeeplearningsettings,discretespacestatemodels(Gu
etal.,2020;Zhangetal.,2023b)discretizetheabovesystemusingaparameter∆:
h =A¯ h +B¯ x ,
t t−1 t
y =Ch , (2)
t t
4GraphMamba
where
A¯ =exp(∆A),
B¯ =(∆A)−1(exp(∆A−I)) .∆B. (3)
Guetal.(2020)showsthatdiscrete-timeSSMsareequivalenttothefollowingconvolution:
K¯ =(cid:0) C¯B¯,C¯A¯B¯,...,C¯A¯L−1B¯(cid:1) ,
y =x∗K¯, (4)
andaccordinglycanbecomputedveryefficiently. Structuredstatespacemodels(S4),anothertypeofSSMs,areefficient
alternativesofattentionsandhaveimprovedefficiencyandscalabilityofSSMsusingreparameterization(Guetal.,2022;
Fuetal.,2023;Nguyenetal.,2023). SSMsshowpromisingperformanceontimeseriesdata(Zhangetal.,2023b;Tang
etal.,2023),Genomicsequence(Nguyenetal.,2023),healthcaredomain(Guetal.,2021),andcomputervision(Guetal.,
2021;Nguyenetal.,2022b). They,however,lackselectionmechanism,causingmissingthecontextasdiscussedbyGu&
Dao(2023). Recently,Gu&Dao(2023)introduceanefficientandpowerfulselectivestructuredstatespacearchitecture,
calledMAMBA,thatusesrecurrentscansalongwithaselectionmechanismtocontrolwhichpartofthesequencecanflow
into the hidden states. The selection mechanism of Mamba can be interpreted as using data-dependent state transition
mechanisms,i.e.,makingB,C,and∆asfunctionofinputx . Mambaoutstandingperformanceinlanguagemodeling,
t
outperformingTransformersofthesamesizeandmatchingTransformerstwiceitssize,motivatesseveralrecentstudiesto
adaptitsarchitecturefordifferentdatamodalitiesandtasks(Liuetal.,2024b;Yangetal.,2024;Zhuetal.,2024;Ahamed&
Cheng,2024;Xingetal.,2024;Liuetal.,2024a;Maetal.,2024).
3.Challenges&Motivations: TransformersvsMamba
Mambaarchitectureisspecificallydesignedforsequencedataandthecomplexnon-causalnatureofgraphsmakesdirectly
applyingMambaongraphschallenging. BasedonthecommonapplicabilityofMambaandTransformersontokenized
sequentialdata,astraightforwardapproachtoadaptMambaforgraphsistoreplaceTransformerswithMambainGTs
frameworks,e.g.,TokenGT(Kimetal.,2022)orGPS(Rampa´sˇeketal.,2022). However,thisapproachmightnotfully
takeadvantageofselectiveSSMsduetoignoringsomeoftheirspecialtraits. Inthissection,wediscussnewchallengesfor
GMNscomparedtoGTs.
Sequencesvs2-DData. Itisknownthattheself-attentivearchitecturecorrespondstoafamilyofpermutationequivariant
functions(Leeetal.,2019;Liuetal.,2020).Thatis,theattentionmechanisminTransformers(Vaswanietal.,2017)assumes
aconnectionbetweeneachpairoftokens,regardlessoftheirpositionsinthesequence,makingitpermutationequivariant.
Accordingly,Transformerslackinductivebiasandsoproperlypositionalencodingiscrucialfortheirperformance,whenever
theorderoftokensmatter(Vaswanietal.,2017;Liuetal.,2020). Ontheotherhand,Mambaisasequentialencoderand
scanstokensinarecurrentmanner(potentiallylesssensitivetopositionalencoding). Thus,itexpectscausaldataasaninput,
makingitchallengingtobeadaptedto2-D(e.g.,images)(Liuetal.,2024b)orcomplexgraph-structureddata. Accordingly,
whileingraphadaptionofTransformersmappingthegraphintoasequenceoftokensalongwithapositional/structural
encodingswereenough,sequentialencoders,likeSSMs,andmorespecificallyMamba,requireanorderingmechanismfor
tokens.
AlthoughthissensitivitytotheorderoftokensmakestheadaptionofSSMstographschallenging,itcanbemorepowerful
whenevertheordermatters. Forexample,learningthehierarchicalstructuresintheneighborhoodofeachnode(k-hops
for k = 1,...,K), which is implicitly ordered, is crucial in different domains (Zhong et al., 2022; Lim et al., 2023b).
Moreover,itprovidestheopportunitytousedomainknowledgewhentheordermatters(Yuetal.,2020). Inourproposed
framework,weprovidetheopportunityforbothcases:(1)usingdomainknowledgeorstructuralproperties(e.g.,Personalized
PageRank(Pageetal.,1998))whentheordermatters,or(2)usingimplicitlyorderedsubgraphs(noorderingisneeded).
Furthermore, our bidirectional encoder scans nodes in two different directions, being capable of learning equivariance
functionsontheinput,wheneveritisneeded.
Long-rangeSequenceModeling. Ingraphdomain,thesequenceoftokens,eithernode,edge,orsubgraph,canbecounted
as the context. Unfortunately, Transformer architecture, and more specifically GTs, are not scalable to long sequence.
Furthermore,intuitively,morecontext(i.e.,longersequence)shouldleadtobetterperformance;however,recentlyithas
beenempiricallyobservedthatmanysequencemodelsdonotimprovewithlongercontextinlanguagemodeling(Shietal.,
5GraphMamba
2023).Mamba,becauseofitsselectionmechanism,cansimplyfilterirrelevantinformationandalsoresetitsstateatanytime.
Accordingly,itsperformanceimprovesmonotonicallywithsequencelength(Gu&Dao,2023). Tothisend,andtofully
takeadvantageofMamba,onecanmapagraphornodetolongsequences,possiblybagsofvarioussubgraphs. Notonlythe
longsequenceoftokenscanprovidemorecontext,butitalsopotentiallycanimprovetheexpressivepower(Bevilacqua
etal.,2022).
Scalability. Duetothecomplexnatureofgraph-structureddata,sequentialencoders,includingTransformersandMamba,
requireproperpositionalandstructuralencodings(Rampa´sˇeketal.,2022;Kimetal.,2022). ThesePEs/SEs,however,often
havequadraticcomputationalcost,whichcanbecomputedoncebeforetraining. Accordingly,duetothequadratictime
complexityofTransformers,computingthesePEs/SEswasdominatedandtheyhavenotbeenthebottleneckfortraining
GTs. GMNs,ontheotherhand,havelinearcomputationalcost(withrespecttobothtimeandmemory),andsoconstructing
complexPEs/SEscanbetheirbottleneckwhentrainingonverylargegraphs. ThisbringanewchallengeforGMNs,asthey
needtoeither(1)donotusePEs/SEs,or(2)usetheirmoreefficientvariantstofullytakeadvantageofSSMsefficiency.
OurarchitecturedesignmaketheuseofPE/SEoptionalandourempiricalevaluationshowsthatGMNswithoutPE/SEcan
achievecompetitiveperformancecomparedtomethodswithcomplexPEs/SEs.
NodeorSubgraph? Inadditiontotheabovenewchallenges,thereisalackofcommonfoundationaboutwhatconstitutesa
goodtokenization,andwhatdifferentiatesthem,eveninGTframeworks. Existingmethodsuseeithernode/edge(Shirzad
etal.,2023;Rampa´sˇeketal.,2022;Kimetal.,2022),orsubgraphtokenizationmethods(Chenetal.,2023;Zhaoetal.,
2021;Heetal.,2023). Whilemethodswithnodetokenizationaremorecapableofcapturinglong-rangedependencies,
methodswithsubgraphtokenshavemoreabilitytolearnlocalneighborhoods,arelessrelyonPE/SE(Chenetal.,2023),
andaremoreefficientinpractice. Ourarchitecturedesignletsswitchingbetweennodeandsubgraphtokenizationusinga
singleparameterm,makingthechoiceoftokenizationatunablehyperparameterduringtraining.
4.GraphMambaNetworks
Inthissection,weprovideourfive-steprecipeforpowerful,flexible,andscalableGraphMambaNetworks. Following
thediscussionabouttheimportanceofeachstep,wepresentourarchitecture. TheoverviewoftheGMNframeworkis
illustratedinFigure1.
Throughoutthissection,weletG=(V,E)beagraph,whereV ={v ,...,v }isthesetofnodesandE ⊆V ×V isthe
1 n
setofedges. Weassumeeachnodev ∈V hasafeaturevectorx(0) ∈X,whereX∈Rn×disthefeaturematrixdescribing
v
theattributeinformationofnodesanddisthedimensionoffeaturevectors. Givenv ∈V,weletN(v)={u|(v,u)∈E}
bethesetofv’sneighbors. GivenasubsetofnodesS ⊆V,weuseG[S]todenotetheinducedsubgraphconstructedby
nodesinS,andX todenotethefeaturematrixdescribingtheattributeinformationofnodesinS.
S
4.1.TokenizationandEncoding
Tokenization, which is the process of mapping the graph into a sequence of tokens, is an inseparable part of adapting
sequentialencoderstographs. Asdiscussedearlier,existingmethodsuseeithernode/edge(Shirzadetal.,2023;Rampa´sˇek
etal.,2022;Kimetal.,2022),orsubgraphtokenizationmethods(Chenetal.,2023;Zhaoetal.,2021;Heetal.,2023),
eachofwhichwithitsown(dis)advantages. Inthispart,wepresentanewsimplebutflexibleandeffectiveneighborhood
samplingforeachnodeanddiscussitsadvantagesoverexistingsubgraphtokenization. Themainandhigh-levelideaofour
tokenizationistofirst,samplesomesubgraphsforeachnodethatcanrepresentthenode’sneighborhoodstructureaswellas
itslocal,andglobalpositionsinthegraph. Thenwevectorize(encode)thesesubgraphstoobtainthenoderepresentations.
NeighborhoodSampling. Givenanodev ∈V,andtwointegersm,M ≥0,foreach0≤mˆ ≤m,wesampleM random
walksstartedfromvwithlengthmˆ. LetT (v)fori=0,...,M bethesetofvisitednodesinthei-thwalk. Wedefinethe
mˆ,i
tokencorrespondstoallwalkswithlengthmˆ as:
(cid:34)M (cid:35)
(cid:91)
G[T (v)]=G T (v) , (5)
mˆ mˆ,i
i=1
which is the union of all walks with length mˆ. One can interpret G[T (v)] as the induced subgraph of a sample of
mˆ
mˆ-hopneighborhoodofnodev. Attheend,foreachnodev ∈V wehavethesequenceofG[T (v)],...,G[T (v)]asits
0 m
correspondingtokens.
6GraphMamba
Usingrandomwalks(withfixedlength)ork-hopneighborhoodofanodeasitsrepresentativetokenshasbeendiscussedin
severalrecentstudies(Dingetal.,2023;Zhangetal.,2022;Chenetal.,2023;Zhaoetal.,2021). Thesemethods,however,
sufferfromasubsetoftheselimitations: (1)theyuseafixed-lengthrandomwalk(Kuangetal.,2021),whichmissesthe
hierarchicalstructureofthenode’sneighborhood. Thisisparticularlyimportantwhenthelong-rangedependenciesofnodes
areimportant. (2)theyuseallnodesinallk-hopneighborhoods(Chenetal.,2023;Dingetal.,2023),resultinginatrade-off
betweenlong-rangedependenciesandover-smoothingorover-squashingproblems. Furthermore,thek-hopneighborhoodof
awell-connectednodemightbethewholegraph,resultinginconsideringthegraphasatokenofanode,whichisinefficient.
Ourneighborhoodsamplingapproachaddressesalltheselimitations. Itsampledthefixednumberofrandomwalkswith
differentlengthsforallnodes,capturinghierarchicalstructureoftheneighborhoodwhileavoidingbothinefficiency,caused
byconsideringtheentiregraph,andover-smoothingandoversquashing,causedbylargeneighborhoodaggregation.
WhyNotMoreSubgraphs? Asdiscussedearlier,empiricalevaluationhasshownthattheperformanceofselectivestate
spacemodelsimprovesmonotonicallywithsequencelength(Gu&Dao,2023). Furthermore,theirlinearcomputational
costallowustousemoretokens,providingthemmorecontext. Accordingly,tofullytakeadvantageofselectivestatespace
models,givenanintegers>0,werepeattheaboveneighborhoodsamplingprocessforstimes. Accordingly,foreachnode
v ∈V wehaveasequenceof
G[T (v)],G[T1(v)],...,G[Ts(v)],...,G[T1(v)],...,G[Ts(v)]
0 1 1 m m
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
stimes stimes
asitscorrespondingsequenceoftokens. Here, wecanseeanotheradvantageofourproposedneighborhoodsampling
comparedtoChenetal.(2023);Dingetal.(2023). WhileinNAGphormer(Chenetal.,2023)thesequencelengthofeach
nodeislimitedbythediameterofthegraph,ourmethodcanproducealongsequenceofdiversesubgraphs.
Theorem4.1. WithlargeenoughM,m,ands>0,GMNs’neighborhoodsamplingisstrictlymoreexpressivethank-hop
neighborhoodsampling.
Structural/PositionalEncoding. TofurtheraugmentourframeworkforGraphMamba,weconsideranoptionalstep,when
weinjectstructuralandpositionalencodingstotheinitialfeaturesofnodes/edges. PEismeanttoprovideinformationabout
thepositionofagivennodewithinthegraph. Accordingly,twoclosenodeswithinagraphorsubgrapharesupposedtohave
closePE.SE,ontheotherhand,ismeanttoprovideinformationaboutthestructureofasubgraph. FollowingRampa´sˇek
etal.(2022),weconcatenateeithereigenvectorsofthegraphLaplacianorRandom-walkstructuralencodingstothenodes’
feature,wheneverPE/SEareneeded: i.e.,
x(new) =x ||p , (6)
v v v
wherep isthecorrespondingpositionalencodingtov. Forthesakeofconsistency,weusex insteadofx throughoutthe
v v v
paper.
NeighborhoodEncoding. Givenanodev ∈V anditssequenceoftokens(subgraphs),weencodethesubgraphviaencoder
ϕ(.). Thatis,weconstructx1,x2,...,xms−1,xms ∈Rdasfollows:
v v v v
(cid:16) (cid:17)
x((i−1)s+j) =ϕ G[Tj(v)],X , (7)
v i Tj(v)
i
where1≤i≤mand1≤j ≤s. Inpractice,thisencodercanbeanMPNN,(e.g.,Gated-GCN(Bresson&Laurent,2017)),
orRWF(To¨nshoffetal.,2023)thatencodesnodeswithrespecttoasampledsetofwalksintofeaturevectorswithfour
parts: (1)nodefeatures,(2)edgefeaturesalongthewalk,and(3,4)localstructuralinformation.
TokenOrdering. ByEquation7,wecancalculatetheneighborhoodembeddingsforvarioussampledneighborhoodsofa
nodeandfurtherconstructasequencetorepresentitsneighborhoodinformation,i.e.,x1,x2,...,xms−1,xms. Asdiscussed
v v v v
in§3,adoptionofsequencemodelslikeSSMstograph-structureddatarequiresanorderonthetokens. Tounderstandwhat
constitutesagoodordering,weneedtorecallselectionmechanisminMamba(Gu&Dao,2023)(wewilldiscussselection
mechanismmoreformallyin§4.2). MambabymakingB,C,and∆asfunctionsofinputx (see§2.3fornotations)letsthe
t
modelfilterirrelevantinformationandselectimportanttokensinarecurrentmanner,meaningthateachtokengetsupdated
basedontokensthatcomebeforetheminthesequence. Accordingly,earliertokenshavelessinformationaboutthecontext
ofsequence,whilelatertokenshaveinformationaboutalmostentiresequence. Thisleadsustoordertokensbasedoneither
theirneedsofknowinginformationaboutothertokensortheirimportancetoourtask.
7GraphMamba
Whenm≥1: For the sake of simplicity first let s = 1. In the case that m ≥ 1, interestingly, our architecture design
providesuswithanimplicitlyorderedsequence. Thatis,givenv ∈V,thei-thtokenisasamplesfromi-hopneighborhood
ofnodev,whichisthesubgraphofallj-hopneighborhoods,wherej ≥i. Thismeans,givenalargeenoughM (numberof
sampledrandomwalks),ourT (v)hasenoughinformationaboutT (v),notviceversa. Tothisend,weusethereverseof
j i
initialorder,i.e.,xm,xm−1,...,x2,x1. Accordingly,innersubgraphscanalsohaveinformationabouttheglobalstructure.
v v v v
Whens≥2,weusethesameprocedureasabove,andreversetheinitialorder,i.e.,xsm,xsm−1,...,x2,x1. Tomakeour
v v v v
modelrobusttothepermutationofsubgraphswiththesamewalklengthmˆ,werandomlyshufflethem. Wewilldiscussthe
orderinginthecaseofm=0later.
4.2.BidirectionalMamba
Asdiscussedin§3,SSMsarerecurrentmodelsandrequireorderedinput,whilegraph-structureddatadoesnothaveany
orderandneedspermutationequivariantencoders. Tothisend,inspiredbyVimincomputervision(Zhuetal.,2024),we
modifyMambaarchitectureandusetworecurrentscanmodulestoscandataintwodifferentdirections(i.e.,forwardand
backward). Accordingly,giventwotokenst andt ,wherei>j andindicesshowtheirinitialorder,inforwardscant
i j i
comesaftert andsohastheinformationaboutt (whichcanbeflownintothehiddenstatesorfilteredbytheselection
j j
mechanism). In backward pass t comes after t and so has the information about t . This architecture is particularly
j i i
importantwhenm=0(nodetokenization),whichwewilldiscusslater.
More formally, in forward pass module, let Φ be the input sequence (e.g., given v, Φ is a matrix whose rows are
xsm,xsm−1,...,x1,calculatedinEquation7),Abetherelativepositionalencodingoftokens,wehave:
v v v
Φ =σ(Conv(W LayerNorm(Φ))),
input input
B=W Φ , C=W Φ , ∆=Softplus(W Φ ),
B input C input ∆ input
A¯ =Discrete (A,∆),
A
B¯ =Discrete (B,∆),
B
y =SSM A¯,B¯,C(Φ input),
y =W (y⊙σ(W LayerNorm(Φ))), (8)
forward forward,1 forward,2
where W,W ,W ,W ,W and W are learnable parameters, σ(.) is nonlinear function (e.g., SiLU),
B C ∆ forward,1 forward,2
LayerNorm(.)islayernormalization(Baetal.,2016),SSM(.)isthestatespacemodeldiscussedinEquations2and4,and
Discrete(.)isdiscretizationprocessdiscussedinEquation3. Weusethesamearchitectureasaboveforthebackward
pass(withdifferentweights)butinsteadweuseΦ astheinput,whichisamatrixwhoserowsarex1,x2,...,xsm. Let
inverse v v v
y betheoutputofthisbackwardmodule,weobtainthefinalencodingsas
backward
y =W (y +y ). (9)
output out forward backward
Inpractice,westacksomelayersofthebidirectionalMambatoachievegoodperformance. Notethatduetoourordering
mechanism,thelaststateoftheoutputcorrespondstothewalkwithlengthmˆ =0,i.e.,thenodeitself. Accordingly,thelast
staterepresentstheupdatednodeencoding.
AugmentationwithMPNNs. WefurtheruseanoptionalMPNNmodulethatsimultaneouslyperformsmessage-passing
andaugmentstheoutputofthebidirectionalMambaviaitsinductivebias. Particularlythismoduleisveryhelpfulwhen
therearerichedgefeaturesandsoanMPNNcanhelptotakeadvantageofthem. Whileinourempiricalevaluationweshow
thatthismoduleisnotnecessaryforthesuccessofGMNsinseveralcases,itcanbeusefulwhenweavoidcomplexPE/SE
andstronginductivebiasisneeded.
HowDoesSelectionWorkonSubgraphs? Asdiscussedearlier,theselectionmechanismcanbeachievedbymaking
B,C,and∆asthefunctionsoftheinputdata(Gu&Dao,2023). Accordingly,inrecurrentscan,basedontheinput,the
modelcanfiltertheirrelevantcontext. TheselectionmechanisminEquation9isimplementedbymakingB,C,and∆as
functionsofΦ ,whichismatrixoftheencodingsofneighborhoods. Therefore,asmodelscansthesampledsubgraphs
input
fromthei-hopneighborhoodsindescendingorderofi,itfiltersirrelevantneighborhoodstothecontext(laststate),whichis
thenodeencoding.
LastLayer(s)ofBidirectionalMamba. Tocapturethelong-rangedependenciesandtoflowinformationacrossthenodes,
weusethenodeencodingsobtainedfromthelaststateofEquation9astheinputofthelastlayer(s)ofbidirectionalMamba.
8GraphMamba
Therefore,therecurrentscanofnodes(inbothdirections)canflowinformationacrossnodes. Thisdesignnotonlyhelps
capturinglong-rangedependenciesinthegraph,butitalsoisakeytotheflexibilityofourframeworktobridgenodeand
subgraphtokenization.
4.3.TokenizationWhenm=0
Inthiscase,foreachnodev ∈V weonlyconsidervitselfasitscorrespondingsequenceoftokens.Basedonourarchitecture,
inthiscase,thefirstlayersofbidirectionMambabecomesimpleprojectionasthelengthofthesequenceisone. However,
thelastlayers,whereweusenodeencodingsastheirinput,treatsnodesastokensandbecomeanarchitecturethatusea
sequentialencoder(e.g.,Mamba)withnodetokenization. Morespecifically,inthisspecialcaseofframework,themodelis
theadaptionofGPS(Rampa´sˇeketal.,2022)framework,whenwereplaceitsTransformerwithourbidirectionalMamba.
Thisarchitecturedesignletsswitchingbetweennodeandsubgraphtokenizationusingasingleparameterm,makingthe
choiceoftokenizationatunablehyperparameterduringtraining. Notethatthisflexibilitycomesmorefromourarchitecture
ratherthanthemethodoftokenization. Thatis,inpracticeonecanuseonly0-hopneighborhoodinNAGphormer(Chen
etal.,2023),resultinginonlyconsideringthenodeitself. However,inthiscase,thearchitectureofNAGphormerbecomesa
stackofMLPs,resultinginpoorperformance.
TokenOrdering. Whenm=0: Oneremainingquestionishowonecanordernodeswhenweusenodetokenization. As
discussedin§4.1,tokensneedtobeorderedbasedoneither(1)theirneedsofknowinginformationaboutothertokens
or(2)theirimportancetoourtask. Whendealingwithnodesandspecificallywhenlong-rangedependenciesmatter,(1)
becomesamustforallnodes. Ourarchitectureovercomesthischallengebyitsbidirectionalscanprocess. Therefore,we
needtoordernodesbasedontheirimportance. Thereareseveralmetricstomeasuretheimportanceofnodesinagraph.
Forexample,variouscentralitymeasures(Latora&Marchiori,2007;Ruhnau,2000),degree,k-core(Lick&White,1970;
Hashemietal.,2022),PersonalizedPageRankorPageRank(Pageetal.,1998),etc. Inourexperiments,forthesakeof
efficiencyandsimplicity,wesortnodesbasedontheirdegree.
HowDoesSelectionWorkonNodes? Similartoselectionmechanismonsubgraphs,themodelbasedontheinputdatacan
filterirrelevanttokens(nodes)tothecontext(downstreamtasks).
4.4.TheoreticalAnalysisofGMNs
Inthissection,weprovidetheoreticaljustificationforthepowerofGMNs. Morespecifically,wefirstshowthatGMNsare
universalapproximatorofanyfunctionongraphs. Next,wediscussthatgivenproperPEandenoughparameters,GMNs
aremorepowerfulthananyWLisomorphismtest,matchingGTs(withthesimilarassumptions). Finally,weevaluatethe
expressivepowerofGMNswhentheydonotuseanyPEorMPNNandshowthattheirexpressivepowerisunbounded
(mightbeincomparable).
Theorem 4.2 (Universality). Let 1 ≤ p < ∞, and ϵ > 0. For any continues function f : [0,1]d×n → Rd×n that is
permutationequivariant,thereexistsaGMNwithpositionalencoding,g ,suchthatℓp(f,g)<ϵ.
p
Theorem4.3(ExpressivePowerw/PE). Giventhefullsetofeigenfunctionsandenoughparameters,GMNscandistinguish
anypairofnon-isomorphicgraphsandaremorepowerfulthananyWLtest.
WeprovetheabovetwotheoremsbasedontherecentworkofWang&Xue(2023), wheretheyprovethatSSMswith
layer-wisenonlinearityareuniversalapproximatorsofanysequence-to-sequencefunction.
Theorem4.4(ExpressivePowerw/oPEandMPNN). Withenoughparameters,foreveryk ≥1therearegraphsthatare
distinguishablebyGMNs,butnotbyk-WLtest,showingthattheirexpressivepowerisnotboundedbyanyWLtest.
WeprovetheabovetheorembasedontherecentworkofTo¨nshoffetal.(2023),wheretheyproveasimilartheoremfor
CRaWl(To¨nshoffetal.,2023). NotethatthistheoremdoesnotrelyontheMamba’spower,andtheexpressivepowercomes
fromthechoiceofneighborhoodsamplingandencoding.
5.Experiments
Inthissection,weevaluatetheperformanceofGMNsinlong-range,small-scale,large-scale,andheterophilicbenchmark
datasets.Wefurtherdiscussitsmemoryefficiencyandperformablationstudytovalidatethecontributionofeacharchitectural
choice. Thedetailedstatisticsofdatasetsandadditionalexperimentsareavailableintheappendix.
9GraphMamba
Table1.BenchmarkonLong-RangeGraphDatasets(Dwivedietal.,2022).Highlightedarethetopfirst,second,andthirdresults.
COCO-SP PascalVOC-SP Peptides-Func Peptides-Struct
Model
F1score↑ F1score↑ AP↑ MAE↓
GCN 0.0841 0.1268 0.5930 0.3496
±0.0010 ±0.0060 ±0.0023 ±0.0013
GIN 0.1339 0.1265 0.5498 0.3547
±0.0044 ±0.0076 ±0.0079 ±0.0045
Gated-GCN 0.2641 0.2873 0.5864 0.3420
±0.0045 ±0.0219 ±0.0077 ±0.0013
CRaWl 0.3219 0.4088 0.6963 0.2506
±0.00106 ±0.0079 ±0.0079 ±0.0022
SAN+LapPE 0.2592 0.3230 0.6384 0.2683
±0.0158 ±0.0039 ±0.0121 ±0.0043
NAGphormer 0.3458 0.4006 - -
±0.0070 ±0.0061
GraphViT - - 0.6855 0.2468
±0.0049 ±0.0015
GPS 0.3774 0.3689 0.6575 0.2510
±0.0150 ±0.0131 ±0.0049 ±0.0015
GPS(BigBird) 0.2622 0.2762 0.5854 0.2842
±0.0008 ±0.0069 ±0.0079 ±0.0130
Exphormer 0.3430 0.3446 0.6258 0.2512
±0.0108 ±0.0064 ±0.0092 ±0.0025
GPS+Mamba 0.3895 0.4180 0.6624 0.2518
±0.0125 ±0.012 ±0.0079 ±0.0012
GMN- 0.3618 0.4169 0.6860 0.2522
±0.0053 ±0.0103 ±0.0012 ±0.0035
GMN 0.3974 0.4393 0.7071 0.2473
±0.0101 ±0.0112 ±0.0083 ±0.0025
5.1.ExperimentalSetup
Dataset. Weusethreemostcommonlyusedbenchmarkdatasetswithlong-range,small-scale,large-scale,andheterophilic
properties. Forlong-rangedatasets,weuseLongeRangeGraphBenchmark(LRGB)dataset(Dwivedietal.,2022). For
smallandlarge-scaledatasets, weuseGNNbenchmark(Dwivedietal.,2023). ToevaluatetheGMNsonheterophilic
graphs,weusefourheterophilicdatasetsfromtheworkofPlatonovetal.(2023). Finally,weusealargedatasetfromOpen
GraphBenchmark(Huetal.,2020). WeevaluatetheperformanceofGMNsonvariousgraphlearningtasks(e.g.,graph
classification,regression,nodeclassificationandlinkclassification). Also,foreachdatasetsweusetheproposemetrics
intheoriginalbenchmarkandreportthemetricacrossmultipleruns,ensuringtherobustness. Wediscussdatasets,their
statisticsandtheirtasksinAppendixA.
Baselines. We compare our GMNs with (1) MPNNs, e.g., GCN (Kipf & Welling, 2016), GIN (Xu et al., 2019), and
Gated-GCN(Bresson&Laurent,2017),(2)RandomwalkbasedmethodCRaWl(To¨nshoffetal.,2023),(3)state-of-the-art
GTs, e.g., SAN (Kreuzer et al., 2021), NAGphormer (Chen et al., 2023), Graph ViT (He et al., 2023), two variants of
GPS(Rampa´sˇeketal.,2022),andExphormer(Shirzadetal.,2023),and(4)ourbaselines(i)GPS+Mamba: whenwe
replacethetransformermoduleinGPSwithbidirectionalMamba. (ii)GMN-: whenwedonotusePE/SEandMPNN.We
usethesametraining,validation,andtestingforallthebaselinestomakesureafaircomparison.
5.2.LongRangeGraphBenchmark
Table1reportstheresultsofGMNsandbaselinesonlong-rangegraphbenchmark. GMNsconsistentlyoutperformbaselines
inalldatasetsthatrequireslong-rangedependenciesbetweennodes. Thereasonforthissuperiorperformanceisthree
folds: (1)GMNsbasedonourdesignuselongsequenceoftokenstolearnnodeencodingsandthenuseanotherselection
mechanismtofilterirrelevantnodes. TheprovidedlongsequenceoftokensenablesGMNstolearnlong-rangedependencies,
withoutfacingscalabilityorover-squashingissues. (2)GMNsusingtheirselectionmechanismarecapableoffilteringthe
neighborhoodaroundeachnode. Accordingly,onlyinformativeinformationflowsintohiddenstates. (3)Therandom-walk
basedneighborhoodsamplingallowGMNstohavediversesamplesofneighborhoods,whilecapturingthehierarchical
natureofk-hopneighborhoods. Also,itisnotablethatGMNconsistentlyoutperformsourbaselineGPS+Mamba,which
showstheimportanceofpayingattentiontothenewchallenges. Thatis,replacingthetransformermodulewithMamba,
whileimprovestheperformance,cannotfullytakeadvantageoftheMambatraits. Interestingly,GMN-,avariantofGMNs
without Transformer, MPNN, and PE/SE that we use to evaluate the importance of these elements in achieving good
performance,canachievecompetitiveperformancewithothercomplexmethods,showingthatwhileTransformers,complex
message-passing,andSE/PEaresufficientforgoodperformanceinpractice,neitherisnecessary.
10GraphMamba
Table2.BenchmarkonGNNBenchmarkDatasets(Dwivedietal.,2023).Highlightedarethetopfirst,second,andthirdresults.
MNIST CIFAR10 PATTERN MalNet-Tiny
Model
Accuracy↑ Accuracy↑ Accuracy↑ Accuracy↑
GCN 0.9071 0.5571 0.7189 0.8100
±0.0021 ±0.0038 ±0.0033 ±0.0000
GIN 0.9649 0.5526 0.8539 0.8898
±0.0025 ±0.0152 ±0.0013 ±0.0055
Gated-GCN 0.9734 0.6731 0.8557 0.9223
±0.0014 ±0.0031 ±0.0008 ±0.0065
CRaWl 0.9794 0.6901 - -
±0.050 ±0.0259
NAGphormer - - 0.8644 -
±0.0003
GPS 0.9811 0.7226 0.8664 0.9298
±0.0011 ±0.0031 ±0.0011 ±0.0047
GPS(BigBird) 0.9817 0.7048 0.8600 0.9234
±0.0001 ±0.0010 ±0.0014 ±0.0034
Exphormer 0.9843 0.7413 0.8670 0.9422
±0.0004 ±0.0050 ±0.0003 ±0.0024
GPS+Mamba 0.9821 0.7341 0.8660 0.9311
±0.0004 ±0.0015 ±0.0007 ±0.0042
GMN 0.9839 0.7576 0.8714 0.9415
±0.0018 ±0.0042 ±0.0012 ±0.0020
Table3.Benchmarkonheterophilicdatasets(Platonovetal.,2023).Highlightedarethetopfirst,second,andthirdresults.
Roman-empire Amazon-ratings Minesweeper Tolokers
Model
Accuracy↑ Accuracy↑ ROCAUC↑ ROCAUC↑
GCN 0.7369 0.4870 0.8975 0.8364
±0.0074 ±0.0063 ±0.0052 ±0.0067
Gated-GCN 0.7446 0.4300 0.8754 0.7731
±0.0054 ±0.0032 ±0.0122 ±0.0114
NAGphormer 0.7434 0.5126 0.8419 0.7832
±0.0077 ±0.0072 ±0.0066 ±0.0095
GPS 0.8200 0.5310 0.9063 0.8371
±0.0061 ±0.0042 ±0.0067 ±0.0048
Exphormer 0.8903 0.5351 0.9074 0.8377
±0.0037 ±0.0046 ±0.0053 ±0.0078
GOAT 0.7159 0.4461 0.8109 0.8311
±0.0125 ±0.0050 ±0.0102 ±0.0104
GPS+Mamba 0.8310 0.4513 0.8993 0.8370
±0.0028 ±0.0097 ±0.0054 ±0.0105
GMN 0.8769 0.5407 0.9101 0.8452
±0.0050 ±0.0031 ±0.0023 ±0.0021
5.3.ComparisononGNNBenchmark
We further evaluate the performance of GMNs in small and large datasets from the GNN benchmark. The results of
GMNsandbaselineperformancearereportedinTable2. GMNandExphormerachievecompetitiveperformanceeach
outperformstheothertwotimes. Ontheotherhandagain,GMNconsistentlyoutperformsGPS+Mambabaseline,showing
theimportanceofdesigninganewframeworkforGMNsratherthenusingexistingframeworksofGTs.
5.4.HeterophilicDatasets
ToevaluatetheperformanceofGMNsontheheterophilcdataaswellasevaluatingtheirrobustnesstoover-squashing
andover-smoothing,wecomparetheirperformancewiththestate-of-the-artbaselinesandreporttheresultsinTable3.
OurGMNoutperformsbaselinesin3outof4datasetsandachievethesecondbestresultintheremainingdataset. These
resultsshowthattheselectionmechanisminGMNcaneffectivelyfilterirrelevantinformationandalsoconsiderlong-range
dependenciesinheterophilicdatasets.
5.5.AblationStudy
ToevaluatethecontributionofeachcomponentofGMNsinitsperformance,weperformablationstudy. Table4reportsthe
results. Thefirstrow,reportstheperformanceofGMNswithitsfullarchitecture. Thenineachrow,wemodifyonethe
elementswhilekeepingtheotherunchanged: Row2removethebidirectionalMambaanduseasimpleMamba. Row3
removetheMPNNandRow4usePPRordering. FinallythelastrowremovePE.Resultsshowthatalltheelementsof
GMNcontributestoitsperformancewithmostcontributionfrombidirectionMamba.
11GraphMamba
Table4. AblationstudyonGMNarchitecture.
Roman-empire Amazon-ratings Minesweeper
Model
Accuracy↑ Accuracy↑ ROCAUC↑
GMN 0.8769 0.5407 0.9101
±0.0050 ±0.0031 ±0.0023
w/obidirectionalMamba 0.8327 0.5016 0.8597
±0.0062 ±0.0045 ±0.0028
w/oMPNN 0.8620 0.5312 0.8983
±0.0043 ±0.0044 ±0.0031
PPRordering 0.8612 0.5299 0.8991
±0.0019 ±0.0037 ±0.0021
w/oPE 0.8591 0.5308 0.9011
±0.0054 ±0.0026 ±0.0025
Figure2.EfficiencyevaluationandaccuracyofGMNsandbaselinesonOBGN-Arxivand GPS
1500 GMN
MalNet-Tiny.Highlightedarethetopfirst,second,andthirdresults.OOM:OutofMemory.
Method Gated-GCN GPS NAGphormer Exphormer† GOAT Ours 1000
GPS+Mamba GMN
OGBN-Arxiv 500
Training/Epoch(s) 0.68 OOM 5.06 1.97 13.09 1.18 1.30
Memory(GB) 11.09 OOM 6.24 36.18 8.41 5.02 3.85 0
Accuracy 0.7141 OOM 0.7013 0.7228 0.7196 0.7239 0.7248 1 300 600 900 1200
Average number of nodes
MalNet-Tiny
Training/Epoch(s) 10.3 148.99 - 57.24 - 36.07 41.00
Figure3.MemoryofGPSandGMNon
Accuracy 0.9223 0.9234 - 0.9224 - 0.9311 0.9415
†Wefollowtheoriginalpaper(Shirzadetal.,2023)anduseonevirtualnodeinefficiencyevaluation. MalNet-Tinydataset.
5.6.Efficiency
Aswediscussedearlier,oneofthemainadvantagesofourmodelisitsefficiencyandmemoryusage. Weevaluatethisclaim
onOGBN-Arxiv(Huetal.,2020)andMalNet-Tiny(Dwivedietal.,2023)datasetsandreporttheresultsinTable2. Our
variantsofGMNsarethemostefficienctmethodswhileachievingthebestperformance. Toshowthetrendofscalability,we
useMalNet-TinyandplotthememoryusageofGPSandGMNinFigure3. WhileGPS,asagraphtransformerframework,
requireshighcomputationalcost(GPUmemoryusage),GMNs’smemoryscaleslinearlywithrespecttotheinputsize.
6.Conclusion
Inthispaper,wepresentGraphMambaNetworks(GMNs)asanewclassofgraphlearningbasedonStateSpaceModel.
WediscussandcategorizethenewchallengeswhenadoptingSSMstograph-structureddata,andpresentfourrequiredand
oneoptionalstepstodesignGMNs,wherewechoose(1)NeighborhoodTokenization,(2)TokenOrdering,(3)Architecture
ofBidirectionalSelectiveSSMEncoder,(4)LocalEncoding,anddispensable(5)PEandSE.Wefurtherprovidetheoretical
justificationforthepowerofGMNsandconductseveralexperimentstoempiricallyevaluatetheirperformance.
PotentialBroaderImpact
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal
consequencesofourwork,noneofwhichwefeelmustbespecificallyhighlightedhere.
12
)BM(
yromeM
UPGGraphMamba
References
Ahamed, M. A. and Cheng, Q. Mambatab: A simple yet effective approach for handling tabular data. arXiv preprint
arXiv:2401.08867,2024.
Alon,U.andYahav,E.Onthebottleneckofgraphneuralnetworksanditspracticalimplications.InInternationalConference
onLearningRepresentations,2021. URLhttps://openreview.net/forum?id=i80OPhOCVH2.
Aoki,M. Statespacemodelingoftimeseries. SpringerScience&BusinessMedia,2013.
Arnaiz-Rodr´ıguez,A.,Begga,A.,Escolano,F.,andOliver,N.M. Diffwire: Inductivegraphrewiringviathelova´szbound.
InTheFirstLearningonGraphsConference,2022. URLhttps://openreview.net/forum?id=IXvfIex0mX6f.
Ba,J.L.,Kiros,J.R.,andHinton,G.E. Layernormalization,2016.
Baek,J.,Kang,M.,andHwang,S.J.Accuratelearningofgraphrepresentationswithgraphmultisetpooling.InInternational
ConferenceonLearningRepresentations,2021. URLhttps://openreview.net/forum?id=JHcqXGaqiGn.
Bar-Shalom,G.,Bevilacqua,B.,andMaron,H. Subgraphormer: SubgraphGNNsmeetgraphtransformers. InNeurIPS
2023Workshop: NewFrontiersinGraphLearning,2023. URLhttps://openreview.net/forum?id=e8ba9Hu1mM.
Behrouz,A.,Delavari,P.,andHashemi,F. Unsupervisedrepresentationlearningofbrainactivityviabridgingvoxelactivity
andfunctionalconnectivity. InNeurIPS2023AIforScienceWorkshop,2023. URLhttps://openreview.net/forum?id=
HSvg7qFFd2.
Bevilacqua,B.,Frasca,F.,Lim,D.,Srinivasan,B.,Cai,C.,Balamurugan,G.,Bronstein,M.M.,andMaron,H. Equivariant
subgraphaggregationnetworks. InInternationalConferenceonLearningRepresentations,2022. URLhttps://openreview.
net/forum?id=dFbKQaRk15w.
Bresson,X.andLaurent,T. Residualgatedgraphconvnets. arXivpreprintarXiv:1711.07553,2017.
Chen,J.,Gao,K.,Li,G.,andHe,K. NAGphormer: Atokenizedgraphtransformerfornodeclassificationinlargegraphs.
InTheEleventhInternationalConferenceonLearningRepresentations,2023. URLhttps://openreview.net/forum?id=
8KYeilT3Ow.
Choromanski,K.M.,Likhosherstov,V.,Dohan,D.,Song,X.,Gane,A.,Sarlos,T.,Hawkins,P.,Davis,J.Q.,Mohiuddin,
A.,Kaiser,L.,Belanger,D.B.,Colwell,L.J.,andWeller,A. Rethinkingattentionwithperformers. InInternational
ConferenceonLearningRepresentations,2021. URLhttps://openreview.net/forum?id=Ua6zuk0WRH.
DiGiovanni,F.,Giusti,L.,Barbero,F.,Luise,G.,Lio,P.,andBronstein,M.M. Onover-squashinginmessagepassing
neural networks: The impact of width, depth, and topology. In International Conference on Machine Learning, pp.
7865–7885.PMLR,2023.
Ding, Y., Orvieto, A., He, B., andHofmann, T. Recurrentdistance-encodingneuralnetworksforgraphrepresentation
learning. arXivpreprintarXiv:2312.01538,2023.
Dwivedi,V.P.,Rampa´sˇek,L.,Galkin,M.,Parviz,A.,Wolf,G.,Luu,A.T.,andBeaini,D. Longrangegraphbenchmark.
InKoyejo,S.,Mohamed,S.,Agarwal,A.,Belgrave,D.,Cho,K.,andOh,A.(eds.),AdvancesinNeuralInformation
ProcessingSystems,volume35,pp.22326–22340.CurranAssociates,Inc.,2022. URLhttps://proceedings.neurips.cc/
paper files/paper/2022/file/8c3c666820ea055a77726d66fc7d447f-Paper-Datasets and Benchmarks.pdf.
Dwivedi,V.P.,Joshi,C.K.,Luu,A.T.,Laurent,T.,Bengio,Y.,andBresson,X. Benchmarkinggraphneuralnetworks.
JournalofMachineLearningResearch,24(43):1–48,2023.
Errica,F.,Christiansen,H.,Zaverkin,V.,Maruyama,T.,Niepert,M.,andAlesiani,F. Adaptivemessagepassing: Ageneral
frameworktomitigateoversmoothing,oversquashing,andunderreaching. arXivpreprintarXiv:2312.16560,2023.
Fan,W.,Ma,Y.,Li,Q.,He,Y.,Zhao,E.,Tang,J.,andYin,D. Graphneuralnetworksforsocialrecommendation. InThe
worldwidewebconference,pp.417–426,2019.
13GraphMamba
Finkelshtein, B., Huang, X., Bronstein, M., and Ceylan, ˙I. ˙I. Cooperative graph neural networks. arXiv preprint
arXiv:2310.01267,2023.
Fu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A., and Re, C. Hungry hungry hippos: Towards language
modelingwithstatespacemodels. InTheEleventhInternationalConferenceonLearningRepresentations,2023. URL
https://openreview.net/forum?id=COZDy0WYGg.
Gu,A.andDao,T. Mamba: Linear-timesequencemodelingwithselectivestatespaces. arXivpreprintarXiv:2312.00752,
2023.
Gu,A.,Dao,T.,Ermon,S.,Rudra,A.,andRe´,C. Hippo:Recurrentmemorywithoptimalpolynomialprojections. Advances
inneuralinformationprocessingsystems,33:1474–1487,2020.
Gu,A.,Johnson,I.,Goel,K.,Saab,K.,Dao,T.,Rudra,A.,andRe´,C. Combiningrecurrent,convolutional,andcontinuous-
timemodelswithlinearstatespacelayers. Advancesinneuralinformationprocessingsystems,34:572–585,2021.
Gu,A.,Goel,K.,andRe,C. Efficientlymodelinglongsequenceswithstructuredstatespaces. InInternationalConference
onLearningRepresentations,2022. URLhttps://openreview.net/forum?id=uYLFoz1vlAC.
Gutteridge,B.,Dong,X.,Bronstein,M.M.,andDiGiovanni,F. Drew: Dynamicallyrewiredmessagepassingwithdelay.
InInternationalConferenceonMachineLearning,pp.12252–12267.PMLR,2023.
Hashemi,F.,Behrouz,A.,andLakshmanan,L.V. Firmcoredecompositionofmultilayernetworks. InProceedingsof
theACMWebConference2022,WWW’22,pp.1589–1600,NewYork,NY,USA,2022.AssociationforComputing
Machinery. ISBN9781450390965. doi: 10.1145/3485447.3512205. URLhttps://doi.org/10.1145/3485447.3512205.
He, X., Hooi, B., Laurent, T., Perold, A., LeCun, Y., andBresson, X. Ageneralizationofvit/mlp-mixertographs. In
InternationalConferenceonMachineLearning,pp.12724–12745.PMLR,2023.
Henaff, M., Bruna, J., and LeCun, Y. Deep convolutional networks on graph-structured data. arXiv preprint
arXiv:1506.05163,2015.
Hu,W.,Fey,M.,Zitnik,M.,Dong,Y.,Ren,H.,Liu,B.,Catasta,M.,andLeskovec,J. Opengraphbenchmark: Datasetsfor
machinelearningongraphs. Advancesinneuralinformationprocessingsystems,33:22118–22133,2020.
Hussain, M. S., Zaki, M. J., and Subramanian, D. Global self-attention as a replacement for graph convolution. In
Proceedingsofthe28thACMSIGKDDConferenceonKnowledgeDiscoveryandDataMining,pp.655–665,2022.
Kim, J., Nguyen, D., Min, S., Cho, S., Lee, M., Lee, H., andHong, S. Puretransformersarepowerfulgraphlearners.
AdvancesinNeuralInformationProcessingSystems,35:14582–14595,2022.
Kipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks. arXiv preprint
arXiv:1609.02907,2016.
Kong,K.,Chen,J.,Kirchenbauer,J.,Ni,R.,Bruss,C.B.,andGoldstein,T. Goat: Aglobaltransformeronlarge-scale
graphs. InInternationalConferenceonMachineLearning,pp.17375–17390.PMLR,2023.
Kreuzer,D.,Beaini,D.,Hamilton,W.,Le´tourneau,V.,andTossou,P. Rethinkinggraphtransformerswithspectralattention.
AdvancesinNeuralInformationProcessingSystems,34:21618–21629,2021.
Kuang,W.,Zhen,W.,Li,Y.,Wei,Z.,andDing,B. Coarformer: Transformerforlargegraphviagraphcoarsening. 2021.
Latora,V.andMarchiori,M. Ameasureofcentralitybasedonnetworkefficiency. NewJournalofPhysics,9(6):188,2007.
Lee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., and Teh, Y. W. Set transformer: A framework for attention-based
permutation-invariantneuralnetworks. InInternationalconferenceonmachinelearning,pp.3744–3753.PMLR,2019.
Lick, D. R. and White, A. T. k-degenerate graphs. Canadian Journal of Mathematics, 22(5):1082–1096, 1970. doi:
10.4153/CJM-1970-125-1.
14GraphMamba
Lim,D.,Robinson,J.D.,Zhao,L.,Smidt,T.,Sra,S.,Maron,H.,andJegelka,S. Signandbasisinvariantnetworksfor
spectralgraphrepresentationlearning. InTheEleventhInternationalConferenceonLearningRepresentations,2023a.
URLhttps://openreview.net/forum?id=Q-UHqMorzil.
Lim,H.,Joo,Y.,Ha,E.,Song,Y.,Yoon,S.,Lyoo,I.K.,andShin,T. Brainagepredictionusingmulti-hopgraphattention
module(MGA)withconvolutionalneuralnetwork. InMedicalImagingwithDeepLearning,shortpapertrack,2023b.
URLhttps://openreview.net/forum?id=brK-VVoDpqo.
Liu,C.,Zhan,Y.,Ma,X.,Ding,L.,Tao,D.,Wu,J.,andHu,W. Gapformer: Graphtransformerwithgraphpoolingfor
nodeclassification. InProceedingsofthe32ndInternationalJointConferenceonArtificialIntelligence(IJCAI-23),pp.
2196–2205,2023.
Liu, J., Yang, H., Zhou, H.-Y., Xi, Y., Yu, L., Yu, Y., Liang, Y., Shi, G., Zhang, S., Zheng, H., et al. Swin-umamba:
Mamba-basedunetwithimagenet-basedpretraining. arXivpreprintarXiv:2402.03302,2024a.
Liu,X.,Yu,H.-F.,Dhillon,I.,andHsieh,C.-J. Learningtoencodepositionfortransformerwithcontinuousdynamical
model. InInternationalconferenceonmachinelearning,pp.6327–6335.PMLR,2020.
Liu,Y.,Tian,Y.,Zhao,Y.,Yu,H.,Xie,L.,Wang,Y.,Ye,Q.,andLiu,Y. Vmamba: Visualstatespacemodel. arXivpreprint
arXiv:2401.10166,2024b.
Liu,Z.,Lin,Y.,Cao,Y.,Hu,H.,Wei,Y.,Zhang,Z.,Lin,S.,andGuo,B. Swintransformer: Hierarchicalvisiontransformer
usingshiftedwindows. InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pp.10012–10022,
2021.
Ma, J.,Li, F., andWang, B. U-mamba: Enhancinglong-rangedependencyforbiomedicalimagesegmentation. arXiv
preprintarXiv:2401.04722,2024.
Morris,C.,Ritzert,M.,Fey,M.,Hamilton,W.L.,Lenssen,J.E.,Rattan,G.,andGrohe,M. Weisfeilerandlemangoneural:
Higher-ordergraphneuralnetworks. InProceedingsoftheAAAIconferenceonartificialintelligence,volume33,pp.
4602–4609,2019.
Morris,C.,Rattan,G.,andMutzel,P. Weisfeilerandlemangosparse: Towardsscalablehigher-ordergraphembeddings.
AdvancesinNeuralInformationProcessingSystems,33:21824–21840,2020.
Mu¨ller,L.,Galkin,M.,Morris,C.,andRampa´sˇek,L. Attendingtographtransformers. arXivpreprintarXiv:2302.04181,
2023.
Murphy, R., Srinivasan, B., Rao, V., and Ribeiro, B. Relational pooling for graph representations. In International
ConferenceonMachineLearning,pp.4663–4673.PMLR,2019.
Nguyen, D. Q., Nguyen, T. D., and Phung, D. Universal graph transformer self-attention networks. In Companion
ProceedingsoftheWebConference2022,pp.193–196,2022a.
Nguyen,E.,Goel,K.,Gu,A.,Downs,G.,Shah,P.,Dao,T.,Baccus,S.,andRe´,C. S4nd: Modelingimagesandvideosas
multidimensionalsignalswithstatespaces. Advancesinneuralinformationprocessingsystems,35:2846–2861,2022b.
Nguyen, E., Poli, M., Faizi, M., Thomas, A. W., Wornow, M., Birch-Sykes, C., Massaroli, S., Patel, A., Rabideau,
C.M.,Bengio,Y.,Ermon,S.,Re,C.,andBaccus,S. HyenaDNA:Long-rangegenomicsequencemodelingatsingle
nucleotide resolution. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:
//openreview.net/forum?id=ubzNoJjOKj.
Page,L.,Brin,S.,Motwani,R.,andWinograd,T. Thepagerankcitationranking: Bringordertotheweb. Technicalreport,
Technicalreport,stanfordUniversity,1998.
Park,J.,Yun,S.,Park,H.,Kang,J.,Jeong,J.,Kim,K.-M.,Ha,J.-w.,andKim,H.J. Deformablegraphtransformer. arXiv
preprintarXiv:2206.14337,2022.
Platonov, O., Kuznedelev, D., Diskin, M., Babenko, A., and Prokhorenkova, L. A critical look at the evaluation of
GNNs under heterophily: Are we really making progress? In The Eleventh International Conference on Learning
Representations,2023. URLhttps://openreview.net/forum?id=tJbbQfw-5wv.
15GraphMamba
Rampa´sˇek,L.,Galkin,M.,Dwivedi,V.P.,Luu,A.T.,Wolf,G.,andBeaini,D. Recipeforageneral,powerful,scalable
graphtransformer. AdvancesinNeuralInformationProcessingSystems,35:14501–14515,2022.
Rong,Y.,Bian,Y.,Xu,T.,Xie,W.,Wei,Y.,Huang,W.,andHuang,J. Self-supervisedgraphtransformeronlarge-scale
moleculardata. AdvancesinNeuralInformationProcessingSystems,33:12559–12571,2020.
Ruhnau,B. Eigenvector-centrality—anode-centrality? Socialnetworks,22(4):357–365,2000.
Rusch, T.K., Bronstein, M.M., andMishra, S. Asurveyonoversmoothingingraphneural networks. arXivpreprint
arXiv:2303.10993,2023.
Sato,R.,Yamada,M.,andKashima,H. Randomfeaturesstrengthengraphneuralnetworks. InProceedingsofthe2021
SIAMinternationalconferenceondatamining(SDM),pp.333–341.SIAM,2021.
Shi,F.,Chen,X.,Misra,K.,Scales,N.,Dohan,D.,Chi,E.H.,Scha¨rli,N.,andZhou,D. Largelanguagemodelscanbe
easilydistractedbyirrelevantcontext. InInternationalConferenceonMachineLearning,pp.31210–31227.PMLR,
2023.
Shirzad,H.,Velingker,A.,Venkatachalam,B.,Sutherland,D.J.,andSinop,A.K. Exphormer: Sparsetransformersfor
graphs. arXivpreprintarXiv:2303.06147,2023.
Shiv, V. and Quirk, C. Novelpositional encodingsto enabletree-based transformers. Advances inneural information
processingsystems,32,2019.
Tang,S.,Dunnmon,J.A.,Liangqiong,Q.,Saab,K.K.,Baykaner,T.,Lee-Messer,C.,andRubin,D.L.Modelingmultivariate
biosignalswithgraphneuralnetworksandstructuredstatespacemodels. InMortazavi,B.J.,Sarker,T.,Beam,A.,andHo,
J.C.(eds.),ProceedingsoftheConferenceonHealth,Inference,andLearning,volume209ofProceedingsofMachine
LearningResearch,pp.50–71.PMLR,22Jun–24Jun2023. URLhttps://proceedings.mlr.press/v209/tang23a.html.
To¨nshoff,J.,Ritzert,M.,Wolf,H.,andGrohe,M. Walkingoutoftheweisfeilerlemanhierarchy: Graphlearningbeyond
messagepassing. TransactionsonMachineLearningResearch,2023. ISSN2835-8856. URLhttps://openreview.net/
forum?id=vgXnEyeWVY.
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,Ł.,andPolosukhin,I. Attentionisall
youneed. Advancesinneuralinformationprocessingsystems,30,2017.
Velicˇkovic´,P.,Cucurull,G.,Casanova,A.,Romero,A.,Lio`,P.,andBengio,Y. Graphattentionnetworks. InInternational
ConferenceonLearningRepresentations,2018. URLhttps://openreview.net/forum?id=rJXMpikCZ.
Wang,H.,Yin,H.,Zhang,M.,andLi,P.Equivariantandstablepositionalencodingformorepowerfulgraphneuralnetworks.
InInternationalConferenceonLearningRepresentations,2022. URLhttps://openreview.net/forum?id=e95i1IHcWj.
Wang,S.andXue,B.State-spacemodelswithlayer-wisenonlinearityareuniversalapproximatorswithexponentialdecaying
memory. InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023. URLhttps://openreview.net/
forum?id=i0OmcF14Kf.
Wang,Y.,Min,Y.,Shao,E.,andWu,J. Moleculargraphcontrastivelearningwithparameterizedexplainableaugmentations.
In2021IEEEInternationalConferenceonBioinformaticsandBiomedicine(BIBM),pp.1558–1563.IEEE,2021.
Wolf,T.,Debut,L.,Sanh,V.,Chaumond,J.,Delangue,C.,Moi,A.,Cistac,P.,Rault,T.,Louf,R.,Funtowicz,M.,etal.
Transformers: State-of-the-artnaturallanguageprocessing. InProceedingsofthe2020conferenceonempiricalmethods
innaturallanguageprocessing: systemdemonstrations,pp.38–45,2020.
Wu,Y.,Xu,Y.,Zhu,W.,Song,G.,Lin,Z.,Wang,L.,andLiu,S. Kdlgt: alineargraphtransformerframeworkviakernel
decompositionapproach. InProceedingsoftheThirty-SecondInternationalJointConferenceonArtificialIntelligence,
pp.2370–2378,2023.
Wu,Z.,Pan,S.,Chen,F.,Long,G.,Zhang,C.,andPhilip,S.Y. Acomprehensivesurveyongraphneuralnetworks. IEEE
transactionsonneuralnetworksandlearningsystems,32(1):4–24,2020.
16GraphMamba
Xing,Z.,Ye,T.,Yang,Y.,Liu,G.,andZhu,L. Segmamba: Long-rangesequentialmodelingmambafor3dmedicalimage
segmentation. arXivpreprintarXiv:2401.13560,2024.
Xu,K.,Hu,W.,Leskovec,J.,andJegelka,S. Howpowerfularegraphneuralnetworks? InInternationalConferenceon
LearningRepresentations,2019. URLhttps://openreview.net/forum?id=ryGs6iA5Km.
Yang, Y., Xing, Z., and Zhu, L. Vivim: a video vision mamba for medical video object segmentation. arXiv preprint
arXiv:2401.14168,2024.
Ying,C.,Cai,T.,Luo,S.,Zheng,S.,Ke,G.,He,D.,Shen,Y.,andLiu,T.-Y. Dotransformersreallyperformbadlyforgraph
representation? AdvancesinNeuralInformationProcessingSystems,34:28877–28888,2021.
Yu,Z.,Cao,R.,Tang,Q.,Nie,S.,Huang,J.,andWu,S. Ordermatters: Semantic-awareneuralnetworksforbinarycode
similaritydetection. InProceedingsoftheAAAIconferenceonartificialintelligence,volume34,pp.1145–1152,2020.
Yun, S., Jeong, M., Kim, R., Kang, J., and Kim, H. J. Graph transformer networks. Advances in neural information
processingsystems,32,2019.
Zaheer,M.,Guruganesh,G.,Dubey,K.A.,Ainslie,J.,Alberti,C.,Ontanon,S.,Pham,P.,Ravula,A.,Wang,Q.,Yang,L.,
etal. Bigbird: Transformersforlongersequences. Advancesinneuralinformationprocessingsystems,33:17283–17297,
2020.
Zhang,B.,Feng,G.,Du,Y.,He,D.,andWang,L. AcompleteexpressivenesshierarchyforsubgraphGNNsviasubgraph
weisfeiler-lehman tests. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.),
Proceedingsofthe40thInternationalConferenceonMachineLearning,volume202ofProceedingsofMachineLearning
Research,pp.41019–41077.PMLR,23–29Jul2023a. URLhttps://proceedings.mlr.press/v202/zhang23k.html.
Zhang,M.,Saab,K.K.,Poli,M.,Dao,T.,Goel,K.,andRe,C. Effectivelymodelingtimeserieswithsimplediscretestate
spaces. InTheEleventhInternationalConferenceonLearningRepresentations,2023b. URLhttps://openreview.net/
forum?id=2EpjkjzdCAa.
Zhang,Z.,Liu,Q.,Hu,Q.,andLee,C.-K. Hierarchicalgraphtransformerwithadaptivenodesampling. AdvancesinNeural
InformationProcessingSystems,35:21171–21183,2022.
Zhao,J.,Li,C.,Wen,Q.,Wang,Y.,Liu,Y.,Sun,H.,Xie,X.,andYe,Y. Gophormer: Ego-graphtransformerfornode
classification. arXivpreprintarXiv:2110.13094,2021.
Zhong,W.,He,C.,Xiao,C.,Liu,Y.,Qin,X.,andYu,Z. Long-distancedependencycombinedmulti-hopgraphneural
networksforprotein–proteininteractionsprediction. BMCbioinformatics,23(1):1–21,2022.
Zhu,L.,Liao,B.,Zhang,Q.,Wang,X.,Liu,W.,andWang,X. Visionmamba: Efficientvisualrepresentationlearningwith
bidirectionalstatespacemodel. arXivpreprintarXiv:2401.09417,2024.
17GraphMamba
Table5. DatasetStatistics.
Setup
Dataset #Graphs Average#Nodes Average#Edges #Class Metric
InputLevel Task
Long-rangeGraphBenchmark(Dwivedietal.,2022)
COCO-SP 123,286 476.9 2693.7 81 Node Classification F1score
PascalVOC-SP 11,355 479.4 2710.5 21 Node Classification F1score
Peptides-Func 15,535 150.9 307.3 10 Graph Classification AveragePrecision
Peptides-Struct 15,535 150.9 307.3 11(regression) Graph Regression MeanAbsoluteError
GNNBenchmark(Dwivedietal.,2023)
MNIST 70,000 70.6 564.5 10 Graph Classification Accuracy
CIFAR10 60,000 117.6 941.1 10 Graph Classification Accuracy
Pattern 14,000 118.9 3,039.3 2 Node Classification Accuracy
MalNet-Tiny 5,000 1,410.3 2,859.9 5 Graph Classification Accuracy
HeterophilicBenchmark(Platonovetal.,2023)
Roman-empire 1 22,662 32,927 18 Node Classification Accuracy
Amazon-ratings 1 24,492 93,050 5 Node Classification Accuracy
Minesweeper 1 10,000 39,402 2 Node Classification ROCAUC
Tolokers 1 11,758 519,000 2 Node Classification ROCAUC
VeryLargeDataset(Huetal.,2020)
OGBN-Arxiv 1 169,343 1,166,243 40 Node Classification Accuracy
A.DetailsofDatasets
ThestatisticsofallthedatasetsarereportedinTable5. Foradditionaldetailsaboutthedatasets,werefertotheLong-range
graphbenchmark(Dwivedietal.,2022),GNNBenchmark(Dwivedietal.,2023),HeterophilicBenchmark(Platonovetal.,
2023),andOpenGraphBenchmark(Huetal.,2020).
B.ExperimentalSetup
B.1.Hyperparameters
Weusetherecommendedsettingsintheofficialimplementationsandperformhyperparametertuningforeachbaseline.
Also,weusegridsearchtotunehyperparameters. Followingpreviousstudies,weusethesamesplitoftraning/test/validation
as(Rampa´sˇeketal.,2022). Wereporttheresultsoverthe4randomseeds.
C.DetailsofGMNArchitecture: Algorithms
Algorithm1showstheforwardpassoftheGraphMambaNetworkwithonelayer. Foreachnode,GMNfirstsamplesM
walkswithlengthmˆ = 1,...,mandconstructsitscorrespondingtokens,eachofwhichastheinducedsubgraphofM
walkswithlengthmˆ. Werepeatthisprocessstimestohavelongersequenceandmoresamplesfromeachhierarchyofthe
neighborhoods. Thispartofthealgorithm,canbecomputedbeforethetrainingprocessandinCPU.Next,GMNsforeach
nodeencodeitstokensusinganencoderϕ(.),whichcanbeanMPNN(e.g.,gated-GCN(Bresson&Laurent,2017))or
RWFencoding(proposedbyTo¨nshoffetal.(2023)). WethenpasstheencodingstoaBidirectionalMambablock,which
wedescribeinAlgorithm2(ThisalgorithmissimpletwoMambablock(Gu&Dao,2023)suchthatweuseoneofthe
backwardorforwardorderingofinputsforeachofthem). Attheendofline15,wehavethenodeencodingsobtainedfrom
subgraphtokenization. Next,wetreateachnodeasatokenandpasstheencodingtoanotherbidirectionalMamba,witha
specificorder. Wehaveuseddegreeorderinginourexperiments,buttherearesomeotherapproachesthatwehavediscussed
inthemainpaper.
D.AdditionalExperimentalResults
D.1.ParameterSensitivity
TheeffectofM. ParameterM isthenumberofwalksthatweaggregatetoconstructasubgraphtoken. Toevaluateitseffect
ontheperformanceoftheGMN,weusetwodatasetsofRoman-empireandPascalVOC-SP,fromtwodifferentbenchmarks,
18GraphMamba
Table6. Searchspaceofhyperparametersforeachdataset†.
Dataset M s #Layers Max#Epochs LearningRate
Long-rangeGraphBenchmark(Dwivedietal.,2022)
COCO-SP {1,2,4,8,16,32} {0,1,2,4,8,16} {4,5} 300 0.001
PascalVOC-SP {1,2,4,8,16,32} {0,1,2,4,8,16} {4,5} 300 0.001
Peptides-Func {1,2,4,8,16,32} {0,1,2,4,8,16} {4,5} 300 0.001
Peptides-Struct {1,2,4,8,16,32} {0,1,2,4,8,16} {4,5} 300 0.001
GNNBenchmark(Dwivedietal.,2023)
MNIST {1,2,4,8,16,32} {0,1,2,4,8,16} {3,4,6} 300 0.001
CIFAR10 {1,2,4,8,16,32} {0,1,2,4,8,16} {3,4,6} 300 0.001
Pattern {1,2,4,8,16,32} {0,1,2,4,8,16} {3,4,6} 300 0.001
MalNet-Tiny {1,2,4,8,16,32} {0,1,2,4,8,16} {3,4,6} 300 0.001
HeterophilicBenchmark(Platonovetal.,2023)
Roman-empire {1,2,4,8,16,32} {0,1,2,4,8,16} {3,4,6} 300 0.001
Amazon-ratings {1,2,4,8,16,32} {0,1,2,4,8,16} {3,4,6} 300 0.001
Minesweeper {1,2,4,8,16,32} {0,1,2,4,8,16} {3,4,6} 300 0.001
Tolokers {1,2,4,8,16,32} {0,1,2,4,8,16} {3,4,6} 300 0.001
VeryLargeDataset(Huetal.,2020)
OGBN-Arxiv {1,2,4,8,16,32} {0,1,2,4,8,16} {3,4,6} 300 0.001
†Thisspaceisnotfullysearchedandpreliminaryresultsarereportedbasedonitssubspace.Wewillupdatetheresultsaccordingly.
Roman-empire
0.8 0.8 0.8 PascalVOC-SP
Roman-empire Roman-empire
PascalVOC-SP PascalVOC-SP
0.6 0.6
0.6
0.4 0.4
0.4
1 2 4 6 8 10 1 5 10 20 30 40 50 60 5 10 15
M m s
Figure4. Theeffectof(Left)M,(Middle)m,and(Right)sontheperformanceofGMNs.
andvarythevalueofM fromXtoY.TheresultsarereportedinFigure4(Left). Theseresultsshowthatperformancepeaks
atcertainvalueofM andtheexactvaluevarieswiththedataset. Themainreasonis,parameterM determinesthathow
manywalkscanbeagoodrepresentativeoftheneighborhoodofanode,andsodependsonthedensity,homophilyscore,
andnetworktopologythisvaluecanbedifferent.
The effect of m. Similar to the above, we use two datasets of Roman-empire and PascalVOC-SP, from two different
benchmarks, and vary the value of m from X to Y. The results are reported in Figure 4 (Middle). The performance is
non-decreasingwithrespecttothevalueofm. Thatis,increasingthevalueofm,i.e.,consideringfarneighborsinthe
tokenizationprocess,doesnotdamagetheperformance(mightleadtobetterresults). Intuitively,usinglargevaluesofmis
expectedtodamagetheperformanceduetotheover-smoothingandover-squashing;however,theselectionmechanism
inBidirectionalMambacanselectinformativetokens(i.e.,neighborhood),filteringinformationthatcauseperformance
damage.
Theeffectofs. Usingthesamesettingastheabove,wereporttheresultswhenvaryingthevalueofsinFigure4(Right).
Result show that increasing the value of s can monotonically improve the performance. As discussed earlier, longer
sequencesoftokenscanprovidemorecontextforourmodelandduetotheselectionmechanisminMamba(Gu&Dao,
2023),GMNscanselectinformativesubgraphs/nodesandfilterirrelevanttokens,resultinginbetterresultswithlonger
sequences.
19
CCA/1F CCA/1F CCA/1FGraphMamba
Algorithm1GraphMambaNetworks(withonelayer)
Input: AgraphG=(V,E),inputfeaturesX∈Rn×d,orderedarrayofnodesV ={v ,...,v },andhyperparametersM,m,ands.
1 n
Optional:MatrixP,whoserowsarepositional/structuralencodingscorrespondtonodes,and/oraMPNNmodelΨ(.).
Output: TheupdatednodeencodingsX .
new
1: forv∈V do ▷Thisblockcanbedonebeforethetraining.
2: formˆ =0,...,mdo
3: forsˆ=1,...,sdo
4: Tsˆ(v)←∅;
mˆ
5: forMˆ =1,...,M do
6: W←Samplearandomwalkwithlengthmˆ startingfromv;
7: Tsˆ(v)←Tsˆ(v)∪{u|u∈W};
mˆ mˆ
8:
9: ▷Startthetraining:
10: Initializealllearnableparameters;
11: forv∈V do
12: forj =1,...,sdo
13: fori=1,...,mdo
(cid:16) (cid:17)
14: x(i−1)s+j ←ϕ G[Tj(v)],X ||P ;
v i Tj(v) Tj(v)
i i
Φ ←∥sm xi; ▷Φ isamatrixwhoserowsarexi.
v i=1 v v v
15: y (v)←BiMamba(Φ ); ▷UsingAlgorithm2.
output v
16: ▷Eachnodeisatoken:
17: Y ←∥sm y (v); ▷yisamatrixwhoserowsarey (v).
i=1 output output
18: Y ←BiMamba(Y)+Ψ(G,X∥P);
output
Table7. ComparisonwithGREDModel.
MNIST CIFAR10 PATTERN Peptides-func Peptides-struct
Model
Accuracy↑ Accuracy↑ Accuracy↑ AP↑ MAE↓
GRED† 0.9822 0.7537 0.8676 0.7041 0.2503
±0.0095 ±0.6210 ±0.0200 ±0.0049 ±0.0019
GMN 0.9839 0.7576 0.8714 0.7071 0.2473
±0.0018 ±0.0042 ±0.0012 ±0.0083 ±0.0025
†ResultsarereportedbyDingetal.(2023).
D.2.ComparisonwithGRED(Dingetal.,2023)
GRED(Dingetal.,2023)isarecentworkonArXivthatusesanRNNonthesetofneighborswithdistancek =1,...,K
toanodeofinterestforthenodeclassificationtask. SincethecodeormodelsofGREDarenotavailable,forthecomparison
ofGMNsandGRED,werunGMNsonthedatasetsusedintheoriginalpaper(Dingetal.,2023). Theresultsarereported
inTable7. GMNsconsistentlyoutperformsGRED(Dingetal.,2023)inalldatasets. Thereasonistwofolds: (1)GMNs
usesampledwalksinsteadofallthenodeswithink-hopneighborhood. AsdiscussedinTheorem4.1,thisapproachwith
largeenoughlengthandsamplesismoreexpressivethanconsideringallnodeswithintheneighborhood. (2)GREDuses
simpleRNNtoaggregatetheinformationaboutallthedifferentneighborhoodsofanode,whileGMNsuseMamba,which
haveaselectionmechanism. Thisselectionmechanismhelpthemodeltochooseneighborhoodsthataremoreinformative
andimportantthanothers.
E.ComplexityAnalysisofGNMs
m≥1. Foreachnodev ∈V,wegenerateM ×swalkswithlengthmˆ =1,...,m,whichrequiresO(M ×s×(m+1))
time. Given K tokens, the complexity of bidirectional Mamba is 2× of Mamba (Gu & Dao, 2023), which is linear
withrespecttoK. Accordingly,sincewehaveO(M ×s×m)tokens,thefinalcomplexityforagivennodev ∈ V is
O(M ×s×(m+1)). Repeatingtheprocessforallnodes, thetimecomplexityisO(M ×s×(m+1)×|V|+|E|),
whichislinearintermsof|V|and|E|(graphsize). TocomparetothequadratictimecomplexityofGTs,evenforsmall
networks,notethatinpractice,M ×s×(m+1)≪|V|,andinourexperimentsusuallyM ×s×(m+1)≤200. Also,
notethatusingMPNNasanoptionalstepcannotaffectthetimecomplexityastheMPNNrequiresO(|V|+|E|)time.
m=0. Inthiscase,eachnodeisatokenandsotheGMNrequiresO(|V|)time. UsingMPNNinthearchitecture,thetime
20GraphMamba
Algorithm2BidirectionalMamba
Input: AsequenceΦ(Orderedmatrix,whereeachrowisatoken).
Output: TheupdatedsequenceencodingsΦ.
1: ▷ForwardScan:
2: Φ =σ(Conv(W LayerNorm(Φ)));
f input,f
3: B =W Φ;
f Bf f
4: C =W Φ;
f Cf f
5: ∆ =Softplus(W Φ);
6:
A¯f
=Discrete
(A∆ ,∆f );f
A
7: B¯ =Discrete (B,∆);
f Bf f
8 9:
:
y Yf = =S WSM A¯, (B¯ yf,C ⊙f( σΦ (f W);
LayerNorm(Φ)));
f f,1 f f,2
10: ▷BackwardScan:
11: Φ←Reverse-rows(Φ); ▷Reversetheorderofrowsinthematrix.
12: Φ =σ(Conv(W LayerNorm(Φ)));
b input,b
13: B =W Φ ;
b Bb b
14: C =W Φ ;
b Cb b
15: ∆ =Softplus(W Φ );
16:
A¯b
=Discrete
(A,∆ ∆b );b
A
17: B¯ =Discrete (B ,∆);
b Bb b
18: y b =SSM A¯,B¯ b,Cb(Φ b);
19: Y =W (y ⊙σ(W LayerNorm(Φ)));
b b,1 b b,2
20: ▷Output:
21: y ←W (Y +Reverse-row(Y ));
output out f b
22: returny ;
output
complexitywouldbeO(|V|+|E|),dominatingbytheMPNNtimecomplexity.
Asdiscussedabove,basedonthepropertiesofMambaarchitecture,longersequenceoftokens(largervalueofs≥1)can
improvetheperformanceofthemethod. Basedontheabovementionedtimecomplexitywhenm≥1,wecanseethatthere
isatrade-offbetweentimecomplexityandtheperformanceofthemodel. Thatis,whilelargersresultinbetterperformance,
itresultsinslowermodel.
F.TheoreticalAnalysisofGMNs
TheoremF.1. WithlargeenoughM,m,ands>0,GMNs’neighborhoodsamplingisstrictlymoreexpressivethank-hop
neighborhoodsampling.
Proof. Wefirstshowthatinthiscondition,therandomwalkneighborhoodsamplingisasexpressiveask-hopneighborhood
sampling. Tothisend,givenanarbitrarysmallϵ>0,weshowthattheprobabilitythatk-hopneighborhoodsamplingis
moreexpressivethanrandomwalkneighborhoodsamplingislessthanϵ. Letm=k,s=1,andp betheprobabilitythat
u,v
wesamplenodevinawalkwithlengthm=kstartingfromnodeu. Thisprbobalityiszeroiftheshortestpathofuandv
ismorethank. Toconstructthesubgraphtokencorrespondstomˆ =k,weuseM samplesandsotheprobabilityofnot
seeingnodevinthesesamplesisq =(1−p )M ≤1. NowletM →∞andv ∈N (u)(i.e.,p ̸=0),wehave
u,v,M u,v k u,v
lim q =0. Accordingly,withlargeenoughM,wehaveq ≤ϵ. ThismeansthatwithalargeenoughM
M→∞ u,v,M u,v,M
whenm=kands=1,wesampleallthenodeswithinthek-hopneighborhood,meaningthatrandomwalkneighborhood
samplingatleastprovideasmuchinformationask-hopneighborhoodsamplingwitharbitrarylargeprobability.
Next,weprovideanexamplethatk-hopneighborhoodsamplingisnotabletodistinguishtwonon-isomorphismgraphs,
whilerandomwalksamplingcan. LetS = {v ,v ,...,v }beasetofnodessuchthatallnodeshaveshortestpathless
1 2 ℓ
thanktou. Usinghyperparamtersm=kandarbitraryM,lettheprobabilitythatwegetG[S]asthesubgraphtokenbe
1>q >0. Usingssamples,theprobabilitythatwedonothaveG[S]asoneofthesubgraphtokensis(1−q )s. Now
S S
usinglarges→∞,wehavelim (1−q )s =0andsoforanyarbitraryϵ>0thereisalarges>0suchthatwesee
s→∞ S
allnon-emptysubgraphsofthek-hopneighborhoodwithprobabilitymorethan1−ϵ,whichismorepowerfulthanthe
neighborhooditself.
Notethattheaboveproofdoesnotnecessarilyguaranteeanefficientsampling,butitguaranteestheexpressivepower.
Theorem F.2 (Universality). Let 1 ≤ p < ∞, and ϵ > 0. For any continues function f : [0,1]d×n → Rd×n that is
21GraphMamba
permutationequivariant,thereexistsaGMNwithpositionalencoding,g ,suchthatℓp(f,g)<ϵ.
p
Proof. Recently,Wang&Xue(2023)showthatSSMswithlayer-wisenonlinearityareuniversalapproximatorsofany
sequence-to-sequencefunction. Weletm = 0,meaningweusenodetokenization. UsingtheuniversalityofSSMsfor
sequence-to-sequence function, the rest of the proof is the same as Kreuzer et al. (2021), where they use the padded
adjacencymatrixofGasapositionalencodingtoprovethesametheoremforTransformers. Infact,theuniversalityfor
sequence-to-sequencefunctionsisenoughtoshowtheuniversalityongraphswithastrongpositionalencoding.
TheoremF.3(ExpressivePowerw/PE). Giventhefullsetofeigenfunctionsandenoughparameters,GMNscandistinguish
anypairofnon-isomorphicgraphsandaremorepowerfulthananyWLtest.
Proof. DuetotheuniversalityofGMNsinTheoremF.3,onecanuseaGMNwiththepaddedadjacencymatrixofGas
positionalencodingandlearnafunctionthatisinvariantundernodeindexpermutationsandmapsnon-isomorphicgraphsto
differentvalues.
TheoremF.4(ExpressivePowerw/oPEandMPNN). Withenoughparameter,foreveryk ≥1therearegraphsthatare
distinguishablebyGMNs,butnotbyk-WLtest,showingthattheirexpressivitypowerisnotboundedbyanyWLtest.
Proof. TheproofofthistheoremdirectlycomesfromtherecentworkofTo¨nshoffetal.(2023),wheretheyproveasimilar
theoremforCRaWl(To¨nshoffetal.,2023). Thatis,usingRWFasϕ(.)inGMNs(withoutMPNN),makestheGMNsas
powerfulasCRaWl. ThereasonisCRaWluses1-dCNNontopofRWF,whileGMNsuseBidirectionalMambablockon
topofRWF:usingabroadcastSMM,thisblockbecomessimilarto1-dCNN.
22