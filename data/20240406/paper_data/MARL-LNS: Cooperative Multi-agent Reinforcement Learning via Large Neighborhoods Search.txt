MARL-LNS:
Cooperative Multi-agent Reinforcement Learning
via Large Neighborhoods Search
Weizhe Chen, Sven Koenig, Bistra Dilkina
{weizhech, skoenig, dilkina}@usc.edu
University of Souhthern California
Abstract
Cooperative multi-agent reinforcement learning (MARL) has been an increasingly
important research topic in the last half-decade because of its great potential for
real-worldapplications. Becauseofthecurseofdimensionality,thepopular"central-
izedtrainingdecentralizedexecution"frameworkrequiresalongtimeintraining,yet
still cannot converge efficiently. In this paper, we propose a general training frame-
work,MARL-LNS,toalgorithmicallyaddresstheseissuesbytrainingonalternating
subsets of agents using existing deep MARL algorithms as low-level trainers, while
notinvolvinganyadditionalparameterstobetrained. Basedonthisframework,we
provide three algorithm variants based on the framework: random large neighbor-
hood search (RLNS), batch large neighborhood search (BLNS), and adaptive large
neighborhood search (ALNS), which alternate the subsets of agents differently. We
test our algorithms on both the StarCraft Multi-Agent Challenge and Google Re-
searchFootball,showingthatouralgorithmscanautomaticallyreduceatleast10%
of training time while reaching the same final skill level as the original algorithm.
1 Introduction
In recent years, multi-agent reinforcement learning (MARL) has split into cooperative multi-agent
reinforcement learning and competitive multi-agent reinforcement learning. Competitive MARL
has many theoretical guarantees following the previous studies in game theory, and has substantial
success in domains like Poker Brown & Sandholm (2018), and Diplomacy Gray et al. (2020). On
the other hand, cooperative multi-agent reinforcement learning focuses more on training a group of
agents and making them coordinate with each other when everyone shares the same goal, and has
also succeeded in many real-world applications like autonomous driving Zhou et al. (2020), swarm
control Hüttenrauch et al. (2017), and traffic scheduling Laurent et al. (2021).
WhilepreviousresearchhasshownthatMARLcanconvergetoagoodpolicythatwinsinagameor
finishes some tasks in a given time, training the agent efficiently is always one of the problems that
people are looking into. The difficulty of training MARL algorithms comes from training both the
reinforcement learning algorithm and the complexity of multi-agent systems. As training a single
agent reinforcement learning, the total time of MARL heavily depends on the sampling efficiency of
the environment, which makes sampling from multiple environments run in parallel now a common
practice to balance the sampling time and training time, and even recently rewriting environments
withJaxRutherfordetal.(2023). Afterparallelsampling,theCPUsamplingtimehasbecome40%
of the total time, making GPU also an unneglectable part of the time used in the training process.
In this paper, we propose a new learning framework that reduces the time used in the training
process of cooperative MARL without harming the performance of the final converged policy. We
split the training into multiple iterations, which we called LNS iterations, and we only consider the
training introduced by a fixed group of agents in each iteration. We choose a subset of agents that
1
4202
rpA
3
]AM.sc[
1v10130.4042:viXraareusedintrainingforacertainnumberoftrainingiterationsusingexistingdeepMARLalgorithms
to update the neural networks, and then alternate the subsets of agents so that we do not overfit
the policy to one certain subgroup of agents while we need to control all of them. We call the
group of agents we are considering the neighborhood, and call our framework large neighborhood
search (MARL-LNS), whose name comes from similar methods used in combinatorial optimization
Shaw (1998). Since we have only modified the training at a high level, integrating MARL-LNS with
any existing MARL algorithm is both straightforward and simple, and we choose to integrate it
with MAPPO in this paper. We provide a theoretical analysis that after multiple LNS iterations,
the optimal action learned in this reduced joint action space could still hold the same convergence
guarantee provided by the low-level MARL algorithm.
Based on our framework, we provide three simple yet powerful algorithms: random large neighbor-
hood search (RLNS), batch large neighborhood search (BLNS), and adaptive large neighborhood
search(ALNS).Noneofthethreeproposedhaveadditionalparametersthatneedtobetrained,and
ALNS do not even have additional hyperparameters that need to be tuned. To show the capability
ofourframework,ouralgorithmsrelyonrandomchoicesofwhichgroupofagentsisusedintraining
and do not include any hand-crafted or learned heuristic to select the neighborhood.
We test our algorithms in many scenarios in the popular StarCraft Multi-Agent Challenge (SMAC)
Samvelyan et al. (2019) and Google Research Football (GRF) Kurach et al. (2020) environments,
andshowthatoursimplealgorithmscanreachatleastthesamelevelofperformancewhilewecanbe
around10%fasterthanthebaselinesintermsoftotalwallclocktime. Weprovideanablationstudy
on how the number of agents in the training neighborhood at each period, i.e., the neighborhood
size, affects the performance of the algorithms and the training time.
2 Related Works
Cooperative multi-agent reinforcement learning has been a popular research topic in the past few
years. The most popular solution framework is the centralized training decentralized execution
(CTDE) framework, in which two of the most popular lines of research are the value-based research
like VDN Sunehag et al. (2017), QMIX Rashid et al. (2018), QTRAN Son et al. (2019), and policy-
based research like MADDPG Lowe et al. (2017), COMA Foerster et al. (2018) and MAPPO Yu
etal.(2021). Afewrecentworksproposeusingonlylocalneighborhoodinformationateachtimestep
to keep the joint state-action space small Zhang et al. (2022). Our algorithm framework is different
fromthosebynotchangingtheneighborhoodusedateachtimestepinoneepisode,butonlychanging
it after enough training iterations. Other works like VAST Phan et al. (2021) proposed to factorize
thecoordinationintoasumoverafewsmallersub-groupswithsmallerjoint-actionspace, whileour
algorithm will only train one sub-group at one time. Iqbal et al. (2021) also proposed to factor the
agents into random subpartitions in value-based MARL, and our work is different from theirs in
training in more than two groups and working on policy-based MARL. DER Hu et al. (2023) also
proposed to use part of the data in training, but they focused on training the policy efficiently in
the beginning, while we are focusing on the overall efficiency and effectiveness.
The idea of large neighborhood search has been extensively used in combinatorial optimization
problems since proposed Shaw (1998), such as vehicle routing Ropke & Pisinger (2006) and solving
mixed integer linear programming Munguía et al. (2018); Song et al. (2020). Similar ideas have
also been used to help convergence in game theory by fixing a group of agents Nair et al. (2003).
Recently, the same technique has been introduced to multi-agent path finding, where they fix the
path of a group of agents and replan the path of other agents at each iteration Li et al. (2021);
Huangetal.(2022);Lietal.(2022). Inthesealgorithms,thedecisionsofthevariables/agentsinthe
chosen neighborhood are updated such that this "move" results in a better overall solution to the
optimization problem. In this paper, we propose a framework that introduces similar ideas into the
MARL community. In one special case of our algorithm, our algorithm becomes iterative learning,
which has been widely used in equilibrium findings Daskalakis et al. (2010); Wang et al. (2018);
2Algorithm 1 MARL-LNS: Large neighborhood search (LNS) framework used in this paper with
MAPPO as low-level algorithm.
1: Initialize value network V and policy network π.
2: repeat
3:
Choose the neighborhood R=NeighborhoodSelect().
4: repeat
Reset the replay buffer
5:
6: repeat
7: Sample trajectories τ =(τ 1,τ 2,τ 3,...,τ n) from the environment according to π
8: Save τ r1,τ r2,...,τ rm to the replay buffer, where R=(r 1,r 2,...,r m)
9: until Sampled Buffer_length trajectories
10: Train V and π with the replay buffer using MAPPO
11: until Trained N Training_per_neighborhood rounds.
12: until Has done N LNS_iterations Iterations
Chen et al. (2021); Nair et al. (2003). While their works mostly use iterations to make the training
more stable and smooth, our work also focuses on reducing the time in each training iteration.
While earlier works mostly focus on improving the speed in sampling on the CPU side Rutherford
et al. (2023), and making parallel sampling a common practice, recently, there has been a growing
interest in optimizing the overall training time, which takes GPU time into account. Yu et al.
(2023) proposed an asynchronous MARL based on MAPPO, thus reducing 10.07% actual training
timecomparedtoMAPPO.Goginenietal.(2023)exploredneighborasamplingstrategytoimprove
cachelocalityandreacheda10.2%end-to-endtrainingtimereduction. Changetal.(2022)proposed
to guide agents with programs designed for parallelization, outperform the baselines in completion
rate, and reduce the overall time by 13.43%. Because the actual time reduction heavily relies on
the specific configuration of the CPU and GPU used in training, the time reduction numbers are
not comparable across different papers. Furthermore, these works that try to reduce the overall
training time are orthogonal to each other, and ideally, all the methods, including the one that we
will propose here, can be combined together to get a huge overall reduction.
3 Preliminaries
In multi-agent reinforcement learning, agents interact with the environment, which is formulated
as a decentralized partially observable Markov decision process (Dec-POMDP). Formally, a Dec-
POMDP is defined by a 7-tuple ⟨S,A,O,R,P,n,γ⟩. Here, n is the total number of agents, S is the
state space, and A is the action space. O is the observation function. P is the transition function.
R(s,a) denotes the shared reward function that every agent received. γ is the discount factor used
in calculating the cumulative reward. During the episode, agent i use a policy π (a |o ) to produce
i i i
an action a from the local observation o . The environment starts with an initial state s1, and
i i
stops when the environment provides a true in the done signal. The objective is to find a policy
π =(π ,π ,...,π )withinthevalidpolicyspaceΠthatoptimizesthediscountedcumultativereward
1 2 n
J =E P γt·rt, where st is the state at timestep t, rt =R(st,at), and at is the joint action
st,at∼π t
at timestep t. A trajectory τ is a list of elements that contains all the information of what happens
from the start to the end of one run of the environment, τ =< s1,a1,r1,s2,a2,r2,...,st,at,rt >.
During the training of CTDE algorithms, the trajectories are split into trajectories in the view of
every agent τ =(τ ,τ ,...,τ ), which contains useful information in terms of each agent and forms
1 2 n
anindividualtrajectory. Theinformationincludedineachτ dependsonthealgorithmthatisused.
i
34 Large Neighborhood Search for MARL
4.1 Large Neighborhood Search Framework
Largeneighborhoodsearchisapopularmeta-heuristicusedincombinatorialoptimizationandmulti-
agent path finding for finding good solutions to challenging problems where finding the optimal
solutionisextremelytime-consumingandinfeasibleinreality. Startingfromaninitialsolution,part
of the solution is selected, called a neighborhood, and then destroyed. The optimizer then needs to
rebuild the solution for the parts in the neighborhood while knowing the solution for the remaining
parts and freezing their values for efficiency.
While the high-level idea can lead to many completely different algorithms, we now give a detailed
description of our framework in MARL, which specifies what destroying the neighborhood means,
and what rebuilding the neighborhood means in the context of MARL. During training, we always
keep a group of m agents, and we call this group of agents a neighborhood R = {r ,r ,...,r },
1 2 m
where m is a hyperparameter specifying neighborhood size. Then, we sample a batch of data from
the environment. During the sampling process, we decouple full information trajectory τ to n
subtrajectories (τ ,τ ,...,τ ), where τ is to be used to train agent i respectively. Then, we only
1 2 n i
keepthetrajectoriesfortheagentsthatareintheneighborhood,andputthemintothereplaybuffer
thatislaterusedfortraining,i.e.,weonlysave{τ ,τ ,...,τ }inthereplaybufferratherthanthe
r1 r2 rm
whole trajectory τ. We do one training when enough data for one batch is sampled, using existing
training algorithms like MAPPO, and then clear the replay buffer. In this way, the training data
onlycontainstheinformationusedtotraintheagentsintheneighborhood. Werepeatthetrajectory
sampling process several times until a new group of agents is resampled as the new neighborhood.
We call the sampling of trajectories and training for one fixed neighborhood an LNS iteration. We
repeat a few LNS iterations until we meet our pre-set stopping criteria, which could be a total
number of steps sampled, which is the same as other MARL algorithms.
Unlike many existing works focusing on sample efficiency, our algorithms gain efficiency by using
fewer data for backpropagation in each batch of training. As a high-level approach, our algorithm
is agnostic to how the data is used for training and how coordination is solved in the framework.
Because of this, the lower-level MARL algorithm, which uses the data to train the neural network,
is very flexible. In this paper, we choose to use MAPPO Yu et al. (2021) as the low-level algorithm
as an example. In MAPPO, a centralized value function concatenates all observations from all
agents in the environment. Even if some agents are not included in the neighborhood and thus not
used in training, their observations are still kept in the input of the value function as global state
information. Besides, the trajectory of each agent contains exactly the same state information as
the original trajectories, and removing the trajectories outside the neighborhood saves us the space
of copying the action information of those agents. We provide this algorithm framework in Alg. 1,
where lines 3, 8, 11, and 12 are the lines that are introduced because of our LNS framework.
Next, we theoretically show that the convergence guarantee will not be affected by the introduced
framework,i.e.,aslongasthelow-levelalgorithmcanlearntocooperate,ourframeworkwillstillbe
abletolearntodothesame. Ouralgorithmcanbereducedtoablockcoordinatedescentalgorithm
(BCD)bylettingeachvariableusedintheoptimizationbethepolicyofeachagent,andtheobjective
value is our reward function. BCD is studied a lot by optimization theory researchers Tseng (2001);
Beck & Tetruashvili (2013); Lu & Xiao (2015), and its convergence rate is proved under different
conditions. Here, we provide a convergence guarantee that specifically proves that with MAPPO as
the low-level algorithm, the expected cumulative reward of the learned policy from MARL-LNS is
the same as the one from MAPPO:
Theorem 1 (Adapted from Lyu (2020)) Assume the expected cumulative reward function J is con-
tinuously differentiable with Lipschitz gradient and convex in each neighborhood partition, and the
training by the low-level algorithm guarantees that the training happening on the i-th neighborhood
is bounded by a high-dimension vector w on the joint policy space Π. Define the optimality gap as
i
∆ (π):=sup J −J , wherec′ isaconstant. SupposeP∞ w2 <∞, andletthepolicy
i πˆ∈Π,|πˆ−π|≤c′wi πˆ π i=1 i
4Algorithm 2 Neighborhoood selection function for Adaptive Large Neighborhood Search (ALNS).
1: Initialize neighborhood size m=2.
2: function NeighborhoodSelect
3: if Agent performance was not improving in the last two LNS iterations then
4: m=min(m+2⌊log 2m⌋−1,⌈n⌉)
2
5: end if
6: R=random.choice(n,m)
7: return R
8: end function
after the k-th LNS iteration be πk. If the optimality gap is uniformly summable, i.e., P∞ ∆ <∞,
i=1 i
then there exists some constant c>0 such that for i≥1,
π−πk c
min sup[− inf⟨∇J , ⟩]≤
1≤k≤i π0∈Π π∈Π πk |π−πk| Pi k=1w k
Specifically, the assumptions on w are common assumptions for convergence in MARL, and are
i
usually handled by the learning rate decay mechanism in the learning optimizer together with the
clip mechanism in reinforcement learning algorithms like TRPO and PPO. Furthermore, because
theexpectedrewardfunctionJ isbasedonpolicyratherthanaction,thecontinuouslydifferentiable
condition is also satisfied in environments with a continuous policy space. This theorem guarantees
that the convergence of MARL-LNS is irrelevant to the neighborhood size m as well as what is
included in each neighborhood. However, the learned policy could still be empirically worse than
the policy learned by the low-level algorithm if the learning rate is not handled properly.
4.2 Random Large Neighborhood Search
WhilemanypreviousworksoflargeneighborhoodsearchinCOandMAPFfocusalotonneighbor-
hoodselection,inthispaper,weshowthecapabilityofourframeworkbychoosingtheneighborhood
randomly and do not introduce any hand-crafted heuristics. We leave some discussion on how some
simpleheuristic-basedapproachestoneighborhoodselectiondonothelptheframeworklearnabetter
policy more efficiently in the appendix. Specifically, the neighborhood selection part is instantiated
with uniformly sample m agents from 1,..,n without replacement as random.choice do in NumPy.
We call this algorithm the random large neighborhood search (RLNS).
4.3 Batch Large Neighborhood Search
While pure random can introduce a lot of variance to the training, here we also provide an alter-
native batch-based large neighborhood search (BLNS) algorithm, which differs from RLNS in the
neighborhood selection function. Unlike RLNS, before any training starts, we create one permuta-
tion (p ,p ,...,p ) of all agents. Again, for simplicity, the permutation is created randomly in this
1 2 n
paper. After creating the permutation, we select the agents in order whenever we want to select
the next group of neighborhoods. In other word, given a fixed neighborhood size m, the first neigh-
borhoodwouldbe{p ,p ,...,p }, thesecondwouldbe{p ,p ,...,p }, andkeepgoinglike
1 2 m m+1 m+2 2m
this. Ifmcannotbedividedbyn, theneighborhoodthatincludesthelastagentp willalsoinclude
n
agent p , and keep going from p ,p to p again.
1 2 3 n
4.4 Adaptive Large Neighborhood Search
While the RLNS and BLNS use a fixed neighborhood size, the adaptive large neighborhood size is
recently becoming popular in the large neighborhood size community in combinatorial optimization
and multi-agent path finding Sonnerat et al. (2021); Huang et al. (2022). Here, we propose another
variantofMARL-LNSthatadaptivelychangestheneighborhoodsize. Inthebeginning, wedefinea
listofkpotentialneighborhoodsizeM =[m ,m ,...,m ],wherem <m <···<m . Intraining,
1 2 k 1 2 k
5if in the last two LNS iterations, the evaluation performance is not getting any improvement, the
current neighborhood size m will be changed to m in the next LNS iteration unless m = m
i i+1 i k
already. While this is orthogonal to the previously mentioned RLNS and BLNS, which focus on
neighborhood selection, we combine this method with RLNS as a new algorithm, the adaptive
largeneighborhoodsearch(ALNS).Forimplementations,becausemostenvironmentshaveasmooth
reward that encourages more agents to collaborate together, we can stay on a small neighborhood
size most of the time. We advocate for setting m
1
=2 and m
i
=min(m i−1+2⌊log 2(mi−1)⌋−1,⌈n 2⌉),
wherenisthetotalnumberofagents, toensureagradualincreaseinneighborhoodsize, optimizing
both efficiency and effectiveness. The corresponding ALNS pseudo-code is presented in Alg. 2.
Besides the original benefit from MARL-LNS, the gradually growing neighborhood size m gives an
additional benefit that fits the nature of MARL: At the beginning of the training, both the value
networkandthepolicynetworkarefarfromaccurateandoptimal. Inthisperiod,MARLalgorithms
are mostly training value functions, and the neighborhood size does not affect the training of the
value function. Later on, training on a subset of agents makes the training similar to iterative
training, which reduces the size of the joint action space to speed up the convergence to local
optimums. When it comes to the end of the training process, the neighborhood size will become
large enough to cover the need of many agents in the environment to collaborate on a single task.
5 Experiments
5.1 Experimental Settings
In this paper, we test our results on both StarCraft Multi-Agent Challenge (SMAC) and Google
Research Football (GRF) environments. We use parameter sharing between agents because it has
been shown to improve the training efficiency while not harming the performance Christianos et al.
(2021). We use some common practice tricks in MAPPO, including Generalized Advantage Esti-
mation (GAE) Schulman et al. (2015) with advantage normalization, value clipping, and includ-
ing both local-agent specific features and global features in the value function Yu et al. (2021).
For our algorithms, instead of setting a number of how many times of training is used for each
LNS iteration, we use an equivalent version of providing the total number of different neighbors
N =N /N , whereN isthenewhyperparameterwecontrol.
Training_per_neighborhood LNS_iterations T T
By default, the neighborhood size is half of the total number of agents. To show that our algorithm
does not introduce extra fine-tuning effort, we do not change the hyperparameters used in our low-
level algorithm MAPPO, e.g., the learning rate, the batch size, etc, as well as the network designs,
and environment configurations. We provide more details in the appendix. This will affect the
conclusion of Thm. 1, but we will use our results to show that this is not affecting the effectiveness
of our algorithm. For ALNS, we use the candidate neighborhood size list as we recommended.
5.2 SMAC Testbed
We test our algorithms on 5 different random seeds. For each random seed, we evaluate our results
following previous works: we compute the win rate over 32 evaluation games after each training
iteration and take the median of the final ten evaluation win rates as the performance to alleviate
the marginal distribution in this environment. We compare our algorithm with MAPPO, IPPO,
and QMIX. We only test our results in scenarios classified as hard or super-hard, since many easy
scenarios have been perfectly solved, and our algorithm will become iterative training, which has
been studied a lot, in scenarios that include only 2 or 3 agents.
We report our results in Table. 1 and Table. 2. In Table. 1, we observe that the time reductions
are consistent across scenarios since the reduction comes from reducing the training data used. The
time reduction is greater than that of other previous works on speeding up the overall time used in
the training of MAPPO. Besides, comparing the reduction between the full training and the early
numberofstepsresults,wefoundthatmostsavingsarefromthefirsthalfofthetraining,wherethe
neighborhood size stays at a very small value, and the MARL algorithm is getting improvement on
6both the value network and the policy network given the huge space for improvement in this phase.
InTable.2,thewinrateofouralgorithmsisatleastasgoodasthecurrentalgorithmswhileactually
getting a higher final win rate in difficult scenarios like 5mvs6m and MMM2. This shows that our
algorithm does not actually trade effectiveness in the trained policy for training efficiency, but gets
the speedup without harming the performance. Furthermore, RLNS generally has a bigger variance
in win rate than BLNS, which is coming from the pure-random-based neighborhood selection, but
this randomness also enables RLNS to get a higher median win rate in the very hard 3s5zvs3s6z
scenariowhereBLNSfailsin3seedsandendsupwithalowmedianvalue. Whentheneighborhood
does not include both types of allies in 3s5zvs3s6z, the evaluation win rate drops quickly and needs
alotofextratrainingefforttomakeupforthisdrop. Ontheotherhand, thisscenarioitselfusually
has a big marginal distribution, and both MAPPO, RLNS, and ALNS are still getting a policy
with a win rate of less than 30% in 2 out of the 5 seeds, leaving a great space for more stable policy
training. OurALNSisalwaysoneofthebestalgorithmsgiventhatitwillatlastusehalfofthetotal
number of agents, but on the other hand, it never outperforms other algorithms, mostly because
they are still in the same high-level framework.
RLNS & BLNS ALNS ALNS (50% Total Steps) ALNS (70% Total Steps)
5mvs6m 5% 5% 5% 5%
MMM2 21% 18% 21% 19%
3s5zvs3s6z 8% 12% 15% 14%
27mvs30m 10% 16% 23% 20%
10mvs11m 12% 19% 22% 21%
Table 1: Average total time reduction used by MARL-LNS compared to MAPPO in SMAC. The
50% and 70% Total steps indicate the scenarios in which each algorithm completes a respective
portion of the total number of steps.
MAPPO IPPO QMix RLNS (ours) BLNS (ours) ALNS (ours)
5mvs6m 89.1 (2.5) 87.5 (2.3) 75.8 (3.7) 96.9 (8.2) 96.9 (3.6) 96.9 (3.6)
MMM2 90.6 (2.8) 86.7 (7.3) 87.5 (2.6) 96.9 (31.8) 96.9 (4.7) 93.9 (2.4)
3s5zvs3s6z 84.4 (34.0) 82.8 (19.1) 82.8 (5.3) 87.5 (44.0) 12.6 (31.8) 87.5 (34.7)
27mvs30m 93.8 (2.4) 69.5 (11.8) 39.1 (9.8) 90.6 (2.5) 93.8 (7.2) 93.8 (4.7)
10mvs11m 96.9 (4.8) 93.0 (7.4) 95.3 (1.0) 93.8 (5.3) 96.9 (2.4) 96.9 (2.4)
Table 2: Median evaluation win rate and standard deviation on SMAC testbed.
5.2.1 Ablation Study on Neighborhood Size
Aftershowingthatouralgorithmsaregoodintermsoflearningthepolicies,wenowuseanablation
study on neighborhood size to show how our method provides flexibility to trade off a tiny win
rate for faster training time. As a special case, when changing the neighborhood size to 1, BLNS
is iterative training, and when the neighborhood size is as big as the total number of agents, our
frameworkisthesameasthelow-levelalgorithmMAPPO.Wedidnotchangethetotalenvironment
steps because we do not see any benefit in doing that.
We test BLNS on the 27mvs30m scenario, because it has the biggest number of agents. We show
our results in Table. 3. We observe that when increasing the neighborhood size, the final win rate is
improving,whilethetimeusageisalsobigger. Whenwesettheneighborhoodsizemto10,thefinal
performance is within one standard deviation of the low-level algorithm, while the training is 15%
faster. And if we set the neighborhood size m to 5, the final performance is within two standard
deviations of MAPPO, with only 77% of the original training time used. ALNS also achieves an
averagetimesavingbetweenaneighborhoodsizeof5and10andafinalwinrateasgoodasMAPPO,
showing that it is a reasonably good algorithm that balances the performance of learned policy and
training speed. Overall, ALNS and BLNS with m = 5 are the two most dominant settings on the
7Pareto frontier of total training time and final win rate. All the savings are because the sampling
time in CPUs is only taking 44% of the total training time of MAPPO, as shown in Table 3. All
other time is spent on transferring data between CPU and GPU and updating the neural networks
on GPU, which is the time related to updating that can be largely saved by removing part of the
training data. On the other hand, we can also observe that the standard deviation of the win rate
of the policies from a small neighborhood size, i.e., m = 1 or m = 3, is growing bigger by the end
of the training. This is because their policies have not actually converged, and given long enough
time, their performance could also reach a good result. But understanding that the primary focus
is training efficiency, they are not allowed to train any longer.
m 1 3 5 10 15 27 (MAPPO) ALNS
Win Rate (%) 3.1 (3.9) 62.5 (28.3) 87.5 (5.3) 90.6 (8.2) 93.8 (7.2) 93.8 (3.8) 93.8 (4.7)
Training Time (s) 7.14 7.35 7.37 8.22 9.29 9.52 8.09
Updating Time (s) 2.10 2.31 2.35 4.02 5.10 5.33 3.88
Table 3: Median value and the standard deviation on evaluation win rate for MARL-RLNS with
varied neighborhood sizes m, together with their average training time and average updating time
(training time includes both sampling time and updating time with a little overlap) for 1k episodes
on 27m_vs_30m scenario from SMAC. Specifically, when m=27, RLNS is the same as MAPPO.
5.3 GRF Testbed
We evaluate our results following the common practice in GRF: compared to the SMAC above,
instead of evaluating in 32 evaluation games, the policies are evaluated in 100 rollouts, and instead
ofreportingthemedianvalue,themeanvalueisreported. BecausemostscenariosinGRFhaveless
than 6 agents, we only test the algorithm in the corner scenario with 5 different random seeds.
Our results are shown in Table. 4. Even if this test case is naturally heterogeneous, we observe
that giving a good hyperparameter to BLNS and RLNS will give our algorithm the same level of
performance as MAPPO that is within one standard deviation, and applying the same group of
hyperparameters to RLNS can learn a slightly worse policy with larger variance. In this hyperpa-
rameters setting, BLNS and RLNS are trained at least 14% faster than MAPPO while ALNS is
25% faster than MAPPO. Additionally, GRF shows a more significant difference than SMAC when
changing the number of different neighborhoods used in the training process. Medium size of 20 is
enough for algorithms to explore collaborations with other agents while not changing it so regularly
and introducing instability to the training.
Win Rate (%) Time Reduction (%)
BLNS (m=7,N =20) 65.6(8.1) 15
T
BLNS(m=5,N =20) 57.4(6.0) 22
T
BLNS(m=7,N =10) 42.4(3.1) 15
T
BLNS(m=7,N =5) 42.2(1.8) 14
T
BLNS(m=7,N =40) 50.4(3.1) 15
T
RLNS(m=7,N =20) 58.0(13.5) 15
T
ALNS(N =20) 63.0(14.7) 25
T
MAPPO 65.53(2.19) 0
Table4: TheaverageevaluationwinrateofBLNSindifferentsettingscomparedtoRLNSandother
baselines on GRF, together with their corresponding time reduction compared to MAPPO.
6 Conclusion
Inthispaper,weproposeanovelextensiveneighborhoodsearchframework(MARL-LNS)forcooper-
ativeMARLtotrainthepoliciesmoreefficientlybyusingsubgroupsofagents,namedneighborhood,
8duringeachtrainingiteration. BuildinguponMARL-LNS,wedesignthreealgorithmsRLNS,BLNS,
and ALNS, that select the neighborhood differently. Specifically, our algorithm does not introduce
any additional parameters to be trained, and ALNS is even hyperparameter-free. Through both
theoretical and empirical analysis, we demonstrate that our algorithms significantly enhance train-
ing efficiency without compromising any training aspects, particularly in challenging environments
such as MMM2 within SMAC, where they also contribute to learning a superior policy.
References
Amir Beck and Luba Tetruashvili. On the convergence of block coordinate descent type methods.
SIAM journal on Optimization, 23(4):2037–2060, 2013.
Noam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats
top professionals. Science, 359(6374):418–424, 2018.
Can Chang, Ni Mu, Jiajun Wu, Ling Pan, and Huazhe Xu. E-MAPP: effi-
cient multi-agent reinforcement learning with parallel program guidance. In
NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/
4f2accafe6fa355624f3ee42207cc7b8-Abstract-Conference.html.
WeizheChen, ZihanZhou, YiWu, andFeiFang. Temporalinducedself-playforstochasticbayesian
games. arXiv preprint arXiv:2108.09444, 2021.
FilipposChristianos,GeorgiosPapoudakis,ArrasyRahman,andStefanoV.Albrecht. Scalingmulti-
agent reinforcement learning with selective parameter sharing. In Marina Meila and Tong Zhang
(eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24
July2021,VirtualEvent,volume139ofProceedingsofMachineLearningResearch,pp.1989–1998.
PMLR, 2021. URL http://proceedings.mlr.press/v139/christianos21a.html.
Constantinos Daskalakis, Rafael M Frongillo, Christos H Papadimitriou, George Pierrakos, and
Gregory Valiant. On learning algorithms for nash equilibria. In SAGT, pp. 114–125. Springer,
2010.
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Proceedings of the AAAI conference on artificial
intelligence, volume 32, 2018.
Kailash Gogineni, Peng Wei, Tian Lan, and Guru Prasadh Venkataramani. Towards efficient multi-
agent learning systems. In Architecture and System Support for Transformer Models (ASSYST@
ISCA 2023), 2023.
Jonathan Gray, Adam Lerer, Anton Bakhtin, and Noam Brown. Human-level performance in no-
pressdiplomacyviaequilibriumsearch. InInternational Conference on Learning Representations,
2020.
Xunhan Hu, Jian Zhao, Wengang Zhou, and Houqiang Li. Discriminative experience replay for
efficient multi-agent reinforcement learning. CoRR, abs/2301.10574, 2023. doi: 10.48550/arXiv.
2301.10574. URL https://doi.org/10.48550/arXiv.2301.10574.
Taoan Huang, Jiaoyang Li, Sven Koenig, and Bistra Dilkina. Anytime multi-agent path finding via
machine learning-guided large neighborhood search. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 36, pp. 9368–9376, 2022.
MaximilianHüttenrauch,AdrianŠošić,andGerhardNeumann. Guideddeepreinforcementlearning
for swarm systems. arXiv preprint arXiv:1709.06011, 2017.
9Shariq Iqbal, Christian A. Schröder de Witt, Bei Peng, Wendelin Boehmer, Shimon Whiteson,
and Fei Sha. Randomized entity-wise factorization for multi-agent reinforcement learning. In
MarinaMeilaandTongZhang(eds.),Proceedingsofthe38thInternationalConferenceonMachine
Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine
Learning Research, pp. 4596–4606. PMLR, 2021. URL http://proceedings.mlr.press/v139/
iqbal21a.html.
Karol Kurach, Anton Raichuk, Piotr Stańczyk, Michał Zając, Olivier Bachem, Lasse Espeholt,
Carlos Riquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, et al. Google research
football: A novel reinforcement learning environment. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 34, pp. 4501–4510, 2020.
Florian Laurent, Manuel Schneider, Christian Scheller, Jeremy Watson, Jiaoyang Li, Zhe Chen,
Yi Zheng, Shao-Hung Chan, Konstantin Makhnev, Oleg Svidchenko, et al. Flatland competition
2020: Mapfandmarlforefficienttraincoordinationonagridworld. InNeurIPS2020Competition
and Demonstration Track, pp. 275–301. PMLR, 2021.
Jiaoyang Li, Zhe Chen, Daniel Harabor, P Stuckey, and Sven Koenig. Anytime multi-agent path
finding via large neighborhood search. In Proceedings of the International Joint Conference on
Artificial Intelligence (IJCAI), 2021.
JiaoyangLi,ZheChen,DanielHarabor,PeterJStuckey,andSvenKoenig. Mapf-lns2: fastrepairing
formulti-agentpathfindingvialargeneighborhoodsearch.InProceedingsoftheAAAIConference
on Artificial Intelligence, volume 36, pp. 10256–10265, 2022.
Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-
agentactor-criticformixedcooperative-competitiveenvironments.Advancesinneuralinformation
processing systems, 30, 2017.
Zhaosong Lu and Lin Xiao. On the complexity analysis of randomized block-coordinate descent
methods. Mathematical Programming, 152:615–642, 2015.
Hanbaek Lyu. Convergence and complexity of block coordinate descent with diminishing radius for
nonconvex optimization. arXiv preprint arXiv:2012.03503, 2020.
Lluís-Miquel Munguía, Shabbir Ahmed, David A Bader, George L Nemhauser, and Yufen Shao.
Alternating criteria search: a parallel large neighborhood search algorithm for mixed integer
programs. Computational Optimization and Applications, 69(1):1–24, 2018.
RanjitNair,MilindTambe,MakotoYokoo,DavidV.Pynadath,andStacyMarsella. Tamingdecen-
tralized pomdps: Towards efficient policy computation for multiagent settings. In Georg Gottlob
and Toby Walsh (eds.), IJCAI-03, Proceedings of the Eighteenth International Joint Conference
on Artificial Intelligence, Acapulco, Mexico, August 9-15, 2003, pp. 705–711. Morgan Kaufmann,
2003. URL http://ijcai.org/Proceedings/03/Papers/103.pdf.
Thomy Phan, Fabian Ritz, Lenz Belzner, Philipp Altmann, Thomas Gabor, and Claudia Linnhoff-
Popien. Vast: Value function factorization with variable agent sub-teams. In Advances in Neural
Information Processing Systems (NeurIPS),volume34,pp.24018–24032.CurranAssociates,Inc.,
2021.
Tabish Rashid, Mikayel Samvelyan, Christian Schröder de Witt, Gregory Farquhar, Jakob N. Foer-
ster, and Shimon Whiteson. QMIX: monotonic value function factorisation for deep multi-agent
reinforcement learning. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Swe-
den, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 4292–4301.
PMLR, 2018. URL http://proceedings.mlr.press/v80/rashid18a.html.
Stefan Ropke and David Pisinger. An adaptive large neighborhood search heuristic for the pickup
and delivery problem with time windows. Transportation science, 40(4):455–472, 2006.
10Alexander Rutherford, Benjamin Ellis, Matteo Gallici, Jonathan Cook, Andrei Lupu, Gardar Ing-
varsson, Timon Willi, Akbir Khan, Christian Schroeder de Witt, Alexandra Souly, Saptarashmi
Bandyopadhyay,MikayelSamvelyan,MinqiJiang,RobertTjarkoLange,ShimonWhiteson,Bruno
Lacerda, Nick Hawes, Tim Rocktaschel, Chris Lu, and Jakob Nicolaus Foerster. Jaxmarl: Multi-
agent rl environments in jax. arXiv preprint arXiv:2311.10090, 2023.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas
Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon
Whiteson. The StarCraft Multi-Agent Challenge. CoRR, abs/1902.04043, 2019.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. arXiv preprint
arXiv:1506.02438, 2015.
Paul Shaw. Using constraint programming and local search methods to solve vehicle routing prob-
lems. In Principles and Practice of Constraint Programming—CP98: 4th International Confer-
ence, CP98 Pisa, Italy, October 26–30, 1998 Proceedings 4, pp. 417–431. Springer, 1998.
KyunghwanSon,DaewooKim,WanJuKang,DavidEarlHostallero,andYungYi. Qtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement learning. In Interna-
tional conference on machine learning, pp. 5887–5896. PMLR, 2019.
Jialin Song, Ravi Lanka, Yisong Yue, and Bistra Dilkina. A general large neighborhood search
framework for solving integer programs. arXiv, 2020.
Nicolas Sonnerat, Pengming Wang, Ira Ktena, Sergey Bartunov, and Vinod Nair. Learning a large
neighborhood search algorithm for mixed integer programs. CoRR, abs/2107.10201, 2021. URL
https://arxiv.org/abs/2107.10201.
PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,ViniciusZambaldi,Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 2017.
Paul Tseng. Convergence of a block coordinate descent method for nondifferentiable minimization.
Journal of optimization theory and applications, 109:475–494, 2001.
Kai Wang, Qingyu Guo, Phebe Vayanos, Milind Tambe, and Bo An. Equilibrium refinement in
security games with arbitrary scheduling constraints. In Elisabeth André, Sven Koenig, Mehdi
Dastani, and Gita Sukthankar (eds.), Proceedings of the 17th International Conference on Au-
tonomous Agents and MultiAgent Systems, AAMAS 2018, Stockholm, Sweden, July 10-15, 2018,
pp.919–927.InternationalFoundationforAutonomousAgentsandMultiagentSystemsRichland,
SC, USA / ACM, 2018. URL http://dl.acm.org/citation.cfm?id=3237836.
Zifan Wu, Chao Yu, Deheng Ye, Junge Zhang, Hankz Hankui Zhuo, et al. Coordinated proximal
policy optimization. Advances in Neural Information Processing Systems, 34:26437–26448, 2021.
Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of ppo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.
Chao Yu, Xinyi Yang, Jiaxuan Gao, Jiayu Chen, Yunfei Li, Jijia Liu, Yunfei Xiang, Ruixin Huang,
Huazhong Yang, Yi Wu, and Yu Wang. Asynchronous multi-agent reinforcement learning for
efficient real-time multi-robot cooperative exploration. In Noa Agmon, Bo An, Alessandro Ricci,
andWilliamYeoh(eds.),Proceedingsofthe2023InternationalConferenceonAutonomousAgents
and Multiagent Systems, AAMAS 2023, London, United Kingdom, 29 May 2023 - 2 June 2023,
pp. 1107–1115. ACM, 2023. doi: 10.5555/3545946.3598752. URL https://dl.acm.org/doi/10.
5555/3545946.3598752.
11ChengweiZhang, YuTian, ZhibinZhang, WanliXue, XiaofeiXie, TianpeiYang, XinGe, andRong
Chen. Neighborhood cooperative multiagent reinforcement learning for adaptive traffic signal
control in epidemic regions. IEEE Transactions on Intelligent Transportation Systems, 23(12):
25157–25168, 2022.
Ming Zhou, Jun Luo, Julian Villella, Yaodong Yang, David Rusu, Jiayu Miao, Weinan Zhang,
MontgomeryAlban,ImanFadakar,ZhengChen,etal.Smarts: Scalablemulti-agentreinforcement
learning training school for autonomous driving. arXiv preprint arXiv:2010.09776, 2020.
A Discussions
A.1 Societal Impact
WhileouralgorithmsareperformingwellinSMACandGRF,therearetwomajorlimitations. First,
ouralgorithmicframeworkMAPF-LNSreliesonthefactthatinbigandcomplexenvironments,not
allagentswouldbeusedtofulfillasingletask,andagentswouldlearntosplitthejobintosub-tasks
and do their own parts. However, the Multi-domain Gaussian Squeeze (MGS) environment used in
the Qtran paper Son et al. (2019) is one counter-example that requires strong cooperation within
all agents and thus leads to unstable and slow convergence when using our algorithms. Second, our
current random-based neighborhood selection algorithms assume that all agents are similar, and
no agents have a higher priority, higher importance, or significant differences from others in the
environment. This is not applicable in some real-world scenarios where certain agents should have
higher priority, for example, a fire engine in the traffic system. For the same reason, the marginal
distribution could be ignored during training. This may lead to fairness issues if the neighborhood
selection isnot taking these into account(as in the proposed RLNS and BLNS), and this could lead
to a potential negative social impact. We do not see any other potential negative societal impact of
our work.
A.2 Naming
About the naming of our algorithm, we have considered using a different name for our algorithm
thatdoesnotincludeLNS.However, twoprimaryreasonsledtoourdecisiontokeepthenamewith
LNS: 1). Although our algorithm is not fully aligned with other LNS approaches, our algorithm is
stilloptimizingonalocalneighborhoodateachiteration, whichistheessenceofanLNSalgorithm.
While the high-level idea of training on subsets of agents has been explored and called differently in
different communities, e.g., subset-based optimization, random coordinate descent, and LNS, using
the name of LNS could connect our future work with the rich literature in the combinatorial search
community on LNS, for example, neighborhood selection. 2). We believe that our algorithm is the
closestalgorithmtoLNSintheMARLcommunity. Themaindifferencebetweenourframeworkand
a typical LNS algorithm, which usually only changes the selected neighborhood subset, is that our
MARLRLNSandBLNSdonotcompletelyfixthepolicyofagentsthatarenotintheneighborhood
because of parameter sharing between the policy of different agents. However, the common practice
of using parameter sharing in MARL is the key to making the training efficient. Our improvement
in terms of time is slightly smaller than the one brought by parameter sharing, so we believe it is
necessary to keep parameter sharing in the framework.
A.3 Theorem on Convergence
Remark that in this paper, we are addressing the efficiency while not expecting the algorithm to
outperform the low-level algorithms, we do not guarantee the number of training per LNS neigh-
borhood is long enough for the value function and policy function to converge. Additionally, in the
most recent iteration of the paper, another update of the theorem is introduced, which, diverging
frompreviousrequirements,doesnotnecessitatethecumulativerewardfunctiontobeLipschitzdif-
ferentiable within each neighborhood partition, instead necessitating a weaker inequality condition.
12Given that differentiable is common in the proof of convergence in numerous other Multi-Agent Re-
inforcement Learning (MARL) algorithms, which are highly likely to be employed as the low-level
algorithm to guarantee the algorithms can learn to cooperate, we have elected not to incorporate
the update here.
A.4 Neighborhood Selection
While neighborhood selection is one of the most important parts of LNS research, in this paper,
our neighborhood selection strategy is purely random based. There are three major reasons for
this: 1. Random selection itself is very strong and robust in domains that are not well-studied
by LNS researchers. Many proposed heuristics for neighborhood selection failed to outperform
random selection on domains that are not included in their paper or even much worse. While the
primary purpose of this paper is to introduce LNS into the context of MARL, a pure random-
based algorithm variant that stably outperforms the low-level MAPPO is already good enough to
fulfill the objective. 2. While MARL is a very general algorithm that could be used for many
environments, how to propose a very strong and general neighborhood selection algorithm is a
very big challenge. If one wants to bring the generalizability of machine learning into this domain,
one might need to think about how to tradeoff the additional time used in training, such as a
neighborhood selection model, and also the inference time used in the training process may reduce
the total time reduction. 3. Even in the case that we do not want generalizable heuristics, we have
tried in SMAC based on geographical clustering to choose local neighborhoods to align with the
previous works in geographical clustering Zhang et al. (2022). This works no better than random,
andisparticularlybadinthe3s5zvs3s6zscenario,whereaswediscussedinthemainpaper,including
both type of agents are necessary for each neighborhood to get a strong policy. 4. Although there
areonlytwomainbranchesincooperativeMARL,namelyvalue-basedandpolicy-basedalgorithms,
the concrete training details are completely different from one MARL algorithm to another. And
what is available for a neighborhood selection algorithm to use is also very different. For example,
for MAPPO, the algorithm only has a joint value function learned, and the optimization is based
on the advantage function in each episode, while in QMIX, each agent also has a local Q function
that takes action into account. These differences can make a neighborhood selection algorithm like
choosing the agents whose local Q function is the smallest in Qmix not applicable to MAPPO.
But our current random-based algorithm does not have such a problem so we really recommend
the current group of RLNS, BLNS and ALNS as a general solution if one just want to get an easy
speedup, no matter what their low-level algorithm is.
A.5 Rejecting Bad Neighborhoods
In the current version of the paper, we do not reject any bad neighborhoods, primarily because the
trainingofMARLalwaysincludesalotoffluctuationscausedbybothlocaloptimalandexploration,
asshowninFig.1andFig.2. Ifwerejectaneighborhoodjustbyad-hocreductionoftheevaluation
win rate, the learned policy will end up in an early-point local optimal that is far from optimal.
Another minor reason is that our main focus of this paper is to provide this robust MARL-LNS
frameworkthatgainsthetotaltrainingtimereductionwithnearlynoextraeffort,andcleverrejection
criteriaaresomethingthatonemayseeasahugeburdenandarewhatwewanttoavoid. However,it
is undeniable that developing a clever rejecting heuristic can help the algorithms to be more stable,
and we would like to leave such research in the future.
A.6 Improvement Margin
In this paper, our improvement is from 5% to 25% , depending on the scenarios and settings of our
algorithms. It needs to be addressed that our reported results are obtained on a server that highly
prioritizesthecapacityofGPU,andthusthesamplingtime(whichisCPUdependent)takesalarger
portionoftimethanitwouldonaserverthatbalancestheconfigurationofCPUandtheGPU.When
moving to such a server that replaces the V100 with an NVIDIA P100 GPU, the improvement for
135mvs6mscenarioinSMAC,whichisthescenariothatwehavetheleastimprovementratio,increases
from 5% to 15%, and could reach a time saving of 25% when everything is done on a single 16-core
xeon-6130 CPU with 32GB memory without any GPU.
B Implementation Details for Experiments
Here we provide the hyperparameter table for our experiments. While most results in the experi-
ment section are from previous papers Yu et al. (2021) and Wu et al. (2021), we only provide the
hyperparameter for our algorithms.
Table. 5 provides the neighborhood size used specifically for each scenario, while Table. 6 provide
other hyperparameters that exist in the low-level training algorithm MAPPO. Hyperparameters
are not specified by search but by directly using the recommended variables used by the low-level
training algorithm MAPPO, except for the number of parallel environments which is limited by the
core of our server. In SMAC environments, the number of neighborhood iterations is set to 8.
All results displayed in this paper are trained on servers with a 16-cores xeon-6130 2.10 GHz CPU
with 64GB memory, and an NVIDIA V100 24GB GPU.
Neighborhood size m
5mvs6m 3
mmm2 2
3s5zvs3z6z 5
27mvs30m 15
10mvs11m 5
Table 5: Hyperparameter Table for neighborhood size used in RLNS and BLNS.
C Additional Experiment Results
WehaveincludedourtrainingcurveinFig.??. Whileitisacknowledgeablethatthetrainingcurves
have many fluctuations due to the reported values being median values, we can still see the same
conclusion as we get from the table in the main paper: our proposed algorithms are as good as our
base-levelalgorithmMAPPO.Areasonableneighborhoodsizemthatislargerthan3canalsomake
our algorithm not significantly worse than the base algorithm.
Besides, as shown in Fig. 1a, we observe that ALNS may not always be the most efficient at any
time, given that training on a limited number of agents like 2 may lead to a slow improvement on
policy function, but ALNS is as good as the low-level algorithm as the training progress and ALNS
increase the neighborhood size.
D Additional Theoretical Guarantee for Convergence of MARL-LNS
In the main paper, we have provided the theorem that guarantees the convergence of MARL-LNS
to be the same as the low-level algorithm. Here we provide a stronger theorem in the case that in
each LNS iteration, the policy is updated to local optimal.
Theorem 2 (Adapted from Lyu (2020)) Assume the expected cumulative reward function J is con-
tinuously differentiable with Lipschitz gradient and convex in each neighborhood partition, and the
training by the low-level algorithm guarantees that the training happening on the i-th neighborhood
is bounded by some high-dimension vector w . Suppose P∞ w2 <∞, then the following hold:
i i=1 i
1. If P∞ |w | = ∞, then for any initial starting point of training, the training can converge
i=1 i
to a stationary point.
14Hyperparameters value
recurrent data chunk length 10
gradient clip norm 10.0
gae lamda 0.95
gamma 0.99
value loss huber loss
huber delta 10.0
batch size num envs × buffer length × num agents
mini batch size batch size / mini-batch
optimizer Adam
optimizer epsilon 1e-5
weight decay 0
network initialization Orthogonal
use reward normalization True
use feature normalization True
num envs (SMAC) 8
num envs (GRF) 15
buffer length 400
num GRU layers 1
RNN hidden state dim 64
fc layer dim 64
num fc 2
num fc after 1
Table 6: Hyperparameter table for the MAPPO training part used in BLNS and RLNS.
(a) 5M vs 6M (b) MMM2
Figure 1: Median value and standard deviation of the RLNS, BLNS, and ALNS training curves
compared to MAPPO on two SMAC scenarios. Although the neighborhood size is set as half of the
total number of agents, the training curves are not much different.
2. If πk is optimized to optimal in each LNS iteration, then there exists some constant c > 0
such that for i≥1,
π−πk c
min sup[− inf⟨∇J , ⟩]≤
1≤k≤i π0∈Π π∈Π πk |π−πk| Pi k=1w k
Comparedtotheoneusedinthemainpaper,thistheoremreliesontheassumptionoftheoptimality
gap to be summable. However in general practice, always making the policy optimized to optimal
in each LNS iteration will lead to an extremely long training time and, thus, is less preferable.
15(a) Win rate corresponds vs. environment (b) Win rate corresponds vs. wall clock
step time
Figure 2: Median value and standard deviation of the BLNS training curve on the 27m_vs_30m
scenario on SMAC for different neighborhood sizes m.
16