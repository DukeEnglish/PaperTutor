Approximate Gradient Coding for Privacy-Flexible
Federated Learning with Non-IID Data
Okko Makkonen∗, Sampo Niemela¨∗, Camilla Hollanti∗, Serge Kas Hanna†
∗Department of Mathematics and Systems Analysis, School of Science, Aalto University, Finland
†I3S laboratory, Coˆte d’Azur University and CNRS, Sophia Antipolis, France
Emails: {okko.makkonen, sampo.niemela, camilla.hollanti}@aalto.fi, serge.kas-hanna@{univ-cotedazur.fr, cnrs.fr}
Abstract
This work focuses on the challenges of non-IID data and stragglers/dropouts in federated learning. We introduce and explore a
privacy-flexible paradigm that models parts of the clients’ local data as non-private, offering a more versatile and business-oriented
perspective on privacy. Within this framework, we propose a data-driven strategy for mitigating the effects of label heterogeneity
and client straggling on federated learning. Our solution combines both offline data sharing and approximate gradient coding
techniques. Through numerical simulations using the MNIST dataset, we demonstrate that our approach enables achieving a
deliberate trade-off between privacy and utility, leading to improved model convergence and accuracy while using an adaptable
portion of non-private data.
I. INTRODUCTION
In the ever-evolving landscape of machine learning (ML), one paradigm has emerged as a promising solution to balance the
demand for data privacy and the need for robust distributed model training: federated learning (FL) [1], [2]. This innovative
approach allows multiple devices or entities, often called clients, to collaboratively train a shared model under the orchestration
of a central server while keeping their data decentralized. The result is a model that benefits from the collective intelligence of
diverse data sources without compromising individual data privacy. Traditional distributed learning [3], [4], on the other hand,
involves a central server that owns all the data and has complete control over how it distributes it to edge devices to parallelize
the learning process, making it more suitable for scenarios where the data is naturally centralized.
While various assumptions can be made when designing algorithms for distributed learning and analyzing their performance,
the most prominent split is between assuming independent, identically distributed (IID) data and non-IID data. The IID data
assumption refers to the case where the data samples across different clients are independently drawn from the same underlying
distribution, i.e., share similar statistical properties. This assumption simplifies the training process and algorithm design
and enables deriving theoretical convergence guarantees such as the ones in [5]–[8]. While this assumption may be valid in
traditional distributed learning settings where the data is centralized, it almost never holds in practical FL settings since the
clients independently collect their own training data, which typically vary in both size and distribution.
In the context of FL, the term non-IID data refers to a scenario where the data available across clients is not statistically
similar. Previous studies have shown that dealing with non-IID data is one of the most significant hurdles encountered by FL.
More specifically, training on non-IID data introduces several drawbacks that can hinder the effectiveness of collaborative model
training, such as slower convergence and lower model accuracy [9]–[12]. The existing solutions in the literature for dealing
with non-IID data in FL can be divided into two main categories that propose orthogonal approaches: algorithm-based methods
and data-driven methods. Algorithm-based methods focus on adjusting the local loss function to align the local model with the
global one [11]–[16], training of personalized models for individual participants instead of employing a uniform global model
[17]–[22], and developing new aggregation schemes to enhance the model aggregation process [9], [13], [14], [23]. Data-driven
approaches predominantly focus on data augmentation methods that mitigate statistical imbalances by artificially expanding the
local dataset of each client through generating synthetic data [24]–[29]. Other data-driven approaches include hybrid federated
learning schemes where a statistically diverse share of the training data is presumed to be present at the central server. In these
hybrid schemes, the central server is expected to actively engage in the training process to compensate for potential statistical
imbalances arising from client-side distributed computations [10], [30].
In addition to the statistical challenge of dealing with non-IID data, FL also suffers from the straggler problem. The straggler
or dropout problem refers to the issue where some participating clients or devices temporarily drop out or fail to complete
their participation in a FL round. This can happen because clients may join or leave the FL system dynamically or due to
other practical considerations such as poor network connections and resource constraints. Previous studies have demonstrated
that the presence of stragglers aggravates the problem of learning on non-IID data, resulting in diminished model quality and
slower convergence [31], [32]. Coding for straggler mitigation is a popular solution that has been extensively studied in the
traditional distributed learning setting [33]–[42], and also more recently in FL [43], [44]. The key idea in these works is to first
introduce training data redundancy in the distributed system and then apply coding techniques that exploit this redundancy to
either approximate or recover the central model update, even when some clients drop out.
While previous works in FL treat all the clients’ local data as equally private, our work introduces and investigates a
privacy-flexible FL paradigm that allows for a deliberate trade-off between privacy and utility, offering more adaptable privacy
4202
rpA
4
]GL.sc[
1v42530.4042:viXrameasures. Our motivation for studying this setting stems from practical and commercial situations where striking the right
balance between privacy and utility is essential for integrating FL in business models. More precisely, in the proposed paradigm,
we model a portion of each client’s data as being non-private. We argue that parts of the local data in FL can be treated as
non-private for many reasons, including: (i) Participants often have diverse datasets of varying sensitivity, and some data may
not contain anything private or sensitive, allowing for differentiation in privacy treatment. (ii) Privacy can be selective, i.e.,
some participants may be incentivized to sacrifice their privacy by revealing or sharing part of their raw data for enhancing the
model’s performance and/or for potential financial rewards.
In this work, we introduce a novel data-driven approach that leverages the concept of privacy flexibility to improve the utility
of FL while addressing the challenges posed by non-IID data and stragglers. As data can be non-IID in different ways, we
specifically address label heterogeneity, which refers to the unequal representation of certain classes or labels across clients.
Our proposed scheme consists of two key components. First, we introduce an offline data sharing mechanism, executed just
once before the training phase. In this process, participants share some of their non-private data with each other to reduce the
statistical imbalances resulting from label heterogeneity. The data sharing also creates redundancy in the training datasets as
some of the data becomes available to multiple clients. The second component of our scheme capitalizes on this redundancy via
approximate gradient coding. This coding method is designed to optimize the training process by providing an unbiased estimate
of the central model update rule of gradient descent in the presence of stragglers, similar to the approach in [39] for traditional
distributed learning settings. Moreover, we theoretically demonstrate that our proposed scheme reduces the variance of the
obtained estimate, suggesting faster convergence, and characterize this reduction in terms of system parameters. Additionally,
we present simulation results on real data using the MNIST dataset [45], which substantiate our theoretical findings. These
simulations reveal that our scheme enables achieving a convergence behavior and model accuracy that closely mirrors the IID
case, contingent on the amount of shared non-private data.
II. PRELIMINARIES
A. Notation
We use bold letters for vectors and sans-serif letters for random variables, e.g., x, X, and X represent a vector, a random
variable, and a random vector, respectively. Uppercase italic letters are used for sets. Let [n] ≜ {1,2,...,n} be the set of
integers from 1 to n (inclusive), and let ∆n ≜(cid:8) (δ 0,δ 1,...,δ n)∈Rn+1(cid:12) (cid:12)(cid:80)n i=0δ
i
=1 and δ
i
≥0(cid:9) be the standard n-simplex.
We represent the Euclidean norm of a vector x by ∥x∥, and the scalar product between two vectors x and y by ⟨x,y⟩.
B. System Model
Consider a FL setting with N >1 clients who want to use their local data to collectively train a machine learning model
with the help of a central server. Suppose that the clients want to run a supervised classification task, i.e., each client has a
set of training examples (x,y) with features x and labels y ∈[L], where L represents the total number of classes. Let D be
i
the local dataset of client i∈[N], and let D
=(cid:83)N
D denote the global training dataset of size M given by the union of
i=1 i
the local datasets across all the clients. The goal of FL is to find an optimal global model β∗ that minimizes a given loss
function L, i.e.,
β∗ =argminL(D,β).
β
The global loss can be often expressed as the sum of the individual losses evaluated at each example (x,y), i.e.,
(cid:88)
L(D,β)= L(x,y,β).
(x,y)∈D
A common approach for solving such optimization problems is using gradient-based methods. Namely, the central server applies
or approximates the following gradient descent update rule at each iteration t∈{0,1,...} of the algorithm
η (cid:88)
β(t+1) =β(t)− ∇L(x,y,β(t)),
M
(x,y)∈D
where ∇L is the gradient of the loss function and η represents the learning rate. In distributed gradient descent, the central
server first sends the current model β(t) to all clients. Each client i then locally computes the individual gradients of the loss
function over the examples in its local dataset D and sends a linear combination of these gradients to the central server. The
i
central server then aggregates these local computations and either recovers the full gradient ∇L(D,β(t)) or obtains an estimate
of it. The previous steps are repeated iteratively until convergence. If some of the clients are unresponsive (straggler/dropout)
during certain iterations, the central server will not be able to recover the full gradient. In this case, the master’s aggregate
gives an estimate of the full gradient. In this work, we consider a straggling model where each client is unresponsive with
probability p in any given iteration, where this probability is independent across clients and iterations.Suppose that the global training dataset D consists of a total of M training examples such that there are K examples
ℓ
corresponding to class ℓ∈[L]. Let us assume that K =K for all ℓ∈[L], such that M =KL. Let Xℓ denote the proportion
ℓ i
of instances of label ℓ ∈ [L] that are owned by client i ∈ [N]. We focus on a label-heterogeneous setting where the initial
proportions Xℓ =(Xℓ,...,Xℓ )∈∆N−1 are typically far from Θ⋆ ≜(cid:0)1, 1,..., 1(cid:1) ∈∆N−1, where Θ⋆ corresponds to the
1 N N N N
perfectly label-homogeneous setting. We adopt the squared Euclidean distances between the label proportions and Θ⋆ as a
measure for label heterogeneity. Some examples for modeling label-heterogeneity are the following:
1) The single-class label-heterogeneous setting, where each client has data belonging only to a single class [10]. For instance,
for every ℓ ∈ [L], the initial label proportions could be Xℓ =(Xℓ,Xℓ,...,Xℓ )=eℓ, where eℓ is the ℓth standard basis
1 2 N
vector of RN, with N =L. In this case, Xℓ is deterministic, with ∥Xℓ−Θ⋆∥2 = N−1 for all ℓ∈[L].
N
2) TheinitiallabeldistributionsarerandomandfollowaDirichletdistributionwithasmallconcentrationparameter[23].Namely,
for every ℓ∈[L], Xℓ =(Xℓ,Xℓ,...,Xℓ )∼Dir (α), where Dir (α) is the Dirichlet distribution with N categories and
1 2 N N N
α>0 is the concentration parameter. Smaller values of α (close to zero) correspond to strongly heterogeneous settings;
while α=∞ corresponds to a perfectly homogeneous one.
Furthermore, we propose and study a privacy-flexible setting where a proportion c∈[0,1] of each client’s data is considered
to be non-private. This proportion is assumed to be evenly distributed across all classes, i.e., each client i∈[N] has Cℓ ≜cXℓK
i i
non-private training examples from class ℓ∈[L].1 We assume that clients collect their local data independently, which implies
that disclosing the non-private data of one client does not leak information about the private data of another client.
III. PROPOSEDSCHEME
To mitigate the effects of label heterogeneity and client straggling, we propose using an offline data sharing scheme that
generates redundancy across clients by replicating non-private data. We will focus on the proportions of some fixed label ℓ∈[L]
and drop the superscript ℓ. Recall that X=(X ,...,X ) denotes the initial label proportions prior to any data sharing, and
1 N
let Y =(Y ,...,Y ) denote the corresponding final label proportions after data sharing. The replication-based data sharing
1 N
scheme is described by S = (S ,...,S ) ∈ ZN, where S ≥ 0 denotes the number of non-private training examples that
1 N i
client i∈[N] receives from other clients.2 Then,
XK+S
Y = , (1)
K+B
where B
=(cid:80)N
S denotes the total amount of shared data.
i=1 i
The goal of data sharing is twofold: (i) “Break” label heterogeneity by generating final label proportions Y that are as close
as possible to Θ⋆ in order to reduce the effects of “non-IIDness”. (ii) Create data redundancy across clients which we will use
to ensure resilience against straggling clients.
As previously mentioned, we use the squared Euclidean distances between the label proportions and Θ⋆ as a measure for
label heterogeneity. Prior to any data sharing the squared distance is ∥X−Θ⋆∥2, and after data sharing, the squared distance
becomes ∥Y−Θ⋆∥2, where Y is given by (1). We aim to devise a data sharing scheme, i.e., determine S, that minimizes
∥Y−Θ⋆∥2 for a given realization X of X. If the label counts of each client’s local data D (private and non-private data)
i
are assumed to be public, one can obtain an optimal deterministic data sharing scheme that minimizes ∥Y−Θ⋆∥2 for any
realization X of X by solving a constrained optimization problem. However, knowing the label counts of each client’s data
can potentially leak some information about the private data of the clients. Therefore, we propose a randomized data sharing
scheme in Section III-A that does not require the knowledge of the label counts. We provide theoretical guarantees on the
performance of the randomized data sharing scheme by evaluating E(cid:2) ∥Y−Θ⋆∥2 |X=X(cid:3) in terms of the initial squared
distance ∥X−Θ⋆∥2, where the expectation is taken over the randomness of the data sharing process. Furthermore, in addition
to minimizing the effects of label heterogeneity, an equally important consequence of the proposed data sharing scheme is
generating redundancy across clients. In Section III-B, we present an approximate gradient coding scheme that leverages this
redundancy to provide resilience against straggling clients.
A. Randomized Data Sharing
To reduce the effects of label heterogeneity without the knowledge of the label counts of each client’s local data, we propose
using a randomized offline data sharing scheme where each client shares its non-private data with d∈{0,1,...,N −1} other
(cid:80)
clientschosenuniformlyatrandom.LetC(cid:101)i = i′̸=iC
i′
=c(1−X i)K denotethetotalnumberofnon-privateexamplesinDthat
are not owned by client i∈[N]. The randomized data sharing scheme places each of the C(cid:101)i non-private examples independently
with probability Nd
−1
at client i ∈ [N]. Thus, client i ∈ [N] receives S
i
new examples, where S
i
∼Binomial(C(cid:101)i, Nd −1) is
a random variable that follows a binomial distribution with parameters C(cid:101)i = c(1−X i)K and Nd −1. Hence, the new label
proportions after the randomized data sharing follow from (1), with B =dcK.
1Tosimplifynotation,weassumethatcXℓK areintegersforalliandℓ.
i
2Similarto[44],client-to-clientofflinecommunicationcanbeassumedtoberoutedthroughthecentralservertoensurenetworkconnectivity.The next theorem expresses the expected value of ∥Y−Θ⋆∥2 in terms of the replication factor d ∈ {0,1,...,N −1},
privacy parameter c∈[0,1], number of clients N, and the initial distance ∥X−Θ⋆∥2 for any realization X ∈∆N−1 of X.
The proof of this theorem is given in Appendix A.
Theorem 1. For any realization X ∈∆N−1 of X, the randomized data sharing generates label proportions satisfying
E(cid:2) ∥Y−Θ⋆∥2 |X=X(cid:3) = dc(N −1−d) + (N −1−dc)2 ∥X−Θ⋆∥2,
(1+dc)2(N −1)K (1+dc)2(N −1)2
where the expectation is over the randomness of the data sharing scheme. Furthermore, for K ≫1, we have
E(cid:2) ∥Y−Θ⋆∥2 |X=X(cid:3) ≈ (N −1−dc)2 ∥X−Θ⋆∥2.
(1+dc)2(N −1)2
The significance of this theorem is that for any given set of input label proportions X, the expected value of the output
distance ∥Y−Θ⋆∥2 is reduced by at least a factor of (1+dc)2 with respect to the initial distance ∥X −Θ⋆∥2 for large
enough L. This shows that the randomized data sharing scheme can effectively reduce heterogeneity without knowing the label
counts of each client’s local data.
B. Approximate Gradient Coding
As previously mentioned, at each iteration t∈{0,1,...}, the central server sends the current model β(t) to all N clients.
Each client participates in any given iteration independently with probability 1−p, i.e., the client is a straggler with probability
p. Let S ⊆[M], i∈[N], be the set of indices of the training examples (x,y)∈D that are owned by client i. The clients
i i
participating in a given iteration t send a linear combination of the partial gradients computed over their local data which is
given by
f (β(t))= (cid:88) W g(t), (2)
i j j
j∈Si
where g(t) ≜∇L(x ,y ,β(t)) denotes the partial gradient evaluated at example (x ,y ), and W is the weighting factor of
j j j j j j
example (x ,y ). The weighting factor of (x ,y ) is W = 1 , where d ∈Z+ is the total number of times the example
j j j j j (1−p)dj j
(x ,y ) is replicated across all N clients. Note that d =1 for all private data, and d =d+1 for all non-private data, where
j j j j
d is the replication factor of the randomized data sharing scheme. The central server then aggregates the local computations of
the clients to obtain an estimate of the full gradient given by
N
gˆ(t) =(cid:88) I(t)f (β(t)), (3)
i i
i=1
where I(t) =1 if client i is participating in iteration t, and I(t) =0 otherwise. One can easily show that gˆ(t) is an unbiased
i i
estimator of the full gradient g(t) =(cid:80)M g(t), i.e., E[gˆ(t)]=g(t) (see Appendix B). Note that the expectations considered
j=1 j
in this section are over the randomness of the straggling process in iteration t, conditioned on the model β(t) given by the
central server to the clients. The variance of gˆ(t) depends on the data sharing scheme being used and has a direct effect on
the rate of convergence of the algorithm. More specifically, it has been shown in [46] that for an unbiased estimator, the
value of E[∥gˆ(t)∥2] is inversely proportional to the rate of convergence under certain assumptions on the loss function. An
important trait of our proposed scheme is that it reduces the variance of the estimator and thus speeds up the convergence
of the algorithm. We theoretically demonstrate this phenomenon in Theorem 2 and illustrate it practically through numerical
simulations in Section IV. For the single-class label-heterogeneous setting described in Section II, Theorem 2 gives a lower
bound on the expected difference between the variance of gˆ(t) before and after data sharing. The bound is expressed in terms
of the replication factor d∈{0,1,...,N −1}, privacy parameter c∈[0,1], straggling probability p∈[0,1), and other system
parameters defined in Section II. The proof of this theorem is given in Appendix B.
Theorem 2. Consider the single-class label-heterogeneous setting described in Section II and let gˆ(t) be the estimate of the
X
gradient obtained in iteration t if no data is shared between clients. Let gˆ(t) be the estimate of the gradient obtained in
Y
iteration t if the offline randomized data sharing scheme is applied with replication factor d and privacy parameter c. Under
Assumption 1 (Appendix B), it holds that
E(cid:104) ∥gˆ(t)∥2−∥gˆ(t)∥2(cid:105) ≥ p d−1 (cid:88) ⟨g(t),g(t)⟩,
X Y 1−pd+1 j1 j2
(j1,j2)∈Jsn ao mn e-priv
where the expectation is over the randomness of the straggling process, and Jnon-priv is the set of all index pairs (j ,j )∈[M]2
same 1 2
such that (x ,y ) and (x ,y ) are two examples belonging to the same class where at least one of them is non-private, with
(cid:12) (cid:12) j1 j1 j2 j2
(cid:12)Jnon-priv(cid:12)=MKc(2−c).
(cid:12) same (cid:12)The result in Theorem 2 aligns with several intuitive aspects regarding the impact of the system parameters on the variance
reduction achieved by the proposed scheme. For instance, the term p indicates that the reduction in variance is more
1−p
pronounced for higher values of p, which is intuitive since one would expect the benefits of data sharing to become more
significant as the number of stragglers increases. Furthermore, the effects of the replication factor d and privacy parameter c are
reflected by the term d−1 and the number of terms of the summation, respectively, where the latter is proportional to c(2−c).
d+1
By analyzing these terms, one could show that for d>1 and a fixed offline communication cost characterized by the product dc,
higher values of c lead to a greater reduction in variance. This observation is also intuitive, as relaxing the privacy constraint
increases the diversity of redundancy generated by data sharing.
Note that the theoretical analysis above relies on Assumption 1, stated formally in Appendix B. Informally, this assumption
is based on the hypothesis that gradients computed on examples from the same class tend to align well in the feature space,
resulting in large positive scalar products between these gradients. On the other hand, gradients from different classes may
lack alignment, leading to smaller and potentially negative scalar products, reflecting the differences between the classes. This
assumption is intuitive in practical scenarios where data in different classes are typically dissimilar, provided that training
parameters are appropriately chosen to ensure proper convergence. A more detailed justification of this assumption is provided
in Appendix B, along with simulation results over real data supporting these claims.
IV. SIMULATIONRESULTS
A. Setup
We simulate a federated learning setup with N =10 clients. The goal is to train a multinomial logistic regression model
on the MNIST dataset [45], which is a supervised image classification task consisting of L=10 different classes. We use a
global training dataset D of size M =300, consisting of K =K =30 images from each class ℓ∈[L] drawn uniformly at
ℓ
random from the 60,000 training images in MNIST. To model IID/non-IID settings, we consider multiple ways for partitioning
theM =300trainingexamplesovertheN =10clients:(i)IIDsetting,wherethedataisrandomlyshuffled,andthenpartitioned
into 10 clients each receiving 30 examples. (ii) Two non-IID settings that follow from the single-class label-heterogeneous
setting and the random Dirichlet distribution with α=0.1, as described in Section II.
We apply the randomized data sharing and approximate gradient coding to the two non-IID settings, and compare the
performance of the trained model to the IID and non-IID cases with no data sharing. For each scenario, we run a total of 1000
simulations for 50 communication rounds (iterations) of the federated learning process and compute the average test accuracy
and the second moment of the gradient estimator (defined in (3)) as a function of the communication round. In each round,
the identities of the stragglers/dropouts are determined according to the straggling model parameterized by p, as explained
in Section II. We train the model using the SGD (stochastic gradient descent) optimizer with a learning rate of η =0.1 and
decay γ =0.97. The source code for these simulations can be found in [47].
Straggling probability p=0.3 Straggling probability p=0.5 Straggling probability p=0.7
0.75 0.75 0.75
0.70 0.70 0.70
0.65 0.65 0.65
0.60 0.60 0.60
0.55 Non-IID 0.55 Non-IID 0.55 Non-IID
IID IID IID
0.50 R Ra an nd do om mi iz ze ed d s sh ha ar ri in ng g ( (c c, ,d d) )= =( (0 0. .1 2, ,2 2) ) 0.50 R Ra an nd do om mi iz ze ed d s sh ha ar ri in ng g ( (c c, ,d d) )= =( (0 0. .1 2, ,3 3) ) 0.50 R Ra an nd do om mi iz ze ed d s sh ha ar ri in ng g ( (c c, ,d d) )= =( (0 0. .1 2, ,4 4) )
Randomized sharing (c,d)=(0.3,2) Randomized sharing (c,d)=(0.3,3) Randomized sharing (c,d)=(0.3,4)
0.45 Randomized sharing (c,d)=(0.4,2) 0.45 Randomized sharing (c,d)=(0.4,3) 0.45 Randomized sharing (c,d)=(0.4,4)
Randomized sharing (c,d)=(0.5,2) Randomized sharing (c,d)=(0.5,3) Randomized sharing (c,d)=(0.5,4)
0.40 0.40 0.40
6 8 10 12 14 16 18 20 6 9 12 15 18 21 24 6 9 12 15 18 21 24 27 30
Communication round Communication round Communication round
Straggling probability p=0.3 Straggling probability p=0.5 Straggling probability p=0.7
106 106
106
105
105
105
104
Non-IID
104
Non-IID Non-IID
103 I RID andomized sharing (c,d)=(0.1,2) I RID andomized sharing (c,d)=(0.1,3) 104 I RID andomized sharing (c,d)=(0.1,4)
Randomized sharing (c,d)=(0.2,2) 103 Randomized sharing (c,d)=(0.2,3) Randomized sharing (c,d)=(0.2,4) Randomized sharing (c,d)=(0.3,2) Randomized sharing (c,d)=(0.3,3) Randomized sharing (c,d)=(0.3,4)
102 R Ra an nd do om mi iz ze ed
d
s sh ha ar ri in ng
g
( (c c, ,d d) )= =( (0 0. .4 5, ,2 2)
) 102
R Ra an nd do om mi iz ze ed
d
s sh ha ar ri in ng
g
( (c c, ,d d) )= =( (0 0. .4 5, ,3 3)
)
103 R Ra an nd do om mi iz ze ed
d
s sh ha ar ri in ng
g
( (c c, ,d d) )= =( (0 0. .4 5, ,4 4)
)
6 8 10 12 14 16 18 20 6 9 12 15 18 21 24 6 9 12 15 18 21 24 27 30
Communication round Communication round Communication round
(a) Dirichlet setting α=0.1, p=0.3 (b) Dirichlet setting α=0.1, p=0.5 (c) Dirichlet setting α=0.1, p=0.7
Fig.1:Averagetestingaccuracyandsecondmomentofthegradientestimator(definedin(3))asafunctionofthecommunication
round (iteration) in the Dirichlet setting with α=0.1.
ycarucca
egarevA
rotamitse
tneidarg
fo
tnemom
dn2
egarevA
ycarucca
egarevA
rotamitse
tneidarg
fo
tnemom
dn2
egarevA
ycarucca
egarevA
rotamitse
tneidarg
fo
tnemom
dn2
egarevAStraggling probability p=0.3 Straggling probability p=0.5 Straggling probability p=0.7
0.75 0.75 0.75
0.70 0.70 0.70
0.65 0.65 0.65
0.60 0.60 0.60
0.55 Non-IID 0.55 Non-IID 0.55 Non-IID
IID IID IID
0.50 R Ra an nd do om mi iz ze ed d s sh ha ar ri in ng g ( (c c, ,d d) )= =( (0 0. .1 2, ,2 2) ) 0.50 R Ra an nd do om mi iz ze ed d s sh ha ar ri in ng g ( (c c, ,d d) )= =( (0 0. .1 2, ,3 3) ) 0.50 R Ra an nd do om mi iz ze ed d s sh ha ar ri in ng g ( (c c, ,d d) )= =( (0 0. .1 2, ,4 4) )
Randomized sharing (c,d)=(0.3,2) Randomized sharing (c,d)=(0.3,3) Randomized sharing (c,d)=(0.3,4)
0.45 Randomized sharing (c,d)=(0.4,2) 0.45 Randomized sharing (c,d)=(0.4,3) 0.45 Randomized sharing (c,d)=(0.4,4)
Randomized sharing (c,d)=(0.5,2) Randomized sharing (c,d)=(0.5,3) Randomized sharing (c,d)=(0.5,4)
0.40 0.40 0.40
6 8 10 12 14 16 18 20 6 9 12 15 18 21 24 6 9 12 15 18 21 24 27 30
Communication round Communication round Communication round
Straggling probability p=0.3 Straggling probability p=0.5 Straggling probability p=0.7
106 106
106
105
105
105
104
Non-IID
104
Non-IID Non-IID
103 I RID andomized sharing (c,d)=(0.1,2) I RID andomized sharing (c,d)=(0.1,3) 104 I RID andomized sharing (c,d)=(0.1,4)
Randomized sharing (c,d)=(0.2,2) 103 Randomized sharing (c,d)=(0.2,3) Randomized sharing (c,d)=(0.2,4) Randomized sharing (c,d)=(0.3,2) Randomized sharing (c,d)=(0.3,3) Randomized sharing (c,d)=(0.3,4)
102 R Ra an nd do om mi iz ze ed
d
s sh ha ar ri in ng
g
( (c c, ,d d) )= =( (0 0. .4 5, ,2 2)
) 102
R Ra an nd do om mi iz ze ed
d
s sh ha ar ri in ng
g
( (c c, ,d d) )= =( (0 0. .4 5, ,3 3)
)
103 R Ra an nd do om mi iz ze ed
d
s sh ha ar ri in ng
g
( (c c, ,d d) )= =( (0 0. .4 5, ,4 4)
)
6 8 10 12 14 16 18 20 6 9 12 15 18 21 24 6 9 12 15 18 21 24 27 30
Communication round Communication round Communication round
(a) Single-class setting, p=0.3 (b) Single-class setting, p=0.5 (c) Single-class setting, p=0.7
Fig.2:Averagetestingaccuracyandsecondmomentofthegradientestimator(definedin(3))asafunctionofthecommunication
round (iteration) in the single-class setting.
B. Results
The average testing accuracy over 1000 simulations is plotted in Figures 1 and 2 for different parameters and compared with
the baseline non-IID (no data sharing) and IID settings. These simulations show that there is a clear increase in the convergence
rate as the parameter c is increased from 0 to 0.5. We also observe a slight difference in the final model accuracy achieved. We
expect this difference to be more significant for more challenging datasets and more complex models. Using our proposed
scheme and by manipulating the parameters c and d, we may interpolate between the non-IID and the IID settings. Furthermore,
the plots of the average second moment of the gradient estimator validate our intuition and theoretical analysis in Section III-B.
Namely, the simulation results show that the increase in convergence rate is correlated with the decrease in the variance of the
gradient estimator, which depends on the parameters of our scheme and the straggling behavior. By comparing Figures 1 and 2,
we see that the single-class heterogeneous setting leads to worse model convergence and accuracy than the Dirichlet setting
with α=0.1. This suggests that given the single-class heterogeneous setting, one would need more data sharing to achieve the
same performance as the Dirichlet case.
V. CONCLUSION
In conclusion, our theoretical and numerical results demonstrate that under the privacy-flexible FL paradigm that we introduce,
combining data sharing with gradient coding enables a deliberate trade-off between privacy (characterized by the parameter
c) and utility (characterized by model convergence and accuracy). This trade-off is achieved at the expense of a one-time
offline communication cost and additional local computation, which is a common price to pay when mitigating the effects of
stragglers and non-IID data in distributed settings. In future work, we intend to analyze this trade-off for different models and
datasets, and also other algorithms such as federated averaging, where multiple local model updates are performed in a single
communication round.
REFERENCES
[1] B.McMahan,E.Moore,D.Ramage,S.Hampson,andB.A.yArcas,“Communication-efficientlearningofdeepnetworksfromdecentralizeddata,”in
Artificialintelligenceandstatistics. PMLR,2017,pp.1273–1282.
[2] P.Kairouz,H.B.McMahan,B.Aventetal.,“Advancesandopenproblemsinfederatedlearning,”FoundationsandTrends®inMachineLearning,
vol.14,no.1–2,pp.1–210,2021.
[3] D.Jakovetic,“Distributedoptimization:Algorithmsandconvergencerates,”PhDThesis,CarnegieMellonUniversity,PittsburghPA,USA,2013.
[4] M.Li,D.G.Andersen,J.W.Park,A.J.Smola,A.Ahmed,V.Josifovski,J.Long,E.J.Shekita,andB.-Y.Su,“Scalingdistributedmachinelearning
withtheparameterserver,”in11thUSENIXSymposiumonoperatingsystemsdesignandimplementation(OSDI14),2014,pp.583–598.
[5] F.ZhouandG.Cong,“Ontheconvergencepropertiesofak-stepaveragingstochasticgradientdescentalgorithmfornonconvexoptimization,”arXiv
preprintarXiv:1708.01012,2017.
[6] S.U.Stich,“LocalSGDconvergesfastandcommunicateslittle,”arXivpreprintarXiv:1805.09767,2018.
[7] J.WangandG.Joshi,“CooperativeSGD:Aunifiedframeworkforthedesignandanalysisoflocal-updateSGDalgorithms,”TheJournalofMachine
LearningResearch,vol.22,no.1,pp.9709–9758,2021.
ycarucca
egarevA
rotamitse
tneidarg
fo
tnemom
dn2
egarevA
ycarucca
egarevA
rotamitse
tneidarg
fo
tnemom
dn2
egarevA
ycarucca
egarevA
rotamitse
tneidarg
fo
tnemom
dn2
egarevA[8] B.E.Woodworth,J.Wang,A.Smith,B.McMahan,andN.Srebro,“Graphoraclemodels,lowerbounds,andgapsforparallelstochasticoptimization,”
Advancesinneuralinformationprocessingsystems,vol.31,2018.
[9] T.-M. H. Hsu, H. Qi, and M. Brown, “Measuring the effects of non-identical data distribution for federated visual classification,” arXiv preprint
arXiv:1909.06335,2019.
[10] Y.Zhao,M.Li,L.Lai,N.Suda,D.Civin,andV.Chandra,“Federatedlearningwithnon-iiddata,”arXivpreprintarXiv:1806.00582,2018.
[11] S. P. Karimireddy, S. Kale, M. Mohri, S. Reddi, S. Stich, and A. T. Suresh, “Scaffold: Stochastic controlled averaging for federated learning,” in
Internationalconferenceonmachinelearning. PMLR,2020,pp.5132–5143.
[12] Y.Shi,J.Liang,W.Zhang,V.Y.Tan,andS.Bai,“Towardsunderstandingandmitigatingdimensionalcollapseinheterogeneousfederatedlearning,”
arXivpreprintarXiv:2210.00226,2022.
[13] J.Wang,Q.Liu,H.Liang,G.Joshi,andH.V.Poor,“Tacklingtheobjectiveinconsistencyprobleminheterogeneousfederatedoptimization,”Advances
inneuralinformationprocessingsystems,vol.33,pp.7611–7623,2020.
[14] H.Wang,M.Yurochkin,Y.Sun,D.Papailiopoulos,andY.Khazaeni,“Federatedlearningwithmatchedaveraging,”arXivpreprintarXiv:2002.06440,
2020.
[15] Q.Li,B.He,andD.Song,“Model-contrastivefederatedlearning,”inProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,
2021,pp.10713–10722.
[16] A.E.Durmus,Z.Yue,M.Ramon,M.Matthew,W.Paul,andS.Venkatesh,“Federatedlearningbasedondynamicregularization,”inInternational
ConferenceonLearningRepresentations,2021.
[17] V.Smith,C.-K.Chiang,M.Sanjabi,andA.S.Talwalkar,“Federatedmulti-tasklearning,”Advancesinneuralinformationprocessingsystems,vol.30,
2017.
[18] T.Li,S.Hu,A.Beirami,andV.Smith,“Ditto:Fairandrobustfederatedlearningthroughpersonalization,”inInternationalConferenceonMachine
Learning. PMLR,2021,pp.6357–6368.
[19] A. Fallah, A. Mokhtari, and A. Ozdaglar, “Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach,”
AdvancesinNeuralInformationProcessingSystems,vol.33,pp.3557–3568,2020.
[20] Y.Deng,M.M.Kamani,andM.Mahdavi,“Adaptivepersonalizedfederatedlearning,”arXivpreprintarXiv:2003.13461,2020.
[21] P.P.Liang,T.Liu,L.Ziyin,N.B.Allen,R.P.Auerbach,D.Brent,R.Salakhutdinov,andL.-P.Morency,“Thinklocally,actglobally:Federatedlearning
withlocalandglobalrepresentations,”arXivpreprintarXiv:2001.01523,2020.
[22] Y. Mansour, M. Mohri, J. Ro, and A. T. Suresh, “Three approaches for personalization with applications to federated learning,” arXiv preprint
arXiv:2002.10619,2020.
[23] M.Yurochkin,M.Agarwal,S.Ghosh,K.Greenewald,N.Hoang,andY.Khazaeni,“Bayesiannonparametricfederatedlearningofneuralnetworks,”in
Internationalconferenceonmachinelearning. PMLR,2019,pp.7252–7261.
[24] H.Zhang,M.Cisse,Y.N.Dauphin,andD.Lopez-Paz,“mixup:Beyondempiricalriskminimization,”arXivpreprintarXiv:1710.09412,2017.
[25] E.Jeong,S.Oh,H.Kim,J.Park,M.Bennis,andS.-L.Kim,“Communication-efficienton-devicemachinelearning:Federateddistillationandaugmentation
undernon-iidprivatedata,”arXivpreprintarXiv:1811.11479,2018.
[26] J.GoetzandA.Tewari,“Federatedlearningviasyntheticdata,”arXivpreprintarXiv:2008.04489,2020.
[27] M.Rasouli,T.Sun,andR.Rajagopal,“Fedgan:Federatedgenerativeadversarialnetworksfordistributeddata,”arXivpreprintarXiv:2006.07228,2020.
[28] W.Hao,M.El-Khamy,J.Lee,J.Zhang,K.J.Liang,C.Chen,andL.C.Duke,“Towardsfairfederatedlearningwithzero-shotdataaugmentation,”in
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,2021,pp.3310–3319.
[29] T.Yoon,S.Shin,S.J.Hwang,andE.Yang,“Fedmix:Approximationofmixupundermeanaugmentedfederatedlearning,”arXivpreprintarXiv:2107.00233,
2021.
[30] N.Yoshida,T.Nishio,M.Morikura,K.Yamamoto,andR.Yonetani,“Hybrid-flforwirelessnetworks:Cooperativelearningmechanismusingnon-IID
data,”inICC2020-2020IEEEInternationalConferenceOnCommunications(ICC). IEEE,2020,pp.1–7.
[31] Z.CharlesandJ.Konecˇny`,“Ontheoutsizedimportanceoflearningratesinlocalupdatemethods,”arXivpreprintarXiv:2007.00878,2020.
[32] A.Mitra,R.Jaafar,G.J.Pappas,andH.Hassani,“Linearconvergenceinfederatedlearning:Tacklingclientheterogeneityandsparsegradients,”Advances
inNeuralInformationProcessingSystems,vol.34,pp.14606–14619,2021.
[33] R.Tandon,Q.Lei,A.G.Dimakis,andN.Karampatziakis,“Gradientcoding:Avoidingstragglersindistributedlearning,”inInternationalConferenceon
MachineLearning,2017,pp.3368–3376.
[34] M.YeandE.Abbe,“Communication-computationefficientgradientcoding,”inInternationalConferenceonMachineLearning. PMLR,2018,pp.
5610–5619.
[35] K.Lee,M.Lam,R.Pedarsani,D.Papailiopoulos,andK.Ramchandran,“Speedingupdistributedmachinelearningusingcodes,”IEEETransactionson
InformationTheory,vol.64,no.3,pp.1514–1529,2018.
[36] C.Karakus,Y.Sun,S.Diggavi,andW.Yin,“Stragglermitigationindistributedoptimizationthroughdataencoding,”inAdvancesinNeuralInformation
ProcessingSystems,2017,pp.5434–5442.
[37] E.Ozfatura,S.Ulukus,andD.Gu¨ndu¨z,“Straggler-awaredistributedlearning:Communication–computationlatencytrade-off,”Entropy,vol.22,no.5,
2020.
[38] Z.Charles,D.Papailiopoulos,andJ.Ellenberg,“Approximategradientcodingviasparserandomgraphs,”arXivpreprintarXiv:1711.06771,2017.
[39] R.Bitar,M.Wootters,andS.ElRouayheb,“Stochasticgradientcodingforstragglermitigationindistributedlearning,”IEEEJournalonSelectedAreas
inInformationTheory,vol.1,no.1,pp.277–291,2020.
[40] H.Wang,Z.Charles,andD.Papailiopoulos,“Erasurehead:Distributedgradientdescentwithoutdelaysusingapproximategradientcoding,”arXivpreprint
arXiv:1901.09671,2019.
[41] S.Wang,J.Liu,andN.Shroff,“Fundamentallimitsofapproximategradientcoding,”ProceedingsoftheACMonMeasurementandAnalysisofComputing
Systems,vol.3,no.3,pp.1–22,2019.
[42] M.GlasgowandM.Wootters,“Approximategradientcodingwithoptimaldecoding,”IEEEJournalonSelectedAreasinInformationTheory,vol.2,
no.3,pp.855–866,2021.
[43] S.Prakash,S.Dhakal,M.R.Akdeniz,Y.Yona,S.Talwar,S.Avestimehr,andN.Himayat,“Codedcomputingforlow-latencyfederatedlearningover
wirelessedgenetworks,”IEEEJournalonSelectedAreasinCommunications,vol.39,no.1,pp.233–250,2020.
[44] R.Schlegel,S.Kumar,E.Rosnes,andA.G.iAmat,“CodedPaddedFLandCodedSecAgg:stragglermitigationandsecureaggregationinfederated
learning,”IEEETransactionsonCommunications,2023.
[45] L. Deng, “The MNIST database of handwritten digit images for machine learning research,” IEEE Signal Processing Magazine, vol. 29, no. 6, pp.
141–142,2012.
[46] A.Rakhlin,O.Shamir,andK.Sridharan,“Makinggradientdescentoptimalforstronglyconvexstochasticoptimization,”arXivpreprintarXiv:1109.5647,
2011.
[47] O.Makkonen,S.Niemela¨,C.Hollanti,andS.KasHanna,“Approximategradientcodingforprivacy-flexiblefederatedlearningwithnon-iiddata(github
repository),”2024,https://github.com/okkomakkonen/label-heterogeneity.APPENDIXA
PROOFOFTHEOREM1
We have
E(cid:2) ∥Y−Θ⋆∥2 |X=X(cid:3)
=E(cid:34) (cid:88)N (cid:18)
Y i− N1
(cid:19)2 (cid:12)
(cid:12) (cid:12)
(cid:12)
X i =X
i(cid:35)
(4)
i=1
N
=(cid:88) E(cid:2) Y2|X =X (cid:3) − 2 E[Y |X =X ]+ 1 . (5)
i i i N i i i N2
i=1
Since B =dcK for the randomized data sharing scheme, then it follows from (1) that
1 (cid:18) E[S ](cid:19) 1 (cid:18) dc (cid:19)
E[Y |X =X ]= X + i = X + (1−X ) . (6)
i i i 1+dc i K 1+dc i N −1 i
Furthermore,
E(cid:2) Y2 |X =X (cid:3) = 1 (cid:18) X2+2E[S i] X + E[S2 i](cid:19) (7)
i i i (1+dc)2 i K i K2
1 (cid:18) dc dc (cid:18) d (cid:19) d2c2 (cid:19)
= X2+2 (1−X )X + 1− (1−X )+ (1−X )2 .
(1+dc)2 i N −1 i i (N −1)K N −1 i (N −1)2 i
(8)
By substituting (6) and (8) in (5) we get
N
E(cid:2) ∥Y−Θ⋆∥2 |X=X(cid:3) = dc(N −1−d) (cid:88) 1−X
(1+dc)2(N −1)2K i
i=1
1 (cid:18) dc d2c2 (cid:19) (cid:88)N
+ 1−2 + X2
(1+dc)2 N −1 (N −1)2 i
i=1
(cid:124) (cid:123)(cid:122) (cid:125)
T1
2 1 (cid:18) d2c2N dcN dc(1+dc)(cid:19) (cid:88)N
− 1+dc+ − − X
N (1+dc)2 (N −1)2 N −1 N −1 i
i=1
(cid:124) (cid:123)(cid:122) (cid:125)
T2
1 1 (cid:18) (1+dc)dcN d2c2N2 (cid:19)
+ (1+dc)2−2 + . (9)
N2(1+dc)2 N −1 (N −1)2
(cid:124) (cid:123)(cid:122) (cid:125)
T3
One can easily verify that
(N −1−dc)2
T =T =T = . (10)
1 2 3 (N −1)2
Therefore,
E(cid:2) ∥Y−Θ⋆∥2 |X=X(cid:3) = dc(N −1−d) (cid:88)N 1−X + (N −1−dc)2 (cid:88)N (cid:18) X − 1 (cid:19)2 . (11)
(1+dc)2(N −1)2K i (1+dc)2(N −1)2 i N
i=1 i=1
Since X ∈∆N−1, we have (cid:80)N 1−X =N −1. The proof is concluded by substituting the latter equality in the first term
i=1 i
in (11), and by expressing the summation in the second term as ∥X−Θ∗∥2.
APPENDIXB
PROOFOFTHEOREM2
The gradient estimate obtained at iteration t can be expressed as
gˆ(t)
=(cid:88)M Z( jt)
g(t),
(1−p)d j
j
j=1
where d ∈[N] is the number of replicates of example (x ,y ) that are available across the N clients and Z(t) ∈[d ] is the
j j j j j
random variable representing the number of non-stragglers participating in iteration t that have example (x ,y ). Suppose that
j j(x ,y ) is replicated across d different clients indexed by i ,i ,...,i . Then, Z(t) =I(t)+I(t)+...+I(t) where I(t) =1 if
j j j 1 2 dj j i1 i2 idj i
client i is participating in iteration t, and I(t) = 0 otherwise. It follows from the straggling model that I(t),I(t),...,I(t) are
i i1 i2 idj
independent Bernoulli random variables, and hence Z(t) is a binomial random variable with parameters d and 1−p. Therefore,
j j
by linearity of expectation, we have
(cid:104) (cid:105)
M E Z(t) M
E(cid:104) gˆ(t)(cid:105) =(cid:88) j g(t) =(cid:88) g(t) =g(t), (12)
(1−p)d j j
j
j=1 j=1
which proves that the estimator is unbiased. Recall that the expectation computed in (12) and henceforth is over the randomness
of the straggling process in iteration t, conditioned on the model β(t) given by the central server to the clients. Furthermore,
the second moment of the gradient estimator is given by
(cid:104) (cid:105)
E Z(t)Z(t)
E(cid:104) ∥gˆ(t)∥2(cid:105) = (cid:88) j1 j2 ⟨g(t),g(t)⟩. (13)
(j1,j2)∈[M]2
(1−p)2d j1d
j2
j1 j2
The analysis of the second moment depends on how the M training examples are distributed across the clients. Consider the
single-class label-heterogeneous setting, and let J and J be the set of all index pairs (j ,j )∈[M]2 such that (x ,y )
same diff 1 2 j1 j1
and (x ,y ) are two examples belonging to the same class and different classes, respectively. Prior to data sharing, we have
j2 j2
d = d = 1 for all (j ,j ) ∈ [M]2. Furthermore, if (j ,j ) ∈ J , we have Z(t)Z(t) = I(t)I(t) for some i ∈ [N], else if
j1 j2 1 2 1 2 same j1 j2 i i
(j ,j )∈J , then Z(t)Z(t) =I(t)I(t) for some (i ,i )∈[N]2 with i ̸=i . Therefore,
1 2 diff j1 j2 i1 i2 1 2 1 2
(cid:104) (cid:105) (cid:104) (cid:105)
E I(t)I(t) E I(t)I(t)
E(cid:104) ∥gˆ(t)∥2(cid:105) = i i (cid:88) ⟨g(t),g(t)⟩+ i1 i2 (cid:88) ⟨g(t),g(t)⟩, (14)
X (1−p)2 j1 j2 (1−p)2 j1 j2
(j1,j2)∈Jsame (j1,j2)∈Jdiff
= 1 (cid:88) ⟨g(t),g(t)⟩+ (cid:88) ⟨g(t),g(t)⟩. (15)
1−p j1 j2 j1 j2
(j1,j2)∈Jsame (j1,j2)∈Jdiff
To evaluate the second moment of the estimator after randomized data sharing is applied, we need to partition [M]2 into more
(cid:104) (cid:105)
parts since the values of d , d , and E Z(t)Z(t) , will depend on whether the considered examples are private or not and
j1 j2 j1 j2
whether they belong to the same class of not. The partitions are denoted by Jp-p , Jp-p. Jnp-p, Jnp-p, Jnp-np, and Jnp-np. The
same diff same diff same diff
superscript p-p indicates that both examples are private, p-np that one example is private and the other is non-private, and
np-np that both examples are non-private. The subscript “same” indicates that both examples belong to the same class and
“diff” that the two examples belong to different classes. Let λp-p ≜
E(cid:104) Z( jt 1)Z j(t 2)(cid:105)
for (j ,j )∈Jp-p , and define similarly the
quantities λp-p, λnp-p, λnp-p, λnp-np, and λnp-np. Hence, we
havesame (1−p)2dj1dj2 1 2 same
diff same diff same diff
E(cid:104) ∥gˆ(t)∥2(cid:105) =λp-p (cid:88) ⟨g(t),g(t)⟩+λp-p (cid:88) ⟨g(t),g(t)⟩+λnp-p (cid:88) ⟨g(t),g(t)⟩
Y same j1 j2 diff j1 j2 same j1 j2
(j1,j2)∈Jsp a- mp e (j1,j2)∈J dp i- ffp (j1,j2)∈Jsn ap m-p e
+λnp-p (cid:88) ⟨g(t),g(t)⟩+λnp-np (cid:88) ⟨g(t),g(t)⟩+λnp-np (cid:88) ⟨g(t),g(t)⟩. (16)
diff j1 j2 same j1 j2 diff j1 j2
(j1,j2)∈J dn ip ff-p (j1,j2)∈Jsn ap m-n ep (j1,j2)∈J dn ip ff-np
Proposition 1. For p∈[0,1) and d∈{0,1,...,N −1}, we have
1 1+d(1−p) 1+d(1−p)
λp-p = , λp-p =1, λnp-p = , 1≤λnp-p ≤λnp-p, 1≤λnp-np ≤ , 1≤λnp-np ≤λnp-np.
same 1−p diff same (1−p)(d+1) diff same same (1−p)(d+1) diff same
From Proposition 1, (15), and (16), we obtain
E(cid:104) ∥gˆ(t)∥2(cid:105) =E(cid:104) ∥gˆ(t)∥2(cid:105) +(cid:18) λnp-p − 1 (cid:19) (cid:88) ⟨g(t),g(t)⟩+(cid:0) λnp-p−1(cid:1) (cid:88) ⟨g(t),g(t)⟩
Y X same 1−p j1 j2 diff j1 j2
(j1,j2)∈Jsn ap m-p e (j1,j2)∈J dn ip ff-p
(cid:18) (cid:19)
+ λnp-np− 1 (cid:88) ⟨g(t),g(t)⟩+(cid:0) λnp-np−1(cid:1) (cid:88) ⟨g(t),g(t)⟩. (17)
same 1−p j1 j2 diff j1 j2
(j1,j2)∈Jsn ap m-n ep (j1,j2)∈J dn ip ff-npBy rearranging the terms we get
E(cid:104) ∥gˆ(t)∥2−∥gˆ(t)∥2(cid:105) =(cid:18) 1 −λnp-p(cid:19) (cid:88) ⟨g(t),g(t)⟩−(cid:0) λnp-p−1(cid:1) (cid:88) ⟨g(t),g(t)⟩
X Y 1−p same j1 j2 diff j1 j2
(j1,j2)∈Jsn ap m-p e (j1,j2)∈J dn ip ff-p
(cid:18) (cid:19)
+ 1 −λnp-np (cid:88) ⟨g(t),g(t)⟩−(cid:0) λnp-np−1(cid:1) (cid:88) ⟨g(t),g(t)⟩. (18)
1−p same j1 j2 diff j1 j2
(j1,j2)∈Jsn ap m-n ep (j1,j2)∈J dn ip ff-np
Assumption 1. We assume that
(cid:26) (cid:27) (cid:26) (cid:27)
(cid:88) ⟨g(t),g(t)⟩≥max 0, (cid:88) ⟨g(t),g(t)⟩ and (cid:88) ⟨g(t),g(t)⟩≥max 0, (cid:88) ⟨g(t),g(t)⟩ .
j1 j2 j1 j2 j1 j2 j1 j2
(j1,j2)∈Jsn ap m-p e (j1,j2)∈J dn ip ff-p (j1,j2)∈Jsn ap m-n ep (j1,j2)∈J dn ip ff-np
We justify this assumption based on our intuition that the scalar products of gradients computed on examples from the same
class tend to have positive and higher values compared to those computed on examples from different classes, provided that the
iterative algorithm is converging properly. We also substantiate this intuition with simulation results over MNIST given in
Fig. 3. More precisely, we expect gradients computed on examples belonging to the same class to align well in the feature
space, leading to large and positive scalar products. Such alignments would indicate that the gradients are reinforcing each
other’s contributions towards adjusting the model parameters to better classify instances of that class. Conversely, for gradients
computed on examples belonging to different classes, there might not be as much alignment between the gradients, leading
to lower and potentially negative scalar products. In this case, the gradients may be pushing the decision boundaries away
from each other or towards different directions in the feature space, reflecting the differences between the classes. Based on
Assumption 1 and Proposition 1 we get
E(cid:104) ∥gˆ(t)∥2−∥gˆ(t)∥2(cid:105) ≥(cid:18) 2−p −λnp-p −λnp-p(cid:19) (cid:88) ⟨g(t),g(t)⟩+(cid:18) 2−p −λnp-np−λnp-np(cid:19) (cid:88) ⟨g(t),g(t)⟩,
X Y 1−p same diff j1 j2 1−p same diff j1 j2
(j1,j2)∈Jsn ap m-p
e
(j1,j2)∈Jsn ap m-n ep
(19)
≥ p d−1 (cid:88) ⟨g(t),g(t)⟩, (20)
1−pd+1 j1 j2
(j1,j2)∈Jsn ap m-p e∪Jsn ap m-n ep
where (cid:12) (cid:12)J sn ap m-p
e
∪J sn ap m-n ep(cid:12) (cid:12)=|J same|−(cid:12) (cid:12)J sp a- mp e(cid:12) (cid:12)=MK−MK(1−c)2 =MKc(2−c).
Straggling probability p=0 Straggling probability p=0.5
1010 1010
108 108
106 106
104 104
102 102
Squared Euclidean norm of full gradient (Type I + Type II) Squared Euclidean norm of full gradient (Type I + Type II)
0 Sum of Type I scalar products 0 Sum of Type I scalar products
Sum of Type II scalar products Sum of Type II scalar products
102 102
104 104
106 106
108 108
1010 1010
0 200 400 600 800 1000 0 200 400 600 800 1000
Iteration Iteration
(a) No data sharing, p=0. (b) Randomized data sharing, (c,d)=(0.2,3) and p=0.5.
Fig.3:Thesamesimulationsetup,asexplainedinSectionIV-AbasedontheMNISTdataset,isconsideredunderthesingle-class
label-heterogeneous setting. The figure shows the sums of the scalar products of gradients computed on examples from the same
and different classes, referred to as Type I and Type II scalar products, respectively. These sums are reported at each iteration t
of the algorithm. Additionally, the squared Euclidean norm of the full gradient g(t) =(cid:80)M g(t), obtained by summing both
j=1 j
Type I and II scalar products, is also given. A fixed learning rate of η =0.01 is used.
mus
tcudorp
ralacS
mus
tcudorp
ralacSRemark 1. The bound derived in Theorem 2 is trivial for d = 1. This is a consequence of Assumption 1, which does not
make any assumptions regarding the signs of the summations of scalar products of gradients computed on examples from
different classes. Our simulation results in Fig. 3 suggest that these scalar products tend to have negative values that decrease
in magnitude as the algorithm converges. Assuming these summations in (18) to be negative, one could obtain
E(cid:104) ∥gˆ(t)∥2−∥gˆ(t)∥2(cid:105) ≥ p d (cid:88) ⟨g(t),g(t)⟩.
X Y 1−pd+1 j1 j2
(j1,j2)∈Jsn ap m-p e∪Jsn ap m-n ep