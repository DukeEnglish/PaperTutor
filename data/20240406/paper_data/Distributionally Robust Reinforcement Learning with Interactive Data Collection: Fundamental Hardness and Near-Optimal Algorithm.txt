Distributionally Robust Reinforcement Learning with Interactive
Data Collection: Fundamental Hardness and Near-Optimal
Algorithm
Miao Lu∗† Han Zhong∗‡ Tong Zhang§ Jose Blanchet†
April 5, 2024
Abstract
Thesim-to-realgap,whichrepresentsthedisparitybetweentrainingandtestingenvironments,poses
asignificantchallengeinreinforcementlearning(RL).Apromisingapproachtoaddressingthischallenge
isdistributionally robustRL,often framed as arobust Markovdecision process(RMDP).Inthisframe-
work,theobjectiveistofindarobustpolicythatachievesgoodperformanceundertheworst-casescenario
amongallenvironmentswithinapre-specifieduncertaintysetcenteredaroundthetrainingenvironment.
Unlikepreviouswork,whichrelies onagenerativemodelorapre-collectedofflinedataset enjoyinggood
coverage of the deployment environment, we tackle robust RL via interactive data collection, where the
learnerinteractswiththetrainingenvironmentonlyandrefinesthepolicythroughtrialanderror. Inthis
robust RL paradigm, two main challenges emerge: managing distributional robustness while striking a
balancebetweenexploration andexploitation duringdatacollection. Initially, weestablish thatsample-
efficientlearningwithoutadditionalassumptionsisunattainableowingtothecurseofsupportshift;i.e.,
thepotentialdisjointednessofthedistributionalsupportsbetweenthetrainingandtestingenvironments.
Tocircumvent such a hardness result, we introducethevanishing minimal value assumption toRMDPs
withatotal-variation(TV)distancerobustset,postulatingthattheminimalvalueoftheoptimalrobust
value function is zero. We prove that such an assumption effectively eliminates the support shift issue
forRMDPswithaTV distancerobustset,andpresentanalgorithm withaprovablesamplecomplexity
guarantee. Ourwork makes the initial step to uncovering the inherent difficulty of robust RL via inter-
activedatacollectionandsufficientconditionsfordesigningasample-efficientalgorithmaccompaniedby
sharp sample complexity analysis.
Keywords: distributionally robust reinforcement learning, interactive data collection, robust Markov deci-
sion process, sample complexity, online regret
Contents
1 Introduction 2
1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.3 Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2 Preliminaries 7
2.1 Robust Markov Decision Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.2 Robust RL with Interactive Data Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3 A Hardness Result: The Curse of Support Shift 10
∗Equalcontributions. Emailtomiaolu@stanford.edu, hanzhong@stu.pku.edu.cn
†DepartmentofManagement ScienceandEngineering,StanfordUniversity.
‡CenterforDataScience,PekingUniversity.
§DepartmentofComputer Science,UniversityofIllinoisUrbana-Champaign.
1
4202
rpA
4
]GL.sc[
1v87530.4042:viXra4 A Solvable Case, Efficient Algorithm, and Sharp Analysis 11
4.1 Vanishing Minimal Value: Eliminating Support Shift . . . . . . . . . . . . . . . . . . . . . . . 12
4.2 Algorithm Design: OPROVI-TV . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
4.2.1 Training Environment Transition Estimation . . . . . . . . . . . . . . . . . . . . . . . 13
4.2.2 Optimistic Robust Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
4.3 Theoretical Guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
4.4 Extensions to Robust Set with Bounded Transition Probability Ratio. . . . . . . . . . . . . . 16
5 Conclusions and Discussions 17
A Proofs for Properties of RMDPs with TV Robust Sets 23
A.1 Proof of Proposition 2.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
A.2 Proof of Proposition 2.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
A.3 Proof of Proposition 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
B Proofs for Hardness Results 27
B.1 Proof of Theorem 3.2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
B.2 Proof of Lemma B.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
C Proofs for Theoretical Analysis of OPROVI-TV 30
C.1 Proof of Theorem 4.6. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
C.2 Key Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
C.3 Proof of Lemma C.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
C.4 Proof of Lemma C.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
C.5 Proof of Lemma C.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
C.6 Proof of Lemma C.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
C.7 Proof of Lemma C.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
C.8 Other Technical Lemmas. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
C.8.1 Concentration Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
C.8.2 Variance Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
C.8.3 Other Auxiliary Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
D Proofs for Extensions in Section 4.4 44
D.1 Proof of Corollary 4.8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
1 Introduction
Reinforcementlearning(RL)servesasaframeworkforaddressingcomplexdecision-makingproblemsthrough
iterative interactions with environments. The advancements in deep reinforcement learning have enabled
thesuccessfulapplicationofthegeneralRLframeworkacrossvariousdomains,includingmasteringstrategic
games,suchasGo(Silver et al.,2017),robotics(Kober et al.,2013),andtuninglargelanguagemodels(LLM
Ouyang et al.(2022)). Thecriticalfactorscontributingtothesesuccessesencompassnotonlythepotencyof
deep neural networks and modern deep RL algorithms but also the availability of substantial training data.
However, there are scenarios, such as healthcare (Wang et al., 2018) and autonomous driving (Kiran et al.,
2021), among others, where collecting data in the target domain is challenging, costly, or even unfeasible.
In such cases, the sim-to-real transfer (Kober et al., 2013; Sadeghi and Levine, 2016; Peng et al., 2018;
Zhao et al., 2020) becomes a remedy – a process in which RL agents are trained in some simulated environ-
ment and subsequently deployed in real-world settings. Nevertheless, the training environment may differ
fromthe real-worldenvironment. Sucha discrepancy,alsoknownasthe sim-to-real gap, willtypicallyresult
in suboptimal performance of RL agents in real-world applications. One promising strategy to control the
impactinperformancedegradationduetothesim-to-realgapisrobustRL(Iyengar,2005;Pinto et al.,2017;
Hu et al., 2022), whichaims to learnpolicies exhibiting strong(i.e. robust)performance under environmen-
taldeviationsfromthetrainingenvironment,effectivelyhedgingtheepistemologicaluncertaintyarisingfrom
the differences between the training environment and the unknown testing environments.
2A robust RL problem is often formulated within a robust Markov decision process (RMDP) framework,
with various types of robust sets characterizing different environmental perturbations. In this robust RL
context, prior works have developed algorithms with provable sample complexity guarantees. However,
these algorithms typically rely on either a generative model1 (Yang et al., 2022; Panagantiand Kalathil,
2022; Xu et al., 2023; Shi et al., 2023) or offline datasets with good coverage of the deployment environ-
ment (Zhou et al., 2021b; Panagantiet al., 2022; Shi and Chi, 2022; Ma et al., 2022; Blanchet et al., 2023).
Notably, the current literature does not explicitly address the exploration problem, which stands as one
of the fundamental challenges in reinforcement learning through trial-and-error (Sutton and Barto, 2018).
Meanwhile, the empirical success of robust RL methods (Pinto et al., 2017; Kuang et al., 2022; Moos et al.,
2022) typically relies on reinforcement learning through interactive data collection in the training environ-
ment, where the agent iteratively and actively interacts with the environment, collecting data, optimizing
and robustifying its policy. Given that all of the existing literature on the theory of robust RL relies on a
generative model or a pre-collected offline dataset, it is natural to ask:
Can we design a provably sample-efficient robust RL algorithm that relies on
interactive data collection in the training environment?
Answering the above question faces a fundamental challenge, namely, that during the interactive data
collection process, the learner no longer has the oracle control over the training data distributions that are
inducedbythepolicylearnedthroughtheinteractionprocess. Inparticular,itcouldbethecasethatcertain
data patterns that are crucial for the policy to be robust across all testing environments are not accessible
throughinteractivedata collection,eventhroughasophisticateddesignofanexplorationmechanismduring
the interaction process. For example, specific states may not be accessible within the training environment
dynamics but could be reached in the testing environment dynamics.
Incontrast,previousworkhasdemonstratedthatrobustRLthroughagenerativemodelorapre-collected
offline dataset with good coverage does not face such difficulty. In the generative model setup, fortunately,
the learner can directly query any state-action pair and obtain the sampled next state from the generator.
Intuitively,oncethestatesthatcouldappearinthetestingenvironmenttrajectoryarequeriedenoughtimes,
it is possible to guarantee the performance of the learned policy in testing environments. The situation is
similar if one has a pre-collected offline dataset that possesses good coverage of the testing environment.
In this work, we make the initial steps towards answering the above questions regarding robust RL with
interactive data collection. At a high level, our results are two-fold.
• (Fundmental hardness.) We prove a hardness result for robust RL with interactive data collection.
Precisely,certainRMDPsthataresolvablesample-efficientlywithagenerativemodelorwithsufficient
offlinedatawithgoodcoveragepropertiesare,incontrast,intractable forrobustRLthroughinteractive
datacollection. ThisshowsagapbetweenrobustRLwiththesetwodifferentkindsofdata-typeoracles.
• (Solvable case and sample-efficient algorithm.) We identify a tractable subclass of those RMDPs, for
which we further propose a novel robust RL algorithm that can provably learn a near-optimal robust
policythroughinteractivedatacollection. Thisimplies thatrobustRLwithinteractivedatacollection
is still possible for certain subclasses of RMDPs.
These two results combined answer the above question. In the following section, we explain more explicitly
the problem setup we study and the theoretical contributions we make.
1.1 Contributions
Inthiswork,westudyrobustRLinafinite-horizonRMDPwithan -rectangulartotal-variationdistance
S×A
(TV) robust set (see Assumption 2.1 and Definition 2.4)2 through interactive data collection. We give both
1A generative model heremeans amechanism that when queriedatsomestate, action, andtimestep, returnsasampleof
next state. Herewe distinguishthis notion with the notion of simulator or simulated environment which generallyrefers to a
human-madetrainingenvironmentthatmimicsthereal-worldenvironment.
2We notice that all of the previous workon sample-efficient robust RL inRMDPs withTV robust sets (Yangetal., 2022;
Panaganti andKalathil, 2022; Panaganti etal.,2022; Xuetal., 2023; Blanchetetal., 2023; Shietal., 2023)relieson defining
theTVdistancethroughthegeneralf-divergencesothatastrongdualityrepresentationholds. Butthisimplicitlyrequiresthe
testingenvironmenttransitionprobabilityisabsolutecontinuousw.r.t. thetrainingenvironmenttransitionprobability. Inthis
paper, wedonot makesuch arestriction. We provethe samestrongdualityeven ifthe absolutecontinuity does not hold. In
fact,allthepreviousworkcanbedirectlyextended tosuchTVdistancedefinitionviaourmoregeneralstrongdualityresult.
3a fundamental hardness resultin the generalcase anda sample-efficientalgorithmwithin tractable settings.
Fundamental hardness. We construct a class of hard-to-learn RMDPs (see Example 3.1 and Figure 1)
anddemonstratethatany learningalgorithminevitablyincursanΩ(ρ HK)-onlineregret(see(2.4))underat
·
leastoneRMDPinstance. Here,ρsignifiestheradiusoftheTVrobustuncertaintyset,H isthehorizon,and
K denotes the number of interactive episodes. This linear regret lower bound underscores the impossibility
of sample-efficient robust RL via interactive data collection in general.
Identifying a tractable case. Uponcloseexaminationofthe challenginginstance,we recognizethatthe
primaryobstacletoachievingsample-efficientlearningliesinthecurseofsupportshift,i.e.,thedisjointedness
ofdistributionalsupportbetweenthetrainingenvironmentandthetestingenvironments. Inabroadersense,
the curseof supportshift alsorefers to the situationwhen the state often appearingin testing environments
are extremely hard to arrive in the training environment3.
To rule out these pathological instances, we propose the vanishing minimal value assumption (Assump-
tion4.1),positingthattheoptimalrobustvaluefunctionreacheszeroataspecificstate. Suchanassumption
naturally applies to the sparse rewardRL paradigmand offers a broader scope comparedto the “fail-state”
assumption utilized in prior studies on offline RMDP with function approximation(Panaganti et al., 2022).
For a comprehensivediscussiononthis comparison,pleaserefer to Remark4.4. Onthe theoreticalfront,we
establish that the vanishing minimal value assumption effectively mitigates the support shift issues between
trainingandthetestingenvironments(Proposition4.2),renderingrobustRLwithinteractivedatacollection
feasible for RMDPs equipped with TV robust sets.
Efficient algorithm with sharp sample complexity. Under the vanishing minimal value assumption,
we develop an algorithm named OPtimistic RObust Value Iteration for TV Robust Set (OPROVI-TV, Algo-
rithm 1), that is capable of finding an ε-optimal robust policy within a total number of
H2SA
min H,ρ 1 (1.1)
−
O { }· ε2 !
e
interactive samples (Theorem 4.6). Here S and A denote the number of states and actions, ρ represents the
radius ofthe TV robustset, andH is the horizonlengthof eachepisode. To the best ofour knowledge, this
is the first provably sample-efficient algorithm for robust RL with interactive data collection.
According to (1.1), the sample complexity of finding an ε-optimal robust policy decreases as the radius
ρ ofthe robustset increases. This coincides with the findings of Shi et al. (2023) who consider robustRL in
infinite-horizondiscountedRMDPswithTVrobustsetswithinthegenerativemodelsetup. Whentheradius
ρ=0,anRMDPreducestoastandardMDP,andthesamplecomplexity(1.1)recoverstheminimax-optimal
samplecomplexityforonlineRLinstandardMDPsuptologarithmfactors,i.e., (H3SA/ε2). Ontheother
O
side, when ρ 14, finding an ε-optimal robust policy turns out to require nearly (H) less samples than
→ O
finding the optimal policy in a standard MDP. e
In the end, we further extend our algorithm and theory to another type of RMDePs, -rectangular
S ×A
discountedRMDP equipped withrobustsetsconsistingoftransitionprobabilitieswithboundedratioto the
nominalkernel(SeeSection4.4). ThisclassofRMDPsnaturallydoesnotsufferfromthesupportshiftissue.
Itisequivalenttothe -rectangularRMDPwithTVrobustsetandvanishingminimalvalueassumption
S×A
in an appropriatesense due to Proposition4.2. Consequently, by a clever usage of Algorithm 1, we can also
solve robust RL for this new model sample-efficiently, as is shown in Corollary4.8. Such a resultechoes our
intuition on the curse of support shift.
3WeremarkthatanexistingworkofDongetal.(2022)alsostudiestheproblemofrobustRLwithinteractivedatacollection.
Theyconsider -rectangularRMDPswithaTVrobustset,assumingthatthesupportofthetrainingenvironmenttransition
is the full stateS× spA ace. They claim the existence of an algorithm that enjoys a e (√K)-online regret. We point out that their
O
proofexhibitsanessentialflaw(misuseofLemma12therein)andthereforetheregrettheyclaimisinvalid.
4Wedonotsignifythesituationwhenρ=1sinceinthatcasetheTVrobustsetcontainsallpossibletransitionprobabilities,
makingtheproblemstatisticallytrivial. Inthatcase,nosampleisneeded.
4Sample complexity
Model Assump. Algorithm Data oracle
ρ [0,1)
∈
RPVL (Xu et al., 2023) generative model
H5SA
O ε2
DRVI (Shi et al., 2023) generative model
min {(cid:16)Hγ,ρ−1 }(cid:17)H γ2SA
O e ε2
general case lower bound (Shi et al., 2023) generative model Ω(cid:16)min {Hγ,ρ−1 }H γ2SA(cid:17)
e ε2
P2MPO (Blanchet et al., 2023) offline dataset (cid:16)
O
Cr⋆ obH ε24S2A (cid:17)
(cid:16) (cid:17)
lower bound (this work) interactive data collection intractable
e
“fail-state”
RFQI (Panaganti et al., 2022) offline dataset
CfullH γ4SA
assumption O ρ2ε2
vanishing (cid:16) (cid:17)
minimal value OPROVI-TV(this work) interactive data collection
O
e min {H,ρ ε− 21 }H2SA
(Assumption 4.1)
(cid:16) (cid:17)
e
Table1: Comparisonbetweenthesamplecomplexityof OPROVI-TVandpriorresultsonrobustRLforRMDP
with -rectangular TV robust sets under various settings (generative model/offline dataset/interactive
S ×A
data collection). For a fair comparison, the sample complexity of the generative model setup is presented
as NSA, where N denotes the number of samples queried for each state-action-step tuple. Meanwhile, we
note that the work of Panagantiet al. (2022) and Shi et al. (2023) considers infinite horizon γ-discounted
RMDPs, where we denote H := (1 γ) 1 as the effective horizon length for ease of comparison. In the
γ −
−
offline setting, ⋆ and represent the robust partial coverage coefficient and full coverage coefficient,
Crob Cfull
respectively. In the generalcase,our lowerboundreadsintractable,meaning that there existhardinstances
where it is impossible to learn the nearly optimal robust policy via a finite number of interactive samples.
Also,thesamplecomplexityofouralgorithmwithinteractivedatacollectionmatchesthesamplecomplexity
andthe lowerbound forthe generativemodelcase for infinite horizondiscountedRMDPs (Shi et al., 2023).
We remark that the worksof Panagantiand Kalathil (2022) and Blanchet et al. (2023) are in the paradigm
of function approximation,and here we reduce their general sample complexity result to the tabular setup.
1.2 Related Works
RobustreinforcementlearninginrobustMarkov decisionprocesses. RobustRLisusuallyframed
asarobustMarkovdecisionprocess(RMDP)(Iyengar,2005; El Ghaoui and Nilim,2005; Wiesemann et al.,
2013). There is a long line of work dedicated to the problem of how to solve for the optimal robust
policy of a given RMDP, i.e., planning (Iyengar, 2005; El Ghaoui and Nilim, 2005; Xu and Mannor, 2010;
Wang and Zou,2022;Wang et al.,2022;Kuang et al.,2022;Wang et al.,2023a;Yu et al.,2023;Zhou et al.,
2023;Li and Lan,2023;Wang et al.,2023c;Ding et al.,2024). Recently,thecommunityhasalsowitnesseda
growingbodyofworkonsample-efficientrobustRLinRMDPswithdifferentdatacollectionoracles,includ-
ingthegenerativemodelsetup(Yang et al.,2022;Panagantiand Kalathil,2022;Si et al.,2023;Wang et al.,
2023b; Yang et al., 2023b; Xu et al., 2023; Clavier et al., 2023; Wang et al., 2023d; Shi et al., 2023), offline
setting (Zhou et al.,2021b; Panagantiet al., 2022;Shi and Chi, 2022;Ma et al., 2022; Blanchet et al., 2023;
Liu and Xu,2024b;Wang et al.,2024),andinteractivedatacollectionsetting(Badrinath and Kalathil,2021;
Wang and Zou, 2021; Liu and Xu, 2024a).
Ourworkfallsintotheparadigmofsample-efficientrobustRLwithinteractivedatacollection. Wang and Zou
(2021) andBadrinath and Kalathil(2021)propose efficientonline learningalgorithmsto obtainthe optimal
robust policy of an infinite horizon RMDP, but none of them handle the challenge of exploration in online
RLbyassumingtheaccesstoexplorative policies. Thisassumptionenablesthelearnertocollecthigh-quality
data essential for effective learning and decision-making. In contrast, our work focuses on developing effi-
cient algorithms for the fully online setting, where there is no predefined exploration policy to use. Under
this more challenging setting, we address the exploration challenge through algorithmic design rather than
relying on assumed access to explorative policies.
Duringthepreparationofthiswork,weareawareofseveralconcurrentandindependentworks(Liu and Xu,
2024a,b; Wang et al., 2024), which study a different type of RMDPs known as d-rectangular linear MDPs
5(Ma et al., 2022; Blanchet et al., 2023). In particular, Liu and Xu (2024b) and Wang et al. (2024) consider
the offline setting, while Liu and Xu (2024a) investigate robust RL through interactive data collection (off-
dynamics learning), thus bearing closer relevance to our work. More specifically, under the existence of a
“fail-state”,the algorithmin Liu and Xu (2024a) canlearnanε-optimalrobustpolicy with provablesample
efficiency. In contrast, our work first explicitly uncovers the fundamental hardness of doing robust RL in
RMDPswithaTVdistancebasedrobustsetandwithoutadditionalassumptions. Toovercometheinherent
difficulty,weadoptavanishingminimalvalueassumptionthatstrictlygeneralizesthe“fail-state”assumption
used in Liu and Xu (2024a). Moreover,our focus is ontabular -rectangularRMDPs, with customized
S×A
algorithmic design and theoretical analysis which allow us to obtain a sharp sample complexity bound.
Finally,inTable1,wecomparethe samplecomplexityofouralgorithmwithpriorworkonrobustRLfor
RMDPs with -rectangular TV robust sets under various settings (generative model/offline dataset).
S×A
Sample-efficient online non-robust reinforcement learning. Our work is also closely related to on-
line non-robust RL, which is often formulated as a Markov decision process (MDP) with online data col-
lection. For non-robust online RL, the key challenge is the exploration-exploitation tradeoff. There has
been a long line of work (Azar et al., 2017; Dann et al., 2017; Jin et al., 2018; Zanette and Brunskill, 2019;
Zhang et al., 2020, 2021; M´enard et al., 2021; Wu et al., 2022; Li et al., 2023; Zhang et al., 2023) address-
ing this challenge in the context of tabular MDPs, where the state space and action space are finite and
also relatively small. In particular, many algorithms (e.g., UCBVI in Azar et al. (2017)) have been proven
capable of finding an ε-optimal policy within (H3SA/ε2) sample complexity. Notably, a standard MDP
O
corresponds to an RMDP with a TV robust set and ρ = 0, suggesting that OPROVI-TV can naturally
achieve nearly minimax-optimality for non-robuestRL. Moving beyond the tabular setups, recentworks also
investigate online non-robust RL with linear function approximation (Jin et al., 2020; Ayoub et al., 2020;
Zhou et al., 2021a; Zhong and Zhang, 2023; Huang et al., 2023b; He et al., 2023; Agarwal et al., 2023) and
even general function approximations (Jiang et al., 2017; Sun et al., 2019; Du et al., 2021; Jin et al., 2021;
Foster et al., 2021; Liu et al., 2022; Zhong et al., 2022; Liu et al., 2023; Huang et al., 2023a; Xu and Zeevi,
2023; Agarwalet al., 2023).
Corruption robustreinforcementlearning. Generallyspeaking,ourresearchisalsorelatedtoanother
formofrobustRL, namely corruptionrobustRL (Lykouris et al., 2021; Wei et al., 2022; Zhang et al., 2022;
Ye et al., 2023a,b; Yang et al., 2023a; Ye et al., 2024). This branch of researches on robust RL addresses
scenarios where training data is corrupted, presenting a distinct challenge from distributionally robust RL.
The latter concerns testing time robustness, where the agent is evaluated in a perturbed environment after
beingtrainedonnominaldata. ThesetwoformsofrobustRL,whilesharingtheoverarchinggoaltoenhance
agentresilience,operatewithindifferentcontextsandconfrontdistinctchallenges. Thus,adirectcomparison
between these two types of robust RL is difficult because each addresses unique aspects of resilience.
1.3 Notations
For anypositive integer H N , we denote 1,2,...,H by [H]. Given a set , we denote ∆( ) asthe set
+
∈ { } X X
of probability distributions over . For any distribution p ∆( ), we define the shorthandfor expectation
X ∈ X
and variance as
E [f]:=E [f(X)], V [f]=E [f2] (E [f])2.
p() X p() p() p() p()
· ∼ · · · − ·
For any set ∆( ), we define the robust expectation operator as
Q⊆ X
E [f]:= inf E [f(X)].
X p()
Q p() ∼ ·
· ∈Q
Foranyx,a R,wedenote(x) =max x,0 andx a=max x,a . Weuse ()tohideabsoluteconstant
+
∈ { } ∨ { } O ·
factors and use to further hide logarithmic factors.
O
e
62 Preliminaries
2.1 Robust Markov Decision Processes
We first introduce our underlying model for doing robust RL, the episodic robust Markov decision process
(RMDP),denotedbyatuple( , ,H,P⋆,R,Φ). Heretheset isthestatespaceandtheset istheaction
S A S A
space,bothwithfinitecardinality. TheintegerH isthelengthofeachepisode. ThesetP⋆ = P⋆ H isthe
{ h}h=1
collection of nominal transition kernels where P⋆ : ∆( ). The set R = R H is the collection
h S ×A 7→ S { h }h=1
of reward functions where R : [0,1]. For simplicity, we denote = P( , ): ∆( ) as
h
S×A7→ P { ·|· · S×A7→ S }
the space of all possible transition kernels, and we denote S = and A= .
|S| |A|
MostimportantlyanddifferentfromstandardMDPs,theRMDPisequippedwithamappingΦ: 2
P
P 7→
that characterizes the robust set of any transition kernel in . Formally, for any transition kernel P ,
P ∈ P
we call Φ(P) the robust set of P. One could interpret the nominal transition kernel P⋆ as the transition of
h
the training environment, while Φ(P⋆) contains all possible transitions of the testing environments.
h
GivenanRMDP ( , ,H,P⋆,R,Φ),we considerusing aMarkovianpolicy to makedecisions. AMarko-
S A
viandecisionpolicy (orsimply, policy)is definedasπ = π H withπ : ∆( ) for eachsteph [H].
{ h }h=1 h S 7→ A ∈
To measure the performance of a policy π in the RMDP, we introduce its robust value function, defined as
H
V hπ ,P⋆,Φ(s):= Pe h∈Φ(Pi h⋆n )f ,1 ≤h ≤HE {Pe h}H h=1, {πh}H h=1" Xi=hR i(s i,a i) (cid:12) (cid:12)s h =s #, ∀s ∈S,
(cid:12)
H (cid:12)
Qπ h,P⋆,Φ(s,a):= Pe h∈Φ(Pi h⋆n )f ,1 ≤h ≤HE {Pe h}H h=1, {πh}H h=1" Xi=hR i(s i,a i)(cid:12) (cid:12) (cid:12)s h =s,a h =a #, ∀(s,a) ∈S×A.
(cid:12)
Here the expectation is taken w.r.t. the state-action trajectories in(cid:12)duced by policy π under the transition
(cid:12)
P. One can also extend the definition of the robust value functions in terms of any collection of transition
kernel P = P H as Vπ and Qπ , which we usually use in the sequel.
{ h }h=1 ⊂P h,P,Φ h,P,Φ
e Amongallthepolicies,wedefinetheoptimalrobustpolicyπ⋆ asthepolicythatcanmaximizetherobust
value function at the initial time step h=1, i.e.,
π⋆ = argmax Vπ (s ), s . (2.1)
1,P⋆,Φ 1
∀
1
∈S
π= {πh}H
h=1
Inotherwords,the optimalrobustpolicy π⋆ maximizes the worstcaseexpectedtotalrewardsinallpossible
testing environments. For simplicity and without loss of generality, we assume in the sequel that the initial
state s is fixed. Our results could be directly generalized to s p () ∆( ). Similarly, we can also
1 1 0
∈S ∼ · ∈ S
define the optimal robustpolicy associatedwith a givenstochastic process defined throughany collectionof
transitionkernelsP = P H inthesamewayas(2.1). Wedenotethe optimalrobustvaluefunctions
{ h }h=1 ⊂P
associated with P as V⋆ and Q⋆ respectively.
h,P,Φ h,P,Φ
-rectangularity and robust Bellman equations. WeconsiderrobustsetsΦthathavethe -
S×A S×A
rectangularstructure(Iyengar,2005). whichrequiresthattherobustsetisdecoupledandindependentacross
different (s,a)-pairs. This kind of structure results in a dynamic programming representation of the robust
valuefunctions(efficientplanning),andisthuscommonlyadoptedintheliteratureofdistributionallyrobust
RL. More specifically, we assume the following.
Assumption 2.1 ( -rectangularity). We assume that the mapping Φ satisfies for any transition kernel
S×A
P , the robust set Φ(P) is in the form of
∈P
Φ(P)= (s,a;P), where (s,a;P) ∆( ).
P P ⊆ S
(s,aO)
∈S×A
Underthe -rectangularity(Assumption2.1),wehavetheso-calledrobustBellmanequation(Iyengar,
S×A
2005; Blanchet et al., 2023) which gives a dynamic programming representation of robust value functions.
Proposition 2.2 (RobustBellmanequation). Under Assumption 2.1, for any transition P = P H
{ h }h=1 ⊆P
and any policy π = π H with π : ∆( ), it holds that for any (s,a,h) [H],
{ h }h=1 h S 7→ A ∈S×A×
Vπ (s)=E Qπ (s, ) , Qπ (s,a)=R (s,a)+E Vπ .
h,P,Φ πh( ·|s) h,P,Φ
·
h,P,Φ h P(s,a;Ph) h+1,P,Φ
(cid:2) (cid:3) (cid:2) (cid:3)
7Forthe robustvaluefunctions ofthe optimalrobustpolicy,wealsohavethe followingdynamicprogram-
ming solution which plays a key role in our algorithm design and theoretical analysis.
Proposition 2.3 (Robust Bellman optimal equation). Under Assumption 2.1, for any P = P H ,
{ h }h=1 ⊆P
the robust value functions of any optimal robust policy of P satisfies that, for any (s,a,h) [H],
∈S×A×
V h⋆ ,P,Φ(s)=m aaxQ⋆ h,P,Φ(s,a), Q⋆ h,P,Φ(s,a)=R h(s,a)+E P(s,a;Ph) V h⋆ +1,P,Φ .
∈A
(cid:2) (cid:3)
By taking π⋆( s)=argmax Q⋆ (s,a), then π⋆ = π⋆ H is an optimal robust policy under P.
h ·| a ∈A h,P,Φ { h}h=1
Weremarkthattheoriginalversionofthe robustBellmanequation(Iyengar, 2005)isforinfinite horizon
RMDPs anda customizedproofofrobustBellmanequationfor finite horizonRMDPs (Proposition2.2)can
be foundinAppendix A.1ofBlanchet et al.(2023). TherobustBellmanoptimalequation(Proposition2.3)
is then a corollary or can be directly proved in a similar manner.
Total-variation distance robust set. InAssumption2.1, therobustset (s,a;P)isoftenmodeledasa
P
“distribution ball” centered at P( s,a). In this paper, we mainly consider this type of robust sets specified
·|
by a total-variation distance ball. We put it in the following definition.
Definition 2.4 (Total-variationdistance robust set). Total-variation distance (TV) robust set is defined as
(s,a;P):= P() ∆( ):D P() P( s,a) ρ ,
ρ TV
P · ∈ S · ·| ≤
n (cid:0) (cid:13) (cid:1) o
for some ρ [0,1), where D ( ) denotees the total variationedis(cid:13)tance defined as
TV
∈ ·k·
1
D p() q() := p(s) q(s) , p(),q() ∆( ). (2.2)
TV
· k · 2 − ∀ · · ∈ S
s
(cid:0) (cid:1) X∈S(cid:12) (cid:12)
(cid:12) (cid:12)
The TV robust set has recently been extensively studied by Yang et al. (2022); Panagantiand Kalathil
(2022); Panagantiet al. (2022); Xu et al. (2023); Blanchet et al. (2023); Shi et al. (2023), whichallfocus on
robust RL with a generative model or with a pre-collected offline dataset. Our work follows this RMDP
setup and studies robust RL via interactive data collection (see Section 2.2).
Moreimportantly,weemphasizethatby(2.2)inDefinition2.4,wedonot definetheTVdistancethrough
thenotionoff-divergencewhichrequiresthatthedistributionpisabsolutecontinuousw.r.t. q,asisgenerally
adoptedbythe abovepreviousworksonRMDP withTVrobustsets. Accordingto(2.2),weallow p to have
a different support than q. That is, there might exist an s such that p(s)>0 and q(s)=0. Given that,
∈S
the TV robust set in Definition 2.4 could contain transition probabilities that have different supports than
the nominal transition probability P⋆( s,a).
·|
Anessentialpropertyofthe TVrobustsetis thatthe robustexpectationinvolvedinthe robustBellman
equations (Propositions 2.2 and 2.3) has a duality representation that only uses the expectation under the
nominal transition kernel. Previous works, e.g., Yang et al. (2022), have proved such a result when the TV
distance is defined through f-divergence. Here we extend such a result to the TV distance defined directly
though (2.2) that allows a difference support between p and q.
Proposition2.5(Strongdualityrepresentation). UnderDefinition 2.4, thefollowing duality representation
for the robust expectation holds, for any V : [0,H] and P : ∆( ),
h
S 7→ S×A7→ S
ρ
E V = sup E (η f) η minV(s) +η . (2.3)
Pρ(s,a;Ph)
(cid:2) (cid:3)
η ∈[0,H](− Ph( ·|s,a)
(cid:2)
− + (cid:3)− 2 · (cid:18) − s ∈S (cid:19)+ )
Proof of Proposition 2.5. Please refer to Appendix A.1 for a detailed proof of Proposition 2.5.
Remark 2.6. Despite all previous works on RMDPs with TV robust sets relying on the definition of TV
distance D (p() q()) with absolute continuity of p with respect to q to obtain the strong duality represen-
TV
· k ·
tation in the form of (2.3), their results can be directly extended to TV distance that allows for different
support between p and q thanks to Proposition 2.5.
8Finally, another useful property of the robust value functions of an RMDP with TV robust sets is a fine
characterization of the gap between the maximum and the minimum of the robust value function, which is
first identified and utilized by Shi et al. (2023) for an infinite horizon RMDP with TV robust sets. In this
work, we prove and use a similar result for the finite horizon case, concluded in the following proposition.
Proposition 2.7 (Gap between maximum and minimum). Under Assumption 2.1 with the robust set spec-
ified by Definition 2.4, the robust value functions satisfies that
max Qπ (s,a) min Qπ (s,a) min H,ρ 1 ,
(s,a)
h,P,Φ
−(s,a)
h,P,Φ
≤
−
∈S×A ∈S×A
maxVπ (s) minVπ (s) min(cid:8) H,ρ 1(cid:9) ,
s
h,P,Φ
− s
h,P,Φ
≤
−
∈S ∈S
(cid:8) (cid:9)
for any transition P = P H , any policy π, and any step h [H].
{ h }h=1 ⊂P ∈
Proof of Proposition 2.7. Please refer to Appendix A.2 for a detailed proof of Proposition 2.7.
WenotethatintheproofofProposition2.7,weactuallyshowatighterformofboundofthegapbetween
the maximum and minimum as
1
1 (1 ρ)H .
ρ · − −
(cid:16) (cid:17)
But in the sequel, we mainly use the form of min H,ρ 1 for its brevity and the fact of (1 (1 ρ)H)/ρ=
−
{ } − −
Θ(min H,ρ 1 ) in the sense that
−
{ }
c min H,ρ 1 (1 (1 ρ)H)/ρ min H,ρ 1
− −
· ≤ − − ≤
for any H H N and ρ [0,1(cid:8)] with s(cid:9)ome absolute constant c>0(cid:8)that is(cid:9)independent of (H,ρ).
0 +
≥ ∈ ∈
IncontrastwithacrudeboundofH,suchafineupperbounddecreaseswhenρislarge,whichisessential
to understanding the statistical limits of doing robust RL in RMDPs with TV robust sets.
2.2 Robust RL with Interactive Data Collection
In this paper, we study how to learn the optimal robust policy π⋆ in (2.1) from interactive data collection.
Specifically, the learner is required to interact with only the training environment, i.e., P⋆, for some K N
∈
episodes. In each episode k [K], the learner adopts a policy πk to interact with the training environment
∈
P⋆ andtocollectdata. Whenthek-thepisodeends,thelearnerupdatesitspolicytoπk+1 basedonhistorical
dataandproceedsto the subsequentk+1-thepisode. Thelearningprocessends afteratotalofK episodes.
Sample complexity. We use the notionofsample complexity asthe keyevaluationmetric. Foranygiven
algorithmandpredeterminedaccuracylevelε>0,thesamplecomplexityistheminimumnumberofepisodes
K required for the algorithm to output an ε-optimal robust policy π which satisfies
V⋆ (s ) Vπb (s ) ε.
1,P⋆,Φ 1 − 1,P⋆,Φ 1 ≤ b
ThegoalistodesignalgorithmswhosesamplecomplexityhassmallorevenoptimaldependenceonS,A,H,ρ,
and 1/ε. Such a metric is connected with the sample complexity used in robust RL with generative models
andofflinesettings(seerelated worksforthereferences),whereinthesamplecomplexitymeanstheminimum
number ofgenerativesamples orpre-collectedoffline data requiredto achieveε-optimality. Incontrast,here
thesamplecomplexityismeasuringtheleastnumberofinteractionswiththetrainingenvironmentneededto
learnπ⋆, where no generativeor offline sample is available. Such a learningprotocolcasts unique challenges
on the algorithmic design and theoretical analysis to get the optimal sample complexity.
Online regret. Another evaluationmetric that is relatedto the minimization ofsample complexity is the
online regret. ForonlineRLinstandardnon-robustMDPs,thenotionofregretreferstothecumulativegaps
between the non-robust optimal value functions and the non-robust value functions of the policies executed
during eachepisode (Auer et al., 2008). Herefor robustRL in RMDPs, wesimilarly define the regretas the
9cumulative difference between the optimal robustpolicy π⋆ andthe executed policies πk K , but in terms
{ }k=1
of their robust value functions Vπ . Its formal definition is given as follows:
1,P⋆,Φ
K
RegretΦ(K):= V 1⋆ ,P⋆,Φ(s 1) −V 1π ,Pk ⋆,Φ(s 1). (2.4)
k=1
X
Thegoalistodesignalgorithmsthatcanachieveasublinear-in-KregretwithsmalldependenceonS,A,H,ρ.
Intuitively,asublinear-regretalgorithmwouldapproximatelylearntheoptimalrobustpolicyπ⋆ purelyfrom
interactingwith the trainingenvironmentP⋆. It turns outthatanysublinear-regretalgorithmcanbe easily
convertedtoapolynomial-samplecomplexityalgorithmbyapplyingthestandardonline-to-batchconversion
(Jin et al., 2018), which we show in detail in our theoretical analysis part.
3 A Hardness Result: The Curse of Support Shift
Unfortunately,weshowinthissectionthatingeneralsuchaproblemofrobustRLwithonlinedatacollection
isimpossible –thereexistsasimpleclassoftwoRMDPssuchthatanΩ(K)-onlineregretlowerboundexists.
However,previousworksonrobustRLwithagenerativemodelorofflinedatawithgoodcoveragedoprovide
sample-efficientwaystofindtheoptimalrobustpolicyforthisclassofRMDPs. Thisisaseparationbetween
robust RL with interactive data collection and generative model/offline data.
Wefirstexplicitly presentthe hardexample,whichis atwo-state,two-actionRMDPwithtotal-variation
distance robust set. Please see also Figure 1 for an illustration of the example.
Example 3.1 (Hard example of robust RL with interactive data collection). Consider two RMDPs
0
M
and which only differ in their nominal transition kernels. The state space is = s ,s , and the
1 good bad
M S { }
action space is = 0,1 . The horizon length H =3. The reward function R always is 1 at the good state
A { }
s and is 0 at the bad state s , i.e.,
good bad
1, s=s
good
R (s,a)= , (a,h) [H].
h
(0, s=s
bad
∀ ∈A×
For the good state s , the next state is always s . For the bad state s , there is a chance to get to
good good bad
the good state s , with the transition probability depending on the action it takes. Formally,
good
P h⋆, Mθ(s
good
|s good,a)=1, ∀(a,h) ∈A×{1,2 }, ∀θ ∈{0,1 },
p, a=θ
P 2⋆, Mθ(s
good
|s bad,a)=
(q, a=1 θ
, ∀θ ∈{0,1 },
−
where p,q are two constants satisfying 0<q <p<1. Intuitively, when at the bad state, the optimal action
would result in a higher transition probability p to the good state than the transition probability q induced by
the other action. Finally, we consider the robust set being specified by a total-variation distance ball centered
at the nominal transition kernel, that is, for any P,
Φ(P)= (s,a;P), where (s,a;P)= P() ∆( ):D P() P( s,a) ρ , (3.1)
ρ ρ TV
P P · ∈ S · ·| ≤
(s,aO) ∈S×A n (cid:0) (cid:13) (cid:1) o
e e (cid:13)
where ρ [0,q] is the parameter characterizing the size of the robust set. We set s =s .
1 good
∈
Forthis classofRMDPs,wehavethe followinghardnessresultfordoingrobustRLwithinteractivedata
collection, an Ω(ρ K)-online regret lower bound.
·
Theorem3.2(Hardnessresult(basedonExample3.1)). ThereexiststwoRMDPs , ,thefollowing
0 1
{M M }
regret lower bound holds,
inf sup E RegretMΦθ, ALG(K) Ω ρ HK ,
≥ ·
ALGθ ∈{0,1
} h i (cid:0) (cid:1)
where RegretMΦθ, ALG(K) refers to the online regret of algorithm for RMDP θ.
ALG M
10s s s
good good good
R 1 =1 R 2 =1 R 3 =1
s s
bad bad
R 2 =0 R 3 =0
Figure 1: Illustration of the hard example in Example 3.1. The solid lines represent possible transitions of
thenominaltransitionkernel. Thedashedlinesrepresentthetransitionsinducedbytheworstcasetransition
kernelinthe robustset. The redsolidline representsthe transitionwherethe two RMDP instancesdiffer in
that different actions lead to higher transition probability from s to s . We notice that when starting
bad good
froms =s ,thenominaltransitionkernelkeepstheagentats andnoinformationats isrevealed.
1 good good bad
Proof of Theorem 3.2. We intuitively explainwhyrobustRL withinteractivedata collectionmayfailin the
Example 3.1 in this section. We refer the readers to a rigorous proof of Theorem 3.2 in Appendix B.1
ThereasonwhyanyalgorithmfailsforthisclassofRMDPsisthesupportshift oftheworst-casetransition
kernel. In robust RL, the performance of a policy π is evaluated via the robust expected total rewards, or
equivalently, the expected return under the most adversarial transition kernel P ,π. In this example, as we
†
explicitlyshowintheproof,wheninthegoodstates ,theworst-casetransitionkernelP ,π wouldtransit
good †
the state to s with a constant probability ρ. But the state s is out of the scope of the data collection
bad bad
processbecause startingfroms =s the nominaltransitionkernelalwaystransitsthe stateto s . As
1 good good
aresult, the performanceofthe learnedpolicyatthe badstate s is notguaranteed,andinevitably incurs
bad
an Ω(ρ K)-lower bound of regret, a hardness result. Furthermore, by strategically constructing RMDPs
·
with the horizon 3H based on Example 3.1, we can derive a lower bound of Ω(ρ HK)
·
Incontrast,doingrobustRL with agenerativemodeloranoffline datasetwith goodcoverageproperties
does not face such difficulty. It turns out that any RMDP with -rectangular total-variation robust
S ×A
set (including Example 3.1) can be solved in a sample-efficient manner therein, see Yang et al. (2022);
Panagantiand Kalathil (2022); Panagantiet al. (2022); Xu et al. (2023); Blanchet et al. (2023); Shi et al.
(2023) and Remark 2.6. The intuitive reason is that, for the generative model setting, the learner can
directly query any state-actionpair to estimate the nominal transition kernel P⋆, and thus no support shift
problem happens. The same reason holds for the offline setup with a good-coveragedataset.
There is a broaderunderstanding of the curse of support shift that hinders the tractability of robust RL
via interactive data collection. The concept of support shift can be comprehended within a broadercontext
beyondthedisjointnessofcertainpartsofthesupportsetsofthetrainingandtestingenvironments. Instead,
ensuring a “high probability of disjointness” is enough to maintain the integrity of the hardness result. For
instance, we can modify the state s in Example 3.1 so that it is no longer an absorbing state. Rather,
good
s could transit to s with a small probability, such as 2 H. This modification expands the support
good bad −
of the training environment to encompass the entire state space. Nevertheless, acquiring information about
s necessitates exponential samples, thereby preserving the hardness result.
bad
Inthe next section ofthis paper, we aim to figure out that for specific types of RMDPs, e.g., the RMDP
withtotal-variationrobustsetasinExample3.1,underwhatkindofstructuralassumptionscanweperform
sample-efficient robust RL with interactive data collection.
4 A Solvable Case, Efficient Algorithm, and Sharp Analysis
Motivated by the hard instance (Example 3.1) in the previous section, in this section, we consider a special
subclassofRMDP with -rectangulartotalvariationrobustsetthatweshowallowsforsample-efficient
S×A
11robustRLthroughinteractivedatacollection. InSection4.1,weintroducetheassumptionweimposeonthe
RMDPweconsider. WeproposeouralgorithmdesigninSection4.2,withtheoreticalanalysisinSection4.3.
Throughout this section, our choice of the mapping Φ is always given by (3.1).
4.1 Vanishing Minimal Value: Eliminating Support Shift
To overcome the difficulty of support shift identified in Section 3, we make the following vanishing minimal
value assumption on the underlying RMDP.
Assumption 4.1 (Vanishing minimal value). We assume that the underlying RMDP satisfies that
minV⋆ (s)=0.
1,P⋆,Φ
s
∈S
Also, without loss of generality, we assume that the initial state s / argmin V⋆ (s).
1 ∈ s ∈S 1,P⋆,Φ
Assumption 4.1 imposes that the minimal robust expected total rewardsoverall possible initial states is
0. Assumingthattheinitialstates / argmin V⋆ (s)avoidsmakingtheproblemtrivial. Acloselook
at Assumption 4.1 actually gives
th1 a∈
t the
mins im∈Sal1 r, oP b⋆ u,Φ
st value function of any policy π at any step is zero,
that is, min Vπ (s) = 0 for any policy π and any step h [H]. With this observation, the following
s
∈S
h,P⋆,Φ
∈
proposition explains why such an assumption can help to overcome the difficulty.
Proposition 4.2 (Equivalentexpressionof TV robustset with vanishing minimal value). For any function
V : [0,H] with min V(s)=0, we have that
s
S 7→ ∈S
ρ
E
Pρ(s,a;P
h⋆)[V]=ρ
′
·E
Bρ′(s,a;P
h⋆)[V], with ρ
′
=1
− 2
>0,
where the total-variation robust set (s,a;P⋆) is defined in (3.1) and the set (s,a;P⋆) is defined as5
Pρ h Bρ′ h
P(s) 1
Bρ′(s,a;P h⋆)= (P( ·) ∈∆( S): ss ′u ∈p
S
P h⋆( es
′|s′
,a) ≤ ρ
′).
e
Proof of Proposition 4.2. Please refer to Appendix A.3 for a detailed proof of Proposition 4.2.
As Proposition4.2indicates, under Assumption4.1, the robustBellmanequations(Propositions2.2and
2.3) at step h [H] is equivalent to taking an infimum over another robust set (s,a;P⋆) that shares the
∈ Bρ′ h
same support as the nominal transition kernel P⋆( s,a), discounted by a constant ρ < 1. Intuitively, this
′
·|
new robust set rules out the difficulty originatedin unseen states in training environments and the discount
factorρ hedgesthedifficultyfromprohibitivelysmallprobabilityofreachingcertainstatesthatmayappear
′
often in the testing environments. This renders robust RL with interactive data collection possible.
To understand this from another perspective, it could be shown that under the conclusions of Proposi-
tion 4.2, the robust value functions of any policy π is equivalent to the robust value functions of this policy
under a another discounted RMDP ( S, A,H,P⋆,R ′,Φ′) with R h′(s,a)=(ρ ′)h −1R h(s,a) and Φ′ given by
Φ(P)= (s,a;P). (4.1)
′ ρ′
B
(s,aO)
∈S×A
And therefore we are equivalently considering this new type of RMDPs. Please refer to Section 4.4 for more
discussions on the connections between the two types of RMDPs.
ExamplesofAssumption4.1. Inthesequel,weprovideaconcreteconditionthatmakesAssumption4.1
hold,whichimposesthatthestatespaceoftheRMDPhasa“closed”subsetof“fail-states”withzerorewards.
Condition 4.3 (Fail-states). There exists a subset of fail states such that
f
S ⊂S
R (s,a)=0, P⋆( s,a)=1, (s,a,h) [H].
h h Sf | ∀ ∈Sf ×A×
5Hereweimplicitlydefine 0 =0and a = foranya>0.
0 0 ∞
12This type of “fail-states”condition is first proposedby Panagantiet al. (2022) (with =1) to handle
f
|S |
thecomputationalissuesforrobustofflineRLunderfunctionapproximations(outofthescopeofourwork).
In contrast, here we make the vanishing minimal value assumption in order for tackling the support shift or
extrapolation issue for the interactivedata collection setup. The comparisonbetweenthe vanishing minimal
value assumption (Assumption 4.1) and the “fail-states” condition (Condition 4.3) is given below.
Remark 4.4 (ComparisonbetweenAssumption4.1andCondition4.3). Wefirstobserve that Condition 4.3
implies that min Vπ (s)=0 for any policy π and step h [H], therefore satisfying the minimal value
s
∈S
h,P⋆,Φ
∈
assumption (Assumption 4.1). Conversely, the vanishing minimal value assumption in Assumption 4.1 is
strictlymoregeneralthanthefail-state condition in Condition 4.3. Toillustrate, onecan consider an RMDP
characterized by the state space = s ,s , action space = a , time horizon H = 2, reward function
1 2 1
S { } A { }
R (s,a)=1 s=s , and transition probabilities defined as follows:
h 2
{ }
P⋆(s s ,a )=1 ρ, P⋆(s s ,a )=ρ, P⋆(s s ,a )=0, P⋆(s s ,a )=1,
1 1 | 1 1 − 1 2 | 1 1 1 1 | 2 1 1 2 | 2 1
whereρistheradiusoftherobustset. Itisevidentthatnofail-stateemergeswithinsuchanRMDPstructure.
However, this RMDP satisfies the vanishing minimal value assumption since V⋆ (s )=0.
1,P⋆,Φ 1
Remark 4.5 (Reductiontonon-robustMDPwithoutlossofgenerality). It is noteworthy that assuming the
vanishing minimal value (Assumption 4.1) or the presence of fail-states (Condition 4.3) in the non-robust
case (ρ = 0) is without loss of generality. This is achievable by expanding the prior state space of MDP
S
to include an additional state s , denoted as the fail-state. More importantly, this augmentation does not
f
alter the optimal value or theoptimal valuefunction of the original MDP. Consequently, it becomes sufficient
to seek the optimal policy within the augmented MDP, which satisfies the conditions of vanishing minimal
value (Assumption 4.1) or the existence of fail-states (Condition 4.3). This indicates that our algorithm and
theoretical analysis in the sequel can be directly reduced to non-robust MDPs without additional assumptions.
4.2 Algorithm Design: OPROVI-TV
In this section, we propose our algorithmthat solves robust RL with interactive data collection for RMDPs
with -rectangulartotal-variation(TV)robustsets(Assumption2.1andDefinition2.4)andsatisfyingthe
S×A
vanishing minimal value assumption (Assumption 4.1). Our algorithm, OPtimistic RObust Value Iteration
forTVRobustSet(OPROVI-TV,Algorithm1),canautomaticallybalanceexploitationandexplorationduring
the interactive data collecting process while managing the distributional robustness of the learned policy.
In each episode k, the algorithm operates in three stages: (i) training environment transition estimation
(Line 3 to 5); (ii) optimistic robustplanning basedonthe training environmenttransition estimator(Line 6
to 11); and finally (iii) executing the policy in the training environment and collecting data (Line 12 to 18).
In the following, we elaborate more on the first two parts of Algorithm 1.
4.2.1 Training Environment Transition Estimation
Atthebeginningofeachepisodek [K],wemaintainanestimateofthetransitionkernelP⋆ ofthetraining
∈
environmentbyusingthehistoricaldataD= {(sτ h,aτ h,sτ h+1) }k τ−=1 1, ,H
h=1
collectedfromtheinteractiionwiththe
training environment. Specifically, we simply adopt a vanilla empirical estimator, defined as
Nk(s,a,s)
Pk(s s,a)= h ′ , (s,a,h,s) [H],
h ′ | Nk(s,a) 1 ∀ ′ ∈S×A×S×
h ∨
where the count functionb s Nk(s,a,s) and Nk(s,a) are calculated on the current dataset D by
h ′ h
k 1
−
N hk(s,a,s′)= 1 (sτ h,aτ h,sτ h+1)=(s,a,s′) , N hk(s,a)= N hk(s,a,s′), (4.2)
τ=1 s′
X (cid:8) (cid:9) X∈S
for any (s,a,h,s) [H]. This just coincides with the transition estimator adopted by existing
′
∈S×A×S×
non-robust online RL algorithms (Auer et al., 2008; Azar et al., 2017; Zhang et al., 2021).
13Algorithm 1 OPtimistic RObust Value Iteration for TV Robust Set (OPROVI-TV)
1: Initialize: dataset D= .
∅
2: for episode k =1, ,K do
···
3: Training environment transition estimation:
4: Update the count functions N hk(s,a,s ′) and N hk(s,a) based on D according to (4.2).
5: Calculate the transition kernel estimator P hk as N hk(s,a,s ′)/(N hk(s,a) ∨1).
6: Optimistic robust planning:
7: Set Vk =Vk =0. b
H+1 H+1
8: for step h=H, ,1 do
9: Set Qk (, ) an· d·· Qk(, ) as (4.4) and (4.5), with the bonus function bonusk(, ) defined in (4.7).
h · · h · · h · ·
1 10 1:
:
enS det foπ rhk( ·|·)=argmax a
∈A
Qk h( ·,a), Vk h( ·)=E π hk( ·|·)[Qk h( ·, ·)], and Vk h( ·)=E π hk( ·|·)[Qk h( ·, ·)].
12: Execute the policy in training environment and collect data:
13: Receive the initial state sk .
1 ∈S
14: for step h=1, ,H do
···
15: Take action ak
h
∼π hk( ·|sk h), observe reward R h(sk h,ak h) and the next state sk h+1.
16: end for
17: Set D as D (sk,ak,sk ) H .
∪{ h h h+1 }h=1
18: end for
19: Output: Randomly (uniformly) return a policy from πk K .
{ }k=1
4.2.2 Optimistic Robust Planning
GivenPk thatestimatesthetrainingenvironment,weperformanoptimisticrobustplanningtoconstructthe
policy πk to execute. Basically,the optimistic robust planning follows the robust Bellman optimal equation
(Propobsition2.3) to approximate the optimal robust policy, but differs in that it maintains an upper bound
andalowerboundoftheoptimalrobustvaluefunctionandchoosesthepolicythatmaximizestheoptimistic
estimate to incentivize explorationduring data collection. Here the purpose of maintaining the lowerbound
estimate is to facilitate the construction of the variance-awareoptimistic bonus (see following), which helps
to sharpen our theoretical analysis.
Simplifying the robust expectation. Tobetterutilizethe vanishingminimalvaluecondition(Assump-
tion4.1),wetakeacloserlookintotherobustBellmanequation. Duetothestrongduality(Proposition2.5),
the robust expectation E [V] for any V [0,H] satisfying min V(s)=0 is equivalent to
Pρ(s,a;P)
∈
s
∈S
ρ
E V = sup E η V η minV(s) +η
Pρ(s,a;P) (cid:2) (cid:3) η ∈[0,H](cid:26)− P( ·|s,a) h(cid:0) − (cid:1)+ i− 2 · (cid:16) −s′ ∈S ′ (cid:17)+ (cid:27)
ρ
= sup E η V + 1 η . (4.3)
η
∈[0,H](cid:26)− P( ·|s,a)
h(cid:0)
− (cid:1)+
i (cid:16)
− 2 (cid:17)·
(cid:27)
Consequently, with a slight abuse of the notation, in the remaining of the paper, we re-define the operator
E [V]as the righthandside of(4.3). Due to Assumption4.1, the robustBellman(optimal) equation
Pρ(s,a;P)
(Proposition 2.2 and Proposition 2.3) still holds under this new definition.
Optimisticrobust planning. Withthisinmind,theoptimisticrobustplanninggoesasfollows. Starting
from Vk =Vk =0, we recursively define that
H+1 H+1
Qk h(s,a)=min R h(s,a)+E Pρ(s,a;Pb hk) Vk h+1 +bonusk h(s,a),min H,ρ −1 , ∀(s,a) ∈S×A, (4.4)
Qk h(s,a)=maxn R h(s,a)+E Pρ(s,a;Pb hk)h Vk h+1i −bonusk h(s,a),0 ,(cid:8) ∀(s,a(cid:9) )o ∈S×A, (4.5)
n h i o
where the robustexpectationE Pρ(s,a;Pb hk) followsthe definition in the righthand side of(4.3), and the bonus
functionbonusk(s,a) 0is definedlater. Here wetruncatethe optimistic estimateQk viathe upper bound
h ≥ h
14min H,ρ 1 of the true optimal robust value function Q⋆ . This truncation arises from the combined
{
−
}
h,P⋆,Φ
implication of Proposition 2.7 and the fact that min Q⋆ (s,a)=0 under Assumption 4.1.
(s,a) h,P⋆,Φ
∈S×A
As we establishin Lemma C.2, Qk andQk formupper andlowerbounds forQ⋆ andQπk under
h h h,P⋆,Φ h,P⋆,Φ
a proper choice of the bonus. After performing (4.4) and (4.5), we choose the data collection policy πk to
h
be the optimal policy with respect to the optimistic estimator Qk and define Vk and Vk accordingly by
h h h
πk( )=argmax Qk (,a), Vk (s)=E Qk (s, ) , Vk(s)=E Qk(s, ) . (4.6)
h ·|·
a
h · h π hk( ·|s) h · h π hk( ·|s) h ·
∈A h i h i
We remark that the purpose of maintaining the lower bound estimate (4.5) is to facilitate the construction
of the bonus and to help to sharpen our theoreticalanalysis. The constructionof the policy πk is still based
ontheoptimisticestimator,whichiswhywenameitoptimisticrobustplanning. Asindicatedbytheory,the
optimisticrobustplanningeffectivelyguidesthepolicytoexploreuncertaintyrobust valuefunctionestimates,
striking a balance between exploration and exploitation while managing distributional robustness.
Bonus function. In Algorithm 1, the bonus function bonusk(s,a) is a Bernstein-style bound defined as
h
bonusk h(s,a)=v uV Pb hk( ·|s,a) h(cid:16)NV kk h (s+ ,1 a+
)
V 1k h+1 (cid:17)/2 ic 1ι + 2E Pb hk( ·|s,a) hV Hk h+1−Vk h+1
i
+ Nkc (2 sH ,a2S )ι
1
+ √1 K(4.7)
u h ∨ h ∨
t
whereι=log(S3AH2K3/2/δ),c ,c >0areabsoluteconstants,andδ signifiesapre-selectedfailprobability.
1 2
Under(4.7),Qk andQk becomeupperandlowerboundsoftheoptimalrobustvaluefunctions(LemmaC.2).
h h
Moreimportantly,thebonus(4.7)iscarefullydesignedforrobustvaluefunctionssuchthatthesummationof
thisbonusterm(especiallytheleadingvariancetermin(4.7))overtimestepsiswellcontrolled,forwhichwe
also develop new analysis methods. This is critical for obtaining a sharp sample complexity of Algorithm 1.
4.3 Theoretical Guarantees
Thissectionestablishesthe onlineregretandthe samplecomplexityof OPROVI-TV(Algorithm1). Ourmain
result is the following theorem, upper bounding the online regret of Algorithm 1.
Theorem 4.6 (Onlineregretof OPROVI-TV). Given an RMDPwith -rectangular total-variation robust
S×A
set of radius ρ [0,1) (Assumption 2.1 and Definition 2.4) satisfying Assumptions 4.1, choosing the bonus
∈
function as (4.7) with sufficiently large c ,c >0, then with probability at least 1 δ, Algorithm 1 satisfies
1 2
−
RegretΦ(K)
≤O
min H,ρ −1 H2SAKι
′
,
(cid:18)q (cid:19)
(cid:8) (cid:9)
where ι =log2(SAHK/δ) and () hides absolute constants and lower order terms in K.
′
O ·
Proof of Theorem 4.6. See Appendix C for a detailed proof of Theorem 4.6.
Theorem 4.6 shows that Algorithm 1 enjoys a sublinear online regret of (√K), meaning that it is able
O
to approximatelyfind the optimalrobustpolicy throughinteractivedata collection. This is incontrastwith
the generalhardnessresultinSection3wheresample-efficientlearningisimpeossible inthe worstcase. Thus
we show the effectiveness of the minimal value assumption for robust RL with interactive data collection.
As a corollary, we have the following sample complexity bound for Algorithm 1.
Corollary 4.7(Samplecomplexityof OPROVI-TV). Underthesamesetupandconditions asinTheorem 4.6,
with probability at least 1 δ, Algorithm 1 can output an ε-optimal policy within
−
min H,ρ 1 H2SAι
− ′′
(4.8)
O (cid:8) ε2 (cid:9) !
episodes, where ι =log(SAH/δ) and () hides absolute constants.
′′
O ·
15Proof of Corollary 4.7. This follows from Theorem 4.6 and a standard online to batch conversion.
ThisfurthershowsthatAlgorithm1isabletofindε-optimalrobustpolicy withinpolynomialinteractive
samplesinH,S,A,andε 1. WenotethatastheradiusρoftheTVrobustsetincreases,thesampleneeded
−
to be ε-optimal decreases. When ρ tends to 1, the sample complexity reduces to nearly (H2SA/ǫ). Thus,
O
we observe that robust RL through interactive data collection for this RMDP example is statistically easier
when the radius ρ increases,which matches the conclusionin the generativemodel setupe(Yang et al., 2022;
Shi et al., 2023) as well as the offline learning setup (Panaganti et al., 2022).
Finally, we comparethe sample complexity (4.8) with priorarts onnon-robustonline RL androbustRL
with a generative model. On the one hand, (4.8) with ρ=0 equals to
H3SA
,
O ε2
(cid:18) (cid:19)
e
whichmatchesthe minimax samplecomplexitylowerboundforonlineRL innon-robustMDPs(Azar et al.,
2017). Thismeansthatouralgorithmdesigncannaturallyhandlenon-robustMDPsasaspecialcase(please
alsoseeRemark4.5forwhyonecanreduceAlgorithm1togeneralnon-robustMDPsunderAssumption4.1).
On the other hand, the previous work of Shi et al. (2023) for robust RL in infinite horizon RMDPs with a
TV robust set and a generative model showcases a minimax optimal sample complexity of
SA
,
O max 1 γ,ρ (1 γ)2ε2
(cid:18) { − } − (cid:19)
e
forρ [0,1),whereγ isthediscountfactoroftheinfinite horizonRMDP.Ifweidentify theeffectivehorizon
∈
H oftheinfinite horizonmodelas1/(1 γ),thenthesamplecomplexity(4.8)ofAlgorithm1matchestheir
γ
−
result. Meanwhile, we highlight that our algorithmdoes not rely ona generativemodel and operatespurely
through interactive data collection.
4.4 Extensions to Robust Set with Bounded Transition Probability Ratio
In this section, we show that our algorithm design (Algorithm 1) can also be applied to -rectangular
S ×A
discountedRMDPswithrobustsetsgivenby(4.1)(i.e.,boundedratiobetweentrainingandtestingtransition
probabilities). WeestablishthatourmaintheoreticalresultinSection4.3canimplyasublinearregretupper
boundforthismodel,whichmeansthatthistypeofRMDPscanalsobesolvedsample-efficientlybyaclever
usage of Algorithm 1. This coincides with our intuition on support shift in Section 4.1.
-rectangular discounted RMDPs with robust set (4.1). We first formally define the model we
S×A
consider. Wedefineafinite-horizondiscountedRMDPasafinite-horizonRMDP
γ
=( , ,H,P⋆,R γ,Φ′),
M S A
where the robust set Φ′ is given by (4.1), i.e.,
P(s) 1
Φ(P)= P() ∆( ): sup ′ := (s,a;P⋆). (4.9)
′
(s,aO)
∈S×A( · ∈ S s′
∈S
P h⋆( es ′|s,a) ≤ ρ ′)
(s,aO)
∈S×ABρ′
e
Thisrobustsetcontainstransitionprobabilitiesthatsharethesamesupportasthenominaltransitionkernel.
The rewardfunction R = γh 1 R H , where γ (0,1) is the discount factor and R [0,1]is the true
γ { − · h }h=1 ∈ h ∈
rewardatstep h. Thatis,the robustvalue function is nowthe worstcase expecteddiscountedtotalreward.
Algorithmand regretbound. NowwetheoreticallyshowthatwecanapplyAlgorithm1tosolverobust
RL in -rectangular discounted RMDPs with robust set (4.9) via interactive data collection.
S×A
As motivated by the discussions under Proposition 4.2, we define an auxiliary finite-horizon TV-RMDP
as =( , ,H,P⋆,R,Φ)whichinclude anadditional“fail-state”s . Morespecifically,the statespace
f
M M S A
= s . The transition kernel P⋆ is defined as, for any step h [H],
f
S S∪{ } ∈
f f e f e e
e P h⋆( ·|s,a)=P h⋆( ·|s,a),e ∀(s,a)
∈S×A
and P h⋆( ·|s f,a)=δ sf( ·), ∀a ∈A. (4.10)
e e
16The reward function R is defined as, for any step h [H],
∈
h 1
e γ −
R (s,a)= R (s,a), (s,a) and R (s ,a)=0, a .
h h h f
ρ · ∀ ∈S×A ∀ ∈A
(cid:18) ′(cid:19)
e e
We suppose that the discount factor γ ρ so that the reward function R [0,1]. The robust mapping Φ
′ h
≤ ∈
is defined as, for any P : ∆( ),
S×A7→ S
e e
Φ(P)= e Pe() ∆( ):De P() P( s,a) ρ := (s,a;P), ρ=2 2ρ.
TV ρ ′
· ∈ S · ·| ≤ P −
e e
(s,aO) ∈S×An (cid:0) (cid:13) (cid:1) o (s,aO) ∈S×A
e e e e (cid:13)e e e
Therefore, is anRMDP with -rectangularTV robustsetof radiusρ andsatisfying Assumption 4.1
M S×A
(because it satisfies the “fail-state” Condition 4.3). Furthermore, for any initial state s s = ,
1 f
∈ S \{ } S
the interactfionwith the transitionkernelP⋆ is equivalentto the interactionwiththe transitionkernel P⋆ of
the original RMDP , since by the definition (4.10), starting from any s=s the agent woeuld follow the
γ f
M 6
same dynamics as P⋆. What’s more, for aeny policy π : ∆( ) for , it naturally induces the unique
h
S 7→ A M
policy π : ∆( ) for the original RMDP .
,h γ
S S 7→ A M
Therefore,wecanrunAlgorithm1ontheauxiliaryeRMeDP ,startingffromtheinitialstates
1
s
f
,
M ∈S\{ }
which oenly needs the interactionwith P⋆. Suppose the output policy by the algorithm is πk K , then the
{ }k=1
following corollary shows the induced policy πk K for the ofriginal RMDP enjoys a sublineareregret.
{ S}k=1 Mγ
e
Corollary 4.8 (Online regret of Algorithm 1 for discounted RMDPs with robust sets (4.9)). Consider an
e
-rectangular γ-discounted RMDP with robust set (4.9) satisfying 0 γ ρ (1/2,1]. There exists an
′
S×A ≤ ≤ ∈
algorithm (specified by the above discussion) such that its online regret for this RMDP is bounded by
ALG
RegretAΦL′ G(K)
≤O
min H,(2 −2ρ ′) −1 H2SAKι
′
,
(cid:18)q (cid:19)
(cid:8) (cid:9)
where ι =log2(SAHK/δ) and () hides absolute constants and lower order terms in K.
′
O ·
Proof of Corollary 4.8. See Appendix D.1 for a detailed proof of Corollary 4.8.
Corollary 4.8 shows that besides -rectangular RMDPs with TV robust set and vanishing minimal
S×A
valueassumption,the -rectangulardiscountedRMDPwithrobustsetofboundedtransitionprobability
S×A
ratio(4.9)canalsobesolvedsample-efficientlybyrobustRLviainteractivedatacollection. Thisalsoechoes
our intuition on the support shift issue in Section 4.1. Furthermore, the regret decays as ρ decays in which
′
case the transition probability ratio bound becomes higher, i.e., the robust set becomes larger.
Remark 4.9. The upper bound in Corollary 4.8 does not depend on the discount factor γ since Algorithm 1
adopts a coarse bound of R 1. The upper boundcan be directly improved to be γ-dependent using a tighter
h
≤
truncation in step (4.4) of Algorithm 1.
e
5 Conclusions and Discussions
Inthis work,weshowthatintheabsenceofanystructuralassumptions,robustRLthroughinteractivedata
collectionnecessarily induces a linear regretlowerbound in the worstcase due to the curse of supportshift.
Meanwhile,underthe vanishingminimalvalueassumption,anassumptionthatisabletoeffectivelyruleout
the potentialsupportshift issuesfor RMDPs with a TV robustset, we proposea sample-efficientrobustRL
algorithm for those RMDPs. We discuss some potential extensions here and the associated challenges next.
Extension to function approximation setting. The vanishing minimal value assumption also suffices
for developing sample-efficient algorithms for -rectangular TV-robust-set RMDPs with linear or even
S ×A
generalfunctionapproximation(Blanchet et al.,2023). Nonetheless,achievingthenearlyoptimalrateunder
general function approximation remains elusive.
17Extension to other types of robust set. Beyondthe TV distance based robustset we consider, recent
literatureonrobustRLalsoinvestigateothertypesofφ-divergencebasedrobustsetincludingKLdivergence,
χ2 distance(Yang et al.,2022;Shi and Chi,2022;Blanchet et al.,2023;Xu et al.,2023;Shi et al.,2023). An
interestingdirectionoffutureworkistoinvestigateisitalsopossibleand,ifpossible,canwedesignprovably
sample-efficientrobustRL algorithmswithinteractivedatacollectionforRMDPs withthosetypesofrobust
sets. Notably, the KL divergence based robust set naturally does not suffer from the curse of support shifts
that gives rise to the hardness for the TV robust set case. However,we find that there are other difficulties
for robust RL in KL divergence based RMDPs through interactive data collection. Meanwhile, the optimal
sample complexity for robustRL in RMDPs with KL divergence robustset is still elusive evenin the offline
learningsetup (Shi and Chi, 2022). We leavethe investigationofRMDPs with KL divergencerobustset for
future work.
Acknowledgement
The authors would like to thank Pan Xu and Zhishuai Liu for their feedbacks on a draft of this work.
References
Agarwal, A., Jiang, N., Kakade, S. M. and Sun, W. (2019). Reinforcement learning: Theory and
algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep 10–4. 44
Agarwal,A.,Jin, Y.andZhang,T.(2023).Voql: Towardsoptimalregretinmodel-freerlwithnonlinear
function approximation. In The Thirty Sixth Annual Conference on Learning Theory. PMLR. 6
Auer, P., Jaksch, T. and Ortner, R. (2008). Near-optimal regret bounds for reinforcement learning.
Advances in neural information processing systems 21. 9, 13
Ayoub, A.,Jia, Z.,Szepesvari, C.,Wang, M.andYang, L.(2020).Model-basedreinforcementlearning
with value-targeted regression. In International Conference on Machine Learning. PMLR. 6
Azar, M. G., Osband, I. and Munos, R. (2017). Minimax regret bounds for reinforcement learning. In
International Conference on Machine Learning. PMLR. 6, 13, 16, 42
Badrinath, K. P. and Kalathil, D. (2021). Robust reinforcement learning using least squares policy
iterationwithprovableperformanceguarantees.InInternationalConferenceonMachineLearning.PMLR.
5
Blanchet, J., Lu, M., Zhang, T. and Zhong, H. (2023). Double pessimism is provably efficient for
distributionallyrobustofflinereinforcementlearning: Genericalgorithmandrobustpartialcoverage.arXiv
preprint arXiv:2305.09659 . 3, 5, 6, 7, 8, 11, 17, 18, 23, 26
Clavier, P., Pennec, E. L. and Geist, M. (2023). Towards minimax optimality of model-based robust
reinforcement learning. arXiv preprint arXiv:2302.05372 . 5
Dann, C., Lattimore, T. and Brunskill, E. (2017). Unifying pac and regret: Uniform pac bounds for
episodic reinforcement learning. Advances in Neural Information Processing Systems 30. 6
Ding, W., Shi, L., Chi, Y. and Zhao, D. (2024). Seeing is not believing: Robust reinforcement learning
against spurious correlation. Advances in Neural Information Processing Systems 36. 5
Dong, J., Li, J., Wang, B. and Zhang, J. (2022). Online policy optimization for robust mdp. arXiv
preprint arXiv:2209.13841 . 4
Du, S., Kakade, S., Lee, J., Lovett, S., Mahajan, G., Sun, W. and Wang, R. (2021). Bilinear
classes: A structuralframeworkfor provablegeneralizationinrl. In International Conference on Machine
Learning. PMLR. 6
18El Ghaoui, L. and Nilim, A. (2005). Robust solutions to markov decision problems with uncertain
transition matrices. Operations Research 53 780–798. 5
Foster,D.J.,Kakade,S.M.,Qian,J.andRakhlin,A.(2021).Thestatisticalcomplexityofinteractive
decision making. arXiv preprint arXiv:2112.13487 . 6
He, J., Zhao, H., Zhou, D.andGu, Q. (2023). Nearlyminimax optimalreinforcementlearningfor linear
markov decision processes. In International Conference on Machine Learning. PMLR. 6
Hu, J.,Zhong, H.,Jin, C.andWang, L.(2022). Provablesim-to-realtransferincontinuousdomainwith
partial observations. arXiv preprint arXiv:2210.15598 . 2
Huang, J., Zhong, H., Wang, L. and Yang, L. F. (2023a). Horizon-free and instance-dependent regret
bounds for reinforcementlearning with general function approximation. arXiv preprint arXiv:2312.04464
. 6
Huang,J.,Zhong,H.,Wang,L.andYang,L.F.(2023b).Tacklingheavy-tailedrewardsinreinforcement
learning with function approximation: Minimax optimal and instance-dependent regret bounds. arXiv
preprint arXiv:2306.06836 . 6
Iyengar, G. N. (2005). Robust dynamic programming. Mathematics of Operations Research 30 257–280.
2, 5, 7, 8
Jiang, N.,Krishnamurthy, A.,Agarwal, A., Langford, J.andSchapire, R. E.(2017). Contextual
decision processes with low bellman rank are pac-learnable. In International Conference on Machine
Learning. PMLR. 6
Jin, C.,Allen-Zhu, Z.,Bubeck, S.andJordan, M. I.(2018). Isq-learningprovablyefficient? Advances
in neural information processing systems 31. 6, 10
Jin, C., Liu, Q. and Miryoosefi, S. (2021). Bellman eluder dimension: New rich classes of rl problems,
and sample-efficient algorithms. Advances in neural information processing systems 34 13406–13418. 6
Jin, C., Yang, Z., Wang, Z. and Jordan, M. I. (2020). Provably efficient reinforcement learning with
linear function approximation. In Conference on Learning Theory. PMLR. 6
Kiran, B. R.,Sobh, I.,Talpaert, V.,Mannion, P.,Al Sallab, A. A.,Yogamani, S.andP´erez, P.
(2021). Deep reinforcementlearning for autonomous driving: A survey. IEEE Transactions on Intelligent
Transportation Systems 23 4909–4926. 2
Kober, J., Bagnell, J. A. and Peters, J. (2013). Reinforcement learning in robotics: A survey. The
International Journal of Robotics Research 32 1238–1274. 2
Kuang, Y., Lu, M., Wang, J., Zhou, Q., Li, B. and Li, H. (2022). Learning robust policy against
disturbanceintransitiondynamicsviastate-conservativepolicyoptimization. InProceedings of the AAAI
Conference on Artificial Intelligence, vol. 36. 3, 5
Li, G., Cai, C., Chen, Y., Wei, Y. and Chi, Y. (2023). Is q-learning minimax optimal? a tight sample
complexity analysis. Operations Research . 6
Li, Y. and Lan, G. (2023). First-order policy optimization for robust policy evaluation. arXiv preprint
arXiv:2307.15890 . 5
Liu, Z.,Lu, M., Wang, Z.,Jordan, M.andYang, Z.(2022). Welfaremaximizationincompetitiveequi-
librium: Reinforcement learning for markov exchange economy. In International Conference on Machine
Learning. PMLR. 6
Liu, Z.,Lu, M.,Xiong, W.,Zhong, H.,Hu, H.,Zhang, S.,Zheng, S.,Yang,Z.andWang, Z.(2023).
One objective to rule them all: A maximization objective fusing estimation and planning for exploration.
arXiv preprint arXiv:2305.18258 . 6
19Liu, Z.andXu, P.(2024a).Distributionallyrobustoff-dynamicsreinforcementlearning: Provableefficiency
with linear function approximation. arXiv preprint arXiv:2402.15399 . 5, 6
Liu, Z. and Xu, P.(2024b). Minimax optimaland computationally efficient algorithmsfor distributionally
robust offline reinforcement learning. arXiv preprint arXiv:2403.09621 . 5, 6
Lykouris, T., Simchowitz, M., Slivkins, A. and Sun, W. (2021). Corruption-robust exploration in
episodic reinforcement learning. In Conference on Learning Theory. PMLR. 6
Ma, X., Liang, Z., Xia, L., Zhang, J., Blanchet, J., Liu, M., Zhao, Q. and Zhou, Z. (2022).
Distributionally robust offline reinforcement learning with linear function approximation. arXiv preprint
arXiv:2209.06620 . 3, 5, 6
Maurer, A.andPontil, M. (2009). Empiricalbernsteinbounds andsample variancepenalization. arXiv
preprint arXiv:0907.3740 . 30
M´enard, P., Domingues, O. D., Shang, X. and Valko, M. (2021). Ucb momentum q-learning: Cor-
recting the bias without forgetting. In International Conference on Machine Learning. PMLR. 6
Moos, J., Hansel, K., Abdulsamad, H., Stark, S., Clever, D. and Peters, J. (2022). Robust
reinforcement learning: A review of foundations and recent advances. Machine Learning and Knowledge
Extraction 4 276–315. 3
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal,
S., Slama, K., Ray, A. et al. (2022). Training language models to follow instructions with human
feedback. Advances in Neural Information Processing Systems 35 27730–27744. 2
Panaganti, K. and Kalathil, D. (2022). Sample complexity of robust reinforcement learning with a
generative model. In International Conference on Artificial Intelligence and Statistics. PMLR. 3, 5, 8, 11
Panaganti, K., Xu, Z., Kalathil, D. and Ghavamzadeh, M. (2022). Robust reinforcement learning
using offline data. arXiv preprint arXiv:2208.05129 . 3, 4, 5, 8, 11, 13, 16
Peng, X. B., Andrychowicz, M., Zaremba, W.andAbbeel, P.(2018). Sim-to-realtransferofrobotic
controlwith dynamicsrandomization. In2018 IEEE international conference on robotics and automation
(ICRA). IEEE. 2
Pinto, L., Davidson, J., Sukthankar, R. and Gupta, A. (2017). Robust adversarial reinforcement
learning. In International Conference on Machine Learning. PMLR. 2, 3
Sadeghi, F. and Levine, S. (2016). Cad2rl: Real single-image flight without a single real image. arXiv
preprint arXiv:1611.04201 . 2
Shi, L. and Chi, Y. (2022). Distributionally robust model-based offline reinforcement learning with near-
optimal sample complexity. arXiv preprint arXiv:2208.05767 . 3, 5, 18
Shi, L., Li, G., Wei, Y., Chen, Y., Geist, M. and Chi, Y. (2023). The curious price of distributional
robustness in reinforcement learning with a generative model. arXiv preprint arXiv:2305.16589 . 3, 4, 5,
8, 9, 11, 16, 18, 25
Si, N., Zhang, F., Zhou, Z. and Blanchet, J. (2023). Distributionally robust batch contextual bandits.
Management Science . 5
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T.,
Baker, L., Lai, M., Bolton, A. et al. (2017). Mastering the game of go without human knowledge.
nature 550 354–359. 2
Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A. and Langford, J. (2019). Model-based rl in
contextual decision processes: Pac bounds and exponential improvements over model-free approaches. In
Conference on learning theory. PMLR. 6
20Sutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press. 3
Wang, H., Shi, L. and Chi, Y. (2024). Sample complexity of offline distributionally robust linear markov
decision processes. arXiv preprint arXiv:2403.12946 . 5, 6
Wang,L.,Zhang,W.,He,X.andZha,H.(2018).Supervisedreinforcementlearningwithrecurrentneural
networkfor dynamic treatment recommendation. InProceedings of the 24th ACM SIGKDD international
conference on knowledge discovery & data mining. 2
Wang, Q., Ho, C. P. and Petrik, M. (2022). On the convergence of policy gradient in robust mdps.
arXiv preprint arXiv:2212.10439 . 5
Wang, Q., Ho, C. P. and Petrik, M. (2023a). Policy gradient in robust MDPs with global convergence
guarantee. InProceedings of the40th InternationalConference on Machine Learning (A.Krause,E.Brun-
skill, K.Cho,B.Engelhardt,S.Sabato andJ.Scarlett,eds.), vol.202of Proceedings of Machine Learning
Research. PMLR. 5
Wang, S., Si, N., Blanchet, J. and Zhou, Z. (2023b). A finite sample complexity bound for distribu-
tionally robust q-learning. In International Conference on Artificial Intelligence and Statistics. PMLR.
5
Wang, S., Si, N., Blanchet, J. and Zhou, Z. (2023c). On the foundation of distributionally robust
reinforcement learning. arXiv preprint arXiv:2311.09018 . 5
Wang, S., Si, N., Blanchet, J. and Zhou, Z. (2023d). Sample complexity of variance-reduced distribu-
tionally robust q-learning. arXiv preprint arXiv:2305.18420 . 5
Wang, Y. and Zou, S. (2021). Online robust reinforcement learning with model uncertainty. Advances in
Neural Information Processing Systems 34 7193–7206. 5
Wang, Y. and Zou, S. (2022). Policy gradient method for robust reinforcement learning. In International
Conference on Machine Learning. PMLR. 5
Wei, C.-Y., Dann, C. and Zimmert, J. (2022). A model selection approach for corruption robust rein-
forcement learning. In International Conference on Algorithmic Learning Theory. PMLR. 6
Wiesemann, W., Kuhn, D. and Rustem, B. (2013). Robust markov decision processes. Mathematics of
Operations Research 38 153–183. 5
Wu, T.,Yang, Y.,Zhong, H.,Wang, L.,Du, S.andJiao, J.(2022).Nearlyoptimalpolicyoptimization
with stable at any time guarantee. In International Conference on Machine Learning. PMLR. 6
Xu, H. and Mannor, S. (2010). Distributionally robust markov decision processes. Advances in Neural
Information Processing Systems 23. 5
Xu, Y.andZeevi, A.(2023). Bayesiandesignprinciplesforfrequentistsequentiallearning. InInternational
Conference on Machine Learning. PMLR. 6
Xu, Z.,Panaganti, K.andKalathil, D.(2023). Improvedsamplecomplexityboundsfordistributionally
robustreinforcementlearning.InInternationalConferenceonArtificialIntelligenceandStatistics.PMLR.
3, 5, 8, 11, 18
Yang, R., Zhong, H., Xu, J., Zhang, A., Zhang, C., Han, L.andZhang, T.(2023a). Towardsrobust
offline reinforcement learning under diverse data corruption. arXiv preprint arXiv:2310.12955 . 6
Yang, W., Wang, H., Kozuno, T., Jordan, S. M. and Zhang, Z. (2023b). Avoiding model estimation
in robust markov decision processes with a generative model. arXiv preprint arXiv:2302.01248 . 5
Yang, W.,Zhang, L.andZhang, Z.(2022). Towardtheoreticalunderstandingsofrobustmarkovdecision
processes: Sample complexity and asymptotics. The Annals of Statistics 50 3223–3248. 3, 5, 8, 11, 16,
18, 23
21Ye, C.,He, J.,Gu, Q.andZhang, T.(2024). Towardsrobustmodel-basedreinforcementlearningagainst
adversarialcorruption. arXiv preprint arXiv:2402.08991 . 6
Ye, C., Xiong, W., Gu, Q. and Zhang, T. (2023a). Corruption-robust algorithms with uncertainty
weightingfornonlinearcontextualbanditsandmarkovdecisionprocesses. InInternational Conference on
Machine Learning. PMLR. 6
Ye, C., Yang, R., Gu, Q. and Zhang, T. (2023b). Corruption-robustoffline reinforcement learning with
general function approximation. arXiv preprint arXiv:2310.14550 . 6
Yu, Z., Dai, L., Xu, S., Gao, S. and Ho, C. P. (2023). Fast bellman updates for wasserstein distribu-
tionally robust mdps. In Thirty-seventh Conference on Neural Information Processing Systems. 5
Zanette, A.andBrunskill, E.(2019). Tighterproblem-dependentregretboundsinreinforcementlearn-
ing without domain knowledge using value function bounds. In International Conference on Machine
Learning. PMLR. 6
Zhang, X., Chen, Y., Zhu, X. and Sun, W. (2022). Corruption-robustoffline reinforcement learning. In
International Conference on Artificial Intelligence and Statistics. PMLR. 6
Zhang, Z., Chen, Y., Lee, J. D. and Du, S. S. (2023). Settling the sample complexity of online rein-
forcement learning. arXiv preprint arXiv:2307.13586 . 6
Zhang, Z.,Ji, X.andDu, S.(2021). Isreinforcementlearningmoredifficultthanbandits? anear-optimal
algorithm escaping the curse of horizon. In Conference on Learning Theory. PMLR. 6, 13
Zhang, Z., Zhou, Y. and Ji, X. (2020). Almost optimal model-free reinforcement learningvia reference-
advantage decomposition. Advances in Neural Information Processing Systems 33 15198–15207. 6
Zhao, W., Queralta, J. P. and Westerlund, T. (2020). Sim-to-real transfer in deep reinforcement
learning for robotics: a survey. In 2020 IEEE Symposium Series on Computational Intelligence (SSCI).
IEEE. 2
Zhong, H., Xiong, W., Zheng, S., Wang, L., Wang, Z., Yang, Z. and Zhang, T. (2022). Gec:
A unified framework for interactive decision making in mdp, pomdp, and beyond. arXiv preprint
arXiv:2211.01962 . 6
Zhong, H. and Zhang, T. (2023). A theoretical analysis of optimistic proximal policy optimization in
linear markov decision processes. arXiv preprint arXiv:2305.08841 . 6
Zhou, D., Gu, Q. and Szepesvari, C. (2021a). Nearly minimax optimal reinforcementlearning for linear
mixture markov decision processes. In Conference on Learning Theory. PMLR. 6
Zhou, R., Liu, T., Cheng, M., Kalathil, D., Kumar, P. and Tian, C. (2023). Natural actor-critic
for robust reinforcement learning with function approximation. In Thirty-seventh Conference on Neural
Information Processing Systems. 5
Zhou, Z.,Zhou, Z.,Bai, Q.,Qiu, L.,Blanchet, J.andGlynn, P.(2021b). Finite-sampleregretbound
fordistributionallyrobustofflinetabularreinforcementlearning. InInternational Conference on Artificial
Intelligence and Statistics. PMLR. 3, 5
22A Proofs for Properties of RMDPs with TV Robust Sets
A.1 Proof of Proposition 2.5
To simplify the notations, we present the following lemma, which directly implies Proposition 2.5.
Lemma A.1 (Strong duality for TV robust set). The following duality for total variation robust set holds,
for f : [0,H],
S 7→
σ
inf E [f]= sup E (η f) η minf(s) +η ,
Q( ·):DTV(Q( ·) kQ⋆( ·)) ≤σ Q( ·) η ∈[0,H](− Q⋆( ·)
(cid:2)
− + (cid:3)− 2 · (cid:18) − s ∈S (cid:19)+ )
where σ [0,1] and the TV distance D (Q() Q⋆()) is defined as
TV
∈ · k ·
1
D (Q() Q⋆())= Q(s) Q⋆(s).
TV
· k · 2 | − |
s
X∈S
Proof of Lemma A.1. First, we note that when Q⋆(s) > 0 for any s , i.e., any Q() ∆( ) is absolute
∈ S · ∈ S
continuous w.r.t. Q⋆(), it has been proved by Yang et al. (2022) that
·
σ
inf E [f]=sup E (η f) η minf(s) +η .
Q( ·):DTV(Q( ·) kQ⋆( ·)) ≤σ Q( ·) η ∈R(− Q⋆( ·) − + − 2 · (cid:18) − s ∈S (cid:19)+ )
(cid:2) (cid:3)
Furthermore, as is shown in Lemma H.8 in Blanchet et al. (2023), the optimal dual variable η⋆ lies in [0,H]
when f [0,H]. Therefore, for Q⋆() such that Q⋆(s)>0 for any s , we have
∈ · ∈S
σ
inf E [f]= sup E (η f) η minf(s) +η .
Q( ·):DTV(Q( ·) kQ⋆( ·)) ≤σ Q( ·) η ∈[0,H](− Q⋆( ·)
(cid:2)
− + (cid:3)− 2 · (cid:18) − s ∈S (cid:19)+ )
NowforanyQ⋆() ∆( ), wecanprovethe sameresultbyaveragingQ⋆() withauniformdistributionand
· ∈ S ·
taking the limit. More specifically, denote U() ∆( ) as the uniform distribution on , i.e., U(s)=1/
· ∈ S S |S|
for any s . Consider the following distributionally robust optimization problem, for any ǫ [0,1],
∈S ∈
P(ǫ):= inf E [f].
Q()
Q( ·):DTV Q( ·) k(1 −ǫ)Q⋆( ·)+ǫ ·U( ·) ≤σ ·
By our previous discussions, since (1
ǫ)Q⋆(s(cid:0)
)+ǫ U(s)>0 for
a(cid:1)
ny s and ǫ>0, we have that
− · ∈S
P(ǫ)=D(ǫ), ǫ (0,1], (A.1)
∀ ∈
where the function D():[0,1] R is defined as
+
· 7→
σ
D(ǫ):= sup (1 ǫ) E (η f) ǫ E (η f) η minf(s) +η .
η ∈[0,H](− − · Q⋆( ·)
(cid:2)
− + (cid:3)− · U( ·)
(cid:2)
− + (cid:3)− 2 · (cid:18) − s ∈S (cid:19)+ )
By the definition of P() and D(), our goalis to prove that P(0)=D(0). To this end, it suffices to prove that
· ·
(i) lim D(ǫ) exists and lim D(ǫ)=D(0); and (ii) lim P(ǫ)=P(0). To prove (i), consider that for
ǫ 0+ ǫ 0+ ǫ 0+
→ → →
any ǫ>0, by the definition of D(),
·
D(0) D(ǫ) sup ǫ E (η f) +ǫ E (η f) ǫ 2H.
Q⋆() + U() +
| − |≤ η [0,H] · · − · · − ≤ ·
∈ n (cid:2) (cid:3) (cid:2) (cid:3)o
Since the right hand side tends to 0 as ǫ tends to 0, we know that lim D(ǫ) exists, lim D(ǫ)=D(0).
ǫ 0+ ǫ 0+
→ →
This also indicates that lim P(ǫ) exists due to (A.1). This proves (i). Now we prove (ii). Notice that
ǫ 0+
→
since the set
Q() ∆( ):D Q() (1 ǫ)Q⋆()+ǫ U() σ
TV
· ∈ S · k − · · · ≤
(cid:8) (cid:0) (cid:1) (cid:9)
23is a closed subset of R , and E [f] is a continuous function of Q() R w.r.t. the -norm, we can
|S| Q() |S| 2
denote the optimal solution to
the·
optimization problem involved
in· P(∈
ǫ) as
k·k
Q ()= arginf E [f],
†ǫ
·
Q( ·)
Q( ·):DTV Q( ·) k(1 −ǫ)Q⋆( ·)+ǫ ·U( ·) ≤σ
which also gives that (cid:0) (cid:1)
P(ǫ)=E [f]= Q (s)f(s).
Q† ǫ( ·) †ǫ
s
X∈S
With these preparations, we are able to prove (ii). On the one hand, consider for any ǫ (0,1],
∈
D
TV
(1 −ǫ) ·Q†0( ·)+ǫ ·U( ·) (1 −ǫ) ·Q⋆( ·)+ǫ ·U( ·) ≤(1 −ǫ) ·σ ≤σ.
Therefore, for any ǫ (cid:0)(0,1], it holds that (cid:13) (cid:1)
∈ (cid:13)
P(ǫ)= inf E [f] E [f]=(1 ǫ) E [f]+ǫ E [f],
Q( ·):DTV Q( ·) k(1 −ǫ)Q⋆( ·)+ǫ ·U( ·) ≤σ Q( ·) ≤ (1 −ǫ) ·Q† 0( ·)+ǫ ·U( ·) − · Q† 0 · U( ·)
which implies that(cid:0) (cid:1)
lim P(ǫ) E [f]=P(0). (A.2)
ǫ 0+ ≤
Q†
0
→
On the other hand, for any ǫ (0,1],
∈
1
σ Q (s) (1 ǫ) Q⋆(s) ǫ U(s) (1 ǫ) D (Q () Q⋆()) ǫ D (Q () U()),
≥ 2
†ǫ
− − · − · ≥ − ·
TV †ǫ
· k · − ·
TV †ǫ
· k ·
s X∈S(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:12)
and by using D (Q () U()) 1, we obtain that
TV †ǫ
· k · ≤
σ+ǫ
D (Q () Q⋆()) . (A.3)
TV †ǫ
· k · ≤ 1 ǫ
−
Considerasequenceof ǫ convergingto0,i.e.,lim ǫ =0. Since Q () isasequencecontained
in a compact subset
of{ Ri }∞i= ,1
it has a converging
(w.r.ti .→0+ i
)
subsequen{ ce†ǫ di e· no} t∞i= ed1
by Q () whose
|S| k·k2
{
†ǫik
·
}∞k=1
limit is denoted as Q () ∆( ). By (A.3), we know that
†
· ∈ S
σ+ǫ
D (Q () Q⋆()) ik. (A.4)
TV †ǫik
· k · ≤ 1 −ǫ
ik
Takinglimitonbothsidesof(A.4)(limitofLHSexistssincetheTVdistanceisacontinuousfunction(w.r.t.
) of its first entry and the limit of RHS obviously exists), we obtain that
2
k·k
D TV(Q†() Q⋆()) σ. (A.5)
· k · ≤
Now we can arrive at the following,
lim P(ǫ)= lim E [f]= lim E [f]=E [f] inf E [f]=P(0),(A.6)
ǫ →0+ ǫ →0+ Q† ǫ( ·) k →0+ Q† ǫik( ·) Q†( ·) ≥Q( ·):DTV(Q( ·) kQ⋆( ·)) ≤σ Q( ·)
where the first and the last equality follows from the definition of P(), the second equality follows from the
choice of the sequence ǫ that convergesto 0, the third equalit· y is due to the continuity of E [f] of
Q() (w.r.t. ),
and{ ti hk e}∞k in= e1
quality follows from (A.5). Finally, with (A.2) and (A.6), we
concludQ e( ·)
that
2
· k·k
lim P(ǫ)=P(0),
ǫ 0+
→
which proves (ii). Consequently, by (i) and (ii)
P(0)= lim P(ǫ)= lim D(ǫ)=D(0).
ǫ 0+ ǫ 0+
→ →
Recalling the definitions of P() and D(), we conclude the proof of Lemma A.1.
· ·
24A.2 Proof of Proposition 2.7
Proof of Proposition 2.7. Here we prove a stronger result that for any policy π and step h [H]
∈
1
(s,am )ax Qπ h,P,Φ(s,a) −(s,am
)
in Qπ h,P,Φ(s,a)
≤ ρ ·
1 −(1 −ρ)H −h+1 , (A.7)
∈S×A ∈S×A (cid:16) (cid:17)
1
m saxV hπ ,P,Φ(s) −m
s
inV hπ ,P,Φ(s)
≤ ρ ·
1 −(1 −ρ)H −h+1 . (A.8)
∈S ∈S (cid:16) (cid:17)
First, we note that for the last step h= H, (A.7) and (A.8) naturally hold since R [0,1]. Now suppose
H
∈
that (A.8) hold for some step h+1. By robust Bellman equation (Proposition 2.2), we have that
Qπ h,P⋆,Φ(s,a)=R h(s,a)+E Pρ(s,a;P h⋆) V hπ +1,P⋆,Φ ≤1+E Pρ(s,a;P h⋆) V hπ +1,P⋆,Φ , ∀(s,a) ∈S×A, (A.9)
h i h i
where the inequality uses the fact that R 1. Now we denote the state with the least robust value as
h
≤
s
0
∈argminV hπ +1,P⋆,Φ(s). (A.10)
s
∈S
Inspired by Shi et al. (2023), we choose a transition kernel P satisfying that
h
P ( s,a) =1 ρ, P⋆(s s,a) P (s s,a) 0, (s,a,s) ,
h ·| 1 − h ′ | ≥ h ′ | e ≥ ∀ ′ ∈S×A×S
(cid:13) (cid:13)
which implies t(cid:13) (cid:13)ha et (cid:13) (cid:13) e
D P ( s,a)+ρ δ () P⋆( s,a) ρ, (s,a) .
TV h ·| · s0 · h ·| ≤ ∀ ∈S×A
(cid:16) (cid:13) (cid:17)
Here δ s0( ·) is the point measu ere centered at s
0
de(cid:13) (cid:13)fined in (A.10). Combined with (A.9), we have that
Qπ h,P⋆,Φ(s,a) ≤1+E Pe h( ·|s,a)+ρ ·δs0( ·) V hπ +1,P⋆,Φ
h i
=1+E Pe h( ·|s,a) V hπ +1,P⋆,Φ +ρ ·V hπ +1,P⋆,Φ(s 0)
1+(1 ρ) mhaxVπ i (s)+ρ minVπ (s). (A.11)
≤ − · s
h+1,P⋆,Φ
· s
h+1,P⋆,Φ
∈S ∈S
Consequently from (A.11), we further obtain that for any (s,a) ,
∈S×A
Qπ (s,a) min Qπ (s,a)
h,P⋆,Φ
−(s,a)
h,P⋆,Φ
∈S×A
1+(1 ρ) maxVπ (s)+ρ minVπ (s) min Qπ (s,a)
≤ − · s
h+1,P⋆,Φ
· s
h+1,P⋆,Φ
−(s,a)
h,P⋆,Φ
∈S ∈S ∈S×A
=1+(1 ρ) maxVπ (s) minVπ (s) +minVπ (s) min Qπ (s,a)
− · s
h+1,P⋆,Φ
− s
h+1,P⋆,Φ
s
h+1,P⋆,Φ
−(s,a)
h,P⋆,Φ
(cid:18) ∈S ∈S (cid:19) ∈S ∈S×A
1+(1 ρ) maxVπ (s) minVπ (s) , (A.12)
≤ − · s
h+1,P⋆,Φ
− s
h+1,P⋆,Φ
(cid:18) ∈S ∈S (cid:19)
where the first inequality uses (A.11) and the last inequality uses the following fact,
(s,am
)
in Qπ h,P⋆,Φ(s,a)= (s,am
)
in R h(s,a)+E Pρ(s,a;P h⋆) V hπ +1,P⋆,Φ ≥m
s
inV hπ +1,P⋆,Φ(s).
∈S×A ∈S×A(cid:26) h i(cid:27) ∈S
Now applying the assumptionthat (A.8)holds atstep h+1 to the righthand side of(A.12), we obtainthat
1 ρ
(s,am )ax Qπ h,P⋆,Φ(s,a) −(s,am
)
in Qπ h,P⋆,Φ(s,a) ≤1+ −
ρ ·
1 −(1 −ρ)H −h
∈S×A ∈S×A (cid:16) (cid:17)
1
= 1 (1 ρ)H h+1 .
−
ρ · − −
(cid:16) (cid:17)
Thus given (A.8) at step h+1, we can derive (A.7) at step h. Now by noticing that
(s,am
)
in Qπ h,P⋆,Φ(s,a) ≤m
s
inV hπ ,P⋆,Φ(s) ≤m saxV hπ ,P⋆,Φ(s) ≤(s,am )ax Qπ h,P⋆,Φ(s,a),
∈S×A ∈S ∈S ∈S×A
we can conclude that (A.8) also holds at step h. As a result, by an induction argument, we finish the proof
of Proposition2.7.
25A.3 Proof of Proposition 4.2
Proof of Proposition 4.2. We consider some fixed (s,a,h) [H] throughout proof. By Lemma A.1,
∈S×A×
we have that
ρ
E Pρ(s,a;P h⋆)[V]=s ηu ∈p R(−E P h⋆( ·|s,a) (η −V) + − 2 · (cid:18)η −m s ∈i SnV(s) (cid:19)++η )
(cid:2) (cid:3)
ρ
= η ∈s [u 0p ,H](−E P h⋆( ·|s,a) (η −V) + − 2 · (cid:18)η −m s ∈i SnV(s) (cid:19)++η )
(cid:2) (cid:3)
ρ
=
η
∈s [u 0p ,H](cid:26)−E P h⋆( ·|s,a) (cid:2)(η −V) + (cid:3)+ (cid:16)1 − 2 (cid:17)·η (cid:27), (A.13)
wherethe secondequalityfollowsfromthe facttheoptimaldualvariableη⋆ is in[0,H]whenV [0,H](see
∈
e.g.,LemmaH.8inBlanchet et al.(2023)),andthelastequalityisobtainedbythefactthatmin V(s)=0.
s
∈S
Part (i). For any η ∈[0,H] and Q ∈Bρ′(s,a;P h⋆), we have that
ρ ρ
−E P h⋆( ·|s,a) (η −V) + + 1 − 2 ·η ≤ 1 − 2 · −E Q( ·) (η −V) + +η
(cid:2) (cid:3) (cid:16) (cid:17) (cid:16) 1 ρ(cid:17) (cid:16) E (cid:2) η V +(cid:3) η (cid:17)
≤ − 2 · − Q( ·) −
=(cid:16) 1 ρ(cid:17) (cid:16) E V ,(cid:2) (cid:3) (cid:17) (A.14)
− 2 · Q( ·)
(cid:16) (cid:17) (cid:2) (cid:3)
where the first inequality uses the definition of (s,a;P⋆), the second equality follows from the fact that
Bρ′ h
(x) x. Furthermore, by (A.14) we have that
+
≥
ρ ρ
η ∈s [u 0p ,H](cid:26)−E P h⋆( ·|s,a) (cid:2)(η −V) + (cid:3)+ (cid:16)1 − 2 (cid:17)·η (cid:27)≤ (cid:16)1 − 2 (cid:17)·Q ∈Bρi ′n (sf ,a;P h⋆)E Q( ·) (cid:2)V (cid:3). (A.15)
Combining (A.13) and (A.15), we have that
E Pρ(s,a;P h⋆) V ≤ρ ′ ·E Bρ′(s,a;P h⋆) V .
(cid:2) (cid:3) (cid:2) (cid:3)
Part (ii). Since ρ [0,1], we know that there exists a η [0,H] such that
∈ ∈
ρ
P⋆(s s,a) 1 P⋆(s s,a),
h ′ | ≤ − 2 ≤e h ′ |
s′:VX(s′)<ηe s′:VX(s′) ≤ηe
which further implies that we have the following interpolation for some λ [0,1]:
∈
ρ
1 =λ P⋆(s s,a)+(1 λ) P⋆(s s,a).
− 2 h ′ | − h ′ |
s′:VX(s′)<ηe s′:VX(s′) ≤ηe
We define a probability measure P⋆ ∆( ) as
∈ S
λP⋆(s s,a) 1 V(s)>η +(1 λ)P⋆(s s,a) 1 V(s) η
P⋆ = h ′ |e · { ′ } − h ′ | · { ′ ≥ }. (A.16)
h 1 ρ
− 2
e e
e
It is not difficult to verify that P⋆ (s,a;P⋆). Hence, we have
h ∈Bρ′ h
ρ ρ
1 − 2 ·Ee Bρ′(s,a;P h⋆)[V] ≤ 1 − 2 ·E Pe h⋆( ·) V
(cid:16) (cid:17) =(cid:16) 1 − ρ 2(cid:17) ·E Pe h⋆( ·)(cid:2) V(cid:3) −η + 1 − ρ 2 ·η
=(cid:16) −E P h⋆( ·|(cid:17) s,a) (η −(cid:2) V) + e+(cid:3) 1(cid:16) − ρ 2 ·(cid:17) η, e (A.17)
(cid:2) (cid:3) (cid:16) (cid:17)
e e
26where the last inequality uses the definition of P⋆ in (A.16). Furthermore, by (A.17) we have that
h
ρ ′ ·E Bρ′(s,a;P h⋆) (cid:2)V
(cid:3)
=≤
Eη
∈s P[u
0
ρp
, (H s,]
ae
(cid:26)
;P
h⋆−
)
E VP h⋆ ,( ·|s,a) (cid:2)(η −V) + (cid:3)+ (cid:16)1 − ρ 2 (cid:17)·η
(cid:27)
(A.18)
(cid:2) (cid:3)
where the equality follows from (A.13).
Combining Part (i) and Part (ii). Finally,combining(A.15)and(A.18),weproveProposition4.2.
B Proofs for Hardness Results
B.1 Proof of Theorem 3.2
Proof of Theorem 3.2. We first explicitly give the expressions of the robust value functions in Example 3.1,
based on which we derive the desired online regret lower bound.
Robust value function. Firstly,wecanexplicitlywritedowntheexpressionoftherobustvaluefunctions
for any policy π under Example 3.1, i.e., Vπ and Qπ . From now on we fix a policy π.
h,P⋆,Mθ,Φ h,P⋆,Mθ,Φ
For step h=3, the robust value function is the reward received. We can directly obtain for any a ,
∈A
Qπ (s ,a)=Vπ (s )=1, Qπ (s ,a)=Vπ (s )=0. (B.1)
3,P⋆,Mθ,Φ good 3,P⋆,Mθ,Φ good 3,P⋆,Mθ,Φ bad 3,P⋆,Mθ,Φ bad
For step h=2, by the robustBellman equation(Proposition2.2), we have that for the good state s ,
good
Qπ (s ,a)=1+ inf E Vπ =1+(1 ρ), a , (B.2)
2,P⋆,Mθ,Φ good
P ∈Pρ(sgood,a;P 2⋆,Mθ)
P( ·)
(cid:2)
3,P⋆,Mθ,Φ
(cid:3)
− ∀ ∈A
where the last equality is because Vπ takes the minimal value 0 at the bad state s and thus the
3,P⋆,Mθ,Φ bad
most adversarialtransition distribution is achieved at
P (s)=(1 ρ) 1 s =s +ρ 1 s =s .
† ′ ′ good ′ bad
− · { } · { }
Similarly, we have that for the bad state s ,
bad
p ρ, a=θ
Qπ (s ,a)=0+ inf E Vπ = − . (B.3)
2,P⋆,Mθ,Φ bad
P ∈Pρ(sbad,a;P 2⋆,Mθ)
P( ·)
(cid:2)
3,P⋆,Mθ,Φ
(cid:3)
(q −ρ, a=1 −θ
Finally by the robust Bellman equation again, we have that
Vπ (s )=1+(1 ρ), Vπ (s )=π (θ s ) (p ρ)+π (1 θ s ) (q ρ).
2,P⋆,Mθ,Φ good
−
2,P⋆,Mθ,Φ bad 2
|
bad
· −
2
− |
bad
· −
Notice that by q <p we know that Vπ (s )<p ρ<1+(1 ρ)<Vπ (s ).
2,P⋆,Mθ,Φ bad − − 2,P⋆,Mθ,Φ good
Forsteph=1,weconsiderthe robustvaluesonthe initialstates =s ,byrobustBellmanequation,
1 good
Qπ (s ,a)=1+ inf E Vπ (B.4)
1,P⋆,Mθ,Φ good
P ∈Pρ(sgood,a;P 1⋆,Mθ)
P( ·)
(cid:2)
2,P⋆,Mθ,Φ
(cid:3)
=1+(1 ρ) 1+(1 ρ) +ρ π (θ s ) (p ρ)+π (1 θ s ) (q ρ) ,
2 bad 2 bad
− · − · | · − − | · −
for any action a . By robust Bellma(cid:2) n equation,(cid:3) we als(cid:2) o derive Vπ (s )=Qπ (s (cid:3) ,a).
∈A 1,P⋆,Mθ,Φ good 1,P⋆,Mθ,Φ good
27Lower bound the online regret under Example 3.1. Withallthe previouspreparation,wecanlower
bound the online regret for robust RL with interactive data collection in Example 3.1. But first, we present
the following general lemma.
Lemma B.1 (Performancedifferencelemmaforrobustvaluefunction). For any RMDP satisfying Assump-
tion 2.1 and any policy π, the following inequality holds,
H
V 1π ,P⋆ ⋆,Φ(s) −V 1π ,P⋆,Φ(s) ≥E (Pπ⋆,†,π⋆)
"
π h⋆(a |s h) −π h(a |s h) ·Qπ h,P⋆,Φ(s h,a) (cid:12)s 1 =s #,
h X=1a X∈A(cid:0) (cid:1) (cid:12) (cid:12)
where the expectation is taken with respect to the trajectories induced by policy π⋆, transiti(cid:12) (cid:12)on kernel Pπ⋆, .
†
Here the transition kernel
Pπ⋆,
is defined as
†
P hπ⋆, †( ·|s,a)=
P
ar (g s,i an ;f P⋆)E
P( ·)
V hπ +⋆
1,P⋆,Φ
,
∈P h (cid:2) (cid:3)
where (s,a;P⋆) is the robust set for state-action pair (s,a) (see Assumption 2.1).
P h
Proof of Lemma B.1. Please refer to Appendix B.2 for a detailed proof of Lemma B.1.
Now back to Example 3.1, our previous calculation actually shows that, by (B.1) for step h=3,
π 3⋆, Mθ(a |s 3) −π 3(a |s 3) ·Qπ 3,P⋆,Mθ,Φ(s 3,a)=0, ∀s
3
∈{s good,s
bad
}. (B.5)
a
X∈A(cid:0) (cid:1)
and by (B.4) we also have that for step h=1,
π 1⋆, Mθ(a |s 1) −π 1(a |s 1) ·Qπ 1,P⋆,Mθ,Φ(s 1,a)=0, where s
1
=s good. (B.6)
a
X∈A(cid:0) (cid:1)
Finally, let’s consider step h=2. By (B.2), we have that for the good state, it holds that
π 2⋆, Mθ(a |s good) −π 2(a |s good) ·Qπ 2,P⋆,Mθ,Φ(s good,a)=0, (B.7)
a
X∈A(cid:0) (cid:1)
Meanwhile, by (B.3), we have that for the bad state, it holds that (recall that q <p)
π 2⋆, Mθ(a |s bad) −π 2(a |s bad) ·Qπ 2,P⋆,Mθ,Φ(s bad,a)
a
X∈A(cid:0) (cid:1)
=max p ρ,q ρ π (θ s ) (p ρ)+π (1 θ s ) (q ρ)
2 bad 2 bad
− − − | · − − | · −
(cid:8) (cid:9) (cid:16) (cid:17)
=p ρ π (θ s ) (p ρ)+π (1 θ s ) (q ρ)
2 bad 2 bad
− − | · − − | · −
p q (cid:16) (cid:17)
= −
2 ·
π 2⋆, Mθ(θ |s bad) −π 2(θ |s bad) + π 2⋆, Mθ(1 −θ |s bad) −π 2(1 −θ |s bad)
(cid:18)(cid:12) (cid:12) (cid:12) (cid:12)(cid:19)
=(p −q) ·D(cid:12) (cid:12)TV π 2⋆, Mθ( ·|s bad) π 2( ·|s ba(cid:12) (cid:12)d) (cid:12) (cid:12), (cid:12) (cid:12) (B.8)
(cid:16) (cid:13) (cid:17)
where according to (B.3) the optimal policy of(cid:13) (cid:13)
Mθ
at h=2 and s
bad
is π 2⋆, Mθ(θ |s bad)=1. Now combining
(B.5), (B.6), (B.7), and (B.8) with Lemma B.1, we can conclude that
Vπ⋆,Mθ (s ) Vπ (s )
1,P⋆,Mθ,Φ good
−
1,P⋆,Mθ,Φ good
E π⋆(as ) π (as ) Qπ (s ,a)
≥ a1∼π 1⋆,Mθ( ·|sgood),s2∼P 1π⋆,Mθ,†( ·|sgood,a1)"
a
2 | 2 − 2 | 2 · 2,P⋆,Mθ,Φ 2 #
X∈A(cid:0) (cid:1)
=P 1π⋆,Mθ, †(s
bad
|s good,0) ·(p −q) ·D
TV
π 2⋆, Mθ( ·|s bad) π 2( ·|s bad) , (B.9)
(cid:16) (cid:13) (cid:17)
(cid:13)
(cid:13)
28where the adversarialtransition kernel P
1π⋆,Mθ,
† is given by
P 1π⋆,Mθ, †( ·|s good,0)=
P
∈P(sa gr oog dm ,0i ;n
P
1⋆,Mθ)E
P( ·)
hV 2π ,P⋆, ⋆M ,Mθ
θ,Φ
i
=(1 ρ) 1 =s +ρ 1 =s . (B.10)
good bad
− · {· } · {· }
Consequently, taking (B.10) back into (B.9), we have that
V 1π ,P⋆, ⋆M ,Mθ θ,Φ(s good) −V 1π ,P⋆,Mθ,Φ(s good) ≥ρ ·(p −q) ·D
TV
π 2⋆, Mθ( ·|s bad) π 2( ·|s bad) .
(cid:16) (cid:13) (cid:17)
Thisimpliesthatforanyalgorithmexecutingπ1, ,πK,itsonlineregretislowerb(cid:13)oundedbythefollowing,
··· (cid:13)
K
RegretMΦθ, ALG(K)= V 1π ,P⋆, ⋆M ,Mθ θ,Φ(s good) −V 1π ,Pk ⋆,Mθ,Φ(s good)
k=1
X
K
≥ρ ·(p −q)
·
D
TV
π 2⋆, Mθ( ·|s bad) π 2k( ·|s bad) .
Xk=1 (cid:16) (cid:13) (cid:17)
(cid:13)
However,since in RMDPs ofExample 3.1, the online interactionprocessis(cid:13)alwayskept ins andthere is
good
no information on θ which can only be accessed at (s,h)=(s ,2). As a result, the estimates πk( s ) of
bad 2 ·| bad
π 2⋆, Mθ( ·|s bad)=1 {·=θ
}
can do no better than a random guess. Put it formally, consider that
θsu 0p
,1
E
Mθ,
ALG
RegretMΦθ, ALG(K)
∈{ } h i
K
≥ρ ·(p −q)
·
θsu 0p
,1
E
Mθ,
ALG"
D
TV
π 2⋆, Mθ( ·|s bad) π 2k( ·|s bad)
#
∈{ } Xk=1 (cid:16) (cid:13) (cid:17)
(cid:13)
K (cid:13)
=ρ (p q) sup E πk(1 θ s ) . (B.11)
· − · θ 0,1 ALG 2 − | bad
∈{ }Xk=1 (cid:2) (cid:3)
Here inthe lastequality we candropthe subscriptionof because the algorithmoutputs πk independent
Mθ 2
of the θ due to our previous discussion. Notice that
K K K
E πk(1 θ s ) = E πk(1 θ s ) = 1=K,
ALG 2 − | bad ALG 2 − | bad
θ ∈X{0,1 }k X=1
(cid:2) (cid:3)
k X=1θ ∈X{0,1
} (cid:2) (cid:3)
Xk=1
which further indicates that
K
K
sup E πk(1 θ s ) . (B.12)
θ 0,1 ALG 2 − | bad ≥ 2
∈{ }Xk=1 (cid:2) (cid:3)
Therefore, by combining (B.11) and (B.12), we conclude that
ρK
Ain Lf
Gθ
∈s {u 0p
,1
}E
Mθ,
ALG
hRegretMΦθ, ALG(K) i≥(p −q)
· 2
.
This is the desiredonline regretlowerbound ofΩ(ρ K) for the RMDPs presentedinExample3.1. Further-
·
more, we can construct two RMDPs , with horizon 3H by concatenating H RMDPs ,
0 1 0 1
presented in Example 3.1. Notably, at{M any sM tep} s {3i+1 }H i=−01, we define {M M }
f f
f
R 3i+1(s bad,a)=1, P 3⋆ i, +M1θ(s
good
|s bad,a)=1, ∀(a,θ) ∈A×{0,1 }.
Then we have
f
Ain Lf
Gθ
∈s {u 0p
,1
}E Mf
θ,
ALG
hRegretMΦθ, ALG(K) i≥H ·Ω(ρ ·K)=Ω(ρ ·HK),
which completes the proof of Theorem 3.2.
29B.2 Proof of Lemma B.1
Proof of Lemma B.1. For any step h [H], we have that by robust Bellman equation (Proposition 2.2),
∈
Qπ h,⋆ P⋆,Φ(s,a) −Qπ h,P⋆,Φ(s,a)=E Pρ(s,a;P h⋆) V hπ +⋆ 1,P⋆,Φ −E Pρ(s,a;P h⋆) V hπ +1,P⋆,Φ .
By the definition of the transition kernel
Pπ⋆,
in
Lemm(cid:2)
a B.1 and
t(cid:3)
he property
o(cid:2)
f
infimum,(cid:3)
we have that
†
Qπ h,⋆ P⋆,Φ(s,a) −Qπ h,P⋆,Φ(s,a) ≥E
P
hπ⋆,†(
·|s,a)
V hπ +⋆ 1,P⋆,Φ −E
P
hπ⋆,†(
·|s,a)
V hπ +1,P⋆,Φ
=E
P
hπ⋆,†( ·|s,a)(cid:2)V hπ +⋆ 1,P⋆,Φ(cid:3) −V hπ +1,P⋆,Φ . (cid:2) (cid:3) (B.13)
(cid:2) (cid:3)
By robust Bellman equation (Proposition 2.2) and (B.13), we further obtain that
V hπ ,P⋆ ⋆,Φ(s) −V hπ ,P⋆,Φ(s)=E π h⋆( ·|s) Qπ h,⋆ P⋆,Φ(s, ·) −E πh( ·|s) Qπ h,P⋆,Φ(s, ·)
=E π h⋆( ·|s)(cid:2)Qπ h,P⋆,Φ(s, ·)(cid:3) −E πh( ·|s)(cid:2)Qπ h,P⋆,Φ(s, ·)(cid:3)
+E π h⋆(cid:2)( ·|s) Qπ h,⋆ P⋆,Φ((cid:3)s, ·) −E π h⋆(cid:2)( ·|s) Qπ h,P⋆,Φ((cid:3)s, ·)
≥
π h⋆(a |s)(cid:2) −π h(a |s) ·(cid:3)Qπ h,P⋆,Φ(s,(cid:2)a) (cid:3)
a
X∈A(cid:0) (cid:1)
+E a ∼π h⋆( ·|s),P hπ⋆,†( ·|s,a) V hπ ,P⋆,Φ −V hπ ,P⋆,Φ . (B.14)
(cid:2) (cid:3)
Thus by recursively applying (B.14) over h [H], we can conclude that
∈
H
V 1π ,P⋆ ⋆,Φ(s) −V 1π ,P⋆,Φ(s) ≥E (Pπ⋆,†,π⋆)
"
π h⋆(a |s h) −π h(a |s h) ·Qπ h,P⋆,Φ(s h,a) (cid:12)s 1 =s #,
h X=1a X∈A(cid:0) (cid:1) (cid:12) (cid:12)
which completes the proof of Lemma B.1. (cid:12)
(cid:12)
C Proofs for Theoretical Analysis of OPROVI-TV
In this section, we prove our main theoretical results (Theorem 4.6). In Appendix C.1, we outline the proof
of the theorem. In Appendix C.2, we list all the key lemmas used in the proof of the theorem. We defer the
proof of all the lemmas to subsequent sections (Appendices C.3 to C.8).
Before presenting all the proofs, we define the typical event as
E
E = 
(cid:12)
(cid:12)(cid:16)E P h⋆( ·|s,a) −E Pb hk( ·|s,a) (cid:17)h(cid:0)η −V h⋆ +1,P⋆,Φ (cid:1)+
i(cid:12)
(cid:12)≤v u
u
tV Pb hk( ·|s,a) h(cid:0)Nη hk− (sV ,h a⋆ + )1 ∨,P 1⋆,Φ (cid:1)+ i·c 1ι + N hk(c s2 ,H a)ι ∨1,
(cid:12) (cid:12)
(cid:12) (cid:12)

min P⋆(s s,a),Pk(s s,a) c ι
P h⋆(s ′ |s,a) −P h(s ′ |s,a) ≤v u n h ′ N| k(s,a)h 1′| o· 1 + Nk(sc ,2 aι ) 1,
(cid:12) (cid:12) u h b ∨ h ∨
(cid:12) (cid:12) b (cid:12) (cid:12) t
∀(s,a,s′,h,k) ∈S×A×S×[H] ×[K], ∀η
∈N1/(S√K)
[0,H] , ι=log S3AH2K3/2/δ ,

(cid:0) (cid:1) (cid:0) (cid:1)
where c ,c >0 are two absolute constants, ([0,H]) denotes an 1/S√K-cover of the interval [0,H].
1 2 N1/S√K
Lemma C.1 (Typical event). For the typical event defined in (C.35), it holds that P( ) 1 δ.
E E ≥ −
Proof of Lemma C.1. ThisisadirectapplicationofBernsteininequalityanditsempiricalversion(Maurer and Pontil,
2009), together with a union bound over (s,a,s,h,k,η) [H] [K] ([0,H]). Note
′ ∈S×A×S× × ×N1/(S√K)
that the size of ([0,H]) is of order SH√K.
N1/(S√K)
In this section, we always let the event hold, which by Lemma C.1 is of probability at least 1 δ.
E −
30C.1 Proof of Theorem 4.6
Proof of Theorem 4.6. With Lemma C.2 (optimism and pessimism), we can upper bound the regret as
K K
RegretΦ(K)= V 1⋆ ,P⋆,Φ(s 1) −V 1π ,Pk ⋆,Φ(s 1)
≤
Vk 1(s 1) −Vk 1(s 1). (C.1)
k=1 k=1
X X
In the sequel, we break our proof into three steps.
Step 1: upper bounding (C.1). According to the choice of Qk , Qk, Vk , Vk in (4.4), (4.5), and (4.6),
h h h h
let’s consider that for any (h,k) [H] [K] and (s,a) ,
∈ × ∈S×A
Qk h(s,a) −Qk h(s,a)=min (cid:26)R h(s,a)+E Pρ(s,a;Pb hk) hVk h+1 i+bonusk h(s,a), min (cid:8)H,ρ −1
(cid:9)(cid:27)
−max (cid:26)R h(s,a)+E Pρ(s,a;Pb hk) hVk h+1 i−bonusk h(s,a), 0
(cid:27)
≤E Pρ(s,a;Pb hk) Vk h+1 −E Pρ(s,a;Pb hk) Vk h+1 +2 ·bonusk h(s,a)
=E
Pρ(s,a;Pb
hk)h Vk h+1i −E
Pρ(s,a;P
h⋆)h Vk h+1i +E
Pρ(s,a;P h⋆)
Vk
h+1
−E
Pρ(s,a;Pb hk)
Vk
h+1
h i h i h i h i
Term (i)
| +E Pρ(s,a;P h⋆) Vk h+1 −E Pρ(s,a;P h⋆) Vk h{ +z 1 +2 ·bonusk h(s,a). (C} .2)
h i h i
Term (ii)
| {z }
Step 1.1: upper bounding Term (i). By using a Bernstein-style concentration argument customized
for TV robust expectations (Lemma C.3), we can bound Term (i) by the bonus function, i.e.,
Term (i) 2 bonusk(s,a). (C.3)
≤ · h
Step 1.2: upper bounding Term (ii). Byourdefinitionofthe operatorE
Pρ(s,a;P
h⋆)[V]in(4.3),wehave
Term (ii)= η ∈s [u 0p ,H](cid:26)−E P h⋆( ·|s,a) (cid:20)(cid:16)η −Vk h+1 (cid:17)+ (cid:21)+ (cid:16)1 − ρ 2 (cid:17)·η (cid:27)
ρ
− η ∈s [u 0p ,H](cid:26)−E P h⋆( ·|s,a) (cid:20)(cid:16)η −Vk h+1 (cid:17)+ (cid:21)+ (cid:16)1 − 2 (cid:17)·η (cid:27)
≤ η ∈s [u 0p ,H](cid:26)E P h⋆( ·|s,a) (cid:20)(cid:16)η −Vk h+1 (cid:17)+− (cid:16)η −Vk h+1 (cid:17)+ (cid:21)(cid:27). (C.4)
By Lemma C.2 whichshowsthatVk Vk andthe factthat (η x) (η y) y xfor anyy >x,
h+1 ≤ h+1 − + − − + ≤ −
we can further upper bound the right hand side of (C.4) by
Term (ii) ≤E P h⋆( ·|s,a) Vk h+1−Vk h+1 . (C.5)
h i
Step 1.3: combining the upper bounds. Now combining (C.3) and (C.5) with (C.2), we have that
Qk h(s,a) −Qk h(s,a) ≤E P h⋆( ·|s,a) Vk h+1−Vk h+1 +4 ·bonusk h(s,a).
h i
By LemmaC.4,we canupper boundthe bonus function, andafter rearrangingtermswe furtherobtainthat
Qk h(s,a) −Qk h(s,a) ≤ 1+ 1 H2 ·E P h⋆( ·|s,a) Vk h+1−Vk h+1
(cid:18) (cid:19) h i
31+4v
uV P h⋆( ·|s, Na) kh(V sh ,π + ak 1 ),P⋆ 1,Φ i·c 1ι
+
N4 kc (2 sH ,a2 )Sι
1
+
√4
K, (C.6)
u h ∨ h ∨
t
where c ,c >0 are two absolute constants. For the sake of brevity, we introduce the following notations of
1 2
differences, for any (h,k) [H] [K],
∈ ×
∆k :=Vk (sk) Vk(sk),
h h h − h h
ζk :=∆k Qk (sk,ak) Qk(sk,ak) , (C.7)
h h− h h h − h h h
ξk :=E (cid:16) Vk Vk ∆k .(cid:17) (C.8)
h P h⋆( ·|sk h,ak h) h− h − h+1
h i
If we further define the filtration as
h,k (h,k) [H] [K]
{F } ∈ ×
=σ (sτ,aτ) (sk,ak) sk ,
Fh,k { i i }(i,τ) ∈[H] ×[k −1] { i i }i ∈[h −1] { h}
(cid:16) [ [ (cid:17)
thenwecanfindthat ζk isamartingaledifferencesequencewithrespectto
and ξk { ish a}( mh, ak r) ∈ ti[ nH g] × al[ eK] difference sequence with respect to ak {Fh,k }(h,k) ∈[ .H A] × l[ sK o,]
{
h}(h,k) ∈[H] ×[K] {Fh,k
∪{
h}}}(h,k) ∈[H] ×[K]
we further have that
∆k =ζk+ Qk (sk,ak) Qk(sk,ak) (C.9)
h h h h h − h h h
(cid:16) (cid:17)
≤ζ hk+ (cid:18)1+ 1 H2 (cid:19)·E
P h⋆( ·|sk h,ak h)
hVk h+1−Vk
h+1
i+4v
u
uV P h⋆( ·| Ns,a hk) (hsV
k
hh ,π + ak 1
k
h, )P ∨⋆,Φ 1i·c 1ι +
N
hk4 (c s2
k
hH ,a2
k
hS )ι
∨1
+ √4
K
t
=ζ hk+ 1+ 1 H2 ·ξ hk+ 1+ 1 H2 ·∆k h+1+4v uV P h⋆( ·| Ns,a k) (hsV kh ,π + ak 1 k, )P⋆,Φ 1i·c 1ι + Nk4 (c s2 kH ,a2 kS )ι
1
+ √4 K,
(cid:18) (cid:19) (cid:18) (cid:19) u h h h ∨ h h h ∨
t
wheretheinequalityapplies(C.6). Recursivelyapplying(C.9)andusingthefactthat(1+12)h (1+12)H
H ≤ H ≤
c for some absolute constant c>0, we can upper bound the right hand side of (C.1) as
RegretΦ(K)
≤
K
∆k
1
≤C
1 ·
K H
(ζ hk+ξ hk)+v
uV P h⋆( N·|s k,a () shkV ,h aπ +k k1 ),P⋆, 1Φ i·ι
+
Nk(sH k,2 aS kι
) 1
+
√1
K.(C.10)
k=1 k=1h=1 u h h h ∨ h h h ∨
X XX t
where C >0 is an absolute constant.
1
Step 2: controlling the summation of variance terms. In view of (C.10), it suffices to upper bound
its righthandside. The keydifficulty isthe analysisofthe summationofthevarianceterms,whichwefocus
on now. By Cauchy-Schwartzinequality,
K H V Vπk K H K H
k X=1h
X=1v u
u
t
P h⋆( N·|s hkk h (,a sk h k h) ,hak hh )+ ∨1,P 1⋆,Φ i ≤v
u uXk=1h
X=1V P h⋆( ·|sk h,ak h) hV hπ +k 1,P⋆,Φ i·
Xk=1h
X=1N hk(sk h,1 ak h) ∨1.(C.11)
t
Onthe righthand side of (C.11), the summationof the inverseofthe count function is a wellbounded term
(LemmaC.13). Sothekeyistoupperboundthethesummationofthevarianceoftherobustvaluefunctions
to obtain a sharp bound. To this end, we invoke Lemma C.5 to obtain that with probability at least 1 δ,
−
K H
V Vπk C min H,ρ 1 HK+min H,ρ 1 3 Hι , (C.12)
P h⋆( ·|sk h,ak h) h+1,P⋆,Φ ≤ 2 · − · − ·
k X=1h X=1 h i (cid:16) (cid:8) (cid:9) (cid:8) (cid:9) (cid:17)
32where C >0 is an absolute constant. With inequality (C.12) and Lemma C.13 that
2
K H
1
C HSAι,
Nk(sk,ak) 1 ≤ 2′ ·
k=1h=1 h h h ∨
XX
with C >0 being another constant, we can upper bound the summation of the variance terms (C.11) as
2′
K H V Vπk
v u P h⋆( N·|s kk h (,a sk h k) ,hakh )+1,P 1⋆,Φ i ≤C 3 min H,ρ −1 ·H2SAKι+min H,ρ −1 3 ·H2SAι2. (C.13)
k=1h=1u h h h ∨ q
XXt (cid:8) (cid:9) (cid:8) (cid:9)
where C >0 is also an absolute constant.
3
Step 3: finishing the proof. With (C.10) and(C.13), it suffices to controlthe remainingterms. For the
summationofthe martingaledifferenceterms,notice thatbythe definitions in(C.7)and(C.8),bothζk and
h
ξk are bounded by min H,ρ 1 according to (4.4) and Lemma C.2 (optimism and pessimism). As a result,
h { − }
using Azuma-Hoeffding inequality, with probability at least 1 δ
−
K H
(ζk +ξk) C min H,ρ 1 √HKι,
h h ≤ 4 · − ·
k=1h=1
XX (cid:8) (cid:9)
where C > 0 is an absolute constant. For the summation of the inverse of the count function in (C.10), it
4
suffices to invoke again Lemma C.13. Combining all together, with probability at least 1 3δ, we have
−
3
RegretΦ(K) ≤C
5
·
min H,ρ −1 ·H2SAKι2+min H,ρ −1 ·H2SAι3
(cid:18)q
(cid:8) (cid:9) (cid:8) (cid:9)
+min H,ρ 1 √HKι+H3S2Aι2+H√K
−
·
(cid:19)
(cid:8) (cid:9)
= min H,ρ 1 H2SAKι ,
O − · ′
(cid:18)q (cid:19)
(cid:8) (cid:9)
whereC >0isanabsoluteconstantandι =log2(SAHK/δ). ThiscompletestheproofofTheorem4.6.
5 ′
C.2 Key Lemmas
LemmaC.2(Optimisticandpessimisticestimationoftherobustvalues). Bysettingthebonusk as in(4.7),
h
then under the typical event , it holds that
E
Qk h(s,a) ≤Qπ h,k P⋆,Φ(s,a) ≤Q⋆ h,P⋆,Φ(s,a) ≤Qk h(s,a), Vk h(s) ≤V hπ ,Pk ⋆,Φ(s) ≤V h⋆ ,P⋆,Φ(s) ≤Vk h(s), (C.14)
for any (s,a,h,k) [H] [K].
∈S×A× ×
Proof of Lemma C.2. See Appendix C.3 for a detailed proof.
Lemma C.3 (ProperbonusforTVrobustsetsandoptimistic andpessimistic valueestimators). By setting
the bonusk as in (4.7), then under the typical event , it holds that
h E
E Pρ(s,a;Pb hk) Vk h+1 −E Pρ(s,a;P h⋆) Vk h+1 +E Pρ(s,a;P h⋆) Vk h+1 −E Pρ(s,a;Pb hk) Vk h+1 ≤2 ·bonusk h(s,a),
h i h i h i h i
Proof of Lemma C.3. See Appendix C.4 for a detailed proof.
Lemma C.4 (Control of the bonus term). Under the typical event , the bonusk in (4.7) is bounded by
E h
bonusk h(s,a) ≤v uV P h⋆( ·|s, Na) kh(V sh ,π + ak 1 ),P⋆ 1,Φ i·c 1ι + 4 ·E P h⋆( ·|s,a) hHVk h+1−Vk h+1 i + Nkc (2 sH ,a2 )Sι 1 + √1 K,
u h ∨ h ∨
t
where ι=log(S3AH2K3/2/δ) and c ,c >0 are absolute constants.
1 2
33Proof of Lemma C.4. See Appendix C.5 for a detailed proof.
Lemma C.5 (Total variance law for robustMDP with TV robust sets). With probability at least 1 δ, the
−
following inequality holds
K H
V P h⋆( ·|sk h,ak h) V hπ +k 1,P⋆,Φ ≤c 3 · min {H,ρ −1 }·HK+min {H,ρ −1 }3 ·Hι .
k X=1h X=1 h i (cid:16) (cid:17)
where ι=log(S3AH2K3/2/δ) and c >0 is an absolute constant.
3
Proof of Lemma C.5. See Appendix C.6 for a detailed proof.
C.3 Proof of Lemma C.2
Proof of Lemma C.2. WeproveLemmaC.2byinduction. Supposetheconclusion(C.14)holdsatsteph+1.
For step h, let’s first consider the robust Q function part. Specifically, by using the robust Bellman optimal
equation (Proposition 2.3) and (4.4), we have that
Q⋆ h,P⋆,Φ(s,a) −Qk h(s,a)
≤max (cid:26)E Pρ(s,a;P h⋆) hV h⋆ +1,P⋆,Φ i−E Pρ(s,a;Pb hk) hVk h+1 i−bonusk h(s,a), Q⋆ h,P⋆,Φ(s,a) −min (cid:8)H,ρ −1
(cid:9)(cid:27)
≤max (cid:26)E Pρ(s,a;P h⋆) hV h⋆ +1,P⋆,Φ i−E Pρ(s,a;Pb hk) hV h⋆ +1,P⋆,Φ i−bonusk h(s,a), 0 (cid:27), (C.15)
where the second inequality follows from the induction of V⋆ Vk at step h+1 and the fact that
h+1,P⋆,Φ ≤ h+1
Q⋆ min H,ρ 1 (by Proposition 2.7 and Assumption 4.1). By Lemma C.7, we have that
h,P⋆,Φ
≤ {
−
}
E Pρ(s,a;P h⋆) hV h⋆ +1,P⋆,Φ i−E Pρ(s,a;Pb hk) hV h⋆ +1,P⋆,Φ i≤v u uV Pb hk( ·|s, Na) hkh(V sh ,⋆ + a1 ),P ∨⋆ 1,Φ i·c 1ι + N hk(c s2 ,H a)ι ∨1 + √1 K,
t
Now by further applying Lemma C.11 to the variance term in the above inequality, we can obtain that
E Pρ(s,a;P h⋆) V h⋆ +1,P⋆,Φ −E Pρ(s,a;Pb hk) V h⋆ +1,P⋆,Φ
h i h i
≤v
u(cid:16)V Pb hk( ·|s,a) h(cid:16)Vk h+1+Vk h+1 (cid:17)/ N2 ik+ (s,4 aH
)
·E 1Pb hk( ·|s,a) hVk h+1−Vk h+1 i(cid:17)·c 1ι
+
Nk(c s2 ,H a)ι
1
+
√1
K
u h ∨ h ∨
t
≤v
uV Pb hk( ·|s,a) h(cid:16)NV kk h (+ s1 ,a+ )Vk h 1+1 (cid:17)/2 i·c 1ι
+v
uE Pb hk( ·|s,a) hV Nk h k+ (s1 ,− a)Vk h 1+1 i·4Hc 1ι
+
Nk(c s2 ,H a)ι
1
+
√1
K
u h ∨ u h ∨ h ∨
t t
≤v
uV Pb hk( ·|s,a) h(cid:16)NV kk h (+ s1 ,a+ )Vk h 1+1 (cid:17)/2 i·c 1ι
+
E Pb hk( ·|s,a) hV Hk h+1−Vk h+1
i +
Nkc (′2 sH ,a2 )ι
1 +
√1
K, (C.16)
u h ∨ h ∨
t
where the first inequality is due to Lemma C.11, the secondinequality is due to √a+b √a+√b, and the
≤
last inequality is from √ab a+b where c > 0 is an absolute constant. Therefore, combining (C.15) and
≤
′2
(C.16), and the choice of bonusk(s,a) in (4.7), we can conclude that
h
Q⋆ (s,a) Qk (s,a).
h,P⋆,Φ ≤ h
Furthermore,it holds that Qπk (s,a) Q⋆ (s,a). Thus it reduces to proveQk(s,a) Qπk (s,a).
h,P⋆,Φ
≤
h,P⋆,Φ
h ≤
h,P⋆,Φ
Again, by using the robust Bellman equation (Proposition 2.2) and (4.5), we have that
Qk(s,a) Qπk (s,a)
h −
h,P⋆,Φ
34≤max (cid:26)E Pρ(s,a;Pb hk) hVk h+1 i−E Pρ(s,a;P h⋆) hV hπ +k 1,P⋆,Φ i−bonusk h(s,a), 0 −Qπ h,k P⋆,Φ(s,a)
(cid:27)
≤max (cid:26)E Pρ(s,a;Pb hk) hV hπ +k 1,P⋆,Φ i−E Pρ(s,a;P h⋆) hV hπ +k 1,P⋆,Φ i−bonusk h(s,a), 0 (cid:27), (C.17)
where the second inequality follows from the induction of Vk Vπk at step h+1 and the fact that
h+1 ≤ h+1,P⋆,Φ
Qπk
0. By Lemma C.8, we have that
h,P⋆,Φ
≥
E Pρ(s,a;Pb hk) V hπ +k 1,P⋆,Φ −E Pρ(s,a;P h⋆) V hπ +k 1,P⋆,Φ
h i h i
≤v
uV Pb hk( ·|s, Na) kh(V sh ,⋆ + a1 ),P⋆ 1,Φ i·c 1ι
+
E Pb hk( ·|s,a) hV Hk h+1−Vk h+1
i +
Nkc (′2 sH ,a2 )Sι
1 +
√1
K.
u h ∨ h ∨
t
Now by applyingLemma C.11 to the varianceterm, with anargumentsimilar to (C.16), we canobtainthat
E Pρ(s,a;Pb hk) V hπ +k 1,P⋆,Φ −E Pρ(s,a;P h⋆) V hπ +k 1,P⋆,Φ (C.18)
h i h i
≤v
uV Pb hk( ·|s,a) h(cid:16)NV kk h (+ s1 ,a+ )Vk h 1+1 (cid:17)/2 i·c 1ι
+
2E Pb hk( ·|s,a) hV Hk h+1−Vk h+1
i +
Nkc (′2 s′H ,a2 )ι
1 +
√1
K,
u h ∨ h ∨
t
Thus by combining (C.17) and (C.18), and the choice of bonusk(s,a) in (4.7), we can conclude that
h
Qk(s,a) Qπk (s,a).
h ≤
h,P⋆,Φ
Therefore, we have proved that at step h, it holds that
Qk h(s,a) ≤Qπ h,k P⋆,Φ(s,a) ≤Q⋆ h,P⋆,Φ(s,a) ≤Qk h(s,a).
FinallyfortherobustV functionpart,considerthatbyrobustBellmanequation(Proposition2.2)and(4.6),
Vk(s)=E Qk(s, ) E Qπk (s, ) =Vπk (s),
h π hk( ·|s) h · ≤ π hk( ·|s) h,P⋆,Φ · h,P⋆,Φ
h i h i
and that by robust Bellman optimal equation (Proposition 2.3), the choice of πk, and (4.6),
V⋆ (s)=maxQ⋆ (s,a) maxQk (s,a)=Vk (s),
h,P⋆,Φ a h,P⋆,Φ ≤ a h h
∈A ∈A
which proves that
Vk h(s) ≤V hπ ,Pk ⋆,Φ(s) ≤V h⋆ ,P⋆,Φ(s) ≤Vk h(s).
Sincetheconclusion(C.14)holdsfortheV functionpartatstepH+1,aninductionprovesLemmaC.2.
C.4 Proof of Lemma C.3
Proof of Lemma C.3. We upper bound the differences by a concentration inequality Lemma C.9,
E
Pρ(s,a;Pb hk)
Vk
h+1
−E
Pρ(s,a;P h⋆)
Vk
h+1
+E
Pρ(s,a;Pb hk)
Vk
h+1
−E
Pρ(s,a;P h⋆)
Vk
h+1
h i h i h i h i
≤2v
uV Pb hk( ·|s, Na) kh(V sh ,⋆ + a1 ),P⋆ 1,Φ i·c 1ι
+
2 ·E Pb hk( ·|s,a) hHVk h+1−Vk h+1
i
+
N2 kc (′2 sH ,a2 )Sι
1
+
√2
K, (C.19)
u h ∨ h ∨
t
35where c ,c >0 areabsoluteconstants. ThenapplyingLemma C.11to the variancetermin (C.19), withan
1 ′2
argument the same as (C.16) in the proof of Lemma C.2, we can obtain that
E
Pρ(s,a;Pb hk)
Vk
h+1
−E
Pρ(s,a;P h⋆)
Vk
h+1
+E
Pρ(s,a;Pb hk)
Vk
h+1
−E
Pρ(s,a;P h⋆)
Vk
h+1
h i h i h i h i
≤2v
uV Pb hk( ·|s,a) h(cid:16)NV kk h (+ s1 ,a+ )Vk h 1+1 (cid:17)/2 i·c 1ι
+
4 ·E Pb hk( ·|s,a) hHVk h+1−Vk h+1
i
+
Nk2 (c s′2′ ,H a)2ι
1
+
√2
K.
u h ∨ h ∨
t
Therefore, by looking into the choice of bonusk(s,a) in (4.7), we can conclude that
h
E Pρ(s,a;Pb hk) Vk h+1 −E Pρ(s,a;P h⋆) Vk h+1 +E Pρ(s,a;Pb hk) Vk h+1 −E Pρ(s,a;P h⋆) Vk h+1 ≤2 ·bonusk h(s,a),
h i h i h i h i
This finishes the proof of Lemma C.3.
C.5 Proof of Lemma C.4
Proof of Lemma C.4. Recall that the bonusk(s,a) is defined as
h
bonusk h(s,a)=v uV Pb hk( ·|s,a) h(cid:16)NV kk h (+ s1 ,a+ )Vk h 1+1 (cid:17)/2 i·c 1ι + 2E Pb hk( ·|s,a) hV Hk h+1−Vk h+1
i
+ Nkc (2 sH ,a2S )ι
1
+ √1 K.
u h ∨ h ∨
t
The main thing we need to consider is to control the first term and the second term. We first deal with the
second term of bonusk(s,a) by invoking Lemma C.10, which gives
h
2E Pb hk( ·|s,a) hV Hk h+1−Vk h+1 i ≤ H2 + H2 2 ·E P h⋆( ·|s,a) Vk h+1−Vk h+1 + Nkc (′2 sH ,aS )ι 1
(cid:18) (cid:19) h i h ∨
3E
P h⋆( ·|s,a)
Vk h+1−Vk
h+1
+
c ′2HSι
, (C.20)
≤ hH i Nk(s,a) 1
h ∨
wherethe secondinequalityis fromH 2. Thenwedealwith the firstterm(varianceterm)ofbonusk(s,a)
≥ h
by invoking Lemma C.12, which gives
v
uV Pb hk( ·|s,a) h(cid:16)NV kk h (+ s1 ,a+ )Vk h 1+1 (cid:17)/2 i·c 1ι
(C.21)
u h ∨
t
≤v
u(cid:16)V P h⋆( ·|s,a) hV hπ +k 1,P⋆,Φ i+4H ·E P h⋆( N·|s k,a (s) h,V a)k h+1−Vk h+1 i+ Nc hk′ 2′ (H s,4 aS ) ∨ι 1 +1 (cid:17)·c 1ι
u h
t
≤v
uV P h⋆( ·|s, Na) kh(V sh ,π + ak 1 ),P⋆ 1,Φ i·c 1ι
+v
u4H ·E P h⋆( ·|s N,a) kh(V s,k h a+ )1− 1Vk h+1 i·c 1ι
+
Nkc (1 sc ,′2′ aS )H2 1ι
+
Nk(sc ,1 aι
) 1
u h ∨ u h ∨ ph ∨ r h ∨
t t
≤v
uV P h⋆( ·|s, Na) kh(V sh ,π + ak 1 ),P⋆ 1,Φ i·c ′1ι
+
E P h⋆( ·|s,a) hV Hk h+1−Vk h+1
i +
4c 1 N+ k(s,c a1c )′2′S 1H2ι
u h ∨ (cid:0) hp ∨(cid:1)
t
Thusbycombining(C.20)and(C.21)withthechoiceofbonusk,wecanconcludetheproofofLemmaC.4.
h
C.6 Proof of Lemma C.5
Proof of Lemma C.5. Thekeyideaistorelatethevisitationdistribution(w.r.t. P⋆)andthevariance(w.r.t.
P⋆)tothevaluefunctionofπk,afterwhichwecanderiveanupperboundforthetotalvariance. Throughout
this proof, we use the shorthand that
H =min H,ρ 1 .
−
(cid:8) (cid:9)
36AccordingtoProposition2.7andAssumption4.1,foranypolicyπ andanysteph,therobustvaluefunction
of π holds that
m saxV hπ ,P⋆,Φ(s) ≤H, (C.22)
∈S
which we usually apply in the sequel. Also, to facilitate our analysis, we define
Tk( s,a)= argmin E Vπk , (s,a,h) [H],
h ·| P( ·) ∈Ph(s,a;P h⋆) P( ·)
h
h+1,P⋆,Φ
i
∀ ∈S×A×
e
and set Tk = Tk H , which is the most adversarialtransition for the true robust value function of πk.
{ h}h=1
Now consider the following decomposition of our target,
e e
K H
V P⋆( sk,ak) V hπ +k 1,P⋆,Φ
h ·| h h
Xk=1h X=1 h i
K H H
= V P h⋆( ·|sk h,ak h) V hπ +k 1,P⋆,Φ −E (sk h,ak h) ∼(P⋆,πk)
"
V P h⋆( ·|sk h,ak h) V hπ +k 1,P⋆,Φ
#
Xk=1h X=1 h i h X=1 h i
K H
+ E V Vπk
(sk,ak) (P⋆,πk) P⋆( sk,ak) h+1,P⋆,Φ
h h ∼ " h ·| h h #
Xk=1 h X=1 h i
K H H
= V Vπk E V Vπk
P h⋆( ·|sk h,ak h) h+1,P⋆,Φ − (sk h,ak h) ∼(P⋆,πk) " P h⋆( ·|sk h,ak h) h+1,P⋆,Φ #
Xk=1h X=1 h i h X=1 h i
Term (i): martingale difference term
| K H {z }
+ E (sk,ak) (Tek,πk) V Tek( sk,ak) V hπ +k 1,P⋆,Φ
h h ∼ " h ·| h h #
Xk=1 h X=1 h i
Term (ii): total variance law
|K H{z } H
+ E (sk h,ak h) ∼(P⋆,πk)
"
V P h⋆( ·|sk h,ak h) V hπ +k 1,P⋆,Φ #−E (sk h,ak h) ∼(Tek,πk)
"
V Te hk( ·|sk h,ak h) V hπ +k 1,P⋆,Φ #.
Xk=1 h X=1 h i h X=1 h i
Term (iii): error from P⋆ to Tk
| {z }
e
In the sequel, we upper bound each of the three terms respectively.
Term (i): martingale differenceterm. Thisisasummationofmartingaledifferenceterm(withrespect
tofiltration =σ( (sτ,aτ) )). ByAzuma-Hoeffding’sinequality,withprobabilityatleast1 δ,
Gk { h h }(h,τ) ∈[H] ×[k] −
Term (i) c H H2 √Kι, (C.23)
≤ · · ·
2
where c>0 is anabsolute constant. We haveutilized the fact of(C.22)to obtainthe upper bound HH on
each martingale difference term in the summation.
Term (ii): total variance law. The upper bound of this term is the core part of the analysis, for which
we summarize it in the following lemma.
Lemma C.6 (Totalvariancelaw). Under the same setup as Theorem 4.6, given any deterministic policy π,
define that
T ( s,a)= argmin E Vπ , (s,a,h) [H], (C.24)
h
·| P( ·) ∈Pρ(s,a;P h⋆)
P( ·)
h
h+1,P⋆,Φ
i
∀ ∈S×A×
e
37and set T = T H . Then we have
{ h }h=1
H
e e E (sh,ah) ∼(Te ,π)
"
V Te h( ·|sh,ah) V hπ +1,P⋆,Φ #≤2H ·H.
h X=1 h i
We defer the proofof Lemma C.6 to Appendix C.7. With Lemma C.6, we consider taking policy π =πk
for k [K] therein (which are deterministic policies), and obtain that the Term (ii) is upper bounded by
∈
Term (ii) 2H H K. (C.25)
≤ · ·
Term (iii): error from P⋆ to Tk. We first relate the visitation distribution under P⋆ to that under Tk.
On the one hand, by the choice of the adversarialtransition kernel Tk, it holds that
h
e e
D P⋆( s,a) Tk( s,a) ρ, (s,a,h) [H]. (C.26)
TV h ·| h ·| ≤ ∀ e ∈S×A×
(cid:16) (cid:13) (cid:17)
On the other hand, by (C.22), we can u(cid:13) (cid:13)pp eer bound the variance term by
V P h⋆( ·|s,a) V hπ +k 1,P⋆,Φ ≤H2 , ∀(s,a,h) ∈S×A×[H]. (C.27)
h i
Therefore, by combining (C.26) and (C.27), we can conclude that
H
E (sk,ak) (P⋆,πk) V P⋆( sk,ak) V hπ +k 1,P⋆,Φ
h h ∼ h " ·| h h #
h X=1 h i
H
≤E (sk h,ak h) ∼(Tek,πk)
"
V P h⋆( ·|sk h,ak h) V hπ +k 1,P⋆,Φ
#
h X=1 h i
+H
·
(s,a,h)sup [H]D TV Tk h( ·|s,a) T hk( ·|s,a)
·
(s,a,h)sup [H]V P h⋆( ·|s,a) V hπ +k 1,P⋆,Φ
∈S×A× (cid:16) (cid:13) (cid:17) ∈S×A× h i
≤E (sk h,ak h) ∼(Tek,πk)
"
H V P h⋆( ·|sk h,ak h) V hπ +k 1,P⋆(cid:13) (cid:13) ,Φe #+ρH ·H2 . (C.28)
h X=1 h i
We then relate the variance term under P⋆ to that under Tk. Specifically, we have
2 2
V P h⋆( ·|s,a) V hπ +k 1,P⋆,Φ =E P h⋆( ·|s,a) V hπ +k 1,P⋆,Φ −eE P h⋆( ·|s,a) V hπ +k 1,P⋆,Φ
h i h(cid:16) (cid:17)i2 (cid:16) h i2(cid:17)
≤E Te hk( ·|s,a) V hπ +k 1,P⋆,Φ − E Te hk( ·|s,a) V hπ +k 1(s ′)
h(cid:16) (cid:17)i (cid:16) h i(cid:17) 2
+2
·
(s,as )u ∈p S×AD
TV
(cid:16)P h⋆( ·|s,a) (cid:13)T hk( ·|s,a)
(cid:17)·
(cid:18)m
s′
∈a SxV hπ +1,P⋆,Φ(s ′)
(cid:19)
≤V Te hk( ·|s,a) V hπ +k 1,P⋆,Φ +2ρ ·H2 , (cid:13) (cid:13) ∀e(s,a,h) ∈S×A×[H], (C.29)
h i
wherethe lastinequalityfollowsfromthe definitionofTk and(C.22). Combining(C.28)and(C.29),wecan
upper bound Term (iii) by the following,
e
Term (iii) (C.30)
K H H
= E (sk h,ak h) ∼(P⋆,πk)
"
V P h⋆( ·|sk h,ak h) V hπ +k 1,P⋆,Φ #−E (sk h,ak h) ∼(Tek,πk)
"
V P h⋆( ·|sk h,ak h) V hπ +k 1,P⋆,Φ
#
Xk=1 h X=1 h i h X=1 h i
K H H
+ E (sk h,ak h) ∼(Tek,πk)
"
V P h⋆( ·|sk h,ak h) V hπ +k 1,P⋆,Φ #−E (sk h,ak h) ∼(Tek,πk)
"
V Te hk( ·|sk h,ak h) V hπ +k 1,P⋆,Φ
#
Xk=1 h X=1 h i h X=1 h i
2
3ρH H K 3H H K,
≤ · · ≤ · ·
where in the last inequality we use the fact that for any ρ [0,1], it holds that
∈
ρH =ρ min H,ρ 1 =min ρH,1 1.
−
· ≤
(cid:8) (cid:9) (cid:8) (cid:9)
38Finishingtheproof. Finally,combiningtheupperboundsforTerms(i),(ii),and(iii),i.e.,(C.23),(C.25),
and (C.30), we conclude that with probability at least 1 δ, it holds that
−
K H
V P h⋆( ·|sk h,ak h) V hπ +k 1,P⋆,Φ ≤c ·H ·H2 ·√Kι+2H ·H ·K+3H ·H ·K
Xk=1h X=1 h i
3
c H H K+c H H ι,
′ ′′
≤ · · · · · ·
whereinthe lastinequalityweuse√ab a+bforanya,b>0. Pluginthe notationthatH =min H,ρ 1
−
≤ { }
and finish the proof of Lemma C.5.
C.7 Proof of Lemma C.6
Proof of Lemma C.6. Using the property of variance, we have that for any (s ,a ) ,
h h
∈S×A
V Te h( ·|sh,ah) V hπ +1,P⋆,Φ =E Te h( ·|sh,ah) V hπ +1,P⋆,Φ 2 − E Te h( ·|sh,ah) V hπ +1,P⋆,Φ 2 . (C.31)
h i h(cid:0) (cid:1) i (cid:16) h i(cid:17)
By robust Bellman equation (Proposition 2.2) and the definition of T in (C.24), we have that
h
V hπ ,P⋆,Φ(s h)=R h(s h,π h(s h))+E Te h( ·|sh,πh(she )) V hπ +1,P⋆,Φ . (C.32)
h i
Therefore, by (C.31) and (C.32), we have that
V Te h( ·|sh,πh(sh)) V hπ +1,P⋆,Φ =E Te h( ·|sh,πh(sh)) V hπ +1,P⋆,Φ 2 − V hπ ,P⋆,Φ(s h) −R h(s h,π h(s h)) 2 . (C.33)
h i h(cid:0) (cid:1) i (cid:16) (cid:17)
For the second term in (C.33), we can calculate it as
Vπ (s ) R (s ,π (s )) 2 = Vπ 2 (s )+2 Vπ (s ) R (s ,π (s )) R2(s ,π (s ))
− h,P⋆,Φ h − h h h h − h,P⋆,Φ h · h,P⋆,Φ h · h h h h − h h h h
(cid:16) (cid:17) (cid:0)Vπ (cid:1)2 (s )+2H, (C.34)
≤−
h,P⋆,Φ h
where the last inequality utilizes the fact(cid:0) s that 0 (cid:1) R (s ,π (s )) 1, R2(s ,π (s )) 0, and (C.22) that
≤ h h h h ≤ h h h h ≥
Vπ (s ) H. Combining (C.33) and (C.34), we have that
h,P⋆,Φ h
≤
V Te h( ·|sh,πh(sh)) V hπ +1,P⋆,Φ ≤E Te h( ·|sh,πh(sh)) V hπ +1,P⋆,Φ 2 − V hπ ,P⋆,Φ 2 (s h)+2H,
h i h(cid:0) (cid:1) i (cid:0) (cid:1)
which further implies that
E (sh,ah) ∼(Te ,π) (cid:20)V Te h( ·|sh,ah) hV hπ +1,P⋆,Φ i(cid:21)=E sh∼(Te ,π) (cid:20)V Te h( ·|sh,πh(sh)) hV hπ +1,P⋆,Φ
i(cid:21)
=≤ EE ss hh +∼ 1( ∼Te (,π Te) ,π(cid:20) )E Te h V( · h| πs +h 1,π ,Ph ⋆(s ,Φh)) 2h(cid:0) −V hπ E+ s1 h,P ∼⋆ (, TeΦ ,π(cid:1) )2 i−
V
h(cid:0)
π
,V Ph ⋆π , ,P Φ⋆, 2Φ (cid:1) +2 + 2H2H
.
(cid:21)
h(cid:0) (cid:1) i h(cid:0) (cid:1) i
Taking summation over h [H] gives that
∈
H
E (sh,ah) ∼(Te ,π),h ∈[H]
"
V Te h( ·|sh,ah) V hπ +1,P⋆,Φ #≤2H ·H =2H ·min H,ρ −1 ,
h X=1 h i
(cid:8) (cid:9)
which concludes the proof of Lemma C.6.
39C.8 Other Technical Lemmas
Before presenting all lemmas, we recall that the typical event is defined as
E
E = 
(cid:12)
(cid:12)(cid:16)E P h⋆( ·|s,a) −E Pb hk( ·|s,a) (cid:17)h(cid:0)η −V h⋆ +1,P⋆,Φ (cid:1)+
i(cid:12)
(cid:12)≤v u
u
tV Pb hk( ·|s,a) h(cid:0)Nη hk− (sV ,h a⋆ + )1 ∨,P 1⋆,Φ (cid:1)+ i·c 1ι + N hk(c s2 ,H a)ι ∨1,
(cid:12) (cid:12)

min P⋆(s s,a),Pk(s s,a) c ι
P h⋆(s ′ |s,a) −P h(s ′ |s,a) ≤v u n h ′ N| k(s,a)h 1′| o· 1 + Nk(sc ,2 aι ) 1,
(cid:12) (cid:12) u h b ∨ h ∨
(cid:12) b (cid:12) t
(cid:12) (cid:12)
(s,a,s,h,k) [H] [K], η [0,H] , ι=log S3AH2K3/2/δ . (C.35)
∀ ′ ∈S×A×S× × ∀ ∈N1/(S√K) 

(cid:0) (cid:1) (cid:0) (cid:1)
where c 1,c
2
>0 are two absolute constants, N1/S√K([0,H]) denotes an 1/S√K-cover of the interval [0,H].
C.8.1 Concentration Inequalities
Lemma C.7 (Bernstein bound for TV robust sets and the optimal robust value function). Under event
E
in (C.35), it holds that
(cid:12)
(cid:12)E Pρ(s,a;Pb hk) hV h⋆ +1,P⋆,Φ i−E Pρ(s,a;P h⋆) hV h⋆ +1,P⋆,Φ
i(cid:12)
(cid:12)≤v u
u
tV Pb hk( ·|s, Na) hkh(V sh ,⋆ + a1 ),P ∨⋆ 1,Φ i·c 1ι + N hk(c s2 ,H a)ι ∨1 + √1 K,
wh(cid:12) (cid:12)ere ι=log(S3AH2K3/2/δ). (cid:12) (cid:12)
Proof of Lemma C.7. By our definition of the operator E Pρ(s,a;Pb hk)[V h⋆ +1,P⋆,Φ] in (4.3), we can arrive that
(cid:12)E Pρ(s,a;Pb hk) V h⋆ +1,P⋆,Φ −E Pρ(s,a;P h⋆) V h⋆ +1,P⋆,Φ
(cid:12)
(cid:12) h i h i(cid:12)
(cid:12) (cid:12) ρ
(cid:12) = (cid:12) (cid:12) (cid:12)η ∈s [u 0p ,H](cid:26)−E Pb hk( ·|s,a) h(cid:0)η −V h⋆ +1,P⋆,Φ (cid:1)+ i(cid:12)+ (cid:16)1 − 2 (cid:17)·η (cid:27)
(cid:12) ρ
(cid:12) − η ∈s [u 0p ,H](cid:26)−E P h⋆( ·|s,a) h(cid:0)η −V h⋆ +1,P⋆,Φ (cid:1)+ i+ (cid:16)1 − 2 (cid:17)·η (cid:27)(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤ ηs [u 0p ,H]((cid:12) E Pb hk( ·|s,a)−E P h⋆( ·|s,a) η −V h⋆ +1,P⋆,Φ + (cid:12)), (cid:12) (C.36)
∈ (cid:12)(cid:16) (cid:17)h(cid:0) (cid:1) i(cid:12)
Now according to the first inequality(cid:12) of event , we have that (cid:12)
(cid:12) (cid:12)
E
(cid:12)
(cid:12)(cid:16)E P h⋆( ·|s,a) −E Pb hk( ·|s,a) (cid:17)h(cid:0)η −V h⋆ +1,P⋆,Φ (cid:1)+
i(cid:12)
(cid:12)≤v u
u
tV Pb hk( ·|s,a) h(cid:0)Nη hk− (sV ,h a⋆ + )1 ∨,P 1⋆,Φ (cid:1)+ i·c 1ι + N hk(c s2 ,H a)ι ∨1
(cid:12) (cid:12)
(cid:12) (cid:12)
≤v
uV Pb hk( ·|s, Na) kh(V sh ,⋆ + a1 ),P⋆ 1,Φ i·c 1ι
+
Nk(c s2 ,H a)ι
1,
u h ∨ h ∨
t
for any η ([0,H]). Here the second inequality is because Var[(a X) ] Var[X]. Therefore,by
∈N1/(S√K) − + ≤
a covering argument, for any η [0,H], it holds that
∈
(cid:12)
(cid:12)(cid:16)E P h⋆( ·|s,a) −E Pb hk( ·|s,a) (cid:17)h(cid:0)η −V h⋆ +1,P⋆,Φ (cid:1)+
i(cid:12)
(cid:12)≤v u
u
tV Pb hk( ·|s, Na) hkh(V sh ,⋆ + a1 ),P ∨⋆ 1,Φ i·c 1ι + N hk(c s2 ,H a)ι ∨1 + √1 K.
This(cid:12) finishes the proof of Lemma C.7. (cid:12)
(cid:12) (cid:12)
40Lemma C.8 (Bernstein bound for TV robust sets and the robust value function of πk). Under event in
E
(C.35), suppose that the optimism and pessimism (C.14) holds at (h+1,k), then it holds that
(cid:12)E Pρ(s,a;Pb hk) V hπ +k 1,P⋆,Φ −E Pρ(s,a;P h⋆) V hπ +k 1,P⋆,Φ
(cid:12)
(cid:12) h i h i(cid:12)
(cid:12) (cid:12)
≤v
uV Pb hk( ·|s, Na) kh(V sh ,⋆ + a1 ),P⋆ 1,Φ i·c 1ι
+
E Pb hk( ·|s,a) h(cid:12) (cid:12)V Hk h+1−Vk h+1
i +
Nkc (′2 sH ,a2S )ι
1 +
√1
K,
u h ∨ h ∨
t
where ι=log(S3AH2K3/2/δ) and c , c are absolute constants.
1 ′2
Proof of Lemma C.8. By our definition of the operator E [Vπk ] in (4.3), we can arrive that,
Pρ(s,a;P) h+1,P⋆,Φ
(cid:12)E Pρ(s,a;Pb hk) V hπ +k 1,P⋆,Φ −E Pρ(s,a;P h⋆) V hπ +k 1,P⋆,Φ
(cid:12)
(cid:12) h i h i(cid:12)
(cid:12) (cid:12) = (cid:12) (cid:12) (cid:12)η ∈s [u 0p ,H](cid:26)−E Pb hk( ·|s,a) h(cid:0)η −V hπ +k 1,P⋆,Φ (cid:1)+ i+(cid:12) (cid:12) (cid:16)1 − ρ 2 (cid:17)·η (cid:27)
(cid:12) (cid:12) − η ∈s [u 0p ,H](cid:26)−E P h⋆( ·|s,a) h(cid:0)η −V hπ +k 1,P⋆,Φ (cid:1)+ i+ (cid:16)1 − ρ 2 (cid:17)·η (cid:27)(cid:12)
(cid:12)
(cid:12)
≤ ηs [u 0p
,H]((cid:12)
E Pb hk( ·|s,a)−E P h⋆( ·|s,a) η −V hπ +k 1,P⋆,Φ +
(cid:12))
(cid:12) (cid:12)
∈ (cid:12)(cid:16) (cid:17)h(cid:0) (cid:1) i(cid:12)
(cid:12) (cid:12)
≤ ηs [u 0p ,H]((cid:12)(cid:12) E Pb hk( ·|s,a)−E P h⋆( ·|s,a) η −V h⋆ +1,P⋆,Φ + (cid:12)(cid:12) )
∈ (cid:12)(cid:16) (cid:17)h(cid:0) (cid:1) i(cid:12)
(cid:12) (cid:12)
(cid:12) Term (i) (cid:12)
| {z }
+ ηs [u 0p
,H]((cid:12)
E Pb hk( ·|s,a)−E P h⋆( ·|s,a) η −V hπ +k 1,P⋆,Φ +− η −V h⋆ +1,P⋆,Φ + (cid:12)),
∈ (cid:12)(cid:16) (cid:17)h(cid:0) (cid:1) (cid:0) (cid:1) i(cid:12)
(cid:12) (cid:12)
(cid:12) Term (ii) (cid:12)
We deal with Term (i|) and Term (ii) resepctively. For Ter{mz (i), this is exactly the same as the}right hand
side of (C.36). Therefore, applying the same argument as Lemma C.7 gives the following upper bound,
Term (i) ≤v
uV Pb hk( ·|s, Na) kh(V sh ,⋆ + a1 ),P⋆ 1,Φ i·c 1ι
+
Nk(c s2 ,H a)ι
1
+
√1
K. (C.37)
u h ∨ h ∨
t
For Term (ii), we first apply the second inequality of event to obtain that,
E
Term (ii) (C.38)
Pk(s s,a) c ι c ι
≤
η
∈s [u 0p
,H]
s X′
∈Ss
bNh hk(′ s|
,a)
∨· 11 +
N
hk(s,2
a) ∨1·
(cid:12)
(cid:12)(cid:0)η −V hπ +k 1,P⋆,Φ(s ′)
(cid:1)+−
(cid:0)η −V h⋆ +1,P⋆,Φ(s ′)
(cid:1)+
(cid:12)
(cid:12)
.
  (cid:12) (cid:12)
By the assumption that (C.14) holds at (h+1,k), we can upper bound the absolute value above by 
η Vπk (s) η V⋆ (s) Vπk (s) V⋆ (s)
−
h+1,P⋆,Φ ′
+− −
h+1,P⋆,Φ ′
+ ≤
h+1,P⋆,Φ ′
−
h+1,P⋆,Φ ′
(cid:12) (cid:12)(cid:0) (cid:1) (cid:0) (cid:1) (cid:12) (cid:12) V(cid:12) (cid:12) k (s) Vk (s). (cid:12) (cid:12) (C.39)
(cid:12) (cid:12)≤(cid:12) h+1 ′ − h+1 ′ (cid:12)
where the firstinequality is due to the 1-Lipschitzcontinuity ofψ (x)=(η x) ,andthe secondinequality
η +
−
is due to (C.14). Thus combining (C.38) and (C.39), we know that
Term (ii)
≤
sP Nhk( ks (′ s|s ,, aa )) ·c 11ι + Nk(sc ,2 aι
) 1·
Vk h+1(s ′) −Vk h+1(s ′) . (C.40)
s X′ ∈S b h ∨ h ∨ (cid:16) (cid:17)
 
41Now following the argument first identified by Azar et al. (2017), we proceed to upper bound (C.40) as
Term (ii)
≤
s X′
∈S
P bhk(s H′ |s,a) +
N
hk(c s1 ,H a)ι
∨1
+
N
hk(sc ,2 aι
) ∨1!·
(cid:16)Vk h+1(s ′) −Vk h+1(s ′)
(cid:17)
E
Pb hk( ·|s,a)
Vk h+1−Vk
h+1
+
c ′2H2Sι
, (C.41)
≤ h H i Nk(s,a) 1
h ∨
where c >0 is another absolute constant. The first inequality is by √ab a+b and the second inequality
′2
≤
is due to Vk ,Vk [0,H]. Finally, combining (C.37) and (C.41), we prove Lemma C.8.
h+1 h+1 ∈
Lemma C.9 (Bernsteinbounds forTVrobustsetsandoptimistic andpessimistic robustvalueestimators).
Under event in (C.35), suppose that the optimism and pessimism (C.14) holds at (h+1,k), it holds that
E
max (cid:26)(cid:12)E Pρ(s,a;Pb hk) hVk h+1 i−E Pρ(s,a;P h⋆) hVk h+1 i(cid:12), (cid:12)E Pρ(s,a;Pb hk) hVk h+1 i−E Pρ(s,a;P h⋆) hVk h+1
i(cid:12)(cid:27)
≤(cid:12) (cid:12)
v
uV Pb hk( ·|s, Na) kh(V sh ,⋆ + a1 ),P⋆ 1,Φ i·c 1ι
+
E Pb hk( ·|s,a(cid:12) (cid:12) ) h(cid:12) (cid:12)V Hk h+1−Vk h+1
i +
Nkc (′2 sH ,a2S )ι
1 +
√1
K,
(cid:12) (cid:12)
u h ∨ h ∨
t
where ι=log(S3AH2K3/2/δ) and c ,c are absolute constants.
1 ′2
Proof of Lemma C.9. This follows from the same proof as Lemma C.8 and is thus omitted.
Lemma C.10 (Non-robust concentration). Under event in (C.35), suppose that the optimism and pes-
E
simism (C.14) holds at (h+1,k), then it holds that
(cid:12)
(cid:12)(cid:16)E Pb hk( ·|s,a)−E P h⋆( ·|s,a) (cid:17)hVk h+1−Vk h+1
i(cid:12)
(cid:12)≤ H1 ·E P h⋆( ·|s,a) hVk h+1−Vk h+1 i+ N hkc (′2 sH ,a2S ) ∨ι 1.
(cid:12) (cid:12)
where ι=(cid:12)log(S2AH2K3/2/δ) and c is an absolut(cid:12)e constant.
′2
Proof of Lemma C.10. According to the second inequality of event , we have that
E
(cid:12)
E
Pb hk(
·|s,a)−E
P h⋆( ·|s,a)
Vk h+1−Vk
h+1
(cid:12)
(cid:12)(cid:16) (cid:17)h i(cid:12)
(cid:12) (cid:12)
≤
s X′
∈S
sP Nh⋆( hks (′| ss ,, aa )) ∨·c 11ι +
N
hk(sc ,2(cid:12) (cid:12) aι
) ∨1!·
(cid:16)Vk h+1(s ′) −Vk h+1(s ′) (cid:17),
where we also apply (C.14) that Vk (s) Vk (s). Now using the same argumentas (C.41) in the proof
h+1 ′ ≥ h+1 ′
of Lemma C.8, we can arrive at
(cid:12)
(cid:12)(cid:16)E Pb hk( ·|s,a)−E P h⋆( ·|s,a) (cid:17)hVk h+1−Vk h+1
i(cid:12)
(cid:12)≤ E P h⋆( ·|s,a) hVk h+1 H(s ′) −Vk h+1(s ′) i + N hkc (′2 sH ,a2S ) ∨ι 1,
(cid:12) (cid:12)
which fi(cid:12)nishes the proof of Lemma C.10. (cid:12)
C.8.2 Variance Analysis
Lemma C.11 (Variance analysis 1). Suppose that the optimism and pessimism (C.14) holds at (h+1,k),
then the following inequality holds,
(cid:12)V Pb hk( ·|s,a) Vk h+1+Vk h+1 /2 −V Pb hk( ·|s,a) V h⋆ +1,P⋆,Φ (cid:12)≤4H ·E Pb hk( ·|s,a) Vk h+1−Vk h+1 .
(cid:12) h(cid:16) (cid:17) i h i(cid:12) h i
(cid:12) (cid:12)
(cid:12) (cid:12)
42Proof of Lemma C.11. Directly consider that the left hand side can be upper bounded by the following,
(cid:12)V Pb hk( ·|s,a) Vk h+1+Vk h+1 /2 −V Pb hk( ·|s,a) V h⋆ +1,P⋆,Φ
(cid:12)
(cid:12) h(cid:16) (cid:17) i h i(cid:12)
(cid:12) (cid:12) ≤(cid:12) (cid:12)E Pb hk( ·|s,a) (cid:20)(cid:16)Vk h+1+Vk h+1 (cid:17)2 /4 (cid:21)−E Pb hk( ·|s,a) (cid:20)(cid:16)V(cid:12) (cid:12) h⋆ +1,P⋆,Φ (cid:17)2 (cid:21)(cid:12)
(cid:12)
(cid:12) (cid:12) (cid:12) +
(cid:12)
E Pb hk( ·|s,a) Vk h+1+Vk h+1 /2 2 − E Pb hk( ·|s,a) V h⋆ +1,P(cid:12) (cid:12) (cid:12)⋆,Φ 2 (cid:12). (C.42)
(cid:12)(cid:16) h(cid:16) (cid:17) i(cid:17) (cid:16) h i(cid:17) (cid:12)
Since all ofVk ,Vk ,V⋆ (cid:12) (cid:12) [0,H](by the correctnessof (C.14) and the definitions(cid:12) (cid:12) of Vk ,Vk ),
h+1 h+1 h+1,P⋆,Φ ∈ h+1 h+1
we can further upper bound the right hand side of (C.42) as
(cid:12) (cid:12) (cid:12)
(cid:12)V Pb hk( ·|s,a) h(cid:16)Vk h+1+Vk h+1 (cid:17)/2 i−V Pb hk( ·|s,a) hV h⋆ +1,P⋆,Φ
i(cid:12) (cid:12) (cid:12)
(cid:12)≤4H ·E Pb hk( ·|s,a)
(cid:20)(cid:12) (cid:12)
(cid:12)(cid:16)Vk h+1+Vk h+1 (cid:17)/2 −V h⋆ +1,P⋆,Φ
(cid:12) (cid:12) (cid:12)(cid:21)
≤4H ·E Pb hk( ·|s,a) Vk h+1−Vk h+1 ,
h i
where the last inequality is due to the correctness of (C.14) at (h+1,k). This proves Lemma C.11.
Lemma C.12 (Varianceanalysis2). Under event in (C.35), suppose that optimism and pessimism (C.14)
E
holds at (h+1,k), then it holds that
(cid:12)V Pb hk( ·|s,a) Vk h+1+Vk h+1 /2 −V P h⋆( ·|s,a) V hπ +k 1,P⋆,Φ (cid:12)≤4H ·E P h⋆( ·|s,a) Vk h+1−Vk h+1 + Nc ′2 hkH (s4 ,S aι ) +1.
(cid:12) h(cid:16) (cid:17) i h i(cid:12) h i
(cid:12) (cid:12)
Pr(cid:12)oof of Lemma C.12. We first relate the variance on P(cid:12)k to the variance on P⋆. Specifically, we have
h h
(cid:12)V Pb hk( ·|s,a) Vk h+1+Vk h+1 /2 −V P h⋆( ·|s,a)b Vk h+1+Vk h+1 /2
(cid:12)
(cid:12) h(cid:16) (cid:17) i h(cid:16) (cid:17) i(cid:12)
(cid:12) (cid:12) 2
(cid:12) = (cid:12)
(cid:12)
(cid:12)E Pb hk( ·|s,a) "(cid:18)(cid:16)Vk h+1+Vk h+1 (cid:17)/2 −E Pb hk( ·|s,a) h(cid:16)Vk h+1+V(cid:12)k h+1 (cid:17)/2
i(cid:19)
#(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) 2
(cid:12) + (cid:12)
(cid:12)
(cid:12)E P h⋆( ·|s,a) "(cid:18)(cid:16)Vk h+1+Vk h+1 (cid:17)/2 −E P h⋆( ·|s,a) h(cid:16)Vk h+1+Vk h+1 (cid:17)/2 i(cid:19)(cid:12) #(cid:12)
(cid:12)
(cid:12). (C.43)
(cid:12) (cid:12)
Since (Vk +Vk )/2 (cid:12) (cid:12) [0,H], we can further upper bound (C.43) by (cid:12) (cid:12)
h+1 h+1 ∈
(cid:12)V Pb hk( ·|s,a) Vk h+1+Vk h+1 /2 −V P h⋆( ·|s,a) Vk h+1+Vk h+1 /2
(cid:12)
(cid:12) h(cid:16) (cid:17) i h(cid:16) (cid:17) i(cid:12)
(cid:12)
(cid:12)
≤H2
·
P h⋆(s′ |s,a) −P h(s′ |s,a) (cid:12)
(cid:12)
s X′ ∈S(cid:12)
(cid:12) (cid:12) P⋆( s,a) b c ι
(cid:12)
(cid:12) (cid:12) c ι
H2 h ·| · 1 + 2
≤ ·
s′
s N hk(s,a) ∨1 N hk(s,a) ∨1!
X∈S
c Sι c Sι
H2 1 + 2
≤ · sN hk(s,a) ∨1 N hk(s,a) ∨1!
c H4Sι
1+
′2
, (C.44)
≤ Nk(s,a) 1
h ∨
wherethesecondinequalityisbythesecondinequalityinevent ,thethirdinequalityisbyCauchy-Schwartz
E
inequality and the probability distribution sums up to 1, and the last inequality is from √ab a+b. Thus
≤
43by (C.44), we can bound our target as
(cid:12)V Pb hk( ·|s,a) Vk h+1+Vk h+1 /2 −V P h⋆( ·|s,a) V hπ +k 1,P⋆,Φ
(cid:12)
(cid:12) (cid:12) (cid:12) ≤ Vh P(cid:16) h⋆( ·|s,a) Vk h+1+(cid:17) Vi k h+1 /2 −Vh P h⋆( ·|s,a) V hπi +k(cid:12) (cid:12) (cid:12)1,P⋆,Φ + Nkc (′2 sH ,a4S )ι 1 +1. (C.45)
(cid:12) (cid:12) h(cid:16) (cid:17) i h i(cid:12) (cid:12) h ∨
(cid:12) (cid:12)
Now by the same pr(cid:12)oof of Lemma C.11, using the correctness of (C.14) at(cid:12)(h+1,k), we can show that
V P h⋆( ·|s,a) Vk h+1+Vk h+1 /2 −V P h⋆( ·|s,a) V hπ +k 1,P⋆,Φ ≤4H ·E P h⋆( ·|s,a) Vk h+1−Vk h+1 . (C.46)
(cid:12) (cid:12)
(cid:12) h(cid:16) (cid:17) i h i(cid:12) h i
(cid:12) (cid:12)
Combinin(cid:12)g (C.45) and (C.46), we can finish the proof of Lemma(cid:12)C.12.
C.8.3 Other Auxiliary Lemmas
Lemma C.13 (Lemma 7.5 in Agarwalet al. (2019)). For the sequences of sk,ak H,K , it holds that
{ h h}h,k=1
K H
1
c HSAlog(K).
Nk(sk,ak) 1 ≤ ·
k=1h=1 h h h ∨
XX
where c>0 is an absolute constant.
Proof of Lemma C.13. See Lemma 7.5 in Agarwalet al. (2019) for a detailed proof.
D Proofs for Extensions in Section 4.4
In this section, we prove the theoretical results in Section 4.4.
D.1 Proof of Corollary 4.8
Proof of Corollary 4.8. WeconsiderapplyingAlgorithm1ontheauxiliary -rectangularRMDP witha
S×A
TVrobustset (seeSection4.4)whichsatisfiesthevanishingminimalvalueassumption(Assumption4.1).
M
Suppose the algorithm outputs π1, ,πK for the K episodes. Then Theorem 4.6 shows that by a proper
···
choice of the hyfperparameters, with probability at least 1 δ
−
e e
K
Regret Φe(K)= m πeaxV 1πe ,Pe⋆,Φe(s 1) −V 1πe ,Pk e⋆,Φe(s 1)
≤O
min H,ρ −1 H2(S+1)AKι
′
,. (D.1)
Xk=1 (cid:18)q
(cid:8) (cid:9)
(cid:19)
where ι =log2(SAHK/δ) and ρ=2 2ρ [0,1). In the sequel, we prove that for any policy π of and
′ ′
− ∈ M
its induced policy π of , their robust value functions coincide at the initial state s , that is,
γ 1
S M ∈S
e f
e
V
1πe
,Pe⋆,Φe(s 1)=V
1πe
,PS ⋆,Φ′(s 1),
where Vπe is the robust value function of π in =( , ,H,P⋆,R,Φ), and Vπe S is the robust value
1,Pe⋆,Φe
M S A
1,P⋆,Φ′
function of π in
γ
=( , ,H,P⋆,R γ,Φ′). To this end, we actually prove a stronger result that for any
step h [H],S it hoM lds thatS A e f e f e e
∈
e
(ρ ′)h −1 ·V hπe ,Pe⋆,Φe(s)=V hπe ,S P⋆,Φ′(s), ∀s ∈S. (D.2)
We prove(D.2)by induction. For stepH,by robustBellmanequation,we havethat, forany(s,a) ,
∈S×A
H 1
(ρ ′)H −1 ·Qπ He ,Pe⋆,Φe(s,a)=(ρ ′)H −1
·
ργ − ·R H(s,a)=R γ,H(s,a)=Qπ He S ,P⋆,Φ′(s,a),
(cid:18) ′(cid:19)
44and thus for any s ,
∈S
(ρ ′)H −1 ·V hπe ,Pe⋆,Φe(s)=E πe( ·|s) (ρ ′)H −1 ·Qπ He ,Pe⋆,Φe(s, ·) =E πe S( ·|s) Q Hπe S ,P⋆,Φ(s, ·) =V Hπe ,S P⋆,Φ′(s).
h i h i
This proves (D.2) for step H. Suppose that (D.2) holds at some step h+1, that is,
(ρ′)h ·V hπe +1,Pe⋆,Φe(s)=V hπe +S 1,P⋆,Φ′(s), ∀s ∈S. (D.3)
Then for step h, by robust Bellman equation and Proposition 4.2, we have that
(ρ′)h −1 ·Qπ he ,Pe⋆,Φe(s,a)=(ρ′)h −1 ·
(cid:18)ργ ′(cid:19)H −1
·R h(s,a)+(ρ′)h −1 ·E Pe ρ(s,a;Pe h⋆) hV hπe +1,Pe⋆,Φe
i
=R γ,h(s,a)+(ρ ′)h −1 ·ρ ′ ·E Be ρ(s,a;Pe h⋆) V hπe +1,Pe⋆,Φe , (D.4)
h i
where the last equality utilizes Proposition 4.2 since min
s
eV
hπe
+1,Pe⋆,Φe(s)=0, and we adopt the notation
∈S
P(s) 1
(s,a;P⋆)= P() ∆( ): sup ′ .
Bρ h ( · ∈ S s′ ∈SeP h⋆( es ′|s,a) ≤ ρ ′)
e e e e
Notice that by the definition (4.10), we know for (s,a) eit holds that P⋆( s,a)=P⋆( s,a) which is
∈S×A h ·| h ·|
supported on . Therefore, we can equivalently write
S
e
P(s) 1
(s,a;P⋆)= P() ∆( ): sup ′
Bρ h ( · ∈ S s′
∈S
P h⋆( es ′|s,a) ≤ ρ ′)
e e e e P(s) 1
= P() ∆( ): sup e ′
( · ∈ S s′
∈S
P h⋆( es ′|s,a) ≤ ρ ′)
= e(s,a;P⋆). (D.5)
Bρ h
Thus by (D.4) and (D.5) and the induction hypothesis (D.3), we obtain that for any (s,a) ,
∈S×A
(ρ ′)h −1 ·Qπ he ,Pe⋆,Φe(s,a)=R γ,h(s,a)+(ρ ′)h ·E Bρ(s,a;P h⋆) V hπe +1,Pe⋆,Φe
=R γ,h(s,a)+E Bρ(s,a;P h⋆) V hπe +S 1,Ph ⋆,Φ =Q hπei ,S P⋆,Φ(s,a),
h i
wherethesecondequalityapplies(D.3)andthelastequalityisfromrobustBellmanequation. Consequently,
for any s , we have that
∈S
(ρ ′)h −1 ·V hπe ,Pe⋆,Φe(s)=E πe( ·|s) (ρ ′)h −1 ·Qπ he ,Pe⋆,Φe(s, ·) =E πe S( ·|s) Qπ he ,S P⋆,Φ(s, ·) =V hπe ,S P⋆,Φ′(s),
h i h i
whichfinishes the inductionargument,provingourclaim(D.2). Bytakingh=1,wecanderivethatforany
initial state s , it holds that for any policy π of and its induced policy π of ,
1 γ
∈S M S M
V
1πe
,Pe⋆,Φe( es 1)=fV
1πe
,PS ⋆,Φ′(s 1).
e
This indicates two facts: the first is that
m πeaxV 1πe ,Pe⋆,Φe(s 1)=m πaxV 1π ,P⋆,Φ′(s 1), (D.6)
where on the right hand side the maximization is with respect to all the policies for ; the second is that
γ
M
V
1πe ,Pk
e⋆,Φe(s 1)=V
1πe ,PSk
⋆,Φ′(s 1), (D.7)
45for each k [K], where recall that πk is the policy output by Algorithm 1 for episode k. As a result, the k
∈
policies πk K of during interactive data collection satisfies with probability at least 1 δ,
{ S}k=1 Mγ −
e
K
e RegretΦ′(K)= m πaxV 1π ,P⋆,Φ′(s 1) −V 1πe ,PSk ⋆,Φ′(s 1)
k=1
X
K
=
maxVπe
(s )
Vπek
(s )
πe
1,Pe⋆,Φe 1
−
1,Pe⋆,Φe 1
k=1
X
min H,(2 2ρ) 1 H2SAKι ,
≤O − ′ − ′
(cid:18)q (cid:19)
(cid:8) (cid:9)
where in the second equality we apply the facts (D.6) and (D.7), and the last inequality follows from (D.1)
and that ρ=2 2ρ. This completes the proof of Corollary 4.8.
′
−
46