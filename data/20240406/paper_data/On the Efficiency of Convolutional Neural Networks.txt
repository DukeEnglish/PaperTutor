On the Efficiency of
Convolutional Neural Networks
Andrew Lavin
April 4, 2024
(a). (b).
IIIdddeeeaaalllLLLaaattteeennncccyyy[[[mmmsss]]]
000 111000 222000 333000 444000 555000
EfficientNet-B5 ConvNeXt-S BC loo cn kv -FF uir ss iot n C Ino dn uv cN toe rXt Effi Ic nie dn ut cN toe rt
ConvFirst-S
83 T B4 T
82
N
B3 N
81
P
80
P
79
F
0.0 2.5 5.0 7.5 10.0 12.5 15.00 100 200 300 400
MACs[billions] ActualLatency[ms]
(c). (d).
3355%% 88%%
5555%%
83 4455%% 3311%% 77%%
82 4455%%
2299%% 66%%
81
2244%%
80 4477%% 66%%
79 55%%
1199%%
0 100 200 300 400 101 102
IdealandActualLatency[ms] IdealandActualLatency[ms]
Efficiencygapplotscompareidealandactuallatencyandquantifythedifferenceascomputa-
tionalefficiency.(a).Dividingarithmeticcomplexity(multiply-accumulatesorMACs)bythe
peakarithmeticthroughputoftheGPUyieldsideallatency.(b).InferencetimeontheGPUisthe
actuallatency.(c).TheideallatencyofConvNeXtislongerthantheactuallatencyofConvFirst.
(d).Lowcomputationalefficiencycausesawideefficiencygap.EfficientNetrangesfrom5%–8%
andhasawidegap.OurConvFirstmodelwithblock-fusionkernelsrangesfrom47%–55%and
hasanarrowgap.WeusedanNVIDIAA5000GPUwith float16 andbatchsize128.
AndrewLavin:PhantomAI.andrew@phantom.ai
4202
rpA
4
]GL.sc[
1v71630.4042:viXra
]%[ycaruccAteNegamI
]%[ycaruccAteNegamIAbstract
Since the breakthrough performance of AlexNet in 2012, convolutional neu-
ralnetworks(convnets)havegrownintoextremelypowerfulvisionmodels.Deep
learning researchers have used convnets to produce accurate results that were
unachievableadecadeago.Yetcomputerscientistsmakecomputationalefficiency
theirprimaryobjective.Accuracywithexorbitantcostisnotacceptable;analgo-
rithmmustalsominimizeitscomputationalrequirements.Confrontedwiththe
dauntingcomputationthatconvnetsuse,deeplearningresearchersalsobecame
interestedinefficiency.Researchersappliedtremendousefforttofindtheconvnet
architecturesthathavethegreatestefficiency.Theyreplacedexpensiveconv2dlay-
erswithasequenceofdegeneratelayersthathavelessarithmeticcomplexity.They
usednetworkarchitecturesearchtofindthemodelhyperparameters—number
ofchannelsperlayer,convolutionkernelsize,bottleneckexpansionratios,etc.—
thatyieldthegreatestaccuracywhileusingthefewestoperations.However,the
engineerswhodeployedtheseefficientconvnetssoonrealizedthattheywereslower
than the previous generation, despite using fewer operations. Many reverted to
oldermodelswithfasterresponsetimes(i.e.,lowerlatency).Henceresearchers
switchedtheobjectiveoftheirsearchfromarithmeticcomplexitytolatencyand
producedanewwaveofmodelsthatperformedbetter.Paradoxically,thesemodels
alsousedmoreoperations.Skepticismgrewamongresearchersandengineersalike
abouttherelevanceofarithmeticcomplexity.
Contrarytotheprevailingviewthatlatencyandarithmeticcomplexityareir-
reconcilable,asimpleformularelatesboththroughcomputationalefficiency.This
insightenabledustoco-optimizetheseparatefactorsthatdeterminelatency.Weob-
servedthatthedegenerateconv2dlayersthatproducethebestaccuracy–complexity
trade-offalsohavelowoperationalintensity.Therefore,kernelsthatimplement
theselayersusesignificantmemoryresources.Wesolvedthisoptimizationprob-
lemwithblock-fusionkernelsthatimplementalllayersofaresidualblock,thereby
creatingtemporallocality,avoidingcommunication,andreducingworkspacesize.
OurConvFirstmodelwithblock-fusionkernelsranapproximatelyfourtimesas
fast as the ConvNeXt baseline with PyTorch Inductor, at equal accuracy on the
ImageNet-1Kclassificationtask.
Along the journey to this result, we created novel analytical tools. We made
theefficiencygapplottoshowaccuracy,arithmeticcomplexity,latency,andcom-
putational efficiency on the same axes. The plot illustrates how computational
efficiencydistortsmodelefficiencytoproducelatency.Wealsoextendedroofline
analysistoasequenceofparallelkernels.Theresultingwaterlineanalysisisasimpleperformancemodelthatmeasurestheeffectofmemory-boundnetworklayerson
modellatency.Weevaluatedtheaccuracyofthemediantoperationalintensityas
aperformanceindicatoranddiscovereditslimitations.Wealsodevisedtheten-
sormachine,asimpleabstractcomputerfortensorprograms,andusedittoplan
block-fusionkernels.Tensormachinesexpressthehigh-levelalgorithmsthatcreate
memory-efficiencyandavoidthelow-leveldetailsthatimpedecomprehension.Our
unifiedapproachtoconvnetefficiencyenvisionsaneweraofmodelsandkernels
thatachievegreateraccuracyatlowercost.Contents
1 Introduction 2
2 RelatedWork 3
3 EfficiencyGap:HowComputationalEfficiencyDistortsModelEfficiency 5
3.1 MeasuringEfficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.2 VisualizingEfficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4 Waterline:ASimplePerformanceModelforaSequenceofParallelKernels 11
4.1 WaterlineAnalysisofConv2dLayers . . . . . . . . . . . . . . . . . . . . 12
4.1.1 Conv2danditsDegenerateCases . . . . . . . . . . . . . . . . . 12
4.1.2 OperationalIntensityofConv2d . . . . . . . . . . . . . . . . . . 14
4.1.3 DegenerateConv2dandtheop:byteWaterline . . . . . . . . . . 17
4.2 WaterlineAnalysisofConvnetModels . . . . . . . . . . . . . . . . . . . 19
4.3 MediantOperationalIntensity . . . . . . . . . . . . . . . . . . . . . . . 20
5 BlockFusion:AnOptimizationforDegenerateConv2dLayers 23
5.1 TensorMachines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
5.2 Feed-ForwardNetwork . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
5.3 ConvFirstBlock . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
5.3.1 Block-FusionKernelforConvFirst . . . . . . . . . . . . . . . . . 27
5.3.2 WaterlineAnalysisofConvFirst . . . . . . . . . . . . . . . . . . 30
5.4 MBConvBlock . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
5.4.1 Block-FusionKernelforMBConv . . . . . . . . . . . . . . . . . . 32
5.4.2 WaterlineAnalysisofMBConv . . . . . . . . . . . . . . . . . . . 36
5.5 CUDAKernels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
5.5.1 ConvFirstCUDAKernel . . . . . . . . . . . . . . . . . . . . . . . 37
5.5.2 MBConvCUDAKernel . . . . . . . . . . . . . . . . . . . . . . . 39
5.5.3 KernelBenchmarks . . . . . . . . . . . . . . . . . . . . . . . . . 40
5.6 Depth-FirstExecution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
6 ConvFirstNet:StrivingforComputationalEfficiencyandModelEfficiency 44
6.1 TheDesignofConvFirstNet . . . . . . . . . . . . . . . . . . . . . . . . . 45
6.2 WaterlineAnalysisofConvFirstNet . . . . . . . . . . . . . . . . . . . . 46
16.3 ConvFirstNetModelExperiments . . . . . . . . . . . . . . . . . . . . . 49
6.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
7 Conclusion 52
A ConvFirstNetSpeedProjections 59
B Amdahl’sRoofline 65
1. Introduction
Theconv2dlayeristhebasicbuildingblockofconvolutionalneuralnetworks(convnets).
Itaccountsformostofaconvnet’scomputation,sonaturallyithasbeenthefocusof
effortstomakeconvnetsmoreefficient.Ontheonehand,computerscientistsimproved
thecomputationalefficiencyoftheconv2dlayerbyexaminingitsarithmeticcomplexity
anddatamovementandapplyingthisunderstandingtocreatemoreefficientalgorithms
andhardware.Ontheotherhand,deeplearningresearchersreducedthearithmetic
complexityofthetraditionalconv2dlayerbyfactoringitintoasequenceofdegenerate
layersthatperformfewertotaloperations.
Computer scientists improved the speed of first-wave convnets, while machine
learning researchers created a second wave that used fewer operations to achieve
the same accuracy. In other words, the original convnets benefited from improved
computationalefficiency whilethenewoneshadgreatermodelefficiency.Inpractice,the
second-wavemodelssufferedfromsignificantlyworsecomputationalefficiencythat
underminedtheirsuperiormodelefficiency.
Deep learning researchers responded in different ways to the efficiency paradox
posed by the second wave. Some employed network architecture search and found
the convnet models with the lowest latency on one or more software and hardware
platforms.Othersexploitedcorrelationsbetweenmodelhyperparametersandcompu-
tationalefficiencyanddiscoveredthatconvnetscouldbemadefastersimplybymaking
themwider(i.e.,increasingthenumberofchannelsineverylayer).Someoptimized
hardwarearchitectureandmemoryplacementalgorithmstomakenewprocessorsthat
ranexistingsoftwaremoreefficiently.
Theresultoftheseeffortswasathirdwaveofconvnetsandacceleratorsthatwere
suboptimalinvariousways.Despitetheirimprovedcomputationalefficiency,third-wave
convnets often had worse model efficiency than second-wave convnets. Third-wave
2accelerators performed with greater computational efficiency on all models, while
devotingmuchlargerdieareatoon-chipmemory(SRAM).Theseacceleratorshadless
peakarithmeticthroughputpermillimeterthanonesthatuselessSRAM,effectively
increasingthecostofanarithmeticoperation.
Wetakeanewapproachthatelevatestheimportanceofcomputation.Wefindthe
simple formula that relates model efficiency, computational efficiency, and latency,
andplotallofthesequantitiesonthesameaxes,therebyrevealingtheefficiencygap
thatexplainstheconvnetefficiencyproblem.Weextendrooflineanalysistocalculate
theattainablelatencyofasequenceofparallelkernels;andwedevelopsimpleplots
thatrevealthelatencyandoperationalintensityofindividuallayers.Weuseasimple
abstracttensormachinetoexpressnewalgorithmsanduseittodesignmemory-efficient
block-fusionkernelsthatgreatlyincreasetheattainableefficiency.Wecontributeanew
convnetthatisco-optimizedformodelefficiencyandcomputationalefficiency,train
it,andbenchmarkitwithCUDAkernelsthatimplementourblock-fusionalgorithms.
Theresultsshowgreatermodelefficiencythanoursecond-wavebaselineandgreater
computationalefficiencythanthethird-wavebaseline.Thecombinationofefficient
modelandkernelsyieldssignificantlylowerlatencyatequallevelsofaccuracy.
Wehopeourapproachcontributestoanewwaveofconvnets,software,andhardware
thatachievessignificantlygreaterefficiency.
2. Related Work
AlexNet [37] began the modern era of convnets with its breakthrough performance
on the ImageNet [13] image classification benchmark. Since then, researchers have
publishedaseriesofinnovativemodels,introducingnewlayersthatimprovedmodel
efficiency.Network-in-Networkusedpoint-wise(1×1)convolutions[44].ResNetcon-
tributed the residual block, enabling deeper models while reducing the number of
operations per block [24]. Squeeze & Excitation networks introduced a lightweight,
channel-wiseattentionmechanismthatincreasedaccuracywhileaddingfewadditional
operations[28].MobileNetV2introducedtheinvertedresidualblockwithdepth-wise
convolutionsandamemory-efficientalgorithmforcomputingit[59].EfficientNetcom-
bined all of these ideas into a scalable model that shattered the state-of-the-art for
modelefficiencyoverawiderangeofscales[63].
Earlyconvnetsusedconv2dlayerswith3×3kernelsorlarger[42,37,60,61,...],
andearlyworksoncomputationalefficiencyfocusedontheselayers.Astraightforward
3implementationachieved96%computationalefficiencyonanNVIDIAGPU,becauseof
thelayer’slargeoperationalintensity[39].Subsequentworksfocusedonfastalgorithms
thatreducedthearithmeticcomplexityoftheconvnetblock[47,68,8],culminatingin
GPUkernelsexceeding100%effectivecomputationalefficiency[41].
Recentconvnetsusedpoint-wise[44]anddepth-wise[7,27]convolutions.Thesede-
generateconv2dlayershavelessarithmeticcomplexityandincreasedmodelefficiency.
Simultaneously,deeplearningacceleratorsbegantousematrixmultiplyaccelerators
that dramatically increased the peak arithmetic throughput [32, 53]. Decreasing op-
erational intensity of the layers and increasing op:byte ratio of the accelerators put
tremendouspressureonthesoftwarethatrunstheconvnets.TheresultwasthatEffi-
cientNet[63]hadpoorcomputationalefficiencyonacontemporaryGPUusingthebest
availablesoftware.
Severalpapersimprovedtheperformanceofmodernconvnetsbyusinginference
latencyasthemodelcostinsteadofarithmeticcomplexity[65,26,14,6,64,...].These
papersmeasuredlatencyusingpopulardeeplearningframeworksandinferenceen-
gineswithoutmodification.
Yang et al. optimized the data-flow, blocking, and memory hierarchy of convnet
acceleratorhardware[74].Zhangetal.co-optimizedthesizeofmatrixmultiplyacceler-
ators,on-chipmemorycapacity,andthememoryplacementofweightsandactivations
tensors[75].Boththeseworksassumedlayer-wiseexecution,andbothincreasedthe
on-chipmemorycapacitytoavoidstoringactivationstoDRAMbetweenlayers.
Radosavovic et al. [56] observed that the number of activations correlates with
inference latency and searched for models with fewer activations. Dollar et al. [14]
usedascalingrulethatcreateswidermodels,motivatedbytheobservationsthatwider
modelshavemoreoperationsperactivationandhencegreatercomputationalefficiency.
Otherworkscomputedasmallnumberoftilesinonelayerbeforecomputingthe
nextlayer’stilesthatdependonthem[1,19].Thisstrategycreatedadepth-firstexecution
oftheconvnetthatreducedthesizeoftheworkspaceneededtoholdactivations,making
itpossibletokeepactivationsinasmallon-chipmemory.Depth-firstexecutionhaslong
beenusedtoexploitspatiallocalityinimageprocessing[57]andstencilprograms[17].
Itcannottraversealayerthatperformsaglobalspatialreduction,suchasSqueeze&
Excitation [28]; nor does it exploit the channels dimension of a convnet. Depth-first
executioncomplementsourblock-fusionstrategy(seeSection5).
PyTorch2addedtheTorchInductorcompilerbackendthatperformstriviallayer
fusionsandacceleratesinference[3].WeusedTorchInductorasourbaselineinference
4engine.
Ourrecentwork[40,38]developedGPUkernelsthatcomputeallthelayersofthe
FusedMBConv [21] or MBConv [59, 63] block in a single kernel. These block-fusion
kernelsexploittemporallocalitytoreduceworkspacesizeandavoidDRAMmemory
transfers.Thispaperextendsthoseresults.
Followingthesuccessofthetransformerarchitectureforsequencemodeling[69],
severalworksproducedvisiontransformers[15,10,45,67,11,...].Visiontransformers
werescaledtoverylargemodelswithhighaccuracy.Smithetal.[62]foundthatconvnets
scaleaswellasvisiontransformerswithasimilartrainingbudget.Thefocusofthispaper
isconvnets,butthemethodologywedevelopedappliesequallywelltotransformers
andconvnet-transformerhybrids.
3. Efficiency Gap: How Computational Efficiency Distorts Model
Efficiency
Inphysicalscienceafirstessentialstepinthedirectionoflearningany
subjectistofindprinciplesofnumericalreckoningandmethodsfor
practicablymeasuringsomequalityconnectedwithit.Ioftensaythatwhen
youcanmeasurewhatyouarespeakingaboutandexpressitinnumbersyou
knowsomethingaboutit;butwhenyoucannotmeasureit,whenyoucannot
expressitinnumbers,yourknowledgeisofameagreandunsatisfactory
kind:itmaybethebeginningofknowledge,butyouhavescarcely,inyour
thoughts,advancedtothestageofscience,whateverthemattermaybe.
WilliamThomson,1stBaronKelvin,1883[34]
Inordertounderstandefficiency,wemustfirstlearnhowtomeasureit.Inthissec-
tionwedefinevariousmeasuresofefficiency.Somewillalreadybeknowntothereader,
othersareborrowedfromthefieldofhigh-performancecomputing,andoneisnovel.
Whereoften-usedmetricswerenotpreciselynamedinpreviousliterature,ournomen-
clature makes deliberate and unambiguous choices. Together our metrics describe
therelationshipbetweenmodelandalgorithmandillustratehoweachcontributesto
performance.
Afterwehavemeasuredthedifferentkindsofefficiency,wewilldevelopanewkind
ofplottovisualizetheirrelationships.
53.1. MeasuringEfficiency
Modelefficiency E (n)measuresaccuracyasafunctionofthenumberofmathematical
m
operations n the model performs. This metric expresses the trade-off between the
quality of the result (the accuracy) and the quantity of work (computation). Thus it
measurestheefficiencywithwhichthemodelachievesthedesiredresult.Theaccuracy
ofamodelshouldincreaseasitscalestoalargernumberofoperations,soweassume
thatthemodelefficiencyfunctionismonotonicallyincreasing.
Modelefficiencyisnotboundedbythephysicsofcomputation.Itsaysnothingabout
therateatwhichcomputationcanbeperformed,soitcannotpredictlatency.
Thereisnotheoryofmachinelearningthatpredictstheaccuracyofatrainedmodel.
Thusmodelefficiencyis,fornow,apurelyempiricalquantity.Onemeasuresmodel
efficiencybyinstantiatingafamilyofmodelsatdifferentscales(n),trainingeachmodel
instanceonthedesiredtask,andmeasuringtheachievedaccuracy.
Peak arithmetic throughput R measures the fastest rate at which a processor can
i
performcomputation.Itrepresentstheidealarithmeticthroughput.Hardwaremanu-
facturersreportpeakarithmeticthroughputinunitsofoperationspersecond(OP/sor
OPS).TheunitTOPSindicatesonetrillion(1012)operationspersecond.Onemultiply
andadd(i.e.,onemultiply-accumulateorMAC)isconsideredtobe2OPs.Almostall
ofthecomputationperformedbyneuralnetworksismultiply-addoperations,sothis
definitionofpeakarithmeticthroughputisusefulforunderstandingidealperformance.
Ideallatency
n
(1) t (n) =
i R
i
isthefastestpossibleresponsetimeanymodelwithnoperationscouldtheoretically
achieveonaprocessorwithpeakarithmeticthroughputR .
i
Idealefficiency
n
(2) E ( ) = E (n)
i R m
i
isthegreatestpossibleaccuracyamodelcanachieveforagivenlatency.
Actualarithmeticthroughput
n
(3) R (n) =
a t (n)
a
6istherateatwhichtheprocessorandsoftwareperformthemodel’soperations,where
t (n) is the actual latency measured for the model with n operations. Achieving high
a
arithmeticthroughputisaphysicsproblem,boundedbythephysicalbehaviorofthe
processor’sarithmeticunits,memorysystems,andthewiresthatconnectthem,and
optimizedbythealgorithmthatschedulestheutilizationoftheseresources.Inprinciple
onecouldbetterorganizetheprocessorandalgorithmtoachievegreaterarithmetic
throughputforagivenmodel.
Actualefficiency
(cid:16) n (cid:17)
(4) E = E (n)
a R (n) m
a
istheaccuracythatthemodel,processor,andsoftwareachieveforagivenlatency.
CombiningEquations(2)and(4),
(cid:16) n (cid:17) (cid:16) n (cid:17)
(5) E (n) = E = E
m i R a R (n)
i a
weobservethatidealandactualefficiencyarebothhorizontaldilationsofthemodel
efficiencyfunction.
Alternatively,wecanwrite
(cid:16) t (cid:17)
(6) E (t) = E
i a C
where
R (n)
(7) C(n) = a
R
i
iscomputationalefficiency,theratioofactualtopeakarithmeticthroughput.Thuscom-
putationalefficiencyexpandstheidealefficiencyfunctiontoproduceactualefficiency.
Wecanalsowritetherelationshipintermsoflatenciesas
t (n)
(8) t (n) = i
a C(n)
Usinglogarithmicscaleweget
(9) log(cid:0) t (n)(cid:1) = log(cid:0) t (n)(cid:1)–log(cid:0)C(n)(cid:1)
a i
7sothat–log(cid:0)C(n)(cid:1)istheamountbywhichthelogarithmofthelatencyisincreased
becauseofimperfectcomputationalefficiency.Inanaccuracyversuslatencyplotusing
logarithmicscaleonthex-axis,theactualefficiencyfunctionisproducedbyshifting
theidealefficiencyfunctionby–log(cid:0)C(n)(cid:1)ateverymodelsizen.Werefertothewidth
ofthislatencyshiftastheefficiencygap.
3.2. VisualizingEfficiency
Theprevioussectionunifiedtheconceptsofmodelefficiency,latency,andcomputa-
tionalefficiency.Nowwevisualizethemwiththeefficiencygapplot.Weintroduceitby
wayofcomparisonbetweentwowellknownconvnetmodels.
EfficientNet[63]setanewstate-of-the-artinconvnetmodelefficiency,usingfewer
operationsthanexistingmodelswhileachievinghigheraccuracy.Inpractice,however,
itranslowerthanmanyoldermodelsonGPUswithavailableinferenceenginesoftware.
ConvNeXt[46]promisedtomodernizeandsimplifyconvnets;andtoprovideanew
baselineforcomparisontovisiontransformer(ViT)models[15].ConvNeXtwassoon
recognizedforitsspeed,anditservedasapracticalalternativetoEfficientNet.
Figure1.a.showsmodelefficiencyE forbothnetworks,withmodelsize(MACs)
m
onthex-axisandImageNetclassificationaccuracy(%)onthey-axis.Thistypeofplotis
wellknownfromseminalpapersaddressingtheissueofmodelefficiency,including
MobileNet [27],MobileNetV2[59],andEfficientNet[63].WeobservedthatEfficientNet
hasbettermodelefficiencythanConvNeXtbecauseitachievesgreateraccuracywith
feweroperations(MACs).
We added a second x-axis to Figure 1.a. to show ideal latency (milliseconds) and
computeditwithEquation(1).Ideallatencybuildsabridgebetweenmodelefficiency
andactuallatencyinsubsequentplots.
WeusedpeakarithmeticthroughputR = 76.7forourNVIDIAAmpereA5000GPU
i
(GA102 [52]) with base clock frequency 1.17 GHz, which is less than the boost clock
frequencyof1.695GHz.Wealsosetthememoryclockto1.25GHz,whichislessthan
the peak 2.0 GHz. The clock reductions make the GPU performance stable during
benchmarkingwhilekeepingtheop:byteratioclosetothatwhichwouldbeachieved
at full clock speed. We used a batch of 128 images, which is a typical workload for
benchmarksonlargeGPUs.
Recognizingthatarithmeticcomplexity(MACs)isanimperfectpredictoroflatency,
researchersswitchedtolatencyastheprimaryoptimizationobjective[65].Figure1.b.
plotstheaccuracyofEfficientNetandConvNeXtversustheirlatencymeasuredwith
8PyTorchInductorsoftwareontheA5000GPU.ConvNeXthasbetteractualefficiencythan
EfficientNetinthisexperimentbecauseitachievesgreateraccuracyatlowerlatency.
The dissonance between model efficiency and latency has vexed deep learning
practitioners in recent years. One might be excited by a paper that claims extraordi-
narymodelefficiencyonlytobefrustratedbythehighlatencyofthedeployedmodel.
Skepticismhasgrownabouttherelevanceofmodelefficiency.
Researchersfoundareasonableapproach:includebothmodelefficiencyandlatency
plotswhenreportingresults[12].Wegoastepfurtherbyplottingbothmetricsonthe
sameaxesandshowingtherelationshipbetweenthem.
Figure1.c.combinesFigures1.a.and1.b.byjoiningtheideallatencyaxiswiththe
actuallatencyaxis.Bothaxeshaveunitsofmilliseconds,sotheycansimplybemerged.
Themodelefficiencyandactualefficiencycurvesappearside-by-sideonthesameaxes.
For each model sample, we plot the accuracy on the y-axis and the ideal and actual
latenciesonthex-axis.Alinebetweentheidealandactuallatenciesshowsthewidthof
theefficiencygap.
Figure1.d.repeatsthesameplotusinglogarithmicscaleonthex-axis.Wecalculated
thecomputationalefficiencyusingEquation(7)foreachmodelsampleandlabeledthe
efficiencygapswiththecorrespondingcomputationalefficiency.
Figure1.d.showsthatlargedifferencesbetweenmodelefficiencyandactuallatency
are caused by low computational efficiency. EfficientNet’s computational efficiency
rangesfrom5%to8%inourexperiments,anditsefficiencygapismuchwiderthanits
modelefficiencyadvantageoverConvNeXt.Therefore,computationalefficiencycanbe
thedominantfactorcontributingtolatency.
AlsoofinterestisthefactthatConvNeXt’scomputationalefficiency,at35%orless,
isunimpressiveinabsoluteterms,despitebeingrelativelygreaterthanEfficientNet’s.
These plots help us understand the factors that contribute to the performance
difference between EfficientNet and ConvNeXt. They show that poor computational
efficiency undermines great model efficiency. They document in precise analytical
detailthesourceofthedeeplearningpractitioner’sfrustrationwithmodelefficiency.
These plots also suggest that the middle-road on which ConvNeXt travels is sub-
optimal,asneitheritsmodelefficiencynoritscomputationalefficiencyisgreat.Best
performancerequiresco-optimizationofmodelefficiencyandcomputationalefficiency.
Isitpossibletoexcelatboth?Toanswerthisquestion,wemustfirstunderstandcompu-
tationalefficiency.
9(a). (b).
IIddeeaallLLaatteennccyy[[mmss]]
00 1100 2200 3300 4400 5500
ConvNeXt
EfficientNet
ConvNeXt-S Inductor
EfficientNet-B5 Inductor
83
B4
T
82
B3
N
81
P
80 B2
79B1
F
0 2 4 6 8 10 12 14 0 50 100 150 200 250 300 350 400
MACs[billions] ActualLatency[ms]
(c). (d).
3355%%
88%%
83 77%%
3311%%
82
66%%
2299%%
81
2244%%
66%%
80
55%%
79
1199%%
0 50 100 150 200 250 300 350 400 101 102
IdealandActualLatency[ms] IdealandActualLatency[ms]
FIGURE1.Efficiencygapplotsillustratethedifferencebetweenidealandactualperformance
fordifferentmodelsandsoftware.Theyhelpusunderstandtheseparatecontributionsofmodel
efficiencyandcomputationalefficiency.ThisfigureshowsImageNet-1Kclassificationaccuracy
and latency for EfficientNet and ConvNeXt using an NVIDIA Ampere A5000 GPU with 76.7
TFLOP/speakarithmeticthroughputrunningPyTorchInductorsoftware.Batchsizeequals128.
(a). Model efficiency measures accuracy as a function of the number of multiply-accumulate
operations(MACs)performed.DividingMACsbypeakarithmeticthroughputoftheprocessor
yieldsideallatency,thelowestpossiblelatencyforthenumberofoperations.(b).Actualefficiency
measuresaccuracyversusactuallatencyforacombinationofmodelandsoftware.(c).Overlaying
modelandactualefficiencygraphsrevealstheefficiencygap,theoffsetbetweenidealandactual
latency.(d).Measuredwithalogarithmicscaleonthelatencyaxis,the(negative)widthofthe
efficiency gap equals the logarithm of computational efficiency, the ratio between actual and
idealperformance.EfficientNet’spoorcomputationalefficiencycreatesawideefficiencygap,
resultinginlongerlatencythanConvNeXt,despitesuperiormodelefficiency.
10
]%[ycaruccAteNegamI
]%[ycaruccAteNegamI4. Waterline: A Simple Performance Model for a Sequence of Parallel
Kernels
Williamsetal.[72]introducedtherooflinemodeltocapturetheideathatmodernmulti-
processors have an increasingly high ratio of peak arithmetic throughput to DRAM
memorybandwidth,andthisimbalancepresentsthefirsthurdlethataparallelkernel
mustclearinordertobecomputationallyefficient.
Wefindthattherooflinemodelexplainsmostoftheperformanceproblemswith
contemporary inference engine software and use it as the foundation for a simple
performancemodelthatpredictstheefficiencyofasequenceofparallelkernels.
WeadopttheformulationoftherooflinemodelfoundinNVIDIA’sGPUPerformance
BackgroundUser’sGuide[49].Op:bytemeasurestheratioofpeakarithmeticthroughput
toDRAMmemorybandwidthforaparticularcomputer.Operationalintensity measures
theratioofoperationsperformed(OPs)tobytestransferredbetweenDRAMandthe
processorforaparallelkernel[72].Wecountasinglemultiply-accumulateoperationas
twoOPs[51].
Iftheoperationalintensityofakernelislessthantheop:byteratioofthecomputer,
thenthekernel’sperformancewillbelimitedbytheDRAMmemorybandwidth.There-
fore,therooflinemodelsaysthatthemaximumattainablearithmeticthroughputfora
parallelkernelis
n
(10) R = min(R,B )
max
b
whereR isthepeakarithmeticthroughput,B isthepeakDRAMmemorybandwidth,
nisthenumberofoperations,bisthenumberofbytestransferred,and n istheopera-
b
tionalintensity[72].
Thisformulaimpliesthattheminimumattainablelatencyforaparallelkernelis
n
(11) tmin =
min(R,Bn)
b
Theminimumattainablelatencyforasequenceofparallelkernelsi is
(cid:88) (cid:88) n
(12) Tmin = t
i
= min(Ri
,Bn i)
i i b
i
11Theminimumattainablelatencyimpliesthemaximumcomputationalefficiency
N
(13) C = Tmin
max
R
where
(cid:88)
(14) N = n
i
i
isthetotalnumberoperationsacrossallkernels.
WevisualizeEquation(12)asasequenceofkernels,side-by-sideonatimeaxis,each
of them reaching upwards with height equal to its operational intensity. If a kernel
reaches above the processor’s op:byte waterline, then it is compute bound, and can
attainmaximumperformance.Otherwise,the“underwater”kernelismemorybound,
anditsperformanceislimitedbylowoperationalintensity.Figure3plotsthewaterline
performanceforanumberofbaselinemodels.
InAppendixBwederiveanextensiontoAmdah’sLawusingwaterlineanalysis.
4.1. WaterlineAnalysisofConv2dLayers
Convolutionalneuralnetworksarecomposedofconv2dlayers.Inthissectionweexam-
inethefunctioncomputedbyconv2dandanalyzeitsoperationalintensity.
4.1.1. Conv2danditsDegenerateCases
Theconv2dlayerisdefinedby
R S C
(cid:88)(cid:88)(cid:88)
(15) Y nhwk = A krscX n(h+r)(w+s)c +b
r s c
forinputtensorX ∈ RN×H×W×C,weightstensorA ∈ RK×R×S×C,biasvectorb ∈ RK,and
outputtensorY ∈ RN×H×W×K;whereN isthebatchsize(i.e.,thenumberofimagesto
beprocessedinparallel),C isthenumberofinputchannels,H ×W isthesizeofthe
inputandoutputfeaturemaps,K isthenumberofoutputchannels,andR×Sisthesize
oftheconvolutionkernel.Typically,X ispadded(implicitlywith R and S zeroson
2 2
⌊ ⌋ ⌊ ⌋
thetop-bottomandleft-rightborders,respectively)tosupportthereceptivefieldofthe
convolutionkernel.
12One understands Equation (15) as a convolution nested inside of a matrix multi-
plication, with matrices A ∈ RK×C and X ∈ RC×N, matrix elements w ∈ RR×S and
kc
x ∈ RH×W,andelementmultiplicationimplementedby2dconvolution[8].Conversely,
cn
itisamatrixmultiplicationnestedinsideofaconvolution,wheretheoperandsare2d
images,theimageelementsarematrices,andelementmultiplicationisimplemented
bymatrixmultiplication[8].Theseformulationslendthemselvestoarithmeticcom-
plexityreductionusingfastmatrixmultiplicationandfastconvolutionalgorithms[8,
41]. The fast algorithms are most efficient when all the tensor dimensions are large,
becausethecostofthealgorithms’transformsareamortizedoverthesizeofthetensor
dimensions[8,41].Fastalgorithmscanstillbeefficientforsmalltensordimensions
withspecializedhardwareacceleration,especiallyifthetransformsonlyuseadditions
andarithmeticshiftoperations.
Early convnets were composed of stacks of conv2d layers with kernel size equal
to 3 × 3 or larger. Conv2d layers are separated by element-wise nonlinear activation
functions(e.g.,ReLU[23,31],SiLU[16,58])andpoolinglayersthatdownsampledthe
feature maps to half resolution [42, 31], sometimes merely by subsampling [5, 24].
ResNet34wasthezenithoftheseearlyconvnets[24].TheResNetresidualblockisshown
inFigure2(a).
ResNet50contributedthebottleneckblock withpoint-wiseconvolutions;adegenerate
case of the conv2d layer with kernel size equal to 1 × 1. Setting R = S = 1 simplifies
Equation(15)to
C
(cid:88)
(16) Y = A X +b
nhwk kc nhwc
c
whichisjusttheC-mode(matrix)product(i.e.,tensormultiplication[36,p.460]inthe
channelsdimension)oftheweightsmatrixandinputtensor,equivalenttothematrix
multiplication
C
(cid:88)
(17) Y = A X +b
pk kc pc
c
where dimensions N × H × W have been unfolded into a single dimension, P. The
bottleneckblockisshowninFigure2(b).
ResNeXt[73]generalizedtheconv2doperatorsuchthattheinputandoutputtensors
aredividedintogroupsofchannels,andweightsconnectinputstooutputsinthesame
13group.Thisgroupedconvolutioncanbewrittenas
R S T
(cid:88)(cid:88)(cid:88)
(18) Y nhwk = A krstX n(h+r)(w+s)(g(k)T+t) +b
r s t
where G is the number of groups, T = C/G is the width of each group, A ∈ RK×R×S×T
istheweightstensor,andthegroupindex g(k) = kC .Groupedconvolutionscanbe
⌊KT⌋
understood as conv2d layers with block-diagonal weights tensors. Equation (15) is a
specialcaseofEquation(18)withonegroup(G = 1)andCchannelspergroup(T = C).
Figure2(c)showstheResNeXtblock.
Xception[7]andMobileNet[27]usedthedegeneratecaseofgroupedconvolutions
wherethegroupwidthequalsasinglechannel.Thisdepth-wiseconvolutionequals
R S
(cid:88)(cid:88)
(19) Y nhwk = A krsX n(h+r)(w+s) kC/K +b
⌊ ⌋
r s
andinthecommoncasewhereK = C,
R S
(cid:88)(cid:88)
(20) Y nhwc = A crsX n(h+r)(w+s)c +b
r s
MobileNetV2[59]usedaninvertedresidualblockwithdepth-wiseconvolution.The
MBConvblockispicturedinFigure2(d).
4.1.2. OperationalIntensityofConv2d
Thearithmeticcomplexityoftheconv2dlayeris
(21) n = 2(NHWK)(CRS)+K
operations.WecanunderstandthisequationascountingCRSmultiply-accumulatesfor
everyelementoftheoutputtensorandKadditionsforthebiasvector.
Inthegeneralcaseofgroupedconvolutionwithgroup-widthT,Equation(21)be-
comes
(22) n = 2(NHWK)(TRS)+K
operations,orTRSmultiply-accumulatesperoutput.
14Assumingtheconv2dkernelperformsonlycompulsoryloads [25],wereasonthatit
loadseveryelementoftheweightstensorfromDRAMexactlyonce.Thisassumptionis
validfordeviceswithcachethatislargeenoughtoholdtheweightstensorofasingle
layer,whichistrueforthemodelsanddevicesconsideredinthispaper.
WealsoassumethateveryinputelementisreadfromDRAMonceandeveryoutput
element is written to DRAM. This assumption is true if the on-chip memory is too
smalltoholdtheinputandoutputactivationtensors.Fornowweconsiderthistobea
simplifyingassumption,anditwillguideustowardsnovelmemory-efficientalgorithms
inthelatersectionsofthepaper.
TheinputtensorhasNHWCelements,theoutputtensorhasNHWK elements,the
weightstensorhasKCRS elements,andthebiasvectorhasK elements.Forgrouped
convolution, the weights tensor has KTRS elements. Therefore, the general conv2d
kerneltransfersatotalof
(23) b = NHWK +NHWC+KTRS+K
elementsbetweentheprocessorandDRAM.
WeadditionallyassumethattheinputtensorisrelativelylargewithNHW TRS.
≫
WeaccomplishthisinpracticebysettingN = 128forallsubsequentexperiments.Other
papersthatmeasurevisionnetworklatencyalsousedalargebatchsize[56,71,64,6,
...].
Largebatchsizeisagoodfitforthelargenumberofprocessorsfoundindiscrete,
desktopGPUs.Itispracticalforserver-side,batchedprocessingofimages,butnotfor
real-time vision systems. Again, this choice is a simplifying assumption that we will
revisit.
Thustheoperationalintensityofconv2dwith float16 arithmeticandtwobytesper
elementequals
n (NHWK)(TRS)+K/2
=
b NHWK +NHWC+KTRS+K
TRS
(24) ≈
1+ C + TRS
K NHW
TRS
≈ if NHW TRS
1+ C ≫
K
whereweassumedthatthenumberofinputandoutputelementsperchannel,NHW,
ismuchgreaterthanthenumberofoperationsperoutput,TRS.Thisalsoimpliesthat
15theactivationtensorsdominatetheDRAMtransfers.
Eachofthedegeneratecasesoftheconv2dlayerreducesthenumberofoperations
and the size of the weights tensor without changing the size of the input and output
tensors.Reducedarithmeticcomplexitycausestheoperationalintensitytodecrease
relativetothefullconv2dlayer.Figure2showstheevolutionofconvnetblocksandthe
operationalintensityofthetheirconv2dlayers.
AccordingtoEquation(24),thefullconv2dlayerwithT = Chasoperationalintensity
n CRS
(25) ≈
b 1+ C
K
whilepoint-wiseconvolutionhasR = S = 1andT = C,yielding
n C
(26) ≈
b 1+ C
K
anddepth-wiseconvolutionhasT = 1,giving
n RS
(27) ≈
b 1+ C
K
Thusfullconv2dhasapproximatelyRS timestheoperationalintensityofpoint-wise
convolutionandCtimestheoperationalintensityofdepth-wiseconvolution.Also,itis
worthnotingthattheoperationalintensityofgroupedanddepth-wiseconvolutionis
approximatelyindependentofthenumberofchannels,C.
As mentioned, we assume that NHW is large in subsequent analyses, which we
accomplishbysettingthebatchsizeN = 128.Inreal-timesystemsthatrequiresmall
batch size, possibly N = 1, the convnet model is typically used as a backbone for an
objectdetectionorsemanticsegmentationnetworkthatusuallyrequireshighresolution
imagesforacceptableaccuracy.Still,aninputimageassmallas512 ×512 willonly
H W
producea16 ×16 activationtensorinthelaststageofatypicalconvnetbackbone.
H W
SoitisworthexaminingthepossibilityNHW issmall.
IfNHW isapproximatelyequaltoTRS,thenbothtermscontributetotheoperational
intensityinEquation(24).Conceptually,thedatamovementincurredbytheweights
and input and output activations are all significant. In the extreme, when T = C and
NHW CRS,thenEquation(24)yields
≪
n
(28) ≈ NHW
b
16TypicallythiscasewouldapplywhenNHW C.Wecanunderstandthisequationas
≪
saying that the cost of loading the weights tensor is amortized over the pixels in the
activationtensor,ifthatnumberissmallrelativetothewidthofthenetwork.Real-time
systemsthatrequireasmallbatchsizeinordertominimizelatencymightbenefitfrom
increasing input image resolution H × W. They could also use smaller width (C) to
maintainthetotalnumberofoperations.
4.1.3. DegenerateConv2dandtheop:byteWaterline
Figure2(e)plotstheoperationalintensityofdifferenttypesconv2dlayersversusthe
number of input channels, C. For reference, it also shows the op:byte ratio of our
baselineprocessor,theNVIDIAA5000GPU.Weseethatthefullconv2dlayerwithR = S =
3hasrelativelylargeoperationalintensityevenforasmallnumberofchannels.Point-
wiseconvolutionwithbottleneckexpansionratioequaltofour(K = 4)is“underwater”
C
untilthenumberofchannelsreachesarelativelylargenumber.Groupedconvolution
with T = 32, as used by ResNeXT101_32x4d, has a flat operational intensity that is
underwater.Depth-wiseconvolutionwithR = S = 3hasoperationalintensitycloseto
zero.
Becauseon-chipSRAMhasgreaterbandwidththanoff-chipDRAM,hardwaredesign-
erscanimprovethecomputationalefficiencyofmemory-boundkernelsbyincreasing
theSRAMsizeuntilalltheactivationsfiton-chip.However,SRAMsizecontributesto
thediesizewhichdeterminescost.Akernelthatusesalargerworkspacerequiresa
greaterdiesizetoachievethesamearithmeticthroughput.Itachievesitsperformance
atagreatercostthanakernelthatusesmemoryefficiently.
17ReLU ReLU
+ +
Batchnorm Batchnorm
Conv(1x1) Conv(1x1)
ReLU
+ ReLU ReLU
Batchnorm Batchnorm Batchnorm
Conv(3x3) Conv(3x3) Conv (3x3)
32
ReLU ReLU ReLU
Batchnorm Batchnorm Batchnorm
Conv(3x3) Conv(1x1) Conv(1x1)
(a).ResNetBlock (b).ResNetBottleneckBlock (c).ResNeXtBottleneckBlock
1000 Conv(3 ×3)
+
Batchnorm 800
Conv(1x1)
Conv4(1x1)
600
ReLU
Batchnorm
400
Conv(3x3)
1
ReLU 200 NVIDIAAmpereA5000GPU
Batchnorm
Conv32(3 ×3)
0
Conv1(3 ×3)
Conv(1x1) 24 25 26 27 28 29
Channels
(d).MBConvInvertedBottleneck (e).Conv2dOperationalIntensityvs.Channels
FIGURE2.Evolutionofconvnetblocksandconvolutionallayers.(a).ResNet34usedresidual
blockswithconv2dlayers:Conv(3×3).(b).ResNet50addedbottleneckblockswithanexpansion
ratio equal to four using point-wise convolutions: Conv4(1×1). (c). ResNeXT101_32x4d used
grouped-convolutionswithgroup-widthequalto32:Conv 32(3×3).(d).MobileNetV2usedinverted
residualblockswithdepth-wiseconvolutions:Conv 1(3×3).(e).Operationalintensityofconvnet
layersasafunctionofthenumberofchannels.TheConv(3×3)layersusedbyearlyconvnets
hadlargeoperationalintensity.Point-wise,grouped,anddepth-wiseconvolutionsprogressively
decreasedtheoperationalintensityofconv2dlayers.
18
]etyb/sPO[ytisnetnIlanoitarepO4.2. WaterlineAnalysisofConvnetModels
InthissectionweuseEquation(12)toinvestigatetheminimumattainablelatencyof
differentconvnetmodels.
100
Kernels
1000 76%
Convolution GroupedConvolution 75
750 PointwiseConvolution Pool,Multiply,Cat,orNorm
DepthwiseConvolution
50
500
ResNet101(7.8GMACs) 25 250
NVIDIAAmpereA5000GPU
0 0
0 20 40 60 80 100
MinimumLatency[ms]
100
1000
75
750
50
500
30%
EfficientNet-B5(9.2GMACs) 25 250
NVIDIAAmpereA5000GPU
0 0
0 20 40 60 80 100
MinimumLatency[ms]
100
1000
75
750
43% 50
500
EfficientNetV2-Small(8.3GMACs) 25 250
NVIDIAAmpereA5000GPU
0 0
0 20 40 60 80 100
MinimumLatency[ms]
100
1000
72% 75
750
50
500
ConvNeXt-Small(288)(14.4GMACs)
25 250
NVIDIAAmpereA5000GPU
0 0
0 20 40 60 80 100
MinimumLatency[ms]
FIGURE3.Waterlineanalysisofbaselinemodels.Theseplotscomparetheoperationalintensity
ofindividuallayerswiththeop:bytewaterlineoftheNVIDIAAmpereA5000GPU.Layer-wise
kernelsareoftenmemoryboundbecausetheirlowoperationalintensitiesare“underwater.”
Memory bound kernels have higher attainable latency and lesser attainable computational
efficiency(maxefficiency).Again,weused float16 arithmeticandbatchsizeequalto128.
Figure3showstheminimumattainablelatencyandthecorrespondingmaximum
computationalefficiencyforbaselineconvnetmodels.Weplottheoperationalintensity
ofeachkernelonthey-axisandthecorrespondingminimumattainablelatencyonthe
x-axis. Kernels that rise above the processor’s op:byte waterline are compute bound,
19
etyb:po
etyb:po
etyb:po
etyb:po
]%[ycneicffiE
.xaM
]%[ycneicffiE
.xaM
]%[ycneicffiE
.xaM
]%[ycneicffiE
.xaMothersarememorybound.Wealsocomputethemaximumcomputationalefficiency
foreachnetwork(withEquation(13))andshowitnexttothewaterlineplot.
Weassumelayer-wiseexecutionofthenetwork,whileallowingfortrivialoperator
fusionsthatcombineconvolution,bias,residualshortcutaddition,andactivationfunc-
tionsinasinglekernel.ThesearethesimplekernelfusionsimplementedinPyTorch
Inductorandotheravailableinferenceengines.
Figure3predictsrelativelyhighmaximumefficiencyforResNet101andConvNeXt-
Small,whichagreeswiththepopularsentimentthatthosemodelsarecomputationally
efficientindeeplearningframeworks.
ResNet101hasmanyconv2dlayerswithlargeoperationalintensity.Italsohasmany
point-wiseconvolutionlayersthatvaryfromlowoperationalintensityintheearlystages
ofthenetworktomoderateoperationalintensityinthelatestageswherethenumber
ofchannelsisgreater.
ConvNeXt-Small has many point-wise convolution layers with large operational
intensity due to the fact they use a large number of channels. It also has depth-wise
convolutionlayerswithextremelylowoperationalintensity,butthesetransferrelatively
littledata,becausetheyareplacedattheblock’sbottleneckwheretheactivationstensor
hasfewerchannels.
EfficientNet-B5 has relatively low maximum attainable efficiency because of the
smallnumberofchannelsintheearlystagesofthenetwork.Also,ithasdepth-wise
convolution layers with low operational intensity, and these layers operate on the
hidden-layertensorsinthebottleneckblockswherethenumberofchannelsisgreatest.
Theseresultssuggestthatnarrow(smallnumberofchannelsperlayer)modelsare
computationallyinefficient.ThisobservationagreeswiththeworkofDollaretal.[14]
whosawthatcomputationalefficiencydecreasesasthenumberofnetworkactivations
increases.Theyreasonedthatmodelsshouldhavemorechannelsandfewerlayersto
increasetheratioofactivationstooperations.Weunderstandthisasasimplestrategy
toincreasetheoperationalintensityofpoint-wiseconvolutions.
Butthiscreatesadilemma,becausethenarroweranddeepermodelslikeEfficient-
Nethavegreatermodelefficiency.Ideallyonecouldimprovecomputationalefficiency
withoutsacrificingmodelefficiency.
4.3. MediantOperationalIntensity
Severalpapersapplytherooflineperformancemodeldirectlytoanentireneuralnet-
work [43, 33, 32, ...]. These papers compute the operational intensity of a network
20simplybydividingthetotaloperationsbythetotalbytestransferredtoandfromDRAM,
∑ n
(29) mediantoperationalintensity = i i
∑ b
i i
for layers i, with n operations and b bytes transferred per layer. Mathematicians
i i
recognizeEquation(29)asthemediant ofthefractions n i.Thetermoriginatesfromthe
b
inequality a < a+c < c [22,18]. i
b b+d d
IfweweretointerpretEquation(29)astheoperationalintensityofasetofkernels,
then we would infer that the kernels run concurrently. The roofline model requires
thisassumption,becausethelatencyofcomputationcannothidethelatencyofdata
movementunlessbothrunconcurrently.
Otherwise,ifthekernelsrunsequentially,thenEquation(29)onlyapproximatesthe
waterlineperformancemodelpresentedearlier.Itiscorrectwhenthekernelsareall
memory-boundorallcompute-bound.Butitoverestimatestheattainableperformance
of a sequence that has both memory-bound and compute-bound kernels. Because
Equation(29)addstheexcessoperationsofthecompute-boundkernelstotheoperations
ofthememory-boundkernels,thesequenceappearstobelessmemory-boundthanit
trulyis.
HowaccurateisEquation(29)whenappliedtoaconvnet?Figure4plotsthemaxi-
mumattainableefficiencyofdifferentconvnetsversustheprocessorop:byteratio.It
showsboththerooflineandwaterlineperformancemodels.Again,weusebatchsize
equalto128and2bytespertensorelement.Forprocessorswithveryhighop:byteratios,
alllayersoftheconvnetsarememorybound,andtherooflinemodel(usingthemediant
operationalintensity)agreeswiththewaterlinemodel.Forlowtomoderateop:byte
ratios,therooflinemodelevaluatestheconvnetsascomputebound,whilethewaterline
modelrecognizesthatsomeofthelayersarememorybound.Thisdiscrepancycauses
therooflinemodeltooverestimatetheattainableefficiency.
FormodelslikeResNet101andConvNeXt-Small,whichhavesomelayerswithrela-
tivelyhighoperationalintensity,rooflinedrasticallyoverestimatestheattainableeffi-
ciency,unlesstheprocessorhasaverylargeop:byteratio.FormodelslikeEfficientNet-
B5,whichhavefewerlayerswithlargeoperationalintensity,rooflineisclosertothe
waterlinemodel.
21100 100
PerformanceModel PerformanceModel
Roofline Roofline
Waterline Waterline
80 80
60 60
40 40
20 ResNet101 20 EfficientNet-B5
0 0
0 200 400 600 0 200 400 600
Processor op:byte Processor op:byte
100 100
PerformanceModel
Roofline
Waterline
80 80
ConvFirst-S
60 60 withBlock-Fusion
40 40
20 20
ConvNeXt-Small(288)
PerformanceModel
Roofline
Waterline
0 0
0 200 400 600 0 200 400 600
Processor op:byte Processor op:byte
FIGURE4.Comparisonoftheattainablecomputationalefficiencycalculatedbythewaterline
androoflineperformancemodels.Rooflinewasoriginallyintendedasaperformancemodel
forasingleparallelkernel[72].Hencerooflineoverestimatestheattainableefficiencyfora
sequenceofconvnetkernels,ifsomeofthekernelsarecomputeboundandothersarememory
bound.Waterlineisaccurateregardless,becauseitmeasureshoweachkernelcontributesto
theminimumlatencyofthesequence.ConvFirstisournewmodel.SeeSection6fordetails.
22
]%[
ycneicffiE
.xaM
]%[
ycneicffiE
.xaM
0005AerepmA
0005AerepmA
]%[
ycneicffiE
.xaM
]%[
ycneicffiE
.xaM
0005AerepmA
0005AerepmA5. Block Fusion: An Optimization for Degenerate Conv2d Layers
Inthissection,weimprovetheoperationalintensityofinvertedresidualblocksbyfusing
their layers into a single kernel. We find a path through each block’s computational
graphthatcreatestemporallocalityandavoidsDRAMmemorytransfers.Theinverted
residualblockbecomesamodulewithlow-bandwidthinterfaceandcomputationally
denseimplementation.Wecallthistechniqueblockfusion.
Blockfusionisfasterthanlayer-wiseexecution,becausethedegenerateconv2dlay-
ersusedinmodernconvnetshavelowoperationalintensitywhencomputedseparately.
Tonavigatethecomplexspaceofkerneldesigns,weintroduceasimplecomputa-
tionalmodelcalledthe“tensormachine.”
5.1. TensorMachines
Webelievethatthecomplexityoflow-levelsourcecodeobscuresthehigh-levelalgo-
rithmicideasthatareessentialforefficientkernels.Wedevisedthetensormachineas
apicturelanguagefordrawingsimplekerneldiagramsthatcapturetheessenceofour
efficientalgorithms.
The tensor machine is a simple abstract computer that helps us visualize kernels
and analyze their theoretical efficiency. It uses simple high-level operations that act
ontensorsandplaceseachtensorinDRAM,globalmemory,orlocalmemory.Tensor
machines define a kernel by showing its operations and data movement using the
symbolsinFigure5.
Tensormachineoperationsincludematrixmultiplication(orchannels-modeprod-
uctofanactivationtensorandaweightsmatrix[36,p.460]),groupedconv2d,element-
wisemultiplication,activationfunctions,andglobalaveragepooling.
Thetensormachine’sthreememoryregionsareDRAM,globalmemory,andlocal
memory.DRAMisslow,off-chipmemorywithlargecapacity.Globalmemoryison-chip
memory that is accessible by all processors. Local memory is fast on-chip memory
internaltoeachprocessor.
Globalmemoryfunctionsasacacheforweightsandoverlappinginputsandasa
sharedmemoryforinter-processorcommunication.
Global and local memory have limited capacity. In this paper we merely assume
thatglobalmemoryissufficientlylargetoholdtheweightsofasingleresidualblock
andasmallnumberofactivations.Thisassumptionisreasonableforallthemodelsin
thispaper.
23Localmemoryisonlylargeenoughtostoresmalltilesofactivationsandweights
tensors.High-resolutiontilesreducetheglobalmemorybandwidthrequirement,be-
causemorecomputationisperformedforeachweightvaluethatistransferredfrom
theglobalmemorycache.Inthispaper,wedeferanalysisoftheexactlocalmemory
capacityandtilesizestotheplatform-specifickerneldevelopmentstage.
We use waterline analysis to compute the minimum attainable latency for a ker-
nel and its tensor machine with a given op:byte ratio. We only count the multiply-
accumulateoperationsoftensor-matrixmultiplicationandconv2doperators,allother
operators do not contribute to the operational intensity. This approximation is use-
fulforneuralnetworksbecausetensor-matrixmultiplicationsandconv2doperations
dominatetheirarithmeticcomplexity.
Thissimplemachinefacilitatesthehigh-levelchoicesthatdominateperformance
andavoidsthelow-leveldetailsthatcreatecomplexity.
DRAM
Global Memory
Local Memory
Loop
Tensor-Matrix Multiplication
Grouped Conv2d
Element-wise Multiplication
SiLU
ReLU
Sigmoid
Global Average Pooling
FIGURE5.Thetensormachineisasimpleabstractcomputerfordesigningconvnetkernels.It
hasthreememoryspaces:DRAM(slow,off-chipmemory),globalmemory(on-chipmemory
sharedbyallprocessors),andlocalmemory(fastmemoryinternaltoeachprocessor).Ituses
basichigh-leveloperations,includingmatrixmultiplication(orchannels-modematrixproduct),
groupedconv2d,element-wisemultiplication,globalaveragepooling,andactivationfunctions.
Theloopsymbolindicatesafor-loopthatiteratesovermemorytiles,loadingonetileatatime
intoalocalmemorybuffer.
5.2. Feed-ForwardNetwork
Position-wiseFeed-ForwardNetwork(FFN)isablockusedprominentlybyTransform-
ers [69] and ConvNeXt [46]. It is also called Multi-Layer Perceptron (MLP) [66]. FFN
24performstwoconsecutivelineartransformationswithanelement-wiseactivationfunc-
tionafterthefirst:
(30) FFN(X) = ϕ(XU +a)V +b
whereX ∈ RP×C istheinput,U ∈ RC×αC andV ∈ RαC×C aretheweights,a ∈ RαC and
b ∈ RC arethebias,andϕisanactivationfunction,withP asthenumberofsamples,C
asthenumberofchannels,andαastheexpansionfactor.
The most direct algorithm to compute FFN operates layer-wise, first computing
hiddenlayerY ∈ RP×αC as
Y = ϕ(XU +a)
thencomputingoutputZ ∈ RP×C as
Z = YV +b
Thelayer-wisealgorithmcreatesalargehiddenlayerY withαtimesasmanychan-
nelsastheinputandoutputlayersX andZ.InastackofFFNblocks,readingandwriting
Y dominates the memory cost of the layer-wise algorithm, accounting for α of all
α+1
activationmemory.
Wewillderiveamemory-efficientalgorithmthatpracticallyeliminatesthehidden
layeractivations.ThisalgorithmisaspecialcaseoftheonethatwasproposedbySandler
etal.fortheMBConvblock[59].
First,computeasinglechannelYr ofthehiddenlayerbymultiplyingtheinputby
columnUr ofweights
(31) Yr = ϕ(XUr +a )
r
andexploitingthefactthatϕisanelement-wisefunction.1
Second,multiplychannelYr byrowV toformanouterproduct,andthesumofthe
r
productsoverr equalsoutputmatrixZ:
αC
Z = ∑ Yr V r +b
r=1
1FormatrixA,wewriteA forrowiandAj forcolumnj [4].
i
25SubstitutingEquation(31)forYr yieldsthecompleteformula:
αC
(cid:88)
(32) FFN(X) = ϕ(XUr +a )V +b
r r
r=1
Equation(32)definesamemory-efficientalgorithmthatcomputesasinglechannel
ofthehiddenlayer,immediatelyusesittoupdatetheoutputlayer,andthendiscardsit.
Thealgorithmiteratesoverallhiddenchannelsandreusesthesamesmallworkspace
to store each of them. Therefore, only a small fraction of the hidden layer exists in
memoryatanygiventime.
Fromanalgorithmicstandpoint,Equation(32)implementsaloopfusionoptimiza-
tion[35,p.254].Inthelayer-wisealgorithm,thefirstlineartransformationloopsover
r ∈ [1.. αC],producinghiddenchannelsYr.Thesecondlineartransformationalso
loopsoverr,consumingYr.ThelargenumberofhiddenchannelsαCcausesthetime
elapsedbetweentheproductionandconsumptionofYr tobelarge.Therefore,alarge
workspaceisneededtostorethefulltensorY.Loopfusionincreasestemporallocality
byproducingandconsumingeachYr inthesameloopiteration.Boththesizeofthe
workspaceandthecommunicationcostofthealgorithmarereduced.
From a model architecture standpoint, Equation (32) virtually eliminates hidden
layerY ∈ RP×αC.TheFFNblockbecomesasinglelayerwithweightsU ∈ RC×αC and
V ∈ RαC×C,inputX ∈ RP×C,outputZ ∈ RP×C,nohiddenlayer,andlargeoperational
intensity.
5.3. ConvFirstBlock
WedesignedaconvnetblockwiththegoalofexploitingtheFFNalgorithmfromEqua-
tion(32).Thefirstlayeroftheblockisagroupedconvolution.Wechosegroupwidth
equalto8channelsbecauseitmatchesthesizeofthematrixmultiplyimplementedby
theNVIDIAtensorcoreinstructionmma.m16n8k8.f16[54].AnFFNlayerfollowsthe
groupedconvolution.Aresidualshortcutconnectstheblockinputtotheoutput,anda
batchnormlayer[30]followseachlinearlayer.ThedownsamplingvariantofConvFirst
usedBlurPool[76].Figure6showstheblockdiagrams.
WecalltheblockConvFirst becauseitputstheconvolutionallayerbeforetheFFN
layers.ConvFirstdiffersfromResNetandMBConv,whichplacetheconvolutioninthe
middleoftheblock.
Puttingtheconvolutionfirstmeansthatthereisnooverlapinthereceptivefields
26of the rest of the block’s layers. FFN layers have a one-to-one dependency between
inputandoutputpixels.Therefore,wecandividetheoutputtensorintomanytilesand
computeeachoftheminparallelwithoutanyinter-tilecommunicationorover-compute.
Onlytheinputtilesoverlap,andtheoverlapishandledgracefullybythememorycache,
soeachinputvalueisloadedfromDRAMatmostonce.
ConvFirst uses the ReLU [23, 31] activation function because it can be computed
efficiently.Thecostofthecomputationisamortizedovertheblock’sinputchannels,
soanadvancedfunctionlikeSiLU[16,58]wouldberelativelyexpensiveifthenumber
of channels were small. ReLU does cause a slight accuracy drop relative to SiLU in
ourmodelexperiments,andSiLUcomputationisreasonablyefficientonprocessors
thatacceleratethetanhspecialfunction.Therefore,SiLUmightbepreferableinlarger
models.
ConvFirst is very similar to ConvNeXt, which was discovered independently [46].
ConvFirst and ConvNeXt are basically the same block with different normalization,
group-width,andkernel-size.
Batchnorm
+ Conv(1x1)
Batchnorm
ReLU
+ Conv(1x1) Batchnorm
Batchnorm ReLU Conv(1x1)
Conv(1x1) Batchnorm
BlurPool
ReLU Conv(1x1) cat
Batchnorm Batchnorm Batchnorm
Conv(3x3) Conv8(3x3) Conv8(3x3)
(a).FusedMBConv (b).ConvFirst (c).ConvFirstStride2
FIGURE6.(a). The FusedMBConv block, as used in EfficientNet-EdgeTPU [21] and Efficient-
NetV2[64],hasaconv2d(3×3)layerthatusesalargenumberofarithmeticoperations.Theblock
alsoaffordsamemory-efficientblock-fusionimplementation.(b).WecreatedtheConvFirst
blocktobealight-weightmimicofFusedMBConv.ConvFirstalsoaffordsblockfusionbutuses
farfeweroperations.ConvFirstisverysimilartoConvNeXt[46].(c).ConvFirstusesBlurPool[76]
fordownsamplingtoimproveshiftinvarianceofdownstreamtasks.
5.3.1. Block-FusionKernelforConvFirst
WedesignedatensormachinekernelthatimplementstheConvFirstblock.Theblock-
fusionstrategycaststheentireblockasasinglecomputationwiththreeweightstensors
27andnohiddenactivations.Actuallyeachhiddenactivationexistsforamomentinlocal
memory,butitisinvisibletotheglobalmemorysystem.Therefore,ConvFirstblocks
havelowcommunicationcostandlargeoperationalintensity,evenwhenthenumber
ofchannelsissmall.
Figure 7(a) shows the tensor machine for the layer-wise ConvFirst kernels. The
layer-wisekernelsproducealargeDRAMtensorforthehidden-layeractivations.
Figure7(b) shows our block-fusion kernel. It loads an input tile from DRAM into
localmemory.Itcomputesthegrouped-convolutionoftheinputtileandretainsthe
resultinlocalmemory.Italsoinitializestheoutputaccumulatorswiththeinput-tile
values,therebyimplementingtheresidualshortcutconnection.
NextitcomputestheFFNlayersusingtheloopfusionalgorithmdefinedinEquation
(32).Asingleloopoverhidden-layerchannelgroupsproducesactivationswiththefirst
linear transform and then immediately projects them onto the output tile with the
secondlineartransform.AfterthelastloopiterationtheConvFirstblockiscomplete
andtheoutputtileisstoredtoDRAM.
Anarrowgroupofhidden-layerchannelsisprocessedduringeachloopiteration,so
thatasmalltileinlocalmemoryholdsthehidden-layeractivations.Thetilereplaces
thelargehidden-layertensorinDRAMthatthelayer-wisekerneluses.
Theblock-fusionkernelrequiresthatthenumberofchannelsissmallenoughthat
theinputandoutputtilesfitinlocalmemory.Thisrequirementissatisfiedforthesmall
visionmodelswecreateinthelaterpartofthispaperandtheNVIDIAGPUonwhich
weimplementthekernel.Itwouldnotbetrueforlargemodels.
The kernel variant shown in Figure7(c) scales the block-fusion kernel for a large
numberofchannels.Weunderstanditaspartitioningtheinputandoutputtileschannel-
wiseacrosstwoprocessors.Eachprocessorloadshalftheinputchannelsandproduces
half the output channels. A small tensor in global memory accumulates the partial
sumsproducedbyeachprocessortoformthehidden-layerchannels.
Thecostoftheaddoperationinglobalmemoryisamortizedoverthechannelsof
theinputandoutputtensors,soweexpectittobeinsignificantwhenthenumberof
channelsislarge.Foraverylargenumberofchannels,wewouldpartitionthemacross
fourormoreprocessors.
Thebatchnormlayersbecomeconstant,channel-wisescaleandoffsetduringin-
ference,soweapplythewell-knowntrickoffoldingtheseconstantsintotheweights
and bias of the preceding conv2d layer. Hence, batchnorm layers disappear during
inference.
28(a).ConvFirstwithlayer-wiseexecution.
(b).ConvFirstwithblock-fusion.
(c).ConvFirstwithblock-fusionandscaling.
FIGURE7.TensormachineforConvFirstkernels.(a).Layer-wisekernelscreatealargetensor
inDRAMforthehidden-layeractivations.(b).Block-fusioncomputesalllayerssimultaneously
usingsmalltilesinlocalmemory(red).(c).Block-fusionscalestosupportalargenumberof
channelsbypartitioningtheinputandoutputtileschannel-wiseacrossmultipleprocessors.
ForlargeNHW,theweightsmatricesarealmostalwaysinglobalmemory(orange)cache.
295.3.2. WaterlineAnalysisofConvFirst
Figure 8 shows the waterline analysis of the ConvFirst kernels for a range of block
parameterswiththeop:byteratiooftheA5000GPUasthewaterline.Theplotcompares
theoperationalintensityandminimumattainablelatencyforthelayer-wisekernels
andtheblock-fusionkernel.Each“well”showstheresultsforadifferentchoiceofblock
parameters.
Layer-wisekernelsarememoryboundforallblocks,whiletheblock-fusionkernels
areonlymemoryboundwhentheexpansionratio(α)issmall.Whentheexpansion
ratioequals6,theblock-fusionkernelsarecomputeboundforasfewas32channels.
Theminimumattainablelatencyoftheblock-fusionkernelsissignificantlylowerfor
allblocks.Thespeedupisgreatestwhenthenumberofchannelsandexpansionratiois
smallest,wherethepoint-wiseconvolutionsareseverelymemorybound.
1200
Kernels
1000 GroupedConvolution
PointwiseConvolution
ConvFirstBlock-Fusion
800
600
400
200 NVIDIAAmpereA5000GPU
0
1538 280 2307 419 3076 559 1188 199 1783 425 2377 735 446106 594184 892 402
16c3α128hw 24c3α128hw 32c3α128hw 32c6α64hw 48c6α64hw 64c6α64hw 48c6α32hw64c6α32hw 96c6α32hw
MinimumAttainableLatency[microseconds]
FIGURE 8. Waterline plot for the ConvFirst block with layer-wise and block-fusion kernels.
Theverticalaxisshowstheoperationalintensityofthekernels,andthehorizontalaxisshows
the minimum latencies derived from roofline analysis. A horizontal blue line indicates the
op:byteratio(i.e.,thewaterline)oftheA5000GPU.“Underwater”kernelsarememorybound.
Theblock-fusionkernelsavoidwritingtheactivationsofthehiddenlayertoDRAM.Therefore,
theyhavesignificantlygreateroperationalintensityandcanattainlowerlatency.
5.4. MBConvBlock
TheMBConvblockwasoriginallydevelopedforMobileNetV2asamemory-efficient
blocksuitableforconvnetsthatrunonsmallmobiledevices[59].EfficientNetadded
Squeeze&ExcitationlayerstotheMBConvblockandscaledthemodeltoverylarge
30
]etyb:po[ytisnetnIlanoitarepOsizeandaccuracywhileachievingstate-of-the-artmodelandparameterefficiency[63].
Figure 9 shows our variant of MBConv. We substituted grouped convolution for
thetraditionaldepth-wiseconvolution,toachievebetterutilizationonmatrixmultiply
accelerators.OnNVIDIAGPUswith float16 tensorcorearithmetic,weusedgroup-width
equaltoeightchannels.
OurMBConvvariantusesBlurPoolfordownsampling[76].ThepurposeofBlurPool
istoimproveshiftinvarianceindownstreamtasks,likeobjectdetectionandsemantic
segmentation.Inourmodelexperiments,weusedBlurPoolwiththeTriangle-3filter.
The waterline analysis of EfficientNet in Section 4.2 showed that all layers of the
MBConvblockarememoryboundintheearlystagesofthenetwork,wherethenumber
ofchannelsissmall.Therefore,weareinterestedinMBConvforthelatestagesofthe
network,wherethenumberofchannelsislargeandtheresolution(heightandwidth)of
theactivationtensorissmall.WedesignedanMBConvblock-fusionkernelthattargets
thishyperparameterdomain.
+ Batchnorm
Batchnorm Conv(1x1)
Conv(1x1) Squeeze & Excitation
Squeeze & Excitation BlurPool
SiLU SiLU
Batchnorm Batchnorm
Conv(3x3) Conv(3x3)
8 8
SiLU SiLU
Batchnorm Batchnorm
Conv(1x1) Conv(1x1)
(a).MBConv (b).MBConvstride2
FIGURE9.MBConvblockwithSqueeze&Excitation.WemodifiedthetraditionalMBConvblock
tousegroup-width8convolutionbecausethatoperationmapsperfectlytoNVIDIAtensorcores.
WeaddedaBlurPoollayerafterthegroupedconvolutionlayertoimplementdownsampling.
315.4.1. Block-FusionKernelforMBConv
Figure10showsthetensormachineforlayer-wisekernelsfortheMBConvblockwith
Squeeze&Excitation.Thetensorshapesarerepresentativeofthosefoundinthelater
stagesofaconvnet,wheretheresolutionoftheactivationtensorislowandthenumber
of channels is large. These hyperparameters produce small activation tensors and
relatively large weights matrices. For the model sizes considered in this paper, the
weightstensorsfitinglobalmemoryofacontemporaryGPU.
Weshowtheweightsmatricesandactivationtensorspartitionedintoeightblocks
each, which correspond with the blocks used by each processor in the block-fusion
algorithm,describedbelow.
Figure11showsthetensormachineforablock-fusionkernel.Wedocumentedthis
samekernelinapreviousarticle[38].
Theblock-fusionkernelusesmultipleprocessors,assigningagroupofhidden-layer
channels to each. Each processor computes its own group of hidden-layer channels
using the corresponding block of weights from the first point-wise convolution and
storingtheactivationsinlocalmemory.ThenitappliestheSiLUactivationfunction.
Then each processor computes the grouped convolution on its channels, again
applyingtheSiLUactivationfunction,producingtheconvolutionlayeroutput,whichis
alsostoredinlocalmemory.Ifthewidthofthegroupedconvolutionevenlydividesthe
numberofchannelsassignedtoeachprocessor,thenthegroupedconvolutioncanbe
computedinparallelwithoutanyinter-processorcommunication.
Then each processor computes the global average pool of that result, yielding a
vectorwithonevalueforeachchannel.Thatvectormultipliesthecorrespondingblock
of the squeeze-matrix, producing a partial sum of the squeeze-vector. The vectors
producedbyallthethread-blocksareaddedinabufferinglobalmemorytoproduce
thesqueeze-vector.Thisadditioninglobalmemoryisasynchronizationpointforall
thethread-blocks.
Theneachprocessorloadsthesqueeze-vectorfromtheglobalmemorybuffer,ap-
plies the ReLU activation function, and multiples it by the block of the excitement
matrixcorrespondingtoitschannels.Asigmoidactivationisappliedtotheresultofthat
transformation,producinggatingvaluesbetween0and1.Thesegatingvaluesmultiply
thecorrespondingchannelsoftheconvolutionlayeroutput,producingtheoutputof
theSqueeze&Excitationlayer(SE).
TheSEoutputmultipliesthecorrespondingblockofweightsfromthesecondpoint-
wiseconvolution.TheresultisaddedontotheMBConvblock’sinputtensoringlobal
32memory,therebyproducingthelayeroutputwithresidualshortcut.Theinter-block
synchronizationintheSElayeralsoensuresthatallthread-blockshavereadtheinput
tensorbeforeanyofthemupdateitwiththeMBConvoutput.
Theblock-fusionkernelcomputesthefullheightandwidthoftheactivationtensors
in each processor, effectively limiting the maximum resolution of the input images.
If the height and width are too large, then the blocks of the activation tensors will
notfitinlocalmemory.Wecouldextendthebasicblock-fusionalgorithmtopartition
the activation tensors in the height or width dimensions in addition to the channels
dimensionandassigndifferentblockstodifferentprocessors.Thiswoulddecreasethe
amountoflocalmemoryusedbyeachprocessor,butitwouldalsorequireadditional
inter-processorcommunicationtoexchangetheoverlapping“halos”oftheinputsto
thegroupedconvolutionlayer.Forexample,eachprocessorcouldloadanh×W ×Ctile
fromtheinputtensor,whereh < H,andcomputethefirstpoint-wiseconvolutionforits
channelgroup,yieldinganh×W ×r tensor,wherer < αC.Theneachprocessorwould
exchangeitstopandbottomrowswithitsneighbor,sothateachnowhasa(h+2)×W ×r
tileofhidden-layeractivations.Theneachprocessorcomputesthegroupedconvolution,
producinganh×W ×r tileofactivations.
The cost of this high-resolution block-fusion kernel is additional global memory
buffers,additionalglobalmemorydatamovementforhaloexchange,andinter-processor
synchronization.Howevertheextradatamovementismuchlessthanwhatisrequired
toloadtheinputactivationtensorsandstoretheoutputactivationtensors.Wecould
increase the number of hidden-layer channels computed by each processor and de-
crease the tile size to keep the local memory allocation constant. This adjustment
wouldincreasethearithmeticintensityofthekernelwithrespecttotheglobalmemory
system.
Thespeedofinter-processorcommunicationandsynchronizationcanberelatively
fastifprocessorsareorganizedtosupportefficientcommunicationwiththeirneighbors.
ThedistributedsharedmemoryintheNVIDIAH100GPUisonesuchexample[50].
33FIGURE10.Tensormachineforthelayer-wiseexecutionofMBConv+Squeeze&Excitation.
Whenusedinthelatestagesofaconvnet,MBConvblockshavearelativelylargenumberof
channels and low-resolution feature maps. The large number of channels causes relatively
largeoperationalintensityforthepoint-wiseconvolutionlayers.Butthegrouped-convolution
andSElayershaveverylowoperationalintensity,becausetheyperformfewoperationsper
activation.Weshowtheweightsmatricesandactivationtensorsdividedintoeightpartitions
thatcorrespondtothematrixandtensorblocksusedbytheblock-fusionkernelinthenext
figure.
34FIGURE11.Tensormachinefortheblock-fusionofMBConv+Squeeze&Excitation.Multiple
processorscomputetheMBConvblockinparallel,witheachblockcomputingapartitionof
thehidden-layerchannels.Thehidden-layeractivationsneverleavethelocalmemoryofthe
processor that computes them. Each processor loads only the weights corresponding to its
channelsassignment.The“squeeze”operationoftheSElayeraccumulatesthecontributions
fromeachprocessorontoatensoringlobalmemory.Theresidualshortcutisimplementedby
accumulatingtheoutputtensorontotheinputtensor.Thekernelhasgreatoperationalintensity
butusesalotofglobalmemorybandwidth.WhenimplementedonanNVIDIAGA102[52],the
performanceislimitedbythelatencyofinter-processorsynchronization.Thekernelwould
bemoreefficientontheHopperGPUarchitecture[50],usingdistributedsharedmemoryfor
inter-processorcommunication.
355.4.2. WaterlineAnalysisofMBConv
Figure 12 shows the waterline analysis of the MBConv layer-wise and block-fusion
kernelsacrossarangeofblockhyperparameters.Asmentioned,thesekernelswere
designedforlow-resolutionfeaturemapsfoundinthelatestagesofaconvnet,sowe
use16 ×16 and8 ×8 .Latevisionblocksalsohavearelativelylargenumberof
H W H W
channels,soweuseC = 128andC = 256.Wealsousebottleneckexpansionratioequal
tofour.
Thepoint-wiseconvolutionsarememoryboundinthelayer-wisekernelswith128
channelsandarebarelycomputeboundwith256channels.Thegroupedconvolution
layersarealsomemorybound,aswellastheSElayers.
The block-fusion kernels have very large operational intensity and are strongly
computebound.Theminimumattainablelatencyoftheblock-fusionkernelsisabout
onehalftoonethirdthatofthelayer-wisekernels.
1200
Kernels
1000 PointwiseConvolution
GroupedConvolution
Pool,Multiply,Cat,orNorm
800
MBConvBlock-Fusion
600
400
200 NVIDIAAmpereA5000GPU
0
544 144 612 178 680 215 816 300 1150 512 13736 155 45 172 54 207 75 290 128
128c4α16hw 144c4α16hw 160c4α16hw 192c4α16hw 256c4α16hw 128c4α8hw 144c4α8hw 160c4α8hw 192c4α8hw 256c4α8hw
MinimumAttainableLatency[microseconds]
FIGURE 12. Waterline plot for the MBConv block with layer-wise and block-fusion kernels.
Layer-wiseexecutionismemory-boundforgroupedconvolution,Squeeze&Excitationlayers,
andsomepoint-wiseconvolutions.Block-fusionisstronglycomputebound.
5.5. CUDAKernels
We implemented the ConvFirst and MBConv block-fusion kernels in CUDA [48]. We
usedcodegenerationtocreatehelperclassestosimplifytensorindexing.Theseclasses
computetheoffsetforagivenindexusingnamed-dimensions.Forexample,thetensor
spec
36
]etyb:po[ytisnetnIlanoitarepOOutputIdx=dict(k8=8, n=128, p=64, q=64, k2=4)
generatesaCUDAclassthatcanbeusedlikethis:
st_outputs[OutputIdx().p(thread_p).q(thread_q).k8(k8)] = val;
Theindexingmethodsuse sothatthecompilercansimplifytheindexing
constexpr
arithmeticatcompiletime.Wefoundthattheuseofdimensionnameswhenaccess-
inghigh-dimensionaltensorsgreatlyimprovedtheclarityofthesourcecodewithout
sacrificingcomputationalefficiency.
Wealsousedcodegenerationtoimplementclassesthatdefinestatickernelparam-
eters,suchastilesizes.Thisenabledustoenumeratemanykernelconfigurationsin
PythonwithoutaddingtothecomplexityoftheCUDAsourcecode.
To launch our block-fusion kernels from Python, we implemented a base
Kernel
classthatusestheCuPy[55]packagetocompileandlaunchkernels.Weimplemented
anauto-tunemethodthatenumeratesallvalidkernelconfigurationsforthegivenblock
parameters,runseachwithrandomweightsandinputs,andselectsthefastest.
TointegrateourkernelswithPyTorch[3],weimplementeda baseclass
TorchBlock
thatderivesfrom .Each derivedclasshasacorresponding
torch.nn.Module TorchBlock
class.Thefirstcalltothe methodcalls
Kernel TorchBlock.forward Kernel.auto_tune
toselectthefastestkernelconfigurationfortheblockparameters.Thekernelisthen
storedintheblockobjectandlaunchedusingCuPy.WeuseCuPytocaptureaCUDA
graphandusethegraphinthebenchmarkstoreducekernellaunchoverhead.
Thisapproachgivesusasimplewaytoimplementacustominferencebackendthat
runsinPyTorch.Inprinciple,wecouldrunanentirenetworkthisway.Inthispaper,
wesimplybenchmarkastagecomposedofmultipleConvFirstorMBConvblocks.
5.5.1. ConvFirstCUDAKernel
TheConvFirstblock-fusionkernelusesthread-blockswitheightwarps(i.e.,hardware-
threads).Thekernelcomputesa p×q×K outputtilebyfirstloadingthecorresponding
(p + 2) × (q + 2) × C input tile into shared local memory (smem), where C and K are
thenumberofinputandoutputchannels,and pandqaretheheightandwidthofthe
outputtile.Thekerneluses arithmeticwithNVIDIAtensorcoreacceleration.
float16
Thewarpscomputethegroupsoftheconvolutionlayerinparallel,usingtheNVIDIA
matrixmultiplyinstruction,whichfitsthegroup-widthofeight,and
mma.m16n8k8.f16
storethe p×q×Coutputtosharedlocalmemory.IfC == K and stride == 1,theblock
hasaresidualshortcutconnection,andeachwarpsimultaneouslyloadsatileof p×q ×C
8
37inputvaluesfromsmemandusesthemtoinitializetheregistersthataccumulatethe
second linear transformation of the FFN layer. Thus the block input is added to the
outputduringthefirstiterationoftheFFNlayer’smultiply-accumulateoperations.
Moresignificantly,theresidualshortcutdoesnotcauseanextraloadfromtheglobal
memorysystem,asitwouldinalayer-wisekernel.Instead,itloadstheinputtilefrom
smem,whereitwaspreviouslystoredforthegroupedconvolution.
Nowthatthegrouped-convolutionlayeriscomputedandtheFFNaccumulatorsare
initialized, we are ready to compute the FFN layer. Each warp loads a p×q × C tile of
8
thegrouped-convolutionoutputsintoregisters.TheneachwarpusesEquation(32)to
computeasmallnumberofhiddenchannelsr,convertthe float32outputto float16,
add bias and apply the ReLU activation function, and reformat the values as input
fragmentsforthe instruction.Thenthefragmentsaremultipliedby
mma.m16n8k8.f16
the corresponding weights of the second linear transform in the FFN layer with the
resultaccumulatedontotheoutputtile.Notethatthehidden-layeractivationsnever
leavetheregisterfile.
The kernel loops over r hidden channels at a time, asynchronously loading the
correspondingcolumnsandrowsoftheFFNlayerweightsmatricesateachiteration.
Afterthelastiteration,thekerneladdstheFFNlayer’soutputbiastotheaccumulators,
convertsthemfrom to ,andstorestheresulttotheoutputtileinglobal
float32 float16
memory.
Ideally p×q isamultipleof16andr isamultipleof8sothatthehiddenactivations
8
tile with shape p 8×q × r fits the NVIDIA mma.m16n8k8.f16 or mma.m16n8k16.f16 matrix
multiply instruction. p×q should be large enough to hide the latency of loading the
weightsfromL2cache,and p×q shouldbelargeenoughtohidethelatencyofloading
8
theweightsfromsmem.
Of course p 8×q × C input values of type float16 and p 8×q × K output accumulators
of type must fit in each warp’s registers, in addition to a smaller number of
float32
registersfortheweightsandaccumulatorsofthehidden-layeractivations.Withabudget
of128registersperwarp,ourCUDAkernelperformswellwithupto96channels.This
providessufficientwidthfortheearlystagesofallbutthelargestconvnets.
Theblock-fusionandscalingkernelshowninFigure7(c)wouldsupportanarbitrary
numberofchannels,butwedidnotrequireitforthenetworksinthispaper.
385.5.2. MBConvCUDAKernel
WewroteaCUDAkernelthatimplementstheMBConvblock.Itusestheblock-fusion
algorithmdefinedbythetensormachineshowninFigure11.Thekernelsuses
float16
arithmeticwithNVIDIAtensorcoreacceleration.
TheCUDAkernelusesmultiplethread-blocks,assigningagroupof64hidden-layer
channelstoeach.Eachthread-blockcorrespondstoasingleprocessorinFigure11.
Weusedatomicaddstoimplementtheadditionsonbuffersinglobalmemory.On
NVIDIA GPUs, the global memory is the L2 cache, and atomic counters are used for
inter-thread-blocksynchronization.
Eachthread-blockcomputesthefullheightandwidthoftheactivationtensor.There-
fore, the kernel computes the convolution layer and global average pooling without
any inter-block communication. The maximum resolution that can be supported is
limitedbytherequirementthattheH ×W ×64 tileofhidden-layeractivationsmustfit
C
insharedlocalmemory.Wetestedourkernelswith16 ×16 and4 ×8 ×8 tensors,
H W N H W
whereN isthebatchdimension,sothatinbothcasesthekerneluses32KBofshared
local memory for activations. This fits easily in the 48 KB of smem available to each
thread-block,leavingroomforaworkspaceusedforloadingweightstensors.
Wecouldsupporthighresolutionbyassigningspatialpartitionsoftheactivationten-
sortodifferentthread-blocks.Additionalinter-blockcommunicationwouldbeneeded
tocomputetheconvolutionandSElayers.Itisunclearhowmuchslowdownthisextra
communicationwouldcauseontheAmpereGPU.
Inter-blockcommunicationisconsiderablyfasteronthenextgenerationHopper
GPUusingdistributedsharedmemory.Thread-blocksareorganizedintoclustersthat
canaccesseachother’ssharedmemory,enablinginter-blockcommunicationthatis
about7×fasterthanglobalmemory[50].Therefore,theHopperGPUshouldmakethe
MBConvkernelmoreefficientandscalable.
Weadditionallyimplementedamicro-batching optimizationfortheMBConvkernel.
GivenastageofDconsecutiveMBConvblockswithfeature-mapsizeH×W,wecompute
the micro-batch size n < N that is required to fully-occupy all 64 streaming multi-
processors on the A5000 GPU. Our kernel computes the first 2n images for the first
MBConv block in the stage, writing the output to global memory. Then the kernel
advancestothenextblockinthestage,usingthe2noutputsfromthepreviousblockas
input.Thekerneliteratesoverd ≤ DMBConvblocksbeforeloopingbacktothefirst
blockandprocessingthenextmicro-batchof2nimages.
The idea is that the 2nHWC bottleneck activations of the micro-batch and the ap-
39proximatelyd(2αC2 +72αC)weightsforthepoint-wiseandgrouped-conv2dlayers(of
the D MBConv blocks with bottleneck expansion ratio α) fit in the L2 cache, so that
intermediateactivationsarenotstoredtoDRAM.Onlytheinputandoutputtothestack
of d blocks are transferred through DRAM. This works because the product HWC is
relativelysmallforthelatestagesoftheconvnetwhereweuseMBConvblocks.Micro-
batchingincreasestheoperationalintensityoftheMBConvkerneluntilDRAMmemory
bandwidthhaslittleimpactonkernelperformance.
Themicro-batchoptimizationbenefitsfromthefactthattheblock-fusionkernel
doesnotwritehidden-layeractivationstoglobalmemory,sothenumberofactivations
thatneedtobecachedisreducedbyafactorofα+1.
5.5.3. KernelBenchmarks
WebenchmarkedourkernelsinPyTorchusingthekernellaunchmethoddescribed
inSection5.5.WeusedCUDAeventstorecordthestartandendtime.Weskipped20
iterationsofwarmuptohidetheauto-tuneandensurestabilityoftheGPUclockspeed.
Wetimed100iterationsofthestageexecutiontogetanaccurateestimateoftheaverage
latency.Webenchmarkedastageofeightlayerstoensurethatweightswerenotcached
betweeniterations.
WebenchmarkedPyTorchInductorbycompilingtheblockmodulewith
torch.compile()
andrunningwith and
torch.no_grad() torch.cuda.amp.autocast(dtype=torch.float16)
contexts.Weagainusedwarmupiterationstohideanyrecompilationthatmightoccur
onthefirstiterations.
OurConvFirstblock-fusionkernelrunsbetween4.5×and14.2×fasterthanPyTorch
Inductoracrossarangeofblockconfigurations.Thespeedupisgreatestwhentheblock
isnarrowest,confirmingtheintuitionthatthelayer-wisekernelsusedbyInductorare
memory-boundandbecomemoreefficientastheoperationalintensityofthelayers
increases.
Thecomputationalefficiencyoftheblock-fusionkernelcanbeaslowas35%when
theblockisverynarrowandthebottleneckexpansionratioissmall.However,whenthe
numberofchannelsreaches32andtheexpansionratioequals6,thekerneliscompute
bound,andthecomputationalefficiencyequals66.9%,a13.2×speedupoverInductor.
With64channels,ourkernelreaches76.2%computationalefficiency.
Our MBConv block-fusion kernel runs between 3.0× and 5.4× as fast as PyTorch
Inductor. The computational efficiency reaches 46.2% with 128 channels and 51.4%
with256channels.Varyingtheheightandwidthoftheactivationtensorbetween16×16
40100
BlockFusion
70 PyTorchInductor
80
60
50
60
40
30 40
20
20
10
0 0
16c3α 32c3α 32c6α 48c6α 64c6α 48c6α 64c6α 96c6α
128hw 64hw 32hw
Channels Expansion Height Width
× × ×
FIGURE13.ConvFirstblockperformanceinPyTorch,comparingtheInductorbackendwith
ourblock-fusionkernels.ThenumberofchannelsC,bottleneckexpansionratioα,andfeature
map height and width hw were selected from our ConvFirst network, and batch size equals
128.Inductor’slayer-wiseoperationscheduleismemorybound,performingpoorlywhenthe
numberofchannelsissmallandimprovinggraduallyasthenumberincreases.Ourblock-fusion
algorithmhasgreateroperationalintensityandthereforeruns14×fasterthanInductorwhenthe
numberofchannelsissmallandachieves76%ofpeakarithmeticthroughputwith64channels.
and8×8causeslittledifferenceinperformance.Performancedropswhenthenumber
of channels is not a power of two. This is caused by our kernel using a number of
thread-blocksthatdoesnotevenlydividetheGPU’s64availableSMs.
TheMBConvkernelusesverylittleDRAMbandwidth.Performanceislimitedby
memory latency and the speed of inter-block synchronization. The kernel would be
moreefficientonacomputerthataffordsfastinter-processorcommunication,suchas
theNVIDIAHopperarchitecturewithdistributedsharedmemory.
41
]s/POLFT[ecnamrofreP
kaeP%TABLE1.ConvFirstperformancewithPyTorch-Inductorversusourblock-fusionkernelsfor
differentnumbersofchannels,bottleneckexpansionratio(α),andactivationtensorsize(Height
×Width).
Inductor Block-Fusion
Channels α Height Width Time[ms] TFLOPS %Peak Time[ms] TFLOPS %Peak Speedup
16 3 128 128 5.88 1.9 2.5 0.42 26.9 35.1 14.2
32 3 128 128 9.89 3.6 4.7 0.99 35.7 46.6 9.9
32 6 64 64 3.94 3.9 5.1 0.30 51.3 66.9 13.2
48 6 64 64 5.06 6.4 8.4 0.59 54.9 71.6 8.6
64 6 64 64 6.75 8.3 10.9 0.96 58.4 76.2 7.0
48 6 32 32 1.33 6.1 8.0 0.16 51.3 67.0 8.4
64 6 32 32 1.70 8.3 10.8 0.25 56.7 74.0 6.8
96 6 32 32 2.60 11.8 15.4 0.58 52.6 68.7 4.5
TABLE2.MBConvperformancewithPyTorch-Inductorversusourblock-fusionkernels.
Inductor Block-Fusion
Channels α Height Width SERatio Time[ms] TFLOPS %Peak Time[ms] TFLOPS %Peak Speedup
128 4 16 16 0.25 11.46 7.7 10.0 2.48 35.4 46.2 4.6
144 4 16 16 0.25 13.38 8.1 10.6 3.19 34.0 44.4 4.2
160 4 16 16 0.25 14.81 8.9 11.6 4.09 32.1 41.9 3.6
192 4 16 16 0.25 17.78 10.3 13.5 5.39 34.1 44.4 3.3
256 4 16 16 0.25 24.30 12.9 16.8 8.01 39.1 51.0 3.0
128 4 8 8 0.25 3.43 6.4 8.4 0.64 34.5 45.0 5.4
144 4 8 8 0.25 4.03 6.7 8.8 1.09 25.0 32.5 3.7
160 4 8 8 0.25 4.36 7.5 9.8 1.37 24.0 31.3 3.2
192 4 8 8 0.25 5.18 8.9 11.6 1.39 33.1 43.1 3.7
256 4 8 8 0.25 7.56 10.4 13.5 1.99 39.4 51.4 3.8
42100
BlockFusion
70 PyTorchInductor
80
60
50
60
40
30 40
20
20
10
0 0
128c 144c 160c 192c 256c 128c 144c 160c 192c 256c
16hw 8hw
Channels Height Width
× ×
FIGURE14.MBConvblockperformancewithPyTorch-Inductorversusourblock-fusionkernels.
We varied the number of channels, height and width over the range of parameters used in
ourConvFirstnetwork.Bottleneckexpansionratioequals4andbatchsizeequals128forall
experiments.Inductor’slayer-wiseexecutionisseverelymemoryboundforsomelayersofthe
MBConv block, while our fused-layer algorithm has large operational intensity. Inter-block
synchronizationprovescostlyforourkernels,limitingperformanceto51%ofpeakarithmetic
throughput.Ourkernelsstillrun3×to5.4×fasterthanInductor.
43
]s/POLFT[ecnamrofreP
kaeP%5.6. Depth-FirstExecution
Manypapershavepursuedtheideaofdepth-firstexecutionofconvnets[1,19,...].The
ideaistocomputeonlyasmalltileofactivationsinthefirstlayer,thenimmediately
compute all the outputs in the next layer that can be produced from it. By chaining
severallayerstogetherinthisfashion,onecancomputeseverallayersatatimewhile
keepingtheactivationtilessmallenoughtofitinon-chipmemory.
GlobaloperatorslikeSqueeze-and-Excitationpreventdepth-firstexecution.Because
everyoutputoftheselayersdependsoneveryinput,thereisnodepth-firstpartition
ofthecomputationalgraph.Therefore,depth-firstexecutioncannotbeappliedtothe
MBConv+SEblocksusedinthispaper.
But depth-first execution complements the ConvFirst block-fusion kernels. One
wouldconsideraConvFirstblockasaspecialized3×3convolutionallayerwithNHWC
inputandoutputtensorsandlargearithmeticintensitywithrespecttoitsweights.The
dependencyanalysisforpartitioningtheactivationtensorsintosmallertilesisthesame
asforaregular3×3convolution.Therefore,wecanenjoythesmallmemoryfootprint
ofadepth-firstexecutionacrossastackofConvFirstblocks.
Withoutblock-fusionkernels,theConvFirstblocksappearasagroupedconvolution
followedbytwopoint-wiseconvolutions.Eachoftheseseparatelyhasmuchsmaller
arithmeticintensitythantheblock-fusionkernel,sodepth-firstexecutionofun-fused
layerswouldrequiremoredatamovementintheon-chipglobalmemorysystem.Alsoit
wouldrequireadditionalglobalmemoryforstoringtilesofthehidden-layeractivations.
Therefore,wecanminimizedatamovementatalllayersofthememoryhierarchyby
usingdepth-firstexecutioninconjunctionwithblock-fusionkernels.
Additionally,depth-firstexecutionrequiresatileschedulethatspansmultiplelayers
withsomesynchronizationbetweentiles.ThismightbedifficulttomaptotheCUDA
programmingmodelandthelargeparallelismofGPUs.Ontheotherhand,itcouldbe
very efficient on small accelerators where the memory efficiency would prove most
beneficial.
6. ConvFirstNet: Striving for Computational Efficiency and Model
Efficiency
Withthegoalofco-optimizingmodelefficiencyandcomputationalefficiency,weused
theConvFirstandMBConvblockstodesignaconvnetmodelcalledConvFirstNet.
446.1. TheDesignofConvFirstNet
WeusedConvFirstblocksfortheearlystages(1-3)andMBConvblocksforthelatestages
(4-5).Thisdivisionbetweenlight-weightearlyvisionstagesandhigh-capacitylatevision
stagesisinspiredbyEfficientNetV2[64].However,ourConvFirstblockusesmanyfewer
operationsandparametersthantheFusedMBConvblockusedbyEfficientNetV2.
SimilartoNFNet[6],ConvFirstNetusesalargeincreaseinthenumberofchannels
betweenthethirdandfourthstages.Unlikeothermodels,ConvFirstNetusesthesame
numberofblocksinthefourthandfifthstages.
Weusedgroup-widthequaltoeightforallgroupedconvolutionsbecauseitisthe
smallestwidththatisdivisiblebytheNVIDIA matrix-multiplyinstruc-
mma.m16n8k8.f16
tion.InCIFARexperiments,wefoundgroup-widthequaltofouryieldedthebestmodel
efficiency.Eightandone (depth-wise) bothusedslightlymoreoperations toachieve
thesameaccuracy.Wewouldprefergroup-widthfouronhardwarethatcomputedit
efficiently.
WeusedanexpansionratioequaltothreeinthebottleneckofthefirstConvFirst
blocktoreducetheamountofcomputationdonebythisstage.Thismighthavebeena
mistake,becausealargerexpansionratiowouldincreasetheoperationalintensityand
yieldbettercomputationalefficiency.Anotherinterestingideaistoreplacethestem
andfirststagewithapatchify layerwithstrideequaltofour,asConvNeXtdoes.
StagestwoandthreeuseConvFirstwithexpansionratioequaltosix.Thisisinspired
bythelargeexpansionratiosusedbyEfficientNet’sbottleneckblocks.
StagesfourandfiveuseMBConvwithexpansionratioequaltofour.Wechosethis
smallerratiotodecreasethenumberofoperationsperformedbyeachblock,because
settinggroup-widthequaltoeightincreasedthenumberofoperationsrelativetothe
typicalMBConvblockwithdepth-wiseconvolution.
TheMBConvblockhasaSqueeze&Excitationlayerwithsqueeze-ratioequalto0.25,
sameasEfficientNet.
ThecompletemodeldefinitionsarelistedinTable3.Thesearchitecturehyperparam-
etersworkedbestacrossasmallrangeoftestedmodels.Thechoicesareundoubtedly
suboptimalandcouldbeimprovedbyathoroughsearchofthedesignspace.Inpartic-
ular,theConvFirst-Smallmodelislikelytoowideandshallow.Simplyincreasingthe
depthofConvFirstNet-SmallmightproduceanefficientConvFirstNet-Medium.
Allmodelsusethesame256×256imageresolution.Thepower-of-twosizeensures
thattheactivationtensorsateverystageareevenlydivisiblebythematrixfragments
usedbyNVIDIAtensorcores.
456.2. WaterlineAnalysisofConvFirstNet
Figure15showswaterlineanalysesofConvFirstNet-Smallusinglayer-wiseandblock-
fusionkernels.SimilartoEfficientNet,ConvFirst-Smallisseverelymemory-boundwith
layer-wiseexecution.TheConvFirstblocksintheearlystagesandthegroupedconvolu-
tionsandSqueeze&Excitationlayersinthelatestagesallhaveverylowoperational
intensity relative to theA5000 op:byte ratio. Theresultingmaximum computational
efficiencyequals36%.
Block-fusion kernels change the picture dramatically. All but the first stage have
operationalintensityhighabovethewaterline.Theresultingmaximumcomputational
efficiencyequals97%.
Figure15illustratestheperilofusinganoff-the-shelfinferenceenginetomeasure
thecomputationalefficiencyofamodel.Hadweusedlayer-wiseexecutiontoevaluate
theefficiencyofConvFirstNet,wewouldhavejudgedittobeinferioranddiscardedit.
ButConvFirstNetcanattainveryhighcomputationalefficiencywhencomputedwith
block-fusionkernels.
Figure16showstheeffectthatincreasingop:byteratiohasonmaximumattainable
efficiency.Block-fusionkernelsenableConvFirsttoperformwellwithrelativelylittle
DRAMbandwidth;80%ofpeakarithmeticthroughputisstillattainablewhenthepro-
cessorop:byteratiois500.Layer-wiseexecutioncausesbothConvFirstandConvNeXt
tobeslowunlesstheprocessorhasaverylowop:byteratio.
46100
Kernels
1000
Convolution Pool,Multiply,Cat,orNorm 75
PointwiseConvolution ConvFirstBlock-Fusion
750 GroupedConvolution MBConvBlock-Fusion
50
500 36%
ConvFirst-S(5.5GMACs) 25
250
NVIDIAAmpereA5000GPU
0 0
0 10 20 30 40 50
MinimumLatency[ms]
97% 100
1000
75
750
ConvFirst-SwithBlock-Fusion(5.5GMACs) 50
500
25
250
NVIDIAAmpereA5000GPU
0 0
0 10 20 30 40 50
MinimumLatency[ms]
FIGURE15.WaterlineanalysisofConvFirstmodels.Layer-wiseexecutionmakesConvFirstNet
memoryboundandseverelylimitsitsmaximumcomputationalefficiency.Block-fusionkernels
makealmosttheentirenetworkcomputeboundandnearperfectcomputationalefficiencyis
attainable.
100
ConvFirst-SwithBlock-Fusion
80
60
ConvNeXt-Small(288)
40
20
ConvFirst-S
0 100 200 300 400 500 600
Processor op:byte
FIGURE16.MaximumattainablecomputationalefficiencyofConvFirstNetandConvNeXtovera
rangeofprocessorop:byteratios.Blockfusionkernelsmakethenetworksefficientevenwhen
theprocessorhassignificantlygreaterop:byteratiothanourbaselineA5000GPU.Layer-wise
executionrequiresverylowop:byteratiosforefficientoperation.
47
etyb:po
etyb:po
]%[
ycneicffiE
.xaM
0005AerepmA
]%[ycneicffiE
.xaM
]%[ycneicffiE
.xaMTABLE3.ConvFirstNet uses narrow, computationally efficient ConvFirst blocks in the early
stagesofthenetworkandwide,highcapacityMBConvblocksinthelatestages.Thedivision
betweenearlyandlatestagevisionwasinspiredbyEfficientNetV2.AllConv2dlayersuse3×3
kernels.IntheConvFirstandMBConvblocks,theConv2dlayersalsousegroup-widthequal
to8,whichmatchesthedimensionsoftheNVIDIAtensorcore instruction.The
mma.m16n8k8
point-wiseconvolutionlayersdonotusegroups.ConvFirst6isaConvFirstblockwith6timesas
manyhiddenchannelsasinputchannels,andMBConv4isanMBConvblockwith4×expansion.
MBConvblockshaveSqueeze&Excitationlayerswithsqueeze-ratioequalto0.25.TheConv
layerstemandConvFirstblocksuseReLUactivationsandMBConvusesSiLU.
ConvFirstNet-Pico
Stage Block Stride #Channels #Blocks
0 Conv 2 16 1
1 ConvFirst3 1 16 1
2 ConvFirst6 2 32 2
3 ConvFirst6 2 48 3
4 MBConv4,SE0.25 2 128 11
5 MBConv4,SE0.25 2 128 11
Head Conv1x1,Pool,FC 1280 1
ConvFirstNet-Nano
Stage Block Stride #Channels #Blocks
0 Conv 2 24 1
1 ConvFirst3 1 24 1
2 ConvFirst6 2 48 3
3 ConvFirst6 2 64 4
4 MBConv4,SE0.25 2 160 14
5 MBConv4,SE0.25 2 160 14
Head Conv1x1,Pool,FC 1280 1
ConvFirstNet-Tiny
Stage Block Stride #Channels #Blocks
0 Conv 2 24 1
1 ConvFirst3 1 24 2
2 ConvFirst6 2 48 5
3 ConvFirst6 2 72 6
4 MBConv4,SE0.25 2 192 18
5 MBConv4,SE0.25 2 192 18
Head Conv1x1,Pool,FC 1280 1
ConvFirstNet-Small
Stage Block Stride #Channels #Blocks
0 Conv 2 32 1
1 ConvFirst3 1 32 2
2 ConvFirst6 2 64 5
3 ConvFirst6 2 96 6
4 MBConv4,SE0.25 2 256 18
5 MBConv4,SE0.25 2 256 18
Head Conv1x1,Pool,FC 1280 1
486.3. ConvFirstNetModelExperiments
WeimplementedtheConvFirstmodelinPyTorch[3]andtraineditusingthePyTorch
ImageModelsframework[70].Wetrainedfor600epochswith precisionusing
float16
andaninitiallearningrateof0.128with3epochsofwarmup[20].Weused
native-amp
imagesizeequalto224×224fortrainingand256×256forinferenceforallmodelsizes.We
trainedwitheightNVIDIAA6000GPUs,eachwithbatchsize256,foratotalbatchsizeof
2048.Momentumwasequalto0.9andweight-decaywas1.0e-05.WeusedtheRMSprop
optimizerwith“TensorFlowstyleepsilon”,decayepochsequalto2.4,anddecayrate
equalto0.97.TheseoptimizersettingswereinspiredbyEfficientNet.ConvFirst-Pico
useddropoutequalto0.2andtherestused0.3.Allmodelsuseddrop-path(i.e.,stochastic
depth [29]) equal to 0.2. We used data augmentation with the auto-augment setting
,whichcorrespondstoRandAugment[9]withmagnitude9andnoise
rand-m9-mstd0.5
ofstandarddeviationequalto0.5.Wealsousedrandom-erase[77]in“pixel”modewith
probabilityequalto0.2.Weusedanexponentialmovingaverageoftheweights.
We benchmarked the baseline models using the implementations included with
PyTorch Image Models and the script. We used precision and
benchmark.py float16
, , ,andsetthebatchsize
--channels-last --torchcompile=inductor --bench=inference
equalto128.WesettheGPUandmemoryclockstothesamevaluesusedintheblock-
fusionbenchmarksintheprevioussection.
Wedidnotimplementblock-fusionkernelsforalltheblocksofConvFirstNet.Specif-
ically,ConvFirstandMBConvwithstride2,ConvFirstwith24channels,andthestem
convolutionwerenotimplemented.Howevertheunimplementedlayersaccountfor
asmallfractionofthetotalnetworkcomputation,sowecanreasonablyestimatethe
performanceoftheentirenetworkusingthemeasuredlatencyfortheimplemented
blocksandestimatesfortherest.SeeAppendixAfordetails.
6.4. Results
ThespeedandaccuracyresultsforConvFirstNetwithblock-fusionkernelsandbaseline
modelswithPyTorchInductorarelistedinTable4.ConvFirstNetwithblock-fusionhas
greatermodelefficiencythanEfficientNetandgreatercomputationalefficiencythan
ConvNeXtwithPyTorchInductor.
Figure17showsefficiencygapplotscomparingConvFirstNetandConvNeXt.Con-
vFirstNethasasignificantmodelefficiencyadvantageoverConvNeXt,andtheblock-
fusionkernelsmakeConvFirstNetmorecomputationallyefficientalso.Thecombination
49ofefficientmodelandoperationallyintensealgorithmsmakesConvFirstNet’sactual
latencylowerthanConvNeXt’sideallatency.
TABLE4.ConvFirstNetSpeedandAccuracy.OurConvFirstmodelwithblock-fusionhasgreater
modelefficiencythanEfficientNetandgreatercomputationalefficiencythanConvNeXt.The
combinedadvantagesofanefficientmodelwithmemory-efficientalgorithmsgivesConvFirst
framerates(FPS)approximately4×fasterthanthebaselineatequalaccuracy.Wemeasured
ImageNet-1KclassificationaccuracyandlatencyusinganNVIDIAAmpereA5000GPUwith76.7
TFLOP/speakarithmeticthroughputandbatchsize128.AllmodelsusedPyTorch.ConvFirst
usedourblock-fusionkernelsandtherestusedPyTorchInductor.
Model Params[M] MACs[B] Latency[ms] FPS %Peak Accuracy[%]
ConvNeXtFemto(288) 5.2 1.3 23.4 5,474.8 18.6 78.7
EfficientNetB1 7.8 0.8 49.9 2,566.8 5.2 79.1
ConvFirstPico 5.9 0.9 6.1 21,059.6 47.2 79.9
EfficientNetB2 9.1 1.1 66.1 1,935.9 5.7 80.1
ConvNeXtPico(288) 9.1 2.3 31.7 4,033.3 23.9 80.4
ConvNeXtNano(288) 15.6 4.1 46.7 2,742.5 29.0 81.5
EfficientNetB3 12.2 2.0 107.5 1,190.4 6.2 81.6
ConvFirstNano 10.2 1.8 13.5 9,447.2 44.6 81.8
ConvNeXtTiny 28.6 7.4 80.6 1,587.8 30.6 82.7
ConvFirstTiny 17.2 3.2 24.1 5,311.0 44.7 82.8
EfficientNetB4 19.3 4.5 203.7 628.3 7.4 82.9
ConvFirstSmall 28.6 5.5 33.2 3,851.0 55.2 83.3
EfficientNetB5 30.4 9.6 385.1 332.4 8.3 83.6
ConvNeXtSmall(288) 50.2 14.4 138.4 925.1 34.7 83.7
EfficientNetv2S 21.5 8.4 146.0 876.8 19.3 83.9
50(a). (b).
IIddeeaallLLaatteennccyy[[mmss]]
00 1100 2200 3300 4400 5500
ConvNeXt
ConvNeXt-S Inductor
ConvFirst
ConvFirst-S Block-Fusion
83
T T
82
N
N
81
P
80
P
79
F
0 2 4 6 8 10 12 14 0 20 40 60 80 100 120 140
MACs[billions] ActualLatency[ms]
(c). (d).
3355%%
5555%%
83
4455%% 3311%%
82
4455%%
2299%%
81
2244%%
80
4477%%
79
1199%%
0 20 40 60 80 100 120 140 101 102
IdealandActualLatency[ms] IdealandActualLatency[ms]
FIGURE17.ConvFirstwithblock-fusionversusConvNeXtwithPyTorchInductor.(a).OurCon-
vFirstmodelhassignificantlybettermodelefficiencythanConvNeXt,achievinggreateraccuracy
withfeweroperations.(b).ConvFirstextendsitsleadwhenactuallatencyismeasuredwith
ourblock-fusionkernels.(c).TheactuallatencyofConvFirstislowerthantheideallatencyof
ConvNeXt.(d).ThenarrowerefficiencygapofConvFirstresultsfromgreatercomputational
efficiencythatrangesfrom47%–55%.
51
]%[ycaruccAteNegamI
]%[ycaruccAteNegamI7. Conclusion
WepresentedanewconvnetcalledConvFirstthathasgreatermodelefficiencythan
EfficientNetandgreatercomputationalefficiencythanConvNeXt,whenimplemented
withourblock-fusionkernels.
Weobservedthatmodernconvnetsuseblockscomposedofdegenerate layers
conv2d
thathaverelativelylowoperationalintensity.Thusthetraditionallayer-wiseexecution
ofconvnetsismemoryboundwithlimitedcomputationalefficiency.
We proposed to make the convnet block the target of optimization and designed
block-fusionkernelsthatrunsignificantlyfasterthansingle-layerkernels.
Wealsodevelopedanalyticaltoolstoexaminemodelefficiency.Wederivedthesim-
pleformulasthatshowhowmodelefficiencyandcomputationalefficiencycombineto
producelatency.Wecreatedtheefficiencygapplottoshowmodelefficiencyandlatency
onthesameaxes,despitetheprevailingviewthattheseconceptsareirreconcilable.We
createdwaterlineanalysistostudythemaximumattainableefficiencyofasequenceof
parallelkernels.Wecreatedtensormachinestoillustratethemechanicsofblock-fusion
kernels.
Wehaveshownthatlargeimprovementsinconvnetefficiencycanbemadebyiden-
tifyingtheseparatemodelandcomputationalsourcesofefficiencyandco-optimizing
both.Wehavealsoshownthattheactualperformanceofefficientmodelscanbeseverely
memoryboundwhenrunwithlayer-wiseexecution.Therefore,itisnolongeraccept-
ablefordeeplearningresearcherstoestimatethecomputationalefficiencyofamodel
withoutalsostudyingtheefficiencyofthealgorithmsthatcomputeit.
Webelievethatdeeplearningmustbestudiednotjustasamathematicalmodeling
problem,butalsoasacomputerscienceproblem.Amodel’sstrengthisdetermined
notonlybytheaccuracythatitachieves,butalsobythecomputationalefficiencythat
it affords. By co-optimizing models and algorithms, we can develop a new wave of
convnetsthatdeliversignificantlygreaterperformance.
Acknowledgment
TheauthorwouldliketothankHyunggiCho,CEO&Co-FounderofPhantomAI.This
projectwouldnothavebeenpossiblewithouthisunwaveringsupport.
52References
[1] ManojAlwanietal.“Fused-layerCNNaccelerators”.In:201649thAnnualIEEE/ACM
InternationalSymposiumonMicroarchitecture(MICRO).IEEE.2016,pp.1–12.
[2] Gene M Amdahl. “Validity of the single processor approach to achieving large
scalecomputingcapabilities”.In:ProceedingsoftheApril18-20,1967,springjoint
computerconference.1967,pp.483–485.
[3] JasonAnseletal.“PyTorch2:FasterMachineLearningThroughDynamicPython
BytecodeTransformationandGraphCompilation”.In:(2024).
[4] TomMApostol.Calculus,Volume2.JohnWiley&Sons,1969.ISBN:0471000078.
[5] “Backpropagationappliedtohandwrittenzipcoderecognition”.In:Neuralcom-
putation1.4(1989),pp.541–551.
[6] Andy Brock et al. “High-performance large-scale image recognition without
normalization”. In: International Conference on Machine Learning. PMLR. 2021,
pp.1059–1071.
[7] François Chollet. “Xception: Deep learning with depthwise separable convolu-
tions”.In:ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecogni-
tion.2017,pp.1251–1258.
[8] JasonCongandBingjunXiao.“Minimizingcomputationinconvolutionalneural
networks”.In:Internationalconferenceonartificialneuralnetworks.Springer.2014,
pp.281–290.
[9] EkinDCubuketal.“Randaugment:Practicalautomateddataaugmentationwith
a reduced search space”. In: Proceedings of the IEEE/CVF conference on computer
visionandpatternrecognitionworkshops.2020,pp.702–703.
[10] ZihangDaietal.“Coatnet:Marryingconvolutionandattentionforalldatasizes”.
In:Advancesinneuralinformationprocessingsystems 34(2021),pp.3965–3977.
[11] MostafaDehghanietal.“Scalingvisiontransformersto22billionparameters”.
In:InternationalConferenceonMachineLearning.PMLR.2023,pp.7480–7512.
[12] MostafaDehghanietal.“Theefficiencymisnomer”.In:arXivpreprintarXiv:2110.12894
(2021).
[13] JiaDengetal.“Imagenet:Alarge-scalehierarchicalimagedatabase”.In:2009
IEEEconferenceoncomputervisionandpatternrecognition.Ieee.2009,pp.248–255.
53[14] Piotr Dollár, Mannat Singh, and Ross Girshick. “Fast and accurate model scal-
ing”. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition.2021,pp.924–932.
[15] AlexeyDosovitskiyetal.“Animageisworth16x16words:Transformersforimage
recognitionatscale”.In:arXivpreprintarXiv:2010.11929(2020).
[16] StefanElfwing,EijiUchibe,andKenjiDoya.“Sigmoid-weightedlinearunitsfor
neuralnetworkfunctionapproximationinreinforcementlearning”.In:Neural
networks 107(2018),pp.3–11.
[17] JohannesdeFineLichtetal.“StencilFlow:Mappinglargestencilprogramstodis-
tributedspatialcomputingsystems”.In:2021IEEE/ACMInternationalSymposium
onCodeGenerationandOptimization(CGO).IEEE.2021,pp.315–326.
[18] DHFowler.“Anapproximationtechnique,anditsusebyWallisandTaylor”.In:
Archiveforhistoryofexactsciences (1991),pp.189–233.
[19] KoenGoetschalckxandMarianVerhelst.“Breakinghigh-resolutionCNNband-
widthbarrierswithenhanceddepth-firstexecution”.In:IEEEJournalonEmerging
andSelectedTopicsinCircuitsandSystems 9.2(2019),pp.323–331.
[20] PriyaGoyaletal.“Accurate,largeminibatchsgd:Trainingimagenetin1hour”.
In:arXivpreprintarXiv:1706.02677 (2017).
[21] SuyogGuptaandMingxingTan.EfficientNet-EdgeTPU:CreatingAccelerator-Optimized
NeuralNetworkswithAutoML.https://blog.research.google/2019/08/
efficientnet-edgetpu-creating.html.2019.
[22] ScottBGuthery.Amotifofmathematics.DocentPress,2011.
[23] RichardHRHahnloseretal.“Digitalselectionandanalogueamplificationcoexist
inacortex-inspiredsiliconcircuit”.In:nature405.6789(2000),pp.947–951.
[24] KaimingHeetal.“Deepresiduallearningforimagerecognition”.In:Proceedings
oftheIEEEconferenceoncomputervisionandpatternrecognition.2016,pp.770–778.
[25] MarkDHillandAlanJaySmith.“EvaluatingassociativityinCPUcaches”.In:IEEE
TransactionsonComputers 38.12(1989),pp.1612–1630.
[26] AndrewHowardetal.“Searchingformobilenetv3”.In:ProceedingsoftheIEEE/CVF
internationalconferenceoncomputervision.2019,pp.1314–1324.
[27] AndrewGHowardetal.“Mobilenets:Efficientconvolutionalneuralnetworksfor
mobilevisionapplications”.In:arXivpreprintarXiv:1704.04861(2017).
54[28] JieHu,LiShen,andGangSun.“Squeeze-and-excitationnetworks”.In:Proceedings
oftheIEEEconferenceoncomputervisionandpatternrecognition.2018,pp.7132–
7141.
[29] Gao Huang et al. “Deep networks with stochastic depth”. In: Computer Vision–
ECCV2016:14thEuropeanConference,Amsterdam,TheNetherlands,October11–14,
2016,Proceedings,PartIV14.Springer.2016,pp.646–661.
[30] Sergey Ioffe and Christian Szegedy. “Batch normalization: Accelerating deep
networktrainingbyreducinginternalcovariateshift”.In:Internationalconference
onmachinelearning.pmlr.2015,pp.448–456.
[31] KevinJarrettetal.“Whatisthebestmulti-stagearchitectureforobjectrecogni-
tion?”In:2009IEEE12thinternationalconferenceoncomputervision.IEEE.2009,
pp.2146–2153.
[32] NormanPJouppietal.“In-datacenterperformanceanalysisofatensorprocess-
ingunit”.In:Proceedingsofthe44thannualinternationalsymposiumoncomputer
architecture.2017,pp.1–12.
[33] NormanPJouppietal.“Tenlessonsfromthreegenerationsshapedgoogle’stpuv4i:
Industrialproduct”.In:2021ACM/IEEE48thAnnualInternationalSymposiumon
ComputerArchitecture(ISCA).IEEE.2021,pp.1–14.
[34] W.T.B.Kelvin.PopularLecturesandAddresses.Natureseriesv.1.Macmillanand
Company, 1889. ISBN: 9780598775993. URL: https://books.google.com/
books?id=MInJzB0gXygC.
[35] KenKennedyandJohnRAllen.Optimizingcompilersformodernarchitectures:a
dependence-basedapproach.MorganKaufmannPublishersInc.,2001.
[36] TamaraGKoldaandBrettWBader.“Tensordecompositionsandapplications”.
In:SIAMreview 51.3(2009),pp.455–500.
[37] AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton.“Imagenetclassification
with deep convolutional neural networks”. In: Advances in neural information
processingsystems 25(2012).
[38] Andrew Lavin. Anatomy of a High-Performance MBConv Block. https://www.
linkedin.com/pulse/anatomy-high-performance-mbconv-block-
andrew-lavin.2023.
[39] AndrewLavin.“maxdnn:Anefficientconvolutionkernelfordeeplearningwith
maxwellgpus”.In:arXivpreprintarXiv:1501.06633(2015).
55[40] AndrewLavin.PhantomInferenceEngine(PiE):ASoftwareApproachtoFasterNeural
Networks. https://www.linkedin.com/pulse/phantom-inference-
engine-pie-software-approach-faster-andrew-lavin.2021.
[41] AndrewLavinandScottGray.“Fastalgorithmsforconvolutionalneuralnetworks”.
In: Proceedings of the IEEE conference on computer vision and pattern recognition.
2016,pp.4013–4021.
[42] YannLeCunetal.“Gradient-basedlearningappliedtodocumentrecognition”.
In:ProceedingsoftheIEEE86.11(1998),pp.2278–2324.
[43] ShengLietal.“Searchingforfastmodelfamiliesondatacenteraccelerators”.In:
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
2021,pp.8085–8095.
[44] MinLin,QiangChen,andShuichengYan.“Networkinnetwork”.In:arXivpreprint
arXiv:1312.4400(2013).
[45] Ze Liu et al. “Swin transformer: Hierarchical vision transformer using shifted
windows”. In: Proceedings of the IEEE/CVF international conference on computer
vision.2021,pp.10012–10022.
[46] ZhuangLiuetal.“Aconvnetforthe2020s”.In:ProceedingsoftheIEEE/CVFconfer-
enceoncomputervisionandpatternrecognition.2022,pp.11976–11986.
[47] MichaelMathieu,MikaelHenaff,andYannLeCun.“Fasttrainingofconvolutional
networksthroughffts”.In:arXivpreprintarXiv:1312.5851(2013).
[48] NVIDIA.CUDAC++ProgrammingGuide.https://docs.nvidia.com/cuda/
cuda-c-programming-guide/index.html.
[49] NVIDIA. GPU Performance Background User’s Guide. https://docs.nvidia.
com/deeplearning/performance/dl-performance-gpu-background/
index.html.AccessedonFebruary3,2024.
[50] NVIDIA. H100 tensor core GPU architecture overview. https://resources.
nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper.2022.
[51] NVIDIA.MatrixMultiplicationBackgroundUser’sGuide.https://docs.nvidia.
com/deeplearning/performance/dl-performance-matrix-multiplication/
index.html.AccessedonFebruary3,2024.
[52] NVIDIA.NVIDIAAMPEREGA102GPUARCHITECTURE.https://www.nvidia.
com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-
whitepaper-v2.pdf.2022.
56[53] NVIDIA.NVIDIATESLAV100GPUARCHITECTURE.https://images.nvidia.
com/content/volta-architecture/pdf/volta-architecture-whitepaper.
pdf.2017.
[54] NVIDIA. Parallel Thread Execution ISA Version 8.4. https://docs.nvidia.
com/cuda/parallel-thread-execution/.2024.
[55] RyosukeOkutaetal.“CuPy:ANumPy-CompatibleLibraryforNVIDIAGPUCalcu-
lations”.In:ProceedingsofWorkshoponMachineLearningSystems(LearningSys)in
TheThirty-firstAnnualConferenceonNeuralInformationProcessingSystems(NIPS).
2017.URL:http://learningsys.org/nips17/assets/papers/paper_
16.pdf.
[56] IlijaRadosavovicetal.“Designingnetworkdesignspaces”.In:Proceedingsofthe
IEEE/CVFconferenceoncomputervisionandpatternrecognition.2020,pp.10428–
10436.
[57] Jonathan Ragan-Kelley et al. “Halide: a language and compiler for optimizing
parallelism,locality,andrecomputationinimageprocessingpipelines”.In:Acm
SigplanNotices 48.6(2013),pp.519–530.
[58] Prajit Ramachandran, Barret Zoph, and Quoc V Le. “Searching for activation
functions”.In:arXivpreprintarXiv:1710.05941(2017).
[59] MarkSandleretal.“Mobilenetv2:Invertedresidualsandlinearbottlenecks”.In:
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.2018,
pp.4510–4520.
[60] PierreSermanetetal.“Overfeat:Integratedrecognition,localizationanddetec-
tionusingconvolutionalnetworks”.In:arXivpreprintarXiv:1312.6229(2013).
[61] KarenSimonyanandAndrewZisserman.“Verydeepconvolutionalnetworksfor
large-scaleimagerecognition”.In:arXivpreprintarXiv:1409.1556 (2014).
[62] SamuelLSmithetal.“Convnetsmatchvisiontransformersatscale”.In:arXiv
preprintarXiv:2310.16764(2023).
[63] MingxingTanandQuocLe.“Efficientnet:Rethinkingmodelscalingforconvolu-
tionalneuralnetworks”.In:Internationalconferenceonmachinelearning.PMLR.
2019,pp.6105–6114.
[64] MingxingTanandQuocLe.“Efficientnetv2:Smallermodelsandfastertraining”.
In:Internationalconferenceonmachinelearning.PMLR.2021,pp.10096–10106.
57[65] Mingxing Tan et al. “Mnasnet: Platform-aware neural architecture search for
mobile”.In:ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition.2019,pp.2820–2828.
[66] IlyaOTolstikhinetal.“Mlp-mixer:Anall-mlparchitectureforvision”.In:Advances
inneuralinformationprocessingsystems 34(2021),pp.24261–24272.
[67] ZhengzhongTuetal.“Maxvit:Multi-axisvisiontransformer”.In:Europeanconfer-
enceoncomputervision.Springer.2022,pp.459–479.
[68] NicolasVasilacheetal.“Fastconvolutionalnetswithfbfft:AGPUperformance
evaluation”.In:arXivpreprintarXiv:1412.7580(2014).
[69] AshishVaswanietal.“Attentionisallyouneed”.In:Advancesinneuralinformation
processingsystems 30(2017).
[70] Ross Wightman. PyTorch Image Models. https://github.com/rwightman/
pytorch-image-models.2019.DOI:10.5281/zenodo.4414861.
[71] RossWightman,HugoTouvron,andHervéJégou.“Resnetstrikesback:Anim-
provedtrainingprocedureintimm”.In:arXivpreprintarXiv:2110.00476 (2021).
[72] SamuelWilliams,AndrewWaterman,andDavidPatterson.“Roofline:aninsight-
fulvisualperformancemodelformulticorearchitectures”.In:Communicationsof
theACM52.4(2009),pp.65–76.
[73] SainingXieetal.“Aggregatedresidualtransformationsfordeepneuralnetworks”.
In: Proceedings of the IEEE conference on computer vision and pattern recognition.
2017,pp.1492–1500.
[74] XuanYangetal.“Interstellar:Usinghalide’sschedulinglanguagetoanalyzednn
accelerators”.In:ProceedingsoftheTwenty-FifthInternationalConferenceonArchi-
tecturalSupportforProgrammingLanguagesandOperatingSystems.2020,pp.369–
383.
[75] DanZhangetal.“Afull-stacksearchtechniquefordomainoptimizeddeeplearn-
ingaccelerators”.In:Proceedingsofthe27thACMInternationalConferenceonArchi-
tecturalSupportforProgrammingLanguagesandOperatingSystems.2022,pp.27–
42.
[76] RichardZhang.“Makingconvolutionalnetworksshift-invariantagain”.In:Inter-
nationalconferenceonmachinelearning.PMLR.2019,pp.7324–7334.
[77] Zhun Zhong et al. “Random erasing data augmentation”. In: Proceedings of the
AAAIconferenceonartificialintelligence.Vol.34.07.2020,pp.13001–13008.
58A. ConvFirstNet Speed Projections
WedidnotimplementalltheConvFirstandMBConvkernelsthatarerequiredtorun
ConvFirstNet.Thestride-2kernels,ConvFirstwithchannelsequalto24,andthestem
kernelsarenotimplemented.Therefore,weestimatethenetworkinferencetimeby
using the benchmark results of the implemented blocks and efficiency estimates of
theunimplementedblocks.Weestimatethatthestride-2blockswillrunat80%ofthe
efficiencyofthestride-1blocks,thattheConvFirstblockwith24channelsandexpansion
factor3willrunat40%computationalefficiency,andthestemconvolutionwillrunat
75%.
Thestemhas3inputchannelsand16,24,or32outputchannels.Itwouldbefused
withthekernelofthefirstConvFirstblock,significantlyreducingthesizeoftheblock’s
inputtensor,likelycausingtheblocktorunfaster.Wefeelthisjustifiestherelatively
highefficiencyestimateforthestem.
The unimplemented kernels are a relatively small fraction of the total network,
so inaccurate estimates will not greatly affect the accuracy of the network run time
projection.
ThefollowingtablescontainthedetailsoftheConvFirstspeedprojections.
5960
.noitcejorpdeepsociP-teNtsriFvnoC.5ELBAT
]snoillim[sPOreyaL
s/POLFT
ycnetaL
.tsE
kaeP%
latoT
htpeD
cxEzqS
jrP
pxE
vnoC
K
R
C
t
wg
sK
W
H
s
N
kcolB
]sm[
kaeP%
5.75
130.0
%0.57
61.41
1
61.41
61
3
3
652
652
2
821
metS
9.62
914.0
%1.53
80.88
1
71.52
71.52
57.73
61
84
61
3
8
3
821
821
1
821
tsriFvnoC
0.14
572.0
%5.35
80.88
1
71.52
71.52
57.73
23
69
61
6
8
3
821
821
2
821
tsriFvnoC
3.15
892.0
%9.66
45.911
1
33.05
33.05
78.81
23
291
23
6
8
3
46
46
1
821
tsriFvnoC
1.14
691.0
%6.35
19.26
1
78.81
71.52
78.81
84
291
23
6
8
3
46
46
2
821
tsriFvnoC
4.15
713.0
%0.76
04.721
2
13.82
13.82
80.7
84
882
84
6
8
3
23
23
1
821
tsriFvnoC
3.82
072.0
%0.73
87.95
1
10.0
85.21
78.81
13.82
821
291
84
4
8
3
23
23
2
821
vnoCBM
4.53
801.3
%2.64
94.068
01
70.0
55.33
55.33
78.81
821
215
821
4
8
3
61
61
1
821
vnoCBM
6.72
282.0
%0.63
88.06
1
70.0
93.8
55.33
78.81
821
215
821
4
8
3
61
61
2
821
vnoCBM
5.43
008.0
%0.54
16.512
01
70.0
93.8
93.8
27.4
821
215
821
4
8
3
8
8
1
821
vnoCBM
2.63
699.5
2.74
39.696,1
latoT61
.noitcejorpdeepsonaN-teNtsriFvnoC.6ELBAT
]snoillim[sPOreyaL
s/POLFT
ycnetaL
.tsE
kaeP%
latoT
htpeD
cxEzqS
jrP
pxE
vnoC
K
R
C
t
wg
sK
W
H
s
N
kcolB
]sm[
kaeP%
5.75
740.0
%0.57
%0.0
32.12
1
32.12
42
3
3
652
652
2
821
metS
7.03
907.0
%0.04
%0.0
78.961
1
26.65
26.65
26.65
42
27
42
3
8
3
821
821
1
821
tsriFvnoC
9.34
594.0
%3.75
%0.0
78.961
1
26.65
26.65
26.65
84
441
42
6
8
3
821
821
2
821
tsriFvnoC
9.45
881.1
%0.0
%6.17
16.905
2
52.311
52.311
13.82
84
882
84
6
8
3
46
46
1
821
tsriFvnoC
8.64
633.0
%0.16
%0.0
86.221
1
57.73
26.65
13.82
46
882
84
6
8
3
46
46
2
821
tsriFvnoC
4.85
327.0
%0.0
%2.67
03.033
3
33.05
33.05
44.9
46
483
46
6
8
3
23
23
1
821
tsriFvnoC
7.52
954.0
%5.33
%0.0
92.29
1
20.0
79.02
55.33
57.73
061
652
46
4
8
3
23
23
2
821
vnoCBM
1.23
656.6
%0.0
%9.14
91.176,1
31
01.0
34.25
34.25
95.32
061
046
061
4
8
3
61
61
1
821
vnoCBM
2.91
595.0
%0.52
%0.0
32.98
1
01.0
11.31
34.25
95.32
061
046
061
4
8
3
61
61
2
821
vnoCBM
0.42
332.2
%0.0
%3.13
08.814
31
01.0
11.31
11.31
09.5
061
046
061
4
8
3
8
8
1
821
vnoCBM
2.43
144.31
%6.44
70.595,3
latoT62
.noitcejorpdeepsyniT-teNtsriFvnoC.7ELBAT
]snoillim[sPOreyaL
s/POLFT
ycnetaL
.tsE
kaeP%
latoT
htpeD
cxEzqS
jrP
pxE
vnoC
K
R
C
t
wg
sK
W
H
s
N
kcolB
]sm[
kaeP%
5.75
740.0
%0.57
%0.0
32.12
1
00.0
00.0
00.0
32.12
42
0
3
0
0
3
652
652
2
821
metS
7.03
714.1
%0.04
%0.0
47.933
2
00.0
26.65
26.65
26.65
42
27
42
3
8
3
821
821
1
821
tsriFvnoC
9.34
594.0
%3.75
%0.0
78.961
1
00.0
26.65
26.65
26.65
84
441
42
6
8
3
821
821
2
821
tsriFvnoC
9.45
673.2
%0.0
%6.17
22.910,1
4
00.0
52.311
52.311
13.82
84
882
84
6
8
3
46
46
1
821
tsriFvnoC
0.34
083.0
%0.65
%0.0
04.721
1
00.0
74.24
26.65
13.82
27
882
84
6
8
3
46
46
2
821
tsriFvnoC
7.35
546.1
%0.07
%0.0
90.096
5
00.0
07.36
07.36
26.01
27
234
27
6
8
3
23
23
1
821
tsriFvnoC
7.52
465.0
%5.33
%0.0
72.311
1
20.0
13.82
74.24
74.24
291
882
27
4
8
3
23
23
2
821
vnoCBM
1.23
151.21
%0.0
%9.14
27.050,3
71
51.0
05.57
05.57
13.82
291
867
291
4
8
3
61
61
1
821
vnoCBM
2.91
918.0
%0.52
%0.0
38.221
1
51.0
78.81
05.57
13.82
291
867
291
4
8
3
61
61
2
821
vnoCBM
0.42
670.4
%0.0
%3.13
65.467
71
51.0
78.81
78.81
80.7
291
867
291
4
8
3
8
8
1
821
vnoCBM
3.43
079.32
%7.44
39.814,6
latoT63
.noitcejorpdeepsllamS-teNtsriFvnoC.8ELBAT
]snoillim[sPOreyaL
s/POLFT
ycnetaL
.tsE
kaeP%
latoT
htpeD
cxEzqS
jrP
pxE
vnoC
K
R
C
t
wg
sK
W
H
s
N
kcolB
]sm[
kaeP%
5.75
360.0
%0.57
%0.0
13.82
1
00.0
00.0
00.0
13.82
23
0
3
0
0
3
652
652
2
821
metS
7.53
389.1
%0.0
%6.64
56.355
2
00.0
66.001
66.001
05.57
23
69
23
3
8
3
821
821
1
821
tsriFvnoC
8.64
857.0
%0.16
%0.0
28.672
1
00.0
66.001
66.001
05.57
46
291
23
6
8
3
821
821
2
821
tsriFvnoC
4.85
858.3
%0.0
%2.67
16.167,1
4
00.0
33.102
33.102
57.73
46
483
46
6
8
3
46
46
1
821
tsriFvnoC
2.24
056.0
%0.55
%0.0
19.312
1
00.0
05.57
66.001
57.73
69
483
46
6
8
3
46
46
2
821
tsriFvnoC
7.25
329.2
%0.0
%7.86
42.302,1
5
00.0
52.311
52.311
61.41
69
675
69
6
8
3
23
23
1
821
tsriFvnoC
3.13
647.0
%8.04
%0.0
94.281
1
40.0
33.05
05.57
26.65
652
483
69
4
8
3
23
23
2
821
vnoCBM
1.93
740.71
%0.0
%0.15
95.902,5
71
62.0
22.431
22.431
57.73
652
4201
652
4
8
3
61
61
1
821
vnoCBM
5.13
538.0
%1.14
%0.0
87.502
1
62.0
55.33
22.431
57.73
652
4201
652
4
8
3
61
61
2
821
vnoCBM
4.93
932.4
%0.0
%4.15
47.503,1
71
62.0
55.33
55.33
44.9
652
4201
652
4
8
3
8
8
1
821
vnoCBM
3.24
201.33
%2.55
41.149,01
latoTTABLE9.ConvFirstNetspeedprojection.Foreachofthenetworksizes,wecopytheestimates
forthearithmeticutilization(i.e.,%Peak)andperformance(TFLOP/s)fromtheprevioustables
anduseittoestimatethelatencyofthewholenetwork’slatencybydividingMACsby(half)
ofTFLOP/s.Thisestimateassumesthatthelayersleftoutoftheprevioustables,namelythe
layersoftheclassifierhead,havethesameaverageperformanceastherestofthenetwork.The
headissmallcomparedtotherestofthenetwork,andthebulkofitisapoint-wiseconvolution
withalargechannelcount,whichlikelycouldbeperformedmoreefficientlythantherestof
thenetwork.Therefore,wearelikelyunderestimatingnetworkspeedslightlybyassumingthe
classifierheadrunsatthesamespeedastherestofthenetwork.
Model Pars[M] MACs[B] Top1[%] Input Batch Batch Device % TFLOP/s Batch FPS
Size MACs[B] TFLOP/s Peak Time[ms]
ConvFirst-P 5.91 0.86 79.858 256 128 110.1 76.7 47.23% 36.2 6.078 21,060.03
ConvFirst-N 10.17 1.812 81.834 256 128 231.9 76.7 44.64% 34.2 13.549 9,446.95
ConvFirst-T 17.24 3.227 82.752 256 128 413.1 76.7 44.69% 34.3 24.101 5,311.04
ConvFirst-S 28.55 5.493 83.312 256 128 703.1 76.7 55.16% 42.3 33.238 3,851.04
64B. Amdahl’s Roofline
What is the maximum speedup that can be achieved for a sequence of parallel ker-
nelsbyreducingtheDRAMtransfersofasinglememory-boundkernel?Ifkernel j is
memory-bound,then n j < R,anditsminimumlatencyist = b j.Byreducingthebytes
b B j B
j
transferredtobˆ = n j orless,thekernelbecomescomputeboundwithnewminimum
j R/B
n
latencyˆt = j.Therefore,themaximumspeedupequals
j R
T T
=
Tˆ T –t +ˆt
j j
1
=
(33)
t ˆt
1– j + j
T T
1
=
1– f + f
B R
where
b n
j j
(34) f = B, f = R
B R
T T
aretheminimumlatenciesofthememoryboundandcomputeboundversionsofkernel
j, measured as a fraction of the total latency before optimization, which is given by
T = ∑ t .Equation(33)canbeunderstoodasavariationofAmdahl’slaw[2]appliedto
i i
rooflineanalysis[72].
65