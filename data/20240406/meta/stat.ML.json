[
    {
        "title": "Leveraging Interpolation Models and Error Bounds for Verifiable Scientific Machine Learning",
        "authors": "Tyler ChangAndrew GilletteRomit Maulik",
        "links": "http://arxiv.org/abs/2404.03586v1",
        "entry_id": "http://arxiv.org/abs/2404.03586v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03586v1",
        "summary": "Effective verification and validation techniques for modern scientific\nmachine learning workflows are challenging to devise. Statistical methods are\nabundant and easily deployed, but often rely on speculative assumptions about\nthe data and methods involved. Error bounds for classical interpolation\ntechniques can provide mathematically rigorous estimates of accuracy, but often\nare difficult or impractical to determine computationally. In this work, we\npresent a best-of-both-worlds approach to verifiable scientific machine\nlearning by demonstrating that (1) multiple standard interpolation techniques\nhave informative error bounds that can be computed or estimated efficiently;\n(2) comparative performance among distinct interpolants can aid in validation\ngoals; (3) deploying interpolation methods on latent spaces generated by deep\nlearning techniques enables some interpretability for black-box models. We\npresent a detailed case study of our approach for predicting lift-drag ratios\nfrom airfoil images. Code developed for this work is available in a public\nGithub repository.",
        "updated": "2024-04-04 16:52:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03586v1"
    },
    {
        "title": "Distributionally Robust Reinforcement Learning with Interactive Data Collection: Fundamental Hardness and Near-Optimal Algorithm",
        "authors": "Miao LuHan ZhongTong ZhangJose Blanchet",
        "links": "http://arxiv.org/abs/2404.03578v1",
        "entry_id": "http://arxiv.org/abs/2404.03578v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03578v1",
        "summary": "The sim-to-real gap, which represents the disparity between training and\ntesting environments, poses a significant challenge in reinforcement learning\n(RL). A promising approach to addressing this challenge is distributionally\nrobust RL, often framed as a robust Markov decision process (RMDP). In this\nframework, the objective is to find a robust policy that achieves good\nperformance under the worst-case scenario among all environments within a\npre-specified uncertainty set centered around the training environment. Unlike\nprevious work, which relies on a generative model or a pre-collected offline\ndataset enjoying good coverage of the deployment environment, we tackle robust\nRL via interactive data collection, where the learner interacts with the\ntraining environment only and refines the policy through trial and error. In\nthis robust RL paradigm, two main challenges emerge: managing distributional\nrobustness while striking a balance between exploration and exploitation during\ndata collection. Initially, we establish that sample-efficient learning without\nadditional assumptions is unattainable owing to the curse of support shift;\ni.e., the potential disjointedness of the distributional supports between the\ntraining and testing environments. To circumvent such a hardness result, we\nintroduce the vanishing minimal value assumption to RMDPs with a\ntotal-variation (TV) distance robust set, postulating that the minimal value of\nthe optimal robust value function is zero. We prove that such an assumption\neffectively eliminates the support shift issue for RMDPs with a TV distance\nrobust set, and present an algorithm with a provable sample complexity\nguarantee. Our work makes the initial step to uncovering the inherent\ndifficulty of robust RL via interactive data collection and sufficient\nconditions for designing a sample-efficient algorithm accompanied by sharp\nsample complexity analysis.",
        "updated": "2024-04-04 16:40:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03578v1"
    },
    {
        "title": "Approximate Gradient Coding for Privacy-Flexible Federated Learning with Non-IID Data",
        "authors": "Okko MakkonenSampo NiemeläCamilla HollantiSerge Kas Hanna",
        "links": "http://arxiv.org/abs/2404.03524v1",
        "entry_id": "http://arxiv.org/abs/2404.03524v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03524v1",
        "summary": "This work focuses on the challenges of non-IID data and stragglers/dropouts\nin federated learning. We introduce and explore a privacy-flexible paradigm\nthat models parts of the clients' local data as non-private, offering a more\nversatile and business-oriented perspective on privacy. Within this framework,\nwe propose a data-driven strategy for mitigating the effects of label\nheterogeneity and client straggling on federated learning. Our solution\ncombines both offline data sharing and approximate gradient coding techniques.\nThrough numerical simulations using the MNIST dataset, we demonstrate that our\napproach enables achieving a deliberate trade-off between privacy and utility,\nleading to improved model convergence and accuracy while using an adaptable\nportion of non-private data.",
        "updated": "2024-04-04 15:29:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03524v1"
    },
    {
        "title": "CountARFactuals -- Generating plausible model-agnostic counterfactual explanations with adversarial random forests",
        "authors": "Susanne DandlKristin BleschTimo FreieslebenGunnar KönigJan KaparBernd BischlMarvin Wright",
        "links": "http://arxiv.org/abs/2404.03506v1",
        "entry_id": "http://arxiv.org/abs/2404.03506v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03506v1",
        "summary": "Counterfactual explanations elucidate algorithmic decisions by pointing to\nscenarios that would have led to an alternative, desired outcome. Giving\ninsight into the model's behavior, they hint users towards possible actions and\ngive grounds for contesting decisions. As a crucial factor in achieving these\ngoals, counterfactuals must be plausible, i.e., describing realistic\nalternative scenarios within the data manifold. This paper leverages a recently\ndeveloped generative modeling technique -- adversarial random forests (ARFs) --\nto efficiently generate plausible counterfactuals in a model-agnostic way. ARFs\ncan serve as a plausibility measure or directly generate counterfactual\nexplanations. Our ARF-based approach surpasses the limitations of existing\nmethods that aim to generate plausible counterfactual explanations: It is easy\nto train and computationally highly efficient, handles continuous and\ncategorical data naturally, and allows integrating additional desiderata such\nas sparsity in a straightforward manner.",
        "updated": "2024-04-04 15:10:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03506v1"
    },
    {
        "title": "LancBiO: dynamic Lanczos-aided bilevel optimization via Krylov subspace",
        "authors": "Bin GaoYan YangYa-xiang Yuan",
        "links": "http://arxiv.org/abs/2404.03331v1",
        "entry_id": "http://arxiv.org/abs/2404.03331v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03331v1",
        "summary": "Bilevel optimization, with broad applications in machine learning, has an\nintricate hierarchical structure. Gradient-based methods have emerged as a\ncommon approach to large-scale bilevel problems. However, the computation of\nthe hyper-gradient, which involves a Hessian inverse vector product, confines\nthe efficiency and is regarded as a bottleneck. To circumvent the inverse, we\nconstruct a sequence of low-dimensional approximate Krylov subspaces with the\naid of the Lanczos process. As a result, the constructed subspace is able to\ndynamically and incrementally approximate the Hessian inverse vector product\nwith less effort and thus leads to a favorable estimate of the hyper-gradient.\nMoreover, we propose a~provable subspace-based framework for bilevel problems\nwhere one central step is to solve a small-size tridiagonal linear system. To\nthe best of our knowledge, this is the first time that subspace techniques are\nincorporated into bilevel optimization. This successful trial not only enjoys\n$\\mathcal{O}(\\epsilon^{-1})$ convergence rate but also demonstrates efficiency\nin a synthetic problem and two deep learning tasks.",
        "updated": "2024-04-04 09:57:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03331v1"
    }
]