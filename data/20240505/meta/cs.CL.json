[
    {
        "title": "Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models",
        "authors": "Seungone KimJuyoung SukShayne LongpreBill Yuchen LinJamin ShinSean WelleckGraham NeubigMoontae LeeKyungjae LeeMinjoon Seo",
        "links": "http://arxiv.org/abs/2405.01535v1",
        "entry_id": "http://arxiv.org/abs/2405.01535v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01535v1",
        "summary": "Proprietary LMs such as GPT-4 are often employed to assess the quality of\nresponses from various LMs. However, concerns including transparency,\ncontrollability, and affordability strongly motivate the development of\nopen-source LMs specialized in evaluations. On the other hand, existing open\nevaluator LMs exhibit critical shortcomings: 1) they issue scores that\nsignificantly diverge from those assigned by humans, and 2) they lack the\nflexibility to perform both direct assessment and pairwise ranking, the two\nmost prevalent forms of assessment. Additionally, they do not possess the\nability to evaluate based on custom evaluation criteria, focusing instead on\ngeneral attributes like helpfulness and harmlessness. To address these issues,\nwe introduce Prometheus 2, a more powerful evaluator LM than its predecessor\nthat closely mirrors human and GPT-4 judgements. Moreover, it is capable of\nprocessing both direct assessment and pair-wise ranking formats grouped with a\nuser-defined evaluation criteria. On four direct assessment benchmarks and four\npairwise ranking benchmarks, Prometheus 2 scores the highest correlation and\nagreement with humans and proprietary LM judges among all tested open evaluator\nLMs. Our models, code, and data are all publicly available at\nhttps://github.com/prometheus-eval/prometheus-eval.",
        "updated": "2024-05-02 17:59:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01535v1"
    },
    {
        "title": "FLAME: Factuality-Aware Alignment for Large Language Models",
        "authors": "Sheng-Chieh LinLuyu GaoBarlas OguzWenhan XiongJimmy LinWen-tau YihXilun Chen",
        "links": "http://arxiv.org/abs/2405.01525v1",
        "entry_id": "http://arxiv.org/abs/2405.01525v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01525v1",
        "summary": "Alignment is a standard procedure to fine-tune pre-trained large language\nmodels (LLMs) to follow natural language instructions and serve as helpful AI\nassistants. We have observed, however, that the conventional alignment process\nfails to enhance the factual accuracy of LLMs, and often leads to the\ngeneration of more false facts (i.e. hallucination). In this paper, we study\nhow to make the LLM alignment process more factual, by first identifying\nfactors that lead to hallucination in both alignment steps:\\ supervised\nfine-tuning (SFT) and reinforcement learning (RL). In particular, we find that\ntraining the LLM on new knowledge or unfamiliar texts can encourage\nhallucination. This makes SFT less factual as it trains on human labeled data\nthat may be novel to the LLM. Furthermore, reward functions used in standard RL\ncan also encourage hallucination, because it guides the LLM to provide more\nhelpful responses on a diverse set of instructions, often preferring longer and\nmore detailed responses. Based on these observations, we propose\nfactuality-aware alignment, comprised of factuality-aware SFT and\nfactuality-aware RL through direct preference optimization. Experiments show\nthat our proposed factuality-aware alignment guides LLMs to output more factual\nresponses while maintaining instruction-following capability.",
        "updated": "2024-05-02 17:54:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01525v1"
    },
    {
        "title": "D2PO: Discriminator-Guided DPO with Response Evaluation Models",
        "authors": "Prasann SinghalNathan LambertScott NiekumTanya GoyalGreg Durrett",
        "links": "http://arxiv.org/abs/2405.01511v1",
        "entry_id": "http://arxiv.org/abs/2405.01511v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01511v1",
        "summary": "Varied approaches for aligning language models have been proposed, including\nsupervised fine-tuning, RLHF, and direct optimization methods such as DPO.\nAlthough DPO has rapidly gained popularity due to its straightforward training\nprocess and competitive results, there is an open question of whether there\nremain practical advantages of using a discriminator, like a reward model, to\nevaluate responses. We propose D2PO, discriminator-guided DPO, an approach for\nthe online setting where preferences are being collected throughout learning.\nAs we collect gold preferences, we use these not only to train our policy, but\nto train a discriminative response evaluation model to silver-label even more\nsynthetic data for policy training. We explore this approach across a set of\ndiverse tasks, including a realistic chat setting, we find that our approach\nleads to higher-quality outputs compared to DPO with the same data budget, and\ngreater efficiency in terms of preference data requirements. Furthermore, we\nshow conditions under which silver labeling is most helpful: it is most\neffective when training the policy with DPO, outperforming traditional PPO, and\nbenefits from maintaining a separate discriminator from the policy model.",
        "updated": "2024-05-02 17:44:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01511v1"
    },
    {
        "title": "Analyzing the Role of Semantic Representations in the Era of Large Language Models",
        "authors": "Zhijing JinYuen ChenFernando GonzalezJiarui LiuJiayi ZhangJulian MichaelBernhard SchölkopfMona Diab",
        "links": "http://arxiv.org/abs/2405.01502v1",
        "entry_id": "http://arxiv.org/abs/2405.01502v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01502v1",
        "summary": "Traditionally, natural language processing (NLP) models often use a rich set\nof features created by linguistic expertise, such as semantic representations.\nHowever, in the era of large language models (LLMs), more and more tasks are\nturned into generic, end-to-end sequence generation problems. In this paper, we\ninvestigate the question: what is the role of semantic representations in the\nera of LLMs? Specifically, we investigate the effect of Abstract Meaning\nRepresentation (AMR) across five diverse NLP tasks. We propose an AMR-driven\nchain-of-thought prompting method, which we call AMRCoT, and find that it\ngenerally hurts performance more than it helps. To investigate what AMR may\nhave to offer on these tasks, we conduct a series of analysis experiments. We\nfind that it is difficult to predict which input examples AMR may help or hurt\non, but errors tend to arise with multi-word expressions, named entities, and\nin the final inference step where the LLM must connect its reasoning over the\nAMR to its prediction. We recommend focusing on these areas for future work in\nsemantic representations for LLMs. Our code:\nhttps://github.com/causalNLP/amr_llm.",
        "updated": "2024-05-02 17:32:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01502v1"
    },
    {
        "title": "Controllable Text Generation in the Instruction-Tuning Era",
        "authors": "Dhananjay AshokBarnabas Poczos",
        "links": "http://arxiv.org/abs/2405.01490v1",
        "entry_id": "http://arxiv.org/abs/2405.01490v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01490v1",
        "summary": "While most research on controllable text generation has focused on steering\nbase Language Models, the emerging instruction-tuning and prompting paradigm\noffers an alternate approach to controllability. We compile and release\nConGenBench, a testbed of 17 different controllable generation tasks, using a\nsubset of it to benchmark the performance of 9 different baselines and methods\non Instruction-tuned Language Models. To our surprise, we find that\nprompting-based approaches outperform controllable text generation methods on\nmost datasets and tasks, highlighting a need for research on controllable text\ngeneration with Instruction-tuned Language Models in specific. Prompt-based\napproaches match human performance on most stylistic tasks while lagging on\nstructural tasks, foregrounding a need to study more varied constraints and\nmore challenging stylistic tasks. To facilitate such research, we provide an\nalgorithm that uses only a task dataset and a Large Language Model with\nin-context capabilities to automatically generate a constraint dataset. This\nmethod eliminates the fields dependence on pre-curated constraint datasets,\nhence vastly expanding the range of constraints that can be studied in the\nfuture.",
        "updated": "2024-05-02 17:24:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01490v1"
    }
]