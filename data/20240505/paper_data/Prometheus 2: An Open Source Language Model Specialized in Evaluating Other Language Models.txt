PROMETHEUS 2: An Open Source Language Model Specialized in
Evaluating Other Language Models
SeungoneKim1,2,3∗ JuyoungSuk1∗ ShayneLongpre4 BillYuchenLin5 JaminShin1
SeanWelleck3 GrahamNeubig3 MoontaeLee2,6 KyungjaeLee2 MinjoonSeo1
KAISTAI1 LGAIResearch2 CarnegieMellonUniversity3 MIT4
AllenInstituteforAI5 UniversityofIllinoisChicago6
seungone@cmu.edu {juyoung, minjoon}@kaist.ac.kr
Abstract
GPT-4

Weak Evaluator
 Claude-3
 Scores
ProprietaryLMssuchasGPT-4areoftenem- Group -Opus

Scores
ployedtoassessthequalityofresponsesfrom GPT-3.5
 Human

various LMs. However, concerns including Scores Scores
High

transparency,controllability,andaffordability Correlation
Low

strongly motivate the development of open- Llama-2-70B
 Correlation Prometheus 2

sourceLMsspecializedinevaluations. Onthe Scores Scores
Prometheus

otherhand,existingopenevaluatorLMsexhibit Scores Strong Evaluator

criticalshortcomings: 1)theyissuescoresthat Group
significantlydivergefromthoseassignedbyhu-
mans,and2)theylacktheflexibilitytoperform
Figure 1: Weak evaluators (e.g., Llama-2-Chat-70B,
both direct assessment and pairwise ranking,
Prometheus,andGPT-3.5-Turbo)achievelowscoring
the two most prevalent forms of assessment.
correlationwithstrongevaluators(e.g.,Humans,GPT-4,
Additionally, they do not possess the ability
andClaude-3-Opus).Ontheotherhand,scoresprovided
toevaluatebasedoncustomevaluationcrite-
bystrongevaluatorshighlycorrelatewitheachother.
ria,focusinginsteadongeneralattributeslike
helpfulnessandharmlessness. Toaddressthese
issues,weintroducePrometheus2,amorepow- 2023;Liuetal.,2023b;Yeetal.,2023;Kimetal.,
erful evaluator LM than it’s predecessor that 2023)ortodeterminewhichoftwooutputsarepre-
closelymirrorshumanandGPT-4judgements. ferred(denotedaspairwiseranking)(Wangetal.,
Moreover,itiscapableofprocessingbothdi-
2023b;Lietal.,2023b;Lambertetal.,2024). Prior
rectassessmentandpair-wiserankingformats
works employing proprietary LMs as evaluators
groupedwithauser-definedevaluationcriteria.
havedemonstratednotonlyhighcorrelationswith
Onfourdirectassessmentbenchmarksandfour
human evaluations but also increased speed and
pairwiserankingbenchmarks,PROMETHEUS
2scoresthehighestcorrelationandagreement cost-effectiveness (Zheng et al., 2023; Liu et al.,
withhumansandproprietaryLMjudgesamong 2023b;Duboisetal.,2023;Yeetal.,2023).
all tested open evaluator LMs. Our models, However,relyingonproprietaryLMsforevalua-
code,anddataareallpubliclyavailable1.
tionposessignificantchallenges. Thelackoftrans-
parencyabouttheirtrainingdatacompromisesboth
1 Introduction
fairness and compliance, making it problematic
tousetheminevaluationpipelines. Additionally,
Evaluatingthequalityofoutputsproducedbylan-
concernsregardingcontrollabilityandaffordability
guage models (LMs) is progressively becoming
also persist (Kim et al., 2023). To address these
difficult, as the outputs cover an extremely di-
issues, recent works have focused on developing
verse distribution of text and complex tasks. To
evaluator LMs that are open-access, transparent,
address this issue, language model-based evalua-
and controllable (Kim et al., 2023; Wang et al.,
tionhasemergedasascalableandcheapparadigm
2023a,b; Li et al., 2023a; Zhu et al., 2023; Jiang
for assessing LM-generated text (Li et al., 2024;
etal.,2023b,c;Leeetal.,2024). Yet,thesemodels
Gao et al., 2024). In this paradigm, LMs are ei-
oftenyieldscoringdecisionsthatdonotcorrelate
therpromptedtooutputascalarindicatorofqual-
wellenoughwithhumanjudgmentsorthosemade
ity (denoted as direct assessment) (Zheng et al.,
by proprietary LMs, failing to effectively simu-
∗equalcontribution.WorkwasdonewhileSeungonewas
latethem. Moreover,openevaluatorLMsarenot
aninternatLGAIResearch.
1https://github.com/prometheus-eval/prometheus-eval flexiblesincetheyaretypicallytrainedonlytoper-
4202
yaM
2
]LC.sc[
1v53510.5042:viXraformeitherdirectassessmentorpairwiseranking • Weshowthatmergingtheweightsofevaluator
andassessbasedongeneralpublicpreferenceslike LMstrainedondirectassessmentandpairwise
helpfulnessandharmlessness,limitingtheirability rankingfeedbackdatasetsresultsinaunified
tohandlediversereal-lifescenarios. evaluatorLMthatexcelsinbothschemes.
To close the gap with proprietary LMs, we in-
2 RelatedWork
vestigateunifyingthetwomodel-basedevaluation
paradigms-directassessmentandpairwiseranking
2.1 LanguageModel-basedEvaluation
-totrainarobustunifiedevaluatorLM.Wepropose
ToassessthegenerationcapabilitiesofLMs,prior
arecipebasedonmergingtheweightsoftwoeval-
works such as the GEM benchmark (Gehrmann
uatorLMstrainedseparatelyondirectassessment
et al., 2021, 2022) employed Rouge (Lin,
andpairwiserankingformats. Ourkeyempirical
2004), BLEU (Papineni et al., 2002), and
observation is that weight merging can yield an
BERTScore (Zhang et al., 2019) as their metric,
evaluatorLMthatnotonlyworksinbothformats,
whichmeasuresthelexicalorsemanticsimilarity
butalsooutperformsevaluatorLMsthatarejointly
betweenareferenceanswerandaresponse. How-
trainedoronlytrainedonasingleformat.
ever,theseconventionalmetricsarepronetofalse
To demonstrate our approach, we develop the
negativesbecausetheyarenotexpressiveenough
PREFERENCE COLLECTION, a new fine-grained
torecognizeresponsesthatareofgoodqualitybut
pairwise ranking feedback dataset that builds on
differfromthereferenceanswer(Schluter,2017;
the FEEDBACK COLLECTION (Kimetal.,2023),
Freitagetal.,2020;HannaandBojar,2021).
whichisadirectassessmentfeedbackdataset. We
Recently,employinglanguagemodelsasajudge
chooseMistral-7B(Jiangetal.,2023a)andMixtral-
has gained attention as a promising paradigm to
8x7B(Jiangetal.,2024)asourbasemodels,and
mimicthedepthandgranularitythathumanevalu-
merge the weights of evaluator LMs separately
ationoffers(Zhengetal.,2023;Liuetal.,2023b;
trained on the FEEDBACK COLLECTION and the
Lietal.,2023b;Chanetal.,2023;Yeetal.,2023).
PREFERENCECOLLECTIONtoobtainourresulting
To reduce the over-reliance on proprietary LMs,
models, PROMETHEUS 2 (7B&8x7B).
follow-upworkssuggesttraininglanguagemodels
Onfourdirectassessmentbenchmarks(Vicuna
specialized in evaluations (Cui et al., 2023; Kim
Bench,MTBench,FLASK,FeedbackBench),the
etal.,2023;Jiangetal.,2023b,c;Lietal.,2023a;
PROMETHEUS 2 modelsdemonstratethehighest
Lee et al., 2024). Yet, open evaluator LMs do
correlation with both human evaluators and pro-
not possess the flexibility to function in different
prietary LM-based judges compared to existing
evaluationschemesandshowweakevaluationper-
open evaluator LMs, with the Pearson correla-
formancescomparedtoproprietaryLMs. Weaim
tionsurpassingotherbaselinesby0.2unitsacross
all datasets. Similarly, on four pairwise ranking
tobridgethisgapbyintroducing PROMETHEUS 2.
benchmarks(HHHAlignment,MTBenchHuman
2.2 WeightMerging
Judgment, Auto-J Eval, Preference Bench), the
Priorworkshavedemonstratedthatweightmerg-
PROMETHEUS 2 models show the highest agree-
ing can enhance performances across various do-
ment with human evaluators among all the open
mains, including language modeling (Li et al.,
evaluatorLMswetested,reducingtheperformance
2022; Matena and Raffel, 2022; Ilharco et al.,
gapwithGPT-4inhalf.
2022;Don-Yehiyaetal.,2022;Gururanganetal.,
Ourcontributionsaresummarizedasfollows:
2023;Yadavetal.,2024;Sukhbaataretal.,2024),
• Weintroduce PROMETHEUS 2 (7B&8x7B), instruction-tuning (Jang et al., 2023b; Yu et al.,
state-of-the-artopenevaluatorLMsthatscore 2023),andaligningtouserperferences(Jangetal.,
highcorrelationswithbothhumanevaluators 2023a; Rame et al., 2024; Wang et al., 2024). In
andproprietaryLM-basedjudgesonbothdi- ourwork,wespecificallyfocusonenhancingthe
rectassessmentandpairwiseranking. evaluationcapabilitiesofopenevaluatorLMs. By
mergingmodelstrainedondifferentassessmentfor-
• We introduce a pairwise ranking feedback mats—specifically,directassessmentandpairwise
dataset called the PREFERENCE COLLEC- ranking—we aim to obtain an evaluator LM that
TION, whichincludes1Kcustomevaluation notonlyfunctionsinbothformatsbutalsoshowsas
criteriabeyondhelpfulnessandharmlessness. goodevaluationperformancesasproprietaryLMs.Instruction
In the field of software development, what is the meaning and significance of “Containerization”? Also, explain the role of Docker in containerization.
Response
 Containerization, in software, is somewhat like putting Response
 Containerization in software development refers to the
things in a box. It means you take your software and all process of packaging up an application along with all its
A B
its parts and put it in a container. Docker is a tool that related configurations files, libraries, and dependencies
helps with this. It helps to put the software in containers required to run, into a standalone unit or a ‘container’. [...]
and makes it easy to use them. [...]
Pairwise Ranking Direct Assessment
Evaluation Criteria
A vs B + Does the response accurately use A + Does the response use simple language and

specific industry terminologies and jargons? explanation that are easy to understand for a beginner?
Verbal Feedback Verbal Feedback
Both response attempt to convey the fundamental concept of containerization, The response effectively uses simple and accessible language to explain
but with varying degrees of clarity and technical details. Response A approaches containerization and Docker, which is great for beginners. The analogy of putting
the concept by likening containerization to “putting things in a box”, a metaphor things in a box is particularly helpful as it visually illustrates the concept of [...]


that while easy to understand, lacks the precision and industry-specific [...]
However, the response could be improved by briefly mentioning why
On the other hand, Response B more effectively employs technical jargon such
 containerization is significant, such as its benefits in ensuring that software runs
as “packaging”, “configuration files”, “libraries”, and “dependencies”. [...] consistently across different computing environments. It loses a point for not
fully addressing the significance of containerization in the broader context of
It can be concluded that Response B is better than Response A. software development, which could add valuable insight for the reader.
Scoring Decision Scoring Decision
B
Figure2: Comparisonofdirectassessmentandpairwiseranking. Bothresponsescouldbeconsidereddecentunder
theumbrellaof‘helpfulness’. However,thescoringdecisionmightchangebasedonaspecificevaluationcriterion.
3 Methodology uations are flexible to specific needs rather than
genericqualities. Specifically,eisrepresentedas
We propose a new recipe for training a unified
a score rubric including a description for the cri-
evaluator LM based on merging the weights of
teriaitselfandasetofdescriptionsforeachscore
modelstrainedfordirectassessmentandpairwise
betweenthescoringrange. Thisisexpressedas:
ranking. We begin with background on direct as-
f : (i,r,a,e) (cid:55)→ (v ,s)
sessmentandpairwiserankingforevaluatorLMs direct r
(1)
(Section 3.1, 3.2), followed by the construction wheres ∈ {1,2,3,4,5}
processofourtrainingdata(Section3.3). Finally,
3.2 PairwiseRanking
wepresentourmethodstotrainthestate-of-the-art
Pairwise ranking is mapping an instruction i and
evaluatorLM,Prometheus2models(Section3.4).
two pair of responses (r , r ) into either i or j,
m n
suchasf : (i,r ,r ) (cid:55)→ swheres ∈ {m,n}.
3.1 DirectAssessment pair m n
Similar to direct assessment, prior works have
Directassessmentismappinganinstructioniand identifiedthatintegratingareferenceansweraand
response r into a scalar value score s, such as verbalfeedbackv intotheevaluationpipeline
rm,rn
f direct : (i,r) (cid:55)→ swheres ∈ R. For the scor- is crucial (Zheng et al., 2023; Li et al., 2023b,a).
ingrange,weusea1-5Likertscalescoring. Inaddition,tosupportgranularassessmentunder
Prior works have identified several recipes to customcriterion,weaddtheevaluationcriteriae
alignthescoresprovidedbyevaluatorLMs(s ) asinputtotheevaluatorLM(Yeetal.,2023;Kim
LM
andthescoresassignedbyhumans(s ). For etal.,2023). Tothebestofourknowledge,weare
human
instance,Liuetal.(2023a)andZhengetal.(2023) the first to study such fine-grained evaluation in
haveshownthatitiscrucialtoaddareferencean- pairwiserankingsettings. Thisisexpressedas:
swera as inputto theevaluator LM tomaximize
f : (i,r ,r ,a,e) (cid:55)→ (v ,s)
the correlation between s and s . Also,
pair m n rm,rn
(2)
LM human
wheres ∈ {m,n}
Zheng et al. (2023) and Ye et al. (2023) showed
thatpromptingthelanguagemodeltowriteverbal Inpairwiseranking,theevaluationcriteriaedo
feedbackv beforesalsoimprovesthecorrelation not include a set of descriptions for each score;
r
betweens ands . Lastly,Yeetal.(2023) instead,onlythedescriptionoftheevaluationcri-
LM human
and Kim et al. (2023) showed that by explicitly terionitself. Also,itisnoteworthythattheverbal
integrating evaluation criteria e, users can define feedbackv comparesthecommonalitiesand
rm,rn
thestandardsformodelassessment,ensuringeval- differencesbetweenr andr concerninge.
m nPREFERENCE FEEDBACK Single-FormatTraining Single-Formattraining
Data
COLLECTION COLLECTION involvestrainingabasemodelθ oneitheronadi-
EvaluationScheme PairwiseRanking DirectAssessment rectassessmentfeedbackdatasetD d orapairwise
#EvaluationCriteria 1,000 1,000 rankingfeedbackdatasetD .
p
#Instructions 20,000 20,000
#ReferenceAnswer 20,000 20,000
JointTraining Jointtraininginvolvestraininga
#Instances 200,000 100,000
#VerbalFeedback 200,000 100,000 basemodelθ onbothadirectassessmentfeedback
datasetD andapairwiserankingfeedbackdataset
d
Table 1: Statistics of our training datasets, the FEED- D . This enables the resulting evaluator LM to
p
BACK COLLECTION andthe PREFERENCE COLLEC- functionacrossbothevaluationformats.
TION. Notethatthe1Kevaluationcriteria,20Kinstruc-
tions,and20Kreferenceanswersareshared amongthe WeightMerging WeightMerginginvolvestrain-
two datasets. Both datasets have an equal number of
ing two models, θ and θ , separately on a direct
d p
scoringdecisions(“A”or“B”;100Keach&1-5;20K
assessment feedback dataset D and a pairwise
d
each)topreventunintendedbiasesaftertraining.
rankingfeedbackdatasetD . Then,weobtainthe
p
finalevaluatorLMθ withlinearmerging:
final
3.3 ThePreferenceCollection
θ = α×θ +(1−α)×θ (3)
final d p
Popular pairwise ranking datasets such as HH-
We conduct experiments by using α = 0.5. In
RLHF (Bai et al., 2022) or Ultra Feedback (Cui
Section6.3,weobservehowalteringthecoefficient
et al., 2023) do not include an evaluation criteria
eandaverbalfeedbackv . Toobtainaneval-
αaffectsdownstreamperformanceoneachevalua-
rm,rn
tionscheme. Weempiricallyfindthatthissimple
uator LM that could assess based on what users
recipe work best when we choose Mistral-7B as
care about, we construct the PREFERENCE COL-
ourbasemodel. Inadditiontolinearmerging,we
LECTIONthatincludes1Kevaluationcriteria.
alsotestdifferentmergingtechniquesincluding:
ConstructionProcess Toconstructthe PREFER-
• Task Arithmetic merging (Ilharco et al.,
ENCE COLLECTION,weapplytwomodifications
2022)whichcanbeexpressedasfollows:
tothe FEEDBACK COLLECTION. First,sincethe
FEEDBACK COLLECTION includesfiveresponses
θ = θ +α×(θ −θ )+
final init d init
foreachinstruction,eachcorrespondingtoascor- (4)
(1−α)×(θ −θ )
ingdecisionbetween1and5, wepairtwooutof p init
thefiveresponses,resultinginatotaloftencombi-
where θ is the weight of the base model.
init
nationsperinstruction. Usingtheexistingscoring
However,weempiricallyfindthattheresult-
decisionsforeachresponse,wedeterminewhich
ingevaluatorLMθ oftendoesnotgener-
final
response is better and assign a new scoring deci-
atevalidscoringdecisions(e.g.,generatingan
sion for that pair (i.e., “Response A is better” or
integerduringpairwiseranking).
“ResponseBisbetter”). Second,togeneratenew
verbalfeedbackv rm,rn foreachpairofresponses, • TIES merging (Yadav et al., 2024), while
wepromptGPT-4-1106toidentifythecommonali- similartoTaskArithmeticmerging,adds(1)a
tiesanddifferencesofthetworesponses. Trimoperationtoremoveredundantweights
Thestatisticsoftheresultingdatasetarelistedin in the task vector θ − θ and θ − θ
d init p init
Table1alongwiththe FEEDBACK COLLECTION. and(2)ElectandDisjointoperationsto
Weexplainaboutourqualityverificationprocess resolvedisagreement(i.e.,oppositedirected
ofthePREFERENCE COLLECTIONinAppendixA. weights)betweenθ d−θ
init
andθ p−θ init.
Also, weincludethepromptsweusefortheaug-
• DAREmerging(Yuetal.,2023),whilealso
mentationprocessinAppendixF.
similartoTaskArithmeticandTIESmerging,
performsaRandom DropandRe-scale
3.4 EmployingEvaluatorLanguageModels
operations in the task vector θ − θ and
d init
Prompting PromptinginvolvesqueryinganLM θ −θ toremoveredundantweights. We
p init
tomakejudgmentsinaspecifiedevaluationformat findthatDAREmergingworkbestwhenwe
withouttrainingonanyfeedbackdataset. chooseMixtral-8x7Basourbasemodel.DIRECTASSESSMENTBENCHMARKS PAIRWISERANKINGBENCHMARKS
VICUNA MT FEEDBACK HHH MTBENCH AUTO-J PREFERENCE
FLASK
BENCH BENCH BENCH ALIGN. HUMANJUDG. Eval BENCH
ProprietaryLMs
JudgmentSource ProprietaryLMs ProprietaryLMs ProprietaryLMs Humans Humans Humans ProprietaryLMs
&Humans
Metrics Correlation Correlation Correlation Correlation Accuracy Accuracy Accuracy Accuracy
ReferenceAnswer Y Y Y Y N N N Y
#ScoreRubrics 80 80 12 200 4 1 1 200
#Instructions 80 80 200 200 221 80 58 200
#Judgments 320 320 2,000 1,000 221 3,360 1,392 2,000
Table2: StatisticsofourevaluationbenchmarkstoassesstheevaluationcapabilitiesofevaluatorLMs.
4 ExperimentalSetup rubrics (helpfulness, harmlessness, honesty,
andother)and221responsepairs(gradedas
Inthissection,weexplainourexperimentalsetup
‘win’or‘lose’)judgedbyhumanevaluators.
toassessevaluatorLMs. Wefirstexplainthebench-
marksandmetricsweemploy(Section4.1)andthe • MTBenchHumanJudgment(Zhengetal.,
baselinesweuseasevaluatorLMs(Section4.2). 2023): Abenchmarkthatsharesthesame80
promptsasMT-Bench. Inaddition,itprovides
4.1 BenchmarksandMetrics
3,360responsepairs(gradedas‘win’,‘tie’,or
ThestatisticsofallthebenchmarksareinTable2. ‘lose’)judgedbyhumanevaluators.
Thefourdirectassessmentbenchmarksare:
• Auto-JEval(Lietal.,2023a): Abenchmark
consisted of 58 prompts and 1,392 response
• VicunaBench(Chiangetal.,2023): Asingle-
pairs(gradedas‘win’,‘tie’,or‘lose’)judged
turn chat benchmark that includes 80 test
byhumanevaluators. Thisbenchmarkisused
prompts,80hand-craftedscorerubricsfrom
asthein-domaintestsetofAuto-J.
Kimetal.(2023),and320responsesobtained
by WizardLM-13B, Vicuna-13B, Llama-2-
• PreferenceBench: Ourin-domaintestsetfor
Chat-13B,GPT-3.5-Turbo-0613.
thePROMETHEUSmodels. Similartohowthe
PREFERENCE COLLECTION wasmadewith
• MT Bench (Zheng et al., 2023): A multi-
the FEEDBACK COLLECTION,weadjustthe
turn chat benchmark that consists of 80 test
FEEDBACK BENCH and pair two out of the
prompts,80hand-craftedscorerubricsfrom
fiveresponses,resultinginatestsetwith200
Kimetal.(2023),and320responsesobtained
prompts,2,000responsepairs(gradedas‘win’
by WizardLM-13B, Vicuna-13B, Llama-2-
or‘lose’),and200evaluationcriteria.
Chat-13B,GPT-3.5-Turbo-0613.
In direct assessment, we conduct reference-
• FLASK (Ye et al., 2023): A fine-grained
basedevaluationsbyappendingthereferencean-
evaluationbenchmarkcomprisedof200test
swerastheinput. WeusePearson,Spearman,and
prompts,12scorerubrics,and2000responses
Kendall-Tauasperformancemetricstomeasure
acquiredfromAlpaca-7B,Vicuna-13B,Bard,
scoringcorrelationsagainstreferenceevaluators.
GPT-3.5-Turbo-0613. In addition to scores
Inpairwiseranking,weconductreference-free
from proprietary LMs, this benchmark also
evaluations. Basedonjudgmentsassignedbyhu-
includesscoresmarkedbyhumanevaluators.
mans,weuseaccuracyasourmetrictomeasure
agreementbetweenevaluatorLMsandhumans.
• FeedbackBench(Kimetal.,2023): Thetest
Also,theMTBenchHumanJudgmentandAuto-
setofthe FEEDBACK COLLECTION with1K
Jtestsetincludesa‘tie’optionassessedbyhuman
score rubrics, 200 instructions, and 1K re-
evaluators. Weevaluateintwoways: byexcluding
sponsesthatdonotoverlapwiththetraindata.
all ‘tie’ options for pairwise ranking (denoted as
Thefourpairwiserankingbenchmarksare: ‘w/otie’),orbyusingdirectassessmentwherere-
sponsesscoredas‘ties’aregrouped,andpairwise
• HHH Alignment (Askell et al., 2021): A rankings are applied to the remaining responses
benchmarkconsistingof221prompts;4score withdifferingscores(denotedas‘w/tie’).VICUNABENCH MTBENCH FLASK FeedbackBench
EvaluatorLM
GPT-4-1106 Claude-3-Opus GPT-4-1106 Claude-3-Opus GPT-4-1106 Claude-3-Opus Humans GPT-4-0613
LLAMA2-CHAT7B 0.205 0.243 0.036 0.055 0.317 0.256 0.299 0.523
LLAMA2-CHAT13B 0.185 0.141 -0.042 -0.002 0.239 0.247 0.263 0.545
LLAMA2-CHAT70B 0.350 0.463 0.178 0.228 0.388 0.402 0.317 0.592
MISTRAL-INSTRUCT-7B 0.486 0.561 0.284 0.396 0.448 0.437 0.377 0.586
MIXTRAL-INSTRUCT-8X7B 0.566 0.579 0.551 0.539 0.483 0.495 0.420 0.673
PROMETHEUS-7B 0.484 0.528 0.378 0.382 0.352 0.331 0.348 0.847
PROMETHEUS-13B 0.492 0.534 0.404 0.477 0.462 0.470 0.449 0.860
AUTO-J(13B) 0.351 0.262 0.432 0.375 0.430 0.370 0.473 0.637
PROMETHEUS-2-7B 0.642 0.610 0.543 0.554 0.645 0.578 0.544 0.878
PROMETHEUS-2-8X7B 0.685 0.635 0.665 0.614 0.659 0.626 0.555 0.898
GPT-3.5-TURBO-0613 0.335 0.349 0.183 0.194 0.437 0.396 0.450 0.594
GPT-4-1106 / 0.694 / 0.717 / 0.736 0.679 0.753
CLAUDE-3-OPUS 0.694 / 0.717 / 0.736 / 0.573 0.788
Table3: DirectAssessmentResultsPearsoncorrelationsbetweenreferenceevaluators(listedontop)andevaluatorLMs.
ThebestcomparablestatisticsareboldedandsecondbestunderlinedexceptproprietaryLMs. SpearmanandKendall-Tau
correlationsarereportedinAppendixC.NotethattheFeedbackBenchisanin-domaintestsetofthePROMETHEUSmodels.
4.2 Baselines 5 ExperimentalResults
PromptingBaselines WeemployLlama-2-Chat-
5.1 DirectAssessmentResults
7,13,70B (Touvron et al., 2023); Mistral-7B-
ThedirectassessmentresultsareshowninTable3.
Instruct-v0.2 (Jiang et al., 2023a); and Mixtral-
8x7B-Instruct-v0.1(Jiangetal.,2024)asourbase-
Thescoringdecisionsof PROMETHEUS-2 models
(7B & 8x7B), GPT-4-1106, Claude-3-Opus, and
lines. It’sworthnotingthatmodelsnotexplicitly
humanevaluatorsallstronglycorrelatewitheach
trainedonfeedbackdataoftenfailtogeneratere-
other,yieldingPearsoncorrelationshigherthan0.5
sponsesintherequiredformat,makingitextremely
regardless of the reference evaluator and bench-
difficulttoparsescoringdecisions. Althoughitis
mark. Ontheotherhand,baseLMs,single-format
impracticalforregularuse,wemakeafaircompari-
trainedLMs,andjointlytrainedLMsshowlower
sonbyinfinitelyloopinguntilscorescanbeparsed.
correlationswithGPT-4-1106,Claude-3-Opus,and
Also,weincludeproprietaryLMssuchasGPT-3.5-
humans,mostlyfallingbelow0.5.
Turbo-0613;GPT-4-1106;andClaude-3-Opus.
Notably, PROMETHEUS 2 models outperform
Single-Format Trained Evaluator LMs For PrometheusandAuto-Jbyatleast0.2unitsacross
single-format trained evaluator LMs, we test benchmarks in their correlation with proprietary
Prometheus-7,13B(Kimetal.,2023)(directassess- LMs. Moreover,ontheFLASKbenchmark,while
ment); UltraRM-13B(Cuietal.,2023)(pairwise the correlation between humans and GPT-4 is
ranking); and PairRM-0.4B (Jiang et al., 2023c) 0.679,thehighestcorrelationpreviouslyachieved
(pairwiseranking). Inaddition,wealsoreportthe by Prometheus-13B with humans was 0.449, but
performancesofsingle-formattrainingMistral-7B- PROMETHEUS-2-8X7Bachievesacorrelationof
Instruct-v0.2 and Mixtral-8x7B-Instruct-v0.1 on 0.555withhumans,effectivelyhalvingthegap.
eitherdirectassessmentorpairwiseranking.
5.2 PairwiseRankingResults
Jointly Trained Evaluator LMs For jointly
ThepairwiserankingresultsareshowninTable4.
trained evaluator LMs, we test Auto-J (Li et al.,
WeexcludetheresultsofPairRM,UltraRMon‘w/
2023a). In addition, we report the performances
Tie’settingssincetheycouldnotgivetieoptions.
ofjointlytrainingMistral-7BandMixtral-8x7Bon
bothdirectassessmentandpairwiseranking.
Onallofthe4benchmarks,the PROMETHEUS
2 modelsachievethehighestscores,showingthat
WeightMerging PROMETHEUS 2(7B&8x7B) theycouldeffectivelysimulatehumanjudgments.
modelsareourweightmergingbaselines. Notably, while HHH Alignment is an in-domain
Detailsonthehyper-parametersfortrainingand testsetforPairRM,andAuto-JEvalisforAuto-
inferencealongwiththeprompttemplatesareall J,PROMETHEUS-2-8X7B achieveshigherscores.
listedinAppendixB,G,H. ThisshowsthattrainingalargeLM(i.e.,Mixtral-HHHALIGNMENT MTBENCHHUMANJUDG. AUTO-JEVAL PreferenceBench
EvaluatorLM
Help. Harm. Hon. Other TotalAvg. w/TIE w/oTIE w/TIE w/oTIE Instance-wiseCriteria
LLAMA2-CHAT7B 55.93 62.07 49.18 62.79 57.01 46.68 50.39 45.76 45.73 58.60
LLAMA2-CHAT13B 71.19 77.59 60.66 62.79 68.33 51.22 49.61 47.84 43.28 63.00
LLAMA2-CHAT70B 62.71 81.03 65.57 65.12 68.78 55.14 60.88 53.38 50.64 64.70
MISTRAL-INSTRUCT-7B 59.32 68.97 63.93 81.40 67.42 53.81 63.82 53.88 60.94 79.40
MIXTRAL-INSTRUCT-8X7B 83.05 87.93 67.21 69.77 77.38 51.85 71.42 53.81 73.50 84.00
PAIRRM(0.4B) 84.75 84.48 80.33 90.70 84.62 - 59.00 - 59.05 81.80
ULTRARM(13B) 86.44 79.31 81.97 88.37 83.71 - 56.00 - 59.85 86.97
AUTO-J(13B) 77.97 79.31 70.49 74.42 75.57 42.56 69.12 43.46 76.64 81.35
PROMETHEUS-2-7B 76.27 87.93 73.77 76.74 78.73 56.18 67.25 57.61 73.80 92.45
PROMETHEUS-2-8X7B 84.75 96.55 81.97 76.74 85.52 55.07 71.96 58.41 79.98 90.65
GPT-3.5-TURBO-0613 77.97 81.03 77.05 67.44 76.47 54.65 69.41 45.98 72.13 75.05
GPT-4-1106-PREVIEW 89.83 96.55 91.80 83.72 90.95 60.38 79.90 52.80 83.12 85.50
CLAUDE-3-OPUS 91.53 100.00 91.80 95.35 94.57 55.35 77.65 60.70 82.92 89.85
Table4: PairwiseRankingResultsAccuracyonhumanpreferencedatasets.Thebestcomparableaccuraciesareboldedand
secondbestunderlinedexceptproprietaryLMs.NotethatHHHAlignmentisanin-domaintestsetforPairRM,Auto-JEvalis
anin-domaintestsetforAuto-J,andthePreferenceBenchisanin-domaintestsetforPrometheus-2models.
HHHALIGNMENT MTBENCHHUMANJUDG. AUTO-JEVAL
EvaluatorLM
Direct2Pair(↑) Pair2Pair(↑) ∆(↓) Direct2Pair(↑) Pair2Pair(↑) ∆(↓) Direct2Pair(↑) Pair2Pair(↑) ∆(↓)
AUTO-J(13B) 46.61 75.57 28.96 48.14 69.12 20.98 47.40 76.64 29.24
PROMETHEUS-2-7B 74.21 78.73 4.52 63.24 67.25 4.01 68.11 73.80 5.69
PROMETHEUS-2-8X7B 81.45 85.52 4.07 61.67 71.96 10.29 66.54 79.98 13.44
GPT-4-1106-PREVIEW 83.71 90.95 7.24 68.04 79.90 11.86 54.27 83.12 28.85
CLAUDE-3-OPUS 84.62 94.57 9.95 62.65 77.65 15.00 61.04 82.90 21.86
Table5: ConsistencyacrossEvaluationFormatsPairwiserankingaccuracywhenassessingindirectassessmentformats
(denotedas‘Direct2Pair’)andpairwiserankingformats(denotedas‘Pair2Pair’).Smaller∆valuesindicatethatevaluatorLMs
canrobustlyevaluateacrossthetwodifferentformats.
8x7B) with feedback data could be an effective 6 Discussions
strategytoobtainarobustevaluatorLMthatcould
To understand the effectiveness of our proposed
generalizebeyonditstrainingdata. Moreover,the
weight merging method in the context of evalua-
PROMETHEUS 2 modelsatleasthalvetheperfor-
tions,weaddressthefollowingresearchquestions:
mancegapwithproprietaryLMscomparedtoex-
istingevaluatorLMsonout-of-domaintestsets. • RQ1: IsWeightMergingmoreeffectivecom-
paredtoJointTraining? (Section6.1)
5.3 ConsistencyAcrossEvaluationFormats
• RQ2: IstheeffectivenessofWeightMerging
In addition to obtaining high correlation and ac- duetomodelensembling? (Section6.2)
curacy,achievinghighconsistencyisanotherim-
• RQ3: To what extent does learning with di-
portantaspectforevaluatorLMs. Specifically,we
rectassessmenthelppairwiserankingperfor-
conduct an experiment testing if evaluator LMs
mance,andviceversa? (Section6.3)
could achieve consistent scores across different
6.1 WeightMergingvsJointTraining
evaluation formats. To do this, we use pairwise
rankingbenchmarksandmeasuretheperformance Table 6 compares the performance of evaluator
differenceswhenpromptedwithdirectassessment LMstrainedviaweightmergingandjointtraining.
formatsandpairwiserankingformats. Specifically, Alongsidethis,wealsoaddandcomparetheresults
following Kim et al. (2023), to process pairwise ofpromptingandsingle-formattraining.
rankingdatasetsinadirectassessmentscheme,we Surprisingly, we observe that evaluator LMs
evaluateeachresponseseparatelyandcomparethe trainedviajointtrainingoftenshowlowerperfor-
scoringdecisions. Wemarkitascorrectiftheeval- mance compared to single-format trained evalua-
uator LM provides a higher score for the human- tor LMs, which indicates negative task transfer.
chosenresponseovertherejectedone. Asshown Specifically,evaluatorLMstrainedonlyondirect
inTable5,theresultsshowthat PROMETHEUS 2 assessmentformatsobtainhighercorrelationscom-
modelsshowlowerperformancedifferencesacross paredtojointlytrainedevaluatorLMsacrossdiffer-
evaluationformats,indicatingtheirrobustness. entmodelscales. Similarly,evaluatorLMstrainedDIRECTASSESSMENTBENCHMARKS PAIRWISERANKINGBENCHMARKS
TrainingMethod
VicunaBen. MTBen. FLASK Average HHHAlign. MTBen.H.J. Auto-JEval Average
Mistral-Instruct-7B
PROMPTING 0.486 0.284 0.480 0.417 67.42 63.82 60.94 64.06
DIRECTASSESSMENTONLY 0.537 0.561 0.519 0.539 73.33 56.76 64.38 64.82
PAIRWISERANKINGONLY - - - - 78.73 67.06 72.03 72.61
JOINTTRAINING 0.548 0.450 0.457 0.485 80.09 65.49 73.60 73.06
WEIGHTMERGING 0.642 0.543 0.645 0.610 78.73 67.25 73.80 73.26
Mixtral-Instruct-8x7B
PROMPTING 0.566 0.551 0.507 0.541 77.38 71.42 73.55 74.56
DIRECTASSESSMENTONLY 0.625 0.664 0.587 0.625 74.21 53.14 65.85 64.40
PAIRWISERANKINGONLY - - - - 84.16 66.27 75.66 75.36
JOINTTRAINING 0.628 0.560 0.596 0.595 82.35 68.73 74.78 75.29
WEIGHTMERGING 0.685 0.665 0.659 0.670 85.52 71.96 79.98 79.15
Table6: Single-FormatTrainingvsJointTrainingvsWeightMergingPearsoncorrelationsbetweenevaluatorLMstrained
withdifferentmethodsandGPT-4-1106. EvaluatorLMstrainedwithweightmergingoutperformsingle-format-trainedand
jointly-trainedevaluatorLMsacrossmultiplebenchmarks.
DIRECTASSESSMENTBENCHMARKS PAIRWISERANKINGBENCHMARKS
Model
VicunaBen. MTBen. FLASK Average HHHAlign. MTBen.H.J. Auto-JEval Average
NOTRAINING(PROMPTING) 0.486 0.284 0.480 0.417 67.42 63.82 60.94 64.06
DIRECTASSESSMENTONLY 0.537 0.561 0.519 0.539 73.33 56.76 64.38 64.82
PAIRWISERANKINGONLY - - - - 78.73 67.06 72.03 72.61
DIRECTASSESSMENT&DIRECTASSESSMENT 0.552 0.493 0.505 0.517 73.30 55.00 63.69 64.13
PAIRWISERANKING&PAIRWISERANKING - - - - 78.70 65.20 72.72 72.21
DIRECTASSESSMENT&PAIRWISERANKING 0.642 0.543 0.645 0.610 78.73 67.25 73.80 73.26
Table7: UnifyingFormatsvsEnsemblingPearsoncorrelationswithGPT-4-1106(VicunaBench,MTBench,FLASK)and
agreementwithhumanevaluators(HHHAlignment,MTBenchHumanJudgment,Auto-JEval).Mergingmodelstrainedwith
thesameevaluationformats(ensembling)underperformsmergingmodelstrainedwithdifferentformats(unifyingformats).
only on pairwise ranking formats obtain higher ically, we merge two evaluator LMs trained on
average accuracy compared to multi-task trained directassessmentformats(denotedas‘DirectAs-
evaluatorLMswhenusingMixtral-8x7Basabase sessment&DirectAssessment’)andtwoevaluator
model. LMstrainedonpairwiserankingformats(denoted
On the other hand, evaluator LMs trained via as ‘Pairwise Ranking & Pairwise Ranking’). We
weight merging show superior performance not useMistral-7B-Instructasourbasemodel.
only compared to jointly trained evaluator LMs ResultsareshowninTable7. Againstourexpec-
but also single-format trained evaluator LMs, in- tations, we observe that in the majority of cases,
dicating positive task transfer. Also, while both merging evaluator LMs trained on the same eval-
benefit each other, merging the pairwise ranking uationformatdoesnotimproveevaluationperfor-
evaluatorLMweightsimprovesdirectassessment mances. Specifically,ondirectassessmentbench-
performancemoresignificantlythanthereverse. marks,mergingtwoevaluatorLMstrainedondi-
rect assessment harms performance on average.
6.2 IstheEffectivenessofWeightMerging
Similarly,onpairwiserankingbenchmarks,merg-
duetoModelEnsembling?
ingtwoevaluatorLMstrainedonpairwiseranking
While we empirically find that weight merging alsoharmsperformanceonaverage. Incontrast,by
works effectively, it is unclear what might be the mergingtwoevaluatorLMseachtrainedondirect
reason. One natural assumption might be that assessment and pairwise ranking formats, the re-
weight merging works effectively due to the ef- sultingevaluatorLMshowssuperiorperformance
fectofensemblingmultiplemodels. Tocheckthe comparedtodifferentsettings. Thissuggeststhat
validityofthishypothesis,weconductanablation thepositivetasktransferthatoccursfromweight
experimentbytrainingmultipleevaluatorLMson mergingcomesfromunifyingdifferentevaluation
differentrandomseedsandmergingthem. Specif- formats,notbyensemblingmultiplemodels.developthePREFERENCE COLLECTION,thefirst
pairwiserankingdatasetthatincludesover1,000
instance-wiseevaluationcriteriabeyondbasicqual-
itiessuchashelpfulnessandharmlessness. Notably,
wefindthatmergingevaluatorLMstrainedonei-
therdirectassessmentorpairwiserankingformats
canleadtoaunifiedevaluatorLMwithstrongper-
formance. Wehopethatourworkencouragesmore
researchonusingopen-sourcelanguagemodelsas
(Direct Assessment : Pairwise Ranking) Merging Ratio
Direct Assessment Correlation Pairwise Ranking Accuracy Average Performance evaluators,movingawayfromrelianceonpropri-
Figure3:FindingtheOptimalAlphaValueDirectAs- etarymodelsforfairandaccessibleevaluations.
sessmentperformances(coloredingreen)andPairwise
Rankingperformances(coloredinblue)whenaltering Acknowledgements
theαvaluetomergeevaluatorLMstrainedondifferent
WethankSungdongKim,SeonghyeonYe,Sohee
formats.
Yang,DongkeunYoon,andHyeonbinHwangfor
theirhelpfulcommentsanddiscussions.
6.3 QuantifyingPositiveTransferacross
EvaluationFormats
References
Toexplorehowtrainingondirectassessmentfeed-
backdatainfluencespairwiserankingaccuracyand AmandaAskell,YuntaoBai,AnnaChen,DawnDrain,
DeepGanguli,TomHenighan,AndyJones,Nicholas
viceversa,weexperimentbyadjustingtheαvalue
Joseph, Ben Mann, Nova DasSarma, Nelson El-
during linear merging. We evaluate the average
hage,ZacHatfield-Dodds,DannyHernandez,Jack-
performanceusingalleightbenchmarksinourex- son Kernion, Kamal Ndousse, Catherine Olsson,
periments. To illustrate the average performance Dario Amodei, Tom Brown, Jack Clark, Sam Mc-
Candlish, Chris Olah, and Jared Kaplan. 2021. A
(coloredinblack),weadjustthescalebymultiply-
generallanguageassistantasalaboratoryforalign-
ing direct assessment Pearson correlations, origi-
ment.
nally from 0 to 1, by 100 before averaging with
pairwiserankingaccuracy. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, AnnaChen, NovaDasSarma, DawnDrain,
The results are shown in Figure 3. For direct
StanislavFort,DeepGanguli,TomHenighan,etal.
assessmentbenchmarks,evaluatorLMsobtainthe 2022. Trainingahelpfulandharmlessassistantwith
optimal performance when α is set to 0.5. This reinforcementlearningfromhumanfeedback. arXiv
indirectlyindicatesthatbothpairwiserankingand preprintarXiv:2204.05862.
directassessmentfeedbackdatacontributeequally.
Chi-MinChan,WeizeChen,YushengSu,JianxuanYu,
On the other hand, for pairwise ranking bench- Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan
marks, the performance is optimal when α is set Liu.2023. Chateval: Towardsbetterllm-basedeval-
uators through multi-agent debate. arXiv preprint
to0.3. Thisalsoindirectlyimpliesthatwhileboth
arXiv:2308.07201.
benefit each other, training on pairwise ranking
improvesdirectassessmentperformancemoresig- Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
nificantlythanthereverse. ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
Zhuang,YonghaoZhuang,JosephE.Gonzalez,Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
7 Conclusion
sourcechatbotimpressinggpt-4with90%*chatgpt
quality.
We introduce PROMETHEUS 2, an open-source
languagemodelspecializedinevaluatingotherre- Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao,
sponses. Unlikeexistingopenevaluatorlanguage WeiZhu,YuanNi,GuotongXie,ZhiyuanLiu,and
MaosongSun.2023. Ultrafeedback: Boostinglan-
modelsthatcannoteffectivelyprocessbothdirect
guage models with high-quality feedback. arXiv
assessment and pairwise ranking—the two most
preprintarXiv:2310.01377.
prevalentevaluationschemes—thePROMETHEUS
2 models demonstrate superior performance and Shachar Don-Yehiya, Elad Venezian, Colin Raffel,
Noam Slonim, Yoav Katz, and Leshem Choshen.
consistent results on both schemes, significantly
2022. Cold fusion: Collaborative descent for
narrowingthegapwithproprietaryLM-basedeval-
distributed multitask finetuning. arXiv preprint
uations. TotrainthePROMETHEUS 2models,we arXiv:2212.01378.
noitalerroC
nosraeP
tnemssessA
tceriD
ycaruccA
tnemeergA
gniknaR
esiwriaPYannDubois,XuechenLi,RohanTaori,TianyiZhang, Albert Q Jiang, Alexandre Sablayrolles, Antoine
IshaanGulrajani,JimmyBa,CarlosGuestrin,Percy Roux,ArthurMensch,BlancheSavary,ChrisBam-
Liang, and Tatsunori B Hashimoto. 2023. Al- ford,DevendraSinghChaplot,DiegodelasCasas,
pacafarm: A simulation framework for methods Emma Bou Hanna, Florian Bressand, et al. 2024.
that learn from human feedback. arXiv preprint Mixtralofexperts. arXivpreprintarXiv:2401.04088.
arXiv:2305.14387.
Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao
Markus Freitag, David Grangier, and Isaac Caswell. Huang, Bill Yuchen Lin, and Wenhu Chen. 2023b.
2020. Bleu might be guilty but references are not Tigerscore: Towards building explainable met-
innocent. arXivpreprintarXiv:2004.06063. ric for all text generation tasks. arXiv preprint
arXiv:2310.00752.
MingqiGao,XinyuHu,JieRuan,XiaoPu,andXiaojun
Wan.2024. Llm-basednlgevaluation: Currentstatus DongfuJiang,XiangRen,andBillYuchenLin.2023c.
andchallenges. arXivpreprintarXiv:2402.01383. Llm-blender: Ensembling large language models
withpairwiserankingandgenerativefusion. arXiv
SebastianGehrmann,TosinAdewumi,KarmanyaAg- preprintarXiv:2306.02561.
garwal, Pawan Sasanka Ammanamanchi, Aremu
Anuoluwapo, Antoine Bosselut, Khyathi Raghavi
Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang,
Chandu,MirunaClinciu,DipanjanDas,KaustubhD
Shayne Longpre, Hwaran Lee, Sangdoo Yun,
Dhole,etal.2021. Thegembenchmark: Naturallan-
SeongjinShin,SungdongKim,JamesThorne,etal.
guagegeneration,itsevaluationandmetrics. arXiv
2023. Prometheus: Inducing fine-grained evalua-
preprintarXiv:2102.01672.
tioncapabilityinlanguagemodels. arXivpreprint
arXiv:2310.08491.
Sebastian Gehrmann, Abhik Bhattacharjee, Abinaya
Mahendiran, Alex Wang, Alexandros Papangelis,
Nathan Lambert, Valentina Pyatkin, Jacob Morrison,
Aman Madaan, Angelina McMillan-Major, Anna
LJ Miranda, Bill Yuchen Lin, Khyathi Chandu,
Shvets,AshishUpadhyay,BingshengYao,etal.2022.
NouhaDziri,SachinKumar,TomZick,YejinChoi,
Gemv2: Multilingualnlgbenchmarkinginasingle
et al. 2024. Rewardbench: Evaluating reward
lineofcode. arXivpreprintarXiv:2206.11249.
models for language modeling. arXiv preprint
arXiv:2403.13787.
Suchin Gururangan, Margaret Li, Mike Lewis, Wei-
jia Shi, Tim Althoff, Noah A Smith, and Luke
Seongyun Lee, Seungone Kim, Sue Hyun Park,
Zettlemoyer.2023. Scalingexpertlanguagemodels
GeewookKim,andMinjoonSeo.2024. Prometheus-
withunsuperviseddomaindiscovery. arXivpreprint
vision: Vision-language model as a judge
arXiv:2303.14177.
for fine-grained evaluation. arXiv preprint
arXiv:2401.06591.
MichaelHannaandOndˇrejBojar.2021. Afine-grained
analysis of bertscore. In Proceedings of the Sixth
Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan,
ConferenceonMachineTranslation,pages507–517.
Hai Zhao, and Pengfei Liu. 2023a. Generative
judge for evaluating alignment. arXiv preprint
GabrielIlharco,MarcoTulioRibeiro,MitchellWorts-
arXiv:2310.05470.
man, Suchin Gururangan, Ludwig Schmidt, Han-
naneh Hajishirzi, and Ali Farhadi. 2022. Edit-
MargaretLi,SuchinGururangan,TimDettmers,Mike
ing models with task arithmetic. arXiv preprint
Lewis,TimAlthoff,NoahASmith,andLukeZettle-
arXiv:2212.04089.
moyer.2022. Branch-train-merge: Embarrassingly
parallel training of expert language models. arXiv
Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong
preprintarXiv:2208.03306.
Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh
Hajishirzi,YejinChoi,andPrithvirajAmmanabrolu.
2023a. Personalizedsoups: Personalizedlargelan- Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan
guagemodelalignmentviapost-hocparametermerg- Taori, Ishaan Gulrajani, Carlos Guestrin, Percy
ing. arXivpreprintarXiv:2310.11564. Liang, and Tatsunori B. Hashimoto. 2023b. Al-
pacaeval: An automatic evaluator of instruction-
Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung following models. https://github.com/
Kim,LajanugenLogeswaran,MoontaeLee,Kyung- tatsu-lab/alpaca_eval.
jae Lee, and Minjoon Seo. 2023b. Exploring the
benefitsoftrainingexpertlanguagemodelsoverin- Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen
structiontuning. arXivpreprintarXiv:2302.03202. Gu, and Chongyang Tao. 2024. Leveraging large
languagemodelsfornlgevaluation: Asurvey. arXiv
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men- preprintarXiv:2401.07103.
sch,ChrisBamford,DevendraSinghChaplot,Diego
delasCasas,FlorianBressand,GiannaLengyel,Guil- Chin-YewLin.2004. Rouge: Apackageforautomatic
laumeLample,LucileSaulnier,etal.2023a. Mistral evaluation of summaries. In Text summarization
7b. arXivpreprintarXiv:2310.06825. branchesout,pages74–81.Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang,
Ruochen Xu, and Chenguang Zhu. 2023a. G-eval: Shizhe Diao, Shuang Qiu, Han Zhao, and Tong
Nlgevaluationusinggpt-4withbetterhumanalign- Zhang. 2024. Arithmetic control of llms for di-
ment. verseuserpreferences: Directionalpreferencealign-
ment with multi-objective rewards. arXiv preprint
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, arXiv:2402.18571.
Ruochen Xu, and Chenguang Zhu. 2023b. Gpte-
val: Nlg evaluation using gpt-4 with better human Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai
alignment. arXivpreprintarXiv:2303.16634. Dai, Yifei Li, Deli Chen, Y Wu, and Zhifang Sui.
2023a. Math-shepherd: A label-free step-by-step
MichaelSMatenaandColinARaffel.2022. Merging verifier for llms in mathematical reasoning. arXiv
modelswithfisher-weightedaveraging. Advancesin preprintarXiv:2312.08935.
NeuralInformationProcessingSystems,35:17703–
17716. Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi
Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang,
KishorePapineni,SalimRoukos,ToddWard,andWei- Rui Xie, Jindong Wang, Xing Xie, et al. 2023b.
JingZhu.2002. Bleu: amethodforautomaticevalu- Pandalm: An automatic evaluation benchmark for
ationofmachinetranslation. InProceedingsofthe llminstructiontuningoptimization. arXivpreprint
40thannualmeetingoftheAssociationforComputa- arXiv:2306.05087.
tionalLinguistics,pages311–318.
PrateekYadav,DerekTam,LeshemChoshen,ColinA
Alexandre Rame, Guillaume Couairon, Corentin Raffel,andMohitBansal.2024. Ties-merging: Re-
Dancette, Jean-Baptiste Gaya, Mustafa Shukor, solving interference when merging models. Ad-
LaureSoulier,andMatthieuCord.2024. Rewarded vances in Neural Information Processing Systems,
soups: towards pareto-optimal alignment by inter- 36.
polatingweightsfine-tunedondiverserewards. Ad-
vances in Neural Information Processing Systems, Seonghyeon Ye, Doyoung Kim, Sungdong Kim,
36. Hyeonbin Hwang, Seungone Kim, Yongrae Jo,
James Thorne, Juho Kim, and Minjoon Seo. 2023.
Natalie Schluter. 2017. The limits of automatic sum- Flask: Fine-grained language model evaluation
marisationaccordingtorouge. InProceedingsofthe based on alignment skill sets. arXiv preprint
15thConferenceoftheEuropeanChapteroftheAsso- arXiv:2307.10928.
ciationforComputationalLinguistics,pages41–45.
AssociationforComputationalLinguistics. LeYu,BowenYu,HaiyangYu,FeiHuang,andYongbin
Li.2023. Languagemodelsaresupermario: Absorb-
SainbayarSukhbaatar,OlgaGolovneva,VasuSharma, ingabilitiesfromhomologousmodelsasafreelunch.
Hu Xu, Xi Victoria Lin, Baptiste Rozière, Ja- arXivpreprintarXiv:2311.03099.
cob Kahn, Daniel Li, Wen-tau Yih, Jason We-
ston,etal.2024. Branch-train-mix: Mixingexpert Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q
llms into a mixture-of-experts llm. arXiv preprint Weinberger,andYoavArtzi.2019. Bertscore: Eval-
arXiv:2403.07816. uating text generation with bert. arXiv preprint
arXiv:1904.09675.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay LianminZheng,Wei-LinChiang,YingSheng,Siyuan
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Bhosale,DanBikel,LukasBlecher,CristianCanton Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.
Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu, Judging llm-as-a-judge with mt-bench and chatbot
JudeFernandes,JeremyFu,WenyinFu,BrianFuller, arena. arXivpreprintarXiv:2306.05685.
CynthiaGao,VedanujGoswami,NamanGoyal,An-
Lianghui Zhu, Xinggang Wang, and Xinlong Wang.
thonyHartshorn,SagharHosseini,RuiHou,Hakan
2023. Judgelm: Fine-tuned large language
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
models are scalable judges. arXiv preprint
IsabelKloumann,ArtemKorenev,PunitSinghKoura,
arXiv:2310.17631.
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
anaLiskovich,YinghaiLu,YuningMao,XavierMar-
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein,RashiRungta,KalyanSaladi,AlanSchelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez,RobertStojnic,SergeyEdunov,andThomas
Scialom.2023. Llama2: Openfoundationandfine-
tunedchatmodels.VerificationStandards RESULTS First,weassessthecoherenceofv rm,rn withthe
scoring decision (i.e., ’A is better’ or ’B is bet-
Coherence 99.5%(Passed)
Suitability 98.5%(Passed) ter’). Second,weevaluatethesuitabilityofv rm,rn
Criticality 88%(Winrate) against the evaluation criteria e. Lastly, to deter-
minethecriticalityofthefeedback,wecompare
Table8: Humanverificationresultstoassessthequalityof
thenewlygeneratedv withaconcatenationof
thePREFERENCECOLLECTION. Weusethreestandardsto rm,rn
assessthequalityofverbalfeedbackv . v and v . This aims to determine if v ef-
rm,rn rm rn rm,rn
fectivelyleveragesthemutualinformationbetween
Temperature 1.0 r mandr n. Annotatorsthenvoteonwhetherv rm,rn
Top_p 0.9 ortheconcatenationofr andr ismorecritical.
m n
MaxNewTokens 1024
TheresultsareshowninTable8.
RepetitionPenalty 1.03
B TrainingandInference
Table9: Hyperparametersusedtoinferencedifferentevalua-
torLMbaselines. Hyperparameters
Theconfigurationsweusedforpromptingandtrain-
BaseModel mistralai/Mistral-7B-Instruct-v0.2
ing evaluator LMs are shown in Table 9, 10, 11.
Torchdtype bfloat16
Epoch 1 ForAuto-J,PairRMandUltraRM,weutilizetheir
TrainData1 FEEDBACKCOLLECTION prompt template, inference hyperparameter men-
TrainData2 PREFERENCECOLLECTION
tionedinthemodelcardsorgithubrepositoriesin
MaxSeqLength 4096
LearningRate 1e-5 order to ensure the configuration is optimal for a
TrainBatchSize 4 fairperformancecomparison. ForproprietaryLMs,
RandomSeed 42
MergingStrategy Linear(α=0.5)
PROMETHEUS 1,andPROMETHEUS 2models,we
TrainingMethod SupervisedFine-tuning usethesameprompttemplateandevaluationcon-
figurations.
Table10: HyperparametersusedtotrainPROMETHEUS 2
7B. C DirectAssessmentResults: Extended
Table 12 and 13 (on the next page) shows the ex-
BaseModel mistralai/Mixtral-8x7B-Instruct-v0.1
Torchdtype bfloat16 tended results Table 3. Even when changing the
Epoch 1
metrics to either Kendall-Tau and Spearman, the
TrainData1 FEEDBACKCOLLECTION
TrainData2 PREFERENCECOLLECTION overall trends are maintained. PROMETHEUS 2
MaxSeqLength 4096
showssuperiorevaluationperformancesamongthe
LearningRate 1e-5
TrainBatchSize 8 open evaluator LMs, achieving high correlations
PEFT True
withhumansandproprietaryLMs.
Lora_r 256
Lora_alpha 512
Lora_Dropout 0.1 D ConsistencyExperimentResults:
LoraTargetModule Qproj,Kproj,Vproj,Oproj,Wproj,LM_Head
Extended
RandomSeed 42
MergingStrategy DAREMerging
Mergingp 0.1 WetestifevaluatorLMscouldgiveconsistentscor-
MergingLambda 1.95
ing decisions in direct assessment formats. We
TrainingMethod SupervisedFine-tuning
inferencingmultipletimeswithnon-deterministic
Table11: HyperparametersusedtotrainPROMETHEUS 2 decoding(e.g.,usingtemperature1.0). Following
8x7B.
theexperimentaldesignfromYeetal.(2023),we
choose to inference 3 times and report the Krip-
A QualityVerificationofthe pendorff’salphavalue. AsshowninTable14,the
PREFERENCE COLLECTION resultsindicatethattrainingonfeedbackdataonly
slightlyimprovesconsistency. Ontheotherhand,
To ensure the quality of the PREFERENCE COL- we find that the LMs with a large number of pa-
LECTION, particularly the generated verbal feed- rametersachievehighconsistency. Thisindicates
backv ,weemployfiveannotatorswithback- theimportanceofselectingalargeLMasthebase
rm,rn
grounds in natural language processing. We ran- model when training an evaluator LM. Notably,
domlysample200instanceswithdifferentinstruc- PROMETHEUS-2-8X7B obtainsthehighestcorre-
tionsandconductathree-partverificationprocess. lationamongopenevaluatorLMs.VICUNABENCH MTBENCH FLASK FeedbackBench
EvaluatorLM
GPT-4-1106 Claude-3-Opus GPT-4-1106 Claude-3-Opus GPT-4-1106 Claude-3-Opus Humans GPT-4-0613
LLAMA2-CHAT7B 0.183 0.203 0.065 0.070 0.229 0.186 0.211 0.419
LLAMA2-CHAT13B 0.145 0.146 -0.019 0.037 0.160 0.174 0.174 0.453
LLAMA2-CHAT70B 0.282 0.382 0.150 0.196 0.310 0.310 0.231 0.487
MISTRAL-INSTRUCT-7B 0.314 0.391 0.208 0.281 0.395 0.384 0.287 0.454
MIXTRAL-INSTRUCT-8X7B 0.395 0.468 0.433 0.419 0.410 0.408 0.304 0.551
PROMETHEUS-7B 0.405 0.425 0.290 0.263 0.282 0.251 0.236 0.770
PROMETHEUS-13B 0.397 0.434 0.299 0.352 0.365 0.352 0.299 0.793
AUTO-J(13B) 0.282 0.242 0.303 0.272 0.312 0.282 0.312 0.515
PROMETHEUS-2-7B 0.515 0.478 0.458 0.421 0.500 0.454 0.376 0.773
PROMETHEUS-2-8X7B 0.559 0.515 0.535 0.483 0.526 0.507 0.388 0.800
GPT-3.5-TURBO-0613 0.255 0.287 0.148 0.157 0.360 0.315 0.298 0.489
GPT-4-1106 / 0.553 / 0.590 / 0.609 0.517 0.662
CLAUDE-3-OPUS 0.553 / 0.590 / 0.609 / 0.453 0.693
Table12: Kendall-Taucorrelationsbetweenreferenceevaluators(listedontop)andevaluatorLMs. Thebestcomparable
statisticsareboldedandsecondbestunderlinedexceptproprietaryLMs.
VICUNABENCH MTBENCH FLASK FeedbackBench
EvaluatorLM
GPT-4-1106 Claude-3-Opus GPT-4-1106 Claude-3-Opus GPT-4-1106 Claude-3-Opus Humans GPT-4-0613
LLAMA2-CHAT7B 0.236 0.255 0.084 0.089 0.301 0.244 0.279 0.511
LLAMA2-CHAT13B 0.178 0.179 -0.025 0.044 0.206 0.222 0.224 0.543
LLAMA2-CHAT70B 0.348 0.466 0.197 0.252 0.391 0.389 0.298 0.585
MISTRAL-INSTRUCT-7B 0.389 0.480 0.266 0.358 0.499 0.478 0.374 0.563
MIXTRAL-INSTRUCT-8X7B 0.476 0.556 0.545 0.517 0.505 0.500 0.386 0.659
PROMETHEUS-7B 0.508 0.528 0.385 0.349 0.367 0.326 0.317 0.876
PROMETHEUS-13B 0.492 0.534 0.401 0.470 0.474 0.454 0.398 0.893
AUTO-J(13B) 0.337 0.297 0.408 0.365 0.402 0.358 0.408 0.623
PROMETHEUS-2-7B 0.643 0.584 0.550 0.524 0.626 0.569 0.490 0.909
PROMETHEUS-2-8X7B 0.660 0.615 0.669 0.605 0.642 0.618 0.496 0.912
GPT-3.5-TURBO-0613 0.319 0.354 0.192 0.198 0.446 0.390 0.374 0.565
GPT-4-1106 / 0.659 / 0.721 / 0.729 0.650 0.753
CLAUDE-3-OPUS 0.659 / 0.721 / 0.729 / 0.567 0.784
Table13: Spearmancorrelationsbetweenreferenceevaluators(listedontop)andevaluatorLMs.Thebestcomparablestatistics
areboldedandsecondbestunderlinedexceptproprietaryLMs.
EvaluatorLM VicunaBen. MTBen. FLASK PREFERENCECOLLECTION
EvaluatorLM
LLAMA2-CHAT7B 0.3558 0.2565 0.4379 Transitivity
LLAMA2-CHAT13B 0.2017 0.2998 0.4038
LLAMA2-CHAT70B 0.5212 0.4559 0.6204
MISTRAL-INSTRUCT-7B 87.10
MISTRAL-INSTRUCT-7B 0.5157 0.4393 0.5884
MIXTRAL-INSTRUCT-8X7B 90.45
MIXTRAL-INSTRUCT-8X7B 0.5459 0.6229 0.6976
PAIRRM 91.40
PROMETHEUS-7B 0.6049 0.5363 0.5970
ULTRARM 94.25
PROMETHEUS-13B 0.5734 0.5181 0.5624
AUTO-J(13B) 89.65
AUTO-J(13B) 0.4976 0.5069 0.6160
PROMETHEUS-2-7B 97.60
PROMETHEUS-2-7B 0.6018 0.5340 0.5991
PROMETHEUS-2-8X7B 96.75
PROMETHEUS-2-8X7B 0.6383 0.6862 0.7874 GPT-3.5-TURBO-0613 84.35
GPT-3.5-TURBO-0613 0.7108 0.4800 0.6389
GPT-4-1106-PREVIEW 95.70
GPT-4-1106-PREVIEW 0.7366 0.8271 0.8355
CLAUDE-3-OPUS 96.20
CLAUDE-3-OPUS 0.8284 0.8601 0.8976
Table 15: Transitivitystatisticstomeasureconsistencyin
Table14: Krippendorff’salphastatisticsforevaluatorLMs pairwiserankingevaluationsettings.
whenprompted3timesvianon-deterministicdecoding.
Moreover, to evaluate consistency in pairwise
rankingsettings(Table15),wemeasuretransitivity
(i.e., a higher score for response B over A, and modelsachieveperformancesonparwithGPT-4,
for C over B, results in a higher score for C over showingthattheycouldproviderobustjudgments
A). As shown in Table 15, the PROMETHEUS 2 inpairwiserankingschemes.DIRECTASSESSMENTBENCHMARKS PAIRWISERANKINGBENCHMARKS
TrainingMethod
VicunaBen. MTBen. FLASK HHHAlign. MTBen.H.J.
Mistral-Instruct-7B
LINEARMERGING 0.642 0.543 0.645 78.73 67.25
DAREMERGING 0.534 0.567 0.584 78.28 67.75
Mixtral-Instruct-8x7B
DAREMERGING 0.685 0.665 0.659 85.52 71.96
Table16: PearsoncorrelationsbetweenevaluatorLMsmergedwithdifferentmergingmethodsandGPT-4-1106.Evaluator
LMstrainedwithweightmergingoutperformsingle-format-trainedandjoint-trainedevaluatorLMsacrossmultiplebenchmarks.
E MergingMethodAblation
InTable16,wetrydifferentmergingmethodsin-
troducedinourprevioussection. Weempirically
findthatmergingevaluatorLMswithTaskArith-
metic(Ilharcoetal.,2022)andTIESmerging(Ya-
davetal.,2024)constantlyresultsinamodelthat
degenerates. Ontheotherhand,fortheMistral-7B
basedevaluatorLMs,wefindthatlinearmerging
and DARE merging (Yu et al., 2023) results in a
modelthatdoesnotdegenerateandcouldprocess
both evaluation formats. Also, for Mixtral-8x7B
based evaluator LMs, we find that only DARE
mergingworkseffectivelyforbothbasemodels.F PREFERENCE COLLECTION G DirectAssessmentPrompt
AugmentationPrompt
DirectAssessmentSystemPrompt
You are a fair judge assistant tasked with
providing clear, objective feedback based
PromptforGeneratingVerbalFeedback on specific criteria, ensuring each assess-
inPairwiseRanking mentreflectstheabsolutestandardssetfor
performance.
###TaskDescription:
An instruction (might include an Input in-
sideit),tworesponsestoevaluate(denoted
as Response A and Response B), a refer- DirectAssessmentPromptTemplate
enceanswer,andascorerubricrepresenting
###TaskDescription:
aevaluationcriteriaaregiven.
An instruction (might include an Input in-
1. Writeadetailedfeedbackexplainingwhy
sideit),aresponsetoevaluate,andascore
{sub_str}, focusing strictly on the aspects
rubricrepresentingaevaluationcriteriaare
highlightedintheevaluationcriteria.
given.
2. Whilewritingthefeedback,makecom-
1. Writeadetailedfeedbackthatassessthe
parisonsbetweenResponseA,ResponseB,
qualityoftheresponsestrictlybasedonthe
andReferenceAnswer. Insteadofexamin-
givenscorerubric,notevaluatingingeneral.
ingResponseAandResponseBseparately,
2. After writing a feedback, write a score
gostraighttothepointandmentionabout
that is an integer between 1 and 5. You
thecommonalitiesanddifferencesbetween
shouldrefertothescorerubric.
them.
3. Theoutputformatshouldlookasfollows:
3. Whilewritingthefeedback,donotstart
"Feedback: (write a feedback for criteria)
by mentioning {sub_str} in the first sen-
[RESULT] (an integer number between 1
tence. Instead,trytowriteareasoningpro-
and5)"
cessthatdelvesintothecommonalitiesand
4. Pleasedonotgenerateanyotheropening,
differences of the two responses and men-
closing,andexplanations.
tion{sub_str}atthelastpartofyourjustifi-
###Theinstructiontoevaluate:
cation.
{orig_instruction}
4. Within the feedback, do not explicitly
###Responsetoevaluate:
mentionaboutthereferenceanswer. Forin-
{orig_response}
stance,donotusephraseslike"Compared
###ScoreRubrics:
tothereferenceanswer". Assumethatyou
{score_rubric}
inherentlyknowthereferenceanswerwhich
###Feedback:
couldbeusedtodeterminedetailsthatare
notpresentinbothresponsesunderassess-
ment.
5. Pleasedonotgenerateanyotheropening,
closing, and explanations. Just write the H PairwiseRankingPrompt
feedback.
6. Within the feedback, generate a string
phrase"[END]"afteryouarefinished. PairwiseRankingSystemPrompt
###Instruction: {instruction}
Youareafairjudgeassistantassignedtode-
###ResponseA:{response_A}
liverinsightfulfeedbackthatcomparesindi-
###ResponseB:{response_B}
vidualperformances,highlightinghoweach
###ReferenceAnswer: {reference_answer}
standsrelativetootherswithinthesameco-
###ScoreRubric: {criteria}
hort.
###Feedback:PairwiseRankingPromptTemplate
###TaskDescription:
An instruction (might include an Input in-
sideit),aresponsetoevaluate,andascore
rubricrepresentingaevaluationcriteriaare
given.
1. Write a detailed feedback that assess
thequalityoftworesponsesstrictlybased
onthegivenscorerubric,notevaluatingin
general.
2. After writing a feedback, choose a bet-
ter response between Response A and Re-
sponse B. You should refer to the score
rubric.
3. Theoutputformatshouldlookasfollows:
"Feedback: (write a feedback for criteria)
[RESULT](AorB)"
4. Pleasedonotgenerateanyotheropening,
closing,andexplanations.
###Instruction:
{orig_instruction}
###ResponseA:
{response_A}
###ResponseB:
{response_B}
###ScoreRubric:
{score_rubric}
###Feedback: