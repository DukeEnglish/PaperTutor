A separability-based approach to quantifying
generalization: which layer is best?
LucianoDyballaa,1,*,EvanGerritza,1,** andStevenW.Zuckera
aDepartmentofComputerScience,YaleUniversity,NewHaven,CT,USA
ORCID(LucianoDyballa): https://orcid.org/0000-0003-1996-2173,ORCID(EvanGerritz):
https://orcid.org/0009-0008-5600-025X,ORCID(StevenW.Zucker): https://orcid.org/0000-0002-2205-6895
Abstract. Generalizationtounseendataremainspoorlyunderstood then investigate the model’s behavior on the remaining classes (or
fordeeplearningclassificationandfoundationmodels.Howcanone unseenclasses).Themotivationforthisapproachisthatthefeatures
assesstheabilityofnetworkstoadapttoneworextendedversionsof learnedfortheseenclassesshouldbeused,onlyindifferentcombi-
theirinputspaceinthespiritoffew-shotlearning,out-of-distribution nations,forrepresentingthecommonfeaturesofthedomain.Thus,
generalization, and domain adaptation? Which layers of a network unseenclassescouldbeorganized/separablewithinthesameembed-
arelikelytogeneralizebest?Weprovideanewmethodforevaluating dingspace.Togeneralizewellinthisscenario,anetworkmusthave
thecapacityofnetworkstorepresentasampleddomain,regardless asufficientnumberofneuronstorepresentarichsetoffeaturesthat
ofwhetherthenetworkhasbeentrainedonallclassesinthedomain. will also be found in the images from unseen classes; this idea is
Ourapproachisthefollowing:afterfine-tuningstate-of-the-artpre- depictedinFigure2.Hence,modelsthattendtolearnmoredetails
trained models for visual classification on a particular domain, we (eventhosenotnecessarilyusefulforclassification),inotherwords
assesstheirperformanceondatafromrelatedbutdistinctvariations learningaricherrepresentationofthefeaturesintheseenclasseswill
inthatdomain.Generalizationpowerisquantifiedasafunctionofthe likelyallowthemodeltogeneralizebettertotheunseenclasses.
latentembeddingsofunseendatafromintermediatelayersforboth Weemphasizethatthisisdifferentfromthestandardgeneraliza-
unsupervisedandsupervisedsettings.Workingthroughoutallstages tionnotionbetweentrainingandtestdata.Inthatscenario,thenet-
ofthenetwork,wefindthat(i)highclassificationaccuracydoesnot work is evaluated on how well it performs on held-out data points
implyhighgeneralizability;and(ii)deeperlayersinamodeldonot belongingtothesameclassesasthosepresentinthetrainingdata.
alwaysgeneralizethebest,whichhasimplicationsforpruning.Since Thiscanbeframedas“weakgeneralization”,andmaybeinterpreted
the trends observed across datasets are largely consistent, we con- geometricallyastestingthenetworkonnovelpointssampledfrom
cludethatourapproachreveals(afunctionof)theintrinsiccapacity thesamemanifoldM∈Rd,withd≤m,wheremisthedimension
ofthedifferentlayersofamodeltogeneralize. oftheinputspace.Thebasicassumptionisthat,ifthenetworkispre-
sentedwithsufficientlyvariedinputs,itshouldbeableto“interpo-
late”betweenthosetoperformwellonunseeninputsfromthesame
1 Introduction
distribution.Thedegreetowhichthiswillbesuccessfulisamatter
The extent to which a network represents a target domain is a key ofhowmuchthenetworkcanavoidoverfitting,andtechniquessuch
question for successful generalization. We work from the observa- asweightregularization[25],dropout,andoptimizingbatchsize[18]
tion that an equivalence class structure underlies successful classi- arecommonlyusedtohelpinthatregard,althoughifhasbeenshown
fication,andexploitthistopologytodevelopameasureofgeneral- thatsomeofthesearenotsufficienttoexplainwhylargenetworks
izabilitybasedonseparability(Figure1).Ourmethodexaminesthe generalizeinpractice[42].Ithasalsobeensuggestedthatthistype
behavioroftheintermediatelayersonexamplesfromclassesmissing ofgeneralizationcouldberelatedtothepresenceofflatnessoflocal
inboththetrainingandtestsets,aproblemconfoundingearlierat- minimainthelossfunctionlandscape[11].
temptstoquantifygeneralizationtoadifferentdatasetwiththesame
classes [1]. Importantly, our measure can be applied to any inter-
1.1 RelatedWork
mediatelayer,allowingustotestthecompetinghypothesesthat(i)
earlylayersshouldcapturebasic,generalfeaturesthataremoreeas-
Problemsrelatedtoourstrongernotionofgeneralizationhavebeen
ilytranslatabletootherdatasets,orthat(ii)thedeepertherepresen-
previouslyinvestigatedinthedeeplearningliterature.Domainadap-
tationis—andthereforeclosertothefinalencoding/outputlayer—,
tation [3, 32] considers a change in distribution/domain of inputs
the more “useful” it should be. Neither perspective, it turns out, is
(e.g., going from photos to paintings), but maintaining the same
true.
classes (or subset thereof, in the case of partial domain adaptation
To empirically study a model’s generalization capacity, we train
(PDA)[2,7]).Thekeygoalistolearndomain-invariantfeaturesfor
it on a subset of the classes from a dataset (the seen classes), and
eachclassthattranslatewellacrossdomains,usinglabeleddatafrom
∗CorrespondingAuthor.Email:luciano.dyballa@yale.edu. boththe‘source’and‘target’domains.Unsuperviseddomainadap-
∗∗CorrespondingAuthor.Email:evan.gerritz@yale.edu. tation(UDA)[4,17,5]isavariantofthisproblemwherethetarget
1Equalcontribution. domainisunlabeled;thisisclosertoourscenario,exceptwetakeit
4202
yaM
2
]GL.sc[
1v42510.5042:viXraastepfurtherinthateventheclassesarenotthesame.Thus,wecan- directlymeasurehowseparabletheunseenclassesareinthelatent
notusethestrategyof’matching’same-classdatapointsfromone space—itaddressesthepracticalcasewhereoneisawareofthenov-
domainintotheother. eltyoftheclassesbeingusedforduringinference(andthereforecan
Out-of-distribution (OOD) or out-of-sample detection [16]—of labeltheoutputs),butfine-tuningthemodelisinfeasible.Thus,the
which anomaly detection, outlier detection, novelty detection, and linearprobeemulatesann-wayfew-shotlearning,wherelabeledun-
open-set recognition are special cases [39]—is related to our set- seenclassescanbeseenasthesupportset.Thissupervisedtechnique
ting because the novel input samples come from classes unrelated alsoresemblestransductivefew-shotlearning[27,10,21],inwhich
tothoseseenduringtraining.However,thetaskistousebinaryclas- allunseenexamplesareclassifiedatonce.
sificationtodistinguishbetweenseendata(training+testsets)and
unseendata(OODsamples).Out-of-distributiongeneralization[23] 2 Methodology
addressesthecasewherethetest-setdistributionmaydivergefrom
thatofthetrainingset,soitcanbealsoseenasdomainadaption. To test our approach, we fine-tuned six, pretrained, state-of-the-
Thenotionoftakingadvantageoflatentfeaturerepresentationsof artvisual-classificationnetworks:ViT-base(ViT)[12],SwinTrans-
the data is particularly important in the field of zero-shot learning former (Swin)[24], Pyramid ViT (PViT) [33], CvT-21 (CvT) [37],
[38,28].Traditionally,thegoalistoinfertheclassofimagesfrom PoolFormer-S12 (PF) [41], ConvNeXt V2 (CNV2) [36]. We used
unseenclassesbasedonsomeformofannotation:semanticattributes twodifferentdatasetsforfinetuning:theCIFAR-100naturalscenes
[13,26,15],wordvector[14,31],orashorttextdescription/caption dataset[20],whichclassifiesimagesbytheircontent,andaChinese
[29] that describes them. Seen and unseen classes are related in a calligraphydataset[34],whichclassifiesgrayscaleimagesofdrawn
so-called‘semanticspace’,wheretheknowledgefromseenclasses characters by the artist that drew them. For each dataset, we sam-
canbetransferredtounseenclassesbymeansoftheannotations.In pled15classestobeseenonlyduringtraining(theseenclasses)and
onesuchapproach,themodellearnsajointembeddingspaceonto
5tobewithheldforassessinggeneralization(theunseenclasses).2
which both the semantic vectors and the visual feature vectors can Ourapproachtofine-tunethemodelsonlyontheseenclassesisin
beprojected[40,22].Analternativeistolearnamappingfromone contrastwithotherworksinvestigatingfew-shotlearningwherethe
totheother[31,14]).Novelimagescanbeclassifiedbyfindingthe modelisfine-tunedonthesupport(unseen)set[e.g.,10].
classthatisnearesttoitinthesemanticspace. The networks were fine-tuned using PyTorch and the
In one-shot and few-shot learning [6, 8, 10], a model is given a transformers package for 500 epochs on the seen classes
single, or few, labeled examples of an unseen class along with an usingthefollowinghyperparameters:learningrate2e−4,batchsize
unlabeledexample.Themodelpredictsthelabelbasedonhowsimi- 72;AdamWoptimizer.
laritistothenovel,labeledexamples.Toachievethis,themodelis To assess generalization, we used the intuition that intermediate
expectedtohaveapowerfulfeature-levelrepresentation,butisstill embeddingsofexamplesfromlearnedclassesshouldformseparable
reliantonlabeledclasses. clusters. Thus, we created a generalization index, g, that measures
Thus,ourproblemismoreakintoageneralizedzero-shotsetting thedegreeofseparabilityofexamples{x}withinlatentspaceem-
[15], in which the goal is to correctly organize samples from both beddings{Φ i(x)}whereiindexestheintermediatelayerproviding
seenandunseenlabels,exceptthatwedonotemploysemanticin- theembedding.
formationbeyondthevisualfeaturesalreadypresentintheinputim- For a given network, g ui nseen is measured via the normalized mu-
ages. Instead, we use multiple unlabeled points as context in order tualinformation[9]betweenaK-meansclusterassignment(withK
to infer class structure. In contrast, a large language model (LLM) equaltothenumberofunseenclasses)computedontheembedding
performszero-shotinferencebyreceivingadditionalcontextabouta ofunseenexamplesandthegroundtruth:
newclass(es)inthesameprompt(‘zero-shotprompting’)[35,19]. (cid:110) (cid:16) (cid:17)(cid:111)
gi = NMI CΦi ,C⋆ (1)
For example: an autoregressive language model may respond cor- unseen unseen
rectly to the prompt: “Classify the sentiment of following sentence
where i indexes the intermediate layers, CΦi denotes the K-
intopositiveornegative:‘Ienjoyedthispaper.’Sentiment:”evenif unseen
meansclusterassignmentsoftheunseenexamplesembeddedinΦ ,
ithadnotbeentrainedtoperformsentimentanalysis.Inourspecific i
andC⋆ denotestheimages’truelabels.Wecanthenalsodefinethe
settingofimageclassification,acontextisgivenintheformofmany
overallgeneralizabilityofanetworkbyusingthelayerthatgeneral-
additionalinputscomingfromtheunseenclasses.
izesthebest:
Weworkwithapurelyvisualsetting,asin[8,10].Theideaisto
g=maxgi. (2)
utilizeaminimalistapproachinordertoavoidconfoundsfromthe i
methodofembeddingsemanticinformationandofrelatingvisualto Tocomparetheseparabilityoftheunseenclassembeddingsgi
unseen
semantic features. This paradigm is relevant for the common real- tothoseoftheseenclassembeddings,wealsocomputegi analo-
seen
worldscenariowheremanyimagesareavailablewithoutannotation. gouslybyobtainingK-meansclusterlabelsforonlytheseenexam-
Ultimately, we aim to test whether the learned feature vectors are
plesandcomparingthosetothegroundtruth.
sufficienttosupportzero-shotlearning(inthesensethatwedonot To validate our choice of metric, we compared g to another un-
havelabelsfortheunseenclasses)orfew-shotlearning(inthesense supervisedmetricbasedonk-nearestneighbors(g ),aswellasa
kNN
that we need multiple examples to assess proximity between data supervisedmetricbasedonlinearprobes(g ).
LPr
points).
Two unsupervised approaches are used to evaluate the success 2Becauseallthenetworkswerepre-trainedonImageNet-1k,adatasetthat
sharesconsiderableoverlapinclasseswithCIFAR-100,weneededtofind
of such predictions: K-means (which assumes that classes should
classesinCIFAR-100thatwerenotpresentinImageNet-1k.Fortunately,
formGaussian-likeclusters)andk-nearestneighbors(whichinstead we observed that ImageNet-1k does not have classes for flower species
of clusters assumes that samples should be closer to the k closest butCIFAR-100does,sowewereabletouse‘sunflower’,‘tulip’,‘orchid’,
samplesfromthesameunseenclassthanthosefromotherclasses). ‘poppy’,and‘rose’astheunseenclasses.TherearenoChinesecalligra-
phyimagesinImageNet-1k,sothiswasnotaconcernforthecalligraphy
Wealsotestasupervisedapproach(linearprobeclassifier)tomore
dataset.a b Intermediate layer embedding (seen classes) Intermediate layer embedding (unseen classes)
Domain, D
Seen classes
Unseen classes
Test set
Figure1:Motivationforourapproach.(a)Exampleofadomainwithanequivalenceclassstructure.Someclassesareusedintrainingand
modelevaluation(seen,ingreen)andtherestarenot(unseen,inred).(b)Typicalexampleofthedisparitybetweenseen-classembeddings
andunseen-classembeddings.Notetheformerarereadilyseparable,butthelatterarenot,despitehightest-setclassificationaccuracy.This
illustrates poor generalization. We formalize the representation’s generalization quality by measures of separability for the unseen classes.
PlotsshowembeddingsofanintermediatelayeroutputfromVGG16[30],visualizedusingPCA.
a image feature vector seen unseen a Training intermediate layers cla as cs cifi uc raa cti yon
xj stag input output y
1.3 snow
Φ(xi)=
0.1 foliage
xi reindeer=grizzly bear ?
X y^ = L
0.2 snow xi
Φ(xj)=
2.1 foliage polar bear
xj
snow snow
b update model parameters
1.3 snow seen unseen
Φ(xi)=10. .1
9
cfo lal wiag se reindeer b Seen classes Unseen classes
xi 0 .. .0 . antlers xj stag Inference input intermediate layers output
0.2 snow grizzly bear
Φ(xj)=02 .. 11 cfo lal wiag se xi X y^
1.7 antlers polar bear X’ prec dl ia cs ts ion
xj ...
snow, snow,
cla.w..s, cla.w..s,
Figure 2: Clustering in different models. (a) An impoverished
modelcanclassifystagsvs.polarbearsbasedonthebackground:fo-
liagevs.snow,butfailsonunseenexamples.Notethelackofcluster feature vectors Φ₁(X) Φi(X) Φn(X)
c hidden state embeddings (unseen classes)
separability(bearsandreindeeraremixedduetotheirsimilarback-
L1, g1 = 0.24 L7, g7 = 0.69 L12, g12 = 0.53
grounds).Φ(x)denotesthefeaturevectorproducedforthedatapoint
x.(b)Arichermodelalso“knows”aboutantlers,claws,hooves,etc.
andusesthosetoseparatereindeerfromgrizzlybears,regardlessof
theirbackground.Notetheclusterseparability.
Thek-nearestneighborsmetricisusedbecauseK-meansimplic-
itly assumes Gaussian-like clusters, thus penalizing latent embed-
dings in which the classes may still be reasonably separable, but Figure 3: Schematic of our method of assessing generalizabil-
more uniformly distributed. The k-nearest neighbors metric avoids itythroughout-of-sampleembeddingsusingintermediatelayers.
thisassumptionviatherelaxed,intuitivenotionthatnearestneigh- (a)Duringtraining,onlyasubsetofallclassesinthedatasetareuti-
borsshouldbelongtothesameclass.Foreachdatapoint,wecom- lized(seenclasses,ingreen).(b)Attheinferencestage,onemayuse
puted its k-nearest neighbors (kNN) in an embedding, where k is the model to classify novel points from the seen classes (green) or
thenumberofexamplesineachclass.Tocomputegi ,weusedthe toextractintermediatefeaturevectorsΦ (X)fromanintermediate
kNN i
mean,overalldatapoints,ofthefractionofadatapointx’sknearest layeritoassessthedegreeofseparabilityoftheunseenclasses,as
neighborsbelongingtothesameclassasx. measuredbyourindexgi(eq.1).(c)Embeddingsfromdifferenthid-
Forthelinearprobe,wetrainedalinearclassificationheadusing denstatesoftheVisionTransformer(ViT)networkproducewidely
eachintermediatelayer’soutputaftershowingitatrainingsetof500 varyingresults.Colorlabelsindicategroundtruth:clusteredunseen
examples from the unseen classes and then testing it on 360 more classesindicatebettergeneralization(g).
examplesfromtheunseenclasses.
Tocontrolforrandomnessinthetraining,wefine-tunedandcal-
culatedmetricsforeachmodelthreetimesusingdifferentseedsand
computedtheaverageofeachresult,alongwithstandarddeviations.
,egailof
egailof
,sre.l.t.na
,egailof
egailof
,sre.l.t.na1.0 ViT - Calligraphy 1.0 1.0 ViT - CIFAR-100 1.0 1.0 ViT - Calligraphy ViT - CIFAR-100
0.8 0.8 0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4 0.4 0.4
giNMI
0.2 g gi i - - s ue ne sn een 0.2 0.2 0.2 0.2 gikNN 0.2
Lin. probe acc.
acc. - seen 0.0 0.0
0.0 0.0 0.0 0.0
0 2 4 6 8 10 12 0 2 4 6 8 10 12
100 200 300 400 500 100 200 300 400 500 hidden state depth, i hidden state depth, i
training step training step
PoolFormer - Calligraphy PoolFormer - CIFAR-100
1.0 Swin - Calligraphy 1.0 1.0 Swin - CIFAR-100 1.0 1.0 0.8
0.8 0.8 0.8 0.8 0.8 0.6
0.6 0.6 0.6 0.6
0.6
0.4
0.4 0.4 0.4 0.4
0.4
0.2 0.2 0.2 0.2 0.2
0.2
0.0 0.0 0.0 0.0 0 1 2 3 0 1 2 3
100 200 300 400 500 100 200 300 400 500 hidden state depth, i hidden state depth, i
training step training step
PViT - Calligraphy PViT - CIFAR-100
PViT - Calligraphy PViT - CIFAR-100 1.0
1.0 1.0 1.0 1.0 0.8
0.8
0.8 0.8 0.8 0.8 0.6 0.6
0.6 0.6 0.6 0.6
0.4 0.4
0.4 0.4 0.4 0.4
0.2 0.2
0.2 0.2 0.2 0.2
0.0 0.0
0.0 0.0 0.0 0.0 0 2 4 6 8 0 2 4 6 8
100 200 300 400 500 100 200 300 400 500 hidden state depth, i hidden state depth, i
training step training step
CvT - Calligraphy CvT - CIFAR-100
CvT - Calligraphy CvT - CIFAR-100 1.0
1.0 1.0 1.0 1.0 0.8
0.8 0.8 0.8 0.8 0.8 0.6
0.6 0.6 0.6 0.6
0.6 0.4
0.4 0.4 0.4 0.4
0.4 0.2
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0 0 1 2 0 1 2
100 200 300 400 500 100 200 300 400 500 hidden state depth, i hidden state depth, i
training step training step ConvNeXtV2 - Calligraphy ConvNeXtV2 - CIFAR-100
1.0 PoolFormer - Calligraphy 1.0 1.0 PoolFormer - CIFAR-100 1.0 1.0 0.8
0.8
0.8 0.8 0.8 0.8 0.6
0.6
0.6 0.6 0.6 0.6 0.4
0.4
0.4 0.4 0.4 0.4
0.2
0.2
0.2 0.2 0.2 0.2
0.0 0.0
0.0 0.0 0.0 0.0 0 1 2 3 4 0 1 2 3 4
100 200 300 400 500 100 200 300 400 500 hidden state depth, i hidden state depth, i
training step training step Swin - Calligraphy Swin - CIFAR-100
ConvNeXtV2 - Calligraphy ConvNeXtV2 - CIFAR-100 1.0
1.0 1.0 1.0 1.0
0.8 0.6
0.8 0.8 0.8 0.8
0.6 0.4
0.6 0.6 0.6 0.6
0.4
0.4 0.4 0.4 0.4 0.2
0.2
0.2 0.2 0.2 0.2
0.0 0.0
0.0 0.0 0.0 0.0 0 1 2 3 4 0 1 2 3 4
100 200 300 400 500 100 200 300 400 500 hidden state depth, i hidden state depth, i
training step training step Figure5:Generalizabilityvariesdifferentlyacrossdepthindiffer-
Figure4:Generalizabilitytounseenclassesvariesacrossarchitec- entnetworks.ForViT,maximumvaluesofgiareachievedinearly
tures,eventhoughaccuracyincreasesroughlymonotonicallyacross layers(top);forSwinonlyatthefinalstage(bottom).giisnotmono-
training epochs. We plot g unseen = max i(gi) and show generaliz- tonicwithdepthand,formanymodels(ViT,PViT,andPoolFormer),
ability to seen classes (g seen). g seen always dominates g unseen (as ex- thebestgeneralizationresidesatintermediatelayers.Thisholdstrue
pected). While one might assume that high classification accuracy acrossdatasetsandmetrics.(Note,especially,theagreementbetween
implies the model has learned a representation of its complete do- the two unsupervised methods, NMI and kNN). In most cases, all
main, these plots suggest that it is fitting well (or overfitting) only metricsidentifiedthesamelayerasthemostgeneralizabletounseen
thesub-domainsampledbythetrainingdata.(Errorbarsdenotestd. classes.Weconcludethat,sincethegicurvesarequalitativelysimi-
dev.) laracrossdatasets,thepatternsobservedfollowfromthenetworks’
architecture,andarenotspecifictoadataset.
g ,ytilibazilareneg
g ,ytilibazilareneg
g ,ytilibazilareneg
g ,ytilibazilareneg
g ,ytilibazilareneg
g ,ytilibazilareneg
)nees(
ycarucca
)nees(
ycarucca
)nees(
ycarucca
)nees(
ycarucca
)nees(
ycarucca
)nees(
ycarucca
g ,ytilibazilareneg
g ,ytilibazilareneg
g ,ytilibazilareneg
g ,ytilibazilareneg
g ,ytilibazilareneg
g ,ytilibazilareneg
)nees(
ycarucca
)nees(
ycarucca
)nees(
ycarucca
)nees(
ycarucca
)nees(
ycarucca
)nees(
ycarucca
ig
,ytilibazilareneg
ig ,ytilibazilareneg
ig
,ytilibazilareneg
ig
,ytilibazilareneg
ig
,ytilibazilareneg
ig
,ytilibazilareneg
ig
,ytilibazilareneg
ig ,ytilibazilareneg
ig
,ytilibazilareneg
ig
,ytilibazilareneg
ig
,ytilibazilareneg
ig
,ytilibazilareneg3 Results 4 Conclusion
Ascurrentmodelsbecomelargerandincreasinglyexpensivetotrain,
Afterfine-tuningsixnetworksontwodatasetsandmeasuringtheir
duetothecostofmanuallylabelingmanyimages,hardware,anden-
generalizationperformanceviaseveralmetrics,wefoundthatg,i.e.
ergyconsumption,thereisarealnecessityfordevelopingmodelsthat
themaxgi acrossalllayers,isalwayslowerontheunseendata,
NMI canreliablyorganizedatafromrelateddomainsinsuchawaythat
comparedtotheseendata(asexpected).Furthermore,thedifference
allows unseen classes to be distinguished (e.g., for few-shot learn-
isoftenquitestark,especiallyontheCIFARdataset,ascanbeseen
ing).
inFigure4.Alowgmeansthatregardlessofclassificationaccuracy,
Intuitively,differentarchitecturesarelikelytoimposedifferentin-
anintermediate-layerbasedembeddingfromthenetworkwouldnot
ductivebiases,whichmayormaynothelpwithgeneralization.First,
be useful unless that particular class had been encountered during
weconfirmedthathigheraccuracyonasubsetofthedomain(seen
training.
classes)doesnotimplyhighergeneralizability:althoughallmodels
Additionally,whiletrainingaparticularnetwork,ahigherclassi-
reachedhighclassificationaccuracyafterfine-tuning(atleast95%),
ficationaccuracydidnotalwaysleadtobettergeneralization.While
theyachievedwidelydifferentgeneralizationpower.
ourgeneralizabilitymetricsontheseenclassestendtoimprovewith
Second, our experiments demonstrated the central role architec-
classificationaccuracy,generalizabilityontheunseenclassesoften
ture plays: some architectures maximize generalization in shallow
plateausand,inmostnetworks,decreasesatleastonceduringtrain-
layers,whileothersonlygeneralizeattheend.Thishasobviousim-
ing.
plications for pruning and improving model efficiency at inference
Looking at generalizability across all layers—not just the best
time. In the case of ViT, for example, less than a third of the full
layer—,thereisnouniversaltrendastowhichlayerwillprovidethe
network is needed to achieve the highest levels of generalizability.
best representation for separating unseen examples; sometimes the
Webelievethatourproposedframeworkcanbeusedtotestarchitec-
lastlayerisbest,butoftenanearlierlayerisbetter,ascanbeseen
turalmodificationsandtheirimpactoninferringunseenclasses,and
in Figure 5. It is usually the case, however, that a network’s most
therebyguidefuturearchitecturaldesignandimprovements.
generalizablelayeridentifiedforonedataset,willbethesamefora
Futureworkinthisareawouldlookatspecificwaystoimprove
differentdataset.Comparingacrossdatasets,thelayergeneralization
generalizabilitythrougharchitecturedesign(e.g.,numberoflayers,
curvesarequalitativelysimilar,indicatingthatourmetriccapturesan
layer size, etc.), training paradigms (e.g., contrastive learning), or
intrinsicaspectofthearchitecture.
regularizationtechniques(e.g.,dropout).Crucially,ourmethodcan
Furthermore,thedifferentmetricstendtoagreequalitatively.The
beusedtoquantifywhichoftheseareactuallyimportant.
g curves align well with g for a given architecture, demon-
kNN NMI
strating that the assumption that the classes should be clustered is
reasonable.Thelinearproberesultsarelikewisesimilarwithregard Acknowledgements
totherelativeperformanceofeachlayer.Itsvaluesarehigheracross
theboard,whichisunsurprisingsincethelinearprobeisasupervised SupportedbyNIHGrant1R01EY031059,NSFGrant1822598.
approachandtrainedonlabeledexamples,incontrasttotheunsuper-
visedcluster-separationbasedapproach.Overall,thefindingsofthe
threemetricsagree,reinforcingtheirconclusions. References
Themodelaccuracyandresultsforallthreemetricsonseenand
unseen classes are reported in Table 1 for the CIFAR-100 dataset, [1] G.AlainandY.Bengio.Understandingintermediatelayersusinglinear
classifierprobes. In5thInternationalConferenceonLearningRepre-
Table2forthecalligraphydataset.
sentations,ICLR2017,Toulon,France,April24-26,2017,Workshop
TrackProceedings.OpenReview.net,2017.
Table1: Generalization g of classification networks for unseen and [2] G. Angeletti, B. Caputo, and T. Tommasi. Adaptive deep learning
throughvisualdomainlocalization. In2018IEEEInternationalCon-
seenclassesafterfine-tuningonCIFAR-100dataset.
ferenceonRoboticsandAutomation(ICRA),pages7135–7142.IEEE,
Network ViT Swin PViT CvT PF CNV2 2018.
accuracy 0.97 0.95 0.92 0.95 0.93 0.95 [3] S.Ben-David,J.Blitzer,K.Crammer,andF.Pereira. Analysisofrep-
g 0.84 0.71 0.76 0.78 0.75 0.79 resentationsfordomainadaptation. AdvancesinNeuralInformation
NMI,seen ProcessingSystems,19,2006.
g 0.26 0.04 0.21 0.15 0.22 0.15
NMI,unseen [4] S.Ben-David,J.Blitzer,K.Crammer,A.Kulesza,F.Pereira,andJ.W.
g 0.20 0.13 0.20 0.20 0.19 0.20
kNN,seen Vaughan.Atheoryoflearningfromdifferentdomains.Machinelearn-
g 0.36 0.24 0.33 0.31 0.33 0.30
kNN,unseen ing,79:151–175,2010.
g 0.96 0.92 0.92 0.95 0.92 0.94
LPr,seen [5] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan.
g LPr,unseen 0.90 0.67 0.82 0.80 0.83 0.80 Unsupervisedpixel-leveldomainadaptationwithgenerativeadversarial
networks. InProceedingsoftheIEEEConferenceonComputerVision
andPatternRecognition,pages3722–3731,2017.
[6] T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal,
Table2: Generalization g of classification networks for unseen and
A.Neelakantan,P.Shyam,G.Sastry,A.Askell,etal. Languagemod-
seenclassesafterfine-tuningoncalligraphydataset. elsarefew-shotlearners. Advancesinneuralinformationprocessing
systems,33:1877–1901,2020.
Network ViT Swin PViT CvT PF CNV2
[7] S.Bucci,A.D’Innocente,andT.Tommasi. Tacklingpartialdomain
accuracy 0.98 0.99 0.98 0.99 0.99 0.99
adaptationwithself-supervision. InImageAnalysisandProcessing–
g 0.97 0.8 0.95 0.94 0.94 0.93
NMI,seen ICIAP2019:20thInternationalConference,Trento,Italy,September
g 0.68 0.59 0.76 0.63 0.78 0.5
NMI,unseen 9–13,2019,Proceedings,PartII20,pages70–81.Springer,2019.
g kNN,seen 0.38 0.34 0.38 0.37 0.37 0.37 [8] W.-Y.Chen,Y.-C.Liu,Z.Kira,Y.-C.F.Wang,andJ.-B.Huang. A
g kNN,unseen 0.56 0.5 0.56 0.54 0.57 0.47 closerlookatfew-shotclassification.arXivpreprintarXiv:1904.04232,
g LPr,seen 0.99 0.95 0.98 0.99 0.98 0.99 2019.
g LPr,unseen 0.97 0.93 0.95 0.96 0.96 0.96 [9] L.Danon,A.Diaz-Guilera,J.Duch,andA.Arenas. Comparingcom-
munitystructureidentification. JournalofStatisticalMechanics:The-
oryandexperiment,2005(09):P09008,2005.[10] G.S.Dhillon,P.Chaudhari,A.Ravichandran,andS.Soatto. Abase- [34] Y. Wang. Chinese calligraphy styles by calligra-
lineforfew-shotimageclassification.arXivpreprintarXiv:1909.02729, phers. https://www.kaggle.com/datasets/yuanhaowang486/
2019. chinese-calligraphy-styles-by-calligraphers,2020.
[11] L.Dinh,R.Pascanu,S.Bengio,andY.Bengio.Sharpminimacangen- [35] J.Wei,M.Bosma,V.Y.Zhao,K.Guu,A.W.Yu,B.Lester,N.Du,
eralizefordeepnets.InInternationalConferenceonMachineLearning, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot
pages1019–1028.PMLR,2017. learners.arXivpreprintarXiv:2109.01652,2021.
[12] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, [36] S.Woo,S.Debnath,R.Hu,X.Chen,Z.Liu,I.S.Kweon,andS.Xie.
T.Unterthiner,M.Dehghani,M.Minderer,G.Heigold,S.Gelly,etal. ConvNeXtV2:Co-designingandscalingconvnetswithmaskedautoen-
Animageisworth16x16words:Transformersforimagerecognitionat coders. InProceedingsoftheIEEE/CVFConferenceonComputerVi-
scale.arXivpreprintarXiv:2010.11929,2020. sionandPatternRecognition,pages16133–16142,2023.
[13] A.Farhadi,I.Endres,D.Hoiem,andD.Forsyth. Describingobjects [37] H.Wu,B.Xiao,N.Codella,M.Liu,X.Dai,L.Yuan,andL.Zhang.
bytheirattributes. In2009IEEEconferenceoncomputervisionand Cvt:Introducingconvolutionstovisiontransformers. InProceedings
patternrecognition,pages1778–1785.IEEE,2009. oftheIEEE/CVFInternationalConferenceonComputerVision,pages
[14] A.Frome,G.S.Corrado,J.Shlens,S.Bengio,J.Dean,M.Ranzato, 22–31,2021.
and T. Mikolov. Devise: A deep visual-semantic embedding model. [38] Y. Xian, C. H. Lampert, B. Schiele, and Z. Akata. Zero-shot learn-
AdvancesinNeuralInformationProcessingSystems,26,2013. ing–acomprehensiveevaluationofthegood,thebadandtheugly.
[15] Z.Han,Z.Fu,S.Chen,andJ.Yang. Contrastiveembeddingforgener- IEEETransactionsonPatternAnalysisandMachineIntelligence,41
alizedzero-shotlearning. InProceedingsoftheIEEE/CVFconference (9):2251–2265,2018.
oncomputervisionandpatternrecognition,pages2371–2381,2021. [39] J.Yang,K.Zhou,Y.Li,andZ.Liu. Generalizedout-of-distribution
[16] D.HendrycksandK.Gimpel. Abaselinefordetectingmisclassified detection:Asurvey.arXivpreprintarXiv:2110.11334,2021.
and out-of-distribution examples in neural networks. arXiv preprint [40] Y.YangandT.M.Hospedales. Aunifiedperspectiveonmulti-domain
arXiv:1610.02136,2016. andmulti-tasklearning.arXivpreprintarXiv:1412.7489,2014.
[17] G.Kang,L.Jiang,Y.Yang,andA.G.Hauptmann. Contrastiveadap- [41] W.Yu,M.Luo,P.Zhou,C.Si,Y.Zhou,X.Wang,J.Feng,andS.Yan.
tationnetworkforunsuperviseddomainadaptation. InProceedingsof Metaformerisactuallywhatyouneedforvision.InProceedingsofthe
theIEEE/CVFConferenceonComputerVisionandPatternRecogni- IEEE/CVFConferenceonComputerVisionandPatternRecognition,
tion,pages4893–4902,2019. pages10819–10829,2022.
[18] N.S.Keskar,D.Mudigere,J.Nocedal,M.Smelyanskiy,andP.T.P. [42] C.Zhang,S.Bengio,M.Hardt,B.Recht,andO.Vinyals. Understand-
Tang. On large-batch training for deep learning: Generalization gap ingdeeplearning(still)requiresrethinkinggeneralization. Communi-
andsharpminima. InInternationalConferenceonLearningRepresen- cationsoftheACM,64(3):107–115,2021.
tations,2017.URLhttps://openreview.net/forum?id=H1oyRlYgg.
[19] T.Kojima,S.S.Gu,M.Reid,Y.Matsuo,andY.Iwasawa. Largelan-
guagemodelsarezero-shotreasoners. Advancesinneuralinformation
processingsystems,35:22199–22213,2022.
[20] A.Krizhevsky. Learningmultiplelayersoffeaturesfromtinyimages.
Technicalreport,2009.
[21] M.Lazarou,T.Stathaki,andY.Avrithis. Iterativelabelcleaningfor
transductive and semi-supervised few-shot learning. In Proceedings
oftheIEEE/CVFInternationalConferenceonComputerVision,pages
8751–8760,2021.
[22] J.LeiBa,K.Swersky,S.Fidler,etal. Predictingdeepzero-shotcon-
volutionalneuralnetworksusingtextualdescriptions. InProceedings
oftheIEEEinternationalConferenceonComputerVision,pages4247–
4255,2015.
[23] J. Liu, Z. Shen, Y. He, X. Zhang, R. Xu, H. Yu, and P. Cui. To-
wards out-of-distribution generalization: A survey. arXiv preprint
arXiv:2108.13624,2021.
[24] Z.Liu,Y.Lin,Y.Cao,H.Hu,Y.Wei,Z.Zhang,S.Lin,andB.Guo.
Swin transformer: Hierarchical vision transformer using shifted win-
dows. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages10012–10022,2021.
[25] C. Louizos, M. Welling, and D. P. Kingma. Learning sparse neural
networks through l0 regularization. In International Conference on
Learning Representations, 2018. URL https://openreview.net/forum?
id=H1Y8hhg0b.
[26] U.Maniyar,A.A.Deshmukh,U.Dogan,V.N.Balasubramanian,etal.
Zero shot domain generalization. arXiv preprint arXiv:2008.07443,
2020.
[27] A.Nichol,J.Achiam,andJ.Schulman. Onfirst-ordermeta-learning
algorithms.arXivpreprintarXiv:1803.02999,2018.
[28] F.Pourpanah,M.Abdar,Y.Luo,X.Zhou,R.Wang,C.P.Lim,X.-Z.
Wang,andQ.J.Wu. Areviewofgeneralizedzero-shotlearningmeth-
ods. IEEEtransactionsonpatternanalysisandmachineintelligence,
45(4):4051–4070,2022.
[29] S.Reed,Z.Akata,H.Lee,andB.Schiele. Learningdeeprepresenta-
tionsoffine-grainedvisualdescriptions. InProceedingsoftheIEEE
ConferenceonComputerVisionandPatternRecognition,pages49–58,
2016.
[30] K.SimonyanandA.Zisserman. Verydeepconvolutionalnetworksfor
large-scaleimagerecognition.arXivpreprintarXiv:1409.1556,2014.
[31] R.Socher,M.Ganjoo,C.D.Manning,andA.Ng. Zero-shotlearning
throughcross-modaltransfer.AdvancesinNeuralInformationProcess-
ingSystems,26,2013.
[32] M.WangandW.Deng.Deepvisualdomainadaptation:Asurvey.Neu-
rocomputing,312:135–153,2018.
[33] W.Wang,E.Xie,X.Li,D.-P.Fan,K.Song,D.Liang,T.Lu,P.Luo,
and L. Shao. Pyramid vision transformer: A versatile backbone for
densepredictionwithoutconvolutions.InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages568–578,2021.