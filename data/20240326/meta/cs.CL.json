[
    {
        "title": "TEI2GO: A Multilingual Approach for Fast Temporal Expression Identification",
        "authors": "Hugo SousaRicardo CamposAlípio Jorge",
        "links": "http://dx.doi.org/10.1145/3583780.3615130",
        "entry_id": "http://arxiv.org/abs/2403.16804v1",
        "pdf_url": "http://arxiv.org/pdf/2403.16804v1",
        "summary": "Temporal expression identification is crucial for understanding texts written\nin natural language. Although highly effective systems such as HeidelTime\nexist, their limited runtime performance hampers adoption in large-scale\napplications and production environments. In this paper, we introduce the\nTEI2GO models, matching HeidelTime's effectiveness but with significantly\nimproved runtime, supporting six languages, and achieving state-of-the-art\nresults in four of them. To train the TEI2GO models, we used a combination of\nmanually annotated reference corpus and developed ``Professor HeidelTime'', a\ncomprehensive weakly labeled corpus of news texts annotated with HeidelTime.\nThis corpus comprises a total of $138,069$ documents (over six languages) with\n$1,050,921$ temporal expressions, the largest open-source annotated dataset for\ntemporal expression identification to date. By describing how the models were\nproduced, we aim to encourage the research community to further explore,\nrefine, and extend the set of models to additional languages and domains. Code,\nannotations, and models are openly available for community exploration and use.\nThe models are conveniently on HuggingFace for seamless integration and\napplication.",
        "updated": "2024-03-25 14:23:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.16804v1"
    },
    {
        "title": "Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback",
        "authors": "Zhangqian BiYao WanZheng WangHongyu ZhangBatu GuanFangxin LuZili ZhangYulei SuiXuanhua ShiHai Jin",
        "links": "http://arxiv.org/abs/2403.16792v1",
        "entry_id": "http://arxiv.org/abs/2403.16792v1",
        "pdf_url": "http://arxiv.org/pdf/2403.16792v1",
        "summary": "Large language models (LLMs) have shown remarkable progress in automated code\ngeneration. Yet, incorporating LLM-based code generation into real-life\nsoftware projects poses challenges, as the generated code may contain errors in\nAPI usage, class, data structure, or missing project-specific information. As\nmuch of this project-specific context cannot fit into the prompts of LLMs, we\nmust find ways to allow the model to explore the project-level code context. To\nthis end, this paper puts forward a novel approach, termed ProCoder, which\niteratively refines the project-level code context for precise code generation,\nguided by the compiler feedback. In particular, ProCoder first leverages\ncompiler techniques to identify a mismatch between the generated code and the\nproject's context. It then iteratively aligns and fixes the identified errors\nusing information extracted from the code repository. We integrate ProCoder\nwith two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and\napply it to Python code generation. Experimental results show that ProCoder\nsignificantly improves the vanilla LLMs by over 80% in generating code\ndependent on project context, and consistently outperforms the existing\nretrieval-based code generation baselines.",
        "updated": "2024-03-25 14:07:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.16792v1"
    },
    {
        "title": "Can Machine Translation Bridge Multilingual Pretraining and Cross-lingual Transfer Learning?",
        "authors": "Shaoxiong JiTimothee MickusVincent SegonneJörg Tiedemann",
        "links": "http://arxiv.org/abs/2403.16777v1",
        "entry_id": "http://arxiv.org/abs/2403.16777v1",
        "pdf_url": "http://arxiv.org/pdf/2403.16777v1",
        "summary": "Multilingual pretraining and fine-tuning have remarkably succeeded in various\nnatural language processing tasks. Transferring representations from one\nlanguage to another is especially crucial for cross-lingual learning. One can\nexpect machine translation objectives to be well suited to fostering such\ncapabilities, as they involve the explicit alignment of semantically equivalent\nsentences from different languages. This paper investigates the potential\nbenefits of employing machine translation as a continued training objective to\nenhance language representation learning, bridging multilingual pretraining and\ncross-lingual applications. We study this question through two lenses: a\nquantitative evaluation of the performance of existing models and an analysis\nof their latent representations. Our results show that, contrary to\nexpectations, machine translation as the continued training fails to enhance\ncross-lingual representation learning in multiple cross-lingual natural\nlanguage understanding tasks. We conclude that explicit sentence-level\nalignment in the cross-lingual scenario is detrimental to cross-lingual\ntransfer pretraining, which has important implications for future cross-lingual\ntransfer studies. We furthermore provide evidence through similarity measures\nand investigation of parameters that this lack of positive influence is due to\noutput separability -- which we argue is of use for machine translation but\ndetrimental elsewhere.",
        "updated": "2024-03-25 13:53:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.16777v1"
    },
    {
        "title": "Synthetic Data Generation and Joint Learning for Robust Code-Mixed Translation",
        "authors": "KartikSanjana SoniAnoop KunchukuttanTanmoy ChakrabortyMd Shad Akhtar",
        "links": "http://arxiv.org/abs/2403.16771v1",
        "entry_id": "http://arxiv.org/abs/2403.16771v1",
        "pdf_url": "http://arxiv.org/pdf/2403.16771v1",
        "summary": "The widespread online communication in a modern multilingual world has\nprovided opportunities to blend more than one language (aka code-mixed\nlanguage) in a single utterance. This has resulted a formidable challenge for\nthe computational models due to the scarcity of annotated data and presence of\nnoise. A potential solution to mitigate the data scarcity problem in\nlow-resource setup is to leverage existing data in resource-rich language\nthrough translation. In this paper, we tackle the problem of code-mixed\n(Hinglish and Bengalish) to English machine translation. First, we\nsynthetically develop HINMIX, a parallel corpus of Hinglish to English, with\n~4.2M sentence pairs. Subsequently, we propose RCMT, a robust perturbation\nbased joint-training model that learns to handle noise in the real-world\ncode-mixed text by parameter sharing across clean and noisy words. Further, we\nshow the adaptability of RCMT in a zero-shot setup for Bengalish to English\ntranslation. Our evaluation and comprehensive analyses qualitatively and\nquantitatively demonstrate the superiority of RCMT over state-of-the-art\ncode-mixed and robust translation methods.",
        "updated": "2024-03-25 13:50:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.16771v1"
    },
    {
        "title": "ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search",
        "authors": "Zehan LiJianfei ZhangChuantao YinYuanxin OuyangWenge Rong",
        "links": "http://arxiv.org/abs/2403.16702v1",
        "entry_id": "http://arxiv.org/abs/2403.16702v1",
        "pdf_url": "http://arxiv.org/pdf/2403.16702v1",
        "summary": "Retrieval-based code question answering seeks to match user queries in\nnatural language to relevant code snippets. Previous approaches typically rely\non pretraining models using crafted bi-modal and uni-modal datasets to align\ntext and code representations. In this paper, we introduce ProCQA, a\nlarge-scale programming question answering dataset extracted from the\nStackOverflow community, offering naturally structured mixed-modal QA pairs. To\nvalidate its effectiveness, we propose a modality-agnostic contrastive\npre-training approach to improve the alignment of text and code representations\nof current code language models. Compared to previous models that primarily\nemploy bimodal and unimodal pairs extracted from CodeSearchNet for\npre-training, our model exhibits significant performance improvements across a\nwide range of code retrieval benchmarks.",
        "updated": "2024-03-25 12:34:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.16702v1"
    }
]