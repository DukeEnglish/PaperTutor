[
    {
        "title": "Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making",
        "authors": "Shuai MaQiaoyi ChenXinru WangChengbo ZhengZhenhui PengMing YinXiaojuan Ma",
        "links": "http://arxiv.org/abs/2403.16812v1",
        "entry_id": "http://arxiv.org/abs/2403.16812v1",
        "pdf_url": "http://arxiv.org/pdf/2403.16812v1",
        "summary": "In AI-assisted decision-making, humans often passively review AI's suggestion\nand decide whether to accept or reject it as a whole. In such a paradigm,\nhumans are found to rarely trigger analytical thinking and face difficulties in\ncommunicating the nuances of conflicting opinions to the AI when disagreements\noccur. To tackle this challenge, we propose Human-AI Deliberation, a novel\nframework to promote human reflection and discussion on conflicting human-AI\nopinions in decision-making. Based on theories in human deliberation, this\nframework engages humans and AI in dimension-level opinion elicitation,\ndeliberative discussion, and decision updates. To empower AI with deliberative\ncapabilities, we designed Deliberative AI, which leverages large language\nmodels (LLMs) as a bridge between humans and domain-specific models to enable\nflexible conversational interactions and faithful information provision. An\nexploratory evaluation on a graduate admissions task shows that Deliberative AI\noutperforms conventional explainable AI (XAI) assistants in improving humans'\nappropriate reliance and task performance. Based on a mixed-methods analysis of\nparticipant behavior, perception, user experience, and open-ended feedback, we\ndraw implications for future AI-assisted decision tool design.",
        "updated": "2024-03-25 14:34:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.16812v1"
    },
    {
        "title": "\"We Have No Idea How Models will Behave in Production until Production\": How Engineers Operationalize Machine Learning",
        "authors": "Shreya ShankarRolando GarciaJoseph M HellersteinAditya G Parameswaran",
        "links": "http://dx.doi.org/10.1145/3653697",
        "entry_id": "http://arxiv.org/abs/2403.16795v1",
        "pdf_url": "http://arxiv.org/pdf/2403.16795v1",
        "summary": "Organizations rely on machine learning engineers (MLEs) to deploy models and\nmaintain ML pipelines in production. Due to models' extensive reliance on fresh\ndata, the operationalization of machine learning, or MLOps, requires MLEs to\nhave proficiency in data science and engineering. When considered holistically,\nthe job seems staggering -- how do MLEs do MLOps, and what are their\nunaddressed challenges? To address these questions, we conducted\nsemi-structured ethnographic interviews with 18 MLEs working on various\napplications, including chatbots, autonomous vehicles, and finance. We find\nthat MLEs engage in a workflow of (i) data preparation, (ii) experimentation,\n(iii) evaluation throughout a multi-staged deployment, and (iv) continual\nmonitoring and response. Throughout this workflow, MLEs collaborate extensively\nwith data scientists, product stakeholders, and one another, supplementing\nroutine verbal exchanges with communication tools ranging from Slack to\norganization-wide ticketing and reporting systems. We introduce the 3Vs of\nMLOps: velocity, visibility, and versioning -- three virtues of successful ML\ndeployments that MLEs learn to balance and grow as they mature. Finally, we\ndiscuss design implications and opportunities for future work.",
        "updated": "2024-03-25 14:13:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.16795v1"
    },
    {
        "title": "As Good As A Coin Toss Human detection of AI-generated images, videos, audio, and audiovisual stimuli",
        "authors": "Di CookeAbigail EdwardsSophia BarkoffKathryn Kelly",
        "links": "http://arxiv.org/abs/2403.16760v1",
        "entry_id": "http://arxiv.org/abs/2403.16760v1",
        "pdf_url": "http://arxiv.org/pdf/2403.16760v1",
        "summary": "As synthetic media becomes progressively more realistic and barriers to using\nit continue to lower, the technology has been increasingly utilized for\nmalicious purposes, from financial fraud to nonconsensual pornography. Today,\nthe principal defense against being misled by synthetic media relies on the\nability of the human observer to visually and auditorily discern between real\nand fake. However, it remains unclear just how vulnerable people actually are\nto deceptive synthetic media in the course of their day to day lives. We\nconducted a perceptual study with 1276 participants to assess how accurate\npeople were at distinguishing synthetic images, audio only, video only, and\naudiovisual stimuli from authentic. To reflect the circumstances under which\npeople would likely encounter synthetic media in the wild, testing conditions\nand stimuli emulated a typical online platform, while all synthetic media used\nin the survey was sourced from publicly accessible generative AI technology.\n  We find that overall, participants struggled to meaningfully discern between\nsynthetic and authentic content. We also find that detection performance\nworsens when the stimuli contains synthetic content as compared to authentic\ncontent, images featuring human faces as compared to non face objects, a single\nmodality as compared to multimodal stimuli, mixed authenticity as compared to\nbeing fully synthetic for audiovisual stimuli, and features foreign languages\nas compared to languages the observer is fluent in. Finally, we also find that\nprior knowledge of synthetic media does not meaningfully impact their detection\nperformance. Collectively, these results indicate that people are highly\nsusceptible to being tricked by synthetic media in their daily lives and that\nhuman perceptual detection capabilities can no longer be relied upon as an\neffective counterdefense.",
        "updated": "2024-03-25 13:39:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.16760v1"
    },
    {
        "title": "Instantaneous Visual Analysis of Blood Flow in Stenoses Using Morphological Similarity",
        "authors": "Pepe EulzerKevin RichterAnna HundertmarkRalf WickenhöferCarsten M. KlingnerKai Lawonn",
        "links": "http://arxiv.org/abs/2403.16653v1",
        "entry_id": "http://arxiv.org/abs/2403.16653v1",
        "pdf_url": "http://arxiv.org/pdf/2403.16653v1",
        "summary": "The emergence of computational fluid dynamics (CFD) enabled the simulation of\nintricate transport processes, including flow in physiological structures, such\nas blood vessels. While these so-called hemodynamic simulations offer\ngroundbreaking opportunities to solve problems at the clinical forefront, a\nsuccessful translation of CFD to clinical decision-making is challenging.\nHemodynamic simulations are intrinsically complex, time-consuming, and\nresource-intensive, which conflicts with the time-sensitive nature of clinical\nworkflows and the fact that hospitals usually do not have the necessary\nresources or infrastructure to support CFD simulations. To address these\ntransfer challenges, we propose a novel visualization system which enables\ninstant flow exploration without performing on-site simulation. To gain\ninsights into the viability of the approach, we focus on hemodynamic\nsimulations of the carotid bifurcation, which is a highly relevant arterial\nsubtree in stroke diagnostics and prevention. We created an initial database of\n120 high-resolution carotid bifurcation flow models and developed a set of\nsimilarity metrics used to place a new carotid surface model into a\nneighborhood of simulated cases with the highest geometric similarity. The\nneighborhood can be immediately explored and the flow fields analyzed. We found\nthat if the artery models are similar enough in the regions of interest, a new\nsimulation leads to coinciding results, allowing the user to circumvent\nindividual flow simulations. We conclude that similarity-based visual analysis\nis a promising approach toward the usability of CFD in medical practice.",
        "updated": "2024-03-25 11:40:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.16653v1"
    },
    {
        "title": "Virtual Co-Pilot: Multimodal Large Language Model-enabled Quick-access Procedures for Single Pilot Operations",
        "authors": "Fan LiShanshan FengYuqi YanChing-Hung LeeYew Soon Ong",
        "links": "http://arxiv.org/abs/2403.16645v1",
        "entry_id": "http://arxiv.org/abs/2403.16645v1",
        "pdf_url": "http://arxiv.org/pdf/2403.16645v1",
        "summary": "Advancements in technology, pilot shortages, and cost pressures are driving a\ntrend towards single-pilot and even remote operations in aviation. Considering\nthe extensive workload and huge risks associated with single-pilot operations,\nthe development of a Virtual Co-Pilot (V-CoP) is expected to be a potential way\nto ensure aviation safety. This study proposes a V-CoP concept and explores how\nhumans and virtual assistants can effectively collaborate. A preliminary case\nstudy is conducted to explore a critical role of V-CoP, namely automated quick\nprocedures searching, using the multimodal large language model (LLM). The\nLLM-enabled V-CoP integrates the pilot instruction and real-time cockpit\ninstrumental data to prompt applicable aviation manuals and operation\nprocedures. The results showed that the LLM-enabled V-CoP achieved high\naccuracy in situational analysis and effective retrieval of procedure\ninformation. The results showed that the LLM-enabled V-CoP achieved high\naccuracy in situational analysis (90.5%) and effective retrieval of procedure\ninformation (86.5%). The proposed V-CoP is expected to provide a foundation for\nfuture virtual intelligent assistant development, improve the performance of\nsingle pilots, and reduce the risk of human errors in aviation.",
        "updated": "2024-03-25 11:31:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.16645v1"
    }
]