[
    {
        "title": "Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making",
        "authors": "Shuai MaQiaoyi ChenXinru WangChengbo ZhengZhenhui PengMing YinXiaojuan Ma",
        "links": "http://arxiv.org/abs/2403.16812v1",
        "entry_id": "http://arxiv.org/abs/2403.16812v1",
        "pdf_url": "http://arxiv.org/pdf/2403.16812v1",
        "summary": "In AI-assisted decision-making, humans often passively review AI's suggestion\nand decide whether to accept or reject it as a whole. In such a paradigm,\nhumans are found to rarely trigger analytical thinking and face difficulties in\ncommunicating the nuances of conflicting opinions to the AI when disagreements\noccur. To tackle this challenge, we propose Human-AI Deliberation, a novel\nframework to promote human reflection and discussion on conflicting human-AI\nopinions in decision-making. Based on theories in human deliberation, this\nframework engages humans and AI in dimension-level opinion elicitation,\ndeliberative discussion, and decision updates. To empower AI with deliberative\ncapabilities, we designed Deliberative AI, which leverages large language\nmodels (LLMs) as a bridge between humans and domain-specific models to enable\nflexible conversational interactions and faithful information provision. An\nexploratory evaluation on a graduate admissions task shows that Deliberative AI\noutperforms conventional explainable AI (XAI) assistants in improving humans'\nappropriate reliance and task performance. Based on a mixed-methods analysis of\nparticipant behavior, perception, user experience, and open-ended feedback, we\ndraw implications for future AI-assisted decision tool design.",
        "updated": "2024-03-25 14:34:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.16812v1"
    },
    {
        "title": "An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems",
        "authors": "Hanqing YangMarie SiewCarlee Joe-Wong",
        "links": "http://arxiv.org/abs/2403.16809v1",
        "entry_id": "http://arxiv.org/abs/2403.16809v1",
        "pdf_url": "http://arxiv.org/pdf/2403.16809v1",
        "summary": "The increasing prevalence of Cyber-Physical Systems and the Internet of\nThings (CPS-IoT) applications and Foundation Models are enabling new\napplications that leverage real-time control of the environment. For example,\nreal-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems\ncan reduce its usage when not needed for the comfort of human occupants, hence\nreducing energy consumption. Collecting real-time feedback on human preferences\nin such human-in-the-loop (HITL) systems, however, is difficult in practice. We\npropose the use of large language models (LLMs) to deal with the challenges of\ndynamic environments and difficult-to-obtain data in CPS optimization. In this\npaper, we present a case study that employs LLM agents to mimic the behaviors\nand thermal preferences of various population groups (e.g. young families, the\nelderly) in a shopping mall. The aggregated thermal preferences are integrated\ninto an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which\nemploys the LLM as a dynamic simulation of the physical environment to learn\nhow to balance between energy savings and occupant comfort. Our results show\nthat LLMs are capable of simulating complex population movements within large\nopen spaces. Besides, AitL-RL demonstrates superior performance compared to the\npopular existing policy of set point control, suggesting that adaptive and\npersonalized decision-making is critical for efficient optimization in CPS-IoT\napplications. Through this case study, we demonstrate the potential of\nintegrating advanced Foundation Models like LLMs into CPS-IoT to enhance system\nadaptability and efficiency. The project's code can be found on our GitHub\nrepository.",
        "updated": "2024-03-25 14:32:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.16809v1"
    },
    {
        "title": "Navigating the EU AI Act: A Methodological Approach to Compliance for Safety-critical Products",
        "authors": "J. KellyS. Ali ZafarL. HeidemannJ. ZacchiD. EspinozaN. Mata",
        "links": "http://arxiv.org/abs/2403.16808v1",
        "entry_id": "http://arxiv.org/abs/2403.16808v1",
        "pdf_url": "http://arxiv.org/pdf/2403.16808v1",
        "summary": "In December 2023, the European Parliament provisionally agreed on the EU AI\nAct. This unprecedented regulatory framework for AI systems lays out guidelines\nto ensure the safety, legality, and trustworthiness of AI products. This paper\npresents a methodology for interpreting the EU AI Act requirements for\nhigh-risk AI systems by leveraging product quality models. We first propose an\nextended product quality model for AI systems, incorporating attributes\nrelevant to the Act not covered by current quality models. We map the Act\nrequirements to relevant quality attributes with the goal of refining them into\nmeasurable characteristics. We then propose a contract-based approach to derive\ntechnical requirements at the stakeholder level. This facilitates the\ndevelopment and assessment of AI systems that not only adhere to established\nquality standards, but also comply with the regulatory requirements outlined in\nthe Act for high-risk (including safety-critical) AI systems. We demonstrate\nthe applicability of this methodology on an exemplary automotive supply chain\nuse case, where several stakeholders interact to achieve EU AI Act compliance.",
        "updated": "2024-03-25 14:32:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.16808v1"
    },
    {
        "title": "Cluster-Based Normalization Layer for Neural Networks",
        "authors": "Bilal FayeHanane AzzagMustapha Lebbah",
        "links": "http://arxiv.org/abs/2403.16798v1",
        "entry_id": "http://arxiv.org/abs/2403.16798v1",
        "pdf_url": "http://arxiv.org/pdf/2403.16798v1",
        "summary": "Deep learning faces significant challenges during the training of neural\nnetworks, including internal covariate shift, label shift, vanishing/exploding\ngradients, overfitting, and computational complexity. While conventional\nnormalization methods, such as Batch Normalization, aim to tackle some of these\nissues, they often depend on assumptions that constrain their adaptability.\nMixture Normalization faces computational hurdles in its pursuit of handling\nmultiple Gaussian distributions.\n  This paper introduces Cluster-Based Normalization (CB-Norm) in two variants -\nSupervised Cluster-Based Normalization (SCB-Norm) and Unsupervised\nCluster-Based Normalization (UCB-Norm) - proposing a groundbreaking one-step\nnormalization approach. CB-Norm leverages a Gaussian mixture model to\nspecifically address challenges related to gradient stability and learning\nacceleration.\n  For SCB-Norm, a supervised variant, the novel mechanism involves introducing\npredefined data partitioning, termed clusters, to normalize activations based\non the assigned cluster. This cluster-driven approach creates a space that\nconforms to a Gaussian mixture model. On the other hand, UCB-Norm, an\nunsupervised counterpart, dynamically clusters neuron activations during\ntraining, adapting to task-specific challenges without relying on predefined\ndata partitions (clusters). This dual approach ensures flexibility in\naddressing diverse learning scenarios.\n  CB-Norm innovatively uses a one-step normalization approach, where parameters\nof each mixture component (cluster in activation space) serve as weights for\ndeep neural networks. This adaptive clustering process tackles both clustering\nand resolution of deep neural network tasks concurrently during training,\nsignifying a notable advancement in the field.",
        "updated": "2024-03-25 14:17:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.16798v1"
    },
    {
        "title": "The Anatomy of Adversarial Attacks: Concept-based XAI Dissection",
        "authors": "Georgii MikriukovGesina SchwalbeFranz MotzkusKorinna Bade",
        "links": "http://arxiv.org/abs/2403.16782v1",
        "entry_id": "http://arxiv.org/abs/2403.16782v1",
        "pdf_url": "http://arxiv.org/pdf/2403.16782v1",
        "summary": "Adversarial attacks (AAs) pose a significant threat to the reliability and\nrobustness of deep neural networks. While the impact of these attacks on model\npredictions has been extensively studied, their effect on the learned\nrepresentations and concepts within these models remains largely unexplored. In\nthis work, we perform an in-depth analysis of the influence of AAs on the\nconcepts learned by convolutional neural networks (CNNs) using eXplainable\nartificial intelligence (XAI) techniques. Through an extensive set of\nexperiments across various network architectures and targeted AA techniques, we\nunveil several key findings. First, AAs induce substantial alterations in the\nconcept composition within the feature space, introducing new concepts or\nmodifying existing ones. Second, the adversarial perturbation itself can be\nlinearly decomposed into a set of latent vector components, with a subset of\nthese being responsible for the attack's success. Notably, we discover that\nthese components are target-specific, i.e., are similar for a given target\nclass throughout different AA techniques and starting classes. Our findings\nprovide valuable insights into the nature of AAs and their impact on learned\nrepresentations, paving the way for the development of more robust and\ninterpretable deep learning models, as well as effective defenses against\nadversarial threats.",
        "updated": "2024-03-25 13:57:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.16782v1"
    }
]