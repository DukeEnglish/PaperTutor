An LLM-Based Digital Twin
for Optimizing Human-in-the Loop Systems
Hanqing Yang∗, Marie Siew†, and Carlee Joe-Wong∗
∗Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA 15213 USA
†Information Systems Technology and Design Pillar, Singapore University of Technology and Design, 487372 Singapore
Emails: {hanqing3, cjoewong}@andrew.cmu.edu, marie_siew@sutd.edu.sg
Abstract—TheincreasingprevalenceofCyber-PhysicalSystems paper, we propose using LLM-powered agents to help build
andtheInternetofThings(CPS-IoT)applicationsandFoundation digital twins for adaptive control of human-in-the-loop
Models are enabling new applications that leverage real-time
(HITL) systems. While technological advances have enabled
control of the environment. For example, real-time control of
the implementation of such adaptive control systems for
Heating, Ventilation and Air-Conditioning (HVAC) systems can
reduce its usage when not needed for the comfort of human applications like urban transportation planning and human
occupants, hence reducing energy consumption. Collecting real- robot collaboration in manufacturing, collecting real-time data
time feedback on human preferences in such human-in-the-loop on human preferences remains a critical challenge. LLM-
(HITL) systems, however, is difficult in practice. We propose the
powered agents can solve this challenge by modeling how
use of large language models (LLMs) to deal with the challenges
multiplepopulationtypesinteractwiththesystem(e.g.interests,
of dynamic environments and difficult-to-obtain data in CPS
optimization. In this paper, we present a case study that employs mobility patterns, preferences, etc). Besides this, the LLM-
LLM agents to mimic the behaviors and thermal preferences of powered autonomous agents possess memory and planning
various population groups (e.g. young families, the elderly) in a capabilities, which helps to model human decision making.
shoppingmall.Theaggregatedthermalpreferencesareintegrated
We can then use data generated by the LLM to learn how
into an agent-in-the-loop based reinforcement learning algorithm
a system should adapt to its human users. In this paper, we
AitL-RL, which employs the LLM as a dynamic simulation of
the physical environment to learn how to balance between energy developanLLM-powereddigitaltwintosimulateuserbehavior
savings and occupant comfort. Our results show that LLMs are inabuilding,andapplyittotemperaturecontrol.Similarsetups
capableofsimulatingcomplexpopulationmovementswithinlarge canbeusedforotherhumanintheloopsystemcontrolsettings.
openspaces.Besides,AitL-RLdemonstratessuperiorperformance
As Cyber-Physical Systems and the Internet of Things
compared to the popular existing policy of set point control,
(CPS-IoT) applications integrate into our daily lives, they
suggesting that adaptive and personalized decision-making is
criticalforefficientoptimizationinCPS-IoTapplications.Through unlock new possibilities such as HITL control systems. One
this case study, we demonstrate the potential of integrating challenge for such systems lies in accurately representing the
advancedFoundationModelslikeLLMsintoCPS-IoTtoenhance complex dynamics of diverse groups’ interactions with their
system adaptability and efficiency. The project’s code can be
environments[9].OursolutionusesaLLMtogeneratedetailed
found on our GitHub repository.
simulations of environments such as a mall, producing data
I. INTRODUCTION on human-environment interactions without privacy concerns.
Large language models (LLMs) have been drawing signif- Furthermore,multi-objectiveoptimizationpresentsasignificant
icant attention, ever since the announcement of ChatGPT in challenge in the absence of a closed-form model to capture
November 2022 [1]. As LLMs have demonstrated powerful human feedback [10]. To address this, we employ Reinforce-
language understanding, reasoning, and generation capabilities, ment Learning (RL), allowing for the dynamic optimization of
there has been a growing research area [2]–[6] involving policies based on real-time human feedback iteratively.
generating multiple LLM-powered agents to simulate human Our case study: The building sector consumes around
behavior, interaction, and decision making, for various tasks 36% of global energy [11], with Heating, Ventilation and Air-
across domains: to play strategic communication games like Conditioning (HVAC) systems the largest contributors [12]. In
Avalon [4] and Werewolf [5], to participate in auctions [6], public buildings, cooling can account for more than 50% of
to see if they can model human behavior in classic social energyusage[12].Temperaturecontrolsolutionscanpotentially
science experiments [7], for planning in industrial automation reduce energy usage by adjusting the HVAC, as long as they
via digital twins [8], etc. In many of the above works, LLM- adaptively balance energy usage with occupant comfort.
powered agents were given different profiles, and prompted Weconsiderahighlyoccupiedpublicspace,suchasapublic
to take the role of their profile in their interactions. In this mall, a cinema, a library, or a communal office space. Unlike
homes or offices with individual temperature control, public
GitHubrepository:https://github.com/HappyEureka/LLM_digital_twin. spaces typically lack user-adjustable settings. Nevertheless,
WegratefullyacknowledgesupportfromMicrosoft’sAcceleratingFoun-
users may find the temperature too cold. At the same time,
dationModelsResearchinitiative.M.SiewissupportedbytheFacultyEarly
CareerAwardatSUTD.(Correspondingauthor:MarieSiew). increasing the temperature could also bring about energy
4202
raM
52
]YS.ssee[
1v90861.3042:viXrasavings. One way to take advantage of these opportunities
is a HITL system for temperature control, which collects real-
timeuserfeedback,e.g.,throughamobileapp,ontheircomfort
levels, and learns to adjust temperatures so as to balance
energy savings with user-comfort. Nevertheless, gathering
crowd-sourced user feedback can be challenging. Users may
be too busy (e.g., being immersed when shopping or watching
movies), or may not take the application seriously, giving false
dataontheirpreferences. Therefore,weproposeaLLM-based
digital twin solution, which uses LLM agents to generate data
for training a RL algorithm that balances energy savings with Fig.1. TheLLM-basedDigitalTwinAgentintheLoopDistributedControl
(AitL-RL)Pipeline.TheLLM-baseddigitaltwinsimulatespopulationbehavior
user comfort. Our contributions are summarized as follows:
in the mall across the day, with multiple population groups such as "teen
• We use GPT to simulate a building (mall) with multiple shoppers".Basedonthesimulation,userpreferencesareaggregatedandinput
intotheAgent-in-the-loopRLalgorithmforofflinetrainingtooptimizeuser
stores. We use LLM-powered autonomous agents, each
comfortandenergysavings.
representing a population group, and imbued with the
group’s unique characteristics like mobility patterns and
temperaturepreferencese.g.,familieswithyoungchildren, temperature optimization for air-conditioners in large public
elderly couples, etc. In every iteration, GPT 3.5 and 4 spaces, while balancing energy savings and user comfort. We
are used to simulate each population group’s arrivals study two control settings for HVAC systems: centralized and
and distribution across stores. Note that only querying is distributed. In the centralized setting, the entire building is
performed, maintaining a low computational footprint. maintainedatauniformtemperature.Incontrast,thedistributed
• We train the RL algorithm offline, using data generated setting involves multiple control points across subsections of
by the digital twin, to optimize energy savings and user the mall, as users in different parts of the building may have
comfort. We propose both centralized and distributed varying thermal comfort requirements e.g. due to usage-based
temperature control versions of the algorithm and show reasons (libraries vs cinemas) or location-based reasons (some
that the policies trained by the algorithms can generalize locations are nearer the entrance).
to new data when deployed in online scenarios.
A. Environment Setup
• Our experimental results demonstrate the superior per-
formance of distributed control settings over centralized With the aid of LLMs, we simulate an environment that
control and set-point control settings. mirrors a typical day in a shopping mall. Different population
groups and their activities in this space are modelled, with
We release the simulator for other researchers, industry
updates at each designated checkpoint. The environment
partners and the public to use and customize for specific
consists of three components: open space, population groups,
buildings; similar LLM-based digital twins can be used for
and movements of population groups in the open space. Fig. 1
other HITL systems in domains like transportation, urban and
shows the simulation system.
retail planning. We present our agents-in-the-loop solution in
Although it is possible to use a real-world mall setting, at
Section II and our evaluation results in Section III before
this preliminary stage, LLM simulations offer a simplified
concluding in Section IV.
and controllable setting that allows for focused study on
II. AGENT-IN-THE-LOOPLEARNING specific aspects of CPS devices’ interactions with humans.
Optimizing distributed HITL systems for conflicting objec- Additionally,simulationsprovidescalabilityandreproducibility
tives, e.g., in our context of balancing energy consumption
with utility, is challenging. Such systems are often inherently
complex and lack real-life data, which is often difficult to
obtain as the system involves human activity, interaction
and preferences. Fortunately, advancements in Foundation
Models have opened new possibilities for simulating complex
environments with believable human behaviours [2]–[6]. By
leveraging these models, it is feasible to create customized
simulations on which we can train reinforcement learning
(RL) techniques for the optimization of distributed systems.
This method bypasses the limitations of the traditional data
collection process, offering a cost-effective and time-efficient
methodology for adaptive distributed systems optimization.
Inthisstudy,weintroduceanovelframeworkfordistributed
system control optimization, the LLM-aided agents-in-the-loop Fig.2. ThelayoutoftheshoppingmallnamedHappyMall,whosefloorplan
wasgeneratedbyGPT4,afterminormanualadjustmentstobetterreplicatea
reinforcement learning framework (AitL-RL). We focus on
realmallsetting.across various conditions and scenarios, facilitating rapid
prototyping and iteration.
Open space: An open space resembling a large shopping
mall,cateringtoapproximately3000dailyconsumers,operates
from 10 a.m. to 8 p.m. The mall’s design is generated using
ChatGPT 4 (refer to Fig. 2), starting with an initial prompt
and refined iteratively. Minor manual adjustments were made
to store locations for a more realistic layout.
Initial generation:Pleasesimulateamid-sizeshoppingmall
named Happy Mall in JSON format. Further, please create a
mapoftheHappyMallona1600by1200canvas,indicating (a) Theaveragepopulationdistributionchangesovertime,with
the location of each component as described. differentlineshighlightingdistinctmovementpatternsforvarious
Refinement: Please make sure each component fits within groups.
the border. In addition, adjust the store distribution to vary
in size and spacing across the canvas more realistically, like
in a real mall setting. Please make corrections.
Population groups: The individuals in the open space are
simulated by categorizing them into groups, such as Teen
Shoppers, Elderly Couples, Tourist Groups, etc., achieved (b) Theaveragepopulationdistributionchangeforeachgroupin
through prompts to ChatGPT 4 with the category prompt. The aday.Thechangesincolourgradientdemonstratethedensityfor
eachgroupthroughouttheday.
movements of population groups are modelled using ChatGPT
3.5 at each 30-minute interval. The population and distribution
prompts determine the number of individuals per group and
the distribution of the groups across stores, respectively.
Categoryprompt:InthecontextoftheHappyMall,identify
the different visitor groups, focusing on their occupation
and age. Please provide this information in JSON format,
including a group description, thermal preference, and their
comfortable temperature range in degrees Celsius, divided
into ’high’ and ’low’ fields.
Population prompt: Located in a town center with a (c) Theaveragepopulationdistributionchangeforeachstorein
population of {TOWN POPULATION}, the Happy Mall aday.Thechangesincolourgradientrepresentthefrequencyof
opens at {OPEN TIME} and closes at {CLOSE TIME}. storevisits.
Currently, it’s {CURRENT TIME}. Descriptions of different
Fig.3. SimulationofmovementinHappyMalloveraday.Thesedistributions
groups in the mall are as follows: {DESCRIPTIONS OF for the digital twin are generated by GPT 3.5 in each time slot. The
GROUPS}. Given the time of day, list the number of people populationgroupsexhibitdifferenttemporalpatterns(resultsaveragedover
in each group using the format: [group name]: [number]; 10experiments).
[reason]. [-2ex]
Distribution prompt: Currently, it’s {CURRENT TIME}
and {INDOOR TEMPERATURE} ◦C. The descriptions of
the stores in the Happy Mall are as follows. {DESCRIP- higher density. We can clearly see that different population
TIONS OF STORES}. You belong to {GROUP NAME},
groupshavedifferentmovementpatterns,such asfamilieswith
described as {DESCRIPTION OF THE GROUP}, with a
young children peaking at 2 pm and 4:30 pm, teen shoppers
thermal preference of {THERMAL PREFERENCE OF THE
GROUP}. What is your group’s distribution in the mall? peaking at 12:30 pm and 3 pm, etc. Fig 3.c presents a heat
List the percentile for each store using the following format. map showing the average population changes in each store.
[store]: [percentile]; [reason]. Each store demonstrates unique peak hours.
Non-deterministic responses: A critical challenge with
Theaveragechangeinpopulationdistributionforeachgroup LLMs is their non-deterministic responses. However, our
is shown in Fig 3.a and Fig 3.b. In Fig 3.a, the line plot shows experiments demonstrate that by setting the temperature of
the average population counts for different groups within the the LLMs to zero, the simulations exhibit consistent trends
mall, along with the confidence level from 10 experiments. with tolerable variations. Human behaviors themselves are
Each line represents the average change in the population of also inherently variable and context-dependent. People respond
a specific group in the mall, highlighting various peak times differently based on a multitude of factors such as mood and
such as noon, afternoon, and evening. Fig 3.b is a heat map personal background. This variability is closely mirrored by
that shows the detail of the change in population for each the non-deterministic nature of LLMs, which makes LLMs
group. The colour gradient indicates the density of each group particularly promising for simulating real human behavior,
in the mall at each checkpoint, where brighter shades represent capturing the essence of human unpredictability and diversityinresponses.Infutureworks,theLLMspromptsforgenerating these weights, the algorithm can shift its focus, emphasizing
the digital twin simulations can be augmented with the limited one aspect over the other.
existing datasets, through retrieval augmented generation. The user comfort score is determined by subtracting the
number of users experiencing discomfort (cold or heat) multi-
B. Reinforcement Learning plied by a weight of 1, and adding the number of comfortable
users multiplied by a weight of 2. We model energy usage
The simulation provides information on the movements of
based on the heat transfer equation [13]:
each group across stores. The information which contains the
thermal preferences of the different groups is fed into the mc ∆T
EnergyUsage= a , (2)
AitL-RL framework to optimize the HVAC system for energy EER
efficiency while maintaining comfort for shoppers throughout
wheremisthemassoftheroom,c istheheatcapacityofdry
a
the day in both centralized and distributed control settings. We
air, ∆T is the temperature difference between the outdoor
train AitL-RL offline in an environment with the following
(ambient) temperature and the indoor (air-con controlled)
characteristics:
temperature,andEERistheratioofthecoolingcapacitytothe
State space: The state space is an amalgamation of the power input. The area of the space is set as 4890 m2, and its
human input [number of votes for "temperature increase",
height is set as 3 meters [14]. The density of dry air is 1.275
numberofvotesfor"temperaturedecrease",numberofvotesfor
kg/L. With this, we compute the mass as volume×density.
"constant temperature"], along with the external environment
The heat capacity of dry air is 1.00 J/(gK). The ambient
variables [current time, outdoor temperature, indoor tempera- temperature is set at 30 ◦C.
ture], as well as the group occupancy for each space.
To optimize the reward function, we input the state space
We simulate the human input with our LLM-powered
intoadeepQ-learningalgorithm[15].Theoutputoftheneural
agents. At each checkpoint, we collect user feedback from
network is the action taken. In Q-learning, the Q-function
the different population groups, i.e. the LLM-powered agents
Q(s,a) represents the value (sum of expected discounted
on their perceptions of the current indoor temperature. These
reward) of taking action a at state s, and the algorithm’s
perceptions are based on the thermal comfort ranges for each
aim is to iterate until convergence at the true Q-value.
population group, generated while creating the environment.
Users can either vote for a temperature increase, decrease,
Q(s,a)←(1−α)Q(s,a)+α(Reward(t)+γmaxQ(s′,a)),
a
or no change, signifying feelings of cold, heat, or comfort (3)
respectively. whereαisthelearningrate,andγisthediscountfactor.Indeep
Action:TheRLalgorithmcontrolstheHVACsystem,which Q-learning, the neural network parameterizes the Q-function.
controls the temperature of the indoor environment. For safety The loss function which the neural network optimizes is
reasons, the actions (temperature set) range between 17 to
L=[Reward+γmaxQ(s′,a′;θ′)−Q(s,a;θ)]2, (4)
29◦C, with intervals of 0.5◦C. In the centralized control
a′
setting, one RL model is trained to control the temperature
WesetthefollowingparametersfortheRLalgorithmwithin
for the entire mall; in the distributed control setting, multiple
the AitL-RL framework (Table 1).
RL models are trained, each responsible for the temperature
of an individual store. As a result, the centralized approach TABLEI
maintains a uniform temperature throughout the mall, whereas
HYPER-PARAMETERSFORAITL-RL
thedistributedapproachallowsforpersonalizedcontrolineach
Name Value Name Value
store, or subsection of the building.
Batchsize 128 LearningRate 10−4
Transition: A transition happens at each checkpoint, driven
Discountγ 0.99 we 1/220
by the arrivals, departures, and mobility patterns of the popu- Init.ϵ 0.9 wc 2.2
lation and the collected thermal feedback. Every timestamp, Finalϵ 0.05 τ 5×10−3
the collected feedback is input into our RL algorithm, AitL- ϵdecay 2000 Explorationrate 0.85e− 20x 00
RL, and the algorithm determines the temperature for the next
timestamp. The environment transitions from one timestamp
III. EVALUATIONANDRESULTS
(checkpoint) to the next timestamp. Note that the thermal
WetrainedAitL-RLofflineusingdatageneratedbytheLLM-
feedback is a function of the previous timestep’s action, which
powereddigitaltwininbothcentralizedanddistributedsettings.
is the indoor mall temperature.
For comparison, we used the set-point control approach as a
Reward: Our objective is to optimize for both energy cost
baseline,andwetrainedAitL-RLwithaspecificfocusoneither
and user comfort by maximizing the reward function:
user comfort or energy cost as alternative baselines.
Reward=w ×UserComfort(t)−w ×EnergyUsage(t), (1)
c e A. Experiment Design
which is a weighted sum of user thermal comfort Our objective is to train an algorithm to learn the optimal
(UserComfort(t)) and energy savings (EnergyUsage(t)), with policyfortemperaturecontrolinamall,balancingusercomfort
w and w representing the respective weights. By adjusting and energy costs. We have designed five different training
c esettings for the algorithm and evaluated their performances. to accommodate user preferences, but this results in higher
These include AitL-RL in both Centralized Control and energy costs, and vice versa when focusing on energy costs.
Distributed Control settings. We compare their results to
those of three baseline policies:
Set-point Control: Set-point Control refers to the technique
of Variable Refrigerant Flow (VRF) systems [16], as VRF
systems are often used in large buildings. Following the
approach of [16], we use a constant temperature of 25◦C,
as ASHRAE 55-2017 [17] recommends a summertime thermal
comfort zone of 23−26◦C, and 25◦C has been found to yield
the highest reward in trials.
User-comfort-focusedControl:thealgorithm’strainingwill
focus entirely on user comfort by setting w to 0.
e
Energy-focused Control: In this setting, the algorithm’s
training will focus entirely on energy cost by setting w to 0.
c
B. Reinforcement Learning Performance
Fig.4. Comparisonoftrainingconvergenceusingdifferentmethods:balanced
We employed reinforcement learning to determine the weights (considering both user comfort and energy cost), energy-focused,
optimal temperature control policy. Figure 4 illustrates the and user-comfort-focused. All settings converge, and the balanced weights
approach achieves the best overall score, emphasizing the need for equal
training process for both centralized and distributed settings.
considerationofmultipleaspects.
Each episode represents a day-long simulation with scores
calculated at every half-hour checkpoint. The generated hourly Figure 6.b displays the distributed control policy. The policy
population distribution was used for efficient offline training. adjusts the temperature for each store based on user activity
The converged RL network was then applied for an online and group, setting an optimum temperature tailored to smaller
scenario distinct from the training data. At each checkpoint, groups. This policy allows for personalized temperature deci-
the rewards were re-calculated based on the policy proposed sions,effectivelycateringtospecificneeds.Duringlowactivity
by the reinforcement learning algorithm. periods, temperatures are set closer to the ambient temperature
Performance metrics: The scores represent the total daily tominimizeenergycosts.InFigure6.c,whichfocusessolelyon
cumulative rewards, with comfort score reflecting the daily energy costs, the temperatures are significantly higher, aligning
cumulative comfort reward and energy score reflecting the more closely with ambient conditions (30◦C). Conversely, in
daily cumulative energy reward. The total score represents the Figure 6.d, when focusing on user comfort, the policy sets
weighted comfort score and the energy score. The first column temperatures significantly lower, as intuitively expected.
demonstrates training with balanced weights, indicating the
algorithm learns the optimal policy by considering both user
comfortandenergycost.Thesecondcolumnrepresentstraining
intheenergy-focusedcontrolsetting,disregardingusercomfort.
The right column displays training in the user-comfort-focused
control setting, ignoring energy costs. The figure reveals that
the balanced approach yields the highest total score. While
baseline policies can effectively optimize for either energy or
comfort, such a singular focus adversely impacts the other
Fig. 5. Comparison of policy performance in an online scenario using a
component, leading to a lower overall score.
balancedweightsapproach.TheAitL-RLpoliciesoutperformtheset-point
AitL-RLresults:Figure5showsthechangesintotalscores, policy,anddistributedcontrolismoreefficientthancentralizedcontrol.
which combine weighted user comfort and energy cost scores,
under centralized, distributed, and set-point control policies
using the balanced weights method. The results are observed
IV. DISCUSSIONANDCONCLUSION
on newly generated days in an online scenario distinct from In this paper, we propose an LLM-based digital twin to
the training data, which incorporates real-time control of the simulate human activity to optimize user comfort and energy
systemandfeedbackfromthepopulation.AitL-RLoutperforms savings. Our simulator uses the capabilities of LLMs to model
thesetpointpolicy(fixedtemperatureof25◦C).Thedistributed multiple population groups and their distinct behavior and
policyoutperformsthecentralizedone,asweintuitivelyexpect. preferences in the mall. A reinforcement learning algorithm is
Figure 6.a illustrates the policies for centralized control thenusedfortemperaturecontrol.LLMscansimulatecomplex
and set-point control. The centralized control policy adjusts human behaviors, facilitating efficient training of the RL
temperatures throughout the day, largely due to varying user algorithmforuser-preference-basedCPScontrols.Additionally,
activity levels. When focusing on user comfort only, the policy distributedcontrolsurpassescentralizedcontrolinperformance,
setstemperaturessignificantlylowercomparedtoothersettings due to its better accommodation of personalized environments.Future work could focus on making the LLM-based digital
twins more accurate simulators of human behaviors, to enable
effective learning of HITL control policies. For instance,
through finetuning them with occasional human feedback for
specificapplications,augmentingLLMpromptswhichgenerate
the simulations or augmenting the LLM-generated data with
existing (limited) datasets, or combining the policy trained
on the digital twin with one trained on partial human data.
The AitL-RL framework, with its LLM digital twin-based
generation of human data, is adaptable to other distributed
CPS scenarios requiring human interaction, such as traffic
management, public transportation planning, etc. For example,
(a) Comparisonofpolicyincentralizedcontrolsettings:
the LLM-based digital twin can simulate the mobility and thebalancedweightsmethodoptimizesforbothenergy
preferences of different population groups across the day cost and user comfort; taking wc = 0 yields a policy
in amusement parks and other tourist attractions, and hence
closetoambienttemperature;andwe=0adaptstouser
preferences.
optimize attraction and resource placement.
REFERENCES
[1] OpenAI,“ChatGPT,”2022,https://chat.openai.com/auth/login.
[2] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen,
J.Tang,X.Chen,Y.Linetal.,“Asurveyonlargelanguagemodelbased
autonomousagents,”arXivpreprintarXiv:2308.11432,2023.
[3] J. S. Park, J. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S.
Bernstein,“Generativeagents:Interactivesimulacraofhumanbehavior,”
inProceedingsofACMUIST,2023,pp.1–22.
[4] Y.Lan,Z.Hu,L.Wang,Y.Wang,D.Ye,P.Zhao,E.-P.Lim,H.Xiong,
andH.Wang,“Llm-basedagentsocietyinvestigation:Collaborationand
confrontationinavalongameplay,”arXivpreprint2310.14985,2023.
[5] Y.Xu,S.Wang,P.Li,F.Luo,X.Wang,W.Liu,andY.Liu,“Exploring (b) Distributed control policy. The policy personalizes decision-
largelanguagemodelsforcommunicationgames:Anempiricalstudy making for each store, given user movements, preferences, and
onwerewolf,”arXivpreprintarXiv:2309.04658,2023. energycosts.
[6] S. Mao, Y. Cai, Y. Xia, W. Wu, X. Wang, F. Wang, T. Ge, and
F.Wei,“Alympics:Languageagentsmeetgametheory,”arXivpreprint
arXiv:2311.03220,2023.
[7] G.V.Aher,R.I.Arriaga,andA.T.Kalai,“Usinglargelanguagemodels
to simulate multiple humans and replicate human subject studies,” in
ICML. PMLR,2023,pp.337–371.
[8] Y.Xia,M.Shenoy,N.Jazdi,andM.Weyrich,“Towardsautonomous
system:flexiblemodularproductionsystemenhancedwithlargelanguage
modelagents,”arXivpreprintarXiv:2304.14721,2023.
[9] M.Ma,W.Lin,D.Pan,Y.Lin,P.Wang,Y.Zhou,andX.Liang,“Data
anddecisionintelligenceforhuman-in-the-loopcyber-physicalsystems:
Referencemodel,recentprogressesandchallenges,”JournalofSignal
ProcessingSystems,vol.90,pp.1167–1178,2018.
[10] Y. Cui, Z. Geng, Q. Zhu, and Y. Han, “Multi-objective optimization (c) Baseline:Distributedcontrolpolicyinaenergy-focusedsetting
methodsandapplicationinenergysaving,”Energy,vol.125,pp.681–704, (wc = 0). The policy generates controls to maximize energy
2017. savings.
[11] M. Santamouris and K. Vasilakopoulou, “Present and future energy
consumptionofbuildings:Challengesandopportunitiestowardsdecar-
bonisation,” e-Prime-Advances in Electrical Engineering, Electronics
andEnergy,vol.1,p.100002,2021.
[12] A.R.Katili,R.Boukhanouf,andR.Wilson,“Spacecoolinginbuildings
in hot and humid climates—a review of the effect of humidity on
theapplicabilityofexistingcoolingtechniques,”in14thInternational
ConferenceonSustainableEnergyTechnologies(SET),2015.
[13] J.P.Holman,Heattransfer. McGrawHill,1986.
[14] J.Shen,J.Cao,andX.Liu,“Bag:Behavior-awaregroupdetectionin
crowdedurbanspacesusingwifiprobes,”WWW,2019.
[15] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare,A.Graves,M.Riedmiller,A.K.Fidjeland,G.Ostrovskietal.,
“Human-levelcontrolthroughdeepreinforcementlearning,”nature,vol.
(d) Baseline:Distributedcontrolpolicyinauser-comfort-focused
518,no.7540,pp.529–533,2015.
[16] J.Kim,D.Song,S.Kim,S.Park,Y.Choi,andH.Lim,“Energy-saving
setting (we = 0). The policy adapts temperature controls to
maximizeusercomfort.
potential of extending temperature set-points in a vrf air-conditioned
building,”Energies,vol.13,no.9,p.2160,2020.
Fig.6. Comparisonofpolicy(temperaturecontrol)inanonlinescenariousing
[17] A.ASHRAE,“Standard55-2017,”Thermalenvironmentalconditions
abalancedweightsapproach.TheAitL-RLshowsthatadaptivecontrolbased
forhumanoccupancy,2017.
onusermovementsandpreferencesresultsinmoreeffectiveoptimizations.