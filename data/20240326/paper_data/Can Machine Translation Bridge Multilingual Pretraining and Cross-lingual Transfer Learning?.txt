Can Machine Translation Bridge Multilingual Pretraining and
Cross-lingual Transfer Learning?
Shaoxiong Ji 1 Timothee Mickus 1 Vincent Segonne 2 Jörg Tiedemann 1
1 UniversityofHelsinki 2 UniversiteGrenobleAlpes
firstname.lastname@helsinki.fi
Abstract
Multilingual pretraining and fine-tuning have remarkably succeeded in various natural language processing
tasks. Transferringrepresentationsfromonelanguagetoanotherisespeciallycrucialforcross-linguallearning.
One can expect machine translation objectives to be well suited to fostering such capabilities, as they involve
the explicit alignment of semantically equivalent sentences from different languages. This paper investigates
the potential benefits of employing machine translation as a continued training objective to enhance language
representation learning, bridging multilingual pretraining and cross-lingual applications. We study this question
through two lenses: a quantitative evaluation of the performance of existing models and an analysis of their
latent representations. Our results show that, contrary to expectations, machine translation as the continued
training fails to enhance cross-lingual representation learning in multiple cross-lingual natural language under-
standingtasks. Weconcludethatexplicitsentence-levelalignmentinthecross-lingualscenarioisdetrimentalto
cross-lingual transfer pretraining, which has important implications for future cross-lingual transfer studies. We
furthermore provide evidence through similarity measures and investigation of parameters that this lack of posi-
tiveinfluenceisduetooutputseparability—whichweargueisofuseformachinetranslationbutdetrimentalelsewhere.
Keywords:Cross-lingual Transfer Learning, Representation Similarity and Explainability, Machine Transla-
tion,MultilingualLanguageModels
1. Introduction lingualtransferdownstreamtasks. Weexpectthat
MTobjectives,astheyprovideexplicitcross-lingual
Thesuccessesofpretrainedmultilinguallanguage alignments, should benefit cross-lingual transfer
models(LM)oncross-lingualtaskshavebeenun- tasks. This paper, therefore, focuses on compar-
derscored time and time again (Wu and Dredze, ingthecross-lingualabilitiesofpubliclyavailable
2019,e.g.,),andappearallthemoresurprisingthat pretrainedmodels—bothMTmodelstrainedfrom
theyareoftenpretrainedondatasetscomprising scratchandmultilingualLMswherepretraininghas
multiplelanguages,withoutexplicitcross-lingualsu- beencontinuedwithanMTobjective. Weattempt
pervision(cf.,forinstance,Liuetal.,2020,though toestablishwhetherMTtrainingobjectivesimplic-
explicit supervision also exists, Xue et al., 2021). itlyfostercross-lingualalignment:
Explicitalignmentssuchaslinearmapping(Wang
(i) Do models (re)trained with the MT objective
et al., 2019) and L2 alignment (Cao et al., 2020)
developcross-lingualrepresentations?
betweensourceandtargetlanguagesdonotnec-
essarilyimprovethequalityofcross-lingualrepre- (ii) Dotheygeneralizewelloncross-lingualtasks?
sentations(WuandDredze,2020).
(iii) Whichfactorsimpacttheirperformances?
Thisissomewhatatoddswithexpectationsfrom
earlierstudiesinmachinetranslation(MT).Inpar-
Findings WefindthatMT(continued)trainingob-
ticular, MT systems have historically connected
jectivesdonotfavortheemergenceofcross-lingual
with the concept of an interlingua—a language-
alignmentsmorethanLMobjectives,basedonthe
independentrepresentationspacethatMTsystems
studyonexistingpubliclyavailablepretrainedmod-
can leverage to perform translation (Masterman,
els. Weprovideevidencefromsimilarityanalyses
1961;Luetal.,2018). Assuch,MTmodelsareex-
andparameter-levelinvestigationsthatthisisdueto
pectedtopickuponlanguage-independentseman-
separability,whichisbeneficialinMTbutdetrimen-
ticfeatures(Tiedemann,2018)—thoughinpractice,
talelsewhere. WeconcludethatMTencourages
thissharedrepresentationspacecanbeinatrade-
behavior that is not necessarily compatible with
off relationship with performance, which benefits
highperformancesincross-lingualtransferlearn-
fromagreaterseparabilityofsourcelanguagerep-
ing.
resentations(Chenetal.,2023,e.g.).
2. Experimental protocol
Research questions This paper investigates
whether machine translation as a learning objec- Our goal is to compare LM and MT models on
tivecanimproveperformancesonzero-shotcross- cross-lingualbenchmarks. Wefirstdescribemulti-
4202
raM
52
]LC.sc[
1v77761.3042:viXralingualLMsandMTsystems,cross-lingualtasks, languages, even if some of them are not directly
anddatasetsusedinourexperiments. involvedinthedownstreamtasks. Catastrophicfor-
gettingisasignificantchallengeincontinuallearn-
ing scenarios. However, we stress that this falls
2.1. Publicly available pretrained models
beyondthescopeofourpaper,asourprimaryfocus
Multilinguallanguagemodels Westudythree isonthemodeltrainingwithdifferentlearningob-
differentmultilingualLMs. Themainmodelwefo- jectives,andtheirempiricalresultsoncross-lingual
cus on is the multilingual sequence-to-sequence taskswithafurtherstepoftrainingondownstream
mBART-largemodel(Tangetal.,2020). Itispre- datasets.
trained with a denoising objective and covers 50
languages. It has a 12-layer encoder, a 12-layer
2.2. Cross-lingual tasks and datasets
decoder,ahiddendimensionof1024,and16atten-
tionheads,foratotalofabout680Mparameters. Westudythemodels’performancesdetailedinSec-
Wealsocomparetheresultswithmaskedlanguage tion 2.1 on standard cross-lingual NLP tasks. In
modelsasreferencesbycontrollingthesamelevel allcases,modelsaretrainedforthedownstream
ofthenumberofparameters,mainlythenumberof applicationinonelanguage(usuallyEnglish),and
transformer layers. We consider mBERT (Devlin thetrainedmodelisthenevaluatedinlanguages
et al., 2019) and XLM-R (Conneau et al., 2020) other than the language used for training. We
12-layerbasearchitecturestogivearelativelyfair use the XGLUE cross-lingual evaluation bench-
comparison. Nevertheless, for the large mBART mark (Liang et al., 2020) and conduct our eval-
architectures,althoughweonlyutilizethe12-layer uation on natural language understanding tasks.
encoder,mBARTencodershaveroughly10%pa- ThespecifictasksconsistofNamedEntityResolu-
rametersmorethanmBERT(Devlinetal.,2019). tion(NER,Sang,2002;SangandMeulder,2003),
Part-of-Speechtagging(POS,Zemanetal.,2020),
News Classification (NC), natural language infer-
Machinetranslationmodel Wefocusonthe“No
ence (XNLI, Conneau et al., 2018), paraphrase
LanguageLeftBehind”translationsystem(‘NLLB’,
detection(PAWS-X,Yangetal.,2019),Query-Ad
Costa-jussàetal.,2022). Thismodeldistinguishes
Matching (QADSM), Web Page Ranking (WPR),
itselfbyusingMixture-of-Expertsfeedforwardsub-
and QA Matching (QAM). Table 3 in Appendix A
layers,intendedtoensurethatthemodelcanhan-
summarizesthebenchmarksusedinourstudy. For
dle inputs from diverse languages. We use the
namedentityrecognition(NER)andwebpagerank-
distilledmodelwith600Mparameterstokeeppa-
ing (WPR), we use the F1 score and normalized
rameter counts roughly consistent with the afore-
discountedcumulativegain(nDCG)astheevalu-
mentionedmultilingualLMs.
ationmetric. Theothertasksuseaccuracyasthe
metric.
MT as continued pretraining Our starting hy-
pothesisisthattheMTobjectiveprovidesanexplicit
2.3. Hyperparameters
cross-lingualsentencealignmentthatislikelyben-
eficialforcross-lingualtransfer. Anatural,testable We control most experimental settings to enable
consequenceofthishypothesisisthatfurthertrain- faircross-lingualevaluationasmuchaspossible.
ingmultilingualLMswithanMTobjectiveshould Weuse12-layerencodersforeachbackbonenet-
bolsterthemodels’performanceoncross-lingual work. For optimization, we use the AdamW opti-
transferlearningbenchmarks. Werefertothisse- mizer(LoshchilovandHutter,2019)andlearning
quentialtrainingonanMTobjectiveascontinued rateschedulewithlinearwarmupanddecay. We
pretrainingorCP,todistinguishitfromtask-specific setthelearningrateto2×10−5 forPOStagging
fine-tuningprocesses. Weusethreepubliclyavail- and5×10−6fortheothertasks. Themaxsequence
ablemBARTmodelswherepretrainingwascontin- lengthis256,andwefine-tuneeachmodelfor10
uedonmachinetranslationobjectives(Tangetal., epochs.
2020): a many-to-many (m2m) which translates
betweenanypairoflanguagesfromapoolof50;a
3. Results and analyses
many-to-one(m2o)fromanyof49languagestoEn-
glish;andaone-to-many(o2m)fromEnglishtoany
3.1. Quantitative performance
of49languages. ThecontinualtrainingofmBART
coversalargernumberoflanguagesthanthedown- We first compare the overall performance of the
stream evaluation. Fine-tuning on a larger set of models listed in Section 2.1 on the downstream
languages might provide the model with a more cross-lingualbenchmarksoutlinedinSection2.2.
diverselinguisticrepresentation. Weareinterested Table 1 shows the overall performance by aver-
inthehypothesisthatthisdiversitycouldpotentially aging the scores of each language. XLM-R dis-
enhance the model’s ability to generalize across playsthehighestperformanceson6outof8tasks,andmBARTobtainsthebestaveragescoreonthe
l
m
f
ia nos
B
rm
mt At ow
R
w
so
T
o
t.
m
r
cM
s
a2
eo sod
et,
he
sm
al .s nBc
MA
lao
R
un
nT
lt
g
ti in
u
lo
iu na2a ggml uely
, aa
m
lp nr Mode dt Tmr ea
lB
mi sn
A
oe
(R
id
d.eT
eo
.
l,n
sm
mM
2
thBmT
aA)
t( Rpi e. ee
T
nr.
)
--, 0000000000 .......... 9999999898 7877765836 0000000000 .......... 9999999889 2202333963 0000000000 .......... 9999999897 7766765153 0000000000 .......... 9999999897 8888875148 0000000000 .......... 9889998787 2880008274
0000 .... 8899 0505
0000000000 .......... 9999988768 7432086734 0000000000 .......... 9888887779 5662409962 0000000000 .......... 9999999877 7555653245 0000000000 .......... 9999998877 8765429134 0000000000 .......... 9888888756 1987641987
000000 ...... 778899 050505
code multiple source languages (i.e., m2m and 00 .. 99 78 00 .. 99 54 00 .. 99 77 00 .. 99 88 00 .. 99 04 0.75 00 .. 99 88 00 .. 99 56 00 .. 99 77 00 .. 99 99 00 .. 99 44 0.65
m2o)displaycomparableorslightlyimprovedper- 1 d.0 e0 0 e.9 n8 0 e.9 s9 1. f0 r0 0 r.9 u7 0 d.9 e9 0 e.9 n7 0 e.9 s8 0. f9 r9 0 r.9 u5 0.60
formances;forexample,mBARTm2moutperforms (a)mBARTvsmBARTm2m (b)XLM-RvsmBARTm2m
mBERTonPAWS-XandmBARTm2ooutperforms
mBERTonXNLIandQADSM.1 MTmodelsbased 00 .. 98 77 00 .. 99 31 00 .. 88 90 00 .. 98 54 00 .. 97 37 0.975 00 .. 58 95 00 .. 69 90 00 .. 77 78 00 .. 78 51 00 .. 57 85 0.95
o thn em EnB gA liR shT ta ec sh ti se ev te ins mat ois sf ta cc ato sr ey sp be ur tfo far im lta on bc re ido gn
e
0000 .... 9999 7650 0000 .... 9998 1125 0000 .... 9998 7655 0000 .... 9998 8754 0000 .... 9998 2327 000 ... 999 025 050 0000 .... 9887 0863 0000 .... 7778 8341 0000 .... 9998 5524 0000 .... 9988 4194 0000 .... 8887 7410 00 .. 89 50
0.97 0.91 0.97 0.98 0.92 0.875 0.92 0.79 0.95 0.95 0.88 0.80
pretraining and cross-lingual transfer learning in 00 .. 99 88 00 .. 99 53 00 .. 99 87 00 .. 99 89 00 .. 99 21 0.850 00 .. 99 43 00 .. 88 86 00 .. 99 55 00 .. 99 76 00 .. 99 10 0.75
other languages. Overall, we find that machine 000 ... 999 888 000 ... 999 854 000 ... 999 888 000 ... 999 999 000 ... 999 455 00 .. 88 02 05 000 ... 999 888 000 ... 999 564 000 ... 999 778 000 ... 999 999 000 ... 999 454 00 .. 67 50
translation as continued pretraining does not im- 0.99 0.98 0.99 0.99 0.99 0.775 0.97 0.97 0.97 0.97 0.97 0.60
de en es fr ru de en es fr ru
provecross-lingualperformance.
(c)mBARTvsmBARTm2o (d)XLM-RvsmBARTm2o
Tasks 0.73 0.93 0.61 0.64 0.41 0.79 0.93 0.69 0.75 0.51
Model 0.70 0.74 0.77 0.83 0.49 0.76 0.92 0.82 0.81 0.61
NC XNLIPAWS-XQAMQADSMWPRNERPOS 0.76 0.76 0.78 0.83 0.54 0.9 0.81 0.87 0.90 0.86 0.65 0.9
0.93 0.92 0.94 0.95 0.73 0.83 0.73 0.91 0.89 0.68
mBERT 81.3 65.2 86.6 64.6 63.1 74.4 77.5 76.0 00 .. 99 53 00 .. 99 20 00 .. 99 64 00 .. 99 76 00 .. 77 83 0.8 00 .. 88 74 00 .. 77 91 00 .. 99 42 00 .. 99 30 00 .. 76 48 0.8
LMXLM-R 82.1 73.5 88.9 67.4 66.9 75.3 78.7 79.7 0.96 0.92 0.96 0.98 0.81 0.7 0.89 0.79 0.94 0.95 0.78
mBART 82.1 67.6 89.2 67.8 65.5 74.7 77.7 72.7 000 ... 999 666 000 ... 999 421 000 ... 999 766 000 ... 999 888 000 ... 988 186 0.6 000 ... 999 622 000 ... 888 936 000 ... 999 745 000 ... 999 876 000 ... 998 206 0.7
MTNLLB600M 76.0 68.3 73.4 61.5 63.9 73.7 54.2 71.4 00 .. 99 87 00 .. 99 65 00 .. 99 88 00 .. 99 99 00 .. 99 74 0.5 00 .. 99 78 00 .. 99 33 00 .. 99 68 00 .. 99 99 00 .. 89 93 0.6
0.99 0.97 0.99 0.99 0.97 0.98 0.96 0.98 0.98 0.94
mBARTm2o 80.4 65.9 85.6 63.9 63.9 73.7 61.5 70.8 de en es fr ru de en es fr ru
CPmBARTo2m 65.4 48.1 81.7 58.4 62.7 73.2 55.1 55.7
(e)mBARTvsmBARTo2m (f)XLM-RvsmBARTo2m
mBARTm2m78.3 60.2 87.2 63.2 62.8 73.7 71.9 69.7
T
1t
aaa 2nsb
-dk
ll ae
s
X
y.
eL1
W
rM: e-eA
nR
cv
u
.
oe
s
m
dr ea eBg
t rAh
.e
e
Rp Tbe
a
sr sf co
e
or rm
a
era scn
h
ac
i
rte
eec
do
tu
en
r re
ic vr efo
o
ds rs
fm
r- oli
B
mn Eg tu
R
ha
T
el 0000000000 .......... 9999999868 4644654385 0000000000 .......... 8888999879 9677010182 0000000000 .......... 9999999767 1554543959 0000000000 .......... 9999999878 8876755210 0000000000 .......... 8888888757 5433454674 000000 ...... 778899 050505 0000000000 .......... 9999999868 6643543042 0000000000 .......... 9888888878 0786667209 0000000000 .......... 9999999868 0234443092 0000000000 .......... 9999999877 8876755429 0000000000 .......... 8888888658 4335898272 000000 ...... 778899 050505
0.94 0.90 0.90 0.97 0.84 0.65 0.96 0.90 0.90 0.98 0.85 0.65
00 .. 99 76 00 .. 98 59 00 .. 99 64 00 .. 99 99 00 .. 68 92 0.60 00 .. 99 86 00 .. 98 47 00 .. 99 73 00 .. 99 99 00 .. 77 09 0.60
de en es fr ru de en es fr ru
3.2. Representation similarity (g)mBERTvsmBARTm2m (h)mBERTvsmBARTm2o
WehaveestablishedthatCPandMTmodelsfare
0.71 0.93 0.65 0.64 0.61
worsethanavailablemultilingualLMs,disproving 000 ... 888 892 000 ... 899 703 000 ... 887 976 000 ... 998 102 000 ... 676 809 00 .. 99 05
our starting hypothesis. We now turn to whether 0.89 0.85 0.91 0.92 0.69 0.85
0.92 0.88 0.94 0.96 0.73
0.92 0.88 0.94 0.96 0.75 0.80 thesequantitativedifferencestranslateintoquali- 0.92 0.89 0.94 0.96 0.80 0.95 0.86 0.95 0.98 0.80 0.75
tative differences that we can observe in the rep- 0.96 0.89 0.94 0.98 0.78 0.70
0.96 0.90 0.93 0.98 0.77
0.95 0.86 0.93 0.98 0.70 0.65
resentational space. We first examine the hid- 0.98 0.93 0.96 0.99 0.70
de en es fr ru
denrepresentationsbycomparingtherepresenta-
(i)mBERTvsmBARTo2m
tionalsimilaritybetweendifferentmodelsusingthe
CenteredKernelAlignment(CKA)(Kornblithetal., Figure 1: Representational similarity between
2019)metric. CKAiscalculatedas mBART-basedMTmodelsandLMs
(cid:13) (cid:13)Y⊤X(cid:13) (cid:13)2
CKA(X,Y)= F
(∥X⊤X∥ ∥Y⊤Y∥ )
F F CPmodelsbasedonmBART(m2mandm2o)learn
whereX∈Rn×d andY ∈Rn×d arepooledrepre- moresimilarlanguagerepresentationstomBART
sentations of n data samples with the dimension thanXLM-RbecausetheMTpretrainingofthese
ofd,and∥ · ∥ denotestheFrobeniusnorm. Fig- modelswascontinuedfromanmBARTcheckpoint.
ure1showstheF representationalsimilaritybetween However, some representations of mBART o2m,
CP models (mBART-based multilingual MT) and especially those in Russian, are highly dissimilar
languagemodels(mBERT,mBART,andXLM-R) to those of mBART. We assume this is an effect
obtainedfrom80datasamplesfromtheNCdataset. ofthecontinuedpretrainingwithatranslationob-
jective from English to other languages: Cyrillic
scriptbeingirrelevanttothistask,wecanexpect
1Thedetailedper-languageperformanceisavailable
inAppendixB.1,Table4. Inshort,mBARTandcorre- thattheo2mCPmodeldoesnotneedtomaintain
spondingMTmodelsperformpoorlyonlanguagesthat thequalityofthecorrespondingword-piecerepre-
areunavailableinitstrainingdata. sentations. Wealsoobservesomeoutliersinthe
211101
9
8 7 6
5
4
3
2 1
0
2111019
8 7 6
5 4
3 2
1
0
2111019
8 7
6
5 4
3
2 1
0
211101
9
8 7 6
5
4
3
2
1 0
2111019
8
7
6 5 4
3
2
1
0
2111019
8 7 6
5
4
3
2 1
0
2111019
8 7 6
5 4
3 2
1
0
2111019
8 7
6
5 4
3
2 1
0
2111019
8 7 6
5
4
3
2
1 0K Q V Out FCup FCdown
Model
∥σ∥ d ∥σ∥ d ∥σ∥ d ∥σ∥ d ∥σ∥ d ∥σ∥ d
mBART 44.76 – 44.85 – 53.73 – 53.45 – 90.25 – 99.63 –
mBARTm2m 48.28 4.23 48.29 4.07 55.65 2.73 55.14 3.01 99.28 9.47 107.94 9.63
mBARTm2o 48.34 4.23 48.35 4.06 56.19 2.95 55.73 2.99 101.06 11.19 109.71 11.18
mBARTo2m 56.13 11.76 56.25 11.74 60.17 7.18 59.32 7.07 116.17 26.34 120.50 22.15
Table2: SVDscalingeffectformBARTandCPmodels;weightmatricesfromthe12thlayer.
representationalsimilarity. TwoCPmodels(m2m basedmultilingualmachinetranslationmodelswith
andm2o)alsolearnmoredistinctrepresentations themBARTlanguagemodel. Wecomputethesin-
forGermaninthe10thand11thlayers. gularvaluesoftheweightmatricesforkey,query,
We now turn to the representational similarity value, and output projections in the transformer
betweenlanguagepairs. Figure2showstherep- multi-head attention sub-layers and both up and
resentational similarity between language pairs down projection weight matrices of the fully con-
learnedbyMTmodelsandLMs. Theresultssug- nected (FC) layers. After decomposition, we cal-
gestthatLMslearnmorelanguage-agnosticrepre- culate the norm of the vector σ = (σ ,...,σ ) of
1 r
sentationsthanMTmodels singularvalues,whichwedenoteas∥σ∥,aswell
Inall,wecannotestablishastrongconnectionbe- asthedifferenceofsingularvaluesinmBARTand
tweenrepresentationalsimilarityanddownstream theCPmodels(denotedasd). Table2reportsthe
performance. Instead,weseeatrendthatCPmod- valuesofthe12thlayer.3
elsmaintaincomparativelysimilarrepresentations CPmodelshavelargersingularvaluesthanthe
tothemodeltheyarederivedfromanddissimilar mBART they are derived from, therefore having
representationstootherLMs. a stronger scaling effect geometrically. Also, re-
markthattranslationdirectioninCPimpactssingu-
larvaluedifferences: o2mlaysnoticeablyfurther
3.3. CP’s effect on scaling
awayfrommBARTthanm2mandm2o. Inall,this
We have established in the previous Section 3.2
suggeststhatmodelstrainedontheMTobjective
that variations in the representation space are
learntospreadtheiroutputsonlargeroutputvector
mostlytiedtothesequenceofupdatesperformed
spaces. Wehypothesizethatthisbehaviorishelp-
on some architecture. Explaining why MT objec-
fulforMTasitentailsthatoutputsaremoreeasily
tivesfailtoenhanceperformancesoncross-lingual
separable (as noted by, e.g., Chen et al., 2023);
tasks, therefore, requires that we study the ac-
butitmightalsohinderdownstreamperformances
tualcomputationsbeingperformedbyCPmodels.
bymakingthemodelhardertoadapttoothertasks
Hence, weinvestigatetheweightmatricesofthe
wheresuchbehaviorisunnecessaryordetrimental.
mBARTLMandCPmodels. Ourintuitionisasfol-
lows: Weightmatricesarelinearmaps,andwecan
4. Related works
make some sense of the specific characteristics
of these maps. More precisely, we focus on the
PretrainedlanguagemodelssuchasBERTshow
magnitudeoftheeigenvalues: Higherabsoluteval-
surprisingperformanceincross-lingualtasks(Wu
uesofeigenvaluesshouldentailnumericallylarger
and Dredze, 2019), a domain that is intensively
componentvaluesintheoutputvectors. Intuitively,
studiedandexhibitsvariousapplications(Pikuliak
thisshouldimpacthowseparabletheoutputvec-
etal.,2021). Huangetal.(2019)furtherenhanced
torsareafterapplyingtheweightmatrixtransforma-
LM cross-lingual performances via universal lan-
tion,whichshouldbeencodedinthecorresponding
guageencoding. Eriguchietal.(2018)conducted
eigenvalues. 2 Inpractice,weapplysingularvalue
an early study on using the encoder of multilin-
decomposition (SVD) to retrieve singular values
gualMTmodelsforthreecross-lingualclassifica-
instead: weight matrices W can be rewritten as
tion tasks in high-resource languages. Similarly,
W=Σr σ u v⊤,whereu andv formtwosets
i=1 i i i i i Chenetal.(2021)utilizedpretrainedmultilingual
oforthogonalvectors,whicharecombinedthrough
MTencodersandtheembeddinglayersofXLM-R
thescalingfactorsσ ,knownassingularvalues.
i toproposeatwo-stagetrainingscheme,yielding
We analyze the scaling effect with or without
improvedperformanceonzero-shotcross-lingual
MTascontinuedpretrainingbycomparingmBART-
machinetranslation. Kaleetal.(2021)investigate
usingparalleldataforpretraininglanguagemodels
2This last expectation of separability might not be
tosolvemultilingualNLPtasks. Ourstudydiffers
borneoutiftheinputsareproportionallylessseparable
fromthisworkinthefollowingthreeaspects. First,
inmodelswithlargereigenvalues. InTransformers,this
ultimatelydependsontheeigenvaluesoftheembedding
weightsandthe(non-linear)computationsperformedin 3Results for all other layers are available in Ap-
earlierlayers. Weleavethisaspectforfuturework. pendixB.2,Table5.de 0.20la 0y .e 1r 40 0.090.19 de 0.23la 0y .e 1r 71 0.100.24 de 0.21la 0y .e 1r 82 0.120.22 de 0.19la 0y .e 1r 73 0.110.20 de 0.16la 0y .e 1r 54 0.090.20 de 0.15la 0y .e 1r 45 0.080.20 de 0.14la 0y .e 1r 46 0.080.19 de 0.13la 0y .e 1r 47 0.080.19 de 0.13la 0y .e 1r 58 0.100.20 de 0.14la 0y .e 1r 79 0.120.22 de 0.14lay 0e .1r 610 0.110.19 de 0.15lay 0e .1r 711 0.100.18 de 0.09lay 0e .1r 412 0.110.14 1.0
en0.20 0.180.130.29 en0.22 0.240.160.38 en0.15 0.250.170.31 en0.13 0.240.150.29 en0.14 0.230.130.29 en0.13 0.220.120.29 en0.13 0.200.120.26 en0.13 0.190.120.25 en0.13 0.180.120.23 en0.13 0.180.130.22 en0.17 0.180.100.17 en0.12 0.170.100.16 en0.06 0.120.110.10 0.8
es0.150.18 0.110.22 es0.170.21 0.120.28 es0.130.17 0.150.28 es0.130.16 0.140.27 es0.130.17 0.120.26 es0.130.16 0.100.24 es0.130.16 0.100.22 es0.130.15 0.100.22 es0.130.16 0.120.23 es0.140.16 0.140.25 es0.180.17 0.130.22 es0.160.13 0.120.19 es0.100.07 0.140.15 0.6
fr0.100.140.12 0.21 fr0.120.160.13 0.23 fr0.080.110.10 0.25 fr0.080.100.10 0.22 fr0.080.110.10 0.19 fr0.080.110.10 0.17 fr0.080.110.10 0.16 fr0.070.100.09 0.17 fr0.070.100.09 0.18 fr0.080.110.10 0.20 fr0.130.140.15 0.17 fr0.120.120.13 0.17 fr0.090.070.12 0.14 0.4
ru0.190.290.220.21 ru0.210.290.240.23 ru0.170.220.220.19 ru0.160.190.200.18 ru0.160.210.210.17 ru0.160.220.210.17 ru0.170.230.210.16 ru0.170.230.210.16 ru0.180.240.210.16 ru0.180.230.220.17 ru0.200.190.210.20 ru0.170.140.190.19 ru0.090.080.110.10 0.2
de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru
(a)mBART(lowertriangle)andmBARTm2m(uppertriangle)
de 0.19la 0y .e 1r 40 0.090.19 de 0.21la 0y .e 1r 51 0.100.20 de 0.18la 0y .e 1r 52 0.100.20 de 0.16la 0y .e 1r 43 0.100.20 de 0.14la 0y .e 1r 44 0.080.20 de 0.13la 0y .e 1r 35 0.080.19 de 0.12la 0y .e 1r 26 0.070.17 de 0.10la 0y .e 1r 27 0.070.15 de 0.08la 0y .e 1r 18 0.080.14 de 0.09la 0y .e 1r 29 0.080.15 de 0.08lay 0e .1r 210 0.090.14 de 0.09lay 0e .1r 511 0.090.15 de 0.08lay 0e .1r 412 0.090.12 1.0
en0.21 0.170.120.29 en0.22 0.210.150.31 en0.19 0.210.140.30 en0.17 0.200.140.28 en0.15 0.200.130.28 en0.13 0.190.120.27 en0.11 0.170.110.22 en0.10 0.140.090.18 en0.10 0.120.090.15 en0.11 0.120.090.15 en0.12 0.110.090.14 en0.14 0.120.090.13 en0.10 0.100.090.09 0.8
es0.130.19 0.100.22 es0.160.22 0.110.24 es0.160.22 0.120.25 es0.150.20 0.120.25 es0.140.19 0.110.24 es0.110.18 0.100.23 es0.110.15 0.090.19 es0.110.14 0.090.18 es0.120.14 0.100.17 es0.140.14 0.100.18 es0.140.13 0.110.18 es0.150.15 0.120.19 es0.160.11 0.120.16 0.6
fr0.080.130.10 0.21 fr0.090.160.11 0.23 fr0.100.150.13 0.24 fr0.090.130.12 0.24 fr0.080.120.10 0.21 fr0.070.100.08 0.19 fr0.070.090.07 0.16 fr0.070.080.08 0.15 fr0.090.090.10 0.14 fr0.110.100.12 0.15 fr0.090.090.10 0.16 fr0.120.120.13 0.16 fr0.120.090.15 0.14 0.4
ru0.180.310.210.20 ru0.210.340.240.22 ru0.190.290.250.21 ru0.180.260.230.20 ru0.170.240.220.17 ru0.160.230.200.15 ru0.150.190.180.14 ru0.150.170.180.14 ru0.170.160.190.15 ru0.180.170.210.17 ru0.150.130.160.19 ru0.170.160.180.18 ru0.170.100.200.16 0.2
de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru
(b)mBARTm2o(lowertriangle)andmBARTo2m(uppertriangle)
d ene 0.170.20la 0 0y . .e 1 1r 5 90 0 0. .1 11 50 0. .1 27
6
d ene 0.170.22la 0 0y . .e 1 2r 4 31 0 0. .0 19 50 0. .2 32
8
d ene 0.160.20la 0 0y . .e 1 2r 4 02 0 0. .1 10 30 0. .2 31
0
d ene 0.170.21la 0 0y . .e 1 2r 6 23 0 0. .1 10 40 0. .2 21
9
d ene 0.180.23la 0 0y . .e 1 2r 8 44 0 0. .1 13 70 0. .2 23
9
d ene 0.160.19la 0 0y . .e 1 2r 4 05 0 0. .0 19 40 0. .2 20
8
d ene 0.150.17la 0 0y . .e 1 1r 2 86 0 0. .0 18 30 0. .1 29
5
d ene 0.160.16la 0 0y . .e 1 1r 2 77 0 0. .0 18 30 0. .1 28
4
d ene 0.170.16la 0 0y . .e 1 1r 1 78 0 0. .0 17 30 0. .1 28
5
d ene 0.160.15la 0 0y . .e 1 1r 1 69 0 0. .0 18 30 0. .2 20
4
d ene 0.140.12lay 0 0e . .1 1r 1 210 0 0. .0 17 00 0. .1 17
7
d ene 0.120.09lay 0 0e . .1 1r 2 011 0 0. .1 10 00 0. .1 14
2
d ene 0.090.07lay 0 0e . .0 0r 9 912 0 0. .1 10 00 0. .1 01
8
01 .. 80
es0.140.18 0.130.19 es0.130.21 0.100.23 es0.120.21 0.100.23 es0.130.21 0.120.24 es0.140.23 0.140.26 es0.160.21 0.100.21 es0.160.20 0.080.19 es0.170.21 0.070.18 es0.200.23 0.070.17 es0.200.23 0.080.17 es0.180.19 0.080.15 es0.170.14 0.110.15 es0.140.10 0.130.13 0.6
fr0.080.130.10 0.20 fr0.070.140.09 0.21 fr0.070.140.09 0.20 fr0.080.140.09 0.20 fr0.090.150.10 0.22 fr0.100.140.12 0.17 fr0.110.140.12 0.15 fr0.120.150.14 0.14 fr0.150.180.18 0.14 fr0.150.160.17 0.14 fr0.140.140.15 0.12 fr0.160.140.17 0.12 fr0.130.110.15 0.12 0.4
ru0.230.370.280.20 ru0.200.380.240.18 ru0.200.370.230.17 ru0.210.340.230.17 ru0.210.340.240.18 ru0.200.260.240.18 ru0.190.230.220.17 ru0.200.260.230.19 ru0.220.250.260.21 ru0.210.230.240.19 ru0.190.190.220.17 ru0.170.140.200.18 ru0.120.090.150.12 0.2
de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru de en es fr ru
(c)mBERT(lowertriangle)andXLM-R(uppertriangle)
Figure2: RepresentationalsimilaritybetweendifferentlanguageswithrepresentationslearnedbyLMs
andMTmodels
objective and hypothesis. While both studies in- thatsucceedoncross-lingualtransfertasks: What
volveincorporatingparalleldataintopre-training, isusefulforMTmaybedetrimentalinothercross-
thestartinghypothesesandobjectivesdiffer. Kale lingualdownstreamapplications. Ourobjectivewas
etal.(2021)exploredthegeneralbenefitsofpre- toshedlightonpotentialpitfallsorchallengesas-
trainingwithparalleldata,whereaswespecifically sociatedwithadditionaltranslationtasklearningin
investigatetheimpactofanMTobjectiveoncross- multilingual language models, rather than strictly
lingualtransfer. Second,methodology. Ourstudy aimingforperformanceimprovement. Theidenti-
introduces the concept of continued pre-training fiedrelationshipbetweenchangesindistributional
(CP)asasequentialtrainingprocessspecifically representationsandperformancedegradationisa
focused on the MT objective. In contrast, Kale valuableinsightthatcontributestoourunderstand-
et al. (2021) performed multi-tasking during pre- ingofmodelbehaviorinmultilingualscenarios.
trainingwithvariousobjectives,includingmachine Infuturework,weintendtopursuetwodistinct
translation. Third, modelconfigurations. Weuse directions: (i)establishingaprincipledcomparison
mBARTmodelswithdifferenttranslationsettings insteadofrelyingonpubliclyavailablepretrained
forCP,whileKaleetal.(2021)focusedonmT5as models to more accurately control for parameter
amassivelymultilingualmodel. Morebroadly,pre- count, architecture design, and training data; (ii)
viousstudieshaveleveragedpretrainedencoder- studyingmoreformallytowhatextentseparability
decoderLMstobuildeffectiveMTmodels(Liuetal., inMTisattestedanddistinctfromwhatweobserve
2020;Tangetal.,2020),whichsuggeststhatMT inLMs.
and LM are not entirely unrelated tasks—though
theevidenceisconflicting(Vázquezetal.,2021).
Ethical Consideration and Limitations
Webelievethisworktocomplywithallethicalstan-
5. Conclusion
dards.
The present study was not conducted in rigor-
This paper reports empirical studies on cross-
ouslycomparablesettings,suchasensuringthat
lingualtransferlearningusingexistingpretrained
modelsareexposedtothesamepretrainingdata.
multilinguallanguageandmachinetranslationmod-
Thislimitsourcapacitytoensurefaircomparisons:
els. Wehaveinvestigatedwhethermachinetrans-
lationascontinuedpretrainingcanbridgemultilin-
• Intheempiricalcomparisonbetweenlanguage
gualpretrainingandcross-lingualtransferlearning.
modelsandcontinued-trainedmachinetrans-
OurempiricalresultsinSection3.1showedthatCP
lation models, the training corpora of those
withtheMTobjectivefailedtoimprovecross-lingual
modelsvary. Especially,mBARThasnotseen
performance. Further analyses of the language
some languages in the downstream bench-
representationslearnedbydifferentmodelsinSec-
marks.
tion3.2andoftheirweightmatricesinSection3.3
showedthatmodelsre-trainedontheMTobjective • mBARTseriesmodelshave10%parameters
displaylargerscalingfactorsthanthecheckpoint thanBERTandXLM-R,makingthecompari-
theywerederivedfrom,suggestingthatmachine son in Table 1 unfair. Nevertheless, mBART
translationfostersoutputseparability. Simplyput, encoders did not benefit from the increased
modelstrainedonMTobjectivesneednothaverep- number of parameters and failed to achieve
resentationsthatmatchthoseofmultilingualLMs betterperformanceincross-lingualtasks.Acknowledgments MartaRCosta-jussà,JamesCross,OnurÇelebi,
MahaElbayad,KennethHeafield,KevinHeffer-
ThisworkispartoftheFoTranproject,fundedby nan,ElaheKalbassi,JaniceLam,DanielLicht,
theEuropeanResearchCouncil(ERC)underthe JeanMaillard,etal.2022. Nolanguageleftbe-
EU’s Horizon 2020 research and innovation pro- hind: Scalinghuman-centeredmachinetransla-
gram (agreement № 771113) and has received tion. arXivpreprintarXiv:2207.04672.
funding from the European Union’s Horizon Eu-
JacobDevlin,Ming-WeiChang,KentonLee,and
rope research and innovation programme under
KristinaToutanova.2019. BERT:Pre-trainingof
GrantagreementNo101070350andfromUKRe-
Deep Bidirectional Transformers for Language
searchandInnovation(UKRI)undertheUKgovern-
ment’s Horizon Europe funding guarantee [grant
Understanding. InProceedingsofthe2019Con-
number10052546]. Theauthorswishtoacknowl- ference of the North American Chapter of the
edge CSC – IT Center for Science, Finland, for Association for Computational Linguistics: Hu-
generouscomputationalresources,includingPuhti,
manLanguageTechnologies.
Mahti,andLUMIextremescaleaccess(MOOMIN
Akiko Eriguchi, Melvin Johnson, Orhan Firat,
andLumiNMT).
HidetoKazawa,andWolfgangMacherey.2018.
Zero-shotcross-lingualclassificationusingmulti-
lingualneuralmachinetranslation.
6. Bibliographical References
Haoyang Huang, Yaobo Liang, Nan Duan, Ming
Gong,LinjunShou,DaxinJiang,andMingZhou.
2019. Unicoder: Auniversallanguageencoder
Steven Cao, Nikita Kitaev, and Dan Klein. 2020. bypre-trainingwithmultiplecross-lingualtasks.
Multilingualalignmentofcontextualwordrepre- InProceedingsofthe2019ConferenceonEm-
sentations.InInternationalConferenceonLearn-
piricalMethodsinNaturalLanguageProcessing
ingRepresentations.
and the 9th International Joint Conference on
NaturalLanguageProcessing(EMNLP-IJCNLP),
GuanhuaChen,ShumingMa,YunChen,LiDong, pages2485–2494,HongKong,China.Associa-
DongdongZhang,JiaPan,WenpingWang,and tionforComputationalLinguistics.
Furu Wei. 2021. Zero-shot cross-lingual trans-
fer of neural machine translation with multilin- Mihir Kale, Aditya Siddhant, Rami Al-Rfou, Lint-
gualpretrainedencoders. InProceedingsofthe ing Xue, Noah Constant, and Melvin Johnson.
2021. nmT5-isparalleldatastillrelevantforpre-
2021ConferenceonEmpiricalMethodsinNatu-
ralLanguageProcessing,pages15–26,Online trainingmassivelymultilinguallanguagemodels?
andPuntaCana,DominicanRepublic.Associa- InProceedingsofthe59thAnnualMeetingofthe
tionforComputationalLinguistics. Association for Computational Linguistics and
the11thInternationalJointConferenceonNatu-
LiangChen,ShumingMa,DongdongZhang,Furu ralLanguageProcessing(Volume2: ShortPa-
Wei,andBaobaoChang.2023. Ontheoff-target pers), pages 683–691, Online. Association for
problemofzero-shotmultilingualneuralmachine ComputationalLinguistics.
translation. In Findings of the Association for
Simon Kornblith, Mohammad Norouzi, Honglak
Computational Linguistics: ACL 2023, pages
Lee,andGeoffreyHinton.2019.Similarityofneu-
9542–9558, Toronto, Canada. Association for
ComputationalLinguistics.
ralnetworkrepresentationsrevisited. InInterna-
tionalConferenceonMachineLearning,pages
3519–3529.PMLR.
Alexis Conneau, Kartikay Khandelwal, Naman
Goyal,VishravChaudhary,GuillaumeWenzek,
Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu,
Francisco Guzmán, Édouard Grave, Myle Ott,
Fenfei Guo, Weizhen Qi, Ming Gong, Linjun
LukeZettlemoyer,andVeselinStoyanov.2020.
Shou,DaxinJiang,GuihongCao,XiaodongFan,
Unsupervisedcross-lingualrepresentationlearn-
RuofeiZhang,RahulAgrawal,EdwardCui,Sin-
ingatscale. InProceedingsofthe58thAnnual
ing Wei, Taroon Bharti, Ying Qiao, Jiun-Hung
MeetingoftheAssociationforComputationalLin- Chen, Winnie Wu, Shuguang Liu, Fan Yang,
guistics,pages8440–8451.
Daniel Campos, Rangan Majumder, and Ming
Zhou.2020. XGLUE:Anewbenchmarkdataset
Alexis Conneau, Guillaume Lample, Ruty Rinott,
forcross-lingualpre-training,understandingand
Adina Williams, Samuel R. Bowman, Holger
generation. arXiv,abs/2004.01401.
Schwenk, and Veselin Stoyanov. 2018. XNLI:
EvaluatingCross-lingualSentenceRepresenta- Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li,
tions. InEMNLP. Sergey Edunov, Marjan Ghazvininejad, MikeLewis,andLukeZettlemoyer.2020. Multilingual pages337–347,Online.AssociationforCompu-
denoisingpre-trainingforneuralmachinetrans- tationalLinguistics.
lation. TransactionsoftheAssociationforCom-
Yuxuan Wang, Wanxiang Che, Jiang Guo, Yijia
putationalLinguistics,8:726–742.
Liu,andTingLiu.2019. Cross-lingualberttrans-
IlyaLoshchilovandFrankHutter.2019. Decoupled formationforzero-shotdependencyparsing. In
weightdecayregularization. Proceedingsofthe2019ConferenceonEmpiri-
calMethodsinNaturalLanguageProcessingand
Yichao Lu, Phillip Keung, Faisal Ladhak, Vikas
the9thInternationalJointConferenceonNatural
Bhardwaj, Shaonan Zhang, and Jason Sun. LanguageProcessing(EMNLP-IJCNLP),pages
2018. A neural interlingua for multilingual ma- 5721–5727.
chine translation. In Proceedings of the Third
ShijieWuandMarkDredze.2019. Beto,bentz,be-
ConferenceonMachineTranslation: Research
Papers,pages84–92,Brussels,Belgium.Asso- cas: Thesurprisingcross-lingualeffectivenessof
ciationforComputationalLinguistics. bert. InProceedingsofthe2019Conferenceon
EmpiricalMethodsinNaturalLanguageProcess-
Margaret Masterman. 1961. Semantic message
ingandthe9thInternationalJointConferenceon
detection for machine translation, using an in- NaturalLanguageProcessing(EMNLP-IJCNLP),
terlingua. In Proceedings of the International pages833–844.
ConferenceonMachineTranslationandApplied
Language Analysis, National Physical Labora- ShijieWuandMarkDredze.2020. Doexplicitalign-
tory,Teddington,UK. mentsrobustlyimprovemultilingualencoders?
InProceedingsofthe2020ConferenceonEm-
MatúšPikuliak,MariánŠimko,andMáriaBieliková. piricalMethodsinNaturalLanguageProcessing
2021. Cross-lingual learning for text process- (EMNLP),pages4471–4482.
ing: Asurvey. ExpertSystemswithApplications,
Linting Xue, Noah Constant, Adam Roberts, Mi-
165:113765.
hirKale,RamiAl-Rfou,AdityaSiddhant,Aditya
ErikF.TjongKimSang.2002. Introductiontothe Barua, and Colin Raffel. 2021. mT5: A mas-
conll-2002sharedtask: Language-independent sivelymultilingualpre-trainedtext-to-texttrans-
namedentityrecognition. ArXiv,cs.CL/0209010. former. InProceedingsofthe2021Conference
oftheNorthAmericanChapteroftheAssociation
Erik F. Tjong Kim Sang and Fien De Meulder.
forComputationalLinguistics: HumanLanguage
2003. Introductiontotheconll-2003sharedtask: Technologies,pages483–498,Online.Associa-
Language-independentnamedentityrecognition. tionforComputationalLinguistics.
ArXiv,cs.CL/0306050.
Yinfei Yang, Yuan Zhang, Chris Tar, and Jason
YuqingTang,ChauTran,XianLi,Peng-JenChen, Baldridge.2019. Paws-x: Across-lingualadver-
Naman Goyal, Vishrav Chaudhary, Jiatao Gu, sarialdatasetforparaphraseidentification.ArXiv,
andAngela Fan.2020. Multilingual translation abs/1908.11828.
withextensiblemultilingualpretrainingandfine-
tuning. arXivpreprintarXiv:2008.00401. Daniel Zeman, Joakim Nivre, Mitchell Abrams,
Elia Ackermann, Noëmi Aepli, Hamid Aghaei,
JörgTiedemann.2018.Emerginglanguagespaces RZiane,andetal.2020. Universaldependen-
learnedfrommassivelymultilingualcorpora. In cies2.5.
Proceedings of the Digital Humanities in the
Nordic Countries 3rd Conference (DHN 2018),
volume2084ofCEURWorkshopProceedings,
pages188–197,Unknown.CEURWorkshopPro-
ceedings. DigitalhumanitiesintheNordicCoun-
tries DHN2018, DHN2018 ; Conference date:
07-03-2018Through09-03-2018.
RaúlVázquez,HandeCelikkanat,MathiasCreutz,
andJörgTiedemann.2021. Onthedifferences
betweenBERTandMTencoderspacesandhow
toaddressthemintranslationtasks. InProceed-
ingsofthe59thAnnualMeetingoftheAssocia-
tionforComputationalLinguisticsandthe11th
International Joint Conference on Natural Lan-
guageProcessing: StudentResearchWorkshop,A. Benchmarks for downstream
evaluation
Asummaryofbenchmarksfordownstreamevalua-
tionisshowninTable3.
B. Supplementary results
B.1. Per-language results of
cross-lingual evaluation
Table 4 shows the overall performance of cross-
lingualevaluationusingLMsandMTmodels. Note
thatmodelsarefine-tunedonlyinEnglishandeval-
uatedinotherlanguages;moreoverbenchmarks
differ in which languages they include. As a con-
sequence,somescoresarenotavailableforsome
languages.
B.2. The scaling effect of MT as
continued training in different layers
Table5showstheresultsofthefirst11layersinthe
encodersofmBARTseriesmodels. Similartothe
analysis in Section 3.3, we calculate the norm of
thevectorizeddiagonalmatricesofsingularvalues
diag(Σ) and their pairwise distance to the corre-
sponding vectors derived from the same weight
matricesinThetransformerattentionmoduleand
fullyconnectedlayersinthebasemBARTmodel.
The results indicate the same conclusion drawn
fromtheanalysisonthe12thlayer.Task #ofLanguages |Train|en |Dev|avg |Test|avg Metric DataSource
NER 4 15.0K 2.8K 3.4K F1 ECIMultilingualTextCorpus
POS 18 25.4K 1.0K 0.9K ACC UDTree-banks(v2.5)
NC 5 100K 10K 10K ACC CommercialNewsWebsite
XNLI 15 433K 2.5K 5K ACC MultiNLICorpus
PAWS-X 4 49.4K 2K 2K ACC Wikipedia
QADSM 3 100K 10K 10K ACC CommercialSearchEngine
WPR 7 100K 10K 10K nDCG CommercialSearchEngine
QAM 3 100K 10K 10K ACC CommercialSearchEngine
Table3: Asummaryofbenchmarksfordownstreamevaluation. Wechoose8downstreamtasksfrom
XGLUE(Liangetal.,2020)forcross-lingualevaluationandxtasksformonolingualevaluation. Thetraining
set of each task is only available in English, with |Train|en denoting the number of labeled instances.
|Dev|avg and|Test|avg denotetheaveragenumbersoflabeledinstancesinthedevsetsandtestsets,
respectively.
Task Model AR BG DE EL EN ES FR HI IT NL PL PT RU SW TH TR UR VI ZH AVG
mBERT - - 81.09 - 91.98 80.73 75.84 - - - - - 76.96 - - - - - - 81.32
XLM-R - - 81.79 - 91.97 82.14 76.10 - - - - - 78.63 - - - - - - 82.13
mBART - - 82.82 - 91.90 81.37 75.95 - - - - - 78.43 - - - - - - 82.09
NC mBARTm2o - - 80.32 - 91.46 78.95 74.11 - - - - - 76.96 - - - - - - 80.36
mBARTo2m - - 63.63 - 91.39 69.12 65.40 - - - - - 37.60 - - - - - - 65.43
mBARTm2m - - 77.44 - 91.78 75.02 71.99 - - - - - 75.19 - - - - - - 78.28
NLLB600M - - 67.52 - 91.79 76.43 71.79 - - - - - 72.41 - - - - - - 75.99
mBERT 62.49 66.79 71.04 65.34 82.29 74.06 73.94 58.92 - - - - 65.62 51.24 51.16 61.41 56.27 68.31 68.63 65.17
XLM-R 70.68 76.27 76.06 74.70 84.86 79.60 78.31 68.15 - - - - 74.18 63.78 71.29 71.89 65.34 74.10 73.21 73.49
mBART 69.40 56.95 75.14 34.82 84.22 78.59 75.82 66.14 - - - - 73.86 58.03 67.31 68.80 62.25 71.65 71.49 67.63
XNLI mBARTm2o 68.96 58.76 74.90 36.87 82.13 76.59 75.62 65.78 - - - - 73.09 42.33 59.72 68.39 60.80 71.16 73.01 65.87
mBARTo2m 42.01 43.49 49.28 34.86 81.85 53.78 55.54 40.00 - - - - 54.70 39.80 39.84 41.57 35.86 56.02 52.45 48.07
mBARTm2m 58.39 50.72 68.71 35.34 83.90 66.31 73.78 65.02 - - - - 57.51 40.84 49.52 60.32 58.71 70.88 62.53 60.17
NLLB600M 68.47 69.88 63.45 72.53 81.12 72.29 72.73 68.76 - - - - 72.37 58.35 64.94 59.28 64.30 68.76 67.63 68.32
mBERT - - 82.20 - 92.85 84.60 86.70 - - - - - - - - - - - - 86.59
XLM-R - - 85.75 - 93.40 88.30 87.95 - - - - - - - - - - - - 88.85
mBART - - 86.70 - 93.65 88.30 88.30 - - - - - - - - - - - - 89.24
PAWS-X mBARTm2o - - 81.80 - 91.00 84.50 85.20 - - - - - - - - - - - - 85.63
mBARTo2m - - 76.35 - 89.90 78.90 81.45 - - - - - - - - - - - - 81.65
mBARTm2m - - 83.95 - 92.20 85.75 86.80 - - - - - - - - - - - - 87.18
NLLB600M - - 67.40 - 82.35 71.65 72.00 - - - - - - - - - - - - 73.35
mBERT - - 62.08 - 69.47 - 62.35 - - - - - - - - - - - - 64.63
XLM-R - - 66.98 - 69.69 - 65.45 - - - - - - - - - - - - 67.37
mBART - - 66.36 - 70.46 - 66.71 - - - - - - - - - - - - 67.84
QAM mBARTm2o - - 64.20 - 65.10 - 62.39 - - - - - - - - - - - - 63.90
mBARTo2m - - 55.27 - 65.41 - 54.60 - - - - - - - - - - - - 58.43
mBARTm2m - - 62.62 - 66.21 - 60.72 - - - - - - - - - - - - 63.18
NLLB600M - - 57.90 - 66.47 - 60.14 - - - - - - - - - - - - 61.50
mBERT - - 59.94 - 67.04 - 62.30 - - - - - - - - - - - - 63.09
XLM-R - - 63.19 - 71.44 - 66.02 - - - - - - - - - - - - 66.88
mBART - - 61.83 - 69.83 - 64.79 - - - - - - - - - - - - 65.48
QADSM mBARTm2o - - 63.15 - 64.07 - 64.34 - - - - - - - - - - - - 63.85
mBARTo2m - - 63.40 - 65.17 - 59.61 - - - - - - - - - - - - 62.73
mBARTm2m - - 60.89 - 65.45 - 62.05 - - - - - - - - - - - - 62.80
NLLB600M - - 64.48 - 64.45 - 62.71 - - - - - - - - - - - - 63.88
mBERT - - 76.64 - 77.29 75.07 73.92 - 66.58 - - 77.04 - - - - - - 62.67 74.42
XLM-R - - 77.08 - 77.79 76.14 74.94 - 67.87 - - 77.93 - - - - - - 62.81 75.29
mBART - - 76.74 - 77.18 75.41 74.22 - 67.40 - - 77.38 - - - - - - 62.86 74.72
WPR mBARTm2o - - 75.60 - 76.17 74.08 73.31 - 66.21 - - 76.59 - - - - - - 62.38 73.66
mBARTo2m - - 75.32 - 75.99 74.07 72.76 - 65.39 - - 75.80 - - - - - - 61.24 73.22
mBARTm2m - - 76.22 - 76.22 74.28 73.23 - 66.35 - - 75.79 - - - - - - 61.93 73.68
NLLB600M - - 76.01 - 76.35 73.81 73.48 - 65.84 - - 76.46 - - - - - - 62.02 73.66
mBERT - - 68.84 - 90.78 73.27 - - - 77.28 - - - - - - - - - 77.54
XLM-R - - 69.99 - 90.45 75.77 - - - 78.62 - - - - - - - - - 78.71
mBART - - 71.31 - 91.35 72.55 - - - 75.57 - - - - - - - - - 77.70
NER mBARTm2o - - 52.41 - 89.61 50.71 - - - 53.36 - - - - - - - - - 61.52
mBARTo2m - - 25.66 - 89.22 53.31 - - - 52.13 - - - - - - - - - 55.08
mBARTm2m - - 65.25 - 88.99 66.86 - - - 66.58 - - - - - - - - - 71.92
< NLLB600M - - 29.4 - 89.46 43.21 - - - 54.83 - - - - - - - - - 54.23
mBERT 57.26 85.84 90.21 82.61 95.84 87.67 85.80 66.57 91.78 87.78 80.93 88.93 80.57 - 41.88 68.87 60.12 55.09 60.19 76.00
XLM-R 69.44 88.70 91.75 87.63 96.43 88.20 89.22 72.10 91.35 88.46 83.82 90.07 87.12 - 58.08 72.76 64.28 57.06 58.45 79.72
mBART 63.55 71.76 90.56 29.74 96.13 87.07 87.75 67.61 90.64 87.51 80.60 88.29 83.35 - 55.56 66.53 55.61 54.62 51.56 72.69
POS mBARTm2o 63.97 71.30 90.64 24.82 95.74 84.98 85.19 64.32 87.45 86.18 80.12 82.91 81.91 - 51.13 66.77 50.65 52.29 53.18 70.75
mBARTo2m 53.78 58.78 61.97 41.62 95.63 63.08 70.43 48.16 59.92 60.01 53.62 58.11 61.60 - 37.12 46.21 42.90 44.95 44.08 55.67
mBARTm2m 64.60 71.58 90.35 21.66 96.06 81.21 86.20 65.94 83.71 85.30 81.28 81.65 84.81 - 41.83 63.55 52.44 51.02 51.33 69.70
NLLB600M 63.76 84.47 77.64 79.22 96.12 82.61 83.36 66.59 84.93 75.9 74.57 80.95 80.92 - 46.56 56.46 58.59 45.63 46.32 71.37
Table 4: The overall performance of cross-lingual natural language understanding. We use the base
architectureformBERTandXLM-R.mBARTmodelsonlyutilizethe12-layerencoders. ‘m2o’means
many-to-one. ‘o2m’meansone-to-many. ‘m2m’meansmany-to-many. ‘-’denotesthatthebenchmark
doesnotcoverthecorrespondinglanguage.K Q V Out FC1 FC2
Layer Model
∥σ∥ d ∥σ∥ d ∥σ∥ d ∥σ∥ d ∥σ∥ d ∥σ∥ d
mBART 39.25 0.00 41.16 0.00 32.82 0.00 30.30 0.00 88.19 0.00 66.57 0.00
mBARTm2m 48.46 9.55 48.83 7.71 31.01 2.14 29.29 3.10 93.43 6.64 73.70 8.30
0
mBARTm2o 48.49 9.59 48.95 7.90 30.51 2.44 28.99 3.55 93.94 7.12 74.39 9.03
mBARTo2m 49.15 10.62 50.54 10.50 35.90 3.87 35.75 6.73 111.30 23.86 94.52 28.87
mBART 44.41 0.00 46.89 0.00 29.21 0.00 31.80 0.00 90.87 0.00 69.86 0.00
mBARTm2m 51.21 7.58 52.62 5.79 27.69 1.73 30.56 2.41 96.44 6.34 76.99 8.09
1
mBARTm2o 51.64 8.07 53.10 6.36 27.49 1.78 30.47 2.63 97.06 7.31 77.83 9.07
mBARTo2m 54.00 9.77 56.38 9.70 35.17 6.40 37.90 6.64 113.14 23.36 96.06 26.69
mBART 46.56 0.00 47.80 0.00 31.80 0.00 32.22 0.00 93.76 0.00 71.51 0.00
mBARTm2m 51.83 5.86 52.66 5.04 31.34 1.32 31.96 2.48 99.62 6.49 78.89 8.19
2
mBARTm2o 52.40 6.53 53.22 5.66 31.26 1.09 31.94 2.53 100.24 7.50 79.71 9.18
mBARTo2m 56.22 9.75 57.47 9.91 38.20 6.91 39.20 7.45 114.89 21.77 96.37 25.03
mBART 48.09 0.00 48.65 0.00 37.77 0.00 36.85 0.00 96.27 0.00 73.45 0.00
mBARTm2m 52.57 4.54 53.05 4.63 39.06 1.65 38.06 2.42 101.80 6.09 80.15 7.27
3
mBARTm2o 53.15 5.15 53.62 5.21 39.15 1.59 38.12 2.40 102.31 7.18 80.84 8.12
mBARTo2m 56.82 8.80 57.69 9.28 45.90 8.25 45.43 9.00 117.17 21.21 97.57 24.24
mBART 51.59 0.00 51.06 0.00 40.80 0.00 38.91 0.00 99.22 0.00 77.23 0.00
mBARTm2m 56.45 5.05 55.97 5.08 43.02 2.51 41.20 2.71 104.97 6.76 84.00 7.36
4
mBARTm2o 57.10 5.70 56.62 5.72 43.00 2.31 41.17 2.50 105.74 8.23 85.01 8.72
mBARTo2m 61.88 10.40 61.58 10.61 49.68 8.98 48.54 9.82 121.06 21.94 101.56 24.42
mBART 59.93 0.00 58.27 0.00 42.09 0.00 38.29 0.00 101.79 0.00 84.25 0.00
mBARTm2m 65.19 5.44 63.76 5.69 44.03 2.11 40.23 2.61 107.87 7.42 91.05 7.56
5
mBARTm2o 65.86 6.14 64.44 6.33 44.22 2.21 40.42 2.63 108.94 8.86 92.49 9.56
mBARTo2m 70.59 10.94 69.24 11.26 51.39 9.36 48.64 10.70 124.78 23.18 108.30 24.15
mBART 57.55 0.00 55.80 0.00 45.27 0.00 40.54 0.00 101.52 0.00 89.89 0.00
mBARTm2m 61.71 4.60 60.09 4.47 48.11 2.94 43.59 3.34 108.45 7.71 97.85 8.11
6
mBARTm2o 62.22 5.12 60.62 4.93 48.44 3.24 43.95 3.63 109.45 8.79 99.23 9.60
mBARTo2m 67.76 10.73 66.31 10.74 55.16 9.96 51.59 11.24 125.45 24.20 113.90 24.10
mBART 53.94 0.00 52.44 0.00 49.99 0.00 46.20 0.00 98.28 0.00 96.88 0.00
mBARTm2m 57.72 4.33 56.30 4.04 52.77 2.98 49.09 3.36 106.68 8.96 105.70 8.94
7
mBARTm2o 58.03 4.61 56.64 4.31 53.23 3.32 49.58 3.72 107.94 10.21 107.26 10.47
mBARTo2m 64.09 10.79 62.87 10.64 59.23 9.49 56.08 10.17 123.84 25.99 120.47 23.76
mBART 48.50 0.00 47.79 0.00 51.22 0.00 49.37 0.00 94.92 0.00 100.91 0.00
mBARTm2m 51.54 3.69 50.86 3.51 54.22 3.17 52.37 3.14 104.18 9.91 109.97 9.35
8
mBARTm2o 51.67 3.75 51.00 3.60 54.76 3.63 52.96 3.67 105.67 11.31 111.65 10.98
mBARTo2m 58.53 10.57 57.90 10.39 60.62 9.65 58.81 9.60 121.89 27.64 124.33 23.66
mBART 44.39 0.00 44.78 0.00 50.78 0.00 49.83 0.00 94.01 0.00 103.16 0.00
mBARTm2m 47.47 3.73 47.82 3.55 53.04 2.78 52.13 2.95 103.53 10.31 112.32 9.64
9
mBARTm2o 47.57 3.76 47.92 3.56 53.55 3.13 52.61 3.15 105.10 11.78 113.97 11.19
mBARTo2m 55.12 11.12 55.55 11.12 58.83 8.49 57.86 8.42 120.99 27.80 126.63 23.92
mBART 43.50 0.00 42.87 0.00 55.41 0.00 54.97 0.00 91.92 0.00 105.62 0.00
mBARTm2m 46.93 4.66 46.28 4.31 57.64 2.89 57.30 4.13 101.92 10.67 114.82 9.94
10
mBARTm2o 46.88 4.50 46.24 4.23 58.24 3.25 57.93 4.21 103.56 12.19 116.47 11.45
mBARTo2m 54.68 12.00 54.22 11.91 62.95 8.24 62.28 8.64 119.35 28.09 128.21 23.29
Table5: ThescalingeffectviasingularvaluedecompositionofmBARTandthecontinued-trainedMT
models.