Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View Planning
Sicong Pan⋆ Liren Jin⋆ Xuying Huang Cyrill Stachniss Marija Popovic´ Maren Bennewitz
Abstract—Object reconstruction is relevant for many au-
tonomous robotic tasks that require interaction with the en-
vironment. A key challenge in such scenarios is planning
view configurations to collect informative measurements for
reconstructing an initially unknown object. One-shot view
planning enables efficient data collection by predicting view
configurations and planning the globally shortest path con-
necting all views at once. However, geometric priors about
the object are required to conduct one-shot view planning.
In this work, we propose a novel one-shot view planning
approach that utilizes the powerful 3D generation capabilities
of diffusion models as priors. By incorporating such geometric
priors into our pipeline, we achieve effective one-shot view
planning starting with only a single RGB image of the object
to be reconstructed. Our planning experiments in simulation
andreal-worldsetupsindicatethatourapproachbalanceswell
between object reconstruction quality and movement cost.
I. INTRODUCTION
Fig. 1: An example of our RGB-based one-shot view planning by
Manyautonomousroboticapplicationsrequire3Dmodels exploiting priors from 3D diffusion models. Our goal is to plan
of objects to perform downstream tasks, e.g., pose esti- a set of views (blue) at once to collect informative RGB images
mation [35], object manipulation [3], and detection [36]. for object reconstruction. The key component in our approach
is a 3D diffusion model generating the corresponding 3D mesh
When deployed in initially unknown environments, a robot
of a single RGB image from the initial camera view (red). By
oftenneedstoreconstructtheobjectsbeforeinteractingwith
leveraging the mesh as geometric priors, our approach produces
them. During this procedure, a challenge is planning a view viewconfigurationsspecificallyassociatedwiththetargetobjectand
sequencetoacquirethemostinformativemeasurementstobe calculates the globally shortest path. In particular, we plan denser
integrated into the reconstruction system while minimizing views to observe more geometrically complex parts (front part of
the object in the example) to improve the reconstruction quality.
the robot’s travel distance or operation time.
Without any prior knowledge about the environment, a
the object is required. Previous works consider planning
common strategy is to plan the next-best-view (NBV) it-
priors based on multi-view images or partial point cloud
eratively based on the current reconstruction state [7, 18,
observations. However, they either only handle a fixed view
19, 23, 29, 33, 37]. However, NBV planning only generates
configuration [28] or rely on depth sensors [6, 26].
a local path to the subsequent view and cannot effectively
distribute the mission time or movement budget, resulting in To address these aforementioned limitations, we propose
suboptimalviewplanningperformance.Analternativelineof integrating geometric priors from 3D diffusion models into
workconsidersone-shotviewplanning[6,26,28].Givenini- one-shot view planning. Recently, 3D diffusion models
tial measurements of an object to be reconstructed, one-shot emerged as a powerful tool for generating 3D content based
view planning predicts a set of views at once and computes on text prompts or a single image. By training on large
the globally shortest path connecting them. A robot’s sensor datasets, 3D diffusion models learn prior knowledge about
thenfollowstheplannedpathtocollectmeasurements,which objects commonly seen in real life [13, 16, 17]. Humans
are used for object reconstruction after the data acquisition similarly exploit prior knowledge to hallucinate 3D models
is completed. By decoupling data collection and object of an object based on semantics and appearance information
reconstruction,theseapproachesdonotrelyoniterativemap contained in RGB observations. However, recovering a 3D
updates for adaptive view planning online during a mission. representation from a single RGB image is inherently an
To perform one-shot view planning, prior knowledge about ill-posed problem and corresponds to multiple plausible
solutions. As a result, models generated by 3D diffusion
⋆Theseauthorscontributedequallytothiswork. models do not reflect the exact representation of the object
All authors are with the University of Bonn, Germany. Cyrill Stach-
to be reconstructed. This prohibits their direct application
niss and Maren Bennewitz are additionally with the Lamarr Institute for
Machine Learning and Artificial Intelligence, Bonn, Germany. This work as a method for accurate 3D representation, as required in
haspartiallybeenfundedbytheDeutscheForschungsgemeinschaft(DFG, robotics tasks. Incorporating the capabilities of 3D diffusion
German Research Foundation) under grant 459376902 – AID4Crops and
models to provide geometric priors in robotics remains an
underGermany’sExcellenceStrategy,EXC-2070–390732324–PhenoRob.
Corresponding:span@uni-bonn.de. unexplored area.
4202
raM
52
]OR.sc[
1v30861.3042:viXraThe main contribution of this work is a novel RGB-based a neural network to predict the utility of candidate views
one-shot view planning approach that exploits the geometric given partial point cloud observations. In the context of
priorsfrom3Ddiffusionmodels.Ourapproachenablesview NBV planning for RGB-based object reconstruction, Jin
planningwithanobject-specificviewconfigurationforobject et al. [8] integrate uncertainty estimation into image-based
reconstruction as shown in Fig. 1. A key component of neural rendering to guide NBV selection in a mapless way.
our pipeline is a 3D diffusion model that outputs a 3D Linetal.[11]andSu¨nderhaufetal.[33]trainanensembleof
mesh of the object given one RGB image as input. This NeRF models, utilizing the ensemble’s variance to measure
generated mesh is a proxy to the inaccessible ground truth uncertainty for NBV planning.
3D model and serves as the basis for our one-shot view While showing promising object reconstruction results,
planning. Given the generated 3D mesh, we convert the NBV planning often relies on computationally intensive
one-shot view planning into a customized set covering opti- online map updates and its greedy nature leads to inefficient
mization to calculate the minimum set of views that densely paths. To address these limitations, recent works propose
covers the mesh, which we solve using linear programming. one-shot view planning paradigm. Given an initial measure-
Our approach adaptively places the views and follows the ment, one-shot view planning predicts all required views at
globallyshortestpathforcollectinginformativeRGBimages once and calculates the globally shortest path connecting
aroundtheobject.Afterthedatacollection,wetrainaNeural them, resulting in reduced movement costs. The pioneering
RadianceField(NeRF)usingall collectedimagestoacquire work SCVP [26] trains a neural network in a supervised
the object’s 3D representation. way to directly predict the global view configuration given
To the best of our knowledge, our approach is the first initial point cloud observations. To generate training labels,
to leverage 3D diffusion models for view planning. We the authors solve the set covering problem to obtain a view
make the following three claims: (i) we exploit the powerful configurationfullycoveringthegroundtruth3Dmodels.Hu
3D diffusion models to enable our one-shot view planning etal.[6]furtherreducestherequiredviewsbyincorporating
starting with only one RGB image as input; (ii) we cast apointcloud-basedimplicitsurfacereconstructionmethodto
the one-shot view planning as a customized set covering complete missing surfaces before conducting one-shot view
optimization problem, yielding view configurations suitable planning.InthedomainofRGB-basedobjectreconstruction,
for RGB-based object reconstruction using NeRFs; (iii) our Pan et al. [28] propose a view prediction network to predict
one-shot view planning allows for adaptive view placement the number of views to reconstruct an object using NeRFs
to account for varying object geometries, achieving a better required to reach its performance upper bound. However,
trade-off between movement cost and reconstruction quality due to the lack of geometric representations during the
comparedtobaselines.Weconductextensiveexperimentson view planning stage, this work only considers distributing
publiclyavailableobjectdatasetsandinrealworldscenarios, the views following a fixed pattern, without adapting view
demonstrating the applicability and generalization ability of configurations to account for varying object geometries.
ourapproach.Tosupportreproducibilityandfutureresearch, Our work shares the same idea of using one-shot view
our implementation will be open-sourced at: https:// planning to reconstruct an unknown object. Different from
github.com/psc0628/DM-OSVP previous works that rely on depth sensors [6, 26] or fixed
view configurations [28], our novel approach only requires
II. RELATEDWORK
RGB inputs and plans view configurations specifically asso-
In this section, we introduce relevant works on view ciatedwiththeobjects,leadingtobetterobjectreconstruction
planning for object reconstruction and diffusion models for performance while reducing movement costs.
3D generation.
B. Diffusion Models for 3D Generation
A. View Planning for Object Reconstruction
Diffusionmodelsarestate-of-the-artgenerativemodelsfor
Objectreconstructionisessentialinmanyroboticapplica- producing plausible high-quality images. Starting from ran-
tions. One important capability in this scenario is to actively domGaussiannoises,diffusionmodelslearntosubsequently
reconstruct the object using a robot sensor. Without any denoise the input to finally recover the true images [9, 31].
prior knowledge, a common approach is to plan the NBV By training on large datasets, diffusion models acquire
iteratively based on the current reconstruction state, thus powerful prior knowledge and show their capabilities in the
maximizingtheinformationoftheobjectinagreedymanner. domain of 2D image generation.
Isler et al. [7] propose selecting the NBV by calculating Inspired by the advances of diffusion models, recent
the information gain based on visibility and the likelihood works investigate using diffusion models for 3D content
of observing new parts of the object to be reconstructed. generation. Given a text prompt describing a desired scene,
Similarly, Pan et al. [24, 25] weight the 3D space based on DreamFusion[30],ProlificDreamer[34],andMVDream[32]
visibility and distance to observe surfaces and then employ optimizeadifferentiable3Drepresentation,e.g.,NeRF,from
coverageoptimizationforNBVplanning.Inaddition,Menon scratchandleverageneuralrenderingtogenerate2Dimages
et al. [19] introduce a shape completion method based on at different viewpoints. These rendered images are then
partially observed objects and conduct NBV planning to fed into 2D diffusion models to calculate the similarity to
cover the estimated missing surfaces. PC-NBV [37] trains the priors learned by the diffusion model, which guide theFig.2:OverviewofourproposedRGB-basedone-shotviewplanningpipeline.GivenasingleRGBimageoftheobjecttobereconstructed,
weleveragea3Ddiffusionmodel,One-2-3-45++[12],togeneratea3Dmesh.Thismeshservesasaproxytothegroundtruthgeometry
and is the basis for our view planning. Based on this prior, we construct the one-shot view planning task as a customized set covering
optimization and solve it to obtain a minimum set of views required to densely covers the mesh surfaces. The RGB camera starts at the
initial view (shown as ⊗) and follows the generated globally shortest path to collect RGB images, which we use to train a NeRF in
Instant-NGP [21] after the data acquisition is completed.
3D shape optimization process. While showing impressive the state-of-the-art 3D diffusion model One-2-3-45++ [12]
results, these methods suffer from prolonged rendering and for generating plausible meshes due to its accurate mesh
optimization times, limiting their robotic applications. generation and efficient inference compared to other 3D
Another line of work investigates fine-tuning pretrained diffusionmodels[30,32,34].One-2-3-45++modelistrained
2D diffusion models for multi-view synthesis from single on Objaverse [2], a large 3D model dataset, to learn the
image inputs [14, 15]. The follow-up work One-2-3-45 [13] prior knowledge of varying geometries of commonly seen
produces3Dmeshesusingimagesgeneratedfromthemulti- objectsandshowsgoodgeneralizationabilityonotherobject
view diffusion models. However, its performance is limited datasets.Leveragingthispowerfultool,weusethegenerated
by the inconsistency between multi-view images. Recent 3D meshes as geometric priors for one-shot view planning
diffusionmodelOne-2-3-45++[12]mitigatestheproblemof introduced next.
inconsistencies by conditioning the multi-view image gen-
B. One-Shot View Planning as Set Covering Optimization
eration on each other. The generated multi-view consistent
images are exploited as the guidance for 3D diffusion to One-shot view planning problem can be treated as a
directly produce high-quality meshes in a short time, i.e., conventionalsetcoveringoptimization.Sincesolvingthisop-
within 60s. In this work, we utilize geometric priors from timizationproblemnecessitatesanexplicit3Drepresentation
3D diffusion models to enable RGB-based one-shot view oftheobjecttobereconstructed,previousworks[6,26]rely
planning for object reconstruction. on depth sensors to acquire initial 3D models of the object.
Instead,byincorporatingthegeometricpriorsof3Ddiffusion
III. OURAPPROACH
models into our planning pipeline, our approach solves the
We propose a novel RGB-based one-shot view planning one-shot view planning problem in an RGB camera setup.
method for unknown object reconstruction. An overview To facilitate the efficiency of set covering optimization,
of our approach is shown in Fig. 2. Given a single RGB sparse surface representations are desired. To this end, we
measurementoftheobject,weleveragea3Ddiffusionmodel first sample a set of surface points from the mesh produced
to generate its corresponding mesh. Based on rich prior by the 3D diffusion model and subsequently voxelize them
information contained in the generated mesh, we formulate using OctoMap [5] to get a sparse surface point set P ,
surf
one-shot view planning as a set covering optimization prob- with surface point p ∈ P . We denote v as a candidate
i surf
lem, which we solve with linear programming to acquire viewwithinadiscretecandidateviewspaceV ⊂R3×SO(3)
the minimum set of views densely covering mesh surfaces. and P as the set of surface points observable from this
v
We calculate the globally shortest path connecting all views view. Each set P is determined via the ray-casting process
v
for data collection using a robot’s RGB camera. After data implemented in OctoMap. We define an indicator function
collection, we train a NeRF model using all collected RGB I(p,v) to represent whether a surface point p is observable
images to generate a 3D representation of the object. from view v:
A. Geometric Priors from 3D Diffusion Model (cid:40)
1 if p∈P
A key component of our approach is a 3D diffusion I(p,v)= v . (1)
0 otherwise
modelforpredictingthecorrespondingmeshgivenonlyone
RGB image as an initial observation. Specifically, we use Given P and each P , the conventional set covering
surf vFig. 3: Illustration of the impact of multi-view constraints. α Fig.4:Illustrationoftheimpactofdistanceconstraints:(a)spatially
denotes the minimum number of views required to observe each clustered views (the orange circle showcases an example area
surface point. Larger α values lead to optimization solutions with of spatially clustered views); (b) spatially more uniform views.
more views densely covering the surfaces. Bothviewconfigurationsarefeasiblesolutionstoouroptimization
problem. By incorporating distance constraints, we express the
optimizationproblemaimstofindtheminimumsetofviews preference for spatially uniform distribution to avoid redundant
required for completely covering the surface points. For information in clustered views.
instance, consider P = {p ,p ,p }, P = {p ,p },
surf 1 2 3 v1 1 2
P = {p ,p }, and P = {p ,p }. The union of these leads to more spatially uniform views, while an excessively
thv r2 ee sets e2 qu3 als the entv ir3 e surfac1 e s3 et, i.e., (cid:83) P =P . large value can render the problem infeasible. For our view
v v surf
However, we can cover all surface points with only two planning,wetrytofindthemaximumβ valuethatstillyields
sets,P andP .Vanillasetcoveringoptimizationrequires an optimization solution. Given that different objects exhibit
v1 v2
that each surface point should be covered by at least one diverse geometries, their respective maximum β values also
view. This definition aligns well with object reconstruction vary. Therefore, we run optimization iteratively to find the
employing depth-sensing modalities [6, 26, 27], as surfaces maximum β for a specific object in an automatic manner.
can be recovered by direct depth fusion when provided with Takingalltheseconditionsintoaccount,weformulateour
acorrespondingpointcloudobservation.However,forRGB- set covering optimization problem as a constrained integer
basedobjectreconstructionusingNeRFs,maprepresentation linear programming problem defined as follows:
learning is achieved by minimizing the photometric loss
(cid:88)
when reprojecting hypothetical surface points back to 2D min: x ,
v
image planes, which requires that a surface point should be
v∈V
observed from different perspectives to recover its true 3D s.t.: (a) x ∈{0,1} ∀v ∈V
v
representation. This implies that planned views covering all (cid:88) (2)
(b) I(p,v)x ≥α ∀p∈P
surface points of the generated mesh once are not sufficient v surf
for object reconstruction using NeRFs. v∈V
To this end, we customize the set covering optimization (c) x v+x v′ ≤1 ∀dv v′ ≤βdm v in,
problem for RGB-based object reconstruction using NeRFs. (cid:80)
where the objective function x is designed to min-
Ratherthanrequiringeachsurfacepointtobeobservedbyat v∈V v
imize the total number of selected views, while subject to
leastoneview,weproposemulti-viewconstraintstoenforce
three constraints: (a) x is a binary variable representing
that a given surface point should be covered by a minimum v
whether a view v is included in the set of selected views
number α∈N+ of views to account for multi-view learning
or not; (b) each surface point p ∈ P must be observed
in NeRFs. Larger α values require denser surface coverage surf
by a minimum of α selected views; and (c) if a view v
inouroptimizationproblem,resultinginsolutionswithmore
is selected, any neighboring view v′, whose distance dv′ is
views required as shown in Fig. 3. Note that when α ≥ 2, v
smaller than βdmin, must not be selected.
we exclude points that are visible from fewer than α views. v
We employ the Gurobi optimizer, a linear programming
This mechanism ensures the optimization problem has a
solver [4], to compute the solution for the problem. We
feasible solution. However, our multi-view covering setup
present an instance solution in Fig. 4(b) showcasing the
may contain multiple feasible solutions since most of the
optimized minimum set of views required for densely cov-
surface points can be observed from a large range of view
ering the Motorbike object surface with α=3 and distance
perspectives. Some of them lead to views clustered closely
constraints.
together in Euclidean space. Fig. 4(a) illustrates an instance
of spatially clustered views for covering the Motorbike
C. Path Generation and Object Reconstruction
object. These spatially clustered views exhibit similarity in
the collected images, thus leading to redundant information By planning all required views before data collection, the
about the object. one-shot view planning paradigm shows a major advantage
To alleviate this issue, we introduce a parameter β ∈R≥0 in reduced movement costs. Given the optimized set of
foradditionaldistanceconstraintstoavoidselectingspatially views introduced above, we plan the globally shortest path
clustered views. We denote dv′ as the Euclidean distance connectingallviewsbysolvingtheshortestHamiltonianpath
v
betweenviewsvandv′,whiledministheEuclideandistance problem on a graph, which is similar to the traveling sales-
v
from view v to its nearest neighboring view. We prevent manproblembutwithoutreturningtothestartingnode[22].
other views within a specific distance βdmin of the view The robot’s RGB camera follows the global path to acquire
v
v from being selected again in the solution. A larger β RGB measurements at planned views.α Planned Views PSNR ↑ SSIM ↓ Movement Cost (m) ↓ Inference Time (s) ↓
1 6.8 ± 1.5 30.167 ± 0.810 0.9365 ± 0.0121 1.754 ± 0.258 140.4 ± 26.9
2 12.8 ± 1.7 31.436 ± 0.622 0.9530 ± 0.0049 2.629 ± 0.224 145.9 ± 29.3
3 17.8 ± 2.4 31.853 ± 0.615 0.9599 ± 0.0038 2.998 ± 0.225 147.9 ± 31.8
4 22.5 ± 3.8 31.995 ± 0.684 0.9633 ± 0.0035 3.214 ± 0.372 148.2 ± 33.1
5 28.7 ± 3.8 32.120 ± 0.786 0.9663 ± 0.0034 3.725 ± 0.312 150.0 ± 40.6
6 34.1 ± 5.1 ⋆32.243 ± 0.779 ⋆0.9684 ± 0.0042 4.093 ± 0.441 147.6 ± 34.1
7 38.8 ± 3.8 †32.248 ± 0.807 †0.9694 ± 0.0041 4.190 ± 0.247 147.3 ± 38.2
TABLE I: Analysis on multi-view constraints. α denotes the minimum number of views required to observe each surface point. Planned
viewsindicatethenumberofoptimizedviewsunderdifferentαvalues.PSNRandSSIMareaveragedover100novelviews.Eachvalue
reportstheaveragemeanandstandarddeviationon10testobjects.Thestarsymbol(⋆)indicatesstatisticallysignificantresultsforα=6
compared to α=5 based on the paired t-test with a p-value of 0.05. Conversely, the dagger symbol (†) indicates non-significant results
for α=7 compared to α=6 based on the paired t-test with a p-value of 0.05. Results show that our optimizer plans more views with
increasingαvaluesandachievespeakperformanceattheα=6.Itisworthmentioningthatincreasingαfrom1to2leadstothehighest
performance gain, indicating that our formulation of set covering benefits NeRF-based reconstruction.
Fig. 5: Ten test objects used in our simulation experiments.
After data collection, we use NeRFs to acquire the fi-
nal 3D representation of the object. Specifically, we adopt Fig. 6: Ablation study on distance constraints. PSNR and SSIM
averaged over 100 novel views. Each value is reported as the
Instant-NGP [21] to train our NeRF, due to its efficient
averagedmeanon10testobjects.Weobservedstatisticallysignif-
training performance and common usage in baseline ap-
icantresultsforourmethodwhencomparedtotheversionwithout
proaches [11, 28, 33]. distance constraints across all α values, as determined through
paired t-tests with a p-value of 0.05. This suggests that the set
IV. EXPERIMENTALRESULTS coveringoptimizationwiththedistanceconstraintsfindsbetterview
configurations, leading to superior NeRF training results.
A. Experimental Setup
In our simulation experiments, we consider an object-
centric hemispherical view space with 144 uniformly dis- mesh surfaces; (2) both PSNR and SSIM metrics exhibit a
tributed view candidates for view planning. We set the view consistent improvement with increasing α. Specifically, we
space radius to 0.3m. We test our approach on 10 geometri- achieve the highest performance gain by changing α = 1
cally complex 3D object models from the HomebrewedDB to α = 2, justifying our modification of the set covering
dataset [10]. The test objects are shown in Fig. 5. We optimizationtoaccountforRGB-basedobjectreconstruction
normalize all objects to fit into a bounding sphere with a using NeRFs; (3) our method reaches its peak performance
radius of 0.1m. All RGB measurements are at 640px × attheαvalueof6,whileincreasingαtoahighervaluedoes
480px resolution. We adopt a grid size of 50 × 50 × 50 notyieldastatisticallysignificantperformanceimprovement.
in OctoMap for voxelizing the mesh surface points. The set
C. Ablation Study on Distance Constraints
covering optimization for view planning runs on an Intel
i7-12700H CPU, while NeRFs training is conducted on an In this ablation study, we investigate the impact of the
NVIDIA RTX3060 laptop GPU. distance constraints introduced in Sec. III-B. To prevent the
To evaluate NeRF reconstruction quality, we report the optimizer from finding a view configuration that leads to
peak signal-to-noise ratio (PSNR) and the structural simi- clusteredviews,weintroducetheparameterβ asthedistance
larity index (SSIM) [20]. Additionally, we evaluate recon- constraints into our optimization formulation. We adopt
struction efficiency in terms of inference time for view binary search in our implementation to find out the object-
planningandaccumulatedmovementcostfordatacollection specific maximum β that still yields a feasible optimization
in Euclidean distance. solution. The search step is set to 0.1 for all experiments.
We evaluate the influence of our distance constraints by
B. Analysis on Multi-View Constraints
performing an ablation study over different α values. Fig. 6
In this section, we explore the influence of multi-view showsthedifferencesbetweenoptimizationwithandwithout
constraints introduced in Sec. III-B. We test our methods the proposed constraints. In all circumstances, optimization
acrossvaryingαvaluesfrom1to7,asdetailedinTABLEI. without considering the distance constraints (β =0) outputs
The outcomes reveal that: (1) with increasing α values, our clusteredviewswithredundantinformationabouttheobject,
optimizer outputs on average more views for covering the leading to inferior NeRF training performance in terms ofFig.7:Comparisontobaselinesonviewplanningperformanceunderdifferentαvaluescorrespondingtothenumberofoptimizedviews.
PSNR and SSIM are averaged over 100 novel views. Each value reports the mean on 10 test objects. PRV is not associated with α
valuesandisrepresentedbyadashedline.Ascanbeseen,(1)ourmethodachieveshigherPSNR/SSIMvaluesagainstrandomandNBV
methods, indicating that leveraging geometric priors from diffusion models leads to more informative views; (2) compared to PRV using
fixed view configuration, our adaptive view configuration is more suitable for object-specific view planning, achieving either a lower
movement cost with an on-par performance (α=5) or a higher performance with a slightly lower movement cost (α=6).
PSNRandSSIM.Thisjustifiesourdesignchoiceofintroduc- powerful geometric priors from 3D diffusion models signifi-
ingthedistanceconstraintstofindbetterviewconfigurations. cantlybenefitsone-shotviewplanningforRGB-basedobject
reconstruction.
D. Evaluation of View Planning for Object Reconstruction Comparison to NBV Methods. Compared to two NBV
baselines, our method achieves higher PSNR and SSIM
Baselines. We compare our novel one-shot view planning
values across all α values with much less movement costs
with the following baselines:
and inference time, as shown in Fig. 7. Specifically, our
• Random selects a certain number of views randomly method excels under various resource constraints, e.g., dif-
and subsequently plans a global path to connect them. ferent planned views according to different α values. This
• EnsembleRGB [11] leverages RGB variance of the implies that using diffusion models for priors leads to more
NeRFensembleasuncertaintyquantificationtoplanthe informative views for unknown object reconstruction com-
NBV that maximize the information gain. pared to NBV methods considering the ensemble’s variance
• EnsembleRGBD [33] extends EnsembleRGB by incor- for uncertainty measurements. We attribute the significant
porating a density-aware epistemic uncertainty com- reductions in movement cost and inference time to global
puted on ray termination probabilities in unobserved pathplanningandtheone-shotnon-iterativeparadigm,which
object areas. avoids iterative map updates and uncertainty computation.
• PRV [28]usesanetworktopredicttherequirednumber Comparison to PRV. Since the PRV method obtains the
of views that achieves the peak performance of NeRF
numberofviewsbypredictingtheupperlimitsofNeRFrep-
training. A fixed hemispherical view configuration is
resentations, it is not associated with α values and is repre-
then generated according to the predicted number of
sentedbyadashedlineinFig.7.Theresultsindicatethatthe
views.
proposedRGB-basedone-shotviewplanningapproach,with
For a fair comparison, we use Instant-NGP [21] with an α=5 setting, delivers nearly identical quality metrics in
the same configuration for the training and testing in all PSNRandSSIMwhencomparedtoPRV,yetitbenefitsfrom
experiments. Therefore, the performance differs purely as reduced movement cost. Moreover, when α is adjusted to 6,
a consequence of collected RGB images using different our method surpasses PRV in terms of PSNR and SSIM
planning strategies. As depicted in Table I, varying α values quality while still maintaining a slightly lower movement
result in different numbers of planned views. Therefore, to cost. This confirms that our adaptive object-specific view
comprehensively assess the performance of our planner, we configurationissuperiortofixedviewconfigurationsinPRV
evaluate all baselines using an equivalent number of views for handling varying geometries of objects.
corresponding to each α value in our approach (excluding In conclusion, our RGB-based one-shot view planning
PRV, which predicts its own required number of views). method demonstrates several advantages over the baselines.
Comparison to Random Selection. As shown in Fig. 7, By integrating powerful geometric priors from 3D diffusion
our RGB-based one-shot view planning approach surpasses models, our method effectively leverages available object
theone-shotRandombaselineacrossallαvaluesintermsof information, resulting in more informative and better dis-
PSNR and SSIM. This is because heuristic random methods tributed views. Moreover, our approach showcases superior
do not utilize any available information about the objects, adaptability through its adaptive view configuration mecha-
in contrast to our approach. The Random method exhibits nism.UnlikethefixedviewconfigurationinPRV,ourmethod
a slightly lower movement cost. We believe that this occurs dynamicallyadjuststheviewconfigurationsfordifferentob-
sinceitcanproducespatiallyclusteredviews,yieldingpoorer jectsbasedontheirvaryinggeometries.However,weobserve
reconstructionquality.Thesefindingsconfirmthatleveraging a longer inference time of our method compared to theFig.9:Real-worldexperimentshowingthetestobject.Weruntwo
testtrialswithdifferentinitialviews.PSNRandSSIMareaveraged
over100novelviews.Eachvalueisreportedastheaveragedmean
and with standard deviation (the error bar) on two test trials. By
adaptingviewsbasedontheobjectgeometries,ourmethodachieves
Fig.8:Analysisofafailurecase.Top:Inputimagetothediffusion a higher performance with lower movement costs.
modelandthegeneratedmeshobservedfromdifferentperspectives.
Red circles indicate the missing parts of the generated mesh
compared to the ground truth model. Bottom: We compare the
camera is activated). MoveIt [1] is employed for robotic
reconstructionresultsusingourone-shotviewplanningbasedonthe
ground truth mesh and the generated mesh, showing that wrongly motion planning. The accompanying video1 illustrates the
generated geometry leads to reduced performance. online reconstruction process.
To validate our findings in Sec. IV-D, we compare our
method against baselines in the real world. It is worth
PRV and random methods, primarily due to the constraints noting that due to imperfect camera poses and noise in
imposed by the generation process of the diffusion model real world experiments, the pose optimization functionality
(approximately 60s) and the online optimization process implemented in Instant-NGP is enabled during our NeRF
(approximately 80s). We plan to improve this in the future. training.Theexperimentalenvironmentandcomparisonsare
shown in Fig. 9. From the results, we confirm that (1)
E. Analysis of Failure Case
our method generalizes to real world environments; and (2)
Although our approach successfully performs one-shot our method adaptively plans view configurations according
view planning from a single RGB image and achieves to object geometries to achieve higher PSNR with lower
promising unknown object reconstruction performance, we movement costs compared to the PRV and NBV methods.
observeperformanceinadequaciesinatestcase.Specifically, Our method achieves peak performance at α = 7, which
thegeneratedmeshfromthe3DdiffusionmodeloftheDrill is larger than the value of 6 determined in Sec. IV-B. This
object, as depicted in Fig. 8(a), demonstrates geometrical mightbecausedbythenoiseinthecameraposeandimages,
discrepanciescomparedtothegroundtruth.Thesedisparities makingitchallengingforviewplanningtasks.Weobservea
might stem from the limited information available due to similar slight performance reduction for the NBV methods.
occlusion in a single input image and the insufficient rep- Nevertheless, when deployed in real-world environments,
resentation of this type of object in the training dataset. To anestimateoftheactualobjectsizeisnecessarytoscalethe
furthervalidatetheimpactofthisissueonthereconstruction, diffusion-generated models, given that the generated mesh
we conducted experiments by replacing the generated mesh lacks scale information.
with the ground truth mesh. Fig. 8(b-c) reveals that our
method using the ground truth mesh achieves higher PSNR V. CONCLUSIONS
and SSIM compared to input with the diffusion-generated In this paper, we present a novel one-shot view planning
mesh. The results indicate that the quality of geometric method starting with only a single RGB image of the
priors, i.e., the mesh generated from diffusion models, is unknown object to be reconstructed. The proposed method
crucial for our one-shot view planning performance. exploits priors from 3D diffusion models as a proxy to the
inaccessible ground truth 3D model as the basis for one-
F. Real-World Experiments
shot view planning. We develop a customized variant of the
We deploy our approach in a real world tabletop environ- set covering optimization problem tailored for NeRF-based
ment using a UR5 robot arm with an Intel Realsense D435
camera mounted on its end-effector (only the RGB optical 1https://youtu.be/EKZPHb5-UZkreconstruction, which aims to adaptively compute a view [18] M.Mendoza,J.I.Vasquez-Gomez,H.Taud,L.E.Sucar,andC.Reta,
configuration that densely covers the generated mesh from “Supervised Learning of the Next-Best-View for 3D Object Recon-
struction,”PatternRecognitionLetters,vol.133,pp.224–231,2020.
3Ddiffusionmodels.Wecomputeagloballyshortestpathon
[19] R.Menon,T.Zaenker,N.Dengler,andM.Bennewitz,“NBV-SC:Next
thisviewconfiguration,correspondingtotheminimumtravel BestViewPlanningbasedonShapeCompletionforFruitMappingand
distance. Our experiments validate that utilizing geometric Reconstruction,” in Proc. of the IEEE/RSJ Intl. Conf. on Intelligent
RobotsandSystems(IROS),2023.
priors from 3D diffusion models yields more informative
[20] B.Mildenhall,P.P.Srinivasan,M.Tancik,J.T.Barron,R.Ramamoor-
views compared to random and next-best-view methods. thi,andR.Ng,“NeRF:RepresentingScenesasNeuralRadianceFields
When compared to the state-of-the-art RGB-based one-shot forViewSynthesis,”inProc.oftheEurop.Conf.onComputerVision
(ECCV),2020.
baseline, our adaptive view placement based on varying
[21] T.Mu¨ller,A.Evans,C.Schied,andA.Keller,“InstantNeuralGraphics
object geometries demonstrates better view planning per- Primitives with a Multiresolution Hash Encoding,” ACM Trans. on
formance compared to a fixed view configuration. The real Graphics,vol.41,no.4,pp.102:1–102:15,2022.
[22] S.Oßwald,M.Bennewitz,W.Burgard,andC.Stachniss,“Speeding-
world experiment suggests the applicability of our method.
UpRobotExplorationbyExploitingBackgroundInformation,”IEEE
RoboticsandAutomationLetters(RA-L),vol.1,no.2,pp.716–723,
REFERENCES 2016.
[23] E.PalazzoloandC.Stachniss,“EffectiveExplorationforMAVsBased
[1] S.Chitta,“MoveIt!:AnIntroduction,”RobotOperatingSystem(ROS) ontheExpectedInformationGain,”Drones,vol.2,no.1,pp.59–66,
TheCompleteReference,vol.1,pp.3–27,2016.
2018.
[2] M.Deitke,D.Schwenk,J.Salvador,L.Weihs,O.Michel,E.Vander- [24] S. Pan and H. Wei, “A Global Max-Flow-Based Multi-Resolution
Bilt,L.Schmidt,K.Ehsani,A.Kembhavi,andA.Farhadi,“Objaverse: Next-Best-ViewMethodforReconstructionof3DUnknownObjects,”
A Universe of Annotated 3D Objects,” in Proc. of the IEEE/CVF IEEERoboticsandAutomationLetters(RA-L),vol.7,no.2,pp.714–
Conf.onComputerVisionandPatternRecognition(CVPR),2023.
721,2022.
[3] N. Dengler, S. Pan, V. Kalagaturu, R. Menon, M. Dawood, and [25] S.PanandH.Wei,“AGlobalGeneralizedMaximumCoverage-Based
M. Bennewitz, “Viewpoint Push Planning for Mapping of Unknown Solutionto theNon-Model-BasedView Planningproblemfor object
ConfinedSpaces,”inProc.oftheIEEE/RSJIntl.Conf.onIntelligent reconstruction,”JournalofComputerVisionandImageUnderstanding
RobotsandSystems(IROS),2023. (CVIU),vol.226,p.103585,2023.
[4] L.GurobiOptimization,“GurobiOptimizerReferenceManual,”2021. [26] S.Pan,H.Hu,andH.Wei,“SCVP:LearningOne-ShotViewPlanning
[5] A.Hornung,K.M.Wurm,M.Bennewitz,C.Stachniss,andW.Bur- viaSetCoveringforUnknownObjectReconstruction,”IEEERobotics
gard, “OctoMap: An Efficient Probabilistic 3D Mapping Framework andAutomationLetters(RA-L),vol.7,no.2,pp.1463–1470,2022.
BasedonOctrees,”AutonomousRobots,vol.34,pp.189–206,2013.
[27] S. Pan, H. Hu, H. Wei, N. Dengler, T. Zaenker, and M. Bennewitz,
[6] H.Hu,S.Pan,L.Jin,M.Popovic´,andM.Bennewitz,“ActiveImplicit “Integrating One-Shot View Planning with a Single Next-Best View
ReconstructionUsingOne-ShotViewPlanning,”inProc.oftheIEEE viaLong-TailMultiviewSampling,”arXivpreprintarXiv:2304.00910,
Intl.Conf.onRobotics&Automation(ICRA),2024.
2023.
[7] S. Isler, R. Sabzevari, J. Delmerico, and D. Scaramuzza, “An Infor- [28] S. Pan, L. Jin, H. Hu, M. Popovic´, and M. Bennewitz, “How Many
mation Gain Formulation for Active Volumetric 3D Reconstruction,” ViewsAreNeededtoReconstructanUnknownObjectUsingNeRF?”
in Proc. of the IEEE Intl. Conf. on Robotics & Automation (ICRA), in Proc. of the IEEE Intl. Conf. on Robotics & Automation (ICRA),
2016. 2024.
[8] L. Jin, X. Chen, J. Ru¨ckin, and M. Popovic´, “NeU-NBV: Next Best [29] X.Pan,Z.Lai,S.Song,andG.Huang,“ActiveNeRF:LearningWhere
ViewPlanningUsingUncertaintyEstimationinImage-BasedNeural toSeewithUncertaintyEstimation,”inProc.oftheEurop.Conf.on
Rendering,”inProc.oftheIEEE/RSJIntl.Conf.onIntelligentRobots ComputerVision(ECCV),2022.
andSystems(IROS),2023.
[30] B. Poole, A. Jain, J. T. Barron, and B. Mildenhall, “DreamFusion:
[9] A.J.JonathanHoandP.Abbeel,“DenoisingDiffusionProbabilistic Text-to-3Dusing2DDiffusion,”inProc.oftheIntl.Conf.onLearning
Models,” in Proc. of the Conf. on Neural Information Processing Representations(ICLR),2023.
Systems(NeurIPS),2020.
[31] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,
[10] R.Kaskman,S.Zakharov,I.Shugurov,andS.Ilic,“HomebrewedDB: “High-Resolution Image Synthesis with Latent Diffusion Models,”
RGB-D Dataset for 6D Pose Estimation of 3D Objects,” in Proc. of in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern
the IEEE/CVF Conf. on Computer Vision and Pattern Recognition Recognition(CVPR),2022.
(CVPR),2019.
[32] Y. Shi, P. Wang, J. Ye, L. Mai, K. Li, and X. Yang, “MVDream:
[11] K. Lin and B. Yi, “Active View Planning for Radiance Fields,” in Multi-viewDiffusionfor3DGeneration,”inProc.oftheIntl.Conf.on
RoboticsScienceandSystems(RSS)WorkshoponImplicitRepresen- LearningRepresentations(ICLR),2024.
tationsforRoboticManipulation,2022.
[33] N.Su¨nderhauf,J.Abou-Chakra,andD.Miller,“Density-AwareNeRF
[12] M.Liu,R.Shi,L.Chen,Z.Zhang,C.Xu,X.Wei,H.Chen,C.Zeng, Ensembles: Quantifying Predictive Uncertainty in Neural Radiance
J. Gu, and H. Su, “One-2-3-45++: Fast Single Image to 3D Objects Fields,” in Proc. of the IEEE Intl. Conf. on Robotics & Automation
with Consistent Multi-View Generation and 3D Diffusion,” arXiv (ICRA),2023.
preprintarXiv:2311.07885,2023.
[34] Z. Wang, C. Lu, Y. Wang, F. Bao, C. Li, H. Su, and J. Zhu,
[13] M. Liu, C. Xu, H. Jin, L. Chen, M. Varma T, Z. Xu, and H. Su, “ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation
“One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without with Variational Score Distillation,” in Proc. of the Conf. on Neural
Per-ShapeOptimization,”inProc.oftheConf.onNeuralInformation InformationProcessingSystems(NeurIPS),2023.
ProcessingSystems(NeurIPS),2023.
[35] Z.Yang,Z.Ren,M.A.Bautista,Z.Zhang,Q.Shan,andQ.Huang,
[14] R. Liu, R. Wu, B. Van Hoorick, P. Tokmakov, S. Zakharov, and “FvOR: Robust Joint Shape and Pose Optimization for Few-View
C. Vondrick, “Zero-1-to-3: Zero-Shot One Image to 3D Object,” in ObjectReconstruction,”inProc.oftheIEEE/CVFConf.onComputer
Proc.oftheIEEE/CVFIntl.Conf.onComputerVision(ICCV),2023. VisionandPatternRecognition(CVPR),2022.
[15] Y. Liu, C. Lin, Z. Zeng, X. Long, L. Liu, T. Komura, and [36] T. Zaenker, J. Ru¨ckin, R. Menon, M. Popovic´, and M. Bennewitz,
W. Wang, “SyncDreamer: Generating Multiview-consistent Images “Graph-BasedViewMotionPlanningforFruitDetection,”inProc.of
from a Single-view Image,” in Proc. of the Intl. Conf. on Learning the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS),
Representations(ICLR),2024.
2023.
[16] X. Long, C. Lin, P. Wang, T. Komura, and W. Wang, “SparseNeuS: [37] R. Zeng, W. Zhao, and Y.-J. Liu, “PC-NBV: A Point Cloud Based
FastGeneralizableNeuralSurfaceReconstructionfromSparseViews,” DeepNetworkforEfficientNextBestViewPlanning,”inProc.ofthe
inProc.oftheEurop.Conf.onComputerVision(ECCV),2022. IEEE/RSJIntl.Conf.onIntelligentRobotsandSystems(IROS),2020.
[17] X. Long, Y.-C. Guo, C. Lin, Y. Liu, Z. Dou, L. Liu, Y. Ma, S.-
H. Zhang, M. Habermann, C. Theobalt et al., “Wonder3D: Sin-
gle Image to 3D Using Cross-Domain Diffusion,” arXiv preprint
arXiv:2310.15008,2023.