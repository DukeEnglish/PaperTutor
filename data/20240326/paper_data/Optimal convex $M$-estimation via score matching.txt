Optimal convex M-estimation via score matching
Oliver Y. Feng , , Yu-Chun Kao , Min Xu and Richard J. Samworth
∗‡ † † ‡
Department of Mathematical Sciences, University of Bath
∗
Department of Statistics, Rutgers University
†
Statistical Laboratory, University of Cambridge
‡
March 26, 2024
Abstract
Inthecontextoflinearregression,weconstructadata-drivenconvexlossfunctionwithrespectto
whichempiricalriskminimisationyieldsoptimalasymptoticvarianceinthedownstreamestimationof
the regression coefficients. Our semiparametric approach targets the best decreasing approximation
of the derivative of the log-density of the noise distribution. At the population level, this fitting
process is a nonparametric extension of score matching, corresponding to a log-concave projection
of the noise distribution with respect to the Fisher divergence. The procedure is computationally
efficient, and we prove that our procedure attains the minimal asymptotic covariance among all
convex M-estimators. As an example of a non-log-concave setting, for Cauchy errors, the optimal
convex loss function is Huber-like, and our procedure yields an asymptotic efficiency greater than
0.87 relative to the oracle maximum likelihood estimator of the regression coefficients that uses
knowledge of this error distribution; in this sense, we obtain robustness without sacrificing much
efficiency. Numerical experiments confirm the practical merits of our proposal.
1 Introduction
In linear models, the Gauss–Markov theorem is the primary justification for the use of ordinary least
squares(OLS)insettingswheretheGaussianityofourerrordistributionmaybeindoubt. Itstatesthat,
provided the errors have a finite second moment, OLS attains the minimal covariance among all linear
unbiased estimators; recent papers on this topic include Hansen (2022), P¨otscher and Preinerstorfer
(2022) and Lei and Wooldridge (2022). Nevertheless, it is now understood that biased, non-linear
estimators can achieve lower mean squared error than OLS when the noise distribution is appreciably
non-Gaussian (Stein, 1956b; Hoerl and Kennard, 1970; Zou and Yuan, 2008; Du¨mbgen et al., 2011).
However, it remains unclear how best to fit linear models in a computationally efficient and adaptive
fashion, i.e. without knowledge of the error distribution.
Consider a linear model where Y = X β +ε for i = 1,...,n. Recall that an M-estimator of
i i⊤ 0 i
β Rd based on a loss function ℓ: R R is defined as an empirical risk minimiser
0
∈ →
n
1 (cid:88)
βˆ argmin ℓ(Y X β), (1)
∈ β ∈Rd n i=1
i
−
i⊤
provided that this exists. If ℓ is differentiable on R with negative derivative ψ = ℓ, then βˆ βˆ solves
′ ψ
− ≡
the corresponding estimating equations
n
1 (cid:88)
X ψ(Y X βˆ )=0 (2)
n
i i
−
i⊤ ψ
i=1
andisreferredtoasaZ-estimator. Westudyarandomdesignsettinginwhich(X ,Y ),...,(X ,Y )are
1 1 n n
independentandidenticallydistributed,withX ,...,X beingRd-valuedcovariatesthatareindependent
1 n
of real-valued errors ε ,...,ε with density p . Suppose further that E X ψ(ε ) = 0. This means
1 n 0 1 1
that βˆ is Fisher consistent in the sense that the population analogue o{ f (2) is s} atisfied by the true
ψ
1
4202
raM
52
]TS.htam[
1v88661.3042:viXraparameter β , i.e. E X ψ(Y X β ) = 0. Under suitable regularity conditions, including ψ being
differentiable0
and
E({
X
1
X
)1
−Rd
1⊤
d
b0
ei}
ng invertible, we have
1 1⊤
∈
×
√n(βˆ β ) d N (cid:0) 0,V (ψ) E(X X ) 1(cid:1) as n , where V (ψ):= Eψ2(ε 1) (3)
ψ − 0 → d p0 ·{ 1 1⊤ }− →∞ p0 Eψ (ε ) 2
{ ′ 1 }
(e.g. van der Vaart, 1998, Theorems 5.21, 5.23 and 5.41). Since the covariates and errors are assumed
to be independent, they contribute separately to the limiting covariance above (a special case of the
‘sandwich’formula(Huber, 1967;YoungandShah,2023)): the matrixE(X X ) 1 dependsonlyonthe
1 1⊤ −
covariate distribution, whereas the scalar V (ψ) depends on the loss function ℓ (through ψ = ℓ) and
p0
−
′
on the error distribution.
If the errors ε ,ε ,... have a known absolutely continuous density p on R, then we can define the
1 2 0
maximum likelihood estimator βˆMLE by taking ℓ = logp in (1). In this case, ψ = ℓ is the score
0 ′
function (for location)1 ψ := (p /p )1 . Und− er appropriate regularity condition− s (e.g. van der
Vaart, 1998, Theorem
5.390
),
inclu′0 din0
g t{
hp a0> t0
t} he Fisher information (for location) i(p ) := (cid:82) ψ2p =
0 R 0 0
(cid:82)
(p )2/p is finite, we have
{p0>0
}
′0 0
(cid:18) E(X X ) 1(cid:19)
√n(βˆMLE β ) d N 0,{ 1 1⊤ }− (4)
0 d
− → i(p )
0
as n . The limiting covariance E(X X ) 1/i(p ) constitutes an efficiency lower bound; see
→ ∞ {
1 1⊤ }− 0
Remark 15 below. In fact, it can be seen directly that 1/i(p ) is the smallest possible value of the
0
asymptotic variance factor V (ψ) in the limiting covariance of √n(βˆ β ) in (3). Indeed, by the
p0 ψ
−
0
Cauchy–Schwarz inequality,
(cid:82) (cid:82)
ψ2p ψ2p 1 1
R 0 R 0
V p0(ψ)= (cid:0)(cid:82) Rψ ′p 0(cid:1)2 = (cid:0)(cid:82) Rψp ′0(cid:1)2 ≥ (cid:82) {p0>0 }(p ′0)2/p 0 = i(p 0) ∈(0, ∞), (5)
whenever the integration by parts in the second step is justified, and equality holds if and only if there
existsλ=0suchthatψ(ε )=λψ (ε )almostsurely. Thisleadstoanequivalentvariationaldefinitionof
1 0 1
̸
theFisherinformation;seeHuberandRonchetti(2009,Theorem4.2),whichwerestateasProposition36
inSection6.4. Thus,when(4)holds,βˆMLEhasminimalasymptoticcovarianceamongallZ-estimatorsβˆ
ψ
for which (3) is valid, with the score function ψ being the optimal choice of ψ.
0
Our goal in this work is to choose ψ in a data-driven manner, such that the corresponding loss
function ℓ in (1) is convex, and such that the scale factor V (ψ) in the asymptotic covariance (3) of
p0
the downstream estimator of β is minimised. Convexity is a particularly convenient property for a loss
0
function, since for the purpose of M-estimation, it leads to more tractable theory and computation.
Indeed, the empirical risk in (1) becomes convex in β, so its local minimisers are global minimisers. In
particular, when ℓ is also differentiable, βˆ is a Z-estimator satisfying (2) if and only if it is an M-
ψ
estimatorsatisfying (1). Theexistence, uniquenessand√n-consistencyofβˆ arethenguaranteedunder
ψ
milder conditions on ℓ than for generic loss functions (Yohai and Maronna, 1979; Maronna and Yohai,
1981; Portnoy, 1985; Mammen, 1989; Arcones, 1998; He and Shao, 2000). Furthermore, an important
practical advantage is that we can compute βˆ efficiently using convex optimisation algorithms with
ψ
guaranteed convergence (Boyd and Vandenberghe, 2004, Chapter 9).
In view of the discussion above, our first main contribution in Section 2 is to determine the optimal
population-level convex loss function in the sense described in the previous paragraph. For a uniformly
continuous error density p , this amounts to finding
0
ψ argmin V (ψ), (6)
0∗
∈
p0
ψ ∈Ψ↓(p0)
(cid:82)
where Ψ (p ) denotes the set of decreasing, right-continuous functions ψ satisfying ψ2p < . We
0 R 0
↓ ∞
will actually define the ratio V (ψ) in a slightly more general way than in (5) to allow us to handle
p0
non-differentiable functions ψ. This turns out to be convenient because, for instance, the robust Huber
loss ℓ given by
K
(cid:40)
z2/2 if z K
ℓ (z):= | |≤ (7)
K K z K2/2 if z >K
| |− | |
1Thescoreisusuallydefinedasafunctionofaparameterθ Rasthederivativeofthelog-likelihood;thelinkwithour
terminologycomesfromconsideringthelocationmodel p0( +∈ θ):θ R ,andevaluatingthescoreattheorigin.
{ · ∈ }
2forK (0, )hasanon-differentiablenegativederivativeψ := ℓ satisfyingψ (z)=( K) ( z)
K
for∈
z
R∞
.
K
−
′K K
− ∨ − ∧
∈
In Section 2.1, we show that minimising V () over Ψ (p ) is equivalent to minimising the score
p0
· ↓
0
matching objective
D (ψ):=E(cid:8) ψ2(ε )+2ψ (ε )(cid:9) , (8)
p0 1 ′ 1
over ψ Ψ (p ), provided that we take appropriate care in defining this expression when ψ is not
0
∈ ↓
absolutelycontinuous. Thisobservationallowsustoobtainanexplicitcharacterisationofthe‘projected’
score function ψ in terms of p and its distribution function F . Indeed, to obtain ψ at z R, we can
0∗ 0 0 0∗
∈
first consider p
0
◦F 0−1 (whose domain is [0,1]), then compute the right derivative of its least concave
majorant, before finally applying the resulting function to F (z). The negative antiderivative ℓ of ψ
0 ∗0 0∗
is then the optimal convex loss function we seek. An important property is that Eψ (Y X β ) = 0,
which ensures that ℓ ∗0 correctly identifies the estimand β 0 on the population level;
e0∗ qui1
va−
lent1⊤ ly,0
βˆ ψ 0∗ is
Fisher consistent.
10 5 0 5 10 10 5 0 5 10 4 2 0 2 4
− − − − − −
z z z
10 5 0 5 10 10 5 0 5 10 4 2 0 2 4
− − − − − −
z z z
Figure 1: Top row: Plots of the score function ψ (green) and projected score function ψ (blue);
0 0∗
Bottom row: their respective negative antiderivatives, namely the negative log-density logp (green)
0
−
and optimal convex loss function ℓ (blue), for each of the following non-log-concave distributions (from
∗0
left to right): (a) Student’st ; (b) symmetrisedPareto(66)withσ =2andα=3; (c) Gaussianmixture
2
0.4N( 2,1)+0.6N(2,1).
−
NotethatβˆMLE isaconvexM-estimatorifandonlyifℓ= logp isconvex,i.e.p islog-concave,in
0 0
−
whichcaseψ =ψ by(5). Wewillbeespeciallyinterestedinerrordensitiesp thatarenotlog-concave,
0∗ 0 0
for which the efficiency lower bound in (5) cannot be achieved by a convex M-estimator corresponding
to a decreasing function ψ. We interpret the minimum ratio V (ψ ) as an analogue of the inverse
p0 0∗
Fisher information, serving as the crucial part of the efficiency lower bound for convex M-estimators.
To reinforce the link with score matching, we will see in Section 2.2 that the density proportional to
e
−ℓ∗
0 is the best log-concave approximation to p 0 with respect to the Fisher divergence defined formally
in (20) below. This is typically different from the well-studied log-concave projection with respect to
Kullback–Leiblerdivergence, andindeedthelattermayyieldconsiderablysuboptimalcovarianceforthe
resulting convex M-estimator; see Proposition 6. In concrete examples where p has heavy tails (e.g. a
0
Cauchy density) or is multimodal (e.g. a mixture density), we compute closed-form expressions for the
projected score function and the optimal convex loss function in Section 2.3. In particular, ℓ turns
∗0
out to be a robust Huber-like loss function in the Cauchy case. More generally, when the errors are
heavy-tailed in the sense that their (two-sided) hazard function is bounded, Lemma 4 shows that the
projected score function ψ is bounded, in which case the corresponding convex loss ℓ grows at most
0∗ ∗0
linearly in the tails and hence is robust to outliers. A major advantage of our framework over the use
of the Huber loss is that it does not require the choice of a transition point K (see (7) above) between
quadratic and linear regimes (which in a regression context amounts to a choice of scale for the error
distribution). Infact,theantitonicscoreprojection,andhencetheFisherdivergenceprojection,isaffine
3
1
5.0
0
5.0 −
1 −
01
8
6
4
2
2
1
0
1
2
51
01
5
0
−
−
2
1
0
1
2
5
4
3
2
−
−equivariant (Remark 8), which reflects the fact that we optimise V () in (6) over a class Ψ (p ) that is
p0
· ↓
0
closed under multiplication by non-negative scalars.
InSection3,weturnourattentiontoalinearregressionsettingwheretheerrordensityp isunknown.
0
To ensure that β is identifiable, we assume either that p is symmetric (Section 3.1) or that the model
0 0
contains an explicit intercept term (Section 3.2). We aim to construct a semiparametric M-estimator of
β thatachievesminimalcovarianceamongallconvexM-estimators, butsincep andhencetheoptimal
0 0
loss function ℓ are unknown, we seek to estimate β and ψ simultaneously. We first show that on the
∗0 0 0∗
population level, the pair (β ,ψ ) solves a joint score matching problem subject to a Fisher consistency
0 0∗
constraint; see (28) and (29). This motivates an alternating optimisation procedure where we start with
an arbitrary initialiser β¯ , and compute a kernel density estimate of the error distribution based on
n
the residuals. We can then apply the linear-time Pool Adjacent Violators Algorithm (PAVA) to obtain
the projected score function of the density estimate, before minimising its negative antiderivative using
Newton optimisation techniques to yield an updated estimator. This process could then be iterated
to convergence, but if we initialise with a √n-consistent pilot estimator β¯ , then one iteration of the
n
alternating algorithm above suffices for our theoretical guarantees, and moreover it ensures that the
procedure is computationally efficient. We prove that a three-fold cross-fitting version of our algorithm
(with the different steps computed on different folds) yields an estimator βˆ that is √n-consistent and
n
asymptotically normal, with limiting covariance attaining our efficiency lower bound for convex M-
estimators. Consistent estimation of the information quantity 1/V (ψ ) is straightforward using our
p0 0∗
nonparametricscorematchingprocedure,socombiningthiswithourasymptoticdistributionalresultfor
βˆ , we can then perform inference for β (Section 3.3).
n 0
Section 4 is devoted to a numerical study of the empirical performance and computational efficiency
of our antitonic score matching estimator. These corroborate our theoretical findings: our proposed
approachachievessmallerestimationerror(sometimesdramaticallysmaller)comparedwithalternatives
such as OLS, the least absolute deviation (LAD) estimator, a semiparametric one-step estimator, and
a semiparametric M-estimator based on the log-concave MLE of the noise distribution. Moreover,
the corresponding confidence sets for β are smaller, while retaining nominal coverage. Finally, we
0
perform a runtime analysis to show that the improved statistical performance comes without sacrificing
computational scalability.
The proofs of all results in Sections 2 and Section 3 are given in the appendix in Sections 6.1 and 6.3
respectively. The appendix (Section 6) also contains additional examples for Section 2 (Section 6.2) and
auxiliary results (Sections 6.4 and 6.5).
1.1 Related work
Score matching (Hyv¨arinen, 2005; Lyu, 2012) is an estimation method designed for statistical models
where the likelihood is only known up to a normalisation constant (e.g. a partition function) that may
be infeasible to compute; see the recent tutorial by Song and Kingma (2021) on ‘energy-based’ mod-
els. Instead of maximising an approximation to the likelihood, score matching circumvents this issue
altogether by estimating the derivative of a log-density, i.e. the score function. More precisely, given a
differentiable density p on Rd with score function ψ :=( p /p )1 , the population version of the
0 0
∇
0 0 {p0>0
}
procedure aims to minimise
E (cid:0) ψ(ε) ψ (ε) 2(cid:1) (9)
p0
∥ −
0
∥
over a suitable class Ψ of differentiable functions ψ (ψ ,...,ψ ): Rd Rd, where ε p . Hyv¨arinen
1 d 0
≡ → ∼
(2005) used integration by parts to show that it is equivalent to minimise
D (ψ):=E(cid:8) ψ(ε) 2+2( ψ)(ε)(cid:9) (10)
p0
∥ ∥ ∇·
over ψ Ψ, where ψ :=
(cid:80)d
∂ψ /∂x . The score matching estimator based on data ε ,...,ε
∈ ∇· j=1 j j 1 n
in Rd is then defined as a minimiser of the empirical analogue Dˆ (ψ) := n 1(cid:80)n ψ(ε ) 2 +2(
n − i=1{∥ i ∥ ∇·
ψ)(ε ) over ψ Ψ; see also Cox (1985). Such estimators are important in the context of Langevin
i
} ∈
Monte Carlo (Parisi, 1981; Roberts and Tweedie, 1996; Betancourt et al., 2017; Cheng et al., 2018) and
diffusion models (Li et al., 2023). The appearance of the score function in the underlying (reverse-time)
stochastic differential equations can be related to Tweedie’s formula, which underpins empirical Bayes
denoising (Efron, 2011; Derenski et al., 2023).
4Likelihood maximisation corresponds to distributional approximation with respect to the Kullback–
Leiblerdivergence; ontheotherhand, scorematchingseekstominimisetheFisher divergence (Johnson,
2004, Section 1.3) from a class of densities to the target p , in view of the equivalence between the opti-
0
misationobjectives(9)and(10);see(18)below.Sriperumbuduretal.(2017)studiedinfinite-dimensional
exponential families indexed by reproducing kernel Hilbert spaces, and proposed and analysed a density
estimatorthatminimisesapenalisedempiricalFisherdivergence. Koehleretal.(2022)usedisoperimet-
ric inequalities to investigate the statistical efficiency of score matching relative to maximum likelihood,
thereby quantifying the effect of eliminating normalisation factors. Lyu (2012) observed that Fisher di-
vergence and Kullback–Leibler divergence are related by an analogue of de Bruijn’s identity (Johnson,
2004, AppendixC;CoverandThomas,2006, Section17.7), whichlinksFisherinformationandShannon
entropy. From an information-theoretic perspective, Johnson and Barron (2004) proved central limit
theorems that establish convergence in Fisher divergence to a limiting Gaussian distribution. Ley and
Swan (2013) extended Stein’s method to derive information inequalities that bound a variety of integral
probability distances in terms of the Fisher divergence.
Scorematchinghasbeengeneralisedindifferentdirectionsandappliedtoavarietyofstatisticalprob-
lems including graphical modelling (e.g. Hyv¨arinen, 2007; Vincent, 2011; Lyu, 2012; Mardia et al., 2016;
Songetal.,2020;Yuetal.,2020,2022;LedererandOesting,2023;Bentonetal.,2024),whereitexhibits
excellent empirical performance while being computationally superior to full likelihood approaches. In
particular, score-based algorithms for generative modelling, via Langevin dynamics (Song and Ermon,
2019)anddiffusionmodels(Songetal.,2021),haveachievedremarkablesuccessinmachinelearningtasks
such as the reconstruction, inpainting and artificial generation of images; see e.g. Jolicoeur-Martineau
et al. (2020), De Bortoli et al. (2022) and many other references therein. In these applications, score
matching is applied to a class of functions parametrised by the weights of a deep neural network. On
the other hand, different statistical considerations lead us to develop a nonparametric extension of score
matchinginSection2,whichweusetoconstructdata-drivenconvexlossfunctionsforefficientsemipara-
metricestimation. WeseethatitisbyminimisingtheFisherdivergenceinsteadoftheKullback–Leibler
divergence to the error distribution that one obtains a convex M-estimator with minimal asymptotic
variance.
TheframeworkinSection3.1includesasaspecialcasetheclassicallocationmodelinwhichweobserve
Y = θ +ε for i = 1,...,n, where θ R is the parameter of interest and ε ,...,ε are independent
i 0 i 0 1 n
∈
errors with an unknown density p that is symmetric about 0. Starting from the seminal paper of Stein
0
(1956a), a series of works (e.g. van Eeden, 1970; Stone, 1975; Beran, 1978; Bickel, 1982; Schick, 1986;
Faraway, 1992; Dalalyan et al., 2006; Gupta et al., 2023) showed that adaptive, asymptotically efficient
estimators of θ can be constructed; see also Doss and Wellner (2019) and Laha (2021) for approaches
0
based on the further assumption that p is log-concave. Many of these traditional semiparametric
0
procedures have drawbacks that limit their practical utility. In particular, the estimated likelihood may
havemultiplelocaloptimaanditmaybedifficulttoguaranteeconvergenceofanoptimisationalgorithm
to a global maximum (van der Vaart, 1998, Example 5.50). This is one of the reasons why prior works
often study a one-step estimator resulting from a single iteration of Newton’s method (Bickel, 1975; Jin,
1990; Mammen and Park, 1997; Laha, 2021), rather than full likelihood maximisation, though finite-
sample performance may remain poor and sensitive to tuning (see Section 4). By contrast, our focus
is not on classical semiparametric adaptive efficiency per se; instead, we directly study the theoretical
propertiesofaminimiseroftheempiricalriskwithrespecttoanestimatedlossfunction,whoseconvexity
ensures that the estimator can computed efficiently by iterating gradient descent or Newton’s method
to convergence.
Recently, Kao et al. (2023) constructed a location M-estimator that can adaptively attain rates of
convergence faster than n 1/2 when the symmetric error density is compactly supported and suitably
−
irregular (e.g. discontinuous at the boundary of its support). They considered ℓq-location estimators
tθˆ oq s:= elea cr tg am nin exθ ∈pR on(cid:80) enn i= t1 qˆ|Y i [− 2,θ |q )b thas ae td mo in nimun isiv ea sr aia pte roo xb yse forv ra tt hio en as syY m1, p. t. o. t, iY cn v, aa rin ad ncu es oed
f
θˆLe .p Tsk hi e’s rem suet lth io nd
g
q
estimator θˆ is shown ∈ to be∞ minimax optimal up to poly-logarithmic factors, and the procedure is
qˆ
extended to linear regression models with unknown symmetric errors. By comparison with Kao et al.
(2023), we study ‘regular’ regression models where the Fisher information is finite and minimax rates
faster than n 1/2 are impossible to achieve. We aim to minimise the asymptotic variance as an end in
−
itself,overtheentirenonparametricclassofconvexlossfunctionsratherthanℓq-lossfunctionsspecifically.
5We finally mention the connections between our work and robust statistics, which deals with heavy-
tailed noise distributions and data that may be contaminated by random or adversarial outliers. As
mentionedpreviously,robustlossfunctionsaredesignedtobetoleranttosuchdatacorruption;examples
include the Huber loss (7), a two-parameter family of loss functions considered by Barron (2019), and
theantiderivativesofCatoni’sinfluencefunctions(Catoni,2012). TheHuberlossfunctionsℓ originally
K
arose as solutions to the following minimax asymptotic variance problem: for every ϵ (0,1), there
∈
exists a unique K K >0 such that
ϵ
≡
(cid:82)
ψ2dP
R
ψ K ≡−ℓ ′K =ar ψg ∈m Ψin
P
∈Ps ϵu syp
m(Φ)
(cid:124)(cid:82)
Rψ (cid:123)(cid:122)′dP
(cid:125),
=:VP(ψ)
whereΨconsistsofall‘sufficientlyregular’ψ: R R,andthesymmetricϵ-contaminationneighbourhood
→
sym(Φ) contains all univariate distributions of the form P = (1 ϵ)N(0,1)+ϵQ for some symmetric
Pϵ −
distribution Q. The pioneering paper of Huber (1964) also developed variational theory for minimising
sup V () more generally when is a convex class of distributions, such as an ϵ-contamination or a
KolP m∈oPgorP ov· neighbourhoodofasymP
metriclog-concavedensity(HuberandRonchetti,2009,Section4.5).
See Donoho and Montanari (2015) for a high-dimensional extension of this line of work. An alternative
to the Huber loss that seeks robustness without serious efficiency loss relative to OLS is the composite
quantile regression (CQR) estimator of Zou and Yuan (2008); in fact, our approach is always at least as
efficient as CQR (see Lemma 17). Other recent papers on robust convex M-estimation include Chinot
etal.(2020)andBrunel(2023);seealsothenotesonrobuststatisticallearningtheorybyLerasle(2019).
More closely related to our optimisation problem (6) is the work of Hampel (1974) on optimal B-
robust estimators,whichhaveminimalasymptoticvariancesubjecttoanupperboundonthegrosserror
sensitivity (Hampel et al., 2011, Section 2.4). In our linear regression setting with ε p , this amounts
1 0
∼
to
(cid:90)
minimising V (ψ) over all ‘regular’ ψ such that ψp =0 and sup ψ(z) b (11)
p0
R
0
z R| |≤
∈
forsomesuitableb>0(vanderVaart,1998,Example5.29;Hampeletal.,2011,p.121andSection2.5d).
In particular, when p is a standard Gaussian density, ψ is again optimal for some K K > 0 that
0 K b
depends non-linearly on b. By contrast with (6) however, the Fisher consistency conditi≡ on Eψ(ε ) = 0
1
must be explicitly included as a constraint in (11), and moreover the L bound on ψ means that the
∞
set of feasible ψ is not closed under non-negative scalar multiplication. Consequently, the resulting
optimal location M-estimators are generally not scale invariant (Hampel et al., 2011, p. 105). In robust
regression, adaptive selection of scale parameters is a non-trivial problem (e.g. van der Vaart, 1998,
Section 5.4; Huber and Ronchetti, 2009, Section 7.7; Loh, 2021); see also Figure 4 below. Finally, we
mention that in a proportional asymptotic regime where n/d κ (1, ) for a sequence of linear
→ ∈ ∞
models with log-concave errors and independent Gaussian covariates, Bean et al. (2013) derived the
(unpenalised)convexM-estimatorwithminimalexpectedout-of-samplepredictionerror. Inthissetting,
the optimisation objective is no longer V (ψ) but instead the solution to a pair of non-linear equations
p0
involving the proximal operator of the convex loss function (El Karoui et al., 2013).
1.2 Notation
Throughout this paper, we will adopt the convention 0/0:=0 and write [n]:= 1,...,n for n N. For
a forfu an lc lt zion Rf:
,
R an→
d
aR n, til se yt m∥ mf ∥ et∞ ric:= (is .eu .p oz ∈ddR )|f i( fz f) (|. z)R =ecall ft (ha zt )f foi rs as ly lm zmet Rri .c F( oi. r{ e. anev oe pn e) n} if sf et(z U)∈ =f R( ,− wz e)
say that∈ f: U R is locally absolutely continuous on− U i− f it is absolut∈ ely continuous on every c⊆ ompact
interval I U.→ Equivalently, there exists a measurable function g: U R such that for every compact
subinterva⊆
l I U, we have
(cid:82)
g < and f(z ) = f(z
)+(cid:82)z2g for→
all z ,z I. In this case, f is
⊆ I| | ∞ 2 1 z1 1 2 ∈
differentiable Lebesgue almost everywhere on U, with f =g almost everywhere.
′
Given a Borel probability measure P on R, we write L2(P) for the set of all Lebesgue measurable
functions f on R such that f := (cid:0)(cid:82) f2dP(cid:1)1/2 < . Denote by f,g := (cid:82) fgdP the
L2(P) R L2(P) R
∥ ∥ ∞ ⟨ ⟩
L2(P)-inner product of f,g L2(P). For measures µ,ν on a general measurable space ( , ), we say
∈ X A
that µ is absolutely continuous with respect to ν, and write µ ν, if µ(A) = 0 whenever ν(A) = 0 for
≪
A . The notation µ ν indicates that µ is not absolutely continuous with respect to ν.
∈A ≪̸
6For a function F: [0,1] R, we write Fˆ for its least concave majorant on [0,1]. Denote by F(L)(u)
→
andF(R)(u)respectivelytheleftandrightderivativesofF atu [0,1], wheneverthesearewell-defined.
Given an integrable function f: (0,1) R with antiderivative∈ F: [0,1] R given by F(u) := (cid:82)u f,
→ → 0
define (cid:99)Rf: [0,1] [ , ] by
M → −∞ ∞
(cid:40)
Fˆ(R)(u) for u [0,1)
( M(cid:99)Rf)(u):=
Fˆ(L)(1) for
u∈
=1,
so that ((cid:99)Rf)(1) = lim
u
1((cid:99)Rf)(u) by Rockafellar (1997, Theorem 24.1). Furthermore, define
M ↗ M
(cid:99)Lf: [0,1] [ , ] by ((cid:99)Lf)(u) := ((cid:99)Rg)(1 u) for u [0,1], where g(u) := f(1 u) for
M → −∞ ∞ M M − ∈ −
all such u.
2 The antitonic score projection
2.1 Construction and basic properties
The aim of this section is to define formally and solve the optimisation problem (6) that yields the
minimal asymptotic covariance of the regression M-estimator in (3). Let P be a probability measure
0
on R with a uniformly continuous density p , which necessarily satisfies p ( ):=lim p (z)=0.
0 0 z 0
Letting suppp := z R : p (z) > 0 , define (p ) := (cid:0) inf(supp± p∞ ),sup(supp→ p±∞ )(cid:1) , which is
0 0 0 0 0 0
{ ∈ } S ≡ S
the smallest open interval that contains suppp . We write Ψ (p ) for the set of all ψ L2(P ) that
0 0 0
↓ ∈
are decreasing and right-continuous. Observe that Ψ (p ) is a convex cone, i.e. c ψ +c ψ Ψ (p )
0 1 1 2 2 0
↓ ∈ ↓
whenever ψ ,ψ Ψ (p ) and c ,c 0. Moreover, every ψ Ψ (p ) is necessarily finite-valued on ,
1 2 0 1 2 0 0
∈ ↓ ≥ (cid:82) ∈ ↓ S
so the corresponding Lebesgue–Stieltjes integral p dψ [ ,0] is well-defined.
0
For ψ Ψ (p ) with
(cid:82)
ψ2dP >0, let
S0 ∈ −∞
0 R 0
∈ ↓
(cid:82)
ψ2dP
R 0
V (ψ):= [0, ], (12)
p0 (cid:0)(cid:82) p dψ(cid:1)2 ∈ ∞
0
S0
where we have modified the denominator in (5) to extend the original definition of the asymptotic
variance factor to non-differentiable functions in Ψ (p ) such as z sgn(z). As a first step towards
0
↓ (cid:55)→ −
minimising V (ψ) over ψ Ψ (p ), note that V (cψ)= V (ψ) for every c> 0, so any minimiser is at
p0
∈ ↓
0 p0 p0
best unique up to a positive scalar. Ignoring unimportant edge cases where the denominator in (12) is
zero or infinity, our optimisation problem can therefore be formulated as a constrained minimisation of
the numerator in (12) subject to the denominator being equal to 1. This motivates the definition of the
Lagrangian
(cid:90) (cid:90)
D (ψ,λ):= ψ2dP +2λ p dψ [ , ) (13)
p0
R
0 0
∈ −∞ ∞
S0
forψ Ψ (p )andλ 0. Ifψ islocallyabsolutelycontinuouson withderivativeψ Lebesguealmost
0 0 ′
∈ ↓ ≥ S
everywhere, then
(cid:90) (cid:90) (cid:90)
D (ψ,λ)= ψ2dP +2λ ψ p = (ψ2+2λψ )dP =E(cid:0) ψ2(ε )+2λψ (ε )(cid:1) (14)
p0 0 ′ 0 ′ 0 1 ′ 1
R R
S0
when ε P , which we recognise as the score matching objective (8) in the introduction when λ=1.
1 0
∼ (cid:82)
The formal link between V () and D ( ,λ) is that for ψ Ψ (p ) with ψ2dP > 0, we have
(cid:82)
p0
·
p0
· ∈ ↓
0 R 0
p dψ 0 and cψ Ψ (p ) for all c 0, so
0 0
S0 ≤ ∈ ↓ ≥
ci ≥nf 0D p0(cψ,λ)= ci ≥nf 0(cid:16) c2(cid:90) Rψ2dP 0+2cλ(cid:90) S0p 0dψ(cid:17) = −λ2(cid:0) (cid:82)(cid:82) RS ψ0 2p 0 dPdψ 0(cid:1)2 =
−V
pλ 0(2
ψ)
(15)
for every λ 0. Thus, minimising V () over Ψ (p ) is equivalent to minimising D ( ,λ) up to a
≥
p0
· ↓
0 p0
·
scalar multiple, but D (,λ) is a convex function that is more tractable than V (). Both Cox (1985,
p0
·
p0
·
Proposition 1) and Theorem 2 below indicate that λ=1 is the ‘canonical’ choice of Lagrange multiplier
in (13), so we define D (ψ):=D (ψ,1) for ψ Ψ (p ).
p0 p0
∈ ↓
0
By exploiting this connection with score matching together with ideas from monotone function esti-
mation,weproveinTheorem2belowthatthesolutiontoourasymptoticvarianceminimisationproblem
is the function ψ that we construct explicitly in the following lemma.
0∗
7Lemma 1. Let P be a distribution with a uniformly continuous density p on R. Let F : [ , ]
0 0 0
−∞ ∞ →
[0,1] be the corresponding distribution function, and for u [0,1], define
∈
F 0−1(u):=inf {z ∈[ −∞, ∞]:F 0(z) ≥u
}
and J 0(u):=(p
0
◦F 0−1)(u).
Then both J and its least concave majorant Jˆ on [0,1] are continuous, with p =J F on R, and
0 0 0 0 0
◦
ψ :=Jˆ(R) F
0∗ 0 ◦ 0
is decreasing and right-continuous as a function from R to [ , ], provided that we set Jˆ(R)(1) :=
−∞ ∞ 0
lim Jˆ(R)(u). Moreover, ψ (z) R if and only if z .
u ↗1 0 0∗ ∈ ∈S0
We refer to J as the density quantile function (Parzen, 1979; Jones, 1992). In the case where p is
0 0
a standard Cauchy density, Figure 2 presents a visualisation of J and its least concave majorant Jˆ, as
0 0
well as the corresponding score functions ψ =p /p and ψ .
0 ′0 0 0∗
J0(u) ψ0(z)
Jˆ 0(u) ψ 0∗(z)
0 0.2 0.4 0.6 0.8 1 6 4 2 0 2 4 6
− − −
u z
Figure2: Left: ThedensityquantilefunctionJ anditsleastconcavemajorantJˆ forastandardCauchy
0 0
density. Right: The corresponding score functions ψ and ψ .
0 0∗
Theorem 2. In the setting of Lemma 1, the following statements hold.
(cid:82)
(a) ψ dP =0.
R 0∗ 0
(cid:82)
(b) Let i (p ):= (ψ )2dP . Then inf D (ψ,λ)= λ2i (p ) for every λ>0.
∗ 0 R 0∗ 0 ψ ∈Ψ↓(p0) p0
−
∗ 0
(c) Suppose that i (p ) < . Then for each λ > 0, the function λψ is the unique minimiser of
∗ 0
∞ (cid:82)
0∗
D ( ,λ) over Ψ (p ). Moreover, for every ψ Ψ (p ) such that ψ2dP >0, we have
p0
· ↓
0
∈ ↓
0 R 0
1
V (ψ) V (ψ )= (0, ), (16)
p0
≥
p0 0∗
i (p ) ∈ ∞
∗ 0
with equality if and only if ψ =λψ for some λ>0.
0∗
(d) Assumefurtherthatp isabsolutelycontinuousonRwithderivativep Lebesguealmosteverywhere,
0 ′0
(cid:82)
corresponding score function2 ψ :=p /p and Fisher information i(p ):= ψ2p . Then
0 ′0 0 0 R 0 0
ψ
0∗
= M(cid:99)R(ψ
0
◦F 0−1) ◦F
0
(17)
and 0 < i (p ) i(p ), with equality if and only if p is log-concave. In particular, if i(p ) < ,
∗ 0 0 0 0
≤ ∞
then the conclusions of (c) hold.
Some remarks are in order here. As mentioned in the introduction, Theorem 2(a) ensures the Fisher
consistency of the regression Z-estimator βˆ ψ∗ defined in (2). This reflects the fact that ψ+c Ψ (p 0)
0 ∈ ↓
2Ourconvention0/0=0meansthatψ0=(p′ 0/p0)1 {p0>0}.
8
3.0
52.0
2.0
51.0
1.0
50.0
0
1
5.0
0
5.0
1
−
−whenever ψ Ψ (p ) and c R; see (46) and the first-order stationarity condition (47). Moreover, (53)
0
∈ ↓ ∈
in the proof of Theorem 2(d) shows that
(cid:90) (cid:90)
D (ψ,λ)= (ψ λψ )2dP (λψ )2dP = ψ λψ 2 λ2i(p )
p0 R − 0 0 − R 0 0 ∥ − 0 ∥L2(P0)− 0
for all ψ Ψ (p ), so if i(p )< , then Theorem 2(c) ensures that
0 0
∈ ↓ ∞
λψ argmin D (ψ,λ)= argmin ψ λψ 2 . (18)
0∗ ∈ p0 ∥ − 0 ∥L2(P0)
ψ ∈Ψ↓(p0) ψ ∈Ψ↓(p0)
Thus, in the terminology of Section 6.5, λψ is a version of the L2(P )-antitonic3 projection of λψ onto
0∗ 0 0
Ψ (p ). Indeed, the explicit representation (17) of ψ as a ‘monotonisation’ of ψ (see the right panel
0 0∗ 0
of↓ Figure 2) is consistent with that given in Proposition 43 for a general L2(P)-antitonic projection,
where P is a univariate probability measure with a continuous distribution function.
(cid:82) (cid:82)
When i(p ) < , the inequality i (p ) = (ψ )2dP ψ2dP = i(p ) in Theorem 2(d) follows
0 ∞ ∗ 0 R 0∗ 0 ≤ R 0 0 0
from the fact that the L2(P )-antitonic projection onto the convex cone Ψ (p ) is 1-Lipschitz with
0 0
↓
respect to ; see (115) in Lemma 44. A statistical explanation of this information inequality
∥·∥L2(P0)
arises from the fact that the 1/i(p ) is the infimum of the asymptotic variance functional V (ψ) in (5)
0 p0
overallsufficientlyregularψ: R R;seeHuberandRonchetti(2009,Theorem4.2),whichwerestateas
→
Proposition 36. On the other hand, by (16), 1/i (p ) is the minimum value of V () over the restricted
∗ 0 p0
·
class Ψ (p ), so in view of our discussion in the introduction, it can be interpreted as an information
0
↓
lower bound for convex M-estimators. When i (p )< , the ratio
∗ 0
∞
i (p )
∗ 0
ARE∗(p 0):=
i(p )
0
thereforequantifiesthepricewepayinstatisticalefficiencyforinsistingthatourlossfunctionbeconvex,
and will be referred to as the antitonic relative efficiency. By Theorem 2(d), ARE∗(p 0) 1 with
≤
equality if and only if p
0
is log-concave, so we can regard 1 ARE∗(p 0) as a measure of departure
−
from log-concavity; see Section 2.2 below. Example 11 shows that ARE∗(p 0) 0.878 when p
0
is the
≈
Cauchy density, whereas Example 25 in the appendix yields a density p
0
for which ARE∗(p 0)=0. More
generally, in Lemma 7 below, we provide a simple lower bound on ARE∗(p 0) that is reasonably tight for
heavy-tailed densities p .
0
Whenp isonlyuniformlycontinuous(andnotabsolutelycontinuous), thescorefunctionandFisher
0
information cannot be defined as above, but we nevertheless refer to ψ and i (p ) as the antitonic
0∗ ∗ 0
projected score function (see Lemma 5 below) and antitonic information (for location) respectively.
Remark 3. Since F
0
is continuous, we have (F
0
◦F 0−1)(u) = u for all u
∈
(0,1). The concave func-
tion Jˆ
0
is therefore absolutely continuous on [0,1] with derivative Jˆ 0(R) = ψ
0∗
◦F 0−1 Lebesgue almost
everywhere (Rockafellar, 1997, Corollary 24.2.1), so
(cid:90) (cid:90) 1 (cid:90) 1
i ∗(p 0)= R(ψ 0∗)2dP
0
=
0
(ψ
0∗
◦F 0−1)2 =
0
(cid:0) Jˆ 0(R)(cid:1)2 .
In the setting of Theorem 2(d) above, a straightforward calculation (cf. (52) in the proof of Theorem 2)
shows that the density quantile function J
0
=p
0
◦F 0−1 is absolutely continuous on [0,1] with derivative
J
0′
= ψ
0
◦F 0−1 Lebesgue almost everywhere. Therefore, ψ
0∗
◦F 0−1 = Jˆ 0(R) = M(cid:99)R(ψ
0
◦F 0−1) almost
everywhere and
(cid:90) (cid:90) 1 (cid:90) 1
i(p 0)= Rψ 02dP
0
=
0
(ψ
0
◦F 0−1)2 =
0
(J 0′)2.
Finally in this subsection, we define the two-sided hazard function h : R [0, ) of p by
0 0
→ ∞
(cid:40)
p (z) p (z)/F (z) if F (z) 1/2
0 0 0 0
h 0(z):= (cid:0) (cid:1) = (cid:0) (cid:1) ≤ (19)
F (z) 1 F (z) p (z)/ 1 F (z) if F (z)>1/2,
0 ∧ − 0 0 − 0 0
where in accordance with our convention 0/0 = 0, we have h (z) = 0 whenever F (z) 0,1 . The
0 0
∈ { }
following simple lemma provides a necessary and sufficient condition on the two-sided hazard function
for ψ to be appropriately bounded, which means that any negative antiderivative ℓ grows at most
0∗ ∗0
linearly, and therefore ensures robustness.
3Antitonicmeansdecreasing,incontrasttoisotonic(increasing)(GroeneboomandJongbloed,2014,Section2.1).
9Lemma 4. In the setting of Lemma 1, define z :=inf(suppp ) and z :=sup(suppp ). Then
min 0 max 0
(a) lim ψ (z)< if and only if limsup h (z)< , in which case z = ;
z →−∞ 0∗ ∞ z ↘zmin 0 ∞ min −∞
(b) lim ψ (z)> if and only if limsup h (z)< , in which case z = .
z →∞ 0∗ −∞ z ↗zmax 0 ∞ max ∞
Recall that a Laplace density has a constant two-sided hazard function, as well as a score function
whose absolute value is constant. Roughly speaking, the conditions on h in Lemma 4 are satisfied by
0
densities whose tails are heavier than those of the Laplace density (Samworth and Johnson, 2004), for
which it is particularly attractive to have bounded projected score functions.
2.2 The log-concave Fisher divergence projection
Let P and P be Borel probability measures on R such that P P . Write suppP for the support
0 1 0 1 0
≪
of P (i.e. the smallest closed set S satisfying P (S) = 1), and Int(suppP ) for its interior. Suppose
0 0 0
that there exists a Radon–Nikodym derivative dP /dP that is continuous on Int(suppP ), and also
0 1 0
strictly positive and differentiable on some subset E Int(suppP ) such that P (Ec)=0.4 The Fisher
0 0
⊆
divergence (also known as the Fisher information distance5) from P to P is defined to be
1 0
(cid:90) (cid:18)(cid:16)
dP
(cid:17)(cid:19)2
I(P ,P ):= log 0 ′ dP . (20)
0 1 0
dP
E 1
If P ,P do not satisfy the assumptions above, then we define I(P ,P ):= . In the case where P ,P
0 1 0 1 0 1
have Lebesgue densities p ,p respectively that are both locally absolutely c∞ ontinuous on R, we have
0 1
 (cid:90) (cid:18)(cid:16) logp 0(cid:17) ′(cid:19)2
p
=(cid:90)
(ψ ψ )2dP if suppp suppp
0 0 1 0 0 1
I(p 0,p 1) ≡I(P 0,P 1)=

{p0>0
}
p 1 R − otherwise,⊆
∞
where we denote by ψ := (logp )1 = p /p the corresponding score functions for j 0,1 .
j j ′ {pj>0
}
′j j
∈ { }
For further background on the Fisher divergence, see Johnson (2004, Definition 1.13), Yang et al. (2019,
Section 2) and references therein.
The following lemma establishes the connection between the projected score function and the Fisher
divergence.
Lemma 5. In the setting of Lemma 1, there is a unique continuous log-concave density p on R such
∗0
that suppp = and logp has right derivative ψ on . In particular, ψ =(logp ) Lebesgue almost
∗0 S0 ∗0 0∗ S0 0∗ ∗0 ′
everywhere on . Furthermore, if p is itself log-concave, then p =p .
S0 0 ∗0 0
When p is absolutely continuous, (18) indicates that the log-concave density p (with score function
0 ∗0
ψ 1 ) in Lemma 5 minimises I(p ,p) over the class of all univariate log-concave densities p.
0∗ S0 0 PLC
Moreover, p = p if and only if p is log-concave. Even when p is only uniformly continuous, we
∗0 0 0 0
refer to p as the log-concave Fisher divergence projection of p . In contrast, the log-concave maximum
∗0 0
likelihood projection pML of the distribution P (Du¨mbgen et al., 2011; Barber and Samworth, 2021)
0 0
can be interpreted as a minimiser of Kullback–Leibler divergence rather than Fisher divergence over
the class of upper semi-continuous log-concave densities. By Du¨mbgen et al. (2011, Theorem 2.2), pML
0
exists and is unique if and only if P is non-degenerate and has a finite mean (but not necessarily a
0
Lebesgue density). On the other hand, moment conditions are not required for p to exist and be
∗0
unique, but p is only defined in Lemma 5 when P has a uniformly continuous density on R. As we will
∗0 0
discuss in Section 2.4, the non-existence of the Fisher divergence projection for discrete measures P has
0
consequences for our statistical methodology.
Whenp isnotlog-concave,pMLusuallydoesnotcoincidewithp evenwhenbothexist,andmoreover
0 0 ∗0
the associated regression M-estimators
n n
(cid:88) (cid:88)
βˆ ψ 0ML ∈ar βg ∈m Rdax i=1logpM 0 L(Y i −X i⊤β) and βˆ ψ 0∗ ∈ar βg ∈m Rdax i=1logp ∗0(Y i −X i⊤β) (21)
4ThisconditionprecludesP0 fromhavinganyisolatedatoms,soinparticular,P0 cannotbeadiscretemeasure.
5ThisisnottobeconfusedwiththeFisherinformation(orFisher–Rao)metric(AmariandNagaoka,2000,Chapter2),
aRiemannianmetriconamanifoldofprobabilitydistributions.
10are generally different; see Examples 12 and 26 below. In fact, the following result shows that there
exist error distributions P for which the asymptotic covariance of βˆ is arbitrarily large compared
0 ψML
0
with that of the optimal convex M-estimator βˆ ψ∗, even when the latter is close to being asymptotically
0
efficient in the sense of (4).
Proposition 6. For every ϵ (0,1), there exists a distribution P with a finite mean and an absolutely
0
∈
continuous density p such that i(p )< , and the log-concave maximum likelihood projection q pML
0 0 ∞ 0 ≡ 0
has corresponding score function ψML :=q(R)/q Ψ (p ) satisfying
0 0 0 ∈ ↓ 0
V (ψ )
V
p0 (ψM0∗
L)
≤ϵ and ARE∗(p 0) ≥1 −ϵ. (22)
p0 0
Our next result provides a simple lower bound on the antitonic information.
Lemma7. Supposethatp isanabsolutelycontinuousdensityonRwithi(p )< . Thenp isbounded
0 0 0
∞
with i (p ) 4 p 2 , so
∗ 0 0
≥ ∥ ∥∞
4 p 2
0
ARE∗(p 0) ∥ ∥∞,
≥ i(p )
0
with equality if and only if p is a Laplace density, i.e. there exist µ R and σ > 0 such that p (z) =
(2σ) 1exp( z µ/σ) for
a∗0
ll z R. ∈
∗0
−
−| − | ∈
Remark8. Areassuringpropertyoftheantitonicprojectionisitsaffineequivariance: ifp isauniformly
0
continuous density, then for a > 0 and b R, the density z ap (az +b) =: p (z) has antitonic
0 a,b
∈ (cid:55)→
projected score function and log-concave Fisher divergence projection given by
ψ (z):=aψ (az+b) and p (z):=ap (az+b)
a∗,b 0∗ ∗a,b ∗0
respectively for z
∈
R. It follows that 1/V pa,b(ψ a∗,b) = i ∗(p a,b) = (cid:82) ∞ (ψ a∗,b)2p a,b = a2i ∗(p 0), so because
p = a p , both the antitonic relative efficiency and the−l∞ower bound in Lemma 7 are affine
a,b 0
∥ ∥∞ ∥ ∥∞
invariant in the sense that they remain unchanged if we replace p with p .
0 a,b
Similarly, if P has a finite mean, then by the affine equivariance of the log-concave maximum like-
0
lihood projection (Du¨mbgen et al., 2011, Remark 2.4), pML(z) = apML(az + b) and hence ψML =
a,b 0 a,b
aψML(az+b) for z R. Thus, V (ψML)=V (ψML)/a2, so the first ratio in (22) is also affine invari-
ant0
.
Consequently,∈
for any C
p (a 0,b
,
)a ,,b
µ
Rp0 and0
ϵ (0,1), there exists a density p satisfying (22)
0
(cid:82) ∈ ∞ ∈ ∈
with i(p )=C and zp (z)dz =µ.
0 R 0
The final result in this subsection relates properties of densities and their log-concave Fisher diver-
gence projections.
Proposition 9. For a uniformly continuous density p : R R, the log-concave Fisher divergence
0
projectionp anditscorrespondingdistributionfunctionF : [ → , ] Rhavethefollowingproperties.
∗0 0∗
−∞ ∞ →
(a) Denote by the set of z such that ψ is non-constant on every open interval containing z.
T ∈
S0 0∗
For z , we have
∈T
p (z) p (z) p (z) p (z)
∗0 0 and ∗0 0 , (23)
F (z) ≤ F (z) 1 F (z) ≤ 1 F (z)
0∗ 0
−
0∗
−
0
whence p (z) p (z).
∗0
≤
0
(cid:82) (cid:82)
(b) p p and i(p )= p dψ p dψ =i (p ).
∥
∗0∥∞
≤∥
0
∥∞
∗0
−
R ∗0 0∗
≤−
R 0 0∗ ∗ 0
We can define the two-sided hazard function h of the log-concave Fisher divergence projection p
∗0 ∗0
analogously to h in (19). Since p is log-concave, its density quantile function J := p (F ) 1 has
0 ∗0 0∗ ∗0
◦
0∗ −
decreasing right derivative (J )(R) =ψ (F ) 1 by (the proof of) Lemma 5, so J is concave on [0,1].
0∗ 0∗
◦
0∗ − 0∗
Thus,
0=J (0) J (u) u(J )(R)(u) and 0=J (1) J (u)+(1 u)(J )(R)(u)
0∗
≤
0∗
−
0∗ 0∗
≤
0∗
−
0∗
for all u [0,1], so for z , Lemma 1 and (23) in Proposition 9(a) imply that
∈ ∈T
(cid:0) (cid:1)
|ψ 0∗(z)
|=(cid:12)
(cid:12)(J
0∗)(R)(cid:0)
F
0∗(z)(cid:1)(cid:12)
(cid:12)
≤ F
(zJ )0∗ (cid:0)F 10∗(z F)
(z)(cid:1) =
F (z)
p (cid:0)∗0 1(z)
F
(z)(cid:1) =h ∗0(z)
0∗
∧ −
0∗ 0∗
∧ −
0∗
p (z)
0
(cid:0) (cid:1) =h 0(z).
≤ F (z) 1 F (z)
0 0
∧ −
11Proposition 9(b) provides inequalities on the supremum norm and antitonic information of the log-
concaveFisherdivergenceprojection. Inparticular, theFisherinformationoftheprojecteddensityisat
most the antitonic information of the original density.
2.3 Examples
Example 10. Let p be the Beta(a,b) density given by p (z) = za 1(1 z)b 11 /B(a,b) for
0 0 − − z (0,1)
a,b>1, where B denotes the beta function. Then p is uniformly continuo− us and lo{ g-∈ conca} ve on R, so
0
a 1 b 1
ψ 0∗(z)=ψ 0(z)=(logp 0) ′(z)= −
z −
1−
z
−
for all z (0,1), while ψ (z) = for z 0 and ψ (z) = for z 1. We have i (p ) = i(p ) =
(cid:82)1
ψ2p
<∈
if and only
0 i∗
f
a,b>∞
2, in
whi≤
ch case
the0∗ conclu− si∞
ons of
Th≥
eorem 2(c,d)
ho∗ ld.0 0
0 0 0 ∞
p (z)
0
p∗0(z)
ℓ (z) 0
ℓ∗0(z)
5 0 5 10 5 0 5 10
− − −
z z
Figure3: Left: Thenegativelog-densityℓ = logp andtheoptimalconvexlossfunctionℓ = logp
0
−
0 ∗0
−
∗0
when p is the standard Cauchy density. Right: The corresponding densities p and p .
0 0 ∗0
Example 11. Let p be the standard Cauchy density given by p (z) =1/(cid:0) π(1+z2)(cid:1) for z R. Then
0 0
p is absolutely continuous on R with i(p )=(cid:82) (p )2/p =1/2 and lim h (z)=0. We∈ will derive
0 0 R ′0 0 z 0
→±∞
an explicit expression for ψ , which is necessarily bounded by Lemma 4. In contrast to the previous
0∗
example, p is not log-concave, so ψ does not coincide with ψ =p /p : z 2z/(1+z2). Indeed,
0 0∗ 0 ′0 0
(cid:55)→−
1 arctan(z) (cid:18) (cid:16) 1(cid:17)(cid:19)
F 0(z)=
2
+
π
for z ∈R, F 0−1(u)=tan π u
− 2
= −cot(πu) for u ∈(0,1),
1 sin2(πu) 1 cos(2πu)
J 0(u)= π(cid:0) 1+cot2(πu)(cid:1) =
π
= −
2π
for u ∈[0,1].
Let ζ 2.33 be the unique ζ (0,π) satisfying ζ = tan(ζ/2), and define u := ζ /(2π) (0,1/2).
0 0 0
Then we ca≈ n verify that Jˆ is linear∈ on [0,u ] and on [1 u ,1], with Jˆ(u)=J (u) for u [u∈ ,1 u ]
0 0 0 0 0 0 0
− ∈ − ∪
0,1 ; see the left panel of Figure 2. It follows that
{ }

sinζ for u [0,u ]
 0
∈
0
Jˆ(R)(u)=
sin(2πu) for u [u ,1 u ]
0 
sinζ for
u∈ [10 u− ,1]0
;
0 0
− ∈ −

sinζ = 2z /(1+z2) for z ( , z ]
ψ 0∗(z)=Jˆ 0(R)(cid:0) F 0(z)(cid:1) = −s si in n0 ( ζ2a− rcta0 nz)= −0 2z/(1+z2)=ψ 0(z) ff oo rr z
z
∈ ∈[
[−
z− z∞ ,0,z−
)0
,]0
0 0
− ∈ ∞
where z := cot(πu ) = cot(ζ /2) 0.43 (0,1) satisfies z arctan(1/z ) = 1/2. Thus, ψ (z) =
ψ
(cid:0)
(z
0
z ) ( z
)(cid:1)0
for z
R;0
see
th≈
e right
p∈
anel of Figure 2.
A0
n
antideriv0
ative ϕ of ψ is
give0∗
n by
0
∧
0
∨ −
0
∈
∗0 0∗
(cid:90) z (cid:40) log(cid:0) π(1+z2)(cid:1) for z [ z ,z ]
0 0
ϕ ∗0(z):= ψ 0∗ −logπ = −
(z z )sinζ
log(cid:0) π(1+z2)(cid:1)
for z
∈ R−
[ z ,z ].
0 − | |− 0 0 − 0 ∈ \ − 0 0
12
7
6
5
4
3
2
1
3.0
52.0
2.0
51.0
1.0
50.0
0AsillustratedintheleftpanelofFigure3,ℓ := ϕ isasymmetricconvexfunctionthatisapproximately
∗0
−
∗0
quadratic on [ z ,z ] and linear outside this interval, so in this respect, it resembles the Huber loss
0 0
−
function (7). This is significant as far as M-estimation is concerned, since Huber-like loss functions are
designed precisely to be robust to outliers, such as those that arise in regression problems with heavy-
tailed Cauchy errors. As discussed in the introduction, ℓ is optimal in the sense that the resulting
∗0
r ae mg ore ns gsi ao ln
l
cM on- ves et xim Mat -eo sr tiβˆ mψ a0∗ to∈ rsa .r Bgm yi dn iβ r∈ecR td c(cid:80) omn i= p1 uℓ t∗0 a( tY ii on−
,
X i⊤β) has minimal asymptotic covariance (3)
1 1 2ζ cos(2ζ ) sin(2ζ ) i (p )
0 0 0 ∗ 0
=i ∗(p 0)= − 0.439, so ARE∗(p 0)= 0.878
V (ψ ) 2 − 4π ≈ i(p ) ≈
p0 0∗ 0
in this case, meaning that the restriction to convex loss functions results in only a small loss of effi-
ciency relative to the maximum likelihood estimator. This may well be outweighed by the increased
computational convenience of optimising a convex empirical risk function as opposed to a Cauchy likeli-
hood function, which typically has several local extrema; see van der Vaart (1998, Example 5.50) for a
discussion of the difficulties involved. Lemma 7 yields the bound ARE∗(p 0) 8 p
0
2 =8/π2 0.811.
≥ ∥ ∥∞ ≈
0 2 4 6 8
K
Figure 4: Plot of the asymptotic relative efficiency r(K) of the Huber M-estimator βˆ compared with
ψK
the optimal convex M-estimator.
respF eo cr tK to> (7)0, hath se asH yu mb pe tr or te icgr re es ls ai to in veM effi-e cs it ei nm ca ytor βˆ
ψK ∈
argmin
β
∈Rd(cid:80)n i=1ℓ K(Y
i
−X i⊤β) defined with
(cid:0) (cid:1)
V (ψ ) π πK2+2K 2(1+K2)arctanK
r(K):= p0 0∗ = −
V (ψ ) 4i (p )arctan2K
p0 K ∗ 0
compared with the optimal convex M-estimator βˆ
ψ
0∗; see Figure 4. The maximum value sup K>0r(K)
≈
0.9998 is attained at K 0.394. Moreover,
∗
≈
4
lim r(K)= 0.922
K →0 π2i ∗(p 0) ≈
is the asymptotic relative efficiency V (ψ )/V (ψ) of the least absolute deviation (LAD) estimator
β tˆ hψ e∈ ota hr eg rm hin anβ ∈dR ,d t(cid:80) hen i= H1 u| bY ei r− loX ssi⊤β ℓ|, f io nrp0 (w 7)h0∗ i cc oh nψ vp e(0 r·) ge: s= p− ois ng tn w( i· s) ea tn od tV hp e0( sψ qu) a= red1/ e(cid:0) r4 rp o0 r(0 lo)2 ss(cid:1) a= sπ K2/4. On
,
K
(cid:82) → ∞
so lim V (ψ ) = z2p (z)dz = and hence lim r(K) = 0. We recall the difficulties of
K
→∞
p0 K R 0
∞
K
→∞
choosing K, and its connection to the choice of scale, from the discussion in the introduction.
The Cauchy density p 0 and its log-concave Fisher divergence projection p ∗0
:=eϕ∗
0 are plotted in the
right panel of Figure 3. Since p =p on [ z ,z ] and ψ is constant on R [ z ,z ], it turns out that
(cid:82) (cid:82)
0 ∗0
−
0 0 0∗
\ −
0 0
i (p )= p dψ = p dψ =i(p ), so both inequalities in Proposition 9(b) are in fact equalities
∗ 0
−
R 0 0∗
−
R ∗0 0∗ ∗0
in this example.
In Example 26 in the appendix, we take P to be a scaled t distribution, which has a finite first
0 2
moment (unlike the Cauchy distribution in Example 11), and verify that βˆ ψ 0ML and βˆ ψ 0∗ in (21) are
different convex M-estimators. Example 27 features a symmetrised Pareto density with polynomially
13
)K(r
1
8.0
6.0
4.0
2.0
0decaying tails, where the optimal convexloss function ℓ is a scale transformation of the robust absolute
∗0
error loss z z .
(cid:55)→| |
Moving on from heavy-tailed distributions, we now consider a density p that fails to be log-concave
0
because it is not unimodal.
Example 12. For ρ (0,1) and µ>0, let p be the two-component Laplace mixture density given by
0
∈
1 ρ ρ
p 0(z)= − e −|z+µ |+ e −|z −µ |
2 2
for z R. The corresponding score function ψ =(logp ) =p /p satisfies
∈
0 0 ′ ′0 0

1
ρez (1 ρ)e z
for z < −µ
−
ψ (z)= − − for z ( µ,µ)
0 ρe 1z+(1 −ρ)e −z
for z
∈ >µ−
,
−
so ψ
0
is strictly increasing on ( −µ,µ) and constant on both ( −∞,µ) and (µ, ∞). Since J
0′
= ψ
0
◦F 0−1
on (0,1) F ( µ),F (µ) , it follows that J is convex on [F ( µ),F (µ)] while being linear on both
0 0 0 0 0
[0,F ( µ\ )]{ and− [F (µ),1]. } Therefore,Jˆ =J on[0,F ( µ)] [F (− µ),1]andJˆ islinearon[F ( µ),F (µ)]
0 0 0 0 0 0 0 0 0
−(cid:0) (cid:1) − ∪ −
with J F ( µ) =p ( µ), so
0 0 0
± ±

ψ 0∗(z)=Jˆ 0(R)(cid:0) F 0(z)(cid:1)
= 1
Fp 00 1(( µµ )) −−p F0 0( (− −µ µ)
)
=2ρ −1
ff
f
oo
o
rr
r
zz
z
∈<
[−
µ−
,µ
µ,µ)
− ≥
and hence

Cez+µ for z µ

≤− 1
p ∗0(z)= C Ce e( (2 2ρ ρ−1 1) )( 2z µ+µ e)
(z µ)
f fo or
r
z
z
∈[
µ−
,µ,µ] where C :=
1+e(2ρ −1)2µ+(cid:82) 02µ e(2ρ
−1)zdz.
− − −
· ≥
By direct calculation, p ( µ) = C < p ( µ) and p (µ) = Ce(2ρ 1)2µ < p (µ), so p = p ( µ)
∗0
−
0
−
∗0 − 0
∥
∗0∥∞ ∗0
− ∨
p (µ)<p ( µ) p (µ)= p and
∗0 0
− ∨
0
∥
0
∥∞
(cid:90)
i (p )= p dψ =2(1 ρ) p ( µ)+2(1+ρ) p (µ)>2(1 ρ) p ( µ)+2(1+ρ) p (µ)=i(p ).
∗ 0
− R
0 0∗
− ·
0
− ·
0
− ·
∗0
− ·
∗0 ∗0
We mention that Proposition 6 can be proved by slightly modifying the construction of p in Exam-
0
ple 12.
2.4 Estimation of the projected score function
Theorem 2 motivates a general-purpose nonparametric procedure to estimate the antitonic projected
iid
score function ψ based on a sample ε ,...,ε p . We will first explain why a naive approach
fails before
descr0∗
ibing our
methodology.1
For
an
loc∼
ally0
absolutely continuous function ψ: R R with
derivative ψ , the empirical analogue of D
(ψ)=E(cid:0)
ψ2(ε )+2ψ (ε
)(cid:1)
in (14) is
→
′ p0 1 ′ 1
n
1 (cid:88)
Dˆ (ψ) Dˆ (ψ;ε ,...,ε ):= ψ2(ε )+2ψ (ε ) . (24)
n n 1 n i ′ i
≡ n { }
i=1
Recall from the introduction that score matching estimates the score function ψ = p /p (associated
0 ′0 0
with a locally absolutely continuous p ) by an empirical risk minimiser ψˆ argmin Dˆ (ψ) over an
appropriate class of functions Ψ.
0 n ∈ ψ ∈Ψ n
However, to obtain a monotone score estimate, we cannot minimise ψ Dˆ (ψ) directly over the
n
class Ψac of all decreasing, locally absolutely continuous ψ: R R. Indee(cid:55)→ d, inf
ψ
ΨacDˆ n(ψ) = ,
as can b↓e seen by constructing differentiable approximations to a→ decreasing step fun∈ ct↓ ion whose ju− m∞ ps
14are at the data points ε ,...,ε . To circumvent this issue, we instead propose the following estimation
1 n
strategy.
Antitonic projected score estimation: Consider smoothing the empirical distribution of ε ,...,ε ,
1 n
for example by convolving it with an absolutely continuous kernel K: R R to obtain a kernel density
estimator z p˜ (z) := n
1(cid:80)n
K (z ε ), where h > 0 is a
suit→
able bandwidth and K () :=
(cid:55)→ n − i=1 h − i h ·
h 1K(/h). We can then define the smoothed empirical score matching objective
−
·
(cid:90) (cid:90)
D˜ (ψ):=D (ψ)= ∞ ψ2p˜ +2 p˜ dψ
n p˜n n n
−∞ S0
for ψ Ψ (p˜ ), which approximates the population expectation in the definition of D (ψ). Then by
∈ ↓
n p0
Theorem 2,
ψˆ :=Jˆ(R) F˜ argmin D˜ (ψ), (25)
n n ◦ n ∈ n
ψ ∈Ψ↓(p˜n)
whereF˜ denotesthedistributionfunctioncorrespondingtop˜ , andJ :=p˜ F˜ 1. Thisisreminiscent
n n n n
◦
n−
of maximum smoothed likelihood estimation of a density or distribution function (e.g. Eggermont and
LaRiccia,2000;GroeneboomandJongbloed,2014,Sections8.2and8.5). Byevaluatingtheantiderivative
ofJ onasuitablyfinegrid,wemayobtainapiecewiseaffineapproximationtoJˆ (andhenceapiecewise
n n
constant approximation to its derivative) using PAVA, whose space and time complexities scale linearly
with the size of the grid (Samworth and Shah, 2024, Section 10.3.1).
ψ0(z)
ψ 0∗(z)
ψˆ n(z)
ℓ (z)
0
ℓ∗0(z)
ℓˆ(z)
n
6 4 2 0 2 4 6 5 0 5
− − − −
z z
Figure 5: Kernel-based estimates of the projected score function and optimal convex loss function based
on a sample of size n=2000 from the Cauchy distribution.
More generally, we can use ε ,...,ε to construct a generic (not necessarily monotone) score estima-
1 n
tor ψ˜ and an estimate Fˆ of the distribution function F corresponding to the density p . By analogy
n n 0 0
with the explicit representation (17) of ψ , we then define the decreasing score estimate
0∗
ψˆ
n
:= M(cid:99)R(ψ˜
n
◦Fˆ n−1) ◦Fˆ n. (26)
As explained above, (an approximation to) ψˆ can be computed efficiently using isotonic regression
n
algorithms. In particular, if Fˆ is taken to be the empirical distribution function of ε ,...,ε , then by
n 1 n
P
i
ro [p no ]s (cid:9)i .tio On u4 r3 d, eψ cˆ rnL ea:= sinM(cid:99)
g
sL c( oψ˜ rn e◦ esFˆ tn i− m1 a) t◦ eFˆ cn anis ba en ta an kt ei nto tn oic bl eea es itt hs eq ru ψa ˆr Les ores tt him
e
a clt oo sr elb yas re ed lato en d(cid:8) ψˆ(cid:0) ε i :, fψ˜ on r( eε vi) e(cid:1) ry:
z∈ R,wehaveψˆ (z)=ψˆL(cid:0) ε(z)(cid:1) ,whereε(z)isthesmallestelementn of ε ,...,ε thatiseitn herstrictly
∈ n n { 1 n }
greater than z or equal to max ε .
i [n] i
The transformation (26) ma∈ y be applied to any appropriate initial score estimator ψ˜ . Since a
n
misspecified parametric method may introduce significant error at the outset, we seek a nonparametric
estimator. For instance, we may take ψ˜ to be a ratio of kernel density estimates of p and p , which
n ′0 0
may be truncated for theoretical and practical convenience to avoid instability in low-density regions;
see (31) in Section 3.1. Observe that (25) is a special case of (26) with ψ˜ =p˜ /p˜ and Fˆ =F˜ .
n ′n n n n
15
1
5.0
0
5.0
1
−
−
7
6
5
4
3
2
1ψ (z) ℓ (z)
0 0 ψ 0∗(z) ℓ∗0(z)
ψˆ (z) ℓˆ(z)
n n
4 2 0 2 4 4 2 0 2 4
− − − −
z z
Figure 6: Kernel-based estimates of the projected score function and optimal convex loss function based
on a sample of size n=104 from the Gaussian mixture distribution 0.4N( 2,1)+0.6N(2,1).
−
3 Semiparametric M-estimation via antitonic score matching
Suppose that we observe independent and identically distributed pairs (X ,Y ),...,(X ,Y ) satisfying
1 1 n n
Y =X β +ε for i [n], (27)
i i⊤ 0 i
∈
where X ,...,X are Rd-valued covariates that are independent of errors ε ,...,ε with an unknown
1 n 1 n
absolutely continuous (Lebesgue) density p on R. We do not necessarily insist that E(ε )=0, because
0 1
we also have in mind settings where we might want to assume that a quantile (e.g. the median) of ε
1
is zero. In fact, we do not even assume that E(ε ) is well-defined, which allows us to consider Cauchy
1
errors,forinstance,asarunningexample. Apricetopayforthisgeneralityisthatifanintercepttermis
presentin(27),i.e.X =1foreachi [n],thenthecorrespondingcomponentβ ofβ isunidentifiable.
id 0d 0
On a related point, a necessary condi∈ tion for β to be identifiable is that E(X X ) is positive definite.
0 1 1⊤
Indeed,ifthismatrixissingular,thenthereexistsv Rd 0 suchthatX v =0forallialmostsurely;
∈ \{ }
i⊤
in that case, the joint distribution of our observed data is unchanged if we replace β with β +v. This
0 0
identifiability issue is discussed in greater detail following the statement of Proposition 13.
For β Rd, define q : R R by
β
∈ →
q (z):=Ep (cid:0) z X (β β)(cid:1) ,
β 0
−
1⊤ 0
−
so that q is the density of Y X β =ε +X (β β).
β 1
−
1⊤ 1 1⊤ 0
−
Our approach to estimating β with data-driven convex loss functions is motivated by the following
0
population-level optimisation problem. We seek to minimise the augmented score matching objective
(cid:18)(cid:90) (cid:90) (cid:19)
Q(β,ψ):=D (ψ)=E ∞ ψ2(cid:0) z+X (β β)(cid:1) p (z)dz+2 p (cid:0) z X (β β)(cid:1) dψ(z) (28)
qβ 1⊤ 0
−
0
R
0
−
1⊤ 0
−
−∞
jointly over β Rd and ψ Ψ (q ) satisfying
β
∈ ∈ ↓
E(cid:0)
X ψ(Y X
β)(cid:1)
=0. (29)
1 1
−
1⊤
Recall from (14) that if ψ Ψ (q ) is locally absolutely continuous on R, then
β
∈ ↓
Q(β,ψ)=Eψ2(Y X β)+2Eψ (Y X β).
1
−
1⊤ ′ 1
−
1⊤
The constraint (29) is the population analogue of the estimating equations (2) based on ψ. It forces β
to be a minimiser of the convex population risk function b Eℓ(Y X b)=:L (b) over , where ℓ
is any negative antiderivative of ψ, and is the convex
set(cid:55)→
of b
1 R−
d
su1 c⊤
h that
Lψ
(b) is
weD ll-ψ
defined in
ψ ψ
D ∈
[ , ].6.
−∞ ∞
The following proposition characterises the global joint minimiser of our constrained score matching
optimisation problem. For j [d], write e for the jth standard basis vector in Rd.
j
∈
6Indeed,ℓisconvexwithψ= ℓ(R),soforβ,β′ Rd,wehave
− ∈
ℓ(Y1 −X 1⊤β′) ≥ℓ(Y1 −X 1⊤β)+(β −β′)⊤X1ψ(Y1 −X 1⊤β).
Ifβ satisfies(29),thentakingexpectationsinthedisplayaboveshowsthatLψ(β′) Lψ(β)forallβ′ ψ.
≥ ∈D
16
2
1
0
1
2
−
−
5
4
3
2Proposition 13. For an absolutely continuous density p with (p )=R, let ψ be the projected score
function defined in (17). Assume that i (p ) = (cid:82) (ψ
)2p0
< S
an0
d that E(X
X0∗
) Rd d is positive
∗ 0 R 0∗ 0
∞
1 1⊤
∈
×
definite. Let
Γ:=(cid:8) (β,ψ):β Rd, ψ Ψ (q ), E(cid:0) X ψ(Y X β)(cid:1) =0(cid:9) .
∈ ∈ ↓
β 1 1
−
1⊤
(a) Suppose that p is symmetric. Denote by Ψanti(q ) the set of all ψ Ψ (q ) such that ψ( z) =
0 β β
lim z′ zψ(z ′), and let Γanti := (β,ψ) Γ↓:ψ Ψanti(q β) . Then ∈ ↓ −
− ↗ { ∈ ∈ ↓ }
(β ,ψ )= argmin Q(β,ψ).
0 0∗
(β,ψ) Γanti
∈
(b) If instead X =1 almost surely, then
1d
(cid:0) (cid:1)
β +ce ,ψ ( +c) argmin Q(β,ψ)
0 d 0∗
· ∈
(β,ψ) Γ
∈
for all c R. Moreover, all minimisers are of this form.
∈
Thus, when we restrict attention to (right-continuous versions of) antisymmetric functions ψ in the
setting of Proposition 13(a), the unique minimiser of our population-level optimisation problem is given
by the pair consisting of the vector β of true regression coefficients, and the antitonic score projection
0
ψ of the error density p . This motivates our statistical methodology below, where we seek to minimise
0∗ 0
sample versions of this population-level objective. From the proof of Proposition 13(a), we see that
for any (β,ψ) Γanti, we also have (β ,ψ) Γanti; moreover, Q(β ,ψ ) = inf Q(β ,ψ) =
∈ 0 ∈ 0 0∗ ψ ∈Ψa ↓nti(qβ0) 0
1/i (p ) < 0. On the other hand, if β = β , and if ψ Ψanti(q ) is such that (β,ψ) Γanti, then
∗ 0 0 β
− Q(β,ψ) 0. In other words, feasible pa̸ irs (β,ψ) with β∈ =↓β are well-separated from∈ the optimal
0
≥ ̸
solution in terms of their objective function values.
In Proposition 13(b), where the errors need not have mean zero and where we no longer restrict
attentiontoantisymmetricψ,thepair(β ,ψ )stillminimisesourobjectiveuptoappropriatetranslations
0 0∗
to account for the lack of identifiability of the intercept term. Indeed, the joint distribution of (X ,Y )
1 1
doesnotchangeifwereplaceβ andε withβ +ce andε crespectively,foranyc R. Thisexplains
0 1 0 d 1
− ∈
why the minimiser in (b) is not unique; observe that ψ ( +c) is the projected score corresponding to
0∗
·
the density p ( +c) of ε c. Nevertheless, (b) indicates that translations of the intercept term are the
0 1
· −
only source of non-uniqueness, so that the first d 1 components of β can still be recovered by solving
0
−
the constrained optimisation problem above on the population level.
Sincetheobjectivefunction(β,ψ) Q(β,ψ)isnotjointlyconvex,wecouldseektominimiseQ(β,ψ)
(cid:55)→
subject to the constraint (29) by alternating the following two steps:
I. For a fixed β, minimise the (convex) score matching objective ψ Q(β,ψ) = D (ψ) based on the
(cid:55)→
qβ
densityq ofY X β,withoutimposing (29). Thisconstraintmustbeomittedhereasotherwiseβ
β 1
−
1⊤
will not be updated in Step II below.
II. For a fixed decreasing and right-continuous ψ, minimise the convex function β Eℓ(Y X β),
(cid:55)→
1
−
1⊤
where ℓ is a negative antiderivative of ψ.
Inanempiricalversionofthisalternatingminimisationalgorithmbasedon(X ,Y ),...,(X ,Y ),we
1 1 n n
canestimateψ inStepIbyminimisingasampleanalogueofD (ψ)overψ. AsdiscussedinSection2.4,
0∗ qβ
the unknown density of Y X β can be approximated by smoothing the empirical distribution of the
1
−
1⊤
residuals(Y X β)n fromthecurrentestimateofβ . StepIItheninvolvesfindinganM-estimator(1)
i − i⊤ i=1 0
of β based on the convex loss function induced by the current estimate of ψ . In practice, we can apply
0 0∗
anysuitableconvexoptimisationalgorithmsuchasgradientdescent,withNewton’smethodbeingafaster
alternative when the score estimate is differentiable. Steps I and II can then be iterated to convergence.
Thefollowingtwosubsectionsfocusinturnonthelinearmodelsinparts(a)and(b)ofProposition13.
We will analyse a specific version of the above procedure that is initialised with a pilot estimator β¯
n
of β . Provided that (β¯ ) is √n-consistent, we show that a single iteration of Steps I and II yields a
0 n
semiparametric convex M-estimator of β that achieves ‘antitonic efficiency’ as n .
0
→∞
173.1 Linear regression with symmetric errors
Under the assumption that p is symmetric, we first approximate ψ via antitonic projection of kernel-
0 0∗
based score estimators. We do not observe the errors ε ,...,ε directly, so in view of the discussion
1 n
above, in the algorithm below we use the residuals from the pilot regression estimator to construct our
initial score estimators. Assume throughout that n 3.
≥
1. Sample splitting: Partition the observations into three folds indexed by disjoint I ,I ,I [n]
1 2 3
⊆
such that I = I = n/3 and I = n I I respectively. For notational convenience, let
1 2 3 1 2
| | | | ⌊ ⌋ | | −| |−| |
I :=I for j 1,2 .
j+3 j
∈{ }
2. Pilot estimators: Let ψ: R R be differentiable, antisymmetric and strictly decreasing with
0<lim ψ(z)< . For j → 1,2,3 , let β¯(j) be an initial Z-estimator satisfying the estimating
z n
→−∞ ∞ ∈{ }
equations
(cid:88)
X
ψ(cid:0)
Y X
β¯(j)(cid:1)
=0. (30)
i i − i⊤ n
i ∈Ij
3. Antitonic projected score estimation: For j 1,2,3 and i I , define out-of-sample
j+1
∈ { } ∈
residuals εˆ := Y X β¯(j). Letting K: R [0, ) be a differentiable kernel and h h > 0 be a
i i
−
i⊤ n
→ ∞ ≡
n
bandwidth that will be specified below, define a kernel density estimator p˜ of p by
n,j 0
1 (cid:88)
p˜ (z):= K (z εˆ)
n,j h i
I −
j+1
| |i ∈Ij+1
for z R, where K ()=h 1K(/h). In addition, let S˜ :=(cid:8) z R: p˜ (z) α , p˜ (z) γ (cid:9) ,
∈
h
·
−
·
n,j
∈ |
′n,j
|≤
n n,j
≥
n
whereα (0, ]andγ (0, )aresuitablychosentruncationparameters,anddefineψ˜ : R R
n n n,j
∈ ∞ ∈ ∞ →
by
p˜ (z)
ψ˜ (z):= ′n,j 1 . (31)
n,j p˜ n,j(z) {z ∈S˜ n,j}
Writing F˜
n,j
for the distribution function corresponding to p˜ n,j, let ψˆ
n,j
:= M(cid:99)R(ψ˜
n,j
◦F˜ n−,j1) ◦F˜
n,j
be
an antitonic projected score estimate, in accordance with (26). Finally, define an estimator ψˆanti
n,j ∈
Ψanti(p ) of ψ by
0 0∗
↓ ψˆ (z) ψˆ ( z)
ψˆanti(z):= n,j − n,j −
n,j 2
for z R.
∈
4. Plug-in cross-fitted convex M-estimator: For j 1,2,3 , let ℓˆsym: R R be the induced
∈ { } n,j →
convex loss function given by ℓˆsym(z):= (cid:82)z ψˆanti, and define
n,j − 0 n,j
(cid:88)
βˆ(j) argmin ℓˆsym(Y X β) (32)
n ∈ β ∈Rd i ∈Ij+2 n,j i − i⊤
to be a corresponding M-estimator of β . Finally, let
0
βˆ(1)+βˆ(2)+βˆ(3)
βˆ := n n n . (33)
n†
3
Remark. For each j 1,2,3 , we claim that β¯(j) and βˆ(j) always exist in Steps 2 and 4 respectively.
n n
Indeed, either ψˆanti ∈ 0{ , or lim} ψˆanti(z) > 0 > lim ψˆanti(z). In the former case, ℓˆsym 0
n,j ≡ z →−∞ n,j z →∞ n,j n,j ≡
and any β Rd minimises the objective function in (32). On the other hand, in the latter case, ℓˆsym
∈ n,j
is a finite, convex function on R that is coercive in the sense that ℓˆsym(z) as z . Thus,
n,j → ∞ | | → ∞
θ (θ ,...,θ ) (cid:80) ℓˆsym(Y θ ) =: (θ) is also convex and coercive on the column space
of≡
the
1 designn ma(cid:55)→
trix
Xi ∈I :j =+2 (n X,j i − Xi
)
Ln R,j
n d, and hence β (Xβ) attains its minimum
1 n ⊤ × n,j
··· ∈ (cid:55)→ L
on Rd. The existence of β¯(j) is guaranteed by similar reasoning. If ψˆanti is also continuous, then any
n n,j
minimiser βˆ(j) satisfies the estimating equations (cid:80) X ψˆanti(Y X βˆ(j))=0.
n i ∈Ij+2 i n,j i − i⊤ n
18In (32), βˆ(j) is certainly not unique if X has does not have full column rank, but this happens
n
with asymptotically vanishing probability as n if E(X X ) is invertible. On the other hand,
→ ∞
1 1⊤
βˆ(j) is unique if X does have full column rank and ψˆanti is strictly decreasing, in which case is
n n,j Ln,j
strictly convex on the column space of X. In practice, our antitonic score estimators may have constant
pieces. Nevertheless, we emphasise that the conclusion of Theorem 14 below applies to all sequences of
minimisers (βˆ(j)), so it is unaffected by non-uniqueness issues.
n
In the above procedure, we use the observations indexed by I ,I ,I to construct the pilot estimator
1 2 3
β¯(1), antitonic score estimate ψˆanti and semiparametric M-estimator βˆ(1) respectively. Since each fold
n n,1 n
(specifically the last one) contains only about one-third of all the data, sample splitting reduces the
efficiency of βˆ(1). We remedy this by cross-fitting (Chernozhukov et al., 2018, Definition 3.1), which
n
involves cyclically permuting the folds to obtain βˆ(2),βˆ(3) analogously to βˆ(1), and then averaging these
n n n
three estimators. This reduces the limiting covariance of βˆ(1) by a factor of three in the theory below,
n
where we show that βˆ(1),βˆ(2),βˆ(3) are ‘asymptotically independent’ in a precise sense.
n n n
A different version of cross-fitting (Chernozhukov et al., 2018, Definition 3.2) instead averages the
empirical risk functions across all three folds, and outputs a single estimator
3
(cid:88) (cid:88)
βˆ argmin ℓˆsym(Y X β), (34)
n‡ ∈ β ∈Rd j=1i ∈Ij+2 n,j i − i⊤
whose existence is similarly guaranteed by the convexity of ℓˆsym for j 1,2,3 .
For a sequence of regression models (27) indexed by n
n N,j
, we
m∈ ak{
e the
f}
ollowing assumptions on
∈
the parameters in our procedure.
(A1) E(X X ) Rd d is positive definite and max X α /γ =o (n1/2).
1 1⊤
∈
× i ∈[n]
∥
i
∥
n n p
(A2) α , γ ,h 0, nh3γ2 and for every fixed a>0, we have (h n a)(α /γ )2 0 as
n →∞ n n → n n →∞ n ∨ − n n →
n .
→∞
(A3) The kernel K is non-negative, twice continuously differentiable and supported on [ 1,1].
−
(A4) p is an absolutely continuous density on R such that i(p ) < and (cid:82) z δp (z)dz < for
0 0 R 0
∞ | | ∞
some δ >0.
(A5) There exists t > 0 such that for t t ,t , the antitonic projected score function ψ satisfies
(cid:82)
0
∈ {−
0 0
}
0∗
(ψ )2(z+t)p (z)dz < .
R 0∗ 0
∞
Theconditionsin(A2)onthetruncationparametersα ,γ andbandwidthh aremild;forinstance,
n n n
we may take α = γ 1 = logn and h = n b for some b (0,1/3). Our procedure does not require
n n− n −
∈
knowledge of the exponent δ > 0 in (A4). Recall from Lemma 1 that ψ (z) is finite if and only if
0∗
z = (cid:0) inf(suppp ),sup(suppp )(cid:1) , so if (A5) holds, then = R and ψ is necessarily finite-valued
on∈RS
.0
The
condition0
s (A4)–(A5)
a0
re satisfied by a variety
ofS0 commonly-en0∗
countered densities p with
0
differenttailbehaviours,rangingfromallt densitieswithν >0degreesoffreedom(includingtheCauchy
ν
density as a special case ν =1) to lighter-tailed Weibull, Laplace, Gaussian and Gumbel densities.
Under the assumptions above, we first establish the L2(P )-consistency of the initial estimates ψ˜
0 n,j
ofthescorefunctionψ , fromwhichitfollowsthattheantitonicfunctionsψˆ consistentlyestimatethe
0 n,j
population-level projected score ψ in L2(P ); see Lemmas 29 and 30 in Section 6.3. This enables us to
0∗ 0
provethatoursemiparametricconvexM-estimatorsβˆ ,βˆ are√n-consistentandhavethesamelimiting
n† n‡
wG ha eu rs esi ℓan dd ei nst or ti eb su ati non opa ts imth ae
l
c‘ oo nra vc el xe’ loc so sn fv ue nx ctM io- nes wti im that ro igr hβ tˆ ψ d0∗ er: i= vata ir vg em ψin .β ∈Rdn −1(cid:80)n i=1ℓ ∗0(Y i −X i⊤β),
∗0 0∗
Theorem 14. Suppose that (A1)–(A5) hold for the linear model (27) with symmetric error density p .
0
Then for any sequence of estimators (βˆ ) with βˆ βˆ ,βˆ for each n, we have
n n
∈{
n† n‡
}
(cid:18) E(X X ) 1(cid:19)
√n(βˆ β ) d N 0,{ 1 1⊤ }−
n 0 d
− → i (p )
∗ 0
as n .
→∞
19Remark 15. Since X and ε are independent, (X ,Y ) has joint density (x,y) p (y x β ) with
1 1 1 1 0 ⊤ 0
respect to the product measure P Leb on Rd R, where we write P for the di(cid:55)→ stributi− on of X , and
X X 1
Leb for Lebesgue measure on R. Th⊗ erefore, the s× core function ℓ˙ : Rd R Rd is given by
β0
× →
ℓ˙ (x,y):=xψ (y x β ),
β0 0
−
⊤ 0
where ψ = p /p . If i(p ) = E(cid:0) ψ (ε )2(cid:1) is finite, then because Eψ (ε ) = 0, the Fisher information
0 ′0 0 0 0 1 0 1
matrix at β Rd is
0
∈
I(β ):=Cov ℓ˙ (X ,Y )=Cov X ψ (ε ) =E(cid:8) X X ψ (ε )2(cid:9) =E(X X )i(p ) Rd d.
0 β0 β0 1 1
{
1 0 1
}
1 1⊤ 0 1 1 1⊤ 0
∈
×
Since i(p ) i (p ) > 0 by Theorem 2(d), I(β ) is positive definite if and only if E(X X ) is positive
0
≥
∗ 0 0 1 1⊤
definite. In this case, the convolution and local asymptotic minimax theorems (van der Vaart, 1998,
Chapter 8) indicate that βˆMLE in (4) has the ‘optimal’ limiting distribution
N
d(cid:0)
0,I(β 0)
−1(cid:1)
=N
d(cid:18) 0,{E(X 1X 1⊤) }−1(cid:19)
i(p )
0
among all (regular) sequences of estimators of β .
0
By analogy with the previous display, the limiting covariance in Theorem 14 can be written as
the inverse I (β ) 1 of the antitonic information matrix I (β ) := E(X X )i (p ). By Theorem 2,
∗ 0 − ∗ 0 1 1⊤ ∗ 0
1/i (p ) = V (ψ ) = min V (ψ), so we see from (3) that I (β ) 1 is the smallest attainable
lim∗ itin0 g covarp i0 anc0∗ e amongψ a∈ llΨ c↓ o(p n0 v) exp0 M-estimators βˆ based on a fix∗ ed0 ψ− Ψ (p ). We can therefore
ψ 0
interpret I (β ) 1 as an antitonic efficiency lower bound. ∈ ↓
∗ 0 −
3.2 Linear regression with an intercept term
For d 2, now consider the linear model
≥
Y =µ +X˜ θ +ε for i [n], (35)
i 0 i⊤ 0 i
∈
where µ is an explicit intercept term, so that β = (θ ,µ ) and X = (X˜ ,1) in (27) for i [n]. As
0 0 0 0 i i⊤ ⊤
∈
mentioned previously, a necessary condition for θ to be identifiable is that
0
(cid:18)E(X˜ X˜ ) E(X˜ )(cid:19)
E(X X )= 1 1⊤ 1
1 1⊤ E(X˜ ) 1
1 ⊤
is positive definite, which is equivalent to Cov(X˜ ), i.e. the Schur complement of 1 in E(X X ), being
1 1 1⊤
positive definite.
Forthefunctionalϕ: (θ,µ) θ ofinterest,whosederivativeisrepresentedbyDϕ(θ,µ)=(I 0)
d 1
R(d 1) d, the information lowe(cid:55)→ r bound is the top-left (d 1) (d 1) submatrix of I(β ) 1, na− mely ∈
− × 0 −
− × −
Cov(X˜ ) 1
Dϕ(β )I(β ) 1 Dϕ(β ) = 1 − R(d 1) (d 1); (36)
0 0 − 0 ⊤ − × −
i(p ) ∈
0
seevanderVaart(1998,Chapters8and25.3). InSection6.3.2,weverifythattheefficientscorefunction
(van der Vaart, 1998, Chapter 25.4) ℓ˜ : Rd R Rd 1 is given by
β0
× →
−
ℓ˜ (x,y):=(cid:0) x˜ E(X˜ )(cid:1) ψ (y x β ), (37)
β0
−
1
·
0
−
⊤ 0
where x=(x ,...,x ) and x˜=(x ,...,x ).
1 d 1 d 1
−
We will construct an adaptive convex M-estimator of β that asymptotically achieves antitonic ef-
0
ficiency. Similarly to Section 3.1, we employ three-fold cross-fitting with the convention I = I for
j+3 j
j 1,2 . WefollowSteps1and2ofthepreviousproceduretoobtainpilotestimatorsβ¯(j) (θ¯(j),µ¯(j))
n n n
of∈ β{ =(}
θ ,µ ) satisfying
(cid:80)
X
ψ(cid:0)
Y X
β¯(j)(cid:1)
=0 for j 1,2,3 , but make the
follow≡
ing modifi-
catio0
ns
to0 sub0
sequent steps.
i ∈Ij i i − i⊤ n ∈{ }
3. Antitonic projected score estimation: For j 1,2,3 and i I , use the out-of-sample
′ j+1
∈ { } ∈
residualsεˆ :=Y X β¯(j) toconstructtheinitialkernel-basedscoreestimatorψ˜ anditsantitonic
i i
−
i⊤ n n,j
projectionψˆ
n,j
:= M(cid:99)R(ψ˜
n,j
◦F˜ n−,j1) ◦F˜
n,j
asbefore. Sincep
0
isnotsymmetricingeneral,weestimate
ψ by ψˆ instead of ψˆanti.
0∗ n,j n,j
204. Plug-in cross-fitted convex M-estimator: For j 1,2,3 , let ℓˆ : R R be the induced
′ n,j
convex loss function given by ℓˆ (z):= (cid:82)z ψˆ , and∈ defi{ ne } →
n,j − 0 n,j
θˆ(j) argmin (cid:88) ℓˆ (cid:0) Y µ¯(j) X¯ θ¯(j) (X˜ X¯ ) θ(cid:1) , (38)
n ∈ θ ∈Rd−1 i ∈Ij+2 n,j i − n − n⊤,j n − i − n,j ⊤
where X¯ := I 1(cid:80) X˜ . Finally, let θˆ :=(cid:0) θˆ(1)+θˆ(2)+θˆ(3)(cid:1) /3. Alternatively, define
n,j | j+2 |− i ∈Ij+2 i n† n n n
3
θˆ argmin(cid:88) (cid:88) ℓˆ (cid:0) Y µ¯(j) X¯ θ¯(j) (X˜ X¯ ) θ(cid:1) .
n‡ ∈ θ ∈Rd−1 j=1i ∈Ij+2 n,j i − n − n⊤,j n − i − n,j ⊤
For the same reasons as in Section 3.1, there exists a minimiser θˆ(j) in (38) if
n
lim ψˆ (z)>0> lim ψˆ (z), (39)
n,j n,j
z z
→−∞ →∞
and θˆ(j) is unique if ψˆ is strictly decreasing and the design matrix has full column rank. If ψˆ is
n n,j n,j
continuous, then any minimiser θˆ(j) satisfies
n
(cid:88) (X˜ X¯ ) ψˆ (cid:0) Y µ¯(j) X¯ θ¯(j) (X˜ X¯ ) θˆ(j)(cid:1) =0.
i − n,j · n,j i − n − n⊤,j n − i − n,j ⊤ n
i ∈Ij+2
This is a variant of the efficient score equations (van der Vaart, 1998, Chapter 25.8) based on (37) and
suitable estimators of the nuisance parameters µ ,ψ respectively. Since we do not assume knowledge
0 0∗
of the population mean E(X˜ ) Rd 1, we replace it with its sample analogue X¯ in each of the three
1 − n,j
∈
folds.
Under the regularity conditions in Section 3.1, it turns out that the projected score functions ψˆ
n,j
are also L2(P )-consistent estimators of ψ in this setting. It follows that with probability tending to 1
0 0∗
as n , (39) holds and hence θˆ and θˆ exist. We then adapt the proof strategy for Theorem 14
→ ∞
n† n‡
above to establish a similar antitonic efficiency result.
Theorem 16. Suppose that (A1)–(A5) hold in the linear model (35). Then for any sequence of estima-
tors (θˆ ) such that θˆ θˆ ,θˆ for each n, we have
n n
∈{
n† n‡
}
(cid:18) Cov(X˜ ) 1(cid:19)
√n(θˆ θ ) d N 0, 1 −
n 0 d 1
− → − i ∗(p 0)
as n .
→∞
Zou and Yuan (2008) introduced a robust alternative to least squares called the composite quantile
regressionestimator,whichremains√n-consistentwhentheerrorvarianceisinfinitebutalsohasasymp-
toticefficiencyatleast70%thatofOLSundersuitableconditionsontheerrordensityp . Thisisachieved
0
byborrowingstrengthacrossseveralquantilesoftheconditionaldistributionofY givenX ,ratherthan
1 1
targeting just a single quantile (e.g. the conditional median) using the quantile loss ℓ : R R given by
τ
ℓ (z):=(τ 1 )z for τ (0,1). More precisely, for K (0, ), let τ :=k/(K+1) fo→ r k [K] and
τ z<0 k
− { } ∈ ∈ ∞ ∈
define
K n
(cid:88)(cid:88)
(θˆCQ,K,µˆ ,...,µˆ ) argmin ℓ (Y µ X˜ θ), (40)
n n,1 n,K ∈ τk i − k − i⊤
(θ,µ1,...,µK) k=1i=1
wheretheargministakenoverallθ Rd 1 andµ ,...,µ R. Underassumption(2)ofZouandYuan
− 1 K
∈ ∈
(2008) in our random design setting, it follows from their Theorems 2.1 and 3.1 that
√n(θˆCQ,K θ ) d N (cid:0) 0,V Cov(X˜ ) 1(cid:1) ,
n − 0 → d −1 p0,CQ,K · 1 −
where, writing J
0
=p
0
◦F 0−1 for the density quantile function of the errors, we have
V :=
(cid:80)K
k,k′=1τ k ∧k′(1 −τ k ∨k′) 1
=:V
p0,CQ,K (cid:0)(cid:80)K J (τ )(cid:1)2 → 12(cid:0)(cid:82)1 J (cid:1)2 p0,CQ
k=1 0 k 0 0
21as K . For every p satisfying their assumptions, Zou and Yuan (2008, Theorem 3.1) established
0
→ ∞
that the CQR estimator (in the notional limit K ) has asymptotic relative efficiency
→∞
V (ψ ) (cid:18)(cid:90) 1 (cid:19)2(cid:90) 6
p0 OLS =12 J z2p (z)dz > 0.703
0 0
V p0,CQ 0 R eπ ≈
relative to OLS, where ψ (z) := z for z R. Nevertheless, Theorem 16 and the following result
OLS
− ∈
imply that the asymptotic covariance of the ‘limiting’ CQR estimator is always at least that of the
semiparametric convex M-estimator θˆ in our framework. Moreover, the former estimator can have
n
arbitrarily low efficiency relative to the latter, even when p is log-concave.
0
Lemma 17. For every uniformly continuous density p , we have V 1/i (p ), with equality if and
0 p0,CQ
≥
∗ 0
only if either i (p )= or p is a logistic density of the form
∗ 0 0
∞
λe λ(z µ)
− −
p (z)=
0 (1+e λ(z µ))2
− −
for z R, where µ R and λ>0. Moreover,
∈ ∈
1
inf =0.
p0∈PLC:i(p0)<
∞
i ∗(p 0)V p0,CQ
3.3 Inference
To perform asymptotically valid inference for β based on Theorems 14 and 16, we require a consistent
0
estimator of the antitonic information i (p ) when p is unknown.
∗ 0 0
Lemma 18. Suppose that (A1)–(A5) hold for a linear model (27) in which either p is symmetric or
0
X =1 almost surely. For j 1,2,3 , letting ε˘ :=Y X β¯(j) for i I , we have
1d
∈{ }
i i
−
i⊤ n
∈
j+2
3
ˆı := 1 (cid:88) (cid:88) ψˆ (ε˘)2 p i (p )
n n,j i ∗ 0
n →
j=1i ∈Ij+2
as n .
→∞
The antitonic information matrix I (β ) can therefore be estimated consistently by the observed
∗ 0
antitonic information matrix
n
Iˆ := ˆı n (cid:88) X X = ˆı n X X,
n
n
i i⊤
n
⊤
i=1
where X = (X X ) Rn d has full column rank with probability tending to 1 under (A1). It
1 n ×
··· ∈
follows from Theorem 14 and Lemma 18 that
√nIˆ1/2(βˆ β ) d N (0,I )
n n − 0 → d d
when p is symmetric. Thus, writing βˆ for the jth component of βˆ and vˆ := (Iˆ 1) for the jth
0 n,j n j n− jj
diagonal entry of Iˆ 1, we have that
n−
(cid:20) (cid:114) (cid:114) (cid:21)
vˆ vˆ
βˆ z j , βˆ +z j
n,j − α/2 n n,j α/2 n
is an asymptotic (1 α)-level confidence interval for the jth component of β , where z denotes the
0 α/2
−
(1 α/2)-quantile of the standard normal distribution. Moreover,
−
(cid:8) b Rd :n(βˆ b) Iˆ (βˆ b) χ2(α)(cid:9)
∈ n − ⊤ n n − ≤ d
is an asymptotic (1 α)-confidence ellipsoid for β , where χ2(α) denotes the (1 α)-quantile of the χ2
d wi hst er ri ebu X¯t nio :n =. nS −im 1(cid:80)ila−
n
ir =ly 1, X˜w ih ,e an ndX
l1 ed
t=
v˜
j
1 :=a (lm
I(cid:101)
n−o 1s )t jjsu fr o0 e rly j, ∈de [dfin −ed 1I(cid:101)
]n
.
T:= hen
n−
b1 y(cid:80) Tn
i h= e1
oˆı
rn
e(− mX˜
i 1−
6
aX¯
nn
d) L(X e˜
mi −
maX¯
1n
8) ,⊤d ,
(cid:20) (cid:114) (cid:114) (cid:21)
v˜ v˜
θˆ z j , θˆ +z j
n,j − α/2 n n,j α/2 n
22is an asymptotic (1 α)-level confidence interval for the jth component of θ for each j [d 1], and
0
− ∈ −
(cid:8) v ∈Rd −1 :n(θˆ
n
−v) ⊤I(cid:101)n(θˆ
n
−v) ≤χ2
d
−1(α)(cid:9) (41)
is an asymptotic (1 α)-confidence ellipsoid for θ . Lemma 18 also ensures that standard linear model
0
−
diagnostics, either based on heuristics such as Cook’s distances (Cook, 1977), or formal goodness-of-fit
tests (Jankov´a et al., 2020), can be applied.
4 Numerical experiments
In our numerical experiments, we generate covariates X˜ ,...,X˜ iid N (1 ,I ), where 1 de-
1 n d 1 d 1 d 1 d 1
∼ − − − −
notes a (d 1)-dimensional all-ones vector, and responses Y ,...,Y according to the linear model (35)
1 n
−
iid
with θ =3. We generate independent errors ε ,...,ε P for the following choices of P :
0 2 1 n 0 0
∥ ∥ ∼
(i) P is standard Gaussian.
0
(ii) P is standard Cauchy.
0
(iii) Gaussian scale mixture: P = 1N(0,1)+ 1N(0,16).
0 2 2
(iv) Gaussianlocationmixture: P = 1N(cid:0) 3, 1 (cid:1) +1N(cid:0)3, 1 (cid:1) . Thisdistributionissimilartotheone
0 2 −2 100 2 2 100
constructedin theproofof Proposition6to show that log-concave maximum likelihood estimation
of the error distribution may result in arbitrarily large efficiency loss.
(v) Smoothed uniform: P is the distribution of U + 1 Z, where U Unif[ 1,1] and Z N(0,1) are
0 10 ∼ − ∼
independent.
(vi) Smoothedexponential: P isthedistributionofW 1+√3Z,whereW Exp(1)andZ N(0,1)
0 − 10 ∼ ∼
are independent. We choose the standard deviation √3 for the Gaussian component so that the
10
ratio of the variances of the non-Gaussian and the Gaussian components is the same as that in the
smoothed uniform setting.
In cases (i), (v) and (vi) P is log-concave, while for the other settings it is not.
0
Oracle ASM Alt LCMLE 1S LAD OLS
Standard Gaussian 8.34 8.88 8.86 10.73 9.72 13.11 8.34
Standard Cauchy 20.07 20.61 20.76 23.85 22.13 21.36 2.03 106
×
Gaussian scale mixture 31.36 32.01 32.33 34.89 36.18 34.97 72.34
Gaussian location mixture 0.16 0.17 0.16 1.51 18.07 319.65 18.51
Smoothed uniform 1.02 1.29 1.15 1.52 2.07 7.92 2.78
Smoothed exponential 1.78 2.13 2.02 2.54 3.36 8.27 8.65
Table 1: Squared estimation error ( 103) for different estimators, with n=600 and d=6.
×
Wecomparedtheperformanceoftwoversionsofourprocedurewithanoracleapproachandfourex-
istingmethods. Thefirstvariantofourprocedure,whichwerefertoasASM(antitonicscorematching)
in all of the plots, is as described in Section 3, except that we do not perform sample splitting, cross-
fitting or truncation of the initial score estimates. These devices are convenient for theoretical analysis
but not essential in practice. More precisely, ASM first constructs a pilot estimator (θ¯ ,µ¯ ), and then
n n
uses the vector of residuals (cid:0) εˆ ,...,εˆ (cid:1) , where εˆ := Y µ¯ X˜ θ¯ , to obtain an initial kernel-based
1 n i i
−
n
−
i⊤ n
scoreestimatorψ˜ ,formedusingaGaussiankernelandthedefaultSilverman’schoiceofbandwidth(Sil-
n
verman,1986, p.48). FollowingStep3 inSection3.2, weestimatetheantitonicprojectedscoreandthe
′
corresponding convex loss function by ψˆ
n
:= M(cid:99)R(ψ˜
n
◦F˜ n−1) ◦F˜
n
and ℓˆ
n
respectively, where F˜
n
denotes
thedistributionfunctionassociatedwiththekerneldensityestimate. Finally,weuseNewton’salgorithm
to compute our semiparametric estimator
n
θˆASM argmin(cid:88) ℓˆ (cid:0) Y µ¯ X¯ θ¯ (X˜ X¯ ) θ(cid:1) ,
n ∈ θ ∈Rd−1 i=1 n i − n − n⊤ n − i − n ⊤
23Standard Gaussian Standard Cauchy
25
20
10
15
10
5
5
0 0
ASM Alt LCMLE 1S OLS LAD ASM Alt LCMLE 1S LAD
Gaussian scale mixture Gaussian location mixture
60 15
40 10
20 5
0 0
ASM Alt LCMLE 1S OLS LAD ASM Alt LCMLE 1S OLS
Smoothed uniform Smoothed exponential
8
7.5
6
5.0
4
2.5
2
0 0.0
ASM Alt LCMLE 1S OLS LAD ASM Alt LCMLE 1S OLS LAD
Figure 7: Plots of the average squared error loss ( 103) of different estimators for noise distributions
×
(i)–(vi), with n=600 and d=6. In each plot, the red dashed line indicates the corresponding value for
the oracle convex M-estimator, and we omit the estimators that have very large estimation error (see
Table 1 for full details).
where X¯ :=n 1(cid:80)n X˜ .
n − i=1 i
In the second version of our procedure, which we refer to as Alt in our plots, we implement the
empirical analogue of the alternating optimisation procedure described at the beginning of Section 3.
We start with an uninformative initialiser (θˆ(0),µˆ(0)) = (0,0) Rd 1 R, and then alternate between
n n −
the following steps for t N: ∈ ×
∈
I. Compute residuals εˆ( it −1) := Y
i
−µˆ( nt −1) −X˜ i⊤θˆ n(t −1) for i
∈
[n] and hence estimate the antitonic
projected score ψˆ n(t −1) and the corresponding convex loss ℓˆ( nt −1) as for ASM.
II. Update (θˆ n(t),µˆ( nt)) ∈argmin (θ,µ) ∈Rd−1 ×R(cid:80)n i=1ℓˆ( nt −1)(Y i −µ −X˜ i⊤θ).
We iterate these steps until convergence of the empirical score matching objective
n
Dˆ (ψˆ(t);εˆ(t),...,εˆ(t))= 1 (cid:88)(cid:8) ψˆ(t)(εˆ(t))+2(ψˆ(t))(εˆ(t))(cid:9)
n n 1 n n n i n ′ i
i=1
defined in (24). We note that the iterates µˆ(t) and ψˆ(t) are not guaranteed to converge due to non-
n n
identifiability of the intercept term µ in (35), but θˆ(t) and the score matching objective values did
0 n
indeed converge in all of our experiments.
24
301
·
ESM
301
·
ESM
301
·
ESM
301
·
ESM
301
·
ESM
301
·
ESMThe alternative approaches that we consider are as follows:
• Oracle: The M-estimator θˆoracle, where
n
(cid:18) θˆoracle(cid:19) (cid:88)n
n argmin ℓ (Y µ X˜ θ)
µˆo nracle ∈ (θ,µ) ∈Rd−1 ×R i=1 ∗0 i − − i⊤
is defined with respect to the optimal convex loss function ℓ . Although θˆoracle asymptotically
∗0 n
attains antitonic efficiency in the sense of Theorem 16, it is not a valid estimator in our semipara-
metric framework since it requires knowledge of p .
0
• LAD: The least absolute deviation estimator θˆLAD, where
n
(cid:18) θˆLAD(cid:19) (cid:88)n
n argmin Y µ X˜ θ .
µˆL nAD ∈ (θ,µ) ∈Rd−1 ×R i=1| i − − i⊤ |
• OLS: The ordinary least squares estimator θˆOLS, where
n
(cid:18) θˆOLS(cid:19) (cid:88)n
n argmin (Y µ X˜ θ)2.
µˆO nLS ∈ (θ,µ) ∈Rd−1 ×R i=1 i − − i⊤
• 1S:Thesemiparametricone-stepmethod,wherewestartwithapilotestimator(θ¯ ,µ¯ ),computea
n n
(notnecessarilydecreasing)nonparametricscoreestimate,andthenupdateθ¯ withasingleNewton
n
stepinsteadofsolvingtheestimatingequationsexactly. OurimplementationfollowsvanderVaart
(1998, Chapter 25.8): we split the data into two folds of equal size indexed by I and I , and then
1 2
use the residuals εˆ := Y X˜ θ¯ µ¯ for i I and i I separately to obtain kernel density
estimates pˆ ,pˆ
i
of p
i
(c−
onsti⊤ rucn
t− ed
an
s for A∈
SM1
). Defin∈
ing2
the score estimates ψˆ :=pˆ /pˆ
n,1 n,2 0 n,j ′n,j n,j
for j 1,2 , we output the cross-fitted estimator θˆ1S, where
∈{ } n
(cid:18) θˆ n1S(cid:19) :=(cid:18) θ¯ n(cid:19) (cid:18) (cid:88)
ψˆ (εˆ)2X X
+(cid:88)
ψˆ (εˆ)2X X
(cid:19) −1
µˆ1 nS µ¯ n −
i ∈I1
n,2 i i i⊤
i ∈I2
n,1 (cid:18)i i i⊤
(cid:19)
(cid:88) (cid:88)
ψˆ (εˆ)X + ψˆ (εˆ)X .
n,2 i i n,1 i i
i ∈I1 i ∈I2
• LCMLE: We estimate the error density p using the log-concave maximum likelihood estima-
0
tor(Culeetal.,2010;Du¨mbgenetal.,2011,2013). Moreprecisely,againwriting forthesetof
LC
P
univariatelog-concavedensities, startwithapilotestimator(θˆ(0),µˆ(0))andalternatethefollowing
n n
two steps for t N:
∈
n
(cid:88)
pˆ(t) argmax logp(Y µˆ(t 1) X˜ θˆ(t 1))
n ∈ i − n− − i⊤ n−
p ∈PLC i=1
(cid:32) (cid:33)
θˆ(t) (cid:88)n
n argmax logpˆ(t)(Y µ X˜ θ).
µˆ( nt) ∈ (θ,µ) ∈Rd−1 ×R i=1 n i − − i⊤
Where required, we took the pilot estimator to be the least absolute deviation estimator for all
methods except in the Gaussian location mixture setting (iv). Here, we chose (θˆOLS,µˆOLS) to be the
n n
pilot estimator since p (0) is close to 0 and hence V (ψ) = 1/(cid:0) 4p (0)2(cid:1) is very large for θˆLAD, where
0 p0 0 n
ψ()= sgn().
· − ·
4.1 Estimation accuracy
In all of the experiments in this subsection, we set d = 6, µ = 2 and drew θ uniformly at random
0 0
from the centred Euclidean sphere in Rd 1 of radius 3. For each of the estimators above, the average
−
squared Euclidean norm errors θˆ θ 2 were computed over 200 repetitions. In the first experiment,
n 0
∥ − ∥
we consider each error distribution in turn and compare the average squared estimation error when
25n 600 1200 2400 n 600 1200 2400 n 600 1200 2400
ASM 0.26 0.61 1.93 ASM 0.27 0.61 1.81 ASM 0.21 0.58 1.96
Alt 4.35 9.99 27.85 Alt 5.03 10.52 28.64 Alt 1.84 3.72 11.36
LCMLE 0.64 2.21 8.34 LCMLE 0.71 1.36 2.72 LCMLE 3.15 7.09 9.29
1S 0.10 0.15 0.29 1S 0.11 0.15 0.29 1S 0.12 0.15 0.29
Standard Gaussian Gaussian scale mixture Gaussian location mixture
Table 2: Mean execution time in seconds for different semiparametric estimators when d=100.
Standard Gaussian Gaussian scale mixture Gaussian location mixture
200 1000
20
150 750
15
100 500
10
50 250 5
0 0 0
ASM Alt LCMLE 1S LAD OLS ASM Alt LCMLE 1S LAD OLS ASM Alt LCMLE
Figure 8: Plot of n (average squared loss) for different estimators, with d = 100 and sample sizes
×
n 600,1200,2400 corresponding to the yellow, green, and pink bars respectively. The black, blue
∈ { }
and red dashed lines indicate the values of n (average squared loss) for the oracle convex M-estimator
×
when n=600,1200,2400 respectively.
n = 600. The results are presented in Table 1 and Figure 7. We observe that our proposed procedures
ASMandAlthavethelowestestimationerrorexceptinthecaseofstandardGaussianerror,whereOLS
coincideswiththeoracleconvexlossestimatorandhasaslightlylowerestimationerror. Itisinteresting
thattheone-stepestimator(1S)canperformverypoorlyinfinitesamples,andinparticular,maybarely
improve on its initialiser. In all settings considered, ASM and Alt have comparable error.
Next,weinvestigatetherunningtimeandestimationaccuracyofthesemiparametricestimatorsASM,
Alt, LCMLEand1Sformoderatelylargesamplesizesn 600,1200,2400 anddimensiond=100. We
∈{ }
consider in turn the error distributions (i), (iii) and (iv); see Table 2 and Figure 8. We see that ASM in
particular is competitive in terms of its running time, and that the performance of our procedures does
not deteriorate relative to its competitors for this larger choice of d.
OurthirdnumericalexperimentdemonstratesthattheestimationerrorofASMiscomparabletothat
of the oracle convex M-estimator even for small sample sizes. For n 50,100,150,200,300 and d=6,
∈{ }
and error distributions (i), (ii) and (iv), we record in Figure 9 the average estimation error of Oracle,
ASM, and LCMLE. With the exception of the smallest sample size n = 50 in the Gaussian location
mixture setting, the estimation error of ASM tracks that of the oracle very closely; on the other hand,
LCMLE is somewhat suboptimal, particularly in the Gaussian location mixture setting.
Finally in this subsection, we verify empirically the suboptimality of LCMLE relative to ASM for
larger sample sizes. We let d = 2 and n 800,1600,3200,6400 , and consider error distribution (iv)
∈ { }
aboveaswellasthet distribution(Example26). Weincludethet distributioninplaceofthestandard
2 2
Cauchy because when P does not have a finite first moment, its population level log-concave maximum
0
likelihoodprojectiondoesnotexist(andindeedwefoundempiricallythatLCMLEwashighlyunstablein
theCauchysetting). Figure10displaystheaveragesquarederrorlossoftheoracleconvexM-estimator,
ASM, and LCMLE. We see that the relative efficiency of ASM with respect to Oracle approaches 1 as n
increases, while LCMLE has an efficiency gap that does not vanish even for large n, in agreement with
the asymptotic calculation in Example 26.
26
ezis
elpmas
·
ESM
ezis
elpmas
·
ESM
ezis
elpmas
·
ESMStandard Normal Standard Cauchy Gaussian location mixture
Oracle Oracle Oracle
ASM ASM ASM
LCMLE LCMLE LCMLE
50 100 150 200 250 300 50 100 150 200 250 300 50 100 150 200 250 300
Sample Size Sample Size Sample Size
Figure 9: Comparison of the mean squared errors of different convex M-estimators for sample sizes
n 50,100,150,200,250,300 and d=6.
∈{ }
Gaussian location mixture t distribution
2
Algorithm Algorithm
ASM ASM
Oracle Oracle
−9 LCMLE −5.5 LCMLE
−6.0
−10
−6.5
−11
−7.0
−12
7.0 7.5 8.0 8.5 7.0 7.5 8.0 8.5
log(Sample Size) log(Sample Size)
Figure 10: Log-log plots of the average squared error loss against sample size for three convex M-
estimators. In each experiment, we set n 800,1600,3200,6400 and d=2.
∈{ }
4.2 Inference
Inthissubsection,weassessthefinite-sampleperformanceoftheinferentialproceduresforθ describedin
0
Section 3.3, but do not perform sample splitting or cross-fitting. Here, we instead estimate the antitonic
information i (p ) by
∗ 0
n
1 (cid:88)
ȷˆ := ψˆ (εˆ)2,
n n i
n
i=1
rather thanˆı , where εˆ :=Y µ¯ X˜ θ¯ for i [n] is the ith residual of the pilot estimator (θ¯ ,µ¯ ).
We then
comn
pute ˜
:=i
ȷ˜n
(cid:80)i
n−
(n
X˜−
Xi⊤
¯
n
)(X˜ ∈ X¯ ) .
n n
Jn n i=1 i − n i − n ⊤
We first verify that the standardised error vector √n ˜1/2(θˆASM θ ) has a distribution very close
Jn n − 0
to N (0,I ) under three different noise distributions: standard Gaussian, standard Cauchy, and the
d 1 d 1
Gauss− ian mix− ture P = 2N(0,1)+ 1N(cid:0)1,9(cid:1) . Setting n = 600 and d = 4, we present the Q-Q plot for
0 3 3 2
8000 repetitions in Figure 11. Moreover, Table 4a demonstrates that the average squared estimation
(cid:0) (cid:1)2
error ȷˆ i (p ) is small.
n ∗ 0
−
Next, we show that the empirical coverage of the confidence ellipsoid
CˆASM :=(cid:8) v Rd 1 :n(θˆASM v) ˜ (θˆASM v) χ2 (α)(cid:9)
n ∈ − n − ⊤ Jn n − ≤ d −1
is close to the nominal level 1 α when n = 600. Indeed, for the standard Gaussian and Gaussian
−
mixturenoisedistributionsabove, Table3revealsthatwemaintainnominalcoveragewhileforstandard
27
rorrE
derauqS
naeM
)ESM(gol
01.0
80.0
60.0
40.0
20.0
rorrE
derauqS
naeM
53.0
52.0
51.0
50.0
)ESM(gol
rorrE
derauqS
naeM
020.0
510.0
010.0
500.0
000.0Standard Cauchy: component one Gaussian mixture: component one Standard Gaussian: component one
−4 −2 0 2 4 −4 −2 0 2 4 −4 −2 0 2 4
Theoretical Quantiles Theoretical Quantiles Theoretical Quantiles
Standard Cauchy: component two Gaussian mixture: component two Standard Gaussian: component two
−4 −2 0 2 4 −4 −2 0 2 4 −4 −2 0 2 4
Theoretical Quantiles Theoretical Quantiles Theoretical Quantiles
Figure 11: Q-Q plots of the standardised errors of estimates of two components of θ . The blue and
0
green vertical lines mark the 90% and 95% theoretical quantiles respectively. In each experiment, we set
n=600 and d=4, and perform 8000 repetitions.
OLS OLS
ASM ASM
q q
0 0
0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8
(a) Standard normal (b) Gaussian mixture
Figure 12: The projection of the 95% confidence ellipsoid of θˆASM onto the first two dimensions (blue)
and the projection of the 95% confidence ellipsoid of θˆOLS onto the first two dimensions (black dashed).
n
The blue dot, black circle and red star correspond to θˆASM, θˆOLS, and θ respectively.
n 0
28
selitnauQ
elpmaS
selitnauQ
elpmaS
8.0
6.0
4.0
2.0
0.0
4
2
0
2−
4−
4
2
0
2−
4−
selitnauQ
elpmaS
selitnauQ
elpmaS
4
2
0
2−
4−
4
2
0
2−
4−
8.0
6.0
4.0
2.0
0.0
selitnauQ
elpmaS
selitnauQ
elpmaS
4
2
0
2−
4−
4
2
0
2−
4−θ θ θ θ θ θ θ θ θ
0,1 0,2 0,3 0,1 0,2 0,3 0,1 0,2 0,3
95% CI 0.965 0.965 0.964 95% CI 0.952 0.955 0.955 95% CI 0.950 0.954 0.952
90% CI 0.924 0.925 0.924 90% CI 0.905 0.908 0.908 90% CI 0.903 0.902 0.901
Standard Cauchy Gaussian mixture Standard Gaussian
Table 3: Empirical coverage of the confidence intervals for each coordinate of θ . In each experiment,
0
we set n=600 and d=4, and perform 8000 repetitions.
Cauchy Mixture Gaussian Cauchy Mixture Gaussian
i (p ) 0.44 0.46 1.0
Vol(CˆASM)
∗ 0 n 0.001 0.49 1.14
RMSE(ȷˆ n) 0.01 0.05 0.1 Vol(Cˆ nOLS)
(a) Root mean squared errors of ȷˆn. (b) Mean ratio of volumes of Cˆ nASM and Cˆ nOLS.
Table 4: Accuracy of estimates of the antitonic information, and comparison of the volumes of the two
confidence ellipsoids. In each experiment, we set n=600 and d=4, and perform 8000 repetitions.
Cauchy errors, our confidence set is slightly conservative. We also compare the volume of CˆASM with
n
that of the OLS confidence ellipsoid
CˆOLS :=(cid:8) v Rd 1 :n(θˆOLS v) IˆOLS(θˆOLS v) χ2 (α)(cid:9) ,
n ∈ − n − ⊤ n n − ≤ d −1
whereIˆOLS :=n 1(cid:80)n σˆ 2(X˜ X¯ )(X˜ X¯ ) andσˆ2 :=n 1(cid:80)n (Y µˆOLS X˜ θˆOLS)2. Weplot
examplen
s of
CˆAS−
M
andi= C1 ˆOn−
LS
ini F− igun
re
12i ,− andn c⊤ omputen
the
av− eragei= ra1 tioi −
of
tn heir− volui⊤ mn
es in Table 4b.
n n
The ASM ellipsoid is appreciably smaller than the OLS ellipsoid except when the noise distribution is
standard Gaussian, in which case the OLS ellipsoid is slightly smaller.
5 Discussion
DespitetheGauss–Markovtheorem,oneofthemessagesofthispaperisthatthesuccessofordinaryleast
squares is relatively closely tied to Gaussian or near-Gaussian error distributions. Our antitonic score
matchingapproachrepresentsamiddlegroundthatfreesthepractitionerfromtheGaussianstraitjacket
whileretainingtheconvenienceandstabilityofworkingwithconvexlossfunctions. TheFisherdivergence
projection framework brings together previously disparate ideas on shape-constrained estimation, score
matching, information theory and classical robust statistics. Given the prevalence of procedures in
statisticsandmachinelearningthatareconstructedasoptimisersofpre-specifiedlossfunctions, welook
forward to seeing how related insights may lead to more flexible, data-driven approaches that combine
robustness and efficiency.
Acknowledgements: The authors thank Cun-Hui Zhang for helpful discussions. The research of
YCK and MX was supported by National Science Foundation grants DMS-2311299 and DMS-2113671;
OYF and RJS were supported by Engineering and Physical Sciences Research Council Programme
Grant EP/N031938/1, while RJS was also supported by European Research Council Advanced Grant
101019498.
References
Amari, S.-i. and Nagaoka, H. (2000). Methods of Information Geometry, volume 191. American Mathe-
matical Society.
Arcones, M. A. (1998). Asymptotic theory for M-estimators over a convex kernel. Econometric Theory,
14(4):387–422.
Barber, R. F. and Samworth, R. J. (2021). Local continuity of log-concave projection, with applications
to estimation under model misspecification. Bernoulli, 27(4):2437–2472.
29Barron, J. T. (2019). A general and adaptive robust loss function. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 4331–4339.
Bean, D., Bickel, P. J., El Karoui, N., and Yu, B. (2013). Optimal M-estimation in high-dimensional
regression. Proceedings of the National Academy of Sciences, 110(36):14563–14568.
Benton,J.,Shi,Y.,DeBortoli,V.,Deligiannidis,G.,andDoucet,A.(2024). Fromdenoisingdiffusionsto
denoising Markov models. Journal of the Royal Statistical Society, Series B: Statistical Methodology,
to appear.
Beran, R. (1978). An efficient and robust adaptive estimator of location. The Annals of Statistics,
6(2):292–313.
Betancourt, M., Byrne, S., Livingstone, S., and Girolami, M. (2017). The geometric foundations of
Hamiltonian Monte Carlo. Bernoulli, 23(4A):2257–2298.
Bickel, P. J. (1975). One-step Huber estimates in the linear model. Journal of the American Statistical
Association, 70(350):428–434.
Bickel, P. J. (1982). On adaptive estimation. The Annals of Statistics, 10(3):647–671.
Bobkov, S. G. (1996). Extremal properties of half-spaces for log-concave distributions. The Annals of
Probability, 24(1):35–48.
Boyd, S. P. and Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press.
Brunel, V.-E. (2023). Geodesically convex M-estimation in metric spaces. In The Thirty Sixth Annual
Conference on Learning Theory, pages 2188–2210. PMLR.
Catoni, O. (2012). Challenging the empirical mean and empirical variance: a deviation study. Annales
de l’Institut Henri Poincar´e – Probabilit´es et Statistiques, 48(4):1148–1185.
Chen, Y. and Samworth, R. J. (2013). Smoothed log-concave maximum likelihood estimation with
applications. Statistica Sinica, 23(3):1373–1398.
Cheng, X., Chatterji, N. S., Bartlett, P. L., and Jordan, M. I. (2018). Underdamped Langevin MCMC:
a non-asymptotic analysis. In Conference on Learning Theory, pages 300–323. PMLR.
Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins, J.
(2018). Double/debiasedmachinelearningfortreatmentandstructuralparameters. TheEconometrics
Journal, 21(1):C1–C68.
Chinot,G.,Lecu´e,G.,andLerasle,M.(2020). RobuststatisticallearningwithLipschitzandconvexloss
functions. Probability Theory and Related Fields, 176(3):897–940.
Cook,R.D.(1977). Detectionofinfluentialobservationinlinearregression. Technometrics,19(1):15–18.
Cover, T. M. and Thomas, J. A. (2006). Elements of Information Theory. John Wiley & Sons, 2nd
edition.
Cox, D. D. (1985). A penalty method for nonparametric estimation of the logarithmic derivative of a
density function. Annals of the Institute of Statistical Mathematics, 37(2):271–288.
Cule,M.,Samworth,R.,andStewart,M.(2010). Maximumlikelihoodestimationofamulti-dimensional
log-concave density. Journal of the Royal Statistical Society Series B: Statistical Methodology,
72(5):545–607.
Dalalyan, A. S., Golubev, G. K., and Tsybakov, A. B. (2006). Penalized maximum likelihood and
semiparametric second-order efficiency. The Annals of Statistics, 34(1):169–201.
DeBortoli,V.,Mathieu,E.,Hutchinson,M.,Thornton,J.,Teh,Y.W.,andDoucet,A.(2022). Rieman-
nian score-based generative modelling. Advances in Neural Information Processing Systems, 35:2406–
2422.
30Derenski,J.,Fan,Y.,James,G.,andXu,M.(2023). AnempiricalBayesshrinkagemethodforfunctional
data. Submitted.
Donoho, D. L. and Montanari, A. (2015). Variance breakdown of Huber M-estimators: n/p (1, ).
∈ ∞
arXiv preprint arXiv:1503.02106.
Doss, C. R. and Wellner, J. A. (2019). Univariate log-concave density estimation with symmetry or
modal constraints. Electronic Journal of Statistics, 13(2):2391–2461.
Du¨mbgen, L., Samworth, R., and Schuhmacher, D. (2011). Approximation by log-concave distributions,
with applications to regression. The Annals of Statistics, 39(2):702–730.
Du¨mbgen, L., Samworth, R. J., and Schuhmacher, D. (2013). Stochastic search for semiparametric
linear regression models. In From Probability to Statistics and Back: High-Dimensional Models and
Processes–AFestschriftinHonorofJonA.Wellner,volume9,pages78–91.InstituteofMathematical
Statistics.
Efron, B. (2011). Tweedie’s formula and selection bias. Journal of the American Statistical Association,
106(496):1602–1614.
Eggermont, P. P. B. and LaRiccia, V. N. (2000). Maximum likelihood estimation of smooth monotone
and unimodal densities. The Annals of Statistics, 28(3):922–947.
El Karoui, N., Bean, D., Bickel, P. J., Lim, C., and Yu, B. (2013). On robust regression with high-
dimensional predictors. Proceedings of the National Academy of Sciences, 110(36):14557–14562.
Faraway, J. J. (1992). Smoothing in adaptive estimation. The Annals of Statistics, 20(1):414–427.
Folland,G.B.(1999). Real Analysis: Modern Techniques and their Applications,volume40. JohnWiley
& Sons.
Groeneboom, P. and Jongbloed, G. (2014). Nonparametric Estimation under Shape Constraints. Cam-
bridge University Press.
Gupta, S., Lee, J. C. H., and Price, E. (2023). Finite-sample symmetric mean estimation with Fisher
information rate. In The Thirty Sixth Annual Conference on Learning Theory, pages 4777–4830.
PMLR.
Hampel, F. R. (1974). The influence curve and its role in robust estimation. Journal of the American
Statistical Association, 69(346):383–393.
Hampel, F. R., Ronchetti, E. M., Rousseeuw, P. J., and Stahel, W. A. (2011). Robust Statistics: The
approach based on influence functions. John Wiley & Sons.
Hansen, B. E. (2022). A modern Gauss–Markov theorem. Econometrica, 90(3):1283–1294.
He, X. and Shao, Q.-M. (2000). On parameters of increasing dimensions. Journal of Multivariate
Analysis, 73(1):120–135.
Hoerl,A.E.andKennard,R.W.(1970).Ridgeregression: biasedestimationfornonorthogonalproblems.
Technometrics, 12(1):55–67.
Huber, P. J. (1964). Robust estimation of a location parameter. The Annals of Mathematical Statistics,
35(1):73–101.
Huber, P. J. (1967). The behavior of maximum likelihood estimates under nonstandard conditions. In
Proceedings of the fifth Berkeley Symposium on Mathematical Statistics and Probability, volume 1,
pages 221–233. Berkeley, CA: University of California Press.
Huber, P. J. and Ronchetti, E. M. (2009). Robust Statistics. Wiley, 2nd edition.
Hyv¨arinen, A. (2005). Estimation of non-normalized statistical models by score matching. Journal of
Machine Learning Research, 6:695–709.
31Hyv¨arinen, A. (2007). Some extensions of score matching. Computational Statistics & Data Analysis,
51(5):2499–2512.
Jankov´a, J., Shah, R. D., Bu¨hlmann, P., and Samworth, R. J. (2020). Goodness-of-fit testing in high
dimensional generalized linear models. Journal of the Royal Statistical Society Series B: Statistical
Methodology, 82(3):773–795.
Jin, K. (1990). Empirical Smoothing Parameter Selection in Adaptive Estimation. University of Califor-
nia, Berkeley.
Johnson, O. (2004). Information Theory and the Central Limit Theorem. World Scientific.
Johnson, O. and Barron, A. (2004). Fisher information inequalities and the central limit theorem.
Probability Theory and Related Fields, 129:391–409.
Jolicoeur-Martineau, A., Pich´e-Taillefer, R., Combes, R. T. d., and Mitliagkas, I. (2020). Adversarial
score matching and improved sampling for image generation. arXiv preprint arXiv:2009.05475.
Jones, M. C. (1992). Estimating densities, quantiles, quantile densities and density quantiles. Annals of
the Institute of Statistical Mathematics, 44:721–727.
Kao, Y.-C., Xu, M., and Zhang, C.-H. (2023). Rate adaptive estimation of the center of a symmetric
distribution. arXiv preprint arXiv:2303.01992.
Koehler, F., Heckett, A., andRisteski, A.(2022). Statisticalefficiencyofscorematching: Theviewfrom
isoperimetry. arXiv preprint arXiv:2210.00726.
Laha, N. (2021). Adaptive estimation in symmetric location model under log-concavity constraint.
Electronic Journal of Statistics, 15(1):2939–3014.
Lederer, J. and Oesting, M. (2023). Extremes in high dimensions: methods and scalable algorithms.
arXiv preprint arXiv:2303.04258.
Lei, L. and Wooldridge, J. (2022). What estimators are unbiased for linear models? arXiv preprint
arXiv:2212.14185.
Lerasle,M.(2019). Selectedtopicsonrobuststatisticallearningtheory. arXivpreprintarxiv:1908.10761.
Ley, C. and Swan, Y. (2013). Stein’s density approach and information inequalities. Electronic Commu-
nications in Probability, 18:1–14.
Li, G., Wei, Y., Chen, Y., and Chi, Y. (2023). Towards faster non-asymptotic convergence for diffusion-
based generative models. arXiv preprint arXiv:2306.09251.
Loh, P.-L. (2021). Scale calibration for high-dimensional robust regression. Electronic Journal of Statis-
tics, 15(2):5933–5994.
Lyu, S. (2012). Interpretation and generalization of score matching. arXiv preprint arXiv:1205.2629.
Mammen, E. (1989). Asymptotics with increasing dimension for robust regression with applications to
the bootstrap. The Annals of Statistics, pages 382–400.
Mammen, E. and Park, B. U. (1997). Optimal smoothing in adaptive location estimation. Journal of
Statistical Planning and Inference, 58(2):333–348.
Mardia, K. V., Kent, J. T., and Laha, A. K. (2016). Score matching estimators for directional distribu-
tions. arXiv preprint arXiv:1604.08470.
Maronna,R.A.andYohai,V.J.(1981). AsymptoticbehaviorofgeneralM-estimatesforregressionand
scalewithrandomcarriers. Zeitschrift fu¨r Wahrscheinlichkeitstheorie und verwandte Gebiete,58:7–20.
Parisi, G. (1981). Correlation functions and computer simulations. Nuclear Physics B, 180(3):378–384.
32Parzen, E. (1979). Nonparametric statistical data modeling. Journal of the American Statistical Asso-
ciation, 74(365):105–121.
Portnoy, S. (1985). Asymptotic behavior of M estimators of p regression parameters when p2/n is large;
II. normal approximation. The Annals of Statistics, 13(4):1403–1417.
P¨otscher, B. M. and Preinerstorfer, D. (2022). A modern Gauss–Markov theorem? Really? arXiv
preprint arXiv:2203.01425.
Roberts, G. O. and Tweedie, R. L. (1996). Exponential convergence of Langevin distributions and their
discrete approximations. Bernoulli, pages 341–363.
Rockafellar, R. T. (1997). Convex Analysis. Princeton University Press.
Samworth, R. and Johnson, O. (2004). Convergence of the empirical process in Mallows distance, with
an application to bootstrap performance. arXiv preprint math/0406603.
Samworth,R.J.andShah,R.D.(2024). Modern Statistical Methods and Theory. CambridgeUniversity
Press.
Schick, A. (1986). On asymptotically efficient estimation in semiparametric models. The Annals of
Statistics, 14(3):1139–1151.
Serrin, J. and Varberg, D. E. (1969). A general chain rule for derivatives and the change of variables
formula for the Lebesgue integral. The American Mathematical Monthly, 76(5):514–520.
Silverman, B. W. (1986). Density Estimation. Chapman & Hall.
Song, Y. and Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution.
Advances in Neural Information Processing Systems, 32:11895–11907.
Song, Y., Garg, S., Shi, J., and Ermon, S. (2020). Sliced score matching: A scalable approach to density
and score estimation. In Uncertainty in Artificial Intelligence, pages 574–584.
Song, Y. and Kingma, D. P. (2021). How to train your energy-based models. arXiv preprint
arXiv:2101.03288.
Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. (2021). Score-based
generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456.
Sriperumbudur,B.,Fukumizu,K.,Gretton,A.,Hyv¨arinen,A.,andKumar,R.(2017).Densityestimation
in infinite dimensional exponential families. Journal of Machine Learning Research, 18:1–59.
Stein, C. (1956a). Efficient nonparametric testing and estimation. In Proceedings of the Third Berkeley
Symposium on Mathematical Statistics and Probability, volume 1, pages 187–195.
Stein, C. (1956b). Inadmissibility of the usual estimator for the mean of a multivariate normal distri-
bution. In Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability,
Volume 1: Contributions to the Theory of Statistics,volume3,pages197–207.UniversityofCalifornia
Press.
Stone, C. J. (1975). Adaptive maximum likelihood estimators of a location parameter. The Annals of
Statistics, 3(2):267–284.
van der Vaart, A. W. (1998). Asymptotic Statistics. Cambridge University Press.
van Eeden, C. (1970). Efficiency-robust estimation of location. The Annals of Mathematical Statistics,
41(1):172–181.
Vincent, P. (2011). A connection between score matching and denoising autoencoders. Neural Compu-
tation, 23(7):1661–1674.
Wainwright,M.J.(2019). High-Dimensional Statistics: A Non-Asymptotic Viewpoint,volume48. Cam-
bridge University Press.
33Yang,Y.,Martin,R.,andBondell,H.(2019). VariationalapproximationsusingFisherdivergence. arXiv
preprint arXiv:1905.05284.
Yohai, V. J. and Maronna, R. A. (1979). Asymptotic behavior of M-estimators for the linear model.
The Annals of Statistics, pages 258–268.
Young, E. H. and Shah, R. D. (2023). Sandwich boosting for accurate estimation in partially linear
models for grouped data. arXiv preprint arXiv:2307.11401.
Yu, M., Gupta, V., and Kolar, M. (2020). Simultaneous inference for pairwise graphical models with
generalized score matching. Journal of Machine Learning Research, 21(91):1–51.
Yu, S., Drton, M., andShojaie, A.(2022). Generalizedscorematchingforgeneraldomains. Information
and Inference: A Journal of the IMA, 11(2):739–780.
Zou, H. and Yuan, M. (2008). Composite quantile regression and the oracle model selection theory. The
Annals of Statistics, 36(3):1108–1126.
6 Appendix
6.1 Proofs for Section 2
Throughout this subsection, we work in the setting of Lemmas 1 and 5. Moreover, define Q (u) :=
0
sup z [ , ] : F (z) u for u [0,1]. Then z := inf(suppp ) = Q (0) and z :=
0 min 0 0 max
sup{ (sup∈ pp− 0)∞ =F∞
0−1(1),
and≤
S0
=}
{z
∈R:∈
F 0(z) ∈(0,1) }.
Proof of Lemma 1. If F 0−1(v)<Q 0(v) for some v ∈[0,1], then because p 0( ±∞)=0 and p 0,F
0
are both
pco 0n (zt )in =uou Js 0,
(cid:0)
Fw 0e (zh )a (cid:1)ve forp 0 a( lz l) z= ∈0 Rfo ar ndall
J
0z
(v∈
)[ =F 0− p1 0( (cid:0)v F) 0, −Q 1(0 v( )v (cid:1))] ==
p{
0z
(cid:0) Q∈
0([
v− )(cid:1)∞
f,
o∞
r
a] l:
l
F v0 ∈(z [) 0,= 1],v
}
w. itT hh Je 0r (e 0fo )r =e,
tJ h0 e(1 c) o= nti0 n. uS iti ync oe
f
plim ou
n↗
[vF 0− ,1(u ]) t= haF t0− J1( iv s) ca on nd tinli um ou
u↘
sv onF 0− [01 ,( 1u ].)= MoQ re0 o(v v) erf ,o tr ha ell lev
as∈ t
c[0 o, n1 c] a, vi et f mol alo jow rs anfr to Jm
ˆ
0 0 0
satisfies sup
Jˆ(u)− =∞ sup∞
J (u)< and
u ∈[0,1] 0 u ∈[0,1] 0 ∞
Jˆ(0)=J (0)=0=J (1)=Jˆ(1). (42)
0 0 0 0
In particular, Jˆ is concave and bounded, so it is also continuous on [0,1]. By Rockafellar (1997,
0
Theorem 24.1), the right derivative Jˆ(R) is decreasing and right-continuous on [0,1), with Jˆ(R)(u) R
for all u (0,1). Thus, ψ is finite-va0 lued on . If z z for some z R, then either u :=0 F (z ∈ )
∈
0∗ S0 n
↘ ∈
n 0 n
↘
F (z) =: u < 1 or u = u = 1 for all n. In both cases, ψ (z ) = Jˆ(R)(u ) Jˆ(R)(u) = ψ (z), so ψ is
ri0 ght-continuous as an function from R to [ , ]. Since0∗ F n is incr0 easingn , ψ↗ is0 decreasing0∗ . 0∗
We have z =inf z R:F (z)>0 − a∞ nd∞ logF (z)
0
as z z
0∗
. Thus, if z > , then
min 0 0 min min
{ ∈ } ↘−∞ ↘ −∞
(cid:0) (cid:1)
J (u) J F (z) p (z)
0 0 0 0
limsup =limsup =limsup =limsup(logF )(z)= , (43)
0 ′
u F (z) F (z) ∞
u ↘0 z ↘zmin 0 z ↘zmin 0 z ↘zmin
where the final equality follows from the mean value theorem. Together with Lemma 46, this implies
that ψ (z) = Jˆ(R)(0) = for all z z . Similarly, if z < , then ψ (z) = Jˆ(R)(1) = for all
z z 0∗ . Ther0 efore, in a∞ ll cases, ψ ≤ (z)min R if and only ifm zax .∞ 0∗ 0 −∞
≥
max 0∗
∈
∈S0
Thenextlemmacharacterisespreciselytheclassofdensityquantilefunctionsofuniformlycontinuous
densitiesonR,andshowsexplicitlyhowtorecoveradensityfromacontinuousdensityquantilefunction
that is strictly positive on (0,1).
Lemma 19. A function J: [0,1] [0, ) is the density quantile function of a uniformly continuous
density on R if and only if → ∞
(cid:90) u 1
J is continuous on [0,1] with J(0)=J(1)=0 and Q (u):= < (44)
J
J ∞
1/2
34for all u (0,1). In this case, Q is a strictly increasing, continuously differentiable bijection from (0,1)
J
to (cid:0) Q (0∈ ),Q (1)(cid:1) , and the function p : R R given by
J J J
→
p
(z):=(cid:40) (Q J−1) ′(z) for z ∈(cid:0) Q J(0),Q J(1)(cid:1)
J
0 otherwise
is a uniformly continuous density with corresponding density quantile function J. If in addition J > 0
on (0,1), then a density p has density quantile function J if and only if p () = p ( µ) for some
0 0 J
µ R. · ·−
∈
Proof. Ifp 0isauniformlycontinuousdensitywithdensityquantilefunctionJ
0
=p
0
◦F 0−1,thenLemma1
s wh eow has vt eha pt (J Z0 )is =co Jnt (i Unu )o au ns dw Pit (cid:0)h JJ (0 U(0 )) == 0J (cid:1)0( =1) P=
(cid:0)
p0. (ZIn )tr =od 0u (cid:1)cin =g 0U
.
∼
MU or( e0 o, v1 e) r,an sid ncZ
e
: F= F is0− c1 o( nU t)
in∼
uoP u0 s,
,
0 0 0 0 0
(F
0
◦F 0−1)(u)=u for all u ∈(0,1), so U =F 0(Z). Therefore,
(cid:90) u 1 (cid:18) 1 (cid:19) (cid:18) 1 (cid:19) (cid:16)1(cid:17)
1/2 J 0
=E
J
0(U)1
{U ∈[1/2,u) }
=E
p
0(Z)1
{Z ∈[F 0−1(1/2),F 0−1(u)) }
≤F 0−1(u) −F 0−1
2
<
∞
(45)
foru ∈[1/2,1),withequalityifandonlyifp 0(z)>0Lebesguealmosteverywhereon(cid:2) F 0−1(1/2),F 0−1(u)(cid:1) .
Similarly, for u (0,1/2], (45) remains true, with an analogous equality condition.
Conversely, ∈ if J: [0,1] R satisfies (44), then Q is strictly increasing with Q (u) = 1/J(u) for
→
J (cid:0)′J
(cid:1)
all u (0,1). Thus, Q is a continuously differentiable bijection from (0,1) to Q (0),Q (1) , so
J J J
by the∈ inverse function theorem, F
J
:= Q−J1: (cid:0) Q J(0),Q J(1)(cid:1)
→
(0,1) is a well-defined, continuously
differentiablebijectionwithderivativep
J
:=F
J′
=1/(Q ′J◦F J)=J ◦F
J
satisfyingJ =p
J
◦F J−1 on(0,1).
Wehavelim F (z)=0andlim F (z)=1,whilelim p (z)=lim p (z)=0
since J is
coz n↘ tiQ nJ u( o0 u) sJ
at 0 and 1.
Thez r↗ efoQ rJ e( ,1) setJ
ting p (z) = 0
forz z↘QJ R(0) (cid:0)J
Q (0),Q
z (↗ 1)Q (cid:1) ,J( w1)
e
cJ
onclude
J J J
that p is a uniformly continuous density on R with corresponding∈ distr\ ibution function F , quantile
J J
function Q and density quantile function J. Moreover, for every µ R, the density p ( µ) also has
J J
∈ ·−
density quantile function J.
Finally, if p
0
is a uniformly continuous density with density quantile function J = p
0
◦F 0−1 > 0 on
(0,1), then for every u (0,1), equality holds in (44) and hence
∈
(cid:90) u 1 (cid:16)1(cid:17)
Q J(u)=
J
=F 0−1(u) −F 0−1
2
.
1/2
Therefore, letting µ:=F 0−1(1/2), we deduce that p 0( ·)=p J( ·−µ), which completes the proof.
Lemma 20. Let be as in Proposition 9(a). If t , then p
(t)=Jˆ(cid:0)
F
(t)(cid:1)
>0.
0 0 0
T ∈T
Proof. Since Jˆ is a non-negative concave function, we must have Jˆ > 0 on (0,1), as otherwise J =
0 0 0
Jˆ = 0 and hence p = J F = 0 on R by Lemma 1, which is a contradiction. Moreover, ψ (t) R,
0 0 0
◦
0 0∗
∈
so again by Lemma 1, t . Thus, v :=F (t) (0,1) and p (t)=(J F )(t)=J (v). Suppose for a
0 0 0 0 0 0
contradiction that J (v)∈ <S Jˆ(v). Then by Lem∈ ma 45 and the fact that◦ J and Jˆ are continuous at v,
0 0 0 0
there exists δ > 0 such that Jˆ is affine on [v δ,v +δ]. Thus, Jˆ(R) is constant on [v δ,v +δ), so
ψ
0∗
=Jˆ 0(R) ◦F
0
isconstantonth0 eopeninterval(cid:0) Q− 0(v −δ),F 0−1(v+δ)(cid:1)0 ,whichcontainst. Th− iscontradicts
the definition of , so p (t)=J (v)=Jˆ(v)>0, as required.
0 0 0
T
Lemma 21. If ψ Ψ (p ) satisfies ψ =ψ P -almost everywhere, then in fact ψ =ψ on R.
∈ ↓
0 0∗ 0 0∗
Proof. First consider z , let u := F (z) (0,1) and let u := inf(cid:8) v [0,1] : Jˆ(R)(v) = Jˆ(R)(u)(cid:9) .
∈ S0 0 ∈ 1 ∈ 0 0
Then Jˆ 0(R)(u 1) = Jˆ 0(R)(u) by the right-continuity of Jˆ 0(R). Moreover, define z
1
:= F 0−1(u 1) and z
2
:=
Q (u), so that z z z and
0 1 2
≤ ≤
F (z δ)<u =F (z ) F (z )=u<F (z +δ)
0 1 1 0 1 0 2 0 2
− ≤
for all δ > 0. Then ψ (z δ) = Jˆ(R)(cid:0) F (z δ)(cid:1) > Jˆ(R)(u ) = ψ (z ) for all such δ, so z and
0∗ 1 − 0 0 1 − (cid:0) 0 1 (cid:1) 0∗ 1 1 ∈ T
hence p (z ) > 0 by Lemma 20. It follows that P [z ,z +δ) > 0 for j 1,2 , so because ψ and ψ
0 1 0 j j
∈ { }
0∗
are decreasing, right-continuous functions that agree P -almost everywhere,
0
ψ(z) ψ(z )=ψ (z )=Jˆ(R)(u )=Jˆ(R)(u)=ψ (z )=ψ(z ) ψ(z).
≤ 1 0∗ 1 0 1 0 0∗ 2 2 ≤
35Thus, ψ(z) = ψ (z) for all z . Together with Lemma 1, this implies that lim ψ(z) =
0∗
∈
S0 z ↘zmin
lim ψ (z) = and lim ψ(z) = lim ψ (z) = , so ψ = ψ = on ( ,z ]
andz ↘ ψz =min
ψ
0∗
=
∞
on [z ,
z )↗ .z Tma hx
us, ψ =ψ
oz n↗Rzm ,a ax
s
r0∗ equired.−∞ 0∗ ∞ −∞ min
0∗
−∞
max
∞
0∗
Proof of Theorem 2. Let χ := 1 Ψ (p ) for t ( , ]. Then, letting U U(0,1), we have
t ( ,t) 0
(F
0
◦F 0−1)(U)=U, so −∞ ∈ ↓ ∈ ∞ ∞ ∼
(cid:90) (cid:90)
Rψ 0∗χ tdP
0
=
(
,t)(Jˆ 0(R) ◦F 0)dP
0
=E(cid:8)(cid:0) Jˆ 0(R) ◦F
0
◦F 0−1(cid:1) (U)1
{F 0−1(U) ≤t
}(cid:9)
−∞
=E(cid:8) Jˆ(R)(U)1 (cid:9)
=(cid:90) F0(t)
Jˆ(R)
0 {U ≤F0(t)
} 0
0
=Jˆ(cid:0) F (t)(cid:1) Jˆ(0) J (cid:0) F (t)(cid:1) =p (t),
0 0 0 0 0 0
− ≥
where the last two equalities are due to Rockafellar (1997, Corollary 24.2.1) and Lemma 1 respectively.
By (42) and Lemma 20, equality holds throughout in the final line if F (t) 0,1 or t , so
0
∈{ } ∈T
(cid:40)
(cid:90)
p (t) for all t ( , ]
0
ψ 0∗χ tdP
0
≥ ∈ ∞ ∞ (46)
R =p 0(t) for all t ( ,z min] [z max, ] .
∈ −∞ ∪ ∞ ∪T
(cid:82)
(a) In particular, taking t= , we have ψ dP =0.
∞
R 0∗ 0
(cid:82)
(b,c) Case 1: Suppose that i (p ) = (ψ )2dP < . Then ψ Ψ (p ) since it is decreasing and
right-continuous. For
measura∗ ble0 functiR ons0∗
ψ
on0
R, w∞ e write
ψ0∗
∈ ψ↓
0
:= ((cid:82) ψ2dP )1/2, and
(cid:82)∥ ∥ ≡ ∥
∥L2(P0) R 0
for ψ ,ψ L2(P ), denote by ψ ,ψ ψ ,ψ := ψ ψ dP their L2(P ) inner product.
1 2
∈
0
⟨
1 2
⟩ ≡ ⟨
1 2 ⟩L2(P0) R 1 2 0 0
Recalling that D ( ,λ) is a convex (quadratic) function on the convex cone Ψ (p ), we will deduce
p0
· ↓
0
from (46) that ψ satisfies the first-order stationarity condition
0∗
(cid:90) (cid:90)
p dψ ψ ψdP = ψ ,ψ R (47)
−
0
≤ R
0∗ 0
⟨
0∗
⟩∈
S0
for all ψ Ψ (p ), with equality when ψ =ψ . To see this, fix t and define g(z,t):=1
1 ∈ for↓ z,t0 R. Then g(,t)=χ () 10∗ for all t R0 a∈ ndS ψ0 (t ) ψ(z)=(cid:82) g(z,t{ )z d< ψt ≤ (tt )0} fo− r
{t0<t ≤z
} ∈(cid:82) ·
t
· −
{t (cid:0)>t0}
∈(cid:1)
0
(cid:82)− S0
all z . Since ψ ψ(t ) ψ dP ψ ψ(t ) + ψ < and ψ dP = 0 by (46), we can
∈
S0 R
|
0∗
||
0
− |
0
≤ ∥
0∗
∥ |
0
| ∥ ∥ ∞
R 0∗ 0
apply Fubini’s theorem to obtain
(cid:90) (cid:90) (cid:90) (cid:90) (cid:90) (cid:90)
ψ ψdP =ψ(t ) ψ dP ψ (z) g(z,t)dψ(t)dP (z)= ψ (z)g(z,t)dP (z)dψ(t).
R
0∗ 0 0
R
0∗ 0
− R
0∗ 0
− R
0∗ 0
S0 S0
(48)
Ontheright-handsideof (48),theouterLebesgue–Stieltjesintegraliswithrespecttoanegativemeasure,
so it follows from (46) that
(cid:90) (cid:90) (cid:90) (cid:18)(cid:90) (cid:19) (cid:90) (cid:18)(cid:90) (cid:19)
ψ (z)g(z,t)dP (z)dψ(t)= ψ χ dP dψ(t)+ ψ (χ 1)dP dψ(t)
S0 R
0∗ 0
(cid:90)(zmin,t0] R
0∗ t 0
(cid:90)
(t0,zmax) R
(cid:90)
0∗ t
−
0
p (t)dψ(t)+ p (t)dψ(t)= p dψ. (49)
0 0 0
≤
(zmin,t0] (t0,zmax) S0
Togetherwith(48),thisprovestheinequality (47)forallψ Ψ (p ). Itremainstoshowthat(47)holds
0
∈ ↓
with equality when ψ = ψ . Let ν for the Lebesgue–Stieltjes measure induced by ψ , so that the set
0∗ ∗ 0∗
in Lemma 20 is the support of ν . Then c is an open set with ν ( c) = 0, so ψ (t ) ψ (z) =
T(cid:82) g(z,t)dψ (t) for all z R.
Thus,∗
(48) holdT s for ψ = ψ when the
L∗
ebT
esgue–Stieltjes0∗ in0
teg−
rals0∗
(with
0∗
∈
0∗
reTspect to ν ) are restricted to . Moreover, (49) is still valid if we intersect the domains of all
∗ 0
S ∩T
outer integrals with , again because ν ( c)=0. We established that (46) is an equality for all t ,
∗
T T ∈T
so (49) and hence (47) hold with equality when ψ =ψ .
0∗
36For ψ Ψ (p ), it follows from (47) that (cid:82) p dψ > and hence D (ψ,λ) R for every λ>0, so
∈ ↓
0
S0
0
−∞
p0
∈
(cid:90) (cid:90) (cid:26)(cid:90) (cid:90) (cid:27)
D (ψ,λ)= (ψ λψ )2dP + (λψ )2dP +2λ ψ (ψ λψ )dP + p dψ
p0
R −
0∗ 0
R
0∗ 0
R
0∗
−
0∗ 0 0
(cid:90) (cid:90)
S0
(ψ λψ )2dP λ2 (ψ )2dP (50)
≥ R −
0∗ 0
− R
0∗ 0
(cid:90) (cid:90) (cid:90)
= (ψ λψ )2dP + (λψ )2dP +2λ2 p dψ
R −
0∗ 0
R
0∗ 0 0 0∗
(cid:90)
S0
= (ψ λψ )2dP +D (λψ ,λ) D (λψ ,λ).
R −
0∗ 0 p0 0∗
≥
p0 0∗
Thus, ψ argmin D (ψ,λ) if and only if ψ = λψ P -almost everywhere. By Lemma 21, this
holds if a∈ nd only iψ f∈ψΨ =↓(p λ0) ψ p o0 n R, which proves the unique0∗ nes0 s assertion in (b).
0∗
Moreover, since Jˆ(R)(0) = lim Jˆ(R)(u) > 0 and F (z) > 0 for all z > Q (0) = z , we have
0 u 0 0 0 0 min
(cid:82) (ψ )2dP = (cid:82) (Jˆ(R) ↘ F )2dP > 0 for all z > z , so i (p ) = ψ 2 > 0. Now for
(zmin,z) 0∗ 0 (cid:82)(zmin,z) 0 ◦ 0 0 (cid:82) min ∗ 0 ∥ 0∗ ∥
ψ Ψ (p ) such that ψ2dP >0, we have p dψ 0 since ψ is decreasing. It follows from (47)
0 R 0 0
an∈
d
th↓
e Cauchy–Schwarz inequality that
− S0 ≥
(cid:82)
ψ2dP ψ 2 1 1
R 0
V (ψ)= ∥ ∥ = (0, ),
p0 (cid:0)(cid:82) p 0dψ(cid:1)2 ≥ ⟨ψ 0∗,ψ ⟩2 ≥ ∥ψ 0∗ ∥2 i ∗(p 0) ∈ ∞
S0
with equality if and only if ψ =λψ P -almost everywhere for some λ>0. By Lemma 21, this holds if
0∗ 0
and only if ψ =λψ on R, so the proof of (c) is complete.
0∗
Case 2: Suppose instead that i (p )=(cid:82) (ψ )2dP = . For each n N, define ψ :=(ψ n) ( n).
∗ 0 R 0∗ 0
∞ ∈
n∗ 0∗
∧ ∨ −
This is bounded, decreasing and right-continuous, so ψ Ψ (p ), and we claim that
n∗
∈ ↓
0
(cid:90) (cid:90)
p dψ = ψ ψ dP R. (51)
−
0 n∗
R
0∗ n∗ 0
∈
S0
Indeed, by the quantile transform used to establish (46), (cid:82) ψ ψ dP n(cid:82) ψ dP = n(cid:82)1 Jˆ(R)
| R 0∗ n∗ 0 | ≤ R | 0∗ | 0 0 | 0 | ≤
2nsup Jˆ(u)< since Jˆ is concave and bounded. Therefore, we may apply Fubini’s theorem as
aboveu t∈o[0 s, e1 e] t0
hat
(48∞
) above
h0
olds when ψ = ψ . Furthermore, writing ν for the Lebesgue–Stieltjes
n∗ n∗
measure associated with ψ , we have ν ( c)=ν ( c I )=0, where I := z R:ψ (z) ( n,n] .
n∗ n∗
T
∗
T ∩
n n
{ ∈
0∗
∈ − }
Consequently, by intersecting the domains of all outer integrals in (49) with and arguing as above, we
deduce that (49) holds with equality for ψ = ψ . This yields (51) for everyT n N. For λ > 0, it then
n∗
∈
follows by the monotone convergence theorem that
(cid:18)(cid:90) (cid:90) (cid:19) (cid:90) (cid:90)
D (λψ ,λ)=λ2 (ψ )2dP 2 ψ ψ dP λ2 (ψ )2dP = λ2 (cid:0) (ψ )2 n2(cid:1) dP
p0 n∗
R
n∗ 0
− R
0∗ n∗ 0
≤− R
n∗ 0
− R
0∗
∧
0
(cid:90)
λ2 (ψ )2dP =
↘− R
0∗ 0
−∞
as n . Thus, inf D (ψ,λ)= .
T→ oge∞ therwith(50ψ ),∈ tΨ h↓ i( sp0 s) howp0 sthatinf−∞ D (ψ,λ)= λ2i (p )inbothcases,whichcompletes
ψ ∈Ψ↓(p0) p0
−
∗ 0
the proof of (b).
(d) Suppose that p is absolutely continuous on R with score function ψ = p /p . Recall that (F
0 0 ′0 0 0
◦
F 0−1)(u)=u for all u ∈(0,1), so that letting U ∼U(0,1) and Z =F 0−1(U) ∼P 0, we have
(cid:90) u
0
ψ
0
◦F 0−1 =E(cid:0) (ψ
0
◦F 0−1)(U)1
{U ≤(F0◦F 0−1)(u)
}(cid:1) (52)
(cid:90) (cid:90) F−1(u)
=E(cid:0) ψ 0(Z)1
{Z ≤F 0−1(u)
}(cid:1) =
(
,F−1(u)]ψ 0dP
0
= 0 p
′0
=(p
0
◦F 0−1)(u)=J 0(u)
−∞ 0 −∞
for all u (0,1). This yields the representation (17) of ψ .
∈
0∗
37(cid:82)
We assume henceforth that i(p ) = ψ2dP = ψ 2 < , since otherwise the inequality i (p )
(cid:82) 0 R 0 0 ∥ 0 ∥ ∞ ∗ 0 ≤
i(p ) holds trivially. Then ψψ dP = ψ , ψ ψ ψ < for all ψ Ψ (p ). In particular,
0 R 0 0 0 0 0
(cid:82) (cid:82) | | ⟨| | | |⟩ ≤ ∥ ∥∥ ∥ ∞ ∈ ↓
p = ψ dP < , so by the dominated convergence theorem,
R
|
′0| R
|
0
|
0
∞
(cid:90) (cid:90) (cid:90) t
(cid:0) (cid:1)
ψ dP = p = lim p = lim p (t) p ( t) =p ( ) p ( )=0.
R
0 0
R
′0
t →∞ t
′0
t →∞
0
−
0
−
0
∞ −
0
−∞
−
Recallingthatg(z,t)=1 1 forz,t Randψ(t ) ψ(z)=(cid:82) g(z,t)dψ(t)forallz ,
(cid:82)
{z<t ≤t0}− {t0<t (cid:0)≤z
} ∈ (cid:1)
0
− S0 (cid:82)
∈S0
we similarly have p (z)g(z,t)dz = p ( ) p (t) = p (t) for all t > t and p (z)g(z,t)dz =
R ′0
−
0
∞ −
0 0 0 R ′0
p (t) p ( )=p (t) for all t t . For ψ Ψ (p ), it then follows from Fubini’s theorem that
0 0 0 0 0
− −∞ ≤ ∈ ↓
(cid:90) (cid:90) (cid:90) (cid:90) (cid:90)
ψψ dP =ψ(t ) ψ dP p (z) g(z,t)dψ(t)dz = p (t)dψ(t)
R
0 0 0
R
0 0
− R
′0
−
0
S0 S0
and hence that
(cid:90) (cid:90)
D (ψ,λ)= ψ2dP +2λ p dψ = ψ 2 2λ ψ,ψ = ψ λψ 2 λψ 2 (53)
p0
R
0 0
∥ ∥ − ⟨
0
⟩ ∥ −
0
∥ −∥
0
∥
S0
λ2 ψ 2 = λ2i(p )> .
0 0
≥− ∥ ∥ − −∞
Thus, by (b), λ2i (p ) = inf D (ψ,λ) λ2i(p ) > , so i (p ) i(p ) < . Equality
−
∗ 0 ψ ∈Ψ↓(p0) p0
≥ −
0
−∞
∗ 0
≤
0
∞
holds if and only if inf ψ λψ = 0, i.e. ψ Ψ (p ), which is equivalent to p being log-
ψ ∈Ψ↓(p0)
∥ −
0
∥
0
∈ ↓
0 0
concave.
Proof of Lemma 4. (a) By Lemma 46 and (42),
lim ψ (z)= lim Jˆ(R)(cid:0) F (z)(cid:1) =Jˆ(R)(0)= sup J 0(u) >0.
z →−∞ 0∗ z →−∞ 0 0 0 u ∈(0,1) u
On the other hand, recalling from Lemma 1 that p =J F , we have
0 0 0
◦
p (z) J (u)
0 0
limsup h (z)=limsup =limsup ,
0
F (z) u
z ↘zmin z ↘zmin 0 u ↘0
which by (43) can only be finite if z = . Since J is continuous, sup J (u)/u < if and
only if limsup J (u)/u< ,
whicm hin imp− lie∞
s the
desire0
d conclusion.
u ∈(0,1) 0 ∞
u ↘0 0 ∞
(b) Likewise,
J (u)
lim ψ (z)=Jˆ(L)(1)= sup 0 [ ,0),
z →∞ 0∗ 0 − u ∈(0,1) 1 −u ∈ −∞
and is finite if and only if
p (z) J (u)
0 0
limsup h (z)=limsup =limsup
0
1 F (z) 1 u
z ↗zmax z ↗zmax
−
0 u ↗1
−
is finite, in which case z = by arguing similarly to (43).
max
∞
Lemma 22. Let s :=inf z R:ψ (z) 0 and t :=sup z R:ψ (z)<0 . For s,t such that
0
{ ∈
0∗
≤ }
0
{ ∈
0∗
}
∈S0
s t, we have
≤ (cid:90) t (cid:40) logJˆ(cid:0) F (t)(cid:1) logJˆ(cid:0) F (s)(cid:1) if t t
0 0 0 0 0
ψ 0∗ ≥ logJˆ(cid:0)
F
(t)(cid:1)− logJˆ(cid:0)
F
(s)(cid:1)
if
s≤
s ,
(54)
s 0 0 0 0 0
≤ − ≥
and hence
(cid:40)
p (t) p
(s)exp(cid:0)(cid:82)t
ψ
(cid:1)
if t t and s
0 ≤ 0 s 0∗ ≤ 0 ∈T (55)
p (t) p
(s)exp(cid:0)(cid:82)t
ψ
(cid:1)
if s s and t .
0 ≥ 0 s 0∗ ≥ 0 ∈T
Proof. For s,t with s t, we have 0 < F (s) F (t) < 1. By Lemma 1 and the fact that ψ is
∈
S0
≤
0
≤
0 0∗
decreasing,
(cid:90) t
<ψ (t)(t s) ψ ψ (s)(t s)< .
−∞
0∗
− ≤
0∗
≤
0∗
− ∞
s
38I ψnt (r Zod )u =cin Jˆg (RU
)(U∼
)U a( n0 d,1 P) (cid:0)a Jnd (UZ ): ==F 0(cid:1)0− =1(U P)
(cid:0) p∼
(P Z0 ), =we 0h (cid:1)a =ve 0F .0( IZ
f
t)=U
t
,b ty ht eh ne Jˆc (o Rn )tin Fuity =of ψF 0. T 0h oe nre (fo sr ,e t],
0∗ 0 0 0 ≤ 0 0 ◦ 0 0∗ ≥
and hence Jˆ(R) 0 on (F (s),F (t)]. Therefore,
0 ≥ 0 0
(cid:90) t (cid:18) ψ (Z) (cid:19) (cid:18) Jˆ(R)(U) (cid:19)
ψ E 0∗ 1 =E 0 1
s 0∗ ≥ p 0(Z) {Z ∈(s,t] } J 0(U) {U ∈(F0(s),F0(t)] }
=(cid:90) F0(t) Jˆ 0(R) (cid:90) F0(t) Jˆ 0(R)
=logJˆ(cid:0)
F
(t)(cid:1) logJˆ(cid:0)
F
(s)(cid:1)
,
F0(s)
J
0
≥
F0(s)
Jˆ
0
0 0 − 0 0
where the final equality holds because logJˆ is finite-valued, Lipschitz and hence absolutely continuous
0
on[F (s),F (t)]withderivativeequaltoJˆ(R)/Jˆ,Lebesguealmosteverywhere. Thisprovesthefirstline
0 0 0 0
of (54). If s , then by Lemma 20,
∈T
logJˆ(cid:0)
F
(t)(cid:1) logJˆ(cid:0)
F
(s)(cid:1) =logJˆ 0(cid:0) F 0(t)(cid:1) logJ 0(cid:0) F 0(t)(cid:1)
=log
p 0(t)
,
0 0 0 0
− p (s) ≥ p (s) p (s)
0 0 0
which yields the first line of (55). The proofs in the case s s t are analogous, except that now
0
≤ ≤
Jˆ(R) F = ψ 0 on (s,t] and hence Jˆ(R) 0 on (F (s),F (t)], so the directions of all inequalities
0 ◦ 0 0∗ ≤ 0 ≤ 0 0
above are reversed.
Proof of Lemma 5. Since sup Jˆ(u) = sup J (u) = sup p (z) > 0 and Jˆ is concave, we
have
u ∈(0,1) 0 u ∈(0,1) 0 z ∈S0 0 0
Jˆ(R)(0)= lim Jˆ(R)(u)>0> lim Jˆ(R)(u)=Jˆ(R)(1).
0 0 0 0
u 0 u 1
↘ ↗
We have (F
0
◦F 0−1)(u) = u for all u
∈
(0,1), so if δ > 0 is sufficiently small, then z
1
:= F 0−1(δ) and
z L2 em:= maF 0 1− ,1 ψ(1 − isδ d) eca rr ee ase il ne gm ae nn dts o zf SR0 s :a ψtis (f zy )ing Rψ 0∗( =z 1) =
,
soJˆ 0(R)(δ) > 0 > Jˆ 0(R)(1 −δ) = ψ 0∗(z 2). By
0∗
{ ∈
0∗
∈ }
S0
(cid:90) z
ϕ (z):= ψ
∗0 0∗
z1
is well-defined in [ , ) for all z R, with ϕ (z) R for all z . By Lemma 22,
−∞ ∞ ∈
∗0
∈
∈S0
(cid:40) (cid:82)z1ψ logJˆ(cid:0) F (z)(cid:1) logJˆ(δ) for z z
ϕ ∗0(z)= −
ϕ
(z
z
)+0∗ (cid:82)≤
z ψ
0 ϕ0
(z
)+− logJˆ0
(cid:0) F (z)(cid:1) logJˆ(1 δ) for z
≤ z1
.
∗0 2 z2 0∗ ≤ ∗0 2 0 0 − 0 − ≥ 2
Together with (42), this shows that ϕ ∗0(z) = ϕ ∗0(z min) = lim z′ ↘zminϕ ∗0(z ′) =
−∞
for all z
≤
z min, and
cϕ o∗0 n(z ti) nu= ouϕ s∗0 a( sz m aax fu) n= ctioli nm fz r′
↗
omzm Raxϕ to∗0( [z ′) =
,
− )∞
,
wif to hr a zll z
R≥
:z ϕm (a zx.
)
>We ded =uce t .h Bat yϕ R∗0 ocis kac fo en llc aa rv (e 19a 9n 7d
,
−∞ ∞ { ∈
∗0
−∞}
S0
Theorem 24.2), ϕ has right derivative ψ on , with (ϕ ) = ψ Lebesgue almost everywhere on .
∗0 0∗ S0 ∗0 ′ 0∗ S0
Since ψ is decreasing,
0∗
(cid:40)
ψ (z )(z z) for z z
ϕ ∗0(z)
≤
−
ϕ
(0 z∗ )1 +(cid:82)1 z−
ψ ϕ (z ) ψ (z )(z z ) for z
≤ z1
,
∗0 2 z2 0∗ ≤ ∗0 2 −| 0∗ 2 | − 2 ≥ 2
so
(cid:90)
eϕ∗
0
eϕ∗ 0(z1) +(cid:90) z2
eϕ∗
0 +
eϕ∗ 0(z2)
< .
R ≤ ψ 0∗(z 1) z1 |ψ 0∗(z 2) | ∞
T
ϕ
∗0h −ere lofo gr (cid:0)e (cid:82), Rp e∗0
ϕ∗
0:=
(cid:1)
he aϕ s∗ 0/ ri(cid:82) gR hteϕ d∗ 0 eri is vaa tic vo en ψti 0∗nu oo nu Ss 0l .og-concave density such that suppp ∗0 = S0 and logp ∗0 =
If p˜ is another continuous log-concave density such that logp˜ has right derivative ψ on , then
by Rockafellar (1997, Corollary 24.2.1), logp˜(z) = logp˜(z )+(cid:82)z ψ for all z R. Thi0 s∗ impS li0 es that
1 z1 0∗ ∈
p˜=p , so p is the unique density with the required properties.
∗0 ∗0
Finally, suppose that p is a continuous log-concave density on R. Then ϕ := logp is concave, and
0 0 0
on = suppp = z R : ϕ (z) > , its right derivative ψ := ϕ(R) is decreasing and right-
S0 0 { ∈ 0 −∞} 0 0
continuous. Thus, p has right derivative p(R) = ψ p on . Moreover, F is strictly increasing and
0 0 0 0 S0 0
39differentiable on
S0
= {z
∈
R : F 0(z)
∈
(0,1) }, so F 0−1 is differentiable on (0,1) with derivative
( pF
0
◦0− F1 0) −′( 1u h) a= sri1 g/ h( tp 0 de◦ riF v0 a− t1 i) v( eu J) 0(f Ro )r (uu )∈ =( p0
(
0, R1 )) (cid:0), Fa 0−n 1d (u( )F (cid:1)0− /p1 0◦(cid:0) FF 0−0) 1( (z u) )(cid:1)= =z (f ψo 0r ◦a Fll 0−z 1)∈ (uS )0 fo. rC uo ∈ns (e 0q ,u 1e )n .t Sly i, ncJ e0 ψ=
0
isdecreasing,J isthereforeconcaveon(0,1)byRockafellar(1997,Theorem24.2);seealsoBobkov(1996,
0
PropositionA.1(c)). Therefore,Jˆ
0
=J
0
andJˆ 0(R) =J 0(R) =ψ
0
◦F 0−1 on(0,1),soψ
0∗
=ψ
0
◦F 0−1 ◦F
0
=ψ
0
on . Together with the arguments in the previous paragraph, this implies that ϕ = ϕ and hence
S0 ∗0 0
p =p , as claimed.
∗0 0
Proof of Proposition 6. Given ϵ (0,1), let a (1 ϵ2)/ϵ2 >(1 ϵ)/ϵ and define p : R R by
0
∈ ≥ − − →
(cid:40)
ϵae a(z 1)/2 for z 1
− | |−
p (z)= | |≥
0 ϵaeb(z 1)/2 for z 1,
| |−
| |≤
where b > 0 uniquely satisfies 1 ϵ =
(cid:82)1
ϵaeb(z 1)/2dz = ϵa(1 e b)/b. Then
(cid:82)
p = 1, so p is a
− 1 | |− − − R 0 0
symmetric,absolutelycontinuousandpie− cewiselog-affinedensity,andb=(1 e b)aϵ/(1 ϵ) aϵ/(1 ϵ).
−
− − ≤ −
Moreover,

a for z >1
p (z)
−
ψ (z):=
′0
= b for z (0,1)
0
p 0(z) 
ψ ( z) for z
∈
<0,
0
− −
(cid:82)
so i(p ) = ψ2p = b2(1 ϵ)+a2ϵ and ψ is increasing on ( 1,0) (0,1). Thus, p is log-convex
0 R 0 0 − 0 (cid:82) − ∪ 0
on [ 1,1], and log-affine on both ( ,1] and [1, ). Since zp (z)dz = 0, the proof of Du¨mbgen
R 0
− −∞ ∞
et al. (2011, Remark 2.11(ii)) ensures that the log-concave maximum likelihood projection q pML of
0 ≡ 0
p satisfies
0

p 0(z) for z 1+δ
pM 0 L(z)= ϵae −aδ | |≥
p 0(1+δ)= for z 1+δ,
2 | |≤
(cid:82) (cid:0) (cid:1)
where δ > 0 uniquely solves 1 = pML = ϵ a(1+δ)+1 e aδ. Then the associated score function
R 0 −
ψML :=q(R)/q is given by
0 0 0

a for z < (1+δ)

−
ψML(z)= 0 for z [ (1+δ),1+δ)
0 
a for z
∈ 1−
+δ.
− ≥
Therefore, ψML Ψ (p ) with
0 ∈ ↓ 0
(cid:90) (cid:90) (cid:90)
(ψML)2dP =2a2 ∞ p =a2ϵe aδ =a(cid:0) p (1+δ)+p ( 1 δ)(cid:1) = p dψML,
R 0 0 1+δ 0 − 0 0 − − − R 0 0
so
1 (cid:0)(cid:82) p dψML(cid:1)2 (cid:90)
V p0(ψ 0ML)
= (cid:82) RR
(ψ
00 ML)20
dP
0
=
−
Rp 0dψ 0ML =a2ϵe −aδ.
F cou nr vth exer om no [r Fe, (J 0′( 1u )) ,F= (( 1ψ )0
],◦
aF n0− d1 l) i( nu e) aw
r
oh nen be ov te hru [0= ,FF (0(z 1) )]fo ar nz d̸=
[F{ (−
11 ),, 10 ], .1
}
H, es no cJ e,0 Ji ˆss =ym Jm oe ntri [c 0,a Fbo (ut 11 )/ ]2,
0 0 0 0 0 0 0
[F (1),1] and
J−ˆ
=p (1) on [F ( 1),F (1)], so
− − ∪
0 0 0 0 0
−

a for z < 1
ψ 0∗(z)=Jˆ 0(R)(cid:0) F 0(z)(cid:1)
=
0
a
ff oo rr z
z
∈[−
1−
.1,1)
− ≥
(cid:82)
Thus, by Theorem 2(c), 1/V (ψ )=i (p )= (ψ )2p =a2ϵ, so
p0 0∗ ∗ 0 R 0∗ 0
V (ψ ) 1 1
V
p0 (ψM0∗
L)
=e −aδ = ϵ(cid:0) a(1+δ)+1(cid:1) <
ϵ(a+1)
≤ϵ
p0 0
and
i (p ) a2ϵ a2ϵ
∗ 0
ARE∗(p 0)=
i(p )
=
b2(1 ϵ)+a2ϵ ≥ (aϵ)2/(1 ϵ)+a2ϵ
=1 −ϵ,
0
− −
as required.
40Proof of Lemma 7. By Lemma 1, Jˆ is continuous on [0,1] with Jˆ(0) = Jˆ(1) = 0, and we can find
0 0 0
u (0,1) such that Jˆ(u )=J (u )= J = p < . Then by the Cauchy–Schwarz inequality
∗ 0 ∗ 0 ∗ 0 0
∈ ∥ ∥∞ ∥ ∥∞ ∞
and Rockafellar (1997, Corollary 24.2.1),
(cid:90) u∗ (cid:0) Jˆ 0(R)(cid:1)2
≥
(cid:0)(cid:82) 0u∗ uJˆ 0(R)(cid:1)2 = Jˆ 0( uu ∗)2 = ∥p u0 ∥2 ∞,
0 ∗ ∗ ∗
(cid:90) 1 (cid:0) Jˆ 0(R)(cid:1)2
≥
(cid:0)(cid:82) u 11 ∗Jˆ 0( uR)(cid:1)2 = Jˆ 10(u ∗ u)2 = ∥ 1p 0 ∥ u2 ∞,
u∗ − ∗ − ∗ − ∗
with equality if and only if Jˆ is linear on both [0,u ] and [u ,1]. Thus, by Remark 3,
0 ∗ ∗
i (p )=(cid:90) 1 (cid:0) Jˆ(R)(cid:1)2 p 2 (cid:16) 1 + 1 (cid:17) 4 p 2 .
∗ 0
0
0 ≥∥ 0 ∥∞ u
∗
1 −u
∗
≥ ∥ 0 ∥∞
Equality holds if and only if u =1/2 and Jˆ(u)=min(u,1 u)/σ for some σ >0. In this case, letting
∗ 0
−
µ:=F 0−1(1/2), we deduce that ψ 0∗(z)=(Jˆ 0(R) ◦F 0)(z)= −sgn(z −µ)/σ for all z ∈R, so p
∗0
is a Laplace
density of the stated form.
To prove Proposition 9, we require some further definitions and lemmas. Let be as in the propo-
sition. Then c is an open subset of R and hence has a unique representation aT s a countable disjoint
union (cid:83)K (sT ,t ) of open intervals, where K N and s <t for every k. Recall
from Lemk= m1 a k 22k that s = inf z R : ψ (z) ∈ 00 ∪ an{ d∞ t} = su− p∞ z≤ Rk : ψk (≤ z)∞ < 0 . For t R, let
ψ (t ):=lim ψ
(z)0
. For
k{ N∈
such
th0∗
at
k≤ K}
,
induc0
tively
d{ efin∈
e p : [
0∗
, ]
}R
by
∈
0
−
z ↗t 0∗
∈ ≤
k
−∞ ∞ →

p
k(z):= pp
p
kk
k−
−
11
1
((
(
zz
s k
))
)e pψ k0∗ −(s 1k () s(z k− )esk ψ)
0∗(sk)(tk−sk)
fffo oor
rr
zzz
∈≤
>[
ts
s
kk
k,t k]
− · p k 1(t k)
−
if s >s , and otherwise let
k 0
 p
k
−1(z)
·
p k −1(t k p) keψ 0 1∗( (t sk k− ))(sk−tk) for z <s
k
p k(z):= p
pk
−1( (t
zk
))eψ 0∗(tk−)(z −− tk) f foo rr zz ∈[ ts
k
.,t k]
k 1 k
− ≥
if s s . When K < , define p := p for all k N with k > K. By Lemmas 1 and 20, ψ (z) R
andk p≤ (z0 ) > 0 for all z∞ . Sincek s ,tK ∈ , and p (s ) = p (s ) for k N s0 u∗ ch t∈ hat
0 k k k 0 k 1 0
k K, it follows by ind∈ uT ction that p >∈ 0T on∪{− f∞ or a∞ ll} k N, with p (s− ) = p (s ).∈ In particular,
k k 0 0 0
≤ T ∈
p (s )>0 whenever s > and p (t )>0 whenever t < .
k 1 k k k 1 k k
− −∞ − ∞
Lemma 23. For every k N , the following statements hold.
0
∈
(a) p p on , and p = p ;
k 0 k 0
≤ T ∥ ∥∞ ∥ ∥∞
(cid:90) t (cid:90) t
(b) p (z) p p (z) p if z s,t for some s,t , such that s t;
0 k k 0
≥ ∈{ } ∈T ∪{−∞ ∞} ≤
s s
(cid:40)
p (z) p (z)/p (s ) if z [s ,t ] for some j N with j >k
(c) k = 0 0 j ∈ j j ∈
p k(s j) p j(z)/p j(s j) if z [s j,t j] for some j N with j <k;
∈ ∈
(cid:90) t (cid:90) t
(d) ψ (t ) p p (t) p (s) ψ (s) p for all s,t , such that s t.
0∗
−
k
≤
k
−
k
≤
0∗ k
∈T ∪{−∞ ∞} ≤
s s
Proof. We will proceed by induction on k. When k =0, (a), (b) and (c) hold trivially, so it remains to
prove (d). This holds with equality when s = t, so now let s,t , be such that s < t. By
∈ T ∪{−∞ ∞}
41the concavity of Jˆ together with Lemmas 1 and 20, v := F (s) and w := F (t) satisfy 0 v < w 1
0 0 0
≤ ≤
and
p 0(t) −p 0(s)=Jˆ 0(w) −Jˆ 0(v)  ≤ ≥J Jˆ ˆ 00( (R L)) (( wv) )( (w w− −v v) )= =ψ ul0 i∗ m( ws) Jˆ(cid:82) 0(s Rt )p (0 u)(w −v)=ψ 0∗(t −)(cid:82) st p 0, (56)
↗
where Jˆ(L)(w)=lim Jˆ(R)(u) by Rockafellar (1997, Theorem 24.1).
0 u w 0
Next, consider a g↗ eneral k N. If k > K, then p = p and hence (a)–(d) hold by induction.
k k 1
∈ −
Supposing now that k K, let a := ψ (s ) if s > and otherwise let a := ψ (t ), so that
≤
k 0∗ k k
−∞
k 0∗ k
−
ψ =a on (s ,t ).
0∗ k k k
(a) By part (c) of the inductive hypothesis and (55),
(cid:40)
p
k
1(t k)eak(sk−tk) p 0(t k)eak(sk−tk) 1 if s
k
s
0
r k := − = ≤ ≤ (57)
p k 1(s k) p 0(s k) 1 if s k s 0.
− ≥ ≥
Therefore,
(cid:40)
p =r p p on ( ,s ] and p =p on [t , ) if s s
k k k 1 k 1 k k k 1 k k 0
− ≤ − −∞ − ∞ ≤ (58)
p
k
=p
k −1
on ( −∞,s k] and p
k
=r k−1p
k −1
≤p
k −1
on [t k, ∞) if s
k
≥s 0,
while for z [s ,t ], we have
k k
∈
(cid:40)
p
k
1(t k)eak(z −tk) p
k
1(t k) if s
k
s
0
p k(z)=
p
k−
1(s k)eak(z
−sk)≤
p
k−
1(s k) if s
k
≤
s 0.
− ≤ − ≥
This shows that p p on R (s ,t ) and p = p , so (a) holds by induction.
k k 1 k k k k 1
≤ − \ ⊇T ∥ ∥∞ ∥ − ∥∞
(b) Taking s=s and t=t in (56), we deduce from part (c) of the inductive hypothesis that
k k
(cid:40)
p (t ) p (s ) p (t ) p (s ) 0 if s s
k 1 k k 1 k 0 k 0 k k 0
− (cid:82)tk−
p
− = (cid:82)t−
kp
=a k ≥
0 if s
≤
s .
(59)
sk k −1 sk 0 ≤ k ≥ 0
There is nothing to prove when s = t, so let s,t , be such that s < t. If either
∈ T ∪ {−∞ ∞}
(s,t) ⊆( −∞,s k) or (s,t) ⊆(t k, ∞), then (58) implies that p
k
=rp
k −1
on [s,t] for some r ∈{1,r k,r k−1 },
so by part (c) of the inductive hypothesis,
(cid:90) t (cid:90) t (cid:90) t (cid:90) t
p (z) p =rp (z) p rp (z) p =p (z) p
0 k 0 k 1 k 1 0 k 0
s s − ≥ − s s
for z s,t . It remains to consider the case (s ,t ) (s,t). Assume first that s s , so that
k k k 0
∈ { } ⊆ ≤
a [0, ). If a =0, then by part (c) of the inductive hypothesis and (55),
k k
∈ ∞
p (z)
0
p (z)= p (s ) p (s ) p (s )=p (t )
k 1 k 1 k k 1 k k 1 k k 1 k
− p 0(s k) · − ≤ − ≤ − −
for all z [s ,t ], so
k k
∈
(cid:90) t p (t ) (cid:90) sk (cid:90) tk (cid:90) t (cid:90) sk (cid:90) tk (cid:90) t (cid:90) t
k 1 k
p k = − p k 1+ p k 1(t k)dz+ p k 1 p k 1+ p k 1+ p k 1 = p k 1.
s p k −1(s k)
−∞
− sk − tk − ≥ s − sk − tk − s −
Suppose instead that a > 0. Taking s = and t = s in part (d) of the inductive hypothesis, we
k k
obtain p (s ) ψ (s
)(cid:82)sk
p a
(cid:82)−sk∞
p , so
k −1 k
≥
0∗ k
−
−∞
k −1
≥
k
−∞
k −1
(cid:82)sk
p k 1 1
−∞ − .
p (s ) ≤ a
k 1 k k
−
42Together with (59), this implies that
(cid:90) tk
p
k
=(cid:90) sk
r kp
k
1+p
k
1(t k)
1 −eak(sk−tk)
=p
k
1(t
k)eak(sk−tk)(cid:18)(cid:82) −sk ∞p k −1 1 (cid:19)
+
p k −1(t k)
− − · a k − p k 1(s k) − a k a k
−∞ −∞ (cid:18)(cid:82)sk p k 1 − 1 (cid:19) p k 1(t k)
p k 1(s k) −∞ − + −
≥ − p k 1(s k) − a k a k
−
(cid:90) sk p (t ) p (s ) (cid:90) tk
k 1 k k 1 k
= p k 1+ − − − = p k 1.
− a k −
−∞ −∞
Combining this with (57) and (58) yields
(cid:90) t (cid:90) tk (cid:90) s (cid:90) t (cid:90) tk (cid:90) s (cid:90) t (cid:90) t
p = p r p + p p p + p = p .
k k k k 1 k 1 k 1 k 1 k 1 k 1
s
−∞
−
−∞
− tk − ≥
−∞
− −
−∞
− tk − s −
Inviewof (58)andpart(b) oftheinductivehypothesis, weconcludethatif(s ,t ) (s,t)ands s ,
k k k 0
⊆ ≤
then
(cid:90) t (cid:90) t (cid:90) t (cid:90) t
p (z) p =p (z) p p (z) p p (z) p
0 k 0 k 1 k 1 0 k 0
s s − ≥ − s ≥ s
for z s,t , which proves (b) in this case. On the other hand, if (s ,t ) (s,t) and s > s , then
k k k 0
∈ { } ⊆
instead a ( ,0], but the arguments are similar and hence omitted.
k
∈ −∞
(c) For j [K] such that j >k, we have (s ,t ) (s ,t )= , so by (58) and part (c) of the inductive
j j k k
∈ ∩ ∅
hypothesis,
(cid:40)
p (z) p (z) p (z)/p (s ) if z [s ,t ] for j N with j >k
k k 1 0 0 j j j
= − = ∈ ∈
p k(s j) p k 1(s j) p j(z)/p j(s j) if z [s j,t j] for j N with j <k.
− ∈ ∈
(d) There is nothing to prove when s = t, so let s,t , be such that s < t. If either
∈ T ∪{−∞ ∞}
(s,t)
⊆
( −∞,s k) or (s,t)
⊆
(t k, ∞), then by (58), p
k
= rp
k −1
on [s,t] for some r
∈
{1,r k,r k−1 }. Thus,
by part (d) of the inductive hypothesis,
(cid:40) (cid:82)t (cid:82)t
rψ (s) p =ψ (s) p
p (t) p (s)=r(cid:0) p (t) p (s)(cid:1) ≤ 0∗ s k −1 0∗ s k
k − k k −1 − k −1 rψ (t )(cid:82)t p =ψ (t )(cid:82)t p ,
≥ 0∗ − s k −1 0∗ − s k
as required. In the remaining case, (s ,t ) (s,t), and similarly by (58) and part (d) of the inductive
k k
⊆
hypothesis,
(cid:90) sk (cid:90) sk
ψ (s ) p p (s ) p (s) ψ (s) p ,
0∗ k
−
k
≤
k k
−
k
≤
0∗ k
s s
(cid:90) t (cid:90) t
ψ (t ) p p (t) p (t ) ψ (t ) p .
0∗
−
k
≤
k
−
k k
≤
0∗ k k
tk tk
Moreover, a k(cid:82) st kkp
k
=a kp k(t k)(cid:82) st kkea(z −tk)dz =p k(t k) −p k(s k) and a
k
=ψ 0∗(s k)=ψ 0∗(t
k
−), so
(cid:90) tk (cid:90) tk
ψ (t ) p =p (t ) p (s )=ψ (s ) p .
0∗ k
−
k k k
−
k k 0∗ k k
sk sk
Since ψ is decreasing, combining this with the two inequalities above yields (d).
0∗
Lemma 24. For z R, we have
∈
p (s )
0 0
lim p (z)= p (z). (60)
k
→∞
k
p ∗0(s 0) ·
∗0
Moreover, let t := inf and t := sup . If s,t , are such that < s t
min max min
T T ∈ T ∪{−∞ ∞} −∞ ∨ ≤
t t < , then
max
∧ ∞ (cid:90) t p (s )(cid:90) t
lim p =
∗0 0
p . (61)
k →∞ s
k
p 0(s 0) s
∗0
43Proof. For k N, let ϕ := logp and define r as in (57). Then as noted above, ϕ > on , and
k k k k
∈ −∞ T
Lemma 23(c) ensures that
(cid:0) (cid:1)
(cid:90) tk
(cid:0) (cid:1)
logr =ψ (s )(t s ) ϕ (t ) ϕ (s ) = ψ (z)dz ϕ (t ) ϕ (s ) . (62)
k 0∗ k k
−
k
−
k −1 k
−
k −1 k
sk
0∗
−
0 k
−
0 k
Fix t such that t s , and for convenience, define [K]= when K =0 and [K]=N when K = .
0
∈T ≥ ∅ ∞
Let be the set of ℓ [K] such that (s ,t ) (s ,t), so that (s ,t ) (s ,t) = for ℓ [K]
ℓ ℓ 0 ℓ ℓ 0
andK hence c (s ,t) ∈ = (cid:83) (s ,t ). Similarly⊆ to the proof of Lemma∩ 22, let U ∅ U(0,1∈ ), so t\ haK t
T ∩ 0 ℓ ℓ ℓ ∼
Z := F 0−1(U)
∼
P 0, F 0(Z) =∈ UK and ψ 0∗(Z) = Jˆ 0(R)(U). We have ψ
0∗
≤
0 and p
0
> 0 on
T
∩(s 0,t), and
moreover Jˆ is Lipschitz on (F (s ),F (t)] with Jˆ(R) 0, so
0 0 0 0 0 ≤
(cid:90) t (cid:18) ψ (Z) (cid:19) (cid:18) Jˆ(R)(U) (cid:19)
ψ 1 =E 0∗ 1 =E 0 1
s0 0∗ T p 0(Z) {Z ∈T∩(s0,t) } Jˆ 0(U) {F 0−1(U) ∈T∩(s0,t) }
(cid:18) Jˆ(R)(U) (cid:19)
(cid:88)
(cid:18) Jˆ(R)(U) (cid:19)
=E 0 1 E 0 1
Jˆ 0(U) {F 0−1(U) ∈(s0,t] } −
ℓ
Jˆ 0(U) {F 0−1(U) ∈(sℓ,tℓ] }
∈K
(cid:90) F0(t) Jˆ(R) (cid:88)(cid:90) F0(tℓ) Jˆ(R)
= 0 0
F0(s0)
Jˆ
0
−
ℓ F0(sℓ)
Jˆ
0
=logJˆ(cid:0)
F
(t)(cid:1) ∈ loK gJˆ(cid:0)
F (s
)(cid:1) (cid:88)(cid:8) logJˆ(cid:0)
F (t
)(cid:1) logJˆ(cid:0)
F (s
)(cid:1)(cid:9)
0 0 0 0 0 0 0 ℓ 0 0 ℓ
− − −
ℓ
(cid:88)(cid:0) ∈K (cid:1)
=ϕ (t) ϕ (s ) ϕ (t ) ϕ (s ) , (63)
0 0 0 0 ℓ 0 ℓ
− − −
ℓ
∈K
where the second and final equalities above follow from Lemma 20 and the fact that s ,t,s ,t
0 ℓ ℓ
, for ℓ [K]. In addition, for all such ℓ, we have ϕ (t) = ϕ (t) + (logr )1 b∈ yT th∪ e
ℓ ℓ 1 ℓ ℓ
{−∞ ∞} ∈ (cid:82)t − { ∈K}
definition of p , and ϕ :=logp satisfies ϕ (t) ϕ (s )= ψ . Thus, by induction together with (62)
ℓ ∗0 ∗0 ∗0 − ∗0 0 s0 0∗
and (63),
(cid:88) (cid:88) (cid:16) (cid:90) tℓ (cid:17)
ϕ (t) ϕ (s )=ϕ (t) ϕ (s )+ logr =ϕ (t) ϕ (s ) ϕ (t ) ϕ (s ) ψ
k
−
0 0 0
−
0 0 ℓ 0
−
0 0
−
0 ℓ
−
0 ℓ
−
0∗
ℓ [k] ℓ [k]
sℓ
∈K∩ ∈K∩
(cid:18)(cid:90) t (cid:88)(cid:90) tℓ (cid:19) (cid:88) (cid:16) (cid:90) tℓ (cid:17)
= ψ 1 + ψ + ϕ (t ) ϕ (s ) ψ
s0
0∗
T ℓ sℓ
0∗
ℓ :ℓ>k
0 ℓ
−
0 ℓ
− sℓ
0∗
∈K ∈K
(cid:90) t (cid:88) (cid:16) (cid:90) tℓ (cid:17)
= ψ + ϕ (t ) ϕ (s ) ψ ϕ (t) ϕ (s )
0∗ 0 ℓ
−
0 ℓ
−
0∗
→
∗0
−
∗0 0
s0 ℓ :ℓ>k sℓ
∈K
as k , which yields (60) for z =t. On the other hand, if t and t<s , then we can instead let
0
→∞ ∈T
be the set of ℓ [K] such that (s ,t ) (t,s ), and deduce by similar reasoning that
ℓ ℓ 0
K ∈ ⊆
(cid:88) (cid:16) (cid:90) tℓ (cid:17)
ϕ (t) ϕ (s )=ϕ (t) ϕ (s )+ ϕ (t ) ϕ (s ) ψ
k
−
0 0 0
−
0 0 0 ℓ
−
0 ℓ
−
0∗
ℓ [k]
sℓ
∈K∩
(cid:90) s0 (cid:88) (cid:16) (cid:90) tℓ (cid:17) (cid:90) t
= ψ ϕ (t ) ϕ (s ) ψ ψ =ϕ (t) ϕ (s )
−
0∗
−
0 ℓ
−
0 ℓ
−
0∗
→
0∗ ∗0
−
∗0 0
t ℓ :ℓ>k sℓ s0
∈K
as k . Having proved that (60) holds for all z , we now consider z c, for which there exists
a
uni→ que∞
ℓ [K] such that z (s ℓ,t ℓ). If s
ℓ
>s 0,
th∈ enT
p ℓ(z)=p ℓ(s ℓ)eψ
0∗(sℓ)(∈
z
−T
sℓ), while if s
ℓ
s 0, then
p ℓ(z)=p ℓ(∈ t ℓ)eψ 0∗(sℓ)(z −tℓ). Fo∈ r k >ℓ, it follows from Lemma 23(c) that ≤
(cid:40)
p k(t ℓ)eψ 0∗(sℓ)(z −tℓ) if s
ℓ
s
0
p (z)= ≤
k p k(s ℓ)eψ 0∗(sℓ)(z −sℓ) if s
ℓ
>s 0.
Moreover, s ,t , , so if s s , then
ℓ ℓ ℓ 0
∈T ∪{−∞ ∞} ≤
p (s ) p (s )
p k(z)=p k(t ℓ)eψ 0∗(sℓ)(z −tℓ)
→
p0 (s0
)
·p ∗0(t ℓ)eψ 0∗(sℓ)(z −tℓ) = p0 (s0
)
·p ∗0(z)
∗0 0 ∗0 0
44as K , and (60) holds similarly when s >s .
ℓ 0
→∞
Finally, let s,t , be such that < s t t t < . If t > t , then
min max max
∈ T ∪{−∞ ∞} −∞ ∨ ≤ ∧ ∞
there exists a unique j [K] such that t = s < t = t = , and by Lemma 5 and its proof,
max j j
∈ ∞
ψ (t )=lim ψ (z) ( ,0). Moreover, by Lemma 23(c),
0∗ max z
→∞
0∗
∈ −∞
p k(z)=p k(t max)eψ 0∗(tmax)(z −tmax)
for all z > t and k j. Similarly, if s < t , then there exists a unique ℓ [K] such that
max min
≥ ∈
=s=s <t =t , and ψ (t )=lim ψ (z) (0, ). In addition, by Lemma 23(c),
−∞
ℓ ℓ min 0∗ min
−
z
→−∞
0∗
∈ ∞
p k(z)=p k(t min)eψ 0∗(tmin−)(z −tmin)
for all z <t and k ℓ. Thus, for k j ℓ, it follows from Lemma 23(a) that in all cases,
min
≥ ≥ ∨
 p 0(t min)eψ 0∗(tmin−)(z −tmin) for z ∈(s,s ∨t min)
p (z) p for z [s t ,t t ]
k ≤∥
p
0(0 t∥ m∞
ax)eψ 0∗(tmax)(z −tmax) for z
∈ (t∨ tm mi an x,t)∧
,
max
∈ ∧
so the pointwise supremum sup p is integrable on (s,t), and hence (61) follows from (60) and the
k j ℓ k
dominated convergence theorem.≥ ∨
Proof of Proposition 9. (a) Wewillprovethefollowingstrongerstatement: fors,t , such
∈T ∪{−∞ ∞}
that s t, and z s,t , we have
≤ ∈{ }
p (z) p (z)
∗0 0
. (64)
F (t) F (s) ≤ F (t) F (s)
0∗
−
0∗ 0
−
0
Assume that at least one of s,t is finite, since otherwise (64) holds trivially. Let t := inf and
min
T
t :=sup . If <s t t t < , then by Lemma 24,
max min max
T −∞ ∨ ≤ ∧ ∞
(cid:90) t p (s ) (cid:90) t p (s ) (cid:90) t (cid:90) t
p (z) p =
∗0 0
p (z) lim p lim
∗0 0
p (z) p =p (z) p (65)
0
s
∗0
p 0(s 0) ·
0
k →∞ s
k
≥k →∞p 0(s 0) ·
k
s
0 ∗0
s
0
for z s,t . Otherwise, it follows from (65) that
∈{ }
(cid:90) (cid:90) t′ (cid:90) t′ (cid:90)
∞ ∞
p (s) p =p (s) sup p p (s) sup p =p (s) p if <s<t =t=
0
s
∗0 0
· t′ s
∗0
≥
∗0
· t′ s
0 ∗0
s
0
−∞
max
∞
∈T ∈T
(cid:90) t (cid:90) t (cid:90) t (cid:90) t
p (t) p =p (t) sup p p (t) sup p =p (t) p if =s=t <t< ,
0 ∗0 0
· s′ s′
∗0
≥
∗0
· s′ s′
0 ∗0 0
−∞
min
∞
−∞ ∈T ∈T −∞
which completes the proof of (64). In particular, for z , taking (s,t) = ( ,z) and (s,t) = (z, )
∈ T −∞ ∞
in (64), we obtain
(cid:90) (cid:90) (cid:90) z (cid:90) z
∞ ∞
p (z) p p (z) p and p (z) p p (z) p ,
0 ∗0
≥
∗0 0 0 ∗0
≥
∗0 0
z z
−∞ −∞
which proves (23). Summing these inequalities yields p (z) p (z), as required.
0
≥
∗0
(b) Since s and p (z) = p (s )exp(cid:0)(cid:82)z ψ (cid:1) p (s ) for z R, we deduce that p = p (s )
0 ∈ T ∗0 ∗0 0 s0 0∗ ≤ ∗0 0 ∈ ∥ ∗0∥∞ ∗0 0 ≤
p (s ) p . Finally, by Theorem 2(c), i (p ) =
(cid:0)(cid:82)
(ψ )2p
(cid:14)
V (ψ
)(cid:1)1/2
=
(cid:82)
p dψ ; see the
0 0
≤ ∥
0
∥∞
(cid:82)∗ 0 R 0∗ 0 p0 0∗
−
R 0 0∗
equality case of (47). Similarly, i(p ) = p dψ , so because p p on the support of the
∗0
−
R ∗0 0∗ ∗0
≤
0
T
Lebesgue–Stieltjes measure induced by ψ , we have
0∗
(cid:90) (cid:90)
i (p )= p dψ p dψ =i(p ),
∗ 0
− R
0 0∗
≥− R
∗0 0∗ ∗0
as required.
456.2 Additional examples for Section 2
Example 25. For densities p that are uniformly continuous and locally absolutely continuous on R, it
0
is possible to have i (p )< =i(p ). This is unfavourable from the perspective of statistical efficiency
∗ 0 0
∞
because ARE∗(p 0)=0 in this case. To construct an example of such a density p 0, let
 (cid:18) (cid:16) π (cid:17)(cid:19)
u 2+sin for u (0,1/2]
J (u):= 4u ∈ and J (u):=J (1 u) for u (1/2,1],
0 0 0
0 for u=0 − ∈
so that J is continuous on [0,1] and infinitely differentiable on (0,1), with u J (u) 3u for all
0 0
u [0,1/2]. The least concave majorant Jˆ of J is given by Jˆ(u) = 3min(u,1≤ u) for u≤ [0,1]. In
0 0 0
∈ − ∈
addition, by the Cauchy–Schwarz inequality,
(cid:18)(cid:90) 1 (cid:19)1/2 (cid:90) 1 (cid:90) 1/2(cid:12) (cid:16) u (cid:17) π (cid:16) u (cid:17)(cid:12)
(J )2 J =2 (cid:12)2+sin cos (cid:12)= ,
0′ ≥ | 0′ | (cid:12) 4π − 4u 4π (cid:12) ∞
0 0 0
so J is not of bounded variation on (0,1). Next, defining Q : (0,1) R by Q (u) := (cid:82)u 1/J , we
0 0 → 0 1/2 0
(cid:82)1/2 (cid:82)1/2
have lim Q (u) = lim Q (u) = 1/J 1/(3u)du = . Then by Lemma 19, Q is a
strictly iu n↗ cr1 eas0 ing, con− tinuou u↘ sl0 y d0 ifferentia0 ble bij0 ec≥ tion0 from (0,1) to R∞ , and p
0
:= (Q−01)
′
is a st0 rictly
positive, uniformly continuous density on R with density quantile function J . Moreover, by Lemma 1,
0
p =(J F )p is continuous, so p is locally absolutely continuous on R. By Remark 3,
′0 0′
◦
0 0 0
(cid:90) 1 (cid:90) 1 (cid:90) 1
i (p )= (cid:0) Jˆ(R)(cid:1)2 = 32 =9< = (J )2 =i(p ),
∗ 0 0 ∞ 0′ 0
0 0 0
so indeed ARE∗(p 0) = 0. By modifying this construction slightly, one can also exhibit a unimodal
density p
0
with ARE∗(p 0)=0.
Example 26. For z R, let
∈ 1
p (z):= (1+z2) 3/2,
0 −
2
so that p is the density of W/√2 when W t . Then by Du¨mbgen et al. (2011, Example 2.9), the
0 2
∼
log-concave maximum likelihood projection pML is a standard Laplace density given by
0
1
pML(z)= e z
0 2 −| |
for z R, with score function ψML() = sgn(). Therefore, the corresponding regression M-estimator
β tˆ hψ e0ML ot∈ ∈ hea rr hgm ana dx ,β b∈yRd s(cid:80) tran i i= g1 hl to fog rp wM 0 a0 L rd(Y ci· o− mX p− ui⊤ tβ at) io= n· sa ,rgmin β ∈Rd(cid:80)n i=1|Y i −X i⊤β | is the LAD estimator. On
3z
ψ (z)=(logp )(z)= ,
0 0 ′ −1+z2
ψ (z)=(Jˆ(R) F )(z)=ψ (cid:0) (z 3 1/2) ( 3 1/2)(cid:1) =(cid:16) 3√3(cid:17) ψ (z) 3√3
0∗ 0 ◦ 0 0 ∧ − ∨ − − − 4 ∨ 0 ∧ 4
for z R, so similarly to Example 11, the optimal convex loss is a Huber-like function given by
∈
 3log(cid:0) (1+z2)/2(cid:1)
(cid:90) z  for z [ 3 −1/2,3 −1/2]
2 ∈ −
ℓ (z)= logp (z)= ψ +log2=
∗0
−
∗0
−
0
0∗ 3(cid:0)
√3 |z
|−1+log(4/9)(cid:1)
for z R [ 3 1/2,3 1/2].
− −
4 ∈ \ −
T wh iti hs tm he ea rn as tit oh oa ft tp h∗0 ei̸=
r
ap sM 0 ymL, pa tn otd icβˆ cψ o0∗ va∈ ria ar ng cm esin bβ e∈inR gd(cid:80) eqn i u= a1 lℓ t∗0 o(Y i −X i⊤β) is usually different from βˆ ψ 0ML,
VV
p0
(ψ(ψ
M0∗
L)
)
=
1/1 (cid:0)/ 4i p∗( (p
00
))
2(cid:1) =
i
(1
p )
=
−(cid:18)(cid:90) 1/√3
p 0dψ
0(cid:19) −1
=
98 30
≈0.860.
p0 0 0 ∗ 0 1/√3
−
46Example 27. For α,σ >0, consider the symmetrised Pareto density p : R R given by
0
→
ασα
p (z):= .
0 2(z +σ)α+1
| |
The corresponding distribution has a finite mean if and only if α > 1, in which case its log-concave
maximum likelihood projection (Du¨mbgen et al., 2011) is a Laplace density given by
(cid:18) (cid:19)
α 1 (α 1)z
pML(z):= − exp − | | (66)
0 2σ − σ
for z R; see Chen and Samworth (2013, p. 1382) or Samworth and Shah (2024, Exercise 10.14). On
∈
the other hand, for general α,σ >0, routine calculations yield
(cid:0) (cid:1)1+1/α
α 2min(u,1 u) αmin(u,1 u)
J (u)= − and Jˆ(u)= −
0 0
2σ σ
for u [0,1]. Since p is symmetric about 0, it follows that ψ (z)=(1 21 )α/σ and hence
∈
0 0∗
−
{z ≥0
}
(cid:16) α (cid:17) αz α (cid:16) αz (cid:17)
ℓ ∗0(z)= −logp ∗0(z)=log
2σ −
σ| |, p ∗0(z)=
2σ
exp
−
σ| |
for z R. Therefore, p is also a Laplace density, but while pML =p when α>1, both corresponding
regres∈ sion M-estimator∗0 s βˆ ψ 0ML and βˆ ψ 0∗ in (21) minimise β (cid:55)→0 (cid:80)̸ n i=∗0 1|Y i − X i⊤β | over Rd, and hence
coincide when there is a unique minimiser (i.e. least absolute deviation estimator).
Proposition 28. For ρ (0,1), µ ,µ R and σ > 0, let P := (1 ρ)N(µ ,σ2)+ρN(µ ,σ2) and
1 2 0 1 2
∈ ∈ −
denote by pML and p its log-concave maximum likelihood and Fisher divergence projections respectively.
0 ∗0
(a) If µ µ 2σ, then P has density pML =p .
| 1 − 2 |≤ 0 0 ∗0
(b) If µ µ >2σ, then pML =p . Moreover, ψ is continuous on R, and there exist z ( ,a]
| 1 − 2 | 0 ̸ ∗0 0∗ 1 ∈ −∞
and z [b, ) such that ψ is decreasing on ( ,z ] [z , ) and constant on [z ,z ].
2
∈ ∞
0∗
−∞
1
∪
2
∞
1 2
Proof. By Cule et al. (2010) and Samworth and Shah (2024, Exercise 10.11), p is log-concave if and
0
only if µ µ 2σ. We will slightly refine this result by first noting that
1 2
| − |≤
1 ρ
(cid:18)
(z µ
)2(cid:19)
ρ
(cid:18)
(z µ
)2(cid:19)
1 2
p (z)= − exp − + exp −
0 √2πσ − 2σ2 √2πσ − 2σ2
1
(cid:26)
(1 ρ)(µ z)
(cid:18)
(z µ
)2(cid:19)
ρ(µ z)
(cid:18)
(z µ
)2(cid:19)(cid:27)
1 1 2 2
ψ 0(z)=(logp 0) ′(z)=
p 0(z)
− √2πσ3− exp
−
−
2σ2
+ √2π−
σ3
exp
−
−
2σ2
g(z) (cid:18) (z µ )2+(z µ )2(cid:19)
1 2
ψ 0′(z)=(logp 0) ′′(z)=
−2πσ4p (z)2
exp
−
−
2σ2
−
0
for z R, where
∈
(cid:16) µ µ (cid:17) (cid:16)µ µ (cid:17)
g(z):=(1 ρ)2exp 2 − 1 (2z µ µ ) +ρ2exp 2 − 1 (2z µ µ )
− − 2σ2 − 1 − 2 2σ2 − 1 − 2
(cid:110) (cid:16)µ µ (cid:17)2(cid:111)
1 2
+ρ(1 ρ) 2 −
− − σ
is a convex function of z with
(cid:110) (cid:16)µ µ (cid:17)2(cid:111)
1 2
ming(z)=ρ(1 ρ) 4 − .
z R − − σ
∈
(a) If µ µ 2σ, then g 0 on R and hence P has a log-concave density p =pML =p .
| 1 − 2 |≤ ≥ 0 0 0 ∗0
(b) Otherwise, if µ µ > 2σ, then there exist a < b such that g 0 and hence ψ 0 on
|
1
−
2
| ≥
0′
≤
( ,a] [b, ),whileg 0andψ 0on[a,b]. Therefore,byDu¨mbgenetal.(2011,Examples2.11(ii)
−∞ ∪ ∞ ≤
0′
≥
and 2.12), there exist a ( ,a] and b [b, ) such that ϕ := logpML agrees with ϕ := logp on
′ ∈ −∞ ′ ∈ ∞ 0 0 0
( ,a] [b, ), and ϕ is affine on [a,b].
′ ′ ′ ′
−∞ ∪ ∞
47Suppose for a contradiction that pML is differentiable on R. Then ψ (a) = ϕ (a) = ϕ(a) =
0 0 ′ ′0 ′ ′ ′
ϕ(b) = ψ (b), so defining ℓ: R R by ℓ(z) := ϕ (a)+ψ (a)(z a), we have ϕ = ℓ on [a,b] and
′ ′ 0 ′ 0 ′ 0 ′ ′ ′ ′
ℓ(z) = ϕ (b)+ψ (b)(z b) for→ z R. Since ϕ is concave on bo− th ( ,a] and [b, ) while being
0 ′ 0 ′ ′ 0
− ∈ −∞ ∞
convex on [a,b], it follows that ϕ ℓ on ( ,a] [b, ) and
0
≤ −∞ ∪ ∞
b z z a b z z a
ϕ (z) − ϕ (a)+ − ϕ (b) − ℓ(a)+ − ℓ(b)=ℓ(z)=ϕ(z)
0 0 0
≤ b a b a ≤ b a b a
− − − −
for all z [a,b]. Therefore, logp = ϕ ϕ = logpML on R, but since (cid:82) p = (cid:82) pML = 1, we must
∈ 0 0 ≤ 0 R 0 R 0
have p =pML because both functions are continuous. However, this contradicts the fact that p is not
0 0 0
log-concave, so pML is not differentiable (at either a or b).
0 ′ ′
F
0−1O in st dh iffe eo rt eh ne tr iah ba ln ed o, np 0 (0a ,n 1d ).it Bs yco Lrr ee msp mo and 4i 7n ,g Jˆq 0u ia sn ct oil ne tf iu nn uc ot ui so ln yF d0 i− ff1 era er ne td iaiff be lere on nti (a 0b ,l 1e )o ,n soR (, ls oo gJ p0 ∗0)= (Rp )0 =◦
ψ =Jˆ(R) F is continuous on R. Therefore, logp is differentiable on R and hence pML =p .
0∗ 0 ◦ 0 ∗0 0 ̸ ∗0
Moreover, by the first line of the proof of (b), J
0′
= ψ
0
◦F 0−1 is increasing on [F 0(a),F 0(b)], and
decreasing on both (0,F (a)] and [F (b),1). We deduce from Lemma 48 that there exist z ( ,a]
0 0 1
∈ −∞
and z [b, ) such that ψ is decreasing on ( ,z ] [z , ) and constant on [z ,z ].
2
∈ ∞
0∗
−∞
1
∪
2
∞
1 2
6.3 Proofs for Section 3
Proof of Proposition 13. In both (a) and (b), X and ε are independent, so by the final assertion
1 1
of Lemma 1, we have E(cid:8) X ψ (Y X β )(cid:9) = E(X )Eψ (ε ) = 0. Therefore, (β ,ψ ) satisfies the
1 0∗ 1
−
1⊤ 0 1 0∗ 1 0 0∗
constraint (29). Moreover, Y X β =ε has density q =p , so by Theorem 2(b,c),
1
−
1⊤ 0 1 β0 0
1
Q(β ,ψ )=D (ψ )= = inf D (ψ)= inf Q(β ,ψ).
0 0∗ p0 0∗
−i ∗(p 0) ψ ∈Ψ↓(p0)
p0
ψ ∈Ψ↓(p0)
0
(a) Since p is symmetric and has a continuous distribution function F , we have F (z) = 1 F ( z)
0 0 0 0
f fo or
r
e av ller uy z
[∈
0,R 1], .s Fo oJ r0 e( vu e) ry:= z(p 0
R◦
,F w0− e1 h)( au v) e= Fp (0 z(cid:0) )−F (0− 01 ,( 11
)−
beu c) a(cid:1) us= eJ 0 (( p1
−
)=u) Ra ,n sd ohence Jˆ 0(u)=− Jˆ 0(1 −− u)
0 0
∈ ∈ ∈ S
ψ ( z)=Jˆ(R)(cid:0) F ( z)(cid:1) =Jˆ(R)(cid:0) 1 F (z)(cid:1) = Jˆ(L)(cid:0) F (z)(cid:1) = lim Jˆ(R)(u)= lim ψ (z ), (67)
0∗ − 0 0 − 0 − 0 − 0 0 −u ↗F0(z) 0 −z′ ↗z 0∗ ′
where the penultimate equality follows from Rockafellar (1997, Theorem 24.1). Thus, ψ Ψanti(p )=
Ψanti(q ).
0∗
∈ ↓
0
β0
↓ Next, we claim that if (β,ψ) Γanti is such that β = β , then necessarily Q(β,ψ) 0 > Q(β ,ψ ),
which proves (a). Indeed, E(X X∈ ) is positive definit̸ e
by0
assumption, so T := X (≥ β β )
sa0 tisfi0∗
es
E(T2) = (β β ) E(X X
)(β1 β1⊤
) > 0 and hence P(T = 0) > 0. By (29), E(cid:0)
Tψ1⊤
(ε −
T0
)(cid:1) = 0, so
−
0 ⊤ 1 1⊤
−
0
̸
1
−
because T and ε are independent, there exists t = 0 such that Eψ(ε t) < . Since ε =d ε
1 1 1 1
and ψ agrees Lebesgue almost everywhere on R wit̸ h an antisymmet| ric fun− ctio| n, w∞ e have Eψ(ε +t− )=
1
Eψ( ε +t) = Eψ(ε t) R. Finally, since ψ(ε ) ψ(ε t) ψ(ε +t), this shows that
1 1 1 1 1
Eψ(− ε ) < . T− hus, Eψ(− ε )=∈Eψ( ε )= Eψ(ε )| , so a| ga≤ in| becau− se T|∨ an| d ε are in| dependent,
1 1 1 1 1
| | ∞ − −
E(cid:0) Tψ(ε )(cid:1) =E(T)Eψ(ε )=0=E(cid:0) Tψ(ε T)(cid:1) . (68)
1 1 1
−
For z R, we have
∈
(cid:90) 1 (cid:90) z+T (cid:90)
T2 p (z+sT)ds=T p (w)dw = ∞ Tp (w)(cid:0)1 1 (cid:1) dw.
0 0 0 w T<z w w<z w T
0 z { − ≤ }− { ≤ − }
−∞
Hence, by Fubini’s theorem and the independence of T and ε , we have
1
(cid:90) 1(cid:90) (cid:90) (cid:90)
E T2p (z+sT)dψ(z)ds=E ∞ p (w) T(cid:0)1 1 (cid:1) dψ(z)dw
0 0 w T<z w w<z w T
0 R R { − ≤ }− { ≤ − }
(cid:90)−∞
=E ∞ T(cid:0) ψ(w) ψ(w T)(cid:1) p (w)dw
0
− −
=E(cid:0) T− ψ∞
(ε
)(cid:1) E(cid:0)
Tψ(ε
T)(cid:1)
=0.
1 1
− −
48Defining g: [0,1] R by g(s) := E(cid:82) T2p (z+sT)dψ(z) 0, we therefore have (cid:82)1 g = 0, so g = 0
Lebesgue almost e→ verywhere. Now− p isR cont0 inuous on R, so b≥ y Fatou’s lemma, 0
0
(cid:90)
0 E T2p (z+T)dψ(z)=g(1) liminfg(s)=0, (69)
0
≤− R ≤ s 1
(cid:90) (cid:90) ↗
0 E(T2) p (z)dψ(z)= E T2p (z)dψ(z)=g(0) liminfg(s)=0. (70)
0 0
≤− R − R ≤ s 0
↘
By (69), (cid:82) p (z +T)1 dψ(z) = 0 almost surely. Moreover, since E(T2) > 0, (70) implies that
R 0 T=0
(cid:82) p (z+T)1 dψ(z{ )̸ =}1 (cid:82) p (z)dψ(z)=0 almost surely, so
R 0 T=0 T=0 R 0
{ } { }
(cid:90) (cid:90)
E p (cid:0) z+X (β β )(cid:1) dψ(z)=E p (z+T)dψ(z)=0. (71)
R
0 1⊤
−
0
R
0
Hence, by the definition of Q(β,ψ) in (28),
(cid:90)
Q(β,ψ)=E ∞ ψ2(cid:0) z X (β β )(cid:1) p (z)dz 0>Q(β ,ψ ),
−
1⊤
−
0 0
≥
0 0∗
−∞
as claimed.
(b) Forc R,β Rd,ψ Ψ (q )andx=(x˜,1) Rd 1 R,wehavex (β β)=x (β β ce )+c,
β − ⊤ 0 ⊤ 0 d
∈ ∈ ∈ ↓ ∈ × − − −
so ψ ():=ψ( +c) satisfies
c
· ·
(cid:90) (cid:90)
∞ ψ2(cid:0)
z+x (β β ce
)(cid:1)
p (z)dz =
∞ ψ2(cid:0)
z+x (β
β)(cid:1)
p (z)dz,
c ⊤ 0 − − d 0 ⊤ 0 − 0
−∞(cid:90) (cid:90)−∞
(cid:0) (cid:1) (cid:0) (cid:1)
and p z x (β β ce ) dψ (z)= p z x (β β ce ) c dψ(z)
0 ⊤ 0 d c 0 ⊤ 0 d
R − − − R − − − −
(cid:90)
(cid:0) (cid:1)
= p z x (β β) dψ(z).
0 ⊤ 0
R − −
It follows that if X =1 almost surely, then Q(β+ce ,ψ )=Q(β,ψ).
1d d c
For β :=β +ce , we have q ()=p ( +c), and ψ Ψ (q ) if and only if ψ Ψ (q )=Ψ (p ).
c 0 d βc
·
0
·
c
∈ ↓
βc
∈ ↓
β0
↓
0
Thus,
1
0> =Q(β ,ψ )= inf Q(β ,ψ)= inf Q(β ,ψ ) (72)
−i ∗(p 0)
0 0∗
ψ ∈Ψ↓(p0)
0
ψc∈Ψ↓(qβc)
c c
for all c R.
Next,∈ for any (β,ψ) Γ, we claim that there exists c R such that Eψ(ε c) = 0. Indeed,
1
X = 1 almost surely, ∈ so T = X (β β ) satisfies Eψ(ε∈ T) = Eψ(Y X− β) = 0. By the
in1 dd
ependence of T and ε, it follows
t1⊤
hat
Ψ−
(t)0
:=
(cid:82)
∞ ψ(z
t)1
p−
0(z)dz =
Eψ(ε1
1−
t)1⊤
is well-defined and
finite for P -almost every t R, where P denot−es∞the d− istribution of T. Sinc− e ψ is decreasing, Ψ is
T T
∈
increasing, so there exist t t in the support of P such that < Ψ(t ) 0 Ψ(t ) < , and
1 2 T 1 2
≤ −∞ ≤ ≤ ∞
ψ(ε t) ψ(ε t ) ψ(ε t ) forallt [t ,t ]. Itfollowsfromthedominatedconvergencetheorem
1 1 1 1 2 1 2
| − |≤| − |∨| − | ∈
that Ψ(t) is continuous in t [t ,t ], so by the intermediate value theorem, there exists c [t ,t ] such
1 2 1 2
that Eψ(ε c)=Ψ(c)=0,∈ as claimed. ∈
1
By (29)− together with the independence of T˜ :=T c=X (β β ) and ε , we deduce that
−
1⊤
−
c 1
E(cid:0) T˜ψ(ε c)(cid:1) =0=E(cid:0) T˜ψ(ε c T˜)(cid:1) .
1 1
− − −
This is similar to (68) in the proof of (a), but with T and ε replaced by T˜ and ε c respectively.
1 1
E(T˜S 2u )p =po (s βe no βw )th Ea (t Xβ X̸= )(β βc′ fo βr )a >ny 0.c ′
B∈
y
aR rg. uiS ni gnc se imE il( aX rl1 yX t1⊤ o) (ai )s ap no dsit ni ov te ind gefi th− n ait te pb (y )a :s =su pm (pt +ion c),
−
c ⊤ 1 1⊤
−
c c
·
0
·
is the density of ε c, we conclude as in (71) that
1
−
(cid:90) (cid:90) (cid:90)
E p (cid:0) z+X (β β )(cid:1) dψ(z)=E p (z+T)dψ(z)=E p (z+T˜)dψ(z)=0.
R
0 1⊤
−
0
R
0
R
c
Thus, if (β,ψ) Γ is such that β =β c′ for any c ′ R, then
∈ ̸ ∈
(cid:90)
Q(β,ψ)=E ∞ ψ2(cid:0) z X (β β )(cid:1) p (z)dz 0>Q(β ,ψ ).
−
1⊤
−
0 0
≥
0 0∗
−∞
Together with (72), this completes the proof.
49In the rest of this section, we establish the antitonic efficiency results in Sections 3.1 and 3.2. We
first present a unified analysis of the score estimators in both settings, before completing the proofs of
Theorems 14 and 16 in Sections 6.3.1 and 6.3.2 respectively.
Lemma 29 below demonstrates the L2(P ) consistency of the initial score estimates ψ˜ . It extends
0 n,j
the proof of van der Vaart (1998, Lemma 25.64) to kernel density estimators based on out-of-sample
residuals εˆ : i I instead of the unobserved regression errors ε . The three folds of the data will
i j+1 i
{ ∈ }
be denoted by := (X ,Y ):i I for j =1,2,3.
j n,j i i j
D ≡D { ∈ }
Lemma29. Under (A2)–(A4),supposethatforj 1,2,3 ,thepilotestimatorsβ¯(j) in (30)aredefined
n
intermsofadifferentiable,antisymmetricandstric∈ tly{ decrea} singfunctionψ satisfyingEψ(ε )=0. Then
1
for each j, we have
(cid:90)
(ψ˜ ψ )2p p 0
n,j 0 0
R − →
as n .
→∞
Proof. It suffices to prove the result for ψ˜ ψ˜ , since ψ˜ ,ψ˜ are obtained by permuting the three
n n,1 n,2 n,3
foldsofthedata(whichhaveroughlythesa≡ mesize). SinceE X ψ(Y X β ) =E(X )Eψ(ε )=0, it
{
1 1
−
1⊤ 0
}
1 1
followsfrome.g.YohaiandMaronna(1979)orHeandShao(2000)thatβ¯ β¯(1) satisfies√n(β¯ β )=
n n n 0
O (1). We have nh2 =(nh3γ2)(h γ2) 1 by (A2), so β¯ β =o≡ (h ) as n . −
p n n n n n − →∞ ∥ n − 0 ∥ p n →∞
By (A4) and the Cauchy–Schwarz inequality, p
(cid:82)
p
(cid:0)(cid:82)
(p )2/p
(cid:1)1/2
= i(p )1/2 <
. Since K is twice continuously differentiable ∥ an0 d∥∞ sup≤ porR t| ed′0| o≤ n [ { 1p ,0 1> ]0 b} y ′0 (A3),0 we have (cid:82)0 K2
R
(cid:82)∞ (K )2 < . Denote by P the distribution of X on Rd. For − y R, n N and h h , le∨ t
R ′ X 1 n
p˜ (y) p˜
∞
(y)= I
1(cid:80)
K (y εˆ) and
∈ ∈ ≡
n ≡ n,1 | 2 |− i ∈I2 h − i
(cid:90) (cid:90)
p n(y):=E(cid:0) p˜ n(y)(cid:12) (cid:12) 1(cid:1) = K h(cid:0) y z x ⊤(β 0 β¯ n)(cid:1) p 0(z)dzdP X(x)
D Rd R − − −
(cid:90) (cid:90)
= p (cid:0) y uh x (β β¯ )(cid:1) K(u)dudP (x).
0 ⊤ 0 n X
Rd R − − −
Since β¯ β =o (h) and p is uniformly continuous on R, it follows from the bounded convergence
n 0 p 0
∥ − ∥
theorem that p (y) p p (y) for every y R. Moreover, by Fubini’s theorem,
n 0
→ ∈
(cid:90)
p p
n 0
R| − |
(cid:90) (cid:12)(cid:90) (cid:90) (cid:12)
= (cid:12) (cid:12) (cid:8) p 0(cid:0) y uh x ⊤(β 0 β¯ n)(cid:1) p 0(y)(cid:9) K(u)dudP X(x)(cid:12) (cid:12)dy
R(cid:12) Rd R − − − − (cid:12)
(cid:90) (cid:90) (cid:90) (cid:90)
≤ R Rd R R|p ′0(z) |(1 {y<z ≤y −uh −x⊤(β0−β¯ n) }+1 {y −uh −x⊤(β0−β¯ n)<z ≤y })dzK(u)dudP X(x)dy
(cid:18)(cid:90) (cid:19)(cid:90) (cid:90)
= p (z) dz uh+x (β¯ β )K(u)dudP (x)
R|
′0
| Rd R|
⊤ n
−
0
|
X
(cid:18)(cid:90) (cid:19)(cid:18) (cid:90) (cid:19)
p h uK(u)du+ β¯ β E X =O (h). (73)
≤ R|
′0|
R| | ∥
n
−
0
∥· ∥
1
∥
p
By (A2) and the dominated convergence theorem, we may differentiate under the integral sign to obtain
(cid:90) (cid:90)
p ′n(y)=E(cid:0) p˜ ′n(y)(cid:12) (cid:12) D1(cid:1) =
Rd
RK h′(cid:0) y −z −x ⊤(β 0 −β¯ n)(cid:1) p 0(z)dzdP X(x).
For y R, we have
∈
Var(cid:0) p˜ n(y)(cid:12) (cid:12) 1(cid:1) 1 (cid:90) (cid:90) K h(cid:0) y z x ⊤(β 0 β¯ n)(cid:1)2 p 0(z)dzdP X(x) ∥p 0 ∥∞(cid:82) RK2 , (74)
D ≤ I 2 Rd R − − − ≤ I 2 h
Var(cid:0) p˜ ′n(y)(cid:12) (cid:12) D1(cid:1)
≤
|
I1
2|
(cid:90) Rd(cid:90) RK h′(cid:0) y −z −x ⊤(β 0 −β¯ n)(cid:1)2 p 0(z)dzdP X(x)
≤
∥p 0
∥|
∞
I
2|(cid:82)
hR( 3K ′)2 . (75)
| | | |
Since I h3 by (A2), it follows from Lemma 41 that p˜ (y) p (y) p 0 and p˜ (y) p (y) p 0.
Definin| g2 | τ(n z)→ :=∞(cid:82)
p (y z) p (y) dy
2(cid:82)
p for z
Rn
,
we− han
ve
lim→ τ(z)′n
= 0
−
by
c′n ontin→
uity
R
|
′0
− −
′0
| ≤
R
|
′0|
∈
z →0
50of translation (Folland, 1999, Proposition 8.5). Thus, since β¯ β p 0, it follows from the bounded
n 0
− →
convergence theorem that
(cid:90) (cid:90) (cid:12)(cid:90) (cid:90) (cid:12)
R|p ′n−p ′0|= R(cid:12) (cid:12)
(cid:12) Rd
R(cid:8) p ′0(cid:0) y −uh −x ⊤(β 0 −β¯ n)(cid:1) −p ′0(y)(cid:9) K(u)dudP X(x)(cid:12) (cid:12) (cid:12)dy
(cid:90) (cid:90)
τ(cid:0) uh+x (β β¯ )(cid:1) K(u)dudP (x) p 0.
⊤ 0 n X
≤ Rd R − →
Thismeansthateverysubsequenceof(p )hasafurthersubsequence(p )suchthatwithprobability1,
n nk
we have p p and hence (p )2/p (p )2/p Lebesgue almost everywhere. By the Cauchy–
′nk
→
′0 ′nk nk
→
′0 0
Schwarz inequality,
p p′n( (y y) )2
=
(cid:0)(cid:82) (cid:82)Rd(cid:82) (cid:82)Rp p′0(cid:0) (cid:0)y y−u uh h−x x⊤( (β
β0
−β¯ β¯n) )(cid:1) (cid:1)K K( (u u) )d du ud dP PX( (x x) )(cid:1)2
n Rd R 0 − − ⊤ 0 − n X
(cid:90) (cid:90) (p ′0)2 (cid:0) y uh x (β β¯ )(cid:1) K(u)dudP (x)
⊤ 0 n X
≤ Rd R p 0 − − −
for y R and n N, so by Fubini’s theorem,
∈ ∈
(cid:90) (p )2 (cid:90) (p )2
′n ′0
(76)
R p n ≤ R p 0
foralln. Thus,byapplyingaslightgeneralisationofScheff´e’slemma(vanderVaart,1998,Lemma2.29)
to subsequences of (p ), we obtain
n
(cid:90) (cid:16) p ′n p ′0 (cid:17)2 p
0 (77)
R √p n − √p 0 →
(cid:82)
as n . Since ψ2p =i(p )< and α while γ 0, we can argue similarly to the proof
→∞ R 0 0 0 ∞ n →∞ n →
of van der Vaart (1998, Lemma 25.64) and deduce by the dominated convergence theorem that
(cid:18)(cid:90) (cid:12) (cid:19) (cid:90)
E
S˜c
(ψ˜ n −ψ 0)2p 0 (cid:12) (cid:12) (cid:12)D1 = Rψ 0(y)2p 0(y) ·(cid:8)P(cid:0) |p˜ ′n(y) |>α n(cid:12) (cid:12) D1(cid:1) +P(cid:0) p˜ n(y)<γ n(cid:12) (cid:12) D1(cid:1)(cid:9) dy →p 0.
n,1
Moreover, on S˜ , we have
n,1
|ψ˜
n
−ψ
0
|√p
0
≤
| pp ˜˜ n′n| |√p
0
−√p
n
|+ √ p˜p nn |p˜ ′n−p ′n|+ √|p p′n n| |p n p˜− np˜ n | +(cid:12) (cid:12)
(cid:12)
√p p′n
n
−
√p p′0 0(cid:12) (cid:12)
(cid:12)
| |
≤
α
γ
nn |p
0
−p
n
|1/2+ √ γp nn |p˜ ′n−p ′n|+ √|p p′n n| |p n γ− np˜ n | +(cid:12) (cid:12)
(cid:12)
√p p′n
n
−
√p p′0 0(cid:12) (cid:12) (cid:12),
so by (74)–(77) and (A2),
(cid:18)(cid:90) (cid:12) (cid:19)
1 E (ψ˜ n ψ 0)2p 0 (cid:12) (cid:12) 1
4 S˜
n,1
− (cid:12)D
α n2 (cid:90)
p p
+(cid:90) p n(y) Var(cid:0)
p˜ (y)
(cid:1) dy+(cid:90) (p ′n)2 (y)Var(cid:0)
p˜ (y)
(cid:1)
dy
≤ γ n2 R| n − 0 | R γ n2 ′n |D1 R γ n2p n n |D1
(cid:90) (cid:16) p p (cid:17)2
+
′n ′0
R √p n − √p 0
≤
α
γ
n2n2 (cid:90)
R|p n −p 0 |+
∥p
0 |∥ I∞ 2
|h(cid:82)
3 nR
γ(K
n2
′)2
+
∥p
|0 I∥ 2∞
|h(cid:82)
nR γ
n2K2 (cid:90)
R
(p
p′0
0)2 +(cid:90) R(cid:16) √p
p′n n −
√p
p′0
0(cid:17)2
(cid:16)α2 (cid:17) (cid:16) 1 (cid:17)
=O nh +O +o (1)=o (1).
p γ2 n p I h3γ2 p p
n | 2 | n n
Therefore, by Lemma 41, (cid:82) (ψ˜ ψ )2p p 0.
R n 0 0
− →
Next,weuseLemma29andpropertiesofantitonicprojections(Section6.5)toshowthattheprojected
score estimates ψˆ are consistent in L2(P ) for the population-level antitonic projection ψ of ψ .
n,j 0 0∗ 0
51Lemma 30. Suppose that (A1)–(A5) hold and that Eψ(ε )=0. Then for j 1,2,3 , we have
1
∈{ }
(cid:90)
(ψˆ ψ )2p p 0. (78)
R
n,j
−
0∗ 0
→
Moreover, Ψˆ (t) := (cid:82) ψˆ (z t)p (z)dz R for all t R, and for any sequence (u ) satisfying
n,j R n,j 0 n
− ∈ ∈
u (α /γ ) 0, we have
n n n
→
(cid:90)
sup (cid:0) ψˆ (z t) ψˆ (z s)(cid:1)2 p (z)dz p 0, (79)
n,j n,j 0
s,t ∈[ −un,un] R − − − →
(cid:12) (cid:12)Ψˆ n,j(t) Ψˆ n,j(s) (cid:12) (cid:12) p
∆ n(u n):= sup (cid:12) − i ∗(p 0)(cid:12) 0 (80)
(cid:12) t s − (cid:12)→
s,t ∈[ −s=un t,un]: −
̸
as n .
→∞
Proof. Similarly to Lemma 29, it suffices to prove this result when j = 1, so we will drop the index j
in all of our notation below. In the terminology of Section 6.5, ψˆ ψˆ = Π (ψ˜ ,P˜ ) is defined as
n n,1 n n
an L2(P˜ )-antitonic projection of the initial score estimator ψ˜ ψ˜≡ , where P↓˜ P˜ denotes the
n n n,1 n n,1
≡ ≡
distribution with density p˜ p˜ . For each n, let
n n.1
≡
ψ˜ :=Π (ψ˜ ,P )
n∗ n 0
↓
denote the L2(P )-antitonic projection of ψ˜ . Then by (116) and the definition (31) of ψ˜ , we have
0 n n
ψˆ ψ˜ ψ˜ α /γ < . We have ψ˜ L2(P ) L2(P˜ ) for all n, so by (114) in
∥
n
∥∞ ∨∥
n∗
∥∞ ≤ ∥
n
∥∞ ≤
n n
∞
n
∈
0
∩
n
Lemma 44 together with the triangle inequality,
(cid:90)
ψˆ ψ˜ 2 (ψ˜ ψˆ )(ψˆ ψ˜ )d(P˜ P )
∥ n − n∗ ∥L2(P0) ≤ R n∗ − n n − n n − 0
(cid:90) (cid:16)α (cid:17)2(cid:90)
(ψ˜ ψˆ )(ψˆ ψ˜ ) p˜ p 4 n (p˜ p + p p ), (81)
≤∥
n∗
−
n n
−
n
∥∞ R|
n
−
0
|≤ γ n R |
n
−
n
| |
n
−
0
|
where p n(y)=E(cid:0) p˜ n(y)(cid:12) (cid:12) 1(cid:1) for y R. By (A4), E(ε 1 δ)=(cid:82) R z δp 0(z)dz < for some δ (0,1]. Fix
ρ (cid:0) 0,δ/(δ+1)(cid:1) . ThenD by Lemma∈ 39, (A1), (A3) a| nd| the fact| th| at β¯ p β w∞ hen Eψ(ε )=∈ 0, we have
n 0 1
∈ →
E(cid:18)(cid:90) R|p˜
n
−p
n
|(cid:12) (cid:12)
(cid:12)
D1(cid:19)
≤
21 −2 (ρ ∥ IK
2
h∥ )∞ ρC δρ ,ρ(1+h)δ(1 −ρ)(cid:16) 1+(cid:90) Rd(cid:90) R|z |δp 0(cid:0) z −x ⊤(β¯
n
−β 0)(cid:1) dzdP X(x)(cid:17)
| |
21 2ρ K Cρ
= − (∥ I h∥ )∞ ρ δ,ρ(1+h)δ(1 −ρ)(cid:0) 1+E(cid:8) |ε 1+X 1⊤(β¯ n −β 0) |δ(cid:12) (cid:12) D1(cid:9)(cid:1)
2
| |
21 2ρ K Cρ
≤
− (∥
I
h∥ )∞
ρ
δ,ρ(1+h)δ(1 −ρ)(cid:8) 1+E( |ε
1
|δ)+E( ∥X
1
∥δ) ·∥β¯
n
−β
0
∥δ(cid:9)
2
| |
=O
(cid:0)
(nh)
ρ(cid:1)
p −
(cid:82)
as n and h 0 with nh , where C = (1+ z ) δ(1 ρ)/ρdz < . Thus, by Lemma 41
δ,ρ R − −
→ ∞(cid:82) → (cid:0) → ∞(cid:1) | | ∞
and (A2), p˜ p = O (nh) ρ = O (n 2ρ/3). Combining this with (73), (81) and (A2) yields
R n n p − p −
ψˆ ψ˜ 2 | − = O|(cid:0) (h n 2ρ/3)(α /γ )2(cid:1) = o (1) as n . Hence, by applying the triangle
∥ n − n∗ ∥L2(P0) p n ∨ − n n p → ∞
inequality for together with (115) and Lemma 29, we obtain
∥·∥L2(P0)
(cid:18)(cid:90) (cid:19)1/2
(ψˆ ψ )2p = ψˆ ψ ψˆ ψ˜ + ψ˜ ψ
R
n
−
0∗ 0
∥
n
−
0∗ ∥L2(P0)
≤∥
n
−
n∗ ∥L2(P0)
∥
n∗
−
0∗ ∥L2(P0)
ψˆ ψ˜ + ψ˜ ψ p 0,
≤∥
n
−
n∗ ∥L2(P0)
∥
n
−
0 ∥L2(P0)
→
which proves (78).
For t R, let p ():=p ( +t). We have ψˆ ψ˜ α /γ for each n, so Ψˆ (t) Ψˆ (t)=
t 0 n n n n n n,1
(cid:82) ψˆ p i∈ s finite for e· very t. · Moreover, ∥ ∥∞ ≤∥ ∥∞ ≤ ≡
R n t
(cid:90) (cid:18)(cid:90) (cid:90) (cid:90) (cid:19)
R(ψˆ
n
−ψ 0∗)2p
t
≤3 Rψˆ n2(cid:0) √p
t
−√p 0(cid:1)2 + R(cid:0) ψˆ n√p
0
−ψ 0∗√p 0(cid:1)2 + R(ψ 0∗)2(cid:0) √p
0
−√p t(cid:1)2
≤3(cid:18)(cid:16)α
γ
nn(cid:17)2(cid:90)
R(cid:0) √p
t
−√p 0(cid:1)2
+(cid:90)
R(ψˆ
n
−ψ 0∗)2p
0+(cid:90)
R(ψ 0∗)2(cid:0) √p
0
−√p
t(cid:1)2(cid:19)
(82)
52and
(cid:90)
(cid:0) ψˆ (z t) ψˆ (z)(cid:1)2 p (z)dz
n n 0
R − −
(cid:90)
3 (cid:8)(cid:0) ψˆ (z t) ψ (z t)(cid:1)2 +(cid:0) ψ (z t) ψ (z)(cid:1)2 +(cid:0) ψ (z) ψˆ (z)(cid:1)2(cid:9) p (z)dz
≤ R
n
− −
0∗
−
0∗
− −
0∗ 0∗
−
n 0
(cid:18)(cid:90) (cid:90) (cid:90) (cid:19)
=3 (ψˆ ψ )2p + (cid:0) ψ (z t) ψ (z)(cid:1)2 p (z)dz+ (ψˆ ψ )2p . (83)
R
n
−
0∗ t
R
0∗
− −
0∗ 0
R
n
−
0∗ 0
Since ψ is decreasing, it is continuous Lebesgue almost everywhere and (ψ )2(z t) (ψ )2(z t )
(ψ
)2(z0∗
+t ) for all z R and t [ t ,t ]. Thus, by (A5) and the
domin0∗
ated− conv≤
ergen0∗
ce th−
eor0
em∨ ,
(cid:82)
0∗ 0
(cid:82) ∈ ∈
−(cid:82)0 0
(ψ )2p = (ψ )2(z t)p (z)dz (ψ )2p as t 0. Hence by the continuity of p and a slight
R 0∗ t R 0∗
−
0
→
R 0∗ 0
→
0
generalisation of Scheff´e’s lemma (van der Vaart, 1998, Lemma 2.29),
(cid:90) (cid:90)
R(ψ
0∗)2(cid:0)
√p
0
−√p
t(cid:1)2
→0 and
R(cid:0)
ψ 0∗(z −t) −ψ
0∗(z)(cid:1)2
p 0(z)dz →0 (84)
as t 0. Furthermore, i(p )< by (A4), so Lemma 37 implies that
0
→ ∞
(cid:90)
(√p √p )2 =O(t2) (85)
t 0
R −
as t 0. Therefore, letting (u ) be such that u (α /γ ) 0, we deduce from (78), (82) and (84) that
n n n n
→ →
(cid:90)
sup (ψˆ ψ )2p p 0. (86)
t ∈[ −un,un] R
n
−
0∗ t
→
Hence by (83) and (84),
(cid:90) (cid:90)
sup (cid:0) ψˆ (z t) ψˆ (z s)(cid:1)2 p (z)dz 4 sup (cid:0) ψˆ (z t) ψˆ (z)(cid:1)2 p (z)dz p 0.
n n 0 n n 0
s,t ∈[ −un,un] R − − − ≤ t ∈[ −un,un] R − − →
(cid:82) (cid:82)
This yields (79). Next, define Ψ (t):= ψ (z t)p (z)dz = ψ p for t [ t ,t ], and observe that
Ψ (t) R for all t [ t ,t ] b∗0 y (A5) aR nd0∗ the− Cauc0 hy–SchwarR z i0 n∗ eqt uality.∈ T− he0 n b0 y Cauchy–Schwarz
∗0
∈ ∈ −
0 0
again,
(cid:12)(cid:90) (cid:12)2
(cid:12) (cid:12)Ψˆ n(t) −Ψˆ n(s) −(cid:0) Ψ ∗0(t) −Ψ ∗0(s)(cid:1)(cid:12) (cid:12)2 =(cid:12) (cid:12)
(cid:12)
R(ψˆ n −ψ 0∗)(p t −p s)(cid:12) (cid:12)
(cid:12)
(cid:26)(cid:90) (cid:27)(cid:26)(cid:90) (cid:27)
≤
R(ψˆ
n
−ψ 0∗)2(cid:0) √p t+√p s(cid:1)2 R(cid:0) √p
t
−√p s(cid:1)2
(cid:26) (cid:90) (cid:27)(cid:26)(cid:90) (cid:27)
≤
2 R(ψˆ
n
−ψ 0∗)2(p t+p s) R(cid:0) √p
t
−√p s(cid:1)2
for s,t [ t 0,t 0]. By (85),
(cid:82) R(cid:0)
√p
t
√p
s(cid:1)2 =(cid:82) R(cid:0)
√p
t s
√p
0(cid:1)2 =O(cid:0)
(t
s)2(cid:1)
as s,t 0, so it follows
∈ − − − − − →
from (86) that
sup
(cid:12) (cid:12) (cid:12)Ψˆ n(t) −Ψˆ n(s) −(cid:0) Ψ ∗0(t) −Ψ ∗0(s)(cid:1)(cid:12) (cid:12)
(cid:12)=o p(1) (87)
(cid:12) t s (cid:12)
s,t ∈[ −s=un t,un]: −
̸
as n . Similarly, by the Cauchy–Schwarz inequality,
→∞
(cid:12)(cid:90) (cid:12)2
(cid:12) (cid:0) (cid:1)(cid:12)2 (cid:12) (cid:0) (cid:1)(cid:0) (cid:1) (cid:12)
(cid:12)Ψ ∗0(t) −Ψ ∗0(s)
−
Ψ ∗0(t −s) −Ψ ∗0(0) (cid:12) =(cid:12)
(cid:12) R
ψ 0∗(z −s) −ψ 0∗(z) p 0(z+t −s) −p 0(z) dz(cid:12)
(cid:12)
(cid:90) (cid:90)
(cid:0) (cid:1)2 (cid:0) (cid:1)2
≤2
R
ψ 0∗(z −s) −ψ 0∗(z) (p
t
−s+p 0)(z)dz
R
√p
t −s
−√p
0
.
(cid:82) (cid:0) (cid:1)2 (cid:82) (cid:0) (cid:1)2
Since ψ (z s) ψ (z) p (z)dz = ψ (z t) ψ (z+s t) p (z)dz, we deduce from (84)
R 0∗
− −
0∗ t −s R 0∗
− −
0∗
−
0
and (85) that
(cid:12) (cid:0) (cid:1)(cid:12)
sup
(cid:12) (cid:12)Ψ ∗0(t) −Ψ ∗0(s)
−
Ψ ∗0(t −s) −Ψ ∗0(0) (cid:12)
(cid:12) 0 (88)
(cid:12) t s (cid:12)→
s,t ∈[ −s=un t,un]: −
̸
53as n . Finally, by (A4), (cid:82) p i(p )1/2 < , so p is of bounded variation on R. Moreover,
→ ∞
R
|
′0|
≤
0
∞
0
by (A4)–(A5),
(cid:90) (cid:90) (cid:90)
0 p (z+t)dψ (z)= ψ (z t)p (z)dz = ψ (z t)ψ (z)p (z)dz
≤− R
0 0∗
R
0∗
−
′0
R
0∗
−
0 0
(cid:18)(cid:90) (cid:19)1/2(cid:18)(cid:90) (cid:19)1/2
ψ (z t)2p (z)dz ψ2(z)p (z)dz <
≤ R 0∗ − 0 R 0 0 ∞
for t t ,t , where the first equality holds by Fubini’s theorem, similarly to (111). Therefore, by
0 0
∈ {− }
Lemma 38(i) and Theorem 2(c), (Ψ )(0) =
(cid:82)
p dψ =
(cid:0)(cid:82)
(ψ )2p
(cid:14)
V (ψ
)(cid:1)1/2
= i (p ) (0, ),
∗0 ′
−
R 0 0∗ R 0∗ 0 p0 0∗ ∗ 0
∈ ∞
so
(cid:12) (cid:12)
sup
(cid:12) (cid:12)Ψ ∗0(t −s) −Ψ ∗0(0)
i ∗(p
0)(cid:12)
(cid:12) 0 (89)
(cid:12) t s − (cid:12)→
s,t ∈[ −s=un t,un]: −
̸
as n . Combining (87)–(89) yields (80).
→∞
6.3.1 Proofs for Section 3.1
Sincethefunctionψin(30)ischosentobeantisymmetricandbounded,theconditionEψ(ε )=(cid:82) ψp =
1 R 0
0 in Lemma 29 is satisfied whenever p is symmetric.
0
Corollary 31. Suppose that p is symmetric and that (A1)–(A5) hold. Then for j 1,2,3 and t R,
0
∈{ } ∈
we have
(cid:90) Ψˆ (t) Ψˆ ( t)
Ψˆanti(t):= ψˆanti(z t)p (z)dz = n,j − n,j − ,
n,j R n,j − 0 2
and the conclusions of Lemma 30 hold for (ψˆanti) and (Ψˆanti) in place of (ψˆ ) and (Ψˆ ) respectively.
n,j n,j n,j n,j
Proof. It suffices to consider j = 1. We write ψˆanti ψˆanti and Ψˆanti Ψˆanti. Since p is symmetric,
ψ (z) = p (z)/p (z) = ψ ( z) for all z R. n By (≡ 67),n, ψ1 (z) = n ψ ≡ ( z)n, f1 or Lebesgu0 e almost every
z0
R, so
′0 0
−
0
− ∈
0∗
−
0∗
−
∈
(cid:90) (cid:90) (cid:18) ψˆ (z) ψˆ ( z) ψ (z) ψ ( z)(cid:19)2
R(ψˆ nanti −ψ 0∗)2p
0
=
R
n −
2
n −
−
0∗ −
2
0∗ − p 0(z)dz
(cid:90) (cid:0) ψˆ (z) ψ (z)(cid:1)2 +(cid:0) ψˆ ( z) ψ ( z)(cid:1)2 (cid:90)
n − 0∗ n − − 0∗ − p (z)dz = (ψˆ ψ )2p
≤ R 2
0
R
n
−
0∗ 0
for each n. Similarly, for s,t R, we have
∈
(cid:90)
(cid:0) ψˆanti(z t) ψˆanti(z s)(cid:1)2 p (z)dz
R n − − n − 0
(cid:90) (cid:0) ψˆ (z t) ψˆ (z s)(cid:1)2 +(cid:0) ψˆ (z+t) ψˆ (z+s)(cid:1)2
n n n n
− − − − p (z)dz
0
≤ R 2
and
(cid:90) ψˆ (z t) ψˆ (t z) (cid:90) ψˆ (z t) ψˆ (t+z) Ψˆ (t) Ψˆ ( t)
Ψˆanti(t)= n − − n − p (z)dz = n − − n p (z)dz = n − n − .
n R 2 0 R 2 0 2
Since Eψ(ε )=0, the desired conclusions for (ψˆanti) and (Ψˆanti) therefore follow from Lemma 30.
1 n,j n,j
The proof of Theorem 14 relies on the following ‘asymptotic equicontinuity’ result, which is similar
to Bickel (1975, Lemma 4.1).
Lemma 32. Supposethat (A1)–(A5)holdforalinearmodel (27)inwhichp issymmetric. Forβ Rd,
0
n N and j 1,2,3 , define ∈
∈ ∈{ }
R (β):= 1 (cid:88) X (cid:8) ψˆanti(ε X β) Ψˆanti(X β)(cid:9) .
n,j √n i n,j i − i⊤ − n,j i⊤
i ∈Ij+2
54Then for every j 1,2,3 and M >0, writing Θ := β Rd : β Mn 1/2 , we have
n −
∈{ } { ∈ ∥ ∥≤ }
sup R (β) R (0) =o (1)
n,j n,j p
∥ − ∥
β ∈Θn
as n .
→∞
Proof. It suffices to consider j =1, so we drop the j subscript from ψˆanti, Ψˆanti and R ; we also write
n,j n,j n,j
:= X : i I . For each β Θ and i I , we have E(cid:8) ψˆanti(ε X β) (cid:9) =
D Ψˆa n′ n≡ ti(D Xn′ i⊤β),D so1 ∪ E(cid:0)D R2 n∪ (β{ )(cid:12) (cid:12) Di ′(cid:1) =∈ 0.3 } Since max i ∈[n∈ ] |X i⊤n β |α n/γ∈ n =3 o p(1) by Caucn hy–Sci h− wari z⊤ and|D (′ A1),
it follows from the weak law of large numbers and Corollary 31 that
(cid:13) (cid:13)Cov(cid:0) R n(β) −R n(0)(cid:12) (cid:12) D′(cid:1)(cid:13) (cid:13) op ≤ n1 (cid:88)(cid:13) (cid:13)Cov(cid:0) X i {ψˆ nanti(ε i −X i⊤β) −ψˆ nanti(ε i) }(cid:12) (cid:12) D′(cid:1)(cid:13) (cid:13) op
i ∈I3
= n1 (cid:88) ∥X i ∥2Var(cid:0) ψˆ nanti(ε i −X i⊤β) −ψˆ nanti(ε i)(cid:12) (cid:12) D′(cid:1)
(cid:18)i ∈I3
(cid:19) (cid:90)
1 (cid:88) X 2 max (cid:0) ψˆanti(z X β) ψˆanti(z)(cid:1)2 p (z)dz
≤ n
i
∈I3∥ i ∥ i ∈I3 R n − i⊤ − n 0
=O (1)o (1)=o (1).
p p p
Thus, for every ϵ>0, we have
P(cid:0)
R n(β) R n(0)
>ϵ(cid:12)
(cid:12)
′(cid:1)
ϵ
−2E(cid:0)
R n(β) R n(0)
2(cid:12)
(cid:12)
′(cid:1)
∥ − ∥ D ≤ ∥ − ∥ D
≤dϵ
−2(cid:13) (cid:13)Cov(cid:0)
R n(β) −R
n(0)(cid:12)
(cid:12)
D′(cid:1)(cid:13)
(cid:13)
op
→p
0
p
as n , so R (β) R (0) 0 for every β Θ by Lemma 41.
n n n
Fo→
r
e∞
ach
δ∥ (0,1)−
and n
∥N→
, we can find
a∈
(δMn 1/2)-covering set Θ of Θ such that Θ
− n,δ n n,δ
∈ ∈ | | ≤
3δ d (e.g. Wainwright, 2019, Example 5.8), so since d is fixed as n , it follows by a union bound
−
→ ∞
that
p
max R (β) R (0) 0.
n n
β ∈Θn,δ∥ − ∥→
In addition, for every β Θ , there exists ϑ Θ such that β ϑ δMn 1/2. Since ψˆanti is
decreasing and Ψˆanti is in∈ creasn ing for each n, wβ e∈ haven,δ ∥ − ∥ ≤ − n
n
ψˆanti(cid:0) ε X ϑ +δMn 1/2 X (cid:1) ψˆanti(ε X β ) ψˆanti(cid:0) ε X ϑ δMn 1/2 X (cid:1) ,
n i − i⊤ β − ∥ i ∥ ≤ n i − i⊤ ′ ≤ n i − i⊤ β − − ∥ i ∥
Ψˆanti(cid:0) X ϑ δMn 1/2 X (cid:1) Ψˆanti(X β ) Ψˆanti(cid:0) X ϑ +δMn 1/2 X (cid:1)
n i⊤ β − − ∥ i ∥ ≤ n i⊤ ′ ≤ n i⊤ β − ∥ i ∥
for β β,ϑ . For each ϑ Θ , let
′ β n,δ
∈{ } ∈
(cid:26) (cid:18) (cid:19) (cid:18) (cid:19)(cid:27)
r n(ϑ):= (cid:88) ∥ √X ni ∥ ψˆ nanti ε
i
−X i⊤ϑ
−
δM √∥ nX i ∥ −ψˆ nanti ε
i
−X i⊤ϑ+ δM √∥ nX i ∥ .
i ∈I3
Then
sup R n(β) R n(ϑ β) max
(cid:8)
r
n(ϑ)+E(cid:0)
r
n(ϑ)(cid:12)
(cid:12)
′(cid:1)(cid:9)
.
β ∈Θn∥ − ∥≤ϑ ∈Θn,δ D
By (A1), V :=max M(1+δ)n 1/2 X =o (γ /α ), so it follows from Corollary 31, the indepen-
n i ∈I3 −
∥
i
∥
p n n
dence of , , and Lemma 42 that
1 2 3
D D D
(cid:0) (cid:12) (cid:1)
Var r n(ϑ)(cid:12)
′
D
≤
k(cid:88)
∈I3
∥X nk ∥2 m
i
∈a I3x(cid:90) R(cid:26) ψˆ nanti(cid:18) z −X i⊤ϑ
−
δM √∥ nX i ∥(cid:19) −ψˆ nanti(cid:18) z −X i⊤ϑ+ δM √∥ nX i ∥(cid:19)(cid:27)2 p 0(z)dz =o p(1).
Togetherwithaunionboundoverϑ ∈Θ n,δ, thisshowsthatmax ϑ
∈Θn,δ(cid:8)
r n(ϑ)
−E(cid:0)
r
n(ϑ)(cid:12)
(cid:12)
D′(cid:1)(cid:9)
=o p(1).
Moreover, by (A1) and Corollary 31,
(cid:26) (cid:18) (cid:19) (cid:18) (cid:19)(cid:27)
ϑm ∈Θa nx ,δE(cid:0) r n(ϑ)(cid:12) (cid:12) D′(cid:1) = ϑm ∈Θa nx
,δ
i(cid:88)
∈I3
∥ √X ni ∥ Ψˆa nnti X i⊤ϑ+ δM √∥ nX i ∥ −Ψˆa nnti X i⊤ϑ
−
δM √∥ nX i ∥
(cid:88) 2δMi ∗(p 0) ∥X i ∥2 (1+η )= 2δMi ∗(p 0) E( X 2)+o (1),
n,i 1 p
≤ n 3 ∥ ∥
i ∈I3
55where by (80) and Lemma 42, (η : i I ) are random variables satisfying max η ∆ (V ) =
n,i
∈
3 i ∈I3| n,i
| ≤
n n
o (1). Thus,
p
sup R (β) R (0) sup R (β) R (ϑ ) + max R (ϑ) R (0)
n n n n β n n
β ∈Θn∥ − ∥≤ β ∈Θn∥ − ∥ ϑ ∈Θn,δ∥ − ∥
max
(cid:8)
r n(ϑ)
E(cid:0)
r
n(ϑ)(cid:12)
(cid:12)
′(cid:1)(cid:9)
+2 max
E(cid:0)
r
n(ϑ)(cid:12)
(cid:12)
′(cid:1)
+ max R n(ϑ) R n(0)
≤ϑ ∈Θn,δ − D ϑ ∈Θn,δ D ϑ ∈Θn,δ∥ − ∥
4δMi (p )
∗ 0 E( X 2)+o (1)
1 p
≤ 3 ∥ ∥
as n . Since this holds for all δ (0,1), the desired conclusion follows.
→∞ ∈
As a first step towards proving the asymptotic normality of βˆ and βˆ in Theorem 14, we show in
n† n‡
Lemma 33 below that they are √n-consistent estimators of β . To this end, we exploit the convexity of
0
the induced loss functions z ℓˆsym(z)= (cid:82)z ψˆanti, similarly to He and Shao (2000, Theorem 2.1). We
(cid:55)→ n,j − 0 n,j
denote the Euclidean unit sphere in Rd by := u Rd : u =1 .
d 1
S − { ∈ ∥ ∥ }
Lemma 33. For n N, j 1,2,3 and t>0, we have
∈ ∈{ }
(cid:26) (cid:27)
(cid:8) βˆ(j) β >t(cid:9) inf u (cid:88) X ψˆanti(cid:0) ε tX u(cid:1) 0 , (90)
∥ n − 0 ∥ ⊆ u ∈Sd−1 ⊤
i ∈Ij+2
i n,j i − i⊤ ≤
(cid:26) 3 (cid:27)
(cid:8) βˆ β >t(cid:9) inf u (cid:88) (cid:88) X ψˆanti(cid:0) ε tX u(cid:1) 0 . (91)
∥ n‡ − 0 ∥ ⊆ u ∈Sd−1 ⊤
j=1i ∈Ij+2
i n,j i − i⊤ ≤
As a consequence, if p is symmetric and (A1)–(A5) are satisfied, then
0
(a) √n(βˆ(j) β )=O (1) and √n(βˆ β )=O (1);
n
−
0 p n‡
−
0 p
3
(b) 1 (cid:88) X ψˆanti(cid:0) Y X βˆ(j)(cid:1) =o (1) and 1 (cid:88) (cid:88) X ψˆanti(cid:0) Y X βˆ (cid:1) =o (1).
√n i n,j i − i⊤ n p √n i n,j i − i⊤ n‡ p
i ∈Ij+2 j=1i ∈Ij+2
Proof. It suffices to consider j = 1. We write ψˆanti ψˆanti and ℓˆsym ℓˆsym. The function β
n ≡ n,1 n ≡ n,1 (cid:55)→
L(cid:80)
ˆsyi ∈ mI (cid:0)3
(ℓˆ 1s nym h(Y
)βi ˆ−
+X
hi⊤
ββ)
(cid:1)
== :: GLˆ (s
n
hym )( isβ) coi ns vc eo xn av ne dx io nn crR ead s, inw git oh nβ [ˆ
0n
,
≡
).βˆ n( S1 i)
nc∈
e
argmin
β
∈RdLˆs nym(β), so h
(cid:55)→
n − n 0 ∞
g(h):= (βˆ β ) (cid:88) X ψˆanti(cid:0) ε (1 h)X (βˆ β )(cid:1)
− n − 0 ⊤ i n i − − i⊤ n − 0
i ∈I3
is a subgradient of G at h, it follows that g(h) 0 for every h>0. Hence, if βˆ β >t>0, then
n 0
≥ ∥ − ∥
0 ≤g(cid:18) 1
− βˆ
t
β
(cid:19) = −(βˆ
n
−β 0) ⊤(cid:88) X iψˆ nanti(cid:18) ε
i
−tX
i⊤
β βˆ ˆn −β β0 (cid:19)
∥ n − 0 ∥ i ∈I3 ∥ n − 0 ∥
(cid:88)
βˆ β inf u X ψˆanti(ε tX u), (92)
≤−∥ n − 0 ∥u
∈Sd−1
⊤
i ∈I3
i n i − i⊤
so (90) holds. By analogous reasoning based on β Lˆ (β) := (cid:80)3 (cid:80) ℓˆsym(Y X β), we
obtain (91).
(cid:55)→ ‡n j=1 i ∈Ij+2 n,j i − i⊤
(a) Fix M > 0 and write Ψˆanti Ψˆanti. By (A1), V := max Mn 1/2 X = o (γ /α ). We have
n ≡ n,1 n i ∈I3 − ∥ i ∥ p n n
I /n 1/3, so for each u , Corollary 31 yields
3 d 1
| | → ∈S −
(cid:88) X i Ψˆanti(cid:16)MX i⊤u(cid:17)
=
(cid:88) X i (cid:26) Ψˆanti(cid:16)MX i⊤u(cid:17) Ψˆanti(0)(cid:27)
=
Mi ∗(p 0) (cid:88)
X X u(1+η )
√n n √n √n n √n − n n i i⊤ n,i
i ∈I3 i ∈I3 i ∈I3
where by (80) and Lemma 42, (η : i I ) are random variables satisfying max η ∆ (V ) =
n,i
∈
3 i ∈I3| n,i
| ≤
n n
o (1). Therefore,
p
(cid:13) (cid:13)
sup (cid:13) (cid:13)(cid:88) X i Ψˆanti(cid:16)MX i⊤u(cid:17) Mi ∗(p 0) E(X X )u(cid:13) (cid:13)
u ∈Sd−1(cid:13) (cid:13)
≤i
∈
MI3
i√ ∗(n
p
0)n
(cid:32)(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)(cid:88)√ Xn
i nX
i⊤−
−
E(X3
1 3X 1⊤)(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)
1 +1⊤
(cid:80) i
∈(cid:13) (cid:13)
I3 n∥X i ∥2 ·∆ n(V n)(cid:33) =o p(1). (93)
i ∈I3 op
56Since E(X X ) is positive definite by (A1), its minimum eigenvalue λ is strictly positive. Then
1 1⊤ min
by (93) and Lemma 32,
inf u
(cid:88) X i ψˆanti(cid:16)
ε
MX i⊤u(cid:17)
= inf u
(cid:26) (cid:88) X i Ψˆanti(cid:16)MX i⊤u(cid:17)
+R
(cid:16)Mu(cid:17)(cid:27)
u ∈Sd−1 ⊤
i ∈I3
√n n i − √n u ∈Sd−1 ⊤
(cid:26)i ∈I3
√n n √n n (cid:27)√n
Mi (p )
= inf u ∗ 0 E(X X )u+R (0) +o (1)
u ∈Sd−1
⊤
3
1 1⊤ n p
Mλ i (p )
min ∗ 0
R (0) +o (1). (94)
n p
≥ 3 −∥ ∥
Writing := X :i I , we have
′ 1 2 i 3
D D ∪D ∪{ ∈ }
(cid:18) (cid:12) (cid:19) (cid:90)
E √1
n
(cid:88) X i(cid:0) ψˆ nanti(ε i) −ψ 0∗(ε i)(cid:1)(cid:12) (cid:12) (cid:12)D′ = √1
n
(cid:88) X i R(ψˆ nanti −ψ 0∗)p 0 =0
i ∈I3 i ∈I3
because p is symmetric and ψˆanti,ψ are antisymmetric. Together with Corollary 31, this implies that
0 n 0∗
(cid:18)(cid:13) (cid:13)2 (cid:12) (cid:19)
E (cid:13) (cid:13) (cid:13)√1
n
(cid:88) X i(cid:0) ψˆ nanti(ε i) −ψ 0∗(ε i)(cid:1)(cid:13) (cid:13)
(cid:13)
(cid:12) (cid:12) (cid:12)D′ = n1 (cid:88) E(cid:8) ∥X i ∥2(ψˆ nanti −ψ 0∗)2(ε i)(cid:12) (cid:12) D′(cid:9)
i ∈I3 i ∈I3
(cid:90)
= 1 (cid:88) X 2 (ψˆanti ψ )2p p 0.
n ∥ i ∥ R n − 0∗ 0 →
i ∈I3
We have E(cid:0) X ψ (ε )(cid:1) =E(X )(cid:82) ψ p =0 for each i, so by the central limit theorem,
i 0∗ i i R 0∗ 0
1 (cid:88) 1 (cid:88)
R (0)= X ψˆanti(ε )= X ψ (ε )+o (1)=O (1) (95)
n √n i n i √n i 0∗ i p p
i ∈I3 i ∈I3
as n . Since Mλ i (p ) > 0, we deduce from (90) and (94) that limsup P( βˆ β
M/√n→ ) ∞ 0asM m ,i sn o∗ √n0 (βˆ β )=O (1)asn . The√n-consistencyofn β→ˆ∞follo∥ wn ss− imi0 l∥ arl≥ y
→ →∞
n
−
0 p
→∞
n‡
from (91).
(b)Sincetheerrorsε haveanabsolutelycontinuousdensityp ,theconditionaldistributionof(Y :i I )
i 0 i 3
given = X :i I is absolutely continuous with respect to Lebesgue measure on
R∈
. We
′ 1 2 i 3
used D D t∪ oD obt∪ ai{ n the c∈ onve} x function ℓˆsym, whose subdifferential at z R is
D1 ∪D2 n ∈
∂ℓˆsym(x)=(cid:2) ψˆanti(z),ψˆanti(z )(cid:3) ,
n n n −
cw oh ue nr te abψˆ lna en .ti T(z h− u) s,:= apl pim lyz in′ ↗ gz Lψˆ en mant mi( az ′ 4). 0S toin tc he eψˆ lna innt ei ai rs sd ue bc sr pe aa cs ein Wg, A :=:= (cid:8) ((cid:8) Xz ∈ β)R : ψˆ :na βnti(z R) d< (cid:9)ψˆ ona fn dti i( mz − en) s(cid:9) ioi ns
i⊤ i ∈I3
∈
at most d, we have
(cid:18) (cid:88) (cid:12) (cid:19)
P max 1 d(cid:12) =1. (96)
β Rd {Yi−X i⊤β ∈A } ≤ (cid:12) D′
∈ i ∈I3
LM ˆso yr meo av ter β, βˆ n R∈
d
Rar og cm kain feβ l∈ laR rd (Lˆ 1s n 9y 9m 7( ,β p) ., 3so 15w ),ri wti eng ha∂ vL eˆs nym(β) for the subdifferential of the convex function
n ∈
(cid:88)
0 ∂Lˆsym(βˆ )= X ∂ℓˆsym(Y X βˆ ),
∈ n n i n i − i⊤ n
i ∈I3
so there exist a ∂Lˆsym(Y X βˆ ) for i I such that (cid:80) X a = 0. For each i I , we have
(cid:12) (cid:12)a i −ψˆ nanti(Y i −i X∈ i⊤βˆ n)n (cid:12) (cid:12) ≤2i ∥ψ− ˆ nantii⊤ ∥∞n 1 {Yi−X i⊤∈ βˆ n∈3 A
}
≤2(α n/γ n)i ∈1I {3 Yi−i Xi i⊤βˆ n∈A }, so by (96)∈ , 3
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) i(cid:88) ∈I3X iψˆ nanti(Y i −X i⊤βˆ n)(cid:12) (cid:12) (cid:12) (cid:12)≤ i(cid:88) ∈I3∥X i ∥·(cid:12) (cid:12)a i −ψˆ nanti(Y i −X i⊤βˆ n)(cid:12) (cid:12) ≤2dm i ∈a I3x ∥X i ∥α γ nn
almost surely. Assumption (A1) ensures that the right-hand side is o (n1/2) as n , so (b) holds for
p
→∞
βˆ =βˆ(1), and the result for βˆ follows similarly by considering Lˆ .
n n n‡ ‡n
57Proof of Theorem 14. For j 1,2,3 , Lemma 33(b) implies that
∈{ }
1 (cid:88) X Ψˆanti(cid:0) X (βˆ(j) β )(cid:1) +R (βˆ(j) β )= 1 (cid:88) X ψˆanti(cid:0) ε X (βˆ(j) β )(cid:1)
√n i n,j i⊤ n − 0 n,j n − 0 √n i n,j i − i⊤ n − 0
i ∈Ij+2 i ∈Ij+2
1 (cid:88)
= X ψˆanti(Y X βˆ(j))=o (1) (97)
√n i n,j i − i⊤ n p
i ∈Ij+2
and, by Lemma 33(a),
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13)√1
n
(cid:88) X iX i⊤(βˆ n(j) −β 0)(cid:13) (cid:13)
(cid:13)≤
n1 (cid:88) ∥X i ∥2 ∥√n(βˆ n(j) −β 0) ∥=O p(1).
i ∈Ij+2 i ∈Ij+2
Moreover, by (A1) and Lemma 33(a), max X (βˆ(j) β ) α /γ = o (1) as n , so arguing
i ∈Ij+2| i⊤ n
−
0
|
n n p
→ ∞
similarly to (93), we have
1 (cid:88) X Ψˆanti(cid:0) X (βˆ(j) β )(cid:1) = i ∗(p 0) E(X X )√n(βˆ(j) β )+o (1). (98)
√n i n,j i⊤ n − 0 3 1 1⊤ n − 0 p
i ∈Ij+2
From (98) and (97), followed by Lemma 32 and Lemma 33(a), and then (95), we deduce that
i (p )
∗ 0 E(X X )√n(βˆ(j) β )= R (βˆ(j) β )+o (1)
3 1 1⊤ n − 0 − n,j n − 0 p
1 (cid:88)
= R (0)+o (1)= X ψ (ε )+o (1)
−
n,j p
−√n
i 0∗ i p
i ∈Ij+2
for each j 1,2,3 . Hence, by the central limit theorem,
∈{ }
√n(βˆ
n†
−β 0)=(cid:88)3 √ 3n (βˆ n(j) −β 0)= −{E( iX (1 pX )1 √⊤) n}−1 (cid:88)3 (cid:88) X iψ 0∗(ε i)+o p(1)
j=1 ∗ 0 j=1i ∈Ij+2
=
−{E( iX
(1
pX
)1
√⊤) n}−1 (cid:88)n
X iψ 0∗(ε i)+o p(1) →d N
d(cid:16) 0,{E(X
i1
(X
p1⊤
)) }−1(cid:17)
∗ 0 i=1 ∗ 0
as n . By analogous reasoning based on Lemmas 32 and 33, √n(βˆ β ) has the same limiting
distri→ but∞
ion; note in particular that R n‡
:=(cid:80)3
j=1R n‡,j satisfies sup ∥β
∥≤Mnn −‡ 1−
/2
∥R0
n‡(β) −R n‡(0) ∥=o p(1).
6.3.2 Proofs for Section 3.2
First,weverifytheexpressionin(37)fortheefficientscorefunction. Thescorefunctionℓ˙ inRemark15
β0
can be decomposed as
(cid:32) ℓ˙(1)(x,y)(cid:33) (cid:18)
x˜ψ (y x β
)(cid:19)
ℓ˙ β0(x,y)
≡
ℓ˙(β 20
)(x,y)
:=
ψ
00
(y
−
x
⊤⊤
β
00
)
∈Rd −1 ×R
β0 −
for x (x ,...,x ) Rd and y R, where x˜ = (x ,...,x ) and ℓ˙(2) is called the nuisance score
≡ 1 d ∈ ∈ 1 d −1 β0
function. Thenuisance tangent space atβ isΛ:= vℓ˙(2) :v Rd 1 . By(A1)and(A4), E( X 2)and
0 { β0 ∈ − } ∥ 1 ∥
i(p )arebothfinite. DenotingbyP thejointdistributionof(X ,Y ),wehaveℓ˙ (X ,Y )=X ψ (ε ),
0 β0 1 1 β0 1 1 1 0 1
so ℓ˙ L2(P )d and Λ is a (d 1)-dimensional subspace of L2(P )d 1. For h L2(P )d 1, let
β0
∈
β0
−
β0 −
∈
β0 −
Π β0h:=ar ggm ΛinE β0(cid:0) ∥h(X 1,Y 1) −g(X 1,Y 1) ∥2(cid:1) =ℓ˙( β2 0) ·a vrg Rm d−in
1
E β0(cid:0)(cid:13) (cid:13)h(X 1,Y 1) −vℓ˙( β2 0)(X 1,Y 1)(cid:13) (cid:13)2(cid:1)
∈ ∈
denote its componentwise orthogonal projection onto Λ, so that
Π h (x,y)=
E β0(cid:8) ℓ˙( β2 0)(X 1,Y 1) ·h j(X 1,Y 1)(cid:9)
ℓ˙(2)(x,y)=
E(cid:8) ψ 0(ε 1) ·h j(X 1,X 1⊤β 0+ε 1)(cid:9)
ψ (y x β )
β0 j E β0(cid:8) ℓ˙( β2 0)(X 1,Y 1)2(cid:9) β0 i(p 0) 0 − ⊤ 0
58forj [d 1]and(x,y) Rd R. Sinceℓ˙(1) L2(P )d 1,theefficientscorefunctionℓ˜ : Rd R Rd 1
∈ − ∈ × β0 ∈ β0 − β0 × → −
is given by
ℓ˜ (x,y):=(cid:0) ℓ˙(1) Π ℓ˙(1)(cid:1) (x,y)=(cid:0) x˜ E(X˜ )(cid:1) ψ (y x β ),
β0 β0 − β0 β0 − 1 · 0 − ⊤ 0
as claimed; see van der Vaart (1998, Chapter 25.4) and Chernozhukov et al. (2018, Section 2.2.1).
We now turn to the proof of Theorem 16, which proceeds along similar lines to that of Theorem 14,
with analogous preliminary lemmas. For n N and j 1,2,3 , let δ(j) := µ¯(j) µ +X¯ (θ¯(j) θ )
∈ ∈ { }
n n
−
0 n⊤,j n
−
0
andZ :=X˜ X¯ fori I . ByreplacingX withZ andψˆanti withψˆ intheproofofLemma32,
i i − n,j ∈ j+2 i i n,j n,j
we obtain the following result.
Lemma 34. For β (θ,µ) Rd 1 R, n N and j 1,2,3 , define
−
≡ ∈ × ∈ ∈{ }
1 (cid:88)
R˜ (β) R˜ (θ,µ):= Z ψˆ (ε µ Z θ) Ψˆ (µ+Z θ) .
n,j
≡
n,j
√n
i
{
n,j i
− −
i⊤
−
n,j i⊤
}
i ∈Ij+2
Suppose that (A1)–(A5) are satisfied for a linear model (35) in which Eψ(ε ) = 0. Then for every
1
j 1,2,3 and M >0, recalling that Θ = β Rd : β Mn 1/2 , we have
n −
∈{ } { ∈ ∥ ∥≤ }
sup R˜ (β) R˜ (0) =o (1).
n,j n,j p
∥ − ∥
β ∈Θn
Lemma 35. For n N, j 1,2,3 and t>0, we have
∈ ∈{ }
(cid:26) (cid:27)
(cid:8) θˆ(j) θ >t(cid:9) inf u (cid:88) Z ψˆ (cid:0) ε δ(j) tZ u(cid:1) 0 , (99)
∥ n − 0 ∥ ⊆ u ∈Sd−2 ⊤
i ∈Ij+2
i n,j i − n − i⊤ ≤
(cid:26) 3 (cid:27)
(cid:8) θˆ θ >t(cid:9) inf u (cid:88) (cid:88) Z ψˆ (cid:0) ε δ(j) tZ u(cid:1) 0 . (100)
∥ n‡ − 0 ∥ ⊆ u ∈Sd−2 ⊤
j=1i ∈Ij+2
i n,j i − n − i⊤ ≤
Consequently, if (A1)–(A5) hold for the linear model (35), then provided that Eψ(ε )=0, we have
1
(a) √n(θˆ(j) θ )=O (1) and √n(θˆ θ )=O (1);
n
−
0 p n‡
−
0 p
3
(b) 1 (cid:88) Z ψˆ (cid:0) ε δ(j) Z (θˆ(j) θ )(cid:1) =o (1), 1 (cid:88) (cid:88) Z ψˆ (cid:0) ε δ(j) Z (θˆ θ )(cid:1) =o (1).
√n i n i − n − i⊤ n − 0 p √n i n i − n − i⊤ n‡ − 0 p
i ∈Ij+2 j=1i ∈Ij+2
Proof. Similarly to the proof of Lemma 33, it suffices to consider j = 1. We write ψˆ ψˆ and
n n,1
≡
Ψˆ Ψˆ . By definition, θˆ θˆ(1) minimises the convex function
n n,1 n n
≡ ≡
θ (cid:88) ℓˆ (cid:0) Y µ¯(1) X¯ θ¯(1) Z θ(cid:1) = (cid:88) ℓˆ (cid:0) ε δ Z (θ θ )(cid:1) =:Lˆ (θ)
(cid:55)→ n i − n − n⊤,1 n − i⊤ n i − n − i⊤ − 0 n
i ∈I3 i ∈I3
over Rd 1, where ℓˆ ℓˆ and δ δ(1). Thus, h Lˆ (cid:0) (1 h)θˆ +hθ (cid:1) =: G(h) is convex and
− n n,1 n n n n 0
≡ ≡ (cid:55)→ −
increasing on [0, ), so
∞
g(h):= (θˆ θ ) (cid:88) Z ψˆ (cid:0) ε δ (1 h)Z (θˆ θ )(cid:1) 0
−
n
−
0 ⊤ i n i
−
n
− −
i⊤ n
−
0
≥
i ∈I3
for h 0. This means that if θˆ θ >t>0, then we can take h=1 t/ θˆ θ [0,1] and argue
n 0 n 0
as in≥ (92) to obtain (99). By∥ simi− lar r∥ easoning based on θ (cid:80)3 (cid:80) − ∥ ℓˆ (cid:0) ε− δ∥ (j∈ ) Z (θ θ )(cid:1) ,
we obtain (100).
(cid:55)→ j=1 i ∈Ij+2 n i − n − i⊤ − 0
(a) By (A1), Cov(X˜ ) is positive definite, so its minimum eigenvalue λ is strictly positive. Since
1 min
Eψ(ε ) = 0, we have β¯(1) β = O (n 1/2) (Yohai and Maronna, 1979), so it follows from (A1) that
1 n 0 p −
−
δ = O (n 1/2) = o (γ /α ) and hence V := δ + n 1/2max Z = o (γ /α ) as n .
Mn oreovep r, (cid:80)− Z =p 0 an ndn n 1(cid:80) Z Z n p Co| vn (| X˜ )/3− . Thus,i f∈ oI r3 M∥ >i ∥ 0 andp un n , we d→ edu∞ ce
from
Lemmai 3∈0I3 thi
at
− i ∈I3 i i⊤ → 1 ∈Sd −2
(cid:88) Z i Ψˆ (cid:16) δ + MZ i⊤u(cid:17) = (cid:88) Z i (cid:110) Ψˆ (cid:16) δ + MZ i⊤u(cid:17) Ψˆ (δ )(cid:111) = M (cid:88) Z Z u(1+η )i (p )
√n
n n
√n √n
n n
√n −
n n
n
i i⊤ n,i ∗ 0
i ∈I3 i ∈I3 i ∈I3
(101)
59whereby(80),(η :i I )arerandomvariablessatisfyingmax η ∆ (V )=o (1). Therefore,
n,i
∈
3 i ∈I3| n,i
|≤
n n p
arguing similarly to (93), we have
(cid:13) (cid:13)
sup (cid:13) (cid:13)(cid:88) Z i Ψˆ (cid:16) δ + MZ i⊤u(cid:17) Mi ∗(p 0) Cov(X˜ )u(cid:13) (cid:13)=o (1),
u ∈Sd−2(cid:13) (cid:13)
i ∈I3
√n n n √n − 3 1 (cid:13) (cid:13) p
so by Lemma 34,
inf u (cid:88) Z i ψˆ (cid:16) ε δ MZ i⊤u(cid:17) = inf u (cid:26) (cid:88) Z i Ψˆ (cid:16) δ + MZ i⊤u(cid:17) +R˜ (cid:16)Mu ,δ (cid:17)(cid:27)
⊤ n i n ⊤ n n n,1 n
u ∈Sd−2
i ∈I3
√n − − √n u ∈Sd−2
i ∈I3
√n √n √n
(cid:110)Mi (p ) (cid:111)
= inf u ∗ 0 Cov(X˜ )u+R˜ (0,0) +o (1)
⊤ 1 n,1 p
u ∈Sd−2 3
Mλ i (p )
min ∗ 0 R˜ (0,0) +o (1). (102)
n,1 p
≥ 3 −∥ ∥
Writing = X :i I , we have
′ 1 2 i 3
D D ∪D ∪{ ∈ }
E(cid:16)1 (cid:88) Z (cid:0) ψˆ (ε ) ψ (ε )(cid:1)(cid:12) (cid:12) (cid:17) = 1 (cid:88) Z (cid:90) (ψˆ ψ )p =0,
n i n i − 0∗ i (cid:12) D′ n i R n − 0∗ 0
i ∈I3 i ∈I3
it follows from Lemma 30 that
E(cid:16)(cid:13) (cid:13) (cid:13)√1 n (cid:88) Z i(cid:0) ψˆ n(ε i) −ψ 0∗(ε i)(cid:1)(cid:13) (cid:13) (cid:13)2 (cid:12) (cid:12) (cid:12) D′(cid:17) = n1 (cid:88) ∥Z i ∥2E(cid:8) (ψˆ n −ψ 0∗)2(ε i)(cid:12) (cid:12) D′(cid:9)
i ∈I3 i ∈I3
(cid:90)
= 1 (cid:88) Z 2 (ψˆ ψ )2p p 0.
n ∥
i
∥ R
n
−
0∗ 0
→
i ∈I3
Moreover, by Lemma 1, n 1(cid:80) ψ (ε ) p (cid:82) ψ p = 0, so letting ν˜ := E(X˜ ), we deduce by the
central limit theorem that
− i ∈I3 0∗ i → R 0∗ 0 1
R˜ (0,0)=
1 (cid:88)
Z ψˆ (ε )=
1 (cid:88)
(X˜ ν˜)ψ (ε ) √n(X¯
ν˜)(cid:88) ψ 0∗(ε i)
+o (1)
n,1
√n
i n i
√n
i
−
0∗ i
−
n,1
− n
p
i ∈I3 i ∈I3 i ∈I3
1 (cid:88)
= (X˜ ν˜)ψ (ε )+o (1)=O (1) (103)
√n
i
−
0∗ i p p
i ∈I3
as n . We have Mλ i (p )>0, so by (99) and (102), limsup P( θˆ θ M/√n) 0 as
M →∞ , so √n(θˆ θ )m =in O∗ (10 ) as n . The √n-consistency ofn θ→ˆ∞follo∥ wn s− sim0 il∥ ar≥ ly from (1→ 00).
→∞
n
−
0 p
→∞
n‡
(b) Arguing similarly to the proof of Lemma 33, we deduce from Lemma 40 that
(cid:18) (cid:88) (cid:12) (cid:19)
P max 1 d 1(cid:12) =1,
θ Rd−1 {εi−δn(j) −Z i⊤(θ −θ0) ∈A } ≤ − (cid:12) D′
∈ i ∈I3
where A denotes the countable set of discontinuities of the decreasing function ψˆ . Thus, by (A1) and
n
the fact that θˆ
n
∈argmin
θ
∈Rd−1Lˆ n(θ), we have
(cid:12) (cid:12)
(cid:12) (cid:12)(cid:88) Z ψˆ (cid:0) ε δ(j) Z (θˆ θ )(cid:1)(cid:12) (cid:12) (d 1)max Z α n =o (n1/2)
(cid:12) (cid:12)
i ∈I3
i n i − n − i⊤ n − 0 (cid:12) (cid:12)≤ − i ∈I3 ∥ i ∥ γ n p
as n . The proof of the analogous conclusion for θˆ is similar.
→∞
n‡
Proof of Theorem 16. Our choice of ψ in (30) ensures that t Eψ(ε t) is strictly increasing and
1
continuous on R, with lim Eψ(ε t) < 0 < lim Eψ(ε(cid:55)→ t). Th− erefore, there exists a unique
t 1 t 1
c R such that Eψ(ε c)→ =− 0∞ , and we− can equivalentl→ y∞ rewrite t− he linear model (35) in the form
1
∈ −
Y =µ˜ +X˜ θ +ε˜
i 0 i⊤ 0 i
60for i [n], where µ˜ := µ +c and ε˜ := ε c for each i. We have Eψ(ε˜ ) = 0, so for the purposes
0 0 i i 1
∈ −
of estimating θ , we may assume without loss of generality in the proof below that c = 0, i.e. that the
0
condition Eψ(ε )=0 in Lemma 29 above is satisfied.
1
ByLemmas5and30,lim ψ (z)>0>lim ψ (z)and(cid:82) (ψˆ ψ )2p p 0forj 1,2,3 .
Thus,withprobabilitytendinz
g→ t− o∞
1a0∗
sn
,wehz
a→ ve∞
lim0∗
ψˆ
R (z)n >,j
−
0>0∗ lim0
→ ψˆ (z∈ ),{ inwhic} h
z n,j z n,j
case there exists θˆ(j) satisfying (38) for e→ ac∞ h j. For j 1,→ 2,− 3∞ , Lemma 35 implies→ t∞ hat
n
∈{ }
1 (cid:88) Z Ψˆ (cid:0) δ(j)+Z (θˆ(j) θ )(cid:1) +R˜ (cid:0) θˆ(j) θ ,δ(j)(cid:1)
√n i n,j n i⊤ n − 0 n,j n − 0 n
i ∈Ij+2
= 1 (cid:88) Z ψˆ (cid:0) ε δ(j) Z (θˆ(j) θ )(cid:1) =o (1) (104)
√n i n,j i − n − i⊤ n − 0 p
i ∈Ij+2
and
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13)√1
n
(cid:88) Z iZ i⊤(θˆ n(j) −θ 0)(cid:13) (cid:13)
(cid:13)≤
n1 (cid:88) ∥Z i ∥2 ∥√n(θˆ n(j) −θ 0) ∥=O p(1).
i ∈Ij+2 i ∈Ij+2
Moreover, by (A1) and Lemma 35(a), (cid:0) δ(j) +max Z (θˆ(j) θ )(cid:1) α /γ = o (1) as n , so
|
n
|
i ∈Ij+2| i⊤ n
−
0
|
n n p
→ ∞
arguing similarly to (101), we have
1 (cid:88) Z Ψˆ (cid:0) δ(j)+Z (θˆ(j) θ )(cid:1) = i ∗(p 0) (cid:88) Z Z √n(θˆ(j) θ )+o (1)
√n i n,j n i⊤ n − 0 n i i⊤ n − 0 p
i ∈Ij+2 i ∈Ij+2
i (p )
= ∗ 0 Cov(X˜ )√n(θˆ(j) θ )+o (1). (105)
3 1 n − 0 p
(cid:80)
We have Z =0, so by (105) and (104) followed by Lemmas 32 and 35(a), and then (103),
i ∈Ij+2 i
i ∗(p 0) Cov(X˜ )√n(θˆ(j) θ )= R˜ (cid:0) θˆ(j) θ ,δ(j)(cid:1) +o (1)
3 1 n − 0 − n,j n − 0 n p
1 (cid:88)
= R˜ (0,0)+o (1)= (X˜ ν˜)ψ (ε )+o (1)
−
n,j p
−√n
i
−
0∗ i p
i ∈Ij+2
for each j 1,2,3 , where ν˜=E(X˜ ). Therefore, by the central limit theorem,
1
∈{ }
√n(θˆ θ
)=(cid:88)3 √n
(θˆ(j) θ )=
Cov(X˜ 1) −1 (cid:88)3 (cid:88)
(X˜ ν˜)ψ (ε )+o (1)
n† − 0 3 n − 0 − i (p )√n i − 0∗ i p
j=1 ∗ 0 j=1i ∈Ij+2
=
Cov(X˜ 1) −1 (cid:88)n
(X˜ ν˜)ψ (ε )+o (1) d N
(cid:16) 0,Cov(X˜ 1) −1(cid:17)
− i ∗(p 0)√n
i=1
i
−
0∗ i p
→
d −1
i ∗(p 0)
as n . By similar reasoning based on Lemmas 34 and 35, √n(θˆ θ ) has the same limiting
→ ∞
n‡
−
0
distribution.
Proof of Lemma 17. By Rockafellar (1997, Corollary 24.2.1) and (42), we have Jˆ(v) = (cid:82)v Jˆ(R) for all
0 0 0
v [0,1], so applying Fubini’s theorem and the Cauchy–Schwarz inequality yields
∈
(cid:18)(cid:90) 1 (cid:19)2 (cid:18)(cid:90) 1 (cid:19)2 (cid:18)(cid:90) 1(cid:90) 1 (cid:19)2 (cid:18)(cid:90) 1 (cid:19)2
J Jˆ = Jˆ(R)(u)1 dudv = (1 u)Jˆ(R)(u)du
0 0 ≤ 0 0 0 0 0 {u ≤v } 0 − 0
(cid:18)(cid:90) 1(cid:16)1 (cid:17) (cid:19)2
= u Jˆ(R)(u)du
2 − 0
0
(cid:18)(cid:90) 1(cid:16)1 u(cid:17)2 du(cid:19)(cid:90) 1 (cid:0) Jˆ(R)(cid:1)2
=
1 (cid:90) 1 (cid:0) Jˆ(R)(cid:1)2
,
≤ 2 − 0 12 0
0 0 0
where the equality in the second line holds because (cid:82)1 Jˆ(R) =Jˆ(1)=0. Thus, by Remark 3,
0 0 0
i (p
)=(cid:90) 1 (cid:0) Jˆ(R)(cid:1)2 12(cid:18)(cid:90) 1
J
(cid:19)2
=
1
,
∗ 0 0 ≥ 0 V
0 0 p0,CQ
61and equality holds if and only if J = Jˆ and there exists λ > 0 such that Jˆ(R)(u) = λ(1 2u) for
u [0,1], i.e. J (u)=λu(1 u) for a0 llsuc0 h u. Now by direct calculation, the log0 isticdensity q− : R R
0 λ
∈ − →
given by
λe λz
−
q (z):=
λ (1+e λz)2
−
has corresponding quantile function J . Since J > 0 on (0,1), it follows from the last assertion of
0 0
Lemma 19 that p ()=q ( µ) for some µ R, as claimed.
0 λ
Finally, given ϵ· (0,1/2· ]− , define the log-c∈ oncave density p : R R by
0
∈ →
(cid:18) (cid:19)
(1 2ϵ 2z ) 0
p (z):=exp − − | | ∧ .
0
2ϵ
Then the corresponding density quantile function J
0
=p
0
◦F 0−1 satisfies
(cid:40)
min(u/ϵ,1) for u [0,1/2]
J (u)= ∈
0
J (1 u) for u [1/2,1],
0
− ∈
so i(p )=i (p
)=(cid:82)1(cid:0) Jˆ(R)(cid:1)2
=2/ϵ< and hence
0 ∗ 0 0 0 ∞
(cid:0)(cid:82)1 (cid:1)2
1 12 J
= 0 0 =6ϵ(1 ϵ)2.
i ∗(p 0)V
p0,CQ
(cid:82) 01(cid:0) Jˆ 0(R)(cid:1)2 −
By taking ϵ to be arbitrarily small, we obtain the final assertion of the lemma.
6.3.3 Proofs for Section 3.3
Proof of Lemma 18. If p is symmetric, then because ψ is bounded and antisymmetric in (30), the
0
condition Eψ(ε ) = 0 in Lemma 29 is satisfied. Otherwise, if X = 1 almost surely, then by the
1 1d
reasoning in the proof of Theorem 16, we may assume without loss of generality that Eψ(ε )=0 below.
1
We first consider j =1, and write ψˆ ψˆ and β¯ β¯(1). Then
n n,1 n n
≡ ≡
(cid:12) (cid:12)
(cid:12) (cid:12)1 (cid:88)(cid:0) ψˆ n(ε˘ i)2 ψˆ n(ε i)2(cid:1)(cid:12) (cid:12)
(cid:12)n − (cid:12)
i ∈(cid:12)I3
(cid:12)
=(cid:12) (cid:12)1 (cid:88)(cid:110)(cid:0) ψˆ n(ε˘ i) ψˆ n(ε i)(cid:1)2 +2(cid:0) ψˆ n(ε˘ i) ψˆ n(ε i)(cid:1) ψˆ n(ε i)(cid:111)(cid:12) (cid:12) (106)
(cid:12)n − − · (cid:12)
i ∈I3
(cid:18) (cid:19)1/2(cid:26)(cid:18) (cid:19)1/2 (cid:18) (cid:19)1/2(cid:27)
1 (cid:88)(cid:0) ψˆ (ε˘) ψˆ (ε )(cid:1)2 1 (cid:88)(cid:0) ψˆ (ε˘) ψˆ (ε )(cid:1)2 +2 1 (cid:88) ψˆ (ε )2
n i n i n i n i n i
≤ n − n − n
i ∈I3 i ∈I3 i ∈I3
and similarly
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)n1 (cid:88)(cid:0) ψˆ n(ε i)2 −ψ 0∗(ε i)2(cid:1)(cid:12) (cid:12)
(cid:12)
(107)
i ∈I3
(cid:18) (cid:19)1/2(cid:26)(cid:18) (cid:19)1/2 (cid:18) (cid:19)1/2(cid:27)
1 (cid:88)(cid:0) ψˆ (ε ) ψ (ε )(cid:1)2 1 (cid:88)(cid:0) ψˆ (ε ) ψ (ε )(cid:1)2 +2 1 (cid:88) ψ (ε )2 .
≤ n
n i
−
0∗ i
n
n i
−
0∗ i
n
0∗ i
i ∈I3 i ∈I3 i ∈I3
SinceEψ(ε )=0,wehave√n(β¯ β )=O (1). Moreover,theresiduals ε˘ =ε +X (β β¯ ):i I
1 n
−
0 p
{
i i i⊤ 0
−
n
∈
3
}
are conditionally independent given = X : i I , and max X α /γ =
o (n1/2) by (A1). Thus, by (79) in
LeD
m′
m≡ aD
30n′
,
D1 ∪D2
∪{
i
∈
3
}
i ∈I3∥ i
∥
n n
p
E(cid:16)1 (cid:88)(cid:0) ψˆ (ε˘) ψˆ (ε )(cid:1)2 (cid:12) (cid:12) (cid:17) = 1 (cid:88)(cid:90) (cid:8) ψˆ (cid:0) z+X (β β¯ )(cid:1) ψˆ (z)(cid:9)2 p (z)dz
n n i − n i (cid:12) D′ n R n i⊤ 0 − n − n 0
i ∈I3 i ∈I (cid:90)3
max (cid:8) ψˆ (cid:0) z+X (β β¯ )(cid:1) ψˆ (z)(cid:9)2 p (z)dz p 0
≤ i ∈I3 R
n i⊤ 0
−
n
−
n 0
→
62and
E(cid:16) n1 (cid:88)(cid:0) ψˆ n(ε i) −ψ 0∗(ε i)(cid:1)2 (cid:12) (cid:12)
(cid:12)
D′(cid:17) = |I n3 |(cid:90) R(ψˆ
n
−ψ 0∗)2p
0
→p 0
i ∈I3
as n . Finally, by the weak law of large numbers and the fact that I /n 1/3, we have
3
n 1(cid:80)→ ∞ ψ (ε )2 p (cid:82) (ψ )2p /3=i (p )/3, so it follows from (106), (107) an| d L| emm→ a 41 that
− i ∈I3 0∗ i → R 0∗ 0 ∗ 0
1 (cid:88)
ψˆ (ε˘)2 =
1 (cid:88)
ψ (ε )2+o (1)=
i ∗(p 0)
+o (1).
n
n i
n
0∗ i p
3
p
i ∈I3 i ∈I3
By arguing similarly for j =2,3, we conclude that
3 3
ˆı = 1 (cid:88) (cid:88) ψˆ (ε˘)2 p (cid:88)i ∗(p 0) =i (p )
n n,j i ∗ 0
n → 3
j=1i ∈Ij+2 j=1
as n .
→∞
6.4 Auxiliary results and proofs
Huber(1964,Theorem3)establishedanequivalentvariationalcharacterisationoftheFisherinformation
for information; see Huber and Ronchetti (2009, Theorem 4.2) for an alternative proof of the following
fact based on Hilbert space theory.
Proposition 36. For a distribution P on R, the following are equivalent:
0
(i) P has an absolutely continuous density p on R with respect to Lebesgue measure, with i(p ) =
0 0 0
(cid:82)
(p )2/p < .
{p0>0
}
′0 0 ∞
(cid:0)(cid:82) (cid:1)2
ψ dP
(ii) I(P 0) := sup (cid:82)R ψ′ 2dP0 < ∞, where the supremum is taken over all compactly supported, con-
ψ R 0
tinuously differentiable ψ: R R such that (cid:82) ψ2dP >0.
R 0
→
Furthermore, if either (i) or (ii) holds, then I(P )=i(p ).
0 0
The proof of Lemma 30 requires the next three lemmas, the first of which is adapted from van der
Vaart (1998, Lemma 7.6 and Example 7.8).
Lemma 37. Suppose that p is an absolutely continuous density on R with i(p ) < . For θ R, let
0 0
p () := p ( θ), so that p () = p ( θ). Then the location model p : θ R i∞ s different∈ iable in
quθ
a·
dratic
0
m· ea−
n at every θ
′θ
R·
, i.e.
′0
·− {
θ
∈ }
∈
(cid:90) (cid:18) √p
θ+h
−√p
θ +
p
′θ
(cid:19)2
0 as h 0
R h 2√p θ → →
for every θ R.
∈
Proof. Under our assumptions on p 0, we will show first that √p
0
is locally absolutely continuous on R
withderivativep ′0/(2√p 0)Lebesguealmosteverywhere. ThisessentiallyfollowsfromSerrinandVarberg
(1969, Theorem 3 and Corollary 8), but we give a direct argument here for completeness. For n N,
define φ ,Φ : R R by φ (z):=1/(2√z) for z >1/n and φ (z):=0 otherwise, and Φ (z):=(cid:82)z∈ φ .
n n → n n n 0 n
Then for every z R, we have Φ (z) z1/2 and φ (z) 1/(2√z)1 =: φ(x) as n . Now
p is absolutely co∈ ntinuous on R an nd ea↗ ch Φ+ is Lipsn chitz↗ on R with w{ ez a> k0 } derivative φ , s→ o Φ∞ p is
0 n n n 0
absolutelycontinuousonRforeachn, with(Φ p )(z)=(φ p )(z) p (z)forLebesguealmost◦ every
z R. Thus, for all x,y R, we have
n
◦
0 ′ n
◦
0
·
′0
∈ ∈
(cid:90) y
(Φ p )(y) (Φ p )(x)= (φ p )p ,
n
◦
0
−
n
◦
0 n
◦
0 ′0
x
63(cid:0)(cid:82)y (cid:1)2 (cid:82) (cid:0) (cid:1)2
andbytheCauchy–Schwarzinequality, (φ p )p y x (φ p )p = y x i(p )/4< .
x | ◦ 0 ′0| ≤| − | R ◦ 0 ′0 | − | 0 ∞
It follows by the dominated convergence theorem that
(cid:112) (cid:112) (cid:0) (cid:1)
p (y) p (x)= lim (Φ p )(y) (Φ p )(x)
0 0 n 0 n 0
− n ◦ − ◦
→∞(cid:90) y (cid:90) y (cid:90) y p
= lim (φ p )p = (φ p )p =
′0
(108)
n →∞ x
n
◦
0 ′0
x ◦
0 ′0
x 2√p 0
for all x,y R, so √p
0
is indeed locally absolutely continuous on R.
Thus, fo∈ r θ R and Lebesgue almost every z R, we have
∈ ∈
(cid:112) (cid:112) (cid:112) (cid:112)
p (z) p (z) p (z θ h) p (z θ)
θ+h θ 0 0
− = − − − −
h h
p (z θ) p (z)
(cid:112)′0
− =
(cid:112)′θ
=:q θ(z) (109)
→−2 p (z θ) −2 p (z)
0 θ
−
as h 0. Moreover, for θ R and h = 0, we deduce from (108), the Cauchy–Schwarz inequality and
→ ∈ ̸
Fubini’s theorem that
(cid:90) (cid:18)(cid:112) p (z) (cid:112) p (z)(cid:19)2 (cid:90) (cid:18)(cid:90) 1 (cid:19)2
θ+h θ
− dz = q (z)dt dz
θ+th
R h R 0
(cid:90) (cid:90) 1 (cid:90) 1(cid:90) p (z)2
q (z)2dtdz = ′θ+th dzdt
θ+th
≤ R 0 0 R 4p θ+th(z)
(cid:90) 1(cid:90) p (z)2 (cid:90) i(p )
= ′θ dzdt= q2(z)dz = 0 < . (110)
0 R 4p θ(z) R θ 4 ∞
It follows from (109), (110) and van der Vaart (1998, Proposition 2.29) that
(cid:90) (cid:18) √p
θ+h
√p
θ
(cid:19)2
− q 0 as h 0,
θ
R h − → →
as required.
Lemma 38. Let ψ: R R be a decreasing and right-continuous function, and let p be a continuous
0
density on R. Suppose t→ hat there exists δ >0 such that
(cid:90)
Ψ(t):= ψ(z t)p (z)dz
0
R −
is finite for t δ,δ . Then Ψ(t) R for all t [ δ,δ]. Assume further that at least one of the
∈ {− } ∈ ∈ −
following conditions holds:
(i) p is of bounded variation on R, and there exists ϵ > 0 such that (cid:82) p (t ϵ)dψ(t) < and
0 R 0
(cid:82) − − ∞
p (t+ϵ)dψ(t)< .
R 0
− ∞
(ii) For some ϵ>0, the function q : R R given by q (t):=sup r 1(cid:0)(cid:82)t p (cid:82)t+r p (cid:1) satisfies
(cid:82)
q dψ < .
ϵ → ϵ r ∈(0,ϵ) − t −r 0 ∨ t 0
R ϵ
− ∞
(cid:82)
Then t Ψ(t) is differentiable at 0 with Ψ (0)= p dψ [0, ).
′ R 0
(cid:55)→ − ∈ ∞
Proof. Since ψ is decreasing, ψ(z t) ψ(z δ) ψ(z +δ) for all z R and t [ δ,δ]. Since
(cid:82) | − | ≤ | − |∨| | ∈ ∈ −
ψ(z t) p (z)dz < for t δ,δ , it follows that Ψ(t) is finite for all t [ δ,δ].
R 0
| − | ∞ ∈{− } ∈ −
Supposethat(i) holds. Sincep isadensityofboundedvariationonR,wemusthavelim p (z)=0
0 z 0
and hence (cid:82) dp = (cid:82) dp = p (t) for all t R. Define g(z,t) := 1 | |→1∞ for
z,t R. Th( e− n∞(cid:82),t) g(z0 ,t+− ϵ)d[ ψt, ∞ (t) ) =0 ψ( ϵ0 ) ψ(z ϵ) f∈ or all z R and (cid:82) g(z,t{ +z< ϵt )≤ d0 p} − (z){ =0< pt ≤ (z t} +ϵ)
R R 0 0
for a∈ ll t R, so by (i) and Fubini’s theo− rem− , − ∈
∈
(cid:90) (cid:90) (cid:90) (cid:90)
(cid:0) (cid:1)
> p (t+ϵ)dψ(t)= g(z,t+ϵ)dp (z)dψ(t)= ψ(z ϵ) ψ( ϵ) dp (z)
0 0 0
∞ − R − R R R − − −
(cid:90)
= ψ(z ϵ)dp (z). (111)
0
R −
64Similarly, (cid:82) Rψ(z+ϵ)dp 0(z)= −(cid:82) Rp 0(t −ϵ)dψ(t) ∈[0, ∞). Writing ν 0 =ν 0+ −ν 0− for the Hahn–Jordan
decomposition of the Lebesgue–Stieltjes measure ν
0
induced by p 0, define |ν
0
|
:= ν 0+ +ν 0−. We have
shown that z ψ (z):= ψ(z ϵ) + ψ(z+ϵ) is ν -integrable. Since ψ is decreasing,
ϵ 0
(cid:55)→ | − | | | | |
(cid:12) (cid:12)1(cid:90) z (cid:12)
(cid:12)
(cid:12) (cid:12)1(cid:90) z+s (cid:12)
(cid:12)
ψ(z) (cid:12) ψ(cid:12) (cid:12) ψ(cid:12) ψ ϵ(z)
| |≤(cid:12)s (cid:12)∨(cid:12)s (cid:12)≤
z s z
−
for all s [ ϵ,ϵ] 0 and z R. Therefore, ψ is ν -integrable, and similarly to (111), (cid:82) ψdp =
0 R 0
(cid:82) ∈ − \{ } ∈ | |
p dψ [0, ). For t (0,δ ϵ], it follows by Fubini’s theorem that
R 0
− ∈ ∞ ∈ ∨
(cid:90) (cid:90) (cid:90)
Ψ(t) Ψ(0) p (z+t) p (z) 1
− = 0 − 0 ψ(z)dz = 1 dp (s)ψ(z)dz
t R t R t R {z<s ≤z+t } 0
(cid:90) 1(cid:90) s
= ψ(z)dzdp (s).
0
R t s t
−
Similarly,
Ψ(0) Ψ( t) (cid:90) 1(cid:90) (cid:90) 1(cid:90) s+t
− − = 1 dp (s)ψ(z)dz = ψ(z)dzdp (s).
t R t R {z −t<s ≤z } 0 R t s 0
Therefore, by the ν -integrability of ψ and the dominated convergence theorem,
0 ϵ
| |
(cid:90) (cid:90)
Ψ(t) Ψ(0)
Ψ ′(0)= lim − = ψ(s)dp 0(s)= p 0dψ.
t 0 t R − R
→
Now suppose that (ii) holds. For t (0,δ], it follows by Fubini’s theorem that
∈
(cid:90) (cid:90) (cid:90)
Ψ(t) Ψ(0) ψ(z) ψ(z t) 1
− = − − p (z)dz = 1 dψ(s)p (z)dz
t − R t 0 − R t R {z −t<s ≤z } 0
(cid:90) 1(cid:90) s+t
= p (z)dzdψ(s)
0
− R t s
and similarly
Ψ(0) Ψ( t) (cid:90) 1(cid:90) (cid:90) 1(cid:90) s
− − = 1 dψ(s)p (z)dz = p (z)dzdψ(s).
t − R t R {z<s ≤z+t } 0 − R s s t 0
−
Since p is continuous, lim t 1(cid:82)s+t p (z)dz = p (s) for all s R. Therefore, by (ii) and the
dominat0
ed convergence
theot r→ em0 ,− (cid:82)s
t
10 (cid:82)s+t
p
(z)d0
zdψ(s)
(cid:82)∈
p (s)dψ(s) as t 0, so Ψ (0) =
(cid:82) − R − s 0 → − R 0 → ′
p dψ.
R 0
−
Lemma 39. Let p : R [0, ) be a Lebesgue density. For h>0, define p : R [0, ] by p (z):=
0 0,h 0,h
→ ∞ → ∞
(2h) 1(cid:82)z+h p . Given ε ,...,ε iid p and a kernel K: R R that is supported on [ 1,1], define
pˆ − : R z −h [0,0 ) by pˆ (1 z) := nn 1∼(cid:80)n0 K (z ε ), where K→ () = h 1K(/h). Then fo− r ρ [0,1/2]
n,h → ∞ n,h − i=1 h − i h · − · ∈
and δ >ρ/(1 ρ), we have
−
E(cid:90)
pˆ (z) Epˆ (z) dz K
(cid:90) min(cid:26) p1 0/ ,h2
,2p
(cid:27) 21 −2ρ ∥K ∥∞(cid:82) Rp1 0−,hρ
R| n,h − n,h | ≤∥ ∥∞ R (nh)1/2 0,h ≤ (nh)ρ
21 −2ρ ∥K ∥∞C δρ ,ρ(1+h)δ(1 −ρ)(cid:0) 1+(cid:82)
R
|z |δp 0(z)dz(cid:1)1 −ρ
,
≤ (nh)ρ
(cid:82) (cid:0) (cid:1)
where C := (1+ z ) δ(1 ρ)/ρdz =2ρ/ (1 ρ)δ ρ (0, ).
ρ,δ R − −
| | − − ∈ ∞
Proof. For z R and h>0, we have
∈
(cid:90)
Epˆ (z) =EK (z ε ) = K(u) p (z uh)du K p (z)
n,h h 1 0 0,h
| | | − | R| | − ≤∥ ∥∞
and similarly
VarK (z ε ) E K (z ε )2 (cid:90) K(u)2 K 2 p (z)
h 1 h 1 0,h
Varpˆ n,h(z)= − { − } = p 0(z uh)du ∥ ∥∞ ,
n ≤ n R nh − ≤ nh
65so
Epˆ (z) Epˆ (z) min(cid:8) Var1/2pˆ (z),2Epˆ (z)(cid:9) K min(cid:26) p 0,h(z)1/2 ,2p (z)(cid:27) .
| n,h − n,h |≤ n,h | n,h | ≤∥ ∥∞ (nh)1/2 0,h
By Fubini’s theorem and the fact that min(a,b) a2ρb1 2ρ for a,b 0 and ρ [0,1/2], we have
−
≤ ≥ ∈
(cid:90) (cid:90) (cid:26) p (z)1/2 (cid:27)
E pˆ (z) Epˆ (z) dz K min 0,h ,2p (z) dz
R| n,h − n,h | ≤∥ ∥∞ R (nh)1/2 0,h
≤∥K ∥∞(cid:90) R(cid:18) p 0 n,h h(z)(cid:19)ρ (cid:0) 2p 0,h(z)(cid:1)1 −2ρ dz = 21 −2ρ ∥K (n∥ h∞ )ρ(cid:82) Rp1 0−,hρ .
Finally, if U U[ 1,1] is independent of ε , then hU +ε has density p , so by H¨older’s inequality,
1 1 0,h
∼ −
(cid:90) (cid:18)(cid:90) (cid:19)ρ(cid:18)(cid:90) (cid:19)1 ρ
Rp1 0−,hρ
≤
R(1+ |z |) −δ(1 −ρ)/ρdz R(1+ |z |)δp 0,h(z)dz −
=C pρ ,δ(cid:2)E(cid:8) (1+ |hU +ε
1
|)δ(cid:9)(cid:3)1 −ρ
(cid:18) (cid:90) (cid:19)1 ρ
≤C pρ ,δ(cid:2)E(cid:8) (1+h |U |)δ ·(1+ |ε
1
|)δ(cid:9)(cid:3)1 −ρ ≤C pρ ,δ(1+h)δ(1 −ρ) 1+ R|z |δp 0(z)dz − .
This completes the proof.
For instance, when p is a Cauchy density and h > 8/n, taking ρ = 1/2 1/log(nh) and δ = 2ρ =
0
1 2/log(nh) in Lemma 39 yields a bound on E(cid:82) pˆ (z) Epˆ (z) dz of− order log(nh)/√nh, which
R n,h n,h
− | − |
is tight up to a universal constant.
Lemma 40. Let W Rn be a linear subspace of dimension d<n. If A R is countable, then
⊆ ⊆
(cid:110) (cid:88)n (cid:111)
:= (y ,...,y ) Rn : 1 d+1 for some (w ,...,w ) W
A
1 n
∈
{yi−wi∈A
} ≥
1 n
∈
i=1
has Lebesgue measure 0.
Proof. For I [n] with I = d+1, denote by E the linear span of (e : i Ic), which has dimension
I i
n d 1. Th⊆ en E +W| | = z+w : z E , w W Rn is a linear sub∈ space of dimension at most
I I
− − { ∈ ∈ } ⊆ (cid:80)
(n d 1)+d = n 1, so it is a null set (i.e. has Lebesgue measure 0). Moreover, A := a e :
− − − I { i I i i
a A for all i I is countable. Therefore, ∈
i
∈ ∈ }
(cid:91) (cid:91)
= (a+E +W)
I
A
I ⊆[n] a ∈AI
I=d+1
| |
is a countable union of null sets, and hence is also a null set.
Lemma 41 (Chernozhukovetal.,2018,Lemma6.1). Suppose that (X ) is a sequence of random vectors
n
and ( ) is a sequence of σ-algebras. If E( X )=o (1), then X =o (1) as n . Similarly,
n n n p n p
if
E(G
X )=O (1), then X =O
(1∥
).
∥|G ∥ ∥ →∞
n n p n p
∥ ∥|G ∥ ∥
Proof. Fix ε > 0. If E( X ) = o (1), then P( X > ε ) ε 1E( X ) p 0 by Markov’s
n n p n n − n n
inequality, so the
bound∥
ed
c∥ o| nG
vergence theorem
im∥ plies∥ that|PG
(
X≤ >ε)=∥ EP∥ (|G
X
→
>ε ) 0 as
n n n
∥ ∥ ∥ ∥ |G →
n . Since ε>0 was arbitrary, we conclude that X =o (1).
n p
→ O∞ n the other hand, suppose that E( X ) =∥ O (1∥ ). Then for any sequence M , we have
n n p n
P( X > M ) M 1E( X )∥ = o∥ (| 1G ) by Markov’s inequality, so similarly P( → X ∞ > M ) =
EP∥
(
n
X∥
>Mn |Gn
≤
)
0n−
as
n∥
n
∥|
.Gn Thus,p
X =O (1).
∥
n
∥
n
n n n n p
∥ ∥ |G → →∞ ∥ ∥
Lemma 42. Let (∆ ) be a sequence of random measurable functions7 ∆ : R R. Assume that for
n n
p →
some deterministic sequence w 0, we have ∆ (v ) 0 as n whenever (v ) is a deterministic
n n n n
→ → → ∞
sequence such that v = o(w ). Then for any sequence of random variables (V ) independent of (∆ ),
n n n n
p
we have ∆ (V ) 0 whenever V =o (w ).
n n n p n
→
7More precisely, writing Ω for the underlying probability space, suppose that there exist jointly measurable functions
∆˜ n:R Ω Rsuchthat∆n(v)()=∆˜ n(v, ):Ω Rforeveryn Nandv R.
× → · · → ∈ ∈
66Proof. Fix ε > 0 and let g (v) := P(∆ (v) > ε) for v R and n N. Then by assumption,
n n
g (v ) 0wheneverv =o(w ).
Since|
(V
)an|
d(∆
)areind∈ ependent,we∈ haveP(cid:0)
∆ (V ) >ε V
(cid:1)
=
n n n n n n n n n
→ | | |
g (V ) p 0wheneverV =o (w ). Therefore,bytheboundedconvergencetheorem,P(∆ (V ) >ε)=
n n n p n n n
EP(cid:0) ∆→
(V ) >ε V
(cid:1)
0, as required.
| |
n n n
| | | →
6.5 Antitonic projections and least concave majorants
Denote by Ψ the set of all decreasing functions on R. For an integrable function f: (0,1) R, recall
↓ →
from Section 1.2 the definitions of (cid:99)Lf and (cid:99)Rf on [0,1].
M M
Proposition 43. If ψ L2(P) for some Borel probability measure P on R with distribution function
∈
F, then
(cid:90)
ψ
P∗
:= M(cid:99)L(ψ ◦F −1) ◦F ∈ar gg ∈m Ψ↓in I(g −ψ)2dP =:Π ↓(ψ,P). (112)
We have ψ L2(P), and moreover ψ Π (ψ,P) if and only if ψ =ψ P-almost everywhere. Further-
P∗
∈ ∈ ↓
P∗
more, if F is continuous, then also (cid:99)R(ψ F −1) F Π (ψ,P).
M ◦ ◦ ∈ ↓
AversionofthisresultappearsasExercise10.24inSamworthandShah(2024). WerefertoΠ (ψ,P)
as the L2(P) antitonic (decreasing isotonic) projection of ψ. Since Ψ is a convex class of functi↓ ons,
↓
(cid:90)
(g ψ )(ψ ψ )dP 0 for all g Ψ ; (113)
I −
P∗
−
P∗
≤ ∈ ↓
see e.g. Samworth and Shah (2024, Theorem 10.36). This can be used to derive the following basic
inequalities.
Lemma 44. Given Borel probability measures P,Q on R and ψ L2(P) L2(Q), define ψ ,ψ as
∈ ∩
P∗ Q∗
in (112). Then
(cid:90)
ψ ψ 2 (ψ ψ )(ψ ψ )d(Q P). (114)
∥ P∗ − Q∗ ∥L2(P) ≤ R − Q∗ Q∗ − P∗ −
Moreover, if ψ ,ψ L2(P) and ψ :=(ψ ) for ℓ=1,2, then
1 2
∈
ℓ∗ ℓ ∗P
ψ ψ ψ ψ , (115)
∥
1∗
−
2∗ ∥L2(P)
≤∥
1
−
2 ∥L2(P)
and if ψ ,ψ are both bounded on R, then
1 2
ψ ψ ψ ψ . (116)
∥
1∗
−
2∗
∥∞ ≤∥
1
−
2
∥∞
Proof. By (113),
(cid:90) (cid:90)
(ψ ψ )(ψ ψ )dP 0 (ψ ψ )(ψ ψ)dQ,
R
Q∗
−
P∗
−
P∗
≤ ≤ R
P∗
−
Q∗ Q∗
−
(cid:82)
and adding (ψ ψ )(ψ ψ)dP to both sides yields (114).
R Q∗
−
P∗ Q∗
−
For the second assertion, define
(cid:13) (cid:13)2 (cid:13) (cid:13)2
D(t):=(cid:13)(1 −t)ψ 1∗+tψ 1 −{(1 −t)ψ 2∗+tψ 2 }(cid:13) L2(P) =(cid:13)(ψ 1∗ −ψ 2∗)+t(ψ 1 −ψ 1∗+ψ 2∗ −ψ 2)(cid:13) L2(P)
(cid:90)
= ψ ψ 2 +2t (ψ ψ )(ψ ψ +ψ ψ )dP +t2 ψ ψ +ψ ψ 2 ,
∥ 1∗ − 2∗ ∥L2(P) R 1∗ − 2∗ 1 − 1∗ 2∗ − 2 ∥ 1 − 1∗ 2∗ − 2 ∥L2(P)
which is a quadratic function of t R. By (113), (cid:82) (ψ ψ )(ψ ψ +ψ ψ )dP 0, so D is
∈
R 1∗
−
2∗ 1
−
1∗ 2∗
−
2
≥
non-decreasing on [0, ). Thus, ψ ψ 2 =D(0) D(1)= ψ ψ 2 , which proves (115).
∞ ∥ 1∗ − 2∗ ∥L2(P) ≤ ∥ 1 − 2 ∥L2(P)
Finally, it follows from the min-max formulae for the isotonic projection (Samworth and Shah, 2024,
Exercise 10.25(b) and Theorem 10.37) that for ψ ,ψ L2(P), we have
1 2
∈
ψ ψ P-almost everywhere ψ ψ P-almost everywhere. (117)
1
≤
2
⇒
1∗
≤
2∗
Now defining d:= ψ ψ for ψ ,ψ L (P), we have ψ d ψ ψ +d P-almost everywhere
1 2 1 2 ∞ 1 2 1
on R, so (117) imp∥ lies− that∥ ψ∞ d = (ψ∈ d) ψ (ψ +− d) ≤ = ψ ≤ +d P-almost everywhere. In
1∗
−
1
−
∗
≤
2∗
≤
1 ∗ 1∗
other words, ψ ψ = ψ ψ d, so (116) holds.
∥
1∗
−
2∗
∥∞ ∥
1∗
−
2∗ ∥L∞(P)
≤
67Lemma 45. For a function F: [0,1] R, suppose that its least concave majorant Fˆ on [0,1] satisfies
F(v)<Fˆ(v)< for some v (0,1) a→ t which F is continuous. Then there exists δ (cid:0) 0,min(v,1 v)(cid:1)
∞ ∈ ∈ −
such that Fˆ is affine on [v δ,v+δ].
−
Proof. Since Fˆ is concave on [0,1] with Fˆ F > and Fˆ(v) R, Rockafellar (1997, Theorem 10.1)
ensures that Fˆ is continuous at v. Ther≥ efore, b− ec∞ ause F is a∈ lso continuous at v, there exists δ
(cid:0) 0,min(v,1 v)(cid:1) such that inf Fˆ(u) > sup F(u). Define ℓ: [0,1] R to be th∈ e
affine
functi−
on that agrees
withu ∈ Fˆ[v − aδ t,v+ vδ]
δ and v
+u δ∈ .[v T−δ h,v e+ nδ]
ℓ > F on [v δ,v
+δ]→
and moreover
ℓ Fˆ F on [0,1] [v δ,v+δ], so ℓ − F on [0,1]. It follows from the defin− ition of the least concave
m≥ ajora≥ nt that Fˆ ℓ\ =Fˆ− and hence tha≥ t Fˆ =ℓ on [v δ,v+δ], as required.
∧ −
Lemma 46. For a continuous function F: [0,1] R, we have Fˆ(u)=F(u) for u 0,1 and
→ ∈{ }
F(u) F(0) F(1) F(u)
sup − =Fˆ(R)(0), inf − =Fˆ(L)(1).
u ∈(0,1) u u ∈(0,1) 1 −u
(cid:0) (cid:1)
Proof. Let C :=sup F(u) F(0) /u, so that F(u) F(0)+Cu for all u [0,1], where we adopt
theconvention0 u ∈ =(0 0,1 .) SinceF− ˆ istheleastconcavema≤ jorantofF,wededuce∈ thatFˆ(u) F(0)+Cu
for all u (0,1]× an∞ d Fˆ(0) = F(0). Thus, sup (cid:0) Fˆ(u) Fˆ(0)(cid:1) /u = C. Moreover, by t≤ he concavity
of Fˆ
and∈
Rockafellar (1997, Corollary
24.2.1),u ∈(0,1) −
(cid:82)u Fˆ(R) Fˆ(u) Fˆ(0)
Fˆ(R)(0)= sup Fˆ(R)(u) sup 0 = sup − Fˆ(R)(0),
≤ u u ≤
u (0,1) u (0,1) u (0,1)
∈ ∈ ∈
so
F(u) F(0) Fˆ(u) Fˆ(0)
sup − = sup − =Fˆ(R)(0).
u u
u (0,1) u (0,1)
∈ ∈
The remaining assertions of the lemma follow similarly by considering u F(1 u) instead.
(cid:55)→ −
Lemma 47. If F: [0,1] R is differentiable at v (0,1) and its least concave majorant Fˆ is finite
at v, then Fˆ is differentia→ ble at v. Moreover, Fˆ(L) an∈ d Fˆ(R) are both continuous (and coincide) at v.
Proof. If F(v)<Fˆ(v), then by Lemma 45, Fˆ is affine on some open interval around v, so Fˆ is differen-
tiable at v in this case. On the other hand, if F(v)=Fˆ(v), then
F(v) F(u) Fˆ(v) Fˆ(u)
liminf − lim − =Fˆ(L)(v)
u v v u ≥u v v u
↗ − ↗ −
Fˆ(u) Fˆ(v) F(u) F(v)
Fˆ(R)(v)= lim − limsup − .
≥ u ↘v u −v ≥ u ↘v u −v
Thus, if F is also differentiable at v, then Fˆ(L)(v) = Fˆ(R)(v) = F (v), and Rockafellar (1997, Theo-
′
rem 24.1) ensures that Fˆ(L) and Fˆ(R) are both continuous at v.
Lemma 48. Given 0 u v 1, suppose that F: [0,1] R is convex on [u,v], and concave on both
[0,u] and [v,1]. Then≤ ther≤ e exi≤ st u [0,u] and v [v,→ 1] such that the least concave majorant Fˆ is
′ ′
∈ ∈
affine on [u,v ] and coincides with F on [0,u] [v ,1].
′ ′ ′ ′
∪
Proof. If u = v, then the conclusion holds with u = u = v = v , so suppose now that u < v and define
′ ′
the affine function ℓ: [0,1] R by
→
v w w u
ℓ(w):= − Fˆ(u)+ − Fˆ(v).
v u v u
− −
Then Fˆ(w)=ℓ(w) for w u,v , so Fˆ ℓ on [0,u] [v,1] by the concavity of Fˆ. Moreover, since F is
∈{ } ≤ ∪
convex on [u,v], we have
v w w u
F(w) − F(u)+ − F(v) ℓ(w) Fˆ(w)
≤ v u v u ≤ ≤
− −
68for all w [u,v]. Therefore, Fˆ ℓ F on [0,1], so by the definition of the least concave majorant,
Fˆ ℓ=Fˆ∈ . Let u :=inf w [0,u∧ ]:Fˆ≥ (w)=ℓ(w) and v :=sup w [v,1]:Fˆ(w)=ℓ(w) . Then Fˆ =ℓ
′ ′
on∧ [u,v ] but Fˆ is not l{ ocal∈ ly affine at either u o} r v , so by Lem{ ma∈ 45, Fˆ(w) = F(w) fo} r w u,v .
′ ′ ′ ′ ′ ′
Since F ℓ is concave on both [0,u] and [v ,1], we conclude that Fˆ =F on [0,u] [v ,1] and∈ Fˆ{ =ℓ o} n
′ ′ ′ ′
≤ ∪
[u,v ], as claimed.
′ ′
69