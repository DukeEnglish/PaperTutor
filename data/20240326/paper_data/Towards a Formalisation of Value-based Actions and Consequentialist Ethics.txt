Towards a Formalisation of Value-based Actions
and Consequentialist Ethics
Adam
Wyner1[0000−0002−2958−3428],
Tomasz
Zurek2[0000−0002−9129−3157],
and
Dorota
Stachura-Zurek3[0000−0002−8157−4932]
1 Department of ComputerScience, Swansea University,Swansea, UK
a.z.wyner@swansea.ac.uk,
2 Complex CyberInfrastructure, Informatics Institute,Faculty of Science, University
of Amsterdam,
t.a.zurek@uva.nl,
3 Independentresearcher,
dorota.stachura1@gmail.com
Abstract. Agents act to bring about a state of the world that is more
compatible with their personal or institutional values. To formalise this
intuition,thepaperproposesanactionframeworkbasedontheSTRIPS
formalisation. Technically,thecontributionexpressesactionsintermsof
Value-based Formal Reasoning (VFR), which provides a set of proposi-
tionsderivedfromanAgent’svalueprofileandtheAgent’sassessmentof
propositions with respect to the profile. Conceptually, the contribution
providesacomputationalframeworkforaformofconsequentialistethics
which is satisficing, pluralistic, act-based, and preferential.
Keywords: computational values, actions, ethics
1 Introduction
In a multi-agent system,the behaviour of individual agents may be guided by a
varietyoffactors,e.g.,inaBelief-Desire-Intentionmodel[6],propositionsmodel
the state of the world, the goals, and the actions to change states. However, it
leavesopenbywhatmeansanagentselects the propositionstoreasonabout,as
the agent selects some propositions that are used to represent a partial state of
thebeliefsabouttheworldandapartialgoalstate.Tomakesuchaselection,we
consider motivated reasoning [10], which aims to address how an agent selects
propositions relative to the agent’s values; the selected propositions can then
be used in the Agent’s knowledge base from which actions are constructed. In
this way, the research addresses ethical agents in the sense that it provides a
foundational,computationalanalysisoftheunderlyingmotivationsforanagent’s
behaviour, which can be further developed to address matters of coordination
and interaction to represent organisational/institutionalbehaviour.
What is it for an Agent to behave with respect to their personal or insti-
tutional values? There are a range of values. Consider personal values such as
4202
raM
52
]AM.sc[
1v91761.3042:viXra2 Adam Wyneret al.
self-enhancement(ambition,wealth),opennesstochange(freedom,daring),con-
servation(obedience,nationalsecurity)amongstothers4.Individualsdospecific
actions which reflect their value preferences: if one highly values wealth, then
one may practice frugality and seek high paying employment. Moreover, some
values may relate to ethical (or moral) choices in terms of how individuals and
institutions ought to behave (or not) towards others and the world they live in,
e.g., self-transcendence (social justice and protecting the environment), though
others might be less directly linked to ethical or moral preferences, e.g. self-
enhancement. As a caveat, the aim of this paper is not to propose a moral ma-
chine, one which optimises an agent’s actions with respect to the “best” moral
position. Rather, we provide a means to define acceptable actions with respect
toanagentandtheirvalues.Inthispaper,wetakeanabstractpositiontowards
such relations or sets of values in order to provide a generic framework about
how actions reflect value preferences.
WeassumethatanAgentperformsactionsinordertobringaboutasituation
(stateoftheworld)thatismorecompatiblewiththatAgent’svaluesthanithad
beenpriortotheexecutionoftheaction;thatis,theAgentperformssomeaction
to realise their values. While an Agent may have other reasons to act, here we
focusonvalues.Whilevaluesareabstract,theycanberealisedorinstantiatedvia
the performance of an action which results in a world that is more compatible
with their values than it was before the performance of the the action. The
proposalisgroundedinValue-basedFormalReasoning(VFR)(presentedin[18]
andsection3),whereinthe locusofvaluesisbasedoneachAgent’svalue profile
andhowtheyregardapropositioninviewoftheirvalues.Fromthis,weconstruct
an Agent’s base of acceptable (from a values point of view) propositions. The
VFRprovidesaframeworkfortheattributionofvaluestopropositions,onwhich
webaseanactionlanguageinthefamiliarstatetransitionmechanismofSTRIPS
-STRIPS (Section4).Thus,theactionssatisfytheAgent’svaluepreferences
VFR
intermsofbringingaboutastatemorecompatiblewiththeirvalueprofile.Note
thatthisisdistinctfromattributingthevaluetothetransitionperse(Section6).
Moreover,sinceourmodelfocusesonconsequencesofactionsratherthanactions
itself, it canbe usedas a representationofkind ofconsequentialist ethics, which
is discussed below.
The key novel technical contribution is to use the VFR (Section 3) to define
STRIPS (Section 4), which is an action language, where Agents execute
VFR
actions to bring about states of the world relative to their value preferences.
The key novelconceptual contributionis to provideconsequentialistethics with
a formal framework in STRIPS (Section 2). We also introduce a proof-of-
VFR
concept, an experimental implementation of our model. The paper closes with
related work, discussion, and conclusion.
4 [13] and values in theEU’s Joint Research Centre
https://op.europa.eu/webpub/jrc/jrc-values-identities/what-are-values.htmlValue-based Actions and Consequentialist Ethics 3
2 Consequentialism
Consequentialism stands for theories which assume that normative properties
depend on consequences [2,14], thus representing one of the major approaches
to normative ethics. The foundation for modern consequentialism was laid by
Jeremy Bentham, the creator of utilitarianism, for whom the goodness of ac-
tions is judged by whether they maximize utility, understood as the greatest
happiness for the greatest number of people [5]. One should therefore follow a
courseofactionwhichpromotespleasure(happiness)andavoidit ifcausespain
(unhappiness). This qualitative approach naturally raised serious reservations,
paving way to the development of Bentham’s ideas and provoking the ongoing
discussion on qualitative differences between pleasures, the pluralism of values
constituting happiness, or approaches to assessing consequences. An interesting
form of consequentialism relevant to research here was formed in response to
oneofthestrongestobjectionsagainstconsequentialism,thedemandingness ob-
jection. Considering that maximizing consequentialistapproacheswhich require
thatwemaximizegoodandactinsuchawayastobringaboutthebestpossible
consequences, many critics find them too demanding or objectionable on other
grounds[16]. Satisficing consequentialism holds that it is morallypermissible to
choose consequences which are less than optimal, but good enough [15,11].
In this paper, STRIPS seems to most closely model consequentialism
VFR
which is satisficing, pluralistic (i.e., a variety of irreducible values), act-based
(i.e.,consequencesareevaluatedforasingleact),andpreferential (i.e.,theagent
seekstheirownvaluepreferencefulfillment).Finally,wearenotadvocatingthat
this form of consequentialism is the only or best form of ethical reasoning, but
only that it is most compatible with the formalism provided here, which then
provides the means to examine the form and implications computationally.
3 Agents, Propositions, Values, and Weights
In this section, we outline a value-based language for decision making:
– Agent, where each element is an agentive entity.
– Prop, where each element is a proposition5
– IncompPropis of type Prop × Prop, where for every pair <x,y> of distinct
elements of type Prop,x and y cannot co-occurin any state. The relationis
symmetric.Theexpressionsofsuchapairarecalledobjectively incompatible,
otherwise, they are objectively compatible.
– Value, where each element of Value is an abstract object that expresses a
value concept such as freedom, security, etc.
– Scale, which is a totally ordered, finite set of scalar elements.
5 Theproposalisneutralastowhethertheseareatomicorcomplex,provideasemantic
representation or are simply strings.4 Adam Wyneret al.
– Weight is of type Scale | ?. 6 ? is a designated entity type which indicates
thataweightisindeterminate(notrelevant).Whiletheremaybealternative
interpretations of ‘weights’, here they reflect the relative ‘importance’ to an
agent,e.g.,family mightbe averyimportantvalueandpersonal status very
unimportant.As?isunorderedwithrespecttootherelementsof Scale,any
expression comparing ? to the other entity is false.
We can quantify over any type in the basic vocabulary with variables and
constants of each type, represented by Greek and Latin subscripts respectively,
e.g., for Agent, agent , agent , ... or agent , agent , ....
α β a b
We first construct an agent’s value profile, AgentValueToWeight,which in-
dicates the degree of importance that the agent ascribes to a value, where the
higher the weight, the more important and the lower the weight the less impor-
tant. AgentValueToWeightis a total function.
AgentValueToWeight=
(Agent × Value) → Weight
We indicate each agent’s value profile with a subscript, e.g., AgentValueTo-
Weight foragentagent .Giventhe?weight,theimportanceanagentasso-
agentk k
ciateswithavaluecanbeindeterminate.SincedifferencesinAgentValueToWeight
of particular agents represents differences in the levels of importance the agents
puts on those values, they can be seen as a subjective or personal value profiles
of those agents.
To represent how an agent assesses an element of type Prop with respect to
an element of type Value and an element of type Weight, we introduce a total
function:
AgentValuePropWeight:
(Agent × Value × Prop) → Weight
Thisexpressesanagent’sdispositiontowardsapropositionwithrespecttovalues
and weights. While AgentValuePropWeight is a total function, the use of ?
signalsthattheremaybepropositionswhicharenotmeaningfullyassessed.The
association of a proposition with a particular value and weight appears in [20]
and in work on case-basedreasoning with factors and values [4].
The functions AgentValueToWeight and AgentValuePropWeight are taken
to be conceptually distinct. In particular, we interpret AgentValueToWeightto
indicate the importance that the agent ascribes to the Value, where the higher
the weight, the more important, and the lower the weight, the less important,
relevant, or disposed. It is a predicate that expresses a disposition of the agent
in general about a Value. In contrast, we interpret AgentValuePropWeight to
indicateanagent’sassessmentoftheValueandWeightascribedtoaproposition.
In other words, AgentValueToWeight expresses an agent’s “ideal” and “global”
view on Values, while AgentValuePropWeightexpresses an agent’s assessment
6 | is Backus-NaurForm for ‘or’.Value-based Actions and Consequentialist Ethics 5
of eachpropositionwith respect to a value. Inthis approach,values aredirectly
associated with propositions relative to agents and their value profile.
To reflect an agent’s value-based world view, we gather all the propositions
that are in some sense “compatible” with an agent’s values. In VFR, this means
thatthe weightanagentassignstoapropositionrelativetoavalue mustnotbe
less thanthe weightthat agentassignsto thatvalue ingeneral.The proposition
must “pass” the filter of acceptability relative to the agent’s value profile7. By
“passing” afilter,weunderstandthattheweighttheagentassignstoaparticular
proposition in the light of particular value is no less than the weight the agent
assigns to this value (see def. 1).
Now we abstract from consideration of Values and Weights and extract
those propositions which represent the agent’s value-based world view. A set
propBaseClean contains all and only those propositions which pass the
agenth
value-weightfilterofagent forallvalues.Itisimportanttoempahsisethatthis
h
is a strict, formally convenient definition, which assesses all propositions and
creates a set ofonly those propositions that pass the agent’svalue filter. We ac-
knowledge matters are more complex and less strict; in other work, we develop
morelenient,flexibledefinitions,thoughspaceprecludestheirpresentationhere.
Definition 1 (propBaseClean) propBaseClean is of type Prop.
Where agent is a variable for elements of type Agent, p is a variable for
α β
elements of type Prop, and v is a variable for elements of type Value, the
γ
denotation relative to an agent is:
α
propBaseClean = {p |¬(AgentValuePropWeight(agent ,v ,p )<
agentα β α γ β
AgentValueToWeight(agent ,v ))}
α γ
Valuesandweightsdiscriminateamongstpropositions.Foraparticularagent,
a lower weighton a particular value implies that there is a lower discriminatory
threshold on the acceptability of propositions, which themselves are associated
with that Value and the Weight. Simply put, if an Agent has a lower Weight
on a particular Value, then more propositions may pass the filter, as they have
higherweightsonthesamevalue.Thehighertheweightmeansthatanagenthas
higher standards with respect to the value; there is greater discrimination such
that fewer propositions pass the filter. An intuitive example may help to clarify
the relations between the value-weights in AgentValueToWeight and AgentVal-
uePropWeight. Suppose an agent is not much bothered by the quality of coffee,
so on the value taste the weight is low, and the agent acts accordingly. When
they are served a coffee that on the value taste has weight that is also low,
then the agent drinks the coffee; when served another coffee with value taste
withweighthigh,thentheagentalsodrinksthecoffee.Ineffect,thetastemakes
littledifferencetothisagent,astheydon’tdiscriminate.Ontheotherhand,sup-
pose the agenthas on a highweighton the value taste. In the firstinstance, the
agent rejects the coffee as not upholding their higher standards on the value; in
7 Note that propositions can be indeterminate (represented by ?) with respect to the
Weight on Values;that is, not every proposition need bevalue-weighted.Given ?is
unorderedwithrespecttoweights,suchapropositionalwayspassesanagent’sfilter6 Adam Wyneret al.
the second instance,the agentis satisfied anddrinks the coffee.propBaseClean
represents an idealised view of an agent’s assessment of a set of propositions,
where all propositions must pass an agent’s highest weighting; presentation of
other, less stringent variations remain for future work.
Anysetof propBaseCleanmaycontainobjectivelyincompatible propositions.
Note that such propositions can pass the filter defined on propBaseClean,even
if such pairs appear in incompProp.Moreover,we can see from propBaseClean
that two agents may each acceptthe same proposition, yetfor different settings
of values and weights.
We assume propBaseCleanrepresents static (all at once) and private (inac-
cessibletoothers)associationsofvalue-weightstoexpressionsbyanagent.Note
that, we do not analyse whether the propositions in propBaseCleanare true or
believed to be true; the set presents propositions which the agent can accept in
the light of his/her value profile. The key point of creation of propBaseClean
is to distinguish propositions which are coherent with the agent’s value profile.
By the same token, the complement to propBaseClean reflects all those
agenth
expressions which are incompatible with the Agent’s values and weights.
propBaseClean =
agentα
{p | p 6∈ propBaseClean }
β β agenth
Considerthevariouswayspropositionsmayappearinintersectingorcomple-
menting sets of two Agents’ propBaseClean.For intersecting: the Agents have
thesamevalue-weightprofileandsamevalue-weightsonsameprop;samevalue-
weight profile and different value-weights on prop, but not sufficient to block;
different value-weight profile and same value-weight on prop, but not sufficient
to block; different both, but not sufficient to block. Where they have the same
props, neither the value-weight profile nor value-weight on prop is sufficient to
discriminate. For complementing: same value-weight profile and different value-
weights on prop, and sufficient to block; different value-weight profile and same
value-weight on prop, and sufficient to block; different both and sufficient to
block. In other words, differences in sets of props arise where value-weight pro-
files or value-weights on props are sufficient to discriminate. Broadly speaking,
where a proposition appears in the intersection of the sets of propBaseClean
of two Agents, we can say the agents agree on that proposition in one sense
or another, while where the proposition is in complementary distribution (in
one set, but not the other), we say there is some sense of disagreement. Note
that there may be different justifications for the agreement or disagreement as
well as different extents of such justification, e.g., greater or lesser difference in
weights associated with the value. Relatedly, two agents can have the same de-
notations for their respective propBaseClean,yetdifferent value-weightprofiles
or different value-weights on the same prop.
Example 1. [Creating propBaseClean]
To illustrate propBaseCleans,suppose an agent (agent ), 4 propositions {p ,
A 1
p ,p ,p }, and 2 values: V ={v ,v }, e.g., privacy or law enforcement, where
2 3 4 Q PValue-based Actions and Consequentialist Ethics 7
AgentValueToWeight(agent ,value )=1
A Q
AgentValueToWeight(agent ,value )=2
A P
Agent has high requirements concerningvalue Q, and lower requirements con-
A
cerning value P
For AgentValuePropWeight, Table 1 is a tabular form for instances of agent,
propositions,values,andweights.ThepropositionsthatareinthepropBaseClean
are indicated in bold.
Table 1. AgentValuePropWeigh for agent A
AgentsPropositionsValuesWeights
agentA p1 valueQ 2
agentA p1 valueP 1
agent
A
p2 value
Q
2
agent
A
p2 value
P
2
agent
A
p3 value
Q
3
agent
A
p3 value
P
2
agent
A
p4 value
Q
1
agent
A
p4 value
P
3
4 State Transitions
In order to represent our model we introduce our definitions and assumptions:
Definition 2 Let S be a set of states. Each state s ∈ S is a consistent set of
β
propositions. A set of propositions will be consistent if: ∀p ,p ∈s :
γ δ β
<p ,p >6∈IncompProp
γ δ
Ourbasicintuitionisthatanagentaimstoexecutethoseactions(statetran-
sitions)relativetoapreconditionstatewhichmakethepostconditionstatemore
compatible with their values; that is, actions remove propositions found in the
propBaseClean complement from the precondition and introduce propositions
associated with propBaseClean into the postcondition. A simple example is:
Example 2. [State Change 1] AgentA propBaseClean: {p1, p2}
Initial-state: {p1,p3}
Action a1: <{p1},{p1,p2}>, Action a2: <{p1},{p2,p3}>
Action a1 is compatible with AgentA’s props (relative to their values), while
Action a2 is not.
To model of ethical decision making into a state transition system, we use
https://en.wikipedia.org/wiki/Stanford_Research_Institute_Problem_SolverSTRIPS
[8], which is a simple, familiar, relatively neutral model of a transition system
(for alternatives see Section 6). STRIPS is used in planning actions from some8 Adam Wyneret al.
initial state through intermediate states to a final goal state. For our purposes
here,wedon’tintroduce the planningalgorithmora choicemechanismbetween
two equally executable actions, which are left for future work.
4.1 STRIPS
Mathematically, a STRIPS instance is a quadruple hP,O,I,Gi, in which each
component has the following meaning:
– P is a set of conditions (i.e., propositional variables8);
– O is a set of operators (i.e., actions); each operator is itself a quadruple
hx,y,z,ti, each element being a set of conditions. These four sets specify, in
order, which conditions must be true for the action to be executable, which
ones must be false, which ones are made true by the action and which ones
are made false;
– I is the initial state, given as the set of conditions P that are initially true
(all others are assumed false);
– G is the specification of the goalstate; this is given as a pair hN,Mi, which
specify which conditions are true and false, respectively, in order for a state
to be considered a goal state (N represents a set of propositions that must
be true, M a set of propositions that must be false).
Astateisrepresentedbythe setofconditionsthataretrueinit.Transitions
betweenstatesaremodeledbyatransitionfunction,whichisafunctionmapping
states into new states that result from the execution of actions.Since states are
representedbysetsofconditions,the transitionfunction relativetotheSTRIPS
instance hP,O,I,Gi is a function:
succ:2P ×O →2P
In order to make a plan, the function succ can be recursively extended to se-
quencesofactions.Detaileddescriptionofthismechanismcanbefoundin[8].If
O andO areoperators,thenby[O ,O ]wedenoteplancontainingperforming
1 2 1 2
O followed by O .
1 2
For this paper and given space constraints, we assume:
– Inertia holds between state changes.
– Someapproachtothe https://en.wikipedia.org/wiki/Frame_problemFrame
Axiom.
– States are objectively consistent.
– An action’s preconditions and postconditions are objectively consistent.
– We model a small sample of actions rather than all possible actions.
– We do not here assume that it is necessary for goals to be attained by
executions of actions, e.g., relating to safety and liveness.
– InthedefinitionofthegoalG,Niswhattheagent“wants” andMiswhatthe
agentdoesn’t“want”,i.e.,propBaseClean andpropBaseClean .9
agentα agentα
8 Wecan takea specification of conditions as a state.
9 Whether G represents the agent’s ideal, everything they “want” and nothing they
don’t “want”, or some subideal, somethings they “want” with something they don’t
“want”,is a modeling issue for futuredevelopment.Value-based Actions and Consequentialist Ethics 9
4.2 STRIPSVFR - Relative to an Agent’s propBaseClean
However, we want to relate the actions that an agent executes to that agent’s
propBaseCleanto imply the compatibility with that agent’s value profile.
We assume the goal state G, < N,M >, where N is consistent and relative
to propBaseClean (also see the footnote 8).
agentα
Definition 3 (Goals Relative to Agent’s propBaseClean) WhereG
agentα
of hP,O ,I,G i is
agentα agentα
<N ,M >:
agentα agentα
N
agentα
∈ 2propBaseCleanAgentα, where
∀γ,δ ∈ N :
Agentα
<γ,δ >6∈IncompProp.
Note that a goal is, in STRIPS, simply a proposition that holds of a goal
state. In VFR, a value is realised with respect to propositions, i.e., those in an
Agent’s PropBaseClean.In this sense, a goal connects with an Agent’s values.
We define those actions which lead to the introduction of a propositionrela-
tive to propBaseClean :
agentα
Definition 4 (Agent’s Actions Introducing Propositions) WereviseOof
hP,O,I,Gi to relativise the actions to the agent – hP,O ,I,G i.
agentα agentα
O is a quadruple hx,y,z ,ti, where x,y and t are as in O, where:
agentα agentα
z ⊆ propBaseClean .
agentα agentα
The revised definition of actions means that the actions an agent takes has to
result in a postcondition where propositions hold that are compatible with the
agent’s values. It does not execute actions which introduce propositions incom-
patible with the agent’s values. However, it also does not necessarily eliminate
fromthepreconditionthosepropositionswhichareincompatiblewiththeagent’s
values and which my be inherited by inertia.
ThetransitionfunctionisalsomodifiedhP,O ,I,G iisafunction:
agentα agentα
Definition 5 (Revised Transition Function) succ:2P ×O →2P
agentα
The above modification of the STRIPS framework allows the agent to take
only the actions which lead to those consequences that are wanted from the
perspective of that agent, i.e. consequences which pass the value profile filter
and are in propBaseClean . In other words, our model can be understood
agentα
as a kind of filter which excludes from the available plans those which do not
lead to consequences that satisfy the Agent’s values and includes those which
do. Note also that this model does not preserve the system from inheriting (by
inertia) bad propositions from previous states.
Example 3. Suppose a set of propositions {p ,p ,p ,p } and a
1 2 3 4
propBaseClean = {p ,p ,p }. Suppose initial state {p ,p } (note that
agentA 2 3 4 1 2
initial state is not in agent’s propBaseClean) Set O contains following op-
agetA
erators:10 Adam Wyneret al.
01: h{p ,p },{},{p },{p }i
1 2 3 1
02: h{p },{},{p },{}i
2 3
03: h{p },{},{p },{}i
3 4
Goal of the agent is: G = h{p },{}i In standard STRIPS there are two
agentA 4
available plans:
– [O1,O3] which, in final state, contains: s ={p ,p ,p }
final 2 3 4
– [O2,O3] which, in final state, contains: s ={p ,p ,p ,p }10
final 1 2 3 4
Sincethesecondplan(followingtheinertiaofstatechanges)containsproposition
p which is not in the propBaseClean of the agent, so leads to a subideal world
1
from the perspective of the Agent.11 If we introduce another action:
03’: h{p },{},{p ,p },{}i
3 1 4
ThenumberofavailableplansallowingforreachingthegoalforstandardSTRIPS
will increase. For STRIPS , however, it will remain the same, because op-
VFR
erator O3’ leads to “immoral” results (it brings about p which is outside of
1
propBaseCleanand not acceptable for the agent) and cannot be used.
If we want actions relative to an Agent to incrementally remove impurities,
then we need to identify those propositions of the precondition which are un-
acceptable to the Agent, i.e., elements that in propBaseClean , then to
agentα
removethemfromthe postconditionstate.Inthis way,the Agentincrementally
acts to make the world more compatible with their propBaseClean .
agentα
Definition 6 (Agent’s Actions Removing Bad Things) O+ is a set
agentα
of quadruples hx,y,z ,t+i, where:
agentα
∀ α: [[α ∈ x ∧ α ∈ propBaseClean ] → α ∈ t+].
agentα
Thisisafilteronactions,thosewhichmeettheconditions.Itsaysthatthere
can be actions that clean with respect to propBaseClean , but there may
agentα
be actionsthat arenotdefined by the Agentandso anundesireableproposition
canremainandbeinherited.Thatis,itmayonlybe partialintermsofcleaning
‘power’, but it is not necessarily total.12 In Example 3, Action 01 ‘positively’
satisfiesDefinition6,whileActions02and03vacuouslysatisfythedefinition.An
action such as Action 01’ h{p ,p },{},{p },{}i does not satisfy the definition.
1 2 3
5 Implementation
In order to verify our model, we prepared its experimental implementation in
PROLOG language. Our program uses standard PROLOG reasoning mecha-
nism, below we present fragments of the code and discuss some key predicates.
We modelled the examples (1,3) presented in the paper. In particular, the
mechanism of propBaseClean creation as well the ethical plan selection. The
listing below presents a fragment of the propBaseCleancreation:
10 Given inertia, neither O2 nor O3 remove p1.
11 The revised definition of O and example can be furtherrefined.
12 Alternativeproposals are possible, but left as future work.Value-based Actions and Consequentialist Ethics 11
proposition(p1).
proposition(p2).
proposition(p3).
proposition(p4).
agentValue(valueP,2).
agentValue(valueQ,1).
agentValueProp(p1,valueP,1).
agentValueProp(p1,valueQ,2).
agentValueProp(p2,valueP,2).
agentValueProp(p2,valueQ,2).
agentValueProp(p3,valueP,2).
agentValueProp(p3,valueQ,3).
agentValueProp(p4,valueP,3).
agentValueProp(p4,valueQ,1).
notpassValue(X,Val):- agentValueProp(X,Val,Wp),
agentValue(Val,Wa), >(Wa,Wp).
propBaseClean(X):- proposition(X), \+ notpassValue(X,\_).
...
Predicate notpassValue(X,Val) defines which propositions do not pass the
valuetest,whereAgentValueToWeightforagivenvalue’sweight(variableWa)is
higher than the weightassignedto the same value with respectto a proposition
(variable Wp). Predicate propBaseClean(X)creates a list of propositions which
pass the test (fails not passing test).
The fragment mechanism of plan creation is presented below:
oper([p1,p2],[],[p3],[p1]).
oper([p2],[],[p3],[]).
...
goal([p4],[]).
ethical(L):-findall(X,propBaseClean(X),L).
satisfied(X):- goal(Z,T), in(Z,X), \+ commonel(X,T).
ethMove(X,Y):-ethOper(Xp,Xn,Zp,Zn), in(Xp,X), \+ commonel(X,
Xn), subtract(X,Zn,Xdel), union(Xdel,Zp,Y), \+in(Y,X).
ethOper(Xp,Xn,Zp,Zn):- oper(Xp,Xn,Zp,Zn), ethical(E),
in(Zp,E).
ethPlan(X,[X]):- satisfied(X).
ethPlan(X,Y):- ethMove(X,Z), ethPlan(Z,Y).
PredicateethPlan(X,Y)(implementingRevisedTransitionFunction,seedef.5)
represents the recursive mechanism of the creation of a plan, predicate
ethOper(Xp,Xn,Zp,Zn) (implementing Agent’s Actions Introducing Proposi-
tions,seedef.4)selectsoperatorstheconsequencesofwhichareinpropBaseCLean12 Adam Wyneret al.
(variable Xp represents conditions which must be true, Xn represents proposi-
tions which must be false, Zp represents propositions which becomes true, Zn
propositions which becomes false). Subsidiary predicates in(X,Y) and
commonel(X,Y)definewhetherpropositionsXarealsoinYorwhethertwolists
of propositions have common elements. Predicate satisfied(X) defines which
states fulfill the goal. Predicate ethMove(X,Y) defines move from one state to
another,takingintoconsiderationonlytheseoperatorswhichleadtoacceptable
(w.r.t. propBaseClean)consequences13.
If we provide a query ethPlan([p1,p2],[p2,p3,p4])then the system will
check whether there is an ethical plan leading from [p1,p2] to [p2,p3,p4] (the
answer is true). Note that the result list should contain a goal (in our case
p4). If we ask ethPlan([p1,p2],X), then the system will return two possible
ethical plans leading to a declared goal. Note that if add another action, e.g.,
oper([p3],[],[p1,p4],[]),the result will be the same (there will be no more
ethicalplans)becausethisoperatorleadstoan“unethical” result(itbringsabout
p1 which does not pass the propBaseCleantest).
6 Related Work and Discussion
RelatedWork Thereareotherwaystorepresentandreasonwithactions,butour
main objective is to formalise actions given an agent’s values. We used the well
known,simple mechanismofSTRIPS, leavingthe moreadvancedapproachesto
actions – situation calculus, event calculus, BDI, Dynamic Logic, Action Log-
ics, AATS, and others – for a future analysis. Below we discuss a selection of
approaches to modeling consequentialist ethics:
Most of the existing approaches to ethical decision making use a kind of
ordering between values. For example, [17] augment a Belief-Desire-Intention
framework with values, where valuings indicate preferences amongst outcomes.
The treatment of values is somewhat similar to our approach in that they are
abstractproperties associatedwithpropositions,but they introduce conditional
orderings over effects on values caused by decision options; these are not neces-
sary in our model and would seem to introduce a high degree of complexity.
Valuesappearindiscussionsaboutactionsinmulti-agentsystemswithAction-
based Alternating Transition Systems with values (AATS+V) [3]. Essentially,
anAATS+Vexpressesactionsastransitionfunctions fromstateto state,where
states are sets of propositions. Values are a set of abstract objects. A valuation
function describes whether a state transition promotes or demotes the value;
in a sense, it is a preference over actions. However, the values relate to states
as wholes, i.e., a set of propositions. Thus, it is unclear just what it is about
the states, i.e., particular propositions,such that the transition promotes or de-
motes the value. STRIPS is more specific, as the Agent executes actions
VFR
whichbringaboutstatescontainingpropositionsthataremorecompatible with
its values. In AATS+V actions are made in order to promote, i.e. increase the
13 Fullversionofourexperimentanddefinitionsofotherpredicatescanbefoundhere:
https://github.com/ZurekTom/PrologStrips/tree/mainValue-based Actions and Consequentialist Ethics 13
levels of satisfaction of values), without considering that a given value can be
already promoted to the satisfactory level (drinking one beer can promote hap-
piness, but drinking 8 beers not, as it will result in hangover). In other words,
AATS+V does not allow for expressing whether a given value reached a nec-
essary level of satisfaction. An extended version of AATS+V [3] introduces the
mechanisms called ‘maximisers’and ‘satisficers’,but these mechanisms are very
complexanditis problematichowto representthatagivenvalue doesnothave
to be promoted to a higher level than it is necessary.
Preferences are considered in AI across topic areas where choices are made
between objects and preference is a comparative ordering of the objects [7]. In
our proposal, there is no direct comparison between objects (propositions) and
an ordering over them. Rather, values filter propositions according to the value
profile of the Agent; essentially,this identifies the belief set that the Agent uses
for reasoning.
Our model shares some similarities with two value-based decision making
models: [20] and later [9], which were constructed on the basis of similar as-
sumptions. Firstly, both models are rooted in Schwartz value theory [13] ([9]
focuses strictly on the list of values introduced in Schwartz, et al., while [20]
has more abstractcharacterwithout specifying concrete values).Secondly, both
models assume that the thresholds on the levels of the satisfaction of values
can function as a motivation of the decisions made by an agent. The differences
between these models are in the details of formal machinery used to model the
analysed phenomena and in the burden of analysis: [20] focuses on the decision
making process, while [9] on the relations between values. The most similar to
our approach is [19], which is based on a formal model from [20], where the
[20]isdiscussedfromthe pointofviewofvariousethicaltheories.Ourapproach
extendstheabovemodelsbyintroducingasimple,butuniversalreasoningmech-
anismandbyintroducingrefineddescriptionofthestates(stateisdefinednotby
one proposition,but by a set of propositions), which allows for a more accurate
representation of the actual states.
Most existing approaches to consequentialist ethics in knowledge-based sys-
temsfocusonthe utilitarianversionofconsequentialism(forexample[1])where
theauthorsfocusonmaximisationofautilityfunction.Ourapproachisdifferent
because we do not aimat maximizing utility, but rather introduce a mechanism
to exclude “immoral” consequences from a plan.
Recently,anumber ofapproachesto consequentialistethics inreinforcement
learning systems have been introduced (for example, [12]). Although they are
out of the scope of this paper, it is worth considering whether our approach
could be useful in embedding ethics into reward function.
Discussion In this paper, we introduce a new mechanism wherein an Agent’s
actions take into consideration the Agent’s value profile. We modify a STRIPS
planner (STRIPS ) as an example of an action (state transition) system,
VFR
but our model can be adapted (after necessary modifications) to other action
formalisms and mechanisms.14 Adam Wyneret al.
Our research based on the observation that the ethical evaluation of deci-
sion options is made on the basis of two individual scales: a value profile which
describes the individual motivations of an Agent concerning the Agent’s values
(which in our paper is represented by a set of thresholds); and a personal eval-
uation of the consequences of decisions (which in our work is represented as a
weightsofvalues assignedto particularpropositions).The decisionaboutwhich
plan to choose is made on the basis of the suitability of the evaluation of the
consequences of the decision with respect to the agent’s value profile.
Such an approachhas a number of consequences: firstly it individualises the
value-based evaluation of decisions, rather homogenises them in a single uni-
versal value hierarchy. Such an approach represents real life situations, because
not every Agent shares the same needs, point of view, or priorities. Secondly,
our model gives a possibility for implementing various approaches to deal with
undesired propositions: from ignoring them (a “liberal” option, see def. 4) to
removing all non-acceptable proposition (“totalitarian” option). Moreover, the
approach relates to conceptions of multi-agent systems where the behaviour of
agents, whether individually or as groups, is the result of interactions amongst
individuals and their preferences (bottom-up) rather than globally (top-down).
Voting is an example.
Our approach can be seen as corresponding to a number of consequentialist
claims. The focus on consequences represented by states of affairs contributes
to an essentially consequentialist outlook on the moral assessment of possible
actions. The Agent filters out propositions, which ensures that the states of
affairs are good enough in moral terms; this accounts for the satisficing variant
of consequentialism. An individualized value profile of the agent corresponds to
the preferential nature of the ethical approachwe have adopted.
Ourmodelallowsforamulti-agentapproachtobehaviourwhichreflectsvalue
choices, e.g., introducing more than one agent and relativisation of STRIPS to
different agents: agents have their own ethical principles expressed by the VFR
mechanism and their propBaseCleans,individualised set of operators,and their
owngoals.Onthe basisofthat wecandiscuss the possibility ofnegotiationand
cooperationamongstagents.Thebasicintuitionis,thattwoagentcancooperate
if any proposition in the intermediate or final states of those agents will be in
propBaseCleans of both agents. This topic requires more detailed discussion.
In the proposal, an agent executes those actions wherein the consequences
are compatible with that agent’s value profile. In this sense, the proposal is a
constraint on the actions which are available to an agent to execute; that is,
the agent can only execute actions compatible with their values. In this, it is
anabstractionandsimplification.Inparticular,the proposaldoes notintroduce
issues related to the selection of actions or plans, wherein an agent might exe-
cute a “suboptimal” action not wholly compatible with their values. In such an
instance, one might consider whether the values an agent aims to achieve can
justify “suboptimal” actions. Such matters raise a range of issues about moral
and ethical reasoning, which are for future work.Value-based Actions and Consequentialist Ethics 15
Finally, we aim to develop a family of approaches of different settings of
parameters (goals, action definitions, relations of actions to goals) that have
different computational properties. Such approaches would be associated with
alternative theories of values in philosophy. By this we could enable computa-
tional exploration of various forms of ethical theories.
References
1. Anderson,M.,Anderson,S.L.,Armen,C.:Towardsmachineethics:Implementing
two action-based ethical theories (2005)
2. Anscombe,G.E.M.: Modern moral philosophy.Philosophy 33(124), 1–19 (1958)
3. Atkinson, K., Bench-Capon, T.J.M.: Value-based argumentation. FLAP 8(6),
1543–1588 (2021)
4. Bench-Capon, T., Prakken, H., Wyner, A., Atkinson, K.: Argument schemes for
reasoning with legal cases usingvalues.In:Proceedings of the14th Conference on
Artificial Intelligence and Law. p. 13–22. ACM, NewYork,NY,USA (2013)
5. Bentham, J.: An Introduction to the Principles of Morals and Legislation. Dover
Publications (1780)
6. Bratman,M.:Intention,Plans,andPracticalReason.CSLIPublications,Stanford,
California (1987)
7. Domshlak,C.,Hüllermeier,E.,Kaci,S.,Prade,H.:PreferencesinAI:anoverview.
Artif. Intell. 175(7-8), 1037–1052 (2011)
8. Fikes, R.E., Nilsson, N.J.: Strips: A new approach to the application of theorem
provingto problem solving. Artificial Intelligence 2(3), 189–208 (1971)
9. Heidari, S., Jensen, M., Dignum, F.: Simulations with values. In: Verhagen, H.,
Borit, M., Bravo, G., Wijermans, N. (eds.) Advances in Social Simulation. pp.
201–215. Springer International Publishing, Cham (2020)
10. Kunda,Z.:The casefor motivated reasoning. Psychological Bulletin 108(3), 480–
498 (1990)
11. McKay,D.: Solving satisficing consequentialism. Philosophia 50, 149–157 (2022)
12. Rodriguez-Soto, M., Antonio Rodriguez Aguilar, J., Lopez-Sanchez, M.: Guar-
anteeing the learning of ethical behaviour through multi-objective reinforcement
learning (2021), aLA2021
13. Schwartz,S.:AnoverviewoftheSchwartztheoryofbasicvalues.OnlineReadings
in Psychology and Culture 2(1) (2012)
14. Sinnott-Armstrong, W.: Consequentialism. In: Zalta, E.N., Nodelman, U. (eds.)
The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford
University,Winter 2022 edn.(2022)
15. Slote,M.,Pettit,P.:Satisficingconsequentialism.AristotelianSocietySupplemen-
tary Volume58(1), 139–176 (July 1984)
16. Vallentyne,P.:Againstmaximizingact-consequentialism.In:Jamie,D.(ed.)Con-
temporary Debates in Moral Theories, pp.21–37. Blackwell (2006)
17. Winikoff,M.,Sidorenko,G.,Dignum,V.,Dignum,F.:Whybadcoffee?explaining
BDI agent behaviourwith valuings. Artif. Intell. 300, 103554 (2021)
18. Wyner,A., Zurek,T.: On legal teleological reasoning. In:Sileno, G., Spanakis, J.,
van Dijck, G. (eds.) Legal Knowledge and Information Systems - JURIX 2023.
FrontiersinArtificialIntelligenceandApplications,vol.379,pp.83–88. IOSPress
(2023)16 Adam Wyneret al.
19. Zurek,T., Stachura-Zurek,D.: Applyingethics to autonomous agents. In:Bylina,
J.(ed.)Selected Topicsin AppliedComputerScience,pp.199–222. Wydawnictwo
UMCS, Lublin,Poland (2021)
20. Zurek,T.:Goals,values,andreasoning.ExpertSystemswithApplications71,442
– 456 (2017)