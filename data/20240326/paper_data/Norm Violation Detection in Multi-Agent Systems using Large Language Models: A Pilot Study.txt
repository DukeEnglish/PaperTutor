Norm Violation Detection in Multi-Agent
Systems using Large Language Models
A Pilot Study
Shawn He1, Surangika Ranathunga1[0000−0003−0701−0204],
Stephen Cranefield2[0000−0001−5638−1648], and
Bastin Tony Roy Savarimuthu2[0000−0003−3213−6319]
1 Massey University, Auckland, New Zealand
Shawn.He.1@uni.massey.ac.nz, S.Ranathunga@massey.ac.nz
2 University of Otago, Dunedin, New Zealand
{stephen.cranefield,tony.savarimuthu}@otago.ac.nz
Abstract. Normsareanimportantcomponentofthesocialfabricofso-
cietybyprescribingexpectedbehaviour.InMulti-AgentSystems(MAS),
agents interacting within a society are equipped to possess social capa-
bilities such as reasoning about norms and trust. Norms have long been
of interest within the Normative Multi-Agent Systems community with
researchers studying topics such as norm emergence, norm violation de-
tection and sanctioning. However, these studies have some limitations:
they are often limited to simple domains, norms have been represented
using a variety of representations with no standard approach emerg-
ing, and the symbolic reasoning mechanisms generally used may suffer
fromalackofextensibilityandrobustness.Incontrast,LargeLanguage
Models (LLMs) offer opportunities to discover and reason about norms
across a large range of social situations. This paper evaluates the capa-
bility of LLMs to detecting norm violations. Based on simulated data
from 80 stories in a household context, with varying complexities, we
investigated whether 10 norms are violated. For our evaluations we first
obtained the ground truth from three human evaluators for each story.
Then, the majority result was compared against the results from three
well-knownLLMmodels(Llama27B,Mixtral7BandChatGPT-4).Our
results show the promise of ChatGPT-4 for detecting norm violations,
with Mixtral some distance behind. Also, we identify areas where these
models perform poorly and discuss implications for future work.
Keywords: Norms·Normviolationdetection·Largelanguagemodels
1 Introduction
Norms prescribe behaviours that are expected by agents in a society. Norms are
pivotaltoestablishingsocialorderwithinasocietyastheyfacilitatecoordination
andcooperationbetweenparticipatingagents[2].Asoftwareagentthatisnorm
capable shouldbeabletoidentifyexistingnorms,detectsituationswherenorms
4202
raM
52
]AM.sc[
1v71561.3042:viXra2 He et al.
are violated and plan to either comply with norms or choose to violate them
withanunderstandingofthelikelyconsequences.Thesecapabilities(andmore)
are the subject of the research field of normative multi-agent systems.
This paper focuses on norm violation detection, also known as norm mon-
itoring. The importance of this ability in human society is evidenced by our
innate ability to detect norm violations. Brain studies in humans using EEG
recordings show a negative deflection of electrocortical potential that occurs at
around400msthatshowstheyhaverecognisedtheviolationofexpectations,in-
cludingthosethatarisefromnorms[13].Withoutarobustmechanismtodetect
normviolations,effectivenormenforcement(e.g.,bysanctioning)isnotpossible,
undermining the spreading and emergence of norms and reducing coordination
between agents.
Prior normative MAS research has a number of limitations. Norms are en-
coded and formalised in a variety of representations with no standard approach
emerging. We believe this slows progress in the field as advances made by one
researcher are not easily exploited by others. Also, norm representations and
reasoning mechanisms are often developed for and evaluated in simple static
problem domains. Furthermore they are predominantly based on the symbolic
AIparadigm,andmaythereforelackrobustnesswhendeployedindomainsout-
sidetheirinitialdesignconstraints[8].Thus,currenttechniquesmayhavelimited
applicability to the types of domain-independent sociotechnical system that are
enabled today by smartphones, social media networks and personal assistants
(whichusersarerapidlybecomingaccustomedtosincetheadventofChatGPT).
LargeLanguageModels(LLMs)offerpromiseinaddressingtheseissuessince
LLMs can work with norms expressed in natural language rather than commit-
tingtooneofthemanynormrepresentationsintheliterature.Recently,Natural
LanguageProcessing(NLP)researchershavestartedtoinvestigateLLMcapabil-
ities for normative reasoning. For example, pre-trained language models already
have been shown to possess some notion of norms, and they can be further
trained with publicly available norm-related datasets (discussed in Section 2.2
below). However, in that research, normative reasoning has been done on text
such as chatbot conversations, narratives or situation descriptions. In contrast,
in an MAS, an agent must deliberate on norm violation based on a sequence of
events it perceives from its environment. How an LLM would be capable of de-
terminingnormviolationsfromsuchaneventsequencehasnotbeeninvestigated
before. This work aims to bridge this gap.
In this work, we evaluated the norm identification capability of commonly
usedpublicandproprietaryLLMs.Wepromptedthemwithasequenceofevents
that occurred in a simulated environment related to a household setup, along
with a set of norms defined in that environment. The LLMs were asked to de-
termine whether each norm was violated or not, in a given scenario. Comparing
theseresultswithhuman-generatedanswerswefoundthatthecapabilitiesofdif-
ferentLLMsfornormviolationdetectionvaried,butthebestmodel,ChatGPT-
4,identifiednormviolationswithanaccuracyof86%.However,theperformance
depended on the type of norm.Norm Violation Detection using LLMs 3
The rest of the paper is organised as follows. Section 2 discusses prior work
relatedtonormviolationdetectioninMASandNLPresearch.Section3discusses
ourexperimentalsetupandSection4providestheresults.Weprovideadetailed
discussion of our results in Section 5. Finally, Section 6 concludes the paper.
2 Prior work
2.1 MAS Research on Norm Violation Detection
In the MAS literature, norms are expressed using a variety of notations such
asconditionalrules,temporallogicconstraintsandactiondescriptionlanguages
(such as the event calculus). A short survey is given by Cranefield et al. [3,
Sec. 2]. Various abstract models of norms have also been proposed that specify
separate components of a norm such as the type (obligation, etc.), triggering
condition, the normative constraint, expiration condition and sanction. There
are also a host of ad hoc domain-specific notations for norms.
Given this diversity, it is not surprising that many mechanisms for norm
violationdetectionhavebeenproposed.Theseincludethosethatarisenaturally
from the underlying formalisms such as forward-chaining rule engines, temporal
projection in the event calculus and temporal logic model checking as well as
the design of separate entities to monitor norms (see Dastani et al. for a survey
of the latter approach [4]).
2.2 NLP Research for Norm Violation Detection
In NLP, instead of norm violation detection, researchers focus on predicting
‘moral judgement’ in a given situation [6,19]. The common approach to train-
ing such a prediction system is to fine-tune a pre-trained language model with
situation description,moral judgementpairs.Here,themoral judgementrefers
to whether an action was normative or not. This kind of model has been com-
monly implemented as a text classification system, where a textual description
of an action gets classified as normative or not [9].
Morerecentworkhasproducedmodelsthatarecapableofmorefine-grained
reasoning. For example, NormBank [19] categorises a given situation as ok,
expected or unexpected. The publicly available Delphi system3 [6] responds with
one of the following judgements: rude, ok, bad, wrong, expected, shouldn’t, when
promptedwithasituation(expressedinasinglesentence).WhileDelphicarries
out normative reasoning in isolation with no reference to surrounding context,
ClarifyDelphi[12]iscapableofaskingquestionstoelicitsalientcontext.Whileall
thisresearchmadeuseofpre-trainedlanguagemodels,nonehasinvestigatedthe
capabilities of the recently released LLMs, which we are using in this research.
Moreover, this NLP research focused on norm violation present in textual de-
scriptions such as narratives or chatbot output. In contrast, we are focusing on
an event sequence received by an agent.
3 https://delphi.allenai.org/4 He et al.
2.3 LLM Agents
ThefieldofLLMagentsisarapidlyadvancingthreadofresearchthatconsiders
how intelligent software agents can leverage the abilities of LLMs to help them
make decisions. Recent survey papers by Wang et al. [15] and Xi et al. [16] give
an overview of this field. A key benefit is that an LLM can provide an agent
with the ability to operate in a wide range of environments unlike traditional
approaches to agent development that are usually restricted to specific problem
domains.
There has been prior research on how LLM-based agents can interact in
a society [16], including mechanisms that enable agents to exhibit “believable
individualandemergentsocialbehaviors”[11].However,priorworkhasnotcon-
sidered how LLMs could assist software agents with social reasoning, including
norm awareness and monitoring.
A companion paper [14] discusses the above points in more detail and pro-
posesanumberofpotentialpathstoextendingLLMagentstobecomenormative
LLM agents.
3 Methodology
3.1 Simulating an Agent Environment
We used the grammar of Nematzadeh et al. [10] to create stories that describe
asequenceofactionsperformedbyasetofagentsinapre-definedenvironment.
IthasbeenusedtotestTheoryofMind(ToM)capabilitiesofLLMs[10,1].This
grammar is defined in terms of a set of entities (i.e., objects that may have
properties, and agents) and a set of predicates. A predicate takes an entity as its
subject or object. A predicate can be an action that an agent carries out (e.g.
move, enter). Each story is generated in such a manner that it constitutes of a
setoftasks.Ineachtask,severalagents(usuallytwo)carryoutdifferentactions
such as entering or exiting rooms or moving things around. In addition to the
agent actions, the stories may contain random events that represent noise.
We introduced the following modifications to the implementation of Ne-
matzadeh et al. [10] to generate more complex scenarios:
1. In their work, tasks in a story are arranged one after the other, meaning
that the agent would perceive actions of one agent before perceiving the
other’s.However,thissimplescenarioisnotwhatwouldhappeninanagent
environment—an agent can perceive actions of different agents in an inter-
leaved fashion. In order to achieve this realistic scenario, we interchanged
actions of different agents using a topological sort algorithm [7].
2. Theirstorieshaveonlyonerandomevent(phone rang).Weaddedtwomore
events(kettle whistled,andcat meowed),andrandomlyaddedthemasnoise
to the generated stories.
3. We generate stories by varying the number of tasks from one to four. This
results in stories that have the number of events ranging from 6 to 30.Norm Violation Detection using LLMs 5
Alexander entered the sunroom.
Alexander moved the clock to the black suitcase.
Peter entered the sunroom.
Peter exited the sunroom.
Peter entered the study.
Peter exited the study.
Emily entered the sunroom.
Emily moved the fork to the blue bucket.
Emily exited the sunroom.
Phone rang.
Alexander exited the sunroom.
Emily entered the study.
Emily moved the tomato to the white crate.
Emily exited the study.
Peter entered the basement.
The hat is in the black bottle.
The fork is in the black suitcase.
Phone rang.
The clock is in the blue bucket.
Peter moved the hat to the red carpet.
Peter exited the basement.
Emily entered the sunroom.
Emily exited the sunroom.
Emily entered the basement.
The tomato is in the green refrigerator.
Emily exited the basement.
Fig.1. An example story used in our experiment
We used the same set of agents and objects that were defined in the original
implementation.Altogether,wegenerated80stories,whereevery20storieshada
differentnumberoftasks(i.e.,20storieseachforone,two,threeandfourtasks).
Figure 1 shows a sample story (with four tasks) generated from the modified
version of the implementation of Nematzadeh et al. [10]. Figure 2 shows the
number of occurrences of each event across the 80 stories. This figure confirms
that most stories are unique. To be specific, out of a total of 1317 events across
the 80 stories, 367 are unique events and only 219 are duplicated events.
3.2 Defining Norms
The list of objects and agents in the simulation suggest that Nematzadeh et
al. [10] have considered events taking place in a home. Therefore we defined 10
normsthatcanberealisticallyappliedinahouseholdsetup.Wedefinedahome
with four agents: mother (Emily) and father (Peter) with two young children
(Alexander, who is a 5-year-old boy and Ann, who is a 7-year-old girl). The set
of norms we used is shown in Figure 3. We defined two types of norms: generic
norms (6 norms: 1 to 6) and role-based norms (4 norms: 7 to 10). They are6 He et al.
Fig.2. Frequency distribution of events across the 80 stories
further divided into Obligation Norms (ON) and Prohibition Norms (PN) as
indicated at the end of each norm in Figure 3.
3.3 Prompting LLMs and Human Evaluation
We used ChatGPT-4 as our proprietary LLM and Llama 2 (Llama-2-7b-chat-
hf) and Mixtral (Mixtral-8x7B-Instruct-v0.1) as the open source LLMs. The
two open source models were selected considering their promising performance
despite the smaller model size. After a preliminary set of experiments and dis-
cussions had within the research team over several rounds, the prompt shown
in Figure 4 was used in the final experiments. The figure shows the ChatGPT-4
version. Prompts for other two models are more or less the same, except for
the model-specific tags. Each of the 80 stories we generated were provided as
prompts to each of the LLMs along with the ten norms investigated for each
story.
Wealsocarriedoutahumanevaluation,inordertoobtainthegroundtruth.
Thesame80storieswereprovidedtoexternalhumanevaluators,alongwiththe
scenariodescriptionandthesetofnorms.Foreachstoryandeachofthenorms,
theywereaskedtodecideamongthefollowingthreepossibilities:Normviolated,
norm not violated, or not enough information to give a judgement. Each story
was evaluated by three human annotators. Altogether we used nine annotators
for the evaluation. Six were Computer Science undergraduates and three were
recent Computer Science graduates. Two were females and the rest were males.Norm Violation Detection using LLMs 7
Generic norms:
1 - You should not enter an occupied bathroom. (PN)
2 -Vegetablesandfruits(exceptbananas)shouldbekeptintherefrigerator.(ON)
3 - Before using the staircase, you should wait until it is free. (ON)
4 - If sharp objects are present in the household, they should be kept out of reach
of children. (ON)
5 - If the phone rings, the person closest to the phone must pick it up. (ON)
6 -Thereshouldbenofruitsandvegetablescontainingcitricacidinsidethehouse.
(PN)
Role-based norms:
7 - Kids should not enter crawl spaces. (PN)
8 - You should not enter a couple’s bedroom without knocking. (PN)
9 - Kids should be supervised inside the workshop all the time. (ON)
10 - Father should not be disturbed while he is in his study room. (PN)
Fig.3. The norms used in our experiment
4 Results
From the human annotated data, we used majority voting to select the decision
corresponding to a norm with respect to a given story. We evaluated model per-
formance using human annotation as the ground truth. However, despite being
giventheoptiontodecidebetweenthreeoptions,yes/no/cannotbedetermined,
humans have always selected one of the two options: yes or no. Therefore, if a
model has the output ‘cannot be determined’, we map that output to ‘no’.
Table1showstheperformance(accuracy)ofeachLLMagainsthumanjudge-
ment. The result indicates that ChatGPT-4 has the overall best performance,
with 86%. Mixtral, the second best model, falls behind by a slight margin. The
result also indicates that the model performance is not consistent across norms,
i.e.,forsomenormsitperformsmuchbetter(e.g.,forMixtral,97.5%fornorm9
when compared to 28.75% for norm 2). Therefore we conducted further analysis
based on norm category (whether the norm is generic or role-based) and also
based on the type of the norm considered (whether the norm is a prohibition
normoranobligationnorm).Table2showsthatallthemodelsstruggletodetect
generic norms than role-based norms. Table 3 shows that all the three models
were able to detect prohibition norms better than obligation norms (82% vs.
68%).
We also calculated the disagreement between ground truth and each of the
modelsindeterminingnormviolation.Theresultfortheworst-performingmodel
(Llama 2) is shown in Figure 5 in the form of a confusion matrix. This model
seems to struggle in identifying norm violations, more than determining that
there is no violation. In other words, out of the instances where a human iden-
tified a norm violation, 81% of the time the model has not been able to identify8 He et al.
There is a family that adheres to certain norms, listed below from Norm 1 to 10. The family
consistsofEmily(wife),Peter(husband),andtheirchildren,Alexander(5-year-oldboy)andAnn
(7-year-oldgirl).
As a sophisticated assistant, you are tasked with analyzing a story related to this family to
identifyanyviolationsofthegivennorms.Yourgoalistosystematicallybreakdowntheanalysis
foreachnorm,ensuringathoroughevaluationbasedontheeventslistedinthestory.
The family’s story unfolds with the whole family initially being in the living room, alongside a
phone.
Thereisnootherpersoninthehouse.
Atthebeginningofthestory,nonormsareviolated.
Ifaneventisnotmentionedinthestory,thenitdidnotoccur.
Your job is to evaluate each event in the story against the below given norms to identify any
violations.
NormstoEvaluate:
{norms}
Belowisthestoryaboutthefamily’sevents.
Story:
{story}
Yourevaluationshouldconsidereachnormindividually.Foreachnorm,provideaclearresponse
inthefollowingformat:
-Norm1
-Violation:[Chooseoneofthreeoptionsgivenbelow]
-No:Thenormwasnotviolatedoritsirrelevancewasgiventhestory.
-Cannotbedetermined:Thereisn’tclearevidencetodeterminewhetherthenormwasviolated
ornot.
-Yes:Thenormwasviolated.
-Reasoning:[Explainyourrationalehere,referencingspecificeventsinthestoryasevidence.]
Pleaseproceedwithyouranalysis,ensuringathoughtfulandcomprehensiveexaminationofeach
normasitrelatestothestory’sevents.
Fig.4. Our prompt template for ChatGPT-4
this violation. On the contrary, when the human determines a non-violation,
the model’s decision differs only by 31.8%. Figure 6 shows the confusion matrix
for the best performing model ChatGPT-4. For this model, out of the instances
where a human identified a norm violation, only 51% of the time the model has
not been able to identify this violation. When the human determines a non-
violation, the model decision differs only by 4%. This further highlights the
superior results of ChatGPT-4.Norm Violation Detection using LLMs 9
Table 1.Overallaccuracy(inpercentages)forthethreemodelsforthe10norms(N1
to N10)
Model N1 N2 N3 N4 N5 N6 N7 N8 N9 N10Average
Llama2 43.75 21.25 92.50 37.5070.00 58.75 95.00 17.50 93.75 51.25 58.0
Mixtral 97.50 28.75 90.00 85.00 68.7593.7596.25 96.2597.50 51.25 81.0
ChatGPT-4100.0062.5096.2587.5070.00 92.50 93.75 96.25 93.75 71.25 86.0
Table 2. Accuracy results (in percentages) based on norm category
Norm categoryChatGPT-4MixtralLlama 2Average
Role-based norm 94.06 95.94 66.25 85.42
Generic norm 81.25 70.21 52.71 68.05
Table 3. Accuracy results (in percentages) based on norm type
Norm type ChatGPT-4MixtralLlama 2Average
Prohibition Norm (PN)90.25 87.25 68.50 82.00
Obligation Norm (ON) 82.50 73.75 47.75 68.00
5 Discussion
Wemakeseveralobservationsbasedontheresults.First,ChatGPT-4performed
betterthantheothermodelswithanaccuracyof86%.Thisresultisnotsurpris-
ing given that GPT Turbo, the model used in ChatGPT-4 tops the leaderboard
forgeneralreasoning[5].However,wenotethattheresultobtainedisfarfrombe-
ingperfect.Softwareagentsperformingactionsbasedontheresultevenfromthis
bestperformingLLMmightstillbeperformingnorm-violatingactionsbasedon
the advice received, and hence may be subject to sanctions. However, note that
in this work we simply prompted the LLMs in a zero-shot manner (i.e. without
showinganyexamplestotheLLM,eitherviain-contextlearningorfine-tuning).
Therefore,wesuggestthecreationoffine-tunedorRAG-basedmodelstofurther
improve norm detection ability of LLMs. Use of advanced reasoning techniques
such as Tree-of-Thought prompting [17] may further improve outcomes. More
broadly, norm competence of LLMs [14], of which norm violation detection is
only one aspect, should be improved.
Second, we observed that for a few norms all the LLMs performed poorly
(Llama27BandMixtral7B,moresothanChatGPT-4).Forexample,violations
of norm 2 about keeping vegetables and fruits in the refrigerator have been
identified poorly. Perhaps, it could be that this norm may have been in conflict
with common knowledge reasoning where only green vegetables are kept in the
refrigerator. In some cases, the relatively poor result of Llama 2 may have been
because of the non-availability of data. For example, norm 6 requires the ability
to determine whether a vegetable or fruit contains citric acid (e.g., beetroot in
onestory).However,itmaybedifficultfortheLLMtoidentifysuchinformation.
There are limited sources on the Internet that may have this information so it
may not be in the LLM’s training data. Also, as we did not provide a floor plan10 He et al.
Fig.5. Confusion Matrix for Llama 2 result against ground truth
ofthehousetotheLLM,itsperformancemayhavebeenimpacted.Forexample,
we noticed that in certain instances, Llama 2 made incorrect inferences related
to various rooms of the house (e.g. the kitchen) having crawlspaces.
Third,wefoundthatprohibitionnormviolationsweremoresuccessfullyiden-
tifiedbyLLMsthanobligationnormviolations.Thismaybebecausetheremay
be more instances of prohibition norms than obligation norms in the dataset to
pre-train the LLM, since prohibitions are more commonly stated than obliga-
tions.
Fourth, our work shows that certain LLMs were better at identifying non-
violation of norms in a story than identifying violations (e.g., for Llama 2 this
difference is about 50%). This may be because the dataset in which these LLMs
are pre-trained are not balanced (i.e., imbalance between violations and non-
violations). This suggests these models need may to be trained with more data
with violations.
Fifth, while most of the human-coded data contained only yes and no classi-
fications,indicatingviolationsandnon-violationsrespectively,twoofthemodels
(Mixtral and ChatGPT-4) contained results that were more nuanced. In many
cases ChatGPT-4 returned ‘cannot be determined’—an option that was avail-
abletothehumansalso,whichimpliesthereisnotenoughevidencetodetermine
whetheranormwasviolated.AlsoMixtralreturned‘notapplicable’(whichcould
mean a conditional norm was not triggered and therefore not violated). For ex-
ample,forthenormthatprohibitsthesecondpersonenteringthestaircasewhen
someoneisalreadyonit,ifthestorydoesnotinvolveastaircase,itreturnedtheNorm Violation Detection using LLMs 11
Fig.6. Confusion Matrix for ChatGPT-4 result against ground truth
result‘notapplicable’sincethestaircasewasnotmentioned.Thiswasasurpris-
ing result since we had provided only yes, no and cannot be determined as the
three options for the language model. In future work, we propose providing four
answer options: yes, no, not applicable and cannot be determined.
Sixth, we observed that the norm identification capability of an LLM de-
pendsontheprompt.Asmentionedearlier,wehadtoexperimentwithmultiple
promptsbeforereachingthemostpromisingprompt.Infact,promptbrittleness
has been highlighted as a limitation of LLMs [18].
Seventh, while this work considered both generic and role-specific norms,
these are relatively simple norms. In the future, we intend to experiment with
more complex norms where there are several conditions involving multiple enti-
ties.
Eighth,thisworkdoesnotproposehownormviolationdetectioninformation
is used by agents for decision-making (e.g., to punish other agents), which can
form the focus of future work.
Finally, this work is a pilot study on investigating normative reasoning fo-
cusing on one aspect, norm violation detection. We encourage other researchers
to pursue other aspects of normative reasoning as suggested in our companion
discussion paper [14].12 He et al.
6 Conclusion
This work aimed at investigating the effectiveness of LLMs in detecting norm
violationsbasedon80stories.OfthethreeLLMsconsideredinthework(Llama
2 7B, Mixtral 7B and ChatGPT-4), we observed that ChatGPT-4 offered the
most promise, with 86% accuracy. We also observed the LLMs detected viola-
tions of prohibition norms with higher accuracy than those of obligation norms.
Additionally,violationsofrole-basednormsweremoreaccuratelyidentifiedthan
those of generic norms. Also, LLMs were better at identifying when norms
weren’t violated than when they were violated. These results demonstrate that,
whileLLMsshowpromiseinnormviolationdetection,thereisscopeforcreating
better models for detecting norm violations. We believe such enhanced models
will be beneficial for recommending actions to be taken by an agent within a
multi-agent system, ¯ın order to create norm-conforming agent societies where
potential norm violations are effectively identified and punished.
7 Acknowledgements
We would like to thank the following individuals who volunteered for the hu-
manevaluation:DilanNayanajith,PaulJanson,SanjeepanSivapiran,Kumuthu
Athukorala, Yasith Heshan, Rumal Costa, Thevin Senath, Harshani Bandara
and Nimesh Ariyarathne.
References
1. Alechina, N., Halpern, J.Y., Kash, I.A., Logan, B.: Incentive-compatible mech-
anisms for norm monitoring in open multi-agent systems. Journal of Artificial
Intelligence Research 62, 433–458 (2018)
2. Bicchieri,C.,Muldoon,R.,Sontuoso,A.,etal.:Socialnorms.TheStanfordEncy-
clopedia of Philosophy (2014)
3. Cranefield,S.,Winikoff,M.,Vasconcelos,W.W.:Modellingandmonitoringinterde-
pendentexpectations.In:Cranefield,S.,vanRiemsdijk,M.B.,Va´zquez-Salceda,J.,
Noriega, P. (eds.) Coordination, Organizations, Institutions, and Norms in Agent
SystemsVII,pp.149–166.No.7254inLectureNotesinComputerScience,Springer
(2011). https://doi.org/10.1007/978-3-642-35545-5 9
4. Dastani, M., Torroni, P., Yorke-Smith, N.: Monitoring norms: a multi-
disciplinary perspective. The Knowledge Engineering Review 33, e25 (2018).
https://doi.org/10.1017/S0269888918000267
5. Fan,L.,Hua,W.,Li,L.,Ling,H.,Zhang,Y.,Hemphill,L.:NPHardEval:Dynamic
benchmark on reasoning ability of large language models via complexity classes.
arXiv preprint arXiv:2312.14890 (2023)
6. Jiang, L., Hwang, J.D., Bhagavatula, C., Bras, R.L., Liang, J., Dodge, J., Sak-
aguchi,K.,Forbes,M.,Borchardt,J.,Gabriel,S.,etal.:Canmachineslearnmoral-
ity? the Delphi experiment. arXiv preprint arXiv:2110.07574 (2021)
7. Kahn, A.: Topological sorting of large networks. Communications of the ACM 5
(1962)Norm Violation Detection using LLMs 13
8. Minsky, M.L.: Logical versus analogical or symbolic versus connec-
tionist or neat versus scruffy. AI Magazine 12(2), 34–51 (1991).
https://doi.org/10.1609/aimag.v12i2.894
9. Nahian, M.S.A., Frazier,S., Riedl, M., Harrison, B.: Learning norms from stories:
A prior for value aligned agents. In: Proceedings of the AAAI/ACM Conference
on AI, Ethics, and Society. pp. 124–130 (2020)
10. Nematzadeh,A.,Burns,K.,Grant,E.,Gopnik,A.,Griffiths,T.:Evaluatingtheory
ofmindinquestionanswering.In:Proceedingsofthe2018ConferenceonEmpirical
Methods in Natural Language Processing. pp. 2392–2400 (2018)
11. Park,J.S.,O’Brien,J.C.,Cai,C.J.,Morris,M.R.,Liang,P.,Bernstein,M.S.:Gen-
erative agents: Interactive simulacra of human behavior. In: 36th Annual ACM
Symposium on User Interface Software and Technology (UIST ’23). ACM (2023)
12. Pyatkin,V.,Hwang,J.D.,Srikumar,V.,Lu,X.,Jiang,L.,Choi,Y.,Bhagavatula,
C.: ClarifyDelphi: Reinforced clarification questions with defeasibility rewards for
social and moral situations. In: Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers). pp. 11253–
11271 (2023)
13. Salvador, C.E., Mu, Y., Gelfand, M.J., Kitayama, S.: When norm violations are
spontaneously detected: an electrocortical investigation. Social Cognitive and Af-
fective Neuroscience 15(3), 319–327 (2020)
14. Savarimuthu, B.T.R., Ranathunga, S., Cranefield, S.: Harnessing the power of
LLMs for normative reasoning in MASs. Discussion paper, University of Otago
(2024), http://hdl.handle.net/10523/16584
15. Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J.,
Chen, X., Lin, Y., Zhao, W.X., Wei, Z., Wen, J.R.: A survey on large language
model based autonomous agents. arXiv preprint arXiv:2308.11432 (2023)
16. Xi,Z.,Chen,W.,Guo,X.,He,W.,Ding,Y.,Hong,B.,Zhang,M.,Wang,J.,Jin,
S.,Zhou,E.,Zheng,R.,Fan,X.,Wang,X.,Xiong,L.,Zhou,Y.,Wang,W.,Jiang,
C., Zou, Y., Liu, X., Yin, Z., Dou, S., Weng, R., Cheng, W., Zhang, Q., Qin, W.,
Zheng, Y., Qiu, X., Huang, X., Gui, T.: The rise and potential of large language
model based agents: A survey. arXiv preprint arXiv:2309.07864 (2023)
17. Yao,S.,Yu,D.,Zhao,J.,Shafran,I.,Griffiths,T.,Cao,Y.,Narasimhan,K.:Tree
ofThoughts:Deliberateproblemsolvingwithlargelanguagemodels.Advancesin
Neural Information Processing Systems 36 (2024)
18. Zamfirescu-Pereira, J., Wong, R.Y., Hartmann, B., Yang, Q.: Why Johnny can’t
prompt:hownon-AIexpertstry(andfail)todesignLLMprompts.In:Proceedings
of the 2023 CHI Conference on Human Factors in Computing Systems. pp. 1–21
(2023)
19. Ziems, C., Dwivedi-Yu, J., Wang, Y.C., Halevy, A., Yang, D.: NormBank: A
knowledge bank of situational social norms. In: Proceedings of the 61st Annual
MeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers).
pp. 7756–7776 (2023)