“We Have No Idea How Models will Behave in Production
until Production”: How Engineers Operationalize Machine
Learning
SHREYASHANKAR∗,UniversityofCalifornia,Berkeley,USA
ROLANDOGARCIA∗,UniversityofCalifornia,Berkeley,USA
JOSEPHM.HELLERSTEIN,UniversityofCalifornia,Berkeley,USA
ADITYAG.PARAMESWARAN,UniversityofCalifornia,Berkeley,USA
Organizationsrelyonmachinelearningengineers(MLEs)todeploymodelsandmaintainMLpipelinesin
production.Duetomodels’extensiverelianceonfreshdata,theoperationalizationofmachinelearning,or
MLOps,requiresMLEstohaveproficiencyindatascienceandengineering.Whenconsideredholistically,
thejobseemsstaggering—howdoMLEsdoMLOps,andwhataretheirunaddressedchallenges?Toaddress
thesequestions,weconductedsemi-structuredethnographicinterviewswith18MLEsworkingonvarious
applications,includingchatbots,autonomousvehicles,andfinance.WefindthatMLEsengageinaworkflow
of (i) data preparation, (ii) experimentation, (iii) evaluation throughout a multi-staged deployment, and
(iv)continualmonitoringandresponse.Throughoutthisworkflow,MLEscollaborateextensivelywithdata
scientists,productstakeholders,andoneanother,supplementingroutineverbalexchangeswithcommunication
toolsrangingfromSlacktoorganization-wideticketingandreportingsystems.Weintroducethe3VsofMLOps:
velocity,visibility,andversioning—threevirtuesofsuccessfulMLdeploymentsthatMLEslearntobalanceand
growastheymature.Finally,wediscussdesignimplicationsandopportunitiesforfuturework.
CCSConcepts:•Human-centeredcomputing→Humancomputerinteraction(HCI);Userstudies.
AdditionalKeyWordsandPhrases:mlops,interviewstudy
ACMReferenceFormat:
ShreyaShankar,RolandoGarcia,JosephM.Hellerstein,andAdityaG.Parameswaran.2024.“WeHaveNoIdea
HowModelswillBehaveinProductionuntilProduction”:HowEngineersOperationalizeMachineLearning.
Proc.ACMHum.-Comput.Interact.8,CSCW1,Article206(April2024),34pages.https://doi.org/10.1145/
3653697
1 INTRODUCTION
Asmachinelearning(ML)modelsareincreasinglyincorporatedintosoftware,anascentsub-field
calledMLOps(shortforMLOperations)hasemergedtoorganizethe“setofpracticesthataim
to deploy and maintain ML models in production reliably and efficiently” [2, 104]. It is widely
recognizedthatMLOpsissuesposechallengestoorganizations.Anecdotalreportsclaimthat90%
ofMLmodelsdon’tmakeittoproduction[103];othersclaimthat85%ofMLprojectsfailtodeliver 206
value[94]—signalingthefactthattranslatingMLmodelstoproductionisdifficult.
∗Bothauthorscontributedequallytothisresearch.
Authors’addresses:ShreyaShankar,UniversityofCalifornia,Berkeley,Berkeley,CA,USA,shreyashankar@berkeley.
edu;RolandoGarcia,UniversityofCalifornia,Berkeley,Berkeley,CA,USA,rogarcia@berkeley.edu;JosephM.Heller-
stein,UniversityofCalifornia,Berkeley,Berkeley,CA,USA,hellerstein@berkeley.edu;AdityaG.Parameswaran,
UniversityofCalifornia,Berkeley,Berkeley,CA,USA,adityagp@berkeley.edu.
Permissiontomakedigitalorhardcopiesofpartorallofthisworkforpersonalorclassroomuseisgrantedwithoutfee
providedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeand
thefullcitationonthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored.Forallotheruses,
contacttheowner/author(s).
©2024Copyrightheldbytheowner/author(s).
2573-0142/2024/4-ART206
https://doi.org/10.1145/3653697
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.
4202
raM
52
]CH.sc[
1v59761.3042:viXra206:2 Shankar&Garciaetal.
Data
Evaluation&
Preparation Experimentation
Deployment
(onaschedule)
Monitoring&Response
Fig.1. CoretasksintheMLOpsworkflow.Priorworkdiscussesaproductiondatascienceworkflowof
preparation,modeling,anddeployment[102].Ourworkexposes(i)thescheduledandrecurringnature
of datapreparation(includingautomatedMLtasks,suchasmodelretraining),identifies(ii)abroader
experimentationstep(whichcouldincludemodelingoraddingnewfeatures),andprovidesmoreinsight
intohuman-centered(iii)evaluation&deployment,and(iv)monitoring&response.
Atthesametime,itisunclearwhyMLOpsissuesaredifficulttodealwith.Ourpresent-day
understandingofMLOpsislimitedtoafragmentedlandscapeofwhitepapers,anecdotes,and
thoughtpieces[18,21,24,61],aswellasacottageindustryofstartupsaimingtoaddressMLOps
issues[34].EarlyworkbySculleyetal.[87]attributesMLOpschallengestotechnicaldebt,analogous
tothatinsoftwareengineeringbutexacerbatedinML.Priorworkhasstudiedgeneralpracticesof
datascientistsworkingonML[37,66,85,110],butsuccessfulMLdeploymentsseemtofurther
involvea“teamofengineerswhospendasignificantportionoftheirtimeonthelessglamorous
aspectsofMLlikemaintainingandmonitoringMLpipelines”—thatis,MLengineers(MLEs)[75].
Itiswell-knownthatMLEstypicallyneedtohavestrongdatascienceandengineeringskills[3],
butitisunclearhowthoseskillsareusedintheirday-to-dayworkflows.
ThereisthusapressingneedtobringclaritytoMLOps—specificallyinidentifyingwhatMLOps
typicallyinvolves—acrossorganizationsandMLapplications.WhilepapersonMLOpshavede-
scribedspecificcasestudies,prescribedbestpractices,andsurveyedtoolstohelpautomatethe
ML lifecycle, there is a pressing need to understand the human-centered workflow required to
supportandsustainthedeploymentofMLmodelsinpractice.Aricherunderstandingofcommon
practicesandchallengesinMLOpscansurfacegapsinpresent-dayprocessesandbetterinform
thedevelopmentofnext-generationMLengineeringtools.Toaddressthisneed,weconducted
asemi-structuredinterviewstudyofMLengineers(MLEs),eachofwhomhasbeenresponsible
foraproductionMLmodel.Withtheintentofidentifyingcommonthemesacrossorganizations
andindustries,wesourced18MLengineersfromdifferentcompaniesandapplications,andasked
themopen-endedquestionstounderstandtheirworkflowandday-to-daychallenges—bothonan
individualandorganizationallevel.
Priorworkfocusingontheearlierstagesofdatasciencehasshownthatitisalargelyiterative
andmanualprocess,requiringhumanstoperformseveralstagesofdatacleaning,exploration,
modelbuilding,andvisualization[30,46,73,85].Beforeembarkingonourstudy,weexpected
thatthesubsequentdeploymentofMLmodelsinproductionwouldinsteadbemoreamenable
toautomation,withlessneedforhumaninterventionandsupervision.Ourinterviews,infact,
revealedtheopposite—muchliketheearlierstagesofdatascience,deployingandmaintaining
modelsinproductionishighlyiterative,manually-intensive,andteam-oriented.Ourinterviewees
emphasizedorganizationalandcollaborativestrategiestosustainMLpipelineperformanceand
minimizepipelinedowntime,mentioningon-callrotations,manualrulesandguardrails,andteams
ofpractitionersinspectingdataqualityalerts.
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.“Wehavenoideahowmodelswillbehaveinproductionuntilproduction”:HowengineersoperationalizeML 206:3
Inthispaper,weprovideinsightintohuman-centeredaspectsofMLOpspracticesandidentify
opportunitiesforfutureMLOpstools.Weconductasemi-structuredinterviewstudysolelyfocused
onMLengineers,anincreasinglyimportantpersonainthebroadersoftwaredevelopmentecosystem
as more applications leverage ML. Our focus on MLEs, and uncovering their workflows and
challengesaspartoftheMLOpsprocess,addressesagapintheliterature.Throughourinterviews,
we characterize an ML engineer’s typical workflow (on top of automated processes) into four
stages(Figure1):(i)datapreparation,(ii)experimentation,(iii)evaluationanddeployment,and(iv)
monitoringandresponse,allcenteredaroundteam-based,collaborativepractices.Keytakeaways
foreachstageareasfollows:
Dataingestionoftenrunsautomatically,butMLEsdrivedatapreparationthroughdata
selection,analysis,labeling,andvalidation(Section4.1).Wefindthatorganizationstypically
leverageteamsofdataengineerstomanagerecurringend-to-endexecutionsofdatapipelines,
allowingMLEstofocusonML-specificstepssuchasdefiningfeatures,aretrainingcadence,anda
labelingcadence.Ifaproblemcanbeautomatedaway,engineersprefertodoso—e.g.,retraining
modelsonaregularcadencetoprotectagainstchangesinthedistributionoffeaturesovertime.
Thus,theycanspendenergyontasksthatrequirehumaninput,suchassupervisingcrowdworkers
whoprovideinputlabelsorresolveinconcistenciesintheselabels.
Eveninproduction,experimentationishighlyiterativeandcollaborative,despitetheuse
ofmodeltrainingtoolsandinfrastructure(Section4.2).Asmentionedearlier,variousarticles
claimthatitisaproblemfor90%ofmodelstonevermakeittoproduction[103],butwefindthat
thisstatisticismisguided.Thenatureofconstantexperimentationisboundtocreatemanyversions
ofmodels,asmallfractionofwhich(i.e.“thebestofthebest”)willmakeittoproduction.MLEs
discussedexercisingjudgmentwhenchoosingnextexperimentstorun,andexpressedreservations
aboutAutoMLtools,or“keepingGPUswarm,”giventhevastsearchspace.MLEsconsultdomain
expertsandstakeholdersingroupmeetings,andprefertoiterateonthedata(e.g.,toidentifynew
featureideas)overinnovatingonmodelarchitectures.
Organizationsemployamulti-stagemodelevaluationanddeploymentprocess,soMLEs
manuallyreviewandauthorizedeploymenttosubsequentstages(Section4.3).Textbook
modelevaluation“bestpractices”donotdojusticetotherigorwithwhichorganizationsthink
aboutdeployments:theygenerallyfocusonusingonetypically-staticheld-outdatasetinanoffline
mannertoevaluatethemodelon[54]andasingleMLmetricchoice(e.g.,precision,recall)[68].We
findthatmanyMLEscarefullydeploychangestoincreasingfractionsofthepopulationinstages.
Ateachstage,MLEsseekfeedbackfromotherstakeholders(e.g.,productmanagersanddomain
experts)andinvestsignificantresourcesinmaintainingmultipleup-to-dateevaluationdatasets
andmetricsovertime—especiallytoensurethatdatasub-populationsofinterestareadequately
covered.
MLEs closely monitor deployed models and stand by, ready to respond to failures in
production(Section4.4).MLEsensuredthatdeploymentswerereliableviastrategiessuchason-
callrotations,datamonitoring,andelaboraterule-basedguardrailstoavoidincorrectoutputs.MLEs
discussedpainpointssuchasalertfatiguefromalertingsystemsandtheheadacheofmanaging
pipelinejungles[87],oramalgamationsofvariousfilters,checks,anddatatransformationsadded
toMLpipelinesovertime.
Therestofourpaperisorganizedasfollows:wecoverbackgroundandworkrelatedtoMLOps
fromtheCSCW,HCI,ML,softwareengineering,anddatasciencecommunities(Section2).Next,
wedescribethemethodsusedinourinterviewstudy(Section3).Then,wepresentourresultsand
discussourfindings,includingopportunitiesfornewtooling(Section4andSection5).Finally,we
concludewithpossibleareasforfuturework.
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.206:4 Shankar&Garciaetal.
2 RELATEDWORK
OurworkbuildsonpreviousstudiesofdataandMLpractitionersinindustry.Webeginwiththe
goalofcharacterizingtheroleofanMLEngineer,startingwithrelateddatasciencerolesinthe
literatureanddrawingdistinctionsthatmakeMLEsunique.Wethenreviewworkthatdiscusses
datascienceandMLworkflows,notspecifictoMLOps.Third,wecoverchallengesthatarisefrom
productionizingML.Fourth,wesurveysoftwareengineeringpracticesintheliteraturethattackle
such challenges. Finally, we review recent work that explicitly attempts to define and discuss
MLOpspractices.
2.1 CharacterizingtheMLEngineer
Datasciencerolesspanvariousengineeringandresearchtasks[40],andmanydata-relatedactivities
areperformedbypeoplewithout“data”or“ML”intheirjobtitle[41],soitcanbehardtoclearly
definejobdescriptions[66].Nonetheless,sincewefocusonproductionMLpipelines,wediscuss
personasrelatedtodatascience,ML,andengineering—culminatinginthedescriptionofthepersona
westudy.
TheDataScientist:Multiplestudieshaveidentifiedsubtypesofdatascientists,someofwhom
aremoreengineering-focusedthanothers[40,110].Zhangetal.[110]describethemanyroles
datascientistscantake—communicator,manager/executive,domainexpert,researcher/scientist,
andengineer/analyst/programmer.Theyfoundconsiderableoverlapinskillsandtasksperformed
betweenthe(i)engineer/analyst/programmerand(ii)researcher/scientistroles:botharehighly
technicalandcollaborateextensively.Separately,Kimetal.taxonomizeddatascientistsas:insight
providers,modelingspecialists,platformbuilders,polymaths,andteamleaders[40].Modeling
specialistsbuildpredictivemodelsusingML,andplatformbuildersbalancebothengineeringand
scienceastheyproducereusablesoftwareacrossproducts.
TheDataEngineer:Whiledatascientistsengageinactivitieslikeexploratorydataanalysis(EDA),
datawrangling,andinsightgeneration[37,106],dataengineersareresponsibleforbuildingrobust
pipelinesthatregularlytransformandpreparedata[51].Dataengineersoftenhaveasoftware
engineeringanddatasystemsbackground[40].Incontrast,datascientiststypicallyhavemodeling,
storytelling,andmathematicsbackgrounds[40,51].SinceproductionMLsystemsinvolvedata
pipelinesandMLmodelsinlargersoftwareservices,theyrequireacombinationofdataengineering,
datascience,andsoftwareengineeringskills.
TheMLEngineer(MLE):OurinterviewstudyfocusesonthedistinctMLEngineer(MLE)persona.
MLEshaveamultifacetedskillset:theyknowhowtotransformdataasinputstoMLpipelines,
trainMLmodels,servemodels,andwrapthesepipelinesinsoftwarerepositories[35,44,81].MLEs
needtoregularlyprocessdataatscale(muchlikedataengineers[44]),employingstatisticsandML
techniquesasdodatascientists[74],andareresponsibleforproductionartifactsasaresoftware
engineers[52].Unliketypicaldatascientistsanddataengineers,MLEsareresponsiblefordeploying
MLmodelsandmaintainingtheminproduction.
WeclassifyproductionMLintotwomodes.One,whichwecallsingle-useML,ismoreclient-
oriented,wherethefocusistogeneratepredictionsoncetomakeaspecificdata-informedbusiness
decision[46].Typically,thisinvolvesproducingreports,primarilyperformedbydatascientists[32,
41].Intheothermode,whichwecallrepeated-useML,predictionsarerepeatedlygenerated,often
asintermediatestepsindatapipelinesoraspartofML-poweredproducts,suchasvoiceassistants
andrecommendersystems[3,40].ContinuouslygeneratingMLpredictionsovertimerequires
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.“Wehavenoideahowmodelswillbehaveinproductionuntilproduction”:HowengineersoperationalizeML 206:5
moredataandsoftwareengineeringexpertise[41,99].Inourstudy,wefocusonMLEswhowork
onthelattermodeofproductionML.
2.2 MachineLearningWorkflows
Here,wecoverliteratureonMLpractitioners’workflows.Wediscussbothtechnicalandcollabora-
tiveworkflows,andthenwedescribetheworkflowourstudyseekstouncover.
Several studies have investigated aspects of the broader ML workflow, mostly in single-use
productionMLapplications.Earlystudiesondatascienceworkflowspointtoindustry-originated
softwaredevelopmentprocessmodels,suchastheAgileframework[13]andtheCRossIndustry
StandardProcessforDataMining(CRISP-DM)[10].Morerecently,Studeretal.[96]introduce
CRISP-ML,anewprocessmodelthataugmentsCRISP-DMwithafinal“monitoringandmainte-
nance”phasetosupportMLworkflows.Mulleretal.[66]interviewpractitionersandfocusonthe
datapracticesofdatascienceworkflows,breakingthemdownintodiscovery,capture,design,and
curation.Wongsuphasawatetal.[106]’sworkflowincludessomeML:itconsistsofdataacquisition,
wrangling,exploration,modeling,andreporting.Wangetal.[102]takesanotherstepbackand
includesproductionization;theyidentifythreehigh-levelphasesofpreparation,modeling,and
deployment.Preparationincludesactivitiesrangingfromwrangling[36]tofeatureengineering[74].
Modelingincludesselection,hyperparameteroptimization,ensembling,andvalidation,anddeploy-
mentincludesmonitoringandimprovement[102,110].Ofthethreelargestages,severalstudies
haveidentifiedpreparationasthemosttime-intensivestageoftheworkflow[27,66],wheredata
scientistscommonlyiterateonrulestohelpgeneratefeatures[30,72].
Whiletheabovestagesofthedatascienceworkflowcomprisealoopoftechnicaltasks,Krossand
Guo[46]identifyanouterloopofdatascience,centeredaroundcollaborativepractices.Theouter
loopconsistsofgroundwork(i.e.,buildingtrust),orienting,problemframing,magic(i.e.,technical
loop),andcounseling.WhileKrossandGuo[46]’sloopfocusesondatascienceworkthatdirectly
interactswithclients,mostlyintheformofsingle-useML,similarthemesemergewhenperforming
repeated-useMLengineeringwork,e.g.,repeatedlygeneratingMLpredictionsinanautomated
fashion.Inproductionsettings,predictionsmustyieldvalueforthebusiness[74],requiringsome
groundwork, orienting, and problem framing. In their paper on tensions around collaborative,
applied data science work, Passi and Jackson [73] discuss that it’s important to align different
stakeholdersonsystemperformancemetrics:forexample,oneoftheirintervieweesmentioned
thataccuracyisa“problematic”metricbecausedifferentusersinterpretitdifferently.Inanother
example,Holsteinetal.[31]saythatasingleglobalmetricdoesn’tcaptureperformanceforcertain
groupsofusers(e.g.,accuracyforasubgroupmightdecreasewhenoverallaccuracyincreases).
Inourstudy,wecharacterizetheworkflowfromarepeated-useMLengineeringperspective,
focusingonspecificpracticeswithindeploymentstages.Somerelatedworkdefinesstepsinthe
MLworkflow,suchasmodeltrainingandmodelmonitoring,throughbothshortpapers[59]and
extensiveliteraturereviews[48].Wetakeadifferentbutcomplementaryapproach:likeMuller
etal.[66]whofocusondatascientists,weconductaninterviewstudyofMLEs,usinggrounded
theorytoanalyzeourfindings[95].Further,ourstudyseekstouncovercollaborativepracticesand
challenges,focusingontheMLengineeringperspective,andhowMLEsalignallstakeholderssuch
thatMLsystemscontinuallygeneratevalue.
2.3 ProductionMLChallenges
Sculleyetal.[87]wereearlyproponentsthatproductionMLsystemsraisespecialchallengesand
canbehardtomaintainovertime,basedontheirexperienceatGoogle.Theycoinedthe“Changing
AnythingChangesEverything”(CACE)principle:ifonemakesaseeminglyinnocuouschangeto
anMLsystem,suchaschangingthedefaultvalueforafeaturefrom0to-1,theentiresystem’s
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.206:6 Shankar&Garciaetal.
behaviorcandrasticallychange.CACEeasilycreatestechnicaldebtandisoftenexacerbatedas
errors“cascade,”orcompound,throughoutanend-to-endpipeline[71,75,76,85,87].Wecover
threewell-studiedchallengesinproductionML:dataquality,reproducibility,andspecificity.
First,MLpredictionsareonlyasgoodastheirinputdata[75,76],requiringactiveeffortsfrom
practitionerstoensuregooddataquality[8].Xinetal.[107]observethatproductionMLpipelines
consistofmodelsthatareautomaticallyretrained,andwefindthatthisretrainingprocedureisa
painpointforpractitionersbecauseitrequiresconstantmonitoringofdata.Ifamodelisretrained
on bad data, all future predictions will be unreliable. Data distribution shift is another known
data-relatedchallengeforproductionMLsystems[63,69,78,97,105],andourworkbuildsontop
oftheliteraturebyreportingonhowpractitionerstackleshiftissues.
Next,reproducibilityindatascienceworkflowsisawell-understoodchallenge,withattempts
topartiallyaddressit[9,16,33,43].Recentworkalsoindicatesthatreproducibilityisanongoing
issue in data science and ML pipelines [4, 39, 47, 84]. Kross and Guo [47] mention that data
scienceeducatorswhocomefromindustryspecificallywantstudentstolearnhowtowrite“robust
andreproduciblescientificcode.”Ininterviewstudies,Xinetal.[108]observetheimportanceof
reproducibilityinAutoMLworkflows,andSambasivanetal.[85]mentionthatpractitionerswho
createreproducibledataassetsavoidsomeerrors.
Finally,otherrelatedworkhasidentifiedthatproductionMLchallengescanbespecifictothe
MLapplicationathand.Forexample,Sambasivanetal.[85]discusseshow,inhigh-stakesdomains
likeautonomousvehicles,dataqualityisextraimportantandexplicitlyrequirescollaborationwith
domainexperts.Theyexplainhowdataerrorscompoundandhavedisastrousimpacts,especially
inresource-constrainedsettings.Unlikethepresentstudy,theirfocusisondataqualityissues
asopposedtounderstandingtypicalMLEworkflowsandchallenges.Paleyesetal.[71]review
publishedreportsofindividualMLdeploymentsandmentionthatnotallMLapplicationscanbe
easilytestedpriortodeployment.Whileadrecommendersystemsmightbeeasilytestedonline
withasmallfractionofusers,otherapplicationsrequiresignificantsimulationtestingdepending
onsafety,security,andscaleissues[49,70].CommonapplicationsofML,suchasmedicine[77],
customerservice[19],andinterviewprocessing[6],havetheirownstudies.Ourworkexpandson
theliteraturebyidentifyingcommonchallengesacrossvariousapplicationsandreportingonhow
MLEshandlethem.
2.4 SoftwareEngineeringforML
Throughinterviewsandpractitionersurveys,somepapersexplore,atahighlevel,howMLengi-
neeringpracticesdifferfromtraditionalsoftwareengineeringpractices.Hilletal.[29]interview
MLapplicationdevelopersandreportchallengesrelatedtobuildingfirstversionsofMLmodels,
especiallyaroundtheearlystagesofexplorationandexperimentation(e.g.,featureengineering,
modeltraining).Theydescribetheprocessofbuildingmodelsas“magic”—similarlyechoedbyLee
etal.[50]whenanalyzingMLprojectsfromGithub—withuniquepracticesofdebuggingdatain
additiontocode.Serbanetal.[88]conductasurveyofpractitionersandlist29softwareengineer-
ingpracticesforML,suchas“UseContinuousIntegration”and“PeerReviewTrainingScripts.”
Muirurietal.[64]interviewFinnishengineersandinvestigatetechnicalchallengesandML-specific
toolsintheMLlifecycle.Amershietal.[3]identifychallengessuchashiddenfeedbackloopsand
componententanglementthroughtheirinterviewswithscientists,engineers,andmanagersat
Microsoft.TheybroadlydiscussstrategiestointegratesupportforMLdevelopmentintotraditional
softwareinfrastructure,suchasend-to-endpipelinesupportfromdataengineersandeducational
conferences for employees. Our work expands on the software engineering for ML ecosystem
byconsideringhuman-centered,operationalrequirementsforMLdeployments,e.g.,overtime,
asMLEsareintroducedtoMLpipelinesthatareunfamiliartothem,orascustomerorproduct
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.“Wehavenoideahowmodelswillbehaveinproductionuntilproduction”:HowengineersoperationalizeML 206:7
requirementschange.UnlikeAmershietal.,wefocusonMLEs,whoareresponsibleformaintaining
MLpipelineperformance.Wealsointerviewpractitionersacrosscompaniesandapplications:we
providenewandspecificexamplesofMLengineeringpracticestosustainMLpipelinesassoftware
andcategorizethesepracticesaroundabroaderhuman-centeredworkflow.
Thedatamanagement,softwareengineering,andCSCWcommunitieshaveproposedvarious
softwaretoolsforMLworkflows.Forexample,sometoolsmanagedataprovenanceandtraining
contextformodeldebuggingpurposes[7,20,28,67].Othershelpensurereproducibilitywhile
iteratingondifferentideas[30,39,90].Withregardstovalidatingchangesinproductionsystems,
some researchers have studied CI (Continuous Integration) for ML and proposed preliminary
solutions—for example, ease.ml/ci streamlines data management and proposes unit tests for
overfitting[1],andsomepapersintroducetoolstoperformvalidationandmonitoringinproduction
MLpipelines[8,38,86].Ourworkiscomplementarytoexistingliteratureonthistooling;wedo
notexplicitlyaskintervieweesquestionsabouttools,nordoweproposeanytools.Wefocuson
behavioralpracticesofMLEs.
2.5 MLOpsPracticesandChallenges
ThetraditionalsoftwareengineeringliteraturedescribestheneedforDevOps,acombination
of software developers and operations teams, to streamline the process of delivering software
in organizations [17,52,55,56]. Similarly, the field of MLOps, or DevOps principles applied to
machinelearning,hasemergedfromtheriseofmachinelearning(ML)applicationdevelopment
insoftwareorganizations.MLOpsisanascentfield,wheremostexistingpapersgivedefinitions
andoverviewsofMLOps,aswellasitsrelationtoML,softwareengineering,DevOps,anddata
engineering [22, 35, 44, 58, 81, 89, 91, 98–100]. Some work in MLOps attempts to characterize
a production ML lifecycle; however, there is little consensus. Symeonidis et al. [98] discuss a
lifecycleofdatapreparation,modelselection,andmodelproductionization,butotherliterature
reviews[22,53]andguidesonbestpracticesdrawingfromauthors’experiences[59]concludethat,
comparedtosoftwareengineering,thereisnotyetastandardMLlifecycle,withconsensusfrom
researchersandindustryprofessionals.WhilestandardizinganMLlifecycleacrossdifferentroles
(e.g.,scientists,researchers,businessleaders,engineers)mightbechallenging,characterizinga
workflowspecifictoacertainrolecouldbemoretractable.
SeveralMLOpspaperspresentcasestudiesofproductionizingMLwithinspecificorganizations
andtheresultingchallenges.Forexample,adheringtodatagovernancestandardsandregulation
isdifficult,asmodeltrainingisdata-hungrybynature[5,25].Gargetal.[22]discussissuesin
continuous end-to-end testing (i.e., continuous integration) because ML development involves
changestodatasetsandmodelparametersinadditiontocode.Toaddresssuchchallenges,other
MLOpspapershavesurveyedtheproliferatingnumberofindustry-originatedtoolsinMLOps[53,
80,83,98].MLOpstoolscanhelpwithgeneralpipelinemanagement,datamanagement,andmodel
management[80].ThesurveysontoolsmotivateunderstandinghowMLEsusesuchtools,toseeif
thereareanygapsoropportunitiesforimprovement.
Priorworkinthisarea—primarilylimitedtoliteraturereviews,surveys,casestudies,andvision
papers—motivatesresearchinunderstandingthehuman-centeredworkflowsandpainpointsin
MLOps. Some MLOps work has interviewed people involved in the production ML lifecycle:
for example, Kreuzberger et al. [44] conduct semi-structured interviews with 8 experts from
differentindustriesspanningdifferentroles,suchasAIarchitectandSeniorDataPlatformEngineer,
anduncoveralistofMLOpsprinciplessuchasCI/CDautomation,workfloworchestration,and
reproducibility,aswellasanorganizationalworkflowofproductinitiation,featureengineering,
experimentation,andautomatedMLworkflowpipeline.WhileKreuzbergeretal.[44]explicitly
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.206:8 Shankar&Garciaetal.
RR Id Role OrgSize Application YrsXp Site Highlights
1 Lg1 MLEMgr. Large Autonomousvehicles 5-10 US-West highvelocityexperimentation;scenariotesting
1 Md1 MLE Medium Autonomousvehicles 5-10 US-West pipeline-on-a-schedule;copy-pasteanomalies
1 Sm1 MLE Small Computerhardware 10-15 US-West exploratorydataanalysis;ABTesting;SLOs
1 Md2 MLE Medium Retail 5-10 US-East retrainingcadence;adaptivetestdata;feedbackdelay
1 Lg2 MLEMgr. Large Ads 5-10 US-West adclickcount;modelownership;keepingGPUswarm
1 Lg3 MLE Large Cloudcomputing 10-15 US-West bucketing/binning;SLOs;hourlybatchedpredictions
2 Sm2 MLE Small Finance 5-10 US-West F1-score;retrainingcadence;progressivevalidation
2 Sm3 MLE Small NLP 10-15 Intl triagequeue;fallbackmodels;false-positiverate
2 Sm4 MLE Small OCR+NLP 5-10 Intl humanannotators;word2vec;airflow
3 Md3 MLEMgr. Medium Banking 10-15 US-West humanannotators;institutionalknowledge;revenue
3 Lg4 MLE Large Cloudcomputing 10-15 US-West onlineinference;pipeline-on-a-schedule;fallbackmodels
3 Sm5 MLE Small Bioinformatics 5-10 US-West modelfine-tuning;someoneelse’sfeatures
4 Md4 MLE Medium Cybersecurity 10-15 US-East model-per-customer;joinpredictionsw/groundtruth
4 Md5 MLE Medium Fintech 10-15 US-West retrainingcadence;droppedspecialcharacters
5 Sm6 MLE Small Marketingandanalytics 5-10 US-East humanannotators;labelquality;adaptivetestdata
5 Md6 MLE Medium Websitebuilder 5-10 US-East SLOs;poordocumentation;datavalidation
6 Lg5 MLE Large Recommendersystems 10-15 US-West pipeline-on-a-schedule;SLOs;progressivevalidation
6 Lg6 MLEMgr. Large Ads 10-15 US-West fallbackmodels;on-callrotations;scale
Table1. Thetableprovidesananonymizeddescriptionofintervieweesfromdifferentsizesofcompanies,roles,
yearsofexperience,applicationareas,andtheircodeattributions.Theintervieweeshailfromadiversesetof
backgrounds.Smallcompanieshavefewerthan100employees;medium-sizedcompanieshave100-1000
employees,andlargecompanieshave1000ormoreemployees.RRdenotesrecruitmentround.Highlights
referstokeycodes(i.e.fromthecodesysteminFig.3)attachedtothatparticipant’stranscript.
interviewprofessionalsfromdifferentrolestounderstandsharedpatternsbetweentheirworkflows—
infact,onlytwooftheeightintervieweeshave“machinelearningengineer”or“deeplearning
engineer”intheirjobtitles,ourworkcomplementstheirfindingsbyfocusingonlyonself-declared
MLengineersresponsibleforrepeated-usemodelsinproductionanduncoveringstrategiestheyuse
tosustainmodelperformance.Assuch,weuncoverandpresentadifferentworkflow—onecentered
aroundMLengineering.Tothebestofourknowledge,wearethefirsttostudythehuman-centered
MLOpsworkflowfromMLengineers’perspectives.
3 METHODS
Uponsecuringapprovalfromourinstitution’sreviewboard,weconductedaninterviewstudyof
18MLEngineers(MLEs)acrossvarioussectors.Ourapproachmirroredazigzagmodelcommonto
GroundedTheory,withalternatingphasesoffieldworkandin-depthcodingandanalysis,directing
further rounds of interviews [15]. The constant comparative method helped iterate and refine
ourcategoriesandoverarchingtheory.Consistentwithqualitativeresearchstandards,theoretical
saturationisgenerallyrecognizedbetween12to30participants,particularlyinmoreuniform
populations[26].Byour16thinterview,prevalentthemesemerged,signalingthatsaturationwas
attained.Laterinterviewsconfirmedthesethemes.
Our goal in conducting this study was to develop better tools for ML deployment, broadly
targetingmonitoring,debugging,andobservabilityissues.Ourstudywasanattemptatidentifying
keychallengesandopenopportunitiesintheMLOpsspace.Thisstudythereforestemsfromour
collectivedesiretoenrichourunderstandingofourtargetcommunityandoffervaluableinsights
intobestpracticesinMLengineeringanddatascience.Subsequentsectionsdelveintoparticipant
recruitment(Subsection3.1),ourinterviewstructure(Subsection3.2),andourcodingandanalysis
techniques(Subsection3.3).
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.“Wehavenoideahowmodelswillbehaveinproductionuntilproduction”:HowengineersoperationalizeML 206:9
3.1 ParticipantRecruitment&Selection
Werecruitedindividualswhowereresponsibleforthedevelopment,regularretraining,monitoring
anddeploymentofMLmodelsinproduction.Adescriptionofthe18MLEsisshowninTable1.
TheMLEsinterviewedvariedintheireducationalbackgrounds,yearsofexperience,roles,team
size,andworksector.Recruitmentwasconductedinroundsoverthecourseofanacademicyear
(2021-2022). Ourrecruitmentstrategywasunderpinnedbyadeliberate,iterativeprocessthatbuilt
upontheinsightsfromeachround.Theprimarygoalwastocultivatearepresentativesamplethat
capturedtherichdiversityofMachineLearningEngineers(MLEs)acrossvariousdimensions.
3.1.1 InitialRecruitment:RelyingonProfessionalNetworks. Intheinitialrecruitmentround(RR=1),
weleanedheavilyontheestablishedprofessionalnetworksofourfacultyco-authors.Thisapproach,
whileconvenientandefficient,resultedinasamplethatwasgeographicallyskewedtowardsthe
US-West.Italsoledtoagreaterrepresentationfromlargerorganizations,aswellascertainsectors
likeAutonomousVehiclesandCloudComputing.Thisinitialcohortprovidedvaluableinsightsbut
alsohighlightedthepotentialbiasesandgapsinoursample.
3.1.2 CourseCorrection:DiversifyingtheSample. Recognizingtheneedforamorerepresentative
anddiversifiedsample,ourstrategyinsubsequentroundsshifted.SpecificallyforRR=2,wemadea
concertedefforttoengagecandidatesoutsideourimmediateprofessionalnetworksandparticu-
larlytargetedthoseatsmallercompanies.Thisshiftinapproachwasoperationalizedbyposting
recruitmentadvertisementsandflyersinvariousonlinecommunities.Prospectiveparticipants
whorespondedtoouroutreachunderwentascreeningprocessforthesameinclusioncriteriamen-
tionedpreviously.Theirprofessionalbackgroundsandaffiliationswereverifiedthroughplatforms
suchasprofessionalwebsites,LinkedIn,andonlineportfolios.Asaresult,weobservedastronger
representationfromdomainslikeNLPandFinance.
3.1.3 BuildingaRepresentativeSample:IterativeRefinement. Eachrecruitmentroundservedasa
feedbackloop,informingthestrategyforthesubsequentround.Aspatternsemergedfromourdata
analysis,wefine-tunedourrecruitmentfocustofillidentifiedgaps.Thisiterativeprocessensured
that,overtime,oursamplegrewtobemorebalancedintermsofroles,experience,organization
sizes,sectors,andgeographicallocations.Byemployingthisiterativerecruitmentstrategy,we
believeourstudyencapsulatesacomprehensivecross-sectionoftheMLEcommunity,offering
insightsthatarebothdeepandbroad.
Ineachround,betweenthreetofivecandidateswerereachedbyemailandinvitedtoparticipate.
WereliedonourprofessionalnetworksandopencallspostedonMLOpschannelsinDiscord1,
Slack2,andTwittertocompilearosterofcandidates.Therosterwasincrementallyupdatedroughly
aftereveryroundofinterviews,integratinginformationgainedfromtheconcurrentcodingand
analysisoftranscripts(Section3.3).Recruitmentroundswererepeateduntilwereachedsaturation
onourfindings[65].
3.2 InterviewProtocol
With each participant, we conducted semi-structured interviews over video call lasting 45 to
75minuteseach.Overthecourseoftheinterview,weaskeddescriptive,structural,andcontrast
questionsabidingbyethnographicinterviewguidelines[92].ThequestionsarelistedinAppendixA.
Specifically,ourquestionsspannedsixcategories:i)thetypeofMLtaskstheyworkon,ii)the
approachesusedforbuildingortuningmodels,iii)thetransitionfromdevelopmenttoproduction,
iv)howtheyevaluatetheirmodelsbeforedeployment,v)howtheymonitortheirdeployedmodels,
1https://discord.com/invite/Mw77HPrgjF
2mlops-community.slack.com
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.206:10 Shankar&Garciaetal.
(a)Color-codedOverviewofTranscripts (b)ColorLegend
Fig.2. Visualsummaryofcodedtranscripts.Thex-axisof(a),thecolor-codedoverview,correspondstoa
segment(orgroup)oftranscriptlines,andthenumberineachcellisthecode’sfrequencyforthattranscript
segmentandforthatparticipant.Segmentsareblankaftertheconclusionofeachinterview,anddifferent
interviewshaddifferenttimeduration.Eachcolorin(a)isassociatedwithatop-levelaxialcodefromour
interviewstudy,andpresentedinthecolorlegend(b).Thecolorlegendalsoshowsthefrequencyofeach
codeacrossallinterviews.
and vi) how they respond to issues or bugs. Participants received and signed a consent form
beforetheinterview,andagreedtoparticipatefreeofcompensation.Asperouragreement,we
automatically transcribed the interviews using Zoom software. In the interest of privacy and
confidentiality,wedidnotrecordaudioorvideooftheinterviews.Transcriptswereredactedof
personallyidentifiableinformationbeforebeinguploadedtoasecureddriveinthecloud.
3.3 TranscriptCoding&Analysis
Weemployedgroundedtheorytosystematicallyanalyzeourinterviewtranscripts.Groundedtheory
isarobustmethodologyfocusedoniterativedatacollectionandanalysisfortheorydiscovery[12,
95].Oneofitskeyfeaturesistheseamlessintegrationofdatacollectionandanalysis,aimingto
identifyemergingthemesandconceptsthroughaconstantreviewoftranscripts.Inemploying
groundedtheory,wefolloweditskeyprocesses:open,axial,andselectivecoding.Duringopen
coding, the initial phase of categorizing data, we deconstructed our interview transcripts into
discreteideasorphenomenaandassignedcodestotheseideas(e.g.,A/Btesting).Then,inaxial
coding,wherethegoalistoidentifypatternsandrelationshipsbetweendifferentconcepts,we
mergedduplicatecodesanddrewedgesbetweensimilarcodes.Forexample,wegroupedthecodes
scenariotestingandA/Btestingunderthebroadertestingcode.Finally,throughselectivecoding,we
distilledourcodesintofivecorethemesthatrepresenttheessenceofourtranscripts.Figure3shows
ourhierarchyofcodes,withcorethemessuchasTasks,Operations,andSystems&Tools.As
illustratedinFigure2,weallocatedroughlyequaltimetoeachmaintheme,whichcorrespondingly
informedourfindings.Thethemesrelatetoourfindingsasfollows:
(1) TasksreferstoactivitiesthatareroutinelyperformedbyMLengineers.Theanalysisofcode
segmentsdescendedfromtasks,anditsdecompositionintoconstituentparts,culminated
inthecreationoftheMLOpsworkflow(Figure1),andisinstrumentalinthestructureand
presentationofSection4(Findings).
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.“Wehavenoideahowmodelswillbehaveinproductionuntilproduction”:HowengineersoperationalizeML 206:11
(1) Tasks
(a) Datacollection,cleaning&labeling:humanannotators,exploratorydataanalysis
(b) Embeddings&featureengineering:normalization,bucketing/binning,word2vec
(c) Datamodeling&experimentation:accuracy,F1-score,precision,recall
(d) Testing:scenariotesting,ABtesting,adaptivetest-data
(2) Biz/OrgManagement
(a) Businessfocus:servicelevelobjectives,adclickcount,revenue
(b) Teams&collaboration:institutionalknowledge,on-callrotations,modelownership
(c) Processmaturityindicators:pipeline-on-a-schedule,fallbackmodels,model-per-customer
(3) Operations
(a) CI/CD:artifactversioning,multi-stageddeployment,progressivevalidation
(b) Dataingestion&integration:automatedfeaturization,datavalidation
(c) Modelretraining:distributedtraining,retrainingcadence,modelfine-tuning
(d) Predictionserving:hourlybatchedpredictions,onlineinference
(e) Livemonitoring:false-positiverate,joinpredictionsw/groundtruth
(4) Bugs&PainPoints
(a) Bugs:dataleakage,droppedspecialcharacters,copy-pasteanomalies
(b) Painpoints:bigorgredtape,performanceregressions,labelquality,scale
(c) Anti-patterns:mutingalerts,keepingGPUswarm,waitingitout
(d) Knownchallenges:datadrift,feedbackdelay,classimbalance,sensordivergence
(e) Missingcontext:someoneelse’sfeatures,poordocumentation,muchtimehaspassed
(5) Systems&Tools
(a) Metadatalayer:Huggingface,Weights&Biases,MLFlow,TensorBoard,DVC
(b) Unitlayer:PyTorch,TensorFlow,Jupyter,Spark
(c) Pipelinelayer:Airflow,Kubeflow,Papermill,DBT,VertexAI
(d) Infrastructurelayer:Slurm,S3,EC2,GCP,HDFS
Fig. 3. Abridged code system: A distilled representation of the evolved code system resulting from our
qualitativestudy,capturingtheprimarytasks,organizationalaspects,operationalmethodologies,challenges,
andtoolsutilizedbyMachineLearningEngineers.
(2) Biz/Org(Business-Organizational)Managementreferstomodesofinteractionbetween
MLEsandtheirco-workersormanagers,andbetweenMLEsandcustomersorotherstake-
holders.Relevantsub-codesformthetheoreticalbasisforSection4.2.2(Collaboration)and
Section4.3.2(ProductMetrics).
(3) Operationsreferstorepeatableworkthatmustbeperformedregularlyandconsistently
for the continued operation of the business. Operations is the “Ops” in MLOps. Relevant
sub-codesformthetheoreticalbasisforSection4.1.1(PipelineAutomation).
(4) Bugs&PainPointsreferstofailuremodesencounteredatanystageintheMLOpsworkflows,
MLEcomplaintsgenerally,andauthor-notedanti-patterns.ThesearediscussedinMonitoring
andResponse(Section4.4).
(5) Systems&Toolsreferstostorageandcomputeinfrastructure,programminglanguages,
MLtrainingframeworks,experimentexecutionframeworks,andothertoolsorsystemsthat
MLEsuseintheirMLOpswork.WediscussimplicationsfortooldesigninSection5.2.We
includeatableofcommontoolsreferencedbyintervieweesinAppendixA.
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.206:12 Shankar&Garciaetal.
4 SUMMARYOFFINDINGS
Goingintotheinterviewstudy,weassumedtheworkflowofhuman-centeredtasksintheproduction
MLlifecyclewassimilartotheproductiondatascienceworkflowpresentedbyWangetal.[102],
whichisaloopconsistingofthefollowing:
(1) Preparation,spanningdataacquisition,datacleaningandlabeling,andfeatureengineering,
(2) Modeling,spanningmodelselection,hyperparametertuning,andmodelvalidation,and
(3) Deployment,spanningmodeldeployment,runtimemonitoring,andmodelimprovement.
Fromourinterviews,wefoundthattherepeated-useproductionMLworkflowthatMLengineers
engage in differs slightly from Wang et al. [102]. As preliminary research papers defining and
providingreferencearchitecturesforMLOpshavepointedout,operationalizingMLbringsnew
requirementstothetable,suchastheneedforteams,notjustindividualpeople,tounderstand,
sustain,andimproveMLpipelinesandsystemsovertime[22,44,98].Inthepipelinesthatour
intervieweesbuildandsupervise,mosttechnicalcomponentsareautomated—e.g.,datapreprocess-
ingjobsrunonaschedule,andmodelsaretypicallyretrainedregularlyonfreshdata.Wefound
theMLengineeringworkflowtorevolvearoundthefollowingstages(Figure1):
(1) DataPreparation,whichincludesscheduleddataacquisition,cleaning,labeling,andtrans-
formation,
(2) Experimentation,whichincludesbothdata-drivenandmodel-drivenchangestoincrease
overallMLperformance,andistypicallymeasuredbymetricssuchasaccuracyormean-
squared-error,
(3) Evaluation&Deployment,whichconsistsofamodelretrainingloopwhichperiodically
rollsoutanewmodel—orofflinebatchedpredictions,oranotherproposedchange—togrowing
fractionsofthepopulation,and
(4) Monitoring&Response,whichsupportstheotherstagesviadataandcodeinstrumentation
(e.g.,trackingavailableGPUsforexperimentationorthefractionofnull-valueddatapoints)
anddispatchesengineersandbugfixestoidentifiedproblemsinthepipeline.
Foreachstage,weidentifiedhuman-centeredpracticesfromtheTasks,Biz/OrgManagement,and
Bugs&PainPointscodes,anddrewonOperationscodesforautomatedpractices(refertoSection3.3
foradescriptionofthesecodes).Anoverviewoffindingsforeachworkflowstagecanbefoundin
Table2.
ThefollowingsubsectionsorganizeourfindingsaroundthefourstagesofMLOps.Webegin
eachsubsectionwithaquotethatstoodouttousandconversationwithpriorwork;then,inthe
contextofwhatisautomated,wediscusscommonhuman-centeredpracticesandpainpoints.
4.1 DataPreparation
“Ittakesexponentiallymoredatatokeepgettinglinearincreasesinperformance.”–Lg1
Datapreparationistheprocessofconstructing“well-structured,completedatasets”fordata
scientists[66].Datapreparationactivitiesconsistofcollection,wrangling,andcleaningandare
known to be challenging, often taking up to 80% of practitioners’ time [36, 102]. This tedious
processencourageslargerorganizationstohavededicatedteamsofdataengineerstomanagedata
preparation[37].LikeAmershietal.[3],weobservedthatmatureMLorganizationsautomateddata
preparationthroughdedicatedteamsasmuchaspossible(Lg1,Lg2,Lg3,Sm3,Md3,Sm6,Md6,Lg5,
Lg6).Asaresult,theMLEsweinterviewedspentasmallerfractionoftheirtimeondatapreparation,
collaboratinginsteadwithdataengineeringteams.Wefirstdiscusspipelineautomationtoprovide
keycontextfortheworkofMLEs.Then,wementiontwokeychallengesMLEsface:ensuring
labelingqualityatscaleandcopingwithfeedbackdelays.
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.“Wehavenoideahowmodelswillbehaveinproductionuntilproduction”:HowengineersoperationalizeML 206:13
Stage Description Non-AutomatedChallenges
Collection,wrangling,andcleaning -Ensuringlabelqualityatscale(Section4.1.2)
DataPreparation
pipelinesrunonaschedule -Handlingfeedbackorground-truthdelays(Sec-
tion4.1.3)
Prototypingideastoimprove -Managingtheunderlyingsoftwareorcodefor
Experimentation end-to-endMLpipeline data-centricexperiments(Section4.2.1)
performancebyiteratingon - Engaging in cross-team collaboration (Sec-
datasets,modelarchitectures,or tion4.2.2)
both -Manuallyandthoughtfullyidentifyingpromis-
ingexperimentconfigurations(Section4.2.3)
Evaluationand Pushingexperimentalchangesto - Continuously updating dynamic validation
Deployment small,thenlargefractionsofusers, datasetsforfutureexperiments(Section4.3.1)
evaluatingateverystep - Using product metrics for evaluation (Sec-
tion4.3.2)
SupervisingliveMLpipeline -Trackingandinvestigatingdataqualityalerts
Monitoringand
performanceandminimizing (Section4.4.1)
Response
pipelinedowntime - Managing pipeline "jungles" of models and
hard-codedrules(Section4.4.2)
-Debuggingaheavy-taileddistributionoferrors
(Section4.4.3)
Table2. OverviewofchallengingactivitiesthatMLengineersengageinforeachstageintheirworkflow.
Whileeachstagereliesonautomatedinfrastructureandpipelines,MLengineersstillhavemanydifficult
manualresponsibilities.
4.1.1 Pipelinesautomaticallyrunonaschedule. Unlikedatascience,wheredatapreparation
isoftenad-hocandinteractive[37,66],wefoundthatdatapreparationinproductionMLisbatched
andmorenarrowlyrestricted,involvinganorganization-widesetofstepsrunningatapredefined
cadence. In interviews, we found that preparation pipelines were defined by Directed Acyclic
Graphs,orDAGs,whichranonaschedule(e.g.,daily).EachDAGnodecorrespondedtoaparticular
task,suchasingestingdatafromasourceorcleaninganewlyingestedpartitionofdata.EachDAG
edgecorrespondedtoadataflowdependencybetweentasks.Whiledataengineerswereprimarily
responsiblefortheend-to-endexecutionofdatapreparationDAGs,MLEsinterfacedwiththese
DAGsbyloadingselectoutputs(e.g.,cleandata)orbyextendingtheDAGwithadditionaltasks,
e.g.tocomputenewfeatures(Md1,Lg2,Lg3,Sm4,Md6,Lg6).
Inmanycases,automatedtasksrelatingtoMLmodels,suchasmodelinference(e.g.,generating
predictionswithatrainedmodel)andretraining,wereexecutedinthesameDAGasdatapreparation
tasks(Lg1,Md1,Sm2,Md4,Md5,Sm6,Md6,Lg5).MLengineersincludedretrainingasanodein
thedatapreparationDAGforsimplicity:asnewdatabecomesavailable,acorrespondingmodelis
automaticallyretrained.Md4mentionedautomaticallyretrainingthemodeleverydaysomodel
performancewouldnotsufferforlongerthanaday:
Whydidwestarttrainingdaily?AsfarasI’maware,wewantedtostartsimple—wecould
justhaveasinglebatchjobthatprocessesnewdataandwewouldn’tneedtoworryabout
separateretrainingschedules.Youdon’treallyneedtoworryaboutifyourmodelhasgone
staleifyou’reretrainingiteveryday.
Retrainingcadencesrangedfromhourly(Lg5)toeveryfewmonths(Md6)andweredifferentfor
differentmodelswithinthesameorganization(Lg1,Md4).Noneoftheparticipantsinterviewed
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.206:14 Shankar&Garciaetal.
reportedanyscientificprocedurefordeterminingthepipelineexecutioncadence.Forexample,Md5
saidthat“the[modelretraining]cadencewasjustlike,fingertothewind.”Cadencesseemedtobe
setinawaythatstreamlinedoperationsfortheorganizationintheeasiestway.Lg5mentioned
that“retrainingtookabout3to4hours,so[they]matchedthecadencewithitsuchthatassoon
as[they]finishedanyonemodel,theykickedoffthenexttraining[job].”Engineersreportedan
inabilitytoretrainunlesstheyhadfreshandlabeleddata,motivatingtheirorganizationstosetup
dedicatedteamsofannotators,orhiringcrowdworkers,tooperationalizelabelingoflivedata(Lg1,
Sm3,Sm4,Md3,Sm6).
4.1.2 MLEsensurelabelqualityatscale. Althoughitiswidelyrecognizedthatmodelper-
formance improves with more labels [82]—and there are tools built especially for data label-
ing[79,111]—ourintervieweescautionedthatthequalityoflabelscandegradeastheytrytolabel
moreandmoredata.Md3said:
Nomatterhowmanylabelsyougenerate,youneedtoknowwhatyou’reactuallylabeling
andyouneedtohaveaveryclearhumandefinitionofwhattheymean.
In many cases, ground truth must be created, i.e., the labels are what a practitioner “thinks
up”[66].Whenoperationalizingthispractice,MLEsfacedproblems.Forone,Sm3spokeabouthow
expensiveitwastooutsourcelabeling.Moreover,labelingconflictscanerodetrustindataquality,
andslowMLprogress[73,85]:Whenscalinguplabeling—throughlabelingserviceprovidersor
analystswithintheorganization—MLEsfrequentlyfounddisagreementsbetweendifferentlabelers,
whichwouldnegativelyimpactqualityifunresolved(Sm3,Md3,Sm6).Attheirorganization,Sm3
mentionedthattherewasahuman-in-the-looplabelingpipelinethatbothoutsourcedlarge-scale
labelingandmaintainedaninternalteamofexpertstoverifythelabelsandresolveerrorsmanually.
Sm6 described a label validation pipeline for outsourced labels that itself used ML models for
estimatinglabelquality.
4.1.3 Feedbackdelayscandisruptpipelinecadences. Inmanyapplications,today’spredic-
tionsaretomorrow’strainingdata,butmanyparticipantssaidthatground-truthlabelsforlive
predictionsoftenarrivedafteradelay,whichcouldvaryunpredictably(e.g.,human-in-the-loopor
networkingdelays)andthuscausedproblemsforknowingreal-timeperformanceorretraining
regularly(Md1,Sm2,Sm3,Md5,Md6,Lg5).ThisisincontrasttoacademicML,whereground-truth
labelsarereadilyavailableforMLpractitionerstotrainmodels[71].Participantsnotedthatthe
impactonmodelswashardtoassesswhenthegroundtruthinvolvedlivedata—forexample,Sm2
feltstronglyaboutthenegativeimpactoffeedbackdelaysontheirMLpipelines:
Ihavenoideahowwell[models]actuallyperformonlivedata.Feedbackisalwaysdelayed
byatleast2weeks.Sometimeswemightnothavefeedback...sowhenwerealizemaybe
somethingwentwrong,itcould[havebeen]2weeksago.
Feedback delays contribute to “data cascades,” or compounding errors in ML systems over
time[85].Sm3mentioneda2-3year efforttodevelopahuman-in-the-looppipelinetomanually
labellivedataasfrequentlyaspossibletoside-stepfeedbackdelays:“youwanttocomeupwith
therateatwhichdataischanging,andthenassignpeopletomanagethisrateroughly”.Sm6said
thattheirorganizationhiredfreelancerstolabel“20orso”datapointsbyhanddaily.Labelingwas
thenconsideredataskinthebroaderpreparationpipelinethatranonaschedule(Section4.1.1).
4.2 Experimentation
“Youwanttoseesomedegreeofexperimentalthoroughness.Peoplewillhaveprincipled
stancesorintuitionsforwhythingsshouldwork.Butthemostimportantthingtodois
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.“Wehavenoideahowmodelswillbehaveinproductionuntilproduction”:HowengineersoperationalizeML 206:15
achievescaryhighexperimentationvelocity...Numberone[KeyPerformanceIndicator]is
rateofexperimentation.”(Lg1
WhilemostpriorworkstudyingthedatascienceandMLOpsworkflowsincludesmodelingas
anexplicitstepintheworkflow[96,102,106,110],wefoundthatiteratingonmodelideasand
architecturesisonlypartofabroader“experimentation”step.Thisisbecauseinmanyproduction
MLpipelines,MLEscanfocusontuningorimprovingexistingmodelsthroughdata-centricde-
velopment,andmodelinginadatasciencesenseisonlynecessarywhenthecompanywishesto
expanditsserviceofferingsorgrowitsMLcapabilities.Infact,manyofourintervieweesdidnot
buildtheinitialmodelinthepipelinethattheirorganizationassignedthemtoworkon,sotheir
goalwasn’tnecessarilytotrainmoremodels.Asanexample,Md6said,“someofourmodelshave
beenaroundfor,like,6or7years.”Gargetal.[22]alsocallthisworkflowstep“experimentation”
insteadof“modeling”intheirMLOpslifecycleoverview,andweexpandonthisfindinginourpaper
byrelatingittocollaborationanddata-drivenexploration,aswellasMLEreservationstoward
experimentautomationorAutoML.
4.2.1 MLEsfinditbettertoinnovateonthedatathanthemodel. Holsteinetal.[31]mention
thatitischallengingforpractitionerstodeterminewheretofocusexperimentationefforts—they
couldtry“switchingtoadifferentmodel,augmentingthetrainingdatainsomeway,collectingmore
ordifferentkindsofdata,post-processingoutputs,changingtheobjectivefunction,orsomething
else.”Ourintervieweesrecommendedfocusingonexperimentsthatprovidedadditionalcontextto
themodel,typicallyvianewfeatures,togetthebiggestgains(Lg2,Lg3,Md3,Lg4,Md4,Sm6,Md6,
Lg5,Lg6).Lg5said:
I’mfocusingmyenergythesedaysonsignalsandfeatureengineeringbecauseevenifyou
keepyourcodestaticandthemodelstatic,itwoulddefinitelyhelpyouwithgettingbetter
modelperformance.
Inaconcurringview,Md3adds:
I’mgonnastartwitha[fixed]modelbecauseitmeansfasteriterations.Andoften—like
mostofthetimeempirically—it’sgoingtobesomethinginourdatathatwecanuseto
pushtheboundary[...]obviously,it’snotadogmatic“wewillnevertouchthemodel”but
itshouldn’tbeourfirstmove.
Interestingly,olderworkclaimsthatiteratingonthemodelisoftenmorefruitfulthaniterating
onthedata[74],butthiscouldbebecauseMLmodelinglibrariesweren’tasmatureastheyarenow.
Recentworkhasalsoidentifiedtheimportanceofdata-centricexperimentationinproductionML
deployments[66,71,79,85].Md6mentionedthatmostMLprojectsattheirorganizationcentered
aroundaddingnewfeatures.Md4mentionedthatoneoftheircurrentprojectswastomovefeature
engineeringpipelinesfromScalatoSparkSQL(alanguagemorefamiliartoMLengineersanddata
scientists),soexperimentideascouldbecodedandvalidatedfaster.
Whenaskedhowtheymanagedtheunderlyingsoftwareorcodefordata-centricexperiments,
intervieweesemphasizedtheimportanceofkeepingtheircodechangesassmallaspossiblefor
multiplereasons,includingfastercodereview,easiervalidation,andfewermergeconflicts(Md1,
Lg2, Lg3, Sm4, Md3, Lg5, Lg6). This is in line with good software engineering practices [3].
Additionally,changesinlargeorganizationswereprimarilymadeinconfiguration(config)files
insteadofmainapplicationcode(Lg1,Md1,Lg2,Sm4,Lg4,Lg6):insteadofeditingparameters
directly in a Python model training script, MLEs preferred to edit a config file of parameters
(e.g.,JSONorYAML),andwouldfeedtheconfigfiletothemodeltrainingscript.Whenlarger
changeswerenecessary,especiallychangestouchingthelanguagelayer(e.g.changingPyTorchor
TensorFlowarchitectures),MLEswouldforkthecodebaseandmadetheireditsin-place(Md2,Lg3).
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.206:16 Shankar&Garciaetal.
Althoughforkingrepositoriescanbeahigh-velocityshortcut,absentstreamlinedmergeprocedures,
thiscanleadtoadivergenceinversionsandaccumulationoftechnicaldebt.Lg3highlightedthe
tensionbetweenexperimentvelocityandstrictsoftwareengineeringdiscipline:
I used to see a lot of people complaining that model developers don’t follow software
engineering.Atthispoint,I’mfeelingmoreconvincedthatit’snotbecausethey’relazy.It’s
because[softwareengineeringis]contradictorytotheagilityofanalysisandexploration.
4.2.2 Featureengineeringissocialandcollaborative. Priorworkhasstressedtheimportance
ofcollaborationindatascienceprojects,oftenlamentingthattechnicaltaskshappeninsilos[46,
71,85,102].Ourintervieweessimilarlybelievedcross-teamcollaborationwascriticalforsuccessful
experiments.Projectideas,suchasnewfeatures,camefromorwerevalidatedearlybydomain
experts:datascientistsandanalystswhohadalreadyperformedalotofexploratorydataanalysis.
Md4 andMd6 independentlyrecounted successful projectideas that camefrom asynchronous
conversationsonSlack:Md6said,“Ilookforfeaturesfromdatascientists,[whohaveideasof]
thingsthatarecorrelatedwithwhatI’mtryingtopredict.”Wefoundthatorganizationsexplicitly
prioritizedcross-teamcollaborationaspartoftheirMLculture.Md3said:
Wereallythinkit’simportanttobridgethatgapbetweenwhat’soften,youknow,a[subject
matterexpert]inoneroomannotatingandthenhandingthingsoverthewiretoadata
scientist—ascenewhereyouhavenocommunication.Sowemakesurethere’sbothdata
scienceandsubjectmatterexpertiserepresentation[inourmeetings].
Tofosteramorecollaborativeculture,Sm6discussedtheconceptof“buildinggoodwill”withother
teamsthroughtedioustasksthatweren’talwaysexplicitlyapartofcompanyplans:“Sometimes
we’llfixsomething[hereand]theretolikebuildsomegoodwill,sothatwecancallontheminthe
future...Idothisstuff[tobuildrelationships],notbecauseI’mreallypassionateaboutdoingit.”
4.2.3 MLEs like having manual control over experiment selection. One challenge that
resultsfromfastexplorationishavingtomanagemanyexperimentversions[44,102].MLEsare
happytodelegateexperimenttrackingandexecutionworktoMLexperimentexecutionframeworks,
suchasWeights&Biases3,butprefertochoosesubsequentexperimentsthemselves.Tobeable
tomakeinformedchoicesofsubsequentexperimentstorun,MLEsmustmaintainawarenessof
whattheyhavetriedandwhattheyhaven’t(Lg2callsitthe“explorationfrontier”).Asaresult,
therearelimitstohowmuchautomationMLEsarewillingtorelyonforexperimentation,afinding
consistentwithresultsfromXinetal.[108].Lg2mentionedthephrase“keepingGPUswarm”to
characterizeaperceivedanti-patternthatwastesresourceswithoutaplan:
One thing that I’ve noticed is, especially when you have as many resources as [large
companies]do,thatthere’sacompulsiveneedtoleveragealltheresourcesthatyouhave.
Andjust,youknow,getalltheexperimentsoutthere.Comeupwithabunchofideas;run
abunchofstuff.Iactuallythinkthat’sbad.Youcanbeoverlyconcernedwithkeepingyour
GPUswarm,[somuch]sothatyoudon’tactuallythinkdeeplyaboutwhatthehighest
valueexperimentis.
Inexecutingexperimentideas,wenoticedatradeoffbetweenaguidedsearchandrandomsearch.
Randomsearchesweremoresuitedtoparallelization—e.g.,hyperparametersearchesorideasthat
didn’tdependoneachother.Althoughcomputinginfrastructurecouldsupportmanydifferent
experimentsinparallel,thecognitiveloadofmanagingsuchexperimentswastoocumbersome
forparticipants(Lg2,Sm4,Lg5,Lg6).Rather,participantsnotedmoresuccesswhenpipelining
3https://wandb.ai/
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.“Wehavenoideahowmodelswillbehaveinproductionuntilproduction”:HowengineersoperationalizeML 206:17
learningsfromoneexperimentintothenext,likeaguidedsearchtofindthebestidea(Lg2,Sm4,
Lg5).Lg5describedtheirideologicalshiftfromrandomsearchtoguidedsearch:
Previously,Itriedtodoalotofparallelization.IfIfocusononeidea,aweekatatime,
thenitboostsmyproductivityalotmore.
Byfollowingaguidedsearch,engineersare,essentially,significantlypruningalargesubsetof
experimentideaswithoutexecutingthem.Whileitmayseemlikethereareunlimitedcomputational
resources,thesearchspaceismuchlarger,anddevelopertimeandenergyislimited.Attheendof
theday,experimentsarehuman-validatedanddeployed.MatureMLengineersknowtheirpersonal
tradeoffbetweenparallelizingdisjointexperimentideasandpipeliningideasthatbuildontopof
eachother,ultimatelyyieldingsuccessfuldeployments.
4.3 EvaluationandDeployment
“We don’t have a good idea of how the model is going to behave in production until
production.”(Lg3)
Aftertheexperimentationphase,whenMLEshaveidentifiedachangetheywanttomaketo
theMLpipeline(e.g.,addinganewfeature),theyneedtoevaluateitanddeployittoproduction.
PriorworkthatprescribesframeworksforMLOpstypicallyseparatesevaluationanddeployment
intotwodifferentstages[48,96,98],butwecombinethemintoonestepbecausetheyaretightly
intertwined,withdeploymentsspanninglongperiodsoftimeandevaluationshappeningmultiple
timesduringdeployment.
Priorworkdescribesevaluationasan“offline,”automatedprocessthathappensattrainingtime:
asmallportionofthetrainingdatasetisheldout,andthemodelshouldachievehighaccuracyon
thisheld-outset[71,106].RecentrelatedworkinMLOpsclaimsthatevaluationanddeploymentare
highlyamenabletoautomation[22,59].Assuch,wealsooriginallyhypothesizedthatevaluation
anddeploymentcouldbeautomated—oncevalidated,anengineercouldsimplycreateanewtask
intheirDAGtoretrainthemodelonacadence(Section4.1.1).
Asexpected,engineersdidautomaticallyvalidateandcodifytheirchangesintoDAGstoretrain
modelsonaschedule.However,theyalsomanuallysupervisedthedeploymentoveralongperiod
oftime,evaluatingthroughoutthetimeframe.Amershietal.[3]statethatsoftwareteams“flight”
changesorupdatestoMLmodels,oftenbytestingthemonafewcasespriortolivedeployment.
OurworkprovidesfurthercontextintotheevaluationanddeploymentprocessforproductionML
pipelines:wefoundthatseveralorganizations,particularlythosewithmanycustomers,employed
amulti-stagedeploymentprocessfornewmodelsormodelchanges,progressivelyevaluatingat
eachstage(Sm1,Lg2,Lg3,Sm2,Sm3,Lg4,Md5,Md6,Lg5,Lg6).Assuch,wecombineevaluation
and deployment into one step, rather than separating the process into evaluation followed by
deploymentasotherpapersdo[96,98].Lg3describedthemulti-stageddeploymentprocessas
follows:
We have designated test clusters, [stage 1] clusters, [stage 2] clusters, then the global
deployment[toallusers].Theideahereisyoudeployincreasinglyalongtheseclusters,so
thatyoucatchproblemsbeforethey’vemetcustomers.
Each organization had different names for its stages (e.g., test, dev, canary, staging, shadow,
A/B)anddifferentnumbersofstagesinthedeploymentprocess(usuallybetweenoneandfour).
Thestageshelpedinvalidatemodelsthatmightperformpoorlyinfullproduction,especiallyfor
brand-neworbusiness-criticalcases.Occasionally,organizationshadanoffline“sandbox”stage
precedingdeploymenttoanyfractionofcustomers—forexample,Md5describedasandboxwhere
theycouldstress-testtheirchatbotproduct:
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.206:18 Shankar&Garciaetal.
Youcanpretendtobeacustomerandsayallsortsofoffensivethings,andseeifthemodel
willsaycusswordsbackatyou,orothersortsofthingslikethat.
Althoughthemodelretrainingprocesswasautomated,wefindthatMLEspersonallyreviewed
validation metrics and manually supervised the promotion from one stage to the next. They
hadoversightovereveryevaluationstage,takinggreatcaretomanagecomplexityandchange
over time: specifically, changes in data, product and business requirements, users, and teams
withinorganizations.Wediscusstwohuman-centeredpractices:maintainingdynamicdatasets
andevaluatingperformanceinthecontextoftheproductorbroaderorganizationalvalue.
4.3.1 MLEscontinuouslyupdatedynamicvalidationdatasets. Manyengineersreported
processestoanalyzelivefailuremodesandupdatethevalidationdatasetstopreventsimilarfailures
fromhappeningagain(Lg1,Md1,Lg2,Lg3,Sm3,Md3,Md5,Sm6,Md6,Lg5).Lg1describedthis
processasadeparturefromwhattheyhadlearnedinschool:
Youhavethisclassicissuewheremostresearchersareevaluat[ing]againstfixeddatasets
[...but]mostindustrymethodschangetheirdatasets.
Wefoundthatthesedynamicvalidationsetsservedtwopurposes:(1)theobviousgoalofmaking
sure the validation set stays current with live data as much as possible, given new knowledge
abouttheproblemandgeneralshiftsinthedatadistribution,and(2)themorespecificgoalof
addressing localized shifts within sub-populations, such as low accuracy for minority groups.
Thechallengewith(2)isthatmanysub-populationsareoftenoverlooked,ortheyarediscovered
post-deployment[31].Inresponse,Md3discussedhowtheysystematicallybucketedtheirdata
pointsbasedonthemodel’serrormetricsandcreatedvalidationsetsforeachunder-performing
bucket:
Some[ofthemetricsinmytool]arestandard,likeaconfusionmatrix,butit’snotreally
effectivebecauseitdoesn’tdrillthingsdown[intospecificsubpopulationsthatuserscare
about].Slicesareuser-defined,butsometimesit’salittlebitmoreautomated.[During
offlineevaluation,we]findtheerrorbucketthat[we]wanttodrilldown,andthen[we]
eitherimprovethemodelinverysystematicwaysorimprove[our]datainverysystematic
ways.
Ratherthanfollowaproactiveapproachofconstructingdifferentfailuremodesinanoffline
validationphaselikeMd3did,Sm3offeredareactivestrategyofspawninganewdatasetforeach
observedlivefailure:“Every[failedprediction]getsintothesamequeue,and3ofussitdownonce
aweekandgothroughthequeue...thenour[analysts]collectmore[similar]data.”Thisdataset
update(ordelta)wasthenmergedintothevalidationdataset,andusedformodelvalidationin
subsequentrounds.Whileprocessestodynamicallyupdatethevalidationdatasetsrangedfrom
human-in-the-loop to periodic synthetic data construction (Lg3), we found that higher-stakes
applicationsofML(e.g.,autonomousvehicles),createddedicatedteamstomanagethedynamic
evaluationprocess.Often,thisinvolvedcreatingsyntheticdatarepresentativeoflivefailures(Lg1,
Lg3,Md4).Forexample,Lg1said:
WhatyouneedtobeabletodoinamatureMLOpspipelineisgoveryquicklyfromuser
recordedbug,tonotonlyareyougoingtofixit,butyoualsohavetobeabletodrive
improvementstothestackbychangingyourdatabasedonthosebugs.
Notwithstanding,participantsfounditchallengingtocollectthevariouskindsoffailuremodes
andmonitoringmetricsforeachmode.Lg6added,“youhavetolookatsomanydifferentmetrics.
Evenveryexperiencedfolksquestionthisprocesslikeadozentimes.”
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.“Wehavenoideahowmodelswillbehaveinproductionuntilproduction”:HowengineersoperationalizeML 206:19
4.3.2 MLEsuseproductmetricsforvalidation. Whilepriorworkdiscusseshowprediction
accuracydoesn’talwayscorrelatewithreal-worldoutcomes[71,102],it’sunclearhowtoarticulate
clear and measurable ML goals. Patel et al. [74] discuss how practitioners trained in statistical
techniques“feltthattheymustoftenmanageconcernsoutsidethefocusoftraditionalevaluation
metrics.”Srivastavaetal.[93]notethatanincreaseinaccuracymightnotimproveoverallsystem
“compatibility.”Inourstudy,wefoundthatsuccessfulMLdeploymentstiedtheirperformanceto
productmetrics.First,wefoundthatpriortoinitialdeployment,matureMLteamsdefinedaproduct
metricinconsultationwithotherstakeholders,suchasbusinessanalysts,productmanagers,or
customers(Lg2,Sm2,Md5,Sm6,Md3,Md6,Lg5,Lg6).Examplesofproductmetricsincludeclick-
throughrateanduserchurnrate.Md3feltthatakeyreasonmanyMLprojectsfailisthatthey
don’tmeasuremetricsthatwillyieldtheorganizationvalue:
Tying[modelperformance]tothebusiness’sKPIs(keyperformanceindicators)isreally
important.Butit’saprocess—youneedtofigureoutwhat[theKPIs]are,andfrankly
I think that’s how people should be doing AI. It [shouldn’t be] like: hey, let’s do these
experimentsandgetcoolnumbersandshowofftheseniceprecision-recallcurvestoour
bossesandcallitaday.Itshouldbelike:hey,let’sactuallyshowthesamebusinessmetrics
thateveryoneelseisheldaccountabletotoourbossesattheendoftheday.
Sinceproduct-specificmetricsare,bydefinition,differentfordifferentMLmodels,itwasimpor-
tantforengineerstotreatthechoiceofmetricsasanexplicitstepintheirworkflowandalignwith
otherstakeholderstomakesuretherightmetricswerechosen.Afteragreeingonaproductmetric,
engineersonlypromotedexperimentideastolaterdeploymentstagesiftherewasanimprovement
in that metric. Md6 said that every model change in production was validated by the product
team:“ifwecangetastatisticallysignificantgreaterpercentage[of]peopletosubscribeto[the
product],then[wecanfullydeploy].”Kimetal.[40]alsohighlighttheimportanceofincluding
otherstakeholders(orpeoplein“decision-makingseats”)intheevaluationprocess.Ateachstage
ofdeployment,someorganizationsplacedadditionalemphasisonimportantcustomersduring
evaluation(Lg3,Sm4).Lg3mentionedthattherewere“hard-coded”rulesfor“mission-critical”
customers:
There’san[ML]systemtoallocateresourcesfor[ourproduct].Wehavehard-codedrules
formissioncriticalcustomers.LikeatthestartofCOVID,therewerehospital[customers]
thatwehadtosave[resources]for.
Finally,participantswhocamefromresearchoracademianotedthattyingevaluationtothe
productmetricswasadifferentexperience.Lg3commentedontheir“mindsetshift”afterleaving
academia:
Ithinkaboutwherethebusinesswillbenefitfromwhatwe’rebuilding.We’renotjust
shippingfakewins,likewe’rereallyinthevaluebusiness.You’vegottoseevaluefrom
AIinyourorganizationinordertofeellikeitwasworthittoyou,andIguessthat’sa
mindsetthatwereallyoughttohave[asacommunity].
4.4 MonitoringandResponse
“Thisdataissupposedtohave50states,there’sonly40,whathappenedtotheother10?”
(Md6)
Wefoundthatorganizationscenteredtheirmonitoringandresponsepracticesaroundengineers,
muchlikeintheDevOpsagileframework,whichorganizessoftwaredevelopmentaroundteams[13].
PriorworkhasstatedthatmonitoringiscriticaltoMLOps[44,59,96,98],and,broadly,thatAgile
practicescanbeusefulinsupervisingproductionML[3].Weprovidefurtherinsightbydiscussing
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.206:20 Shankar&Garciaetal.
twospecificexamplesofAgilepracticesthatourintervieweescommonlyadaptedtotheMLcontext.
First,Lg3,Lg4,Md4,Sm6,Lg5,andLg6describedon-callprocessesforsupervisingproductionML
models.Foreachmodel,atanypointintime,someMLengineerwouldbeoncall,orprimarily
responsibleforit.Anybugorincidentobserved(e.g.,usercomplaint,pipelinefailure)wouldreceive
aticket,createdbytheon-callengineer.On-callrotationstypicallylastedoneortwoweeks.Atthe
endofashift,anengineerwouldcreateanincidentreport—possiblyoneforeachbug—detailing
majorissuesthatoccurredandhowtheywerefixed.Additionally,Lg3,Sm2,Sm4,andMd5discussed
havingService-LevelObjectives(SLOs),orcommitmentstominimumstandardsofperformance,for
pipelinesintheirorganizations.Forexample,apipelinetoclassifyimagescouldhaveanSLOof95%
accuracy.AbenefitofusingtheSLOframeworkforMLpipelinesisaclearindicationofwhethera
pipelineisperformingwellornot—iftheSLOisnotmet,thepipelineisbroken,bydefinition.
OurintervieweesstressedtheimportanceofloggingdataacrossallstagesoftheMLpipeline
(e.g.,featureengineering,modeltraining)touseforfuturedebugging.MonitoringMLpipelines
and responding to bugs involved tracking live metrics (via queries or dashboards), slicing and
dicingsub-populationstoinvestigatepredictionquality,patchingthemodelwithnon-MLheuristics
forknownfailuremodes,andfindingin-the-wildfailuresthatcouldbeaddedtofuturedynamic
validationdatasets.WhileMLEstriedtoautomatemonitoringandresponseasmuchaspossible,
wefoundthatsolutionswerelackingandrequiredsignificanthuman-in-the-loopintervention.
Next,wediscussdataqualityalerts,pipelinejungles,anddiagnostics.
4.4.1 On-callMLEstrackdataqualityalertsandinvestigateafractionofthem. Indata
science,dataqualityisofutmostimportance[37,73].Priorworkhasstressedtheimportanceof
monitoringdatainproductionMLpipelines[40,85,87],andthedatamanagementliteraturehas
proposednumerousdataqualitymetrics[8,76,86].Butwhatmetricsdopractitionersactuallyuse,
whatdatadopractitionersmonitor,andhowdotheymanuallyengagewiththesemetrics?We
foundthatengineerscontinuouslymonitoredfeaturesforandpredictionsfromproductionmodels
(Lg1,Md1,Lg3,Sm3,Md4,Sm6,Md6,Lg5,Lg6):Md1discussedhardconstraintsforfeaturecolumns
(e.g.,boundsonvalues),Lg3talkedaboutmonitoringcompleteness(i.e.,fractionofnon-nullvalues)
forfeatures,Sm6mentionedembeddingtheirpipelineswith"commonsensechecks,"implemented
ashardconstraintsoncolumns,andSm3describedschemachecks—makingsureeachdataitem
adherestoanexpectedsetofcolumnsandtheirtypes.Thesecheckswereautomatedandexecuted
aspartofthelargerpipeline(Section4.1.1).
While off-the-shelf data validation was definitely useful for the participants, many of them
expressedpainpointswithexistingtechniquesandsolutions.Lg3discussedthatitwashardto
figureouthowtotriggeralertsbasedondataquality:
Monitoringisbothmetricsandthenapredicateoverthosemetricsthattriggersalerts.
Thatsecondpiecedoesn’texist—notbecausetheinfrastructureishard,butbecausenoone
knowshowtosetthosepredicatevalues...foralotofthisstuffnow,there’sengineering
headcount to support a team doing this stuff. This is people’s jobs now; this constant,
periodicevaluationofmodels.
Wealsofoundthatemployeeturnovermakesdatavalidationunsustainable(Sm2,Md4,Sm6,Md6,
Lg5). If one engineer manually defined checks and bounds for each feature and then left the
team,anotherengineerwouldhavetroubleinterpretingthepredefineddatavalidationcriteria.To
circumventthisproblem,someparticipantsdiscussedusingblack-boxdatamonitoringservicesbut
lamentedthattheirstatisticsweren’tinterpretableoractionable(Sm2,Md4).
Anothercommonlydiscussedpainpointwasfalse-positivealerts,oralertstriggeredevenwhen
theMLperformanceisadequate.Engineersoftenmonitoredandplaceddataqualityalertsoneach
featureandprediction(Lg2,Lg3,Sm3,Md3,Md4,Sm6,Md6,Lg5,Lg6).Ifthenumberofmetrics
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.“Wehavenoideahowmodelswillbehaveinproductionuntilproduction”:HowengineersoperationalizeML 206:21
trackedgrewtoolarge,false-positivealertscouldbecomeaproblem.Anexcessoffalse-positive
alertsledtofatigueandsilencingofalerts,whichcouldmissactualperformancedrops.Sm3said
“people[were]gettingbombedwiththesealerts.”Lg5sharedasimilarsentiment,thattherewas
“nothingcriticalinmostofthealerts.”Theonlytimetherewassomethingcriticalwas“wayback
when[they]hadtoactuallywakeupinthemiddleofthenighttosolveit...theonlytime[inyears].”
Whenweaskedwhattheydidaboutthenoncriticalalertsandhowtheyactedonthealerts,Lg5
said:
Youtypicallyignoremostalerts...IguessonrecordI’dsay90%ofthemaren’timmediate.
Youjusthavetoacknowledgethem[internally],likejustbeawarethatthereissomething
happening.
SeasonedMLEsthuspreferredtoviewandfilteralertsthemselves,thantosilenceorlowerthe
alertreportingrate.Inasense,evenfalse-positivescanprovideinformationaboutsystemhealth,
iftheMLEknowshowtoreadthealertsandisaccustomedtothesystem’sreportingpatterns.
Whenalertfatiguematerialized,itwastypicallywhenengineerswereon-call,orresponsiblefor
MLpipelinesduringa7or14-dayshift.Lg6recountedhowon-callrotationsweredreadedamongst
theirteam,particularlyfornewteammembers,duetothehighrateoffalse-positivealerts.They
said:
On-callMLengineersfreakoutinthefirst2rotations.Theydon’tknowwheretolook.So
wehavethemactasashadow,untiltheyknowthepatterns.
Lg6 also discussed an initiative at their company to reduce the alert fatigue, ironically with
anothermodel,whichwouldconsiderhowmanytimesanengineerhistoricallyactedonanalertof
agiventypebeforedeterminingwhethertosurfaceanewalertofthattype.
4.4.2 Overtime,MLpipelinesmayturninto“jungles”ofrulesandmodels. Sculleyetal.
[87]introducethephrase“pipelinejungles”(i.e.,differentversionsofdatatransformationsand
modelsgluedtogether),whichwaslateradoptedbyparticipantsinourstudy.Whilepriorwork
demonstratestheirexistenceandmaintenancechallenges,weprovideinsightintowhyandhow
thesepipelinesbecomejunglesinthefirstplace.OurintervieweesnotedthatreactingtoanML-
relatedbuginproductionusuallytookalongtime,motivatingthemtofindstrategiestoquickly
restoreperformance(Lg1,Sm2,Sm3,Sm4,Md4,Md5,Md6,Lg6).Thesestrategiesprimarilyinvolved
addingnon-MLrulesandfilterstothepipeline.WhenSm3observed,foranentityrecognitiontask,
thatthemodelwasmisdetectingtheEgyptianpresidentduetothemanywaysofwritinghisname,
theythoughtitwouldbebettertopatchthepredictionsfortheindividualcasethantofixorretrain
themodel:
Suppose we deploy [a new model] in the place of the existing model. We’d have to go
throughallthetrainingdataandthenrelabelitand[expletive],that’ssomuchwork.
OnewayengineersreactedtoMLbugswasbyaddingfiltersformodels.FortheEgyptexample,
Sm3addedasimplestringsimilarityruletoidentifythepresident’sname.Md4andMd5each
discussedhowtheirmodelswereaugmentedwithafinal,rule-basedlayertokeeplivepredictions
morestable.Forexample,Md4mentionedworkingonananomalydetectionmodelandaddinga
heuristicslayerontoptofilterthesetofanomaliesthatsurfacebasedondomainknowledge.Md5
discussedoneoftheirlanguagemodelsforacustomersupportchatbot:
Themodelmightnothaveenoughconfidenceinthesuggestedreply,sowedon’treturn[the
recommendation].Also,languagemodelscansayallsortsofthingsyoudon’tnecessarily
wantitto—anotherreasonthatwedon’tshowsomesuggestions.Forexample,ifsomebody
askswhenthebusinessisopen,themodelmighttrytoquoteatimewhenitthinksthe
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.206:22 Shankar&Garciaetal.
businessisopen.[Itmightsay]“9am,”butthemodeldoesn’tknowthat.Soifwedetect
time,thenwefilterthat[reply].Wehavealotoffilters.
Constructingsuchfilterswasaniterativeprocess—Md5mentionedconstantlystress-testingthe
modelinasandbox,aswellasobservingsuggestedrepliesinearlystagesofdeployment,tocome
upwithfilterideas.Creatingfilterswasamoreeffectivestrategythantryingtoretrainthemodel
tosaytherightthing;thegoalwastokeepsomeversionofamodelworkinginproductionwith
littledowntime.Asaresult,filterswouldaccumulateinthepipelineovertime,effectivelycreating
apipelinejungle.Evenwhenmodelswereimproved,Lg5notedthatitwastooriskytoremove
thefilters,sincethefilterswerealreadyinproduction,andaremovalmightleadtocascadingor
unforeseenfailures.
Several engineers also maintained fallback models for reverting to: either older or simpler
versions(Lg2,Lg3,Md6,Lg5,Lg6).Lg5mentionedthatitwasimportanttoalwayskeepsomemodel
upandrunning,evenifthey“switchedtoalesseconomicmodelandhadtojustcutthelosses.”
Similarly,whendoingdatasciencework,bothPassiandJackson[73]andWangetal.[102]echo
theimportanceofhavingsomesolutiontomeetclients’needs,evenifitisnotthebestsolution.
Anothersimplesolutionengineersdiscussedwasservingaseparatemodelforeachcustomer(Lg1,
Lg3,Sm2,Sm4,Md3,Md4).Wefoundthatengineerspreferredaper-customermodelbecauseit
minimizeddowntime:iftheservicewasn’tworkingforaparticularcustomer,itcouldstillworkfor
othercustomers.Pateletal.[74]alsonotedthatper-customermodelscouldyieldhigheroverall
performance.
4.4.3 BugsinproductionMLfollowaheavy-taileddistribution. MLdebuggingisdifferent
fromdebuggingduringstandardsoftwareengineering,whereonecanwritetestcasestocoverthe
spaceofpotentialbugs[3,71].Lg3,Sm2,Sm3,Sm4,Lg4,Md4,Md5,Sm6,Lg5,andLg6mentioned
havingacentralqueueofproductionMLbugsthateveryengineeraddedticketstoandprocessed
ticketsfrom.Oftenthisqueuewaslargerthanwhatengineerscouldprocessinatimelymanner,so
theyassignedtagstoticketstoprioritizewhattodebug.
Interviewees discussed ad-hoc approaches to debugging production ML issues, which could
require them to spend a lot of time diagnosing any given bug (Lg3, Lg2, Sm3, Sm4, Lg5). One
commonissuewasdataleakage—i.e.,assumingduringtrainingthatthereisaccesstodatathatdoes
notexistatservingtime—anerrortypicallydiscoveredafterthemodelwasdeployedandseveral
incorrectlivepredictionsweremade(Lg1,Md1,Md5,Lg5).Intervieweesfeltthatanticipatingall
possibleformsofdataleakageduringexperimentationwastedious;thus,sometimesleakagewas
retroactivelycheckedduringcodereviewinanevaluationstage(Lg1.Therewereothertypesofbugs
thatwerediscussedbymultipleparticipants,suchasaccidentallyflippinglabelsinclassification
models (Lg1, Sm1, Lg3, Md3) and forgetting to set random seeds in distributed training when
initializingworkersinparallel(Lg1,Lg4,Sm5).However,thevastmajorityofbugsdescribedto
usintheinterviewswereseeminglybespokeandnotsharedamongparticipants.Forexample,
Sm3forgottodropspecialcharacters(e.g.,apostrophes)fortheirlanguagemodels.Lg3foundthat
theimputationvalueformissingfeatureswasoncecorrupted.Lg5mentionedthatafeatureof
unstructureddatatype(e.g.,JSON)hadhalfofthekeys’valuesmissingfora“longtime.”
Whenaskedhowtheydetecttheseone-offbugs,intervieweesmentionedthattheirbugsshowed
similarsymptomsoffailure.Onesymptomwasalargediscrepancybetweenofflinevalidation
accuracyandproductionaccuracyimmediatelyafterdeployment(Lg1,Lg3,Md4,Lg5).However,
if there were no ground-truth labels available immediately after deployment (as discussed in
Section4.1.3),intervieweeshadtoresorttootherstrategies.Forexample,someinspectedtheresults
ofdataqualitychecks(Section4.4.1).Lg1discussedtheirstruggletodebugwithout“ground-truth:”:
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.“Wehavenoideahowmodelswillbehaveinproductionuntilproduction”:HowengineersoperationalizeML 206:23
Um,yeah,it’sreallyhard.Basicallythere’snosurefirestrategy.TheclosestthatI’veseenis
forpeopletointegrateaveryhighdegreeofobservabilityintoeverypartoftheirpipeline.
It starts with having really good raw data, observability, and visualization tools. The
abilitytoquery.I’venoticed,youknow,somuchofthis[ad-hocbugexploration]isjust—if
youmakethefriction[todebug]lower,peoplewilldoitmore.Soasanorganization,you
needtomakethefrictionverylowforinvestigatingwhatthedataactuallylookslike,
[suchas]lookingatspecificexamples.
Todiagnosebugs,intervieweestypicallyslicedanddiceddatafordifferentgroupsofcustomers
or data points (Md1, Lg3, Md3, Md4, Md6, Lg6). Slicing and dicing is known to be useful for
identifying bias in models [31, 85], but we observed that our interviewees used this technique
beyonddebuggingbiasandfairness;theyslicedanddicedtodeterminecommonfailuremodesand
datapointssimilartothesefailures.Md4discussedannotatingbugsandonlydrillingdowninto
theirqueueofbugswhentheyobserved“systematicmistakesforalargenumberofcustomers.”
Interviewees mentioned that after several iterations of chasing bespoke ML-related bugs in
production,theyhaddevelopedasenseofparanoiawhileevaluatingmodelsoffline—possiblyasa
copingmechanism(Lg1,Md1,Lg3,Md5,Md6,Lg6).Lg1said:
ML[bugs]don’tgetcaughtbytestsorproductionsystemsandjustsilentlycauseerrors.
Thisiswhy[you]needtobeparanoidwhenyou’rewritingMLcode,andthenbeparanoid
whenyou’recoding.
Lg1thenrecountedabugthatwas“impossibletodiscover”afteradeploymenttoproduction:the
codeforachangethataddednewdataaugmentationtothetrainingprocedurehadtwovariables
flipped,andthisbugwasmiraculouslycaughtduringcoderevieweventhoughthetrainingaccuracy
washigh.Lg1claimedthattherewas“nomechanismbywhich[they]wouldhavefoundthisbesides
someonejustcuriouslyreadingthecode.”SinceMLbugsdon’tcausesystemstogodown,sometimes
theonlywaytofindthemistobecautiouswheninspectingcode,data,andmodels.
5 DISCUSSION
Our findings suggest that automated production ML pipelines are enabled by MLEs working
throughacontinuousloopofi)datapreparation,ii)experimentation,iii)evaluation&deployment,
andiv)monitoringandresponse(Figure1).Althoughengineersleveragedifferenttoolstohelp
withtechnicalaspectsoftheirworkflow,suchasexperimenttrackinganddatavalidation[8,109],
patternsbegantoemergewhenwestudiedhowMLEpracticesvariedacrosscompanysizesand
experiencelevels.Wediscussthesepatternsas“thethreeVsofMLOps”(Section5.1),andfollow
ourdiscussionwithimplicationsforbothproductionMLtooling(Section5.2),andopportunities
forfuturework(Section5.3).
5.1 Velocity,Visibility,Versioning:ThreeVsofMLOps
FindingsfromourworkandpriorworksuggestthreebroadthemesofsuccessfulMLOpspractices:
Velocity,Visibility,andVersioning.Thesethemeshavesynergiesandtensionsacrosseachstageof
MLEs’workflow,aswediscussnext.
5.1.1 Velocity. Since ML is so experimental in nature, it’s important to be able to prototype
anditerateonideasquickly(e.g.,gofromanewideatoatrainedmodelinaday).Interviewees
attributedtheirproductivitytodevelopmentenvironmentsthatprioritizedhighexperimentation
velocityanddebuggingenvironmentsthatallowedthemtotesthypothesesquickly.Priorwork
hasextensivelydocumentedtheAgiletendenciesofMLEs,describinghowtheyiteratequickly
(i.e.withvelocity)toexplorealargeMLordatasciencesearchspace[3,30,50,74,110].Amershi
et al. [3] describe how experimentation can be sped up when labels are annotated faster (i.e.,
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.206:24 Shankar&Garciaetal.
rapiddatapreparation).Garciaetal.[20]exploretoolingtohelpMLEscorrectloggingoversights
fromtoomuchvelocityinexperimentation,andPaleyesetal.[71]mentiontheneedtodiagnose
productionbugsquicklytopreventfuturesimilarissuesfromoccurring.First,ourstudyre-enforces
theviewtheMLEsareagileworkerswhovaluefastresults.P1saidthatpeoplewhoachievethebest
outcomesfromexperimentationarepeoplewith“scaryhighexperimentationvelocity.”Similarly,
themulti-stagedeploymentstrategycanbeviewedasanoptimisticorhigh-velocitysolutionto
deployment:deployfirst,andvalidategraduallyacrossstages.Moreover,ourstudyprovidesdeeper
insightintohowpractitionersrapidlydebugdeployments—weidentifyanddescribepracticessuch
ason-callrotations,human-interpretablefiltersonmodelbehavior,dataqualityalerts,andmodel
rollbacks.
At the same time, high velocity can lead to trouble if left unchecked. Sambasivan et al. [85]
observedthat,forhigh-stakescustomers,practitionersiteratedtooquickly,causingMLsystemsto
fail—practitioners“movedfast,hackedmodelperformance(throughhyperparametersratherthan
dataquality),anddidnotappeartobeequippedtorecogniseupstreamanddownstreampeople
issues.”Ourstudyexposedstrategiesthatpractitionersusedtopreventthemselvesfromiterating
tooquickly:forexample,inSection4.3.1,wedescribedhowsomeapplications(e.g.,autonomous
vehicles)requireseparateteamstomanageevaluation,makingsurethatbadmodelsdon’tget
promotedfromdevelopmenttoproduction.Moreover,whenmeasuringMLmetricsoutsideof
accuracy,e.g.,fairness[31]orproductmetrics(Section4.3.2),itischallengingtomakesureall
metricsimproveforeachchangetotheMLpipeline[71].Understandingwhichmetricstoprioritize
requiresdomainandbusinessexpertise[40],whichcouldhindervelocity.
5.1.2 Visibility. In organizations, since many stakeholders and teams are impacted by ML-
powered applications and services, it is important for MLEs to have an end-to-end view of
ML pipelines. P1 explicitly mentioned integrating “very high degree of observability into ev-
erypartof[the]pipeline”(Section4.4.3).Priorworkdescribestheimportanceofvisibility:for
example,telemetrydatafromMLpipelines(e.g.,logsandtraces)canhelpengineersknowifthe
pipelineisbroken[40],modelexplainabilitymethodscanestablishcustomers’trustinMLpredic-
tions[42,71,102],anddashboardsonMLpipelinehealthcanhelpalignnontechnicalstakeholders
withengineers[37,46].Inourview,thepopularityofJupyternotebooksamongMLEs,including
amongtheparticipantsinourstudy,canbeexplainedbyJupyter’sgainsinvelocityandvisibility
forMLexperimentation,asiteffectivelycombinesREPL(Read-Eval-Print-Loop)-styleinteraction
andvisualizationcapabilitiesdespiteitsversioningshortcomings.Ourfindingscorroboratethese
priorfindingsandprovidefurtherinsightonhowvisibilityisachievedinpractice.Forexample,
engineersproactivelymonitorfeedbackdelays(Section4.1.3).Theyalsodocumentlivefailures
frequentlytokeepvalidationdatasetscurrent(Section4.3.1),andtheyengageinon-callrotations
toinvestigatedataqualityalerts(Section4.4).
Visibilityalsohelpswithvelocity.Ifengineerscanquicklyidentifythesourceofabug,they
can fix it faster. Or, if other stakeholders, such as product managers or business analysts, can
understandhowanexperimentormulti-stageddeploymentisprogressing,theycanbetterusetheir
domainknowledgetoassessmodelsaccordingtoproductmetrics(seeSection4.3.2),andintervene
soonerifthere’sevidenceofaproblem.Oneofthepainpointsweobservedwasthatend-to-end
experimentation—fromtheconceptionofanideatoimproveMLperformancetovalidationofthe
idea—tooktoolong.Theuncertaintyofprojectsuccessstemsfromtheunpredictable,real-world
natureofexperiments.
5.1.3 Versioning. Amershietal.[3]mentionthat“fast-pacedmodeliteration”requirescareful
versioningofdataandcode.Otherworkidentifiesaneedtoalsomanagemodelversions[101,109].
Ourworksuggeststhatmanangingallartifacts—data,code,models,dataqualitymetrics,filters,
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.“Wehavenoideahowmodelswillbehaveinproductionuntilproduction”:HowengineersoperationalizeML 206:25
rules—in tandem is extremely challenging but vital to the success of ML deployments. Prior
workexplainshowtheseartifactscanbequeriedduringdebugging[7,11,87],andourfindings
additionallyshowthatversioningisparticularlyusefulwhenteamsofpeopleworkonMLpipelines.
For instance, during monitoring, on-call engineers may receive a flood of false-positive alerts;
lookingatoldalertsmighthelpthemunderstandwhetheraspecifictypeofalertactuallyrequires
action. In another example, during experimentation, ML engineers often work on models and
pipelines they didn’t initially create. Versioning increases visibility: engineers can inspect old
versionsofexperimentstounderstandideasthatmayormaynothaveworked.
Notonlydoesversioningaidvisibility,butitalsoenablesworkflowstomaintainhighvelocity.
InSection4.4.2,weexplainedhowpipelinejunglesarecreatedbyquicklyrespondingtoMLbugs
byconstructingvariousfiltersandrules.Ifengineershadtofixthetrainingdatasetormodelfor
everybug,theywouldnotbeabletoiteratequickly.Maintainingdifferentversionsfordifferent
typesofinputs(e.g.,rulestoauto-rejectincompletedataordifferentmodelsfordifferentusers)
alsoenableshighvelocity.However,thereisalsoatensionbetweenvelocityandversioning:in
Section4.2.3,wediscussedhowparallelizingexperimentideasproducesmanyversions,andML
engineerscouldnotcognitivelykeeptrackofthem.Inotherwords,havinghighvelocitycanmean
drowninginaseaofversions.
5.2 OpportunitiesforMLTooling
OurmaintakeawayisthatproductionMLtoolingneedstoaidhumansintheirworkflows,not
justautomatetechnicalpractices(e.g.,generatingafeatureortrainingamodel).Toolsshouldhelp
improve at least one of the three Vs, and there are opportunities for tools in each stage of the
workflow.Wediscusseachinturn.
5.2.1 DataPreparation. AsmentionedinSection4.1,separateteamsofdataengineerstypically
managepipelinestoingest,clean,andpreprocessdataonaschedule.Whileexistingtoolsautomate
schedulingtheseactivities,thereareunadressedMLneedsaroundretrainingandlabeling.Prior
workandourinterviewsindicatethatMLengineersretrainmodelsonsomearbitrarycadence[44,
71],withoutunderstandingtheeffectofthecadenceonthequalityofpredictions.Modelsmight
bestaleiftheyareretrainedonlymonthly,ortheymightretrainusinginvalidorcorruptdataif
theyareretrainedfasterthanthedataisvalidatedandcleaned(e.g.,hourly).Moreover,theoptimal
retrainingcadencedependsonthedata,MLtask,andamountoforganizationalresources,such
ascompute,trainingtime,andnumberofengineersontheteam.Newtoolscanhelpwiththese
challengesanddeterminethebestretrainingcadenceforMLpipelines.Withrespecttolabeling,
existingtoolshelpwitheitherlabelingatscale[79]orlabelingwithhighquality[45],butitis
hardtoachieveboth.Asaresult,organizationshavecustominfrastructureandteamstoresolve
labelmismatches,applydomainknowledge,andrejectincorrectlabels.Labelingtoolscanleverage
ensemblingandaddpostprocessingfilterstorejectandresolveincorrectandinconsistentlabels.
Moreover,theyshouldtrackfeedbackdelaysandsurfacethisinformationtousers.
5.2.2 Experimentation. AsdiscussedinSection4.2.3,experimentsaretypicallydoneindevelopment
environmentsandthenpromotedtoproductionclustersduringdeployment.Themismatchbetween
the two (or more!) environments can cause bugs, creating an opportunity for new tools. The
development cluster should maximize iteration speed (velocity), while the production cluster
shouldminimizeend-userpredictionlatency[14].Hardwareandsoftwarecanbedifferentineach
cluster,e.g.,GPUsfortrainingvs.CPUsforinference,andPythonvs.C++,whichmakesthisproblem
challenging.Newtoolsareprioritizingreproducibility—turningJupyternotebooksintoproduction
scripts[90],forinstance—butshouldalsostandardizehowengineersinteractwithexperimentation
workflows.Forexample,whileexperimenttrackingtoolscanliterallykeeptrackofthousands
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.206:26 Shankar&Garciaetal.
ofexperiments,howcanengineerssortthroughalltheseversionsandactuallyunderstandwhat
thebestexperimentsaredoing?Ourfindingsandpriorworkshowthattheexperimentalnature
ofMLanddatascienceleadstoundocumentedtribalknowledgewithinorganizations[37,41].
Documentationsolutionsfordeployedmodelsanddatasetshavebeenproposed[23,60],butwesee
anopportunityfortoolstohelpdocumentexperiments—particularly,failedones.Forcingengineers
towritedowninstitutionalknowledgeaboutwhatideasworkordon’tworkslowsthemdown,
andautomateddocumentationassistancewouldbequiteuseful.
5.2.3 EvaluationandDeployment. Priorworkhasidentifiedseveralopportunitiesintheevaluation
anddeploymentspace.Forexample,thereisaneedtomapMLmetricgainstoproductorbusiness
gains[40,44,57].Additionally,toolscouldhelpdefineandcalculatesubpopulation-specificperfor-
mancemetrics[31].Fromourstudy,wehaveobservedaneedfortoolingaroundthemulti-staged
deploymentprocess.Withmultiplestages,theturnaroundtimefromexperimentideatohaving
afullproductiondeployment(i.e.,deployedtoallusers)cantakeseveralmonths.Invalidating
ideasinearlierstagesofdeploymentcanincreaseoverall,end-to-endvelocity.Ourinterviewees
discussedhowsomefeatureideasnolongermakesenseafterafewmonths,giventhenatureof
howuserbehaviorschange,whichwouldcauseaninitiallygoodideatoneverfullyandfinally
deploytoproduction.Additionally,anorganization’skeyproductmetrics—e.g.,revenueornumber
ofclicks—mightchangeinthemiddleofamulti-stagedeployment,killingthedeployment.This
negativelyimpactstheengineersresponsibleforthedeployment.Weseethisasanopportunityfor
newtoolstostreamlineMLdeploymentsinthismulti-stagepattern,tominimizewastedworkand
helppractitionerspredicttheend-to-endgainsfortheirideas.
5.2.4 MonitoringandResponse. RecentworkinMLobservabilityidentifiesaneedfortoolsto
giveend-to-endvisibilityonMLpipelinebehavioranddebugMLissuesfaster[7,91].Basicdata
qualitystatistics,suchasmissingdataandtypeorschemachecks,failtocaptureanomaliesin
thevaluesofdata[8,62,76].Ourintervieweescomplainedthatexistingtoolsthatattempttoflag
anomaliesinthevaluesofdatapointsproducetoomanyfalsepositives(Section4.4.1).Anexcessive
numberoffalse-positivealerts,i.e.,datapointsflaggedasinvalideveniftheyarevalid,leadstotwo
painpoints:(1)unnecessarilymaintainingmanymodelversionsorsimpleheuristicsforinvalid
datapoints,whichcanbehardtokeeptrackof,and(2)aloweroverallaccuracyorMLmetric,
asbaselinemodelsmightnotservehigh-qualitypredictionsfortheseinvalidpoints.Moreover,
duetofeedbackdelays,itmaynotbepossibletotrackMLperformance(e.g.,accuracy)inreal
time.Whatmetricscanbereliablymonitoredinrealtime,andwhatcriteriashouldtriggeralertsto
maximizeprecisionandrecallwhenidentifyingmodelperformancedrops?Howcanthesemetrics
andalertingcriteriaautomaticallytunethemselvesovertime,astheunderlyingdatachanges?We
envisionthistobeanopportunityfornewdatamanagementtools.
Moreover,asdiscussedinSection4.4.2,whenengineersquicklyrespondtoproductionbugs,
theycreatepipelinejungles.Suchjunglestypicallyconsistofseveralversionsofmodels,rules,
andfilters.MostoftheMLpipelinesthatourintervieweesdiscussedwerepipelinejungles.This
combinationofmodernmodel-drivenMLandold-fashionedrule-basedAIindicatesaneedfor
managingfilters(andversionsoffilters)inadditiontomanaginglearnedmodels.Theengineers
weinterviewedmanagedtheseartifactsthemselves.
5.3 LimitationsandFutureWork
SincewewantedtofindcommonthemesinproductionMLworkflowsacrossdifferentapplications
andorganizations,ourinterviewstudy’sscopewasquitebroad:wesetoutonaquesttodiscover
sharedpatterns,ratherthantopredictorexplain.Weaskedpractitionersopen-endedquestions
abouttheirMLOpsworkflowsandchallenges,butdidnotprobethemaboutquestionsoffairness,
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.“Wehavenoideahowmodelswillbehaveinproductionuntilproduction”:HowengineersoperationalizeML 206:27
risk,anddatagovernance:thesequestionscouldbestudiedinfutureinterviews.Moreover,we
didnotfocusonthedifferencesbetweenpractitioners’workflowsbasedontheircompanysizes,
educationalbackgrounds,orindustries.Whilethereareinterviewstudiesforspecificapplicationsof
ML[6,19,77],weseefurtheropportunitiestostudytheeffectoforganizationalfocusandmaturity
ontheproductionMLworkflow.Therearealsoquestionsforwhichinterviewstudiesareapoorfit.
GivenourfindingsontheimportanceofcollaborativeandsocialdimensionsofMLOps,wewould
liketoexploretheseideasfurtherthroughparticipantactionresearchorcontextualinquiry.
Moreover,ourpaperfocusesonahuman-centeredworkflowsurroundingproductionMLpipelines.
Focusingontheautomated workflowsinMLpipelines—forexample,continuousintegrationand
continuousdeployment(CI/CD)—couldproveafruitfulresearchdirection.Finally,weonlyinter-
viewedMLengineers,nototherstakeholders,suchassoftwareengineersorproductmanagers.
Kreuzbergeretal.[44]presentadiagramoftechnicalcomponentsoftheMLpipeline(e.g.,feature
engineering,modeltraining)andinteractionsbetweenMLengineersandotherstakeholders.An-
otherinterviewstudycouldobservetheseinteractionsandprovidefurtherinsightintopractitioners’
workflows.
6 CONCLUSION
Inthispaper,wepresentedresultsfromasemi-structuredinterviewstudyof18MLengineers
spanningdifferentorganizationsandapplicationstounderstandtheirworkflow,bestpractices,
andchallenges.Engineersreportedseveralstrategiestosustainandimprovetheperformanceof
productionMLpipelines,andweidentifiedfourstagesoftheirMLOpsworkflow:i)datapreparation,
ii)experimentation,iii)evaluationanddeployment,andiv)monitoringandresponse.Throughout
thesestages,wefoundthatsuccessfulMLOpspracticescenteraroundhavinggoodvelocity,visibility,
andversioning.Finally,wediscussedopportunitiesfortooldevelopmentandresearch.
ACKNOWLEDGMENTS
We acknowledge support from grants DGE2243822, IIS-2129008, IIS-1940759, and IIS-1940757
awarded by the National Science Foundation, an NDSEG Fellowship, funds from the Alfred P.
SloanFoundation,aswellasEPIClabsponsors:GResearch,Adobe,Microsoft,Google,andSigma
Computing.
REFERENCES
[1] Leonel Aguilar, David Dao, Shaoduo Gan, Nezihe Merve Gurel, Nora Hollenstein, Jiawei Jiang, Bojan Kar-
las, Thomas Lemmin, Tian Li, Yang Li, Susie Rao, Johannes Rausch, Cedric Renggli, Luka Rimanic, Mau-
rice Weber, Shuai Zhang, Zhikuan Zhao, Kevin Schawinski, Wentao Wu, and Ce Zhang. 2021. Ease.ML: A
Lifecycle Management System for MLDev and MLOps. In Conference on Innovative Data Systems Research
(CIDR2021). https://www.microsoft.com/en-us/research/publication/ease-ml-a-lifecycle-
management-system-for-mldev-and-mlops/
[2] SridharAllaandSumanKalyanAdari.2021.Whatismlops?InBeginningMLOpswithMLFlow.Springer,79–124.
[3] SaleemaAmershi,AndrewBegel,ChristianBird,RobertDeLine,HaraldGall,EceKamar,NachiappanNagappan,
BesmiraNushi,andThomasZimmermann.2019.SoftwareEngineeringforMachineLearning:ACaseStudy.In2019
IEEE/ACM41stInternationalConferenceonSoftwareEngineering:SoftwareEngineeringinPractice(ICSE-SEIP).291–300.
https://doi.org/10.1109/ICSE-SEIP.2019.00042
[4] Anonymous.2021. MLReproducibilitySystems:StatusandResearchAgenda. https://openreview.net/
forum?id=v-6XBItNld2
[5] AmitabhaBanerjee,Chien-ChiaChen,Chien-ChunHung,XiaoboHuang,YifanWang,andRazvanChevesaran.2020.
ChallengesandExperienceswith{MLOps}forPerformanceDiagnosticsin{Hybrid-Cloud}EnterpriseSoftware
Deployments.In2020USENIXConferenceonOperationalMachineLearning(OpML20).
[6] CatherineBillington,GonzaloRivero,AndrewJannett,andJiatingChen.2022.AMachineLearningModelHelps
ProcessInterviewerCommentsinComputer-assistedPersonalInterviewInstruments:ACaseStudy.FieldMethods
(2022),1525822X221107053.
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.206:28 Shankar&Garciaetal.
[7] MikeBrachmann,CarlosBautista,SoniaCastelo,SuFeng,JulianaFreire,BorisGlavic,OliverKennedy,HeikoMüeller,
RémiRampin,WilliamSpoth,andYingYang.2019.DataDebuggingandExplorationwithVizier.InProceedingsof
the2019InternationalConferenceonManagementofData(Amsterdam,Netherlands)(SIGMOD’19).Associationfor
ComputingMachinery,NewYork,NY,USA,1877–1880. https://doi.org/10.1145/3299869.3320246
[8] EricBreck,MartyZinkevich,NeoklisPolyzotis,StevenWhang,andSudipRoy.2019.DataValidationforMachine
Learning.InProceedingsofSysML. https://mlsys.org/Conferences/2019/doc/2019/167.pdf
[9] StevenPCallahan,JulianaFreire,EmanueleSantos,CarlosEScheidegger,CláudioTSilva,andHuyTVo.2006.
VisTrails:visualizationmeetsdatamanagement.InProceedingsofthe2006ACMSIGMODinternationalconferenceon
Managementofdata.745–747.
[10] PeteChapman,JulianClinton,RandyKerber,ThomasKhabaza,ThomasReinartz,ColinShearer,andRüdigerWirth.
1999.TheCRISP-DMuserguide.In4thCRISP-DMSIGWorkshopinBrusselsinMarch,Vol.1999.sn.
[11] AmitChavan,SiluHuang,AmolDeshpande,AaronElmore,SamuelMadden,andAdityaParameswaran.2015.
Towardsaunifiedquerylanguageforprovenanceandversioning.In7th{USENIX}WorkshopontheTheoryand
PracticeofProvenance(TaPP15).
[12] JiYoungChoandEun-HeeLee.2014.Reducingconfusionaboutgroundedtheoryandqualitativecontentanalysis:
Similaritiesanddifferences.Qualitativereport19,32(2014).
[13] DavidCohen,MikaelLindvall,andPatriciaCosta.2004.Anintroductiontoagilemethods.Adv.Comput.62,03(2004),
1–66.
[14] DanielCrankshaw,XinWang,GuilioZhou,MichaelJFranklin,JosephEGonzalez,andIonStoica.2017.Clipper:A
{Low-Latency}OnlinePredictionServingSystem.In14thUSENIXSymposiumonNetworkedSystemsDesignand
Implementation(NSDI17).613–627.
[15] JohnWCreswellandCherylNPoth.2016.Qualitativeinquiryandresearchdesign:Choosingamongfiveapproaches.
Sagepublications.
[16] SusanBDavidsonandJulianaFreire.2008.Provenanceandscientificworkflows:challengesandopportunities.In
Proceedingsofthe2008ACMSIGMODinternationalconferenceonManagementofdata.1345–1350.
[17] ChristofEbert,GorkaGallardo,JosuneHernantes,andNicolasSerrano.2016. DevOps. IeeeSoftware33,3(2016),
94–100.
[18] MihailEric.[n.d.].MLOpsisamessbutthat’stobeexpected. https://www.mihaileric.com/posts/mlops-
is-a-mess/
[19] AsbjørnFølstad,CecilieBertinussenNordheim,andCatoAlexanderBjørkli.2018.Whatmakesuserstrustachatbotfor
customerservice?Anexploratoryinterviewstudy.InInternationalconferenceoninternetscience.Springer,194–208.
[20] RolandoGarcia,EricLiu,VikramSreekanti,BobbyYan,AnushaDandamudi,JosephE.Gonzalez,JosephMHellerstein,
andKoushikSen.2021.HindsightLoggingforModelTraining.InVLDB.
[21] RolandoGarcia,VikramSreekanti,NeerajaYadwadkar,DanielCrankshaw,JosephEGonzalez,andJosephMHeller-
stein.2018.Context:Themissingpieceinthemachinelearninglifecycle.InCMI.
[22] SatvikGarg,PradyumnPundir,GeetanjaliRathee,P.K.Gupta,SomyaGarg,andSaranshAhlawat.2021.OnContinuous
Integration/ContinuousDeliveryforAutomatedDeploymentofMachineLearningModelsusingMLOps.In2021
IEEEFourthInternationalConferenceonArtificialIntelligenceandKnowledgeEngineering(AIKE).25–28. https:
//doi.org/10.1109/AIKE52691.2021.00010
[23] TimnitGebru,JamieMorgenstern,BrianaVecchione,JenniferWortmanVaughan,HannaWallach,HalDauméIii,
andKateCrawford.2021.Datasheetsfordatasets.Commun.ACM64,12(2021),86–92.
[24] Samadrita Ghosh. 2021. Mlops challenges and how to face them. https://neptune.ai/blog/mlops-
challenges-and-how-to-face-them
[25] TuomasGranlund,AleksiKopponen,VladStirbu,LalliMyllyaho,andTommiMikkonen.2021.MLOpschallengesin
multi-organizationsetup:Experiencesfromtworeal-worldcases.In2021IEEE/ACM1stWorkshoponAIEngineering-
SoftwareEngineeringforAI(WAIN).IEEE,82–88.
[26] GregGuest,ArwenBunce,andLauraJohnson.2006.Howmanyinterviewsareenough?Anexperimentwithdata
saturationandvariability.Fieldmethods18,1(2006),59–82.
[27] PhilipJ.Guo,SeanKandel,JosephM.Hellerstein,andJeffreyHeer.2011. ProactiveWrangling:Mixed-Initiative
End-UserProgrammingofDataTransformationScripts.InProceedingsofthe24thAnnualACMSymposiumonUser
InterfaceSoftwareandTechnology(SantaBarbara,California,USA)(UIST’11).AssociationforComputingMachinery,
NewYork,NY,USA,65–74. https://doi.org/10.1145/2047196.2047205
[28] JosephMHellerstein,VikramSreekanti,JosephEGonzalez,JamesDalton,AkonDey,SreyashiNag,KrishnaRa-
machandran,SudhanshuArora,ArkaBhattacharyya,ShirshankaDas,etal.2017.Ground:ADataContextService..
InCIDR.
[29] CharlesHill,RachelK.E.Bellamy,ThomasErickson,andMargaretM.Burnett.2016. Trialsandtribulationsof
developersofintelligentsystems:Afieldstudy. 2016IEEESymposiumonVisualLanguagesandHuman-Centric
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.“Wehavenoideahowmodelswillbehaveinproductionuntilproduction”:HowengineersoperationalizeML 206:29
Computing(VL/HCC)(2016),162–170.
[30] FredHohman,KanitWongsuphasawat,MaryBethKery,andKayurPatel.2020.Understandingandvisualizingdata
iterationinmachinelearning.InProceedingsofthe2020CHIconferenceonhumanfactorsincomputingsystems.1–13.
[31] KennethHolstein,JenniferWortmanVaughan,HalDaumé,MiroDudik,andHannaWallach.2019. Improving
FairnessinMachineLearningSystems:WhatDoIndustryPractitionersNeed?.InProceedingsofthe2019CHI
ConferenceonHumanFactorsinComputingSystems-CHI’19.ACMPress,Glasgow,ScotlandUk,1–16. https:
//doi.org/10.1145/3290605.3300830
[32] YouyangHouandDakuoWang.2017.HackingwithNPOs:CollaborativeAnalyticsandBrokerRolesinCivicData
Hackathons.Proc.ACMHum.-Comput.Interact.1,CSCW,Article53(dec2017),16pages. https://doi.org/10.
1145/3134688
[33] DuncanHull,KatyWolstencroft,RobertStevens,CaroleGoble,MathewRPocock,PeterLi,andTomOinn.2006.
Taverna:atoolforbuildingandrunningworkflowsofservices.Nucleicacidsresearch34,suppl_2(2006),W729–W732.
[34] ChipHuyen.2020.MachinelearningtoolslandscapeV2(+84newtools). https://huyenchip.com/2020/12/
30/mlops-v2.html
[35] MeenuMaryJohn,HelenaHolmströmOlsson,andJanBosch.2021. Towardsmlops:Aframeworkandmaturity
model.In202147thEuromicroConferenceonSoftwareEngineeringandAdvancedApplications(SEAA).IEEE,1–8.
[36] SeanKandel,AndreasPaepcke,JosephHellerstein,andJeffreyHeer.2011.Wrangler:Interactivevisualspecification
ofdatatransformationscripts.InProceedingsoftheSIGCHIConferenceonHumanFactorsinComputingSystems.
3363–3372.
[37] SeanKandel,AndreasPaepcke,JosephM.Hellerstein,andJeffreyHeer.2012. EnterpriseDataAnalysisandVisu-
alization:AnInterviewStudy.IEEETransactionsonVisualizationandComputerGraphics18,12(2012),2917–2926.
https://doi.org/10.1109/TVCG.2012.219
[38] DanielKang,DeeptiRaghavan,PeterBailis,andMateiZaharia.[n.d.]. Modelassertionsfordebuggingmachine
learning.
[39] MaryBethKery,AmberHorvath,andBradAMyers.2017.Variolite:SupportingExploratoryProgrammingbyData
Scientists..InCHI,Vol.10.3025453–3025626.
[40] MiryungKim,ThomasZimmermann,RobertDeLine,andAndrewBegel.2016.TheEmergingRoleofDataScientists
onSoftwareDevelopmentTeams.InProceedingsofthe38thInternationalConferenceonSoftwareEngineering(Austin,
Texas)(ICSE’16).AssociationforComputingMachinery,NewYork,NY,USA,96–107. https://doi.org/10.
1145/2884781.2884783
[41] MiryungKim,ThomasZimmermann,RobertDeLine,andAndrewBegel.2017.Datascientistsinsoftwareteams:
Stateoftheartandchallenges.IEEETransactionsonSoftwareEngineering44,11(2017),1024–1038.
[42] JanisKlaise,ArnaudVanLooveren,CliveCox,GiovanniVacanti,andAlexandruCoca.2020. Monitoringand
explainabilityofmodelsinproduction.ArXivabs/2007.06299(2020).
[43] JohannesKösterandSvenRahmann.2012.Snakemake—ascalablebioinformaticsworkflowengine.Bioinformatics
28,19(2012),2520–2522.
[44] DominikKreuzberger,NiklasKühl,andSebastianHirschl.2022.MachineLearningOperations(MLOps):Overview,
Definition,andArchitecture. https://doi.org/10.48550/ARXIV.2205.02302
[45] SanjayKrishnanandEugeneWu.2017.PALM:MachineLearningExplanationsForIterativeDebugging.InProceedings
ofthe2ndWorkshoponHuman-In-the-LoopDataAnalytics-HILDA’17.ACMPress,Chicago,IL,USA,1–6. https:
//doi.org/10.1145/3077257.3077271
[46] SeanKrossandPhilipGuo.2021.Orienting,Framing,Bridging,Magic,andCounseling:HowDataScientistsNavigate
theOuterLoopofClientCollaborationsinIndustryandAcademia.Proc.ACMHum.-Comput.Interact.5,CSCW2,
Article311(oct2021),28pages. https://doi.org/10.1145/3476052
[47] SeanKrossandPhilipJGuo.2019. Practitionersteachingdatascienceinindustryandacademia:Expectations,
workflows,andchallenges.InProceedingsofthe2019CHIconferenceonhumanfactorsincomputingsystems.1–14.
[48] IndikaKumara,RowanArts,DarioDiNucci,WillemJanVanDenHeuvel,andDamianAndrewTamburri.2022.
RequirementsandReferenceArchitectureforMLOps:InsightsfromIndustry.(2022).
[49] SampoKuutti,R.Bowden,YaochuJin,PhilBarber,andSaberFallah.2021.ASurveyofDeepLearningApplications
toAutonomousVehicleControl.IEEETransactionsonIntelligentTransportationSystems22(2021),712–733.
[50] AngelaLee,DorisXin,DorisLee,andAdityaParameswaran.2020. DemystifyingaDarkArt:Understanding
Real-WorldMachineLearningModelDevelopment. https://doi.org/10.48550/ARXIV.2005.01520
[51] ChengHanLee.2020.3datacareersdecodedandwhatitmeansforyou. https://www.udacity.com/blog/
2014/12/data-analyst-vs-data-scientist-vs-data-engineer.html
[52] LeonardoLeite,CarlaRocha,FabioKon,DejanMilojicic,andPauloMeirelles.2019.AsurveyofDevOpsconcepts
andchallenges.ACMComputingSurveys(CSUR)52,6(2019),1–35.
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.206:30 Shankar&Garciaetal.
[53] AndersonLima,LucianoMonteiro,andAnaPaulaFurtado.2022.MLOps:Practices,MaturityModels,Roles,Tools,
andChallenges-ASystematicLiteratureReview.ICEIS(1)(2022),308–320.
[54] ZhiqiuLin,JiaShi,DeepakPathak,andDevaRamanan.2021. TheCLEARBenchmark:ContinualLEArningon
Real-WorldImagery.InThirty-fifthConferenceonNeuralInformationProcessingSystemsDatasetsandBenchmarks
Track(Round2). https://openreview.net/forum?id=43mYF598ZDB
[55] MikeLoukides.2012.WhatisDevOps? "O’ReillyMedia,Inc.".
[56] LucyEllenLwakatare,TerhiKilamo,TeemuKarvonen,TanjaSauvola,VilleHeikkilä,JuhaItkonen,PasiKuvaja,
TommiMikkonen,MarkkuOivo,andCasperLassenius.2019. DevOpsinpractice:Amultiplecasestudyoffive
companies.InformationandSoftwareTechnology114(2019),217–230.
[57] MichaelA.Madaio,LukeStark,JenniferWortmanVaughan,andHannaWallach.2020. Co-DesigningChecklists
toUnderstandOrganizationalChallengesandOpportunitiesaroundFairnessinAI.InProceedingsofthe2020CHI
ConferenceonHumanFactorsinComputingSystems(Honolulu,HI,USA)(CHI’20).AssociationforComputing
Machinery,NewYork,NY,USA,1–14. https://doi.org/10.1145/3313831.3376445
[58] SasuMäkinen,HenrikSkogström,EeroLaaksonen,andTommiMikkonen.2021. WhoneedsMLOps:Whatdata
scientistsseektoaccomplishandhowcanMLOpshelp?.In2021IEEE/ACM1stWorkshoponAIEngineering-Software
EngineeringforAI(WAIN).IEEE,109–112.
[59] BeatrizMAMatsuiandDeniseHGoya.2022.MLOps:fivestepstoguideitseffectiveimplementation.InProceedings
ofthe1stInternationalConferenceonAIEngineering:SoftwareEngineeringforAI.33–34.
[60] MargaretMitchell,SimoneWu,AndrewZaldivar,ParkerBarnes,LucyVasserman,BenHutchinson,ElenaSpitzer,
InioluwaDeborahRaji,andTimnitGebru.2019.Modelcardsformodelreporting.InProceedingsoftheconferenceon
fairness,accountability,andtransparency.220–229.
[61] MLReef.2021.GlobalmlopsandMLToolsLandscape:Mlreef. https://about.mlreef.com/blog/global-
mlops-and-ml-tools-landscape/
[62] AkshayNareshModietal.2017.TFX:ATensorFlow-BasedProduction-ScaleMachineLearningPlatform.InKDD
2017.
[63] JoseG.Moreno-Torres,TroyRaeder,RocíoAlaiz-Rodríguez,NiteshV.Chawla,andFranciscoHerrera.2012. A
unifyingviewondatasetshiftinclassification.PatternRecognition45,1(2012),521–530. https://doi.org/10.
1016/j.patcog.2011.06.019
[64] DennisMuiruri,LucyEllenLwakatare,JukkaKNurminen,andTommiMikkonen.2022.PracticesandInfrastructures
forMLSystems–AnInterviewStudyinFinnishOrganizations.(2022).
[65] MichaelMuller.2014. Curiosity,creativity,andsurpriseasanalytictools:Groundedtheorymethod. InWaysof
KnowinginHCI.Springer,25–48.
[66] MichaelMuller,IngridLange,DakuoWang,DavidPiorkowski,JasonTsay,QVeraLiao,CaseyDugan,andThomas
Erickson.2019.Howdatascienceworkersworkwithdata:Discovery,capture,curation,design,creation.InProceedings
ofthe2019CHIconferenceonhumanfactorsincomputingsystems.1–15.
[67] MohammadHosseinNamaki,AvriliaFloratou,FotisPsallidas,SubruKrishnan,AshvinAgrawal,YinghuiWu,Yiwen
Zhu,andMarkusWeimer.2020.Vamsa:Automatedprovenancetrackingindatasciencescripts.InProceedingsofthe
26thACMSIGKDDInternationalConferenceonKnowledgeDiscovery&DataMining.1542–1551.
[68] AndrewNg,EddyShyu,AartiBagul,andGeoffLadwig.[n.d.].Evaluatingamodel-adviceforapplyingmachine
learning. https://www.coursera.org/lecture/advanced-learning-algorithms/evaluating-a-
model-26yGi
[69] YanivOvadia,EmilyFertig,J.Ren,ZacharyNado,D.Sculley,SebastianNowozin,JoshuaV.Dillon,BalajiLakshmi-
narayanan,andJasperSnoek.2019. CanYouTrustYourModel’sUncertainty?EvaluatingPredictiveUncertainty
UnderDatasetShift.InNeurIPS.
[70] CosminPaduraru,DanielJ.Mankowitz,GabrielDulac-Arnold,JerryLi,NirLevine,SvenGowal,andToddHester.
2021. ChallengesofReal-WorldReinforcementLearning:Definitions,Benchmarks&Analysis. MachineLearning
Journal(2021).
[71] AndreiPaleyes,Raoul-GabrielUrma,andNeilD.Lawrence.2022. ChallengesinDeployingMachineLearning:A
SurveyofCaseStudies.ACMComput.Surv.(apr2022). https://doi.org/10.1145/3533378JustAccepted.
[72] SamirPassiandStevenJ.Jackson.2017.DataVision:LearningtoSeeThroughAlgorithmicAbstraction.Proceedings
ofthe2017ACMConferenceonComputerSupportedCooperativeWorkandSocialComputing(2017).
[73] SamirPassiandStevenJJackson.2018. Trustindatascience:Collaboration,translation,andaccountabilityin
corporatedatascienceprojects.ProceedingsoftheACMonHuman-ComputerInteraction2,CSCW(2018),1–28.
[74] KayurPatel,JamesFogarty,JamesA.Landay,andBeverlyL.Harrison.2008.Investigatingstatisticalmachinelearning
asatoolforsoftwaredevelopment.InInternationalConferenceonHumanFactorsinComputingSystems.
[75] NeoklisPolyzotis,SudipRoy,StevenEuijongWhang,andMartinZinkevich.2017. Datamanagementchallenges
inproductionmachinelearning.InProceedingsofthe2017ACMInternationalConferenceonManagementofData.
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.“Wehavenoideahowmodelswillbehaveinproductionuntilproduction”:HowengineersoperationalizeML 206:31
1723–1726.
[76] NeoklisPolyzotis,SudipRoy,StevenEuijongWhang,andMartinZinkevich.2018. DataLifecycleChallengesin
ProductionMachineLearning:ASurvey.SIGMODRecord47,2(2018),12.
[77] LuisaPumplun,MariskaFecho,NihalWahl,FelixPeters,PeterBuxmann,etal.2021.Adoptionofmachinelearning
systemsformedicaldiagnosticsinclinics:qualitativeinterviewstudy. JournalofMedicalInternetResearch23,10
(2021),e29301.
[78] StephanRabanser,StephanGünnemann,andZacharyChaseLipton.2019.FailingLoudly:AnEmpiricalStudyof
MethodsforDetectingDatasetShift.InNeurIPS.
[79] AlexanderRatner,StephenHBach,HenryEhrenberg,JasonFries,SenWu,andChristopherRé.2017.Snorkel:Rapid
trainingdatacreationwithweaksupervision.InProceedingsoftheVLDBEndowment.InternationalConferenceon
VeryLargeDataBases,Vol.11.NIHPublicAccess,269.
[80] GilbertoRecupito,FabianoPecorelli,GemmaCatolino,SergioMoreschini,DarioDiNucci,FabioPalomba,and
DamianATamburri.2022. Amultivocalliteraturereviewofmlopstoolsandfeatures.In202248thEuromicro
ConferenceonSoftwareEngineeringandAdvancedApplications(SEAA).IEEE,84–91.
[81] CedricRenggli,LukaRimanic,NeziheMerveGürel,BojanKarlaš,WentaoWu,andCeZhang.2021.Adataquality-
drivenviewofmlops.arXivpreprintarXiv:2102.07750(2021).
[82] YujiRoh,GeonHeo,andStevenEuijongWhang.2019. ASurveyonDataCollectionforMachineLearning:A
BigData-AIIntegrationPerspective.IEEETransactionsonKnowledgeandDataEngineering(2019),1–1. https:
//doi.org/10.1109/TKDE.2019.2946162 ConferenceName:IEEETransactionsonKnowledgeandData
Engineering.
[83] PhilippRuf,ManavMadan,ChristophReich,andDjaffarOuld-Abdeslam.2021.Demystifyingmlopsandpresentinga
recipefortheselectionofopen-sourcetools.AppliedSciences11,19(2021),8861.
[84] LukasRupprecht,JamesCDavis,ConstantineArnold,YanivGur,andDeepavaliBhagwat.2020.Improvingrepro-
ducibilityofdatasciencepipelinesthroughtransparentprovenancecapture.ProceedingsoftheVLDBEndowment13,
12(2020),3354–3368.
[85] NithyaSambasivan,ShivaniKapania,HannahHighfill,DianaAkrong,PraveenParitosh,andLoraMAroyo.2021.
“Everyonewantstodothemodelwork,notthedatawork”:DataCascadesinHigh-StakesAI.Inproceedingsofthe
2021CHIConferenceonHumanFactorsinComputingSystems.1–15.
[86] SebastianSchelteretal.2018.AutomatingLarge-ScaleDataQualityVerification.InPVLDB’18.
[87] D.Sculley,GaryHolt,DanielGolovin,EugeneDavydov,ToddPhillips,DietmarEbner,VinayChaudhary,Michael
Young,Jean-FrançoisCrespo,andDanDennison.2015. HiddenTechnicalDebtinMachineLearningSystems.In
NIPS.
[88] AlexSerban,KoenvanderBlom,HolgerHoos,andJoostVisser.2020.AdoptionandEffectsofSoftwareEngineering
BestPracticesinMachineLearning.InProceedingsofthe14thACM/IEEEInternationalSymposiumonEmpirical
SoftwareEngineeringandMeasurement(ESEM)(Bari,Italy)(ESEM’20).AssociationforComputingMachinery,New
York,NY,USA,Article3,12pages. https://doi.org/10.1145/3382494.3410681
[89] ShreyaShankar,BerneaseHerman,andAdityaG.Parameswaran.2022.RethinkingStreamingMachineLearning
Evaluation.ArXivabs/2205.11473(2022).
[90] ShreyaShankar,StephenMacke,SarahChasins,AndrewHead,andAdityaParameswaran.2023.Bolt-on,Compact,
andRapidProgramSlicingforNotebooks.Proc.VLDBEndow.(sep2023).
[91] ShreyaShankarandAdityaG.Parameswaran.2022.TowardsObservabilityforProductionMachineLearningPipelines.
ArXivabs/2108.13557(2022).
[92] JamesPSpradley.2016.Theethnographicinterview.WavelandPress.
[93] MeghaSrivastava,BesmiraNushi,EceKamar,S.Shah,andEricHorvitz.2020.AnEmpiricalAnalysisofBackward
CompatibilityinMachineLearningSystems. Proceedingsofthe26thACMSIGKDDInternationalConferenceon
KnowledgeDiscovery&DataMining(2020).
[94] SteveNunez.2022.WhyAIinvestmentsfailtodeliver. https://www.infoworld.com/article/3639028/
why-ai-investments-fail-to-deliver.html[Online;accessed15-September-2022].
[95] AnselmStraussandJulietCorbin.1994.Groundedtheorymethodology:Anoverview.(1994).
[96] StefanStuder,ThanhBinhBui,ChristianDrescher,AlexanderHanuschkin,LudwigWinkler,StevenPeters,andKlaus-
RobertMüller.2021.TowardsCRISP-ML(Q):amachinelearningprocessmodelwithqualityassurancemethodology.
Machinelearningandknowledgeextraction3,2(2021),392–413.
[97] MasashiSugiyamaetal.2007.CovariateShiftAdaptationbyImportanceWeightedCrossValidation.InJMLR.
[98] GeorgiosSymeonidis,EvangelosNerantzis,ApostolosKazakis,andGeorgeA.Papakostas.2022.MLOps-Definitions,
ToolsandChallenges.In2022IEEE12thAnnualComputingandCommunicationWorkshopandConference(CCWC).
0453–0460. https://doi.org/10.1109/CCWC54503.2022.9720902
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.206:32 Shankar&Garciaetal.
[99] DamianATamburri.2020. Sustainablemlops:Trendsandchallenges.In202022ndinternationalsymposiumon
symbolicandnumericalgorithmsforscientificcomputing(SYNASC).IEEE,17–23.
[100] MatteoTesti,MatteoBallabio,EmanueleFrontoni,GiulioIannello,SaraMoccia,PaoloSoda,andGennaroVessio.
2022.MLOps:Ataxonomyandamethodology.IEEEAccess10(2022),63606–63618.
[101] ManasiVartak.2016.ModelDB:asystemformachinelearningmodelmanagement.InHILDA’16.
[102] DakuoWang,JustinD.Weisz,MichaelMuller,ParikshitRam,WernerGeyer,CaseyDugan,YlaTausczik,Horst
Samulowitz, and Alexander Gray. 2019. Human-AI Collaboration in Data Science: Exploring Data Scientists’
Perceptions of Automated AI. Proc. ACM Hum.-Comput. Interact. 3, CSCW, Article 211 (nov 2019), 24 pages.
https://doi.org/10.1145/3359313
[103] JoyceWeiner.2020.WhyAI/datascienceprojectsfail:howtoavoidprojectpitfalls.SynthesisLecturesonComputation
andAnalytics1,1(2020),i–77.
[104] Wikipediacontributors.2022.MLOps—Wikipedia,TheFreeEncyclopedia. https://en.wikipedia.org/w/
index.php?title=MLOps&oldid=1109828739[Online;accessed15-September-2022].
[105] OliviaWiles,SvenGowal,FlorianStimberg,Sylvestre-AlviseRebuffi,IraKtena,KrishnamurthyDvijotham,and
AliTaylanCemgil.2021.AFine-GrainedAnalysisonDistributionShift.ArXivabs/2110.11328(2021).
[106] KanitWongsuphasawat,YangLiu,andJeffreyHeer.2019.Goals,Process,andChallengesofExploratoryDataAnalysis:
AnInterviewStudy.ArXivabs/1911.00568(2019).
[107] DorisXin,HuiMiao,AdityaParameswaran,andNeoklisPolyzotis.2021. Productionmachinelearningpipelines:
Empiricalanalysisandoptimizationopportunities.InProceedingsofthe2021InternationalConferenceonManagement
ofData.2639–2652.
[108] DorisXin,EvaYiweiWu,DorisJung-LinLee,NiloufarSalehi,andAdityaParameswaran.2021.WhitherAutoML?
UnderstandingtheRoleofAutomationinMachineLearningWorkflows.InProceedingsofthe2021CHIConferenceon
HumanFactorsinComputingSystems(Yokohama,Japan)(CHI’21).AssociationforComputingMachinery,NewYork,
NY,USA,Article83,16pages. https://doi.org/10.1145/3411764.3445306
[109] M.Zahariaetal.2018. AcceleratingtheMachineLearningLifecyclewithMLflow. IEEEDataEng.Bull.41(2018),
39–45.
[110] AmyXZhang,MichaelMuller,andDakuoWang.2020.Howdodatascienceworkerscollaborate?roles,workflows,
andtools.ProceedingsoftheACMonHuman-ComputerInteraction4,CSCW1(2020),1–23.
[111] YuZhang,YunWang,HaidongZhang,BinZhu,SimingChen,andDongmeiZhang.2022.OneLabeler:AFlexible
SystemforBuildingDataLabelingTools.InCHIConferenceonHumanFactorsinComputingSystems.1–22.
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.“Wehavenoideahowmodelswillbehaveinproductionuntilproduction”:HowengineersoperationalizeML 206:33
A SEMI-STRUCTUREDINTERVIEWQUESTIONS
Inthebeginningofeachinterview,weexplainedthepurposeoftheinterview—tobetterunderstand
processeswithintheorganizationforvalidatingchangesmadetoproductionMLmodels,ideally
throughstoriesofMLdeployments.Wethenkickstartedtheinformation-gatheringprocesswith
aquestiontobuildrapportwiththeinterviewee,suchastellusaboutamemorablepreviousML
modeldeployment.ThisquestionhelpedusisolateanMLpipelineorproducttodiscuss.Wethen
askedaseriesofopen-endedquestions:
(1) NatureofMLtask
• WhatistheMLtaskyouaretryingtosolve?
• Isitaclassificationorregressiontask?
• Aretheclassrepresentationsbalanced?
(2) Modelingandexperimentationideas
• Howdoyoucomeupwithexperimentideas?
• Whatmodelsdoyouuse?
• Howdoyouknowifanexperimentideaisgood?
• Whatfractionofyourexperimentideasaregood?
(3) Transitionfromdevelopmenttoproduction
• What processes do you follow for promoting a model from the development phase to
production?
• Howmanypullrequestsdoyoumakeorreview?
• Whatdoyoulookforincodereviews?
• Whatautomatedtestsrunatthistime?
(4) Validationdatasets
• Howdidyoucomeupwiththedatasettoevaluatethemodelon?
• Dothevalidationdatasetseverchange?
• DoeseveryengineerworkingonthisMLtaskusethesamevalidationdatasets?
(5) Monitoring
• Doyoutracktheperformanceofyourmodel?
• Ifso,whenandhowdoyourefreshthemetrics?
• Whatinformationdoyoulog?
• Doyourecordprovenance?
• HowdoyoulearnofanML-relatedbug?
(6) Response
• Whathistoricalrecords(e.g.,trainingcode,trainingset)doyouinspectinthedebugging
process?
• WhatorganizationalprocessesdoyouhaveforrespondingtoML-relatedbugs?
• Doyoumaketickets(e.g.,Jira)forthesebugs?
• Howdoyoureacttothesebugs?
• Whendoyoudecidetoretrainthemodel?
B TOOLSREFERENCEDININTERVIEWS
Table3listsseveralofthetoolsthatwerecommonlyreferencedbytheinterviewees.
Received15January2023
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.206:34 Shankar&Garciaetal.
DataCollection Experimentation EvaluationandDeployment MonitoringandResponse
Metadata Data catalogs, Weights & Biases, MLFlow, Dashboards,SQL,
Amundsen,AWS train/test set parameter configs, metric functions
Glue,Hivemetas- A/Btesttrackingtools andwindowsizes
tores
Unit Data cleaning Tensorflow, ML- OctoML, TVM, Scikit-learn
tools lib, PyTorch, joblib,pickle metric functions,
Scikit-learn, Great Expecta-
XGBoost tions,Deequ
Python,Pandas,Spark,SQL,C++,ONNX
Pipeline In-house or AutoML Github Actions, Prometheus,
outsourcedanno- TravisCI,Predic- AWS Cloud-
tators tionservingtools, Watch
Kafka,Flink
Airflow,Kubeflow,Argo,TensorflowExtended(TFX),VertexAI,DBT
Infrastructure Annotation Jupyternotebook Edge devices, Logging and
schema,cleaning setups,GPUs CPUs observability
criteriaconfigs services (e.g.,
DataDog)
Cloud(e.g.,AWS,GCP),computeclusters,storage(e.g.,AWSS3,Snowflake),Docker,Kubernetes
Table3. Commontoolsreferencedininterviewtranscripts,segmentedbystageintheMLOpsworkflow
andlayerinthestack.Themetadatalayerisconcernedwithartifactsforcomponentruns,likeresultsof
atrainingscript.Theunitlayerrepresentsindividualpiecesorcomponentsofapipeline,suchasfeature
engineeringormodeltraining.Thepipelinelayerconnectscomponentsthroughorchestrationframeworks,
andthelowestlayeristheinfrastructure(e.g.,compute).
Proc.ACMHum.-Comput.Interact.,Vol.8,No.CSCW1,Article206.Publicationdate:April2024.