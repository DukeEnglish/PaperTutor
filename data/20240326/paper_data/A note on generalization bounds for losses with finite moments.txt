A note on generalization bounds
for losses with finite moments
Borja Rodríguez-Gálvez∗, Omar Rivasplata†, Ragnar Thobaben∗, Mikael Skoglund∗
∗KTH Royal Institute of Technology, {borjarg, ragnart, skoglund}@kth.se
†UCL, o.rivasplata@ucl.ac.uk
Abstract—This paper studies the truncation method In this paper, we consider PAC-Bayesian bounds [4–7]. This
from Alquier [1] to derive high-probability PAC-Bayes bounds framework considers the algorithm as a Markov kernel PS
W
for unbounded losses with heavy tails. Assuming that the p-th that returns a distribution PS=s on the hypothesis class, for
moment is bounded, the resulting bounds interpolate between W
a slow rate 1/√ n when p=2, and a fast rate 1/n when p→∞ every dataset realization s. Then, the resulting bounds depend
and the loss is essentially bounded. Moreover, the paper derives not only on the hypothesis class, but also on the dependence
a high-probability PAC-Bayes bound for losses with a bounded of the hypothesis W = A(S) on the random training set S.
variance. This bound has an exponentially better dependence We are interested in the case of unbounded losses.
on the confidence parameter and the dependency measure than
previous bounds in the literature. Finally, the paper extends all A. PAC-Bayesian bounds
results to guarantees in expectation and single-draw PAC-Bayes.
In order to so, it obtains analogues of the PAC-Bayes fast rate The original PAC-Bayesian bound of McAllester [5, 6, 7]
bound for bounded losses from [2] in these settings. assumes bounded losses ℓ(w,z) [0,1] and states that if Q
W
∈
is a distribution on , independent of the training set S, and
W
I. INTRODUCTION β (0,1) is a confidence parameter, then, with probability no
sm∈ allerthan1 β overtherandomtrainingsetS P =P⊗n,
Consider a sequence of n instances s=(z ,...,z ) n − ∼ S Z
1 n
∈Z (cid:115)
of a problem with instance space . A learning algorithm D(PS Q )+logξ(n)
A is a (possibly randomized) mecZ hanism that generates a ESR(W) ESR(cid:98)(W,S)+ W∥ W β (1)
≤ 2n
hypothesis w of the solution of the problem when it is
∈W
given the sequence s, which is commonly referred to as the holds simultaneously PS , where ξ(n) [√n,2 +
the training set. The performance of a hypothesis w on an √2n] [2, 8, 9], is
the∀ setW of∈ allP
Markov kernels
P∈
S from
instance z is evaluated by a loss function ℓ : W ×Z → R + to distributionsP on W such that PS W ≪Q W, and EW S denoteS s
so that smaller values of ℓ(w,z) indicate a better performance the conditional expectation operator with respect to the σ-
of the hypothesis w on the problem instance z, while larger algebra induced by S. The dependency of the hypothesis on
values indicate a worse performance. Assume the instances of the dataset is measured by the relative entropy D(PS Q ) of
the problem follow a distribution P Z; the goal of the learning thealgorithm’shypothesiskernelPS W,orposterior,wW it∥ hreW spect
algorithm is to produce a hypothesis w that has as low as to the data-independent distribution Q , or prior, on the
W
possible expected loss on samples Z from the distribution P Z, hypothesis space. When the confidence penalty is logarithmic,
that is, a small population risk R(w):=Eℓ(w,Z). that is, log1/β, we say that the bound is of high probability.
Often, we do not have a direct access to the problem Note that the PAC-Bayesian guarantee from (1) is on the
distribution P , and hence calculating the population risk algorithm’s output distribution PS , and not on any particular
Z W
is unfeasible. Nonetheless, we can employ the available realization from it. To simplify the notation, in the rest of the
training set s to construct an estimate of the population paper we will use R:=ESR(W), R(cid:98) :=ESR(cid:98)(W,S), D:=
r ei msk pira in cd alb ro isu knd R(cid:98)it (s w,d se )via :=tion n1. (cid:80)A n i=co 1m ℓ(m wo ,n z i)e ,st wim hia ct he i is s t th he e D th( aP tS W th∥ esQ eW qu) aa nn td itiC en s,β a, rS e: r= anD do+ mlo vg aξ r( in ab)/ lβ e; sw wh hil oe seun rd ae nr ds ota mnd ni en sg s
average loss of the hypothesis w on the training set instances comes from the random training set S.
z i. Notice that the population risk can be decomposed as There have been multiple efforts to generalize McAllester’s
(cid:0) (cid:1)
R(w) = R(cid:98)(w,s)+ R(w) R(cid:98)(w,s) , where the second bound (1) to unbounded losses. These results often require
−
term is usually referred to as the generalization gap. some assumptions on the tail behavior of the random loss
Probablyapproximatelycorrect(PAC)theorystudiesbounds ℓ(w,Z) with respect to the problem distribution P and gener-
Z
on the generalization gap that hold with a probability larger alize classical concentration inequalities to the PAC-Bayesian
than a user-chosen threshold. Classically, these bounds hold setting.Forexample,thecumulativegeneratingfunction(CGF)
uniformly for all elements of a hypothesis class and only Λ (λ) :=
logEexp(cid:0)
λ(ℓ(w,Z)
Eℓ(w,Z)(cid:1)
completely
ℓ(w,Z)
W −
depend on the complexity of the said class, which is measured, characterizes the tails of ℓ(w,Z) for fixed w. The Cramér-
forexample,bytheVapnik–Cherovenkis(VC)dimensionorthe Chernoff method determines the connection between the CGF
Rademachercomplexity.See[3]foranintroductiontothetopic. and the tails behavior [10, Section 2.3]. More precisely, if
4202
raM
52
]LM.tats[
1v18661.3042:viXrathere is a convex and continuously differentiable function ψ(λ) In particular, we focus on losses with heavy tails that have
defined on [0,b) for some b R such that ψ(0)=ψ′(0)=0 a bounded p-th moment. Our contributions are:
∈
and Λ −ℓ(w,Z)(λ) ψ(λ) for all λ [0,b), then the Chernoff • We refine the decomposition proposed in [1] and further
inequality
establish≤
es that
Eℓ(w,Z)∈
≤ℓ(w,Z)+ψ ∗−1(log1/β) study the resulting bounds. In particular, we show that,
withprobabilitynosmallerthan1 β.In[2,Corollary15],the contrary to what is mentioned in [24, Section 5.2.1],
−
authorsbuildon[11,12]toderiveaPAC-Bayesiananalogueto there are choices of the parameter λ such that the term
the Chernoff inequality accounting for the dependence of the associated to the loss’ tail does not dominate and slows
training set S and the hypothesis W. Namely, with probability down the rate. In fact, we show that the resulting bound’s
no smaller than 1 −β,
rateisin
(cid:0) n−p− p1(cid:1)
.Thisisappealingsinceitinterpolates
(cid:18)1.1D+log10eπ2(cid:19) between O a slow rate of 1/√ n when only the 2nd moment
R R(cid:98) +ψ−1 β (2) isbounded, toa fastrate of 1/n whenall themoments are
≤ ∗ n bounded and the loss is bounded P -almost surely (a.s.).
Z
holdssimultaneously PS .Someexamplesoflosseswith • For p = 2, we derive new high-probability PAC-Bayes
∀ W ∈P
aboundedCGFincludebothsub-Gaussianandsub-exponential bounds for losses with a bounded variance that are tighter
losses, which were also studied individually in [1, 13–15]. than [22, Theorem 1] and [25, Corollary 2].
A weaker assumption is to consider losses with bounded • Finally, we extend all the presetned results to bounds in
moments for all hypotheses w . For a fixed hypothesis w, expectation and single-draw PAC-Bayes bounds.
thep-th(raw)momentofthelos∈ siW sEℓ(w,Z)p.Theassumption
II. ALQUIER’STRUNCATIONMETHOD
of bounded moments is weaker since if the CGF exists, then
In his Ph.D. thesis, Alquier [1] discussed a method to find
all the moments are bounded. However, the reverse is not true:
PAC-Bayesian bounds for unbounded losses. This method
for example, the log-normal distribution has bounded moments
consists of considering the following bound on the loss
of all orders, but it does not have a CGF [16, Chapter 14] [17].
The smaller the order of the bounded moment, the weaker the ℓ(w,z) ℓ− (w,z)+ℓ+ (w,z),
assumption as Eℓ(w,Z)p Eℓ(w,Z)q for all p q. When ≤ n/λ n/λ
thelosshasaboundedp-th≤ momentbutitdoesnoth≤ aveaCGF, where ℓ− and ℓ+ are defined in (3) and (4) respectively.
n/λ n/λ
thelossissaidtohaveaheavytail.Thereareworksthatobtain Therefore, the population risk can be bounded as R R− +
≤ n/λ
PAC-Bayesian bounds similar to (2) assuming a bounded 2nd R+ , where R− and R+ are defined as the population
moment [1, 18–20] or a bounded 2nd and 3rd moments [21]. risn k/ sλ of ℓ− andn/ ℓλ + respen c/λ tively. Then, it is clear that one
Alquier and Guedj [22] also developed PAC-Bayesian bounds n/λ n/λ
can bound each of these two risk terms separately.
for losses with bounded moments, but they considered the p-th The first term R− is especially easy to bound since it has
central moment Eℓ(w,Z) Eℓ(w,Z)p, which can be much n/λ
| − | a bounded range in [0,n/λ]. Alquier [1, Corollary 2.5] used a
smaller than the raw moment. However, in these bounds the
bound à la Catoni [13]. Instead, we will consider [2, Theorem
confidence penalty 1/β is linear and not logarithmic, and they
7],whichisastightastheSeeger–Langfordbound[26,27]and
consider other f-divergences as the dependency measure.
is easier to interpret. To simplify the expressions henceforth,
Finally, Haddouche et al. [23] considered a different kind (cid:0) (cid:1) (cid:0)
we define κ
1
:=cγlog γ/(γ−1) , κ
2
:=cγ, and κ
3
:=γ 1
of condition called the hypothesis-dependent range (HYPE), (cid:1) −
c(1 logc) , with the understanding that they are functions
which states that there is a function κ with positive range such −
of the parameters c (0,1] and γ >1 from [2, Theorem 7].
that sup ℓ(w,z) κ(w) for all hypotheses w ; but ∈
z∈Z ≤ ∈ W The bound on the second term depends on the tails of the
their bounds decrease at a slower rate than (1) when they are
loss and varies depending on the available information. We
restricted to the bounded case.
make this explicit using [28, Lemma 4.4]. Namely,
B. Contributions (cid:110) n (cid:111)
R+ =ESmax ℓ(W,Z) ,0
In this paper, we build upon Alquier [1]’s truncation method n/λ − λ
(cid:90) ∞ (cid:104) (cid:110) n (cid:111) (cid:105)
and demonstrate its potential. This method consists of studying = PS max ℓ(W,Z) ,0 >t dt
a truncated version of the loss. To this effect, let 0 − λ
(cid:90) ∞ (cid:104) n(cid:105)
ℓ− n/λ(w,z):=min {ℓ(w,z),n/λ
}
(3)
≤ 0
PS ℓ(w,Z)>t+
λ
dt
(cid:90) ∞
and
ℓ+ n/λ(w,z):=(cid:2) ℓ(w,z) −n/λ(cid:3)
+
(4) =
n λ
PS(cid:2) ℓ(w,Z)>t(cid:3)
dt.
where [x] := max x,0 and where λ R is suitably Lemma 1 (Alquier [1, Corollary 2.5, adapted]). For all β
chosen. T+ hus, we h{ ave ℓ} (w,z) ℓ− (w∈ ,z)+ + ℓ+ (w,z). (0,1) and all λ>0, with probability no smaller than 1 β∈ ,
≤ n/λ n/λ −
T t lr oh u se n sn ec s, a ,to e an d ne dlom s ta rs ay nℓ sb −
n
l/o aλu ten ud s thint ah g te ts op tao n Pp d Au al Ca rdt -i Bo tn ae ycr ehi ss n ik i aq na us e bs s oo uc foi na r dte sbd o fouto rndt th e hde
e
R ≤κ 1 ·R(cid:98) n− /λ+κ 2 ·C n λ,β,S+κ 3 ·n λ+(cid:90) n/∞
λ
PS(cid:2) ℓ(w,Z)>t(cid:3) dt
unbounded loss ℓ accounting for the loss’ tail Eℓ+ (w,Z). holds simultaneously (PS ,c,γ) (0,1] [1, ).
n/λ ∀ W ∈P × × ∞In this way, if we have some knowledge about the tails of A. Alquier’s modification for losses with a bounded moment
the loss, we can trade off (i) the penalty of the loss’ tail after a
Alquier [1, Theorem 2.7] presented a result similar to
thresholdn/λfor(ii)thepenaltyoftherangen/λwhileexploit-
Lemma 3 for losses with a bounded p-th moment. However,
ing the existing sharp bounds for losses with a bounded range.
he did not obtain it with the straightforward technique outlined
A. Refining the method above. Instead, he considered the truncated loss function
As hinted later by Alquier [24, Section 5.2.1] and made (cid:20) 1(cid:16)p 1(cid:17)p−1(cid:16)λ(cid:17)p−1 (cid:21)
ℓ (w,z)= ℓ(w,z) − ℓ(w,z)p .
explicit above in Lemma 1, this method is rooted into p,n/λ −p p n ·| |
+
decomposing the loss into a bounded part where ℓ(w,z) n/λ
and an unbounded part where ℓ(w,z) > n/λ. This
ca≤
n be
Importantly, this loss function satisfies that ℓ
p,n/λ
≤
n/λ.
Then, let R be the population risk associated to ℓ . It
further untangled with the decomposition p,n/λ p,n/λ
directly follows that
ℓ(w,z)=ℓ (w,z)+ℓ (w,z),
≤n/λ >n/λ
1(cid:16)p 1(cid:17)p−1(cid:16)λ(cid:17)p−1
where R ≤R p,n/λ+
p
−
p n
·ES |ℓ(W,Z) |p.
ℓ (w,z):=ℓ(w,z)1 (w,z),
≤n/λ {ℓ(w,z)≤n/λ} In this way, like before, the term R
p,n/λ
can be bounded
ℓ (w,z):=ℓ(w,z)1 (w,z), using any standard PAC-Bayes bound for bounded losses and
>n/λ {ℓ(w,z)>n/λ}
and 1 (w,z) is the indicator function returning 1 if (w,z) now the second term is bounded by construction. As before,
A
∈ we will present the result using [2, Theorem 7] instead of a
and 0 otherwise. Therefore, the population risk can be
A bound à la Catoni [13]. For this purpose, let R(cid:98) be the
decomposed similarly to before as R = R
≤n/λ
+ R >n/λ,
empirical risk associated to the loss ℓ .
p,n/λ
where R
≤n/λ
and R
>n/λ
are defined as the population risks p,n/λ
of ℓ ≤n/λ and ℓ <n/λ respectively. Lemma 4 (Alquier [1, Theorem 2.7, adapted]). For every loss
Proceedingasbefore,thetworisktermscanbebounded.The with a p-th moment bounded by m , for all β (0,1) and all
p
firstterm R ≤n/λ isalsoboundedin[0,n/λ],butitispotentially λ>0, with probability no smaller than 1 β∈ ,
much smaller than R− since ES[ℓ− (W,Z)ℓ(W,Z) > −
n/λ]=n/λ, while ES[ℓ ≤n/ n/λ λ(W,Z) |ℓ(Wn ,/ Zλ )>n/λ| ]=0. Also, R ≤κ 1 ·R(cid:98) p,n/λ+κ 2 ·C n λ,β,S+κ 3 ·n λ+m pp(cid:16)p − p 1(cid:17)p−1(cid:16) nλ(cid:17)p−1
the second term R can be bounded by exactly the same
>n/λ
quantity as with Alquier [1]’s original decomposition, namely holds simultaneously (PS ,c,γ) (0,1] [1, ).
∀ W ∈P × × ∞
R =ESℓ(W,Z)1 (W,S) Comparing Lemma 4 to the truncation method with the
>n/λ ℓ(w,z)>n/λ
(cid:90) ∞ straightforward Lemma 3, we see that the result stemming
=
PS(cid:2) ℓ(W,Z)>t(cid:3)
dt.
fromAlquier[1]’smodifiedconstructionimprovestheconstant
n/λ of the term associated to the tail from 1/p−1 to (p−1/p)p−1 1/p.
Lemma 2 (Refinement of Lemma 1). For all β (0,1) and ·
For p=2, the constant is 4 times smaller changing from 1 to
∈
all λ>0, with probability no smaller than 1 β,
−
1/4; and for p the constant is e times smaller, although
R ≤κ 1 ·R(cid:98) ≤n/λ+κ 2 ·C n λ,β,S+κ 3 ·n λ+(cid:90) n/∞
λ
PS(cid:2) ℓ(w,Z)>t(cid:3) dt b bo eth smte an lld erto th0 a.→ nO R(cid:98)n∞ pt ,h n/e λo .t Th her eh rean sud l, tsR(cid:98) d≤ en ri/ vλ edha is nth the ep ro et se tn oti fal thto
e
holds simultaneously (PS ,c,γ) (0,1] [1, ). paper use Lemma 3 as a starting point, but analogous results
∀ W ∈P × × ∞ trivially follow from Lemma 4 with slightly different constants
If the tail is bounded by some function α(n,λ), i.e.,
and changing R(cid:98) to R(cid:98) .
(cid:82)∞ PS(cid:2)
ℓ(w,Z) >
t(cid:3)
dt α(n,λ), then the bound result-
≤n/λ p,n/λ
n/λ ≤ InLemmata1to4,thetermκ3n/λdoesnotaffectthebound’s
ing from Lemma 2 is optimized by the Gibbs posterior
rate as choosing c = 1 implies κ = 0. The coefficients κ
dPS W=s(w) ∝dQ W(w)e−λ· κκ 21·R(cid:98)≤n/λ(w,s) independently of α. and κ
2
are chosen adaptively to m3 inimize the empirical risk1
and complexity contributions as discussed in [2].
III. LOSSESWITHABOUNDEDMOMENT
If the loss has a bounded p-th moment Eℓ(w,Z)p m p < B. Optimizing the parameter in the bound
≤
forallw ,thenonemayfindPAC-Bayesianboundsus-
∞ ∈W Alquier [1, 24] considered the data-independent λ = √n.
ingAlquier[1]’struncationmethod.Moreprecisely,employing This gives a bound with a rate of 1/√ n for any loss with
Markov’s inequality [10, Section 2.1] to the term associated
a bounded p-th moment where p > 2. A better choice is
to the loss’ tail in Lemma 2 stems the following result. λ=(cid:0) np−1/mp(cid:1)1/p.Thisresultsinaboundwitharateofn−p− p1
.
Lemma 3. For every loss with p-th moment bounded by m ,
p Theorem 1. For every loss with a bounded p-th moment, for
for all β (0,1) and all λ>0, with probability no smaller
∈ all β (0,1), with probability no smaller than 1 β,
than 1 β, ∈ −
−
R ≤κ 1 ·R(cid:98) ≤n/λ+κ 2 ·C n λ,β,S +κ 3 ·n λ + pm p 1(cid:16) nλ(cid:17)p−1 (5) R ≤κ 1 ·R(cid:98) ≤(mpn)p1 +(cid:16) nm p−p 1(cid:17) p1(cid:16) κ 2 ·C n,β,S+κ 3 ·n+ p 1 1(cid:17)
− −
holds simultaneously (PS ,c,γ) (0,1] [1, ). holds simultaneously (PS ,c,γ) (0,1] [1, ).
∀ W ∈P × × ∞ ∀ W ∈P × × ∞Inthisway,therateforp=2isexactlythesame,aslowrate can be replicated under the weaker condition that the loss has
of 1/√ n. However, as the order of the known bounded moment a bounded p-th moment with respect to the algorithm’s output,
increases, that is p →∞, the rate becomes a fast rate of 1/n. that is, that m′
p
:=ESℓ(W,Z)p is bounded P S-a.s.
Hence,thischoiceofλallowsustointerpolatebetweenaslow Although this condition is weaker, it is harder to guarantee
and a fast rate depending on how much knowledge about the as it requires some knowledge of the data distribution P
Z
tails is available to us. Furthermore, as we gain knowledge of and the algorithm’s Markov kernel PS . This knowledge could
W
the tails, the truncation of the loss ℓ becomes less instead be used to directly find a bound on R=ESℓ(W,Z).
≤(mpn)1/p
dependent on the number of training data n and in the limit However, results under this condition can be useful in some
p only depends of the P -a.s. boundedness of the loss, situations. For example, they can be used to derive new results
Z
→∞
namely lim (m n)1/p =sup esssupℓ(w,Z). for losses with a bounded variance (as shown later in Sec-
p→∞ p w∈W
Instead of choosing a data-independent parameter λ, we can tion IV) and to obtain more meaningful findings when p .
→∞
use the event space discretization technique from [2] to get Theorem 2, when specialized to p , gives us a fast
a better dependence on the relative entropy. In particular, the rate result when the loss is P Z-a.s. b→ oun∞ ded, that is, when
following result follows by not considering any “uninteresting esssupℓ(w,Z) < for all w . This condition of the
event” and following the technique as outlined in [2, Corollary lossbeingP Z-essen∞ tiallybounded∈ caW nbeastrongrequirement,
15]. Henceforth, let us define C′
n,β,S
:=1.1D+log10eπ2ξ(n)/β. similar to the one of bounded losses. However, when we have
The full proof is given in Appendix A. more information about the algorithm, then we can obtain a
fast rate result when the loss is P P -a.s. bounded, that
Theorem 2. For every loss with a p-th moment bounded by W,S ⊗ Z
is, when esssupℓ(W,Z)<v. This condition is much weaker
m , for all β (0,1), with probability no smaller than 1 β,
p ∈ − than the previous essential boundedness or just boundedness
R ≤κ
1
·R(cid:98)≤t⋆ +m
pp1(cid:16)
p
p 1(cid:17)(cid:16)
κ
2 ·
C′
n n,β,S +κ
3(cid:17)p− p1 o suf ct hhe thl ao tss P. (N ℓ(a Wm ,e Zly ), o <ne v)ne =ed 1s .t Ao sk an now ext ah mat pt lh ee
,
ca olg no sr idit eh rm thi es
− squared loss ℓ(w,z)=(w z)2 and some data that belongs
holds simultaneously ∀(PS W,c,γ) ∈P×(0,1] ×[1, ∞), where to some interval of length 1− with probability 1, that is P(Z
t⋆
:=mp1(cid:16)
κ
C′
n,β,S +κ
(cid:17)− p1
.
[a,a+1]) = 1, but where we ignore a. Consider w
∈
R∈ ,
p 2 · n 3 the simple algorithm that returns the average of the training
In this way, the rate is maintained, while the dependence
instances
A(s)=(cid:80)n
i=1zi/n ensures that esssupℓ(W,Z)<1,
while sup esssupℓ(w,Z) .
on the relative entropy changed from linear to polynomial of w∈R
→∞
order (p−1)/p. For order p=2, this corresponds to the square IV. LOSSESWITHABOUNDEDVARIANCE
root, and only goes to the linear case when p , when we
→∞ A particularly important case is the one of losses with a
also achieve a fast rate of 1/n.
boundedsecondmoment.Theorem2recoverstheexpectedrate
Following the insights of [2, Section 3.2.4], we may (cid:112)
of m2D/nfrom[2,Theorem11].Thisisthesmallestmoment
use Theorem 2 to obtain an equivalent result, but in the form with a rate no slower than 1/√ n. However, as mentioned in [2],
of Lemma 2 that holds simultaneously for all λ.
the raw second moment m can be much larger than the vari-
2
Theorem 3. For every loss with a p-th moment bounded by ance,orcentralsecondmoment.Whenthevarianceisbounded,
m , for all β (0,1), with probability no smaller than 1 β, that is E(ℓ(w,Z) Eℓ(w,Z))2 σ2 < for all w , the
p ∈ − − ≤ ∞ ∈W
only PAC-Bayesian results we are aware of are [22, 25].
C′ n m (cid:16)λ(cid:17)p−1
R ≤κ
1
·R(cid:98) ≤n/λ+κ
2 ·
n λ,β,S +κ
3 · λ
+
p
p
1 · n Theorem 4 (Alquier and Guedj [22, Theorem 1] and Ohnishi
− andHonorio[25,Corollary2]). Foreverylosswithavariance
holdssimultaneously (PS ,c,γ,λ) (0,1] [1, ) R .
∀ W ∈P× × ∞ × + bounded by σ2, for all β (0,1), with probability no smaller
∈
From Theorem 3, we understand that the posterior that than 1 β, each of the inequalities
−
optimizes both Theorems 2 and 3 is the Gibbs posterior (cid:115)
dPS W=s(w) ∝dQ W(w)e−λ 2· κκ 21R(cid:98)≤n/λ(w,s),wherenowc,λ,and R ≤R(cid:98) + σ2(χ n2 β+1) (6)
γ can be chosen adaptively after observing the realization of
(cid:115)
the data s. This way, the choice of the parameter λ can be (cid:112)
σ2 χ2+1
madetooptimizetheboundemergingfromthatdatarealization. R R(cid:98) + (7)
≤ nβ
On the other hand, the Gibbs distribution emerging from the
(cid:115)
optimizationofLemma2needstocommittoafixed parameter χ2+(cid:0) σ2/β(cid:1)2
λ before observing the training data and is data-independent. R R(cid:98) + (8)
≤ 2n
C. The case p and essentially bounded losses hold simultaneously PS , where χ2 := χ2(PS ,Q )
→∞ ∀ W ∈ P W W
So far we only considered the algorithm-independent con- is the chi-squared divergence.1
dition of losses with a bounded p-th moment Eℓ(w,Z)p for
1TheresultisoriginallygivenbyVarS(ℓ(W,Z)),whichusuallyrequires
all w . This condition only depends on the loss and the
proble∈ mW distribution P Z. Nonetheless, all the previous results wto io thm thu ech alk gn oo rw ithle mdg -ie ndo en peth ne dea nlg to vr ait rh iam nca end σ2da ≥ta Vd ais rt Srib (ℓu (ti Won ,s Z.W ))e . presentedit1.50 1.50 1.50 1.50
1.25 1.25 1.25 1.25
1.00 1.00 1.00 1.00
0.75 0.75 0.75 0.75
0.50 0.50 0.50 0.50
0.25 0.25 0.25 0.25
0.00 0.00 0.00 0.00
0.900 0.920 0.940 0.960 0.980 0.999 0 200 400 0.0 0.1 0.2 0.3 0.4 0 2500 5000 7500 10000
1 β χ2 Empiricalrisk n
−
Fig. 1: Illustration comparing [22, 25] ((6) in black, (7) in gray, and (8) in orange) and our Theorem 5 (in blue) for varying
values of β, χ2, R(cid:98), and n. To help the comparison, we actually use the upper bound relaxation (10) of Theorem 5. When they
are not varying, the values of the parameters are fixed to β =0.025, χ2 =200, R(cid:98) =0.025, n=10,000, and σ2 =1.
Although this bound still achieves an expected slow rate to obtain our algorithm-independent variance. After that, re-
of 1/√ n, there are two main differences between this theorem arrangingtheequationandacceptingtheconventionthat1/0
→
and those presented in the preceding sections. First, and most completes the proof. The full proof is in Appendix B.
∞
notable, the dependence with the confidence penalty 1/β is
AlthoughtheTheorem5isofhighprobabilityandconsiders
not logarithmic, but polynomial. This can result in a loose
therelativeentropy,itishardtocompareTheorem4duetothe
bound when high confidence is demanded: for example, for
first factor [1 2(C′′ )1/2]−1. This factor ensures the bound
β = 0.05 we have that log1/β 3 while 1/β = 20. Second, − n,β,S +
≈ isonlyusefulwhen2(C′′ )1/2 <1,whichistherangewhere
the dependency measure changed from the relative entropy D n,β,S
the bound would be effective without the said factor anyway.
to the chi-squared divergence χ2. The chi-squared divergence
also measures the dissimilarity between the posterior PS and To effectively compare the two bounds, we bound Theorem 5
the prior Q , but it can be much larger. More preciselW y, from above using the relative entropy upper bound (9), that is,
W
(cid:104) (cid:113) (cid:105)−1(cid:104) (cid:113) (cid:105)
0 ≤D ≤log(1+χ2) ≤χ2 (9) R ≤ 1 −2 C′ n′ ,β,S,χ2 + κ 1 ·R(cid:98) +2 σ2C′ n′ ,β,S,χ2 (10)
and no lower bound of the relative entropy D is possible in where
terms of the chi-squared divergence χ2 [29, Section 7.7]. 1.1log(1+χ2)+log10eπ2ξ(n)
Studying Theorem 2 with the weaker condition that C′′ :=κ β +κ .
Eℓ(W,Z)2 m′ as discussed in Section III-C, we can obtain n,β,S,χ2 2 · n 3
a high-proba≤ bility2 PAC-Bayes boundfor losses with abounded Also, we fix c=1 and γ =e/(e−1). Even with this relaxation,
variance that has the relative entropy as the dependency the presented high probability bound is tighter than Theorem 4
measure. As in the previous section, the method and proof in many regimes (see Figure 1).
technique also extends to an analysis starting from Lemma 4
V. EXTENSIONOFTHERESULTS
resulting in slightly different constants and using R(cid:98) as an
p,n/λ
AlthoughAlquier[1]devisedthetruncationmethodforPAC-
estimator instead of R(cid:98) . Similarly, the method also extends
≤n/λ Bayesboundsandwepresentedourresultsinthissetting,there
to the semi-empirical bound from [2, Theorem 11].
is nothing stopping us to use this technique to derive bounds
Theorem 5. For every loss with a variance bounded by σ2, in expectation or single-draw PAC-Bayes bounds.
for all β (0,1), with probability no smaller than 1 β, Bounds in expectation and single-draw PAC-Bayes bounds
∈ −
are similar to the PAC-Bayes bounds from Section I-A
(cid:104) (cid:113) (cid:105)−1(cid:104) (cid:113) (cid:105)
R
≤
1 −2 C′ n′
,β,S +
κ
1
·R(cid:98) +2 σ2C′ n′
,β,S
a gn ud ar( a1 n) t. eeB so .u Hnd es rei ,n te hx epe ec xt pa eti co tn edpr po ov pid ue law tioe nake rir skgen Ee Rra (li Wza )tio in
s
holds simultaneously ∀(PS W,c,γ) ∈P×(0,1] ×[1, ∞), where bounded using the expected empirical risk ER(cid:98)(W,S). That
C′ n′
,β,S
:=κ2C′ n,β,S/n+κ 3. is, the bound holds on average over the draw of the random
training set S and the returned hypothesis W, and there is no
Sketch of the proof. Consider the relaxed version of Theo-
confidence parameter. Single-draw PAC-Bayes bounds, on the
rem 2 from Section III-C for p = 2 and note that m′ =
2 other hand, provide stronger generalization guarantees. More
VarS(ℓ(W,Z))+R2.Then,forallβ (0,1),withprobability
∈ precisely,theyprovideguaranteesforthepopulationriskR(W)
no smaller than 1 β we have
− that hold with probability 1 β with respect to the draw of
(cid:113) −
R ≤κ
1
·R(cid:98) +2 (cid:0) VarS(ℓ(W,Z))+R2(cid:1) ·C′ n′
,β,S
the Inra And po pm ent dr ia ci en sin Cg s ae nt dS Da ,nd wethe der re it vu ern “e id nh ey xp po et ch te as tii os nW
”
.
and
simultaneouslyforallc (0,1]andallγ >1,whereC′′ is “single-draw PAC-Bayes” analogues to the PAC-Bayes fast
∈ n,β,S
defined as in the theorem statement. Then, we may employ the rate bound from [2]. Then, all the presented results extend to
inequality √x+y √x+√y to separate the square root and those settings routinely, and they are collected in the appendix
the inequality
VarS≤
(ℓ(W,Z)) sup Var(ℓ(w,Z))=σ2 for completeness.
≤ w∈W
dnuobksirnoitalupoP dnuobksirnoitalupoP dnuobksirnoitalupoP dnuobksirnoitalupoPREFERENCES inequalitiesofsomerandomvariablessequences,”JournalofInequalities
[1] P.Alquier,“Transductiveandinductiveadaptativeinferenceforregression andApplications,vol.2015,no.1,pp.1–8,2015.
anddensityestimation,”UniversityParis6,2006. [19] M. Haddouche and B. Guedj, “PAC-Bayes generalisation bounds
[2] B. Rodríguez-Gálvez, R. Thobaben, and M. Skoglund, “More for heavy-tailed losses through supermartingales,” Transactions
PAC-Bayes bounds: From bounded losses, to losses with general on Machine Learning Research, 2023. [Online]. Available: https:
tail behaviors, to anytime-validity,” 2023. [Online]. Available: //openreview.net/forum?id=qxrwt6F3sf
https://arxiv.org/abs/2306.12214 [20] B. Chugg, H. Wang, and A. Ramdas, “A unified recipe for deriving
[3] S.Shalev-ShwartzandS.Ben-David,UnderstandingMachineLearning: (time-uniform)PAC-Bayesbounds,”arXivpreprintarXiv:2302.03421,
FromTheorytoAlgorithms. CambridgeUniversityPress,2014. 2023.
[4] J.Shawe-TaylorandR.C.Williamson,“APACanalysisofaBayesianes- [21] M. Holland, “PAC-Bayes under potentially heavy tails,” Advances in
timator,”inProceedingsofthetenthannualconferenceonComputational NeuralInformationProcessingSystems,vol.32,2019.
LearningTheory,1997,pp.2–9. [22] P. Alquier and B. Guedj, “Simpler PAC-Bayesian bounds for hostile
[5] D.A.McAllester,“SomePAC-Bayesiantheorems,”inProceedingsofthe data,”MachineLearning,vol.107,no.5,pp.887–902,2018.
eleventhannualconferenceonComputationalLearningTheory,1998, [23] M. Haddouche, B. Guedj, O. Rivasplata, and J. Shawe-Taylor, “PAC-
pp.230–234. Bayesunleashed:Generalisationboundswithunboundedlosses,”Entropy,
[6] ——,“PAC-Bayesianmodelaveraging,”inProceedingsofthetwelfth vol.23,no.10,p.1330,2021.
annualconferenceonComputationalLearningTheory,1999,pp.164– [24] P. Alquier, “User-friendly introduction to PAC-Bayes bounds,” arXiv
170. preprintarXiv:2110.11216,2021.
[7] ——, “PAC-Bayesian stochastic model selection,” Machine Learning, [25] Y.OhnishiandJ.Honorio,“Novelchangeofmeasureinequalitieswith
vol.51,no.1,pp.5–21,2003. applicationstoPAC-BayesianboundsandMonteCarloestimation,”in
[8] A. Maurer, “A note on the PAC Bayesian theorem,” arXiv preprint InternationalConferenceonArtificialIntelligenceandStatistics. PMLR,
cs/0411099,2004. 2021,pp.1711–1719.
[9] P.Germain,A.Lacasse,F.Laviolette,M.March,andJ.-F.Roy,“Risk [26] J.LangfordandM.Seeger,“Boundsforaveragingclassifiers,”School
Bounds for the Majority Vote: From a PAC-Bayesian Analysis to a ofComputerScience,CarnegieMellonUniversity,Tech.Rep.,2001.
LearningAlgorithm,”JournalofMachineLearningResearch,vol.16, [27] M. Seeger, “PAC-Bayesian generalisation error bounds for Gaussian
processclassification,”JournalofMachineLearningResearch,vol.3,
no.26,pp.787–860,2015.
no.Oct,pp.233–269,2002.
[10] S.Boucheron,G.Lugosi,andP.Massart,Concentrationinequalities:A
[28] O.Kallenberg,FoundationsofModernProbability. Springer,1997.
nonasymptotictheoryofindependence. OxfordUniversityPress,2013.
[29] Y.PolyanskiyandY.Wu,InformationTheory:FromCodingtoLearning,
[11] T.Zhang,“Information-theoreticupperandlowerboundsforstatistical
1sted. CambridgeUniversityPress,2022.
estimation,”IEEETransactionsonInformationTheory,vol.52,no.4,
[30] O. Catoni, “PAC-Bayesian Supervised Classification: The Thermody-
pp.1307–1321,2006.
namicsofStatisticalLearning,”IMSLectureNotesMonographSeries,
[12] P.K.BanerjeeandG.Montúfar,“Informationcomplexityandgeneral-
vol.56,p.163pp,2007.
izationbounds,”in2021IEEEInternationalSymposiumonInformation
[31] B.Rodríguez-Gálvez,G.Bassi,R.Thobaben,andM.Skoglund,“Tighter
Theory(ISIT). IEEE,2021,pp.676–681.
expectedgeneralizationerrorboundsviaWassersteindistance,”Advances
[13] O. Catoni, Statistical Learning Theory and Stochastic Optimization:
inNeuralInformationProcessingSystems,vol.34,pp.19109–19121,
Ecoled’EtédeProbabilitésdeSaint-Flour,SummerSchoolXXXI-2001.
2021.
SpringerScience&BusinessMedia,2004,vol.1851.
[32] M.D.DonskerandS.S.Varadhan,“Asymptoticevaluationofcertain
[14] F. Hellström and G. Durisi, “Generalization bounds via information
Markovprocessexpectationsforlargetime,i,”CommunicationsonPure
densityandconditionalinformationdensity,”IEEEJournalonSelected
andAppliedMathematics,vol.28,no.1,pp.1–47,1975.
AreasinInformationTheory,vol.1,no.3,pp.824–839,2020.
[33] P.Germain,A.Lacasse,F.Laviolette,andM.Marchand,“PAC-Bayesian
[15] B.GuedjandL.Pujol,“Stillnofreelunches:thepricetopayfortighter
learning of linear classifiers,” in Proceedings of the 26th Annual
PAC-Bayesbounds,”Entropy,vol.23,no.11,p.1529,2021.
InternationalConferenceonMachineLearning,2009,pp.353–360.
[16] N. B. Norman L. Johnson, Samuel Kotz, Continuous Univariate
[34] O.Rivasplata,I.Kuzborskij,C.Szepesvári,andJ.Shawe-Taylor,“PAC-
Distributions. JohnWiley&SonsInc.,1994,vol.1.
Bayesanalysisbeyondtheusualbounds,”AdvancesinNeuralInformation
[17] S.Asmussen,J.L.Jensen,andL.Rojas-Nandayapa,“OntheLaplace
ProcessingSystems,vol.33,pp.16833–16845,2020.
transformofthelognormaldistribution,”MethodologyandComputing
[35] L.Bégin,P.Germain,F.Laviolette,andJ.-F.Roy,“PAC-Bayesiantheory
inAppliedProbability,vol.18,pp.441–458,2016.
fortransductivelearning,”inArtificialIntelligenceandStatistics. PMLR,
[18] Z. Wang, L. Shen, Y. Miao, S. Chen, and W. Xu, “PAC-Bayesian
2014,pp.105–113.APPENDIX to obtain our algorithm-independent variance. In this way, for
A. Proof of Theorem 2 all β (0,1), with probability no smaller than 1 β,
∈ −
(cid:113) (cid:113)
eleC mo en ns tid iser C(5) fr .o Lm etLem bm ea th3 ea cn od mn po lete mt eh na tt ot fh te heon el vy enra tn indo (m
5)
R ≤κ
1
·R(cid:98) +2 σ2 ·C′ n′
,β,S
+2R
·
C′ n′
,β,S
n,β,S k
with parameters β (B 0,1) and λ >0 such that P[ ]<β . holds simultaneously for all posteriors PS , all c (0,1], and
k ∈ k Bk k W ∈
Then, further let β
k
= 6/π2 β/k2 and define the sub-events all γ >1.
·
:= k 1 D < k and the indices := s n : Re-arranging the equation proves the theorem statement:
k
E λk k∈ >N 0{ ,: wP− i[ tE hk p] r≤ > oba0 b} i. liI tyn } nth ois law rga ey r, tf ho ar na Pll
[
β kK ∈ k( ],0, t{ h1 e) re∈ an edZ xisa tl sl w abh oe vn e,1 an≥ d2 w( hC e′ n′ n,β 1,S)1/ 22 (, Cth ′′e th )e 1o /2re ,m theho thl ed os reb my t hh oe ldr sea trs io vn iain llg
y
a posterior PS
W
∈P, a c ∈(0,1], and a γB >| 1E such that by the convention th≤ at 1/0n →,β,S ∞.
R>κ 1 ·R(cid:98)≤ λn k+κ 2 ·k+log λπ k2ξ 6( βn)k2 +κ 3 ·λn k+ pm p 1(cid:16)λ nk(cid:17)p−1 . C. TE ox st te an rs t,io wn eto firb so tu on bd tas inin ae nxp “ie nct ea xti po en
ctation” analogue to the
−
Optimizing the parameter λ k guarantees that for all β PAC-Bayes fast rate bound from [2].
(0,1) and all λ >0, with probability no larger than P[ ∈ ],
k k k
there exists a posterior PS , a c (0,1], and aB γ | >E 1 Theorem 6. For every loss with a range bounded in [0,b], the
W ∈ P ∈ inequality
such that
(cid:20) (cid:21)
R>κ
1
·R(cid:98)≤t⋆ k+m
pp1(cid:16)
p
p 1(cid:17)(cid:16)
κ
2
·k+log nπ2ξ 6( βn)k2
+κ
3(cid:17)p− p1
,
E[R(W)] ≤κ 1 ·E[R(cid:98)(W,S)]+b κ 2
·
I(W n;S) +κ 3
− holds for all c (0,1] and all γ > 1, where κ :=
where (cid:0) (cid:1) ∈ (cid:0) 1 (cid:1)
cγlog γ/(γ−1) , κ
2
:=cγ, and κ
3
:=γ 1 c(1 logc) .
t⋆
:=mp1(cid:16)
κ
k+logπ2ξ 6( βn)k2
+κ
(cid:17)− p1
. Proof. The proof starts by recalling [30,
Th− eorem−
1.2.6]. This
k p 2 · n 3 states that for every loss with a range bounded in [0,1],
Then, noting that k D + 1 given , noting that the
inequality x+logeπ2(x+≤ 1)2 (cid:0)a+3(cid:1) x+lE ok geπ2(a+1)2 2a E[R(W)] 1 (cid:104) 1 e− nλ·E[R(cid:98)(W,S)]−I(W n;S)(cid:105)
holds for all a>0,
an6 dβ usin≤
g
thia s+ i1
nequality
with6β a=1− 9,a w+1
e
≤ 1 −e− nλ −
have that for all β (0,1) and all λ > 0, with probability holds for all λ > 0. First, we can do the change of variable
no larger than P[ ∈ ], there existsk a posterior PS , a λ:=nγlog(cid:0) γ/γ−1(cid:1) suchthatγ >1.Afterthat,wecanusethat
c (0,1], and a γB >k |E 1k such that W ∈ P the function 1 e−x is a non-decreasing, concave, continuous
∈ function for x− >0 and therefore can be upper-bounded by its
R>κ
1
·R(cid:98)≤t⋆ +m pp1(cid:16)
p
p 1(cid:17)(cid:16) κ
2 ·
C′ n n,β,S +κ 3(cid:17)p− p1 , (11) e Un sv ie nl gop te h, eth ea nt vi es l, o1
pe−
ie n− tx he= ei qn uf a a> ti0
o{
ne− aba ox v+
e
1
an−
de l− eta ti( n1 g+ ca :) =}.
−
where e−a (0,1] completes the proof for losses with a range
t⋆ :=mp1(cid:16) κ C′ n,β,S +κ (cid:17)− p1 . bound∈ ed in [0,1]. Finally, the proof is completed by scaling
p 2 · n 3 the loss appropriately.
If we let be the event described by (11), we can bound
B A single-letter version of Theorem 6 can be easily derived if
its probability by
we consider an estimation of the population risk with a single
(cid:88) (cid:88) (cid:88)
P[ ]= P[ k]P[ k] P[
k
k]P[ k] P[ k] sample ℓ(W,Z i). In this way, Theorem 6 states that for every
B B|E E ≤ B |E E ≤ B loss with a range bounded in [0,b], the inequality
k∈K k∈K k∈K
and therefore P[ ]<β, which completes the proof. E[R(W)] κ E[ℓ(W,Z )]+b[κ I(W;Z )+κ ]
B ≤ 1,i · i 2,i · i 3,i
B. Proof of Theorem 5 holds for all c (0,1] and all γ > 1, where κ :=
i i 1,i
(cid:0) (cid:1)∈ (cid:0)
Consider the relaxed version of Theorem 2 from Sec- c iγ ilog
(cid:1)
γi/(γi−1) , κ 2,i := c iγ i, and κ 3,i := γ i 1 −c i(1
−
tion III-C for p=2 and note that m′
2
=VarS(ℓ(W,Z))+R2. logc i) . Then, taking the average of the theorem for all
Then,forallβ (0,1),withprobabilitynosmallerthan1 β, instances Z i yields the following result.
∈ −
R ≤κ
1
·R(cid:98)
+2(cid:113) (cid:0) VarS(ℓ(W,Z))+R2(cid:1)
·C′ n′
,β,S
Theorem 7. For every loss with a (cid:34)range b nounded in [0,b],
(cid:35)
h alo lld γs >sim 1,u wlt han ere eou Cs
′
nl ′y ,β,f Sor isal dl ep fio ns et deri ao srs inP tS
W
he, ta hl el oc
re∈
m( s0 t, a1 te] m, a en nd
t.
E[R(W)] ≤κ¯
1
·E[R(cid:98)(W,S)]+b κ¯
2 ·
n1 (cid:88) i=1I(W;Z i)+κ¯
3
Then, we may employ the inequality √x+y √x+√y
holds for all c (0,1] and all γ > 1, where κ :=
≤ i i 1,i
to separate the square root and the inequality (cid:0) (cid:1)∈ (cid:0) (cid:1)
VarS(ℓ(W,Z))
≤
sup Var(ℓ(w,Z))=σ2
ac i nγ dil κ¯o jg :=γi/ (cid:80)(γ
n
ii =− 11) κj, ,i/κ n2,i fo:=
r
ac li lγ ji, ∈κ 3 {, 1i ,:= 2,3γ }i .1 −c i(1 −logc i) ,
w∈WThis single-letter theorem is tighter than Theorem 6 since Theorem 10. For every loss with a range bounded in [0,b],
(cid:80)n
I(W;Z ) I(W;S) and since one could chose κ = with probability no smaller than 1 β,
i=1 i ≤ i,j −
κ j for all j 1,2,3 . (cid:20) C (cid:21)
With Theo∈ re{ m 6 at} hand, it is clear that all the presented R(W) ≤κ 1 ·R(cid:98)(W,S)+b κ 2
·
n,β n,S,W +κ 3
results can be replicated in this setting. Moreover, the choice
of the optimal parameter λ is simpler since this can be holds for all c (0,1] and all γ > 1, where κ 1 :=
(cid:0) (cid:1) ∈ (cid:0) (cid:1)
chosenadaptivelywithoutresortingtotheevents’discretization cγlog γ/(γ−1) , κ 2 :=cγ, and κ 3 :=γ 1 c(1 logc) .
− −
technique from [2].
Proof. Consider Theorem 13, which states that for every
Forcompleteness,weincludethetwomostimportantresults measurable function f : R, for every β (0,1),
below.WewillpresenttheresultsintermsofTheorem7,where W ×S → ∈
with probability 1 β,
weunderstandthatκ¯ aredefinedasaboveforallj 1,2,3 . −
j
For losses with a bounded p-th moment, as far∈ as{ we ar} e 1 (cid:20) (cid:18) dPS (cid:19) ∆(cid:21)
f(W,S) log W (W) +log (12)
aware, the following is the first result of this kind. ≤ n dQ β
W
Theorem 8. For every loss with a p-th moment bounded by holds, where ∆=Eenf(W′,S) and W′ is distributed according
m p, the inequality to the data-independent distribution Q W.
R ≤κ¯
1
·R(cid:98)≤t⋆+m
pp1(cid:16)
p
p 1(cid:17)(cid:16)
κ¯
2
·n1 (cid:88)n
I(W;Z i)+κ¯
3(cid:17)p− p1 VaT rah di hs at nhe [o 3r 2e ,m Leis ma ms ain 2g .1le ]-d ar na dw thv eer ps rio on bao bf ilt ih tye iD so tan ks ek ner wa in td
h
respecttothedrawoftherandomtrainingsetS andtherandom
− i=1
returned hypothesis W.
holds for all c (0,1] and all γ >1, where
i i
∈ In particular, for every loss with a range bounded in [0,1],
t⋆
:=mp1(cid:16)
κ¯
1 (cid:88)n
I(W;Z )+κ¯
(cid:17)− p1
.
considering the convex function
p 2 · n i 3 (cid:0) (cid:1)
i=1 f(W,S)=d R(cid:98)(W,S) R(W)
∥
For losses with a bounded variance, the tightest result we
as in [33, Corollary 2.1] ensures that ∆ = ξ(n), where ξ(n)
know of is from [31, Appendix H], where they show that if
is the same as in (1), and then, for every β (0,1), with
the loss has a variance bounded by σ2, then ∈
probability no smaller than 1 β,
E[R(W)] ≤E[R(cid:98)(W,S)]+ n1 (cid:88) i=n 1(cid:113) σ2χ2(PZ Wi,P W)
d(cid:0)
R(cid:98)(W,S)
R(W)(cid:1)
log− (cid:16) dd QPS W W(W)(cid:17) +logξ( βn)
(13)
∥ ≤ n
and that (cid:0) (cid:1)
holds, where d(rˆ r)=D Ber(rˆ) Ber(r) . Equation (13) is a
(cid:114) χ2 single-draw versi∥ on of the Seeger∥ –Langford bound [26, 27].
E[R(W)] E[R(cid:98)(W,S)]+ σ2 .
≤ · n Finally, using the variational representation of the relative
Similarly to before, the presented Theorem 9 improves these entropy borrowed from f-divergences [29, Theorem 7.24] and
results exponentially on the dependence with χ2 due to the operating like in [2, Appendix A.1] completes the proof for
relative entropy bound D log(1+χ2). losses with a range contained in [0,1]. Scaling appropriately
≤ extends it to losses with a range contained in [0,b].
Theorem 9. For every loss with a variance bounded by σ2,
the inequality At this point, with Theorem 10, following the techniques
outlined in the main body of the paper to replicate the results
(cid:104) (cid:112) (cid:105)−1(cid:104) (cid:112) (cid:105)
R 1 2 C
MI
κ¯
1
R(cid:98) +2 σ2C
MI
for single-draw PAC-Bayes guarantees is routine. The only
≤ − + · relevant difference is that to choose the optimal parameter λ in
holds for all c i (0,1] and all γ i >1, where Theorem 2 as outlined in Appendix A, the quantization of the
∈
1
(cid:88)n event space is now done with respect to log(cid:0) dPS W/dQ W(W)(cid:1)
C :=κ¯ I(W;Z )+κ¯ . andtakingintoaccountthatthisquantityisrandomwithrespect
MI 2 · n i 3
i=1 to both the training set S and the hypothesis W. That is, the
D. Extension to single-draw PAC-Bayes bounds sub-events in the proof are defined as
Like in the previous section, we first obtain a (cid:26) (cid:16)dPS=s (cid:17) (cid:27)
:= (s,w) n :k 1 log W (w) k .
“single-draw PAC-Bayes” analogue to the fast rate Ek ∈Z ×W − ≤ dQ ≤
W
bound from [2]. Throughout this section, we will
d Ce ′ nfi ,βn ,e S,WC n, :β =,S,W 2log: (cid:0)= dPS W/lo dg Q W(cid:0) d (P WS W/ )d (cid:1)Q +W(W log)(cid:1) (cid:0) π2+ e2ξl (o ng )/(cid:0) βξ (cid:1)( ,n)/ aβ n(cid:1) d, T ofhT e Toh hre ee omla rs e1t m2tw b 2o e alr o ne w dsu , Tl at hrw e eoe th rp ee mr se is n 5e g ,n lt ae si -n l pet t rh t oei ms rs ( isse eic n dt gio blen e-, fd oT r rah ewe .o ) Or ee nxm ct ee1 n a1 s gia o an in nd s
,
C′ n′
,β,S,W
:= κ2/n ·C′
n,β,S,W
+κ 3, while understanding that
these extensions are enabled by Theorem 10. To the best of
these two are random variables whose randomness comes from
our knowledge, these are also the first single-draw PAC-Bayes
therandomtrainingsetS andtherandomoutputhypothesisW.
results of this kind.Theorem 11. For every loss with a p-th moment bounded by beadistributionon independentofS suchthatPS Q
W W ≪ W
m , for all β (0,1), with probability no smaller than 1 β, a.s. and W′ be a random variable distributed according to
p
R(W) ≤κ
1
·R(cid:98)∈
≤t⋆(W,S)+m
pp1(cid:16)
p−p
1(cid:17)(cid:16)
κ
2
·C′
n,β n,S,W+κ
3(cid:17)− p− p1 Q prW ob. aD bie lfi itn ye n∆
o
s:= maE lle en rf t( hW a′ n,S 1). −Th βe ,n,foreveryβ ∈(0,1),with
1 (cid:20) (cid:18) dPS (cid:19) ∆(cid:21)
holds simultaneously for all c (0,1] and all γ >1, where f(W,S) log W(W) +log .
∈ ≤ n dQ β
t⋆ :=m
pp1(cid:16)
κ 2
·
C′
n,β n,S,W +κ
3(cid:17)− p1
. Proof. Consider the non-negative random variable
Theorem 12. For every loss with a variance bounded by σ2, X =enf(W,S)−logd dPS W Q (W).
for all β (0,1), with probability no smaller than 1 β,
∈ −
(cid:104) (cid:113) (cid:105)−1(cid:104) (cid:113) (cid:105) Using a change of measure we have that
R
≤
1 −2 C′ n′
,β,S,W +
κ
1
·R(cid:98) +2 σ2C′ n′
,β,S,W E(cid:20) ES(cid:20) enf(W,S)−logd dPS W
Q
(W)(cid:21)(cid:21)
holds simultaneously for all c (0,1] and all γ >1.
E. Extending Rivasplata’s sing∈ le-draw PAC-Bayesian theorem =E(cid:20) ES(cid:20) enf(W′,S)−logd dPS W
Q
(W′)
·
d dP QS W(W′)(cid:21)(cid:21)
Similarly to Germain et al. [33]’s PAC-Bayesian bound,
(cid:104) (cid:105)
Rivasplata et al. [34]’s single-draw PAC-Bayesian bound =E enf(W′,S) .
requires simultaneously that PS Q and that Q PS
W ≪ ≪ W Then, applying Markov’s inequality to the random variable X
a.s., since at some point in their proof they use the equality
d SP imS W/ ild aQ rly= t(cid:0) od BQ/ éd gP iS
W
n(cid:1) e− t1 a, l.w [3h 5ic ]h
,
won hl oy lih fo teld ds thw eh re en quth iri es mh ea np tp te hn as t. we h Pav (cid:20)e et nh fa (t
W,S)−logd dPS W Q (W) 1 E(cid:104) enf(W′,S)(cid:105)(cid:21) β.
Q PS a.s., we present below Rivasplata et al. [34]’s result ≥ β · ≤
≪ W
without that extra requirement as well as a simple proof to
Since the logarithm is a non-decreasing, monotonic function
avoid that requirement.
we can take the logarithm to both sides of the inequality and
Theorem 13 (Extension of Rivasplata et al. [34, Theorem 1 re-arrange the terms to obtain the desired result.
(i)]). Considerameasurablefunctionf : R.LetQ
W
W×S →