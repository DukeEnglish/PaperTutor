The Anatomy of Adversarial Attacks:
Concept-based XAI Dissection
Georgii Mikriukov1,2[0000−0002−2494−6285], Gesina
Schwalbe3[0000−0003−2690−2478], Franz Motzkus1[0009−0009−4362−7907], and
Korinna Bade2[0000−0001−9139−8947]
1 Continental AG, Germany
{firstname.lastname}@continental-corporation.com
2 Hochschule Anhalt, Germany
{firstname.lastname}@hs-anhalt.de
3 University of Lübeck, Germany
{firstname.lastname}@uni-luebeck.de
Abstract. Adversarial attacks (AAs) pose a significant threat to the
reliability and robustness of deep neural networks. While the impact of
these attacks on model predictions has been extensively studied, their
effect on the learned representations and concepts within these models
remains largely unexplored. In this work, we perform an in-depth anal-
ysis of the influence of AAs on the concepts learned by convolutional
neural networks (CNNs) using eXplainable artificial intelligence (XAI)
techniques. Through an extensive set of experiments across various net-
work architectures and targeted AA techniques, we unveil several key
findings. First, AAs induce substantial alterations in the concept com-
positionwithinthefeaturespace,introducingnewconceptsormodifying
existing ones. Second, the adversarial perturbation itself can be linearly
decomposedintoasetoflatentvectorcomponents,withasubsetofthese
beingresponsiblefortheattack’ssuccess.Notably,wediscoverthatthese
components are target-specific, i.e., are similar for a given target class
throughout different AA techniques and starting classes. Our findings
provide valuable insights into the nature of AAs and their impact on
learnedrepresentations,pavingthewayforthedevelopmentofmorero-
bustandinterpretabledeeplearningmodels,aswellaseffectivedefenses
against adversarial threats.
Keywords: Concept-basedXAI·ConceptActivationVectors·Concept
Discovery · Adversarial Attack · Security
1 Introduction
Deep neural networks (DNN), in particular convolutional DNNs (CNNs), have
achieved remarkable success in computer vision [54], but are known to be vul-
nerable to adversarial evasion attacks (AA) [1,51,28,34]: For a given input an
attackercaneasilyfindtargeted,smallperturbationsoftheinputthatareseem-
ingly irrelevant [6] or even imperceptible [28] to humans, but strongly change
4202
raM
52
]GL.sc[
1v28761.3042:viXra2 G. Mikriukov et al.
the CNN output [23,51], possibly causing erroneous outputs. CNN vulnerability
to AAs has raised significant concerns regarding their reliability and robust-
ness [51,23]. While the impact of AAs on model predictions has been exten-
sivelystudied[1,34,23,32],theireffectontheinternalrepresentationslearnedby
these models remains largely unexplored. Understanding how AAs influence the
learnedrepresentationsiscrucialfordevelopingrobustCNNs,aswellaseffective
defenses against adversarial threats.
Recent advancements in XAI have provided valuable tools for probing the
internal representations learned by deep neural networks [46,48,30,18]. In par-
ticular, XAI methods for unsupervised concept embedding analysis [47] have
proven successful in identifying frequent patterns in CNN latent representations
that are used for the CNN’s decision-making [53,22,20]. These reoccurring pat-
terns respectively linear components are referred to as concepts.
In this work, we perform an in-depth analysis of the influence of strong
targeted AAs on the usage of concepts learned by CNNs using concept-based
XAI techniques. Inspired by the perspective of Ilyas et. al [28] that “adversarial
examples are no bugs, they are features”, we hypothesize that AAs exploit and
manipulate the learned concepts within latent spaces of these models. Through
a comprehensive set of experiments across various network architectures and
targeted strong white-box AA types, we unveil several key findings that shed
light on the nature of AAs and their impact on learned representations:
(1) Adversarial attacks substantially alter the concept composition
learned by CNNs, introducing new concepts or modifying existing ones.
(2) Adversarial perturbations can be decomposed into linear compo-
nents,onlyasubsetofwhichisprimarilyresponsiblefortheattack’ssuccess.
(3) Different attacks comprise similar components, suggesting they nudge the
CNN intermediate outputs towards specific common adversarial feature
space directions, albeit with varying magnitudes.
(4) The learned adversarial concepts are mostly specific to the target class,
agnostic of the class the attack starts from. This implies that attacks abuse
target-specific feature space directions.
These insights into AAs’ impact on learned representations open up new direc-
tions for the future design of more robust models and adversarial defenses.
The remainder of the paper is structured as follows: After setting this paper
in the context of related work (Sec.2), we introduce the necessary background
on used AA and XAI techniques (Sec.3), and the common experimental setup
(Sec.4)forinvestigatingourhypothesis.Section5thenpresentstheexperimental
studies and their results. We conclude in Section6 with a discussion of implica-
tions for the understanding and handling of AAs.
2 Related Work
Adversarial attacks AAsonCNNshavereceivedincreasedattention[1]since
the first description of the phenomenon in 2014 [51,23]. These attacks generateThe Anatomy of Adversarial Attacks: Concept-based XAI Dissection 3
perturbations to input data that can lead to erroneous model predictions with
high confidence [23]. Numerous attack methods have been proposed, ranging
from digital to physically implemented manipulations [15], and from white-box
attacks against fully known models to purely query-based ones [1]. Commonly
usedstrongAAs,i.e.,onesachievingthelargestoutputdeviationatminorinput
changes, are the here considered white-box attacks [1,8]. First of its kind was
thegradient-basedFastGradientSignMethod(FGSM)[23],refinedtotheBasic
IterativeMethod(BIM)[32],andProjectedGradientDescent(PGD)[34].These
werelatersupersededbytheoptimization-basedCarliniandWagner(C&W)[8]
attack. Another common type of attack is adversarial patch [6], which aims to
simulatereal-worldscenariosbyrestrainingperturbationstobelocal.Thisallows
them to be physically realized, e.g., via stickers [15].
Considerableeffortswereinvestedtodevelopdefensesagainstadversarialat-
tacks [3,1]. However, many of these are still vulnerable to adaptive attacks [2,7],
highlighting the ongoing arms race between attacks and defenses. Meanwhile,
theunderlyingnatureandcausesofadversarialvulnerabilitiesisnotsatisfyingly
clarified:Hypothesesincludethattheyoriginfrominvisible,class-correlatedfea-
tures in the data [28], high-frequency image features [52], high dimensionality
[21] and curvature [13] of CNN latent spaces, and error amplification along sin-
gularvulnerablelatentfeatures[33].Butnonecouldbeproventocoverallcases
of adversaries, leaving the nature of AAs an open question.
Concept-based XAI Methods While it might be more desirable to use
fully transparent models [43], black-box CNNs are hard to replace for vision
tasks [54]. Fortunately, post-hoc XAI methods allow to explain many aspects
of a trained black-box CNN [46], including, e.g., the importance of input fea-
tures for decisions [44], more general model behavior (using simplified surro-
gates)[41,10],andinternalinformationflow [25].Thequestionwhatinformation
is encoded in CNN intermediate outputs is tackled by the subfield of concept-
based XAI methods: Here, CNN latent space structures are associated with
human-understandable symbolic concepts [45]. Early works started to explain
the meaning of single CNN units [4,38]. Kim et al. [30] soon after found that
more generally vectors respectively directions in CNN latent space are a favor-
abletargetforexplainingtheinformationencodedinCNNintermediateoutputs
(otherthannon-linearapproaches[14]).Linearapproachesnowadayseitherwork
supervised, i.e., find concept vectors for user-pre-defined concepts [30,18]; or, as
considered here, unsupervised, by extracting main linear components from CNN
intermediate outputs, to then visualize and interactively interpret them [53,22].
Note that, unlike supervised concept-based analysis, these patterns, i.e., con-
cepts, are not always human-interpretable. The concept discovery is commonly
achievedthroughtechniquessuchasclusteringofactivations[22],activationpix-
els [40], and—more effective—matrix decomposition methods like NMF [53,16],
PCA [53,16], ICA [16] or RCA [16]. This study will focus on NMF and PCA, as
the former provides more interpretable concepts, while the latter offers greater
numerical efficiency [53]. Visualization of a component is done by highlighting
spatialinputregionswherethecomponentisprominentinCNNactivationmap4 G. Mikriukov et al.
locations. That way, unsupervised concept-based approaches allow us to gain
insights into the semantic features the CNN has learned to use, enabling us to
observe changes in used knowledge at the semantic level. This is here leveraged
to assess the influence of adversarial attacks on CNN knowledge.
AdversarialAttacksandXAI WhileadversarialattacksandXAItechniques
have been extensively studied, their intersection has received limited attention.
Recent interdisciplinary works have only explored the use of feature importance
XAI methods for the detection of adversarial samples [19,17,29,49,9,42]; and
showed, that both feature importance [13,21] and concept representations [5]
can be attacked, i.e., modified in a targeted manner via input perturbations.
However,whentargetingCNNoutputs,theimpactofAAsonthelearnedrepre-
sentationsandconceptswithindeepneuralnetworksremainslargelyunexplored.
First investigations on the nature of AAs only investigated origins in input fea-
tures[28].Laterones,goinginsidetheCNN,wererestrictedtooverallstructural
properties like dimensionality [21] and curvature [13], or the impact of singular
neurons to amplify errors [33]. To our knowledge, we are the first to take a look
at the interplay between AAs and learned concepts in CNN latent spaces.
3 Background
In this section, we provide details of adversarial attacks (Sec. 3.1), concept dis-
covery (Sec. 3.2), and concept comparison methods (Sec. 3.3) used in this work.
3.1 Adversarial Attacks
Given an input image x∈Rh×w and a classification model f: Rh×w →Y with
original prediction y =f(x)∈Y, the primary objective of an adversarial attack
istodiscoverorgenerateaperturbationδofmaximumsize∥δ∥≤ϵsuchthatthe
outputoff ontheadversarialexamplex+δ changes(f(x+δ)̸=y,respectively
the output difference ∥f(x+δ)−f(x)∥ is large for chosen metric ∥·∥). The size
constraintensuresthatxanditsperturbedversionx+δ aresimilarrespectively
indifferentiable for humans.
In this work, we aim to investigate the impact of adversarial attacks on the
level of concepts, and in particular relative to the AA’s impact on the DNN
output. As this requires fine-grained control on the severity and direction of
attacks,weherefocusontargeted white-box optimization-based attacks.Targeted
attacks aim to manipulate the model’s prediction from the true class y to a
specificpre-definedtargetclassy′,thusensuringf(x+δ)=y′.White-box attacks
utilize model internal information like gradients to generate the perturbation,
and optimization-based ones optimize from x to x+δ in a controllable step-
wise manner. Our utilized attacks cover a wide range of common white-box
attack techniques explained in the following: BIM [32], PGD [34], C&W [8] and
adversarial patch [6], visualized in Fig. 1.The Anatomy of Adversarial Attacks: Concept-based XAI Dissection 5
x
“Fire truck”
+ + + +
BIM PGD C&W Patch
δ ✖.031 ✖.031 ✖.047 Perturbation
= = = =
x + δ
“Banana”
Fig.1: Examples of BIM, PGD, C&W, and Patch Attack adversarial samples:
“fire truck” attacked with target “banana”.
BIM The Basic Iterative Method (BIM) [32] is an iterative variant of the orig-
inal FGSM attack [23]. In BIM, adversarial perturbations are iteratively com-
puted by changing x in small steps in direction of the gradient until x+δ is
misclassified. Formally:
xt+1 =clip (cid:0) xt+α·sign(∇ J(f(xt),y′))(cid:1) (1)
ϵ x
where t represents the optimization step, J(·) denotes the cost function (e.g.,
cross-entropy), ∇ the gradient at x, α is a step size hyperparameter, sign(·) is
x
theelement-wisesignumfunction,andclip (·)clipsx+δ element-wisesuchthat
ϵ
it lies in the L ball of radius ϵ around x.
∞
PGD Another powerful iterative attack is the Projected Gradient Descent
(PGD) [34]. Unlike the value clipping approach used in BIM, the PGD attack
projects gradients to the L∞ ϵ-ball around the original image:
xt+1 =proj (cid:0) xt+α·sign(∇ J(f(xt),y′))(cid:1) (2)
ϵ x
where proj denotes the projection onto the ϵ-ball centered at x.
ϵ
C&W TheCarlini&Wagner(C&W)[8]attackaimstofindthesmallestpertur-
bation δ that leads to misclassification. It formulates the attack as an optimiza-
tionproblem,usingtheL norm(typicallyL )tomeasurethesizeofthepertur-
p 2
bation. Formally, given a helper function h(·) fulfilling h(x′) ≤ 0 ⇔ f(x′) = y′,
they optimize:
min∥δ∥ +β·h(x+δ) (3)
p
δ
where ∥·∥ denotes the L norm, and β is a hyperparameter that controls the
p p
trade-off between the size of the perturbation and the induced output change.
h(·) can be set to, e.g., h(x′)=1−J(f(x′),y′), where J(·) is the cross-entropy.6 G. Mikriukov et al.
Adversarial Patch An adversarial patch [6] is the most common real-world
attack[1,15,6].Here,δ isconstrainedtobelocaltoaregionloc,i.e.,onlychange
pixels within loc. The induced input changes may be visible to humans, but are
easily overlooked as being irrelevant, and can be physically implemented (e.g.,
as stickers [15]), cf. Fig.1. The choice of the patch δloc and its location loc on
the image can vary depending on the specific objectives of the attack [1].
3.2 Concept Discovery with Matrix Factorization
For the concept discovery with matrix factorization, the main assumption is
that CNNs learn few relevant concepts which can be represented as a linear
combination of convolutional filters [18,53]. I.e., concept-related information is
distributed across channels of sample activation maps. For the formalization
assume that we are given a set X of b probing input images with activations
A = f (X) ∈ R(b×h×w)×c in a selected layer L of the tested CNN f, where
→L
b, h, w, and c are batch, height, width, and channel dimensions respectively.
The decomposition techniques aim to find a matrix M ∈ Rk×c of k compo-
nents/concepts that allows to (approximately) write each activation map pixel
a∈Rc inAasalinearcombinationofthecomponentsinM.Formally,oneopti-
mizestheconceptsM andweightsW ∈R(b×h×w)×k suchthatthereconstruction
error is minimized with respect to the Frobenius norm:
min∥A−WM∥ (4)
W,M
To visualize how concepts are activated in new images, one can project concept
information back to the input: Assume we already have concept components
M from optimization on a concept probing set A. Now we want to find for
a new activation map A′ ∈ R(1×h×w)×c which image region activated which
concepts. To do so we follow [53,16,37] and obtain W′ := A′ · M. Resulting
W′ ∈R(1×h×w)×k holds for each of the k components a concept saliency map of
size h×w that tells at which activation map pixel how much of the component
waspartofthepixelvector.Usingthespatialalignmentofactivationmappixels
withtheinput,onecanscalesuchconceptsaliencymapsfromanylayertomatch
the input and, e.g., visualize them as overlays (cf. Fig.4).
PCA Here, one uses mean-centered activations A = A − µ , where µ is
⊙ A A
the mean of A, ensuring that the found principal components represent the
directions of maximum variance in the data. In PCA, the component tensor
MPCA represents the top-k largest eigenvectors of the covariance matrix of A .
⊙
Theseeigenvectorscapturethedirectionsofmaximumvarianceinthedata.The
factorization reads:
A ≈WMPCA (5)
⊙The Anatomy of Adversarial Attacks: Concept-based XAI Dissection 7
NMF A fundamental constraint of NMF is the non-negativity of activations,
i.e., it works on A+ (with (·)+ := ReLU(·), where ReLu(z) = max(0,z)), and
yieldscomponentsandweightswithnon-negativeentries(M =M+,W =W+).
Thisconstraintalignswiththeinterpretationthatthek componentsandshould
represent additive contributions. Formally:
A+ ≈W+M+ (6)
3.3 Concept Comparison
Comparisonofconceptsisimportantfortheestimationofthesimilarityofknowl-
edge they represent. Concepts within the same feature space can be effectively
comparedusingvariousvectoroperations:cosinesimilarity[30,35],vectorarith-
metic [18], and distance-based metrics [11,53,40,36]. In this work, due to the
nature of concepts — a linear combination of convolutional filters — obtained
from decomposition-based concept discovery methods, we utilize cosine similar-
ity for comparing any two discovered concepts u,v:
u·v
sim(u,v)= (7)
∥u∥·∥v∥
This ensures that only the direction of vectors is considered for the comparison.
Whencomparingconceptsobtainedfromdifferentlayersand/ormodels,i.e.,
indistinctlatentspaces,oneneedstoprojecttheconceptinformationtoacom-
mon space, such as input or output. One output-based approach are metrics
for estimating the attribution of concepts to the CNN outputs. These metrics
includegradient-basedapproaches[30,11],saliency-basedmethods[37],orthose
based on concept similarity ranking [37]. We here compare concepts quantita-
tivelyusingtheJaccardIndex(IoU)oftheirconceptsaliencymapsafterscaling
them to match the input size, as proposed in [37].
4 Experimental Setup
In this section we present the selection of models (Sec. 4.1), data used in exper-
iments (Sec. 4.2), and layers selection for the analysis of concepts and internal
representations (Sec. 4.3).
4.1 Models
Inourexperimentsweuseclassificationmodelsofdifferentarchitecturestrained
on ImageNet [12] dataset from PyTorch model zoo1:
– VGG-11 [50] (VGG)
– Compressed SqueezeNet1.1 [27] (SqueezeNet),
– Inverted residual MobileNetV3-Large [26] (MobileNet)
– ResidualResNet18andResNet50[24].Twonetworksofthesamearchitecture
are used to investigate the impact of the model size.
All models are pre-trained on the ImageNet1k [12] dataset.
1 https://pytorch.org/vision/stable/models#classification8 G. Mikriukov et al.
Table1:SelectedCNNlayersforlatentspacecomparisonandconceptdiscovery
(f=features, l=layer).
Selected layers
Classifier
Latent space comparison Concept discovery
VGG f.4 f.9 f.14 f.19 f.19
SqueezeNet f.3 f.9 f.9 f.12 f.12
MobileNet f.3 f.7 f.16 f.11 f.16
ResNet18 l1.1 l2.1 l3.1 l4.1 l4.0
ResNet50 l1.1 l2.2 l3.4 l4.2 l4.2
4.2 Data
We conduct our experiments using diverse classes from a validation subset of
ILSVRC2017 [12]. Specifically, we selected four classes from the vehicle su-
percategory (taxi, fire truck, garbage truck, pickup truck), two classes
from the animal supercategory (horse, zebra), and two classes from the fruit
supercategory (orange, banana).
For each of the selected classes, we chose 50 images, and for each net-
work subjected them to targeted cross-attacks using gradient-based BIM [32],
PGD [34], and C&W [8] (see Sec. 3.1). Gradient attacks were executed using
torchattacks [31] library2. Additionally, we used the ImageNet-Patch3 [39]
dataset, which comprises pretrained adversarial patches across 10 categories, to
implementnetwork-agnosticpatchattacks(seeSec.3.1).Specifically,weapplied
patches from the banana category to all images.
As a result, for our experiments, we utilized 400 clean images, 400 patch-
attacked images, and a total of 42000 = (8 × 7) × 50 × 3 × 5 (class pairs ×
images/attack×attack types×models) gradient-attacked images (Fig. 1).
4.3 Layer Selection
For our experiments, we defined two groups of layers: layers for latent space
comparison and layers for concept discovery. The selected layers for all tested
networks are listed in Table 1.
Layers for comparing latent space representations (see Sec. 5.1) are evenly
distributed along the depth of the model. In these layers, we assess the impact
of adversarial attacks on the internal representations of the models.
Fortheconceptdiscoveryinadversarialattacks(see.Sec5.2,Sec5.3)weuse
another set of layers, which are located deeper in the networks. This allows us
to extract and compare high-level abstract concepts. This selection is based on
the subjective qualitative assessment of the extracted layers.
2 https://github.com/Harry24k/adversarial-attacks-pytorch
3 https://github.com/pralab/ImageNet-PatchThe Anatomy of Adversarial Attacks: Concept-based XAI Dissection 9
5 Experimental Results
Inthissection,weexaminetheinfluenceofAAsonsamplerepresentationswithin
thelatentspace(Sec.5.1).Subsequently,leveragingthisanalysis,weemploycon-
cept discovery (mining) to quantitatively and qualitatively assess the alteration
of concepts before and after the attacks (Sec.5.2). Finally, we analyze the com-
ponents of adversarial perturbation through concept discovery (Sec.5.3).
5.1 Adversarial Attacks Impact on Latent Space Representations
? Research Question:What is the impact of adversarial attacks on sample
embeddings within the feature space?
To assess this, we evaluate the cosine similarities between attacked and non-
attackedsamplesacrosseachorigin-targetclasspair.Figure2andFigure3show
the mean curves and standard deviation intervals for the garbage truck →
garbage truck banana orange taxi pickup zebra
1.0
0.8
0.6 BIM
PGD BIM BIM
0.4 CW PGD PGD
PATCH CW CW
0.2
f.4 f.9 f.14 f.19 f.4 f.9 f.14 f.19 f.4 f.9 f.14 f.19
1.0
0.8
0.6 BIM
PGD BIM BIM
0.4 CW PGD PGD
PATCH CW CW
0.2
f.3 f.7 f.11 f.16 f.3 f.7 f.11 f.16 f.3 f.7 f.11 f.16
1.0
0.8
0.6 BIM
PGD BIM BIM
0.4 CW PGD PGD
PATCH CW CW
0.2
l1.1 l2.2 l3.4 l4.2 l1.1 l2.2 l3.4 l4.2 l1.1 l2.2 l3.4 l4.2
Layers
Fig.2:Meanandstandarddeviationvaluesofcosinesimilaritiesfororiginaland
attacked activation maps of test samples for several attacks.
ytiralimiS
91.f
:GGV
61.f
:teNeliboM
2.4l
:05teNseR10 G. Mikriukov et al.
garbage truck banana orange taxi pickup zebra
1.0
0.8
0.6 BIM
PGD BIM BIM
0.4 CW PGD PGD
PATCH CW CW
0.2
f.3 f.6 f.9 f.12 f.3 f.6 f.9 f.12 f.3 f.6 f.9 f.12
1.0
0.8
0.6 BIM
PGD BIM BIM
0.4 CW PGD PGD
PATCH CW CW
0.2
l1.1 l2.1 l3.1 l4.1 l1.1 l2.1 l3.1 l4.1 l1.1 l2.1 l3.1 l4.1
Layers
Fig.3:Meanandstandarddeviationvaluesofcosinesimilaritiesfororiginaland
attacked activation maps of test samples for several attacks.
banana, orange → taxi, and pickup → zebra attacks across selected layers
(Tab. 1) of all tested networks.
Across all attack types, we observe a “snowball effect” of cosine similarity
decline: the similarity diminishes exponentially as we move closer to the deeper
layers, indicating a higher impact of attacks on internal representations within
these layers. This effect is particularly pronounced in deeper networks contain-
ingmorenon-linearlayers.InthefinallayersofResNet50andMobileNet,cosine
similaritynearlyreachesavalueof0.2forBIMandPGDattacks.Incomparison
tootherattacks,theC&Wattacktypicallyinducessmallerperturbations,align-
ing with its original intention of seeking the minimally sufficient perturbation.
The perturbation observed in the initial layers of the adversarial patch attack
(PATCH)(garbage truck→banana)resemblesthatofBIMandPGDattacks,
yetthedeclineinsimilarityislesssteep,endinginthelastlayeratapproximately
the same level as C&W. Based on these findings, we proceed with the following
conceptdiscoveryexperimentinthedeeplayersofthenetwork,whereactivation
maps are most substantially perturbed.
⋆ Summary:Adversarial attacks cause attacked latent space representations
with increasing depth to increasingly deviate from the original representations
with respect to cosine similarity. This error amplification over depth is superlin-
ear and holds across all networks, attacks, and origin-target class pairs.
ytiralimiS
21.f
:teNezeeuqS
0.4l
:81teNseRThe Anatomy of Adversarial Attacks: Concept-based XAI Dissection 11
5.2 Concept Discovery in Adversarial Samples
? Research Question:Whatistheimpactofadversarialattacksonthemain
components (concepts) present in latent representations?
Original: fire truck BIM: fire truck taxi CW: fire truck taxi
Concept Similarity Concept Similarity
100 100
c0 67.1 3.4 6.1 4.6 4.3 c0 88.2 1.6 1.1 7.0 4.7
80 80
c1 1.2 33.5 0.7 12.4 3.4 c1 1.0 75.8 0.8 1.1 2.5
60 60
c2 0.9 3.6 30.3 3.3 0.5 c2 1.1 1.4 93.0 3.8 1.1
40 40
c3 7.2 1.7 29.8 3.3 5.5 c3 6.4 1.2 3.2 91.5 2.9
20 20
c4 9.8 8.6 1.3 8.9 57.3 c4 8.3 6.9 0.7 1.7 71.2
0 0
c3 c1 c2 c4 c0 c0 c1 c2 c3 c4
BIM: fire truck taxi CW: fire truck taxi
Fig.4:ConceptminingresultsforBIMandC&W(fire truck→taxi)attacks
in layer layer4.0 of ResNet18. Top: pairs of top-2 most relevant prototypes of
discoveredconceptscXwithrankXandconceptweights(importances);Bottom:
discovered concept similarities (Sec. 3.3) for original vs. BIM (left) and original
vs. C&W (right)
16.7-
:0C
77.12-
:4C
57.32-
:3C
29.46-
:2C
68.56-
:1C
kcurt
erif
:lanigirO
89.82
:4C
94.2-
:0C
79.42-
:3C
57.84-
:2C
79.76-
:1C
kcurt
erif
:lanigirO
77.11-
:3C
01.21-
:4C
54.72-
:0C
47.63-
:2C
56.64-
:1C12 G. Mikriukov et al.
In Figure 4, we showcase qualitative and quantitative outcomes of concept
discovery using ICE [53] and saliency-based concept comparison [37] in layer4.0
ofResNet18.Wefocusonoriginalfire trucksamplesandsamplesattackedby
BIM and C&W targeting taxi (PGD results omitted due to their similarity to
BIM; patch attack results omitted as they lead expectedly to the discovery of
the banana patches as a distinct concept).
Qualitatively, (1) AAs modify the concept information, resulting in concept
saliencymapchanges;e.g.,windshieldconcept(originalc4,BIMc0,andC&W
c4) highlights different areas in 1st prototypes. (2) AAs may introduce new
concepts; e.g. BIM c4 is a new spurious concept that cannot be interpreted. (3)
Additionally, a change in most similar concept prototypes can be observed: e.g.,
2ndprototypesof windshield(originalc4,BIMc0,andC&Wc4)aredifferent.
Quantitatively, we observe (1) changes of values in concept similarity ma-
trices (Fig. 4, bottom); (2) alterations of concept importance (e.g., concept
cabin bottom weights; e.g., original c0: -7.61, BIM c3: -24.97, and C&W
c0: -27.45),whichmayresultin(3)conceptrankshuffling(e.g.,conceptcabin
bottom: original c0: 1st, BIM c3: 3rd, and C&W c0: 3rd).
To find similar concepts (counterparts), columns of the concept similarity
matrix are permuted to maximize the sum of the matrix diagonal. The diagonal
valuesindicatethemagnitudeofsimilaritybetweenthesecounterparts.Stronger
attacks result in more pronounced perturbations, leading to the emergence of
new concepts or the disappearance of old ones. Some concepts may have very
weak counterparts, which can be interpreted as no counterpart or that “concept
was changed”: e.g., BIM c4 (Fig. 4, bottom left matrix).
To measure it, for all discovered concepts for every origin-target attack pair,
we threshold to matrix diagonal values and estimate mean and 99% confidence
intervalsofcasecountswhenthevalueislowerthanusedthreshold.Theseresults
for all networks and threshold values of 75, 50, and 25 are depicted in Figure 5.
These results represent the average number of “concept change” occurrences un-
dertheadversarialattacks,servingasameasureofattackstrength.Numerically,
Threshold = 75 Threshold = 50 Threshold = 25
5
4
3
ResNet50
2 ResNet18
SqueezeNet
1 MobileNet
VGG
0
BIM PGD CW PATCH BIM PGD CW PATCH BIM PGD CW PATCH
Fig.5: Mean numbers of “concept changes” with 99% confidence intervals for
thresholdvalues75,50,and25inalltestedmodelsfortestedadversarialattacks.The Anatomy of Adversarial Attacks: Concept-based XAI Dissection 13
we observe similar behavior between BIM and PGD. C&W, which induces the
smallest perturbations among the tested attacks, results in the lowest number
of concept changes across all networks.
In some rare instances, depending on the chosen threshold, no concepts may
be modified. However, the impact of the AA can still be observed through the
evaluation of changes in concept weights, concept ranks, or concept prototypes.
For instance, in Fig. 4, each C&W attack concept (bottom right matrix) cor-
responds to a counterpart for which the concept weight is known. To estimate
changesinweightsandconceptranks,Spearman’srankcorrelationandPearson’s
correlation can be measured. For the C&W attack and the setup illustrated in
Figure5,thesecorrelationsare0.6and0.76,respectively,enablingthedetection
of concept changes.
⋆ Summary:Comparing concept composition in clean sample representations
to ones in adversarially attacked counterparts shows: AAs modify all of concept
saliencymaps,conceptweightsrespectivelyranking,andconceptsimilarities;and
even replace concepts in case of strong attacks.
5.3 Concept Analysis of Adversarial Perturbation
Denote by f = f ◦f be the decomposition of the CNN into the part up
L→ →L
to layer L an the part from L onwards. The representation of an adversarial
perturbation δ for a sample x in L’s latent space can be defined as
δ˜=δ :=f (x+δ)−f (x) (8)
x,L →L →L
Information Distribution in Adversarial Perturbation. Here, our goal
is to discover whether the information within adversarial perturbation δ˜ has
dominant directions or/and has an imbalanced distribution. In other words:
? Research Question:Can an adversarial perturbation in latent space be
efficiently represented using linear components? What is the minimum number
of components needed?
For this, we quantify the information distribution across channel dimensions
(see Sec. 3.2) of δ˜with PCA decomposition and assess the cumulative variance
explained by obtained components. We determine the minimum part of PCA
components (relative to the total number of components in a layer) needed to
preserve a defined fraction of the variance cumulatively. The total number is
equaltothenumberofconvolutionalfiltersineachdiscoverylayer(seeTable1).
In Table 2, we present the mean and standard deviations of these ratios, ex-
pressed as percentages (%). Across selected layers (Tab. 1 “Concept Discovery”)
of all networks and attack types, we observe an uneven distribution of informa-
tion among components: depending on the model and attack type, (1) 0.3% to
11.5% of components are sufficient to retain 50% of the whole variance; and (2)
preserving 7.6% to 58.3% of components, we can explain 90% of variance. (3)
In selected layers with a larger amount of filters, like in MobileNet (960) and
ResNet50 (2048), 99% of variance is explained by 27.1% (C&W of ResNet50)14 G. Mikriukov et al.
Table 2: Mean and standard deviation ratios (in %) of preserved PCA compo-
nents relative to the total number of components in layer (#filters) required
to retain a specified amount of variance.
Model:Layer RetainedVariance
Attack
(#filters) 50% 70% 80% 90% 95% 99%
BIM 8.5±1.1 22.1±1.9 34.1±2.2 53.1±2.5 68.2±2.5 88.4±1.8
VGG:f.19 PGD 8.6±1.2 22.3±2.0 34.3±2.3 53.3±2.5 68.4±2.4 88.5±1.7
(512) CW 6.5±1.5 18.0±2.8 28.9±3.6 47.4±4.2 63.2±4.0 86.1±2.4
PATCH 0.4±0.1 2.2±0.4 6.6±1.0 19.6±1.6 35.3±1.8 66.9±1.7
BIM 8.9±1.2 22.4±1.8 34.2±2.0 53.2±2.0 68.6±1.7 89.4±0.9
SqueezeNet:f.12 PGD 8.9±1.1 22.4±1.7 34.3±2.0 53.3±1.9 68.6±1.6 89.4±0.9
(512) CW 9.4±1.8 23.2±2.6 35.0±2.8 53.9±2.7 69.1±2.2 89.7±1.1
PATCH 0.9±0.2 4.9±0.6 11.1±1.1 25.8±1.5 41.7±1.9 72.5±1.4
BIM 3.7±0.6 11.5±1.3 19.8±1.7 35.6±2.0 50.9±2.0 77.9±1.4
MobileNet:f.16 PGD 3.8±0.6 11.6±1.2 19.9±1.6 35.7±1.9 51.0±1.9 77.9±1.3
(960) CW 2.3±0.7 7.7±2.0 13.9±3.4 26.5±6.0 39.5±8.6 65.4±13.4
PATCH 0.7±0.1 3.6±0.5 7.7±1.0 17.9±1.8 30.2±2.2 60.0±2.4
BIM 11.5±0.7 26.9±1.0 39.4±1.1 58.3±1.1 72.8±1.0 91.5±0.6
ResNet18:l4.0 PGD 11.5±0.8 26.9±1.1 39.4±1.2 58.3±1.2 72.9±1.0 91.5±0.6
(512) CW 10.4±2.0 24.0±3.9 35.2±5.3 52.8±6.9 67.2±7.6 87.7±7.0
PATCH 1.1±0.2 7.5±0.8 15.7±1.0 32.1±1.2 48.3±1.2 77.3±1.0
BIM 2.6±0.4 8.0±0.7 13.4±0.9 23.8±1.2 34.3±1.4 56.2±1.8
ResNet50:l4.2 PGD 2.7±0.4 8.0±0.7 13.4±0.9 23.7±1.2 34.2±1.4 56.0±1.6
(2048) CW 1.3±0.7 3.8±2.1 6.6±3.5 12.1±6.3 17.9±9.5 30.5±16.7
PATCH 0.3±0.1 1.4±0.3 3.2±0.5 7.6±0.9 13.0±1.3 27.1±1.8
to 77.9% (BIM, PGD of MobileNet) of components, whereas in models with
fewer filters (VGG, SqueezeNet, ResNet18: 512 filters in all selected layers) 99%
of variance is explained by 66.9% (PATCH of VGG) to 91.5% (BIM, PGD of
ResNet18).
BIM and PGD attacks create larger perturbations and consistently require
the highest portions of components to explain δ˜variance properly. In contrast,
the adversarial patch (PATCH) attack necessitates the smallest portion of com-
ponents. The behavior of the C&W attack varies across models: in some cases,
its results align with those of BIM and PGD, while in others, they resemble the
outcomes of the PATCH attack.
⋆ Summary:The majority of δ˜information is concentrated in a small subset
of linear components (concepts), i.e., δ˜ is built from few meaningful directions
in the feature space.
Concept Discovery in Adversarial Perturbations. From the previous ex-
periments we know that latent space perturbations originating from a given at-
tack do give rise to a global linear decomposition. For the discovery of concepts
in δ˜ we next utilize NMF, as it learns more meaningful directions than other
decomposition methods [53]. To ensure the non-negativity constraint in NMF,
like in ICE [53], we apply δ˜+ = ReLu(δ˜). Even though negative perturbationThe Anatomy of Adversarial Attacks: Concept-based XAI Dissection 15
information is lost, this approach still results in successful concept discovery.
Following the application of NMF (see Sec. 3.2) to δ˜+, we obtain M+ ∈ Rk×c.
Foreaseofnotationletm =M+/∥M+∥bethenormalizedi-thconceptvector.
i i i
Given the concepts, our next question is:
? Research Question:Are single linear components of adversarial pertur-
bations in latent space sufficient to reproduce the attack? In other words: Can
adversarial attacks be described by linear translations in latent space?
Recall that the m-component of δ˜for a vector m (i.e., the amount by which
δ˜points into direction m) is:
proj
(cid:0) δ˜(cid:1) :=(cid:0) δ˜◦m(cid:1)
m (9)
m
Wewouldnowliketocompareontheoutputstheeffectofslowlyapplyingδ˜with
the effect of slowly applying any of its components m . Slow application is here
i
realized by linearly interpolating between x˜:=f (x) and x˜+δ˜=f (x+δ)
→L →L
via x˜+γδ˜, γ ∈ [0,1]. To ensure comparability of the linear interpolations, we
interpolate each m via x˜+γproj (δ˜). This ensures that at most δ˜ is applied
i mi
to x˜. Finally, we estimate and compare the influence of the interpolations on
prediction confidences of original/target class cls for components m ,i ≤ k at
i
the same γ ∈[0,1]:
(cid:16) (cid:17) (cid:16) (cid:17)
f x˜+γδ˜ versus f x˜+γproj (δ˜) (10)
L→
cls
L→ mi
cls
Whenγ =0,thepredictionismadefortheoriginalimage,andatγ =1,forthe
image attacked by full perturbation respectively its full ith component.
In Figure 6, we display dependency graphs depicting the averaged — for
all related test samples — true class y (solid lines) and attacker target class y′
(dashedlines)outputconfidencecorrespondingtothemagnitudeγ ofδ˜(black)
andk =3NMFcomponentprojections(red,green,blue).Resultsareshownfor
everytechniquesofattackingpickup→bananaandforthreemodels(remainder
skipped due to space constraints; behavior was similar).
Walking towards original perturbation: In the case of original perturba-
tion δ˜ (black lines), we observe that (1) gradient attacks, which create large
perturbations (PGD, BIM), reach target confidences f (x˜+γδ˜) ≈1.0 at
L→ target
γ = 1 and and maintain this value thereafter, while (2) the low perturbation
attack (C&W) does not reach a value close to 1.0; however, it can be further
amplified by increasing γ > 1. (3) PATCH attack behaves similarly to C&W,
butinsomecasesthetargetconfidencesagaindropathighγ values(seeVGG).
In other models and origin-target combinations of AAs, the observed behavior
was consistent.
Walking towards linear perturbation components: For k = 3 concepts
discovered with NMF decomposition, we observe that (1) one or several NMF
components (typically the two most prominent ones red and green) in each of
thedemonstratedcasesleadtosuccessfulattacks,i.e.,increasingthetargetclass
confidences and decreasing original class ones. However, the decrease of original
and growth of target class confidences are slower than these for δ˜ (black). (2)
Discoveredconceptsbehavedifferently;thebluecomponentusuallycontributes16 G. Mikriukov et al.
pickup banana pickup NMF0 pickup NMF 1 pickup NMF 2 pickup
banana NMF 0 banana NMF 1 banana NMF 2 banana
VGG: f.19 MobileNet: f.16 ResNet50: l4.2
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.8
0.6
0.4
0.2
0.0
0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5
Perturbation (component) magnitude, γ
Fig.6:Thedependencyoforiginalandtargetclassprobabilitiesontheadversar-
ial perturbation magnitude γ averaged for all test samples. Original adversarial
perturbation(black)iscomparedtoperturbationprojectedonto3conceptsdis-
covered with NMF (red, green, blue).
positively to original class y probability, reinforcing it. This may be caused by
partial information loss resulting from the NMF non-negativity constraint. The
observed behavior of NMF concepts is consistent across all networks and ad-
versarial attack origin-target combinations. We observed a similar behavior for
ytilibaborp
ssalc
detciderP
MIB
DGP
WC
HCTAPThe Anatomy of Adversarial Attacks: Concept-based XAI Dissection 17
conceptsdiscoveredinPCA;however,theimpactofγ wasweaker,meaningthat
the decrease in y and the growth in y′ were slower. Due to this similarity, we do
not present visual results.
⋆ Summary:A given adversarial attack can be replaced by walking in latent
space into the direction of the most prominent linear component(s) of its adver-
sarial perturbations.
Similarity of Concepts Discovered in Adversarial Perturbation. Con-
sideringtheobservedsimilarityinthebehaviorofperturbationcomponents(red,
green, blue) and of the original perturbation δ˜(black) across all test cases, it
0.0 0.2 0.4 0.6 0.8 1.0
pickup taxi zebra banana banana zebra
C1 C1 C1
B1
B2 B1
P1
P1 P2 P1
C0 B0 B2
C0
B1 P2
PA0
P0 PA2 C0
C2 PA1 C2
C2
B0 B0
B2
P2 P0 P0
C2 B1 C0
P1
B1 B0
C1
P1 PA1 P0
C1 PA0 C1
B2
B2 B1
P0
P2 C0 P1
C0 PA2 C2
P2
B0 B2
B0
P0 C2 P2
C1 B1 C1
P1
B1 B1
C1
P1 PA1 P1
C0 PA0 C0
C2
B0 B2
B2
P0 P2 P0
C2 PA2 C2
C0
B2 B0
B0
P2 P0 P2
Fig.7: Similarity heatmaps of concepts (3 per attack) discovered in adversarial
perturbationswithNMF.ConceptsaredenotedasAttackType-ConceptIdpairs
(Attack types: B=BIM, P=PGD, C=C&W, PA=Patch Attack).
91.f
:GGV
61.f
:teNeliboM
2.4l
:05teNseR
1C
2C
1C
2B
1B
1B
1P
1P
1P
0C
1C
0C
1B
2B
0B
0P
2P
0P
2C
0C
2C
0B
0B
2B
2P
0P
2P
1C
1B
1B
1B
1P
1P
1P
1C
1C
2P
1AP
1AP
0B
0AP
0AP
0C
2B
2C
0AP
0P
2B
2AP
0C
2P
1AP
2AP
2AP
2C
2P
0C
2B
0B
0B
0P
2C
0P
1C
0C
1C
1B
0B
1B
1P
0P
1P
2B
1C
0C
2P
1B
2B
0C
1P
0P
2C
2C
2C
0B
2B
0B
0P
2P
2P18 G. Mikriukov et al.
0.0 0.2 0.4 0.6 0.8 1.0
pickup taxi zebra banana banana zebra
C1 PA2 C1
PA0
B2 B1
P2
P2 B1 P1
C0 C1 C0
C0
B1 B0
B0
P1 P0 P0
C2 PA1 C2
C2
B0 B2
B2
P0 P1 P2
C1 PA1 C2
C1
B1 B1
B1
P1 P1 P1
C0 C0 C0
B0
B0 B0
P0
P0 C2 P0
C2 B2 C1
P2
B2 B2
PA0
P2 PA2 P2
Fig.8: Similarity heatmaps of concepts (3 per attack) discovered in adversarial
perturbationswithNMF.ConceptsaredenotedasAttackType-ConceptIdpairs
(Attack types: B=BIM, P=PGD, C=C&W, PA=Patch Attack).
is a valid assumption that we detected similar concepts in tested AAs, possibly
with varying magnitude depending on the attack technique. In other words:
? Research Question:Do adversarial attacks comprise concept vectors of
similar directions in the feature space?
To investigate the similitude of concept vectors discovered in δ˜across differ-
ent attack techniques we estimate cosine similarities (see Eq. 7) for each pair
of vectors. In Figs.7 and 8 we present the similarity clustermaps4 (clustered
heatmaps) of concept vectors M+ discovered with NMF for all tested attack
techniques and different origin-target attack pairs.
Similarities across attack techniques: We observe that (1) concepts dis-
covered in different attack techniques for the same are similar (co-directed) and
form clusters (visible as groups of brighter pixels). For instance, PA1, C1, B1,
and P1 group of ResNet18 (for attacking zebra→banana) contains one vector
of each category. (2) At least one of such groups is observed per clustermap.
Similar behavior was observed in other origin-target attack pairs not visualized
here.(3) Adversarial patch attack vectors are the most distinct, as evident in
the zebra→banana column. This distinction can be attributed to the different
nature of the attack compared to the other gradient-based attack techniques.
4 https://seaborn.pydata.org/generated/seaborn.clustermap
21.f
:teNezeeuqS
0.4l
:81teNseR
1C
1C
2B
1B
2P
1P
0C
0C
1B
0B
1P
0P
2C
2C
0B
2B
0P
2P
2AP
1AP
0AP
1C
2P
1B
1B
1P
1C
0C
0C
0B
0B
0P
0P
2C
1AP
2B
2C
2P
2B
0AP
1P
2AP
1C
2C
1B
1B
1P
1P
0C
0C
0B
0B
0P
0P
2C
1C
2B
2B
2P
2PThe Anatomy of Adversarial Attacks: Concept-based XAI Dissection 19
Similarities across origin classes: We further expand the pairwise concept
vectorcomparisontovectorsoriginatingfromanycategory,onlyfixingtheattack
target class. In other words, we investigate whether the learned directions of
conceptvectorsM+ discoveredwithNMFarepurelytarget-specific,respectively
in how far they are agnostic to attack type and original true class. In Fig.9 we
showcase a concept similarity clustermap for ResNet50, where concept vectors
are targeting the taxi class (any → taxi). Similar to notations in Fig.7, rows
and columns are indexed as OriginClassId-AttackType-ConceptId. Here, the
ResNet50: l4.2 any taxi 0.0 0.2 0.4 0.6 0.8 1.0
717_C0
717_B0
717_P0
569_C0
555_C2
555_B0
555_P0
950_B0
950_P0
954_B0
954_P0
339_B2
339_P2
340_B0
340_P2
569_B1
569_P0
954_C1
340_C0
950_C1
339_C1
340_C2
340_B2
340_P0
954_B2
954_P2
950_B2
950_P2
339_B0
339_P0
569_B0
717_B1
717_P1
555_C1
569_P2
555_B1
555_P1
717_C1
954_C2
569_C2
555_C0
555_B2
555_P2
717_C2
717_B2
717_P2
954_C0
954_B1
954_P1
950_C0
950_B1
950_P1
569_P1
569_B2
569_C1
950_C2
340_B1
340_P1
339_C0
339_B1
339_P1
339_C2
340_C1
Fig.9: Similarity of all concepts discovered with NMF in adversarial per-
turbations aiming “taxi” class in layer4.2 of ResNet50. Concepts are
denoted as OriginClassId-AttackType-ConceptId pairs (Attack types:
B=BIM, P=PGD, C=C&W).
0C_717 0B_717 0P_717 0C_965 2C_555 0B_555 0P_555 0B_059 0P_059 0B_459 0P_459 2B_933 2P_933 0B_043 2P_043 1B_965 0P_965 1C_459 0C_043 1C_059 1C_933 2C_043 2B_043 0P_043 2B_459 2P_459 2B_059 2P_059 0B_933 0P_933 0B_965 1B_717 1P_717 1C_555 2P_965 1B_555 1P_555 1C_717 2C_459 2C_965 0C_555 2B_555 2P_555 2C_717 2B_717 2P_717 0C_459 1B_459 1P_459 0C_059 1B_059 1P_059 1P_965 2B_965 1C_965 2C_059 1B_043 1P_043 0C_933 1B_933 1P_933 2C_933 1C_04320 G. Mikriukov et al.
additional OriginClassId represents the ImageNet class ID of the AA origin.
From such results of target-specific comparison of concept vectors, we observe
two large groups of similar (co-directed) concepts (approximately 20×20 pixels
each)andseveralsmallergroups(around3×3pixels).Thelargegroupsinclude
concept vectors of attacks originating from all tested supercategories (vehicle,
animal, and fruit) and targeting taxi class. Similar results were observed for
different attack targets and models: at least one large cluster of concept vectors
was observed.
⋆ Summary:Our results imply that adversarial attacks comprise concept vec-
tors of target-specific direction (albeit with varying magnitudes depending on
the AA perturbation strength). In particular, AAs can be characterized by di-
rections in feature space that are mostly agnostic to attack technique and attack
origin class. This knowledge can be further leveraged in the explainable design
of adversarial attacks and defenses against them.
6 Conclusion and Outlook
This work for the first time conducted an in-depth analysis of the influence of
AAs on the concepts learned by CNNs using concept-based XAI techniques.
Through experiments our results revealed that AAs induce substantial alter-
ations in the concept composition within the feature space, introducing new
concepts or modifying existing ones. Remarkably, we demonstrated that the ad-
versarialperturbationitselfcanbedecomposedintoasetofconcepts,asubsetof
whichissufficienttoreproducetheattack’ssuccess.Furthermore,wediscovered
that different AAs learn similar concept vectors, and that these vectors are only
specific to the attack target class irrespective of the origin class.
ThesefindingsprovidevaluableinsightsintothenatureofAAsandtheirim-
pactonlearnedrepresentations,pavingthewayforthedevelopmentofmorero-
bustandinterpretabledeeplearningmodels,aswellaseffectivedefensesagainst
adversarial threats.
Limitations: Our experiments focused on pairwise training scenarios and tar-
getedwhite-boxattacks.Extendingtheanalysistonon-targetedattacks,further
black-box attacks, and universal attacks as well as other model types is needed
in future to broaden our understanding of AAs’ impact on learned concepts.
Future Work on Countering Attacks: It will be interesting to see whether
the concept-level patterns and alterations induced by AAs are useful for (real-
time)detectionofadversaries.Additionally,ourfindingsonhowtogloballyrep-
resent AAs as linear shifts in latent space could inform the design of adversarial
elimination techniques that aim to remove or mitigate the impact of adversarial
concepts within the learned representations.
Another intriguing direction is the explainable design of AAs themselves.
By leveraging our understanding of the target-specific nature of adversarialThe Anatomy of Adversarial Attacks: Concept-based XAI Dissection 21
concepts, novel attack strategies that optimize for specific concept directions
could be developed, potentially leading to more effective and robust adversarial
examples—whichagaincanserveinadversarialtrainingforcreatingsuper-robust
models.
A potential next iteration of our approach is the exploration of non-linear
AAs that manipulate the interactions between concepts rather than solely tar-
geting individual concept directions. Such attacks could potentially circumvent
existing defenses and uncover important challenges to the robustness of deep
learning models.
In conclusion, our study has demonstrated the value of leveraging explain-
able AI techniques to gain insights into the impact of AAs on learned represen-
tations and concepts within deep neural networks. By bridging the gap between
adversarial robustness and CNN latent space interpretability, we hope to have
paved the way towards more reliable and trustworthy AI systems capable of
withstanding adversarial threats whilst providing transparent and interpretable
decision-making processes.
References
1. Akhtar, N., Mian, A., Kardan, N., Shah, M.: Advances in adversarial attacks and
defenses in computer vision: A survey. IEEE Access 9, 155161–155196 (2021)
2. Athalye, A., Carlini, N., Wagner, D.: Obfuscated gradients give a false sense of
security:Circumventingdefensestoadversarialexamples.In:Internationalconfer-
ence on machine learning. pp. 274–283. PMLR (2018)
3. Bai, T., Luo, J., Zhao, J., Wen, B., Wang, Q.: Recent Advances in Adversarial
TrainingforAdversarialRobustness.In:Twenty-NinthInternationalJointConfer-
ence on Artificial Intelligence. vol. 5, pp. 4312–4321 (2021)
4. Bau,D.,Zhou,B.,Khosla,A.,Oliva,A.,Torralba,A.:Networkdissection:Quanti-
fyinginterpretabilityofdeepvisualrepresentations.In:Proc.IEEEconf.computer
vision and pattern recognition. pp. 6541–6549 (2017)
5. Brown, D., Kvinge, H.: Making corgis important for honeycomb classification:
Adversarial attacks on concept-based explainability tools. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.620–627
(2023)
6. Brown, T.B., Mané, D., Roy, A., Abadi, M., Gilmer, J.: Adversarial patch. arXiv
preprint arXiv:1712.09665 (2017)
7. Carlini,N.,Wagner,D.:AdversarialExamplesAreNotEasilyDetected:Bypassing
Ten Detection Methods. In: Proceedings of the 10th ACM Workshop on Artificial
Intelligence and Security. pp. 3–14. ACM (Nov 2017)
8. Carlini,N.,Wagner,D.:Towardsevaluatingtherobustnessofneuralnetworks.In:
2017 ieee symposium on security and privacy (sp). pp. 39–57. Ieee (2017)
9. Chen,J.,Yu,T.,Wu,C.,Zheng,H.,Zhao,W.,Pang,L.,Li,H.:Adversarialattack
detectionbasedonexamplesemanticsandmodelactivationfeatures.In:20225th
International Conference on Data Science and Information Technology (DSIT).
pp. 1–6. IEEE (2022)
10. Chyung, C., Tsang, M., Liu, Y.: Extracting interpretable concept-based decision
trees from CNNs. In: Proc. 2019 ICML Workshop Human in the Loop Learning.
vol. 1906.04664. CoRR (Jun 2019)22 G. Mikriukov et al.
11. Crabbé,J.,vanderSchaar,M.:ConceptActivationRegions:AGeneralizedFrame-
workForConcept-BasedExplanations.AdvancesinNeuralInformationProcessing
Systems 35, 2590–2607 (Dec 2022)
12. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-
scalehierarchicalimagedatabase.In:2009IEEEonf.computervisionandpattern
recognition. pp. 248–255. Ieee (2009)
13. Dombrowski, A.K., Alber, M., Anders, C., Ackermann, M., Müller, K.R., Kessel,
P.: Explanations can be manipulated and geometry is to blame. In: Advances in
Neural Information Processing Systems. vol. 32. Curran Associates, Inc. (2019)
14. Esser,P.,Rombach,R.,Ommer,B.:Adisentanglinginvertibleinterpretationnet-
work for explaining latent representations. In: Proc. 2020 IEEE Conf. Comput.
Vision and Pattern Recognition. pp. 9220–9229. IEEE (Jun 2020)
15. Eykholt,K.,Evtimov,I.,Fernandes,E.,Li,B.,Rahmati,A.,Xiao,C.,Prakash,A.,
Kohno, T., Song, D.: Robust physical-world attacks on deep learning visual clas-
sification. In: Proc. 2018 IEEE Conf. Computer Vision and Pattern Recognition.
pp. 1625–1634. IEEE Computer Society (2018)
16. Fel, T., Picard, A., Bethune, L., Boissin, T., Vigouroux, D., Colin, J., Cadène,
R., Serre, T.: Craft: Concept recursive activation factorization for explainability.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 2711–2721 (2023)
17. Fidel,G.,Bitton,R.,Shabtai,A.:Whenexplainabilitymeetsadversariallearning:
Detecting adversarial examples using shap signatures. In: 2020 international joint
conference on neural networks (IJCNN). pp. 1–8. IEEE (2020)
18. Fong, R., Vedaldi, A.: Net2vec: Quantifying and explaining how concepts are en-
codedbyfiltersindeepneuralnetworks.In:Proc.IEEEconf.computervisionand
pattern recognition. pp. 8730–8738 (2018)
19. Garcia,W.,Choi,J.I.,Adari,S.K.,Jha,S.,Butler,K.R.:Explainableblack-boxat-
tacksagainstmodel-basedauthentication.arXivpreprintarXiv:1810.00024(2018)
20. Ge,Y.,Xiao,Y.,Xu,Z.,Zheng,M.,Karanam,S.,Chen,T.,Itti,L.,Wu,Z.:Apeek
intothereasoningofneuralnetworks:Interpretingwithstructuralvisualconcepts.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 2195–2204 (2021)
21. Ghorbani, A., Abid, A., Zou, J.: Interpretation of neural networks is fragile. Pro-
ceedingsoftheAAAIConferenceonArtificialIntelligence33(01),3681–3688(Jul
2019)
22. Ghorbani, A., Wexler, J., Zou, J.Y., Kim, B.: Towards automatic concept-based
explanations. Advances in Neural Information Processing Systems 32 (2019)
23. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial
examples. In: 3rd Int. Conf. Learning Representations, Conf. Track Proc. (May
2015)
24. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: Proc. IEEE onf. computer vision and pattern recognition. pp. 770–778 (2016)
25. Hohman, F., Park, H., Robinson, C., Polo Chau, D.H.: Summit: Scaling Deep
Learning Interpretability by Visualizing Activation and Attribution Summariza-
tions. IEEE Transactions on Visualization and Computer Graphics 26(1), 1096–
1106 (Jan 2020)
26. Howard,A.,Sandler,M.,Chu,G.,Chen,L.C.,Chen,B.,Tan,M.,Wang,W.,Zhu,
Y., Pang, R., Vasudevan, V., et al.: Searching for mobilenetv3. In: Proceedings of
theIEEE/CVFinternationalconferenceoncomputervision.pp.1314–1324(2019)The Anatomy of Adversarial Attacks: Concept-based XAI Dissection 23
27. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.:
Squeezenet:Alexnet-levelaccuracywith50xfewerparametersand<0.5mbmodel
size. arXiv preprint arXiv:1602.07360 (2016)
28. Ilyas,A.,Santurkar,S.,Tsipras,D.,Engstrom,L.,Tran,B.,Madry,A.:Adversarial
examplesarenotbugs,theyarefeatures.Advancesinneuralinformationprocessing
systems 32 (2019)
29. Kao,C.Y.,Chen,J.,Markert,K.,Böttinger,K.:Rectifyingadversarialinputsusing
xaitechniques.In:202230thEuropeanSignalProcessingConference(EUSIPCO).
pp. 573–577. IEEE (2022)
30. Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., et al.: Inter-
pretabilitybeyondfeatureattribution:Quantitativetestingwithconceptactivation
vectors (tcav). In: Int. conf. machine learning. pp. 2668–2677. PMLR (2018)
31. Kim,H.:Torchattacks:Apytorchrepositoryforadversarialattacks.arXivpreprint
arXiv:2010.01950 (2020)
32. Kurakin, A., Goodfellow, I.J., Bengio, S.: Adversarial examples in the physical
world. In: Artificial intelligence safety and security, pp. 99–112. Chapman and
Hall/CRC (2018)
33. Madaan, D., Shin, J., Hwang, S.J.: Adversarial Neural Pruning with Latent Vul-
nerability Suppression. In: Proceedings of the 37th International Conference on
Machine Learning. pp. 6575–6585. PMLR (Nov 2020)
34. Madry,A.,Makelov,A.,Schmidt,L.,Tsipras,D.,Vladu,A.:Towardsdeeplearning
models resistant to adversarial attacks. In: Proc. 6th Int. Conf. Learning Repre-
sentations. OpenReview.net (2018)
35. Mikriukov,G.,Schwalbe,G.,Hellert,C.,Bade,K.:EvaluatingtheStabilityofSe-
mantic Concept Representations in CNNs for Robust Explainability. In: Explain-
able Artificial Intelligence. pp. 499–524. Communications in Computer and Infor-
mation Science, Springer Nature Switzerland (2023)
36. Mikriukov, G., Schwalbe, G., Hellert,C., Bade, K.: Gcpv: Guided concept projec-
tion vectors for the explainable inspection of cnn feature spaces. arXiv preprint
arXiv:2311.14435 (2023)
37. Mikriukov, G., Schwalbe, G., Hellert, C., Bade, K.: Revealing similar semantics
insideCNNs:Aninterpretableconcept-basedcomparisonoffeaturespaces.In:Ma-
chineLearningandPrinciplesandPracticeofKnowledgeDiscoveryinDatabases.
Springer (Apr 2023)
38. Olah, C., Mordvintsev, A., Schubert, L.: Feature visualization. Distill 2(11), e7
(2017)
39. Pintor,M.,Angioni,D.,Sotgiu,A.,Demetrio,L.,Demontis,A.,Biggio,B.,Roli,F.:
Imagenet-patch: A dataset for benchmarking machine learning robustness against
adversarial patches. Pattern Recognition 134, 109064 (2023)
40. Posada-Moreno, A.F., Surya, N., Trimpe, S.: ECLAD: extracting concepts with
local aggregated descriptors. Pattern Recognit. 147, 110146 (2024). https://doi.
org/10.1016/J.PATCOG.2023.110146
41. Rabold, J., Siebers, M., Schmid, U.: Explaining black-box classifiers with ILP –
empoweringLIMEwithAlephtoapproximatenon-lineardecisionswithrelational
rules. In: Proc. Int. Conf. Inductive Logic Programming. pp. 105–117. Lecture
Notes in Computer Science, Springer International Publishing (2018)
42. Rieger,L.,Hansen,L.K.:Asimpledefenseagainstadversarialattacksonheatmap
explanations. arXiv preprint arXiv:2007.06381 (2020)
43. Rudin, C.: Stop explaining black box machine learning models for high stakes
decisions and use interpretable models instead. Nature Machine Intelligence 1(5),
206–215 (May 2019)24 G. Mikriukov et al.
44. Samek, W., Müller, K.R.: Towards explainable artificial intelligence. In: Explain-
ableAI:Interpreting,ExplainingandVisualizingDeepLearning.LectureNotesin
Computer Science, vol. 11700, pp. 5–22. Springer (2019)
45. Schwalbe,G.:ConceptEmbeddingAnalysis:AReview.arXiv:2203.13909[cs,stat]
(Mar 2022)
46. Schwalbe, G., Finzel, B.: A comprehensive taxonomy for explainable artificial in-
telligence: A systematic survey of surveys on methods and concepts. Data Mining
and Knowledge Discovery (Jan 2023)
47. Schwalbe, G., Schels, M.: Concept enforcement and modularization as methods
for the iso 26262 safety argumentation of neural networks. In: Proceeding of the
10thEuropeanCongressonEmbeddedRealTimeSoftwareandSystems.pp.1–10
(2020)
48. Schwalbe,G.,Wirth,C.,Schmid,U.:Conceptembeddingsforfuzzylogicverifica-
tionofdeepneuralnetworksinperceptiontasks.arXivpreprintarXiv:2201.00572
(2022)
49. Serrurier, M., Mamalet, F., Fel, T., Béthune, L., Boissin, T.: When adver-
sarial attacks become interpretable counterfactual explanations. arXiv preprint
arXiv:2206.06854 (2022)
50. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. In: Proc. 3rd Int. Conf. Learning Representations (2015)
51. Szegedy,C.,Zaremba,W.,Sutskever,I.,Bruna,J.,Erhan,D.,Goodfellow,I.,Fer-
gus,R.:Intriguingpropertiesofneuralnetworks.In:Proc.2ndInt.Conf.Learning
Representations (2014)
52. Varghese,S.,Gujamagadi,S.,Klingner,M.,Kapoor,N.,Bär,A.,Schneider,J.D.,
Maag,K.,Schlicht,P.,Hüger,F.,Fingscheidt,T.:Anunsupervisedtemporalcon-
sistency(TC)losstoimprovetheperformanceofsemanticsegmentationnetworks.
In: 2021 IEEE/CVF Conf. Comput. Vision and Pattern Recognition Workshops.
pp. 12–20 (2021)
53. Zhang, R., Madumal, P., Miller, T., Ehinger, K.A., Rubinstein, B.I.: Invertible
concept-based explanations for cnn models with non-negative concept activation
vectors. In: Proc. AAAI Conf. Artificial Intelligence. pp. 11682–11690 (2021)
54. Zhao, Z.Q., Zheng, P., Xu, S.t., Wu, X.: Object detection with deep learning: A
review.IEEEtransactionsonneuralnetworksandlearningsystems30(11),3212–
3232 (2019)