Cluster-Based Normalization Layer for Neural Networks
Bilal FAYE1, Hanane AZZAG2, Mustapha Lebbah3
e-mail: faye@lipn.univ-paris13.fr, azzag@univ-paris13.fr, mustapha.lebbah@uvsq.fr
Abstract—Deep learning faces significant challenges training, the equal variance property may become disrupted.
during the training of neural networks, including internal Fromthisperspective,thenormalizationofactivationsduring
covariate shift, label shift, vanishing/exploding gradients,
training emerges as a pivotal strategy to maintain the
overfitting, and computational complexity. While conventional
advantages associated with normalizing inputs. By ensuring
normalization methods, such as Batch Normalization,
aim to tackle some of these issues, they often depend that activations maintain a consistent statistical distribution
on assumptions that constrain their adaptability. Mixture across layers, deep neural networks can benefit from stable
Normalization faces computational hurdles in its pursuit and efficient training, ultimately leading to improved model
of handling multiple Gaussian distributions. This paper
performance.
introduces Cluster-Based Normalization (CB-Norm) in
Batch Normalization (BN), as introduced by Ioffe and
two variants—Supervised Cluster-Based Normalization
(SCB-Norm) and Unsupervised Cluster-Based Normalization Szegedy in their seminal work [4], stands out as the
(UCB-Norm)—introducing a groundbreaking one-step prevailing and widely adopted method for the normalization
normalization approach. CB-Norm leverages a Gaussian of activations. BN achieves this by standardizing activations
mixture model to specifically address challenges related to
using batch statistics, which facilitates the utilization of
gradient stability and learning acceleration. For SCB-Norm, a
higher learning rates. Nevertheless, BN exhibits limitations
supervised variant, the novel mechanism involves introducing
predefined data partitioning, termed clusters, to normalize tiedtobatchsizedependenceandtheassumptionofuniform
activations based on the assigned cluster. This cluster-driven data distribution. To mitigate these limitations, specialized
approach creates a space that conforms to a Gaussian mixture variants of BN have been developed [5], focusing on
model. On the other hand, UCB-Norm, an unsupervised
addressing issues related to batch size. On a different
counterpart, dynamically clusters neuron activations during
front, Mixture Normalization (MN) [6] offers an alternative
training, adapting to task-specific challenges without relying
on predefined data partitions (clusters). This dual approach approach, aiming to overcome the constraints imposed
ensures flexibility in addressing diverse learning scenarios. by data distribution assumptions. In stark contrast to
CB-Norminnovativelyusesaone-stepnormalizationapproach, BN, MN accommodates data samples drawn from diverse
where parameters of each mixture component (cluster in
distributions.ItemploystheExpectation-Maximization(EM)
activation space) serve as weights for deep neural networks.
algorithm [7] for data clustering based on components,
This adaptive clustering process tackles both clustering and
resolution of deep neural network tasks concurrently during allowing for the normalization of each sample using
training, signifying a notable advancement in the field. component-specific statistics. MN’s distinctive approach
enhances convergence, particularly in convolutional neural
I. INTRODUCTION
networks,whencomparedtoBN.However,theincorporation
Normalization in the context of deep learning is of the EMalgorithm in mixture models mayentail a notable
a fundamental data transformation process aimed at increaseincomputationaltime,potentiallyhamperingoverall
conferring specific statistical properties upon the data. efficiency. This stems from the necessity of estimating
Within thedomain of data normalization,various techniques parameters across the entire dataset prior to neural network
are employed, such as centering, scaling, standardizing, training,ensuringtheprovisionofnormalizationparameters.
decorrelating, and whitening [1]. In deep neural networks To address this issues, we propose a new normalization
(DNNs), input normalization plays a crucial role during method titled Cluster-Based Normalization (CB-Norm).
training by eliminating the disparities in feature magnitudes. CB-Normservesasanormalizationlayerwithindeepneural
This normalization process facilitates the rapid convergence network architectures. Operating on the hypothesis that
of linear model optimization [2]. However, in complex activations can be represented as a Gaussian mixture model,
layered neural networks, where the input is directly CB-Norm normalizes these activations during deep neural
linked only to the initial weight matrix, it becomes network training to estimate parameters for each mixture
less transparent how the input influences the optimization component. These parameters are treated as learnable
landscape in relation to other weight matrices. Typically, parameters and are updated as neural network weights
these initial weights are not normalized, and this omission during backpropagation, aligning with the target task. To
cansignificantlyimpactthegradientoptimizationprocedure. operationalize this strategy, we suggest two approaches: a
Toaddressthisissue,variousweight-initializationtechniques supervisedversion(SCB-Norm)andanunsupervisedversion
have been introduced, aiming to equalize variances in (UCB-Norm).
layer input/output gradients across different layers [3]. SCB-Norm operates in two steps: first, creating clusters
Nevertheless, as weight matrices are updated throughout to group samples with similar characteristics, and then
4202
raM
52
]GL.sc[
1v89761.3042:viXranormalizing activations during training based on these ”mode collapse” in GANs, improving their learning
clusters (activations from samples within the same cluster from a diverse set of modes.
are normalized with the same parameters). Building clusters
II. RELATEDWORK
involves expert knowledge for effective data grouping, or
it can be derived from partitioning using another clustering A. Batch Normalization
algorithm. For example, in a dataset with classes and To ensure clear understanding and consistency in our
superclasses (groupings of classes), each superclass can be model, and to facilitate comparison with existing work,
treated as a distinct cluster. In domain adaptation, each we adopt the same notations as those used in [6]. In a
specific domain within the dataset can be considered an convolutional neural network layer, let x ∈ RN×C×H×W
independent cluster. represent the neuron’s activation, where N, C, H, and
UCB-Norm is a one-step method where the normalization W denote batch, channel, height, and width axes. Batch
of activations during neural network training is not normalization (BN) [4] standardizes x with m samples
conditioned on prior knowledge (clusters). The mixture mini-batch B = {x : m ∈ [1,N] × [1,H] × [1,W]},
1:m
components in the activation space are constructed by flattening x except the channel axis, as follows:
considering the contribution of all parameters of each
x −µ
mixture component for the normalization of activations. xˆ = i B , (1)
i (cid:112)
σ2 +ϵ
It’sworthnotingthatthechoiceofaGaussiandistribution B
assumption for clusters is made to ensure a coherent basis where µ = 1 (cid:80)m x and σ2 = 1 (cid:80)m (x − µ )2
B m i=1 i B m i=1 i B
for comparison. However, it is important to emphasize that represent the mean and variance, while ϵ > 0 serves as
alternative hypotheses for data distribution could also be a small value to mitigate numerical instability. To mitigate
considered in different scenarios and experiments. As an the constraints imposed by standardization, BN introduces
integral layer within the deep neural network, CB-Norm learnable parameters γ and β to remove the linear regime of
plays a pivotal role in standardizing activations originating nonlinearity in certain activations:
fromthesameclusterusingtheparametersacquiredthrough
backpropagation. This process facilitates the estimation of x˜ i =γxˆ i+β
parameters for each cluster, ultimately enhancing the data
representation’s discriminative capacity with respect to the
When the samples in the mini-batch originate from the
target task.
samedistribution,thetransformationdescribedinEquation1
This study presents four noteworthy contributions:
yields a distribution with a mean of zero and a variance of
one.Thisconstraintofzeromeanandunitvarianceservesto
• Cluster-Based Normalization (CB-Norm) stabilize the activation distribution, ultimately aiding in the
Advancements: CB-Norm introduces an approach
training process.
dynamically learning mixture component parameters
While BN demonstrates effective performance, it exhibits
during neural network training. Notably, CB-Norm
two notable limitations: its reliance on batch size, leading
excels in gradient stability, learning acceleration, and
to suboptimal performance with small batches, and the
adaptability across diverse learning scenarios.
simplistic assumption that sample within the mini-batch are
• Two CB-Norm Applications for Enhanced from the same distribution, a scenario rarely encountered
Normalization: We present two CB-Norm methods -
in real-world data. In response to these challenges, several
Supervised Cluster-Based Normalization (SCB-Norm),
alternative approaches have been introduced.
utilizing predefined data partitioning based (clusters),
and Unsupervised Cluster-Based Normalization B. Extensions of Batch Normalization
(UCB-Norm), which dynamically clusters neuron
Several extensionsto batchnormalization (BN)have been
activations during training for increased flexibility.
introduced,includingLayerNormalization(LN)[8],Instance
• Versatile Application in Deep Learning Normalization(IN)[9],GroupNormalization(GN)[10],and
Architectures: CB-Norm exhibits versatility across
Mixture Normalization (MN) [6]. These variants all employ
various deep neural network architectures, including the common transformation x → xˆ, which is applied to the
Transformers and Convolutional Neural Networks. It flattened x along the spatial dimension L = H × W, as
serves as a layer at different depths, expediting training
follows:
processes and improving generalization performance
v
consistently. v i =x i−E Bi(x), xˆ i = (cid:112)E (vi 2)+ϵ, (2)
• Integration Recommendations for Improved Bi
Performance: We recommend integrating CB-Norm where B = {j : j ∈ [1,N],j ∈ [i ],j ∈ [1,L]},
i N C C L
intodomainssuchasdomainadaptationandGenerative and i = (i ,i ,i ) a vector indexing the activations
N C L
Adversarial Networks (GANs). CB-Norm addresses x∈RN×C×L.
challenges in domain adaptation effectively, enhancing
model performance across diverse data distributions. Layer Normalization (LN): Alleviates inter-dependency
Moreover,CB-Normcontributestoresolvingissueslike among batch-normalized activations by computing meanand variance using input from individual layer neurons. LN is the kth Gaussian in the mixture model p(x), µ the mean
k
proves beneficial for recurrent networks but can encounter vector and Σ is the covariance matrix.
k
difficultieswithconvolutionallayersduetospatialvariations The probability that x has been generated by the kth
in visual data. It can be formulated as Equation 2 when Gaussian component in the mixture model can be defined
B ={j :j ∈[i ],j ∈[1,C],j ∈[1,L]}. as:
i N N C L
λ p(x|k)
τ (x)=p(k|x)= k , (5)
Instance Normalization: A normalization approach
k (cid:80)K
λ p(x|j)
j=1 j
that normalizes each sample individually, primarily
Based on these assumptions and the general transform in
aimed at eliminating style information, particularly
Equation(2),theMixtureNormalizingTransformforagiven
in images. IN enhances the performance of specific
x is defined as
deep neural networks (DNNs) by computing mean i
values and standard deviations in the spatial domain. It K
finds extensive use in applications such as image style xˆ i
=(cid:88)τ √k( λx i)
xˆk i, (6)
transfer [11] [12]. It can be formulated as Equation 2 when k=1 k
B ={j :j ∈[i ],j ∈[i ],j ∈[1,L]}.
i N N C C L given
Group Normalization (GN): A normalization method vk =x −E [τˆ (x).x], xˆk = v ik ,
that partitions neurons into groups and autonomously i i Bi k i (cid:112)E [τˆ (x).(vk)2]+ϵ
Bi k
standardizes layer inputs for each sample within these (7)
groups. GN performs exceptionally well in visual tasks where
τ (x )
characterized by small batch sizes, such as object detection τˆ (x )= k i ,
k i (cid:80) τ (x )
and segmentation. GN partitions the channels into several j∈Bi k j
groups (G), calculating statistics along the L axis, but
is the normalized contribution of x with respect to the
i
restricting this computation to subgroups of the channels.
mini-batch B = {j : j ∈ [i ],j ∈ [i ],j ∈ [1,L]} in
i N N C C L
When G=C, GN is equivalent to IN, and it is identical to theestimationofthestatisticalmeasuresofthekth Gaussian
LN when G=1.
component.
With this approach, Mixture Normalization can be
Mixture Normalization (MN): In the realm of deep
implemented in two stages:
neural networks (DNNs), the distribution of activations
• Estimation of the mixture model’s parameters
inherently encompasses multiple modes of variation due
θ = {λ ,µ ,Σ : k = 1,...,K} using the
to the presence of non-linearities. The batch normalization k k k
Expectation-Maximization (EM) algorithm [7].
(BN) [4] assumption that a Gaussian distribution suffices
• Normalization of each sample based on the estimated
to model the generative process of mini-batch samples
parameters (see Equation (7)) and aggregation using
becomeslessapplicable.Inresponse,MixtureNormalization
posterior probabilities (see Equation (6)).
(MN) [6] approaches BN from the perspective of Fisher
kernels, originating from generative probability models. In the context of convolutional neural networks, this
Instead of computing mean and standard deviation across method enables Mixture Normalization to outperform batch
entireinstanceswithinamini-batch,MNemploysaGaussian normalization in terms of convergence and accuracy in
Mixture Model (GMM) to assign each instance in the supervised learning tasks.
mini-batchtoacomponentandsubsequentlynormalizeswith
III. PROPOSEDMETHOD:CLUSTER-BASED
regard to multiple means and standard deviations associated
NORMALIZATION
with different modes of variation in the underlying data
distribution. Our Cluster-Based Normalization (CB-Norm) method
MN normalizes each sample within the mini-batch by introduces a one-step normalization method, simultaneously
utilizing the mean and standard deviation corresponding to clustering neuron activations and addressing deep neural
themixturecomponenttowhichthesampleisassigned.The network tasks. By leveraging a latent space assumption
probabilitydensityfunctionp thatcharacterizesthedatacan with neuron activations following a Gaussian mixture
θ
be represented as a parameterized Gaussian Mixture Model model, AN learns parameters for each mixture component
(GMM). Let x∈RD, and θ ={λ ,µ ,Σ :k =1,...,K}, during neural network training. These learned parameters
k k k
serve as weights, contributing to adaptive components
K K
(cid:88) (cid:88) tailored to the target task, thereby enhancing gradient
p(x)= λ p(x|k), s.t. ∀ : λ ≥0, λ =1, (3)
k k k k
stability, accelerating convergence, and improving overall
k=1 k=1
neural network performance. The implementations of
where
CB-Norm, namely Supervised Cluster-Based Normalization
1 (cid:18) (x−µ )TΣ−1(x−µ )(cid:19) (SCB-Norm)andUnsupervisedCluster-BasedNormalization
p(x|k)= exp − k k k ,
(2π)D/2|Σ |1/2 2 (UCB-Norm), provide specific strategies for incorporating
k
(4) cluster information into the normalization process.A. Supervised Cluster-Based Normalization (SCB-Norm) ℓ through this transformation. Moreover, it is imperative
InSCB-Norm,weleveragepriorknowledgecalledcluster.
A cluster is simply a grouping of data that shares similar Algorithm 1: Training an Supervised
characteristics. Creating clusters requires either specialized Adaptive-Normalized Network
expertise to effectively group data or can be accomplished Input : Deep neural network Net with trainable
throughpartitioningusinganalternativeclusteringalgorithm. parameters Θ; subset of activations and its
Forinstance,inadatasetcontainingclassesandsuperclasses clusters {x ,k }m , with k ∈{1,...,K},
i i i=1 i
(collections of classes), each superclass can be regarded where K is the number of clusters
as a separate cluster. In the context of domain adaptation, Output: Supervised Cluster-based Normalized deep
each distinct domain within the dataset can be treated as neural network for inference,
an independent cluster. SCB-Norm uses these clusters to Netinf
SCB−Norm
condition the construction of the Gaussian Mixture Model 1 Nett Sr CB−Norm =Net // Training SCB-Norm deep
(GMM). This implies that activations from samples within neural network
the same cluster undergo normalization using identical 2 for i←1 to m do
p na er twam ore kter as n, dw ah reich upa dr ae tel dea dr un re id ngas bap ca kr pt roo pf at gh ae tiod nee bp asn ee dur oa nl • NAd ed ttrtransformati (o En quxˆ ai ti= onS 8C )B−Norm θki(x i) to
SCB−Norm
thetargettask.Onthelatentspaceofactivations,clustersare • Modify each layer in Nett Sr CB−Norm with input x i to
expected to follow a GMM. SCB-Norm essentially tailors take xˆ instead
i
the normalization process based on predefined clusters, 3 end
enhancing adaptability to specific features in the data. 4 Train Nett Sr CB−Norm to optimize the parameters:
Let x denote an activation tensor in the RN×C×H×W Θ=Θ∪{µ ,σ }K
k k k=1
space. Here, B i = {j : j N ∈ [i N],j C ∈ [i C],j L ∈ 5 Neti Sn Cf B−Norm =Nett Sr CB−Norm // Inference
[1,L]} signifies a set of activations obtained by flattening SCB-Norm network with frozen parameters
x along the L = H × W axis. SCB-Norm functions by 6 for i←1 to m do
standardizing each activation x i in B i, derived from the • Retrieve the parameters associated with the cluster of
transformationofasampleassociatedwithaspecificmixture x : θ
i ki
componentk.Thenormalizationiscarriedoutusingmixture • transform xˆ i = SCB−Norm θki(x i) using Equation 8
component-specific parameters θ ki ={µ ki,σ ki,λ ki}, where 7 end
λ representsaconstant,assumingthatmixturecomponents
ki
are equiprobable. This assumption implies that each mixture
component contributes equally to the overall distribution, to compute gradients with respect to the SCB-Norm
simplifying the weighting scheme within the Gaussian transformation’s parameters. This gradient computation
Mixture Model (GMM): involves employing the chain rule, as demonstrated in the
forthcoming expression (prior to any simplifications):
x −µ
SCB−Norm (x ):xˆ ← i ki (8)
θki i i (cid:113)
σ2 +ϵ
∂ℓ
=
∂ℓ
.
∂xˆ
i =−
∂ℓ
.(σ2 +ϵ)−1/2
ki ∂µ ∂xˆ ∂µ ∂xˆ ki
ki i ki i
In the backpropagation process, the deep neural network
∂ℓ ∂ℓ ∂xˆ µ +x
dynamically acquires parameters θ = {µ ,σ }K as = . i = ki i
k k k=1 ∂σ2 ∂xˆ ∂σ2 2(σ2 +ϵ)3/2
weights of the neural network (see Figure 1), with K ki i ki ki
representing the number of specified mixture components. SCB-Norm serves as a differentiable operation within deep
Notably, λ , being a constant, remains unchanged. The neural networks, enabling activations normalization. The
k
continuous learning through backpropagation ensures core of SCB-Norm involves the construction of a GMM
adaptive updates that align the deep neural network in the latent space. This GMM is defined by parameters
representations with the nuances of the target task. This θ = {µ ,σ }K , which are learned during training. It
k k k=1
dynamic adaptation contributes to an iterative refinement continually adapts model representations to the target task,
of the model, allowing it to capture intricate patterns and enhancing overall performance. The normalization process
variations in the data. effectively mitigates variations in activation distributions,
allowing the model to emphasize relevant patterns and
Each normalized activation xˆ can be viewed as an features. By introducing the GMM in the latent space,
i
input to a sub-network composed of the linear transform SCB-Norm captures intricate relationships in the data. The
(Equation 8), followed by the other processing done by the differentiabilityofSCB-Normensuresefficientgradientflow
original network. The processing within this sub-network during training, aiding parameter updates while preserving
plays a crucial role in amplifying feature extraction, thereby differentiation through the normalization process. This
enriching the overall representational capacity of the model. adaptability in the latent space facilitates improved model
During the training process, as outlined in Algorithm 1, it is generalization and task-specific adaptation.
essential to backpropagate the gradient of the loss function In domain adaptation, where each domain acts as...
Fig. 1: Supervised Cluster-Based Normalization (SCB-Norm) applied on activation x . The mixture component identifier k determines
i i
the cluster to which x belongs, facilitating the selection of the respective trainable parameters θ ={µ ,σ } for normalization. The
i ki ki ki
normalizedactivation,ascomputedusingEquation8,servesasinputfortheDeepNeuralNetwork(DNN).Subsequenttolosscomputation,
the parameter θ undergo updates based on the target task.
ki
a distinct cluster, the normalization applied to each this iterative process, activations within a given cluster
cluster yields localized, task-specific representations. This are projected onto a shared space, following a Gaussian
approach positively impacts data representation for both distribution tailored to the specifics of the target task. The
source and target domains, aligning them with the target beauty of this approach lies in its adaptability, allowing the
task. SCB-Norm, as a differentiable operation in deep model to dynamically adjust its representations based on the
neural networks, underpins this adaptation by continuously characteristics of the data and the learning task at hand.
fine-tuning model representations for the specific task, Consequently, UCB-Norm’s technical methodology ensures
ultimately boosting overall performance. thatthenormalizationofactivationsiscontextuallyinformed,
leadingtoamorerefinedandtask-adaptiveGaussianmixture
B. Unsupervised Cluster-Based Normalization (UCB-Norm) model.
In UCB-Norm, we take a different approach by not The update of these mixture component parameters
relying on prior knowledge (clusters). Instead, the deep involves two approaches (see Algorithm 2): the first
neural network autonomously discovers a latent space over correspondstoanupdatealignedwiththetargettaskensuring
activationsthatadherestoaGMM.UCB-Normdynamically alignment with specific objectives. This targeted update
clustersneuronactivationsduringtrainingwithoutpredefined strategy enhances the model’s capability to specialize and
partitions. This allows the model to adapt to task-specific optimize its performance for the intended task, resulting
challenges without relying on prior cluster information. The in improved task-specific learning and generalization. The
absence of predefined partitions provides UCB-Norm with second approach consists of estimating parameters using the
increased flexibility, as it allows the deep neural network to moving averages method within each batch. This method
explore and adapt to patterns in the data on its own. offers several benefits, including enhanced stability and
To align with the representation SCB-Norm, UCB-Norm robustness in the learning process. By smoothing out
introduces a technical approach that revolves around fluctuations in parameter values across batches, the moving
estimating the parameters of a Gaussian mixture model averagesmethodcontributestoamoreconsistentandreliable
(GMM) during the normalization process. Let K denote training trajectory, mitigating the impact of noisy or outlier
the number of components in the mixture, and θ = batches. Additionally, this approach provides a form of
{λ ,µ ,σ }K . As part of the normalization procedure, regularization, preventing overfitting and promoting better
k k k k=1
given the absence of prior knowledge regarding the context generalization to unseen data. In summary, the dual strategy
of each sample, UCB-Norm employs a strategy involving of updating parameters through both task-aligned updates
the consideration of all mixture components for the and the moving averages method combines the benefits
normalization of x , utilizing Equation 6. This method is of task-specific adaptation and robust, stable learning,
i
designed to aggregate the normalization of x , ensuring contributingtotheoveralleffectivenessandversatilityofthe
i
that each mixture component contributes proportionally to training process.
the normalization process (see Figure 2). By doing so,
the mixture component to which x belongs exerts a more
i
substantial influence on the normalization of x . Through
i
ssap
drawroF
ssap
drawkcaB...
Learnable parmeters
Fig. 2: Unsupervised Cluster-Based Normalization (UCB-Norm) applied on activation x . The parameter K corresponds to the
i
number of mixture components to be estimated during the training process. In the forward pass, the mixture component parameters
θ={λ ,µ ,σ }K are utilized to normalize the activation x through Equation 6. The resulting normalized activation, serves as input
k k k k=1 i
fortheDeepNeuralNetwork(DNN).Afterlosscomputation,inthebackwardpass,theparametersθ undergoupdatesbasedonthetarget
task.
IV. EXPERIMENTS • Tiny ImageNet: A dataset that is a reduced version of
the ImageNet dataset, containing 200 classes with 500
In the upcoming experiments, we plan to evaluate how
training images and 50 test images per class [15].
well the Cluster-Based Normalization (CB-Norm) performs
and to conduct a comparison with Batch Normalization • MNIST digits: Contains 70,000 grayscale images of
size 28 × 28 representing the 10 digits, with around
(BN) and Mixture Normalization (MN).
6,000 training images and 1,000 testing images per
We suggest applying both implementations of CB-Norm:
class [16].
Supervised version (SCB-Norm) and Unsupervised
version (UCB-Norm)—to tasks involving classification • SVHN: A challenging dataset with over 600,000 digit
images, focusing on recognizing digits and numbers in
(Section IV-B, IV-C and IV-F), domain adaptation
natural scene images [17].
(Section IV-E), and generative neural networks
(Section IV-D). Table II provides a visual representation of the mapping
In addition to SCB-Norm, we will incorporate a variant between each dataset and the specific experiments in which
known as SCB-Norm-base. In SCB-Norm-base, two it is utilized.
separate neural networks estimate the K clusters (mixture
components)parametersθ ={µ ,σ }K .Foragiveninput B. A Comparative Study: Cluster-Based Normalization vs.
k k k=1 Batch Normalization and Mixture Normalization Using
x in the SCB-Norm-base layer, the cluster identifier k of
i i
Shallow Convolutional Neural Network
x is initially encoded using one-hot encoding and serves
i
as input for both networks. The first neural network outputs In this study, we employ a shallow convolutional neural
µ , while the second provides σ2 , which are then used to network (Shallow CNN) structure outlined in Table III. Five
ki ki
normalize x . models are constructed based on the Shallow CNN (see
i
Detailed descriptions of the methods under consideration Table III). The first model, denoted as BN, aligns with the
are provided in Table I. foundationalarchitectureoftheShallowCNN,incorporating
only Batch Normalization layers for normalization. The
A. Datasets
remaining four models—MN, SCB-Norm, SCB-Norm-base,
The experiments in this study make use of several andUCB-Norm—areobtainedbysubstitutingthethirdBatch
benchmark datasets that are widely recognized within the Normalization layer in the Shallow CNN with the respective
classification community: layersofMN,SCB-Norm,SCB-Norm-base,andUCB-Norm
• CIFAR-10: A dataset with 50,000 training images (see Table I). As per the findings in [6], we conduct
and 10,000 test images, each of size 32 × 32 pixels, experimentsemployingtwodistinctlearningrates,whereone
distributed across 10 classes [13]. is five times larger than the other.
• CIFAR-100: Derived from the Tiny Images dataset, The mini-batch size is fixed at 256, and all models
it consists of 50,000 training images and 10,000 test undergo training for 100 epochs using the AdamW
imagesofsize32×32,dividedinto100classesgrouped optimizer [19,20] with a weight decay of 1e − 4. In
into 20 superclasses [14]. the training phase on CIFAR-10, CIFAR-100, and Tiny
ssap
drawroF
ssap
drawkcaB(a)SCB-Norm
(b)UCB-Norm
Fig. 3: Visualization of Clustering Patterns in Activations: The t-SNE visualization of activations from models trained on CIFAR-10
with SCB-Norm and UCB-Norm normalization layers. The figure illustrates the formation of distinct mixture components aligning with
predicted target classes, showcasing improved clustering patterns over the course of training.
ImageNet datasets, we utilize the MN method, which learning rate from 0.001 to 0.005. The widening gap in
entails estimating a Gaussian mixture model through convergence with the elevated learning rate underscores
Maximum Likelihood Estimation (MLE). For comparative the effectiveness of our normalization technique in adeptly
analysis, the three mixture components identified by the harnessing higher learning rates during training.
Expectation-Maximization (EM) algorithm on MN serve as
separate clusters (prior knowledge) (K =3) for SCB-Norm During the training of the Shallow CNN on the
and SCB-Norm-base. In UCB-Norm, we define the number CIFAR-10 dataset, we consistently captured snapshots
of mixture components for estimating as K = 3. Our at 25-epoch intervals, integrating both SCB-Norm
main goal is not to attain state-of-the-art results, which and UCB-Norm techniques. Following this, the trained
typically demand computationally expensive architectures models were employed on a randomly chosen CIFAR-10
and meticulous parameter tuning. Rather, our focus is batch, and the resulting activations underwent t-SNE
on illustrating that the replacement or integration of our visualization. The visual representations revealed the
cluster-based normalization technique (SCB-Norm and formation of clusters closely resembling the target classes
UCB-Norm) can enhance the convergence rate, resulting predicted by the individual models. Notably, as training
in improved test accuracy. This underscores the substantial progressed, a noticeable refinement in these clusters was
impact of our approach in enhancing model performance. observed, contributing to a comprehensive improvement in
In Figure 4, we illustrate the activation distribution for performance, as depicted in Figure 3.
models employing CB-Norm (SCB-Norm, UCB-Norm) and
thosewithoutCB-Norm(BN,MN).ModelsusingCB-Norm To substantiate the hypothesis that CB-Norm can
exhibit a closer resemblance to the normal distribution significantly contribute to accelerating the convergence
and display probability densities with reduced skewness. of a neural network training, we will employ a Deep
Clearly, CB-Norm provides a more accurate approximation Convolutional Neural Network in the following experiment
of p(x), as shown by the solid green curve, exhibiting a using the CIFAR-100 dataset.
closer alignment with the underlying distribution compared
to models without CB-Norm. Figure 7 highlights the C. A Comparative Study: Cluster-Based Normalization vs.
swifter convergence of SCB-Norm, its compact variant BatchNormalizationandMixtureNormalizationUsingDeep
SCB-Norm-base, and UCB-Norm in comparison to BN Convolutional Neural Network
and MN. This accelerated convergence translates into
We have demonstrated that the incorporation of
enhanced performance on the validation dataset, with an
cluster-based normalization techniques (SCB-Norm,
average accuracy improvement of 2% on CIFAR-10, 3%
SCB-Norm-base, and UCB-Norm) results in enhanced
on CIFAR-100, and 4% on Tiny ImageNet, as detailed in
convergence speed and improved performance in models
Table IV. This positive trend holdsconsistent across varying
where batch normalization serves as the baseline layer
class numbers (10, 100, 200), even with an augmented
normalization. It’s worth noting that our experiments were(a)BN
(b)MN
(c)SCB-Norm
(d)UCB-Norm
Fig. 4: Usingarandomlyselectedmini-batchonCIFAR-100,wedepictthedistributionofactivationsrelatedtoasubsetof128channels
withinthe”conv2”layeroftheShallowCNNarchitecture(specifiedinTableIII).Thesolidgreencurverepresentstheprobabilitydensity
function, while dashed curves depict distinct mixture components, each distinguished by various colors.
conducted on a relatively shallow architecture comprising we create five models for each variant (DenseNet-40 and
only four convolutional layers. This raises a valid question DenseNet-100). The initial model reflects the foundational
about whether this observed behavior persists in more structure of DenseNet, featuring Batch Normalization layers
extensive and contemporary architectures. To investigate exclusively for normalization. The second model mirrors
this, we select DenseNet [21] as our preferred architecture. the fundamental DenseNet architecture but introduces
We leverage two foundational architectures featuring the MN layer as the initial layer. Models three through
40 and 100 layers, aligning with the structure outlined five correspondingly involve integrating SCB-Norm-base,
in the mixture normalization paper. These architectures SCB-Norm, and UCB-Norm layers as the initial layer in the
encompass three dense blocks and two transition layers, base DenseNet model. Placing these normalization layers
maintaining a consistent growth rate of 12. DenseNet is as the initial layer facilitates the direct infusion of cluster
characterized by its dense connectivity pattern, where each information for adaptive normalization directly into the
layer receives direct input from all preceding layers within image. In the case of constructing components with MN,
a dense block. This unique structure promotes feature reuse we set K = 5. Additionally, components derived from
and enhances model expressiveness. All models undergo the EM algorithm are regarded as prior clusters (K = 5)
200 epochs of training on the CIFAR-100 dataset, with for SCB-Norm-base and SCB-Norm. For the unsupervised
a batch size of 64 and utilizing Nesterov’s accelerated version of CB-Norm, we also adopt K = 5 for the number
gradient [22]. The learning rate initiates at 0.1 and is of mixture components.
reduced by a factor of 10 at 50% and 75% of the total Throughout the CIFAR-100 training regimen, we
training epochs. Additionally, weight decay and momentum systematically recorded snapshots at intervals of 25 epochs,
values are set to 10−4 and 0.9, respectively. To compare incorporating the SCB-Norm and UCB-Norm techniques.
the three normalization methods (batch normalization, Subsequently,thetrainedmodelswereappliedtoarandomly
mixture normalization, and cluster-based normalization), selected CIFAR-100 batch, and the resulting activations(a)CIFAR-10 (b)CIFAR-100 (c)TinyImageNet
Fig. 5: Learning rate = 0.001
(a)CIFAR-10 (b)CIFAR-100 (c)TinyImageNet
Fig. 6: Learning rate = 0.005
Fig. 7: Test error curves when Shallow CNN (detailed in Table III) architecture is trained under different learning rate. We observe
that onCIFAR-10, CIFAR-100and TinyImageNet, Cluster-BasedNormalization method(SCB-Norm, SCB-Norm-baseand UCB-Norm)
performs consistently in both small and large learning rate regimes.
(a) (b) (c) (d)
Fig. 8: DenseNet-40
(a) (b) (c) (d)
Fig. 9: DenseNet-100
Fig.10:ExperimentsonCIFAR-100usingDenseNet[21]areshowcasedinFigure8and9.Thetraining/testerrorandcross-entropylossare
depictedforDenseNetwith40layers(DenseNet-40)andDenseNetwith100layers(DenseNet-100).Notably,theincorporationofCB-Norm
(SCB-Norm-base, SCB-Norm, and UCB-Norm) significantly enhances the training process, expediting optimization. Concurrently, it
demonstrates superior generalization, as evidenced by a consistently substantial margin compared to its batch normalized and mixture
normalized counterparts in the error curve.
underwent t-SNE visualization. The visual representations a discernible refinement in these clusters was observed
exposed the emergence of clusters closely mirroring the with progressive training, contributing to a comprehensive
target classes predicted by the respective models. Notably, performance enhancement, as illustrated in Figure 11.Normalization Description
Algorithm 2: Training an Unsupervised
Layer
Cluster-Based Normalization Network BN Layer with batch normalizing transform (ref.
Input : Deep neural network Net with trainable SectionII-A)
parameters Θ; subset of activations {x }m ; MN Layer with mixture normalizing transform (ref.
i i=1 SectionII-B)
number of clusters K; momentum m; SCB-Norm Layer with supervised cluster-based
update∈{weight,moving average} normalizationtransform(ref.Algorithm1)
Output: Unsupervised Cluster-Based Normalized SCB-Norm-base A softened version of SCB-Norm involves
employing two distinct neural networks to
deep neural network for inference,
independently learn µ and σ for all mixture
Netinf
components. The cluster identifier one-hot
UCB−Norm
1 Nett Ur CB−Norm =Net // Training UCB-Norm deep e nn ec uo rad lin ng etwon ore kh so .t(k) serves as input for both
neural network
UCB-Norm Layer with unsupervised cluster-based
2 Initialize the parameters for each cluster as follows: normalization(ref.Algoirthm2)
(λ , µ , σ2) for k ∈{1,...,K}, subject to the
k k k TABLE I: Normalization method used in the experiments
conditions that (cid:80)K λ =1 and σ2 >0 for all
k=1 k k
k ∈{1,...,K}.
3 for i←1 to m do Dataset Task
• Add transformation xˆ i = UCB−Norm θ(x i) to CIFAR-10 and This datasets are used for training a shallow
Nettr (Equation 6): TinyImageNet convolutionalneuralnetwork(ref.SectionIV-B).
UCB−Norm CIFAR-100 Three experiments (ref. Sections IV-B, IV-C,
and IV-D) involve the use of this dataset. These
K
xˆ
←(cid:88)τ √k(x i)
xˆk
experiments encompass the training of shallow
i λ i and deep convolutional neural networks, as well
k=1 k asthetrainingofagenerativeneuralnetwork.
MNIST digits ThisdatasetsareusedtotraintheAdaMatch[18]
• Modify each layer in Nett Ur CB−Norm with input x i toandSVHN model in unsupervised domain adaptation (ref.
take xˆ instead SectionIV-E).
i
4 end TABLE II: Summary of datasets and associated training
5 if update=weight then experiments
6 // Update parameters as weights of the neural network
7 Train Nett Ur CB−Norm to optimize the parameters:
Θ=Θ∪{λ ,µ ,σ }K layer type size kernel (stride,pad)
k k k k=1 input input 3×32×32
8 end
conv1 conv+bn+relu 64×32×32 5×5 (1,2)
9 if update=moving average then pool1 maxpool 64×16×16 3×3 (2,0)
10 // Update parameters using moving average according conv2 conv+bn+relu 128×16×16 5×5 (1,2)
pool2 maxpool 128×8×8 3×3 (2,0)
the batch
conv3 conv+bn+relu 128×8×8 5×5 (1,2)
11 for r ←1 to K do pool3 maxpool 128×4×4 3×3 (2,0)
12 λ k ←m×λ k+(1−m)×mean(τ k) conv4 conv+bn+relu 256×4×4 3×3 (1,1)
13 µ k ←m×µ k+(1−m)×mean(xˆk) lp io no eal4 r la iv ng eap rool 12 05 ,6 1× 001 o× r21 00 4×4 (1,0)
14 σ k2 ←m×σ k2+(1−m)×variance(xˆk)
15 end TABLE III: Shallow CNN Architecture as Described in the
MixtureNormalization(MN)Paper:AComparisonofMNandBN
16 end under Small and Large Learning Rate Regimes.
17 Neti Un Cf B−Norm =Nett Ur CB−Norm // Inference
UCB-Norm network with frozen parameters
18 for i←1 to m do
• Transform xˆ i =UCB−Norm θ(x i) using Equationac6ross different architectural configurations. This advantage
becomesparticularlypronouncedinthemoreintricatesetting
K
xˆ
←(cid:88)τ √k(x i)
xˆk of the DenseNet architecture. Utilizing DenseNet with 40
i λ k i layers (DenseNet-40) on CIFAR-100, the test error for
k=1
the Batch Normalization (BN) and Mixture Normalization
• Modify each layer in Neti Un Cf B−Norm with input x i(t Mo N) variants stands at 0.38 and 0.37, respectively, whereas
take xˆ instead
i the Cluster-Based Normalization counterpart achieves a
19 end notably lower test error of 0.36. Transitioning to the deeper
DenseNetwith100layers(DenseNet-100),thetesterrorsfor
the BN and MN models drop to 0.3 and 0.29, respectively,
while the cluster-based normalization variants achieve the
Figure 10 vividly demonstrates that the integration of best test error of 0.28.
CB-Norm (SCB-Norm-base, SCB-Norm, and SCB-Norm) Inthefollowingsection,wewilldemonstrateaspecialized
goes beyond simply expediting optimization during application of CB-Norm with the unsupervised version
training—it consistently yields superior generalization (UCB-Norm) in image generation.CIFAR-10
model learningrate 25epochs 50epochs 75epochs 100epochs
BN-1 0.001 84,34 86,49 86,41 86,90
BN-2 0.005 83.44 84.39 85.65 85.65
MN-1 0.001 84,45 86,60 86,6 87,07
MN-2 0.005 83.47 84.60 85.68 85.80
SCB-Norm-base-1 0.001 85,9 86,93 87,09 87,11
SCB-Norm-base-2 0.005 83.65 85.09 86.28 86.41
SCB-Norm-1 0.001 84.68 86.61 87.38 87.57
SCB-Norm-2 0.005 84.85 86.54 87.07 87.07
UCB-Norm-1 0.001 85.15 85.80 87.97 87.97
UCB-Norm-2 0.005 86.04 86.04 87.49 87.49
CIFAR-100
model learningrate 25epochs 50epochs 75epochs 100epochs
BN-1 0.001 57,41 59,74 59,82 59,82
BN-2 0.005 55.64 56.97 56.29 56.29
MN-1 0.001 56,90 59,80 59,80 60,10
MN-2 0.005 55.80 55.80 56.56 57.09
SCB-Norm-base-1 0.001 57,74 61,01 61,01 61,01
SCB-Norm-base-2 0.005 55.47 57.06 57.06 57.60
SCB-Norm-1 0.001 58.50 59.25 59.65 61.02
SCB-Norm-2 0.005 55.81 57.68 57.68 57.68
UCB-Norm-1 0.001 59.44 61.10 61.10 61.10
UCB-Norm-2 0.005 58.31 60.29 60.29 60.29
TinyImageNet
model learningrate 25epochs 50epochs 75epochs 100epochs
BN-1 0.001 37.17 37.17 37.17 37.17
BN-2 0.005 34.11 34.11 34.11 34.11
MN-1 0.001 38,18 38,17 38,5 38,5
MN-2 0.005 34.56 34.99 35.12 35.27
SCB-Norm-base-1 0.001 38.53 40.08 40.08 40.08
SCB-Norm-base-2 0.005 34.78 35.44 35.44 36.71
SCB-Norm-1 0.001 37.72 39.45 40.29 40.29
SCB-Norm-2 0.005 37.97 38.31 38.31 38.31
UCB-Norm-1 0.001 39.71 39.71 39.71 39.71
UCB-Norm-2 0.005 35.78 37.03 37.03 37.03
TABLE IV: EvaluatingPerformanceonCIFAR-10,CIFAR-100,andTinyImageNetUsingtheShallowCNNarchitecture(seeTableIII)
with the Integration of Batch Normalization (BN), Mixture Normalization (MN), and Cluster-Based Normalization (SCB-Norm-base,
SCB-Norm and UCB-Norm). Each model is associated with a specific numerical identifier, denoting the learning rate.
(a)SCB-Norm
(b)UCB-Norm
Fig. 11: Visualization of Clustering Patterns in Activations: The t-SNE visualization of activations from models trained on CIFAR-100
withSCB-NormandSCB-Normnormalizationlayers.Thefigureillustratestheformationofdistinctclustersaligningwithpredictedtarget
classes, showcasing improved clustering patterns over the course of training.
D. UnsupervisedCluster-BasedNormalization(UCB-Norm) Thegeneratorcreatesdata,whilethediscriminatorevaluates
Generative Adversial Networks (GANs) its authenticity. The training process occurs through
iterations, where the generator seeks to improve its ability
GANs, or Generative Adversarial Networks [23], are
to deceive the discriminator, while the discriminator aims
deep learning models used to generate realistic data, such
to become more effective at distinguishing generated data
as images [24]–[26]. The GAN architecture consists of two
from real data.
competing neural networks: a generator and a discriminator.However, a common challenge encountered when using
GANs is the issue of ”mode collapse”. This phenomenon
occurs when the generator produces only a restricted
subset of possible data, leading to a loss of diversity in
the generated results. In other words, the generator may
focus on producing a specific type of data, neglecting
the generation of other potential variations. This problem
can compromise the quality and variety of the generated
data, requiring specific techniques and strategies to address
and enhance the overall performance of the GAN model.
UCB-Norm independently standardizes internal activations
across diverse disentangled modes of variation, utilizing
a normalization-based approach with multiple mixture
Fig. 12: Unsupervised Cluster-Based Normalization (UCB-Norm)
components. We postulate that integrating this approach
incorporated into the architecture of the deep Convolutional
into the generator has the potential to markedly improve the
Generative Adversarial Network (DCGAN) [24]. Our findings
training procedure of GANs. indicate that integrating our proposed cluster-based normalization
To test our hypothesis, we examine the Deep into the generator of DCGAN (DCGAN-UCB-Norm) enhances
Convolutional Generative Adversarial Network the efficiency of the training process. In contrast to the
standard DCGAN employing batch normalization (DCGAN-BN)
(DCGAN) [24], which is a dedicated GAN specifically
and DCGAN using mixture normalization (DCGAN-MN),
crafted for image generation tasks. In the architecture We
DCGAN-UCB-Norm not only demonstrates accelerated
used,thegeneratoriscomposedofalinearlayerfollowedby convergence but also attains superior (lower) Fre´chet Inception
four deconvolutional layers. The first three deconvolutional Distance (FID) scores.
layers are succeeded by a batch normalization layer
and then a LeakyReLU [27] activation function. In
this setup, the linear layer likely serves as an initial This method enhances the applicability of the supervised
mapping from a latent space to a higher-dimensional version of CB-Norm. We will employ diverse neural
representation. The four deconvolutional layers, also known network architectures to thoroughly explore the versatility.
as transposed convolutional layers, work to upsample The mixture normalization, requiring data partitioning
the input progressively, transforming it into a realistic with the EM algorithm to find normalization parameters
output. Batch normalization is applied after the first three corresponding to the components, cannot be applied as a
deconvolutional layers, aiding in stabilizing and expediting baseline.
the training process. The LeakyReLU activation function
E. Cluster-Based Normalization In Domain Adaptation
introduces non-linearity, enabling the model to learn more
intricate mappings. We replace the batch normalization In this experimental setup, we showcase how the
layersassociatedwiththefirsttwodeconvolutionlayerswith effectiveness of cluster-based normalization in improving
mixture normalization (utilizing three mixture components) local representations can result in substantial advancements
and unsupervised cluster-based normalization, also with in domain adaptation. As elucidated in [29], domain
K = 3 mixture components. The training of all models adaptation entails harnessing knowledge acquired by a
is conducted on CIFAR-100 for 200 epochs, employing model from a related domain, where there is an ample
the Adam optimizer [20] with α = 0.0002, β = 0, amount of labeled data, to enhance the model’s performance
1
and β = 0.9 for both the generator and discriminator. in a target domain characterized by limited labeled data.
2
The assessment of GAN quality is gauged using the In this context, we consider two domains (clusters)
”Fre´chet Inception Distance” (FID) [28], calculated (K = 2): the ”source domain” and the ”target domain”. To
every 10 epochs for computational efficiency. Figure 12 exemplify this, we employ the two versions of cluster-based
illustrates that the DCGAN incorporating cluster-based normalization (SCB-Norm with its variant SCB-Norm-base,
normalization (DCGAN-UCB-Norm) exhibits not only a and UCB-Norm) in conjunction with AdaMatch [18], a
quicker convergence compared to its batch-normalized and method that integrates the tasks of unsupervised domain
mixture-normalized counterparts but also achieves superior adaptation (UDA), semi-supervised learning (SSL), and
(lower) Fre´chet Inception Distance (FID) scores. semi-supervised domain adaptation (SSDA). In UDA, we
At the conclusion of the model training, Figure 13 have access to a labeled dataset from the source domain and
displays examples of generated images produced by an unlabeled dataset from the target domain. The objective
DCGANs with batch normalization, mixture normalization, is to train a model capable of generalizing effectively to
and cluster-based normalization. the target dataset. It’s important to note that the source and
target datasets exhibit variations in distribution. Specifically,
In the upcoming experiments, we will focus on practical we utilize the MNIST dataset as the source dataset, while
approaches to constructing prior knowledge clusters in the the target dataset is SVHN. Both datasets encompass
caseofSCB-Normwithoutrelyingonanyspecificalgorithm. various factors of variation, including texture, viewpoint,(a)DCGAN-BN
(b)DCGAN-MN
(c)DCGAN-UCB-Norm
Fig. 13: Examples of generated images at epoch 200 are showcased for DCGAN-BN, DCGAN-MN, and DCGAN-UCB-Norm in
Figure 13a,13b, and13c, respectively.MNIST(sourcedomain)
this manner can be challenging in this use case, as the
model accuracy precision recall f1-score
information from each component is incorporated into
AdaMatch 97.36 87.33 79.39 78.09
AdaMatch+SCB-Norm-base 99.26 99.20 99.32 99.26 all images, regardless of their domain. This can strongly
AdaMatch+SCB-Norm 98.92 98.93 98.92 98.92 influence the representation of the data and consequently
AdaMatch+UCB-Norm 98.9 98.5 98.90 98.95
impact performance. Figure 14 visually underscores the
SVHN(targetdomain)
model accuracy precision recall f1-score invaluable contribution of cluster-based normalization
AdaMatch 25.08 31.64 20.46 24.73 through SCB-Norm in stabilizing the gradient across the
AdaMatch+SCB-Norm-base 43.10 53.83 43.10 47.46 entire training process, yielding benefits for both the
AdaMatch+SCB-Norm 54.70 59.74 54.70 54.55
source and target domains. Consequently, this stabilization
AdaMatch+UCB-Norm 33.4 43.83 40.28 42.87
significantly contributes to accelerated convergence and an
TABLE V: Comparing model performance: AdaMatch
overall enhancement in model performance.
vs. AdaMatch+SCB-Norm-base, AdaMatch+SCB-Norm and
AdaMatch+UCB-Norm on MNIST (source) and SVHN (target)
datasets. In the following experiment, we will demonstrate
another approach to construct prior knowledge (clusters)
on supervised version (SCB-Norm) on a Vision
appearance, etc., and their domains, or distributions, are Transformer [32] and then on a Convolutional Neural
distinct from each other. Network (CNN).
A model, referred to as AdaMatch [30] (using batch
F. Supervised Cluster-Based Normalization (SCB-Norm)
normalization as normalization), is trained from the ground
Using Superclasses As Prior Knowledge
up using wide residual networks [31] on pairs of datasets,
serving as the baseline model. The training of this model As delineated in Section IV-A, both the CIFAR-100
involves utilizing the Adam optimizer [20] with a cosine dataset and the Oxford-IIIT Pet dataset incorporate a
decay schedule, gradually reducing the initial learning superclass structure alongside their class divisions. In our
rate initialized at 0.03. SCB-Norm, SCB-Norm-base, and proposed supervised cluster-based normalization approach,
UCB-Norm are incorporated as initial layers in AdaMatch, weharnessthissuperclassinformationas”priorknowledge”
leading to the creation of three AdaMatch models based for classification tasks. Specifically, each superclass is
on cluster-based normalization (AdaMatch+SCB-Norm, treated as a distinct cluster. We have incorporated the
AdaMatch+SCB-Norm-base, and AdaMatch+UCB-Norm). adaptivenormalizationtechniqueintotheVisionTransformer
This integration allows the cluster identifier (source domain (ViT) architecture for CIFAR-100 classification. For our
and target domain) to influence the image normalization ViT implementation, we utilized a Keras baseline provided
process. Since the labels in the source domain are known, by [33]. Additionally, for the Oxford-IIIT Pet dataset, we
the model can offer a more accurate representation of adoptedaConvolutionalNeuralNetwork(CNN)architecture
this domain in comparison to the target domain where as our baseline, as described in [34].
the labels are unknown. This advantageous aspect is 1) Using CIFAR-100 Superclass as Prior Knowledge
harnessed by considering the cluster identifier of the in supervised cluster-based normalization: The central
source domain during inference in AdaMatch+SCB-Norm innovation in this experiment revolves around utilizing
and AdaMatch+SCB-Norm-base, thereby improving the CIFAR-100 superclasses as prior knowledge (clusters) for
model’s performance on the target domain. Undoubtedly, predicting the dataset’s 100 classes, particularly in the case
within a wider context, Table V unequivocally highlights a of SCB-Norm-base and SCB-Norm methods.
remarkable enhancement in validation metrics attributable Three distinct models were employed: the base ViT
to the incorporation of cluster-based normalization. This model obtained from Keras [33], a modified version
advancement is conspicuous, manifesting as a substantial incorporating a Batch Normalization (BN) layer as its initial
boost in accuracy by 18.02% with SCB-Norm-base, component,andtwoalternativemodelsthatreplacedtheBN
29.62% with SCB-Norm, and 8.32%. Such marked layer with SCB-Norm-base and SCB-Norm layers. Training
improvement notably fortifies the performance of the included early stopping based on validation performance,
AdaMatch model, leading to a significantly expedited and images were pre-processed by normalizing them with
convergence during training. The difference in performance respect to the dataset’s mean and standard deviation. Data
between the supervised approach (SCB-Norm and its augmentation techniques, such as horizontal flipping and
variant SCB-Norm-base) and the unsupervised approach random cropping, were applied to enhance the dataset.
(UCB-Norm) can be explained by the fact that SCB-Norm The AdamW optimizer, with a learning rate of 10−3
takes prior knowledge information (clusters that are source and a weight decay of 10−4, was chosen to prevent
and target domains), as input and directly integrates it overfittingandoptimizemodelparameters[19,20].TableVI
into the normalization process. This allows for a specific presents the notable performance enhancements achieved
representation for each domain. In contrast, UCB-Norm by our innovative Supervised Cluster-Based Normalization
does not take prior knowledge as input but only considers (SCB-Norm and its variant SCB-Norm-base) in comparison
the number of mixture components to be estimated, to training the ViT architecture from scratch on CIFAR-100
which is K = 2. Estimating mixture components in with Batch Normalization (BN). The SCB-Norm approach(a)Sourcedomain(MNIST)
(b)Targetdomain(SVHN)
Fig. 14: Evolution of the gradient variance during the training of AdaMatch and AdaMatch+SCB-Norm models on the source (MNIST)
and target (SVHN) domains. The figures on the left correspond to the maximum gradient variance for each epoch, while the figures on
the right correspond to the average gradient variance per epoch.
model accuracy precision recall f1-score
illustrates this approach, showing a representation of data
ViT+BN 55.63 8.96 90.09 54.24
ViT+SCB-Norm-base 65.87 23.36 98.53 65.69 accordingtopriorknowledge.Theemergingclusterstructure
ViT+SCB-Norm 67.38 22.93 99.00 67.13 providesabetterrepresentation,therebycontributingtoclass
TABLE VI: Assessing the Performance of CIFAR-100 Using separation and facilitating prediction.
the ViT Architecture [33] incorporating BN, SCB-Norm-base,
and SCB-Norm, with CIFAR-100 superclasses serving as prior
2) Using Oxford-IIIT Superclass as Prior Knowledge in
knowledge or clusters.
supervised cluster-based normalization: In this experiment,
mirroring the one in the preceding section with CIFAR-100,
we leverage the superclasses (dog and cat) from the
demonstrates an accuracy improvement of approximately Oxford-IIIT Pet Dataset to classify the 37 categories
12% over BN. Furthermore, it is noteworthy that both within the dataset. In this methodology, we employ a
SCB-Norm-base and SCB-Norm exhibit faster convergence CNN (OxfordCNN) as the foundational model [34] and
compared to BN, requiring less training time to attain integrate our SCB-Norm layer just before the neural
superior performance. This observation is further supported network’s prediction layer (linear layer). This experiment
by the comparison of train and validation loss depicted in illustrates that supervised cluster-based normalization can
Figure 15, indicating that SCB-Norm promotes accelerated function as a layer at various levels of the neural network,
learning while improving classification performance. These thereby enhancing both performance and convergence.
findings suggest that the proposed SCB-Norm method not The models underwent training for 200 epochs with
only stabilizes data distributions and mitigates internal early stopping criteria based on validation performance.
covariateshiftbutalsosignificantlyreducestrainingtimefor Image preprocessing involved normalization using the mean
enhancedresults.ViT+SCB-Norm-baseandViT+SCB-Norm and standard deviation of the dataset. Data augmentation
achieve outstanding performance, surpassing all known ViT techniques, including horizontal flipping and random
modelswhentrainedfromscratchontheCIFAR-100dataset. cropping,wereemployedtoenhancethedataset.Thetraining
utilized the Adam optimizer [20] with a learning rate of
After training the ViT model with the SCB-Norm layer 10−4.
(ViT+SCB-Norm), it is applied to a randomly selected The results presented in Table VII reinforce the
batch of data to analyze the effect of SCB-Norm on earlier observations with CIFAR-100. It is apparent
the representation of data according to prior knowledge that cluster-based normalization leads to significantly
(superclasses using as distinct clusters). After retrieving the faster convergence, a trend further supported by the
outputs, Principal Component Analysis (PCA) is applied, performance achieved by SCB-Norm. SCB-Norm reaches a
with 5 components, to visualize the behavior of our performancelevelafter80epochsthatsurpassesthebaseline
supervised cluster-based normalization method. Figure 16 (OxfordCNN),whichtakes113epochstoreachconvergence.Fig. 15: Contrasting Training and Validation Loss Curves: Within the ViT Architecture on CIFAR-100, SCB-Norm and SCB-Norm-base
exhibit swifter convergence and reduced validation loss, enhancing learning efficiency and classification performance compared to BN.
Metrics
Model Epoch
Accuracy Precision Recall F1-Score
OxfordCNNbase Epoch80 63.12 50.23 80.11 69.23
Epoch113 66.55 53.11 84.2 72.15
OxfordCNN+SCB-Norm-base Epoch80 64.90 52.34 83.10 73.11
Epoch113 67.52 55.12 86.13 75.90
OxfordCNN+SCB-Norm Epoch80 67.92 54.24 89.11 80.12
Epoch113 73.61 69.12 92.34 90.11
TABLE VII: Assessing Performance on the Oxford-IIIT Pet Dataset Using the OxfordCNN Architecture [34], incorporating
SCB-Norm-base and SCB-Norm with ”dog” and ”cat” superclasses serving as prior knowledge (distinct clusters).
NormalizationMethod ComputationalComplexity ParameterComplexity
BatchNormalization(BN) O(d) O(d)
MixtureNormalization(MN) O(d·K·log(K));O(d·n·K) -
SupervisedCluster-BasedNormalization(SCB-Norm-base) - O(d)
SupervisedCluster-BasedNormalization(SCB-Norm) - O(K·d)
UnsupervisedCluster-BasedNormalization(UCB-Norm) - O(K·d)
TABLE VIII: Complexities in Estimating Parameters for Normalization Methods. Computational Complexity corresponds to the time
requiredforparameterestimation,whileParameterComplexitycorrespondstothenumberofparameterstobeestimated.Here,drepresents
the dimension of the data, K denotes the number of mixture component centers, and n the number of samples.
V. ANALYZINGCOMPUTATIONALCOMPLEXITYIN the centers of the mixture components. Subsequently, the
DETAIL Expectation-Maximization (EM) algorithm is iteratively
The computational overhead of cluster-based applied to estimate the parameters of the mixture model.
normalization, when compared to batch normalization [4] In the context of the mixture normalization paper,
and mixture normalization [6], stems from the task of K-means++ [35] is specifically chosen as the seeding
estimating mixture components parameters.Let B denote a procedure due to its applicability without assuming any
batch of activations located in Rd, where d signifies the characteristics about the underlying data. The complexity of
dimensionality of activations. the K-means++ algorithm for a given batch B of samples
Batch normalization computes the mean (µ ) and withdimensiondisO(d·K·log(k)),whereK isthenumber
B
standard deviation (σ ) by considering the entire set of of cluster center. For the EM algorithm, the complexity
B
activations within the batch B. In terms of complexity, this of each iteration is O(d · n · K), where n is the number
involvescalculating themean (µ)and standarddeviation (σ) of samples in B. The EM algorithm typically converges
for each dimension of the batch (see Equation 1), resulting in a small number of iterations, often less than 10 or 20.
in a computational complexity of O(d). Additionally, this Consequently,theoverallcomplexityofrunningK-means++
method requires learning two parameters, γ and β, for each followed by the EM algorithm on a batch of samples B
dimension of the batch, contributing to a parameter learning is primarily determined by the initialization step, resulting
complexity of O(d). Therefore, the overall complexity, in O(d·K ·log(K)), with an additional contribution from
considering both computational and parameter aspects, is the EM iterations. The total number of iterations for the
O(d) for the entire batch B. EM algorithm depends on the convergence criteria and the
The computational challenge associated with mixture specific characteristics of the data. In practice, the overall
normalization arises from the intricate process of accurately complexity is often dominated by the initialization step for
estimating the parameters of the Gaussian mixture model K-means++.
(GMM). This process involves a two-stage approach, For our cluster-based normalization approach, we will
where a seeding procedure is first employed to initialize detail the complexity for the supervised version (SCB-NormFig. 16: ProjectionofViT+SCB-Normoutputsafter applyingPrincipalComponentAnalysis(PCA)fordimensionreduction.Coloringis
basedonsuperclasses.Aclearseparationisobservedbetweenthesuperclasses,representingthepriorknowledge(clusters).Thisseparation
leads to an enhanced representation, significantly facilitating the classification task. The green line on the histograms corresponds to the
probability density function (pdf).
and its variant SCB-Norm-base) and the unsupervised variant, utilizes two linear layers of dimension d. It employs
version (UCB-Norm). Consider the activation batch B, with these two linear layers across all clusters K to estimate the
dimension d. Assuming that the defined number of prior parameters µ and σ, reducing the complexity from O(K·d)
knowledge (clusters) is K, the complexity of SCB-Norm to O(d). The unsupervised version, UCB-Norm, estimates
is correlated with K. Specifically, for each cluster k, two three learnable parameters for each mixture component
learnable parameters of dimension d are associated with it K: λ of dimension 1, and µ and σ of dimension d.
k k k
(µ andσ ).Therefore,thetotalnumberofparameterstobe This results in a total of 2×K ×d+K parameters to be
k k
learnedforallclustersis2×K×d,resultinginacomplexity learned, with a complexity of O(K · d). Unlike mixture
ofO(K·d)forSCB-Norm.SCB-Norm-base,beingalighter normalization, both the supervised and unsupervisedversions of cluster-based normalization are performed in a [13] A.Krizhevsky,V.Nair,andG.Hinton,“CIFAR-10(canadianinstitute
one-stage process, and the complexity is solely associated foradvancedresearch),”2009.
[14] A.Krizhevsky,V.Nair,andG.Hinton,“CIFAR-100(canadianinstitute
with the parameter estimation during the backpropagation
foradvancedresearch),”2009.
process. A detailed comparison is presented in Table VIII. [15] Y.LeandX.Yang,“Tinyimagenetvisualrecognitionchallenge,”CS
231N,vol.7,no.7,p.3,2015.
VI. CONCLUSION [16] Y.LeCunandC.Cortes,“MNISThandwrittendigitdatabase,”2010.
[17] P. Sermanet, S. Chintala, and Y. LeCun, “Convolutional neural
In conclusion, Cluster-Based Normalization (CB-Norm)
networksappliedtohousenumbersdigitclassification,”inProceedings
emergesasagroundbreakingone-stepnormalizationmethod, of the 21st international conference on pattern recognition
addressing challenges in deep neural network training. Its (ICPR2012),pp.3288–3291,IEEE,2012.
[18] D. Berthelot, R. Roelofs, K. Sohn, N. Carlini, and A. Kurakin,
adaptability and unique approach, incorporating Gaussian
“Adamatch: A unified approach to semi-supervised learning and
mixture models, mark a significant advancement in domainadaptation,”arXivpreprintarXiv:2106.04732,2021.
overcoming issues like internal covariate shift and gradient [19] I.LoshchilovandF.Hutter,“Decoupledweightdecayregularization,”
arXivpreprintarXiv:1711.05101,2017.
stability. Through Supervised Cluster-Based Normalization
[20] D. P. Kingma and J. Ba, “Adam: A method for stochastic
(SCB-Norm)andUnsupervisedCluster-BasedNormalization optimization,”arXivpreprintarXiv:1412.6980,2014.
(UCB-Norm),CB-Normdemonstratesversatilityinhandling [21] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger,
“Densely connected convolutional networks,” in Proceedings of
diverse learning scenarios. SCB-Norm utilizes predefined
the IEEE conference on computer vision and pattern recognition,
data partitioning (clusters), while UCB-Norm dynamically pp.4700–4708,2017.
clusters activations witout using prior knowledge, both [22] Y.Bengio,N.Boulanger-Lewandowski,andR.Pascanu,“Advancesin
optimizingrecurrentnetworks,”in2013IEEEinternationalconference
contributing to enhanced adaptability. Our experiments aim
on acoustics, speech and signal processing, pp. 8624–8628, IEEE,
to assess CB-Norm’s performance, comparing it with Batch 2013.
Normalization (BN) and Mixture Normalization (MN). We [23] I.Goodfellow,J.Pouget-Abadie,M.Mirza,B.Xu,D.Warde-Farley,
S.Ozair,A.Courville,andY.Bengio,“Generativeadversarialnets,”
applySCB-NormandUCB-Normtovarioustasks,including
Advancesinneuralinformationprocessingsystems,vol.27,2014.
classification, domain adaptation, and generative neural [24] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation
networks.Furthermore,weintroduceSCB-base,asimplified learning with deep convolutional generative adversarial networks,”
arXivpreprintarXiv:1511.06434,2015.
variant, maintaining a fixed cluster identifier (k) and
[25] C.Ledig,L.Theis,F.Husza´r,J.Caballero,A.Cunningham,A.Acosta,
leveraging neural networks to encode cluster information. A.Aitken,A.Tejani,J.Totz,Z.Wang,etal.,“Photo-realisticsingle
These experiments collectively aim to showcase the efficacy image super-resolution using a generative adversarial network,” in
Proceedings of the IEEE conference on computer vision and pattern
and versatility of CB-Norm in improving deep neural
recognition,pp.4681–4690,2017.
network performance across a spectrum of applications. [26] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image
translation with conditional adversarial networks,” in Proceedings of
REFERENCES the IEEE conference on computer vision and pattern recognition,
pp.1125–1134,2017.
[1] A. Kessy, A. Lewin, and K. Strimmer, “Optimal whitening and [27] A. L. Maas, A. Y. Hannun, and A. Y. Ng, “Rectifier nonlinearities
decorrelation,”TheAmericanStatistician,vol.72,no.4,pp.309–314, improve neural network acoustic models,” arXiv preprint
2018. arXiv:1303.5778,2013.
[2] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. Mu¨ller, “Efficient [28] M.Heusel,H.Ramsauer,T.Unterthiner,B.Nessler,andS.Hochreiter,
backprop,”inNeuralnetworks:Tricksofthetrade,pp.9–50,Springer,
“Ganstrainedbyatwotime-scaleupdateruleconvergetoalocalnash
2002. equilibrium,”inAdvancesinNeuralInformationProcessingSystems,
[3] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers: 2017.
Surpassing human-level performance on imagenet classification,” in [29] A. Farahani, S. Voghoei, K. Rasheed, and H. R. Arabnia, “A
ProceedingsoftheIEEEinternationalconferenceoncomputervision, brief review of domain adaptation,” Advances in Data Science and
pp.1026–1034,2015. Information Engineering: Proceedings from ICDATA 2020 and IKE
[4] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep 2020,pp.877–894,2021.
networktrainingbyreducinginternalcovariateshift,”inInternational
[30] S. Paul, “Unifying semi-supervised learning and
conferenceonmachinelearning,pp.448–456,PMLR,2015.
unsupervised domain adaptation with adamatch,” 2019.
[5] L.Huang,J.Qin,Y.Zhou,F.Zhu,L.Liu,andL.Shao,“Normalization https://github.com/keras-team/keras-io/tree/master.
techniques in training dnns: Methodology, analysis and application,” [31] S. Zagoruyko and N. Komodakis, “Wide residual networks,” arXiv
arXivpreprintarXiv:2009.12836,2020. preprintarXiv:1605.07146,2016.
[6] M.M.KalayehandM.Shah,“Trainingfasterbyseparatingmodesof [32] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
variationinbatch-normalizedmodels,”IEEEtransactionsonpattern T.Unterthiner,M.Dehghani,M.Minderer,G.Heigold,S.Gelly,etal.,
analysisandmachineintelligence,vol.42,no.6,pp.1483–1500,2019.
“Animageisworth16x16words:Transformersforimagerecognition
[7] A.P.Dempster,N.M.Laird,andD.B.Rubin,“Maximumlikelihood atscale,”arXivpreprintarXiv:2010.11929,2020.
from incomplete data via the EM algorithm,” JOURNAL OF THE [33] K. Salama, “Implementing the vision transformer
ROYALSTATISTICALSOCIETY,SERIESB,vol.39,no.1,pp.1–38,
(vit) model for image classification,” 2021.
1977. https://github.com/keras-team/keras-io/tree/master.
[8] J.L.Ba,J.R.Kiros,andG.E.Hinton,“Layernormalization,”arXiv [34] M. Garg, “Pet image classification,” 1021.
preprintarXiv:1607.06450,2016.
https://github.com/mayur7garg.
[9] D. Ulyanov, A. Vedaldi, and V. Lempitsky, “Instance normalization: [35] D.ArthurandS.Vassilvitskii,“k-means++:Theadvantagesofcareful
The missing ingredient for fast stylization,” arXiv preprint seeding,”ProceedingsoftheeighteenthannualACM-SIAMsymposium
arXiv:1607.08022,2016. onDiscretealgorithms,pp.1027–1035,2007.
[10] Y. Wu and K. He, “Group normalization,” in Proceedings of the
Europeanconferenceoncomputervision(ECCV),pp.3–19,2018.
[11] V.Dumoulin,J.Shlens,andM.Kudlur,“Alearnedrepresentationfor
artisticstyle,”arXivpreprintarXiv:1610.07629,2016.
[12] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz, “Multimodal
unsupervised image-to-image translation,” in Proceedings of the
Europeanconferenceoncomputervision(ECCV),pp.172–189,2018.