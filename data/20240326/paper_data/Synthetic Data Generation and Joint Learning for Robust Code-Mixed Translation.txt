Synthetic Data Generation and Joint Learning for Robust
Code-Mixed Translation
Kartik1, Sanjana Soni2, Anoop Kunchukuttan3,
Tanmoy Chakraborty4, Md Shad Akhtar2
1WashingtonPost,2IIITDelhi,4IITDelhi,3MicrosoftIndia.
kartikaggarwal98@gmail.com, anoop.kunchukuttan@microsoft.com
tanchak@iitd.ac.in, {sanjana19097, shad.akhtar}@iiitd.ac.in
Abstract
Thewidespreadonlinecommunicationinamodernmultilingualworldhasprovidedopportunitiestoblendmorethan
one language (aka code-mixed language) in a single utterance. This has resulted a formidable challenge for the
computationalmodelsduetothescarcityofannotateddataandpresenceofnoise. Apotentialsolutiontomitigatethe
datascarcityprobleminlow-resourcesetupistoleverageexistingdatainresource-richlanguagethroughtranslation.
Inthispaper, wetackletheproblemofcode-mixed(HinglishandBengalish)toEnglishmachinetranslation. First,
wesyntheticallydevelopHINMIX,aparallelcorpusofHinglishtoEnglish,with~4.2M sentencepairs. Subsequently,
we propose RCMT, a robust perturbation based joint-training model that learns to handle noise in the real-world
code-mixedtextbyparametersharingacrosscleanandnoisywords. Further,weshowtheadaptabilityofRCMTina
zero-shotsetupforBengalishtoEnglishtranslation. Ourevaluationandcomprehensiveanalysesqualitativelyand
quantitativelydemonstratethesuperiorityofRCMToverstate-of-the-artcode-mixedandrobusttranslationmethods.
1. Introduction very action packed even for today). The problem
isexacerbatedbythemultilingualnatureofonline
Recentexplosionofdigitalcommunicationaround code-mixed content, making it essential to under-
theworldhasbeenmarkedbythegrowinguseof standCMconcerningacommonlanguage.
informallanguageinonlineconversations. These
NeuralMachineTranslation(NMT)modelshave
conversations often feature the use of words and
becomestate-of-the-artinsequence-to-sequence
phrases from multiple languages back and forth
tasks (Sutskever et al., 2014; Bahdanau et al.,
into a single utterance: a phenomenon referred
2015). At the root of this advancement are
toascode-mixing(CM)orcode-switching(Myers-
two interrelated issues: (i) NMT models need
Scotton, 1993a,b; Duran, 1994). Code-mixing
a vast amount of parallel data for satisfactory
has become a standard practice both as a form
performance; and (ii) NMT models are brittle to
of speech and text in multilingual communities
even a slight amount of input noise (Belinkov
suchasHindi-English(Hinglish),Spanish-English
and Bisk, 2018). In order to circumvent all
(Spanglish),etc.,wherepeoplesubconsciouslyal-
thesechallenges,weproposeRobustCode-Mixed
terbetweenlanguages. Consideringit’sprominent
Translation (RCMT) using a joint learning frame-
use, it is imperative to build NLP technologies for
work. First, to handle the scarcity of code-mixed
code-mixeddata.
parallel data, we construct a synthetic Hinglish-
However, due to the unavailability of annotated
English dataset by leveraging a bilingual Hindi-
data, code-mixing in the domain of text remains
English (Hi-En) corpus. For this, we identify vari-
largely unexplored. With no oﬀicial references of
ousgrammaticalpatternsinthecontinuousswitch-
CM text in books and articles, online social net-
ing of two languages and formulate a general
works (OSNs) remain the only source of mixed
pipelineforcreatingasyntheticCMcorpus.
data collection. Further, the real-world unstruc-
turedtextishighlysusceptibletotypographicaler- The generated parallel data is then passed
rors and misspellings. These mistakes become throughanadversarialmodulethatinjectsdifferent
more prevalent when languages written in non- types of naturally occurring adversarial perturba-
romanized scripts such as Japanese, Hindi etc. tionstogenerateasource-sidenoisyversionofthe
are adopted to code-mixed scenarios as each code-mixeddataset. InspiredbymultilingualNMT
wordintheoriginatingscriptcanbemappedtomul- models, we train a joint model for translation of
tiple probable transliterations, e.g., “haan bilakul cleanandnoisyCMtexttomaketheCode-Mixed
(bilkul). yahekklaasik(classic)hai,lekinphirbhee Translationrobusttonoisyinput. Ourexperiments
bahut hee ekshan (action) aaj ke lie bhee paik show that by jointly training both noisy and clean
(pack) hai” (Yes, definitely. It is a classic, but still textinamultilingualsetting,themodelcanencode
diverselexicalvariationsofcode-mixedwordsinto
∗The work was carried out when Kartik was a re- the shared representation space; thereby, sub-
searchinternatIIITDelhi. stantially improving the translation quality. Addi-tionally,theneedofaparallelCMcorpusforevery et al. (2018) initiated the effort to create a 6K pair
new language pair limits the applicability of NMT gold-standard Hindi-English CM dataset. Follow-
models for code-mixed translation. Further, the ingthis,manyresearchersproposedvariousmeth-
availabilityandaccuracyoflanguagespecificPOS- ods for synthetic CM generation – Pratapa et al.
taggers, translation dictionaries, filtering tools be- (2018) utilized parse trees, whereas, pointer gen-
come pivotal for building a synthetic CM corpus. eratornetworkwasemployedbySeeetal.(2017)
Toeasethischallenge,weproposezero-shot CM and Winata et al. (2019). Recently, Gupta et al.
translation,whereabilingualBengali-English(Bn- (Gupta et al., 2020) explored linguistic properties
En) parallel corpus is trained along with a code- and employed an encoder-decoder model to gen-
mixedHindi-Englishparallelcorpus. Thisway,the erateCMsentencesautomaticallywithoutparallel
model learns to adapt to the multilingual scenario corpus. In another work, Gupta et al. (2021) pro-
andtranslateBengaliCMtexttoEnglish. posedanmBERT-based(Devlinetal.,2019)tech-
Precisely,thecontributionsofourworkare: nique including alignment to find switch words to
• We formulate a linguistically-informed pipeline convertexistingparallelcorpustocode-mixed.
for synthetically generating codemix data from
parallelnon-code-mixedcorpora.
The presence of annotated code-mixed data
• We develop HINMIX, the first large-scale
does not ease the target task due to the exten-
Hinglish Code-Mixed parallel corpus consisting
sive amount of typos, slang, and phonetic varia-
of ~4.2M parallel sentences. Weannotate 2787
tions in the data; thus, making it implausible to
goldstandardCMsentencesfortheevaluation.
overlooktherobustnessagainstthenoiseofexist-
• We propose a novel RCMT model for effec-
ing solutions. (Belinkov and Bisk, 2018) showed
tively translating real-world noisy code-mixed
thatthemodel’sperformancesignificantlygetsaf-
sentencestoEnglish.
fected in the presence of moderate noisy texts.
• We explore Zero-Shot Code-Mixed Translation
They also provided structure-invariant word rep-
for Bengali code-mixed to English translation
resentations and robust training on noisy text ap-
withoutanyparallelCMcorpus.
proaches to boost system performance. In an-
otherwork,Karpukhinetal.(2019)presentedsyn-
Reproducibility: Code and datasets are avail- thetic character-level noise to improve the robust-
able at https://github.com/LCS2-IIITD/ nesstonaturalmisspellingsforMTsystems. How-
Robust_CodeMIX_MT. ever,itlackstogeneralizetoinformaltextpresent
on social media discourse. Arguing the need of
perturbation-invariantlearning,Chengetal.(2018,
2. Related Work
2020) adopted an adversarial stability training ob-
jective to learn a perturbation-invariant encoder.
Phenomena of code-mixing and intrasentential
Furthermore, Sato et al. (2019) showed promis-
code-switching have been fairly studied (Verma,
ingresultsbyemployingadversarialregularization
1976;Joshi,1982;Singh,1985). Joshi(1982)pro-
techniquesinanNMTmodelandarguedthatsuch
posedaformalframeworkconsideringthetwolan-
methodsimprovethequalityofthetranslation. An
guage systems and a mechanism to switch be-
application of adversarial subword regularization
tweenthemandfurthercapturedessentialaspects
(ADVSR) framework was incorporated to expose
of intrasentential code-switching. Sankoff (1998)
subword segmentations to regularize NMT mod-
explained the presence of consistent tree label-
els (Park et al., 2020). Although these schemes
ing, implying a constraint on an equivalence or-
satisfy the robustness criteria of an NMT model,
derinconstituentsaroundaswitchpoint,whereas,
the nature of noise in CM language largely re-
Gardner-Chloros and Edwards (2004) analyzed
mainsunexploredinIndianlanguages,whichisex-
grammaticalrulesincode-switchingbasedonvari-
tremelychallengingconsideringthemorphological
ousunderlyingassumptions. Despiteagoodnum-
richnessofthelanguage.
ber of linguistic-groundedcode-mixed studiesare
existing, only a few studies (Dhar et al., 2018;
Guptaetal.,2021)haveexploreditwithinthetrans- Ourproposedworkismotivatedbythegapinre-
lation domain, primarily due to the scarcity of the searchtobuildanall-inclusivecode-mixedtransla-
parallelcorpora. tionsystemthathandlesthediverseswitchingna-
The prevalent usage of CM in day-to-day spo- ture in CM communities and is robust to a wide
ken conversations and online written content has range of CM noise. Moreover, the existing works
instilled the successful application of CM data in onCMdatagenerationforMTdonotguaranteea
various downstream tasks, such as, POS tagging large-scaledataset. Furthermore,wealsoexplore
(Jamatia et al., 2016), sentiment analysis (Patwa thezero-shotsettingtotranslatebetweenmultiple
etal.,2020),speechrecognition(Luoetal.,2018), language-pairs without the necessity to create in-
andmachinetranslation(Dharetal.,2018). Dhar dividualCMdatasets.3. Dataset thateverywordinL canbereplacedfromtheset
m
ofpotentialsubstitutewords. However,theswitch-
In this section, we describe the pipeline used to ing paradigm in a CM utterance depends upon a
createHINMIXutilizingIITBEnglish-Hindiparallel range of factors such as lexical information avail-
corpus (Kunchukuttan et al., 2018) – which con- able with the speaker, their relative fluency in the
tains text from TED Talks, Judicial domain, news languages,speaker’sintentiontoswitch,andmost
articles,Wikipediaheadlines,etc. Givenasource- importantly, the intrinsic structure of involved lan-
target sentence pair S ∥ T , we generate the syn- guages(Krolletal.,2008). Hence,insteadofsub-
theticcode-mixeddatabysubstitutingwordsinthe stituting every candidate word and generating a
matrix language sentence with the corresponding singleCMsentence,wefollowarandomizedword-
wordsfromtheembeddedlanguagesentence. selection and filtering method to obtain multiple
CMcombinationsofasinglesourcesentence.
Candidate Word Selection: We select nouns, • WordSelection: Giventhattherecanbe2r−1
adjectives (JJ), and quantifiers to be part of an CM combinations in a sentence of r candidate
inclusion list I to identify potential candidates for words, we adopt following length-based heuris-
code-switching. Given a source sentence S = tics to limit the CM sentences to be generated.
{s ,s ,...,s } ∈ L and a target sentence T = Thisallowsustonarrowdownthesamplespace,
1 2 n m
{t ,t ,...,t }∈L ,weobtainPOStagsforeach whichotherwise,wouldhavebeencomputation-
1 2 m e
word in S. Subsequently, we shortlist the candi- allyexpensiveforlarger:
date words S￿ = s such that their corresponding Heuristicforcandidatewordselection:
i
POS tags belong the inclusion list p ∈ I. These – Forr≤4: Useallvalidcombinations. Forex-
i
words can be substituted with their English coun- ample,ann-wordsentencewith3candidate
terpartsE’=e toformacode-mixedsentence. wordswillhave23−1=7CMsentences.
j
Notethatwedonotincludeverbs(VB)andother – For 5≤r≤7: Use r − 3 to r candidate
tagsinI astheyusuallydon’tfollowaone-to-one word combinations. For example, a sen-
replacementruleinthecode-switchedtextandof- tence with 5 candidate words will have
tencannotbedirectlyreplacedduetothemorpho- 5C +5C +5C +5C =26CMsentences.
2 3 4 5
logical richness of Hindi language. For example, – For r≥8: Use 0.6r to 0.7r candidate
insentence‘ (Heisplaying.)’,theverb word combinations. For example, a sen-
वहखेलरहाहै।
‘playing’ is mapped to ‘ ’ in Hindi. Choosing tence with 15 candidate words will have
खेल रहा
either‘ ’or‘ ’aspotentialcandidateswould 15C +15C =8008CMsentences.
खेलरहा खेल 9 10
result in inaccurate CM sentences (‘ playing • SentenceFiltering: Tofurthernarrowdownthe
वह है।
or play ). For simplicity, we only choose selection pool and incorporate language struc-
वह रहा है।
nouns,adjectives,andquantifiersininclusionlist. tures of bilingual languages into synthetic CM
sentences, we use a combination of probabilis-
Building Substitution Dictionary: Once the ticanddeterministicNLPevaluationmetrics.
corpus is POS-tagged using the LTRC parser1 – We use an unsupervised cross-lingual
andcandidatewordsareshortlisted,thesubstitute XLM (Conneau and Lample, 2019) model
words from L need to be determined. We pro- to calculate the perplexity of CM sentences.
e
pose an alignment-based strategy to build a sub- We observe a good correlation between the
stitution dictionary. At first, we train an alignment fluencyoftheCMsentenceanditsperplexity,
model on IITB Hi-En parallel corpus (Kunchukut- even when provided with Devanagari Hindi
tan et al., 2018) to learn word-level correspon- andEnglishtextinasingleCMsentence.
dence between each parallel sentence. We use – We employ code-mixed specific measures
the fast-align (Dyer et al., 2013) symmetric align- such as Code-Mixing Index (CMI) (Gambäck
ment model to obtain the source-target alignment and Das, 2016) and Switch Point Fraction
matrix. Next,asubstitutiondictionaryD foreach (SPF)(Guptaetal.,2020)toselectsentences
i
sentenceisobtained,consistingofonlywordswith betweenacertainthreshold.
one-to-onesource-targetmapping. Thisapproach Figure1showstheprocessofgeneratingCMsen-
allows us to deal with the word-sense ambiguity tencesinHINMIX.Thisformsourcode-mixedpar-
problembysubstitutingcontext-dependentforeign allel dataset with Hindi (Devanagari)-English CM
wordsineachsentence,therebyformingadiverse pairs, Hi c-En. Finally, for each case, we use
setofcode-mixedvocabularyinthecorpus. Google Transliterate API2 to produce the roman-
izedversionoftheCMparallelcorpora–Hi -En.
cr
Intotal,weobtain~4.2Mparallelsentences.
Language Switching: It might appear that the
decision to switch a word is a binary choice and
2https://developers.google.com/
1http://ltrc.iiit.ac.in/analyzer/ transliterate/v1/getting_startedEn: The tendency to give physical training to the whole society resulted in many disastrous consequences.
Hi: समस्त समाज को शारी  रक प्र  शक्षण देने के कारण बहुत बुरे प  रणाम हुए।
HINMIX dataset
Hicrn: whole society ko physical training dene ke karan bahut disastrous consequences huee.
Hicrn: whole samaaj ko physical training dene k karan bahoot bure pairnaam hue.
POS Tagging Alignment Model H Hi ic cr rn n: : sw ah mol ae s s tha m soa cj ik eo tyo k p oh py hsi yc sa icl atr la ti rn ain ing in d ge dn ea n k ee k k ea kra an a rb na bh at hb uu tr e b uc ro en es e pq au rie nn ac me s h uh eu .ye.
…
Adding Noisy Perturbation
Candidate words as per
the Inclusion list I Substitution Dictionary D H Hi ic cr r: : w wh ho ol le e ss ao mcie at jy k k oo p p hh yy ss icic aa l l t rt ara inin inin gg d d ee nn ee k k ee k k aa rara nn b b aa hh uu t t b d ui rs ea s pt aro riu ns a ac mon hs ue eq .uences hue .
समस्त - JJ समस्त ⇔ whole H H …i ic cr r: : sw ah mol ae s s tha m soa cj ieko ty p kh oy psi hc ya sl it cr aa li n trin ag in d ine gn de e k ne e k ka er a kn a rb aa nh bu at hb uu tr e b uc ro en s pe aq riu ne an ac me s h uh eu .e.
समाज - NN समाज ⇔ society
शारी  रक - JJ शारी  रक ⇔ physical Romanization / Transliteration
r = 7 प्र  शक्षण - NN प्र  शक्षण ⇔ training H Hi ic c: : w wh ho ol le e सso मc ाi जet y क क ो pो hp yh sy is ci ac la tl r t ar ia ni in ni gn g द ेनदेन े के ेक के क ारा णरण ब हब ुतहुत ब रुd े i पsa  रs णtr ाo मu s ह ुएc ।onsequences हुए।
ब बह
ुरे
ुत
-
J- JQF ब बह
ुरे
ुत ⇔
⇔
dm isa an sy
trous
H H …i ic c: : w सh मo स्l तe स soम cा iज et yक को p ो h py hs yi sc ia cl a t lr a tri an ii nn ig n gद ेन देे नक े े क क े कार ाण रण ब ह बुत हु तब रु बे रुc े o पn  रs णeq ामue हn ुएc ।es हुए।
प  रणाम - NN प  रणाम ⇔ consequence Sentence Filtering
(using Probabilistic & Deterministic metrics)
H cae nu dri is dt aic tes wto o l rim dsit ct oh me bination H H H H H …i i i i ic c c c c: : : : : w w w w wh h h h ho o o o ol l l l le e e e e स सs s so o o म मc c c ा ाi i i ज जe e et t t y y y क क क क क ो ो p pो ो ो h hp p p y yh h h s sy y y i is s s c ci i i a ac c c l la a a t tl l l r r प्र प्रt a ar i i   a n nश शi i in n nक्ष क्षi g gn ण ण g द द े ेद द न नद े ेे न नन े े क के े े े ेक कक क के ेे क कक ा ार रा ाा ण णर रर ण णण m ब mब हब a ुतहह naुु तत yn ब yd रुd ब े ii d रुs पs े iaa  s c रss a o णtt s nrr o ाto s मru eu o s qs u ह us ुएcप eo ।c  nर n oण cs n eeा sम sq e u q हह e ुएuुए n e ।। c ne cs e sह ुए हु। ए।
5 <= r < 7=
C
47 :
+
U 7Cse
5
r
+
−
7C
3
6
t +o 7r
C
c 7a =n d 6i 4d a ct oe mw bo ir nd
a
c tio om nsbinations H H …i ic c: : w सh मo स्l तe स soम cा iज et yक को p ो h py hs yi sc ia cl a t lr a tri an ii nn ig n gद ेन देे नक े े क क े कार ाण रण ब ह बुत हु तब रु बे रुc े o पn  रs णeq ामue हn ुएc ।es हुए।
Figure1: Processofcode-mixedsentencegenerationinHINMIX.
HumanEvaluation: Wereportafewsamplesof Hi: पितकीप्रेरणासेउन्हाेंनेसंस्कृतमेंिलिखतरामायणकाबांग्लामेंसंिक्षप्तरूपांतरण
HINMIX in Table 1. We can argue that the first िकया। (Patikipreranaseunhonnesanskritmenlikhitramayan
kabanglamensankshiptrupantarkiya.)
twosamplesaregoodtranslation,asbothofthem En: Atherhusband'spersuasionshetranslatedintoBengalian
arepreservingahigherdegreeofsemanticswhile abridgedversionoftheRamayanafromSanskrit.
CM: Husbandkipersuasionseunhonnesanskritmenlikhitramayan
maintaining the language syntax. We also show kabanglamenabridgedrupantarkiya.
the third example which signifies a bad transla- Hi: यहसरुक्षाप्रमाणपत्रिवश्वशनीयनहींहै।(Yehsurakshapramanpatra
tion primarily due to the wrong POS tag for the vishvashniyenahihai.)
En: Thissecuritycertificateisnottrusted.
word “khate” – the word “khate” has two common CM: Yehsecuritycertificatetrustednahihai.
senses,i.e.,account(noun)andeating(verb),and
Hi: हमखानेके बादआमखातेथे। (Humkhanekebaadaamkhatethe.)
thePOStaggermisclassifyitasanouninsteadof En: Weatemangoesafterlunch
CM: Humkhanekebaadmangoesatethe
a verb. To further assess the quality of our syn-
thetic CM sentences, we perform a human evalu-
Table1: SamplesofgeneratedCMsentences.
ation on 50 randomly selected Hinglish samples.
Three bilingual speakers proficient in English and
Hindiwereaskedtoratetheadequacyandfluency and6%shufflenoisetoHi forproducinga60%
cr
ofeachsampleona5-pointLikertscale. Theeval- word-level noisy code-mixed corpus Hi -En.
crn
uators report the average adequacy and fluency Both clean (Hi -En) and noisy (Hi -En) cor-
cr crn
scoresof4.76and4.44,respectively. poraarefurtherusedtotrainajointmodel.
Adversarial Module: The transliteration of non- Development of Gold-standard dataset: For
romanlanguagesdependsuponthephonetictran- the gold standard annotation, we take the ser-
scription of each word, varying heavily with the vice of two professional annotators – a male and
writer’s interpretation of involved languages. With a female. The annotators are proficient bilingual
noconsistentspellingofaword,itbecomescrucial speakersintheagerangeof25-35yearswiththeir
to simulate the real-world variations for the practi- first and second languages as Hindi and English,
calapplicationofanyCMTmodel. Hence,wepro- respectively. Given a Devanagari Hindi sentence,
posetoaddword-leveladversarialperturbationsto annotatorswereassignedtowritetheHinglishcon-
thetransliterationofnon-romanwordsasfollows: versionthatappearsasafirstthoughtinthemind.
• Switch: “transfer”vs“trasnfer”. Thetime-frameforcodemixconversionshouldnot
• Omission: “amazing”vs“amzng”. exceed 5 seconds once a sentence is read. As
• Proximitytypo: “mobile”vs“movile”. thereisnostandardschemeforromantranslitera-
• RandomShuffle: “laptop”vs“loptap”. tionofIndicscripts,weaskannotatorstotransliter-
We inject 30% switch, 12% omission, 12% typo, atetheDevanagariwordsaspertheirunderstand-Sentence-level Token-level Char-level
Statistics Type
#Sent #Unique CMI SPF #Hisrc #Ensrc #ENtgt Mean Median Mean Median
Train Synthetic 4.2M 0.67M 27.9 44.3 0.25M 0.11M 0.19M 100.9 88 18.24 16
Dev Gold 280 280 32.6 47 711 667 1392 65.6 64 12.17 12
Test Gold 2507 2507 32.4 45.5 4194 5923 11255 124.9 111 22.8 20
Table2: StatisticsofHINMIXcode-mixeddataset.
ing of word structure and its sound pattern. This occurrence probabilities, directly applying the tok-
way the code-mixed sentences are annotated in enization to the corpora would result in the under-
romanizedformwithnofixedspellingofanyword representationoflow-resourcelanguages. There-
–asaconsequence,samewordmaytakedifferent fore,weundersamplethehigh-resourcelanguage
representations (spelling) in different sentences. by randomly choosing a fixed set of sentences
This ensures robustness of models as such vari- fromthecorporatoobtaintheshareddictionary.
ationcanactasnaturalnoiseduringtesting. We also define an extended setup, RCMT_-
roman+devan, where we append two non-
Statistics: The detailed statistics of the syn- romanized (or devanagari) code-mixed directions
thetic and gold-standard annotated code-mixed in RCMT_roman: bidirectional Hindi-English de-
datasetsareprovidedinTable2. Weusethesyn- vanagari corpus (Hi c⇌En). This setup is moti-
thetic dataset for the training purpose, while the vatedbythefactthatthesubwordstokensofHi cr
manually annotated gold dataset is divided into a and Hi crn sentences would contain substantial
developmentsetandatestset. Intotal,thereare amountofoverlapduetothejointvocabulary. Any
4.2M,280,and2507parallelsentencepairsinthe noiseduetolexical,phonetic,ororthographicvari-
train,development,andtestsets,respectively. ationsonlyperturbsthewordatthecharacterlevel,
Wealsoevaluatethecomplexityofdatasetsus- therebyobtainingsimilarsubwordstosomeextent.
ingcodemix-specificmetricssuchasCode-Mixing Further, when translating two structurally different
Index(CMI)andSwitchPointFraction(SPF).CMI sentences(i.e.,Hi cr andHi crn versionsofasen-
measuresthepercentageofcode-mixinginasen- tence)tothesametargetlanguage,thejointmodel
tence, whereas SPF computed the percentage of would learn the relationship between those sub-
switch-pointsbetweenthematrix(i.e.,Hi)andem- wordsbyutilizingtheirsamesyntacticandseman-
bedding (i.e., En) language words in a sentence. ticproperties. Therefore,thenon-canonicalnature
We observed that both CMI and SPF of synthetic ofnoisytextwouldbenefitfromthestrongimplicit
and gold standard datasets have similar scores. supervision of clean sentences even when they
It suggests that the synthetically-generated sen- are morphologically dissimilar. Since both noisy
tences are closely aligned with the manually writ- andcleancorporafollowthesameorigin(Devana-
tenCMsentencesintermsoftheusageofEnglish gari Hindi), we additionally incorporate two non-
wordsandtheirfrequencyinaCMsentence. romanized code-mixed directions. This modifica-
tionwouldenableRCMTtobetterhandlethedepen-
dencies among Devanagari and romanized char-
4. Robust Code-Mixed Translation
acters besides minimizing the morphological am-
biguityacrosssentences.
In this section, we describe our approach for
robust translation of CM sentences to English.
To capture the context-dependent lexical varia- ArchitectureandLearningObjective: Inspired
tions between the noisy and clean corpora, we bythesuccessofmultilingualmodels,weleverage
formulate the cross-lingual translation setting to asequence-to-sequencejointlearningframework
the code-mixed scenario, referred to as Robust totranslatecode-mixedsentencestoEnglish. Un-
Code-MixedTranslation(RCMT).Forthis,wejointly like typical NMT models trained on a single lan-
train a transformer model in three directions: bidi- guage pair for one direction, the joint model con-
rectional Hindi-English clean code-mixed roman- sistsofasingleencoderandadecoderfordifferent
ized corpus (Hi ⇌En) and Hindi to English corpora (code-mixed/romanized/noisy) and direc-
cr
noisy code-mixedromanizedcorpus(Hi →En), tionsallowingthemtosimultaneouslylearnuseful
crn
where c, r, and n represent the code-mixed, ro- informationacrosslanguageboundaries. Fortrain-
manized,andnoisyversionsofadataset,respec- ing the joint model from multiple sources to multi-
tively. WetermthissetupasRCMT_roman. ple targets (many-to-many), a proxy token for the
WeemploySentencePiece3tokenizerwithaun- targetlanguageisinsertedatthebeginningofthe
igram subword model (Kudo, 2018) to generate a sourcesentence,indicatingtheintendedtargetat
vocabulary directly from the raw text. As the uni- thedecodingstage. Ahigh-levelarchitecturaldia-
grammodelcalculatessubwordsaccordingtothe gramofRCMTisillustratedinFigure2.
The joint model is trained to optimize the sum
3https://github.com/google/sentencepiece of categorical cross-entropy (CE) loss with labelRCMT Model c c+r c+r+n
( s T hr aa rn es df o dr em ce or d w eri t fh o rs h aa ll r ce od n e fin gc uo rad te ior na sn d ) B M B M B M
TFM(Vaswanietal.,2017) 9.97 39.7 10.02 36.2 9.70 37.4
<Hi cr> 2En h 1 h 2 … h n <En> e 1 e 2 … e n FCN(Gehringetal.,2017) 7.89 33.2 8.07 33.1 5.69 27.5
mT5(Xueetal.,2021) 4.27 22.6 4.28 25.9 2.80 19.5
<En> 2Hi cr e 1 e 2 … e n <Hi cr> h 1 h 2 … h n mBART(Liuetal.,2020b) 5.38 29.5 7.07 35.7 3.19 21.7
PtrGen(Guptaetal.,2020) 6.51 27.18 4.68 21.15 3.04 16.1
<Hi crn> 2En h’ 1 hh ’ n’ 2 … <En> e 1 e 2 … e n M MT TT NT(Z (h Vo au ibe ht aa vl. e, t2 a0 l1 .,9 2) 019) -- - - 8.- 48 35- .1 1 50 .9.4 24 23 88 .. 00
AdvSR(Parketal.,2020) - - 9.63 36.7 7.28 32.7
<Hi c> 2En h 1 h 2 … h n <En> e 1 e 2 … e n RCMT_roman - - 13.58 45.7 11.54 41.5
RCMT_roman+devan 13.81 46.2 13.72 45.7 11.30 40.8
<En> 2Hi c e 1 e 2 … e n <Hi c> h 1 h 2 … h n
Table3: ComparativeresultsforRCMTonHINMIX.
Src language Tgt Token Src Tokens Tgt language Tgt Tokens Here,c,r,andndenotecodemix,romanized,and
noisy version of a dataset. (B: SacreBLEU and
M: METEOR). Blank entries: The original MTT,
Figure 2: The proposed RCMT model. The sub-
MTNT, and AdvSR models were especially de-
scripts c, r, and n denote codemix, romanized,
signed for the romanized code-mixed data. More-
and noisy version of a dataset. The target token
over, MTT was specifically designed for noisy
[2T]intheencoderinputindicatestheintendedtar-
codemixedgeneration.
get language T followed by tokens in the source
language S. The target tokens are passed to the
decodersequentiallyformodeltraining.
fully convolutional network for Robust CMT task.
• mT5: Xue et al. (2021) put forward a “span-
corruption” objective to pre-train a massive mul-
smoothing (Szegedy et al., 2016) across all lan-
tilingual masked LM for sequence generation.
guage pairs. As our code-mixed datasets are
• mBART: Liu et al. (2020b) used a seq2seq
synthetically prepared by replacing words using
denoising-based autoencoder pre-trained on a
the matrix language framework (Myers-Scotton,
large common-crawl corpus. • PtrGen: Gupta
1993b), learning the model directly using the CE
etal.(2020)useaBiLSTMencoderinitializedwith
loss would tend to memorize the labels for incor-
XLMfeatureandpointergeneratortodecodesen-
rect source tokens and degrade the model perfor-
tences. •MTNT:Vaibhavetal.(2019)proposedto
mance. Therefore, we adopt label smoothing to
enhancetherobustnessofMTonthenoisytextby
trainourproposedmodel.
pre-training an LSTM model with a clean corpus
and fine-tuning it on noisy artificial data. • MTT:
5. Experiments and Results Zhou et al. (2019) presented a Multi-task Trans-
formerforrobustMTthatusesdualdecoders,one
We use a standard seq2seq Transformer model to generate the clean text and another to provide
(Vaswanietal.,2017)inallourexperimentstoen- the translation given the noisy input. • AdvSR:
sure the same number of parameters. Both en- Park et al. (2020) introduced an adversarial sub-
coder and decoder consist of a stack of 6 iden- wordregularizationschemeforon-the-flyselection
tical layers. Each layer comprises a Multi-Head of diverse subword segmentation in a sequence
Attention layer with 4 attention heads and a Feed- resulting in character-level robustness of an NMT
forwardlayerwithaninnerdimensionof1024. The model.
shared input and output embedding dimensions ToensureafaircomparisonwithRCMT,wefine
are set to 512. We use a dropout rate of 0.1, a tune each baseline on HINMIX. Moreover, since
learning rate of 5×10−4 and an Adam optimizer MTNT,MTT,andAdvSRmodelsaredesignedfor
withwarmupstepsof4000. Aunigrammodelwith robust (noisy) machine translation, we train them
charactercoverage1.0istrainedonalllanguages fromscratchonHINMIX.
to obtain a common vocabulary of size 32000. To
implementourmodel,thefairseq(Ottetal.,2019)
Results: Table 3 presents the results of our ro-
toolkit is employed. Finally, we evaluate the qual-
bustCMTexperiments. WeobservethatRCMTsig-
ityofmodelsonSacreBLEU(Ottetal.,2019)and
nificantlyoutperformsallCMandrobustMTbase-
METEOR(BanerjeeandLavie,2005)metrics.
lines. Furthermore, we observe minor decline in
results with the increase in the corpus/languages
Baselines: We conduct experiments with multi- (RCMT_roman → RCMT_roman+devan). We at-
ple CM and robust MT baselines for fair compar- tributethistothelessernumberofparametersfor
ison of our RCMT approach: • TFM: We employ each pair in a joint model when more pairs are
avanillaTransformerwiththesamehyperparame- added. Regardless, our proposed model handles
ters as RCMT for each configuration. • FCN: Fol- anall-inclusiveCMinput(Devanagari,English,ro-
lowing (Gehring et al., 2017), we adapt seq2seq manized,andnoisywords)inaneﬀicientmanner,
namor_TMCR
RCMT_roman+devthusmakingitasuitablecandidateforpracticalap- be because it uses a dual decoding scheme to
plications. Inthefollowingsubsections, weelabo- jointlymaximizecleantextandthetranslatedtext.
rateontheobtainedresultsandtheircomparisons Moreover, the AdvSR model, trained exclusively
withthebaselinesandstate-of-the-artsystems. on noisy corpus, yields better performance than
Code-mixed MT Results: Seq2Seq models the MTNT model, which is trained on clean cor-
such as transformers (TFM) and convolutional pus Hi cr→En and finetuned on the noisy corpus
attention networks (FCN) have become the de-
Hi crn→En.
facto standard to evaluate MT systems (Liu et al., In comparison, RCMT reports an average im-
2020a; Wu et al., 2019). Following their competi- provementofapprox. +1.0BLEUscorethanMTT
tive performance in code-mixed translation tasks (the best baseline model). Furthermore, RCMT_-
(Nagoudi et al., 2021; Appicharla et al., 2021; roman has lesser parameter, as compared to
Dowlagar and Mamidi, 2021), we train individ- MTT, which accounts for increased model size to
ualmodelsineachdirection(Hi →En,Hi →En, allocate parameters for the second decoder mod-
c cr
Hi →En). Table 3 shows the superior perfor- ule. On the other hand, RCMT has the capability
crn
mance of TFM over FCN with an avg. improve- to adapt to any number of pairs without increas-
mentof+2.47&+2.68BLEUacrossCM(c,c+r) ing the model size. We observed even better per-
and robust CM (c+r+n) translation models, re- formance on meteor scores, where RCMT_roman
spectively. A substantial gain of +3.31B, +7.25M (41.5) and RCMT_roman+devan (40.8) report ap-
score (on avg.) over TFM is observed on noisy prox+2.8and+3.5bettermeteorscoresthanthe
corpus (Hi →En) when it is trained simultane- bestbaseline,MTT(38.0),respectively.
crn
ously with clean corpora (Hi ⇌En) in RCMT_- Furthermore, RCMT (47.9M) is significantly
cr
roman. Furthermore, the inclusion of Devanagari lighter (in terms of number of parameters) than
CM (Hi ⇌En) in RCMT_roman+devan improves most of the comparative systems including MTT
c
CMperformance;however,itdoesnotprovidead- – AdvSR (76.9M), MTT (120.6M), FCN (152.1M),
ditional support in the robustness of the system. mT5 (300M), and mBART (680M). Only MTNT
Also, for Hi →En, RCMT shows stronger results (21.1M)andTFM(43.8M)havelesserparameters
c
thanTFMmodelevenwhenDevanagarisubwords buttheirperformancesarenotatparwithRCMT.
arenotsharedwithanyotherpair. Wehypothesize
that training on a common target En enables the GeneralizabilityofRCMT: Tofurthersolidifythe
encoder to learn overlapping representations for robustnessofourRCMTmodels, weemploythree
allinputs(Hi c,Hi cr,Hi crn), therebyreducingthe additional MT datasets: LinCE(Aguilar et al.,
effect of script variation and reinforcing the same 2020), SpokenTutorial (Gupta et al., 2021), and
familycorrelation. IITB Hi-En (Kunchukuttan et al., 2018) datasets.
PreviousworksinCMThaveprimarilyreliedon LinCE and SpokenTutorial datasets contain code-
large-scale multilingual models such as mBART mixed sentences, whereas, IITB is a non-CM
and mT5 (Xue et al., 2021; Liu et al., 2020b; Hindi-English dataset. Moreover, LinCE contains
Gautam et al., 2021; Jawahar et al., 2021). For real-world noisy tweets collected from Twitter, a
comparison, we adopt the existing approach by suitable candidate to assess robustness of the
finetuning mT5 and mBART models on our CM model. We evaluate our trained RCMT models
datasets. Table3(row-3androw-4)highlightsthe on the test sets of these three dataset. As seen
CMperformanceonthesefinetunedmodels. Sur- in Table 4, our models obtain better performance
prisingly, the romanized code-mixed MT (c+r) across all datasets with avg. BLEU and Meteor
demonstratescomparableavg. meteorscorewith scoresof14.17and42.08,respectively. OnLinCE,
+1.35%improvementoveritsDevanagaricounter- RCMT models yield comparatively lower scores,
part (c), even though the romanized Hindi text is possiblyduetothehigherpercentageofnoiseand
seenonlyduringfinetuning. ConclusivelyfromTa- thepresenceofinformaltokens(emoticons,hash-
ble 3, these transfer learning approaches still lag tags,etc.). Also,ourmodelisabletotranslatenon-
behindRCMT,especiallyinrobustCMTasthepre- CM text with comparable performance as that of
trained procedure did not involve any kind of CM CMtranslations. TheseresultsindicatethatRCMT
data. However,itgivesusadirectiontoexploreby performsgoodonunseendatasetsaswell.
includingCMdatainthepre-trainingsteps.
Robust MT Results: In order to corroborate Zero-shot Code-mixed MT (ZCMT): Develop-
the robustness capabilities of RCMT models, we ment of a code-mixed parallel corpus for a new
test three noise-robust MT models as baselines: language pair (e.g., Bengalish ⇌ English) is non-
MTT, MTNT, and AdvSR. MTT proves to be most trivialduetovariouschallenges(PoStagger,align-
resilient to synthetic noise among other robust ment model, etc.). Therefore, to negate the lim-
baselines with an average BLEU score of 10.44 itation of data scarcity, we propose a zero-shot
against5.92ofMTNTand7.28ofAdvSR.Itcould transfer learning approach for code-mix transla-RCMT_roman RCMT_roman+devan Hindi Bangla
Datasets
Model
B M B M B M B M
IITB(non-CM) 12.25 40.8 12.75 40.9 MMT − 13.59 45.0 15.66 47.7
SpokenTutorial(CM) 22.58 52.1 23.07 52.5 r 13.05 44.1 13.83 44.3
LinCE(CM) 11.06 33.9 10.28 33.5 c 14.00 46.7 15.41 49.8
ZCMT
HINMIX(CM) 13.58 45.7 13.72 45.7 c+r 13.69 46.1 14.01 47.6
Table 4: Comparison of trained (c+r) RCMT Table 5: Comparative performance of RCMT in
modelsonotherCMandnon-CMcorpus. a zero-shot setting (ZCMT). Training: Bengali-
English (Bn→En) and code-mixed Hindi-English
(Hi →En). Testing: Code-mixed Bengali-
cr
tion in a new language pair. In this approach, we English(Bn →En,Bn →En).
c cr
use the previously generated CM corpora to ex-
ploit the transfer learning characteristic of cross- Source(Hicr): Isthoughtkosabhiplacesparsupportnahinmila.
lingual models for CMT in an unseen pair. The Target(En): Theconceptisnotauniversalhit.
RCMT_roman Thisthoughtdidnotsupportatalltheplaces.
idea is to utilize the existing non-CM parallel cor- Source(Hicr) Yahaapkerelativesaurlovedoneskeliyeekcompletegifthai.
Target(En) Itisperfectgiftforyourrelativesandlovedones.
pus of language l and a CM parallel corpus of
1 RCMT_roman Thisisacompletegiftforyourrelativesandlovedones
language l for the translation of CM sentences
2
of l 1. To this end, we train RCMT with Bengali- Table 6: Sample translation of code-mixed (Hi cr)
English (Bn-En) and Hinglish-English (Hi -En) sentences to English (En) produced by the pro-
cr
parallelcorpora. Subsequently,thetrainedmodel posedRCMT_romanmodel.
is employed to convert a code-mixed Bengali
(Bn ,Bn )sentencetoEnglish. Wearguethatthe
c cr
vanagariandromanizedtextsisthatitoutperforms
trained model would be able to transfer the code-
RCMT_romanandRCMT_roman+devanscoresin
mixing behaviour onto the network activations in
Table3. Thisindicatesthataddinglanguagesfrom
azero-shotway. WechooseBengali(Bn)dueto
the same family (Indo-Aryan) can sometimes im-
theavailabilityofbothBn-Enlargeparallel-corpora
prove the code-mixed translation quality despite
(Hasanetal.,2020)andBengalicode-mixedSpo-
varyingscripts(Devanagarivs. Eastern-Nagari).
kenTutorialdatasetBn -En(Guptaetal.,2021)–it
c
consistsof28Kutterancestranscribedfromcode-
mixedvideolectures. Werandomlyselect500and Qualitative Analysis: Table 6 shows the out-
2000 sentences as the dev and test sets, respec- puts of a few samples generated through RCMT_-
tively. TheZCMTissummarizedasfollows: roman. WeobservedthatRCMT_romanlearnsto
match the words in source and target sentences
• Training: Code-mixed Hindi to English [De-
vanagari (Hi ⇌En), romanized (Hi ⇌En), –word“thought”and“complete”aretranslatedas
c cr
noisy romanized (Hi →En)] + Bengali to En- it is from the source sentences to the generated
crn
glish [Eastern-Nagari (Bn⇌En) and romanized sentences. Inthefirstexample,weencounterdis-
(Bn ⇌En)]. tortedsemanticsinthetranslatedtext,whereas,a
r
higherdegreeofsemanticcontentispreservedin
• Testing: Code-mixed Bengali to English
[Bn →En,Bn →En] the second example, albeit missing a more suit-
c cr
able word “perfect”. In general, we observed that
fluencyandadequacyofthegeneratedsentences
Results: Table5showstheeffectivenessofzero-
shot CM translation ({Bn ,Bn }→En) by train- are encouraging; however, the usage of related
c cr
ing a joint model using a bilingual Bn-En cor- or synonym words against the expected words in
pus and our synthetic code-mixed Hi-En cor- the generated translation (e.g., “perfect” vs “com-
pus. For the baseline model, we test Bn , and plete”, “concept” vs “thought”) poses a challenge
c
Bn r code-mixed translation without training on
forRCMT.
c
CM text in a multilingual manner (MMT), i.e.,
{Hi,Hi r,Bn,Bn r}⇌En+Hi rn→En. Interestingly, 6. Conclusion
MMT demonstrates appreciable performance on
the Bn test set; however, ZCMT obtains +3.25 In this work, we proposed a two-phase strategy
improvement on METEOR scores over the MMT to translate the real-world code-mixed sentences
model. Apossiblereasonforthiscanbethenature in multiple languages to English. First, a linguisti-
oftheSpokenTutorial(Guptaetal.,2021)testset, callyinformedpipelinewasintroducedtogenerate
whichmostlycontainstechnicalwordsandproper alarge-scaleHINMIXcode-mixedcorporasynthet-
nounsasEnglish(L e)wordsinBengali(L m)code- ically using a bilingual Hindi-English parallel cor-
mixedtext. pus. Next,wecreatedaperturbedcorpusbypass-
Another surprising benefit of our ZCMT model ingthecleancode-mixedcorpustoanadversarial
is observed in Hindi CM translation in both De- module–bothofwhicharesimultaneouslytrainedin a joint learning mechanism to learn robust CM References
representations. Finally, we showed the effective-
ness of zero-shot learning on code-mixed MT in GustavoAguilar,SudiptaKar,andThamarSolorio.
Bengali language. Our evaluation showed satis- 2020. LinCE: A Centralized Benchmark for
fying performance for both robust Hindi CM and Linguistic Code-switching Evaluation. In Pro-
zero-shotBengaliCMtranslation. ceedingsofThe12thLanguageResourcesand
In the future, we would like to extend our work EvaluationConference,pages1803–1813,Mar-
tomultipleworldwidecode-mixedlanguagesusing seille, France. European Language Resources
supervised and unsupervised methods. Addition- Association.
ally, we plan to handle the real-world noise in so-
cialmediacode-mixedtextstofurtherimprovethe Ramakrishna Appicharla, Kamal Kumar Gupta,
robustnessofthesystem. Asif Ekbal, and Pushpak Bhattacharyya. 2021.
IITP-MT at CALCS2021: English to Hinglish
neural machine translation using unsupervised
Limitation:
synthetic code-mixed parallel corpus. In Pro-
ceedings of the Fifth Workshop on Computa-
Syntheticdatagenerationalwaysinstillsaconcern
tionalApproachestoLinguisticCode-Switching,
regarding the quality of the synthesized sample;
pages31–35,Online.AssociationforComputa-
however, at the same time, it enables us to gen-
tionalLinguistics.
erateawlargeamountofsamplesinaquicktime.
Though we did our best to maintain the quality of
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
thedatasetinHINMIX,therearefewcasesofbad
Bengio. 2015. Neural machine translation by
translation mainly because of the following rea-
jointly learning to align and translate. In Pro-
sons:
ceedings of the 3rd International Conference
• Alignment Errors: Despite the context-
onLearningRepresentations,ICLR,SanDiego,
dependent word substitution in HINMIX, it is
CA,US.
susceptible to all the alignment errors. Incor-
rect word mapping between the source-target Satanjeev Banerjee and Alon Lavie. 2005. ME-
could completely alter its CM meaning. Also, TEOR: An automatic metric for MT evalua-
we substitute words with an only one-to-one tion with improved correlation with human judg-
correspondencebetweenthesourceandtarget, ments. In Proceedings of the ACL Workshop
thereby abandoning all words with multiple on Intrinsic and Extrinsic Evaluation Measures
alignment mapping may have caused issue in for Machine Translation and/or Summarization,
appropriatetranslationinsomecases. pages65–72,AnnArbor,Michigan.Association
• POSTaggingErrors: AgoodPOStaggerforms forComputationalLinguistics.
thebasisofourcode-mixeddatasetcreationpro-
cess. In cases a word in the source sentence Yonatan Belinkov and Yonatan Bisk. 2018. Syn-
is incorrectly tagged to a tag in POS inclusion thetic and natural noise both break neural ma-
list I, its substitute word will not be appropriate. chine translation. In 6th International Confer-
For example in Table 1, the verb “khate” gets ence on Learning Representations, ICLR 2018,
mistaggedtoanoun,therebybeingreplacedby Vancouver, BC, Canada, April 30 - May 3,
itstranslation“ate”. 2018,ConferenceTrackProceedings.OpenRe-
view.net.
Language Resource References
GraemeBlackwood,MiguelBallesteros,andTodd
Ward. 2018. Multilingual neural machine trans-
• Kunchukuttanetal.(2018): IITBombayHindi-
lationwithtask-specificattention. arXivpreprint
Englishparallelcorpus.
arXiv:1806.03280.
• Hasan et al. (2020): Bengali-English parallel
corpus.
Ondřej Bojar, Christian Buck, Christian Feder-
• Gupta et al. (2021): SpokenTutorial parallel
mann,BarryHaddow,PhilippKoehn,Johannes
corpus.
Leveling, Christof Monz, Pavel Pecina, Matt
• Aguilaretal.(2020): LinCEparallelcorpus.
Post, Herve Saint-Amand, Radu Soricut, Lucia
Specia, and Aleš Tamchyna. 2014. Findings of
Acknowledgment the2014workshoponstatisticalmachinetrans-
lation. In Proceedings of the Ninth Workshop
Md Shad Akhtar would like to acknowledge the onStatisticalMachineTranslation,pages12–58,
support of SERB-CRG grant and Infosys founda- Baltimore,Maryland,USA.AssociationforCom-
tionthroughCenterofAI(CAI)-IIITDelhi. putationalLinguistics.Yong Cheng, Lu Jiang, Wolfgang Macherey, and Suman Dowlagar and Radhika Mamidi. 2021.
JacobEisenstein.2020. AdvAug: Robustadver- Gated convolutional sequence to sequence
sarial augmentation for neural machine transla- based learning for English-hingilsh code-
tion. InProceedingsofthe58thAnnualMeeting switched machine translation. In Proceedings
oftheAssociationforComputationalLinguistics, of the Fifth Workshop on Computational Ap-
pages5961–5970,Online.AssociationforCom- proaches to Linguistic Code-Switching, pages
putationalLinguistics. 26–30, Online. Association for Computational
Linguistics.
Yong Cheng, Zhaopeng Tu, Fandong Meng, Jun-
LuisaDuran.1994.Towardabetterunderstanding
jie Zhai, and Yang Liu. 2018. Towards robust
ofcodeswitchingandinterlanguageinbilingual-
neural machine translation. In Proceedings of
ity: Implications for bilingual instruction. The
the 56th Annual Meeting of the Association for
journalofeducationalissuesoflanguageminor-
Computational Linguistics (Volume 1: Long Pa-
itystudents,14(2):69–88.
pers), pages 1756–1766, Melbourne, Australia.
AssociationforComputationalLinguistics.
ChrisDyer,VictorChahuneau,andNoahA.Smith.
2013. Asimple,fast,andeffectivereparameter-
S. T. Chung and R. L. Morris. 1978. Isolation
ization of IBM model 2. In Proceedings of the
and characterization of plasmid deoxyribonu-
2013 Conference of the North American Chap-
cleicacidfromstreptomycesfradiae. Paperpre-
teroftheAssociationforComputationalLinguis-
sented at the 3rd international symposium on
tics: Human Language Technologies, pages
the genetics of industrial microorganisms, Uni-
644–648,Atlanta,Georgia.AssociationforCom-
versityofWisconsin,Madison,4–9June1978.
putationalLinguistics.
Alexis Conneau and Guillaume Lample. 2019. BjörnGambäckandAmitavaDas.2016. Compar-
Cross-lingual language model pretraining. Ad- ing the level of code-switching in corpora. In
vances in Neural Information Processing Sys- Proceedings of the Tenth International Confer-
tems,32:7059–7069. ence on Language Resources and Evaluation
(LREC’16), pages 1850–1855, Portorož, Slove-
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and nia. European Language Resources Associa-
KristinaToutanova.2019. BERT:Pre-trainingof tion(ELRA).
deepbidirectionaltransformersforlanguageun-
derstanding. InProceedingsofthe2019Confer- PenelopeGardner-ChlorosandMalcolmEdwards.
ence of the North American Chapter of the As- 2004. Assumptions behind grammatical ap-
sociationforComputationalLinguistics: Human proachestocode-switching: whentheblueprint
Language Technologies, Volume 1 (Long and isaredherring. TransactionsofthePhilological
Short Papers), pages 4171–4186, Minneapolis, Society,102(1):103–129.
Minnesota. Association for Computational Lin-
Devansh Gautam, Prashant Kodali, Kshitij Gupta,
guistics.
Anmol Goel, Manish Shrivastava, and Ponnu-
rangam Kumaraguru. 2021. CoMeT: Towards
MrinalDhar,VaibhavKumar,andManishShrivas-
code-mixed translation using parallel monolin-
tava. 2018. Enabling code-mixed translation:
gual sentences. In Proceedings of the Fifth
Parallel corpus creation and MT augmentation
WorkshoponComputationalApproachestoLin-
approach. InProceedingsoftheFirstWorkshop
guistic Code-Switching, pages 47–55, Online.
on Linguistic Resources for Natural Language
AssociationforComputationalLinguistics.
Processing, pages 131–140, Santa Fe, New
Mexico,USA.AssociationforComputationalLin-
Jonas Gehring, Michael Auli, David Grangier, De-
guistics.
nis Yarats, and Yann N Dauphin. 2017. Convo-
lutionalsequencetosequencelearning. InInter-
AnujDiwan,RakeshVaideeswaran,SanketShah,
nationalconferenceonmachinelearning,pages
Ankita Singh, Srinivasa Raghavan, Shreya
1243–1252.PMLR.
Khare, Vinit Unni, Saurabh Vyas, Akash Ra-
jpuria, Chiranjeevi Yarra, Ashish Mittal, Pras- AbhirutGupta,AdityaVavre,andSunitaSarawagi.
anta Kumar Ghosh, Preethi Jyothi, Kalika Bali, 2021. Training data augmentation for code-
Vivek Seshadri, Sunayana Sitaram, Samarth mixed translation. In Proceedings of the 2021
Bharadwaj, Jai Nanavati, Raoul Nanavati, Conference of the North American Chapter of
Karthik Sankaranarayanan, Tejaswi Seeram, the Association for Computational Linguistics:
andBasilAbraham.2021.Multilingualandcode- Human Language Technologies, pages 5760–
switchingasrchallengesforlowresourceindian 5766,Online.AssociationforComputationalLin-
languages. guistics.Deepak Gupta, Asif Ekbal, and Pushpak Bhat- VladimirKarpukhin,OmerLevy,JacobEisenstein,
tacharyya. 2020. A semi-supervised approach and Marjan Ghazvininejad. 2019. Training on
to generate the code-mixed text using pre- synthetic noise improves robustness to natural
trained encoder and transfer learning. In Find- noise in machine translation. In Proceedings
ings of the Association for Computational Lin- of the 5th Workshop on Noisy User-generated
guistics: EMNLP 2020, pages 2267–2280, On- Text (W-NUT 2019), pages 42–47, Hong Kong,
line.AssociationforComputationalLinguistics. China. Association for Computational Linguis-
tics.
Tahmid Hasan, Abhik Bhattacharjee, Kazi Samin,
Masum Hasan, Madhusudan Basak, M. Sohel Judith F. Kroll, Susan C. Bobb, Maya Misra, and
Rahman, and Rifat Shahriyar. 2020. Not low- TaomeiGuo.2008. Languageselectioninbilin-
resource anymore: Aligner ensembling, batch gualspeech: Evidenceforinhibitoryprocesses.
filtering, and new datasets for Bengali-English Acta Psychologica, 128(3):416–430. Bilingual-
machinetranslation.InProceedingsofthe2020 ism: Functionalandneuralperspectives.
Conference on Empirical Methods in Natural
TakuKudo.2018.Subwordregularization: Improv-
Language Processing (EMNLP), pages 2612–
ing neural network translation models with mul-
2623,Online.AssociationforComputationalLin-
tiplesubwordcandidates. InProceedingsofthe
guistics.
56thAnnualMeetingoftheAssociationforCom-
Anupam Jamatia, Björn Gambäck, and Amitava putationalLinguistics(Volume1: LongPapers),
Das. 2016. Collecting and annotating indian pages66–75,Melbourne,Australia.Association
social media code-mixed corpora. In Interna- forComputationalLinguistics.
tionalConferenceonIntelligentTextProcessing
Anoop Kunchukuttan, Pratik Mehta, and Push-
andComputationalLinguistics,pages406–417.
pak Bhattacharyya. 2018. The IIT Bombay
Springer.
English-Hindi parallel corpus. In Proceedings
of the Eleventh International Conference on
Ganesh Jawahar, El Moatez Billah Nagoudi,
Language Resources and Evaluation (LREC
Muhammad Abdul-Mageed, and Laks Laksh-
2018), Miyazaki, Japan. European Language
manan, V.S. 2021. Exploring text-to-text trans-
ResourcesAssociation(ELRA).
formers for English to Hinglish machine trans-
lation with synthetic code-mixing. In Proceed-
Chin-YewLin.2004. ROUGE:Apackageforauto-
ingsoftheFifthWorkshoponComputationalAp-
maticevaluationofsummaries. InTextSumma-
proaches to Linguistic Code-Switching, pages
rizationBranchesOut,pages74–81,Barcelona,
36–46, Online. Association for Computational
Spain. Association for Computational Linguis-
Linguistics.
tics.
BaijunJi,ZhiruiZhang,XiangyuDuan,MinZhang,
XiaodongLiu,KevinDuh,LiyuanLiu,andJianfeng
Boxing Chen, and Weihua Luo. 2020. Cross-
Gao.2020a. Verydeeptransformersforneural
lingual pre-training based transfer for zero-shot
machinetranslation. CoRR,abs/2008.07772.
neural machine translation. In Proceedings of
the AAAI Conference on Artificial Intelligence, Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li,
volume34,pages115–122. Sergey Edunov, Marjan Ghazvininejad, Mike
Lewis, and Luke Zettlemoyer. 2020b. Multilin-
Melvin Johnson, Mike Schuster, Quoc V Le, gual denoising pre-training for neural machine
Maxim Krikun, Yonghui Wu, Zhifeng Chen, translation. Transactions of the Association for
Nikhil Thorat, Fernanda Viégas, Martin Watten- ComputationalLinguistics,8:726–742.
berg,GregCorrado,etal.2017. Google’smulti-
lingual neural machine translation system: En- Ne Luo, Dongwei Jiang, Shuaijiang Zhao, Caixia
abling zero-shot translation. Transactions of Gong, Wei Zou, and Xiangang Li. 2018. To-
the Association for Computational Linguistics, wardsend-to-endcode-switchingspeechrecog-
5:339–351. nition. CoRR,abs/1810.13091.
Aravind K. Joshi. 1982. Processing of sentences Thang Luong, Ilya Sutskever, Quoc Le, Oriol
with intra-sentential code-switching. In Coling Vinyals,andWojciechZaremba.2015.Address-
1982: Proceedings of the Ninth International ing the rare word problem in neural machine
ConferenceonComputationalLinguistics. translation. In Proceedings of the 53rd Annual
Meeting of the Association for Computational
SarvnazKarimi,FalkScholer,andAndrewTurpin. Linguistics and the 7th International Joint Con-
2011. Machine transliteration survey. ACM ference on Natural Language Processing (Vol-
Comput.Surv.,43(3). ume 1: Long Papers), pages 11–19, Beijing,China. Association for Computational Linguis- (online). International Committee for Computa-
tics. tionalLinguistics.
Carol Myers-Scotton. 1993a. Common and un- CarolW.Pfaff.1979.Constraintsonlanguagemix-
commonground: Socialandstructuralfactorsin ing: Intrasentential code-switching and borrow-
codeswitching.LanguageinSociety,22(4):475– ing in spanish/english. Language, 55(2):291–
503. 318.
Carol Myers-Scotton. 1993b. Duelling languages: Aleksandra Piktus, Necati Bora Edizel, Piotr Bo-
Grammaticalstructureincodeswitching. janowski,EdouardGrave,RuiFerreira,andFab-
rizio Silvestri. 2019. Misspelling oblivious word
ElMoatezBillahNagoudi,AbdelRahimElmadany,
embeddings. In Proceedings of the 2019 Con-
and Muhammad Abdul-Mageed. 2021. Inves-
ferenceoftheNorthAmericanChapteroftheAs-
tigating code-mixed Modern Standard Arabic-
sociationforComputationalLinguistics: Human
EgyptiantoEnglishmachinetranslation. InPro-
Language Technologies, Volume 1 (Long and
ceedings of the Fifth Workshop on Computa-
Short Papers), pages 3226–3234, Minneapolis,
tionalApproachestoLinguisticCode-Switching,
Minnesota. Association for Computational Lin-
pages56–64,Online.AssociationforComputa-
guistics.
tionalLinguistics.
ShanaPoplack.1978. Syntacticstructureandso-
Yurii E Nesterov. 1983. A method for solving the
cial function of code-switching, volume 2. Cen-
convexprogrammingproblemwithconvergence
trodeEstudiosPuertorriqueños,[CityUniversity
rateo(1/k^2). InDokl.akad.naukSssr,volume
ofNewYork].
269,pages543–547.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Matt Post. 2018. A call for clarity in reporting
Fan, Sam Gross, Nathan Ng, David Grangier, BLEUscores. InProceedingsoftheThirdCon-
and Michael Auli. 2019. fairseq: A fast, ex- ference on Machine Translation: Research Pa-
tensible toolkit for sequence modeling. In Pro- pers,pages186–191,Brussels,Belgium.Asso-
ceedings of the 2019 Conference of the North ciationforComputationalLinguistics.
American Chapter of the Association for Com-
AdithyaPratapa,GayatriBhat,MonojitChoudhury,
putational Linguistics (Demonstrations), pages
Sunayana Sitaram, Sandipan Dandapat, and
48–53,Minneapolis,Minnesota.Associationfor
KalikaBali.2018. Languagemodelingforcode-
ComputationalLinguistics.
mixing: The role of linguistic theory based syn-
KishorePapineni,SalimRoukos,ToddWard,and thetic data. In Proceedings of the 56th Annual
Wei-Jing Zhu. 2002. Bleu: a method for auto- Meeting of the Association for Computational
maticevaluationofmachinetranslation. InPro- Linguistics (Volume 1: Long Papers), pages
ceedings of the 40th annual meeting of the As- 1543–1553, Melbourne, Australia. Association
sociation for Computational Linguistics, pages forComputationalLinguistics.
311–318.
AlecRadford,KarthikNarasimhan,TimSalimans,
Jungsoo Park, Mujeen Sung, Jinhyuk Lee, and and Ilya Sutskever. 2018. Improving language
Jaewoo Kang. 2020. Adversarial subword reg- understandingbygenerativepre-training.
ularizationforrobustneuralmachinetranslation.
InFindingsoftheAssociationforComputational Mohd Sanad Zaki Rizvi, Anirudh Srinivasan,
Linguistics: EMNLP 2020, pages 1945–1953, Tanuja Ganu, Monojit Choudhury, and
Online. Association for Computational Linguis- Sunayana Sitaram. 2021. GCM: A toolkit
tics. for generating synthetic code-mixed text. In
Proceedings of the 16th Conference of the
PeymanPassban,PuneethS.M.Saladi,andQun European Chapter of the Association for
Liu. 2020. Revisiting robust neural machine Computational Linguistics: System Demonstra-
translation: A transformer case study. CoRR, tions, pages 205–211, Online. Association for
abs/2012.15710. ComputationalLinguistics.
Parth Patwa, Gustavo Aguilar, Sudipta Kar, Suraj
David Sankoff. 1998. A formal production-based
Pandey, Srinivas PYKL, Björn Gambäck, Tan-
explanationofthefactsofcode-switching. Bilin-
moyChakraborty,ThamarSolorio,andAmitava
gualism: languageandcognition,1(1):39–50.
Das. 2020. SemEval-2020 task 9: Overview
of sentiment analysis of code-mixed tweets. In Motoki Sato, Jun Suzuki, and Shun Kiyono. 2019.
ProceedingsoftheFourteenthWorkshoponSe- Effective adversarial regularization for neural
mantic Evaluation, pages 774–790, Barcelona machinetranslation. InProceedingsofthe57thAnnual Meeting of the Association for Compu- Genta Indra Winata, Andrea Madotto, Chien-
tational Linguistics, pages 204–210, Florence, Sheng Wu, and Pascale Fung. 2018. Code-
Italy.AssociationforComputationalLinguistics. switching language modeling using syntax-
aware multi-task learning. In Proceedings
AbigailSee,PeterJ.Liu,andChristopherD.Man-
of the Third Workshop on Computational Ap-
ning.2017.Gettothepoint: Summarizationwith
proaches to Linguistic Code-Switching, pages
pointer-generator networks. In Proceedings of
62–67, Melbourne, Australia. Association for
the 55th Annual Meeting of the Association for
ComputationalLinguistics.
Computational Linguistics (Volume 1: Long Pa-
pers), pages 1073–1083, Vancouver, Canada. Genta Indra Winata, Andrea Madotto, Chien-
AssociationforComputationalLinguistics. Sheng Wu, and Pascale Fung. 2019. Code-
switched language models using neural based
Rajendra Singh. 1985. Grammatical constraints
synthetic data from parallel sentences. arXiv
on code-mixing: Evidence from hindi-english.
preprintarXiv:1909.08582.
Canadian Journal of Linguistics/Revue canadi-
ennedelinguistique,30(1):33–45. Felix Wu, Angela Fan, Alexei Baevski, Yann
Dauphin,andMichaelAuli.2019.Paylessatten-
Sunayana Sitaram, Khyathi Raghavi Chandu,
tion with lightweight and dynamic convolutions.
Sai Krishna Rallabandi, and Alan W. Black.
In International Conference on Learning Repre-
2019. A survey of code-switched speech and
sentations.
languageprocessing. CoRR,abs/1904.00784.
Linting Xue, Noah Constant, Adam Roberts, Mi-
VivekSrivastavaandMayankSingh.2020. Phinc:
hir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya
a parallel hinglish social media code-mixed cor-
Barua, and Colin Raffel. 2021. mT5: A mas-
pus for machine translation. arXiv preprint
sively multilingual pre-trained text-to-text trans-
arXiv:2004.09447.
former. InProceedingsofthe2021Conference
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. of the North American Chapter of the Associa-
2014. Sequencetosequencelearningwithneu- tion for Computational Linguistics: Human Lan-
ral networks. In Proceedings of the 27th Inter- guage Technologies, pages 483–498, Online.
national Conference on Neural Information Pro- AssociationforComputationalLinguistics.
cessing Systems - Volume 2, NIPS’14, page
Shuyan Zhou, Xiangkai Zeng, Yingqi Zhou, An-
3104–3112,Cambridge,MA,USA.MITPress.
tonios Anastasopoulos, and Graham Neubig.
C.Szegedy,V.Vanhoucke,S.Ioffe,J.Shlens,and 2019. Improving robustness of neural machine
Z. Wojna. 2016. Rethinking the inception archi- translation with multi-task learning. In Proceed-
tecture for computer vision. In 2016 IEEE Con- ingsoftheFourthConferenceonMachineTrans-
ferenceonComputerVisionandPatternRecog- lation (Volume 2: Shared Task Papers, Day 1),
nition(CVPR),pages2818–2826,LosAlamitos, pages 565–571, Florence, Italy. Association for
CA,USA.IEEEComputerSociety. ComputationalLinguistics.
Vaibhav Vaibhav, Sumeet Singh, Craig Stewart,
and Graham Neubig. 2019. Improving ro-
bustness of machine translation with synthetic
noise. In Proceedings of the 2019 Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Lan-
guageTechnologies,Volume1(LongandShort
Papers), pages 1916–1920, Minneapolis, Min-
nesota. Association for Computational Linguis-
tics.
Ashish Vaswani, Noam Shazeer, Niki Parmar,
Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
undefinedukasz Kaiser, and Illia Polosukhin.
2017. Attentionisallyouneed. InProceedings
of the 31st International Conference on Neu-
ral Information Processing Systems, NIPS’17,
page6000–6010,RedHook,NY,USA.
ShivendraKVerma.1976. Code-switching: Hindi-
english. Lingua,38(2):153–165.