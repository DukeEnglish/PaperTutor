ProCQA: A Large-scale Community-based Programming Question
Answering Dataset for Code Search
Zehan Li1,2, Jianfei Zhang3, Chuantao Yin2, Yuanxin Ouyang3, Wenge Rong1,3
1StateKeyLaboratoryofComplex&CriticalSoftwareEnvironment,BeihangUniversity,China
2Sino-FrenchEngineerSchool,BeihangUniversity,China
3SchoolofComputerScienceandEngineering,BeihangUniversity,China
{lizehan,zhangjf,chuantao.yin,oyyx,w.rong}@buaa.edu.cn
Abstract
Retrieval-basedcodequestionansweringseekstomatchuserqueriesinnaturallanguagetorelevantcodesnippets.
Previousapproachestypicallyrelyonpretrainingmodelsusingcraftedbi-modalanduni-modaldatasetstoaligntext
andcoderepresentations. Inthispaper, weintroduceProCQA,alarge-scaleprogrammingquestionanswering
datasetextractedfromtheStackOverflowcommunity,offeringnaturallystructuredmixed-modalQApairs. Tovalidate
its effectiveness, we propose a modality-agnostic contrastive pre-training approach to improve the alignment of
text and code representations of current code language models. Compared to previous models that primarily
employbimodalandunimodalpairsextractedfromCodeSearchNetforpre-training,ourmodelexhibitssignificant
performanceimprovementsacrossawiderangeofcoderetrievalbenchmarks.
Keywords:CodeQADataset,CodeSearch,ContrastivePretraining
1. Introduction
Code
Query Doc
Text
Align
CodeQuestionAnswering(CodeQA)representsa
pivotalresearchareainsoftwareintelligence. One Uni-modal
populartaskformulationisretrieval-basedQA(Gu
et al., 2018), in which the primary objective is to
Bi-modal
effectivelymatchuserqueriesexpressedinnatural
languagetorelevantcodesnippetsfromanexist-
ing corpus. The prevailing approach for retrieval- Mixed-modal
based Code QA has been the utilization of dual-
encoder-basedrepresentationmodels. Thecore
Figure1: Illustrationofdifferentdataformatsused
idea underlying this approach is to map natural
forcontrastiverepresentationalignment. Colorrep-
languagequeriesandcodesnippetsintoashared
resents chunk modality. Unimodal data focuses
representationspace,wherecloselylocatedvec-
oncode-to-codematching,whilebimodaldataem-
torscorrespondtosemanticallysimilarmeanings.
phasizescross-modalmatching. Themixed-modal
To learn a shared representation space for
datainProCQAenablessimultaneouslearningof
text and code, early research efforts adopted
allmatchingpatterns.
masked language modeling (MLM) objective on
pairedtext-codedatasettoaligndifferentmodali-
ties(Kanadeetal.,2020;Fengetal.,2020),similar
to the monolingual and cross-lingual pre-training ent modalities. In Figure 1, we illustrate different
approaches(Devlinetal.,2019;ConneauandLam- dataformatsusedforcontrastivepre-trainingand
ple, 2019). Subsequent work discovered the po- analysetheirchunk-levelmatchingpatterns. Uni-
tentialofcontrastivepre-training,andapplieditto modal data offers code-code matching patterns,
coderepresentationlearningbyconstructinglarge- whereasbi-modaldataimpliescode-textmatching
scale paired datasets (Jain et al., 2021; Li et al., patterns. Whileacombinationofbothdatatypes
2022). duringpre-trainingcanenablemodelstolearnboth
matchingsignals,amoredata-efficientapproach
Currentcontrastivecoderepresentationlearning
tocaptureallmatchingpatternsisthroughmixed-
methods such as CodeRetriever (Li et al., 2022)
modaldata.
typically rely on curated uni-modal (code-code
pairs)orbi-modaldata(text-codepairs). Fewwork Besides,themajorityofcodeembeddingmod-
evenusesdistinctencoderfortextandcode(Hey- els(Jainetal.,2021;Wangetal.,2021a;Lietal.,
manandCutsem,2020;Salzaetal.,2023). Such 2022)haveprimarilyreliedonCodeSearchNet(Hu-
pre-training design emphasizes the concept of sain et al., 2019) as the main pre-training cor-
modalitydistinction,divergingfromthegoalofes- pus. WhileCodeSearchNetisavaluableresource,
tablishingaunifiedrepresentationspacefordiffer- itssizeanddatadistributionhaveinherentlimita-
4202
raM
52
]LC.sc[
1v20761.3042:viXraQuestion Segmentationfaultwhilecopyingastringtothememoryallocatedarray
Description FollowingisaprogramIampracticing;intmain()
int i = 0; char **grid = (char **) malloc(5*sizeof(int)); for (i = 0 ; i < 5 ; i++) grid[i] =
(char*)malloc(6);strcpy(grid[0],"eabcd");strcpy(grid[1],"fghij");strcpy(grid[2],"olkmn");
strcpy(grid[3],"trpqs");strcpy(grid[4],"xywuv");/*SegmentationFaultatthisline*/return
0; Iamgettingasegmentationfaultatthelinestrcpy(grid[4],"xywuv"); . Whatcould
be the reason? I have allocated the array to have 5 strings(rows) of 6 characters
each(columns).
Answer Youareallocatingthewrongtypeatline3char**grid=(char**)malloc(5*sizeof(int));
Should be char **grid = (char **) malloc(5*sizeof(char*)); This is because you are
declaringstring-array. Therefore,themallocshouldbechar*(string/characterpointer)
Alsothesameifyouweretryingtodeclare2-Dintegerarray. Itwillbeint**grid=(int**)
malloc(5*sizeof(int*));
Table1: AnexamplesampledfromtheCprogramminglanguagesubsetofProCQA.Textandcodeare
interleavedintheseQApairs.
tions that may impact the quality and diversity of codelanguagemodels, MACP achievessubstan-
learned code representations. Recent work has tial improvements on most tasks we considered,
proposedtocuratelarge-scalecodedatasetsfrom advancingthepreviousbestcoderetrievalmodel
GitHub(Allaletal.,2023). Yettheireffortsmainly CodeRetriever (Li et al., 2022) by 1∼10% points
focusontraininglarge-scalegenerativelanguage acrossdifferentevaluationbenchmarks. Compre-
models(LMs). Inparallel,someresearchendeav- hensive ablation and analysis demonstrates the
ors have aimed to create code-related question- effectivenessofourproposedapproach.
answeringdatasetsfromdiversesources,asevi- Thecontributionsofthispapercanbesumma-
dencedbyHuangetal.(2021);Leeetal.(2022). rizedasfollows:
Nevertheless,mostofthesedatasetsremaincon-
strainedbytheirscale,renderingthemmoresuit- • WecreateProCQA,alarge-scaledatasetfor
ableforstand-aloneevaluationbenchmarksrather programmingquestionanswering. ProCQAis
thancomprehensivepre-trainingcorpus. characterizedbyitspracticality,diversityand
Therefore in this research, we try to bridge mixed-modal data format. We demonstrate
these gaps by proposing ProCQA, a large-scale its potential as an evaluation benchmark for
community-basedprogrammingquestionanswer- comparingdifferentcodelanguagemodels.
ing dataset mined from StackOverflow. ProCQA
• Based on ProCQA, we present MACP, a
encompasses an extensive collection of approxi-
code representation model pre-trained with
mately 5 million QA pairs, spanning 11 different
modality-agnosticcontrastivelearningonthe
programming languages. This dataset is distin-
large-scale code-mixing dataset. MACP
guishedbyitscomprehensivelanguagecoverage,
demonstratesremarkableperformancegains
thediversityofuserqueries, anditscode-mixing
overpriorapproachesacrossawiderangeof
dataformat1. Itcanbeusedasbothanevaluation
coderetrievaltasks.
benchmarkandapre-trainingcorpus. Weprovide
strictrule-basedfilteringanddatadecontamination
procedure to ensure its quality and fairness. Dif- 2. Related Work
ferent types of baseline models are trained and
compared on this dataset to test its suitability as 2.1. Code QA
anevaluationbenchmark.
Toassesstheefficacyofourproposeddataset Code-basedquestionansweringisasub-problem
asapre-trainingcorpus,weconductedlarge-scale of question answering. Different from the gener-
modality-agnosticcontrastivepretraining(MACP) ative formulation, retrieval-based code QA aims
onthecode-mixingdataset,withoutmakingdistinc- to retrieve the most similar code from a large-
tionsbetweentextandcodemodalities. Todemon- scale code corpus, satisfying user requests. To
strate whether MACP can learn a better aligned evaluatetheneuralcodesearchabilityofcurrent
representationspace,weevaluateditonextensive models,CodeSearchNet(Husainetal.,2019)was
code retrieval benchmarks, covering supervised, constructedbymininglarge-scalecomment-code
zero-shot, and out-of-domain scenarios. Experi- pairsfrompublicGitHubrepositories. Additionally,
mentsrevealthatcomparedtopreviouspre-trained toevaluatethecodecomprehensionabilityoflan-
guage models, Liu and Wan (2021) introduced
1PleaserefertoTable1foranillustrativeexample. CodeQA, a free-form code question-answeringdataset. This dataset was derived from existing 3. ProCQA
codesummarizationdatasetsminedfromGitHub,
includingtwowidely-usedprogramminglanguages Inthissection,weoutlinethemethodologiesem-
Python and Java. CodeQA synthesizes various ployedinthecreationoftheProCQAdataset,along
types of question-answer pairs from code com- withthefilteringstrategiesappliedtoensuredata
mentsanddocumentationstringsusingmanually quality and fairness. Additionally, we present an
curatedrules,templates,andarangeofNLPtoolk- analysis of various dataset statistics and define
its. twotasksutilizingthisdatasettoevaluatedifferent
Recent work has been focused on construct- baselinemodels. Thesourcecodeisavailableat
ing code QA dataset from real-world scenarios. https://github.com/jordane95/procqa.
Forexample,CoSQA(Huangetal.,2021)mines
real-worlduserqueriesfromBingsearchlogsand
utilizes models trained on CodeSearchNet and
3.1. Data Acquisition
humans to label corresponding code. Moreover,
educationalprogrammingQAdatasetshavealso Toensurethediversityandreflectrealworlduser
gained attention. CS1QA (Lee et al., 2022) col- problems, we crawl our dataset from StackOver-
lectsstudentquestionsandanswersfromteaching flow,aquestionansweringcommunityfocusingon
assistantsonanonlineforumdesignedforanintro- solving programming problems. Users can post
ductoryPythonprogrammingcourse. Thisdataset theirproblemsonthewebsiteandwaitforothers’
offersinsightsintotheeducationalapplicationsof answers. Onecharacteristicofthisdatasetisthat
code-basedquestionanswering. boththequestionandanswerarecode-mixing,i.e.,
text and code are interleaved within these fields.
2.2. Code Language Models Suchdataformatisveryusefultoindoctrinateand
evaluate the model’s matching ability of different
Languagemodelspre-trainedonlarge-scaleunla-
patterns.
beled corpora have demonstrated significant po-
Weusethepublicdumpsasof2022/12forraw
tentialincodeunderstandingandgenerationtasks.
datadownloading2. Weextractthetextualcontent
PriorworkssuchasCodeBERT(Fengetal.,2020)
consistingofcodeandtextsfromXMLfiles. Three
employed replaced language modeling on uni-
fields(title,question,answer)arekept. HTMLtags
modal and bi-modal data for pre-training. Graph-
areremovedandonlytextcontentarekeptusing
CodeBERT (Guo et al., 2021) advanced this ap-
BeautifulSouplibrary.
proach by harnessing data flow encoded in the
AbstractSyntaxTree(AST)ofcodetoenrichcode
structural information during pre-training. UniX-
Coder(Guoetal.,2022)unifiedthreepre-training 3.2. Data Cleaning
designs into one architecture and utilized AST
structureandcodecommenttoenhancethecross- A critical problem with these QA communities is
modal alignment. There are also some work on that there are many unanswered questions and
adaptinggenerativelanguagemodelsforcode,as wrong answers. To handle this issue, we ap-
exemplifiedbyCodeT5(Wangetal.,2021b)and plysomerule-basedapproachestofilteroutlow-
PLBART(Ahmadetal.,2021). Thesemodelsincor- qualityquestionsandanswers.
poratecodestructureinformationintothedesign Morespecifically,wefilteroutquestions/answers
ofspecificpre-trainingtasks. Contrastivemethods that are either too short (< 20 characters) or too
have also been introduced into code pre-training long (> 4096 characters). We only keep ques-
byseveralrecentworkswithdifferentapproaches tions that have answers marked as accepted by
proposed for constructing positive and negative thequestionersinceitisanaturalannotationsignal
pairs(Jainetal.,2021;Wangetal.,2021a;Ding indicatingtheanswerishelpfulfortheuser.
etal.,2022;Buietal.,2021;Lietal.,2022).
It is worth noting that current code language
models’pre-trainingcorpusareprimarilysourced 3.3. Data Format
fromCodeSearchNet,consistingof2millioncode-
text pairs. Limited efforts have been dedicated DatainProCQAisformattedastriplesillustrated
tomininglarge-scaledatasetsfromGitHub(Allal inTable1. Thequestionisaconciseuserrequest.
etal.,2023;Lietal.,2023),buttheymainlyfocus Itiscoupledwithadetaileddescriptionwhichex-
ontrainingdecoderlanguagemodelsratherthan plains the problem in more detail. The answer is
coderepresentationmodels. Anexceptionisthe posted by other user and is the one accepted by
work by OpenAI (Neelakantan et al., 2022), but the questioner. Note that in all data fields, code
theirmodelsareonlyavailableviapaidAPIsand andtextareinterleaved,whichprovidesanatural
trainingdataisnotdetailed. supervisionsignalforaligningthetwomodalities.PL C C++ Java Python Ruby Lisp JavaScript C# Go Rust PHP
Size 204746 418346 831697 1008478 131218 4612 1217095 817970 36011 15514 567357
Table2: NumberofQApairsforeachprogramminglanguageinProCQA.
3.4. Data Statistics 3.6. Comparison to previous datasets
Tobetterunderstandthedifferencewithprevious
We partition the dataset into different program-
dataset, we summarize some key factors of our
minglanguagesubsetsaccordingtotheirtagscon-
ProCQA and previous ones in Table 3, including
tained in meta information. We consider the fol-
thenumberofsupportedprogramminglanguages
lowingelevenlanguagesbasedontheirpopularity:
(PLs),dataformat,sizeanddatasource.
Python,Java,JavaScript,Ruby,C,C++,C#,Rust,
CodeNN (Iyer et al., 2016) is also a dataset
PHP, Lisp and Go. Dataset statistics are shown
mined from StackOverflow for code summariza-
in Table 2. We split the dataset into train / valid /
tionbutcontainsmuchsmalleramountoftraining
testsetbyaproportionof80%:10%:10%following
examplesandlanguages. CodeSearchNet(CSN)
chronologicalorderofpostingdate.
isonpairwithProCQAintermsoflanguagesand
Inaddition,weanalysethequestionandanswer size but drawn from a different data distribution
length distribution of our ProCQA dataset in Fig- (GitHub). Its queries are either documentation
ure 2. Most of the QA pairs in ProCQA contain stringsorcommentsratherthannaturallanguage
dozens or hundreds of words, which are much questions, limiting its practicality in real scenar-
closertorealuserquestions. ios. CoSQA and CS1QA contain some real user
queriescollectedfromBinglogsandclassrooms
butonlycoverPythonandarelimitedinsize.
Insummary,ProCQAdiffersfrompreviouswork
inthefollowingmainaspects:
1. Morediverselanguagedistributionatalarger
scale.
2. Long-form questions and answers more
alignedwithreal-worldscenarios.
3.7. Tasks
Wedefinetwotasksbasedonthecollecteddataset
Figure2: Questionandanswerlengthdistribution
forpilotexploration,includinganswerretrievaland
inProCQA(Csubset).
generation. WechooseCsubsetasatestbedfor
comparingmultiplelanguagemodels.
Answer Retrieval This task is defined as find-
ingthecorrectanswerfromalarge-scaleanswer
3.5. Decontamination
corpus. We use answers from all splits of the
datasettoformretrievalcorpus. Thequeryisthe
Since ProCQA is crawled from StackOverflow, concatenation of question and description. We
it many overlap with some evaluation sets con- choose BM25 and some recent neural language
structedfromthesamesource. Toavoiddatacon- models,suchasBERT(Devlinetal.,2019),Code-
tamination,weperformevaluationdatadeduplica- BERT (Feng et al., 2020) and UniXCoder (Guo
tionforourProCQAtrainingset. et al., 2022). Neural LMs are fine-tuned with the
Specifically, we employ two methods for dedu- contrastivelearningobjective(i.e.,InfoNCEloss)
plication. The first one is based on substring onthequestionanswerpairsfromthetrainingset.
matching. Training example in ProCQA dataset Allmodelsaretrainedfor3epochswiththebatch
isdroppedifitcontainsanysubstringthatispart sizeof32andthelearningrateof2e-5. Bothques-
ofthequeriesintheevaluationset. Weusethree tions and answers are truncated to be maximum
evaluationsetstoperformdeduplication(CoNaLa, of256tokens. WechooseMRR@10,Recall@10
SO-DSandStaQC).Afterthisstep,about0.5%ex- andRecall@100asmainevaluationmetrics.
amplesfromthePythonsubsetaredropped. Other Results are demonstrated in Table 4. We ob-
subsetsareinfluencedlightly. Wealsoapplyfuzzy serve that text-only language models such as
deduplication method based on MinHash but no
additionalduplicateisfound. 2https://archive.org/details/stackexchangeDataset #ofPLs DataFormat Size DataSource
CodeNN 2 Title,code ∼187Kpairs StackOverflow
CodeSearchNet 6 Comment,code ∼2Mpairs GitHub
CodeQA 2 Question,answer,code ∼190Kpairs GitHub
CoSQA 1 Query,code ∼20Kpairs Websearch
CS1QA 1 Chatlog,question,answer,type,code ∼9Kpairs Classroom
ProCQA 11 Question,description,answer ∼5Mpairs StackOverflow
Table3: Comparisonbetweendifferentcode-baseddatasets.
Model MRR@10 R@10 R@100 ThisindicatesthatProCQAisachallengingdataset
forlong-formgenerativeQAtask. Howtoimprove
BM25 51.7 61.1 73.1
thelong-formquestion-answeringperformanceof
BERT 48.3 62.0 79.7
language models with limited parameters is also
CodeBERT 53.0 66.8 83.5
aninterestingdirectionforfutureresearch.
UniXCoder 58.4 71.8 86.1
4. Experiments
Table4: Answerretrievalperformanceofdifferent
languagemodelsontheCsubsetofProCQA.
To assess the quality and utility of our proposed
dataset, we evaluate its benefits to other code
BERTareeveninferiortounsupervisedBM25,in searchbenchmarkswhenactingasapre-training
termsofMRR@10. Withcode-specificpre-training, corpus. We also conduct ablation experiments
CodeBERTcanoutperformthestrongBM25base- todemonstratetheeffectivenessofProCQAover
line. Morerecentcodelanguagemodelssuchas existingpre-trainingcorpusCSN.
UniXCoderperformsbestonthistask.
4.1. Settings
AnswerGeneration Wealsoconsideragenera-
Our model basically follows the two-tower archi-
tivetaskformulation,inwhichthemodelisrequired
tecture in which vector representations for code
to directly generate the answer to the question
and text are produced by mean pooling over the
withoutadditionalreference. Similarly,webench-
lastlayerhiddenrepresentationsofthelanguage
markseveralgenerativelanguagemodelsonthis
models. It is trained via the contrastive objective
task. Selected baseline models include T5 (Raf-
usingtheInfoNCEloss
fel et al., 2020), CodeT5 (Wang et al., 2021b),
PLBART(Ahmadetal.,2021). Modelsaretrained es(q,d)/τ
L=−log . (1)
inasequence-to-sequencemannerbyoptimizing es(q,d)/τ + (cid:80) es(q,d′)/τ
the cross-entropy loss of the answer sequence d′∈D−
given question sequence with the same training
whereq denotesthequestion,ddenotesthecorre-
hyperparameters as stated above. During infer-
spondinganswer,D isasetofnegativesamples,
ence,beamsearchdecodingisusedwithabeam −
τ is the temperature. D is also enlarged with
size of 5. We use ROUGE (Lin, 2004) as main −
otherexamplesfromthesamebatch.
evaluation metrics for this task and demonstrate
ThemainbaselineswecomparetoareGraph-
resultsinTable5.
CodeBERT (Guo et al., 2021) and CodeRe-
triever (Li et al., 2022). In addition to the text-
Model ROUGE ROUGE ROUGE
1 2 L
code pairs in CSN used by GraphCodeBERT,
T5 14.3 2.6 11.8 CodeRetriever also employs sophisticated rules
CodeT5 17.6 4.8 14.0 andlearnedmodelsformininghigh-qualitycode-
PLBART 19.9 5.9 15.3 codeandcode-textpairsfromtherawCSNcode
corpus. Instead we use commonly available QA
Table 5: Answer generation results of different pairs from ProCQA mined by weak supervision.
baselinesonProCQA(Csubset). We use the training split across all languages to
construct different types of mixed-modal positive
Itisfoundthatcodelanguagemodelsisbetter pairs. Weapplymodality-agnosticcontrastivepre-
thantext-onlymodels,indicatingtheeffectiveness trainingontheProCQAandCSNdatasetandcom-
ofcode-specificpre-training. Eventhebestmodel pareourmodeltopreviouscodeembeddingmod-
strugglesonthistaskbecausetheanswersarerel- elsonvariousretrievaltasks. Ourmodelisdenoted
ativelylong(mostly100-200words,seeFigure2). as MACP.4.2. Implementation Details Then, code-code search results on POJ-
104 (Mou et al., 2016) is also reported to eval-
For fair comparison, our model is initialized with
uatetheintra-modalretrievalability. Inthisdataset,
GraphCodeBERT,sameasCodeRetriever. MACP
Python program solutions of the same problem
ispre-trainedwiththecontrastiveobjectiveinEqua-
is regarded as positive pairs. The objective is to
tion 1 using cosine similarity and τ = 0.01. To
retrieverelevantcodesnippetswhichanswerthe
balancelow-resourcelanguages,wesampleeach
same problem. To investigate the cross-lingual
data batch from a multi-nominal distribution over
coderetrievalability,weuseCodeNet(Purietal.,
differentlanguagesubsets
2021)asanevaluationbenchmark,whichisalso
nα aproblem-solutiondatasetsimilartoPOJ-104but
p i = (cid:80)n i nα, (2) covers more languages. On CodeNet, we con-
j=1 j
siderthezero-shotretrievalofthreeprogramming
with n i equal to the size of subset i and smooth- languages(Ruby,PythonandJava)followingGuo
ing parameter α = 0.5. We run the contrastive etal.(2022),wherecodeispre-processedbyre-
pre-trainingfor10kstepswithaglobalbatchsize movingcommentsandreplacingallseparatorswith
of6192. In-batchnegativesareusedandshared whitespace. PerformanceisevaluatedbyMAP.
across different GPUs. Each sequence is trun- Finally,totestwhetherourmodelcangeneralize
cated at a maximum length of 128. The learning to out-of-domain languages, we choose two text-
rateisinitiallywarmedupto2e-4forthefirst10% codesearchdatasetswithlanguagesunseendur-
steps,followedbyalineardecay. ingpre-training. SmartContracts(SC)(Yangetal.,
Weutilizethesamecontrastivelossduringfine- 2021)containsSolidityprogramminglanguageand
tuning on each downstream dataset. Each fine- Spider(Yuetal.,2018)consistsofSQL-querypairs.
tuningexperimentonlyinvolvesonedatasetsowe We use the dataset split released by Chai et al.
directlysampledataaftershufflingit. Modelsare (2022). ModelsareevaluatedbyRecall@{1,5,10}
trainedusingapeaklearningrateof2e-5withthe and MRR@1000. The statistics of downstream
samescheduleraspre-training. Themaximumse- evaluationbenchmarksareillustratedinTable6.
quencelengthis512. Batchsizeis128andeach
sampleisaccompaniedwith7randomlysampled Dataset Lang Train Valid Test
negatives. Trainingepochsis3. Otherhyperparam-
CSN Ruby 24.9K 1.4K 1.3K
etersaresameaspre-training. Weonlyconsider
CSN JS 58K 3.9K 3.3K
in-batch negatives for contrastive learning so we
CSN Go 167K 7.3K 8.1K
comparemodelsunderthissetting.
CSN Python 252K 13.9K 14.9K
WeconductallexperimentsontwoNVIDIAA100
CSN Java 165K 5.2K 10.9K
GPUs with 40G memory. We use DeepSpeed,
CSN PHP 241K 13.0K 14.0K
gradientcheckpointingandmixedprecision(FP16)
Adv Python 28.0K 9.6K 19.2K
encodingtoreducememorycost. Thepre-training
CoSQA Python 19K 0.5K 0.5K
processtakesabout18hours. Fine-tuningonall
CoNaLa Python 2.8K - 0.8K
datasetsisfinishedinoneday.
SO-DS Python 14.2K 0.9K 1.1K
StaQC Python 20.4K 2.6K 2.7K
4.3. Evaluation Benchmarks
POJ104 Python 32K 8K 12K
Toprovideanextensiveevaluationofthegeneral- CodeNet Ruby - - 11.7K
izationabilityofourpre-trainedmodels,weselect CodeNet Python - - 15.6K
alargevarietyofcoderetrievaltasksfromdifferent CodeNet Java - - 23.5K
domainsunderdifferentsettings. SC Solidity 57K 4.1K 1K
WefirstevaluateontheCodeSearchNetbench- Spider SQL 14K 2.1K 1K
mark (Husain et al., 2019), which is widely used
forevaluatingthetext-codesearchabilityofcode Table 6: Statistics of downstream evaluation
retrieval models. One drawback of CodeSearch- datasets.
Net is the queries are not aligned to real user
questions. So, we also evaluate on some more
challenging datasets, Adv Test (Lu et al., 2021),
4.4. Results
CoSQA(Huangetal.,2021), CoNaLa(Yinetal.,
2018), SO-DS (Heyman and Cutsem, 2020), In this section, we report and discuss the perfor-
StaQC (Yao et al., 2018). The last three evalu- mance of MACP on the evaluation benchmarks
ation datasets follow the setting of Heyman and introducedintheprevioussection,spanningboth
Cutsem(2020),whereduringinferencebothtext supervised and zero-shot settings. In the super-
descriptionandcodesnippetareusedformatching. vised setting, MACP is directly fine-tuned on full
ThemainevaluationmetricisMRR. trainingsetandthelastcheckpointisevaluatedonMethod Ruby Javascript Go Python Java PHP Overall
ContraCode(Jainetal.,2021) - 30.6 - - - - -
SyncoBERT(Wangetal.,2021a) 72.2 67.7 91.3 72.4 72.3 67.8 74.0
CodeBERT(Fengetal.,2020) 67.9 62.0 88.2 67.2 67.6 62.8 69.3
GraphCodeBERT(Guoetal.,2021) 70.3 64.4 89.7 69.2 69.1 64.9 71.3
UniXcoder(Guoetal.,2022) 74.0 68.4 91.5 72.0 72.6 67.6 74.4
CodeRetriever(Lietal.,2022) 75.3 69.5 91.6 73.3 74.0 68.2 75.3
MACP 77.8 72.5 92.4 76.1 75.7 70.1 77.4
Table7: MRR@1konsixprogramminglanguagetestsetsoftheCodeSearchNet.
Method Adv CoSQA CoNaLa SO-DS StaQC Overall
SyncoBERT(Wangetal.,2021a) 38.1 - - - - -
CodeBERT(Fengetal.,2020) 27.2 64.7 20.9 23.1 23.4 31.9
GraphCodeBERT(Guoetal.,2021) 35.2 67.5 23.5 25.3 23.8 35.1
UniXcoder(Guoetal.,2022) 41.3 70.1 - - - -
CodeRetriever(Lietal.,2022) 43.0 70.6 29.6 27.1 25.5 39.0
MACP 39.7 72.0 61.0 48.8 23.3 49.0
Table8: Text-codesearchperformance(MRR@1k)ondatasetsthatareclosertotherealscenario.
the test set. In the zero-shot setting, it is directly Method MAP
evaluatedonthetestset.
RoBERTa(Liuetal.,2019) 76.67
CodeBERT(Fengetal.,2020) 82.67
Text-Code Search We first present evaluation GraphCodeBERT(Guoetal.,2021) 85.16
results on six programming language subsets of SynCoBERT(Wangetal.,2021a) 88.24
CodeSearchNetinTable7. MACPtrainedwiththe DISCO(Dingetal.,2022) 82.77
newlyproposeddatasetoutperformspreviousbest Corder(Buietal.,2021) 84.10
modelCodeRetrieveronalllanguagesubsets,by CodeRetriever(Lietal.,2022) 88.85
anaverageof2.1points. MACP 90.23
Next, we look at results on several challeng-
ingbenchmarks,allcollectedfromreal-worlduser Table9: PerformanceofPythoncode-to-codere-
queries instead of docstrings. As shown in Ta- trievaltaskonPOJ-104.
ble 8, our model significantly outperforms prior
state-of-the-artmodelsbyupto10pointsonaver-
Cross-Domain Code Search In Table 11, we
age. We attribute the improvement to real-world
comparedifferentmodels’performanceonSolid-
userqueriesfromProCQA.
ity and SQL, two languages unseen during pre-
training. PreviousbestmodelMAML(Chaietal.,
Code-Code Search After evaluating the cross- 2022) applied model-agnostic meta learning on
modalsearchabilityofourembeddingmodel,we CodeBERT (Feng et al., 2020) using Java and
zoomintotheintra-modalretrievalperformanceby PythonsubsetsfromCSNforpre-training. Inaddi-
evaluatingonacodeclonedetectionbenchmark, tion,wealsoreporttheperformanceofGraphCode-
POJ-104. Results are illustrated in Table 9. Our BERTusingourcodebaseasanotherbaselinefor
modeloutperformspreviousbestbaselineCodeRe- comparison. Ourmodelsignificantlyimprovesthe
trieverby+1.38points. cross-domaincodesearchperformanceonunseen
languages. Onepossiblereasonisthatthediver-
sity of language coverage in ProCQA equips the
Zero-ShotCross-LingualCodeSearch Welist
modelwithbetterlanguageadaptationability.
thecross-lingualcoderetrievalperformanceofour
model MACP andotherbaselinesfrom Guoetal.
(2022)inTable10. UniXCoderhassignificantlybet- 4.5. Analysis
terzero-shotcoderetrievalperformance,owingto
itscontrastiveobjectiveduringpre-training. MACP Impactofpretrainingdatadistribution Wefirst
consistentlyoutperformspreviousbaselinesbya investigate the effect of different pre-training cor-
large margin, setting new state-of-the-art perfor- pus by doing a series of controlled experiments
manceonthistask. where only the pre-training data distribution isRuby Python Java
Method Overall
Ruby Python Java Ruby Python Java Ruby Python Java
CodeBERT 13.55 3.18 0.71 3.12 14.39 0.96 0.55 0.42 7.62 4.94
GraphCodeBERT 17.01 9.29 6.38 5.01 19.34 6.92 1.77 3.50 13.31 9.17
PLBART 18.60 10.76 1.90 8.27 19.55 1.98 1.47 1.27 10.41 8.25
CodeT5 18.22 10.02 1.81 8.74 17.83 1.58 1.13 0.81 10.18 7.81
UniXcoder 29.05 26.36 15.16 23.96 30.15 15.07 13.61 14.53 16.12 20.45
MACP 44.74 43.11 31.26 40.59 45.77 29.75 32.8 33.75 30.74 36.95
Table10: MAPscore(%)ofzero-shotcode-to-codesearchtaskonCodeNet.
Solidity SQL
Method
R@1 R@5 R@10 MRR R@1 R@5 R@10 MRR
CodeBERT(Fengetal.,2020) 53.2 77.9 84.8 64.4 67.5 92.0 96.0 78.2
MAML(Chaietal.,2022) 65.8 82.9 87.9 73.4 74.6 95.2 97.2 83.7
GraphCodeBERT(Guoetal.,2021) 72.9 85.5 89.3 78.5 78.5 94.5 96.6 85.5
MACP 75.2 87.3 90.6 80.7 85.4 96.0 97.4 90.3
Table11: Resultsofcross-domaincoderetrievalonprogramminglanguagesunseenduringpretraining.
changed. We run two additional experiments by onlyrequirescode-textmatching,yettrainingwith
using the CSN and ProCQA dataset individually mixed-modaldataformatsstillhasbenefits.
for pre-training. Due to space limitation, we re-
portdownstreamfine-tunedretrievalperformance Setting AdvTest
onCodeSearchNetinFigure3. Resultsonother
bi-modal 38.8
evaluationbenchmarksfollowthesametrends.
mixed-modal 39.1
Despite CSN belongs to in-domain data for
this evaluation benchmark, it still underperforms
Table 12: Effect of the data format used in
ProCQAwhenbeingusedasapre-trainingcorpus.
CodeSearchNet corpus pre-training. We report
Combiningtwodatasetsgivesbetterresults. This
MRR@1konAdvTestset.
showcases the effectiveness of ProCQA dataset
beingusedasamixed-modalcorpusforretrieval-
orientedcontrastivepre-training.
Quantifying the effect of data contamination
To avoid data contamination and ensure fair-
100 ness,weperformedde-duplicationfortheProCQA
CSN ProCQA Both
dataset with respect to the relevant evaluation
50 benchmarks from the same source, including
CoNaLa,SO-DSandStaQC.InTable13,wepro-
0 PHP JavaScript Java Python Ruby Go Average v ci od ne taa mq inu aa ten dtita dativ tae foa rn ea aly cs his eo van luth ae tiop nro sep to art nio dn tho ef
performanceusingrawandfilteredversionofthe
Figure3: Ablationofthepre-trainingcorpus. Re- ProCQAdatasetforpre-training. Althoughalarge-
sultscomparedontestsetsofCodeSearchNet. proportionoftheevaluationsetisincludedinthe
rawpre-trainingdata,removingthemraisesalim-
ited degradation of model performance, as they
only make up a small portion of the large-scale
Effectofmodality-agnosticdata Weablateon
pre-trainingdata.
thechoiceofmodality-agnosticcontrastivelearn-
ingbycomparingtoanothersettingwhichweex-
Dataset Proportion Unfiltered Filtered
plicitly distinguish text and code in data design.
Due to the high difficulty of parsing incomplete CoNaLa 83.7% 62.9 61.0
codesnippetsinProCQA,weconductthisablation SO-DS 48.4% 49.8 48.8
on the CSN pre-training corpus where code are StaQC 31.4% 23.6 23.3
wellformedandcanbeparsedbyexistingtools. Bi-
modalsetting removes allcommentsin thecode Table13: Analysisontheinfluenceofdatacontam-
while mixed-modal setting keeps them. We list inationonthreeevaluationdatasets.
results in Table 12. The evaluation set Adv Test
k1@RRM5. Conclusion the 44th International ACM SIGIR Conference
on Research and Development in Information
In this work we introduce ProCQA, a large-scale Retrieval,pages511–521.
community-basedprogrammingquestionanswer-
ing dataset mined from StackOverflow with strict YitianChai,HongyuZhang,BeijunShen,andXi-
filteringstrategiesforqualityandfairnesscontrol. aodong Gu. 2022. Cross-domain deep code
ProCQAisfeaturedbyitspracticality,diversityand search with meta learning. In Proceedings of
code-mixing data format. Furthermore, through the44thIEEE/ACMInternationalConferenceon
modality-agnosticcontrastivepre-trainingoninter- SoftwareEngineering,pages487–498.
leavedcodeandtextdata,ournewdatasetyields
Alexis Conneau and Guillaume Lample. 2019.
alanguagemodelthathasabetteralignedrepre-
Cross-lingual language model pretraining. In
sentationspacebetweencodeandtext,achieving
Proceedingsofthe2019AnnualConferenceon
state-of-the-artperformanceonalargespectrum
NeuralInformationProcessingSystems,pages
of code retrieval tasks. In future work, it would
7057–7067.
beinterestingtoexplorethebenefitofProCQAto
othergenerativecodeQAtasks. JacobDevlin,Ming-WeiChang,KentonLee,and
Kristina Toutanova. 2019. BERT: Pre-training
6. Acknowledgements ofdeepbidirectionaltransformersforlanguage
understanding. InProceedingsofthe2019Con-
ThisworkwassupportedbytheNationalNatural ferenceoftheNorthAmericanChapteroftheAs-
ScienceFoundationofChina(No.61977003)and sociationforComputationalLinguistics: Human
the State Key Laboratory of Complex & Critical LanguageTechnologies,pages4171–4186.
SoftwareEnvironment(CCSE-2024ZX-16).
Yangruibo Ding, Luca Buratti, Saurabh Pujar,
Alessandro Morari, Baishakhi Ray, and Saikat
7. Bibliographical References
Chakraborty. 2022. Towards learning (dis)-
similarityofsourcecodefromprogramcontrasts.
InProceedingsofthe60thAnnualMeetingofthe
AssociationforComputationalLinguistics,pages
WasiUddinAhmad,SaikatChakraborty,Baishakhi
6300–6312.
Ray, and Kai-Wei Chang. 2021. Unified pre-
trainingforprogramunderstandingandgenera- ZhangyinFeng,DayaGuo,DuyuTang,NanDuan,
tion. InProceedingsofthe2021Conferenceof XiaochengFeng,MingGong,LinjunShou,Bing
theNorthAmericanChapteroftheAssociation Qin,TingLiu,DaxinJiang,andMingZhou.2020.
forComputationalLinguistics: HumanLanguage CodeBERT: A pre-trained model for program-
Technologies,pages2655–2668. ming and natural languages. In Findings of
the Association for Computational Linguistics:
Loubna Ben Allal, Raymond Li, Denis Ko-
EMNLP2020,pages1536–1547.
cetkov, Chenghao Mou, Christopher Akiki,
Carlos Muñoz Ferrandis, Niklas Muennighoff, XiaodongGu,HongyuZhang,andSunghunKim.
Mayank Mishra, Alex Gu, Manan Dey, Lo- 2018. Deep code search. In Proceedings of
gesh Kumar Umapathi, Carolyn Jane Ander- the40thInternationalConferenceonSoftware
son, Yangtian Zi, Joel Lamy-Poirier, Hailey Engineering,pages933–944.
Schoelkopf,SergeyTroshin,DmitryAbulkhanov,
DayaGuo,ShuaiLu,NanDuan,YanlinWang,Ming
ManuelRomero,MichaelLappert,FrancescoDe
Zhou, and Jian Yin. 2022. UniXcoder: Unified
Toni,BernardoGarcíadelRío,QianLiu,Shamik
cross-modalpre-trainingforcoderepresentation.
Bose, Urvashi Bhattacharyya, Terry Yue Zhuo,
InProceedingsofthe60thAnnualMeetingofthe
Ian Yu, Paulo Villegas, Marco Zocca, Sourab
AssociationforComputationalLinguistics,pages
Mangrulkar,DavidLansky,HuuNguyen,Danish
7212–7225.
Contractor,LuisVilla,JiaLi,DzmitryBahdanau,
YacineJernite,SeanHughes,DanielFried,Arjun
Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng,
Guha,HarmdeVries,andLeandrovonWerra.
Duyu Tang, Shujie Liu, Long Zhou, Nan Duan,
2023. Santacoder: Don’t reach for the stars!
Alexey Svyatkovskiy, Shengyu Fu, Michele Tu-
CoRR,abs/2301.03988.
fano,ShaoKunDeng,ColinB.Clement,Dawn
Drain,NeelSundaresan,JianYin,DaxinJiang,
Nghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang.
and Ming Zhou. 2021. GraphCodeBERT: Pre-
2021. Self-supervised contrastive learning for
trainingcoderepresentationswithdataflow. In
coderetrievalandsummarizationviasemantic-
Proceedingsofthe9thInternationalConference
preserving transformations. In Proceedings of
onLearningRepresentations.GeertHeymanandTomVanCutsem.2020.Neural Zhang, Nour Moustafa-Fahmy, Urvashi Bhat-
codesearchrevisited: Enhancingcodesnippet tacharyya,WenhaoYu,SwayamSingh,Sasha
retrievalthroughnaturallanguageintent. CoRR, Luccioni, Paulo Villegas, Maxim Kunakov, Fe-
abs/2008.12193. dorZhdanov,ManuelRomero,TonyLee,Nadav
Timor,JenniferDing,ClaireSchlesinger,Hailey
JunjieHuang,DuyuTang,LinjunShou,MingGong,
Schoelkopf,JanEbert,TriDao,MayankMishra,
KeXu,DaxinJiang,MingZhou,andNanDuan.
AlexGu,JenniferRobinson,CarolynJaneAnder-
2021. CoSQA: 20, 000+ web queries for code
son,BrendanDolan-Gavitt,DanishContractor,
searchandquestionanswering. InProceedings
Siva Reddy, Daniel Fried, Dzmitry Bahdanau,
of the 59th Annual Meeting of the Association
Yacine Jernite, Carlos Muñoz Ferrandis, Sean
forComputationalLinguisticsandthe11thInter-
Hughes, Thomas Wolf, Arjun Guha, Leandro
nationalJointConferenceonNaturalLanguage
von Werra, and Harm de Vries. 2023. Star-
Processing,pages5690–5700.
Coder: May the source be with you! CoRR,
Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, abs/2305.06161.
Miltiadis Allamanis, and Marc Brockschmidt.
Xiaonan Li, Yeyun Gong, Yelong Shen, Xipeng
2019. CodeSearchNet challenge: Evaluating
Qiu,HangZhang,BolunYao,WeizhenQi,Daxin
the state of semantic code search. CoRR,
Jiang, Weizhu Chen, and Nan Duan. 2022.
abs/1909.09436.
CodeRetriever: A large scale contrastive pre-
Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, trainingmethodforcodesearch. InProceedings
and Luke Zettlemoyer. 2016. Summarizing of the 2022 Conference on Empirical Methods
sourcecodeusinganeuralattentionmodel. In inNaturalLanguageProcessing,pages2898–
Proceedingsofthe54thAnnualMeetingofthe 2910.
AssociationforComputationalLinguistics,pages
Chin-Yew Lin. 2004. ROUGE: A package for au-
2073–2083.
tomatic evaluation of summaries. In Proceed-
Paras Jain, Ajay Jain, Tianjun Zhang, Pieter ings of ACL Workshop on Text Summarization
Abbeel,JosephGonzalez,andIonStoica.2021. BranchesOut,pages74–81.
Contrastivecoderepresentationlearning. InPro-
ChenxiaoLiuandXiaojunWan.2021. CodeQA:A
ceedingsofthe2021ConferenceonEmpirical
questionansweringdatasetforsourcecodecom-
MethodsinNaturalLanguageProcessing,pages
prehension. In Findings of the Association for
5954–5971.
ComputationalLinguistics: EMNLP2021,pages
AdityaKanade,PetrosManiatis,GogulBalakrish- 2618–2632.
nan,andKensenShi.2020. Learningandevalu-
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,
atingcontextualembeddingofsourcecode. In
Mandar Joshi, Danqi Chen, Omer Levy, Mike
Proceedings of the 37th International Confer-
Lewis,LukeZettlemoyer,andVeselinStoyanov.
enceonMachineLearning,pages5110–5121.
2019. Roberta: ArobustlyoptimizedBERTpre-
Changyoon Lee, Yeon Seonwoo, and Alice Oh. trainingapproach. CoRR,abs/1907.11692.
2022. CS1QA: A dataset for assisting code-
basedquestionansweringinanintroductorypro- Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang,
gramming course. In Proceedings of the 2022 AlexeySvyatkovskiy,AmbrosioBlanco,ColinB.
Conference of the North American Chapter of Clement,DawnDrain,DaxinJiang,DuyuTang,
the Association for Computational Linguistics: Ge Li, Lidong Zhou, Linjun Shou, Long Zhou,
Human Language Technologies, pages 2026– Michele Tufano, Ming Gong, Ming Zhou, Nan
2040. Duan, Neel Sundaresan, Shao Kun Deng,
ShengyuFu,andShujieLiu.2021.CodeXGLUE:
Raymond Li, Loubna Ben Allal, Yangtian Zi, A machine learning benchmark dataset for
NiklasMuennighoff,DenisKocetkov,Chenghao code understanding and generation. CoRR,
Mou, Marc Marone, Christopher Akiki, Jia Li, abs/2102.04664.
Jenny Chim, Qian Liu, Evgenii Zheltonozhskii,
TerryYueZhuo,ThomasWang,OlivierDehaene, LiliMou,GeLi,LuZhang,TaoWang,andZhiJin.
MishigDavaadorj,JoelLamy-Poirier,JoãoMon- 2016. Convolutionalneuralnetworksovertree
teiro,OlehShliazhko,NicolasGontier,Nicholas structuresforprogramminglanguageprocessing.
Meade,ArmelZebaze,Ming-HoYee,LogeshKu- InProceedingsofthe30thAAAIConferenceon
marUmapathi,JianZhu,BenjaminLipkin,Muh- ArtificialIntelligence,pages1287–1293.
tashamOblokulov,ZhiruoWang,RudraMurthy
ArvindNeelakantan,TaoXu,RaulPuri,AlecRad-
V,JasonStillerman,SivaSankalpPatel,Dmitry
ford,JesseMichaelHan,JerryTworek,Qiming
Abulkhanov,MarcoZocca,MananDey,ZhihanYuan,NikolasTezak,JongWookKim,ChrisHal- Yue Wang, Weishi Wang, Shafiq R. Joty, and
lacy,JohannesHeidecke,PranavShyam,Boris Steven C. H. Hoi. 2021b. CodeT5: Identifier-
Power, Tyna Eloundou Nekoul, Girish Sastry, awareunifiedpre-trainedencoder-decodermod-
Gretchen Krueger, David Schnurr, Felipe Pet- els for code understanding and generation. In
roski Such, Kenny Hsu, Madeleine Thompson, Proceedingsofthe2021ConferenceonEmpir-
Tabarak Khan, Toki Sherbakov, Joanne Jang, ical Methods in Natural Language Processing,
Peter Welinder, and Lilian Weng. 2022. Text pages8696–8708.
andcodeembeddingsbycontrastivepre-training.
ZhenYang,JackyKeung,XiaoYu,XiaodongGu,
CoRR,abs/2201.10005.
ZhengyuanWei,XiaoxueMa,andMiaoZhang.
Ruchir Puri, David S. Kung, Geert Janssen, Wei 2021. A multi-modal transformer-based code
Zhang, Giacomo Domeniconi, Vladimir Zolo- summarizationapproachforsmartcontracts. In
tov,JulianDolby,JieChen,MihirR.Choudhury, Proceedingsofthe29thIEEE/ACMInternational
Lindsey Decker, Veronika Thost, Luca Buratti, ConferenceonProgramComprehension,pages
SaurabhPujar,ShyamRamji,UlrichFinkler,Su- 1–12.
san Malaika, and Frederick Reiss. 2021. Co-
deNet: A large-scale AI for code dataset for Ziyu Yao, Daniel S. Weld, Wei-Peng Chen, and
learningadiversityofcodingtasks. InProceed- HuanSun.2018.StaQC:Asystematicallymined
ings of the 35th Annual Conference on Neu- question-code dataset from stack overflow. In
ral Information Processing Systems Track on Proceedingsofthe2018WorldWideWebCon-
DatasetsandBenchmarks. ference,pages1693–1703.
Colin Raffel, Noam Shazeer, Adam Roberts, PengchengYin, BowenDeng, EdgarChen, Bog-
KatherineLee,SharanNarang,MichaelMatena, dan Vasilescu, and Graham Neubig. 2018.
YanqiZhou,WeiLi,andPeterJ.Liu.2020. Ex- Learningtominealignedcodeandnaturallan-
ploringthelimitsoftransferlearningwithauni- guagepairsfromstackoverflow. InProceedings
fiedtext-to-texttransformer. JournalofMachine ofthe15thInternationalConferenceonMining
LearningResearch,21:140:1–140:67. SoftwareRepositories,pages476–486.
PasqualeSalza,ChristophSchwizer,JianGu,and TaoYu,RuiZhang,KaiYang,MichihiroYasunaga,
Harald C. Gall. 2023. On the effectiveness of Dongxu Wang, Zifan Li, James Ma, Irene Li,
transfer learning for code search. IEEE Trans- QingningYao,ShanelleRoman,ZilinZhang,and
actions on Software Engineering, 49(4):1804– DragomirR.Radev.2018. Spider: Alarge-scale
1822. human-labeled dataset for complex and cross-
domainsemanticparsingandText-to-SQLtask.
Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou,
InProceedingsofthe2018ConferenceonEm-
Yao Wan, Xiao Liu, Li Li, Hao Wu, Jin Liu, and
piricalMethodsinNaturalLanguageProcessing,
XinJiang.2021a. SynCoBERT:Syntax-guided
pages3911–3921.
multi-modalcontrastivepre-trainingforcoderep-
resentation. CoRR,abs/2108.04556.