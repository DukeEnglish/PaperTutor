Real-time Adaptation for Condition Monitoring Signal Prediction
using Label-aware Neural Processes
Seokhyun Chung∗1 and Raed Al Kontar2
1Department of Systems & Information Engineering, University of Virginia
2Department of Industrial & Operations Engineering, University of Michigan
Abstract
Buildingapredictivemodelthatrapidlyadaptstoreal-timeconditionmonitoring(CM)sig-
nals is critical for engineering systems/units. Unfortunately, many current methods suffer from
atrade-offbetweenrepresentationpowerandagilityinonlinesettings. Forinstance,parametric
methods that assume an underlying functional form for CM signals facilitate efficient online
prediction updates. However, this simplification leads to vulnerability to model specifications
and an inability to capture complex signals. On the other hand, approaches based on over-
parameterized or non-parametric models can excel at explaining complex nonlinear signals, but
real-time updates for such models pose a challenging task. In this paper, we propose a neural
process-based approach that addresses this trade-off. It encodes available observations within a
CM signal into a representation space and then reconstructs the signal’s history and evolution
forprediction. Oncetrained,themodelcanencodeanarbitrarynumberofobservationswithout
requiringretraining,enablingon-the-spotreal-timepredictionsalongwithquantifieduncertainty
andcanbereadilyupdatedasmoreonlinedataisgathered. Furthermore,ourmodelisdesigned
to incorporate qualitative information (i.e., labels) from individual units. This integration not
only enhances individualized predictions for each unit but also enables joint inference for both
signals and their associated labels. Numerical studies on both synthetic and real-world data in
reliabilityengineeringhighlighttheadvantageousfeaturesofourmodelinreal-timeadaptation,
enhanced signal prediction with uncertainty quantification, and joint prediction for labels and
signals.
Keywords: real-time adaptation; neural processes; meta-learning; condition monitoring; degra-
dation
∗Corresponding author: schung@virginia.edu
1
4202
raM
52
]GL.sc[
1v77361.3042:viXra1 Introduction
Data-driven predictive modeling of condition monitoring (CM) signals is a key component of reli-
ability analysis for engineering systems/units nowadays. It involves building a model that encodes
knowledge from historical CM signals and then utilizing it to predict the future evolution of CM
signals from an in-service unit with online observations collected up until the current point. In this
framework, refining predictions to accommodate online observations is instrumental in capturing
unit-specific patterns inherent in the CM signal. Such an online update process is often referred
to as “adaptation” or “personalization” since it aims to tailor predictions to each individual unit
under consideration, given its real-time data collected.
However, advances in modern systems pose key challenges in adaptation. CM data collected
through Internet of Things (IoT) sensors is often acquired in real-time and at a high frequency,
necessitating immediate adaptation to online observations. At the same time, the ever-increasing
complexity of engineering systems gives rise to complicated and nonstationary CM signals, de-
manding models with strong representational capabilities. Unfortunately, existing methods suffer
from a trade-off between representation power and agility to online data. Methods that assume a
simplified form for CM signals, such as polynomial mixed-effects models [1], can perform efficient
online updates but are susceptible to model misspecification and struggle when modeling highly
non-linear signals. On the other hand, methods with strong representational power, such as over-
parameterized [2] or non-parametric models [3], are hard to adapt in real-time and often require
retraining the predictive model when new data is collected. As a consequence, their broad applica-
tionisgreatlyconstrainedinrecentCMpracticesthatdemandbothagilityandrichexpressiveness,
simultaneously.
Another significant challenge in adaptation emerges when units are of different types or condi-
tions, identified by categorical information (i.e., labels). To illustrate, consider the CM of electric
vehicle batteries. These batteries may be from different production lots or original equipment man-
ufacturers, or they may operate within vehicles of different brands, models, or generations. The
presence of such categorical disparities often results in significant variations in CM signals among
units based on their respective labels. That said, in practices such as degradation-based CM, these
variations remain dormant initially and become evident only at a later stage due to cumulative ef-
fects [4]. For example, while initially presenting as healthy, lithium-ion batteries from a disqualified
lot often exhibit abnormal degradation trends at the later stage, unacceptably deviating from the
2degradation trend of normal batteries, as presented in our case study in Sec. 5. In such cases, an
early-stage adaptation that relies solely on indistinguishable initial trends can easily fail to learn
future deviations. This necessitates leveraging label information from the units along with their
CM signals.
This article aims to address both challenges in adaptation outlined above – real-time person-
alization for complex signals and the incorporation of label information. Specifically, we present
a real-time prognostic approach that dynamically predicts and quantifies uncertainty in real-time
as data is collected from an in-service unit. The real-time adaptivity comes with augmented pre-
dictive capabilities by incorporating label information. In the spirit of representation learning, our
approach learns an encoder that maps a CM signal along with its label into a representation space,
and a decoder that reconstructs the signal using the encoded representation enriched by label in-
formation. Notably, the encoder is trained to efficiently encode signals containing any quantity of
contextual observations. That is, a trained encoder can encode an arbitrary number of online ob-
servations. This, in turn, facilitates an instantaneous update of predictions upon the arrival of new
data without the need for any retraining procedures. This advantage is underpinned by the strong
representation power achieved by using deep neural networks (DNNs) to parameterize the encoders
and the decoder. Our framework does not impose a requirement for all units to possess labels; it
remainsapplicableevenwhensomeunitshavemissinglabelinformation. Ourlabel-awaremodeling
not only significantly improves personalized predictions but also facilitates the joint prediction of
the unit’s future signal evolution and its label when missing. We also offer a data augmentation
approach for cases where the number of historical units is insufficient for training our model.
We summarize our contributions as follows.
• WeproposeanNP-basedframeworkforpredictivemodelingofCMdata. Ourmodelishighly
scalable and achieves instantaneous predictions and uncertainty quantification in real-time as
data is collected from an in-service unit.
• We propose label-aware modeling to seamlessly integrate label information from units into
our prognostic framework while allowing for missing label information. The integration is
shown to substantially enhance predictive power. Notably, our framework can jointly predict
future CM signal evolution and a unit’s label when it is missing.
• We validate our framework using synthetic data and a real-world reliability engineering
dataset. Results demonstrate the advantageous features of our model in fast adaptation,
3enhanced predictions, and uncertainty quantification. In particular, the case study under-
scores the appealing capability of our model to make joint predictions for curves and labels.
It also highlights the ability to make accurate predictions in the early stages, which may be
critical for early anomaly detection within in-service units.
The remainder of the paper is organized as follows. Sec. 2 sets the stage by introducing NPs
within our problem context. Sec. 3 introduces the proposed approach. Sec. 4 discusses relevant
literature. Sec. 5 assesses the proposed model using simulation and real-world data. Sec. 6
concludes the paper.
2 Background: NPs
The NP is a group of models that learn a distribution over functions [5]. Given a set of functions
as a training set, the training process of NPs can be viewed as learning a prior distribution over
the functions. Then in the prediction stage, it estimates a specific function by deriving a posterior
predictive distribution given its available observations. In the context of CM, the training process
learns a distribution over historical CM signals, and the prediction process estimates the CM signal
of the in-service unit based on its online observations. Before delving into our proposed model,
this section situates NPs within a general framework, where we introduce how NPs construct a
probabilistic model for a single arbitrary function.
2.1 Modeling a function using NPs
Consideranarbitrary regressionfunctionf : Rdin → Rdout sampledfromasetoffunctionsthatshare
the same domain. Let D
i
= (x i,y i) denote an input-output pair, indexed by i, where x
i
∈ Rdin
and output y
i
∈ Rdout are a respective input-output pair from the function. We will call such a
tuple an observation. The collection of available observations from the function is called contexts
D := {D } . The input-output pairs for which predictions are to be made are called targets
C i i∈C
D := {D } ; here C and T denote the index sets of contexts and targets, respectively. For
T i i∈T
conciseness, we further define collective notations x := {x } , x := {x } , y := {y } ,
C i i∈C T i i∈T C i i∈C
y := {y } , and D := D ∪D .
T i i∈T C T
The first member of the NP family was proposed by [6]. Motivated by the fact that the
true trajectory of a regression function should be independent of the order of its context points,
they model the targets as a conditional distribution given the contexts while being invariant
4to the contexts’ order. Specifically, the NP builds a distribution conditioned on u ∈ Rdu, a
C
finite-dimensional representation of the contexts D invariant to the permutation in C, that is,
C
p(y |x ,D ) := p(y |x ,u ). This is enabled by introducing a deep neural network u(·) that
T T C T T C
maps each context point to a finite-dimensional representation u(D ) = u after which a permu-
i i
tation invariant operation is performed over the representations. In particular, the NP employs
averaging u = 1 (cid:80) u and thus, u is invariant to the permutation of D . The likelihood
C |C| i∈C i C C
p(y |x ,u ) can be formulated as a Gaussian distribution factorized over the target points, rep-
T T C
resented as
(cid:89) (cid:89)
p(y |x ,D ) := p(y |x ,u ) = N(y ;µ ,σ2) = N(y ;µ (x ,u ),diag(σ2(x ,u ))),
T T C T T C i d d i d i C d i C
i∈T i∈T
such that mean µ and variance σ2 functions are parameterized by respective DNNs µ (·) and
d d d
σ2(·), where each outputs a vector in Rdout. The notation diag(a) indicates the diagonal matrix
d
corresponding to the vector a.
Notable subsequent work by [7] introduces a finite-dimensional global latent variable z ∈ Rd lat
characterizedbyafactorizedGaussian. ThislatentvariableversionofNPsisoftencalledthelatent
NP (LNP). By incorporating z, LNP can consider the potential uncertainty in mapping D to the
C
latent space. It models the likelihood as being conditioned on z, and then derives the marginal
likelihood by marginalizing z out. Specifically, the likelihood conditioning on z is expressed as
(cid:89)
p(y |x ,D ,z) = p(y |x ,u ,z) = N(y ;µ (x ,u ,z),diag(σ2(x ,u ,z))). (1)
T T C T T C i d i C d i C
i∈T
Unlike common latent variable models that involve a prior distribution of z in the marginalization,
LNPs employ a distribution of z conditioned on D . The rationale is to introduce a mapping of
C
the contexts D to the latent representation space in a way to be permutation-invariant to C. The
C
conditional distribution is written by
q(z|D ) := q(z|v ) = N(z;µ (v ),diag(σ2(v ))) (2)
C C e C e C
where v = 1 (cid:80) v is obtained in a similar way to u yet using a DNN v(·), after which v
C |C| i∈C i C C
is fed to the DNNs µ (·) and σ2(·) that parameterize the mean and variance of the factorized
e e
Gaussian q(z|D ), respectively. This results in that v is invariant to the permutation in D and
C C C
5so is q(z|D ) := q(z|v ). Given q(z|v ), the marginal likelihood modeled by LNP is obtained by
C C C
(cid:90)
p(y |x ,D ) := p(y |x ,u ,z)q(z|v )dz. (3)
T T C T T C C
In NP literature, u(·), v(·) and q(z|·) that encode contexts into a representation space are referred
to as the encoders, while p(y |x ,·) that reconstructs a function from a representation is referred
T T
to as the decoder. Fig. 1 illustrates an LNP model with the latent variable z.
Figure 1: A schematic illustration of LNP.
So far, we have seen how a single arbitrary signal is modeled within the NP framework. In the
next section, we will discuss our NP-based method that integratively models a CM signal along
with its label, and then see its estimation on a set of historical signals and labels.
3 Proposed Approach
Now we introduce LANP, our proposed model. In Sec. 3.1, we establish LANP built upon hierarchical
modeling and derive a variational lower bound for model inference using amortized VI. In Sec. 3.2,
we discuss some practical considerations in the implementation of LANP.
3.1 LANP: Label-aware Neural Processes
Consider a set of historical units/systems with collected CM signals from each. Suppose that
units are categorized into classes where the categorization is inherently reflected in the evolution of
6their CM signals. For instance, units can be tested as abnormal or normal, with their CM trends
exhibiting some heterogeneity according to their labels (i.e., normal vs. abnormal). We consider a
case such that label information is available only for some units. Our proposed model, LANP, trains
on available data for historical units in a way that incorporates the possibly incomplete labels into
the NP framework to facilitate joint learning of distributions for both signals and labels. This
results in the enhancement of signal predictions as well as the capability for label estimation. After
training LANP on historical data, LANP can perform real-time joint inference for (i) future CM signal
evolution and (ii) the label of an in-service unit, given its online observations as contexts.
3.1.1 Notations
We start with defining notations. Consider units/systems indexed by j ∈ J := J ∪ {r} that
all
comprises the index set for historical units J = {1,...,J} and an in-service unit {r} ̸⊂ J. While
each unit collects its own CM signal, the full CM signal is available for the historical units j ∈ J,
yet an online CM signal collected only until the present time is available for the in-service unit
r. Now, let us focus on the CM signals of historical units j ∈ J. We denote the index set of
contexts and targets among signal observations of unit j as Cj and Tj, respectively. Without loss
of generality, we randomly split all available CM observations Dj into contexts and targets for each
historical unit. That is, we denote the targets by Dj := {Dj} = {(xj,yj)} and the context
T i i∈Tj i i i∈Tj
by Dj := {Dj} = {(xj,yj)} where Dj = Dj ∪ Dj . While for now we generally regard
C i i∈Cj i i i∈Cj C T
contexts as any random subset, we will shortly discuss a practical selection strategy in the training
stage in Sec. 3.2.
In addition to CM signals, suppose class labels cj ∈ {1,...,L} are available for some historical
units j ∈ J ⊂ J whereas unavailable for units j ∈ J ⊂ J. Note that J ∪ J = J and
L U L U
J ∩J = ∅. Encompassing both labels and CM signals, we can collectively define all available
L U
historical data as DJ := (cid:0) {cj} ,{Dj} (cid:1) . Here, what we are interested in, is building an NP
j∈J j∈J
L
model that exploits historical data DJ to predict the future CM signal trajectory of the in-service
unit r as well as its label, given its real-time observations collected as contexts Dr.
C
Recall that in CM, xj often denotes the i-th observational time point at which a continuous
i
signal yj (such as degradation) from unit j is recorded. Nevertheless, it’s important to note that
i
xj ∈ Rdin need not solely represent temporal aspects; it can also incorporate various other inputs.
i
Additionally, our methodology does not presuppose uniform data collection time points across
units, nor does it require units to share the same number of observations.
73.1.2 Model development
Now let us build LANP. Our goal is to learn an NP model capable of mapping the target points Dj
T
by leveraging the available contexts Dj as well as label information cj if available1. We start by
C
discussing modeling an arbitrary single historical unit that is with or without label information.
Then, we will build an integrated model that encompasses all historical units. For notational
brevity, we will temporarily drop j from the notations when discussing modeling for a CM signal
from a single arbitrary unit j with or without label information.
Modeling a single CM signal with label information. Let us first consider an arbitrary
historical unit with label information. Specifically, we consider a likelihood that includes a latent
variable z written as
(cid:89)
p(y |x ,c,u ,z) := N(y ;µ (x ,c,u ,z),σ2(x ,c,u ,z)), (4)
T T C i d i C d i C
i∈T
conditioning on both D and c. We then introduce a conditional distribution for z and c by
C
conditioning on D as follows:
C
q(c|D ) := q(c|w ) = cat(c;ϕ(w )) (5)
C C C
q(z|c,D ) := q(z|c,v ) = N(z;µ (c,v ),diag(σ2(v ))) (6)
C C e C e C
where w = 1 (cid:80) w(D ) is a finite-dimensional representation obtained similarly to u =
C |C| i∈C i C
1 (cid:80) u(D ) and v = 1 (cid:80) v(D ) but with a DNN w(·) and thus, invariant to the permu-
|C| i∈C i C |C| i∈C i
tation of D ; ϕ(·) indicates a DNN that outputs a probability parameter vector of the categorical
C
distribution with L classes.
The underlying intuition of introducing the above conditional distributions is as follows. Eq.
(5) implies that estimating the DNNs w(·) and ϕ(·) in q(c|D ) allows for inferring a unit’s label
C
based on its available CM observations D . Eq. (6) presents that the distribution of z is inferred
C
based on both label information and available CM observations of the unit. In particular, the
mean depends on both c and D , which encourages the distributions for z of different units to be
C
clusteredaccordingtotheirlabels. ThisissensibleinthatbothcandD wouldbeneededtomodel
C
the class-level heterogeneity of CM signals. While, the variance depends on D only as D would
C C
1The notation cj is abused to denote its corresponding one-hot encoding when it is used as an input to a DNN.
8suffice to infer unit-level deviations within the class.
Given (4), (5), and (6), the marginal likelihood is given by
(cid:90)
p(y ,c|x ,D ) := q(c|w ) p(y |x ,c,u ,z)q(z|c,v )dz. (7)
T T C C T T C C
Fig. 2 illustrates the encoding and decoding process of LANP.
Figure 2: A schematic illustration of LANP.
Parameterizing the likelihood by DNNs renders the marginalization (7) for maximum likeli-
hood intractable. Instead, we resort to amortized variational inference (VI) [8]. The framework
maximizes a lower bound of the marginal log-likelihood, referred to as the evidence lower bound
(ELBO), as an alternative to the original marginal log-likelihood. We refer the reader to [8] for
details of VI. The derivation of the ELBO builds upon Jensen’s inequality, written as
logp(y ,c|x ,D )
T T C
(cid:90)
p(y |x ,c,u ,z)q(z|c,v )
T T C C
= log q(z|c,v )dz+logq(c|w )
T C
q(z|c,v )
T
(cid:88)
≥ E [logp(y |x ,c,u ,z)]−KL(q(z|c,v )∥q(z|c,v ))+logq(c|w ) =: L (θ;c,D),
q(z|c,vT) i i C T C C L
i∈T
(8)
9with v = 1 (cid:80) v(D ). This bound will be used shortly to establish an integrated lower bound
T |T| i∈T i
for both cases with or without label information. Note that we will abuse the notation θ to denote
the collection of all parameters in the encoders and decoders to be estimated.
Modeling a CM signal without label information. Now,wediscussthecaseofunitswithout
label information. As we do not have labels, c is regarded as a latent variable that needs to be
estimated. The marginal likelihood becomes
(cid:90)
p(y |x ,D ) := p(y |x ,c,u ,z)q(z|c,v )q(c|w )dzdc. (9)
T T C T T C C C
Note that Eq. (9) is different from Eq. (7) in that it marginalizes out c. The lower bound of (9) is
derived by
(cid:90)
p(y |x ,c,u ,z)q(z|c,v )q(c|w )
T T C C C
logp(y |x ,D ) := log q(z|c,v )q(c|w )dzdc
T T C T T
q(z|c,v )q(c|w )
T T
L
(cid:88)
≥ − q(c = l|w )L (θ;c,D)+H(q(c|w )) =: L (θ;D), (10)
T L T U
l=1
where H(·) denotes the entropy, that is, H(−q(c|w )) := −(cid:80)L q(c = l|w )logq(c = l|w ); w
T l=1 T T T
is calculated similarly to w but for D .
C T
The integrated model and its estimation. Thus far, we have derived the lower bound (8)
and (10) for a single arbitrary unit with and without a label, respectively. Now, let’s build an
integrated lower bound for the entire dataset DJ for all historical units j ∈ J that encompasses
both labeled and unlabeled units. This is expressed as
(cid:88) (cid:88)
L (θ;DJ) = L (θ;cj,Dj)+ L (θ;Dj). (11)
LU L U
j∈J j∈J
L U
The estimation of θ is done by solving max L (θ;DJ) using stochastic optimization with the
θ LU
help of the reparameterization trick [9]. In practice, a set of units JB ⊂ J and JB ⊂ J is
L L U U
randomly sampled to form a batch JB := JB ∪JB at each optimization iterate to approximate
L U
(11). In doing so, one can randomly split Dj into Dj and Dj for each unit j ∈ JB.
T C
Joint prediction. AftertrainingLANPonthehistoricaldata,LANPcanmakeajointpredictionfor
the CM signal trajectory and the label for in-service unit r. Given available online observations Dr
C
10as context, the label is inferred by cˆr = argmax {ϕr} where [ϕr]⊤ is the output vector
l l l=1,...,L l l=1,...,L
of ϕ(wr) with wr = 1 (cid:80) w(Dr). The predicted mean and variance of y at the arbitrary
C C |Cr| i∈Cr i ∗
input x are calculated by
∗
K (cid:32) K (cid:33)2
1 (cid:88) 1 (cid:88)
E[y ] ≈ yˆ ; var[y ] ≈ yˆ2 − yˆ +σˆ2, (12)
∗ ∗ ∗ K ∗k K ∗k ∗
k=1 k=1
with yˆ = µ (x ,cˆr,ur,µr) and σˆ2 = σ (x ,cˆr,ur,µr) with µr = µ (cˆr,vr), where ur and vr
∗ d ∗ C e ∗ d ∗ C e e e C C C
are representations of Dr encoded by u(·) and v(·), respectively. Here yˆ can be calculated
C ∗k
similarly to yˆ yet a sample zr ∼ q(z|cˆr,vr) is passed through the decoder instead of µr, that is,
∗ k C e
(cid:16) (cid:17)2
yˆ = µ (x ,cˆr,ur,zr). One can interpret var[y ] as that 1 (cid:80)K yˆ2 − 1 (cid:80)K yˆ captures
∗k d ∗ C k ∗ K k=1 ∗k K k=1 ∗k
theuncertaintyimposedbyencodingcontextstothelatentspace, whileσˆ2 estimatesinherentnoise
∗
in the observations [10]. Note that, in case label information cr is known, we can simply replace cˆr
with cr in the prediction process above.
3.1.3 Modeling advantages
LANP has critical advantageous properties as a means for predictive modeling of CM signals.
Fast adaptation. CM signals are often acquired in real-time. It necessitates instantaneous up-
dates on the predictive model to incorporate newly arrived data on the fly. LANP achieves this very
efficiently without model retraining, by simply passing a new set of online observations Dr through
C
the encoders and decoder already learned.
Label-awareness. Our label-aware modeling seamlessly integrates label inference and regression
within the VI framework. This facilitates enhancing signal predictions for units using their label
information (e.g., system abnormality/status) or identifying their labels based on signal evolution
when the labels are missing.
Representation power and flexibility. Modeling CM signals assuming a parametric form
is becoming more challenging due to the increased complexity and non-stationarity of systems
nowadays. By parameterizing distributions by DNNs, LANP is endowed with strong representation
power and flexibility that facilitate learning a wide range of function-generating distributions.
11Uncertainty estimation. In CM-based reliability engineering, quantifying uncertainty in pre-
dictions is crucial for subsequent decision-making, such as maintenance planning or inventory man-
agement. LANP that includes a latent variable can capture inherent measurement noises and also
uncertainty in the encoding process.
3.2 Practical considerations
We discuss some considerations for practitioners to implement our proposed approach in practice.
Sampling contexts in training. Typically, inputs from CM signals represent observation time
points. Assumingthatsignalsarefunctionsdefinedovertheinputdomain[0,τ],oursettinginvolves
estimating the function fr for in-service unit r across the entire domain [0,τ] based on its online
observations Dr where xr ∈ [0,τ˜] with the latest monitoring time τ˜ < τ. Thus, employing a
C i
na¨ıve random sampling approach to create contexts {Dj} distributed throughout the entire
C j∈J
domain [0,τ] during training does not reflect online observations of unit r collected up to a specific
time, often leading to inefficient learning. A simple solution is drawing τ∗ ∼ unif(0,τ) at each
optimization iterate and randomly choosing a certain number of observations available in [0,τ∗] to
form contexts. As such, this context sampling strategy allows the model to learn how to predict
the entire curve over [0,τ] based on online observations collected till a certain time point.
A regularized objective function. To further enhance learning of the inference network for
labels q(c|w ), we can add a term to regularize the objective function [11]:
C
L˜ (θ;DJ) = L (θ;DJ)+λ
(cid:88) (cid:104) −logq(cj|wj)(cid:105)
,
LU LU C
j∈J
L
which assigns an extra weight to −logq(c|w ). This encourages learning q(c|w ) based on the
C C
labeled signals j ∈ J . In general, we found λ = 0.1 works well, as in [11].
L
Attention modules. Incorporatingattentionmodules[12]intoNPsisapopularwaytoalleviate
a well-known underfitting issue in NPs [13]. We place self-attention layers to the encoder DNNs
u(·),v(·), and w(·) after their initial fully-collected layers. Also, we further replace the averaging
operator for u with a cross-attention module across the context representations {u(Dj)} and
C i i∈Cj
a target input xj with i ∈ Tj; thus, u depends on xj. The self-attention module can produce
i C i
12a richer representation by modeling interactions within context points. Furthermore, the cross-
attention module helps place stronger attention on the context points close to the location of the
target point, encouraging the prediction at the target point xj to be closer to the outputs of the
i
contexts nearby. Such a structure is adopted in our numerical studies in Sec. 5.1. We provide a
pictorial description of the attention module-integrated structure in Appendix A.
Functional data augmentation. A key challenge in training NPs is the need for enough func-
tional observations. The challenge comes from the nature of NPs, which directly learns a dis-
tribution over functions. In the context of CM, this implies that we may need many historical
units to train LANP, which may not always align with real-world scenarios. Such an issue becomes
further challenging when CM signals exhibit highly nonparametric trends or heterogeneity across
units, which requires more function samples to learn the underlying function-generating distribu-
tion. Appendix B and C provide a practical data augmentation framework based on functional
data analysis to alleviate this issue, along with a simulation study demonstrating its usefulness.
4 Related work
A large body of literature exists on the predictive modeling of CM signals and NPs. As such, this
section discusses studies particularly related to our work. For comprehensive reviews, please see
the excellent review paper for predictive modeling in CM [14] and for the NP family [5].
Personalized predictions for CM signals. One natural way to achieve personalized predic-
tions for an individual CM signal is based on a two-step approach. The first step characterizes the
general trend from historical CM data. Then, the trend is gradually refined to model an in-service
unit individually. A popular approach uses a mixed-effect model [1]. They estimate mixed-effect
coefficients using historical CM signals. Then, the random effects are updated from online obser-
vations using empirical Bayes. Along this line, many extensions have been explored. Those include
studies that account for heterogeneity in historical CM signals [15], external sources of system fail-
ures [16], time-varying system degradation rates [17], and multiple phases in degradation processes
[18, 19], to name a few. Note that the methods above assume a parametric form for signals or a
linear mean degradation path [20]. While facilitating fast adaptation to online data, such paramet-
ric modeling results in severe vulnerability to model misspecification. To tackle this, another line
of two-step approaches inspired by functional data analysis has been proposed [21, 4, 22]. They
13estimate eigenfunctions from historical CM signals and express each signal as a linear combination
of the eigenfunctions. Then, personalization is done by updating the coefficients based on the
online data. In contrast to parametric model-based approaches, the alternative methods do not
impose a restriction on CM signals to be modeled as a parametric form or a monotonic mean trend.
Unfortunately, these approaches suffer from limited scalability in the number of historical units.
Alternatives to two-step modeling directly derive a predictive distribution given both historical
and online observations. In this regard, our model falls within this category. A notable approach
in this category is based on multi-output Gaussian processes (GPs) [20]. It pools all training
and testing CM signals into a large multivariate GP, through which information transfer occurs
from the historical to the testing units. Based on this idea, multiple studies further improve the
model by incorporating auxiliary data [3], adaptively selecting more informative units [23], and
distributing model inference efforts to individual units while preserving their privacy [24], among
others. They all are capable of personalized predictions using non-parametric modeling, a key
benefitinherited fromGPs. However, theirpersonalization processintroduces asubstantial latency
for online updates, as it requires model retraining each time a new data point is obtained. Needless
to say, this is a tedious task, especially for high complex models such as GPs. Recent studies
propose online learning schemes for multi-output GPs to avoid model retraining from scratch [e.g.,
25, 26]. Nonetheless, their iterative and nonparallelizable nature greatly impedes their use when
the number of online observations is excessive.
NPs and their applications. Recently NPs have drawn significant attention due to their
rich expressiveness, scalability in both training and test stages, and the ability to learn meta-
representations [5]. After their first introduction by [6], multiple studies have made extensions in
various directions. For example, studies aim to capture global uncertainties in the encoding process
[7, 27], mitigate underfitting to the context points [13, 28, 29], account for correlations across func-
tions [30, 31] or across target output points [32], ensure invariant predictions under input shifts or
transformations [33]. Built upon the methodological advances above, NPs have been widely used
in a range of domains, including clinical data analysis [34], climate science [35], image classification
[36], robotics [37, 38], and so on. Despite that, their applications in the predictive modeling of CM
signals have yet to be investigated.
In the sense of using incomplete label information, there are few recent studies relevant to
LANP. A study proposed an NP-based model, called SNPAD, for anomaly detection using partially
14labeled data [39]. SNPAD includes an encoder that maps both labeled and unlabeled observations
into a latent space, followed by a decoder that estimates an anomaly score of each observation.
Another related study in this regard proposes NP-Match [36], an NP-based model designed for
semi-supervised image classification tasks where labeled and unlabeled observations are regarded
as contexts and targets, respectively. While NP-Match, SNPAD, and LANP consider both labeled
and unlabeled observations simultaneously, a direct comparison is not straightforward. This is due
to their different objectives and settings where LANP (i) focuses on real-time personalization in CM
and (ii) provides joint predictions for functions and their labels, whereas SNPAD and NP-Match
aim to predict anomaly scores or labels only.
5 Model validation
In this section, we design and discuss numerical studies to validate our proposed model using both
simulated and real-world data. Sec. 5.1 assesses our proposed approach using simulated data. Sec.
5.2 tests our model on a reliability engineering application about CM-based prognosis for lithium-
ion batteries. We note that Appendix C includes an additional simulation study that validates our
proposed functional data augmentation scheme.
5.1 Simulation study
Our simulation study is designed to answer the following questions. (i) Can LANP leverage label
information to predict heterogeneous future trends of signals when early-stage online data is in-
sufficient to reveal the future heterogeneity? (ii) Can LANP outperform benchmark models over
scenarios where varying proportions of observations are labeled?
Benchmark models. We compare LANP with the benchmark models denoted as follows.
• ANP: an attentive NP model [13]. This model builds upon LNP, including both self- and cross-
attention modules. The comparison of LANP with ANP will highlight the benefit of leveraging
labels.
• VMGP: a variational multi-output GP model [40]. This model builds cross-correlation between
units based on convolution processes. Note that we chose a variational version of multi-
output GPs to make it scalable to our numerical study. When online data arrives, VMGP needs
retraining for an updated prediction.
15For a fair comparison, ANP has a similar network configuration to LANP, where both are equipped
with attention modules and d = 8. Detailed settings can be found in Appendix A.
lat
Setup. We consider two groups of signals generated from data generation models:
• Group I: y = 0.3x2−2sin(b πx)+b +ϵ, x ∈ (3,10].
1 2

 0.3x2−2sin(b 1πx)+b 2+ϵ, x ∈ (0,3],
• Group II: y =
 1.8x2−2sin(b πx)+b −2.7+ϵ, x ∈ (3,10],
1 2
with b ∼ unif(0.35,0.45), b ∼ unif(0,3), and the white noise ϵ ∼ N(0,0.032). Samples of the
1 2
signals are displayed in the first column in Fig. 3. It is noteworthy that both data-generating
models share a common underlying function in the range x ∈ (0,3]. We call this as the dormant
stage. This is common in CM practices, where the future evolution of CM signals is dependent
on different operational modes or configurations (i.e., labels), but their heterogeneity is not easily
detected in the early stage [4]. Predicting a future trend solely based on online observations at
x ∈ (0,3] would be very challenging in such a case. By comparing LANP with benchmark models,
we can evaluate if LANP can effectively leverage label information and, therefore, accurately predict
future heterogeneous trends.
Training samples ANP prediction LANP prediction
Dormant Online observation
phase True curve
Predictive mean
I
p
Prediction interval
u
o
r
G
Dormant
phase
II
p
u
o
r
G
Figure 3: Predictions by ANP and LANP (α = 0.3,γ = 0.25).
WetrainLANPandANPthroughout25,000iterations. Ateachiterationwegenerateabatchof16
signals, comprised of 8 signals from each group, to train the models. We consider partially labeled
cases by randomly removing group labels for 16(1−γ) signals for γ ∈ {0,0.25,0.5,0.75,1}. That
16is, setting γ = 0 leads to the case without any group information, whereas γ = 1 represents that
the group information of all signals is available. A generated signal comprises 45 observations. To
form contexts for the signal, a random number of observations greater than 2 and less than 15 are
selectedinawaydescribedinSec. 3.2,whiletheremainderissettotargetobservations. Meanwhile,
we generate 100 training signals from each group to train VMGP. This restriction is imposed due
to the rapid unscalability of VMGP as J increases. Yet, we observed that training VMGP on more
than 100 training signals does not lead to a substantial increase in its predictive performance, and
thus a fair comparison is still available. We validate the models on 20 test signals for each group.
For a test signal, we generate 20 observations evenly spaced over x ∈ (0,10). We consider the
“α-degradation” stage: where observations in the first 100α% of the entire trajectory are collected
as online data that forms contexts in NP-based models. Evaluation for model predictions is based
on the root mean squared of errors (RMSE) metric at 400 points evenly spaced over x ∈ (0,10).
Results. Now we discuss experimental results. Table 1 includes average RMSEs over 20 test sig-
nals for the compared models with α = 0.3,0.5,0.7. Based on the results, we can obtain important
insights. Specifically, LANP significantly outperforms ANP when γ > 0, that is, label information is
provided. ThisdemonstratesLANP’scapabilityoflabel-awarenessthatcontributestomoreaccurate
signal prediction. Fig. 3 illustrates the predictions of LANP and ANP for two representative signals
from Group I and II, when current online observations are not sufficient to distinguish signals from
different groups. As shown, LANP can distinguish heterogeneity in the future evolution based on
label information, while ANP fails to do so. We also emphasize that such merits in LANP can be
achieved even with a small portion of labeled signals in the training dataset (γ = 0.25). Meanwhile,
it is not surprising that the predictive accuracy of LANP improves as more online observations or
labeled signals are provided.
Table 1: Average RMSEs over different degradation stages (group I).
Models γ α = 0.3 α = 0.5 α = 0.7
VMGP - 0.406 (0.181) 0.226 (0.200) 0.189 (0.045)
ANP - 0.187 (0.019) 0.106 (0.090) 0.025 (0.012)
0 0.245 (0.027) 0.144 (0.094) 0.076 (0.020)
0.25 0.022 (0.011) 0.017 (0.003) 0.011 (0.004)
LANP 0.5 0.014 (0.005) 0.009 (0.002) 0.011 (0.002)
0.75 0.018 (0.007) 0.009 (0.002) 0.009 (0.002)
1 0.020 (0.011) 0.007 (0.002) 0.007 (0.003)
To investigate the label-awareness ability of LANP, Fig. 4 depicts 2-dimensional embeddings of
17Figure 4: t-SNE embeddings of representations at the dormant stage (α = 0.3).
the means of z in the representation space. We generate online observations of 100 test signals
from each group at the dormant stage and plot the t-SNE embeddings [41] of their 8-dimensional
representations encoded by ANP and LANP. We directly see from Fig. 4 the means of z encoded by
ANP (i.e., µ (vr)) are not clustered by different groups, as signals are not distinguishable with their
e C
online observations at the dormant stage. On the other hand, LANP can cluster the means of z (i.e.,
µ (cr,vr)) by their groups using their label information c. Such clustering in the representation
e C
space results in distinguished decodings by groups. Consequently, LANP is able to differentiate
predictions for future trajectories according to their groups.
Table 2 provides times in seconds taken by each model to update its prediction using online
observations. WecanobservethatLANPandANPdramaticallyreducethetimeforupdatecompared
to VMGP. This sheds light on the critical ability of LANP to facilitate real-time inference in online
regimes. LANP does not require retraining at runtime when adapting to new online observations.
Instead, the adaptation process simply passes new observations through already trained DNNs
and thus is parallelizable. As such, real-time adaptation remains scalable even when many online
observations are introduced simultaneously. This highlights LANP’s potential within a range of real-
world CM applications where real-time updates are instrumental. In addition to these merits, LANP
comes with strong representation power resulting from the parameterization using DNNs, which
in turn results in more accurate signal predictions as shown in Table 1. Finally, we note that an
additional simulation study on the functional data augmentation scheme is provided in Appendix
C.
18Table 2: Times in seconds for online updates. Figures indicate times averaged through the updates
for 20 online observations.
VMGP ANP LANP
Time 58.43 (0.05) 0.0033 (0.00) 0.0051 (0.00)
5.2 Case study: data-driven prognosis and anomaly detection for lithium-ion
batteries
The case study presents an application of LANP in the prognosis of lithium-ion batteries, where a
decrease in capacity characterizes battery degradation. The reliable operation of systems/devices
withalithium-ionbatteryhingesontimelymaintenanceorreplacement. Inthisregard, itiscrucial
to achieve an accurate estimation of future degradation trends and early detection of anomalies.
To this end, data-driven approaches build a prediction model based on data from both historical
batteries and the in-service battery. A critical challenge is that capacity degradation of the in-
service battery is often collected in real-time through advanced IoT sensors. Hence, the capability
of rapid adaption to incoming data streams is imperative.
Dataset. We use the CALCE battery anomaly detection dataset [42]. It contains capacity de-
crease trajectories obtained from 23 battery cells. Among them, 14 batteries are regarded as
representative of the qualified production lot, whereas 9 batteries are categorized as disqualified.
That is, the label of a battery denotes its qualification. While initially presenting as healthy, the
disqualification is inherently manifested at the later stage as abnormal degradation trends unac-
ceptably deviating from a normal trend. Details can be found in [42].
Setup. In our experiment, we carry out leave-one-out cross-validation to evaluate models. Thus,
we assess models over 23 cases, each choosing one battery cell for testing. We aim (i) to fore-
cast the future evolution of the in-service battery’s capacity and (ii) to determine its qualification
status (qualified or not). The benchmark models take a two-stage approach to detect anomalies:
estimating a degradation curve, and categorizing it whether qualified or not using a classification
algorithm. We adopt k-nearest neighbors (KNN) or support vector machine (SVM) as the clas-
sification algorithm. We also augment the training data using the proposed data augmentation
method, discussed in Appendix B.
19Results. ResultsarepresentedinTable3, Fig. 5, andFig. 6. Weseethat, LANPoutperformsANP
in curve predictions over different degradation phases. It demonstrates that our label-awareness
modeling leads to better learning of the function-generating distribution. This comes with well-
quantified predictive uncertainties, as presented in Fig. 5. Furthermore, Fig. 6 presents that LANP
improvesdetectinganomaliesofbatteriesoverthebenchmarkmodels. Thisrevealstheadvantageof
our joint estimation for curves and labels, compared to the two-stage approaches. It is particularly
promisingtoseeLANP’sabilitytodetectanomaliesatanearlydegradationstage. Importantly, such
advantages of LANP come along with its real-time adaptability. This further highlights the strong
potential of LANP in reliability engineering for advanced systems with real-time CM data.
Table 3: Average RMSEs of future signal predictions given different proportions of available online
observations.
Models α = 0.3 α = 0.5 α = 0.7
VMGP 0.0912 (0.0814) 0.0693 (0.0391) 0.0501 (0.0321)
ANP 0.0922 (0.0936) 0.0616 (0.0495) 0.0493 (0.0354)
LANP 0.0824 (0.0611) 0.0560 (0.0377) 0.0469 (0.0318)
Figure 5: CM signal predictions over different proportions of available online observations (battery
cell ID: #23).
6 Conclusion
The successful deployment of modern digital-twin technologies and cyber-physical systems hinges
upon real-time adaptation to system changes. Endeavors to develop predictive models capable of
20Figure 6: Accuracy of label prediction over different numbers of online observations. ANP-SVM
denotes the benchmark model that first estimates signals using ANP-KNN and then classifies them
using SVM to determine qualifications. Similar notation applies to ANP-KNN.
rapid prediction updates on real-time online data are often challenged by the trade-off between
representation power and agility to new data. Our study is able to tackle the trade-off, achieving
both capabilities simultaneously.
This paper presents an approach for instantaneous updates of predictions personalized to in-
dividual units based on their online CM data. Building upon the NP framework, our model can
attain both real-time prediction updates and strong representation power. This is achieved along
with improved predictions through label-aware modeling that enables the use of label information
of curves that may be incomplete, as well as inference of labels in case of missingness. Numerical
studies demonstrate that our model is capable of accurate curve predictions with well-quantified
uncertainty, joint inference of curves and labels, and real-time updates of predictions based on
online observations.
References
[1] Nagi Z Gebraeel, Mark A Lawley, Rong Li, and Jennifer K Ryan. Residual-life distributions
from component degradation signals: A bayesian approach. IiE Transactions, 37(6):543–557,
2005.
[2] Yongzhi Zhang, Rui Xiong, Hongwen He, and Michael G Pecht. Long short-term memory
recurrent neural network for remaining useful life prediction of lithium-ion batteries. IEEE
Transactions on Vehicular Technology, 67(7):5695–5705, 2018.
21[3] RaedKontar,ShiyuZhou,ChaitanyaSankavaram,XinyuDu,andYiluZhang. Nonparametric-
condition-based remaining useful life prediction incorporating external factors. IEEE Trans-
actions on Reliability, 67(1):41–52, 2017.
[4] Seokhyun Chung and Raed Kontar. Functional principal component analysis for extrapolating
multistream longitudinal data. IEEE Transactions on Reliability, 70(4):1321–1331, 2020.
[5] Saurav Jha, Dong Gong, Xuesong Wang, Richard E Turner, and Lina Yao. The neural process
family: Survey, applications and perspectives. arXiv preprint arXiv:2209.00517, 2022.
[6] MartaGarnelo, DanRosenbaum, ChristopherMaddison, TiagoRamalho, DavidSaxton, Mur-
ray Shanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural pro-
cesses. In International conference on machine learning, pages 1704–1713. PMLR, 2018.
[7] Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Es-
lami, and Yee Whye Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018.
[8] Cheng Zhang, Judith Bu¨tepage, Hedvig Kjellstr¨om, and Stephan Mandt. Advances in varia-
tional inference. IEEE transactions on pattern analysis and machine intelligence, 41(8):2008–
2026, 2018.
[9] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
[10] Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for
computer vision? Advances in neural information processing systems, 30, 2017.
[11] Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-
supervised learning with deep generative models. Advances in neural information processing
systems, 27, 2014.
[12] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
L(cid:32) ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems, 30, 2017.
[13] Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosen-
baum, Oriol Vinyals, and Yee Whye Teh. Attentive neural processes. arXiv preprint
arXiv:1901.05761, 2019.
22[14] Xiao-Sheng Si, Wenbin Wang, Chang-Hua Hu, and Dong-Hua Zhou. Remaining useful life
estimation–a review on the statistical data driven approaches. European journal of operational
research, 213(1):1–14, 2011.
[15] Tao Yuan and Yizhen Ji. A hierarchical bayesian degradation model for heterogeneous data.
IEEE Transactions on Reliability, 64(1):63–70, 2014.
[16] Hongda Gao, Lirong Cui, and Qingan Qiu. Reliability modeling for degradation-shock de-
pendence systems with multiple species of shocks. Reliability Engineering & System Safety,
185:133–143, 2019.
[17] Xiao-Sheng Si, Wenbin Wang, Chang-Hua Hu, and Dong-Hua Zhou. Estimating remaining
useful life with three-source variability in degradation modeling. IEEE Transactions on Reli-
ability, 63(1):167–190, 2014.
[18] YuxinWen,JianguoWu,DevashishDas,andTzu-LiangBillTseng. Degradationmodelingand
rul prediction using wiener process subject to multiple change points and unit heterogeneity.
Reliability Engineering & System Safety, 176:113–124, 2018.
[19] Hongyu Wang, Xiaobing Ma, and Yu Zhao. A mixed-effects model of two-phase degradation
process for reliability assessment and rul prediction. Microelectronics Reliability, 107:113622,
2020.
[20] RaedKontar, ShiyuZhou, ChaitanyaSankavaram, XinyuDu, andYiluZhang. Nonparametric
modelingandprognosisofconditionmonitoringsignalsusingmultivariategaussianconvolution
processes. Technometrics, 60(4):484–496, 2018.
[21] Rensheng R Zhou, Nicoleta Serban, and Nagi Gebraeel. Degradation modeling applied to
residual lifetime prediction using functional data analysis. The Annals of Applied Statistics,
pages 1586–1610, 2011.
[22] Amirhossein Fallahdizcheh and Chao Wang. Transfer learning of degradation modeling and
prognosis based on multivariate functional analysis with heterogeneous sampling rates. Relia-
bility engineering & system safety, 223:108448, 2022.
[23] Xinming Wang, Chao Wang, Xuan Song, Levi Kirby, and Jianguo Wu. Regularized multi-
output gaussian convolution process with domain adaptation. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 45(5):6142–6156, 2022.
23[24] Seokhyun Chung and Raed Al Kontar. Federated multi-output gaussian processes. Techno-
metrics, pages 1–14, 2023.
[25] Le Yang, Ke Wang, and Lyudmila Mihaylova. Online sparse multi-output gaussian process
regression and learning. IEEE Transactions on Signal and Information Processing over Net-
works, 5(2):258–272, 2018.
[26] Zhiyong Hu and Chao Wang. Nonlinear online multioutput gaussian process for multistream
data informatics. IEEE transactions on industrial informatics, 18(6):3885–3893, 2021.
[27] Juho Lee, Yoonho Lee, Jungtaek Kim, Eunho Yang, Sung Ju Hwang, and Yee Whye Teh.
Bootstrapping neural processes. Advances in neural information processing systems, 33:6606–
6615, 2020.
[28] Mingyu Kim, Kyeongryeol Go, and Se-Young Yun. Neural processes with stochastic attention:
Paying more attention to the context dataset. arXiv preprint arXiv:2204.05449, 2022.
[29] Tung Nguyen and Aditya Grover. Transformer neural processes: Uncertainty-aware meta
learning via sequence modeling. arXiv preprint arXiv:2207.04179, 2022.
[30] Gautam Singh, Jaesik Yoon, Youngsung Son, and Sungjin Ahn. Sequential neural processes.
Advances in Neural Information Processing Systems, 32, 2019.
[31] Jaesik Yoon, Gautam Singh, and Sungjin Ahn. Robustifying sequential neural processes. In
International Conference on Machine Learning, pages 10861–10870. PMLR, 2020.
[32] Stratis Markou, James Requeima, Wessel Bruinsma, and Richard Turner. Efficient gaussian
neural processes for regression. arXiv preprint arXiv:2108.09676, 2021.
[33] Jonathan Gordon, Wessel P Bruinsma, Andrew YK Foong, James Requeima, Yann Dubois,
and Richard E Turner. Convolutional conditional neural processes. arXiv preprint
arXiv:1910.13556, 2019.
[34] Seyed Mostafa Kia and Andre F Marquand. Neural processes mixed-effect models for deep
normative modeling of clinical neuroimaging data. In International Conference on Medical
Imaging with Deep Learning, pages 297–314. PMLR, 2019.
[35] Anna Vaughan, Will Tebbutt, J Scott Hosking, and Richard E Turner. Convolutional condi-
tional neural processes for local climate downscaling. arXiv preprint arXiv:2101.07950, 2021.
24[36] Jianfeng Wang, Thomas Lukasiewicz, Daniela Massiceti, Xiaolin Hu, Vladimir Pavlovic, and
Alexandros Neophytou. Np-match: When neural processes meet semi-supervised learning. In
International Conference on Machine Learning, pages 22919–22934. PMLR, 2022.
[37] Ruijie Chen, Ning Gao, Ngo Anh Vien, Hanna Ziesche, and Gerhard Neumann. Meta-learning
regrasping strategies for physical-agnostic objects. arXiv preprint arXiv:2205.11110, 2022.
[38] Yumeng Li, Ning Gao, Hanna Ziesche, and Gerhard Neumann. Category-agnostic 6d pose
estimation with conditional neural processes. arXiv preprint arXiv:2206.07162, 2022.
[39] Fan Zhou, Guanyu Wang, Kunpeng Zhang, Siyuan Liu, and Ting Zhong. Semi-supervised
anomalydetectionvianeuralprocess. IEEETransactionsonKnowledgeandDataEngineering,
2023.
[40] Mauricio Alvarez and Neil Lawrence. Sparse convolved gaussian processes for multi-output
regression. Advances in neural information processing systems, 21, 2008.
[41] LaurensVanderMaatenandGeoffreyHinton. Visualizingdatausingt-sne. Journalofmachine
learning research, 9(11), 2008.
[42] Jinwoo Lee, Daeil Kwon, and Michael G Pecht. Reduction of li-ion battery qualification time
based on prognostics and health management. IEEE Transactions on industrial electronics,
66(9):7310–7315, 2018.
[43] J. O. Ramsay and B. W. Silverman. Functional Data Analysis. Springer, NY, 2nd edition,
2005.
[44] Jeff Goldsmith, Fabian Scheipl, Lei Huang, Julia Wrobel, Chongzhi Di, Jonathan Gellar,
Jaroslaw Harezlak, Mathew W. McLean, Bruce Swihart, Luo Xiao, Ciprian Crainiceanu, and
Philip T. Reiss. refund: Regression with Functional Data, 2023. R package version 0.1-32.
[45] Carlos Ramos-Carren˜o, Jos´e Luis Torrecilla, Miguel Carbajo-Berrocal, Pablo Marcos, and
Alberto Su´arez. scikit-fda: a python package for functional data analysis. arXiv preprint
arXiv:2211.02566, 2022.
[46] ArthurPDempster, NanMLaird, andDonaldBRubin. Maximumlikelihoodfromincomplete
data via the em algorithm. Journal of the royal statistical society: series B (methodological),
39(1):1–22, 1977.
25[47] Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning,
volume 4. Springer, 2006.
APPENDIX
A Model configuration
Fig. A.1 illustrates the configuration of LANP’s encoders equipped with attention modules [12] used
in the numerical studies. Note that all notations but functions (u,v,w,µ ,σ ,ϕ) in the figure omit
e e
the index j. The encoders u(·),v(·), and w(·) start with two fully-connected layers, followed by
uniform self-attention modules. While v and w are the aggregation of representations by the
C C
averaging operator, u is obtained by a multi-head cross-attention module across contexts and the
C
target input. Specifically, the representations {u } , context inputs {x } , and a target input
i i∈C i i∈C
x serve as values, keys and a query in the cross-attention module. Other encoding DNNs σ (·),
i e
µ (·), ϕ(·) consist of a single fully-connected layer. We set d = 8 for the dimension of u and
e lat C
z. The decoding DNNs σ (·) and µ (·) consist of two fully-connected layers. All fully-connected
d d
layers have 128 nodes.
B Functional data augmentation
This section discusses our proposed data augmentation framework for historical CM signals based
on a statistical method for functional data analysis, called functional principal component analysis
(FPCA) [43].
FPCA has a rich history in longitudinal data analysis. It is an extension of PCA to a func-
tional data setting, where each observation is a function (or a curve) rather than a single vector.
Conceptually, it can be thought of as dimensionality reduction for functional data, which encodes
functionstothecorrespondingfinite-dimensionalvectorsinthespace“spanned”byafinitenumber
of orthonormal basis functions [4]. Here the projected vectors and the basis functions are called
functional principal component (FPC) scores and eigenfunctions, respectively. To apply FPCA to
our historical CM signals, let yj(x) denote the observation of unit j’s CM signal at time x ∈ P ⊂ R
26Figure A.1: The encoders of LANP with attention modules used in numerical studies.
where P indicates a time domain, respectively. FPCA approximates yj(x) as
N
(cid:88)
yj(x) ≈ βˆ(x)+ ξˆ ψˆ (x)+ϵ(x), for j ∈ J, (A.1)
jn n
n=1
where βˆ(x) represents an estimated mean trend across y1(x),...,yJ(x), ξˆj := [ξ ,...,ξ ]⊤ ∈ RN
j1 jN
is an estimated FPC score vector for unit j, ψˆ (x) represents the estimated n-th eigenfunction,
n
and ϵ(x) is the i.i.d. Gaussian noise. We note that there are multiple off-the-shelf packages for
FPCA, such as refund [44] in R and scikit-fda [45] in Python. For the theoretical properties
and detailed estimation processes of FPCA, the reader is referred to [43].
Our functional data augmentation approach is inspired by the crucial property of FPCA: given
ψˆ (x) and βˆ(x), the CM signal of unit j is characterized by its FPC score estimate ξˆj . The
n
key idea is to build a generative model that estimates the distribution of FPC scores
{ξˆj
} .
j∈J
Then we can draw an FPC score sample ξ˜ = [ξ˜ ]⊤ ∈ RN from the estimated generative
n n=1,...,N
model and reconstruct a function y˜(x) corresponding to the sample using Eq. (A.1), written by
y˜(x) = βˆ(x)+(cid:80)N ξ˜ ψˆ (x). As such, our generation process does not directly adjust functions;
n=1 n n
27rather, it generates samples in the FPC space and reconstructs their corresponding functions. Fig.
A.2 depicts the proposed functional data augmentation method. While any generative model can
be employed, we use a Gaussian mixture model with Q components. Specifically, we model the
joint probability distribution of
{ξˆj
} by
j∈J
Q
p(ξˆ1 ,...,ξˆJ
) =
(cid:89) p(ξˆj
) =
(cid:89) (cid:88)
ω
N(ξˆj
;m ,Σ )
q q q
j∈J j∈J q=1
where the q-th component of the mixture model is characterized by a Gaussian distribution with
the mean m and covariance Σ , weighted by ω ≥ 0 with
(cid:80)Q
ω = 1. The parameters can be
q q q q=1 q
estimated by the expectation-maximization algorithm [46]. For more details on the estimation and
sampling process for Gaussian mixture models, please refer to [47].
CM data FPC scores FPC scores Augmented CM data
FPCA
2
CPF
2
CPF
FPC 1 FPC 1
(a) (b) (c) (d)
Figure A.2: Signal augmentation by the proposed FPCA-based approach. (a) Original CM data.
(b) FPCs for CM signals estimated by FPCA (cross marks). (c) A generative model fitted to FPCs
and generated samples (red dots). (d) Augmented CM data with reconstructed synthetic signals
(red solid lines).
The proposed approach for functional data augmentation enjoys several notable advantages.
Specifically, the estimated eigenfunctions and the distribution of FPC scores can capture an under-
lying structure and variability inherent across the historical signals. Thus, leveraging these eigen-
functions and the distribution of FPC scores enables the generation of realistic synthetic signals.
Moreover, the generation process can be done in an efficient way. This is because the generative
model builds upon FPC scores, which are low-dimensional representations of signals, rather than
directly dealing with the signals. Finally, the approach generates smooth signals. This is a direct
consequence of using FPCA, which estimates smooth eigenfunctions. Here we should note that the
proposed approach is amenable to cases for one-dimensional input, e.g., temporal data. That being
said, thismethodishighlypracticalincommonCMapplications, giventhatCMobservationsoften
manifest as time-series signals.
28Model training with synthetic signals. Whileexhibitingarealistictrend,syntheticsignalsdo
not have a label. We thus use them as though unlabeled signals. When training LANP in practice, a
batch at each iteration comprises labeled and unlabeled real signals, as well as unlabeled synthetic
signals if needed.
C Additional simulation study
Here, we validate our proposed data augmentation method based on FPCA. We assess the perfor-
manceof LANPtrainedonaugmenteddataversusoriginaldata. Wevarytheparameterdistribution
of the data generation model to identify which case the proposed data augmentation approach ex-
cels. We evaluate models based on their predictions for signals and labels.
Setup. We consider two data generation functions as follows.
• Group I: y = b cos(x)+1.5x+b +ϵ, x ∈ (0,10)
1 2
• Group II: y = b sin(x)+1.5x+b +ϵ, x ∈ (0,10)
1 2
where b ∼ unif(0.5,1+δ), b ∼ unif(0,2+2δ), and ϵ ∼ N(0,0.032) denotes random white noise.
1 2
We consider two cases by setting δ to two different values: 0.5 and 2. It is important to note
that δ controls the difficulty of learning a function-generating distribution. For instance, a greater
δ induces the generation of training samples that exhibit greater deviations. Inference of the
function-generating distribution based on such samples is more challenging compared to cases with
lessdeviatingsamples(i.e. withsmallerδ). Ineachofthetwocases,wegenerateadatasetconsisting
of 16 partially-labeled training signals. This dataset comprises 8 training signals from each group,
where we intentionally remove group labels for 4 signals selected at random. In contrast to Setting
I, we have a limited training dataset of only 16 signals in this setting. This reflects possible real-
world scenarios in CM applications where only a limited number of historical units are available,
where data augmentation can help. During each iteration in training LANP, we randomly select
8 labeled original signals and generate 8 synthetic signals by the method detailed in Sec. B. We
compare this approach with training LANP solely on the original signals. Other configurations for
data generation, models, and optimization remain the same in Setting I. We iterate the experiment
5 times.
29Results. Table 4 shows results on average RMSEs for cases with δ = 0.5 and δ = 2. Fig. A.3
shows label prediction accuracies over different numbers of online observations.
Table 4: Average RMSEs of LANP with or without data augmentation. In the first column, “Orig-
inal” and “Augmented” refer to the cases where LANP is trained on the original dataset and the
augmented dataset, respectively.
GroupI GroupII
Dataset δ
α=0.25 α=0.5 α=0.75 α=0.25 α=0.5 α=0.75
Original 0.0199(0.0031) 0.0182(0.0029) 0.0180(0.0039) 0.0119(0.0015) 0.0095(0.0030) 0.0112(0.0026)
0.5
Augmented 0.0182(0.0036) 0.0176(0.0040) 0.0171(0.0041) 0.0129(0.0013) 0.0099(0.0025) 0.0100(0.0026)
Original 0.0370(0.0159) 0.0311(0.0128) 0.0330(0.0126) 0.0254(0.0150) 0.0148(0.0046) 0.0156(0.0052)
2
Augmented 0.0273(0.0120) 0.0254(0.0117) 0.0257(0.0118) 0.0218(0.0067) 0.0126(0.0035) 0.0127(0.0037)
Figure A.3: Accuracy of label prediction over different numbers of online observations.
According to the results, data augmentation leads to a substantial enhancement in predictive
performance for both labels and signals when δ = 2. This implies that our data augmentation
approach is useful especially when a significant signal deviation exists but lacks enough training
signals. Our method that generates realistic synthetic signals provides sufficient training signals
to characterize their underlying distributions in such cases. Meanwhile, LANP trained only on the
originalsignalsgivessatisfactoryperformanceswhenδ = 0.5. Thisshowsthatitwouldbeenoughto
trainonlyontheoriginaldatasetiftheoriginaltrainingsignalsarealreadysufficienttocharacterize
the distribution effectively. Finally, the accuracy of the label prediction increases as α increases.
This is expected since signal discrepancies among different groups become more prominent as more
context observations are available.
30