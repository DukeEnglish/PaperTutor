Iso-Diffusion: Improving Diffusion Probabilistic
Models Using the Isotropy of the Additive
Gaussian Noise
Dilum Fernando1, Dhanajaya Jayasundara2, Roshan Godaliyadda1, Chaminda
Bandara3, Parakrama Ekanayake1, and Vijitha Herath1
1 University of Peradeniya, Peradeniya, Sri Lanka
2 Johns Hopkins University, Baltimore MD 21218, USA
3 Apple Inc., USA
Abstract. DenoisingDiffusionProbabilisticModels(DDPMs)haveac-
complished much in the realm of generative AI. Despite their high per-
formance, there is room for improvement, especially in terms of sample
fidelitybyutilizingstatisticalpropertiesthatimposestructuralintegrity,
suchasisotropy.Minimizingthemeansquarederrorbetweentheadditive
and predicted noise alone does not impose constraints on the predicted
noise to be isotropic. Thus, we were motivated to utilize the isotropy of
the additive noise as a constraint on the objective function to enhance
thefidelityofDDPMs.Ourapproachissimpleandcanbeappliedtoany
DDPMvariant.Wevalidateourapproachbypresentingexperimentscon-
ducted on four synthetic 2D datasets as well as on unconditional image
generation. As demonstrated by the results, the incorporation of this
constraint improves the fidelity metrics, Precision and Density for the
2D datasets as well as for the unconditional image generation.
Keywords: Generative Models · Diffusion Models · Structure · Image
Generation · Gaussian Noise
1 Introduction
Diffusion models have been accomplishing great feats in the realm of genera-
tive AI, specifically in terms of unconditional and conditional image generation
( [12], [5], [18], [11], [15], [17], [6]). Starting with the revolutionary paper by Ho
et al. [4] and the improvements by Nichol et al. [12] as well as the Latent Dif-
fusion Model by Rombach et al. [17], these models have had the biggest impact
in this context. The fidelity and diversity of these generated images are surpris-
ingly amazing. Yet, as with all models, these models can still be improved upon
closer inspection. As with the improvements done by Nichol et al. [12] to the
original Denoising Diffusion Probabilistic Model (DDPM) by introducing tech-
niques such as the cosine-based variance schedule as well as making the model
learn the variance rather than keeping it fixed, helped improve the performance
of DDPMs. Our goal in this paper is to make a similar contribution with re-
gards to the improvement of the important fidelity metrics, Density [10] and
Precision [8].
4202
raM
52
]GL.sc[
1v09761.3042:viXra2 D. Fernando et al.
(a) DDPM (b) Oursλ=0.01 (c) Oursλ=0.05
(d) Oursλ=0.1 (e) Oursλ=0.2 (f) Oursλ=0.3
Fig.1: ThevariationoftheGeneratedDistribution(inblue)comparedtotheGround
TruthDistribution(inorange).Theplotsindicatethatthegeneratedsamplesaremuch
more closely connected and densely packed with the increase of the regularization
parameter,λ.Theplotsshowtheutilityofimposingtheisotropyconstraintbasedloss
function which enables the generation of more realistic samples as most of generated
samples are concentrated near the ground truth samples.
Although DDPMs perform well, we noticed that these existing models do
notnecessarilyincorporateanydistributional(structural)informationaboutthe
particular dataset it tries to generate. Typically, the DDPM’s forward process
gradually pushes the dataset towards a white gaussian, which can be thought of
asastructuralvanishingpointofthedatadistribution.Thisimpliesawellplaced
pointoforiginforthegenerativeprocess(reversepath)fromapointofcomplete
lack of structure towards the final destination which is the data distribution. In
theDDPMimplementation,thelearningprocessconsiderstheexpectedsquared
norm difference between the additive gaussian noise and the predicted noise
as its objective function. Therefore, for the generative process, to enhance the
aforementioned creation of structure, the objective function can be modified to
include any structural measure, such as isotropy. In the context of this paper,
"isotropy" describes the affinity towards a white distribution quantified by the
expected squared norm of the distribution. However, the objective function in
theexistingmodelsonlyfocusontheexpectedsquarednormdifferencebetween
the additive gaussian noise and the predicted noise.
Thus,weweremotivatedtoincludetheisotropicnatureoftheadditivegaus-
sian noise when optimizing for the objective to further enhance the statistical
properties of the predicted noise. Our intuition is that by capturing the statis-
tical properties of the noise in more detail, the model will be able to produceIso-Diffusion 3
Fig.2: Comparison of the generated images via the DDPM (left) and Iso-Diffusion
(right). The DDPM generated images contain much more artefacts and do not seem
realistic. However, the generated images via Iso-Diffusion are much more realistic and
thus, they are of high fidelity.
higher fidelity samples as it would have much more information regarding the
distributional structure of the samples.
As the rationale for introducing isotropy to the objective function has been
established. Now, we let’s see how isotropy establishes convergence and quanti-
fiesstructuralinformationaboutthedistribution.Forexample,theisotropyofa
white gaussian vector in Rn is the expected squared norm of that vector, which
isequaltoitsdimension,n[23].Thisestablishestheupperboundinthelimitfor
a normalized distribution with complete lack of structure, which in other words
is white. On the hand, the desired distribution, which has more structure and is
more colored, would consequently have a lower isotropy value. This implies that
the generative process, in its drive towards a structural distribution minimizes
isotropy. Furthermore, when analyzing the mean square error objective, we ob-
served that the isotropic nature of the noise, when included, effectively makes
the objective function equal to 0 in expectation.
Theinclusionofthisconstraintdoesnotincuralargecomputationalcostand
can be readily applied to any of the diffusion model variants. In this work, we
experimentedonfour2Dsyntheticdatasetswithourmodifiedobjectivefunction
and show that the fidelity metrics, in particular the Density, improves signifi-
cantly. This implies that the proposed constraint attempt to latch on to the
more information rich dense modes of the desired distribution. Furthermore, we
validateourapproachonunconditionalimagegenerationusingtheOxfordFlow-
ers[13]andOxford-IIITPet[14]datasets.Wecomparethefidelityanddiversity
of the generated samples based on key evaluation metrics such as Precision and
Recall [8], Density and Coverage [10], Frechet Inception Distance (FID) [2] and
Inception Score (IS) [19].
The contributions of this work are as follows:
– WeintroduceIso-Diffusion:amodifiedapproachthatintroducesanisotropic
constraint on the predicted noise objective function to steer the generative
process in a structurally coherent manner. This results in improved fidelity
ofthegenerateddatadistribution.Webelieve,tothebestofourknowledge,
that we are the first to propose such a modified loss based on the properties
of the noise.4 D. Fernando et al.
– We analyze the simple loss function in the DDPM and its connection to
isotropy.Moreover,weshowthattheisotropyofthedatadistributionmono-
tonically increases and converges to the maximum isotropy value which cor-
respondstoawhitegaussiandistribution.Thisconfirmsthatthedefinitionof
isotropy, mentioned in this paper, conveys information about the structural
information of the data distribution when the data distribution undergoes
the forward process in DDPMs.
– Weevaluateandvalidateourapproachonfour2Dsyntheticdatasetsaswell
as on the task of unconditional image generation on Oxford Flowers and
Oxford-IIIT Pet datasets. Considering the key evaluation metrics, such as
the Precision, Recall,Density, Coverage,FID and IS,the modified objective
is able to surpass the original DDPM with a significant gap in terms of the
fidelity metrics, Density and Precision.
2 Related Work
Deep Generative ModelsGenerativemodels(GANs[1],VAEs[7],flow-based
models [16], and diffusion models [4]) learn the probability distribution of given
data,allowingustosamplenewdatapointsfromthedistribution.Deepgenera-
tive models have been used for generating images, videos [3], 3d objects [9], etc.
Moreover, these models have been used for inverse problem solving [22] and to
understanding the latent representations of the distributions.
Diffusion Models Diffusion models, in particular, have been making huge
improvementsandhavebeenusedinmanydomainsduetotheirhighgenerative
capabilities. There are mainly two types of diffusion models, one is the Score
based approach introduced by Song and Ermon [21] and the other, which is the
focus of this work, is the one introduced by Ho et al. [4]. Both modeling types
have been able to achieve state-of-the-art performance in generative modeling
tasks and have motivated the growth of many subsequent works in generative
models.
Improving Diffusion Models In the context of DDPMs [4], there have
been several seminal papers that have contributed to the improvement of these
models.Inparticular,Nicholetal.’s[12]workpresentedseveralkeyinsightsinto
how one could improve the training of these models. One such insight is the use
ofacosine-basedvariancescheduleratherthanthelinearvariancescheduleused
by Ho et al. [4]. These changes were able to improve the DDPM further.
However,mostoftheseimprovementswerefocusedonimprovingthemodels
based on the most widely used metrics for image generation, FID and IS. But,
some of the recent work ( [8], [10]), in generative models have pointed out that
FID and IS are not necessarily indicative of the actual fidelity of the samples
generatedbygenerativemodels.Thus,researchershavebeenfocusingonfinding
othermetrics,suchasPrecisionandDensity,toassessthefidelityofthesegener-
atedsamples. Inparticularwe observedthattheDensitytakesthe local context
(measuring how close it is to densely packed samples of the true distribution)
of a sample into account during its calculation. We believe that this makes the
Density a vital metric to asses the samples’ fidelity.Iso-Diffusion 5
3 Background
Diffusion probabilistic models were first introduced by Sohl-Dickstein et al. [20]
These models fall under the category of generative models which learn the dis-
tribution of the data so that they can sample from these data distributions.
However, it wasn’t until Ho et al. [4] that Diffusion Probabilistic Models took
off. In the next few subsections, we will provide a brief overview of the DDPM
definitions that will be useful to understanding our work.
3.1 Definitions
In the DDPM, we simply add a gaussian noise, which varies according to a
specific variance schedule, β ∈ (0,1). The noise at each time-step corrupts the
t
data, such that by the time the time-step reaches its final value, T, the data
will be mapped to an almost white gaussian distribution. However, the learning
occurs when we try to learn the reverse process by which we try to denoise
along the same trajectory starting from the almost white gaussian distribution.
The first process, in which we add noise, is called the forward process and the
latter, in which we denoise, the reverse process. The forward process is often
characterized by q and the reverse process by p. Both of which are modeled as
gaussian distributions.
The forward process is defined as follows,
T
(cid:89)
q(x ,x ,...x |x )= q(x |x ) (1)
1 2 T 0 t t−1
t=1
(cid:112)
q(x |x )∼N(x ; 1−β x ,β I) (2)
t t−1 t t t−1 t
Moreoever, by introducing α = 1 − β as well as α¯ =
(cid:81)t
α the for-
t t t i=1 i
ward process can be further simplified into the following expression via the re-
parametrization trick [7]. Since,
(cid:112)
q(x |x )∼N(x ; 1−β x ,β I) (3)
t t−1 t t t−1 t
√ √
q(x |x )∼N(x ; α¯x , 1−α¯I) (4)
t 0 t t 0 t
√ √
x = α¯x + 1−α¯ϵ (5)
t t 0 t
where, ϵ∈N(0,I).
The reverse process, given by p ∼ N(x |x ), can be obtained in terms of
t−1 t
theforwardprocessdistributionqandtheBaye’sTheorem.However,thereverse
process only becomes tractable when the posterior distribution q(x |x ), is
t−1 t
conditioned on the input data x . Thus, during training, the model tries to
0
learn the tractable q(x |x ,x ) distribution. This distribution, which is also a
t−1 t 0
gaussian distribution, is defined by the following equation and parameters.6 D. Fernando et al.
q(x |x ,x )∼N(x ;µ˜(x ,x ),β˜I) (6)
t−1 t 0 t−1 t 0 t
1−α¯
β˜ = t−1β (7)
t 1−α¯ t
t
√ √
α¯ β α (1−α¯ )
µ˜ (x ,x )= t−1 tx + t t−1 x (8)
t t 0 1−α¯ 0 1−α¯ t
t t
3.2 Training Process
To train, however, one could make the model predict the mean of the reverse
processdistributionateachtimestep.But,Hoetal.[4]mentionsthatpredicting,
the additive noise, ϵ, leads to better results. The additive noise and the mean
of the reverse process distribution at each time step are elegantly linked by
equations(5)and(8).Thisresultsinthefollowingre-parametrizationofµ˜(x ,t),
t
(cid:18) (cid:19)
1 1−α
µ˜(x ,t)= √ x − √ t ϵ (9)
t α t 1−α¯
t t
Therefore,predictingtheadditivenoise,ϵ,isadequateforthetaskofpredict-
ing the mean of the backward process distribution. Moreover, since the forward
process’ variance schedule is fixed, the reverse process variance, β˜, is also as-
t
sumed to be fixed according to β˜.
t
Thus, Ho et al. [4] proposes to optimize the following simple objective func-
tion during the training process.
L =E [||ϵ−ϵ (x ,t)||2] (10)
simple t,x0,ϵ θ t
where, ϵ (x ,t) is the predicted noise.
θ t
3.3 Hidden Statistical Properties of ϵ
UponcloserinspectionoftheL objectivefunction,weseethattheobjective
simple
oftheU-Netistominimizethemeansquarederrorbetweenϵandϵ .Yet,ifthe
θ
simple loss is expanded further, a rather informative mathematical expression
can be obtained.
E[||ϵ−ϵ ||2]=E[(ϵ−ϵ )T(ϵ−ϵ )] (11)
θ θ θ
=E(ϵTϵ)+E(ϵTϵ )−2E(ϵTϵ) (12)
θ θ θ
Now, since we know that ϵ ∼ N(0,I), it is an isotropic distribution. Thus,
by definition, since ϵ is an isotropic random vector in Rn, the expected norm of
the random vector, E(ϵTϵ)=n.
Furthermore, since the goal is to predict the noise as accurately as possible,
ϵ should also be distributed according to a white gaussian distribution, i.e.,
θIso-Diffusion 7
ϵ ∼N(0,I)).Hence,ifϵandϵ arebothindependentidenticalisotropicrandom
θ θ
vectors,
E[||ϵ−ϵ ||2]=E(ϵTϵ)+E(ϵTϵ )−2E(ϵTϵ) (13)
θ θ θ θ
=n+n−2n (14)
=0 (15)
4 Analysis on the Isotropy of x
t
Inspired, we wanted to find out further implications of imposing structural in-
formation in the DDPM. As it turns out, we were able to gain more interesting
insights about the forward process of the DDPM. For example, if we consider
equation(5)andconsidertheisotropy,expectedsquarednormofx ,weseethat,
t
(cid:112)
E(||x ||2)=E(xTx )=α¯ E(xTx )+(1−α¯ )E(ϵTϵ)+2 (α¯ )(1−α¯ )E(xTϵ)
t t t t 0 0 t t t 0
(16)
However,sinceϵisawhitegaussianrandomvector,itisisotropic.Moreover,
by assuming that it is independent of the distribution of x , when x is non-
0 0
isotropic, we see that,
E(xTϵ)=0 (17)
0
Therefore,
E(xTx )=α¯ E(xTx )+(1−α¯ )n (18)
t t t 0 0 t
=n+α¯ (E(xTx )−n) (19)
t 0 0
Thus, when the input data are normalized and they are distributed accord-
ing to a non-isotropic distribution, we note that the maximum of the expected
squared norm of x , E(xTx ) = n. Hence, E(xTx )−n ≤ 0. Thus, during
0 0 0 max 0 0
the forward process, since α¯ > 0, the expected squared norm of x can be at
t t
most n, ∀t∈[1,T] and attains the maximum value at the final time-step T.
E(xTx )≤n (20)
t t
Moreover, when we consider two consecutive time steps, t and t+1, we see
that,
E(xT x )=n+α¯ (E(xTx )−n) (21)
t+1 t+1 t+1 0 0
E(xTx )=n+α¯ (E(xTx )−n) (22)
t t t 0 0
E(xT x )−E(xTx )=(E(xTx )−n)(α¯ −α¯ ) (23)
t+1 t+1 t t 0 0 t+1 t
We know that E(xTx )−n≤0 and that α¯ −α¯ ≤0. Thus,
0 0 t+1 t
E(xT x )−E(xTx )≥0 (24)
t+1 t+1 t t
E(xT x )≥E(xTx ) (25)
t+1 t+1 t t8 D. Fernando et al.
Therefore, for any particular normalized data distribution, we see that during
the forward process, the isotropy of the data distribution increases, and finally
converges to the isotropy of a white gaussian vector, when the data distribution
completely converts into a white gaussian distribution. Hence, the definition of
isotropy given in this paper, aligns perfectly with the fact that the isotropy
quantifies structural information about the data distribution.
5 Isotropy Based Loss Function
Armed with the above analyses, we proceeded to modify the objective function
L to include a regularization term which penalizes the model, if the model
simple
predicts a noise which is not necessarily isotropic. Hence, the new modified ob-
jective function we propose to optimize is,
L =E(||ϵ−ϵ ||2)+λ(E(ϵTϵ )−n)2 (26)
modified θ θ θ
where λ is the regularization parameter.
However,thismodifiedobjectiveneedstobefurthersimplifiedsoastomake
thisnewerrorbeindependentofthesizeofthedimensionoftherandomvector.
Thus, we make the following modification during implementation.
(cid:18) (cid:18) ϵTϵ (cid:19) (cid:19)2
L =E(||ϵ−ϵ ||2)+λ E θ θ −1 (27)
modified θ n
6 Experiments
6.1 Experimental Setup
Tovalidateourapproachweconsider2Dsyntheticdataaswellasimages.Forthe
2D data, we utilized a conditional dense network consisting of 3 fully-connected
hidden layers with ReLU activations. The learning rate was fixed at 1e-3. All
the datasets were learned using 1000 time-steps and 1000 epochs.
For the image datasets, we consider the same version of the U-Net utilized
in the original DDPM implementation with a learning rate of 2e-4. The U-Net
was trained with 1000 time-steps for 1000 epochs as well.
For each dataset, when reporting the metrics, we consider the average of 3
training runs for each of the models. Moreover, all the experiments were run on
one Quadro GV-100 GPU with 32GB of VRAM.
6.2 Synthetic Data
Forthesyntheticgenerationexperiments,weconsiderfour2Dsyntheticdatasets,
namely,theSwissRoll,theInter-twiningmoons,theS-Curveandthe8-gaussians
datasets. To evaluate the proposed method, we utilize the most informative
generative model metrics, such as the Precision, Recall, Density and Coverage.Iso-Diffusion 9
Table 1: Comparison of Evaluation Metrics for the two methods : DDPM and Iso-
Diffusion for the 2D Datasets
Swiss Roll Moons S-Curve 8-gaussians
Metrics
DDPM Ours DDPM Ours DDPM Ours DDPM Ours
Precision 0.90 0.982 0.987 0.991 0.989 0.992 0.994 0.997
Recall 0.998 0.984 0.999 0.997 0.996 0.989 0.997 0.988
Density 0.83 0.989 0.957 0.979 0.978 0.982 0.983 0.989
Coverage 0.895 0.917 0.943 0.938 0.938 0.91 0.932 0.90
Table 2: Metrics Variation with the Regularization Parameter for the Swiss Roll
Dataset (λ)
Method Precision Recall Density Coverage
DDPM ( [4]) 0.90 (±0.005) 0.998 (±0.0001) 0.83 (±0.02) 0.895 (±0.025)
Ours λ=0.01 0.93 (±0.022) 0.998 (±0.0002) 0.885 (±0.0035) 0.90 (±0.055)
Ours λ=0.05 0.971 (±0.0009) 0.992 (±0.0002) 0.955 (±0.001) 0.92 (±0.015)
Ours λ=0.10 0.982 (±0.0001) 0.984 (±0.003) 0.989 (±0.005) 0.917 (±0.001)
Ours λ=0.20 0.99 (±0.001) 0.983 (±0.003) 1.0 (±0.001) 0.875 (±0.025)
Ours λ=0.30 0.983(±0.003) 0.96 (±0.015) 1.0 (±0.01) 0.85 (±0.035)
Of these metrics, Precision and Density measure the fidelity of the generated
samples,whereastheRecallandCoveragemeasurethediversityofthegenerated
samples.
Table 1 summarizes the demonstrated improvements made by our modified
loss in terms of these metrics. Across all these datasets we observe that the
fidelity metrics, Precision and Density have been improved. Specifically, when
consideringtheSwissRollandtheMoonsdatasets,wehavebeenabletoachieve
much larger improvement in terms of the Density.
The visualized plots of the generated data distributions (figures 1, 3, 4, and
5) indicate that the modified loss function based samples are much more closely
gathered and concentrated near the ground-truth data. On the contrary, we
can observe that the samples generated via the original DDPM loss function,
Table 3: MetricsVariationwiththeRegularizationParameterfortheMoonsDataset
(λ)
Method Precision Recall Density Coverage
DDPM ( [4]) 0.987 (±0.001) 0.999 (±0.001) 0.957 (±0.009) 0.943 (±0.009)
Ours λ=0.01 0.980 (±0.0004) 0.999 (±0.0008) 0.942 (±0.0003) 0.926 (±0.025)
Ours λ=0.05 0.985 (±0.004) 0.998 (±0.0001) 0.957 (±0.02) 0.947 (±0.0045)
Ours λ=0.10 0.991 (±0.001) 0.997 (±0.001) 0.979 (±0.0001) 0.938 (±0.017)
Ours λ=0.20 0.995 (±0.001) 0.991 (±0.0028) 0.992 (±0.005) 0.924 (±0.013)
Ours λ=0.30 0.992 (±0.001) 0.992 (±0.005) 1.0 (±0.006) 0.90 (±0.005)10 D. Fernando et al.
Table4:MetricsVariationwiththeRegularizationParameterfortheS-CurveDataset
(λ)
Method Precision Recall Density Coverage
DDPM ( [4]) 0.989 (±0.003) 0.996 (±0.003) 0.978 (±0.008) 0.938 (±0.008)
Ours λ=0.01 0.988 (±0.003) 0.996 (±0.002) 0.969 (±0.011) 0.931 (±0.01)
Ours λ=0.05 0.989 (±0.004) 0.994 (±0.005) 0.974 (±0.011) 0.925 (±0.014)
Ours λ=0.10 0.991 (±0.004) 0.993 (±0.005) 0.977 (±0.012) 0.921 (±0.019)
Ours λ=0.20 0.992 (±0.004) 0.989 (±0.012) 0.982 (±0.015) 0.910 (±0.036)
Ours λ=0.30 0.993 (±0.004) 0.983 (±0.018) 0.986 (±0.017) 0.898 (±0.043)
Table 5: Metrics Variation with the Regularization Parameter for the 8-gaussians
Dataset (λ)
Method Precision Recall Density Coverage
DDPM ( [4]) 0.994 (±0.003) 0.997 (±0.001) 0.983 (±0.009) 0.932 (±0.011)
Ours λ=0.01 0.995 (±0.002) 0.997 (±0.001) 0.982 (±0.007) 0.936 (±0.011)
Ours λ=0.05 0.995 (±0.002) 0.996 (±0.002) 0.983 (±0.006) 0.931 (±0.013)
Ours λ=0.10 0.996 (±0.002) 0.995 (±0.003) 0.986 (±0.007) 0.924 (±0.018)
Ours λ=0.20 0.996 (±0.002) 0.992 (±0.007) 0.988 (±0.008) 0.918 (±0.022)
Ours λ=0.30 0.997 (±0.002) 0.988 (±0.012) 0.989 (±0.009) 0.90 (±0.042)
are not concentrated, but are scattered about. We believe that the isotropy
based loss function helps the U-Net learn in such a way that the generated
distributiondoesnotcontaintoomanysampleswhicharefarawayfromthemost
densely packed modes of the ground-truth data distribution. This shows that
the proposed loss function, enforces the generated samples to contain properties
that pushes them to be closely linked to the real data. Thus, we can directly
observeanimprovementintheDensitymetricasitmeasuresthesamplefidelity.
Furthermore,withoutanystructuralinformationduringthetrainingprocess,the
original DDPM is not capable of generating samples which capture the densely
packedstructureofthedistributionandhenceitisnotnecessarilyconcentrated,
but is scattered.
6.3 Unconditional Image Generation
For the unconditional image generation task, we experiment with the Oxford
flowers dataset and the Oxford-IIIT Pet dataset. The FID and IS, along with
thePrecision,Recall,DensityandCoveragewereusedtoevaluatethequalityof
the generated samples.
Theresultsoftable6furtherconfirmtheimprovementsmadebyourmodified
loss on the quality of samples. The Density of the generated images have been
significantly improved for the two datasets. Moreover, the FID score has been
significanlty improved in the Oxford Flowers dataset by the proposed method.
Although the FID and IS are considered to be the most widely used evaluationIso-Diffusion 11
(a) DDPM (b) Oursλ=0.01 (c) Oursλ=0.05
(d) Oursλ=0.1 (e) Oursλ=0.2 (f) Oursλ=0.3
Fig.3: Variation of the Generated and Ground Truth Distributions with Changing
Regularization Parameter Values
metricsforassessingimagegeneration,weseethatinthecaseoftheOxford-IIIT-
Petdataset,theyconveylittletonodiscerninginformationaboutthegenerative
ability of the proposed method and the original DDPM. But, by using other
metrics such as the Precision, Recall, Density and Coverage, we can state that
whileourproposedmethodsuffersabitintermsofRecall,thegeneratedsamples,
see figure 2 are very close to being real as indicated by the improvements in the
Precision and Density metrics. Moreover, this is further validated by some of
the sample generated by both the original DDPM and the proposed method.
The samples generated by the proposed method convey much more realistic
characteristics in the images than what can be obtained by using the original
DDPM. Thus, this should motivate the research community to propose new
evaluation metrics such as Density, which is a much more meaningful measure
of fidelity over FID and IS, to assess generative models.
6.4 Variation of the Metrics with the Regularization Parameter
Althoughtheperformanceofthemodifiedlossfunctionhasbeenabletoproduce
samplewhichsurpasstheoriginalDDPM’ssamplesquality,thequalitydepends
on the regularization parameter of the modified loss function. In particular,
we performed a few more experiments by considering a range of values for the
regularization parameter.
The metrics for the 2D synthetic datasets with different values of the regu-
larization parameter ranging from 0.01 to 0.30 are tabulated in tables 2, 3, 4,
and5.Weseethatthefidelitymetrics,PrecisionandDensity,graduallyimprove
with the increase of the regularization parameter. On the other hand, however,12 D. Fernando et al.
(a) DDPM (b) Oursλ=0.01 (c) Oursλ=0.05
(d) Oursλ=0.1 (e) Oursλ=0.2 (f) Oursλ=0.3
Fig.4: Variation of the Generated and Ground Truth Distributions with Changing
Regularization Parameter Values
we can see that the diversity metrics, Recall and Coverage, gradually decline
with the parameter. We believe that this is a direct consequence of imposing
a structural constraint on the objective function. It is evident that by focusing
on the structure or the isotropy of the distribution, our method is capable of
capturing highly dense mode regions and generating samples near them rather
than being too diverse. Thus, it increase the fidelity but decreases the diversity
of the generated samples.
Table 6: Comparison of Evaluation Metrics for the two Methods in Unconditional
Image Generation
Oxford Flowers Oxford-IIIT-Pet
Metrics
DDPM Iso-Diffusion DDPM Iso-Diffusion
FID (↓) 49.60 (±5.77)44.77 (±5.77) 34 (±0.76) 34.47 (±0.17)
IS (↑) 3.93 (±0.12) 3.94 (±0.12) 12.0 (±0.27) 12.0 (±0.27)
Precision (↑) 0.75 (±0.01) 0.86 (±0.01) 0.71 (±0.03) 0.80 (±0.03)
Recall (↑) 0.21 (±0.06) 0.14 (±0.01) 0.29 (±0.02) 0.24 (±0.22)
Density (↑) 2.48 (±0.99) 3.99 (±0.35) 2.11 (±0.22) 2.99 (±0.37)
Coverage (↑) 0.93 (±0.06) 0.97 (±0.01) 0.98 (±0.01) 0.99 (±0.01)Iso-Diffusion 13
(a) DDPM (b) Oursλ=0.01 (c) Oursλ=0.05
(d) Oursλ=0.1 (e) Oursλ=0.2 (f) Oursλ=0.3
Fig.5: Variation of the Generated and Ground Truth Distributions with Changing
Regularization Parameter Values
7 Conclusion
Denoising Diffusion Probabilistic Models have achieved state-of-the-art perfor-
manceingenerativemodelingtaskssuchasunconditionalimagegenerationand
image super resolution. However, these models can still be improved upon and
muchworkhasbeenputintoimprovingthem.Inthispaper,weproposeanother
improvement method which is built on the premise that since the distribution
that the forward process terminates and the reverse process initiates at a white
gaussian distribution, which is isotropic and is void of any structure, it is well
motivated, that the incorporation of isotropy as a measure of structure on the
loss function will improve the DDPMs’ generated sample fidelity. We, theoret-
ically, show that isotropy is well a defined metric to measure the structure of
thedistributionduringtheforwardprocessandtheproposedmodificationhelps
the DDPM to converge to better solutions based on the simple modified loss.
Finally, we validate and show that our modified objective function improves the
performanceoftheDDPM,viaexperimentsperformedon2Dsyntheticdatasets
and on unconditional image generation.
References
1. Goodfellow,I.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,
Courville,A.,Bengio,Y.:Generativeadversarialnetworks.Communicationsofthe
ACM 63(11), 139–144 (2020) 4
2. Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,Hochreiter,S.:Ganstrained
byatwotime-scaleupdateruleconvergetoalocalnashequilibrium.Advancesin
neural information processing systems 30 (2017) 314 D. Fernando et al.
3. Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D.P.,
Poole, B., Norouzi, M., Fleet, D.J., et al.: Imagen video: High definition video
generation with diffusion models. arXiv preprint arXiv:2210.02303 (2022) 4
4. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in
neural information processing systems 33, 6840–6851 (2020) 1, 4, 5, 6, 9, 10
5. Ho,J.,Saharia,C.,Chan,W.,Fleet,D.J.,Norouzi,M.,Salimans,T.:Cascadeddif-
fusionmodelsforhighfidelityimagegeneration.TheJournalofMachineLearning
Research 23(1), 2249–2281 (2022) 1
6. Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint
arXiv:2207.12598 (2022) 1
7. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114 (2013) 4, 5
8. Kynkäänniemi,T.,Karras,T.,Laine,S.,Lehtinen,J.,Aila,T.:Improvedprecision
andrecallmetricforassessinggenerativemodels.AdvancesinNeuralInformation
Processing Systems 32 (2019) 1, 3, 4
9. Mo, S., Xie, E., Wu, Y., Chen, J., Nießner, M., Li, Z.: Fast training of diffusion
transformer with extreme masking for 3d point clouds generation (2023) 4
10. Naeem, M.F., Oh, S.J., Uh, Y., Choi, Y., Yoo, J.: Reliable fidelity and diversity
metrics for generative models (2020) 1, 3, 4
11. Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B.,
Sutskever,I.,Chen,M.:Glide:Towardsphotorealisticimagegenerationandediting
with text-guided diffusion models. arXiv preprint arXiv:2112.10741 (2021) 1
12. Nichol, A.Q., Dhariwal, P.: Improved denoising diffusion probabilistic models. In:
InternationalConferenceonMachineLearning.pp.8162–8171.PMLR(2021) 1,4
13. Nilsback,M.E.,Zisserman,A.:Automatedflowerclassificationoveralargenumber
of classes. In: 2008 Sixth Indian conference on computer vision, graphics & image
processing. pp. 722–729. IEEE (2008) 3
14. Parkhi,O.M.,Vedaldi,A.,Zisserman,A.,Jawahar,C.V.:Catsanddogs.In:IEEE
Conference on Computer Vision and Pattern Recognition (2012) 3
15. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-
conditional image generation with clip latents. arXiv preprint arXiv:2204.06125
1(2), 3 (2022) 1
16. Rezende,D.,Mohamed,S.:Variationalinferencewithnormalizingflows.In:Inter-
national conference on machine learning. pp. 1530–1538. PMLR (2015) 4
17. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. pp. 10684–10695 (2022) 1
18. Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Denton,E.L.,Ghasemipour,
K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-
to-image diffusion models with deep language understanding. Advances in Neural
Information Processing Systems 35, 36479–36494 (2022) 1
19. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.:
Improved techniques for training gans. Advances in neural information processing
systems 29 (2016) 3
20. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsuper-
visedlearningusingnonequilibriumthermodynamics.In:Internationalconference
on machine learning. pp. 2256–2265. PMLR (2015) 5
21. Song, Y., Ermon, S.: Generative modeling by estimating gradients of the data
distribution. Advances in neural information processing systems 32 (2019) 4Iso-Diffusion 15
22. Song,Y.,Shen,L.,Xing,L.,Ermon,S.:Solvinginverseproblemsinmedicalimag-
ing with score-based generative models. arXiv preprint arXiv:2111.08005 (2021)
4
23. Vershynin,R.:High-dimensionalprobability:Anintroductionwithapplicationsin
data science, vol. 47. Cambridge university press (2018) 3