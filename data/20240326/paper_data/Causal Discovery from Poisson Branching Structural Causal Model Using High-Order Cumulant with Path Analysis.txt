Causal Discovery from Poisson Branching Structural Causal Model Using
High-Order Cumulant with Path Analysis
JieQiao1*,YuXiang1*,ZhengmingChen1 ,RuichuCai1,2† ,ZhifengHao3
1SchoolofComputerScience,GuangdongUniversityofTechnology,Guangzhou,China
2PengChengLaboratory,Shenzhen,China
3CollegeofScience,ShantouUniversity,Shantou,China
{qiaojie.chn,thexiang2000,chenzhengming1103,cairuichu}@gmail.com,haozhifeng@stu.edu.cn
Abstract
Countdatanaturallyariseinmanyfields,suchasfinance,neu-
roscience,andepidemiology,anddiscoveringcausalstructure
amongcountdataisacrucialtaskinvariousscientificand
industrialscenarios.Oneofthemostcommoncharacteristics
ofcountdataistheinherentbranchingstructuredescribedby
abinomialthinningoperatorandanindependentPoissondis-
(a) BranchingStructure (b) CausalGraph
tributionthatcapturesbothbranchingandnoise.Forinstance,
in a population count scenario, mortality and immigration
contributetothecount,wheresurvivalfollowsaBernoulli Figure1:Illustrationofbranchingstructurecausalmodeling.
distribution,andimmigrationfollowsaPoissondistribution.
However,causaldiscoveryfromsuchdataischallengingdue
tothenon-identifiabilityissue:asinglecausalpairisMarkov andScheines2000;Zhangetal.2018;Glymour,Zhang,and
equivalent,i.e.,X →Y andY →X aredistributedequiva- Spirtes2019;Caietal.2018).Inparticular,constraint-based
lent.Fortunately,inthiswork,wefoundthatthecausalorder
methods(Pearl2009;Spirtes,Meek,andRichardson1995),
from X to its child Y is identifiable if X is a root vertex
score-basedmethods(Chickering2002;Tsamardinos,Brown,
andhasatleasttwodirectedpathstoY,ortheancestorof
andAliferis2006)identifythecausalstructurebyexploring
XwiththemostdirectedpathtoXhasadirectedpathtoY
theconditionalindependencerelationamongvariables,but
withoutpassingX.Specifically,weproposeaPoissonBranch-
ingStructureCausalModel(PB-SCM)andperformapath thesemethodsonlyfocusonthecategorydomainandcan
analysisonPB-SCMusinghigh-ordercumulants.Theoretical onlyidentifyuptotheMarkovequivalentclass(Pearl2009).
resultsestablishtheconnectionbetweenthepathandcumu- Thus,propercountdatamodelingisrequiredtofurtheriden-
lantanddemonstratethatthepathinformationcanbeobtained tify the causal structure beyond the equivalence class. Re-
fromthecumulant.Withthepathinformation,causalorderis centworkby(ParkandRaskutti2015)introducesaPoisson
identifiableundersomegraphicalconditions.Apracticalalgo- Bayesiannetworktomodelthecountdataandshowsthatit
rithmforlearningcausalstructureunderPB-SCMisproposed
isidentifiableusingtheoverdispersionpropertiesofPoisson
andtheexperimentsdemonstrateandverifytheeffectiveness
BNs.Subsequently,ithasbeenextendedbyaccommodating
oftheproposedmethod.
abroaderspectrumofdistributions(ParkandRaskutti2017).
Inaddition,themodelingofthezero-inflatedPoissondata
Introduction
(Choi,Chapkin,andNi2020)andtheordinalrelationdata
Causaldiscoveryfromobservationaldataespeciallyforcount (NiandMallick2022)anditsidentifiabilityofcausalstruc-
dataisacrucialtaskthatarisesinnumerousapplicationsin tureareinvestigated.However,themajorityofthesemethods
biology(WiufandStumpf2006),economic(WeißandKim modelthecountdatausingBayesiannetworkignoringthe
2014),networkoperationmaintenance(Qiaoetal.2023;Cai inherentbranchingstructureamongthecountingrelationship
etal.2022),etc.Inonlineservices,forinstance,thereason whichisfrequentlyencountered(Weiß2018).
forthenumberofproductpurchasesisofparticularinterest, TakeFigure1asanexample,thecauseofthepurchasing
while finding the underlying causal structure among user event can be inherited from some of the searching events,
behavior from purely observational data is appealing and the pop-up ads event, or exogenously occurs. As a result,
pivotalforonlineoperation. the causal relationship among counts constitutes a branch-
Muchefforthasbeenmadetoaddresstheidentificationof ing structure that can be modeled by a binomial thinning
causalstructurefromobservationaldata(Spirtes,Glymour, operator ‘◦’ (Steutel and van Harn 1979) with an additive
independentPoissondistributionforinnovation.Thatis,the
*Theseauthorscontributedequally.
purchasing count (Y) is affected by the pop-up ads count
†Correspondingauthor.
(X )andthesearchingcount(X )whichcanbemodeledby
Copyright©2024,AssociationfortheAdvancementofArtificial 2 1
Intelligence(www.aaai.org).Allrightsreserved. Y =a ◦X +a ◦X +ϵwherea◦X ≔∑X ξ(a),and
1 1 2 2 n=1 n
4202
raM
52
]LM.tats[
1v32561.3042:viXraξ n(a) ∼ Bern(a),ϵ ∼ Pois.Generallyspeaking,thethinning nomialthinningoperationwithξ n(α) i.i∼.d. Bern(α),Bern(α)
operatormodelsthebranchingstructurethatnoteveryclick istheBernoullidistributionwithparameterα.
willleadtopurchasingwhiletheadditionalnoisemodelsthe
Wefurtherdefinesomegraphicalconcepts.WeusePi↝j =
generalcountofexogenousevents.Thatis,acountrepresents
therandomsizeofanimaginarypopulation,andthethinning {Pi↝j
}∣Pi↝j∣
denotesthesetofalldirectedpathsfromvertex
operation randomly deletes some of the members of this k k=1
populationwhileconcurrentlyintroducingnewimmigration.
itoj,whereP ki↝j = (i,k 1,k 2,...,k p,j),p = ∣P ki↝j ∣−2,
denotethek-thdirectedpathfromvertexitoj.Foreachdi-
Thismodelingapproachfindswidespreadutilityacrossvari-
ousdomains,notablywithinthecontextoftheinteger-value rectedpathPi↝j ,weuseAi↝j =(α ,α ,...,α )
k k i,k1 k1,k2 kp,j
autoregressivemodel(Weiß2018),whichisfirstproposed i↝j
denotethecorrespondingcoefficientssequenceofpathP .
byAl-OshandAlzaid(1987);McKenzie(1985).Despiteits WeletPi↝i ={Pi↝i }alsobeavaliddirectedpathforsk
im-
extensiveused,howtoidentifythecausalstructureinsuch
typ Te oo ef xm plo icd ie tl lyfr ao cm cop uu nr te fly oro tb hs eer bv ra at nio chn ia nl gda st ta rui cs tust ri ell ,u wn ec ple ra or -. p αl ii ,c ki 1ty ◦. XB ie dsi ed ne os t, ew toe pu es re foA rmi k↝ aj c◦ oX nsi ec≔ utiα vk ep t, hj i◦ nn⋯ ing◦ oα pek r1 a,k ti2 o◦
n
pose a Poisson Branching Structural Causal Model (PB- onX ibasedonthepathsequence.
SCM).Weestablishtheidentifiabilitytheoryfortheproposed Goal:Giveni.i.d.samplesD ={x 1(j),...,x ∣( Vj) ∣}m j=1fromthe
PB-SCMusinghigh-ordercumulantwithpathanalysis.The-
jointdistributionP(X),ourgoalistoidentifytheunknown
oreticalresultssuggestthatforanyadjacentvertexX and
causal structure G from D, assuming the data generative
Y,thecausalorderisidentifiableifX isarootvertexand
mechanismfollowsPB-SCM.
has at least two directed paths to Y, or the ancestor of X
with the most directed path to X has a directed path to Y Preliminary
withoutpassingX.Basedontheresultsofthecausalorder
ToaddresstheidentificationofPB-SCM,cumulantareused
wefurtherproposeanefficientcausalskeletonlearningap-
inourworkforbuildingaconnectiontothepath,providing
proachfeaturedwithFFTacceleration.Wedemonstratethe
a solution to the identifiability issue. Here, we recall the
effectivenessoftheproposedcausaldiscoverymethodusing
definitionofcumulantandsomebasisproperties.
syntheticdataandrealdata.
Definition 2 (k-th order joint cumulant tensor). The k-
PoissonBranchingStructuralCausalModel th order joint cumulant tensor of a random vector X =
Inthissection,wefirstformalizethePoissonbranchingstruc- [X 1,...,X n]T is the k-way tensor T X(k) in Rn×⋯×n ≡
turalcausalmodel,andthenweintroducethepreliminaryof (Rn )k whoseentryin(i ,...,i )isthejointcumulant:
1 k
cumulantandsomenecessarypropertiesinthismodel.
ProblemFormulation
T( Xk)
i1,...,ik
=κ(X i1,...,X ik)∶=
Our framework is in the causal graphical models. We use ∑ (−1)L−1 (L−1)!E[∏ X j]⋯E[∏ X j], (2)
Pa(i) = {j∣j →i}, An(i) = {j∣j ↝ i} denote the set (B1,...,BL) j∈B1 j∈BL
ofparents,ancestorsofvertexiinadirectedacyclicgraph
wherethesumistakenoverallpartitions(B ,...,B )of
(DAG),respectively,andAn(i,j)=An(i)∩An(j)denote 1 L
themultiset{i ,...,i }.
thesetofcommonancestorsofvertexiandvertexj.More- 1 k
over,wedefineadirectedpathP =(i ,i ,...,i )inGisa Inthiswork,weusethefollowingspecificcumulantform:
0 1 n
sequenceofverticesofGwherethereisadirectededgefrom Definition3(2Dsliceofjointcumulanttensor). Foraran-
i oj ft eo aci j h+ e1 df go er .a Tn hy e0 s⩽ etj of⩽ vn er− tic1 esw ci ath nt bh ee ac ro raef nfi gc ei den intα ci aj u,i sj a+1
l
dom vector X with k-th order joint cumulant tensor T X(k)
wherek ≥ 2,denoteits2Dmatrixsliceofk-thorderjoint
order,suchthatnolatervariablecausesanyearliervariable.
cumulanttensorasC(k),where
Now,weshowthecausalrelationshipinacausalgraphcan
b Me of do er lm (a Pl Biz -e Sd Ca Ms )th .e LeP to Xiss =on {B Xra ,n .c .h .i ,n Xg Str }uc dt eu nra ol teC sa au ss ea tl C i( ,k j) ∶=κ(X i,X j,⋯,X j). (3)
1 ∣V∣ (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207)
ofrandomPoissoncounts,ofwhichthecausalrelationship k−1times
consist of a causal DAG G(V,E) with the vertex set V =
Cumulant has the property of multilinearity such that
{1,2,...,∣V∣}andedgesetE suchthateachcausalrelation κ(X + Y,Z ,...) = κ(X,Z ,...) + κ(Y,Z ,...). Fur-
followsthePB-SCM: 1 1 1
thermore,anycumulantinvolvingtwo(ormore)independent
Definition1(PoissonBranchingStructuralCausalModel). randomvariablesequalszero,i.e.,κ(ϵ ,ϵ ,...)=0ifϵ and
i j i
ForeachrandomvariableX i ∈X,letϵ i ∼Pois(µ i)bethe ϵ jareindependent.Moreimportantly,anytwovariablesincu-
noisecomponentofX i,thenX iisgeneratedby: mulantareexchangeable,e.g.,κ(X,Y,...)=κ(Y,X,...).
X = ∑ α ◦X +ϵ , (1)
i j,i j i Identifiability
j∈Pa(i)
In this section, we deal with the identification problem of
whereα
j,i
∈(0,1]isthecoefficientfromvertexjtoi,Pa(i)
causal structure under PB-SCM. Due to our identifiability
istheparentsetofX inG,andα◦X ∶=∑Xi ξ(α)isaBi- resultbenefitfromthe‘reducibility’ofcumulantinPoisson
i i n=1 nFig.3(a)andFig.4(a).Interestingly,wehaveC(2) =C(3)in
2,1 2,1
thereversedirection(Fig.4(a))butC(2) ≠C(3)inthecausal
1,2 1,2
direction (Fig. 3(a)), i.e., there exists an asymmetry in the
inequalityrelationsofcumulants.Suchasymmetryintriguing
possibilitytoidentifythecausalorderbetweentwovariables
usingthecumulant.
Figure2:Triangularstructure.Forsimplicity,wedenotedi-
Tounderstandhowthisasymmetryoccursandhenceuse
rectedpathP ∶X −→a X andP ∶X −b →1 X −b →2 X with ittoidentifythecausalrelations.Wefirstdiscusstheiden-
1 1 2 2 1 3 2
sequenceofpathcoefficientsA =(a)andA =(b ,b ). tificationinthesimplescenariothatthecausevariableisa
1 2 1 2
rootvertexinG,andthenwegeneralizesuchresultsintothe
scenariothatthecausevariableisnotroot.
distribution,wefirstcharacterizesuchpropertyinTheorem
1.Afterwhich,anexampleisprovidedtorevealtheintrinsic IdentificationWhenCauseVariableIsRoot
relationbetweenthecumulantandthepathinacausalgraph Westartwiththecasethatthecausevariableisrootvertex,
underPB-SCM.Basedonsuchconnection,wecompletethe inwhichourgoalistoidentifycausaldirectioneventhough
identifiabilityresultsthataredividedintothecasewhenthe we do not know it is a root vertex. Recall the previous ex-
cause variable is root (Theorem 3) and the case when the ample,thekeyofidentificationistheinequalityC(2) ≠C(3)
causevariableisnotroot(Theorem6). 1,2 1,2
renderinganasymmetryforacausalpair.Tounderstandhow
Wefirstintroduceafundamentalpropertyofcumulantin
itoccurs,weseektocharacterandleveragesuchinequality
PB-SCMthatthecumulantisreducible:
constraintsofcumulantsinacausalgraphtoinferthecausal
Theorem1(Reducibility). GivenaPoissonrandomvariable order(Theorem4).
ϵandndistinctsequencesofcoefficientsA 1,...,A n,wehave Here,webeginwithtwobasicobservations,whichillus-
κ(A ◦ϵ,...,A ◦ϵ,...,A ◦ϵ,...,A ◦ϵ) tratethatinequalityconstraintsofcumulantsaredrivenby
(cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)1 (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)1 (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)n (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)n (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) the number of paths between two variables. As shown in
k1times kntimes (4) Fig.3(a),onemayseethat(i)thedecompositionofC
1,2
is
=κ(A 1◦ϵ,...,A n◦ϵ) composedbyaseriesofcumulantsofthecommonnoise(ϵ
1
in this example) between X and X , which is due to the
whereeachA ◦ϵrepeatsk ≥1timesintheoriginalcumu- 1 2
i i factthatanycumulantinvolvingtwo(ormore)independent
lantandonlyappearsonceinthereducedcumulant.
randomvariablesequalszero;(ii)moreover,suchdecomposi-
SucharesultisageneralizationofthepropertyofthePois- tionrelatestothenumberofpathsbetweenX andX since
1 2
sondistributionsincethecumulantofthePoissondistribution X = A ◦ϵ +A ◦ϵ +b ◦ϵ +ϵ andbymultilinear-
2 1 1 2 1 2 3 2
isidenticalineveryorder. ity,thecumulantwillbesplitexponentiallyastheorderof
cumulantincrease.Withtheseobservations,thereasonwhy
MotivatingExample C(2) ≠ C(3) is that there exists more than one path in the
1,2 1,2
Beforedescribingourtheoreticalresults,weuseamotivating causaldirectionwhilezeropathinthereversedirection,i.e.,
example to show the challenges of the non-identifiability ∣P1↝2 ∣ = 2,∣P2↝1 ∣ = 0. As a result, C(2) = C(k) for all
issues and then introduce the basic intuition regarding in 2,1 2,1
k ≥2ordercumulantinthereversedirection.
whatcaseandhowcanweidentifythePB-SCM.
Inthefollowing,wearticulatetheunderlyinglawofthe
To see the non-identifiability issue, we can show that a
cumulant in PB-SCM and propose a closed-form solution
reversedmodelalwaysexistsinatwo-variablesystem.
to it. The first important observation is that due to the re-
Remark1. Foranytwovariablescausalgraph,thecausal
ducibility and the exchangeability of cumulant, the
C(k)
direction of PB-SCM is not identifiable and a distributed 1,2
for k ≥ 3 is only composed by three distinct cumulants:
equivalentreversedmodelexists.
κ(ϵ ,A ◦ϵ ),κ(ϵ ,A ◦ϵ ),andκ(ϵ ,A ◦ϵ ,A ◦ϵ )
1 1 1 1 2 1 1 1 1 2 1
Forinstance,considerX 1 →X 3inFig.2,thedistributed withvaryingnumberofthesecumulants.Inparticular,ifwe
equivalentreversemodelsatisfiesX =ˆb ◦X +ϵˆ ,where definethesummationofcumulantsthatonlycontainsone
1 1 3 1
ˆb 1 =b 1µ 1/(b 1µ 1+µ 3)andϵˆ 1 ∼Pois(µ 1−b 1µ 1)suchthat pathasΛ1 1↝2 (ϵ 1 ↝X 2)≔κ(ϵ 1,A 1◦ϵ 1)+κ(ϵ 1,A 2◦ϵ 1)
thisdirectionisnotidentifiable. andthesummationofcumulantsthatcontainstwopathsas
Fortunately,wefindthatthecausaldirectionisstillpossi- Λ1↝2 (ϵ ↝X )≔κ(ϵ ,A ◦ϵ ,A ◦ϵ ),wewillhavethe
2 1 2 1 1 1 2 1
bletoidentifyinamoregeneralstructure.Consideringthe followingclosed-formsolution:
causalrelationshipbetweenX andX inFig.2,herewepro-
1 2
videanintuitiveexampletoshowhowtoidentifysuchcausal 3
directionbyutilizingtherelationshipbetweencumulantand C 1(4 ,2) =Λ1 1↝2 (ϵ 1↝X 2)+ ∑ (m m )Λ1 2↝2 (ϵ 1↝X 2) (5)
1 2
path. Considering the cumulant C
1,2
with different orders, mm1 1+ ,mm 22 >= 03
wecanobservedifferentbehaviorsofcumulantinthecausal
directionandthereversedirection.Thankstothereducibility where( 3 )isthemultinomialcoefficient,indicatingthe
inTheorem1,e.g.,κ(A ◦ϵ ,ϵ )=κ(A ◦ϵ ,ϵ ,ϵ ),the numberm o1 fm w2 aysofplacing3distinctobjectsinto2distinct
1 1 1 1 1 1 1
cumulantswithdifferentordersforX andX isshownin binswithm objectsinthefirstbin,m objectsinthesecond
1 2 1 2(2)
(2) 𝐶𝐶3,2 =𝜅𝜅(𝑋𝑋3,𝑋𝑋2)=𝜅𝜅 ϵ3,𝑏𝑏2∘𝜖𝜖3 +𝜅𝜅 𝑏𝑏1∘ϵ1,A1∘𝜖𝜖1 +𝜅𝜅 𝑏𝑏1∘ϵ1,A2∘𝜖𝜖1
𝐶𝐶1,2 =𝜅𝜅 𝑋𝑋1,𝑋𝑋2 = 𝜅𝜅 ϵ1,A1∘𝜖𝜖1 +𝜅𝜅 ϵ1,A2∘𝜖𝜖1 ≔(𝑏𝑏1,𝐴𝐴1) ≔(𝑏𝑏1,𝐴𝐴2)
≔(1,𝐴𝐴1) ≔(1,𝐴𝐴2) split 𝜅𝜅ov ϵe 3r
,𝑏𝑏2 ∘𝜖𝜖3,𝑋𝑋2 +𝜅𝜅
𝑏𝑏1∘ϵ1,A1∘𝜖𝜖1,s 𝑋𝑋pl 2it o +v e 𝜅𝜅r
𝑏𝑏1 ∘ϵ1,A2∘𝜖𝜖1,𝑋𝑋2
split over (3)
(3) 𝜅𝜅 ϵ1,A1∘𝜖𝜖1,𝑋𝑋2 +𝜅𝜅 ϵ1,A2∘𝜖𝜖1,𝑋𝑋2 𝐶𝐶3,2 = X2 X2
𝐶𝐶1,2 = X2 𝜅𝜅 ϵ3,𝑏𝑏2∘𝜖𝜖3,𝑏𝑏2∘𝜖𝜖3 + 𝑏𝑏1,A1,A1 + 𝑏𝑏1,A1,A2 + 𝑏𝑏1,A2,A1 + 𝑏𝑏1,A2,A2
1,A1,A1 + 1,A1,A2 + 1,A2,A1 + 1,A2,A2 2×Λ1 2↝2 (𝑏𝑏∘𝜖𝜖1↝𝑋𝑋2)
𝐶𝐶1( ,4 2) = 1,A1,A1,A1 +
1,A12 ,A× 1,Λ A1 22↝
+
2
⋯
(1 +∘ 1𝜖𝜖 ,1 A2↝ ,A2𝑋𝑋 ,2 A)
1 + 1,A2,A2,A2
𝐶𝐶3( ,4 2) =𝜅𝜅 ϵ3,𝑏𝑏F2ro ∘𝜖𝜖m3 , 𝑋𝑋𝑏𝑏 32∘to 𝜖𝜖 3 𝑋𝑋, 2𝑏𝑏2∘𝜖𝜖3 + 𝑏𝑏1,A1 F,A ro1, mA1
th
6+
e
×co𝑏𝑏 Λ1
m1
2, ↝A m21 ,
o
(A 𝑏𝑏n1
∘
, aA 𝜖𝜖n12
c
↝e+
st
𝑋𝑋o⋯
2r
)+ 𝑏𝑏 to1, A2,A2,A2
(a) Cumulantdecomp 6os ×it Λio1 2↝n2o (f 1t ∘he 𝜖𝜖1c ↝au 𝑋𝑋sa2)lpairX 1↝X 2(b) Cumulantdecompositiono fthecausalpairX
3
↝X
2
𝑋𝑋w 1he 𝑋𝑋re 2X 3is
whereX isroot. notroot.
1
Figure 3: Illustration of decomposing the cumulant of causal direction, C and C , in triangular structure (Fig. 2). For
1,2 3,2
simplicity,wedenoteκ(ϵ ,A ◦ϵ ,...,A ◦ϵ )by(1,A ,...,A )anddenoteκ(b ◦ϵ ,A ◦ϵ ,...,A ◦ϵ )by(b ,A ,...,A ).
i i i j i i j 1 i i i j i 1 i j
where( n−1 )= (n−1)! isthemultinomialcoeffi-
(2) (2)
m1m2⋯mk m1!m2!⋯mk!
𝐶𝐶2,1=𝜅𝜅𝐴𝐴1∘ 𝜖𝜖1,𝜖𝜖1+ 𝜅𝜅(𝐴𝐴2∘ 𝜖𝜖1,𝜖𝜖1) 𝐶𝐶2,3= 𝜅𝜅𝑏𝑏2 ∘𝜖𝜖3,𝜖𝜖3 + 𝜅𝜅𝐴𝐴1 ∘𝜖𝜖1,𝑏𝑏1 𝜖𝜖1+ 𝜅𝜅(𝐴𝐴2 ∘𝜖𝜖1,𝑏𝑏1 𝜖𝜖1) cients.
≔(𝐴𝐴1,1) ≔(𝐴𝐴2,1) ≔(𝐴𝐴1,𝑏𝑏1) ≔(𝐴𝐴2,𝑏𝑏1)
𝐶𝐶2 (( 4,3 1 )) = 𝐴𝐴1,1,1 +(𝐴𝐴2,1,1) 𝐶𝐶 𝐶𝐶2 2( (,3 ,43 3)
)
== 𝜅𝜅𝜅𝜅
F
𝑏𝑏r𝑏𝑏 2o2
∘m
∘ 𝜖𝜖𝜖𝜖 3,3 𝜖𝜖, 3𝜖𝜖 ,3
𝜖𝜖t
3o,𝜖𝜖
,
𝜖𝜖3 3+
+
𝐴𝐴 𝐴𝐴1 1, ,𝑏𝑏 𝑏𝑏11
F
,, 𝑏𝑏r𝑏𝑏 1o1
,m
𝑏𝑏1+ +( 𝐴𝐴 (2 𝐴𝐴t,
o
2𝑏𝑏
,
1 𝑏𝑏, 1𝑏𝑏 ,𝑏𝑏1 1)
,𝑏𝑏1)
o thf
eT
th
jh oee io
c
nar te
u
cm
s ua
ml2
o
up
r ld
ala
e
ny
r
ts
a
asa nn diti pim
n
atp
tr
hoo dr it nua fcn oet
rs
mro
th
ale
e
tiocin
no
.nth
n
Me
e
oci
t
rd
i
eoe onn vt ebifi
re
,c
tw
sa it neio
e
cnn
e
(
s𝐶𝐶
a
it2
)
i,1 oC=
n𝐶𝐶2u
(𝐴𝐴
,2 1
om)1
=
f, 𝐶𝐶u1 X2,
(l
,31 1a), =1
n ↝𝐶𝐶t
2(+
,4d X1)
=( e𝐴𝐴
c
𝜅𝜅2 .o, 𝑋𝑋1 m2, ,1 𝑋𝑋p, 11 o)
-( Xb) .C umu 𝐶𝐶l2 (
a𝑋𝑋
,2
3n)3
t =d
𝐶𝐶𝑋𝑋 e2(3
c,3 3o) m =p 𝐶𝐶2o( ,4 3s) it =io
𝜅𝜅n𝑋𝑋1
𝑋𝑋o2f
,𝑋𝑋𝑋𝑋 X33
2 ↝ e Ev qe .r (y 3o ),rd ae nr do tf ht uh se e2 vD ers yli oc re dj eo rin ot fc Λum ku cala nn at lc sa on bb ee oo bb tt aa ii nn ee dd bb yy
2 1 3
solvingtheequationinEq.(C.1).ByusingΛ weareableto
k
understandtheidentifiabilityinthefollowingtheorem:
Figure4:Illustrationofdecomposingthecumulantofreverse
direction,C andC ,intriangularstructure(Fig.2).
2,1 2,3 Theorem3(Identifiabilityforrootvertex). Foranyvertexi
andj,whereiistherootvertexingraphG,ifC(3)−C(2) ≠0,
i,j i,j
bin.Asaresult,wewilleventuallyhave6×Λ1 2↝2 (ϵ 1 ↝X 2) thenC(3)−C(2) =0andX istheancestorofX .
asshowninFig.3(a).Generally,wedefineΛi↝j (A◦ϵ ↝ j,i j,i i j
k i
X j)asthesummationofcumulantsthatcontainkpathsfrom Intuitively,basedonTheorem2,wehaveC(3) −C(2) =
rootvertexitoj: i,j i,j
Definition4(k-pathcumulantssummationforrootvertex).
Λi 2↝j (1◦ϵ i↝X j),andthusC i( ,3 j) −C i( ,2 j) ≠ 0indicatesthat
Giventwoverticesiandj,fork
⩽∣Pi↝j
∣,thek-pathcumu-
thereexistsmorethanonepathfromitoj thanthereverse
direction.Thatis,thecausaldirectionforrootvertexisiden-
lantssummationfromvertexitoj isgivenby:
tifiableifthereareatleasttwodirectedpaths:
Λi↝j (A◦ϵ ↝X )
k i j Theorem4(GraphicalImplicationofIdentifiabilityforRoot
= ∑ κ(A◦ϵ i,Ai l1↝j ◦ϵ i,...,A li k↝j ◦ϵ i), (6) Vertex). Forapairofverticesiandj ingraphG,ifvertexi
1≤l1<l2<...<lk≤∣Pi↝j∣ isarootvertexandexistsatleasttwodirectedpathsfromi
where l ,...,l ∈
Z+
, A is an arbitrary sequence of co-
toj,i.e.,∣Pi↝j ∣≥2,thenthecausalorderbetweeniandj
1 k
efficients. For k > ∣Pi↝j ∣, Λi↝j ≡ 0 and for k = 1, isidentifiable.
k
Λi↝i (A◦ϵ ↝X )=κ(A◦ϵ ,ϵ ),andk >1,Λi↝i ≡0.
1 i i i i k
IdentificationWhenCauseVariableIsNotRoot
Intuitively,Eq.(6)isasummationofallcumulantsthat
contain k paths information from vertex i to j , and
Λi↝i
1 Inthissection,weaimtogeneralizetheidentificationresult
denotes the relation from the noise to itself. Based on the
fromtherootvertextothenon-rootvertex.
k-path cumulants summation,
C(n)
can be decomposed as
i,j Whenvertexiisnotroot,themaindifferenceisthatthere
follows:
mightexistmorethanonecommonnoisebetweentwovari-
Theorem 2. For any two vertices i and j where i is root
ablesduetothepossiblecommonancestor.Therefore,one
vertex,i.e.,vertexihasanemptyparentset,the2Dsliceof
may extend the result from the root vertex by considering
jointcumulantC(n)satisfies:
eachnoisetermastheseparatedrootvertex.Wepresenta
i,j
generalversionofk-pathcumulantssummationasfollows,
C i( ,n j) =∑n k=− 11 ∑ (
m
1n m− 2⋯1
m
k)Λi k↝j (1◦ϵ i↝X j). w cuh mic uh lac na tsn sb ue me mx ap tr ie os ns se fd oras tht eh re ooa tg vg ere rtg ia ct ei so .n of the k-path
m1+⋯ m+ lm >k 0=n−1
(7) Definition 5 (k-path cumulants summation). The k-pathcumulantssummationfromvertexitovertexj isgivenby:
Λ˜ (X ↝X )=Λi↝j (1◦ϵ ↝X )
k i j k i j
∣Pm↝i∣ (a) Notidentifiable. (b) Identifiable.
+ ∑ ∑ Λm↝j (Am↝i◦ϵ ↝X ).
k h m j
m∈An(i,j)∪{j} h=1 Figure5:IllustrationoftheidentifiabilityofX →Y.
(8)
whereΛ isthek-pathcumulantssummationforrootvertex,
k
∣Pm↝i ∣isthenumberofdirectedpathsfrommtoi. Inaddition,thek-pathcumulantssummationΛ˜ k(X
i
↝
X )willbe‘dominated’bythevariables(mightbethecom-
Withthegeneralk-pathcumulantssummation,thegeneral j
monancestororiitself)thathasthemostpathstoj sinceit
jointcumulantcanbedecomposedasfollows:
istheaggregationofallthedirectedpathsfrombothcom-
Theorem 5. For any two vertices i and j, the 2D slice of mon ancestor and i. Therefore, for a non-root vertex, it is
jointcumulantC(n)satisfies: possibletobenon-identifiablebyTheorem3ifthedominant
i,j
variableisthecommonancestor.Specifically,weprovidethe
C i( ,n j)=∑ kn =− 1 m1 1+⋯∑ +mk=n−1(m 1n m− 2⋯1 m k)Λ˜ k(X i↝X j), (9) Tgr ha ep oh ric ea mlim 7p (Glic raat pi ho in cao lf Is muc ph lii cd ae tn ioti nfia ob fil Ii dty eng ti iv fie an bia ls itf yo )l .lo Fw os r:
ml>0 a pair of causal relationship i → j. The causal order of
where( n−1 )= (n−1)! isthemultinomialcoeffi- i,j isidentifiablebyTheorem6,if(i)vertexiisarootver-
cients. m1m2⋯mk m1!m2!⋯mk! tex and ∣Pi↝j ∣ ≥ 2; or (ii) there exists a common ances-
tork ∈argmax{∣Pl↝i ∣∣l∈An(i,j)}hasadirectedpath
Toseetheconnectionwiththecaseofrootvertex,wetake
l
X 3 →X 2inFig.2asexample.SinceX 3canbeexpressedas fromktoj withoutpassingiinG.
X =b ◦ϵ +ϵ ,asshowninFig.3(b),wecanseparatethe
3 1 1 3 OneoftheexamplesisgiveninFig.5,inwhichFig.5(a)is
cumulantintotwopartsκ(ϵ ,X ),κ(b ◦ϵ ,X ),whichcan
3 2 1 1 2 notidentifiablebutFig.5(b)isidentifiable.Thereasonisthat
beconsideredasthecumulantstartingfromvertexX toX
3 2 ZisthedominantcommonancestorofX,Y,andalldirected
andX toX ,respectively.Asaresult,thegeneralk-path
1 2 pathsfromZtoY willpassXmakingitunidentifiablebased
cumulantssummationcanbeexpressedastheaggregateof
onTheorem7.Incontrast,Fig.5(b)includesanadditional
alldifferentΛ startingwiththecorrespondingnoiseterms.
k directed path Z → C → Y without passing X making
Forinstance,forX →X inFig.2,wehave:
3 2 X →Y identifiable.Thisintriguinglyimpliesthatadenser
Λ˜ (X ↝X ) structurewouldfacilitatetheeffectivenessofourmethod.
2 3 2
Generallyspeaking,oncethecausalorderisidentified,one
=Λ3 2↝2 (1◦ϵ 3 ↝X 2)+Λ1 2↝2 (b 1◦ϵ 1 ↝X 2)≠0, (10) mayidentifythecompletecausalstructurebyorientingedges
(cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207)
=0 =κ(b1◦ϵ1,A1◦ϵ1,A2◦ϵ1) basedonthecausalorderinthecausalskeleton.Suchimple-
mentationwillbeprovidedinthenextsection.Bythis,the
whereEq.(10)containstwodifferenttermsstartingfromϵ
3 identifiabilityofcausalstructureunderPB-SCMisanswered.
andϵ ,respectively.Inparticular,sincethereonlyexistsone
1
directedpathfromX 3 toX 2,Λ3 2↝2 iszerowhileX 1 toX 2 LearningCasualStructureForPB-SCM
hastwopathsandthusΛ1↝2
isnotzero.Similarly,forthe
2 Inthissection,weproposeacausalstructurelearningalgo-
reversedirection,wehave
rithmforPB-SCM.Ourmethodinvolvestwosteps:learning
Λ˜ (X ↝X )=Λ2↝3 (1◦ϵ ↝X )+Λ3↝3 (b ◦ϵ ↝X ) the skeleton of DAG G and inferring the causal direction
2 2 3 (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)2 (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)2 (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)3 (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)2 (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)2 (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)3 (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)3 (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) usingtheresultsdevelopedinTheorem6.
=0 =0
+Λ1↝3 (A ◦ϵ ↝X )+Λ1↝3 (A ◦ϵ ↝X )=0, LearningCausalSkeleton Tolearnthecausalskeleton,
2 1 1 3 2 2 1 3 insteadofusingtheconstraint-basedmethod,weproposea
(cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207)
=0 =0 likelihood-basedmethod.Thisboostssampleefficiencyas
(11)
thelikelihoodofPB-SCMcapturesitsbranchingstructure
whereΛ˜ iszerosincethereare0directedpathfromX to
2 2 buttheconstraint-basedmethoddoesnot.
X andonly1directedpathfromX orϵ toX .Intuitively,
3 1 3 3 Given a set of count data D and model parameters
t tuh re eg se thn eer na ul mk- bp ea rt oh fc du im reu ctl ea dnt ps as tu hm sfm roa mtio thn eΛ˜ c( oX mi m↝ onX anj c) ec sa top r- Θ = {A=[α i,j]∈[0,1]∣V∣×∣V∣,µ=[µ i]∈R∣ ≥V 0∣ }, the
toj.Moreover,foranytwoadjacencyvertexi→j andtheir log-likelihoodisMarkovrespecttoG,thatisL(G,Θ;D)=
c jo im sgm ro ean tea rnc oe rs eto qr um al, toth te hn atu fm rob mer mofd toir ie ,c ate nd dp that uh ss ,f thro em cam ust ao
l
∑ j∣D =1∣∑∣ iV =1∣logP Θ(X
i
=x( ij) ∣X
Pa(i)
=x( Pj a) (i)). However,
calculatingthelikelihooddirectlyusingtheprobabilitymass
ordercanbeidentifiedusingthefollowingstrategy:
function is costly. Therefore, we propose to calculate the
Theorem6(IdentificationofPB-SCM). Ifthereexistk
∈Z+
probabilitymassfunctionbyusingtheprobability-generating
suchthatΛ˜ (X ↝X )≠0andΛ˜ (X ↝X )=0forany function(PGF).Indetail,foreachconditionaldistributionof
k i j k j i
twoadjacencyvertexiandj,thenX istheparentofX . X ,thelikelihoodcanbecalculatedasfollows:
i j iTheorem8. LetG Xi∣XPa(i)(s)bethePGFofrandomvari- Algorithm1:CausalDiscoveryforPB-SCM
ableX givenitsparentsvariableX ,wehave:
i Pa(i) Input:DatasetD,MaxorderK
P(X i=k∣X Pa(i)=x Pa(i))= k1 !∂kG X (i ∂∣X s)P ka(i)(s)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)s=0
1
GOu ′ ←tpu et m:L pe tyar gn rin ag phC ,a Lus ∗ pal ←G −ra ∞ph ;G
// Learning Causal Skeleton
= ∑ µt iiexp(−µ i) ∏ (x j)tjα jtj ,i(1−α j,i)xj−tj , 2 whileL∗ p(G∗ ,Θ∗ ;D)<L′ p(G′ ,Θ′ ;D)do
ti+ ∑ tj=k t i! j∈Pa(i) t j! 3 G∗ ←G′ withlargestL′ p(G′ ,Θ′ ;D)
j∈Pa(i) (12) 4 foreveryG′ ∈V(G∗ )do
′ ′ ′ ′
wheret ≤ x ,(x ) ≔
x j!
isthefallingfactorial,
5 EstimateΘ andrecordscoreL p(G,Θ;D)
j j j tj (x
j
−t j)!
6
G←TransferG∗
toaskeleton
µ =E[ϵ ],andϵ isthenoisecomponentofX .
i i i i // Learning Causal Direction
TheresultofEq.(G.1)canbeconvertedtoapolynomial 7 foreachpairX i−X j ∈Gdo
coefficientaftertakingpolynomialmultiplication,whichcan 8 fork ←1∶K do
be accelerated via Fast Fourier Transform (FFT) (Cormen 9 ObtainΛ˜ k ateachsidebysolvingEq.(E.1)
eta Gl. e2 n0 e2 ra2 l) l. yA ,thd eet la ii kle ed lihd ois oc du -s bs aio sen dis mg eiv the on din wt ih le lts eu np dpl te om pe rn ot -. 10 TestwhetherΛ˜ k equalto0foreachside
duceexcessiveredundantcausaledges.Sucheffectcanbe 11 ifΛ˜ k(X i↝X j)≠0∧Λ˜ k(X j↝X i)=0then
alleviatedbyintroducingtheBayesianInformationCriterion 12 Orient“X i →X j”inG
(BIC)penaltydlog(m)/2intotheL(G,Θ;D),wheredis 13 ifΛ˜ k(X i↝X j)=0∧Λ˜ k(X j↝X i)≠0then
thenumberofedgeofGandmisthesizeofdatasetD.The 14 Orient“X i ←X j”inG
penalizedobjectivefunctionisupdatedasfollows:
L (G,Θ;D)=L(G,Θ;D)−dlog(m)/2 (13) 15 ReturnG
p
WemaximumtheobjectivefunctionL (G,Θ;D)byusing
p
aHill-Climbing-basedalgorithmasshowninLines2-6of
A fol rg mor ait sh tm ruc1 t. uI rt em sea ain rcl hy inco gn ss ci hst es mo ef btw yo tap kh ina gse os n. eFi sr ts et p,w ade dp ine gr-
,
Eq. (13) is O(∑m j=1∑∣ iV =1∣ ( (∣ ∣V V∣ ∣+ −x i)( ij !x)− i(ji )) !! ), by using
∗ FFT acceleration, this complexity can be reduced to
deleting,andreversingthegraphG inthelastiteration,i.e.,
in Line 4, V(G∗ ) represents a collection of the one-step O(∑m j=1∑∣ iV =1∣ (∣V∣−i+1)2x i(j)log(∣V∣−i+1)2x( ij) ),
∗ ′ wheremisthesamplesize.
modifiedgraphofG .Second,byfixingthegraphG,we
′
estimatetheparameterΘ ofthemodelviaoptimizerwith
Experiment
initialvaluesfromapproximatedcovarianceestimatesand
′ ′ ′
thencalculatetheL (G,Θ;D)inLines5.Iteratingthetwo SyntheticExperiments
p
stepsaboveuntilthelikelihoodnolongerincreases.Inthe
∗ Inthissection,wetesttheproposedPB-SCMonsynthetic
end,wetransformG intoaskeleton(Line6).Thecorrect-
data.Wedesigncontrolexperimentsusingsyntheticdatato
nessofsuchaprocedurecanbeguaranteedbytheconsistent
testthesensitivityofsamplesize,numberofvertices,and
propertyofBICwhichisdiscussedin(Chickering2002).
differentindegreerate.ThebaselinemethodsincludeOCD
Learning Causal Direction Given the learned skeleton, (NiandMallick2022),PC(Spirtes,Glymour,andScheines
weorienteachundirectededgeusingthek-pathcumulants 2000),GES(Chickering2002).Wefurtherprovidetheresults
summation,accordingtoTheorem6.Indetail,foreachundi- usingthetrueskeletonaspriorknowledge(PB-SCM-P)to
rected edge (i,j) ∈ E, we calculate Λ˜ (X ↝ X ) and demonstratetheeffectivenessoflearningcausaldirection.
k i j
Λ˜ (X ↝ X ) for k = 1,...,K until one of them being Inthesensitivityexperiment,wesynthesizedatawithfixed
k j i parameterswhiletraversingthetargetparameterasshownin
zeroorkreachestheupperlimitK.Wethenorientthedirec-
Fig.6.Thedefaultsettingsareasfollows,samplesize=30000,
tionbasedonTheorem6(Lines11-14).
To assess whether Λ˜ is equal to 0, a bootstrap hypoth- number of vertices=10, indegree rate=3.0, range of causal
k coefficientα ∈ [0.1,0.5],rangeofthemeanofPoisson
esis test is conducted (Efron and Tibshirani 1994) while a i,j
noise µ ∈ [1.0,3.0], the max order of cumulant K = 4.
thresholdcanbeusedfororientationoncesuchtestingfails. i
In detail,we calculatethe statistic
Λ˜+
from N resampling
Eachsimulationisrepeated30times.
k As shown in Fig.6, we conduct three different control
+ + +
datasetD ∈{D i∣D
i=1,..,N
⊂D,}.Then,weestimatethe
experimentsforPB-SCM.Overall,ourmethodoutperforms
distributionP(Λ˜+
)bykerneldensityestimatorandcentralize allthebaselinemethodsinallthreecontrolexperiments.
k
ittomeanzero.Finally,thep-valueofΛ˜ fromtheoriginal In the control experiments of the indegree rate given in
k
datasetcanbeobtained. Fig.6(a),astheindegreeratecontrolsthesparseofcausal
Complexity Analysis We provide the complexity structure,thehighertheindegreerate,thelesssparseincausal
of calculating likelihood in the worst cases—when structureleadingtoadecreaseofperformanceofthebaseline
graph is complete. Specifically, the complexity of methods.Incontrast,PB-SCMkeepsgivingthebestresults1.00 1.00 1.00
0.75 Algorithm 0.75 Algorithm 0.75 Algorithm
PB-SCM-P PB-SCM-P PB-SCM-P
0.50 PB-SCM 0.50 PB-SCM 0.50 PB-SCM
PC PC PC
0.25 GES 0.25 GES 0.25 GES
OCD OCD OCD
0.00 0.00 0.00
2.0 2.5 3.0 3.5 3 5 7 10 13 15 1000 3000 5000 700010000200003000050000
Avg. Indegree Rate Number of vertices Sample Size
(a) SensitivitytoAvg.IndegreeRate (b) SensitivitytoNumberofvertices (c) SensitivitytoSampleSize
Figure6:F1intheSensitivityExperiments
inallindegreerates.Thereasonisthatourmethodbenefits Cause(X) Effect(Y) X →Y Y →X
fromthesparsityofthegraphandthedenserstructurewould
resultinmorecausalorderbeingidentifiedwhichverified Yellowcard Λ˜ ≠0 Λ˜ =0
k=2 k=2
thetheoreticalresultinourwork. Foul 2ndY.card Λ˜ ≠0 Λ˜ =0
k=3 k=2
Inthecontrolexperimentsofthenumberofverticesgiven Redcard Λ˜ =0 Λ˜ =0
inFig.6(b).Ourmethodoutperformsallthebaselinemethods,
k=1 k=1
2ndY.card Λ˜ ≠0 Λ˜ =0
showingaslightdecreaseasthenumberofnodesincreases, Yellowcard k=3 k=2
yetstilldemonstratingreasonableperformance.Thereason Substitution Λ˜ k=2 ≠0 Λ˜ k=2 =0
might be that with an increasing number of vertices, the 2ndY.card Redcard Λ˜ ≠0 Λ˜ =0
k=2 k=2
number of paths for both directions also increases, which
requires a higher-order cumulant to obtain the asymmetry.
Table1:Theresultofreal-worlddatasetexperiment.
However,estimatinghigh-ordercumulantisdifficultandhas
alargevariancewhichleadstoadecreaseinperformance.
InthecontrolexperimentsofsamplesizeshowninFig.6(c),
asthesamplesizeincreases,ourmethod’sperformancecon-
tinuestoimproveandoutperformsallthebaselinemethods.
Thissuggestsasufficientsamplesizeisbeneficialforesti- (a) GroundTruth (b) Result
matingaccuratecumulant.
Figure7:FootballDatasetResult(F:Foul,Y :Yellowcard,
RealWorldExperiments 1
Y :Secondyellowcard,R:Redcard,S:Substitution).
2
WealsotesttheproposedPB-SCMonareal-worldfootball
eventsdataset1,whichcontains941,009eventsfrom9,074
footballgamesacrossEurope.Forthisexperiment,wefocus ThissuggestsahiddenconfounderbetweenFoulandYellow
onthecausalrelationinthefollowingcountofevents:Foul, card,possiblyrelatedtothefootballteam’sstylewhichalso
Yellowcard,Secondyellowcard(abbreviatedas2ndY.card), coincideswithotherpathfindings.Moreover,thecausaldi-
Redcard,andSubstitution.Theseeventspossessclearcausal rection between Yellow card and Substitution is identified
relationshipsaccordingtotherulesofthefootballgame.Our suggesting a hidden confounder or indirect relation exists.
goalistoidentifythecausalrelationshipfromtheobserved This result suggests the effectiveness of our method when
count data while reasoning the possible number of paths dealingwithcomplexreal-worldscenarios.
betweentwoeventsasabyproductofourmethod.
Conclusion
In detail, we employ the bootstrap hypothesis test with
0.05 significance level to test whether Λ˜ is equal to zero. Inthiswork,westudytheidentificationofthePoissonbranch-
k
TheresultisshowninTable1.ThecolumnofX→Y shows ingstructuralcausalmodelusinghigh-ordercumulant.We
thehighestorderofcumulantssummationΛ˜ (X↝Y)that establishalinkbetweencumulantsandpathsinthecausal
k
isnotequaltozerowhilethecolumnofY →X showsthe graphunderPB-SCM,showingthatcumulantsencompass
lowestorderofcumulantssummationthatequalszero. informationaboutthenumberofpathsbetweentwovertices,
TheresultsaregiveninFig.7(b).Generally,PB-SCMsuc- whichisretrievable.Byleveragingthislink,weproposethe
cessfullyidentifiesfivecause-effectpairs,exceptforFoul→ identifiabilityofthecausalorderofPB-SCManditsgraphi-
Redcard.Thepossiblereasonmightbeattributedtotheweak calimplication.Withtheidentifiabilityresult,weproposea
causalinfluencesinceonlyafewseriousfoulswillresultina causalstructurelearningalgorithmforPB-SCMconsisting
redcard.Interestingly,WefindΛ˜ (Foul→Yellowcard)≠ oflearningcausalskeletonandlearningcausaldirection.Our
2
0,indicatingtwopathsfromF oritsancestortoYellowcard. theoreticalresultsandthepracticalalgorithmwillhopefully
furtherinspireaseriesoffuturemethodstodealwithcount
1https://www.kaggle.com/datasets/secareanualin/football- dataandmovetheresearchofcausaldiscoveryfurthertoward
events achievingreal-worldimpactsindifferentrespects.
1F 1F 1FAcknowledgments Spirtes,P.;Meek,C.;andRichardson,T.1995. Causalin-
ferenceinthepresenceoflatentvariablesandselectionbias.
ThisresearchwassupportedinpartbyNationalKeyR&D
InProceedingsoftheEleventhconferenceonUncertaintyin
ProgramofChina(2021ZD0111501),NationalScienceFund
artificialintelligence,499–506.
forExcellentYoungScholars(62122022),NaturalScience
FoundationofChina(61876043,61976052),themajorkey Steutel,F.W.;andvanHarn,K.1979. Discreteanaloguesof
projectofPCL(PCL2021A12).ZM’sresearchwassupported self-decomposabilityandstability.TheAnnalsofProbability,
bytheChinaScholarshipCouncil(CSC). 893–899.
Tsamardinos,I.;Brown,L.E.;andAliferis,C.F.2006. The
References max-minhill-climbingBayesiannetworkstructurelearning
algorithm. Machinelearning,65:31–78.
Al-Osh,M.A.;andAlzaid,A.A.1987. First-orderinteger-
valuedautoregressive(INAR(1))process. JournalofTime Weiß,C.H.2018. Anintroductiontodiscrete-valuedtime
SeriesAnalysis,8(3):261–275. series. JohnWiley&Sons.
Weiß,C.H.;andKim,H.-Y.2014. Diagnosingandmodeling
Cai,R.;Qiao,J.;Zhang,Z.;andHao,Z.2018.Self:structural
extra-binomialvariationfortime-dependentcounts. Applied
equational likelihood framework for causal discovery. In
StochasticModelsinBusinessandIndustry,30(5):588–608.
ProceedingsoftheAAAIConferenceonArtificialIntelligence,
volume32. Wiuf,C.;andStumpf,M.P.2006. Binomialsubsampling.
ProceedingsoftheRoyalSocietyA:Mathematical,Physical
Cai,R.;Wu,S.;Qiao,J.;Hao,Z.;Zhang,K.;andZhang,X.
andEngineeringSciences,462(2068):1181–1195.
2022. THPs: Topological Hawkes Processes for Learning
CausalStructureonEventSequences. IEEETransactionson Zhang,K.;Schölkopf,B.;Spirtes,P.;andGlymour,C.2018.
NeuralNetworksandLearningSystems. Learningcausalityandcausality-relatedlearning:somere-
centprogress. Nationalsciencereview,5(1):26–29.
Chickering, D. M. 2002. Optimal Structure Identification
withGreedySearch. Journalofmachinelearningresearch,
3(Nov):507–554.
Choi, J.; Chapkin, R.; and Ni, Y. 2020. Bayesian causal
structurallearningwithzero-inflatedpoissonbayesiannet-
works. Advancesinneuralinformationprocessingsystems,
33:5887–5897.
Cormen,T.H.;Leiserson,C.E.;Rivest,R.L.;andStein,C.
2022. Introductiontoalgorithms. MITpress.
Efron,B.;andTibshirani,R.J.1994. Anintroductiontothe
bootstrap. CRCpress.
Glymour, C.; Zhang, K.; and Spirtes, P. 2019. Review of
causaldiscoverymethodsbasedongraphicalmodels. Fron-
tiersingenetics,10:524.
McKenzie,E.1985. Somesimplemodelsfordiscretevari-
ate time series 1. JAWRA Journal of the American Water
ResourcesAssociation,21(4):645–650.
Ni,Y.;andMallick,B.2022. Ordinalcausaldiscovery. In
UncertaintyinArtificialIntelligence,1530–1540.PMLR.
Park,G.;andRaskutti,G.2015.Learninglarge-scalepoisson
dagmodelsbasedonoverdispersionscoring. Advancesin
neuralinformationprocessingsystems,28.
Park,G.;andRaskutti,G.2017.LearningQuadraticVariance
Function(QVF)DAGModelsviaOverDispersionScoring
(ODS). Journal of Machine Learning Research, 18(224):
1–44.
Pearl,J.2009. Causality. Cambridgeuniversitypress.
Qiao, J.; Cai, R.; Wu, S.; Xiang, Y.; Zhang, K.; and Hao,
Z.2023. StructuralHawkesProcessesforLearningCausal
StructurefromDiscrete-TimeEventSequences. InProceed-
ingsoftheThirty-SecondInternationalJointConferenceon
ArtificialIntelligence,IJCAI-23,5702–5710.
Spirtes,P.;Glymour,C.N.;andScheines,R.2000.Causation,
prediction,andsearch. MITpress.SupplementaryMaterialof“CausalDiscoveryfromPoissonBranchingStructuralCausalModel
UsingHigh-OrderCumulantwithPathAnalysis”
A ProofofTheorem1
Theorem1(Reducibility). GivenaPoissonrandomvariableϵandndistinctsequencesofcoefficientsA ,...,A ,wehave
1 n
κ(A ◦ϵ,...,A ◦ϵ,...,A ◦ϵ,...,A ◦ϵ)=κ(A ◦ϵ,...,A ◦ϵ)
1 1 n n 1 n
(cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) (A.1)
k1times kntimes
whereeachA ◦ϵrepeatsk timesintheoriginalcumulantandonlycontainsonetimeinthereducedcumulant.
i i
OutlineofProof
Firstofall,weintroducethemoment-generatingfunction(MGF)andcumulant-generatingfunction(CGF).
Definition6(Moment-generatingfunction). ForX =[X ,...,X ]T,ann-dimensionalrandomvector,themoment-generating
1 n
functionofX isgivenby:
M
(t)∶=E[etTX ]=E[et1X1+t2X2+⋯+tnXn]
(A.2)
X
wheret=[t ,...,t ]isafixedvector.
1 n
Definition 7 (Cumulant-generating function). For X = [X ,...,X ]T, an n-dimensional random vector, the cumulant-
1 n
generatingfunctionofX isgivenby:
K (t)=lnM (t) (A.3)
X X
whereM (t)isthemoment-generatingfunctionofX.
X
WithCGF,wecancalculatethejointcumulantofagivenrandomvectorX bytakingdeviateofCGF:
κ(X 1,X 2,...,X n)=
∂nK
∂tX
1∂(t
t1
2, ⋯t
2
∂.. t. n,t n)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)t1=0,...,tn=0
(A.4)
Furthermore,ifeachX intherandomvectorX repeatedk times,thenweonlyneedtotakek timesofderivativesoftheCGF
i i i
withrespecttothecorrespondingt ,i.e.,
i
κ(X (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)1 (cid:210) k(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210), 1(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210). (cid:210)(cid:210) t(cid:209). im. (cid:210)(cid:210)(cid:210), (cid:210)(cid:210)(cid:210)(cid:210) e(cid:210)(cid:210)(cid:210)X (cid:210)(cid:210) s(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)1 (cid:210)(cid:210)(cid:210)(cid:207), (cid:205)X (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)2 (cid:210) k(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210), 2(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210). (cid:210)(cid:210) t(cid:209). im. (cid:210)(cid:210)(cid:210), (cid:210)(cid:210)(cid:210)(cid:210) e(cid:210)(cid:210)(cid:210)X (cid:210)(cid:210) s(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)2 (cid:210)(cid:210)(cid:210)(cid:207),...,X (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)n (cid:210)(cid:210) k(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) n, (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210). (cid:210)(cid:210)(cid:209) t. i. m(cid:210)(cid:210)(cid:210), (cid:210)(cid:210)(cid:210)(cid:210)(cid:210) e(cid:210)(cid:210)X (cid:210)(cid:210)(cid:210) s(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)n (cid:210)(cid:210)(cid:210)(cid:210)(cid:207))=
∂k1+k2+ ∂⋯
tk
1+ 1k ∂n tK
k 22X
⋯( ∂t
1
t,
k
nt n2...,t n)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)t1=0,...,tn=0
(A.5)
Therefore,Theorem1isequivalenttoshowthefollowingequalityhold:
∂nK
∂tX
1∂(t
t1
2, ⋯t
2
∂.. t. n,t n)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)t1=0,...,tn=0
=
∂k1+k2+ ∂⋯
tk
1+ 1k ∂n tK
k
22X
⋯( ∂t
1
t,
k
nt n2...,t n)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)t1=0,...,tn=0.
(A.6)
Todoshow,wewillprovethatthe
∂nKX(t1,t2...,tn)
hastheformofexponentialfunction,i.e.
∂t1∂t2⋯∂tn
∂nK X(t 1,t 2...,t n) =βet1+t2+⋯tn,
(A.7)
∂t ∂t ⋯∂t
1 2 n
whichisafunctionthatremainsunchangedwhentakingderivativeswithrespecttoanyt andthustheEq.(A.6)holds.
i
Followingthisoutline,considerarandomvectorR=[A ◦ϵ,A ◦ϵ,...,A ◦ϵ]T,A ≠A ,whereϵrepresentsthePoisson
1 2 n i j
noisecomponentofavertexX ingraphG,andA isasequenceofpathcoefficientscorrespondingtoadirectpathfromX to
i
oneofitsdescendantvertices.ThenaccordingtothedefinitionofMGF,wehave:
M
(t)=E[etTR ]=E[et1×A1◦ϵ+⋯+tn×An◦ϵ
] (A.8)
R
wheret=[t ,t ,...,t ]T isafixedvector.
1 2 n
Followingtheoutline,wefirstprovideanintuitionofproofthroughaspecificcasethateachvertexareconditionalindependce
bytherootvertexϵ.ProofofSpecificCase
GivenaPoissonrandomvariableϵ ∼ Pois(µ)andndistinctsequencesofcoefficientsA ,...,A ,inwhichA = (a(i) )∣Ai∣
1 n i k k=1
wherea(i)isthek-thcoefficientsofA
.
k i
Assumethereexistnok =1,2,...min(∣A i∣,∣A j∣)betweenanytwoA iandA
j
suchthat(a( li) )k
l=1
=(a( lj) )k l=1,whichmeans
thatthereexistnotwopathsP andP sharingthesamepartfromthesourcepoint.
i j
WeconsidertherandomvectorR=(A ◦ϵ,A ◦ϵ,...,A ◦ϵ),whereeachrandomvariableA ◦X appearsuniquely.The
1 2 n i
momentgeneratingfunction(MGF)ofRis:
M
(t)=E[et1×A1◦ϵ+⋯+tn×An◦ϵ
]. (A.9)
R
Accordingtothelawoftotalexpectation,wehave:
n
M
R(t)=E[E[et1×A1◦ϵ+⋯+tn×An◦ϵ ∣ϵ]]=E[∏E[eti×Ai◦ϵ
∣ϵ]], (A.10)
i=1
sinceA ◦ϵ∣ϵ⊥⊥A ◦ϵ∣ϵforalli≠j.
i j
Next,accordingtothepropertyofthinningoperation,wehaveA i◦ϵ∣ϵ=d Binorm(n=ϵ,p=∏∣ jA =1i∣a( ji) ),where‘=d ’means
distributionequalityandBinorm(n,p)isthebinomialdistribution,thentheE[exp(t i×A
i◦ϵ)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)ϵ]istheMGFofabinomial
variableA ◦ϵ∣ϵ,wehave:
i
ϵ
E[exp(t i×A
i◦ϵ)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)ϵ]=⎛
⎜
⎝1−∏∣ jA =i 1∣
a(
ji)+∏∣ jA =i 1∣
a(
ji)eti⎞
⎟
⎠
. (A.11)
Substitutingequation(A.11)intoequation(A.10),wehave:
M
R(t)=E⎡ ⎢⎢⎢⎢⎢⎢⎢
⎣∏
i=n
1
⎛
⎜
⎝1−∏∣ jA =i 1∣
a(
ji)+∏∣ jA =i 1∣
a(
ji)eti⎞
⎟
⎠ϵ⎤ ⎥⎥⎥⎥⎥⎥⎥
⎦=
k+
∑
=∞ 0⎡ ⎢⎢⎢⎢⎢⎢⎢⎢
⎣P(ϵ=k)∏
i=n
1
⎛
⎜
⎝1−∏∣ jA =i 1∣
a(
ji)+∏∣ jA =i 1∣
a(
ji)eti⎞
⎟
⎠k⎤ ⎥⎥⎥⎥⎥⎥⎥⎥
⎦
=
k+
∑
=∞ 0⎡ ⎢⎢⎢⎢⎢⎢⎢⎢ ⎣µk ke !−µ
∏
i=n
1
⎛
⎜
⎝1−∏∣ jA =i 1∣
a(
ji)+∏∣ jA =i 1∣
a(
ji)eti⎞
⎟
⎠k⎤ ⎥⎥⎥⎥⎥⎥⎥⎥
⎦
(A.12)
+∞ [µ∏n i=1(1−∏∣ jA =1i∣a( ji)+∏∣ jA =1i∣a( ji)eti)]k
=exp(−µ)∑ .
k!
k=0
Accordingtothepowerseriesexpansionfortheexponentialfunction,i.e.expx=∑+∞ xn
,wehave:
x=0 n!
M
R(t)=exp(−µ)exp⎡ ⎢⎢⎢⎢⎢⎢⎢
⎣µ∏
i=n
1
⎛
⎜
⎝1−∏∣ jA =i 1∣
a(
ji)+∏∣ jA =i 1∣
a(
ji)eti⎞
⎟
⎠⎤ ⎥⎥⎥⎥⎥⎥⎥
⎦, (A.13)
thenthecumulant-generatingfunction(CGF)ofRisgivenby:
n ⎛ ∣Ai∣ ∣Ai∣ ⎞
K (t)=logM (t)=µ∏ ⎜1−∏a(i)+∏a(i)eti⎟−µ, (A.14)
R R j j
i=1 ⎝ j=1 j=1 ⎠
Weobtainthecumulantbythepartialderivativesofthecumulantgeneratingfunction:
∂nK R(t) =µ∏n ∏∣Ai∣
a(i)eti.
(A.15)
∂t ∂t ⋯∂t j
1 2 n i=1 j=1
Since
∂nKR(t)
hastheformoftheexponentialfunction,furtherpartialderivativesofitwillalsoretainthesameform:
∂t1∂t2⋯∂tn
∂k1+k2+⋯knK R(t)
=
∂nK R(t) =µ∏n ∏∣Ai∣
a(i)eti. (A.16)
∂tk 11∂tk 22⋯∂tk nn ∂t 1∂t 2⋯∂t n i=1 j=1 jTherefore,wehave:
κ(A 1◦ϵ,A 2◦ϵ,...,A n◦ϵ)= ∂t∂ 1n ∂K
t
2R ⋯(t ∂)
t
n(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)t1=0,...,tn=0=µ∏ i=n
1
∏∣ jA =i 1∣ a( ji),
(A.17)
κ(A
(cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)1
(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)◦ (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)ϵ
(cid:210)(cid:210)
k(cid:210)(cid:210)(cid:210)(cid:210)(cid:210),
(cid:210)(cid:210)
1(cid:210)(cid:210)(cid:210)(cid:210)(cid:210).
(cid:210)(cid:210) t(cid:209)
i. m. (cid:210)(cid:210)(cid:210),
(cid:210)(cid:210)(cid:210)
e(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)A
s(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)1
(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)◦ (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)ϵ (cid:210)(cid:207),...,A
(cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)n
(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)◦ (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)ϵ
(cid:210)
k(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210),
(cid:210)
n(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210).
(cid:210)(cid:210)
t(cid:209). im. (cid:210)(cid:210)(cid:210),
(cid:210)(cid:210)(cid:210)(cid:210)
e(cid:210)(cid:210)(cid:210)A
(cid:210)(cid:210) s(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)n
(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)◦ (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)ϵ (cid:210)(cid:207))= ∂k ∂1 t+
k
1k 12 ∂+ t⋯
k
22k ⋯nK ∂R
tk
n( nt)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)t1=0,...,tn=0=µ∏ i=n
1
∏∣ jA =i 1∣ a( ji),
whichfinishestheproof:
κ(A ◦ϵ,...,A ◦ϵ,...,A ◦ϵ,...,A ◦ϵ)=κ(A ◦ϵ,A ◦ϵ,...,A ◦ϵ)
1 1 n n 1 2 n
(cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) (A.18)
k1times kntimes
TheaboveproofofspecificcasehighlightsthatthewaytocalculatetheMGFinvolvesdecomposingtheexpectationE[etTR
]
usingthelawoftotalexpectationtoestablishconditionalindependence.Inthespecificcase,theconditionalindependencecanbe
simplyestablishedbyconditionontheϵsincethereisnocommonsub-sequencebetweenanyA andA .
i j
However,givenϵisnotenoughtobuildtheconditionalindependenceifA andA sharethesamesub-sequence,i.e.there
i j
existak suchthat(A i) 1∶k =(A j) 1∶k.
Before going into the formal proof, we provide an example to illustrate why simply conditioning on ϵ cannot establish
conditional independence. Subsequently, we further show how to establish conditional independence in the presence of a
commonsub-sequence.
AnExamplewhenCommonsub-sequenceExists
Considerrandomvariables:
A ◦ϵ=b◦a◦ϵ,A ◦ϵ=c◦a◦ϵandA ◦ϵ=c◦d◦ϵ,
1 2 3
whereA =(a,b), A =(a,c), A =(d,c)andA andA hasthecommonsub-sequence(a).Thegeneratingprocesscanbe
1 2 3 1 2
representedthroughatreestructure,asshowninFig.1.
𝜖
𝑎 𝑑
𝑎∘𝜖 𝑑∘𝜖
𝑏 𝑐 𝑐
𝑏∘𝑎∘𝜖 𝑐∘𝑎∘𝜖 𝑐∘𝑑∘𝜖
Figure1:GeneratingprocessofA ◦ϵ,A ◦ϵ, A ◦ϵ,eachleafnodecorrespondstoanoriginalrandomvariable.
1 2 3
Now, if we condition on ϵ, we obtain conditional independence A ◦ϵ∣ϵ⊥⊥A ◦ϵ∣ϵ and A ◦ϵ∣ϵ⊥⊥A ◦ϵ∣ϵ, how-
1 3 2 3
ever, A ◦ϵ∣ϵ⊥/⊥A ◦ϵ∣ϵ. This is because both A ◦ϵ∣ϵ and A ◦ϵ∣ϵ are dependent on the binomial random variable
1 2 1 2
a◦ϵ∣ϵ=d B(n=ϵ,p=α)generatedbythecommonsub-sequence(α).
Suchdependenceoccursduetoperformingthethinningoperation◦onarandomvariable,resultinginthecreationofanew
randomvariable,distinctfromthestraightforwardlinearoperationsinvolvingmerecoefficientmultiplication.
Therefore,tobuildconditionalindependencebetweenA ◦ϵ∣ϵandA ◦ϵ∣ϵ,weneedtofurtherconditionona◦ϵ∣ϵ.Sucha
1 2
processofestablishingconditionalindependencestepbystepcanberepresentedthroughthetreestructureinFig.1,asshownin
Fig.2.
Specifically,theMGFofR=[A ◦ϵ,A ◦ϵ,A ◦ϵ]T isgivenby
1 2 3
M (t ,t ,t
)=E[et1×A1◦ϵ et2×A2◦ϵ et3×A3◦ϵ ]=E[et1×b◦a◦ϵ et2×c◦a◦ϵ et3×c◦d◦ϵ
]
R 1 2 3
=E
[E[et1×b◦a◦ϵ et2×c◦a◦ϵ et3×c◦d◦ϵ
∣ϵ]] (A.19)
ϵ
=E
[E[et1×b◦a◦ϵ et2×c◦a◦ϵ ∣ϵ]E[et3×c◦d◦ϵ
∣ϵ]],
ϵ
whereE[et1b◦a◦ϵet2c◦a◦ϵ ∣ϵ]andE[et3c◦d◦ϵ
∣ϵ]correspondtotheblueboxandthegreenboxinFig.2,respectively.
The next step is to establish conditional independence and separate
E[et1b◦a◦ϵet2c◦a◦ϵ
∣ϵ]. By applying the law of total
expectationtoit,weobtain
E[et1b◦a◦ϵ et2c◦a◦ϵ ∣ϵ]=E[E[et1b◦a◦ϵ ∣a◦ϵ]E[et2c◦a◦ϵ ∣a◦ϵ]∣ϵ], (A.20)𝜖 𝜖 𝜖
𝑎 𝑑 Condition 𝑎 𝑑 Condition 𝑎 𝑑
on 𝜖 on 𝑎∘𝜖|𝜖
𝑎∘𝜖 𝑑∘𝜖 𝑎∘𝜖|𝜖 𝑑∘𝜖|𝜖 𝑎∘𝜖|𝜖 𝑑∘𝜖|𝜖
𝑏 𝑐 𝑐 𝑏 𝑐 𝑐 𝑏 𝑐
𝑐
𝑏∘𝑎∘𝜖 𝑐∘𝑎∘𝜖 𝑐∘𝑑∘𝜖 𝑏∘𝑎∘𝜖|𝜖 𝑐∘𝑑∘𝜖|𝜖 𝑏∘𝑎∘𝜖|𝑎∘𝜖 𝑐∘𝑑∘𝜖|𝜖
𝑐∘𝑎∘𝜖|𝜖
𝑐∘𝑎∘𝜖|𝑎∘𝜖
Figure2:Obtainconditionalindependenceaccordingtothehierarchicalstructureofthetree
whichiscalculablesinceE[et1b◦a◦ϵ ∣a◦ϵ]istheMGFofBinorm(n=a◦ϵ,p=b)andthesameforE[et2c◦a◦ϵ ∣a◦ϵ].
Motivatedbytheaboveexample,whenconditioningonavertexinatree,conditionalindependenceisestablishedamongthe
randomvariablescorrespondingtoeachsubtreeofthatvertex(ifthesubtreeexists),enablingtheseparationofexpectations.
Therefore, one can calculate the MGF by conditioning the random variables layer by layer according to the hierarchical
structureofthetreeinthegeneratingprocess.
Here,toformalizethecomputationoftheMGF,weintroducethefollowingdefinitionofatreetomodelthegeneratingprocess
oftherandomvectorR=(A ◦ϵ,A ◦ϵ,...,A ◦ϵ).
1 2 n
Definition 8 (Tree representation of the generating process of random vector in PB-SCM). For a given random vector
R=[A ◦ϵ,A ◦ϵ,...,A ◦ϵ]T,∀ A ≠A ,thegeneratingprocessofeachrandomvariableinRcanbesummarizedby
1 2 n i,j i j
atreeT R.Let{T 0,T 1,T 2,...}denotealltheverticesofT R,whereT
0
=ϵistherootvertexofthetreeandT
j
=α
i→j
◦T i.Let
L={L ,L ,...,L }withindexi=1,2,...,ndenotetheleafverticesinthetree,suchthatL =A ◦ϵ
1 2 n i i
Moreover,letAj
i
denotesthesub-sequenceofA ithatstartfromT j.Forexample,A
i
={α 0→1,α 1→2,α 2→3},thenA0
i
=A
i
andA1
i
={α 1→2,α 2→3}.LetL(T i)={k∣T
k
isleaf∧k ∈L(T i)}denotesthesetofleafvertexinthetree.
TherecursiverelationofMGF
Based on this definition, several lemmas are introduced to establish the recursive relation for the MGF of
R=[A ◦ϵ,A ◦ϵ,...,A ◦ϵ]T.
1 2 n
Lemma1(StartfromrootvertexT ). ForagivenrandomvectorR=[A ◦ϵ,A ◦ϵ,...,A ◦ϵ]T anditstreerepresentation
0 1 2 n
T ,A ≠A ,theMGFofRsatisfy
R i j
⎡ ⎤
M R(t)=E[e∑n i=1ti×Ai◦ϵ ]=E T0⎢⎢⎢⎢⎢⎢⎢ ∏ E[e∑ i∈L(Tj)ti×Aj i◦Tj∣T 0]⎥⎥⎥⎥⎥⎥⎥ (A.21)
⎣j∈Ch(T0) ⎦
Proof. TheresultisstraightforwardsinceT thetherootofthetree,thengiventheconditionofT eachchildofT willbe
0 0 0
conditionalindependence:
M
(t)=E[e∑n i=1ti×Ai◦ϵ
]
R
=E[e∑n i=1ti×Ai◦T0]
⎡ ⎤
=E⎢⎢⎢⎢⎢⎢⎢ ∏ E[e∑ i∈L(Tj)ti×Aj i◦α0→j◦T0∣T 0]⎥⎥⎥⎥⎥⎥⎥ (A.22)
⎣j∈Ch(T0) ⎦
⎡ ⎤
=E⎢⎢⎢⎢⎢⎢⎢ ∏ E[e∑ i∈L(Tj)ti×Aj i◦Tj∣T 0]⎥⎥⎥⎥⎥⎥⎥
⎣j∈Ch(T0) ⎦
ByLemma1,MGFcanbedecomposedintoseparatedconditionalexpectationinthefirstlevelofthetree.Next,wewill
investigatehowsuchconditionalexpectationcanbefurtherdecomposed.
Lemma2(FromvertexT toT ). ForagivenrandomvectorR=[A ◦ϵ,A ◦ϵ,...,A ◦ϵ]T anditstreerepresentation
j k 1 2 n
T .LetT beanodeinalevelthatdecomposedtheconditionalexpectationintotheproductofitschild.Then,oneofsuch
R jdecomposedexpectationofitschildT ,canbefurtherdecomposedifT isnotleaf,
k k
⎡ ⎤
E[e∑ i∈L(Tk)ti×Ak i◦Tk∣T j]=E⎢⎢⎢⎢⎢⎢⎢ ∏ E[e∑ i∈L(Tl)ti×Al i◦Tl∣T k]∣T j⎥⎥⎥⎥⎥⎥⎥ (A.23)
⎣l∈Ch(k) ⎦
andifT isleaf,
k
E[e∑ i∈L(Tk)ti×Ak i◦Tk∣T j]=[M B(αj→k)(t L(Tk))]Tj. (A.24)
Proof. IfT isnotleaf,wecanseparatetheexpectationaccordingitschild:
k
⎡ ⎤
E[e∑ i∈L(Tk)ti×Ak i◦Tk∣T j]=E[e∑ l∈Ch(Tk)∑ i∈L(Tl)ti×Al i◦αk→l◦Tk∣T j]=E⎢⎢⎢⎢⎢⎢⎢ ∏ e∑ i∈L(Tl)ti×Al i◦αk→l◦Tk∣T j⎥⎥⎥⎥⎥⎥⎥. (A.25)
⎣l∈Ch(k) ⎦
Then,accordingtothelawoftotalexpectation,wehave
⎡ ⎤ ⎡ ⎤
E⎢⎢⎢⎢⎢⎢⎢ ∏ e∑ i∈L(Tl)ti×Al i◦αk→l◦Tk∣T j⎥⎥⎥⎥⎥⎥⎥=E⎢⎢⎢⎢⎢⎢⎢ ∏ E[e∑ i∈L(Tl)ti×Al i◦αk→l◦Tk∣T k]∣T j⎥⎥⎥⎥⎥⎥⎥
⎣l∈Ch(k) ⎦ ⎣l∈Ch(k) ⎦
(A.26)
⎡ ⎤
=E⎢⎢⎢⎢⎢⎢⎢ ∏ E[e∑ i∈L(Tl)ti×Al i◦Tl∣T k]∣T j⎥⎥⎥⎥⎥⎥⎥
⎣l∈Ch(k) ⎦
IfT isleaf,whichmeansi=L(T )istheexactlyindexoftheleafvertexandAk isempty,andthenwehave:
k k i
E[e∑ i∈L(Tk)ti×Ak i◦Tk∣T j]=E[etL(Tk)×Tk∣T j]=E[etL(Tk)×αj→k◦Tj∣T
j]. (A.27)
Accordingtothedefinitionofthinoperator,wehaveα j→k◦T
j
=∑T l=j 1ξ l(αj→k) withξ l(αj→k) i.i∼.d. B(α j→k),whereB(α j→k)is
Bernoullidistributionwithparameterα .Thus,
j→k
E[etL(Tk)×αj→k◦Tj∣T ]=E[etL(Tk)×∑T l=j 1ξ l(αj→k)
∣T
]=E[∏Tj etL(Tk)×ξ l(αj→k)
∣T
]=∏Tj E[etL(Tk)×ξ l(αj→k) ]=E[etL(Tk)×ξ l(αj→k) ]Tj
.
j j l=1 j l=1
(A.28)
NotethatE[etL(Tk)×ξ l(αj→k) ]istheMGFofξ(αj→k)
.Intheend,weobtain:
l
E[etL(Tk)×ξ l(αj→k) ]Tj =[M (t )]Tj. (A.29)
B(αj→k) L(Tk)
Torepresenttherecursiverelation,wenowintroducetheprobability-generatingfunction(PGF):
Definition 9 (Probability-generating function). For X = [X ,...,X ]T, where each X is adiscrete random variable, the
1 n i
probability-generatingfunctionofX isgivenby:G
(z)∶=E[zX1zX2⋯zXn],wherez=[z
,...,z ].
X 1 2 n 1 n
Then,followinglemmadisclosetherecursiverelationofMGF.
Lemma 3. For a given random vector R = [A ◦ϵ,A ◦ϵ,...,A ◦ϵ]T and its tree representation T . Let M (t) ∶=
1 2 n R j,k
E[e∑ i∈L(Tk)ti×Ak i◦Tk∣T j] and M˜ j,k(t L(Tk)) = [M j,k(t L(Tk))]T1 j, where t
L(Tk)
= {t i∣i ∈ L(T k)}. The joint MGF can be
expressedasfollows:
⎛ ⎞
M (t)=G ⎜ ∏ M˜ (t )⎟, (A.30)
R T0⎝ 0,j L(Tj)
⎠
j∈Ch(T0)
where
M˜ j,k(t L(Tk))={G MB(αj→k)( (t∏ l∈Ch )(k)M˜ k,l(t L(Tk))) i of thT ek rwis in seotleafvertex . (A.31)
B(αj→k) L(Tk)Proof. First,byLemma2,wehavethefollowingrecursiveformula
E[∏ M (t )∣T ] ifT isnotleafvertex
M j,k(t L(Tk))={
[M
l∈Ch( (k t) )]Tk j,l L(Tl) j othek
rwise
(A.32)
B(αj→k) i
andthus
⎡ ⎤
M R(t)=E
T0⎢⎢⎢⎢⎢⎢⎢
∏ M 0,j(t
L(Tj))⎥⎥⎥⎥⎥⎥⎥.
(A.33)
⎣j∈Ch(T0) ⎦
Then,sinceM˜ j,k(t L(Tk))=[M j,k(t L(Tk))]T1 j,basedontherecursiveformulainEq.(A.32),wehave
M˜ j,k(t L(Tk))=M j,k(t L(Tk))T1 j
1
={(E[∏ l∈Ch(k)M˜ k,l(t L(Tl))Tk∣T j])Tj ifT
k
isnotleafvertex
M (t ) otherwise (A.34)
B(αj→k) i
=⎧ ⎪⎪⎪ ⎨(E[(∏ l∈Ch(k)M˜ k,l(t L(Tl)))αj→k◦Tj ∣T j])T1 j ifT
k
isnotleafvertex
⎪⎪⎪M
(t ) otherwise
⎩ B(αj→k) L(Tk)
ConsidertheexpectationinEq.(A.34)whenT isnotleafvertex.SinceT isconditionedsuchthatα ◦T followsthe
k j j→k j
distributionBinorm(n=T j,p=α j→k),thenbytheprobabilitygeneratingfunctionofBinomialdistribution,wehave
E⎡
⎢⎢⎢⎢⎢⎢⎢⎢ ⎣⎛
⎜
⎝
l∈∏ Ch(k)M˜ k,l(t
L(Tl))⎞
⎟
⎠αj→k◦Tj
∣T
j⎤
⎥⎥⎥⎥⎥⎥⎥⎥
⎦=G
B(αj→k)⎛
⎜
⎝
l∈∏ Ch(k)M˜ k,l(t
L(Tl))⎞
⎟
⎠Tj
. (A.35)
whereG (⋅ )istheprobabilitygeneratingfunctionofBernoullidistributionaccordingtotherelationbetweenBernoulli
B(αj→k)
andBinomialdistribution.SubstitutingEq.(A.35)intoEq.(A.34)wehave
M˜ j,k(t L(Tk))={G MB(αj→k)( (t∏ )l∈Ch(k)M˜ k,l(t L(Tl))) oif thT ek rwis isn eotleafvertex . (A.36)
B(αj→k) i
AsforthejointMGF,similarly,wehave
M R(t)=E
T0⎡ ⎢⎢⎢⎢⎢⎢⎢
⎣j∈C∏ h(T0)M 0,j(t
L(Tj))⎤ ⎥⎥⎥⎥⎥⎥⎥
⎦=E
T0⎡ ⎢⎢⎢⎢⎢⎢⎢⎢ ⎣⎛
⎜
⎝
j∈C∏ h(T0)M˜ 0,j(t
L(Tj))⎞
⎟
⎠T0⎤ ⎥⎥⎥⎥⎥⎥⎥⎥
⎦=G
T0⎛
⎜
⎝
j∈C∏ h(T0)M˜ 0,j(t
L(Tj))⎞
⎟ ⎠, (A.37)
whichfinishestheproof.
Afterderivingtherecursiverelation,wenowstepintotheformalproofofTheorem1followingtheproofoutline.
FormalProofofTheorem1
AccordingtotheLemma3,foragivenrandomvectorR=[A ◦ϵ,A ◦ϵ,...,A ◦ϵ]T anditstreerepresentationT ,the
1 2 n R
MGFofitisM (t)=G (∏ M˜ (t )),whereT =ϵ∼Pois(µ)isaPoissonrandomvariable.Thecumulant
R T0 j∈Ch(T0) 0,j L(Tj) 0
generatingfunctionofRisgivenby:
K (t)=logM (t)=logexp[µ(∏ M˜ (t )−1)]=µ(∏ M˜ (t )−1). (A.38)
R R j∈Ch(T0) 0,j L(Tj) j∈Ch(T0) 0,j L(Tj)
OurgoalistoshowthederivativeofK (t)isoftheexponentialform:
R
∂nK R(t) =βet1+t2+⋯+tn.
(A.39)
∂t ∂t ⋯∂t
1 1 n
Westartwith
∂nK (t) ∂n ⎛ ⎞ ∂n
R = µ⎜ ∏ M˜ (t )−1⎟=µ ∏ M˜ (t ), (A.40)
∂t 1∂t 1⋯∂t
n
∂t 1∂t 1⋯∂t
n ⎝ j∈Ch(T0)
0,j L(Tj)
⎠
∂t 1∂t 1⋯∂t
n j∈Ch(T0)
0,j L(Tj)whereM˜ (t )isafunctioninvolvingonly{t ∣i∈L(T )},wethenhave:
0,j L(Tj) i j
∂n ∂∣L(Tj)∣
µ ∏ M˜ (t )=µ∏ M˜ (t ). (A.41)
∂t 1∂t 1⋯∂t
n j∈Ch(T0)
0,j L(Tj) j∈Ch(T0) ∏ i∈L(Tj)∂t
i
0,j L(Tj)
Wethenintroducetherecursiverepresentationof ∂∣L(Tj)∣ M˜ (t ).
∏ i∈L(Tj)∂ti 0,j L(Tj)
Lemma4. ThehigherorderpartialderivativeofM˜ (t )canbegivenby:
j,k L(Tk)
∂∣L( ∏Tk) i∣ ∈M L˜ (j T,k k)(t ∂L t( iTk)) =⎧ ⎪⎪⎪ ⎨
⎪⎪⎪
⎩α αj j→ →k ke∏ tL(l T∈ kC )h ,(k) ∂∣L ∏(Tl t) i∣ ∈M˜ Lk (T,l l( )t ∂L t( iTl)), oif thT ek rwis in seo .tleafvertex, (A.42)
Proof. WhenT isnotaleafvertex,accordingtotheLemma3,wehave:
k
∂∣L(Tk)∣M˜ (t ) ∂∣L(Tk)∣ ⎛ ⎞
j,k L(Tk) = G ⎜ ∏ M˜ (t )⎟
∏ i∈L(Tk)∂t i ∏ i∈L(Tk)∂t i B(αj→k) ⎝ l∈Ch(k) k,l L(Tl) ⎠
∂∣L(Tk)∣ ⎛ ⎞
= ∏ i∈L(Tk)∂t i ⎜ ⎝1−α j→k+α j→k l∈∏ Ch(k)M˜ k,l(t L(Tl))⎟ ⎠ (A.43)
∂∣L(Tk)∣
=α
j→k∏
∂t
∏ M˜ k,l(t L(Tl)).
i∈L(Tk) i l∈Ch(k)
SinceM˜ (t )isafunctioninvolvingonly{t ∣i∈L(T )},wehave:
k,l L(Tl) i l
∂∣L(Tk)∣ ∂∣L(Tl)∣M˜ (t )
α
j→k∏ ∂t
∏ M˜ k,l(t L(Tl))=α
j→k
∏
∏
k,l ∂L t(Tl) . (A.44)
i∈L(Tk) i l∈Ch(k) l∈Ch(k) i∈L(Tk) i
Otherwise,whenT isaleafvertex,wehave:
k
∂∣L( ∏Tk)∣M˜ j,k(t ∂L t(Tk)) = ∂M B( ∂αj t→k)(t L(Tk)) = ∂(1−α j→ ∂k t+α j→ketL(Tk)) =α j→ketL(Tk), (A.45)
i∈L(Tk) i L(Tk) L(Tk)
whichfinishestheproof.
According to Lemma 4, as the recursion terminates with an exponential function upon reaching the leaf vertex, we can
deducethattheexpansionof ∂nKR(t) resultsintheproductofeti foralli∈[n],alongwithaseriesofcorrespondingpath
∂t1∂t1⋯∂tn
coefficients.Moreover,ourfocusdoesnotlieinthespecificformofthesecoefficients,andthuswedenotethecoefficientasβ.
Weconclude:
∂nK R(t) =βet1+t2+⋯+tn.
(A.46)
∂t ∂t ⋯∂t
1 1 n
Finally,weobtain:
κ(A 1◦ϵ,A 2◦ϵ,...,A n◦ϵ)=
∂t∂ 1n ∂K
t
2R
⋯(t ∂)
t
n(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)t1=0,...,tn=0=β,
κ(A
(cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)1
(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)◦ (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)ϵ
(cid:210)(cid:210)
k(cid:210)(cid:210)(cid:210)(cid:210)(cid:210),
(cid:210)(cid:210)
1(cid:210)(cid:210)(cid:210)(cid:210)(cid:210).
(cid:210)(cid:210) t(cid:209)
i. m. (cid:210)(cid:210)(cid:210),
(cid:210)(cid:210)(cid:210)
e(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)A
s(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)1
(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)◦ (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)ϵ (cid:210)(cid:207),...,A
(cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)n
(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)◦ (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)ϵ
(cid:210)
k(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210),
(cid:210)
n(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210).
(cid:210)(cid:210)
t(cid:209). im. (cid:210)(cid:210)(cid:210),
(cid:210)(cid:210)(cid:210)(cid:210)
e(cid:210)(cid:210)(cid:210)A
(cid:210)(cid:210) s(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)n
(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)◦ (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)ϵ (cid:210)(cid:207))=
∂k ∂1 t+
k
1k 12 ∂+ t⋯
k
22k ⋯nK
∂R
tk
n( nt)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)t1=0,...,tn=0=β, (A.47)
whichfinishestheproof:
κ(A ◦ϵ,...,A ◦ϵ,...,A ◦ϵ,...,A ◦ϵ)=κ(A ◦ϵ,A ◦ϵ,...,A ◦ϵ).
1 1 n n 1 2 n
(cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) (A.48)
k1times kntimesB ProofofRemark1
Remark1. Foranytwovariablescausalgraph,thecausaldirectionofPB-SCMisnotidentifiableandadistributedequivalent
reversedmodelexists.
Proof. WeprovebytheequalityofPGFsofbothdirections.GivenatwovariablescausalgraphX
−→α
Y,whereX =ϵ ,Y =
X
α◦X+ϵ ,ϵ ∼Pois(µ ),andwedenotethereversemodelbyY −→αˆ X,whereY =ϵˆ,X =αˆ◦Y +ϵˆ .Wenowshowthe
Y i i Y X
solutionofαˆ,ϵˆ andϵˆ.
X Y
Forthecausaldirection,thePGFisgivenby:
G (z ,s
)=E[zX zα◦X+ϵY]
X,Y 1 2 1 2
=E[zϵXzα◦ϵX]E[zϵY]
1 2 2
=G (z G (z ))G (z ) (B.1)
ϵX 1 B(α) 2 ϵY 2
=G (z (1−α+αz ))G (z )
ϵX 1 2 ϵY 2
=eµX(z1(1−α+αz2)−1)eµY(z2−1).
Forthereversedirection,thePGFisgivenby:
Gˆ (z ,z )=E[zαˆ◦Y+ϵˆXzY ]
X,Y 1 2 1 2
=E[zαˆ◦Y zY ]E[zϵˆX]
1 2 1
=G (z G (z ))G (z ) (B.2)
Y 2 B(αˆ) 1 ϵˆX 1
=G (z (1−αˆ+αˆz ))G (z )
Y 2 1 ϵˆX 1
=eE[Y](z2(1−αˆ+αˆz1)−1)eµˆx(z1−1).
Ifthesetwomodelsareequivalent,wehaveG (z ,z )=Gˆ (z ,z ),i.e.
X,Y 1 2 X,Y 1 2
µ (z (1−α+αz )−1)+µ (z −1)=E[Y](z (1−αˆ+αˆz )−1)+µˆ (z −1). (B.3)
X 1 2 Y 2 2 1 X 1
AsY isarootvertexinthereversemodel,wehaveϵ ∼Pois(E[Y])=Pois(αµ +µ ).Thenwehave:
Y X Y
µ (z (1−α+αz )−1)+µ (z −1)=(αµ +µ )(z (1−αˆ+αˆz )−1)+µˆ (z −1). (B.4)
X 1 2 Y 2 x y 2 1 X 1
Expandingtheexpressionandsimplifying,weobtain
αµ z z +µ (1−α)z +µ z −µ −µ
X 1 2 X 1 Y 2 X Y
(B.5)
=(αµ +µ )αˆz z +µˆ z + (αµ +µ )(1−αˆ)z − (αµ +µ )−µˆ
X Y 1 2 X 1 X Y 2 X Y X
Toensuretheequalityholds,weequatethecoefficients,resultinginfollowingsystemofequations:
αµ =(αµ +µ )αˆ
X X Y
µ (1−α)=µˆ
X X
(B.6)
µ =(αµ +µ )(1−αˆ)
Y X Y
µ +µ =(αµ +µ )+µˆ ,
X Y X Y X
wherethesolutionofitisαˆ =αµ /(αµ +µ ), µˆ =µ (1−α).Thiscompletestheproof.
X X Y X X
C ProofofTheorem2
Theorem2. Foranytwovertexiandj whereiisrootvertex,i.e.,vertexihasemptyparentset,the2Dsliceofjointcumulant
C(n)satisfies:
i,j
C i( ,n j) =∑n k=− 11 ∑ ( m 1mn 2− ⋯1 m k)Λi k↝j (1◦X i↝X j). (C.1)
m1+⋯ m+ lm >k 0=n−1
where( n−1 )= (n−1)! isthemultinomialcoefficients.
m1m2⋯mk m1!m2!⋯mk!Proof
Foranytwovertexiandj,whereiisrootvertex,letPi↝j ={Pi↝j ,Pi↝j ,...,Pi↝j
}bethesetofpathsfromvertexitoj
1 2 ∣Pi↝j∣
withthecorrespondingsetofsequencesofcoefficientsAi↝j ={Ai↝j ,Ai↝j ,...,Ai↝j }.AccordingtothedefinitionofC(n),
1 2 ∣Pi↝j∣ i,j
wehave:
⎛ ⎞ ⎛ ⎞
C i( ,n j) =κ⎜⎜X i,X j,...,X j⎟⎟=κ⎜⎜ϵ i,X j,...,X j⎟⎟ (C.2)
⎝ (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207)⎠ ⎝ (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207)⎠
n−1times n−1times
thenweexpandX accordingtothestructuralequationofX :
j j
C(n) =κ(ϵ ,Ai↝j ◦ϵ +⋯+Ai↝j ◦ϵ ,...,Ai↝j ◦ϵ +⋯+Ai↝j ◦ϵ ).
i,j i 1 i ∣Pi↝j∣ i 1 i ∣Pi↝j∣ i
(cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) (C.3)
∣Pi↝j∣times
(cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207)
n−1times
Byapplyingthemultilinearityofcumulant,weobtainthefollowingdecomposition:
∣Pi↝j∣ ∣Pi↝j∣
C(n) = ∑ ⋯ ∑ κ(ϵ ,Ai↝j ◦ϵ ,Ai↝j ◦ϵ ,...,Ai↝j ◦ϵ ), (C.4)
i,j i l1 i l2 i ln−1 i
l1=1 ln−1=1
whichyields∣Pi↝j ∣× (n−1)cumulanttermwhereeachtermcorrespondenttoadifferentcombinationofcoefficientAi↝j .
l
i↝j i↝j
TocharacterizethecombinationsofA withineachcumulantinEq.(C.3),wecanconceptualizethatchoosedifferentA
l l
inton−1boxfrom∣Pi↝j ∣numberofdifferentcoefficient,,i.e.,wecanselectaAi↝j fromAi↝j ={Ai↝j ,Ai↝j ,...,Ai↝j
}
l 1 2 ∣Pi↝j∣
foreachpositioninthedecomposedcumulant.
Toestablishtheconnectionbetweenthecumulantandthek-pathsummation,wefirstrecallthedefinitionofΛi↝j (1◦X ↝
k i
X ):
j
Λi↝j (1◦ϵ ↝X )= ∑ κ(1◦ϵ ,Ai↝j ◦ϵ ,...,Ai↝j ◦ϵ ),
k i j i l1 i lk i (C.5)
1≤l1<l2<...<lk≤∣Pi↝j∣
i↝j
whichisthesumofcumulantsandeachcumulantinvolvekdistinctA .
l
Notethatduetothereducibility,theC(n)canbereducedtoseveraldistinctcumulants.Inparticular,thek-pathsummation
i,j
containsallthedistinctcumulantsofC(n)whichinvolvekdistinctAi↝j
.Therefore,theconnectionbetweenEq.(C.5)andEq.
i,j l
i↝j
(C.4)canbeformulatedashowmanynumbersforeachdistinctA occursafterthereducing.
l
ForeachdistinctcumulantinΛi↝j (1◦ϵ ↝X ),thenumberofoccurrencesisthesamebecauseeachcumulantisconstructed
k i j
bykdifferentpathssharingthesamepropertyintermofnumber.
Thus, we only need to count the number of each distinct cumulant for some specific k paths. Without loss of generality,
considerthecumulantwithk pathinformation:κ(1◦ϵ ,Ai↝j ◦ϵ ,...,Ai↝j ◦ϵ ).Sincebeforethereducingstep,thereare
i 1 i k i
n−1positionsforeachA,andthecountcanbeformulatedbycountingthenumberofwaystoplacekdistinguishableAinto
n−1indistinguishableboxeswithreplacementsuchthateachballmustappearatleastonce.Suchanumbercanbecalculated
by ∑ ( n−1 ),whichisthecoefficientofΛi↝j (1◦ϵ ↝X ).Bycombiningeachorderofk,wecanobtainthe
m1+⋯ m+ lm >k 0=n−1
m1m2⋯mk k i j
close-formsolutionofC(n)inEq.(C.3).Thiscompletestheproof.
i,j
D ProofofTheorem3andTheorem4
Theorem3(Identifiabilityforrootvertex). Foranyvertexiandj,whereiistherootvertexingraphG,ifC(3)−C(2) ≠0,then
i,j i,j
C(3)−C(2) =0andX istheancestorofX .
j,i j,i i j
Proof. Forthereversedirection,sinceX isarootvertex,wehave:
i
⎛∣Pi↝j∣
⎞
C j( ,2 i) =κ(X j,X i)=κ⎜⎜ ∑ Ai l↝j ◦ϵ i,ϵ i⎟⎟,
⎝ l=1 ⎠
(D.1)
⎛∣Pi↝j∣
⎞
C j( ,3 i) =κ(X j,X i,X i)=κ⎜⎜ ∑ Ai l↝j ◦ϵ i,ϵ i,ϵ i⎟⎟.
⎝ l=1 ⎠BasedonTheorem1,Eq.(D.1)canbereducedasfollow:
⎛∣Pi↝j∣
⎞
⎛∣Pi↝j∣
⎞
C j( ,3 i) =κ⎜⎜ ∑ Ai l↝j ◦ϵ i,ϵ i,ϵ i⎟⎟=κ⎜⎜ ∑ Ai l↝j ◦ϵ i,ϵ i⎟⎟=C j( ,2 i) (D.2)
⎝ l=1 ⎠ ⎝ l=1 ⎠
thusC(3)−C(2) =0.
j,i j,i
Forthecausaldirection,basedonTheorem2,wehave:
C(2) =Λi↝j (X ↝X )
i,j 1 i j
(D.3)
C(3) =Λi↝j (X ↝X )+2Λi↝j (X ↝X )
i,j 1 i j 2 i j
ThenwehaveC(3)−C(2) =2Λi↝j (X ↝X )≠0whichmeansthattherearemorethanonepathfromitoj,i.e.∣Pi↝j ∣≥2.
i,j i,j 2 i j
Therefore,X istheancestorofX .Thiscompletestheproof.
i j
Theorem4(GraphicalImplicationofIdentifiabilityforRootVertex). Forapairofverticesiandj ingraphG,ifvertexiisa
rootvertexandexistsatleasttwodirectedpathsfromitoj,i.e.,∣Pi↝j ∣≥2thenthecausalorderbetweeniandjisidentifiable.
Proof. Supposethecausalorderbetweeniandj cannotbeidentifiedbyTheorem3.Thentheremustbeinthefollowingcases:
(i)C(3)−C(2) =0;(ii)C(3)−C(2) ≠0.
i,j i,j j,i j,i
For(i),sinceΛi↝j (1◦ϵ ↝X )=C(3)−C(2) =0indicatingthatthereexistszerooronepathfromitoj whichcontradict
2 i j i,j i,j
tothefactthat∣Pi↝j ∣≥2.
For(ii),C(3)−C(2) ≠ 0iscontradictedtoTheorem3thatC(3)−C(2) = 0whenC(3)−C(2) ≠ 0.Bycombiningthesetwo
j,i j,i j,i j,i i,j i,j
cases,wecompletetheproof.
E ProofofTheorem5
Theorem5.
Foranytwovertexiandj,the2DsliceofjointcumulantC(n)satisfies:
i,j
C i( ,n j) =∑n k=− 11 ∑ ( m 1mn 2− ⋯1 m k)Λ˜ k(X i↝X j). (E.1)
m1+⋯ m+ lm >k 0=n−1
where( n−1 )= (n−1)! isthemultinomialcoefficients.
m1m2⋯mk m1!m2!⋯mk!
Proof. Sinceiisnotarootvertex,thestructuralequationofX isX =∑ ∑∣Pm↝i∣Am↝i◦ϵ +ϵ ,whereAm↝i is
i i m∈An(i) h=1 h m i h
thesequenceofcoefficientscorrespondingtotheh-thpathfromm,oneoftheancestorofi,toi.
AccordingthestructuralequationofX ,wehave:
i
⎛ ⎞ ⎛
∣Pm↝i∣
⎞
C i( ,n j) =κ⎜⎜X i,X j,...,X j⎟⎟=κ⎜⎜ ∑ ∑ Am h↝i◦ϵ m+ϵ i,X j,...,X j⎟⎟ (E.2)
⎝ (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) n(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) −(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) 1(cid:210)(cid:210)(cid:210)(cid:209) ti(cid:210) m(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) e(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) s(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207)⎠ ⎝m∈An(i) h=1 (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) n(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) −(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) 1(cid:210)(cid:210)(cid:210)(cid:209) ti(cid:210) m(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) e(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) s(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207)⎠
thenwedecomposeC(n)accordingtothestructuralequationofX
,wehave:
i,j i
∣Pm↝i∣
⎛ ⎞ ⎛ ⎞
C i( ,n j) = ∑ ∑ κ⎜⎜Am h↝i◦ϵ m,X j,...,X j⎟⎟+κ⎜⎜ϵ i,X j,...,X j⎟⎟
m∈An(i) h=1 ⎝ (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) n(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) −(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) 1(cid:210)(cid:210)(cid:210)(cid:209) ti(cid:210) m(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) e(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) s(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207)⎠ ⎝ (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) n(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) −(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) 1(cid:210)(cid:210)(cid:210)(cid:209) ti(cid:210) m(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) e(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) s(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207)⎠
(E.3)
∣Pm↝i∣
⎛ ⎞ ⎛ ⎞
= ∑ ∑ κ⎜⎜Am h↝i◦ϵ m,X j,...,X j⎟⎟+κ⎜⎜ϵ i,X j,...,X j⎟⎟
m∈An(i,j) h=1 ⎝ (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) n(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) −(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) 1(cid:210)(cid:210)(cid:210)(cid:209) ti(cid:210) m(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) e(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) s(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207)⎠ ⎝ (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) n(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) −(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) 1(cid:210)(cid:210)(cid:210)(cid:209) ti(cid:210) m(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) e(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210) s(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207)⎠
Asthosecumulantsinvolveindependentnoisecomponentsequaltozero,misthecommonancestorofiandj intheEq.(E.3).
Now,weconsiderthedecompositionofcumulant:(i)κ(Am↝i◦ϵ
,X ,...,X )and(ii)κ(ϵ ,X ,...,X ).For(i),the
h m j j i j j
termκ(Am↝i◦ϵ
,X ,...,X )hasthesimilarformasthecumulantweprovedinTheorem2,i.e.,
h m j j
∣Pm↝j∣ ∣Pm↝j∣
κ(Am↝i◦ϵ ,X ,...,X )= ∑ ⋯ ∑ κ(Am↝i◦ϵ ,Am↝j ◦ϵ ,Am↝j ◦ϵ ,...,Am↝j ◦ϵ ) (E.4)
h m j j h m l1 m l2 m ln−1 m
l1=1 ln−1=1wheretheonlydifferenceisthefirstnoisecomponent,whichisAm↝i◦ϵ
insteadofϵ .Thisvariationdoesnotimpactthe
h m m
resultinTheorem2leadingto
κ(Am h↝i◦ϵ m,X j,...,X j)=n ∑−1 ∑ ( m mn− ⋯1 m )Λm k↝j (Am h↝i◦ϵ m ↝X j) (E.5)
1 2 k
k=1m1+⋯ m+ lm >k 0=n−1
For(ii),κ(ϵ ,X ,...,X )hasthesameformasthecumulantweprovedinTheorem2,wehave:
i j j
κ(ϵ i,X j,...,X j)=∑n k=− 11 ∑ ( m 1mn 2− ⋯1 m k)Λi k↝j (1◦X i ↝X j). (E.6)
m1+⋯ m+ lm >k 0=n−1
SubstitutingEq.(E.5)andEq.(E.6)intoEq.(E.3),wehave
C i( ,n j) = ∑ ∣P ∑m↝i∣n ∑−1 ∑ ( m mn− ⋯1 m )Λm k↝j (Am h↝i◦ϵ m ↝X j)
1 2 k
m∈An(i,j) h=1 k=1m1+⋯ m+ lm >k 0=n−1
(E.7)
+∑n k=− 11 ∑ ( m 1mn 2− ⋯1 m k)Λi k↝j (1◦X i ↝X j).
m1+⋯ m+ lm >k 0=n−1
ByrewritingEq.(E.7),wehave
C i( ,n j) =n k∑ =− 11
m1+⋯
m∑
+ lm >k
0=n−1( m 1mn 2− ⋯1 m k)⎡ ⎢⎢⎢⎢⎢⎢⎢⎢ ⎣m∈A∑ n(i,j)∣P h∑m =↝ 1i∣ Λm k↝j (Am h↝i◦ϵ m ↝X j)+Λi k↝j (1◦X i ↝X j)⎤ ⎥⎥⎥⎥⎥⎥⎥⎥
⎦
(E.8)
n−1 n−1
= ∑ ∑ ( m m ⋯m )Λ˜ k(X i ↝X j).
1 2 k
k=1m1+⋯ m+ lm >k 0=n−1
Thiscompletestheproof.
F ProofofTheorem6andTheorem7
Theorem6(IdentificationofPB-SCM). Ifthereexistk ∈Z+ suchthatΛ˜ (X ↝X )≠0andΛ˜ (X ↝X )=0foranytwo
k i j k j i
adjacencyvertexiandj,thenX istheancestorofX
i j
Proof. ForthecasethatX isarootvertex.SupposeX isnottheancestorofX ,thenX andX areindependentsinceX is
i i j i j i
therootvertexandX isnottheancestorofX .Inthiscase,theΛ˜ (X ↝X )=0foreachksinceX andX areindependent.
j i k i j i j
ForthecasethatX isnotarootvertex.SupposeX isnottheancestorofX ,thentheremustexistkpathfromthecommon
i i j
ancestortoX sinceΛ˜ (X ↝X )≠0.However,thiscontradictstheconditionΛ˜ (X ↝X )=0asitindicatesthattherenot
i k i j k j i
existkpathsfromthecommonancestortoX .Hence,weconcludethatX istheancestorofX .
i i j
Theorem7(GraphicalImplicationofIdentifiability). Forapairofverticesiandj,ifiisanancestorofj.Thecausalorder
of i,j is identifiable by Theorem 6, if (i) vertex i is a root vertex and ∣Pi↝j ∣ ≥ 2; or (ii) there exists a common ancestor
k ∈argmax{∣Pl↝i ∣∣l∈An(i,j)}hasadirectedpathfromktoj withoutpassingiinG.
l
Proof. (i)Ifvertexiisarootvertexand∣Pi↝j ∣≥2,wehave
Λ˜ (X ↝X )=Λi↝j (1◦ϵ ↝X )≠0
2 i j 2 i j
∣Pj↝i∣ (F.1)
Λ˜ (X ↝X )= ∑ Λj↝i (Aj↝i◦ϵ ↝X )=0
2 j i 2 h i i
h=1
Sincethereare⩾2pathsfromitoj andnotwopathsfromitoi(fromnoisecomponentofitoi),wehaveΛ˜ (X ↝X )≠0
2 i j
andΛ˜ (X ↝X )=0.BasedonTheorem6,iistheancestorofj.
2 j i
(ii)Accordingtotheacyclicconstraints,thenumberofpathsfromtheircommonancestorstoj iseitherequalormorethan
thenumberofpathsfromtheircommonancestorstoi,sinceiisanancestorofj andthosepathstoimustreachj.
Ifthereexistsacommonancestork ∈argmax{∣Pl↝i ∣∣l∈An(i,j)}hasadirectedpathfromktoj withoutpassingiinG,
l
itimpliesthatthenumberofpathsfromktojgreaterthanthattoi,Consequently,theremustexistavaluen=∣Pk↝j
∣suchthat
Λ˜ (X ↝X )≠0andΛ˜ (X ↝X )=0,andthecausalorderofi,j isidentifiablebasedonTheorem6.
n i j n j iG ProofofTheorem8
Theorem8. LetG (s)bethePGFofrandomvariableX givenitsparentsvariableX ,wehave:
Xi∣XPa(i) i Pa(i)
P(X
i
=k∣X
Pa(i)
=x Pa(i))= k1 !∂kG X (i ∂∣X s)P ka(i)(s)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)s=0
= ∑
µt iiexp(−µ i)
∏
(x j) tjα jtj ,i(1−α j,i)xj−tj
,
(G.1)
t ! t !
ti+ ∑ tj=k i j∈Pa(i) j
j∈Pa(i)
wheret ≤x ,(x ) = xj! isthefallingfactorial,µ =E[ϵ ],andϵ isthenoisecomponentofX .
j j j tj (xj−tj)! i i i i
Proof. ForprobabilitymassfunctionP(X ∣X ),whichcanbedecomposedasfollow:
i Pa(i)
P(X ∣X )=P(ϵ ) ∏ P(α ◦X ∣X ).
i Pa(i) i j,i j j (G.2)
j∈Pa(i)
LetG ϵi(s)representtheprobabilitygeneratingfunction(PGF)ofP(ϵ i),whichisthenoisecomponentofX i,andG αj,i◦Xj∣Xj(s)
denotethePGFofP(α ◦X ∣X ),wehave:
j,i j j
G Xi∣XPa(i)(s)=G ϵi(s) ∏ G αj,i◦Xj∣Xj(s),
(G.3)
j∈Pa(i)
whereG ϵi(s)=exp[µ i(s−1)]andG αj,i◦Xj∣Xj(s)=(1−α j,i+α j,is)Xj.
AccordingtothepropertyofPGF,wecancalculatetheprobabilitymassfunctionbytakingderivativesofG (s),and
Xi∣XPa(i)
thederivativeisexpressedas:
∂kG Xi∣XPa(i)(s)
=
∂k (G ϵi(s)∏ j∈Pa(i)G αj,i◦Xj∣Xj(s))
(G.4)
(∂s)k (∂s)k
Accordingtotheproductruleofhigherderivatives,i.e.
n (k) k n
(∏f i) = ∑ (
t ,t ,...,t
)∏f i(ti)
1 2 n
i=1 t1+t2+⋯+tn=k i=1
(G.5)
k! n n f(ti)
= ∑ ∏f(ti) =k! ∑ ∏ i ,
t !t !⋯t ! i t !
t1+t2+⋯+tn=k 1 2 n i=1 t1+t2+⋯+tn=k i=1 i
wehave:
∂kG Xi∣XPa(i)(s)
=k! ∑
G( ϵt ii) (s)
∏
G α(t jj ,) i◦Xj∣Xj(s)
. (G.6)
(∂s)k
ti+ ∑ tj=k
t i!
j∈Pa(i)
t j!
j∈Pa(i)
Furthermore,wehave
G(ti) (s)=µtiexp(µ (s−1)),
ϵi i i
G(tj) (s)={(X j) tjα jtj ,i(1−α j,i)Xj−tj t j ⩽X j . (G.7)
αj,i◦Xj∣Xj 0 t >X
j j
GivenX
i
=x i,X
Pa(i)
=x Pa(i),alongwiththemodelparameterΘ={A=[α i,j]∈[0,1]∣V∣×∣V∣,µ=[µ i]∈R∣ ≥V 0∣ },wecan
computetheprobabilitymassfunctionP(X ∣X )asfollow:
i Pa(i)P(X
i
=k∣X
Pa(i)
=x Pa(i))= k1 !∂kG Xi∣X (P ∂a s(i ))= kxPa(i)(s)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187)s=0
=
1
∑
G( ϵt ii) (0)
∏
G α(t jj ,) i◦Xj∣Xj=xj(0)
k! t ! t !
ti+ ∑ tj=k i j∈Pa(i) j
j∈Pa(i)
=
1
∑
G( ϵt ii) (0)
∏
G α(t jj ,) i◦Xj∣Xj=xj(0) (G.8)
k! t ! t !
ti+ ∑ tj=k i j∈Pa(i) j
j∈Pa(i)
= ∑
µt iiexp(−µ i)
∏
(x j) tjα jtj ,i(1−α j,i)xj−tj
,
t ! t !
ti+ ∑ tj=k, i j∈Pa(i) j
j∈Pa(i)
wheret ⩽x .Thiscompletestheproof.
j j
AcceleratingLikelihoodComputationUsingFFT
Tocomputethelikelihoodfunction,wehavetocalculatetheEq.(G.8).However,thistaskremainscomputationallyintensivedue
tothenumerousparametercombinationssatisfyingthespecificsummationconditiont + ∑ t =kandt ⩽x .
i j j j
j∈Pa(i)
Toaddressthisissue,weshowthatthelikelihoodinEq.(G.1)canbeformulatedastheproblemofobtainingthecoefficientof
apolynomialproduct.
Specifically,theproductionofpolynomialscanbeconstructedasfollows:
F(y)=(µ0
i
exp(−µ
i) +
µ1
i
exp(−µ
i) y+⋯+
µk
i
exp(−µ
i) yk
)
0! 1! k!
× ∏
((x j) 0α j,i(1−α j,i)xj
+
(x j) 1α j,i(1−α j,i)xj−1
y+⋯+
(x j) xjα j,i(1−α j,i)xj−xj
yxj)
(G.9)
0! 1! x !
j∈Pa(i) j
ThenthelikelihoodinEq.(G.8)isexactlythecoefficientofykaftertheproduction.Toobtainthecoefficientofsuchproduction,
wecanemploytheFastFourierTransform(FFT).Indetail,wecancreateaseriesofvectorsofthecoefficientofeachpolynomial
in(G.9),andpadthelistwith0sincethehighestpowerofxisk× ∣Pa(i)∣:
⎡ ⎤
a
0
=⎢⎢⎢⎢⎢⎢⎢⎢⎢µ0
i
exp 0( !−µ i) ,µ1
i
exp 1( !−µ i) ,...,µk
i
exp k( !−µ i)
, 0 (cid:205)(cid:210)(cid:210)(cid:210)(cid:210), (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210). (cid:210)(cid:210)(cid:209).. (cid:210)(cid:210)(cid:210), (cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)0
(cid:210)(cid:210)(cid:210)(cid:207)
⎥⎥⎥⎥⎥⎥⎥⎥⎥
⎣ k×∣Pa(i)∣−k+1times⎦
a
jp
=⎡ ⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢ ⎣(x jp) 0α jp,i( 01 !−α jp,i)xjp ,(x jp) 1α jp,i(1 1!−α jp,i)xjp−1 ,...,(x jp) xjpα jp,i( x1 jp− !α jp,i)xjp−xjp
, k×∣Pa((cid:205)0
i(cid:210)
)(cid:210)(cid:210)(cid:210),
(cid:210)(cid:210) ∣(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)
−. (cid:210)(cid:210)(cid:209). x. (cid:210)(cid:210)(cid:210),
(cid:210)(cid:210) j(cid:210)(cid:210)(cid:210)(cid:210)
p(cid:210)(cid:210)0
(cid:210)(cid:210)(cid:210)(cid:207)
+1times⎤ ⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥
⎦
(G.10)
wherej ∈Pa(i)andp=1,2,...,∣Pa(i)∣.ThenthecoefficientvectoroftheexpansionofEq.(G.9)isgivenby:
p
aˆ =IFFT(FFT(a )⊙FFT(a )⊙⋯⊙FFT(a )) (G.11)
0 j1 j ∣Pa(i)∣
Here,⊙istheelement-wisemultiplication,FFT(⋅ )andIFFT(⋅ )istheimplementationoffastFouriertransformandInversefast
Fouriertransformrespectively.Consequently,thek+1-thelementinthevectoraˆisthecoefficientofyk intheexpansionofEq.
(G.9),whichisthelikelihoodgivenEq.(G.8).
H AdditionalExperiments
ThemainpaperhasshowntheF1scoresandotherbaselinesinsyntheticdataexperiments.Here,wefurtherprovidethePrecision,
Recall,andStructuralHammingDistance(SHD)intheseexperiments,asshowninFig.3,Fig.4andFig.5.           
           
           
     $ O J R U L W K P      $ O J R U L W K P      $ O J R U L W K P
 3 %  6 & 0  3  3 %  6 & 0  3  3 %  6 & 0  3
 3 %  6 & 0  3 %  6 & 0  3 %  6 & 0
     3 &      3 &      3 &
 * ( 6  * ( 6  * ( 6
     2 & '      2 & '      2 & '
                                                                      
 $ Y J   , Q G H J U H H  5 D W H  1 X P E H U  R I  Y H U W L F H V  6 D P S O H  6 L ] H
(a) SensitivitytoAvg.IndegreeRate (b) SensitivitytoNumberofvertices (c) SensitivitytoSampleSize
Figure3:PrecisionintheSensitivityExperiments
           
           
           
     $ O J R U L W K P      $ O J R U L W K P      $ O J R U L W K P
 3 %  6 & 0  3  3 %  6 & 0  3  3 %  6 & 0  3
 3 %  6 & 0  3 %  6 & 0  3 %  6 & 0
     3 &      3 &      3 &
 * ( 6  * ( 6  * ( 6
     2 & '      2 & '      2 & '
                                                                      
 $ Y J   , Q G H J U H H  5 D W H  1 X P E H U  R I  Y H U W L F H V  6 D P S O H  6 L ] H
(a) SensitivitytoAvg.IndegreeRate (b) SensitivitytoNumberofvertices (c) SensitivitytoSampleSize
Figure4:RecallintheSensitivityExperiments
  
  
  
     
  
     
  
     
 $ O J R U L W K P  $ O J R U L W K P  $ O J R U L W K P
    3 %  6 & 0  3     3 %  6 & 0  3     3 %  6 & 0  3
 3 %  6 & 0  3 %  6 & 0  3 %  6 & 0
    3 &  3 &     3 &
  
 * ( 6  * ( 6  * ( 6
   2 & '  2 & '    2 & '
 
                                                                      
 $ Y J   , Q G H J U H H  5 D W H  1 X P E H U  R I  Y H U W L F H V  6 D P S O H  6 L ] H
(a) SensitivitytoAvg.IndegreeRate (b) SensitivitytoNumberofvertices (c) SensitivitytoSampleSize
Figure5:SHDintheSensitivityExperiments
Table1:SensitivitytothemaxorderofcumulantK
Scoretype
K
F1 Precision Recall SHD
2 0.69±0.04 0.56±0.05 0.90±0.05 23.38±3.88
3 0.82±0.06 0.82±0.08 0.83±0.06 9.53±2.71
4 0.82±0.06 0.82±0.07 0.83±0.06 9.47±2.78
5 0.83±0.05 0.82±0.07 0.84±0.05 9.47±2.68
 Q R L V L F H U 3
 O O D F H 5
 ' + 6
 Q R L V L F H U 3
 O O D F H 5
 ' + 6
 Q R L V L F H U 3
 O O D F H 5
 ' + 6