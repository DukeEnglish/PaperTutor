JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 1
CurbNet: Curb Detection Framework Based on
LiDAR Point Cloud Segmentation
Guoyang Zhao1, Fulong Ma1, Yuxuan Liu2, Weiqing Qi1, and Ming Liu1,2,†
Abstract—Curb detection is an important function in intelli-
gent driving and can be used to determine drivable areas of the
road. However, curbs are difficult to detect due to the complex
road environment. This paper introduces CurbNet, a novel
framework for curb detection, leveraging point cloud segmenta-
tion. Addressing the dearth of comprehensive curb datasets and
the absence of 3D annotations, we have developed the 3D-Curb
dataset,encompassing7,100frames,whichrepresentsthelargest
andmostcategoricallydiversecollectionofcurbpointcloudscur-
rentlyavailable.Recognizingthatcurbsareprimarilycharacter-
ized by height variations, our approach harnesses spatially-rich
3D point clouds for training. To tackle the challenges presented
by the uneven distribution of curb features on the xy-plane and
theirrelianceonz-axishigh-frequencyfeatures,weintroducethe
multi-scale and channel attention (MSCA) module, a bespoke
solution designed to optimize detection performance. Moreover,
weproposeanadaptiveweightedlossfunctiongroup,specifically
formulated to counteract the imbalance in the distribution of
curb point clouds relative to other categories. Our extensive
experimentation on 2 major datasets has yielded results that
surpass existing benchmarks set by leading curb detection and Fig. 1. Curb detection challenges and our proposed method. Three
pointcloudsegmentationmodels.Byintegratingmulti-clustering main challenges of curb detection are shown (a) height feature extraction
andcurvefittingtechniquesinourpost-processingstage,wehave (b)differentdensitydistributionofpointclouds(c)Curbpointcloudquantity
substantiallyreduced noiseincurbdetection, therebyenhancing proportionimbalance.Solution:Firstproposea3D-Curbdataset.TheMSCA
moduleisdesignedtoextractfeaturesofthexy,xzandyzplanesandperform
precisionto0.8744.Notably,CurbNethasachievedanexceptional
multi-feature fusion. The loss group is proposed to solve the imbalance
averagemetricsofover0.95atatoleranceofjust0.15m,thereby
problem.Finally,weusepost-processingtofurtherimproveperformance
establishinganewbenchmark.Furthermore,corroborativereal-
world experiments and dataset analyzes mutually validate each Research in curb detection varies based on sensor technol-
other, solidifying CurbNet’s superior detection proficiency and
ogy, broadly categorized into vision-based and LiDAR-based
its robust generalizability. The code and dataset will be open
methods [7]. This includes the use of monocular cameras,
sourced to: https://github.com/guoyangzhao/CurbNet/.
stereovision,andcombinationsof2Dand3DLiDARsensors.
IndexTerms—Pointcloud,Curbdetection,Segmentation,Deep
Vision-basedmethodscanproviderichcontextualinformation
learning, Autonomous driving.
and achieve effective detection results. However, camera per-
formanceisgreatlyinfluencedbylightandweatherconditions
I. INTRODUCTION
andfailstoprovideaccuratedepthinformationdirectly,which
Autonomous vehicles fundamentally depend on analyzing is a critical need for autonomous driving applications [8], [9].
data from onboard sensors to understand their surrounding Furthermore,thesubtlecolordifferencesbetweenregularroad
environment, a cornerstone for safe driving [1], [2]. In this surfaces and curbs make it difficult for cameras to accurately
context,roadboundarydetectionisacrucialaspectofpercep- detect curbs. In contrast, LiDAR sensors demonstrate robust-
tion, delineating road and non-road areas [3]. This distinction ness under various weather and lighting conditions and offer
is vital for the positioning, planning, and decision-making precise distance measurements [10]. Recently, 3D LiDAR has
of self-driving cars, especially under conditions where GPS become one of the most important sensors for perceiving 3D
signalsareobscuredbytreesandbuildings.Insuchurbanroad environments in autonomous vehicles [11], [12].
environments, curbs serve as a key and effective feature for
Curb detection using LiDAR can be classified into manual
vehicle localization [4], [5]. Despite their importance, curbs,
featuremethodsandlearningbasedmethods[13].Manualde-
typicallylinear,thin,andlong,poseasignificantchallengefor
signfeaturemethods[14],[15]typicallyanalyzegeometricre-
detection in complex road environments [6].
lationshipssuchasheightandanglechangesbetweenadjacent
ManuscriptreceivedApril19,2021;† correspondingauthor. points, given the difference between drivable roads and curbs
1G.Zhao,F.Ma,W.QiandM.LiuarewiththeRoboticsandAutonomous [16], [17]. Most curb detection methods follow a sequential
Systems Thrust, The Hong Kong University of Science and Technology point cloud processing procedure [18], [19], including stages
(Guangzhou),Guangzhou,China.
of candidate region extraction, manual feature setting and
2Y. Liu is with The Hong Kong University of Science and Technology,
HongKongSAR,China clustering, and post-processing for fitting estimation. These
4202
raM
52
]VC.sc[
1v49761.3042:viXraJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 2
methods, due to their interpretability in terms of safety, are dataset, to our knowledge which is the largest and most
widelyusedincurbdetectionforautonomousdrivingsystems. diverse currently available.
However,thereliabilityofthesemanuallydesignedrule-based 2) Proposing a new multi-scale and channel attention mod-
processes is limited in practical applications, as errors in ule and an imbalance loss strategy, based on the distri-
early detection stages can severely impact subsequent recog- bution characteristics of curb point clouds.
nition performance. Additionally, they heavily rely on manual 3) Implementing a multi-cluster fitting post-processing ap-
feature design, necessitating extensive parameter adjustments proach to further enhance detection performance.
for various scenarios such as straight roads, curved roads, 4) Achieving state-of-the-art detection results in complex,
and different types of intersections, resulting in low practical large-scale intersection scenarios.
efficiency and generalization [16], [20].
As deep learning has made breakthrough applications in II. RELATEDWORKS
perception, its capabilities in automatic feature extraction and
A. Manual Feature Extraction Methods
learning have significantly reduced the tedium of manual
IntheearlydevelopmentofLiDARtechnology,researchers
feature design and parameter adjustment [21]. Particularly,
designedmathematicalfunctionstomanuallyextractcurbfea-
in addressing different road scenarios, it has substantially
turesbyunderstandingtheprinciplesofLiDARscanning.This
enhancedmodelrecognitionperformanceandrobustness[22].
process primarily involves stages of feature point extraction,
Recently,someresearchershaveexploredcurbdetectionusing
feature point classification and filtering, and curb curve fitting
CNN-based methods, projecting 3D LiDAR point cloud data
and estimation. In [28], feature points are extracted through
into 2D images from a Bird’s Eye View (BEV) perspective,
image segmentation and energy minimization, followed by
followed by processing these images with CNN models to
the application of principal curves and surfaces methods in
detect curbs [23], [24]. However, this direct projection of 3D
[29] for fitting detected curbs. Studies like [14], [15], [30],
LiDAR data can lead to the loss of essential spatial structural
[31] utilize the horizontal and vertical continuity of point
information,particularlythecrucialheightdifferenceinforma-
clouds,employingangleandheightthresholdsforcurbfeature
tion for curb detection [25].
extraction and using Gaussian Process Regression (GPR) and
Considering current research in curb detection and the
RandomSampleConsensusforcurbcurvefitting.[15],[16]in-
physical properties of 3D LiDAR, we have identified several
tegrate the generalized curvature method from LOAM [23]
challenges that need addressing, as shown in Fig. 1: (a)
intocurbdetection,refiningtheprocesswithGaussianProcess
The primary distinguishing feature of curbs is the subtle
Regression.[16],[32]detectcurbsbyanalyzingringcompres-
height variation from the road surface [14], a challenging
sion in dense 3D LIDAR data, employing false positive filters
feature for models to accurately learn. (b) Curb point clouds
and least-squares regression filters based on height values,
fromLiDARscanningshowsignificantdistributiondifferences
respectively. [19], [33] propose sliding-beam segmentation
across various distances [26]. (c) Curbs occupy only a small
andsliding-windowdetectionmethodsbyanalyzingindividual
portion of LiDAR point clouds, appearing as long and narrow
LiDAR scan lines, focusing on specific curb detection in each
lines, making effective model training difficult [25].
frame.
We first propose the 3D-Curb dataset, which contains up to
However, these manual feature extraction and sequential
7100framesof3Dannotatedpointclouddata,coveringawide
processing methods are inefficient. Not only is feature cre-
range of road scenes. Unlike most existing algorithms that
ation laborious and requires specialized knowledge, but early-
project 3D point clouds onto 2D images, we extract features
stage erroneous selections can impact later detection phases,
directly from 3D point clouds, preserving crucial 3D spatial
making them inadequate for complex road scenes and diverse
information.Forthechallenge(a)and(b),wedrawinspiration
curb shapes as noted in [34]. In this context, the advent of
from the cylindrical voxelization approach [27], aligning the
deep learning methods effectively solved the issues of feature
model’s input data more closely with the physical distribution
extraction and generalization.
of LiDAR scans. We then propose a multi-scale and multi-
channel attention module, focusing not only on changes in
the Z-axis height but also on enhancing the performance of B. Deep Learning Methods
multi-scale feature fusion. For the challenge (c), the difficulty With the advancement of deep learning, some researchers
of curb training in autonomous driving is objectively caused have begun using CNNs to detect curbs in LiDAR point
bythesparsequantityofcurbpointclouds.Categoriesrelated clouds, achieving notable performance. A common charac-
to curbs (like roads and sidewalks) can assist in the model’s teristic of deep learning-based curb detection methods is the
detection of curbs. Addressing the severe imbalance in point transformation of input 3D point cloud data into 2D view
cloud quantities among different categories, we introduce a imagesorvoxelization.[35]usescameraimages,LiDAR,and
new loss combination, including a adaptive cross-entropy loss elevation gradients of LiDAR as inputs, employing convolu-
and an IoU-focused loss. Finally, in order to further improve tional recurrent networks to extract road boundaries in 2D
theprecisionofcurbdetection,weproposedapost-processing BEVimagestoconstructsemanticmaps.[24]projectsmotion-
scheme of multi-cluster and then fitting, which effectively accumulated 3D point cloud data onto 2D BEV images,
removes the noise around the curb detection results. initiallydetectingvisibleroadedgesthroughaU-Netnetwork
Our primary contributions are summarized as follows: [36], followed by predicting obscured road boundaries using
1) Introducing a comprehensive 3D-Curb point cloud multi-layer convolutional networks with expanded receptiveJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 3
fields[37].Similarly,[23]proposesatwo-stagecurbdetection TABLEI
framework, initially employing U-Net for visible curb detec- COMPARISONOFRELATEDCURBDATASETS.
tion, then incorporating uncertainty quantification to improve
Dataset LiDAR Public Frames 3Dlabeling Categories
detection performance in obscured areas.
Zhang[19] 32-line Yes 200 No 1
Compared to traditional methods, these approaches demon-
strate robustness in various driving environments and reduce Liang[35] - No - No 1
the burden of manual parameter tuning. However, converting Suleymanov[24] - No - No 1
point clouds to 2D images results in significant loss of 3D Uncertainty[23] 32-line Yes 5224 No 2
information, especially the height difference features crucial NRS[25] 128-line Yes 6220 No 2
for distinguishing curbs from other categories. [38] explores
3D-Curb(ours) 64-line Yes 7100 Yes 29
voxelizing the raw 3D point cloud as a superimage input
to the model, using CNNs to detect road edges and lane
markings. However, 3D model recognition accuracy remains
III. METHODOLOGY
low. LCDeT [25] employs a Transformer model for curb
detection in voxelized point clouds, introducing dual attention A. 3D-Curb Dataset Construction
mechanismsinbothtemporalandspatialdimensionstoensure
Comparedtootherscenariosinautonomousdriving,thereis
detection stability and accuracy. Yet, this method relies on
anotablelackofrelevantcurbpointclouddatasets,especially
complexmodelstructuresandhigh-resolutionLiDARsensors.
those annotated for 3D point clouds. We have developed and
Ourproposedmethodusesasingle,original3Dpointcloud
proposed the 3D-Curb dataset based on the large-scale, open-
as input, preserving more 3D feature information. Addressing
source SemanticKITTI dataset [41], adding a new curb cate-
the uneven distribution of LiDAR features in curb recognition
gorywhileretainingtheotheroriginal28semanticcategories.
scenesandtheeasylossofcurbheightdifferenceinformation,
This dataset was collected using a 64-line LiDAR, providing
we introduce multi-scale and multi-channel attention mecha-
a comprehensive view of various street scenes as a universal
nisms to enhance model recognition performance.
autonomousdrivingdataset.Inthedatasetcreationprocess,we
adopted the method proposed by [34] to initially extract curb
C. Curb Detection Datasets locationsinaBird’sEyeView(BEV)perspective,followedby
manual annotation of representative scenes in 3D point cloud
There is a significant amount of research in LiDAR-
data. The 3D-Curb dataset focuses on curb annotations in the
basedcurbdetection;however,high-qualitydatasetswithcurb
forward direction of vehicle travel, with an average range of
annotations are scarce. Major automotive datasets such as
40.43 meters along the forward y-axis. To accentuate the curb
NuScenes [39], KITTI [40], and SemanticKITTI [41] do not
areas,thelateralx-axisrangeissetto1.3timestheroadwidth.
include curb annotations. The robustness of deep learning
The 3D-Curb dataset encompasses all scenes from se-
methods is closely related to the volume of data collected
quences 00-10 in the SemanticKITTI dataset, totaling 7100
under various environmental conditions. These factors have
frames of finely annotated curb point cloud data. To our
somewhat hindered the application of deep learning methods
knowledge, this is the largest curb point cloud dataset to
in this task. [19] created a public dataset for curb detection,
date and the only one annotated in 3D. Table 1 presents a
comprising 200 scans collected across five different scenes.
comparison with other relevant curb datasets.
[23] developed a public dataset for curb detection, including
5200scanswithBird’sEyeView(BEV)labels,collectedfrom
urban areas. LCDeT [25] introduced a curb dataset for 128-
B. Overview of Model Framework
beam LiDAR, containing 6200 frames of point clouds from
urban road scenes, both during daytime and nighttime. [26] As shown in Fig. 2, the CurbNet framework primarily
proposed a method for 3D curb detection and annotation in consists of three parts: data voxelization, feature extraction,
LiDAR point clouds, effectively reducing manual annotation and feature decomposition and segmentation.
time by 50%. In the data voxelization section, to better align the input
The above-mentioned curb point cloud datasets only label data structure to the physical characteristics of LiDAR point
the curb category, which cannot be directly used in au- clouds (where the density of the circularly scanned point
tonomousdrivingscenarios,asrecognitionofothercategories clouds decreases with increasing distance), we adopted a
such as vehicles and roads is also necessary. Similarly, other cylindrical partitioning format for point cloud voxelization,
categories surrounding the curb, like roads and sidewalks, can inspired by Cylinder3D [27]. We converted the Cartesian
alsoassistthemodelinmoreaccuratelylearningcurbfeatures. coordinates (x,y,z) of the point cloud into polar coordinates
Based on the existing large SemanticKITTI dataset, we added (radius ρ and azimuth angle θ), aligning more accurately
annotations for the curb category, thereby covering a richer with the scanning pattern of LiDAR and preserving the point
array of real road scenes, totaling up to 7100 frames of point cloud’s geometric structure to the greatest extent. The voxel
cloud data. To our knowledge, this is currently the largest and division was then performed uniformly across the ρ, θ, and
mostcomprehensivecurbpointclouddatasetwithannotations z dimensions, ensuring larger voxels for greater distances
relevant to autonomous vehicles (AV). Table. I illustrates the to minimize the impact of varying point cloud densities on
related LiDAR datasets for curb detection. feature learning.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 4
Fig.2. Overview of proposed CurbNet framework.Fromlefttoright,firstistheinputdatacoordinatetransformationandcylindricalvoxelization.Then
thereistheU-Netshapedencoder-decoderstructure.Thisisfollowedbyfeaturedecompositionandsegmentationhead.Finally,themodelcalculateslossand
outputsdetectionresults.
In the feature extraction stage, the voxelized features of
dimensions F×H×W×D were fed into an end-to-end 3D U-
Netmodel.DivergingfromtheclassicU-Netmodel’sEncoder
and Decoder structure, we proposed a Multi Scale and Chan-
nel Attention Module (MSCA) tailored to the characteristic
distribution of curb point clouds, employing stride=2 Sparse
Convolution for pooling functions.
Inthefeaturedecompositionandsegmentationstage,wede-
composedtheoutputhigh-dimensionalfeaturesofF×H×W×D.
Afterthetraining,,thefeaturetensorishigh-rank[42],andits
rich context encoding information incurs substantial modeling
costs in the point cloud segmentation process. Inspired by
high-rank matrix decomposition theory [43], we performed
low-rank decomposition on the H×W×D dimensions using
three one-dimensional convolution kernels: 3×1×1, 1×3×1,
and 1×1×3, respectively. We use two layers of low-rank
decomposition and add dense connections to thereby increase
feature aggregation and provide more reference features for
Fig.3. Structure of multi-scale and channel attention (MSCA) module.
the segmentation head. The weights for each dimension were
Threeconvolutionsareusedtomatchthetargetareaonthespatialxy,xzand
modulated through the Sigmoid function. Finally, context yzplanes,anddenseconnectionsareusedtoobtainthemulti-scalefeatures
features were aggregated to consolidate features and output ofthecurb.
segmentation results efficiently. algorithm, which only focuses on feature learning in the xy
planeandoverlookscurb’shigh-frequencyfeatureinformation
C. Multi Scale and Channel Attention (MSCA) Module
on the Z-axis, we have added a convolution module for the
The structure of the MSCA as shown in Fig. 3, constitutes Z-axis D dimension. This module makes the model more
a fundamental component of both the Encoder and Decoder. attunedtocurb’shigh-frequencyfeaturesinheightdifferences.
Curb point clouds typically form a long curve along both Inspired by text detection methods [26], we employed three
sides of the road, but with the increasing scanning distance asymmetrical convolution kernels to match the target areas in
of LiDAR, the point cloud density decreases, leading to the xy, xz, and yz planes of the space, thereby capturing the
sparser curb point cloud features. This results in significant curb feature information. Compared to the structure of two
scale differences in the features represented by the same traditional 3×3×3 convolutions chained together, the MSCA
numberofpointcloudsatdifferentdistancesaftervoxelization. module (Fig. 3) achieves thrice the efficiency in feature
Drawing on multi-feature fusion methods [44], we uniformly extraction and blending, with the same receptive field and
blendmulti-scalefeaturesderivedfromdifferentconvolutional computational resource consumption.
kernel outputs.
The primary distinguishing features of curbs lie in the
D. Loss Group
subtle height differences between the road and sidewalks, i.e.,
the differences along the Z-axis in the point cloud features. In the context of curb detection for autonomous driving,
Unlike the Cylinder3D [27] general point cloud segmentation we propose a novel combination of loss functions to adapt toJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 5
the class imbalance in training data. In real-world scenarios, This index provides the ratio between the intersection and
the point cloud data for curbs comprises only a small fraction union of the true and predicted masks within the range [0, 1],
comparedto othercategoriessuch asroadsand buildings.Us- with the convention 0/0 = 1. The corresponding loss function
ing uniform loss weights can lead to training imbalances, ad- employed in empirical risk minimization is:
versely affecting the recognition performance for the minority
∆ (y∗,y)=1−IoU (y∗,y) (6)
class, i.e., curbs. To address this, we introduce a combination IoUc (cid:101) c (cid:101)
of Adaptive Cross-Entropy (CE) Loss and Lova´sz-Softmax For multi-label datasets, it is customary to average across
Loss. classes, yielding the Mean IoU (mIoU).
1) Adaptive CE Loss The Lova´sz-Softmax loss extends this concept by applying
This paper improves upon the standard Cross-Entropy (CE) theLova´szextensiontothesoftmaxprobabilitiesofamodel’s
Loss by adjusting weights based on the distribution of point output.ItoptimizesaconvexsurrogateoftheIoUscore,which
clouddataacrossdifferentclasses,enablingthemodeltofocus is more suitable for gradient-based optimization. Specifically,
more on learning from scarce sample data. The standard CE theLova´sz-SoftmaxlossL forasetofclassesC isdefined
IoU
Loss is defined as: as:
(cid:88)
L y∗,y)= ∆ (y∗,y) (7)
L =−y ∗log(p )−y ∗log(p ) (1) ( (cid:101) IoUc (cid:101)
CE 1 1 2 2
c∈C
where y i represents the true label and p i represents the The computation involves ordering the pixels by their error
predicted probability for class i. marginandcomputingaweightedsumoftheindividualerrors,
To address class imbalance, we modify the loss weights for thus directly targeting the errors that most impact the IoU
different classes using the α parameter, akin to the weighted score.
CE Loss. In this context, α is computed based on the inverse In practice, the Lova´sz-Softmax loss is particularly benefi-
ratio of point cloud quantities of each class, followed by cialforsegmentingobjectswithirregularorsparseboundaries,
normalization. The formulation of the Weighted CE Loss as it focuses on the segments of the predictions that have the
(L ) is: largest impact on the overall IoU score, rather than treating
WCE
all errors equally.
L =−α ∗y ∗log(p )−α ∗y log(p ) (2)
WCE 1 1 1 2 2 2
where α is the class-specific weight calculated as: E. Multi-Cluster and Curve Fitting
i
1 This paper introduces a post-processing method based on
α = (3)
i log(δ+ N ) multi-cluster refitting to filter noise points from LiDAR data
Ni segmentationresults,therebyenhancingthedetectionaccuracy
Here, N is the total number of points in the point cloud, N ofcurbs.DuetotheincreasingsparsityofLiDARpointclouds
i
is the number of points in class i, and δ is a small constant to withdistanceandthepotentialinterruptionofcurblinesdueto
prevent division by zero. obstructions, direct curb clustering along the sides of roads is
Further drawing inspiration from Focal Loss, we introduce challenging,asshowninFig.4.Thus,weadoptamulti-cluster
a modulation factor (1 − P )γ into the cross-entropy loss, strategy, treating the curb in multiple segments.
t
effectively reducing the weights for easily classified samples To address this challenge, we initially apply the Density-
and increasing them for the more challenging ones, thereby Based Spatial Clustering of Applications with Noise (DB-
directing the training focus towards difficult-to-classify sam- SCAN) algorithm [46] for preliminary segmentation of the
ples. The Adaptive CE Loss (L ) is therefore formulated detected curbs. DBSCAN is characterized by its ability to
ACE
as: identify clusters of arbitrary shapes without a predefined
numberofclusters,efficientlyhandlingnoisepoints.Thecore
L =−α ∗(1−p )γ ∗y ∗log(p )
ACE 1 1 1 1
(4) idea of DBSCAN revolves around setting a neighborhood
−α ∗(1−p )γ ∗y ∗log(p )
2 2 2 2 radius ε (eps) and a minimum sample number minPts (min-
samples) to determine cluster membership. In our study, we
The focusing parameter γ, which modulates the emphasis on
set eps to 1 and min-samples to 5.
difficult samples, is set to 2 in this study.
Let P be a point in the point cloud; its ε-neighborhood,
2) Lova´sz-Softmax Loss
denoted as N (P), is defined as:
Lova´szLossisparticularlyeffectiveinhandlingimbalanced ε
datasets and excels in addressing sparse boundary issues [45].
N (P)={Q∈ Dataset |dist(P,Q)≤ε} (8)
Compared to traditional cross-entropy loss, it demonstrates ε
superiorperformanceintermsofIntersectionoverUnion(IoU)
where dist(P,Q) represents the distance between points
scores, commonly used for evaluating image segmentation
P and Q. A point P is considered a core point if its ε-
results due to their quality perception and scale invariance. In
neighborhood contains at least minPts points, i.e., |N (P)|≥
Lova´sz Loss, for a given true label vector y∗ and a predicted ε
minPts.
label vector y, the IoU index for class c is defined as:
(cid:101) Post-clustering, we fit polynomial curves to each indepen-
|{y∗ =c}∩{y =c}| dent curb segment. The key is to precisely fit the geometric
IoU (y∗,y)= (cid:101) (5)
c (cid:101) |{y∗ =c}∪{y =c}| shapeofthecurbwhileeliminatingnoisepointsnotbelonging
(cid:101)JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 6
Algorithm 1 Multi-Cluster and Curve Fitting Post-Processing
Cluster
Require: Point cloud data, ε, minPts, δ
Ensure: Refined curb line segmentation
1: Step 1: Apply DBSCAN to point cloud data
2: for each point P in point cloud do
3: Compute N ε(P)
4: if |N ε(P)|≥minPts then
5: Mark P as a core point
6: end if
7: end for
Fitted curve Boundary 8: Step 2: Segment curb lines into clusters
Curb point cloud
9: Step 3: Fit polynomial curves to each curb segment
Noise point cloud
10: for each curb segment do
Multiple Clustering Curve Fitting - Noise Removal 11: Fit polynomial curve f(x)
Fig. 4. Process of multiple clustering and fitting to remove noise 12: for each point P(x,y) in segment do
points.Theleftfigureshowstheeffectofmultipleclusteringindiscontinuous 13: Calculate perpendicular distance d(P,f)
scenes.Therightfigureshowsthemethodofcurvefittingandsettingdistance
14: if d(P,f)>δ then
toremovenoisepoints.
15: Remove P as noise
tothecurb.Afterpolynomialcurvefittingofeachsegment,by 16: end if
calculatingthedistancefrompointstothefittedcurve,wecan 17: end for
effectively identify and eliminate noise points located outside 18: end for
the fitted curve, as shown in Fig. 4. 19: Step 4: Output refined curb line segments
During the polynomial curve fitting of curbs, the aim is to
eliminate noise points not belonging to the curb. Assuming
the curve equation is f(x), for any point P(x,y) in the point
The performance of our curb detection method was rig-
cloud,wecalculatetheperpendiculardistancedfromthepoint
orously evaluated using standard metrics. These include Pre-
to the curve:
cision, Recall, and F-1 score, which are quintessential for
d(P,f)=|y−f(x)| (9) quantifying the accuracy and reliability of classification mod-
els. Precision, defined as TP , measures the proportion of
If d(P,f) exceeds a predetermined threshold δ, the point P TP+FP
correctly predicted positive observations to the total predicted
is considered noise and is removed from the dataset:
positives. Recall, calculated as TP , assesses the propor-
TP+FN
tion of actual positives that were correctly identified. The F-
If d(P,f)>δ, then P is noise (10)
1 score, given by 2× Precision×Recall, harmonizes the balance
Precision+Recall
The complete operation process is shown in Algorithm. 1. between Precision and Recall, providing a single measure of
Through this approach, combining the DBSCAN algorithm efficacy. Here, TP (True Positives) represents the number of
withpolynomialcurvefittingeffectivelyidentifiesandextracts correct positive predictions, FP (False Positives) denotes the
accurate curb lines from LiDAR point cloud data, while elim- count of negative instances incorrectly classified as positive,
inating noise points, thus improving the overall segmentation TN (True Negatives) refers to the count of correct negative
accuracy. predictions, and FN (False Negatives) signifies the instances
where positive cases were wrongly predicted as negative.
IV. EXPERIMENTANDANALYSIS These metrics collectively offer a comprehensive view of our
model’s performance, crucial for its validation in high-level
A. Experiment Setup
applications.
1) Training Details
Our model training was conducted in an Ubuntu 20.04
B. Quantitative Results of Curb Detection
environment, utilizing an Intel(R) Xeon(R) Gold 5318S CPU
@ 2.10GHz and an NVIDIA RTX 3090 GPU. We employed 1) Model Training Results
the PyTorch framework for model training, using the Adam In the NRS dataset experiments (refer to Table. II), this
optimizer. The training parameters were set with a batch size studycomparedclassicsegmentationalgorithmssuchasPoint-
of 8 and a total of 100 epochs, at a learning rate of 0.001. Pillars, U-Net, Swin-Transformer, and CSWin-Transformer,
Regarding the datasets, we employed two distinct datasets as well as the state-of-the-art curb detection model, LCDeT.
fortrainingandevaluation:thepubliclyavailableNRSdataset Thanks to the specially designed MSCA module for Curb
and our custom-built 3D-Curb dataset. Both datasets com- 3D scenarios, our proposed CurbNet achieved the highest
prised curb data collected from forward-facing trajectories, detection performance on the NRS dataset, with Precision,
eachextendingover40meters.Toenhancefeaturelearningfor Recall, and F-1 scores reaching 0.8280, 0.8341, and 0.8309,
curbdetection,weaugmentedthetrainingprocessbyincluding respectively.
two additional categories: road and sidewalk. The experiments in the 3D-Curb dataset not only compared
2) Evaluation Metrics classic and advanced deep learning model algorithms butJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 7
TABLEII truth to further validate the model’s detection performance.
COMPARISONOFRESULTSINNRS-DATASET[25]. Fig. 5 illustrates the visualization results in scenarios without
obstructions, showcasing the curb detection results in five
Method Precision Recall F-1score
common road scenes: straight road, curved road, right-angle
PointPillars[47] 0.759 0.6019 0.6524 intersection, curved intersection, and cross intersection. The
U-Net[36] 0.7546 0.7018 0.7172 images clearly demonstrate that the CurbNet model success-
Swin-T[48] 0.7216 0.7034 0.7001 fully identified all curb lines present in the ground truth, even
CSWin-T[49] 0.7692 0.6597 0.6966 incomplexintersectionscenarios.Notably,inthecurvedinter-
section scenario, the results identified by CurbNet were even
LCDeT[25] 0.8257 0.8050 0.8092
more precise than the ground truth annotations, showcasing
CurbNet(ours) 0.8280 0.8341 0.8309
our model’s exceptional feature extraction capabilities and
CurbNet-post(ours) 0.8420 0.8497 0.8458
generalizability.
Fig. 6 also displays the visual recognition results in scenar-
TABLEIII
COMPARISONOFRESULTSIN3D-CURBDATASET. ios with obstructions, where yellow dashed circles highlight
the obstructed areas. Despite the absence of point clouds in
Difference Methods Precision Recall F-1score obstructed areas, the model accurately identified curbs in the
Zhang[19] 0.6854 0.6564 0.6053 otherpartswithoutbeinginfluencedbytheobstructedregions.
Manualfeature Sun[9] 0.6878 0.6864 0.6297 As long as the input data contained curb features, CurbNet
Wang[14] 0.7209 0.7013 0.6973 couldaccuratelydetectthem,unaffectedbytheobscuredareas.
Furthermore, we conducted a visual analysis of the results
3DU-Net[50] 0.7695 0.7492 0.7592
ontheNRSdataset,asshowninFig.7.Thisanalysisprimarily
Cylinder3D[27] 0.8049 0.8038 0.7942
DeepLearning showcases curb detection in five common road scenarios
CurbNet(ours) 0.8293 0.8569 0.8429
and four special intersection types. Through a comparative
CurbNet-post(ours) 0.8744 0.8648 0.8696
visualization of the detection results and ground truth, our
proposed CurbNet accurately identified the respective curb
also included three traditional methods of manual feature
features.
extraction, as shown in Table. III. Owing to the robust au-
tomatic feature extraction capabilities, deep learning methods
significantly outperformed in curb detection, surpassing the D. Post-Processing Experiment
other methods by over 10 points in Precision, Recall, and
In this paper, we conducted controlled experiments to
F-1 score. Among the deep learning methods, our CurbNet
compare the proposed post-processing method. Given the
surpassed the best-performing supervised learning model on
use of multiple clustering followed by curve fitting in post-
theSemantickITTIdataset,Cylinder3D,bymorethan2points
processing,thesettingofclusteringparametersplaysacrucial
in Precision. This demonstrates the exceptional performance
role in its effectiveness. Based on the road width and point
of CurbNet in the field of curb point cloud segmentation.
cloud density characteristics of the 3D-Curb dataset, we ex-
2) Tolerance Results
perimented with varying the distance variable Eps (from 1m
Since curbs resemble elongated curves, relevant research
to 4m) and the minimum sample points variable minPts (from
often further tests model performance within a certain error
2 to 200), as illustrated in Fig. 8.
range. Experiments are typically conducted in meters and
AstheclusteringdistanceEpsincreases,changesintheper-
pixels,with1pixelapproximatelyequalto0.1mofTolerance,
formancemetricsofpost-processingbecomemoregradualand
and common experimental settings range from 0.1m to 0.4m.
similar. However, when the minimum sample points minPts
In our study, we conducted Tolerance performance tests on
are lower, the performance decreases compared to when Eps
the3D-Curbdataset,settingfourTolerancesrangingfromjust
is 1m. This is attributed to the larger curb clustering caused
0.05m to 0.2m, as shown in Table. IV. Tests were carried
by greater clustering distances, resulting in minimal changes
out on three models: 3D U-Net, Cylinder 3D, and CurbNet
post-curvefitting.Additionally,largerclusterstendtooverlook
(ours). With the increase in error Tolerance, performance
sparsely distributed point clouds during curve fitting, thereby
metrics improved significantly. At 0.05m Tolerance, Precision
reducing performance.
improved by an average of 4 points; at 0.1m, by 9 points; at
Fig. 8 clearly demonstrates that as the minimum sample
0.15m, by 12 points; and at 0.2m, by 13 points. As Tolerance
points variable minPts increases, the Precision metric of post-
increased, performance gains gradually reached saturation.
processing gradually improves, but both Recall and F-1 score
Notably, our CurbNet exceeded 0.95 in the average values of
metricssignificantlydecrease,especiallyatEpssettingsof1m
Precision, Recall, and F-1 score at just 0.15m Tolerance. This
and 2m. This decline is due to the increase in the number of
representstheoptimalperformanceachievedincurbdetection
minPtsleadingtotheneglectofsparselydistributedcurbpoint
based on point cloud segmentation.
clouds at greater distances, resulting in a noticeable drop in
Recall and F-1 scores.
C. Visualization Results of Curb Detection
Based on comparative experimental results and trade-off
We conducted a visual analysis of the test results obtained between metrics, smaller Eps distances and fewer minPts
using our CurbNet model, comparing them with the ground numbers yield the most optimal post-processing outcomes.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 8
TABLEIV
COMPARISONOFDIFFERENTTOLERANCESIN3D-CURBDATASET.
Precision Recall F-1score
Tolerance(m) 0.05 0.10 0.15 0.20 0.05 0.10 0.15 0.20 0.05 0.10 0.15 0.20
3DU-Net[50] 0.8293 0.8583 0.8881 0.9012 0.8078 0.8498 0.8745 0.8843 0.7933 0.8389 0.8662 0.8777
Cylinder3D[27] 0.8403 0.8909 0.9221 0.9360 0.8588 0.9007 0.9253 0.9355 0.8391 0.8856 0.9136 0.9257
CurbNet(ours) 0.8665 0.9160 0.9460 0.9589 0.9031 0.9433 0.9664 0.9761 0.8844 0.9294 0.9561 0.9674
CurbNet-post(ours) 0.9139 0.9532 0.9682 0.9740 0.8974 0.9301 0.9439 0.9501 0.9056 0.9415 0.9559 0.9619
Fig.5. Curbdetection resultin 3D-Curbdataset.Wecomparedthecurbdetectionresultsatfiveclassicintersections.Themodelaccuratelydetectedthe
curbareaandwasevenbetterthanthegroundtruthonthecurveroad.
Fig. 6. Curb detection result in 3D-Curb dataset under occlusion. We compared the curb detection results at five classic intersections with occlusion.
Evenifthereisocclusion,itdoesnotaffectthecurbdetectioninotherareasatall.
E. Ablation Study results, due to the design of the L loss addressing the
ACE
imbalance in the number of curb point clouds compared to
As shown in Table. V, this study conducted comparative
othercategories,itsdisproportionateweightsettingscausedthe
ablation experiments focusing on the main loss functions and
model to overly focus on recall during training. However, the
crucial module designs of our model. We first conducted
interaction with the L loss led to a more balanced overall
individualexperimentsontheemployedL (Cross-Entropy), IoU
CE
performance, achieving optimal detection capabilities. The
L (Adaptive Cross-Entropy), and L (Intersection
ACE IoU
L +L loss combination outperformed the L +L
over Union) losses. Subsequently, we tested combinations of ACE IoU CE IoU
loss group by 1.5 points in Precision and 2 points in Recall.
L +L loss and L +L loss. In the experimental
CE IoU ACE IoUJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 9
Fig.7. Curb detection result in NRS Dataset.Theexcellentdetectionperformanceandgeneralizationoftheproposedmethodarewelldemonstratedon
theNRSdataset,andaccuratedetectioncanbeperformedevenatthespecialintersection(f-h).
Fig.8. Parameter adjustment experiments for multiple clustering and fitting.Weconductcontrolvariableexperimentsonthemainparameterdistance
variableEpsandtheminimumsamplepointvariableminPtsofDBSCANclustering.ThefewerminPts,thebettertheoverallperformance.
TABLEV addition to the dataset experiments. As depicted in Fig. 9, we
ABLATIONOFDIFFERENTLOSSFUNCTIONSANDMODULES utilized an autonomous delivery vehicle as our experimental
LCE LACE LIoU MSCA Precision Recall F-1score platform, equipping it with a LiDAR sensor system mounted
✓ ✓ 0.8174 0.8303 0.8238 on its top. The LiDAR used was the OS1-U model with 128
✓ ✓ 0.7833 0.8807 0.8290 beams,manufacturedbyOuster.Toensurethegeneralizability
✓ ✓ 0.8234 0.8367 0.8339 of our experiments, the autonomous vehicle was driven on
roads within the HKUST Guangzhou campus. Data collection
✓ ✓ ✓ 0.8186 0.8472 0.8374
and curb detection were carried out in five different road
✓ ✓ 0.8297 0.8496 0.8395
segments (Scene A, B, C, D, and E), as shown in the map in
✓ ✓ ✓ 0.8310 0.8698 0.8499
Fig. 9. These segments included both standard road scenarios
(straight roads, bends, and intersections) and complex ones
Finally, we compared the model’s performance with and (roundabout turns and intricate junctions).
without the MSCA module. Under the L +L loss
ACE IoU The curb detection results in the 5 scenes are illustrated
setting, including the MSCA module significantly enhanced
in Fig. 10, where we selected 8 representative images from
thecorrespondingperformancemetrics,particularlyincreasing
each scene for visual analysis. Overall, our method achieved
Recall by 2 points and F-1 score by 1 point.
commendable curb detection results in each scene, further
demonstrating the robust curb feature extraction capability of
F. Real Scene Experiment
the CurbNet model. In individual cases, our proposed method
To further validate the performance and effectiveness of accurately detected not only the evident, extensive curbs, but
theproposedmethod,weconductedreal-worldexperimentsin also achieved remarkable results on small-scale curbs, whichJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 10
to LiDAR technology, some road areas remain undetected in
the point cloud, leading to an inability of the model to extract
corresponding curb features. This necessitates future research
involving more advanced sensors to minimize scanning blind
spots. Additionally, the development of a model incorporating
a curb prediction module that operates effectively in areas
withoutscanningblindspotsisessentialtomitigatetheimpact
of these blind spots on curb detection.
VI. CONCLUSION
In this paper, we established the 3D-Curb dataset, com-
prising 7,100 frames. To our knowledge, this is currently the
largest and most diverse curb point cloud dataset with the
Fig. 9. Setup of real scene experiment.We used the delivery vehicle most extensive range of annotated categories. Notably, this is
equippedwithLiDARtoconductrealsceneexperiments.Thereareatotalof also the first dataset to feature 3D point cloud annotations
fiveexperimentalsectionsdistributedontheHKUSTGuangzhoucampus.
for curbs, which will significantly aid future related research.
TABLEVI Within the CurbNet framework, we introduced the Multi-
RESULTSINFIVEREALSCENES. Scale and Channel Attention (MSCA) module, addressing
the challenges of uneven distribution of curb features on the
Site Precision Recall F-1score
xy-plane and the reliance on high-frequency z-axis features.
SceneA 0.8331 0.8821 0.8569 Additionally,wedevelopedanadaptiveweightedlossfunction
SceneB 0.8950 0.8123 0.8517 group to resolve the imbalance in the number of curb point
SceneC 0.8610 0.8366 0.8530 clouds relative to other categories. Extensive experiments on
SceneD 0.8018 0.8533 0.8268 both the NRS and 3D-Curb datasets demonstrated that our
approach outperforms the current leading curb detection and
SceneE 0.8579 0.8388 0.8483
point cloud segmentation models. In tolerance experiments,
AllScene 0.8462 0.8443 0.8453
CurbNetachievedover0.95averageperformanceinPrecision,
aretypicallylessconspicuousandeasilyoverlooked,asseenin Recall, and F-1 score metrics at just 0.15m tolerance, setting
SceneB(imaged)andSceneD(imagesbandc).Thissuccess a new standard. Furthermore, our post-processing approach of
canbeattributedtothemodel’sdesignfocusingonmulti-scale multi-clustering and curve fitting effectively eliminated noise
feature fusion and multi-channel feature extraction. Similarly, in the curb results, enhancing the Precision, Recall, and F-
our method also exhibited superior detection performance in 1 score metrics to 0.8744, 0.8648, and 0.8696, respectively.
complexintersectionscenarioswithirregularcurbdistributions Finally,theexcellentdetectionperformanceandgeneralization
(Scene B and Scene C). Particularly in Scene B, the method of our proposed method were further verified in real scene
precisely detected allcurbs present in the LiDARpoint cloud, experiments.
a feat attributable to the powerful curb feature extraction The 3D-Curb dataset and the CurbNet framework estab-
ability of the MSCA module. lishedinthisstudylayafoundationforfutureresearchincurb
Finally, this paper also quantitatively evaluates the real detection.Inourupcomingresearch,weplantocreateamore
sceneexperimentsbytestingkeymetrics,theresultsasshown comprehensive dataset incorporating additional modalities.
in Table. VI. By comparing with manually annotated ground Similarly, we aim to explore and enhance the capabilities of
truthdata,CurbNetachievedanaveragePrecision,Recall,and the CurbNet framework, improving its performance in multi-
F1 score of 0.8462, 0.8443, and 0.8453, respectively, across modal data contexts.
the five scenes. Among them, the Recall and F-1 score indi-
cators obtained in Scene A are the highest, which are 0.8821 REFERENCES
and 0.8569 respectively, and the Precision indicators obtained
[1] T.Luettel,M.Himmelsbach,andH.-J.Wuensche,“Autonomousground
in Scene B are the highest 0.8950. These results corroborate
vehicles—conceptsandapathtothefuture,”ProceedingsoftheIEEE,
withthoseobtainedfromdatasettesting,furthersubstantiating vol.100,no.SpecialCentennialIssue,pp.1831–1839,2012.
theexcellentdetectionperformanceandgeneralizabilityofthe [2] F. Ma, X. Yan, Y. Liu, and M. Liu, “Every dataset counts: Scaling
up monocular 3d object detection with joint datasets training,” arXiv
CurbNet model.
preprintarXiv:2310.00920,2023.
[3] Z.Xu,Y.Sun,andM.Liu,“icurb:Imitationlearning-baseddetectionof
roadcurbsusingaerialimagesforautonomousdriving,”IEEERobotics
V. LIMITATION
andAutomationLetters,vol.6,no.2,pp.1097–1104,2021.
While the method presented in this paper demonstrates [4] J.K.Suhr,J.Jang,D.Min,andH.G.Jung,“Sensorfusion-basedlow-
costvehiclelocalizationsystemforcomplexurbanenvironments,”IEEE
effective and accurate detection of curbs in road scenes,
Transactions on Intelligent Transportation Systems, vol. 18, no. 5, pp.
thus providing a basis for navigable area determination for 1078–1086,2016.
autonomous driving, it currently has limitations in detecting [5] A. Y. Hata, F. T. Ramos, and D. F. Wolf, “Monte carlo localization
on gaussian process occupancy maps for urban environments,” IEEE
curbssolelywithintheLiDARpointcloud.Duetofactorssuch
Transactions on Intelligent Transportation Systems, vol. 19, no. 9, pp.
as the scanning angle, field of view, and obstructions inherent 2893–2902,2017.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 11
Fig.10. Curbdetectionresultsofrealexperimentsinfivefieldscenarios.Ourmethodachievesexcellentcurbdetectionresultsinfivereal-worldscenarios.
Inparticular,italsoshowsexcellentperformanceincomplexintersectionscenariosBandCwithirregularcurbdistribution.
[6] L.M.Romero,J.A.Guerrero,andG.Romero,“Roadcurbdetection: point clouds,” IEEE Transactions on Geoscience and Remote Sensing,
Ahistoricalsurvey,”Sensors,vol.21,no.21,p.6952,2021. vol.55,no.2,pp.996–1009,2016.
[7] A.B.Hillel,R.Lerner,D.Levi,andG.Raz,“Recentprogressinroad [19] Y. Zhang, J. Wang, X. Wang, and J. M. Dolan, “Road-segmentation-
andlanedetection:asurvey,”Machinevisionandapplications,vol.25, basedcurbdetectionmethodforself-drivingviaa3d-lidarsensor,”IEEE
no.3,pp.727–745,2014. transactions on intelligent transportation systems, vol. 19, no. 12, pp.
[8] C. Wei, H. Li, J. Shi, G. Zhao, H. Feng, and L. Quan, “Row anchor 3981–3991,2018.
selection classification method for early-stage crop row-following,” [20] B. Qin, Z. Chong, T. Bandyopadhyay, M. H. Ang, E. Frazzoli, and
ComputersandElectronicsinAgriculture,vol.192,p.106577,2022. D. Rus, “Curb-intersection feature based monte carlo localization on
[9] P.Sun,X.Zhao,Z.Xu,R.Wang,andH.Min,“A3dlidardata-based urbanroads,”in2012IEEEInternationalConferenceonRoboticsand
dedicatedroadboundarydetectionalgorithmforautonomousvehicles,” Automation. IEEE,2012,pp.2640–2646.
IEEEAccess,vol.7,pp.29623–29638,2019.
[21] G.Zhao,L.Quan,H.Li,H.Feng,S.Li,S.Zhang,andR.Liu,“Real-
[10] F. Ma, S. Wang, and M. Liu, “An automatic multi-lidar extrinsic time recognition system of soybean seed full-surface defects based on
calibrationalgorithmusingcornerplanes,”in2022IEEEInternational deep learning,” Computers and Electronics in Agriculture, vol. 187, p.
Conference on Robotics and Biomimetics (ROBIO). IEEE, 2022, pp. 106230,2021.
235–240.
[22] Z.Xu,Y.Sun,L.Wang,andM.Liu,“Cp-loss:Connectivity-preserving
[11] F.Ma,Y.Liu,S.Wang,J.Wu,W.Qi,andM.Liu,“Self-superviseddriv-
lossforroadcurbdetectioninautonomousdrivingwithaerialimages,”
ableareasegmentationusinglidar’sdepthinformationforautonomous
in 2021 IEEE/RSJ International Conference on Intelligent Robots and
driving,” in 2023 IEEE/RSJ International Conference on Intelligent
Systems(IROS). IEEE,2021,pp.1117–1123.
RobotsandSystems(IROS). IEEE,2023,pp.41–48.
[12] S.O¨.Demir,T.E.Ertop,A.B.Koku,andE.˙I.Konukseven,“Anadaptive [23] Y. Jung, M. Jeon, C. Kim, S.-W. Seo, and S.-W. Kim, “Uncertainty-
awarefastcurbdetectionusingconvolutionalnetworksinpointclouds,”
approach for road boundary detection using 2d lidar sensor,” in 2017
in 2021 IEEE International Conference on Robotics and Automation
IEEE International Conference on Multisensor Fusion and Integration
(ICRA). IEEE,2021,pp.12882–12888.
forIntelligentSystems(MFI). IEEE,2017,pp.206–211.
[24] T. Suleymanov, L. Kunze, and P. Newman, “Online inference and
[13] E.Horva´th,C.Pozna,andM.Unger,“Real-timelidar-basedurbanroad
detectionofcurbsinpartiallyoccludedsceneswithsparselidar,”in2019
andsidewalkdetectionforautonomousvehicles,”Sensors,2021.
IEEE Intelligent Transportation Systems Conference (ITSC). IEEE,
[14] G. Wang, J. Wu, R. He, and B. Tian, “Speed and accuracy tradeoff
2019,pp.2693–2700.
for lidar data based road boundary detection,” IEEE/CAA Journal of
AutomaticaSinica,vol.8,no.6,pp.1210–1220,2020. [25] J. Gao, H. Jie, B. Xu, L. Liu, J. Hu, and W. Liu, “Lcdet: Lidar
curb detection network with transformer,” in 2023 International Joint
[15] T. Chen, B. Dai, D. Liu, J. Song, and Z. Liu, “Velodyne-based curb
detection up to 50 meters away,” in 2015 IEEE Intelligent Vehicles ConferenceonNeuralNetworks(IJCNN). IEEE,2023,pp.1–9.
Symposium(IV). IEEE,2015,pp.241–248. [26] J.L.Apella´niz,M.Garc´ıa,N.Aranjuelo,J.Barandiara´n,andM.Nieto,
[16] A. Y. Hata, F. S. Osorio, and D. F. Wolf, “Robust curb detection and “Lidar-based curb detection for ground truth annotation in automated
vehicle localization in urban environments,” in 2014 IEEE Intelligent drivingvalidation,”arXivpreprintarXiv:2312.00534,2023.
VehiclesSymposiumProceedings. IEEE,2014,pp.1257–1262. [27] H. Zhou, X. Zhu, X. Song, Y. Ma, Z. Wang, H. Li, and D. Lin,
[17] L.ZhouandG.Vosselman,“Mappingcurbstonesinairborneandmobile “Cylinder3d:Aneffective3dframeworkfordriving-scenelidarsemantic
laserscanningdata,”InternationalJournalofAppliedEarthObservation segmentation,”arXivpreprintarXiv:2008.01550,2020.
andGeoinformation,vol.18,pp.293–304,2012. [28] D.Zai,J.Li,Y.Guo,M.Cheng,Y.Lin,H.Luo,andC.Wang,“3-droad
[18] S.Xu,R.Wang,andH.Zheng,“Roadcurbextractionfrommobilelidar boundaryextractionfrommobilelaserscanningdataviasupervoxelsandJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 12
graph cuts,” IEEE Transactions on Intelligent Transportation Systems, [49] X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen, and
vol.19,no.3,pp.802–813,2017. B. Guo, “Cswin transformer: A general vision transformer backbone
[29] U. Ozertem and D. Erdogmus, “Locally defined principal curves and withcross-shapedwindows,”inProceedingsoftheIEEE/CVFConfer-
surfaces,”TheJournalofMachineLearningResearch,2011. ence on Computer Vision and Pattern Recognition, 2022, pp. 12124–
[30] H. Jie, J. Gao, Q. Zhao, Z. Ning, J. Hu, L. Liu, and W. Liu, “An 12134.
efficientcurbdetectionandtrackingmethodforintelligentvehiclesviaa [50] O¨. C¸ic¸ek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ron-
high-resolution3d-lidar,”in4thInternationalConferenceonInformation neberger, “3d u-net: learning dense volumetric segmentation from
Science, Electrical, and Automation Engineering (ISEAE 2022), vol. sparseannotation,”inMedicalImageComputingandComputer-Assisted
12257. SPIE,2022,pp.310–317. Intervention–MICCAI 2016: 19th International Conference, Athens,
[31] W.Yao,Z.Deng,andL.Zhou,“Roadcurbdetectionusing3dlidarand Greece,October17-21,2016,Proceedings,PartII19. Springer,2016,
integral laser points for intelligent vehicles,” in The 6th International pp.424–432.
Conference on Soft Computing and Intelligent Systems, and The 13th
International Symposium on Advanced Intelligence Systems. IEEE,
2012,pp.100–105.
VII. BIOGRAPHYSECTION
[32] A. Y. Hata and D. F. Wolf, “Feature detection for vehicle localization
inurbanenvironmentsusingamultilayerlidar,”IEEETransactionson
GuoyangZHAO(StudentMember,IEEE)iscur-
IntelligentTransportationSystems,vol.17,no.2,pp.420–429,2015. rentlypursuingaMasterofphilosophydegreeinthe
Intelligent Autonomous Driving Center, Thrust of
[33] B.Yang,L.Fang,andJ.Li,“Semi-automatedextractionanddelineation
RoboticsandAutonomousSystems,TheHongKong
of 3d roads of street scene from mobile laser scanning point clouds,”
UniversityofScienceandTechnology, Guangzhou,
ISPRS Journal of Photogrammetry and Remote Sensing, vol. 79, pp.
China. His research interests include computer vi-
80–93,2013.
sion,robotics,anddeeplearning.
[34] D.Bai,T.Cao,J.Guo,andB.Liu,“Howtobuildacurbdatasetwith
lidardataforautonomousdriving,”in2022InternationalConferenceon
RoboticsandAutomation(ICRA). IEEE,2022,pp.2576–2582.
[35] J. Liang, N. Homayounfar, W.-C. Ma, S. Wang, and R. Urtasun,
“Convolutional recurrent network for road boundary extraction,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and Fulong Ma (Member, IEEE) received the B.E
PatternRecognition,2019,pp.9512–9521. degree in automation from University of Science
[36] O.Ronneberger,P.Fischer,andT.Brox,“U-net:Convolutionalnetworks and Technology of China, Hefei, China, in 2018.
for biomedical image segmentation,” in Medical Image Computing He is currently pursuing the Ph.D degree with the
and Computer-Assisted Intervention–MICCAI 2015: 18th International Thrust of Robotics and Autonomous Systems, The
Conference,Munich,Germany,October5-9,2015,Proceedings,PartIII Hong Kong University of Science and Technology,
18. Springer,2015,pp.234–241. Guangzhou, China. His research interests include
[37] X. Pan, J. Shi, P. Luo, X. Wang, and X. Tang, “Spatial as deep: computervision,sensorcalibration,anddeeplearn-
Spatialcnnfortrafficsceneunderstanding,”inProceedingsoftheAAAI ing.
ConferenceonArtificialIntelligence,vol.32,no.1,2018.
[38] D.Kukolj,I.Marinovic´,andS.Nemet,“Roadedgedetectionbasedon
combined deep learning and spatial statistics of lidar data,” Journal of Yuxuan Liu (Student Member, IEEE) received
SpatialScience,vol.68,no.2,pp.245–259,2023. his Bachelor’s degree from Zhejiang University,
[39] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, Zhejiang, China in 2019, majoring in Mechatronic.
A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A He is now a Ph.D candidate at the Department of
multimodal dataset for autonomous driving,” in Proceedings of the Electronic and Computer Engineering, The Hong
IEEE/CVFconferenceoncomputervisionandpatternrecognition,2020, Kong University of Science and Technology, Hong
pp.11621–11631. Kong, China. His current research interests include
[40] A.Geiger,P.Lenz,C.Stiller,andR.Urtasun,“Visionmeetsrobotics: autonomous driving, deep learning, robotics, visual
The kitti dataset,” The International Journal of Robotics Research, 3Dobjectdetection,visualdepthprediction,etc..
vol.32,no.11,pp.1231–1237,2013.
[41] J.Behley,M.Garbade,A.Milioto,J.Quenzel,S.Behnke,C.Stachniss,
andJ.Gall,“Semantickitti:Adatasetforsemanticsceneunderstanding
Weiqing Qi received his Bachelor’s degree from
of lidar sequences,” in Proceedings of the IEEE/CVF international
UniversityofCalifornia,SantaBarbarain2021,ma-
conferenceoncomputervision,2019,pp.9297–9307.
joringinComputerScience.Heiscurrentlypursuing
[42] H. Zhang, H. Zhang, C. Wang, and J. Xie, “Co-occurrent features in
theMasterofphilosophydegreewiththeThrustof
semantic segmentation,” in Proceedings of the IEEE/CVF conference
RoboticsandAutonomousSystems,TheHongKong
oncomputervisionandpatternrecognition,2019,pp.548–557.
UniversityofScienceandTechnology,HongKong,
[43] W.Chen,X.Zhu,R.Sun,J.He,R.Li,X.Shen,andB.Yu,“Tensorlow-
China. His current research interests include lane
rank reconstruction for semantic segmentation,” in Computer Vision–
detection,drivableareasegmentation,andsemantics
ECCV2020:16thEuropeanConference,Glasgow,UK,August23–28,
segmentation,etc.
2020,Proceedings,PartXVII16. Springer,2020,pp.52–69.
[44] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++:
Redesigning skip connections to exploit multiscale features in image
segmentation,” IEEE transactions on medical imaging, vol. 39, no. 6, Ming Liu (Senior Member, IEEE) received the
pp.1856–1867,2019. B.A. degree in automation from Tongji University,
Shanghai,China,in2005,andthePh.D.degreefrom
[45] S. Jadon, “A survey of loss functions for semantic segmentation,” in
2020 IEEE conference on computational intelligence in bioinformatics the Department of Mechanical and Process Engi-
andcomputationalbiology(CIBCB). IEEE,2020,pp.1–7. neering,ETHZu¨rich,Zu¨rich,Switzerland,in2013.
During his master’s degree with Tongji University,
[46] E. Schubert, J. Sander, M. Ester, H. P. Kriegel, and X. Xu, “Dbscan
hestayedoneyearatErlangen-Nu¨nbergUniversity,
revisited, revisited: why and how you should (still) use dbscan,” ACM
Erlangen, Germany, and the Fraunhofer Institute
TransactionsonDatabaseSystems(TODS),2017.
IISB,Erlangen,asaMasterVisitingScholar.Heis
[47] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom,
currently an Associate Professor with the Depart-
“Pointpillars: Fast encoders for object detection from point clouds,”
ment of Electronic and Computer Engineering, the
in Proceedings of the IEEE/CVF conference on computer vision and
DepartmentofComputerScienceandEngineering,andtheChengKar-Shun
patternrecognition,2019,pp.12697–12705.
Robotics Institute, The Hong Kong University of Science and Technology,
[48] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and
Hong Kong. His research interests include dynamic environment modeling,
B.Guo,“Swintransformer:Hierarchicalvisiontransformerusingshifted
deeplearningforrobotics,3-Dmapping,machinelearning,andvisualcontrol..
windows,”inProceedingsoftheIEEE/CVFinternationalconferenceon
computervision,2021,pp.10012–10022.