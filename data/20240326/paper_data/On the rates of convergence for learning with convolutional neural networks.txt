On the rates of convergence for learning with convolutional
neural networks
Yunfei Yang ∗ Han Feng † Ding-Xuan Zhou ‡
Abstract
We study the approximation and learning capacities of convolutional neural networks
(CNNs). Our first result proves a new approximation bound for CNNs with certain
constraint on the weights. Our second result gives a new analysis on the covering number
of feed-forward neural networks, which include CNNs as special cases. The analysis
carefully takes into account the size of the weights and hence gives better bounds than
existing literature in some situations. Using these two results, we are able to derive rates
of convergence for estimators based on CNNs in many learning problems. In particular,
we establish minimax optimal convergence rates of the least squares based on CNNs for
learningsmoothfunctionsinthenonparametricregressionsetting. Forbinaryclassification,
we derive convergence rates for CNN classifiers with hinge loss and logistic loss. It is also
shown that the obtained rates are minimax optimal in several settings.
Keywords: convolutional neural network, convergence rate, regression, classifi-
cation, approximation, covering number
1 Introduction
Deep leaning has made remarkable successes in many applications and research fields such as
image classification, speech recognition, natural language processing and scientific computing
[LeCunetal.,2015;Goodfellowetal.,2016]. Thisbreakthroughofdeeplearningalsomotivated
many theoretical researches on understanding and explaining the empirical successes of deep
neuralnetworksfromvariousperspectives. Inparticular,recentstudieshaveestablishedoptimal
approximations of smooth function classes by fully connected neural networks [Yarotsky, 2017,
2018;Shenetal.,2020;Luetal.,2021]. Ithasalsobeenshowedthatthesenetworkscanachieve
minimax optimal rates of convergence in many learning problems, including nonparametric
regression [Schmidt-Hieber, 2020; Kohler and Langer, 2021] and classification [Kim et al.,
2021].
Our main interests in this paper are the approximation and learning properties of convo-
lutional neural networks (CNNs), which are widely used in image classification and related
applications [Krizhevsky et al., 2012]. Recently, substantial progress has been made in the
theoretical study of CNNs. It has been shown that CNNs are universal for approximation
∗Department of Mathematics, City University of Hong Kong, Kowloon, Hong Kong, China. (Corresponding
author, E-mail: yunfyang@cityu.edu.hk)
†Department of Mathematics, City University of Hong Kong, Kowloon, Hong Kong, China. (E-mail:
hanfeng@cityu.edu.hk)
‡School of Mathematics and Statistics, University of Sydney, Sydney, NSW 2006, Australia. (E-mail:
dingxuan.zhou@sydney.edu.au)
1
4202
raM
52
]GL.sc[
1v95461.3042:viXra[Zhou, 2020b] and universal consistent for nonparametric regression [Lin et al., 2022]. Approx-
imation bounds and representational advantages for CNNs have been proven in several works
[Oono and Suzuki, 2019; Zhou, 2020a; Fang et al., 2020; Mao et al., 2021]. Furthermore, rates
of convergence of estimators based on CNNs were established for nonparametric regression
[Zhou and Huo, 2024; Yang and Zhou, 2024] and classification [Kohler and Langer, 2020; Liu
et al., 2021; Feng et al., 2023]. However, in contrast with the minimax optimal learning rates
for fully connected neural networks, many of these results for CNNs are not optimal.
In this paper, we take a step to close this gap by providing a new analysis on the
approximation and learning capacities of CNNs. Specifically, we prove new bounds for the
approximation of smooth functions by CNNs with certain constraint on the weights. We also
derive bounds for the covering number of these networks. Using the obtained bounds, we are
able to establish convergence rates for CNNs in many learning problems. These rates are
known to be minimax optimal in several settings. We summarize the our contributions in the
following.
(1) We prove the bound O(L−α/d) for the approximation error of smooth functions with
smoothnessα < (d+3)/2byCNNs, wheredisthedimensionandListhedepthofCNNs.
The main advantage of our result is that we have an explicit control on the network
weights (through κ(θ) defined by (2.3)). It is proven that one can choose κ(θ) ≤ M with
M = O(L3d+ 23 d−2α ) to ensure that the approximation rate O(L−α/d) holds.
(2) We provide a new framework to estimate the covering number of feed-forward neural
networks. AnapplicationofthisresultgivestheboundO(Llog(LM/ϵ))fortheϵ-covering
number of CNNs with depth L and weight constraint κ(θ) ≤ M. When M grows at
most polynomially on L, our bound is better than the general bound O(L2log(L/ϵ)) in
the literature.
(3) For regression, we establish the minimax optimal rate for the least squares with CNNs,
when the regression function is smooth.
(4) For binary classification, we establish rates of convergence for CNN classifiers with hinge
loss and logistic loss, under the Tsybakov noise condition (4.3). For the hinge loss, the
obtained rate for the excess classification risk is minimax optimal. For the logistic loss,
the obtained rate may not be optimal for the excess classification risk. But it is optimal
for the excess logistic risk, at least in some situations.
The remainder of this paper is organized as follows. In Section 2, we describe the
architecture of convolutional neural networks used in this paper, and derive bounds for the
approximation capacity and covering number of these networks. Sections 3 and 4 study the
nonparametric regression and classification problems, respectively. We derive convergence
rates of the excess risk for CNNs in these two sections. Section 5 concludes this paper with a
discussion on future studies.
1.1 Notations
For i,j ∈ Z with i ≤ j, we use the notation [i : j] := {i,i+1,...,j}. When i = 1, we also
denote [j] := [1 : j] for convenience. We use the following conversion for tensors, where we
take the tensor x = (x ) ∈ Rm×n×r as an example. We use ∥x∥ to denote
i,j,k i∈[m],j∈[n],k∈[r] p
the p-norm of the tensor x by viewing it as a vector of Rmnr. The notation x denotes the
:,j,k
2tensor (x ) ∈ Rm, which is also viewed as a vector. We use x to denote the tensor
i,j,k i∈[m] :,j,:
(x ) ∈ Rm×r. Other notations, such as x and x , are similarly defined. If X
i,j,k i∈[m],k∈[r] i,:,k :,:,k
and Y are two quantities, we denote their maximal value by X ∨Y := max{X,Y}. We use
X ≲ Y or Y ≳ X to denote the statement that X ≤ CY for some constant C > 0. We also
denote X ≍ Y when X ≲ Y ≲ X. The notation X ≲ Poly(Y) means that X is smaller than
some polynomial of Y. For any function f : R → R, we will often extend its definition to Rd
by applying f coordinate-wisely. Throughout this paper, we assume that the dimension d ≥ 2
is a fixed integer.
2 Convolutional neural networks
Letusfirstdefineconvolutionalneuralnetworksusedinthispaper. Letw = (w ,...,w )⊺ ∈ Rs
1 s
be a filter with filter size s ∈ [d]. We define the convolution matrix T on Rd by
w
 
w ··· w w
1 s−1 s
 ... ... ... ... 
 
 
T w :=  

w 1 · w· 1· w ·s ·− ·1 ww s−s 1 

∈ Rd×d
 


... . .
.


w
1
This convolution matrix corresponds to the one-sided padding and stride-one convolution by
the filter w. It is essentially the same as the convolution used in [Oono and Suzuki, 2019]
(up to a matrix transpose). But it is different from the convolution matrix used in [Zhou,
2020a,b; Fang et al., 2020; Mao et al., 2021; Feng et al., 2023; Yang and Zhou, 2024], which is
of dimension (d+s)×d, rather than d×d. So, in their setting, the network width increases
after every application of the convolution, while the network width remains the same in our
setting. We define convolutional layers as follows. Let s,J,J′ ∈ N be a filter size, input
channel size, and output channel size. For a filter w = (w ) ∈ Rs×J′×J
i,j′,j i∈[s],j′∈[J′],j∈[J]
and a bias vector b = (b ,...,b )⊺ ∈ RJ′, we define the convolutional layer as an operator
1 J′
Conv : Rd×J → Rd×J′ by
w,b
J
(cid:88)
(Conv (x)) := T x +b , x = (x ) ∈ Rd×J.
w,b :,j′ w :,j′,j :,j j′ i,j i∈[d],j∈[J]
j=1
Next, we define convolutional neural networks. Let s ∈ [d] and J,L ∈ N be the filter size,
channel size and depth. We denote by CNN(s,J,L) the set of functions f that can be
θ
parameterized by θ = (w(0),b(0),...,w(L−1),b(L−1),w(L)) in the following form
(cid:68) (cid:69)
f (x) := w(L),σ◦ Conv ◦···◦σ◦ Conv (x) , x ∈ [0,1]d, (2.1)
θ w(L−1),b(L−1) w(0),b(0)
where w(0) ∈ Rs×J×1,b(0) ∈ RJ,w(L) ∈ Rd×J,w(ℓ) ∈ Rs×J×J,b(ℓ) ∈ RJ for ℓ ∈ [L−1], and
the activation σ(t) = t∨0 is the ReLU function. Note that we have assumed the channel
sizes in each layers are the same, because we can always increase the channel sizes by adding
appropriate zero filters and biases. For convenience, we will often view w(0) ∈ Rs×J×J
3by adding zeros to the filter and the input. The number of parameters in the network is
(sJ +1)JL+(d+s−sJ)J ≲ J2L, which grows linearly on the depth L.
In order to control the complexity of convolutional neural networks, we introduce the
following norm for the pair (w,b) ∈ Rs×J×J ×RJ
(cid:0)(cid:13) (cid:13) (cid:1)
∥(w,b)∥ := jm ′∈a [Jx
]
(cid:13)w :,j′,:(cid:13) 1+|b j′| .
Note that ∥(w,b)∥ quantifies the size of the affine transform Conv :
w,b
 
J
(cid:88)(cid:13) (cid:13)
∥Conv w,b(x)∥ ∞ ≤ jm ′∈a [Jx ] (cid:13) (cid:13)T w :,j′,jx :,j(cid:13) (cid:13) ∞+|b j′|
j=1
≤ ∥(w,b)∥(∥x∥ ∨1). (2.2)
∞
Following the idea of [Jiao et al., 2023], we define a constraint on the weights as follows
L−1
(cid:89) (cid:16) (cid:17)
κ(θ) := ∥w(L)∥ ∥(w(ℓ),b(ℓ))∥∨1 . (2.3)
1
ℓ=0
For any M ≥ 0, we denote the function class consisting of wights constrained CNNs by
CNN(s,J,L,M) := {f ∈ CNN(s,J,L) : κ(θ) ≤ M}.
θ
Several properties of this function class are summarized in Appendix A.1. In Sections 2.1
and 2.2, we study the approximation capacity and covering number of CNN(s,J,L,M).
These results are used in Sections 3 and 4 to study the convergence rates of CNNs on the
nonparametric regression and classification problems.
2.1 Approximation
We consider the capacity of CNNs for approximating smooth functions. Given a smoothness
index α > 0, we write α = r+β where r ∈ N := N∪{0} and β ∈ (0,1]. Let Cr,β(Rd) be the
0
H¨older space with the norm
(cid:26) (cid:27)
∥f∥ := max ∥f∥ , max |∂sf| ,
Cr,β(Rd) Cr(Rd) C0,β(Rd)
∥s∥1=r
where s = (s ,...,s ) ∈ Nd is a multi-index and
1 d 0
∥f∥ := max ∥∂sf∥ ,
Cr(Rd) L∞(Rd)
∥s∥1≤r
|f(x)−f(y)|
|f| := sup .
C0,β(Rd)
x̸=y∈Rd
∥x−y∥β
2
Here, we use ∥·∥ to denote the supremum norm since we only consider continuous functions.
L∞
We write Cr,β([0,1]d) for the Banach space of all restrictions to [0,1]d of functions in Cr,β(Rd).
The norm is given by ∥f∥ = inf{∥g∥ : g ∈ Cr,β(Rd) and g = f on [0,1]d}. For
Cr,β([0,1]d) Cr,β(Rd)
convenience, we will denote the ball of Cr,β([0,1]d) with radius R > 0 by
(cid:110) (cid:111)
Hα(R) := f ∈ Cr,β([0,1]d) : ∥f∥ ≤ R .
Cr,β([0,1]d)
Note that, for α = 1, H1(R) is a class of Lipschitz continuous functions.
Our first result estimates the error of approximating H¨older functions by CNNs.
4Theorem 2.1. Let 0 < α < (d+3)/2 and s ∈ [2 : d]. If L ≥ ⌈d−1⌉ and M ≳ L3d+ 23 d−2α , then
s−1
sup inf ∥h−f∥
L∞([0,1]d)
≲ L−α d.
h∈Hα(1)f∈CNN(s,6,L,M)
The approximation rate O(L−α/d) is slightly better than the rate O((L/logL)−α/d) in
[Oono and Suzuki, 2019, Corollary 4] for ResNet-type CNNs. Furthermore, the result of [Oono
and Suzuki, 2019] requires that the depth of residual blocks grow with the approximation
error, while our result does not need any residual blocks. Our approximation rate is the same
as the result of [Yang and Zhou, 2024], which used slightly different CNNs, and the rate in
[Feng et al., 2023], which considered the approximation of smooth functions on spheres. The
main advantage of our result is that Theorem 2.1 also provides an estimate on the weight
constraint M, which is useful for the study of statistical properties of CNNs (see Sections 3
and 4).
The proof of Theorem 2.1 is given in Appendix A.2. Similar to many existing works for
CNNs, such as [Oono and Suzuki, 2019; Zhou, 2020b], our proof uses the idea that smooth
functions are well approximated by fully-connected neural networks and one can construct
CNNs to implement fully-connected neural networks. Our result is based on the approximation
bound for shallow neural networks proven in [Yang and Zhou, 2024]. To introduce the idea,
let us denote the function class of shallow neural networks by
(cid:40) N N (cid:41)
(cid:88) ⊺ (cid:88)
NN(N,M) := f(x) = c σ(a x+b ) : |c |(∥a ∥ +|b |) ≤ M . (2.4)
i i i i i 1 i
i=1 i=1
It was shown by [Yang and Zhou, 2024, Corollary 2.4] that, if α < (d+3)/2, then
sup inf ∥h−f∥ L∞([0,1]d) ≲ N−α d ∨M− d+2 3α −2α. (2.5)
h∈Hα(1)f∈NN(N,M)
In Appendix A.2, we show that functions in NN(N,M) can also be parameterized by a CNN
with the same order of number of parameters. Specifically, for any s ∈ [2 : d] and L = ⌈d−1⌉,
0 s−1
we prove that
NN(N,M) ⊆ CNN(s,6,NL ,3L0+1NM). (2.6)
0
Theorem 2.1 is a direct consequence of the approximation bound (2.5) and the inclusion (2.6).
The requirement on the smoothness α < (d+3)/2 is of course due to the use of the approx-
imation bound (2.5). For high smoothness α > (d+3)/2, one can also derive approximation
bounds for CNNs by using the results of [Yang and Zhou, 2024], as discussed in the following
remark.
Remark 2.2. If α > (d+3)/2, it was shown by [Yang and Zhou, 2024, Theorem 2.1] that
Hα(1) ⊆ F (M) for some constant M > 0, where
σ
(cid:26) (cid:90) (cid:27)
⊺
F (M) := f (x) = σ((x ,1)v)dµ(v) : ∥µ∥ ≤ M . (2.7)
σ µ
Sd
Here, Sd is the unit sphere of Rd+1 and ∥µ∥ = |µ|(Sd) is the total variation of the measure µ.
F (M) is a ball with radius M of the variation space corresponding to shallow ReLU neural
σ
networks studied in many recent papers, such as [Bach, 2017; E et al., 2022; Siegel and Xu,
52022, 2023; Siegel, 2023]. The function class F (M) can be viewed as an infinitely wide neural
σ
network. It is the limit of NN(N,M) as the number of neurons N → ∞ [Yang and Zhou,
2024, Proposition 2.2]. The recent work [Siegel, 2023] showed that
sup inf ∥h−f∥
L∞([0,1]d)
≲ N−d 2+ d3 .
h∈Fσ(1)f∈NN(N,1)
Combining this bound with the inclusion (2.6), we can obtain
sup inf ∥h−f∥
L∞([0,1]d)
≲ L−d 2+ d3 , (2.8)
h∈Fσ(1)f∈CNN(s,6,L,M)
for M ≳ L. This approximation bound can be used to study machine learning problems with
smoothness assumption α > (d+3)/2, see Remark 3.3 for example. But it seems that the
bound (2.8) is not optimal for CNNs and high smoothness.
2.2 Covering number
In statistical learning theory, we often control the generalization error of learning algorithms
by certain complexities of the models. The complexity we use in this paper is the covering
number (or metric entropy) defined in the following.
Definition 2.3 (Covering number and entropy). Let ρ be a metric on M and F ⊆ M. For
ϵ > 0, a set S ⊆ M is called an ϵ-cover (or ϵ-net) of F if for any x ∈ F, there exists y ∈ S
such that ρ(x,y) ≤ ϵ. The ϵ-covering number of F is denoted by
N(ϵ,F,ρ) := min{|S| : S is an ϵ-cover of F},
where |S| is the cardinality of the set S. The logarithm of the covering number logN(ϵ,F,ρ)
is called (metric) entropy.
It is often the case that themetricρ is induced by a norm ∥·∥. In this case, we denote the ϵ-
covering number by N(ϵ,F,∥·∥) for convenience. We will mostly consider the covering number
of function classes F parameterized by neural networks in the normed space L∞([0,1]d). In the
following, we first give a general framework to estimate the covering number of feed-forward
neural networks and then apply the result to CNNs.
We consider neural networks of the following form
f (x) = x ∈ [0,1]d,
0
f (x) = σ(φ (f (x))), ℓ ∈ [0 : L−1], (2.9)
ℓ+1 θ ℓ
ℓ
f (x) = φ (f (x)),
θ θL L
where φ θ : Rd ℓ → Rd ℓ+1 is an affine map parameterized by a vector θ ℓ ∈ RN ℓ with d 0 = d,
ℓ
d = 1 and the vector of parameters θ := (θ⊺ ,...,θ⊺ )⊺ ∈ RN. Here, we use N = (cid:80)L N
L+1 0 L ℓ=0 ℓ
to denote the number of parameters in the network. Note that we have restricted the input
of the networks to [0,1]d for convenience. We assume that the parameterization satisfies the
following conditions: for any x,x′ ∈ Rd ℓ and ℓ ∈ [0 : L],
∥θ∥ ≤ B,
∞
∥φ (x)∥ ≤ γ (∥x∥ ∨1),
θ ∞ ℓ ∞
ℓ (2.10)
∥φ (x)−φ (x′)∥ ≤ γ ∥x−x′∥ ,
θ θ ∞ ℓ ∞
ℓ ℓ
∥φ (x)−φ (x)∥ ≤ λ ∥θ −θ′∥ (∥x∥ ∨1),
θ ℓ θ′ ℓ ∞ ℓ ℓ ℓ ∞ ∞
6where θ′ denotes any parameters satisfying ∥θ′∥ ≤ B.
∞
Note that, if the affine map have the following matrix form
(cid:18) (cid:19)
x
φ (x) = A x+b = (A ,b ) ,
θ ℓ θ ℓ θ ℓ θ ℓ θ ℓ 1
then we can choose γ to be the matrix operator norm (induced by ∥·∥ )
ℓ ∞
γ = ∥(A ,b )∥ ,
ℓ θ ℓ θ ℓ l∞→l∞
and choose λ to be the Lipschitz constant of the parameterization
ℓ
(cid:13) (cid:13)
(cid:13)(A ,b )−(A ,b )(cid:13) ≤ λ ∥θ −θ′∥ .
(cid:13) θ ℓ θ ℓ θ′ ℓ θ′ ℓ (cid:13) l∞→l∞ ℓ ℓ ℓ ∞
Recall that the matrix operator norm ∥(A,b)∥ is the maximal 1-norm of rows of the
l∞→l∞
matrix (A,b). In most constructions, (A ,b ) is linear on the parameter θ , which implies
θ θ ℓ
ℓ ℓ
that we can choose λ ≲ d +1.
ℓ ℓ
The next lemma estimates the covering number of the neural networks described above.
Lemma 2.4. Let F be the class of functions f that can be parameterized in the form (2.9),
θ
where the parameterization satisfies (2.10) with λ ≥ 0 and γ ≥ 1 for ℓ = [0 : L]. Then, the
ℓ ℓ
ϵ-covering number of F in the L∞([0,1]d) norm satisfies
N(ϵ,F,∥·∥ ) ≤ (C B/ϵ)N,
L∞([0,1]d) L
where N is the number of parameters and C can be computed inductively by
L
ℓ
(cid:89)
C = λ , C = γ C +λ γ .
0 0 ℓ+1 ℓ+1 ℓ ℓ+1 i
i=0
In particular,
 
L L
(cid:88) (cid:89)
C L ≤  λ j γ i.
j=0 i=0
Proof. For any θ,θ′ ∈ [−B,B]N with ∥θ−θ′∥ ≤ ϵ, we claim that, for any x ∈ [0,1]d and
∞
ℓ ∈ [0 : L],
ℓ−1
(cid:89)
∥f (x)∥ ≤ γ ,
ℓ ∞ i
i=−1
∥φ (f (x))−φ (f′(x))∥ ≤ C ϵ,
θ ℓ ℓ θ′ ℓ ℓ ∞ ℓ
 
ℓ ℓ
(cid:88) (cid:89)
C ℓ ≤  λ j γ i,
j=0 i=0
where we set γ = 1 and f′ denotes the function in (2.9) parameterized by θ′. Thus, any
−1 ℓ
ϵ-cover of [−B,B]N gives a C ϵ-cover of F in the L∞([0,1]d) norm. Since the ϵ-covering
L
number of [−B,B]N is at most (B/ϵ)N, we get the desire bound for N(ϵ,F,∥·∥ ).
L∞([0,1]d)
7We prove the claim by induction on ℓ ∈ [0 : L]. The claim is trivial for ℓ = 0 by definition.
Assume that the claim is true for some 0 ≤ ℓ < L, we are going to prove it for ℓ+1. By
induction hypothesis,
ℓ
(cid:89)
∥f (x)∥ ≤ ∥φ (f (x))∥ ≤ γ (∥f (x)∥ ∨1) ≤ γ ,
ℓ+1 ∞ θ ℓ ∞ ℓ ℓ ∞ i
ℓ
i=0
where we use γ ≥ 1 in the last inequality. By the Lipschitz continuity of ReLU,
i
∥f (x)−f′ (x)∥ ≤ ∥φ (f (x))−φ (f′(x))∥ ≤ C ϵ.
ℓ+1 ℓ+1 ∞ θ ℓ ℓ θ′ ℓ ℓ ∞ ℓ
Therefore,
∥φ (f (x))−φ (f′ (x))∥
θ ℓ+1 ℓ+1 θ′ ℓ+1 ℓ+1 ∞
≤∥φ (f (x))−φ (f (x))∥ +∥φ (f (x))−φ (f′ (x))∥
θ ℓ+1 ℓ+1 θ′ ℓ+1 ℓ+1 ∞ θ′ ℓ+1 ℓ+1 θ′ ℓ+1 ℓ+1 ∞
≤λ ϵ(∥f (x)∥ ∨1)+γ ∥f (x)−f′ (x)∥
ℓ+1 ℓ+1 ∞ ℓ+1 ℓ+1 ℓ+1 ∞
(cid:32) ℓ (cid:33)
(cid:89)
≤ λ γ +γ C ϵ = C ϵ.
ℓ+1 i ℓ+1 ℓ ℓ+1
i=0
Finally, by induction hypothesis,
ℓ
(cid:89)
C = γ C +λ γ
ℓ+1 ℓ+1 ℓ ℓ+1 i
i=0
 
ℓ ℓ+1 ℓ
(cid:88) (cid:89) (cid:89)
≤  λ j γ i+λ ℓ+1 γ i
j=0 i=0 i=0
 
ℓ+1 ℓ+1
(cid:88) (cid:89)
≤  λ j γ i,
j=0 i=0
which completes the proof.
Now, we apply Lemma 2.4 to the convolutional neural network CNN(s,J,L,M). In this
case, φ = Conv for ℓ ∈ [0 : L−1] and φ (·) = ⟨w(L),·⟩. By Proposition A.2 in the
θ
ℓ
w(ℓ),b(ℓ) θL
Appendix, we can assume ∥w(L)∥ ≤ M and ∥w(ℓ),b(ℓ))∥ ≤ 1 for all ℓ ∈ [0 : L−1], which
1
implies B = M ∨1. Using the inequality (2.2) and
(cid:13)
(cid:13)Conv w,b(x)− Conv
w,b(x′)(cid:13)
(cid:13)
∞
 
J
(cid:88)(cid:13) (cid:13)
≤ jm ′∈a [Jx ] (cid:13) (cid:13)T w :,j′,jx :,j −T w :,j′,jx′ :,j(cid:13) (cid:13) ∞
j=1
≤∥(w,b)∥∥x−x′∥ ,
∞
we can set γ = 1 and γ = M. It is easy to see that we can choose λ = sJ +1 and λ = dJ.
ℓ L ℓ L
Consequently,
 
L L
(cid:88) (cid:89)
C L ≤  λ j γ i = (dJ +sJL+L)M ≤ 3dJLM,
j=0 i=0
where we use s ≤ d in the last inequality. We summarize the result in the next theorem.
8Theorem 2.5. Let s,J,L ∈ N and M ≥ 1. The entropy of CNN(s,J,L,M) satisfies
logN(ϵ,CNN(s,J,L,M),∥·∥ ) ≤ N log(3dJLM2/ϵ),
L∞([0,1]d)
where N = (sJ +1)JL+(d+s−sJ)J is the number of parameters in the network.
In the analysis of neural networks, many papers, such as [Schmidt-Hieber, 2020; Feng
et al., 2023], simply assume that the parameters in the networks are bounded. In this case,
the entropy is often bounded as O(NLlog(N/ϵ)), where N is the number of parameters and
L is the depth. For convolutional neural networks with bounded width, we have N ≍ L
and hence the entropy is O(L2log(L/ϵ)). For comparison, Theorem 2.5 gives the bound
O(Llog(LM/ϵ)). If one only assumes that the parameters are bounded by B, then M ≲ BL
and our bound is consistent with the previous bound. However, if the weight constraint M
grows at most polynomially on L, then we get a better bound O(Llog(L/ϵ)) on the entropy.
This improvement is essential to obtain optimal rates for many learning algorithms that we
discuss in next two sections.
3 Regression
In this section, we consider the classical nonparametric regression problem. Assume that
(X,Y) is a [0,1]d×R-valued random vector satisfying E[Y2] < ∞. Let us denote the marginal
distribution of X by µ and the regression function by
h(x) := E[Y|X = x].
Suppose we are given a data set of n samples D = {(X ,Y )}n , which are independent
n i i i=1
and have the same distribution as the random vector (X,Y). The goal of nonparametric
regression problem is to construct an estimator f(cid:98)n, based on D n, to reconstruct the regression
function h. The estimation performance is evaluated by the L2-error
(cid:104) (cid:105)
∥f(cid:98)n−h∥2
L2(µ)
= E
X
(f(cid:98)n(X)−h(X))2 .
One of the popular algorithms to solve the regression problem is the empirical least squares
n
1 (cid:88)
f(cid:98)n ∈ argmin (f(X i)−Y i)2, (3.1)
n
f∈Fn
i=1
where F is a prescribed hypothesis class. For simplicity, we assume here and in the sequel
n
that the minimum above indeed exists. We are interested in the case that the function class
F
n
is parameterized by a CNN. In order to study the convergence rate of f(cid:98)n → h as n → ∞,
we will assume that h ∈ Hα(R) for some constant R > 0 and make the following assumption
on the distribution of (X,Y): there exists a constant c > 0 such that
E(cid:2) exp(cY2)(cid:3)
< ∞. (3.2)
In the statistical analysis of learning algorithms, we often require that the hypothesis class
is uniformly bounded. We define the truncation operator π with level B > 0 for real-valued
B
functions f as

B f(x) > B,


π f(x) = f(x) |f(x)| ≤ B, (3.3)
B

−B f(x) < −B.
9NotethatthetruncationoperatorcanbeimplementedbyaCNN(seeLemmaA.6forexample).
Since we assume that the regression function h is bounded, truncating the output of the
estimator f(cid:98)n appropriately dose not increase the estimation error. The following theorem
provides convergence rates for least squares estimators based on CNNs.
Theorem 3.1. Assume that the condition (3.2) holds and the regression function h ∈ Hα(R)
for some 0 < α < (d + 3)/2 and R > 0. Let f(cid:98)n be the estimator defined by (3.1) with
F = CNN(s,J,L ,M ), where s ∈ [2 : d], J ≥ 6 and
n n n
(cid:18) (cid:19) d (cid:18) (cid:19)3d+3−2α
n 2α+d n 4α+2d
L ≍ , ≲ M ≲ Poly(n).
n log3n log3n n
If B = c logn for some constant c > 0, then
n 1 1
(cid:104) (cid:105)
(cid:18) log3n(cid:19) 2α2α
+d
E
Dn
∥π Bnf(cid:98)n−h∥2
L2(µ)
≲
n
.
It is well-known that the rate n− 2α2α +d is minimax optimal for learning functions in Hα(R)
[Stone, 1982]:
(cid:104) (cid:105)
inf sup E
Dn
∥f(cid:98)n−h∥2
L2(µ)
≳ n− 2α2α +d,
f(cid:98)n h∈Hα(R)
where the infimum is taken over all estimators based on the training data D . Recent works
n
have established the minimax rates (up to logarithm factors) for least squares estimators using
fully-connected neural networks [Schmidt-Hieber, 2020; Kohler and Langer, 2021; Yang and
Zhou, 2023]. For convolutional neural networks, [Oono and Suzuki, 2019] proved the optimal
rates for ResNet-type CNNs, under the requirement that the depth of the residual blocks
grows with the sample size n, or the residual blocks are suitably masked. Theorem 3.1 removes
the requirements on the residual blocks for low smoothness α < (d+3)/2.
Our proof of Theorem 3.1 is based on the following lemma from [Kohler and Langer,
2021, Lemma 18, Appendix B]. It decomposes the estimation error of the estimator into
generalization error and approximation error, and bounds the generalization error by the
covering number of the hypothesis class F .
n
Lemma 3.2. Assume that the condition (3.2) holds. Let f(cid:98)n be the estimator (3.1) and set
B = c logn for some constant c > 0. Then,
n 1 1
(cid:104) (cid:105)
E
Dn
∥π Bnf(cid:98)n−h∥2
L2(µ)
c (logn)2sup log(N(n−1B−1,π F ,∥·∥ )+1)
≤ 2 X1:n n Bn n L1(X1:n) +2 inf ∥f −h∥2 ,
n f∈Fn L2(µ)
for n > 1 and some constant c
2
> 0 (independent of n and f(cid:98)n), where X
1:n
= (X 1,...,X n)
denotes a sequence of sample points in [0,1]d and N(ϵ,π F ,∥·∥ ) is the ϵ-covering
Bn n L1(X1:n)
number of the function class π F := {π f,f ∈ F } in the metric ∥f − g∥ =
Bn n Bn n L1(X1:n)
1 (cid:80)n |f(X )−g(X )|.
n i=1 i i
We can give a proof of Theorem 3.1 by using Theorem 2.1 to bound the approximation
error and using Theorem 2.5 to estimate the covering number.
10Proof of Theorem 3.1. It is easy to see that N(ϵ,F ,∥·∥ ) ≤ N(ϵ,F ,∥·∥ ).
n L1(X1:n) n L∞([0,1]d)
Notice that the projection π does not increase the covering number. By Theorem 2.5, we
B
have
logN(ϵ,π F ,∥·∥ ) ≲ L log(L M /ϵ).
Bn n L∞([0,1]d) n n n
3d+3−2α
If M ≳ L 2d , by Theorem 2.1, we get
n n
inf ∥f −h∥2 ≲
L−2 dα
.
L2(µ) n
f∈Fn
As a consequence, Lemma 3.2 implies
E
Dn(cid:104)
∥π Bnf(cid:98)n−h∥2
L2(µ)(cid:105)
≲
log n2n
L nlog(nL nM nB n)+L
n−2 dα
≲
log3n
L
+L−2 dα
,
n n
n
where we use L ,M ≲ Poly(n) and B = clogn in the last inequality. Finally, by choosing
n n n
L ≍ (n/log3n)d/(2α+d), we finishes the proof.
n
Remark 3.3. As we noted in Remark 2.2, if α > (d+3)/2, then Hα(1) ⊆ F (R) for some
σ
constant R > 0. When the regression function h ∈ F (R), we can use the approximation
σ
bound (2.8) to show that
(cid:104) (cid:105)
(cid:18) log3n(cid:19) 2d d+ +3
3
E
Dn
∥π Bnf(cid:98)n−h∥2
L2(µ)
≲
n
,
if we choose L ≍ (n/log3n)d/(2d+3) ≲ M ≲ Poly(n). This rate is minimax optimal (up to
n n
logarithm factors) for the function class F (R) [Yang and Zhou, 2024]. For comparison, [Yang
σ
and Zhou, 2024] only established the sub-optimal rate O(n− 3d d+ +3 3 log4n) for CNNs. Our result
is also better than the recent analysis of CNNs in [Zhou and Huo, 2024], which proved the
rate O(n−1/3log2n) for Hα(R) with α > (d+4)/2.
4 Binary classification
In binary classification, we observe a dataset D := {(X ,Y ) : i = 1,...,n} of n i.i.d. copies
n i i
of a random vector (X,Y), where we assume that the input vector X ∈ [0,1]d and the label
Y ∈ {−1,1}. The marginal distribution of X is denoted by P and the conditional class
X
probability function is denoted by
η(x) := P(Y = 1|X = x).
Forareal-valuedfunctionf definedon[0,1]d, wecandefineaclassifierC (x) := sgn(f(x)).
f
The classification error of f is defined as
E(f) = E [C (X) ̸= Y] = E [1(Yf(X) < 0)],
X,Y f X,Y
where 1(·) is 1 if (·) is true, and is 0 otherwise. A Bayes classifier C∗ = C is a classifier that
f∗
minimizes the classification error E(f∗) = min E(f), where M is the set of all measurable
f∈M
11functionson[0,1]d. NotethatC∗ = sgn(2η−1)isaBayesclassifierandE(C∗) = 1E[1−|2η−1|].
2
The goal of binary classification is to construct a classifier with small classification error by
using the dataset D .
n
Since we only have finite observed samples, one natural approach to estimate the Bayes
classifier is the empirical risk minimization (with 0−1 loss)
n
1 (cid:88)
argmin 1(Y f(X ) < 0), (4.1)
i i
n
f∈Fn
i=1
where F is a prescribed function class. However, this procedure is computational infeasible
n
due to the NP-hardness of the minimization problem. In general, one replace the 0−1 loss by
surrogate losses. For a given surrogate loss function ϕ : R → [0,∞), the ϕ-risk is defined as
L (f) := E [ϕ(Yf(X))]
ϕ X,Y
= E [η(X)ϕ(f(X))+(1−η(X))ϕ(−f(X))].
X
Its minimizer is denoted by f∗ ∈ argmin L (f). Note that f∗ can be explicitly computed
ϕ f∈M ϕ ϕ
by using the conditional class probability function η for many convex loss functions ϕ [Zhang,
2004]. Instead of using (4.1), we can estimate the Bayes classifier by minimizing the empirical
ϕ-risk over a function class F :
n
n
1 (cid:88)
f(cid:98)ϕ,n ∈ argmin ϕ(Y if(X i)). (4.2)
n
f∈Fn
i=1
The goal of this section is to estimate the convergence rates of the excess classification risk
and excess ϕ-risk defined by
R(f(cid:98)ϕ,n) := E(f(cid:98)ϕ,n)−E(C∗),
R ϕ(f(cid:98)ϕ,n) := L ϕ(f(cid:98)ϕ,n)−L ϕ(f ϕ∗),
when F is parameterized by a CNN. The convergence rates certainly depend on the property
n
of the conditional class probability function η. One of the well known assumption on η is
the Tsybakov noise condition [Mammen and Tsybakov, 1999; Tsybakov, 2004]: there exist
q ∈ [0,∞] and c > 0 such that for any t > 0,
q
P (|2η(X)−1| ≤ t) ≤ c tq. (4.3)
X q
The constant q is usually called the noise exponent. It is obvious that the Tsybakov noise
condition always holds for q = 0, whereas noise exponent q = ∞ means that η is bounded
away from the critical level 1/2. We will consider classifications with hinge loss and logistic
loss under the Tsybakov noise condition.
4.1 Hinge loss
For the hinge loss ϕ(t) = max{1 − t,0}, we have f∗ = sgn(2η − 1) = C∗ and L (f∗) =
ϕ ϕ ϕ
E[1−|2η−1|]. It is well known that the following calibration inequality holds [Zhang, 2004;
Bartlett et al., 2006]
R(f) ≤ R (f). (4.4)
ϕ
12Hence, any convergence rate for the excess ϕ-risk R ϕ(f(cid:98)ϕ,n) implies the same convergence rate
for the excess classification risk R(f(cid:98)ϕ,n). One can also check that [Zhang, 2004, Section 3.3], if
|f| ≤ 1, then
R (f) = E[|f −f∗||2η−1|]. (4.5)
ϕ ϕ
Tousethisequality,itisnaturaltotruncatetheoutputoftheestimatorbyusingthetruncation
operator π defined by (3.3).
1
In the following theorem, we provide convergence rates for the excess ϕ-risk of the CNN
classifier with hinge loss, under the assumption that the conditional class probability function
η is smooth and satisfies the Tsybakov noise condition.
Theorem 4.1. Assume the noise condition (4.3) holds for some q ∈ [0,∞] and η ∈ Hα(R)
for some 0 < α < (d+3)/2 and R > 0. Let ϕ be the hinge loss and f(cid:98)ϕ,n be the estimator
defined by (4.2) with F = {π f : f ∈ CNN(s,J,L ,M )}, where s ∈ [2 : d], J ≥ 6 and
n 1 n n
(cid:18) (cid:19) d (cid:18) (cid:19) 3d+3
n (q+2)α+d n 2(q+2)α+2d
L ≍ , ≲ M ≲ Poly(n),
n log2n log2n n
then, for sufficiently large n,
(cid:104) (cid:105)
(cid:18) log2n(cid:19) (q( +q+ 2)1 α)α
+d
E
Dn
R ϕ(f(cid:98)ϕ,n) ≲
n
.
Audibert and Tsybakov [2007] showed that the minimax lower bound for the excess
classification risk is
infsupE Dn(cid:104) R(f(cid:98)n)(cid:105) ≳ n− (q( +q+ 2)1 α)α +d, (4.6)
f(cid:98)n η
where the supremum is taken over all η ∈ Hα(R) that satisfies Tsybakov noise condition
(4.3) and the infimum is taken over all estimators based on the training data D . Hence, by
n
the calibration inequality (4.4), the convergence rate in Theorem 4.1 is minimax optimal up
to a logarithmic factor. Similar results have been established in [Kim et al., 2021] for fully
connected neural networks with hinge loss. However, their results rely on the sparsity of neural
networks and hence one need to optimize over different network architectures to obtain the
optimal rate. Our result show that CNNs, whose architecture is specifically defined, are able
to achieve the optimal rate.
4.2 Logistic loss
For the logistic loss ϕ(t) = log(1+e−t), we have f∗ = log( η ) and L (f∗) = E[−ηlogη−
ϕ 1−η ϕ ϕ
(1−η)log(1−η)]. Consequently, one can show that
(cid:104) (cid:16) (cid:17) (cid:16) (cid:17)(cid:105)
R (f) = E ηlog η(1+e−f) +(1−η)log (1−η)(1+ef) .
ϕ
Let us denote the KL-divergence by
(cid:18) (cid:19) (cid:18) (cid:19)
p 1−p
D (p,q) := plog +(1−p)log , p,q ∈ [0,1],
KL
q 1−q
13where D (p,q) = ∞ if q = 0 and p ̸= 0, or q = 1 and p ̸= 1. If we define the logistic function
KL
by
1
ψ(t) := ∈ [0,1], t ∈ [−∞,∞], (4.7)
1+e−t
then a direct calculation shows that η = ψ(f∗) and
ϕ
R (f) = E[D (η,ψ(f))]. (4.8)
ϕ KL
When the Tsybakov noise condition (4.3) holds, we have the following calibration inequality
[Steinwart and Christmann, 2008, Theorem 8.29]
1 q+1
R(f) ≤ 4c qq+2R ϕ(f)q+2. (4.9)
Forthelogisticloss, theconvergenceratedependsnotonlyontheTsybakovnoisecondition,
but also upon the Small Value Bound (SVB) condition introduced by [Bos and Schmidt-Hieber,
2022]. We say the distribution of (X,Y) satisfies the SVB condition, if there exists β ≥ 0 and
C > 0 such that for any t ∈ (0,1],
β
P (η(X) ≤ t) ≤ C tβ, P (1−η(X) ≤ t) ≤ C tβ. (4.10)
X β X β
Note that this condition always holds for β = 0 with C = 1. The index β is completely
β
determined by the behavior of η near 0 and 1. If η is bounded away form 0 and 1, then the
SVB condition holds for all β > 0. In contrast, the Tsybakov noise condition provides a
control on the behavior of η near the decision boundary {x : η(x) = 1/2}. This difference is
due to the loss: the 0−1 loss only cares about the classification error, while the logistic loss
measures how well the conditional class probability is estimated in the KL-divergence (4.8),
which puts additional emphasis on small and large conditional class probabilities.
The following theorem gives convergence rates for CNNs under the SVB condition. As
pointed out by [Bos and Schmidt-Hieber, 2022], we do not get any gain in the convergence
rate when the SVB index β > 1. So, we assume β ∈ [0,1] in the theorem.
Theorem 4.2. Assume the SVB condition (4.10) holds for some β ∈ [0,1] and η ∈ Hα(R)
for some 0 < α < (d+3)/2 and R > 0. Let ϕ be the logistic loss and f(cid:98)ϕ,n be the estimator
defined by (4.2) with F = {π f : f ∈ CNN(s,J,L ,M )}, where s ∈ [2 : d], J ≥ 6 and
n Bn n n
(cid:18) (cid:19) d (cid:18) (cid:19) 3d+3+2α
n (1+β)α+d n 2(1+β)α+2d
L ≍ , ≲ M ≲ Poly(n), B ≍ logn,
n n n
logn logn
then, for sufficiently large n,
(cid:18) (cid:19) (1+β)α
(cid:104) (cid:105) logn (1+β)α+d
E
Dn
R ϕ(f(cid:98)ϕ,n) ≲
n
logn.
The convergence rate in Theorem 4.2 is the same as [Bos and Schmidt-Hieber, 2022, Theo-
rem 3.3], which studied multi-class classification using fully-connected deep neural networks
with cross entropy loss. If, in addition, the Tsybakov noise condition (4.3) holds, by combining
Theorem 4.2 with the calibration inequality (4.9), we can get the following convergence rate
for the classification risk:
E Dn[R(f(cid:98)ϕ,n)] ≲
n−q q+ +1 2(1( +1+ β)β α)α
+d log2n.
14Note that this rate is the same as the optimal rate (4.6) up to a logarithmic factor, when
q = 0 and β = 1. But, for β = 0, the obtained rate is not minimax optimal for the excess
classification risk. However, as shown by [Zhang et al., 2023, Corollary 2.1], the rate in
Theorem 4.2 is indeed minimax optimal up to a logarithmic factor for the excess ϕ-risk when
β = 0. So, even if the logistic classification can achieve the minimax optimal convergence rates
for classification, it is in general not possible to prove it through the rates for excess ϕ-risk.
There are other papers [Kohler and Langer, 2020; Liu et al., 2021] studying the convergence
rates of CNNs with logistic loss. Kohler and Langer [2020] imposed a max-pooling structure
assumption for the conditional class probability that is related to the structure of convolutional
networks. So, their result is not comparable to ours. Liu et al. [2021] used a similar setting as
− α
ours and derived the rate n 2α+2(α∨d) (ignoring logarithmic factors) for the excess ϕ-risk under
the assumption that η is supported on a manifold of d dimension. Our rate n− αα +d in Theorem
4.2 (for β = 0) is better, but their result can be applied to low-dimensional distributions. It
would be interesting to generalize our result to these distributions.
4.3 Sketch of proofs
Our proofs of Theorems 4.1 and 4.2 are based on the following lemma, which is summarized
from [Kim et al., 2021, Appendix A.2].
Lemma 4.3. Let ϕ be a surrogate loss function and {F n} n∈N be a sequence of function
classes. Assume that the random vector (X,Y) ∈ [0,1]d×{−1,1} and the following regularity
conditions hold:
(A1) ϕ is Lipschitz, i.e., there exists a constant c > 0 such that |ϕ(t )−ϕ(t )| ≤ c |t −t |
1 1 2 1 1 2
for any t ,t ∈ R.
1 2
(A2) For a positive sequence {a n} n∈N, there exists f
n
∈ F
n
such that
R (f ) = L (f )−L (f∗) ≤ a .
ϕ n ϕ n ϕ ϕ n
(A3) There exists a sequence {B n} n∈N with B n ≳ 1 such that
sup ∥f∥ ≤ B .
L∞([0,1]d) n
f∈Fn
(A4) There exists a constant ν ∈ (0,1] such that, for any f ∈ F ,
n
E (cid:2) (ϕ(Yf(X))−ϕ(Yf∗(X)))2(cid:3) ≤ c B2−νR (f)ν,
X,Y ϕ 2 n ϕ
where c > 0 is a constant depending only on ϕ and the conditional class probability
2
function η.
(A5) There exist a sequence {δ n} n∈N and a constant c 3 > 0 such that
(cid:18)
δ
(cid:19)2−ν
n
logN(δ ,F ,∥·∥ ) ≤ c n .
n n L∞([0,1]d) 3
B
n
Let ϵ
n
≍ a n∨δ
n
and f(cid:98)ϕ,n be the empirical ϕ-risk minimizer (4.2) over the function class F n.
Then,
P(R ϕ(f(cid:98)ϕ,n) ≥ ϵ n) ≲ exp(−c 4n(ϵ n/B n)2−ν),
for some constant c > 0. In particular, if n(ϵ /B )2−ν ≳ (logn)1+r for some r > 0, then
4 n n
(cid:104) (cid:105)
E
Dn
R ϕ(f(cid:98)ϕ,n) ≲ ϵ n.
15Lemma 4.3 provides a systematic way to derive convergence rates of the excess ϕ-risk
for general surrogate losses. For the hinge loss ϕ(t) = max{1−t,0} and the logistic loss
ϕ(t) = log(1+e−t), the condition (A1) is satisfied with c = 1. When the function class F is
1 n
parameterized by a convolutional neural network, the covering number bound in the condition
(A5) can be estimated by using Theorem 2.5. Note that a in the condition (A2) quantifies
n
how well f∗ can be approximated by F in the ϕ-loss.
ϕ n
For the hinge loss, we know that f∗ = sgn(2η−1). Since f∗ is bounded, we can simply
ϕ ϕ
chooseB = 1inthecondition(A3). Thevarianceboundin(A4)wasestablishedby[Steinwart
n
and Scovel, 2007, Lemma 6.1]. We summarize their result in the following lemma. It shows
that we can choose ν = q/(q+1) in the condition (A4) of Lemma 4.3.
Lemma 4.4. Assume the noise condition (4.3) holds for some q ∈ [0,∞]. Let ϕ be the hinge
loss. For any f : [0,1]d → R satisfying ∥f∥ ≤ B, it holds that
L∞([0,1]d)
E
X,Y
(cid:2) (ϕ(Yf(X))−ϕ(Yf ϕ∗(X)))2(cid:3) ≤ c η,q(B+1)q q+ +2 1R ϕ(f)q+q 1,
where c > 0 is a constant depending on η and q.
η,q
Under the assumption that η ∈ Hα(R), we can use Theorem 2.1 to construct a CNN h
that approximates 2η −1. We can then approximate f∗ = sgn(2η −1) by a CNN of the
ϕ
form g◦h, where g is a piece-wise linear function that approximates the sign function. The
approximation error can be estimated by using the expression (4.5). Under the noise condition
−(q+1)α/d
(4.3), we proves that one can choose a ≍ L in the condition (A2). Furthermore,
n n
q+1
Theorem 2.5 shows that we can choose δ
n
≍ (L nn−1logn)q+2 in (A5). The trade-off between
a and δ tells us how to choose the depth L and gives the desired rate in Theorem 4.1.
n n n
For the logistic loss, the convergence rate in Theorem 4.2 can be derived in a similar
manner. The following lemma shows that one can choose ν = 1 in the condition (A4) of
Lemma 4.3.
Lemma 4.5. Let ϕ be the logistic loss. For any f : [0,1]d → R satisfying ∥f∥ ≤ B
L∞([0,1]d)
with B ≥ 2, it holds that
E (cid:2) (ϕ(Yf(X))−ϕ(Yf∗(X)))2(cid:3) ≤ 3BR (f).
X,Y ϕ ϕ
Proof. Since η(X) = P(Y = 1|X),
E (cid:2) (ϕ(Yf(X))−ϕ(Yf∗(X)))2(cid:3)
X,Y ϕ
(cid:34) (cid:32) (cid:33)(cid:35)
1+e−Yf(X)
=E log2
X,Y 1+e−Yf ϕ∗(X)
(cid:34) (cid:32) (cid:33) (cid:32) (cid:33)(cid:35)
1+e−f(X) 1+ef(X)
=E η(X)log2 +(1−η(X))log2
X 1+e−f ϕ∗(X) 1+ef ϕ∗(X)
(cid:20) (cid:18) (cid:19) (cid:18) (cid:19)(cid:21)
η(X) 1−η(X)
=E η(X)log2 +(1−η(X))log2 ,
X
ψ(f(X)) 1−ψ(f(X))
where ψ is the logistic function defined by (4.7) and we use f∗ = log( η ) in the last equality.
ϕ 1−η
Since |f(X)| ≤ B with B ≥ 2, we have ψ(f(X)) ∈ [ψ(−B),ψ(B)] = [ψ(−B),1−ψ(−B)] with
16ψ(−B) ≤ e−2. We can apply the following inequalities proven in Proposition A.8:
(cid:18) (cid:19) (cid:18) (cid:18) (cid:19) (cid:19)
η η
ηlog2 ≤ 2log(1+eB) ηlog −η+ψ(f) ,
ψ(f) ψ(f)
(cid:18) (cid:19) (cid:18) (cid:18) (cid:19) (cid:19)
1−η 1−η
(1−η)log2 ≤ 2log(1+eB) (1−η)log +η−ψ(f) ,
1−ψ(f) 1−ψ(f)
where we omit the variable X. Thus,
E (cid:2) (ϕ(Yf(X))−ϕ(Yf∗(X)))2(cid:3)
X,Y ϕ
(cid:20) (cid:18) (cid:19) (cid:18) (cid:19)(cid:21)
η 1−η
≤2log(1+eB)E ηlog +(1−η)log
ψ(f) 1−ψ(f)
≤3BR (f),
ϕ
where we use the equality (4.8) and log(1+eB) ≤ 3B/2.
Recall that, for the logistic loss, f∗ = log( η ) and η = ψ(f∗), where ψ is defined by
ϕ 1−η ϕ
(4.7). Since η ∈ Hα(R), it can be approximated by a CNN h by using Theorem 2.1. We
further construct a CNN f = g◦h, where g(t) is a function that approximates the mapping
t (cid:55)→ logt−log(1−t), such that ψ(f) approximates η well. By equality (4.8), the excess
ϕ-risk R (f) = E[D (η,ψ(f))] can be estimated by using the following lemma, which is a
ϕ KL
modification from [Bos and Schmidt-Hieber, 2022, Theorem 3.2].
Lemma 4.6. Let u ∈ (0,1/2) and suppose the SVB condition (4.10) holds for some β ∈ [0,1].
If the function h : [0,1]d → [u,1−u] satisfies ∥h−η∥ ≤ Cu for some constant C > 0,
L∞([0,1]d)
then
(cid:40) 2(2−β)C β(C+1)2+β u1+β, β < 1,
E [D (η(X),h(X))] ≤ 1−β
X KL
2C (C +1)3u2log(u−1), β = 1.
1
−(1+β)α/d
Using Lemma 4.6, we proves that one can choose a ≍ L logn and B ≍ logn
n n n
in conditions (A2) and (A3) of Lemma 4.3. Since ν = 1, Theorem 2.5 shows that we can
choose δ ≍ L B n−1logn in (A5). The trade-off between a and δ tells us how to choose
n n n n n
the depth L and gives the rate in Theorem 4.2.
n
5 Conclusion
In this paper, we have studied the approximation and learning capacities of convolutional
neural networks. We have derived new approximation bounds for CNNs with norm constraint
on the weights. To study the generalization performance of these networks, we also proved new
bounds for their covering number. Based on these results, we established rates of convergence
for CNNs in nonparametric regression and classification problems. Many of the obtained
convergence rates are known to be minimax optimal.
Thereisarestrictiononthesmoothnessofthetargetfunctionsinourresults. Wethinkthis
restriction is due to the proof techniques of our approximation bound (Theorem 2.1), rather
than the architecture of CNNs. It may be possible to use the ideas of network constructions
from related works, such as [Oono and Suzuki, 2019], to remove the restriction, which we leave
as a future work.
17A Appendix
A.1 Basic properties of CNNs
In this section, we give some properties of the function class CNN(s,J,L,M), which will be
useful for neural network construction.
PropositionA.1. IfJ ≤ J′, L ≤ L′ andM ≤ M′, thenCNN(s,J,L,M) ⊆ CNN(s,J′,L′,M′).
Proof. It is easy to check that CNN(s,J,L,M) ⊆ CNN(s,J′,L,M′) by adding zero filters
appropriatelyandusingthedefinitionofweightconstraint(2.3). ToproveCNN(s,J′,L,M′) ⊆
CNN(s,J′,L′,M′), we observe that the convolution with the filter (1,0,...,0)⊺ ∈ Rs is the
identity map of Rd → Rd. Hence, we can increase the depth of CNN by adding the identity
map in the last layer.
The next proposition shows that we can always rescale the parameters in CNNs so that
the norms of filters in the hidden layers are at most one.
Proposition A.2 (Rescaling). Every f ∈ CNN(s,J,L,M) can be parameterized in the form
(2.1) such that ∥w(L)∥ ≤ M and ∥(w(ℓ),b(ℓ))∥ ≤ 1 for all ℓ ∈ [0 : L−1].
1
Proof. The proof is essentially the same as [Jiao et al., 2023, Proposition 2.4]. We give the
proof here for completeness. Note that f ∈ CNN(s,J,L,M) parameterized in the form (2.1)
can be written inductively by
(cid:16) (cid:17)
f(x) = ⟨w(L),f (x)⟩, f (x) = σ Conv (f (x)) , f (x) = x.
L ℓ+1 w(ℓ),b(ℓ) ℓ 0
Let us denote m := max{∥(w(ℓ),b(ℓ))∥,1} for all ℓ ∈ [0 : L − 1] and let w = w /m ,
ℓ (cid:101)(ℓ) ℓ ℓ
(cid:101)b
ℓ
= b ℓ/((cid:81)ℓ i=0m i) and w
(cid:101)(L)
= w(L)(cid:81)L i=− 01m i. We consider the functions defined inductively
by
(cid:16) (cid:17)
f(cid:101) (x) = σ Conv (f(cid:101) (x)) , f(cid:101) (x) = x.
ℓ+1
w
(cid:101)(ℓ),(cid:101)b(ℓ) ℓ 0
It is easy to check that ∥w ∥ ≤ M and
(cid:101)(L)
(cid:13)(cid:32) (cid:33)(cid:13)
(cid:13)
(cid:13) (cid:13)(w
(cid:101)(ℓ),(cid:101)b(ℓ) )(cid:13)
(cid:13)
(cid:13)
=
m1
ℓ
(cid:13)
(cid:13)
(cid:13)
(cid:13)
w(ℓ),
(cid:81)ℓ
ib =−( 01ℓ)
m i
(cid:13)
(cid:13)
(cid:13)
(cid:13)
≤
m1
ℓ
(cid:13)
(cid:13)
(cid:13)(w(ℓ),b(ℓ))(cid:13)
(cid:13)
(cid:13)
≤ 1,
where the first inequality is due to m ≥ 1.
i
(cid:16) (cid:17)
Next, we show that f ℓ(x) = (cid:81)ℓ i=− 01m
i
f(cid:101) ℓ(x) by induction. For ℓ = 1, by the absolute
homogeneity of the ReLU function,
(cid:16) (cid:17)
f (x) = σ Conv (x)
1 w(0),b(0)
(cid:16) (cid:17)
= m 0σ Conv
w
(cid:101)(0),(cid:101)b(0)(x) = m 0f(cid:101) 1(x).
18Inductively, one can conclude that
(cid:16) (cid:17)
f (x) = σ Conv (f (x))
ℓ+1 w(ℓ),b(ℓ) ℓ
(cid:32) ℓ (cid:33) (cid:32) (cid:32) (cid:33)(cid:33)
(cid:89) f (x)
= m σ Conv ℓ
i=0
i w (cid:101)(ℓ),(cid:101)b(ℓ) (cid:81)ℓ i=− 01m
i
(cid:32) ℓ (cid:33)
(cid:89) (cid:16) (cid:16) (cid:17)(cid:17)
= m
i
σ Conv
w
(cid:101)(ℓ),(cid:101)b(ℓ)
f(cid:101) ℓ(x)
i=0
(cid:32) ℓ (cid:33)
(cid:89)
= m
i
f(cid:101) ℓ+1(x),
i=0
where the third equality is due to induction. Therefore,
(cid:42) (cid:32)L−1 (cid:33) (cid:43)
(cid:89)
f(x) = ⟨w(L),f L(x)⟩ = w(L), m
i
f(cid:101) L(x) = ⟨w (cid:101)(L),f(cid:101) L(x)⟩,
i=0
which means f can be parameterized by (w (cid:101)(0),(cid:101)b(0) ,...,w (cid:101)(L−1),(cid:101)b(L−1) ,w (cid:101)(L)) and we finish
the proof.
A.2 Proof of Theorem 2.1
⊺
We first construct a CNN to implement the function of the form x (cid:55)→ cσ(a x+b).
Lemma A.3. Let s ∈ [2 : d] and L = ⌈d−1⌉. For any a ∈ Rd and b,c ∈ R, there exists
s−1
f ∈ CNN(s,3,L,M) such that f(x) = cσ(a⊺ x+b) for x ∈ [0,1]d and M = 3L−1|c|(∥a∥ +|b|).
1
Furthermore, the output weights w(L) ∈ Rd×3 can be chosen to satisfy w(L) = 0 except for
i,j
i = j = 1.
Proof. By the homogeneity of ReLU, we can assume that ∥a∥ + |b| = 1. Observe that
1
convolution with the filter (u ,...,u )⊺ ∈ Rs computes the inner product with the first s
1 s
elements of the input signal (cid:80) u x . Convolution with the filter (0,...,0,1)⊺ ∈ Rs is the
i∈[s] i i
“left-translation” by s−1. We construct the CNN parameterized in the form (2.1) as follows.
We define w(0) ∈ Rs×3×1 and b(0) ∈ R3 by
 
  0
a
1 .
w :( ,0 1) ,1 = −w :( ,0 2) ,1 =   . . .  , w :( ,0 3) ,1 =   . .  , b(0) = 0.
0
a
s 1
Then, the pre-activated output of the first layer, i.e. Conv (x), is
w(0),b(0)
(cid:80) (cid:80) 
a x − a x x
i∈[s] i i i∈[s] i i s
 . . 
  ∗ ∗ .   ∈ Rd×3,
. .
 . . 
 . . x d
∗ ∗ ∗
19whereweuse∗todenotethevaluesthatwedonotcare. Byusingtheequalityt = σ(t)−σ(−t),
(cid:80)
weareabletocomputethepartialinnerproduct a x afterapplyingtheReLUactivation.
i∈[s] i i
Since we assume x ∈ [0,1]d, the x = σ(x ), i ∈ [s : d], are stored in the output of the first
i i
layer. For ℓ ∈ [L−2], we define w(ℓ) ∈ Rs×3×3 and b(ℓ) ∈ R3 by
   
1 −1 0 0 0 0
w(ℓ) = −w(ℓ) =  0 . 0 . a ℓ(s− .1)+2  , w(ℓ) =  . . . . . . . . . , b(ℓ) = 0.
:,1,: :,2,: . . .  :,3,:  
. . .  0 0 0
0 0 a 0 0 1
(ℓ+1)(s−1)+1
Then, the pre-activated output of the ℓ+1-th layer is
(cid:80) (cid:80) 
a x − a x x
i∈[(ℓ+1)(s−1)+1] i i i∈[(ℓ+1)(s−1)+1] i i (ℓ+1)(s−1)+1
 . . 
  ∗ ∗ .   ∈ Rd×3.
. .
 . . 
 . . x d 
∗ ∗ ∗
For ℓ = L−1, we let b(L−1) = (b,0,0)⊺ . The filter w(L−1) ∈ Rs×3×3 and the pre-activated
output o ∈ Rd×3 are given below
L
 
1 −1 0
 ⊺ 
a x+b 0 0
0 0 a 
(L−1)(s−1)+2
 . . .   ∗ 0 0
w :( ,L 1,− :1) =    . . .
.
. . .
.
. .    , w :( ,L 2,− :1) = w :( ,ℓ 3) ,: = 0, o L =    . . . . . . . . .  .
. . a d  ∗ 0 0
0 0 0
Finally, fortheoutputweightsw(L) ∈ Rd×3, weletw(L) = candw(L) = 0otherwise. Then, the
1,1 i,j
outputoftheCNNisexactlycσ(a⊺ x+b). Itiseasytoseethat∥(w(0),b(0))∥ ≤ 1,∥w(L)∥ = |c|,
1
∥(w(ℓ),b(ℓ))∥ ≤ 3 for ℓ ∈ [L−1]. Hence, for this parameterization, κ(θ) ≤ 3L−1|c|.
Next, we show that shallow neural networks defined by (2.4) can be parameterized by a
CNN.
Lemma A.4. Let s ∈ [2 : d] and L = ⌈d−1⌉. Then, for any f ∈ NN(N,M), there exists
0 s−1
f
θ
∈ CNN(s,6,NL 0,3L0+1NM) such that f θ(x) = f(x) for all x ∈ [0,1]d.
Proof. Given a parameter R > 0, which will be chosen later, any function f ∈ NN(N,M)
can be written as
N
M (cid:88) ⊺
f(x) = c σ(a x+b ),
R i i i
i=1
where
(cid:80)N
|c |(∥a ∥ +|b |) ≤ R. By Lemma A.3, the function x (cid:55)→ c
σ(a⊺
x+b ) can be
i=1 i i 1 i i i i
implemented by CNN(s,3,L 0,3L0−1R), where L
0
= ⌈d s−− 11⌉. We denote the corresponding
parameters by (w(0)(i),b(0)(i),...,w(L0−1)(i),b(L0−1)(i),w(L0)(i)), where w(L0) (i) = 0 except
j,k
for j = k = 1. By Proposition A.2, we can further assume that |w(L0) (i)| ≤ 3L0−1R and
1,1
∥(w(ℓ)(i),b(ℓ)(i))∥ ≤ 1 for all ℓ ∈ [0 : L −1].
0
20(cid:80) ⊺
In order to compute the summation c σ(a x + b ) in a sequential way, we use the
i i i
4th channel in the CNN to store the input x ∈ [0,1]d, and the 5th and 6th channels to
(cid:80) ⊺
store the partial summations of the positive part c σ(a x+b ) and the negative part
(cid:80) −c σ(a⊺ x+b ), respectively. The filters w(ℓ)
c ∈i> R0 s×i 6×6i
and
bii
as b(ℓ) ∈ R6 are defined
ci<0 i i i
as follows. For ℓ = 0,
w(0) = w(0)(1), w(0) = (1,0,...,0)⊺ , b(0) = (b(0)(1)⊺ ,0,0,0)⊺ .
:,1:3,1 :,4,1
Here and in the sequel, we use zero filters and biases when they are not specific defined.
(ℓ) (ℓ) (ℓ) ⊺
We always use the filter w = w = w = (1,0,...,0) to store the input and partial
:,4,4 :,5,5 :,6,6
summations, except for the output layer. If ℓ = (i−1)L +j for some i ∈ [N] and j ∈ [L −1],
0 0
then we define
w(ℓ) = w(j)(i), b(ℓ) = (b(j)(i)⊺ ,0,0,0)⊺ .
:,1:3,1:3
Recall that the only nonzero element in w(L0)(i) is w 1(L ,10) (i). For ℓ = iL
0
for some i ∈ [N −1],
we define
w(ℓ) = w(0)(i+1), b(ℓ) = (b(0)(i+1)⊺ ,0,0,0)⊺ ,
:,1:3,4
and
w(ℓ)
=
(w(L0)
(i),0,···
,0)⊺
, if c > 0,
:,5,1 1,1 i
w(ℓ)
=
(−w(L0)
(i),0,···
,0)⊺
, if c < 0.
:,6,1 1,1 i
Then, the activated output of the iL layer is of the form
0

c
σ(a⊺
x+b
)/w(L0)
(i) ∗ ∗ x
(cid:80)
c
σ(a⊺
x+b )
(cid:80)
−c
σ(a⊺
x+b
)
i i i 1,1 1 j j j j j j
 cj>0,j<i cj<0,j<i 
 . ,
 . 
∗ ∗ ∗ . ∗ ∗
 
∗ ∗ ∗ x ∗ ∗
d
where ∗ denotes the values that we do not care. It is easy to check that we correctly compute
(cid:80) ⊺ (cid:80) ⊺
the partial summations c σ(a x+b ) and − c σ(a x+b ) in the iL +1
cj>0,j≤i j j j cj>0,j≤i j j j 0
layer. Finally, for the output layer, i.e. ℓ = NL , we define
0
 w(L0)
(N) 0 0 0 1
−1
M 1,1
w(NL0) =  0 0 0 0 0 0 .
R  . . . . . . 
. . . . . .
. . . . . .
Then, the constructed CNN f (x) implements the function f(x).
θ
In our construction, ∥(w(ℓ),b(ℓ))∥ = 1 + |w 1(L ,10) (i)| ≤ 1 + 3L0−1R if ℓ = iL
0
for some
i ∈ [N −1] and ∥(w(ℓ),b(ℓ))∥ ≤ 1 for other ℓ ∈ [0 : NL
0
−1], and ∥w(NL0)∥
1
= MR−1(2+
|w(L0) (N)|) ≤ MR−1(2+3L0−1R). Therefore, the norm constraint of the parameter is
1,1
M
κ(θ) ≤ (2+3L0−1R)(1+3L0−1R)N−1.
R
If we choose R = 31−L0N−1, then
2M
κ(θ) ≤ (1+3L0−1R)N ≤ 3L0+1NM,
R
where we use (1+1/N)N ≤ e ≤ 3.
21We give a remark on the construction in the proof of Lemma A.4. This remark is useful
for constructing CNNs to approximate functions of the form g(f (x)) with g : R → R.
θ
Remark A.5. We can replace the output layer (i.e. the parameters w(NL0)) of f by
θ
a convolutional layer with parameters w(NL0) = −w(NL0) = w(NL0) and w(NL0) = 0 for
:,1,: :,2,: :,i,:
i = 3,4,5,6. Then, the activated output of this CNN (without linear layer) is
 
σ(f(x)) σ(−f(x)) 0 0 0 0
. . . .
  ∗ ∗ . . . . . . . . . (A.1)
∗ ∗ 0 0 0 0
Thus, we can recover f(x) by using σ(f(x))−σ(−f(x)). The norm of this CNN can also be
bounded by 3L0+1NM.
Now, we can give a proof of Theorem 2.1 by using Lemma A.4.
Proof of Theorem 2.1. Let L = ⌈d−1⌉ and N = ⌊L/L ⌋. By Lemma A.4 and Proposition
0 s−1 0
A.1, we have the inclusion NN(N,M 0) ⊆ CNN(s,6,L,M) for M
0
= 3−L0−1N−1M. If
M ≳ L3d+ 23 d−2α , then M
0
≳ Ld+3 2− d2α and, by the approximation bound (2.5),
sup inf ∥h−f∥ L∞([0,1]d) ≲ N−α d ∨M 0− d+2 3α −2α ≲ L−α d,
h∈Hα(1)f∈CNN(s,6,L,M)
which completes the proof.
A.3 Proof of Theorem 4.1
Recall that, for the hing loss, f∗ = sgn(2η −1). In order to estimate the approximation
ϕ
bound (A2) in Lemma 4.3, we need to construct a CNN to approximate sgn(2η−1). If η
is smooth, we can approximate 2η −1 by a CNN f using Theorem 2.1. The sign function
can be approximated by a piece-wise linear function g. Hence, we need to construct a CNN
to implement the composition g ◦f. The following lemma give such a construction. This
lemma can be seen as an extension of Lemma A.4. Recall that we use NN(N,M) to denote
the function class of shallow neural networks (2.4). We will use the notation NN (N,M) to
d
emphasize that the input dimension is d.
Lemma A.6. Let s ∈ [2 : d], L = ⌈d−1⌉ and f ∈ NN (N,M). If g ∈ NN (K,M ), then
0 s−1 d 1 0
there exists f
θ
∈ CNN(s,6,NL
0
+K +1,36·3L0NMKM 0) such that f θ(x) = g(f(x)) for
all x ∈ [0,1]d.
Proof. Using the construction in the proof of Lemma A.4 and Remark A.5, we can construct
NL convolutional layers such that the activated output is given by (A.1) and the norm of
0
these layers is at most 3L0+1NM. Thus, we only need to implement the one-dimensional
function g by a d-dimensional CNN.
Without loss of generality, for any R > 0, we can normalize g such that
K K
2M 0 (cid:88) 1 (cid:88)
g(t) = c σ(a t+b ), |a |+|b | = , |c | ≤ R.
k k k k k k
R 2
k=1 k=1
22The construction is similar to Lemma A.4. We use 2nd and 3rd channels channels to store
the input information σ(f(x)) and σ(−f(x)). The 4th and 5th channels are used to store
(cid:80)
the partial summations of the positive part c σ(a f(x)+b ) and the negative part
c >0 k k k
(cid:80) k
−c σ(a f(x) + b ), respectively. These can be done in a way similar to the proof
c <0 k k k
k
of Lemma A.4. So, we only give the bias and the filter for the first channel: b(NL0+k) =
⊺
(b ,0,0,0,0) ,
k
 
0 a −a 0 0
k k
w(NL0+k) = 0 0 0 0 0.
:,1,: . . . . .
. . . . .
. . . . .
Then, the activated output of the NL +k-th layer is
0
(cid:32) (cid:80) (cid:80) (cid:33)
σ(a f(x)+b ) σ(f(x)) σ(−f(x)) c σ(a f(x)+b ) −c σ(a f(x)+b )
k k i i i i i i
ci>0,i<k ci<0,i<k ,
∗ ∗ ∗ ∗ ∗
where, as before, ∗ denotes the values that we do not care. Finally, the output layer is
given by w(L) ∈ Rd×5 with L = NL +K +1 and nonzero entries w(L) = 2M c /R and
0 1,1 0 K
(L) (L)
w = −w = 2M /R.
1,4 1,5 0
The norm of the construed CNN is
K−1
κ(θ) ≤ 3L0+1NM · 2M 0 (2+|c |) (cid:89) (1+|c |)
K k
R
k=1
(1+R)K
≤ 12·3L0NMM
0
R
≤ 36·3L0NMKM ,
0
where we choose R = 1/K and use (1+1/K)K ≤ e ≤ 3 in the last inequality.
We are ready to prove Theorem 4.1 by using Lemma 4.3.
Proof of Theorem 4.1. Since η ∈ Hα(R) by assumption, we have 2η−1 ∈ Hα(2R+1). For
a given u
n
∈ (0,1), by the approximation bound (2.5), there exists h ∈ NN(N(cid:101)n,M(cid:102)n) with
N(cid:101)n ≍ u− nd/α and M(cid:102)n ≳ u− n(d+3−2α)/(2α) such that
∥2η−1−h∥ ≤ u . (A.2)
L∞([0,1]d) n
Recall that f∗ = sgn(2η−1) for the hing loss. We define
ϕ

1 t ≥ u ,
 n

g(t) = t/u |t| < u ,
n n

−1 t ≤ −u ,
n
which can also be written as
g(t) = u−1σ(t+u )−u−1σ(t−u )−σ(0t+1).
n n n n
Since J ≥ 6 and 2u−1(1+u )+1 ≲ u−1, by Lemma A.6, there exists f ∈ CNN(s,J,L ,M )
n n n n n
with L
n
≍ N(cid:101)n ≍ u− nd/α and M
n
≍ N(cid:101)nM(cid:102)nu− n1 ≳ u− n(3d+3)/(2α) such that f = g ◦h on [0,1]d.
Notice that π f = f by construction, which implies f ∈ F .
1 n
23Let us denote Ω := {x ∈ [0,1]d : |2η(x)−1| ≥ 2u }, then |h(x)| ≥ u on Ω by (A.2).
n n n n
As a consequence, f∗(x) = sgn(2η(x)−1) = g(h(x)) = f(x) for any x ∈ Ω . Using equality
ϕ n
(4.5), we get
R (f) = E[|f −f∗||2η−1|]
ϕ ϕ
(cid:90)
= |f(x)−f∗(x)||2η(x)−1|dP (x)
ϕ X
[0,1]d\Ωn
≤ 4u P (|2η(X)−1| ≤ 2u ) ≲ uq+1.
n X n n
Thus, we have shown that one can choose a ≍ uq+1 and B = 1 in conditions (A2) and (A3)
n n n
of Lemma 4.3. By Lemma 4.4, we can choose ν = q/(q+1) in condition (A4) of Lemma 4.3.
To select δ in condition (A5) of Lemma 4.3, we apply Theorem 2.5 to estimate the entropy of
n
F (note that π does not increase the entropy):
n 1
logN(δ ,F ,∥·∥ ) ≲ L log(L M2/δ )
n n L∞([0,1]d) n n n n
≲ u−d/αlog(nu−1δ−1),
n n n
where we use M ≲ Poly(n) in the last inequality. This bound shows that we can choose
n
δ n ≍ (n−1u− nd/α logn)q q+ +1 2 under the assumption that u− n1 ≲ Poly(n). Finally, if we set
(cid:18) log2n(cid:19) (q+2α
)α+d
u ≍ ,
n
n
then ϵ ≍ a ∨δ ≍ a = uq+1 satisfies the condition n(ϵ /B )2−ν ≳ log2n in Lemma 4.3,
n n n n n n n
and hence we have
(cid:104) (cid:105)
(cid:18) log2n(cid:19) (q( +q+ 2)1 α)α
+d
E
Dn
R ϕ(f(cid:98)ϕ,n) ≲ ϵ
n
≲
n
,
which completes the proof.
A.4 Proof of Theorem 4.2
Recall that we use NN (N,M) to denote the function class of shallow neural networks (2.4)
1
with one dimensional input.
Lemma A.7. For any integer N ≥ 3, there exists g ∈ NN (2N,6N) such that the following
1
conditions hold
(1) |g(t)| ≤ logN for all t ∈ R.
(2) g(t) = −logN for t ≤ 0, and g(t) = logN for t ≥ 1.
(3) For any t ∈ [0,1], |ψ(g(t))−t| ≤ 3N−1, where ψ is the logistic function (4.7).
Proof. Let us first construct a function h that approximates the logarithm function on [0,1].
For convenience, we denote t = i/N for i ∈ [0 : N]. We let h(t) = logt = −logN for t ≤ t
i 1 1
and h(t) = 0 for t ≥ t = 1. In the interval [t ,t ], we let h be the continuous piecewise
N 1 N
linear function with N −1 pieces such that h(t ) = logt for i ∈ [1 : N]. The function h can
i i
be written as
N−1
(cid:88)
h(t) = −logN +k σ(t−t )−k σ(t−t )+ (k −k )σ(t−t ),
1 1 N−1 N i i−1 i
i=2
24where k = N(logt −logt ) ≤ t−1 is the slope of h on the interval (t ,t ). Let us consider
i i+1 i i i i+1
the approximation error f(t) = logt−h(t). By construction, f(t) ≥ 0 on [t ,t ] and f(t ) = 0
1 N i
for i ∈ [1 : N]. Observe that |f′(t)−f′(x)| = |t−1 −x−1| ≤ t−2|t−x| for t,x ∈ [t ,t ],
i i i+1
i ∈ [1 : N −1]. It is well known that such a Lipschitz gradient property implies the following
inequality [Nesterov, 2018, Lemma 1.2.3]
1
|f(x)−f(t)−f′(t)(x−t)| ≤ |x−t|2, x,t ∈ [t ,t ]. (A.3)
2t2 i i+1
i
If the function f(t) attains its maximal value on [t ,t ] at some point t∗ ∈ (t ,t ), then
i i+1 i i i+1
f′(t∗) = 0 and hence, by choosing x = t and t = t∗ in (A.3), we get
i i i
1 1
f(t∗) ≤ |t −t∗|2 ≤ .
i 2t2 i i 2t2N2
i i
As a consequence, for t ∈ [t ,t ] with i ∈ [1 : N −1], we have
i i+1
t + 1
|eh(t)−t| = t(1−e−f(t)) ≤ tf(t) ≤ i N
2t2N2
i
1 1 1
= + ≤ ,
2t N2 2t2N3 N
i i
where we use t N = i ≥ 1 in the last inequality. Observe that, for t ∈ [0,t ], we have
i 1
|eh(t)−t| = |t −t| ≤ N−1. We conclude that |eh(t)−t| ≤ N−1 holds for any t ∈ [0,1].
1
Next, we define g(t) = h(t)−h(1−t), then g ∈ NN (2N,M), where
1
N−1
(cid:88)
M ≤ 3k +3k +3 (k −k )
1 N−1 i−1 i
i=2
= 6k ≤ 6t−1 = 6N.
1 1
It is easy to check that conditions (1) and (2) hold. For t ∈ [0,1],
(cid:12) (cid:12)
(cid:12) eh(t) (cid:12)
|ψ(g(t))−t| = (cid:12) −t(cid:12)
(cid:12)eh(t)+eh(1−t) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) eh(t) (cid:12)
≤ (cid:12)eh(t)−t(cid:12)+(cid:12) −eh(t)(cid:12)
(cid:12) (cid:12) (cid:12)eh(t)+eh(1−t) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) eh(t) (cid:12) (cid:12)
= (cid:12)eh(t)−t(cid:12)+ (cid:12)t−eh(t)+1−t−eh(1−t)(cid:12)
(cid:12) (cid:12) eh(t)+eh(1−t) (cid:12) (cid:12)
≤ 3N−1,
which proves condition (3).
Proof of Theorem 4.2. As in the proof of Theorem 4.1, for a given u ∈ (0,1/3), there exists
n
h ∈ NN(N(cid:101)n,M(cid:102)n) with N(cid:101)n ≍ u− nd/α and M(cid:102)n ≳ u− n(d+3−2α)/(2α) such that
∥η−h∥ ≤ u .
L∞([0,1]d) n
25Let g ∈ NN (2⌈u−1⌉,6⌈u−1⌉) be a function that satisfies Lemma A.7 with N = ⌈u−1⌉. Thus,
1 n n n
|g(t)| ≤ log⌈u−1⌉ for all t ∈ R, and
n
|ψ(g(t))−t| ≤ 3u t ∈ [0,1].
n
Since J ≥ 6, by Lemma A.6, there exists f ∈ CNN(s,J,L n,M n) with L
n
≍ N(cid:101)n +⌈u− n1⌉ ≍
u− nd/α and M
n
≍ N(cid:101)nM(cid:102)n⌈u− n1⌉2 ≳ u− n(3d+3+2α)/(2α) such that f = g ◦h on [0,1]d. If we let
B = log⌈u−1⌉, then π f = f by construction, which implies f ∈ F .
n n Bn n
Define the function (cid:101)h : [0,1]d → [0,1] by

0, h(x) < 0,


(cid:101)h(x) = h(x), h(x) ∈ [0,1],

1, h(x) > 1.
By the second condition in Lemma A.7 for the function g, we have f = g◦h = g◦(cid:101)h. Since
η(x) ∈ [0,1] for any x ∈ [0,1]d, we also have |(cid:101)h(x)−η(x)| ≤ |h(x)−η(x)| ≤ u n. Therefore,
|ψ(f(x))−η(x)| ≤ |ψ(g((cid:101)h(x)))−(cid:101)h(x)|+|(cid:101)h(x)−η(x)|
≤ 3u +u = 4u .
n n n
Since ψ(f(x)) ∈ [ψ(−B ),ψ(B )] ⊆ [u /2,1−u /2], Lemma 4.6 implies that
n n n n
R (f) = E[D (η,ψ(f))] ≲ u1+βlogu−1.
ϕ KL n n
Thus, we have shown that one can choose a ≍ u1+βlogu−1 and B = log⌈u−1⌉ in conditions
n n n n n
(A2) and (A3) of Lemma 4.3. By Lemma 4.5, we can choose ν = 1 in condition (A4) of Lemma
4.3. Using Theorem 2.5, we can estimate the entropy of F as
n
logN(δ ,F ,∥·∥ ) ≲ L log(L M2/δ )
n n L∞([0,1]d) n n n n
≲ u−d/αlog(nu−1δ−1).
n n n
where we use M ≲ Poly(n) in the last inequality. This bound shows that we can choose
n
δ ≍ n−1u−d/α B logn in condition (A5) of Lemma 4.3, under the assumption that u−1 ≲
n n n n
Poly(n). Finally, if we set
(cid:18) (cid:19) α
logn (1+β)α+d
u ≍ ,
n
n
then ϵ ≍ a ∨δ ≍ a ≍ δ satisfies the condition nϵ /B ≳ log2n in Lemma 4.3, and hence
n n n n n n n
we have
(cid:18) (cid:19) (1+β)α
(cid:104) (cid:105) logn (1+β)α+d
E R ϕ(f(cid:98)ϕ,n) ≲ ϵ
n
≲ logn,
n
which completes the proof.
A.5 Proof of Lemma 4.6
Using the inequality logt ≤ t−1 for t > 0, we have for any x ∈ [0,1]d,
(cid:18) (cid:19) (cid:18) (cid:19)
η(x) 1−η(x)
D (η(x),h(x)) ≤ η(x) −1 +(1−η(x)) −1
KL
h(x) 1−h(x)
(η(x)−h(x))2 C2u2 C2u2
= ≤ + . (A.4)
h(x)(1−h(x)) h(x) 1−h(x)
26By assumption, we have h(x) ≥ u and h(x) ≥ η(x)−Cu. Notice that, if η(x)−Cu ≥ u, then
Cη(x) η(x)
η(x)−Cu ≥ η(x)− = .
C +1 C +1
Therefore,
1 1 C +1
≤ 1 + 1 .
h(x) u {η(x)<(C+1)u} η(x) {η(x)≥(C+1)u}
By the SVB condition (4.10), we get
E [h(X)−1] ≤ C (C +1)βuβ−1+(C +1)I ,
X β u
where
(cid:90)
1
I : = dP (x)
u X
η(x)
{η(x)≥(C+1)u}
(cid:90) ∞ (cid:18) 1 (cid:19)
= P 1 ≥ t dt
X η(X) {η(X)≥(C+1)u}
0
(cid:90) 1 (cid:18) (cid:19)
≤ (C+1)u P η(X) ≤ 1 dt.
X
t
0
If β < 1, then by the SVB condition (4.10),
I ≤ C (cid:90) (C+1 1)u t−βdt = C β(C +1)β−1 uβ−1.
u β
1−β
0
If β = 1, the SVB condition (4.10) implies that C ≥ 1 and P (η(X) ≤ t−1) ≤ min{1,C t−1},
β X β
which leads to
(cid:90) C (cid:90) 1
I ≤ β 1dt+C (C+1)u t−1dt ≤ C (1+log(u−1)).
u β β
0 C
β
Thus, we have given a bound for E [h(X)−1]. Similarly, one can show that the same bound
X
holds for E [(1−h(X))−1]. Combining these bounds with (A.4) finishes the proof.
X
A.6 A useful inequality
Proposition A.8. If p ∈ [0,1] and q ∈ [u,1] with 0 < u ≤ e−2, then
plog2(p/q) ≤ log(u−2)(plog(p/q)−p+q).
Proof. It is easy to check that the inequality holds for p = 0, since 0log20 = 0log0 = 0. So,
we only consider p > 0. If we denote t = q/p ≥ u, then the desired inequality is equivalent to
log2t
t−1−logt ≥ . (A.5)
−2logu
It is easy to see that this inequality holds for t = 1. For t ∈ (1,∞), it can be proven by letting
t = es with s > 0 and using
s2 s2
es−1−s ≥ ≥ .
2 −2logu
27For t ∈ [u,1), we consider the function
log2t
f(t) = .
t−1−logt
Sincet−1−logt > 0, theinequality(A.5)isequivalenttof(t) ≤ −2logu. Adirectcalculation
shows
(cid:18) (cid:19)
logt 2 logt
f′(t) = 2− −logt−
(t−1−logt)2 t t
logt
=: g(t).
(t−1−logt)2
Since
1−t+logt
g′(t) = < 0,
t2
we know that g(t) > g(1) = 0 and hence f′(t) < 0 for t ∈ [u,1). Therefore, f is decreasing on
[u,1) and
log2u
f(t) ≤ f(u) = ≤ −2logu,
u−1−logu
where we use u−1 ≥ log(e−1) ≥ 1 logu because u ≤ e−2.
2
References
Jean-Yves Audibert and Alexandre B. Tsybakov. Fast learning rates for plug-in classifiers.
The Annals of Statistics, 35(2):608–633, 2007.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of
Machine Learning Research, 18(19):1–53, 2017.
Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classification, and risk
bounds. Journal of the American Statistical Association, 101(473):138–156, 2006.
Thijs Bos and Johannes Schmidt-Hieber. Convergence rates of deep ReLU networks for
multiclass classification. Electronic Journal of Statistics, 16(1), 2022.
Weinan E, Chao Ma, and Lei Wu. The Barron space and the flow-induced function spaces for
neural network models. Constructive Approximation, 55(1):369–406, 2022.
Zhiying Fang, Han Feng, Shuo Huang, and Ding-Xuan Zhou. Theory of deep convolutional
neural networks II: Spherical analysis. Neural Networks, 131:154–162, 2020.
HanFeng,ShuoHuang,andDing-XuanZhou. GeneralizationanalysisofCNNsforclassification
on spheres. IEEE Transactions on Neural Networks and Learning Systems, 34(9):6200–6213,
2023.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.
Yuling Jiao, Yang Wang, and Yunfei Yang. Approximation bounds for norm constrained
neural networks with applications to regression and GANs. Applied and Computational
Harmonic Analysis, 65:249–278, 2023.
28Yongdai Kim, Ilsang Ohn, and Dongha Kim. Fast convergence rates of deep neural networks
for classification. Neural Networks, 138:179–197, 2021.
Michael Kohler and Sophie Langer. Statistical theory for image classification using deep
convolutional neural networks with cross-entropy loss. arXiv:2011.13602, 2020.
Michael Kohler and Sophie Langer. On the rate of convergence of fully connected deep neural
network regression estimates. The Annals of Statistics, 49(4):2231–2249, 2021.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification with deep
convolutional neural networks. In Advances in Neural Information Processing Systems. 2012.
Yann LeCun, Yoshua Bengio, and Geoffrey E. Hinton. Deep learning. Nature, 521(7553):
436–444, 2015.
Shao-Bo Lin, Kaidong Wang, Yao Wang, and Ding-Xuan Zhou. Universal consistency of deep
convolutional neural networks. IEEE Transactions on Information Theory, 68(7):4610–4617,
2022.
Hao Liu, Minshuo Chen, Tuo Zhao, and Wenjing Liao. Besov function approximation and
binary classification on low-dimensional manifolds using convolutional residual networks. In
Proceedings of the 38th International Conference on Machine Learning, pages 6770–6780.
2021.
Jianfeng Lu, Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network approximation for
smooth functions. SIAM Journal on Mathematical Analysis, 53(5):5465–5506, 2021.
Enno Mammen and Alexandre B. Tsybakov. Smooth discrimination analysis. The Annals of
Statistics, 27(6):1808–1829, 1999.
Tong Mao, Zhongjie Shi, and Ding-Xuan Zhou. Theory of deep convolutional neural networks
III: Approximating radial functions. Neural Networks, 144:778–790, 2021.
Yurii Nesterov. Lectures on Convex Optimization, volume 137. Springer, 2018.
Kenta Oono and Taiji Suzuki. Approximation and non-parametric estimation of ResNet-type
convolutional neural networks. In Proceedings of the 36th International Conference on
Machine Learning, pages 4922–4931. 2019.
Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU
activation function. The Annals of Statistics, 48(4):1875–1897, 2020.
Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network approximation characterized
by number of neurons. Communications in Computational Physics, 28(5):1768–1811, 2020.
Jonathan W. Siegel. Optimal approximation of zonoids and uniform approximation by shallow
neural networks. arXiv: 2307.15285, 2023.
Jonathan W. Siegel and Jinchao Xu. Sharp bounds on the approximation rates, metric entropy,
and n-widths of shallow neural networks. Foundations of Computational Mathematics, pages
1–57, 2022.
29Jonathan W. Siegel and Jinchao Xu. Characterization of the variation spaces corresponding
to shallow neural networks. Constructive Approximation, 57(3):1109–1132, 2023.
Ingo Steinwart and Andreas Christmann. Support Vector Machines. Springer Science &
Business Media, 2008.
Ingo Steinwart and Clint Scovel. Fast rates for support vector machines using Gaussian kernels.
The Annals of Statistics, 35(2):575–607, 2007.
Charles J. Stone. Optimal global rates of convergence for nonparametric regression. The
Annals of Statistics, 10(4):1040–1053, 1982.
Alexander B. Tsybakov. Optimal aggregation of classifiers in statistical learning. The Annals
of Statistics, 32(1):135–166, 2004.
Yunfei Yang and Ding-Xuan Zhou. Nonparametric regression using over-parameterized shallow
ReLU neural networks. arXiv: 2306.08321, 2023.
Yunfei Yang and Ding-Xuan Zhou. Optimal rates of approximation by shallow ReLUk neural
networks and applications to nonparametric regression. Constructive Approximation, 2024.
DmitryYarotsky. ErrorboundsforapproximationswithdeepReLUnetworks. Neural Networks,
94:103–114, 2017.
DmitryYarotsky. OptimalapproximationofcontinuousfunctionsbyverydeepReLUnetworks.
In Proceedings of the 31st Conference on Learning Theory, pages 639–649. 2018.
Tong Zhang. Statistical behavior and consistency of classification methods based on convex
risk minimization. The Annals of Statistics, 32(1):56–134, 2004.
Zihan Zhang, Lei Shi, and Ding-Xuan Zhou. Classification with deep neural networks and
logistic loss. arXiv: 2307.16792, 2023.
Ding-Xuan Zhou. Theory of deep convolutional neural networks: Downsampling. Neural
Networks, 124:319–327, 2020a.
Ding-Xuan Zhou. Universality of deep convolutional neural networks. Applied and Computa-
tional Harmonic Analysis, 48(2):787–794, 2020b.
Tian-Yi Zhou and Xiaoming Huo. Learning ability of interpolating deep convolutional neural
networks. Applied and Computational Harmonic Analysis, 68:101582, 2024.
30