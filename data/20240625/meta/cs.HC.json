[
    {
        "title": "Preserving Real-World Finger Dexterity Using a Lightweight Fingertip Haptic Device for Virtual Dexterous Manipulation",
        "authors": "Yunxiu XUSiyu WangShoichi Hasegawa",
        "links": "http://arxiv.org/abs/2406.16835v1",
        "entry_id": "http://arxiv.org/abs/2406.16835v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16835v1",
        "summary": "This study presents a lightweight, wearable fingertip haptic device that\nprovides physics-based haptic feedback for dexterous manipulation in virtual\nenvironments without hindering real-world interactions. The device's design\nutilizes thin strings and actuators attached to the fingernails, minimizing the\nweight (1.76g each finger) while preserving finger flexibility. Multiple types\nof haptic feedback are simulated by integrating the software with a physics\nengine. Experiments evaluate the device's performance in pressure perception,\nslip feedback, and typical dexterous manipulation tasks. and daily operations,\nwhile subjective assessments gather user experiences. Results demonstrate that\nparticipants can perceive and respond to pressure and vibration feedback. These\nlimited haptic cues are crucial as they significantly enhance efficiency in\nvirtual dexterous manipulation tasks. The device's ability to preserve tactile\nsensations and minimize hindrance to real-world operations is a key advantage\nover glove-type haptic devices. This research offers a potential solution for\ndesigning haptic interfaces that balance lightweight, haptic feedback for\ndexterous manipulation and daily wearability.",
        "updated": "2024-06-24 17:44:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16835v1"
    },
    {
        "title": "A Digital Human Model for Symptom Progression of Vestibular Motion Sickness based on Subjective Vertical Conflict Theory",
        "authors": "Shota InoueHailong LiuTakahiro Wada",
        "links": "http://arxiv.org/abs/2406.16737v1",
        "entry_id": "http://arxiv.org/abs/2406.16737v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16737v1",
        "summary": "Digital human models of motion sickness have been actively developed, among\nwhich models based on subjective vertical conflict (SVC) theory are the most\nactively studied. These models facilitate the prediction of motion sickness in\nvarious scenarios such as riding in a car. Most SVC theory models predict the\nmotion sickness incidence (MSI), which is defined as the percentage of people\nwho would vomit with the given specific motion stimulus. However, no model has\nbeen developed to describe milder forms of discomfort or specific symptoms of\nmotion sickness, even though predicting milder symptoms is important for\napplications in automobiles and daily use vehicles. Therefore, the purpose of\nthis study was to build a computational model of symptom progression of\nvestibular motion sickness based on SVC theory. We focused on a model of\nvestibular motion sickness with six degrees-of-freedom (6DoF) head motions. The\nmodel was developed by updating the output part of the state-of-the-art SVC\nmodel, termed the 6DoF-SVC (IN1) model, from MSI to the MIsery SCale (MISC),\nwhich is a subjective rating scale for symptom progression. We conducted an\nexperiment to measure the progression of motion sickness during a straight\nfore-aft motion. It was demonstrated that our proposed method, with the\nparameters of the output parts optimized by the experimental results, fits well\nwith the observed MISC.",
        "updated": "2024-06-24 15:44:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16737v1"
    },
    {
        "title": "ChatGPT's financial discrimination between rich and poor -- misaligned with human behavior and expectations",
        "authors": "Dmitri BershadskyyFlorian E. SachsJoachim Weimann",
        "links": "http://arxiv.org/abs/2406.16572v1",
        "entry_id": "http://arxiv.org/abs/2406.16572v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16572v1",
        "summary": "ChatGPT disrupted the application of machine-learning methods and drastically\nreduced the usage barrier. Chatbots are now widely used in a lot of different\nsituations. They provide advice, assist in writing source code, or assess and\nsummarize information from various sources. However, their scope is not only\nlimited to aiding humans; they can also be used to take on tasks like\nnegotiating or bargaining. To understand the implications of Chatbot usage on\nbargaining situations, we conduct a laboratory experiment with the ultimatum\ngame. In the ultimatum game, two human players interact: The receiver decides\non accepting or rejecting a monetary offer from the proposer. To shed light on\nthe new bargaining situation, we let ChatGPT provide an offer to a human\nplayer. In the novel design, we vary the wealth of the receivers. Our results\nindicate that humans have the same beliefs about other humans and chatbots.\nHowever, our results contradict these beliefs in an important point: Humans\nfavor poor receivers as correctly anticipated by the humans, but ChatGPT favors\nrich receivers which the humans did not expect to happen. These results imply\nthat ChatGPT's answers are not aligned with those of humans and that humans do\nnot anticipate this difference.",
        "updated": "2024-06-24 12:09:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16572v1"
    },
    {
        "title": "PenSLR: Persian end-to-end Sign Language Recognition Using Ensembling",
        "authors": "Amirparsa SalmankhahAmirreza RajabiNegin KheirmandAli FadaeimaneshAmirreza TarabkhahAmirreza KazemzadehHamed Farbeh",
        "links": "http://arxiv.org/abs/2406.16388v1",
        "entry_id": "http://arxiv.org/abs/2406.16388v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16388v1",
        "summary": "Sign Language Recognition (SLR) is a fast-growing field that aims to fill the\ncommunication gaps between the hearing-impaired and people without hearing\nloss. Existing solutions for Persian Sign Language (PSL) are limited to\nword-level interpretations, underscoring the need for more advanced and\ncomprehensive solutions. Moreover, previous work on other languages mainly\nfocuses on manipulating the neural network architectures or hardware\nconfigurations instead of benefiting from the aggregated results of multiple\nmodels. In this paper, we introduce PenSLR, a glove-based sign language system\nconsisting of an Inertial Measurement Unit (IMU) and five flexible sensors\npowered by a deep learning framework capable of predicting variable-length\nsequences. We achieve this in an end-to-end manner by leveraging the\nConnectionist Temporal Classification (CTC) loss function, eliminating the need\nfor segmentation of input signals. To further enhance its capabilities, we\npropose a novel ensembling technique by leveraging a multiple sequence\nalignment algorithm known as Star Alignment. Furthermore, we introduce a new\nPSL dataset, including 16 PSL signs with more than 3000 time-series samples in\ntotal. We utilize this dataset to evaluate the performance of our system based\non four word-level and sentence-level metrics. Our evaluations show that PenSLR\nachieves a remarkable word accuracy of 94.58% and 96.70% in subject-independent\nand subject-dependent setups, respectively. These achievements are attributable\nto our ensembling algorithm, which not only boosts the word-level performance\nby 0.51% and 1.32% in the respective scenarios but also yields significant\nenhancements of 1.46% and 4.00%, respectively, in sentence-level accuracy.",
        "updated": "2024-06-24 07:59:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16388v1"
    },
    {
        "title": "Flowy: Supporting UX Design Decisions Through AI-Driven Pattern Annotation in Multi-Screen User Flows",
        "authors": "Yuwen LuZiang TongQinyi ZhaoYewon OhBryan WangToby Jia-Jun Li",
        "links": "http://arxiv.org/abs/2406.16177v1",
        "entry_id": "http://arxiv.org/abs/2406.16177v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16177v1",
        "summary": "Many recent AI-powered UX design tools focus on generating individual static\nUI screens from natural language. However, they overlook the crucial aspect of\ninteractions and user experiences across multiple screens. Through formative\nstudies with UX professionals, we identified limitations of these tools in\nsupporting realistic UX design workflows. In response, we designed and\ndeveloped Flowy, an app that augments designers' information foraging process\nin ideation by supplementing specific user flow examples with distilled design\npattern knowledge. Flowy utilizes large multimodal AI models and a high-quality\nuser flow dataset to help designers identify and understand relevant abstract\ndesign patterns in the design space for multi-screen user flows. Our user study\nwith professional UX designers demonstrates how Flowy supports realistic UX\ntasks. Our design considerations in Flowy, such as representations with\nappropriate levels of abstraction and assisted navigation through the solution\nspace, are generalizable to other creative tasks and embody a human-centered,\nintelligence augmentation approach to using AI in UX design.",
        "updated": "2024-06-23 18:09:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16177v1"
    }
]