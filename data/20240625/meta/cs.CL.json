[
    {
        "title": "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees",
        "authors": "Yuhui LiFangyun WeiChao ZhangHongyang Zhang",
        "links": "http://arxiv.org/abs/2406.16858v1",
        "entry_id": "http://arxiv.org/abs/2406.16858v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16858v1",
        "summary": "Inference with modern Large Language Models (LLMs) is expensive and\ntime-consuming, and speculative sampling has proven to be an effective\nsolution. Most speculative sampling methods such as EAGLE use a static draft\ntree, implicitly assuming that the acceptance rate of draft tokens depends only\non their position. Interestingly, we found that the acceptance rate of draft\ntokens is also context-dependent. In this paper, building upon EAGLE, we\npropose EAGLE-2, which introduces a new technique of context-aware dynamic\ndraft tree into drafting modeling. This improvement leverages the fact that the\ndraft model of EAGLE is well-calibrated: the confidence scores from the draft\nmodel approximate acceptance rates with small errors. We conducted extensive\nevaluations on three series of LLMs and six tasks, with EAGLE-2 achieving\nspeedup ratios 3.05x-4.26x, which is 20%-40% faster than EAGLE-1. EAGLE-2 also\nensures that the distribution of the generated text remains unchanged, making\nit a lossless acceleration algorithm.",
        "updated": "2024-06-24 17:59:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16858v1"
    },
    {
        "title": "Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts",
        "authors": "Aditya SharmaMichael SaxonWilliam Yang Wang",
        "links": "http://arxiv.org/abs/2406.16851v1",
        "entry_id": "http://arxiv.org/abs/2406.16851v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16851v1",
        "summary": "We present LoCoVQA, a dynamic benchmark generator for evaluating long-context\nextractive reasoning in vision language models (VLMs). LoCoVQA augments test\nexamples for mathematical reasoning, VQA, and character recognition tasks with\nincreasingly long visual contexts composed of both in-distribution and\nout-of-distribution distractor images.\n  Across these tasks, a diverse set of VLMs rapidly lose performance as the\nvisual context length grows, often exhibiting a striking exponential decay\ntrend. This test assesses how well VLMs can ignore irrelevant information when\nanswering queries -- a task that is quite easy for language models (LMs) in the\ntext domain -- demonstrating that current state-of-the-art VLMs lack this\nessential capability for many long-context applications.",
        "updated": "2024-06-24 17:58:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16851v1"
    },
    {
        "title": "RaTEScore: A Metric for Radiology Report Generation",
        "authors": "Weike ZhaoChaoyi WuXiaoman ZhangYa ZhangYanfeng WangWeidi Xie",
        "links": "http://arxiv.org/abs/2406.16845v1",
        "entry_id": "http://arxiv.org/abs/2406.16845v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16845v1",
        "summary": "This paper introduces a novel, entity-aware metric, termed as Radiological\nReport (Text) Evaluation (RaTEScore), to assess the quality of medical reports\ngenerated by AI models. RaTEScore emphasizes crucial medical entities such as\ndiagnostic outcomes and anatomical details, and is robust against complex\nmedical synonyms and sensitive to negation expressions. Technically, we\ndeveloped a comprehensive medical NER dataset, RaTE-NER, and trained an NER\nmodel specifically for this purpose. This model enables the decomposition of\ncomplex radiological reports into constituent medical entities. The metric\nitself is derived by comparing the similarity of entity embeddings, obtained\nfrom a language model, based on their types and relevance to clinical\nsignificance. Our evaluations demonstrate that RaTEScore aligns more closely\nwith human preference than existing metrics, validated both on established\npublic benchmarks and our newly proposed RaTE-Eval benchmark.",
        "updated": "2024-06-24 17:49:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16845v1"
    },
    {
        "title": "Exploring Factual Entailment with NLI: A News Media Study",
        "authors": "Guy Mor-LanEffi Levi",
        "links": "http://arxiv.org/abs/2406.16842v1",
        "entry_id": "http://arxiv.org/abs/2406.16842v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16842v1",
        "summary": "We explore the relationship between factuality and Natural Language Inference\n(NLI) by introducing FactRel -- a novel annotation scheme that models\n\\textit{factual} rather than \\textit{textual} entailment, and use it to\nannotate a dataset of naturally occurring sentences from news articles. Our\nanalysis shows that 84\\% of factually supporting pairs and 63\\% of factually\nundermining pairs do not amount to NLI entailment or contradiction,\nrespectively, suggesting that factual relationships are more apt for analyzing\nmedia discourse. We experiment with models for pairwise classification on the\nnew dataset, and find that in some cases, generating synthetic data with GPT-4\non the basis of the annotated dataset can improve performance. Surprisingly,\nfew-shot learning with GPT-4 yields strong results on par with medium LMs\n(DeBERTa) trained on the labelled dataset. We hypothesize that these results\nindicate the fundamental dependence of this task on both world knowledge and\nadvanced reasoning abilities.",
        "updated": "2024-06-24 17:47:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16842v1"
    },
    {
        "title": "From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models",
        "authors": "Sean WelleckAmanda BertschMatthew FinlaysonHailey SchoelkopfAlex XieGraham NeubigIlia KulikovZaid Harchaoui",
        "links": "http://arxiv.org/abs/2406.16838v1",
        "entry_id": "http://arxiv.org/abs/2406.16838v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16838v1",
        "summary": "One of the most striking findings in modern research on large language models\n(LLMs) is that scaling up compute during training leads to better results.\nHowever, less attention has been given to the benefits of scaling compute\nduring inference. This survey focuses on these inference-time approaches. We\nexplore three areas under a unified mathematical formalism: token-level\ngeneration algorithms, meta-generation algorithms, and efficient generation.\nToken-level generation algorithms, often called decoding algorithms, operate by\nsampling a single token at a time or constructing a token-level search space\nand then selecting an output. These methods typically assume access to a\nlanguage model's logits, next-token distributions, or probability scores.\nMeta-generation algorithms work on partial or full sequences, incorporating\ndomain knowledge, enabling backtracking, and integrating external information.\nEfficient generation methods aim to reduce token costs and improve the speed of\ngeneration. Our survey unifies perspectives from three research communities:\ntraditional natural language processing, modern LLMs, and machine learning\nsystems.",
        "updated": "2024-06-24 17:45:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16838v1"
    }
]