[
    {
        "title": "Data Debiasing with Datamodels (D3M): Improving Subgroup Robustness via Data Selection",
        "authors": "Saachi JainKimia HamidiehKristian GeorgievAndrew IlyasMarzyeh GhassemiAleksander Madry",
        "links": "http://arxiv.org/abs/2406.16846v1",
        "entry_id": "http://arxiv.org/abs/2406.16846v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16846v1",
        "summary": "Machine learning models can fail on subgroups that are underrepresented\nduring training. While techniques such as dataset balancing can improve\nperformance on underperforming groups, they require access to training group\nannotations and can end up removing large portions of the dataset. In this\npaper, we introduce Data Debiasing with Datamodels (D3M), a debiasing approach\nwhich isolates and removes specific training examples that drive the model's\nfailures on minority groups. Our approach enables us to efficiently train\ndebiased classifiers while removing only a small number of examples, and does\nnot require training group annotations or additional hyperparameter tuning.",
        "updated": "2024-06-24 17:51:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16846v1"
    },
    {
        "title": "Concentration Inequalities for $(f,Γ)$-GANs",
        "authors": "Jeremiah Birrell",
        "links": "http://arxiv.org/abs/2406.16834v1",
        "entry_id": "http://arxiv.org/abs/2406.16834v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16834v1",
        "summary": "Generative adversarial networks (GANs) are unsupervised learning methods for\ntraining a generator distribution to produce samples that approximate those\ndrawn from a target distribution. Many such methods can be formulated as\nminimization of a metric or divergence. Recent works have proven the\nstatistical consistency of GANs that are based on integral probability metrics\n(IPMs), e.g., WGAN which is based on the 1-Wasserstein metric. IPMs are defined\nby optimizing a linear functional (difference of expectations) over a space of\ndiscriminators. A much larger class of GANs, which allow for the use of\nnonlinear objective functionals, can be constructed using\n$(f,\\Gamma)$-divergences; these generalize and interpolate between IPMs and\n$f$-divergences (e.g., KL or $\\alpha$-divergences). Instances of\n$(f,\\Gamma)$-GANs have been shown to exhibit improved performance in a number\nof applications. In this work we study the statistical consistency of\n$(f,\\Gamma)$-GANs for general $f$ and $\\Gamma$. Specifically, we derive\nfinite-sample concentration inequalities. These derivations require novel\narguments due to nonlinearity of the objective functional. We demonstrate that\nour new results reduce to the known results for IPM-GANs in the appropriate\nlimit while also significantly extending the domain of applicability of this\ntheory.",
        "updated": "2024-06-24 17:42:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16834v1"
    },
    {
        "title": "Improved Regret Bounds for Bandits with Expert Advice",
        "authors": "Nicolò Cesa-BianchiKhaled EldowaEmmanuel EspositoJulia Olkhovskaya",
        "links": "http://arxiv.org/abs/2406.16802v1",
        "entry_id": "http://arxiv.org/abs/2406.16802v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16802v1",
        "summary": "In this research note, we revisit the bandits with expert advice problem.\nUnder a restricted feedback model, we prove a lower bound of order $\\sqrt{K T\n\\ln(N/K)}$ for the worst-case regret, where $K$ is the number of actions, $N>K$\nthe number of experts, and $T$ the time horizon. This matches a previously\nknown upper bound of the same order and improves upon the best available lower\nbound of $\\sqrt{K T (\\ln N) / (\\ln K)}$. For the standard feedback model, we\nprove a new instance-based upper bound that depends on the agreement between\nthe experts and provides a logarithmic improvement compared to prior results.",
        "updated": "2024-06-24 17:14:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16802v1"
    },
    {
        "title": "Conformal time series decomposition with component-wise exchangeability",
        "authors": "Derck W. E. PrinzhornThijmen NijdamPutri A. van der LindenAlexander Timans",
        "links": "http://arxiv.org/abs/2406.16766v1",
        "entry_id": "http://arxiv.org/abs/2406.16766v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16766v1",
        "summary": "Conformal prediction offers a practical framework for distribution-free\nuncertainty quantification, providing finite-sample coverage guarantees under\nrelatively mild assumptions on data exchangeability. However, these assumptions\ncease to hold for time series due to their temporally correlated nature. In\nthis work, we present a novel use of conformal prediction for time series\nforecasting that incorporates time series decomposition. This approach allows\nus to model different temporal components individually. By applying specific\nconformal algorithms to each component and then merging the obtained prediction\nintervals, we customize our methods to account for the different\nexchangeability regimes underlying each component. Our decomposition-based\napproach is thoroughly discussed and empirically evaluated on synthetic and\nreal-world data. We find that the method provides promising results on\nwell-structured time series, but can be limited by factors such as the\ndecomposition step for more complex data.",
        "updated": "2024-06-24 16:23:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16766v1"
    },
    {
        "title": "Inferring stochastic low-rank recurrent neural networks from neural data",
        "authors": "Matthijs PalsA Erdem SağtekinFelix PeiManuel GloecklerJakob H Macke",
        "links": "http://arxiv.org/abs/2406.16749v1",
        "entry_id": "http://arxiv.org/abs/2406.16749v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16749v1",
        "summary": "A central aim in computational neuroscience is to relate the activity of\nlarge populations of neurons to an underlying dynamical system. Models of these\nneural dynamics should ideally be both interpretable and fit the observed data\nwell. Low-rank recurrent neural networks (RNNs) exhibit such interpretability\nby having tractable dynamics. However, it is unclear how to best fit low-rank\nRNNs to data consisting of noisy observations of an underlying stochastic\nsystem. Here, we propose to fit stochastic low-rank RNNs with variational\nsequential Monte Carlo methods. We validate our method on several datasets\nconsisting of both continuous and spiking neural data, where we obtain lower\ndimensional latent dynamics than current state of the art methods.\nAdditionally, for low-rank models with piecewise linear nonlinearities, we show\nhow to efficiently identify all fixed points in polynomial rather than\nexponential cost in the number of units, making analysis of the inferred\ndynamics tractable for large RNNs. Our method both elucidates the dynamical\nsystems underlying experimental recordings and provides a generative model\nwhose trajectories match observed trial-to-trial variability.",
        "updated": "2024-06-24 15:57:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16749v1"
    }
]