[
    {
        "title": "StableNormal: Reducing Diffusion Variance for Stable and Sharp Normal",
        "authors": "Chongjie YeLingteng QiuXiaodong GuQi ZuoYushuang WuZilong DongLiefeng BoYuliang XiuXiaoguang Han",
        "links": "http://arxiv.org/abs/2406.16864v1",
        "entry_id": "http://arxiv.org/abs/2406.16864v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16864v1",
        "summary": "This work addresses the challenge of high-quality surface normal estimation\nfrom monocular colored inputs (i.e., images and videos), a field which has\nrecently been revolutionized by repurposing diffusion priors. However, previous\nattempts still struggle with stochastic inference, conflicting with the\ndeterministic nature of the Image2Normal task, and costly ensembling step,\nwhich slows down the estimation process. Our method, StableNormal, mitigates\nthe stochasticity of the diffusion process by reducing inference variance, thus\nproducing \"Stable-and-Sharp\" normal estimates without any additional ensembling\nprocess. StableNormal works robustly under challenging imaging conditions, such\nas extreme lighting, blurring, and low quality. It is also robust against\ntransparent and reflective surfaces, as well as cluttered scenes with numerous\nobjects. Specifically, StableNormal employs a coarse-to-fine strategy, which\nstarts with a one-step normal estimator (YOSO) to derive an initial normal\nguess, that is relatively coarse but reliable, then followed by a\nsemantic-guided refinement process (SG-DRN) that refines the normals to recover\ngeometric details. The effectiveness of StableNormal is demonstrated through\ncompetitive performance in standard datasets such as DIODE-indoor, iBims,\nScannetV2 and NYUv2, and also in various downstream tasks, such as surface\nreconstruction and normal enhancement. These results evidence that StableNormal\nretains both the \"stability\" and \"sharpness\" for accurate normal estimation.\nStableNormal represents a baby attempt to repurpose diffusion priors for\ndeterministic estimation. To democratize this, code and models have been\npublicly available in hf.co/Stable-X",
        "updated": "2024-06-24 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16864v1"
    },
    {
        "title": "Revisiting Referring Expression Comprehension Evaluation in the Era of Large Multimodal Models",
        "authors": "Jierun ChenFangyun WeiJinjing ZhaoSizhe SongBohuai WuZhuoxuan PengS. -H. Gary ChanHongyang Zhang",
        "links": "http://arxiv.org/abs/2406.16866v1",
        "entry_id": "http://arxiv.org/abs/2406.16866v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16866v1",
        "summary": "Referring expression comprehension (REC) involves localizing a target\ninstance based on a textual description. Recent advancements in REC have been\ndriven by large multimodal models (LMMs) like CogVLM, which achieved 92.44%\naccuracy on RefCOCO. However, this study questions whether existing benchmarks\nsuch as RefCOCO, RefCOCO+, and RefCOCOg, capture LMMs' comprehensive\ncapabilities. We begin with a manual examination of these benchmarks, revealing\nhigh labeling error rates: 14% in RefCOCO, 24% in RefCOCO+, and 5% in RefCOCOg,\nwhich undermines the authenticity of evaluations. We address this by excluding\nproblematic instances and reevaluating several LMMs capable of handling the REC\ntask, showing significant accuracy improvements, thus highlighting the impact\nof benchmark noise. In response, we introduce Ref-L4, a comprehensive REC\nbenchmark, specifically designed to evaluate modern REC models. Ref-L4 is\ndistinguished by four key features: 1) a substantial sample size with 45,341\nannotations; 2) a diverse range of object categories with 365 distinct types\nand varying instance scales from 30 to 3,767; 3) lengthy referring expressions\naveraging 24.2 words; and 4) an extensive vocabulary comprising 22,813 unique\nwords. We evaluate a total of 24 large models on Ref-L4 and provide valuable\ninsights. The cleaned versions of RefCOCO, RefCOCO+, and RefCOCOg, as well as\nour Ref-L4 benchmark and evaluation code, are available at\nhttps://github.com/JierunChen/Ref-L4.",
        "updated": "2024-06-24 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16866v1"
    },
    {
        "title": "FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models",
        "authors": "Haonan QiuZhaoxi ChenZhouxia WangYingqing HeMenghan XiaZiwei Liu",
        "links": "http://arxiv.org/abs/2406.16863v1",
        "entry_id": "http://arxiv.org/abs/2406.16863v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16863v1",
        "summary": "Diffusion model has demonstrated remarkable capability in video generation,\nwhich further sparks interest in introducing trajectory control into the\ngeneration process. While existing works mainly focus on training-based methods\n(e.g., conditional adapter), we argue that diffusion model itself allows decent\ncontrol over the generated content without requiring any training. In this\nstudy, we introduce a tuning-free framework to achieve trajectory-controllable\nvideo generation, by imposing guidance on both noise construction and attention\ncomputation. Specifically, 1) we first show several instructive phenomenons and\nanalyze how initial noises influence the motion trajectory of generated\ncontent. 2) Subsequently, we propose FreeTraj, a tuning-free approach that\nenables trajectory control by modifying noise sampling and attention\nmechanisms. 3) Furthermore, we extend FreeTraj to facilitate longer and larger\nvideo generation with controllable trajectories. Equipped with these designs,\nusers have the flexibility to provide trajectories manually or opt for\ntrajectories automatically generated by the LLM trajectory planner. Extensive\nexperiments validate the efficacy of our approach in enhancing the trajectory\ncontrollability of video diffusion models.",
        "updated": "2024-06-24 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16863v1"
    },
    {
        "title": "Dreamitate: Real-World Visuomotor Policy Learning via Video Generation",
        "authors": "Junbang LiangRuoshi LiuEge OzgurogluSruthi SudhakarAchal DavePavel TokmakovShuran SongCarl Vondrick",
        "links": "http://arxiv.org/abs/2406.16862v1",
        "entry_id": "http://arxiv.org/abs/2406.16862v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16862v1",
        "summary": "A key challenge in manipulation is learning a policy that can robustly\ngeneralize to diverse visual environments. A promising mechanism for learning\nrobust policies is to leverage video generative models, which are pretrained on\nlarge-scale datasets of internet videos. In this paper, we propose a visuomotor\npolicy learning framework that fine-tunes a video diffusion model on human\ndemonstrations of a given task. At test time, we generate an example of an\nexecution of the task conditioned on images of a novel scene, and use this\nsynthesized execution directly to control the robot. Our key insight is that\nusing common tools allows us to effortlessly bridge the embodiment gap between\nthe human hand and the robot manipulator. We evaluate our approach on four\ntasks of increasing complexity and demonstrate that harnessing internet-scale\ngenerative models allows the learned policy to achieve a significantly higher\ndegree of generalization than existing behavior cloning approaches.",
        "updated": "2024-06-24 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16862v1"
    },
    {
        "title": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs",
        "authors": "Shengbang TongEllis BrownPenghao WuSanghyun WooManoj MiddepoguSai Charitha AkulaJihan YangShusheng YangAdithya IyerXichen PanAustin WangRob FergusYann LeCunSaining Xie",
        "links": "http://arxiv.org/abs/2406.16860v1",
        "entry_id": "http://arxiv.org/abs/2406.16860v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16860v1",
        "summary": "We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a\nvision-centric approach. While stronger language models can enhance multimodal\ncapabilities, the design choices for vision components are often insufficiently\nexplored and disconnected from visual representation learning research. This\ngap hinders accurate sensory grounding in real-world scenarios. Our study uses\nLLMs and visual instruction tuning as an interface to evaluate various visual\nrepresentations, offering new insights into different models and architectures\n-- self-supervised, strongly supervised, or combinations thereof -- based on\nexperiments with over 20 vision encoders. We critically examine existing MLLM\nbenchmarks, addressing the difficulties involved in consolidating and\ninterpreting results from various tasks, and introduce a new vision-centric\nbenchmark, CV-Bench. To further improve visual grounding, we propose the\nSpatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that\nintegrates high-resolution vision features with LLMs while reducing the number\nof tokens. Additionally, we discuss the curation of high-quality visual\ninstruction-tuning data from publicly available sources, emphasizing the\nimportance of data source balancing and distribution ratio. Collectively,\nCambrian-1 not only achieves state-of-the-art performance but also serves as a\ncomprehensive, open cookbook for instruction-tuned MLLMs. We provide model\nweights, code, supporting tools, datasets, and detailed instruction-tuning and\nevaluation recipes. We hope our release will inspire and accelerate\nadvancements in multimodal systems and visual representation learning.",
        "updated": "2024-06-24 17:59:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16860v1"
    }
]