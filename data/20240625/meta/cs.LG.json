[
    {
        "title": "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees",
        "authors": "Yuhui LiFangyun WeiChao ZhangHongyang Zhang",
        "links": "http://arxiv.org/abs/2406.16858v1",
        "entry_id": "http://arxiv.org/abs/2406.16858v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16858v1",
        "summary": "Inference with modern Large Language Models (LLMs) is expensive and\ntime-consuming, and speculative sampling has proven to be an effective\nsolution. Most speculative sampling methods such as EAGLE use a static draft\ntree, implicitly assuming that the acceptance rate of draft tokens depends only\non their position. Interestingly, we found that the acceptance rate of draft\ntokens is also context-dependent. In this paper, building upon EAGLE, we\npropose EAGLE-2, which introduces a new technique of context-aware dynamic\ndraft tree into drafting modeling. This improvement leverages the fact that the\ndraft model of EAGLE is well-calibrated: the confidence scores from the draft\nmodel approximate acceptance rates with small errors. We conducted extensive\nevaluations on three series of LLMs and six tasks, with EAGLE-2 achieving\nspeedup ratios 3.05x-4.26x, which is 20%-40% faster than EAGLE-1. EAGLE-2 also\nensures that the distribution of the generated text remains unchanged, making\nit a lossless acceleration algorithm.",
        "updated": "2024-06-24 17:59:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16858v1"
    },
    {
        "title": "GeoMFormer: A General Architecture for Geometric Molecular Representation Learning",
        "authors": "Tianlang ChenShengjie LuoDi HeShuxin ZhengTie-Yan LiuLiwei Wang",
        "links": "http://arxiv.org/abs/2406.16853v1",
        "entry_id": "http://arxiv.org/abs/2406.16853v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16853v1",
        "summary": "Molecular modeling, a central topic in quantum mechanics, aims to accurately\ncalculate the properties and simulate the behaviors of molecular systems. The\nmolecular model is governed by physical laws, which impose geometric\nconstraints such as invariance and equivariance to coordinate rotation and\ntranslation. While numerous deep learning approaches have been developed to\nlearn molecular representations under these constraints, most of them are built\nupon heuristic and costly modules. We argue that there is a strong need for a\ngeneral and flexible framework for learning both invariant and equivariant\nfeatures. In this work, we introduce a novel Transformer-based molecular model\ncalled GeoMFormer to achieve this goal. Using the standard Transformer modules,\ntwo separate streams are developed to maintain and learn invariant and\nequivariant representations. Carefully designed cross-attention modules bridge\nthe two streams, allowing information fusion and enhancing geometric modeling\nin each stream. As a general and flexible architecture, we show that many\nprevious architectures can be viewed as special instantiations of GeoMFormer.\nExtensive experiments are conducted to demonstrate the power of GeoMFormer. All\nempirical results show that GeoMFormer achieves strong performance on both\ninvariant and equivariant tasks of different types and scales. Code and models\nwill be made publicly available at https://github.com/c-tl/GeoMFormer.",
        "updated": "2024-06-24 17:58:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16853v1"
    },
    {
        "title": "Data Debiasing with Datamodels (D3M): Improving Subgroup Robustness via Data Selection",
        "authors": "Saachi JainKimia HamidiehKristian GeorgievAndrew IlyasMarzyeh GhassemiAleksander Madry",
        "links": "http://arxiv.org/abs/2406.16846v1",
        "entry_id": "http://arxiv.org/abs/2406.16846v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16846v1",
        "summary": "Machine learning models can fail on subgroups that are underrepresented\nduring training. While techniques such as dataset balancing can improve\nperformance on underperforming groups, they require access to training group\nannotations and can end up removing large portions of the dataset. In this\npaper, we introduce Data Debiasing with Datamodels (D3M), a debiasing approach\nwhich isolates and removes specific training examples that drive the model's\nfailures on minority groups. Our approach enables us to efficiently train\ndebiased classifiers while removing only a small number of examples, and does\nnot require training group annotations or additional hyperparameter tuning.",
        "updated": "2024-06-24 17:51:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16846v1"
    },
    {
        "title": "From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models",
        "authors": "Sean WelleckAmanda BertschMatthew FinlaysonHailey SchoelkopfAlex XieGraham NeubigIlia KulikovZaid Harchaoui",
        "links": "http://arxiv.org/abs/2406.16838v1",
        "entry_id": "http://arxiv.org/abs/2406.16838v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16838v1",
        "summary": "One of the most striking findings in modern research on large language models\n(LLMs) is that scaling up compute during training leads to better results.\nHowever, less attention has been given to the benefits of scaling compute\nduring inference. This survey focuses on these inference-time approaches. We\nexplore three areas under a unified mathematical formalism: token-level\ngeneration algorithms, meta-generation algorithms, and efficient generation.\nToken-level generation algorithms, often called decoding algorithms, operate by\nsampling a single token at a time or constructing a token-level search space\nand then selecting an output. These methods typically assume access to a\nlanguage model's logits, next-token distributions, or probability scores.\nMeta-generation algorithms work on partial or full sequences, incorporating\ndomain knowledge, enabling backtracking, and integrating external information.\nEfficient generation methods aim to reduce token costs and improve the speed of\ngeneration. Our survey unifies perspectives from three research communities:\ntraditional natural language processing, modern LLMs, and machine learning\nsystems.",
        "updated": "2024-06-24 17:45:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16838v1"
    },
    {
        "title": "Concentration Inequalities for $(f,Γ)$-GANs",
        "authors": "Jeremiah Birrell",
        "links": "http://arxiv.org/abs/2406.16834v1",
        "entry_id": "http://arxiv.org/abs/2406.16834v1",
        "pdf_url": "http://arxiv.org/pdf/2406.16834v1",
        "summary": "Generative adversarial networks (GANs) are unsupervised learning methods for\ntraining a generator distribution to produce samples that approximate those\ndrawn from a target distribution. Many such methods can be formulated as\nminimization of a metric or divergence. Recent works have proven the\nstatistical consistency of GANs that are based on integral probability metrics\n(IPMs), e.g., WGAN which is based on the 1-Wasserstein metric. IPMs are defined\nby optimizing a linear functional (difference of expectations) over a space of\ndiscriminators. A much larger class of GANs, which allow for the use of\nnonlinear objective functionals, can be constructed using\n$(f,\\Gamma)$-divergences; these generalize and interpolate between IPMs and\n$f$-divergences (e.g., KL or $\\alpha$-divergences). Instances of\n$(f,\\Gamma)$-GANs have been shown to exhibit improved performance in a number\nof applications. In this work we study the statistical consistency of\n$(f,\\Gamma)$-GANs for general $f$ and $\\Gamma$. Specifically, we derive\nfinite-sample concentration inequalities. These derivations require novel\narguments due to nonlinearity of the objective functional. We demonstrate that\nour new results reduce to the known results for IPM-GANs in the appropriate\nlimit while also significantly extending the domain of applicability of this\ntheory.",
        "updated": "2024-06-24 17:42:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.16834v1"
    }
]