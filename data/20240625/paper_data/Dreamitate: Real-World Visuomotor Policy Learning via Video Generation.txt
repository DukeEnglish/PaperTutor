Dreamitate:
Real-World Visuomotor Policy Learning via Video Generation
JunbangLiang∗1 RuoshiLiu∗1 EgeOzguroglu1 SruthiSudhakar1
AchalDave2 PavelTokmakov2 ShuranSong3 CarlVondrick1
1ColumbiaUniversity 2ToyotaResearchInstitute 3StanfordUniversity
dreamitate.cs.columbia.edu
Generated Video
Tool
Input Image … …
Video
Generator
… …
Classical
Visuomotor Policy
Robot Execution
Figure 1: Real-World Visuomotor Policy Learning via Video Generation. Dreamitate is a vi-
suomotorpolicylearningframeworkthatfine-tunesavideogenerativemodeltosynthesizevideos
(indicatedby )ofhumansusingtoolstocompleteatask. Thetool’strajectoryinthegener-
atedvideoistracked,andtherobotexecutesthistrajectorytoaccomplishthetaskinthereal-world.
Abstract: Akeychallengeinmanipulationislearningapolicythatcanrobustly
generalize to diverse visual environments. A promising mechanism for learning
robustpoliciesistoleveragevideogenerativemodelsthatarepretrainedonlarge-
scale datasets of internet videos. In this paper, we propose a visuomotor policy
learningframeworkthatfine-tunesavideodiffusionmodelonhumandemonstra-
tionsofagiventask. Attesttime, wegenerateavideoshowinganexecutionof
the task conditioned on images of a novel scene, and use this synthesized video
to directly control the robot. Our key insight is that using common tools allows
us to effortlessly bridge the embodiment gap between the human hand and the
robot manipulator. We evaluate our approach on four tasks of increasing com-
plexity and demonstrate that harnessing internet-scale generative models allows
thelearnedpolicytoachieveasignificantlyhigherdegreeofgeneralizationthan
existingbehaviorcloningapproaches.
Keywords: VisuomotorPolicyLearning,ImitationLearning,VideoGeneration
1 Introduction
Learningvisuomotorpoliciesthatdecideactionsfromperceptionhasbeenalongstandingchallenge
in robotics because they need to generalize to diverse physical situations [1]. Recently, behavior
cloningfromhumandemonstrations[2,3]hasbecomethemethodofchoiceformanymanipulation
∗Equalcontribution.
4202
nuJ
42
]OR.sc[
1v26861.6042:viXra
Dream
Track
Executetasks,wherevisuomotorpolicylearningisformulatedasaregressionproblemtomapvisualobser-
vationstoactionssupervisedbyhumandemonstrations. However,behaviorcloningrequiresground
truthrobotactions,whichmakesscalingtodiversesituationschallenging.
We aim to learn generalizable visuomotor policies by leveraging video generation. Trained on
large-scale datasets of internet videos, video generation provides a promising avenue for general-
izationbecauseextensivepriorsfromhumanbehaviorcanbetransferredintorobotbehavior. Many
recent methods have also explored how to capitalize on these priors, such as synthesizing videos
of human behavior [4] or synthesizing videos of robot behavior [5]. However, while videos of
peoplecapturethediversityofbehavior,generatedhumanactionsaredifficulttotransfertorobots
duetotheembodimentgap. Whiledirectlysynthesizingvideosofrobotswouldbemorerealizable
physically,thescaleofavailabledataismanyordersofmagnitudesmallerthanvideosofhumans,
makingthelearnedpolicylessrobustinanin-the-wildenvironment.
Howelsecanweleveragevideogenerationforpolicylearning? Inthiswork,weintroduceDreami-
tate,1 a visuomotor policy that controls robot behavior via conditional video generation. Our key
insight is that the end-effector is the most important part of the embodiment for manipulation: its
interactionwithdiverseobjectsischallengingtomodelexplicitlyandcanbenefitmostfromcom-
monsenseknowledgefromvideogeneration. Therestofrobotembodimentcanbesolvedthrough
inversekinematics. Givenavisualobservation,ourpolicysynthesizesavideoofapersonperform-
ing tasks with a tool. To translate into robot behaviors, we 3D track the tool in the synthesized
video, andsimplytransferthetrajectoryintoexplicitrobotactions. Figure1showsanexampleof
theactionscreatedfromthispolicy.
Ourformulationoffersseveralmajoradvantagescomparedtotraditionalvisuomotorpolicies:
• Generalizability. Ourunderlyingvideogenerationmodelispretrainedonamassiveamountof
videosavailableontheinternet,includingawidevarietyofhumanmanipulationinallkindsof
environments. Byformulatingpolicylearning asvideomodelfinetuning, theprior knowledge
learnedfrompre-tainingcanbepreserved,allowingourmodeltolearngeneralizablemanipula-
tionskillswiththesameamountofdemonstrationdata.
• Scalability. Ourfinetuningvideosarerecordingsofhumandemonstrationsinsteadofteleoper-
ation,makingourdatasetcollectionmorescalable.
• Interpretability. Duringinferencetime,ourvideomodelpredictsfutureexecutionplansinthe
formofvideosbeforetheactualrobotexecutionsareperformed. Comparedtoablack-boxend-
to-endpolicy,ourformulationoffersanintermediaterepresentationofpolicythatisinterpretable
tohumans,akeyfeatureforhuman-robotinteractionapplications.
Weevaluatedourapproachacrossfourreal-worldtasks, includingbimanualmanipulation, precise
3D manipulation, and long-horizon tasks, using a small number of expert human demonstrations.
We found that the video model consistently outperformed the baseline behavior cloning model in
generalizingtounseenscenarios. Additionally,weanalyzedthemodel’sperformancewhenscaling
downthetrainingdatasetandfoundthatthevideomodelmaintainedstronggeneralizationperfor-
mance even with fewer demonstrations. We will release the code and data for reproducing our
results. Pleaserefertothesupplementarymaterialfortherobotvideos.
2 RelatedWork
Behavior Cloning. Behavior cloning (BC) has emerged as a leading approach in manipulation
tasks,learningpoliciesfromdemonstrationsthroughsupervisedlearning. EarlyBCeffortsinvolved
end-to-end models mapping states to actions, but struggled with multimodal behaviors and high-
precision tasks [6, 7, 8]. Subsequently, Energy-Based Models (EBMs) were explored, predicting
actionsbyminimizingenergyinsequenceoptimization[9,10,11,12]. Recently,conditionalgen-
erative models have shown promise in capturing multimodal demonstration behaviors, enhancing
1Dreamthenimitate
2Human Demonstration Setup Training Dataset of Video Demonstrations Robot Execution Setup
CaS mte ere rao 1 CaS mte ere rao 2 CaS mte ere rao 1 CaS mte ere rao
2
Input v0 Track in Synthesized Video T({vt̂}) Robot Action {at}
…
Robot
Human Tool Tool Video
Input v0 Synthesized Video {vt̂} Diffusion fθ
Video CAD to Track
Diffusion fθ
Fine-tuning Video Generation
Figure2:MethodOverview. Foreachtask,(Step1)wecapturestereocamerarecordingsofhuman
demonstrations of tool use from a top and a side view. (Step 2) The video model is fine-tuned to
recreate these demonstrations from an initial scene image. (Step 3) In a new, unseen scenario, a
stereoimagepairisprovidedtothevideomodeltogenerate( )themanipulation. Thetool’s
trajectoryinthegeneratedvideoistrackedandexecutedbytherobottocompletethetask.
tasksuccessrates[2,3,13]. Differentfrompreviousmodels,weintegratevideopredictionand3D
trackingtopredictactionprediction.
Visual Pretraining for Policy Learning. Prior works have extensively explored various ways to
pre-traintheperceptionmodelinavisuomotorpolicyforlearningmorerobustvisualrepresentation.
One of the most popular pertaining objectives has been video prediction [14, 15, 16, 17, 18]. By
learning to predict future based on the current observation, a model can learn the dynamic and
causalityoftheworldcrucialforphysicalinteraction. Contrastivelearning[15,19,20,21]aswell
as masked autoencoding [22, 23, 24] are two very popular self-supervised learning objectives for
learningvisualrepresentationforrobotics.Besides,anotherlineofwork[25,26,27]studieslearning
ageneralizablerewardfunctionthroughvisualpertainingforreinforcementlearning.
Video Models for Decision-Making. More recently, in light of the fast progress of text-to-
video generative models [28, 29, 30, 31, 32], the hope to leverage internet-scale video pertaining
for robotics is rekindled [33]. A line of work uses video generative model as a world simula-
tor[34,5,28],whichcanpredictfuturevideoconditionedonanaction. Anotherlineofworkuses
video-languagemodelforlonger-horizonplanning[35,36,37]. Differentfrompriorworksonthis
topic which typically use video prediction models as a world model or simulator in order to per-
formplanninganddecisionmaking,Dreamitateusesvideopredictionmodelaspartofavisuomotor
policyanddirectlyusesthepredictedvideoforactionprediction.
3 Approach
3.1 Overview
Givenavideoframeofthescenev , ourgoalistoplanandexecuterobotactionsa ∈ SE(3)via
0 t
videogeneration. Weapproachthisproblemthroughtheframework:
a =T(vˆ) where {vˆ}=f (v ) (1)
t t t θ 0
wheref (·)isagenerativevideomodelwithlearnableparametersθ andvˆ isasynthesizedframe.
θ t
Wetrainf togeneratevideosofahumandoingthetaskusingatoolthatwecantrackin3D.T tracks
the trajectory of the tool in the generated video vˆ, which we can directly use to control the end-
t
effectorstatetoperformmanipulationtasks. Figure2visualizesthisframework. Thiscomposition
ofsynthesize-then-trackfacilitatescheapdatacollectionwithoutteleoperation. Moreover,italigns
withpretainingvideodata,whichiscomposedofmostlyvideosshowinghumanbehavior.
3.2 VideoGeneration
Starting with a video generator pre-trained on internet videos, we fine-tune it on a small video
demonstration dataset in order to combine the priors from internet videos with behaviors from
demonstration videos. To collect videos for fine-tuning, we build a tabletop setup with two cal-
ibrated cameras positioned 45 degrees apart for better visibility. We capture several pair of stereo
videos(vˆ1,vˆ2)∈Vofhumansperformingthetaskusinga3Dprintedtool(seeFigure3fordetails).
3Human Behavior Human Tools Robot Tools Robot Behavior Figure3: Aligning Human and
Robot Behavior via Trackable
Rotation
(Fig. 4) Tools. We use custom tools with
known CAD models for human
Scooping
(Fig. 5) demonstrations, which facilitate
precise 3D tracking, across four
Sweeping real-world manipulation tasks. We
(Fig. 6)
designuniquetools(e.g., aspoonfor
Push-Shape scooping)toshowthegeneralizability
(Fig. 7) ofourapproachforvaryingtasks.
Weusethesedemonstrationvideostofine-tunethevideogenerativemodel. Sincewewillimitate
thetrajectoryofthesynthesizedtoolinthephysicalworld,wegeneratestereovideostomakeaction
possiblein3D.Let(vˆ1,vˆ2)bethepredictedstereovideoframesattimetand(v1,v2)betheground
t t t t
truthstereoframesfromdemonstrationvideos. Weoptimizethevideopredictionobjective:
(cid:34) T (cid:35)
m θinE v∈V (cid:88)(cid:12) (cid:12)(cid:12) (cid:12)(cid:0) vˆ t1−v t1(cid:1)(cid:12) (cid:12)(cid:12) (cid:12) 2+(cid:12) (cid:12)(cid:12) (cid:12)(cid:0) vˆ t2−v t2(cid:1)(cid:12) (cid:12)(cid:12) (cid:12) 2 for {vˆ t}=f θ(v 0) (2)
t=1
We initialize θ to the pre-trained weights learned from large-scale internet video datasets (Stable
VideoDiffusion[29]). Wetrainaseparatef foreachtask(e.g. sweepingorscooping).
θ
Following the implementation from [38], the encoder and decoder are frozen such that only the
spatial/temporalattentionlayersarefine-tuned. Theper-frameimageembeddinginputtothemodel
ismodifiedbasedontheviewingangleoftheoutputframetofacilitatestereovideogeneration.This
ensuresthatthefirsthalfoftheframesgeneratethefirstviewandthehalfgeneratesthesecondview,
usingtheinitialimageembeddingfromtherespectiveview.
Attesttime,wewillbegivenastereoimagepairofthescenethatwewishtherobottomanipulate.
Weapplythefine-tunedvideomodelf(v )andobtainageneratedstereovideoframesvˆ ofthetask
0 t
beingperformed.
3.3 TrackthenAct
Thesynthesizedvideoframes{vˆ}serveastheintermediaterepresentationofourpolicytoobtain
t
an action trajectory, {a }, that we then execute on the robot. Actions a ∈ SE(3) are represented
t t
asthe6Dposeofthetoolrelativetothecamera. ByusingaknownCADmodel,wecanefficiently
and accurately track the tool in each frame of the generated videos and therefore obtain precise
locations of the tool. Each action a in the action trajectory corresponds to one frame from the
i
synthesizedvideo. For3Dconsistency, weusethegeneratedstereopair, alongwiththecalibrated
camera parameters to resolve 6D pose of the tool. For tracking the tool from an RGB image, we
useMegapose[39]andoperateon768×448resolutionvideoframeswithcameraintrinsicderived
from the default values of the Intel Realsense camera. Finally, during execution, the customized
toolis mountedon therobot armand actionsa ,...,a areexecuted bythe robotarm. Theentire
0 T
data-collection, training, and fine-tuning pipeline are agent-independent. This is possible because
we use tool’s as manipulators and track known tools in generated videos, removing the need for
learninganyagent-specificactionspaces.
4 Experiments
In this section, we evaluate the performance of our video model in real-world robot manipulation
tasks. The evaluation, summarized in Table 1, encompasses four distinct tasks including object
rotation, granular material scooping, table top sweeping and shape pushing (see Figure 3 for a vi-
sualization of the used tools). The details of each task are further explained in the corresponding
sectionsbelow.Foranassessmentofourmodel’scapabilities,webenchmarkitsperformanceagainst
DiffusionPolicy[40]asabaseline.
4Input Image Ours (Execution) Diffusion Policy (Execution)
Target
Orientation
… …
Generated Video
… …
… …
Figure4: RotationQualitativeResults. Forthistask,weprovideasinputtheimageontheleftfor
eachrow,withanexampleofasuccessfulrotationoverlaid. Ourvideogeneration-basedapproach
succeeds,whileDiffusionPolicyfailstoselectstablegraspingpointsontheobject,causingittoslip
duringmanipulation. Generatedvideosareindicatedby .
4.1 ExperimentalSetup
Toevaluatethemodel’sgeneralizability,thedatawascollectedbyahumandemonstratoratdifferent
locations for training and evaluation using the same camera setup, though the exact tabletop and
lightingconditionsvaried.Tofurthertestthemodel’srobustnessinreal-worldscenarios,weensured
thatthesetsofobjectsusedintrainingandevaluationarenon-overlapping.Toinitializetooltracking
withMegapose[39],backgroundsubtractionandhandremovalbasedonskincolorwereemployed.
Inrarecaseswherethisprocessfailed,humancorrectionswereappliedtotheboundingboxes(both
forourmethodandforthebaseline).
WeusedthesametrainingdatatotrainDiffusionPolicywithstereoimageinput. Thevideoswere
preprocessedusingMegaposefortooltracking, providingtargettrajectoriesfortraining. Weused
a pretrained ResNet-18 variant [40] as the baseline, as the pretrained CLIP encoder [41] variant
showed lower performance in our tasks. We trained Diffusion Policy for 200 epochs to predict
actionsforthenext12timesteps,maintaininganopen-loopsystemwithoutsubsamplingthetraining
data,similartothevideomodel.
TrainingObjects Demonstrations TestObjects TestTrials
Rotation 31 371 10 40
Scooping 17Bowls,8Particles 368 8Bowls,4Particles 40
Sweeping 6Particles 356 6Particles 40
Push-Shape 26Letters 727 8Shapes 32
Table1: TasksSummary. Wedetailthetrainandtestsetupforeachtaskabove. Foreachtask,we
useadistinctsetofobjectsduringtestingthantraining.
4.2 ObjectRotationTask
Task. We design a rotation task, shown in Figure 4, to test our policy’s ability to coordinate end-
effectorsandchooseappropriategraspingpointstomanipulatereal-worldobjects. Ineachtraining
demonstration,werandomlyplaceanobjectonthetable,anduseagripper,showninFigure3(top),
tograsptheobjectatappropriatepointsandrotateitcounterclockwiseupto45degrees.
Setup. We collect training data with 31 objects, with 14 real-world objects (such as boxes or a
hammer)and17customcoloredshapesmadeoutoffoam. Weuse10unseenreal-worldobjectsfor
evaluation. We mark a trajectory as successful if the robot makes and maintains contact with the
objectthroughouttherotation,androtatesitatleast25degreescounterclockwise.
Results. As shown in Table 2, our policy significantly outperforms Diffusion Policy (92.5% vs.
55%). Fig.4illustratescaseswhereourpolicysucceedswhileDiffusionPolicyfails. Weobserve
5Figure 5: Scooping Qualitative Results. We provide the input image on the left (without the
overlaysprovidedforillustration)toourmodelandDiffusionPolicy,andshowtheoutputtrajectory
on the right. Our approach succeeds in most trials, while Diffusion Policy is often distracted by
otherobjectsinthescene, placingmaterialinthewrongcontainerandfailingtoscoopaccurately.
Generatedvideosareindicatedby .
that Diffusion Policy can fail to move the end-effector into contact with the object. In more chal-
lengingcases,DiffusionPolicyispronetoselectingunstablegraspingpointscausingtheobjectto
slip during manipulation. In contrast, our policy consistently makes contact with the object, with
limitedfailureswhenselectingappropriategraspingpointsforparticularlychallengingshapes,such
asatransparentbagwithtoysinside.
Rotation Scooping Sweeping Push-Shape
Model SuccessRate SuccessRate SuccessRate mIoU Rot.Error
DiffusionPolicy[2] 22/40 22/40 5/40 0.550 48.2◦
Ours 37/40 34/40 37/40 0.731 8.0◦
Table2: QuantitativeResults. WecompareourmethodtoDiffusionPolicyonfourtasksquanti-
tatively. Wereportsuccessrates(successfultrials/totaltrials)forrotation,scooping,andsweeping
tasks. For Push-Shape, we report mean intersection-over-union (mIoU) and average rotation error
(Rot. Error). Ourapproachperformswellacrossalltasks, whereasDiffusionPolicyshowsworse
performanceoverallanddegradesinthemorechallengingsweepingandPush-Shapescenarios.
4.3 GranularMaterialScoopingTask
Task. This task, shown in Figure 5, requires scooping granular material (e.g., beans) from a full
containertoanemptyonewhileavoidingdistractorobjects. Thistaskrequiresthepolicytoperform
precisemanipulationofascoopingtool,identifythefullandtheemptycontainers,alongwiththeir
preciselocations,inarbitrarypositions,andignoredistractors.
Setup. Wecollectdemonstrationswith17bowlsand8coloredbeanswithonly1distractionobject
inthesceneatatime. Attesttime,weuseanunseensetof8bowls,4newcoloredparticles,and5
distractorobjectspertrialsampledfromafixedsetof15everydayobjects. Wemarkatrajectoryas
successfuliftherobottransfersanyparticlestotheemptybowl.
Results. AsshowninTable2(middle),ourpolicysignificantlyoutperformsDiffusionPolicy(85%
vs. 55%). This is a particularly challenging task for our approach, due to the small target (the
scooper) for object tracking and video generation. This demonstrates that stereo video generation
canaccuratelydeterminetheobject’sposetoperform3Dmanipulation. Bycomparison,Diffusion
Policyisoftendistractedbyobjectsinthescene,placingmaterialinthewrongcontainer,andfailing
morefrequentlyinscoopingduetomisjudgingthebowl’sheightonthetable,asillustratedinFig.5.
4.4 TableTopSweepingTask
Task. The sweeping task, shown in Figure 6, requires policies to use a brush to sweep randomly
placed particles to a target location marked by a randomly placed star, while avoiding obstacles.
Thistaskisdesignedtotestpolicy’sabilitytohandlemulti-modaldistributionsinthetrainingdata.
6Input Image Ours (Execution) Diffusion Policy (Execution)
Ob Sje wct
e
t po
t
be Ge Vn ie dr ea oted … …
Target
… …
… …
Figure6: SweepingQualitativeResults. Weprovidetheinputimageontheleft(minusthewhite
andgreencirclesoverlaidforillustration)toourmodelandDiffusionPolicy, andshowtheoutput
trajectories on the right. Our approach again achieves high success rate even on this challenging,
multi-modal task, whereas Diffusion Policy generates trajectories that often collide with obstacles
andfailtosweeptothetarget. Generatedvideosareindicatedby .
Setup. Trainingdataincludes6coloredparticlesand25distractionobjects,with5to6distractions
atatime. Attesttime,weuse6newparticlesand15unseendistractions,with2to4beansand5
distractionsrandomlyplacedonthetable. Thetrainingdatacontainssomevariationswithmultiple
waystoachievethegoal, suchaschoosingwhichparticletosweepinthescene. Duetothesmall
size of the particles, we remove the final pooling layer from Diffusion Policy ResNet-18 encoder
to provide a higher spatial resolution and improve baseline performance. We mark a trajectory as
successfulifanyparticleistransferredtobewithin50mmfromthetarget.
Results. InTable2(right), weobservethatthevideomodelmaintainsstrongperformanceinthis
taskwitha92.5%successrate. Incontrast,DiffusionPolicygeneratestrajectoriesthatoftencollide
with obstacles and fail to sweep to the target (see Fig. 6), achieving only a 12.5% success rate.
This experiment demonstrates that the capitalizing on internet-scale pre-trained video generation
models allows to better handle multi-modal demonstrations and achieve a much larger degree of
generalizationinthischallengingscenario.
4.5 Push-ShapeTask(LongHorizon)
Task. Push-Shape,showninFigure7,isachallengingversionofthelong-horizonPush-Ttask[2]:
weplaceafoamshapeonthetable,andtasktherobotwithpushingtheobjecttoaspecifiedtarget
goal mask (given as input to the policy) over consecutive steps. This is a challenging task as it
requiresadjustingthepositionandorientationoftheshape,requiringpredictingtheshape’smove-
ment, which depends on the contact between the object and the end-effector, as well as material
propertiesofthetable.
Setup. Wetrainwithasetof26foamobjectseachintheshapeofaletterfromthealphabet, and
teston 8unseenfoamshapes (includingdigitsand polygons). As thisisa challenging, multi-step
task,wescorethebestof4rolloutsforeachtrial,andreportthemeanintersect-over-union(mIoU)
withthetargetmaskaswellastheaveragerotationerrorfromthetarget.
Results. As shown in Table 2, over the 32 trials, the video model achieved a significantly higher
mIoUof0.731comparedto0.550mIoUfromDiffusionPolicy,andarotationerrorof8.0degrees
compared to 48.2 degrees from Diffusion Policy. We find that Diffusion Policy tends to push the
objecttothetarget,butfailstoeffectivelyrotatetheobjecttomatchthemask.Bycontrast,thevideo
modelproducesappropriatepushingactionstoadjusttheobject’spositionandorientation,thereby
reducingtherotationerror. QualitativeexamplesofthemodelrolloutsareillustratedinFig.7.
7Input Image Step 1 Step 2 Step 3 Step 4
Ours Generated
(Execution) Video
Target
Diffusion
Policy
(Execution)
Figure 7: Push-Shape Qualitative Results. We provide the image on the left, including the gray
overlayindicatingthetargetobjectpositionandorientation,toourmodelandDiffusionPolicy,and
show output trajectories on the right. Our method produces appropriate pushing actions to adjust
theobject’sposition. Incontrast,DiffusionPolicyfailstoeffectivelyrotatetheobjecttomatchthe
mask. Generatedvideos(toprow,bottomright)areindicatedby .
4.6 PerformanceScalingCurve
Westudiedtheimpactoftrainingsetsizeonthegeneral-
ization ability of our policy and compared it with Dif-
1 Ours
fusion Policy using a rotation task. Both models per- DiffusionPolicy
formed well with the full dataset. When re-trained with 0.8
two-thirds and one-third of the dataset and tested over
0.6
40 trials, the results depicted in Figure 8 indicate that
while Diffusion Policy’s performance declines signifi- 0.4
cantlywithreduceddata. Incontrast,ourmodelremains
0.2
stable and maintains high success rates even with only
one-thirdofthedata.Thishighlightsourmodel’ssuperior
0
generalization, which can be attributed to the extensive 1/3 2/3 Full
TrainingDatasetFraction
pre-trainingonInternet-scalevideogenerationmodels.
Figure8: NumberofDemonstrations.
5 Limitations Ourapproachmaintainsstronggeneral-
ization performance with less training
We highlight a few limitations and directions for future datacomparedtoDiffusionPolicy.
work. By visually tracking the tools used for manipula-
tion, our implementation is limited to generating visually trackable robot actions. Although ad-
vancesinvideogenerationandobjecttrackingmodelswillimproveaccuracy,ourapproachcanfail
whentheend-effectorisheavilyoccluded. Additionally, relianceonrigidtoolslimitstheapplica-
bilityofourapproachtothetaskrequiringfine-grainedcontrol. Finally,videomodelshavehigher
computationalcosts,makingreal-timeclosed-loopcontrolinfeasible,thoughthiscanbemitigated
withrecentadvances [42,43]inacceleratingvideomodelinference.
6 Conclusion
In this work, we investigate how video generative models can be used to learn generalizable vi-
suomotor policies. We propose to fine-tune a video diffusion model on human demonstrations to
synthesizeanexecutionplanintheformofvideosattest-time. Thesesynthesizedexecutionvideos
are then directly used to control the robot. Our key insight is that using common tools allows us
toeasilybridgetheembodimentgapbetweenthehumanhandinthedemonstrationsandtherobot
manipulator in the real-world. Our experiments validate that capitalizing on Internet-scale video
diffusionmodelsallowsourapproachtoachieveamuchlargerdegreeofgeneralizationcompared
topreviousbehaviorcloningmethodslikeDiffusionPolicy.
8
etaRsseccuSAcknowledgments
This research is based on work partially supported by the Toyota Research Institute and the NSF
NRIAward#2132519.
References
[1] A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany,
M. K. Srirama, L. Y. Chen, K. Ellis, P. D. Fagan, J. Hejna, M. Itkina, M. Lepert, Y. J. Ma,
P.T.Miller,J.Wu,S.Belkhale,S.Dass,H.Ha,A.Jain,A.Lee,Y.Lee,M.Memmel,S.Park,
I. Radosavovic, K. Wang, A. Zhan, K. Black, C. Chi, K. B. Hatch, S. Lin, J. Lu, J. Mer-
cat, A. Rehman, P. R. Sanketi, A. Sharma, C. Simpson, Q. Vuong, H. R. Walke, B. Wulfe,
T.Xiao,J.H.Yang,A.Yavary,T.Z.Zhao,C.Agia,R.Baijal,M.G.Castro,D.Chen,Q.Chen,
T. Chung, J. Drake, E. P. Foster, J. Gao, D. A. Herrera, M. Heo, K. Hsu, J. Hu, D. Jackson,
C. Le, Y. Li, K. Lin, R. Lin, Z. Ma, A. Maddukuri, S. Mirchandani, D. Morton, T. Nguyen,
A.O’Neill,R.Scalise,D.Seale,V.Son,S.Tian,E.Tran,A.E.Wang,Y.Wu,A.Xie,J.Yang,
P. Yin, Y. Zhang, O. Bastani, G. Berseth, J. Bohg, K. Goldberg, A. Gupta, A. Gupta, D. Ja-
yaraman,J.J.Lim,J.Malik,R.Mart´ın-Mart´ın,S.Ramamoorthy,D.Sadigh,S.Song,J.Wu,
M.C.Yip, Y.Zhu, T.Kollar, S.Levine, andC.Finn. Droid: Alarge-scalein-the-wildrobot
manipulationdataset,2024.
[2] C.Chi,Z.Xu,S.Feng,E.Cousineau,Y.Du,B.Burchfiel,R.Tedrake,andS.Song. Diffusion
policy: Visuomotorpolicylearningviaactiondiffusion,2024.
[3] T.Z.Zhao, V.Kumar, S.Levine, andC.Finn. Learningfine-grainedbimanualmanipulation
withlow-costhardware. arXivpreprintarXiv:2304.13705,2023.
[4] H.Bharadhwaj,A.Gupta,V.Kumar,andS.Tulsiani. Towardsgeneralizablezero-shotmanip-
ulationviatranslatinghumaninteractionplans. arXivpreprintarXiv:2312.00775,2023.
[5] Y.Du,M.Yang,B.Dai,H.Dai,O.Nachum,J.B.Tenenbaum,D.Schuurmans,andP.Abbeel.
Learninguniversalpoliciesviatext-guidedvideogeneration,2023.
[6] D. A. Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Advances in
neuralinformationprocessingsystems,1,1988.
[7] T. Zhang, Z. McCarthy, O. Jow, D. Lee, X. Chen, K. Goldberg, and P. Abbeel. Deep imita-
tionlearningforcomplexmanipulationtasksfromvirtualrealityteleoperation. In2018IEEE
internationalconferenceonroboticsandautomation(ICRA),pages5628–5635.IEEE,2018.
[8] P.Florence,L.Manuelli,andR.Tedrake.Self-supervisedcorrespondenceinvisuomotorpolicy
learning,2019.
[9] Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F. Huang. A tutorial on energy-based
learning. Predictingstructureddata,1(0),2006.
[10] Y.DuandI.Mordatch. Implicitgenerationandgeneralizationinenergy-basedmodels,2020.
[11] W.Huang,C.Wang,R.Zhang,Y.Li,J.Wu,andL.Fei-Fei. Voxposer: Composable3dvalue
mapsforroboticmanipulationwithlanguagemodels,2023.
[12] P. Florence, C. Lynch, A. Zeng, O. Ramirez, A. Wahid, L. Downs, A. Wong, J. Lee, I. Mor-
datch,andJ.Tompson. Implicitbehavioralcloning,2021.
[13] S.Lee,Y.Wang,H.Etukuru,H.J.Kim,N.M.M.Shafiullah,andL.Pinto.Behaviorgeneration
withlatentactions. arXivpreprintarXiv:2403.03181,2024.
[14] C.Finn,I.Goodfellow,andS.Levine. Unsupervisedlearningforphysicalinteractionthrough
videoprediction,2016.
9[15] P.Sermanet,C.Lynch,Y.Chebotar,J.Hsu,E.Jang,S.Schaal,andS.Levine.Time-contrastive
networks: Self-supervisedlearningfromvideo,2018.
[16] M. Babaeizadeh, C. Finn, D. Erhan, R. H. Campbell, and S. Levine. Stochastic variational
videoprediction. arXivpreprintarXiv:1710.11252,2017.
[17] A.X.Lee,R.Zhang,F.Ebert,P.Abbeel,C.Finn,andS.Levine. Stochasticadversarialvideo
prediction,2018.
[18] D. Suris, R. Liu, and C. Vondrick. Learning the predictability of the future. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages
12607–12617,June2021.
[19] A. Srinivas, M. Laskin, and P. Abbeel. Curl: Contrastive unsupervised representations for
reinforcementlearning,2020.
[20] S.Nair,A.Rajeswaran,V.Kumar,C.Finn,andA.Gupta. R3m: Auniversalvisualrepresen-
tationforrobotmanipulation,2022.
[21] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P.Mishkin, J.Clark, G.Krueger, andI.Sutskever. Learningtransferablevisualmodelsfrom
naturallanguagesupervision,2021.
[22] Y.Seo,D.Hafner,H.Liu,F.Liu,S.James,K.Lee,andP.Abbeel.Maskedworldmodelsforvi-
sualcontrol. InK.Liu,D.Kulic,andJ.Ichnowski,editors,ProceedingsofThe6thConference
onRobotLearning,volume205ofProceedingsofMachineLearningResearch,pages1332–
1344.PMLR,14–18Dec2023. URLhttps://proceedings.mlr.press/v205/seo23a.
html.
[23] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik, and T. Darrell. Real-world robot
learningwithmaskedvisualpre-training. InK.Liu,D.Kulic,andJ.Ichnowski,editors,Pro-
ceedings of The 6th Conference on Robot Learning, volume 205 of Proceedings of Machine
LearningResearch,pages416–426.PMLR,14–18Dec2023. URLhttps://proceedings.
mlr.press/v205/radosavovic23a.html.
[24] I. Radosavovic, B. Shi, L. Fu, K. Goldberg, T. Darrell, and J. Malik. Robot learning with
sensorimotorpre-training,2023.
[25] Y. J. Ma, W. Liang, V. Som, V. Kumar, A. Zhang, O. Bastani, and D. Jayaraman. Liv:
Language-imagerepresentationsandrewardsforroboticcontrol,2023.
[26] A.S. Chen, S. Nair, and C.Finn. Learninggeneralizable roboticreward functionsfrom ”in-
the-wild”humanvideos,2021.
[27] A.Escontrela, A.Adeniji, W.Yan, A.Jain, X.B.Peng, K.Goldberg, Y.Lee, D.Hafner, and
P.Abbeel. Videopredictionmodelsasrewardsforreinforcementlearning,2023.
[28] T. Brooks, B. Peebles, C. Holmes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Tay-
lor, T. Luhman, E. Luhman, C. Ng, R. Wang, and A. Ramesh. Video genera-
tion models as world simulators. 2024. URL https://openai.com/research/
video-generation-models-as-world-simulators.
[29] A.Blattmann,T.Dockhorn,S.Kulal,D.Mendelevitch,M.Kilian,D.Lorenz,Y.Levi,Z.En-
glish,V.Voleti,A.Letts,etal. Stablevideodiffusion: Scalinglatentvideodiffusionmodelsto
largedatasets. arXivpreprintarXiv:2311.15127,2023.
[30] A.Blattmann,R.Rombach,H.Ling,T.Dockhorn,S.W.Kim,S.Fidler,andK.Kreis. Align
yourlatents: High-resolutionvideosynthesiswithlatentdiffusionmodels,2023.
10[31] S. Zhang, J. Wang, Y. Zhang, K. Zhao, H. Yuan, Z. Qin, X. Wang, D. Zhao, and J. Zhou.
I2vgen-xl: High-qualityimage-to-videosynthesisviacascadeddiffusionmodels,2023.
[32] J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole,
M. Norouzi, D. J. Fleet, and T. Salimans. Imagen video: High definition video generation
withdiffusionmodels,2022.
[33] S. Yang, J. Walker, J. Parker-Holder, Y. Du, J. Bruce, A. Barreto, P. Abbeel, and D. Schuur-
mans. Videoasthenewlanguageforreal-worlddecisionmaking,2024.
[34] Z.Yang,Y.Chen,J.Wang,S.Manivasagam,W.-C.Ma,A.J.Yang,andR.Urtasun. Unisim:
Aneuralclosed-loopsensorsimulator,2023.
[35] Y.Du,M.Yang,P.Florence,F.Xia,A.Wahid,B.Ichter,P.Sermanet,T.Yu,P.Abbeel,J.B.
Tenenbaum,L.Kaelbling,A.Zeng,andJ.Tompson. Videolanguageplanning,2023.
[36] A.Ajay,S.Han,Y.Du,S.Li,A.Gupta,T.Jaakkola,J.Tenenbaum,L.Kaelbling,A.Srivastava,
andP.Agrawal. Compositionalfoundationmodelsforhierarchicalplanning,2023.
[37] K. Black, M. Nakamoto, P. Atreya, H. Walke, C. Finn, A. Kumar, and S. Levine. Zero-
shot robotic manipulation with pretrained image-editing diffusion models. arXiv preprint
arXiv:2310.10639,2023.
[38] B.VanHoorick,R.Wu,E.Ozguroglu,K.Sargent,R.Liu,P.Tokmakov,A.Dave,C.Zheng,
andC.Vondrick. Generativecameradolly:Extrememonoculardynamicnovelviewsynthesis.
arXivpreprintarXiv:2405.14868,2024.
[39] Y. Labbe´, L. Manuelli, A. Mousavian, S. Tyree, S. Birchfield, J. Tremblay, J. Carpentier,
M. Aubry, D. Fox, and J. Sivic. Megapose: 6d pose estimation of novel objects via render
&compare. arXivpreprintarXiv:2212.06870,2022.
[40] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song. Diffusion policy:
Visuomotorpolicylearningviaactiondiffusion. arXivpreprintarXiv:2303.04137,2023.
[41] C.Chi,Z.Xu,C.Pan,E.Cousineau,B.Burchfiel,S.Feng,R.Tedrake,andS.Song. Universal
manipulationinterface: In-the-wildrobotteachingwithoutin-the-wildrobots. InProceedings
ofRobotics: ScienceandSystems(RSS),2024.
[42] S.Luo,Y.Tan,S.Patil,D.Gu,P.vonPlaten,A.Passos,L.Huang,J.Li,andH.Zhao.Lcm-lora:
Auniversalstable-diffusionaccelerationmodule. arXivpreprintarXiv:2311.05556,2023.
[43] X. Li, Y. Liu, L. Lian, H. Yang, Z. Dong, D. Kang, S. Zhang, and K. Keutzer. Q-diffusion:
Quantizing diffusion models. In Proceedings of the IEEE/CVF International Conference on
ComputerVision,pages17535–17545,2023.
[44] K.He, X.Zhang, S.Ren, andJ.Sun. Deepresiduallearningforimagerecognition. InPro-
ceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages770–778,
2016.
11A VideoModelImplementation
We adapted the pretrained Stable Video Diffusion model [29], which generates 25-frame videos
at a time. In our adaptation, the first 13 frames correspond to the stereo view 1, and the last 12
frames correspond to the stereo view 2, captured from the two cameras. To condition the video
modeltogenerateastereovideo,wemodifiedtheper-frameimageembeddingbasedontheviewing
angle of each output frame. Since each frame of the stereo videos should be paired but the video
modelgeneratesanoddnumberofframes, thefirstframeofthevideomodeloutputisalwaysthe
same as the input and discarded at test time. The model training hyperparameters are given in
Table3. Duringinference,weuse30denoisingstepswithaconstantclassifier-freeguidanceof1.0.
AdditionalqualitativeresultsforthefourtasksareshowninFig9, 10, 11and 12.
H-Param Res Lr BatchSize TrainSteps ClipDuration Fps MotScr
Rotation(FullDS) 768×448 1e-5 4 16384 2.0 6 200
Rotation(2/3DS) 768×448 1e-5 4 16384 2.0 6 200
Rotation(1/3DS) 768×448 1e-5 3 15360 2.0 6 200
Scooping 768×448 1e-5 4 16384 3.0 5 200
Sweeping 768×448 1e-5 4 16384 3.0 5 200
Push-Shape 768×448 1e-5 4 17408 2.0 6 200
Table 3: Hyperparameters for Video Model Training. Res: image and video resolution, Lr:
learning rate, Batch Size: batch size, Training Steps: training steps for the evaluation checkpoint,
Clip Duration: single demonstration video length in seconds, Fps: the video sub-sampling frame
rateandmodelfpsparameter,MotScr: modelmotionscoreparameter.
B ExperimentalSetup
ThestereocamerasetupconsistsoftwoIntelRealSenseD435icamerasspacedapproximately660
mm apart at a 45◦ angle. The distance between the cameras and the table is about 760 mm. The
real-worlddatacollectionandtherobotexperimentsetupsareshowninFig.17and 18.Thetraining
videosarerecordedataresolutionof1280×720andarethencroppedandresizedtotheappropriate
resolution for model input. The table surface used for data collection and experiments is covered
with a black cloth, which introduces variations in friction and increases uncertainty in the Push-
Shape experiments. For the rotation and scooping experiments, UFACTORY xArm 7 robots are
used, while UR5 robots are used for the sweeping and Push-Shape experiments. For calculating
the mIoU in the Push-Shape experiment, the view from the stereo camera 1 is used. In each trial
withmultiplesteps,theresultingimagewiththehighestIoUwiththetargetisusedtocalculatethe
rotationerror. Insweepingandpush-shapeexperiments,therobotend-effectorheightislimitedto
avoidrobotcollisionwiththetabletop.
C DataCollection
For all tasks, the first frame of the human demonstration video is an image of the scene. The
subsequentframesincludethehumandemonstratorusingthetooltoperformthemanipulation. In
thePush-Shapedemonstration,anobjectispushedtoalocationinmultiplesteps. Thefinalposition
oftheobjectisusedasamaskandblendedwiththeentirevideoforthetargetposition. Theobjects
usedintrainingandtestingfordifferenttasksareshowninFig.13,14,15and 16.
D ObjectTracking
In the videos, the tool is tracked using MegaPose [39]. Utilizing a stereo setup, the center of the
tracked object from each camera are projected into 3D space as a straight line. The translation
componentoftheobjectin3Dspaceisdeterminedbyfindingthemidpointbetweentheprojected
lines from the two cameras. The rotation component of the object is obtained by averaging the
12object rotations from the two views. This refined object pose from the stereo setup enhances the
accuracyoftheobject’sdepthmeasurementfromthecameras. Inthescoopingtask,onlythehandle
of the scooper is tracked to avoid inaccuracies due to occlusion by particles. In the sweeping and
Push-Shape tasks, the tool without the handle is tracked, as the handle is occluded by the human
hand.ToobtainthetooltrajectoriesfortrainingDiffusionPolicy,thesamestereotrackingisapplied
tothedemonstrationvideos.
E DiffusionPolicyBaseline
We use a CNN-based Diffusion Policy as our baseline, employing two pretrained ResNet-18 [44]
image encoders to process the stereo images of the scene. The input images have a resolution of
384×224,similartotheoriginalimplementationresolution. Wefoundthathigherresolutioninput
imagesdidnotimprovemodelperformance.
Input Image Ours (Trajectory) Diffusion Policy (Trajectory)
Start End
Figure9: AdditionalRotationQualitativeResults. Thetrajectoriesoftheend-effectorsarepro-
jectedontotheinputimage.
13Input Image Ours (Trajectory) Diffusion Policy (Trajectory)
Start End
Figure10:AdditionalScoopingQualitativeResults.Thetrajectoryoftheend-effectorisprojected
ontotheinputimage.
Input Image Ours (Trajectory) Diffusion Policy (Trajectory)
Start End
Figure 11: Additional Sweeping Qualitative Results. The trajectory of the end-effector is pro-
jectedontotheinputimage.
14Ours Diffusion Policy Ours Diffusion Policy Ours Diffusion Policy Ours Diffusion Policy
Figure12: AdditionalPush-ShapeQualitativeResults. Thetrajectoryoftheend-effectorispro-
jectedontotheinputimage.
Training Objects Testing Objects
Figure13:RotationObjects.Thetrainingsetincludes14real-worldobjectsand17customcolored
shapesmadeoutoffoam. Thetestingsetincludes10challengingreal-worldobjects.
15
egamI
laniF
4
petS
3
petS
2
petS
1
petS
egamI
tupnI
dnE
tratSTraining Objects Testing Objects
Figure 14: Scooping Objects. The training set includes 17 bowls, 8 colored beans and 4 real-
world objects. The testing set includes 8 bowls, 4 colored particles and 15 real-world distraction
objects. Duringdatacollection,boththereal-worldobjectsandinvertedbowlsareusedasthesingle
distractionobjectinthescene.
Training Objects Testing Objects
Figure15: SweepingObjects. Thetrainingsetincludes25distractionobjectsand6coloredbeans,
with5to6distractionsinthesceneatatimeduringdatacollection. Thetestingsetincludes15real-
worlddistractionobjectsand6coloredparticles. Thestarisusedasthetargetintheexperiments.
Training Objects Testing Objects
Figure 16: Push-Shape Objects. The training set includes all 26 capital letters of the alphabet,
whilethetestingsetconsistsof8shapes,includingdigitsandpolygons.
16Stereo
Camera 1
Stereo
Camera 2
Human
Tool
Figure 17: Real-world Data Collection Setup. The data collection setup has the same camera
arrangementastherobotexperimentsetup.
Stereo
Camera 1
Stereo
Camera 2
Robot
Tools
Figure18:Real-worldRobotExperimentSetup.Therobotexperimentsetupincludesthe3robots
toperformalltheexperiments.
17