Losing Visual Needles in Image Haystacks: Vision Language Models are
Easily Distracted in Short and Long Contexts
AdityaSharma∗ MichaelSaxon∗ WilliamYangWang
UniversityofCalifornia,SantaBarbara
LoCoVQA.github.io
OK-VQA MMStar MMBench
0.6 0.4 0.6
0.4
0.3 0.4
0.2
0.2
0.2
0.0
0 10 20 30 0 10 20 30 0 10 20 30
Visual Context Length Visual Context Length Visual Context Length
LLaVA-1.5 (cmp) Moondream2 (cmp) GPT-4V (cmp) Gemini 1.0 (cmp) Gemini 1.5 (cmp) PaliGemma (cmp)
LLaVA-1.6 (cmp) Mantis (int) GPT-4V (int) Gemini 1.0 (int) Gemini 1.5 (int) Random
Figure1: Theimpactofvisualcontextonvision-languagemodels(VLMs)inourmodified,multi-imageversionsof
theOK-VQA,MMStar,andMMBenchevaluationbenchmarks. Distractorimagesaroundthetargetimageincrease
thevisualcontextlengthneededtoanswerthequestions. VLMperformanceexhibitsanexponentialdecayagainst
distractorcount,evidentinbothsinglecomposed(cmp)andmultipleinterleaved(int)inputimageconfigurations.
Abstract 2023), researchers have developed robust evalua-
tions to compare their capabilities against those
WepresentLOCOVQA,adynamicbenchmark ofproprietaryLMs(Reidetal.,2024;Jiangetal.,
generatorforevaluatinglong-contextextractive
2024a). However,untilveryrecently(Jiangetal.,
reasoninginvisionlanguagemodels(VLMs).
2024b), the small set of long-context VLMs sup-
LOCOVQAaugmentstestexamplesformathe-
portingmulti-imageinputhavebeeninaccessibleto
maticalreasoning,VQA,andcharacterrecogni-
tiontaskswithincreasinglylongvisualcontexts thepublic(OpenAIetal.,2023),soevaluationprac-
composed of both in-distribution and out-of- ticesforlong-contextVLMs(Hongetal.,2023;Li
distributiondistractorimages. etal.,2023b;Daietal.,2024)areintheirinfancy
(Zhangetal.,2024). Mostreasoning-basedVLM
Across these tasks, a diverse set of VLMs
rapidlyloseperformanceasthevisualcontext evalshaveanaloguesintext-domainLMtasks. For
length grows, often exhibiting a striking ex- example,imagecaptioning(Lietal.,2022;Wang
ponentialdecaytrend. Thistestassesseshow etal.,2022;Nguyenetal.,2022)canbelikenedto
wellVLMscanignoreirrelevantinformation textsummarization,andvisualquestionanswering
when answering queries—a task that is quite
(Chen et al., 2022; Wang et al., 2023; Xu et al.,
easy for language models (LMs) in the text
2023a)totextQA.
domain—demonstrating that current state-of-
the-artVLMslackthisessentialcapabilityfor Weaimtobridgegapsinmulti-imageVLMeval-
manylong-contextapplications. uation by drawing analogies to established long-
contextLMtasks. Long-documentQA(Fanetal.,
1 Introduction 2019)isaneasy-to-evaluatetestthatdirectlyshow-
casesmodelperformanceonausefulapplication,
Long-context vision-language models (VLMs)
andhintsatitspotentialinretrieval-augmentedgen-
which accept multiple image inputs are pushing
eration(RAG)morebroadly. Centraltotheseeval-
the boundaries of multimodal reasoning applica-
uationsisthemodel’sabilitytorecognizewhich
tions,butevaluationmethodshavenotkeptup. As
elements in a lengthy input are necessary for
open-weightlong-contextlanguagemodels(LMs)
answeringaqueryandwhicharenot,askillwe
havebeenaroundforthelastfewyears(Dongetal.,
termextractivereasoning. Dolong-contextVLMs
*Equalcontribution. alsoexhibittheseextractivereasoningcapabilities?
1
4202
nuJ
42
]LC.sc[
1v15861.6042:viXra
)%(
ycaruccA
)%(
ycaruccA
)%(
ycaruccAImage(X) Question(Q) Answer(A)
RandomlychosenfromdatasetD Whatcartooncharacterisonthiscake? WinniethePooh
(a)1-ImageContextLength (b)4-ImageContextLength (c)9-ImageContextLength
Figure2: Exampleof Image(X) correspondingtoquestion-answerpair(Q,A)underincreasingvisualcontext
lengthsinthecomposedsetting. Thegreenboxisforillustrationpurposes;notincludedinmodelinputs.
WhilecurrentVLMbenchmarksalignwithlong- • Use LOCOVQA to create a visual needle in a
document summarization tasks, such as whole- haystacktest,revealingsubstantialpositionalbi-
videoquestionanswering(Huangetal.,2021)or asesinSOTAVLMsinextractivereasoning.
summarization(DilawariandKhan,2019),noexist-
ingbenchmarkseffectivelycaptureaVLM’sability 2 RelatedWork
tofilteroutirrelevantimagesinalongcontextto
Text-based long-context tasks such as long-
reasonoransweraquery—essentially,perform
document question-answering (QA) (Chen et al.,
extractivereasoningoverimages.
2023), summarization (Phang et al., 2022), and
Measuring extractive reasoning over image se-
retrieval-augmented generation (RAG) (Lewis
quences is crucial. Just as extractive textual rea-
et al., 2020; Xu et al., 2023b) offer analogues to
soningfacilitatestext-domainRAG,multi-modal
long-context reasoning in VLMs. Many of these
RAGdemandsthatVLMsefficientlyreasonover
tasks(e.g.,QA,RAG)requirelanguageextractive
andextractinformationfromdocumentsfeaturing
reasoning,asdiscussedabove.
multipleimages. Similarly,videoQAnecessitates
Few VLM long-context evals measure extrac-
thatmodelsfocussolelyonframescontainingrele-
tivereasoning. ExistingVQAbenchmarksinvolv-
vantinformation,muchlikelong-documentQA.
ingdistractorinformation(therebyextraction)do
Weintroduce LOCOVQA,abenchmarkgenera-
notfocusonlongcontexts,andlong-contextVQA
torforLongContextVisualQuestionAnswering
benchmarksdonotinvolvedistractors. Multipan-
(VQA)withdistractors. LOCOVQA enablesthe
elVQA(Fanetal.,2024)includesdistractorinfor-
creationoflong-contextimageunderstandingeval-
mation,butonlywithinsingleinputimages. Some
uationsfromanyimagecomprehensiondatasetby
VLM evaluations focused on hallucinations indi-
presenting a sequence with the question-relevant
rectlycaptureanotionofdistraction(Ghoshetal.,
image alongside a configurable set of visual dis-
2024; Favero et al., 2024) but do not explicitly
tractors. This allows us to accurately assess how
measureitalone. MILEBench(Songetal.,2024)
effectivelyVLMsextractonlythepertinentinfor-
evaluatesVQAinlongcontexts,butonlyonnon-
mationtoaquerywithinaclutteredcontext.
distractor tasks such as video summarization or
WefindthateventopproprietaryVLMsstruggle
differencedetection,whereallinputsarerelevant.
withthiscapability,evenovershortvisualcontexts,
Needle-in-a-haystack evaluation tasks (asking
likelyduetofundamentaldeficienciesintheirtrain-
models to recover hidden passphrases at various
ingobjectives. Byunveilingthis LOCOVQA iden-
positions)dorequirelong-contextextraction,but
tifiesacrucialareaforperformanceimprovement
reasoningisnotinvolved. Gemini1.5(Reidetal.,
infutureVLMs. Insummary,we:
2024)extendedthistasktovideocomprehension
• Evaluatelong-contextextractivereasoningacross by hiding a simple passphrase in many positions
awidearrayofVLMsusing LOCOVQA. withinavideo. Wangetal.(2024)presentastatic
• Find that all existing VLMs exhibit significant benchmarkconcurrentlytoourscenteredonmulti-
fragilitywithincreasingdistractorcontextsizes. modalneedle-in-a-haystack.
2(a)1digit:[2] (b)4digits:[6, 6, 1, 0] (c)8digits:[9, 5, 2, 5, 3, 7, 5, 6]
Figure3: EachsubfigurerepresentsavariablenumberofhiddenMNISTdigits(1,4,8)ina9-imagecontextlength.
3 LOCOVQA GenerationMethod istheonlyLOCOVQAexpansionthatmakessense:
few VQA samples can be combined such that a
SamplessynthesizedthroughLOCOVQA contain
plausiblenewQApairrequiringinformationfrom
oneormorecontentimagesX correspondingtoa
multipleimages. Sincemostinterleavedmodelswe
questionandanswerpair(Q,A). Contentimages
evaluatesupport36orfewerimagesassequential
canbesampledfromvariousimagecomprehension
inputswithoutmodification,wedonotevaluateany
benchmark,suchasOK-VQA(Marinoetal.,2019),
modelswithcontextlengthsbeyond36. However,
MMStar(Chenetal.,2024),MNIST(LeCunetal.,
LOCOVQA scales to any size. For the single-
1998),andothers. Alongsidethecontentimage(s),
contextreasoningtasks,weexclusivelysamplethe
each sample includes up to 35 distractor images,
distractorsin-distribution.
whichareeithersampledin-distributionfromthe
contentimageset(ensuringnocontentimagecolli- 3.1.1 OK-VQA
sionstopreventambiguity,asdescribedin§3.1.3)
OK-VQA(Marinoetal.,2019)isasingle-image
or out-of-distribution from other image sets such
visualquestionansweringdatasetcontaining5,072
asMSCOCO(Linetal.,2014).
question-answer-image triples. It requires exter-
Samplescanbearrangedasinterleavedimage
nal knowledge to reason beyond the image. LO-
sequences for VLMs that accept multi-image in-
COVQA generates in-distribution long-context
putsorascompositeimagesarrangedinagrid,as
OK-VQA samples, ensuring that no content im-
depicted in Figure 2. For all models capable of
ageshaveconceptcollisionsthatmaycomplicate
ingestinginterleavedsequences,weevaluateboth
evaluation. Forinstance,thequestionaboutachar-
interleavedandcompositeexamples.
acter on top of a cake, as shown in Figure 2, is
Visual context refers to the visual elements sampledfromOK-VQA.
withinanimageorsequenceofimagesthatarerele-
Since OK-VQA is an open-domain, free-form
vanttoansweringaquestion. Thisincludesthecon-
answerdataset, wescorethesamplesusingthree
tentimageanddistractorimages. Thechallengeis
metrics: exact match (full score if the model’s
toidentifyandfocusonthepertinentdetailswhile
response contains any ground truth answer as a
ignoring irrelevant information. We can sample
substring),andcontinuoustextgenerationmetrics
images both in-distribution to create challenging
BERTScore (Zhang et al., 2020) and ROUGE
L
visualcontextsorout-of-distributiontoisolateex-
(Lin,2004)betweencandidatesandreferences.
tractioncapabilitiesinthesimplestsetting.
Although this method can be applied to any 3.1.2 MMStar
vision-languagedataset,wewilldescribethethree MMStar(Chenetal.,2024)isamulti-domainvi-
specifictasksforwhichwegeneratedlong-context sualquestionansweringdatasetcombiningexam-
visualdistractorbenchmarks. plesfromvariousexistingdatasets: 424questions
fromSEEDBench(Lietal.,2023a),366questions
3.1 Single-imageReasoningTasks
fromMMBench(Liuetal.,2023b),100questions
First,wediscussthevisualreasoningbenchmarks from MMMU (Yue et al., 2023), 345 questions
which we expand into long-context samples con- fromMathVista(Luetal.,2023),69examplesfrom
taining one content image per sample. For most ScienceQA (Lu et al., 2022), and 196 examples
questionansweringandvisualreasoningtasks,this fromAI2D(Kembhavietal.,2016). MMStarcon-
3tains1,500high-qualitymultiple-choicequestions Vision-LanguageModel Context BaseLM ↓sample
thatrequirevisualinformationfromtheimagesto Single-ImageInputVLMs
answer,afilteringstepnotinitiallyperformedon Moondream2-1.6B 2K Phi-1.5 ✓
LLaVA-1.5-7B 4K Vicuna ✓
the source datasets. For example, over 50% of
LLaVA-1.6-7B 8K Mistral ✓
ScienceQAquestionscanbesolvedbyatext-only PaliGemma-3B 8K Gemma
LLM(Chenetal.,2024). SimilartoOK-VQA,we Multi-ImageInputVLMs
Mantis-Bakllava-7B 8K Mistral ✓
generate LOCOVQA samples for MMStar using
Mantis-Idefics2-8B 8K Mistral ✓
thecollisionfilteringtechniquetoproducepseudo- Gemini1.0ProVision 32K Gemini
documentscomposedofmultipleexampleimages. GPT-4Vision 128K GPT-4
Gemini1.5Flash 1M Gemini
As a multiple choice dataset, scoring MMStar
is more straightforward. Full details on how we
Table1: Overviewof open-sourceand proprietary
faithfullyextractmultiplechoiceanswersfromthe vision-languagemodels(VLMs). Downsamplingmod-
modelsisprovidedinAppendixD. els(↓sample)rescaleinputimagesinthesingle-image
settingratherthantakeasmanyViTinputsasnecessary.
3.1.3 FilteringCollisionsin LOCOVQA
Toaddresstheproblemofcontent-distractorcolli- images. Theremainingdistractorimagesareran-
sions—wheremultiplesimilarin-distributionim- domly sampled from a subset of 5K high-quality
ages in the visual context make the QA pair MS COCO (Lin et al., 2014) validation images.
ambiguous—weimplementarobustLM-basedfil- TheVLMisthenpromptedtolistallhandwritten
teringmethod. Foreachvisualcontextimage,we digitspresentinthesequence.
promptGPT-4tolistthetopfiveentities;ifthereis Byvaryingthenumberofdigitsinthesequence,
overlap,weconsiderthequestionpotentiallyam- we can dynamically adjust the difficulty level of
biguous. Detailedimplementationsandexamples the multi-image distractor OCR task. Figure 3
ofourfilteringmethodareprovidedinAppendixC. illustrates examples with 1, 4, and 8 digits in a
Tovalidatethisapproach,wemanuallyassesseda 9-imagecontext. Anoutputisconsideredcorrect
subsetofLOCOVQA generationsandfounditto onlyifthestoredstringofgenerateddigitsexactly
beconsistent,withnosuchcollisions. matchesthegroundtruth,withnopartialcredit.
3.2 Multi-imageReasoningTasks 4 Experiments
In §3.1, we explored tests designed to evaluate
Weevaluatetheperformanceofninecurrentvision-
whether VLMs can extract a single relevant im-
language models on our LOCOVQA-generated
age from a sequence to answer a query, thereby
benchmarks. The open-weight models tested are
probing their long-context reasoning capabilities.
Moondream2 (Moondream, 2024), LLaVA-1.5
ExtendingthistotesthowwellVLMscanextract
(Liu et al., 2023a), LLaVA-1.6 (Liu et al., 2024),
informationfromamulti-imagesequenceisanatu-
PaliGemma-3b (Google, 2024), and two Mantis
ralprogression. However,VQAexamplescannot
variants (Jiang et al., 2024b). Additional details
be easily combined in a way that requires multi-
areprovidedinAppendixA. BothMantisvariants
ple images to answer a single question. There-
areVLMsfurthertunedoninterleavedmulti-image
fore, we turn to constructing “sequential VQA”
instructionfollowing. Thethreeproprietarymod-
setsusingsynthetictasks. Opticalcharacterrecog-
elsweevaluateareGPT-4V(Achiametal.,2023),
nition (OCR) is a straightforward task to convert
Gemini 1.0 Pro Vision (Team et al., 2023), and
intomulti-imagequestionansweringbyincluding
Gemini1.5Flash(Reidetal.,2024). Table1show-
multipleOCRexamplesasinterleavedimagesand
casesallthemodelsalongwiththemodelcontext
askingtheVLMtolistallthetext.
sizesandimagedownsampling.
3.2.1 MNIST
Interleaved image support. All closed-source
WeuseMNIST(LeCunetal.,1998)asitisacanon- modelsandMantissupportmulti-imageinterleaved
icaldatasetforOCR.Foradesiredvisualcontext inputs,whiletheothersonlysupportsingleimages.
length, we sample between 1 and 8 randomly- Formulti-imagemodels,weevaluateboththecom-
colored digits from the MNIST training set of posed (cmp) & interleaved (int) settings. For
around60Kimages,resizingthemtobetween1/6 single-imagemodels,weonlytestthecomposed
and 1/2 of the maximum height of other context (cmp)setting.
4SeedBench MMBench MMMU
0.4
0.4 0.6
0.3
0.3
0.4 0.2
0.2 0.1
0.2
0 10 20 30 0 10 20 30 0 10 20 30
Visual Context Length Visual Context Length Visual Context Length
MathVista ScienceQA AI2D
0.4 0.8 0.5
0.6 0.4 0.3
0.4 0.3
0.2
0.2 0.2
0.1
0.1
0 10 20 30 0 10 20 30 0 10 20 30
Visual Context Length Visual Context Length Visual Context Length
OK-VQA OK-VQA BERT OK-VQA ROUGE
0.6
0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0 10 20 30 0 10 20 30 0 10 20 30
Visual Context Length Visual Context Length Visual Context Length
LLaVA-1.5 (cmp) Moondream2 (cmp) GPT-4V (cmp) Gemini 1.0 (cmp) Gemini 1.5 (cmp) PaliGemma (cmp)
LLaVA-1.6 (cmp) Mantis (int) GPT-4V (int) Gemini 1.0 (int) Gemini 1.5 (int) Random
Figure4:VLMPerformanceonMMStarandOK-VQA.Notetheclearlydecliningexponentialfittrendsformanyof
themodels. The(model,task)pairsforwhichthesetrendsdonotholdbyandlargearebelowtherandombaseline.
Downsamplingimages. Somemodelsacceptim- withincreasingvisualcontextlength. Correlation
agesofarbitraryresolution,whileothersautomati- coefficients, r2, and p-values for each trendline
callydownscaleinputs. Thisdifferenceiscrucial, are reported in Table 3, Appendix D. In general,
especiallyinthecmpsetting,asincreasingthevi- the closed-source models outperform their open-
sualcontextcanleadtoinformationloss. Forthe weightcounterparts,especiallyonmultiple-choice
downsamplingmodelsthatsupportbothcmpand tasks. Amongtheopen-weightmodels,PaliGemma
intsettings,weassessboth. Anyperformancedif- performs the best, likely due to its substantially
ferencesbetweenthesesettingswouldhighlightthe moreextensivetrainingonvision-languagetasks
impactofdownsamplingonthecmpsetting. comparedtoitscounterpartslikeLLavaorMantis.
On several multiple-choice tasks, Mantis (the
5 Results
interleaved open-weight model) outperforms the
Figure4illustrateshowmodelperformance(across otheropen-weightmodelsthatrelysolelyoncom-
10experiments,inbothcomposedandinterleaved positeinputs. Thisislikelyduetotheimagesub-
settings) changes with increasing visual context sampling required by several of the other open-
lengths on single-image LOCOVQA tasks. The weightmodelsnegativelyimpactingperformance
first two rows display results from the MMStar whenusingcompositeinputs. However,ontheOK-
dataset, which contains filtered-image-necessary VQA task, Mantis and Moondream are the least
examples from six other datasets (Chen et al., performant, even at low context lengths. These
2024), with titles in yellow and random guess modelswerelikelynottrainedonvisualquestion-
thresholds indicated by dotted black lines. The answeringtaskstothesameextentasLLaVAvari-
bottomrowpresentsOK-VQAscoresusingthree antsduringtheirinstructiontuningsteps.
scoringmetrics,withtitlesinsalmon. The most noteworthy point is this: the expo-
Across all models on OK-VQA and the ma- nentialdecaytrendholdsequallywellininter-
jority on MMStar subsets for SeedBench, MM- leaved and composed settings. This indicates
Bench,ScienceQA,andAI2D,weobservestriking that the performance-visual context length trend
exponential decay trends in model performance is fundamental and cannot be attributed solely
5
)%(
ycaruccA
)%(
ycaruccA
)%(
ycaruccA
)%(
ycaruccA
)%(
ycaruccA
erocS
TREB
)%(
ycaruccA
)%(
ycaruccA
erocS
EGUOR1-Image Context 9-Image Context 25-Image Context
MMStar MMStar MMStar
OK-VQA OK-VQA OK-VQA
SEEDBench SEEDBench SEEDBench
ROUGE ROUGE ROUGE
OK OK OK
VQA MMBenchVQA MMBenchVQA MMBench
BERT BERT BERT
OK-VQA MMMUOK-VQA MMMUOK-VQA MMMU
AI2D MathVista AI2D MathVista AI2D MathVista
ScienceQA ScienceQA ScienceQA
LLaVA-1.5 LLaVA-1.6 GPT-4V (int) PaliGemma Gemini 1.5 (cmp)
Mantis GPT-4V (cmp) Gemini 1.0 (cmp) MoonDream2 Gemini 1.5 (int)
Figure5: RadarplotsofVLMperformanceacross8multimodalbenchmarkswithvariedvisualcontextlengths.
9-Image Context 16-Image Context 25-Image Context 36-Image Context
0.7 0.7 0.7 0.7
0.6 0.6 0.6 0.6
0.5 0.5 0.5 0.5
0.4 0.4 0.4 0.4
0.3 0.3 0.3 0.3
0.2 0.2 0.2 0.2
0.1 0.1 0.1 0.1
0.0 0.0 0.0 0.0
2 4 6 8 2 4 6 8 2 4 6 8 2 4 6 8
# of Digits # of Digits # of Digits # of Digits
LLaVA-1.5 Mantis GPT-4V (cmp) Gemini 1.5 Flash (cmp) PaliGemma
LLaVA-1.6 Moondream2 GPT-4V (int) Gemini 1.5 Flash (int) Random
Figure6: VLMPerformanceontheMNIST-Digitstranscriptiontaskasafunctionof#ofdigitstotranscribe. These
plotshaveadifferentx-axisthantheplotsinFigure1andFigure4:ratherthantherelationshipbetweencontextsize
andperformance,weareassessingtherelationshipbetween"taskdifficulty"andperformance,atfourcontextsizes.
to downsampling effects in the composite set- Note the x-axis in this figure is changed; it is
ting. Furthermore, for the models tested in both checkingforafixedcontextlengththerelation-
conditions(GPT-4VandGemini),thesametrends ship to number of digits. While there is a trend
are observed in both interleaved and composite ofdecreasingperformancewithanincreasingnum-
settings. Sub-chanceperformanceonthemultiple- berofimages,itdoesnotfollowasimplepattern
choice question-answering datasets, particularly likethecontext-lengthtrend. Forexample,Gemini
for the closed-weight models, occur due to high 1.5experiencesminimalperformancedegradation
refusalrates. Thismayillustratehow“alignment evenasthetargetdigitlengthincreases.
hampering”canhinderperformance. Characterizing difficulty as a function of digit
length is complex—as digit counts increase in a
TaskwisePerformancebycontextlength. Fig-
fixedcontextwindow,thetheoutputlabelsearch
ure5illustratesthemodels’performanceatcontext
space grows, while the ratio of relevant images
lengths of 1, 9, and 25 for each task, to compare
toirrelevantimagesincreases. Thismayexplain
the relative advantages different models have on
whysomemodelshaveconsistentorevenincreas-
varioustasks. Forexample,PaliGemmaexcelson
ingperformanceas#digitsincreases. Analyzing
OK-VQA compared to the other models, while
whydifferentmodelshandletheseaxesofdifficulty
MantisperformaswellonAI2D.Thesedifferences
differentlyisaninterestingfuturedirection.
arelikelyduetovariationsintrainingtasks.
However,akintothesingle-imagetasks,increas-
PerformanceonMulti-imageTasks. Figure6 ingtheoverallvisualcontextlength(seeninsame-
presents model performance on the MNIST tran- color,xvaluepointsacrossplots),makesthetask
scription task by number of digits to transcribe. moredifficult,albeitwithoutasclearacorrelation.
6
)%(
ycaruccA
)%(
ycaruccA
)%(
ycaruccA
)%(
ycaruccA1-Image 4-Image 9-Image 1-Image 4-Image 9-Image 1-Image 4-Image 9-Image
10 10 10
0 0 9 0 0 9 0 0 9
0 1 8 0 1 8 0 1 8
1 7 1 7 1 7
2 6 2 6 2 6
16-Im0age 250-Imag1e 306-Im1ag2e 5 16-Im0age 250-Imag1e 306-Im1ag2e 5 16-Im0age 250-Imag1e 306-Im1ag2e 5
0 0 0 4 0 0 0 4 0 0 0 4
1 1 1 2 3 1 1 1 2 3 1 1 1 2 3
2 2 3 3 4 12 2 2 3 3 4 12 2 2 3 3 4 12
3 4 5 3 4 5 3 4 5
0 1 2 3 0 1 2 3 4 0 2 4 0 0 1 2 3 0 1 2 3 4 0 2 4 0 0 1 2 3 0 1 2 3 4 0 2 4 0
(a)ComposedHaystack(LLaVA1.6) (b)ComposedHaystack(GPT-4V) (c)ComposedHaystack(Gemini1.5)
0% 10 0% 10 0% 10
(Top) (Top) (Top)
9 9 9
8 8 8
25% 25% 25%
7 7 7
6 6 6
50% 5 50% 5 50% 5
4 4 4
3 3 3
75% 75% 75%
2 2 2
1 1 1
100% 100% 100%
(Bottom) 0 (Bottom) 0 (Bottom) 0
1 4 9 16 25 36 1 4 9 16 25 36 1 4 9 16 25 36
Visual Context Length Visual Context Length Visual Context Length
(d)InterleavedHaystack(Mantis) (e)InterleavedHaystack(GPT-4V) (f)InterleavedHaystack(Gemini1.5)
Figure7:EvaluationoftheVisualNeedleinaHaystacktaskusingGPT-4V,best-performingVLM,conductedunder
bothcomposedandinterleavedhaystacksettings. Retrievalaccuracymeasuresthefrequencyofcorrectanswers
producedbytheVLM,inthiscaseidentifyingtheMNISTdigit. Inthecomposedsetting,retrievalaccuracyis
measuredbyplacingtheneedleateachcell. Intheinterleavedsetting,needledepthsignifiesthepositionwithinthe
imagesequence,with0%representingthefirstimageand100%representingthelast. Ourevaluationhighlightsa
consistentdeclineinretrievalaccuracyasthevisualcontextlengthincreases.
6 Ablation: NeedleinaHaystack LLaVA1.6andMantisasanalogousmethodsbe-
tweentheinterleavedandcomposedsettings.
Asperformancewithincontextwindowsdecreases, Acrossthethreecomposedtests(subfiguresa,b,
a natural question arises: Are performance fail- andc),wedonotfindasystematicbiastowardany
uresequallydistributedacrossthevisualcontext particularpositiononthesingle-digitrecognition
range? Toinvestigatethis,weadaptourMNIST- task. Surprisingly, LLaVA 1.6 performance the
basedvisualcontextOCRtaskintoavisualneedle best,probablyduetomoreMNIST-likeOCRdata
inahaystacktask. Needleinahaystacktestsare initstrainingmixcomparedtotheothers.
a common minimal test of long-context capabili- However,wefindstrongsystematicbiaseswith
ties in LMs (Kamradt, 2023), involving hiding a respecttopositionintheinterleavedsettingforall
passphrase,suchastheword“needle,”invarious models (subfigures d, e, and f). Mantis shows a
positions within a long document and asking the preference for late positions in sequences of any
modeltoretrieveit. Toadaptthisconcept,wemod- length,whileGPT-4VandGeminiexhibitweakbi-
ified our single-digit MNIST task by sampling a asestowardsearlypositions. Giventhatthebiases
setof10coloredMNISTdigits(oneforeachnum- are evident even in the relatively simple MNIST-
ber,e.g. blue3,green7,etc.) andhidingthemin baseOCRtask, itislikelythatthiseffectplaysa
eachpossiblepositionwithinbothinterleavedand significant role in the performance penalty these
composedvisualcontextsequences. modelsexperiencewithlongercontexts.
Byassessingamodel’ssingle-digitrecoveryrate Theseresultsaresupportedbysimilarfindingsin
at each position, we can identify any systematic fromtheimage-needletestfromWangetal.(2024).
biasinextractioncapabilitiesbypostition. Figure7 Theyalsofoundthat—contrarytoGoogle’sclaims
presentstheperformanceofasubsetofthetested tosignificantneedle-in-a-haystackperformancefor
VLMs on the composed and interleaved MNIST GeminiReidetal.(2024)—fornon-trivialhidden
visualneedleinahaystacktasks. WetestGPT-4V visualinformationtaskscurrentSOTAVLMsare
and Gemini 1.5 Flash in both settings, and treat woefullynon-performant.
7
htpeD
eldeeN
laveirteR
fo
ycaruccA
laveirteR
fo
ycaruccA
htpeD
eldeeN
laveirteR
fo
ycaruccA
laveirteR
fo
ycaruccA
htpeD
eldeeN
laveirteR
fo
ycaruccA
laveirteR
fo
ycaruccA7 Discussion thanGPT-4Vonthecompositehaystacktaskatall
sizes. Thisoutcomewasdrivenbymultiplefactors:
Whydoweobservesuchstrikingexponentialde-
GPT-4V tends to refuses some tasks rather than
cayinperformancewithincreasingvisualcontext
guessing, while LLaVA always provides a guess.
length? The fact that the trend is consistent in
Furthermore,GPT4Voften“hallucinates”multiple
bothinterleavedandcompositesettingsthatdown-
numbersinsteadofjustone.
sampling effects in the latter are not the primary
cause of performance decay. There may be an Possibility of memorization Some of the sur-
information-theoreticinterpretationofthisbehav- prisingperformanceresultsaredrivenbythetrain-
ior: anincreasingsignal-to-noiseratio. Asthevi- ing mix. For example, Mantis’s significant lead
sualcontextlengthincreases,theamountofsignal onAI2D,andLLaVA’sstrongperformanceonthe
(relevantinformation)remainsconstant,whilethe MNISTtaskmayresultfromhavingmorerelevant
amountofnoise(distractorinformation)increases. dataorthosespecifictrainingsetsincludedintheir
However,thissignal-to-noiseratioproblemalso data. However,evenconsideringthatsomemodels
exists in text-only tasks, yet long-context LLMs weretrainedonthesetasks,thepronounceddrops
perform well on needle in a haystack and long- ingeneralizationperformanceasthecontextlength
contextQAandsummarizationtasks,underlying increases are even more striking. This illustrates
their performance in retrieval-augmented genera- thatcurrentVLMsfundamentallystruggletoattend
tion. When it comes to visual contextual tasks imagesequencesaswellastheydowithtext.
thatinvolvedistractorinformation,VLMsstruggle
toperformevenontheeasiestlong-contexttasks, UpperboundforvideoQA Amoreconstruct-
suchastranscribingMNISTcharacters. Theyare validtestforreal-worldvisualextractivereasoning
even less capable on more complex, real-world wouldbeQAoveralongvideowhereonlyasub-
taskslikedistractorVQA.Thecoreissueappearsto element is required. E.g., putting the entire film
bethatbothshort-contextandlong-contextVLMs StarWarsasinputforQApair(Q:“whichcharacter
are not trained on tasks requiring attention over shootsagreenalienintheMosEisleycantina?”,A:
multiple context images to retrieve information. HanSolo). Informationfromasinglescenemust
Incontrast,forlongpassagesoftext,information be extracted to perform this task, thereby requir-
from throughout the passage remains relevant— ingextractivereasoning. However,itisplausible
referringbacktoaspecificsentenceearlyinadoc- thatsequencesofrelatedimages,unlikesequences
umentisintrinsictotheLMtrainingobjective. of unrelated images, are more in-distribution for
However,thesamemaynotnecessarilybetrue long-contextVLMsandextractivetasksinvolving
for the sequential image language modeling task. themcouldbeeasierforthemtosolve. Byprovid-
Intheexistinginterleavedvision-languagetraining ingcompletelyunrelatedimages,ourtaskmaybe
corpora,imagesarelikelytofollowedimmediately harderthanvideo-basedVQA,andmayrepresent
bytheirrelevanttext. Asaresult,attendingtomuch anupper-boundforvisualextractivereasoning.
earlierimagesinthedocumentsmaynotbecrucial
8 Conclusion
forachievinglowLMloss. Additionally,noopen
VLMistrainedonimagegenerationtaskscondi-
Vision-languagemodelsstrugglewithlong-context
tionedontextwhileupdatingthecoretransformer
visual extractive reasoning. Many models fail to
weights. Overlooking this objective may prevent
extractnecessaryinformationfromevenshortcon-
VLMsfromrobustlymodelingtherelationshipbe-
texts of 36, 25, or 16 images, resulting in near-
tweenimagesandtext,whichcouldbecrucialfor
or sub-baseline performance across tasks. LO-
performingwellonthesetasks.
COVQA presentsasimplebenchmark-generating
Interestingexamplesofpoorperformance Our processapplicabletoanyVQAorretrievalevalu-
analysisunveiledafewparticularlysurprisingre- ation dataset, making it easy to assess extractive
sults. For example, contrary to our expectation reasoninginVLMs. Ourfindingssuggestthattrain-
thatmodelscapableofhandlinginterleavedinput ingtasksrequiringattentionacrossmultiplecontext
would perform better with it than with compos- images to extract information–rather than simple
ite input, we found that GPT-4V actually favors single-image tasks–should be included in VLM
composite input on many subtasks. Additionally, training. By measuring this capacity it offers an
tooursurpriseLLaVA1.6performedmuchbetter appealingdirectionforfuturework.
8Limitations rightwayforevaluatinglargevision-languagemod-
els? ArXivpreprint,abs/2403.20330.
AlthoughLOCOVQAisageneralizedprocessap-
Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Pier-
plicable to any VLM benchmark, we only evalu-
giovanni, Piotr Padlewski, Daniel Salz, Sebastian
ateditonthreetasks. Whilethestrongexponential
Goodman, Adam Grycner, Basil Mustafa, Lucas
decaytrendsbetweenvisualcontextlengthandper- Beyer, et al. 2022. Pali: A jointly-scaled mul-
formanceobservedacrossallthreearecompelling, tilingual language-image model. ArXiv preprint,
abs/2209.06794.
expansionofLOCOVQAtoadditionaltaskswould
renderthefindingsclearer. Wenliang Dai, Junnan Li, Dongxu Li, Anthony
While LOCOVQA samples distractor images Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale N Fung, and Steven Hoi.
from the same datasets for open-domain and
2024. Instructblip: Towardsgeneral-purposevision-
multiple-choiceVQA,andourprocessappearsto
languagemodelswithinstructiontuning. Advances
accurately filter out collisions, it is likely that a inNeuralInformationProcessingSystems,36.
smallnumberofcollisionsstilloccur,asitisinher-
AniqaDilawariandMuhammadUsmanGhaniKhan.
entlydifficulttoensurenofailuresinanautomated
2019. Asovs: abstractive summarization of video
generatingprocess(Saxonetal.,2024). Thismay sequences. IEEEAccess,7:29253–29263.
lead to a natural ceiling on VLM performance at
Zican Dong, Tianyi Tang, Lunyi Li, and Wayne Xin
eachvisualcontextlength.
Zhao. 2023. A survey on long text modeling with
Otherimportantlong-contextcapabilitieslikely transformers. ArXivpreprint,abs/2302.14502.
existthatarenotcapturedbyLOCOVQAorprior
AngelaFan,YacineJernite,EthanPerez,DavidGrang-
worksuchasMILEBench(Songetal.,2024). Aug-
ier, Jason Weston, and Michael Auli. 2019. ELI5:
menting these evaluations with tests that capture Long form question answering. In Proceedings of
additionalorthogonalVLMlong-contextcapabili- the57thAnnualMeetingoftheAssociationforCom-
putationalLinguistics,pages3558–3567,Florence,
tiesisanimportantdirectionforfuturework.
Italy.AssociationforComputationalLinguistics.
Acknowledgements&Contributions Yue Fan, Jing Gu, Kaiwen Zhou, Qianqi Yan, Shan
Jiang,Ching-ChenKuo,XinzeGuan,andXinEric
ThankstoYujieLu,LukeYoffe,TejasSrinivasan, Wang.2024. Muffinorchihuahua? challenginglarge
Weixi Feng, and Deepak Nathani for discussion vision-languagemodelswithmultipanelvqa. ArXiv
preprint,abs/2401.15847.
and comments. This work was supported in part
bytheNationalScienceFoundationGraduateRe- Alessandro Favero, Luca Zancato, Matthew Trager,
searchFellowshipunderGrantNo. 1650114,and SiddharthChoudhary,PramudithaPerera,Alessan-
dro Achille, Ashwin Swaminathan, and Stefano
CAREERAwardunderGrantNo. 2048122.
Soatto. 2024. Multi-modal hallucination control
ASwroteallcode,designedandexecutedexper- by visual information grounding. ArXiv preprint,
iments, produced all figures, conducted analysis, abs/2403.14003.
and wrote the manuscript. MS designed experi-
SreyanGhosh,ChandraKiranReddyEvuru,SonalKu-
ments, conducted analysis and was the primary
mar,UtkarshTyagi,OriolNieto,ZeyuJin,andDi-
writerandeditorofthemanuscript. neshManocha.2024. Vdgd: Mitigatinglvlmhalluci-
nationsincognitivepromptsbybridgingthevisual
perceptiongap. ArXivpreprint,abs/2405.15683.
References
Google.2024. Paligemma. https://ai.google.dev/
gemma/docs/paligemma.
JoshAchiam,StevenAdler,SandhiniAgarwal,Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng
DiogoAlmeida,JankoAltenschmidt,SamAltman,
Xu,WenmengYu,JunhuiJi,YanWang,ZihanWang,
ShyamalAnadkat,etal.2023. Gpt-4technicalreport.
YuxiaoDong,MingDing,etal.2023. Cogagent: A
ArXivpreprint,abs/2303.08774.
visuallanguagemodelforguiagents. ArXivpreprint,
abs/2312.08914.
Hung-TingChen,FangyuanXu,ShaneA.Arora,and
Eunsol Choi. 2023. Understanding retrieval aug- LuyangHuang,ShuyangCao,NikolausParulian,Heng
mentationforlong-formquestionanswering. ArXiv Ji,andLuWang.2021. Efficientattentionsforlong
preprint,abs/2310.12150. documentsummarization. InProceedingsofthe2021
Conference of the North American Chapter of the
LinChen,JinsongLi,XiaoyiDong,PanZhang,Yuhang AssociationforComputationalLinguistics: Human
Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, LanguageTechnologies,pages1419–1436,Online.
Yu Qiao, Dahua Lin, et al. 2024. Are we on the AssociationforComputationalLinguistics.
9Albert Q Jiang, Alexandre Sablayrolles, Arthur Men- JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.
sch,ChrisBamford,DevendraSinghChaplot,Diego 2023b. Blip-2: Bootstrappinglanguage-imagepre-
delasCasas,FlorianBressand,GiannaLengyel,Guil- training with frozen image encoders and large lan-
laumeLample,LucileSaulnier,etal.2023. Mistral guage models. In International conference on ma-
7b. ArXivpreprint,abs/2310.06825. chinelearning,pages19730–19742.PMLR.
Albert Q Jiang, Alexandre Sablayrolles, Antoine Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie
Roux,ArthurMensch,BlancheSavary,ChrisBam- Del Giorno, Suriya Gunasekar, and Yin Tat Lee.
ford,DevendraSinghChaplot,DiegodelasCasas, 2023c. Textbooksareallyouneedii: phi-1.5techni-
Emma Bou Hanna, Florian Bressand, et al. 2024a. calreport. ArXivpreprint,abs/2309.05463.
Mixtralofexperts. ArXivpreprint,abs/2401.04088.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
maticevaluationofsummaries. InTextSummariza-
DongfuJiang,XuanHe,HuayeZeng,CongWei,Max
tionBranchesOut,pages74–81,Barcelona,Spain.
Ku, Qian Liu, and Wenhu Chen. 2024b. Mantis:
AssociationforComputationalLinguistics.
Interleaved multi-image instruction tuning. ArXiv
preprint,abs/2405.01483.
Tsung-YiLin,MichaelMaire,SergeBelongie,James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
GregoryKamradt.2023. Needleinahaystack-pressure
and C Lawrence Zitnick. 2014. Microsoft coco:
testing llms. https://github.com/gkamradt/
Common objects in context. In Computer Vision–
LLMTest_.
ECCV 2014: 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings,
AniruddhaKembhavi,MikeSalvato,EricKolve,Min-
PartV13,pages740–755.Springer.
joon Seo, Hannaneh Hajishirzi, and Ali Farhadi.
2016. A diagram is worth a dozen images. In Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae
ComputerVision–ECCV2016: 14thEuropeanCon-
Lee.2023a. Improvedbaselineswithvisualinstruc-
ference,Amsterdam,TheNetherlands,October11–
tiontuning.
14,2016,Proceedings,PartIV14,pages235–251.
Springer. HaotianLiu,ChunyuanLi,YuhengLi,BoLi,Yuanhan
Zhang,ShengShen,andYongJaeLee.2024. Llava-
HugoLaurençon, LéoTronchon, MatthieuCord, and next: Improvedreasoning,ocr,andworldknowledge.
Victor Sanh. 2024. What matters when build-
ing vision-language models? ArXiv preprint, Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,
abs/2405.02246. Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi
Wang, Conghui He, Ziwei Liu, et al. 2023b. Mm-
YannLeCun,LéonBottou,YoshuaBengio,andPatrick bench: Is your multi-modal model an all-around
Haffner. 1998. Gradient-based learning applied to player? ArXivpreprint,abs/2307.06281.
document recognition. Proceedings of the IEEE,
PanLu,HritikBansal,TonyXia,JiachengLiu,Chun-
86(11):2278–2324.
yuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-
WeiChang,MichelGalley,andJianfengGao.2023.
Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-
Mathvista: Evaluating mathematical reasoning of
tus, Fabio Petroni, Vladimir Karpukhin, Naman
foundationmodelsinvisualcontexts. ArXivpreprint,
Goyal,HeinrichKüttler,MikeLewis,Wen-tauYih,
abs/2310.02255.
Tim Rocktäschel, Sebastian Riedel, and Douwe
Kiela. 2020. Retrieval-augmented generation for
Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-
knowledge-intensiveNLPtasks. InAdvancesinNeu-
WeiChang,Song-ChunZhu,OyvindTafjord,Peter
ralInformationProcessingSystems33: AnnualCon-
Clark,andAshwinKalyan.2022. Learntoexplain:
ferenceonNeuralInformationProcessingSystems
Multimodalreasoningviathoughtchainsforscience
2020,NeurIPS2020,December6-12,2020,virtual.
questionanswering. InThe36thConferenceonNeu-
ralInformationProcessingSystems(NeurIPS).
BohaoLi,RuiWang,GuangzhiWang,YuyingGe,Yix-
iaoGe,andYingShan.2023a. Seed-bench: Bench-
Kenneth Marino, Mohammad Rastegari, Ali Farhadi,
marking multimodal llms with generative compre-
andRoozbehMottaghi.2019. OK-VQA:Avisual
hension. ArXivpreprint,abs/2307.16125.
question answering benchmark requiring external
knowledge. InIEEEConferenceonComputerVision
Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang,
andPatternRecognition,CVPR2019,LongBeach,
MingYan,BinBi,JiaboYe,HeChen,GuohaiXu,
CA,USA,June16-20,2019,pages3195–3204.Com-
ZhengCao,JiZhang,SongfangHuang,FeiHuang,
puterVisionFoundation/IEEE.
JingrenZhou,andLuoSi.2022. mPLUG:Effective
andefficientvision-languagelearningbycross-modal Moondream.2024. tinyvisionlanguagemodel. https:
skip-connections. InProceedingsofthe2022Con- //github.com/vikhyat/moondream.
ferenceonEmpiricalMethodsinNaturalLanguage
Processing, pages 7241–7259, Abu Dhabi, United Van-QuangNguyen,MasanoriSuganuma,andTakayuki
ArabEmirates.AssociationforComputationalLin- Okatani. 2022. Grit: Faster and better image cap-
guistics. tioning transformer using dual visual features. In
10European Conference on Computer Vision, pages RaulPuri,AlecRadford,JackRae,AdityaRamesh,
167–184.Springer. CameronRaymond,FrancisReal,KendraRimbach,
Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-
OpenAI,JoshAchiam,StevenAdler,SandhiniAgarwal, der,MarioSaltarelli,TedSanders,ShibaniSanturkar,
Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale- GirishSastry,HeatherSchmidt,DavidSchnurr,John
man,DiogoAlmeida,JankoAltenschmidt,SamAlt- Schulman, Daniel Selsam, Kyla Sheppard, Toki
man,ShyamalAnadkat,RedAvila,IgorBabuschkin, Sherbakov, Jessica Shieh, Sarah Shoker, Pranav
SuchirBalaji,ValerieBalcom,PaulBaltescu,Haim- Shyam,SzymonSidor,EricSigler,MaddieSimens,
ing Bao, Mohammad Bavarian, Jeff Belgum, Ir- JordanSitkin,KatarinaSlama,IanSohl,Benjamin
wanBello,JakeBerdine,GabrielBernadett-Shapiro, Sokolowsky, Yang Song, Natalie Staudacher, Fe-
ChristopherBerner,LennyBogdonoff,OlegBoiko, lipePetroskiSuch,NatalieSummers,IlyaSutskever,
MadelaineBoyd,Anna-LuisaBrakman,GregBrock- Jie Tang, Nikolas Tezak, Madeleine B. Thompson,
man, Tim Brooks, Miles Brundage, Kevin Button, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,
TrevorCai,RosieCampbell,AndrewCann,Brittany PrestonTuggle,NickTurley,JerryTworek,JuanFe-
Carey, Chelsea Carlson, Rory Carmichael, Brooke lipeCerónUribe,AndreaVallone,ArunVijayvergiya,
Chan,CheChang,FotisChantzis,DerekChen,Sully ChelseaVoss,CarrollWainwright,JustinJayWang,
Chen, Ruby Chen, Jason Chen, Mark Chen, Ben AlvinWang,BenWang,JonathanWard,JasonWei,
Chess,ChesterCho,CaseyChu,HyungWonChung, CJWeinmann,AkilaWelihinda,PeterWelinder,Ji-
Dave Cummings, Jeremiah Currier, Yunxing Dai, ayiWeng,LilianWeng,MattWiethoff,DaveWillner,
Cory Decareaux, Thomas Degry, Noah Deutsch, Clemens Winter, Samuel Wolrich, Hannah Wong,
Damien Deville, Arka Dhar, David Dohan, Steve Lauren Workman, Sherwin Wu, Jeff Wu, Michael
Dowling,SheilaDunning,AdrienEcoffet,AttyEleti, Wu,KaiXiao,TaoXu,SarahYoo,KevinYu,Qim-
TynaEloundou,DavidFarhi,LiamFedus,NikoFelix, ingYuan,WojciechZaremba,RowanZellers,Chong
SimónPosadaFishman, JustonForte, IsabellaFul- Zhang, Marvin Zhang, Shengjia Zhao, Tianhao
ford,LeoGao,ElieGeorges,ChristianGibson,Vik Zheng,JuntangZhuang,WilliamZhuk,andBarret
Goel,TarunGogineni,GabrielGoh,RaphaGontijo- Zoph.2023. Gpt-4technicalreport. ArXivpreprint,
Lopes, Jonathan Gordon, Morgan Grafstein, Scott abs/2303.08774.
Gray,RyanGreene,JoshuaGross,ShixiangShane
BaolinPeng,ChunyuanLi,PengchengHe,MichelGal-
Gu,YufeiGuo,ChrisHallacy,JesseHan,JeffHarris,
ley,andJianfengGao.2023. Instructiontuningwith
YuchenHe,MikeHeaton,JohannesHeidecke,Chris
gpt-4. ArXivpreprint,abs/2304.03277.
Hesse,AlanHickey,WadeHickey,PeterHoeschele,
Brandon Houghton, Kenny Hsu, Shengli Hu, Xin
JasonPhang,YaoZhao,andPeterJ.Liu.2022. Investi-
Hu, Joost Huizinga, Shantanu Jain, Shawn Jain,
gatingefficientlyextendingtransformersforlongin-
Joanne Jang, Angela Jiang, Roger Jiang, Haozhun
putsummarization. ArXivpreprint,abs/2208.04347.
Jin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-
woo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Ka- Machel Reid, Nikolay Savinov, Denis Teplyashin,
mali, Ingmar Kanitscheider, Nitish Shirish Keskar, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste
Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Alayrac,RaduSoricut,AngelikiLazaridou,OrhanFi-
Christina Kim, Yongjik Kim, Jan Hendrik Kirch- rat,JulianSchrittwieser,etal.2024. Gemini1.5: Un-
ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, lockingmultimodalunderstandingacrossmillionsof
Łukasz Kondraciuk, Andrew Kondrich, Aris Kon- tokensofcontext. ArXivpreprint,abs/2403.05530.
stantinidis, Kyle Kosic, Gretchen Krueger, Vishal
Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan MichaelSaxon,YiranLuo,SharonLevy,ChittaBaral,
Leike, Jade Leung, Daniel Levy, Chak Ming Li, YezhouYang,andWilliamYangWang.2024. Lostin
Rachel Lim, Molly Lin, Stephanie Lin, Mateusz translation? translationerrorsandchallengesforfair
Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, assessmentoftext-to-imagemodelsonmultilingual
AnnaMakanju,KimMalfacini,SamManning,Todor concepts. ArXivpreprint,abs/2403.11092.
Markov, Yaniv Markovski, Bianca Martin, Katie
Mayer,AndrewMayne,BobMcGrew,ScottMayer SkunkworksAI. 2023. Bakllava. https://
McKinney, Christine McLeavey, Paul McMillan,
huggingface.co/llava-hf/bakLlava-v1-hf.
Jake McNeil, David Medina, Aalok Mehta, Jacob
DingjieSong,ShunianChen,GuimingHardyChen,Fei
Menick, Luke Metz, Andrey Mishchenko, Pamela
Yu,XiangWan,andBenyouWang.2024. Milebench:
Mishkin, Vinnie Monaco, Evan Morikawa, Daniel
Benchmarkingmllmsinlongcontext. ArXivpreprint,
Mossing,TongMu,MiraMurati,OlegMurk,David
abs/2404.18532.
Mély,AshvinNair,ReiichiroNakano,RajeevNayak,
ArvindNeelakantan,RichardNgo,HyeonwooNoh, Gemini Team, Rohan Anil, Sebastian Borgeaud,
LongOuyang,CullenO’Keefe,JakubPachocki,Alex Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
Paino, Joe Palermo, Ashley Pantuliano, Giambat- Radu Soricut, Johan Schalkwyk, Andrew M Dai,
tistaParascandolo,JoelParish,EmyParparita,Alex Anja Hauth, et al. 2023. Gemini: a family of
Passos,MikhailPavlov,AndrewPeng,AdamPerel- highlycapablemultimodalmodels. ArXivpreprint,
man,FilipedeAvilaBelbutePeres,MichaelPetrov, abs/2312.11805.
Henrique Ponde de Oliveira Pinto, Michael, Poko-
rny,MichellePokrass,VitchyrH.Pong,TollyPow- Gemma Team, Thomas Mesnard, Cassidy Hardin,
ell, Alethea Power, Boris Power, Elizabeth Proehl, RobertDadashi,SuryaBhupatiraju,ShreyaPathak,
11LaurentSifre,MorganeRivière,MihirSanjayKale, A EvaluatedModels
Juliette Love, et al. 2024. Gemma: Open models
based on gemini research and technology. ArXiv (i) Moondream2-1.6b (Moondream, 2024)
preprint,abs/2403.08295. basedonPhi-1.5(Lietal.,2023c),
(ii) LLaVA-1.5(Liuetal.,2023a)withVicuna-
JianfengWang,ZhengyuanYang,XiaoweiHu,Linjie
7b(Pengetal.,2023)astheLLMbackbone,
Li, KevinLin, ZheGan, ZichengLiu, CeLiu, and
LijuanWang.2022. Git: Agenerativeimage-to-text (iii) LLaVA-1.6(LLaVA-Next)(Liuetal.,2024),
transformerforvisionandlanguage. ArXivpreprint, animprovementoverLLaVA-1.5withhigher
abs/2205.14100.
imageresolutionandbettervisualreasoning,
Weiyun Wang, Shuibo Zhang, Yiming Ren, Yuchen usesMistral-7b(Jiangetal.,2023),
Duan, Tiantong Li, Shuo Liu, Mengkang Hu, Zhe (iv) PaliGemma-3b (Google, 2024), based on
Chen,KaipengZhang,LeweiLu,etal.2024. Nee- open components from the SigLip (Zhai
dle in a multimodal haystack. ArXiv preprint,
etal.,2023)imageencoderandtheGemma
abs/2406.07230.
(Teametal.,2024)languagemodel,
Wenhui Wang, Hangbo Bao, Li Dong, Johan (v) Mantis-bakllava-7b (Jiang et al., 2024b)
Bjorck, ZhiliangPeng, QiangLiu, KritiAggarwal,
fine-tunedfromBakLLaVA(SkunkworksAI,
OwaisKhanMohammed,SakshamSinghal,Subhojit
2023) and derived from LLaVA but using
Som,etal.2023. Imageasaforeignlanguage: Beit
pretrainingforvisionandvision-languagetasks. In Mistral-7b(Jiangetal.,2023),
ProceedingsoftheIEEE/CVFConferenceonCom- (vi) Mantis-Idefics2-8b(Jiangetal.,2024b),the
puterVisionandPatternRecognition,pages19175– currentstate-of-the-artMantisvariant,based
19186.
onIdefics2-8b(Laurençonetal.,2024).
HaiyangXu,QinghaoYe,MingYan,YayaShi,Jiabo
Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian,
B LOCOVQA ReleaseInformation
Wei Wang, et al. 2023a. mplug-2: A modularized
Fullsourcecodeforgeneratingdistractdatasetsis
multi-modal foundation model across text, image
andvideo. InInternationalConferenceonMachine availableatlocovqa.github.io. LOCOVQA is
Learning,pages38728–38748.PMLR. releasedundertheApachev2.0license.
Peng Xu, Wei Ping, Xianchao Wu, Lawrence C.
C FilteringCollisionsin LOCOVQA
McAfee, Chen Zhu, Zihan Liu, Sandeep Subra-
manian, Evelina Bakhturina, Mohammad Shoeybi,
Figure 8 shows an example method used to fil-
andBryanCatanzaro.2023b. Retrievalmeetslong
terforcollisionsin4-ImageContextLengthinput
context large language models. ArXiv preprint,
abs/2310.03025. fromFigure2b. TheLLMQuery(Q)wasusedto
promptGPT-4LLM.Thesecondfigureshowsan
XiangYue,YuanshengNi,KaiZhang,TianyuZheng,
exampleofacollisionoccurringwhencreatinga4-
RuoqiLiu,GeZhang,SamuelStevens,DongfuJiang,
imagecontext. Inthiscase,thetwoimagescontain
Weiming Ren, Yuxuan Sun, et al. 2023. Mmmu:
Amassivemulti-disciplinemultimodalunderstand- oranges,soweresampleoneoftheimagestoavoid
ingandreasoningbenchmarkforexpertagi. ArXiv collision. TheOK-VQAquestionsinthiscaseare
preprint,abs/2311.16502.
“What type of plant do these fruits grow from?”
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, forthecontentimageand“InwhichUSstatesare
andLucasBeyer.2023. Sigmoidlossforlanguage thesefruitscommonlygrown?” forthedistractor
imagepre-training. InProceedingsoftheIEEE/CVF
image. Bothrequirevisualreasoningstepthatthe
InternationalConferenceonComputerVision,pages
fruitdescribedinthisimageisanorange.
11975–11986.
Gengyuan Zhang, Yurui Zhang, Kerui Zhang, and D Model-levelScoringDetails
Volker Tresp. 2024. Can vision-language models
be a good guesser? exploring vlms for times and OK-VQA. FortheOK-VQAfree-formground
locationreasoning. InProceedingsoftheIEEE/CVF truthanswers,ifanyofthegroundtruthcandidate
WinterConferenceonApplicationsofComputerVi-
answersisasubstringofthemodel-generatedan-
sion,pages636–645.
swer,weawardfullpointsfortheexact-matching
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. setting. The other two metrics we used were
Weinberger,andYoavArtzi.2020. Bertscore: Evalu- BERTScore and ROUGE scores. BERTScore
atingtextgenerationwithBERT. In8thInternational
(Zhang et al., 2020) is a robust text compari-
ConferenceonLearningRepresentations,ICLR2020,
son metric which matches candidate and refer-
AddisAbaba,Ethiopia,April26-30,2020.OpenRe-
view.net. ence based on the cosine similarities of the em-
12LLMQuery(Q):Pleaselistatmost5entitiesinthisimage LLMQuery(Q):Pleaselistatmost5entitiesinthisimage
X 1 Broccoli,Carrot,PineNuts,Plate,Garlic X 1 Cat,Laptop,YellowFigurine,Keyboard,Screen
X 2 Boat,Beach,Sky,Sand,BoatHull X 2 Oranges,Leaves,Branches,WaterDroplets
X Cake,Candles,Cupcake,Sandwiches,DessertTray X Bird,Dome,Buildings,TelevisionTower,Trees
3 3
X 4 TrafficLight,RoadSign,Building,Wheelbarrow,Car X 4 Man,Oranges,FlatbedCart,Street,Cardboard
✓ CollisionCheck:X 1∩X 2∩X 3∩X 4=∅(Valid) ✗ CollisionCheck:X 1∩X 2∩X 3∩X 4={Oranges}
Figure8: CollisionFilteringMethodforLOCOVQA bypromptingLLMwithqueryQtoidentifyentities. Cell
with representstheentitiesforeachimageX . Iftherearenoentitiesincommon,therearenocollisionssowe
i
markcellwith indicatingthisisavalidconstructionofimagesforLOCOVQA.
beddings. Using the Sentence Transformer pack- First, we checked if the answer was between the
ageall-MiniLM-L6-v2model,wecalculatedthe <answer>tagsand,ifso,extractedtheMCQchoice
maximum BERTScore between all the ground directly from the tag. If not, we noted that out-
truth answers against the model answer. For the putsoftenfollowedtheformat,“Answer: choice,”
ROUGE-score,wecomparedthegroundtruthan- where the choice follows directly after. We also
swerandmodelanswerusingthedefaultsettings checkededgecases,includinginstanceswherethe
withROUGE-L,whichmeasurethelongestcom- firstletterinthestringisamultiple-choicefollowed
monsubsequenceatthesentence-level byacolon,choicesprovidedinparentheses,only
the answer text without the corresponding letter,
MMStar. For multiple-choice answer form in
andasingleletterprovided.
MMStar, when prompting the VLM, we ask it
to produce the answer in the following format:
MNIST. ForMNISTevaluation,weensurethat
“Please provide the answer in format <answer>
the model response contains a list of digits sep-
</answer> where the answer is between the tags.
arated by commas. If the output is separated by
For example if the result is apple, your answer
spaces,weparseitintoanarraytoprovideacom-
shouldbe<answer>apple</answer>.” Thisformat
pletelyfairevaluation. Theresponsemustcontain
ensure some grammatical structure and requires
exactlythesamenumberofdigitsasthoseinthe
the answer to be enclosed within the <answer>
MNISTdigits. Wesortboththecandidateandref-
tags. GPT4V, Gemini 1.0, and Gemini 1.5 un-
erencelistsandcomparethemtocheckforequality.
der both composed and interleaved settings pro-
ducedanswersfollowingthisformat. However,we APImodelconsiderations. Open-weightmod-
didnotobserveconsistentbehaviorfromLLaVA- elsconsistentlyattempttoprovideananswerwhen
based and Mantis-based variants. Moondream prompted,evenifitmeansoccasionallyproducing
andGemmaconsistentlyrespondedwithasingle- halluncinated responses. In contrast, API-based
choiceanswer,makingtheevaluationofthesetwo modelslikeGeminiandGPT-4Vsometimesrefuse
modelstheeasiest. DuetothevarianceinVLMre- toanswerquestionsundercertaincircumstances–
sponses,weadoptedarobustevaluationprocedure. suchaslowconfidence,potentiallyunsafelanguage
13Non-Response Rate Non-Template Response
Gemini Family
400 GPT-4V (cmp) 600
GPT-4V (int)
500
300
Gemini 1.0 (cmp)
400 Gemini 1.0 (int)
Gemini 1.5 (cmp)
200 300 Gemini 1.5 (int)
GPT-4V (cmp)
GPT-4V (int)
200
100
100
0 0
0 5 10 15 20 25 30 35 0 5 10 15 20 25 30 35
Visual Context Length Visual Context Length
Figure9: Plotsofnon-responserateandnon-templateresponsesofclosed-sourceVLMs.
4-Image Context 16-Image Context 36-Image Context
MMStar MMStar MMStar
OK-VQA OK-VQA OK-VQA
SEEDBench SEEDBench SEEDBench
ROUGE ROUGE ROUGE
OK OK OK
VQA MMBenchVQA MMBenchVQA MMBench
BERT BERT BERT
OKVQA MMMUOK-VQA MMMUOK-VQA MMMU
AI2D MathVista AI2D MathVista AI2D MathVista
ScienceQA ScienceQA ScienceQA
LLaVA-1.5 LLaVA-1.6 GPT-4V (int) PaliGemma Gemini 1.5 (cmp)
Mantis GPT-4V (cmp) Gemini 1.0 (cmp) MoonDream2 Gemini 1.5 (int)
Figure10: RadarplotsofVLMperformanceacross8multimodalbenchmarkswithcontextlengthsk =4,16,36.
intheresponse,orfailuretoadheretotheresponse is crucial to monitor the trend. As visual context
template. We categorize these instances into two length increases VLMs often struggle to adhere
primary error types: non-response rate and non- tothepromptedresponseformatting. Amongthe
template response rate. Figure 9 illustrates these models tested, Gemini 1.5 demonstrates the best
error types for both Gemini and GPT-4V models adherencetorequestedformatting,withonlyasin-
onOK-VQAdatasetofsize5072. gleerror. Gemini1.0exhibitsslightlymoreissues,
withnomorethan100errorsintemplateadherence.
A non-response case is identified when the
GPT-4Vshowsthemostsignificantstruggle,with
modelreturnsanemptystring. TheGeminimod-
up to 600 formatting errors. We classify model
els(1.0and1.5)exhibitthehighestnon-responses
refusals, possibly due to low confidence, as fail-
rates,recording72non-responsesinvisualcontext
ures. Fundamentally,correctresponseformatting
with a size of 36, but fewer than 10 for all other
isnecesarytoenableuseofLMswithin
contextlengths. Thispatternmaybeattributedto
thesafetymechanismsemployedbyGeminiorthe
E SupplementaryResults
model’suncertainityduetothepresenceofmulti-
pledistractors. Meanwhile,thenon-responserate Figure 10 shows the radar plots for image con-
forGPT-4Vintheinterleavedsettingshowsadis- text lengths k = 4,16,36 to extend Figure 5. Ta-
tinct increasing trend, possibly because GPT-4V ble 2 and Table 3 display the r2 values curve fits
struggles with processing a set of 36 interleaved inFigure4fit. p-valasterisksdenotethestatistical
images. Thenon-responserateforGPT-4Vinter- significanceoftheoverallfit. Theredhighlightsig-
leavedis425non-responseinvisualcontextof36. nifiessubchanceperformance(highlightedifmore
than half of the data points were below random
Anon-templateresponseoccurswhenthemodel
choice). Adownarrowindicatesanegativecorrela-
failstoencloseitsoutputwith<answer>tags. This
tion(asvisualcontextlengthincrease,performance
isnotpenalizedinourevaluations–wherewestill
increases).
calculate exact match, BERT, ROUGE scores–it
14
etaR
rorrE
etaR
rorrEMethods LLaVA(c) LLaVA-1.6(c) Moondream(c) PaliGemma(c) Mantis(i)
MMStar .924∗∗∗ .957∗∗∗ .903∗∗∗ .947∗∗∗ .967∗∗∗
SEEDBench .119 .825∗∗∗ .825∗∗∗ .988∗∗∗ .885∗∗∗
MMBench .959∗∗∗ .992∗∗∗ .937∗∗∗ .946∗∗∗ .972∗∗∗
MMMU .367↓∗∗ .691↓∗∗∗ .586∗∗ .923↓ .282∗∗∗
MathVista .513∗ .907↓∗∗∗ .004 .132↓ .848∗∗∗
ScienceQA .080↓ .020 .279∗ .907∗∗∗ .209↓∗
AI2D .658↓∗∗ .604∗∗ .375 .882∗∗∗ .857∗∗∗
OK-VQA .985∗∗∗ .989∗∗∗ .979∗∗∗ .968∗∗∗ .991∗∗∗
BERT .986∗∗∗ .973∗∗∗ .981∗∗∗ .968∗∗∗ .986∗∗∗
ROUGE .988∗∗∗ .973∗∗∗ .985∗∗∗ .968∗∗∗ .918∗∗∗
Haystack-9 .067 .850 .000 .895∗ .000
Haystack-16 .000 .790 .000 .535 .000
Haystack-25 .000 .804∗ .000 .651 .000
Haystack-36 .000 .596 .000 .668 .000
Table2: Exponentialcurvefitr2valuesarereportedforeachopen-weightmodel,followedbyasymboldenoting
thep-valueforstatisticalsignificance. Thesymbolsrepresent: ∗forp≤0.05,∗∗forp≤0.01,&∗∗∗forp≤0.001.
Lightpinkcellsrepresentcorrelationsforsetsofvaluesthatfallbelowchanceperformance.
Methods GPT-4V(c) GPT-4V(i) Gemini1.0(c) Gemini1.0(i) Gemini1.5(c) Gemini1.5(i)
MMStar .880∗∗∗ .937∗∗∗ .972∗∗∗ .972∗∗∗ .985∗∗∗ .981∗∗∗
SEEDBench .737∗∗∗ .956∗∗∗ .765∗∗∗ .765∗∗∗ .977∗∗∗ .980∗∗∗
MMBench .951∗∗∗ .959∗∗∗ .977∗∗∗ .977∗∗∗ .964∗∗∗ .948∗∗∗
MMMU .861∗∗∗ .372∗∗∗ .493∗∗ .493∗∗ .487∗∗ .533∗∗
MathVista .676∗∗∗ .858∗∗∗ .832∗∗∗ .832∗∗∗ .840∗∗∗ .690∗∗∗
ScienceQA .932∗∗∗ .820∗∗∗ .268∗ .268∗ .546∗∗ .309∗
AI2D .743∗∗∗ .635∗∗ .880∗∗∗ .880∗∗∗ .969∗∗∗ .882∗∗∗
OK-VQA .963∗∗∗ .975∗∗∗ .962∗∗∗ .962∗∗∗ .991∗∗∗ .989∗∗∗
BERT .969∗∗∗ .977∗∗∗ .955∗∗∗ .955∗∗∗ .989∗∗∗ .993∗∗∗
ROUGE .958∗∗∗ .979∗∗∗ .955∗∗∗ .955∗∗∗ .981∗∗∗ .989∗∗∗
Haystack-9 .505 .632 - - .052 .895∗
Haystack-16 .366 .866∗ - - .025 .256
Haystack-25 .863∗ .704 - - .118 .146
Haystack-36 .841∗ .645 - - .489 .896
Table3: Exponentialcurvefitr2valuesarereportedforeachclosed-sourcemodel,followedbyasymboldenoting
thep-valueforstatisticalsignificance. Thesymbolsrepresent: ∗forp≤0.05,∗∗forp≤0.01,&∗∗∗forp≤0.001.
Lightpinkcellsrepresentcorrelationsforsetsofvaluesthatfallbelowchanceperformance.
15