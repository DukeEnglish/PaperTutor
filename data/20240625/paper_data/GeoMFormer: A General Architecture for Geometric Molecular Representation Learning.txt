GeoMFormer: A General Architecture for
Geometric Molecular Representation Learning
TianlangChen*1 ShengjieLuo*2 DiHe2 ShuxinZheng3 Tie-YanLiu3 LiweiWang24
Abstract 1.Introduction
Deeplearningapproacheshaveemergedasapowerfultool
Molecularmodeling,acentraltopicinquantum
forawiderangeoftasks(Heetal.,2016;Devlinetal.,2019;
mechanics,aimstoaccuratelycalculatetheprop-
Brownetal.,2020). Recently,researchershavestartedin-
ertiesandsimulatethebehaviorsofmolecularsys-
vestigatingwhetherthepowerofneuralnetworkscouldhelp
tems. Themolecularmodelisgovernedbyphys-
solveproblemsinphysicsandchemistry,suchaspredicting
ical laws, which impose geometric constraints
thepropertyofmoleculeswith3Dcoordinatesandsimu-
suchasinvarianceandequivariancetocoordinate
lating how each atom moves in Euclidean space (Schütt
rotation and translation. While numerous deep
etal.,2018;Gasteigeretal.,2020b;Satorrasetal.,2021).
learningapproacheshavebeendevelopedtolearn
Thesemolecularmodelingtasksrequirethelearnedmodel
molecularrepresentationsundertheseconstraints,
tosatisfygeneralphysicallaws,suchastheinvarianceand
mostofthemarebuiltuponheuristicandcostly
equivariance conditions: The model’s prediction should
modules. We argue that there is a strong need
reactphysicallywhentheinputcoordinateschangeaccord-
forageneralandflexibleframeworkforlearning
ingtothetransformationofthecoordinatesystem,suchas
both invariant and equivariant features. In this
rotationandtranslation.
work, we introduce a novel Transformer-based
molecularmodelcalledGeoMFormertoachieve Avarietyofmethodshavebeenproposedtodesignneural
thisgoal. UsingthestandardTransformermod- architectures that intrinsically satisfy the invariance or
ules,twoseparatestreamsaredevelopedtomain- equivarianceconditions(Thomasetal.,2018;Schüttetal.,
tainandlearninvariantandequivariantrepresenta- 2021;Batzneretal.,2022). Tosatisfytheinvariantcondi-
tions. Carefullydesignedcross-attentionmodules tion,severalapproachesincorporateinvariantfeatures,such
bridgethetwostreams,allowinginformationfu- astherelativedistancebetweeneachatompair,intoclassic
sionandenhancinggeometricmodelingineach neuralnetworks(Schüttetal.,2018;Shietal.,2022). How-
stream. As a general and flexible architecture, ever,thismayhinderthemodelfromeffectivelyextracting
weshowthatmanypreviousarchitecturescanbe the molecular structural information (Pozdnyakov et al.,
viewedasspecialinstantiationsofGeoMFormer. 2020;Joshietal.,2023). Forexample,computingdihedral
Extensiveexperimentsareconductedtodemon- angles from coordinates is straightforward but requires
stratethepowerofGeoMFormer. Allempirical much more operations using relative distances (Schütt
results show that GeoMFormer achieves strong etal.,2021). Tosatisfytheequivariantcondition,several
performance on both invariant and equivariant works design neural networks with equivariant operation
tasksofdifferenttypesandscales. Codeandmod- only,suchastensorproductbetweenirreduciblerepresenta-
els will be made publicly available at https: tions(Thomasetal.,2018;Fuchsetal.,2020;Batzneretal.,
//github.com/c-tl/GeoMFormer. 2022)andvectoroperations(Satorrasetal.,2021;Schütt
et al., 2021; Thölke & De Fabritiis, 2022). However, the
numberofsuchoperationsarelimitedduetotheequivariant
*Equal contribution 1School of EECS, Peking University constraints, which are either costly to scale or lead to
2National Key Laboratory of General Artificial Intelligence, fairly complex network architecture designs to guarantee
SchoolofIntelligenceScienceandTechnology,PekingUniversity
sufficient expressive power. More importantly, many
3MicrosoftResearchAI4Science4CenterforMachineLearning
real-worldapplicationsrequireamodelthatcaneffectively
Research,PekingUniversity. Correspondenceto: ShengjieLuo
<luosj@stu.pku.edu.cn>,DiHe<dihe@pku.edu.cn>,LiweiWang perform both invariant and equivariant prediction with
<wanglw@pku.edu.cn>. strongperformanceatthesametime. Whilesomerecent
works study this direction (Schütt et al., 2021; Thölke &
Proceedings of the 41st International Conference on Machine
DeFabritiis,2022),mostproposednetworksaredesigned
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by
heuristicallyandlackgeneraldesignprinciples.
theauthor(s).
1
4202
nuJ
42
]GL.sc[
1v35861.6042:viXraGeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
We argue that developing a general and flexible architec- ourarchitecturecanpreciselyforecastthepositions(equiv-
ture that can effectively learn both invariant and equiv- ariant) for a set of particles controlled by physical rules.
ariant representations of high quality and simultaneously Ablationstudyfurthershowsbenefitsbroughtbyeachde-
achievestrongperformanceonbothtasksisessential. In sign choice of our framework. All the empirical results
thiswork,weintroduceGeoMFormertoachievethisgoal. highlightthegeneralityandeffectivenessofGeoMFormer.
GeoMFormerusesastandardTransformer-basedarchitec-
ture(Vaswanietal.,2017)butwithtwostreams. Aninvari-
2.RelatedWorks
antstreamlearnsinvariantrepresentations,andanequivari-
antstreamlearnsequivariantrepresentations. Eachstream
InvariantRepresentationLearning. Inrecentyears,in-
iscomposedofinvariant/equivariantself-attentionandfeed-
variancehasbeenrecognizedasoneofthefundamentalprin-
forwardlayers. ThekeydesigninGeoMFormeristouse
ciplesguidingthedevelopmentofmolecularmodels. Tode-
cross-attentionmechanismsbetweenthetwostreams,let-
scribethepropertiesofamolecularsystem,themodel’spre-
tingeachstreamincorporatetheinformationfromtheother
dictionshouldremainunchangedifweconductanyrotation
and enhance itself. In each layer of the invariant stream,
ortranslationactionsonthecoordinatesofthewholesystem.
wedevelopaninvariant-to-equivariantcross-attentionmod-
Previous works usually rely on relative structural signals
ule,wheretheinvariantrepresentationsareusedtoquery
from the coordinates, which intrinsically preserve the in-
key-valuepairsintheequivariantstream. Anequivariant-
variance. InSchNet(Schüttetal.,2018), theinteratomic
to-invariantcross-attentionmoduleisdesignedsimilarlyin
distancesareencodedviaradialbasisfunctions,whichserve
theequivariantstream. Weshowthatthedesignofallself-
astheweightsofthedevelopedcontinuous-filterconvolu-
attentionandcross-attentionmodulesisflexibleandhowto
tionallayers. PhysNet(Unke&Meuwly,2019)similarly
satisfytheinvariant/equivariantconditionseffectively.
incorporatedbothatomicfeaturesandinteratomicdistances
Our proposed architecture has several advantages com- initsinteractionblocks. Graphormer-3D(Shietal.,2022)
pared to previous works. GeoMFormer decomposes the developedaTransformer-basedmodelbyencodingtherela-
invariant/equivariantrepresentationlearningthroughself- tivedistanceasattentionbiasterms,whichperformwellon
attentionandcross-attentionmodules. Byinteractingthe large-scaledatasets(Chanussotetal.,2021).
two streams using cross-attention modules, the invariant
Beyond the interatomic distance, other works further in-
streamreceivesmorestructuralsignals(fromtheequivariant
corporatehigh-orderinvariantsignals. BasedonPhysNet,
stream),andtheequivariantstreamobtainsmorenon-linear
DimeNet/DimeNet++(Gasteigeretal.,2020b;a)addition-
transformation (from the invariant stream), which allows
allyencodethebondangleinformationusingFourier-Bessel
simultaneouslyandcompletelymodelinginteratomicinter-
basisfunctions.Moreover,GemNet/GemNet-OC(Gasteiger
actions within/across feature spaces in a unified manner.
etal.,2021;2022)carefullystudiedtheconnectionsbetween
Furthermore,wedemonstratethattheproposeddecomposi-
sphericalrepresentationsanddirectionalinformation,which
tionisgeneralbyshowingthatmanyexistingmethodscan
inspiredtoleveragethedihedralangles,i.e.,anglesbetween
beregardedasspecialcasesinourframework. Forexample,
planes formed by bonds. SphereNet (Liu et al., 2022b)
PaiNN(Schüttetal.,2021)andTorchMD-NET(Thölke&
and ComENet (Wang et al., 2022) consider the torsional
DeFabritiis,2022)canbeformulatedasaspecialinstanti-
informationtoaugmentthemolecularmodels. Duringthe
ationbyfollowingthedesignphilosophyofGeoMFormer
development in the literature, more complex features are
andusingproperinstantiationsofkeybuildingcomponents.
incorporatedduetothelossystructuralinformationwhen
Fromthisperspective,webelieveourmodelcanoffermany
purelylearninginvariantrepresentations,whilelargelyin-
differentoptionsindiversescenariosinrealapplications.
creasingthecosts.Besides,theseinvariantmodelsaregener-
Weconductexperimentscoveringdiversedatamodalities, allyunabletodirectlyperformequivariantpredictiontasks.
scalesandtaskswithbothinvariantandequivarianttargets.
OntheOpenCatalyst2020(OC20)dataset(Chanussotetal.,
EquivariantRepresentationLearning. Insteadofbuild-
2021),whichcontainslargeatomicsystemscomposedof
inginvariantblocksonly,therearevariousworksthataimto
adsorbate-catalystpairs,ourmodelisabletopredictthesys-
learnequivariantrepresentations. Inreal-worldapplications,
tem’senergy(invariant)andrelaxedstructure(equivariant)
therearealsomanymoleculartasksthatrequirethemodel
withhighaccuracy. Additionally,ourarchitectureachieves
toperformequivariantpredictions,e.g.,predictingtheforce,
state-of-the-artperformanceforpredictinghomo-lumoen-
position, velocity, and other tensorized properties in dy-
ergygap(invariant)ofamoleculeonPCQM4Mv2(Huetal.,
namic simulation tasks. If a rotation action is performed
2021)andMolecule3D(Xuetal.,2021)datasets,bothof
on each position, then these properties should also corre-
which consist of molecules collected from the chemical
spondinglyrotate. Oneclassicalapproach(Thomasetal.,
database(Maho,2015;Nakata&Shimazaki,2017). More-
2018;Fuchsetal.,2020;Batzneretal.,2022;Musaelian
over,weconductanN-bodysimulationexperimentwhere
etal.,2023)toencodingtheequivariantconstraintsisusing
2GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
irreducible representations (irreps) via spherical harmon- theory(Cotton,1991;Cornwell,1997;Scott,2012).
ics(Goodman&Wallach,2000). Withequivariantconvolu-
Formally, let ϕ : X → Y denote a function mapping be-
tionsbasedontensorproductsbetweenirreps,eachblock
tweenvectorspaces.GivenagroupG,letρX andρY denote
of the model preserves the equivariance. However, their
itsgrouprepresentations. Afunctionϕ:X →Y issaidto
operationsareingeneralcostly(Schüttetal.,2021;Satorras
beequivariant/invariantifitsatisfiesthefollowingcondi-
et al., 2021; Frank et al., 2022; Luo et al., 2024), which
tionsrespectively:
largely hinders the model from deploying on large-scale
molecularsystems. Besides, thesemodelsalsodonotal- Equivariance:ρY(g)[ϕ(x)]=ϕ(cid:0) ρX(g)[x](cid:1) ,forallg∈G,x∈X
wayssignificantlyoutperforminvariantmodelsoninvariant
Invariance:
ϕ(x)=ϕ(cid:0) ρX(g)[x](cid:1)
,forallg∈G,x∈X
tasks(Liuetal.,2022b). (1)
Intuitively,anequivariantfunctionmappingtransformsthe
On the other hand, several recent works maintain both
output predictably in response to transformations on the
invariant and equivariant representations. The invariant
input,whereasaninvariantfunctionmappingproducesan
representationsinEGNN(Satorrasetal.,2021)encodetype
outputthatremainsunchangedbytransformationsapplied
informationandrelativedistance,andarefurtherusedinvec-
totheinput. Forfurtherdetailsonthebackgroundofgroup
torscalingfunctionstotransformtheequivariantrepresenta-
theory,wereferreaderstotheappendixof(Thomasetal.,
tions. PaiNN(Schüttetal.,2021)extendedthisframework
2018;Andersonetal.,2019;Fuchsetal.,2020).
to include the Hardamard product operation to transform
theequivariantrepresentations. Basedontheoperationsof Molecular systems are naturally located in the three-
PaiNN,TorchMD-Net(Thölke&DeFabritiis,2022)further dimensionalEuclideanspace,andthegrouprelatedtotrans-
proposedamodifiedversionoftheself-attentionmodules lationsandrotationsisknownasSE(3). Foreachelement
to update invariant representations and achieved better g intheSE(3)group,itsrepresentationonR3 canbepa-
performanceoninvarianttasks. Allegro(Musaelianetal., rameterizedbypairsoftranslationvectorst ∈ R3 andor-
2023) instead uses tensor product operations to update thogonaltransformationmatricesR∈R3×3,det(R)=1,
equivariantfeaturesandinteractsequivariantandinvariant i.e., g = (t,R). Given a vector x ∈ R3, we have
featuresbyusingweight-generationmodules. Incontrast, ρR3(g)[x] := Rx+t. For molecular modeling, it is es-
our GeoMFormer aims to achieve strong performance
sentialtolearnmolecularrepresentationsthatencodethe
on both invariant and equivariant tasks at the same time,
rotationequivarianceandtranslationinvarianceconstraints.
whichmotivatesageneraldesignphilosophytolearnboth
Formally,letV denotethespaceofmolecularsystems,for
M
invariantandequivariantrepresentationsofhighquality,en- eachatomi,wedefineequivariantrepresentationϕE and
ablingsimultaneouslyandcompletelymodelinginteratomic invariantrepresentationϕI if∀g =(t,R)∈SE(3),M=
interactionswithin/acrossfeaturespacesinaunifiedmanner
(X,R)∈V ,thefollowingconditionsaresatisfied:
M
(Sec4.1). WereferinterestedreaderstoAppendixC.3for
morediscussionsandAppendixDformorerelatedworks. ϕE :V M →R3×d,
RϕE(X,{r ,...,r })=ϕE(X,{Rr ,...,Rr })
1 n 1 n
3.Preliminary
ϕE :V →R3×d,
M
(2)
3.1.Notations&GeometricConstraints ϕE(X,{r ,...,r })=ϕE(X,{r +t,...,r +t})
1 n 1 n
ϕI :V →Rd,
WedenoteamolecularsystemasM,whichismadeupofa M
collectionofatomsheldtogetherbyattractiveforces. Let ϕI(X,{r ,...,r })=ϕI(X,{Rr +t,...,Rr +t})
1 n 1 n
X ∈Rn×d denotetheatomswithfeatures,wherenisthe
number of atoms, and d is the feature dimension. Given 3.2.Attentionmodule
atomi,weuser ∈ R3 todenoteitscartesiancoordinate
i The attention module lies at the core of the Transformer
inthethree-dimensionalEuclideanspace. WedefineM=
architecture (Vaswani et al., 2017), and it is formu-
(X,R),whereR={r ,...,r }.
1 n lated as querying a dictionary with key-value pairs, e.g.,
Innature, molecularsystemsaresubjecttophysicallaws Attention(Q,K,V) = softmax(Q √KT )V, where d is the
d
thatimposegeometricconstraintsontheirpropertiesand hiddendimension,andQ(Query),K (Key),V (Value)are
behaviors. For instance, if the position of each atom in specifiedasthehiddenfeaturesofthepreviouslayer. The
a molecular system is translated by a constant vector in multi-headvariantoftheattentionmoduleiswidelyused,
Euclidean space, the total energy of the system remains asitallowsthemodeltojointlyattendtoinformationfrom
unchanged. If a rotation is applied to each position, the differentrepresentationsubspaces. Itisdefinedasfollows:
directionoftheforceoneachatomwillalsorotate. Math-
Multi-head(Q,K,V)=Concat(head ,··· ,head )WO
ematically,thesegeometricconstraintsaredirectlyrelated 1 H
to the concepts of invariance and equivariance in group head =Attention(QWQ,KWK,VWV),
k k k k
3GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
whereWQ ∈ Rd×dH,WK ∈ Rd×dH,WV ∈ Rd×dH,and red) and equivariant (colored in blue) representations are
k k k
WO ∈RHdH×darelearnablematrices,H isthenumberof updatedinthefollowingmanner:
heads. d isthedimensionofeachattentionhead.
H

Servingasagenericbuildingblock,theattentionmodule
 Z′I,l =ZI,l +Inv-Self-Attn(QI,l,KI,l,VI,l)
Z′′I,l =Z′I,l +Inv-Cross-Attn(QI,l,KI_E,l,VI_E,l)
can be used in various ways. On the one hand, the self- 
ZI,l+1 =Z′′I,l+Inv-FFN(Z′′I,l), InvariantStream
attention module specifies Query, Key, and Value as the

samehiddenrepresentation,therebyextractingcontextual  Z′E,l =ZE,l +Equ-Self-Attn(QE,l,KE,l,VE,l)
informationfortheinput. Ithasbeenoneofthekeycom- Z′′E,l =Z′E,l +Equ-Cross-Attn(QE,l,KE_I,l,VE_I,l)
ponents in Transformer-based foundation models across

ZE,l+1 =Z′′E,l+Equ-FFN(Z′′E,l), EquivariantStream
variousdomains(Devlinetal.,2019;Brownetal.,2020; (3)
Dosovitskiyetal.,2021;Liuetal.,2021;Yingetal.,2021a;
whereldenotesthelayerindex. Inthisframework,theself-
Jumper et al., 2021; Ji et al., 2023). On the other hand,
attentionmodulesandfeed-forwardnetworksareusedto
thecross-attentionmodulespecifiesthehiddenrepresenta-
iterativelyupdaterepresentationsineachstream. Thecross-
tionfromonespaceasQuery,andtherepresentationfrom
attentionmodulesuserepresentationsfromonestreamto
theotherspaceasKey-Valuepairs, e.g. encoder-decoder
queryKey-Valuepairsfromtheotherstream. Byusingthis
attentionforsequence-to-sequencelearning. Asthecross-
mechanism, abidirectionalbridgeisestablishedbetween
attentionmodulebridgestwospaces,ithasbeenalsowidely
invariantandequivariantstreams. Besidesthecontextual
usedbeyondTransformerforinformationfusionandimprov-
informationfromtheinvariantstreamitself, theinvariant
ing representations (Lee et al., 2018; Huang et al., 2019;
representationscanfreelyattendtomoregeometricalsig-
Jaegleetal.,2021;2022).
nals from the equivariant stream. Similarly, the equivari-
antrepresentationscanbenefitfromusingmorenon-linear
4.GeoMFormer transformationsintheinvariantrepresentations. Withthe
cross-attentionmodules,theexpressivenessofbothinvariant
In this section, we introduce GeoMFormer, a novel
andequivariantrepresentationlearningislargelyimproved,
Transformer-basedmolecularmodelforlearninginvariant
whichallowssimultaneouslyandcompletelymodelingin-
andequivariantrepresentationsofhighquality. Webeginby
teratomicinteractionswithin/acrossfeaturespacesinauni-
elaboratingonthekeydesignsofGeoMFormer,whichform
fied manner. In this regard, as highlighted by different
ageneralframeworktoguidethedevelopmentofgeometric
colors,theQuery,Key,andValueintheself-attentionmod-
molecularmodels(Sec4.1),Nextwethoroughlydiscussthe
ules(Inv-Self-Attn,Equ-Self-Attn)andthecross-attention
implementationdetailsofGeoMFormer(Sec4.2).
modules (Inv-Cross-Attn,Equ-Cross-Attn) are differ-
entlyspecified,whichshouldcarefullyencodethegeometric
4.1.AGeneralDesignPhilosophy constraintsmentionedinSection3.1,asintroducedbelow.
Aspreviouslystated,severalexistingworkslearnedinvari-
antrepresentationsusinginvariantfeatures,e.g.,distancein- DesiderataforInvariantSelf-Attention. Giventhein-
formation,whichmayhavedifficultyinextractingotheruse- variant representation ZI, the Query, Key and Value
fulstructuralsignalsandcannotdirectlyperformequivariant in Inv-Self-Attn are calculated via a function mapping
tasks. Someotherworksdevelopedequivariantmodelsvia ψI : Rn×d → Rn×d, i.e., QI = ψ QI(ZI),KI =
equivariantoperations,whichareeitherheuristicorcostly ψI (ZI),VI =ψI (ZI). Essentially,theattentionmodule
K V
anddonotguaranteebetterperformanceoninvarianttasks linearlytransformstheValueVI,withtheweightsbeing
comparedtoinvariantmodels. Instead,weaimtodevelopa calculatedfromthedotproductbetweentheQueryandKey
generaldesignprinciple,whichguidesthedevelopmentof (i.e., attention scores). In this regard, if both VI and the
amodelthataddressesthedisadvantagesaforementionedin attention scores preserve the invariance, then the output
bothinvariantandequivariantrepresentationlearning. satisfiestheinvariantconstraint, i.e., ψI isrequiredtobe
invariant.Underthiscondition,itiseasytochecktheoutput
We call our model GeoMFormer, which is a two-stream
representationofthismodulekeepstheinvariance,whichis
Transformermodeltoencodeinvariantandequivariantinfor-
provedinAppendixB.1.
mation. EachstreamisbuiltupusingstackedTransformer
blocks, eachofwhichconsistsofaself-attentionmodule
Desiderata for Equivariant Self-Attention. Similarly,
andacross-attentionmodule,followedbyafeed-forward
network. For each atom k ∈ [n], we use zI ∈ Rd and
giventheequivariantinputZE,theQuery,KeyandValue
zE ∈ R3×d to denote its invariant and equivk ariant repre- in Equ-Self-Attn are calculated via a function mapping
k ψE : Rn×3×d → Rn×3×d, i.e., QE = ψE(ZE),KE =
sentationsrespectively. LetZI = [zI⊤ ;...;zI⊤ ] ∈ Rn×d Q
1 n ψE(ZE),VE =ψE(ZE). Similarly,ψE isrequiredtobe
andZE =[zE;...;zE]∈Rn×3×d,theinvariant(coloredin K V
1 n equivariant. However,thisstillcannotguaranteethemodule
4GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
tobeequivariantifstandardattentionisused. Wemodified 4.2.ImplementationDetailsofGeoMFormer
α =(cid:80)d QE KE ⊤ ,whereQE ∈R3de-
ij k=1 [i,:,k] [j,:,k] [i,:,k] Following the design guidance in Section 4.1, we pro-
notesthek-thdimensionoftheatomi’sQuery. Itisstraight-
pose Geometric Molecular Transformer (GeoMFormer).
forward to check the equivariance is preserved, which is
TheoverallarchitectureofGeoMFormerisshowninFig-
provedinAppendixB.1.
ure1,whichiscomposedofstackedGeoMFormerblocks
(Eqn.(4)). We introduce the instantiations of the self-
DesiderataforCross-attentionsbetweenthetwoStreams.
attention,cross-attentionandFFNmodulesbelowandprove
In each stream, the cross-attention module is used to
thepropertiestheysatisfyinAppendixB.2. Wealsoincor-
leverage information from the other stream. We call
poratewidelyusedmoduleslikeLayerNormalization(Ba
thecrossattentionintheinvariantstreaminvariant-cross-
etal.,2016)andStructuralEncodings(Shietal.,2022)for
equivariant attention, and call the cross attention in the
betterempiricalperformance. Duetothespacelimits,we
equivariant stream equivariant-cross-invariant attention,
referreaderstoAppendixAforfurtherdetails.
i.e.,Inv-Cross-AttnandEqu-Cross-Attn. Thedifference
betweenthetwocrossattentionliesinhowtheQuery,Key,
InstantiationofSelf-Attention. InGeoMFormer,thelin-
Valuearespecified:
earfunctionisusedtoimplementbothψI :Rn×d →Rn×d
Invariant-cross-EquivariantAttention (Inv-Cross-Attn)
andψE :Rn×3×d →Rn×3×d:
QI_E =ψI(ZI),KI_E =ψI_E(ZI,ZE),VI_E =ψI_E(ZI,ZE)
Q K V QI =ψI(ZI)=ZIWI, QE =ψE(ZE)=ZEWE,
Q Q Q Q
Equivariant-cross-InvariantAttention (Equ-Cross-Attn) KI =ψI (ZI)=ZIWI, KE =ψE(ZE)=ZEWE, (5)
K K K K
QE_I =ψE(ZE),KE_I =ψE_I(ZE,ZI),VE_I =ψE_I(ZE,ZI) VI =ψI(ZI)=ZIWI VE =ψE(ZE)=ZEWE
Q K V V V V V
(4)
First, for Query QI_E and QE_I, the requirement to ψI whereW{I,E} arelearnableparameters.
{Q,K,V}
andψE remainsthesameaspreviouslystated. Moreover,
as distinguished by different colors, the Key-Value pairs InstantiationofCross-Attention. Aspreviouslystated,
andtheQueryarecalculatedindifferentways,forwhich both ψI_E and ψE_I in the cross-attention modules fuse
therequirementshouldbeseparatelyconsidered. Notethat representationsfromdifferentspaces(invariant&equivari-
bothVI_E andVE_I arestilllinearlytransformedbythe ant)intotargetspaces. IntheInvariant-cross-Equivariant
cross-attentionmodules. IfVI_E preservestheinvariance attention module (Inv-Cross-Attn), to obtain the Key-
andVE_I preservestheequivariance, thentheremaining Valuepairs,theequivariantrepresentationsaremappedto
condition is to keep the invariance of the attention score the invariant space. For the sake of simplicity, we use
calculation. Thatistosay,fortheInv-Cross-Attn,bothψI the dot-product operation < ·,· > to instantiate ψI_E.
and ψI_E are required to be invariant. It is similar to the Given X,Y ∈ Rn×3×d, Z =< X,Y >∈ Rn×d, where
Equ-Cross-AttnthatbothψE andψE_I arerequiredtobe Z = X ⊤Y . Then the Key-Value pairs in
[i,k] [i,:,k] [i,:,k]
equivariant. Inthisway,theoutputsofbothcross-attention Inv-Cross-Attnarecalculatedas:
modulesareunderthecorrespondinggeometricconstraints,
KI_E =ψI_E(ZI,ZE)=<ZEWI_E,ZEWI_E >,
whichisprovedinAppendixB.1. K K,1 K,2 (6)
VI_E =ψI_E(ZI,ZE)=<ZEWI_E,ZEWI_E >
V V,1 V,2
Discussion. Thecarefullydesignedblocksoutlinedabove whereWI_E,WI_E,WI_E,WI_E ∈ Rd×dH forKeyand
provide a general design philosophy for encoding the ge- K,1 K,2 V,1 V,2
Value are learnable parameters. On the other hand, the
ometric constraints and bridging the invariant and equiv-
invariant representations are mapped to the equivariant
ariant molecular representations, which lie at the core of
space in the Equivariant-cross-Invariant attention mod-
ourframework. Notethatthetranslationinvariancecanbe
ule (Equ-Cross-Attn). To achieve this goal, we use
easilypreservedbyencodingrelativestructuresignalsofthe
the scalar product ⊙ to instantiate ψE_I. Given X ∈
input. Itisalsoworthpointingoutthatwedonotrestrictthe Rn×3×d,Y ∈ Rn×d, Z = X ⊙ Y ∈ Rn×3×d, where
specificinstantiationofeachcomponent,andvariousdesign
Z = X ·Y . Using this operation, the Key-
choices can be adopted as long as they meet the require- [i,j,k] [i,j,k] [i,k]
ValuepairsinEqu-Cross-Attnarecalculatedas:
mentsmentionedabove.Moreover,weprovethatourframe-
workcanincludemanypreviousmodelsasaninstantiation, KE_I =ψE_I(ZE,ZI)=ZEWE_I ⊙ZIWE_I,
K K,1 K,2 (7)
e.g.,PaiNN(Schüttetal.,2021)andTorchMD-Net(Thölke VE_I =ψE_I(ZE,ZI)=ZEWE_I ⊙ZIWE_I
V V,1 V,2
&DeFabritiis,2022),canbeextendedtoencodeadditional
geometricconstraints(Cornwell,1997),whicharepresented whereWE_I,WE_I,WE_I,WE_I ∈Rd×dH arelearnable.
K,1 K,2 V,1 V,2
inAppendixB.1.Inthiswork,wepresentasimpleyeteffec-
tivemodelinstancethatimplementsthisdesignphilosophy, InstantiationofFeed-ForwardNetworks. Besidesthe
whichwewillthoroughlyintroduceinthenextsubsection. attention modules, the feed-forward networks also play
5GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
GeoMFormer Block Inv-Self-Attn Inv-Cross-Attn
ZI ZI ZE
ZI, l ZE, l
        Inv-FFN
Q𝑊𝑊𝑄𝑄𝐼𝐼 K𝑊𝑊𝐾𝐾𝐼𝐼 V𝑊𝑊𝑉𝑉𝐼𝐼 𝑊𝑊𝑄𝑄𝐼𝐼_𝐸𝐸 𝑊𝑊𝐾𝐾<𝐼𝐼 ,_ 1𝐸𝐸 ,>𝑊𝑊𝐾𝐾𝐼𝐼 ,_ 2𝐸𝐸 𝑊𝑊𝑉𝑉<𝐼𝐼 ,_ 1𝐸𝐸 ,>𝑊𝑊𝑉𝑉𝐼𝐼 ,_ 2𝐸𝐸
ZI
Inv Inv Equ Equ Q K V
Invariant Attention
Self Cross Cross Self 
Invariant Attention
Attn Attn Attn Attn AttnProb × AttnProb × GELU( 𝑊𝑊1𝐼𝐼 )𝑊𝑊2𝐼𝐼
Invariant Output Invariant Output Invariant Output
ZE ZE ZE ZI ZE ZI
         
Inv-FFN Equ-FFN Q𝑊𝑊𝑄𝑄𝐸𝐸 K𝑊𝑊𝐾𝐾𝐸𝐸 V𝑊𝑊𝑉𝑉𝐸𝐸 Q𝑊𝑊𝑄𝑄𝐸𝐸_𝐼𝐼 𝑊𝑊𝐾𝐾𝐸𝐸 ,1_𝐼𝐼⊙ K𝑊𝑊𝐾𝐾𝐸𝐸 ,2_𝐼𝐼 𝑊𝑊𝑉𝑉𝐸𝐸 ,1_⊙𝐼𝐼 V𝑊𝑊𝑉𝑉𝐸𝐸 ,2_𝐼𝐼 𝑊𝑊1𝐸𝐸⊙ GELU( 𝑊𝑊2𝐼𝐼 )

𝐸𝐸
Equivariant Attention Equivaria𝑊𝑊nt3 Output
Equivariant Attention
AttnProb × AttnProb × Equ-FFN
Equivariant Output Equivariant Output
ZI, l+1 ZE, l+1
Equ-Self-Attn Equ-Cross-Attn
Figure1. AnillustrationofourGeoMFormermodelarchitecture.
important roles in refining contextual representations. In 5.Experiments
the invariant stream, the feed-forward network is kept
Inthissection,weempiricallyinvestigateourGeoMFormer
unchanged from the standard Transformer model, i.e.,
Inv-FFN(Z′′I) = GELU(Z′′IWI)WI, where WI ∈ onextensivetasks. Inparticular,wecarefullydesignseveral
Rd×r,WI ∈ Rr×d and r denotes1 the h2 idden dimen1 sion experimentscoveringdifferenttypesoftasks(invariant&
2 equivariant),data(simplemolecules&adsorbate-catalyst
of the FFN layer. In the equivariant stream, it is worth
complexes & particle systems), and scales, as shown in
notingthatcommonlyusednon-linearactivationfunctions
Table1. Wealsoconductanablationstudytothoroughly
breaktheequivariantconstraints. InourGeoMFormer,we
verify the effectiveness of each design choice of our Ge-
use the invariant representations as a gating function to
oMFormer. Due to space limits, we present more results
non-linearly activate the equivariant representations, i.e.,
Equ-FFN(Z′′E) = (Z′′EWE ⊙ GELU(Z′′IWI))WE, (MD17,AblationStudy)inAppendixE.
1 2 3
whereWE,WI ∈Rd×r,WE ∈Rr×d.
1 1 2
5.1.OC20Performance(Invariant&Equivariant)
Input Layer. Given a molecular system M = (X,R),
TheOpenCatalyst2020(OC20)dataset(Chanussotetal.,
we set the invariant representation at the input as ZI,0 =
2021)wascreatedforcatalystdiscoveryandoptimization,
X, where X ∈ Rd is a learnable embedding vector in-
i whichisoneofthelargestmolecularmodelingbenchmarks
dexed by the atom i’s type. For the equivariant repre-
and has great significance to advance renewable energy
sentation, we set ZE,0 = ˆr′g(||r′||)⊤ ∈ R3×d, where
i i i processes for crucial social and energy challenges. Each
we consider both the direction ˆr′ ∈ R3 and the scale
i data is in the form of the adsorbate-catalyst complex.
g(||r′||) ∈ Rd of the each atom’s mean-centered posi-
i Giventheinitialstructureofacomplex,DensityFunctional
tion r′. g : R → Rd is instantiated by the Gaussian
i Theory (DFT) tools are used to accurately simulate the
Basis Kernel, i.e., g(||r′||) = ψ W, ψ = [ψ1;...;ψd]⊤,
ψk = −√ 1 expi (cid:18) −1(cid:16) γi i∥r′ i∥+i βi−µk(cid:17)i 2(cid:19) ,k i = r se cl ea nx aa rt ii oo sn ,p thr eoc re es las xu en dti el na ec rh gi yev ai nn dg se tq ru ui cl ti ub rr eiu om f. thIn ep cora mct pi lc ea xl
i 2π|σk| 2 |σk|
are of great interest for catalyst discovery. In this regard,
1,...,d,whereW ∈Rd×dislearnable,γ ,β arelearnable
i i we focus on two significant tasks: Initial Structure to
scalarsindexedbytheatomtype,andµk,σk arelearnable
RelaxedEnergy(IS2RE)andInitialStructuretoRelaxed
kernelcenterandscalingfactorofthek-thKernel.Notethat
Structure (IS2RS), which require a model to directly
ourGeoMFormerisnotrestrictedtothesechoices,which
predict the relaxed energy and structure given the initial
canencodeadditionalfeaturesiftheconstraintsaresatisfied,
asdiscussedinAppendixB.2.
6GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
Table1. Summarizationofempiricalevaluationsetup.
Dataset TaskDescription TaskType DataType Trainingsetsize
OC20,IS2RE(Chanussotetal.,2021) EquilibriumEnergyPrediction(Sec5.1.1) Invariant Adsorbate-Catalystcomplex 460,328
OC20,IS2RS(Chanussotetal.,2021) EquilibriumStructurePrediction(Sec5.1.2) Equivariant Adsorbate-Catalystcomplex 460,328
PCQM4Mv2(Huetal.,2021) HOMO-LUMOGapPrediction(Sec5.2) Invariant Simplemolecule 3,378,606
Molecule3D(Wangetal.,2022) HOMO-LUMOGapPrediction(Sec5.3) Invariant Simplemolecule 2,339,788
N-BodySimulation(Satorrasetal.,2021) PositionPrediction(Sec5.4) Equivariant ParticleSystem 3,000
MD17(Chmielaetal.,2017) ForceFieldModeling(SecE.6) Inv&Equ Simplemolecule 950
AblationStudy Energy/Force/PositionPrediction(SecE.7) Inv&Equ - -
Table2.ResultsonOC20IS2REvalset.Wereporttheofficialresultsofbaselinesfromtheoriginalpaper.Boldvaluesdenotethebest.
EnergyMAE(eV)↓ EwT(%)↑
Model ID OODAds. OODCat. OODBoth Average ID OODAds. OODCat. OODBoth Average
CGCNN(Xie&Grossman,2018) 0.6203 0.7426 0.6001 0.6708 0.6585 3.36 2.11 3.53 2.29 2.82
SchNet(Schüttetal.,2018) 0.6465 0.7074 0.6475 0.6626 0.6660 2.96 2.22 3.03 2.38 2.65
DimeNet++(Gasteigeretal.,2020a) 0.5636 0.7127 0.5612 0.6492 0.6217 4.25 2.48 4.4 2.56 3.42
GemNet-T(Gasteigeretal.,2021) 0.5561 0.7342 0.5659 0.6964 0.6382 4.51 2.24 4.37 2.38 3.38
SphereNet(Liuetal.,2022b) 0.5632 0.6682 0.5590 0.6190 0.6024 4.56 2.70 4.59 2.70 3.64
Graphormer-3D(Shietal.,2022) 0.4329 0.5850 0.4441 0.5299 0.4980 - - - - -
GNS(Pfaffetal.,2020) 0.47 0.51 0.48 0.46 0.4800 - - - - -
Equiformer(Liao&Smidt,2023) 0.4156 0.4976 0.4165 0.4344 0.4410 7.47 4.64 7.19 4.84 6.04
GeoMFormer(ours) 0.3883 0.4562 0.4037 0.4083 0.4141 11.26 6.70 9.97 6.42 8.59
structureasinputrespectively1. Thetrainingsetforboth Table3.ResultsonOC20IS2RSvalidationset. Allmodelsare
tasks is composed of over 460,328 catalyst-adsorbate trained and evaluated under the direct prediction setting. Bold
complexes. Tobetterevaluatethemodel’sperformance,the valuesindicatethebest.
validationandtestsetsconsiderthein-distribution(ID)and ADwT(%)↑
Model ID OODAds OODCat OODBoth Average
out-of-distributionsettingswhichusesunseenadsorbates
PaiNN(Schüttetal.,2021) 3.29 2.37 3.10 2.33 2.77
(OOD-Ads), catalysts (OOD-Cat) or both (OOD-Both), TorchMD-Net(Thölke&DeFabritiis,2022) 3.32 3.35 2.94 2.89 3.13
containingapproximately200,000complexesintotal. Spinconv(Shuaibietal.,2021) 5.81 4.88 5.63 4.84 5.29
GemNet-dT(Gasteigeretal.,2021) 6.87 7.10 6.03 7.08 6.77
GemNet-OC(Gasteigeretal.,2022) 11.31 12.20 4.40 5.55 8.36
5.1.1.IS2REPERFORMANCE(INVARIANT) GeoMFormer(ours) 11.45 10.52 9.94 10.78 10.67
As an energy prediction task, the IS2RE task evaluates significant considering the challenging task. The results
how well the model learns invariant representations. We indeeddemonstratetheeffectivenessofourGeoMFormer
followtheexperimentalsetupofGraphormer-3D(Shietal., frameworkonlearninginvariantrepresentations.
2022). ThemetricoftheIS2REtaskistheMeanAbsolute
Error(MAE)andthepercentageofdatainstancesinwhich 5.1.2.IS2RSPERFORMANCE(EQUIVARIANT)
thepredictedenergyiswithina0.02eVthreshold(EwT).
Furthermore,weusetheIS2RStasktoevaluatethemodel’s
We choose several strong baselines covering geometric
ability to perform the equivariant prediction task. The
molecular models using different approaches. Due to
metric of the IS2RS task is the Average Distance within
space limits, the detailed description of training settings
Threshold (ADwT) across different thresholds. The Dis-
and baselines is presented in Appendix E.1. The results
tance within Threshold is computed as the percentage of
areshowninTable2. OurGeoMFormeroutperformsthe
structureswiththeatompositionMAEbelowthethreshold.
compared baselines significantly, achieving impressive
Were-implementseveralcompetitivebaselinesunderthe
performanceespeciallyontheout-of-distributionvalidation
directpredictionsettingforcomparison. Werefertheread-
sets, e.g., 42.2% relative EwT improvement on average
erstoAppendixE.2formoredetailsonthesettings. From
comparedtothebestbaseline. Inparticular,theimprove-
Table 3, we can see that the IS2RS task under the direct
mentontheEnergywithinThreshold(EwT)metricisalso
predictionsettingisratherdifficult. Thecomparedbaseline
1Insteadofusingtheiterativerelaxationsettingthatrequires modelsconsistentlyachievelowADwT.OurGeoMFormer
massivesingle-pointstructure-to-energy-forcedatatotraininga achievesthebest(e.g.,27.6%relativeADwTimprovement
force-fieldmodel(Chanussotetal.,2021),wefocusonthedirect on average compared to the best baseline), which indeed
predictionsettingthatonlyusesinitial-relaxedstructurepairsdata
verifiesthesuperiorabilityofourGeoMFormerframework
astheinputandlabel,whichisefficientwhilemorechallenging.
toperformequivariantmoleculartasks.
7GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
Table4.Results on PCQM4Mv2. The evaluation metric is the Table6.Results on N-body System Simulation. We report the
Mean Absolute Error (MAE). We report the official results of officialresultsofbaselines.Boldvaluesindicatethebest.
baselines. ∗indicatesthebestperformanceachievedbymodels Model MSE↓
withthesamecomplexity(ndenotesthenumberofatoms). SE(3)Transformer(Fuchsetal.,2020) 0.0244
Model Complexity ValidMAE↓ TensorFieldNetwork(Thomasetal.,2018) 0.0155
MLP-Fingerprint(Huetal.,2021) 0.1735 GraphNeuralNetwork(Gilmeretal.,2017) 0.0107
GINE-VN(Brossardetal.,2020;Gilmeretal.,2017) 0.1167 RadialField(Köhleretal.,2019) 0.0104
GCN-VN(Kipf&Welling,2017;Gilmeretal.,2017) O(n) 0.1153 EGNN(Satorrasetal.,2021) 0.0071
GIN-VN(Xuetal.,2019;Gilmeretal.,2017) 0.1083
GeoMFormer(ours) 0.0047
DeeperGCN-VN(Lietal.,2020;Gilmeretal.,2017) 0.1021*
TokenGT(Kimetal.,2022) 0.0910
EGT(Hussainetal.,2022) 0.0869
GRPE(Parketal.,2022) 0.0867
FromTable4. OurGeoMFormerachievesthelowestMAE
Graphormer(Yingetal.,2021a;Shietal.,2022) O(n2) 0.0864
GraphGPS(Rampášeketal.,2022) 0.0858 amongthequadraticmodels, e.g., 6.7%relativeMAEre-
GPS++(Mastersetal.,2022) 0.0778
duction compared to the previous best model. Besides,
Transformer-M(Luoetal.,2023) 0.0787
GEM-2(Liuetal.,2022a) O(n3) 0.0793 compared to the best model Uni-Mol+ (Lu et al., 2023),
Uni-Mol+(Luetal.,2023) 0.0708* ourGeoMFormerachievescompetitiveperformancewhile
GeoMFormer(ours) O(n2) 0.0734*
keepingtheefficiency(O(n2)complexity),whichcanbe
morebroadlyappliedtolargemolecularsystems. Overall,
Table5.Results on Molecule3D for both random and scaffold
theresultsfurtherverifytheeffectivenessofGeoMFormer
splits. We report the official results of baselines. Bold values
oninvariantrepresentationlearning.
denotethebest.
MAE↓
Model Random Scaffold 5.3.Molecule3DPerformance(Invariant)
GIN-Virtual(Huetal.,2021) 0.1036 0.2371
SchNet(Schüttetal.,2018) 0.0428 0.1511 Molecule3D (Xu et al., 2021) is a newly proposed large-
DimeNet++(Gasteigeretal.,2020a) 0.0306 0.1214
scaledatasetcuratedfromthePubChemQCproject(Maho,
SphereNet(Liuetal.,2022b) 0.0301 0.1182
ComENet(Wangetal.,2022) 0.0326 0.1273 2015;Nakata&Shimazaki,2017). Eachmoleculehasthe
PaiNN(Schüttetal.,2021) 0.0311 0.1208
DFT-calculatedequilibriumgeometricstructure. Thetaskis
TorchMD-Net(Thölke&DeFabritiis,2022) 0.0303 0.1196
GeoMFormer(ours) 0.0252 0.1045 topredicttheHOMO-LUMOenergygap,whichisthesame
asPCQM4Mv2. Thedatasetcontains3,899,647molecules
in total and is split into training, validation, and test sets
5.2.PCQM4Mv2Performance(Invariant)
withthesplittingratio6:2:2. Inparticular,bothrandom
PCQM4Mv2isoneofthelargestquantumchemicalprop- and scaffold splitting methods are adopted to thoroughly
erty datasets from the OGB Large-Scale Challenge (Hu evaluatethein-distributionandout-of-distributionperfor-
etal.,2021). Givenamolecule,itsHOMO-LUMOenergy manceofgeometricmolecularmodels. Following(Wang
gapoftheequilibriumstructureisrequiredtopredict,evalu- et al., 2022), we compare our GeoMFormer with several
atingthemodel’sabilityofinvariantprediction. Thisprop- competitive baselines. Detailed descriptions of the train-
ertyishighlyrelatedtoreactivity,photoexcitation,charge ing settings and baselines are presented in Appendix E.4.
transport,andotherrealapplications. DFTtoolsareused It can be easily seen from Table 5 that our GeoMFormer
tocalculatetheHOMO-LUMOgapforground-truthlabels. consistentlyoutperformsallbaselinesonbothrandomand
Thetotalnumberoftrainingsamplesisaround3.37million. scaffoldsplitsettings,e.g.,16.3%and11.6%relativeMAE
reductioncomparedtothepreviousbestmodelrespectively.
Inapracticalsetting,theDFT-calculatedequilibriumgeo-
metricstructureofeachtrainingsampleisprovided,while
5.4.N-BodySimulationPerformance(Equivariant)
onlyinitialstructurescanbegeneratedbyefficientbutinac-
curatetools(e.g.,RDKit(Landrum,2016))foreachvalida- Simulating dynamical systems consisting of a set of geo-
tionsample. Inthisregard,weadoptonerecentapproach metricobjectsinteractingunderphysicallawsiscrucialin
(Uni-Mol+ (Lu et al., 2023)) to handle this task. During manyapplications,e.g. moleculardynamicsimulation. Fol-
training,themodelreceivesRDKit-generatedinitialstruc- lowingFuchsetal.(2020);Satorrasetal.(2021),weusea
tures as the input, and predicts both the HOMO-LUMO syntheticn-bodysystemsimulationtaskasanextensionof
energygapandtheequilibriumstructurebyusingbothin- molecularmodelingtasks. Thistaskrequiresthemodelto
variantandequivariantrepresentations. Aftertraining,the forecastthepositionsofasetofparticles,whicharemod-
modelcanbeusedtopredicttheHOMO-LUMOgaptarget eled by simple interaction rules, yet can exhibit complex
byonlyusingtheinitialstructure,whichmeetstherequire- dynamics. Thus,themodel’sabilitytoperformequivariant
mentofthesettings. Wecomparevariousbaselinesinthe predictiontasksiscarefullyevaluated. Inthisdataset,the
leaderboardforcomparison. Moredetailsofthesettingsare simulatedsystemconsistsof5particles,eachofwhichcar-
presentedinAppendixE.3. riesapositiveornegativechargeandhasaninitialposition
8GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
andvelocityinthethree-dimensionalEuclideanspace. The Therealsoexistsomelimitationstoourwork. Servingas
systemiscontrolledbyphysicalrulesinvolvingattractive ageneralarchitecture,theabilitytoscaleupboththemodel
andrepulsiveforces. Thedatasetcontains3.000trajectories anddatasetsizesisofconsiderableinteresttothecommu-
fortraining,2.000trajectoriesforvalidation,and2.000tra- nity, which has been partially explored in our extensive
jectoriesfortesting. Wecompareseveralstrongbaselines experiment.Additionally,ourmodelcanalsobeextendedto
following Satorras et al. (2021). Due to space limits, the encompassadditionaldownstreaminvariantandequivariant
detailsofthedatageneration,trainingsettingsandbaselines tasks,whichwehaveearmarkedforfutureresearch.
arepresentedinAppendixE.5. TheresultsareshowninTa-
ble6.OurGeoMFormerachievesthebestperformancecom-
References
paredtoallbaselines. Inparticular,thesignificant33.8%
MSE reduction indeed demonstrates the GeoMFormer’s Anderson, B., Hy, T.S., andKondor, R. Cormorant: Co-
superiorabilityonlearningequivariantrepresentations. variantmolecularneuralnetworks. Advancesinneural
informationprocessingsystems,32,2019.
6.Conclusion
Ba,J.L.,Kiros,J.R.,andHinton,G.E.Layernormalization.
In this paper, we propose a general and flexible architec- arXivpreprintarXiv:1607.06450,2016.
ture,calledGeoMFormer,forlearninggeometricmolecular
representations. UsingthestandardTransformerbackbone, Batzner, S., Musaelian, A., Sun, L., Geiger, M., Mailoa,
twostreamsaredevelopedforlearninginvariantandequiv- J. P., Kornbluth, M., Molinari, N., Smidt, T. E., and
ariantrepresentationsrespectively. Inparticular,thecross- Kozinsky,B. E(3)-equivariantgraphneuralnetworksfor
attention mechanism is used to bridge these two streams, data-efficientandaccurateinteratomicpotentials. Nature
lettingeachstreamleveragecontextualinformationfromthe
communications,13(1):2453,2022.
otherstreamandenhanceitsrepresentations. Thissimple
Brandstetter, J., Hesselink, R., van der Pol, E., Bekkers,
yeteffectivedesignsignificantlyboostsbothinvariantand
E. J., and Welling, M. Geometric and physical quan-
equivariant modeling. Within the newly proposed frame-
tities improve e(3) equivariant message passing. In
work, many existing methods can be regarded as special
InternationalConferenceonLearningRepresentations,
instances, showing the generality of our method. All the
2022. URLhttps://openreview.net/forum?
empiricalresultsshowthatourGeoMFormercanachieve
id=_xwr8gOBeV1.
strongperformanceindifferentscenarios. Thepotentialof
ourGeoMFormercanbefurtherexploredinabroadrange
Brossard,R.,Frigo,O.,andDehaene,D. Graphconvolu-
ofapplicationsinmolecularmodeling.
tionsthatcanfinallymodellocalstructure. arXivpreprint
arXiv:2011.15069,2020.
Acknowledgements
Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,
We thank all the anonymous reviewers for the very care-
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
ful and detailed reviews as well as the valuable sugges-
Askell,A.,etal. Languagemodelsarefew-shotlearners.
tions. Their help has further enhanced our work. Di He
Advancesinneuralinformationprocessingsystems,33:
is supported by National Key R&D Program of China
1877–1901,2020.
(2022ZD0160300) and National Science Foundation of
China(NSFC62376007). LiweiWangissupportedbyNa-
Chanussot,L.,Das,A.,Goyal,S.,Lavril,T.,Shuaibi,M.,
tionalScienceFoundationofChina(NSFC62276005).
Riviere,M.,Tran,K.,Heras-Domingo,J.,Ho,C.,Hu,W.,
etal. Opencatalyst2020(oc20)datasetandcommunity
ImpactStatement
challenges. AcsCatalysis,11(10):6059–6072,2021.
This work newly proposes a general framework to learn
Chmiela,S.,Tkatchenko,A.,Sauceda,H.E.,Poltavsky,I.,
geometricmolecularrepresentations,whichhasgreatsig-
Schütt,K.T.,andMüller,K.-R. Machinelearningofac-
nificance in molecular modeling. Our model has demon-
curateenergy-conservingmolecularforcefields. Science
stratedconsiderablepositivepotentialforvariousphysical
advances,3(5):e1603015,2017.
andchemicalapplications,suchascatalystdiscoveryandop-
timization,whichcansignificantlycontributetotheadvance-
Cornwell,J.F. Grouptheoryinphysics: Anintroduction.
mentofrenewableenergyprocesses.However,itisessential
Academicpress,1997.
toacknowledgethepotentialnegativeimpactsincludingthe
developmentoftoxicdrugsandmaterials. Thus,stringent
Cotton,F.A. Chemicalapplicationsofgrouptheory. John
measuresshouldbeimplementedtomitigatetheserisks.
Wiley&Sons,1991.
9GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K. Bert: J., and Battaglia, P. Simple GNN regularisation for
Pre-training of deep bidirectional transformers for lan- 3d molecular property prediction and beyond. In In-
guageunderstanding. InProceedingsofthe2019Confer- ternational Conference on Learning Representations,
enceoftheNorthAmericanChapteroftheAssociationfor 2022. URLhttps://openreview.net/forum?
ComputationalLinguistics: HumanLanguageTechnolo- id=1wVvweK3oIb.
gies,Volume1(LongandShortPapers),pp.4171–4186,
Goodman, R. and Wallach, N. R. Representations and
2019.
invariantsoftheclassicalgroups. CambridgeUniversity
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, Press,2000.
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,
He,K.,Zhang,X.,Ren,S.,andSun,J. Deepresiduallearn-
M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby,
ingforimagerecognition. InProceedingsoftheIEEE
N. An image is worth 16x16 words: Transformers for
conferenceoncomputervisionandpatternrecognition,
imagerecognitionatscale. InInternationalConference
pp.770–778,2016.
on Learning Representations, 2021. URL https://
openreview.net/forum?id=YicbFdNTTy. Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B.,
Catasta, M., andLeskovec, J. Opengraphbenchmark:
Frank,T.,Unke,O.,andMüller,K.-R. So3krates: Equiv-
Datasets for machine learning on graphs. Advances in
ariantattentionforinteractionsonarbitrarylength-scales
neuralinformationprocessingsystems,33:22118–22133,
inmolecularsystems. AdvancesinNeuralInformation
2020.
ProcessingSystems,35:29400–29413,2022.
Hu, W., Fey, M., Ren, H., Nakata, M., Dong, Y., and
Fuchs, F., Worrall, D., Fischer, V., and Welling, M. Se
Leskovec, J. OGB-LSC: A large-scale challenge for
(3)-transformers: 3droto-translationequivariantattention machinelearningongraphs. InThirty-fifthConference
networks. AdvancesinNeuralInformationProcessing onNeuralInformationProcessingSystemsDatasetsand
Systems,33:1970–1981,2020. Benchmarks Track (Round 2), 2021. URL https:
//openreview.net/forum?id=qkcLxoC52kL.
Gasteiger, J., Giri, S., Margraf, J. T., and Günnemann,
S. Fast and uncertainty-aware directional message Huang,W.,Han,J.,Rong,Y.,Xu,T.,Sun,F.,andHuang,J.
passingfornon-equilibriummolecules. arXivpreprint Equivariantgraphmechanicsnetworkswithconstraints.
arXiv:2011.14115,2020a. InInternationalConferenceonLearningRepresentations,
2022. URLhttps://openreview.net/forum?
Gasteiger, J., Groß, J., and Günnemann, S. Direc-
id=SHbhHHfePhP.
tional message passing for molecular graphs. In In-
ternational Conference on Learning Representations, Huang,Z.,Wang,X.,Huang,L.,Huang,C.,Wei,Y.,and
2020b.URLhttps://openreview.net/forum? Liu,W.Ccnet:Criss-crossattentionforsemanticsegmen-
id=B1eWbxStPH. tation. In Proceedings of the IEEE/CVF international
conferenceoncomputervision,pp.603–612,2019.
Gasteiger,J.,Becker,F.,andGünnemann,S. Gemnet: Uni-
versaldirectionalgraphneuralnetworksformolecules. Hussain,M.S.,Zaki,M.J.,andSubramanian,D. Global
AdvancesinNeuralInformationProcessingSystems,34: self-attentionasareplacementforgraphconvolution. In
6790–6802,2021. Proceedingsofthe28thACMSIGKDDConferenceon
Knowledge Discovery and Data Mining, pp. 655–665,
Gasteiger, J., Shuaibi, M., Sriram, A., Günnemann, S., 2022.
Ulissi, Z. W., Zitnick, C. L., and Das, A. Gemnet-
OC: Developing graph neural networks for large and Jaegle,A.,Gimeno,F.,Brock,A.,Vinyals,O.,Zisserman,
diverse molecular simulation datasets. Transactions A.,andCarreira,J. Perceiver: Generalperceptionwithit-
on Machine Learning Research, 2022. ISSN 2835- erativeattention. InInternationalconferenceonmachine
8856. URLhttps://openreview.net/forum? learning,pp.4651–4664.PMLR,2021.
id=u8tvSxm4Bs.
Jaegle, A., Borgeaud, S., Alayrac, J.-B., Doersch, C.,
Ionescu, C., Ding, D., Koppula, S., Zoran, D., Brock,
Gilmer,J.,Schoenholz,S.S.,Riley,P.F.,Vinyals,O.,and
A.,Shelhamer,E.,Henaff,O.J.,Botvinick,M.,Zisser-
Dahl,G.E. Neuralmessagepassingforquantumchem-
man, A., Vinyals, O., and Carreira, J. Perceiver IO: A
istry. InInternationalconferenceonmachinelearning,
generalarchitectureforstructuredinputs&outputs. In
pp.1263–1272.PMLR,2017.
InternationalConferenceonLearningRepresentations,
Godwin, J., Schaarschmidt, M., Gaunt, A. L., Sanchez- 2022. URLhttps://openreview.net/forum?
Gonzalez,A.,Rubanova,Y.,Velicˇkovic´,P.,Kirkpatrick, id=fILj7WpI-g.
10GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
Ji, G.-P., Zhuge, M., Gao, D., Fan, D.-P., Sakaridis, C., Liao, Y.-L., Wood, B., Das, A., and Smidt, T.
andGool,L.V. Maskedvision-languagetransformerin Equiformerv2:Improvedequivarianttransformerforscal-
fashion. MachineIntelligenceResearch,20(3):421–434, ing to higher-degree representations. In The Twelfth
2023. InternationalConferenceonLearningRepresentations,
2024. URLhttps://openreview.net/forum?
Joshi, C. K., Bodnar, C., Mathis, S. V., Cohen, T., and id=mCOBKZmrzD.
Lio, P. On the expressive power of geometric graph
neuralnetworks. InKrause,A.,Brunskill,E.,Cho,K., Liu,L.,He,D.,Fang,X.,Zhang,S.,Wang,F.,He,J.,and
Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Pro- Wu, H. Gem-2: Next generation molecular property
ceedings of the 40th International Conference on Ma- prediction network by modeling full-range many-body
chineLearning,volume202ofProceedingsofMachine interactions,2022a.
LearningResearch,pp.15330–15355.PMLR,23–29Jul
2023.URLhttps://proceedings.mlr.press/ Liu, Y., Wang, L., Liu, M., Lin, Y., Zhang, X., Oztekin,
v202/joshi23a.html. B.,andJi,S. Sphericalmessagepassingfor3dmolec-
ular graphs. In International Conference on Learning
Jumper,J.,Evans,R.,Pritzel,A.,Green,T.,Figurnov,M., Representations(ICLR),2022b.
Ronneberger,O.,Tunyasuvunakool,K.,Bates,R.,Žídek,
Liu, Y., Cheng, J., Zhao, H., Xu, T., Zhao, P., Tsung, F.,
A.,Potapenko,A.,etal. Highlyaccurateproteinstructure
Li,J.,andRong,Y. SEGNO:Generalizingequivariant
predictionwithalphafold. Nature,596(7873):583–589,
graph neural networks with physical inductive biases.
2021.
In The Twelfth International Conference on Learning
Kim,J.,Nguyen,D.T.,Min,S.,Cho,S.,Lee,M.,Lee,H., Representations,2024. URLhttps://openreview.
andHong,S. Puretransformersarepowerfulgraphlearn- net/forum?id=3oTPsORaDH.
ers. InOh,A.H.,Agarwal,A.,Belgrave,D.,andCho,
Liu,Z.,Lin,Y.,Cao,Y.,Hu,H.,Wei,Y.,Zhang,Z.,Lin,
K. (eds.), Advances in Neural Information Processing
Systems,2022. URLhttps://openreview.net/ S.,andGuo,B. Swintransformer: Hierarchicalvision
forum?id=um2BxfgkT2_. transformerusingshiftedwindows. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision
Kipf, T. N. and Welling, M. Semi-supervised classi- (ICCV),pp.10012–10022,October2021.
fication with graph convolutional networks. In In-
Lu, S., Gao, Z., He, D., Zhang, L., and Ke, G. Highly
ternational Conference on Learning Representations,
2017. URLhttps://openreview.net/forum? accuratequantumchemicalpropertypredictionwithuni-
id=SJU4ayYgl. mol+,2023.
Luo,S.,Li,S.,Zheng,S.,Liu,T.-Y.,Wang,L.,andHe,D.
Köhler,J.,Klein,L.,andNoé,F. Equivariantflows: sam-
Yourtransformermaynotbeaspowerfulasyouexpect.
pling configurations for multi-body systems with sym-
InKoyejo,S.,Mohamed,S.,Agarwal,A.,Belgrave,D.,
metricenergies. arXivpreprintarXiv:1910.00753,2019.
Cho, K., and Oh, A. (eds.), Advances in Neural Infor-
Landrum, G. Rdkit: Open-source cheminformat- mationProcessingSystems,volume35,pp.4301–4315.
ics software. Github, 2016. URL https: CurranAssociates,Inc.,2022.
//github.com/rdkit/rdkit/releases/
tag/Release_2016_09_4. Luo,S.,Chen,T.,Xu,Y.,Zheng,S.,Liu,T.-Y.,Wang,L.,
andHe,D. Onetransformercanunderstandboth2d&3d
Lee,K.-H.,Chen,X.,Hua,G.,Hu,H.,andHe,X. Stacked moleculardata.InTheEleventhInternationalConference
crossattentionforimage-textmatching. InProceedings on Learning Representations, 2023. URL https://
oftheEuropeanconferenceoncomputervision(ECCV), openreview.net/forum?id=vZTp1oPV3PC.
pp.201–216,2018.
Luo,S.,Chen,T.,andKrishnapriyan,A.S. Enablingeffi-
Li, G., Xiong, C., Thabet, A., and Ghanem, B. Deep- cientequivariantoperationsinthefourierbasisviagaunt
ergcn: Allyouneedtotraindeepergcns. arXivpreprint tensor products. In The Twelfth International Confer-
arXiv:2006.07739,2020. enceonLearningRepresentations,2024. URLhttps:
//openreview.net/forum?id=mhyQXJ6JsK.
Liao,Y.-L.andSmidt,T. Equiformer: Equivariantgraph
attention transformer for 3d atomistic graphs. In The Maho, N. The pubchemqc project: A large chemical
EleventhInternationalConferenceonLearningRepresen- database from the first principle calculations. In AIP
tations, 2023. URL https://openreview.net/ conferenceproceedings,volume1702,pp.090058.AIP
forum?id=KwmPfARgOTD. PublishingLLC,2015.
11GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
Masters, D., Dean, J., Klaser, K., Li, Z., Maddrell- Schütt,K.,Unke,O.,andGastegger,M. Equivariantmes-
Mander, S., Sanders, A., Helal, H., Beker, D., Ram- sage passing for the prediction of tensorial properties
pavsek, L., and Beaini, D. Gps++: An optimised hy- andmolecularspectra. InInternationalConferenceon
bridmpnn/transformerformolecularpropertyprediction. MachineLearning,pp.9377–9388.PMLR,2021.
ArXiv,abs/2212.02229,2022.
Schütt, K. T., Sauceda, H. E., Kindermans, P.-J.,
Musaelian,A.,Batzner,S.,Johansson,A.,Sun,L.,Owen, Tkatchenko,A.,andMüller,K.-R. Schnet–adeeplearn-
C. J., Kornbluth, M., and Kozinsky, B. Learning local ingarchitectureformoleculesandmaterials. TheJournal
equivariantrepresentationsforlarge-scaleatomisticdy- ofChemicalPhysics,148(24):241722,2018.
namics. NatureCommunications,14(1):579,2023.
Scott,W.R. Grouptheory. CourierCorporation,2012.
Nakata,M.andShimazaki,T. Pubchemqcproject: alarge-
scalefirst-principleselectronicstructuredatabasefordata- Shi, Y., Zheng, S., Ke, G., Shen, Y., You, J., He, J.,
drivenchemistry. Journalofchemicalinformationand Luo,S.,Liu,C.,He,D.,andLiu,T.-Y. Benchmarking
modeling,57(6):1300–1308,2017. graphormeronlarge-scalemolecularmodelingdatasets.
arXivpreprintarXiv:2203.04810,2022.
Park, W., Chang, W.-G., Lee, D., Kim, J., et al. Grpe:
Relative positional encoding for graph transformer. In Shuaibi,M.,Kolluru,A.,Das,A.,Grover,A.,Sriram,A.,
ICLR2022MachineLearningforDrugDiscovery,2022. Ulissi, Z., and Zitnick, C. L. Rotation invariant graph
neuralnetworksusingspinconvolutions. arXivpreprint
Passaro, S. and Zitnick, C. L. Reducing SO(3) convo- arXiv:2106.09575,2021.
lutions to SO(2) for efficient equivariant GNNs. In
Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Thölke,P.andDeFabritiis,G. Torchmd-net: equivariant
Sabato, S., and Scarlett, J. (eds.), Proceedings of transformersforneuralnetworkbasedmolecularpoten-
the 40th International Conference on Machine Learn- tials. arXivpreprintarXiv:2202.02541,2022.
ing, volume 202 of Proceedings of Machine Learn-
ing Research, pp. 27420–27438. PMLR, 23–29 Jul Thomas, N., Smidt, T., Kearnes, S., Yang, L., Li, L.,
2023.URLhttps://proceedings.mlr.press/ Kohlhoff, K., and Riley, P. Tensor field networks:
v202/passaro23a.html. Rotation-andtranslation-equivariantneuralnetworksfor
3dpointclouds. arXivpreprintarXiv:1802.08219,2018.
Pfaff, T., Fortunato, M., Sanchez-Gonzalez, A., and
Battaglia, P. W. Learning mesh-based simulation with Unke,O.T.andMeuwly,M. Physnet: Aneuralnetworkfor
graphnetworks. arXivpreprintarXiv:2010.03409,2020. predictingenergies,forces,dipolemoments,andpartial
charges. Journalofchemicaltheoryandcomputation,15
Pozdnyakov, S. N., Willatt, M. J., Bartók, A. P., Ort- (6):3678–3693,2019.
ner, C., Csányi, G., and Ceriotti, M. Incompleteness
of atomic structure representations. Phys. Rev. Lett., Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
125:166001,Oct2020. doi: 10.1103/PhysRevLett.125. L.,Gomez,A.N.,Kaiser,Ł.,andPolosukhin,I. Atten-
166001. URL https://link.aps.org/doi/10. tionisallyouneed. InAdvancesinNeuralInformation
1103/PhysRevLett.125.166001. ProcessingSystems,pp.6000–6010,2017.
Rampášek,L.,Galkin,M.,Dwivedi,V.P.,Luu,A.T.,Wolf, Wang, L., Liu, Y., Lin, Y., Liu, H., and Ji, S. Comenet:
G.,andBeaini,D. Recipeforageneral,powerful,scal- Towardscompleteandefficientmessagepassingfor3d
ablegraphtransformer. AdvancesinNeuralInformation moleculargraphs. AdvancesinNeuralInformationPro-
ProcessingSystems,35:14501–14515,2022. cessingSystems,35:650–664,2022.
Satorras, V. G., Hoogeboom, E., and Welling, M. E (n) Wang, Y. and Chodera, J. Spatial attention kinetic net-
equivariantgraphneuralnetworks. InInternationalCon- works with e(n)-equivariance. In The Eleventh In-
ference on Machine Learning, pp. 9323–9332. PMLR, ternational Conference on Learning Representations,
2021. 2023. URLhttps://openreview.net/forum?
id=3DIpIf3wQMC.
Scholkopf,B.,Sung,K.-K.,Burges,C.J.,Girosi,F.,Niyogi,
P.,Poggio,T.,andVapnik,V. Comparingsupportvector Xie, T. and Grossman, J. C. Crystal graph convolutional
machineswithgaussiankernelstoradialbasisfunction neuralnetworksforanaccurateandinterpretablepredic-
classifiers. IEEEtransactionsonSignalProcessing,45 tionofmaterialproperties. Physicalreviewletters,120
(11):2758–2765,1997. (14):145301,2018.
12GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing,
C.,Zhang,H.,Lan,Y.,Wang,L.,andLiu,T. Onlayer
normalizationinthetransformerarchitecture. InInter-
nationalConferenceonMachineLearning,pp.10524–
10533.PMLR,2020.
Xu,K.,Hu,W.,Leskovec,J.,andJegelka,S. Howpowerful
aregraphneuralnetworks? InInternationalConference
on Learning Representations, 2019. URL https://
openreview.net/forum?id=ryGs6iA5Km.
Xu,Z.,Luo,Y.,Zhang,X.,Xu,X.,Xie,Y.,Liu,M.,Dicker-
son,K.,Deng,C.,Nakata,M.,andJi,S. Molecule3d: A
benchmarkforpredicting3dgeometriesfrommolecular
graphs. arXivpreprintarXiv:2110.01717,2021.
Ying,C.,Cai,T.,Luo,S.,Zheng,S.,Ke,G.,He,D.,Shen,Y.,
andLiu,T.-Y. Dotransformersreallyperformbadlyfor
graphrepresentation? AdvancesinNeuralInformation
ProcessingSystems,34:28877–28888,2021a.
Ying, C., Yang, M., Zheng, S., Ke, G., Luo, S., Cai, T.,
Wu, C., Wang, Y., Shen, Y., and He, D. First place
solution of kdd cup 2021 & ogb large-scale challenge
graphpredictiontrack. arXivpreprintarXiv:2106.08279,
2021b.
Zhang, B., Luo, S., Wang, L., and He, D. Rethinking
theexpressivepowerofGNNsviagraphbiconnectivity.
InTheEleventhInternationalConferenceonLearning
Representations,2023. URLhttps://openreview.
net/forum?id=r9hNv76KoT3.
Zitnick, C.L., Das, A., Kolluru, A., Lan, J., Shuaibi, M.,
Sriram, A., Ulissi, Z. W., and Wood, B. M. Spheri-
cal channels for modeling atomic interactions. In Oh,
A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
Advances in Neural Information Processing Systems,
2022. URLhttps://openreview.net/forum?
id=5Z3GURcqwT.
13GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
A.ImplementationDetailsofGeoMFormer
LayerNormalizations. BeingaTransformer-basedmodel,GeoMFormeralsoadoptsthelayernormalization(LN)(Ba
etal.,2016)modulefortrainingstability. Intheinvariantstream,theLNmoduleremainsunchangedfromthestandard
design(Baetal.,2016;Xiongetal.,2020). Inparticular,wespecializedtheLNmoduleasEqu-LNintheequivariant
stream to satisfy the geometric constraints. Formally, given the equivariant representation zE ∈ R3×d of the atom i,
i
Equ-LN(zE)=U(zE−µ1⊤)⊙γ,whereµ= 1(cid:80)d ZE ∈R3,γ ∈Rdisalearnablevector,andU∈R3×3denotes
i i d k=1 [i,:,k]
theinversesquarerootofthecovariancematrix,i.e.,U−2 = (zE i −µ1⊤)(zE i −µ1⊤)⊤ .
d
StructuralEncodings. Wefollow(Shietal.,2022)toincorporatethe3Dstructuralencoding,whichservesasthebias
terminthesoftmaxattentionmodule. Inparticular,weconsidertheEuclideandistance||r −r ||betweenatomiand
i j
j. TheGaussianBasisKernelfunction(Scholkopfetal.,1997)isusedtoencodetheinteratomicdistance,i.e.,bk =
(i,j)
−√ 1
exp(−1(γ(i,j)||ri−rj||+β(i,j)−µk
)2),k = 1,...,K, whereK isthenumberofGaussianBasiskernels. The3D
2π|σk| 2 |σk|
structuralencodingisobtainedbyB =GELU(b W1)W2,whereb =[b1 ;...;bK ]⊤,W1 ∈RK×K,W2 ∈
ij (i,j) D D (i,j) (i,j) (i,j) D D
RK×1arelearnableparameters.γ ,β arelearnablescalarsindexedbythepairofatomtypes,andµk,σkarelearnable
(i,j) (i,j)
kernelcenterandlearnablescalingfactorofthek-thGaussianBasisKernel. DenoteBasthematrixformofthe3Ddistance
encoding,whoseshapeisn×n. Thentheattentionprobabilityiscalculatedbysoftmax(Q √K⊤ +B),whereQandK are
d
thequeryandkeyintroducedinSection3.
B.ProofofGeometricConstraints
In this section, we provide thorough proof of the aforementioned conditions in Section 4 that satisfy the geometric
constraints. Forthesakeofconvenience,werestatethenotationsandgeometricconstraintshere. Formally,letV denote
M
thespaceofmolecularsystems,foreachatomi,wedefineequivariantrepresentationϕE andinvariantrepresentationϕI if
∀g =(t,R)∈SE(3),M=(X,R)∈V ,thefollowingconditionsaresatisfied:
M
ϕE :V →R3×d, RϕE(X,{r ,...,r })=ϕE(X,{Rr ,...,Rr }) (8a)
M 1 n 1 n
ϕE :V →R3×d, ϕE(X,{r ,...,r })=ϕE(X,{r +t,...,r +t}) (8b)
M 1 n 1 n
ϕI :V →Rd, ϕI(X,{r ,...,r })=ϕI(X,{Rr +t,...,Rr +t}) (8c)
M 1 n 1 n
where t ∈ R3,R ∈ R3×3,det(R) = 1 and X ∈ Rn×d denotes the atoms with features, R = {r ,...,r },r ∈ R3
1 n i
denotesthecartesiancoordinateofatomi. WepresenttheproofoftheGeneralDesignPhilosophy(SectionB.1)andour
GeoMFormermodel(SectionB.2)respectively.
B.1.ProofoftheGeneralDesignPhilosophy.
GiveninvariantandequivariantrepresentationsZI,0 ∈Rn×d,ZE,0 ∈Rn×3×dattheinput,weprovethattheupdaterules
showninEqn.(3)satisfytheaboveconstraintsinproperconditions. Inparticular,wefirstseparatelystudyeachcomponent
oftheblock,i.e.,Inv-Self-Attn, Equ-Self-Attn, Inv-Cross-Attn, Equ-Cross-Attn,andthencheckthepropertiesofthe
wholeframework.
InvariantSelf-Attention. GiveninvariantrepresentationZI,l ∈ Rn×d,QI,l = ψI,l(ZI,l),KI,l = ψI,l(ZI,l),VI,l =
Q K
ψI,l(ZI,l), as stated in Section 4.1, where ψI,l : Rn×d → Rn×d is invariant. In this regard, ∀g = (t,R) ∈ SE(3),
V
QI,l,KI,l,VI,l remainunchanged,whichmeansthatInv-Self-Attn(QI,l,KI,l,VI,l)alsoremainsunchanged. Thenthe
invarianceoftheoutputrepresentationsispreserved.
Equivariant Self-Attention. Given equivariant representation ZE,l ∈ Rn×3×d, QE,l = ψE,l(ZE,l),KE,l =
Q
ψE,l(ZE,l),VE,l = ψE,l(ZE,l), as stated in Section 4.1, where ψE,l : Rn×3×d → Rn×3×d is equivariant. Be-
K V
sides, the attention score is modified as α = (cid:80)d QE KE ⊤ , where QE ∈ R3 denotes the k-
ij k=1 [i,:,k] [j,:,k] [i,:,k]
th dimension of the atom i’s Query. First, we check the rotation equivariance of the Equ-Self-Attn. Given
any orthogonal transformation matrix R ∈ R3×3,det(R) = 1, we have (cid:80)d QE R(KE R)⊤ =
k=1 [i,:,k] [j,:,k]
(cid:80)d QE RR⊤KE ⊤ =(cid:80)d QE KE ⊤ =α ,whichpreservestheinvariance.AsψE,lisequivariant,
k=1 [i,:,k] [j,:,k] k=1 [i,:,k] [j,:,k] ij
14GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
wehaveψE,l([RZE,l;,...,;RZE,l])=[RψE,l(ZE,l) ;,...,;RψE,l(ZE,l) ]. Sincetheoutputequivariantrepresentation
1 n 1 n
of atom i preserves the equivariance, i.e., (cid:80)n exp(αij) RVE,l = R((cid:80)n exp(αij) VE,l), the rotation
j=1 (cid:80)n j′=1exp(α ij′) j j=1 (cid:80)n j′=1exp(α ij′) j
equivarianceissatisfied. Moreover,sincetheequivariantrepresentationZE,l preservesthetranslationinvariance(Eqn.(8b)),
theoutputequivariantrepresentationofEqu-Self-Attnnaturallysatisfiesthisconstraint.
Cross-Attention modules. As stated in Section 4.1, the Query, Key, and Value of Inv-Cross-Attn are specified
as QI_E,l = ψI,l(ZI,l),KI_E,l = ψI_E,l(ZI,l,ZE,l),VI_E,l = ψI_E,l(ZI,l,ZE,l), where ψI,l,ψI_E,l are invari-
Q K V
ant. That is to say, ∀g = (t,R) ∈ SE(3), QI_E,l,KI_E,l,VI_E,l remain unchanged. Then the invariance
of its output representations is preserved as in Inv-Self-Attn. On the other hand, the Query, Key, and Value of
Equ-Cross-AttnarespecifiedasQE_I,l =ψE,l(ZE,l),KE_I,l =ψE_I,l(ZE,l,ZI,l),VE_I,l =ψE_I,l(ZE,l,ZI,l),where
Q K V
ψE,l,ψE_I,l areequivariant,i.e.,ψE_I,l([RZE,l;,...,;RZE,l],ZI,l)=[RψE_I,l(ZE,l,ZI,l) ;,...,;RψE,l(ZE,l,ZI,l) ]
1 n 1 n
andψE,l([RZE,l;,...,;RZE,l])=[RψE,l(ZE,l) ;,...,;RψE,l(ZE,l) ]. AsstatedinEqu-Self-Attn,theoutputequivari-
1 n 1 n
antrepresentationsofEqu-Cross-Attnpreservetherotationequivariance. Similarly,thetranslationinvariancepropertyis
alsonaturallysatisfied.
Feed-Forward Networks. As Inv-FFN and Equ-FFN satisfy the invariance and equivariance constraints re-
spectively, we can directly obtain that ∀g = (t,R) ∈ SE(3), the output of Inv-FFN remains un-
changed, and the output of Equ-FFN preserves the rotation equivariance, i.e., Equ-FFN([RZE,l;,...,;RZE,l]) =
1 n
[REqu-FFN(ZE,l) ;,...,;REqu-FFN(ZE,l) ]. ThetranslationinvarianceisalsonaturallypreservedbyEqu-FFN.
1 n
Withtheaboveanalysis,theupdaterulesstatedinEqn.(3)satisfythegeometricconstraints(Eqn.(8a),Eqn.(8b)andEqn.(8c)).
Asourmodeliscomposedofstackedblocks,theinvariantandequivariantoutputrepresentationsofthewholemodelalso
preservetheconstraints.
B.2.ProofoftheGeoMFormer
Next, we provide proof of the instantiation of our GeoMFormer in Section 4.2 that satisfies the geometric constraints.
Similarly,weseparatelycheckthepropertiesofeachcomponentasourGeoMFormeriscomposedofstackedGeoMFormer
blocks. Oncetheconstraintsaresatisfiedbyeachcomponent,theoutputinvariantandequivariantrepresentationsofthe
wholemodelnaturallysatisfythegeometricconstraints(Eqn.(8a),Eqn.(8b)andEqn.(8c)).
Input layer. As stated in Section 4.2, the invariant representation at the input is set as ZI,0 = X, where X ∈ Rd
i
isa learnableembeddingvectorindexedbythe atomi’stype. Since ZI,0 doesnot containany informationfromR =
{r ,...,r },itnaturallysatisifiestheinvarianceconstraint(Eqn.(8c)). Theequivariantrepresentationattheinputisset
1 n
as ZE,0 = ˆr′g(||r′||)⊤ ∈ R3×d, where r′ denotes the mean-centered position of atom i, i.e., r′ = r − 1 (cid:80)n r ,
i i i i i i n k=1 k
ˆr′ = r′ i ,andg :R→RdisinstantiatedbytheGaussianBasisKernelfunction. First,thetranslationinvarianceconstraint
i ||r′||
i
(Eqn.(8b)) is satisfied. Given any translation vector t ∈ R3, r +t− 1 (cid:80)n (r +t) = r − 1 (cid:80)n r , and ZE,0
i n k=1 k i n k=1 k i
remainsunchanged. Second,therotationequivariance(Eqn.(8a))isalsopreserved. Givenanyorthogonaltransformation
matrix R ∈ R3×3,det(R) = 1, we have ||Rr′|| = ||r′||. With Rr as the input, we have Rr − 1 (cid:80)n Rr =
R(r − 1 (cid:80)n r )=Rr′ andg(||Rr′||)=g(|i |r′||),whi ichmeansthai ttherotationequivariancecoi nstran intik s= s1 atisfik ed.
i n k=1 k i i i
Self-Attentionmodules. ForInv-Self-AttnandEqu-Self-Attn,weusethelinearfunctiontoimplementbothψI andψE,
i.e.,QI =ψI(ZI)=ZIWI,KI =ψI (ZI)=ZIWI,VI =ψI (ZI)=ZIWI andQE =ψE(ZE)=ZEWE,KE =
Q Q K K V V Q Q
ψE(ZE) = ZEWE,VE = ψE(ZE) = ZEWE. ItisstraightforwardthattheconditionsmentionedinSectionB.1are
K K V V
satisfied. ThelinearfunctionkeepstheinvarianceofZI (Eqn.(8c))andtherotationequivarianceofZE (Eqn.(8a)),e.g.,
∀R∈R3×3,det(R)=1,(RZE)WE =R(ZEWE)=RZE. NotethatthetranslationinvarianceofZE (Eqn.(8b))isnot
i Q i Q i
changedbythelinearfunction.
Cross-Attention modules. For Inv-Cross-Attn, we use the linear function to implement ψI, which satisfies
Q
the constraints as previously stated. Besides, we instantiate KI_E and VI_E as KI_E = ψI_E(ZI,ZE) =<
K
ZEWI_E,ZEWI_E >,VI_E = ψI_E(ZI,ZE) =< ZEWI_E,ZEWI_E >. Here we prove that such instantia-
K,1 K,2 V V,1 V,2
tion preserve the invariance. First, given any orthogonal transformation matrix R ∈ R3×3,det(R) = 1, we have
< ([RZE;...;RZE])WI_E,([RZE;...;RZE])WI_E >=< ZEWI_E,ZEWI_E >. The reason is that given X,Y ∈
1 n K,1 1 n K,2 K,1 K,2
15GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
Rn×3×d,Z =< X,Y >∈ Rn×d,whereZ = X ⊤Y = X ⊤R⊤RY = (RX )⊤(RY ). The
[i,k] [i,:,k] [i,:,k] [i,:,k] [i,:,k] [i,:,k] [i,:,k]
translationinvarianceofZE isalsopreserved.
ForEqu-Cross-Attn,wealsousethelinearfunctiontoimplementψE,whichsatisfiestheconstraintsaspreviouslystated.
Q
Besides,weinstantiateKE_I andVE_I asKE_I = ψE_I(ZE,ZI) = ZEWE_I ⊙ZIWE_I,VE_I = ψE_I(ZE,ZI) =
K K,1 K,2 V
ZEWE_I ⊙ZIWE_I. First,givenanyorthogonaltransformationmatrixR∈R3×3,wehave([RZE;...;RZE])WE_I ⊙
V,1 V,2 1 n K,1
ZIWE_I =[R(ZEWE_I ⊙ZIWE_I) ;...;R(ZEWE_I ⊙ZIWE_I) ],whichpreservestherotationequivariance. The
K,2 K,1 K,2 1 K,1 K,2 n
reason lies in that given X ∈ Rn×3×d,Y ∈ Rn×d, Z = RX ⊙Y ∈ R3×d, where Z = (RX )·Y =
i i i [i,:,k] [i,:,k] [i,k]
R(X ·Y ). Additionally,thetranslationinvarianceofbothKE_I andVE_I ispreservedbecauseofthetranslation
[i,:,k] [i,k]
invarianceofZE andZI. Inthisway,theinstantiationsofcross-attentionmodulessatisfythegeometricconstraints.
Feed-ForwardNetworks. ForInv-FFN(Z′′I)=GELU(Z′′IWI)WI,theinvarianceconstraint(Eqn.8c)isnaturally
1 2
preserved. ForEqu-FFN(Z′′E)=(Z′′EWE⊙GELU(Z′′IWI))WE,therotationequivarianceconstraintisalsosimilarly
1 2 3
preservedasinEqu-Cross-Attn. Besides,thetranslationinvarianceofEqu-FFN(Z′′E)isalsopreservedwiththeproperty
ofZ′′E andZ′′I.
Layer Normalizations. As introduced in Section A, we use the layer normalization modules for both invariant and
equivariant streams. For the invariant stream, the layer normalization remains unchanged, and the invariance con-
straint is naturally preserved. For the equivariant stream, given the equivariant representation zE ∈ R3×d of the
i
atom i, Equ-LN(zE) = U(zE − µ1⊤) ⊙ γ, where µ = 1(cid:80)d ZE ∈ R3, γ ∈ Rd is a learnable vector, and
i i d k=1 [i,:,k]
U ∈ R3×3 denotestheinversesquarerootofthecovariancematrix,i.e.,U−2 = (zE i −µ1⊤)(zE i −µ1⊤)⊤ . First,givenany
d
orthogonaltransformationmatrixR ∈ R3×3,det(R) = 1, (RzE i −Rµ1⊤)(RzE i −Rµ1⊤)⊤ = (RzE i −Rµ1⊤)(RzE i −Rµ1⊤)⊤ =
d d
R(zE i −µ1⊤)(zE i −µ1⊤)⊤ R⊤ = RU−2R⊤ = RU−1R⊤RU−1R⊤ = (RUR⊤)−2, then we have Equ-LN(RzE) =
d i
RUR⊤(RzE−Rµ1⊤)⊙γ =R(U(zE−µ1⊤))=REqu-LN(zE),whichpreservestherotationequivariance(Eqn.(8a)).
i i i
ThetranslationinvarianceofZE isalsopreserved.
StructuralEncodings. AsintroducedinSectionA,thestructuralencodingsserveasthebiasterminthesoftmaxattention
module. Sinceonlytherelativedistance||r −r ||,∀i,j ∈ [n]isused,theinvarianceconstraintispreserved,i.e.,given
i j
∀g =(t,R)∈SE(3),||Rr +t−Rr +t||=||r −r ||.
i j i j
C.Discussions
C.1.ConnectionstoPreviousApproaches
In this section, we present a detailed discussion of how previous models (PaiNN (Schütt et al., 2021) and TorchMD-
Net(Thölke&DeFabritiis,2022))canbeviewedasspecialinstantiationsbyextendingthedesignphilosophydescribed
inSection4.1. Withoutlossofgenerality,weomitthecutoffconditionsusedintheseworksforreadability,whichcanbe
naturallyincludedinourframework.
PaiNN(Schüttetal.,2021). BothinvariantrepresentationsZI =[zI⊤ ;...;zI⊤ ]∈Rn×dandequivariantrepresentations
1 n
ZE =[zE;...;zE]∈Rn×3×daremaintainedinPaiNN,wherezI ∈RdandzE ∈R3×daretheinvariantandequivariant
1 n i i
representationsforatomi,respectively. Ineachlayer,therepresentationsareupdatedasfollows:
Z′I,l =ZI,l +Message-Block-Inv(ZI,l)
Z′E,l =ZE,l +Message-Block-Equ(ZI,l,ZE,l)
(9)
ZI,l+1 =Z′I,l +Update-Block-Inv(Z′I,l,Z′E,l)
ZE,l+1 =Z′E,l +Update-Block-Equ(Z′I,l,Z′E,l)
16GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
Inthemessageblock,theinvariantandequivariantrepresentationsareupdatedinthefollowingmanner. Forbrevity,we
omitthelayerindexl.
Message-Block-Inv(zI)=(cid:88) ϕ (zI)◦W (||r −r ||)
i s j s i j
j
Message-Block-Equ(zI i,zE i )=(cid:88) zE j ⊙(cid:16) ϕ vv(zI j)◦W vv(||r i−r j||)(cid:17) (10)
j
+ r i−r j (cid:16) ϕ (zI)◦W′ (||r −r ||)(cid:17)⊤
||r −r || vs j vs i j
i j
Thescalarproduct⊙isdefinedthesamewayasinSection4.2,i.e.,givenx∈R3×d,y ∈Rd,z =x⊙y ∈R3×d,where
z = x ·y . ◦ denotes the element-wise product, ϕ ,ϕ ,ϕ : Rd → Rd are all 2-layer MLP with the SiLU
[i,j] [i,k] [k] s vv vs
activation,W ,W ,W′ :R→Rdareinstantiatedbylearnableradialbasisfunctions. ri−rj ∈R3denotestherelative
s vv vs ||ri−rj||
directionbetweenatomi’sandj’spositions.
Intheupdateblock,theinvariantandequivariantrepresentationsareupdatedinthefollowingmanner:
Update-Block-Inv(zI,zE)=a (zI,||zEV||)+a (zI,||zEV||)◦<zEU,zEV>
i i ss i i sv i i i i
Update-Block-Equ(zI,zE)=a (zI,||zEV||)⊙(zEU)
i i vv i i i
V,U∈Rd×darelearnableparameters. <·,·>isdefinedthesamewayasinSection4.2,i.e.,givenx,y ∈R3×d,z =<
x,y >∈ Rd, where z = x ⊤y . Norm || · || : R3×d → Rd is calculated along the spatial dimension, i.e.,
[k] [:,k] [:,k]
||·||=<·,·>. ◦denotestheelement-wiseproduct. ⊙isalsodefinedthesameasinSection4.2. a(·,·):Rd×Rd →Rd
firstconcatenatesthetwoinputsalongthefeaturedimensionandthenapplya2-layerMLPwithSiLUactivation.
Weprovethatboththeinvariantandequivariantmessageblockscanbeviewedasspecialinstancesbyextendingtheinvariant
self-attentionmoduleandtheequivariantcross-attentionmoduleofourframeworkrespectively. Inparticular,weextend
ψI ,ψE_I introducedintheSection4.1tobequery-dependent,i.e.,ψI,i,ψE_I,ithatdependsontheatomi’srepresentations.
V V V V
Concretely,intheinvariantself-attentionmodule,wesetψI,i(zI)=ϕ (zI)⊙W (||r −r ||). Similarly,intheequivariant
V j s j s i j
cross-attentionmodule,wesetψE_I,i(zI,zE)=zE ⊙ϕ (zI)·W (||r −r ||)+ϕ (zI)·W′ ri−rj . Insuchway,
V j j j vv j vv i j vs j vs||ri−rj||
theinvariantself-attentionmoduleandtheequivariantcross-attentionmodulecanexpresstheinvariantandequivariant
messageblocksrespectively,e.g.,theparameterstotransformQueryandKeyaretrained/initializedtozero,andthenumber
ofatomscanbeequippedbyinitialization,whichisnecessarytoexpressthesumoperatorbyusingtheattentionasshown
in(Yingetal.,2021a).
Moreover, we prove that the update blocks can also be viewed as special instances by extending the FFN blocks in
our framework. In particular, we set Inv-FFN(zI) = a (zI,||zEV||) + a (zI,||zEV||) < zEU,zEV > and
Equ-FFN(zE) = a (zI,||zEV||)(cid:0) zEU(cid:1) , then bi oth Inv-s Fs FNi andi Equ-FFNsv cani expi ress the updi ate bli ocks. Note
i vv i i i
thattheparametersoftheremainingblocks(Inv-Cross-Attn,Equ-Self-Attn)canbetrained/initializedtobezero. Insuch
way,thePaiNNmodelcanbeinstantiatedthroughourdesignphilosophyintroducedinSection4.1.
TorchMD-Net(Thölke&DeFabritiis,2022). SimilarlytoPaiNN,bothinvariantrepresentationsZI =[zI⊤ ;...;zI⊤ ]∈
1 n
Rn×dandequivariantrepresentationsZE =[zE;...;zE]∈Rn×3×daremaintainedinTorchMD-Net,wherezI ∈Rdand
1 n i
zE ∈R3×daretheinvariantandequivariantrepresentationsforatomi,respectively. Ineachlayer,therepresentationsare
i
updatedasfollows:
Z′I,l =ZI,l +TorchMD-Inv-Block-1(ZI,l)
ZI,l+1 =Z′I,l +TorchMD-Inv-Block-2(Z′I,l,ZE,l) (11)
ZE,l+1 =ZE,l +TorchMD-Equ-Block(ZI,l,ZE,l)
17GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
TheinvariantrepresentationsinTorchMD-Inv-Block-1andTorchMD-Inv-Block-2areupdatedasfollows. Forbrevity,
weomitthelayerindexl.
Q =WQzI,K =WKzI,V(1) =WV(1)zI
i i j j j j
α
=SiLU(cid:16) Q⊤(cid:0)
K
◦DK(cid:1)(cid:17)
ij i j ij
(cid:32) (cid:33)
TorchMD-Inv-Block-1(zI)=O (cid:88) α ·V(1)◦DV(1) (12)
i 1 ij j ij
j
(cid:32) (cid:33)
TorchMD-Inv-Block-2(zI,zE)=O (cid:88) α ·V(1)◦DV(1) ◦<zEU ,zEU >
i i 2 ij j ij i 1 i 2
j
WQ,WK,WV(1),U ,U ∈Rd×darelearnableparameters. ◦denotestheelement-wiseproduct. DK,DV(1) :R→Rd
1 2 ij ij
takes||r −r ||asinputandusesradialbasisfunctionsfollowedbyanon-linearactivationtotransformit.O ,O :Rd →Rd
i j 1 2
arelearnablelineartransformations. < ·,· >isdefinedthesamewayasinSection4.2, i.e., givenx,y ∈ R3×d,z =<
x,y >∈Rd,wherez =x ⊤y . Ontheotherhand,theequivariantrepresentationsareupdatedasfollows:
[k] [:,k] [:,k]
V(2) =WV(2)zI,V(3) =WV(3)zI
j j j j
(cid:18) (cid:19)
TorchMD-Equ-Block(zI,zE)=(cid:88) (V(2)◦DV(2))⊙zE+ r i−r j (V(3)◦DV(3))⊤
i i j ij j ||r −r || j ij (13)
i j
j
(cid:32) (cid:33)
+O (cid:88) α ·V(1)◦DV(1) ⊙zEU
3 ij j ij i 3
j
WV(2),WV(3),U ∈Rd×darelearnableparameters. ◦denotestheelement-wiseproduct. ⊙isdefinedthesamewayasin
3
Section4.2,i.e.,givenx ∈ R3×d,y ∈ Rd,z = x⊙y ∈ R3×d,wherez = x ·y . DV(2),DV(3) : R → Rd takes
[i,j] [i,k] [k] ij ij
||r −r ||asinputanduseradialbasisfunctionsfollowedbyanon-linearactivationtotransformit. O :Rd →Rd isa
i j 3
learnablelineartransformation. ri−rj ∈R3denotestherelativedirectionbetweenatomi’sandj’spositions.
||ri−rj||
WeprovethattheTorchMD-Inv-Block-1andTorchMD-Inv-Block-2canbeviewedasspecialinstancesbyextending
theinvariantself-attentionmoduleandinvariantcross-attentionmoduleofourframeworkrespectively. Concretely,inthe
(cid:16) (cid:17)
invariantself-attentionmodule,wesetψI(zI)=WQzI,ψI,i(zI)=WKzI ◦DK,ψI,i(zI)=O WV(1)zI ◦DV(1)
Q i i K j j ij V j 1 j ij
anduseSiLUinsteadofSoftmaxforcalculatingattentionprobability.ByrewritingTorchMD-Inv-Block-1intheequivalent
(cid:16) (cid:17)
formTorchMD-Inv-Block-1(zI) = (cid:80) α ·O V(1)◦DV(1) , theinvariantself-attentionmodulecanexpressitby
i j ij 1 j ij
equippingthenumberofatomsforexpressingthesumoperationusingtheattention.
In the invariant cross-attention module, we set ψI(zI) = WQzI,ψI_E,i(zI,zE) = WKzI ◦DK,ψI_E,i(zI,zE) =
Q i i K j j j ij V j j
(cid:16) (cid:17)
O WV(1)zI ◦DV(1) ◦ < U zE,U zE >, and use SiLU instead of Softmax for calculating attention probability.
2 j ij 1 i 2 i
(cid:16) (cid:17)
ByusingtheequivalentformTorchMD-Inv-Block-2(zI,zE) = (cid:80) α ·O V(1)◦DV(1) ◦ < U zE,U zE >, the
i i j ij 2 j ij 1 i 2 i
invariantcross-attentionmodulecanexpressitbyequippingthenumberofatoms.
Moreover, we prove that the TorchMD-Equ-Block can be viewed as a special instance by extending the equivari-
ant cross-attention module of our framework. In particular, we set ψE_I,i(zI,zE) = (WV(2)zI ◦ DV(2)) ⊙ zE +
V j j j ij j
(cid:16) (cid:17)
ri−rj (WV(3)zI ◦ DV(3))⊤ + α · O WV(1)zI ◦DV(1) ⊙ U zE. By rewriting TorchMD-Equ-Block in the
||ri−rj|| j ij ij 3 j ij 3 i
(cid:16) (cid:17)
equivalent form TorchMD-Equ-Block(zI,zE) = (cid:80) (V(2)◦DV(2))⊙zE + ri−rj (V(3)◦DV(3))⊤ +(cid:80) α ·
i i j j ij j ||ri−rj|| j ij j ij
(cid:32) (cid:33)
(cid:16) (cid:17) (cid:16) (cid:17)
O V(1)◦DV(1) ⊙U zE =(cid:80) (V(2)◦DV(2))⊙zE+ ri−rj (V(3)◦DV(3))⊤+α ·O V(1)◦DV(1) ⊙U zE ,
3 j ij 3 i j j ij j ||ri−rj|| j ij ij 3 j ij 3 i
itisstraightforwardthattheequivariantcross-attentionmodulecanexpresstheTorchMD-Equ-Block,e.g.,theparameters
totransformQueryandKeyaretrained/initializedtozero,andthenumberofatomscanbeequippedbyinitialization. Note
thattheparametersoftheremainingblocks(Equ-Self-Attn,Inv-FFN,Equ-FFN)canbetrained/initializedtobezero. In
suchways,theTorchMD-NetmodelcanbeinstantiatedthroughourdesignphilosophyintroducedinSection4.1.
18GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
C.2.ExtensiontoOtherGeometricConstraints
Inthissubsection,weshowcasehowtoextendourframeworktoencodeothergeometricconstraints. Inparticular,we
consider the E(3) group, which comprises translation, rotation and reflection. Formally, let V denote the space of
M
molecular systems, for each atom i, we define equivariant representation ϕE and invariant representation ϕI if ∀ g =
(t,R)∈E(3),M=(X,R)∈V ,thefollowingconditionsaresatisfied:
M
ϕE :V →R3×d, RϕE(X,{r ,...,r })+t1⊤ =ϕE(X,{Rr +t,...,Rr +t}) (14a)
M 1 n 1 n
ϕI :V →Rd, ϕI(X,{r ,...,r })=ϕI(X,{Rr +t,...,Rr +t}) (14b)
M 1 n 1 n
wheret ∈ R3 isatranslationvector,R ∈ R3×3,det(R) = ±1isanorthogonaltransformationmatrixandX ∈ Rn×d
denotestheatomswithfeatures,R = {r ,...,r },r ∈ R3 denotesthecartesiancoordinateofatomi. Inparticular,the
1 n i
additionalrequirementistoencodethetranslationandreflectionequivarianceoftheequivariantrepresentations,whichcan
beachievedbymodifyingtheconditionsofourframework(Eqn.(3)).
With the invariant representation ZI and the equivariant representation ZE that satisfy the constraints (Eqn.(14a) and
Eqn.(14b)), we separately redefine the conditions of each component. It is worth noting that the reflection invariance
isdirectlysatisfied(RR⊤ = R⊤R = I)fromtheanalysisinSectionB.1andSectionB.2,whichisrequiredin(1)the
calculationofattentionprobabilityinEqu-Self-Attn,Equ-Cross-Attn;(2)thecalculationofKI_E andVI_E. Thus,we
onlyneedtoencodethetranslationequivarianceconstraint. Giventheupdaterules(Eqn.(3)),itcanbeachievedbysimply
setting each component (Inv-Self-Attn,Inv-Cross-Attn,Equ-Self-Attn,Equ-Cross-Attn,Inv-FFN,Equ-FFN) to be
translation-invariant. Inthisway,theoutputequivariantrepresentationcanpreservetheequivariancetotheE(3)group.
Weextendourframeworktoachievethisgoal,whichisintroducedbelow:
Self-Attentionmodules. ForInv-Self-Attn,theconditionremainsunchanged. ForEqu-Self-Attn,theadditionalcondition
is that ψE should keep the translation invariance. Here we give a simple instantiation: QE = ψE(ZE) = (ZE −
Q
µ )WE,KE =ψE(ZE)=(ZE−µ )WE,VE =ψE(ZE)=(ZE−µ )WE,whereµ = 1(cid:80)n ZE 1⊤.
ZE Q K ZE K V ZE V ZE,i d k=1 [i,:,k]
Cross-Attentionmodules. ForInv-Cross-Attn,theconditionforψI remainsunchanged,whileψI_E shouldkeepthe
translationinvariance. ForEqu-Cross-Attn,bothψE andψE_I arerequiredtobetranslation-invariant. Herewegivean
instantiation: QE =ψE(ZE)=(ZE −µ )WE,and
Q ZE Q
KI_E =<(ZE−µ )WI_E,(ZE−µ )WI_E >, VI_E =<(ZE−µ )WI_E,(ZE−µ )WI_E >
ZE K,1 ZE K,2 ZE V,1 ZE V,2 (15)
KE_I =(ZE−µ )WE_I ⊙ZIWE_I, VE_I =(ZE−µ )WE_I ⊙ZIWE_I
ZE K,1 K,2 ZE V,1 V,2
Feed-ForwardNetworks. Similarly,theconditionforInv-FFNremainsunchanged. ForEqu-FFN,italsoshouldkeep
thetranslationinvariance,e.g.,Equ-FFN(Z′′E)=((Z′′E −µ )WE ⊙GELU(Z′′IWI))WE.
ZE 1 2 3
Remark. Withtheaboveadditionalconditions,ourframeworkcanadditionallybeextendedtoencodegeometricconstraints
towardsE(3)group. Notethatthedesignoftheinputlayershouldalsoencodetheconstraints(Eqn.(14a)andEqn.(14b)).
Forexample,theinvariantrepresentationremainsunchangedasZI,0 = X. whiletheequivariantrepresentationcanbe
directlysetasZE,0 =r . Inthisway,thegeometricconstraintsarewellsatisfied.
i i
C.3.FurtherDiscussionontheDesignPhilosophyoftheCross-AttentionModule
In this subsection, we further elucidate the disparity between invariant and equivariant representations in terms of the
informationtheyencapsulate,substantiatedbyadditionalsupportingevidence. Specifically,theadvantagesofinvariant
andequivariantrepresentationscomplementeachother,promptingustobridgethemandcombinetheirstrengthsthrough
cross-attentionmodules.
Fromtheperspectiveofneuralnetworkdesign,equivariantrepresentationspreservemorecompletestructuralinformation
inastraightforwardway,buttheequivariantconstraintsrestrictthechoiceofoperationsthatcanbeused. Ontheother
hand,invariantrepresentationsoffermoreflexibilityfornon-linearoperations,butintroducepotentialrisksofstructural
informationlossorinefficientutilization.
First,weprovidemoreexplanationsandsupportingevidenceonwhyequivariantrepresentationspreservemorecomplete
structuralinformationinastraightforwardway. Intheliterature,existinginvariantmodelscommonlyuseinvariantfeatures
of molecules (e.g., interatomic distances, bond angles, dihedral/torsion angles, improper angles, and so on) which are
19GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
extractedfromtheraw3Dgeometricstructuresofmolecules(thepositionofeachatominthemolecule). Althoughthese
invariant features are important to describe the properties of molecules, the mapping from raw geometric structures to
invariantfeaturesmayintroduceinformationlossorinefficiency:
• Inarecentstudyontheexpressivepowerofgeometricgraphneuralnetworks(Joshietal.,2023),theauthorsprovided
athoroughtheoreticalanalysisoftheexpressivepowerofinvariantlayersandequivariantlayerscommonlyusedin
geometricgraphneuralnetworksbyleveragingtheGeometricWeisfeiler-Lehmantest.Eachinvariantlayeringeometric
graph neural networks corresponds to the proposed Invariant Geometric Weisfeiler-Lehman test (IGWL). In their
framework,previouslymentionedinvariantfeaturescanbecategorizedintodifferentclassesintermsoftheleastnumber
ofnodes(atomsinthecontextofmolecularmodeling)thatareinvolvedincomputingit. Forexample,interatomic
distancesinvolvetwoatoms,whilebondanglesinvolvethreeatoms. Thesefeaturesarethencalledk-bodyscalars.
Basedonthisframework,theauthorscarefullycharacterizedtheexpressivepowerofinvariantlayersandprovedthat
"anynumberofiterationsofIGWLcannotdistinguishany1-hopidenticalgeometricgraphsG andG andwheretheun-
1 2
derlyingattributedgraphsareisomorphic"and"GWL(correspondstoequivariantlayer)candistinguishbypropagating
informationfromthegeometricstructure",whichindeedindicatestheinformationlossfromrawgeometricstructures
toinvariantfeatures(e.g.,usinginteratomicdistancesorbondangles). Althoughaddingmorecomplexfeatureslike
dihedralanglescouldmitigatethisissue,italsobringsinefficiencyduetothehighcomplexityofsuchfeatures.
• In PaiNN (Schütt et al., 2021), the authors also provided intuition on the point that there exist limits of invariant
representations. Firstly,theauthorsdemonstratedthattoexpressenoughinformation(likebondanglesofallneighbors
ofanatom),usinginvariantrepresentationswouldrequirecomputationswithhighercomplexitythanusingequivariant
representations(seetheanalysisofTable1in(Schüttetal.,2021)). Moreover,theauthorsalsoshowedthatequivariant
representations allow the propagation of crucial geometrical information beyond the neighborhood, which is not
possibleintheinvariantcase(seetheanalysisofFigure1in(Schüttetal.,2021)). Thisevidencemotivatedtheauthors
todevelopmodelsbasedonequivariantrepresentations.
Second, we also demonstrate that invariant representations allow more flexibility for non-linear operations while the
equivariantconstraintsrestrictthechoiceofequivariantoperationsthatcanbeused. Forinvariantrepresentations,there
existnoconstraintsontheoperationsthatcanbeused. Theinvariantconstraintsarenaturallysatisfiedoncetheinvariant
featuresareextracted. Hence,wecanfreelychoosedifferentoperationdesignswitharbitrarynon-linearitytoincreasethe
modelcapacityandincorporatepriorsfortargetedtasks. However,itisnotthesameforequivariantrepresentations. To
satisfytheequivariantconstraints,operationclassesthatarequalifiedtocorrectlyprocessequivariantrepresentationsare
restricted,andnon-lineartransformationscommonlyusedinneuralnetworkswouldbreaktheconstraintsonequivariant
representations. Thatistosay,althoughequivariantrepresentationscontainmorecompleteinformation,itisrestrictedfor
modelstowellprocessandtransformthisinformation:
• Most existing works use (1) vector operations; and (2) tensor products of irreducible representations to develop
equivariant operations. For the former one, the non-linearity is constrained. For the latter one, the computational
complexityishighwhichimpedesthedeploymenttolarge3Dsystems.
• Empirically,previousequivariantmodelscannotconsistentlyachievesuperiorperformanceoninvarianttaskscompared
toinvariantmodels,duetotherestrictednon-linearityorcomputationalcomplexity. Forexample,previousinvariant
modelsarestilldominantininvariantpredictiontasksonPCQM4Mv2andMolecule3D(Table4and5). Moreover,the
modelcapacityofexistingequivariantmodelsisthuslimited,andtheabilitytoscalethemodelsizeupisalsorestricted.
InourpreliminaryexperimentsonpreviousmodelslikePaiNN/TorchMD-Net,itishardtotrainthesemodelswith
largermodelsizes,onwhichweobservedthetraininginstabilityissues.
Based on the above explanations and evidence, we can see that invariant and equivariant representations play indeed
differentroles. Comparedtoinvariantrepresentations,equivariantrepresentationscontainmorecompleteinformationonthe
geometricalstructures. Comparedtoequivariantrepresentations,theinformationcontainedbyinvariantrepresentations
canbebettertransformedandprocessedwithmoreflexibledesignchoicesofoperations. Fromtheseperspectives, the
advantagesofinvariantandequivariantrepresentationscomplementeachother,whichmotivatesustobridgethemand
combinetheirpowerviathecross-attentionmodules,bringingtheunifiedframeworkofourGeoMFormer.
20GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
D.MoreRelatedWorks
Insteadofstickingtothestandardtensorproductoperation,SCN(Zitnicketal.,2022)usedarotationtricktoencodethe
atomicenvironmentbyusingtheirreps. NotethatSCNdoesnotstrictlyobeytheequivarianceconstraint. eSCN(Passaro&
Zitnick,2023)presentedaninterestingobservationthatthesphericalharmonicsfiltershavespecificsparsitypatternsifa
properrotationactsontheinputsphericalcoordinates. CombinedwithpatternsoftheClebsch-Gordancoefficients,eSCN
proposedanefficientequivariantconvolutiontoreduceSO(3)tensorproductstoSO(2)linearoperations,achievingefficient
computation. BuiltupontheeSCNconvolution,EquiformerV2(Liaoetal.,2024)developedascalableTransformer-based
modelthatcanbeappliedtothedomainof3Datomisticsystems.
EGNN(Satorrasetal.,2021)proposedasimpleyeteffectiveequivariantgraphneuralnetworkframework. BasedonEGNN,
GMN(Huangetal.,2022)furtherdevelopsmulti-channelequivariantmodelingusingforwardkinematicsinformationin
physicalsystem. SEGNN(Brandstetteretal.,2022)generalizesthenodeandedgefeaturestoincludevectorsortensors,
andincorporatesgeometricandphysicalinformationbyusingthetensorproductoperations. SAKE(Wang&Chodera,
2023)proposedspatialattentionmechanismthatusesneurallyparametrizedlinearcombinationsofedgevectorstoachieve
equivariance,whichislesscomputationallyintensivethantraditionalsphericalharmonics-basedmethods. UsingGMNas
itsbackbone,SEGNO(Liuetal.,2024)integratedNeuralODEtoapproximatecontinuoustrajectoriesbetweenstatesand
incorporatedsecond-ordermotionequationstoupdatethepositionandvelocityinphysicalsimulation.
E.ExperimentalDetails
E.1.OC20IS2RE
Baselines. WecompareourGeoMFormerwithseveralcompetitivebaselinesforlearninggeometricmolecularrepresenta-
tions. CrystalGraphConvolutionalNeuralNetwork(CGCNN)(Xie&Grossman,2018)developednovelapproachesto
modelingperiodiccrystalsystemswithdiversefeaturesasnodeembeddings. SchNet(Schüttetal.,2018)leveragedthe
interatomicdistancesencodedviaradialbasisfunctions,whichserveastheweightsofcontinuous-filterconvolutionallayers.
DimeNet++(Gasteigeretal.,2020a)introducedthedirectionalmessagepassingthatencodesbothdistanceandangular
informationbetweentripletsofatoms.
GemNet(Gasteigeretal.,2021)embeddedallatompairswithinagivencutoffdistancebasedoninteratomicdirections,
andproposedthreeformsofinteractiontoupdatethedirectionalembeddings: Two-hopgeometricmessagepassing(Q-MP),
one-hopgeometricmessagepassing(T-MP),andatomself-interactions. AnefficientvariantnamedGemNet-Tisproposed
tousecheaperformsofinteraction.
SphereNet(Liuetal.,2022b)usedthesphericalcoordinatesystemtorepresenttherelativelocationofeachatominthe
3Dspaceandproposedthesphericalmessagepassing. GNS(Pfaffetal.,2020)isaframeworkforlearningmesh-based
simulations using graph neural networks and can handle complex physical systems. Graphormer-3D (Shi et al., 2022)
extendedGraphormer(Yingetal.,2021a)tolearngeometricmolecularrepresentations, whichencodestheinteratomic
distanceasattentionbiastermsandperformedwellonlarge-scaledatasets. Equiformer(Liao&Smidt,2023)usesthe
tensorproductoperationstobuildanewscalableequivariantTransformerarchitectureandoutperformsstrongbaselineson
thelarge-scaleOC20dataset(Chanussotetal.,2021).
Settings. AsintroducedinSection5.1.1,wefollowtheexperimentalsetupofGraphormer-3D(Shietal.,2022)fora
faircomparison. OurGeoMFormermodelconsistsof12layers. Thedimensionofhiddenlayersandfeed-forwardlayers
issetto768. Thenumberofattentionheadsissetto48. ThenumberofGaussianBasiskernelsissetto128. Weuse
AdamWastheoptimizerandsetthehyper-parameterϵto1e-6and(β ,β )to(0.9,0.98). Thegradientclipnormissetto
1 2
5.0. Thepeaklearningrateissetto2e-4. Thebatchsizeissetto128. Thedropoutratiosfortheinputembeddings,attention
matrices,andhiddenrepresentationsaresetto0.0,0.1,and0.0respectively. Theweightdecayissetto0.0. Themodelis
trainedfor1millionstepswitha60k-stepwarm-upstage. Afterthewarm-upstage,thelearningratedecayslinearlytozero.
FollowingLiao&Smidt(2023),wealsousethenoisynodedataaugmentationstrategy(Godwinetal.,2022)toimprovethe
performance. Themodelistrainedon16NVIDIATeslaV100GPUs.
E.2.OC20IS2RS
Baselines. Inthisexperiment,wechooseseveralcompetitivebaselinesthatperformwellonequivariantpredictiontasks
formolecules. PaiNN(Schüttetal.,2021)builtupontheframeworkofEGNN(Satorrasetal.,2021)tomaintainboth
21GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
invariantandequivariantrepresentationsandfurtherusedtheHardamardproductoperationtotransformtheequivariant
representations. Specializedtensorpredictionblockswerealsodevelopedfordifferentmolecularproperties. TorchMD-
Net(Thölke&DeFabritiis,2022)developedanequivariantTransformerarchitecturebyusingsimilarHardamardproduct
operationsandachievedstrongperformanceonvarioustasks.
SpinConv(Shuaibietal.,2021)encodedangularinformationwithalocalreferenceframedefinedbytwoatomsandused
aspinconvolutiononthesphericalrepresentationtocapturerichangularinformationwhilemaintainingrotationinvariance.
Anadditionalpredictionheadisusedtoperformtheequivariantpredictiontask,GemNet-dT(Gasteigeretal.,2021)is
avariantofGemNet-Tthatcandirectlyperformforcepredictionandotherequivarianttasks,e.g.,therelaxedpositions
inthisexperiment. GemNet-OC(Gasteigeretal.,2022)isanextensionofGemNetbyusingmoreefficientcomponents
andachievedbetterperformanceonOC20tasks.
Settings. As introduced in Section 5.1.2, we adopt the direct prediction setting for comparing the ability to perform
equivariantpredictiontasksonOC20IS2RS.Inparticular,were-implementedthebaselinesandcarefullytrainedthese
modelsforafaircomparison. OurGeoMFormermodelconsistsof12layers. Thedimensionofhiddenlayersandfeed-
forwardlayersissetto768. Thenumberofattentionheadsissetto48. ThenumberofGaussianBasiskernelsissetto128.
WeuseAdamWastheoptimizerandsetthehyper-parameterϵto1e-6and(β ,β )to(0.9,0.98). Thegradientclipnormis
1 2
setto5.0. Thepeaklearningrateissetto2e-4. Thebatchsizeissetto64. Thedropoutratiosfortheinputembeddings,
attentionmatrices,andhiddenrepresentationsaresetto0.0,0.1,and0.0respectively. Theweightdecayissetto0.0. The
model is trained for 1 million steps with a 60k-step warm-up stage. After the warm-up stage, the learning rate decays
linearlytozero. Themodelistrainedon16NVIDIATeslaV100GPUs.
E.3.PCQM4Mv2
Baselines. WecompareourGeoMFormerwithseveralcompetitivebaselinesfromtheleaderboardofOGBLarge-Scale
Challenge(Huetal.,2021). First,wecompareseveralmessage-passingneuralnetwork(MPNN)variants. Twowidelyused
models,GCN(Kipf&Welling,2017)andGIN(Xuetal.,2019)arecomparedalongwiththeirvariantswithvirtualnode
(VN)(Gilmeretal.,2017;Huetal.,2020). Besides,wecompareGINE-VN(Brossardetal.,2020)andDeeperGCN-VN(Li
et al., 2020). GINE is the multi-hop version of GIN. DeeperGCN is a 12-layer GNN model with carefully designed
aggregators. TheresultofMLP-Fingerprint(Huetal.,2021)isalsoreported. Thecomplexityofthesemodelsisgenerally
O(n),wherendenotesthenumberofatoms.
Additionally,wecomparewithafamilyofstrongarchitectures,GraphTransformer(Yingetal.,2021a;Luoetal.,2022;
2023;Zhangetal.,2023),whosecomputationalcomplexityisO(n2). TokenGT(Kimetal.,2022)purelyusednodeand
edgerepresentationsastheinputandadoptedthestandardTransformerarchitecturewithoutgraph-specificmodifications.
EGT(Hussainetal.,2022)usedglobalself-attentionasanaggregationmechanismandutilizededgechannelstocapture
structuralinformation. GRPE(Parketal.,2022)consideredbothnode-spatialandnode-edgerelationsandproposedagraph-
specificrelativepositionalencoding. Graphormer(Yingetal.,2021a)developedgraphstructuralencodingsandintegrated
themintoastandardTransformermodel,whichachievedimpressiveperformanceacrossseveralworldcompetitions(Ying
etal.,2021b;Shietal.,2022). GraphGPS(Rampášeketal.,2022)proposedaframeworktointegratethepositionaland
structuralencodings,localmessage-passingmechanism,andglobalattentionmechanismintotheTransformermodel. All
thesemodelsaredesignedtolearn2Dmolecularrepresentations.
Therealsoexistseveralmodelscapableofutilizingthe3DgeometricstructureinformationinthetrainingsetofPCQM4Mv2.
Transformer-M(Luoetal.,2023)isaTransformer-basedMolecularmodelthatcantakemoleculardataof2Dor3Dformatsas
inputandlearnmolecularrepresentations,whichwaswidelyadoptedbythewinnersofthe2ndOGBLarge-ScaleChallenge.
GPS++(Mastersetal.,2022)isahybridMPNNandTransformermodelbuiltontheGraphGPSframework (Rampášeketal.,
2022).ItfollowsTransformer-Mtoutilize3Datompositionsandauxiliarytaskstowinfirstplaceinthelarge-scalechallenge.
Last,weincludetwocomplexmodelswithO(n3)complexity. GEM-2(Liuetal.,2022a)usedmultiplebranchestoencode
thefull-rangeinteractionsbetweenmany-bodyobjectsanddesignedanaxialattentionmechanismtoefficientlyapproximate
theinteractionwithlowcomputationalcost. Uni-Mol+(Luetal.,2023)proposedaniterativepredictionframeworkto
achieveaccuratequantumpropertyprediction. Itfirstgenerated3Dgeometricstructuresfromthe2Dmoleculargraphusing
fastyetinaccuratemethods,e.g.,RDKit(Landrum,2016). Giventheinaccurate3Dstructureastheinput,themodelis
requiredtopredicttheequilibriumstructureinaniterativemanner. Thepredictedequilibriumstructureisusedtopredictthe
quantumproperty. Uni-Mol+simultaneouslymaintainbothatomrepresentationsandpairrepresentations,whichinducethe
tripletcomplexitywhenupdatingthepairrepresentations. Withthecarefullydesignedtrainingstrategy,Uni-Mol+achieves
22GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
state-of-the-artperformanceonPCQM4Mv2whileyieldinghighcomputationalcosts.
Settings. Aspreviouslystated,DFT-calculatedequilibriumgeometricstructuresareprovidedformoleculesinthetraining
set. Themoleculesinthevalidationsetdonothavesuchinformation. WefollowUni-Mol+(Luetal.,2023)totrainour
GeoMFormer. Inparticular,ourmodeltakestheRDKit-generatedgeometricstructuresastheinputandisrequiredtopredict
boththeHOMO-LUMOenergygapandtheequilibriumstructurebyleveraginginvariantandequivariantrepresentations
respectively. Aftertraining, themodelisabletopredicttheHOMO-LUMOgapusingtheRDKit-generatedgeometric
structures. WereferthereaderstoUni-Mol+(Luetal.,2023)formoredetailsonthetrainingstrategies.
OurGeoMFormermodelconsistsof8layers. Thedimensionofhiddenlayersandfeed-forwardlayersissetto512. The
numberofattentionheadsissetto32. ThenumberofGaussianBasiskernelsissetto128. WeuseAdamWastheoptimizer,
andsetthehyper-parameterϵto1e-8and(β ,β )to(0.9,0.999). Thegradientclipnormissetto5.0. Thepeaklearning
1 2
rateissetto2e-4. Thebatchsizeissetto1024. Thedropoutratiosfortheinputembeddings,attentionmatrices,andhidden
representationsaresetto0.0,0.1,and0.1respectively. Theweightdecayissetto0.0. Themodelistrainedfor1.5million
stepswitha150k-stepwarm-upstage. Afterthewarm-upstage,thelearningratedecayslinearlytozero. Otherhyper-
parametersarekeptthesameastheUni-Mol+forafaircomparison. Themodelistrainedon16NVIDIATeslaV100GPUs.
E.4.Molecule3D
Baselines. Wefollow(Wangetal.,2022)touseseveralcompetitivebaselinesforcomparisonincludingGIN-Virtual(Hu
etal.,2021),SchNet(Schüttetal.,2018),DimeNet++(Gasteigeretal.,2020a),SphereNet(Liuetal.,2022b)whichhave
alreadybeenintroducedinprevioussections. ComENet(Wangetal.,2022)proposedamessage-passinglayerthatoperates
withinthe1-hopneighborhoodofatomsandencodedtherotationanglestofulfillglobalcompleteness. Wealsoimplement
bothPaiNN(Schüttetal.,2021)andTorchMD-Net(Thölke&DeFabritiis,2022)forcomparisons.
Settings. Following(Wangetal.,2022),weevaluateourGeoMFormermodelonbothrandomandscaffoldsplits. Our
GeoMFormer modelconsists of12layers. The dimensionofhidden layersand feed-forwardlayers isset to768. The
numberofattentionheadsissetto48. ThenumberofGaussianBasiskernelsissetto128. WeuseAdamWastheoptimizer,
andsetthehyper-parameterϵto1e-8and(β ,β )to(0.9,0.999). Thegradientclipnormissetto5.0. Thepeaklearningrate
1 2
issetto3e-4. Thebatchsizeissetto1024. Thedropoutratiosfortheinputembeddings,attentionmatrices,andhidden
representationsaresetto0.0,0.1,and0.1respectively. Theweightdecayissetto0.0. Themodelistrainedfor1million
stepswitha60k-stepwarm-upstage. Afterthewarm-upstage,thelearningratedecayslinearlytozero. Themodelistrained
on16NVIDIAV100GPUs.
E.5.N-BodySimulation
Baselines. Following(Satorrasetal.,2021),wechooseseveralcompetitivebaselinesforcomparison. RadialField(Köhler
etal.,2019)developedtheoreticaltoolsforconstructingequivariantflowsandcanbeusedtoperformequivariantprediction
tasks.TensorFieldNetwork(Thomasetal.,2018)embeddedthepositionofanobjectintheCartesianspaceintohigher-order
representationsviaproductsbetweenlearnableradialfunctionsandsphericalharmonics. InSE(3)-Transformer(Fuchsetal.,
2020),thestandardattentionmechanismwasadaptedtoequivariantfeaturesusingoperationsintheTensorFieldNetwork
model. EGNN(Satorrasetal.,2021)proposedasimpleframework. Itsinvariantrepresentationsencodetypeinformation
andrelativedistance,andarefurtherusedinvectorscalingfunctionstotransformtheequivariantrepresentations.
Settings. Theinputofthemodelincludesinitialpositionsp0 = {p0,...,p0} ∈ R5×3 offiveobjects,andtheirinitial
1 5
velocitiesv0 = {v0,...,v0} ∈ R5×3 andrespectivechargesc = {c ,...,c } ∈ {−1,1}5. Weencodepositionsand
1 5 1 5
velocitiesviaseparateequivariantstreams,andupdatedthemwithseparateinvariantrepresentationsviacross-attention
modules. Theequivariantpredictionisbasedonbothequivariantrepresentations.
Wefollowthesettingsin(Satorrasetal.,2021)forafaircomparison. OurGeoMFormermodelconsistsof4layers. The
dimensionofhiddenlayersandfeed-forwardlayersissetto80. Thenumberofattentionheadsissetto8. Thenumberof
GaussianBasiskernelsissetto64. WeuseAdamastheoptimizer,andsetthehyper-parameterϵto1e-8and(β ,β )to
1 2
(0.9,0.999). Thelearningrateisfixedto3e-4. Thebatchsizeissetto100. Thedropoutratiosfortheinputembeddings,
attentionmatrices,activationfunctions,andhiddenrepresentationsareallsetto0.4,andthedroppathprobabilityissetto
0.4. Themodelistrainedfor10,000epochs. Thenumberoftrainingsamplesissetto3.000. Themodelistrainedon1
NVIDIAV100GPUs.
23GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
Table7.ResultsonMDtrajectoriesfromtheMD17dataset.ScoresaregivenbytheMAEofenergypredictions(kcal/mol)andforces
(kcal/mol/Å).NequIPdoesnotprovideerrorsonenergy,forPaiNNweincludetheresultswithlowerforceerroroutoftrainingonlyon
forcesversusonforcesandenergy.BenzenecorrespondstothedatasetoriginallyreleasedinChmielaetal.(2017),whichissometimes
leftoutfromtheliterature.Ourresultsareaveragedoverthreerandomsplits.
Molecule SchNet PhysNet DimeNet PaiNN NequIP TorchMD-Net GeoMFormer
energy 0.37 0.230 0.204 0.167 - 0.123 0.118
Aspirin
forces 1.35 0.605 0.499 0.338 0.348 0.253 0.171
energy 0.08 - 0.078 - - 0.058 0.052
Benzene
forces 0.31 - 0.187 - 0.187 0.196 0.146
energy 0.08 0.059 0.064 0.064 - 0.052 0.047
Ethanol
forces 0.39 0.160 0.230 0.224 0.208 0.109 0.062
energy 0.13 0.094 0.104 0.091 - 0.077 0.071
Malondialdehyde
forces 0.66 0.319 0.383 0.319 0.337 0.169 0.133
energy 0.16 0.142 0.122 0.116 - 0.085 0.081
Naphthalene
forces 0.58 0.310 0.215 0.077 0.097 0.061 0.040
energy 0.20 0.126 0.134 0.116 - 0.093 0.099
SalicylicAcid
forces 0.85 0.337 0.374 0.195 0.238 0.129 0.098
energy 0.12 0.100 0.102 0.095 - 0.074 0.078
Toluene
forces 0.57 0.191 0.216 0.094 0.101 0.067 0.041
energy 0.14 0.108 0.115 0.106 - 0.095 0.095
Uracil
forces 0.56 0.218 0.301 0.139 0.173 0.095 0.068
E.6.MD17
MD17(Xuetal.,2021)consistsofmoleculardynamicstrajectoriesofseveralsmallorganicmolecules. Eachmoleculehas
itsgeometricstructurealongwiththecorrespondingenergyandforce. Thetaskistopredictboththeenergyandforceof
themolecule’sgeometricstructureinthecurrentstate. Toevaluatetheperformanceofmodelsinalimiteddatasetting,all
modelsaretrainedononly1,000samplesfromwhich50areusedforvalidation. Theremainingdataisusedforevaluation.
Foreachmolecule,wetrainaseparatemodelondatasamplesofthismoleculeonly. Wesetthemodelparameterbudgetthe
sameasThölke&DeFabritiis(2022). Following(Thölke&DeFabritiis,2022),wecompareourGeoMFormerwithseveral
competitivebaselines: (1)SchNet(Schüttetal.,2018);(2)PhysNet(Unke&Meuwly,2019);(3)DimeNet(Gasteigeretal.,
2020b);(4)PaiNN(Schüttetal.,2021);(5)NequIP(Batzneretal.,2022);(6)TorchMD-Net(Thölke&DeFabritiis,2022).
TheresultsarepresentedinTable7. ItcanbeeasilyseenthatourGeoMFormerachievescompetitiveperformanceonthe
energypredictiontask(5bestand1tieoutof8molecules)andconsistentlyoutperformsthebestbaselinesbyasignificantly
largemarginontheforcepredictiontask,i.e.,30.6%relativeforceMAEreductioninaverage.
E.7.AblationStudy
Table8.ImpactoftheattentionmodulesonMD17energypredictiontask.Allotherhyperparametersarethesameforafaircomparison.
Inv-Self-Attn Inv-Cross-Attn Equ-Self-Attn Equ-Cross-Attn Aspirin Benzene Ethanol Malondialdehyde Naphthalene SalicylicAcid Toluene Uracil
✓ ✓ ✓ ✓ 0.118 0.052 0.047 0.071 0.081 0.099 0.078 0.095
× ✓ ✓ ✓ 0.156 0.063 0.058 0.085 0.092 0.115 0.090 0.102
✓ × ✓ ✓ 0.161 0.062 0.060 0.086 0.111 0.113 0.094 0.101
✓ ✓ × ✓ 0.131 0.059 0.052 0.081 0.084 0.104 0.085 0.097
✓ ✓ ✓ × 0.143 0.056 0.051 0.078 0.097 0.106 0.081 0.099
✓ × × ✓ 0.169 0.062 0.061 0.087 0.113 0.115 0.094 0.103
✓ × ✓ × 0.172 0.064 0.061 0.086 0.112 0.116 0.095 0.103
× ✓ × ✓ 0.184 0.069 0.064 0.094 0.121 0.124 0.102 0.107
× ✓ ✓ × 0.181 0.071 0.067 0.093 0.118 0.121 0.101 0.109
✓ × × × 0.193 0.076 0.071 0.097 0.126 0.129 0.105 0.113
In this subsection, we conduct comprehensive experiments for ablation study on each building component of
our GeoMFormer model, including self-attention modules (Inv-Self-Attn,Equ-Self-Attn), cross-attention modules
(Inv-Cross-Attn,Equ-Cross-Attn),feed-forwardnetworks(Inv-FFN,Equ-FFN),layernormalizations(Inv-LN,Equ-LN)
andthestructuralencoding. Alltheresultsshowthateachdesignchoiceconsistentlycontributetotheoverallperformance
ofGeoMFormer,stronglysupportingthemotivationofourdesignphilosophy.
ImpactoftheAttentionModules. AsstatedinSection4,ourGeoMFormermodelconsistsoffourattentionmodules.
Without loss of generality, we conduct the experiments on the MD17 task and the N-body Simulation task to evaluate
the contribution of these attention modules to the overall performance. In particular, we consider all possible ablation
24GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
Table9.ImpactoftheattentionmodulesonMD17forcespredictiontask.Allotherhyperparametersarethesameforafaircomparison.
Inv-Self-Attn Inv-Cross-Attn Equ-Self-Attn Equ-Cross-Attn Aspirin Benzene Ethanol Malondialdehyde Naphthalene SalicylicAcid Toluene Uracil
✓ ✓ ✓ ✓ 0.171 0.146 0.062 0.133 0.040 0.098 0.041 0.068
× ✓ ✓ ✓ 0.223 0.152 0.086 0.162 0.051 0.117 0.062 0.079
✓ × ✓ ✓ 0.257 0.159 0.104 0.178 0.063 0.126 0.076 0.091
✓ ✓ × ✓ 0.292 0.167 0.143 0.311 0.079 0.203 0.096 0.134
✓ ✓ ✓ × 0.281 0.160 0.167 0.272 0.094 0.185 0.081 0.113
× ✓ ✓ × 0.313 0.164 0.196 0.319 0.134 0.191 0.105 0.195
✓ × ✓ × 0.366 0.187 0.212 0.352 0.159 0.264 0.161 0.237
× ✓ × ✓ 0.324 0.173 0.189 0.331 0.129 0.237 0.127 0.167
✓ × × ✓ 0.358 0.182 0.219 0.337 0.172 0.272 0.134 0.241
× × ✓ × 0.411 0.194 0.227 0.361 0.188 0.289 0.173 0.258
Table10.ImpactoftheattentionmodulesontheN-bodySimulationtask.Allotherhyperparametersarethesameforafaircomparison.
Inv-Self-Attn Inv-Cross-Attn Equ-Self-Attn Equ-Cross-Attn PerformanceDrop
✗ ✓ ✓ ✓ -8.5%
✓ ✗ ✓ ✓ -8.5%
✓ ✓ ✗ ✓ -19.1%
✓ ✓ ✓ ✗ -14.9%
✗ ✓ ✓ ✗ -14.9%
✓ ✗ ✓ ✗ -21.3%
✗ ✓ ✗ ✓ -17.0%
✓ ✗ ✗ ✓ -21.3%
✗ ✗ ✓ ✗ -25.5%
configurations that involve ablating one or more of the four modules. Note that for an equivariant prediction task (N-
bodysimulationtaskandMD17forcespredictiontask),thepreservationofatleastoneequivariantattentionmoduleis
necessary. Similarly,foraninvariantpredictiontask(MD17energypredictiontask),atleastoneinvariantattentionmodule
should be preserved. The results are presented in Table 8 , Table 9 and Table 10. All the results consistently indicate
thatallfourattentionmodulessignificantlycontributetoboostingthemodel’sperformance. Specifically,theinclusion
of the cross-attn modules (Inv-Cross-Attn, Equ-Cross-Attn) consistently yields notably significant improvements on
bothinvariantandequivariantpredictiontasks. IntheMD17energypredictiontask(invariant),thereisan18.7%relative
improvementforInv-Cross-Attn,a9.8%relativeimprovementforEqu-Cross-Attn,anda20.8%relativeimprovement
whenbothInv-Cross-AttnandEqu-Cross-Attnareutilized. FortheMD17forcepredictiontask(equivariant),therelative
improvementsare28.0%forInv-Cross-Attn,43.9%forEqu-Cross-Attn,andanimpressive60.8%forthecombineduse
ofInv-Cross-AttnandEqu-Cross-Attn. IntheN-bodysimulationtask(equivariant),therelativeimprovementsare7.8%
forInv-Cross-Attn,13.0%forEqu-Cross-Attn,and17.5%forbothInv-Cross-AttnandEqu-Cross-Attn.
Table11.ImpactoftheFFNmodulesonGe- Table12.ImpactoftheLNmodulesonGeoM- Table13.Impactofstructuralencod-
oMFormer.Allotherhyperparametersarekept Former.Allotherhyperparametersarekeptthe ing on GeoMFormer. All other hy-
thesameforafaircomparison. sameforafaircomparison. perparametersarekeptthesamefor
Inv-FFN Equ-FFN PerformanceDrop Inv-LN Equ-LN PerformanceDrop afaircomparison.
✗ ✓ -4.26% ✗ ✓ -8.51% StructuralEncoding PerformanceDrop
✓ ✗ -17.0% ✓ ✗ -63.8% ✗ -53.2%
✗ ✗ -21.3% ✗ ✗ -55.3%
Impact of the FFN. We perform ablation study on the N-body Simulation task to ascertain the contribution of both
invariantandequivariantFFNmodulestothemodel’sperformance. Specifically,weexamineallpossiblesettingsinvolving
theablationofoneorbothoftheFFNmodules. TheresultsarepresentedinTable11,whichdemonstratesthatbothFFN
modulespositivelycontributetoenhancingperformance.
ImpactoftheLN. WeemployinvariantandequivariantLNtostabilizetraining. Toinvestigatewhethertheinvariantand
equivariantLNmodulesimproveperformance,weconductablationstudyontheN-bodySimulationtaskthatencompassall
possiblesettingsofablatingoneorbothLNmodules. TheresultsaredisplayedinTable12,demonstratingthatbothLN
modulesconsistentlyhelptoenhanceperformance.
ImpactoftheStructuralEncoding. Weincorporatethestructuralencodingasabiastermwhencalculatingattention
probabilityinourGeoMFormer,asdescribedinSectionA.WeconductablationstudyontheN-bodySimulationtasktosee
ifithelpsboostperformance. ResultsareshowninTable13. Itcanbeseenthattheintroductionofstructuralencodingleads
toimprovedperformance.
25