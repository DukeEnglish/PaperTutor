FreeTraj: Tuning-Free Trajectory Control
in Video Diffusion Models
HaonanQiu1,ZhaoxiChen1,ZhouxiaWang1,
YingqingHe2,MenghanXia∗3,ZiweiLiu∗∗1
1NanyangTechnologicalUniversity
2HongKongUniversityofScienceandTechnology
3TencentAILab
http://haonanqiu.com/projects/FreeTraj.html
Abstract
Diffusionmodelhasdemonstratedremarkablecapabilityinvideogeneration,which
furthersparksinterestinintroducingtrajectorycontrolintothegenerationprocess.
Whileexistingworksmainlyfocusontraining-basedmethods(e.g.,conditional
adapter),wearguethatdiffusionmodelitselfallowsdecentcontroloverthegener-
atedcontentwithoutrequiringanytraining.Inthisstudy,weintroduceatuning-free
frameworktoachievetrajectory-controllablevideogeneration,byimposingguid-
anceonbothnoiseconstructionandattentioncomputation. Specifically,1)wefirst
showseveralinstructivephenomenonsandanalyzehowinitialnoisesinfluencethe
motiontrajectoryofgeneratedcontent. 2)Subsequently,weproposeFreeTraj,a
tuning-freeapproachthatenablestrajectorycontrolbymodifyingnoisesampling
andattentionmechanisms. 3)Furthermore,weextendFreeTrajtofacilitatelonger
andlargervideogenerationwithcontrollabletrajectories. Equippedwiththese
designs,usershavetheflexibilitytoprovidetrajectoriesmanuallyoroptfortrajecto-
riesautomaticallygeneratedbytheLLMtrajectoryplanner. Extensiveexperiments
validatetheefficacyofourapproachinenhancingthetrajectorycontrollabilityof
videodiffusionmodels.
1 Introduction
Thanks to the powerful modeling capabilities of diffusion models, significant progress has been
madeinopen-worldvisualcontentgeneration,asevidencedbynumerousfoundationaltext-to-video
models(Wangetal.,2023b;Chenetal.,2024). Thesemodelscangeneratevividdynamiccontent
based on arbitrary text prompts. However, while text prompts offer flexibility, they fall short of
concretelyexpressingusers’intentions,particularlyregardinggeometriccontrol. Althoughexisting
trajectorycontrolworksprimarilyrelyontrainingControlNet-likestructures(Wangetal.,2023c;
Chen et al., 2023d), we contend that diffusion model itself contains the potential of substantial
controloverthegeneratedcontentwithoutnecessitatingadditionaltraining. Inthispaper,weaimto
investigatethedynamicsmodelingmechanismsofvideodiffusionmodelsandexplorethepossibility
ofexplicitlycontrollingobjecttrajectoriesbyleveragingtheirinternalproperties. Whilemostofthe
existingeffortsaremadebymodifyingtextembeddingsoradjustingattentionmechanismstoenable
controlorediting(Renetal.,2024;Geyeretal.,2023),theinfluenceofinitialnoisesonvideomotion
remainsunder-explored.
∗CorrespondingAuthors
Preprint.Underreview.
4202
nuJ
42
]VC.sc[
1v36861.6042:viXraFor text-to-video diffusion models, there is considerable diversity in the generated content (i.e.,
motion trajectories) from the same text prompt, depending on the choice of initial noises. This
phenomenonmotivatesustoraiseaquestion: Isitpossibletoregulatethemotiontrajectorieswith
somedesignsoverinitialnoises? FreeInit(Wuetal.,2023c)observedthatlow-frequencysignals
aremoreresistanttoadditivenoises,whichmakesthediffusionmodelbiasedtoinheritlayoutor
shapeinformationfromtheinitialnoises. Consequently,byarrangingthelow-frequencycomponents
ofnoisesacrossframes,wecanmanipulatetheinter-framecontentcorrelation,i.e.,thetemporal
movementsofthegeneratedvideo.However,thisconstraintisnotthatreliablebecausetheinter-frame
regioncorrelationisnotdirectlyalignedwithobjectsemantics. Priorworks (Jainetal.,2023a;Ma
etal.,2023a)havedemonstratedthattrajectoriescanalsobeinfluencedbyadjustingtheattention
weights assigned to different objects in some specific areas. Thus, to achieve object-level-based
trajectorycontrol,weproposetoutilizetext-basedattentiontolocatethetargetobjectsincooperation
withnoisespacemanipulation.
However, introducing alterations to the noise or attention mechanism carries the risk of causing
artifactsinthegeneratedvideos. Forexample,applyingalocalmasktotheself-attentionoperation
cancausepartiallyabnormalvaluesbecausethisdivergesfromthecaseencounteredbythemodels
duringtraining. Furthermore,theseminoranomaliescanpropagatethroughsubsequentlayersand
becomeamplifiedinthefollowingdenoisingsteps,ultimatelyresultinginthetargetregionbeing
filled with artifacts. We call such phenomenon as attention isolation. Previous work (Jain et al.,
2023a)suffersfromthisproblemandiseasytogenerateartifactsintheareaswithmasks. Inour
proposedFreeTrajsystem, wearefullyawareofthisissueandmitigatetheserisksbyapplying
ouroperationstothenoiseandattentionmechanismswithatailor-madescheme. Insteadofhard
attentionmasksusedinPeekaboo(Jainetal.,2023a),ourdesignedsoftattentionmasksrelievethe
phenomenonofattentionisolation. Thisapproachstrikesabalancebetweenstayingclosetothe
trainingdistributionandmaintainingtheabilitytocontroltrajectories.
Inaddition,FreeTrajcanbeseamlesslyintegratedintothelongvideogenerationframework,enriching
themotiontrajectorieswithinthegeneratedlongvideos.Currentvideogenerationmodelsaretypically
trainedonarestrictednumberofframes,leadingtolimitationsingeneratinghigh-fidelitylongvideos
duringinference. FreeNoise(Qiuetal.,2023)proposesatuning-freeandtime-efficientparadigmfor
longervideogenerationbasedonpre-trainedvideodiffusionmodels. AlthoughFreeNoisebrings
satisfactoryvideoqualityandvisualconsistency,ithasnoguaranteeforthevarioustrajectoriesof
generatedobjects,whicharesupposedtoappearinlongvideos. Withthehelpofsometechnical
points proposed by FreeNoise, our FreeTraj successfully generates trajectory-controllable long
videos. FreeTraj is also valuable in larger video generation. When we directly generate videos
with resolutions larger than those in the model training process, we will easily get results with
duplicatedmainobjectsHeetal.(2024). However,FreeTrajwillconstraintheinformationofthe
main objects to the target areas. Signals of main objects are suppressed in other areas thus the
duplicationphenomenonwillbereduced.
Ourcontributionsaresummarizedasfollows: 1)Weinvestigatethemechanismofhowinitialnoises
influencethetrajectoryofgeneratedobjectsthroughseveralinstructivephenomenons. 2)Wepropose
FreeTraj, an effective paradigm for tuning-free trajectory control with both noise guidance and
attentionguidance.3)Weextendthecontrolmechanismtoachievelongerandlargervideogeneration
withacontrollabletrajectory.
2 RelatedWork
DiffusionModelsforVisualGeneration. Diffusionmodelshaverevolutionizedimageandvideo
generation,showcasingtheirabilitytoproducehigh-qualitysamples. DDPM(Hoetal.,2020)and
GuidedDiffusion(Dhariwal&Nichol,2021)aregroundbreakingworksthatshowdiffusionmodels
cangeneratehigh-qualitysamples. Toimproveefficiency,LDM(Rombachetal.,2022)introduces
latentspacediffusionmodelsthatoperateinalower-dimensionalspace,reducingcomputationalcosts
andtrainingtime,whichservesasthefoundationofStableDiffusion. SDXL(Podelletal.,2023)
buildsuponStableDiffusion,achievinghigh-resolutionimagegeneration. Pixart-alpha(Chenetal.,
2023b)replacesthebackbonewithapuretransformer,resultinginhigh-qualityandcost-effective
imagegeneration. Intermsofvideogeneration,VDM(Hoetal.,2022b)isthefirstvideogeneration
modelthatutilizesdiffusion. LVDM(Heetal.,2022)takesitastepfurtherbyproposingalatent
videodiffusionmodelandhierarchicalLVDMframeworkandachievesverylongvideogeneration.
2Align-Your-Latents(Blattmannetal.,2023b)andAnimateDiff(Guoetal.,2023)proposetoinsert
temporaltransformersintopre-trainedtext-to-imagegenerationmodelstoachievetext-to-video(T2V)
generation. VideoComposer(Wangetal.,2023c)presentsacontrollabletext-to-videogeneration
frameworkthatiscapableofcontrollingbothspatialandtemporalsignals. VideoCrafter(Chenetal.,
2023a,2024)andSVD(Blattmannetal.,2023a)scaleupthelatentvideodiffusionmodeltolarge
datasets. Lumiere(Bar-Taletal.,2024)introducestemporaldownsamplingtothespace-timeU-Net.
Sora(OpenAI,2024)isaclosed-sourcevideogeneratorthathasimpressiveresultsannouncedmost
recentlyandhasgarneredmuchattention. Inthiswork,wechooseVideoCrafter2.0(referredtoas
VideoCrafterintherestofthepaper)asourpre-trainedbasemodel,asitisacurrentstate-of-the-art
open-sourcingmodelbasedonthecomprehensiveevaluationsfromVbench(Huangetal.,2023b)
andEvalCrafter(Liuetal.,2023b).
TrajectoryControlinVideoGeneration. Giventhecriticalroleofmotioninvideogeneration,
research on motion control in generated videos has garnered increasing attention. One intuitive
methodinvolvesutilizingmotionextractedfromreferencevideos(Liuetal.,2023a;Weietal.,2023;
Wangetal.,2023c;Zhaoetal.,2023a). Forinstance,approachessuchasTune-A-Video(Wuetal.,
2023a), MotionDirector (Zhao et al., 2023b), and LAMP (Wu et al., 2023b) use specific videos
as references to generalize their motions to various generated videos. Although these methods
achieve significant motion control in video generation, they require training for each reference
motion. Tocircumventtheneedforspecificmotiontraining, ControlNet-likestructures, suchas
VideoComposer (Wang et al., 2023c) and Control-A-Video (Chen et al., 2023d), employ depths,
sketches,ormovingvectorsextractedfromreferencevideosasconditionstocontrolthemotionof
generatedvideos. However,thesemethodsarelimitedtogeneratingvideoswithpre-existingmotions,
constrainingtheircreativityandcustomization.Incontrast,controllingthemotionofgeneratedvideos
usingtrajectoriesorboundingboxesoffersmoreflexibilityanduser-friendliness(Chenetal.,2023c;
Dengetal.,2023;Wangetal.,2024;Yangetal.,2024;Huangetal.,2023a). Whiletraining-based
methods(Chenetal.,2023c;Yinetal.,2023a;Dengetal.,2023;Wangetal.,2023d,2024)have
demonstratedsignificantmotioncontrollability,theydemandsubstantialcomputingresourcesandare
labor-intensiveduringdatacollection. Consequently,severaltraining-freeapproaches(Yangetal.,
2024;Huangetal.,2023a)haveemerged. Thesemethods,suchasPeekaboo(Jainetal.,2023b)and
TrailBlazer(Maetal.,2023b),employexplicitattentioncontroltodirectthemovementofgenerated
objectsaccordingtospecifiedtrajectories.Ourworkalsoadoptsatraining-freeapproach.Weenhance
motion controllability in generated videos by imposing guidance on both noise construction and
attentioncomputation,resultinginimprovedperformanceinbothmotioncontrolandvideoquality.
3 Methodology
3.1 Preliminaries: VideoDiffusionModels
VideoDiffusionModels(VDM)(Hoetal.,2022a)denotesdiffusionmodelsusedforvideogeneration,
which formulates a fixed forward diffusion process to gradually add noise to the 4D video data
x ∼ p(x )andlearnadenoisingmodeltoreversethisprocess. TheforwardprocesscontainsT
0 0
timesteps,whichgraduallyaddnoisetothedatasamplex toyieldx throughaparameterization
0 t
trick:
(cid:112) √
q(x |x )=N(x ; 1−β x ,β I), q(x |x )=N(x ; α¯ x ,(1−α¯ )I), (1)
t t−1 t t t−1 t t 0 t t 0 t
whereβ isapredefinedvarianceschedule,tisthetimestep,α¯
=(cid:81)t
α ,andα =1−β . The
t t i=1 i t t
reversedenoisingprocessobtainslessnoisydatax fromthenoisyinputx ateachtimestep:
t−1 t
p (x |x )=N (x ;µ (x ,t),Σ (x ,t)). (2)
θ t−1 t t−1 θ t θ t
Hereµ andΣ aredeterminedthroughanoisepredictionnetworkϵ (x ,t),whichissupervised
θ θ θ t
bythefollowingobjectivefunction,whereϵissampledgroundtruthnoiseandθ isthelearnable
networkparameters.
minE ∥ϵ−ϵ (x ,t)∥2, (3)
θ
t,x0,ϵ θ t 2
Oncethemodelistrained,wecansynthesizeadatapointx fromrandomnoisex bysamplingx
0 T t
iteratively. Consideringthehighcomplexityandinter-frameredundancyofvideos,LatentDiffusion
Model(LDM)(Rombachetal.,2022)iswidelyadoptedtoformulatethediffusionanddenoising
3Frame 1
Frame 8
Frame 15
Full 𝑧"#$% Resample 50% Resample 75% Resample 90% Resample 100%
!
Figure 1: Noise resampling of initial high-frequency components. Gradually increasing the
proportionofresampledhigh-frequencyinformationintheframe-wisesharednoisescansignificantly
reducetheartifactinthegeneratedvideo. However,thisalsoleadstoagraduallossintrajectory
controlability. Aresamplingpercentageof75%strikesabetterbalancebetweenmaintainingcontrol
andimprovingthequalityofthegeneratedvideo.
processinamorecompactlatentspace. LatentVideoDiffusionModels(LVDM)isrealizedthrough
perceptualcompressionwithaVariationalAuto-Encoder(VAE)Kingma&Welling(2014),where
an encoder E maps x ∈ R3×F×H×W to its latent code z ∈ R4×F×H′×W′ and a decoder D
0 0
reconstructstheimage x fromthez . Then, thediffusionmodelθ operatesonthevideolatent
0 0
variablestopredictthenoiseϵˆ.
z =E(x ), xˆ =D(z )≈x , ϵˆ=ϵ (z ,y,t), (4)
0 0 0 0 0 θ t
whereydenotesconditionsliketextprompts. MostmainstreamLVDMs(Blattmannetal.,2023b;
Wangetal.,2023b;Chenetal.,2023a)areimplementedbyaUNetequippedwithconvolutional
modules,spatialattentions,andtemporalattentions. Thebasiccomputationblock(whosefeature
inputandoutputarehandh′respectively)couldbedenotedas:
h′ =TT(ST(Tconv(Conv(h,t)),y)), TT=Proj ◦(Attn ◦Attn ◦MLP)◦Proj .
in temp temp out
(5)
HereConvandSTareresidualconvolutionalblockandspatialtransformer,whileTconvdenotes
temporalconvolutionalblockandTTdenotestemporaltransformers,servingascross-frameoperation
modules.
3.2 NoiseInfluenceonTrajectoryControl
During the training process of the video diffusion model, it cannot fully corrupt the semantics,
leavingsubstantialspatio-temporalcorrelationsinthelow-frequencycomponents(Wuetal.,2023c).
Thoselow-frequencycorrelationsmaystillcontaintheinformationoftrajectory. Therefore,ifwe
simulatethenoisesofthetrainingprocessandmanuallyaddsomespatio-temporalcorrelationsinthe
low-frequencycomponents,wehaveachancetocontrolthemotionofthegeneratedvideo.
NoiseFlow. Ourfirstattemptistomakethenoiseflowamongframes. Insteadofrandomlysampling
initialnoisesforallframes,weonlysamplethenoiseforthefirstframe. Thenwemovethenoise
fromthetop-lefttothebottom-rightwithstride2andrepeatthisoperationuntilwegetinitialnoises
zflow forallframes. Specially,initialnoiseϵforeachframef inposition[i,j]is:
T
ϵ[i,j]f =ϵ[(i−2)(modH),(j−2)(modW)]f−1. (6)
Afterdenoisingzflow,althoughwewillgetavideowithstrongartifacts(Figure1),wecanstillfind
T
avaluablephenomenon: objectsandtexturesinthevideoalsoflowinthesamedirection(top-leftto
bottom-right). Thisphenomenonverifiesthatthetrajectoryoftheinitialnoisescanguidethemotion
trajectoryofgeneratedresults.
High-FrequencyNoiseResampling. ArtifactsinNoiseFlowaremainlycausedbydeviationfrom
theindependentrandomdistributionoftheinitialnoises.Therefore,ifweresamplesomenewrandom
independentnoisestoreplacesomedependentnoisesinzflow,morerealisticresultsareexpectedto
T
4Trajectory
Frame 1
Frame 8
Frame 15
(Rough) Success Cases Failure Cases
Figure2: Trajectorycontrolviaframe-wisesharedlow-frequencynoise. Thesuccesscaseson
theleftdemonstratethatthemovingobjectsinthegeneratedvideoscanberoughlycontrolledby
sharinglow-frequentnoiseacrosstheboundingboxesofthegiventrajectory. However,theprecision
ofcontrolandthesuccessrateremainsomewhatconstrained,asevidencedbythefailureinstanceson
theright.
begenerated. AccordingtotheanalysisofFreeInit(Wuetal.,2023c),thetrajectoryinformationis
mainlyobtainedinthelow-frequencynoise. Therefore,weuseFourierTransformationtoresample
high-frequencynoiseandgetnewlatentz˜ toperformfurtherdenoising.
T
Flow =FFT (z )⊙H,
zT 3D T
Fhigh =FFT (η)⊙(1−H), (7)
η 3D
z˜ =IFFT (cid:0) FL +Fhigh(cid:1) ,
T 3D zT η
whereFFT istheFastFourierTransformationoperatedonbothspatialandtemporaldimensions,
3D
andIFFT istheInverseFastFourierTransformationthatmapsnoisebackfromtheblended
3D
frequencydomain. Histhespatial-temporalLowPassFilter(LPF),whichisatensorofthesame
shapeasthelatent. ηisanewlysampledrandomnoisetoreplacethehigh-frequencyoftheoriginal
noise. Inthiscase,z =zflow.
T T
Figure1showsthatthevisualqualityissignificantlyimprovedastheproportionofhigh-frequency
noiseresampledincreases. Correspondingly,theflowphenomenonisweakened. When90%high-
frequencynoiseisresampled,theflowisalmoststoppedwithonlysomesimilartexturesremaining
(e.g.,branchesfromtop-lefttobottomright).Overall,75%resamplingstrikesagoodbalancebetween
sportinessandimagequality.
Trajectory Injection. In noise flow, all objects in the foreground and background tend to move
toward the direction of flow. If we only control the flow happening in the local area with some
trajectories,canweguidetheonlymainobjecttomovefollowingthecorrespondingtrajectories?
Toanswerit,wedesignsometrajectoriesfromsimpletocomplexandmaketheflowareaoccupya
quarterofthearea,asshowninthefirstrowofFigure2.
Instead of directly denoising random noises, we inject trajectory into the initial noises. We first
initializearandomlocalnoiseϵ accordingtotheareaoftheinputmaskandF framesofrandom
local
noises[ϵ ,ϵ ,...,ϵ ]independently. Thenforeachframef,theinitialnoiseϵ willbereplacedby
1 2 F f
theϵ ifintheareaoftheinputmask:
local
(cid:26)
ϵ [i,j] ifM [i,j]=0
ϵ˜[i,j]= f f , (8)
f ϵ [i∗,j∗] ifM [i,j]=1
local f
whereϵ ,ϵ ∼N (0,I),M istheinputmaskofframef,andM [i,j]=1iftheposition(i,j)
f local f f
isinsidetheboundingboxoftrajectory. M [i,j]=0otherwise. (i∗,j∗)isthecorrespondinglocal
f
positioninthebox.
AsshownintheleftofFigure2,someobjectsarewellgeneratedandfollowthetrajectoryinjectedin
initialnoisesalthoughtheymaynotbefullyalignedwiththegivenboundingboxes. Whilethese
5Trajectory Injection Iterative Denoising with Guidance in Attention
Denoising U-Net
Init Noise Frequency
Noise Flow Fusion Spatial SA Text CA Temporal SA 𝑡 !
Noise Resampling Eq. 8 Eq. 7 Eq. 11 Eq. 9 Eq. 11
𝑡"
Spatial mask sequences 𝑺
O “b Aje dc et eT rr waj ae lc kt io nr gy in a snowy field.” Text Prompt h x w h x w 𝑴 h x $ w" A deerwalk Ai n deg ei rn wa as l An k io dnw eg ey i rf ni we al alsd. kn𝑴 io nw g y i! nf i" ae l sd. nowy field. 𝑡! 𝑡"𝑡𝑡 !# 𝑡"𝑴 𝑡 𝑡# !#" 𝑡"𝑡# 𝑡 #
Figure3: AnoverviewofFreeTraj. Ourframeworkmainlycontainstwoparts: guidanceinnoise
andguidanceinattention. Fornoise,weinjectthetargettrajectoryintothelow-frequencypart. For
attention,wedesigndifferentreweighingstrategiesaccordingtothesupposedbehaviorsindifferent
attentionlayers. HereS,M ,M ,andM aredifferentattentionmasks.
CA SA TA
objectsmovealongthetrajectory,theywillalsotrytofollowthepriorknowledgeofthephysical
worldcontainedinthemodel(e.g. dolphinscannotgotoofarfromtheseaafterjumping). Andthe
rightofFigure2showssomefailurecases. Theyareeitherpoorinvisualqualityorintrajectory
alignment.
Basedonthoseobservations,althoughwecanutilizeinitialnoisestoguidethetrajectory,westill
needtoinvolveadditionalcontrolmechanismstoachieveaccuratetrajectorycontrol,especiallywhen
thetargettrajectorydeviatesfromapriorknowledgeofthephysicalworldcontainedinthemodel.
3.3 TheFrameworkofFreeTraj
Givenatargetboundingboxforaforegroundobjectinthevideo,wesupposethepre-trainedvideo
modeltogenerateresultswhosetrajectoryisalignedwiththegivenbox. Toachievethat,wepropose
FreeTraj,whichdesignsguidanceinbothnoiseandattentionasshowninFigure3.
3.3.1 GuidanceinNoise
As analyzed in Section 3.2, frame-wise shared low-frequency noise can guide the trajectory of
generatedobjects. Therefore,weinjecttrajectoryintheinitialnoisesthroughEquation8. Toreduce
thephenomenaofattentionisolation,westillneedtoremovesomeoftheinjectednoisesthrough
High-FrequencyNoiseResampling(Equation7).
3.3.2 GuidanceinAttention
Objecttrajectoriesingeneratedvideoswithonlynoiseguidancestilltendtofollowthepriorinforma-
tionofthevideomodel. Tomakethecontrolmoreaccurateprecisely,wealsoaddtrajectoryguidance
in attention. There are three kinds of attention layers in the UNet of VideoCrafter (Chen et al.,
2024): spatialcross-attention,spatialself-attention,andtemporalself-attention. Unlikeprevious
workPeekaboo(Jainetal.,2023a)directlymaskstheforegroundandbackgroundrespectivelyfor
allattentionlayers,wedesigndifferentstrategiesaccordingtothesupposedbehaviorsindifferent
attention layers. All attention editing is performed in the early steps t ∈ {T,...,T −N} of the
denoising process, where T is the total number of denoising timesteps, and N is the number of
timestepsforattentionediting.
AttentionIsolation. Wefindthepreviousdesignsinattentionmaycauseattentionisolation. Itisa
phenomenonthatsomeregionsbecomeisolatedeitherspatiallyortemporallyandrarelypayattention
toinformationoutsidetheirownregion. Thisisoftencausedbythevaluesinthisareadeviating
too much from the training distribution. Unlucky, it is difficult for this region to restore itself to
normallevelsthroughvaluableinformationfromtheotherregionsduetotheisolation. Therefore,it
isnecessarytoavoidattentionisolationwhenwemodifytheattentionmechanismwithoutre-training.
Wewilldiscussmoreintheablationstudyandappendix.
Cross Attention Guidance. Spatial cross-attention is the only place for prompts to inject the
informationfromtextembedding. Originally,themodelwouldassigntheobjectaccordingtothe
promptsandinitialnoises. Itisarandomandunpredictablebehavior. Toforcethemodeltoonly
generatethetargetobjectinthegivenboundingbox,wefirstaddguidancetothecross-attention.
6
w x h w x h w x h
redoceDGivenqueryQ,keyK,valueV ofcross-attention,andthere-scaledbinary2DattentionmasksM
a
andM′,whichindicatetheforegroundandbackgroundareasofthegeneratedvideorespectively.
a
Ourguidedcross-attentionis:
(cid:18) QKT (cid:19)
GuidedCrossAttention(Q,K,V,M ,M′)=(softmax √ +M +S)V,
a a
d
(9)
(cid:26) 0 ifM [i,j]=0 (cid:26) −∞ ifM′[i,j]=0
whereS[i,j]= a ,andM[i,j]= a .
αg(i,j) ifM [i,j]=1 0 ifM′[i,j]=1
a a
Hereαisacoefficienttoenhancetheinfluenceoftargetpromptsintheforegroundandg(·,·)isa
Gaussianweight(Maetal.,2023a). NotethattheattentionmasksM a,M a′ ∈ {0,1}dq×dk,where
d andd arethelengthsofqueriesandkeys,respectively. Theyareattainedwithagivenprompt
q k
P andthetargetmaskMf [i]offramef (Mf [i]isa1-DflattenformofM inEq.8). Inthe
target target f
cross-attentionlayer,M andM′ arerespectivelydenotedasM andM′ ,where
a a CA CA
(cid:16) (cid:17)
Mf [i,j]=fg Mf [i] ∗fg(P[j]),
CA target
(10)
(cid:16) (cid:16) (cid:17)(cid:17)
M′f [i,j]= 1−fg Mf [i] ∗(1−fg(P[j])),
CA target
wherefgisafunctionthattakesapixeloratexttokenasinput,returning1ifitcorrespondstothe
foregroundofthevideo,and0otherwise.
Self Attention Guidance. Self-attention consists of the spatial part and temporal part. Without
mandatoryconstraints,theinformationintheforegroundandbackgroundwillinteract. Inthiscase,
the video model may still generate target objects at unexpected locations. Therefore, we design
guidedself-attention:
(cid:18) QKT (cid:19)
GuidedSelfAttention(Q,K,V,M )=softmax √ ×W V,
a
d
(11)
(cid:26)
β ifM [i,j]=0
whereW[i,j]= a .
1 ifM [i,j]=1
a
Here β is a coefficient to weaken the influence of the interaction of foreground and background.
Comparedtothehardmaskusing−∞toforbidtheinteractionofforegroundandbackground,this
softmaskdesigncanavoidsomeartifactscausedbyattentionisolation.
TheattentionmaskM designedinself-attentionfollowsthePeekaboo(Jainetal.,2023a). Specifi-
a
cally,inthespatialself-attentionlayer,M isdenotedasM ,where
a SA
(cid:16) (cid:17) (cid:16) (cid:17)
Mf [i,j]=fg Mf [i] ∗fg Mf [j]
SA target target
(12)
(cid:16) (cid:16) (cid:17)(cid:17) (cid:16) (cid:16) (cid:17)(cid:17)
+ 1−fg Mf [i] ∗ 1−fg Mf [j] ,
target target
andinthetemporalself-attentionlayer,M isdenotedasM ,where
a TA
Mi [f,k]=fg(cid:16) Mf [i](cid:17) ∗fg(cid:0) Mk [i](cid:1)
TA target target
(13)
+(cid:16) 1−fg(cid:16) Mf [i](cid:17)(cid:17) ∗(cid:0) 1−fg(cid:0) Mk [i](cid:1)(cid:1) .
target target
3.4 LongerVideoGeneration
FreeTrajcanalsobeintegratedintothelongervideogenerationframeworkFreeNoise(Qiuetal.,
2023)togeneraterichmotiontrajectoriesinlongvideos. FreeNoisemainlyappliesLocalWindow
FusiontothetemporalattentiontoguaranteevisualqualityandutilizeNoiseReschedulinginthe
noiseinitializationtoreservevideoconsistency.
LocalWindowFusiondividesthetemporalattentionintoseveraloverlappedlocalwindowsalong
the temporal dimension and then fuses them together. In order to cooperate with Local Window
Fusion,ourguidanceintemporalattentionisonlyappliedwithineachLocalWindowFusion. Noise
Reschedulingreusesandshufflesthesub-fragmentofinitialnoises. Toavoidourguidanceinnoise
being destroyed, Equation 8 and Equation 7 are applied after Noise Rescheduling. As shown in
Figure6,ourmethodachievestrajectorycontrolsoveralongvideosequencewithoutanyfinetuning.
7Table1:Quantitativecomparisonoftrajectorycontrol.FreeTrajachievescompetitiveperformance
inmetricsaboutvideoqualityandgainsthebestscoresinmetricsthatarerelatedtotrajectorycontrol.
Method FVD(↓) KVD(↓) CLIP-SIM(↑) mIoU(↑) CD(↓)
Direct 118.19 -2.28 0.980 0.161 0.225
Peekaboo(Jainetal.,2023a) 403.00 25.30 0.963 0.235 0.179
TrailBlazer(Maetal.,2023a) 556.00 42.14 0.958 0.179 0.219
Ours 436.22 29.85 0.956 0.281 0.154
“A corgi running on the “A swan floating
grassland on the grassland.” gracefully on a lake.”
Figure4: Qualitativecomparisonoftrajectorycontrol. WecompareourproposedFreeTrajwith
directinference(Direct),Peekaboo(Peek),andTrailBlazer(TraB).FreeTrajsuccessfullygenerates
high-fidelityresultsandismoreaccuratefortrajectorycontrol.
4 Experiments
Based on performance and accessibility considerations, we choose the most recently published
open-sourcevideodiffusionmodel,VideoCrafter(Chenetal.,2024),asourpre-trainedvideomodel
inthispaper. Allexperimentsareconductedbasedonthismodel.
EvaluationMetrics. Toevaluatevideoquality,wereportFréchetVideoDistance(FVD)(Unterthiner
etal.,2018),KernelVideoDistance(KVD)(Unterthineretal.,2019). Sincethetuning-freemethods
are supposed to keep the quality of the original pre-trained inference, we calculate the FVD and
KVDbetweentheoriginalgeneratedvideosandvideosgeneratedbytrajectorycontrolmethods. We
useCLIPSimilarity(CLIP-SIM)(Radfordetal.,2021)tomeasurethesemanticsimilarityamong
frames. Inaddition,weutilizetheoff-the-shelfdetectionmodel,OWL-ViT-large(Mindereretal.,
2022), to obtain the bounding box of the synthesized objects. Then Mean Intersection of Union
(mIoU)andCentroidDistance(CD)arecalculatedtoevaluatethetrajectoryalignment. CDisthe
distancebetweenthecentroidofthegeneratedobjectandtheinputmask,normalizedto1. When
OWL-ViT-largefailstodetectthetargetobjectinthegeneratedvideos, thefarthestpointwillbe
assignedasthepenaltyinCD.
4.1 EvaluationofTrajectoryControl
We compare our proposed FreeTraj to other tuning-free trajectory-controllable video generation
methodswithdiffusionmodelsPeekaboo(Jainetal.,2023a)andTrailBlazer(Maetal.,2023a).
AsshowninFigure4,TrailBlazerhastheworstcontrolbecauseitdoesnotapplythecontrolinspatial
self-attentionwhiletheothertwomethodsdo. VideosgeneratedbyPeekaboowillroughlyfollow
thegiventrajectorybutnotprecisely. Inaddition,Peekaboogeneratesanadditionalblackswanwith
weirdartifacts,whichisprobablycausedbythehardattentionmaskusedinself-attentionlayers. Our
FreeTrajsucceedsindrivingthetargetobjectfollowingthegiventrajectorieswithvividmotions.
ForquantitativeresultsshowninTable1,TrailBlazerhastheworstmIoUandCD,whichareonly
slightlybetterthanthoseindirectinference. PeekaboohasbettermIoUandCD,showingtherough
control ability of trajectory but is still significantly weakened than our proposed FreeTraj. Both
qualitativeandquantitativeresultsshowthatourFreeTrajismoreaccuratefortrajectorycontrol.
However,wefindtheFVD,KVD,andCLIP-SIM,whicharethereferencesforvideoquality,are
8
tceriD
keeP
BarT
sruOTable2: Userstudy. UsersarerequestedtopickthebestoneamongourproposedFreeTrajwiththe
otherbaselinemethodsintermsoftrajectoryalignment,video-textalignment,andvideoquality.
Method TrajectoryAlignment Video-TextAlignment VideoQuality
Peekaboo(Jainetal.,2023a) 6.48% 15.12% 13.58%
TrailBlazer(Maetal.,2023a) 8.03% 6.79% 6.79%
Oursw/oNoise 19.75% 15.43% 15.74%
Ours 65.74% 62.65% 63.89%
“A bear climbing down a “A corgi running on the
tree after spotting a threat.” grassland on the grassland.”
Figure5: Ablationresults. (a)Nonoiseguidance,(b)nohigh-frequencynoiseresampling,(c)hard
attentionmask,and(d)ourwholemethod.
slightlyworsethanthoseinPeekaboo. AsshowninFigure4,Peekabootendstogeneratevideos
whoseobjectsactaroundthecenteroftheframe. Thisbehaviorissimilartoreferencevideoswhich
aredirectlygeneratedbyVideoCrafter,leadingtoabetterFVDandKVD.TrailBlazeralsomentions
thisphenomenonoflazymovementforbetterFVD.
Inaddition,weconductedauserstudytoevaluateourresultsbasedonhumansubjectiveperception.
Participantswereaskedtowatchthegeneratedvideosfromallmethods,witheachexampledisplayed
inarandomordertoavoidbias. Theyweretheninstructedtoselectthebestvideointhreeevaluation
aspects: trajectory alignment, video-text alignment, and video quality. The results, as shown in
Table2,demonstratethatourapproachoutperformsthebaselinemethodsbyasignificantmargin,
achievingthehighestscoresinallaspects. Notably,ourmethodreceivednearly70%votesintermsof
trajectoryalignment. Thisuserstudyconfirmsthesuperiorityofourapproachintermsoftrajectory
alignment,video-textalignment,andvideoquality.
4.2 AblationStudies
AblationofNoiseGuidance. Toshowtheeffectivenessofnoiseguidance,werunourdesigned
attention guidance solely. Figure 5 (a) shows that pure attention guidance can also control the
trajectorybutmaylosesomeaccuracy.
AblationofAttentionIsolation. Wealsostudytwosettingsthatmaycauseattentionisolation. The
firstoneisusingthehardattentionmaskinEquation11. Thesecondoneusesnohigh-frequency
noiseresamplingwhenapplyingtrajectoryinjectionininitialnoises(Equation11). Usually,diffusion
modelshavesomerobustnesstodealwiththeinputwithsmalldeviationandrecoverittogenerate
qualifiedresults. However,bothofthesetwostrategieswilleasilycausethevalueoftheattention
layertodeviatefarfromthedatadistributioninthetrainingstage. Itwillleadtoattentionisolation
whereisolatedregionsalmostpaynoattentiontootherregions,losingthechancetorecoverback
tothenormaldistribution. AsshowninFigure5(b)and(c),blockyartifactsappearandfollowthe
giventrajectoryinthegeneratedvideos. Inaddition,thoseartifactshappentofallatthepositionof
theattentionmaskorinjectlocalnoise.
5 Conclusion
In conclusion, our study has revealed several instructive phenomenons about how initial noises
influence the generated results of video diffusion models. Leveraging the noise guidance and
9
)a(
)b(
)c(
sruO
)d(combining it with careful modifications to the attention mechanism, we introduce a tuning-free
framework, FreeTraj, for trajectory-controllable video generation using diffusion models. We
demonstrate that diffusion models inherently possess the capability to control generated content
withoutadditionaltraining. Byguidingnoiseconstructionandattentioncomputation, weenable
trajectorycontrolandextendittolongerandlargervideogeneration. Althoughnotshowninthis
paper, our approach offers flexibility for users to provide trajectories manually or automatically
generatedbytheLLMtrajectoryplanner. Extensiveexperimentsvalidatetheeffectivenessofour
approachinenhancingthetrajectorycontrollabilityofvideodiffusionmodels,providingapractical
andefficientsolutionforgeneratingvideoswithdesiredmotiontrajectories.However,thistuning-free
paradigmisstilllimitedbytheunderlyingmodel,suchastheconsistencyofobjectappearancethat
easilychangesduringlargemovements. Wehopethatthestudyofinitialnoisescanalsoinspirethe
developmentofbasicvideomodels.
References
OmerBar-Tal,HilaChefer,OmerTov,CharlesHerrmann,RoniPaiss,ShiranZada,ArielEphrat,
JunhwaHur, YuanzhenLi, TomerMichaeli, etal. Lumiere: Aspace-timediffusionmodelfor
videogeneration. arXivpreprintarXiv:2401.12945,2024.
AndreasBlattmann,TimDockhorn,SumithKulal,DanielMendelevitch,MaciejKilian,Dominik
Lorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts,etal. Stablevideodiffusion: Scaling
latentvideodiffusionmodelstolargedatasets. arXivpreprintarXiv:2311.15127,2023a.
AndreasBlattmann,RobinRombach,HuanLing,TimDockhorn,SeungWookKim,SanjaFidler,and
KarstenKreis. Alignyourlatents: High-resolutionvideosynthesiswithlatentdiffusionmodels.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.
22563–22575,2023b.
Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo
Xing,YaofangLiu,QifengChen,XintaoWang,etal. Videocrafter1: Opendiffusionmodelsfor
high-qualityvideogeneration. arXivpreprintarXiv:2310.19512,2023a.
HaoxinChen,YongZhang,XiaodongCun,MenghanXia,XintaoWang,ChaoWeng,andYingShan.
Videocrafter2: Overcomingdatalimitationsforhigh-qualityvideodiffusionmodels,2024.
JunsongChen,JinchengYu,ChongjianGe,LeweiYao,EnzeXie,YueWu,ZhongdaoWang,James
Kwok,PingLuo,HuchuanLu,andZhenguoLi. Pixart-α: Fasttrainingofdiffusiontransformer
forphotorealistictext-to-imagesynthesis,2023b.
Tsai-ShienChen,ChiehHubertLin,Hung-YuTseng,Tsung-YiLin,andMing-HsuanYang. Motion-
conditioneddiffusionmodelforcontrollablevideosynthesis. arXivpreprintarXiv:2304.14404,
2023c.
Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin.
Control-a-video: Controllable text-to-video generation with diffusion models. arXiv preprint
arXiv:2305.13840,2023d.
YufanDeng,RuidaWang,YuhaoZhang,Yu-WingTai,andChi-KeungTang. Dragvideo: Interactive
drag-stylevideoediting. arXivpreprintarXiv:2312.02216,2023.
PrafullaDhariwalandAlexanderNichol. Diffusionmodelsbeatgansonimagesynthesis. Advances
inneuralinformationprocessingsystems,34:8780–8794,2021.
SongweiGe,ThomasHayes,HarryYang,XiYin,GuanPang,DavidJacobs,Jia-BinHuang,and
DeviParikh. Longvideogenerationwithtime-agnosticvqganandtime-sensitivetransformer. In
EuropeanConferenceonComputerVision,pp.102–118.Springer,2022.
MichalGeyer,OmerBar-Tal,ShaiBagon,andTaliDekel. Tokenflow: Consistentdiffusionfeatures
forconsistentvideoediting. arXivpreprintarXiv:2307.10373,2023.
YuweiGuo,CeyuanYang,AnyiRao,YaohuiWang,YuQiao,DahuaLin,andBoDai. Animatediff:
Animateyourpersonalizedtext-to-imagediffusionmodelswithoutspecifictuning. arXivpreprint
arXiv:2307.04725,2023.
10WilliamHarvey,SaeidNaderiparizi,VadenMasrani,ChristianWeilbach,andFrankWood. Flexible
diffusion modeling of long videos. Advances in Neural Information Processing Systems, 35:
27953–27965,2022.
YingqingHe,TianyuYang,YongZhang,YingShan,andQifengChen. Latentvideodiffusionmodels
forhigh-fidelityvideogenerationwitharbitrarylengths. arXivpreprintarXiv:2211.13221,2022.
YingqingHe,MenghanXia,HaoxinChen,XiaodongCun,YuanGong,JinboXing,YongZhang,
XintaoWang,ChaoWeng,YingShan,etal.Animate-a-story:Storytellingwithretrieval-augmented
videogeneration. arXivpreprintarXiv:2307.06940,2023.
YingqingHe,ShaoshuYang,HaoxinChen,XiaodongCun,MenghanXia,YongZhang,XintaoWang,
RanHe,QifengChen,andYingShan.Scalecrafter:Tuning-freehigher-resolutionvisualgeneration
with diffusion models. In The Twelfth International Conference on Learning Representations,
2024.
RobertoHenschel,LevonKhachatryan,DaniilHayrapetyan,HaykPoghosyan,VahramTadevosyan,
ZhangyangWang,ShantNavasardyan,andHumphreyShi. Streamingt2v: Consistent,dynamic,
andextendablelongvideogenerationfromtext. arXivpreprintarXiv:2403.14773,2024.
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin
NeuralInformationProcessingSystems,33:6840–6851,2020.
JonathanHo,WilliamChan,ChitwanSaharia,JayWhang,RuiqiGao,AlexeyGritsenko,DiederikP
Kingma,BenPoole,MohammadNorouzi,DavidJFleet,etal. Imagenvideo: Highdefinition
videogenerationwithdiffusionmodels. arXivpreprintarXiv:2210.02303,2022a.
JonathanHo,TimSalimans,AlexeyGritsenko,WilliamChan,MohammadNorouzi,andDavidJ
Fleet.Videodiffusionmodels.AdvancesinNeuralInformationProcessingSystems,35:8633–8646,
2022b.
Hsin-PingHuang,Yu-ChuanSu,DeqingSun,LuJiang,XuhuiJia,YukunZhu,andMing-Hsuan
Yang. Fine-grainedcontrollablevideogenerationviaobjectappearanceandcontext. arXivpreprint
arXiv:2312.02919,2023a.
ZiqiHuang,YinanHe,JiashuoYu,FanZhang,ChenyangSi,YumingJiang,YuanhanZhang,Tianxing
Wu,QingyangJin,NattapolChanpaisit,etal. Vbench: Comprehensivebenchmarksuiteforvideo
generativemodels. arXivpreprintarXiv:2311.17982,2023b.
YashJain,AnshulNasery,VibhavVineet,andHarkiratBehl. Peekaboo: Interactivevideogeneration
viamasked-diffusion. arXivpreprintarXiv:2312.07509,2023a.
YashJain,AnshulNasery,VibhavVineet,andHarkiratBehl. Peekaboo: Interactivevideogeneration
viamasked-diffusion. arXivpreprintarXiv:2312.07509,2023b.
DiederikPKingmaandMaxWelling. Auto-encodingvariationalbayes. InInternationalConference
onLearningRepresentations(ICLR),2014.
JianLiang,ChenfeiWu,XiaoweiHu,ZheGan,JianfengWang,LijuanWang,ZichengLiu,Yuejian
Fang,andNanDuan. Nuwa-infinity: Autoregressiveoverautoregressivegenerationforinfinite
visualsynthesis. AdvancesinNeuralInformationProcessingSystems,35:15420–15432,2022.
ShaotengLiu,YuechenZhang,WenboLi,ZheLin,andJiayaJia. Video-p2p: Videoeditingwith
cross-attentioncontrol. arXivpreprintarXiv:2303.04761,2023a.
Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu,
TieyongZeng,RaymondChan,andYingShan. Evalcrafter: Benchmarkingandevaluatinglarge
videogenerationmodels. arXivpreprintarXiv:2310.11440,2023b.
Wan-DuoKurtMa,JPLewis,andWBastiaanKleijn. Trailblazer: Trajectorycontrolfordiffusion-
basedvideogeneration. arXivpreprintarXiv:2401.00896,2023a.
Wan-DuoKurtMa,JPLewis,andWBastiaanKleijn. Trailblazer: Trajectorycontrolfordiffusion-
basedvideogeneration. arXivpreprintarXiv:2401.00896,2023b.
11MatthiasMinderer,AlexeyGritsenko,AustinStone,MaximNeumann,DirkWeissenborn,Alexey
Dosovitskiy,AravindhMahendran,AnuragArnab,MostafaDehghani,ZhuoranShen,etal. Simple
open-vocabularyobjectdetection. InEuropeanConferenceonComputerVision, pp.728–755.
Springer,2022.
OpenAI. Video generation models as world simulators. Technical report, OpenAI, 2024. URL
https://openai.com/research/video-generation-models-as-world-simulators.
YichenOuyang, HaoZhao, GaoangWang, etal. Flexifilm: Longvideogenerationwithflexible
conditions. arXivpreprintarXiv:2404.18620,2024.
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe
Penna,andRobinRombach. Sdxl: Improvinglatentdiffusionmodelsforhigh-resolutionimage
synthesis. arXivpreprintarXiv:2307.01952,2023.
Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei
Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. arXiv preprint
arXiv:2310.15169,2023.
AlecRadford, JongWookKim, ChrisHallacy, AdityaRamesh, GabrielGoh, SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,pp.
8748–8763.PMLR,2021.
Jiawei Ren, Mengmeng Xu, Jui-Chieh Wu, Ziwei Liu, Tao Xiang, and Antoine Toisoul. Move
anythingwithlayeredscenediffusion. arXivpreprintarXiv:2404.07178,2024.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFConfer-
enceonComputerVisionandPatternRecognition,pp.10684–10695,2022.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprintarXiv:2010.02502,2020.
ThomasUnterthiner,SjoerdvanSteenkiste,KarolKurach,RaphaelMarinier,MarcinMichalski,and
SylvainGelly. Towardsaccurategenerativemodelsofvideo: Anewmetric&challenges. arXiv
preprintarXiv:1812.01717,2018.
ThomasUnterthiner,SjoerdvanSteenkiste,KarolKurach,RaphaelMarinier,MarcinMichalski,and
SylvainGelly. Towardsaccurategenerativemodelsofvideo: Anewmetric&challenges. ICLR,
2019.
RubenVillegas,MohammadBabaeizadeh,Pieter-JanKindermans,HernanMoraldo,HanZhang,
MohammadTaghiSaffar,SantiagoCastro,JuliusKunze,andDumitruErhan. Phenaki: Variable
lengthvideogenerationfromopendomaintextualdescription. arXivpreprintarXiv:2210.02399,
2022.
Fu-YunWang,WenshuoChen,GuangluSong,Han-JiaYe,YuLiu,andHongshengLi. Gen-l-video:
Multi-texttolongvideogenerationviatemporalco-denoising. arXivpreprintarXiv:2305.18264,
2023a.
Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang
Li. Boximator: Generating rich and controllable motions for video synthesis. arXiv preprint
arXiv:2402.01566,2024.
JiuniuWang,HangjieYuan,DayouChen,YingyaZhang,XiangWang,andShiweiZhang. Mod-
elscopetext-to-videotechnicalreport,2023b.
XiangWang,HangjieYuan,ShiweiZhang,DayouChen,JiuniuWang,YingyaZhang,YujunShen,
Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion
controllability. InAdvancesinNeuralInformationProcessingSystems,pp.7594–7611,2023c.
Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying
Shan. Motionctrl: Aunifiedandflexiblemotioncontrollerforvideogeneration. arXivpreprint
arXiv:2312.03641,2023d.
12YujieWei,ShiweiZhang,ZhiwuQing,HangjieYuan,ZhihengLiu,YuLiu,YingyaZhang,Jingren
Zhou,andHongmingShan. Dreamvideo: Composingyourdreamvideoswithcustomizedsubject
andmotion. arXivpreprintarXiv:2312.04433,2023.
JayZhangjieWu,YixiaoGe,XintaoWang,StanWeixianLei,YuchaoGu,YufeiShi,WynneHsu,
YingShan,XiaohuQie,andMikeZhengShou. Tune-a-video: One-shottuningofimagediffusion
modelsfortext-to-videogeneration. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pp.7623–7633,2023a.
RuiqiWu,LiangyuChen,TongYang,ChunleGuo,ChongyiLi,andXiangyuZhang. Lamp: Learna
motionpatternforfew-shot-basedvideogeneration. arXivpreprintarXiv:2310.10769,2023b.
TianxingWu,ChenyangSi,YumingJiang,ZiqiHuang,andZiweiLiu. Freeinit: Bridginginitializa-
tiongapinvideodiffusionmodels. arXivpreprintarXiv:2312.07537,2023c.
ShiyuanYang,LiangHou,HaibinHuang,ChongyangMa,PengfeiWan,DiZhang,XiaodongChen,
andJingLiao. Direct-a-video: Customizedvideogenerationwithuser-directedcameramovement
andobjectmotion. arXivpreprintarXiv:2402.03162,2024.
ShengmingYin,ChenfeiWu,JianLiang,JieShi,HouqiangLi,GongMing,andNanDuan.Dragnuwa:
Fine-grainedcontrolinvideogenerationbyintegratingtext,image,andtrajectory. arXivpreprint
arXiv:2308.08089,2023a.
ShengmingYin,ChenfeiWu,HuanYang,JianfengWang,XiaodongWang,MinhengNi,Zhengyuan
Yang,LinjieLi,ShuguangLiu,FanYang,etal. Nuwa-xl: Diffusionoverdiffusionforextremely
longvideogeneration. arXivpreprintarXiv:2303.12346,2023b.
MinZhao,RongzhenWang,FanBao,ChongxuanLi,andJunZhu.Controlvideo:Addingconditional
controlforoneshottext-to-videoediting. arXivpreprintarXiv:2305.17098,2023a.
RuiZhao,YuchaoGu,JayZhangjieWu,DavidJunhaoZhang,JiaweiLiu,WeijiaWu,JussiKeppo,
andMikeZhengShou. Motiondirector: Motioncustomizationoftext-to-videodiffusionmodels.
arXivpreprintarXiv:2310.08465,2023b.
136 Implementation
6.1 Hyperparameters
Duringsampling,weperformDDIMsampling(Songetal.,2020)with50denoisingsteps,setting
DDIMetato0. Theinferenceresolutionisfixedat320×512pixelsandthevideolengthis16frames
inthenormalsetting. Thevideolengthoflongerinferenceis64framesandtheinferenceresolution
oflargerinferenceis640×512pixels. Thescaleoftheclassifier-freeguidanceissetto12. αin
Equation9is 0.25 andβ inEquation11is0.01.
len_target_prompts×proportion_target_box
Forquantitativecomparison,wegenerateatotalof896videosforeachinferencemethod,utilizing
56prompts. Weinitialize16randominitialnoisesforeachpromptfordirectinference. Fortrajectory
controlmethods,eachpromptisappliedto8differenttrajectorieswith2randominitialnoises.
Intheuserstudy,wemixedourgeneratedvideoswiththosegeneratedbytheotherthreebaselines. A
totalof27userswereaskedtopickthebestoneaccordingtothetrajectoryalignment,video-text
alignment,andvideoquality,respectively.
6.2 Prompts
Ourpromptsaremostlyextendedfrompreviousbaselines(Jainetal.,2023a;Maetal.,2023a)but
replacesomepromptsthatconflictwithobjectmovement,likestandingorlying.
• Awoodpeckerclimbingupatreetrunk.
• Asquirreldescendingatreeaftergatheringnuts.
• Abirddivingtowardsthewatertocatchfish.
• Afrogleapinguptocatchafly.
• Aparrotflyingupwardstowardsthetreetops.
• Asquirreljumpingfromonetreetoanother.
• Arabbitburrowingdownwardsintoitswarren.
• AsatelliteorbitingEarthinouterspace.
• Askateboarderperformingtricksataskatepark.
• Aleaffallinggentlyfromatree.
• Apaperplaneglidingintheair.
• Abearclimbingdownatreeafterspottingathreat.
• Aduckdivingunderwaterinsearchoffood.
• Akangaroohoppingdownagentleslope.
• Anowlswoopingdownonitspreyduringthenight.
• Ahotairballoondriftingacrossaclearsky.
• Areddouble-deckerbusmovingthroughLondonstreets.
• Ajetplaneflyinghighinthesky.
• Ahelicopterhoveringaboveacityscape.
• Arollercoasterloopinginanamusementpark.
• Astreetcartrundlingdowntracksinahistoricdistrict.
• Arocketlaunchingintospacefromalaunchpad.
• Adeerwalkinginasnowyfield.
• Ahorsegrazinginameadow.
• Afoxrunninginaforestclearing.
• Aswanfloatinggracefullyonalake.
• Apandawalkingandmunchingbambooinabambooforest.
14“A corgi running on the grassland in the snow.”
“A dark knight riding a horse on the grassland.”
Figure 6: Longer video generation. Longer video generation allows us to plan some complex
trajectories. FreeTrajsucceedsingeneratingrichmotiontrajectoriesinlongvideos.
• Apenguinwalkingonaniceberg.
• Alionwalkinginthesavannagrass.
• Anowlflyinginatreeatnight.
• Adolphinjustbreakingtheoceansurface.
• Acamelwalkinginadesertlandscape.
• AkangaroojumpingintheAustralianoutback.
• Acolorfulhotairballoontetheredtotheground.
• Acorgirunningonthegrasslandonthegrassland.
• Acorgirunningonthegrasslandinthesnow.
• Amaningrayclothesrunninginthesummer.
• Aknightridingahorseonaracecourse.
• Ahorsegallopingonastreet.
• Alionrunningonthegrasslands.
• Adogrunningacrossthegarden,photorealistic,4k.
• Atigerwalkingintheforest,photorealistic,4k,highdefinition.
• IronMansurfingonthesea.
• Atigerrunningintheforest,photorealistic,4k,highdefinition.
• Ahorserunning,photorealistic,4k,volumetriclightingunrealengine.
• Apandasurfingintheuniverse.
• Achihuahuainanastronautsuitfloatingintheuniverse,cinematiclighting,gloweffect.
• Anastronautwavinghishandsonthemoon.
• Ahorsegallopingthroughameadow.
• Abearrunningintheruins,photorealistic,4k,highdefinition.
• Abarrelfloatinginariver.
• Adarkknightridingahorseonthegrassland.
• Awoodenboatmovingonthesea.
• Aredcarturningaroundonacountrysideroad,photorealistic,4k.
• Amajesticeaglesoaringhighabovethetreetops,surveyingitsterritory.
• Abaldeagleflyinginthebluesky.
15Direct Ours
“A fox running in a forest clearing.”
“A lion running on the grasslands.”
“A dark knight riding a horse on the grassland.”
Figure7: Largervideogeneration. Directlygeneratinglargervideoswilleasilyleadtotheresults
with duplicated main objects anywhere. FreeTraj plans the trajectory for the main object and
suppressestheduplicationphenomenon.
7 LongerandLargerVideoGeneration
7.1 RelatedWorkofLongVideoGeneration.
Longvideogenerationisachallengingbutimportantprobleminvideogeneration. TATs(Geetal.,
2022),longvideoGAN(Geetal.,2022),LVDM(Heetal.,2022),andflexiblediffusion(Harveyetal.,
2022)achievelongvideogenerationinsmalldomainsandwithouttextualguidance.Phenaki(Villegas
etal.,2022),NUWA-Infinity(Liangetal.,2022),NUWA-XL(Yinetal.,2023b),andSora(OpenAI,
2024) are text-guided long video generation approaches for open-domain generation. Animate-
A-Story (He et al., 2023) achieves multi-scene long video generation via character consistency.
Streamingt2v(Henscheletal.,2024)andFlexiFilm(Ouyangetal.,2024)aretraining-basedmethods
thattrainaconditionalmoduleontopofpre-trainedvideodiffusionmodelsconditioningonprevious-
frames. Genlvideo (Wang et al., 2023a) and FreeNoise (Qiu et al., 2023) are recently proposed
tuning-freemethodsforgeneratinglongervideosbasedonpre-trainedvideodiffusionmodelsto
extend their generated length. In this work, we propose a tuning-free approach for long video
generationbasedonlong-termtrajectorycontrol.
7.2 ResultsofLongerGeneration
FreeTrajcanbeintegratedintothelongervideogenerationframeworkFreeNoise(Qiuetal.,2023).
WiththehelpofsometechnicalpointsproposedbyFreeNoise,ourFreeTrajsuccessfullygenerated
trajectory-controllablelongvideos. AsshowninFigure6,weplantwocomplexpathsandFreeTraj
succeedsingeneratingrichmotiontrajectoriesinlongvideos.
7.3 ResultsofLargerGeneration
Whenwedirectlyusepre-trainedvideodiffusionmodelstogeneratevideoswithhigherresolutions
comparedtothoseintraining, theywilleasilygenerateresultswithduplicatedmainobjectsany-
whereHeetal.(2024).However,FreeTrajwillplanthetrajectoryforthemainobject,andinformation
aboutthemainobjectwillbereducedoutofthetargetareas. Therefore,theduplicationphenomenon
willbesuppressedbyFreeTraj(Figure7).
16Frame 1
Frame 8
Frame 15
Full 𝑧"#$% Resample 50% Resample 75% Resample 90% Resample 100%
!
Figure 8: Noise resampling of initial high-frequency components. Gradually increasing the
proportionofresampledhigh-frequencyinformationintheframe-wisesharednoisescansignificantly
reducetheartifactinthegeneratedvideo. However,thisalsoleadstoagraduallossintrajectory
controlability. Aresamplingpercentageof75%strikesabetterbalancebetweenmaintainingcontrol
andimprovingthequalityofthegeneratedvideo.
8 MoreObservations
8.1 MoreAboutNoiseFlow
Hereweshowanotherdirectionofnoiseflow. Insteadofrandomlysamplinginitialnoisesforall
frames,weonlysamplethenoiseforthefirstframe. Thenwemovethenoisefromthebottom-leftto
thetop-rightwithstride2andrepeatthisoperationuntilwegetinitialnoiseszflow forallframes.
T
Specially,initialnoiseϵforeachframef inposition[i,j]is:
ϵ[i,j]f =ϵ[(i+2)(modH),(j−2)(modW)]f−1. (14)
After denoising zflow, results in Figure 8 show that objects and textures in the video also flow
T
in the same direction (bottom-left to top-right). This phenomenon verifies that the trajectory of
the initial noises can have an impact on the motion trajectory of the generated result. When the
proportionofhigh-frequencynoiseresampledincreases,thevisualqualityissignificantlyimproved.
Correspondingly,theflowphenomenonisweakened.
8.2 AttentionIsolationinTemporalDimension
Usually,weinitialize16framesofrandomnoisesindependently. Insteadofnormalsampling,wetry
partialrepeatedsamplingbypartiallyrepeatingsomeinitialnoises:
NormalSampling: [ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ],
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
(15)
PartialRepeatedSampling: [ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ,ϵ ].
1 1 1 1 2 3 4 5 6 7 8 9 10 10 10 10
Sincespatio-temporalcorrelationsinthelow-frequencycomponentsofinitialnoiseswillguidethe
trajectoryofgeneratedobjects,partialrepeatedsamplingforinitialnoiseswillbringtypicalmotion
mode.AsshowninFigure9(b),theowlisstationaryinthebeginningandendingframesandonlyhas
significantactioninthemiddleframes. However,duetotheattentionisolation,framesofgenerated
resultshaveobviousartifacts.Wevisualizeoneheatmapoftemporalattentionandfindthatstationary
framesmainlypayattentiontoframeswiththesameinitialnoises. Whencalculatingtheattention
weightsreceivedbyisolatedframes,manuallysplittingaportionofattentionweightsfromisolated
framestootherframeswillremoveartifacts. AsshowninFigure9(c),anowliswellgeneratedand
itsmotionstillfitsthemodein(b).
17(a)
(b)
(c)
Frame 1 Frame 4 Frame 9 Frame 13 Frame 16
(a) (b) (c)
Figure9: Attentionisolationintemporaldimension. Comparedtonormalsamplingforinitial
noises (a), partial repeated sampling will lead to significant attention isolation in the temporal
dimensionandbringstrongartifacts(b). Whencalculatingtheattentionweightsreceivedbyisolated
frames,manuallysplittingaportionofattentionweightsfromisolatedframestootherframeswill
removeartifacts(c).
18