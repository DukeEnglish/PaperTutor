USDC: A Dataset of User Stance and Dogmatism in
Long Conversations
MounikaMarreddy1,SubbaReddyOota2,VenkataCharanChinni3,ManishGupta4,LucieFlek1
1UniversityofBonn,Germany,2TUBerlin,Germany,3Couture.ai,India,4Microsoft,Hyderabad,India
mmarredd@uni-bonn.de, subba-reddy.oota@inria.fr, venkatacharan635@gmail.com,
gmanish@microsoft.com, flek@bit.uni-bonn.de
Submission Title: Capitalism vs. Socialism
Submission Body: What are your thoughts on capitalism vs. socialism ? I feel socialism is
somewhat better. Which system do you think is better and why?
𝑎𝑎1
: I believe capitalism drives innovation and economic growth. It rewards hard work and creativity.
𝑎𝑎2 𝑐𝑐1 : But capitalism also leads to income inequality and can neglect social welfare.
𝑎𝑎1 𝑐𝑐2 : True, but socialism can stifle innovation & lead to inefficiencies. There's a balance to be found.
𝑎𝑎2 𝑐𝑐3 : Agreed. Capitalism inherently nurtures innovation and efficiency. Socialism often suffers from
bureaucratic red tape and lack of competition.
𝑐𝑐4
𝑎𝑎1
: A mixed economy that combines elements of both systems might be the best approach. What do
you all think?
𝑐𝑐5
𝑎𝑎3 : I appreciate your perspective, but now I feel that capitalism, when allowed to operate with
minimal interference, has consistently proven to drive innovation, efficiency, and economic growth.
𝑐𝑐6
𝑎𝑎1
Mistral Stro 𝐜𝐜𝟏𝟏ngly Som 𝐜𝐜e𝟐𝟐what Som 𝐜𝐜e𝟑𝟑what Som 𝐜𝐜e𝟒𝟒what Stan 𝐜𝐜c𝟓𝟓e Not Stro 𝒄𝒄n𝟔𝟔gly In Mistral Firm 𝒂𝒂𝟏𝟏 But Op 𝒂𝒂e𝟐𝟐n To Fle𝒂𝒂x𝟑𝟑ible
Large Against In Favor Against Against Inferrable Favor Large Open Dialogue
Strongly Somewhat Somewhat Strongly Stance Not Somewhat Firm But Open To
GPT4 GPT4 Flexible
Against In Favor Against Against Inferrable In Favor Open Dialogue
Figure1: SampleRedditconversationon“Capitalismvs. Socialism”withStance(foreverycomment
{c }6 ) and Dogmatism (for every author {a }3 ) labels from Mistral Large and GPT-4. The
i i=1 j j=1
submissioncontentfavorssocialismandexamineshowtheauthorspositiontheiropinionsregarding
socialismvs. capitalism.
Abstract
Identifyinguser’sopinionsandstancesinlongconversationthreadsonvarious
topics can be extremely critical for enhanced personalization, market research,
political campaigns, customer service, conflict resolution, targeted advertising,
andcontentmoderation. Hence,traininglanguagemodelstoautomatethistaskis
critical. However,totrainsuchmodels,gatheringmanualannotationshasmultiple
challenges: 1)Itistime-consumingandcostly;2)Conversationthreadscouldbe
verylong,increasingchancesofnoisyannotations;and3)Interpretinginstances
whereauserchangestheiropinionwithinaconversationisdifficultbecauseoften
such transitions are subtle and not expressed explicitly. Inspired by the recent
successoflargelanguagemodels(LLMs)forcomplexnaturallanguageprocess-
ing (NLP) tasks, we leverage Mistral Large and GPT-4 to automate the human
annotationprocessonthefollowingtwotaskswhilealsoprovidingreasoning: i)
UserStanceclassification,whichinvolveslabelingauser’sstanceofapostina
conversationonafive-pointscale;ii)UserDogmatismclassification,whichdeals
withlabelingauser’soverallopinionintheconversationonafour-pointscale. The
majorityvotingonzero-shot,one-shot,andfew-shotannotationsfromthesetwo
LLMson764multi-userRedditconversationshelpsuscuratetheUSDCdataset.
USDC is then used to finetune and instruction-tune multiple deployable small
Preprint.Underreview.
4202
nuJ
42
]LC.sc[
1v33861.6042:viXra
ecnatS
msitamgoDlanguagemodelsforthe5-classstanceand4-classdogmatismclassificationtasks.
Wemakethecodeanddatasetpubliclyavailable1.
.
1 Introduction
Understandingtheuser’s(orauthor’s)opinioninaconversationisafundamentalaspectofsuccessful
interpersonalinteractions,anditisessentialfordevelopingbetterinterpersonalcommunicationskills,
empathydevelopment,andinformeddecision-making.Thisuserunderstandingisparticularlyrelevant
in the context of dogmatism, a phenomenon observed in various areas such as politics, religion,
culture,intellect,andscience,whererigidadherencetobeliefsoftenhindersopen-mindednessand
empathy(Rokeach,1954). Advertiserscantargettheircampaignsmoreeffectivelybyaligningwith
the opinions and stances of potential customers. Companies can use this information for market
researchtotailorproductsandservicestomeetconsumerneedsandpreferences. Politicalgroupscan
gaugepublicreactiontopoliciesandcampaignsandadjusttheirstrategiesaccordingly. Identifying
differingopinionscanhelpconflictresolutionbyunderstandingallpartiesperspectives. Societycan
promotetoleranceandmaintainsocialharmonybyrecognizingandrespectingdiverseopinions.
Fig.1showsasampleRedditconversationonthetopicofCapitalismvs. Socialism. Werefertoan
author’sinitialpost(containingtitleandbody)asasubmission. Multipleauthorscanthensharetheir
opinionsascommentsonthesubmission. Specificallythisexamplecontains6comments{c }6
i i=1
from3authors{a }3 . Wealsoshowstanceanddogmatismpredictionsfromtwolargelanguage
j j=1
models(LLMs): MistralLargeandGPT-4. Someauthors,likea ,changetheirviewsduringthe
1
discussionbasedonthebeliefsoropinionsofothers. Atthebeginningofthedialogue,wenotethat
authora issomewhatfavoringsocialism(insubmissionandc ). Buttheauthorshiftstheirstance
1 2
tosomewhatfavorscapitalism(inc )afterconsideringtheviewpointsofauthora incommentsc
4 2 1
andc ,illustratingauthora ’sfirmyetopen-mindedapproach. Ontheotherhand,authora seems
3 1 3
veryflexiblebasedontheircommentc . Understandingconversationsrequiresunderstandingthe
5
fine-grainedtopicsbeingdiscussedandthedynamicviewpointsoftheindividualusers.
Given the importance of understanding these user dynamics in conversations, training language
modelstoperformthistaskautomaticallyatscaleiscritical. Whilenumerousdatasetsareavailable
foranalyzingindividualuserposts(Fast&Horvitz,2016;Sakketouetal.,2022),typicallythrough
randomsubsamplingofpostsorselectingpostswithalimitednumberoftokens,theexplorationofa
specificuser’sopinionacrosseachpostwithinanentireconversationalthreadremainsunder-explored.
Crowdsourcing is one possible approach to address the need for a suitable dataset. However, a
significantlimitationinmanuallyannotatingdatasetsforuseropinionsisthetime-consumingnature
oftheprocess,asannotatorsmustreadentireconversationstolabeleachuser’spost,makingdata
acquisitioncostly. Additionally,manualannotationoftenfaceschallengesrelatedtoquality,asaccu-
ratelylabelingopinionsrequiresunderstandingdemographicdetailsanddomain-specificknowledge.
Giventheselimitations,achievingacomprehensiveandaccuratesetofuseropinionscorresponding
topostsaboutatopicoftenrequiresmultipleannotatorsoriterativeroundsofannotation. Sinceusers
couldchangetheiropinion(oftenwithsubtletransitionsandnotwithexplicitstatements)withina
conversation,trackingsuchchangesacrossmultipleusersmanuallybecomesverycumbersome.
Recently,largelanguagemodels(LLMs),especiallythosebuiltonTransformerarchitectures(Vaswani
etal.,2017)andpretrainedonlargedatasets,haveresultedinstate-of-the-artaccuraciesonseveral
complexnaturallanguageprocessing(NLP)tasks(Brownetal.,2020;Chungetal.,2024). LLMsare
alsofrequentlyusedfordialogresponsegeneration(Zhangetal.,2020;Baoetal.,2019;Rolleretal.,
2021;Adiwardanaetal.,2020). Giventhecomplexandcumbersomenatureofconversationunder-
standing,wehypothesizethatLLMscaneffectivelycapturethenuancesinvolvedinunderstanding
useropinionsandtheirshiftsinmulti-userconversationalcontexts. Also,sincethesemodelspossess
long-rangememorycapabilities,webelievetheycanreasonoverextendedconversationalthreads
involvingnumerousparticipants,asgoodashumanannotators,ifnotbetter.
Inthiswork,weleverageLLMslikeMistralLargeandGPT-4toperformtwotasks: i)UserStance
classification, which involves labeling a user’s stance of a post in a conversation on a five-point
scale;ii)UserDogmatismclassification,whichdealswithlabelingauser’soverallopinioninthe
1https://anonymous.4open.science/r/USDC-0F7F
2Figure2:GeneratingannotationsusingLLMs:WepasstheentireconversationforeachRedditthread
inJSONformat. TheJSONhighlightsthetoptwoauthorswhopostedthemostcomments,alongside
annotationguidelinesforstanceanddogmatismlabelsinthesystemprompt.
conversationonafour-pointscale. Besidestheclasslabels,wealsoobtainreasoningbehindthese
labelsfromtheseLLMs. Weexperimentwiththesetwomodelsashuman-likeannotatorstogenerate
useropinionsinfull-length,multi-userRedditconversationsinazero-shot,one-shot,andfew-shot
setup.Thus,overall,foreverysample,weobtainsixannotations({MistralLarge,GPT-4}×{zero-shot,
one-shot,few-shot}). Fig.2presentsourLLM-basedannotationpipelineforuser-levelStanceand
Dogmatismtasks. Weconsidermajorityvotingoverthesesixasourfinalannotations. Overall,this
helpsuscurateourUSDC(adatasetofuserstanceanddogmatisminconversations)dataset,which
consistsof764multi-userconversationsfrom22subreddits,including1,528user-leveldogmatism
samplesand9,618stancesamplesacrossallpostsfromselectedusers. Overall,theannotationsin
thedatasethighlightspecificuseropinionsineachpostrelatedtostance,trackopinionfluctuations
leadingtoadogmaticnature,andprovidereasoningaboutwhyusersholdspecificopinions.
USDCaddressesseveralweaknessesofexistingpostlevelstanceanddogmatismdatasets. First,the
full-lengthmulti-userconversationaspectofUSDCenablesittocapturecontextualandopinionshifts
ofmultipleusers. Thisfeatureallowsittoserveasbothaninstruction-tuninguseropiniondatasetand
anevaluationbenchmark. Webelievethattheabilitytoperforminstruction-tuningforuseropinions
atalargescalecanbridgethegapbetweenopen-sourceandcommercialusertraitunderstanding
models. Additionally,thein-contextlearningannotationsusingstate-of-the-artLLMsinUSDCmake
itamorecomprehensivemeasureofhowcurrentLLMsunderstandcomplextaskslikecapturing
opinions. Thisaspectmakesitavaluableresource,especiallyforsocialmediaagentsseekingdeeper
insightsintouserbehavior.
In this work, we utilize our USDC dataset to finetune and instruction-tune open-source LLMs
forgeneratingstanceanddogmatismlabelsforusers. Weexperimentwiththreepretrainedsmall
languagemodels(SLMs)likeLLaMA-2-7B,LLaMA-3-8B,andFalcon-7B.Wealsoexperimentwith
fourinstruction-tunedSLMslikeLLaMA-2-chat-7B,LLaMA-3-8B-instruct,Vicuna-7B-v.1.5,and
Falcon-7B-instruct. WereportweightedF1-scoresobtainedusingthesemodelsforbothtasks.
Wemakethefollowingcontributions:1)WecontributeUSDC(adatasetofuserstanceanddogmatism
inconversations)datasetconsistingof764multi-userconversationslabeledwith1,528user-level
dogmatism samples and 9,618 stance samples. 2) We report initial results for the stance and
dogmatism detection tasks using seven small language models for the UDSC dataset. We find
thatstancedetectionperformanceimproveswithinstruction-tuning(F1-scoreof56.2)comparedto
finetuning(F1-scoreof54.9). However,dogmatismdetectionperformsworsewithinstruction-tuning
(F1-scoreof49.2)comparedtofinetuning(F1-scoreof51.4),highlightingthecomplexityofthis
task. 3)Wemakethecodeanddatasetpubliclyavailable1. Also,thefinetunedandinstruction-tuned
modelsaremadeavailableaswell.
32 RelatedWork
Opinionfluctuationsinuserconversations. Ourworkiscloselyrelatedtopreviousstudies(Fast
& Horvitz, 2016; Sakketou et al., 2022), which explore Stance and Dogmatism at the post level,
wherepostsarerandomlysampledfromconversationthreads. Fast&Horvitz(2016)predicteduser
dogmatismonrandomlysampledRedditpostsfromconversations,witheachpostlimitedto200-300
characters. Onemajorlimitationofthisworkistheunavailabilityofapublicdatasetandmissing
annotatordemographicdetails.Sakketouetal.(2022)createdthepostlevelStancedataset,SPINOS,
whereeachpostisconsideredindependent,andsubmissionpostsaremissingwhileannotatorslabel
thedata. Additionally,thequalityofthedatasetisnotvalidatedduetomissingdemographicdetails
oftheseannotators. OurworkovercomesthelimitationsofpreviousstudiesandpresentsStance
detectionforpostsandDogmatismlabelsofusersinconversations,consideringtheentirecontext
whilepreservingsubmissionIDs. Hence,ourdatasetprovidesclearuser-levelpostsanddogmatism
data,whichareusefulformodelingdynamicuserrepresentations.
GeneratingannotationsforNLPtasksusingLargeLanguageModelsOurworkalsorelatestoa
growingbodyofliteraturesuggestingthatlargelanguagemodelscanperformsimilarlytohuman
annotatorsinlabelingcomplexNLPtasks(Zhouetal.,2022;Zhangetal.,2023;Bansal&Sharma,
2023;Lowmanstoneetal.,2023;Wadhwaetal.,2023;Honovichetal.,2023;Zhengetal.,2024;Ye
etal.,2022a;Mengetal.,2022). SeveralstudieshaveexploredLLM-basedannotationgeneration
inzero-shotorfew-shottasksettings(Yeetal.,2022a;Mengetal.,2022;Yeetal.,2022b),while
othershavecomparedpairsoflanguagemodelstoassessthequalityofannotationsgeneratedby
theseLLMs(Zhengetal.,2024). However,thesestudiesfocusedongeneratingannotationsforNLP
taskssuchassentimentanalysis, naturallanguageinference(Gilardietal.,2023;Alizadehetal.,
2023), or creating synthetic dialogues, but only for dyadic conversations (Lee et al., 2023). Our
approachcomplementsthesepreviousstudiesbyfocusingongeneratingannotationsofuseropinions
incomplexmulti-userconversations.
3 USDCDatasetCuration
Inthissection,wewilldiscussthreemainthings: 1)CollectionofRedditconversations,2)Obtaining
LLMannotations,and3)Inter-annotatoragreementwithLLMsasannotators.
3.1 CollectionofRedditConversationThreads
Initialcrawl. Wecrawlayear(2019)worthofmulti-userconversationdatafrom22subredditsof
Reddit2usingprawAPI3.Thisdatasetincludessubmissionsandallassociatedusercomments. Each
submission,whichservesastheinitialmessageoftheconversation,containsatitleandcontentbody.
Thisisfollowedbycommentsandrepliestothesubmissionorothercomments. Overall,wecrawled
3,619Redditconversationsacrossthe22subreddits. AsampleRedditconversationisdisplayedin
Fig.1.
Quality filtering of conversations. Since submission content on Reddit can sometimes include
videos,weperformthefollowingfilteringsteps.1)Weonlyconsidersubmissionswherethecontentis
text. 2)Weremoveconversationswith[deleted]tagsandemptycontent. 3)Weexcludeconversations
wherethepostswerediscardedbyusersorremovedbymoderators.
Reddituserconversationscanbeverylong,andweobservedupto591commentsinasinglecrawled
conversationdata. Consideringthemaximumsequencelengthallowedbyvariouslanguagemodels,
weretainedonlythoseconversationsthatcontainatleast20and,atmost,70comments. Considering
conversationswithfewerthan20commentsresultsintoofewcommentstoaccuratelygaugeuser
opinionsbasedonsmallsamples. Further,weensurethatatleasttwouserscovering∼50%ofthe
commentsintheconversations. Wedidnotremoveanycommentsorreducethepostlengthinthe
selectedconversations. Outoftheinitial3,619conversations,thesefilteringstepsresultinto764
conversationsgettingselected. Table.4intheAppendixshowsdetailedsubredditlevelstatistics.
2https://www.reddit.com/
3https://github.com/praw-dev/praw
4Submission ID: drq2co Submission ID: e8ja1o
IDs: {'f6laqfs', 'f6mr52d', 'f6l9r75', 'f6mmzx1', 'f6mna88'} IDs: {'fad308g', 'fad7y5w', 'fad8t5b', 'fad33tu', 'fad2weo'}
IDs: {'drq2co', 'f6lijhv', 'f6li730', 'f6li2n3', 'f6liboo'} IDs: {'fadk1jm', 'fadjycs', 'fadk08d'}
𝑎𝑎1 𝑎𝑎1
𝑎𝑎2 'id': 'f6laqfs', 'label': 'somewhat_against' 𝑎𝑎2 'id': 'fad308g', 'label': 'somewhat_in_favor'
'id': 'f6mr52d', 'label': 'somewhat_against' 'id': 'fad7y5w', 'label': 'somewhat_in_favor’
'id': 'f6l9r75', 'label': 'somewhat_against' 'id': 'fad8t5b', 'label': 'somewhat_in_favor'
𝑎𝑎1 'id': 'f9mmzx1', 'label': 'stance_not_inferrable' 𝑎𝑎1 'id': 'fad33tu', 'label': 'strongly_against’
'id': 'f9mna88', 'label': 'stance_not_inferrable' 'id': 'fad2weo', 'label': 'somewhat_in_favor'
'id': 'drq2co', 'label': 'somewhat_against',
'id': 'f6lijhv', 'label': 'somewhat_against' 'id': 'fadk1jm', 'label': 'strongly_against'
'id': 'f6li730', 'label': 'stance_not_inferrable' 'id': 'fadjycs', 'label': 'strongly_against'
𝑎𝑎2 'id': 'f6li2n3', 'label': 'stance_not_inferrable' 𝑎𝑎2 'id': 'fadk08d', 'body': 'stance_not_inferrable'
'id': 'f6liboo', 'label': 'somewhat_against'
Figure3:FailurecasesofLLMs:MistralLargefew-shotoutput(left),theids(“f6mmzx1”,“f6mna88”)
weremismatchedwithgeneratedids(“f9mmzx1”,“f9mna88”),GPT-4zero-shotoutput(right),the
key“label”wasmismatchedwithgeneratedkey“body”.
3.2 ObtainingLLMAnnotations
RepresentingRedditconversationsinJSONformat.
Tocreatetheprompt,wefollowthenestedhierarchicalstructureofRedditconversationstomaintain
thecontext. Specifically,wemaintainaJSONstructureforeachconversation,whereeachauthorhas
theirpostIDs,andcommentsorrepliesareavailableinthebodysection. AnexampleofaReddit
conversationinJSONformatisprovidedinAppendixD.NotethattheJSONexplicitlyincludesthe
top-2authorswhopostedthemostcommentsintheconversation,andtheirrespectivepostIDs. Our
emphasisonthesetop-2users(covering47%postsoftotalpostsonaverage)aimedataccurately
assigningStanceandDogmatismlabels,acknowledgingthechallengeofmodelingauser’sopinion
beliefbasedonaverynumberofpostswithinaconversation.
UsingLLMsashuman-likeannotators. Toannotatetheposition(orStance)ofausertowardsa
subreddittopicateachpostandtheoverallopinion(orDogmatismlevel)ofauserinaconversation,
weemploytwowell-knowncommercializedAPI-basedLLMs: GPT-4(OpenAI,2023)andMistral
Large(Jiangetal.,2024). OpenAIGPT-4isadecoder-basedlanguagemodelwithacontextwindow
of32kto128ktokens. MistralLargefeaturesacontextwindowof32ktokens. Additionally,we
examinedotherversionsofthesemodels,suchasGPT-3.5andMistral-smallandmedium,butfound
that these models failed to produce annotations in the desired format. We briefly discuss these
limitationsinSection6.
For both GPT-4 and Mistral Large, we supplied a system prompt that contains the definition of
StanceandDogmatism,guidelinesforannotatingeachuserconversation,andthenecessarylabels
forStanceandDogmatism,asshowninFig2. ThesystempromptisdetailedintheAppendixB.
Alongwiththesystemprompt,weprovidedauserpromptcomprisingtheentireuserconversation
inastructuredJSONformat,asdiscussedabove. Additionally,wepromptedthemodeltogenerate
reasoning for each label, explaining why the LLMs assigned a particular label to a specific user
post. Weusedzero-shot, one-shot, andfew-shotsettingstogettheLLM-basedannotations. For
thefew-shotsetting, weaddedtwoexamplesintheprompt. Samplesofgeneratedoutputsusing
GPT-4inzero-shot,one-shot,andfew-shotsettingsareshowninAppendixE.1,E.2,E.3respectively.
Similarly,samplesofgeneratedoutputsusingMistralLargeinzero,one,andfew-shotsettingsare
showninAppendixE.4,E.5,E.6respectively.
Annotationtasks. WeprompttheLLMstoperformtwoannotationtasks: 1)Stancedetection,which
determinesifausercommentorpostisStronglyInFavor,StronglyAgainst,StanceNotInferrable,
SomewhatInFavor,orSomewhatAgainsttowardsspecificsubredditsubmissioncontent;2)Dog-
matismidentification, whichevaluatestheuser’soverallopinionin conversationandcategorizes
them into one of four categories: Firm but Open, Open to Dialogue, Flexible or Deeply Rooted.
Thisassessmentrevealswhetherusersareopentochangingtheirbeliefsorremainsteadfastintheir
opinionsbasedoninteractionswithotherusers.
AddressingLLMresponseandJSONparsingfailures. Sometimes,theLLMsgotconfusedwith
theauthorIDsandmissedStancelabelsforsomeauthorIDs(Fig.3(left)). Sometimes,therewere
5Somewhat In Favor Strongly Against
Stance Not Inferrable Strongly In Favor Open to Dialogue Deeply Rooted
Somewhat Against Firm but Open Flexible
Stance Dogmatism
3000
600
2500
500
2000
400
1500 300
1000 200
500 100
0 0
Figure4: DistributionofclasslabelsforStance(left)andDogmatism(right)tasks. Theseclasslabels
aredeterminedbymajorityvotingacrossGPT-4andMistralLargemodels.
minorerrorsinkeynaming(‘label’vs‘body’inFig.3(right)). Weobservedsucherrorsinaround
15casesforeachLLMsetting. WemanuallyfixedJSONparseerrorsandmissingStancelabelsfor
someauthorIDs.
Majorityvoting. Afterobtainingsixannotations({MistralLarge,GPT-4}×{zero,one,andfew-
shot})foreachsample,weaggregateusingmajorityvotingtodeterminethefinalgoldannotations
fortheStanceandDogmatismtasks. Fig.4presentstheclassdistributionsforbothannotationtasks.
Additionally, wepresenttheclassdistributionsobtainedfromeachmodelwiththethreesettings
(zero,one,andfew-shot)fortwotasksinAppendixFigs.5and6respectively.
3.3 Inter-annotatorAgreementwithLLMsasAnnotators
Asthequalityoflabelingonsubjectivetasksischallenging,wevalidatedtheinter-annotatoragree-
ment(IAA)betweenthesixLLMs(GPT-4Zero-shot,GPT-4One-shot,GPT-4Few-shot,Mistral
Large Zero-shot, Mistral Large One-shot, and Mistral Large Few-shot) for the Stance as well as
Dogmatismtasks. WeperformIAAusingtwoapproaches: i)Cohen’skappascore(Cohen,1960)
andii)Fleiss’kappascore(Fleiss,1971). Cohen’skappameasurestheagreementbetweentworaters,
whileFleiss’kappaextendsthistomultipleraters. Hence,weemployedCohen’skappaforpairwise
comparisonsandFleiss’kappaforoverallagreementacrossallmodels.
Fig.7intheAppendixshowsthepairwiseCohen’skappavaluesforbothStanceandDogmatism
tasks. WeobservethatCohen’skappavaluesrangefrom0.36to0.72forStanceand0.31to0.61for
dogmatism,indicatingmoderateagreementbetweenthemodels. Broadly,kappavaluesarehigherfor
modelpairswithinafamily(GPT-4orMistrallarge). Thus,thelargevarianceinthekappascores
is not due to the various in-context learning settings (ZS, OS, FS) but rather due to architectural
differences.
TheoverallFleiss’kappavaluewascalculatedas0.485forStanceand0.435fordogmatism,sug-
gestingmoderateagreementamongallsixmodels. ComparingLLMIAAwithpreviousstudies,
weobservethatfordogmatism,theLLMIAAof0.435matcheswith0.44asmentionedinFast&
Horvitz(2016). Similarly,forStance,theLLMIAAof0.485ismuchhigherthan0.34asreported
inSakketouetal.(2022). ItisimportanttonotethatpreviousstudiesonStanceandDogmatism
datasetswerecreatedonpostleveldatawithlimitedtokenlengths,whereasourworkfocuseson
entire user conversations. This suggests that LLMs can be considered as competent annotators
forcomplexsubjectivetasks. However,themoderateagreementlevelsindicatepotentialareasfor
improvementandalignwiththeobservedperformancevariationsamongthemodels.
4 TrainingSmallLanguageModels
In this section, we briefly discuss the small language models that we experiment with. We also
discusstheirfinetuningandinstruction-tuningdetails.
6
ycneuqerF ycneuqerF4.1 SmallLanguageModels
Wetrainthreepretrainedsmalllanguagemodels(LLaMA-2-7B,LLaMA-3-8B,Falcon-7B)andfour
instruction-tunedsmalllanguagemodels(LLaMA-2-chat-7B,LLaMA-3-8B-instruct,Vicuna-7B-
v.1.5,andFalcon-7B-instruct). Wefinetuneandinstruction-tunethesemodelsusingtheproposed
USDCdataset. WeusepretrainedmodelcheckpointsfromHuggingFace. AlloftheseLLMshavea
contextlengthof4096tokens.
LLaMAmodels(Touvronetal.,2023a)aredecoder-onlyLLMstrainedon1.6trilliontokensfroma
mixtureofcorporaincludingC4,EnglishCommonCrawl,Wikipedia,Github,andmore. Weusetwo
versionsofmodelsinourstudy: LLaMa-2-7B(Touvronetal.,2023b)andLLaMa-3-8Bandtheir
instruction-tunedvariants.
Falcon models (Almazrouei et al., 2023) are decoder-only LLMs trained on ≥ 1 trillion tokens
of text, particularly emphasizing the RefinedWeb corpus. For Falcon, we use the pretrained and
instruction-tuned7Bparametervariantsinourstudy.
Vicunamodel(Chiangetal.,2023)isfinetunedfromtheLLaMA7Bmodelonapproximately70K
user-sharedconversationsgatheredfromShareGPT.comandweusedthe7Bparametervariants.
4.2 ExperimentalSetup
Train-testsetup.Weconductedbothfinetuningandinstruction-tuningofsmalllanguagemodels.For
thispurpose,wedividedthedatasetof764conversationsintotrain(∼75%)andtestsplits(∼25%).
Thetrainingdatasetcomprised564conversations,including1128samplesofdogmatismlabelsand
7520samplesofstancelabels. Conversely,thetestingdatasetconsistedof200conversations,with
400samplesofdogmatismlabelsand1831samplesofstancelabelsacrosstwoauthor’sposts.
Implementationdetailsforreproducibility.Allexperimentswereconductedonamachineequipped
with an NVIDIA A100 GPU with 80 GB of GPU RAM, partitioned into two devices of 40 GB
each. Weemployed4-bitquantizationwithnormalizedfloatingprecision(nf4)fromthebitsandbytes
library4. Additionally,weutilizedLoRA(Huetal.,2021)witharankof64andanalphavalueof16
duringtask-basedinstruction-tuning. Finally,weusePEFT(ParameterEfficientFinetuning)5library
totrainlargelanguagemodelswiththeSFTT(SupervisedFinetuningTrainer)setting. Tofurther
enhanceperformance,wedividedthetrainingdatasetintoavalidationsetcomprisingarandomly
chosen10%subsetfromthetrainingset,usedexclusivelyforhyperparametertuning. Moredetails
aboutbitsandbytes,PEFT,andSFTTparametersarereportedinAppendix.
4.3 FinetuningandInstruction-tuningofSmallLanguageModels(SLMs)
FinetuningofSLMs. ForStanceclassification,wetreateachuserpostasanindependentsample. In
contrast,forthedogmatismclassification,weconsidertheentireuserconversationasasinglesample
byconcatenatingallthethreadsfromauserinthatconversation. ToloadthepretrainedSLMs,we
perform4-bitquantization,applytheLoRAtechnique(Huetal.,2021),andfinetunethemodelswith
SFTTbeforesavingthefinetunedmodel. Forfinetuning,weusedpromptforStanceclassificationas
showninFig.8(seeAppendix). Similarly,Fig.9(seeAppendix)displayspromptforDogmatism
identification.
Instruction-tuningofSLMs. Weinstruction-tunetheSLMsonuserconversationsalongwiththeir
gold labels from the training part of the USDC dataset. For instruction-tuning, we use the same
promptasusedforLLMstogeneratetheUSDCdataset(alsoshowninAppendixB).Similarto
finetuning,weusesametrain-testsplitsforinstruction-tuning.
4https://pypi.org/project/bitsandbytes/
5https://github.com/huggingface/peft
7Table1: Finetuning: weightedF1-scoreforStanceclassificationusingSLMsonUSDCtestset. ZS:
Zero-shot,OS:One-shot,FS:Few-shot.
StanceClassification DogmatismClassification
GPT-4 MistralLarge GPT-4 MistralLarge
Model Majority Majority
ZS OS FS ZS OS FS ZS OS FS ZS OS FS
LLaMA-2-7B 51.8 52.9 52.7 35.1 49.2 46.0 54.0 42.1 44.2 45.2 39.3 47.6 43.7 43.4
LLaMA-2-chat-7B 52.8 51.4 51.8 34.7 47.5 46.5 51.3 42.1 42.5 48.8 41.1 49.7 45.5 48.3
LLaMA-3-8B 51.3 52.2 52.9 34.9 48.5 47.0 54.9 42.0 47.8 45.3 39.9 47.4 36.3 51.4
LLaMA-3-8B-instruct 51.2 52.6 52.7 33.9 49.5 45.6 54.5 44.8 46.2 49.7 46.1 45.8 46.1 50.8
Falcon-7B 50.7 51.1 51.6 34.9 47.2 43.9 53.2 41.5 42.1 43.3 36.5 38.4 37.5 40.1
Falcon-7B-instruct 51.2 51.5 51.6 35.1 47.7 44.2 51.0 41.7 42.1 42.9 36.8 38.5 36.9 39.7
Vicuna-7B-v.1.5 51.0 53.0 53.2 35.1 48.5 45.8 54.7 42.9 48.3 40.8 45.9 42.6 46.2 42.3
Table2: Instruction-tuning: weightedF1-scoreforStanceclassificationusingSLMsonUSDCtest
set. ZS:Zero-shot,OS:One-shot,FS:Few-shot.
StanceClassification DogmatismClassification
GPT-4 MistralLarge GPT-4 MistralLarge
Model Majority Majority
ZS OS FS ZS OS FS ZS OS FS ZS OS FS
LLaMA-2-7B 53.2 54.0 54.5 36.8 50.3 47.2 55.5 43.0 45.0 46.3 40.6 48.2 45.0 44.0
LLaMA-2-chat-7B 54.0 54.5 55.0 36.5 50.7 47.6 54.0 43.2 45.5 47.0 40.8 48.5 45.5 43.8
LLaMA-3-8B 53.5 54.8 55.5 37.0 50.5 48.0 56.2 43.5 46.0 47.5 41.0 48.8 45.8 45.1
LLaMA-3-8B-instruct 53.0 54.2 55.0 36.0 50.0 47.0 55.5 43.8 46.5 47.8 41.5 49.2 46.0 44.8
Falcon-7B 52.8 53.4 54.0 36.5 49.5 46.5 54.8 42.5 44.6 45.8 39.8 47.0 44.0 43.8
Falcon-7B-instruct 53.0 53.8 54.2 36.8 49.8 46.8 54.5 42.8 44.8 46.0 40.0 47.2 44.2 43.0
Vicuna-7B-v.1.5 53.3 54.5 55.2 37.0 50.2 47.8 55.2 43.7 46.8 47.2 41.2 48.2 46.5 44.8
5 Results
5.1 DoSLMsfinetunedwithtask-specificLLMannotationsaccuratelyperformStanceand
Dogmatismtasksonuseropinions?
WeshowtheweightedF1ofvariousSLMsfinetunedwithtask-specificLLMannotationsonthestance
anddogmatismdetectiontasksontheUSDCtestsetinTable1. WereportAUCscoresandother
qualitativeanalysisinAppendixF(Fig.11and 12). Wemakethefollowingobservationsfromthese
results: 1)Forbothtasks,themajorityvotinglabelsasgroundtruthhasarelativelyhighperformance,
scoringabove50%weightedF1-scoreacrossseveralmodels. 2)LLaMa-3models(LLaMA-3-8B
andLLaMA-3-8B-instruct)performbetteracrossboththetasks. 3)ForGPT-4annotations,inmost
cases,SLMsfinetunedwithfew-shotannotationsoutperformthosetrainedwithzeroandone-shot
annotations. For Mistral Large annotations, SLMs finetuned with one-shot annotations perform
the best. 4) Specifically, for the stance detection task, Vicuna-7B-v.1.5 finetuned using few-shot
annotations is the best model trained with GPT-4 annotations. Similarly, LLaMA-3-8B-instruct
finetunedwithone-shotannotationsisthebestmodeltrainedwithMistralLargeannotations. 5)For
the dogmatismdetection task, LLaMA-3-8B-instruct finetunedusing few-shot annotations isthe
bestmodeltrainedwithGPT-4annotations. Similarly,LLaMA-2-chat-7Bfinetunedwithone-shot
annotationsisthebestmodeltrainedwithMistralLargeannotations. 6)Overall,weobservethat
instruction-tunedSLMsperformbetterthanthepretrainedSLMs.
5.2 DoSLMsinstruction-tunedwithtask-specificLLMannotationsperformbetterthan
SLMsfinetunedwithtask-specificLLMannotationsfortheStanceandDogmatismtasks?
WeshowtheweightedF1ofvariousSLMsinstruction-tunedwithtask-specificLLMannotationson
thestanceanddogmatismdetectiontasksontheUSDCtestsetinTable2. WereportAUCscores
andotherqualitativeanalysisinAppendixF(seeFig.13). Wemakethefollowingobservationsfrom
theseresults: 1)SLMswithinstruction-tuningresultinhigherweightedF1-scoresthanSLMswith
finetuningforstancedetection,whileSLMswithfinetuningoutperformSLMswithinstruction-tuning
indogmatismdetection. 2)ContrarytoresultsinTable1,Table2demonstratesthatusingmajority
votinglabelsasgroundtruth,SLMinstruction-tuningyieldsrelativelyhighperformanceonlyforthe
stancedetectiontask,butnotforthedogmatismdetection. 3)SimilartoresultsinTable1,LLaMA-3
models(LLaMA-3-8BandLLaMA-3-8B-instruct)performbetteracrossbothtasks. Additionally,
GPT-4annotations yieldthe best results in thefew-shotsetting, while MistralLargeannotations
performbestintheone-shotsetting.
Overall,wedrawthefollowingconclusionswhencomparingSLMfinetuningandinstruction-tuning:
(1) Since dogmatism detection is inherently a more complex and varied than stance detection,
the model might struggle to generalize from the instructional data. (2) The system prompt used
in finetuning is much simpler than the original system prompt for instruction-tuning, making it
8Table3: ConfusionmatrixforLLaMa-3-8BStancedetectionmodelsonUSDCtestset: finetuning
(left)andinstruction-tuning(right). SOA:SomewhatAgainst,SOIF:SomewhatInFavor,SNI:Stance
NotInferrable,SGA:StronglyAgainst,SIF:StronglyInFavor.
Predicted Predicted
SOA SOIF SNI SGA SIF SOA SOIF SNI SGA SIF
SOA 151 132 34 44 2 SOA 143 125 37 54 4
SOIF 93 537 113 17 14 SOIF 82 543 106 27 16
Actual SNI 23 78 259 5 0 Actual SNI 22 82 253 6 2
SGA 52 35 13 115 17 SGA 41 35 11 131 14
SIF 18 50 12 25 27 SIF 16 53 10 23 30
challengingtohandlethecontextlengthforlongerconversations. Weperformanerroranalysisto
furtheranalyzetheresultsinthenextsubsection.
5.3 ErrorAnalysis
Table 3 illustrates the confusion matrix for stance detection for LLaMa-3-8B finetuning and
instruction-tuning. Wemakethefollowingobservationsinthistable: 1)Forbothfinetuningand
instruction-tuning,thereisasignificantmisclassificationbetween“SomewhatAgainst”and“Some-
whatInFavor,”aswellasbetween“SomewhatInFavor”and“StanceNotInferrable.” Theseoverlaps
suggestchallengesdistinguishingmoderatestances,indicatinganeedforenhancedfeaturerepresen-
tationandclearerclassdefinitionstoimprovemodelperformance. Wereporttheconfusionmatrixfor
dogmatismdetectiontaskinAppendixFig.10.Fig.10showssignificantmisclassifications,especially
for the “Deeply Rooted” and “Flexible” labels, with zero accuracy and F1-scores. On the other
hand,themodelperformsmoderatelybetterfor“FirmbutOpen”and“OpentoDialogue”classes
withaccuraciesof48.7%and64.4%respectively. Theconfusionmatrixalsoindicatessubstantial
confusiontodistinguishbetweenintermediatelevelsofdogmatism,suchas“FirmbutOpen”and
“OpentoDialogue”.TheareaundertheROCcurve(AUC)measuresthemodel’sabilitytodistinguish
betweenclasses. Hence,wefurtherreporttheROCcurve,whichshowsthetrade-offbetweenthe
truepositiverate(TPR)andfalsepositiverate(FPR)foreachclassforstanceanddogmatismtasks,
seeFigs.11and.12inAppendixF.
5.4 VerificationusingHumanInteraction
Duetothetime-consumingnatureofthemanualannotationprocess,weperformhumanannotations
onasetof200testconversations. Intheformsforhumanannotations,wedisplayedthetop2author’s
Redditpostsfromtheconversation,alongwiththesubmissiontitleandcontent. Wealsoprovideda
linktotheoriginalRedditURLsothatannotatorscouldlookatthefullconversation. Weprovided
detailedannotationguidelines(similartotheonesmentionedinthepromptinAppendixB)toinstruct
humanannotatorsincarryingoutthesetasks. HereisasampleGoogleform6. Withthreehuman
annotatorsonasampleof10conversations,theagreementofmajoritylabels(i.e.,USDCtestset
labels)withhumanlabelsis0.56forthestancedetectiontaskand0.45forthedogmatismtask. The
annotatorsincludedtwomalesandonefemale,affiliatedwithacademiaandindustry,agedbetween
20and40,whowereveryfamiliarwithReddittopics.
5.5 StanceDetectionEvaluationonSPINOSDataset
ToevaluatethequalityofLLMgeneratedannotations,weperformtransferlearningbyfinetuning
theSLMsontheUSDCdataset. Wethentestthemodel’sperformanceontheSPINOSdatasetfor
a5-classStancedetectiontask,asdescribedbySakketouetal.(2022). Weusethesametraining
datasetmentionedinSection4.2. Fortesting,weusetheSPINOSdataset,whichconsistsof3,238
postlevelexamplesacrossfivestancelabels.
Fig.14inAppendixHillustratestheconfusionmatrixforstancedetectionforLLaMa-3-8Bfinetuning
onUSDCandtestingonSPINOS.WemakethefollowingobservationsfromFig.14: 1)Thereisa
significantmisclassificationacrossallclasses,withthe“StanceNotInferrable”labelbeingthemost
commonlypredictedclass,resultinginmanyfalsepositivesforthislabel. 2)Themodelperforms
bestintermsofaccuracyforthreestanceclasses: “SomewhatInFavor”(0.456),“StronglyAgainst”
(0.400),and“SomewhatAgainst”(0.381),whileperformingtheworstforthe“StronglyInFavor”
6https://forms.gle/dbPQBsNyfNJjvUeR9
9stance(0.115). Theseoverlapssuggestchallengesindistinguishingwhetherapostcontainsstanceor
not,indicatinganeedforenhancedfeaturerepresentationandclearerclassdefinitionstoimprove
modelperformance.
IncomparisontotheSPINOSdatasetresultsreportedinthepaperbySakketouetal.(2022),where
thebestmodel(traditional machinelearningclassifier)achievedanF1-scoreof0.341, arandom
baselineachieved0.230,andamajoritybaselineachieved0.124. OurapproachusingLLaMa-3-8B
finetuningontheUSDCdatasetachievedaweightedF1-scoreof0.320onSPINOS.Thisscoreis
close to the best model performance on the SPINOS dataset, indicating that our LLM-generated
annotationsontheUSDCdatasetarecloseinqualitytohumanannotations. Itisimportanttonote
thatourweightedF1-scoreissignificantlyimpactedbythe“StanceNotInferrable”class, which
comprisesthemajorityofsamplesintheSPINOSdataset. OurfinetunedSLMstruggledtoclassify
thisclassaccurately,leadingtoaloweroverallweightedF1-score.
WealsovalidatedtheSPINOSperformanceusingotherSLMssuchasLLaMa-3-8B-Instruct,LLaMa-
2-7B,LLaMa-2-7B-Chat,andVicuna-7Bmodels. Figs.15,16,17and 18inAppendixHdisplay
thesemodelresults. ThesefiguresindicatethatthesemodelsreportweightedF1-scoresof0.320,
0.305, 0.286, and 0.291 respectively. These results show that all models perform better than the
randomandmajoritybaselines. Additionally,theLLaMa-3-8B-Instructmodel’sperformanceisclose
totheSPINOSbenchmarkonthe5-classstancedetectiontask.
Inconclusion,theresultsindicatethatLLM-generatedannotationsoftheUSDCdatasetareaviable
alternativetohumanlabelsforstancedetectiontasks, demonstratingthesubstantialpotentialfor
automatingandscalingupsuchcomplexannotationprocessesinlonguserconversationdata.
6 Discussion&Conclusion
In this paper, using LLMs as human-like annotators, we introduced USDC, a large-scale dataset
ofuserstanceanddogmatisminconversations. Thisisachievedbyprovidingdetailedannotation
guidelines in the system prompt and full-length conversation as a user prompt. Commercialized
API-based LLMs generate author-level stance and dogmatism labels via zero, one, and few-shot
settings. Thefull-lengthmulti-userconversationaspectofUSDCallowsittocapturethecontextual
andopinionshiftsofmultipleusersinaconversation.Webelievethattheabilitytoperformfinetuning
orinstruction-tuningSLMsforuseropinionsatalargescalecanbridgethegapbetweenSLMsand
commercialLLMsforunderstandingusertraits. WhilefinetuningSLMsshowsgoodF1-scoreon
bothstanceanddogmatismtasks,theF1-scoreremainsbelow60%(54.9%forstanceand51.4%for
dogmatism). Ontheotherhand,instruction-tuningofSLMsonlyimprovesF1-scoreperformance
onstance,notthedogmatismtask. Further,theperformancestillfallsshortof60%,withweighted
F1-scoresof56.2%forstanceand49.2%fordogmatism. Thesefindingsindicatethatthereisstill
significantroomforimprovementinunderstandinguseropinionsfromatextsegment.
Ourfindingsoncross-modalstancedetection(i.e.,finetuningtheSLMsontheUSDCdatasetand
testingonSPINOS)indicatethatLLM-generatedannotationsoftheUSDCdatasetareclosetohuman
labels for stance detection tasks. In the future, leveraging user-LLMs (Ning et al., 2024) in user
conversationsmayenableustocapturedynamicuserembeddingswithcontextualizedLLMsbased
onchangesinopinions.
Limitations. We plan to extend this work along the following directions in the future. 1) We
performedthisworkonEnglishconversationsonly. Itwouldbenicetoextendthistomulti-lingual
conversationsandverifyhowaccuratelySLMsandLLMsperformonthestanceanddogmatismtasks
inthemulti-lingualscenario. 2)Weanalyzeduserdogmatismbasedontheirpostswithinasingle
conversation. Thisapproachcouldbeextendedtoincludepostsacrossmultipleconversationsand
utilizesimilarprofileinformationifavailable. 3)Weanalyzeddogmatisminformationforonlythe
toptwoauthors. Thiswasmainlybecauseconsideringmoreauthorsincreasestheoutputgeneration
length,andwewereconstrainedbyourbudget. Thisimpliesthatourcurrentmodelshavenotbeen
evaluatedforauthorswhodonotpostfrequently.
References
DanielAdiwardana,Minh-ThangLuong,DavidRSo,JamieHall,NoahFiedel,RomalThoppilan,
ZiYang,ApoorvKulshreshtha,GauravNemade,YifengLu,etal. Towardsahuman-likeopen-
10domainchatbot. arXivpreprintarXiv:2001.09977,2020.
MeysamAlizadeh,MaëlKubli,ZeynabSamei,ShirinDehghani,JuanDiegoBermeo,MariaKo-
robeynikova,andFabrizioGilardi. Open-sourcelargelanguagemodelsoutperformcrowdworkers
andapproachchatgptintext-annotationtasks. arXivpreprintarXiv:2307.02179,2023.
EbtesamAlmazrouei,HamzaAlobeidli,AbdulazizAlshamsi,AlessandroCappelli,RuxandraCojo-
caru,MérouaneDebbah,ÉtienneGoffinet,DanielHesslow,JulienLaunay,QuentinMalartic,etal.
Thefalconseriesofopenlanguagemodels. arXivpreprintarXiv:2311.16867,2023.
ParikshitBansalandAmitSharma. Largelanguagemodelsasannotators: Enhancinggeneralization
ofnlpmodelsatminimalcost. arXivpreprintarXiv:2306.15766,2023.
SiqiBao,HuangHe,FanWang,HuaWu,andHaifengWang. Plato: Pre-traineddialoguegeneration
modelwithdiscretelatentvariable. arXivpreprintarXiv:1910.07931,2019.
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:
Anopen-sourcechatbotimpressinggpt-4with90%*chatgptquality,March2023. URLhttps:
//lmsys.org/blog/2023-03-30-vicuna/.
HyungWonChung,LeHou,ShayneLongpre,BarretZoph,YiTay,WilliamFedus,YunxuanLi,
XuezhiWang,MostafaDehghani,SiddharthaBrahma,etal.Scalinginstruction-finetunedlanguage
models. JournalofMachineLearningResearch,25(70):1–53,2024.
Jacob Cohen. A coefficient of agreement for nominal scales. Educational and psychological
measurement,20(1):37–46,1960.
Ethan Fast and Eric Horvitz. Identifying dogmatism in social media: Signals and models. In
Proceedingsofthe2016ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.
690–699,2016.
JosephLFleiss. Measuringnominalscaleagreementamongmanyraters. Psychologicalbulletin,76
(5):378,1971.
Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. Chatgpt outperforms crowd workers for
text-annotationtasks. ProceedingsoftheNationalAcademyofSciences,120(30):e2305016120,
2023.
Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning
languagemodelswith(almost)nohumanlabor. InProceedingsofthe61stAnnualMeetingofthe
AssociationforComputationalLinguistics(Volume1: LongPapers),pp.14409–14428,2023.
EdwardJHu,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,WeizhuChen,
et al. Lora: Low-rank adaptation of large language models. In International Conference on
LearningRepresentations,2021.
Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris
Bamford,DevendraSinghChaplot,DiegodelasCasas,EmmaBouHanna,FlorianBressand,etal.
Mixtralofexperts. arXivpreprintarXiv:2401.04088,2024.
Dong-HoLee,JayPujara, MohitSewak, RyenWhite,andSujayJauhar. Makinglargelanguage
models better data creators. In Proceedings of the 2023 Conference on Empirical Methods in
NaturalLanguageProcessing,pp.15349–15360,2023.
LondonLowmanstone,RuyuanWan,RisakoOwan,JaehyungKim,andDongyeopKang. Annota-
tionimputationtoindividualizepredictions: Initialstudiesondistributiondynamicsandmodel
predictions. arXivpreprintarXiv:2305.15070,2023.
11YuMeng,JiaxinHuang,YuZhang,andJiaweiHan. Generatingtrainingdatawithlanguagemodels:
Towardszero-shotlanguageunderstanding. AdvancesinNeuralInformationProcessingSystems,
35:462–477,2022.
LinNing,LuyangLiu,JiaxingWu,NeoWu,DevoraBerlowitz,SushantPrakash,BradleyGreen,
ShawnO’Banion,andJunXie. User-llm: Efficientllmcontextualizationwithuserembeddings.
arXivpreprintarXiv:2402.13598,2024.
ROpenAI. Gpt-4technicalreport.arxiv2303.08774. ViewinArticle,2(5),2023.
MiltonRokeach. Thenatureandmeaningofdogmatism. PsychologicalReview,61(3),1954.
StephenRoller,EmilyDinan,NamanGoyal,DaJu,MaryWilliamson,YinhanLiu,JingXu,Myle
Ott,EricMichaelSmith,Y-LanBoureau,andJasonWeston. Recipesforbuildinganopen-domain
chatbot.InProceedingsofthe16thConferenceoftheEuropeanChapteroftheAssociationforCom-
putationalLinguistics: MainVolume,pp.300–325,Online,2021.AssociationforComputational
Linguistics.
Flora Sakketou, Allison Lahnala, Liane Vogel, and Lucie Flek. Investigating user radicaliza-
tion: A novel dataset for identifying fine-grained temporal shifts in opinion. arXiv preprint
arXiv:2204.10190,2022.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timothée
Lacroix, BaptisteRozière, NamanGoyal, EricHambro, FaisalAzhar, etal. Llama: Openand
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023a.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundation
andfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023b.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformationprocessing
systems,30,2017.
ManyaWadhwa,JifanChen,JunyiJessyLi,andGregDurrett. Usingnaturallanguageexplanations
torescalehumanjudgments. arXivpreprintarXiv:2305.14770,2023.
JiachengYe,JiahuiGao,QintongLi,HangXu,JiangtaoFeng,ZhiyongWu,TaoYu,andLingpeng
Kong. Zerogen: Efficientzero-shotlearningviadatasetgeneration. InProceedingsofthe2022
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.11653–11669,2022a.
Jiacheng Ye, Jiahui Gao, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. Progen:
Progressivezero-shotdatasetgenerationviain-contextfeedback. InFindingsoftheAssociation
forComputationalLinguistics: EMNLP2022,pp.3671–3683,2022b.
RuoyuZhang,YanzengLi,YongliangMa,MingZhou,andLeiZou. Llmaaa: Makinglargelanguage
modelsasactiveannotators. arXivpreprintarXiv:2310.19596,2023.
Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng
Gao, Jingjing Liu, and Bill Dolan. DIALOGPT : Large-scale generative pre-training for con-
versational response generation. In Proceedings of the 58th Annual Meeting of the Associa-
tion for Computational Linguistics: System Demonstrations, pp. 270–278, Online, 2020. As-
sociation for Computational Linguistics. doi: 10.18653/v1/2020.acl-demos.30. URL https:
//aclanthology.org/2020.acl-demos.30.
LianminZheng,Wei-LinChiang,YingSheng,SiyuanZhuang,ZhanghaoWu,YonghaoZhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and
chatbotarena. AdvancesinNeuralInformationProcessingSystems,36,2024.
Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,
and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint
arXiv:2211.01910,2022.
12A DetailedStatisticsoftheUSDCDataset
Table4showsthedetailedstatisticsofourUSDCdatasetatthesubredditlevel. Fig.5showsthe
distributionofstancelabelsacrossLLMannotationsacrosszero-shot,one-shot,andfew-shotsettings.
Fig.6showsthedistributionofdogmatismlabelsacrossLLMannotationsacrosszero-shot,one-shot,
andfew-shotsettings.
Table4: StatisticsoftheUserConversationDataset.
subreddit num_conversations min_total_token_count max_total_token_count
DebateCommunism 73 529 11557
Abortiondebate 70 1271 7401
CapitalismVSocialism 61 665 16927
prochoice 60 582 7278
brexit 56 637 4553
climateskeptics 56 734 7550
prolife 54 672 13342
gunpolitics 52 683 7889
MensRights 52 623 5774
climatechange 49 520 7427
nuclear 41 572 5282
progun 39 436 3632
NuclearPower 23 629 4589
Vegetarianism 22 627 3958
AntiVegan 20 351 5052
climate 13 701 4678
Egalitarianism 10 665 4060
VeganActivism 8 460 3685
Veganism 2 1332 1738
AnimalRights 1 845 845
animalwelfare 1 1363 1363
GunsAreCool 1 2945 2945
GPT-4: Zero-shot GPT-4: One-shot GPT-4: Few-shot
3500 3000
3500
3000 3000 2500
2500 2500 2000
2000 2000
1500
1500 1500
1000
1000 1000
500 500 500
0 0 0
Mistral Large: Zero-shot Mistral Large: One-shot Mistral Large: Few-shot
2000 2500 2500
2000 2000
1500
1500 1500
1000
1000 1000
500
500 500
0 0 0
Figure5: DistributionofStancelabelsacrossLLMannotations.
13
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerFGPT-4: Zero-shot GPT-4: One-shot GPT-4: Few-shot
700 700
600
600 600
500 500 500
400 400 400
300 300 300
200 200 200
100 100 100
0 0 0
Mistral Large: Zero-shot Mistral Large: One-shot Mistral Large: Few-shot
700
700 700
600
600 600
500 500 500
400 400 400
300 300 300
200 200 200
100 100 100
0 0 0
Figure6: DistributionofdogmatismlabelsacrossLLMannotations.
Inter-Annotator Agreement (Cohen's Kappa) Inter-Annotator Agreement (Cohen's Kappa)
1.0 1.0
GPT-4: ZS 1.00 0.65 0.60 0.38 0.41 0.41 GPT-4: ZS 1.00 0.55 0.51 0.37 0.45 0.43
0.8 0.8
GPT-4: OS 0.65 1.00 0.65 0.36 0.42 0.40 GPT-4: OS 0.55 1.00 0.61 0.35 0.41 0.38
GPT-4: FS 0.60 0.65 1.00 0.40 0.47 0.47 0.6 GPT-4: FS 0.51 0.61 1.00 0.31 0.36 0.34 0.6
Mistral Large: ZS 0.38 0.36 0.40 1.00 0.53 0.58 0.4 Mistral Large: ZS 0.37 0.35 0.31 1.00 0.46 0.44 0.4
Mistral Large: OS 0.41 0.42 0.47 0.53 1.00 0.72 Mistral Large: OS 0.45 0.41 0.36 0.46 1.00 0.59
0.2 0.2
Mistral Large: FS 0.41 0.40 0.47 0.58 0.72 1.00 Mistral Large: FS 0.43 0.38 0.34 0.44 0.59 1.00
0.0 0.0
GPT-4: ZS GPT-4: OS GPT-4: FS
Mistral
Large:
M
iZ stS
ral
Large: MiO stS
ral
Large: FS GPT-4: ZS GPT-4: OS GPT-4: FS
Mistral
Large:
M
iZ stS
ral
Large: MiO stS
ral
Large: FS
Figure 7: Inter-annotator agreement (IAA): Cohen’s Kappa score across six different models (2
models×3settings)forStance(left)andDogmatism(right)tasks.
B SystemPromptforLLMAnnotation
WeusedthefollowingprompttogetannotationsfromLLMsaswellasforinstruction-tuningof
SLMs.
"""
### Introduction
**Objective**: Analyze Reddit conversations to identify the stance of
specific authors on sociopolitical topics and determine their level of
dogmatism.
**Stance Definition**: Stance is defined as the expression of the author’s
standpoint and judgement towards a given topic.
**Dogmatism Definition**: Dogmatism is an opinion strongly believed as a fact
to support a stance without a question or allowance for conversation.
**Task**: Given a JSON formatted Reddit submission and its comment thread,
classify the stance of text segments related to ‘‘author1’’ and
‘‘author2’’ by assigning one of the following five predefined stance
labels: ‘strongly_against’, ‘somewhat_against’, ‘somewhat_in_favor’,
‘strongly_in_favor’, ‘stance_not_inferrable’. Also, assign a dogmatism
14
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerFlabel for each author by assigning one of the following four predefined
labels: ‘Deeply Rooted’, ‘Firm but Open’, ‘Open to Dialogue’, ‘Flexible’.
### Description of Stance Labels:
1. **strongly_against / strongly_in_favor**: Marks text showing strong
opinions, emotional expressions, or argumentative tones.
2. **somewhat_against / somewhat_in_favor**: Identifies texts with openness
to discussion, less certainty, or showing interest in different
viewpoints.
3. **stance_not_inferrable**: Use for texts that are neutral, support both
stances, or where the stance is unclear despite being on-topic.
### Description of Dogmatism Labels:
1. **Deeply Rooted**: Reflects a strong, unchangeable belief. This label
conveys the idea of someone who is firm in their opinion and unlikely to
be swayed.
2. **Firm but Open**: Indicates a person who is not likely to change their
mind but does not impose their views authoritatively. It captures the
essence of being steadfast in one’s beliefs without being dismissive of
others.
3. **Open to Dialogue**: Describes someone who holds a certain opinion but is
genuinely interested in considering other viewpoints. This label suggests
a willingness to engage in meaningful conversation about differing
perspectives.
4. **Flexible**: Denotes a person who is not firmly committed to their stance
and is open to changing their opinion. This label is indicative of
flexibility and openness to new information or arguments.
### Input Data Format
The input data will be in JSON format and will include several key elements
to represent a Reddit submission and its associated comments. Each
element provides specific information as described below:
- ‘id’: This is the unique identifier for the Reddit submission.
- ‘title’: The title of the post. This is what users see first and often
summarizes or hints at the content of the submission.
- ‘content’: The main post’s detailed description. This text segment provides
the core message or information the author wishes to communicate with the
Reddit community. It may include narratives, questions, or any
information relevant to the title.
- ‘author1’ or ‘author2’: The username of our focus author. This field is
applicable if the post or comment is made by one of the specific authors
we are tracking in the dataset.
- ‘comments’: An array (list) of comments related to the Reddit submission.
Each comment in this array includes the following fields:
- ‘id’: The unique identifier for the comment, allowing for identification
and reference within the dataset.
- ‘author1’ or ‘author2’: The username of the comment’s author, if it is
made by one of our focus authors. This helps in tracking contributions
by specific individuals.
- ‘body’: The text of the comment. This is the main content of the comment
where the author responds to the post or another comment, providing
insights, opinions, or further information.
- ‘replies’: An array of comments that are direct responses to this
comment. The structure of each reply follows the same format as the
initial comment, including ‘id’, ‘author1’ or ‘author2’ (if
applicable), ‘body’, and potentially more ‘replies’.
### Output Data Format
Submit your annotations in JSON format, grouping all stance annotations under
the key ‘‘stance_annotations’’. Each entry should be a dictionary
containing the segment’s ‘‘id’’, your ‘‘label’’, and the ‘‘reason’’ for
your choice. Include the dogmatism label and its justification under
‘‘dogmatism_label’’ and ‘‘dogmatism_reason’’ keys, respectively.
15The output should follow this structure:
‘‘‘json
{
"author1": {
"name": "[author_name]",
"stance_annotations": [
{
"id": "[segment_id]",
"label": "[chosen_label]",
"reason": "[Justification in <50 words]"
},
...
],
"dogmatism_label": "[chosen_dogmatism_label]",
"dogmatism_reason": "[Justification in <50 words]"
},
"author2": {
"name": "[author_name]",
"stance_annotations": [
{
"id": "[segment_id]",
"label": "[chosen_label]",
"reason": "[Justification in <50 words]"
},
...
],
"dogmatism_label": "[chosen_dogmatism_label]",
"dogmatism_reason": "[Justification in <50 words]"
}
}
’’’
### Instructions for Effective Annotation
1. **Labeling Stance**: For each segment (including the original Reddit
submission, comments, or replies) where "author1" or "author2" is
mentioned, assign a stance label that best represents the stance
expressed towards the discussed topic in the submission. This
comprehensive approach ensures no relevant contribution by "author1" or
"author2" is overlooked. Evaluate the stance based on the content’s tone,
argumentation, and engagement level with the topic.
2. **Providing Justification**: For each label assigned, include a concise
reason, aiming for less than 50 words. Focus on the stance and
argumentative indicators present in the text.
3. **Dogmatism Assessment**: After reviewing all segments from "author1" and
"author2", assign a single dogmatism label reflecting the overall tone
and approach in their contributions.
"""
C PromptsforFinetuningSLMs
Fig.8and9showsthepromptsusedforfinetuningSLMsforthestanceanddogmatismclassification
tasksrespectively.
16StanceClassification
Analyzethestanceofthepostenclosedinsquarebrackets.
Categorizeeachpostintooneofthefollowingcategoriesbasedonitsstance:
• SomewhatInFavor
• SomewhatAgainst
• StanceNotInferrable
• StronglyInFavor
• StronglyAgainst
andreturntheanswerasoneofthecorrespondingstancelabels.
[{data_point["stance_id_comment"]}]
Figure8: Promptforstanceclassification,forfinetuningSLMs.
UserDogmatismIdentification
Analyzethecommentsofauserinconversationenclosedinsquarebrackets.
Categorizetheopinionfluctuationoftheuserintooneofthefollowingcategoriesbasedon
itschange:
• OpentoDialogue
• FirmbutOpen
• DeeplyRooted
• Flexible
Returntheanswerasoneofthecorrespondingdogmatismlabels.
[{data_point["comments_string_for_dogmatism"]}]
Figure9: Promptfordogmatismclassification,forfinetuningSLMs.
D SampleofUserInputPrompt
"""
### User Prompt
Now complete the given task for the respective authors i.e., author1
name is "rookerin0" and respective ids are ['dhoxyz', 'f3pghji', '
f3tywb4', 'f3uomn2']. author2 name is "MikeWillTerminate" and
respective ids are ['f3rt0bf', 'f3rqu2u'] for the data in json
format
{
"id":"dhoxyz",
"author1":"rookerin0",
"title":"This sub should encourage anti vs. pro-gun discussions
instead of shutting them down instantly",
"content":"Honesly, I followed this sub especifically to take part
in these discussions, but everytime I see a comment that even
remotely suggests anti gun ideals or a discussion on the
subject just gets ignored and downvoted to hell. Kind of
expecting this to go the same way (my karma anus is ready,
downvotes) , but I have to hope for healthy discussions on the
subject.",
"comments":[
{
"id":"f3p9n2c",
"body":"I think the problem now is the two sides are at an
impasse. Everytime there is a "compromise" pro gun loses
17something. Now days pro gun is interpreting the
Constitution more literal, which leaves even the most
mild policies of anti gun as infringements. To further
compound this anti gun is only considering the most
extreme measures. "Assault Weapons" bans, mandatory
buybacks, red flag laws, etc.. I think at this point
there is just nothing left to talk about. The middle
ground is gone.",
"replies":[
{
"id":"f3pati9",
"replies":[
{
"id":"f3pdu44",
"body":"You are exactly right. I'm done with the
idea that there can be real compromise. We
should have at least gotten national
reciprocity and shall-issue in every state in
exchange for what we've given up. Now you
have to be a goddamn lawyer to exercise your
rights without violating the law."
},
{
"author2":"MikeWillTerminate",
"id":"f3rt0bf",
"body":"I am prepared for UBCs, if they do this:
1. Lower the age to buy handguns to 18,
nationwide.
2. Repeal the Hughes Amendment:
3. A FOPA-like ban on assault weapon bans (what
the FOPA did with a registry)
4. The punishment for violation is a monetary
fine only
5. A repeal of the GCA ban on foreign NFA weapons
6. A repeal of the National Minimum Drinking Age
Act of 1984"
}
]
},
{
"id":"f3pd55z",
"body":"Everytime there is a "compromise" pro gun loses
something. That and today's compromise is tomorrow
's loophole to be closed. All such compromises do
is push that policy off until the next round."
}
]
},
{
"id":"f3paf0j",
"body":"Yeah this sub it's not conducive to conversion. Its
quickly devolving to little more than "Boogaloo" memes
and shouting "SHALL. NOT." at each other. However, as
far as I know, the mods won't delete your thread and ban
you from the sub for trying to have a good faith
discussion, like some of the gun control subs will.",
"replies":[
{
"id":"f3pusbm",
"body":"Unfortunately this sub's mod team takes a very
passive approach to moderation. With very little
effort they could make this sub into a quality
progun meeting ground *without having to resort to
censorship*. Instead they promote low-effort memes
and endless duplication of posts through their
18inaction. whubbard has the chops to resurrect this
sub. Let's see if he's up to the challenge.",
"replies":[
{
"id":"f3q8xj6",
"body":"We voted to ban memes last week. All
about rolling it out now.",
"replies":[
{
"id":"f3qn4p8",
"body":"Damn I might have to eat some crow
here then..."
}
]
}
]
}
]
},
{
"id":"f3pafqa",
"body":"Found the gun grabber!!",
"replies":[
{
"id":"f3pcw4h",
"body":"Witch hunter."
}
]
},
{
"id":"f3pal5l",
"body":"I see people have discussions when it makes sense to.
Not much reason to spend time responding to the same gun
control measures over and over though."
},
{
"id":"f3paw3h",
"body":"I get where you're coming from, but people's ability
to protect themselves and own their own property isn't
something that is compromisable. Anything less, and they
cease to own their own property. It's like breathing,
there can be nothing less than total ability to breath
when and how someone wants. It's just that simple."
},
{
"id":"f3pax9m",
"body":"My take on this, What kind of open discussion is
possible for a right that is guaranteed and most
importantly, not to be infringed upon? They're making all
these unlawful laws to portray it as it's somehow
legitimate. They are not, We are at an apex, to which
both political spectrums and even us to a degree are
liable for.\nI certainly believe both sides are waiting
for this to boil over so each can finger point. I just
speculate it's going to be the hell humanity been
whispering about but never thought it would ever occur."
},
{
"id":"f3pb6ny",
"body":"The time for discussion is over."
},
{
"id":"f3pfqwq",
"body":"I don't know what you're talking about. Sure people
downvote, but they also talk. We get "why do you need
19guns" posts at least weekly, and several people will
engage in actual conversation with them, citing facts,
clearing up statistics, and telling stories to illustrate
why this is important to them, but they are usually met
with "you stupid @#$%, you think you're Rambo" or
something equally clever. People who come here to discuss
and learn will be treated well. People who are just
trolling are treated like trolls.",
"replies":[
{
"author1":"rookerin0",
"id":"f3pghji",
"body":"I made this post because I'm always seeing
rational, conversation seeking comments getting
blown to downvote hell.",
"replies":[
{
"id":"f3pi9xv",
"body":"[Like this one?](https://www.reddit.com/r/
progun/comments/dhcu92/yup/f3p75tg/)> One smart
man in a sub full of... welp... "strong opinions
". You start off with arrogance, as the sole
arbiter of what constitutes a "smart man". Then
you back it up with a dismissive swipe at what
you term "strong opinions".> Every other country
can see that PROPER gun control reduces gun
violence by a ton, More arrogance. False
equivalence. Unsupported claims.> but the US
refuses to let go of it's antique laws In a
shocking turn of events, more arrogance.> Fully
aware that this is a fully pro gun sub, willing
to take the downvotes in order to spark a
discussion and crack some heads. You aren't the
first arrogant asshole to grace this sub with
posts like this. Try bringing something other
than your own self-importance to the discussion.
Edit: And then there's [this gem](https://www.
reddit.com/r/unpopularopinion/comments/d3w5z1/
people_living_in_the_us_are_living_in_one_of_the/
f06r3sg/.> Wanna feel like you could be shot at
every single moment? Move to the US, it'll prob
happen to you either as a bystander, or you'd be
shot by a random citizen (sometimes police)."
},
{
"id":"f3pj8k0",
"body":"As is tradition. We're done with that
condescending bullshit from antis, you dont
come here for good faith discussion and
whether you get a reasonable response or not,
nothing ever changes, easier to downvote you
and move on because we get the same
treatment anytime we attempt to speak out in
anti subs."
},
{
"id":"f3plgf4",
"body":"If downvotes hurt your feelings, you
shouldn't be on reddit. People tend to
downvote anything they disagree with (which
is why some subs specifically ask you to only
downvote things that contribute nothing to
the discussion). It's a bad habit, but that's
the way it is. People downvote and *still*
enage. You want to post a view contrary to
20the prevailing view of the sub, take your
lumps and participate in what conversation
you are offered. But if you're only here to
preach about how stupid, misguided, unevolved
, uneducated, irrational, and/or violent we
are, don't expect a polite response."
},
{
"id":"f3tcgf1",
"body":"An arrogant Israeli trying to tell
another nation how they should be run. You're
just a walking stereotype aren't you? And
before you say anything, I popped into your
comment history. That's where the calling you
Israeli comes from.",
"replies":[
{
"author1":"rookerin0",
"id":"f3tywb4",
"body":"I thought that trying to tell other
nations how they should run was your
guys's stereotype.",
"replies":[
{
"id":"f3u0vkq",
"body":"No we go in and try to make
them work our way."
}
]
}
]
}
]
}
]
},
{
"id":"f3pzseh",
"body":"It's a little unfortunate but the grabbers who come
on here tend to be intellectually dishonest and/or
uninformed. There was some Australian post a few days ago
that pretty much asked why we like our guns more than
children. No discussion to be had there. There's also
some posts that clearly demonstrate the poster should
inform himself or herself a little."
},
{
"author2":"MikeWillTerminate",
"id":"f3rqu2u",
"body":"Actually, do that. It shows everyone that they tend
to be crazy, unstable, ignorant, stereotyping, arrogant
bastards who hate black people with a hair trigger."
},
{
"id":"f3t7tgg",
"body":"Welcome to reddit, home of every single safe place
for anything that doesnt violate the TOS. At least its
slightly better than r/politics"
},
{
"id":"f3unt9z",
"body":"This isn't r/gundebate. This is a pro gun subreddit.
That said, we do allow some debate provided it remains
civil.",
"replies":[
21{
"author1":"rookerin0",
"id":"f3uomn2",
"body":"Sadly tho, r/gundebate is pretty dead..."
}
]
},
{
"id":"f4dip6o",
"body":"Anything else you want to give away for free?"
}
]
}
#### Response
E SamplesofJSONOutputsfromLLMs
E.1 SampleJSONOutputfromLLM:GPT-4Zero-shot
{
"author1": {
"name": "rookerin0",
"stance_annotations": [
{
"id": "dhoxyz",
"label": "somewhat_in_favor",
"reason": "Advocates for open discussions, showing interest in
dialogue."
},
{
"id": "f3pghji",
"label": "somewhat_in_favor",
"reason": "Expresses desire for rational discussions, despite
downvotes."
},
{
"id": "f3tywb4",
"label": "somewhat_against",
"reason": "Sarcastic retort, but lacks strong emotional
language."
},
{
"id": "f3uomn2",
"label": "stance_not_inferrable",
"reason": "Comment is factual, no clear stance on the debate
itself."
}
],
"dogmatism_label": "Open to Dialogue",
"dogmatism_reason": "Shows interest in discussion, acknowledges
downvotes."
},
"author2": {
"name": "MikeWillTerminate",
"stance_annotations": [
{
"id": "f3rt0bf",
"label": "somewhat_in_favor",
"reason": "Proposes conditions for UBCs, indicating some
openness."
},
22{
"id": "f3rqu2u",
"label": "strongly_against",
"reason": "Uses strong negative descriptors for opponents,
indicating firm stance."
}
],
"dogmatism_label": "Firm but Open",
"dogmatism_reason": "Mixed; proposes solutions but also dismisses
opposition."
}
}
E.2 SampleJSONOutputfromLLM:GPT-4One-shot
{
"author1": {
"name": "rookerin0",
"stance_annotations": [
{
"id": "dhoxyz",
"label": "somewhat_in_favor",
"reason": "Advocates for open discussions, indicating a pro-
discussion stance."
},
{
"id": "f3pghji",
"label": "somewhat_in_favor",
"reason": "Expresses desire for rational conversation, showing
a supportive stance for dialogue."
},
{
"id": "f3tywb4",
"label": "somewhat_against",
"reason": "Uses sarcasm to counter a stereotype, indicating a
stance against the opposing view."
},
{
"id": "f3uomn2",
"label": "somewhat_in_favor",
"reason": "Expresses disappointment in lack of active debate,
supporting open discussion."
}
],
"dogmatism_label": "Open to Dialogue",
"dogmatism_reason": "Shows a consistent preference for discussion
and engagement, without dogmatic assertions."
},
"author2": {
"name": "MikeWillTerminate",
"stance_annotations": [
{
"id": "f3rt0bf",
"label": "somewhat_in_favor",
"reason": "Lists conditions for agreement, showing openness to
compromise."
},
{
"id": "f3rqu2u",
"label": "strongly_against",
"reason": "Uses strong language to describe opponents,
indicating a firm stance against them."
}
23],
"dogmatism_label": "Firm but Open",
"dogmatism_reason": "While showing strong opinions, also lists
specific conditions for compromise."
}
}
E.3 SampleJSONOutputfromLLM:GPT-4largeFew-shot
{
"author1": {
"name": "rookerin0",
"stance_annotations": [
{
"id": "dhoxyz",
"label": "somewhat_in_favor",
"reason": "Expresses desire for open discussions, indicating a
stance in favor of dialogue."
},
{
"id": "f3pghji",
"label": "somewhat_in_favor",
"reason": "Advocates for rational conversation despite
observing negative reactions."
},
{
"id": "f3tywb4",
"label": "somewhat_against",
"reason": "Uses sarcasm to counter a stereotype, showing a
stance against it."
},
{
"id": "f3uomn2",
"label": "stance_not_inferrable",
"reason": "Comment on the state of another subreddit does not
reveal a stance on the gun debate."
}
],
"dogmatism_label": "Open to Dialogue",
"dogmatism_reason": "Shows a consistent
preference for discussion over confrontation."
},
"author2": {
"name": 'MikeWillTerminate",
"stance_annotations": [
{
"id": 'f3rt0bf",
"labe"': 'somewhat_in_favor",
"reason": 'Lists conditions for accepting UBCs, indicating
some openness to compromise."
},
{
"id": 'f3rqu2u",
"label": "strongly_against",
"reason": "Uses derogatory language to describe opponents,
showing strong opposition."
}
],
"dogmatism_label": "Firm but Open",
"dogmatism_reason": "While showing strong opinions, also suggests
conditions for policy acceptance."
}
}
24E.4 SampleJSONOutputfromLLM:MistralLargeZero-shot
{
"author1": {
"name": "rookerin0",
"stance_annotations": [
{
"id": "dhoxyz",
"label": "somewhat_in_favor",
"reason": "Author expresses desire for open discussions on
anti-gun ideals."
},
{
"id": "f3pghji",
"label": "stance_not_inferrable",
"reason": "Author is commenting on downvotes, not expressing
stance."
},
{
"id": "f3tywb4",
"label": "stance_not_inferrable",
"reason": "Author is making a stereotypical comment, not
expressing stance."
},
{
"id": "f3uomn2",
"label": "stance_not_inferrable",
"reason": "Author is commenting on the state of another
subreddit, not expressing stance."
}
],
"dogmatism_label": "Open to Dialogue",
"dogmatism_reason": "Author expresses desire for open discussions
and does not dismiss opposing views."
},
"author2": {
"name": "MikeWillTerminate",
"stance_annotations": [
{
"id": "f3rt0bf",
"label": "strongly_in_favor",
"reason": "Author lists conditions for accepting UBCs, showing
strong pro-gun stance."
},
{
"id": "f3rqu2u",
"label": "strongly_in_favor",
"reason": "Author"s comment is dismissive of anti-gun
advocates, showing strong pro-gun stance."
}
],
"dogmatism_label": "Deeply Rooted",
"dogmatism_reason": "Author"s comments are dismissive of opposing
views and show a strong, unchangeable belief in pro-gun ideals
."
}
}
E.5 SampleJSONOutputfromLLM:MistralLargeOne-shot
{
"author1": {
"name": "rookerin0",
25"stance_annotations": [
{
"id": "dhoxyz",
"label": "somewhat_in_favor",
"reason": "Expresses desire for open discussions on gun
control, but follows pro-gun subreddit."
},
{
"id": "f3pghji",
"label": "stance_not_inferrable",
"reason": "Comments on downvotes, but no clear stance on gun
control."
},
{
"id": "f3tywb4",
"label": "stance_not_inferrable",
"reason": "Comment about stereotypes, but no clear stance on
gun control."
},
{
"id": "f3uomn2",
"label": "stance_not_inferrable",
"reason": "Mentions r/gundebate being dead, but no clear
stance on gun control."
}
],
"dogmatism_label": "Open to Dialogue",
"dogmatism_reason": "Seeks open discussions and does not express
strong dogmatic views."
},
"author2": {
"name": "MikeWillTerminate",
"stance_annotations": [
{
"id": "f3rt0bf",
"label": "somewhat_in_favor",
"reason": "Lists conditions for supporting UBCs, showing
openness to compromise."
},
{
"id": "f3rqu2u",
"label": "strongly_in_favor",
"reason": "Strong negative language against gun control
advocates, suggesting strong pro-gun stance."
}
],
"dogmatism_label": "Firm but Open",
"dogmatism_reason": "Expresses strong pro-gun views but open to
compromise under certain conditions."
}
}
E.6 SampleJSONOutputfromLLM:MistralLargeFew-shot
{
"author1": {
"name": "rookerin0",
"stance_annotations": [
{
"id': "dhoxyz",
"label": "somewhat_in_favor",
"reason": "Expresses desire for open discussions on anti vs.
pro-gun debates."
26},
{
"id": "f3pghji",
"label": "stance_not_inferrable",
"reason": "Comments on downvotes, but no clear stance on the
topic."
},
{
"id": "f3tywb4",
"label": "stance_not_inferrable",
"reason": "Makes a stereotypical comment, but no clear stance
on the topic."
},
{
"id": "f3uomn2",
"label": "stance_not_inferrable",
"reason": "Mentions r/gundebate being dead, but no clear
stance on the topic."
}
],
"dogmatism_label": "Open to Dialogue",
"dogmatism_reason": "Seeks open discussions and engages in
conversation without strong dogmatic language."
},
"author2": {
"name": "MikeWillTerminate",
"stance_annotations": [
{
"id": "f3rt0bf",
"label": "somewhat_in_favor",
"reason": "Lists conditions for supporting UBCs, showing
openness to discussion."
},
{
"id": "f3rqu2u",
"label": "strongly_against",
"reason": "Uses derogatory language to express strong
opposition to 'grabbers'."
}
],
"dogmatism_label": "Firm but Open",
"dogmatism_reason": "Expresses strong opinions but also shows
willingness to consider certain conditions for compromise."
}
}
LLaMa-3-8B finetuning: Majority Voting, Confusion Matrix Heatmap LLaMa-3-8B instruction-tuning: Majority Voting, Confusion Matrix Heatmap
1 32 0 9 0 28 0 12
0 119 0 77 0 92 0 97
0 3 1 17 0 0 0 23
0 69 0 131 0 74 0 134
DR FBO FX OTD DR FBO FX OTD
Predicted Labels Predicted Labels
Figure 10: Confusion matrix for LLaMa-3-8B Dogmatism detection models on USDC test set:
finetuning(left)andinstruction-tuning(right). Here,DR:DeeplyRooted,FX:Flexible,FBO:Firm
butOpen,OTD:OpentoDialogue
27
slebaL
eurT
RD
OBF
XF
DTO
slebaL
eurT
RD
OBF
XF
DTOF SLMFinetuning: AUC(AreaUndertheCurve)Analysis
Fig.10illustratestheconfusionmatrixfordogmatismdetectionforLLaMa-3-8Bfinetuningand
instruction-tuning. WemakethefollowingobservationsfromFig.10: 1)Forbothfinetuningand
instruction-tuning,therearesignificantmisclassifications,especiallyforthe"DeeplyRooted"and
"Flexible" labels, with both having zero accuracy and F1-scores. While "Firm but Open" and
"OpentoDialogue"performmoderatelybetter,withaccuraciesof48.7%and64.4%respectively.
Theconfusionmatrixindicatessubstantialconfusiontodistinguishbetweenintermediatelevelsof
dogmatism,suchas"FirmbutOpen"and"OpentoDialogue. WefurtherreporttheROCcurveshows
thetrade-offbetweenthetruepositiverate(TPR)andfalsepositiverate(FPR)foreachclassfor
stanceanddogmatismtasks,inFigs.11and.12. TheareaundertheROCcurve(AUC)measuresthe
model’sabilitytodistinguishbetweenclasses.
LLaMa-3-8B finetuning: GPT-4 zero-shot, ROC for Multi-class LLaMa-3-8B finetuning: GPT-4 one-shot, ROC for Multi-class
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
micro-average ROC curve (area = 0.72) micro-average ROC curve (area = 0.72)
ROC curve of class Somewhat Against (area = 0.66) ROC curve of class Somewhat Against (area = 0.64)
ROC curve of class Somewhat In Favor (area = 0.67) ROC curve of class Somewhat In Favor (area = 0.69)
0.2 0.2
ROC curve of class Stance Not Inferrable (area = 0.77) ROC curve of class Stance Not Inferrable (area = 0.78)
ROC curve of class Strongly Against (area = 0.71) ROC curve of class Strongly Against (area = 0.71)
ROC curve of class Strongly In Favor (area = 0.56) ROC curve of class Strongly In Favor (area = 0.60)
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate False Positive Rate
LLaMa-3-8B finetuning: GPT-4 few-shot, ROC for Multi-class LLaMa-3-8B finetuning: GPT-4 Majority, ROC for Multi-class
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
micro-average ROC curve (area = 0.72) micro-average ROC curve (area = 0.74)
ROC curve of class Somewhat Against (area = 0.67) ROC curve of class Somewhat Against (area = 0.64)
ROC curve of class Somewhat In Favor (area = 0.67) ROC curve of class Somewhat In Favor (area = 0.72)
0.2 0.2
ROC curve of class Stance Not Inferrable (area = 0.78) ROC curve of class Stance Not Inferrable (area = 0.79)
ROC curve of class Strongly Against (area = 0.72) ROC curve of class Strongly Against (area = 0.75)
ROC curve of class Strongly In Favor (area = 0.61) ROC curve of class Strongly In Favor (area = 0.60)
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate False Positive Rate
Figure11: LLaMa-3-8Bfinetuningforstancedetectiontask: VisualizetheROCcurvesforeachclass
alongwiththeirAUCvaluesforGPT-4annotationsacrosszero-shot,one-shot,few-shotandmajority
labels.
G SLMinstruction-tuning: AUC(AreaUndertheCurve)analysis
Fig.13showstheROCcurvetrade-offbetweenthetruepositiverate(TPR)andfalsepositiverate
(FPR)foreachclassforstancetaskusingLLaMa-3-8Binstruction-tuning. Thisinstruction-tuning
isperformedonGPT-4(zero-shot,one-shot,few-shot)andmajorityvotinglabelsfromtheUSDC
dataset. WemakethefollowingobservationsfromFig.13: 1)Acrossallfoursettings,theareaunder
thecurve(AUC)forallstancelabelsis>=0.5. Thisindicatesthatthemodelpredictseachstance
labelmoreaccuratelythanrandomguessingforallclasses. 2)Amongallsettings,themajorityvoting
labelsfromtheUSDCdatasetshowahigherAUCforeachclasscomparedtozero-shot,one-shot,
andfew-shotlabels. 3)Amongallstanceclasses,the"StanceNotInferrable"classhasthehighest
AUC(0.8),whilethe"StronglyInFavor"classhasthelowestAUC(0.6). Overall,LLaMa-3-8B
instruction-tuningdemonstratessuperiorperformanceinthestancedetectiontask. However,thereis
stillsignificantroomforimprovementinunderstandinguseropinionsfromtextsegments.
28
etaR
evitisoP
eurT
etaR
evitisoP
eurT
etaR
evitisoP
eurT
etaR
evitisoP
eurTLLaMa-3-8B finetuning: Majority Voting, ROC curves for Multi-class LLaMa-3-8B finetuning: GPT-4 One-shot, ROC curves for Multi-class
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
micro-average ROC curve (area = 0.66) micro-average ROC curve (area = 0.67)
ROC curve of class Deeply Rooted (area = 0.50) ROC curve of class Deeply Rooted (area = 0.50)
0.2 ROC curve of class Firm but Open (area = 0.56) 0.2 ROC curve of class Firm but Open (area = 0.59)
ROC curve of class Flexible (area = 0.50) ROC curve of class Flexible (area = 0.60)
ROC curve of class Open to Dialogue (area = 0.56) ROC curve of class Open to Dialogue (area = 0.59)
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate False Positive Rate
LLaMa-3-8B finetuning: GPT-4 Majority, ROC for Multi-class
LLaMa-3-8B finetuning: GPT-4 Majority, ROC curves for Multi-class
1.0
1.0
0.8 0.8
0.6 0.6
0.4 0.4
micro-average ROC curve (area = 0.74)
micro-average ROC curve (area = 0.65) ROC curve of class Somewhat Against (area = 0.64)
0.2 R RO OC C c cu ur rv ve e o of f c cl la as ss s D Fie rmep bly u tR Ooo pt ee nd (( aa rr ee aa == 00 .. 55 32 )) 0.2 R RO OC C c cu ur rv ve e o of f c cl la as ss s S So tam ne cew h Na ot t I In n fF ea rv rao br l( ea r (e aa re = a =0. 7 02 .7) 9)
ROC curve of class Flexible (area = 0.61) ROC curve of class Strongly Against (area = 0.75)
ROC curve of class Open to Dialogue (area = 0.58) ROC curve of class Strongly In Favor (area = 0.60)
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate False Positive Rate
Figure12: LLaMa-3-8Bfinetuningfordogmatismtask: VisualizetheROCcurvesforeachclass
alongwiththeirAUCvaluesforGPT-4annotationsacrosszero-shot,one-shot,few-shotandmajority
labels.
H SLMfinetuning: Cross-modalAnalysisonSPINOSDataset
Fig.14illustratestheconfusionmatrixforStancedetectionforLLaMa-3-8BfinetuningonUSDC
andtransferlearningonSPINOS.WealsovalidatedtheSPINOSperformanceusingotherSLMssuch
asLLaMa-3-8B-Instruct,LLaMa-2-7B,LLaMa-2-7B-Chat,andVicuna-7Bmodels. Figs.15,16,17
and 18displaythesemodelresults.
29
etaR
evitisoP
eurT
etaR
evitisoP
eurT
etaR
evitisoP
eurT
etaR
evitisoP
eurTLLaMa-3-8B instruction-tuning: GPT-4 Zero-shot, ROC for Multi-class LLaMa-3-8B instruction-tuning: GPT-4 One-shot, ROC for Multi-class
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
micro-average ROC curve (area = 0.72) micro-average ROC curve (area = 0.72)
ROC curve of class Somewhat Against (area = 0.63) ROC curve of class Somewhat Against (area = 0.66)
ROC curve of class Somewhat In Favor (area = 0.69) ROC curve of class Somewhat In Favor (area = 0.69)
0.2 ROC curve of class Stance Not Inferrable (area = 0.80) 0.2 ROC curve of class Stance Not Inferrable (area = 0.76)
ROC curve of class Strongly Against (area = 0.68) ROC curve of class Strongly Against (area = 0.70)
ROC curve of class Strongly In Favor (area = 0.58) ROC curve of class Strongly In Favor (area = 0.58)
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate False Positive Rate
LLaMa-3-8B instruction-tuning: GPT-4 Majority, ROC for Multi-class LLaMa-3-8B instruction-tuning: GPT-4 Majority, ROC for Multi-class
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
micro-average ROC curve (area = 0.74) micro-average ROC curve (area = 0.74)
ROC curve of class Somewhat Against (area = 0.65) ROC curve of class Somewhat Against (area = 0.65)
ROC curve of class Somewhat In Favor (area = 0.71) ROC curve of class Somewhat In Favor (area = 0.71)
0.2 ROC curve of class Stance Not Inferrable (area = 0.80) 0.2 ROC curve of class Stance Not Inferrable (area = 0.80)
ROC curve of class Strongly Against (area = 0.72) ROC curve of class Strongly Against (area = 0.72)
ROC curve of class Strongly In Favor (area = 0.59) ROC curve of class Strongly In Favor (area = 0.59)
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate False Positive Rate
Figure13: LLaMa-3-8Binstruction-tuningforstancedetectiontask: VisualizetheROCcurvesfor
eachclassalongwiththeirAUCvaluesforGPT-4annotationsacrosszero-shot,one-shot,few-shot
andmajoritylabels.
LLaMa-3-8B USDC -> Spinos: Majority Voting, Confusion Matrix Heatmap
363 472 19 490 186
55 334 29 212 103
7 113 30 39 72
40 159 6 192 107
9 54 3 60 84
SNI SOIF SIF SOA SGA
Predicted Labels
Figure14:ConfusionmatrixforLLaMa-3-8BStancedetectionmodelsonSPINOStestset:finetuning
onUSDCandtestitonSPINOS.SOA:SomewhatAgainst,SOIF:SomewhatInFavor,SNI:Stance
NotInferrable,SGA:StronglyAgainst,SIF:StronglyInFavor.
30
etaR
evitisoP
eurT
etaR
evitisoP
eurT
slebaL
eurT
INS
FIOS
FIS
AOS
AGS
etaR
evitisoP
eurT
etaR
evitisoP
eurTLLaMa-3-8B-instruct USDC -> Spinos: Majority Voting, Confusion Matrix Heatmap
352 508 32 400 238
44 338 43 177 131
9 108 36 27 81
36 148 25 171 124
9 48 11 47 95
SNI SOIF SIF SOA SGA
Predicted Labels
Figure15: ConfusionmatrixforLLaMa-3-8B-instructStancedetectionmodelsonSPINOStestset:
finetuningonUSDCandtestitonSPINOS.SOA:SomewhatAgainst,SOIF:SomewhatInFavor,
SNI:StanceNotInferrable,SGA:StronglyAgainst,SIF:StronglyInFavor.
LLaMa-2-7B USDC -> Spinos: Majority Voting, Confusion Matrix Heatmap
313 471 32 489 225
35 329 41 195 133
3 99 44 32 83
30 146 21 173 134
7 38 15 53 97
SNI SOIF SIF SOA SGA
Predicted Labels
Figure16:ConfusionmatrixforLLaMa-2-7BStancedetectionmodelsonSPINOStestset:finetuning
onUSDCandtestitonSPINOS.SOA:SomewhatAgainst,SOIF:SomewhatInFavor,SNI:Stance
NotInferrable,SGA:StronglyAgainst,SIF:StronglyInFavor.
31
slebaL
eurT
slebaL
eurT
INS
FIOS
FIS
AOS
AGS
INS
FIOS
FIS
AOS
AGSLLaMa-2-7B-chat USDC -> Spinos: Majority Voting, Confusion Matrix Heatmap
272 546 30 468 214
28 366 37 179 123
4 108 40 36 73
24 173 22 163 122
7 46 23 54 80
SNI SOIF SIF SOA SGA
Predicted Labels
Figure17: ConfusionmatrixforLLaMa-2-7B-chatStancedetectionmodelsonSPINOStestset:
finetuningonUSDCandtestitonSPINOS.SOA:SomewhatAgainst,SOIF:SomewhatInFavor,
SNI:StanceNotInferrable,SGA:StronglyAgainst,SIF:StronglyInFavor.
Vicuna-7B USDC -> Spinos: Majority Voting, Confusion Matrix Heatmap
280 494 35 486 235
35 335 53 169 141
4 94 45 30 88
34 142 28 179 121
10 43 23 42 92
SNI SOIF SIF SOA SGA
Predicted Labels
Figure18: ConfusionmatrixforVicuna-7BStancedetectionmodelsonSPINOStestset: finetuning
onUSDCandtestitonSPINOS.SOA:SomewhatAgainst,SOIF:SomewhatInFavor,SNI:Stance
NotInferrable,SGA:StronglyAgainst,SIF:StronglyInFavor.
32
slebaL
eurT
slebaL
eurT
INS
FIOS
FIS
AOS
AGS
INS
FIOS
FIS
AOS
AGS