Revisiting Referring Expression Comprehension
Evaluation in the Era of Large Multimodal Models
JierunChen∗ FangyunWei∗†
HKUST MicrosoftResearchAsia
jcheneh@cse.ust.hk fawe@microsoft.com
JinjingZhao SizheSong BohuaiWu
TheUniversityofSydney HKUST HKUST
jzha0100@uni.sydney.edu.au ssongad@cse.ust.hk bwual@cse.ust.hk
ZhuoxuanPeng S.-H.GaryChan HongyangZhang
HKUST HKUST UniversityofWaterloo
zpengac@cse.ust.hk gchan@cse.ust.hk hongyang.zhang@uwaterloo.ca
Abstract
Referringexpressioncomprehension(REC)involveslocalizingatargetinstance
basedonatextualdescription. RecentadvancementsinREChavebeendrivenby
largemultimodalmodels(LMMs)likeCogVLM,whichachieved92.44%accuracy
onRefCOCO.However,thisstudyquestionswhetherexistingbenchmarkssuchas
RefCOCO,RefCOCO+,andRefCOCOg,captureLMMs’comprehensivecapabil-
ities. Webeginwithamanualexaminationofthesebenchmarks,revealinghigh
labelingerrorrates:14%inRefCOCO,24%inRefCOCO+,and5%inRefCOCOg,
whichunderminestheauthenticityofevaluations. Weaddressthisbyexcluding
problematicinstancesandreevaluatingseveralLMMscapableofhandlingtheREC
task, showing significant accuracy improvements, thus highlighting the impact
of benchmark noise. In response, we introduce Ref-L4, a comprehensive REC
benchmark,specificallydesignedtoevaluatemodernRECmodels.Ref-L4isdistin-
guishedbyfourkeyfeatures: 1)asubstantialsamplesizewith45,341annotations;
2)adiverserangeofobjectcategorieswith365distincttypesandvaryinginstance
scalesfrom30to3,767;3)lengthyreferringexpressionsaveraging24.2words;and
4)anextensivevocabularycomprising22,813uniquewords. Weevaluateatotalof
24largemodelsonRef-L4andprovidevaluableinsights. Thecleanedversionsof
RefCOCO,RefCOCO+,andRefCOCOg,aswellasourRef-L4benchmarkand
evaluationcode,areavailableathttps://github.com/JierunChen/Ref-L4.
1 Introduction
Referringexpressioncomprehension(REC)[47,38,81,21,43,75,65]involvesthetaskoflocalizing
aspecifictargetinstancebasedonagiventextualdescription. TheadvancementofREChasbeen
significantlypropelledbythesuperiorlanguageprocessingcapabilitiesoflargelanguagemodels
(LLMs)[55,56,37,9,15,1,28,19,26]. Thisprogressisparticularlyevidentintheexceptional
performanceoflargemultimodalmodels(LMMs)[62,31,13,60,2,5,66,16,57,17,80]onwell-
knownbenchmarkssuchasRefCOCO[71],RefCOCO+[71],andRefCOCOg[36]. Thesemodels
havedemonstratedremarkableaccuracy,withCogVLM[62],forinstance,achievinganimpressive
accuracyrateof92.44%ontheRefCOCObenchmark.
∗Equalcontribution.
†Correspondingauthor.
Preprint.Underreview.
4202
nuJ
42
]VC.sc[
1v66861.6042:viXraTable1:StatisticsofthelabelingerrorratesforRefCOCO,RefCOCO+,andRefCOCOg,respectively.
Foreachbenchmark,thestatisticsareconductedonthecombinationofthevalidationandtestsets.
Benchmark Annotations Errors LabelingErrorRate
RefCOCO[71] 21,586 3,054 14%
RefCOCO+[71] 21,373 5,201 24%
RefCOCOg[36] 14,498 675 5%
Table2: TheperformanceoffourLMMscapableofhandlingtheRECtaskonboththecleanedand
originalversionsoftheRefCOCO,RefCOCO+,andRefCOCOgbenchmarks,usingtheconventional
accuracyastheevaluationmetric. Theevaluationisperformedonthecombinationofthevalidation
andtestsetsforeachbenchmark. †: modelsfine-tunedonthespecificdataset.
ONE- CogVLM-
Benchmark OFA-L†[59] OFA-L[59] Qwen-VL[2]
PEACE†[60] Grounding[62]
RefCOCO[71] 92.15 89.85 85.13 88.51 92.44
RefCOCO(Cleaned) 94.11(+1.96) 92.06(+2.22) 87.95(+2.81) 90.68(+2.18) 94.58(+2.13)
RefCOCO+[71] 88.14 85.06 77.56 82.52 88.55
RefCOCO+(Cleaned) 90.79(+2.66) 87.38(+2.32) 80.50(+2.94) 85.60(+3.08) 91.43(+2.87)
RefCOCOg[36] 89.18 84.77 79.25 85.11 90.67
RefCOCOg(Cleaned) 90.75(+1.57) 86.39(+1.62) 80.89(+1.64) 86.79(+1.68) 92.36(+1.68)
Thispaperbeginswithacriticalquestion: doexistingRECbenchmarkstrulycapturethecomprehen-
sivecapabilitiesofLMMs? Thefoundationalbenchmarks,RefCOCO[71],RefCOCO+[71],and
RefCOCOg[36],wereintroducedsequentiallyin2015,2016,and2016,respectively. InRefCOCO,
thereferringexpressionsarenotablysuccinct,rangingfromsinglewordslike“lady”and“yellow”
tobriefdescriptionssuchas“farleftperson”and“whiteshirt”. RefCOCO+intentionallyexcludes
locationalprepositionscommonlyfoundinRefCOCO,favoringshortyetsemanticallyrichexpres-
sionslike“plasticcupwithjustice”and“manonscreen”. Conversely,RefCOCOgprovidesmore
elaborateannotations,includingexamplessuchas“atableoffood,withplates,apizza,pitchers,and
glasses”and“aredandwhitecheckeredtablewithtwowoodenchairs”.Thesevariationshighlightthe
evolutionandcomplexityofreferringexpressionsacrossdifferentbenchmarks,raisingthequestion
ofwhethertheycaneffectivelyassessthenuancedcapabilitiesofmodernLMMsinunderstanding
diverselinguisticinputsandassociatinglanguageswithvisualelements.
LabelingErrorRatesofExistingBenchmarks. Tobegin,wemanuallyassessthelabelingerror
ratesofthevalidationandtestsetsinRefCOCO,RefCOCO+,andRefCOCOg,discoveringahigh
error rate across these benchmarks. The labeling errors include, typos, misalignment between
referringexpressionsandtargetinstances,aswellasinaccurateboundingboxannotations,asdepicted
in Section A. As illustrated in Table 1, the labeling error rates for RefCOCO, RefCOCO+, and
RefCOCOg are 14%, 24%, and 5%, respectively, indicating that evaluations performed on these
benchmarksmaylackauthenticity.
ReevaluationonRefCOCO,RefCOCO+andRefCOCOg. Inresponse, wemanuallyexclude
the problematic instances from the validation and test sets of RefCOCO, RefCOCO+, and Ref-
COCOg. Subsequently, we reevaluate four LMMs capable of handling the REC task—namely
ONE-PEACE[60],OFA-L[59],Qwen-VL[2],andCogVLM-Grounding[62]—onboththecleaned
andoriginalversionsofthesedatasets,asshowninTable2. Acrossallmodelsandcleanedbench-
marks,weobserveasignificantaccuracyimprovement,rangingfrom1.57to3.08,comparedtotheir
performanceontheoriginalversions. Thisdemonstratesthatnoiseinthebenchmarkshasimpacted
themodels’truecapabilities. TosupportfurtherresearchintheRECfield,wereleasethecleaned
versionsofRefCOCO,RefCOCO+,andRefCOCOg.
Ref-L4: AComprehensiveRECBenchmarkforModernLMMEvaluation. WepresentRef-L4,
whereL4signifiesfourkeyaspects: aLargenumberoftestingsamples,Largediversityinobject
categoriesandinstancescales,Longreferringexpressions,andaLargevocabulary. Thesefeatures
makeRef-L4acomprehensivebenchmarkforassessingtheRECcapabilitiesofcontemporaryLMMs.
Table3providesadetailedcomparisonbetweenRef-L4andotherbenchmarksincludingRefCOCO,
RefCOCO+,andRefCOCOg. OurRef-L4benchmarkstandsoutduetothefollowingcharacteristics:
2Table3: ComparisonbetweenourRef-L4benchmarkandotherRECbenchmarks,includingRef-
COCO[71],RefCOCO+[71],andRefCOCOg[36]. Forthelatterthreebenchmarks,wecombine
theirvalidationandtestsetsforstatistics. Theinstancesizeandimagesizearerepresentedbytheir
respectivesquareroots. Avg. length: averagelengthofannotations. Vocab.: vocabularysize.
Avg. Instance Image
Benchmark Images Instances Annotations Categories Vocab.
Length Size Size
RefCOCO[71] 3,000 7,596 21,586 71 3.6 105-607 230-640 3,525
RefCOCO+[71] 3,000 7,578 21,373 71 3.6 105-607 230-640 4,387
RefCOCOg[36] 3,900 7,596 14,498 78 8.4 83-610 277-640 5,050
Ref-L4(Ours) 9,735 18,653 45,341 365 24.2 30-3,767 230-6,606 22,813
The pale green rectangular eraser features a depiction of a bear, accompanied by
the word "ERASER" inscribed in green. A transparent plastic covering with
patterns partially envelops it. Positioned at the bottom right corner of the picture,
the eraser rests on a cluttered desk surrounded by an assortment of artistic
materials and drawings.​
The game board is a square, wooden framework positioned at the lower part of
the picture, featuring a grid of tiny recessed circles containing circular tokens. It
is placed on the floor close to a shelf showcasing an assortment of objects, such
as a teapot and bottles. The elevated borders of the board indicate that it is
intended for gameplay, potentially involving tactics or positioning.
A decorative baseball with a unique red and gold color scheme, situated amongst
various baseball memorabilia.
Figure1: ExamplesfromourRef-L4benchmark. Weofferadetailedreferringexpressionforeach
targetinstancerepresentedbyaboundingbox. Zoominforbettervisualization.
• Large-Scale. Ref-L4includes9,735images,18,653uniqueinstances,andatotalof45,341
annotations,significantlysurpassingRefCOCO,RefCOCO+,andRefCOCOg. Forinstance,
RefCOCOgoffers3,900images,7,596instances,and14,498annotations.
• HighDiversity. Ref-L4features365uniquecategories. SincetheRefCOCOseriesderive
from the COCO 2014 dataset, they encompass up to 78 categories. Additionally, our
benchmark covers a wider range of instance scales, from 30 to 3,767, measured by the
squarerootoftheinstancearea.
• LengthyReferringExpressions. EachreferringexpressioninRef-L4isadetaileddescription
ofaspecificinstance, withlengthsrangingfrom3to117wordsandanaverageof24.2
words. In comparison, the average annotation lengths in RefCOCO, RefCOCO+, and
RefCOCOgare3.6,3.6,and8.4words,respectively. ExamplescanbefoundinFigure1.
• Extensive Vocabulary. Due to the detailed nature of the referring expressions, Ref-L4
boastsalargevocabularyof22,813words,whichisfourtosixtimeslargerthanthoseof
RefCOCO,RefCOCO+,andRefCOCOg.
EvaluationonRef-L4. Weconductanevaluationof24representativeLMMsthatcanperformthe
REC task. In addition to the standard accuracy metric, which considers predictions with an IoU
greaterthan0.5asaccurate(Acc ),wealsoreportaccuraciesathigherIoUthresholds: Acc and
0.5 0.75
Acc . Furthermore,weintroduceameanaccuracy(mAcc),calculatedastheaverageaccuracyfrom
0.9
Acc toAcc inincrementsof0.05. Togaindeeperinsightsintothemodels’capabilities,we
0.5 0.9
conductadetailedanalysisofRECperformanceacrossdifferentinstancescalesandcategories. The
3Ref-L4benchmarkandtheevaluationcodeareavailableathttps://github.com/JierunChen/
Ref-L4.
2 RelatedWork
RECandItsBenchmarks. ReferringExpressionComprehension(REC)[47,38,81,21,43,75]isa
taskthatinvolvesidentifyingaspecificobjectwithinanimagebasedonagivenreferringexpression.
Unlike object detection [30, 23, 52, 50, 4], which operates within fixed categories and a single
visualmodality, RECnecessitatesunderstandingfree-formtexttolocateobjectsofanycategory.
PhraseGrounding[44,67,14,34,27,76,61]issimilarbuttypicallyinvolvesshorterphrasesand
identifiesmultipleregions,whereasRECrequiresparsinglongerexpressionstopinpointasingle
uniqueregion. ThiscomplexitymakesRECanidealtaskforevaluatingemerginglargemultimodal
models. CurrentRECbenchmarkssuchasRefCOCO[71],RefCOCO+[71],andRefCOCOg[36]
includetensofthousandsofannotationsbutarelimitedbytheirshortexpressionlengths—averaging
3.6,3.6,and8.4words,respectively. Additionally,theyencompassfewerthan80categories,lacking
real-worlddiversity. OtherRECbenchmarks [33,8,48,7,64,24,58,10,3,12,11,18]areoften
designed for specific scenarios. For example, CLEVR-Ref+[33] focuses on simple objects like
boxes,spheres,andcylinders. SK-VG[8]integratespriorsceneknowledgeasadditionalinput,while
RefCrowd[48]targetsidentifyingapersonwithinacrowd. Bycontrast,weintroduceRef-L4,amore
generalandcomprehensivebenchmarkencompassing365categoriesand45,341annotations. Ref-L4
featuresexpressionsaveraging24.2wordsandavocabularyof22,813words,facilitatingtheaccurate
evaluationofRECmodelsoncomplexexpressionsanddiverseobjects.
REC Models. The evolution of REC models has transitioned from specialized models [20, 72,
32,54,82,68,83]togeneralistmodelsorlargemultimodalmodels(LMMs)[62,31,13,60,2,5,
66, 78, 73, 74, 45, 77, 63, 53, 35, 46, 22]. Notable examples of these LMMs include CogVLM-
Grounding[62], SPHINX [31, 13], ONE-PEACE [60], Qwen-VL-Chat [2], MiniGPTv2 [5], and
Lenna [66]. These models, benefiting from larger model sizes and extensive training on diverse
datasets,exhibitremarkableperformanceonconventionalRECdatasets. Forexample,CogVLM-
Groundingachievesanaccuracyof94.58%onRefCOCO(cleaned). Additionally,theperformance
gap among models is shrinking, with many LMMs surpassing 90% accuracy. This performance
saturationraisesconcernsabouttheadequacyofcurrentRECbenchmarksformakingmeaningful
comparisons. Inresponse,weproposeRef-L4,amorecomprehensiveandchallengingbenchmark.
Wehavealsoconductedrigorousevaluationsof24LMMmodels,offeringholisticcomparisonsthat
highlighttheirweaknessesandsuggestdirectionsforimprovement.
3 Ref-L4
3.1 BenchmarkCreation
DataSources. Ourbenchmarkisderivedfromtwosources: 1)ourcleanedvalidationandtestsets
oftheRefCOCO[71],RefCOCO+[71],andRefCOCOg[36]datasets;and2)thetestsetfromthe
large-scale object detection dataset Objects365 [52]. The Objects365 dataset provides a broader
rangeofcategories,varyinginstancesizes,higherimageresolutions,andmoreintricatescenes. In
theRefCOCOseries,eachinstanceincludesaboundingbox,acategoryname,andanextremelybrief
expressionlike“rightteddybea”. Incontrast,theObjects365benchmarklabelseachinstancewith
mainlyaboundingboxandtherelevantcategory.
For the RefCOCO (cleaned) series, we begin by consolidating duplicate images and instances,
resulting in a subset of 6,502 images containing 14,186 unique instances. For Objects365, we
select samples from its testing set based on several criteria: 1) each image has both height
and width greater than 800 pixels; 2) each image is sufficiently complex, containing more than
(cid:112)
10 categories and 20 instances; 3) each instance has a square normalized size (hw)/(HW)
greater than 0.05, where (h,w) represents the instance size and (H,W) denotes the image size;
4) we randomly sample N instances for each of the 365 classes defined in Objects365, with
N = min(35,thenumberofinstancesforthespecificclass); 5) we review and exclude instances
with erroneous bounding box annotations or those difficult to describe uniquely. For a few rare
classes,werelaxcriterion-1to512pixelsandcriterion-2to10instances. Consequently,wecollect
3,233 images and 4,467 instances from Objects365. Overall, our Ref-L4 benchmark comprises
9,735imagesand18,653instances,sourcedfromtheRefCOCOseriesandObjects365.
4Instance with: Please briefly describe the [Category Name] in one sentence.
1) Bounding Box
2) Category Name Or
3) Brief Expression Instance with:
1) Bounding Box
“Right teddy bear”
2) Category Name Crop
Draw a Red Circle on the Target
Expression
The pale green rectangular eraser features a depiction of a
bear, accompanied by the word "ERASER" inscribed in
GPT-4V
green.
You are a powerful referring expression generator. Given an image and Manual Review
a hint ([Expression]), please generate a discriminative and unambiguous
• Uniqueness
expression to describe the target instance highlighted by a red circle.
• Factuality
• Relevance
Final Referring Expression
• Harmlessness
The pale green rectangular eraser … artistic materials and drawings.​ • No Hallucinations
GPT-4V
Figure2: Pipelineofgeneratingareferringexpressionforatargetinstance.
ReferringExpressionGeneration. Givenatargetinstanceanditscorrespondingimage,weleverage
GPT-4Vwithhumanreviewersinthelooptogenerateitspreciseanddetailedreferringexpressions.
Figure2illustratesthethree-stepgenerationprocess:
Step-1: EachinstanceintheObjects365datasetislinkedtoaboundingboxandacategoryname. We
beginbycroppingtheseinstancesfromtheoriginalimages. Next,weinputeachcroppedareaalong
withthepromptdetailedinSectionB.1intoGPT-4Vtoproduceacontext-independentdescription.
ForinstancesfromtheRefCOCOseries, thisstepisomittedaseachinstancealreadyhasabrief
expression.
Step-2: DrawinginspirationfromrecentstudiesonGPT-4V[69],whereGPT-4Visabletopaymore
attentiontoinstanceshighlightedbyaredcirclewithinanimage,wesimilarlyencirclethetarget
instanceinredtofacilitateGPT-4Vingeneratingacontext-awarereferringexpression. Following
this,asdepictedinFigure2,weprocesstheimageandusethepromptoutlinedinSectionB.2to
generateacontext-awarereferringexpressionforeachinstance. WeinstructGPT-4Vtodescribe
various features such as color, size, position, and context. Additionally, we provide a hint (the
context-independentdescriptionfromStep-1)intheprompttomitigatehallucinationissues,resulting
inmoreaccuratedescriptions.
Step-3: Wemanuallyreviewallgeneratedreferringexpressionstocorrectanyhallucinationissues.
Weensurethateachexpressionuniquelydescribestheinstanceandisfactual,accurate,andharmless.
Annotation Expansion. To date, we have compiled 18,653 unique referring expressions, each
describingadistinctinstance. ToassesstherobustnessofRECmodelstodiverselanguageinputs,we
employatwo-stagerephrasingprocesstoexpandourbenchmark: 1)utilizingGPT-4withtheprompt
detailedinSectionB.3,togeneraterephrasedversionsofeachexpression;2)conductingamanual
reviewtoensurethattherephrasedexpressionsareunique,factual,relevant,andharmless. Conse-
quently,ourfinalRef-L4benchmarkencompasses9,735imageswith45,341referringexpressions,
eachaccuratelydescribingoneofthe18,653uniqueinstances.
3.2 Analysis
ExpressionLength. Figure3aillustratesthedistributionofexpressionlengthsacrossfourdifferent
datasets: RefCOCO,RefCOCO+,RefCOCOg,andourRef-L4. Duethethehighoverlapofdata
samples, RefCOCO and RefCOCO+ exhibit similar distributions, with a high density of shorter
expressionspeakingataround3.6words. RefCOCOgfeaturesslightlylongerexpressionsonaverage,
50.15
RefCOCO
RefCOCO
RefCOCO+
0.006 RefCOCO+
0.10 RefCOCOg
RefCOCOg
Ref-L4 (ours)
0.004 Ref-L4 (ours)
0.05
0.002
0.00
0 10 20 30 40 50 60 90117 30.350 100 300 1000 3767.3
Expression Length Instance Size
(a)Thedistributionofexpressionlength. (b)Thedistributionofinstancesize.
1500
100
50
10
5
1
Person Bottle S U LV adder StBe od p Sign Sh Sio dv ee l Ta Rbl ee cor Gd ole fr Clu TTb rr ao fp fih c y Light Crab Deer M So cp iss Eor gs gpla Sn at usag Se occer O Hy ast mier m Gel rao pn efrui Mt ark Ner oddl Des olphi Mn onkey Tennis
(c)Thedistributionofinstancenumbersover365categories.
Figure3: Analysisofreferringexpressionlength,instancesize,andcategorydistribution.
side positioned partially of
shirt situated next on
man wearing directly in
image featuring slightly with
table standing close to
woman located far by
hand adorned away at
individual has partly behind
person dressed just from
plate seated prominently near
0 3000 6000 0 4000 8000 0 1000 2000 0 15000 30000
(a)Nouns. (b)Verbs. (c)Adverbs. (d)Prepositions.
Figure4: Thefrequencyofthe10mostfrequentlyusedwordsineachpart-of-speechcategory,as
parsedusingtheSpaCylibrary.
peaking at approximately 8.4 words. In contrast, our Ref-L4 displays a significantly different
distribution,withexpressionsrangingmuchlonger,peakingataround24.2wordsandhavingalong
tailextendingupto117words. ThissuggeststhatourRef-L4benchmarkisdesignedtopushthe
boundariesofcurrentRECmodels,requiringthemtoprocessandcomprehendmoreintricateand
detaileddescriptions.
Instance Size. In Figure 3b, we present a density plot comparing the instance sizes across four
(cid:112)
benchmarks. Wedefinetheinstancesizeasthesquarerootofthenormalizedsize, (hw)/(HW),
where(h,w)representsthedimensionsoftheinstanceand(H,W)representsthedimensionsofthe
image. Allbenchmarksexhibitapeakdensityaroundaninstancesizeof160. OurRef-L4benchmark
showsawiderdistributionrangecomparedtotheotherthree,indicatingthatourRef-L4capturesa
broaderspectrumofinstancesizes.
Categories. OurRef-4Lbenchmarkcomprises18,653instancesspanning365distinctcategories,
providingmorecomplexanddiverseevaluationscenarios. Incontrast,RefCOCOandRefCOCO+
6
ytisneD
ycneuqerF
ytisneDconsistsof71categories,whileRefCOCOgcovers78categories. Figure3cpresentsthedistribution
of instances among these 365 categories. Notably, the ten categories with the highest number
of instances are “Person”, “Chair”, “Hat”, “Desk”, “Lamp”, “Cabinet/shelf”, “Car”, “Sneakers”,
“Handbag/Satchel”,and“‘Flag”.
Vocabulary. Ourbenchmark’sreferringexpressionscompriseavocabularytotaling22,813unique
words. ThisissignificantlylargerthanthevocabularysizesofRefCOCO,RefCOCO+, andRef-
COCOg, whichare3,525, 4,387, and5,050words, respectively. Figure4illustratesthe10most
frequentlyusednouns,verbs,adverbs,andprepositions.
3.3 Evaluation
EvaluationMetrics. Weproposethreedistinctevaluationprotocols:
1. Accuracy. This is the conventional metric used in REC. For a given referring expression and
correspondingimage,thetargetinstanceisconsideredsuccessfullylocalizediftheIoUbetween
thepredictedboundingboxandthegroundtruthexceeds0.5. Accuracyisthencalculatedasthe
ratioofsuccessfullylocalizedsamplestothetotalnumberofsamples,referredtoasAcc in
0.5
thiswork. TobetterassessthelocalizationcapabilitiesofmodernRECmodels,wealsoreport
accuraciesathigherIoUthresholds: Acc ,Acc ,andmAcc,whichistheaverageaccuracy
0.75 0.9
fromAcc toAcc inincrementsof0.05.
0.5 0.9
2. Scale-AwarePerformance. Togaindeeperinsightsintomodelcapabilities,wereportperformance
basedoninstancesizes: small,medium,andlarge. Thesizeofaninstanceisdefinedasthesquare
(cid:112)
rootofitsarea, (hw),where(h,w)arethedimensionsoftheinstance. Smallinstancesare
thosewithasizelessthan128,mediuminstancesarebetween128and256,andlargeinstances
exceed256. Intotal,thereare9345,23280,and12716referringexpressionsdescribing2,954
small,10,442medium,and5,257largeinstances,respectively.
3. Per-CategoryPerformance. Ourbenchmarkencompassesawiderangeofcategories,upto365in
total. Weprovideanevaluationprotocoltoassessperformanceonaper-categorybasis.
BenchmarkDivision. Modernlargemultimodalmodels(LMMs)thatareabletohandletheREC
tasktypicallyuseunrestrictedandextensivedatafortraining. OurRef-L4benchmarkisdesignedto
assessthecapabilitiesoftheseadvancedmodelswithoutimposinganylimitationsonthetrainingdata
sources. Thebenchmarkisdividedintotwosubsets: avalidationset,comprising30%ofthedata
with7,231images,10,311instances,and13,420referringexpressions;andatestset,comprising
70%ofthedatawith9,467images,17,242instances,and31,921referringexpressions. Giventhat
ourbenchmarkincludesinstancesfrom365categories,weensurethateachcategoryhasatleastone
sampleinboththevalidationandtestsets. Whileweprovidethesetwosplits, weencouragethe
combineduseofbothsetsformodelevaluation,especiallyinthecurrentLMMera,wheretheuseof
unrestrictedtrainingdataisprevalent.
4 Experiments
MainResult. Weevaluateatotalof24LMMsthatcanperformtheRECtask,dividingtheminto
twocategoriesbasedontheiroutputtype: thosethatproduceboundingboxesandthosethatproduce
segmentationmasks. Formodelsthatoutputsegmentationmasks,weconvertthesemasksintotight
boundingboxestoenableevaluationonourRef-L4benchmark. Table4presentstheperformance
ofthesemodelsonthevalidationset,testset,andthecombinedset,usingthemetricsdefinedin
Section3.3. TheevaluationpromptofGPT-4VisavailableinSectionB.4. Amongthemodelsthat
outputboundingboxes,CogVLM-Grounding[62]showsthebestperformance,whileGlaMM[49]
leadsinperformanceamongthemodelsthatoutputmasks.
Category-WisePerformance. Eachinstanceinourbenchmarkisassignedacategorylabelfromone
of365classes. Figure5illustratestheperformanceofthetopfourmodelsacrossthesecategories,
sortedindescendingorderbasedontheiraverageper-categoryperformance. Theresultsindicatea
trainingbiasissue,asallfourmodelsexhibitpoorperformanceonsomecommoncategories.
Scale-AwareEvaluation. InSection3.3,wepresentascale-awareevaluationtoassessthemodel’s
abilitytohandledifferentinstancescales. Specifically,wecategorizeallsamplesinourbenchmark
intothreesetsbasedoninstancesize: small,medium,andlarge. Theperformanceof24modelsis
7Table4: Performanceevaluationacross24modelsonourRef-L4benchmark. NVIDIAA100GPUs
(80G)areutilized. Thesymbol†denotesmodelsthatoutputssegmentationmasks.
Val+Test Val Test
Model
Acc Acc Acc mAcc mAcc mAcc
0.5 0.75 0.9
GPT-4V[39–41] 9.91 1.19 0.12 2.88 2.96 2.85
KOSMOS-2[42] 48.53 38.34 17.54 34.72 34.89 34.64
OFA-Tiny[59] 55.21 43.22 27.70 41.44 41.53 41.40
OFA-Large[59] 72.53 62.31 45.02 59.17 59.42 59.07
Ferret-7b[70] 57.54 42.44 21.01 40.29 40.31 40.28
Ferret-13b[70] 64.44 49.04 27.46 46.88 47.31 46.71
GroundingGPT[29] 60.84 40.48 12.00 38.19 38.42 38.09
Shikra-7b[6] 65.06 39.62 10.45 38.60 38.91 38.47
Lenna[66] 65.90 58.55 45.58 55.69 55.88 55.60
MiniGPTv2[5] 66.93 50.50 25.30 47.15 47.43 47.03
Qwen-VL-Chat[2] 73.80 58.05 37.16 55.94 56.18 55.83
ONE-PEACE[60] 70.82 60.09 36.12 55.07 55.49 54.89
SPHINX-MoE[13] 66.23 44.90 15.32 42.38 42.80 42.21
SPHINX-MoE-1k[13] 74.45 62.70 38.85 58.07 58.35 57.95
SPHINX[31] 74.78 53.65 21.15 50.09 50.33 49.99
SPHINX-1k[31] 78.52 62.17 32.95 57.57 57.91 57.42
SPHINX-v2-1k[31] 81.31 70.49 46.59 65.39 65.67 65.27
CogVLM-Grounding[62] 81.70 70.77 48.35 66.09 66.25 66.02
PixelLM-7B†[51] 41.83 27.57 13.32 27.10 27.09 27.11
PixelLM-13B†[51] 49.89 35.37 18.42 34.10 34.52 33.92
LISA-Explanatory†[25] 65.12 52.35 38.26 50.77 50.89 50.72
LISA†[25] 66.23 54.02 39.73 52.18 52.44 52.07
PSALM†[79] 67.26 58.22 44.11 55.46 55.68 55.37
GlaMM†[49] 71.90 60.27 45.15 57.89 58.16 57.78
CogVLM-Grounding SPHINX-v2-1k Qwen-VL-Chat ONE-PEACE
100
75
50
25
0
1 53 105 157 209 261 313 365
Sorted Class Index
Figure5: Category-wiseperformanceofthefourtop-performingmodelsontheval+testset,sortedin
descendingorderbasedontheiraverageper-categoryperformance. Theperformanceofallmodels
canbefoundinSectionC.1.
detailedinTable5. Amongthebounding-box-outputmodels,CogVLM-Grounding[62]excelswith
smallandmediuminstances,whileSPHINX-v2-1k[31]achievesthebestperformancewithlarge
instances. Formask-outputmodels,GlaMM[49]outperformsallothermodelsacrossallthreesets.
Evaluation on Diverse Data Sources. Our benchmark is derived from COCO and Objects365
datasets. Weassesstheperformanceofthetopfourmodelswithboundingboxoutputsandthetop
twomodelswithmaskoutputsacrossvarioussubsetsoriginatingfromeitherCOCOorObjects365.
Thesesubsetsare: 1)theCOCO-derivedset(referredtoas“COCO”);2)asubsetfromObjects365,
wheretheinstanceshavecategoriesthatalsoexistinCOCO(referredtoas“O365-P1”);3)another
8
ccAmTable5: Scale-awareevaluationacross24modelsonourRef-L4benchmark.
SmallSize MediumSize LargeSize
Model
Acc mAcc Acc mAcc Acc mAcc
0.5 0.5 0.5
GPT-4V[39–41] 2.13 0.49 10.29 2.78 14.93 4.83
KOSMOS-2[42] 24.19 11.63 46.95 32.91 69.32 54.98
OFA-Tiny[59] 17.91 11.49 65.13 49.00 64.46 49.61
OFA-Large[59] 40.13 27.07 81.03 66.49 80.78 69.36
Ferret-7b[70] 30.93 14.57 62.40 43.72 68.18 52.92
Ferret-13b[70] 36.46 17.88 70.50 51.86 73.92 59.09
GroundingGPT[29] 24.43 10.28 67.67 41.04 75.09 53.47
Shikra-7b[6] 43.91 18.50 75.98 46.27 60.60 39.34
Lenna[66] 31.02 23.48 72.90 61.53 78.72 68.66
MiniGPTv2[5] 32.99 14.85 73.67 51.16 79.52 63.53
Qwen-VL-Chat[2] 47.66 26.26 79.80 61.06 82.01 68.37
ONE-PEACE[60] 22.18 13.98 83.26 63.39 83.81 70.04
SPHINX-MoE[13] 39.48 16.39 72.97 46.38 73.55 54.17
SPHINX-MoE-1k[13] 58.96 37.61 77.80 61.53 79.70 66.77
SPHINX[31] 48.82 22.08 80.56 54.10 83.27 63.34
SPHINX-1k[31] 59.48 33.21 82.95 61.82 84.40 67.68
SPHINX-v2-1k[31] 65.23 43.43 84.00 68.45 88.21 75.91
CogVLM-Grounding[62] 75.06 52.85 86.43 71.31 77.91 66.25
PixelLM-7B†[51] 8.25 4.05 43.90 27.33 62.72 43.64
PixelLM-13B†[51] 17.05 8.54 53.40 35.48 67.59 50.34
LISA-Explanatory†[25] 39.11 27.16 70.03 54.61 75.25 61.09
LISA†[25] 39.24 27.49 71.17 56.05 77.01 63.22
PSALM†[79] 37.35 28.43 75.06 61.79 74.97 63.74
GlaMM†[49] 47.07 34.36 77.17 62.28 80.50 67.14
75
COCO
50 O365-P1
O365-P2
25
CogVLM-Ground Sin Pg HINX-v2-1k ONE-PEACE Qwen-VL-Chat GlaMM PSALM
Figure6: Evaluationofsixmodelsonvariousdatasources,withmAccactingasthemetric. The
resultsofallmodelscanbefoundinSectionC.2.
subset from Objects365, where the instances have categories not found in COCO (referred to as
“O365-P2”).Figure6presentstheperformanceofthesemodelsacrossthethreesubsets.The“COCO”
setshowshigheraccuracycomparedtotheothertwosets,partiallybecausemostmodelsaretrained
ontheRefCOCOseriesandhavelimitedexposuretoObjects365images. “O365-P1”exhibitshigher
accuracythan“O365-P2”,asthelatterincludesmorerarecategories.
5 Conclusion
Inthiswork,wefirstpointoutseverallimitationsofthecurrentRECbenchmarks,suchassubstantial
labelinginaccuraciesandverybriefreferringexpressions. Tobetterassessthecapabilitiesofmodels,
particularlythoseLMMsthatcanperformtheRECtask,wepresentRef-L4,whichfeaturesfourkey
characteristics: 1)alarge-scaledatasetwith45,341annotations;2)awiderangeofobjectcategories
and varying instance scales; 3) detailed referring expressions; and 4) an extensive vocabulary
comprising22,813uniquewords. Weevaluateatotalof24modelsusingvariousevaluationprotocols.
WewishthatRef-L4couldserveasavaluableresourceforresearchersanddevelopers,fosteringthe
developmentofmorerobustandversatileRECmodelsintheLMMera.
9
ccAmReferences
[1] J.Bai,S.Bai,Y.Chu,Z.Cui,K.Dang,X.Deng,Y.Fan,W.Ge,Y.Han,F.Huang,etal. Qwentechnical
report. arXivpreprintarXiv:2309.16609,2023.
[2] J.Bai,S.Bai,S.Yang,S.Wang,S.Tan,P.Wang,J.Lin,C.Zhou,andJ.Zhou. Qwen-vl:Afrontierlarge
vision-languagemodelwithversatileabilities. arXivpreprintarXiv:2308.12966,2023.
[3] Y. Bu, L. Li, J. Xie, Q. Liu, Y. Cai, Q. Huang, and Q. Li. Scene-text oriented referring expression
comprehension. IEEETransactionsonMultimedia,2022.
[4] N.Carion,F.Massa,G.Synnaeve,N.Usunier,A.Kirillov,andS.Zagoruyko. End-to-endobjectdetection
withtransformers. InEuropeanconferenceoncomputervision,pages213–229.Springer,2020.
[5] J. Chen, D. Zhu, X. Shen, X. Li, Z. Liu, P. Zhang, R. Krishnamoorthi, V. Chandra, Y. Xiong, and
M.Elhoseiny. Minigpt-v2: largelanguagemodelasaunifiedinterfaceforvision-languagemulti-task
learning. arXivpreprintarXiv:2310.09478,2023.
[6] K.Chen, Z.Zhang, W.Zeng, R.Zhang, F.Zhu, andR.Zhao. Shikra: Unleashingmultimodalllm’s
referentialdialoguemagic. arXivpreprintarXiv:2306.15195,2023.
[7] Z.Chen,P.Wang,L.Ma,K.-Y.K.Wong,andQ.Wu. Cops-ref:Anewdatasetandtaskoncompositional
referringexpressioncomprehension. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pages10086–10095,2020.
[8] Z.Chen,R.Zhang,Y.Song,X.Wan,andG.Li. Advancingvisualgroundingwithsceneknowledge:
Benchmarkandmethod. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages15039–15049,2023.
[9] W.-L.Chiang,Z.Li,Z.Lin,Y.Sheng,Z.Wu,H.Zhang,L.Zheng,S.Zhuang,Y.Zhuang,J.E.Gonzalez,
etal. Vicuna: Anopen-sourcechatbotimpressinggpt-4with90%*chatgptquality. Seehttps://vicuna.
lmsys.org(accessed14April2023),2(3):6,2023.
[10] V. Cirik, T. Berg-Kirkpatrick, and L.-P. Morency. Refer360 degree: A referring expression recogni-
tiondatasetin360degreeimages. InProceedingsofthe58thAnnualMeetingoftheAssociationfor
ComputationalLinguistics,pages7189–7202,2020.
[11] H.DeVries,F.Strub,S.Chandar,O.Pietquin,H.Larochelle,andA.Courville. Guesswhat?!visualobject
discoverythroughmulti-modaldialogue. InProceedingsoftheIEEEConferenceonComputerVisionand
PatternRecognition,pages5503–5512,2017.
[12] C.Gao,B.Yang,H.Wang,M.Yang,W.Yu,Y.Liu,andX.Bai. Textrec:Adatasetforreferringexpression
comprehensionwithreadingcomprehension. InInternationalConferenceonDocumentAnalysisand
Recognition,pages402–420.Springer,2023.
[13] P.Gao,R.Zhang,C.Liu,L.Qiu,S.Huang,W.Lin,S.Zhao,S.Geng,Z.Lin,P.Jin,etal.Sphinx-x:Scaling
dataandparametersforafamilyofmulti-modallargelanguagemodels. arXivpreprintarXiv:2402.05935,
2024.
[14] A.Gupta,P.Dollar,andR.Girshick. Lvis: Adatasetforlargevocabularyinstancesegmentation. In
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages5356–5364,
2019.
[15] Y. Hao, H. Song, L. Dong, S. Huang, Z. Chi, W. Wang, S. Ma, and F. Wei. Language models are
general-purposeinterfaces. arXivpreprintarXiv:2206.06336,2022.
[16] J.He,Y.Wang,L.Wang,H.Lu,J.-Y.He,J.-P.Lan,B.Luo,andX.Xie. Multi-modalinstructiontuned
llmswithfine-grainedvisualperception. arXivpreprintarXiv:2403.02969,2024.
[17] Z.Huang,Z.Zhang,Z.-J.Zha,Y.Lu,andB.Guo. Relationvlm:Makinglargevision-languagemodels
understandvisualrelations. arXivpreprintarXiv:2403.12801,2024.
[18] B.Jia,Y.Chen,H.Yu,Y.Wang,X.Niu,T.Liu,Q.Li,andS.Huang. Sceneverse: Scaling3dvision-
languagelearningforgroundedsceneunderstanding. arXivpreprintarXiv:2401.09340,2024.
[19] A.Q.Jiang,A.Sablayrolles,A.Roux,A.Mensch,B.Savary,C.Bamford,D.S.Chaplot,D.d.l.Casas,
E.B.Hanna,F.Bressand,etal. Mixtralofexperts. arXivpreprintarXiv:2401.04088,2024.
10[20] A.Kamath,M.Singh,Y.LeCun,G.Synnaeve,I.Misra,andN.Carion. Mdetr-modulateddetectionfor
end-to-endmulti-modalunderstanding. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages1780–1790,2021.
[21] S.Kazemzadeh,V.Ordonez,M.Matten,andT.Berg. Referitgame:Referringtoobjectsinphotographsof
naturalscenes.InProceedingsofthe2014conferenceonempiricalmethodsinnaturallanguageprocessing
(EMNLP),pages787–798,2014.
[22] M.KOSAREVA. Pushingthelimitsofvisualgrounding:Pre-trainingonlargesyntheticdatasets.
[23] R.Krishna,Y.Zhu,O.Groth,J.Johnson,K.Hata,J.Kravitz,S.Chen,Y.Kalantidis,L.-J.Li,D.A.Shamma,
etal. Visualgenome: Connectinglanguageandvisionusingcrowdsourceddenseimageannotations.
Internationaljournalofcomputervision,123:32–73,2017.
[24] S.Kurita,N.Katsura,andE.Onami.Refego:Referringexpressioncomprehensiondatasetfromfirst-person
perceptionofego4d. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pages15214–15224,2023.
[25] X.Lai,Z.Tian,Y.Chen,Y.Li,Y.Yuan,S.Liu,andJ.Jia.Lisa:Reasoningsegmentationvialargelanguage
model. arXivpreprintarXiv:2308.00692,2023.
[26] M.Lewis,Y.Liu,N.Goyal,M.Ghazvininejad,A.Mohamed,O.Levy,V.Stoyanov,andL.Zettlemoyer.
Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and
comprehension. arXivpreprintarXiv:1910.13461,2019.
[27] L.H.Li,P.Zhang,H.Zhang,J.Yang,C.Li,Y.Zhong,L.Wang,L.Yuan,L.Zhang,J.-N.Hwang,etal.
Groundedlanguage-imagepre-training. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pages10965–10975,2022.
[28] Y.Li,S.Bubeck,R.Eldan,A.DelGiorno,S.Gunasekar,andY.T.Lee. Textbooksareallyouneedii:
phi-1.5technicalreport. arXivpreprintarXiv:2309.05463,2023.
[29] Z.Li,Q.Xu,D.Zhang,H.Song,Y.Cai,Q.Qi,R.Zhou,J.Pan,Z.Li,V.T.Vu,etal. Lego:Language
enhancedmulti-modalgroundingmodel. arXivpreprintarXiv:2401.06071,2024.
[30] T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan,P.Dollár,andC.L.Zitnick. Microsoft
coco:Commonobjectsincontext. InComputerVision–ECCV2014:13thEuropeanConference,Zurich,
Switzerland,September6-12,2014,Proceedings,PartV13,pages740–755.Springer,2014.
[31] Z.Lin,C.Liu,R.Zhang,P.Gao,L.Qiu,H.Xiao,H.Qiu,C.Lin,W.Shao,K.Chen,etal. Sphinx:The
jointmixingofweights,tasks,andvisualembeddingsformulti-modallargelanguagemodels. arXiv
preprintarXiv:2311.07575,2023.
[32] J.Liu,L.Wang,andM.-H.Yang. Referringexpressiongenerationandcomprehensionviaattributes. In
ProceedingsoftheIEEEInternationalConferenceonComputerVision,pages4856–4864,2017.
[33] R.Liu,C.Liu,Y.Bai,andA.L.Yuille.Clevr-ref+:Diagnosingvisualreasoningwithreferringexpressions.
InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages4185–4194,
2019.
[34] S.Liu,Z.Zeng,T.Ren,F.Li,H.Zhang,J.Yang,C.Li,J.Yang,H.Su,J.Zhu,etal. Groundingdino:
Marryingdinowithgroundedpre-trainingforopen-setobjectdetection. arXivpreprintarXiv:2303.05499,
2023.
[35] C.Ma,Y.Jiang,J.Wu,Z.Yuan,andX.Qi.Groma:Localizedvisualtokenizationforgroundingmultimodal
largelanguagemodels. arXivpreprintarXiv:2404.13013,2024.
[36] J.Mao,J.Huang,A.Toshev,O.Camburu,A.L.Yuille,andK.Murphy. Generationandcomprehensionof
unambiguousobjectdescriptions. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pages11–20,2016.
[37] A.Meta. Introducingmetallama3:Themostcapableopenlyavailablellmtodate. MetaAI.,2024.
[38] V.K.Nagaraja,V.I.Morariu,andL.S.Davis. Modelingcontextbetweenobjectsforreferringexpression
understanding.InComputerVision–ECCV2016:14thEuropeanConference,Amsterdam,TheNetherlands,
October11–14,2016,Proceedings,PartIV14,pages792–807.Springer,2016.
[39] OpenAI. Gpt-4technicalreport,2023.
11[40] OpenAI. Gpt-4v(ision)systemcard. 2023. URLhttps://cdn.openai.com/papers/GPTV_System_
Card.pdf.
[41] OpenAI. Gpt-4v(ision) technical work and authors. 2023. URL https://cdn.openai.com/
contributions/gpt-4v.pdf.
[42] Z.Peng,W.Wang,L.Dong,Y.Hao,S.Huang,S.Ma,andF.Wei. Kosmos-2: Groundingmultimodal
largelanguagemodelstotheworld. arXivpreprintarXiv:2306.14824,2023.
[43] R.Pi,L.Yao,J.Gao,J.Zhang,andT.Zhang. Perceptiongpt:Effectivelyfusingvisualperceptionintollm.
arXivpreprintarXiv:2311.06612,2023.
[44] B.A.Plummer,L.Wang,C.M.Cervantes,J.C.Caicedo,J.Hockenmaier,andS.Lazebnik. Flickr30k
entities:Collectingregion-to-phrasecorrespondencesforricherimage-to-sentencemodels. InProceedings
oftheIEEEinternationalconferenceoncomputervision,pages2641–2649,2015.
[45] S.Pramanick,G.Han,R.Hou,S.Nag,S.-N.Lim,N.Ballas,Q.Wang,R.Chellappa,andA.Almahairi.
Jackofalltasks,masterofmany:Designinggeneral-purposecoarse-to-finevision-languagemodel. arXiv
preprintarXiv:2312.12423,2023.
[46] L. Qi, Y.-W. Chen, L. Yang, T. Shen, X. Li, W. Guo, Y. Xu, and M.-H. Yang. Generalizable entity
groundingviaassistanceoflargelanguagemodel. arXivpreprintarXiv:2402.02555,2024.
[47] Y.Qiao,C.Deng,andQ.Wu. Referringexpressioncomprehension:Asurveyofmethodsanddatasets.
IEEETransactionsonMultimedia,23:4426–4440,2020.
[48] H.Qiu,H.Li,T.Zhao,L.Wang,Q.Wu,andF.Meng. Refcrowd: Groundingthetargetincrowdwith
referringexpressions. InProceedingsofthe30thACMInternationalConferenceonMultimedia,pages
4435–4444,2022.
[49] H.Rasheed,M.Maaz,S.Shaji,A.Shaker,S.Khan,H.Cholakkal,R.M.Anwer,E.Xing,M.-H.Yang,and
F.S.Khan. Glamm:Pixelgroundinglargemultimodalmodel. arXivpreprintarXiv:2311.03356,2023.
[50] J.Redmon,S.Divvala,R.Girshick,andA.Farhadi.Youonlylookonce:Unified,real-timeobjectdetection.
InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages779–788,2016.
[51] Z.Ren, Z.Huang, Y.Wei, Y.Zhao, D.Fu, J.Feng, andX.Jin. Pixellm: Pixelreasoningwithlarge
multimodalmodel. arXivpreprintarXiv:2312.02228,2023.
[52] S.Shao,Z.Li,T.Zhang,C.Peng,G.Yu,X.Zhang,J.Li,andJ.Sun. Objects365: Alarge-scale,high-
qualitydatasetforobjectdetection.InProceedingsoftheIEEE/CVFinternationalconferenceoncomputer
vision,pages8430–8439,2019.
[53] H.Shen,T.Zhao,M.Zhu,andJ.Yin. Groundvlp: Harnessingzero-shotvisualgroundingfromvision-
languagepre-trainingandopen-vocabularyobjectdetection. InProceedingsoftheAAAIConferenceon
ArtificialIntelligence,volume38,pages4766–4775,2024.
[54] W.Su,X.Zhu,Y.Cao,B.Li,L.Lu,F.Wei,andJ.Dai. Vl-bert:Pre-trainingofgenericvisual-linguistic
representations. arXivpreprintarXiv:1908.08530,2019.
[55] H.Touvron,T.Lavril,G.Izacard,X.Martinet,M.-A.Lachaux,T.Lacroix,B.Rozière,N.Goyal,E.Hambro,
F.Azhar,etal. Llama:Openandefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,
2023.
[56] H.Touvron,L.Martin,K.Stone,P.Albert,A.Almahairi,Y.Babaei,N.Bashlykov,S.Batra,P.Bhargava,
S.Bhosale,etal. Llama2:Openfoundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,
2023.
[57] H.Wang,H.Tang,L.Jiang,S.Shi,M.F.Naeem,H.Li,B.Schiele,andL.Wang. Git:Towardsgeneralist
visiontransformerthroughuniversallanguageinterface. arXivpreprintarXiv:2403.09394,2024.
[58] P.Wang,D.Liu,H.Li,andQ.Wu. Givemesomethingtoeat:referringexpressioncomprehensionwith
commonsenseknowledge.InProceedingsofthe28thACMInternationalConferenceonMultimedia,pages
28–36,2020.
[59] P.Wang,A.Yang,R.Men,J.Lin,S.Bai,Z.Li,J.Ma,C.Zhou,J.Zhou,andH.Yang. Ofa: Unifying
architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In
InternationalConferenceonMachineLearning,pages23318–23340.PMLR,2022.
12[60] P.Wang,S.Wang,J.Lin,S.Bai,X.Zhou,J.Zhou,X.Wang,andC.Zhou. One-peace: Exploringone
generalrepresentationmodeltowardunlimitedmodalities. arXivpreprintarXiv:2305.11172,2023.
[61] Q.Wang,H.Tan,S.Shen,M.W.Mahoney,andZ.Yao. Maf: Multimodalalignmentframeworkfor
weakly-supervisedphrasegrounding. arXivpreprintarXiv:2010.05379,2020.
[62] W.Wang,Q.Lv,W.Yu,W.Hong,J.Qi,Y.Wang,J.Ji,Z.Yang,L.Zhao,X.Song,etal. Cogvlm:Visual
expertforpretrainedlanguagemodels. arXivpreprintarXiv:2311.03079,2023.
[63] W.Wang,Z.Chen,X.Chen,J.Wu,X.Zhu,G.Zeng,P.Luo,T.Lu,J.Zhou,Y.Qiao,etal.Visionllm:Large
languagemodelisalsoanopen-endeddecoderforvision-centrictasks. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[64] W.Wang,Y.Zhang,X.He,Y.Yan,Z.Zhao,X.Wang,andJ.Liu. Beyondliteraldescriptions:Understand-
ingandlocatingopen-worldobjectsalignedwithhumanintentions. arXivpreprintarXiv:2402.11265,
2024.
[65] Y.Wang,Z.Ji,D.Wang,Y.Pang,andX.Li. Towardsunsupervisedreferringexpressioncomprehension
withvisualsemanticparsing. Knowledge-BasedSystems,285:111318,2024.
[66] F.Wei,X.Zhang,A.Zhang,B.Zhang,andX.Chu. Lenna: Languageenhancedreasoningdetection
assistant. arXivpreprintarXiv:2312.02433,2023.
[67] C.Wu,Z.Lin,S.Cohen,T.Bui,andS.Maji. Phrasecut: Language-basedimagesegmentationinthe
wild. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
10216–10225,2020.
[68] B.Yan,Y.Jiang,J.Wu,D.Wang,P.Luo,Z.Yuan,andH.Lu. Universalinstanceperceptionasobject
discoveryandretrieval. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages15325–15336,2023.
[69] Z. Yang, L. Li, K. Lin, J. Wang, C.-C. Lin, Z. Liu, and L. Wang. The dawn of lmms: Preliminary
explorationswithgpt-4v(ision). arXivpreprintarXiv:2309.17421,9(1):1,2023.
[70] H.You,H.Zhang,Z.Gan,X.Du,B.Zhang,Z.Wang,L.Cao,S.-F.Chang,andY.Yang. Ferret:Referand
groundanythinganywhereatanygranularity. arXivpreprintarXiv:2310.07704,2023.
[71] L.Yu,P.Poirson,S.Yang,A.C.Berg,andT.L.Berg. Modelingcontextinreferringexpressions. In
ComputerVision–ECCV2016:14thEuropeanConference,Amsterdam,TheNetherlands,October11-14,
2016,Proceedings,PartII14,pages69–85.Springer,2016.
[72] L.Yu,Z.Lin,X.Shen,J.Yang,X.Lu,M.Bansal,andT.L.Berg. Mattnet:Modularattentionnetwork
forreferringexpressioncomprehension. InProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,pages1307–1315,2018.
[73] Y.Zhan,Y.Zhu,Z.Chen,F.Yang,M.Tang,andJ.Wang. Griffon:Spellingoutallobjectlocationsatany
granularitywithlargelanguagemodels. arXivpreprintarXiv:2311.14552,2023.
[74] Y.Zhan,Y.Zhu,H.Zhao,F.Yang,M.Tang,andJ.Wang. Griffonv2:Advancingmultimodalperception
withhigh-resolutionscalingandvisual-languageco-referring. arXivpreprintarXiv:2403.09333,2024.
[75] C.Zhang,W.Li,W.Ouyang,Q.Wang,W.-S.Kim,andS.Hong.Referringexpressioncomprehensionwith
semanticvisualrelationshipandwordmapping. InProceedingsofthe27thACMInternationalConference
onMultimedia,pages1258–1266,2019.
[76] H.Zhang,P.Zhang,X.Hu,Y.-C.Chen,L.Li,X.Dai,L.Wang,L.Yuan,J.-N.Hwang,andJ.Gao. Glipv2:
Unifyinglocalizationandvision-languageunderstanding. AdvancesinNeuralInformationProcessing
Systems,35:36067–36080,2022.
[77] H.Zhang,H.Li,F.Li,T.Ren,X.Zou,S.Liu,S.Huang,J.Gao,L.Zhang,C.Li,etal. Llava-grounding:
Groundedvisualchatwithlargemultimodalmodels. arXivpreprintarXiv:2312.02949,2023.
[78] H.Zhang,H.You,P.Dufter,B.Zhang,C.Chen,H.-Y.Chen,T.-J.Fu,W.Y.Wang,S.-F.Chang,Z.Gan,
etal. Ferret-v2: Animprovedbaselineforreferringandgroundingwithlargelanguagemodels. arXiv
preprintarXiv:2404.07973,2024.
[79] Z.Zhang,Y.Ma,E.Zhang,andX.Bai. Psalm: Pixelwisesegmentationwithlargemulti-modalmodel.
arXivpreprintarXiv:2403.14598,2024.
13[80] H.Zhao, W.Ge, andY.-c.Chen. Llm-optic: Unveilingthecapabilitiesoflargelanguagemodelsfor
universalvisualgrounding. arXivpreprintarXiv:2405.17104,2024.
[81] D.Zheng,T.Kong,Y.Jing,J.Wang,andX.Wang. Towardsunifyingreferenceexpressiongenerationand
comprehension. arXivpreprintarXiv:2210.13076,2022.
[82] Z.Zheng,W.Wang,S.Qi,andS.-C.Zhu.Reasoningvisualdialogswithstructuralandpartialobservations.
InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages6669–6678,
2019.
[83] X.Zou,J.Yang,H.Zhang,F.Li,L.Li,J.Wang,L.Wang,J.Gao,andY.J.Lee. Segmenteverything
everywhereallatonce. AdvancesinNeuralInformationProcessingSystems,36,2024.
14A LabelingErrorsinExistingBenchmarks
IntheRECtask,areferringexpressionshoulduniquelydescribeaninstance,whichisrepresentedby
anaccurateboundingbox. Wehaveidentifiedandvisualizedthreecommontypesoflabelingerrors
intheRefCOCO,RefCOCO+,andRefCOCOgbenchmarks: 1)non-uniquereferringexpressions
(Figure7),whichrefertomultipleinstanceswithinthesameimage;2)inaccurateboundingboxes
(Figure8);and3)misalignmentbetweentargetinstancesandtheirreferringexpressions(Figure9),
wherethereferringexpressionsareeitherambiguousordonotrefertoanyinstanceintheimage.
(a)manwithglasseson (b)ablackcolurchair (c)businfenceframe1
(d)anelephantwalkinginthegrass (e)awhitecomputerscreen (f)whitecouch
Figure7: Visualizationoflabelingerrors,whereareferringexpressionreferstomultipleinstances
withinthesameimage. Foreachsub-figure,wedisplaytheoriginalboundingboxannotationwitha
redrectangleandincludethecorrespondingreferringexpressioninthecaption.
(a)tailofelephant (b)redjacket (c)nameonoven
(d)pumpkin (e)aknifecuttingacake (f)whitehair
Figure8: Visualizationoflabelingerrors,wheretheboundingboxannotationsareinaccurate. For
eachsub-figure,wedisplaytheoriginalboundingboxannotationwitharedrectangleandincludethe
correspondingreferringexpressioninthecaption.
15(a)left (b)nexttohim (c)yep
(d)nottheslice (e) why is the game doing this (f)lasthotdogmessthing
checkershirt
Figure9: Visualizationoflabelingerrors,wherethereferringexpressionsareeitherambiguousor
donotrefertoanyinstanceintheimage. Foreachsub-figure,wedisplaytheoriginalboundingbox
annotationwitharedrectangleandincludethecorrespondingreferringexpressioninthecaption.
B Prompts
B.1 PromptforContext-IndependentDescriptionGeneration
Brieflydescribethe[CategoryName]inonesentence. Beginyourdescriptionwiththeobjectname,
includingadjectivesifappropriatetodescribeitscolororshape. Focusonlyonvisiblefeaturesand
avoidmentioningblurriness.
Inputimage: [CroppedImage].
B.2 PromptforContext-AwareDescriptionGeneration
Youareasophisticatedreferringexpressiongenerator. Yourtaskistogenerateaclearandspecific
descriptionforthetargetinstancehighlightedbyaredcircleintheprovidedimage,basedonagiven
hintandthefollowingcriteria:
Criteria 1: The description should enable individuals to understand and accurately identify the
specifiedregionwithintheimage.
Criteria 2: The description may should various attributes such as category, shape, size, color,
visibility,exposure,texture,orientation,absoluteposition,relativeposition,facialfeatures,clothing,
accessories, gestures, context, semantic attributes, emotions, age, gender, posture, action, and
especially interactions with other instances. The selection of features should be relevant to the
particularregionandtheimagecontext.
Criteria3: Theredcircleissolelyforhighlightingtheregionofinterest. Donotrefertoitinyour
descriptions.
Criteria 4: Avoid using unnecessary words like “look for”, “spot”, “observe”, “find”, “notice”,
“identify”,“outline”,“target”and“question”.
Criteria5: Ensurethatthesubjectofeachsentencematchesthesubjectgiveninthehints. Donot
incorrectlyusethesubjectastheobject.
Criteria6: Usethecorrectsingularorpluralformwhenreferringtothetarget,whichmaybeasingle
object,apairofobjects,oragroupofobjects.
16Criteria7: Integrateallrelevantinformationfromthehints,notingthatsomehintsmayberedundant
orcontainerrors.
Inputimage: [RawImage].
Hint: [Context-IndependentDescription].
B.3 PromptforRephrasingReferringExpressions
Rewritethesubsequentdescriptionwhilepreservingthemaininformation. Utilizevariedexpressions
andreorganizethesentencesifnecessary. Begineachsentencewiththesamesubjectbeingreferred
to.
Description: [TheReferringExpressiontobeRephrased].
B.4 PromptforGPT4-VEvaluation
Youareanexpertinreferringexpressioncomprehensionandlocalization. Yourtaskistolocatethe
objectintheimagebasedontheprovidedexpression. Thecoordinatesrangefromthetopleft(0,0)
tothebottomright([ImageWidth],[ImageHeight]). Pleaseprovidetheboundingboxintheformat
(x ,y ,x ,y ),where(x ,y )representsthetop-leftcornerand(x ,y )representsthebottom-right
0 0 1 1 0 0 1 1
corner.
Expression: [TheReferringExpression].
C MoreExperiments
C.1 Category-WisePerformance.
Figure5presentstheper-categoryperformanceofthetopfourmodels. InFigures10and11,we
showtheperformanceforall24modelsonaper-categorybasis,withmAccservingasthemetric,
alongwiththeaverageperformanceforeachmodelacrossallcategories.
C.2 EvaluationonDiverseDataSources.
Figure6illustratestheperformanceofsixmodelsacrossthreesubsets,namely“COCO”,“O365-P1”
and“O365-P2”. InFigure12,thecomprehensiveresultsof24modelsacrossthesamethreesubsets
aredisplayed.
D LimitationsandBroadImpacts
Ref-L4providesamorecomprehensiveanddetailedevaluationofRECcapabilities,helpingtobetter
understandandimprovetheperformanceoflargemultimodalmodelscapableofhandlingtheREC
task. The public availability of Ref-L4 and its evaluation code encourages further research and
collaboration,drivinginnovationandadvancementsinthefieldofRECandbeyond. WhileRef-L4
aimstocoverawiderangeofscenarios,itmaystillmissoutonspecificedgecasesoruniquecontexts
thatcouldbeencounteredinreal-worldapplications. Thedetailedandlengthyreferringexpressions
mightposeachallengeforcurrentmodels,requiringsignificantadvancementsinnaturallanguage
processingandcomprehensioncapabilities.
17CogVLM-Grounding SPHINX-v2-1k SPHINX-1k SPHINX
100
75
50
25
0
1 53 105 157 209 261 313 365
Sorted Class Index
(a)Theaverageperformanceacrossallcategories(dotlines)forCogVLM-Grounding[62],SPHINX-v2-1k[31],
SPHINX-1k[31],andSPHINX1[31]are52.56,46.40,36.01,and26.95,respectively.
CogVLM-Grounding SPHINX-MoE-1k Qwen-VL-Chat ONE-PEACE SPHINX-MoE
100
75
50
25
0
1 53 105 157 209 261 313 365
Sorted Class Index
(b)Theaverageperformanceacrossallcategories(dotlines)forSPHINX-MoE-1k[13],Qwen-VL-Chat[2],
ONE-PEACE[60],andSPHINX-MoE[13]are36.84,31.41,24.11,and18.77,respectively.
CogVLM-Grounding Lenna Shikra-7b MiniGPTv2 GroundingGPT
100
75
50
25
0
1 53 105 157 209 261 313 365
Sorted Class Index
(c)Theaverageperformanceacrossallcategories(dotlines)forLenna[66],Shikra-7b[6],MiniGPTv2[5],and
GroundingGPT[29]are34.30,21.22,21.13,and14.60,respectively.
Figure10: Category-wiseperformanceof24models(part-1),sortedinthesameorderasinFigure5.
WeuseCogVLM-Groundingasareferenceforcomparisonineachsub-figure.
18
ccAm
ccAm
ccAmCogVLM-Grounding OFA-Large Ferret-13b Ferret-7b OFA-Tiny
100
75
50
25
0
1 53 105 157 209 261 313 365
Sorted Class Index
(a)Theaverageperformanceacrossallcategories(dotlines)forOFA-Large[59],Ferret-13b[70],Ferret-7b[70]
andOFA-Tiny[59]are32.88,23.33,20.27,and15.37,respectively.
CogVLM-Grounding GlaMM PSALM KOSMOS-2 GPT-4V
100
75
50
25
0
1 53 105 157 209 261 313 365
Sorted Class Index
(b)Theaverageperformanceacrossallcategories(dotlines)forGlaMM[49],PSALM[79],KOSMOS-2[42]
andGPT-4V[39–41]are36.25,27.62,19.37,and1.42,respectively.
CogVLM-Grounding LISA LISA-Explanatory PixelLM-13B PixelLM-7B
100
75
50
25
0
1 53 105 157 209 261 313 365
Sorted Class Index
(c)Theaverageperformanceacrossallcategories(dotlines)forLISA[25],LISA-Explanatory[25],PixelLM-
13B[51]andPixelLM-7B[51]are31.22,29.87,13.19,and8.74,respectively.
Figure11: Category-wiseperformanceof24models(part-2),sortedinthesameorderasinFigure5.
WeuseCogVLM-Groundingasareferenceforcomparisonineachsub-figure.
19
ccAm
ccAm
ccAmCOCO O365-P1 O365-P2
CogVLM-Grounding
SPHINX-v2-1k
SPHINX-1k
SPHINX
SPHINX-MoE-1k
SPHINX-MoE
ONE-PEACE
Qwen-VL-Chat
MiniGPTv2
Lenna
Shikra-7b
GroundingGPT
Ferret-13b
Ferret-7b
OFA-Large
OFA-Tiny
KOSMOS-2
GPT-4V
GlaMM
PSALM
LISA
LISA-Explanatory
PixelLM-13B
PixelLM-7B
15 30 45 60 75
mAcc
Figure12: Evaluationof24modelsonvariousdatasources,withmAccactingasthemetric.
20