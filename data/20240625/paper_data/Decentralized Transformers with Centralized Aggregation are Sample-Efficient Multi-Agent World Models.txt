Decentralized Transformers with Centralized
Aggregation are Sample-Efficient Multi-Agent World
Models
YangZhang1,2∗, ChenjiaBai2†, BinZhao2,3, JunchiYan2,4, XiuLi1, XuelongLi2,5
1TsinghuaUniversity,2ShanghaiAILaboratory,3NorthwesternPolytechnicalUniversity
4SchoolofArtificialIntelligence,ShanghaiJiaoTongUniversity
5InstituteofArtificialIntelligence(TeleAI),ChinaTelecom
Abstract
Learningaworldmodelformodel-freeReinforcementLearning(RL)agentscan
significantly improve the sample efficiency by learning policies in imagination.
However,buildingaworldmodelforMulti-AgentRL(MARL)canbeparticularly
challengingduetothescalabilityissueinacentralizedarchitecturearisingfrom
a large number of agents, and also the non-stationarity issue in a decentralized
architecturestemmingfromtheinter-dependencyamongagents. Toaddressboth
challenges,weproposeanovelworldmodelforMARLthatlearnsdecentralized
localdynamicsforscalability,combinedwithacentralizedrepresentationaggrega-
tionfromallagents. Wecastthedynamicslearningasanauto-regressivesequence
modelingproblemoverdiscretetokensbyleveragingtheexpressiveTransformer
architecture,inordertomodelcomplexlocaldynamicsacrossdifferentagentsand
provideaccurateandconsistentlong-termimaginations. Asthefirstpioneering
Transformer-basedworldmodelformulti-agentsystems,weintroduceaPerceiver
Transformer as an effective solution to enable centralized representation aggre-
gationwithinthiscontext. ResultsonStarcraftMulti-AgentChallenge(SMAC)
showthatitoutperformsstrongmodel-freeapproachesandexistingmodel-based
methodsinbothsampleefficiencyandoverallperformance.
1 Introduction
Multi-AgentReinforcementLearning(MARL)hasmaderemarkableprogress,whichwasdriven
largelybymodel-freealgorithms[31]. However,duetothecomplexityofmulti-agentsystemsarising
fromlargestate-actionspaceandpartialobservability,suchalgorithmsusuallydemandextensive
interactionstolearncoordinativebehaviors[16]. Apromisingsolutionisbuildingaworldmodel
thatapproximatestheenvironment,whichhasexhibiteditssuperiorsampleefficiencycomparedto
model-freeapproachesinsingle-agentRL[9,52,11,12,13,14]. However,extendingthedesignof
worldmodelinsingle-agentdomaintothemulti-agentcontextencounterssignificantchallengesdue
totheuniquebiasesandcharacteristicsinherenttomulti-agentenvironments.
Thechallengesprimarilystemfromtwodifferentmeansformulti-agentdynamicslearning: central-
izedanddecentralized.Learningaworldmodeltoapproximatethecentralizeddynamicsencapsulates
theinter-dependencybetweenagentsbutstrugglestobescalabletoanincreasingnumberofagents,
whichleadstotheexponentialsurgeinspatialcomplexity[16,31]. Conversely,applyingadecentral-
izedworldmodeltoapproximatingthelocaldynamicsofeachagentmitigatesthescalabilityissue
∗WorkdoneduringaninternshipatShanghaiAILab.
†Correspondenceto:ChenjiaBai<baichenjia@pjlab.org.cn>
Preprint.Underreview.
4202
nuJ
22
]GL.sc[
1v63851.6042:viXrayetincursnon-stationarity,asunexpectedinterventionsfromotheragentsmayoccurineachagent’s
individualenvironment[32]. Furthermore,beyondtheseuniquechallengesinherenttomulti-agent
scenarios, existing model-based MARL approaches [47, 5, 48] excessively neglect the fact that
the policy learned in imaginations of the world model heavily relies on the quality of imagined
trajectories[30]. Ittherebynecessitatesaccuratelong-termprediction,especiallywithrespecttothe
non-stationarylocaldynamics. InspiredbythecapabilityofTransformer[45]inmodelingcomplex
sequencesandlong-termdependency[2,4,30],weseektoconstructaTransformer-basedworld
model within the multi-agent context for decentralized local dynamics together with centralized
featureaggregation,combiningthebenefitsoftwodistinctivedesigns.
Inthispaper,weintroduceMARIE(Multi-Agentauto-RegressiveImaginationforEfficientlearning),
thefirstTransformer-basedmulti-agentworldmodelforsample-efficientpolicylearning. Specifically,
thehighlightsofthispaperare:
1. Totackletheinherentchallengeswithinthemulti-agentcontext,webuildaneffectiveworldmodel
viascalabledecentralizeddynamicsmodelingandessentialcentralizedrepresentationaggregating,
whichmirrorstheprincipleofCentralizedTrainingandDecentralizedExecution.
2. Toenableaccurateandconsistentlong-termimaginationsfromthenon-stationarylocaldynamics,
wecastthedecentralizeddynamicslearningassequencemodelingoverdiscretetokensbylever-
aginghighlyexpressiveTransformerarchitectureasthebackbone. Inparticular,wesuccessfully
presentthefirstTransformer-basedworldmodelformulti-agentsystems.
3. WhileitremainsopenforhowtoeffectivelyenablecentralizedrepresentationwiththeTransformer
as the backbone, we achieve it by innovatively introducing a Perceiver Transformer [19] for
efficientglobalinformationaggregationacrossallagents.
4. ExperimentsontheStarcraftMulti-AgentChallenge(SMAC)[38]benchmarkinlowdataregime
showMARIEoutperformsbothmodel-freeandexistingmodel-basedMARLmethodsw.r.t. both
sampleefficiencyandoverallperformanceanddemonstratetheeffectivenessofMARIE.
2 RelatedWorksandPreliminaries
Multi-AgentReinforcementLearning. Inamodel-freesetting,atypicalapproachforcooperative
MARLiscentralizedtraininganddecentralizedexecution(CTDE),whichtacklesthescalability
and non-stationarity issues in MARL. During the training phase, it leverages global information
tofacilitateagents’policylearning;whileduringtheexecutionphase,itblindsitselfandhasonly
accesstothepartialobservationaroundeachagentformulti-agentdecision-making. Model-free
MARLmethodswiththisparadigmcanbedividedinto2categories: value-based[42,35,41,46]
andpolicy-based[28,7,18,37,27,25,33,49,51,50]. Incontrasttomodel-freeapproaches,model-
basedMARLalgorithmsremainfairlyunderstudied. MAMBPO[47]incorporatesMBPO-style[20]
techniquesintomulti-agentpolicylearningundertheCTDEframework. Tesseract[29]introduces
thetensorisedBellmanequationandevaluatestheQ-valuefunctionusingDynamicProgramming
(DP)togetherwithanestimatedenvironmentmodel. Similartooursettingwhereagentslearninside
ofanapproximateworldmodel,MAMBA[5]integratesthebackboneproposedinDreamerV2[11]
with an attention mechanism across agents to sustain an effective world model in environments
withanarbitrarynumberofagents,whichleadstonotablysuperiorsampleefficiencytoexisting
model-freeapproaches. Intermsofmodel-basedalgorithmcoupledwithplanning,MAZero[26]
expands the MCTS planning-based Muzero [39] framework to the model-based MARL settings.
However,learning-basedorplanning-basedpoliciesinthesetwoapproachesarebothoverlycoupled
withtheirworldmodels,downgradingtheirinferenceefficiencyandfurtherlimitingexpansionin
combinationswithotherpopularmodel-freeapproaches.
Learningbehaviorswithintheimaginationofworldmodels. TheDynaarchitecture[43]first
emphasizestheutilityofanestimateddynamicsmodelinfacilitatingthetrainingofthevaluefunction
andpolicy. Inspiredbythecognitivesystemofhumanbeings,theconceptofworldmodel[8]is
initiallyintroducedbycomposingavariationalAuto-Encoder(VAE)[24]andarecurrentnetworkto
mimicthecompleteenvironmentaldynamics,thenanartificialagentistrainedentirelyinsidethe
hallucinatedimaginationgeneratedbytheworldmodel. SimPLe[52]showsthataPPOpolicy[40]
learnedinapredictivemodeldelivererasuper-humanperformanceinAtaridomains. Dreamer[9]
builtstheworldmodeluponaRecurrentStateSpaceModel(RSSM)[10]thatcombinesthedetermin-
isticlatentstatewiththestochasticlatentstatetoallowthemodeltonotonlycapturemultiplefutures
butalsorememberinformationovermulti-steps. DreamerV2[11]furtherdemonstratestheadvantage
2Joint obs-action sequence
Agents
𝑛𝑛 𝐾𝐾+1 ×𝐷𝐷
… …
1:𝑛𝑛 𝒓𝒓𝑡𝑡𝑖𝑖 𝜸𝜸𝑡𝑡𝑖𝑖 𝒙𝒙1𝑡𝑡,1 𝒙𝒙1 𝑡𝑡,𝐾𝐾 𝒂𝒂1𝑡𝑡 Late𝒙𝒙𝑡𝑡𝑛𝑛 n,1
t array
𝒙𝒙𝑡𝑡𝑛𝑛 ,𝐾𝐾 𝒂𝒂𝑡𝑡𝑛𝑛
ML𝑜𝑜P𝑖𝑖− E1 n𝑜𝑜c𝑖𝑖 o𝑜𝑜d𝑖𝑖 e+ r1 𝒙𝒙𝑡𝑡𝑖𝑖 ,2 𝒙𝒙𝑡𝑡𝑖𝑖 ,3 𝒙𝒙𝑡𝑡𝑖𝑖 ,𝐾𝐾 𝒙𝒙𝑡𝑡𝑖𝑖 +1 ,1 K V (𝑛𝑛×…
𝐷𝐷)
Cross Q
Codebook 𝐸𝐸 Shared Transformer Attention
Quantize Agents
𝒵𝒵 1:𝑛𝑛 …
(⋅)
𝒙𝒙1𝑖𝑖 𝒙𝒙2𝑖𝑖 𝒙𝒙3𝑖𝑖 𝒙𝒙𝐾𝐾𝑖𝑖
𝒙𝒙𝑡𝑡𝑖𝑖 ,1 𝒙𝒙𝑡𝑡𝑖𝑖 ,2 𝒙𝒙𝑡𝑡𝑖𝑖 ,3 𝒙𝒙𝑡𝑡𝑖𝑖 ,𝐾𝐾 𝒂𝒂𝑡𝑡𝑖𝑖 𝒆𝒆𝑡𝑡𝑖𝑖 Perceiver Transformer
MLP Decoder Agent-wise
Aggregation
𝐷𝐷 𝒆𝒆1𝑡𝑡 𝒆𝒆𝑡𝑡2 𝒆𝒆𝑡𝑡3 𝒆𝒆𝑡𝑡𝑛𝑛
Observation tokens Aggregated features
Legend:
Action tokens Invalid outputs
… …
𝑖𝑖−1 𝑖𝑖 𝑖𝑖+1
Figure 1:𝑜𝑜�Ov𝑜𝑜�er𝑜𝑜�view of the proposed world model architecture in MARIE. VQ-VAE (left) maps
local observations oi of each agent i into discrete latent codes (xi,...,xi ), where (E,D,Z) is
1 K
sharedacrossallagents. Togetherwithdiscreteactions,thisprocessformslocaldiscretesequences
(...,xi ,...,xi ,ai,...)ofeachagent. ThenthePerceiver(right)performsaggregationofjointdis-
t,1 t,K t
cretesequencesofallagents(x1 ,...,x1 ,a1,...,xn ,...,xn ,an)independentlyateachtimestep
t,1 t,K t t,1 t,K t
t, and inserts the aggregated global representations (e1,e2,...,en) into original local discrete se-
t t t
quencesrespectively. Theresultingsequences(...,xi ,...,xi ,ai,ei...)containrichinformation
t,1 t,K t t
betweentransitionsinlocaldynamicsandarefedintothesharedTransformer(middle),whichlearns
observationtokenpredictionsinanautoregressivemanner. Predictionsofindividualrewardriand
t
discountγiattimesteptarecomputedbasedonallhistoricalsequence(xi ,...,xi ,ai ,ei ).
t ≤t,1 ≤t,K ≤t ≤t
ofdiscretelatentstatesoverGaussianstates. ForMARL,MAMBA[5]extendsDreamerV2tomulti-
agentcontextsbyusingRSSM,underscoringthepotentialofmulti-agentlearningintheimagination
ofworldmodels. Recently, motivatedbythesuccessoftheTransformer[45], TransDreamer[3]
andTWM[36]exploredvariantsofDreamerV2,whereinthebackbonesoftheworldmodelwere
substitutedwithTransformers. Insteadofincorporatingdeterministicandstochasticlatentstates,
IRIS[30]appliestheTransformertodirectlymodelingsequencesofobservationtokensandactions
ofsingle-agentRLandachievesimpressiveresultsonAtari-100k. Incontrast,theproposedMARIE
concentratesonestablishingeffectiveTransformer-basedworldmodelsinmulti-agentcontextswith
shareddynamicsandglobalrepresentations.
Preliminaries. Wefocusonfullycooperativemulti-agentsystemswhereallagentsshareateam
rewardsignal. WeformulatethesystemasadecentralizedpartiallyobservableMarkovdecision
process (Dec-POMDP) [32], which can be described by a tuple (N,S,A,P,R,Ω,O,γ). N =
{1,...,n}denotesasetofagents,S isthefiniteglobalstatespace,A=(cid:81)n Aiistheproductof
i=1
finiteactionsspacesofallagents,i.e.,thejointactionspace,P :S×A×S →[0,1]istheglobal
transitionprobabilityfunction,R:S×A→Risthesharedrewardfunction,Ω=(cid:81)n Ωiisthe
i=1
productoffiniteobservationspacesofallagents,i.e.,thejointobservationspace,O ={Oi,i∈N}
isthesetofobservingfunctionsofallagents. Oi :S →Ωi mapsglobalstatestotheobservations
foragenti,andγ isthediscountfactor. Givenaglobalstates attimestept,agentiisrestricted
t
to obtaining solely its local observation oi = Oi(s ), takes an action ai drawn from its policy
t t t
πi(·|oi )basedonthehistoryofitslocalobservationsoi ,whichtogetherwithotheragents’actions
≤t ≤t
gives a joint action a = (a1,...,an) ∈ A, equivalently drawn from a joint policy π(·|o ) =
t t t ≤t
(cid:81)n πi(·|oi ). Thentheagentsreceiveasharedrewardr =R(s ,a ),andtheenvironmentmoves
i=1 ≤t t t t
tonextstates withprobabilityP(s |s ,a ). Theaimofallagentsistolearnajointpolicyπ
t+1 t+1 t t
thatmaximizestheexpecteddiscountedreturnJ(π)=E [(cid:80)∞ γtR(s ,a )].
s0,a0,...∼π t=0 t t
3 Methodologies
Ourapproachcomprisesthreetypicalparts:(i)collectingexperiencebyexecutingthepolicy,(ii)learn-
ing the world model from the collected experience, and (iii) learning the policy via imagination
inside the world model. Throughout the process, the historical experiences stored in the replay
3
…bufferareusedfortrainingtheworldmodelonly,whilepoliciesarelearnedfromunlimitedimagined
trajectoriesfromtheworldmodel. Inthefollowing,wefirstdescribethreecorecomponentsofour
worldmodelin§3.1and§3.2,andgiveanoverviewoftheproposedworldmodelinFig.1. Thenwe
describethepolicy-learningprocessinsidetheworldmodelin§3.3. Thecomprehensivedetailsof
themodelarchitectureandhyperparameterareprovidedin§A.
3.1 DiscretizingObservation
WeconsideratrajectoryτiofagenticonsistsofT localobservationsandactions,as
τi =(oi,ai,...,oi,ai,...,oi ,ai ).
1 1 t t T T
ToutilizetheexpressiveTransformerarchitecture,weneedtoexpressthetrajectoryintoadiscrete
tokensequenceformodeling. Accountingforcontinuousobservations,aprevalentbutnaiveapproach
fordiscretizationinvolvesdiscretizingthescalarintooneofmfixed-widthbinsineachdimension
independently[21].However,withahigherdimensionoftheobservation,theTransformerencounters
highercomputationalcomplexity,whichnecessitatesanapproachthatusesadiscretecodebookof
learnedrepresentations. Tothisend,weemploytheideafromneuraldiscreterepresentationlearning
[44],andlearnaVectorQuantised-VariationalAutoEncoder(VQ-VAE)toplayarolethatresembles
thetokenizerinNaturalLanguageProcessing[4,2]. TheVQ-VAEiscomposedofanencoderE,
adecoderD,andacodebookZ. WedefinethediscretecodebookZ ={z j}N
j=1
⊂Rnz,whereN
isthesizeofthecodebookandn isthedimensionofcodes. TheencoderE takesanobservation
z
oi ∈Rnobs asinputandoutputsaK n z-dimensionallatentszˆi ∈RK×nz reshapedfromthedirect
outputsofencoder. Subsequently,thetokens{xi}K ∈ {0,1,...,N −1}K forrepresentingoi is
k k=1
obtainedbyanearestneighbourlook-upusingthecodebookZ wherexi = argmin ∥zˆi −z ∥.
k j k j
ThenthedecoderD : {0,1,...,N −1}K → Rnobs convertsK tokensbackintoanreconstructed
observationoˆi. Bylearningthisdiscretecodebook,wecompresstheredundantinformationviaa
succinctsequenceoftokens,whichhelpsimprovesequencemodeling. See§4.2foradiscussion.
3.2 ModelingLocalDynamicswithGlobalRepresentations
Here, we consider discrete actions like those in SMAC, and the continuous actions can also be
discretizedbysplittingthevalueineachdimensionintofixedbins[21,1]. Therefore,atrajectoryτi
ofagenticanbetreatedasasequenceoftokens,
τi =(...,oi,ai,...)=(...,xi ,xi ,...,xi ,ai,...) (1)
t t t,1 t,2 t,K t
wherexi isthej-thtokenoftheobservationofagentiattimestept. Givenarbitrarysequencesof
t,j
observationandactiontokensinEq.(1),wetrytolearnoverdiscretemultimodaltokens.
Theworldmodelconsistsofatokenizertodiscretethelocalobservation,aTransformertolearnthe
localdynamics,anagent-wiserepresentationaggregationmodule,andpredictorsfortherewardand
discount. TheTransformerϕpredictsthefuturelocalobservation{xˆi }K ,thefutureindividual
t+1,j j=1
reward rˆi and discount γˆi, based on the agent’s individual historical observation-action history
t t
(xi ,ai )andaggregatedglobalfeatureei oftheagent. ThemodulesareshowninEqs.(2)–(5).
≤t,· ≤t t
Transition: xˆi ∼p (xˆi |xi ,ai ,ei )with xˆi ∼p (xˆi |xi ,ai ,ei ,xi )
t+1,· ϕ t+1,· ≤t,· ≤t ≤t t+1,k ϕ t+1,k ≤t,· ≤t ≤t t+1,<k
(2)
Reward: rˆi ∼p (rˆi|xi ,ai ,ei ) (3)
t ϕ t ≤t,· ≤t ≤t
Discount: γˆi ∼p (γˆi|xi ,ai ,ei ) (4)
t ϕ t ≤t,· ≤t ≤t
Aggregation: (e1,e2,...,en)=f (x1 ,x1 ,...,x1 ,a1,...,xn ,xn ,...,xn ,an) (5)
t t t θ t,1 t,2 t,K t t,1 t,2 t,K t
TransitionPrediction.InthetransitionpredictioninEq.(2),thek-thobservationtokenisadditionally
conditioned on the tokens that were already predicted xi ≜ (xi ,xi ,...,xi ),
t+1,<k t+1,1 t+1,2 t+1,k−1
ensuringtheautoregressivetokenpredictiontofacilitatemodelingoverthetrajectorysequence.
DiscountPrediction. ThediscountpredictoroutputsaBernoullilikelihoodandletsusestimatethe
probabilityofanindividualagent’sepisodeendingwhenlearningbehaviorsfrommodelpredictions.
RewardPrediction.Sincetherewardsdistributioniswidespreadanddiverseinmulti-agentscenarios
[38], we discrete the reward and learn a reward predictor via discrete regression [6] instead of a
mean-squarederror. Specifically,therewardpredictoroutputsanestimatedcategoricaldistribution
4MLP Encoder
1:𝑛𝑛
𝑜𝑜1 Agen𝐸𝐸t-wise Aggregation 𝑟𝑟1̂𝑖𝑖 Agent-wise Aggregation 𝑟𝑟2̂𝑖𝑖
𝑖𝑖 𝑖𝑖
𝛾𝛾�1 𝛾𝛾�2
…
𝒙𝒙𝒙𝒙1𝑖𝑖 𝒙𝒙1,𝑖𝑖1 1𝑖𝑖,1,1 𝒙𝒙𝒙𝒙1𝑖𝑖 𝒙𝒙1,𝑖𝑖2 1𝑖𝑖,2,2 𝒙𝒙𝒙𝒙1𝑖𝑖 𝒙𝒙1,𝑖𝑖3 1𝑖𝑖,3,3 𝒙𝒙𝒙𝒙1𝑖𝑖 𝒙𝒙1,𝑖𝑖𝐾𝐾 1𝑖𝑖,𝐾𝐾,𝐾𝐾 𝒂𝒂𝒂𝒂1𝑖𝑖 𝒂𝒂1𝑖𝑖 1𝑖𝑖 𝒆𝒆𝒆𝒆1𝑖𝑖 𝒆𝒆1𝑖𝑖 1𝑖𝑖 𝒙𝒙𝒙𝒙1𝑖𝑖 𝒙𝒙1,𝑖𝑖1 2𝑖𝑖,1,1 𝒙𝒙𝒙𝒙1𝑖𝑖 𝒙𝒙1,𝑖𝑖2 2𝑖𝑖,2,2 𝒙𝒙𝒙𝒙1𝑖𝑖 𝒙𝒙1,𝑖𝑖3 2𝑖𝑖,3,3 𝒙𝒙𝒙𝒙1𝑖𝑖 𝒙𝒙1,𝑖𝑖𝐾𝐾 2𝑖𝑖,𝐾𝐾,𝐾𝐾 𝒂𝒂𝒂𝒂1𝑖𝑖 𝒂𝒂1𝑖𝑖 2𝑖𝑖 𝒆𝒆𝒆𝒆1𝑖𝑖 𝒆𝒆1𝑖𝑖 2𝑖𝑖
Agents
1:𝑛𝑛 MLP Decoder MLP Decoder
𝐷𝐷 𝐷𝐷
1:𝑛𝑛 1:𝑛𝑛
1:𝑛𝑛 𝜋𝜋𝜓𝜓 1:𝑛𝑛 1:𝑛𝑛 𝜋𝜋𝜓𝜓 1:𝑛𝑛
Figure2: Imagin𝑜𝑜�1ationprocedur𝑎𝑎e1inMARIE.Weunrolltheim𝑜𝑜�a2ginationofall𝑎𝑎a2gents{1,...,n}in
parallel. Initially,eachagent’sobservationisderivedfromajointobservationsampledfromareplay
buffer. Apolicy,depictedinredarrows,generatesactionsbasedonreconstructedobservations. Then,
thePerceiverintegratesjointactionsandobservationsintoglobalrepresentationsfromeachagent,
appendingthemtoeachagent’slocalsequence. TheTransformerthenpredictsindividualrewards
anddiscounts,depictedbygreenandpurplearrowsrespectively,whilegeneratingnextobservation
tokensforeachagentinanautoregressivemanner,shownbybluearrows. Thisparallelimagination
iteratesforH steps. Thepoliciesπ1:nareexclusivelytrainedusingimaginedtrajectories.
ψ
R over M buckets centered at {b }M through the softmax function. The estimated reward is
m m=1
representedastheexpectedvalueofthisdistribution,as
rˆi =E(cid:2) R(xi ,ai ,ei )(cid:3) , R(xi ,ai ,ei )=(cid:88)M p (b |xi ,ai ,ei )·δ (6)
t ≤t,· ≤t ≤t ≤t,· ≤t ≤t
m=1
ϕ m ≤t,· ≤t ≤t bm
Thenwelearntherewardpredictorbyusingthecross-entropylossfunction. Thetargetdistribution
R
=(cid:80)M
p ·δ canbecalculatedviaHL-Gauss[6],whichperformslabelsmoothingforthe
tar m=1 m bm
targetcategoricaldistributionusingaGaussiandistribution. Thedetailsaregivenin§B.
Agent-wise Aggregation. Due to the partial environment, the non-stationarity issue stems from
thesophisticatedagent-wiseinter-dependencyonlocalobservationsgeneration. Toaddressit,we
introduceaPerceiver[19]toperformagent-wiserepresentationaggregationwhichplaysasimilarrole
tocommunication. Tosustainthedecentralizedmannerintransitionprediction,wehopeeveryagent
canpossessitsowninnerperceptionofthesituationsthatallagentsarein. Nonetheless,withdiscrete
representationforlocalobservation,theobservation-actionpairofagentiattimesteptisprojected
intoasequence(xi ,xi ,...,xi ,ai)oflengthK+1. Itleadstoasequenceoflengthn(K+1)
t,1 t,2 t,K t
thatlinearlyscaleswiththenumberofagents,whichrepresentsthejointobservation-actionpairof
allagents. Therefore,wechoosethePerceiverastheagent-wiserepresentationaggregationmodule,
whichexcelsatdealingwiththecasethatthesizeofinputsandoutputsscaleslinearly. Equipped
withaflexiblequeryingmechanismandself-attentionmechanism,thePerceiveraggregatesthejoint
representationsequence(x1 ,x1 ,...,x1 ,a1,...,xn ,xn ,...,xn ,an)oflengthn(K+1)intoa
t,1 t,2 t,K t t,1 t,2 t,K t
sequenceofnfeaturevectors(e1,e2,...,en),whereeachfeaturevectorservesasanintrinsicglobal
t t t
abstractionoftheenvironmentalcontextsperceivedfromeachagent’sviewpoint.
The world model ϕ is trained with trajectory segments of a fixed horizon H sampled from the
replaybufferDinaself-supervisedmanner. Thetransitionpredictor,discountpredictor,andreward
predictorareoptimizedtomaximizethelog-likelihoodoftheircorrespondingtargets:
(cid:104)(cid:88)H
L (ϕ,θ)=E E −logp (ri|xi ,ai ,ei)−logp (γi|xi ,ai ,ei)
Dyn i∼N τi∼D ϕ t ≤t,· ≤t t ϕ t ≤t,· ≤t t
t=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
rewardloss discountloss
(cid:16)(cid:88)K (cid:17)(cid:105)
− logp (xi |xi ,ai ,ei,xi ) (7)
ϕ t+1,k ≤t,· ≤t t t+1,<k
k=1
(cid:124) (cid:123)(cid:122) (cid:125)
transitionloss
where(e1,e2,...,en)=f (x1 ,x1 ,...,x1 ,a1,...,xn ,xn ,...,xn ,an),∀t.
t t t θ t,1 t,2 t,K t t,1 t,2 t,K t
We jointly minimize this loss function in Eq. (7) with respect to the model parameters of local
dynamics(i.e.,ϕ)andglobalrepresentation(i.e.,θ)usingtheAdamoptimizer[23].
3.3 LearningBehavioursinImagination
WeutilizetheActor-Criticframeworktolearnthebehaviorofeachagent,wheretheactorandcritic
areparameterizedbyψ andξ,respectively. Inthefollowing,wetakeagentiasanexemplarcase
5forclarityandomitthesuperscriptfordenotingtheindexoftheagenttoavoidpotentialconfusion.
Benefitedfromthesharedlocaldynamics,thelocaltrajectoriesofallagentsareimaginedinparallel,
asillustratedinFig.2. Attimestept,theactortakesareconstructedobservationoˆ asinput,and
t
samplesanactiona ∼π (a |oˆ).Theworldmodelthenpredictstheindividualrewardrˆ,individual
t ψ t t t
discountγˆ andnextlocalobservationoˆ . Startingfrominitialobservationssampledfromthe
t t+1
replaybuffer,thisimaginationprocedureisrolledoutforH steps.Tostimulatelong-horizonbehavior
learning, the critic accounts for rewards beyond the fixed imagination horizon and estimates the
individualexpectedreturnV (oˆ)≃E [(cid:80) γl−trˆ].
ξ t πψ l≥t l
Inourapproach,wetraintheactorandcriticinaMAPPO-like[49]manner. UnlikeotherCTDE
model-freeapproachesthatrequireaglobaloraclestatefromtheenvironment,wecannotobtainthe
oraclestatefromtheworldmodel,andonlythepredictedobservationsofeachagentareavailable.
To approximate the oracle information in critic training, we enhance each agent’s critic with the
capability to access the observations of other agents. Since the actor and critic only rely on the
reconstructedobservations,decouplingfromtheinnerhiddenstatesoftheTransformer-basedworld
model,weallowfastinferenceintheenvironmentwithouttheparticipationoftheworldmodel. It
is important for the deployment of policies learned with data-efficient imagination in real-world
applications. λ-targetinDreamer[9]isusedtoupdatedthevaluefunction. Thedetailsofbehavior
learningobjectivesandalgorithmicdescriptionofMARIEarepresentedin§Cand§G,respectively.
4 Experiments
Weconsiderthemostcommonbenchmark–StarCraftIIMulti-AgentChallenge(SMAC)[38]for
evaluatingourmethod. Tohighlightthesampleefficiencybroughtbymodel-basedimagination,we
adoptalowdataregimethatresemblesasimilarsettinginsingle-agentAtaridomain[52].
4.1 EvaluationsonSMAC
StarCraftIIMulti-AgentChallenge. SMAC[38],asuiteofcooperativemulti-agentenvironments
basedonStarCraftII,consistsofasetofStarCraftIIscenarios. Eachscenariodepictsaconfrontation
betweentwoarmiesofunits,oneofwhichiscontrolledbythebuilt-ingameAIandtheotherby
ouralgorithm. Theinitialposition,number,andtypeofunitsineacharmyvariesfromscenarioto
scenario,asdoesthepresenceorabsenceofelevatedorimpassableterrain. Andthegoalistowinthe
gamewithinthepre-specifiedtimelimit. SMACemphasizesmasteringmicromanagementtechniques
acrossmultipleagentstoachieveeffectivecoordinationandovercomeadversaries. Thisnecessitates
bothsufficientexplorationandappropriatecreditassignmentforeachagent’saction. Anothernotable
propertyofSMACisthatnotallactionsareaccessibleduringdecision-makingofeachagent,which
requiresworldmodelstopossessanin-depthcomprehensionoftheunderlyinggamemechanicssoas
toconsistentlyprovidevalidavailableactionmaskestimationwithintheimaginationhorizon.
ExperimentalSetup. Wechoose13representativescenariosfromSMACthatincludesthreelevels
ofdifficulty–Easy,Hard,andSuperHard. SpecificchosenscenarioscanbefoundinTable1. In
termsofdifferentlevelsofdifficulty,weadoptasimilarsettingakintothatin[5]andrestrictthe
numberofsamplesfromtherealenvironmentto100kforEasyscenarios,200kforHardscenarios
and400kforSuperHardscenarios,toestablishalowdataregimeinSMAC.WecompareMARIE
withthreestrongmodel-freebaselines–MAPPO[49],QMIX[35]andQPLEX[46],andtwostrong
model-basedbaselines–MBVD[48]andMAMBA[5]onSMACbenchmark.Specially,asarecently
proposedmulti-agentvariantofDreamerV2[11],MAMBAachievesstate-of-the-artsampleefficiency
invariousSMACscenariosvialearninginimagination. Foreachrandomseed,wecomputethewin
rateacross10evaluationgamesatfixedintervalsofenvironmentalsteps.
MainResults. Overall,wefindMARIEachievessignificantlybettersampleefficiencyandahigher
winratecomparedwithotherstrongbaselines. Wereportthemedianwinratesoverfourseedsin
Table 1 and provide additional learning curves of several chosen scenarios, shown as Fig. 3. As
presentedinTable1andFig.3,MARIEdemonstratessuperiorperformanceandsampleefficiency
across almost all scenarios. The improvements in sample efficiency and performance become
particularlypronouncedwithincreasingdifficultyofscenarios,especiallycomparedtoMAMBAthat
adoptsRSSMasthebackbonefortheworldmodel. Weattributesuchresultstothemodelcapability
oftheTransformerinlocaldynamicsmodelingandglobalfeatureaggregation. Benefitingfrommore
powerfulstrengthinmodelingsequences,theTransformer-basedworldmodelcangeneratemore
accurateandconsistentimaginationsthanthoserelyingontherecurrentbackbone,whichfacilitates
6MAPPO QMIX QPLEX MAMBA MBVD MARIE
2s3z 8m 1c3s5z 3s_vs_3z
1.0 1.0 0.8 1.0
0.8 0.8 0.8
0.6
0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Environment Steps 1e5 Environment Steps 1e5 Environment Steps 1e5 Environment Steps 1e5
3s_vs_4z 3s_vs_5z 2c_vs_64zg corridor
0.6 1.0 0.30 0.6
0.5 0.8 0.25 0.5 0.4 0.6 0.20 0.4
0.3 0.15 0.3 0.2 0.4 0.10 0.2
0.1 0.2 0.05 0.1
0.0 0.0 0.00 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.4 0.8 1.2 1.6 2.0 0.0 0.4 0.8 1.2 1.6 2.0 0.0 0.8 1.6 2.4 3.2 4.0
Environment Steps 1e5 Environment Steps 1e5 Environment Steps 1e5 Environment Steps 1e5
Figure3: Curvesofevaluationwinrateformethodsin8chosenSMACmaps. SeeTable1forwin
rates. Yaxis: winrate;Xaxis: numberofstepstakenintherealenvironment.
Table1: Medianevaluationwinrateandstandarddeviationon13SMACmapsfordifferentmethods
over4randomseeds. Weboldthevaluesofthemaximum.
Methods
Maps Difficulty Steps
MARIE(Ours) MAMBA[5] MAPPO[49] QMIX[35] QPLEX[46] MBVD[48]
1c3s5z 84.1(9.4) 76.8(15.3) 22.7(11.0) 44.1(29.2) 67.2(7.4) 62.7(11.4)
2m_vs_1z 100.0(7.9) 95.0(2.3) 86.7(3.2) 74.1(14.8) 88.4(10.8) 33.9(24.5)
2s_vs_1sc 100.0(7.1) 99.1(7.1) 100.0(0.0) 0.0(0.0) 7.1(19.5) 0.3(14.8)
2s3z 81.8(9.3) 69.1(12.7) 31.3(12.9) 37.7(15.5) 51.1(8.4) 52.3(4.1)
3m 99.5(0.4) 86.4(7.1) 84.4(12.8) 53.7(22.7) 89.0(6.9) 72.4(6.9)
Easy 100K
3s_vs_3z 99.5(1.5) 92.3(10.1) 0.8(1.3) 0.0(0.0) 0.0(0.0) 0.0(0.0)
3s_vs_4z 63.6(24.9) 27.7(12.3) 0.0(0.0) 0.0(0.0) 0.0(0.0) 0.0(0.0)
8m 89.1(3.9) 65.0(7.7) 77.3(19.5) 74.5(12.8) 83.0(6.4) 74.7(9.7)
MMM 25.0(3.4) 45.0(27.6) 4.7(4.5) 25.0(17.3) 88.4(35.1) 20.4(2.1)
so_many_baneling 97.7(5.9) 93.6(4.1) 43.8(15.0) 22.7(8.9) 31.2(6.1) 12.6(10.4)
3s_vs_5z 88.6(38.8) 10.5(14.0) 0.0(0.0) 0.0(0.0) 0.0(0.0) 0.0(0.0)
Hard 200K
2c_vs_64zg 23.6(14.3) 7.7(8.7) 3.1(10.2) 0.5(0.5) 0.0(0.1) 0.0(0.4)
corridor SuperHard 400K 47.1(32.2) 21.1(15.2) 0.0(0.7) 0.0(0.0) 0.0(0.0) 0.0(0.0)
betterpolicylearningwithintheimaginationoftheworldmodel. Whilethescenariosbecomeharder,
e.g. 3s_vs_5z, our world model can address the challenge of learning more intricate underlying
dynamicsandfurtherlargequantitiesofaccurateimaginations,therebysignificantlyoutperforming
otherbaselinesonthesescenarios. ForMMM wherethemodel-freebaselineperformsbetter,we
hypothesizethattheagentmainlyreliesonshort-termbehaviorsforcooperation,withoutrequiring
long-termpredictionsforbetterperformance. Moreover, aspecialscenario2c_vs_64zgdeserves
attention,whichfeaturesonly2agentsbutwithaconsiderablylargeactionspaceofupto70discrete
actions for each agent. It is easy for the world model to generate ridiculous estimated available
actionmaskswithoutunderstandingthemechanicsbehindthisscenario,furtherleadingtoinvalid
orevenerroneouspolicylearningintheimaginationsoftheworldmodel. Theperformancegapon
2c_vs_64zgprovesthatourTransformer-basedworldmodelhashigherpredictionaccuracyanda
deeperunderstandingoftheunderlyingmechanics.
4.2 AblationStudies
Incorporating CTDE principle with the design of the world model makes MARIE scalable
and robust to different number of agents. We compare our method with a centralized variant
ofourmethod,whereintheworldmodellearnsthejointdynamicsofallagentstogetheroverthe
jointtrajectoryτ =(...,o1,o2,...,on,a1,a2,...,an,...). Giventhatτ alreadycontainsthejoint
t t t t t t
observationsandactions,wedisabletheaggregationmoduleinthiscentralizedvariant. Asillustrated
inFigure4,ourcomparisonsspanscenariosinvolving2to7agents. Whenthenumberofagentsis
smallenough,reducingthemulti-agentsystemtoasingle-agentoneoverthejointobservationand
actionspacewouldnotcauseaprominentscalabilityissue,asindicatedbytheresultin2s_vs_1sc.
However,thescalabilityissueisexacerbatedbyagrowingnumberofagents. Inscenariosfeaturing
morethan3agents,thesampleefficiencyofthecentralizedvariantencountersasignificantdrop,
suffering from the exponential surge in spatial complexity of the joint observation-action space.
7
etaR
niW
etaR
niW
etaR
niW
etaR
niW
etaR
niW
etaR
niW
etaR
niW
etaR
niWDecentralized Manner Centralized Manner
2s_vs_1sc (2 agents) 3s_vs_3z (3 agents) 2s3z (5 agents) so_many_baneling (7 agents)
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Environment Steps 1e5 Environment Steps 1e5 Environment Steps 1e5 Environment Steps 1e5
Figure4: Ablationonwhatmannertointegrateintothedesignoftheworldmodel. Decentralized
MannerdenotesthestandardimplementationofMARIE,whileCentralizedMannerdenotesthatthe
worldmodelisdesignedforlearningthejointdynamicsofallagentsoverthejointtrajectory.
MARIE MARIE w/o aggregation
3s_vs_3z (3 agents) so_many_baneling (7 agents) 1c3s5z (9 agents)
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Environment Steps 1e5 Environment Steps 1e5 Environment Steps 1e5
Figure5: ComparisonsbetweenMARIEwithandwithouttheusageoftheaggregationmodule.
Furthermore, withequalpredictionhorizons, theparameteramountsinthecentralized variantis
increased by a factor of 4 or larger. And to achieve the same number of environment steps, the
centralizedvariantdemandsovertwicetheoriginalcomputationaltime. Instead,withdecentralized
localdynamicsandaggregatedglobalfeatures,MARIEdeliversstableandsuperiorsampleefficiency.
Agent-wiseaggregationhelpsMARIEcapturethesophisticatedinter-dependencyonthegenera-
tionofeachagent’slocalobservation. Tostudytheinfluenceofagent-wiseaggregation,weconduct
ablationexperimentsontheaggregationmoduleoverscenarioswherethenumberofagentsgradually
increases. AsshowninFig.5,inthe3-agentsscenario(e.g.,3s_vs_3z),thecorrelationamongeach
agent’slocalobservationtendstobenegligible. Therefore,thenearlyindependentgenerationofeach
agent’slocalobservationwithoutanyaggregatedglobalfeaturestillleadstoperformancecomparable
tothatofstandardimplementation. Butasmoreagentsgetinvolved,theinter-dependencybecomes
dominant. Lackingtheglobalfeaturesderivedfromagent-wiseaggregation,thesharedTransformer
strugglestoinferaccuratefuturelocalobservations,thushinderingpolicylearningintheimaginations
oftheworldmodelandresultinginnotabledegradationinthewinrateevaluation.
VQ-VAEencapsulateslocalobservationswithinasuccinctsequenceoftokens,promotingthe
learningoftheTransformer-basedworldmodelandeffectivelyimprovingalgorithmperfor-
mance. ComparedtoVQ-VAEthatdiscretizeseachobservationtoK tokensfromZ, perhapsa
morenaivetokenizerisprojectingthevalueineachdimensionintooneofmfixed-widthbins[21],
resultinginan -longtokensequenceforeachobservation,whichwetermBinsDiscretization.
obs
We set the number of bins m equal to the size of codebook |Z| and compare these two types of
tokenizers in different environments with various n . As shown in Fig. 6, the performance of
obs
thetwotokenizersarecomparableonlyin2s_vs_1scwheren iscloseto16. Evenworse,Bins
obs
Discretizationexperiencesapronounceddeclineasn increasesinmorecomplexenvironments
obs
(e.g.,3s_vs_4z)underidenticaltrainingdurations. Wehypothesizethatforasinglelocalobservation,
an -token-longverbosesequenceyieldedbyBinsDiscretizationcontainsmoreredundantinfor-
obs
mationcomparedtoVQ-VAEthatlearnsamorecompacttokenizerthroughreconstructionThisnot
onlyrendersthetokensequencesofBinsDiscretizationobscureandchallengingtocomprehend,but
alsoresultsinanincreaseinmodelparameteramounts,beingmorecomputationallycostly. Dueto
thesetwofactors,BinsDiscretizationexhibitsanotablyslowconvergence. Meanwhile,theresultin
2m_vs_1zindicatesBinsDiscretizationmayignorethecorrelationofdifferentdimensions,which
wouldbehelpfulinsequencemodeling.
4.3 ModelAnalysis
ErrorAccumulation. Aquantitativeevaluationofthemodel’saccumulatederrorversusprediction
horizonisprovidedinFig.7. Sincelearningtheworldmodelistiedtoaprogressivelyimproving
8
etaR
niW
etaR
niW
etaR
niW
etaR
niW
etaR
niW
etaR
niW
etaR
niWTokenizer (16 tokens) Bins Discretization (nobs tokens)
2m_vs_1z (nobs=16) 2s_vs_1sc (nobs=17) 3s_vs_4z (nobs=42) 2s3z (nobs=80)
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0 2 4 6 8 10 0 2 4 6 8 10 0 2 4 6 8 10 0 4 8 12 16 20
Timecost(hours) Timecost(hours) Timecost(hours) Timecost(hours)
Figure 6: Ablation on the type of discretization for local observations. Tokenizer denotes the
standardimplementationofMARIE;BinsDiscretizationdenotesthevariantofMARIEwherethe
n -dimensional observation discretization is performed by projecting the value into one of m
obs
fixed-widthbinsineachdimensionindependently. X-axis: cumulativeruntimeofalgorithmsinthe
sameplatform.
Agent 1 local imagination Agent 2 local imagination Agent 3 local imagination
2.00 2.00 2.00
MARIE MARIE MARIE
1.75 MAMBA 1.75 MAMBA 1.75 MAMBA
1.50 1.50 1.50
1.25 1.25 1.25
1.00 1.00 1.00
0.75 0.75 0.75
0.50 0.50 0.50
0.25 0.25 0.25
0.00 0.00 0.00
5 10 15 20 25 5 10 15 20 25 5 10 15 20 25
Horizons Horizons Horizons
Figure7: Compoundingmodelerrors. WecomparetheimaginationaccuracyofMARIEtothatof
MAMBAoverthecourseofaplanninghorizonin3s_vs_5zscenario. MARIEhasremarkablybetter
errorcompoundingwithrespecttopredictionhorizonthanMAMBA.
policybothinMARIEandMAMBA,weseparatelyusetheirfinalpoliciestosample10episodesfor
fairness. WethencomputeL errorsperobservationdimensionbetween1000trajectorysegments
1
randomly sampled from these 20 episodes and their imagined counterpart. The result in Fig. 7
suggests architecture differences play a large role in the world model’s long-horizon accuracy.
This also provides additional evidence that policy learning can benefit from accurate long-term
imaginations,explainingMARIE’snotableperformanceinthe3s_vs_5zscenario. Moreprecisely,
lowergeneralizationerrorbetweentheestimateddynamicsandtruedynamicsbringsatighterbound
betweenoptimalpoliciesderivedfromthesetwodynamicsaccordingtotheoreticalresults[20].
AttentionPatterns. Duringmodelprediction,wedelveintotheattentionmapsinsidetheshared
TransformerandthecrossattentionmapsinthePerceiver. Interestingly,weobservetwodistinct
attention patterns involved in the local dynamics prediction. One exhibits a Markovian pattern
whereintheobservationpredictionlaysitsfocusmostlyontheprevioustransition,whiletheotheris
regularlystriatedwhereinthemodelattendstospecifictokensinmultiplepriortransitions. During
theagent-wiseaggregation,wealsoidentifytwodistinctpatterns–individualityandcommonality
amongagents. SuchdiversepatternsintheTransformerandPerceivermaybepivotalforachieving
accurateandconsistentimaginationsofthesophisticatedlocaldynamics. Wereferto§Dforfurther
detailsandvisualizationresults.
5 Conclusion
Wehaveintroducedamodel-basedmulti-agentalgorithm–MARIE,whichutilizesasharedTrans-
formeraslocaldynamicmodelandaPerceiverasaglobalagent-wiseaggregationmoduletoconstruct
a world model within the multi-agent context. By providing long-term imaginations with policy
learning, it significantly boosts the sample efficiency and improves final performance compared
state-of-the-artmodel-freemethodsandexistingmodel-basedmethodswithsamelearningparadigm,
inthelowdataregime. AsthefirstTransformer-basedmulti-agentworldmodelforsample-efficient
policylearning,weopenanewavenueforcombiningthepowerfulstrengthoftheTransformerwith
sample-efficientMARL.Consideringthenotorioussampleinefficiencyinmulti-agentscenarios,it
holdsimportantpromiseforapplicationinmanyrealisticmulti-robotsystems,whereincollecting
tremendoussamplesforoptimalpolicylearningiscostlyandimpracticalduetothesafety. Whileit
hasthegreatpotentialtobrightthefuturetowardsachievingsmartermulti-agentsystems,therestill
9
etaR
niW
srorrE
detalumuccA
etaR
niW
srorrE
detalumuccA
etaR
niW
srorrE
detalumuccA
etaR
niWexistlimitationsinMARIE.Forinstance,itwouldsufferfrommuchslowerinferencespeedwhen
usedwithaverylongpredictionhorizons,duetotheauto-regressiveproperty.
References
[1] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea
Finn, Keerthana Gopalakrishnan, Karol Hausman, AlexHerzog, JasmineHsu, JulianIbarz,
BrianIchter,AlexIrpan,TomasJackson,SallyJesmonth,NikhilJoshi,RyanJulian,Dmitry
Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav
Malla, DeekshaManjunath, IgorMordatch, OfirNachum, CarolinaParada, JodilynPeralta,
EmilyPerez,KarlPertsch,JornellQuiambao,KanishkaRao,MichaelRyoo,GreciaSalazar,
PannagSanketi,KevinSayed,JaspiarSingh,SumedhSontakke,AustinStone,ClaytonTan,
HuongTran,VincentVanhoucke,SteveVega,QuanVuong,FeiXia,TedXiao,PengXu,Sichun
Xu,TianheYu,andBriannaZitkovich. Rt-1: Roboticstransformerforreal-worldcontrolat
scale. InRobotics: ScienceandSystems(RSS),2023.
[2] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,Ariel
Herbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRadford,Ilya
Sutskever,andDarioAmodei. Languagemodelsarefew-shotlearners. InAdvancesinNeural
InformationProcessingSystems,2020.
[3] ChangChen,Yi-FuWu,JaesikYoon,andSungjinAhn. Transdreamer: Reinforcementlearning
withtransformerworldmodels. arXivpreprintarXiv:2202.09481,2022.
[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training
of deep bidirectional transformers for language understanding. In Proceedings of the 2019
ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:
HumanLanguageTechnologies,NAACL-HLT,2019.
[5] Vladimir Egorov and Alexei Shpilman. Scalable multi-agent model-based reinforcement
learning. In Proceedings of the 21st International Conference on Autonomous Agents and
MultiagentSystems,2022.
[6] JesseFarebrother,JordiOrbay,QuanVuong,AdrienAliTaïga,YevgenChebotar,TedXiao,
Alex Irpan, Sergey Levine, Pablo Samuel Castro, Aleksandra Faust, et al. Stop regressing:
Trainingvaluefunctionsviaclassificationforscalabledeeprl. arXivpreprintarXiv:2403.03950,
2024.
[7] Jakob N. Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon
Whiteson. Counterfactualmulti-agentpolicygradients. InProceedingsoftheThirty-Second
AAAIConferenceonArtificialIntelligence,2018.
[8] David Haand Jürgen Schmidhuber. Recurrentworld modelsfacilitate policy evolution. In
AdvancesinNeuralInformationProcessingSystems,2018.
[9] DanijarHafner, TimothyLillicrap, JimmyBa, andMohammadNorouzi. Dreamtocontrol:
Learningbehaviorsbylatentimagination. InInternationalConferenceonLearningRepresenta-
tions,2020.
[10] DanijarHafner,TimothyLillicrap,IanFischer,RubenVillegas,DavidHa,HonglakLee,and
JamesDavidson. Learninglatentdynamicsforplanningfrompixels. InProceedingsofthe36th
InternationalConferenceonMachineLearning,ProceedingsofMachineLearningResearch.
PMLR,2019.
[11] DanijarHafner,TimothyPLillicrap,MohammadNorouzi,andJimmyBa. Masteringatariwith
discreteworldmodels. InInternationalConferenceonLearningRepresentations,2021.
[12] DanijarHafner,JurgisPasukonis,JimmyBa,andTimothyLillicrap. Masteringdiversedomains
throughworldmodels. arXivpreprintarXiv:2301.04104,2023.
[13] NicklasHansen,HaoSu,andXiaolongWang.Temporaldifferencelearningformodelpredictive
control.InProceedingsofthe39thInternationalConferenceonMachineLearning,Proceedings
ofMachineLearningResearch.PMLR,2022.
10[14] NicklasHansen,HaoSu,andXiaolongWang. TD-MPC2: Scalable,robustworldmodelsfor
continuous control. In The Twelfth International Conference on Learning Representations,
2024.
[15] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint
arXiv:1606.08415,2016.
[16] Pablo Hernandez-Leal, Bilal Kartal, and Matthew E. Taylor. A very condensedsurvey and
critiqueofmultiagentdeepreinforcementlearning. InProceedingsofthe19thInternational
ConferenceonAutonomousAgentsandMultiAgentSystems,2020.
[17] EhsanImaniandMarthaWhite. Improvingregressionperformancewithdistributionallosses.
In Proceedings of the 35th International Conference on Machine Learning, Proceedings of
MachineLearningResearch.PMLR,2018.
[18] Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In
Proceedingsofthe36thInternationalConferenceonMachineLearning,ProceedingsofMachine
LearningResearch.PMLR,2019.
[19] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao
Carreira. Perceiver: Generalperceptionwithiterativeattention. InInternationalconferenceon
machinelearning,2021.
[20] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model:
Model-based policy optimization. In Advances in Neural Information Processing Systems,
2019.
[21] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big
sequencemodelingproblem. InAdvancesinNeuralInformationProcessingSystems,2021.
[22] AndrejKarpathy. mingpt: Aminimalpytorchre-implementationoftheopenaigpt(generative
pretrainedtransformer)training. https://github.com/karpathy/minGPT,2020.
[23] DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InYoshua
BengioandYannLeCun,editors,3rdInternationalConferenceonLearningRepresentations,
2015.
[24] DiederikP.KingmaandMaxWelling. Auto-EncodingVariationalBayes. In2ndInternational
ConferenceonLearningRepresentations,2014.
[25] JakubGrudzienKuba,RuiqingChen,MuningWen,YingWen,FangleiSun,JunWang,and
Yaodong Yang. Trust region policy optimisation in multi-agent reinforcement learning. In
InternationalConferenceonLearningRepresentations,2021.
[26] Qihan Liu, Jianing Ye, Xiaoteng Ma, Jun Yang, Bin Liang, and Chongjie Zhang. Efficient
multi-agentreinforcementlearningbyplanning. InTheTwelfthInternationalConferenceon
LearningRepresentations,2024.
[27] YongLiu,WeixunWang,YujingHu,JianyeHao,XingguoChen,andYangGao. Multi-agent
gameabstractionviagraphattentionneuralnetwork. InTheThirty-FourthAAAIConferenceon
ArtificialIntelligence,2020.
[28] RyanLowe,YiWu,AvivTamar,JeanHarb,PieterAbbeel,andIgorMordatch.Multi-agentactor-
criticformixedcooperative-competitiveenvironments. InProceedingsofthe31stInternational
ConferenceonNeuralInformationProcessingSystems,2017.
[29] AnujMahajan,MikayelSamvelyan,LeiMao,ViktorMakoviychuk,AnimeshGarg,JeanKos-
saifi,ShimonWhiteson,YukeZhu,andAnimashreeAnandkumar. Tesseract: Tensorisedactors
formulti-agentreinforcementlearning. InProceedingsofthe38thInternationalConferenceon
MachineLearning,ProceedingsofMachineLearningResearch.PMLR,2021.
[30] VincentMicheli,EloiAlonso,andFrançoisFleuret. Transformersaresample-efficientworld
models. InTheEleventhInternationalConferenceonLearningRepresentations,2023.
[31] ThanhThiNguyen,NgocDuyNguyen,andSaeidNahavandi. Deepreinforcementlearningfor
multiagentsystems: Areviewofchallenges,solutions,andapplications. IEEETransactionson
Cybernetics,50:3826–3839,2020.
[32] FransAOliehoek,ChristopherAmato,etal. AconciseintroductiontodecentralizedPOMDPs,
volume1. Springer,2016.
11[33] BeiPeng,TabishRashid,ChristianSchroederdeWitt,Pierre-AlexandreKamienny,PhilipTorr,
WendelinBoehmer,andShimonWhiteson. FACMAC:Factoredmulti-agentcentralisedpolicy
gradients. InAdvancesinNeuralInformationProcessingSystems,2021.
[34] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Languagemodelsareunsupervisedmultitasklearners. OpenAIblog,2019.
[35] TabishRashid,MikayelSamvelyan,ChristianSchroeder,GregoryFarquhar,JakobFoerster,
andShimonWhiteson. QMIX:Monotonicvaluefunctionfactorisationfordeepmulti-agent
reinforcement learning. In Proceedings of the 35th International Conference on Machine
Learning,ProceedingsofMachineLearningResearch.PMLR,2018.
[36] JanRobine,MarcHöftmann,TobiasUelwer,andStefanHarmeling. Transformer-basedworld
modelsarehappywith100kinteractions.InTheEleventhInternationalConferenceonLearning
Representations,2023.
[37] HeechangRyu,HayongShin,andJinkyooPark. Multi-agentactor-criticwithhierarchicalgraph
attentionnetwork. InProceedingsoftheAAAIConferenceonArtificialIntelligence,2020.
[38] MikayelSamvelyan,TabishRashid,ChristianSchroederdeWitt,GregoryFarquhar,Nantas
Nardelli,TimG.J.Rudner,Chia-ManHung,PhilipH.S.Torr,JakobFoerster,andShimon
Whiteson. The starcraft multi-agent challenge. In Proceedings of the 18th International
ConferenceonAutonomousAgentsandMultiAgentSystems,2019.
[39] JulianSchrittwieser,IoannisAntonoglou,ThomasHubert,KarenSimonyan,L.Sifre,Simon
Schmitt,ArthurGuez,EdwardLockhart,DemisHassabis,ThoreGraepel,TimothyP.Lillicrap,
andDavidSilver.Masteringatari,go,chessandshogibyplanningwithalearnedmodel.Nature,
588:604–609,2020.
[40] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximal
policyoptimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.
[41] KyunghwanSon,DaewooKim,WanJuKang,DavidEarlHostallero,andYungYi. QTRAN:
Learningtofactorizewithtransformationforcooperativemulti-agentreinforcementlearning.
In Proceedings of the 36th International Conference on Machine Learning, Proceedings of
MachineLearningResearch.PMLR,2019.
[42] PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,ViniciusZambaldi,
MaxJaderberg,MarcLanctot,NicolasSonnerat,JoelZ.Leibo,KarlTuyls,andThoreGraepel.
Value-decomposition networks for cooperative multi-agent learning based on team reward.
InProceedingsofthe17thInternationalConferenceonAutonomousAgentsandMultiAgent
Systems,2018.
[43] RichardSSutton. Dyna,anintegratedarchitectureforlearning,planning,andreacting. ACM
SigartBulletin,2(4):160–163,1991.
[44] AaronvandenOord,OriolVinyals,andKorayKavukcuoglu. Neuraldiscreterepresentation
learning. InAdvancesinNeuralInformationProcessingSystems,2017.
[45] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. InAdvancesinNeuralInforma-
tionProcessingSystems,2017.
[46] JianhaoWang,ZhizhouRen,TerryLiu,YangYu,andChongjieZhang.QPLEX:Duplexdueling
multi-agentq-learning. InInternationalConferenceonLearningRepresentations,2021.
[47] Daniël Willemsen, Mario Coppola, and Guido CHE de Croon. Mambpo: Sample-efficient
multi-robotreinforcementlearningusinglearnedworldmodels.In2021IEEE/RSJInternational
ConferenceonIntelligentRobotsandSystems(IROS).IEEE,2021.
[48] ZhiweiXu,DapengLi,BinZhang,YuanZhan,YunpengBaiia,andGuoliangFan. Mingling
foresightwithimagination: Model-basedcooperativemulti-agentreinforcementlearning. In
AdvancesinNeuralInformationProcessingSystems,2022.
[49] ChaoYu,AkashVelu,EugeneVinitsky,JiaxuanGao,YuWang,AlexandreBayen,andYiWu.
ThesurprisingeffectivenessofPPOincooperativemulti-agentgames. InThirty-sixthConfer-
enceonNeuralInformationProcessingSystemsDatasetsandBenchmarksTrack,2022.
[50] QiaoshengZhang,ChenjiaBai,ShuyueHu,ZhenWang,andXuelongLi. Provablyefficient
information-directedsamplingalgorithmsformulti-agentreinforcementlearning.arXivpreprint
arXiv:2404.19292,2024.
12[51] YangZhang,ShixinYang,ChenjiaBai,FeiWu,XiuLi,XuelongLi,andZhenWang.Towardsef-
ficientllmgroundingforembodiedmulti-agentcollaboration. arXivpreprintarXiv:2405.14314,
2024.
[52] ŁukaszKaiser,MohammadBabaeizadeh,PiotrMiłos,Błaz˙ejOsin´ski,RoyHCampbell,Konrad
Czechowski,DumitruErhan,ChelseaFinn,PiotrKozakowski,SergeyLevine,AfrozMohiuddin,
RyanSepassi,GeorgeTucker,andHenrykMichalewski. Modelbasedreinforcementlearning
foratari. InInternationalConferenceonLearningRepresentations,2020.
13A WorldModelsDetailsandHyperparameters
A.1 ObservationTokenizer
Ourtokenizerforlocalobservationdiscretizationisbasedontheimplementation3ofavanillaVQ-
VAE [44]. Faced with continuous non-vision observation, we build the encoder and decoder as
Multi-LayerPerceptrons(MLPs). Thedecoderisdesignedwiththesamehyperparametersastheones
oftheencoder. ThehyperparametersarelistedasTable2. Duringthephaseofcollectingexperience
fromtheexternalenvironment, eachagenttakesthereconstructedobservationsprocessedbythe
VQ-VAEasinputinsteadtoavoidthedistributionshiftbetweenpolicylearningandpolicyexecution.
FortrainingthisvanillaVQ-VAE,weuseastraight-throughestimatortoenablegradientbackpropa-
gationthroughthenon-differentiablequantizationoperationinthequantizationofVQ-VAE.Theloss
functionforlearningtheautoencoderisasfollows:
L (E,D,Z)=E E (cid:2) ∥oi−oˆi∥2+∥sg[E(oi)]−zi∥2+β∥sg[zi]−E(oi)∥2(cid:3) (8)
VQ−VAE i∼N oi q q
whereN ={1,2,...,n}denotesthesetofagents,sg[·]denotesthestop-gradientoperationandβ is
thecoefficientofthecommitmentloss∥sg[zi]−E(oi)∥2. Inpractice,wefoundthecodebookZ can
q
sufferfromcodebookcollapsewhenlearningfromscratch. Thus,weadopttheExponentialMoving
Averages(EMA)[44]techniquetoalleviatethisproblem.
Table2: VQVAEhyperparameters.
Hyperparameter Value
Encoder&Decoder
Layers 3
Hiddensize 512
Activation GELU[15]
Codebook
Codebooksize(N) 512
Tokensperobservation(K) 16
Codedimension 128
Coef. ofcommitmentloss(β) 10.0
A.2 Transformer
The shared Transformer serving as the local dynamics model is based on the implementation of
minGPT[22].GivenafixedimaginationhorizonH,itfirsttakesatokensequenceoflengthH(K+1)
composedofobservationtokensandactiontokens,andembedsitintoaH(K+1)×Dtensorvia
separateembeddingtablesforobservationsandactions. Then,theaggregatedfeaturetensor,returned
bytheagent-wiseaggregationmodule,isinsertedaftertheactionembeddingtensorateverytimestep,
formingafinalembeddingtensorofshapeH(K+2)×D. Thistensorisforwardedthroughfixed
Transformerblocks. Here,weadoptGPT2-likeblocks[34]asthebasicblocks. Thehyperparameters
arelistedasTable3. ToenabletrainingacrossallenvironmentsonasingleNVIDIARTX3090GPU,
weadaptimaginationhorizonH basedonthenumberofagents.
Table3: Transformerhyperparameters.
Hyperparameter Value
Imaginationhorizon(H) {15,8,5}
Embeddingdimension 256
Layers 10
Attentionheads 4
Weightdecay 0.01
Embeddingdropout 0.1
Attentiondropout 0.1
Residualdropout 0.1
3Codecanbefoundinhttps://github.com/lucidrains/vector-quantize-pytorch
14Table4: Perceiverhyperparameters.
Hyperparameter Value
Lengthoflatentquerying n(numberofagents)
Crossattentionheads 8
InnerTransformerlayers 2
Transformerattentionheads 8
Dimensionperattentionhead 64
Embeddingdropout 0.1
Attentiondropout 0.1
Residualdropout 0.1
A.3 Perceiver
ThePerceiver[19]isbasedontheopen-sourceimplementation4. Byaligningthelengthofthelatent
queryingarraywiththenumberofagentsn,weobtaintheintrinsicglobalrepresentationfeature
correspondingtoeachindividualagent. Wefurtherdiveintotheprocessofagent-wiserepresentation
aggregation: (i) the embedding tensor of shape (K + 1) × D at each timestep, mentioned in
AppendixA.2,isconcatenatedwithothersfromallagents,therebygettingan(K+1)×Dsequence
forthejointobservation-actionpairatthecurrenttimestep;(ii)throughthecross-attentionmechanism
withthelatentqueryingarray,theoriginalsequenceiscompressedfromlengthn(K+1)ton;(iii)the
compressedsequenceisthenforwardedthroughastandardtransformerwithbidirectionalattention
insidethePerceiver. ThehyperparametersarelistedasTable4.
B RewardLabelsinHL-Gauss
HL-Gaussmethod[6]leveragesHistogramLossesintroducedby[17]. Firstly,wecandefinethe
random variable Y with probability density f and cumulative distribution function F whose
Y Y
expectation is ri. Given M buckets of width ς = (r −r )/M centered at {b }M , the
t max min m m=1
distributionofY projectedontothehistogramwiththesebucketscanbecalculatedviaintegrating
overtheinterval[b −ς/2,b +ς/2],
m m
(cid:90) bm+ς/2
p (b )= f (y)dy
m m Y
bm−ς/2
=F (b +ς/2)−F (b −ς/2) (9)
Y m Y m
To stabilize classification learning over potential diverse reward distribution, we can model the
distributionofY asaGaussiandistributionN(µ = ri,σ2),whichsmoothsthetargetdistribution
t
computedbyEq.(9). Thehyperparameterσisusedtocontrolthedegreeoflabelsmoothing,andis
typicallysettobe0.75ς,asadvisedby[6]. Wereferto[6]formoredetails.
C BehaviourLearningDetails
InMARIE,weuseMAPPO-like[49]actorandcritic,wheretheactorandcriticshouldhavebeen
3-layerMLPs. However,unlikeotherCTDEmodel-freeapproaches,whosecritictakesadditional
globaloraclestatesfromtheenvironmentinthetrainingphase,ourworldmodelhardlyprovides
relatedpredictionsintheimaginedtrajectories. Toalleviatethisissue,weaugmentthecriticwith
anattentionmechanismandprovideitallreconstructedobservationsoˆ ofallagents. Therefore,
t
the actor ψ remains a 3-layer MLP with ReLU activation, while the critic ξ is enhanced with an
extralayerofself-attention, builtontopoftheoriginal3-layerMLP,i.e., weoverwritethecritic
Vi(oˆ)≃E ((cid:80) γl−trˆi)foragenti. Similartooff-the-shelfCTDEmodel-freeapproaches,we
ξ t πi l≥t l
ψ
adoptparametersharingacrossagents.
Criticlossfunction Weutilizeλ-returninDreamer[9],whichemploysanexponentially-weighted
averageofdifferentk-stepsTDtargetstobalancebiasandvarianceastheregressiontargetforthe
4Codecanbefoundinhttps://github.com/lucidrains/perceiver-pytorch
15Table5: Behaviourlearninghyperparameters.
Hyperparameter Value
ImaginationHorizon(H) {15,8,5}
Predicteddiscountlabelγ 0.99
λ 0.95
η 0.001
Clippingparameterϵ 0.2
critic. Givenanimaginedtrajectory{oˆi,ai,rˆi,γˆi}H foragenti,λ-returniscalculatedrecursively
τ τ τ τ t=1
as,
(cid:40) (cid:104) (cid:105)
rˆi+γˆi (1−λ)Vi(oˆ)+λVi(oˆ ) if t<H
Vi(oˆ)= t t ξ t λ t+1 (10)
λ t Vi(oˆ) if t=H
ξ t
The objective of the critic ξ is to minimize the mean squared difference Li with λ-returns over
ξ
imaginedtrajectoriesforeachagenti,as
(cid:20) (cid:21)
Li =E (cid:88)H−1(cid:0) Vi(oˆ)−sg(Vi(oˆ))(cid:1)2 (11)
ξ πi ξ t λ t
ψ t=1
wheresg(·)donotesthestop-gradientoperation. Weoptimizethecriticlosswithrespecttothecritic
parametersξusingtheAdamoptimizer.
Actorlossfunction Theobjectivefortheactionmodelπ (·|oˆi)istooutputactionsthatmaximize
ψ t
thepredictionoflong-termfuturerewardsmadebythecritic. Toincorporateintermediaterewards
moredirectly,wetraintheactortomaximizethesameλ-returnthatwascomputedfortrainingthe
critic. Intermsofthenon-stationarityissueinmulti-agentscenarios,weadoptPPOupdates,which
introduceimportantsamplingforactorlearning. Theactorlossfunctionforagentiis:
H−1
(cid:104) (cid:88) (cid:16) (cid:17) (cid:105)
Li =−E min ri(ψ)Ai,clip(ri(ψ),1−ϵ,1+ϵ)Ai +ηH(πi(·|oˆi)) (12)
ψ pϕ,π ψi
old
t t t t ψ t
t=0
whereri(ψ) = πi/πi isthepolicyratioandAi = sg(Vi(oˆ)−Vi(oˆ))istheadvantage. We
t ψ ψold t λ t ξ t
optimize the actor loss with respect to the actor parameters ψ using the Adam optimizer. In the
discountpredictionofMARIE,wesetitslearningtargetγ tobe0.99. Overallhyperparametersare
showninTable5.
D ExtendedAnalysisonAttentionPatterns
Toprovidequalitativeanalysisofourworldmodel,weselecttypicalscenarios–3s_vs_5zwhere
ourmethodachievesthemostsignificantimprovementcomparedtootherbaselinesforvisualizing
attentionmapsinsidetheTransformer. Forthesakeofsimpleandclearvisualization, wesetthe
imaginationhorizonH as5. Intermsofcross-attentionmapsintheaggregationmodule,weselecta
scenario2s3zincluding5agentsforvisualization. VisualizationresultsaredepictedasFig.8and
Fig.9.
Thepredictionoflocaldynamicsentailstwodistinctattentionpatterns. TheleftoneinFig.8can
be interpreted as a Markovian pattern, in which the observation prediction lays its focus on the
previoustransition. Incontrast,therightoneisregularlystriated,withthemodelattendingtospecific
tokensinmultiplepriorobservations. Intermsoftheagent-wiseaggregation,wealsoidentifytwo
distinctpatterns: individualityandcommonality. ThetoponeinFig.9illustratesthateachagent
flexibly attends to different tokens according to their specific needs. In contrast, the bottom one
exhibitsconsistentattentionallocationacrossallagents,withattentionhighlightedinnearlyidentical
positions. ThediversepatternsintheTransformerandPerceivermaybethekeytoaccurateand
consistentimagination.
16𝑖𝑖 𝑖𝑖
𝑜𝑜𝑡𝑡 𝑜𝑜𝑡𝑡
𝑖𝑖 𝑖𝑖
𝑎𝑎𝑡𝑡 𝑎𝑎𝑡𝑡
𝑖𝑖 𝑖𝑖
𝑒𝑒𝑡𝑡 𝑒𝑒𝑡𝑡
𝑖𝑖 𝑖𝑖
𝑜𝑜𝑡𝑡+5 𝑜𝑜𝑡𝑡+5
𝑖𝑖 𝑖𝑖
𝑎𝑎𝑡𝑡+5 𝑎𝑎𝑡𝑡+5
Fig𝑖𝑖ure8:AttentionpatternsintheTransformer. We𝑖𝑖observetwodistincttypesofattentionweights
𝑒𝑒𝑡𝑡+5 𝑒𝑒𝑡𝑡+5
during the prediction of local dynamics. In the first one (left), the next observation prediction is
primarilydependentonthelasttransition,whichmeanstheworldmodelhaslearnedtheMarkov
propertycorrespondingtoDec-POMDPs. Thesecondtype(right)exhibitsaregularlystriatedpattern,
wherethenextobservationpredictionhingesoverwhelminglyonthesamedimensionofmultiple
previous timesteps. The above attention weights are produced by a sixth-layer and ninth-layer
attentionheadduringimaginationsonthe3s_vs_5zscenario.
Cross Attention Head 1
agent 1
agent 2
agent 3
agent 4
agent 5
agent
1′
s
𝑜𝑜𝑡𝑡1 ,𝑎𝑎𝑡𝑡1
tokens agent
2′
s
𝑜𝑜𝑡𝑡2 ,𝑎𝑎𝑡𝑡2
tokens agCernots s3
A′ stte𝑜𝑜n𝑡𝑡3 t,io𝑎𝑎n𝑡𝑡3
H teoakde 6ns agent
4′
s
𝑜𝑜𝑡𝑡4 ,𝑎𝑎𝑡𝑡4
tokens agent
5′
s
𝑜𝑜𝑡𝑡5 ,𝑎𝑎𝑡𝑡5
tokens
agent 1
agent 2
agent 3
agent 4
agent 5
Figure9ag:enCt
1′ rso𝑜𝑜s𝑡𝑡1 ,s𝑎𝑎𝑡𝑡1
a totkteensntionagepnt
a2′ stt𝑜𝑜e𝑡𝑡2 ,r𝑎𝑎𝑡𝑡2
n tsokeinns theagPenet
3r′ sce𝑜𝑜𝑡𝑡3 i,v𝑎𝑎𝑡𝑡3
e tro.keWns eobagsenet
r4v′ se𝑜𝑜𝑡𝑡4 t,𝑎𝑎h𝑡𝑡4
e toiknendsividaugeantl
5i′ tsy𝑜𝑜𝑡𝑡5 a,𝑎𝑎n𝑡𝑡5
d tokcenosmmonality
intheagent-wiseaggregation. Thetoppartofthefigurerepresentstheindividuality,whereagents
adjusttheirattentionsoverthewholejointtokensequenceattimesteptflexiblyaccordingtotheir
ownneeds. Incontrast, thebottomexhibitsthecommonality, whereeveryagent’sattentionover
thejointtokensequenceisemphasizedinthesimilarpositionsofthesequence. Thecrossattention
weightsmentionedaboveareproducedbythefirstandsixthheadofthecrossattentionwithinthe
Perceiver,duringtheagent-wiseaggregationonthe2s3zscenario.
E BaselineImplementationDetails
MAMBA [5] is evaluated based on the open-source implementation: https://github.com/
jbr-ai-labs/mambawiththehyperparametersinTable6.
MAPPO [49] is evaluated based on the open-source implementation: https://github.com/
marlbenchmark/on-policywiththecommonhyperparametersinTable7.
QMIX [35] is evaluated based on the open-source implementation: https://github.com/
oxwhirl/pymarlwiththehyperparametersinTable8.
QPLEX [46] is evaluated based on the open-source implementation: https://github.com/
wjh720/QPLEX with the hyperparameters in Table 9. Since its implementation is mostly based
ontheopen-sourceimplementation: PyMARL[38],itsmosthyperparameterssettingremainsthe
sameastheoneinQMIXinadditiontoitsownspecialhyperparameters.
MBVD[48]isevaluatedbasedontheimplementationinitssupplementarymaterialfromhttps:
//openreview.net/forum?id=flBYpZkW6ST with the hyperparameters in Table 10. Akin to
QPLEX,itsimplementationisbasedontheopen-sourceimplementation:PyMARL,itsmosthyperpa-
rameterssettingremainsthesameastheoneinQMIXinadditiontoitsownspecialhyperparameters.
17
…
…
…
…
…
…Table6: HyperparametersforMAMBAinSMACenvironments.
Hyperparameter Value
Batchsize 256
λforλ-returncomputation 0.95
Entropycoefficient 0.001
Entropyannealing 0.99998
Numberofpolicyupdates 4
Epochsperpolicyupdate 5
Clippingparameterϵ 0.2
ActorLearningrate 0.0005
CriticLearningrate 0.0005
Discountfactorγ 0.99
ModelLearningrate 0.0002
Numberofmodeltrainingepochs 60
Numberofimaginedrollouts 800
Sequencelength 20
ImaginationhorizonH 15
Buffersize 2.5×105
Numberofcategoricals 32
Numberofclasses 32
KLbalancingentropyweight 0.2
KLbalancingcrossentropyweight 0.8
Gradientclipping 100
Collectedtrajectoriesbetweenupdates 1
Hiddensize 256
Table7: CommonhyperparametersforMAPPOinSMACenvironments.
Hyperparameter Value
Batchsize numenvs×bufferlength×numagents
Minibatchsize batchsize/mini-batch
Recurrentdatachunklength 10
GAEλ 0.95
Discountfactorγ 0.99
Valueloss huberloss
Huberdelta 10.0
Optimizer Adam
Optimizerlearningrate 0.0005
Optimizerepsilon 1×10−5
Weightdecay 0.0
Gradientclipping 10
Networkinitialization orthogonal
Userewardnormalization True
Usefeaturenormalization True
18Table8: HyperparametersforQMIXinSMACenvironments.
Hyperparameter Value
Batchsize 32episodes
Buffersize 5000episodes
Epsiloninepsilon-greedy 1.0→0.05
Epsilonannealtime 50000timesteps
Traininterval 1episode
Discountfactorγ 0.99
Optimizer RMSProp
RMSPropα 0.99
RMSPropϵ 10−5
Gradientclipping 10
Table9: HyperparametersforQPLEXinSMACenvironments.
Hyperparameter Value
Batchsize 32episodes
Buffersize 5000episodes
Epsiloninepsilon-greedy 1.0→0.05
Epsilonannealtime 50000timesteps
Traininterval 1episode
Discountfactorγ 0.99
Optimizer RMSProp
RMSPropα 0.99
RMSPropϵ 10−5
Gradientclipping 10
NumberoflayersinHyperNetwork 1
Numberofheadsintheattentionmodule 4
Table10: HyperparametersforMBVDinSMACenvironments.
Hyperparameter Value
Batchsize 32episodes
Buffersize 5000episodes
Epsiloninepsilon-greedy 1.0→0.05
Epsilonannealtime 50000timesteps
Traininterval 1episode
Discountfactorγ 0.99
Optimizer RMSProp
RMSPropα 0.99
RMSPropϵ 10−5
Gradientclipping 10
NumberoflayersinHyperNetwork 1
Numberofheadsintheattentionmodule 4
Horizonoftheimaginedrollout 3
KLbalancingα 0.3
Dimensionofthelatentstatesˆ numagentsx16
19Table11: HyperparametersforMARIEinSMACenvironments.
Hyperparameter Value
Batchsizefortokenizertraining 256
Batchsizeforworldmodeltraining 30
Optimizerfortokenizer AdamW
Optimizerforworldmodel AdamW
Optimizerforactor&critic Adam
Tokenizerlearningrate 0.0003
Worldmodellearningrate 0.0001
Actorlearningrate 0.0005
Criticlearningrate 0.0005
Gradientclippingforactor&critic 100
Gradientclippingfortokenizer 10
Gradientclippingforworldmodel 10
Weightdecayforworldmodel 0.01
λforλ-returncomputation 0.95
Discountfactorγ 0.99
Entropycoefficient 0.001
Buffersize(transitions) 2.5×105
Numberoftokenizertrainingepochs 200
Numberofworldmodeltrainingepochs 200
Collectedtransitionsbetweenupdates 100or200
Epochsperpolicyupdate(PPOepochs) 5
PPOClippingparameterϵ 0.2
Numberofimaginedrollouts 600or400
ImaginationhorizonH {15,8,5}
Numberofpolicyupdates {4,10,30}
Numberofstackingobservations 5
Observeagentid False
Observelastactionofitself False
F ParametersSettinginSMAC
AllourexperimentsarerunonamachinewithasingleNVIDIARTX3090GPU,a36-coreCPU,
and128GBRAM.WeprovidethehyperparametersofMARIEforexperimentsinSMAC,shownas
Table11. ToenabletherunningofexperimentsinallscenarioswithasingleNVIDIARTX3090
GPU, we set the imagination horizon H as 8 for other scenarios involving the number of agents
n > 5, 15 for n ≤ 5. In so_many_baneling and 2s3z, we set the imagination horizon H as 5.
Correspondingly,thenumberofpolicyupdatesinimaginationsvarieswithimaginationhorizonH.
Additionally,itshouldbenotedthattheresult HL-Gauss Smooth L1
in 3s_vs_5z was obtained with using L1 loss
3s_vs_5z
functionforrewardprediction,whichwasthe 1.0
sameastherewardpredictioninMAMBA.We 0.8
foundtheperformanceinthisscenarioisquite
0.6
sensitive to the selection of reward loss func-
tion, illustrated by Fig. 10. Considering the 0.4
significantlylargeactionspaceof2c_vs_64zg,
0.2
weenabletheobservationofagentidandlast
actionforeachagentanddisablestackingthe 0.0
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75
last5observationsasinputtothepolicy.
Environment Steps 1e5
Figure 10: Performance between different loss
functionsforrewardpredictionin3s_vs_5z.
20
etaR
niWG OverviewofMARIEAlgorithm
Pseudo-codeissummarizedasAlgorithm1.
Algorithm1MARIE
//mainloopoftraining
forepochsdo
collect_experience(num_transitions)
forlearning_world_model_steps_per_epochdo
train_world_model()
endfor
forlearning_behaviour_steps_per_epochdo
train_agents()
endfor
endfor
functioncollect_experience(n):
o ←env.reset()
0
fort=0,...,n−1do
//processedbyVQ-VAE
oˆ ←D(E(o ))
t t
Sampleai ∼πi(ai|oˆi),∀i
t ψ t t
o ,r ,done←env.step(a )
t+1 t t
ifdone=Truethen
o ←env.reset()
t+1
γ ←0.
t
else
γ ←0.99
t
endif
endfor
D ←D∪{o ,a ,r ,γ }n−1
t t t t t=0
functiontrain_world_model():
Sample{o ,a ,r ,γ }t=τ+H−1
t t t t t=τ
Update(E,D,Z)viaL overobservations{o }t=τ+H−1
VQ−VAE t t=τ
foragenti=1,...,ndo
Updateϕ,θviaL (ϕ,θ)overlocaltrajectories{oi,ai,r ,γ }t=τ+H−1
Dyn t t t t t=τ
endfor
functiontrain_agents():
Sampleaninitialobservationo ∼D
0
{xi }K ←E(oi),oˆi ←D(E(oi)),∀i
0,j j=1 0 0 0
fort=0,...,H −1do
Sampleai ∼πi(ai|oˆi),∀i
t ψ t t
Aggregate(x1 ,...,x1 ,a1,...,xn ,...,xn ,an)into(e1,...,en)viathePerceiverθ
t,1 t,K t t,1 t,K t t t
Samplexˆi ,rˆi,γˆi ∼p (xˆi ,rˆi,γˆi|xi ,ai,ei,...,xˆi ,ai,eˆi),∀i
t+1,· t t ϕ t+1,· t t 0,· 0 0 t,· t t
oˆi ←D(xˆi ),∀i
t+1 t+1,·
endfor
foragenti=1,...,ndo
Updateactorπi andcriticViviaL (ϕ,θ)overimaginedtrajectories{oˆi,ai,rˆi,γˆi}t=H−1
ψ ξ Dyn t t t t t=0
endfor
21