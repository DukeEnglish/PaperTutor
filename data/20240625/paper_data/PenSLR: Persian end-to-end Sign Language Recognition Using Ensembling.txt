PenSLR: Persian end-to-end Sign Language Recognition
Using Ensembling
AMIRPARSASALMANKHAH∗,
AmirkabirUniversityofTechnology(TehranPolytechnic),Iran
AMIRREZARAJABI∗,
AmirkabirUniversityofTechnology(TehranPolytechnic),Iran
NEGINKHEIRMAND∗,
AmirkabirUniversityofTechnology(TehranPolytechnic),Iran
ALIFADAEIMANESH∗,
AmirkabirUniversityofTechnology(TehranPolytechnic),Iran
AMIRREZATARABKHAH∗,
AmirkabirUniversityofTechnology(TehranPolytechnic),Iran
AMIRREZAKAZEMZADEH∗,
AmirkabirUniversityofTechnology(TehranPolytechnic),Iran
HAMEDFARBEH†,
AmirkabirUniversityofTechnology(TehranPolytechnic),Iran
SignLanguageRecognition(SLR)isafast-growingfieldthataimstofillthecommunicationgapsbetweenthe
hearing-impairedandpeoplewithouthearingloss.ExistingsolutionsforPersianSignLanguage(PSL)are
limitedtoword-levelinterpretations,underscoringtheneedformoreadvancedandcomprehensivesolutions.
Moreover,previousworkonotherlanguagesmainlyfocusesonmanipulatingtheneuralnetworkarchitectures
orhardwareconfigurationsinsteadofbenefitingfromtheaggregatedresultsofmultiplemodels.Inthis
paper,weintroducePenSLR,aglove-basedsignlanguagesystemconsistingofanInertialMeasurementUnit
(IMU)andfiveflexiblesensorspoweredbyadeeplearningframeworkcapableofpredictingvariable-length
sequences.Weachievethisinanend-to-endmannerbyleveragingtheConnectionistTemporalClassification
(CTC)lossfunction,eliminatingtheneedforsegmentationofinputsignals.Tofurtherenhanceitscapabilities,
weproposeanovelensemblingtechniquebyleveragingamultiplesequencealignmentalgorithmknown
asStarAlignment.Furthermore,weintroduceanewPSLdataset,including16PSLsignswithmorethan
3000time-seriessamplesintotal.Weutilizethisdatasettoevaluatetheperformanceofoursystembased
onfourword-levelandsentence-levelmetrics.OurevaluationsshowthatPenSLRachievesaremarkable
wordaccuracyof94.58%and96.70%insubject-independentandsubject-dependentsetups,respectively.These
achievementsareattributabletoourensemblingalgorithm,whichnotonlybooststheword-levelperformance
by0.51%and1.32%intherespectivescenariosbutalsoyieldssignificantenhancementsof1.46%and4.00%,
respectively,insentence-levelaccuracy.
CCSConcepts:•Human-centeredcomputing→Ubiquitousandmobiledevices;•Computingmethod-
ologies→Ensemblemethods;•Hardware→Signalprocessingsystems.
∗Theseauthorscontributedequallytothisresearch.
†HamedFarbehisthecorrespondingauthor.
Authors’ContactInformation:AmirparsaSalmankhah,amirparsa.s@aut.ac.ir,AmirkabirUniversityofTechnology(Tehran
Polytechnic),DepartmentofComputerEngineering,Tehran,Iran;AmirrezaRajabi,dr.mrajabi.mr@aut.ac.ir,Amirkabir
UniversityofTechnology(TehranPolytechnic),DepartmentofComputerEngineering,Tehran,Iran;NeginKheirmand,
neginkheirmand@gmail.com,AmirkabirUniversityofTechnology(TehranPolytechnic),DepartmentofComputerEngi-
neering,Tehran,Iran;AliFadaeimanesh,alifadaeimanesh@aut.ac.ir,AmirkabirUniversityofTechnology(TehranPoly-
technic),DepartmentofComputerEngineering,Tehran,Iran;AmirrezaTarabkhah,tarabkhah2@aut.ac.ir,Amirkabir
UniversityofTechnology(TehranPolytechnic),DepartmentofComputerEngineering,Tehran,Iran;AmirrezaKazemzadeh,
ar.kazemzade@gmail.com,AmirkabirUniversityofTechnology(TehranPolytechnic),DepartmentofComputerEngineering,
Tehran,Iran;HamedFarbeh,farbeh@aut.ac.ir,AmirkabirUniversityofTechnology(TehranPolytechnic),Departmentof
ComputerEngineering,Tehran,Iran.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfee
providedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthe
fullcitationonthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthantheauthor(s)mustbehonored.
Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requires
priorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
ACM1551-6865/2024/6-ART
https://doi.org/XXXXXXX.XXXXXXX
ACMTrans.MultimediaComput.Commun.Appl.,Vol.1,No.1,Article.Publicationdate:June2024.
4202
nuJ
42
]CH.sc[
1v88361.6042:viXra2 Salmankhahetal.
AdditionalKeyWordsandPhrases:SignLanguageRecognition,GestureRecognition,EnsembleMethods,
MultipleSequenceAlignment
ACMReferenceFormat:
AmirparsaSalmankhah,AmirrezaRajabi,NeginKheirmand,AliFadaeimanesh,AmirrezaTarabkhah,Amirreza
Kazemzadeh,andHamedFarbeh.2024.PenSLR:Persianend-to-endSignLanguageRecognitionUsingEnsem-
bling.ACMTrans.MultimediaComput.Commun.Appl.1,1(June2024),20pages.https://doi.org/XXXXXXX.
XXXXXXX
1 INTRODUCTION
Therearemorethan72millionusersworldwidewhomakeuseofsignlanguageonadailybasis.In
Iran,thenumberofdeafandhard-of-hearingindividualsisestimatedtobeover3millionasof2019
[17].Thesepeoplefacecommunicationproblems,leadingtosocialisolation,whichcanimpact
theirqualityoflife.Thetwomainsolutionsforcommunicationbetweensignlanguageusersand
therestofthepopulationareeitherthroughhandwritingoraninterpreter,whichcansometimes
beneitherfeasibleineverydayscenariosnorfastorinteractive.Moreover,signlanguagescanbe
diverse,withmultiplevariationsexistinginasingleregionorcountry.Thiscouldleadtodifficulties
incommunication,evenbetweensignlanguageusers.
SignLanguageRecognition(SLR)hasemergedasarapidlydevelopingfieldwithintheresearch
community. It focuses on addressing the problem of recognizing sentence-level sign language
glosses.SLRcanhelpbridgecommunicationgapsbetweendeaforhard-of-hearingindividuals
who use sign language and those who do not, enabling more inclusive interactions in various
settings.Inunderstandingsignlanguages,PersianSignLanguage(PSL),likeothers,consistsofa
combinationofintricatefingerpositions,handmovements,andfacemimicsthatcollectivelydefine
itsuniquecharacteristics.Thesegesturesfallintotwocategories:manualandnon-manualmarkers.
Theformerindicatesthefingerpositionsandtrajectoryofthehandthroughoutthegesture,andthe
latterreferstofacialexpressionsorheadmotion.Giventhecriticalroleofmanualandnon-manual
markers in SLR, any effective SLR system must incorporate as many of these features into its
recognitionprocessaspossible.
AnumberofapproachesexisttotackletheSLRproblem,eachwithitsownsetofprosandcons.
Themainmethodsarevision-basedandwearable-basedSLRsystems.Thevision-basedapproach
consistsofanalyzingvisualsignalsthroughtheuseofcamerasorothervisualsensors,andithas
theadvantageofconsideringbothmanualandnon-manualmarkers[6,9,14,25,29].However,this
approachalsocomeswithitssetofdrawbacks,suchasitsunderminingofprivacy,complexityof
thedatagatheringprocess,andsensitivitytolightingconditions.Thesecondapproachinvolves
wearable-basedsystemsthatdependonagloveoranyothertypeofwearabledeviceattachedto
differentpartsofthehandsorhead.Theembeddedsensorsprovidethedatathatwillbeprocessed
andtranslatedintosignlanguageglosses.Anumberofthesemethodsmakeuseoftraditional
machinelearningapproaches[10,11,13,24,28]whileothersleveragethepowerofdeeplearning
[1,4,12,20,22,23,26,27]torecognizeandtranslatesignlanguage.Althoughthesesystemsmostly
come with the disadvantage of not taking into account the non-manual markers and depend
onindividualanatomy,theirprivacy-preservingnature,portability,andaffordabilitymakethem
suitablechoicesforSLR.Recentresearchinthisfieldhasalsointroducedanewapproachbasedon
wirelesssensing[7,8,18,19].ThesetypesofSLRsystemsdependontheanalysisofacousticornon-
acousticwavesandusuallyhavetheadvantageoflesscomputationalcomplexityandportability.
However,thesesolutionsmayexperienceinterferencefromexternalwavesandunderperformin
environmentswhereobstacleshinderwavepropagation.
ResearchersmayfaceseveralchallengeswhiledevelopinganSLRsystem.Firstly,theinputdatais
dependentonbothspatialandtemporalfeatures.Theformerisduetothespatialnatureofthistask,
ACMTrans.MultimediaComput.Commun.Appl.,Vol.1,No.1,Article.Publicationdate:June2024.PenSLR:Persianend-to-endSignLanguageRecognitionUsingEnsembling 3
whilethelatteroriginatesfromthecomplexityofglossesaswellasthevariationsinsignlanguage
users’speed.Secondly,Itcanbechallengingtodistinguishbetweensomesignlanguagegestures
duetotheirsimilaritiesinhandmovements,fingerpositions,orfacialexpressions.Moreover,dueto
variationsinthewaythatsignlanguageusersperformgestures,thetrainedmodelmustgeneralize
trainingdatatomaintainaccuracywheninputfromanewuserisfedintothemodel.Thisalsomust
betakenintoaccountwhenpredictingapreviouslyunseensequencefromauserinthetraining
set.Lastly,themodelmustbecapableofdetectingthetransitionsbetweentheglossestoachieve
highaccuracywhenfacingnewsentenceswithmuchhigherlengthsthanthetrainingsamples.
Inthispaper,weproposeaglove-basedSLRsystemtodetectvariable-lengthsentencesfromPSL
usinganend-to-enddeeplearningframework.Weleverageacustomizedsignlanguageglovewitha
low-costInertialMeasurementUnit(IMU)attachedtothebackofthehandandfiveflexiblesensors
mountedonfingers.Wecollecttwoshort-lengthandlong-lengthdatasetstoprovetheabilityof
ourframeworktogeneralizeinlongerunseensentences.Ourdeeplearningframeworkconsistsof
twomainparts.Firstly,wedesignaConvolutionalRecurrentNeuralNetwork(CRNN)toextract
thespatio-temporalfeaturesfromthegiveninputsequenceandpredictthecorrespondinglabel.We
achievethisbyutilizingthewidely-usedConnectionistTemporalClassification(CTC)lossfunction
[5],whichallowsustooptimizethealignmentsbetweentheinputsequencesandthegroundtruth
withoutanypriorknowledgeaboutthealignmentsoranysegmentationscheme.Secondly,we
proposeanovelapproachforensemblingthatmakesuseofmultipletrainedmodelsandperforms
avotingprocesstoobtainthefinalresult.OurensemblingalgorithmutilizesStarAlignment,a
popularmultiplesequencealignmentalgorithm,toalignthepredictedsequencesofthemodels,
ensuringtheyareofequallength.Itwillthentakeamajorityvotebetweenthealignedsequences
ateachpositiontogeneratethefinalprediction.Ourframeworkleadstobenefitsregardingunseen
sentences,specificallylongerones.Furthermore,thisschemeimprovesthesystem’srobustness
inadaptingtonewindividuals.Inaddition,sincetheensemblingmethoddoesnotrelyonany
particularcharacteristicofPSL,futureworkinthisdomaincouldtakeadvantageofittoenhance
theirresult.Ultimately,itsapplicabilityextendstootherdomainslikegesturerecognition,givenits
independencefromlinguisticattributes.
Inordertoassesstheeffectivenessofoursystem,wegatheredadatasetcomprisingover3000
samples from 16 commonly used PSL glosses. The dataset was collected with the help of five
volunteersandincludessentencesofuptothreewordsfromtheselectedglosses.Anotherdataset,
containing4to8-wordsentences,wasgatheredwiththeaimofevaluatingthemodel’sability
torecognizelongersentences.Sinceitisnotarealisticscenariototrainanewmodelforeach
newuser,weconductasubject-independentanalysistoguaranteethepracticalityofoursystem.
Wecomparethetwobestmodelsobtainedbyanablationstudyintermsoffourword-leveland
sentence-levelevaluationmetrics.Theresultsdepictthatoursystemachievesalmost94%word-level
accuracyinbothdatasets,showcasingitsproficiencyindetectingbothshortandlongsequences.
Themodeliscapableofpredictingthelengthofthesequencesinapproximately95%ofsamples,
andasentence-levelexactmatchratioofalmost88%and80%isachievedfortheshort-lengthand
long-lengthdatasets,respectively.
Tosumup,ourmaincontributionsaresummarizedasfollows:
• Wedesignalow-costsignlanguagegloveusinganIMUandfiveflexiblesensorscapableof
capturingawiderangeoffingerbendingsandhandmovements.
• Wecollecttwodatasetscontainingshort-lengthandlong-lengthtime-seriesdatafromPSL
sentencescontaining16widely-usedPSLglosses.Wemakeourdatasetpubliclyavailable1in
1https://github.com/Persian-Sign-Language/PenSLR-dataset
ACMTrans.MultimediaComput.Commun.Appl.,Vol.1,No.1,Article.Publicationdate:June2024.4 Salmankhahetal.
ordertomakeitaccessibletoresearchersinthefieldofSLR,especiallytheonesworkingon
PSL.
• We develop a CRNN architecture with the ability to process variable-length signals and
predictcompletesignlanguagesentencesinanend-to-endfashion.
• WeproposeabrandnewensemblingschemeusingStarAlignmentasitsbackbone,whichis
adaptabletootherSLRorsequence-to-sequencetasks.
Therestofthepaperisorganizedasfollows.Section2presentsthepreviousworkinthefieldof
SLRandprovidesdetailsaboutmultiplesequencealignmentalgorithmsrelatedtoourensembling
method.Section3describesourdatasetaswellasthedatagatheringprocess.Section4deeply
investigatesdifferentpartsofPenSLR,includingthedesignedglove,thearchitectureofthemodel,
andourensemblingscheme.InSection5,theperformanceofthesystemisdiscussedusingmultiple
experimentsandevaluationmetrics.Then,inSection6weinvestigatethelimitationsofoursystem
andexplorepossibleavenuesforfutureresearch.Finally,theconclusionofthepaperisdrawnin
Section7.
2 RELATEDWORK
Inthissection,weexploretherelatedworksinthefieldofsignlanguagerecognitionandprovidea
briefexplanationofmultiplesequencealignmentalgorithms.
2.1 SignLanguageRecognition
Twoleadingsolutionsexisttotackletheproblemofsignlanguagerecognition(SLR)basedon
sensing technologies: vision-based and mobile/wearable solutions. Also, a recently emerging
approachisbasedonwirelesssensing.
2.1.1 Vision-basedMethods. Vision-basedmethodsusuallyutilizecamerasetupandvisualsignals,
thushavetheadvantageofconsideringnon-manualmarkers.Anearlywork[9]inthisareaforPSL
consistsoftheuseofDiscreteWaveletTransform(DWT)forfeatureextractionandaMulti-layer
Perceptron(MLP)neuralnetworkforcategorizingsignlanguagegestures.Inanotherstudy,C2ST
[25]takesintoaccountthelinguisticfeaturesofglosssequencesusingalinguisticmodel.Later
researchinthevisionfieldensuresspatialattentionconsistencyusingakeypoint-guidedspatial
attentionmodule[29].Anotherworkinthisfieldalsotakesintoaccountnon-manualmarkersusing
acustom-developedGlobal-localenhancementnetwork(GLE-Net)architecture[6].Moreover,[14]
suggestsusingaLeapMotionsensortoextractthecoordinatesofdifferentpartsofthehands.It
alsoproposeamodifiedversionofLongShortTermMemory(LSTM),introducinganewresetgate
thatresetsthememoryofLSTMwheneveranon-activesituationisdetected.
2.1.2 Wirelesssensing-basedMethods. Thefollowingmethodsintroducewirelesssensing-based
SLRsystemsthatuseelectromagneticoracousticwavestodetectbodymovements.In[19],aWi-Fi
receiverandtwoWi-fitransmittersareusedtocollectdata,andthenaKernel-basedSupportVector
Machine(SVM)modelisusedtoclassifygesturesondifferentsystems.Anotherwork,mmASL[18],
takesadvantageofmillimeterwavesandamulti-taskdeeplearningmodeltorecognizeAmerican
SignLanguage(ASL)gestures.Additionally,SonicASL[7],areal-timegesturerecognitionsystem,
identifiessignlanguagethroughtheuseofearphoneswithbuilt-inmicrophonesandspeakers,
achievinghighaccuracyratesinbothwordandsentencerecognition.Inalaterwork,SmartASL
[8] not only utilizes earbud signals but also employs IMU sensors to detect both manual and
non-manualmarkers.ThelasttwomentionedworksemployCRNNarchitecturealongwithCTC
lossfunctiontoperformsignlanguagerecognitioninreal-time.
ACMTrans.MultimediaComput.Commun.Appl.,Vol.1,No.1,Article.Publicationdate:June2024.PenSLR:Persianend-to-endSignLanguageRecognitionUsingEnsembling 5
2.1.3 Wearable-basedMethods. Mobile/Wearablesystemsdesignedtorecognizehandorbody
gestures are another approach to the SLR task. Although these solutions do not consider non-
manualmarkers,theycanhelpsolvetherangeoffailurepointsthatthepreviouslymentioned
vision-basedandwirelesssensing-basedsolutionshave.
Anearlyworkinthisfieldusedcustomizedbendingsensors,accelerometers,andHalleffect
sensorsinordertodetectgesturesofdigits(0to9)throughlogisticregression[3].Furthermore,
authorsin[15]suggesta5-bitrepresentationofthewordsorsentencesandassignabittotheoutput
ofeachflexiblesensoronfingerstopredictthegesturesbasedontheperceivedvalues.Another
study involves using a combination of IMU and sEMG sensors to gather data [24]. The sEMG
sensorsmakeitpossibleforthemtoautomaticallysplittheinputsignalsintosegmentsthatare
fedtoanSVMclassifierforthefinalprediction.Moreover,astudyonPSLsuggestsusingasimilar
sensorsettingalongwithaKNNclassifierandK-foldcross-validationtodetect20commonlyused
isolatedsigngestures[10].However,itdidnotprovideasolutionforrecognizingPSLsentences.In
[28],ayarn-basedstretchablesensorwasintroduced,whichisnotonlycapableofcapturinghand
gesturesbutcanalsobeattachedtoeyebrowsandmouth.Then,anSVMmodelwasusedtoperform
theclassificationtask.Authorsin[13]tookauniqueapproachbystoringpredefinedsentencesina
databasewhileharnessingacustomizedglovewithanIMUandflexiblesensors.Duringthetesting
phase,anovelDTWdistancewasproposedtofindthenearestdatainthedatabaseandpredictthe
labelofthenewinputsignal.In[11],theauthorssuggestedanapproachtocalculatethemovement
trajectoriesofgesturesandtookadvantageofanothercustomizedDTWdistancetoclassifythe
signals.
Laterworksleveragethepowerofdeeplearningtoachievebetterresultsandaccomplishmore
complextasks.AworkonPSL[1]collectedatime-seriesdatasetof15wordswith600samplesin
total.Thecollecteddatawerevirtuallyaugmentedto30000imagesusingtheState-Imageapproach,
andaCNNnetworkwasproposedtotrainontheseimages.Similarto[10],thisstudylackedthe
abilitytopredictcontinuoussentences.Anotherstudysuggestsusingasliding-windowapproach
to segment the input signals and construct the predicted sentence by feeding the segments to
aCNN-basedneuralnetwork[23].Thisenabledthemodeltoachievehighresultsindetecting
50 words and 20 predefined sentences, as well as new sentences that could be constructed by
differentcombinationsofthosewords.Authorsin[12]utilizedasimilarCNN-basednetworkwith
acustomizedglovecontaininganIMUandstrainsensorstodetect48ChineseSignLanguage(CSL)
gestures.Theytookadvantageofmultipleslidingwindowswithdifferentlengthsandaggregated
theirresultsforsentence-levelprediction.Inanotherstudy,aglovewasdesignedusingconductive
knitfabricandanaccelerometer[4].Togeneratetheoutput,anLSTMnetworkwasproposedto
trainonadatasetcontaining12distinctclassesfromASLusingasliding-windowapproach.
ThemostrecentapproachesinvolveusingCRNNmodelsalongwithCTClossfunctionoran
extensionoftheminvolvingattention-basednetworks.Myosign[27]suggestsanarmbandcapable
ofcollectingdatafromdifferentmodalities,i.e.,accelerometer,gyroscope,orientation,andEMG.
TheyproposeaCRNNnetworkconsistingofmultimodalCNNandBidirectionalLSTM(BiLSTM)
layers to build an end-to-end system for predicting ASL sentences. They achieve this through
the use of CTC loss, which enables them to perform SLR without any prior knowledge of the
alignments between input and output. In [20], a transfer learning scheme was employed on a
similararchitectureasMyosign,enablingthesystemtoconvergefasterwhilemaintainingaccuracy.
TheirwearablesystemconsistedofmultipleIMUsensorsonbothforearms.BuildingonMyosign,
Wearsign[26]introducesanencoder-decoderframeworkthattakesadvantageoftheattention
mechanismtotranslatethesignlanguageglossestothespokentext.Moreover,theyutilizedthe
back-translationtechniquetoaugmentandextendtheirASLdataset.Similarly,HearSign[22]
ACMTrans.MultimediaComput.Commun.Appl.,Vol.1,No.1,Article.Publicationdate:June2024.6 Salmankhahetal.
proposesanattention-basedencoder-decoderapproachwithamulti-channelCNNtotrainonthe
datacollectedbytwoMYOarmbandseffectively.
2.2 MultipleSequenceAlignment
Due to the increase of data in today’s era, especially sequence data, various algorithms have
beendesignedtoalignmultiplesequencestogether.Theapplicationofthesealgorithmsismainly
inthefieldofbioinformatics.Forexample,understandingthefunctionalsignificanceofgenetic
variationsacrossdiversespeciescountsasoneoftheirprimaryapplications.Thetaskofaligningtwo
sequences,pairwisesequencealignment,servesasthefoundationforaligningmultiplesequences.
Pairwise alignment is primarily categorized into local alignment and global alignment. Local
alignmentfocusesonidentifyingandaligningsimilarlocalregions,whileglobalalignmentinvolves
aligningsequencesinanend-to-endmanner[2].Inthissection,ourprimaryfocusreliesonglobal
alignmentbecauseitistheapproachweuseinourproposedalgorithm.
TheNeedleman-Wunsch(NW)algorithm[16]isawidelyemployedglobalalignmentalgorithm
thatleveragesadynamicprogrammingapproachtocalculatepairwisesequencealignment,taking
advantageoftheoptimalsubstructureofthisproblem.However,usingthisalgorithmformultiple
sequencealignmentisnotfeasibleduetoitstimecomplexity.Inordertotacklethisproblem,two
mainstrategiesaresuggested:staralignmentandprogressivealignment.Staralignmentworksby
automaticallyselectingoneofthesequencesasthecenterandaggregatingtheresultsofpairwise
alignmentbetweenthecenterandtheothersequencestoobtainthefinalalignment.Thisapproach
considerablyreducesthetimeneededtocomputethefinalalignments,thusmakingitasuitable
choiceforreal-timetasks.Ontheotherhand,progressivealignmentalgorithms,suchasClustalW
[21],arebasedonaguidetreethatdeterminestheorderbywhichthesequencesshouldgetaligned.
Therearetwomaindrawbackstotheprogressivealignmentalgorithms.First,thefinalresultis
highly dependent on the quality of the guide tree. Second, they need more time to execute as
hierarchicalclusteringisneededtoconstructtheguidetree,andinternalnodesofthetreerequire
morecomplexoperationsforcomputingthealignments[2].
3 DATASET
Wecollectedourowndatasetfortworeasons.First,asmentionedinSection2.1.3,therewasonly
onepublicPSLdataset[1]available,whichwasneitherlargeenoughnorcontainedsentence-level
data.Second,noneofthepreviousworkonPSLusedourhardwaresettings,forcingustocollect
newdatatoensurethecompatibilityofourglove-baseddesignwithourdataset.Unlikemany
conductedstudiesintheareaofSLR,whichusedseparatedword-levelandsentence-leveldatasets
totrainthemodel,wecollectedaunifieddatasetcontainingwordcombinations(sentences)ofup
tothreewords.SentencesarerandompermutationsoftheselectedPSLsignsanddonotnecessarily
conveymeaning.Moreover,thereasonforlimitingthenumberofwordsinsentenceswastoshow
thecapabilityofourSeq2Seqmodeltopredictsentencescontainingmorethanthreewordswithout
beingtrainedonthem.
Todemonstratetheeffectivenessofourmodel,wecherry-picked16wordsfromPSL,categorizing
themintofivesimilaritygroups.Table1demonstratesthegroupsandtherelationshipbetweenthe
similarwords.Thewordswithinthesametupleingroups1and2mutuallysharethecharacteristics
ofthegroup.Thatis,theysharesimilaritiesinatleastoneaspect,eitherthroughhandmovements
orfingerpositions,makingthemindistinguishablewithouttheconcurrentuseofbothsensors.
Ontheotherhand,groups3to5featurethewordsthathavefixed,dynamic,orrotationalhand
movements,respectively.Asaresult,achievinghighaccuracyinthissetupcouldshowcasethe
model’sproficiencyindetectinganddistinguishingminorvariationsbetweensimilargestures.For
clearerunderstanding,Figure1depictstheexecutionoftwopairsofourPSLwordsdistinguished
ACMTrans.MultimediaComput.Commun.Appl.,Vol.1,No.1,Article.Publicationdate:June2024.PenSLR:Persianend-to-endSignLanguageRecognitionUsingEnsembling 7
ID GroupCharacteristic Members
1 Samefingerpositionbutdifferenthandmovement (Agreement,Disagreement)-(Yesterday,Father,Luck,Year)-(is,Very,Hopeful)
2 Samehandmovementbutdifferentfingerposition (Luck,Summer)
3 Fixedhandmovementandfingerposition Good-Agreement-Disgreement
4 Dynamichandmovementandfingerposition Day-Forget-Mother
5 Rotationalmovements Blue-Green-Year
Table1. OurselectedPSLwordscategorizedintofivegroups
bydifferentcolors.Theredgroupshowsapairofgestures("Blue"and"Year")withrotationalhand
movementsondifferentaxes.Thebluegroupillustratesgestures("is"and"Very"),whichshare
fingerpositionsbuthaveminordifferencesinhandmovement.
Weaskedfivevolunteerstoperformsignlanguagegestures.Althoughrecordinghand-picked
sentencesmultipletimesbydifferentvolunteersisacommonwaytobuildasignlanguagedataset,
werandomlygeneratedthesentencesinourdataset,ensuringnohumanbiasisinvolvedinthe
selectionprocess.Forourprimarydataset(Dataset1-3),eachvolunteerrecorded,onaverage,100
one-wordsentences,300two-wordsentences,and200three-wordsentences.Moreover,weasked
eachvolunteertorecord20extrasentencescontaining4to8wordstoevaluatetheabilityofour
modelinlongerunseensentences(Dataset4-8).Additionally,aGUIapplicationwasdesignedand
implementedtoincreasethespeedandeaseoftherecordingprocess.
Eachdatainthedatasetisasequenceofvaluesreceivedfromthesensorsatarateof100Hz.
IMUfeaturesincludetotalacceleration,linearacceleration,gyroscope,andgravityaccelerationin
theX,Y,andZaxes.Combiningthesefeatureswithfivefeaturesreturnedfromflexiblesensors
forms17distinctfeaturesperdatapoint.Therefore,everysampleinthedatasetisatimeseries
datathatcanhavedifferentlengthsdependingontheexecutiontimeofeachgesture.
Table2depictstheaveragenumberofdatapointsderivedfromsequencescontainingoneto
threewordsacrossdifferentsubjects.Accordingtothetable,firstly,theaverageamountoftime
toexecutesentenceswiththesamelengthisdifferentfordifferentsubjects.Secondly,subjects
showdifferentbehaviorsfordifferentsentencelengthsanddonothaveaconstantspeedwhile
Blue
Year
is
Very
Fig.1. Illustrationofstep-by-stepexecutionoftwopairsofPSLglosses("Blue","Year")and("is","Very")
belongingtotwodistinctsimilaritygroups.Theblueglosseshavesimilarfingerpositionsbutdifferenthand
movements,whiletheredonesareexamplesofrotationalgestures.
ACMTrans.MultimediaComput.Commun.Appl.,Vol.1,No.1,Article.Publicationdate:June2024.8 Salmankhahetal.
Subject1 Subject2 Subject3 Subject4 Subject5 Average
Length1 13.0 11.7 11.6 9.7 10.7 11.34
Length2 19.0 19.1 18.2 19.8 20.2 19.26
Length3 25.4 28.0 27.3 30.2 26.5 27.48
Table2. TheaveragenumberofsamplelengthsinDataset1-3groupedbydifferentsubjects.Theslowestand
thefastestsubjectsineachrowaredenotedbygreenandred,respectively.
Fig.2. OursignlanguagegloveequippedwithanAdafruitBNO055IMUmountedonthebackofthehand
andfiveflexiblesensorsoneachfinger.
performinggestures.Forexample,onaverage,subject4hasperformedsingle-wordsentencesin
theleastamountoftimebutexhibitstheslowestpacefor3-wordsentences.Thesevariationspose
achallengeforthemodelindetectingwordtransitionswithinsentences.
ThedatasetisintendedtofacilitatetheresearchtowardPSLoranyothersignlanguagevariation.
Toensurethereproducibilityandaccessibilityofourwork,wehavemadethedatasetpublicly
available2.Weencourageotherresearchers,especiallytheonesworkingonPSL,touseourdataset
tobecomefamiliarwiththechallengesthatmayarisewhileworkingwithsignlanguagedatasets
ortoproposenewarchitecturesthatcouldachievebetterresultsthanourmodel.
4 PROPOSEDMETHOD
Inthissection,wedescribedifferentpartsofPenSLR.Thisincludesthedesignofthesignlanguage
glove,thedatacollectionprocess,preprocessingtechniques,thedeeplearningframework,andthe
proposedensemblingmethod.
4.1 GloveDesign
As depicted in Figure 2, our designed glove incorporates two types of sensors: an IMU sensor
mountedonthebackofthehandandfiveflexiblesensorsattachedtoeachfinger.Weutilized
ArduinoMegatocollectthesensors’data.TheIMUsensorsendsitsdatatothemicrocontrollervia
I2Cprotocol,whereastheflexiblesensorsachievethisbyanaloginputpins.
Oneofthemaingoalsofourresearchwastodesignalow-costbutaccurateglove.Asaresult,
weusedtheaffordableAdafruitBNO055IMU,whichcandeliveravarietyofmetrics,including
acceleration,orientation,andgravity,alongthreedistinctaxes.Capturingthesefeaturesenablesus
2https://github.com/Persian-Sign-Language/PenSLR-dataset
ACMTrans.MultimediaComput.Commun.Appl.,Vol.1,No.1,Article.Publicationdate:June2024.PenSLR:Persianend-to-endSignLanguageRecognitionUsingEnsembling 9
Temporal Feature Extractor
MLP Classifier
SpatFioea-tteumrepsoral
B × n2
Feature Fusion
Sample (B × Nf × Tinp) Output (B × Nc × Tout)
Fig.3. ArchitectureofourSeq2Seqmodel
todistinguishbetweendifferenthandmovementsbasedonorientationpatterns.flexiblesensors,
ontheotherhand,aresensitivetobending;thus,theirvalueschangewiththeclosingandopening
offingers.Consideringthesetupabove,ourgloveiscapableofcoveringawiderangeofhand
gestureswhilebeingcost-effectiveandeasilyrepairable.
4.2 Seq2SeqModel
Thissectiondescribesthespecificsofourmodel,suchaspreprocessingsteps,themodelarchitecture
andthelossfunctionweuseinourmodel.
4.2.1 Preprocessing. Since errors (caused by human or hardware factors) may affect the data
collectionprocess,itisnecessarytoidentifyoutliersandremovethemfromthedatasetbefore
usingthem.Toachievethis,weusedtheInterquartileRange(IQR)method.Inthismethod,thefirst
quartile𝑄 1,𝑓 andthethirdquartile𝑄 3,𝑓 ofdataarecalculatedforeachfeatureandthedifference
betweenthem𝐼𝑄𝑅 𝑓 isusedtodeterminearangefornon-outlierdata.Tobeprecise,whenadata
pointfallsoutsidetherangeof [𝑄 1,𝑓 −1.5·𝐼𝑄𝑅 𝑓,𝑄 3,𝑓 +1.5·𝐼𝑄𝑅 𝑓] foratleastonefeature,the
wholesampleisconsideredasanoutlierandwillberemovedfromthedataset.
Datanormalizationwasanotherstepinourpreprocessingpipeline.Tonormalizethedata,we
calculatedtheminimumvalue𝑥 𝑚𝑖𝑛,𝑓 andthemaximumvalue𝑥 𝑚𝑎𝑥,𝑓 ofeachfeaturebasedonthe
trainingdata.Then,wenormalizedthevalueofeachfeature𝑥 𝑓 intrainingandvalidationdataas
follows:
𝑥
𝑓
−𝑥
𝑚𝑖𝑛,𝑓
𝑥 𝑓 = 𝑥
𝑚𝑎𝑥,𝑓
−𝑥
𝑚𝑖𝑛,𝑓
(1)
Duringthetestingphase,thetestdatawerenormalizedwithrespecttothevalueswecalculated
beforehandusingthetrainingdataset.
4.2.2 ModelArchitecture. AsillustratedinFigure3,weemployedthreemaincomponentsinour
neuralnetworkmodel:afeaturefusion(FF)moduletocombineinputfeatures,atemporalfeature
extractor(TFE)todetecttemporalpatternsofeachsignlanguagegesture,andanMLPclassifierto
outputthepredictedgestureusingthefeaturesobtainedfromthepreviouscomponents.
Aswehadsamplesofvaryinglengthsinourdataset,weaddedzeropaddingtothesamples
inthesamebatch.Moreformally,weformeda𝐵×𝑁 𝑓 ×𝑇 𝑖𝑛𝑝 matrixforeachbatch,where𝐵 is
thebatchsize,𝑁 𝑓 isthenumberoffeatures,and𝑇 𝑖𝑛𝑝 isthelengthofthelongestsequenceinthe
batch.Bydoingthis,wewereabletoleveragetheefficiencyofbatchgradientdescent,thereby
acceleratingtheprocess.
ACMTrans.MultimediaComput.Commun.Appl.,Vol.1,No.1,Article.Publicationdate:June2024.
... ... ... ... ... ... ... ... ...10 Salmankhahetal.
TheFFcomponentaimstocombinethefeatureswitheachother.Itisessentialtoextractthe
dependenciesbetweenthevaluesofdifferentfeaturesbeforefeedingthemtotheTFEmodule,which
extractstemporaldependenciesbetweenthem.Moreover,FFextractslocaltemporaldependencies,
whichguidesTFEinfindingmorecomplexpatterns.Inotherwords,therawvaluesofdifferent
featuresinFFarecombinedwiththehelpofaconvolutionlayertocreatehigher-levelfeatures.
For this purpose, we used a 2D convolutional layer with a kernel size of 3×3 to extract local
time-relateddependencieswhilecombiningdifferentfeatures.
ThefeaturesgeneratedbytheFFcomponententertheTFEmodule,whichincludestwoconsecu-
tiveBidirectionalLSTM(BiLSTM)layerswithhiddenstatesizes𝑛 =64and𝑛 =128,respectively.
1 2
Eachhiddenstateunitℎ 𝑡 inBiLSTMcontainsinformationfromthepastandfutureoftheinput
sequenceattime𝑡;thus,ithelpsusdistinguishsignlanguagegestureswiththesamebeginning
butdifferentendings.Ultimately,eachhiddenstateunitinthelastBiLSTMlayeryieldsanoutput
intheformofamatrixwithsize𝐵×𝑛 ,representingthespatio-temporalfeaturesdistilledfrom
2
thesequencesofeachbatch.
The final step in our model is when the generated spatio-temporal feature matrices are fed
intoanMLPclassifiertoproducethepredictedsequence.Weusedashared3-layeredperceptron
networkcontaining64,32,and𝑁 𝑐𝑙𝑎𝑠𝑠𝑒𝑠+1perceptronsalongwiththeCTClossfunctiontopredict
theoutputlabelatanytimestep.SincetheCTClossfunctionrequiresanextrablanklabel,we
addedanextraunittothelastlayer,resultinginatotalof17distinctclasses.Wefurtherdiscuss
thedetailsoftheCTClossfunctioninSection4.2.3.
Toimprovetheperformanceofourneuralnetwork,weusedbatchnormalizationanddropout
techniques.Batchnormalizationcontributessignificantlytothefasterconvergenceofthenetwork
bypreventingthecovariantshift.Therefore,weuseditaftertheconvolutionlayeroftheFFmodule
andbetweeneachtwolayersoftheMLPclassifier.Later,inSection5.2,wewillexaminetheeffect
ofaddingbatchnormalizationbetweendifferentlayersofournetwork.Finally,weplacedadropout
layer with a probability of 0.3 after each of the BiLSTM layers. This prevents the model from
overfittingandallowsittolearngeneralizablepatternswhiletraining.
4.2.3 CTC Loss. The Connectionist Temporal Classification (CTC) loss function is one of the
mostpopularlossfunctionsinsequence-to-sequencetasks,suchasopticalcharacterrecognition
andspeechrecognition.Themainadvantageofthisfunctionisitscapabilitytoaligntheinput
sequencestothetargetsequence,especiallyinscenarioswherethisalignmentisnotone-to-one.In
otherwords,thisfunctionisusedwhenthelengthofinputandtargetsequencesdonotmatch,and
theexactlocationofeachoutputeventintheinputsequenceisnotspecified.Duetotheinherent
variabilityintheexecutionofsignlanguagegestures,usingtheCTClossfunctioninSLRcanbean
effectivesolutiontoimprovetheperformanceofdeeplearningmodelsinthisfield.Thatisbecause
peopleexecutewordsatdifferentspeeds,andtheremaybelongpausesbetweenmovements,which
canposeachallengeinaccuratelyrecognizingsignlanguagegestures.
CTCintroducesablanksymbolandallowstheneuralnetworktooutputrepeatedoccurrences
betweentheactualsymbols,thusenablingvariable-lengthoutputsequences.Forexample,both
(AA-B-C-CC)and(A-B–CC-C)representthesequenceABCC.Duringtraining,theCTCalgorithm
alignstheseoutputsequenceswiththetargetsequences,consideringallpossiblealignmentsusing
adynamicprogrammingapproach.Finally,itcomputesalossfunctionbasedonthenegativelog
probabilityofthesealignments,guidingthenetworktoproduceoutputsequencesthatarelikelyto
matchthetargetsequences.
ACMTrans.MultimediaComput.Commun.Appl.,Vol.1,No.1,Article.Publicationdate:June2024.PenSLR:Persianend-to-endSignLanguageRecognitionUsingEnsembling 11
4.3 SequenceAlignment
Inthissection,weproposeanovelmethodbasedonensemblingtoimprovetheperformanceof
SLR Seq2Seq models. One way to improve the models’ performance in deep learning is to use
ensembling.Inensemblemethods,severalmodelsaretrainedonthedata,andtheiroutputsare
usedtoproducethefinalprediction.Onewaytobuildeachofthesemodelsistosplitthedatainto
𝑘 foldsandtrain𝑘 differentmodelswhereeachmodelistrainedon𝑘−1foldsandvalidatedusing
theremainingfold(K-foldcross-validation).IntheSLRtask,however,givenasinglesample,the
outputsofthemodelstrainedusingK-foldcross-validationcouldhavevariablelengths,makingit
challengingtocombinethemtogeneratethefinalresult.Toovercomethisproblem,wesuggest
usingmultiplesequencealignment(MSA)algorithmstomaketheoutputofthemodelsequalin
lengthandthenperformavotingprocesstoproducethefinalprediction.
Needleman-Wunsch(NW)isanalgorithmthatusesdynamicprogrammingtocomputeoptimal
globalalignment(GA)betweentwostrings.AsshowninAlgorithm1,NWcreatesatableofsize
𝑚+1by𝑛+1toaligntwostringsoflength𝑚and𝑛.Then,thefirstrowandthefirstcolumnare
initializedasfollows:
𝐷 0,𝑖 =𝐷 𝑖,0 =𝑖∗𝑆 𝑔𝑎𝑝 (2)
where𝑆 𝑔𝑎𝑝 isthepenaltyforcreatingagapinoneofthesequences.Finally,itfillsthetableusing
Eq.3andEq.4:
𝐷 𝑖𝑗
=max 𝐷𝐷 𝑖𝑖 −− 11 ,, 𝑗𝑗− +1 𝑆+ 𝑔𝑎𝑆 𝑝(𝑋 𝑖,𝑌 𝑗)
(3)
𝐷 𝑖,𝑗−1+𝑆
𝑔𝑎𝑝

(cid:40)
𝑆(𝑐 ,𝑐 ) =
𝑆
𝑚𝑎𝑡𝑐ℎ
if𝑐
1
=𝑐
2 (4)
1 2 𝑆 𝑚𝑖𝑠 o.w
where𝑋 and𝑌 are the input sequences,𝑆 𝑚𝑎𝑡𝑐ℎ is the score of matching two characters in the
sequences,and𝑆 𝑚𝑖𝑠 isthepenaltyofmismatchbetweentwocharacters.Then,thefinalalignment
scoreandthealignedsequencescanbeobtainedbybacktrackingfromthefinalposition [𝑚,𝑛]
Algorithm1PairwiseGlobalAlignment
1: Input: Sequences𝑠 1[0,...,𝑚−1] and𝑠 2[0,...,𝑛−1]
2: Parameters: 𝑆 𝑚𝑖𝑠,𝑆 𝑚𝑎𝑡𝑐ℎ,𝑆 𝑔𝑎𝑝
3: Output: 2AlignedSequences,TheAlignmentScore
4: Initialize𝑑𝑝[𝑚+1,𝑛+1] withzeros
5:
Initialize𝑑𝑝[𝑖,0] =𝑖×𝑆
𝑔𝑎𝑝
6:
for𝑖 from1to𝑚do
7: for 𝑗 from1to𝑛do
8: if𝑠 1[𝑖] =𝑠 2[𝑗] then
9: 𝑑𝑝[𝑖,𝑗] =max{𝑑𝑝[𝑖−1,𝑗 −1]+𝑆 𝑚𝑎𝑡𝑐ℎ,𝑑𝑝[𝑖−1,𝑗]+𝑆 𝑔𝑎𝑝,𝑑𝑝[𝑖,𝑗 −1]+𝑆 𝑔𝑎𝑝}
10: else
11: 𝑑𝑝[𝑖,𝑗] =max{𝑑𝑝[𝑖−1,𝑗 −1]+𝑆 𝑚𝑖𝑠,𝑑𝑝[𝑖−1,𝑗]+𝑆 𝑔𝑎𝑝,𝑑𝑝[𝑖,𝑗 −1]+𝑆 𝑔𝑎𝑝}
12: endif
13: endfor
14: endfor
15:
𝐴𝑙𝑖𝑔𝑛𝑒𝑑 𝑠1,𝐴𝑙𝑖𝑔𝑛𝑒𝑑 𝑠2=Backtrack(𝑑𝑝)
16:
return𝐴𝑙𝑖𝑔𝑛𝑒𝑑 𝑠1,𝐴𝑙𝑖𝑔𝑛𝑒𝑑 𝑠2,𝑑𝑝[𝑚,𝑛]
ACMTrans.MultimediaComput.Commun.Appl.,Vol.1,No.1,Article.Publicationdate:June2024.12 Salmankhahetal.
Algorithm2StarAlignment
1:
Input:𝑘 sequences𝑠 0to𝑠
𝑘−1
2: Output:𝑘 AlignedSequences,TheAlignmentScore
3: Calculatepairwisealignmentsandscoresbetweenallpairs(𝑠 𝑖,𝑠 𝑗)where𝑖 ≠ 𝑗
(cid:16) (cid:17)
4: Choosethesequence𝑠 𝑐 asthecenter,where𝑐 =argmax𝑗 (cid:205)𝑘 𝑖=− 01Score(𝑠 𝑗,𝑠 𝑖)
5: Sortindices𝑖 ≠𝑐 indecreasingorderwithrespecttoScore(𝑠 𝑐,𝑠 𝑖)
6: foreachindex𝑖 inthesortedindicesdo
7: Utilizethealignmentsbetween𝑠 𝑐and𝑠 𝑖toupdatethegaplocationsofthealignedsequences
𝑠 𝑐′ and𝑠′
𝑗
where 𝑗 ≤𝑖
8: endfor
9:
returnTheAlignedSequences𝑠 𝑖′,TotalAlignmentScore
to the starting position [0,0]. Although this method can be extended to align𝑘 sequences, its
exponentialcomputationalcomplexitypreventsusfromusingitinareal-timesystem.Therefore,a
methodwithlesscomputationalcomplexityispreferabletoalignthesequences.
StarAlignment(SA)isaheuristicmethodforapproximatingGA,primarilyappliedinbioin-
formatics for aligning DNA sequences. Algorithm 2 shows how SA aligns multiple sequences.
First,itcallsNWalgorithmoneachpairofinputsequencestocomputetheiralignmentsandthe
correspondingsimilarityscores.Then,thesequenceexhibitingthemaximumcumulativesimilarity
isdesignatedasthecentralsequence.Subsequently,othersequencesarearrangedindescending
orderbasedontheirsimilaritytothecentralsequence.Finally,traversingthesequencesinorder,
somegapsareaddedtothecentralstringandallpreviouslytraversedstrings(ifnecessary)to
equalize their lengths, using the alignments calculated in the first step. For instance, Figure 4
illustratesanstep-by-stepprogressofrunningSAonfivearbitrarysequences.
WeutilizeSAalongwithK-foldcross-validationtoenhancetheaccuracyofoursystem.Algorithm
3providesadetailedoverviewofourmethod.Inthetrainingphase,weuse5-foldcross-validation
totrainfivedistinctmodelsonDataset1-3,andwhentesting,weusethosemodelstopredictfive
S 2: A B - C B B C
S':A B - C B B C
S1 S2 S3 S4 S5 S 5:A B E F B B C S'2
:A B E F B B C
S' 2:A B - C B B C
S1 - 9 8 4 7 S 2:A B C B B C S'5 :A B - C F B - S' 5:A B E F B B C S' 2:A B - C B B C
S2 9 - 9 5 12 S1:A B C F B - 1 S' 1: A B - C F B - S' 5:A B E F B B C
S3 8 9 - 7 7 S': A - - C F B C S' 1: A B - C F B -
S4 4 5 7 - 3 S 2: A B C B B C 3 S' 3: A - - C F B C
S5 7 12 7 3 - S3: A - C F B C S' 3: - B - C F C C
28 35 31 19 29 S 2:A B C B B C
S 4:- B C F C C
Fig.4. AnillustrationofhowStarAlignmentalgorithmcomputesthesimilaritymatrixbetween5sequences
(𝑆 1=”𝐴𝐵𝐶𝐹𝐵”,𝑆 2=”𝐴𝐵𝐶𝐵𝐵𝐶”,𝑆 3=”𝐴𝐶𝐹𝐵𝐶”,𝑆 4=”𝐵𝐶𝐹𝐶𝐶”,𝑆 5=”𝐴𝐵𝐸𝐹𝐵𝐵𝐶”)anduseittoprogressively
alignthem.Sequence𝑆 isdesignatedasthecentersincethesumofsimilaritiesinthesecondcolumnisthe
2
highest.(𝑆
𝑚𝑎𝑡𝑐ℎ
=3,𝑆
𝑔𝑎𝑝
=−2,and𝑆
𝑚𝑖𝑠
=−1)
ACMTrans.MultimediaComput.Commun.Appl.,Vol.1,No.1,Article.Publicationdate:June2024.PenSLR:Persianend-to-endSignLanguageRecognitionUsingEnsembling 13
Algorithm3OurEnsemblingAlgorithm
1:
Input:Sample𝑥,MappingTfromthechosensignlanguagewordstoarbitrarycharacters
2: Output:ThePredictedSequence
3: Train5modelsvia5-foldcross-validation
4:
Predicttheoutputs(𝑠 0to𝑠 4)fortheinput𝑥
5:
Replaceeachcharacter𝑐ℎin𝑠
𝑖
with𝑇[𝑐ℎ]
6:
Passsequences𝑠
𝑖
toStarAignmenttogetsequences𝑠 𝑖′
7:
for𝑡 from0to𝑠′.𝑙𝑒𝑛𝑔𝑡ℎ−1do
0
8:
𝑜𝑢𝑡 =𝑜𝑢𝑡 +Vote(𝑠 0[𝑡],...,𝑠 4[𝑡])
9: endfor
10:
Removegapsfrom𝑜𝑢𝑡
11:
Usetheinversemappingof𝑇 toupdate𝑜𝑢𝑡𝑝𝑢𝑡
12:
return𝑜𝑢𝑡
sequences𝑠 to𝑠 foreachinputsample.Next,weusethepredictedsequencesastheinputsofthe
0 4
SAalgorithmtoobtainalignedsequences𝑠′ to𝑠′.Thefinalstepincludesvotingateachtimestep
0 4
𝑡 ofthefinalalignments.Inotherwords,themostprevalentcharacterat𝑡 isaddedtothefinal
resultunlessthecharacterisagap.ItshouldbenotedthatbeforeusingSA,itisnecessarytomap
thesignlanguagewordstoarbitrarycharacters.Therefore,werestorethewordsusinginverse
mappingafterthefinalvoting.Figure5depictstheexecutionoftheexplainedprocess.
Theintuitionbehindourmodelreliesonthefactthatthemodelstrainedondistinctfoldstend
tolearndifferentpatterns,especiallywhenthedatasetisnotverylarge.Thatis,theiraccuracyin
detectingdifferentclassesmayvary,andeachcouldmanagetopredictsegmentsoftheground
Model 1 A B C F B
Model 2 A B C B B C A B - C F B -
A B - C B B C
Sequence
A - - C F B C
Alignment
- B - C F C C
Model 3 A C F B C
A B E F B B C
Sample (Nf × T) Voting
Label: A B C F B C
A B - C F B C
Model 4 B C F C C Gap Ignoring
Predicted Sequence: A B C F B C
Model 5 A B E F B B C
Fig.5. Anexampleoftheexecutionofourensemblingalgorithm.Theoutputsproducedbyallmodelsare
alignedusingStarAlignment,afterwhichavotingprocessisperformedtoobtainthemostprobablegesture
(denotedbycharactersAtoF)ateachposition.Finally,thegapsareignored,andthefinalresultisgenerated.
Thisexampleisasituationwhenthebestmodel(Model1)cannotcompletelypredictthegroundtruth
sequence,buttheensemblingmethodhelpsthesystemtogenerateitsuccessfully.
ACMTrans.MultimediaComput.Commun.Appl.,Vol.1,No.1,Article.Publicationdate:June2024.14 Salmankhahetal.
truthsequence.Asaresult,byaligningtheirpredictionsandvotingbetweenthem,thefinalresult
isexpectedtohavealowererrorratethanthepredictionofthebestone.Aswillbeexplainedin
Section5.3,thismethoddoesnotalwaysimprovetheaccuracy.Thissituationoccurswhenthebest
modelhasmuchhigheraccuracythanothersorwhenallmodelsdonothaveenoughaccuracy,
sotheperformancedropsbyvotingbetweenthepredictions.However,theresultsdepictthat,on
average,themethodcouldsignificantlyimprovetheperformanceofthesystem.Furthermore,since
ourensemblingtechniquedoesnotdependonanylinguisticfeature,itcanbeadaptedforstudies
onothervariationsofsignlanguageorlanguage-independentsequence-to-sequencetaskssuchas
gesturerecognition.
5 EXPERIMENTSETUPANDRESULTS
Inthissection,weexaminetheperformanceoftheproposedmodelandourensemblingalgorithm.
Weuseasubject-independentapproachinallexperimentsunlesswhereitismentionedexplicitly.
Byusingleave-one-subject-outcross-validation,wetraineachmodelonthedataof4subjectsand
testitwiththedataoftheremainingsubject.Inthetrainingphase,weuseDataset1-3,mentioned
in Section 3, which includes sentences with up to three words. In addition, we use the 5-fold
cross-validationapproachtoapplyourensemblingalgorithm,whereeachfoldcontains20%of
thedata.Also,weusetheAdamWoptimizerwithabatchsizeof9andlearningratescheduling
in 3 steps to optimize the cost function as much as possible. On the other hand, when testing,
our experiment settings involve both Dataset1-3 and Dataset4-8, enabling us to evaluate user-
independentgeneralizationaswellasgeneralizationinlongersentences,respectively.Furthermore,
in the testing phase, we assigns scores to gap penalties, mismatches, and matches as follows:
𝑆 𝑔𝑎𝑝 = −1,𝑆 𝑚𝑖𝑠 = −1,and𝑆 𝑚𝑎𝑡𝑐ℎ = 0.Thisconfigurationwasdeterminedtobeoptimalforour
recognitiontaskthroughrigorousexperimentation,resultinginthebestperformance.
5.1 EvaluationMetrics
Weusethefollowingfourmetricstoanalyzetheperformanceofourmodels:
SequenceLengthAccuracy(SLAcc):Itdepictsthepercentageofsequenceswhoselengthsare
accuratelypredictedbythemodelrelativetothetotalnumberofsequences.
SequenceAccuracy(SAcc):Itindicatesthepercentageofsequencesthatexactlymatchtheir
groundtruthoutofthetotalnumberofsequences.Sinceachievinghighresultsinthismetricis
quitechallenging,mainlywhenthedatasetcontainslengthysequences,manypreviouslyconducted
studieshavenotreportedtheirresultsbasedonthismetric.Despitethis,wereportourresultsto
showtheeffectivenessofourensemblingapproachincorrectingthemispredictedsequences.
WordAccuracy(WAcc):ThismetricisdefinedbasedonanothermetriccalledWordErrorRate
(WER),whichisusedintaskswherethelengthofthepredictedsentencescouldbedifferentfrom
theiractuallength.TocalculateWERforagivenpairofgroundtruthandpredictedsequences,we
usethefollowingequation:
𝑆 +𝐷+𝐼
𝑊𝐸𝑅 = ×100 (5)
𝐿
where𝐿isthelengthofthegroundtruthsequence,𝑆isthenumberofsubstitutions,𝐼 isthenumber
ofinsertions,and𝐷 isthenumberofdeletionsneededtoconvertthegroundtruthsequenceto
the predicted sequence. In fact, the numerator is the widely used Levenshtein distance, which
calculatestheminimumnumberofsingle-charactereditsneededtotransformonesequenceinto
another.Inourtask,eachindividualsignlanguagegestureinpredictedsequencescorrespondstoa
characterintheLevenshteindistancecalculation.Then,wecomputewordaccuracytoshowthe
ACMTrans.MultimediaComput.Commun.Appl.,Vol.1,No.1,Article.Publicationdate:June2024.PenSLR:Persianend-to-endSignLanguageRecognitionUsingEnsembling 15
similarityofthepredictedsequencetothecorrespondinggroundtruth:
WAcc=100−WER (6)
Finally,wecalculatetotalwordaccuracyasfollows:
(cid:205)
WAcc𝑖
TotalWAcc= (7)
𝑁
where𝑊𝐴𝑐𝑐 𝑖 isthewordaccuracyforthe𝑖-thsampleinthedataset,and𝑁 isthetotalnumberof
samples.
WeightedWordAccuracy(WWAcc):Insteadofcomputingasimpleaverageovertheobtained
wordaccuracies,wecantakeaweightedaveragetoamplifytheimpactoflengthysentencesinthe
finalresult.Thus,wecancalculateweightedwordaccuracyusingthefollowingequation:
(cid:205) WAcc𝑖.𝐿
𝑖
WWAcc= (8)
(cid:205)𝐿
𝑖
Contrastingtheseresultswiththepreviousmetriccanshowcasetheextendabilityofourapproach
tolongersentences.
5.2 AblationStudy
AsmentionedinSection4.2.2,ourmodelincludesa3×3convolutionallayer,twoBiLSTMlayers,
andanMLPclassifierwithbatchnormalizationbetweenanytwolayers.Toshowtheimpactof
differentcomponentsintheproposedarchitecture,weremoveorchangedifferentpartsofthe
model.Table3showstheresultsofthisworkwithoutapplyingourensemblingmethod.Inthe
lastcolumnofthistable,theaccuraciesobtainedfromtheperformanceoftheproposedmodelare
shown.Othercolumnsdepicttheamountofchangeinthemodel’sperformanceafterremoving
orchangingoneofthemodel’scomponents.Itshouldbenotedthatthenumbersshownarethe
averagetestresultsofthemodelacrossdifferentsubjectswhilehavingtrainedonaspecificfoldof
Dataset1-3withoutapplyingourensemblingmethod.
The modelachievesa WAccof 90.39% and aWWAcc of 90.85%, resultingin an exact match
ratioof81.69%.Moreover,themodelisabletoproducesequenceswiththesamelengthasthe
groundtruthin91.47%ofsamples.Duetothenegativenessofothernumbersinthetable,applying
anyofthementionedchangesinthearchitecturecausestheperformanceofthemodeltodrop.
AvoidingtheuseoftheconvolutionalorBiLSTMlayershasasignificantimpactonthemodel’s
performance,whichrespectivelyindicatesthehighimportanceoftheFeatureFusioncomponent
andtheroleofthebidirectionalLSTMindetectinggestureswiththesameopeningmovements.
Also,usingjustoneLSTMlayerinsteadoftwolayersorusinga5×5kernelfortheconvolution
layerreducestheperformance.TheformerisduetothefactthatusingtwolayersofLSTMcan
helpusdetectmorecomplexpatternsinthedata,andthelatterisbecausethe3×3kernelallows
CNN LSTM Bidirectional BatchNorm OurModel
No 5×5 1 No 0 1
SLAcc -4.92 -3.05 -4.22 -12.33 -1.50 -1.86 91.47
SAcc -15.12 -2.51 -1.55 -12.09 -4.41 -0.23 81.69
WAcc -8.91 -2.07 -1.24 -6.97 -2.71 -0.39 90.39
WWAcc -8.71 -1.38 -0.70 -6.01 -2.56 -0.11 90.85
Average -9.41 -2.25 -1.93 -9.35 -2.79 -0.65 88.60
Table3. Thepercentageofperformancedegradationofmodelsresultingfromthedeletionormodificationof
eachcomponentwithinourproposedarchitecture.
ACMTrans.MultimediaComput.Commun.Appl.,Vol.1,No.1,Article.Publicationdate:June2024.16 Salmankhahetal.
featurestobecombinedinmoreways.Finally,theuseofbatchnormalizationbetweeneachpairof
perceptronlayersmakesthelengthofthepredictedsentencesmuchclosertotheiractualvalue,
whichultimatelyincreasestheaccuracyofthemodel.
5.3 EnsemblingPerformance
Inthissection,weexaminetheperformanceoftheproposedensemblingmethodanditseffectiveness
inimprovingaccuracy.AccordingtoTable3,sincetheresultsofthemodelthatusesonebatch
normalizationlayer(𝑀 )are,onaverage,closetoourproposedmodel(𝑀 ),wewillcomparethese
1 2
models in our experiments. Also, in addition to Dataset1-3, we use Dataset4-8 to evaluate the
performanceofthemodelinlongersentences.
Thetablesinthissectionshowtheperformanceof𝑀 and𝑀 beforeandafterapplyingensem-
1 2
blingondifferentdatasetsandwithdifferentapproaches.ThemodelsrelatedtoTable4andTable
5weretrainedonthedataof4subjectsfromDataset1-3andtestedonthedataoftheremaining
subjectfromDataset1-3andDataset4-8,respectively.Ontheotherhand,Table6showstheperfor-
manceofthemodelsbybeingtrainedonthedataof4subjectsfromDataset1-3andtestedonall
thedataofDataset4-8;therefore,theresultsinthistablearenotbasedonthesubject-independent
approach.Itshouldbenotedthatineachcell,thenumbersontheleftindicatetheperformance
ofthebestmodelamongthefivemodelstrainedusing5-foldcross-validationbeforeapplying
ensembling.Incontrast,thenumbersontherightshowtheperformanceoftheensembledmodels.
Althoughsomecellsinthetablesshowminordegradation,intheaveragecolumn,eitheroneof
theensembledmodelsoutperformsothersacrossall12cases.Notably,ineightcases,thetoptwo
ranksarealsosecuredbyensembledmodels.Intheremainingfourcases,𝑀 aloneoutperformsthe
2
ensembledmodelof𝑀 duetoitsinherentexcellence.Byassigningnumericalvaluesfrom1to4to
1
thespectrumofcolors,rangingfromredtogreen,andsubstitutingthemwiththecorresponding
valuesineachcell,wecancomputeanaveragerankforeachmodel.Thisprovidesanoverallview
of the effect of the ensembling method. The result of this analysis is represented in Table 7. It
illustratesasimilarpatternintheresultsofTable4andTable6,wheretheensembledmodelsof
𝑀 and𝑀 areinthefirstandsecondplace,respectively.Onthecontrary,theresultsofTable5
2 1
show𝑀 anditsensembleyieldingthebestresults.Thisobservationindicatesthattheuseofbatch
2
normalizationbetweenalllayersintheclassifierimprovestheperformanceof𝑀 ,suchthatitcan
2
performevenbetterthantheensembledversionof𝑀 .Inotherwords,batchnormalizationenables
1
Subject1 Subject2 Subject3 Subject4 Subject5 Average
Metric Model
1 93.22/90.58 91.54/95.34 96.01/95.51 95.45/97.56 97.55/96.73 94.79/95.15
SLAcc
2 86.94/90.91 96.03/97.58 95.85/93.69 96.10/97.89 96.73/98.85 94.33/95.78
1 82.31/82.98 86.87/89.64 89.04/90.70 83.77/85.39 91.82/91.16 86.76/87.95
SAcc
2 78.02/82.81 89.12/91.88 86.38/85.71 87.01/87.50 91.82/91.82 86.46/87.92
1 91.32/91.76 93.09/94.39 94.38/95.60 90.34/91.31 95.34/95.74 92.89/93.75
WAcc
2 89.39/91.76 94.27/95.54 93.27/92.91 91.75/92.32 95.74/95.72 92.87/93.63
1 91.10/91.48 93.68/95.00 93.82/94.94 92.22/93.13 95.94/95.72 93.35/94.04
WWAcc
2 89.43/91.33 94.77/96.02 92.45/92.37 93.66/94.18 96.17/96.02 93.29/93.97
Table4. Evaluationofmodelsbasedonthesubject-independentapproachonDataset1-3.Eachcolumn
(excepttheaveragecolumn)illustratestheresultsobtainedbytestingthemodelforthecorrespondingsubject
whilehavingtrainedonthedatafromothersubjects.Thenumbersinthefirstandsecondrowsofeach
cellrepresentmodels𝑀 and𝑀 ,respectively.Thenumbersontheleftindicatetheperformancebefore
1 2
ensembling,whilethenumbersontherightindicatetheperformanceafterensembling.
ACMTrans.MultimediaComput.Commun.Appl.,Vol.1,No.1,Article.Publicationdate:June2024.PenSLR:Persianend-to-endSignLanguageRecognitionUsingEnsembling 17
Subject1 Subject2 Subject3 Subject4 Subject5 Average
Metric Model
1 55.00/75.00 75.00/90.00 100.0/100.0 95.00/100.0 85.00/85.00 82.00/90.00
SLAcc
2 90.00/90.00 85.00/90.00 100.0/100.0 100.0/100.0 95.00/95.00 94.00/95.00
1 45.00/60.00 55.00/60.00 90.00/95.00 85.00/90.00 55.00/70.00 66.00/75.00
SAcc
2 75.00/75.00 45.00/55.00 95.00/95.00 95.00/95.00 65.00/75.00 75.00/79.00
1 82.04/88.90 90.20/90.73 94.38/95.00 96.92/98.33 89.49/93.87 90.61/93.37
WAcc
2 94.88/93.88 86.68/88.90 95.00/95.00 99.17/99.17 94.61/95.95 94.07/94.58
1 83.33/90.83 90.83/90.83 93.33/94.17 97.50/98.33 90.83/94.17 91.16/93.67
WWAcc
2 95.83/95.00 87.50/89.17 94.17/94.17 99.17/99.17 94.17/95.83 94.17/94.67
Table5. Evaluationofmodelsbasedonthesubject-independentapproachonDataset4-8
Subject1 Subject2 Subject3 Subject4 Subject5 Average
Metric Model
1 86.00/91.00 84.00/95.00 89.00/92.00 84.00/94.00 87.00/94.00 86.00/93.20
SLAcc
2 90.00/94.00 88.00/94.00 87.00/95.00 93.00/96.00 91.00/97.00 89.80/95.20
1 74.00/81.00 73.00/86.00 79.00/90.00 75.00/88.00 73.00/84.00 74.80/85.80
SAcc
2 79.00/86.00 77.00/84.00 79.00/88.00 87.00/92.00 82.00/90.00 80.80/88.00
1 92.46/95.21 92.40/96.15 95.30/97.05 94.27/97.03 93.21/96.08 93.53/96.30
WAcc
2 95.55/96.50 94.14/95.28 94.78/96.66 96.46/97.70 95.99/97.37 95.38/96.70
1 92.17/95.33 92.83/96.17 95.33/97.17 94.33/97.00 93.67/96.17 93.67/96.37
WWAcc
2 95.50/96.50 94.17/95.50 95.00/96.83 96.67/97.67 96.00/97.33 95.47/96.77
Table6. Evaluationofmodelsbasedonthesubject-dependentapproachonDataset4-8.Eachcolumn(except
theaveragecolumn)illustratestheresultsobtainedbytestingthemodelforthecorrespondingsubjectwhile
havingtrainedonthedatafromallsubjects.
themodeltogeneralizebetterinlongerunseensequenceswhenthesubject-independentapproach
isused.
Sofar,wehavedemonstratedthatdespitecausingminordegradationsincertainmetricsfor
specificsubjects,theapplicationofensembling,onaverage,improvestheoverallperformance.
Now,wewillexaminetheextentofthisimpact.Table4indicatesthatensemblingboostsboth
WAccandSLAcc,resultingin1.19%and1.46%improvementinSAccof𝑀1and𝑀2,respectively.
Table5revealsthatensemblingeffectivelyaddresses𝑀 ’sweaknessinrecognizingsequenceswith
1
4to8words,bringingitcloserto𝑀 ’sperformancewitha2.76%and0.51%increaseinWAccfor𝑀
2 1
and𝑀 ,respectively.Thistranslatestoaremarkable9.00%and4.00%enhancementinrecognizing
2
completesequencesforeachmodel.Moreover,themodelsachievedslightlyhigherresultsinterms
of WWAcc, indicating that they are capable of maintaining their accuracy while dealing with
longersentences.TheresultsofTable6showthattheimpactofensemblingisevenhigherwhile
usingthesubject-dependentapproach.Inthatexperiment,𝑀 and𝑀 witnesseda2.77%and1.32%
1 2
increaseinWAcc,resultinginan11.00%and7.20%improvementinrecognizingcompletesequences,
respectively. Moreover, our method significantly improved the ability of models to predict the
lengthofoutputsequence,resultingin7.20%and5.40%improvement,respectively.
Towrapup,ourexperimentsinthissectionshowtwomainpoints.First,abettergeneralization
inlongersequencesistheunderlyingcauseforusingbatchnormalizationbetweenalllayersin
theclassifier.Second,usingourensemblingapproachyieldsconsiderableimprovementsinboth
models,makingitasuitablechoiceforourSLRtask.
ACMTrans.MultimediaComput.Commun.Appl.,Vol.1,No.1,Article.Publicationdate:June2024.18 Salmankhahetal.
Model Table2 Table3 Table4
1 1.96/2.83 1.21/2.33 1.13/3.21
Averageranks
2 2.08/3.13 2.75/3.71 1.95/3.71
Table7. Theaveragerankingofeachmodelbasedontheresultsofexperiments.Theranksrangefrom1to4
correspondingtomodels’performanceinascendingorder.
6 LIMITATIONSANDFUTUREWORK
6.1 NeglectingNon-manualMarkers
AlthoughPenSLRiscapableofcapturingdiversehandmovementsandfingerpositions,itlacks
the ability to detect non-manual markers, such as facial expressions and head movements. As
previouslymentioned,non-manualmarkersplayacrucialroleinallsignlanguagevariations.In
fact,somegesturesmaysharesimilarhandmovementsandfingerconfigurations,yetdifferin
subtlefacialexpressions.Therefore,incorporatingnon-manualmarkersnotonlyexpandstherange
ofglossesbutalsoenhancesthesystem’sabilitytodifferentiatebetweengestureswithsimilar
manualmarkers.Inthefuture,wecouldmanipulateandimproveourglove-baseddesignsothat
wecantakeintoaccountthesenon-manualmarkers.Toachievethisgoal,onepromisingapproach
involvesemployingstretchablesensorsattachedtotheface,assuggestedin[28],orharnessing
earbudsignals,asexploredinSmartASL[8].
6.2 LimitedDataset
AsexplainedinSection3,wecollectedadatasetofover3000samplesfrom16PSLglosses.Although
ourdatasetstandsasthelargestwithinthePSLdomain,thePSLlexiconextendsbeyondthislimited
scope,andforeffectivecommunicationamongsignlanguageusers,abroadervocabularyisessential.
Therefore,itiscrucialtoprioritizetheexpansionofourdatasettoincludeamoreextensivearray
ofPSLsigns.ThiswillfacilitatemorecomprehensivecommunicationwithinthePSLcommunity.
Additionally,collaboratingwiththePersianDeafCommunityAssociation,particularlyinthedata
gatheringprocess,couldhelpustoimprovethequalityofourdatasetandtailorittoreal-world
applications.
6.3 EnsemblingLimitations
Our ensembling algorithm does not take into account any linguistic relationship between the
predictedsigns.Althoughthiscountsasabenefitinlanguage-independenttasks,suchasgesture
recognition,thesituationisdifferentwhenlinguisticfeaturesaredecisive.Forinstance,insign
languagetranslation,wherethegoalistoconvertsignlanguageglossesintonaturallanguagetext,
ouralgorithmmayfallshortinpreservinglexicalandgrammaticalcorrectness.Thislimitation
arisesbecausetheuseofsequencealignmentcandisruptthecoherenceofsentencestructures.
However,toovercomethisissue,alanguagemodelcanbeusedtorevisetheoutputofensembling
topreservethesentencestructure.
7 CONCLUSION
Inthispaper,weproposePenSLR,thefirstPersiansignlanguagerecognitionsystemcapableof
detectingsignlanguagesentencesinanend-to-endmanner.Toachievethis,wedesignacost-
effective glove-based system that can capture a wide range of manual markers. Moreover, we
constructandpublishthelargestPSLdatasettodate,comprisingmorethan3000samplesfrom
16commonlyusedsigns.Totackletherecognitiontask,wedeployaCRNNarchitecturecoupled
withtheCTClossfunction,facilitatingthegenerationofvariable-lengthoutputsbasedoninput
ACMTrans.MultimediaComput.Commun.Appl.,Vol.1,No.1,Article.Publicationdate:June2024.PenSLR:Persianend-to-endSignLanguageRecognitionUsingEnsembling 19
signals.Furthermore,weintroduceanovelensemblingtechniquebasedonsequencealignmentand
utilizemultipleinstancesofourmodeltogenerateanimprovedoutputsequence.PenSLRachieves
an impressive 94.07% word-level accuracy while testing on longer unseen sentences using the
subject-independentapproach.Notably,whenemployingourensemblingscheme,thisrepresents
a0.51%enhancementovertheperformanceachievedsolelywiththeCRNNmodel,leadingtoa
94.58%word-levelaccuracy.Thiseffectoftheensemblingisevenhigherwhenasubject-dependent
approachisused,wherethetestdatabelongstothesamesubjectsasthetrainingdata.
ACKNOWLEDGMENTS
Wewouldliketoextendaspecialthankstothefivevolunteerswhogenerouslydonatedtheirtime
andefforttohelpusrecordandcollectthenecessarydata.Theirunwaveringcommitmentwas
crucialtothesuccessofourproject.
REFERENCES
[1] SalarBasiri,AlirezaTaheri,AliF.Meghdari,MehrdadBoroushaki,andMinooAlemi.2023. DynamicIranianSign
LanguageRecognitionUsinganOptimizedDeepNeuralNetwork:AnImplementationviaaRobotic-BasedArchitecture.
InternationalJournalofSocialRobotics15,4(April2023),599–619. https://doi.org/10.1007/s12369-021-00819-0
[2] JiannanChao,FurongTang,andLeiXu.2022. DevelopmentsinAlgorithmsforSequenceAlignment:AReview.
Biomolecules12,4(April2022),546. https://doi.org/10.3390/biom12040546
[3] TusharChouhan,AnkitPanse,AnveshKumarVoona,andS.M.Sameer.2014.Smartglovewithgesturerecognition
abilityforthehearingandspeechimpaired.InProceedingsofthe2014IEEEGlobalHumanitarianTechnologyConference
-SouthAsiaSatellite(GHTC-SAS).IEEE,Trivandrum,India,105–110. https://doi.org/10.1109/GHTC-SAS.2014.6967567
[4] JosephDelPreto,JosieHughes,MatteoD’Aria,MarcoDeFazio,andDanielaRus.2022.AWearableSmartGloveand
ItsApplicationofPoseandGestureDetectiontoSignLanguageClassification.IEEERoboticsandAutomationLetters7,
4(Oct.2022),10589–10596. https://doi.org/10.1109/LRA.2022.3191232
[5] AlexGraves,SantiagoFernández,FaustinoGomez,andJürgenSchmidhuber.2006.Connectionisttemporalclassification:
labellingunsegmentedsequencedatawithrecurrentneuralnetworks.InProceedingsofthe23rdinternationalconference
onMachinelearning(ICML).ACMPress,Pittsburgh,Pennsylvania,369–376. https://doi.org/10.1145/1143844.1143891
[6] HezhenHu,WengangZhou,JunfuPu,andHouqiangLi.2021.Global-LocalEnhancementNetworkforNMF-Aware
SignLanguageRecognition. ACMTransactionsonMultimediaComputing,Communications,andApplications17,3
(Aug.2021),1–19. https://doi.org/10.1145/3436754
[7] YinchengJin,YangGao,YanjunZhu,WeiWang,JiyangLi,SeokminChoi,ZhangyuLi,JagmohanChauhan,AnindK.
Dey,andZhanpengJin.2021.SonicASL:AnAcoustic-basedSignLanguageGestureRecognizerUsingEarphones.In
ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitousTechnologies(IMWUT),Vol.5.ACM,NewYork,
NY,USA,1–30. https://doi.org/10.1145/3463519
[8] YinchengJin,ShiboZhang,YangGao,XuhaiXu,SeokminChoi,ZhengxiongLi,HenryJ.Adler,andZhanpeng
Jin.2023. SmartASL:"Point-of-Care"ComprehensiveASLInterpreterUsingWearables.InProceedingsoftheACM
onInteractive,Mobile,WearableandUbiquitousTechnologies(IMWUT),Vol.7.ACM,NewYork,NY,USA,1–21.
https://doi.org/10.1145/3596255
[9] AliKarami,BahmanZanj,andAzadehKianiSarkaleh.2011.Persiansignlanguage(PSL)recognitionusingwavelet
transformandneuralnetworks.ExpertSystemswithApplications38,3(March2011),2661–2667. https://doi.org/10.
1016/j.eswa.2010.08.056
[10] SaraAskariKhomamiandSinaShamekhi.2021. PersiansignlanguagerecognitionusingIMUandsurfaceEMG
sensors.Measurement168(Jan.2021),108471. https://doi.org/10.1016/j.measurement.2020.108471
[11] WenguoLi,ZhizengLuo,andXugangXi.2020. MovementTrajectoryRecognitionofSignLanguageBasedon
OptimizedDynamicTimeWarping.Electronics9,9(Aug.2020),1400. https://doi.org/10.3390/electronics9091400
[12] YuxuanLiu,XijunJiang,XinggeYu,HuaidongYe,ChaoMa,WanyiWang,andYoufanHu.2023.Awearablesystem
forsignlanguagerecognitionenabledbyaconvolutionalneuralnetwork. NanoEnergy116(Nov.2023),108767.
https://doi.org/10.1016/j.nanoen.2023.108767
[13] ChenghongLu,ShingoAmino,andLeiJing.2023. DataGlovewithBendingSensorandInertialSensorBasedon
WeightedDTWFusionforSignLanguageRecognition. Electronics12,3(Jan.2023),613. https://doi.org/10.3390/
electronics12030613
[14] AnshulMittal,PradeepKumar,ParthaPratimRoy,RamanBalasubramanian,andBidyutB.Chaudhuri.2019. A
ModifiedLSTMModelforContinuousSignLanguageRecognitionUsingLeapMotion.IEEESensorsJournal19,16
ACMTrans.MultimediaComput.Commun.Appl.,Vol.1,No.1,Article.Publicationdate:June2024.20 Salmankhahetal.
(Aug.2019),7056–7063. https://doi.org/10.1109/JSEN.2019.2909837
[15] NitinThoppeyMuralidharan,RahulRamS.,RohidhM.R.,SenthilNathanM.,andHarikumarM.E.2022.Modelling
ofSignLanguageSmartGloveBasedonBitEquivalentImplementationUsingFlexSensor.In2022International
ConferenceonWirelessCommunicationsSignalProcessingandNetworking(WiSPNET).IEEE,Chennai,India,99–104.
https://doi.org/10.1109/WiSPNET54241.2022.9767137
[16] SaulB.NeedlemanandChristianD.Wunsch.1970. Ageneralmethodapplicabletothesearchforsimilarities
intheaminoacidsequenceoftwoproteins. JournalofMolecularBiology 48,3(March1970),443–453. https:
//doi.org/10.1016/0022-2836(70)90057-4
[17] AliSanjabi,AbbasAliBehmanesh,ArdavanGuity,SaraSiyavoshi,MartinWatkins,andJulieA.Hochgesang.2016.
ZabanEsharehIrani(ZEI)andItsFingerspellingSystem. SignLanguageStudies 16,4(2016),500–534. https:
//doi.org/10.1353/sls.2016.0010
[18] PanneerSelvamSanthalingam,AlAminHosain,DingZhang,ParthPathak,HuzefaRangwala,andRajaKushalnagar.
2020. mmASL:Environment-IndependentASLGestureRecognitionUsing60GHzMillimeter-waveSignals.In
ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitousTechnologies(IMWUT),Vol.4.ACM,NewYork,
NY,USA,1–30. https://doi.org/10.1145/3381010
[19] JiachengShangandJieWu.2017. ARobustSignLanguageRecognitionSystemwithMultipleWi-FiDevices.In
ProceedingsoftheWorkshoponMobilityintheEvolvingInternetArchitecture(MobiArch).ACM,LosAngelesCAUSA,
19–24. https://doi.org/10.1145/3097620.3097624
[20] S.Sharma,R.Gupta,andA.Kumar.2023. Continuoussignlanguagerecognitionusingisolatedsignsdataand
deeptransferlearning. JournalofAmbientIntelligenceandHumanizedComputing14,3(March2023),1531–1542.
https://doi.org/10.1007/s12652-021-03418-z
[21] JulieD.Thompson,DesmondG.Higgins,andTobyJ.Gibson.1994. CLUSTALW:improvingthesensitivityof
progressivemultiplesequencealignmentthroughsequenceweighting,position-specificgappenaltiesandweight
matrixchoice.NucleicAcidsResearch22,22(1994),4673–4680. https://doi.org/10.1093/nar/22.22.4673
[22] ZhiboWang,TengdaZhao,JinxinMa,HongkaiChen,KaixinLiu,HuajieShao,QianWang,andJuRen.2020.Hear
SignLanguage:AReal-timeEnd-to-EndSignLanguageRecognitionSystem.IEEETransactionsonMobileComputing
21,7(2020),2398–2410. https://doi.org/10.1109/TMC.2020.3038303
[23] FengWen,ZixuanZhang,TianyiyiHe,andChengkuoLee.2021. AIenabledsignlanguagerecognitionandVR
spacebidirectionalcommunicationusingtriboelectricsmartglove.NatureCommunications12,1(Sept.2021),5378.
https://doi.org/10.1038/s41467-021-25637-w
[24] JianWu,LuSun,andRoozbehJafari.2016.AWearableSystemforRecognizingAmericanSignLanguageinReal-Time
UsingIMUandSurfaceEMGSensors.IEEEJournalofBiomedicalandHealthInformatics20,5(Sept.2016),1281–1290.
https://doi.org/10.1109/JBHI.2016.2598302
[25] HuaiwenZhang,ZihangGuo,YangYang,XinLiu,andDeHu.2023. C2ST:Cross-modalContextualizedSequence
TransductionforContinuousSignLanguageRecognition.InProceedingsofthe2023IEEE/CVFInternationalConference
onComputerVision(ICCV).IEEE,Paris,France,20996–21005. https://doi.org/10.1109/ICCV51070.2023.01925
[26] QianZhang,JiaZhenJing,DongWang,andRunZhao.2022.WearSign:PushingtheLimitofSignLanguageTranslation
UsingInertialandEMGWearables.InProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitousTechnologies
(IMWUT),Vol.6.ACM,NewYork,NY,USA,1–27. https://doi.org/10.1145/3517257
[27] QianZhang,DongWang,RunZhao,andYinggangYu.2019.MyoSign:enablingend-to-endsignlanguagerecognition
withwearables.InProceedingsofthe24thInternationalConferenceonIntelligentUserInterfaces(IUI).ACM,Marinadel
RayCalifornia,650–660. https://doi.org/10.1145/3301275.3302296
[28] ZhihaoZhou,KyleChen,XiaoshiLi,SonglinZhang,YufenWu,YihaoZhou,KeyuMeng,ChenchenSun,Qiang
He,WenjingFan,EndongFan,ZhiweiLin,XulongTan,WeiliDeng,JinYang,andJunChen.2020. Sign-to-speech
translationusingmachine-learning-assistedstretchablesensorarrays.NatureElectronics3,9(June2020),571–578.
https://doi.org/10.1038/s41928-020-0428-6
[29] RonglaiZuoandBrianMak.2024.ImprovingContinuousSignLanguageRecognitionwithConsistencyConstraints
andSignerRemoval.ACMTransactionsonMultimediaComputing,Communications,andApplications20,6(June2024),
1–25. https://doi.org/10.1145/3640815
ACMTrans.MultimediaComput.Commun.Appl.,Vol.1,No.1,Article.Publicationdate:June2024.