Improved Regret Bounds for Bandits with Expert Advice
Nicol`o Cesa-Bianchi nicolo.cesa-bianchi@unimi.it
Universit`a degli Studi di Milano, Milan, Italy
Politecnico di Milano, Milan, Italy
Khaled Eldowa khaled.eldowa@unimi.it
Universit`a degli Studi di Milano, Milan, Italy
Politecnico di Torino, Turin, Italy
Emmanuel Esposito emmanuel@emmanuelesposito.it
Universit`a degli Studi di Milano, Milan, Italy
Istituto Italiano di Tecnologia, Genoa, Italy
Julia Olkhovskaya julia.olkhovskaya@gmail.com
TU Delft, Delft, Netherlands
Abstract
Inthisresearchnote,werevisitthebanditswitheaxpertadviceproblem. Underarestricted
feedback model, we prove a lower bound of order KT lnpN{Kq for the worst-case regret,
where K is the number of actions, N ąK the number of experts, and T the time horizon.
This matches a previously knoawn upper bound of the same order and improves upon the
best available lower bound of KTplnNq{plnKq. For the standard feedback model, we
prove a new instance-based upper bound that depends on the agreement between the
experts and provides a logarithmic improvement compared to prior results.
1. Introduction
The problem of bandits with expert advice provides a simple and general framework for
incorporating contextual information into the non-stochastic multi-armed bandit problem.
In this framework, the learner receives in every round a recommendation, in the form of a
probabilitydistributionovertheactions, fromeachexpertinagivenset. Thissetofexperts
can be seen as a set of strategies each mapping an unobserved context to a (randomized)
action choice. The goal of the learner is to minimize their expected regret with respect to
the best expert in hindsight; that is, the difference between their expected cumulative loss
and that of the best expert. This problem was formulated by Auer, Cesa-Bianchi, Freund,
andSchapire(2002),whoproposedtheEXP4algorithmasasolutionstrategythathassince
become an important baseline or building block for addressing many related problems; for
example, sleeping bandits (Kleinberg, Niculescu-Mizil, & Sharma, 2010), online multi-class
classification (Daniely & Helbertal, 2013), online non-parametric learning (Cesa-Bianchi,
Gaillard, Gentile, & Gerchinovitz, 2017), and non-stationary bandits (Luo, Wei, Agarwal,
?
& Langford, 2018). Auer et al. (2002) proved a bound of order KT lnN on the expected
regret incurred by the EXP4 strategy, where T denotes the number of rounds, K the
number of actions, and N the number of experts. This result is of a worst-case nature, in
4202
nuJ
42
]GL.sc[
1v20861.6042:viXraCesa-Bianchi, Eldowa, Esposito, & Olkhovskaya
that it holds for any sequence of losses assigned to the actions and any sequence of expert
recommendations.
The appealing feature of the bound of Auer et al. (2002) is that it exhibits only a log-
?
arithmic dependence on the number of experts, in addition to the K dependence on
the number of actions known to be unavoidable in the classical bandit problem, where
the learner competes with the best fixed action. While the minimax regret1 in the latter
?
problem has been shown to be of order KT (Audibert & Bubeck, 2009), a similar exact
characterization remains missing for the expert advice problem. Kale (2014) studied a gen-
eralized version of the bandits with expert advice problem—originally proposed by Seldin,
Crammer, and Bartlett (2013)—where the learner is only allowed to query the advice of
aM ď N experts. When M “ N, the results of Kale (2014) imply an upper bound of order
mintK,NuTp1`lnpN{mintK,Nuqq on the minimax regret, improving upon the bound
of Auer et al. (2002). Unlike the latter, the logarithmic factor in Kale’s (2014) bound
?
diminishes as K increases with respect to N, leading to a bound of order NT when
N ď K, which is tight in general as the experts in that case can be made to emulate an
N-armed bandit problem. This improved bound was achieved via the PolyINF algorithm
(Audibert&Bubeck,2009,2010)playedontheexpertsetutilizingtheimportance-weighted
laoss estimators of EXP4. Later, Seldin and Lugosi (2016) proved a lower bound of order
KTplnNq{plnKq for N ě K.
As these upper and lower bounds still do not match, the correct minimax rate remains
unclear. In this work, we take a step towards resolving this issue by showing that the upper
bound is not improvable in general under a restricted feedback model in which the impor-
tance weighted loss estimators used by EXP4 or PolyINF remain implementable. In this
restricted model, without observing the experts’ recommendations, the learner picks an ex-
pert(possiblyatrandom)atthebeginningofeachround,andtheenvironmentsubsequently
samples the action to be executed from the chosen expert’s distribution. Afterwards, the
learner only observes the distributions of the experts that had assigned positive probabil-
ity to the chosen action. Via a reduction from the problem of multi-armed bandits with
feedback graphsa, we use the recent results of Chen, He, and Zhang (2024) to obtain a lower
bound of order KT lnpN{Kq for N ą K.
Departingfromtheworst-caseresultsdiscussedthusfar,afewworkshaveobtainedinstance-
dependent bounds for this problem. The dependence on the instance can be in terms of the
assigned sequence of losses through small loss bounds (see Allen-Zhu, Bubeck, & Li, 2018),
or in terms of the sequence of expert recommendations through bounds that reflect the
similarity between the recommended expert distributions (see McMahan & Streeter, 2009;
Lattimore & Szepesv´ari, 2020, Theorem 18.3; Eldowa, Cesa-Bianchi, Metelli, & Restelli,
2024). Our focus here is on theblatter case, where to the best of our knowledge the state
ř
of the art is a bound of order T C lnN, shown in the recent work of Eldowa et al.
t t
(2024) for the EXP4 algorithm. Here, C is the (chi-squared) capacity of the recommended
t
distributions at round t. This quantity measures the dissimilarity between the experts’
recommendations and satisfies 0 ď C ď mintK,Nu´1. Improving upon this result, we
t
1. The best achievable worst-case regret guarantee.
2Improved Regret Bounds for Bandits with Expert Advice
b
ř ` ˘
illustrate that it is possible to achieve a bound of order T C 1`lnpN{maxtC ,1uq ,
ř t t T
where C “ T C {T is the average capacity. This bound combines the best of the bound
T t t
of Eldowa et al. (2024) (its dependence on the agreement between the experts) and that of
Kale (2014) (its improved log factor), simultaneously outperforming both.
Road map. We formalize the problem setting in the next section. In Section 3, as a pre-
liminary building block, we present Algorithm 1, an instance of the follow-the-regularized-
leader (FTRL) algorithm with the (negative) q-Tsallis entropy as the regularizer. This
algorithm is essentially equivalent to the PolyINF algorithm (see Audibert, Bubeck, & Lu-
gosi, 2011; Abernethy, Lee, & Tewari, 2015), which was used by Kale (2014) to achieve
the best known worst-case upper bound. We then show in Section 4 that combining this
algorithm with a doubling trick allows us to achieve the improved instance-based bound
mentioned above. The lower bound for the restricted feedback setting is presented in Sec-
tion 5. Finally, we provide some concluding remarks in Section 6.
2. Preliminaries
Notation. For a positive integer n, rns denotes the set t1,...,nu. For x,y P R, let
x_y :“ maxtx,yu and x^y :“ mintx,yu. Moreover, we define x :“ x_0.
`
Problem setting. Let V “ rNs be a set of N experts and A “ rKs be a set of K
actions. We consider a sequential decision-making problem where a learner interacts with
an unknown environment for T rounds. The environment is characterized by a fixed and
unknown sequence of loss vectors pℓ q , where ℓ P r0,1sK is the assignment of losses
t tPrTs t
for the actions at round t, and a fixed and unknown sequence of expert advice pθiq ,
t iPV,tPrTs
where θi P ∆ is the distribution over actions recommended by expert i at round t.2 At
t K
the beginning of each round t P rTs, the expert recommendations pθiq are revealed to the
t iPV
learner, who then selects (possibly at random) an action A
t
P A and řsubsequently suffers
and observes the loss ℓ pA q. For an expert i P V, we define y piq :“ θipaqℓ paq as its
t t t aPA t t
loss in round t. The goal is to minimize the expected regret with respect to the best expert
in hindsight:
„ ȷ
ÿT ÿT
R :“ E ℓ pA q ´min y piq,
T t t t
iPV
t“1 t“1
where the expectation is taken with respect to the randomization of the learner.
3. q-FTRL for Bandits with Expert Advice
TheEXP4algorithmcanbeseenasaninstanceoftheFTRLframework(see,e.g.,Orabona,
2023, Chapter 7) where a distribution p over the experts is maintained at each round t and
t
ř
2. Forapositiveintegerd,welet∆ denotetheprobabilitysimplexinRd definedastuPRd: d upjq“
d j“1
1andupjqě0@j Prdsu.
3Cesa-Bianchi, Eldowa, Esposito, & Olkhovskaya
Algorithm 1 q-FTRL for bandits with expert advice
input: q P p0,1q, η ą 0
initialization: p piq Ð 1{N for all i P V
1
for t “ 1,...,T do
receive expert advice pθiq
t iPV
draw expert I „ p and action A „ θIt
t t t t
construct yp P RN where yppiq :“ ř θ tipAtq ℓ pA q for all i P V
t @řt jPDV ptpjqθ tjpAtq t t
let p Ð argmin η t yp ,p `ψ ppq
t`1 pP∆N s“1 s q
end for
updated as follows
B F
ÿt ÿ
p Ð argminη yp ,p ` ppiqlnppiq,
t`1 s
pP∆N
s“1 iPV
where η ą 0 is the learning rate, the second term is the negative Shannon entropy of p, and
yp spiq is an im řportance-weighted estimate of y spiq. The action A
t
is drawn from the mixture
distribution p piqθip¨q. Consider a more general algorithm (outlined in Algorithm 1)
iPV t t
where the negative Shannon entropy is replaced with the negative q-Tsallis entropy, which
for q P p0,1q is given by
˜ ¸
ÿ
1
ψ pxq :“ 1´ xpiqq @x P ∆ .
q N
1´q
iPV
In the limit when q Ñ 1, the negative Shannon entropy is recovered. The following theorem
provides a regret bound for the algorithm. This result is not novel, a similar bound is
implied by Theorem 2 in Kale (2014) for a closely related algorithm in a more general
setting. We provide a concise proof of the result for completeness. As mentioned before,
when N ď K, this bound is trivially tight in general. While when N ą K, we prove an
order-wise matching minimax lower bound in Section 5 under additional restrictions on the
received feedback.
Theorem 3.1. Algorithm 1 run with
¨ ` ˘ ˛ d
q “ 1 ˝ 1` b `ln N{pK ^ ˘Nq ‚ P r1{2,1q and η “ 2qN1´q ,
2 2 Tp1´qqpK ^Nqq
ln N{pK ^Nq `4`2
satisfies b
` ` ˘˘
R ď 2 epK ^NqT 2`ln N{pK ^Nq .
T
ř ř ` ˘
Proof. Let i˚ P argmin T y piq, and note that R “ E T y pI q ´ y pi˚q as
iPV t“1 t T t“1 t t t
Eℓ pA q “ Ey pI q. For round t P rTs, let F :“ σpI ,A ,...,I ,A q denote the σ-algebra
t t t t t 1 1 t t
generated by the random events up to the end of round t, and let řE tr¨s :“ Er¨ | F t´1s with
F being the trivial σ-algebra. For action a P A, let ϕ paq :“ p piqθipaq and note
0 t iPV t t
4Improved Regret Bounds for Bandits with Expert Advice
that conditioned on F , A is distributed according to ϕ . As p is F -measurable, it
t´1 t t t t´1
is then easy to verify that E yp “ y . Hence, Lemma 2 in Eldowa, Esposito, Cesari, and
t t t
Cesa-Bianchi (2023) implies that
« ff
N1´q η
ÿT ÿ
R ď ` E p piq2´q yppiq2 . (1)
T t t
p1´qqη 2q
t“1 iPV
For fixed t P rTs and i P V, we have that
« ff
„ ȷ „ ȷ
“ ‰ θipA q2 θipA q2 ÿ θipaq2 ÿ θipaq2
E yppiq2 “ E t t ℓ pA q2 ď E t t “ E t Ita “ A u “ t
t t t ϕ pA q2 t t t ϕ pA q2 t ϕ paq2 t ϕ paq
t t t t t t
aPA aPA
(2)
where the inequality holds because ℓ pA q P r0,1s and the final equality holds because
t t
E Ita “ A u “ Ppa “ A | F q “ ϕ paq. Hence, it holds that
t t t t´1 t
« ff
ř
ÿ ÿ
p piq2´qθipaq2
E p piq2´q yppiq2 “ iPV t t
t t t
ϕ paq
t
iPV aPA ř
ÿ
p piq2´qθipaq2´q
ď iPV t t maxθipaqq
ϕ tpaq iPV t
aPA `ř ˘
ÿ p piqθipaq 2´q
ď iPV t t maxθipaqq
ϕ tpaq iPV t
aPA ˆ ˙ ˆ ˙
ÿ max θipaq q ÿ q
“ ϕ paq iPV t ď maxθipaq ď pK ^Nqq,
t ϕ tpaq iPV t
aPA aPA
wherethesecondinequalityfollowsfromthesuperadditivityofx2´q forx ě 0andq P p0,1q,
the third inequality follows from the concavity of xq for q P p␣0,1 řq because(of Jensen’s in-
equality,andthelastinequalityholdssincemax θipaq ď min 1, θipaq . Substituting
iPV t iPV t
back into (1) yields that
N1´q η
R ď ` pK ^NqqT .
T
p1´qqη 2q
For brevity, let ξ :“ pK ^Nq. In a similar manner to the proof of Theorem 1 in Eldowa
et al. (2023), substituting the specified values of η and q allows us to conclude the proof:
d
2N1´qξq
R ď T
T
qp1´qq
d
ˆ b ˙ˆ b ˙
1 1
“ 2T exp 1` lnpξNq´ lnpN{ξq2 `4 2` lnpN{ξq2 `4
2 2
d
ˆ ˙ˆ b ˙
1 1
ď 2T exp 1` lnpξNq´ lnpN{ξq 2` lnpN{ξq2 `4
2 2
d c
ˆ b ˙ b
“ 2eξT 2` lnpN{ξq2 `4 ď 2 eξT lnpN{ξq2 `4
a
ď 2 eξT p2`lnpN{ξqq.
5Cesa-Bianchi, Eldowa, Esposito, & Olkhovskaya
4. An Improved Instance-Based Regret Bound
We now obtain a more refined regret bound whose form is analogous to the bound of
Theorem3.1exceptthatitdependsonthesimilaritybetweentheexperts’recommendations
at each round, replacing K ^N with an effective number of experts. Before discussing the
algorithm, we introduce some relevant quantities from Eldowa et al. (2024). For any round
t P rTs and τ P ∆ , define
N
ř
Q pτq :“
ÿ τpiqχ2` θi› ›ř τpjqθj˘
“
ÿ
řiPV
τpiqθ tipaq2
´1,
t t jPV t τpjqθj paq
iPV aPA jPV t
ř ` ˘ ř
where χ2pp}qq :“ qpaq ppaq{qpaq ´ 1 2 “ ppaq2{qpaq ´ 1 is the chi-squared
aPA aPA
divergence between distributions p,q P ∆ . Additionally, let
K
ÿT
1
C :“ sup Q pτq and C :“ C
t t T t
T
τP∆N
t“1
be the chi-squared capacity of the recommended distributions at round t and its average
over the T rounds. As remarked before, C is never larger than pK ^ Nq ´ 1 and can
t
be arbitrarily smaller depending on the agreement between the experts at round t. In
particular, it vanishes when all recommendations are identical.
The idea of Algorithm 2 is to tune Algorithm 1 as done in Theorem 3.1 but with C replac-
T
ing K^N. However, to avoid requiring prior knowledge of C , we rely on a doubling trick
T
to adapt to its value. In a given round t, we maintain a running instance of Algorithm 1
tuned with an estimate for C . Let m be the round when the present execution of Algo-
T t ř
rithm 1 had started. If the current estimate is found to be smaller than 1 t Q pp q,
2T s“mt s s
the algorithm is restarted and the estimate is (at least) doubled. This quantity we test
against is a simple lower bound for C {2 that can be constructed without computing the
T
capacity at any round. As the value of C can be arbitrarily close to zero, the initial
T
guess (which ideally should be a lower bound) is left as a user-specified parameter for the
algorithm, and appears in the first (and more general) bound of Theorem 4.1. The sec-
ond statement of the theorebm shows that choosing lnpe2Nq{T as the initial guess suffices
ř ` ˘
to obtain a bound of order T C 1`lnpN{maxtC ,1uq , up to an additive lnN term.
t t b T
ř
This simultaneously outperforms the T C lnN bound of Eldowa et al. (2024) and the
b t t
` ˘
pK ^NqT 1`lnpN{pK ^Nqq bound of Kale (2014).
The proof combines elements from the proof of Theorem 1 of Eldowa et al. (2024) and the
proof of Theorem 3 of Eldowa et al. (2023), who adopt a similar algorithm to address online
learning with time-varying feedback graphs. Compared to the latter work, we require a
more refined analysis to account for the case when C ă 1. This refinement is achieved in
T
part via the use of Lemma A.1, which also allows adapting the analysis of Eldowa et al.
(2024) to account for the fact that we use the q-Tsallis entropy as a regularizer in place of
the Shannon entropy.
6Improved Regret Bounds for Bandits with Expert Advice
Algorithm 2 q-FTRL with the doubling trick for bandits with expert advice
1: input: J P p0,Ns P T
2: initialization: r 1 Ð log 2J ´1, m 1 Ð 1, p 1piq Ð 1{N for all i P V
3: define: For each integer r P p´8,log Ns,
2
ˆ ˙
1 lnpN{2rq
q :“ 1` a
r
2 lnpN{2rq2`4`2
#d +
´ ¯
η
r
:“ min eTq r pp 1N ´1´ qqr qp´ 2r1 qq
qr
, 1´q r
q
1´eq 2r ´´ qr1
r r
4: for t “ 1,...,T do
5: receive expert advice pθ tiq iPV
6: draw expert I t „ p t and action A t „ θ tIt
7: const řruct yp t P RN where yp tpiq :“ ř jPVθ pti tp pA jqt θq tjpAtqℓ tpA tq for all i P V
8: if T1 t s“mtQ spp sq ą 2rt`1 then
9: p t`1piq ÐP 1{N` f řor all i P V ˘T
10: r t`1 Ð log 2 T1 t s“mtQ spp sq ´1, m t`1 Ð t`1
11: else @ř D
12: p t`1 Ð argmin pP∆N η rt t s“mtyp s,p `ψ qrtppq
13: r t`1 Ð r t, m t`1 Ð m t
14: end if
15: end for
Theorem 4.1. Assuming that T ě lnpe2Nq, Algorithm 2 run with input J P p0,Ns satisfies
d
ˆ ˙ ˆ ˙
` ˘ e2N C
T
R ď 38e C _J T ln `log
T T C _J _1 2 J
T ˜ `` ` ˘ ˘¸
18e 4 JT _C T ^lnpe2Nq ` ˘
` log T ln e2N `1.
5 2 JT
`
In particular, setting J “ lnpe2Nq{T yields that
d
ˆ ˙ ˆ ˙
e2N C T ` ˘
R ď 38e C T ln `log T `46eln e2N `1.
T T C _1 2 lnpe2Nq
T `
P T P T
Proof. For brevity, we define U :“ C _J. Let s :“ log J ´1 and n :“ log U ´1, the
T 2 2
latter of which is the largest value that r can take, since for any round t,
t
ÿt ÿT ÿT
1 1 1
Q pp q ď Q pp q ď C ď 2n`1.
s s s s s
T T T
s“mt s“1 s“1
Without loss of generality, we assume that for any (integer) r P ts,...,nu, there are at least
two rounds in which r “ r, and we use T to refer to the index of the first such round.
t r
7Cesa-Bianchi, Eldowa, Esposito, & Olkhovskaya
Additionally, we defi řne T
n`1
:“ T ` 2. Note that for any r in this range, q
r
P r1{2,1q.
Let i˚ P argmin T y piq. We start by decomposing the regret over the intervals
iPV t“1 t
corresponding to fixed values of r P ts,...,nu and bounding the instantaneous regret at
t
the last step of each but the last interval by 1:
„ ȷ
ÿT ` ˘
R “ E y pI q´y pi˚q
T t t t
t“1
„ ȷ
ÿn Trÿ `1´2` ˘
ď E y pI q´y pi˚q `n´s
t t t
r“s t“Tr
„ ȷ
ÿn Trÿ `1´2` ˘ ` ˘
ď E y pI q´y pi˚q `log U{J `1. (3)
t t t 2
r“s t“Tr
Let e P RN be the indicator vector for i˚ and define y˜ P RN where y˜piq :“ yppiq´ℓ pA q
i˚ t t t t t
for every i P V. Similar to the proof of Theorem 3 in Eldowa et al. (2023), we note that for
each r P ts,...,nu,
« ff « ff
" *
Trÿ `1´2` ˘ ÿT
1
ÿt ` ˘
E y pI q´y pi˚q “ E I r “ r, Q pp q ď 2rt y pI q´y pi˚q
t t t t s s t t t
T
t“Tr «t“1 s“mt ff
" *
ÿT ÿt
p “aq E I r “ r, 1 Q pp q ď 2rt xp ´e ,ypy
t s s t i˚ t
T
«t“1 s“mt ff
" *
ÿT ÿt
p “bq E I r “ r, 1 Q pp q ď 2rt xp ´e ,y˜y
t s s t i˚ t
T
«t“1 s“m fft
Trÿ `1´2
“ E xp ´e ,y˜y
t i˚ t
t“Tr
“ ‰ ř “ ‰
wherepaqfollowssinceE y pI q “ p piqy piq,E yp “ y ,andtheindicatoratroundt
t t t iPV t t t t t
is measurable with respect to F (where F and E are defined in the same way as in
t´1 t´1 t
the proof of Theorem 3.1); and pbq follows since p t,e
i˚
P ∆
N
and yp tpi@q´y˜ tpiq “ ℓ tpDA tq is
identical for all i P V. Similarly to the last argument, the fact that y˜ ´yp ,p´q “ 0
s s
holds for any p,q@Př∆
N
at aDny round s implies that p
t`1
can be equivalently defined
as argmin η t y˜ ,p ` ψ ppq. Hence, using that y˜piq ě ´1, we can invoke
pP∆N rt s“mt s qrt t
Lemma A.1 (with b “ 1 and c “ e) to obtain that
Trÿ `1´2
xp ´e ,y˜y ď
N1´qr ´1
`
eη
r
Trÿ `1´2ÿ
p piq2´qry˜piq2.
t i˚ t t t
p1´q qη 2q
r r r
t“Tr t“Tr iPV
8Improved Regret Bounds for Bandits with Expert Advice
ř
For any round t P rTs and action a P A, recall the definition ϕ paq :“ p piqθipaq.
t iPV t t
Similar to (2) in the proof of Theorem 3.1, we have that
« ` ˘ ff
“ ‰ θipA q´ϕ pA q 2
E y˜piq2 “ E ℓ pA q2 t t t t
t t t t t ϕ pA q2
t t
«` ˘ ff
θipA q´ϕ pA q 2
ď E t t t t
t ϕ pA q2
t t
` ˘ ˆ ˙
“
ÿ θ tipaq´ϕ tpaq 2
“
ÿ
ϕ paq
θ tipaq
´1
2
“ χ2pθi}ϕ q.
ϕ paq t ϕ paq t t
t t
aPA aPA
Hence, for any round t and any r P ts,...,nu, it holds that
« ff
ÿ ÿ
E p piq2´qry˜piq2 ď p piq2´qrχ2pθi}ϕ q
t t t t t t
iPV iPV
ÿ
p piqχ2pθi}ϕ q
“ Q pp q t t t p piq1´qr
t t t
Q pp q
t t
˜iPV ¸
ÿ
p piqχ2pθi}ϕ q
1´qr
ď Q pp q t t t p piq
t t t
Q pp q
t t
˜iPV ¸
ÿ 1´qr
“ Q pp qqr p piq2χ2pθi}ϕ q
t t t t t
˜iPV ¸
ÿ ÿ
θipaq2
ÿ 1´qr
“ Q pp qqr p piq2 t ´ p piq2
t t t t
ϕ paq
t
˜iPV aPA iPV ¸
ř
ÿ
p piq2θipaq2
ÿ 1´qr
“ Q pp qqr řiPV t t ´ p piq2
t t p pjqθj paq t
˜aPA jPV t t iPV
¸
ÿ ÿ ÿ 1´qr
ď Q pp qqr p piqθipaq´ p piq2
t t t t t
˜aPAiPV ¸ iPV
ÿ 1´qr
“ Q pp qqr 1´ p piq2 ď Q pp qqr ,
t t t t t
iPV
where the second inequality follows from the definition of Q tpp tq and the fact that x1´qr is
concave in x ě 0, and the third inequality uses the superadditivity of x2 for non-negative
realnumbersandthenon-negativityofthequantityinbrackets. LetT :“ T ´T ´1,
r:r`1 r`1 r
9Cesa-Bianchi, Eldowa, Esposito, & Olkhovskaya
it then holds that
« ff « ff
" *
Trÿ `1´2ÿ ÿT
1
ÿt ÿ
E p piq2´qry˜piq2 “ E I r “ r, Q pp q ď 2rt p piq2´qry˜piq2
t t t s s t t
T
t“Tr iPV «t“1 ffs“mt iPV
Trÿ `1´2
ď E Q pp qqr
t t
«
t“Tr
˜ ¸ ff
1
Trÿ `1´2 qr
ď E T Q pp q
r:r`1 t t
T
r:r`1
„ ˆ t“T˙r ȷ
T qr
ď E T 2r`1 ď 2T p2rqqr ,
r:r`1
T
r:r`1
where the s řecond inequality uses the concavity of xqr in x ě 0 and the third inequality uses
that p1{Tq Tr`1´2 Q pp q ď 2r`1 since the algorithm is not reset in the interval rT ,T ´
t“Tr t t r r`1
2s. Overall, we have shown that
« ff
E
Trÿ `1´2`
y pI q´y
pi˚q˘
ď
N1´qr ´1
`
eη
r p2rqqr T .
t t t
p1´q qη q
r r r
t“Tr
b ´ ¯
If eTqr pp 1N ´1 q´ rqq pr 2´ r1 qqq
r
ď 1´qr
qr
1´e2qr ´´ qr1 , then substituting the values of η
r
and q
r
gives that
d
N1´qr ´1
`
eη
r p2rqqr T “ 2
epN1´qr ´1qp2rqqr T
p1´q qη q q p1´q q
r r r r r
c d
N1´qr ´1 eN1´qr p2rqqr T
“ 2
N1´qr q rp1´q rq
c
a
? N1´qr ´1
ď 2e 2 2rp2`lnpN2´rqqT
˜dN1´qr
¸
a
? lnN
ď 2e 2 ^1 2rp2`lnpN2´rqqT
lnpN2´rq
b
? ` ˘
“ 2e 2 2rln e2Np2´r ^1q T ,
where the first inequality holds via the same arguments laid in the last passage of the proof
of Theorem 3.1, and the second inequality holds since
N1´qr ´1 ` ` ˘˘
“ 1´exp ´ln N1´qr
N1´qr
ď p1´q qlnN
ˆ r ˙
1 lnpN{2rq
“ 1´ a lnN
2 lnpN{2rq2`4`2
´ a ¯
lnN lnN
“ 2`lnpN{2rq´ lnpN{2rq2`4 ď ,
2lnpN{2rq lnpN{2rq
10Improved Regret Bounds for Bandits with Expert Advice
b
where the inequality follows from the fact that 1´e´x ď x. Otherwise, if qrpN1´qr´1q ą
´ ¯ eTp1´qrqp2rqqr
1´qr
qr
1´eq 2r ´´ qr1 , then η
r
takes the latter value and we obtain that
¨ ˛
2
N1´qr ´1 ` eη r p2rqqr T ď N1´qr ´1 `η N1´qr ´1 ˝ ´1´q r ¯‚
p1´q qη q p1´q qη r p1´q q qr´1
r r r r r r q
r
1´e2´qr
N1´qr ´1
“ 2 ´ ¯
qr´1
q
r
1´e2´qr
` ˘
18 N1´qr ´1
ď
5q p1´q q
r `r ˘
18p2rq´qr N1´qr ´1 p2rqqr
“
5q p1´q q
r r
` ˘
18e
ď p2rq1´qr ln e2Np2´r ^1q
5
18e` ? ˘ ` ˘
ď 1_ 2r ln e2Np2´r ^1q ,
5
where the last inequality holds since q ě 1{2, and the second inequality holds since
r
ˆ ˙
1´eq 2r ´´ qr1 ě 1 2´ ´q qr ´ 21 1 2´ ´q qr 2 “ 2p3 2´ ´q qr q2p1´q rq ě 5 9p1´q rqln` e2Np2´r ^1q˘ ,
r r r
where the first step uses that e´x ď 1´x`x2{2 for x ě 0, and the last step uses again
that q ě 1{2. Hence, the results above yield that
r
« ff
Trÿ `1´2` ˘ " ? b ` ˘
E y pI q´y pi˚q ď max 2e 2 2rT ln e2Np2´r ^1q ,
t t t
t“Tr *
18e` ? ˘ ` ˘
1_ 2r ln e2Np2´r ^1q . (4)
5
LetM :“ lnpe2Nq{T andm :“ log M,andnotethatm ď 0(andM ď 1)bytheassumption
2
that T ě lnpe2Nq. In the case when n ď 0, we have that
« ff
ÿn Trÿ `1´2` ˘
E y pI q´y pi˚q
t t t
r“s t“Tr
18e` ˘ ` ˘ ? ÿn b ` ˘
ď pn^tmuq´s`1 ln e2N `2e 2 2rT ln e2N
5 `
r“n^rms
b
` ˘ ` ˘ ` ˘
18e
ď log 4pU ^Mq{J ln e2N `8e 2UT ln e2N ,
5 2 `
where the second inequality uses that
`? ˘
?
ÿn `? ˘ `? ˘ nÿ´α`? ˘ `? ˘ 2 n´α`1 ´1 2 `? ˘ ?
r α r α n
2 “ 2 2 “ 2 ? ď ? 2 ď 4 U ,
2´1 2´1
r“α r“0
11Cesa-Bianchi, Eldowa, Esposito, & Olkhovskaya
with α :“ n^rms. Otherwise, if n ą 0, then
« ff
ÿn Trÿ `1´2` ˘
E y pI q´y pi˚q
t t t
r“s t“Tr
« ff
18e
` ˘ b ` ˘ ÿn Trÿ `1´2` ˘
ď log p4M{Jq ln e2N `8e 2T ln e2N `E y pI q´y pi˚q
5 2 ` t t t
r“s` t“Tr
` ˘ b ` ˘ ÿn b ` ˘
18e 18e
ď log p4M{Jq ln e2N `8e 2T ln e2N ` 2rln e2N2´r T
5 2 ` 5
b br“0
` ˘ ` ˘ ` ˘
18e
ď log p4M{Jq ln e2N `8e 2T ln e2N `26e UT ln e2N{U
5 2 `
b
` ˘ ` ˘
18e
ď log p4M{Jq ln e2N `38e UT ln e2N{U ,
5 2 `
where the first inequality follows from the analysis of the first case with n “ 0, the second
inequality uses that r ě 0 and the assumption that T ě lnpe2Nq, the third inequality uses
Lemma 4 in Eldowa et al. (2023), and the fourth uses that xlnpe2N{xq is increasing in
r0,eNs and that U ě 2 in this case. The theorem then follows by combining the bounds
provided for the two cases with (3).
5. A Lower Bound for Restricted Advice via Feedback Graphs
In this section, we provide a novel lower bound on the minimax regret for a slightly harder
formulation of the multi-armed bandit problem with expert advice. We consider a setting
where the learner picks an expert I (possibly at random) at the beginning of each round
t
t P rTs without observing any of the experts’ recommendations beforehand. The action
A to be executed is subsequently drawn by the environment from the chosen expert’s
t
distribution, i.e., A „ θIt. Afterwards, the learner observes A , the incurred loss ℓ pA q,
t t t t t
and the advice θi only of experts i P V that have the drawn action A in their support,
t t
i.e., θipA q ą 0. For experts outside this set, the learner can only infer that, by definition,
t t
θipA q “ 0. We will refer to this variation of the problem as the multi-armed bandit
t t
with restricted expert advice (note that this differs from the limited expert advice model
studiedbyKale, 2014). ObservethatAlgoarithm1isstillimplementableinthisscenarioand
guarantees a regret upper bound of order ξT p1`lnpN{ξqq for ξ :“ K^N, as previously
analyzed. Here we show that the regret of Algorithm 1 is the best regret we can hope for,
up to constant factors, for any number K of actions and any number N of experts. While
?
a Ωp NTq regret lower bound in`tahe case N ď K˘is immediate (as mentioned before), the
following th `e aorem provides an Ω
˘
KT lnpN{Kq lower bound when N ą K, improving
upon the Ω KTplnNq{plnKq lower bound of Seldin and Lugosi (2016).
In what follows, we fix N ą K ě 2. We derive the lower bound relying on a reduction from
themulti-armedbanditproblemwithfeedbackgraphs(Mannor&Shamir,2011; Alonetal.,
2013, 2015, 2017). In this variant of the bandit problem, we assume there exists a graph
G “ pV,Eq over a finite set V “ rNs of actions from which the learner selects one action
J P V at each round t P rTs. Then, the learner observes the losses of the neighbours of J
t t
12Improved Regret Bounds for Bandits with Expert Advice
in G. For the construction of the lower bound, it suffices to assume that G is undirected
and contains all self-loops, i.e., pi,iq P E for each i P V. Consequently, the learner always
observes the loss of the selected action and the graph G is strongly observable—see Alon
etal.(2015)foraclassificationoffeedbackgraphs. Weparticularlyfocusonaspecificfamily
of graphs (also considered in the recent work of Chen et al., 2024) where the N vertices
are partitioned into disjoint cliques with self-loops. Precisely, we let M :“ tK{2u ě 1 be
the number of disjoint cliques in G. For any k P rMs, let C be the set of vertices of the
k
k-th clique in G. Since each C
k
is a clique with all Ťself-loops, we have that pi,jq P E if and
only if i,j P C for some k P rMs, and thus E “ pC ˆC q. Additionally, for our
k kPrMs k␣ k (
purposes, we only consider the partition into cliques C “ i P rNs : i ” k mod M of
k
roughly the same size |C | ě tN{Mu ě t2N{Ku ě N{K.
k
Hence, we will focus on the class of instances, denoted by Ξ , of the multi-armed bandit
FG
problem with feedback graphs where the graph assumes the particular structure described
above. Inparticular,anyinstanceI P Ξ isdefinedasatupleI :“ pT,G,Lqcontainingthe
FG
number T of rounds, the feedback graph G “ pV,Eq over V “ rNs composed of the disjoint
cliques C ,...,C as defined above, and the sequence L :“ pℓ q of binary loss functions
1 M t tPrTs
ℓ : V Ñ t0,1u over V. On the other hand, we let Ξ be the class of instances for the
t BEA
multi-armed bandit problem with res`tricted exper˘t advice, with N experts and K actions.
AninstanceI P Ξ isatupleI :“ T,V,A,Θ,L containingthenumberT ofrounds, the
BEA
set V “ rNs of experts, the set A “ rKs of actions, the sequence Θ :“ pθiq of expert
t iPV,tPrTs
advice where θi P ∆ , and the sequence L :“ pℓ q of loss functions ℓ : A Ñ t0,1u over
t K t tPrTs t
A. The sought result is established by showing that the worst-case regret of any algorithm
againstaparticularsubsetofinstancesinΞ isorder-wiseatleastaslargeastheminimax
BEA
regret on Ξ , combined with a lower bound on the latter quantity by Chen et al. (2024).
FG
Theorem 5.1. Let B be any possibly randomized algorithm for the multi-armed bandit
problem with restricted expert advice for any number K ě 2 of actions A “ rKs and any
number N ą K of experts V “ rNs. Then, for a sufficiently large T, there exist a sequence
ℓ ,...,ℓ : A Ñ t0,1u of binary loss functions and a sequence pθiq of expert advice
1 T `a ˘ t iPV,tPrTs
such that the expected regret of B is Ω KT lnpN{Kq .
Proof. We first describe a reduction from the multi-armed bandit problem with feedback
graphs to the multi-armed bandit problem with restricted expert advice. We accomplish
this by providing a mapping ρ: Ξ Ñ Ξ from the considered instance class Ξ of the
FG BEA FG
former problem to the instance class Ξ of the latter.
BEA
Consider any instance I :“ pT,G,Lq P Ξ and recall that G “ pV,Eq is a union of
FG
M “ tK{2u disjoint cliques C ,...,C over V “ rNs. The mapped instance ρpIq :“
1 M
pT,V,A,Θ,L1q P Ξ is defined over the same number of rounds T and an experts set
BEA
corresponding to the actions V in the original instance I, whose sequence of recommen-
dations is provided by Θ “ pθiq . We first observe that the cardinality of the new
t iPV,tPrTs
action set A “ rKs does relate to the number of cliques M. In particular, considering the
partition of experts given by the cliques in G, we also partition the actions (in the expert
advice instance ρpIq) by associating 2 actions to each clique. Precisely, for any k P rMs,
we associate actions A :“ t2k ´1,2ku to C . If K is even, this partitions the entire set
k k
13Cesa-Bianchi, Eldowa, Esposito, & Olkhovskaya
of actions A, while it leaves out action K otherwise. We can ignore the latter case and
assume K is even without loss of generality, since we can otherwise leave action K outside
of the support of any expert advice θi P ∆ in the following construction (thus becoming
t K
a spurious action).
Second, wefocusontheconstructionofthelosssequenceL1 :“ pℓ1,...,ℓ1 q. Foranyt P rTs,
1 T
we define ℓ1 P t0,1uA as
t
ℓ1p2k´1q :“ 0 and ℓ1p2kq :“ 1 @k P rMs¨
t t
Finally, we define the sequence of expert advice pθiq depending on the sequence of
t iPV,tPrTs
losses L of the starting instance I. For any t P rTs, any k P rMs, and any i P C , we define
k
θi P ∆ as
t K
#
δ if ℓ piq “ 0
θi :“ 2k´1 t ,
t
δ if ℓ piq “ 1
2k t
where δ
j
P ∆
K
is th ře Dirac delta at j P A. This ensures that the loss of expert i at round
t, given by y piq “ θipaqℓ1paq coincides with ℓ piq, the loss of action i in the original
t aPA t t t
feedback graphs instance at the same round. Moreover, the knowledge of ℓ piq suffices to
t
infer θi.
t
At this point, given our instance mapping ρ and our algorithm B, we design an algorithm
B for the class Ξ . Consider any instance I P Ξ . Over the interaction period, the
ρ FG FG
algorithm B , without requiring prior knowledge of I, maintains a running realization of
ρ
B on instance ρpIq. At any round t P rTs, let I be the expert selected by algorithm B in
t
ρpIq, and let k P rMs be the index of the clique I belongs to, i.e., I P C . Algorithm
t t t kt
B , interacting with the instance I, executes action J “ I provided by B and observes
ρ t t
the losses pℓ piqq . Then, thanks to the design of the mapping ρ, B can construct
t iPC
kt
ρ
and provide B the feedback it requires and which complies with instance ρpIq. Namely, it
determines that A “ 2k ´1 if ℓ pJ q “ 0 or else that A “ 2k , then passes A , its loss
t t t t t t t
ℓ1pA q (trivially determined), and the restricted advice pθiq to B. The last of which is a
t t t iPC kt
super-set of the recommended distributions having positive support on A since A is never
t t
picked by experts outside C by construction.
kt
Now, let
« ff « ff
ÿT ÿT ÿ ÿT ÿT ÿ
RBpI1q :“ E ℓ1pA q ´min θipaqℓ1paq “ E y pI q ´min θipaqℓ1paq
t t t t t t t t
iPV iPV
t“1 t“1aPA t“1 t“1aPA
` ˘
betheexpectedregretofalgorithmBonsomeinstanceI1 “ T,V,A,pθiq ,pℓ1q P
t iPV,tPrTs t tPrTs
Ξ BEA. Similarly, let « ff
ÿT ÿT
RBρpIq :“ E ℓ pJ q ´min ℓ piq
t t t
iPV
t“1 t“1
` ˘
be the expected regret of algorithm B on some instance I “ T,G,pℓ q P Ξ . Since
ρ t tPrTs FG
J “ I , we have that y pI q “ ℓ pJ q via the properties of ρ laid out before. Hence, we can
t t t t t t
14Improved Regret Bounds for Bandits with Expert Advice
␣
conc(lude that RBpρpIqq “ RBρpIq for any instance I P Ξ FG. Define ρpΞ FGq :“ ρpIq : I P
Ξ Ď Ξ as the subclass of instances in Ξ obtained from Ξ via ρ. Then, it holds
FG BEA BEA FG
that
sup RBpIq ě sup RBpIq “ sup RBpρpIqq “ sup RBρpIq .
IPΞBEA IPρpΞFGq IPΞFG IPΞFG
On the other hand, Lemma E.1 in Chen et al. (2024) implies that
˜ ¸
d
ÿ `a ˘
sup
RBρpIq
“ Ω T lnp1`|C |q “ Ω KT lnpN{Kq
T k
IPΞFG
kPrMs
ř
for sufficiently large T since lnp1 ` |C |q ě M lnpN{Mq ě Klnp2N{Kq{4, thus
kPrMs k
concluding the proof.
6. Conclusion
As the lower bound of Theorem 5.1 was proved for a harder formulation of the problem,
it remains to be shown whether the same impossibility result holds for the more standard
setup. We conjecture it should be possible to prove such a lower bound. If it indeed holds,
this would imply that the minimax regret in the two variants is of the same order; that
is, as far as we are only concerned with the worst-case regret, the standard feedback setup
would be shown to be essentially as hard as the restricted one.
Acknowledgments
NCB,EE,andKEacknowledgethefinancialsupportfromtheMURPRINgrant2022EKNE5K
(Learning in Markets and Society), the FAIR (Future Artificial Intelligence Research)
project, and the EU Horizon RIA under grant agreement 101120237, project ELIAS (Eu-
ropean Lighthouse of AI for Sustainability).
Appendix A. Auxiliary Results
Lemma A.1. Let q P p0,1q, b ą 0, c ą 1, and py qT be a sequence of non-negative loss
t t“1
vectors in RN satisfying y piq ě ´b for all t P rTs and i P rNs. Let pp qT`1 be the predictions
t t t“1
of FTRL with decision set ∆ and the q-Tsallis regularizer ψ over this sequence of losses;
N q
that is, p “ argmin ψ ppq, and for t P rTs,
1 pP∆N q
ÿt @ D
p “ argminη y ,p `ψ ppq,
t`1 s q
pP∆N
s“1
´ ¯
assuming the learning rate η satisfies 0 ă η ď p1´q
qqb
1´cq 2´ ´1 q . Then for any u P ∆ N,
ÿT
N1´q ´1 ηc
ÿT ÿN
xp ´u,y y ď ` p piq2´q y piq2.
t t t t
p1´qqη 2q
t“1 t“1i“1
15Cesa-Bianchi, Eldowa, Esposito, & Olkhovskaya
Proof. Let p1 :“ argmin xp,y y ` D pp,p q, where D p¨,¨q denotes the Bregman
t`1 pPRN
ě0
t ψq t ψq
divergence based on ψ . Via Lemma 7.14 in (Orabona, 2023) we have that
q
ÿT ÿT ÿN
ψ puq´ψ pp q η
xp ´u,y y ď q q 1 ` z piq2´q y piq2
t t t t
η 2q
t“1 t“1i“1
N1´q ´1 η
ÿT ÿN
ď ` z piq2´q y piq2,
t t
p1´qqη 2q
t“1i“1
where z lies on the line segment between p and p1 . A simple derivation shows that
t t t`1
˜ ¸
1
1 1´q
p1 piq “ p piq ,
t`1 t 1`η1´qy piqp piq1´q
q t t
for each i P rNs. On the other hand, it holds that
η1´q y tpiqp tpiq1´q ě ´η1´q bp tpiq1´q ě ´η1´q b ě cq 2´ ´1 q ´1,
q q q
where the first inequality uses that y tpiq ě´´b (and¯that p tpiq,η ą 0), the second uses that
p tpiq ď 1,andthethirdusesthatη ď p1´q
qqb
1´cq 2´ ´1 q . Thisentailsthatp1 t`1piq ď c2´1 qp tpiq,
1
which implies that z tpiq ď c2´qp tpiq concluding the proof.
References
Abernethy, J. D., Lee, C., & Tewari, A. (2015). Fighting bandits with a new kind of
smoothness. InCortes,C.,Lawrence,N.,Lee,D.,Sugiyama,M.,&Garnett,R.(Eds.),
Advances in Neural Information Processing Systems, Vol. 28. Curran Associates, Inc.
Allen-Zhu,Z.,Bubeck,S.,&Li,Y.(2018). Maketheminoritygreatagain:First-orderregret
bound for contextual bandits. In Proceedings of the 35th International Conference on
Machine Learning, Vol. 80 of Proceedings of Machine Learning Research, pp. 186–194.
PMLR.
Alon, N., Cesa-Bianchi, N., Gentile, C., Mannor, S., Mansour, Y., & Shamir, O. (2017).
Nonstochastic multi-armed bandits with graph-structured feedback. SIAM Journal
on Computing, 46(6), 1785–1826.
Alon, N., Cesa-Bianchi, N., Gentile, C., & Mansour, Y. (2013). From bandits to experts:
A tale of domination and independence. Advances in Neural Information Processing
Systems, 26.
Alon, N., Cesa-Bianchi, N., Dekel, O., & Koren, T. (2015). Online learning with feedback
graphs: Beyond bandits. In Proceedings of The 28th Conference on Learning Theory,
Vol. 40 of Proceedings of Machine Learning Research, pp. 23–35. PMLR.
16Improved Regret Bounds for Bandits with Expert Advice
Audibert, J., & Bubeck, S. (2009). Minimax policies for adversarial and stochastic bandits.
In Proceedings of the 22nd Conference on Learning Theory.
Audibert, J.-Y., & Bubeck, S. (2010). Regret bounds and minimax policies under partial
monitoring. Journal of Machine Learning Research, 11(94), 2785–2836.
Audibert, J.-Y., Bubeck, S., & Lugosi, G. (2011). Minimax policies for combinatorial pre-
diction games. In Kakade, S. M., & von Luxburg, U. (Eds.), Proceedings of the 24th
Annual Conference on Learning Theory, Vol. 19 of Proceedings of Machine Learning
Research, pp. 107–132. PMLR.
Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. E. (2002). The nonstochastic mul-
tiarmed bandit problem. SIAM Journal on Computing, 32(1), 48–77.
Cesa-Bianchi, N., Gaillard, P., Gentile, C., & Gerchinovitz, S. (2017). Algorithmic chaining
and the role of partial feedback in online nonparametric learning. In Proceedings of
the 2017 Conference on Learning Theory, Vol. 65 of Proceedings of Machine Learning
Research, pp. 465–481. PMLR.
Chen, H., He, Y., & Zhang, C. (2024). On interpolating experts and multi-armed bandits.
In Forty-first International Conference on Machine Learning.
Daniely, A., & Helbertal, T. (2013). The price of bandit information in multiclass online
classification. In Proceedings of the 26th Annual Conference on Learning Theory,
Vol. 30 of Proceedings of Machine Learning Research, pp. 93–104. PMLR.
Eldowa, K., Cesa-Bianchi, N., Metelli, A. M., & Restelli, M. (2024). Information capacity
regret bounds for bandits with mediator feedback. arXiv preprint, arXiv:2402.10282.
Eldowa, K., Esposito, E., Cesari, T., & Cesa-Bianchi, N. (2023). On the minimax regret for
online learning with feedback graphs. In Advances in Neural Information Processing
Systems, Vol. 36, pp. 46122–46133. Curran Associates, Inc.
Kale, S. (2014). Multiarmed bandits with limited expert advice. In Proceedings of The 27th
Conference on Learning Theory,Vol.35ofProceedings of Machine Learning Research,
pp. 107–122. PMLR.
Kleinberg,R.,Niculescu-Mizil,A.,&Sharma,Y.(2010). Regretboundsforsleepingexperts
and bandits. Machine learning, 80(2), 245–272.
Lattimore, T., & Szepesv´ari, C. (2020). Bandit algorithms. Cambridge University Press.
Luo, H., Wei, C.-Y., Agarwal, A., & Langford, J. (2018). Efficient contextual bandits in
non-stationary worlds. In Proceedings of the 31st Conference On Learning Theory,
Vol. 75 of Proceedings of Machine Learning Research, pp. 1739–1776. PMLR.
Mannor,S.,&Shamir,O.(2011).Frombanditstoexperts:Onthevalueofside-observations.
In Advances in Neural Information Processing Systems, Vol. 24. Curran Associates,
Inc.
McMahan, H. B., & Streeter, M. J. (2009). Tighter bounds for multi-armed bandits with
expert advice. In Proceedings of the 22nd Conference on Learning Theory.
17Cesa-Bianchi, Eldowa, Esposito, & Olkhovskaya
Orabona, F. (2023). A modern introduction to online learning. arXiv preprint,
arXiv:1912.13213.
Seldin, Y., Crammer, K., & Bartlett, P. (2013). Open problem: Adversarial multiarmed
banditswithlimitedadvice. InProceedingsofthe26thAnnualConferenceonLearning
Theory, Vol. 30 of Proceedings of Machine Learning Research, pp. 1067–1072. PMLR.
Seldin, Y., & Lugosi, G. (2016). A lower bound for multi-armed bandits with expert advice.
In The 13th European Workshop on Reinforcement Learning (EWRL).
18