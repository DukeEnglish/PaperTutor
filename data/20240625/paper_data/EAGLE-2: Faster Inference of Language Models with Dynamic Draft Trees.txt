EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees
YuhuiLi♠ FangyunWei‡ ChaoZhang♠ HongyangZhang♣†
♠PekingUniversity ‡MicrosoftResearch ♣UniversityofWaterloo †VectorInstitute
hongyang.zhang@uwaterloo.ca
https://github.com/SafeAILab/EAGLE
Abstract
EAGLE-2 EAGLE Speculative sampling
4 3.80x 3.92x
InferencewithmodernLargeLanguageModels 3.05x 3.19x
2.68x
(LLMs) is expensive and time-consuming, and 2.13x 2.32x 2.22x
speculative sampling has proven to be an effec- 2 1.50x 1.62x N/A N/A
tivesolution. Mostspeculativesamplingmethods
0
s au ssc uh ma is nE gA thG aL ttE heus ae cca es pt ta at nic ced rr aa tf ett or fee d, raim ftp toli kc eit nly
s
Vicuna 7B Vicuna 13B LLaMA2-Chat 7B LLaMA2-Chat 13B
dependsonlyontheirposition. Interestingly,we Models
foundthattheacceptancerateofdrafttokensis Figure1: Speedupratiosofdifferentmethodsattempera-
also context-dependent. In this paper, building ture=1. For speculative sampling, the Vicuna series uses
uponEAGLE,weproposeEAGLE-2,whichintro- Vicuna-68M as the draft model. LLaMA2-Chat lacks a
ducesanewtechniqueofcontext-awaredynamic suitabledraftmodel,andismarkedasN/A.Methodslike
drafttreeintodraftingmodeling. Thisimprove- Medusarelaxacceptanceconditionsundernon-greedyset-
ment leverages the fact that the draft model of tings,whichdonotguaranteelosslessacceleration. Inthis
EAGLEiswell-calibrated: theconfidencescores paper, weonlycomparewithspeculativesamplingbased
from the draft model approximate acceptance methodsensuringtheoutputtextdistributionremainscon-
rateswithsmallerrors. Weconductedextensive stant. InTable1,wepresentcomparisonswithadditional
evaluationsonthreeseriesofLLMsandsixtasks, methods,butthisfigureonlyshowcasesasubset,including
with EAGLE-2 achieving speedup ratios 3.05x- thefastestamongthesemethods,EAGLE.
4.26x,whichis20%-40%fasterthanEAGLE-1.
EAGLE-2alsoensuresthatthedistributionofthe
generated text remains unchanged, making it a
losslessaccelerationalgorithm. Standardspeculativesampling(Leviathanetal.,2023;Chen
etal.,2023a)usesachain-structureddraft. Toimproveac-
ceptancelength, recentworkinspeculativesamplinghas
1.Introduction employedtree-structureddrafts. Sequoia(Chenetal.,2024)
explicitlyassumesthattheacceptancerateofadrafttoken
ModernLargeLanguageModels(LLMs)(OpenAI,2023; dependsonlyonitspositioninthetree. EAGLE(Lietal.,
Touvron et al., 2023) exhibit impressive capabilities and 2024b)andMedusa(Caietal.,2024)usethesamestatic
arewidelyappliedacrossvariousdomains. However,their drafttreestructureinallcontexts:atthei-thstepofthedraft
parametersizeshavegrownsubstantially,evenexceeding phase,kcandidatesareadded,withkbeingfixed. Thisim-
hundredsofbillions.Duringautoregressivegeneration,each plicitlyassumestheaforementionedhypothesis. However,
tokengenerationrequiresaccessingallmodelparameters. this assumption appears to contradict the insight of spec-
Inasingledialogue,hundredstothousandsoftokensmight ulativesamplingthatsometokensaresimplerandcanbe
begenerated,makingLLMinferenceslowandexpensive. predictedbysmallermodels. Ourexperiments(seeSection
Speculativesampling(Leviathanetal.,2023;Chenetal., 3.1) reveal that the acceptance rate of draft tokens is not
2023a)methodsaimtoaddressthisissuebyrapidlygenerat- onlyposition-dependentbutalsohighlycontext-dependent.
ingdrafttokensandthenverifyingtheminparallel. These Therefore, the static structure of draft trees has inherent
methodsgeneratemultipletokensinasingleforwardpass, limitations. Dynamicallyadjustingthedrafttreestructure
significantlyreducinginferencelatency. based on the acceptance rates of draft tokens in different
1
4202
nuJ
42
]LC.sc[
1v85861.6042:viXra
pudeepSEAGLE-2:FasterInferenceofLanguageModelswithDynamicDraftTrees
EAGLE-2 EAGLE Medusa Lookahead Speculative sampling
4.5 4.26x 4.21x
4.0
3.5 3.62x 3.43x 3.51x 3.46x 3.29x
3.0 2.90x 3.07x 2.78x 3.03x 3.01x 2.72x 2.83x
2.5
2.07x
2.0 1.91x1.82x 1.93x
1.5 1.61x 1.58x 1.52x1.45x 1.50x 1.43x1.41x
N/A N/A N/A
1.0
0.5
0.0
Vicuna
7B
Vicuna
13B
LLaMA2-Chat
7B
LLaMA2-Chat
13B
LLaMA2-Chat
70B
LLaMA3-Instruct
8B
LLaMA3-Instruct
70B
Models
Figure2: Speedupratiosofdifferentmethodsattemperature=0. Forspeculativesampling,theVicunaseriesusesVicuna-
68Masthedraftmodel. LLaMA2-Chat7B,13B,andLLaMA3-Instruct8Blacksuitabledraftmodelsandaremarkedas
N/A.LLaMA2-Chat70BandLLaMA3-Instruct70BuseLLaMA2-Chat7BandLLaMA3-Instruct8Basdraftmodels,
respectively. InTable1,wepresentcomparisonswithadditionalmethods,butthisfigureonlyshowcasesasubset,including
thefastestamongthesemethods,EAGLE.
contextscanyieldbetterresults. turnconversationdatasetthatcloselyresemblesreal-world
scenariosformodelslikeChatGPTandisfrequentlyused
However,obtainingtheacceptancerateofdrafttokensre-
toevaluatestate-of-the-artopen-sourceandclosed-source
quires the forward results from the original LLM, which
models. On the MT-bench dataset, EAGLE-2 is approxi-
conflictswiththegoalofspeculativesamplingtoreducethe
mately 2x faster than Medusa and about 2.3x faster than
numberofforwardsfortheoriginalLLM.Fortunately,we
Lookahead,whileensuringtheoutputdistributionremains
findthatEAGLEiswell-calibrated: theconfidencescore
unchanged.
(probability)ofthedraftmodelisagoodapproximationof
theacceptancerateofdrafttokens(seeSection3.2). This Besidesperformance,EAGLE-2offersthefollowingadvan-
makesitfeasibletouseacontext-dependentdynamicdraft tages:
treestructure.
• Out-of-the-box usability. Comparing to EAGLE,
We propose EAGLE-2, which leverages the confidence
EAGLE-2doesnotrequiretraininganyextramodels.
scoresfromthedraftmodeltoapproximateacceptancerates.
Itdoesnottrainaseparatemodeltopredictthedraft
Basedonthis,itdynamicallyadjuststhedrafttreestructure,
treestructure. Instead,itadjuststhedrafttreestructure
increasingthenumberofacceptedtokens. Weconducted
basedontheconfidencescoresfromthedraftmodel,
comprehensiveandextensivetestsonsixtasks: multi-turn
whichisessentialforspeculativesampling. Therefore,
conversation,codegeneration,mathematicalreasoning,in-
EAGLE-2requiresnoadditionaltraining.
structionfollowing,summarization,andquestionanswering.
ThedatasetsusedwereMT-bench(Zhengetal.,2023),Hu- • Reliability. EAGLE-2doesnotfine-tuneorupdatethe
manEval(Chenetal.,2021),GSM8K(Cobbeetal.,2021), parametersoftheoriginalLLM,nordoesitrelaxac-
Alpaca(Taorietal.,2023),CNN/DailyMail(Nallapatietal., ceptanceconditions. Thisensuresthatthedistribution
2016), andNaturalQuestions(Kwiatkowskietal.,2019). ofthegeneratedtextremainsexactlythesamewith
Our comparisons included six advanced speculative sam- thatoftheoriginalLLM,provably.
plingmethods: standardspeculativesampling(Leviathan
et al., 2023; Chen et al., 2023a; Joao Gante, 2023), PLD
(Saxena,2023),Medusa(Caietal.,2024),Lookahead(Fu 2.Preliminaries
etal.,2023),Hydra(Ankneretal.,2024),andEAGLE(Li
2.1.SpeculativeSampling
etal.,2024b). Weconductedexperimentsonthreeseries
ofLLMs: Vicuna,LLaMA2-Chat,andLLaMA3-Instruct.
The core idea of speculative sampling (Leviathan et al.,
Inallexperiments,EAGLE-2demonstratedthebestperfor-
2023;Chenetal.,2023a;Sunetal.,2024c;b)istofirstdraft
mance, achieving a speedup of 2.5x-5x. Figures 1 and 2
andthenverify: quicklygenerateapotentiallycorrectdraft
showthespeedupratiosofEAGLE-2andotherspeculative
andthencheckwhichtokensinthedraftcanbeaccepted.
sampling methods on MT-bench. MT-bench is a multi-
We use t to denote the i-th token and T to represent
i a:b
2
pudeepSEAGLE-2:FasterInferenceofLanguageModelswithDynamicDraftTrees
Speculative SSapmecpulilnagtive Sampling EAGLE EAGLE Speculative SamSppleincgulative SamplingEAGLE EAGLE
𝑡2 𝑡3 To𝑡2ken D r𝑡A a3u ftt o Mre og dr eeT lsosikveen D rA au ftt o M𝑡r 4e og dr ee lssive 𝑓𝑡 12𝑡4 𝑓𝑡3 2 Fea 𝑓𝑡t 12u Dre r aA 𝑓𝑡 f3 2u t t Mor oe dg ere lFsesaivteu Dre r aA fu t t M𝑓o 3r oe dg ere ls 𝑡s 4ive 𝑓3 𝑡4 𝑡4 Original 𝑡4 ✓ Origina𝑡4 l ✓ Original 𝑡4 ✓ Original ✓
𝑡2 𝑡3 𝑡4 𝑡2 To 𝑡3ken D rA 𝑡 a4u ftt o Mre og dr eeT lsosikveen D rA au ftt o M𝑡r 5e og dr ee lssive 𝑓𝑡 12 𝑓𝑡3 2𝑡5 𝑓𝑡4
3
𝑓𝑡 12Fea 𝑓𝑡t3 2u Dre r aA 𝑓𝑡 f4 3u t t Mor oe dg ere lFsesaivteu Dre r aA fu t t M𝑓o 4r oe dg ere ls 𝑡s 5ive 𝑓4 𝑡5 𝑡5 LLM 𝑡5 × LL 𝑡5M 𝑡6 ×LLM 𝑡5 𝑡× 6 ✓LLM × ✓
(a)Draftingstage. (b)Verificationstage.
Figure3: ComparisonofstandardspeculativesamplingandEAGLE.Forsimplicity,EAGLE’stree-structureddraftisshown
onlyintheverificationstage,whiletheillustrationofthedraftingstageusesachain-structureddraft. Here,t denotesthe
i
i-thtokenembedding,andf denotesthei-thfeaturevectorinthesecond-to-top-layerofLLMbeforeLMhead.
i
thetokensequencet ,t ,··· ,t . Speculativesampling EAGLE EAGLE-2
a a+1 b
alternatesbetweendraftingandverificationstages.
ConsideraprefixT ,inthedraftingstage,speculativesam-
1:j
10+2 10+2= 10+2 10+2=
plinginvokesadraftmodel(asmallerLLMthanoriginal
LLM) to autoregressively generate a draft Tˆ with
j+1:j+k
T 1:j as the prefix, while also recording the probability pˆ 1
foreachtoken. Intheverificationstage,speculativesam- = + 1 3 = +
plingcallstheoriginalLLMtocheckthedraftTˆ and
j+1:j+k 2
recorditsprobabilityp. Then,speculativesamplingdeter-
minestheacceptanceofdrafttokenssequentiallyfromfront
Figure4: DifferencesbetweenEAGLEandEAGLE-2. EA-
toback. Fortokentˆ ,theprobabilityofitbeingaccepted
j+i GLE always uses a fixed draft shape. When the query is
ismin(1,p (tˆ )/pˆ (tˆ )). Ifthetokenisaccepted,
j+i j+i j+i j+i “10+2=”,thenexttokenisverylikelytobecorrectlypre-
itproceedstocheckthenextone. Otherwise,itsamplesa
dicted as “1”. However, with a static draft tree, EAGLE
tokenfromthedistributionnorm(max(0,p −pˆ ))to
j+i j+i wouldstilladdtwocandidates,eventhoughtheprobability
replacetˆ anddiscardstheremainingtokensinthedraft.
j+i oftheothercandidate“3”beingcorrectisverylow.EAGLE-
AppendixA.1of(Leviathanetal.,2023)provesthatspecu-
2,ontheotherhand,adjuststheshapeofdrafttreebased
lativesamplingisconsistentwiththedistributionofvanilla
onthecontext. Whenthequeryis“10+2”,thenexttoken
autoregressivedecoding.BothEAGLEandEAGLE-2apply
is difficult to predict, so EAGLE-2 adds two candidates.
thisframework.
Forthesimplerquery“10+2=”,EAGLE-2addsonlyone
candidate“1”.
2.2.EAGLE
EAGLE (Li et al., 2024b) is an improvement over specu-
DifferencesbetweenEAGLEandEAGLE-2. Theshape
lativesampling. Atthesubmissionofthiswork,EAGLE
of EAGLE’s draft tree is fixed, with the drafting phase
ranksfirstintheSpec-Bench(Xiaetal.,2024),acomprehen-
filling in the corresponding positions. EAGLE-2 aims to
sivebenchmarkdesignedforassessingspeculativedecoding
improvethisbyintroducingadynamicallyadjustabledraft
methodsacrossdiversescenarios.
tree. Figure4illustratesthedifferencebetweenEAGLEand
Drafting Stage. Unlike standard speculative sampling, EAGLE-2withasimpleexample.
whichautoregressivelypredictstokensequences,EAGLE
performsautoregressionatthemorestructuredfeature(be-
3.Observations
foreLMhead)levelandthenusestheLMHeadoforiginal
LLMtoobtainthedrafttokens. Thesamplingprocessintro- 3.1.Context-DependentAcceptanceRates
ducesuncertaintyinthefeaturesequence. Toaddressthis,
EAGLEalsoinputsatokensequenceadvancedbyonetime First, we evaluate the necessity of using a dynamic draft
stepintothedraftmodel,asshowninFigure3a. tree. Thisdependsonwhethertheacceptanceratesofdraft
tokensaresolelyrelatedtotheirpositions. Wetestedthe
VerificationStage. Instandardspeculativesampling,the
acceptanceratesoftokensatdifferentpositionsinthedraft
draftischain-structured,requiringthediscardingofallsub-
treeontheAlpacadatasetandVicuna7B.Theresultsare
sequent tokens if a draft token is rejected. EAGLE uses
shown in Figure 5. Overall, the acceptance rate of draft
atree-structureddraft,allowingalternativebranchestobe
tokensisposition-dependent,withthehighestacceptance
attemptedifadrafttokenisrejected. Figure3billustrates
rate at position P1 and the lowest at position P6. Draft
thedifferencesbetweenthetwo.
tokensintheupperleftsideofthedrafttree(suchasposition
P1)havehigheracceptancerates,whilethoseinthelower
3EAGLE-2:FasterInferenceofLanguageModelswithDynamicDraftTrees
1.0
1.0
0.8
Query 0.6 0.8
0.4
0.6
P1 P2 0.2
0.0
0.4
1 2 3 4 5 6
Position
P3 P4 P5 P6 (b)Acceptanceratesoftokens
0.2
atdifferentpositions,witheach
(a)Drafttreestructure. pointrepresentingaquery.
0.0
0.0 0.2 0.4 0.6 0.8 1.0
Figure5: Acceptanceratesofdrafttokensatdifferentposi- Confidence
tions.Intheleftfigure,P1-P6indicatepositionsinthetoken
Figure6: Averageacceptanceratesfordifferentconfidence
tree,correspondingtopositions1-6onthehorizontalaxis
score intervals of the draft model. The red dashed line
intherightfigure. Therightfigureshowstheacceptance
connects (0,0) and (1,1) to aid in visual assessment. The
ratesofdrafttokensatpositionsP1-P6.
originalLLMisVicuna7B.
rightside(suchaspositionP6)haveloweracceptancerates. phase,weinputthemostpromisingnodesfromthelatest
This supports the rationale for having more nodes in the layerofthedrafttreeintothedraftmodeltoformthenext
upperleftandfewerinthelowerrightinstaticdrafttrees layer. Duringthererankingphase,weselectthetokenswith
usedbymethodslikeEAGLEandMedusa. However,we higher acceptance probabilities to form the input for the
alsoobservedsignificantvarianceinacceptanceratesatthe originalLLMduringtheverificationphase.
sameposition,indicatingthattheprobabilityofadrafttoken
Inthedrafttree,anoderepresentsatoken. Inthefollowing
beingaccepteddependsnotonlyonitspositionbutalsoon
text,weuse“node”and“token”interchangeably.
the context. This suggests that a context-aware dynamic
drafttreehasgreaterpotentialthanastaticdrafttree.
4.1.ExpansionPhase
3.2.Well-CalibratedDraftModel
Thankstotreeattention,thedraftmodelcansimultaneously
input all tokens from the current layer and compute the
Toapplyadynamicdrafttree,weneedalow-costmethod
probabilities for the next tokens in a single forward pass,
toestimatetheacceptanceratesofdrafttokenswithoutin-
therebyexpandingalltokensinthecurrentlayer. However,
vokingtheoriginalLLM.Weconductedexperimentsonthe
inputtingtoomanytokensatoncecanslowdownthedraft
Alpacadatasettoexploretherelationshipbetweenthedraft
model’s forward pass, and the number of tokens in each
model’sconfidencescore(theoutputprobabilityofLLM
layerofthedrafttreegrowsexponentially. Therefore,we
w.r.t. each token) and the acceptance rate. As shown in
needtoselectivelyexpandthedrafttree.
Figure6,thereisastrongpositivecorrelationbetweenthe
draftmodel’sconfidencescoreandtheacceptancerateof Wechoosethetop-ktokenswiththehighestglobalaccep-
thetoken. Drafttokenswithconfidencescorebelow0.05 tanceprobabilitiesfromthecurrentlayerforexpansion. In
haveanacceptancerateofapproximately0.04,whilethose speculative sampling, rejecting a draft token leads to dis-
withconfidencescoreabove0.95haveanacceptancerateof cardingallsubsequenttokens;atokenisultimatelyaccepted
about0.98. Therefore,wecanusethedraftmodel’sconfi- onlyifallitsprefixesareaccepted. Theglobalacceptance
dencescoretoestimateacceptancerateswithoutadditional rateofatokent istheproductoftheacceptanceratesofall
i
overhead,enablingdynamicadjustmentstothedrafttree. tokensonthepathfromtherootnodetot . Wedefineitas
i
thevalueV :
i
4.Context-AwareDynamicDraftTree
(cid:89) (cid:89)
V = p ≈ c ,
i j j
Buildingontheaforementionedobservations,weintroduce
tj∈Path(root,ti) tj∈Path(root,ti)
EAGLE-2,anaccelerationalgorithmforLLMinferencethat
dynamicallyadjuststhedrafttree. EAGLE-2doesnotalter wherePath(root,t )representsthepathfromtherootnode
i
the training and inference of the draft model, nor does it tothenodet inthedrafttree,p representstheacceptance
i j
affecttheverificationstage. Itsimprovementsfocusontwo rateofthenodet ,andc representstheconfidencescore
j j
aspects: howtoexpandthedrafttree(Section4.1)andhow oft fromthedraftmodel. ExperimentsinSection3.2show
j
torerankdrafttokens(Section4.2). Duringtheexpansion thatconfidencescoreisstronglypositivelycorrelatedwith
4
etaR
tpeccA
etaR
tpeccAEAGLE-2:FasterInferenceofLanguageModelswithDynamicDraftTrees
acceptancerate. Weleveragethisrelationshiptoapproxi-
Expand (Top-2)
matethevalue.
It (1.0)
Branchesstartingfromtokenswithhighervaluesaremore
0.6 0.2
likelytobeaccepted. Therefore,weselectthetop-knodes
is (0.6) has (0.2)
withthehighestvaluesinthelastlayerastheinputtothe
0.8 0.1 0.7 0.1
draftmodelandexpandthedrafttreebasedontheoutput.
ThetopofFigure7illustratestheexpansionphase. a (0.48) the (0.06) to (0.14) a (0.02)
0.7 0.1 0.6 0.2
4.2.RerankingPhase good (0.34) nice (0.05) be (0.08) do (0.03)
Thepurposeoftheexpansionphaseistodeepenthedraft
tree.Sinceacceptanceratesrangebetween0and1,thevalue Rerank(Top-8)
ofadeepertokenislower.Someshallownodesthatwerenot
expandedmayhavehighervaluesthanthedeeperexpanded It (1.0)
0.6 0.2
nodes. Therefore,wedonotusethetokensselectedduring
theexpansionphaseasthedraftdirectly. Instead,wererank is (0.6) has (0.2)
alldrafttokensandselectthetopmtokenswiththehighest 0.8 0.1 0.7 0.1
values. Thevalueofanodeisalwayslessthanorequalto a (0.48) the (0.06) to (0.14) a (0.02)
thatofitsparentnode. Fornodeswiththesamevalue,we 0.7 0.1 0.6 0.2
prioritizeselectingshallowernodes. Thisensuresthatthe good (0.34) nice (0.05) be (0.08) do (0.03)
topmtokensselectedafterrerankingstillformaconnected
tree.
Afterwards, we flatten the selected tokens into a one- Flatten to 1D It is has a the to good be
dimensionalsequencetoserveastheinputfortheverifica-
tionphase. Toensureconsistencywithvanillaautoregres- Attention mask
sivedecoding,wealsoneedtoadjusttheattentionmask. In
It is has a the to good be
vanillaautoregressivedecoding,eachtokencanseeallpre-
cedingtokens,resultinginalowertriangularattentionma- It ✓
trix. Whenusingadrafttree,tokensfromdifferentbranches
shouldnotbevisibletoeachother. Therefore,theattention is ✓ ✓
mask must be adjusted according to the tree structure to
has ✓ ✓
ensurethateachtokencanonlyseeitsancestornodes. The
bottomofFigure7illustratesthererankingPhase. ✓ ✓ ✓
a
✓ ✓ ✓
5.Experiments the
Models. WeconductexperimentsonVicuna7B,13B(Chi- to ✓ ✓ ✓
angetal.,2023), LLaMA2-Chat7B,13B,70B(Touvron
✓ ✓ ✓ ✓
good
etal.,2023),andLLaMA3-Instruct8B,70Bmodels(Meta,
2024).
✓ ✓ ✓ ✓
be
Tasks. Weconductcomprehensiveevaluationsonsixgen-
eration tasks. For multi-turn conversation, code genera-
Figure7: IllustrationofEAGLE-2. Thenumbersbesidethe
tion, mathematicalreasoning, instructionfollowing, sum-
edges represent the confidence scores of the draft model,
marization, and question answering tasks, we chose the
andthenumbersinbracketswithintheblocksrepresentthe
MT-bench (Zheng et al., 2023), HumanEval (Chen et al.,
valueofthenodes. Duringtheexpansionphase,weselect
2021),GSM8K(Cobbeetal.,2021),Alpaca(Taorietal.,
thetop2nodeswiththehighestvaluefromthecurrentlayer
2023),CNN/DailyMail(Nallapatietal.,2016),andNatural
(orange blocks) as inputs to the draft model and connect
Questions(Kwiatkowskietal.,2019)datasets,respectively.
the generated tokens (green blocks) to the draft tree. In
Wefollowedthecommonlyusedzero-shot/few-shotsettings
thererankphase,weselectthetop8nodeswiththehigh-
intheLLMscommunity,meaningthatthesamedraftmodel
estvaluefromallnodes(blueblocks),flattenthemintoa
weightswereusedfortheoriginalLLMacrossalltasks.
1-dimensional sequence to form the final draft. We then
constructtheattentionmaskaccordingtothetreestructure,
ensuringeachtokencanonlyseeitsancestornodes.
5EAGLE-2:FasterInferenceofLanguageModelswithDynamicDraftTrees
Metrics. EAGLE-2neitherfine-tunestheoriginalLLMnor andSFTdatasets,withthepre-trainingdatasetbeingmuch
relaxesacceptanceconditions,makingitalosslessaccelera- largerthantheSFTdataset.
tionmethod. Therefore,wedonotevaluatethegeneration
Tables 1 and 2 show the average acceptance lengths for
qualityandinsteadusethefollowingmetricstoassessac-
different methods, which is a hardware-independent met-
celerationperformance:
ric. Across all datasets and LLMs we tested, EAGLE-
2 achieved the longest average acceptance length. Each
• SpeedupRatio: Theactualtestspeedupratiorelative
drafting-verificationcycleofEAGLE-2generatesapprox-
tovanillaautoregressivedecoding.
imately4-5.5tokens,significantlyhigherthanothermeth-
ods, roughly twice that of standard speculative sampling
• Average Acceptance Length τ: The average num-
and Medusa. PLD and Lookahead have shorter average
beroftokensgeneratedperdrafting-verificationcycle,
acceptancelengths,butsincetheyeitherlackadraftmodel
whichcorrespondstothenumberoftokensaccepted
ortheirdraftmodelisnotaneuralnetwork,theoverhead
fromthedraft. Theadvantageofaverageacceptance
duringthedraftingphaseisverylow,resultinginaspeedup
lengthisthatitisindependentofhardwareandruntime
ratioveryclosetotheiraverageacceptancelength.
environment,whileitsdisadvantageisthatitdoesnot
reflecttheoverheadofthedraftmodel. Medusa, Hydra, EAGLE, and EAGLE-2 have lower av-
erageacceptancelengthsonQA(NaturalQuestions)and
Why is acceptance rate not included? The acceptance summarization(CNN/DM)taskscomparedtoothertasks,
rateonlyreflectstheperformanceofthedraftmodel. Since whereasstandardspeculativesamplingdoesnotshowthis
EAGLE-2doesnotmodifythestructureofthedraftmodel, reduction. The same pattern is observed for the speedup
theacceptancerateremainsthesameasthatofEAGLE. ratios. This discrepancy may be attributed to differences
inthetrainingdataforthedraftmodels. Thedraftmodel
Comparison. We use vanilla autoregressive decoding as
forstandardspeculativesamplingusesbothpretrainingand
the baseline, which serves as the benchmark for speedup
SFTdatasets,whileMedusa,Hydra,EAGLE,andEAGLE-
ratios(1.00x). WecompareEAGLE-2withrecentlossless
2 only use the SFT dataset. Natural Questions involves
speculativesamplingmethods,includingstandardspecula-
questionsaboutworldknowledge,suchas“Wherewasthe
tive sampling (Leviathan et al., 2023; Chen et al., 2023a;
2015rugbyunionworldcupheld?”,andworldknowledge
JoaoGante,2023),PLD(Saxena,2023),Medusa(Caietal.,
isprimarilyacquiredthroughpretrainingratherthanSFT.
2024),Lookahead(Fuetal.,2023),Hydra(Ankneretal.,
SummarizationtasksarealsolessrepresentedintheSFT
2024), and EAGLE (Li et al., 2024b). The speedup ra-
dataset. Thissuggeststhepotentialbenefitsofexpanding
tioishardware-dependent,sowetesteddifferentmethods
thedraftmodel’strainingdata. Despitethis,EAGLE-2still
on the same devices to ensure fairness. Our comparative
outperforms standard speculative sampling on these two
experiments utilized Spec-Bench (Xia et al., 2024). The
datasets.
implementationdetailsofthesemethodsandEAGLEcan
befoundinAppendixA.
5.2.AblationStudy
5.1.Effectiveness Inthissection,weconducttheablationstudy.
Figures 1 and 2, along with Tables 1 and 2, present the
5.2.1.VALUEANDCONFIDENCESCORE
speedupratiosofdifferentmethods. Acrossalldatasetsand
LLMswetested,EAGLE-2achievedthehighestspeedupra- EAGLE’s draft model provides a good approximation of
tios. Mostspeculativesamplingmethodsexhibitthehighest acceptancerates,butitislocalandcannotreflecttheactual
speedup on the code generation task (HumanEval), ben- probabilityofadrafttokenbeingaccepted.Therefore,when
efiting from the extensive use of fixed templates in code. selectingnodesforexpansion,weusethevalue,whichisthe
EAGLEachievedaspeedupofupto5xoncodegeneration productofadrafttoken’sconfidencescoreanditsancestor
tasks. PLDachievedthehighestspeedupratioonsumma- nodes’confidencescores,asthebasisforranking. Inthis
rizationtasks(CNN/DM)whenusingVicunaastheoriginal section,wecomparetheperformanceimpactofexpanding
LLM,duetoPLD’sretrieval-baseddraftgenerationandthe basedonvalueversusconfidencescore. Theexperimental
highoverlapincontextwhenVicunaperformssummariza- resultsinTable3showthatthespeedupratioandaverage
tion. Standardspeculativesampling,usingVicuna-68Mas acceptancelengtharebothhigherwhenexpandingbased
thedraftmodel,alsoachievedsignificantspeedupsbuthad onvalue,demonstratingtherationalebehindtheEAGLE-2
muchhighertrainingoverheadcomparedtoothermethods. approach.
PLDandLookaheaddonotrequiretraining,whileMedusa,
Hydra,EAGLE,andEAGLE-2useSFTdatasetsfortrain-
ingtheirdraftmodels. Vicuna-68Musedbothpre-training
6EAGLE-2:FasterInferenceofLanguageModelswithDynamicDraftTrees
Table 1: Speedup ratios and average acceptance lengths τ of different methods. V represents Vicuna, L2 represents
LLaMA2-Chat. SpSdenotesstandardspeculativesampling,withitsdraftmodelbeingVicuna-68M.MethodslikeMedusa
relaxacceptanceconditionsundernon-greedysettings,whichdonotguaranteelosslessacceleration. Therefore,wedonot
compareEAGLE-2withthesemethods.
MT-bench HumanEval GSM8K Alpaca CNN/DM NaturalQues. Mean
Model Method Speedup τ Speedup τ Speedup τ Speedup τ Speedup τ Speedup τ Speedup τ
Temperature=0
SpS 1.93x 2.27 2.23x 2.57 1.77x 2.01 1.76x 2.03 1.93x 2.33 1.66x 1.88 1.88x 2.18
PLD 1.58x 1.63 1.85x 1.93 1.68x 1.73 1.16x 1.19 2.42x 2.50 1.14x 1.17 1.64x 1.69
Medusa 2.07x 2.59 2.50x 2.78 2.23x 2.64 2.08x 2.45 1.71x 2.09 1.81x 2.10 2.07x 2.44
V13B Lookahead 1.65x 1.69 1.71x 1.75 1.81x 1.90 1.46x 1.51 1.46x 1.50 1.36x 1.39 1.58x 1.62
Hydra 2.88x 3.65 3.28x 3.87 2.93x 3.66 2.86x 3.53 2.05x 2.81 2.11x 2.88 2.69x 3.40
EAGLE 3.07x 3.98 3.58x 4.39 3.08x 3.97 3.03x 3.95 2.49x 3.52 2.42x 3.11 2.95x 3.82
EAGLE-2 4.26x 4.83 4.96x 5.41 4.22x 4.79 4.25x 4.89 3.40x 4.21 3.13x 3.74 4.04x 4.65
PLD 1.42x 1.46 1.63x 1.70 1.41x 1.44 1.16x 1.20 1.42x 1.45 1.12x 1.15 1.36x 1.40
Lookahead 1.58x 1.64 1.80x 1.85 1.65x 1.69 1.47x 1.50 1.46x 1.53 1.42x 1.45 1.56x 1.61
L213B EAGLE 3.03x 3.90 3.76x 4.52 3.20x 4.03 3.01x 3.83 2.70x 3.59 2.83x 3.47 3.09x 3.89
EAGLE-2 4.21x 4.75 5.00x 5.52 4.31x 4.90 4.13x 4.61 3.45x 4.24 3.51x 4.04 4.10x 4.68
SpS 1.82x 2.36 1.99x 2.61 1.71x 2.26 1.65x 2.21 1.81x 2.44 1.60x 2.16 1.76x 2.34
PLD 1.61x 1.68 1.82x 1.87 1.82x 1.99 1.21x 1.31 2.53x 2.72 1.23x 1.44 1.70x 1.84
Medusa 1.91x 2.52 2.02x 2.67 1.89x 2.59 1.79x 2.48 1.42x 2.02 1.51x 2.09 1.76x 2.40
V7B Lookahead 1.63x 1.69 1.72x 1.77 1.84x 1.99 1.38x 1.57 1.44x 1.53 1.45x 1.60 1.58x 1.69
Hydra 2.69x 3.60 2.98x 3.79 2.73x 3.66 2.66x 3.58 2.01x 2.70 2.25x 2.86 2.55x 3.37
EAGLE 2.90x 3.94 3.33x 4.29 3.01x 4.00 2.79x 3.89 2.33x 3.42 2.31x 3.21 2.78x 3.79
EAGLE-2 3.62x 4.98 3.95x 5.33 3.63x 4.97 3.46x 4.86 2.94x 4.12 2.76x 3.82 3.39x 4.68
PLD 1.38x 1.43 1.52x 1.59 1.32x 1.37 1.15x 1.19 1.48x 1.52 1.15x 1.20 1.33x 1.38
Lookahead 1.61x 1.66 1.72x 1.77 1.58x 1.65 1.49x 1.52 1.49x 1.54 1.48x 1.53 1.56x 1.61
L27B EAGLE 2.78x 3.62 3.17x 4.24 2.91x 3.82 2.78x 3.71 2.43x 3.41 2.61x 3.44 2.78x 3.71
EAGLE-2 3.43x 4.70 4.03x 5.39 3.52x 4.77 3.45x 4.66 3.01x 4.12 3.15x 4.19 3.43x 4.64
Temperature=1
SpS 1.62x 1.84 1.72x 1.97 1.46x 1.73 1.52x 1.78 1.66x 1.89 1.43x 1.70 1.55x 1.82
V13B EAGLE 2.32x 3.20 2.65x 3.63 2.57x 3.60 2.45x 3.57 2.23x 3.26 2.14x 3.06 2.39x 3.39
EAGLE-2 3.80x 4.40 4.22x 4.89 3.77x 4.41 3.78x 4.37 3.25x 3.97 3.07x 3.54 3.65x 4.26
EAGLE 2.68x 3.45 2.89x 3.78 2.82x 3.67 2.66x 3.55 2.41x 3.39 2.37x 3.31 2.64x 3.53
L213B EAGLE-2 3.92x 4.51 4.58x 5.29 4.21x 4.80 3.85x 4.48 3.31x 4.08 3.43x 3.89 3.88x 4.51
SpS 1.50x 1.87 1.55x 1.95 1.53x 1.82 1.56x 1.85 1.63x 1.91 1.33x 1.72 1.52x 1.85
V7B EAGLE 2.13x 3.17 2.39x 3.43 2.34x 3.29 2.21x 3.30 2.08x 3.12 1.95x 2.86 2.18x 3.20
EAGLE-2 3.05x 4.28 3.33x 4.65 3.07x 4.49 3.08x 4.43 2.63x 3.76 2.48x 3.56 2.94x 4.20
EAGLE 2.22x 3.30 2.61x 3.79 2.40x 3.52 2.29x 3.33 2.19x 3.15 2.22x 3.12 2.32x 3.37
L27B EAGLE-2 3.19x 4.41 3.67x 5.06 3.35x 4.62 3.20x 4.48 2.73x 3.85 2.81x 4.01 3.15x 4.41
5.2.2.RERANKING 2018; Shen et al., 2020; Kim et al., 2021; Zadeh et al.,
2020;Zafriretal.,2019),pruning(Galeetal.,2019;Sanh
ThepurposeofEAGLE-2’sexpansionphaseistodeepen
etal.,2020),andknowledgedistillation(Hintonetal.,2015).
thedrafttree,butthetokensselectedmaybegloballyless
Thesemethodsreducegenerationlatencybydecreasingthe
optimalthanshallownodesthatwerenotselected. There-
computationalcostofeachforwardpassoftheLLM.How-
fore, during the reranking phase, we rerank all the draft
ever,theseapproachesoftendegradeLLMperformanceto
tokens. Weconductedanablationstudyonthisoperation
some extent, resulting in a trade-off between generation
using the MT-bench and GSM8K dataset. As shown in
qualityandcomputationaloverhead.
Table3, rerankingimprovedboththeaverageacceptance
lengthandthespeedupratio. Speculativesamplingmethodsachievelosslessacceleration
byusingtheoriginalLLMforverification.Earlyspeculative
decodingmethods(Sternetal.,2018;Sunetal.,2021)accel-
6.RelatedWork
eratedgenerationingreedysettings,whileLeviathanetal.
WithwidespreadapplicationsofLLMs,therehasbeensig- (2023);Chenetal.(2023a)proposedspeculativesampling
nificant work (Liu et al., 2023b) focused on accelerating to extend the draft-verification framework to non-greedy
LLMinference,suchaslow-bitquantization(Hubaraetal., generation. Subsequentworkhaslargelyfocusedonreduc-
7EAGLE-2:FasterInferenceofLanguageModelswithDynamicDraftTrees
Table 2: Speedup ratios and average acceptance lengths originalLLM’sparametersasthedraftmodel. REST(Fu
τ with LLaMA2-Chat 70B, LLaMA3-Instruct 70B, and etal.,2024)andLLMA(Yangetal.,2023a)generatedrafts
LLaMA3-Instruct8BastheoriginalLLMs,withthetem- throughretrieval. OnlineSpeculativeDecoding(Liuetal.,
peraturesetto0,ontheMT-benchdataset. 2023a)andDistillSpec(Zhouetal.,2024)furtheralignthe
draftmodelwiththeoriginalLLMthroughadditionaltrain-
Model Method Speedup τ ing. CascadeSpeculativeDrafting(Chenetal.,2023b)and
StagedSpeculativeDecoding(Spector&Re,2023)cascade
PLD 1.31x 1.39
draftmodelsofdifferentsizes.
Lookahead 1.52x 1.64
LLaMA2-Chat70B EAGLE 3.01x 3.81 Speculativesamplingmethodscanachievelosslessacceler-
EAGLE-2 3.51x 4.48 ation,buttheycanalsotradeoffqualityforhigherspeedup
ratios. For example, BiLD (Kim et al., 2024) relaxes the
EAGLE 2.83x 3.62
LLaMA3-Instruct70B EAGLE-2 3.29x 4.16 acceptanceconditions,whileMedusa-2(Caietal.,2024),
CLLMs (Kou et al., 2024), and SPACE (Yi et al., 2024)
EAGLE 2.72x 3.65
fine-tunetheoriginalLLMs.
LLaMA3-Instruct8B EAGLE-2 3.46x 4.53
7.Conclusion
Table3: Ablationexperimentresultswithtemperatureset
to0onVicuna7B.“w/ovalue”indicatesnotusingvalue Inthispaper,weintroduceEAGLE-2,anefficientandloss-
and directly using confidence, “w/o reranking” indicates lessspeculativesamplingmethod. WefoundthatEAGLE’s
notperformingreranking,and“w/oboth”indicatesneither draftmodelconfidenceisagoodapproximationoftheac-
valuenorrerankingisused. ceptance rate for draft tokens. Based on this, EAGLE-2
employs a context-dependent draft tree structure, signifi-
cantlyincreasingthenumberofaccepteddrafttokensand
MT-bench GSM8K
resultinginbetterspeedupratios. EAGLE-2ensuresthat
Method Speedup τ Speedup τ thegeneratedresultsareconsistentwiththeoriginalLLMs
and does not require additional training. We conducted
w/oboth 2.81x 3.92 2.85x 3.93
extensiveevaluationsusingvariousLLMsacrossmultiple
w/ovalue 3.21x 4.39 2.93x 3.96
datasetsandcomparedEAGLE-2withseveralstate-of-the-
w/oreranking 3.48x 4.86 3.50x 4.85
artspeculativesamplingmethods. Inallourexperiments,
EAGLE-2 3.62x 4.98 3.63x 4.97
EAGLE-2achievedthehighestspeedupratios.
References
ingdraftoverheadandenhancingconsistencybetweenthe
draftandtheoriginalLLM.SpecInfer(Miaoetal.,2023) Ankner, Z., Parthasarathy, R., Nrusimha, A., Rinard, C.,
integrates multiple small models as the draft model, ag- Ragan-Kelley,J.,andBrandon,W. Hydra: Sequentially-
gregating their drafts into a tree and using tree attention dependent draft heads for medusa decoding. arXiv
for parallel verification. Medusa (Cai et al., 2024) trains preprintarXiv:2402.05109,2024.
a set of MLPs to parallelly predict multiple tokens using
theoriginalLLM’sfeatures,significantlyreducingthela- Cai, T., Li, Y., Geng, Z., Peng, H., Lee, J. D., Chen, D.,
tencyduringthedraftingphase. EAGLE(Lietal.,2024b) andDao,T. Medusa: Simplellminferenceacceleration
autoregressively predicts feature sequences instead of to- frameworkwithmultipledecodingheads. arXivpreprint
kensequencesandinputsthesamplingresultsintothedraft arXiv: 2401.10774,2024.
modeltoaddressuncertaintyatthefeaturelevel,substan-
tiallyimprovingthedraftmodel’saccuracy. Thisprinciple Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre,
of eliminating uncertainty is also used in Hydra (Ankner L., and Jumper, J. Accelerating large language model
et al., 2024) and Recurrent Drafter (Zhang et al., 2024). decoding with speculative sampling. arXiv preprint
Parallel Decoding (Santilli et al., 2023), Lookahead (Fu arXiv:2302.01318,2023a.
et al., 2023), Ouroboros (Zhao et al., 2024), and CLLMs
(Kou et al., 2024) generate drafts using Jacobi iterations. Chen,M.,Tworek,J.,Jun,H.,Yuan,Q.,Pinto,H.P.d.O.,
Methods(Hooperetal.,2023;Yangetal.,2023b;Monea Kaplan,J.,Edwards,H.,Burda,Y.,Joseph,N.,Brockman,
etal.,2023;Lietal.,2024a;Yietal.,2024;Liuetal.,2024; G., etal. Evaluatinglargelanguagemodelstrainedon
Sunetal.,2024a;Elhoushietal.,2024;Svirschevskietal., code. arXivpreprintarXiv:2107.03374,2021.
2024)likeDraft&Verify(Zhangetal.,2023)utilizetech-
niquessuchaslayerskippingorearlyexit,usingpartsofthe Chen,Z.,Yang,X.,Lin,J.,Sun,C.,Huang,J.,andChang,
8EAGLE-2:FasterInferenceofLanguageModelswithDynamicDraftTrees
K.C.-C. Cascadespeculativedraftingforevenfasterllm Kim, S., Gholami, A., Yao, Z., Mahoney, M. W., and
inference. arXivpreprintarXiv:2312.11462,2023b. Keutzer, K. I-bert: Integer-only bert quantization. In
Internationalconferenceonmachinelearning,pp.5506–
Chen,Z.,May,A.,Svirschevski,R.,Huang,Y.,Ryabinin,
5518.PMLR,2021.
M.,Jia,Z.,andChen,B. Sequoia: Scalable,robust,and
hardware-aware speculative decoding. arXiv preprint Kim, S., Mangalam, K., Moon, S., Malik, J., Mahoney,
arXiv:2402.12374,2024. M.W.,Gholami,A.,andKeutzer,K. Speculativedecod-
ingwithbiglittledecoder. AdvancesinNeuralInforma-
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,
tionProcessingSystems,36,2024.
H.,Zheng,L.,Zhuang,S.,Zhuang,Y.,Gonzalez,J.E.,
Stoica, I., and Xing, E. P. Vicuna: An open-source Kou, S., Hu, L., He, Z., Deng, Z., and Zhang, H. Cllms:
chatbot impressing gpt-4 with 90%* chatgpt quality, Consistency large language models. arXiv preprint
March 2023. URL https://lmsys.org/blog/ arXiv:2403.00835,2024.
2023-03-30-vicuna/.
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M.,
Cobbe,K.,Kosaraju,V.,Bavarian,M.,Chen,M.,Jun,H., Parikh,A.,Alberti,C.,Epstein,D.,Polosukhin,I.,Devlin,
Kaiser,L.,Plappert,M.,Tworek,J.,Hilton,J.,Nakano, J.,Lee,K.,etal.Naturalquestions:abenchmarkforques-
R.,etal. Trainingverifierstosolvemathwordproblems. tionansweringresearch. TransactionsoftheAssociation
arXivpreprintarXiv:2110.14168,2021. forComputationalLinguistics,7:453–466,2019.
Elhoushi,M.,Shrivastava,A.,Liskovich,D.,Hosmer,B., Leviathan,Y.,Kalman,M.,andMatias,Y. Fastinference
Wasti, B., Lai, L., Mahmoud, A., Acun, B., Agarwal, from transformers via speculative decoding. In Inter-
S., Roman, A., et al. Layer skip: Enabling early exit nationalConferenceonMachineLearning,pp.19274–
inferenceandself-speculativedecoding. arXivpreprint 19286.PMLR,2023.
arXiv:2404.16710,2024.
Li, M., Chen, X., Holtzman, A., Chen, B., Lin, J., Yih,
W.-t., and Lin, X. V. Nearest neighbor speculative de-
Fu, Y., Bailis, P., Stoica, I., and Zhang, H. Break-
codingforllmgenerationandattribution. arXivpreprint
ing the sequential dependency of llm infer-
arXiv:2405.19325,2024a.
ence using lookahead decoding, November
2023. URL https://lmsys.org/blog/
Li,Y.,Wei,F.,Zhang,C.,andZhang,H. Eagle: Specula-
2023-11-21-lookahead-decoding/.
tivesamplingrequiresrethinkingfeatureuncertainty. In
InternationalConferenceonMachineLearning,2024b.
Fu, Y., Bailis, P., Stoica, I., and Zhang, H. Break the se-
quential dependency of llm inference using lookahead
Liu, F., Tang, Y., Liu, Z., Ni, Y., Han, K., and Wang, Y.
decoding. arXivpreprintarXiv:2402.02057,2024.
Kangaroo: Losslessself-speculativedecodingviadouble
earlyexiting. arXivpreprintarXiv:2404.18911,2024.
Gale, T., Elsen, E., and Hooker, S. The state of spar-
sity in deep neural networks.(2019). arXiv preprint Liu, X., Hu, L., Bailis, P., Stoica, I., Deng, Z., Cheung,
cs.LG/1902.09574,2019. A.,andZhang,H. Onlinespeculativedecoding. arXiv
preprintarXiv:2310.07177,2023a.
Hinton, G., Vinyals, O., and Dean, J. Distilling
the knowledge in a neural network. arXiv preprint Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z.,
arXiv:1503.02531,2015. Shrivastava,A.,Zhang,C.,Tian,Y.,Re,C.,etal. Deja
vu:Contextualsparsityforefficientllmsatinferencetime.
Hooper, C., Kim, S., Mohammadzadeh, H., Genc, H.,
In International Conference on Machine Learning, pp.
Keutzer, K., Gholami, A., and Shao, S. Speed: Spec-
22137–22176.PMLR,2023b.
ulativepipelinedexecutionforefficientdecoding. arXiv
preprintarXiv:2310.12072,2023. Meta. LLaMA3. https://github.com/
pytorch-labs/gpt-fast/,2024.
Hubara,I.,Courbariaux,M.,Soudry,D.,El-Yaniv,R.,and
Bengio,Y. Quantizedneuralnetworks: Trainingneural Miao,X.,Oliaro,G.,Zhang,Z.,Cheng,X.,Wang,Z.,Wong,
networkswithlowprecisionweightsandactivations.jour- R.Y.Y., Chen, Z., Arfeen, D., Abhyankar, R., andJia,
nalofmachinelearningresearch,18(187):1–30,2018. Z. SpecInfer: AcceleratinggenerativeLLMservingwith
speculativeinferenceandtokentreeverification. arXiv
Joao Gante. Assisted generation: a new direc-
preprintarXiv:2305.09781,2023.
tion toward low-latency text generation, 2023.
URL https://huggingface.co/blog/ Monea,G.,Joulin,A.,andGrave,E. Pass: Parallelspecula-
assisted-generation. tivesampling. arXivpreprintarXiv:2311.13581,2023.
9EAGLE-2:FasterInferenceofLanguageModelswithDynamicDraftTrees
Nallapati,R.,Zhou,B.,Gulcehre,C.,Xiang,B.,etal. Ab- Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li,
stractivetextsummarizationusingsequence-to-sequence X., Guestrin, C., Liang, P., and Hashimoto, T. B.
rnnsandbeyond. arXivpreprintarXiv:1602.06023,2016. Stanford alpaca: An instruction-following llama
model. https://github.com/tatsu-lab/
OpenAI,R. Gpt-4technicalreport.arxiv2303.08774. View stanford_alpaca,2023.
inArticle,2(5),2023.
Touvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux,
Sanh,V.,Wolf,T.,andRush,A. Movementpruning: Adap- M.-A.,Lacroix,T.,Rozie`re,B.,Goyal,N.,Hambro,E.,
tivesparsitybyfine-tuning. AdvancesinNeuralInforma- Azhar,F.,etal. Llama:Openandefficientfoundationlan-
tionProcessingSystems,33:20378–20389,2020. guagemodels(2023). arXivpreprintarXiv:2302.13971,
2023.
Santilli,A.,Severino,S.,Postolache,E.,Maiorca,V.,Man-
Xia,H.,Yang,Z.,Dong,Q.,Wang,P.,Li,Y.,Ge,T.,Liu,T.,
cusi,M.,Marin,R.,andRodola`,E. Acceleratingtrans-
Li,W.,andSui,Z.Unlockingefficiencyinlargelanguage
former inference for translation via parallel decoding.
modelinference: Acomprehensivesurveyofspeculative
arXivpreprintarXiv:2305.10427,2023.
decoding,2024.
Saxena, A. Prompt lookup decoding, November 2023.
Yang, N., Ge, T., Wang, L., Jiao, B., Jiang, D., Yang, L.,
URL https://github.com/apoorvumang/
Majumder, R., and Wei, F. Inference with reference:
prompt-lookup-decoding/.
Lossless acceleration of large language models. arXiv
preprintarXiv:2304.04487,2023a.
Shen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A.,
Mahoney,M.W.,andKeutzer,K. Q-bert: Hessianbased Yang, S., Lee, G., Cho, J., Papailiopoulos, D., and
ultralowprecisionquantizationofbert. InProceedings Lee, K. Predictive pipelined decoding: A compute-
of the AAAI Conference on Artificial Intelligence, vol- latencytrade-offforexactllmdecoding. arXivpreprint
ume34,pp.8815–8821,2020. arXiv:2307.05908,2023b.
Spector, B. and Re, C. Accelerating llm inference Yi,H.,Lin,F.,Li,H.,Ning,P.,Yu,X.,andXiao,R. Gen-
with staged speculative decoding. arXiv preprint erationmeetsverification: Acceleratinglargelanguage
arXiv:2308.04623,2023. modelinferencewithsmartparallelauto-correctdecod-
ing. arXivpreprintarXiv:2402.11809,2024.
Stern,M.,Shazeer,N.,andUszkoreit,J. Blockwiseparallel
decodingfordeepautoregressivemodels. Advancesin Zadeh, A. H., Edo, I., Awad, O. M., and Moshovos, A.
NeuralInformationProcessingSystems,31,2018. Gobo: Quantizingattention-basednlpmodelsforlowla-
tencyandenergyefficientinference.In202053rdAnnual
Sun, H., Chen, Z., Yang, X., Tian, Y., and Chen, B. Tri- IEEE/ACMInternationalSymposiumonMicroarchitec-
force: Losslessaccelerationoflongsequencegeneration ture(MICRO),pp.811–824.IEEE,2020.
with hierarchical speculative decoding. arXiv preprint
Zafrir, O., Boudoukh, G., Izsak, P., and Wasserblat, M.
arXiv:2404.11912,2024a.
Q8bert: Quantized 8bit bert. In 2019 Fifth Workshop
on Energy Efficient Machine Learning and Cognitive
Sun,X.,Ge,T.,Wei,F.,andWang,H. Instantaneousgram-
Computing-NeurIPSEdition(EMC2-NIPS),pp.36–39.
maticalerrorcorrectionwithshallowaggressivedecoding.
IEEE,2019.
arXivpreprintarXiv:2106.04970,2021.
Zhang, A., Wang, C., Wang, Y., Zhang, X., and Cheng,
Sun,Z.,Ro,J.H.,Beirami,A.,andSuresh,A.T. Optimal
Y. Recurrent drafter for fast speculative decoding in
block-leveldraftverificationforacceleratingspeculative
largelanguagemodels. arXivpreprintarXiv:2403.09919,
decoding. arXivpreprintarXiv:2403.10444,2024b.
2024.
Sun, Z., Suresh, A. T., Ro, J. H., Beirami, A., Jain, H., Zhang,J.,Wang,J.,Li,H.,Shou,L.,Chen,K.,Chen,G.,
andYu,F. Spectr: Fastspeculativedecodingviaoptimal andMehrotra,S. Draft&verify: Losslesslargelanguage
transport. AdvancesinNeuralInformationProcessing modelaccelerationviaself-speculativedecoding. arXiv
Systems,36,2024c. preprintarXiv:2309.08168,2023.
Svirschevski,R.,May,A.,Chen,Z.,Chen,B.,Jia,Z.,and Zhao,W.,Huang,Y.,Han,X.,Xiao,C.,Liu,Z.,andSun,
Ryabinin,M. Specexec: Massivelyparallelspeculative M. Ouroboros: Speculativedecodingwithlargemodel
decodingforinteractivellminferenceonconsumerde- enhanced drafting. arXiv preprint arXiv:2402.13720,
vices. arXivpreprintarXiv:2406.02532,2024. 2024.
10EAGLE-2:FasterInferenceofLanguageModelswithDynamicDraftTrees
Zheng,L.,Chiang,W.-L.,Sheng,Y.,Zhuang,S.,Wu,Z.,
Zhuang,Y.,Lin,Z.,Li,Z.,Li,D.,Xing,E.,etal. Judging
llm-as-a-judgewithmt-benchandchatbotarena. arXiv
preprintarXiv:2306.05685,2023.
Zhou, Y., Lyu, K., Rawat, A. S., Menon, A. K., Ros-
tamizadeh,A.,Kumar,S.,Kagy,J.-F.,andAgarwal,R.
Distillspec: Improvingspeculativedecodingviaknowl-
edge distillation. In The Twelfth International Confer-
enceonLearningRepresentations,2024. URLhttps:
//openreview.net/forum?id=rsY6J3ZaTF.
11EAGLE-2:FasterInferenceofLanguageModelswithDynamicDraftTrees
A.ImplementationDetails
Vanilla: WeusemodelsfromtheHuggingface.transformerslibrarywiththePyTorchbackendandpre-allocatedKVcache.
Othermethodsalsousethesemodelsastheirbase.
(Standard)SpeculativeSampling: WeusetheassistedgenerationfeaturefromtheHuggingFaceTransformerslibrary.
PLD,Lookahead,Medusa,andHydra: Weusethedefaultsettingsandtheofficiallyreleasedweights.
EAGLE:VicunaandLLaMA2-Chatdraftmodelsusetheofficiallyreleasedweights,whileLLaMA3-Instructistrained
usingtheShareGPTdataset(consistentwithMedusaandHydra).
EAGLE-2: For the 7B (8B), 13B, and 70B original LLMs, we set the total number of draft tokens to 60, 50, and 48,
respectively,withadrafttreedepthof6,andselect10nodesduringtheexpansionphase.
12